<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240623.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "GeoLRM: Geometry-Aware Large Reconstruction Model for High-Quality 3D\n  Gaussian Generation", "author": "Chubin Zhang and Hongliang Song and Yi Wei and Yu Chen and Jiwen Lu and Yansong Tang", "abstract": "  In this work, we introduce the Geometry-Aware Large Reconstruction Model\n(GeoLRM), an approach which can predict high-quality assets with 512k Gaussians\nand 21 input images in only 11 GB GPU memory. Previous works neglect the\ninherent sparsity of 3D structure and do not utilize explicit geometric\nrelationships between 3D and 2D images. This limits these methods to a\nlow-resolution representation and makes it difficult to scale up to the dense\nviews for better quality. GeoLRM tackles these issues by incorporating a novel\n3D-aware transformer structure that directly processes 3D points and uses\ndeformable cross-attention mechanisms to effectively integrate image features\ninto 3D representations. We implement this solution through a two-stage\npipeline: initially, a lightweight proposal network generates a sparse set of\n3D anchor points from the posed image inputs; subsequently, a specialized\nreconstruction transformer refines the geometry and retrieves textural details.\nExtensive experimental results demonstrate that GeoLRM significantly\noutperforms existing models, especially for dense view inputs. We also\ndemonstrate the practical applicability of our model with 3D generation tasks,\nshowcasing its versatility and potential for broader adoption in real-world\napplications.\n", "link": "http://arxiv.org/abs/2406.15333v1", "date": "2024-06-21", "relevancy": 3.197, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7064}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6521}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5597}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoLRM%3A%20Geometry-Aware%20Large%20Reconstruction%20Model%20for%20High-Quality%203D%0A%20%20Gaussian%20Generation&body=Title%3A%20GeoLRM%3A%20Geometry-Aware%20Large%20Reconstruction%20Model%20for%20High-Quality%203D%0A%20%20Gaussian%20Generation%0AAuthor%3A%20Chubin%20Zhang%20and%20Hongliang%20Song%20and%20Yi%20Wei%20and%20Yu%20Chen%20and%20Jiwen%20Lu%20and%20Yansong%20Tang%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20introduce%20the%20Geometry-Aware%20Large%20Reconstruction%20Model%0A%28GeoLRM%29%2C%20an%20approach%20which%20can%20predict%20high-quality%20assets%20with%20512k%20Gaussians%0Aand%2021%20input%20images%20in%20only%2011%20GB%20GPU%20memory.%20Previous%20works%20neglect%20the%0Ainherent%20sparsity%20of%203D%20structure%20and%20do%20not%20utilize%20explicit%20geometric%0Arelationships%20between%203D%20and%202D%20images.%20This%20limits%20these%20methods%20to%20a%0Alow-resolution%20representation%20and%20makes%20it%20difficult%20to%20scale%20up%20to%20the%20dense%0Aviews%20for%20better%20quality.%20GeoLRM%20tackles%20these%20issues%20by%20incorporating%20a%20novel%0A3D-aware%20transformer%20structure%20that%20directly%20processes%203D%20points%20and%20uses%0Adeformable%20cross-attention%20mechanisms%20to%20effectively%20integrate%20image%20features%0Ainto%203D%20representations.%20We%20implement%20this%20solution%20through%20a%20two-stage%0Apipeline%3A%20initially%2C%20a%20lightweight%20proposal%20network%20generates%20a%20sparse%20set%20of%0A3D%20anchor%20points%20from%20the%20posed%20image%20inputs%3B%20subsequently%2C%20a%20specialized%0Areconstruction%20transformer%20refines%20the%20geometry%20and%20retrieves%20textural%20details.%0AExtensive%20experimental%20results%20demonstrate%20that%20GeoLRM%20significantly%0Aoutperforms%20existing%20models%2C%20especially%20for%20dense%20view%20inputs.%20We%20also%0Ademonstrate%20the%20practical%20applicability%20of%20our%20model%20with%203D%20generation%20tasks%2C%0Ashowcasing%20its%20versatility%20and%20potential%20for%20broader%20adoption%20in%20real-world%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15333v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoLRM%253A%2520Geometry-Aware%2520Large%2520Reconstruction%2520Model%2520for%2520High-Quality%25203D%250A%2520%2520Gaussian%2520Generation%26entry.906535625%3DChubin%2520Zhang%2520and%2520Hongliang%2520Song%2520and%2520Yi%2520Wei%2520and%2520Yu%2520Chen%2520and%2520Jiwen%2520Lu%2520and%2520Yansong%2520Tang%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520the%2520Geometry-Aware%2520Large%2520Reconstruction%2520Model%250A%2528GeoLRM%2529%252C%2520an%2520approach%2520which%2520can%2520predict%2520high-quality%2520assets%2520with%2520512k%2520Gaussians%250Aand%252021%2520input%2520images%2520in%2520only%252011%2520GB%2520GPU%2520memory.%2520Previous%2520works%2520neglect%2520the%250Ainherent%2520sparsity%2520of%25203D%2520structure%2520and%2520do%2520not%2520utilize%2520explicit%2520geometric%250Arelationships%2520between%25203D%2520and%25202D%2520images.%2520This%2520limits%2520these%2520methods%2520to%2520a%250Alow-resolution%2520representation%2520and%2520makes%2520it%2520difficult%2520to%2520scale%2520up%2520to%2520the%2520dense%250Aviews%2520for%2520better%2520quality.%2520GeoLRM%2520tackles%2520these%2520issues%2520by%2520incorporating%2520a%2520novel%250A3D-aware%2520transformer%2520structure%2520that%2520directly%2520processes%25203D%2520points%2520and%2520uses%250Adeformable%2520cross-attention%2520mechanisms%2520to%2520effectively%2520integrate%2520image%2520features%250Ainto%25203D%2520representations.%2520We%2520implement%2520this%2520solution%2520through%2520a%2520two-stage%250Apipeline%253A%2520initially%252C%2520a%2520lightweight%2520proposal%2520network%2520generates%2520a%2520sparse%2520set%2520of%250A3D%2520anchor%2520points%2520from%2520the%2520posed%2520image%2520inputs%253B%2520subsequently%252C%2520a%2520specialized%250Areconstruction%2520transformer%2520refines%2520the%2520geometry%2520and%2520retrieves%2520textural%2520details.%250AExtensive%2520experimental%2520results%2520demonstrate%2520that%2520GeoLRM%2520significantly%250Aoutperforms%2520existing%2520models%252C%2520especially%2520for%2520dense%2520view%2520inputs.%2520We%2520also%250Ademonstrate%2520the%2520practical%2520applicability%2520of%2520our%2520model%2520with%25203D%2520generation%2520tasks%252C%250Ashowcasing%2520its%2520versatility%2520and%2520potential%2520for%2520broader%2520adoption%2520in%2520real-world%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15333v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoLRM%3A%20Geometry-Aware%20Large%20Reconstruction%20Model%20for%20High-Quality%203D%0A%20%20Gaussian%20Generation&entry.906535625=Chubin%20Zhang%20and%20Hongliang%20Song%20and%20Yi%20Wei%20and%20Yu%20Chen%20and%20Jiwen%20Lu%20and%20Yansong%20Tang&entry.1292438233=%20%20In%20this%20work%2C%20we%20introduce%20the%20Geometry-Aware%20Large%20Reconstruction%20Model%0A%28GeoLRM%29%2C%20an%20approach%20which%20can%20predict%20high-quality%20assets%20with%20512k%20Gaussians%0Aand%2021%20input%20images%20in%20only%2011%20GB%20GPU%20memory.%20Previous%20works%20neglect%20the%0Ainherent%20sparsity%20of%203D%20structure%20and%20do%20not%20utilize%20explicit%20geometric%0Arelationships%20between%203D%20and%202D%20images.%20This%20limits%20these%20methods%20to%20a%0Alow-resolution%20representation%20and%20makes%20it%20difficult%20to%20scale%20up%20to%20the%20dense%0Aviews%20for%20better%20quality.%20GeoLRM%20tackles%20these%20issues%20by%20incorporating%20a%20novel%0A3D-aware%20transformer%20structure%20that%20directly%20processes%203D%20points%20and%20uses%0Adeformable%20cross-attention%20mechanisms%20to%20effectively%20integrate%20image%20features%0Ainto%203D%20representations.%20We%20implement%20this%20solution%20through%20a%20two-stage%0Apipeline%3A%20initially%2C%20a%20lightweight%20proposal%20network%20generates%20a%20sparse%20set%20of%0A3D%20anchor%20points%20from%20the%20posed%20image%20inputs%3B%20subsequently%2C%20a%20specialized%0Areconstruction%20transformer%20refines%20the%20geometry%20and%20retrieves%20textural%20details.%0AExtensive%20experimental%20results%20demonstrate%20that%20GeoLRM%20significantly%0Aoutperforms%20existing%20models%2C%20especially%20for%20dense%20view%20inputs.%20We%20also%0Ademonstrate%20the%20practical%20applicability%20of%20our%20model%20with%203D%20generation%20tasks%2C%0Ashowcasing%20its%20versatility%20and%20potential%20for%20broader%20adoption%20in%20real-world%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15333v1&entry.124074799=Read"},
{"title": "Image Conductor: Precision Control for Interactive Video Synthesis", "author": "Yaowei Li and Xintao Wang and Zhaoyang Zhang and Zhouxia Wang and Ziyang Yuan and Liangbin Xie and Yuexian Zou and Ying Shan", "abstract": "  Filmmaking and animation production often require sophisticated techniques\nfor coordinating camera transitions and object movements, typically involving\nlabor-intensive real-world capturing. Despite advancements in generative AI for\nvideo creation, achieving precise control over motion for interactive video\nasset generation remains challenging. To this end, we propose Image Conductor,\na method for precise control of camera transitions and object movements to\ngenerate video assets from a single image. An well-cultivated training strategy\nis proposed to separate distinct camera and object motion by camera LoRA\nweights and object LoRA weights. To further address cinematographic variations\nfrom ill-posed trajectories, we introduce a camera-free guidance technique\nduring inference, enhancing object movements while eliminating camera\ntransitions. Additionally, we develop a trajectory-oriented video motion data\ncuration pipeline for training. Quantitative and qualitative experiments\ndemonstrate our method's precision and fine-grained control in generating\nmotion-controllable videos from images, advancing the practical application of\ninteractive video synthesis. Project webpage available at\nhttps://liyaowei-stu.github.io/project/ImageConductor/\n", "link": "http://arxiv.org/abs/2406.15339v1", "date": "2024-06-21", "relevancy": 3.0405, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6173}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6165}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image%20Conductor%3A%20Precision%20Control%20for%20Interactive%20Video%20Synthesis&body=Title%3A%20Image%20Conductor%3A%20Precision%20Control%20for%20Interactive%20Video%20Synthesis%0AAuthor%3A%20Yaowei%20Li%20and%20Xintao%20Wang%20and%20Zhaoyang%20Zhang%20and%20Zhouxia%20Wang%20and%20Ziyang%20Yuan%20and%20Liangbin%20Xie%20and%20Yuexian%20Zou%20and%20Ying%20Shan%0AAbstract%3A%20%20%20Filmmaking%20and%20animation%20production%20often%20require%20sophisticated%20techniques%0Afor%20coordinating%20camera%20transitions%20and%20object%20movements%2C%20typically%20involving%0Alabor-intensive%20real-world%20capturing.%20Despite%20advancements%20in%20generative%20AI%20for%0Avideo%20creation%2C%20achieving%20precise%20control%20over%20motion%20for%20interactive%20video%0Aasset%20generation%20remains%20challenging.%20To%20this%20end%2C%20we%20propose%20Image%20Conductor%2C%0Aa%20method%20for%20precise%20control%20of%20camera%20transitions%20and%20object%20movements%20to%0Agenerate%20video%20assets%20from%20a%20single%20image.%20An%20well-cultivated%20training%20strategy%0Ais%20proposed%20to%20separate%20distinct%20camera%20and%20object%20motion%20by%20camera%20LoRA%0Aweights%20and%20object%20LoRA%20weights.%20To%20further%20address%20cinematographic%20variations%0Afrom%20ill-posed%20trajectories%2C%20we%20introduce%20a%20camera-free%20guidance%20technique%0Aduring%20inference%2C%20enhancing%20object%20movements%20while%20eliminating%20camera%0Atransitions.%20Additionally%2C%20we%20develop%20a%20trajectory-oriented%20video%20motion%20data%0Acuration%20pipeline%20for%20training.%20Quantitative%20and%20qualitative%20experiments%0Ademonstrate%20our%20method%27s%20precision%20and%20fine-grained%20control%20in%20generating%0Amotion-controllable%20videos%20from%20images%2C%20advancing%20the%20practical%20application%20of%0Ainteractive%20video%20synthesis.%20Project%20webpage%20available%20at%0Ahttps%3A//liyaowei-stu.github.io/project/ImageConductor/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15339v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage%2520Conductor%253A%2520Precision%2520Control%2520for%2520Interactive%2520Video%2520Synthesis%26entry.906535625%3DYaowei%2520Li%2520and%2520Xintao%2520Wang%2520and%2520Zhaoyang%2520Zhang%2520and%2520Zhouxia%2520Wang%2520and%2520Ziyang%2520Yuan%2520and%2520Liangbin%2520Xie%2520and%2520Yuexian%2520Zou%2520and%2520Ying%2520Shan%26entry.1292438233%3D%2520%2520Filmmaking%2520and%2520animation%2520production%2520often%2520require%2520sophisticated%2520techniques%250Afor%2520coordinating%2520camera%2520transitions%2520and%2520object%2520movements%252C%2520typically%2520involving%250Alabor-intensive%2520real-world%2520capturing.%2520Despite%2520advancements%2520in%2520generative%2520AI%2520for%250Avideo%2520creation%252C%2520achieving%2520precise%2520control%2520over%2520motion%2520for%2520interactive%2520video%250Aasset%2520generation%2520remains%2520challenging.%2520To%2520this%2520end%252C%2520we%2520propose%2520Image%2520Conductor%252C%250Aa%2520method%2520for%2520precise%2520control%2520of%2520camera%2520transitions%2520and%2520object%2520movements%2520to%250Agenerate%2520video%2520assets%2520from%2520a%2520single%2520image.%2520An%2520well-cultivated%2520training%2520strategy%250Ais%2520proposed%2520to%2520separate%2520distinct%2520camera%2520and%2520object%2520motion%2520by%2520camera%2520LoRA%250Aweights%2520and%2520object%2520LoRA%2520weights.%2520To%2520further%2520address%2520cinematographic%2520variations%250Afrom%2520ill-posed%2520trajectories%252C%2520we%2520introduce%2520a%2520camera-free%2520guidance%2520technique%250Aduring%2520inference%252C%2520enhancing%2520object%2520movements%2520while%2520eliminating%2520camera%250Atransitions.%2520Additionally%252C%2520we%2520develop%2520a%2520trajectory-oriented%2520video%2520motion%2520data%250Acuration%2520pipeline%2520for%2520training.%2520Quantitative%2520and%2520qualitative%2520experiments%250Ademonstrate%2520our%2520method%2527s%2520precision%2520and%2520fine-grained%2520control%2520in%2520generating%250Amotion-controllable%2520videos%2520from%2520images%252C%2520advancing%2520the%2520practical%2520application%2520of%250Ainteractive%2520video%2520synthesis.%2520Project%2520webpage%2520available%2520at%250Ahttps%253A//liyaowei-stu.github.io/project/ImageConductor/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15339v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image%20Conductor%3A%20Precision%20Control%20for%20Interactive%20Video%20Synthesis&entry.906535625=Yaowei%20Li%20and%20Xintao%20Wang%20and%20Zhaoyang%20Zhang%20and%20Zhouxia%20Wang%20and%20Ziyang%20Yuan%20and%20Liangbin%20Xie%20and%20Yuexian%20Zou%20and%20Ying%20Shan&entry.1292438233=%20%20Filmmaking%20and%20animation%20production%20often%20require%20sophisticated%20techniques%0Afor%20coordinating%20camera%20transitions%20and%20object%20movements%2C%20typically%20involving%0Alabor-intensive%20real-world%20capturing.%20Despite%20advancements%20in%20generative%20AI%20for%0Avideo%20creation%2C%20achieving%20precise%20control%20over%20motion%20for%20interactive%20video%0Aasset%20generation%20remains%20challenging.%20To%20this%20end%2C%20we%20propose%20Image%20Conductor%2C%0Aa%20method%20for%20precise%20control%20of%20camera%20transitions%20and%20object%20movements%20to%0Agenerate%20video%20assets%20from%20a%20single%20image.%20An%20well-cultivated%20training%20strategy%0Ais%20proposed%20to%20separate%20distinct%20camera%20and%20object%20motion%20by%20camera%20LoRA%0Aweights%20and%20object%20LoRA%20weights.%20To%20further%20address%20cinematographic%20variations%0Afrom%20ill-posed%20trajectories%2C%20we%20introduce%20a%20camera-free%20guidance%20technique%0Aduring%20inference%2C%20enhancing%20object%20movements%20while%20eliminating%20camera%0Atransitions.%20Additionally%2C%20we%20develop%20a%20trajectory-oriented%20video%20motion%20data%0Acuration%20pipeline%20for%20training.%20Quantitative%20and%20qualitative%20experiments%0Ademonstrate%20our%20method%27s%20precision%20and%20fine-grained%20control%20in%20generating%0Amotion-controllable%20videos%20from%20images%2C%20advancing%20the%20practical%20application%20of%0Ainteractive%20video%20synthesis.%20Project%20webpage%20available%20at%0Ahttps%3A//liyaowei-stu.github.io/project/ImageConductor/%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15339v1&entry.124074799=Read"},
{"title": "Voxel-Based Point Cloud Localization for Smart Spaces Management", "author": "F. S. Mortazavi and O. Shkedova and U. Feuerhake and C. Brenner and M. Sester", "abstract": "  This paper proposes a voxel-based approach for creating a digital twin of an\nurban environment that is capable of efficiently managing smart spaces. The\npaper explains the registration and localization procedure of the point cloud\ndataset, which uses the KISS ICP for scan point cloud combination and the\nRANSAC method for the initial alignment of the combined point cloud. The mobile\nmapping point cloud using Riegl VMX-250 serves as the reference map, and\nVelodyne scans are used for localization purposes. The point-to-plane iterative\nclosest-point method is then employed to refine the alignment. The paper\nevaluates the efficacy of the proposed method by calculating the errors between\nthe estimated and ground truth positions. The results indicate that the\nvoxel-based approach is capable of accurately estimating the position of the\nsensor platform, which are applicable for various use cases. A specific use\ncase in the context is smart parking space management, which is described and\ninitial visualization results are shown.\n", "link": "http://arxiv.org/abs/2406.15110v1", "date": "2024-06-21", "relevancy": 2.7583, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5735}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5413}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Voxel-Based%20Point%20Cloud%20Localization%20for%20Smart%20Spaces%20Management&body=Title%3A%20Voxel-Based%20Point%20Cloud%20Localization%20for%20Smart%20Spaces%20Management%0AAuthor%3A%20F.%20S.%20Mortazavi%20and%20O.%20Shkedova%20and%20U.%20Feuerhake%20and%20C.%20Brenner%20and%20M.%20Sester%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20voxel-based%20approach%20for%20creating%20a%20digital%20twin%20of%20an%0Aurban%20environment%20that%20is%20capable%20of%20efficiently%20managing%20smart%20spaces.%20The%0Apaper%20explains%20the%20registration%20and%20localization%20procedure%20of%20the%20point%20cloud%0Adataset%2C%20which%20uses%20the%20KISS%20ICP%20for%20scan%20point%20cloud%20combination%20and%20the%0ARANSAC%20method%20for%20the%20initial%20alignment%20of%20the%20combined%20point%20cloud.%20The%20mobile%0Amapping%20point%20cloud%20using%20Riegl%20VMX-250%20serves%20as%20the%20reference%20map%2C%20and%0AVelodyne%20scans%20are%20used%20for%20localization%20purposes.%20The%20point-to-plane%20iterative%0Aclosest-point%20method%20is%20then%20employed%20to%20refine%20the%20alignment.%20The%20paper%0Aevaluates%20the%20efficacy%20of%20the%20proposed%20method%20by%20calculating%20the%20errors%20between%0Athe%20estimated%20and%20ground%20truth%20positions.%20The%20results%20indicate%20that%20the%0Avoxel-based%20approach%20is%20capable%20of%20accurately%20estimating%20the%20position%20of%20the%0Asensor%20platform%2C%20which%20are%20applicable%20for%20various%20use%20cases.%20A%20specific%20use%0Acase%20in%20the%20context%20is%20smart%20parking%20space%20management%2C%20which%20is%20described%20and%0Ainitial%20visualization%20results%20are%20shown.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15110v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVoxel-Based%2520Point%2520Cloud%2520Localization%2520for%2520Smart%2520Spaces%2520Management%26entry.906535625%3DF.%2520S.%2520Mortazavi%2520and%2520O.%2520Shkedova%2520and%2520U.%2520Feuerhake%2520and%2520C.%2520Brenner%2520and%2520M.%2520Sester%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520voxel-based%2520approach%2520for%2520creating%2520a%2520digital%2520twin%2520of%2520an%250Aurban%2520environment%2520that%2520is%2520capable%2520of%2520efficiently%2520managing%2520smart%2520spaces.%2520The%250Apaper%2520explains%2520the%2520registration%2520and%2520localization%2520procedure%2520of%2520the%2520point%2520cloud%250Adataset%252C%2520which%2520uses%2520the%2520KISS%2520ICP%2520for%2520scan%2520point%2520cloud%2520combination%2520and%2520the%250ARANSAC%2520method%2520for%2520the%2520initial%2520alignment%2520of%2520the%2520combined%2520point%2520cloud.%2520The%2520mobile%250Amapping%2520point%2520cloud%2520using%2520Riegl%2520VMX-250%2520serves%2520as%2520the%2520reference%2520map%252C%2520and%250AVelodyne%2520scans%2520are%2520used%2520for%2520localization%2520purposes.%2520The%2520point-to-plane%2520iterative%250Aclosest-point%2520method%2520is%2520then%2520employed%2520to%2520refine%2520the%2520alignment.%2520The%2520paper%250Aevaluates%2520the%2520efficacy%2520of%2520the%2520proposed%2520method%2520by%2520calculating%2520the%2520errors%2520between%250Athe%2520estimated%2520and%2520ground%2520truth%2520positions.%2520The%2520results%2520indicate%2520that%2520the%250Avoxel-based%2520approach%2520is%2520capable%2520of%2520accurately%2520estimating%2520the%2520position%2520of%2520the%250Asensor%2520platform%252C%2520which%2520are%2520applicable%2520for%2520various%2520use%2520cases.%2520A%2520specific%2520use%250Acase%2520in%2520the%2520context%2520is%2520smart%2520parking%2520space%2520management%252C%2520which%2520is%2520described%2520and%250Ainitial%2520visualization%2520results%2520are%2520shown.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15110v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Voxel-Based%20Point%20Cloud%20Localization%20for%20Smart%20Spaces%20Management&entry.906535625=F.%20S.%20Mortazavi%20and%20O.%20Shkedova%20and%20U.%20Feuerhake%20and%20C.%20Brenner%20and%20M.%20Sester&entry.1292438233=%20%20This%20paper%20proposes%20a%20voxel-based%20approach%20for%20creating%20a%20digital%20twin%20of%20an%0Aurban%20environment%20that%20is%20capable%20of%20efficiently%20managing%20smart%20spaces.%20The%0Apaper%20explains%20the%20registration%20and%20localization%20procedure%20of%20the%20point%20cloud%0Adataset%2C%20which%20uses%20the%20KISS%20ICP%20for%20scan%20point%20cloud%20combination%20and%20the%0ARANSAC%20method%20for%20the%20initial%20alignment%20of%20the%20combined%20point%20cloud.%20The%20mobile%0Amapping%20point%20cloud%20using%20Riegl%20VMX-250%20serves%20as%20the%20reference%20map%2C%20and%0AVelodyne%20scans%20are%20used%20for%20localization%20purposes.%20The%20point-to-plane%20iterative%0Aclosest-point%20method%20is%20then%20employed%20to%20refine%20the%20alignment.%20The%20paper%0Aevaluates%20the%20efficacy%20of%20the%20proposed%20method%20by%20calculating%20the%20errors%20between%0Athe%20estimated%20and%20ground%20truth%20positions.%20The%20results%20indicate%20that%20the%0Avoxel-based%20approach%20is%20capable%20of%20accurately%20estimating%20the%20position%20of%20the%0Asensor%20platform%2C%20which%20are%20applicable%20for%20various%20use%20cases.%20A%20specific%20use%0Acase%20in%20the%20context%20is%20smart%20parking%20space%20management%2C%20which%20is%20described%20and%0Ainitial%20visualization%20results%20are%20shown.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15110v1&entry.124074799=Read"},
{"title": "Improving Interpretability and Robustness for the Detection of\n  AI-Generated Images", "author": "Tatiana Gaintseva and Laida Kushnareva and German Magai and Irina Piontkovskaya and Sergey Nikolenko and Martin Benning and Serguei Barannikov and Gregory Slabaugh", "abstract": "  With growing abilities of generative models, artificial content detection\nbecomes an increasingly important and difficult task. However, all popular\napproaches to this problem suffer from poor generalization across domains and\ngenerative models. In this work, we focus on the robustness of AI-generated\nimage (AIGI) detectors. We analyze existing state-of-the-art AIGI detection\nmethods based on frozen CLIP embeddings and show how to interpret them,\nshedding light on how images produced by various AI generators differ from real\nones. Next we propose two ways to improve robustness: based on removing harmful\ncomponents of the embedding vector and based on selecting the best performing\nattention heads in the image encoder model. Our methods increase the mean\nout-of-distribution (OOD) classification score by up to 6% for cross-model\ntransfer. We also propose a new dataset for AIGI detection and use it in our\nevaluation; we believe this dataset will help boost further research. The\ndataset and code are provided as a supplement.\n", "link": "http://arxiv.org/abs/2406.15035v1", "date": "2024-06-21", "relevancy": 2.7574, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5616}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5537}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5392}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Interpretability%20and%20Robustness%20for%20the%20Detection%20of%0A%20%20AI-Generated%20Images&body=Title%3A%20Improving%20Interpretability%20and%20Robustness%20for%20the%20Detection%20of%0A%20%20AI-Generated%20Images%0AAuthor%3A%20Tatiana%20Gaintseva%20and%20Laida%20Kushnareva%20and%20German%20Magai%20and%20Irina%20Piontkovskaya%20and%20Sergey%20Nikolenko%20and%20Martin%20Benning%20and%20Serguei%20Barannikov%20and%20Gregory%20Slabaugh%0AAbstract%3A%20%20%20With%20growing%20abilities%20of%20generative%20models%2C%20artificial%20content%20detection%0Abecomes%20an%20increasingly%20important%20and%20difficult%20task.%20However%2C%20all%20popular%0Aapproaches%20to%20this%20problem%20suffer%20from%20poor%20generalization%20across%20domains%20and%0Agenerative%20models.%20In%20this%20work%2C%20we%20focus%20on%20the%20robustness%20of%20AI-generated%0Aimage%20%28AIGI%29%20detectors.%20We%20analyze%20existing%20state-of-the-art%20AIGI%20detection%0Amethods%20based%20on%20frozen%20CLIP%20embeddings%20and%20show%20how%20to%20interpret%20them%2C%0Ashedding%20light%20on%20how%20images%20produced%20by%20various%20AI%20generators%20differ%20from%20real%0Aones.%20Next%20we%20propose%20two%20ways%20to%20improve%20robustness%3A%20based%20on%20removing%20harmful%0Acomponents%20of%20the%20embedding%20vector%20and%20based%20on%20selecting%20the%20best%20performing%0Aattention%20heads%20in%20the%20image%20encoder%20model.%20Our%20methods%20increase%20the%20mean%0Aout-of-distribution%20%28OOD%29%20classification%20score%20by%20up%20to%206%25%20for%20cross-model%0Atransfer.%20We%20also%20propose%20a%20new%20dataset%20for%20AIGI%20detection%20and%20use%20it%20in%20our%0Aevaluation%3B%20we%20believe%20this%20dataset%20will%20help%20boost%20further%20research.%20The%0Adataset%20and%20code%20are%20provided%20as%20a%20supplement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15035v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Interpretability%2520and%2520Robustness%2520for%2520the%2520Detection%2520of%250A%2520%2520AI-Generated%2520Images%26entry.906535625%3DTatiana%2520Gaintseva%2520and%2520Laida%2520Kushnareva%2520and%2520German%2520Magai%2520and%2520Irina%2520Piontkovskaya%2520and%2520Sergey%2520Nikolenko%2520and%2520Martin%2520Benning%2520and%2520Serguei%2520Barannikov%2520and%2520Gregory%2520Slabaugh%26entry.1292438233%3D%2520%2520With%2520growing%2520abilities%2520of%2520generative%2520models%252C%2520artificial%2520content%2520detection%250Abecomes%2520an%2520increasingly%2520important%2520and%2520difficult%2520task.%2520However%252C%2520all%2520popular%250Aapproaches%2520to%2520this%2520problem%2520suffer%2520from%2520poor%2520generalization%2520across%2520domains%2520and%250Agenerative%2520models.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520the%2520robustness%2520of%2520AI-generated%250Aimage%2520%2528AIGI%2529%2520detectors.%2520We%2520analyze%2520existing%2520state-of-the-art%2520AIGI%2520detection%250Amethods%2520based%2520on%2520frozen%2520CLIP%2520embeddings%2520and%2520show%2520how%2520to%2520interpret%2520them%252C%250Ashedding%2520light%2520on%2520how%2520images%2520produced%2520by%2520various%2520AI%2520generators%2520differ%2520from%2520real%250Aones.%2520Next%2520we%2520propose%2520two%2520ways%2520to%2520improve%2520robustness%253A%2520based%2520on%2520removing%2520harmful%250Acomponents%2520of%2520the%2520embedding%2520vector%2520and%2520based%2520on%2520selecting%2520the%2520best%2520performing%250Aattention%2520heads%2520in%2520the%2520image%2520encoder%2520model.%2520Our%2520methods%2520increase%2520the%2520mean%250Aout-of-distribution%2520%2528OOD%2529%2520classification%2520score%2520by%2520up%2520to%25206%2525%2520for%2520cross-model%250Atransfer.%2520We%2520also%2520propose%2520a%2520new%2520dataset%2520for%2520AIGI%2520detection%2520and%2520use%2520it%2520in%2520our%250Aevaluation%253B%2520we%2520believe%2520this%2520dataset%2520will%2520help%2520boost%2520further%2520research.%2520The%250Adataset%2520and%2520code%2520are%2520provided%2520as%2520a%2520supplement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15035v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Interpretability%20and%20Robustness%20for%20the%20Detection%20of%0A%20%20AI-Generated%20Images&entry.906535625=Tatiana%20Gaintseva%20and%20Laida%20Kushnareva%20and%20German%20Magai%20and%20Irina%20Piontkovskaya%20and%20Sergey%20Nikolenko%20and%20Martin%20Benning%20and%20Serguei%20Barannikov%20and%20Gregory%20Slabaugh&entry.1292438233=%20%20With%20growing%20abilities%20of%20generative%20models%2C%20artificial%20content%20detection%0Abecomes%20an%20increasingly%20important%20and%20difficult%20task.%20However%2C%20all%20popular%0Aapproaches%20to%20this%20problem%20suffer%20from%20poor%20generalization%20across%20domains%20and%0Agenerative%20models.%20In%20this%20work%2C%20we%20focus%20on%20the%20robustness%20of%20AI-generated%0Aimage%20%28AIGI%29%20detectors.%20We%20analyze%20existing%20state-of-the-art%20AIGI%20detection%0Amethods%20based%20on%20frozen%20CLIP%20embeddings%20and%20show%20how%20to%20interpret%20them%2C%0Ashedding%20light%20on%20how%20images%20produced%20by%20various%20AI%20generators%20differ%20from%20real%0Aones.%20Next%20we%20propose%20two%20ways%20to%20improve%20robustness%3A%20based%20on%20removing%20harmful%0Acomponents%20of%20the%20embedding%20vector%20and%20based%20on%20selecting%20the%20best%20performing%0Aattention%20heads%20in%20the%20image%20encoder%20model.%20Our%20methods%20increase%20the%20mean%0Aout-of-distribution%20%28OOD%29%20classification%20score%20by%20up%20to%206%25%20for%20cross-model%0Atransfer.%20We%20also%20propose%20a%20new%20dataset%20for%20AIGI%20detection%20and%20use%20it%20in%20our%0Aevaluation%3B%20we%20believe%20this%20dataset%20will%20help%20boost%20further%20research.%20The%0Adataset%20and%20code%20are%20provided%20as%20a%20supplement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15035v1&entry.124074799=Read"},
{"title": "Advancing Fine-Grained Classification by Structure and Subject\n  Preserving Augmentation", "author": "Eyal Michaeli and Ohad Fried", "abstract": "  Fine-grained visual classification (FGVC) involves classifying closely\nrelated sub-classes. This task is difficult due to the subtle differences\nbetween classes and the high intra-class variance. Moreover, FGVC datasets are\ntypically small and challenging to gather, thus highlighting a significant need\nfor effective data augmentation. Recent advancements in text-to-image diffusion\nmodels offer new possibilities for augmenting classification datasets. While\nthese models have been used to generate training data for classification tasks,\ntheir effectiveness in full-dataset training of FGVC models remains\nunder-explored. Recent techniques that rely on Text2Image generation or Img2Img\nmethods, often struggle to generate images that accurately represent the class\nwhile modifying them to a degree that significantly increases the dataset's\ndiversity. To address these challenges, we present SaSPA: Structure and Subject\nPreserving Augmentation. Contrary to recent methods, our method does not use\nreal images as guidance, thereby increasing generation flexibility and\npromoting greater diversity. To ensure accurate class representation, we employ\nconditioning mechanisms, specifically by conditioning on image edges and\nsubject representation. We conduct extensive experiments and benchmark SaSPA\nagainst both traditional and recent generative data augmentation methods. SaSPA\nconsistently outperforms all established baselines across multiple settings,\nincluding full dataset training, contextual bias, and few-shot classification.\nAdditionally, our results reveal interesting patterns in using synthetic data\nfor FGVC models; for instance, we find a relationship between the amount of\nreal data used and the optimal proportion of synthetic data. Code is available\nat https://github.com/EyalMichaeli/SaSPA-Aug.\n", "link": "http://arxiv.org/abs/2406.14551v2", "date": "2024-06-21", "relevancy": 2.7026, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5539}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5343}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5334}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Fine-Grained%20Classification%20by%20Structure%20and%20Subject%0A%20%20Preserving%20Augmentation&body=Title%3A%20Advancing%20Fine-Grained%20Classification%20by%20Structure%20and%20Subject%0A%20%20Preserving%20Augmentation%0AAuthor%3A%20Eyal%20Michaeli%20and%20Ohad%20Fried%0AAbstract%3A%20%20%20Fine-grained%20visual%20classification%20%28FGVC%29%20involves%20classifying%20closely%0Arelated%20sub-classes.%20This%20task%20is%20difficult%20due%20to%20the%20subtle%20differences%0Abetween%20classes%20and%20the%20high%20intra-class%20variance.%20Moreover%2C%20FGVC%20datasets%20are%0Atypically%20small%20and%20challenging%20to%20gather%2C%20thus%20highlighting%20a%20significant%20need%0Afor%20effective%20data%20augmentation.%20Recent%20advancements%20in%20text-to-image%20diffusion%0Amodels%20offer%20new%20possibilities%20for%20augmenting%20classification%20datasets.%20While%0Athese%20models%20have%20been%20used%20to%20generate%20training%20data%20for%20classification%20tasks%2C%0Atheir%20effectiveness%20in%20full-dataset%20training%20of%20FGVC%20models%20remains%0Aunder-explored.%20Recent%20techniques%20that%20rely%20on%20Text2Image%20generation%20or%20Img2Img%0Amethods%2C%20often%20struggle%20to%20generate%20images%20that%20accurately%20represent%20the%20class%0Awhile%20modifying%20them%20to%20a%20degree%20that%20significantly%20increases%20the%20dataset%27s%0Adiversity.%20To%20address%20these%20challenges%2C%20we%20present%20SaSPA%3A%20Structure%20and%20Subject%0APreserving%20Augmentation.%20Contrary%20to%20recent%20methods%2C%20our%20method%20does%20not%20use%0Areal%20images%20as%20guidance%2C%20thereby%20increasing%20generation%20flexibility%20and%0Apromoting%20greater%20diversity.%20To%20ensure%20accurate%20class%20representation%2C%20we%20employ%0Aconditioning%20mechanisms%2C%20specifically%20by%20conditioning%20on%20image%20edges%20and%0Asubject%20representation.%20We%20conduct%20extensive%20experiments%20and%20benchmark%20SaSPA%0Aagainst%20both%20traditional%20and%20recent%20generative%20data%20augmentation%20methods.%20SaSPA%0Aconsistently%20outperforms%20all%20established%20baselines%20across%20multiple%20settings%2C%0Aincluding%20full%20dataset%20training%2C%20contextual%20bias%2C%20and%20few-shot%20classification.%0AAdditionally%2C%20our%20results%20reveal%20interesting%20patterns%20in%20using%20synthetic%20data%0Afor%20FGVC%20models%3B%20for%20instance%2C%20we%20find%20a%20relationship%20between%20the%20amount%20of%0Areal%20data%20used%20and%20the%20optimal%20proportion%20of%20synthetic%20data.%20Code%20is%20available%0Aat%20https%3A//github.com/EyalMichaeli/SaSPA-Aug.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14551v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Fine-Grained%2520Classification%2520by%2520Structure%2520and%2520Subject%250A%2520%2520Preserving%2520Augmentation%26entry.906535625%3DEyal%2520Michaeli%2520and%2520Ohad%2520Fried%26entry.1292438233%3D%2520%2520Fine-grained%2520visual%2520classification%2520%2528FGVC%2529%2520involves%2520classifying%2520closely%250Arelated%2520sub-classes.%2520This%2520task%2520is%2520difficult%2520due%2520to%2520the%2520subtle%2520differences%250Abetween%2520classes%2520and%2520the%2520high%2520intra-class%2520variance.%2520Moreover%252C%2520FGVC%2520datasets%2520are%250Atypically%2520small%2520and%2520challenging%2520to%2520gather%252C%2520thus%2520highlighting%2520a%2520significant%2520need%250Afor%2520effective%2520data%2520augmentation.%2520Recent%2520advancements%2520in%2520text-to-image%2520diffusion%250Amodels%2520offer%2520new%2520possibilities%2520for%2520augmenting%2520classification%2520datasets.%2520While%250Athese%2520models%2520have%2520been%2520used%2520to%2520generate%2520training%2520data%2520for%2520classification%2520tasks%252C%250Atheir%2520effectiveness%2520in%2520full-dataset%2520training%2520of%2520FGVC%2520models%2520remains%250Aunder-explored.%2520Recent%2520techniques%2520that%2520rely%2520on%2520Text2Image%2520generation%2520or%2520Img2Img%250Amethods%252C%2520often%2520struggle%2520to%2520generate%2520images%2520that%2520accurately%2520represent%2520the%2520class%250Awhile%2520modifying%2520them%2520to%2520a%2520degree%2520that%2520significantly%2520increases%2520the%2520dataset%2527s%250Adiversity.%2520To%2520address%2520these%2520challenges%252C%2520we%2520present%2520SaSPA%253A%2520Structure%2520and%2520Subject%250APreserving%2520Augmentation.%2520Contrary%2520to%2520recent%2520methods%252C%2520our%2520method%2520does%2520not%2520use%250Areal%2520images%2520as%2520guidance%252C%2520thereby%2520increasing%2520generation%2520flexibility%2520and%250Apromoting%2520greater%2520diversity.%2520To%2520ensure%2520accurate%2520class%2520representation%252C%2520we%2520employ%250Aconditioning%2520mechanisms%252C%2520specifically%2520by%2520conditioning%2520on%2520image%2520edges%2520and%250Asubject%2520representation.%2520We%2520conduct%2520extensive%2520experiments%2520and%2520benchmark%2520SaSPA%250Aagainst%2520both%2520traditional%2520and%2520recent%2520generative%2520data%2520augmentation%2520methods.%2520SaSPA%250Aconsistently%2520outperforms%2520all%2520established%2520baselines%2520across%2520multiple%2520settings%252C%250Aincluding%2520full%2520dataset%2520training%252C%2520contextual%2520bias%252C%2520and%2520few-shot%2520classification.%250AAdditionally%252C%2520our%2520results%2520reveal%2520interesting%2520patterns%2520in%2520using%2520synthetic%2520data%250Afor%2520FGVC%2520models%253B%2520for%2520instance%252C%2520we%2520find%2520a%2520relationship%2520between%2520the%2520amount%2520of%250Areal%2520data%2520used%2520and%2520the%2520optimal%2520proportion%2520of%2520synthetic%2520data.%2520Code%2520is%2520available%250Aat%2520https%253A//github.com/EyalMichaeli/SaSPA-Aug.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14551v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Fine-Grained%20Classification%20by%20Structure%20and%20Subject%0A%20%20Preserving%20Augmentation&entry.906535625=Eyal%20Michaeli%20and%20Ohad%20Fried&entry.1292438233=%20%20Fine-grained%20visual%20classification%20%28FGVC%29%20involves%20classifying%20closely%0Arelated%20sub-classes.%20This%20task%20is%20difficult%20due%20to%20the%20subtle%20differences%0Abetween%20classes%20and%20the%20high%20intra-class%20variance.%20Moreover%2C%20FGVC%20datasets%20are%0Atypically%20small%20and%20challenging%20to%20gather%2C%20thus%20highlighting%20a%20significant%20need%0Afor%20effective%20data%20augmentation.%20Recent%20advancements%20in%20text-to-image%20diffusion%0Amodels%20offer%20new%20possibilities%20for%20augmenting%20classification%20datasets.%20While%0Athese%20models%20have%20been%20used%20to%20generate%20training%20data%20for%20classification%20tasks%2C%0Atheir%20effectiveness%20in%20full-dataset%20training%20of%20FGVC%20models%20remains%0Aunder-explored.%20Recent%20techniques%20that%20rely%20on%20Text2Image%20generation%20or%20Img2Img%0Amethods%2C%20often%20struggle%20to%20generate%20images%20that%20accurately%20represent%20the%20class%0Awhile%20modifying%20them%20to%20a%20degree%20that%20significantly%20increases%20the%20dataset%27s%0Adiversity.%20To%20address%20these%20challenges%2C%20we%20present%20SaSPA%3A%20Structure%20and%20Subject%0APreserving%20Augmentation.%20Contrary%20to%20recent%20methods%2C%20our%20method%20does%20not%20use%0Areal%20images%20as%20guidance%2C%20thereby%20increasing%20generation%20flexibility%20and%0Apromoting%20greater%20diversity.%20To%20ensure%20accurate%20class%20representation%2C%20we%20employ%0Aconditioning%20mechanisms%2C%20specifically%20by%20conditioning%20on%20image%20edges%20and%0Asubject%20representation.%20We%20conduct%20extensive%20experiments%20and%20benchmark%20SaSPA%0Aagainst%20both%20traditional%20and%20recent%20generative%20data%20augmentation%20methods.%20SaSPA%0Aconsistently%20outperforms%20all%20established%20baselines%20across%20multiple%20settings%2C%0Aincluding%20full%20dataset%20training%2C%20contextual%20bias%2C%20and%20few-shot%20classification.%0AAdditionally%2C%20our%20results%20reveal%20interesting%20patterns%20in%20using%20synthetic%20data%0Afor%20FGVC%20models%3B%20for%20instance%2C%20we%20find%20a%20relationship%20between%20the%20amount%20of%0Areal%20data%20used%20and%20the%20optimal%20proportion%20of%20synthetic%20data.%20Code%20is%20available%0Aat%20https%3A//github.com/EyalMichaeli/SaSPA-Aug.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14551v2&entry.124074799=Read"},
{"title": "High Resolution Surface Reconstruction of Cultural Heritage Objects\n  Using Shape from Polarization Method", "author": "F. S. Mortazavi and M. Saadatseresht", "abstract": "  Nowadays, three-dimensional reconstruction is used in various fields like\ncomputer vision, computer graphics, mixed reality and digital twin. The\nthree-dimensional reconstruction of cultural heritage objects is one of the\nmost important applications in this area which is usually accomplished by close\nrange photogrammetry. The problem here is that the images are often noisy, and\nthe dense image matching method has significant limitations to reconstruct the\ngeometric details of cultural heritage objects in practice. Therefore,\ndisplaying high-level details in three-dimensional models, especially for\ncultural heritage objects, is a severe challenge in this field. In this paper,\nthe shape from polarization method has been investigated, a passive method with\nno drawbacks of active methods. In this method, the resolution of the depth\nmaps can be dramatically increased using the information obtained from the\npolarization light by rotating a linear polarizing filter in front of a digital\ncamera. Through these polarized images, the surface details of the object can\nbe reconstructed locally with high accuracy. The fusion of polarization and\nphotogrammetric methods is an appropriate solution for achieving high\nresolution three-dimensional reconstruction. The surface reconstruction\nassessments have been performed visually and quantitatively. The evaluations\nshowed that the proposed method could significantly reconstruct the surfaces'\ndetails in the three-dimensional model compared to the photogrammetric method\nwith 10 times higher depth resolution.\n", "link": "http://arxiv.org/abs/2406.15121v1", "date": "2024-06-21", "relevancy": 2.6015, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5258}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5258}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High%20Resolution%20Surface%20Reconstruction%20of%20Cultural%20Heritage%20Objects%0A%20%20Using%20Shape%20from%20Polarization%20Method&body=Title%3A%20High%20Resolution%20Surface%20Reconstruction%20of%20Cultural%20Heritage%20Objects%0A%20%20Using%20Shape%20from%20Polarization%20Method%0AAuthor%3A%20F.%20S.%20Mortazavi%20and%20M.%20Saadatseresht%0AAbstract%3A%20%20%20Nowadays%2C%20three-dimensional%20reconstruction%20is%20used%20in%20various%20fields%20like%0Acomputer%20vision%2C%20computer%20graphics%2C%20mixed%20reality%20and%20digital%20twin.%20The%0Athree-dimensional%20reconstruction%20of%20cultural%20heritage%20objects%20is%20one%20of%20the%0Amost%20important%20applications%20in%20this%20area%20which%20is%20usually%20accomplished%20by%20close%0Arange%20photogrammetry.%20The%20problem%20here%20is%20that%20the%20images%20are%20often%20noisy%2C%20and%0Athe%20dense%20image%20matching%20method%20has%20significant%20limitations%20to%20reconstruct%20the%0Ageometric%20details%20of%20cultural%20heritage%20objects%20in%20practice.%20Therefore%2C%0Adisplaying%20high-level%20details%20in%20three-dimensional%20models%2C%20especially%20for%0Acultural%20heritage%20objects%2C%20is%20a%20severe%20challenge%20in%20this%20field.%20In%20this%20paper%2C%0Athe%20shape%20from%20polarization%20method%20has%20been%20investigated%2C%20a%20passive%20method%20with%0Ano%20drawbacks%20of%20active%20methods.%20In%20this%20method%2C%20the%20resolution%20of%20the%20depth%0Amaps%20can%20be%20dramatically%20increased%20using%20the%20information%20obtained%20from%20the%0Apolarization%20light%20by%20rotating%20a%20linear%20polarizing%20filter%20in%20front%20of%20a%20digital%0Acamera.%20Through%20these%20polarized%20images%2C%20the%20surface%20details%20of%20the%20object%20can%0Abe%20reconstructed%20locally%20with%20high%20accuracy.%20The%20fusion%20of%20polarization%20and%0Aphotogrammetric%20methods%20is%20an%20appropriate%20solution%20for%20achieving%20high%0Aresolution%20three-dimensional%20reconstruction.%20The%20surface%20reconstruction%0Aassessments%20have%20been%20performed%20visually%20and%20quantitatively.%20The%20evaluations%0Ashowed%20that%20the%20proposed%20method%20could%20significantly%20reconstruct%20the%20surfaces%27%0Adetails%20in%20the%20three-dimensional%20model%20compared%20to%20the%20photogrammetric%20method%0Awith%2010%20times%20higher%20depth%20resolution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15121v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh%2520Resolution%2520Surface%2520Reconstruction%2520of%2520Cultural%2520Heritage%2520Objects%250A%2520%2520Using%2520Shape%2520from%2520Polarization%2520Method%26entry.906535625%3DF.%2520S.%2520Mortazavi%2520and%2520M.%2520Saadatseresht%26entry.1292438233%3D%2520%2520Nowadays%252C%2520three-dimensional%2520reconstruction%2520is%2520used%2520in%2520various%2520fields%2520like%250Acomputer%2520vision%252C%2520computer%2520graphics%252C%2520mixed%2520reality%2520and%2520digital%2520twin.%2520The%250Athree-dimensional%2520reconstruction%2520of%2520cultural%2520heritage%2520objects%2520is%2520one%2520of%2520the%250Amost%2520important%2520applications%2520in%2520this%2520area%2520which%2520is%2520usually%2520accomplished%2520by%2520close%250Arange%2520photogrammetry.%2520The%2520problem%2520here%2520is%2520that%2520the%2520images%2520are%2520often%2520noisy%252C%2520and%250Athe%2520dense%2520image%2520matching%2520method%2520has%2520significant%2520limitations%2520to%2520reconstruct%2520the%250Ageometric%2520details%2520of%2520cultural%2520heritage%2520objects%2520in%2520practice.%2520Therefore%252C%250Adisplaying%2520high-level%2520details%2520in%2520three-dimensional%2520models%252C%2520especially%2520for%250Acultural%2520heritage%2520objects%252C%2520is%2520a%2520severe%2520challenge%2520in%2520this%2520field.%2520In%2520this%2520paper%252C%250Athe%2520shape%2520from%2520polarization%2520method%2520has%2520been%2520investigated%252C%2520a%2520passive%2520method%2520with%250Ano%2520drawbacks%2520of%2520active%2520methods.%2520In%2520this%2520method%252C%2520the%2520resolution%2520of%2520the%2520depth%250Amaps%2520can%2520be%2520dramatically%2520increased%2520using%2520the%2520information%2520obtained%2520from%2520the%250Apolarization%2520light%2520by%2520rotating%2520a%2520linear%2520polarizing%2520filter%2520in%2520front%2520of%2520a%2520digital%250Acamera.%2520Through%2520these%2520polarized%2520images%252C%2520the%2520surface%2520details%2520of%2520the%2520object%2520can%250Abe%2520reconstructed%2520locally%2520with%2520high%2520accuracy.%2520The%2520fusion%2520of%2520polarization%2520and%250Aphotogrammetric%2520methods%2520is%2520an%2520appropriate%2520solution%2520for%2520achieving%2520high%250Aresolution%2520three-dimensional%2520reconstruction.%2520The%2520surface%2520reconstruction%250Aassessments%2520have%2520been%2520performed%2520visually%2520and%2520quantitatively.%2520The%2520evaluations%250Ashowed%2520that%2520the%2520proposed%2520method%2520could%2520significantly%2520reconstruct%2520the%2520surfaces%2527%250Adetails%2520in%2520the%2520three-dimensional%2520model%2520compared%2520to%2520the%2520photogrammetric%2520method%250Awith%252010%2520times%2520higher%2520depth%2520resolution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15121v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High%20Resolution%20Surface%20Reconstruction%20of%20Cultural%20Heritage%20Objects%0A%20%20Using%20Shape%20from%20Polarization%20Method&entry.906535625=F.%20S.%20Mortazavi%20and%20M.%20Saadatseresht&entry.1292438233=%20%20Nowadays%2C%20three-dimensional%20reconstruction%20is%20used%20in%20various%20fields%20like%0Acomputer%20vision%2C%20computer%20graphics%2C%20mixed%20reality%20and%20digital%20twin.%20The%0Athree-dimensional%20reconstruction%20of%20cultural%20heritage%20objects%20is%20one%20of%20the%0Amost%20important%20applications%20in%20this%20area%20which%20is%20usually%20accomplished%20by%20close%0Arange%20photogrammetry.%20The%20problem%20here%20is%20that%20the%20images%20are%20often%20noisy%2C%20and%0Athe%20dense%20image%20matching%20method%20has%20significant%20limitations%20to%20reconstruct%20the%0Ageometric%20details%20of%20cultural%20heritage%20objects%20in%20practice.%20Therefore%2C%0Adisplaying%20high-level%20details%20in%20three-dimensional%20models%2C%20especially%20for%0Acultural%20heritage%20objects%2C%20is%20a%20severe%20challenge%20in%20this%20field.%20In%20this%20paper%2C%0Athe%20shape%20from%20polarization%20method%20has%20been%20investigated%2C%20a%20passive%20method%20with%0Ano%20drawbacks%20of%20active%20methods.%20In%20this%20method%2C%20the%20resolution%20of%20the%20depth%0Amaps%20can%20be%20dramatically%20increased%20using%20the%20information%20obtained%20from%20the%0Apolarization%20light%20by%20rotating%20a%20linear%20polarizing%20filter%20in%20front%20of%20a%20digital%0Acamera.%20Through%20these%20polarized%20images%2C%20the%20surface%20details%20of%20the%20object%20can%0Abe%20reconstructed%20locally%20with%20high%20accuracy.%20The%20fusion%20of%20polarization%20and%0Aphotogrammetric%20methods%20is%20an%20appropriate%20solution%20for%20achieving%20high%0Aresolution%20three-dimensional%20reconstruction.%20The%20surface%20reconstruction%0Aassessments%20have%20been%20performed%20visually%20and%20quantitatively.%20The%20evaluations%0Ashowed%20that%20the%20proposed%20method%20could%20significantly%20reconstruct%20the%20surfaces%27%0Adetails%20in%20the%20three-dimensional%20model%20compared%20to%20the%20photogrammetric%20method%0Awith%2010%20times%20higher%20depth%20resolution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15121v1&entry.124074799=Read"},
{"title": "An End-to-End, Segmentation-Free, Arabic Handwritten Recognition Model\n  on KHATT", "author": "Sondos Aabed and Ahmad Khairaldin", "abstract": "  An end-to-end, segmentation-free, deep learning model trained from scratch is\nproposed, leveraging DCNN for feature extraction, alongside Bidirectional\nLong-Short Term Memory (BLSTM) for sequence recognition and Connectionist\nTemporal Classification (CTC) loss function on the KHATT database. The training\nphase yields remarkable results 84% recognition rate on the test dataset at the\ncharacter level and 71% on the word level, establishing an image-based sequence\nrecognition framework that operates without segmentation only at the line\nlevel. The analysis and preprocessing of the KFUPM Handwritten Arabic TexT\n(KHATT) database are also presented. Finally, advanced image processing\ntechniques, including filtering, transformation, and line segmentation are\nimplemented. The importance of this work is highlighted by its wide-ranging\napplications. Including digitizing, documentation, archiving, and text\ntranslation in fields such as banking. Moreover, AHR serves as a pivotal tool\nfor making images searchable, enhancing information retrieval capabilities, and\nenabling effortless editing. This functionality significantly reduces the time\nand effort required for tasks such as Arabic data organization and\nmanipulation.\n", "link": "http://arxiv.org/abs/2406.15329v1", "date": "2024-06-21", "relevancy": 2.5364, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5207}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5039}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20End-to-End%2C%20Segmentation-Free%2C%20Arabic%20Handwritten%20Recognition%20Model%0A%20%20on%20KHATT&body=Title%3A%20An%20End-to-End%2C%20Segmentation-Free%2C%20Arabic%20Handwritten%20Recognition%20Model%0A%20%20on%20KHATT%0AAuthor%3A%20Sondos%20Aabed%20and%20Ahmad%20Khairaldin%0AAbstract%3A%20%20%20An%20end-to-end%2C%20segmentation-free%2C%20deep%20learning%20model%20trained%20from%20scratch%20is%0Aproposed%2C%20leveraging%20DCNN%20for%20feature%20extraction%2C%20alongside%20Bidirectional%0ALong-Short%20Term%20Memory%20%28BLSTM%29%20for%20sequence%20recognition%20and%20Connectionist%0ATemporal%20Classification%20%28CTC%29%20loss%20function%20on%20the%20KHATT%20database.%20The%20training%0Aphase%20yields%20remarkable%20results%2084%25%20recognition%20rate%20on%20the%20test%20dataset%20at%20the%0Acharacter%20level%20and%2071%25%20on%20the%20word%20level%2C%20establishing%20an%20image-based%20sequence%0Arecognition%20framework%20that%20operates%20without%20segmentation%20only%20at%20the%20line%0Alevel.%20The%20analysis%20and%20preprocessing%20of%20the%20KFUPM%20Handwritten%20Arabic%20TexT%0A%28KHATT%29%20database%20are%20also%20presented.%20Finally%2C%20advanced%20image%20processing%0Atechniques%2C%20including%20filtering%2C%20transformation%2C%20and%20line%20segmentation%20are%0Aimplemented.%20The%20importance%20of%20this%20work%20is%20highlighted%20by%20its%20wide-ranging%0Aapplications.%20Including%20digitizing%2C%20documentation%2C%20archiving%2C%20and%20text%0Atranslation%20in%20fields%20such%20as%20banking.%20Moreover%2C%20AHR%20serves%20as%20a%20pivotal%20tool%0Afor%20making%20images%20searchable%2C%20enhancing%20information%20retrieval%20capabilities%2C%20and%0Aenabling%20effortless%20editing.%20This%20functionality%20significantly%20reduces%20the%20time%0Aand%20effort%20required%20for%20tasks%20such%20as%20Arabic%20data%20organization%20and%0Amanipulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15329v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520End-to-End%252C%2520Segmentation-Free%252C%2520Arabic%2520Handwritten%2520Recognition%2520Model%250A%2520%2520on%2520KHATT%26entry.906535625%3DSondos%2520Aabed%2520and%2520Ahmad%2520Khairaldin%26entry.1292438233%3D%2520%2520An%2520end-to-end%252C%2520segmentation-free%252C%2520deep%2520learning%2520model%2520trained%2520from%2520scratch%2520is%250Aproposed%252C%2520leveraging%2520DCNN%2520for%2520feature%2520extraction%252C%2520alongside%2520Bidirectional%250ALong-Short%2520Term%2520Memory%2520%2528BLSTM%2529%2520for%2520sequence%2520recognition%2520and%2520Connectionist%250ATemporal%2520Classification%2520%2528CTC%2529%2520loss%2520function%2520on%2520the%2520KHATT%2520database.%2520The%2520training%250Aphase%2520yields%2520remarkable%2520results%252084%2525%2520recognition%2520rate%2520on%2520the%2520test%2520dataset%2520at%2520the%250Acharacter%2520level%2520and%252071%2525%2520on%2520the%2520word%2520level%252C%2520establishing%2520an%2520image-based%2520sequence%250Arecognition%2520framework%2520that%2520operates%2520without%2520segmentation%2520only%2520at%2520the%2520line%250Alevel.%2520The%2520analysis%2520and%2520preprocessing%2520of%2520the%2520KFUPM%2520Handwritten%2520Arabic%2520TexT%250A%2528KHATT%2529%2520database%2520are%2520also%2520presented.%2520Finally%252C%2520advanced%2520image%2520processing%250Atechniques%252C%2520including%2520filtering%252C%2520transformation%252C%2520and%2520line%2520segmentation%2520are%250Aimplemented.%2520The%2520importance%2520of%2520this%2520work%2520is%2520highlighted%2520by%2520its%2520wide-ranging%250Aapplications.%2520Including%2520digitizing%252C%2520documentation%252C%2520archiving%252C%2520and%2520text%250Atranslation%2520in%2520fields%2520such%2520as%2520banking.%2520Moreover%252C%2520AHR%2520serves%2520as%2520a%2520pivotal%2520tool%250Afor%2520making%2520images%2520searchable%252C%2520enhancing%2520information%2520retrieval%2520capabilities%252C%2520and%250Aenabling%2520effortless%2520editing.%2520This%2520functionality%2520significantly%2520reduces%2520the%2520time%250Aand%2520effort%2520required%2520for%2520tasks%2520such%2520as%2520Arabic%2520data%2520organization%2520and%250Amanipulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15329v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20End-to-End%2C%20Segmentation-Free%2C%20Arabic%20Handwritten%20Recognition%20Model%0A%20%20on%20KHATT&entry.906535625=Sondos%20Aabed%20and%20Ahmad%20Khairaldin&entry.1292438233=%20%20An%20end-to-end%2C%20segmentation-free%2C%20deep%20learning%20model%20trained%20from%20scratch%20is%0Aproposed%2C%20leveraging%20DCNN%20for%20feature%20extraction%2C%20alongside%20Bidirectional%0ALong-Short%20Term%20Memory%20%28BLSTM%29%20for%20sequence%20recognition%20and%20Connectionist%0ATemporal%20Classification%20%28CTC%29%20loss%20function%20on%20the%20KHATT%20database.%20The%20training%0Aphase%20yields%20remarkable%20results%2084%25%20recognition%20rate%20on%20the%20test%20dataset%20at%20the%0Acharacter%20level%20and%2071%25%20on%20the%20word%20level%2C%20establishing%20an%20image-based%20sequence%0Arecognition%20framework%20that%20operates%20without%20segmentation%20only%20at%20the%20line%0Alevel.%20The%20analysis%20and%20preprocessing%20of%20the%20KFUPM%20Handwritten%20Arabic%20TexT%0A%28KHATT%29%20database%20are%20also%20presented.%20Finally%2C%20advanced%20image%20processing%0Atechniques%2C%20including%20filtering%2C%20transformation%2C%20and%20line%20segmentation%20are%0Aimplemented.%20The%20importance%20of%20this%20work%20is%20highlighted%20by%20its%20wide-ranging%0Aapplications.%20Including%20digitizing%2C%20documentation%2C%20archiving%2C%20and%20text%0Atranslation%20in%20fields%20such%20as%20banking.%20Moreover%2C%20AHR%20serves%20as%20a%20pivotal%20tool%0Afor%20making%20images%20searchable%2C%20enhancing%20information%20retrieval%20capabilities%2C%20and%0Aenabling%20effortless%20editing.%20This%20functionality%20significantly%20reduces%20the%20time%0Aand%20effort%20required%20for%20tasks%20such%20as%20Arabic%20data%20organization%20and%0Amanipulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15329v1&entry.124074799=Read"},
{"title": "Stochastic Optimisation Framework using the Core Imaging Library and\n  Synergistic Image Reconstruction Framework for PET Reconstruction", "author": "Evangelos Papoutsellis and Casper da Costa-Luis and Daniel Deidda and Claire Delplancke and Margaret Duff and Gemma Fardell and Ashley Gillman and Jakob S. J\u00f8rgensen and Zeljko Kereta and Evgueni Ovtchinnikov and Edoardo Pasca and Georg Schramm and Kris Thielemans", "abstract": "  We introduce a stochastic framework into the open--source Core Imaging\nLibrary (CIL) which enables easy development of stochastic algorithms. Five\nsuch algorithms from the literature are developed, Stochastic Gradient Descent,\nStochastic Average Gradient (-Am\\'elior\\'e), (Loopless) Stochastic Variance\nReduced Gradient. We showcase the functionality of the framework with a\ncomparative study against a deterministic algorithm on a simulated 2D PET\ndataset, with the use of the open-source Synergistic Image Reconstruction\nFramework. We observe that stochastic optimisation methods can converge in\nfewer passes of the data than a standard deterministic algorithm.\n", "link": "http://arxiv.org/abs/2406.15159v1", "date": "2024-06-21", "relevancy": 2.5237, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5059}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5042}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5041}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stochastic%20Optimisation%20Framework%20using%20the%20Core%20Imaging%20Library%20and%0A%20%20Synergistic%20Image%20Reconstruction%20Framework%20for%20PET%20Reconstruction&body=Title%3A%20Stochastic%20Optimisation%20Framework%20using%20the%20Core%20Imaging%20Library%20and%0A%20%20Synergistic%20Image%20Reconstruction%20Framework%20for%20PET%20Reconstruction%0AAuthor%3A%20Evangelos%20Papoutsellis%20and%20Casper%20da%20Costa-Luis%20and%20Daniel%20Deidda%20and%20Claire%20Delplancke%20and%20Margaret%20Duff%20and%20Gemma%20Fardell%20and%20Ashley%20Gillman%20and%20Jakob%20S.%20J%C3%B8rgensen%20and%20Zeljko%20Kereta%20and%20Evgueni%20Ovtchinnikov%20and%20Edoardo%20Pasca%20and%20Georg%20Schramm%20and%20Kris%20Thielemans%0AAbstract%3A%20%20%20We%20introduce%20a%20stochastic%20framework%20into%20the%20open--source%20Core%20Imaging%0ALibrary%20%28CIL%29%20which%20enables%20easy%20development%20of%20stochastic%20algorithms.%20Five%0Asuch%20algorithms%20from%20the%20literature%20are%20developed%2C%20Stochastic%20Gradient%20Descent%2C%0AStochastic%20Average%20Gradient%20%28-Am%5C%27elior%5C%27e%29%2C%20%28Loopless%29%20Stochastic%20Variance%0AReduced%20Gradient.%20We%20showcase%20the%20functionality%20of%20the%20framework%20with%20a%0Acomparative%20study%20against%20a%20deterministic%20algorithm%20on%20a%20simulated%202D%20PET%0Adataset%2C%20with%20the%20use%20of%20the%20open-source%20Synergistic%20Image%20Reconstruction%0AFramework.%20We%20observe%20that%20stochastic%20optimisation%20methods%20can%20converge%20in%0Afewer%20passes%20of%20the%20data%20than%20a%20standard%20deterministic%20algorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15159v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStochastic%2520Optimisation%2520Framework%2520using%2520the%2520Core%2520Imaging%2520Library%2520and%250A%2520%2520Synergistic%2520Image%2520Reconstruction%2520Framework%2520for%2520PET%2520Reconstruction%26entry.906535625%3DEvangelos%2520Papoutsellis%2520and%2520Casper%2520da%2520Costa-Luis%2520and%2520Daniel%2520Deidda%2520and%2520Claire%2520Delplancke%2520and%2520Margaret%2520Duff%2520and%2520Gemma%2520Fardell%2520and%2520Ashley%2520Gillman%2520and%2520Jakob%2520S.%2520J%25C3%25B8rgensen%2520and%2520Zeljko%2520Kereta%2520and%2520Evgueni%2520Ovtchinnikov%2520and%2520Edoardo%2520Pasca%2520and%2520Georg%2520Schramm%2520and%2520Kris%2520Thielemans%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520stochastic%2520framework%2520into%2520the%2520open--source%2520Core%2520Imaging%250ALibrary%2520%2528CIL%2529%2520which%2520enables%2520easy%2520development%2520of%2520stochastic%2520algorithms.%2520Five%250Asuch%2520algorithms%2520from%2520the%2520literature%2520are%2520developed%252C%2520Stochastic%2520Gradient%2520Descent%252C%250AStochastic%2520Average%2520Gradient%2520%2528-Am%255C%2527elior%255C%2527e%2529%252C%2520%2528Loopless%2529%2520Stochastic%2520Variance%250AReduced%2520Gradient.%2520We%2520showcase%2520the%2520functionality%2520of%2520the%2520framework%2520with%2520a%250Acomparative%2520study%2520against%2520a%2520deterministic%2520algorithm%2520on%2520a%2520simulated%25202D%2520PET%250Adataset%252C%2520with%2520the%2520use%2520of%2520the%2520open-source%2520Synergistic%2520Image%2520Reconstruction%250AFramework.%2520We%2520observe%2520that%2520stochastic%2520optimisation%2520methods%2520can%2520converge%2520in%250Afewer%2520passes%2520of%2520the%2520data%2520than%2520a%2520standard%2520deterministic%2520algorithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15159v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stochastic%20Optimisation%20Framework%20using%20the%20Core%20Imaging%20Library%20and%0A%20%20Synergistic%20Image%20Reconstruction%20Framework%20for%20PET%20Reconstruction&entry.906535625=Evangelos%20Papoutsellis%20and%20Casper%20da%20Costa-Luis%20and%20Daniel%20Deidda%20and%20Claire%20Delplancke%20and%20Margaret%20Duff%20and%20Gemma%20Fardell%20and%20Ashley%20Gillman%20and%20Jakob%20S.%20J%C3%B8rgensen%20and%20Zeljko%20Kereta%20and%20Evgueni%20Ovtchinnikov%20and%20Edoardo%20Pasca%20and%20Georg%20Schramm%20and%20Kris%20Thielemans&entry.1292438233=%20%20We%20introduce%20a%20stochastic%20framework%20into%20the%20open--source%20Core%20Imaging%0ALibrary%20%28CIL%29%20which%20enables%20easy%20development%20of%20stochastic%20algorithms.%20Five%0Asuch%20algorithms%20from%20the%20literature%20are%20developed%2C%20Stochastic%20Gradient%20Descent%2C%0AStochastic%20Average%20Gradient%20%28-Am%5C%27elior%5C%27e%29%2C%20%28Loopless%29%20Stochastic%20Variance%0AReduced%20Gradient.%20We%20showcase%20the%20functionality%20of%20the%20framework%20with%20a%0Acomparative%20study%20against%20a%20deterministic%20algorithm%20on%20a%20simulated%202D%20PET%0Adataset%2C%20with%20the%20use%20of%20the%20open-source%20Synergistic%20Image%20Reconstruction%0AFramework.%20We%20observe%20that%20stochastic%20optimisation%20methods%20can%20converge%20in%0Afewer%20passes%20of%20the%20data%20than%20a%20standard%20deterministic%20algorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15159v1&entry.124074799=Read"},
{"title": "Discovering Common Information in Multi-view Data", "author": "Qi Zhang and Mingfei Lu and Shujian Yu and Jingmin Xin and Badong Chen", "abstract": "  We introduce an innovative and mathematically rigorous definition for\ncomputing common information from multi-view data, drawing inspiration from\nG\\'acs-K\\\"orner common information in information theory. Leveraging this\ndefinition, we develop a novel supervised multi-view learning framework to\ncapture both common and unique information. By explicitly minimizing a total\ncorrelation term, the extracted common information and the unique information\nfrom each view are forced to be independent of each other, which, in turn,\ntheoretically guarantees the effectiveness of our framework. To estimate\ninformation-theoretic quantities, our framework employs matrix-based\nR{\\'e}nyi's $\\alpha$-order entropy functional, which forgoes the need for\nvariational approximation and distributional estimation in high-dimensional\nspace. Theoretical proof is provided that our framework can faithfully discover\nboth common and unique information from multi-view data. Experiments on\nsynthetic and seven benchmark real-world datasets demonstrate the superior\nperformance of our proposed framework over state-of-the-art approaches.\n", "link": "http://arxiv.org/abs/2406.15043v1", "date": "2024-06-21", "relevancy": 2.5068, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5044}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4998}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4998}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discovering%20Common%20Information%20in%20Multi-view%20Data&body=Title%3A%20Discovering%20Common%20Information%20in%20Multi-view%20Data%0AAuthor%3A%20Qi%20Zhang%20and%20Mingfei%20Lu%20and%20Shujian%20Yu%20and%20Jingmin%20Xin%20and%20Badong%20Chen%0AAbstract%3A%20%20%20We%20introduce%20an%20innovative%20and%20mathematically%20rigorous%20definition%20for%0Acomputing%20common%20information%20from%20multi-view%20data%2C%20drawing%20inspiration%20from%0AG%5C%27acs-K%5C%22orner%20common%20information%20in%20information%20theory.%20Leveraging%20this%0Adefinition%2C%20we%20develop%20a%20novel%20supervised%20multi-view%20learning%20framework%20to%0Acapture%20both%20common%20and%20unique%20information.%20By%20explicitly%20minimizing%20a%20total%0Acorrelation%20term%2C%20the%20extracted%20common%20information%20and%20the%20unique%20information%0Afrom%20each%20view%20are%20forced%20to%20be%20independent%20of%20each%20other%2C%20which%2C%20in%20turn%2C%0Atheoretically%20guarantees%20the%20effectiveness%20of%20our%20framework.%20To%20estimate%0Ainformation-theoretic%20quantities%2C%20our%20framework%20employs%20matrix-based%0AR%7B%5C%27e%7Dnyi%27s%20%24%5Calpha%24-order%20entropy%20functional%2C%20which%20forgoes%20the%20need%20for%0Avariational%20approximation%20and%20distributional%20estimation%20in%20high-dimensional%0Aspace.%20Theoretical%20proof%20is%20provided%20that%20our%20framework%20can%20faithfully%20discover%0Aboth%20common%20and%20unique%20information%20from%20multi-view%20data.%20Experiments%20on%0Asynthetic%20and%20seven%20benchmark%20real-world%20datasets%20demonstrate%20the%20superior%0Aperformance%20of%20our%20proposed%20framework%20over%20state-of-the-art%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15043v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscovering%2520Common%2520Information%2520in%2520Multi-view%2520Data%26entry.906535625%3DQi%2520Zhang%2520and%2520Mingfei%2520Lu%2520and%2520Shujian%2520Yu%2520and%2520Jingmin%2520Xin%2520and%2520Badong%2520Chen%26entry.1292438233%3D%2520%2520We%2520introduce%2520an%2520innovative%2520and%2520mathematically%2520rigorous%2520definition%2520for%250Acomputing%2520common%2520information%2520from%2520multi-view%2520data%252C%2520drawing%2520inspiration%2520from%250AG%255C%2527acs-K%255C%2522orner%2520common%2520information%2520in%2520information%2520theory.%2520Leveraging%2520this%250Adefinition%252C%2520we%2520develop%2520a%2520novel%2520supervised%2520multi-view%2520learning%2520framework%2520to%250Acapture%2520both%2520common%2520and%2520unique%2520information.%2520By%2520explicitly%2520minimizing%2520a%2520total%250Acorrelation%2520term%252C%2520the%2520extracted%2520common%2520information%2520and%2520the%2520unique%2520information%250Afrom%2520each%2520view%2520are%2520forced%2520to%2520be%2520independent%2520of%2520each%2520other%252C%2520which%252C%2520in%2520turn%252C%250Atheoretically%2520guarantees%2520the%2520effectiveness%2520of%2520our%2520framework.%2520To%2520estimate%250Ainformation-theoretic%2520quantities%252C%2520our%2520framework%2520employs%2520matrix-based%250AR%257B%255C%2527e%257Dnyi%2527s%2520%2524%255Calpha%2524-order%2520entropy%2520functional%252C%2520which%2520forgoes%2520the%2520need%2520for%250Avariational%2520approximation%2520and%2520distributional%2520estimation%2520in%2520high-dimensional%250Aspace.%2520Theoretical%2520proof%2520is%2520provided%2520that%2520our%2520framework%2520can%2520faithfully%2520discover%250Aboth%2520common%2520and%2520unique%2520information%2520from%2520multi-view%2520data.%2520Experiments%2520on%250Asynthetic%2520and%2520seven%2520benchmark%2520real-world%2520datasets%2520demonstrate%2520the%2520superior%250Aperformance%2520of%2520our%2520proposed%2520framework%2520over%2520state-of-the-art%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15043v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discovering%20Common%20Information%20in%20Multi-view%20Data&entry.906535625=Qi%20Zhang%20and%20Mingfei%20Lu%20and%20Shujian%20Yu%20and%20Jingmin%20Xin%20and%20Badong%20Chen&entry.1292438233=%20%20We%20introduce%20an%20innovative%20and%20mathematically%20rigorous%20definition%20for%0Acomputing%20common%20information%20from%20multi-view%20data%2C%20drawing%20inspiration%20from%0AG%5C%27acs-K%5C%22orner%20common%20information%20in%20information%20theory.%20Leveraging%20this%0Adefinition%2C%20we%20develop%20a%20novel%20supervised%20multi-view%20learning%20framework%20to%0Acapture%20both%20common%20and%20unique%20information.%20By%20explicitly%20minimizing%20a%20total%0Acorrelation%20term%2C%20the%20extracted%20common%20information%20and%20the%20unique%20information%0Afrom%20each%20view%20are%20forced%20to%20be%20independent%20of%20each%20other%2C%20which%2C%20in%20turn%2C%0Atheoretically%20guarantees%20the%20effectiveness%20of%20our%20framework.%20To%20estimate%0Ainformation-theoretic%20quantities%2C%20our%20framework%20employs%20matrix-based%0AR%7B%5C%27e%7Dnyi%27s%20%24%5Calpha%24-order%20entropy%20functional%2C%20which%20forgoes%20the%20need%20for%0Avariational%20approximation%20and%20distributional%20estimation%20in%20high-dimensional%0Aspace.%20Theoretical%20proof%20is%20provided%20that%20our%20framework%20can%20faithfully%20discover%0Aboth%20common%20and%20unique%20information%20from%20multi-view%20data.%20Experiments%20on%0Asynthetic%20and%20seven%20benchmark%20real-world%20datasets%20demonstrate%20the%20superior%0Aperformance%20of%20our%20proposed%20framework%20over%20state-of-the-art%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15043v1&entry.124074799=Read"},
{"title": "Two Complementary Perspectives to Continual Learning: Ask Not Only What\n  to Optimize, But Also How", "author": "Timm Hess and Tinne Tuytelaars and Gido M. van de Ven", "abstract": "  Recent years have seen considerable progress in the continual training of\ndeep neural networks, predominantly thanks to approaches that add replay or\nregularization terms to the loss function to approximate the joint loss over\nall tasks so far. However, we show that even with a perfect approximation to\nthe joint loss, these approaches still suffer from temporary but substantial\nforgetting when starting to train on a new task. Motivated by this 'stability\ngap', we propose that continual learning strategies should focus not only on\nthe optimization objective, but also on the way this objective is optimized.\nWhile there is some continual learning work that alters the optimization\ntrajectory (e.g., using gradient projection techniques), this line of research\nis positioned as alternative to improving the optimization objective, while we\nargue it should be complementary. In search of empirical support for our\nproposition, we perform a series of pre-registered experiments combining\nreplay-approximated joint objectives with gradient projection-based\noptimization routines. However, this first experimental attempt fails to show\nclear and consistent benefits. Nevertheless, our conceptual arguments, as well\nas some of our empirical results, demonstrate the distinctive importance of the\noptimization trajectory in continual learning, thereby opening up a new\ndirection for continual learning research.\n", "link": "http://arxiv.org/abs/2311.04898v2", "date": "2024-06-21", "relevancy": 2.5062, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.511}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5036}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4891}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Two%20Complementary%20Perspectives%20to%20Continual%20Learning%3A%20Ask%20Not%20Only%20What%0A%20%20to%20Optimize%2C%20But%20Also%20How&body=Title%3A%20Two%20Complementary%20Perspectives%20to%20Continual%20Learning%3A%20Ask%20Not%20Only%20What%0A%20%20to%20Optimize%2C%20But%20Also%20How%0AAuthor%3A%20Timm%20Hess%20and%20Tinne%20Tuytelaars%20and%20Gido%20M.%20van%20de%20Ven%0AAbstract%3A%20%20%20Recent%20years%20have%20seen%20considerable%20progress%20in%20the%20continual%20training%20of%0Adeep%20neural%20networks%2C%20predominantly%20thanks%20to%20approaches%20that%20add%20replay%20or%0Aregularization%20terms%20to%20the%20loss%20function%20to%20approximate%20the%20joint%20loss%20over%0Aall%20tasks%20so%20far.%20However%2C%20we%20show%20that%20even%20with%20a%20perfect%20approximation%20to%0Athe%20joint%20loss%2C%20these%20approaches%20still%20suffer%20from%20temporary%20but%20substantial%0Aforgetting%20when%20starting%20to%20train%20on%20a%20new%20task.%20Motivated%20by%20this%20%27stability%0Agap%27%2C%20we%20propose%20that%20continual%20learning%20strategies%20should%20focus%20not%20only%20on%0Athe%20optimization%20objective%2C%20but%20also%20on%20the%20way%20this%20objective%20is%20optimized.%0AWhile%20there%20is%20some%20continual%20learning%20work%20that%20alters%20the%20optimization%0Atrajectory%20%28e.g.%2C%20using%20gradient%20projection%20techniques%29%2C%20this%20line%20of%20research%0Ais%20positioned%20as%20alternative%20to%20improving%20the%20optimization%20objective%2C%20while%20we%0Aargue%20it%20should%20be%20complementary.%20In%20search%20of%20empirical%20support%20for%20our%0Aproposition%2C%20we%20perform%20a%20series%20of%20pre-registered%20experiments%20combining%0Areplay-approximated%20joint%20objectives%20with%20gradient%20projection-based%0Aoptimization%20routines.%20However%2C%20this%20first%20experimental%20attempt%20fails%20to%20show%0Aclear%20and%20consistent%20benefits.%20Nevertheless%2C%20our%20conceptual%20arguments%2C%20as%20well%0Aas%20some%20of%20our%20empirical%20results%2C%20demonstrate%20the%20distinctive%20importance%20of%20the%0Aoptimization%20trajectory%20in%20continual%20learning%2C%20thereby%20opening%20up%20a%20new%0Adirection%20for%20continual%20learning%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.04898v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTwo%2520Complementary%2520Perspectives%2520to%2520Continual%2520Learning%253A%2520Ask%2520Not%2520Only%2520What%250A%2520%2520to%2520Optimize%252C%2520But%2520Also%2520How%26entry.906535625%3DTimm%2520Hess%2520and%2520Tinne%2520Tuytelaars%2520and%2520Gido%2520M.%2520van%2520de%2520Ven%26entry.1292438233%3D%2520%2520Recent%2520years%2520have%2520seen%2520considerable%2520progress%2520in%2520the%2520continual%2520training%2520of%250Adeep%2520neural%2520networks%252C%2520predominantly%2520thanks%2520to%2520approaches%2520that%2520add%2520replay%2520or%250Aregularization%2520terms%2520to%2520the%2520loss%2520function%2520to%2520approximate%2520the%2520joint%2520loss%2520over%250Aall%2520tasks%2520so%2520far.%2520However%252C%2520we%2520show%2520that%2520even%2520with%2520a%2520perfect%2520approximation%2520to%250Athe%2520joint%2520loss%252C%2520these%2520approaches%2520still%2520suffer%2520from%2520temporary%2520but%2520substantial%250Aforgetting%2520when%2520starting%2520to%2520train%2520on%2520a%2520new%2520task.%2520Motivated%2520by%2520this%2520%2527stability%250Agap%2527%252C%2520we%2520propose%2520that%2520continual%2520learning%2520strategies%2520should%2520focus%2520not%2520only%2520on%250Athe%2520optimization%2520objective%252C%2520but%2520also%2520on%2520the%2520way%2520this%2520objective%2520is%2520optimized.%250AWhile%2520there%2520is%2520some%2520continual%2520learning%2520work%2520that%2520alters%2520the%2520optimization%250Atrajectory%2520%2528e.g.%252C%2520using%2520gradient%2520projection%2520techniques%2529%252C%2520this%2520line%2520of%2520research%250Ais%2520positioned%2520as%2520alternative%2520to%2520improving%2520the%2520optimization%2520objective%252C%2520while%2520we%250Aargue%2520it%2520should%2520be%2520complementary.%2520In%2520search%2520of%2520empirical%2520support%2520for%2520our%250Aproposition%252C%2520we%2520perform%2520a%2520series%2520of%2520pre-registered%2520experiments%2520combining%250Areplay-approximated%2520joint%2520objectives%2520with%2520gradient%2520projection-based%250Aoptimization%2520routines.%2520However%252C%2520this%2520first%2520experimental%2520attempt%2520fails%2520to%2520show%250Aclear%2520and%2520consistent%2520benefits.%2520Nevertheless%252C%2520our%2520conceptual%2520arguments%252C%2520as%2520well%250Aas%2520some%2520of%2520our%2520empirical%2520results%252C%2520demonstrate%2520the%2520distinctive%2520importance%2520of%2520the%250Aoptimization%2520trajectory%2520in%2520continual%2520learning%252C%2520thereby%2520opening%2520up%2520a%2520new%250Adirection%2520for%2520continual%2520learning%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.04898v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Two%20Complementary%20Perspectives%20to%20Continual%20Learning%3A%20Ask%20Not%20Only%20What%0A%20%20to%20Optimize%2C%20But%20Also%20How&entry.906535625=Timm%20Hess%20and%20Tinne%20Tuytelaars%20and%20Gido%20M.%20van%20de%20Ven&entry.1292438233=%20%20Recent%20years%20have%20seen%20considerable%20progress%20in%20the%20continual%20training%20of%0Adeep%20neural%20networks%2C%20predominantly%20thanks%20to%20approaches%20that%20add%20replay%20or%0Aregularization%20terms%20to%20the%20loss%20function%20to%20approximate%20the%20joint%20loss%20over%0Aall%20tasks%20so%20far.%20However%2C%20we%20show%20that%20even%20with%20a%20perfect%20approximation%20to%0Athe%20joint%20loss%2C%20these%20approaches%20still%20suffer%20from%20temporary%20but%20substantial%0Aforgetting%20when%20starting%20to%20train%20on%20a%20new%20task.%20Motivated%20by%20this%20%27stability%0Agap%27%2C%20we%20propose%20that%20continual%20learning%20strategies%20should%20focus%20not%20only%20on%0Athe%20optimization%20objective%2C%20but%20also%20on%20the%20way%20this%20objective%20is%20optimized.%0AWhile%20there%20is%20some%20continual%20learning%20work%20that%20alters%20the%20optimization%0Atrajectory%20%28e.g.%2C%20using%20gradient%20projection%20techniques%29%2C%20this%20line%20of%20research%0Ais%20positioned%20as%20alternative%20to%20improving%20the%20optimization%20objective%2C%20while%20we%0Aargue%20it%20should%20be%20complementary.%20In%20search%20of%20empirical%20support%20for%20our%0Aproposition%2C%20we%20perform%20a%20series%20of%20pre-registered%20experiments%20combining%0Areplay-approximated%20joint%20objectives%20with%20gradient%20projection-based%0Aoptimization%20routines.%20However%2C%20this%20first%20experimental%20attempt%20fails%20to%20show%0Aclear%20and%20consistent%20benefits.%20Nevertheless%2C%20our%20conceptual%20arguments%2C%20as%20well%0Aas%20some%20of%20our%20empirical%20results%2C%20demonstrate%20the%20distinctive%20importance%20of%20the%0Aoptimization%20trajectory%20in%20continual%20learning%2C%20thereby%20opening%20up%20a%20new%0Adirection%20for%20continual%20learning%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.04898v2&entry.124074799=Read"},
{"title": "From Overfitting to Robustness: Quantity, Quality, and Variety Oriented\n  Negative Sample Selection in Graph Contrastive Learning", "author": "Adnan Ali and Jinlong Li and Huanhuan Chen and Ali Kashif Bashir", "abstract": "  Graph contrastive learning (GCL) aims to contrast positive-negative\ncounterparts to learn the node embeddings, whereas graph data augmentation\nmethods are employed to generate these positive-negative samples. The\nvariation, quantity, and quality of negative samples compared to positive\nsamples play crucial roles in learning meaningful embeddings for node\nclassification downstream tasks. Less variation, excessive quantity, and\nlow-quality negative samples cause the model to be overfitted for particular\nnodes, resulting in less robust models. To solve the overfitting problem in the\nGCL paradigm, this study proposes a novel Cumulative Sample Selection (CSS)\nalgorithm by comprehensively considering negative samples' quality, variations,\nand quantity. Initially, three negative sample pools are constructed: easy,\nmedium, and hard negative samples, which contain 25%, 50%, and 25% of the total\navailable negative samples, respectively. Then, 10% negative samples are\nselected from each of these three negative sample pools for training the model.\nAfter that, a decision agent module evaluates model training results and\ndecides whether to explore more negative samples from three negative sample\npools by increasing the ratio or keep exploiting the current sampling ratio.\nThe proposed algorithm is integrated into a proposed graph contrastive learning\nframework named NegAmplify. NegAmplify is compared with the SOTA methods on\nnine graph node classification datasets, with seven achieving better node\nclassification accuracy with up to 2.86% improvement.\n", "link": "http://arxiv.org/abs/2406.15044v1", "date": "2024-06-21", "relevancy": 2.4937, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5286}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4975}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4701}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Overfitting%20to%20Robustness%3A%20Quantity%2C%20Quality%2C%20and%20Variety%20Oriented%0A%20%20Negative%20Sample%20Selection%20in%20Graph%20Contrastive%20Learning&body=Title%3A%20From%20Overfitting%20to%20Robustness%3A%20Quantity%2C%20Quality%2C%20and%20Variety%20Oriented%0A%20%20Negative%20Sample%20Selection%20in%20Graph%20Contrastive%20Learning%0AAuthor%3A%20Adnan%20Ali%20and%20Jinlong%20Li%20and%20Huanhuan%20Chen%20and%20Ali%20Kashif%20Bashir%0AAbstract%3A%20%20%20Graph%20contrastive%20learning%20%28GCL%29%20aims%20to%20contrast%20positive-negative%0Acounterparts%20to%20learn%20the%20node%20embeddings%2C%20whereas%20graph%20data%20augmentation%0Amethods%20are%20employed%20to%20generate%20these%20positive-negative%20samples.%20The%0Avariation%2C%20quantity%2C%20and%20quality%20of%20negative%20samples%20compared%20to%20positive%0Asamples%20play%20crucial%20roles%20in%20learning%20meaningful%20embeddings%20for%20node%0Aclassification%20downstream%20tasks.%20Less%20variation%2C%20excessive%20quantity%2C%20and%0Alow-quality%20negative%20samples%20cause%20the%20model%20to%20be%20overfitted%20for%20particular%0Anodes%2C%20resulting%20in%20less%20robust%20models.%20To%20solve%20the%20overfitting%20problem%20in%20the%0AGCL%20paradigm%2C%20this%20study%20proposes%20a%20novel%20Cumulative%20Sample%20Selection%20%28CSS%29%0Aalgorithm%20by%20comprehensively%20considering%20negative%20samples%27%20quality%2C%20variations%2C%0Aand%20quantity.%20Initially%2C%20three%20negative%20sample%20pools%20are%20constructed%3A%20easy%2C%0Amedium%2C%20and%20hard%20negative%20samples%2C%20which%20contain%2025%25%2C%2050%25%2C%20and%2025%25%20of%20the%20total%0Aavailable%20negative%20samples%2C%20respectively.%20Then%2C%2010%25%20negative%20samples%20are%0Aselected%20from%20each%20of%20these%20three%20negative%20sample%20pools%20for%20training%20the%20model.%0AAfter%20that%2C%20a%20decision%20agent%20module%20evaluates%20model%20training%20results%20and%0Adecides%20whether%20to%20explore%20more%20negative%20samples%20from%20three%20negative%20sample%0Apools%20by%20increasing%20the%20ratio%20or%20keep%20exploiting%20the%20current%20sampling%20ratio.%0AThe%20proposed%20algorithm%20is%20integrated%20into%20a%20proposed%20graph%20contrastive%20learning%0Aframework%20named%20NegAmplify.%20NegAmplify%20is%20compared%20with%20the%20SOTA%20methods%20on%0Anine%20graph%20node%20classification%20datasets%2C%20with%20seven%20achieving%20better%20node%0Aclassification%20accuracy%20with%20up%20to%202.86%25%20improvement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15044v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Overfitting%2520to%2520Robustness%253A%2520Quantity%252C%2520Quality%252C%2520and%2520Variety%2520Oriented%250A%2520%2520Negative%2520Sample%2520Selection%2520in%2520Graph%2520Contrastive%2520Learning%26entry.906535625%3DAdnan%2520Ali%2520and%2520Jinlong%2520Li%2520and%2520Huanhuan%2520Chen%2520and%2520Ali%2520Kashif%2520Bashir%26entry.1292438233%3D%2520%2520Graph%2520contrastive%2520learning%2520%2528GCL%2529%2520aims%2520to%2520contrast%2520positive-negative%250Acounterparts%2520to%2520learn%2520the%2520node%2520embeddings%252C%2520whereas%2520graph%2520data%2520augmentation%250Amethods%2520are%2520employed%2520to%2520generate%2520these%2520positive-negative%2520samples.%2520The%250Avariation%252C%2520quantity%252C%2520and%2520quality%2520of%2520negative%2520samples%2520compared%2520to%2520positive%250Asamples%2520play%2520crucial%2520roles%2520in%2520learning%2520meaningful%2520embeddings%2520for%2520node%250Aclassification%2520downstream%2520tasks.%2520Less%2520variation%252C%2520excessive%2520quantity%252C%2520and%250Alow-quality%2520negative%2520samples%2520cause%2520the%2520model%2520to%2520be%2520overfitted%2520for%2520particular%250Anodes%252C%2520resulting%2520in%2520less%2520robust%2520models.%2520To%2520solve%2520the%2520overfitting%2520problem%2520in%2520the%250AGCL%2520paradigm%252C%2520this%2520study%2520proposes%2520a%2520novel%2520Cumulative%2520Sample%2520Selection%2520%2528CSS%2529%250Aalgorithm%2520by%2520comprehensively%2520considering%2520negative%2520samples%2527%2520quality%252C%2520variations%252C%250Aand%2520quantity.%2520Initially%252C%2520three%2520negative%2520sample%2520pools%2520are%2520constructed%253A%2520easy%252C%250Amedium%252C%2520and%2520hard%2520negative%2520samples%252C%2520which%2520contain%252025%2525%252C%252050%2525%252C%2520and%252025%2525%2520of%2520the%2520total%250Aavailable%2520negative%2520samples%252C%2520respectively.%2520Then%252C%252010%2525%2520negative%2520samples%2520are%250Aselected%2520from%2520each%2520of%2520these%2520three%2520negative%2520sample%2520pools%2520for%2520training%2520the%2520model.%250AAfter%2520that%252C%2520a%2520decision%2520agent%2520module%2520evaluates%2520model%2520training%2520results%2520and%250Adecides%2520whether%2520to%2520explore%2520more%2520negative%2520samples%2520from%2520three%2520negative%2520sample%250Apools%2520by%2520increasing%2520the%2520ratio%2520or%2520keep%2520exploiting%2520the%2520current%2520sampling%2520ratio.%250AThe%2520proposed%2520algorithm%2520is%2520integrated%2520into%2520a%2520proposed%2520graph%2520contrastive%2520learning%250Aframework%2520named%2520NegAmplify.%2520NegAmplify%2520is%2520compared%2520with%2520the%2520SOTA%2520methods%2520on%250Anine%2520graph%2520node%2520classification%2520datasets%252C%2520with%2520seven%2520achieving%2520better%2520node%250Aclassification%2520accuracy%2520with%2520up%2520to%25202.86%2525%2520improvement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15044v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Overfitting%20to%20Robustness%3A%20Quantity%2C%20Quality%2C%20and%20Variety%20Oriented%0A%20%20Negative%20Sample%20Selection%20in%20Graph%20Contrastive%20Learning&entry.906535625=Adnan%20Ali%20and%20Jinlong%20Li%20and%20Huanhuan%20Chen%20and%20Ali%20Kashif%20Bashir&entry.1292438233=%20%20Graph%20contrastive%20learning%20%28GCL%29%20aims%20to%20contrast%20positive-negative%0Acounterparts%20to%20learn%20the%20node%20embeddings%2C%20whereas%20graph%20data%20augmentation%0Amethods%20are%20employed%20to%20generate%20these%20positive-negative%20samples.%20The%0Avariation%2C%20quantity%2C%20and%20quality%20of%20negative%20samples%20compared%20to%20positive%0Asamples%20play%20crucial%20roles%20in%20learning%20meaningful%20embeddings%20for%20node%0Aclassification%20downstream%20tasks.%20Less%20variation%2C%20excessive%20quantity%2C%20and%0Alow-quality%20negative%20samples%20cause%20the%20model%20to%20be%20overfitted%20for%20particular%0Anodes%2C%20resulting%20in%20less%20robust%20models.%20To%20solve%20the%20overfitting%20problem%20in%20the%0AGCL%20paradigm%2C%20this%20study%20proposes%20a%20novel%20Cumulative%20Sample%20Selection%20%28CSS%29%0Aalgorithm%20by%20comprehensively%20considering%20negative%20samples%27%20quality%2C%20variations%2C%0Aand%20quantity.%20Initially%2C%20three%20negative%20sample%20pools%20are%20constructed%3A%20easy%2C%0Amedium%2C%20and%20hard%20negative%20samples%2C%20which%20contain%2025%25%2C%2050%25%2C%20and%2025%25%20of%20the%20total%0Aavailable%20negative%20samples%2C%20respectively.%20Then%2C%2010%25%20negative%20samples%20are%0Aselected%20from%20each%20of%20these%20three%20negative%20sample%20pools%20for%20training%20the%20model.%0AAfter%20that%2C%20a%20decision%20agent%20module%20evaluates%20model%20training%20results%20and%0Adecides%20whether%20to%20explore%20more%20negative%20samples%20from%20three%20negative%20sample%0Apools%20by%20increasing%20the%20ratio%20or%20keep%20exploiting%20the%20current%20sampling%20ratio.%0AThe%20proposed%20algorithm%20is%20integrated%20into%20a%20proposed%20graph%20contrastive%20learning%0Aframework%20named%20NegAmplify.%20NegAmplify%20is%20compared%20with%20the%20SOTA%20methods%20on%0Anine%20graph%20node%20classification%20datasets%2C%20with%20seven%20achieving%20better%20node%0Aclassification%20accuracy%20with%20up%20to%202.86%25%20improvement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15044v1&entry.124074799=Read"},
{"title": "A Dual Attention-aided DenseNet-121 for Classification of Glaucoma from\n  Fundus Images", "author": "Soham Chakraborty and Ayush Roy and Payel Pramanik and Daria Valenkova and Ram Sarkar", "abstract": "  Deep learning and computer vision methods are nowadays predominantly used in\nthe field of ophthalmology. In this paper, we present an attention-aided\nDenseNet-121 for classifying normal and glaucomatous eyes from fundus images.\nIt involves the convolutional block attention module to highlight relevant\nspatial and channel features extracted by DenseNet-121. The channel\nrecalibration module further enriches the features by utilizing edge\ninformation along with the statistical features of the spatial dimension. For\nthe experiments, two standard datasets, namely RIM-ONE and ACRIMA, have been\nused. Our method has shown superior results than state-of-the-art models. An\nablation study has also been conducted to show the effectiveness of each of the\ncomponents. The code of the proposed work is available at:\nhttps://github.com/Soham2004GitHub/DADGC.\n", "link": "http://arxiv.org/abs/2406.15113v1", "date": "2024-06-21", "relevancy": 2.489, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5031}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4995}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4909}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Dual%20Attention-aided%20DenseNet-121%20for%20Classification%20of%20Glaucoma%20from%0A%20%20Fundus%20Images&body=Title%3A%20A%20Dual%20Attention-aided%20DenseNet-121%20for%20Classification%20of%20Glaucoma%20from%0A%20%20Fundus%20Images%0AAuthor%3A%20Soham%20Chakraborty%20and%20Ayush%20Roy%20and%20Payel%20Pramanik%20and%20Daria%20Valenkova%20and%20Ram%20Sarkar%0AAbstract%3A%20%20%20Deep%20learning%20and%20computer%20vision%20methods%20are%20nowadays%20predominantly%20used%20in%0Athe%20field%20of%20ophthalmology.%20In%20this%20paper%2C%20we%20present%20an%20attention-aided%0ADenseNet-121%20for%20classifying%20normal%20and%20glaucomatous%20eyes%20from%20fundus%20images.%0AIt%20involves%20the%20convolutional%20block%20attention%20module%20to%20highlight%20relevant%0Aspatial%20and%20channel%20features%20extracted%20by%20DenseNet-121.%20The%20channel%0Arecalibration%20module%20further%20enriches%20the%20features%20by%20utilizing%20edge%0Ainformation%20along%20with%20the%20statistical%20features%20of%20the%20spatial%20dimension.%20For%0Athe%20experiments%2C%20two%20standard%20datasets%2C%20namely%20RIM-ONE%20and%20ACRIMA%2C%20have%20been%0Aused.%20Our%20method%20has%20shown%20superior%20results%20than%20state-of-the-art%20models.%20An%0Aablation%20study%20has%20also%20been%20conducted%20to%20show%20the%20effectiveness%20of%20each%20of%20the%0Acomponents.%20The%20code%20of%20the%20proposed%20work%20is%20available%20at%3A%0Ahttps%3A//github.com/Soham2004GitHub/DADGC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15113v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Dual%2520Attention-aided%2520DenseNet-121%2520for%2520Classification%2520of%2520Glaucoma%2520from%250A%2520%2520Fundus%2520Images%26entry.906535625%3DSoham%2520Chakraborty%2520and%2520Ayush%2520Roy%2520and%2520Payel%2520Pramanik%2520and%2520Daria%2520Valenkova%2520and%2520Ram%2520Sarkar%26entry.1292438233%3D%2520%2520Deep%2520learning%2520and%2520computer%2520vision%2520methods%2520are%2520nowadays%2520predominantly%2520used%2520in%250Athe%2520field%2520of%2520ophthalmology.%2520In%2520this%2520paper%252C%2520we%2520present%2520an%2520attention-aided%250ADenseNet-121%2520for%2520classifying%2520normal%2520and%2520glaucomatous%2520eyes%2520from%2520fundus%2520images.%250AIt%2520involves%2520the%2520convolutional%2520block%2520attention%2520module%2520to%2520highlight%2520relevant%250Aspatial%2520and%2520channel%2520features%2520extracted%2520by%2520DenseNet-121.%2520The%2520channel%250Arecalibration%2520module%2520further%2520enriches%2520the%2520features%2520by%2520utilizing%2520edge%250Ainformation%2520along%2520with%2520the%2520statistical%2520features%2520of%2520the%2520spatial%2520dimension.%2520For%250Athe%2520experiments%252C%2520two%2520standard%2520datasets%252C%2520namely%2520RIM-ONE%2520and%2520ACRIMA%252C%2520have%2520been%250Aused.%2520Our%2520method%2520has%2520shown%2520superior%2520results%2520than%2520state-of-the-art%2520models.%2520An%250Aablation%2520study%2520has%2520also%2520been%2520conducted%2520to%2520show%2520the%2520effectiveness%2520of%2520each%2520of%2520the%250Acomponents.%2520The%2520code%2520of%2520the%2520proposed%2520work%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/Soham2004GitHub/DADGC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15113v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Dual%20Attention-aided%20DenseNet-121%20for%20Classification%20of%20Glaucoma%20from%0A%20%20Fundus%20Images&entry.906535625=Soham%20Chakraborty%20and%20Ayush%20Roy%20and%20Payel%20Pramanik%20and%20Daria%20Valenkova%20and%20Ram%20Sarkar&entry.1292438233=%20%20Deep%20learning%20and%20computer%20vision%20methods%20are%20nowadays%20predominantly%20used%20in%0Athe%20field%20of%20ophthalmology.%20In%20this%20paper%2C%20we%20present%20an%20attention-aided%0ADenseNet-121%20for%20classifying%20normal%20and%20glaucomatous%20eyes%20from%20fundus%20images.%0AIt%20involves%20the%20convolutional%20block%20attention%20module%20to%20highlight%20relevant%0Aspatial%20and%20channel%20features%20extracted%20by%20DenseNet-121.%20The%20channel%0Arecalibration%20module%20further%20enriches%20the%20features%20by%20utilizing%20edge%0Ainformation%20along%20with%20the%20statistical%20features%20of%20the%20spatial%20dimension.%20For%0Athe%20experiments%2C%20two%20standard%20datasets%2C%20namely%20RIM-ONE%20and%20ACRIMA%2C%20have%20been%0Aused.%20Our%20method%20has%20shown%20superior%20results%20than%20state-of-the-art%20models.%20An%0Aablation%20study%20has%20also%20been%20conducted%20to%20show%20the%20effectiveness%20of%20each%20of%20the%0Acomponents.%20The%20code%20of%20the%20proposed%20work%20is%20available%20at%3A%0Ahttps%3A//github.com/Soham2004GitHub/DADGC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15113v1&entry.124074799=Read"},
{"title": "Generative AI Misuse: A Taxonomy of Tactics and Insights from Real-World\n  Data", "author": "Nahema Marchal and Rachel Xu and Rasmi Elasmar and Iason Gabriel and Beth Goldberg and William Isaac", "abstract": "  Generative, multimodal artificial intelligence (GenAI) offers transformative\npotential across industries, but its misuse poses significant risks. Prior\nresearch has shed light on the potential of advanced AI systems to be exploited\nfor malicious purposes. However, we still lack a concrete understanding of how\nGenAI models are specifically exploited or abused in practice, including the\ntactics employed to inflict harm. In this paper, we present a taxonomy of GenAI\nmisuse tactics, informed by existing academic literature and a qualitative\nanalysis of approximately 200 observed incidents of misuse reported between\nJanuary 2023 and March 2024. Through this analysis, we illuminate key and novel\npatterns in misuse during this time period, including potential motivations,\nstrategies, and how attackers leverage and abuse system capabilities across\nmodalities (e.g. image, text, audio, video) in the wild.\n", "link": "http://arxiv.org/abs/2406.13843v2", "date": "2024-06-21", "relevancy": 2.467, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5475}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4668}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20AI%20Misuse%3A%20A%20Taxonomy%20of%20Tactics%20and%20Insights%20from%20Real-World%0A%20%20Data&body=Title%3A%20Generative%20AI%20Misuse%3A%20A%20Taxonomy%20of%20Tactics%20and%20Insights%20from%20Real-World%0A%20%20Data%0AAuthor%3A%20Nahema%20Marchal%20and%20Rachel%20Xu%20and%20Rasmi%20Elasmar%20and%20Iason%20Gabriel%20and%20Beth%20Goldberg%20and%20William%20Isaac%0AAbstract%3A%20%20%20Generative%2C%20multimodal%20artificial%20intelligence%20%28GenAI%29%20offers%20transformative%0Apotential%20across%20industries%2C%20but%20its%20misuse%20poses%20significant%20risks.%20Prior%0Aresearch%20has%20shed%20light%20on%20the%20potential%20of%20advanced%20AI%20systems%20to%20be%20exploited%0Afor%20malicious%20purposes.%20However%2C%20we%20still%20lack%20a%20concrete%20understanding%20of%20how%0AGenAI%20models%20are%20specifically%20exploited%20or%20abused%20in%20practice%2C%20including%20the%0Atactics%20employed%20to%20inflict%20harm.%20In%20this%20paper%2C%20we%20present%20a%20taxonomy%20of%20GenAI%0Amisuse%20tactics%2C%20informed%20by%20existing%20academic%20literature%20and%20a%20qualitative%0Aanalysis%20of%20approximately%20200%20observed%20incidents%20of%20misuse%20reported%20between%0AJanuary%202023%20and%20March%202024.%20Through%20this%20analysis%2C%20we%20illuminate%20key%20and%20novel%0Apatterns%20in%20misuse%20during%20this%20time%20period%2C%20including%20potential%20motivations%2C%0Astrategies%2C%20and%20how%20attackers%20leverage%20and%20abuse%20system%20capabilities%20across%0Amodalities%20%28e.g.%20image%2C%20text%2C%20audio%2C%20video%29%20in%20the%20wild.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.13843v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520AI%2520Misuse%253A%2520A%2520Taxonomy%2520of%2520Tactics%2520and%2520Insights%2520from%2520Real-World%250A%2520%2520Data%26entry.906535625%3DNahema%2520Marchal%2520and%2520Rachel%2520Xu%2520and%2520Rasmi%2520Elasmar%2520and%2520Iason%2520Gabriel%2520and%2520Beth%2520Goldberg%2520and%2520William%2520Isaac%26entry.1292438233%3D%2520%2520Generative%252C%2520multimodal%2520artificial%2520intelligence%2520%2528GenAI%2529%2520offers%2520transformative%250Apotential%2520across%2520industries%252C%2520but%2520its%2520misuse%2520poses%2520significant%2520risks.%2520Prior%250Aresearch%2520has%2520shed%2520light%2520on%2520the%2520potential%2520of%2520advanced%2520AI%2520systems%2520to%2520be%2520exploited%250Afor%2520malicious%2520purposes.%2520However%252C%2520we%2520still%2520lack%2520a%2520concrete%2520understanding%2520of%2520how%250AGenAI%2520models%2520are%2520specifically%2520exploited%2520or%2520abused%2520in%2520practice%252C%2520including%2520the%250Atactics%2520employed%2520to%2520inflict%2520harm.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520taxonomy%2520of%2520GenAI%250Amisuse%2520tactics%252C%2520informed%2520by%2520existing%2520academic%2520literature%2520and%2520a%2520qualitative%250Aanalysis%2520of%2520approximately%2520200%2520observed%2520incidents%2520of%2520misuse%2520reported%2520between%250AJanuary%25202023%2520and%2520March%25202024.%2520Through%2520this%2520analysis%252C%2520we%2520illuminate%2520key%2520and%2520novel%250Apatterns%2520in%2520misuse%2520during%2520this%2520time%2520period%252C%2520including%2520potential%2520motivations%252C%250Astrategies%252C%2520and%2520how%2520attackers%2520leverage%2520and%2520abuse%2520system%2520capabilities%2520across%250Amodalities%2520%2528e.g.%2520image%252C%2520text%252C%2520audio%252C%2520video%2529%2520in%2520the%2520wild.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.13843v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20AI%20Misuse%3A%20A%20Taxonomy%20of%20Tactics%20and%20Insights%20from%20Real-World%0A%20%20Data&entry.906535625=Nahema%20Marchal%20and%20Rachel%20Xu%20and%20Rasmi%20Elasmar%20and%20Iason%20Gabriel%20and%20Beth%20Goldberg%20and%20William%20Isaac&entry.1292438233=%20%20Generative%2C%20multimodal%20artificial%20intelligence%20%28GenAI%29%20offers%20transformative%0Apotential%20across%20industries%2C%20but%20its%20misuse%20poses%20significant%20risks.%20Prior%0Aresearch%20has%20shed%20light%20on%20the%20potential%20of%20advanced%20AI%20systems%20to%20be%20exploited%0Afor%20malicious%20purposes.%20However%2C%20we%20still%20lack%20a%20concrete%20understanding%20of%20how%0AGenAI%20models%20are%20specifically%20exploited%20or%20abused%20in%20practice%2C%20including%20the%0Atactics%20employed%20to%20inflict%20harm.%20In%20this%20paper%2C%20we%20present%20a%20taxonomy%20of%20GenAI%0Amisuse%20tactics%2C%20informed%20by%20existing%20academic%20literature%20and%20a%20qualitative%0Aanalysis%20of%20approximately%20200%20observed%20incidents%20of%20misuse%20reported%20between%0AJanuary%202023%20and%20March%202024.%20Through%20this%20analysis%2C%20we%20illuminate%20key%20and%20novel%0Apatterns%20in%20misuse%20during%20this%20time%20period%2C%20including%20potential%20motivations%2C%0Astrategies%2C%20and%20how%20attackers%20leverage%20and%20abuse%20system%20capabilities%20across%0Amodalities%20%28e.g.%20image%2C%20text%2C%20audio%2C%20video%29%20in%20the%20wild.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.13843v2&entry.124074799=Read"},
{"title": "Large Batch Analysis for Adagrad Under Anisotropic Smoothness", "author": "Yuxing Liu and Rui Pan and Tong Zhang", "abstract": "  Adaptive gradient algorithms have been widely adopted in training large-scale\ndeep neural networks, especially large foundation models. Despite their huge\nsuccess in practice, their theoretical advantages over stochastic gradient\ndescent (SGD) have not been fully understood, especially in the large\nbatch-size setting commonly used in practice. This is because the only\ntheoretical result that can demonstrate the benefit of Adagrad over SGD was\nobtained in the original paper of Adagrad for nonsmooth objective functions.\nHowever, for nonsmooth objective functions, there can be a linear slowdown of\nconvergence when batch size increases, and thus a convergence analysis based on\nnonsmooth assumption cannot be used for large batch algorithms. In this work,\nwe resolve this gap between theory and practice by providing a new analysis of\nAdagrad on both convex and nonconvex smooth objectives suitable for the large\nbatch setting. It is shown that under the anisotropic smoothness and noise\nconditions, increased batch size does not slow down convergence for Adagrad,\nand thus it can still achieve a faster convergence guarantee over SGD even in\nthe large batch setting. We present detailed comparisons between SGD and\nAdagrad to provide a better understanding of the benefits of adaptive gradient\nmethods. Experiments in logistic regression and instruction following\nfine-tuning tasks provide strong evidence to support our theoretical analysis.\n", "link": "http://arxiv.org/abs/2406.15244v1", "date": "2024-06-21", "relevancy": 2.4492, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4996}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4887}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4812}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Batch%20Analysis%20for%20Adagrad%20Under%20Anisotropic%20Smoothness&body=Title%3A%20Large%20Batch%20Analysis%20for%20Adagrad%20Under%20Anisotropic%20Smoothness%0AAuthor%3A%20Yuxing%20Liu%20and%20Rui%20Pan%20and%20Tong%20Zhang%0AAbstract%3A%20%20%20Adaptive%20gradient%20algorithms%20have%20been%20widely%20adopted%20in%20training%20large-scale%0Adeep%20neural%20networks%2C%20especially%20large%20foundation%20models.%20Despite%20their%20huge%0Asuccess%20in%20practice%2C%20their%20theoretical%20advantages%20over%20stochastic%20gradient%0Adescent%20%28SGD%29%20have%20not%20been%20fully%20understood%2C%20especially%20in%20the%20large%0Abatch-size%20setting%20commonly%20used%20in%20practice.%20This%20is%20because%20the%20only%0Atheoretical%20result%20that%20can%20demonstrate%20the%20benefit%20of%20Adagrad%20over%20SGD%20was%0Aobtained%20in%20the%20original%20paper%20of%20Adagrad%20for%20nonsmooth%20objective%20functions.%0AHowever%2C%20for%20nonsmooth%20objective%20functions%2C%20there%20can%20be%20a%20linear%20slowdown%20of%0Aconvergence%20when%20batch%20size%20increases%2C%20and%20thus%20a%20convergence%20analysis%20based%20on%0Anonsmooth%20assumption%20cannot%20be%20used%20for%20large%20batch%20algorithms.%20In%20this%20work%2C%0Awe%20resolve%20this%20gap%20between%20theory%20and%20practice%20by%20providing%20a%20new%20analysis%20of%0AAdagrad%20on%20both%20convex%20and%20nonconvex%20smooth%20objectives%20suitable%20for%20the%20large%0Abatch%20setting.%20It%20is%20shown%20that%20under%20the%20anisotropic%20smoothness%20and%20noise%0Aconditions%2C%20increased%20batch%20size%20does%20not%20slow%20down%20convergence%20for%20Adagrad%2C%0Aand%20thus%20it%20can%20still%20achieve%20a%20faster%20convergence%20guarantee%20over%20SGD%20even%20in%0Athe%20large%20batch%20setting.%20We%20present%20detailed%20comparisons%20between%20SGD%20and%0AAdagrad%20to%20provide%20a%20better%20understanding%20of%20the%20benefits%20of%20adaptive%20gradient%0Amethods.%20Experiments%20in%20logistic%20regression%20and%20instruction%20following%0Afine-tuning%20tasks%20provide%20strong%20evidence%20to%20support%20our%20theoretical%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15244v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Batch%2520Analysis%2520for%2520Adagrad%2520Under%2520Anisotropic%2520Smoothness%26entry.906535625%3DYuxing%2520Liu%2520and%2520Rui%2520Pan%2520and%2520Tong%2520Zhang%26entry.1292438233%3D%2520%2520Adaptive%2520gradient%2520algorithms%2520have%2520been%2520widely%2520adopted%2520in%2520training%2520large-scale%250Adeep%2520neural%2520networks%252C%2520especially%2520large%2520foundation%2520models.%2520Despite%2520their%2520huge%250Asuccess%2520in%2520practice%252C%2520their%2520theoretical%2520advantages%2520over%2520stochastic%2520gradient%250Adescent%2520%2528SGD%2529%2520have%2520not%2520been%2520fully%2520understood%252C%2520especially%2520in%2520the%2520large%250Abatch-size%2520setting%2520commonly%2520used%2520in%2520practice.%2520This%2520is%2520because%2520the%2520only%250Atheoretical%2520result%2520that%2520can%2520demonstrate%2520the%2520benefit%2520of%2520Adagrad%2520over%2520SGD%2520was%250Aobtained%2520in%2520the%2520original%2520paper%2520of%2520Adagrad%2520for%2520nonsmooth%2520objective%2520functions.%250AHowever%252C%2520for%2520nonsmooth%2520objective%2520functions%252C%2520there%2520can%2520be%2520a%2520linear%2520slowdown%2520of%250Aconvergence%2520when%2520batch%2520size%2520increases%252C%2520and%2520thus%2520a%2520convergence%2520analysis%2520based%2520on%250Anonsmooth%2520assumption%2520cannot%2520be%2520used%2520for%2520large%2520batch%2520algorithms.%2520In%2520this%2520work%252C%250Awe%2520resolve%2520this%2520gap%2520between%2520theory%2520and%2520practice%2520by%2520providing%2520a%2520new%2520analysis%2520of%250AAdagrad%2520on%2520both%2520convex%2520and%2520nonconvex%2520smooth%2520objectives%2520suitable%2520for%2520the%2520large%250Abatch%2520setting.%2520It%2520is%2520shown%2520that%2520under%2520the%2520anisotropic%2520smoothness%2520and%2520noise%250Aconditions%252C%2520increased%2520batch%2520size%2520does%2520not%2520slow%2520down%2520convergence%2520for%2520Adagrad%252C%250Aand%2520thus%2520it%2520can%2520still%2520achieve%2520a%2520faster%2520convergence%2520guarantee%2520over%2520SGD%2520even%2520in%250Athe%2520large%2520batch%2520setting.%2520We%2520present%2520detailed%2520comparisons%2520between%2520SGD%2520and%250AAdagrad%2520to%2520provide%2520a%2520better%2520understanding%2520of%2520the%2520benefits%2520of%2520adaptive%2520gradient%250Amethods.%2520Experiments%2520in%2520logistic%2520regression%2520and%2520instruction%2520following%250Afine-tuning%2520tasks%2520provide%2520strong%2520evidence%2520to%2520support%2520our%2520theoretical%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15244v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Batch%20Analysis%20for%20Adagrad%20Under%20Anisotropic%20Smoothness&entry.906535625=Yuxing%20Liu%20and%20Rui%20Pan%20and%20Tong%20Zhang&entry.1292438233=%20%20Adaptive%20gradient%20algorithms%20have%20been%20widely%20adopted%20in%20training%20large-scale%0Adeep%20neural%20networks%2C%20especially%20large%20foundation%20models.%20Despite%20their%20huge%0Asuccess%20in%20practice%2C%20their%20theoretical%20advantages%20over%20stochastic%20gradient%0Adescent%20%28SGD%29%20have%20not%20been%20fully%20understood%2C%20especially%20in%20the%20large%0Abatch-size%20setting%20commonly%20used%20in%20practice.%20This%20is%20because%20the%20only%0Atheoretical%20result%20that%20can%20demonstrate%20the%20benefit%20of%20Adagrad%20over%20SGD%20was%0Aobtained%20in%20the%20original%20paper%20of%20Adagrad%20for%20nonsmooth%20objective%20functions.%0AHowever%2C%20for%20nonsmooth%20objective%20functions%2C%20there%20can%20be%20a%20linear%20slowdown%20of%0Aconvergence%20when%20batch%20size%20increases%2C%20and%20thus%20a%20convergence%20analysis%20based%20on%0Anonsmooth%20assumption%20cannot%20be%20used%20for%20large%20batch%20algorithms.%20In%20this%20work%2C%0Awe%20resolve%20this%20gap%20between%20theory%20and%20practice%20by%20providing%20a%20new%20analysis%20of%0AAdagrad%20on%20both%20convex%20and%20nonconvex%20smooth%20objectives%20suitable%20for%20the%20large%0Abatch%20setting.%20It%20is%20shown%20that%20under%20the%20anisotropic%20smoothness%20and%20noise%0Aconditions%2C%20increased%20batch%20size%20does%20not%20slow%20down%20convergence%20for%20Adagrad%2C%0Aand%20thus%20it%20can%20still%20achieve%20a%20faster%20convergence%20guarantee%20over%20SGD%20even%20in%0Athe%20large%20batch%20setting.%20We%20present%20detailed%20comparisons%20between%20SGD%20and%0AAdagrad%20to%20provide%20a%20better%20understanding%20of%20the%20benefits%20of%20adaptive%20gradient%0Amethods.%20Experiments%20in%20logistic%20regression%20and%20instruction%20following%0Afine-tuning%20tasks%20provide%20strong%20evidence%20to%20support%20our%20theoretical%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15244v1&entry.124074799=Read"},
{"title": "Surface Normal Reconstruction Using Polarization-Unet", "author": "F. S. Mortazavi and S. Dajkhosh and M. Saadatseresht", "abstract": "  Today, three-dimensional reconstruction of objects has many applications in\nvarious fields, and therefore, choosing a suitable method for high resolution\nthree-dimensional reconstruction is an important issue and displaying\nhigh-level details in three-dimensional models is a serious challenge in this\nfield. Until now, active methods have been used for high-resolution\nthree-dimensional reconstruction. But the problem of active three-dimensional\nreconstruction methods is that they require a light source close to the object.\nShape from polarization (SfP) is one of the best solutions for high-resolution\nthree-dimensional reconstruction of objects, which is a passive method and does\nnot have the drawbacks of active methods. The changes in polarization of the\nreflected light from an object can be analyzed by using a polarization camera\nor locating polarizing filter in front of the digital camera and rotating the\nfilter. Using this information, the surface normal can be reconstructed with\nhigh accuracy, which will lead to local reconstruction of the surface details.\nIn this paper, an end-to-end deep learning approach has been presented to\nproduce the surface normal of objects. In this method a benchmark dataset has\nbeen used to train the neural network and evaluate the results. The results\nhave been evaluated quantitatively and qualitatively by other methods and under\ndifferent lighting conditions. The MAE value (Mean-Angular-Error) has been used\nfor results evaluation. The evaluations showed that the proposed method could\naccurately reconstruct the surface normal of objects with the lowest MAE value\nwhich is equal to 18.06 degree on the whole dataset, in comparison to previous\nphysics-based methods which are between 41.44 and 49.03 degree.\n", "link": "http://arxiv.org/abs/2406.15118v1", "date": "2024-06-21", "relevancy": 2.445, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.495}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4887}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4833}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Surface%20Normal%20Reconstruction%20Using%20Polarization-Unet&body=Title%3A%20Surface%20Normal%20Reconstruction%20Using%20Polarization-Unet%0AAuthor%3A%20F.%20S.%20Mortazavi%20and%20S.%20Dajkhosh%20and%20M.%20Saadatseresht%0AAbstract%3A%20%20%20Today%2C%20three-dimensional%20reconstruction%20of%20objects%20has%20many%20applications%20in%0Avarious%20fields%2C%20and%20therefore%2C%20choosing%20a%20suitable%20method%20for%20high%20resolution%0Athree-dimensional%20reconstruction%20is%20an%20important%20issue%20and%20displaying%0Ahigh-level%20details%20in%20three-dimensional%20models%20is%20a%20serious%20challenge%20in%20this%0Afield.%20Until%20now%2C%20active%20methods%20have%20been%20used%20for%20high-resolution%0Athree-dimensional%20reconstruction.%20But%20the%20problem%20of%20active%20three-dimensional%0Areconstruction%20methods%20is%20that%20they%20require%20a%20light%20source%20close%20to%20the%20object.%0AShape%20from%20polarization%20%28SfP%29%20is%20one%20of%20the%20best%20solutions%20for%20high-resolution%0Athree-dimensional%20reconstruction%20of%20objects%2C%20which%20is%20a%20passive%20method%20and%20does%0Anot%20have%20the%20drawbacks%20of%20active%20methods.%20The%20changes%20in%20polarization%20of%20the%0Areflected%20light%20from%20an%20object%20can%20be%20analyzed%20by%20using%20a%20polarization%20camera%0Aor%20locating%20polarizing%20filter%20in%20front%20of%20the%20digital%20camera%20and%20rotating%20the%0Afilter.%20Using%20this%20information%2C%20the%20surface%20normal%20can%20be%20reconstructed%20with%0Ahigh%20accuracy%2C%20which%20will%20lead%20to%20local%20reconstruction%20of%20the%20surface%20details.%0AIn%20this%20paper%2C%20an%20end-to-end%20deep%20learning%20approach%20has%20been%20presented%20to%0Aproduce%20the%20surface%20normal%20of%20objects.%20In%20this%20method%20a%20benchmark%20dataset%20has%0Abeen%20used%20to%20train%20the%20neural%20network%20and%20evaluate%20the%20results.%20The%20results%0Ahave%20been%20evaluated%20quantitatively%20and%20qualitatively%20by%20other%20methods%20and%20under%0Adifferent%20lighting%20conditions.%20The%20MAE%20value%20%28Mean-Angular-Error%29%20has%20been%20used%0Afor%20results%20evaluation.%20The%20evaluations%20showed%20that%20the%20proposed%20method%20could%0Aaccurately%20reconstruct%20the%20surface%20normal%20of%20objects%20with%20the%20lowest%20MAE%20value%0Awhich%20is%20equal%20to%2018.06%20degree%20on%20the%20whole%20dataset%2C%20in%20comparison%20to%20previous%0Aphysics-based%20methods%20which%20are%20between%2041.44%20and%2049.03%20degree.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15118v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurface%2520Normal%2520Reconstruction%2520Using%2520Polarization-Unet%26entry.906535625%3DF.%2520S.%2520Mortazavi%2520and%2520S.%2520Dajkhosh%2520and%2520M.%2520Saadatseresht%26entry.1292438233%3D%2520%2520Today%252C%2520three-dimensional%2520reconstruction%2520of%2520objects%2520has%2520many%2520applications%2520in%250Avarious%2520fields%252C%2520and%2520therefore%252C%2520choosing%2520a%2520suitable%2520method%2520for%2520high%2520resolution%250Athree-dimensional%2520reconstruction%2520is%2520an%2520important%2520issue%2520and%2520displaying%250Ahigh-level%2520details%2520in%2520three-dimensional%2520models%2520is%2520a%2520serious%2520challenge%2520in%2520this%250Afield.%2520Until%2520now%252C%2520active%2520methods%2520have%2520been%2520used%2520for%2520high-resolution%250Athree-dimensional%2520reconstruction.%2520But%2520the%2520problem%2520of%2520active%2520three-dimensional%250Areconstruction%2520methods%2520is%2520that%2520they%2520require%2520a%2520light%2520source%2520close%2520to%2520the%2520object.%250AShape%2520from%2520polarization%2520%2528SfP%2529%2520is%2520one%2520of%2520the%2520best%2520solutions%2520for%2520high-resolution%250Athree-dimensional%2520reconstruction%2520of%2520objects%252C%2520which%2520is%2520a%2520passive%2520method%2520and%2520does%250Anot%2520have%2520the%2520drawbacks%2520of%2520active%2520methods.%2520The%2520changes%2520in%2520polarization%2520of%2520the%250Areflected%2520light%2520from%2520an%2520object%2520can%2520be%2520analyzed%2520by%2520using%2520a%2520polarization%2520camera%250Aor%2520locating%2520polarizing%2520filter%2520in%2520front%2520of%2520the%2520digital%2520camera%2520and%2520rotating%2520the%250Afilter.%2520Using%2520this%2520information%252C%2520the%2520surface%2520normal%2520can%2520be%2520reconstructed%2520with%250Ahigh%2520accuracy%252C%2520which%2520will%2520lead%2520to%2520local%2520reconstruction%2520of%2520the%2520surface%2520details.%250AIn%2520this%2520paper%252C%2520an%2520end-to-end%2520deep%2520learning%2520approach%2520has%2520been%2520presented%2520to%250Aproduce%2520the%2520surface%2520normal%2520of%2520objects.%2520In%2520this%2520method%2520a%2520benchmark%2520dataset%2520has%250Abeen%2520used%2520to%2520train%2520the%2520neural%2520network%2520and%2520evaluate%2520the%2520results.%2520The%2520results%250Ahave%2520been%2520evaluated%2520quantitatively%2520and%2520qualitatively%2520by%2520other%2520methods%2520and%2520under%250Adifferent%2520lighting%2520conditions.%2520The%2520MAE%2520value%2520%2528Mean-Angular-Error%2529%2520has%2520been%2520used%250Afor%2520results%2520evaluation.%2520The%2520evaluations%2520showed%2520that%2520the%2520proposed%2520method%2520could%250Aaccurately%2520reconstruct%2520the%2520surface%2520normal%2520of%2520objects%2520with%2520the%2520lowest%2520MAE%2520value%250Awhich%2520is%2520equal%2520to%252018.06%2520degree%2520on%2520the%2520whole%2520dataset%252C%2520in%2520comparison%2520to%2520previous%250Aphysics-based%2520methods%2520which%2520are%2520between%252041.44%2520and%252049.03%2520degree.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15118v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Surface%20Normal%20Reconstruction%20Using%20Polarization-Unet&entry.906535625=F.%20S.%20Mortazavi%20and%20S.%20Dajkhosh%20and%20M.%20Saadatseresht&entry.1292438233=%20%20Today%2C%20three-dimensional%20reconstruction%20of%20objects%20has%20many%20applications%20in%0Avarious%20fields%2C%20and%20therefore%2C%20choosing%20a%20suitable%20method%20for%20high%20resolution%0Athree-dimensional%20reconstruction%20is%20an%20important%20issue%20and%20displaying%0Ahigh-level%20details%20in%20three-dimensional%20models%20is%20a%20serious%20challenge%20in%20this%0Afield.%20Until%20now%2C%20active%20methods%20have%20been%20used%20for%20high-resolution%0Athree-dimensional%20reconstruction.%20But%20the%20problem%20of%20active%20three-dimensional%0Areconstruction%20methods%20is%20that%20they%20require%20a%20light%20source%20close%20to%20the%20object.%0AShape%20from%20polarization%20%28SfP%29%20is%20one%20of%20the%20best%20solutions%20for%20high-resolution%0Athree-dimensional%20reconstruction%20of%20objects%2C%20which%20is%20a%20passive%20method%20and%20does%0Anot%20have%20the%20drawbacks%20of%20active%20methods.%20The%20changes%20in%20polarization%20of%20the%0Areflected%20light%20from%20an%20object%20can%20be%20analyzed%20by%20using%20a%20polarization%20camera%0Aor%20locating%20polarizing%20filter%20in%20front%20of%20the%20digital%20camera%20and%20rotating%20the%0Afilter.%20Using%20this%20information%2C%20the%20surface%20normal%20can%20be%20reconstructed%20with%0Ahigh%20accuracy%2C%20which%20will%20lead%20to%20local%20reconstruction%20of%20the%20surface%20details.%0AIn%20this%20paper%2C%20an%20end-to-end%20deep%20learning%20approach%20has%20been%20presented%20to%0Aproduce%20the%20surface%20normal%20of%20objects.%20In%20this%20method%20a%20benchmark%20dataset%20has%0Abeen%20used%20to%20train%20the%20neural%20network%20and%20evaluate%20the%20results.%20The%20results%0Ahave%20been%20evaluated%20quantitatively%20and%20qualitatively%20by%20other%20methods%20and%20under%0Adifferent%20lighting%20conditions.%20The%20MAE%20value%20%28Mean-Angular-Error%29%20has%20been%20used%0Afor%20results%20evaluation.%20The%20evaluations%20showed%20that%20the%20proposed%20method%20could%0Aaccurately%20reconstruct%20the%20surface%20normal%20of%20objects%20with%20the%20lowest%20MAE%20value%0Awhich%20is%20equal%20to%2018.06%20degree%20on%20the%20whole%20dataset%2C%20in%20comparison%20to%20previous%0Aphysics-based%20methods%20which%20are%20between%2041.44%20and%2049.03%20degree.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15118v1&entry.124074799=Read"},
{"title": "Efficient Perception, Planning, and Control Algorithm for Vision-Based\n  Automated Vehicles", "author": "Der-Hau Lee", "abstract": "  Autonomous vehicles have limited computational resources and thus require\nefficient control systems. The cost and size of sensors have limited the\ndevelopment of self-driving cars. To overcome these restrictions, this study\nproposes an efficient framework for the operation of vision-based automatic\nvehicles; the framework requires only a monocular camera and a few inexpensive\nradars. The proposed algorithm comprises a multi-task UNet (MTUNet) network for\nextracting image features and constrained iterative linear quadratic regulator\n(CILQR) and vision predictive control (VPC) modules for rapid motion planning\nand control. MTUNet is designed to simultaneously solve lane line segmentation,\nthe ego vehicle's heading angle regression, road type classification, and\ntraffic object detection tasks at approximately 40 FPS for 228 x 228 pixel RGB\ninput images. The CILQR controllers then use the MTUNet outputs and radar data\nas inputs to produce driving commands for lateral and longitudinal vehicle\nguidance within only 1 ms. In particular, the VPC algorithm is included to\nreduce steering command latency to below actuator latency, preventing\nperformance degradation during tight turns. The VPC algorithm uses road\ncurvature data from MTUNet to estimate the appropriate correction for the\ncurrent steering angle at a look-ahead point to adjust the turning amount. The\ninclusion of the VPC algorithm in a VPC-CILQR controller leads to higher\nperformance on curvy roads than the use of CILQR alone. Our experiments\ndemonstrate that the proposed autonomous driving system, which does not require\nhigh-definition maps, can be applied in current autonomous vehicles.\n", "link": "http://arxiv.org/abs/2209.07042v6", "date": "2024-06-21", "relevancy": 2.4433, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6375}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6057}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Perception%2C%20Planning%2C%20and%20Control%20Algorithm%20for%20Vision-Based%0A%20%20Automated%20Vehicles&body=Title%3A%20Efficient%20Perception%2C%20Planning%2C%20and%20Control%20Algorithm%20for%20Vision-Based%0A%20%20Automated%20Vehicles%0AAuthor%3A%20Der-Hau%20Lee%0AAbstract%3A%20%20%20Autonomous%20vehicles%20have%20limited%20computational%20resources%20and%20thus%20require%0Aefficient%20control%20systems.%20The%20cost%20and%20size%20of%20sensors%20have%20limited%20the%0Adevelopment%20of%20self-driving%20cars.%20To%20overcome%20these%20restrictions%2C%20this%20study%0Aproposes%20an%20efficient%20framework%20for%20the%20operation%20of%20vision-based%20automatic%0Avehicles%3B%20the%20framework%20requires%20only%20a%20monocular%20camera%20and%20a%20few%20inexpensive%0Aradars.%20The%20proposed%20algorithm%20comprises%20a%20multi-task%20UNet%20%28MTUNet%29%20network%20for%0Aextracting%20image%20features%20and%20constrained%20iterative%20linear%20quadratic%20regulator%0A%28CILQR%29%20and%20vision%20predictive%20control%20%28VPC%29%20modules%20for%20rapid%20motion%20planning%0Aand%20control.%20MTUNet%20is%20designed%20to%20simultaneously%20solve%20lane%20line%20segmentation%2C%0Athe%20ego%20vehicle%27s%20heading%20angle%20regression%2C%20road%20type%20classification%2C%20and%0Atraffic%20object%20detection%20tasks%20at%20approximately%2040%20FPS%20for%20228%20x%20228%20pixel%20RGB%0Ainput%20images.%20The%20CILQR%20controllers%20then%20use%20the%20MTUNet%20outputs%20and%20radar%20data%0Aas%20inputs%20to%20produce%20driving%20commands%20for%20lateral%20and%20longitudinal%20vehicle%0Aguidance%20within%20only%201%20ms.%20In%20particular%2C%20the%20VPC%20algorithm%20is%20included%20to%0Areduce%20steering%20command%20latency%20to%20below%20actuator%20latency%2C%20preventing%0Aperformance%20degradation%20during%20tight%20turns.%20The%20VPC%20algorithm%20uses%20road%0Acurvature%20data%20from%20MTUNet%20to%20estimate%20the%20appropriate%20correction%20for%20the%0Acurrent%20steering%20angle%20at%20a%20look-ahead%20point%20to%20adjust%20the%20turning%20amount.%20The%0Ainclusion%20of%20the%20VPC%20algorithm%20in%20a%20VPC-CILQR%20controller%20leads%20to%20higher%0Aperformance%20on%20curvy%20roads%20than%20the%20use%20of%20CILQR%20alone.%20Our%20experiments%0Ademonstrate%20that%20the%20proposed%20autonomous%20driving%20system%2C%20which%20does%20not%20require%0Ahigh-definition%20maps%2C%20can%20be%20applied%20in%20current%20autonomous%20vehicles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.07042v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Perception%252C%2520Planning%252C%2520and%2520Control%2520Algorithm%2520for%2520Vision-Based%250A%2520%2520Automated%2520Vehicles%26entry.906535625%3DDer-Hau%2520Lee%26entry.1292438233%3D%2520%2520Autonomous%2520vehicles%2520have%2520limited%2520computational%2520resources%2520and%2520thus%2520require%250Aefficient%2520control%2520systems.%2520The%2520cost%2520and%2520size%2520of%2520sensors%2520have%2520limited%2520the%250Adevelopment%2520of%2520self-driving%2520cars.%2520To%2520overcome%2520these%2520restrictions%252C%2520this%2520study%250Aproposes%2520an%2520efficient%2520framework%2520for%2520the%2520operation%2520of%2520vision-based%2520automatic%250Avehicles%253B%2520the%2520framework%2520requires%2520only%2520a%2520monocular%2520camera%2520and%2520a%2520few%2520inexpensive%250Aradars.%2520The%2520proposed%2520algorithm%2520comprises%2520a%2520multi-task%2520UNet%2520%2528MTUNet%2529%2520network%2520for%250Aextracting%2520image%2520features%2520and%2520constrained%2520iterative%2520linear%2520quadratic%2520regulator%250A%2528CILQR%2529%2520and%2520vision%2520predictive%2520control%2520%2528VPC%2529%2520modules%2520for%2520rapid%2520motion%2520planning%250Aand%2520control.%2520MTUNet%2520is%2520designed%2520to%2520simultaneously%2520solve%2520lane%2520line%2520segmentation%252C%250Athe%2520ego%2520vehicle%2527s%2520heading%2520angle%2520regression%252C%2520road%2520type%2520classification%252C%2520and%250Atraffic%2520object%2520detection%2520tasks%2520at%2520approximately%252040%2520FPS%2520for%2520228%2520x%2520228%2520pixel%2520RGB%250Ainput%2520images.%2520The%2520CILQR%2520controllers%2520then%2520use%2520the%2520MTUNet%2520outputs%2520and%2520radar%2520data%250Aas%2520inputs%2520to%2520produce%2520driving%2520commands%2520for%2520lateral%2520and%2520longitudinal%2520vehicle%250Aguidance%2520within%2520only%25201%2520ms.%2520In%2520particular%252C%2520the%2520VPC%2520algorithm%2520is%2520included%2520to%250Areduce%2520steering%2520command%2520latency%2520to%2520below%2520actuator%2520latency%252C%2520preventing%250Aperformance%2520degradation%2520during%2520tight%2520turns.%2520The%2520VPC%2520algorithm%2520uses%2520road%250Acurvature%2520data%2520from%2520MTUNet%2520to%2520estimate%2520the%2520appropriate%2520correction%2520for%2520the%250Acurrent%2520steering%2520angle%2520at%2520a%2520look-ahead%2520point%2520to%2520adjust%2520the%2520turning%2520amount.%2520The%250Ainclusion%2520of%2520the%2520VPC%2520algorithm%2520in%2520a%2520VPC-CILQR%2520controller%2520leads%2520to%2520higher%250Aperformance%2520on%2520curvy%2520roads%2520than%2520the%2520use%2520of%2520CILQR%2520alone.%2520Our%2520experiments%250Ademonstrate%2520that%2520the%2520proposed%2520autonomous%2520driving%2520system%252C%2520which%2520does%2520not%2520require%250Ahigh-definition%2520maps%252C%2520can%2520be%2520applied%2520in%2520current%2520autonomous%2520vehicles.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2209.07042v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Perception%2C%20Planning%2C%20and%20Control%20Algorithm%20for%20Vision-Based%0A%20%20Automated%20Vehicles&entry.906535625=Der-Hau%20Lee&entry.1292438233=%20%20Autonomous%20vehicles%20have%20limited%20computational%20resources%20and%20thus%20require%0Aefficient%20control%20systems.%20The%20cost%20and%20size%20of%20sensors%20have%20limited%20the%0Adevelopment%20of%20self-driving%20cars.%20To%20overcome%20these%20restrictions%2C%20this%20study%0Aproposes%20an%20efficient%20framework%20for%20the%20operation%20of%20vision-based%20automatic%0Avehicles%3B%20the%20framework%20requires%20only%20a%20monocular%20camera%20and%20a%20few%20inexpensive%0Aradars.%20The%20proposed%20algorithm%20comprises%20a%20multi-task%20UNet%20%28MTUNet%29%20network%20for%0Aextracting%20image%20features%20and%20constrained%20iterative%20linear%20quadratic%20regulator%0A%28CILQR%29%20and%20vision%20predictive%20control%20%28VPC%29%20modules%20for%20rapid%20motion%20planning%0Aand%20control.%20MTUNet%20is%20designed%20to%20simultaneously%20solve%20lane%20line%20segmentation%2C%0Athe%20ego%20vehicle%27s%20heading%20angle%20regression%2C%20road%20type%20classification%2C%20and%0Atraffic%20object%20detection%20tasks%20at%20approximately%2040%20FPS%20for%20228%20x%20228%20pixel%20RGB%0Ainput%20images.%20The%20CILQR%20controllers%20then%20use%20the%20MTUNet%20outputs%20and%20radar%20data%0Aas%20inputs%20to%20produce%20driving%20commands%20for%20lateral%20and%20longitudinal%20vehicle%0Aguidance%20within%20only%201%20ms.%20In%20particular%2C%20the%20VPC%20algorithm%20is%20included%20to%0Areduce%20steering%20command%20latency%20to%20below%20actuator%20latency%2C%20preventing%0Aperformance%20degradation%20during%20tight%20turns.%20The%20VPC%20algorithm%20uses%20road%0Acurvature%20data%20from%20MTUNet%20to%20estimate%20the%20appropriate%20correction%20for%20the%0Acurrent%20steering%20angle%20at%20a%20look-ahead%20point%20to%20adjust%20the%20turning%20amount.%20The%0Ainclusion%20of%20the%20VPC%20algorithm%20in%20a%20VPC-CILQR%20controller%20leads%20to%20higher%0Aperformance%20on%20curvy%20roads%20than%20the%20use%20of%20CILQR%20alone.%20Our%20experiments%0Ademonstrate%20that%20the%20proposed%20autonomous%20driving%20system%2C%20which%20does%20not%20require%0Ahigh-definition%20maps%2C%20can%20be%20applied%20in%20current%20autonomous%20vehicles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.07042v6&entry.124074799=Read"},
{"title": "Landscape More Secure Than Portrait? Zooming Into the Directionality of\n  Digital Images With Security Implications", "author": "Benedikt Lorch and Rainer B\u00f6hme", "abstract": "  The orientation in which a source image is captured can affect the resulting\nsecurity in downstream applications. One reason for this is that many\nstate-of-the-art methods in media security assume that image statistics are\nsimilar in the horizontal and vertical directions, allowing them to reduce the\nnumber of features (or trainable weights) by merging coefficients. We show that\nthis artificial symmetrization tends to suppress important properties of\nnatural images and common processing operations, causing a loss of performance.\nWe also observe the opposite problem, where unaddressed directionality causes\nlearning-based methods to overfit to a single orientation. These are vulnerable\nto manipulation if an adversary chooses inputs with the less common\norientation. This paper takes a comprehensive approach, identifies and\nsystematizes causes of directionality at several stages of a typical\nacquisition pipeline, measures their effect, and demonstrates for three\nselected security applications (steganalysis, forensic source identification,\nand the detection of synthetic images) how the performance of state-of-the-art\nmethods can be improved by properly accounting for directionality.\n", "link": "http://arxiv.org/abs/2406.15206v1", "date": "2024-06-21", "relevancy": 2.4383, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5086}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4809}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4735}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Landscape%20More%20Secure%20Than%20Portrait%3F%20Zooming%20Into%20the%20Directionality%20of%0A%20%20Digital%20Images%20With%20Security%20Implications&body=Title%3A%20Landscape%20More%20Secure%20Than%20Portrait%3F%20Zooming%20Into%20the%20Directionality%20of%0A%20%20Digital%20Images%20With%20Security%20Implications%0AAuthor%3A%20Benedikt%20Lorch%20and%20Rainer%20B%C3%B6hme%0AAbstract%3A%20%20%20The%20orientation%20in%20which%20a%20source%20image%20is%20captured%20can%20affect%20the%20resulting%0Asecurity%20in%20downstream%20applications.%20One%20reason%20for%20this%20is%20that%20many%0Astate-of-the-art%20methods%20in%20media%20security%20assume%20that%20image%20statistics%20are%0Asimilar%20in%20the%20horizontal%20and%20vertical%20directions%2C%20allowing%20them%20to%20reduce%20the%0Anumber%20of%20features%20%28or%20trainable%20weights%29%20by%20merging%20coefficients.%20We%20show%20that%0Athis%20artificial%20symmetrization%20tends%20to%20suppress%20important%20properties%20of%0Anatural%20images%20and%20common%20processing%20operations%2C%20causing%20a%20loss%20of%20performance.%0AWe%20also%20observe%20the%20opposite%20problem%2C%20where%20unaddressed%20directionality%20causes%0Alearning-based%20methods%20to%20overfit%20to%20a%20single%20orientation.%20These%20are%20vulnerable%0Ato%20manipulation%20if%20an%20adversary%20chooses%20inputs%20with%20the%20less%20common%0Aorientation.%20This%20paper%20takes%20a%20comprehensive%20approach%2C%20identifies%20and%0Asystematizes%20causes%20of%20directionality%20at%20several%20stages%20of%20a%20typical%0Aacquisition%20pipeline%2C%20measures%20their%20effect%2C%20and%20demonstrates%20for%20three%0Aselected%20security%20applications%20%28steganalysis%2C%20forensic%20source%20identification%2C%0Aand%20the%20detection%20of%20synthetic%20images%29%20how%20the%20performance%20of%20state-of-the-art%0Amethods%20can%20be%20improved%20by%20properly%20accounting%20for%20directionality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15206v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLandscape%2520More%2520Secure%2520Than%2520Portrait%253F%2520Zooming%2520Into%2520the%2520Directionality%2520of%250A%2520%2520Digital%2520Images%2520With%2520Security%2520Implications%26entry.906535625%3DBenedikt%2520Lorch%2520and%2520Rainer%2520B%25C3%25B6hme%26entry.1292438233%3D%2520%2520The%2520orientation%2520in%2520which%2520a%2520source%2520image%2520is%2520captured%2520can%2520affect%2520the%2520resulting%250Asecurity%2520in%2520downstream%2520applications.%2520One%2520reason%2520for%2520this%2520is%2520that%2520many%250Astate-of-the-art%2520methods%2520in%2520media%2520security%2520assume%2520that%2520image%2520statistics%2520are%250Asimilar%2520in%2520the%2520horizontal%2520and%2520vertical%2520directions%252C%2520allowing%2520them%2520to%2520reduce%2520the%250Anumber%2520of%2520features%2520%2528or%2520trainable%2520weights%2529%2520by%2520merging%2520coefficients.%2520We%2520show%2520that%250Athis%2520artificial%2520symmetrization%2520tends%2520to%2520suppress%2520important%2520properties%2520of%250Anatural%2520images%2520and%2520common%2520processing%2520operations%252C%2520causing%2520a%2520loss%2520of%2520performance.%250AWe%2520also%2520observe%2520the%2520opposite%2520problem%252C%2520where%2520unaddressed%2520directionality%2520causes%250Alearning-based%2520methods%2520to%2520overfit%2520to%2520a%2520single%2520orientation.%2520These%2520are%2520vulnerable%250Ato%2520manipulation%2520if%2520an%2520adversary%2520chooses%2520inputs%2520with%2520the%2520less%2520common%250Aorientation.%2520This%2520paper%2520takes%2520a%2520comprehensive%2520approach%252C%2520identifies%2520and%250Asystematizes%2520causes%2520of%2520directionality%2520at%2520several%2520stages%2520of%2520a%2520typical%250Aacquisition%2520pipeline%252C%2520measures%2520their%2520effect%252C%2520and%2520demonstrates%2520for%2520three%250Aselected%2520security%2520applications%2520%2528steganalysis%252C%2520forensic%2520source%2520identification%252C%250Aand%2520the%2520detection%2520of%2520synthetic%2520images%2529%2520how%2520the%2520performance%2520of%2520state-of-the-art%250Amethods%2520can%2520be%2520improved%2520by%2520properly%2520accounting%2520for%2520directionality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15206v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Landscape%20More%20Secure%20Than%20Portrait%3F%20Zooming%20Into%20the%20Directionality%20of%0A%20%20Digital%20Images%20With%20Security%20Implications&entry.906535625=Benedikt%20Lorch%20and%20Rainer%20B%C3%B6hme&entry.1292438233=%20%20The%20orientation%20in%20which%20a%20source%20image%20is%20captured%20can%20affect%20the%20resulting%0Asecurity%20in%20downstream%20applications.%20One%20reason%20for%20this%20is%20that%20many%0Astate-of-the-art%20methods%20in%20media%20security%20assume%20that%20image%20statistics%20are%0Asimilar%20in%20the%20horizontal%20and%20vertical%20directions%2C%20allowing%20them%20to%20reduce%20the%0Anumber%20of%20features%20%28or%20trainable%20weights%29%20by%20merging%20coefficients.%20We%20show%20that%0Athis%20artificial%20symmetrization%20tends%20to%20suppress%20important%20properties%20of%0Anatural%20images%20and%20common%20processing%20operations%2C%20causing%20a%20loss%20of%20performance.%0AWe%20also%20observe%20the%20opposite%20problem%2C%20where%20unaddressed%20directionality%20causes%0Alearning-based%20methods%20to%20overfit%20to%20a%20single%20orientation.%20These%20are%20vulnerable%0Ato%20manipulation%20if%20an%20adversary%20chooses%20inputs%20with%20the%20less%20common%0Aorientation.%20This%20paper%20takes%20a%20comprehensive%20approach%2C%20identifies%20and%0Asystematizes%20causes%20of%20directionality%20at%20several%20stages%20of%20a%20typical%0Aacquisition%20pipeline%2C%20measures%20their%20effect%2C%20and%20demonstrates%20for%20three%0Aselected%20security%20applications%20%28steganalysis%2C%20forensic%20source%20identification%2C%0Aand%20the%20detection%20of%20synthetic%20images%29%20how%20the%20performance%20of%20state-of-the-art%0Amethods%20can%20be%20improved%20by%20properly%20accounting%20for%20directionality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15206v1&entry.124074799=Read"},
{"title": "Masked Extended Attention for Zero-Shot Virtual Try-On In The Wild", "author": "Nadav Orzech and Yotam Nitzan and Ulysse Mizrahi and Dov Danon and Amit H. Bermano", "abstract": "  Virtual Try-On (VTON) is a highly active line of research, with increasing\ndemand. It aims to replace a piece of garment in an image with one from\nanother, while preserving person and garment characteristics as well as image\nfidelity. Current literature takes a supervised approach for the task,\nimpairing generalization and imposing heavy computation. In this paper, we\npresent a novel zero-shot training-free method for inpainting a clothing\ngarment by reference. Our approach employs the prior of a diffusion model with\nno additional training, fully leveraging its native generalization\ncapabilities. The method employs extended attention to transfer image\ninformation from reference to target images, overcoming two significant\nchallenges. We first initially warp the reference garment over the target human\nusing deep features, alleviating \"texture sticking\". We then leverage the\nextended attention mechanism with careful masking, eliminating leakage of\nreference background and unwanted influence. Through a user study, qualitative,\nand quantitative comparison to state-of-the-art approaches, we demonstrate\nsuperior image quality and garment preservation compared unseen clothing pieces\nor human figures.\n", "link": "http://arxiv.org/abs/2406.15331v1", "date": "2024-06-21", "relevancy": 2.4291, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6442}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6172}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Masked%20Extended%20Attention%20for%20Zero-Shot%20Virtual%20Try-On%20In%20The%20Wild&body=Title%3A%20Masked%20Extended%20Attention%20for%20Zero-Shot%20Virtual%20Try-On%20In%20The%20Wild%0AAuthor%3A%20Nadav%20Orzech%20and%20Yotam%20Nitzan%20and%20Ulysse%20Mizrahi%20and%20Dov%20Danon%20and%20Amit%20H.%20Bermano%0AAbstract%3A%20%20%20Virtual%20Try-On%20%28VTON%29%20is%20a%20highly%20active%20line%20of%20research%2C%20with%20increasing%0Ademand.%20It%20aims%20to%20replace%20a%20piece%20of%20garment%20in%20an%20image%20with%20one%20from%0Aanother%2C%20while%20preserving%20person%20and%20garment%20characteristics%20as%20well%20as%20image%0Afidelity.%20Current%20literature%20takes%20a%20supervised%20approach%20for%20the%20task%2C%0Aimpairing%20generalization%20and%20imposing%20heavy%20computation.%20In%20this%20paper%2C%20we%0Apresent%20a%20novel%20zero-shot%20training-free%20method%20for%20inpainting%20a%20clothing%0Agarment%20by%20reference.%20Our%20approach%20employs%20the%20prior%20of%20a%20diffusion%20model%20with%0Ano%20additional%20training%2C%20fully%20leveraging%20its%20native%20generalization%0Acapabilities.%20The%20method%20employs%20extended%20attention%20to%20transfer%20image%0Ainformation%20from%20reference%20to%20target%20images%2C%20overcoming%20two%20significant%0Achallenges.%20We%20first%20initially%20warp%20the%20reference%20garment%20over%20the%20target%20human%0Ausing%20deep%20features%2C%20alleviating%20%22texture%20sticking%22.%20We%20then%20leverage%20the%0Aextended%20attention%20mechanism%20with%20careful%20masking%2C%20eliminating%20leakage%20of%0Areference%20background%20and%20unwanted%20influence.%20Through%20a%20user%20study%2C%20qualitative%2C%0Aand%20quantitative%20comparison%20to%20state-of-the-art%20approaches%2C%20we%20demonstrate%0Asuperior%20image%20quality%20and%20garment%20preservation%20compared%20unseen%20clothing%20pieces%0Aor%20human%20figures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15331v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMasked%2520Extended%2520Attention%2520for%2520Zero-Shot%2520Virtual%2520Try-On%2520In%2520The%2520Wild%26entry.906535625%3DNadav%2520Orzech%2520and%2520Yotam%2520Nitzan%2520and%2520Ulysse%2520Mizrahi%2520and%2520Dov%2520Danon%2520and%2520Amit%2520H.%2520Bermano%26entry.1292438233%3D%2520%2520Virtual%2520Try-On%2520%2528VTON%2529%2520is%2520a%2520highly%2520active%2520line%2520of%2520research%252C%2520with%2520increasing%250Ademand.%2520It%2520aims%2520to%2520replace%2520a%2520piece%2520of%2520garment%2520in%2520an%2520image%2520with%2520one%2520from%250Aanother%252C%2520while%2520preserving%2520person%2520and%2520garment%2520characteristics%2520as%2520well%2520as%2520image%250Afidelity.%2520Current%2520literature%2520takes%2520a%2520supervised%2520approach%2520for%2520the%2520task%252C%250Aimpairing%2520generalization%2520and%2520imposing%2520heavy%2520computation.%2520In%2520this%2520paper%252C%2520we%250Apresent%2520a%2520novel%2520zero-shot%2520training-free%2520method%2520for%2520inpainting%2520a%2520clothing%250Agarment%2520by%2520reference.%2520Our%2520approach%2520employs%2520the%2520prior%2520of%2520a%2520diffusion%2520model%2520with%250Ano%2520additional%2520training%252C%2520fully%2520leveraging%2520its%2520native%2520generalization%250Acapabilities.%2520The%2520method%2520employs%2520extended%2520attention%2520to%2520transfer%2520image%250Ainformation%2520from%2520reference%2520to%2520target%2520images%252C%2520overcoming%2520two%2520significant%250Achallenges.%2520We%2520first%2520initially%2520warp%2520the%2520reference%2520garment%2520over%2520the%2520target%2520human%250Ausing%2520deep%2520features%252C%2520alleviating%2520%2522texture%2520sticking%2522.%2520We%2520then%2520leverage%2520the%250Aextended%2520attention%2520mechanism%2520with%2520careful%2520masking%252C%2520eliminating%2520leakage%2520of%250Areference%2520background%2520and%2520unwanted%2520influence.%2520Through%2520a%2520user%2520study%252C%2520qualitative%252C%250Aand%2520quantitative%2520comparison%2520to%2520state-of-the-art%2520approaches%252C%2520we%2520demonstrate%250Asuperior%2520image%2520quality%2520and%2520garment%2520preservation%2520compared%2520unseen%2520clothing%2520pieces%250Aor%2520human%2520figures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15331v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Masked%20Extended%20Attention%20for%20Zero-Shot%20Virtual%20Try-On%20In%20The%20Wild&entry.906535625=Nadav%20Orzech%20and%20Yotam%20Nitzan%20and%20Ulysse%20Mizrahi%20and%20Dov%20Danon%20and%20Amit%20H.%20Bermano&entry.1292438233=%20%20Virtual%20Try-On%20%28VTON%29%20is%20a%20highly%20active%20line%20of%20research%2C%20with%20increasing%0Ademand.%20It%20aims%20to%20replace%20a%20piece%20of%20garment%20in%20an%20image%20with%20one%20from%0Aanother%2C%20while%20preserving%20person%20and%20garment%20characteristics%20as%20well%20as%20image%0Afidelity.%20Current%20literature%20takes%20a%20supervised%20approach%20for%20the%20task%2C%0Aimpairing%20generalization%20and%20imposing%20heavy%20computation.%20In%20this%20paper%2C%20we%0Apresent%20a%20novel%20zero-shot%20training-free%20method%20for%20inpainting%20a%20clothing%0Agarment%20by%20reference.%20Our%20approach%20employs%20the%20prior%20of%20a%20diffusion%20model%20with%0Ano%20additional%20training%2C%20fully%20leveraging%20its%20native%20generalization%0Acapabilities.%20The%20method%20employs%20extended%20attention%20to%20transfer%20image%0Ainformation%20from%20reference%20to%20target%20images%2C%20overcoming%20two%20significant%0Achallenges.%20We%20first%20initially%20warp%20the%20reference%20garment%20over%20the%20target%20human%0Ausing%20deep%20features%2C%20alleviating%20%22texture%20sticking%22.%20We%20then%20leverage%20the%0Aextended%20attention%20mechanism%20with%20careful%20masking%2C%20eliminating%20leakage%20of%0Areference%20background%20and%20unwanted%20influence.%20Through%20a%20user%20study%2C%20qualitative%2C%0Aand%20quantitative%20comparison%20to%20state-of-the-art%20approaches%2C%20we%20demonstrate%0Asuperior%20image%20quality%20and%20garment%20preservation%20compared%20unseen%20clothing%20pieces%0Aor%20human%20figures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15331v1&entry.124074799=Read"},
{"title": "Gaussian Splatting to Real World Flight Navigation Transfer with Liquid\n  Networks", "author": "Alex Quach and Makram Chahine and Alexander Amini and Ramin Hasani and Daniela Rus", "abstract": "  Simulators are powerful tools for autonomous robot learning as they offer\nscalable data generation, flexible design, and optimization of trajectories.\nHowever, transferring behavior learned from simulation data into the real world\nproves to be difficult, usually mitigated with compute-heavy domain\nrandomization methods or further model fine-tuning. We present a method to\nimprove generalization and robustness to distribution shifts in sim-to-real\nvisual quadrotor navigation tasks. To this end, we first build a simulator by\nintegrating Gaussian Splatting with quadrotor flight dynamics, and then, train\nrobust navigation policies using Liquid neural networks. In this way, we obtain\na full-stack imitation learning protocol that combines advances in 3D Gaussian\nsplatting radiance field rendering, crafty programming of expert demonstration\ntraining data, and the task understanding capabilities of Liquid networks.\nThrough a series of quantitative flight tests, we demonstrate the robust\ntransfer of navigation skills learned in a single simulation scene directly to\nthe real world. We further show the ability to maintain performance beyond the\ntraining environment under drastic distribution and physical environment\nchanges. Our learned Liquid policies, trained on single target manoeuvres\ncurated from a photorealistic simulated indoor flight only, generalize to\nmulti-step hikes onboard a real hardware platform outdoors.\n", "link": "http://arxiv.org/abs/2406.15149v1", "date": "2024-06-21", "relevancy": 2.3871, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6228}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5787}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Splatting%20to%20Real%20World%20Flight%20Navigation%20Transfer%20with%20Liquid%0A%20%20Networks&body=Title%3A%20Gaussian%20Splatting%20to%20Real%20World%20Flight%20Navigation%20Transfer%20with%20Liquid%0A%20%20Networks%0AAuthor%3A%20Alex%20Quach%20and%20Makram%20Chahine%20and%20Alexander%20Amini%20and%20Ramin%20Hasani%20and%20Daniela%20Rus%0AAbstract%3A%20%20%20Simulators%20are%20powerful%20tools%20for%20autonomous%20robot%20learning%20as%20they%20offer%0Ascalable%20data%20generation%2C%20flexible%20design%2C%20and%20optimization%20of%20trajectories.%0AHowever%2C%20transferring%20behavior%20learned%20from%20simulation%20data%20into%20the%20real%20world%0Aproves%20to%20be%20difficult%2C%20usually%20mitigated%20with%20compute-heavy%20domain%0Arandomization%20methods%20or%20further%20model%20fine-tuning.%20We%20present%20a%20method%20to%0Aimprove%20generalization%20and%20robustness%20to%20distribution%20shifts%20in%20sim-to-real%0Avisual%20quadrotor%20navigation%20tasks.%20To%20this%20end%2C%20we%20first%20build%20a%20simulator%20by%0Aintegrating%20Gaussian%20Splatting%20with%20quadrotor%20flight%20dynamics%2C%20and%20then%2C%20train%0Arobust%20navigation%20policies%20using%20Liquid%20neural%20networks.%20In%20this%20way%2C%20we%20obtain%0Aa%20full-stack%20imitation%20learning%20protocol%20that%20combines%20advances%20in%203D%20Gaussian%0Asplatting%20radiance%20field%20rendering%2C%20crafty%20programming%20of%20expert%20demonstration%0Atraining%20data%2C%20and%20the%20task%20understanding%20capabilities%20of%20Liquid%20networks.%0AThrough%20a%20series%20of%20quantitative%20flight%20tests%2C%20we%20demonstrate%20the%20robust%0Atransfer%20of%20navigation%20skills%20learned%20in%20a%20single%20simulation%20scene%20directly%20to%0Athe%20real%20world.%20We%20further%20show%20the%20ability%20to%20maintain%20performance%20beyond%20the%0Atraining%20environment%20under%20drastic%20distribution%20and%20physical%20environment%0Achanges.%20Our%20learned%20Liquid%20policies%2C%20trained%20on%20single%20target%20manoeuvres%0Acurated%20from%20a%20photorealistic%20simulated%20indoor%20flight%20only%2C%20generalize%20to%0Amulti-step%20hikes%20onboard%20a%20real%20hardware%20platform%20outdoors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15149v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Splatting%2520to%2520Real%2520World%2520Flight%2520Navigation%2520Transfer%2520with%2520Liquid%250A%2520%2520Networks%26entry.906535625%3DAlex%2520Quach%2520and%2520Makram%2520Chahine%2520and%2520Alexander%2520Amini%2520and%2520Ramin%2520Hasani%2520and%2520Daniela%2520Rus%26entry.1292438233%3D%2520%2520Simulators%2520are%2520powerful%2520tools%2520for%2520autonomous%2520robot%2520learning%2520as%2520they%2520offer%250Ascalable%2520data%2520generation%252C%2520flexible%2520design%252C%2520and%2520optimization%2520of%2520trajectories.%250AHowever%252C%2520transferring%2520behavior%2520learned%2520from%2520simulation%2520data%2520into%2520the%2520real%2520world%250Aproves%2520to%2520be%2520difficult%252C%2520usually%2520mitigated%2520with%2520compute-heavy%2520domain%250Arandomization%2520methods%2520or%2520further%2520model%2520fine-tuning.%2520We%2520present%2520a%2520method%2520to%250Aimprove%2520generalization%2520and%2520robustness%2520to%2520distribution%2520shifts%2520in%2520sim-to-real%250Avisual%2520quadrotor%2520navigation%2520tasks.%2520To%2520this%2520end%252C%2520we%2520first%2520build%2520a%2520simulator%2520by%250Aintegrating%2520Gaussian%2520Splatting%2520with%2520quadrotor%2520flight%2520dynamics%252C%2520and%2520then%252C%2520train%250Arobust%2520navigation%2520policies%2520using%2520Liquid%2520neural%2520networks.%2520In%2520this%2520way%252C%2520we%2520obtain%250Aa%2520full-stack%2520imitation%2520learning%2520protocol%2520that%2520combines%2520advances%2520in%25203D%2520Gaussian%250Asplatting%2520radiance%2520field%2520rendering%252C%2520crafty%2520programming%2520of%2520expert%2520demonstration%250Atraining%2520data%252C%2520and%2520the%2520task%2520understanding%2520capabilities%2520of%2520Liquid%2520networks.%250AThrough%2520a%2520series%2520of%2520quantitative%2520flight%2520tests%252C%2520we%2520demonstrate%2520the%2520robust%250Atransfer%2520of%2520navigation%2520skills%2520learned%2520in%2520a%2520single%2520simulation%2520scene%2520directly%2520to%250Athe%2520real%2520world.%2520We%2520further%2520show%2520the%2520ability%2520to%2520maintain%2520performance%2520beyond%2520the%250Atraining%2520environment%2520under%2520drastic%2520distribution%2520and%2520physical%2520environment%250Achanges.%2520Our%2520learned%2520Liquid%2520policies%252C%2520trained%2520on%2520single%2520target%2520manoeuvres%250Acurated%2520from%2520a%2520photorealistic%2520simulated%2520indoor%2520flight%2520only%252C%2520generalize%2520to%250Amulti-step%2520hikes%2520onboard%2520a%2520real%2520hardware%2520platform%2520outdoors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15149v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Splatting%20to%20Real%20World%20Flight%20Navigation%20Transfer%20with%20Liquid%0A%20%20Networks&entry.906535625=Alex%20Quach%20and%20Makram%20Chahine%20and%20Alexander%20Amini%20and%20Ramin%20Hasani%20and%20Daniela%20Rus&entry.1292438233=%20%20Simulators%20are%20powerful%20tools%20for%20autonomous%20robot%20learning%20as%20they%20offer%0Ascalable%20data%20generation%2C%20flexible%20design%2C%20and%20optimization%20of%20trajectories.%0AHowever%2C%20transferring%20behavior%20learned%20from%20simulation%20data%20into%20the%20real%20world%0Aproves%20to%20be%20difficult%2C%20usually%20mitigated%20with%20compute-heavy%20domain%0Arandomization%20methods%20or%20further%20model%20fine-tuning.%20We%20present%20a%20method%20to%0Aimprove%20generalization%20and%20robustness%20to%20distribution%20shifts%20in%20sim-to-real%0Avisual%20quadrotor%20navigation%20tasks.%20To%20this%20end%2C%20we%20first%20build%20a%20simulator%20by%0Aintegrating%20Gaussian%20Splatting%20with%20quadrotor%20flight%20dynamics%2C%20and%20then%2C%20train%0Arobust%20navigation%20policies%20using%20Liquid%20neural%20networks.%20In%20this%20way%2C%20we%20obtain%0Aa%20full-stack%20imitation%20learning%20protocol%20that%20combines%20advances%20in%203D%20Gaussian%0Asplatting%20radiance%20field%20rendering%2C%20crafty%20programming%20of%20expert%20demonstration%0Atraining%20data%2C%20and%20the%20task%20understanding%20capabilities%20of%20Liquid%20networks.%0AThrough%20a%20series%20of%20quantitative%20flight%20tests%2C%20we%20demonstrate%20the%20robust%0Atransfer%20of%20navigation%20skills%20learned%20in%20a%20single%20simulation%20scene%20directly%20to%0Athe%20real%20world.%20We%20further%20show%20the%20ability%20to%20maintain%20performance%20beyond%20the%0Atraining%20environment%20under%20drastic%20distribution%20and%20physical%20environment%0Achanges.%20Our%20learned%20Liquid%20policies%2C%20trained%20on%20single%20target%20manoeuvres%0Acurated%20from%20a%20photorealistic%20simulated%20indoor%20flight%20only%2C%20generalize%20to%0Amulti-step%20hikes%20onboard%20a%20real%20hardware%20platform%20outdoors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15149v1&entry.124074799=Read"},
{"title": "Unsupervised Morphological Tree Tokenizer", "author": "Qingyang Zhu and Xiang Hu and Pengyu Ji and Wei Wu and Kewei Tu", "abstract": "  As a cornerstone in language modeling, tokenization involves segmenting text\ninputs into pre-defined atomic units. Conventional statistical tokenizers often\ndisrupt constituent boundaries within words, thereby corrupting semantic\ninformation. To address this drawback, we introduce morphological structure\nguidance to tokenization and propose a deep model to induce character-level\nstructures of words. Specifically, the deep model jointly encodes internal\nstructures and representations of words with a mechanism named\n$\\textit{MorphOverriding}$ to ensure the indecomposability of morphemes. By\ntraining the model with self-supervised objectives, our method is capable of\ninducing character-level structures that align with morphological rules without\nannotated training data. Based on the induced structures, our algorithm\ntokenizes words through vocabulary matching in a top-down manner. Empirical\nresults indicate that the proposed method effectively retains complete\nmorphemes and outperforms widely adopted methods such as BPE and WordPiece on\nboth morphological segmentation tasks and language modeling tasks. The code\nwill be released later.\n", "link": "http://arxiv.org/abs/2406.15245v1", "date": "2024-06-21", "relevancy": 2.3742, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4973}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4677}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4595}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Morphological%20Tree%20Tokenizer&body=Title%3A%20Unsupervised%20Morphological%20Tree%20Tokenizer%0AAuthor%3A%20Qingyang%20Zhu%20and%20Xiang%20Hu%20and%20Pengyu%20Ji%20and%20Wei%20Wu%20and%20Kewei%20Tu%0AAbstract%3A%20%20%20As%20a%20cornerstone%20in%20language%20modeling%2C%20tokenization%20involves%20segmenting%20text%0Ainputs%20into%20pre-defined%20atomic%20units.%20Conventional%20statistical%20tokenizers%20often%0Adisrupt%20constituent%20boundaries%20within%20words%2C%20thereby%20corrupting%20semantic%0Ainformation.%20To%20address%20this%20drawback%2C%20we%20introduce%20morphological%20structure%0Aguidance%20to%20tokenization%20and%20propose%20a%20deep%20model%20to%20induce%20character-level%0Astructures%20of%20words.%20Specifically%2C%20the%20deep%20model%20jointly%20encodes%20internal%0Astructures%20and%20representations%20of%20words%20with%20a%20mechanism%20named%0A%24%5Ctextit%7BMorphOverriding%7D%24%20to%20ensure%20the%20indecomposability%20of%20morphemes.%20By%0Atraining%20the%20model%20with%20self-supervised%20objectives%2C%20our%20method%20is%20capable%20of%0Ainducing%20character-level%20structures%20that%20align%20with%20morphological%20rules%20without%0Aannotated%20training%20data.%20Based%20on%20the%20induced%20structures%2C%20our%20algorithm%0Atokenizes%20words%20through%20vocabulary%20matching%20in%20a%20top-down%20manner.%20Empirical%0Aresults%20indicate%20that%20the%20proposed%20method%20effectively%20retains%20complete%0Amorphemes%20and%20outperforms%20widely%20adopted%20methods%20such%20as%20BPE%20and%20WordPiece%20on%0Aboth%20morphological%20segmentation%20tasks%20and%20language%20modeling%20tasks.%20The%20code%0Awill%20be%20released%20later.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15245v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Morphological%2520Tree%2520Tokenizer%26entry.906535625%3DQingyang%2520Zhu%2520and%2520Xiang%2520Hu%2520and%2520Pengyu%2520Ji%2520and%2520Wei%2520Wu%2520and%2520Kewei%2520Tu%26entry.1292438233%3D%2520%2520As%2520a%2520cornerstone%2520in%2520language%2520modeling%252C%2520tokenization%2520involves%2520segmenting%2520text%250Ainputs%2520into%2520pre-defined%2520atomic%2520units.%2520Conventional%2520statistical%2520tokenizers%2520often%250Adisrupt%2520constituent%2520boundaries%2520within%2520words%252C%2520thereby%2520corrupting%2520semantic%250Ainformation.%2520To%2520address%2520this%2520drawback%252C%2520we%2520introduce%2520morphological%2520structure%250Aguidance%2520to%2520tokenization%2520and%2520propose%2520a%2520deep%2520model%2520to%2520induce%2520character-level%250Astructures%2520of%2520words.%2520Specifically%252C%2520the%2520deep%2520model%2520jointly%2520encodes%2520internal%250Astructures%2520and%2520representations%2520of%2520words%2520with%2520a%2520mechanism%2520named%250A%2524%255Ctextit%257BMorphOverriding%257D%2524%2520to%2520ensure%2520the%2520indecomposability%2520of%2520morphemes.%2520By%250Atraining%2520the%2520model%2520with%2520self-supervised%2520objectives%252C%2520our%2520method%2520is%2520capable%2520of%250Ainducing%2520character-level%2520structures%2520that%2520align%2520with%2520morphological%2520rules%2520without%250Aannotated%2520training%2520data.%2520Based%2520on%2520the%2520induced%2520structures%252C%2520our%2520algorithm%250Atokenizes%2520words%2520through%2520vocabulary%2520matching%2520in%2520a%2520top-down%2520manner.%2520Empirical%250Aresults%2520indicate%2520that%2520the%2520proposed%2520method%2520effectively%2520retains%2520complete%250Amorphemes%2520and%2520outperforms%2520widely%2520adopted%2520methods%2520such%2520as%2520BPE%2520and%2520WordPiece%2520on%250Aboth%2520morphological%2520segmentation%2520tasks%2520and%2520language%2520modeling%2520tasks.%2520The%2520code%250Awill%2520be%2520released%2520later.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15245v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Morphological%20Tree%20Tokenizer&entry.906535625=Qingyang%20Zhu%20and%20Xiang%20Hu%20and%20Pengyu%20Ji%20and%20Wei%20Wu%20and%20Kewei%20Tu&entry.1292438233=%20%20As%20a%20cornerstone%20in%20language%20modeling%2C%20tokenization%20involves%20segmenting%20text%0Ainputs%20into%20pre-defined%20atomic%20units.%20Conventional%20statistical%20tokenizers%20often%0Adisrupt%20constituent%20boundaries%20within%20words%2C%20thereby%20corrupting%20semantic%0Ainformation.%20To%20address%20this%20drawback%2C%20we%20introduce%20morphological%20structure%0Aguidance%20to%20tokenization%20and%20propose%20a%20deep%20model%20to%20induce%20character-level%0Astructures%20of%20words.%20Specifically%2C%20the%20deep%20model%20jointly%20encodes%20internal%0Astructures%20and%20representations%20of%20words%20with%20a%20mechanism%20named%0A%24%5Ctextit%7BMorphOverriding%7D%24%20to%20ensure%20the%20indecomposability%20of%20morphemes.%20By%0Atraining%20the%20model%20with%20self-supervised%20objectives%2C%20our%20method%20is%20capable%20of%0Ainducing%20character-level%20structures%20that%20align%20with%20morphological%20rules%20without%0Aannotated%20training%20data.%20Based%20on%20the%20induced%20structures%2C%20our%20algorithm%0Atokenizes%20words%20through%20vocabulary%20matching%20in%20a%20top-down%20manner.%20Empirical%0Aresults%20indicate%20that%20the%20proposed%20method%20effectively%20retains%20complete%0Amorphemes%20and%20outperforms%20widely%20adopted%20methods%20such%20as%20BPE%20and%20WordPiece%20on%0Aboth%20morphological%20segmentation%20tasks%20and%20language%20modeling%20tasks.%20The%20code%0Awill%20be%20released%20later.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15245v1&entry.124074799=Read"},
{"title": "Fingerprint Membership and Identity Inference Against Generative\n  Adversarial Networks", "author": "Saverio Cavasin and Daniele Mari and Simone Milani and Mauro Conti", "abstract": "  Generative models are gaining significant attention as potential catalysts\nfor a novel industrial revolution. Since automated sample generation can be\nuseful to solve privacy and data scarcity issues that usually affect learned\nbiometric models, such technologies became widely spread in this field. In this\npaper, we assess the vulnerabilities of generative machine learning models\nconcerning identity protection by designing and testing an identity inference\nattack on fingerprint datasets created by means of a generative adversarial\nnetwork. Experimental results show that the proposed solution proves to be\neffective under different configurations and easily extendable to other\nbiometric measurements.\n", "link": "http://arxiv.org/abs/2406.15253v1", "date": "2024-06-21", "relevancy": 2.3695, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.489}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4676}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4651}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fingerprint%20Membership%20and%20Identity%20Inference%20Against%20Generative%0A%20%20Adversarial%20Networks&body=Title%3A%20Fingerprint%20Membership%20and%20Identity%20Inference%20Against%20Generative%0A%20%20Adversarial%20Networks%0AAuthor%3A%20Saverio%20Cavasin%20and%20Daniele%20Mari%20and%20Simone%20Milani%20and%20Mauro%20Conti%0AAbstract%3A%20%20%20Generative%20models%20are%20gaining%20significant%20attention%20as%20potential%20catalysts%0Afor%20a%20novel%20industrial%20revolution.%20Since%20automated%20sample%20generation%20can%20be%0Auseful%20to%20solve%20privacy%20and%20data%20scarcity%20issues%20that%20usually%20affect%20learned%0Abiometric%20models%2C%20such%20technologies%20became%20widely%20spread%20in%20this%20field.%20In%20this%0Apaper%2C%20we%20assess%20the%20vulnerabilities%20of%20generative%20machine%20learning%20models%0Aconcerning%20identity%20protection%20by%20designing%20and%20testing%20an%20identity%20inference%0Aattack%20on%20fingerprint%20datasets%20created%20by%20means%20of%20a%20generative%20adversarial%0Anetwork.%20Experimental%20results%20show%20that%20the%20proposed%20solution%20proves%20to%20be%0Aeffective%20under%20different%20configurations%20and%20easily%20extendable%20to%20other%0Abiometric%20measurements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15253v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFingerprint%2520Membership%2520and%2520Identity%2520Inference%2520Against%2520Generative%250A%2520%2520Adversarial%2520Networks%26entry.906535625%3DSaverio%2520Cavasin%2520and%2520Daniele%2520Mari%2520and%2520Simone%2520Milani%2520and%2520Mauro%2520Conti%26entry.1292438233%3D%2520%2520Generative%2520models%2520are%2520gaining%2520significant%2520attention%2520as%2520potential%2520catalysts%250Afor%2520a%2520novel%2520industrial%2520revolution.%2520Since%2520automated%2520sample%2520generation%2520can%2520be%250Auseful%2520to%2520solve%2520privacy%2520and%2520data%2520scarcity%2520issues%2520that%2520usually%2520affect%2520learned%250Abiometric%2520models%252C%2520such%2520technologies%2520became%2520widely%2520spread%2520in%2520this%2520field.%2520In%2520this%250Apaper%252C%2520we%2520assess%2520the%2520vulnerabilities%2520of%2520generative%2520machine%2520learning%2520models%250Aconcerning%2520identity%2520protection%2520by%2520designing%2520and%2520testing%2520an%2520identity%2520inference%250Aattack%2520on%2520fingerprint%2520datasets%2520created%2520by%2520means%2520of%2520a%2520generative%2520adversarial%250Anetwork.%2520Experimental%2520results%2520show%2520that%2520the%2520proposed%2520solution%2520proves%2520to%2520be%250Aeffective%2520under%2520different%2520configurations%2520and%2520easily%2520extendable%2520to%2520other%250Abiometric%2520measurements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15253v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fingerprint%20Membership%20and%20Identity%20Inference%20Against%20Generative%0A%20%20Adversarial%20Networks&entry.906535625=Saverio%20Cavasin%20and%20Daniele%20Mari%20and%20Simone%20Milani%20and%20Mauro%20Conti&entry.1292438233=%20%20Generative%20models%20are%20gaining%20significant%20attention%20as%20potential%20catalysts%0Afor%20a%20novel%20industrial%20revolution.%20Since%20automated%20sample%20generation%20can%20be%0Auseful%20to%20solve%20privacy%20and%20data%20scarcity%20issues%20that%20usually%20affect%20learned%0Abiometric%20models%2C%20such%20technologies%20became%20widely%20spread%20in%20this%20field.%20In%20this%0Apaper%2C%20we%20assess%20the%20vulnerabilities%20of%20generative%20machine%20learning%20models%0Aconcerning%20identity%20protection%20by%20designing%20and%20testing%20an%20identity%20inference%0Aattack%20on%20fingerprint%20datasets%20created%20by%20means%20of%20a%20generative%20adversarial%0Anetwork.%20Experimental%20results%20show%20that%20the%20proposed%20solution%20proves%20to%20be%0Aeffective%20under%20different%20configurations%20and%20easily%20extendable%20to%20other%0Abiometric%20measurements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15253v1&entry.124074799=Read"},
{"title": "Detecting Synthetic Lyrics with Few-Shot Inference", "author": "Yanis Labrak and Gabriel Meseguer-Brocal and Elena V. Epure", "abstract": "  In recent years, generated content in music has gained significant\npopularity, with large language models being effectively utilized to produce\nhuman-like lyrics in various styles, themes, and linguistic structures. This\ntechnological advancement supports artists in their creative processes but also\nraises issues of authorship infringement, consumer satisfaction and content\nspamming. To address these challenges, methods for detecting generated lyrics\nare necessary. However, existing works have not yet focused on this specific\nmodality or on creative text in general regarding machine-generated content\ndetection methods and datasets. In response, we have curated the first dataset\nof high-quality synthetic lyrics and conducted a comprehensive quantitative\nevaluation of various few-shot content detection approaches, testing their\ngeneralization capabilities and complementing this with a human evaluation. Our\nbest few-shot detector, based on LLM2Vec, surpasses stylistic and statistical\nmethods, which are shown competitive in other domains at distinguishing\nhuman-written from machine-generated content. It also shows good generalization\ncapabilities to new artists and models, and effectively detects post-generation\nparaphrasing. This study emphasizes the need for further research on creative\ncontent detection, particularly in terms of generalization and scalability with\nlarger song catalogs. All datasets, pre-processing scripts, and code are\navailable publicly on GitHub and Hugging Face under the Apache 2.0 license.\n", "link": "http://arxiv.org/abs/2406.15231v1", "date": "2024-06-21", "relevancy": 2.3371, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4785}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4638}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20Synthetic%20Lyrics%20with%20Few-Shot%20Inference&body=Title%3A%20Detecting%20Synthetic%20Lyrics%20with%20Few-Shot%20Inference%0AAuthor%3A%20Yanis%20Labrak%20and%20Gabriel%20Meseguer-Brocal%20and%20Elena%20V.%20Epure%0AAbstract%3A%20%20%20In%20recent%20years%2C%20generated%20content%20in%20music%20has%20gained%20significant%0Apopularity%2C%20with%20large%20language%20models%20being%20effectively%20utilized%20to%20produce%0Ahuman-like%20lyrics%20in%20various%20styles%2C%20themes%2C%20and%20linguistic%20structures.%20This%0Atechnological%20advancement%20supports%20artists%20in%20their%20creative%20processes%20but%20also%0Araises%20issues%20of%20authorship%20infringement%2C%20consumer%20satisfaction%20and%20content%0Aspamming.%20To%20address%20these%20challenges%2C%20methods%20for%20detecting%20generated%20lyrics%0Aare%20necessary.%20However%2C%20existing%20works%20have%20not%20yet%20focused%20on%20this%20specific%0Amodality%20or%20on%20creative%20text%20in%20general%20regarding%20machine-generated%20content%0Adetection%20methods%20and%20datasets.%20In%20response%2C%20we%20have%20curated%20the%20first%20dataset%0Aof%20high-quality%20synthetic%20lyrics%20and%20conducted%20a%20comprehensive%20quantitative%0Aevaluation%20of%20various%20few-shot%20content%20detection%20approaches%2C%20testing%20their%0Ageneralization%20capabilities%20and%20complementing%20this%20with%20a%20human%20evaluation.%20Our%0Abest%20few-shot%20detector%2C%20based%20on%20LLM2Vec%2C%20surpasses%20stylistic%20and%20statistical%0Amethods%2C%20which%20are%20shown%20competitive%20in%20other%20domains%20at%20distinguishing%0Ahuman-written%20from%20machine-generated%20content.%20It%20also%20shows%20good%20generalization%0Acapabilities%20to%20new%20artists%20and%20models%2C%20and%20effectively%20detects%20post-generation%0Aparaphrasing.%20This%20study%20emphasizes%20the%20need%20for%20further%20research%20on%20creative%0Acontent%20detection%2C%20particularly%20in%20terms%20of%20generalization%20and%20scalability%20with%0Alarger%20song%20catalogs.%20All%20datasets%2C%20pre-processing%20scripts%2C%20and%20code%20are%0Aavailable%20publicly%20on%20GitHub%20and%20Hugging%20Face%20under%20the%20Apache%202.0%20license.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15231v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520Synthetic%2520Lyrics%2520with%2520Few-Shot%2520Inference%26entry.906535625%3DYanis%2520Labrak%2520and%2520Gabriel%2520Meseguer-Brocal%2520and%2520Elena%2520V.%2520Epure%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520generated%2520content%2520in%2520music%2520has%2520gained%2520significant%250Apopularity%252C%2520with%2520large%2520language%2520models%2520being%2520effectively%2520utilized%2520to%2520produce%250Ahuman-like%2520lyrics%2520in%2520various%2520styles%252C%2520themes%252C%2520and%2520linguistic%2520structures.%2520This%250Atechnological%2520advancement%2520supports%2520artists%2520in%2520their%2520creative%2520processes%2520but%2520also%250Araises%2520issues%2520of%2520authorship%2520infringement%252C%2520consumer%2520satisfaction%2520and%2520content%250Aspamming.%2520To%2520address%2520these%2520challenges%252C%2520methods%2520for%2520detecting%2520generated%2520lyrics%250Aare%2520necessary.%2520However%252C%2520existing%2520works%2520have%2520not%2520yet%2520focused%2520on%2520this%2520specific%250Amodality%2520or%2520on%2520creative%2520text%2520in%2520general%2520regarding%2520machine-generated%2520content%250Adetection%2520methods%2520and%2520datasets.%2520In%2520response%252C%2520we%2520have%2520curated%2520the%2520first%2520dataset%250Aof%2520high-quality%2520synthetic%2520lyrics%2520and%2520conducted%2520a%2520comprehensive%2520quantitative%250Aevaluation%2520of%2520various%2520few-shot%2520content%2520detection%2520approaches%252C%2520testing%2520their%250Ageneralization%2520capabilities%2520and%2520complementing%2520this%2520with%2520a%2520human%2520evaluation.%2520Our%250Abest%2520few-shot%2520detector%252C%2520based%2520on%2520LLM2Vec%252C%2520surpasses%2520stylistic%2520and%2520statistical%250Amethods%252C%2520which%2520are%2520shown%2520competitive%2520in%2520other%2520domains%2520at%2520distinguishing%250Ahuman-written%2520from%2520machine-generated%2520content.%2520It%2520also%2520shows%2520good%2520generalization%250Acapabilities%2520to%2520new%2520artists%2520and%2520models%252C%2520and%2520effectively%2520detects%2520post-generation%250Aparaphrasing.%2520This%2520study%2520emphasizes%2520the%2520need%2520for%2520further%2520research%2520on%2520creative%250Acontent%2520detection%252C%2520particularly%2520in%2520terms%2520of%2520generalization%2520and%2520scalability%2520with%250Alarger%2520song%2520catalogs.%2520All%2520datasets%252C%2520pre-processing%2520scripts%252C%2520and%2520code%2520are%250Aavailable%2520publicly%2520on%2520GitHub%2520and%2520Hugging%2520Face%2520under%2520the%2520Apache%25202.0%2520license.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15231v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20Synthetic%20Lyrics%20with%20Few-Shot%20Inference&entry.906535625=Yanis%20Labrak%20and%20Gabriel%20Meseguer-Brocal%20and%20Elena%20V.%20Epure&entry.1292438233=%20%20In%20recent%20years%2C%20generated%20content%20in%20music%20has%20gained%20significant%0Apopularity%2C%20with%20large%20language%20models%20being%20effectively%20utilized%20to%20produce%0Ahuman-like%20lyrics%20in%20various%20styles%2C%20themes%2C%20and%20linguistic%20structures.%20This%0Atechnological%20advancement%20supports%20artists%20in%20their%20creative%20processes%20but%20also%0Araises%20issues%20of%20authorship%20infringement%2C%20consumer%20satisfaction%20and%20content%0Aspamming.%20To%20address%20these%20challenges%2C%20methods%20for%20detecting%20generated%20lyrics%0Aare%20necessary.%20However%2C%20existing%20works%20have%20not%20yet%20focused%20on%20this%20specific%0Amodality%20or%20on%20creative%20text%20in%20general%20regarding%20machine-generated%20content%0Adetection%20methods%20and%20datasets.%20In%20response%2C%20we%20have%20curated%20the%20first%20dataset%0Aof%20high-quality%20synthetic%20lyrics%20and%20conducted%20a%20comprehensive%20quantitative%0Aevaluation%20of%20various%20few-shot%20content%20detection%20approaches%2C%20testing%20their%0Ageneralization%20capabilities%20and%20complementing%20this%20with%20a%20human%20evaluation.%20Our%0Abest%20few-shot%20detector%2C%20based%20on%20LLM2Vec%2C%20surpasses%20stylistic%20and%20statistical%0Amethods%2C%20which%20are%20shown%20competitive%20in%20other%20domains%20at%20distinguishing%0Ahuman-written%20from%20machine-generated%20content.%20It%20also%20shows%20good%20generalization%0Acapabilities%20to%20new%20artists%20and%20models%2C%20and%20effectively%20detects%20post-generation%0Aparaphrasing.%20This%20study%20emphasizes%20the%20need%20for%20further%20research%20on%20creative%0Acontent%20detection%2C%20particularly%20in%20terms%20of%20generalization%20and%20scalability%20with%0Alarger%20song%20catalogs.%20All%20datasets%2C%20pre-processing%20scripts%2C%20and%20code%20are%0Aavailable%20publicly%20on%20GitHub%20and%20Hugging%20Face%20under%20the%20Apache%202.0%20license.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15231v1&entry.124074799=Read"},
{"title": "Perks and Pitfalls of Faithfulness in Regular, Self-Explainable and\n  Domain Invariant GNNs", "author": "Steve Azzolin and Antonio Longa and Stefano Teso and Andrea Passerini", "abstract": "  As Graph Neural Networks (GNNs) become more pervasive, it becomes paramount\nto build robust tools for computing explanations of their predictions. A key\ndesideratum is that these explanations are faithful, i.e., that they portray an\naccurate picture of the GNN's reasoning process. A number of different\nfaithfulness metrics exist, begging the question of what faithfulness is\nexactly, and what its properties are. We begin by showing that existing metrics\nare not interchangeable -- i.e., explanations attaining high faithfulness\naccording to one metric may be unfaithful according to others -- and can be\nsystematically insensitive to important properties of the explanation, and\nsuggest how to address these issues. We proceed to show that, surprisingly,\noptimizing for faithfulness is not always a sensible design goal. Specifically,\nwe show that for injective regular GNN architectures, perfectly faithful\nexplanations are completely uninformative. The situation is different for\nmodular GNNs, such as self-explainable and domain-invariant architectures,\nwhere optimizing faithfulness does not compromise informativeness, and is also\nunexpectedly tied to out-of-distribution generalization.\n", "link": "http://arxiv.org/abs/2406.15156v1", "date": "2024-06-21", "relevancy": 2.3004, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4742}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4559}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perks%20and%20Pitfalls%20of%20Faithfulness%20in%20Regular%2C%20Self-Explainable%20and%0A%20%20Domain%20Invariant%20GNNs&body=Title%3A%20Perks%20and%20Pitfalls%20of%20Faithfulness%20in%20Regular%2C%20Self-Explainable%20and%0A%20%20Domain%20Invariant%20GNNs%0AAuthor%3A%20Steve%20Azzolin%20and%20Antonio%20Longa%20and%20Stefano%20Teso%20and%20Andrea%20Passerini%0AAbstract%3A%20%20%20As%20Graph%20Neural%20Networks%20%28GNNs%29%20become%20more%20pervasive%2C%20it%20becomes%20paramount%0Ato%20build%20robust%20tools%20for%20computing%20explanations%20of%20their%20predictions.%20A%20key%0Adesideratum%20is%20that%20these%20explanations%20are%20faithful%2C%20i.e.%2C%20that%20they%20portray%20an%0Aaccurate%20picture%20of%20the%20GNN%27s%20reasoning%20process.%20A%20number%20of%20different%0Afaithfulness%20metrics%20exist%2C%20begging%20the%20question%20of%20what%20faithfulness%20is%0Aexactly%2C%20and%20what%20its%20properties%20are.%20We%20begin%20by%20showing%20that%20existing%20metrics%0Aare%20not%20interchangeable%20--%20i.e.%2C%20explanations%20attaining%20high%20faithfulness%0Aaccording%20to%20one%20metric%20may%20be%20unfaithful%20according%20to%20others%20--%20and%20can%20be%0Asystematically%20insensitive%20to%20important%20properties%20of%20the%20explanation%2C%20and%0Asuggest%20how%20to%20address%20these%20issues.%20We%20proceed%20to%20show%20that%2C%20surprisingly%2C%0Aoptimizing%20for%20faithfulness%20is%20not%20always%20a%20sensible%20design%20goal.%20Specifically%2C%0Awe%20show%20that%20for%20injective%20regular%20GNN%20architectures%2C%20perfectly%20faithful%0Aexplanations%20are%20completely%20uninformative.%20The%20situation%20is%20different%20for%0Amodular%20GNNs%2C%20such%20as%20self-explainable%20and%20domain-invariant%20architectures%2C%0Awhere%20optimizing%20faithfulness%20does%20not%20compromise%20informativeness%2C%20and%20is%20also%0Aunexpectedly%20tied%20to%20out-of-distribution%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15156v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerks%2520and%2520Pitfalls%2520of%2520Faithfulness%2520in%2520Regular%252C%2520Self-Explainable%2520and%250A%2520%2520Domain%2520Invariant%2520GNNs%26entry.906535625%3DSteve%2520Azzolin%2520and%2520Antonio%2520Longa%2520and%2520Stefano%2520Teso%2520and%2520Andrea%2520Passerini%26entry.1292438233%3D%2520%2520As%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520become%2520more%2520pervasive%252C%2520it%2520becomes%2520paramount%250Ato%2520build%2520robust%2520tools%2520for%2520computing%2520explanations%2520of%2520their%2520predictions.%2520A%2520key%250Adesideratum%2520is%2520that%2520these%2520explanations%2520are%2520faithful%252C%2520i.e.%252C%2520that%2520they%2520portray%2520an%250Aaccurate%2520picture%2520of%2520the%2520GNN%2527s%2520reasoning%2520process.%2520A%2520number%2520of%2520different%250Afaithfulness%2520metrics%2520exist%252C%2520begging%2520the%2520question%2520of%2520what%2520faithfulness%2520is%250Aexactly%252C%2520and%2520what%2520its%2520properties%2520are.%2520We%2520begin%2520by%2520showing%2520that%2520existing%2520metrics%250Aare%2520not%2520interchangeable%2520--%2520i.e.%252C%2520explanations%2520attaining%2520high%2520faithfulness%250Aaccording%2520to%2520one%2520metric%2520may%2520be%2520unfaithful%2520according%2520to%2520others%2520--%2520and%2520can%2520be%250Asystematically%2520insensitive%2520to%2520important%2520properties%2520of%2520the%2520explanation%252C%2520and%250Asuggest%2520how%2520to%2520address%2520these%2520issues.%2520We%2520proceed%2520to%2520show%2520that%252C%2520surprisingly%252C%250Aoptimizing%2520for%2520faithfulness%2520is%2520not%2520always%2520a%2520sensible%2520design%2520goal.%2520Specifically%252C%250Awe%2520show%2520that%2520for%2520injective%2520regular%2520GNN%2520architectures%252C%2520perfectly%2520faithful%250Aexplanations%2520are%2520completely%2520uninformative.%2520The%2520situation%2520is%2520different%2520for%250Amodular%2520GNNs%252C%2520such%2520as%2520self-explainable%2520and%2520domain-invariant%2520architectures%252C%250Awhere%2520optimizing%2520faithfulness%2520does%2520not%2520compromise%2520informativeness%252C%2520and%2520is%2520also%250Aunexpectedly%2520tied%2520to%2520out-of-distribution%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15156v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perks%20and%20Pitfalls%20of%20Faithfulness%20in%20Regular%2C%20Self-Explainable%20and%0A%20%20Domain%20Invariant%20GNNs&entry.906535625=Steve%20Azzolin%20and%20Antonio%20Longa%20and%20Stefano%20Teso%20and%20Andrea%20Passerini&entry.1292438233=%20%20As%20Graph%20Neural%20Networks%20%28GNNs%29%20become%20more%20pervasive%2C%20it%20becomes%20paramount%0Ato%20build%20robust%20tools%20for%20computing%20explanations%20of%20their%20predictions.%20A%20key%0Adesideratum%20is%20that%20these%20explanations%20are%20faithful%2C%20i.e.%2C%20that%20they%20portray%20an%0Aaccurate%20picture%20of%20the%20GNN%27s%20reasoning%20process.%20A%20number%20of%20different%0Afaithfulness%20metrics%20exist%2C%20begging%20the%20question%20of%20what%20faithfulness%20is%0Aexactly%2C%20and%20what%20its%20properties%20are.%20We%20begin%20by%20showing%20that%20existing%20metrics%0Aare%20not%20interchangeable%20--%20i.e.%2C%20explanations%20attaining%20high%20faithfulness%0Aaccording%20to%20one%20metric%20may%20be%20unfaithful%20according%20to%20others%20--%20and%20can%20be%0Asystematically%20insensitive%20to%20important%20properties%20of%20the%20explanation%2C%20and%0Asuggest%20how%20to%20address%20these%20issues.%20We%20proceed%20to%20show%20that%2C%20surprisingly%2C%0Aoptimizing%20for%20faithfulness%20is%20not%20always%20a%20sensible%20design%20goal.%20Specifically%2C%0Awe%20show%20that%20for%20injective%20regular%20GNN%20architectures%2C%20perfectly%20faithful%0Aexplanations%20are%20completely%20uninformative.%20The%20situation%20is%20different%20for%0Amodular%20GNNs%2C%20such%20as%20self-explainable%20and%20domain-invariant%20architectures%2C%0Awhere%20optimizing%20faithfulness%20does%20not%20compromise%20informativeness%2C%20and%20is%20also%0Aunexpectedly%20tied%20to%20out-of-distribution%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15156v1&entry.124074799=Read"},
{"title": "Latent Space Translation via Inverse Relative Projection", "author": "Valentino Maiorca and Luca Moschella and Marco Fumero and Francesco Locatello and Emanuele Rodol\u00e0", "abstract": "  The emergence of similar representations between independently trained neural\nmodels has sparked significant interest in the representation learning\ncommunity, leading to the development of various methods to obtain\ncommunication between latent spaces. \"Latent space communication\" can be\nachieved in two ways: i) by independently mapping the original spaces to a\nshared or relative one; ii) by directly estimating a transformation from a\nsource latent space to a target one. In this work, we combine the two into a\nnovel method to obtain latent space translation through the relative space. By\nformalizing the invertibility of angle-preserving relative representations and\nassuming the scale invariance of decoder modules in neural models, we can\neffectively use the relative space as an intermediary, independently projecting\nonto and from other semantically similar spaces. Extensive experiments over\nvarious architectures and datasets validate our scale invariance assumption and\ndemonstrate the high accuracy of our method in latent space translation. We\nalso apply our method to zero-shot stitching between arbitrary pre-trained text\nand image encoders and their classifiers, even across modalities. Our method\nhas significant potential for facilitating the reuse of models in a practical\nmanner via compositionality.\n", "link": "http://arxiv.org/abs/2406.15057v1", "date": "2024-06-21", "relevancy": 2.2796, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5723}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.572}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latent%20Space%20Translation%20via%20Inverse%20Relative%20Projection&body=Title%3A%20Latent%20Space%20Translation%20via%20Inverse%20Relative%20Projection%0AAuthor%3A%20Valentino%20Maiorca%20and%20Luca%20Moschella%20and%20Marco%20Fumero%20and%20Francesco%20Locatello%20and%20Emanuele%20Rodol%C3%A0%0AAbstract%3A%20%20%20The%20emergence%20of%20similar%20representations%20between%20independently%20trained%20neural%0Amodels%20has%20sparked%20significant%20interest%20in%20the%20representation%20learning%0Acommunity%2C%20leading%20to%20the%20development%20of%20various%20methods%20to%20obtain%0Acommunication%20between%20latent%20spaces.%20%22Latent%20space%20communication%22%20can%20be%0Aachieved%20in%20two%20ways%3A%20i%29%20by%20independently%20mapping%20the%20original%20spaces%20to%20a%0Ashared%20or%20relative%20one%3B%20ii%29%20by%20directly%20estimating%20a%20transformation%20from%20a%0Asource%20latent%20space%20to%20a%20target%20one.%20In%20this%20work%2C%20we%20combine%20the%20two%20into%20a%0Anovel%20method%20to%20obtain%20latent%20space%20translation%20through%20the%20relative%20space.%20By%0Aformalizing%20the%20invertibility%20of%20angle-preserving%20relative%20representations%20and%0Aassuming%20the%20scale%20invariance%20of%20decoder%20modules%20in%20neural%20models%2C%20we%20can%0Aeffectively%20use%20the%20relative%20space%20as%20an%20intermediary%2C%20independently%20projecting%0Aonto%20and%20from%20other%20semantically%20similar%20spaces.%20Extensive%20experiments%20over%0Avarious%20architectures%20and%20datasets%20validate%20our%20scale%20invariance%20assumption%20and%0Ademonstrate%20the%20high%20accuracy%20of%20our%20method%20in%20latent%20space%20translation.%20We%0Aalso%20apply%20our%20method%20to%20zero-shot%20stitching%20between%20arbitrary%20pre-trained%20text%0Aand%20image%20encoders%20and%20their%20classifiers%2C%20even%20across%20modalities.%20Our%20method%0Ahas%20significant%20potential%20for%20facilitating%20the%20reuse%20of%20models%20in%20a%20practical%0Amanner%20via%20compositionality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15057v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatent%2520Space%2520Translation%2520via%2520Inverse%2520Relative%2520Projection%26entry.906535625%3DValentino%2520Maiorca%2520and%2520Luca%2520Moschella%2520and%2520Marco%2520Fumero%2520and%2520Francesco%2520Locatello%2520and%2520Emanuele%2520Rodol%25C3%25A0%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520similar%2520representations%2520between%2520independently%2520trained%2520neural%250Amodels%2520has%2520sparked%2520significant%2520interest%2520in%2520the%2520representation%2520learning%250Acommunity%252C%2520leading%2520to%2520the%2520development%2520of%2520various%2520methods%2520to%2520obtain%250Acommunication%2520between%2520latent%2520spaces.%2520%2522Latent%2520space%2520communication%2522%2520can%2520be%250Aachieved%2520in%2520two%2520ways%253A%2520i%2529%2520by%2520independently%2520mapping%2520the%2520original%2520spaces%2520to%2520a%250Ashared%2520or%2520relative%2520one%253B%2520ii%2529%2520by%2520directly%2520estimating%2520a%2520transformation%2520from%2520a%250Asource%2520latent%2520space%2520to%2520a%2520target%2520one.%2520In%2520this%2520work%252C%2520we%2520combine%2520the%2520two%2520into%2520a%250Anovel%2520method%2520to%2520obtain%2520latent%2520space%2520translation%2520through%2520the%2520relative%2520space.%2520By%250Aformalizing%2520the%2520invertibility%2520of%2520angle-preserving%2520relative%2520representations%2520and%250Aassuming%2520the%2520scale%2520invariance%2520of%2520decoder%2520modules%2520in%2520neural%2520models%252C%2520we%2520can%250Aeffectively%2520use%2520the%2520relative%2520space%2520as%2520an%2520intermediary%252C%2520independently%2520projecting%250Aonto%2520and%2520from%2520other%2520semantically%2520similar%2520spaces.%2520Extensive%2520experiments%2520over%250Avarious%2520architectures%2520and%2520datasets%2520validate%2520our%2520scale%2520invariance%2520assumption%2520and%250Ademonstrate%2520the%2520high%2520accuracy%2520of%2520our%2520method%2520in%2520latent%2520space%2520translation.%2520We%250Aalso%2520apply%2520our%2520method%2520to%2520zero-shot%2520stitching%2520between%2520arbitrary%2520pre-trained%2520text%250Aand%2520image%2520encoders%2520and%2520their%2520classifiers%252C%2520even%2520across%2520modalities.%2520Our%2520method%250Ahas%2520significant%2520potential%2520for%2520facilitating%2520the%2520reuse%2520of%2520models%2520in%2520a%2520practical%250Amanner%2520via%2520compositionality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15057v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent%20Space%20Translation%20via%20Inverse%20Relative%20Projection&entry.906535625=Valentino%20Maiorca%20and%20Luca%20Moschella%20and%20Marco%20Fumero%20and%20Francesco%20Locatello%20and%20Emanuele%20Rodol%C3%A0&entry.1292438233=%20%20The%20emergence%20of%20similar%20representations%20between%20independently%20trained%20neural%0Amodels%20has%20sparked%20significant%20interest%20in%20the%20representation%20learning%0Acommunity%2C%20leading%20to%20the%20development%20of%20various%20methods%20to%20obtain%0Acommunication%20between%20latent%20spaces.%20%22Latent%20space%20communication%22%20can%20be%0Aachieved%20in%20two%20ways%3A%20i%29%20by%20independently%20mapping%20the%20original%20spaces%20to%20a%0Ashared%20or%20relative%20one%3B%20ii%29%20by%20directly%20estimating%20a%20transformation%20from%20a%0Asource%20latent%20space%20to%20a%20target%20one.%20In%20this%20work%2C%20we%20combine%20the%20two%20into%20a%0Anovel%20method%20to%20obtain%20latent%20space%20translation%20through%20the%20relative%20space.%20By%0Aformalizing%20the%20invertibility%20of%20angle-preserving%20relative%20representations%20and%0Aassuming%20the%20scale%20invariance%20of%20decoder%20modules%20in%20neural%20models%2C%20we%20can%0Aeffectively%20use%20the%20relative%20space%20as%20an%20intermediary%2C%20independently%20projecting%0Aonto%20and%20from%20other%20semantically%20similar%20spaces.%20Extensive%20experiments%20over%0Avarious%20architectures%20and%20datasets%20validate%20our%20scale%20invariance%20assumption%20and%0Ademonstrate%20the%20high%20accuracy%20of%20our%20method%20in%20latent%20space%20translation.%20We%0Aalso%20apply%20our%20method%20to%20zero-shot%20stitching%20between%20arbitrary%20pre-trained%20text%0Aand%20image%20encoders%20and%20their%20classifiers%2C%20even%20across%20modalities.%20Our%20method%0Ahas%20significant%20potential%20for%20facilitating%20the%20reuse%20of%20models%20in%20a%20practical%0Amanner%20via%20compositionality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15057v1&entry.124074799=Read"},
{"title": "You Only Acquire Sparse-channel (YOAS): A Unified Framework for\n  Dense-channel EEG Generation", "author": "Hongyu Chen and Weiming Zeng and Luhui Cai and Yueyang Li and Lei Wang and Jia Lu and Hongjie Yan and Wai Ting Siok and Nizhuan Wang", "abstract": "  High-precision acquisition of dense-channel electroencephalogram (EEG)\nsignals is often impeded by the costliness and lack of portability of\nequipment. In contrast, generating dense-channel EEG signals effectively from\nsparse channels shows promise and economic viability. However, sparse-channel\nEEG poses challenges such as reduced spatial resolution, information loss,\nsignal mixing, and heightened susceptibility to noise and interference. To\naddress these challenges, we first theoretically formulate the dense-channel\nEEG generation problem as by optimizing a set of cross-channel EEG signal\ngeneration problems. Then, we propose the YOAS framework for generating\ndense-channel data from sparse-channel EEG signals. The YOAS totally consists\nof four sequential stages: Data Preparation, Data Preprocessing, Biased-EEG\nGeneration, and Synthetic EEG Generation. Data Preparation and Preprocessing\ncarefully consider the distribution of EEG electrodes and low signal-to-noise\nratio problem of EEG signals. Biased-EEG Generation includes sub-modules of\nBiasEEGGanFormer and BiasEEGDiffFormer, which facilitate long-term feature\nextraction with attention and generate signals by combining electrode position\nalignment with diffusion model, respectively. Synthetic EEG Generation\nsynthesizes the final signals, employing a deduction paradigm for multi-channel\nEEG generation. Extensive experiments confirmed YOAS's feasibility, efficiency,\nand theoretical validity, even remarkably enhancing data discernibility. This\nbreakthrough in dense-channel EEG signal generation from sparse-channel data\nopens new avenues for exploration in EEG signal processing and application.\n", "link": "http://arxiv.org/abs/2406.15269v1", "date": "2024-06-21", "relevancy": 2.2763, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4677}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4524}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20You%20Only%20Acquire%20Sparse-channel%20%28YOAS%29%3A%20A%20Unified%20Framework%20for%0A%20%20Dense-channel%20EEG%20Generation&body=Title%3A%20You%20Only%20Acquire%20Sparse-channel%20%28YOAS%29%3A%20A%20Unified%20Framework%20for%0A%20%20Dense-channel%20EEG%20Generation%0AAuthor%3A%20Hongyu%20Chen%20and%20Weiming%20Zeng%20and%20Luhui%20Cai%20and%20Yueyang%20Li%20and%20Lei%20Wang%20and%20Jia%20Lu%20and%20Hongjie%20Yan%20and%20Wai%20Ting%20Siok%20and%20Nizhuan%20Wang%0AAbstract%3A%20%20%20High-precision%20acquisition%20of%20dense-channel%20electroencephalogram%20%28EEG%29%0Asignals%20is%20often%20impeded%20by%20the%20costliness%20and%20lack%20of%20portability%20of%0Aequipment.%20In%20contrast%2C%20generating%20dense-channel%20EEG%20signals%20effectively%20from%0Asparse%20channels%20shows%20promise%20and%20economic%20viability.%20However%2C%20sparse-channel%0AEEG%20poses%20challenges%20such%20as%20reduced%20spatial%20resolution%2C%20information%20loss%2C%0Asignal%20mixing%2C%20and%20heightened%20susceptibility%20to%20noise%20and%20interference.%20To%0Aaddress%20these%20challenges%2C%20we%20first%20theoretically%20formulate%20the%20dense-channel%0AEEG%20generation%20problem%20as%20by%20optimizing%20a%20set%20of%20cross-channel%20EEG%20signal%0Ageneration%20problems.%20Then%2C%20we%20propose%20the%20YOAS%20framework%20for%20generating%0Adense-channel%20data%20from%20sparse-channel%20EEG%20signals.%20The%20YOAS%20totally%20consists%0Aof%20four%20sequential%20stages%3A%20Data%20Preparation%2C%20Data%20Preprocessing%2C%20Biased-EEG%0AGeneration%2C%20and%20Synthetic%20EEG%20Generation.%20Data%20Preparation%20and%20Preprocessing%0Acarefully%20consider%20the%20distribution%20of%20EEG%20electrodes%20and%20low%20signal-to-noise%0Aratio%20problem%20of%20EEG%20signals.%20Biased-EEG%20Generation%20includes%20sub-modules%20of%0ABiasEEGGanFormer%20and%20BiasEEGDiffFormer%2C%20which%20facilitate%20long-term%20feature%0Aextraction%20with%20attention%20and%20generate%20signals%20by%20combining%20electrode%20position%0Aalignment%20with%20diffusion%20model%2C%20respectively.%20Synthetic%20EEG%20Generation%0Asynthesizes%20the%20final%20signals%2C%20employing%20a%20deduction%20paradigm%20for%20multi-channel%0AEEG%20generation.%20Extensive%20experiments%20confirmed%20YOAS%27s%20feasibility%2C%20efficiency%2C%0Aand%20theoretical%20validity%2C%20even%20remarkably%20enhancing%20data%20discernibility.%20This%0Abreakthrough%20in%20dense-channel%20EEG%20signal%20generation%20from%20sparse-channel%20data%0Aopens%20new%20avenues%20for%20exploration%20in%20EEG%20signal%20processing%20and%20application.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15269v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYou%2520Only%2520Acquire%2520Sparse-channel%2520%2528YOAS%2529%253A%2520A%2520Unified%2520Framework%2520for%250A%2520%2520Dense-channel%2520EEG%2520Generation%26entry.906535625%3DHongyu%2520Chen%2520and%2520Weiming%2520Zeng%2520and%2520Luhui%2520Cai%2520and%2520Yueyang%2520Li%2520and%2520Lei%2520Wang%2520and%2520Jia%2520Lu%2520and%2520Hongjie%2520Yan%2520and%2520Wai%2520Ting%2520Siok%2520and%2520Nizhuan%2520Wang%26entry.1292438233%3D%2520%2520High-precision%2520acquisition%2520of%2520dense-channel%2520electroencephalogram%2520%2528EEG%2529%250Asignals%2520is%2520often%2520impeded%2520by%2520the%2520costliness%2520and%2520lack%2520of%2520portability%2520of%250Aequipment.%2520In%2520contrast%252C%2520generating%2520dense-channel%2520EEG%2520signals%2520effectively%2520from%250Asparse%2520channels%2520shows%2520promise%2520and%2520economic%2520viability.%2520However%252C%2520sparse-channel%250AEEG%2520poses%2520challenges%2520such%2520as%2520reduced%2520spatial%2520resolution%252C%2520information%2520loss%252C%250Asignal%2520mixing%252C%2520and%2520heightened%2520susceptibility%2520to%2520noise%2520and%2520interference.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520first%2520theoretically%2520formulate%2520the%2520dense-channel%250AEEG%2520generation%2520problem%2520as%2520by%2520optimizing%2520a%2520set%2520of%2520cross-channel%2520EEG%2520signal%250Ageneration%2520problems.%2520Then%252C%2520we%2520propose%2520the%2520YOAS%2520framework%2520for%2520generating%250Adense-channel%2520data%2520from%2520sparse-channel%2520EEG%2520signals.%2520The%2520YOAS%2520totally%2520consists%250Aof%2520four%2520sequential%2520stages%253A%2520Data%2520Preparation%252C%2520Data%2520Preprocessing%252C%2520Biased-EEG%250AGeneration%252C%2520and%2520Synthetic%2520EEG%2520Generation.%2520Data%2520Preparation%2520and%2520Preprocessing%250Acarefully%2520consider%2520the%2520distribution%2520of%2520EEG%2520electrodes%2520and%2520low%2520signal-to-noise%250Aratio%2520problem%2520of%2520EEG%2520signals.%2520Biased-EEG%2520Generation%2520includes%2520sub-modules%2520of%250ABiasEEGGanFormer%2520and%2520BiasEEGDiffFormer%252C%2520which%2520facilitate%2520long-term%2520feature%250Aextraction%2520with%2520attention%2520and%2520generate%2520signals%2520by%2520combining%2520electrode%2520position%250Aalignment%2520with%2520diffusion%2520model%252C%2520respectively.%2520Synthetic%2520EEG%2520Generation%250Asynthesizes%2520the%2520final%2520signals%252C%2520employing%2520a%2520deduction%2520paradigm%2520for%2520multi-channel%250AEEG%2520generation.%2520Extensive%2520experiments%2520confirmed%2520YOAS%2527s%2520feasibility%252C%2520efficiency%252C%250Aand%2520theoretical%2520validity%252C%2520even%2520remarkably%2520enhancing%2520data%2520discernibility.%2520This%250Abreakthrough%2520in%2520dense-channel%2520EEG%2520signal%2520generation%2520from%2520sparse-channel%2520data%250Aopens%2520new%2520avenues%2520for%2520exploration%2520in%2520EEG%2520signal%2520processing%2520and%2520application.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15269v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=You%20Only%20Acquire%20Sparse-channel%20%28YOAS%29%3A%20A%20Unified%20Framework%20for%0A%20%20Dense-channel%20EEG%20Generation&entry.906535625=Hongyu%20Chen%20and%20Weiming%20Zeng%20and%20Luhui%20Cai%20and%20Yueyang%20Li%20and%20Lei%20Wang%20and%20Jia%20Lu%20and%20Hongjie%20Yan%20and%20Wai%20Ting%20Siok%20and%20Nizhuan%20Wang&entry.1292438233=%20%20High-precision%20acquisition%20of%20dense-channel%20electroencephalogram%20%28EEG%29%0Asignals%20is%20often%20impeded%20by%20the%20costliness%20and%20lack%20of%20portability%20of%0Aequipment.%20In%20contrast%2C%20generating%20dense-channel%20EEG%20signals%20effectively%20from%0Asparse%20channels%20shows%20promise%20and%20economic%20viability.%20However%2C%20sparse-channel%0AEEG%20poses%20challenges%20such%20as%20reduced%20spatial%20resolution%2C%20information%20loss%2C%0Asignal%20mixing%2C%20and%20heightened%20susceptibility%20to%20noise%20and%20interference.%20To%0Aaddress%20these%20challenges%2C%20we%20first%20theoretically%20formulate%20the%20dense-channel%0AEEG%20generation%20problem%20as%20by%20optimizing%20a%20set%20of%20cross-channel%20EEG%20signal%0Ageneration%20problems.%20Then%2C%20we%20propose%20the%20YOAS%20framework%20for%20generating%0Adense-channel%20data%20from%20sparse-channel%20EEG%20signals.%20The%20YOAS%20totally%20consists%0Aof%20four%20sequential%20stages%3A%20Data%20Preparation%2C%20Data%20Preprocessing%2C%20Biased-EEG%0AGeneration%2C%20and%20Synthetic%20EEG%20Generation.%20Data%20Preparation%20and%20Preprocessing%0Acarefully%20consider%20the%20distribution%20of%20EEG%20electrodes%20and%20low%20signal-to-noise%0Aratio%20problem%20of%20EEG%20signals.%20Biased-EEG%20Generation%20includes%20sub-modules%20of%0ABiasEEGGanFormer%20and%20BiasEEGDiffFormer%2C%20which%20facilitate%20long-term%20feature%0Aextraction%20with%20attention%20and%20generate%20signals%20by%20combining%20electrode%20position%0Aalignment%20with%20diffusion%20model%2C%20respectively.%20Synthetic%20EEG%20Generation%0Asynthesizes%20the%20final%20signals%2C%20employing%20a%20deduction%20paradigm%20for%20multi-channel%0AEEG%20generation.%20Extensive%20experiments%20confirmed%20YOAS%27s%20feasibility%2C%20efficiency%2C%0Aand%20theoretical%20validity%2C%20even%20remarkably%20enhancing%20data%20discernibility.%20This%0Abreakthrough%20in%20dense-channel%20EEG%20signal%20generation%20from%20sparse-channel%20data%0Aopens%20new%20avenues%20for%20exploration%20in%20EEG%20signal%20processing%20and%20application.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15269v1&entry.124074799=Read"},
{"title": "Branches: A Fast Dynamic Programming and Branch & Bound Algorithm for\n  Optimal Decision Trees", "author": "Ayman Chaouki and Jesse Read and Albert Bifet", "abstract": "  Decision Tree Learning is a fundamental problem for Interpretable Machine\nLearning, yet it poses a formidable optimization challenge. Despite numerous\nefforts dating back to the early 1990's, practical algorithms have only\nrecently emerged, primarily leveraging Dynamic Programming (DP) and Branch &\nBound (B&B) techniques. These breakthroughs led to the development of two\ndistinct approaches. Algorithms like DL8.5 and MurTree operate on the space of\nnodes (or branches), they are very fast, but do not penalise complex Decision\nTrees, i.e. they do not solve for sparsity. On the other hand, algorithms like\nOSDT and GOSDT operate on the space of Decision Trees, they solve for sparsity\nbut at the detriment of speed. In this work, we introduce Branches, a novel\nalgorithm that integrates the strengths of both paradigms. Leveraging DP and\nB&B, Branches achieves exceptional speed while also solving for sparsity.\nCentral to its efficiency is a novel analytical bound enabling substantial\npruning of the search space. Furthermore, Branches does not necessitate binary\nfeatures. Theoretical analysis demonstrates that Branches has a lower\ncomplexity bound compared to state-of-the-art methods, a claim validated\nthrough extensive empirical evaluation. Our results illustrate that Branches\noutperforms the state of the art in terms of speed and number of iterations\nwhile consistently yielding optimal Decision Trees.\n", "link": "http://arxiv.org/abs/2406.02175v2", "date": "2024-06-21", "relevancy": 2.2748, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5026}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4331}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4292}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Branches%3A%20A%20Fast%20Dynamic%20Programming%20and%20Branch%20%26%20Bound%20Algorithm%20for%0A%20%20Optimal%20Decision%20Trees&body=Title%3A%20Branches%3A%20A%20Fast%20Dynamic%20Programming%20and%20Branch%20%26%20Bound%20Algorithm%20for%0A%20%20Optimal%20Decision%20Trees%0AAuthor%3A%20Ayman%20Chaouki%20and%20Jesse%20Read%20and%20Albert%20Bifet%0AAbstract%3A%20%20%20Decision%20Tree%20Learning%20is%20a%20fundamental%20problem%20for%20Interpretable%20Machine%0ALearning%2C%20yet%20it%20poses%20a%20formidable%20optimization%20challenge.%20Despite%20numerous%0Aefforts%20dating%20back%20to%20the%20early%201990%27s%2C%20practical%20algorithms%20have%20only%0Arecently%20emerged%2C%20primarily%20leveraging%20Dynamic%20Programming%20%28DP%29%20and%20Branch%20%26%0ABound%20%28B%26B%29%20techniques.%20These%20breakthroughs%20led%20to%20the%20development%20of%20two%0Adistinct%20approaches.%20Algorithms%20like%20DL8.5%20and%20MurTree%20operate%20on%20the%20space%20of%0Anodes%20%28or%20branches%29%2C%20they%20are%20very%20fast%2C%20but%20do%20not%20penalise%20complex%20Decision%0ATrees%2C%20i.e.%20they%20do%20not%20solve%20for%20sparsity.%20On%20the%20other%20hand%2C%20algorithms%20like%0AOSDT%20and%20GOSDT%20operate%20on%20the%20space%20of%20Decision%20Trees%2C%20they%20solve%20for%20sparsity%0Abut%20at%20the%20detriment%20of%20speed.%20In%20this%20work%2C%20we%20introduce%20Branches%2C%20a%20novel%0Aalgorithm%20that%20integrates%20the%20strengths%20of%20both%20paradigms.%20Leveraging%20DP%20and%0AB%26B%2C%20Branches%20achieves%20exceptional%20speed%20while%20also%20solving%20for%20sparsity.%0ACentral%20to%20its%20efficiency%20is%20a%20novel%20analytical%20bound%20enabling%20substantial%0Apruning%20of%20the%20search%20space.%20Furthermore%2C%20Branches%20does%20not%20necessitate%20binary%0Afeatures.%20Theoretical%20analysis%20demonstrates%20that%20Branches%20has%20a%20lower%0Acomplexity%20bound%20compared%20to%20state-of-the-art%20methods%2C%20a%20claim%20validated%0Athrough%20extensive%20empirical%20evaluation.%20Our%20results%20illustrate%20that%20Branches%0Aoutperforms%20the%20state%20of%20the%20art%20in%20terms%20of%20speed%20and%20number%20of%20iterations%0Awhile%20consistently%20yielding%20optimal%20Decision%20Trees.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02175v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBranches%253A%2520A%2520Fast%2520Dynamic%2520Programming%2520and%2520Branch%2520%2526%2520Bound%2520Algorithm%2520for%250A%2520%2520Optimal%2520Decision%2520Trees%26entry.906535625%3DAyman%2520Chaouki%2520and%2520Jesse%2520Read%2520and%2520Albert%2520Bifet%26entry.1292438233%3D%2520%2520Decision%2520Tree%2520Learning%2520is%2520a%2520fundamental%2520problem%2520for%2520Interpretable%2520Machine%250ALearning%252C%2520yet%2520it%2520poses%2520a%2520formidable%2520optimization%2520challenge.%2520Despite%2520numerous%250Aefforts%2520dating%2520back%2520to%2520the%2520early%25201990%2527s%252C%2520practical%2520algorithms%2520have%2520only%250Arecently%2520emerged%252C%2520primarily%2520leveraging%2520Dynamic%2520Programming%2520%2528DP%2529%2520and%2520Branch%2520%2526%250ABound%2520%2528B%2526B%2529%2520techniques.%2520These%2520breakthroughs%2520led%2520to%2520the%2520development%2520of%2520two%250Adistinct%2520approaches.%2520Algorithms%2520like%2520DL8.5%2520and%2520MurTree%2520operate%2520on%2520the%2520space%2520of%250Anodes%2520%2528or%2520branches%2529%252C%2520they%2520are%2520very%2520fast%252C%2520but%2520do%2520not%2520penalise%2520complex%2520Decision%250ATrees%252C%2520i.e.%2520they%2520do%2520not%2520solve%2520for%2520sparsity.%2520On%2520the%2520other%2520hand%252C%2520algorithms%2520like%250AOSDT%2520and%2520GOSDT%2520operate%2520on%2520the%2520space%2520of%2520Decision%2520Trees%252C%2520they%2520solve%2520for%2520sparsity%250Abut%2520at%2520the%2520detriment%2520of%2520speed.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Branches%252C%2520a%2520novel%250Aalgorithm%2520that%2520integrates%2520the%2520strengths%2520of%2520both%2520paradigms.%2520Leveraging%2520DP%2520and%250AB%2526B%252C%2520Branches%2520achieves%2520exceptional%2520speed%2520while%2520also%2520solving%2520for%2520sparsity.%250ACentral%2520to%2520its%2520efficiency%2520is%2520a%2520novel%2520analytical%2520bound%2520enabling%2520substantial%250Apruning%2520of%2520the%2520search%2520space.%2520Furthermore%252C%2520Branches%2520does%2520not%2520necessitate%2520binary%250Afeatures.%2520Theoretical%2520analysis%2520demonstrates%2520that%2520Branches%2520has%2520a%2520lower%250Acomplexity%2520bound%2520compared%2520to%2520state-of-the-art%2520methods%252C%2520a%2520claim%2520validated%250Athrough%2520extensive%2520empirical%2520evaluation.%2520Our%2520results%2520illustrate%2520that%2520Branches%250Aoutperforms%2520the%2520state%2520of%2520the%2520art%2520in%2520terms%2520of%2520speed%2520and%2520number%2520of%2520iterations%250Awhile%2520consistently%2520yielding%2520optimal%2520Decision%2520Trees.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02175v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Branches%3A%20A%20Fast%20Dynamic%20Programming%20and%20Branch%20%26%20Bound%20Algorithm%20for%0A%20%20Optimal%20Decision%20Trees&entry.906535625=Ayman%20Chaouki%20and%20Jesse%20Read%20and%20Albert%20Bifet&entry.1292438233=%20%20Decision%20Tree%20Learning%20is%20a%20fundamental%20problem%20for%20Interpretable%20Machine%0ALearning%2C%20yet%20it%20poses%20a%20formidable%20optimization%20challenge.%20Despite%20numerous%0Aefforts%20dating%20back%20to%20the%20early%201990%27s%2C%20practical%20algorithms%20have%20only%0Arecently%20emerged%2C%20primarily%20leveraging%20Dynamic%20Programming%20%28DP%29%20and%20Branch%20%26%0ABound%20%28B%26B%29%20techniques.%20These%20breakthroughs%20led%20to%20the%20development%20of%20two%0Adistinct%20approaches.%20Algorithms%20like%20DL8.5%20and%20MurTree%20operate%20on%20the%20space%20of%0Anodes%20%28or%20branches%29%2C%20they%20are%20very%20fast%2C%20but%20do%20not%20penalise%20complex%20Decision%0ATrees%2C%20i.e.%20they%20do%20not%20solve%20for%20sparsity.%20On%20the%20other%20hand%2C%20algorithms%20like%0AOSDT%20and%20GOSDT%20operate%20on%20the%20space%20of%20Decision%20Trees%2C%20they%20solve%20for%20sparsity%0Abut%20at%20the%20detriment%20of%20speed.%20In%20this%20work%2C%20we%20introduce%20Branches%2C%20a%20novel%0Aalgorithm%20that%20integrates%20the%20strengths%20of%20both%20paradigms.%20Leveraging%20DP%20and%0AB%26B%2C%20Branches%20achieves%20exceptional%20speed%20while%20also%20solving%20for%20sparsity.%0ACentral%20to%20its%20efficiency%20is%20a%20novel%20analytical%20bound%20enabling%20substantial%0Apruning%20of%20the%20search%20space.%20Furthermore%2C%20Branches%20does%20not%20necessitate%20binary%0Afeatures.%20Theoretical%20analysis%20demonstrates%20that%20Branches%20has%20a%20lower%0Acomplexity%20bound%20compared%20to%20state-of-the-art%20methods%2C%20a%20claim%20validated%0Athrough%20extensive%20empirical%20evaluation.%20Our%20results%20illustrate%20that%20Branches%0Aoutperforms%20the%20state%20of%20the%20art%20in%20terms%20of%20speed%20and%20number%20of%20iterations%0Awhile%20consistently%20yielding%20optimal%20Decision%20Trees.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02175v2&entry.124074799=Read"},
{"title": "Federated Learning over Connected Modes", "author": "Dennis Grinwald and Philipp Wiesner and Shinichi Nakajima", "abstract": "  Statistical heterogeneity in federated learning poses two major challenges:\nslow global training due to conflicting gradient signals, and the need of\npersonalization for local distributions. In this work, we tackle both\nchallenges by leveraging recent advances in \\emph{linear mode connectivity} --\nidentifying a linearly connected low-loss region in the weight space of neural\nnetworks, which we call solution simplex. We propose federated learning over\nconnected modes (\\textsc{Floco}), where clients are assigned local subregions\nin this simplex based on their gradient signals, and together learn the shared\nglobal solution simplex. This allows personalization of the client models to\nfit their local distributions within the degrees of freedom in the solution\nsimplex and homogenizes the update signals for the global simplex training. Our\nexperiments show that \\textsc{Floco} accelerates the global training process,\nand significantly improves the local accuracy with minimal computational\noverhead.\n", "link": "http://arxiv.org/abs/2403.03333v2", "date": "2024-06-21", "relevancy": 2.247, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.451}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.449}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Learning%20over%20Connected%20Modes&body=Title%3A%20Federated%20Learning%20over%20Connected%20Modes%0AAuthor%3A%20Dennis%20Grinwald%20and%20Philipp%20Wiesner%20and%20Shinichi%20Nakajima%0AAbstract%3A%20%20%20Statistical%20heterogeneity%20in%20federated%20learning%20poses%20two%20major%20challenges%3A%0Aslow%20global%20training%20due%20to%20conflicting%20gradient%20signals%2C%20and%20the%20need%20of%0Apersonalization%20for%20local%20distributions.%20In%20this%20work%2C%20we%20tackle%20both%0Achallenges%20by%20leveraging%20recent%20advances%20in%20%5Cemph%7Blinear%20mode%20connectivity%7D%20--%0Aidentifying%20a%20linearly%20connected%20low-loss%20region%20in%20the%20weight%20space%20of%20neural%0Anetworks%2C%20which%20we%20call%20solution%20simplex.%20We%20propose%20federated%20learning%20over%0Aconnected%20modes%20%28%5Ctextsc%7BFloco%7D%29%2C%20where%20clients%20are%20assigned%20local%20subregions%0Ain%20this%20simplex%20based%20on%20their%20gradient%20signals%2C%20and%20together%20learn%20the%20shared%0Aglobal%20solution%20simplex.%20This%20allows%20personalization%20of%20the%20client%20models%20to%0Afit%20their%20local%20distributions%20within%20the%20degrees%20of%20freedom%20in%20the%20solution%0Asimplex%20and%20homogenizes%20the%20update%20signals%20for%20the%20global%20simplex%20training.%20Our%0Aexperiments%20show%20that%20%5Ctextsc%7BFloco%7D%20accelerates%20the%20global%20training%20process%2C%0Aand%20significantly%20improves%20the%20local%20accuracy%20with%20minimal%20computational%0Aoverhead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.03333v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Learning%2520over%2520Connected%2520Modes%26entry.906535625%3DDennis%2520Grinwald%2520and%2520Philipp%2520Wiesner%2520and%2520Shinichi%2520Nakajima%26entry.1292438233%3D%2520%2520Statistical%2520heterogeneity%2520in%2520federated%2520learning%2520poses%2520two%2520major%2520challenges%253A%250Aslow%2520global%2520training%2520due%2520to%2520conflicting%2520gradient%2520signals%252C%2520and%2520the%2520need%2520of%250Apersonalization%2520for%2520local%2520distributions.%2520In%2520this%2520work%252C%2520we%2520tackle%2520both%250Achallenges%2520by%2520leveraging%2520recent%2520advances%2520in%2520%255Cemph%257Blinear%2520mode%2520connectivity%257D%2520--%250Aidentifying%2520a%2520linearly%2520connected%2520low-loss%2520region%2520in%2520the%2520weight%2520space%2520of%2520neural%250Anetworks%252C%2520which%2520we%2520call%2520solution%2520simplex.%2520We%2520propose%2520federated%2520learning%2520over%250Aconnected%2520modes%2520%2528%255Ctextsc%257BFloco%257D%2529%252C%2520where%2520clients%2520are%2520assigned%2520local%2520subregions%250Ain%2520this%2520simplex%2520based%2520on%2520their%2520gradient%2520signals%252C%2520and%2520together%2520learn%2520the%2520shared%250Aglobal%2520solution%2520simplex.%2520This%2520allows%2520personalization%2520of%2520the%2520client%2520models%2520to%250Afit%2520their%2520local%2520distributions%2520within%2520the%2520degrees%2520of%2520freedom%2520in%2520the%2520solution%250Asimplex%2520and%2520homogenizes%2520the%2520update%2520signals%2520for%2520the%2520global%2520simplex%2520training.%2520Our%250Aexperiments%2520show%2520that%2520%255Ctextsc%257BFloco%257D%2520accelerates%2520the%2520global%2520training%2520process%252C%250Aand%2520significantly%2520improves%2520the%2520local%2520accuracy%2520with%2520minimal%2520computational%250Aoverhead.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.03333v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Learning%20over%20Connected%20Modes&entry.906535625=Dennis%20Grinwald%20and%20Philipp%20Wiesner%20and%20Shinichi%20Nakajima&entry.1292438233=%20%20Statistical%20heterogeneity%20in%20federated%20learning%20poses%20two%20major%20challenges%3A%0Aslow%20global%20training%20due%20to%20conflicting%20gradient%20signals%2C%20and%20the%20need%20of%0Apersonalization%20for%20local%20distributions.%20In%20this%20work%2C%20we%20tackle%20both%0Achallenges%20by%20leveraging%20recent%20advances%20in%20%5Cemph%7Blinear%20mode%20connectivity%7D%20--%0Aidentifying%20a%20linearly%20connected%20low-loss%20region%20in%20the%20weight%20space%20of%20neural%0Anetworks%2C%20which%20we%20call%20solution%20simplex.%20We%20propose%20federated%20learning%20over%0Aconnected%20modes%20%28%5Ctextsc%7BFloco%7D%29%2C%20where%20clients%20are%20assigned%20local%20subregions%0Ain%20this%20simplex%20based%20on%20their%20gradient%20signals%2C%20and%20together%20learn%20the%20shared%0Aglobal%20solution%20simplex.%20This%20allows%20personalization%20of%20the%20client%20models%20to%0Afit%20their%20local%20distributions%20within%20the%20degrees%20of%20freedom%20in%20the%20solution%0Asimplex%20and%20homogenizes%20the%20update%20signals%20for%20the%20global%20simplex%20training.%20Our%0Aexperiments%20show%20that%20%5Ctextsc%7BFloco%7D%20accelerates%20the%20global%20training%20process%2C%0Aand%20significantly%20improves%20the%20local%20accuracy%20with%20minimal%20computational%0Aoverhead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03333v2&entry.124074799=Read"},
{"title": "NAVSIM: Data-Driven Non-Reactive Autonomous Vehicle Simulation and\n  Benchmarking", "author": "Daniel Dauner and Marcel Hallgarten and Tianyu Li and Xinshuo Weng and Zhiyu Huang and Zetong Yang and Hongyang Li and Igor Gilitschenski and Boris Ivanovic and Marco Pavone and Andreas Geiger and Kashyap Chitta", "abstract": "  Benchmarking vision-based driving policies is challenging. On one hand,\nopen-loop evaluation with real data is easy, but these results do not reflect\nclosed-loop performance. On the other, closed-loop evaluation is possible in\nsimulation, but is hard to scale due to its significant computational demands.\nFurther, the simulators available today exhibit a large domain gap to real\ndata. This has resulted in an inability to draw clear conclusions from the\nrapidly growing body of research on end-to-end autonomous driving. In this\npaper, we present NAVSIM, a middle ground between these evaluation paradigms,\nwhere we use large datasets in combination with a non-reactive simulator to\nenable large-scale real-world benchmarking. Specifically, we gather\nsimulation-based metrics, such as progress and time to collision, by unrolling\nbird's eye view abstractions of the test scenes for a short simulation horizon.\nOur simulation is non-reactive, i.e., the evaluated policy and environment do\nnot influence each other. As we demonstrate empirically, this decoupling allows\nopen-loop metric computation while being better aligned with closed-loop\nevaluations than traditional displacement errors. NAVSIM enabled a new\ncompetition held at CVPR 2024, where 143 teams submitted 463 entries, resulting\nin several new insights. On a large set of challenging scenarios, we observe\nthat simple methods with moderate compute requirements such as TransFuser can\nmatch recent large-scale end-to-end driving architectures such as UniAD. Our\nmodular framework can potentially be extended with new datasets, data curation\nstrategies, and metrics, and will be continually maintained to host future\nchallenges. Our code is available at\nhttps://github.com/autonomousvision/navsim.\n", "link": "http://arxiv.org/abs/2406.15349v1", "date": "2024-06-21", "relevancy": 2.2344, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5695}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.557}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NAVSIM%3A%20Data-Driven%20Non-Reactive%20Autonomous%20Vehicle%20Simulation%20and%0A%20%20Benchmarking&body=Title%3A%20NAVSIM%3A%20Data-Driven%20Non-Reactive%20Autonomous%20Vehicle%20Simulation%20and%0A%20%20Benchmarking%0AAuthor%3A%20Daniel%20Dauner%20and%20Marcel%20Hallgarten%20and%20Tianyu%20Li%20and%20Xinshuo%20Weng%20and%20Zhiyu%20Huang%20and%20Zetong%20Yang%20and%20Hongyang%20Li%20and%20Igor%20Gilitschenski%20and%20Boris%20Ivanovic%20and%20Marco%20Pavone%20and%20Andreas%20Geiger%20and%20Kashyap%20Chitta%0AAbstract%3A%20%20%20Benchmarking%20vision-based%20driving%20policies%20is%20challenging.%20On%20one%20hand%2C%0Aopen-loop%20evaluation%20with%20real%20data%20is%20easy%2C%20but%20these%20results%20do%20not%20reflect%0Aclosed-loop%20performance.%20On%20the%20other%2C%20closed-loop%20evaluation%20is%20possible%20in%0Asimulation%2C%20but%20is%20hard%20to%20scale%20due%20to%20its%20significant%20computational%20demands.%0AFurther%2C%20the%20simulators%20available%20today%20exhibit%20a%20large%20domain%20gap%20to%20real%0Adata.%20This%20has%20resulted%20in%20an%20inability%20to%20draw%20clear%20conclusions%20from%20the%0Arapidly%20growing%20body%20of%20research%20on%20end-to-end%20autonomous%20driving.%20In%20this%0Apaper%2C%20we%20present%20NAVSIM%2C%20a%20middle%20ground%20between%20these%20evaluation%20paradigms%2C%0Awhere%20we%20use%20large%20datasets%20in%20combination%20with%20a%20non-reactive%20simulator%20to%0Aenable%20large-scale%20real-world%20benchmarking.%20Specifically%2C%20we%20gather%0Asimulation-based%20metrics%2C%20such%20as%20progress%20and%20time%20to%20collision%2C%20by%20unrolling%0Abird%27s%20eye%20view%20abstractions%20of%20the%20test%20scenes%20for%20a%20short%20simulation%20horizon.%0AOur%20simulation%20is%20non-reactive%2C%20i.e.%2C%20the%20evaluated%20policy%20and%20environment%20do%0Anot%20influence%20each%20other.%20As%20we%20demonstrate%20empirically%2C%20this%20decoupling%20allows%0Aopen-loop%20metric%20computation%20while%20being%20better%20aligned%20with%20closed-loop%0Aevaluations%20than%20traditional%20displacement%20errors.%20NAVSIM%20enabled%20a%20new%0Acompetition%20held%20at%20CVPR%202024%2C%20where%20143%20teams%20submitted%20463%20entries%2C%20resulting%0Ain%20several%20new%20insights.%20On%20a%20large%20set%20of%20challenging%20scenarios%2C%20we%20observe%0Athat%20simple%20methods%20with%20moderate%20compute%20requirements%20such%20as%20TransFuser%20can%0Amatch%20recent%20large-scale%20end-to-end%20driving%20architectures%20such%20as%20UniAD.%20Our%0Amodular%20framework%20can%20potentially%20be%20extended%20with%20new%20datasets%2C%20data%20curation%0Astrategies%2C%20and%20metrics%2C%20and%20will%20be%20continually%20maintained%20to%20host%20future%0Achallenges.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/autonomousvision/navsim.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15349v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNAVSIM%253A%2520Data-Driven%2520Non-Reactive%2520Autonomous%2520Vehicle%2520Simulation%2520and%250A%2520%2520Benchmarking%26entry.906535625%3DDaniel%2520Dauner%2520and%2520Marcel%2520Hallgarten%2520and%2520Tianyu%2520Li%2520and%2520Xinshuo%2520Weng%2520and%2520Zhiyu%2520Huang%2520and%2520Zetong%2520Yang%2520and%2520Hongyang%2520Li%2520and%2520Igor%2520Gilitschenski%2520and%2520Boris%2520Ivanovic%2520and%2520Marco%2520Pavone%2520and%2520Andreas%2520Geiger%2520and%2520Kashyap%2520Chitta%26entry.1292438233%3D%2520%2520Benchmarking%2520vision-based%2520driving%2520policies%2520is%2520challenging.%2520On%2520one%2520hand%252C%250Aopen-loop%2520evaluation%2520with%2520real%2520data%2520is%2520easy%252C%2520but%2520these%2520results%2520do%2520not%2520reflect%250Aclosed-loop%2520performance.%2520On%2520the%2520other%252C%2520closed-loop%2520evaluation%2520is%2520possible%2520in%250Asimulation%252C%2520but%2520is%2520hard%2520to%2520scale%2520due%2520to%2520its%2520significant%2520computational%2520demands.%250AFurther%252C%2520the%2520simulators%2520available%2520today%2520exhibit%2520a%2520large%2520domain%2520gap%2520to%2520real%250Adata.%2520This%2520has%2520resulted%2520in%2520an%2520inability%2520to%2520draw%2520clear%2520conclusions%2520from%2520the%250Arapidly%2520growing%2520body%2520of%2520research%2520on%2520end-to-end%2520autonomous%2520driving.%2520In%2520this%250Apaper%252C%2520we%2520present%2520NAVSIM%252C%2520a%2520middle%2520ground%2520between%2520these%2520evaluation%2520paradigms%252C%250Awhere%2520we%2520use%2520large%2520datasets%2520in%2520combination%2520with%2520a%2520non-reactive%2520simulator%2520to%250Aenable%2520large-scale%2520real-world%2520benchmarking.%2520Specifically%252C%2520we%2520gather%250Asimulation-based%2520metrics%252C%2520such%2520as%2520progress%2520and%2520time%2520to%2520collision%252C%2520by%2520unrolling%250Abird%2527s%2520eye%2520view%2520abstractions%2520of%2520the%2520test%2520scenes%2520for%2520a%2520short%2520simulation%2520horizon.%250AOur%2520simulation%2520is%2520non-reactive%252C%2520i.e.%252C%2520the%2520evaluated%2520policy%2520and%2520environment%2520do%250Anot%2520influence%2520each%2520other.%2520As%2520we%2520demonstrate%2520empirically%252C%2520this%2520decoupling%2520allows%250Aopen-loop%2520metric%2520computation%2520while%2520being%2520better%2520aligned%2520with%2520closed-loop%250Aevaluations%2520than%2520traditional%2520displacement%2520errors.%2520NAVSIM%2520enabled%2520a%2520new%250Acompetition%2520held%2520at%2520CVPR%25202024%252C%2520where%2520143%2520teams%2520submitted%2520463%2520entries%252C%2520resulting%250Ain%2520several%2520new%2520insights.%2520On%2520a%2520large%2520set%2520of%2520challenging%2520scenarios%252C%2520we%2520observe%250Athat%2520simple%2520methods%2520with%2520moderate%2520compute%2520requirements%2520such%2520as%2520TransFuser%2520can%250Amatch%2520recent%2520large-scale%2520end-to-end%2520driving%2520architectures%2520such%2520as%2520UniAD.%2520Our%250Amodular%2520framework%2520can%2520potentially%2520be%2520extended%2520with%2520new%2520datasets%252C%2520data%2520curation%250Astrategies%252C%2520and%2520metrics%252C%2520and%2520will%2520be%2520continually%2520maintained%2520to%2520host%2520future%250Achallenges.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/autonomousvision/navsim.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15349v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NAVSIM%3A%20Data-Driven%20Non-Reactive%20Autonomous%20Vehicle%20Simulation%20and%0A%20%20Benchmarking&entry.906535625=Daniel%20Dauner%20and%20Marcel%20Hallgarten%20and%20Tianyu%20Li%20and%20Xinshuo%20Weng%20and%20Zhiyu%20Huang%20and%20Zetong%20Yang%20and%20Hongyang%20Li%20and%20Igor%20Gilitschenski%20and%20Boris%20Ivanovic%20and%20Marco%20Pavone%20and%20Andreas%20Geiger%20and%20Kashyap%20Chitta&entry.1292438233=%20%20Benchmarking%20vision-based%20driving%20policies%20is%20challenging.%20On%20one%20hand%2C%0Aopen-loop%20evaluation%20with%20real%20data%20is%20easy%2C%20but%20these%20results%20do%20not%20reflect%0Aclosed-loop%20performance.%20On%20the%20other%2C%20closed-loop%20evaluation%20is%20possible%20in%0Asimulation%2C%20but%20is%20hard%20to%20scale%20due%20to%20its%20significant%20computational%20demands.%0AFurther%2C%20the%20simulators%20available%20today%20exhibit%20a%20large%20domain%20gap%20to%20real%0Adata.%20This%20has%20resulted%20in%20an%20inability%20to%20draw%20clear%20conclusions%20from%20the%0Arapidly%20growing%20body%20of%20research%20on%20end-to-end%20autonomous%20driving.%20In%20this%0Apaper%2C%20we%20present%20NAVSIM%2C%20a%20middle%20ground%20between%20these%20evaluation%20paradigms%2C%0Awhere%20we%20use%20large%20datasets%20in%20combination%20with%20a%20non-reactive%20simulator%20to%0Aenable%20large-scale%20real-world%20benchmarking.%20Specifically%2C%20we%20gather%0Asimulation-based%20metrics%2C%20such%20as%20progress%20and%20time%20to%20collision%2C%20by%20unrolling%0Abird%27s%20eye%20view%20abstractions%20of%20the%20test%20scenes%20for%20a%20short%20simulation%20horizon.%0AOur%20simulation%20is%20non-reactive%2C%20i.e.%2C%20the%20evaluated%20policy%20and%20environment%20do%0Anot%20influence%20each%20other.%20As%20we%20demonstrate%20empirically%2C%20this%20decoupling%20allows%0Aopen-loop%20metric%20computation%20while%20being%20better%20aligned%20with%20closed-loop%0Aevaluations%20than%20traditional%20displacement%20errors.%20NAVSIM%20enabled%20a%20new%0Acompetition%20held%20at%20CVPR%202024%2C%20where%20143%20teams%20submitted%20463%20entries%2C%20resulting%0Ain%20several%20new%20insights.%20On%20a%20large%20set%20of%20challenging%20scenarios%2C%20we%20observe%0Athat%20simple%20methods%20with%20moderate%20compute%20requirements%20such%20as%20TransFuser%20can%0Amatch%20recent%20large-scale%20end-to-end%20driving%20architectures%20such%20as%20UniAD.%20Our%0Amodular%20framework%20can%20potentially%20be%20extended%20with%20new%20datasets%2C%20data%20curation%0Astrategies%2C%20and%20metrics%2C%20and%20will%20be%20continually%20maintained%20to%20host%20future%0Achallenges.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/autonomousvision/navsim.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15349v1&entry.124074799=Read"},
{"title": "FA-Net: A Fuzzy Attention-aided Deep Neural Network for Pneumonia\n  Detection in Chest X-Rays", "author": "Ayush Roy and Anurag Bhattacharjee and Diego Oliva and Oscar Ramos-Soto and Francisco J. Alvarez-Padilla and Ram Sarkar", "abstract": "  Pneumonia is a respiratory infection caused by bacteria, fungi, or viruses.\nIt affects many people, particularly those in developing or underdeveloped\nnations with high pollution levels, unhygienic living conditions, overcrowding,\nand insufficient medical infrastructure. Pneumonia can cause pleural effusion,\nwhere fluids fill the lungs, leading to respiratory difficulty. Early diagnosis\nis crucial to ensure effective treatment and increase survival rates. Chest\nX-ray imaging is the most commonly used method for diagnosing pneumonia.\nHowever, visual examination of chest X-rays can be difficult and subjective. In\nthis study, we have developed a computer-aided diagnosis system for automatic\npneumonia detection using chest X-ray images. We have used DenseNet-121 and\nResNet50 as the backbone for the binary class (pneumonia and normal) and\nmulti-class (bacterial pneumonia, viral pneumonia, and normal) classification\ntasks, respectively. We have also implemented a channel-specific spatial\nattention mechanism, called Fuzzy Channel Selective Spatial Attention Module\n(FCSSAM), to highlight the specific spatial regions of relevant channels while\nremoving the irrelevant channels of the extracted features by the backbone. We\nevaluated the proposed approach on a publicly available chest X-ray dataset,\nusing binary and multi-class classification setups. Our proposed method\nachieves accuracy rates of 97.15\\% and 79.79\\% for the binary and multi-class\nclassification setups, respectively. The results of our proposed method are\nsuperior to state-of-the-art (SOTA) methods. The code of the proposed model\nwill be available at: https://github.com/AyushRoy2001/FA-Net.\n", "link": "http://arxiv.org/abs/2406.15117v1", "date": "2024-06-21", "relevancy": 2.2313, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4554}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.442}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4414}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FA-Net%3A%20A%20Fuzzy%20Attention-aided%20Deep%20Neural%20Network%20for%20Pneumonia%0A%20%20Detection%20in%20Chest%20X-Rays&body=Title%3A%20FA-Net%3A%20A%20Fuzzy%20Attention-aided%20Deep%20Neural%20Network%20for%20Pneumonia%0A%20%20Detection%20in%20Chest%20X-Rays%0AAuthor%3A%20Ayush%20Roy%20and%20Anurag%20Bhattacharjee%20and%20Diego%20Oliva%20and%20Oscar%20Ramos-Soto%20and%20Francisco%20J.%20Alvarez-Padilla%20and%20Ram%20Sarkar%0AAbstract%3A%20%20%20Pneumonia%20is%20a%20respiratory%20infection%20caused%20by%20bacteria%2C%20fungi%2C%20or%20viruses.%0AIt%20affects%20many%20people%2C%20particularly%20those%20in%20developing%20or%20underdeveloped%0Anations%20with%20high%20pollution%20levels%2C%20unhygienic%20living%20conditions%2C%20overcrowding%2C%0Aand%20insufficient%20medical%20infrastructure.%20Pneumonia%20can%20cause%20pleural%20effusion%2C%0Awhere%20fluids%20fill%20the%20lungs%2C%20leading%20to%20respiratory%20difficulty.%20Early%20diagnosis%0Ais%20crucial%20to%20ensure%20effective%20treatment%20and%20increase%20survival%20rates.%20Chest%0AX-ray%20imaging%20is%20the%20most%20commonly%20used%20method%20for%20diagnosing%20pneumonia.%0AHowever%2C%20visual%20examination%20of%20chest%20X-rays%20can%20be%20difficult%20and%20subjective.%20In%0Athis%20study%2C%20we%20have%20developed%20a%20computer-aided%20diagnosis%20system%20for%20automatic%0Apneumonia%20detection%20using%20chest%20X-ray%20images.%20We%20have%20used%20DenseNet-121%20and%0AResNet50%20as%20the%20backbone%20for%20the%20binary%20class%20%28pneumonia%20and%20normal%29%20and%0Amulti-class%20%28bacterial%20pneumonia%2C%20viral%20pneumonia%2C%20and%20normal%29%20classification%0Atasks%2C%20respectively.%20We%20have%20also%20implemented%20a%20channel-specific%20spatial%0Aattention%20mechanism%2C%20called%20Fuzzy%20Channel%20Selective%20Spatial%20Attention%20Module%0A%28FCSSAM%29%2C%20to%20highlight%20the%20specific%20spatial%20regions%20of%20relevant%20channels%20while%0Aremoving%20the%20irrelevant%20channels%20of%20the%20extracted%20features%20by%20the%20backbone.%20We%0Aevaluated%20the%20proposed%20approach%20on%20a%20publicly%20available%20chest%20X-ray%20dataset%2C%0Ausing%20binary%20and%20multi-class%20classification%20setups.%20Our%20proposed%20method%0Aachieves%20accuracy%20rates%20of%2097.15%5C%25%20and%2079.79%5C%25%20for%20the%20binary%20and%20multi-class%0Aclassification%20setups%2C%20respectively.%20The%20results%20of%20our%20proposed%20method%20are%0Asuperior%20to%20state-of-the-art%20%28SOTA%29%20methods.%20The%20code%20of%20the%20proposed%20model%0Awill%20be%20available%20at%3A%20https%3A//github.com/AyushRoy2001/FA-Net.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15117v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFA-Net%253A%2520A%2520Fuzzy%2520Attention-aided%2520Deep%2520Neural%2520Network%2520for%2520Pneumonia%250A%2520%2520Detection%2520in%2520Chest%2520X-Rays%26entry.906535625%3DAyush%2520Roy%2520and%2520Anurag%2520Bhattacharjee%2520and%2520Diego%2520Oliva%2520and%2520Oscar%2520Ramos-Soto%2520and%2520Francisco%2520J.%2520Alvarez-Padilla%2520and%2520Ram%2520Sarkar%26entry.1292438233%3D%2520%2520Pneumonia%2520is%2520a%2520respiratory%2520infection%2520caused%2520by%2520bacteria%252C%2520fungi%252C%2520or%2520viruses.%250AIt%2520affects%2520many%2520people%252C%2520particularly%2520those%2520in%2520developing%2520or%2520underdeveloped%250Anations%2520with%2520high%2520pollution%2520levels%252C%2520unhygienic%2520living%2520conditions%252C%2520overcrowding%252C%250Aand%2520insufficient%2520medical%2520infrastructure.%2520Pneumonia%2520can%2520cause%2520pleural%2520effusion%252C%250Awhere%2520fluids%2520fill%2520the%2520lungs%252C%2520leading%2520to%2520respiratory%2520difficulty.%2520Early%2520diagnosis%250Ais%2520crucial%2520to%2520ensure%2520effective%2520treatment%2520and%2520increase%2520survival%2520rates.%2520Chest%250AX-ray%2520imaging%2520is%2520the%2520most%2520commonly%2520used%2520method%2520for%2520diagnosing%2520pneumonia.%250AHowever%252C%2520visual%2520examination%2520of%2520chest%2520X-rays%2520can%2520be%2520difficult%2520and%2520subjective.%2520In%250Athis%2520study%252C%2520we%2520have%2520developed%2520a%2520computer-aided%2520diagnosis%2520system%2520for%2520automatic%250Apneumonia%2520detection%2520using%2520chest%2520X-ray%2520images.%2520We%2520have%2520used%2520DenseNet-121%2520and%250AResNet50%2520as%2520the%2520backbone%2520for%2520the%2520binary%2520class%2520%2528pneumonia%2520and%2520normal%2529%2520and%250Amulti-class%2520%2528bacterial%2520pneumonia%252C%2520viral%2520pneumonia%252C%2520and%2520normal%2529%2520classification%250Atasks%252C%2520respectively.%2520We%2520have%2520also%2520implemented%2520a%2520channel-specific%2520spatial%250Aattention%2520mechanism%252C%2520called%2520Fuzzy%2520Channel%2520Selective%2520Spatial%2520Attention%2520Module%250A%2528FCSSAM%2529%252C%2520to%2520highlight%2520the%2520specific%2520spatial%2520regions%2520of%2520relevant%2520channels%2520while%250Aremoving%2520the%2520irrelevant%2520channels%2520of%2520the%2520extracted%2520features%2520by%2520the%2520backbone.%2520We%250Aevaluated%2520the%2520proposed%2520approach%2520on%2520a%2520publicly%2520available%2520chest%2520X-ray%2520dataset%252C%250Ausing%2520binary%2520and%2520multi-class%2520classification%2520setups.%2520Our%2520proposed%2520method%250Aachieves%2520accuracy%2520rates%2520of%252097.15%255C%2525%2520and%252079.79%255C%2525%2520for%2520the%2520binary%2520and%2520multi-class%250Aclassification%2520setups%252C%2520respectively.%2520The%2520results%2520of%2520our%2520proposed%2520method%2520are%250Asuperior%2520to%2520state-of-the-art%2520%2528SOTA%2529%2520methods.%2520The%2520code%2520of%2520the%2520proposed%2520model%250Awill%2520be%2520available%2520at%253A%2520https%253A//github.com/AyushRoy2001/FA-Net.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15117v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FA-Net%3A%20A%20Fuzzy%20Attention-aided%20Deep%20Neural%20Network%20for%20Pneumonia%0A%20%20Detection%20in%20Chest%20X-Rays&entry.906535625=Ayush%20Roy%20and%20Anurag%20Bhattacharjee%20and%20Diego%20Oliva%20and%20Oscar%20Ramos-Soto%20and%20Francisco%20J.%20Alvarez-Padilla%20and%20Ram%20Sarkar&entry.1292438233=%20%20Pneumonia%20is%20a%20respiratory%20infection%20caused%20by%20bacteria%2C%20fungi%2C%20or%20viruses.%0AIt%20affects%20many%20people%2C%20particularly%20those%20in%20developing%20or%20underdeveloped%0Anations%20with%20high%20pollution%20levels%2C%20unhygienic%20living%20conditions%2C%20overcrowding%2C%0Aand%20insufficient%20medical%20infrastructure.%20Pneumonia%20can%20cause%20pleural%20effusion%2C%0Awhere%20fluids%20fill%20the%20lungs%2C%20leading%20to%20respiratory%20difficulty.%20Early%20diagnosis%0Ais%20crucial%20to%20ensure%20effective%20treatment%20and%20increase%20survival%20rates.%20Chest%0AX-ray%20imaging%20is%20the%20most%20commonly%20used%20method%20for%20diagnosing%20pneumonia.%0AHowever%2C%20visual%20examination%20of%20chest%20X-rays%20can%20be%20difficult%20and%20subjective.%20In%0Athis%20study%2C%20we%20have%20developed%20a%20computer-aided%20diagnosis%20system%20for%20automatic%0Apneumonia%20detection%20using%20chest%20X-ray%20images.%20We%20have%20used%20DenseNet-121%20and%0AResNet50%20as%20the%20backbone%20for%20the%20binary%20class%20%28pneumonia%20and%20normal%29%20and%0Amulti-class%20%28bacterial%20pneumonia%2C%20viral%20pneumonia%2C%20and%20normal%29%20classification%0Atasks%2C%20respectively.%20We%20have%20also%20implemented%20a%20channel-specific%20spatial%0Aattention%20mechanism%2C%20called%20Fuzzy%20Channel%20Selective%20Spatial%20Attention%20Module%0A%28FCSSAM%29%2C%20to%20highlight%20the%20specific%20spatial%20regions%20of%20relevant%20channels%20while%0Aremoving%20the%20irrelevant%20channels%20of%20the%20extracted%20features%20by%20the%20backbone.%20We%0Aevaluated%20the%20proposed%20approach%20on%20a%20publicly%20available%20chest%20X-ray%20dataset%2C%0Ausing%20binary%20and%20multi-class%20classification%20setups.%20Our%20proposed%20method%0Aachieves%20accuracy%20rates%20of%2097.15%5C%25%20and%2079.79%5C%25%20for%20the%20binary%20and%20multi-class%0Aclassification%20setups%2C%20respectively.%20The%20results%20of%20our%20proposed%20method%20are%0Asuperior%20to%20state-of-the-art%20%28SOTA%29%20methods.%20The%20code%20of%20the%20proposed%20model%0Awill%20be%20available%20at%3A%20https%3A//github.com/AyushRoy2001/FA-Net.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15117v1&entry.124074799=Read"},
{"title": "MantisScore: Building Automatic Metrics to Simulate Fine-grained Human\n  Feedback for Video Generation", "author": "Xuan He and Dongfu Jiang and Ge Zhang and Max Ku and Achint Soni and Sherman Siu and Haonan Chen and Abhranil Chandra and Ziyan Jiang and Aaran Arulraj and Kai Wang and Quy Duc Do and Yuansheng Ni and Bohan Lyu and Yaswanth Narsupalli and Rongqi Fan and Zhiheng Lyu and Yuchen Lin and Wenhu Chen", "abstract": "  The recent years have witnessed great advances in video generation. However,\nthe development of automatic video metrics is lagging significantly behind.\nNone of the existing metric is able to provide reliable scores over generated\nvideos. The main barrier is the lack of large-scale human-annotated dataset. In\nthis paper, we release VideoFeedback, the first large-scale dataset containing\nhuman-provided multi-aspect score over 37.6K synthesized videos from 11\nexisting video generative models. We train MantisScore (initialized from\nMantis) based on VideoFeedback to enable automatic video quality assessment.\nExperiments show that the Spearman correlation between MantisScore and humans\ncan reach 77.1 on VideoFeedback-test, beating the prior best metrics by about\n50 points. Further result on other held-out EvalCrafter, GenAI-Bench, and\nVBench show that MantisScore has consistently much higher correlation with\nhuman judges than other metrics. Due to these results, we believe MantisScore\ncan serve as a great proxy for human raters to (1) rate different video models\nto track progress (2) simulate fine-grained human feedback in Reinforcement\nLearning with Human Feedback (RLHF) to improve current video generation models.\n", "link": "http://arxiv.org/abs/2406.15252v1", "date": "2024-06-21", "relevancy": 2.2304, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5637}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5623}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MantisScore%3A%20Building%20Automatic%20Metrics%20to%20Simulate%20Fine-grained%20Human%0A%20%20Feedback%20for%20Video%20Generation&body=Title%3A%20MantisScore%3A%20Building%20Automatic%20Metrics%20to%20Simulate%20Fine-grained%20Human%0A%20%20Feedback%20for%20Video%20Generation%0AAuthor%3A%20Xuan%20He%20and%20Dongfu%20Jiang%20and%20Ge%20Zhang%20and%20Max%20Ku%20and%20Achint%20Soni%20and%20Sherman%20Siu%20and%20Haonan%20Chen%20and%20Abhranil%20Chandra%20and%20Ziyan%20Jiang%20and%20Aaran%20Arulraj%20and%20Kai%20Wang%20and%20Quy%20Duc%20Do%20and%20Yuansheng%20Ni%20and%20Bohan%20Lyu%20and%20Yaswanth%20Narsupalli%20and%20Rongqi%20Fan%20and%20Zhiheng%20Lyu%20and%20Yuchen%20Lin%20and%20Wenhu%20Chen%0AAbstract%3A%20%20%20The%20recent%20years%20have%20witnessed%20great%20advances%20in%20video%20generation.%20However%2C%0Athe%20development%20of%20automatic%20video%20metrics%20is%20lagging%20significantly%20behind.%0ANone%20of%20the%20existing%20metric%20is%20able%20to%20provide%20reliable%20scores%20over%20generated%0Avideos.%20The%20main%20barrier%20is%20the%20lack%20of%20large-scale%20human-annotated%20dataset.%20In%0Athis%20paper%2C%20we%20release%20VideoFeedback%2C%20the%20first%20large-scale%20dataset%20containing%0Ahuman-provided%20multi-aspect%20score%20over%2037.6K%20synthesized%20videos%20from%2011%0Aexisting%20video%20generative%20models.%20We%20train%20MantisScore%20%28initialized%20from%0AMantis%29%20based%20on%20VideoFeedback%20to%20enable%20automatic%20video%20quality%20assessment.%0AExperiments%20show%20that%20the%20Spearman%20correlation%20between%20MantisScore%20and%20humans%0Acan%20reach%2077.1%20on%20VideoFeedback-test%2C%20beating%20the%20prior%20best%20metrics%20by%20about%0A50%20points.%20Further%20result%20on%20other%20held-out%20EvalCrafter%2C%20GenAI-Bench%2C%20and%0AVBench%20show%20that%20MantisScore%20has%20consistently%20much%20higher%20correlation%20with%0Ahuman%20judges%20than%20other%20metrics.%20Due%20to%20these%20results%2C%20we%20believe%20MantisScore%0Acan%20serve%20as%20a%20great%20proxy%20for%20human%20raters%20to%20%281%29%20rate%20different%20video%20models%0Ato%20track%20progress%20%282%29%20simulate%20fine-grained%20human%20feedback%20in%20Reinforcement%0ALearning%20with%20Human%20Feedback%20%28RLHF%29%20to%20improve%20current%20video%20generation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15252v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMantisScore%253A%2520Building%2520Automatic%2520Metrics%2520to%2520Simulate%2520Fine-grained%2520Human%250A%2520%2520Feedback%2520for%2520Video%2520Generation%26entry.906535625%3DXuan%2520He%2520and%2520Dongfu%2520Jiang%2520and%2520Ge%2520Zhang%2520and%2520Max%2520Ku%2520and%2520Achint%2520Soni%2520and%2520Sherman%2520Siu%2520and%2520Haonan%2520Chen%2520and%2520Abhranil%2520Chandra%2520and%2520Ziyan%2520Jiang%2520and%2520Aaran%2520Arulraj%2520and%2520Kai%2520Wang%2520and%2520Quy%2520Duc%2520Do%2520and%2520Yuansheng%2520Ni%2520and%2520Bohan%2520Lyu%2520and%2520Yaswanth%2520Narsupalli%2520and%2520Rongqi%2520Fan%2520and%2520Zhiheng%2520Lyu%2520and%2520Yuchen%2520Lin%2520and%2520Wenhu%2520Chen%26entry.1292438233%3D%2520%2520The%2520recent%2520years%2520have%2520witnessed%2520great%2520advances%2520in%2520video%2520generation.%2520However%252C%250Athe%2520development%2520of%2520automatic%2520video%2520metrics%2520is%2520lagging%2520significantly%2520behind.%250ANone%2520of%2520the%2520existing%2520metric%2520is%2520able%2520to%2520provide%2520reliable%2520scores%2520over%2520generated%250Avideos.%2520The%2520main%2520barrier%2520is%2520the%2520lack%2520of%2520large-scale%2520human-annotated%2520dataset.%2520In%250Athis%2520paper%252C%2520we%2520release%2520VideoFeedback%252C%2520the%2520first%2520large-scale%2520dataset%2520containing%250Ahuman-provided%2520multi-aspect%2520score%2520over%252037.6K%2520synthesized%2520videos%2520from%252011%250Aexisting%2520video%2520generative%2520models.%2520We%2520train%2520MantisScore%2520%2528initialized%2520from%250AMantis%2529%2520based%2520on%2520VideoFeedback%2520to%2520enable%2520automatic%2520video%2520quality%2520assessment.%250AExperiments%2520show%2520that%2520the%2520Spearman%2520correlation%2520between%2520MantisScore%2520and%2520humans%250Acan%2520reach%252077.1%2520on%2520VideoFeedback-test%252C%2520beating%2520the%2520prior%2520best%2520metrics%2520by%2520about%250A50%2520points.%2520Further%2520result%2520on%2520other%2520held-out%2520EvalCrafter%252C%2520GenAI-Bench%252C%2520and%250AVBench%2520show%2520that%2520MantisScore%2520has%2520consistently%2520much%2520higher%2520correlation%2520with%250Ahuman%2520judges%2520than%2520other%2520metrics.%2520Due%2520to%2520these%2520results%252C%2520we%2520believe%2520MantisScore%250Acan%2520serve%2520as%2520a%2520great%2520proxy%2520for%2520human%2520raters%2520to%2520%25281%2529%2520rate%2520different%2520video%2520models%250Ato%2520track%2520progress%2520%25282%2529%2520simulate%2520fine-grained%2520human%2520feedback%2520in%2520Reinforcement%250ALearning%2520with%2520Human%2520Feedback%2520%2528RLHF%2529%2520to%2520improve%2520current%2520video%2520generation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15252v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MantisScore%3A%20Building%20Automatic%20Metrics%20to%20Simulate%20Fine-grained%20Human%0A%20%20Feedback%20for%20Video%20Generation&entry.906535625=Xuan%20He%20and%20Dongfu%20Jiang%20and%20Ge%20Zhang%20and%20Max%20Ku%20and%20Achint%20Soni%20and%20Sherman%20Siu%20and%20Haonan%20Chen%20and%20Abhranil%20Chandra%20and%20Ziyan%20Jiang%20and%20Aaran%20Arulraj%20and%20Kai%20Wang%20and%20Quy%20Duc%20Do%20and%20Yuansheng%20Ni%20and%20Bohan%20Lyu%20and%20Yaswanth%20Narsupalli%20and%20Rongqi%20Fan%20and%20Zhiheng%20Lyu%20and%20Yuchen%20Lin%20and%20Wenhu%20Chen&entry.1292438233=%20%20The%20recent%20years%20have%20witnessed%20great%20advances%20in%20video%20generation.%20However%2C%0Athe%20development%20of%20automatic%20video%20metrics%20is%20lagging%20significantly%20behind.%0ANone%20of%20the%20existing%20metric%20is%20able%20to%20provide%20reliable%20scores%20over%20generated%0Avideos.%20The%20main%20barrier%20is%20the%20lack%20of%20large-scale%20human-annotated%20dataset.%20In%0Athis%20paper%2C%20we%20release%20VideoFeedback%2C%20the%20first%20large-scale%20dataset%20containing%0Ahuman-provided%20multi-aspect%20score%20over%2037.6K%20synthesized%20videos%20from%2011%0Aexisting%20video%20generative%20models.%20We%20train%20MantisScore%20%28initialized%20from%0AMantis%29%20based%20on%20VideoFeedback%20to%20enable%20automatic%20video%20quality%20assessment.%0AExperiments%20show%20that%20the%20Spearman%20correlation%20between%20MantisScore%20and%20humans%0Acan%20reach%2077.1%20on%20VideoFeedback-test%2C%20beating%20the%20prior%20best%20metrics%20by%20about%0A50%20points.%20Further%20result%20on%20other%20held-out%20EvalCrafter%2C%20GenAI-Bench%2C%20and%0AVBench%20show%20that%20MantisScore%20has%20consistently%20much%20higher%20correlation%20with%0Ahuman%20judges%20than%20other%20metrics.%20Due%20to%20these%20results%2C%20we%20believe%20MantisScore%0Acan%20serve%20as%20a%20great%20proxy%20for%20human%20raters%20to%20%281%29%20rate%20different%20video%20models%0Ato%20track%20progress%20%282%29%20simulate%20fine-grained%20human%20feedback%20in%20Reinforcement%0ALearning%20with%20Human%20Feedback%20%28RLHF%29%20to%20improve%20current%20video%20generation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15252v1&entry.124074799=Read"},
{"title": "Generative Topological Networks", "author": "Alona Levy-Jurgenson and Zohar Yakhini", "abstract": "  Generative models have seen significant advancements in recent years, yet\noften remain challenging and costly to train and use. We introduce Generative\nTopological Networks (GTNs) -- a new class of generative models that addresses\nthese shortcomings. GTNs are trained deterministically using a simple\nsupervised learning approach grounded in topology theory. GTNs are fast to\ntrain, and require only a single forward pass in a standard feedforward neural\nnetwork to generate samples. We demonstrate the strengths of GTNs in several\ndatasets, including MNIST, celebA and the Hands and Palm Images dataset.\nFinally, the theory behind GTNs offers insights into how to train generative\nmodels for improved performance.\n", "link": "http://arxiv.org/abs/2406.15152v1", "date": "2024-06-21", "relevancy": 2.2185, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5816}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5399}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5241}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Topological%20Networks&body=Title%3A%20Generative%20Topological%20Networks%0AAuthor%3A%20Alona%20Levy-Jurgenson%20and%20Zohar%20Yakhini%0AAbstract%3A%20%20%20Generative%20models%20have%20seen%20significant%20advancements%20in%20recent%20years%2C%20yet%0Aoften%20remain%20challenging%20and%20costly%20to%20train%20and%20use.%20We%20introduce%20Generative%0ATopological%20Networks%20%28GTNs%29%20--%20a%20new%20class%20of%20generative%20models%20that%20addresses%0Athese%20shortcomings.%20GTNs%20are%20trained%20deterministically%20using%20a%20simple%0Asupervised%20learning%20approach%20grounded%20in%20topology%20theory.%20GTNs%20are%20fast%20to%0Atrain%2C%20and%20require%20only%20a%20single%20forward%20pass%20in%20a%20standard%20feedforward%20neural%0Anetwork%20to%20generate%20samples.%20We%20demonstrate%20the%20strengths%20of%20GTNs%20in%20several%0Adatasets%2C%20including%20MNIST%2C%20celebA%20and%20the%20Hands%20and%20Palm%20Images%20dataset.%0AFinally%2C%20the%20theory%20behind%20GTNs%20offers%20insights%20into%20how%20to%20train%20generative%0Amodels%20for%20improved%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15152v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Topological%2520Networks%26entry.906535625%3DAlona%2520Levy-Jurgenson%2520and%2520Zohar%2520Yakhini%26entry.1292438233%3D%2520%2520Generative%2520models%2520have%2520seen%2520significant%2520advancements%2520in%2520recent%2520years%252C%2520yet%250Aoften%2520remain%2520challenging%2520and%2520costly%2520to%2520train%2520and%2520use.%2520We%2520introduce%2520Generative%250ATopological%2520Networks%2520%2528GTNs%2529%2520--%2520a%2520new%2520class%2520of%2520generative%2520models%2520that%2520addresses%250Athese%2520shortcomings.%2520GTNs%2520are%2520trained%2520deterministically%2520using%2520a%2520simple%250Asupervised%2520learning%2520approach%2520grounded%2520in%2520topology%2520theory.%2520GTNs%2520are%2520fast%2520to%250Atrain%252C%2520and%2520require%2520only%2520a%2520single%2520forward%2520pass%2520in%2520a%2520standard%2520feedforward%2520neural%250Anetwork%2520to%2520generate%2520samples.%2520We%2520demonstrate%2520the%2520strengths%2520of%2520GTNs%2520in%2520several%250Adatasets%252C%2520including%2520MNIST%252C%2520celebA%2520and%2520the%2520Hands%2520and%2520Palm%2520Images%2520dataset.%250AFinally%252C%2520the%2520theory%2520behind%2520GTNs%2520offers%2520insights%2520into%2520how%2520to%2520train%2520generative%250Amodels%2520for%2520improved%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15152v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Topological%20Networks&entry.906535625=Alona%20Levy-Jurgenson%20and%20Zohar%20Yakhini&entry.1292438233=%20%20Generative%20models%20have%20seen%20significant%20advancements%20in%20recent%20years%2C%20yet%0Aoften%20remain%20challenging%20and%20costly%20to%20train%20and%20use.%20We%20introduce%20Generative%0ATopological%20Networks%20%28GTNs%29%20--%20a%20new%20class%20of%20generative%20models%20that%20addresses%0Athese%20shortcomings.%20GTNs%20are%20trained%20deterministically%20using%20a%20simple%0Asupervised%20learning%20approach%20grounded%20in%20topology%20theory.%20GTNs%20are%20fast%20to%0Atrain%2C%20and%20require%20only%20a%20single%20forward%20pass%20in%20a%20standard%20feedforward%20neural%0Anetwork%20to%20generate%20samples.%20We%20demonstrate%20the%20strengths%20of%20GTNs%20in%20several%0Adatasets%2C%20including%20MNIST%2C%20celebA%20and%20the%20Hands%20and%20Palm%20Images%20dataset.%0AFinally%2C%20the%20theory%20behind%20GTNs%20offers%20insights%20into%20how%20to%20train%20generative%0Amodels%20for%20improved%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15152v1&entry.124074799=Read"},
{"title": "Investigating the impact of 2D gesture representation on co-speech\n  gesture generation", "author": "Teo Guichoux and Laure Soulier and Nicolas Obin and Catherine Pelachaud", "abstract": "  Co-speech gestures play a crucial role in the interactions between humans and\nembodied conversational agents (ECA). Recent deep learning methods enable the\ngeneration of realistic, natural co-speech gestures synchronized with speech,\nbut such approaches require large amounts of training data. \"In-the-wild\"\ndatasets, which compile videos from sources such as YouTube through human pose\ndetection models, offer a solution by providing 2D skeleton sequences that are\npaired with speech. Concurrently, innovative lifting models have emerged,\ncapable of transforming these 2D pose sequences into their 3D counterparts,\nleading to large and diverse datasets of 3D gestures. However, the derived 3D\npose estimation is essentially a pseudo-ground truth, with the actual ground\ntruth being the 2D motion data. This distinction raises questions about the\nimpact of gesture representation dimensionality on the quality of generated\nmotions, a topic that, to our knowledge, remains largely unexplored. In this\nwork, we evaluate the impact of the dimensionality of the training data, 2D or\n3D joint coordinates, on the performance of a multimodal speech-to-gesture deep\ngenerative model. We use a lifting model to convert 2D-generated sequences of\nbody pose to 3D. Then, we compare the sequence of gestures generated directly\nin 3D to the gestures generated in 2D and lifted to 3D as post-processing.\n", "link": "http://arxiv.org/abs/2406.15111v1", "date": "2024-06-21", "relevancy": 2.1687, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5671}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5249}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20the%20impact%20of%202D%20gesture%20representation%20on%20co-speech%0A%20%20gesture%20generation&body=Title%3A%20Investigating%20the%20impact%20of%202D%20gesture%20representation%20on%20co-speech%0A%20%20gesture%20generation%0AAuthor%3A%20Teo%20Guichoux%20and%20Laure%20Soulier%20and%20Nicolas%20Obin%20and%20Catherine%20Pelachaud%0AAbstract%3A%20%20%20Co-speech%20gestures%20play%20a%20crucial%20role%20in%20the%20interactions%20between%20humans%20and%0Aembodied%20conversational%20agents%20%28ECA%29.%20Recent%20deep%20learning%20methods%20enable%20the%0Ageneration%20of%20realistic%2C%20natural%20co-speech%20gestures%20synchronized%20with%20speech%2C%0Abut%20such%20approaches%20require%20large%20amounts%20of%20training%20data.%20%22In-the-wild%22%0Adatasets%2C%20which%20compile%20videos%20from%20sources%20such%20as%20YouTube%20through%20human%20pose%0Adetection%20models%2C%20offer%20a%20solution%20by%20providing%202D%20skeleton%20sequences%20that%20are%0Apaired%20with%20speech.%20Concurrently%2C%20innovative%20lifting%20models%20have%20emerged%2C%0Acapable%20of%20transforming%20these%202D%20pose%20sequences%20into%20their%203D%20counterparts%2C%0Aleading%20to%20large%20and%20diverse%20datasets%20of%203D%20gestures.%20However%2C%20the%20derived%203D%0Apose%20estimation%20is%20essentially%20a%20pseudo-ground%20truth%2C%20with%20the%20actual%20ground%0Atruth%20being%20the%202D%20motion%20data.%20This%20distinction%20raises%20questions%20about%20the%0Aimpact%20of%20gesture%20representation%20dimensionality%20on%20the%20quality%20of%20generated%0Amotions%2C%20a%20topic%20that%2C%20to%20our%20knowledge%2C%20remains%20largely%20unexplored.%20In%20this%0Awork%2C%20we%20evaluate%20the%20impact%20of%20the%20dimensionality%20of%20the%20training%20data%2C%202D%20or%0A3D%20joint%20coordinates%2C%20on%20the%20performance%20of%20a%20multimodal%20speech-to-gesture%20deep%0Agenerative%20model.%20We%20use%20a%20lifting%20model%20to%20convert%202D-generated%20sequences%20of%0Abody%20pose%20to%203D.%20Then%2C%20we%20compare%20the%20sequence%20of%20gestures%20generated%20directly%0Ain%203D%20to%20the%20gestures%20generated%20in%202D%20and%20lifted%20to%203D%20as%20post-processing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15111v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520the%2520impact%2520of%25202D%2520gesture%2520representation%2520on%2520co-speech%250A%2520%2520gesture%2520generation%26entry.906535625%3DTeo%2520Guichoux%2520and%2520Laure%2520Soulier%2520and%2520Nicolas%2520Obin%2520and%2520Catherine%2520Pelachaud%26entry.1292438233%3D%2520%2520Co-speech%2520gestures%2520play%2520a%2520crucial%2520role%2520in%2520the%2520interactions%2520between%2520humans%2520and%250Aembodied%2520conversational%2520agents%2520%2528ECA%2529.%2520Recent%2520deep%2520learning%2520methods%2520enable%2520the%250Ageneration%2520of%2520realistic%252C%2520natural%2520co-speech%2520gestures%2520synchronized%2520with%2520speech%252C%250Abut%2520such%2520approaches%2520require%2520large%2520amounts%2520of%2520training%2520data.%2520%2522In-the-wild%2522%250Adatasets%252C%2520which%2520compile%2520videos%2520from%2520sources%2520such%2520as%2520YouTube%2520through%2520human%2520pose%250Adetection%2520models%252C%2520offer%2520a%2520solution%2520by%2520providing%25202D%2520skeleton%2520sequences%2520that%2520are%250Apaired%2520with%2520speech.%2520Concurrently%252C%2520innovative%2520lifting%2520models%2520have%2520emerged%252C%250Acapable%2520of%2520transforming%2520these%25202D%2520pose%2520sequences%2520into%2520their%25203D%2520counterparts%252C%250Aleading%2520to%2520large%2520and%2520diverse%2520datasets%2520of%25203D%2520gestures.%2520However%252C%2520the%2520derived%25203D%250Apose%2520estimation%2520is%2520essentially%2520a%2520pseudo-ground%2520truth%252C%2520with%2520the%2520actual%2520ground%250Atruth%2520being%2520the%25202D%2520motion%2520data.%2520This%2520distinction%2520raises%2520questions%2520about%2520the%250Aimpact%2520of%2520gesture%2520representation%2520dimensionality%2520on%2520the%2520quality%2520of%2520generated%250Amotions%252C%2520a%2520topic%2520that%252C%2520to%2520our%2520knowledge%252C%2520remains%2520largely%2520unexplored.%2520In%2520this%250Awork%252C%2520we%2520evaluate%2520the%2520impact%2520of%2520the%2520dimensionality%2520of%2520the%2520training%2520data%252C%25202D%2520or%250A3D%2520joint%2520coordinates%252C%2520on%2520the%2520performance%2520of%2520a%2520multimodal%2520speech-to-gesture%2520deep%250Agenerative%2520model.%2520We%2520use%2520a%2520lifting%2520model%2520to%2520convert%25202D-generated%2520sequences%2520of%250Abody%2520pose%2520to%25203D.%2520Then%252C%2520we%2520compare%2520the%2520sequence%2520of%2520gestures%2520generated%2520directly%250Ain%25203D%2520to%2520the%2520gestures%2520generated%2520in%25202D%2520and%2520lifted%2520to%25203D%2520as%2520post-processing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15111v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20the%20impact%20of%202D%20gesture%20representation%20on%20co-speech%0A%20%20gesture%20generation&entry.906535625=Teo%20Guichoux%20and%20Laure%20Soulier%20and%20Nicolas%20Obin%20and%20Catherine%20Pelachaud&entry.1292438233=%20%20Co-speech%20gestures%20play%20a%20crucial%20role%20in%20the%20interactions%20between%20humans%20and%0Aembodied%20conversational%20agents%20%28ECA%29.%20Recent%20deep%20learning%20methods%20enable%20the%0Ageneration%20of%20realistic%2C%20natural%20co-speech%20gestures%20synchronized%20with%20speech%2C%0Abut%20such%20approaches%20require%20large%20amounts%20of%20training%20data.%20%22In-the-wild%22%0Adatasets%2C%20which%20compile%20videos%20from%20sources%20such%20as%20YouTube%20through%20human%20pose%0Adetection%20models%2C%20offer%20a%20solution%20by%20providing%202D%20skeleton%20sequences%20that%20are%0Apaired%20with%20speech.%20Concurrently%2C%20innovative%20lifting%20models%20have%20emerged%2C%0Acapable%20of%20transforming%20these%202D%20pose%20sequences%20into%20their%203D%20counterparts%2C%0Aleading%20to%20large%20and%20diverse%20datasets%20of%203D%20gestures.%20However%2C%20the%20derived%203D%0Apose%20estimation%20is%20essentially%20a%20pseudo-ground%20truth%2C%20with%20the%20actual%20ground%0Atruth%20being%20the%202D%20motion%20data.%20This%20distinction%20raises%20questions%20about%20the%0Aimpact%20of%20gesture%20representation%20dimensionality%20on%20the%20quality%20of%20generated%0Amotions%2C%20a%20topic%20that%2C%20to%20our%20knowledge%2C%20remains%20largely%20unexplored.%20In%20this%0Awork%2C%20we%20evaluate%20the%20impact%20of%20the%20dimensionality%20of%20the%20training%20data%2C%202D%20or%0A3D%20joint%20coordinates%2C%20on%20the%20performance%20of%20a%20multimodal%20speech-to-gesture%20deep%0Agenerative%20model.%20We%20use%20a%20lifting%20model%20to%20convert%202D-generated%20sequences%20of%0Abody%20pose%20to%203D.%20Then%2C%20we%20compare%20the%20sequence%20of%20gestures%20generated%20directly%0Ain%203D%20to%20the%20gestures%20generated%20in%202D%20and%20lifted%20to%203D%20as%20post-processing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15111v1&entry.124074799=Read"},
{"title": "GOAL: A Generalist Combinatorial Optimization Agent Learner", "author": "Darko Drakulic and Sofia Michel and Jean-Marc Andreoli", "abstract": "  Machine Learning-based heuristics have recently shown impressive performance\nin solving a variety of hard combinatorial optimization problems (COPs).\nHowever they generally rely on a separate neural model, specialized and trained\nfor each single problem. Any variation of a problem requires adjustment of its\nmodel and re-training from scratch. In this paper, we propose GOAL (for\nGeneralist combinatorial Optimization Agent Learning), a generalist model\ncapable of efficiently solving multiple COPs and which can be fine-tuned to\nsolve new COPs. GOAL consists of a single backbone plus light-weight\nproblem-specific adapters, mostly for input and output processing. The backbone\nis based on a new form of mixed-attention blocks which allows to handle\nproblems defined on graphs with arbitrary combinations of node, edge and\ninstance-level features. Additionally, problems which involve heterogeneous\nnodes or edges, such as in multi-partite graphs, are handled through a novel\nmulti-type transformer architecture, where the attention blocks are duplicated\nto attend only the relevant combination of types while relying on the same\nshared parameters. We train GOAL on a set of routing, scheduling and classic\ngraph problems and show that it is only slightly inferior to the specialized\nbaselines while being the first multi-task model that solves a variety of COPs.\nFinally, we showcase the strong transfer learning capacity of GOAL by\nfine-tuning or learning the adapters for new problems, with only few shots and\nlittle data.\n", "link": "http://arxiv.org/abs/2406.15079v1", "date": "2024-06-21", "relevancy": 2.1389, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.549}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.533}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5211}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GOAL%3A%20A%20Generalist%20Combinatorial%20Optimization%20Agent%20Learner&body=Title%3A%20GOAL%3A%20A%20Generalist%20Combinatorial%20Optimization%20Agent%20Learner%0AAuthor%3A%20Darko%20Drakulic%20and%20Sofia%20Michel%20and%20Jean-Marc%20Andreoli%0AAbstract%3A%20%20%20Machine%20Learning-based%20heuristics%20have%20recently%20shown%20impressive%20performance%0Ain%20solving%20a%20variety%20of%20hard%20combinatorial%20optimization%20problems%20%28COPs%29.%0AHowever%20they%20generally%20rely%20on%20a%20separate%20neural%20model%2C%20specialized%20and%20trained%0Afor%20each%20single%20problem.%20Any%20variation%20of%20a%20problem%20requires%20adjustment%20of%20its%0Amodel%20and%20re-training%20from%20scratch.%20In%20this%20paper%2C%20we%20propose%20GOAL%20%28for%0AGeneralist%20combinatorial%20Optimization%20Agent%20Learning%29%2C%20a%20generalist%20model%0Acapable%20of%20efficiently%20solving%20multiple%20COPs%20and%20which%20can%20be%20fine-tuned%20to%0Asolve%20new%20COPs.%20GOAL%20consists%20of%20a%20single%20backbone%20plus%20light-weight%0Aproblem-specific%20adapters%2C%20mostly%20for%20input%20and%20output%20processing.%20The%20backbone%0Ais%20based%20on%20a%20new%20form%20of%20mixed-attention%20blocks%20which%20allows%20to%20handle%0Aproblems%20defined%20on%20graphs%20with%20arbitrary%20combinations%20of%20node%2C%20edge%20and%0Ainstance-level%20features.%20Additionally%2C%20problems%20which%20involve%20heterogeneous%0Anodes%20or%20edges%2C%20such%20as%20in%20multi-partite%20graphs%2C%20are%20handled%20through%20a%20novel%0Amulti-type%20transformer%20architecture%2C%20where%20the%20attention%20blocks%20are%20duplicated%0Ato%20attend%20only%20the%20relevant%20combination%20of%20types%20while%20relying%20on%20the%20same%0Ashared%20parameters.%20We%20train%20GOAL%20on%20a%20set%20of%20routing%2C%20scheduling%20and%20classic%0Agraph%20problems%20and%20show%20that%20it%20is%20only%20slightly%20inferior%20to%20the%20specialized%0Abaselines%20while%20being%20the%20first%20multi-task%20model%20that%20solves%20a%20variety%20of%20COPs.%0AFinally%2C%20we%20showcase%20the%20strong%20transfer%20learning%20capacity%20of%20GOAL%20by%0Afine-tuning%20or%20learning%20the%20adapters%20for%20new%20problems%2C%20with%20only%20few%20shots%20and%0Alittle%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15079v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGOAL%253A%2520A%2520Generalist%2520Combinatorial%2520Optimization%2520Agent%2520Learner%26entry.906535625%3DDarko%2520Drakulic%2520and%2520Sofia%2520Michel%2520and%2520Jean-Marc%2520Andreoli%26entry.1292438233%3D%2520%2520Machine%2520Learning-based%2520heuristics%2520have%2520recently%2520shown%2520impressive%2520performance%250Ain%2520solving%2520a%2520variety%2520of%2520hard%2520combinatorial%2520optimization%2520problems%2520%2528COPs%2529.%250AHowever%2520they%2520generally%2520rely%2520on%2520a%2520separate%2520neural%2520model%252C%2520specialized%2520and%2520trained%250Afor%2520each%2520single%2520problem.%2520Any%2520variation%2520of%2520a%2520problem%2520requires%2520adjustment%2520of%2520its%250Amodel%2520and%2520re-training%2520from%2520scratch.%2520In%2520this%2520paper%252C%2520we%2520propose%2520GOAL%2520%2528for%250AGeneralist%2520combinatorial%2520Optimization%2520Agent%2520Learning%2529%252C%2520a%2520generalist%2520model%250Acapable%2520of%2520efficiently%2520solving%2520multiple%2520COPs%2520and%2520which%2520can%2520be%2520fine-tuned%2520to%250Asolve%2520new%2520COPs.%2520GOAL%2520consists%2520of%2520a%2520single%2520backbone%2520plus%2520light-weight%250Aproblem-specific%2520adapters%252C%2520mostly%2520for%2520input%2520and%2520output%2520processing.%2520The%2520backbone%250Ais%2520based%2520on%2520a%2520new%2520form%2520of%2520mixed-attention%2520blocks%2520which%2520allows%2520to%2520handle%250Aproblems%2520defined%2520on%2520graphs%2520with%2520arbitrary%2520combinations%2520of%2520node%252C%2520edge%2520and%250Ainstance-level%2520features.%2520Additionally%252C%2520problems%2520which%2520involve%2520heterogeneous%250Anodes%2520or%2520edges%252C%2520such%2520as%2520in%2520multi-partite%2520graphs%252C%2520are%2520handled%2520through%2520a%2520novel%250Amulti-type%2520transformer%2520architecture%252C%2520where%2520the%2520attention%2520blocks%2520are%2520duplicated%250Ato%2520attend%2520only%2520the%2520relevant%2520combination%2520of%2520types%2520while%2520relying%2520on%2520the%2520same%250Ashared%2520parameters.%2520We%2520train%2520GOAL%2520on%2520a%2520set%2520of%2520routing%252C%2520scheduling%2520and%2520classic%250Agraph%2520problems%2520and%2520show%2520that%2520it%2520is%2520only%2520slightly%2520inferior%2520to%2520the%2520specialized%250Abaselines%2520while%2520being%2520the%2520first%2520multi-task%2520model%2520that%2520solves%2520a%2520variety%2520of%2520COPs.%250AFinally%252C%2520we%2520showcase%2520the%2520strong%2520transfer%2520learning%2520capacity%2520of%2520GOAL%2520by%250Afine-tuning%2520or%2520learning%2520the%2520adapters%2520for%2520new%2520problems%252C%2520with%2520only%2520few%2520shots%2520and%250Alittle%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15079v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GOAL%3A%20A%20Generalist%20Combinatorial%20Optimization%20Agent%20Learner&entry.906535625=Darko%20Drakulic%20and%20Sofia%20Michel%20and%20Jean-Marc%20Andreoli&entry.1292438233=%20%20Machine%20Learning-based%20heuristics%20have%20recently%20shown%20impressive%20performance%0Ain%20solving%20a%20variety%20of%20hard%20combinatorial%20optimization%20problems%20%28COPs%29.%0AHowever%20they%20generally%20rely%20on%20a%20separate%20neural%20model%2C%20specialized%20and%20trained%0Afor%20each%20single%20problem.%20Any%20variation%20of%20a%20problem%20requires%20adjustment%20of%20its%0Amodel%20and%20re-training%20from%20scratch.%20In%20this%20paper%2C%20we%20propose%20GOAL%20%28for%0AGeneralist%20combinatorial%20Optimization%20Agent%20Learning%29%2C%20a%20generalist%20model%0Acapable%20of%20efficiently%20solving%20multiple%20COPs%20and%20which%20can%20be%20fine-tuned%20to%0Asolve%20new%20COPs.%20GOAL%20consists%20of%20a%20single%20backbone%20plus%20light-weight%0Aproblem-specific%20adapters%2C%20mostly%20for%20input%20and%20output%20processing.%20The%20backbone%0Ais%20based%20on%20a%20new%20form%20of%20mixed-attention%20blocks%20which%20allows%20to%20handle%0Aproblems%20defined%20on%20graphs%20with%20arbitrary%20combinations%20of%20node%2C%20edge%20and%0Ainstance-level%20features.%20Additionally%2C%20problems%20which%20involve%20heterogeneous%0Anodes%20or%20edges%2C%20such%20as%20in%20multi-partite%20graphs%2C%20are%20handled%20through%20a%20novel%0Amulti-type%20transformer%20architecture%2C%20where%20the%20attention%20blocks%20are%20duplicated%0Ato%20attend%20only%20the%20relevant%20combination%20of%20types%20while%20relying%20on%20the%20same%0Ashared%20parameters.%20We%20train%20GOAL%20on%20a%20set%20of%20routing%2C%20scheduling%20and%20classic%0Agraph%20problems%20and%20show%20that%20it%20is%20only%20slightly%20inferior%20to%20the%20specialized%0Abaselines%20while%20being%20the%20first%20multi-task%20model%20that%20solves%20a%20variety%20of%20COPs.%0AFinally%2C%20we%20showcase%20the%20strong%20transfer%20learning%20capacity%20of%20GOAL%20by%0Afine-tuning%20or%20learning%20the%20adapters%20for%20new%20problems%2C%20with%20only%20few%20shots%20and%0Alittle%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15079v1&entry.124074799=Read"},
{"title": "V-RECS, a Low-Cost LLM4VIS Recommender with Explanations, Captioning and\n  Suggestions", "author": "Luca Podo and Marco Angelini and Paola Velardi", "abstract": "  NL2VIS (natural language to visualization) is a promising and recent research\narea that involves interpreting natural language queries and translating them\ninto visualizations that accurately represent the underlying data. As we\nnavigate the era of big data, NL2VIS holds considerable application potential\nsince it greatly facilitates data exploration by non-expert users. Following\nthe increasingly widespread usage of generative AI in NL2VIS applications, in\nthis paper we present V-RECS, the first LLM-based Visual Recommender augmented\nwith explanations(E), captioning(C), and suggestions(S) for further data\nexploration. V-RECS' visualization narratives facilitate both response\nverification and data exploration by non-expert users. Furthermore, our\nproposed solution mitigates computational, controllability, and cost issues\nassociated with using powerful LLMs by leveraging a methodology to effectively\nfine-tune small models. To generate insightful visualization narratives, we use\nChain-of-Thoughts (CoT), a prompt engineering technique to help LLM identify\nand generate the logical steps to produce a correct answer. Since CoT is\nreported to perform poorly with small LLMs, we adopted a strategy in which a\nlarge LLM (GPT-4), acting as a Teacher, generates CoT-based instructions to\nfine-tune a small model, Llama-2-7B, which plays the role of a Student.\nExtensive experiments-based on a framework for the quantitative evaluation of\nAI-based visualizations and on manual assessment by a group of\nparticipants-show that V-RECS achieves performance scores comparable to GPT-4,\nat a much lower cost. The efficacy of the V-RECS teacher-student paradigm is\nalso demonstrated by the fact that the un-tuned Llama fails to perform the task\nin the vast majority of test cases. We release V-RECS for the visualization\ncommunity to assist visualization designers throughout the entire visualization\ngeneration process.\n", "link": "http://arxiv.org/abs/2406.15259v1", "date": "2024-06-21", "relevancy": 2.1175, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5344}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5302}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20V-RECS%2C%20a%20Low-Cost%20LLM4VIS%20Recommender%20with%20Explanations%2C%20Captioning%20and%0A%20%20Suggestions&body=Title%3A%20V-RECS%2C%20a%20Low-Cost%20LLM4VIS%20Recommender%20with%20Explanations%2C%20Captioning%20and%0A%20%20Suggestions%0AAuthor%3A%20Luca%20Podo%20and%20Marco%20Angelini%20and%20Paola%20Velardi%0AAbstract%3A%20%20%20NL2VIS%20%28natural%20language%20to%20visualization%29%20is%20a%20promising%20and%20recent%20research%0Aarea%20that%20involves%20interpreting%20natural%20language%20queries%20and%20translating%20them%0Ainto%20visualizations%20that%20accurately%20represent%20the%20underlying%20data.%20As%20we%0Anavigate%20the%20era%20of%20big%20data%2C%20NL2VIS%20holds%20considerable%20application%20potential%0Asince%20it%20greatly%20facilitates%20data%20exploration%20by%20non-expert%20users.%20Following%0Athe%20increasingly%20widespread%20usage%20of%20generative%20AI%20in%20NL2VIS%20applications%2C%20in%0Athis%20paper%20we%20present%20V-RECS%2C%20the%20first%20LLM-based%20Visual%20Recommender%20augmented%0Awith%20explanations%28E%29%2C%20captioning%28C%29%2C%20and%20suggestions%28S%29%20for%20further%20data%0Aexploration.%20V-RECS%27%20visualization%20narratives%20facilitate%20both%20response%0Averification%20and%20data%20exploration%20by%20non-expert%20users.%20Furthermore%2C%20our%0Aproposed%20solution%20mitigates%20computational%2C%20controllability%2C%20and%20cost%20issues%0Aassociated%20with%20using%20powerful%20LLMs%20by%20leveraging%20a%20methodology%20to%20effectively%0Afine-tune%20small%20models.%20To%20generate%20insightful%20visualization%20narratives%2C%20we%20use%0AChain-of-Thoughts%20%28CoT%29%2C%20a%20prompt%20engineering%20technique%20to%20help%20LLM%20identify%0Aand%20generate%20the%20logical%20steps%20to%20produce%20a%20correct%20answer.%20Since%20CoT%20is%0Areported%20to%20perform%20poorly%20with%20small%20LLMs%2C%20we%20adopted%20a%20strategy%20in%20which%20a%0Alarge%20LLM%20%28GPT-4%29%2C%20acting%20as%20a%20Teacher%2C%20generates%20CoT-based%20instructions%20to%0Afine-tune%20a%20small%20model%2C%20Llama-2-7B%2C%20which%20plays%20the%20role%20of%20a%20Student.%0AExtensive%20experiments-based%20on%20a%20framework%20for%20the%20quantitative%20evaluation%20of%0AAI-based%20visualizations%20and%20on%20manual%20assessment%20by%20a%20group%20of%0Aparticipants-show%20that%20V-RECS%20achieves%20performance%20scores%20comparable%20to%20GPT-4%2C%0Aat%20a%20much%20lower%20cost.%20The%20efficacy%20of%20the%20V-RECS%20teacher-student%20paradigm%20is%0Aalso%20demonstrated%20by%20the%20fact%20that%20the%20un-tuned%20Llama%20fails%20to%20perform%20the%20task%0Ain%20the%20vast%20majority%20of%20test%20cases.%20We%20release%20V-RECS%20for%20the%20visualization%0Acommunity%20to%20assist%20visualization%20designers%20throughout%20the%20entire%20visualization%0Ageneration%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15259v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DV-RECS%252C%2520a%2520Low-Cost%2520LLM4VIS%2520Recommender%2520with%2520Explanations%252C%2520Captioning%2520and%250A%2520%2520Suggestions%26entry.906535625%3DLuca%2520Podo%2520and%2520Marco%2520Angelini%2520and%2520Paola%2520Velardi%26entry.1292438233%3D%2520%2520NL2VIS%2520%2528natural%2520language%2520to%2520visualization%2529%2520is%2520a%2520promising%2520and%2520recent%2520research%250Aarea%2520that%2520involves%2520interpreting%2520natural%2520language%2520queries%2520and%2520translating%2520them%250Ainto%2520visualizations%2520that%2520accurately%2520represent%2520the%2520underlying%2520data.%2520As%2520we%250Anavigate%2520the%2520era%2520of%2520big%2520data%252C%2520NL2VIS%2520holds%2520considerable%2520application%2520potential%250Asince%2520it%2520greatly%2520facilitates%2520data%2520exploration%2520by%2520non-expert%2520users.%2520Following%250Athe%2520increasingly%2520widespread%2520usage%2520of%2520generative%2520AI%2520in%2520NL2VIS%2520applications%252C%2520in%250Athis%2520paper%2520we%2520present%2520V-RECS%252C%2520the%2520first%2520LLM-based%2520Visual%2520Recommender%2520augmented%250Awith%2520explanations%2528E%2529%252C%2520captioning%2528C%2529%252C%2520and%2520suggestions%2528S%2529%2520for%2520further%2520data%250Aexploration.%2520V-RECS%2527%2520visualization%2520narratives%2520facilitate%2520both%2520response%250Averification%2520and%2520data%2520exploration%2520by%2520non-expert%2520users.%2520Furthermore%252C%2520our%250Aproposed%2520solution%2520mitigates%2520computational%252C%2520controllability%252C%2520and%2520cost%2520issues%250Aassociated%2520with%2520using%2520powerful%2520LLMs%2520by%2520leveraging%2520a%2520methodology%2520to%2520effectively%250Afine-tune%2520small%2520models.%2520To%2520generate%2520insightful%2520visualization%2520narratives%252C%2520we%2520use%250AChain-of-Thoughts%2520%2528CoT%2529%252C%2520a%2520prompt%2520engineering%2520technique%2520to%2520help%2520LLM%2520identify%250Aand%2520generate%2520the%2520logical%2520steps%2520to%2520produce%2520a%2520correct%2520answer.%2520Since%2520CoT%2520is%250Areported%2520to%2520perform%2520poorly%2520with%2520small%2520LLMs%252C%2520we%2520adopted%2520a%2520strategy%2520in%2520which%2520a%250Alarge%2520LLM%2520%2528GPT-4%2529%252C%2520acting%2520as%2520a%2520Teacher%252C%2520generates%2520CoT-based%2520instructions%2520to%250Afine-tune%2520a%2520small%2520model%252C%2520Llama-2-7B%252C%2520which%2520plays%2520the%2520role%2520of%2520a%2520Student.%250AExtensive%2520experiments-based%2520on%2520a%2520framework%2520for%2520the%2520quantitative%2520evaluation%2520of%250AAI-based%2520visualizations%2520and%2520on%2520manual%2520assessment%2520by%2520a%2520group%2520of%250Aparticipants-show%2520that%2520V-RECS%2520achieves%2520performance%2520scores%2520comparable%2520to%2520GPT-4%252C%250Aat%2520a%2520much%2520lower%2520cost.%2520The%2520efficacy%2520of%2520the%2520V-RECS%2520teacher-student%2520paradigm%2520is%250Aalso%2520demonstrated%2520by%2520the%2520fact%2520that%2520the%2520un-tuned%2520Llama%2520fails%2520to%2520perform%2520the%2520task%250Ain%2520the%2520vast%2520majority%2520of%2520test%2520cases.%2520We%2520release%2520V-RECS%2520for%2520the%2520visualization%250Acommunity%2520to%2520assist%2520visualization%2520designers%2520throughout%2520the%2520entire%2520visualization%250Ageneration%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15259v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V-RECS%2C%20a%20Low-Cost%20LLM4VIS%20Recommender%20with%20Explanations%2C%20Captioning%20and%0A%20%20Suggestions&entry.906535625=Luca%20Podo%20and%20Marco%20Angelini%20and%20Paola%20Velardi&entry.1292438233=%20%20NL2VIS%20%28natural%20language%20to%20visualization%29%20is%20a%20promising%20and%20recent%20research%0Aarea%20that%20involves%20interpreting%20natural%20language%20queries%20and%20translating%20them%0Ainto%20visualizations%20that%20accurately%20represent%20the%20underlying%20data.%20As%20we%0Anavigate%20the%20era%20of%20big%20data%2C%20NL2VIS%20holds%20considerable%20application%20potential%0Asince%20it%20greatly%20facilitates%20data%20exploration%20by%20non-expert%20users.%20Following%0Athe%20increasingly%20widespread%20usage%20of%20generative%20AI%20in%20NL2VIS%20applications%2C%20in%0Athis%20paper%20we%20present%20V-RECS%2C%20the%20first%20LLM-based%20Visual%20Recommender%20augmented%0Awith%20explanations%28E%29%2C%20captioning%28C%29%2C%20and%20suggestions%28S%29%20for%20further%20data%0Aexploration.%20V-RECS%27%20visualization%20narratives%20facilitate%20both%20response%0Averification%20and%20data%20exploration%20by%20non-expert%20users.%20Furthermore%2C%20our%0Aproposed%20solution%20mitigates%20computational%2C%20controllability%2C%20and%20cost%20issues%0Aassociated%20with%20using%20powerful%20LLMs%20by%20leveraging%20a%20methodology%20to%20effectively%0Afine-tune%20small%20models.%20To%20generate%20insightful%20visualization%20narratives%2C%20we%20use%0AChain-of-Thoughts%20%28CoT%29%2C%20a%20prompt%20engineering%20technique%20to%20help%20LLM%20identify%0Aand%20generate%20the%20logical%20steps%20to%20produce%20a%20correct%20answer.%20Since%20CoT%20is%0Areported%20to%20perform%20poorly%20with%20small%20LLMs%2C%20we%20adopted%20a%20strategy%20in%20which%20a%0Alarge%20LLM%20%28GPT-4%29%2C%20acting%20as%20a%20Teacher%2C%20generates%20CoT-based%20instructions%20to%0Afine-tune%20a%20small%20model%2C%20Llama-2-7B%2C%20which%20plays%20the%20role%20of%20a%20Student.%0AExtensive%20experiments-based%20on%20a%20framework%20for%20the%20quantitative%20evaluation%20of%0AAI-based%20visualizations%20and%20on%20manual%20assessment%20by%20a%20group%20of%0Aparticipants-show%20that%20V-RECS%20achieves%20performance%20scores%20comparable%20to%20GPT-4%2C%0Aat%20a%20much%20lower%20cost.%20The%20efficacy%20of%20the%20V-RECS%20teacher-student%20paradigm%20is%0Aalso%20demonstrated%20by%20the%20fact%20that%20the%20un-tuned%20Llama%20fails%20to%20perform%20the%20task%0Ain%20the%20vast%20majority%20of%20test%20cases.%20We%20release%20V-RECS%20for%20the%20visualization%0Acommunity%20to%20assist%20visualization%20designers%20throughout%20the%20entire%20visualization%0Ageneration%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15259v1&entry.124074799=Read"},
{"title": "Multimodal Deformable Image Registration for Long-COVID Analysis Based\n  on Progressive Alignment and Multi-perspective Loss", "author": "Jiahua Li and James T. Grist and Fergus V. Gleeson and Bart\u0142omiej W. Papie\u017c", "abstract": "  Long COVID is characterized by persistent symptoms, particularly pulmonary\nimpairment, which necessitates advanced imaging for accurate diagnosis.\nHyperpolarised Xenon-129 MRI (XeMRI) offers a promising avenue by visualising\nlung ventilation, perfusion, as well as gas transfer. Integrating functional\ndata from XeMRI with structural data from Computed Tomography (CT) is crucial\nfor comprehensive analysis and effective treatment strategies in long COVID,\nrequiring precise data alignment from those complementary imaging modalities.\nTo this end, CT-MRI registration is an essential intermediate step, given the\nsignificant challenges posed by the direct alignment of CT and Xe-MRI.\nTherefore, we proposed an end-to-end multimodal deformable image registration\nmethod that achieves superior performance for aligning long-COVID lung CT and\nproton density MRI (pMRI) data. Moreover, our method incorporates a novel\nMulti-perspective Loss (MPL) function, enhancing state-of-the-art deep learning\nmethods for monomodal registration by making them adaptable for multimodal\ntasks. The registration results achieve a Dice coefficient score of 0.913,\nindicating a substantial improvement over the state-of-the-art multimodal image\nregistration techniques. Since the XeMRI and pMRI images are acquired in the\nsame sessions and can be roughly aligned, our results facilitate subsequent\nregistration between XeMRI and CT, thereby potentially enhancing clinical\ndecision-making for long COVID management.\n", "link": "http://arxiv.org/abs/2406.15172v1", "date": "2024-06-21", "relevancy": 2.1159, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5445}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5251}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Deformable%20Image%20Registration%20for%20Long-COVID%20Analysis%20Based%0A%20%20on%20Progressive%20Alignment%20and%20Multi-perspective%20Loss&body=Title%3A%20Multimodal%20Deformable%20Image%20Registration%20for%20Long-COVID%20Analysis%20Based%0A%20%20on%20Progressive%20Alignment%20and%20Multi-perspective%20Loss%0AAuthor%3A%20Jiahua%20Li%20and%20James%20T.%20Grist%20and%20Fergus%20V.%20Gleeson%20and%20Bart%C5%82omiej%20W.%20Papie%C5%BC%0AAbstract%3A%20%20%20Long%20COVID%20is%20characterized%20by%20persistent%20symptoms%2C%20particularly%20pulmonary%0Aimpairment%2C%20which%20necessitates%20advanced%20imaging%20for%20accurate%20diagnosis.%0AHyperpolarised%20Xenon-129%20MRI%20%28XeMRI%29%20offers%20a%20promising%20avenue%20by%20visualising%0Alung%20ventilation%2C%20perfusion%2C%20as%20well%20as%20gas%20transfer.%20Integrating%20functional%0Adata%20from%20XeMRI%20with%20structural%20data%20from%20Computed%20Tomography%20%28CT%29%20is%20crucial%0Afor%20comprehensive%20analysis%20and%20effective%20treatment%20strategies%20in%20long%20COVID%2C%0Arequiring%20precise%20data%20alignment%20from%20those%20complementary%20imaging%20modalities.%0ATo%20this%20end%2C%20CT-MRI%20registration%20is%20an%20essential%20intermediate%20step%2C%20given%20the%0Asignificant%20challenges%20posed%20by%20the%20direct%20alignment%20of%20CT%20and%20Xe-MRI.%0ATherefore%2C%20we%20proposed%20an%20end-to-end%20multimodal%20deformable%20image%20registration%0Amethod%20that%20achieves%20superior%20performance%20for%20aligning%20long-COVID%20lung%20CT%20and%0Aproton%20density%20MRI%20%28pMRI%29%20data.%20Moreover%2C%20our%20method%20incorporates%20a%20novel%0AMulti-perspective%20Loss%20%28MPL%29%20function%2C%20enhancing%20state-of-the-art%20deep%20learning%0Amethods%20for%20monomodal%20registration%20by%20making%20them%20adaptable%20for%20multimodal%0Atasks.%20The%20registration%20results%20achieve%20a%20Dice%20coefficient%20score%20of%200.913%2C%0Aindicating%20a%20substantial%20improvement%20over%20the%20state-of-the-art%20multimodal%20image%0Aregistration%20techniques.%20Since%20the%20XeMRI%20and%20pMRI%20images%20are%20acquired%20in%20the%0Asame%20sessions%20and%20can%20be%20roughly%20aligned%2C%20our%20results%20facilitate%20subsequent%0Aregistration%20between%20XeMRI%20and%20CT%2C%20thereby%20potentially%20enhancing%20clinical%0Adecision-making%20for%20long%20COVID%20management.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15172v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Deformable%2520Image%2520Registration%2520for%2520Long-COVID%2520Analysis%2520Based%250A%2520%2520on%2520Progressive%2520Alignment%2520and%2520Multi-perspective%2520Loss%26entry.906535625%3DJiahua%2520Li%2520and%2520James%2520T.%2520Grist%2520and%2520Fergus%2520V.%2520Gleeson%2520and%2520Bart%25C5%2582omiej%2520W.%2520Papie%25C5%25BC%26entry.1292438233%3D%2520%2520Long%2520COVID%2520is%2520characterized%2520by%2520persistent%2520symptoms%252C%2520particularly%2520pulmonary%250Aimpairment%252C%2520which%2520necessitates%2520advanced%2520imaging%2520for%2520accurate%2520diagnosis.%250AHyperpolarised%2520Xenon-129%2520MRI%2520%2528XeMRI%2529%2520offers%2520a%2520promising%2520avenue%2520by%2520visualising%250Alung%2520ventilation%252C%2520perfusion%252C%2520as%2520well%2520as%2520gas%2520transfer.%2520Integrating%2520functional%250Adata%2520from%2520XeMRI%2520with%2520structural%2520data%2520from%2520Computed%2520Tomography%2520%2528CT%2529%2520is%2520crucial%250Afor%2520comprehensive%2520analysis%2520and%2520effective%2520treatment%2520strategies%2520in%2520long%2520COVID%252C%250Arequiring%2520precise%2520data%2520alignment%2520from%2520those%2520complementary%2520imaging%2520modalities.%250ATo%2520this%2520end%252C%2520CT-MRI%2520registration%2520is%2520an%2520essential%2520intermediate%2520step%252C%2520given%2520the%250Asignificant%2520challenges%2520posed%2520by%2520the%2520direct%2520alignment%2520of%2520CT%2520and%2520Xe-MRI.%250ATherefore%252C%2520we%2520proposed%2520an%2520end-to-end%2520multimodal%2520deformable%2520image%2520registration%250Amethod%2520that%2520achieves%2520superior%2520performance%2520for%2520aligning%2520long-COVID%2520lung%2520CT%2520and%250Aproton%2520density%2520MRI%2520%2528pMRI%2529%2520data.%2520Moreover%252C%2520our%2520method%2520incorporates%2520a%2520novel%250AMulti-perspective%2520Loss%2520%2528MPL%2529%2520function%252C%2520enhancing%2520state-of-the-art%2520deep%2520learning%250Amethods%2520for%2520monomodal%2520registration%2520by%2520making%2520them%2520adaptable%2520for%2520multimodal%250Atasks.%2520The%2520registration%2520results%2520achieve%2520a%2520Dice%2520coefficient%2520score%2520of%25200.913%252C%250Aindicating%2520a%2520substantial%2520improvement%2520over%2520the%2520state-of-the-art%2520multimodal%2520image%250Aregistration%2520techniques.%2520Since%2520the%2520XeMRI%2520and%2520pMRI%2520images%2520are%2520acquired%2520in%2520the%250Asame%2520sessions%2520and%2520can%2520be%2520roughly%2520aligned%252C%2520our%2520results%2520facilitate%2520subsequent%250Aregistration%2520between%2520XeMRI%2520and%2520CT%252C%2520thereby%2520potentially%2520enhancing%2520clinical%250Adecision-making%2520for%2520long%2520COVID%2520management.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15172v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Deformable%20Image%20Registration%20for%20Long-COVID%20Analysis%20Based%0A%20%20on%20Progressive%20Alignment%20and%20Multi-perspective%20Loss&entry.906535625=Jiahua%20Li%20and%20James%20T.%20Grist%20and%20Fergus%20V.%20Gleeson%20and%20Bart%C5%82omiej%20W.%20Papie%C5%BC&entry.1292438233=%20%20Long%20COVID%20is%20characterized%20by%20persistent%20symptoms%2C%20particularly%20pulmonary%0Aimpairment%2C%20which%20necessitates%20advanced%20imaging%20for%20accurate%20diagnosis.%0AHyperpolarised%20Xenon-129%20MRI%20%28XeMRI%29%20offers%20a%20promising%20avenue%20by%20visualising%0Alung%20ventilation%2C%20perfusion%2C%20as%20well%20as%20gas%20transfer.%20Integrating%20functional%0Adata%20from%20XeMRI%20with%20structural%20data%20from%20Computed%20Tomography%20%28CT%29%20is%20crucial%0Afor%20comprehensive%20analysis%20and%20effective%20treatment%20strategies%20in%20long%20COVID%2C%0Arequiring%20precise%20data%20alignment%20from%20those%20complementary%20imaging%20modalities.%0ATo%20this%20end%2C%20CT-MRI%20registration%20is%20an%20essential%20intermediate%20step%2C%20given%20the%0Asignificant%20challenges%20posed%20by%20the%20direct%20alignment%20of%20CT%20and%20Xe-MRI.%0ATherefore%2C%20we%20proposed%20an%20end-to-end%20multimodal%20deformable%20image%20registration%0Amethod%20that%20achieves%20superior%20performance%20for%20aligning%20long-COVID%20lung%20CT%20and%0Aproton%20density%20MRI%20%28pMRI%29%20data.%20Moreover%2C%20our%20method%20incorporates%20a%20novel%0AMulti-perspective%20Loss%20%28MPL%29%20function%2C%20enhancing%20state-of-the-art%20deep%20learning%0Amethods%20for%20monomodal%20registration%20by%20making%20them%20adaptable%20for%20multimodal%0Atasks.%20The%20registration%20results%20achieve%20a%20Dice%20coefficient%20score%20of%200.913%2C%0Aindicating%20a%20substantial%20improvement%20over%20the%20state-of-the-art%20multimodal%20image%0Aregistration%20techniques.%20Since%20the%20XeMRI%20and%20pMRI%20images%20are%20acquired%20in%20the%0Asame%20sessions%20and%20can%20be%20roughly%20aligned%2C%20our%20results%20facilitate%20subsequent%0Aregistration%20between%20XeMRI%20and%20CT%2C%20thereby%20potentially%20enhancing%20clinical%0Adecision-making%20for%20long%20COVID%20management.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15172v1&entry.124074799=Read"},
{"title": "AGLA: Mitigating Object Hallucinations in Large Vision-Language Models\n  with Assembly of Global and Local Attention", "author": "Wenbin An and Feng Tian and Sicong Leng and Jiahao Nie and Haonan Lin and QianYing Wang and Guang Dai and Ping Chen and Shijian Lu", "abstract": "  Despite their great success across various multimodal tasks, Large\nVision-Language Models (LVLMs) are facing a prevalent problem with object\nhallucinations, where the generated textual responses are inconsistent with\nground-truth objects in the given image. This paper investigates various LVLMs\nand pinpoints attention deficiency toward discriminative local image features\nas one root cause of object hallucinations. Specifically, LVLMs predominantly\nattend to prompt-independent global image features, while failing to capture\nprompt-relevant local features, consequently undermining the visual grounding\ncapacity of LVLMs and leading to hallucinations. To this end, we propose\nAssembly of Global and Local Attention (AGLA), a training-free and\nplug-and-play approach that mitigates object hallucinations by exploring an\nensemble of global features for response generation and local features for\nvisual discrimination simultaneously. Our approach exhibits an image-prompt\nmatching scheme that captures prompt-relevant local features from images,\nleading to an augmented view of the input image where prompt-relevant content\nis reserved while irrelevant distractions are masked. With the augmented view,\na calibrated decoding distribution can be derived by integrating generative\nglobal features from the original image and discriminative local features from\nthe augmented image. Extensive experiments show that AGLA consistently\nmitigates object hallucinations and enhances general perception capability for\nLVLMs across various discriminative and generative benchmarks. Our code will be\nreleased at https://github.com/Lackel/AGLA.\n", "link": "http://arxiv.org/abs/2406.12718v2", "date": "2024-06-21", "relevancy": 2.1046, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5286}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5283}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AGLA%3A%20Mitigating%20Object%20Hallucinations%20in%20Large%20Vision-Language%20Models%0A%20%20with%20Assembly%20of%20Global%20and%20Local%20Attention&body=Title%3A%20AGLA%3A%20Mitigating%20Object%20Hallucinations%20in%20Large%20Vision-Language%20Models%0A%20%20with%20Assembly%20of%20Global%20and%20Local%20Attention%0AAuthor%3A%20Wenbin%20An%20and%20Feng%20Tian%20and%20Sicong%20Leng%20and%20Jiahao%20Nie%20and%20Haonan%20Lin%20and%20QianYing%20Wang%20and%20Guang%20Dai%20and%20Ping%20Chen%20and%20Shijian%20Lu%0AAbstract%3A%20%20%20Despite%20their%20great%20success%20across%20various%20multimodal%20tasks%2C%20Large%0AVision-Language%20Models%20%28LVLMs%29%20are%20facing%20a%20prevalent%20problem%20with%20object%0Ahallucinations%2C%20where%20the%20generated%20textual%20responses%20are%20inconsistent%20with%0Aground-truth%20objects%20in%20the%20given%20image.%20This%20paper%20investigates%20various%20LVLMs%0Aand%20pinpoints%20attention%20deficiency%20toward%20discriminative%20local%20image%20features%0Aas%20one%20root%20cause%20of%20object%20hallucinations.%20Specifically%2C%20LVLMs%20predominantly%0Aattend%20to%20prompt-independent%20global%20image%20features%2C%20while%20failing%20to%20capture%0Aprompt-relevant%20local%20features%2C%20consequently%20undermining%20the%20visual%20grounding%0Acapacity%20of%20LVLMs%20and%20leading%20to%20hallucinations.%20To%20this%20end%2C%20we%20propose%0AAssembly%20of%20Global%20and%20Local%20Attention%20%28AGLA%29%2C%20a%20training-free%20and%0Aplug-and-play%20approach%20that%20mitigates%20object%20hallucinations%20by%20exploring%20an%0Aensemble%20of%20global%20features%20for%20response%20generation%20and%20local%20features%20for%0Avisual%20discrimination%20simultaneously.%20Our%20approach%20exhibits%20an%20image-prompt%0Amatching%20scheme%20that%20captures%20prompt-relevant%20local%20features%20from%20images%2C%0Aleading%20to%20an%20augmented%20view%20of%20the%20input%20image%20where%20prompt-relevant%20content%0Ais%20reserved%20while%20irrelevant%20distractions%20are%20masked.%20With%20the%20augmented%20view%2C%0Aa%20calibrated%20decoding%20distribution%20can%20be%20derived%20by%20integrating%20generative%0Aglobal%20features%20from%20the%20original%20image%20and%20discriminative%20local%20features%20from%0Athe%20augmented%20image.%20Extensive%20experiments%20show%20that%20AGLA%20consistently%0Amitigates%20object%20hallucinations%20and%20enhances%20general%20perception%20capability%20for%0ALVLMs%20across%20various%20discriminative%20and%20generative%20benchmarks.%20Our%20code%20will%20be%0Areleased%20at%20https%3A//github.com/Lackel/AGLA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12718v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAGLA%253A%2520Mitigating%2520Object%2520Hallucinations%2520in%2520Large%2520Vision-Language%2520Models%250A%2520%2520with%2520Assembly%2520of%2520Global%2520and%2520Local%2520Attention%26entry.906535625%3DWenbin%2520An%2520and%2520Feng%2520Tian%2520and%2520Sicong%2520Leng%2520and%2520Jiahao%2520Nie%2520and%2520Haonan%2520Lin%2520and%2520QianYing%2520Wang%2520and%2520Guang%2520Dai%2520and%2520Ping%2520Chen%2520and%2520Shijian%2520Lu%26entry.1292438233%3D%2520%2520Despite%2520their%2520great%2520success%2520across%2520various%2520multimodal%2520tasks%252C%2520Large%250AVision-Language%2520Models%2520%2528LVLMs%2529%2520are%2520facing%2520a%2520prevalent%2520problem%2520with%2520object%250Ahallucinations%252C%2520where%2520the%2520generated%2520textual%2520responses%2520are%2520inconsistent%2520with%250Aground-truth%2520objects%2520in%2520the%2520given%2520image.%2520This%2520paper%2520investigates%2520various%2520LVLMs%250Aand%2520pinpoints%2520attention%2520deficiency%2520toward%2520discriminative%2520local%2520image%2520features%250Aas%2520one%2520root%2520cause%2520of%2520object%2520hallucinations.%2520Specifically%252C%2520LVLMs%2520predominantly%250Aattend%2520to%2520prompt-independent%2520global%2520image%2520features%252C%2520while%2520failing%2520to%2520capture%250Aprompt-relevant%2520local%2520features%252C%2520consequently%2520undermining%2520the%2520visual%2520grounding%250Acapacity%2520of%2520LVLMs%2520and%2520leading%2520to%2520hallucinations.%2520To%2520this%2520end%252C%2520we%2520propose%250AAssembly%2520of%2520Global%2520and%2520Local%2520Attention%2520%2528AGLA%2529%252C%2520a%2520training-free%2520and%250Aplug-and-play%2520approach%2520that%2520mitigates%2520object%2520hallucinations%2520by%2520exploring%2520an%250Aensemble%2520of%2520global%2520features%2520for%2520response%2520generation%2520and%2520local%2520features%2520for%250Avisual%2520discrimination%2520simultaneously.%2520Our%2520approach%2520exhibits%2520an%2520image-prompt%250Amatching%2520scheme%2520that%2520captures%2520prompt-relevant%2520local%2520features%2520from%2520images%252C%250Aleading%2520to%2520an%2520augmented%2520view%2520of%2520the%2520input%2520image%2520where%2520prompt-relevant%2520content%250Ais%2520reserved%2520while%2520irrelevant%2520distractions%2520are%2520masked.%2520With%2520the%2520augmented%2520view%252C%250Aa%2520calibrated%2520decoding%2520distribution%2520can%2520be%2520derived%2520by%2520integrating%2520generative%250Aglobal%2520features%2520from%2520the%2520original%2520image%2520and%2520discriminative%2520local%2520features%2520from%250Athe%2520augmented%2520image.%2520Extensive%2520experiments%2520show%2520that%2520AGLA%2520consistently%250Amitigates%2520object%2520hallucinations%2520and%2520enhances%2520general%2520perception%2520capability%2520for%250ALVLMs%2520across%2520various%2520discriminative%2520and%2520generative%2520benchmarks.%2520Our%2520code%2520will%2520be%250Areleased%2520at%2520https%253A//github.com/Lackel/AGLA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12718v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AGLA%3A%20Mitigating%20Object%20Hallucinations%20in%20Large%20Vision-Language%20Models%0A%20%20with%20Assembly%20of%20Global%20and%20Local%20Attention&entry.906535625=Wenbin%20An%20and%20Feng%20Tian%20and%20Sicong%20Leng%20and%20Jiahao%20Nie%20and%20Haonan%20Lin%20and%20QianYing%20Wang%20and%20Guang%20Dai%20and%20Ping%20Chen%20and%20Shijian%20Lu&entry.1292438233=%20%20Despite%20their%20great%20success%20across%20various%20multimodal%20tasks%2C%20Large%0AVision-Language%20Models%20%28LVLMs%29%20are%20facing%20a%20prevalent%20problem%20with%20object%0Ahallucinations%2C%20where%20the%20generated%20textual%20responses%20are%20inconsistent%20with%0Aground-truth%20objects%20in%20the%20given%20image.%20This%20paper%20investigates%20various%20LVLMs%0Aand%20pinpoints%20attention%20deficiency%20toward%20discriminative%20local%20image%20features%0Aas%20one%20root%20cause%20of%20object%20hallucinations.%20Specifically%2C%20LVLMs%20predominantly%0Aattend%20to%20prompt-independent%20global%20image%20features%2C%20while%20failing%20to%20capture%0Aprompt-relevant%20local%20features%2C%20consequently%20undermining%20the%20visual%20grounding%0Acapacity%20of%20LVLMs%20and%20leading%20to%20hallucinations.%20To%20this%20end%2C%20we%20propose%0AAssembly%20of%20Global%20and%20Local%20Attention%20%28AGLA%29%2C%20a%20training-free%20and%0Aplug-and-play%20approach%20that%20mitigates%20object%20hallucinations%20by%20exploring%20an%0Aensemble%20of%20global%20features%20for%20response%20generation%20and%20local%20features%20for%0Avisual%20discrimination%20simultaneously.%20Our%20approach%20exhibits%20an%20image-prompt%0Amatching%20scheme%20that%20captures%20prompt-relevant%20local%20features%20from%20images%2C%0Aleading%20to%20an%20augmented%20view%20of%20the%20input%20image%20where%20prompt-relevant%20content%0Ais%20reserved%20while%20irrelevant%20distractions%20are%20masked.%20With%20the%20augmented%20view%2C%0Aa%20calibrated%20decoding%20distribution%20can%20be%20derived%20by%20integrating%20generative%0Aglobal%20features%20from%20the%20original%20image%20and%20discriminative%20local%20features%20from%0Athe%20augmented%20image.%20Extensive%20experiments%20show%20that%20AGLA%20consistently%0Amitigates%20object%20hallucinations%20and%20enhances%20general%20perception%20capability%20for%0ALVLMs%20across%20various%20discriminative%20and%20generative%20benchmarks.%20Our%20code%20will%20be%0Areleased%20at%20https%3A//github.com/Lackel/AGLA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12718v2&entry.124074799=Read"},
{"title": "Injecting Bias in Text-To-Image Models via Composite-Trigger Backdoors", "author": "Ali Naseh and Jaechul Roh and Eugene Bagdasaryan and Amir Houmansadr", "abstract": "  Recent advances in large text-conditional image generative models such as\nStable Diffusion, Midjourney, and DALL-E 3 have revolutionized the field of\nimage generation, allowing users to produce high-quality, realistic images from\ntextual prompts. While these developments have enhanced artistic creation and\nvisual communication, they also present an underexplored attack opportunity:\nthe possibility of inducing biases by an adversary into the generated images\nfor malicious intentions, e.g., to influence society and spread propaganda. In\nthis paper, we demonstrate the possibility of such a bias injection threat by\nan adversary who backdoors such models with a small number of malicious data\nsamples; the implemented backdoor is activated when special triggers exist in\nthe input prompt of the backdoored models. On the other hand, the model's\nutility is preserved in the absence of the triggers, making the attack highly\nundetectable. We present a novel framework that enables efficient generation of\npoisoning samples with composite (multi-word) triggers for such an attack. Our\nextensive experiments using over 1 million generated images and against\nhundreds of fine-tuned models demonstrate the feasibility of the presented\nbackdoor attack. We illustrate how these biases can bypass conventional\ndetection mechanisms, highlighting the challenges in proving the existence of\nbiases within operational constraints. Our cost analysis confirms the low\nfinancial barrier to executing such attacks, underscoring the need for robust\ndefensive strategies against such vulnerabilities in text-to-image generation\nmodels.\n", "link": "http://arxiv.org/abs/2406.15213v1", "date": "2024-06-21", "relevancy": 2.104, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5308}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.528}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5204}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Injecting%20Bias%20in%20Text-To-Image%20Models%20via%20Composite-Trigger%20Backdoors&body=Title%3A%20Injecting%20Bias%20in%20Text-To-Image%20Models%20via%20Composite-Trigger%20Backdoors%0AAuthor%3A%20Ali%20Naseh%20and%20Jaechul%20Roh%20and%20Eugene%20Bagdasaryan%20and%20Amir%20Houmansadr%0AAbstract%3A%20%20%20Recent%20advances%20in%20large%20text-conditional%20image%20generative%20models%20such%20as%0AStable%20Diffusion%2C%20Midjourney%2C%20and%20DALL-E%203%20have%20revolutionized%20the%20field%20of%0Aimage%20generation%2C%20allowing%20users%20to%20produce%20high-quality%2C%20realistic%20images%20from%0Atextual%20prompts.%20While%20these%20developments%20have%20enhanced%20artistic%20creation%20and%0Avisual%20communication%2C%20they%20also%20present%20an%20underexplored%20attack%20opportunity%3A%0Athe%20possibility%20of%20inducing%20biases%20by%20an%20adversary%20into%20the%20generated%20images%0Afor%20malicious%20intentions%2C%20e.g.%2C%20to%20influence%20society%20and%20spread%20propaganda.%20In%0Athis%20paper%2C%20we%20demonstrate%20the%20possibility%20of%20such%20a%20bias%20injection%20threat%20by%0Aan%20adversary%20who%20backdoors%20such%20models%20with%20a%20small%20number%20of%20malicious%20data%0Asamples%3B%20the%20implemented%20backdoor%20is%20activated%20when%20special%20triggers%20exist%20in%0Athe%20input%20prompt%20of%20the%20backdoored%20models.%20On%20the%20other%20hand%2C%20the%20model%27s%0Autility%20is%20preserved%20in%20the%20absence%20of%20the%20triggers%2C%20making%20the%20attack%20highly%0Aundetectable.%20We%20present%20a%20novel%20framework%20that%20enables%20efficient%20generation%20of%0Apoisoning%20samples%20with%20composite%20%28multi-word%29%20triggers%20for%20such%20an%20attack.%20Our%0Aextensive%20experiments%20using%20over%201%20million%20generated%20images%20and%20against%0Ahundreds%20of%20fine-tuned%20models%20demonstrate%20the%20feasibility%20of%20the%20presented%0Abackdoor%20attack.%20We%20illustrate%20how%20these%20biases%20can%20bypass%20conventional%0Adetection%20mechanisms%2C%20highlighting%20the%20challenges%20in%20proving%20the%20existence%20of%0Abiases%20within%20operational%20constraints.%20Our%20cost%20analysis%20confirms%20the%20low%0Afinancial%20barrier%20to%20executing%20such%20attacks%2C%20underscoring%20the%20need%20for%20robust%0Adefensive%20strategies%20against%20such%20vulnerabilities%20in%20text-to-image%20generation%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15213v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInjecting%2520Bias%2520in%2520Text-To-Image%2520Models%2520via%2520Composite-Trigger%2520Backdoors%26entry.906535625%3DAli%2520Naseh%2520and%2520Jaechul%2520Roh%2520and%2520Eugene%2520Bagdasaryan%2520and%2520Amir%2520Houmansadr%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520large%2520text-conditional%2520image%2520generative%2520models%2520such%2520as%250AStable%2520Diffusion%252C%2520Midjourney%252C%2520and%2520DALL-E%25203%2520have%2520revolutionized%2520the%2520field%2520of%250Aimage%2520generation%252C%2520allowing%2520users%2520to%2520produce%2520high-quality%252C%2520realistic%2520images%2520from%250Atextual%2520prompts.%2520While%2520these%2520developments%2520have%2520enhanced%2520artistic%2520creation%2520and%250Avisual%2520communication%252C%2520they%2520also%2520present%2520an%2520underexplored%2520attack%2520opportunity%253A%250Athe%2520possibility%2520of%2520inducing%2520biases%2520by%2520an%2520adversary%2520into%2520the%2520generated%2520images%250Afor%2520malicious%2520intentions%252C%2520e.g.%252C%2520to%2520influence%2520society%2520and%2520spread%2520propaganda.%2520In%250Athis%2520paper%252C%2520we%2520demonstrate%2520the%2520possibility%2520of%2520such%2520a%2520bias%2520injection%2520threat%2520by%250Aan%2520adversary%2520who%2520backdoors%2520such%2520models%2520with%2520a%2520small%2520number%2520of%2520malicious%2520data%250Asamples%253B%2520the%2520implemented%2520backdoor%2520is%2520activated%2520when%2520special%2520triggers%2520exist%2520in%250Athe%2520input%2520prompt%2520of%2520the%2520backdoored%2520models.%2520On%2520the%2520other%2520hand%252C%2520the%2520model%2527s%250Autility%2520is%2520preserved%2520in%2520the%2520absence%2520of%2520the%2520triggers%252C%2520making%2520the%2520attack%2520highly%250Aundetectable.%2520We%2520present%2520a%2520novel%2520framework%2520that%2520enables%2520efficient%2520generation%2520of%250Apoisoning%2520samples%2520with%2520composite%2520%2528multi-word%2529%2520triggers%2520for%2520such%2520an%2520attack.%2520Our%250Aextensive%2520experiments%2520using%2520over%25201%2520million%2520generated%2520images%2520and%2520against%250Ahundreds%2520of%2520fine-tuned%2520models%2520demonstrate%2520the%2520feasibility%2520of%2520the%2520presented%250Abackdoor%2520attack.%2520We%2520illustrate%2520how%2520these%2520biases%2520can%2520bypass%2520conventional%250Adetection%2520mechanisms%252C%2520highlighting%2520the%2520challenges%2520in%2520proving%2520the%2520existence%2520of%250Abiases%2520within%2520operational%2520constraints.%2520Our%2520cost%2520analysis%2520confirms%2520the%2520low%250Afinancial%2520barrier%2520to%2520executing%2520such%2520attacks%252C%2520underscoring%2520the%2520need%2520for%2520robust%250Adefensive%2520strategies%2520against%2520such%2520vulnerabilities%2520in%2520text-to-image%2520generation%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15213v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Injecting%20Bias%20in%20Text-To-Image%20Models%20via%20Composite-Trigger%20Backdoors&entry.906535625=Ali%20Naseh%20and%20Jaechul%20Roh%20and%20Eugene%20Bagdasaryan%20and%20Amir%20Houmansadr&entry.1292438233=%20%20Recent%20advances%20in%20large%20text-conditional%20image%20generative%20models%20such%20as%0AStable%20Diffusion%2C%20Midjourney%2C%20and%20DALL-E%203%20have%20revolutionized%20the%20field%20of%0Aimage%20generation%2C%20allowing%20users%20to%20produce%20high-quality%2C%20realistic%20images%20from%0Atextual%20prompts.%20While%20these%20developments%20have%20enhanced%20artistic%20creation%20and%0Avisual%20communication%2C%20they%20also%20present%20an%20underexplored%20attack%20opportunity%3A%0Athe%20possibility%20of%20inducing%20biases%20by%20an%20adversary%20into%20the%20generated%20images%0Afor%20malicious%20intentions%2C%20e.g.%2C%20to%20influence%20society%20and%20spread%20propaganda.%20In%0Athis%20paper%2C%20we%20demonstrate%20the%20possibility%20of%20such%20a%20bias%20injection%20threat%20by%0Aan%20adversary%20who%20backdoors%20such%20models%20with%20a%20small%20number%20of%20malicious%20data%0Asamples%3B%20the%20implemented%20backdoor%20is%20activated%20when%20special%20triggers%20exist%20in%0Athe%20input%20prompt%20of%20the%20backdoored%20models.%20On%20the%20other%20hand%2C%20the%20model%27s%0Autility%20is%20preserved%20in%20the%20absence%20of%20the%20triggers%2C%20making%20the%20attack%20highly%0Aundetectable.%20We%20present%20a%20novel%20framework%20that%20enables%20efficient%20generation%20of%0Apoisoning%20samples%20with%20composite%20%28multi-word%29%20triggers%20for%20such%20an%20attack.%20Our%0Aextensive%20experiments%20using%20over%201%20million%20generated%20images%20and%20against%0Ahundreds%20of%20fine-tuned%20models%20demonstrate%20the%20feasibility%20of%20the%20presented%0Abackdoor%20attack.%20We%20illustrate%20how%20these%20biases%20can%20bypass%20conventional%0Adetection%20mechanisms%2C%20highlighting%20the%20challenges%20in%20proving%20the%20existence%20of%0Abiases%20within%20operational%20constraints.%20Our%20cost%20analysis%20confirms%20the%20low%0Afinancial%20barrier%20to%20executing%20such%20attacks%2C%20underscoring%20the%20need%20for%20robust%0Adefensive%20strategies%20against%20such%20vulnerabilities%20in%20text-to-image%20generation%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15213v1&entry.124074799=Read"},
{"title": "Behaviour Distillation", "author": "Andrei Lupu and Chris Lu and Jarek Liesen and Robert Tjarko Lange and Jakob Foerster", "abstract": "  Dataset distillation aims to condense large datasets into a small number of\nsynthetic examples that can be used as drop-in replacements when training new\nmodels. It has applications to interpretability, neural architecture search,\nprivacy, and continual learning. Despite strong successes in supervised\ndomains, such methods have not yet been extended to reinforcement learning,\nwhere the lack of a fixed dataset renders most distillation methods unusable.\nFilling the gap, we formalize behaviour distillation, a setting that aims to\ndiscover and then condense the information required for training an expert\npolicy into a synthetic dataset of state-action pairs, without access to expert\ndata. We then introduce Hallucinating Datasets with Evolution Strategies\n(HaDES), a method for behaviour distillation that can discover datasets of just\nfour state-action pairs which, under supervised learning, train agents to\ncompetitive performance levels in continuous control tasks. We show that these\ndatasets generalize out of distribution to training policies with a wide range\nof architectures and hyperparameters. We also demonstrate application to a\ndownstream task, namely training multi-task agents in a zero-shot fashion.\nBeyond behaviour distillation, HaDES provides significant improvements in\nneuroevolution for RL over previous approaches and achieves SoTA results on one\nstandard supervised dataset distillation task. Finally, we show that\nvisualizing the synthetic datasets can provide human-interpretable task\ninsights.\n", "link": "http://arxiv.org/abs/2406.15042v1", "date": "2024-06-21", "relevancy": 2.087, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5297}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5211}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5038}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Behaviour%20Distillation&body=Title%3A%20Behaviour%20Distillation%0AAuthor%3A%20Andrei%20Lupu%20and%20Chris%20Lu%20and%20Jarek%20Liesen%20and%20Robert%20Tjarko%20Lange%20and%20Jakob%20Foerster%0AAbstract%3A%20%20%20Dataset%20distillation%20aims%20to%20condense%20large%20datasets%20into%20a%20small%20number%20of%0Asynthetic%20examples%20that%20can%20be%20used%20as%20drop-in%20replacements%20when%20training%20new%0Amodels.%20It%20has%20applications%20to%20interpretability%2C%20neural%20architecture%20search%2C%0Aprivacy%2C%20and%20continual%20learning.%20Despite%20strong%20successes%20in%20supervised%0Adomains%2C%20such%20methods%20have%20not%20yet%20been%20extended%20to%20reinforcement%20learning%2C%0Awhere%20the%20lack%20of%20a%20fixed%20dataset%20renders%20most%20distillation%20methods%20unusable.%0AFilling%20the%20gap%2C%20we%20formalize%20behaviour%20distillation%2C%20a%20setting%20that%20aims%20to%0Adiscover%20and%20then%20condense%20the%20information%20required%20for%20training%20an%20expert%0Apolicy%20into%20a%20synthetic%20dataset%20of%20state-action%20pairs%2C%20without%20access%20to%20expert%0Adata.%20We%20then%20introduce%20Hallucinating%20Datasets%20with%20Evolution%20Strategies%0A%28HaDES%29%2C%20a%20method%20for%20behaviour%20distillation%20that%20can%20discover%20datasets%20of%20just%0Afour%20state-action%20pairs%20which%2C%20under%20supervised%20learning%2C%20train%20agents%20to%0Acompetitive%20performance%20levels%20in%20continuous%20control%20tasks.%20We%20show%20that%20these%0Adatasets%20generalize%20out%20of%20distribution%20to%20training%20policies%20with%20a%20wide%20range%0Aof%20architectures%20and%20hyperparameters.%20We%20also%20demonstrate%20application%20to%20a%0Adownstream%20task%2C%20namely%20training%20multi-task%20agents%20in%20a%20zero-shot%20fashion.%0ABeyond%20behaviour%20distillation%2C%20HaDES%20provides%20significant%20improvements%20in%0Aneuroevolution%20for%20RL%20over%20previous%20approaches%20and%20achieves%20SoTA%20results%20on%20one%0Astandard%20supervised%20dataset%20distillation%20task.%20Finally%2C%20we%20show%20that%0Avisualizing%20the%20synthetic%20datasets%20can%20provide%20human-interpretable%20task%0Ainsights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15042v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBehaviour%2520Distillation%26entry.906535625%3DAndrei%2520Lupu%2520and%2520Chris%2520Lu%2520and%2520Jarek%2520Liesen%2520and%2520Robert%2520Tjarko%2520Lange%2520and%2520Jakob%2520Foerster%26entry.1292438233%3D%2520%2520Dataset%2520distillation%2520aims%2520to%2520condense%2520large%2520datasets%2520into%2520a%2520small%2520number%2520of%250Asynthetic%2520examples%2520that%2520can%2520be%2520used%2520as%2520drop-in%2520replacements%2520when%2520training%2520new%250Amodels.%2520It%2520has%2520applications%2520to%2520interpretability%252C%2520neural%2520architecture%2520search%252C%250Aprivacy%252C%2520and%2520continual%2520learning.%2520Despite%2520strong%2520successes%2520in%2520supervised%250Adomains%252C%2520such%2520methods%2520have%2520not%2520yet%2520been%2520extended%2520to%2520reinforcement%2520learning%252C%250Awhere%2520the%2520lack%2520of%2520a%2520fixed%2520dataset%2520renders%2520most%2520distillation%2520methods%2520unusable.%250AFilling%2520the%2520gap%252C%2520we%2520formalize%2520behaviour%2520distillation%252C%2520a%2520setting%2520that%2520aims%2520to%250Adiscover%2520and%2520then%2520condense%2520the%2520information%2520required%2520for%2520training%2520an%2520expert%250Apolicy%2520into%2520a%2520synthetic%2520dataset%2520of%2520state-action%2520pairs%252C%2520without%2520access%2520to%2520expert%250Adata.%2520We%2520then%2520introduce%2520Hallucinating%2520Datasets%2520with%2520Evolution%2520Strategies%250A%2528HaDES%2529%252C%2520a%2520method%2520for%2520behaviour%2520distillation%2520that%2520can%2520discover%2520datasets%2520of%2520just%250Afour%2520state-action%2520pairs%2520which%252C%2520under%2520supervised%2520learning%252C%2520train%2520agents%2520to%250Acompetitive%2520performance%2520levels%2520in%2520continuous%2520control%2520tasks.%2520We%2520show%2520that%2520these%250Adatasets%2520generalize%2520out%2520of%2520distribution%2520to%2520training%2520policies%2520with%2520a%2520wide%2520range%250Aof%2520architectures%2520and%2520hyperparameters.%2520We%2520also%2520demonstrate%2520application%2520to%2520a%250Adownstream%2520task%252C%2520namely%2520training%2520multi-task%2520agents%2520in%2520a%2520zero-shot%2520fashion.%250ABeyond%2520behaviour%2520distillation%252C%2520HaDES%2520provides%2520significant%2520improvements%2520in%250Aneuroevolution%2520for%2520RL%2520over%2520previous%2520approaches%2520and%2520achieves%2520SoTA%2520results%2520on%2520one%250Astandard%2520supervised%2520dataset%2520distillation%2520task.%2520Finally%252C%2520we%2520show%2520that%250Avisualizing%2520the%2520synthetic%2520datasets%2520can%2520provide%2520human-interpretable%2520task%250Ainsights.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15042v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Behaviour%20Distillation&entry.906535625=Andrei%20Lupu%20and%20Chris%20Lu%20and%20Jarek%20Liesen%20and%20Robert%20Tjarko%20Lange%20and%20Jakob%20Foerster&entry.1292438233=%20%20Dataset%20distillation%20aims%20to%20condense%20large%20datasets%20into%20a%20small%20number%20of%0Asynthetic%20examples%20that%20can%20be%20used%20as%20drop-in%20replacements%20when%20training%20new%0Amodels.%20It%20has%20applications%20to%20interpretability%2C%20neural%20architecture%20search%2C%0Aprivacy%2C%20and%20continual%20learning.%20Despite%20strong%20successes%20in%20supervised%0Adomains%2C%20such%20methods%20have%20not%20yet%20been%20extended%20to%20reinforcement%20learning%2C%0Awhere%20the%20lack%20of%20a%20fixed%20dataset%20renders%20most%20distillation%20methods%20unusable.%0AFilling%20the%20gap%2C%20we%20formalize%20behaviour%20distillation%2C%20a%20setting%20that%20aims%20to%0Adiscover%20and%20then%20condense%20the%20information%20required%20for%20training%20an%20expert%0Apolicy%20into%20a%20synthetic%20dataset%20of%20state-action%20pairs%2C%20without%20access%20to%20expert%0Adata.%20We%20then%20introduce%20Hallucinating%20Datasets%20with%20Evolution%20Strategies%0A%28HaDES%29%2C%20a%20method%20for%20behaviour%20distillation%20that%20can%20discover%20datasets%20of%20just%0Afour%20state-action%20pairs%20which%2C%20under%20supervised%20learning%2C%20train%20agents%20to%0Acompetitive%20performance%20levels%20in%20continuous%20control%20tasks.%20We%20show%20that%20these%0Adatasets%20generalize%20out%20of%20distribution%20to%20training%20policies%20with%20a%20wide%20range%0Aof%20architectures%20and%20hyperparameters.%20We%20also%20demonstrate%20application%20to%20a%0Adownstream%20task%2C%20namely%20training%20multi-task%20agents%20in%20a%20zero-shot%20fashion.%0ABeyond%20behaviour%20distillation%2C%20HaDES%20provides%20significant%20improvements%20in%0Aneuroevolution%20for%20RL%20over%20previous%20approaches%20and%20achieves%20SoTA%20results%20on%20one%0Astandard%20supervised%20dataset%20distillation%20task.%20Finally%2C%20we%20show%20that%0Avisualizing%20the%20synthetic%20datasets%20can%20provide%20human-interpretable%20task%0Ainsights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15042v1&entry.124074799=Read"},
{"title": "Latent Functional Maps", "author": "Marco Fumero and Marco Pegoraro and Valentino Maiorca and Francesco Locatello and Emanuele Rodol\u00e0", "abstract": "  Neural models learn data representations that lie on low-dimensional\nmanifolds, yet modeling the relation between these representational spaces is\nan ongoing challenge. By integrating spectral geometry principles into neural\nmodeling, we show that this problem can be better addressed in the functional\ndomain, mitigating complexity, while enhancing interpretability and\nperformances on downstream tasks. To this end, we introduce a multi-purpose\nframework to the representation learning community, which allows to: (i)\ncompare different spaces in an interpretable way and measure their intrinsic\nsimilarity; (ii) find correspondences between them, both in unsupervised and\nweakly supervised settings, and (iii) to effectively transfer representations\nbetween distinct spaces. We validate our framework on various applications,\nranging from stitching to retrieval tasks, demonstrating that latent functional\nmaps can serve as a swiss-army knife for representation alignment.\n", "link": "http://arxiv.org/abs/2406.14183v2", "date": "2024-06-21", "relevancy": 2.0804, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5231}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5185}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5167}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latent%20Functional%20Maps&body=Title%3A%20Latent%20Functional%20Maps%0AAuthor%3A%20Marco%20Fumero%20and%20Marco%20Pegoraro%20and%20Valentino%20Maiorca%20and%20Francesco%20Locatello%20and%20Emanuele%20Rodol%C3%A0%0AAbstract%3A%20%20%20Neural%20models%20learn%20data%20representations%20that%20lie%20on%20low-dimensional%0Amanifolds%2C%20yet%20modeling%20the%20relation%20between%20these%20representational%20spaces%20is%0Aan%20ongoing%20challenge.%20By%20integrating%20spectral%20geometry%20principles%20into%20neural%0Amodeling%2C%20we%20show%20that%20this%20problem%20can%20be%20better%20addressed%20in%20the%20functional%0Adomain%2C%20mitigating%20complexity%2C%20while%20enhancing%20interpretability%20and%0Aperformances%20on%20downstream%20tasks.%20To%20this%20end%2C%20we%20introduce%20a%20multi-purpose%0Aframework%20to%20the%20representation%20learning%20community%2C%20which%20allows%20to%3A%20%28i%29%0Acompare%20different%20spaces%20in%20an%20interpretable%20way%20and%20measure%20their%20intrinsic%0Asimilarity%3B%20%28ii%29%20find%20correspondences%20between%20them%2C%20both%20in%20unsupervised%20and%0Aweakly%20supervised%20settings%2C%20and%20%28iii%29%20to%20effectively%20transfer%20representations%0Abetween%20distinct%20spaces.%20We%20validate%20our%20framework%20on%20various%20applications%2C%0Aranging%20from%20stitching%20to%20retrieval%20tasks%2C%20demonstrating%20that%20latent%20functional%0Amaps%20can%20serve%20as%20a%20swiss-army%20knife%20for%20representation%20alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14183v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatent%2520Functional%2520Maps%26entry.906535625%3DMarco%2520Fumero%2520and%2520Marco%2520Pegoraro%2520and%2520Valentino%2520Maiorca%2520and%2520Francesco%2520Locatello%2520and%2520Emanuele%2520Rodol%25C3%25A0%26entry.1292438233%3D%2520%2520Neural%2520models%2520learn%2520data%2520representations%2520that%2520lie%2520on%2520low-dimensional%250Amanifolds%252C%2520yet%2520modeling%2520the%2520relation%2520between%2520these%2520representational%2520spaces%2520is%250Aan%2520ongoing%2520challenge.%2520By%2520integrating%2520spectral%2520geometry%2520principles%2520into%2520neural%250Amodeling%252C%2520we%2520show%2520that%2520this%2520problem%2520can%2520be%2520better%2520addressed%2520in%2520the%2520functional%250Adomain%252C%2520mitigating%2520complexity%252C%2520while%2520enhancing%2520interpretability%2520and%250Aperformances%2520on%2520downstream%2520tasks.%2520To%2520this%2520end%252C%2520we%2520introduce%2520a%2520multi-purpose%250Aframework%2520to%2520the%2520representation%2520learning%2520community%252C%2520which%2520allows%2520to%253A%2520%2528i%2529%250Acompare%2520different%2520spaces%2520in%2520an%2520interpretable%2520way%2520and%2520measure%2520their%2520intrinsic%250Asimilarity%253B%2520%2528ii%2529%2520find%2520correspondences%2520between%2520them%252C%2520both%2520in%2520unsupervised%2520and%250Aweakly%2520supervised%2520settings%252C%2520and%2520%2528iii%2529%2520to%2520effectively%2520transfer%2520representations%250Abetween%2520distinct%2520spaces.%2520We%2520validate%2520our%2520framework%2520on%2520various%2520applications%252C%250Aranging%2520from%2520stitching%2520to%2520retrieval%2520tasks%252C%2520demonstrating%2520that%2520latent%2520functional%250Amaps%2520can%2520serve%2520as%2520a%2520swiss-army%2520knife%2520for%2520representation%2520alignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14183v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent%20Functional%20Maps&entry.906535625=Marco%20Fumero%20and%20Marco%20Pegoraro%20and%20Valentino%20Maiorca%20and%20Francesco%20Locatello%20and%20Emanuele%20Rodol%C3%A0&entry.1292438233=%20%20Neural%20models%20learn%20data%20representations%20that%20lie%20on%20low-dimensional%0Amanifolds%2C%20yet%20modeling%20the%20relation%20between%20these%20representational%20spaces%20is%0Aan%20ongoing%20challenge.%20By%20integrating%20spectral%20geometry%20principles%20into%20neural%0Amodeling%2C%20we%20show%20that%20this%20problem%20can%20be%20better%20addressed%20in%20the%20functional%0Adomain%2C%20mitigating%20complexity%2C%20while%20enhancing%20interpretability%20and%0Aperformances%20on%20downstream%20tasks.%20To%20this%20end%2C%20we%20introduce%20a%20multi-purpose%0Aframework%20to%20the%20representation%20learning%20community%2C%20which%20allows%20to%3A%20%28i%29%0Acompare%20different%20spaces%20in%20an%20interpretable%20way%20and%20measure%20their%20intrinsic%0Asimilarity%3B%20%28ii%29%20find%20correspondences%20between%20them%2C%20both%20in%20unsupervised%20and%0Aweakly%20supervised%20settings%2C%20and%20%28iii%29%20to%20effectively%20transfer%20representations%0Abetween%20distinct%20spaces.%20We%20validate%20our%20framework%20on%20various%20applications%2C%0Aranging%20from%20stitching%20to%20retrieval%20tasks%2C%20demonstrating%20that%20latent%20functional%0Amaps%20can%20serve%20as%20a%20swiss-army%20knife%20for%20representation%20alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14183v2&entry.124074799=Read"},
{"title": "Attention as a Hypernetwork", "author": "Simon Schug and Seijin Kobayashi and Yassir Akram and Jo\u00e3o Sacramento and Razvan Pascanu", "abstract": "  Transformers can under some circumstances generalize to novel problem\ninstances whose constituent parts might have been encountered during training\nbut whose compositions have not. What mechanisms underlie this ability for\ncompositional generalization? By reformulating multi-head attention as a\nhypernetwork, we reveal that a low-dimensional latent code specifies key-query\nspecific operations. We find empirically that this latent code is highly\nstructured, capturing information about the subtasks performed by the network.\nUsing the framework of attention as a hypernetwork we further propose a simple\nmodification of multi-head linear attention that strengthens the ability for\ncompositional generalization on a range of abstract reasoning tasks. In\nparticular, we introduce a symbolic version of the Raven Progressive Matrices\nhuman intelligence test on which we demonstrate how scaling model size and data\nenables compositional generalization and gives rise to a functionally\nstructured latent code in the transformer.\n", "link": "http://arxiv.org/abs/2406.05816v2", "date": "2024-06-21", "relevancy": 2.0621, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5318}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5198}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4975}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attention%20as%20a%20Hypernetwork&body=Title%3A%20Attention%20as%20a%20Hypernetwork%0AAuthor%3A%20Simon%20Schug%20and%20Seijin%20Kobayashi%20and%20Yassir%20Akram%20and%20Jo%C3%A3o%20Sacramento%20and%20Razvan%20Pascanu%0AAbstract%3A%20%20%20Transformers%20can%20under%20some%20circumstances%20generalize%20to%20novel%20problem%0Ainstances%20whose%20constituent%20parts%20might%20have%20been%20encountered%20during%20training%0Abut%20whose%20compositions%20have%20not.%20What%20mechanisms%20underlie%20this%20ability%20for%0Acompositional%20generalization%3F%20By%20reformulating%20multi-head%20attention%20as%20a%0Ahypernetwork%2C%20we%20reveal%20that%20a%20low-dimensional%20latent%20code%20specifies%20key-query%0Aspecific%20operations.%20We%20find%20empirically%20that%20this%20latent%20code%20is%20highly%0Astructured%2C%20capturing%20information%20about%20the%20subtasks%20performed%20by%20the%20network.%0AUsing%20the%20framework%20of%20attention%20as%20a%20hypernetwork%20we%20further%20propose%20a%20simple%0Amodification%20of%20multi-head%20linear%20attention%20that%20strengthens%20the%20ability%20for%0Acompositional%20generalization%20on%20a%20range%20of%20abstract%20reasoning%20tasks.%20In%0Aparticular%2C%20we%20introduce%20a%20symbolic%20version%20of%20the%20Raven%20Progressive%20Matrices%0Ahuman%20intelligence%20test%20on%20which%20we%20demonstrate%20how%20scaling%20model%20size%20and%20data%0Aenables%20compositional%20generalization%20and%20gives%20rise%20to%20a%20functionally%0Astructured%20latent%20code%20in%20the%20transformer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05816v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttention%2520as%2520a%2520Hypernetwork%26entry.906535625%3DSimon%2520Schug%2520and%2520Seijin%2520Kobayashi%2520and%2520Yassir%2520Akram%2520and%2520Jo%25C3%25A3o%2520Sacramento%2520and%2520Razvan%2520Pascanu%26entry.1292438233%3D%2520%2520Transformers%2520can%2520under%2520some%2520circumstances%2520generalize%2520to%2520novel%2520problem%250Ainstances%2520whose%2520constituent%2520parts%2520might%2520have%2520been%2520encountered%2520during%2520training%250Abut%2520whose%2520compositions%2520have%2520not.%2520What%2520mechanisms%2520underlie%2520this%2520ability%2520for%250Acompositional%2520generalization%253F%2520By%2520reformulating%2520multi-head%2520attention%2520as%2520a%250Ahypernetwork%252C%2520we%2520reveal%2520that%2520a%2520low-dimensional%2520latent%2520code%2520specifies%2520key-query%250Aspecific%2520operations.%2520We%2520find%2520empirically%2520that%2520this%2520latent%2520code%2520is%2520highly%250Astructured%252C%2520capturing%2520information%2520about%2520the%2520subtasks%2520performed%2520by%2520the%2520network.%250AUsing%2520the%2520framework%2520of%2520attention%2520as%2520a%2520hypernetwork%2520we%2520further%2520propose%2520a%2520simple%250Amodification%2520of%2520multi-head%2520linear%2520attention%2520that%2520strengthens%2520the%2520ability%2520for%250Acompositional%2520generalization%2520on%2520a%2520range%2520of%2520abstract%2520reasoning%2520tasks.%2520In%250Aparticular%252C%2520we%2520introduce%2520a%2520symbolic%2520version%2520of%2520the%2520Raven%2520Progressive%2520Matrices%250Ahuman%2520intelligence%2520test%2520on%2520which%2520we%2520demonstrate%2520how%2520scaling%2520model%2520size%2520and%2520data%250Aenables%2520compositional%2520generalization%2520and%2520gives%2520rise%2520to%2520a%2520functionally%250Astructured%2520latent%2520code%2520in%2520the%2520transformer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05816v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention%20as%20a%20Hypernetwork&entry.906535625=Simon%20Schug%20and%20Seijin%20Kobayashi%20and%20Yassir%20Akram%20and%20Jo%C3%A3o%20Sacramento%20and%20Razvan%20Pascanu&entry.1292438233=%20%20Transformers%20can%20under%20some%20circumstances%20generalize%20to%20novel%20problem%0Ainstances%20whose%20constituent%20parts%20might%20have%20been%20encountered%20during%20training%0Abut%20whose%20compositions%20have%20not.%20What%20mechanisms%20underlie%20this%20ability%20for%0Acompositional%20generalization%3F%20By%20reformulating%20multi-head%20attention%20as%20a%0Ahypernetwork%2C%20we%20reveal%20that%20a%20low-dimensional%20latent%20code%20specifies%20key-query%0Aspecific%20operations.%20We%20find%20empirically%20that%20this%20latent%20code%20is%20highly%0Astructured%2C%20capturing%20information%20about%20the%20subtasks%20performed%20by%20the%20network.%0AUsing%20the%20framework%20of%20attention%20as%20a%20hypernetwork%20we%20further%20propose%20a%20simple%0Amodification%20of%20multi-head%20linear%20attention%20that%20strengthens%20the%20ability%20for%0Acompositional%20generalization%20on%20a%20range%20of%20abstract%20reasoning%20tasks.%20In%0Aparticular%2C%20we%20introduce%20a%20symbolic%20version%20of%20the%20Raven%20Progressive%20Matrices%0Ahuman%20intelligence%20test%20on%20which%20we%20demonstrate%20how%20scaling%20model%20size%20and%20data%0Aenables%20compositional%20generalization%20and%20gives%20rise%20to%20a%20functionally%0Astructured%20latent%20code%20in%20the%20transformer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05816v2&entry.124074799=Read"},
{"title": "Reinforcement Learning with Latent State Inference for Autonomous\n  On-ramp Merging under Observation Delay", "author": "Amin Tabrizian and Zhitong Huang and Peng Wei", "abstract": "  This paper presents a novel approach to address the challenging problem of\nautonomous on-ramp merging, where a self-driving vehicle needs to seamlessly\nintegrate into a flow of vehicles on a multi-lane highway. We introduce the\nLane-keeping, Lane-changing with Latent-state Inference and Safety Controller\n(L3IS) agent, designed to perform the on-ramp merging task safely without\ncomprehensive knowledge about surrounding vehicles' intents or driving styles.\nWe also present an augmentation of this agent called AL3IS that accounts for\nobservation delays, allowing the agent to make more robust decisions in\nreal-world environments with vehicle-to-vehicle (V2V) communication delays. By\nmodeling the unobservable aspects of the environment through latent states,\nsuch as other drivers' intents, our approach enhances the agent's ability to\nadapt to dynamic traffic conditions, optimize merging maneuvers, and ensure\nsafe interactions with other vehicles. We demonstrate the effectiveness of our\nmethod through extensive simulations generated from real traffic data and\ncompare its performance with existing approaches. L3IS shows a 99.90% success\nrate in a challenging on-ramp merging case generated from the real US Highway\n101 data. We further perform a sensitivity analysis on AL3IS to evaluate its\nrobustness against varying observation delays, which demonstrates an acceptable\nperformance of 93.84% success rate in 1-second V2V communication delay.\n", "link": "http://arxiv.org/abs/2403.11852v3", "date": "2024-06-21", "relevancy": 2.0602, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5234}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5198}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5069}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcement%20Learning%20with%20Latent%20State%20Inference%20for%20Autonomous%0A%20%20On-ramp%20Merging%20under%20Observation%20Delay&body=Title%3A%20Reinforcement%20Learning%20with%20Latent%20State%20Inference%20for%20Autonomous%0A%20%20On-ramp%20Merging%20under%20Observation%20Delay%0AAuthor%3A%20Amin%20Tabrizian%20and%20Zhitong%20Huang%20and%20Peng%20Wei%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20approach%20to%20address%20the%20challenging%20problem%20of%0Aautonomous%20on-ramp%20merging%2C%20where%20a%20self-driving%20vehicle%20needs%20to%20seamlessly%0Aintegrate%20into%20a%20flow%20of%20vehicles%20on%20a%20multi-lane%20highway.%20We%20introduce%20the%0ALane-keeping%2C%20Lane-changing%20with%20Latent-state%20Inference%20and%20Safety%20Controller%0A%28L3IS%29%20agent%2C%20designed%20to%20perform%20the%20on-ramp%20merging%20task%20safely%20without%0Acomprehensive%20knowledge%20about%20surrounding%20vehicles%27%20intents%20or%20driving%20styles.%0AWe%20also%20present%20an%20augmentation%20of%20this%20agent%20called%20AL3IS%20that%20accounts%20for%0Aobservation%20delays%2C%20allowing%20the%20agent%20to%20make%20more%20robust%20decisions%20in%0Areal-world%20environments%20with%20vehicle-to-vehicle%20%28V2V%29%20communication%20delays.%20By%0Amodeling%20the%20unobservable%20aspects%20of%20the%20environment%20through%20latent%20states%2C%0Asuch%20as%20other%20drivers%27%20intents%2C%20our%20approach%20enhances%20the%20agent%27s%20ability%20to%0Aadapt%20to%20dynamic%20traffic%20conditions%2C%20optimize%20merging%20maneuvers%2C%20and%20ensure%0Asafe%20interactions%20with%20other%20vehicles.%20We%20demonstrate%20the%20effectiveness%20of%20our%0Amethod%20through%20extensive%20simulations%20generated%20from%20real%20traffic%20data%20and%0Acompare%20its%20performance%20with%20existing%20approaches.%20L3IS%20shows%20a%2099.90%25%20success%0Arate%20in%20a%20challenging%20on-ramp%20merging%20case%20generated%20from%20the%20real%20US%20Highway%0A101%20data.%20We%20further%20perform%20a%20sensitivity%20analysis%20on%20AL3IS%20to%20evaluate%20its%0Arobustness%20against%20varying%20observation%20delays%2C%20which%20demonstrates%20an%20acceptable%0Aperformance%20of%2093.84%25%20success%20rate%20in%201-second%20V2V%20communication%20delay.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11852v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcement%2520Learning%2520with%2520Latent%2520State%2520Inference%2520for%2520Autonomous%250A%2520%2520On-ramp%2520Merging%2520under%2520Observation%2520Delay%26entry.906535625%3DAmin%2520Tabrizian%2520and%2520Zhitong%2520Huang%2520and%2520Peng%2520Wei%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520approach%2520to%2520address%2520the%2520challenging%2520problem%2520of%250Aautonomous%2520on-ramp%2520merging%252C%2520where%2520a%2520self-driving%2520vehicle%2520needs%2520to%2520seamlessly%250Aintegrate%2520into%2520a%2520flow%2520of%2520vehicles%2520on%2520a%2520multi-lane%2520highway.%2520We%2520introduce%2520the%250ALane-keeping%252C%2520Lane-changing%2520with%2520Latent-state%2520Inference%2520and%2520Safety%2520Controller%250A%2528L3IS%2529%2520agent%252C%2520designed%2520to%2520perform%2520the%2520on-ramp%2520merging%2520task%2520safely%2520without%250Acomprehensive%2520knowledge%2520about%2520surrounding%2520vehicles%2527%2520intents%2520or%2520driving%2520styles.%250AWe%2520also%2520present%2520an%2520augmentation%2520of%2520this%2520agent%2520called%2520AL3IS%2520that%2520accounts%2520for%250Aobservation%2520delays%252C%2520allowing%2520the%2520agent%2520to%2520make%2520more%2520robust%2520decisions%2520in%250Areal-world%2520environments%2520with%2520vehicle-to-vehicle%2520%2528V2V%2529%2520communication%2520delays.%2520By%250Amodeling%2520the%2520unobservable%2520aspects%2520of%2520the%2520environment%2520through%2520latent%2520states%252C%250Asuch%2520as%2520other%2520drivers%2527%2520intents%252C%2520our%2520approach%2520enhances%2520the%2520agent%2527s%2520ability%2520to%250Aadapt%2520to%2520dynamic%2520traffic%2520conditions%252C%2520optimize%2520merging%2520maneuvers%252C%2520and%2520ensure%250Asafe%2520interactions%2520with%2520other%2520vehicles.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Amethod%2520through%2520extensive%2520simulations%2520generated%2520from%2520real%2520traffic%2520data%2520and%250Acompare%2520its%2520performance%2520with%2520existing%2520approaches.%2520L3IS%2520shows%2520a%252099.90%2525%2520success%250Arate%2520in%2520a%2520challenging%2520on-ramp%2520merging%2520case%2520generated%2520from%2520the%2520real%2520US%2520Highway%250A101%2520data.%2520We%2520further%2520perform%2520a%2520sensitivity%2520analysis%2520on%2520AL3IS%2520to%2520evaluate%2520its%250Arobustness%2520against%2520varying%2520observation%2520delays%252C%2520which%2520demonstrates%2520an%2520acceptable%250Aperformance%2520of%252093.84%2525%2520success%2520rate%2520in%25201-second%2520V2V%2520communication%2520delay.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.11852v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcement%20Learning%20with%20Latent%20State%20Inference%20for%20Autonomous%0A%20%20On-ramp%20Merging%20under%20Observation%20Delay&entry.906535625=Amin%20Tabrizian%20and%20Zhitong%20Huang%20and%20Peng%20Wei&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20approach%20to%20address%20the%20challenging%20problem%20of%0Aautonomous%20on-ramp%20merging%2C%20where%20a%20self-driving%20vehicle%20needs%20to%20seamlessly%0Aintegrate%20into%20a%20flow%20of%20vehicles%20on%20a%20multi-lane%20highway.%20We%20introduce%20the%0ALane-keeping%2C%20Lane-changing%20with%20Latent-state%20Inference%20and%20Safety%20Controller%0A%28L3IS%29%20agent%2C%20designed%20to%20perform%20the%20on-ramp%20merging%20task%20safely%20without%0Acomprehensive%20knowledge%20about%20surrounding%20vehicles%27%20intents%20or%20driving%20styles.%0AWe%20also%20present%20an%20augmentation%20of%20this%20agent%20called%20AL3IS%20that%20accounts%20for%0Aobservation%20delays%2C%20allowing%20the%20agent%20to%20make%20more%20robust%20decisions%20in%0Areal-world%20environments%20with%20vehicle-to-vehicle%20%28V2V%29%20communication%20delays.%20By%0Amodeling%20the%20unobservable%20aspects%20of%20the%20environment%20through%20latent%20states%2C%0Asuch%20as%20other%20drivers%27%20intents%2C%20our%20approach%20enhances%20the%20agent%27s%20ability%20to%0Aadapt%20to%20dynamic%20traffic%20conditions%2C%20optimize%20merging%20maneuvers%2C%20and%20ensure%0Asafe%20interactions%20with%20other%20vehicles.%20We%20demonstrate%20the%20effectiveness%20of%20our%0Amethod%20through%20extensive%20simulations%20generated%20from%20real%20traffic%20data%20and%0Acompare%20its%20performance%20with%20existing%20approaches.%20L3IS%20shows%20a%2099.90%25%20success%0Arate%20in%20a%20challenging%20on-ramp%20merging%20case%20generated%20from%20the%20real%20US%20Highway%0A101%20data.%20We%20further%20perform%20a%20sensitivity%20analysis%20on%20AL3IS%20to%20evaluate%20its%0Arobustness%20against%20varying%20observation%20delays%2C%20which%20demonstrates%20an%20acceptable%0Aperformance%20of%2093.84%25%20success%20rate%20in%201-second%20V2V%20communication%20delay.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11852v3&entry.124074799=Read"},
{"title": "Neural Incremental Data Assimilation", "author": "Matthieu Blanke and Ronan Fablet and Marc Lelarge", "abstract": "  Data assimilation is a central problem in many geophysical applications, such\nas weather forecasting. It aims to estimate the state of a potentially large\nsystem, such as the atmosphere, from sparse observations, supplemented by prior\nphysical knowledge. The size of the systems involved and the complexity of the\nunderlying physical equations make it a challenging task from a computational\npoint of view. Neural networks represent a promising method of emulating the\nphysics at low cost, and therefore have the potential to considerably improve\nand accelerate data assimilation. In this work, we introduce a deep learning\napproach where the physical system is modeled as a sequence of coarse-to-fine\nGaussian prior distributions parametrized by a neural network. This allows us\nto define an assimilation operator, which is trained in an end-to-end fashion\nto minimize the reconstruction error on a dataset with different observation\nprocesses. We illustrate our approach on chaotic dynamical physical systems\nwith sparse observations, and compare it to traditional variational data\nassimilation methods.\n", "link": "http://arxiv.org/abs/2406.15076v1", "date": "2024-06-21", "relevancy": 2.0441, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5126}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5124}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5037}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Incremental%20Data%20Assimilation&body=Title%3A%20Neural%20Incremental%20Data%20Assimilation%0AAuthor%3A%20Matthieu%20Blanke%20and%20Ronan%20Fablet%20and%20Marc%20Lelarge%0AAbstract%3A%20%20%20Data%20assimilation%20is%20a%20central%20problem%20in%20many%20geophysical%20applications%2C%20such%0Aas%20weather%20forecasting.%20It%20aims%20to%20estimate%20the%20state%20of%20a%20potentially%20large%0Asystem%2C%20such%20as%20the%20atmosphere%2C%20from%20sparse%20observations%2C%20supplemented%20by%20prior%0Aphysical%20knowledge.%20The%20size%20of%20the%20systems%20involved%20and%20the%20complexity%20of%20the%0Aunderlying%20physical%20equations%20make%20it%20a%20challenging%20task%20from%20a%20computational%0Apoint%20of%20view.%20Neural%20networks%20represent%20a%20promising%20method%20of%20emulating%20the%0Aphysics%20at%20low%20cost%2C%20and%20therefore%20have%20the%20potential%20to%20considerably%20improve%0Aand%20accelerate%20data%20assimilation.%20In%20this%20work%2C%20we%20introduce%20a%20deep%20learning%0Aapproach%20where%20the%20physical%20system%20is%20modeled%20as%20a%20sequence%20of%20coarse-to-fine%0AGaussian%20prior%20distributions%20parametrized%20by%20a%20neural%20network.%20This%20allows%20us%0Ato%20define%20an%20assimilation%20operator%2C%20which%20is%20trained%20in%20an%20end-to-end%20fashion%0Ato%20minimize%20the%20reconstruction%20error%20on%20a%20dataset%20with%20different%20observation%0Aprocesses.%20We%20illustrate%20our%20approach%20on%20chaotic%20dynamical%20physical%20systems%0Awith%20sparse%20observations%2C%20and%20compare%20it%20to%20traditional%20variational%20data%0Aassimilation%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15076v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Incremental%2520Data%2520Assimilation%26entry.906535625%3DMatthieu%2520Blanke%2520and%2520Ronan%2520Fablet%2520and%2520Marc%2520Lelarge%26entry.1292438233%3D%2520%2520Data%2520assimilation%2520is%2520a%2520central%2520problem%2520in%2520many%2520geophysical%2520applications%252C%2520such%250Aas%2520weather%2520forecasting.%2520It%2520aims%2520to%2520estimate%2520the%2520state%2520of%2520a%2520potentially%2520large%250Asystem%252C%2520such%2520as%2520the%2520atmosphere%252C%2520from%2520sparse%2520observations%252C%2520supplemented%2520by%2520prior%250Aphysical%2520knowledge.%2520The%2520size%2520of%2520the%2520systems%2520involved%2520and%2520the%2520complexity%2520of%2520the%250Aunderlying%2520physical%2520equations%2520make%2520it%2520a%2520challenging%2520task%2520from%2520a%2520computational%250Apoint%2520of%2520view.%2520Neural%2520networks%2520represent%2520a%2520promising%2520method%2520of%2520emulating%2520the%250Aphysics%2520at%2520low%2520cost%252C%2520and%2520therefore%2520have%2520the%2520potential%2520to%2520considerably%2520improve%250Aand%2520accelerate%2520data%2520assimilation.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520deep%2520learning%250Aapproach%2520where%2520the%2520physical%2520system%2520is%2520modeled%2520as%2520a%2520sequence%2520of%2520coarse-to-fine%250AGaussian%2520prior%2520distributions%2520parametrized%2520by%2520a%2520neural%2520network.%2520This%2520allows%2520us%250Ato%2520define%2520an%2520assimilation%2520operator%252C%2520which%2520is%2520trained%2520in%2520an%2520end-to-end%2520fashion%250Ato%2520minimize%2520the%2520reconstruction%2520error%2520on%2520a%2520dataset%2520with%2520different%2520observation%250Aprocesses.%2520We%2520illustrate%2520our%2520approach%2520on%2520chaotic%2520dynamical%2520physical%2520systems%250Awith%2520sparse%2520observations%252C%2520and%2520compare%2520it%2520to%2520traditional%2520variational%2520data%250Aassimilation%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15076v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Incremental%20Data%20Assimilation&entry.906535625=Matthieu%20Blanke%20and%20Ronan%20Fablet%20and%20Marc%20Lelarge&entry.1292438233=%20%20Data%20assimilation%20is%20a%20central%20problem%20in%20many%20geophysical%20applications%2C%20such%0Aas%20weather%20forecasting.%20It%20aims%20to%20estimate%20the%20state%20of%20a%20potentially%20large%0Asystem%2C%20such%20as%20the%20atmosphere%2C%20from%20sparse%20observations%2C%20supplemented%20by%20prior%0Aphysical%20knowledge.%20The%20size%20of%20the%20systems%20involved%20and%20the%20complexity%20of%20the%0Aunderlying%20physical%20equations%20make%20it%20a%20challenging%20task%20from%20a%20computational%0Apoint%20of%20view.%20Neural%20networks%20represent%20a%20promising%20method%20of%20emulating%20the%0Aphysics%20at%20low%20cost%2C%20and%20therefore%20have%20the%20potential%20to%20considerably%20improve%0Aand%20accelerate%20data%20assimilation.%20In%20this%20work%2C%20we%20introduce%20a%20deep%20learning%0Aapproach%20where%20the%20physical%20system%20is%20modeled%20as%20a%20sequence%20of%20coarse-to-fine%0AGaussian%20prior%20distributions%20parametrized%20by%20a%20neural%20network.%20This%20allows%20us%0Ato%20define%20an%20assimilation%20operator%2C%20which%20is%20trained%20in%20an%20end-to-end%20fashion%0Ato%20minimize%20the%20reconstruction%20error%20on%20a%20dataset%20with%20different%20observation%0Aprocesses.%20We%20illustrate%20our%20approach%20on%20chaotic%20dynamical%20physical%20systems%0Awith%20sparse%20observations%2C%20and%20compare%20it%20to%20traditional%20variational%20data%0Aassimilation%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15076v1&entry.124074799=Read"},
{"title": "Tri-VQA: Triangular Reasoning Medical Visual Question Answering for\n  Multi-Attribute Analysis", "author": "Lin Fan and Xun Gong and Cenyang Zheng and Yafei Ou", "abstract": "  The intersection of medical Visual Question Answering (Med-VQA) is a\nchallenging research topic with advantages including patient engagement and\nclinical expert involvement for second opinions. However, existing Med-VQA\nmethods based on joint embedding fail to explain whether their provided results\nare based on correct reasoning or coincidental answers, which undermines the\ncredibility of VQA answers. In this paper, we investigate the construction of a\nmore cohesive and stable Med-VQA structure. Motivated by causal effect, we\npropose a novel Triangular Reasoning VQA (Tri-VQA) framework, which constructs\nreverse causal questions from the perspective of \"Why this answer?\" to\nelucidate the source of the answer and stimulate more reasonable forward\nreasoning processes. We evaluate our method on the Endoscopic Ultrasound (EUS)\nmulti-attribute annotated dataset from five centers, and test it on medical VQA\ndatasets. Experimental results demonstrate the superiority of our approach over\nexisting methods. Our codes and pre-trained models are available at\nhttps://anonymous.4open.science/r/Tri_VQA.\n", "link": "http://arxiv.org/abs/2406.15050v1", "date": "2024-06-21", "relevancy": 2.0434, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.542}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5192}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.49}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tri-VQA%3A%20Triangular%20Reasoning%20Medical%20Visual%20Question%20Answering%20for%0A%20%20Multi-Attribute%20Analysis&body=Title%3A%20Tri-VQA%3A%20Triangular%20Reasoning%20Medical%20Visual%20Question%20Answering%20for%0A%20%20Multi-Attribute%20Analysis%0AAuthor%3A%20Lin%20Fan%20and%20Xun%20Gong%20and%20Cenyang%20Zheng%20and%20Yafei%20Ou%0AAbstract%3A%20%20%20The%20intersection%20of%20medical%20Visual%20Question%20Answering%20%28Med-VQA%29%20is%20a%0Achallenging%20research%20topic%20with%20advantages%20including%20patient%20engagement%20and%0Aclinical%20expert%20involvement%20for%20second%20opinions.%20However%2C%20existing%20Med-VQA%0Amethods%20based%20on%20joint%20embedding%20fail%20to%20explain%20whether%20their%20provided%20results%0Aare%20based%20on%20correct%20reasoning%20or%20coincidental%20answers%2C%20which%20undermines%20the%0Acredibility%20of%20VQA%20answers.%20In%20this%20paper%2C%20we%20investigate%20the%20construction%20of%20a%0Amore%20cohesive%20and%20stable%20Med-VQA%20structure.%20Motivated%20by%20causal%20effect%2C%20we%0Apropose%20a%20novel%20Triangular%20Reasoning%20VQA%20%28Tri-VQA%29%20framework%2C%20which%20constructs%0Areverse%20causal%20questions%20from%20the%20perspective%20of%20%22Why%20this%20answer%3F%22%20to%0Aelucidate%20the%20source%20of%20the%20answer%20and%20stimulate%20more%20reasonable%20forward%0Areasoning%20processes.%20We%20evaluate%20our%20method%20on%20the%20Endoscopic%20Ultrasound%20%28EUS%29%0Amulti-attribute%20annotated%20dataset%20from%20five%20centers%2C%20and%20test%20it%20on%20medical%20VQA%0Adatasets.%20Experimental%20results%20demonstrate%20the%20superiority%20of%20our%20approach%20over%0Aexisting%20methods.%20Our%20codes%20and%20pre-trained%20models%20are%20available%20at%0Ahttps%3A//anonymous.4open.science/r/Tri_VQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15050v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTri-VQA%253A%2520Triangular%2520Reasoning%2520Medical%2520Visual%2520Question%2520Answering%2520for%250A%2520%2520Multi-Attribute%2520Analysis%26entry.906535625%3DLin%2520Fan%2520and%2520Xun%2520Gong%2520and%2520Cenyang%2520Zheng%2520and%2520Yafei%2520Ou%26entry.1292438233%3D%2520%2520The%2520intersection%2520of%2520medical%2520Visual%2520Question%2520Answering%2520%2528Med-VQA%2529%2520is%2520a%250Achallenging%2520research%2520topic%2520with%2520advantages%2520including%2520patient%2520engagement%2520and%250Aclinical%2520expert%2520involvement%2520for%2520second%2520opinions.%2520However%252C%2520existing%2520Med-VQA%250Amethods%2520based%2520on%2520joint%2520embedding%2520fail%2520to%2520explain%2520whether%2520their%2520provided%2520results%250Aare%2520based%2520on%2520correct%2520reasoning%2520or%2520coincidental%2520answers%252C%2520which%2520undermines%2520the%250Acredibility%2520of%2520VQA%2520answers.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520construction%2520of%2520a%250Amore%2520cohesive%2520and%2520stable%2520Med-VQA%2520structure.%2520Motivated%2520by%2520causal%2520effect%252C%2520we%250Apropose%2520a%2520novel%2520Triangular%2520Reasoning%2520VQA%2520%2528Tri-VQA%2529%2520framework%252C%2520which%2520constructs%250Areverse%2520causal%2520questions%2520from%2520the%2520perspective%2520of%2520%2522Why%2520this%2520answer%253F%2522%2520to%250Aelucidate%2520the%2520source%2520of%2520the%2520answer%2520and%2520stimulate%2520more%2520reasonable%2520forward%250Areasoning%2520processes.%2520We%2520evaluate%2520our%2520method%2520on%2520the%2520Endoscopic%2520Ultrasound%2520%2528EUS%2529%250Amulti-attribute%2520annotated%2520dataset%2520from%2520five%2520centers%252C%2520and%2520test%2520it%2520on%2520medical%2520VQA%250Adatasets.%2520Experimental%2520results%2520demonstrate%2520the%2520superiority%2520of%2520our%2520approach%2520over%250Aexisting%2520methods.%2520Our%2520codes%2520and%2520pre-trained%2520models%2520are%2520available%2520at%250Ahttps%253A//anonymous.4open.science/r/Tri_VQA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15050v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tri-VQA%3A%20Triangular%20Reasoning%20Medical%20Visual%20Question%20Answering%20for%0A%20%20Multi-Attribute%20Analysis&entry.906535625=Lin%20Fan%20and%20Xun%20Gong%20and%20Cenyang%20Zheng%20and%20Yafei%20Ou&entry.1292438233=%20%20The%20intersection%20of%20medical%20Visual%20Question%20Answering%20%28Med-VQA%29%20is%20a%0Achallenging%20research%20topic%20with%20advantages%20including%20patient%20engagement%20and%0Aclinical%20expert%20involvement%20for%20second%20opinions.%20However%2C%20existing%20Med-VQA%0Amethods%20based%20on%20joint%20embedding%20fail%20to%20explain%20whether%20their%20provided%20results%0Aare%20based%20on%20correct%20reasoning%20or%20coincidental%20answers%2C%20which%20undermines%20the%0Acredibility%20of%20VQA%20answers.%20In%20this%20paper%2C%20we%20investigate%20the%20construction%20of%20a%0Amore%20cohesive%20and%20stable%20Med-VQA%20structure.%20Motivated%20by%20causal%20effect%2C%20we%0Apropose%20a%20novel%20Triangular%20Reasoning%20VQA%20%28Tri-VQA%29%20framework%2C%20which%20constructs%0Areverse%20causal%20questions%20from%20the%20perspective%20of%20%22Why%20this%20answer%3F%22%20to%0Aelucidate%20the%20source%20of%20the%20answer%20and%20stimulate%20more%20reasonable%20forward%0Areasoning%20processes.%20We%20evaluate%20our%20method%20on%20the%20Endoscopic%20Ultrasound%20%28EUS%29%0Amulti-attribute%20annotated%20dataset%20from%20five%20centers%2C%20and%20test%20it%20on%20medical%20VQA%0Adatasets.%20Experimental%20results%20demonstrate%20the%20superiority%20of%20our%20approach%20over%0Aexisting%20methods.%20Our%20codes%20and%20pre-trained%20models%20are%20available%20at%0Ahttps%3A//anonymous.4open.science/r/Tri_VQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15050v1&entry.124074799=Read"},
{"title": "Tempora-Fusion: Time-Lock Puzzle with Efficient Verifiable Homomorphic\n  Linear Combination", "author": "Aydin Abadi", "abstract": "  To securely transmit sensitive information into the future, Time-Lock Puzzles\n(TLPs) have been developed. Their applications include scheduled payments,\ntimed commitments, e-voting, and sealed-bid auctions. Homomorphic TLP is a key\nvariant of TLP that enables computation on puzzles from different clients. This\nallows a solver/server to tackle only a single puzzle encoding the\ncomputation's result. However, existing homomorphic TLPs lack support for\nverifying the correctness of the computation results. We address this\nlimitation by introducing Tempora-Fusion, a TLP that allows a server to perform\nhomomorphic linear combinations of puzzles from different clients while\nensuring verification of computation correctness. This scheme avoids\nasymmetric-key cryptography for verification, thus paving the way for efficient\nimplementations. We discuss our scheme's application in various domains, such\nas federated learning, scheduled payments in online banking, and e-voting.\n", "link": "http://arxiv.org/abs/2406.15070v1", "date": "2024-06-21", "relevancy": 2.0413, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4201}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4061}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.3986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tempora-Fusion%3A%20Time-Lock%20Puzzle%20with%20Efficient%20Verifiable%20Homomorphic%0A%20%20Linear%20Combination&body=Title%3A%20Tempora-Fusion%3A%20Time-Lock%20Puzzle%20with%20Efficient%20Verifiable%20Homomorphic%0A%20%20Linear%20Combination%0AAuthor%3A%20Aydin%20Abadi%0AAbstract%3A%20%20%20To%20securely%20transmit%20sensitive%20information%20into%20the%20future%2C%20Time-Lock%20Puzzles%0A%28TLPs%29%20have%20been%20developed.%20Their%20applications%20include%20scheduled%20payments%2C%0Atimed%20commitments%2C%20e-voting%2C%20and%20sealed-bid%20auctions.%20Homomorphic%20TLP%20is%20a%20key%0Avariant%20of%20TLP%20that%20enables%20computation%20on%20puzzles%20from%20different%20clients.%20This%0Aallows%20a%20solver/server%20to%20tackle%20only%20a%20single%20puzzle%20encoding%20the%0Acomputation%27s%20result.%20However%2C%20existing%20homomorphic%20TLPs%20lack%20support%20for%0Averifying%20the%20correctness%20of%20the%20computation%20results.%20We%20address%20this%0Alimitation%20by%20introducing%20Tempora-Fusion%2C%20a%20TLP%20that%20allows%20a%20server%20to%20perform%0Ahomomorphic%20linear%20combinations%20of%20puzzles%20from%20different%20clients%20while%0Aensuring%20verification%20of%20computation%20correctness.%20This%20scheme%20avoids%0Aasymmetric-key%20cryptography%20for%20verification%2C%20thus%20paving%20the%20way%20for%20efficient%0Aimplementations.%20We%20discuss%20our%20scheme%27s%20application%20in%20various%20domains%2C%20such%0Aas%20federated%20learning%2C%20scheduled%20payments%20in%20online%20banking%2C%20and%20e-voting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15070v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTempora-Fusion%253A%2520Time-Lock%2520Puzzle%2520with%2520Efficient%2520Verifiable%2520Homomorphic%250A%2520%2520Linear%2520Combination%26entry.906535625%3DAydin%2520Abadi%26entry.1292438233%3D%2520%2520To%2520securely%2520transmit%2520sensitive%2520information%2520into%2520the%2520future%252C%2520Time-Lock%2520Puzzles%250A%2528TLPs%2529%2520have%2520been%2520developed.%2520Their%2520applications%2520include%2520scheduled%2520payments%252C%250Atimed%2520commitments%252C%2520e-voting%252C%2520and%2520sealed-bid%2520auctions.%2520Homomorphic%2520TLP%2520is%2520a%2520key%250Avariant%2520of%2520TLP%2520that%2520enables%2520computation%2520on%2520puzzles%2520from%2520different%2520clients.%2520This%250Aallows%2520a%2520solver/server%2520to%2520tackle%2520only%2520a%2520single%2520puzzle%2520encoding%2520the%250Acomputation%2527s%2520result.%2520However%252C%2520existing%2520homomorphic%2520TLPs%2520lack%2520support%2520for%250Averifying%2520the%2520correctness%2520of%2520the%2520computation%2520results.%2520We%2520address%2520this%250Alimitation%2520by%2520introducing%2520Tempora-Fusion%252C%2520a%2520TLP%2520that%2520allows%2520a%2520server%2520to%2520perform%250Ahomomorphic%2520linear%2520combinations%2520of%2520puzzles%2520from%2520different%2520clients%2520while%250Aensuring%2520verification%2520of%2520computation%2520correctness.%2520This%2520scheme%2520avoids%250Aasymmetric-key%2520cryptography%2520for%2520verification%252C%2520thus%2520paving%2520the%2520way%2520for%2520efficient%250Aimplementations.%2520We%2520discuss%2520our%2520scheme%2527s%2520application%2520in%2520various%2520domains%252C%2520such%250Aas%2520federated%2520learning%252C%2520scheduled%2520payments%2520in%2520online%2520banking%252C%2520and%2520e-voting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15070v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tempora-Fusion%3A%20Time-Lock%20Puzzle%20with%20Efficient%20Verifiable%20Homomorphic%0A%20%20Linear%20Combination&entry.906535625=Aydin%20Abadi&entry.1292438233=%20%20To%20securely%20transmit%20sensitive%20information%20into%20the%20future%2C%20Time-Lock%20Puzzles%0A%28TLPs%29%20have%20been%20developed.%20Their%20applications%20include%20scheduled%20payments%2C%0Atimed%20commitments%2C%20e-voting%2C%20and%20sealed-bid%20auctions.%20Homomorphic%20TLP%20is%20a%20key%0Avariant%20of%20TLP%20that%20enables%20computation%20on%20puzzles%20from%20different%20clients.%20This%0Aallows%20a%20solver/server%20to%20tackle%20only%20a%20single%20puzzle%20encoding%20the%0Acomputation%27s%20result.%20However%2C%20existing%20homomorphic%20TLPs%20lack%20support%20for%0Averifying%20the%20correctness%20of%20the%20computation%20results.%20We%20address%20this%0Alimitation%20by%20introducing%20Tempora-Fusion%2C%20a%20TLP%20that%20allows%20a%20server%20to%20perform%0Ahomomorphic%20linear%20combinations%20of%20puzzles%20from%20different%20clients%20while%0Aensuring%20verification%20of%20computation%20correctness.%20This%20scheme%20avoids%0Aasymmetric-key%20cryptography%20for%20verification%2C%20thus%20paving%20the%20way%20for%20efficient%0Aimplementations.%20We%20discuss%20our%20scheme%27s%20application%20in%20various%20domains%2C%20such%0Aas%20federated%20learning%2C%20scheduled%20payments%20in%20online%20banking%2C%20and%20e-voting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15070v1&entry.124074799=Read"},
{"title": "UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world\n  Document Analysis", "author": "Yulong Hui and Yao Lu and Huanchen Zhang", "abstract": "  The use of Retrieval-Augmented Generation (RAG) has improved Large Language\nModels (LLMs) in collaborating with external data, yet significant challenges\nexist in real-world scenarios. In areas such as academic literature and finance\nquestion answering, data are often found in raw text and tables in HTML or PDF\nformats, which can be lengthy and highly unstructured. In this paper, we\nintroduce a benchmark suite, namely Unstructured Document Analysis (UDA), that\ninvolves 2,965 real-world documents and 29,590 expert-annotated Q&A pairs. We\nrevisit popular LLM- and RAG-based solutions for document analysis and evaluate\nthe design choices and answer qualities across multiple document domains and\ndiverse query types. Our evaluation yields interesting findings and highlights\nthe importance of data parsing and retrieval. We hope our benchmark can shed\nlight and better serve real-world document analysis applications. The benchmark\nsuite and code can be found at https://github.com/qinchuanhui/UDA-Benchmark.\n", "link": "http://arxiv.org/abs/2406.15187v1", "date": "2024-06-21", "relevancy": 2.0255, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5431}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5349}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UDA%3A%20A%20Benchmark%20Suite%20for%20Retrieval%20Augmented%20Generation%20in%20Real-world%0A%20%20Document%20Analysis&body=Title%3A%20UDA%3A%20A%20Benchmark%20Suite%20for%20Retrieval%20Augmented%20Generation%20in%20Real-world%0A%20%20Document%20Analysis%0AAuthor%3A%20Yulong%20Hui%20and%20Yao%20Lu%20and%20Huanchen%20Zhang%0AAbstract%3A%20%20%20The%20use%20of%20Retrieval-Augmented%20Generation%20%28RAG%29%20has%20improved%20Large%20Language%0AModels%20%28LLMs%29%20in%20collaborating%20with%20external%20data%2C%20yet%20significant%20challenges%0Aexist%20in%20real-world%20scenarios.%20In%20areas%20such%20as%20academic%20literature%20and%20finance%0Aquestion%20answering%2C%20data%20are%20often%20found%20in%20raw%20text%20and%20tables%20in%20HTML%20or%20PDF%0Aformats%2C%20which%20can%20be%20lengthy%20and%20highly%20unstructured.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20benchmark%20suite%2C%20namely%20Unstructured%20Document%20Analysis%20%28UDA%29%2C%20that%0Ainvolves%202%2C965%20real-world%20documents%20and%2029%2C590%20expert-annotated%20Q%26A%20pairs.%20We%0Arevisit%20popular%20LLM-%20and%20RAG-based%20solutions%20for%20document%20analysis%20and%20evaluate%0Athe%20design%20choices%20and%20answer%20qualities%20across%20multiple%20document%20domains%20and%0Adiverse%20query%20types.%20Our%20evaluation%20yields%20interesting%20findings%20and%20highlights%0Athe%20importance%20of%20data%20parsing%20and%20retrieval.%20We%20hope%20our%20benchmark%20can%20shed%0Alight%20and%20better%20serve%20real-world%20document%20analysis%20applications.%20The%20benchmark%0Asuite%20and%20code%20can%20be%20found%20at%20https%3A//github.com/qinchuanhui/UDA-Benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15187v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUDA%253A%2520A%2520Benchmark%2520Suite%2520for%2520Retrieval%2520Augmented%2520Generation%2520in%2520Real-world%250A%2520%2520Document%2520Analysis%26entry.906535625%3DYulong%2520Hui%2520and%2520Yao%2520Lu%2520and%2520Huanchen%2520Zhang%26entry.1292438233%3D%2520%2520The%2520use%2520of%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520has%2520improved%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520in%2520collaborating%2520with%2520external%2520data%252C%2520yet%2520significant%2520challenges%250Aexist%2520in%2520real-world%2520scenarios.%2520In%2520areas%2520such%2520as%2520academic%2520literature%2520and%2520finance%250Aquestion%2520answering%252C%2520data%2520are%2520often%2520found%2520in%2520raw%2520text%2520and%2520tables%2520in%2520HTML%2520or%2520PDF%250Aformats%252C%2520which%2520can%2520be%2520lengthy%2520and%2520highly%2520unstructured.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520a%2520benchmark%2520suite%252C%2520namely%2520Unstructured%2520Document%2520Analysis%2520%2528UDA%2529%252C%2520that%250Ainvolves%25202%252C965%2520real-world%2520documents%2520and%252029%252C590%2520expert-annotated%2520Q%2526A%2520pairs.%2520We%250Arevisit%2520popular%2520LLM-%2520and%2520RAG-based%2520solutions%2520for%2520document%2520analysis%2520and%2520evaluate%250Athe%2520design%2520choices%2520and%2520answer%2520qualities%2520across%2520multiple%2520document%2520domains%2520and%250Adiverse%2520query%2520types.%2520Our%2520evaluation%2520yields%2520interesting%2520findings%2520and%2520highlights%250Athe%2520importance%2520of%2520data%2520parsing%2520and%2520retrieval.%2520We%2520hope%2520our%2520benchmark%2520can%2520shed%250Alight%2520and%2520better%2520serve%2520real-world%2520document%2520analysis%2520applications.%2520The%2520benchmark%250Asuite%2520and%2520code%2520can%2520be%2520found%2520at%2520https%253A//github.com/qinchuanhui/UDA-Benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15187v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UDA%3A%20A%20Benchmark%20Suite%20for%20Retrieval%20Augmented%20Generation%20in%20Real-world%0A%20%20Document%20Analysis&entry.906535625=Yulong%20Hui%20and%20Yao%20Lu%20and%20Huanchen%20Zhang&entry.1292438233=%20%20The%20use%20of%20Retrieval-Augmented%20Generation%20%28RAG%29%20has%20improved%20Large%20Language%0AModels%20%28LLMs%29%20in%20collaborating%20with%20external%20data%2C%20yet%20significant%20challenges%0Aexist%20in%20real-world%20scenarios.%20In%20areas%20such%20as%20academic%20literature%20and%20finance%0Aquestion%20answering%2C%20data%20are%20often%20found%20in%20raw%20text%20and%20tables%20in%20HTML%20or%20PDF%0Aformats%2C%20which%20can%20be%20lengthy%20and%20highly%20unstructured.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20benchmark%20suite%2C%20namely%20Unstructured%20Document%20Analysis%20%28UDA%29%2C%20that%0Ainvolves%202%2C965%20real-world%20documents%20and%2029%2C590%20expert-annotated%20Q%26A%20pairs.%20We%0Arevisit%20popular%20LLM-%20and%20RAG-based%20solutions%20for%20document%20analysis%20and%20evaluate%0Athe%20design%20choices%20and%20answer%20qualities%20across%20multiple%20document%20domains%20and%0Adiverse%20query%20types.%20Our%20evaluation%20yields%20interesting%20findings%20and%20highlights%0Athe%20importance%20of%20data%20parsing%20and%20retrieval.%20We%20hope%20our%20benchmark%20can%20shed%0Alight%20and%20better%20serve%20real-world%20document%20analysis%20applications.%20The%20benchmark%0Asuite%20and%20code%20can%20be%20found%20at%20https%3A//github.com/qinchuanhui/UDA-Benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15187v1&entry.124074799=Read"},
{"title": "Fine-grained Attention in Hierarchical Transformers for Tabular\n  Time-series", "author": "Raphael Azorin and Zied Ben Houidi and Massimo Gallo and Alessandro Finamore and Pietro Michiardi", "abstract": "  Tabular data is ubiquitous in many real-life systems. In particular,\ntime-dependent tabular data, where rows are chronologically related, is\ntypically used for recording historical events, e.g., financial transactions,\nhealthcare records, or stock history. Recently, hierarchical variants of the\nattention mechanism of transformer architectures have been used to model\ntabular time-series data. At first, rows (or columns) are encoded separately by\ncomputing attention between their fields. Subsequently, encoded rows (or\ncolumns) are attended to one another to model the entire tabular time-series.\nWhile efficient, this approach constrains the attention granularity and limits\nits ability to learn patterns at the field-level across separate rows, or\ncolumns. We take a first step to address this gap by proposing Fieldy, a\nfine-grained hierarchical model that contextualizes fields at both the row and\ncolumn levels. We compare our proposal against state of the art models on\nregression and classification tasks using public tabular time-series datasets.\nOur results show that combining row-wise and column-wise attention improves\nperformance without increasing model size. Code and data are available at\nhttps://github.com/raphaaal/fieldy.\n", "link": "http://arxiv.org/abs/2406.15327v1", "date": "2024-06-21", "relevancy": 2.0246, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5174}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5098}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-grained%20Attention%20in%20Hierarchical%20Transformers%20for%20Tabular%0A%20%20Time-series&body=Title%3A%20Fine-grained%20Attention%20in%20Hierarchical%20Transformers%20for%20Tabular%0A%20%20Time-series%0AAuthor%3A%20Raphael%20Azorin%20and%20Zied%20Ben%20Houidi%20and%20Massimo%20Gallo%20and%20Alessandro%20Finamore%20and%20Pietro%20Michiardi%0AAbstract%3A%20%20%20Tabular%20data%20is%20ubiquitous%20in%20many%20real-life%20systems.%20In%20particular%2C%0Atime-dependent%20tabular%20data%2C%20where%20rows%20are%20chronologically%20related%2C%20is%0Atypically%20used%20for%20recording%20historical%20events%2C%20e.g.%2C%20financial%20transactions%2C%0Ahealthcare%20records%2C%20or%20stock%20history.%20Recently%2C%20hierarchical%20variants%20of%20the%0Aattention%20mechanism%20of%20transformer%20architectures%20have%20been%20used%20to%20model%0Atabular%20time-series%20data.%20At%20first%2C%20rows%20%28or%20columns%29%20are%20encoded%20separately%20by%0Acomputing%20attention%20between%20their%20fields.%20Subsequently%2C%20encoded%20rows%20%28or%0Acolumns%29%20are%20attended%20to%20one%20another%20to%20model%20the%20entire%20tabular%20time-series.%0AWhile%20efficient%2C%20this%20approach%20constrains%20the%20attention%20granularity%20and%20limits%0Aits%20ability%20to%20learn%20patterns%20at%20the%20field-level%20across%20separate%20rows%2C%20or%0Acolumns.%20We%20take%20a%20first%20step%20to%20address%20this%20gap%20by%20proposing%20Fieldy%2C%20a%0Afine-grained%20hierarchical%20model%20that%20contextualizes%20fields%20at%20both%20the%20row%20and%0Acolumn%20levels.%20We%20compare%20our%20proposal%20against%20state%20of%20the%20art%20models%20on%0Aregression%20and%20classification%20tasks%20using%20public%20tabular%20time-series%20datasets.%0AOur%20results%20show%20that%20combining%20row-wise%20and%20column-wise%20attention%20improves%0Aperformance%20without%20increasing%20model%20size.%20Code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/raphaaal/fieldy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15327v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-grained%2520Attention%2520in%2520Hierarchical%2520Transformers%2520for%2520Tabular%250A%2520%2520Time-series%26entry.906535625%3DRaphael%2520Azorin%2520and%2520Zied%2520Ben%2520Houidi%2520and%2520Massimo%2520Gallo%2520and%2520Alessandro%2520Finamore%2520and%2520Pietro%2520Michiardi%26entry.1292438233%3D%2520%2520Tabular%2520data%2520is%2520ubiquitous%2520in%2520many%2520real-life%2520systems.%2520In%2520particular%252C%250Atime-dependent%2520tabular%2520data%252C%2520where%2520rows%2520are%2520chronologically%2520related%252C%2520is%250Atypically%2520used%2520for%2520recording%2520historical%2520events%252C%2520e.g.%252C%2520financial%2520transactions%252C%250Ahealthcare%2520records%252C%2520or%2520stock%2520history.%2520Recently%252C%2520hierarchical%2520variants%2520of%2520the%250Aattention%2520mechanism%2520of%2520transformer%2520architectures%2520have%2520been%2520used%2520to%2520model%250Atabular%2520time-series%2520data.%2520At%2520first%252C%2520rows%2520%2528or%2520columns%2529%2520are%2520encoded%2520separately%2520by%250Acomputing%2520attention%2520between%2520their%2520fields.%2520Subsequently%252C%2520encoded%2520rows%2520%2528or%250Acolumns%2529%2520are%2520attended%2520to%2520one%2520another%2520to%2520model%2520the%2520entire%2520tabular%2520time-series.%250AWhile%2520efficient%252C%2520this%2520approach%2520constrains%2520the%2520attention%2520granularity%2520and%2520limits%250Aits%2520ability%2520to%2520learn%2520patterns%2520at%2520the%2520field-level%2520across%2520separate%2520rows%252C%2520or%250Acolumns.%2520We%2520take%2520a%2520first%2520step%2520to%2520address%2520this%2520gap%2520by%2520proposing%2520Fieldy%252C%2520a%250Afine-grained%2520hierarchical%2520model%2520that%2520contextualizes%2520fields%2520at%2520both%2520the%2520row%2520and%250Acolumn%2520levels.%2520We%2520compare%2520our%2520proposal%2520against%2520state%2520of%2520the%2520art%2520models%2520on%250Aregression%2520and%2520classification%2520tasks%2520using%2520public%2520tabular%2520time-series%2520datasets.%250AOur%2520results%2520show%2520that%2520combining%2520row-wise%2520and%2520column-wise%2520attention%2520improves%250Aperformance%2520without%2520increasing%2520model%2520size.%2520Code%2520and%2520data%2520are%2520available%2520at%250Ahttps%253A//github.com/raphaaal/fieldy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15327v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-grained%20Attention%20in%20Hierarchical%20Transformers%20for%20Tabular%0A%20%20Time-series&entry.906535625=Raphael%20Azorin%20and%20Zied%20Ben%20Houidi%20and%20Massimo%20Gallo%20and%20Alessandro%20Finamore%20and%20Pietro%20Michiardi&entry.1292438233=%20%20Tabular%20data%20is%20ubiquitous%20in%20many%20real-life%20systems.%20In%20particular%2C%0Atime-dependent%20tabular%20data%2C%20where%20rows%20are%20chronologically%20related%2C%20is%0Atypically%20used%20for%20recording%20historical%20events%2C%20e.g.%2C%20financial%20transactions%2C%0Ahealthcare%20records%2C%20or%20stock%20history.%20Recently%2C%20hierarchical%20variants%20of%20the%0Aattention%20mechanism%20of%20transformer%20architectures%20have%20been%20used%20to%20model%0Atabular%20time-series%20data.%20At%20first%2C%20rows%20%28or%20columns%29%20are%20encoded%20separately%20by%0Acomputing%20attention%20between%20their%20fields.%20Subsequently%2C%20encoded%20rows%20%28or%0Acolumns%29%20are%20attended%20to%20one%20another%20to%20model%20the%20entire%20tabular%20time-series.%0AWhile%20efficient%2C%20this%20approach%20constrains%20the%20attention%20granularity%20and%20limits%0Aits%20ability%20to%20learn%20patterns%20at%20the%20field-level%20across%20separate%20rows%2C%20or%0Acolumns.%20We%20take%20a%20first%20step%20to%20address%20this%20gap%20by%20proposing%20Fieldy%2C%20a%0Afine-grained%20hierarchical%20model%20that%20contextualizes%20fields%20at%20both%20the%20row%20and%0Acolumn%20levels.%20We%20compare%20our%20proposal%20against%20state%20of%20the%20art%20models%20on%0Aregression%20and%20classification%20tasks%20using%20public%20tabular%20time-series%20datasets.%0AOur%20results%20show%20that%20combining%20row-wise%20and%20column-wise%20attention%20improves%0Aperformance%20without%20increasing%20model%20size.%20Code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/raphaaal/fieldy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15327v1&entry.124074799=Read"},
{"title": "SiT: Symmetry-Invariant Transformers for Generalisation in Reinforcement\n  Learning", "author": "Matthias Weissenbacher and Rishabh Agarwal and Yoshinobu Kawahara", "abstract": "  An open challenge in reinforcement learning (RL) is the effective deployment\nof a trained policy to new or slightly different situations as well as\nsemantically-similar environments. We introduce Symmetry-Invariant Transformer\n(SiT), a scalable vision transformer (ViT) that leverages both local and global\ndata patterns in a self-supervised manner to improve generalisation. Central to\nour approach is Graph Symmetric Attention, which refines the traditional\nself-attention mechanism to preserve graph symmetries, resulting in invariant\nand equivariant latent representations. We showcase SiT's superior\ngeneralization over ViTs on MiniGrid and Procgen RL benchmarks, and its sample\nefficiency on Atari 100k and CIFAR10.\n", "link": "http://arxiv.org/abs/2406.15025v1", "date": "2024-06-21", "relevancy": 2.0109, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5608}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4936}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SiT%3A%20Symmetry-Invariant%20Transformers%20for%20Generalisation%20in%20Reinforcement%0A%20%20Learning&body=Title%3A%20SiT%3A%20Symmetry-Invariant%20Transformers%20for%20Generalisation%20in%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Matthias%20Weissenbacher%20and%20Rishabh%20Agarwal%20and%20Yoshinobu%20Kawahara%0AAbstract%3A%20%20%20An%20open%20challenge%20in%20reinforcement%20learning%20%28RL%29%20is%20the%20effective%20deployment%0Aof%20a%20trained%20policy%20to%20new%20or%20slightly%20different%20situations%20as%20well%20as%0Asemantically-similar%20environments.%20We%20introduce%20Symmetry-Invariant%20Transformer%0A%28SiT%29%2C%20a%20scalable%20vision%20transformer%20%28ViT%29%20that%20leverages%20both%20local%20and%20global%0Adata%20patterns%20in%20a%20self-supervised%20manner%20to%20improve%20generalisation.%20Central%20to%0Aour%20approach%20is%20Graph%20Symmetric%20Attention%2C%20which%20refines%20the%20traditional%0Aself-attention%20mechanism%20to%20preserve%20graph%20symmetries%2C%20resulting%20in%20invariant%0Aand%20equivariant%20latent%20representations.%20We%20showcase%20SiT%27s%20superior%0Ageneralization%20over%20ViTs%20on%20MiniGrid%20and%20Procgen%20RL%20benchmarks%2C%20and%20its%20sample%0Aefficiency%20on%20Atari%20100k%20and%20CIFAR10.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15025v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSiT%253A%2520Symmetry-Invariant%2520Transformers%2520for%2520Generalisation%2520in%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DMatthias%2520Weissenbacher%2520and%2520Rishabh%2520Agarwal%2520and%2520Yoshinobu%2520Kawahara%26entry.1292438233%3D%2520%2520An%2520open%2520challenge%2520in%2520reinforcement%2520learning%2520%2528RL%2529%2520is%2520the%2520effective%2520deployment%250Aof%2520a%2520trained%2520policy%2520to%2520new%2520or%2520slightly%2520different%2520situations%2520as%2520well%2520as%250Asemantically-similar%2520environments.%2520We%2520introduce%2520Symmetry-Invariant%2520Transformer%250A%2528SiT%2529%252C%2520a%2520scalable%2520vision%2520transformer%2520%2528ViT%2529%2520that%2520leverages%2520both%2520local%2520and%2520global%250Adata%2520patterns%2520in%2520a%2520self-supervised%2520manner%2520to%2520improve%2520generalisation.%2520Central%2520to%250Aour%2520approach%2520is%2520Graph%2520Symmetric%2520Attention%252C%2520which%2520refines%2520the%2520traditional%250Aself-attention%2520mechanism%2520to%2520preserve%2520graph%2520symmetries%252C%2520resulting%2520in%2520invariant%250Aand%2520equivariant%2520latent%2520representations.%2520We%2520showcase%2520SiT%2527s%2520superior%250Ageneralization%2520over%2520ViTs%2520on%2520MiniGrid%2520and%2520Procgen%2520RL%2520benchmarks%252C%2520and%2520its%2520sample%250Aefficiency%2520on%2520Atari%2520100k%2520and%2520CIFAR10.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15025v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SiT%3A%20Symmetry-Invariant%20Transformers%20for%20Generalisation%20in%20Reinforcement%0A%20%20Learning&entry.906535625=Matthias%20Weissenbacher%20and%20Rishabh%20Agarwal%20and%20Yoshinobu%20Kawahara&entry.1292438233=%20%20An%20open%20challenge%20in%20reinforcement%20learning%20%28RL%29%20is%20the%20effective%20deployment%0Aof%20a%20trained%20policy%20to%20new%20or%20slightly%20different%20situations%20as%20well%20as%0Asemantically-similar%20environments.%20We%20introduce%20Symmetry-Invariant%20Transformer%0A%28SiT%29%2C%20a%20scalable%20vision%20transformer%20%28ViT%29%20that%20leverages%20both%20local%20and%20global%0Adata%20patterns%20in%20a%20self-supervised%20manner%20to%20improve%20generalisation.%20Central%20to%0Aour%20approach%20is%20Graph%20Symmetric%20Attention%2C%20which%20refines%20the%20traditional%0Aself-attention%20mechanism%20to%20preserve%20graph%20symmetries%2C%20resulting%20in%20invariant%0Aand%20equivariant%20latent%20representations.%20We%20showcase%20SiT%27s%20superior%0Ageneralization%20over%20ViTs%20on%20MiniGrid%20and%20Procgen%20RL%20benchmarks%2C%20and%20its%20sample%0Aefficiency%20on%20Atari%20100k%20and%20CIFAR10.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15025v1&entry.124074799=Read"},
{"title": "Uncertainty-Aware Probabilistic Graph Neural Networks for Road-Level\n  Traffic Accident Prediction", "author": "Xiaowei Gao and Xinke Jiang and Dingyi Zhuang and Huanfa Chen and Shenhao Wang and Stephen Law and James Haworth", "abstract": "  Traffic accidents present substantial challenges to human safety and\nsocioeconomic development in urban areas. Developing a reliable and responsible\ntraffic accident prediction model is crucial to addressing growing public\nsafety concerns and enhancing the safety of urban mobility systems. Traditional\nmethods face limitations at fine spatiotemporal scales due to the sporadic\nnature of highrisk accidents and the predominance of nonaccident\ncharacteristics. Furthermore, while most current models show promising\noccurrence prediction, they overlook the uncertainties arising from the\ninherent nature of accidents, and then fail to adequately map the hierarchical\nranking of accident risk values for more precise insights. To address these\nissues, we introduce the Spatiotemporal ZeroInflated Tweedie Graph Neural\nNetwork ,STZITDGNN, the first uncertainty-aware probabilistic graph deep\nlearning model in roadlevel traffic accident prediction for multi-steps. This\nmodel integrates the interpretability of the statistical Tweedie family model\nand the expressive power of graph neural networks. Its decoder innovatively\nemploys a compound Tweedie model, a Poisson distribution to model the frequency\nof accident occurrences and a Gamma distribution to assess injury severity,\nsupplemented by a zeroinflated component to effectively identify exessive\nnon-incident instances. Empirical tests using realworld traffic data from\nLondon, UK, demonstrate that the STZITDGNN surpasses other baseline models\nacross multiple benchmarks and metrics, including accident risk value\nprediction, uncertainty minimisation, nonaccident road identification and\naccident occurrence accuracy. Our study demonstrates that STZTIDGNN can\neffectively inform targeted road monitoring, thereby improving urban road\nsafety strategies.\n", "link": "http://arxiv.org/abs/2309.05072v2", "date": "2024-06-21", "relevancy": 2.0049, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5089}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5086}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4908}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty-Aware%20Probabilistic%20Graph%20Neural%20Networks%20for%20Road-Level%0A%20%20Traffic%20Accident%20Prediction&body=Title%3A%20Uncertainty-Aware%20Probabilistic%20Graph%20Neural%20Networks%20for%20Road-Level%0A%20%20Traffic%20Accident%20Prediction%0AAuthor%3A%20Xiaowei%20Gao%20and%20Xinke%20Jiang%20and%20Dingyi%20Zhuang%20and%20Huanfa%20Chen%20and%20Shenhao%20Wang%20and%20Stephen%20Law%20and%20James%20Haworth%0AAbstract%3A%20%20%20Traffic%20accidents%20present%20substantial%20challenges%20to%20human%20safety%20and%0Asocioeconomic%20development%20in%20urban%20areas.%20Developing%20a%20reliable%20and%20responsible%0Atraffic%20accident%20prediction%20model%20is%20crucial%20to%20addressing%20growing%20public%0Asafety%20concerns%20and%20enhancing%20the%20safety%20of%20urban%20mobility%20systems.%20Traditional%0Amethods%20face%20limitations%20at%20fine%20spatiotemporal%20scales%20due%20to%20the%20sporadic%0Anature%20of%20highrisk%20accidents%20and%20the%20predominance%20of%20nonaccident%0Acharacteristics.%20Furthermore%2C%20while%20most%20current%20models%20show%20promising%0Aoccurrence%20prediction%2C%20they%20overlook%20the%20uncertainties%20arising%20from%20the%0Ainherent%20nature%20of%20accidents%2C%20and%20then%20fail%20to%20adequately%20map%20the%20hierarchical%0Aranking%20of%20accident%20risk%20values%20for%20more%20precise%20insights.%20To%20address%20these%0Aissues%2C%20we%20introduce%20the%20Spatiotemporal%20ZeroInflated%20Tweedie%20Graph%20Neural%0ANetwork%20%2CSTZITDGNN%2C%20the%20first%20uncertainty-aware%20probabilistic%20graph%20deep%0Alearning%20model%20in%20roadlevel%20traffic%20accident%20prediction%20for%20multi-steps.%20This%0Amodel%20integrates%20the%20interpretability%20of%20the%20statistical%20Tweedie%20family%20model%0Aand%20the%20expressive%20power%20of%20graph%20neural%20networks.%20Its%20decoder%20innovatively%0Aemploys%20a%20compound%20Tweedie%20model%2C%20a%20Poisson%20distribution%20to%20model%20the%20frequency%0Aof%20accident%20occurrences%20and%20a%20Gamma%20distribution%20to%20assess%20injury%20severity%2C%0Asupplemented%20by%20a%20zeroinflated%20component%20to%20effectively%20identify%20exessive%0Anon-incident%20instances.%20Empirical%20tests%20using%20realworld%20traffic%20data%20from%0ALondon%2C%20UK%2C%20demonstrate%20that%20the%20STZITDGNN%20surpasses%20other%20baseline%20models%0Aacross%20multiple%20benchmarks%20and%20metrics%2C%20including%20accident%20risk%20value%0Aprediction%2C%20uncertainty%20minimisation%2C%20nonaccident%20road%20identification%20and%0Aaccident%20occurrence%20accuracy.%20Our%20study%20demonstrates%20that%20STZTIDGNN%20can%0Aeffectively%20inform%20targeted%20road%20monitoring%2C%20thereby%20improving%20urban%20road%0Asafety%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.05072v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty-Aware%2520Probabilistic%2520Graph%2520Neural%2520Networks%2520for%2520Road-Level%250A%2520%2520Traffic%2520Accident%2520Prediction%26entry.906535625%3DXiaowei%2520Gao%2520and%2520Xinke%2520Jiang%2520and%2520Dingyi%2520Zhuang%2520and%2520Huanfa%2520Chen%2520and%2520Shenhao%2520Wang%2520and%2520Stephen%2520Law%2520and%2520James%2520Haworth%26entry.1292438233%3D%2520%2520Traffic%2520accidents%2520present%2520substantial%2520challenges%2520to%2520human%2520safety%2520and%250Asocioeconomic%2520development%2520in%2520urban%2520areas.%2520Developing%2520a%2520reliable%2520and%2520responsible%250Atraffic%2520accident%2520prediction%2520model%2520is%2520crucial%2520to%2520addressing%2520growing%2520public%250Asafety%2520concerns%2520and%2520enhancing%2520the%2520safety%2520of%2520urban%2520mobility%2520systems.%2520Traditional%250Amethods%2520face%2520limitations%2520at%2520fine%2520spatiotemporal%2520scales%2520due%2520to%2520the%2520sporadic%250Anature%2520of%2520highrisk%2520accidents%2520and%2520the%2520predominance%2520of%2520nonaccident%250Acharacteristics.%2520Furthermore%252C%2520while%2520most%2520current%2520models%2520show%2520promising%250Aoccurrence%2520prediction%252C%2520they%2520overlook%2520the%2520uncertainties%2520arising%2520from%2520the%250Ainherent%2520nature%2520of%2520accidents%252C%2520and%2520then%2520fail%2520to%2520adequately%2520map%2520the%2520hierarchical%250Aranking%2520of%2520accident%2520risk%2520values%2520for%2520more%2520precise%2520insights.%2520To%2520address%2520these%250Aissues%252C%2520we%2520introduce%2520the%2520Spatiotemporal%2520ZeroInflated%2520Tweedie%2520Graph%2520Neural%250ANetwork%2520%252CSTZITDGNN%252C%2520the%2520first%2520uncertainty-aware%2520probabilistic%2520graph%2520deep%250Alearning%2520model%2520in%2520roadlevel%2520traffic%2520accident%2520prediction%2520for%2520multi-steps.%2520This%250Amodel%2520integrates%2520the%2520interpretability%2520of%2520the%2520statistical%2520Tweedie%2520family%2520model%250Aand%2520the%2520expressive%2520power%2520of%2520graph%2520neural%2520networks.%2520Its%2520decoder%2520innovatively%250Aemploys%2520a%2520compound%2520Tweedie%2520model%252C%2520a%2520Poisson%2520distribution%2520to%2520model%2520the%2520frequency%250Aof%2520accident%2520occurrences%2520and%2520a%2520Gamma%2520distribution%2520to%2520assess%2520injury%2520severity%252C%250Asupplemented%2520by%2520a%2520zeroinflated%2520component%2520to%2520effectively%2520identify%2520exessive%250Anon-incident%2520instances.%2520Empirical%2520tests%2520using%2520realworld%2520traffic%2520data%2520from%250ALondon%252C%2520UK%252C%2520demonstrate%2520that%2520the%2520STZITDGNN%2520surpasses%2520other%2520baseline%2520models%250Aacross%2520multiple%2520benchmarks%2520and%2520metrics%252C%2520including%2520accident%2520risk%2520value%250Aprediction%252C%2520uncertainty%2520minimisation%252C%2520nonaccident%2520road%2520identification%2520and%250Aaccident%2520occurrence%2520accuracy.%2520Our%2520study%2520demonstrates%2520that%2520STZTIDGNN%2520can%250Aeffectively%2520inform%2520targeted%2520road%2520monitoring%252C%2520thereby%2520improving%2520urban%2520road%250Asafety%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.05072v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty-Aware%20Probabilistic%20Graph%20Neural%20Networks%20for%20Road-Level%0A%20%20Traffic%20Accident%20Prediction&entry.906535625=Xiaowei%20Gao%20and%20Xinke%20Jiang%20and%20Dingyi%20Zhuang%20and%20Huanfa%20Chen%20and%20Shenhao%20Wang%20and%20Stephen%20Law%20and%20James%20Haworth&entry.1292438233=%20%20Traffic%20accidents%20present%20substantial%20challenges%20to%20human%20safety%20and%0Asocioeconomic%20development%20in%20urban%20areas.%20Developing%20a%20reliable%20and%20responsible%0Atraffic%20accident%20prediction%20model%20is%20crucial%20to%20addressing%20growing%20public%0Asafety%20concerns%20and%20enhancing%20the%20safety%20of%20urban%20mobility%20systems.%20Traditional%0Amethods%20face%20limitations%20at%20fine%20spatiotemporal%20scales%20due%20to%20the%20sporadic%0Anature%20of%20highrisk%20accidents%20and%20the%20predominance%20of%20nonaccident%0Acharacteristics.%20Furthermore%2C%20while%20most%20current%20models%20show%20promising%0Aoccurrence%20prediction%2C%20they%20overlook%20the%20uncertainties%20arising%20from%20the%0Ainherent%20nature%20of%20accidents%2C%20and%20then%20fail%20to%20adequately%20map%20the%20hierarchical%0Aranking%20of%20accident%20risk%20values%20for%20more%20precise%20insights.%20To%20address%20these%0Aissues%2C%20we%20introduce%20the%20Spatiotemporal%20ZeroInflated%20Tweedie%20Graph%20Neural%0ANetwork%20%2CSTZITDGNN%2C%20the%20first%20uncertainty-aware%20probabilistic%20graph%20deep%0Alearning%20model%20in%20roadlevel%20traffic%20accident%20prediction%20for%20multi-steps.%20This%0Amodel%20integrates%20the%20interpretability%20of%20the%20statistical%20Tweedie%20family%20model%0Aand%20the%20expressive%20power%20of%20graph%20neural%20networks.%20Its%20decoder%20innovatively%0Aemploys%20a%20compound%20Tweedie%20model%2C%20a%20Poisson%20distribution%20to%20model%20the%20frequency%0Aof%20accident%20occurrences%20and%20a%20Gamma%20distribution%20to%20assess%20injury%20severity%2C%0Asupplemented%20by%20a%20zeroinflated%20component%20to%20effectively%20identify%20exessive%0Anon-incident%20instances.%20Empirical%20tests%20using%20realworld%20traffic%20data%20from%0ALondon%2C%20UK%2C%20demonstrate%20that%20the%20STZITDGNN%20surpasses%20other%20baseline%20models%0Aacross%20multiple%20benchmarks%20and%20metrics%2C%20including%20accident%20risk%20value%0Aprediction%2C%20uncertainty%20minimisation%2C%20nonaccident%20road%20identification%20and%0Aaccident%20occurrence%20accuracy.%20Our%20study%20demonstrates%20that%20STZTIDGNN%20can%0Aeffectively%20inform%20targeted%20road%20monitoring%2C%20thereby%20improving%20urban%20road%0Asafety%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.05072v2&entry.124074799=Read"},
{"title": "Embracing Federated Learning: Enabling Weak Client Participation via\n  Partial Model Training", "author": "Sunwoo Lee and Tuo Zhang and Saurav Prakash and Yue Niu and Salman Avestimehr", "abstract": "  In Federated Learning (FL), clients may have weak devices that cannot train\nthe full model or even hold it in their memory space. To implement large-scale\nFL applications, thus, it is crucial to develop a distributed learning method\nthat enables the participation of such weak clients. We propose EmbracingFL, a\ngeneral FL framework that allows all available clients to join the distributed\ntraining regardless of their system resource capacity. The framework is built\nupon a novel form of partial model training method in which each client trains\nas many consecutive output-side layers as its system resources allow. Our study\ndemonstrates that EmbracingFL encourages each layer to have similar data\nrepresentations across clients, improving FL efficiency. The proposed partial\nmodel training method guarantees convergence to a neighbor of stationary points\nfor non-convex and smooth problems. We evaluate the efficacy of EmbracingFL\nunder a variety of settings with a mixed number of strong, moderate (~40%\nmemory), and weak (~15% memory) clients, datasets (CIFAR-10, FEMNIST, and\nIMDB), and models (ResNet20, CNN, and LSTM). Our empirical study shows that\nEmbracingFL consistently achieves high accuracy as like all clients are strong,\noutperforming the state-of-the-art width reduction methods (i.e. HeteroFL and\nFjORD).\n", "link": "http://arxiv.org/abs/2406.15125v1", "date": "2024-06-21", "relevancy": 1.9838, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5069}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.491}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embracing%20Federated%20Learning%3A%20Enabling%20Weak%20Client%20Participation%20via%0A%20%20Partial%20Model%20Training&body=Title%3A%20Embracing%20Federated%20Learning%3A%20Enabling%20Weak%20Client%20Participation%20via%0A%20%20Partial%20Model%20Training%0AAuthor%3A%20Sunwoo%20Lee%20and%20Tuo%20Zhang%20and%20Saurav%20Prakash%20and%20Yue%20Niu%20and%20Salman%20Avestimehr%0AAbstract%3A%20%20%20In%20Federated%20Learning%20%28FL%29%2C%20clients%20may%20have%20weak%20devices%20that%20cannot%20train%0Athe%20full%20model%20or%20even%20hold%20it%20in%20their%20memory%20space.%20To%20implement%20large-scale%0AFL%20applications%2C%20thus%2C%20it%20is%20crucial%20to%20develop%20a%20distributed%20learning%20method%0Athat%20enables%20the%20participation%20of%20such%20weak%20clients.%20We%20propose%20EmbracingFL%2C%20a%0Ageneral%20FL%20framework%20that%20allows%20all%20available%20clients%20to%20join%20the%20distributed%0Atraining%20regardless%20of%20their%20system%20resource%20capacity.%20The%20framework%20is%20built%0Aupon%20a%20novel%20form%20of%20partial%20model%20training%20method%20in%20which%20each%20client%20trains%0Aas%20many%20consecutive%20output-side%20layers%20as%20its%20system%20resources%20allow.%20Our%20study%0Ademonstrates%20that%20EmbracingFL%20encourages%20each%20layer%20to%20have%20similar%20data%0Arepresentations%20across%20clients%2C%20improving%20FL%20efficiency.%20The%20proposed%20partial%0Amodel%20training%20method%20guarantees%20convergence%20to%20a%20neighbor%20of%20stationary%20points%0Afor%20non-convex%20and%20smooth%20problems.%20We%20evaluate%20the%20efficacy%20of%20EmbracingFL%0Aunder%20a%20variety%20of%20settings%20with%20a%20mixed%20number%20of%20strong%2C%20moderate%20%28~40%25%0Amemory%29%2C%20and%20weak%20%28~15%25%20memory%29%20clients%2C%20datasets%20%28CIFAR-10%2C%20FEMNIST%2C%20and%0AIMDB%29%2C%20and%20models%20%28ResNet20%2C%20CNN%2C%20and%20LSTM%29.%20Our%20empirical%20study%20shows%20that%0AEmbracingFL%20consistently%20achieves%20high%20accuracy%20as%20like%20all%20clients%20are%20strong%2C%0Aoutperforming%20the%20state-of-the-art%20width%20reduction%20methods%20%28i.e.%20HeteroFL%20and%0AFjORD%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15125v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbracing%2520Federated%2520Learning%253A%2520Enabling%2520Weak%2520Client%2520Participation%2520via%250A%2520%2520Partial%2520Model%2520Training%26entry.906535625%3DSunwoo%2520Lee%2520and%2520Tuo%2520Zhang%2520and%2520Saurav%2520Prakash%2520and%2520Yue%2520Niu%2520and%2520Salman%2520Avestimehr%26entry.1292438233%3D%2520%2520In%2520Federated%2520Learning%2520%2528FL%2529%252C%2520clients%2520may%2520have%2520weak%2520devices%2520that%2520cannot%2520train%250Athe%2520full%2520model%2520or%2520even%2520hold%2520it%2520in%2520their%2520memory%2520space.%2520To%2520implement%2520large-scale%250AFL%2520applications%252C%2520thus%252C%2520it%2520is%2520crucial%2520to%2520develop%2520a%2520distributed%2520learning%2520method%250Athat%2520enables%2520the%2520participation%2520of%2520such%2520weak%2520clients.%2520We%2520propose%2520EmbracingFL%252C%2520a%250Ageneral%2520FL%2520framework%2520that%2520allows%2520all%2520available%2520clients%2520to%2520join%2520the%2520distributed%250Atraining%2520regardless%2520of%2520their%2520system%2520resource%2520capacity.%2520The%2520framework%2520is%2520built%250Aupon%2520a%2520novel%2520form%2520of%2520partial%2520model%2520training%2520method%2520in%2520which%2520each%2520client%2520trains%250Aas%2520many%2520consecutive%2520output-side%2520layers%2520as%2520its%2520system%2520resources%2520allow.%2520Our%2520study%250Ademonstrates%2520that%2520EmbracingFL%2520encourages%2520each%2520layer%2520to%2520have%2520similar%2520data%250Arepresentations%2520across%2520clients%252C%2520improving%2520FL%2520efficiency.%2520The%2520proposed%2520partial%250Amodel%2520training%2520method%2520guarantees%2520convergence%2520to%2520a%2520neighbor%2520of%2520stationary%2520points%250Afor%2520non-convex%2520and%2520smooth%2520problems.%2520We%2520evaluate%2520the%2520efficacy%2520of%2520EmbracingFL%250Aunder%2520a%2520variety%2520of%2520settings%2520with%2520a%2520mixed%2520number%2520of%2520strong%252C%2520moderate%2520%2528~40%2525%250Amemory%2529%252C%2520and%2520weak%2520%2528~15%2525%2520memory%2529%2520clients%252C%2520datasets%2520%2528CIFAR-10%252C%2520FEMNIST%252C%2520and%250AIMDB%2529%252C%2520and%2520models%2520%2528ResNet20%252C%2520CNN%252C%2520and%2520LSTM%2529.%2520Our%2520empirical%2520study%2520shows%2520that%250AEmbracingFL%2520consistently%2520achieves%2520high%2520accuracy%2520as%2520like%2520all%2520clients%2520are%2520strong%252C%250Aoutperforming%2520the%2520state-of-the-art%2520width%2520reduction%2520methods%2520%2528i.e.%2520HeteroFL%2520and%250AFjORD%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15125v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embracing%20Federated%20Learning%3A%20Enabling%20Weak%20Client%20Participation%20via%0A%20%20Partial%20Model%20Training&entry.906535625=Sunwoo%20Lee%20and%20Tuo%20Zhang%20and%20Saurav%20Prakash%20and%20Yue%20Niu%20and%20Salman%20Avestimehr&entry.1292438233=%20%20In%20Federated%20Learning%20%28FL%29%2C%20clients%20may%20have%20weak%20devices%20that%20cannot%20train%0Athe%20full%20model%20or%20even%20hold%20it%20in%20their%20memory%20space.%20To%20implement%20large-scale%0AFL%20applications%2C%20thus%2C%20it%20is%20crucial%20to%20develop%20a%20distributed%20learning%20method%0Athat%20enables%20the%20participation%20of%20such%20weak%20clients.%20We%20propose%20EmbracingFL%2C%20a%0Ageneral%20FL%20framework%20that%20allows%20all%20available%20clients%20to%20join%20the%20distributed%0Atraining%20regardless%20of%20their%20system%20resource%20capacity.%20The%20framework%20is%20built%0Aupon%20a%20novel%20form%20of%20partial%20model%20training%20method%20in%20which%20each%20client%20trains%0Aas%20many%20consecutive%20output-side%20layers%20as%20its%20system%20resources%20allow.%20Our%20study%0Ademonstrates%20that%20EmbracingFL%20encourages%20each%20layer%20to%20have%20similar%20data%0Arepresentations%20across%20clients%2C%20improving%20FL%20efficiency.%20The%20proposed%20partial%0Amodel%20training%20method%20guarantees%20convergence%20to%20a%20neighbor%20of%20stationary%20points%0Afor%20non-convex%20and%20smooth%20problems.%20We%20evaluate%20the%20efficacy%20of%20EmbracingFL%0Aunder%20a%20variety%20of%20settings%20with%20a%20mixed%20number%20of%20strong%2C%20moderate%20%28~40%25%0Amemory%29%2C%20and%20weak%20%28~15%25%20memory%29%20clients%2C%20datasets%20%28CIFAR-10%2C%20FEMNIST%2C%20and%0AIMDB%29%2C%20and%20models%20%28ResNet20%2C%20CNN%2C%20and%20LSTM%29.%20Our%20empirical%20study%20shows%20that%0AEmbracingFL%20consistently%20achieves%20high%20accuracy%20as%20like%20all%20clients%20are%20strong%2C%0Aoutperforming%20the%20state-of-the-art%20width%20reduction%20methods%20%28i.e.%20HeteroFL%20and%0AFjORD%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15125v1&entry.124074799=Read"},
{"title": "Fine-grained analysis of non-parametric estimation for pairwise learning", "author": "Junyu Zhou and Shuo Huang and Han Feng and Puyu Wang and Ding-Xuan Zhou", "abstract": "  In this paper, we are concerned with the generalization performance of\nnon-parametric estimation for pairwise learning. Most of the existing work\nrequires the hypothesis space to be convex or a VC-class, and the loss to be\nconvex. However, these restrictive assumptions limit the applicability of the\nresults in studying many popular methods, especially kernel methods and neural\nnetworks. We significantly relax these restrictive assumptions and establish a\nsharp oracle inequality of the empirical minimizer with a general hypothesis\nspace for the Lipschitz continuous pairwise losses. Our results can be used to\nhandle a wide range of pairwise learning problems including ranking, AUC\nmaximization, pairwise regression, and metric and similarity learning. As an\napplication, we apply our general results to study pairwise least squares\nregression and derive an excess generalization bound that matches the minimax\nlower bound for pointwise least squares regression up to a logrithmic term. The\nkey novelty here is to construct a structured deep ReLU neural network as an\napproximation of the true predictor and design the targeted hypothesis space\nconsisting of the structured networks with controllable complexity. This\nsuccessful application demonstrates that the obtained general results indeed\nhelp us to explore the generalization performance on a variety of problems that\ncannot be handled by existing approaches.\n", "link": "http://arxiv.org/abs/2305.19640v2", "date": "2024-06-21", "relevancy": 1.963, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5287}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4853}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-grained%20analysis%20of%20non-parametric%20estimation%20for%20pairwise%20learning&body=Title%3A%20Fine-grained%20analysis%20of%20non-parametric%20estimation%20for%20pairwise%20learning%0AAuthor%3A%20Junyu%20Zhou%20and%20Shuo%20Huang%20and%20Han%20Feng%20and%20Puyu%20Wang%20and%20Ding-Xuan%20Zhou%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20are%20concerned%20with%20the%20generalization%20performance%20of%0Anon-parametric%20estimation%20for%20pairwise%20learning.%20Most%20of%20the%20existing%20work%0Arequires%20the%20hypothesis%20space%20to%20be%20convex%20or%20a%20VC-class%2C%20and%20the%20loss%20to%20be%0Aconvex.%20However%2C%20these%20restrictive%20assumptions%20limit%20the%20applicability%20of%20the%0Aresults%20in%20studying%20many%20popular%20methods%2C%20especially%20kernel%20methods%20and%20neural%0Anetworks.%20We%20significantly%20relax%20these%20restrictive%20assumptions%20and%20establish%20a%0Asharp%20oracle%20inequality%20of%20the%20empirical%20minimizer%20with%20a%20general%20hypothesis%0Aspace%20for%20the%20Lipschitz%20continuous%20pairwise%20losses.%20Our%20results%20can%20be%20used%20to%0Ahandle%20a%20wide%20range%20of%20pairwise%20learning%20problems%20including%20ranking%2C%20AUC%0Amaximization%2C%20pairwise%20regression%2C%20and%20metric%20and%20similarity%20learning.%20As%20an%0Aapplication%2C%20we%20apply%20our%20general%20results%20to%20study%20pairwise%20least%20squares%0Aregression%20and%20derive%20an%20excess%20generalization%20bound%20that%20matches%20the%20minimax%0Alower%20bound%20for%20pointwise%20least%20squares%20regression%20up%20to%20a%20logrithmic%20term.%20The%0Akey%20novelty%20here%20is%20to%20construct%20a%20structured%20deep%20ReLU%20neural%20network%20as%20an%0Aapproximation%20of%20the%20true%20predictor%20and%20design%20the%20targeted%20hypothesis%20space%0Aconsisting%20of%20the%20structured%20networks%20with%20controllable%20complexity.%20This%0Asuccessful%20application%20demonstrates%20that%20the%20obtained%20general%20results%20indeed%0Ahelp%20us%20to%20explore%20the%20generalization%20performance%20on%20a%20variety%20of%20problems%20that%0Acannot%20be%20handled%20by%20existing%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.19640v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-grained%2520analysis%2520of%2520non-parametric%2520estimation%2520for%2520pairwise%2520learning%26entry.906535625%3DJunyu%2520Zhou%2520and%2520Shuo%2520Huang%2520and%2520Han%2520Feng%2520and%2520Puyu%2520Wang%2520and%2520Ding-Xuan%2520Zhou%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520are%2520concerned%2520with%2520the%2520generalization%2520performance%2520of%250Anon-parametric%2520estimation%2520for%2520pairwise%2520learning.%2520Most%2520of%2520the%2520existing%2520work%250Arequires%2520the%2520hypothesis%2520space%2520to%2520be%2520convex%2520or%2520a%2520VC-class%252C%2520and%2520the%2520loss%2520to%2520be%250Aconvex.%2520However%252C%2520these%2520restrictive%2520assumptions%2520limit%2520the%2520applicability%2520of%2520the%250Aresults%2520in%2520studying%2520many%2520popular%2520methods%252C%2520especially%2520kernel%2520methods%2520and%2520neural%250Anetworks.%2520We%2520significantly%2520relax%2520these%2520restrictive%2520assumptions%2520and%2520establish%2520a%250Asharp%2520oracle%2520inequality%2520of%2520the%2520empirical%2520minimizer%2520with%2520a%2520general%2520hypothesis%250Aspace%2520for%2520the%2520Lipschitz%2520continuous%2520pairwise%2520losses.%2520Our%2520results%2520can%2520be%2520used%2520to%250Ahandle%2520a%2520wide%2520range%2520of%2520pairwise%2520learning%2520problems%2520including%2520ranking%252C%2520AUC%250Amaximization%252C%2520pairwise%2520regression%252C%2520and%2520metric%2520and%2520similarity%2520learning.%2520As%2520an%250Aapplication%252C%2520we%2520apply%2520our%2520general%2520results%2520to%2520study%2520pairwise%2520least%2520squares%250Aregression%2520and%2520derive%2520an%2520excess%2520generalization%2520bound%2520that%2520matches%2520the%2520minimax%250Alower%2520bound%2520for%2520pointwise%2520least%2520squares%2520regression%2520up%2520to%2520a%2520logrithmic%2520term.%2520The%250Akey%2520novelty%2520here%2520is%2520to%2520construct%2520a%2520structured%2520deep%2520ReLU%2520neural%2520network%2520as%2520an%250Aapproximation%2520of%2520the%2520true%2520predictor%2520and%2520design%2520the%2520targeted%2520hypothesis%2520space%250Aconsisting%2520of%2520the%2520structured%2520networks%2520with%2520controllable%2520complexity.%2520This%250Asuccessful%2520application%2520demonstrates%2520that%2520the%2520obtained%2520general%2520results%2520indeed%250Ahelp%2520us%2520to%2520explore%2520the%2520generalization%2520performance%2520on%2520a%2520variety%2520of%2520problems%2520that%250Acannot%2520be%2520handled%2520by%2520existing%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.19640v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-grained%20analysis%20of%20non-parametric%20estimation%20for%20pairwise%20learning&entry.906535625=Junyu%20Zhou%20and%20Shuo%20Huang%20and%20Han%20Feng%20and%20Puyu%20Wang%20and%20Ding-Xuan%20Zhou&entry.1292438233=%20%20In%20this%20paper%2C%20we%20are%20concerned%20with%20the%20generalization%20performance%20of%0Anon-parametric%20estimation%20for%20pairwise%20learning.%20Most%20of%20the%20existing%20work%0Arequires%20the%20hypothesis%20space%20to%20be%20convex%20or%20a%20VC-class%2C%20and%20the%20loss%20to%20be%0Aconvex.%20However%2C%20these%20restrictive%20assumptions%20limit%20the%20applicability%20of%20the%0Aresults%20in%20studying%20many%20popular%20methods%2C%20especially%20kernel%20methods%20and%20neural%0Anetworks.%20We%20significantly%20relax%20these%20restrictive%20assumptions%20and%20establish%20a%0Asharp%20oracle%20inequality%20of%20the%20empirical%20minimizer%20with%20a%20general%20hypothesis%0Aspace%20for%20the%20Lipschitz%20continuous%20pairwise%20losses.%20Our%20results%20can%20be%20used%20to%0Ahandle%20a%20wide%20range%20of%20pairwise%20learning%20problems%20including%20ranking%2C%20AUC%0Amaximization%2C%20pairwise%20regression%2C%20and%20metric%20and%20similarity%20learning.%20As%20an%0Aapplication%2C%20we%20apply%20our%20general%20results%20to%20study%20pairwise%20least%20squares%0Aregression%20and%20derive%20an%20excess%20generalization%20bound%20that%20matches%20the%20minimax%0Alower%20bound%20for%20pointwise%20least%20squares%20regression%20up%20to%20a%20logrithmic%20term.%20The%0Akey%20novelty%20here%20is%20to%20construct%20a%20structured%20deep%20ReLU%20neural%20network%20as%20an%0Aapproximation%20of%20the%20true%20predictor%20and%20design%20the%20targeted%20hypothesis%20space%0Aconsisting%20of%20the%20structured%20networks%20with%20controllable%20complexity.%20This%0Asuccessful%20application%20demonstrates%20that%20the%20obtained%20general%20results%20indeed%0Ahelp%20us%20to%20explore%20the%20generalization%20performance%20on%20a%20variety%20of%20problems%20that%0Acannot%20be%20handled%20by%20existing%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.19640v2&entry.124074799=Read"},
{"title": "Gradient-Mask Tuning Elevates the Upper Limits of LLM Performance", "author": "Haoling Li and Xin Zhang and Xiao Liu and Yeyun Gong and Yifan Wang and Yujiu Yang and Qi Chen and Peng Cheng", "abstract": "  Large language models (LLMs) have revolutionized lots of fields of research.\nAlthough it is well-known that fine-tuning is essential for enhancing the\ncapabilities of LLMs, existing research suggests that there is potential\nredundancy in the fine-tuning process and therefore proposes to update only a\nsubset of parameters. However, these methods fail to leverage the task-specific\ninformation to identify important parameters during training. Based on the\ninsight that gradients inherently contain information on task-specific data, we\npropose Gradient-Mask Tuning (GMT), a method that selectively updates\nparameters during training based on their gradient information. Specifically,\nwe compute the absolute values of the gradients and apply masking to those with\nrelatively smaller magnitudes. Our empirical results across various tasks\ndemonstrate that GMT not only outperforms traditional fine-tuning methods but\nalso elevates the upper limits of LLM performance. Further analysis indicates\nthat GMT exhibits insensitivity to mask ratio and possesses computational\nefficiency comparable to vanilla SFT.\n", "link": "http://arxiv.org/abs/2406.15330v1", "date": "2024-06-21", "relevancy": 1.9571, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5041}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4968}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient-Mask%20Tuning%20Elevates%20the%20Upper%20Limits%20of%20LLM%20Performance&body=Title%3A%20Gradient-Mask%20Tuning%20Elevates%20the%20Upper%20Limits%20of%20LLM%20Performance%0AAuthor%3A%20Haoling%20Li%20and%20Xin%20Zhang%20and%20Xiao%20Liu%20and%20Yeyun%20Gong%20and%20Yifan%20Wang%20and%20Yujiu%20Yang%20and%20Qi%20Chen%20and%20Peng%20Cheng%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20revolutionized%20lots%20of%20fields%20of%20research.%0AAlthough%20it%20is%20well-known%20that%20fine-tuning%20is%20essential%20for%20enhancing%20the%0Acapabilities%20of%20LLMs%2C%20existing%20research%20suggests%20that%20there%20is%20potential%0Aredundancy%20in%20the%20fine-tuning%20process%20and%20therefore%20proposes%20to%20update%20only%20a%0Asubset%20of%20parameters.%20However%2C%20these%20methods%20fail%20to%20leverage%20the%20task-specific%0Ainformation%20to%20identify%20important%20parameters%20during%20training.%20Based%20on%20the%0Ainsight%20that%20gradients%20inherently%20contain%20information%20on%20task-specific%20data%2C%20we%0Apropose%20Gradient-Mask%20Tuning%20%28GMT%29%2C%20a%20method%20that%20selectively%20updates%0Aparameters%20during%20training%20based%20on%20their%20gradient%20information.%20Specifically%2C%0Awe%20compute%20the%20absolute%20values%20of%20the%20gradients%20and%20apply%20masking%20to%20those%20with%0Arelatively%20smaller%20magnitudes.%20Our%20empirical%20results%20across%20various%20tasks%0Ademonstrate%20that%20GMT%20not%20only%20outperforms%20traditional%20fine-tuning%20methods%20but%0Aalso%20elevates%20the%20upper%20limits%20of%20LLM%20performance.%20Further%20analysis%20indicates%0Athat%20GMT%20exhibits%20insensitivity%20to%20mask%20ratio%20and%20possesses%20computational%0Aefficiency%20comparable%20to%20vanilla%20SFT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15330v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient-Mask%2520Tuning%2520Elevates%2520the%2520Upper%2520Limits%2520of%2520LLM%2520Performance%26entry.906535625%3DHaoling%2520Li%2520and%2520Xin%2520Zhang%2520and%2520Xiao%2520Liu%2520and%2520Yeyun%2520Gong%2520and%2520Yifan%2520Wang%2520and%2520Yujiu%2520Yang%2520and%2520Qi%2520Chen%2520and%2520Peng%2520Cheng%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520revolutionized%2520lots%2520of%2520fields%2520of%2520research.%250AAlthough%2520it%2520is%2520well-known%2520that%2520fine-tuning%2520is%2520essential%2520for%2520enhancing%2520the%250Acapabilities%2520of%2520LLMs%252C%2520existing%2520research%2520suggests%2520that%2520there%2520is%2520potential%250Aredundancy%2520in%2520the%2520fine-tuning%2520process%2520and%2520therefore%2520proposes%2520to%2520update%2520only%2520a%250Asubset%2520of%2520parameters.%2520However%252C%2520these%2520methods%2520fail%2520to%2520leverage%2520the%2520task-specific%250Ainformation%2520to%2520identify%2520important%2520parameters%2520during%2520training.%2520Based%2520on%2520the%250Ainsight%2520that%2520gradients%2520inherently%2520contain%2520information%2520on%2520task-specific%2520data%252C%2520we%250Apropose%2520Gradient-Mask%2520Tuning%2520%2528GMT%2529%252C%2520a%2520method%2520that%2520selectively%2520updates%250Aparameters%2520during%2520training%2520based%2520on%2520their%2520gradient%2520information.%2520Specifically%252C%250Awe%2520compute%2520the%2520absolute%2520values%2520of%2520the%2520gradients%2520and%2520apply%2520masking%2520to%2520those%2520with%250Arelatively%2520smaller%2520magnitudes.%2520Our%2520empirical%2520results%2520across%2520various%2520tasks%250Ademonstrate%2520that%2520GMT%2520not%2520only%2520outperforms%2520traditional%2520fine-tuning%2520methods%2520but%250Aalso%2520elevates%2520the%2520upper%2520limits%2520of%2520LLM%2520performance.%2520Further%2520analysis%2520indicates%250Athat%2520GMT%2520exhibits%2520insensitivity%2520to%2520mask%2520ratio%2520and%2520possesses%2520computational%250Aefficiency%2520comparable%2520to%2520vanilla%2520SFT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15330v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient-Mask%20Tuning%20Elevates%20the%20Upper%20Limits%20of%20LLM%20Performance&entry.906535625=Haoling%20Li%20and%20Xin%20Zhang%20and%20Xiao%20Liu%20and%20Yeyun%20Gong%20and%20Yifan%20Wang%20and%20Yujiu%20Yang%20and%20Qi%20Chen%20and%20Peng%20Cheng&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20revolutionized%20lots%20of%20fields%20of%20research.%0AAlthough%20it%20is%20well-known%20that%20fine-tuning%20is%20essential%20for%20enhancing%20the%0Acapabilities%20of%20LLMs%2C%20existing%20research%20suggests%20that%20there%20is%20potential%0Aredundancy%20in%20the%20fine-tuning%20process%20and%20therefore%20proposes%20to%20update%20only%20a%0Asubset%20of%20parameters.%20However%2C%20these%20methods%20fail%20to%20leverage%20the%20task-specific%0Ainformation%20to%20identify%20important%20parameters%20during%20training.%20Based%20on%20the%0Ainsight%20that%20gradients%20inherently%20contain%20information%20on%20task-specific%20data%2C%20we%0Apropose%20Gradient-Mask%20Tuning%20%28GMT%29%2C%20a%20method%20that%20selectively%20updates%0Aparameters%20during%20training%20based%20on%20their%20gradient%20information.%20Specifically%2C%0Awe%20compute%20the%20absolute%20values%20of%20the%20gradients%20and%20apply%20masking%20to%20those%20with%0Arelatively%20smaller%20magnitudes.%20Our%20empirical%20results%20across%20various%20tasks%0Ademonstrate%20that%20GMT%20not%20only%20outperforms%20traditional%20fine-tuning%20methods%20but%0Aalso%20elevates%20the%20upper%20limits%20of%20LLM%20performance.%20Further%20analysis%20indicates%0Athat%20GMT%20exhibits%20insensitivity%20to%20mask%20ratio%20and%20possesses%20computational%0Aefficiency%20comparable%20to%20vanilla%20SFT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15330v1&entry.124074799=Read"},
{"title": "NeuroCUT: A Neural Approach for Robust Graph Partitioning", "author": "Rishi Shah and Krishnanshu Jain and Sahil Manchanda and Sourav Medya and Sayan Ranu", "abstract": "  Graph partitioning aims to divide a graph into disjoint subsets while\noptimizing a specific partitioning objective. The majority of formulations\nrelated to graph partitioning exhibit NP-hardness due to their combinatorial\nnature. Conventional methods, like approximation algorithms or heuristics, are\ndesigned for distinct partitioning objectives and fail to achieve\ngeneralization across other important partitioning objectives. Recently machine\nlearning-based methods have been developed that learn directly from data.\nFurther, these methods have a distinct advantage of utilizing node features\nthat carry additional information. However, these methods assume\ndifferentiability of target partitioning objective functions and cannot\ngeneralize for an unknown number of partitions, i.e., they assume the number of\npartitions is provided in advance. In this study, we develop NeuroCUT with two\nkey innovations over previous methodologies. First, by leveraging a\nreinforcement learning-based framework over node representations derived from a\ngraph neural network and positional features, NeuroCUT can accommodate any\noptimization objective, even those with non-differentiable functions. Second,\nwe decouple the parameter space and the partition count making NeuroCUT\ninductive to any unseen number of partition, which is provided at query time.\nThrough empirical evaluation, we demonstrate that NeuroCUT excels in\nidentifying high-quality partitions, showcases strong generalization across a\nwide spectrum of partitioning objectives, and exhibits strong generalization to\nunseen partition count.\n", "link": "http://arxiv.org/abs/2310.11787v3", "date": "2024-06-21", "relevancy": 1.9565, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.498}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.487}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeuroCUT%3A%20A%20Neural%20Approach%20for%20Robust%20Graph%20Partitioning&body=Title%3A%20NeuroCUT%3A%20A%20Neural%20Approach%20for%20Robust%20Graph%20Partitioning%0AAuthor%3A%20Rishi%20Shah%20and%20Krishnanshu%20Jain%20and%20Sahil%20Manchanda%20and%20Sourav%20Medya%20and%20Sayan%20Ranu%0AAbstract%3A%20%20%20Graph%20partitioning%20aims%20to%20divide%20a%20graph%20into%20disjoint%20subsets%20while%0Aoptimizing%20a%20specific%20partitioning%20objective.%20The%20majority%20of%20formulations%0Arelated%20to%20graph%20partitioning%20exhibit%20NP-hardness%20due%20to%20their%20combinatorial%0Anature.%20Conventional%20methods%2C%20like%20approximation%20algorithms%20or%20heuristics%2C%20are%0Adesigned%20for%20distinct%20partitioning%20objectives%20and%20fail%20to%20achieve%0Ageneralization%20across%20other%20important%20partitioning%20objectives.%20Recently%20machine%0Alearning-based%20methods%20have%20been%20developed%20that%20learn%20directly%20from%20data.%0AFurther%2C%20these%20methods%20have%20a%20distinct%20advantage%20of%20utilizing%20node%20features%0Athat%20carry%20additional%20information.%20However%2C%20these%20methods%20assume%0Adifferentiability%20of%20target%20partitioning%20objective%20functions%20and%20cannot%0Ageneralize%20for%20an%20unknown%20number%20of%20partitions%2C%20i.e.%2C%20they%20assume%20the%20number%20of%0Apartitions%20is%20provided%20in%20advance.%20In%20this%20study%2C%20we%20develop%20NeuroCUT%20with%20two%0Akey%20innovations%20over%20previous%20methodologies.%20First%2C%20by%20leveraging%20a%0Areinforcement%20learning-based%20framework%20over%20node%20representations%20derived%20from%20a%0Agraph%20neural%20network%20and%20positional%20features%2C%20NeuroCUT%20can%20accommodate%20any%0Aoptimization%20objective%2C%20even%20those%20with%20non-differentiable%20functions.%20Second%2C%0Awe%20decouple%20the%20parameter%20space%20and%20the%20partition%20count%20making%20NeuroCUT%0Ainductive%20to%20any%20unseen%20number%20of%20partition%2C%20which%20is%20provided%20at%20query%20time.%0AThrough%20empirical%20evaluation%2C%20we%20demonstrate%20that%20NeuroCUT%20excels%20in%0Aidentifying%20high-quality%20partitions%2C%20showcases%20strong%20generalization%20across%20a%0Awide%20spectrum%20of%20partitioning%20objectives%2C%20and%20exhibits%20strong%20generalization%20to%0Aunseen%20partition%20count.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.11787v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuroCUT%253A%2520A%2520Neural%2520Approach%2520for%2520Robust%2520Graph%2520Partitioning%26entry.906535625%3DRishi%2520Shah%2520and%2520Krishnanshu%2520Jain%2520and%2520Sahil%2520Manchanda%2520and%2520Sourav%2520Medya%2520and%2520Sayan%2520Ranu%26entry.1292438233%3D%2520%2520Graph%2520partitioning%2520aims%2520to%2520divide%2520a%2520graph%2520into%2520disjoint%2520subsets%2520while%250Aoptimizing%2520a%2520specific%2520partitioning%2520objective.%2520The%2520majority%2520of%2520formulations%250Arelated%2520to%2520graph%2520partitioning%2520exhibit%2520NP-hardness%2520due%2520to%2520their%2520combinatorial%250Anature.%2520Conventional%2520methods%252C%2520like%2520approximation%2520algorithms%2520or%2520heuristics%252C%2520are%250Adesigned%2520for%2520distinct%2520partitioning%2520objectives%2520and%2520fail%2520to%2520achieve%250Ageneralization%2520across%2520other%2520important%2520partitioning%2520objectives.%2520Recently%2520machine%250Alearning-based%2520methods%2520have%2520been%2520developed%2520that%2520learn%2520directly%2520from%2520data.%250AFurther%252C%2520these%2520methods%2520have%2520a%2520distinct%2520advantage%2520of%2520utilizing%2520node%2520features%250Athat%2520carry%2520additional%2520information.%2520However%252C%2520these%2520methods%2520assume%250Adifferentiability%2520of%2520target%2520partitioning%2520objective%2520functions%2520and%2520cannot%250Ageneralize%2520for%2520an%2520unknown%2520number%2520of%2520partitions%252C%2520i.e.%252C%2520they%2520assume%2520the%2520number%2520of%250Apartitions%2520is%2520provided%2520in%2520advance.%2520In%2520this%2520study%252C%2520we%2520develop%2520NeuroCUT%2520with%2520two%250Akey%2520innovations%2520over%2520previous%2520methodologies.%2520First%252C%2520by%2520leveraging%2520a%250Areinforcement%2520learning-based%2520framework%2520over%2520node%2520representations%2520derived%2520from%2520a%250Agraph%2520neural%2520network%2520and%2520positional%2520features%252C%2520NeuroCUT%2520can%2520accommodate%2520any%250Aoptimization%2520objective%252C%2520even%2520those%2520with%2520non-differentiable%2520functions.%2520Second%252C%250Awe%2520decouple%2520the%2520parameter%2520space%2520and%2520the%2520partition%2520count%2520making%2520NeuroCUT%250Ainductive%2520to%2520any%2520unseen%2520number%2520of%2520partition%252C%2520which%2520is%2520provided%2520at%2520query%2520time.%250AThrough%2520empirical%2520evaluation%252C%2520we%2520demonstrate%2520that%2520NeuroCUT%2520excels%2520in%250Aidentifying%2520high-quality%2520partitions%252C%2520showcases%2520strong%2520generalization%2520across%2520a%250Awide%2520spectrum%2520of%2520partitioning%2520objectives%252C%2520and%2520exhibits%2520strong%2520generalization%2520to%250Aunseen%2520partition%2520count.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.11787v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuroCUT%3A%20A%20Neural%20Approach%20for%20Robust%20Graph%20Partitioning&entry.906535625=Rishi%20Shah%20and%20Krishnanshu%20Jain%20and%20Sahil%20Manchanda%20and%20Sourav%20Medya%20and%20Sayan%20Ranu&entry.1292438233=%20%20Graph%20partitioning%20aims%20to%20divide%20a%20graph%20into%20disjoint%20subsets%20while%0Aoptimizing%20a%20specific%20partitioning%20objective.%20The%20majority%20of%20formulations%0Arelated%20to%20graph%20partitioning%20exhibit%20NP-hardness%20due%20to%20their%20combinatorial%0Anature.%20Conventional%20methods%2C%20like%20approximation%20algorithms%20or%20heuristics%2C%20are%0Adesigned%20for%20distinct%20partitioning%20objectives%20and%20fail%20to%20achieve%0Ageneralization%20across%20other%20important%20partitioning%20objectives.%20Recently%20machine%0Alearning-based%20methods%20have%20been%20developed%20that%20learn%20directly%20from%20data.%0AFurther%2C%20these%20methods%20have%20a%20distinct%20advantage%20of%20utilizing%20node%20features%0Athat%20carry%20additional%20information.%20However%2C%20these%20methods%20assume%0Adifferentiability%20of%20target%20partitioning%20objective%20functions%20and%20cannot%0Ageneralize%20for%20an%20unknown%20number%20of%20partitions%2C%20i.e.%2C%20they%20assume%20the%20number%20of%0Apartitions%20is%20provided%20in%20advance.%20In%20this%20study%2C%20we%20develop%20NeuroCUT%20with%20two%0Akey%20innovations%20over%20previous%20methodologies.%20First%2C%20by%20leveraging%20a%0Areinforcement%20learning-based%20framework%20over%20node%20representations%20derived%20from%20a%0Agraph%20neural%20network%20and%20positional%20features%2C%20NeuroCUT%20can%20accommodate%20any%0Aoptimization%20objective%2C%20even%20those%20with%20non-differentiable%20functions.%20Second%2C%0Awe%20decouple%20the%20parameter%20space%20and%20the%20partition%20count%20making%20NeuroCUT%0Ainductive%20to%20any%20unseen%20number%20of%20partition%2C%20which%20is%20provided%20at%20query%20time.%0AThrough%20empirical%20evaluation%2C%20we%20demonstrate%20that%20NeuroCUT%20excels%20in%0Aidentifying%20high-quality%20partitions%2C%20showcases%20strong%20generalization%20across%20a%0Awide%20spectrum%20of%20partitioning%20objectives%2C%20and%20exhibits%20strong%20generalization%20to%0Aunseen%20partition%20count.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.11787v3&entry.124074799=Read"},
{"title": "DASB -- Discrete Audio and Speech Benchmark", "author": "Pooneh Mousavi and Luca Della Libera and Jarod Duret and Artem Ploujnikov and Cem Subakan and Mirco Ravanelli", "abstract": "  Discrete audio tokens have recently gained considerable attention for their\npotential to connect audio and language processing, enabling the creation of\nmodern multimodal large language models. Ideal audio tokens must effectively\npreserve phonetic and semantic content along with paralinguistic information,\nspeaker identity, and other details. While several types of audio tokens have\nbeen recently proposed, identifying the optimal tokenizer for various tasks is\nchallenging due to the inconsistent evaluation settings in existing studies. To\naddress this gap, we release the Discrete Audio and Speech Benchmark (DASB), a\ncomprehensive leaderboard for benchmarking discrete audio tokens across a wide\nrange of discriminative tasks, including speech recognition, speaker\nidentification and verification, emotion recognition, keyword spotting, and\nintent classification, as well as generative tasks such as speech enhancement,\nseparation, and text-to-speech. Our results show that, on average, semantic\ntokens outperform compression tokens across most discriminative and generative\ntasks. However, the performance gap between semantic tokens and standard\ncontinuous representations remains substantial, highlighting the need for\nfurther research in this field.\n", "link": "http://arxiv.org/abs/2406.14294v2", "date": "2024-06-21", "relevancy": 1.9555, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5179}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4703}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DASB%20--%20Discrete%20Audio%20and%20Speech%20Benchmark&body=Title%3A%20DASB%20--%20Discrete%20Audio%20and%20Speech%20Benchmark%0AAuthor%3A%20Pooneh%20Mousavi%20and%20Luca%20Della%20Libera%20and%20Jarod%20Duret%20and%20Artem%20Ploujnikov%20and%20Cem%20Subakan%20and%20Mirco%20Ravanelli%0AAbstract%3A%20%20%20Discrete%20audio%20tokens%20have%20recently%20gained%20considerable%20attention%20for%20their%0Apotential%20to%20connect%20audio%20and%20language%20processing%2C%20enabling%20the%20creation%20of%0Amodern%20multimodal%20large%20language%20models.%20Ideal%20audio%20tokens%20must%20effectively%0Apreserve%20phonetic%20and%20semantic%20content%20along%20with%20paralinguistic%20information%2C%0Aspeaker%20identity%2C%20and%20other%20details.%20While%20several%20types%20of%20audio%20tokens%20have%0Abeen%20recently%20proposed%2C%20identifying%20the%20optimal%20tokenizer%20for%20various%20tasks%20is%0Achallenging%20due%20to%20the%20inconsistent%20evaluation%20settings%20in%20existing%20studies.%20To%0Aaddress%20this%20gap%2C%20we%20release%20the%20Discrete%20Audio%20and%20Speech%20Benchmark%20%28DASB%29%2C%20a%0Acomprehensive%20leaderboard%20for%20benchmarking%20discrete%20audio%20tokens%20across%20a%20wide%0Arange%20of%20discriminative%20tasks%2C%20including%20speech%20recognition%2C%20speaker%0Aidentification%20and%20verification%2C%20emotion%20recognition%2C%20keyword%20spotting%2C%20and%0Aintent%20classification%2C%20as%20well%20as%20generative%20tasks%20such%20as%20speech%20enhancement%2C%0Aseparation%2C%20and%20text-to-speech.%20Our%20results%20show%20that%2C%20on%20average%2C%20semantic%0Atokens%20outperform%20compression%20tokens%20across%20most%20discriminative%20and%20generative%0Atasks.%20However%2C%20the%20performance%20gap%20between%20semantic%20tokens%20and%20standard%0Acontinuous%20representations%20remains%20substantial%2C%20highlighting%20the%20need%20for%0Afurther%20research%20in%20this%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14294v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDASB%2520--%2520Discrete%2520Audio%2520and%2520Speech%2520Benchmark%26entry.906535625%3DPooneh%2520Mousavi%2520and%2520Luca%2520Della%2520Libera%2520and%2520Jarod%2520Duret%2520and%2520Artem%2520Ploujnikov%2520and%2520Cem%2520Subakan%2520and%2520Mirco%2520Ravanelli%26entry.1292438233%3D%2520%2520Discrete%2520audio%2520tokens%2520have%2520recently%2520gained%2520considerable%2520attention%2520for%2520their%250Apotential%2520to%2520connect%2520audio%2520and%2520language%2520processing%252C%2520enabling%2520the%2520creation%2520of%250Amodern%2520multimodal%2520large%2520language%2520models.%2520Ideal%2520audio%2520tokens%2520must%2520effectively%250Apreserve%2520phonetic%2520and%2520semantic%2520content%2520along%2520with%2520paralinguistic%2520information%252C%250Aspeaker%2520identity%252C%2520and%2520other%2520details.%2520While%2520several%2520types%2520of%2520audio%2520tokens%2520have%250Abeen%2520recently%2520proposed%252C%2520identifying%2520the%2520optimal%2520tokenizer%2520for%2520various%2520tasks%2520is%250Achallenging%2520due%2520to%2520the%2520inconsistent%2520evaluation%2520settings%2520in%2520existing%2520studies.%2520To%250Aaddress%2520this%2520gap%252C%2520we%2520release%2520the%2520Discrete%2520Audio%2520and%2520Speech%2520Benchmark%2520%2528DASB%2529%252C%2520a%250Acomprehensive%2520leaderboard%2520for%2520benchmarking%2520discrete%2520audio%2520tokens%2520across%2520a%2520wide%250Arange%2520of%2520discriminative%2520tasks%252C%2520including%2520speech%2520recognition%252C%2520speaker%250Aidentification%2520and%2520verification%252C%2520emotion%2520recognition%252C%2520keyword%2520spotting%252C%2520and%250Aintent%2520classification%252C%2520as%2520well%2520as%2520generative%2520tasks%2520such%2520as%2520speech%2520enhancement%252C%250Aseparation%252C%2520and%2520text-to-speech.%2520Our%2520results%2520show%2520that%252C%2520on%2520average%252C%2520semantic%250Atokens%2520outperform%2520compression%2520tokens%2520across%2520most%2520discriminative%2520and%2520generative%250Atasks.%2520However%252C%2520the%2520performance%2520gap%2520between%2520semantic%2520tokens%2520and%2520standard%250Acontinuous%2520representations%2520remains%2520substantial%252C%2520highlighting%2520the%2520need%2520for%250Afurther%2520research%2520in%2520this%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14294v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DASB%20--%20Discrete%20Audio%20and%20Speech%20Benchmark&entry.906535625=Pooneh%20Mousavi%20and%20Luca%20Della%20Libera%20and%20Jarod%20Duret%20and%20Artem%20Ploujnikov%20and%20Cem%20Subakan%20and%20Mirco%20Ravanelli&entry.1292438233=%20%20Discrete%20audio%20tokens%20have%20recently%20gained%20considerable%20attention%20for%20their%0Apotential%20to%20connect%20audio%20and%20language%20processing%2C%20enabling%20the%20creation%20of%0Amodern%20multimodal%20large%20language%20models.%20Ideal%20audio%20tokens%20must%20effectively%0Apreserve%20phonetic%20and%20semantic%20content%20along%20with%20paralinguistic%20information%2C%0Aspeaker%20identity%2C%20and%20other%20details.%20While%20several%20types%20of%20audio%20tokens%20have%0Abeen%20recently%20proposed%2C%20identifying%20the%20optimal%20tokenizer%20for%20various%20tasks%20is%0Achallenging%20due%20to%20the%20inconsistent%20evaluation%20settings%20in%20existing%20studies.%20To%0Aaddress%20this%20gap%2C%20we%20release%20the%20Discrete%20Audio%20and%20Speech%20Benchmark%20%28DASB%29%2C%20a%0Acomprehensive%20leaderboard%20for%20benchmarking%20discrete%20audio%20tokens%20across%20a%20wide%0Arange%20of%20discriminative%20tasks%2C%20including%20speech%20recognition%2C%20speaker%0Aidentification%20and%20verification%2C%20emotion%20recognition%2C%20keyword%20spotting%2C%20and%0Aintent%20classification%2C%20as%20well%20as%20generative%20tasks%20such%20as%20speech%20enhancement%2C%0Aseparation%2C%20and%20text-to-speech.%20Our%20results%20show%20that%2C%20on%20average%2C%20semantic%0Atokens%20outperform%20compression%20tokens%20across%20most%20discriminative%20and%20generative%0Atasks.%20However%2C%20the%20performance%20gap%20between%20semantic%20tokens%20and%20standard%0Acontinuous%20representations%20remains%20substantial%2C%20highlighting%20the%20need%20for%0Afurther%20research%20in%20this%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14294v2&entry.124074799=Read"},
{"title": "Towards General Negotiation Strategies with End-to-End Reinforcement\n  Learning", "author": "Bram M. Renting and Thomas M. Moerland and Holger H. Hoos and Catholijn M. Jonker", "abstract": "  The research field of automated negotiation has a long history of designing\nagents that can negotiate with other agents. Such negotiation strategies are\ntraditionally based on manual design and heuristics. More recently,\nreinforcement learning approaches have also been used to train agents to\nnegotiate. However, negotiation problems are diverse, causing observation and\naction dimensions to change, which cannot be handled by default linear policy\nnetworks. Previous work on this topic has circumvented this issue either by\nfixing the negotiation problem, causing policies to be non-transferable between\nnegotiation problems or by abstracting the observations and actions into\nfixed-size representations, causing loss of information and expressiveness due\nto feature design. We developed an end-to-end reinforcement learning method for\ndiverse negotiation problems by representing observations and actions as a\ngraph and applying graph neural networks in the policy. With empirical\nevaluations, we show that our method is effective and that we can learn to\nnegotiate with other agents on never-before-seen negotiation problems. Our\nresult opens up new opportunities for reinforcement learning in negotiation\nagents.\n", "link": "http://arxiv.org/abs/2406.15096v1", "date": "2024-06-21", "relevancy": 1.9526, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5163}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4844}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4806}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20General%20Negotiation%20Strategies%20with%20End-to-End%20Reinforcement%0A%20%20Learning&body=Title%3A%20Towards%20General%20Negotiation%20Strategies%20with%20End-to-End%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Bram%20M.%20Renting%20and%20Thomas%20M.%20Moerland%20and%20Holger%20H.%20Hoos%20and%20Catholijn%20M.%20Jonker%0AAbstract%3A%20%20%20The%20research%20field%20of%20automated%20negotiation%20has%20a%20long%20history%20of%20designing%0Aagents%20that%20can%20negotiate%20with%20other%20agents.%20Such%20negotiation%20strategies%20are%0Atraditionally%20based%20on%20manual%20design%20and%20heuristics.%20More%20recently%2C%0Areinforcement%20learning%20approaches%20have%20also%20been%20used%20to%20train%20agents%20to%0Anegotiate.%20However%2C%20negotiation%20problems%20are%20diverse%2C%20causing%20observation%20and%0Aaction%20dimensions%20to%20change%2C%20which%20cannot%20be%20handled%20by%20default%20linear%20policy%0Anetworks.%20Previous%20work%20on%20this%20topic%20has%20circumvented%20this%20issue%20either%20by%0Afixing%20the%20negotiation%20problem%2C%20causing%20policies%20to%20be%20non-transferable%20between%0Anegotiation%20problems%20or%20by%20abstracting%20the%20observations%20and%20actions%20into%0Afixed-size%20representations%2C%20causing%20loss%20of%20information%20and%20expressiveness%20due%0Ato%20feature%20design.%20We%20developed%20an%20end-to-end%20reinforcement%20learning%20method%20for%0Adiverse%20negotiation%20problems%20by%20representing%20observations%20and%20actions%20as%20a%0Agraph%20and%20applying%20graph%20neural%20networks%20in%20the%20policy.%20With%20empirical%0Aevaluations%2C%20we%20show%20that%20our%20method%20is%20effective%20and%20that%20we%20can%20learn%20to%0Anegotiate%20with%20other%20agents%20on%20never-before-seen%20negotiation%20problems.%20Our%0Aresult%20opens%20up%20new%20opportunities%20for%20reinforcement%20learning%20in%20negotiation%0Aagents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15096v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520General%2520Negotiation%2520Strategies%2520with%2520End-to-End%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DBram%2520M.%2520Renting%2520and%2520Thomas%2520M.%2520Moerland%2520and%2520Holger%2520H.%2520Hoos%2520and%2520Catholijn%2520M.%2520Jonker%26entry.1292438233%3D%2520%2520The%2520research%2520field%2520of%2520automated%2520negotiation%2520has%2520a%2520long%2520history%2520of%2520designing%250Aagents%2520that%2520can%2520negotiate%2520with%2520other%2520agents.%2520Such%2520negotiation%2520strategies%2520are%250Atraditionally%2520based%2520on%2520manual%2520design%2520and%2520heuristics.%2520More%2520recently%252C%250Areinforcement%2520learning%2520approaches%2520have%2520also%2520been%2520used%2520to%2520train%2520agents%2520to%250Anegotiate.%2520However%252C%2520negotiation%2520problems%2520are%2520diverse%252C%2520causing%2520observation%2520and%250Aaction%2520dimensions%2520to%2520change%252C%2520which%2520cannot%2520be%2520handled%2520by%2520default%2520linear%2520policy%250Anetworks.%2520Previous%2520work%2520on%2520this%2520topic%2520has%2520circumvented%2520this%2520issue%2520either%2520by%250Afixing%2520the%2520negotiation%2520problem%252C%2520causing%2520policies%2520to%2520be%2520non-transferable%2520between%250Anegotiation%2520problems%2520or%2520by%2520abstracting%2520the%2520observations%2520and%2520actions%2520into%250Afixed-size%2520representations%252C%2520causing%2520loss%2520of%2520information%2520and%2520expressiveness%2520due%250Ato%2520feature%2520design.%2520We%2520developed%2520an%2520end-to-end%2520reinforcement%2520learning%2520method%2520for%250Adiverse%2520negotiation%2520problems%2520by%2520representing%2520observations%2520and%2520actions%2520as%2520a%250Agraph%2520and%2520applying%2520graph%2520neural%2520networks%2520in%2520the%2520policy.%2520With%2520empirical%250Aevaluations%252C%2520we%2520show%2520that%2520our%2520method%2520is%2520effective%2520and%2520that%2520we%2520can%2520learn%2520to%250Anegotiate%2520with%2520other%2520agents%2520on%2520never-before-seen%2520negotiation%2520problems.%2520Our%250Aresult%2520opens%2520up%2520new%2520opportunities%2520for%2520reinforcement%2520learning%2520in%2520negotiation%250Aagents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15096v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20General%20Negotiation%20Strategies%20with%20End-to-End%20Reinforcement%0A%20%20Learning&entry.906535625=Bram%20M.%20Renting%20and%20Thomas%20M.%20Moerland%20and%20Holger%20H.%20Hoos%20and%20Catholijn%20M.%20Jonker&entry.1292438233=%20%20The%20research%20field%20of%20automated%20negotiation%20has%20a%20long%20history%20of%20designing%0Aagents%20that%20can%20negotiate%20with%20other%20agents.%20Such%20negotiation%20strategies%20are%0Atraditionally%20based%20on%20manual%20design%20and%20heuristics.%20More%20recently%2C%0Areinforcement%20learning%20approaches%20have%20also%20been%20used%20to%20train%20agents%20to%0Anegotiate.%20However%2C%20negotiation%20problems%20are%20diverse%2C%20causing%20observation%20and%0Aaction%20dimensions%20to%20change%2C%20which%20cannot%20be%20handled%20by%20default%20linear%20policy%0Anetworks.%20Previous%20work%20on%20this%20topic%20has%20circumvented%20this%20issue%20either%20by%0Afixing%20the%20negotiation%20problem%2C%20causing%20policies%20to%20be%20non-transferable%20between%0Anegotiation%20problems%20or%20by%20abstracting%20the%20observations%20and%20actions%20into%0Afixed-size%20representations%2C%20causing%20loss%20of%20information%20and%20expressiveness%20due%0Ato%20feature%20design.%20We%20developed%20an%20end-to-end%20reinforcement%20learning%20method%20for%0Adiverse%20negotiation%20problems%20by%20representing%20observations%20and%20actions%20as%20a%0Agraph%20and%20applying%20graph%20neural%20networks%20in%20the%20policy.%20With%20empirical%0Aevaluations%2C%20we%20show%20that%20our%20method%20is%20effective%20and%20that%20we%20can%20learn%20to%0Anegotiate%20with%20other%20agents%20on%20never-before-seen%20negotiation%20problems.%20Our%0Aresult%20opens%20up%20new%20opportunities%20for%20reinforcement%20learning%20in%20negotiation%0Aagents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15096v1&entry.124074799=Read"},
{"title": "Deep UAV Path Planning with Assured Connectivity in Dense Urban Setting", "author": "Jiyong Oh and Syed M. Raza and Lusungu J. Mwasinga and Moonseong Kim and Hyunseung Choo", "abstract": "  Unmanned Ariel Vehicle (UAV) services with 5G connectivity is an emerging\nfield with numerous applications. Operator-controlled UAV flights and manual\nstatic flight configurations are major limitations for the wide adoption of\nscalability of UAV services. Several services depend on excellent UAV\nconnectivity with a cellular network and maintaining it is challenging in\npredetermined flight paths. This paper addresses these limitations by proposing\na Deep Reinforcement Learning (DRL) framework for UAV path planning with\nassured connectivity (DUPAC). During UAV flight, DUPAC determines the best\nroute from a defined source to the destination in terms of distance and signal\nquality. The viability and performance of DUPAC are evaluated under simulated\nreal-world urban scenarios using the Unity framework. The results confirm that\nDUPAC achieves an autonomous UAV flight path similar to base method with only\n2% increment while maintaining an average 9% better connection quality\nthroughout the flight.\n", "link": "http://arxiv.org/abs/2406.15225v1", "date": "2024-06-21", "relevancy": 1.9524, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5566}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4397}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4379}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20UAV%20Path%20Planning%20with%20Assured%20Connectivity%20in%20Dense%20Urban%20Setting&body=Title%3A%20Deep%20UAV%20Path%20Planning%20with%20Assured%20Connectivity%20in%20Dense%20Urban%20Setting%0AAuthor%3A%20Jiyong%20Oh%20and%20Syed%20M.%20Raza%20and%20Lusungu%20J.%20Mwasinga%20and%20Moonseong%20Kim%20and%20Hyunseung%20Choo%0AAbstract%3A%20%20%20Unmanned%20Ariel%20Vehicle%20%28UAV%29%20services%20with%205G%20connectivity%20is%20an%20emerging%0Afield%20with%20numerous%20applications.%20Operator-controlled%20UAV%20flights%20and%20manual%0Astatic%20flight%20configurations%20are%20major%20limitations%20for%20the%20wide%20adoption%20of%0Ascalability%20of%20UAV%20services.%20Several%20services%20depend%20on%20excellent%20UAV%0Aconnectivity%20with%20a%20cellular%20network%20and%20maintaining%20it%20is%20challenging%20in%0Apredetermined%20flight%20paths.%20This%20paper%20addresses%20these%20limitations%20by%20proposing%0Aa%20Deep%20Reinforcement%20Learning%20%28DRL%29%20framework%20for%20UAV%20path%20planning%20with%0Aassured%20connectivity%20%28DUPAC%29.%20During%20UAV%20flight%2C%20DUPAC%20determines%20the%20best%0Aroute%20from%20a%20defined%20source%20to%20the%20destination%20in%20terms%20of%20distance%20and%20signal%0Aquality.%20The%20viability%20and%20performance%20of%20DUPAC%20are%20evaluated%20under%20simulated%0Areal-world%20urban%20scenarios%20using%20the%20Unity%20framework.%20The%20results%20confirm%20that%0ADUPAC%20achieves%20an%20autonomous%20UAV%20flight%20path%20similar%20to%20base%20method%20with%20only%0A2%25%20increment%20while%20maintaining%20an%20average%209%25%20better%20connection%20quality%0Athroughout%20the%20flight.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15225v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520UAV%2520Path%2520Planning%2520with%2520Assured%2520Connectivity%2520in%2520Dense%2520Urban%2520Setting%26entry.906535625%3DJiyong%2520Oh%2520and%2520Syed%2520M.%2520Raza%2520and%2520Lusungu%2520J.%2520Mwasinga%2520and%2520Moonseong%2520Kim%2520and%2520Hyunseung%2520Choo%26entry.1292438233%3D%2520%2520Unmanned%2520Ariel%2520Vehicle%2520%2528UAV%2529%2520services%2520with%25205G%2520connectivity%2520is%2520an%2520emerging%250Afield%2520with%2520numerous%2520applications.%2520Operator-controlled%2520UAV%2520flights%2520and%2520manual%250Astatic%2520flight%2520configurations%2520are%2520major%2520limitations%2520for%2520the%2520wide%2520adoption%2520of%250Ascalability%2520of%2520UAV%2520services.%2520Several%2520services%2520depend%2520on%2520excellent%2520UAV%250Aconnectivity%2520with%2520a%2520cellular%2520network%2520and%2520maintaining%2520it%2520is%2520challenging%2520in%250Apredetermined%2520flight%2520paths.%2520This%2520paper%2520addresses%2520these%2520limitations%2520by%2520proposing%250Aa%2520Deep%2520Reinforcement%2520Learning%2520%2528DRL%2529%2520framework%2520for%2520UAV%2520path%2520planning%2520with%250Aassured%2520connectivity%2520%2528DUPAC%2529.%2520During%2520UAV%2520flight%252C%2520DUPAC%2520determines%2520the%2520best%250Aroute%2520from%2520a%2520defined%2520source%2520to%2520the%2520destination%2520in%2520terms%2520of%2520distance%2520and%2520signal%250Aquality.%2520The%2520viability%2520and%2520performance%2520of%2520DUPAC%2520are%2520evaluated%2520under%2520simulated%250Areal-world%2520urban%2520scenarios%2520using%2520the%2520Unity%2520framework.%2520The%2520results%2520confirm%2520that%250ADUPAC%2520achieves%2520an%2520autonomous%2520UAV%2520flight%2520path%2520similar%2520to%2520base%2520method%2520with%2520only%250A2%2525%2520increment%2520while%2520maintaining%2520an%2520average%25209%2525%2520better%2520connection%2520quality%250Athroughout%2520the%2520flight.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15225v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20UAV%20Path%20Planning%20with%20Assured%20Connectivity%20in%20Dense%20Urban%20Setting&entry.906535625=Jiyong%20Oh%20and%20Syed%20M.%20Raza%20and%20Lusungu%20J.%20Mwasinga%20and%20Moonseong%20Kim%20and%20Hyunseung%20Choo&entry.1292438233=%20%20Unmanned%20Ariel%20Vehicle%20%28UAV%29%20services%20with%205G%20connectivity%20is%20an%20emerging%0Afield%20with%20numerous%20applications.%20Operator-controlled%20UAV%20flights%20and%20manual%0Astatic%20flight%20configurations%20are%20major%20limitations%20for%20the%20wide%20adoption%20of%0Ascalability%20of%20UAV%20services.%20Several%20services%20depend%20on%20excellent%20UAV%0Aconnectivity%20with%20a%20cellular%20network%20and%20maintaining%20it%20is%20challenging%20in%0Apredetermined%20flight%20paths.%20This%20paper%20addresses%20these%20limitations%20by%20proposing%0Aa%20Deep%20Reinforcement%20Learning%20%28DRL%29%20framework%20for%20UAV%20path%20planning%20with%0Aassured%20connectivity%20%28DUPAC%29.%20During%20UAV%20flight%2C%20DUPAC%20determines%20the%20best%0Aroute%20from%20a%20defined%20source%20to%20the%20destination%20in%20terms%20of%20distance%20and%20signal%0Aquality.%20The%20viability%20and%20performance%20of%20DUPAC%20are%20evaluated%20under%20simulated%0Areal-world%20urban%20scenarios%20using%20the%20Unity%20framework.%20The%20results%20confirm%20that%0ADUPAC%20achieves%20an%20autonomous%20UAV%20flight%20path%20similar%20to%20base%20method%20with%20only%0A2%25%20increment%20while%20maintaining%20an%20average%209%25%20better%20connection%20quality%0Athroughout%20the%20flight.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15225v1&entry.124074799=Read"},
{"title": "Full-Scale Indexing and Semantic Annotation of CT Imaging: Boosting\n  FAIRness", "author": "Hannes Ulrich and Robin Hendel and Santiago Pazmino and Bj\u00f6rn Bergh and Bj\u00f6rn Schreiweis", "abstract": "  Background: The integration of artificial intelligence into medicine has led\nto significant advances, particularly in diagnostics and treatment planning.\nHowever, the reliability of AI models is highly dependent on the quality of the\ntraining data, especially in medical imaging, where varying patient data and\nevolving medical knowledge pose a challenge to the accuracy and\ngeneralizability of given datasets. Results: The proposed approach focuses on\nthe integration and enhancement of clinical computed tomography (CT) image\nseries for better findability, accessibility, interoperability, and\nreusability. Through an automated indexing process, CT image series are\nsemantically enhanced using the TotalSegmentator framework for segmentation and\nresulting SNOMED CT annotations. The metadata is standardized with HL7 FHIR\nresources to enable efficient data recognition and data exchange between\nresearch projects. Conclusions: The study successfully integrates a robust\nprocess within the UKSH MeDIC, leading to the semantic enrichment of over\n230,000 CT image series and over 8 million SNOMED CT annotations. The\nstandardized representation using HL7 FHIR resources improves discoverability\nand facilitates interoperability, providing a foundation for the FAIRness of\nmedical imaging data. However, developing automated annotation methods that can\nkeep pace with growing clinical datasets remains a challenge to ensure\ncontinued progress in large-scale integration and indexing of medical imaging\nfor advanced healthcare AI applications.\n", "link": "http://arxiv.org/abs/2406.15340v1", "date": "2024-06-21", "relevancy": 1.9444, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5036}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.487}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4782}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Full-Scale%20Indexing%20and%20Semantic%20Annotation%20of%20CT%20Imaging%3A%20Boosting%0A%20%20FAIRness&body=Title%3A%20Full-Scale%20Indexing%20and%20Semantic%20Annotation%20of%20CT%20Imaging%3A%20Boosting%0A%20%20FAIRness%0AAuthor%3A%20Hannes%20Ulrich%20and%20Robin%20Hendel%20and%20Santiago%20Pazmino%20and%20Bj%C3%B6rn%20Bergh%20and%20Bj%C3%B6rn%20Schreiweis%0AAbstract%3A%20%20%20Background%3A%20The%20integration%20of%20artificial%20intelligence%20into%20medicine%20has%20led%0Ato%20significant%20advances%2C%20particularly%20in%20diagnostics%20and%20treatment%20planning.%0AHowever%2C%20the%20reliability%20of%20AI%20models%20is%20highly%20dependent%20on%20the%20quality%20of%20the%0Atraining%20data%2C%20especially%20in%20medical%20imaging%2C%20where%20varying%20patient%20data%20and%0Aevolving%20medical%20knowledge%20pose%20a%20challenge%20to%20the%20accuracy%20and%0Ageneralizability%20of%20given%20datasets.%20Results%3A%20The%20proposed%20approach%20focuses%20on%0Athe%20integration%20and%20enhancement%20of%20clinical%20computed%20tomography%20%28CT%29%20image%0Aseries%20for%20better%20findability%2C%20accessibility%2C%20interoperability%2C%20and%0Areusability.%20Through%20an%20automated%20indexing%20process%2C%20CT%20image%20series%20are%0Asemantically%20enhanced%20using%20the%20TotalSegmentator%20framework%20for%20segmentation%20and%0Aresulting%20SNOMED%20CT%20annotations.%20The%20metadata%20is%20standardized%20with%20HL7%20FHIR%0Aresources%20to%20enable%20efficient%20data%20recognition%20and%20data%20exchange%20between%0Aresearch%20projects.%20Conclusions%3A%20The%20study%20successfully%20integrates%20a%20robust%0Aprocess%20within%20the%20UKSH%20MeDIC%2C%20leading%20to%20the%20semantic%20enrichment%20of%20over%0A230%2C000%20CT%20image%20series%20and%20over%208%20million%20SNOMED%20CT%20annotations.%20The%0Astandardized%20representation%20using%20HL7%20FHIR%20resources%20improves%20discoverability%0Aand%20facilitates%20interoperability%2C%20providing%20a%20foundation%20for%20the%20FAIRness%20of%0Amedical%20imaging%20data.%20However%2C%20developing%20automated%20annotation%20methods%20that%20can%0Akeep%20pace%20with%20growing%20clinical%20datasets%20remains%20a%20challenge%20to%20ensure%0Acontinued%20progress%20in%20large-scale%20integration%20and%20indexing%20of%20medical%20imaging%0Afor%20advanced%20healthcare%20AI%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15340v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFull-Scale%2520Indexing%2520and%2520Semantic%2520Annotation%2520of%2520CT%2520Imaging%253A%2520Boosting%250A%2520%2520FAIRness%26entry.906535625%3DHannes%2520Ulrich%2520and%2520Robin%2520Hendel%2520and%2520Santiago%2520Pazmino%2520and%2520Bj%25C3%25B6rn%2520Bergh%2520and%2520Bj%25C3%25B6rn%2520Schreiweis%26entry.1292438233%3D%2520%2520Background%253A%2520The%2520integration%2520of%2520artificial%2520intelligence%2520into%2520medicine%2520has%2520led%250Ato%2520significant%2520advances%252C%2520particularly%2520in%2520diagnostics%2520and%2520treatment%2520planning.%250AHowever%252C%2520the%2520reliability%2520of%2520AI%2520models%2520is%2520highly%2520dependent%2520on%2520the%2520quality%2520of%2520the%250Atraining%2520data%252C%2520especially%2520in%2520medical%2520imaging%252C%2520where%2520varying%2520patient%2520data%2520and%250Aevolving%2520medical%2520knowledge%2520pose%2520a%2520challenge%2520to%2520the%2520accuracy%2520and%250Ageneralizability%2520of%2520given%2520datasets.%2520Results%253A%2520The%2520proposed%2520approach%2520focuses%2520on%250Athe%2520integration%2520and%2520enhancement%2520of%2520clinical%2520computed%2520tomography%2520%2528CT%2529%2520image%250Aseries%2520for%2520better%2520findability%252C%2520accessibility%252C%2520interoperability%252C%2520and%250Areusability.%2520Through%2520an%2520automated%2520indexing%2520process%252C%2520CT%2520image%2520series%2520are%250Asemantically%2520enhanced%2520using%2520the%2520TotalSegmentator%2520framework%2520for%2520segmentation%2520and%250Aresulting%2520SNOMED%2520CT%2520annotations.%2520The%2520metadata%2520is%2520standardized%2520with%2520HL7%2520FHIR%250Aresources%2520to%2520enable%2520efficient%2520data%2520recognition%2520and%2520data%2520exchange%2520between%250Aresearch%2520projects.%2520Conclusions%253A%2520The%2520study%2520successfully%2520integrates%2520a%2520robust%250Aprocess%2520within%2520the%2520UKSH%2520MeDIC%252C%2520leading%2520to%2520the%2520semantic%2520enrichment%2520of%2520over%250A230%252C000%2520CT%2520image%2520series%2520and%2520over%25208%2520million%2520SNOMED%2520CT%2520annotations.%2520The%250Astandardized%2520representation%2520using%2520HL7%2520FHIR%2520resources%2520improves%2520discoverability%250Aand%2520facilitates%2520interoperability%252C%2520providing%2520a%2520foundation%2520for%2520the%2520FAIRness%2520of%250Amedical%2520imaging%2520data.%2520However%252C%2520developing%2520automated%2520annotation%2520methods%2520that%2520can%250Akeep%2520pace%2520with%2520growing%2520clinical%2520datasets%2520remains%2520a%2520challenge%2520to%2520ensure%250Acontinued%2520progress%2520in%2520large-scale%2520integration%2520and%2520indexing%2520of%2520medical%2520imaging%250Afor%2520advanced%2520healthcare%2520AI%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15340v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Full-Scale%20Indexing%20and%20Semantic%20Annotation%20of%20CT%20Imaging%3A%20Boosting%0A%20%20FAIRness&entry.906535625=Hannes%20Ulrich%20and%20Robin%20Hendel%20and%20Santiago%20Pazmino%20and%20Bj%C3%B6rn%20Bergh%20and%20Bj%C3%B6rn%20Schreiweis&entry.1292438233=%20%20Background%3A%20The%20integration%20of%20artificial%20intelligence%20into%20medicine%20has%20led%0Ato%20significant%20advances%2C%20particularly%20in%20diagnostics%20and%20treatment%20planning.%0AHowever%2C%20the%20reliability%20of%20AI%20models%20is%20highly%20dependent%20on%20the%20quality%20of%20the%0Atraining%20data%2C%20especially%20in%20medical%20imaging%2C%20where%20varying%20patient%20data%20and%0Aevolving%20medical%20knowledge%20pose%20a%20challenge%20to%20the%20accuracy%20and%0Ageneralizability%20of%20given%20datasets.%20Results%3A%20The%20proposed%20approach%20focuses%20on%0Athe%20integration%20and%20enhancement%20of%20clinical%20computed%20tomography%20%28CT%29%20image%0Aseries%20for%20better%20findability%2C%20accessibility%2C%20interoperability%2C%20and%0Areusability.%20Through%20an%20automated%20indexing%20process%2C%20CT%20image%20series%20are%0Asemantically%20enhanced%20using%20the%20TotalSegmentator%20framework%20for%20segmentation%20and%0Aresulting%20SNOMED%20CT%20annotations.%20The%20metadata%20is%20standardized%20with%20HL7%20FHIR%0Aresources%20to%20enable%20efficient%20data%20recognition%20and%20data%20exchange%20between%0Aresearch%20projects.%20Conclusions%3A%20The%20study%20successfully%20integrates%20a%20robust%0Aprocess%20within%20the%20UKSH%20MeDIC%2C%20leading%20to%20the%20semantic%20enrichment%20of%20over%0A230%2C000%20CT%20image%20series%20and%20over%208%20million%20SNOMED%20CT%20annotations.%20The%0Astandardized%20representation%20using%20HL7%20FHIR%20resources%20improves%20discoverability%0Aand%20facilitates%20interoperability%2C%20providing%20a%20foundation%20for%20the%20FAIRness%20of%0Amedical%20imaging%20data.%20However%2C%20developing%20automated%20annotation%20methods%20that%20can%0Akeep%20pace%20with%20growing%20clinical%20datasets%20remains%20a%20challenge%20to%20ensure%0Acontinued%20progress%20in%20large-scale%20integration%20and%20indexing%20of%20medical%20imaging%0Afor%20advanced%20healthcare%20AI%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15340v1&entry.124074799=Read"},
{"title": "DUAL-REFLECT: Enhancing Large Language Models for Reflective Translation\n  through Dual Learning Feedback Mechanisms", "author": "Andong Chen and Lianzhang Lou and Kehai Chen and Xuefeng Bai and Yang Xiang and Muyun Yang and Tiejun Zhao and Min Zhang", "abstract": "  Recently, large language models (LLMs) enhanced by self-reflection have\nachieved promising performance on machine translation. The key idea is guiding\nLLMs to generate translation with human-like feedback. However, existing\nself-reflection methods lack effective feedback information, limiting the\ntranslation performance. To address this, we introduce a DUAL-REFLECT\nframework, leveraging the dual learning of translation tasks to provide\neffective feedback, thereby enhancing the models' self-reflective abilities and\nimproving translation performance. The application of this method across\nvarious translation tasks has proven its effectiveness in improving translation\naccuracy and eliminating ambiguities, especially in translation tasks with\nlow-resource language pairs.\n", "link": "http://arxiv.org/abs/2406.07232v2", "date": "2024-06-21", "relevancy": 1.9333, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4965}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4776}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DUAL-REFLECT%3A%20Enhancing%20Large%20Language%20Models%20for%20Reflective%20Translation%0A%20%20through%20Dual%20Learning%20Feedback%20Mechanisms&body=Title%3A%20DUAL-REFLECT%3A%20Enhancing%20Large%20Language%20Models%20for%20Reflective%20Translation%0A%20%20through%20Dual%20Learning%20Feedback%20Mechanisms%0AAuthor%3A%20Andong%20Chen%20and%20Lianzhang%20Lou%20and%20Kehai%20Chen%20and%20Xuefeng%20Bai%20and%20Yang%20Xiang%20and%20Muyun%20Yang%20and%20Tiejun%20Zhao%20and%20Min%20Zhang%0AAbstract%3A%20%20%20Recently%2C%20large%20language%20models%20%28LLMs%29%20enhanced%20by%20self-reflection%20have%0Aachieved%20promising%20performance%20on%20machine%20translation.%20The%20key%20idea%20is%20guiding%0ALLMs%20to%20generate%20translation%20with%20human-like%20feedback.%20However%2C%20existing%0Aself-reflection%20methods%20lack%20effective%20feedback%20information%2C%20limiting%20the%0Atranslation%20performance.%20To%20address%20this%2C%20we%20introduce%20a%20DUAL-REFLECT%0Aframework%2C%20leveraging%20the%20dual%20learning%20of%20translation%20tasks%20to%20provide%0Aeffective%20feedback%2C%20thereby%20enhancing%20the%20models%27%20self-reflective%20abilities%20and%0Aimproving%20translation%20performance.%20The%20application%20of%20this%20method%20across%0Avarious%20translation%20tasks%20has%20proven%20its%20effectiveness%20in%20improving%20translation%0Aaccuracy%20and%20eliminating%20ambiguities%2C%20especially%20in%20translation%20tasks%20with%0Alow-resource%20language%20pairs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07232v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDUAL-REFLECT%253A%2520Enhancing%2520Large%2520Language%2520Models%2520for%2520Reflective%2520Translation%250A%2520%2520through%2520Dual%2520Learning%2520Feedback%2520Mechanisms%26entry.906535625%3DAndong%2520Chen%2520and%2520Lianzhang%2520Lou%2520and%2520Kehai%2520Chen%2520and%2520Xuefeng%2520Bai%2520and%2520Yang%2520Xiang%2520and%2520Muyun%2520Yang%2520and%2520Tiejun%2520Zhao%2520and%2520Min%2520Zhang%26entry.1292438233%3D%2520%2520Recently%252C%2520large%2520language%2520models%2520%2528LLMs%2529%2520enhanced%2520by%2520self-reflection%2520have%250Aachieved%2520promising%2520performance%2520on%2520machine%2520translation.%2520The%2520key%2520idea%2520is%2520guiding%250ALLMs%2520to%2520generate%2520translation%2520with%2520human-like%2520feedback.%2520However%252C%2520existing%250Aself-reflection%2520methods%2520lack%2520effective%2520feedback%2520information%252C%2520limiting%2520the%250Atranslation%2520performance.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520DUAL-REFLECT%250Aframework%252C%2520leveraging%2520the%2520dual%2520learning%2520of%2520translation%2520tasks%2520to%2520provide%250Aeffective%2520feedback%252C%2520thereby%2520enhancing%2520the%2520models%2527%2520self-reflective%2520abilities%2520and%250Aimproving%2520translation%2520performance.%2520The%2520application%2520of%2520this%2520method%2520across%250Avarious%2520translation%2520tasks%2520has%2520proven%2520its%2520effectiveness%2520in%2520improving%2520translation%250Aaccuracy%2520and%2520eliminating%2520ambiguities%252C%2520especially%2520in%2520translation%2520tasks%2520with%250Alow-resource%2520language%2520pairs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07232v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DUAL-REFLECT%3A%20Enhancing%20Large%20Language%20Models%20for%20Reflective%20Translation%0A%20%20through%20Dual%20Learning%20Feedback%20Mechanisms&entry.906535625=Andong%20Chen%20and%20Lianzhang%20Lou%20and%20Kehai%20Chen%20and%20Xuefeng%20Bai%20and%20Yang%20Xiang%20and%20Muyun%20Yang%20and%20Tiejun%20Zhao%20and%20Min%20Zhang&entry.1292438233=%20%20Recently%2C%20large%20language%20models%20%28LLMs%29%20enhanced%20by%20self-reflection%20have%0Aachieved%20promising%20performance%20on%20machine%20translation.%20The%20key%20idea%20is%20guiding%0ALLMs%20to%20generate%20translation%20with%20human-like%20feedback.%20However%2C%20existing%0Aself-reflection%20methods%20lack%20effective%20feedback%20information%2C%20limiting%20the%0Atranslation%20performance.%20To%20address%20this%2C%20we%20introduce%20a%20DUAL-REFLECT%0Aframework%2C%20leveraging%20the%20dual%20learning%20of%20translation%20tasks%20to%20provide%0Aeffective%20feedback%2C%20thereby%20enhancing%20the%20models%27%20self-reflective%20abilities%20and%0Aimproving%20translation%20performance.%20The%20application%20of%20this%20method%20across%0Avarious%20translation%20tasks%20has%20proven%20its%20effectiveness%20in%20improving%20translation%0Aaccuracy%20and%20eliminating%20ambiguities%2C%20especially%20in%20translation%20tasks%20with%0Alow-resource%20language%20pairs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07232v2&entry.124074799=Read"},
{"title": "Assessing Good, Bad and Ugly Arguments Generated by ChatGPT: a New\n  Dataset, its Methodology and Associated Tasks", "author": "Victor Hugo Nascimento Rocha and Igor Cataneo Silveira and Paulo Pirozelli and Denis Deratani Mau\u00e1 and Fabio Gagliardi Cozman", "abstract": "  The recent success of Large Language Models (LLMs) has sparked concerns about\ntheir potential to spread misinformation. As a result, there is a pressing need\nfor tools to identify ``fake arguments'' generated by such models. To create\nthese tools, examples of texts generated by LLMs are needed. This paper\nintroduces a methodology to obtain good, bad and ugly arguments from\nargumentative essays produced by ChatGPT, OpenAI's LLM. We then describe a\nnovel dataset containing a set of diverse arguments, ArGPT. We assess the\neffectiveness of our dataset and establish baselines for several\nargumentation-related tasks. Finally, we show that the artificially generated\ndata relates well to human argumentation and thus is useful as a tool to train\nand test systems for the defined tasks.\n", "link": "http://arxiv.org/abs/2406.15130v1", "date": "2024-06-21", "relevancy": 1.9171, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.485}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4768}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Assessing%20Good%2C%20Bad%20and%20Ugly%20Arguments%20Generated%20by%20ChatGPT%3A%20a%20New%0A%20%20Dataset%2C%20its%20Methodology%20and%20Associated%20Tasks&body=Title%3A%20Assessing%20Good%2C%20Bad%20and%20Ugly%20Arguments%20Generated%20by%20ChatGPT%3A%20a%20New%0A%20%20Dataset%2C%20its%20Methodology%20and%20Associated%20Tasks%0AAuthor%3A%20Victor%20Hugo%20Nascimento%20Rocha%20and%20Igor%20Cataneo%20Silveira%20and%20Paulo%20Pirozelli%20and%20Denis%20Deratani%20Mau%C3%A1%20and%20Fabio%20Gagliardi%20Cozman%0AAbstract%3A%20%20%20The%20recent%20success%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20sparked%20concerns%20about%0Atheir%20potential%20to%20spread%20misinformation.%20As%20a%20result%2C%20there%20is%20a%20pressing%20need%0Afor%20tools%20to%20identify%20%60%60fake%20arguments%27%27%20generated%20by%20such%20models.%20To%20create%0Athese%20tools%2C%20examples%20of%20texts%20generated%20by%20LLMs%20are%20needed.%20This%20paper%0Aintroduces%20a%20methodology%20to%20obtain%20good%2C%20bad%20and%20ugly%20arguments%20from%0Aargumentative%20essays%20produced%20by%20ChatGPT%2C%20OpenAI%27s%20LLM.%20We%20then%20describe%20a%0Anovel%20dataset%20containing%20a%20set%20of%20diverse%20arguments%2C%20ArGPT.%20We%20assess%20the%0Aeffectiveness%20of%20our%20dataset%20and%20establish%20baselines%20for%20several%0Aargumentation-related%20tasks.%20Finally%2C%20we%20show%20that%20the%20artificially%20generated%0Adata%20relates%20well%20to%20human%20argumentation%20and%20thus%20is%20useful%20as%20a%20tool%20to%20train%0Aand%20test%20systems%20for%20the%20defined%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15130v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAssessing%2520Good%252C%2520Bad%2520and%2520Ugly%2520Arguments%2520Generated%2520by%2520ChatGPT%253A%2520a%2520New%250A%2520%2520Dataset%252C%2520its%2520Methodology%2520and%2520Associated%2520Tasks%26entry.906535625%3DVictor%2520Hugo%2520Nascimento%2520Rocha%2520and%2520Igor%2520Cataneo%2520Silveira%2520and%2520Paulo%2520Pirozelli%2520and%2520Denis%2520Deratani%2520Mau%25C3%25A1%2520and%2520Fabio%2520Gagliardi%2520Cozman%26entry.1292438233%3D%2520%2520The%2520recent%2520success%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520sparked%2520concerns%2520about%250Atheir%2520potential%2520to%2520spread%2520misinformation.%2520As%2520a%2520result%252C%2520there%2520is%2520a%2520pressing%2520need%250Afor%2520tools%2520to%2520identify%2520%2560%2560fake%2520arguments%2527%2527%2520generated%2520by%2520such%2520models.%2520To%2520create%250Athese%2520tools%252C%2520examples%2520of%2520texts%2520generated%2520by%2520LLMs%2520are%2520needed.%2520This%2520paper%250Aintroduces%2520a%2520methodology%2520to%2520obtain%2520good%252C%2520bad%2520and%2520ugly%2520arguments%2520from%250Aargumentative%2520essays%2520produced%2520by%2520ChatGPT%252C%2520OpenAI%2527s%2520LLM.%2520We%2520then%2520describe%2520a%250Anovel%2520dataset%2520containing%2520a%2520set%2520of%2520diverse%2520arguments%252C%2520ArGPT.%2520We%2520assess%2520the%250Aeffectiveness%2520of%2520our%2520dataset%2520and%2520establish%2520baselines%2520for%2520several%250Aargumentation-related%2520tasks.%2520Finally%252C%2520we%2520show%2520that%2520the%2520artificially%2520generated%250Adata%2520relates%2520well%2520to%2520human%2520argumentation%2520and%2520thus%2520is%2520useful%2520as%2520a%2520tool%2520to%2520train%250Aand%2520test%2520systems%2520for%2520the%2520defined%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15130v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assessing%20Good%2C%20Bad%20and%20Ugly%20Arguments%20Generated%20by%20ChatGPT%3A%20a%20New%0A%20%20Dataset%2C%20its%20Methodology%20and%20Associated%20Tasks&entry.906535625=Victor%20Hugo%20Nascimento%20Rocha%20and%20Igor%20Cataneo%20Silveira%20and%20Paulo%20Pirozelli%20and%20Denis%20Deratani%20Mau%C3%A1%20and%20Fabio%20Gagliardi%20Cozman&entry.1292438233=%20%20The%20recent%20success%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20sparked%20concerns%20about%0Atheir%20potential%20to%20spread%20misinformation.%20As%20a%20result%2C%20there%20is%20a%20pressing%20need%0Afor%20tools%20to%20identify%20%60%60fake%20arguments%27%27%20generated%20by%20such%20models.%20To%20create%0Athese%20tools%2C%20examples%20of%20texts%20generated%20by%20LLMs%20are%20needed.%20This%20paper%0Aintroduces%20a%20methodology%20to%20obtain%20good%2C%20bad%20and%20ugly%20arguments%20from%0Aargumentative%20essays%20produced%20by%20ChatGPT%2C%20OpenAI%27s%20LLM.%20We%20then%20describe%20a%0Anovel%20dataset%20containing%20a%20set%20of%20diverse%20arguments%2C%20ArGPT.%20We%20assess%20the%0Aeffectiveness%20of%20our%20dataset%20and%20establish%20baselines%20for%20several%0Aargumentation-related%20tasks.%20Finally%2C%20we%20show%20that%20the%20artificially%20generated%0Adata%20relates%20well%20to%20human%20argumentation%20and%20thus%20is%20useful%20as%20a%20tool%20to%20train%0Aand%20test%20systems%20for%20the%20defined%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15130v1&entry.124074799=Read"},
{"title": "LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs", "author": "Ziyan Jiang and Xueguang Ma and Wenhu Chen", "abstract": "  In traditional RAG framework, the basic retrieval units are normally short.\nThe common retrievers like DPR normally work with 100-word Wikipedia\nparagraphs. Such a design forces the retriever to search over a large corpus to\nfind the `needle' unit. In contrast, the readers only need to extract answers\nfrom the short retrieved units. Such an imbalanced `heavy' retriever and\n`light' reader design can lead to sub-optimal performance. In order to\nalleviate the imbalance, we propose a new framework LongRAG, consisting of a\n`long retriever' and a `long reader'. LongRAG processes the entire Wikipedia\ninto 4K-token units, which is 30x longer than before. By increasing the unit\nsize, we significantly reduce the total units from 22M to 700K. This\nsignificantly lowers the burden of retriever, which leads to a remarkable\nretrieval score: answer recall@1=71% on NQ (previously 52%) and answer\nrecall@2=72% (previously 47%) on HotpotQA (full-wiki). Then we feed the top-k\nretrieved units ($\\approx$ 30K tokens) to an existing long-context LLM to\nperform zero-shot answer extraction. Without requiring any training, LongRAG\nachieves an EM of 62.7% on NQ, which is the best known result. LongRAG also\nachieves 64.3% on HotpotQA (full-wiki), which is on par of the SoTA model. Our\nstudy offers insights into the future roadmap for combining RAG with\nlong-context LLMs.\n", "link": "http://arxiv.org/abs/2406.15319v1", "date": "2024-06-21", "relevancy": 1.9151, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4986}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4666}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4597}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LongRAG%3A%20Enhancing%20Retrieval-Augmented%20Generation%20with%20Long-context%20LLMs&body=Title%3A%20LongRAG%3A%20Enhancing%20Retrieval-Augmented%20Generation%20with%20Long-context%20LLMs%0AAuthor%3A%20Ziyan%20Jiang%20and%20Xueguang%20Ma%20and%20Wenhu%20Chen%0AAbstract%3A%20%20%20In%20traditional%20RAG%20framework%2C%20the%20basic%20retrieval%20units%20are%20normally%20short.%0AThe%20common%20retrievers%20like%20DPR%20normally%20work%20with%20100-word%20Wikipedia%0Aparagraphs.%20Such%20a%20design%20forces%20the%20retriever%20to%20search%20over%20a%20large%20corpus%20to%0Afind%20the%20%60needle%27%20unit.%20In%20contrast%2C%20the%20readers%20only%20need%20to%20extract%20answers%0Afrom%20the%20short%20retrieved%20units.%20Such%20an%20imbalanced%20%60heavy%27%20retriever%20and%0A%60light%27%20reader%20design%20can%20lead%20to%20sub-optimal%20performance.%20In%20order%20to%0Aalleviate%20the%20imbalance%2C%20we%20propose%20a%20new%20framework%20LongRAG%2C%20consisting%20of%20a%0A%60long%20retriever%27%20and%20a%20%60long%20reader%27.%20LongRAG%20processes%20the%20entire%20Wikipedia%0Ainto%204K-token%20units%2C%20which%20is%2030x%20longer%20than%20before.%20By%20increasing%20the%20unit%0Asize%2C%20we%20significantly%20reduce%20the%20total%20units%20from%2022M%20to%20700K.%20This%0Asignificantly%20lowers%20the%20burden%20of%20retriever%2C%20which%20leads%20to%20a%20remarkable%0Aretrieval%20score%3A%20answer%20recall%401%3D71%25%20on%20NQ%20%28previously%2052%25%29%20and%20answer%0Arecall%402%3D72%25%20%28previously%2047%25%29%20on%20HotpotQA%20%28full-wiki%29.%20Then%20we%20feed%20the%20top-k%0Aretrieved%20units%20%28%24%5Capprox%24%2030K%20tokens%29%20to%20an%20existing%20long-context%20LLM%20to%0Aperform%20zero-shot%20answer%20extraction.%20Without%20requiring%20any%20training%2C%20LongRAG%0Aachieves%20an%20EM%20of%2062.7%25%20on%20NQ%2C%20which%20is%20the%20best%20known%20result.%20LongRAG%20also%0Aachieves%2064.3%25%20on%20HotpotQA%20%28full-wiki%29%2C%20which%20is%20on%20par%20of%20the%20SoTA%20model.%20Our%0Astudy%20offers%20insights%20into%20the%20future%20roadmap%20for%20combining%20RAG%20with%0Along-context%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15319v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLongRAG%253A%2520Enhancing%2520Retrieval-Augmented%2520Generation%2520with%2520Long-context%2520LLMs%26entry.906535625%3DZiyan%2520Jiang%2520and%2520Xueguang%2520Ma%2520and%2520Wenhu%2520Chen%26entry.1292438233%3D%2520%2520In%2520traditional%2520RAG%2520framework%252C%2520the%2520basic%2520retrieval%2520units%2520are%2520normally%2520short.%250AThe%2520common%2520retrievers%2520like%2520DPR%2520normally%2520work%2520with%2520100-word%2520Wikipedia%250Aparagraphs.%2520Such%2520a%2520design%2520forces%2520the%2520retriever%2520to%2520search%2520over%2520a%2520large%2520corpus%2520to%250Afind%2520the%2520%2560needle%2527%2520unit.%2520In%2520contrast%252C%2520the%2520readers%2520only%2520need%2520to%2520extract%2520answers%250Afrom%2520the%2520short%2520retrieved%2520units.%2520Such%2520an%2520imbalanced%2520%2560heavy%2527%2520retriever%2520and%250A%2560light%2527%2520reader%2520design%2520can%2520lead%2520to%2520sub-optimal%2520performance.%2520In%2520order%2520to%250Aalleviate%2520the%2520imbalance%252C%2520we%2520propose%2520a%2520new%2520framework%2520LongRAG%252C%2520consisting%2520of%2520a%250A%2560long%2520retriever%2527%2520and%2520a%2520%2560long%2520reader%2527.%2520LongRAG%2520processes%2520the%2520entire%2520Wikipedia%250Ainto%25204K-token%2520units%252C%2520which%2520is%252030x%2520longer%2520than%2520before.%2520By%2520increasing%2520the%2520unit%250Asize%252C%2520we%2520significantly%2520reduce%2520the%2520total%2520units%2520from%252022M%2520to%2520700K.%2520This%250Asignificantly%2520lowers%2520the%2520burden%2520of%2520retriever%252C%2520which%2520leads%2520to%2520a%2520remarkable%250Aretrieval%2520score%253A%2520answer%2520recall%25401%253D71%2525%2520on%2520NQ%2520%2528previously%252052%2525%2529%2520and%2520answer%250Arecall%25402%253D72%2525%2520%2528previously%252047%2525%2529%2520on%2520HotpotQA%2520%2528full-wiki%2529.%2520Then%2520we%2520feed%2520the%2520top-k%250Aretrieved%2520units%2520%2528%2524%255Capprox%2524%252030K%2520tokens%2529%2520to%2520an%2520existing%2520long-context%2520LLM%2520to%250Aperform%2520zero-shot%2520answer%2520extraction.%2520Without%2520requiring%2520any%2520training%252C%2520LongRAG%250Aachieves%2520an%2520EM%2520of%252062.7%2525%2520on%2520NQ%252C%2520which%2520is%2520the%2520best%2520known%2520result.%2520LongRAG%2520also%250Aachieves%252064.3%2525%2520on%2520HotpotQA%2520%2528full-wiki%2529%252C%2520which%2520is%2520on%2520par%2520of%2520the%2520SoTA%2520model.%2520Our%250Astudy%2520offers%2520insights%2520into%2520the%2520future%2520roadmap%2520for%2520combining%2520RAG%2520with%250Along-context%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15319v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LongRAG%3A%20Enhancing%20Retrieval-Augmented%20Generation%20with%20Long-context%20LLMs&entry.906535625=Ziyan%20Jiang%20and%20Xueguang%20Ma%20and%20Wenhu%20Chen&entry.1292438233=%20%20In%20traditional%20RAG%20framework%2C%20the%20basic%20retrieval%20units%20are%20normally%20short.%0AThe%20common%20retrievers%20like%20DPR%20normally%20work%20with%20100-word%20Wikipedia%0Aparagraphs.%20Such%20a%20design%20forces%20the%20retriever%20to%20search%20over%20a%20large%20corpus%20to%0Afind%20the%20%60needle%27%20unit.%20In%20contrast%2C%20the%20readers%20only%20need%20to%20extract%20answers%0Afrom%20the%20short%20retrieved%20units.%20Such%20an%20imbalanced%20%60heavy%27%20retriever%20and%0A%60light%27%20reader%20design%20can%20lead%20to%20sub-optimal%20performance.%20In%20order%20to%0Aalleviate%20the%20imbalance%2C%20we%20propose%20a%20new%20framework%20LongRAG%2C%20consisting%20of%20a%0A%60long%20retriever%27%20and%20a%20%60long%20reader%27.%20LongRAG%20processes%20the%20entire%20Wikipedia%0Ainto%204K-token%20units%2C%20which%20is%2030x%20longer%20than%20before.%20By%20increasing%20the%20unit%0Asize%2C%20we%20significantly%20reduce%20the%20total%20units%20from%2022M%20to%20700K.%20This%0Asignificantly%20lowers%20the%20burden%20of%20retriever%2C%20which%20leads%20to%20a%20remarkable%0Aretrieval%20score%3A%20answer%20recall%401%3D71%25%20on%20NQ%20%28previously%2052%25%29%20and%20answer%0Arecall%402%3D72%25%20%28previously%2047%25%29%20on%20HotpotQA%20%28full-wiki%29.%20Then%20we%20feed%20the%20top-k%0Aretrieved%20units%20%28%24%5Capprox%24%2030K%20tokens%29%20to%20an%20existing%20long-context%20LLM%20to%0Aperform%20zero-shot%20answer%20extraction.%20Without%20requiring%20any%20training%2C%20LongRAG%0Aachieves%20an%20EM%20of%2062.7%25%20on%20NQ%2C%20which%20is%20the%20best%20known%20result.%20LongRAG%20also%0Aachieves%2064.3%25%20on%20HotpotQA%20%28full-wiki%29%2C%20which%20is%20on%20par%20of%20the%20SoTA%20model.%20Our%0Astudy%20offers%20insights%20into%20the%20future%20roadmap%20for%20combining%20RAG%20with%0Along-context%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15319v1&entry.124074799=Read"},
{"title": "ApiQ: Finetuning of 2-Bit Quantized Large Language Model", "author": "Baohao Liao and Christian Herold and Shahram Khadivi and Christof Monz", "abstract": "  Memory-efficient finetuning of large language models (LLMs) has recently\nattracted huge attention with the increasing size of LLMs, primarily due to the\nconstraints posed by GPU memory limitations and the effectiveness of these\nmethods compared to full finetuning. Despite the advancements, current\nstrategies for memory-efficient finetuning, such as QLoRA, exhibit inconsistent\nperformance across diverse bit-width quantizations and multifaceted tasks. This\ninconsistency largely stems from the detrimental impact of the quantization\nprocess on preserved knowledge, leading to catastrophic forgetting and\nundermining the utilization of pretrained models for finetuning purposes. In\nthis work, we introduce a novel quantization framework, ApiQ, designed to\nrestore the lost information from quantization by concurrently initializing the\nLoRA components and quantizing the weights of LLMs. This approach ensures the\nmaintenance of the original LLM's activation precision while mitigating the\nerror propagation from shallower into deeper layers. Through comprehensive\nevaluations conducted on a spectrum of language tasks with various LLMs, ApiQ\ndemonstrably minimizes activation error during quantization. Consequently, it\nconsistently achieves superior finetuning results across various bit-widths.\n", "link": "http://arxiv.org/abs/2402.05147v3", "date": "2024-06-21", "relevancy": 1.9129, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4824}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4754}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4749}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ApiQ%3A%20Finetuning%20of%202-Bit%20Quantized%20Large%20Language%20Model&body=Title%3A%20ApiQ%3A%20Finetuning%20of%202-Bit%20Quantized%20Large%20Language%20Model%0AAuthor%3A%20Baohao%20Liao%20and%20Christian%20Herold%20and%20Shahram%20Khadivi%20and%20Christof%20Monz%0AAbstract%3A%20%20%20Memory-efficient%20finetuning%20of%20large%20language%20models%20%28LLMs%29%20has%20recently%0Aattracted%20huge%20attention%20with%20the%20increasing%20size%20of%20LLMs%2C%20primarily%20due%20to%20the%0Aconstraints%20posed%20by%20GPU%20memory%20limitations%20and%20the%20effectiveness%20of%20these%0Amethods%20compared%20to%20full%20finetuning.%20Despite%20the%20advancements%2C%20current%0Astrategies%20for%20memory-efficient%20finetuning%2C%20such%20as%20QLoRA%2C%20exhibit%20inconsistent%0Aperformance%20across%20diverse%20bit-width%20quantizations%20and%20multifaceted%20tasks.%20This%0Ainconsistency%20largely%20stems%20from%20the%20detrimental%20impact%20of%20the%20quantization%0Aprocess%20on%20preserved%20knowledge%2C%20leading%20to%20catastrophic%20forgetting%20and%0Aundermining%20the%20utilization%20of%20pretrained%20models%20for%20finetuning%20purposes.%20In%0Athis%20work%2C%20we%20introduce%20a%20novel%20quantization%20framework%2C%20ApiQ%2C%20designed%20to%0Arestore%20the%20lost%20information%20from%20quantization%20by%20concurrently%20initializing%20the%0ALoRA%20components%20and%20quantizing%20the%20weights%20of%20LLMs.%20This%20approach%20ensures%20the%0Amaintenance%20of%20the%20original%20LLM%27s%20activation%20precision%20while%20mitigating%20the%0Aerror%20propagation%20from%20shallower%20into%20deeper%20layers.%20Through%20comprehensive%0Aevaluations%20conducted%20on%20a%20spectrum%20of%20language%20tasks%20with%20various%20LLMs%2C%20ApiQ%0Ademonstrably%20minimizes%20activation%20error%20during%20quantization.%20Consequently%2C%20it%0Aconsistently%20achieves%20superior%20finetuning%20results%20across%20various%20bit-widths.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.05147v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApiQ%253A%2520Finetuning%2520of%25202-Bit%2520Quantized%2520Large%2520Language%2520Model%26entry.906535625%3DBaohao%2520Liao%2520and%2520Christian%2520Herold%2520and%2520Shahram%2520Khadivi%2520and%2520Christof%2520Monz%26entry.1292438233%3D%2520%2520Memory-efficient%2520finetuning%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520recently%250Aattracted%2520huge%2520attention%2520with%2520the%2520increasing%2520size%2520of%2520LLMs%252C%2520primarily%2520due%2520to%2520the%250Aconstraints%2520posed%2520by%2520GPU%2520memory%2520limitations%2520and%2520the%2520effectiveness%2520of%2520these%250Amethods%2520compared%2520to%2520full%2520finetuning.%2520Despite%2520the%2520advancements%252C%2520current%250Astrategies%2520for%2520memory-efficient%2520finetuning%252C%2520such%2520as%2520QLoRA%252C%2520exhibit%2520inconsistent%250Aperformance%2520across%2520diverse%2520bit-width%2520quantizations%2520and%2520multifaceted%2520tasks.%2520This%250Ainconsistency%2520largely%2520stems%2520from%2520the%2520detrimental%2520impact%2520of%2520the%2520quantization%250Aprocess%2520on%2520preserved%2520knowledge%252C%2520leading%2520to%2520catastrophic%2520forgetting%2520and%250Aundermining%2520the%2520utilization%2520of%2520pretrained%2520models%2520for%2520finetuning%2520purposes.%2520In%250Athis%2520work%252C%2520we%2520introduce%2520a%2520novel%2520quantization%2520framework%252C%2520ApiQ%252C%2520designed%2520to%250Arestore%2520the%2520lost%2520information%2520from%2520quantization%2520by%2520concurrently%2520initializing%2520the%250ALoRA%2520components%2520and%2520quantizing%2520the%2520weights%2520of%2520LLMs.%2520This%2520approach%2520ensures%2520the%250Amaintenance%2520of%2520the%2520original%2520LLM%2527s%2520activation%2520precision%2520while%2520mitigating%2520the%250Aerror%2520propagation%2520from%2520shallower%2520into%2520deeper%2520layers.%2520Through%2520comprehensive%250Aevaluations%2520conducted%2520on%2520a%2520spectrum%2520of%2520language%2520tasks%2520with%2520various%2520LLMs%252C%2520ApiQ%250Ademonstrably%2520minimizes%2520activation%2520error%2520during%2520quantization.%2520Consequently%252C%2520it%250Aconsistently%2520achieves%2520superior%2520finetuning%2520results%2520across%2520various%2520bit-widths.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.05147v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ApiQ%3A%20Finetuning%20of%202-Bit%20Quantized%20Large%20Language%20Model&entry.906535625=Baohao%20Liao%20and%20Christian%20Herold%20and%20Shahram%20Khadivi%20and%20Christof%20Monz&entry.1292438233=%20%20Memory-efficient%20finetuning%20of%20large%20language%20models%20%28LLMs%29%20has%20recently%0Aattracted%20huge%20attention%20with%20the%20increasing%20size%20of%20LLMs%2C%20primarily%20due%20to%20the%0Aconstraints%20posed%20by%20GPU%20memory%20limitations%20and%20the%20effectiveness%20of%20these%0Amethods%20compared%20to%20full%20finetuning.%20Despite%20the%20advancements%2C%20current%0Astrategies%20for%20memory-efficient%20finetuning%2C%20such%20as%20QLoRA%2C%20exhibit%20inconsistent%0Aperformance%20across%20diverse%20bit-width%20quantizations%20and%20multifaceted%20tasks.%20This%0Ainconsistency%20largely%20stems%20from%20the%20detrimental%20impact%20of%20the%20quantization%0Aprocess%20on%20preserved%20knowledge%2C%20leading%20to%20catastrophic%20forgetting%20and%0Aundermining%20the%20utilization%20of%20pretrained%20models%20for%20finetuning%20purposes.%20In%0Athis%20work%2C%20we%20introduce%20a%20novel%20quantization%20framework%2C%20ApiQ%2C%20designed%20to%0Arestore%20the%20lost%20information%20from%20quantization%20by%20concurrently%20initializing%20the%0ALoRA%20components%20and%20quantizing%20the%20weights%20of%20LLMs.%20This%20approach%20ensures%20the%0Amaintenance%20of%20the%20original%20LLM%27s%20activation%20precision%20while%20mitigating%20the%0Aerror%20propagation%20from%20shallower%20into%20deeper%20layers.%20Through%20comprehensive%0Aevaluations%20conducted%20on%20a%20spectrum%20of%20language%20tasks%20with%20various%20LLMs%2C%20ApiQ%0Ademonstrably%20minimizes%20activation%20error%20during%20quantization.%20Consequently%2C%20it%0Aconsistently%20achieves%20superior%20finetuning%20results%20across%20various%20bit-widths.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.05147v3&entry.124074799=Read"},
{"title": "Deciphering the Definition of Adversarial Robustness for post-hoc OOD\n  Detectors", "author": "Peter Lorenz and Mario Fernandez and Jens M\u00fcller and Ullrich K\u00f6the", "abstract": "  Detecting out-of-distribution (OOD) inputs is critical for safely deploying\ndeep learning models in real-world scenarios. In recent years, many OOD\ndetectors have been developed, and even the benchmarking has been standardized,\ni.e. OpenOOD. The number of post-hoc detectors is growing fast and showing an\noption to protect a pre-trained classifier against natural distribution shifts,\nclaiming to be ready for real-world scenarios. However, its efficacy in\nhandling adversarial examples has been neglected in the majority of studies.\nThis paper investigates the adversarial robustness of the 16 post-hoc detectors\non several evasion attacks and discuss a roadmap towards adversarial defense in\nOOD detectors.\n", "link": "http://arxiv.org/abs/2406.15104v1", "date": "2024-06-21", "relevancy": 1.9108, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4945}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4828}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deciphering%20the%20Definition%20of%20Adversarial%20Robustness%20for%20post-hoc%20OOD%0A%20%20Detectors&body=Title%3A%20Deciphering%20the%20Definition%20of%20Adversarial%20Robustness%20for%20post-hoc%20OOD%0A%20%20Detectors%0AAuthor%3A%20Peter%20Lorenz%20and%20Mario%20Fernandez%20and%20Jens%20M%C3%BCller%20and%20Ullrich%20K%C3%B6the%0AAbstract%3A%20%20%20Detecting%20out-of-distribution%20%28OOD%29%20inputs%20is%20critical%20for%20safely%20deploying%0Adeep%20learning%20models%20in%20real-world%20scenarios.%20In%20recent%20years%2C%20many%20OOD%0Adetectors%20have%20been%20developed%2C%20and%20even%20the%20benchmarking%20has%20been%20standardized%2C%0Ai.e.%20OpenOOD.%20The%20number%20of%20post-hoc%20detectors%20is%20growing%20fast%20and%20showing%20an%0Aoption%20to%20protect%20a%20pre-trained%20classifier%20against%20natural%20distribution%20shifts%2C%0Aclaiming%20to%20be%20ready%20for%20real-world%20scenarios.%20However%2C%20its%20efficacy%20in%0Ahandling%20adversarial%20examples%20has%20been%20neglected%20in%20the%20majority%20of%20studies.%0AThis%20paper%20investigates%20the%20adversarial%20robustness%20of%20the%2016%20post-hoc%20detectors%0Aon%20several%20evasion%20attacks%20and%20discuss%20a%20roadmap%20towards%20adversarial%20defense%20in%0AOOD%20detectors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15104v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeciphering%2520the%2520Definition%2520of%2520Adversarial%2520Robustness%2520for%2520post-hoc%2520OOD%250A%2520%2520Detectors%26entry.906535625%3DPeter%2520Lorenz%2520and%2520Mario%2520Fernandez%2520and%2520Jens%2520M%25C3%25BCller%2520and%2520Ullrich%2520K%25C3%25B6the%26entry.1292438233%3D%2520%2520Detecting%2520out-of-distribution%2520%2528OOD%2529%2520inputs%2520is%2520critical%2520for%2520safely%2520deploying%250Adeep%2520learning%2520models%2520in%2520real-world%2520scenarios.%2520In%2520recent%2520years%252C%2520many%2520OOD%250Adetectors%2520have%2520been%2520developed%252C%2520and%2520even%2520the%2520benchmarking%2520has%2520been%2520standardized%252C%250Ai.e.%2520OpenOOD.%2520The%2520number%2520of%2520post-hoc%2520detectors%2520is%2520growing%2520fast%2520and%2520showing%2520an%250Aoption%2520to%2520protect%2520a%2520pre-trained%2520classifier%2520against%2520natural%2520distribution%2520shifts%252C%250Aclaiming%2520to%2520be%2520ready%2520for%2520real-world%2520scenarios.%2520However%252C%2520its%2520efficacy%2520in%250Ahandling%2520adversarial%2520examples%2520has%2520been%2520neglected%2520in%2520the%2520majority%2520of%2520studies.%250AThis%2520paper%2520investigates%2520the%2520adversarial%2520robustness%2520of%2520the%252016%2520post-hoc%2520detectors%250Aon%2520several%2520evasion%2520attacks%2520and%2520discuss%2520a%2520roadmap%2520towards%2520adversarial%2520defense%2520in%250AOOD%2520detectors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15104v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deciphering%20the%20Definition%20of%20Adversarial%20Robustness%20for%20post-hoc%20OOD%0A%20%20Detectors&entry.906535625=Peter%20Lorenz%20and%20Mario%20Fernandez%20and%20Jens%20M%C3%BCller%20and%20Ullrich%20K%C3%B6the&entry.1292438233=%20%20Detecting%20out-of-distribution%20%28OOD%29%20inputs%20is%20critical%20for%20safely%20deploying%0Adeep%20learning%20models%20in%20real-world%20scenarios.%20In%20recent%20years%2C%20many%20OOD%0Adetectors%20have%20been%20developed%2C%20and%20even%20the%20benchmarking%20has%20been%20standardized%2C%0Ai.e.%20OpenOOD.%20The%20number%20of%20post-hoc%20detectors%20is%20growing%20fast%20and%20showing%20an%0Aoption%20to%20protect%20a%20pre-trained%20classifier%20against%20natural%20distribution%20shifts%2C%0Aclaiming%20to%20be%20ready%20for%20real-world%20scenarios.%20However%2C%20its%20efficacy%20in%0Ahandling%20adversarial%20examples%20has%20been%20neglected%20in%20the%20majority%20of%20studies.%0AThis%20paper%20investigates%20the%20adversarial%20robustness%20of%20the%2016%20post-hoc%20detectors%0Aon%20several%20evasion%20attacks%20and%20discuss%20a%20roadmap%20towards%20adversarial%20defense%20in%0AOOD%20detectors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15104v1&entry.124074799=Read"},
{"title": "FT-AED: Benchmark Dataset for Early Freeway Traffic Anomalous Event\n  Detection", "author": "Austin Coursey and Junyi Ji and Marcos Quinones-Grueiro and William Barbour and Yuhang Zhang and Tyler Derr and Gautam Biswas", "abstract": "  Early and accurate detection of anomalous events on the freeway, such as\naccidents, can improve emergency response and clearance. However, existing\ndelays and errors in event identification and reporting make it a difficult\nproblem to solve. Current large-scale freeway traffic datasets are not designed\nfor anomaly detection and ignore these challenges. In this paper, we introduce\nthe first large-scale lane-level freeway traffic dataset for anomaly detection.\nOur dataset consists of a month of weekday radar detection sensor data\ncollected in 4 lanes along an 18-mile stretch of Interstate 24 heading toward\nNashville, TN, comprising over 3.7 million sensor measurements. We also collect\nofficial crash reports from the Nashville Traffic Management Center and\nmanually label all other potential anomalies in the dataset. To show the\npotential for our dataset to be used in future machine learning and traffic\nresearch, we benchmark numerous deep learning anomaly detection models on our\ndataset. We find that unsupervised graph neural network autoencoders are a\npromising solution for this problem and that ignoring spatial relationships\nleads to decreased performance. We demonstrate that our methods can reduce\nreporting delays by over 10 minutes on average while detecting 75% of crashes.\nOur dataset and all preprocessing code needed to get started are publicly\nreleased at https://vu.edu/ft-aed/ to facilitate future research.\n", "link": "http://arxiv.org/abs/2406.15283v1", "date": "2024-06-21", "relevancy": 1.9099, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4877}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.474}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4686}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FT-AED%3A%20Benchmark%20Dataset%20for%20Early%20Freeway%20Traffic%20Anomalous%20Event%0A%20%20Detection&body=Title%3A%20FT-AED%3A%20Benchmark%20Dataset%20for%20Early%20Freeway%20Traffic%20Anomalous%20Event%0A%20%20Detection%0AAuthor%3A%20Austin%20Coursey%20and%20Junyi%20Ji%20and%20Marcos%20Quinones-Grueiro%20and%20William%20Barbour%20and%20Yuhang%20Zhang%20and%20Tyler%20Derr%20and%20Gautam%20Biswas%0AAbstract%3A%20%20%20Early%20and%20accurate%20detection%20of%20anomalous%20events%20on%20the%20freeway%2C%20such%20as%0Aaccidents%2C%20can%20improve%20emergency%20response%20and%20clearance.%20However%2C%20existing%0Adelays%20and%20errors%20in%20event%20identification%20and%20reporting%20make%20it%20a%20difficult%0Aproblem%20to%20solve.%20Current%20large-scale%20freeway%20traffic%20datasets%20are%20not%20designed%0Afor%20anomaly%20detection%20and%20ignore%20these%20challenges.%20In%20this%20paper%2C%20we%20introduce%0Athe%20first%20large-scale%20lane-level%20freeway%20traffic%20dataset%20for%20anomaly%20detection.%0AOur%20dataset%20consists%20of%20a%20month%20of%20weekday%20radar%20detection%20sensor%20data%0Acollected%20in%204%20lanes%20along%20an%2018-mile%20stretch%20of%20Interstate%2024%20heading%20toward%0ANashville%2C%20TN%2C%20comprising%20over%203.7%20million%20sensor%20measurements.%20We%20also%20collect%0Aofficial%20crash%20reports%20from%20the%20Nashville%20Traffic%20Management%20Center%20and%0Amanually%20label%20all%20other%20potential%20anomalies%20in%20the%20dataset.%20To%20show%20the%0Apotential%20for%20our%20dataset%20to%20be%20used%20in%20future%20machine%20learning%20and%20traffic%0Aresearch%2C%20we%20benchmark%20numerous%20deep%20learning%20anomaly%20detection%20models%20on%20our%0Adataset.%20We%20find%20that%20unsupervised%20graph%20neural%20network%20autoencoders%20are%20a%0Apromising%20solution%20for%20this%20problem%20and%20that%20ignoring%20spatial%20relationships%0Aleads%20to%20decreased%20performance.%20We%20demonstrate%20that%20our%20methods%20can%20reduce%0Areporting%20delays%20by%20over%2010%20minutes%20on%20average%20while%20detecting%2075%25%20of%20crashes.%0AOur%20dataset%20and%20all%20preprocessing%20code%20needed%20to%20get%20started%20are%20publicly%0Areleased%20at%20https%3A//vu.edu/ft-aed/%20to%20facilitate%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15283v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFT-AED%253A%2520Benchmark%2520Dataset%2520for%2520Early%2520Freeway%2520Traffic%2520Anomalous%2520Event%250A%2520%2520Detection%26entry.906535625%3DAustin%2520Coursey%2520and%2520Junyi%2520Ji%2520and%2520Marcos%2520Quinones-Grueiro%2520and%2520William%2520Barbour%2520and%2520Yuhang%2520Zhang%2520and%2520Tyler%2520Derr%2520and%2520Gautam%2520Biswas%26entry.1292438233%3D%2520%2520Early%2520and%2520accurate%2520detection%2520of%2520anomalous%2520events%2520on%2520the%2520freeway%252C%2520such%2520as%250Aaccidents%252C%2520can%2520improve%2520emergency%2520response%2520and%2520clearance.%2520However%252C%2520existing%250Adelays%2520and%2520errors%2520in%2520event%2520identification%2520and%2520reporting%2520make%2520it%2520a%2520difficult%250Aproblem%2520to%2520solve.%2520Current%2520large-scale%2520freeway%2520traffic%2520datasets%2520are%2520not%2520designed%250Afor%2520anomaly%2520detection%2520and%2520ignore%2520these%2520challenges.%2520In%2520this%2520paper%252C%2520we%2520introduce%250Athe%2520first%2520large-scale%2520lane-level%2520freeway%2520traffic%2520dataset%2520for%2520anomaly%2520detection.%250AOur%2520dataset%2520consists%2520of%2520a%2520month%2520of%2520weekday%2520radar%2520detection%2520sensor%2520data%250Acollected%2520in%25204%2520lanes%2520along%2520an%252018-mile%2520stretch%2520of%2520Interstate%252024%2520heading%2520toward%250ANashville%252C%2520TN%252C%2520comprising%2520over%25203.7%2520million%2520sensor%2520measurements.%2520We%2520also%2520collect%250Aofficial%2520crash%2520reports%2520from%2520the%2520Nashville%2520Traffic%2520Management%2520Center%2520and%250Amanually%2520label%2520all%2520other%2520potential%2520anomalies%2520in%2520the%2520dataset.%2520To%2520show%2520the%250Apotential%2520for%2520our%2520dataset%2520to%2520be%2520used%2520in%2520future%2520machine%2520learning%2520and%2520traffic%250Aresearch%252C%2520we%2520benchmark%2520numerous%2520deep%2520learning%2520anomaly%2520detection%2520models%2520on%2520our%250Adataset.%2520We%2520find%2520that%2520unsupervised%2520graph%2520neural%2520network%2520autoencoders%2520are%2520a%250Apromising%2520solution%2520for%2520this%2520problem%2520and%2520that%2520ignoring%2520spatial%2520relationships%250Aleads%2520to%2520decreased%2520performance.%2520We%2520demonstrate%2520that%2520our%2520methods%2520can%2520reduce%250Areporting%2520delays%2520by%2520over%252010%2520minutes%2520on%2520average%2520while%2520detecting%252075%2525%2520of%2520crashes.%250AOur%2520dataset%2520and%2520all%2520preprocessing%2520code%2520needed%2520to%2520get%2520started%2520are%2520publicly%250Areleased%2520at%2520https%253A//vu.edu/ft-aed/%2520to%2520facilitate%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15283v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FT-AED%3A%20Benchmark%20Dataset%20for%20Early%20Freeway%20Traffic%20Anomalous%20Event%0A%20%20Detection&entry.906535625=Austin%20Coursey%20and%20Junyi%20Ji%20and%20Marcos%20Quinones-Grueiro%20and%20William%20Barbour%20and%20Yuhang%20Zhang%20and%20Tyler%20Derr%20and%20Gautam%20Biswas&entry.1292438233=%20%20Early%20and%20accurate%20detection%20of%20anomalous%20events%20on%20the%20freeway%2C%20such%20as%0Aaccidents%2C%20can%20improve%20emergency%20response%20and%20clearance.%20However%2C%20existing%0Adelays%20and%20errors%20in%20event%20identification%20and%20reporting%20make%20it%20a%20difficult%0Aproblem%20to%20solve.%20Current%20large-scale%20freeway%20traffic%20datasets%20are%20not%20designed%0Afor%20anomaly%20detection%20and%20ignore%20these%20challenges.%20In%20this%20paper%2C%20we%20introduce%0Athe%20first%20large-scale%20lane-level%20freeway%20traffic%20dataset%20for%20anomaly%20detection.%0AOur%20dataset%20consists%20of%20a%20month%20of%20weekday%20radar%20detection%20sensor%20data%0Acollected%20in%204%20lanes%20along%20an%2018-mile%20stretch%20of%20Interstate%2024%20heading%20toward%0ANashville%2C%20TN%2C%20comprising%20over%203.7%20million%20sensor%20measurements.%20We%20also%20collect%0Aofficial%20crash%20reports%20from%20the%20Nashville%20Traffic%20Management%20Center%20and%0Amanually%20label%20all%20other%20potential%20anomalies%20in%20the%20dataset.%20To%20show%20the%0Apotential%20for%20our%20dataset%20to%20be%20used%20in%20future%20machine%20learning%20and%20traffic%0Aresearch%2C%20we%20benchmark%20numerous%20deep%20learning%20anomaly%20detection%20models%20on%20our%0Adataset.%20We%20find%20that%20unsupervised%20graph%20neural%20network%20autoencoders%20are%20a%0Apromising%20solution%20for%20this%20problem%20and%20that%20ignoring%20spatial%20relationships%0Aleads%20to%20decreased%20performance.%20We%20demonstrate%20that%20our%20methods%20can%20reduce%0Areporting%20delays%20by%20over%2010%20minutes%20on%20average%20while%20detecting%2075%25%20of%20crashes.%0AOur%20dataset%20and%20all%20preprocessing%20code%20needed%20to%20get%20started%20are%20publicly%0Areleased%20at%20https%3A//vu.edu/ft-aed/%20to%20facilitate%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15283v1&entry.124074799=Read"},
{"title": "Information Guided Regularization for Fine-tuning Language Models", "author": "Mandar Sharma and Nikhil Muralidhar and Shengzhe Xu and Raquib Bin Yousuf and Naren Ramakrishnan", "abstract": "  The pretraining-fine-tuning paradigm has been the de facto strategy for\ntransfer learning in modern language modeling. With the understanding that task\nadaptation in LMs is often a function of parameters shared across tasks, we\nargue that a more surgical approach to regularization needs to exist for\nsmoother transfer learning. Towards this end, we investigate how the\npretraining loss landscape is affected by these task-sensitive parameters\nthrough an information-theoretic lens. We then leverage the findings from our\ninvestigations to devise a novel approach to dropout for improved model\nregularization and better downstream generalization. This approach, named\nguided dropout, is both task & architecture agnostic and adds no computational\noverhead to the fine-tuning process. Through empirical evaluations, we showcase\nthat our approach to regularization yields consistently better performance,\neven in scenarios of data paucity, compared to standardized baselines.\n", "link": "http://arxiv.org/abs/2406.14005v2", "date": "2024-06-21", "relevancy": 1.886, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4791}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.479}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Information%20Guided%20Regularization%20for%20Fine-tuning%20Language%20Models&body=Title%3A%20Information%20Guided%20Regularization%20for%20Fine-tuning%20Language%20Models%0AAuthor%3A%20Mandar%20Sharma%20and%20Nikhil%20Muralidhar%20and%20Shengzhe%20Xu%20and%20Raquib%20Bin%20Yousuf%20and%20Naren%20Ramakrishnan%0AAbstract%3A%20%20%20The%20pretraining-fine-tuning%20paradigm%20has%20been%20the%20de%20facto%20strategy%20for%0Atransfer%20learning%20in%20modern%20language%20modeling.%20With%20the%20understanding%20that%20task%0Aadaptation%20in%20LMs%20is%20often%20a%20function%20of%20parameters%20shared%20across%20tasks%2C%20we%0Aargue%20that%20a%20more%20surgical%20approach%20to%20regularization%20needs%20to%20exist%20for%0Asmoother%20transfer%20learning.%20Towards%20this%20end%2C%20we%20investigate%20how%20the%0Apretraining%20loss%20landscape%20is%20affected%20by%20these%20task-sensitive%20parameters%0Athrough%20an%20information-theoretic%20lens.%20We%20then%20leverage%20the%20findings%20from%20our%0Ainvestigations%20to%20devise%20a%20novel%20approach%20to%20dropout%20for%20improved%20model%0Aregularization%20and%20better%20downstream%20generalization.%20This%20approach%2C%20named%0Aguided%20dropout%2C%20is%20both%20task%20%26%20architecture%20agnostic%20and%20adds%20no%20computational%0Aoverhead%20to%20the%20fine-tuning%20process.%20Through%20empirical%20evaluations%2C%20we%20showcase%0Athat%20our%20approach%20to%20regularization%20yields%20consistently%20better%20performance%2C%0Aeven%20in%20scenarios%20of%20data%20paucity%2C%20compared%20to%20standardized%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14005v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInformation%2520Guided%2520Regularization%2520for%2520Fine-tuning%2520Language%2520Models%26entry.906535625%3DMandar%2520Sharma%2520and%2520Nikhil%2520Muralidhar%2520and%2520Shengzhe%2520Xu%2520and%2520Raquib%2520Bin%2520Yousuf%2520and%2520Naren%2520Ramakrishnan%26entry.1292438233%3D%2520%2520The%2520pretraining-fine-tuning%2520paradigm%2520has%2520been%2520the%2520de%2520facto%2520strategy%2520for%250Atransfer%2520learning%2520in%2520modern%2520language%2520modeling.%2520With%2520the%2520understanding%2520that%2520task%250Aadaptation%2520in%2520LMs%2520is%2520often%2520a%2520function%2520of%2520parameters%2520shared%2520across%2520tasks%252C%2520we%250Aargue%2520that%2520a%2520more%2520surgical%2520approach%2520to%2520regularization%2520needs%2520to%2520exist%2520for%250Asmoother%2520transfer%2520learning.%2520Towards%2520this%2520end%252C%2520we%2520investigate%2520how%2520the%250Apretraining%2520loss%2520landscape%2520is%2520affected%2520by%2520these%2520task-sensitive%2520parameters%250Athrough%2520an%2520information-theoretic%2520lens.%2520We%2520then%2520leverage%2520the%2520findings%2520from%2520our%250Ainvestigations%2520to%2520devise%2520a%2520novel%2520approach%2520to%2520dropout%2520for%2520improved%2520model%250Aregularization%2520and%2520better%2520downstream%2520generalization.%2520This%2520approach%252C%2520named%250Aguided%2520dropout%252C%2520is%2520both%2520task%2520%2526%2520architecture%2520agnostic%2520and%2520adds%2520no%2520computational%250Aoverhead%2520to%2520the%2520fine-tuning%2520process.%2520Through%2520empirical%2520evaluations%252C%2520we%2520showcase%250Athat%2520our%2520approach%2520to%2520regularization%2520yields%2520consistently%2520better%2520performance%252C%250Aeven%2520in%2520scenarios%2520of%2520data%2520paucity%252C%2520compared%2520to%2520standardized%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14005v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Information%20Guided%20Regularization%20for%20Fine-tuning%20Language%20Models&entry.906535625=Mandar%20Sharma%20and%20Nikhil%20Muralidhar%20and%20Shengzhe%20Xu%20and%20Raquib%20Bin%20Yousuf%20and%20Naren%20Ramakrishnan&entry.1292438233=%20%20The%20pretraining-fine-tuning%20paradigm%20has%20been%20the%20de%20facto%20strategy%20for%0Atransfer%20learning%20in%20modern%20language%20modeling.%20With%20the%20understanding%20that%20task%0Aadaptation%20in%20LMs%20is%20often%20a%20function%20of%20parameters%20shared%20across%20tasks%2C%20we%0Aargue%20that%20a%20more%20surgical%20approach%20to%20regularization%20needs%20to%20exist%20for%0Asmoother%20transfer%20learning.%20Towards%20this%20end%2C%20we%20investigate%20how%20the%0Apretraining%20loss%20landscape%20is%20affected%20by%20these%20task-sensitive%20parameters%0Athrough%20an%20information-theoretic%20lens.%20We%20then%20leverage%20the%20findings%20from%20our%0Ainvestigations%20to%20devise%20a%20novel%20approach%20to%20dropout%20for%20improved%20model%0Aregularization%20and%20better%20downstream%20generalization.%20This%20approach%2C%20named%0Aguided%20dropout%2C%20is%20both%20task%20%26%20architecture%20agnostic%20and%20adds%20no%20computational%0Aoverhead%20to%20the%20fine-tuning%20process.%20Through%20empirical%20evaluations%2C%20we%20showcase%0Athat%20our%20approach%20to%20regularization%20yields%20consistently%20better%20performance%2C%0Aeven%20in%20scenarios%20of%20data%20paucity%2C%20compared%20to%20standardized%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14005v2&entry.124074799=Read"},
{"title": "Benchmarking Pathology Feature Extractors for Whole Slide Image\n  Classification", "author": "Georg W\u00f6lflein and Dyke Ferber and Asier R. Meneghetti and Omar S. M. El Nahhas and Daniel Truhn and Zunamys I. Carrero and David J. Harrison and Ognjen Arandjelovi\u0107 and Jakob Nikolas Kather", "abstract": "  Weakly supervised whole slide image classification is a key task in\ncomputational pathology, which involves predicting a slide-level label from a\nset of image patches constituting the slide. Constructing models to solve this\ntask involves multiple design choices, often made without robust empirical or\nconclusive theoretical justification. To address this, we conduct a\ncomprehensive benchmarking of feature extractors to answer three critical\nquestions: 1) Is stain normalisation still a necessary preprocessing step? 2)\nWhich feature extractors are best for downstream slide-level classification? 3)\nHow does magnification affect downstream performance? Our study constitutes the\nmost comprehensive evaluation of publicly available pathology feature\nextractors to date, involving more than 10,000 training runs across 14 feature\nextractors, 9 tasks, 5 datasets, 3 downstream architectures, 2 levels of\nmagnification, and various preprocessing setups. Our findings challenge\nexisting assumptions: 1) We observe empirically, and by analysing the latent\nspace, that skipping stain normalisation and image augmentations does not\ndegrade performance, while significantly reducing memory and computational\ndemands. 2) We develop a novel evaluation metric to compare relative downstream\nperformance, and show that the choice of feature extractor is the most\nconsequential factor for downstream performance. 3) We find that\nlower-magnification slides are sufficient for accurate slide-level\nclassification. Contrary to previous patch-level benchmarking studies, our\napproach emphasises clinical relevance by focusing on slide-level biomarker\nprediction tasks in a weakly supervised setting with external validation\ncohorts. Our findings stand to streamline digital pathology workflows by\nminimising preprocessing needs and informing the selection of feature\nextractors.\n", "link": "http://arxiv.org/abs/2311.11772v5", "date": "2024-06-21", "relevancy": 1.8685, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4718}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4639}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4635}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Pathology%20Feature%20Extractors%20for%20Whole%20Slide%20Image%0A%20%20Classification&body=Title%3A%20Benchmarking%20Pathology%20Feature%20Extractors%20for%20Whole%20Slide%20Image%0A%20%20Classification%0AAuthor%3A%20Georg%20W%C3%B6lflein%20and%20Dyke%20Ferber%20and%20Asier%20R.%20Meneghetti%20and%20Omar%20S.%20M.%20El%20Nahhas%20and%20Daniel%20Truhn%20and%20Zunamys%20I.%20Carrero%20and%20David%20J.%20Harrison%20and%20Ognjen%20Arandjelovi%C4%87%20and%20Jakob%20Nikolas%20Kather%0AAbstract%3A%20%20%20Weakly%20supervised%20whole%20slide%20image%20classification%20is%20a%20key%20task%20in%0Acomputational%20pathology%2C%20which%20involves%20predicting%20a%20slide-level%20label%20from%20a%0Aset%20of%20image%20patches%20constituting%20the%20slide.%20Constructing%20models%20to%20solve%20this%0Atask%20involves%20multiple%20design%20choices%2C%20often%20made%20without%20robust%20empirical%20or%0Aconclusive%20theoretical%20justification.%20To%20address%20this%2C%20we%20conduct%20a%0Acomprehensive%20benchmarking%20of%20feature%20extractors%20to%20answer%20three%20critical%0Aquestions%3A%201%29%20Is%20stain%20normalisation%20still%20a%20necessary%20preprocessing%20step%3F%202%29%0AWhich%20feature%20extractors%20are%20best%20for%20downstream%20slide-level%20classification%3F%203%29%0AHow%20does%20magnification%20affect%20downstream%20performance%3F%20Our%20study%20constitutes%20the%0Amost%20comprehensive%20evaluation%20of%20publicly%20available%20pathology%20feature%0Aextractors%20to%20date%2C%20involving%20more%20than%2010%2C000%20training%20runs%20across%2014%20feature%0Aextractors%2C%209%20tasks%2C%205%20datasets%2C%203%20downstream%20architectures%2C%202%20levels%20of%0Amagnification%2C%20and%20various%20preprocessing%20setups.%20Our%20findings%20challenge%0Aexisting%20assumptions%3A%201%29%20We%20observe%20empirically%2C%20and%20by%20analysing%20the%20latent%0Aspace%2C%20that%20skipping%20stain%20normalisation%20and%20image%20augmentations%20does%20not%0Adegrade%20performance%2C%20while%20significantly%20reducing%20memory%20and%20computational%0Ademands.%202%29%20We%20develop%20a%20novel%20evaluation%20metric%20to%20compare%20relative%20downstream%0Aperformance%2C%20and%20show%20that%20the%20choice%20of%20feature%20extractor%20is%20the%20most%0Aconsequential%20factor%20for%20downstream%20performance.%203%29%20We%20find%20that%0Alower-magnification%20slides%20are%20sufficient%20for%20accurate%20slide-level%0Aclassification.%20Contrary%20to%20previous%20patch-level%20benchmarking%20studies%2C%20our%0Aapproach%20emphasises%20clinical%20relevance%20by%20focusing%20on%20slide-level%20biomarker%0Aprediction%20tasks%20in%20a%20weakly%20supervised%20setting%20with%20external%20validation%0Acohorts.%20Our%20findings%20stand%20to%20streamline%20digital%20pathology%20workflows%20by%0Aminimising%20preprocessing%20needs%20and%20informing%20the%20selection%20of%20feature%0Aextractors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.11772v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Pathology%2520Feature%2520Extractors%2520for%2520Whole%2520Slide%2520Image%250A%2520%2520Classification%26entry.906535625%3DGeorg%2520W%25C3%25B6lflein%2520and%2520Dyke%2520Ferber%2520and%2520Asier%2520R.%2520Meneghetti%2520and%2520Omar%2520S.%2520M.%2520El%2520Nahhas%2520and%2520Daniel%2520Truhn%2520and%2520Zunamys%2520I.%2520Carrero%2520and%2520David%2520J.%2520Harrison%2520and%2520Ognjen%2520Arandjelovi%25C4%2587%2520and%2520Jakob%2520Nikolas%2520Kather%26entry.1292438233%3D%2520%2520Weakly%2520supervised%2520whole%2520slide%2520image%2520classification%2520is%2520a%2520key%2520task%2520in%250Acomputational%2520pathology%252C%2520which%2520involves%2520predicting%2520a%2520slide-level%2520label%2520from%2520a%250Aset%2520of%2520image%2520patches%2520constituting%2520the%2520slide.%2520Constructing%2520models%2520to%2520solve%2520this%250Atask%2520involves%2520multiple%2520design%2520choices%252C%2520often%2520made%2520without%2520robust%2520empirical%2520or%250Aconclusive%2520theoretical%2520justification.%2520To%2520address%2520this%252C%2520we%2520conduct%2520a%250Acomprehensive%2520benchmarking%2520of%2520feature%2520extractors%2520to%2520answer%2520three%2520critical%250Aquestions%253A%25201%2529%2520Is%2520stain%2520normalisation%2520still%2520a%2520necessary%2520preprocessing%2520step%253F%25202%2529%250AWhich%2520feature%2520extractors%2520are%2520best%2520for%2520downstream%2520slide-level%2520classification%253F%25203%2529%250AHow%2520does%2520magnification%2520affect%2520downstream%2520performance%253F%2520Our%2520study%2520constitutes%2520the%250Amost%2520comprehensive%2520evaluation%2520of%2520publicly%2520available%2520pathology%2520feature%250Aextractors%2520to%2520date%252C%2520involving%2520more%2520than%252010%252C000%2520training%2520runs%2520across%252014%2520feature%250Aextractors%252C%25209%2520tasks%252C%25205%2520datasets%252C%25203%2520downstream%2520architectures%252C%25202%2520levels%2520of%250Amagnification%252C%2520and%2520various%2520preprocessing%2520setups.%2520Our%2520findings%2520challenge%250Aexisting%2520assumptions%253A%25201%2529%2520We%2520observe%2520empirically%252C%2520and%2520by%2520analysing%2520the%2520latent%250Aspace%252C%2520that%2520skipping%2520stain%2520normalisation%2520and%2520image%2520augmentations%2520does%2520not%250Adegrade%2520performance%252C%2520while%2520significantly%2520reducing%2520memory%2520and%2520computational%250Ademands.%25202%2529%2520We%2520develop%2520a%2520novel%2520evaluation%2520metric%2520to%2520compare%2520relative%2520downstream%250Aperformance%252C%2520and%2520show%2520that%2520the%2520choice%2520of%2520feature%2520extractor%2520is%2520the%2520most%250Aconsequential%2520factor%2520for%2520downstream%2520performance.%25203%2529%2520We%2520find%2520that%250Alower-magnification%2520slides%2520are%2520sufficient%2520for%2520accurate%2520slide-level%250Aclassification.%2520Contrary%2520to%2520previous%2520patch-level%2520benchmarking%2520studies%252C%2520our%250Aapproach%2520emphasises%2520clinical%2520relevance%2520by%2520focusing%2520on%2520slide-level%2520biomarker%250Aprediction%2520tasks%2520in%2520a%2520weakly%2520supervised%2520setting%2520with%2520external%2520validation%250Acohorts.%2520Our%2520findings%2520stand%2520to%2520streamline%2520digital%2520pathology%2520workflows%2520by%250Aminimising%2520preprocessing%2520needs%2520and%2520informing%2520the%2520selection%2520of%2520feature%250Aextractors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.11772v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Pathology%20Feature%20Extractors%20for%20Whole%20Slide%20Image%0A%20%20Classification&entry.906535625=Georg%20W%C3%B6lflein%20and%20Dyke%20Ferber%20and%20Asier%20R.%20Meneghetti%20and%20Omar%20S.%20M.%20El%20Nahhas%20and%20Daniel%20Truhn%20and%20Zunamys%20I.%20Carrero%20and%20David%20J.%20Harrison%20and%20Ognjen%20Arandjelovi%C4%87%20and%20Jakob%20Nikolas%20Kather&entry.1292438233=%20%20Weakly%20supervised%20whole%20slide%20image%20classification%20is%20a%20key%20task%20in%0Acomputational%20pathology%2C%20which%20involves%20predicting%20a%20slide-level%20label%20from%20a%0Aset%20of%20image%20patches%20constituting%20the%20slide.%20Constructing%20models%20to%20solve%20this%0Atask%20involves%20multiple%20design%20choices%2C%20often%20made%20without%20robust%20empirical%20or%0Aconclusive%20theoretical%20justification.%20To%20address%20this%2C%20we%20conduct%20a%0Acomprehensive%20benchmarking%20of%20feature%20extractors%20to%20answer%20three%20critical%0Aquestions%3A%201%29%20Is%20stain%20normalisation%20still%20a%20necessary%20preprocessing%20step%3F%202%29%0AWhich%20feature%20extractors%20are%20best%20for%20downstream%20slide-level%20classification%3F%203%29%0AHow%20does%20magnification%20affect%20downstream%20performance%3F%20Our%20study%20constitutes%20the%0Amost%20comprehensive%20evaluation%20of%20publicly%20available%20pathology%20feature%0Aextractors%20to%20date%2C%20involving%20more%20than%2010%2C000%20training%20runs%20across%2014%20feature%0Aextractors%2C%209%20tasks%2C%205%20datasets%2C%203%20downstream%20architectures%2C%202%20levels%20of%0Amagnification%2C%20and%20various%20preprocessing%20setups.%20Our%20findings%20challenge%0Aexisting%20assumptions%3A%201%29%20We%20observe%20empirically%2C%20and%20by%20analysing%20the%20latent%0Aspace%2C%20that%20skipping%20stain%20normalisation%20and%20image%20augmentations%20does%20not%0Adegrade%20performance%2C%20while%20significantly%20reducing%20memory%20and%20computational%0Ademands.%202%29%20We%20develop%20a%20novel%20evaluation%20metric%20to%20compare%20relative%20downstream%0Aperformance%2C%20and%20show%20that%20the%20choice%20of%20feature%20extractor%20is%20the%20most%0Aconsequential%20factor%20for%20downstream%20performance.%203%29%20We%20find%20that%0Alower-magnification%20slides%20are%20sufficient%20for%20accurate%20slide-level%0Aclassification.%20Contrary%20to%20previous%20patch-level%20benchmarking%20studies%2C%20our%0Aapproach%20emphasises%20clinical%20relevance%20by%20focusing%20on%20slide-level%20biomarker%0Aprediction%20tasks%20in%20a%20weakly%20supervised%20setting%20with%20external%20validation%0Acohorts.%20Our%20findings%20stand%20to%20streamline%20digital%20pathology%20workflows%20by%0Aminimising%20preprocessing%20needs%20and%20informing%20the%20selection%20of%20feature%0Aextractors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.11772v5&entry.124074799=Read"},
{"title": "ExDAG: Exact learning of DAGs", "author": "Pavel Ryt\u00ed\u0159 and Ale\u0161 Wodecki and Jakub Mare\u010dek", "abstract": "  There has been a growing interest in causal learning in recent years.\nCommonly used representations of causal structures, including Bayesian networks\nand structural equation models (SEM), take the form of directed acyclic graphs\n(DAGs). We provide a novel mixed-integer quadratic programming formulation and\nassociated algorithm that identifies DAGs on up to 50 vertices, where these are\nidentifiable. We call this method ExDAG, which stands for Exact learning of\nDAGs. Although there is a superexponential number of constraints that prevent\nthe formation of cycles, the algorithm adds constraints violated by solutions\nfound, rather than imposing all constraints in each continuous-valued\nrelaxation. Our empirical results show that ExDAG outperforms local\nstate-of-the-art solvers in terms of precision and outperforms state-of-the-art\nglobal solvers with respect to scaling, when considering Gaussian noise. We\nalso provide validation with respect to other noise distributions.\n", "link": "http://arxiv.org/abs/2406.15229v1", "date": "2024-06-21", "relevancy": 1.8565, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4789}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4655}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ExDAG%3A%20Exact%20learning%20of%20DAGs&body=Title%3A%20ExDAG%3A%20Exact%20learning%20of%20DAGs%0AAuthor%3A%20Pavel%20Ryt%C3%AD%C5%99%20and%20Ale%C5%A1%20Wodecki%20and%20Jakub%20Mare%C4%8Dek%0AAbstract%3A%20%20%20There%20has%20been%20a%20growing%20interest%20in%20causal%20learning%20in%20recent%20years.%0ACommonly%20used%20representations%20of%20causal%20structures%2C%20including%20Bayesian%20networks%0Aand%20structural%20equation%20models%20%28SEM%29%2C%20take%20the%20form%20of%20directed%20acyclic%20graphs%0A%28DAGs%29.%20We%20provide%20a%20novel%20mixed-integer%20quadratic%20programming%20formulation%20and%0Aassociated%20algorithm%20that%20identifies%20DAGs%20on%20up%20to%2050%20vertices%2C%20where%20these%20are%0Aidentifiable.%20We%20call%20this%20method%20ExDAG%2C%20which%20stands%20for%20Exact%20learning%20of%0ADAGs.%20Although%20there%20is%20a%20superexponential%20number%20of%20constraints%20that%20prevent%0Athe%20formation%20of%20cycles%2C%20the%20algorithm%20adds%20constraints%20violated%20by%20solutions%0Afound%2C%20rather%20than%20imposing%20all%20constraints%20in%20each%20continuous-valued%0Arelaxation.%20Our%20empirical%20results%20show%20that%20ExDAG%20outperforms%20local%0Astate-of-the-art%20solvers%20in%20terms%20of%20precision%20and%20outperforms%20state-of-the-art%0Aglobal%20solvers%20with%20respect%20to%20scaling%2C%20when%20considering%20Gaussian%20noise.%20We%0Aalso%20provide%20validation%20with%20respect%20to%20other%20noise%20distributions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15229v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExDAG%253A%2520Exact%2520learning%2520of%2520DAGs%26entry.906535625%3DPavel%2520Ryt%25C3%25AD%25C5%2599%2520and%2520Ale%25C5%25A1%2520Wodecki%2520and%2520Jakub%2520Mare%25C4%258Dek%26entry.1292438233%3D%2520%2520There%2520has%2520been%2520a%2520growing%2520interest%2520in%2520causal%2520learning%2520in%2520recent%2520years.%250ACommonly%2520used%2520representations%2520of%2520causal%2520structures%252C%2520including%2520Bayesian%2520networks%250Aand%2520structural%2520equation%2520models%2520%2528SEM%2529%252C%2520take%2520the%2520form%2520of%2520directed%2520acyclic%2520graphs%250A%2528DAGs%2529.%2520We%2520provide%2520a%2520novel%2520mixed-integer%2520quadratic%2520programming%2520formulation%2520and%250Aassociated%2520algorithm%2520that%2520identifies%2520DAGs%2520on%2520up%2520to%252050%2520vertices%252C%2520where%2520these%2520are%250Aidentifiable.%2520We%2520call%2520this%2520method%2520ExDAG%252C%2520which%2520stands%2520for%2520Exact%2520learning%2520of%250ADAGs.%2520Although%2520there%2520is%2520a%2520superexponential%2520number%2520of%2520constraints%2520that%2520prevent%250Athe%2520formation%2520of%2520cycles%252C%2520the%2520algorithm%2520adds%2520constraints%2520violated%2520by%2520solutions%250Afound%252C%2520rather%2520than%2520imposing%2520all%2520constraints%2520in%2520each%2520continuous-valued%250Arelaxation.%2520Our%2520empirical%2520results%2520show%2520that%2520ExDAG%2520outperforms%2520local%250Astate-of-the-art%2520solvers%2520in%2520terms%2520of%2520precision%2520and%2520outperforms%2520state-of-the-art%250Aglobal%2520solvers%2520with%2520respect%2520to%2520scaling%252C%2520when%2520considering%2520Gaussian%2520noise.%2520We%250Aalso%2520provide%2520validation%2520with%2520respect%2520to%2520other%2520noise%2520distributions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15229v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ExDAG%3A%20Exact%20learning%20of%20DAGs&entry.906535625=Pavel%20Ryt%C3%AD%C5%99%20and%20Ale%C5%A1%20Wodecki%20and%20Jakub%20Mare%C4%8Dek&entry.1292438233=%20%20There%20has%20been%20a%20growing%20interest%20in%20causal%20learning%20in%20recent%20years.%0ACommonly%20used%20representations%20of%20causal%20structures%2C%20including%20Bayesian%20networks%0Aand%20structural%20equation%20models%20%28SEM%29%2C%20take%20the%20form%20of%20directed%20acyclic%20graphs%0A%28DAGs%29.%20We%20provide%20a%20novel%20mixed-integer%20quadratic%20programming%20formulation%20and%0Aassociated%20algorithm%20that%20identifies%20DAGs%20on%20up%20to%2050%20vertices%2C%20where%20these%20are%0Aidentifiable.%20We%20call%20this%20method%20ExDAG%2C%20which%20stands%20for%20Exact%20learning%20of%0ADAGs.%20Although%20there%20is%20a%20superexponential%20number%20of%20constraints%20that%20prevent%0Athe%20formation%20of%20cycles%2C%20the%20algorithm%20adds%20constraints%20violated%20by%20solutions%0Afound%2C%20rather%20than%20imposing%20all%20constraints%20in%20each%20continuous-valued%0Arelaxation.%20Our%20empirical%20results%20show%20that%20ExDAG%20outperforms%20local%0Astate-of-the-art%20solvers%20in%20terms%20of%20precision%20and%20outperforms%20state-of-the-art%0Aglobal%20solvers%20with%20respect%20to%20scaling%2C%20when%20considering%20Gaussian%20noise.%20We%0Aalso%20provide%20validation%20with%20respect%20to%20other%20noise%20distributions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15229v1&entry.124074799=Read"},
{"title": "Fast sampling from constrained spaces using the Metropolis-adjusted\n  Mirror Langevin algorithm", "author": "Vishwak Srinivasan and Andre Wibisono and Ashia Wilson", "abstract": "  We propose a new method called the Metropolis-adjusted Mirror Langevin\nalgorithm for approximate sampling from distributions whose support is a\ncompact and convex set. This algorithm adds an accept-reject filter to the\nMarkov chain induced by a single step of the Mirror Langevin algorithm (Zhang\net al., 2020), which is a basic discretisation of the Mirror Langevin dynamics.\nDue to the inclusion of this filter, our method is unbiased relative to the\ntarget, while known discretisations of the Mirror Langevin dynamics including\nthe Mirror Langevin algorithm have an asymptotic bias. For this algorithm, we\nalso give upper bounds for the number of iterations taken to mix to a\nconstrained distribution whose potential is relatively smooth, convex, and\nLipschitz continuous with respect to a self-concordant mirror function. As a\nconsequence of the reversibility of the Markov chain induced by the inclusion\nof the Metropolis-Hastings filter, we obtain an exponentially better dependence\non the error tolerance for approximate constrained sampling. We also present\nnumerical experiments that corroborate our theoretical findings.\n", "link": "http://arxiv.org/abs/2312.08823v3", "date": "2024-06-21", "relevancy": 1.8443, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4747}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4573}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20sampling%20from%20constrained%20spaces%20using%20the%20Metropolis-adjusted%0A%20%20Mirror%20Langevin%20algorithm&body=Title%3A%20Fast%20sampling%20from%20constrained%20spaces%20using%20the%20Metropolis-adjusted%0A%20%20Mirror%20Langevin%20algorithm%0AAuthor%3A%20Vishwak%20Srinivasan%20and%20Andre%20Wibisono%20and%20Ashia%20Wilson%0AAbstract%3A%20%20%20We%20propose%20a%20new%20method%20called%20the%20Metropolis-adjusted%20Mirror%20Langevin%0Aalgorithm%20for%20approximate%20sampling%20from%20distributions%20whose%20support%20is%20a%0Acompact%20and%20convex%20set.%20This%20algorithm%20adds%20an%20accept-reject%20filter%20to%20the%0AMarkov%20chain%20induced%20by%20a%20single%20step%20of%20the%20Mirror%20Langevin%20algorithm%20%28Zhang%0Aet%20al.%2C%202020%29%2C%20which%20is%20a%20basic%20discretisation%20of%20the%20Mirror%20Langevin%20dynamics.%0ADue%20to%20the%20inclusion%20of%20this%20filter%2C%20our%20method%20is%20unbiased%20relative%20to%20the%0Atarget%2C%20while%20known%20discretisations%20of%20the%20Mirror%20Langevin%20dynamics%20including%0Athe%20Mirror%20Langevin%20algorithm%20have%20an%20asymptotic%20bias.%20For%20this%20algorithm%2C%20we%0Aalso%20give%20upper%20bounds%20for%20the%20number%20of%20iterations%20taken%20to%20mix%20to%20a%0Aconstrained%20distribution%20whose%20potential%20is%20relatively%20smooth%2C%20convex%2C%20and%0ALipschitz%20continuous%20with%20respect%20to%20a%20self-concordant%20mirror%20function.%20As%20a%0Aconsequence%20of%20the%20reversibility%20of%20the%20Markov%20chain%20induced%20by%20the%20inclusion%0Aof%20the%20Metropolis-Hastings%20filter%2C%20we%20obtain%20an%20exponentially%20better%20dependence%0Aon%20the%20error%20tolerance%20for%20approximate%20constrained%20sampling.%20We%20also%20present%0Anumerical%20experiments%20that%20corroborate%20our%20theoretical%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.08823v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520sampling%2520from%2520constrained%2520spaces%2520using%2520the%2520Metropolis-adjusted%250A%2520%2520Mirror%2520Langevin%2520algorithm%26entry.906535625%3DVishwak%2520Srinivasan%2520and%2520Andre%2520Wibisono%2520and%2520Ashia%2520Wilson%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520new%2520method%2520called%2520the%2520Metropolis-adjusted%2520Mirror%2520Langevin%250Aalgorithm%2520for%2520approximate%2520sampling%2520from%2520distributions%2520whose%2520support%2520is%2520a%250Acompact%2520and%2520convex%2520set.%2520This%2520algorithm%2520adds%2520an%2520accept-reject%2520filter%2520to%2520the%250AMarkov%2520chain%2520induced%2520by%2520a%2520single%2520step%2520of%2520the%2520Mirror%2520Langevin%2520algorithm%2520%2528Zhang%250Aet%2520al.%252C%25202020%2529%252C%2520which%2520is%2520a%2520basic%2520discretisation%2520of%2520the%2520Mirror%2520Langevin%2520dynamics.%250ADue%2520to%2520the%2520inclusion%2520of%2520this%2520filter%252C%2520our%2520method%2520is%2520unbiased%2520relative%2520to%2520the%250Atarget%252C%2520while%2520known%2520discretisations%2520of%2520the%2520Mirror%2520Langevin%2520dynamics%2520including%250Athe%2520Mirror%2520Langevin%2520algorithm%2520have%2520an%2520asymptotic%2520bias.%2520For%2520this%2520algorithm%252C%2520we%250Aalso%2520give%2520upper%2520bounds%2520for%2520the%2520number%2520of%2520iterations%2520taken%2520to%2520mix%2520to%2520a%250Aconstrained%2520distribution%2520whose%2520potential%2520is%2520relatively%2520smooth%252C%2520convex%252C%2520and%250ALipschitz%2520continuous%2520with%2520respect%2520to%2520a%2520self-concordant%2520mirror%2520function.%2520As%2520a%250Aconsequence%2520of%2520the%2520reversibility%2520of%2520the%2520Markov%2520chain%2520induced%2520by%2520the%2520inclusion%250Aof%2520the%2520Metropolis-Hastings%2520filter%252C%2520we%2520obtain%2520an%2520exponentially%2520better%2520dependence%250Aon%2520the%2520error%2520tolerance%2520for%2520approximate%2520constrained%2520sampling.%2520We%2520also%2520present%250Anumerical%2520experiments%2520that%2520corroborate%2520our%2520theoretical%2520findings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.08823v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20sampling%20from%20constrained%20spaces%20using%20the%20Metropolis-adjusted%0A%20%20Mirror%20Langevin%20algorithm&entry.906535625=Vishwak%20Srinivasan%20and%20Andre%20Wibisono%20and%20Ashia%20Wilson&entry.1292438233=%20%20We%20propose%20a%20new%20method%20called%20the%20Metropolis-adjusted%20Mirror%20Langevin%0Aalgorithm%20for%20approximate%20sampling%20from%20distributions%20whose%20support%20is%20a%0Acompact%20and%20convex%20set.%20This%20algorithm%20adds%20an%20accept-reject%20filter%20to%20the%0AMarkov%20chain%20induced%20by%20a%20single%20step%20of%20the%20Mirror%20Langevin%20algorithm%20%28Zhang%0Aet%20al.%2C%202020%29%2C%20which%20is%20a%20basic%20discretisation%20of%20the%20Mirror%20Langevin%20dynamics.%0ADue%20to%20the%20inclusion%20of%20this%20filter%2C%20our%20method%20is%20unbiased%20relative%20to%20the%0Atarget%2C%20while%20known%20discretisations%20of%20the%20Mirror%20Langevin%20dynamics%20including%0Athe%20Mirror%20Langevin%20algorithm%20have%20an%20asymptotic%20bias.%20For%20this%20algorithm%2C%20we%0Aalso%20give%20upper%20bounds%20for%20the%20number%20of%20iterations%20taken%20to%20mix%20to%20a%0Aconstrained%20distribution%20whose%20potential%20is%20relatively%20smooth%2C%20convex%2C%20and%0ALipschitz%20continuous%20with%20respect%20to%20a%20self-concordant%20mirror%20function.%20As%20a%0Aconsequence%20of%20the%20reversibility%20of%20the%20Markov%20chain%20induced%20by%20the%20inclusion%0Aof%20the%20Metropolis-Hastings%20filter%2C%20we%20obtain%20an%20exponentially%20better%20dependence%0Aon%20the%20error%20tolerance%20for%20approximate%20constrained%20sampling.%20We%20also%20present%0Anumerical%20experiments%20that%20corroborate%20our%20theoretical%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.08823v3&entry.124074799=Read"},
{"title": "Equivariance via Minimal Frame Averaging for More Symmetries and\n  Efficiency", "author": "Yuchao Lin and Jacob Helwig and Shurui Gui and Shuiwang Ji", "abstract": "  We consider achieving equivariance in machine learning systems via frame\naveraging. Current frame averaging methods involve a costly sum over large\nframes or rely on sampling-based approaches that only yield approximate\nequivariance. Here, we propose Minimal Frame Averaging (MFA), a mathematical\nframework for constructing provably minimal frames that are exactly\nequivariant. The general foundations of MFA also allow us to extend frame\naveraging to more groups than previously considered, including the Lorentz\ngroup for describing symmetries in space-time, and the unitary group for\ncomplex-valued domains. Results demonstrate the efficiency and effectiveness of\nencoding symmetries via MFA across a diverse range of tasks, including $n$-body\nsimulation, top tagging in collider physics, and relaxed energy prediction. Our\ncode is available at https://github.com/divelab/MFA.\n", "link": "http://arxiv.org/abs/2406.07598v4", "date": "2024-06-21", "relevancy": 1.8429, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4674}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4579}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Equivariance%20via%20Minimal%20Frame%20Averaging%20for%20More%20Symmetries%20and%0A%20%20Efficiency&body=Title%3A%20Equivariance%20via%20Minimal%20Frame%20Averaging%20for%20More%20Symmetries%20and%0A%20%20Efficiency%0AAuthor%3A%20Yuchao%20Lin%20and%20Jacob%20Helwig%20and%20Shurui%20Gui%20and%20Shuiwang%20Ji%0AAbstract%3A%20%20%20We%20consider%20achieving%20equivariance%20in%20machine%20learning%20systems%20via%20frame%0Aaveraging.%20Current%20frame%20averaging%20methods%20involve%20a%20costly%20sum%20over%20large%0Aframes%20or%20rely%20on%20sampling-based%20approaches%20that%20only%20yield%20approximate%0Aequivariance.%20Here%2C%20we%20propose%20Minimal%20Frame%20Averaging%20%28MFA%29%2C%20a%20mathematical%0Aframework%20for%20constructing%20provably%20minimal%20frames%20that%20are%20exactly%0Aequivariant.%20The%20general%20foundations%20of%20MFA%20also%20allow%20us%20to%20extend%20frame%0Aaveraging%20to%20more%20groups%20than%20previously%20considered%2C%20including%20the%20Lorentz%0Agroup%20for%20describing%20symmetries%20in%20space-time%2C%20and%20the%20unitary%20group%20for%0Acomplex-valued%20domains.%20Results%20demonstrate%20the%20efficiency%20and%20effectiveness%20of%0Aencoding%20symmetries%20via%20MFA%20across%20a%20diverse%20range%20of%20tasks%2C%20including%20%24n%24-body%0Asimulation%2C%20top%20tagging%20in%20collider%20physics%2C%20and%20relaxed%20energy%20prediction.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/divelab/MFA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07598v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEquivariance%2520via%2520Minimal%2520Frame%2520Averaging%2520for%2520More%2520Symmetries%2520and%250A%2520%2520Efficiency%26entry.906535625%3DYuchao%2520Lin%2520and%2520Jacob%2520Helwig%2520and%2520Shurui%2520Gui%2520and%2520Shuiwang%2520Ji%26entry.1292438233%3D%2520%2520We%2520consider%2520achieving%2520equivariance%2520in%2520machine%2520learning%2520systems%2520via%2520frame%250Aaveraging.%2520Current%2520frame%2520averaging%2520methods%2520involve%2520a%2520costly%2520sum%2520over%2520large%250Aframes%2520or%2520rely%2520on%2520sampling-based%2520approaches%2520that%2520only%2520yield%2520approximate%250Aequivariance.%2520Here%252C%2520we%2520propose%2520Minimal%2520Frame%2520Averaging%2520%2528MFA%2529%252C%2520a%2520mathematical%250Aframework%2520for%2520constructing%2520provably%2520minimal%2520frames%2520that%2520are%2520exactly%250Aequivariant.%2520The%2520general%2520foundations%2520of%2520MFA%2520also%2520allow%2520us%2520to%2520extend%2520frame%250Aaveraging%2520to%2520more%2520groups%2520than%2520previously%2520considered%252C%2520including%2520the%2520Lorentz%250Agroup%2520for%2520describing%2520symmetries%2520in%2520space-time%252C%2520and%2520the%2520unitary%2520group%2520for%250Acomplex-valued%2520domains.%2520Results%2520demonstrate%2520the%2520efficiency%2520and%2520effectiveness%2520of%250Aencoding%2520symmetries%2520via%2520MFA%2520across%2520a%2520diverse%2520range%2520of%2520tasks%252C%2520including%2520%2524n%2524-body%250Asimulation%252C%2520top%2520tagging%2520in%2520collider%2520physics%252C%2520and%2520relaxed%2520energy%2520prediction.%2520Our%250Acode%2520is%2520available%2520at%2520https%253A//github.com/divelab/MFA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07598v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Equivariance%20via%20Minimal%20Frame%20Averaging%20for%20More%20Symmetries%20and%0A%20%20Efficiency&entry.906535625=Yuchao%20Lin%20and%20Jacob%20Helwig%20and%20Shurui%20Gui%20and%20Shuiwang%20Ji&entry.1292438233=%20%20We%20consider%20achieving%20equivariance%20in%20machine%20learning%20systems%20via%20frame%0Aaveraging.%20Current%20frame%20averaging%20methods%20involve%20a%20costly%20sum%20over%20large%0Aframes%20or%20rely%20on%20sampling-based%20approaches%20that%20only%20yield%20approximate%0Aequivariance.%20Here%2C%20we%20propose%20Minimal%20Frame%20Averaging%20%28MFA%29%2C%20a%20mathematical%0Aframework%20for%20constructing%20provably%20minimal%20frames%20that%20are%20exactly%0Aequivariant.%20The%20general%20foundations%20of%20MFA%20also%20allow%20us%20to%20extend%20frame%0Aaveraging%20to%20more%20groups%20than%20previously%20considered%2C%20including%20the%20Lorentz%0Agroup%20for%20describing%20symmetries%20in%20space-time%2C%20and%20the%20unitary%20group%20for%0Acomplex-valued%20domains.%20Results%20demonstrate%20the%20efficiency%20and%20effectiveness%20of%0Aencoding%20symmetries%20via%20MFA%20across%20a%20diverse%20range%20of%20tasks%2C%20including%20%24n%24-body%0Asimulation%2C%20top%20tagging%20in%20collider%20physics%2C%20and%20relaxed%20energy%20prediction.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/divelab/MFA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07598v4&entry.124074799=Read"},
{"title": "Pessimistic asynchronous sampling in high-cost Bayesian optimization", "author": "Amanda A. Volk and Kristofer G. Reyes and Jeffrey G. Ethier and Luke A. Baldwin", "abstract": "  Asynchronous Bayesian optimization is a recently implemented technique that\nallows for parallel operation of experimental systems and disjointed workflows.\nContrasting with serial Bayesian optimization which individually selects\nexperiments one at a time after conducting a measurement for each experiment,\nasynchronous policies sequentially assign multiple experiments before\nmeasurements can be taken and evaluate new measurements continuously as they\nare made available. This technique allows for faster data generation and\ntherefore faster optimization of an experimental space. This work extends the\ncapabilities of asynchronous optimization methods beyond prior studies by\nevaluating four additional policies that incorporate pessimistic predictions in\nthe training data set. Combined with a conventional greedy policy, the five\ntotal policies were evaluated in a simulated environment and benchmarked with\nserial sampling. Under some conditions and parameter space dimensionalities,\nthe pessimistic asynchronous policy reached optimum experimental conditions in\nsignificantly fewer experiments than equivalent serial policies and proved to\nbe less susceptible to convergence onto local optima at higher dimensions.\nWithout accounting for the faster sampling rate, the pessimistic asynchronous\nalgorithm presented in this work could result in more efficient algorithm\ndriven optimization of high-cost experimental spaces. Accounting for sampling\nrate, the presented asynchronous algorithm could allow for faster optimization\nin experimental spaces where multiple experiments can be run before results are\ncollected.\n", "link": "http://arxiv.org/abs/2406.15291v1", "date": "2024-06-21", "relevancy": 1.8411, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5013}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4543}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pessimistic%20asynchronous%20sampling%20in%20high-cost%20Bayesian%20optimization&body=Title%3A%20Pessimistic%20asynchronous%20sampling%20in%20high-cost%20Bayesian%20optimization%0AAuthor%3A%20Amanda%20A.%20Volk%20and%20Kristofer%20G.%20Reyes%20and%20Jeffrey%20G.%20Ethier%20and%20Luke%20A.%20Baldwin%0AAbstract%3A%20%20%20Asynchronous%20Bayesian%20optimization%20is%20a%20recently%20implemented%20technique%20that%0Aallows%20for%20parallel%20operation%20of%20experimental%20systems%20and%20disjointed%20workflows.%0AContrasting%20with%20serial%20Bayesian%20optimization%20which%20individually%20selects%0Aexperiments%20one%20at%20a%20time%20after%20conducting%20a%20measurement%20for%20each%20experiment%2C%0Aasynchronous%20policies%20sequentially%20assign%20multiple%20experiments%20before%0Ameasurements%20can%20be%20taken%20and%20evaluate%20new%20measurements%20continuously%20as%20they%0Aare%20made%20available.%20This%20technique%20allows%20for%20faster%20data%20generation%20and%0Atherefore%20faster%20optimization%20of%20an%20experimental%20space.%20This%20work%20extends%20the%0Acapabilities%20of%20asynchronous%20optimization%20methods%20beyond%20prior%20studies%20by%0Aevaluating%20four%20additional%20policies%20that%20incorporate%20pessimistic%20predictions%20in%0Athe%20training%20data%20set.%20Combined%20with%20a%20conventional%20greedy%20policy%2C%20the%20five%0Atotal%20policies%20were%20evaluated%20in%20a%20simulated%20environment%20and%20benchmarked%20with%0Aserial%20sampling.%20Under%20some%20conditions%20and%20parameter%20space%20dimensionalities%2C%0Athe%20pessimistic%20asynchronous%20policy%20reached%20optimum%20experimental%20conditions%20in%0Asignificantly%20fewer%20experiments%20than%20equivalent%20serial%20policies%20and%20proved%20to%0Abe%20less%20susceptible%20to%20convergence%20onto%20local%20optima%20at%20higher%20dimensions.%0AWithout%20accounting%20for%20the%20faster%20sampling%20rate%2C%20the%20pessimistic%20asynchronous%0Aalgorithm%20presented%20in%20this%20work%20could%20result%20in%20more%20efficient%20algorithm%0Adriven%20optimization%20of%20high-cost%20experimental%20spaces.%20Accounting%20for%20sampling%0Arate%2C%20the%20presented%20asynchronous%20algorithm%20could%20allow%20for%20faster%20optimization%0Ain%20experimental%20spaces%20where%20multiple%20experiments%20can%20be%20run%20before%20results%20are%0Acollected.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15291v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPessimistic%2520asynchronous%2520sampling%2520in%2520high-cost%2520Bayesian%2520optimization%26entry.906535625%3DAmanda%2520A.%2520Volk%2520and%2520Kristofer%2520G.%2520Reyes%2520and%2520Jeffrey%2520G.%2520Ethier%2520and%2520Luke%2520A.%2520Baldwin%26entry.1292438233%3D%2520%2520Asynchronous%2520Bayesian%2520optimization%2520is%2520a%2520recently%2520implemented%2520technique%2520that%250Aallows%2520for%2520parallel%2520operation%2520of%2520experimental%2520systems%2520and%2520disjointed%2520workflows.%250AContrasting%2520with%2520serial%2520Bayesian%2520optimization%2520which%2520individually%2520selects%250Aexperiments%2520one%2520at%2520a%2520time%2520after%2520conducting%2520a%2520measurement%2520for%2520each%2520experiment%252C%250Aasynchronous%2520policies%2520sequentially%2520assign%2520multiple%2520experiments%2520before%250Ameasurements%2520can%2520be%2520taken%2520and%2520evaluate%2520new%2520measurements%2520continuously%2520as%2520they%250Aare%2520made%2520available.%2520This%2520technique%2520allows%2520for%2520faster%2520data%2520generation%2520and%250Atherefore%2520faster%2520optimization%2520of%2520an%2520experimental%2520space.%2520This%2520work%2520extends%2520the%250Acapabilities%2520of%2520asynchronous%2520optimization%2520methods%2520beyond%2520prior%2520studies%2520by%250Aevaluating%2520four%2520additional%2520policies%2520that%2520incorporate%2520pessimistic%2520predictions%2520in%250Athe%2520training%2520data%2520set.%2520Combined%2520with%2520a%2520conventional%2520greedy%2520policy%252C%2520the%2520five%250Atotal%2520policies%2520were%2520evaluated%2520in%2520a%2520simulated%2520environment%2520and%2520benchmarked%2520with%250Aserial%2520sampling.%2520Under%2520some%2520conditions%2520and%2520parameter%2520space%2520dimensionalities%252C%250Athe%2520pessimistic%2520asynchronous%2520policy%2520reached%2520optimum%2520experimental%2520conditions%2520in%250Asignificantly%2520fewer%2520experiments%2520than%2520equivalent%2520serial%2520policies%2520and%2520proved%2520to%250Abe%2520less%2520susceptible%2520to%2520convergence%2520onto%2520local%2520optima%2520at%2520higher%2520dimensions.%250AWithout%2520accounting%2520for%2520the%2520faster%2520sampling%2520rate%252C%2520the%2520pessimistic%2520asynchronous%250Aalgorithm%2520presented%2520in%2520this%2520work%2520could%2520result%2520in%2520more%2520efficient%2520algorithm%250Adriven%2520optimization%2520of%2520high-cost%2520experimental%2520spaces.%2520Accounting%2520for%2520sampling%250Arate%252C%2520the%2520presented%2520asynchronous%2520algorithm%2520could%2520allow%2520for%2520faster%2520optimization%250Ain%2520experimental%2520spaces%2520where%2520multiple%2520experiments%2520can%2520be%2520run%2520before%2520results%2520are%250Acollected.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15291v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pessimistic%20asynchronous%20sampling%20in%20high-cost%20Bayesian%20optimization&entry.906535625=Amanda%20A.%20Volk%20and%20Kristofer%20G.%20Reyes%20and%20Jeffrey%20G.%20Ethier%20and%20Luke%20A.%20Baldwin&entry.1292438233=%20%20Asynchronous%20Bayesian%20optimization%20is%20a%20recently%20implemented%20technique%20that%0Aallows%20for%20parallel%20operation%20of%20experimental%20systems%20and%20disjointed%20workflows.%0AContrasting%20with%20serial%20Bayesian%20optimization%20which%20individually%20selects%0Aexperiments%20one%20at%20a%20time%20after%20conducting%20a%20measurement%20for%20each%20experiment%2C%0Aasynchronous%20policies%20sequentially%20assign%20multiple%20experiments%20before%0Ameasurements%20can%20be%20taken%20and%20evaluate%20new%20measurements%20continuously%20as%20they%0Aare%20made%20available.%20This%20technique%20allows%20for%20faster%20data%20generation%20and%0Atherefore%20faster%20optimization%20of%20an%20experimental%20space.%20This%20work%20extends%20the%0Acapabilities%20of%20asynchronous%20optimization%20methods%20beyond%20prior%20studies%20by%0Aevaluating%20four%20additional%20policies%20that%20incorporate%20pessimistic%20predictions%20in%0Athe%20training%20data%20set.%20Combined%20with%20a%20conventional%20greedy%20policy%2C%20the%20five%0Atotal%20policies%20were%20evaluated%20in%20a%20simulated%20environment%20and%20benchmarked%20with%0Aserial%20sampling.%20Under%20some%20conditions%20and%20parameter%20space%20dimensionalities%2C%0Athe%20pessimistic%20asynchronous%20policy%20reached%20optimum%20experimental%20conditions%20in%0Asignificantly%20fewer%20experiments%20than%20equivalent%20serial%20policies%20and%20proved%20to%0Abe%20less%20susceptible%20to%20convergence%20onto%20local%20optima%20at%20higher%20dimensions.%0AWithout%20accounting%20for%20the%20faster%20sampling%20rate%2C%20the%20pessimistic%20asynchronous%0Aalgorithm%20presented%20in%20this%20work%20could%20result%20in%20more%20efficient%20algorithm%0Adriven%20optimization%20of%20high-cost%20experimental%20spaces.%20Accounting%20for%20sampling%0Arate%2C%20the%20presented%20asynchronous%20algorithm%20could%20allow%20for%20faster%20optimization%0Ain%20experimental%20spaces%20where%20multiple%20experiments%20can%20be%20run%20before%20results%20are%0Acollected.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15291v1&entry.124074799=Read"},
{"title": "Are LLMs Naturally Good at Synthetic Tabular Data Generation?", "author": "Shengzhe Xu and Cho-Ting Lee and Mandar Sharma and Raquib Bin Yousuf and Nikhil Muralidhar and Naren Ramakrishnan", "abstract": "  Large language models (LLMs) have demonstrated their prowess in generating\nsynthetic text and images; however, their potential for generating tabular data\n-- arguably the most common data type in business and scientific applications\n-- is largely underexplored. This paper demonstrates that LLMs, used as-is, or\nafter traditional fine-tuning, are severely inadequate as synthetic table\ngenerators. Due to the autoregressive nature of LLMs, fine-tuning with random\norder permutation runs counter to the importance of modeling functional\ndependencies, and renders LLMs unable to model conditional mixtures of\ndistributions (key to capturing real world constraints). We showcase how LLMs\ncan be made to overcome some of these deficiencies by making them\npermutation-aware.\n", "link": "http://arxiv.org/abs/2406.14541v2", "date": "2024-06-21", "relevancy": 1.8382, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4771}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4613}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20LLMs%20Naturally%20Good%20at%20Synthetic%20Tabular%20Data%20Generation%3F&body=Title%3A%20Are%20LLMs%20Naturally%20Good%20at%20Synthetic%20Tabular%20Data%20Generation%3F%0AAuthor%3A%20Shengzhe%20Xu%20and%20Cho-Ting%20Lee%20and%20Mandar%20Sharma%20and%20Raquib%20Bin%20Yousuf%20and%20Nikhil%20Muralidhar%20and%20Naren%20Ramakrishnan%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20their%20prowess%20in%20generating%0Asynthetic%20text%20and%20images%3B%20however%2C%20their%20potential%20for%20generating%20tabular%20data%0A--%20arguably%20the%20most%20common%20data%20type%20in%20business%20and%20scientific%20applications%0A--%20is%20largely%20underexplored.%20This%20paper%20demonstrates%20that%20LLMs%2C%20used%20as-is%2C%20or%0Aafter%20traditional%20fine-tuning%2C%20are%20severely%20inadequate%20as%20synthetic%20table%0Agenerators.%20Due%20to%20the%20autoregressive%20nature%20of%20LLMs%2C%20fine-tuning%20with%20random%0Aorder%20permutation%20runs%20counter%20to%20the%20importance%20of%20modeling%20functional%0Adependencies%2C%20and%20renders%20LLMs%20unable%20to%20model%20conditional%20mixtures%20of%0Adistributions%20%28key%20to%20capturing%20real%20world%20constraints%29.%20We%20showcase%20how%20LLMs%0Acan%20be%20made%20to%20overcome%20some%20of%20these%20deficiencies%20by%20making%20them%0Apermutation-aware.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14541v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520LLMs%2520Naturally%2520Good%2520at%2520Synthetic%2520Tabular%2520Data%2520Generation%253F%26entry.906535625%3DShengzhe%2520Xu%2520and%2520Cho-Ting%2520Lee%2520and%2520Mandar%2520Sharma%2520and%2520Raquib%2520Bin%2520Yousuf%2520and%2520Nikhil%2520Muralidhar%2520and%2520Naren%2520Ramakrishnan%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520their%2520prowess%2520in%2520generating%250Asynthetic%2520text%2520and%2520images%253B%2520however%252C%2520their%2520potential%2520for%2520generating%2520tabular%2520data%250A--%2520arguably%2520the%2520most%2520common%2520data%2520type%2520in%2520business%2520and%2520scientific%2520applications%250A--%2520is%2520largely%2520underexplored.%2520This%2520paper%2520demonstrates%2520that%2520LLMs%252C%2520used%2520as-is%252C%2520or%250Aafter%2520traditional%2520fine-tuning%252C%2520are%2520severely%2520inadequate%2520as%2520synthetic%2520table%250Agenerators.%2520Due%2520to%2520the%2520autoregressive%2520nature%2520of%2520LLMs%252C%2520fine-tuning%2520with%2520random%250Aorder%2520permutation%2520runs%2520counter%2520to%2520the%2520importance%2520of%2520modeling%2520functional%250Adependencies%252C%2520and%2520renders%2520LLMs%2520unable%2520to%2520model%2520conditional%2520mixtures%2520of%250Adistributions%2520%2528key%2520to%2520capturing%2520real%2520world%2520constraints%2529.%2520We%2520showcase%2520how%2520LLMs%250Acan%2520be%2520made%2520to%2520overcome%2520some%2520of%2520these%2520deficiencies%2520by%2520making%2520them%250Apermutation-aware.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14541v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20LLMs%20Naturally%20Good%20at%20Synthetic%20Tabular%20Data%20Generation%3F&entry.906535625=Shengzhe%20Xu%20and%20Cho-Ting%20Lee%20and%20Mandar%20Sharma%20and%20Raquib%20Bin%20Yousuf%20and%20Nikhil%20Muralidhar%20and%20Naren%20Ramakrishnan&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20their%20prowess%20in%20generating%0Asynthetic%20text%20and%20images%3B%20however%2C%20their%20potential%20for%20generating%20tabular%20data%0A--%20arguably%20the%20most%20common%20data%20type%20in%20business%20and%20scientific%20applications%0A--%20is%20largely%20underexplored.%20This%20paper%20demonstrates%20that%20LLMs%2C%20used%20as-is%2C%20or%0Aafter%20traditional%20fine-tuning%2C%20are%20severely%20inadequate%20as%20synthetic%20table%0Agenerators.%20Due%20to%20the%20autoregressive%20nature%20of%20LLMs%2C%20fine-tuning%20with%20random%0Aorder%20permutation%20runs%20counter%20to%20the%20importance%20of%20modeling%20functional%0Adependencies%2C%20and%20renders%20LLMs%20unable%20to%20model%20conditional%20mixtures%20of%0Adistributions%20%28key%20to%20capturing%20real%20world%20constraints%29.%20We%20showcase%20how%20LLMs%0Acan%20be%20made%20to%20overcome%20some%20of%20these%20deficiencies%20by%20making%20them%0Apermutation-aware.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14541v2&entry.124074799=Read"},
{"title": "Reinforcement-Learning based routing for packet-optical networks with\n  hybrid telemetry", "author": "A. L. Garc\u00eda Navarro and Nataliia Koneva and Alfonso S\u00e1nchez-Maci\u00e1n and Jos\u00e9 Alberto Hern\u00e1ndez and \u00d3scar Gonz\u00e1lez de Dios and J. M. Rivas-Moscoso", "abstract": "  This article provides a methodology and open-source implementation of\nReinforcement Learning algorithms for finding optimal routes in a\npacket-optical network scenario. The algorithm uses measurements provided by\nthe physical layer (pre-FEC bit error rate and propagation delay) and the link\nlayer (link load) to configure a set of latency-based rewards and penalties\nbased on such measurements. Then, the algorithm executes Q-learning based on\nthis set of rewards for finding the optimal routing strategies. It is further\nshown that the algorithm dynamically adapts to changing network conditions by\nre-calculating optimal policies upon either link load changes or link\ndegradation as measured by pre-FEC BER.\n", "link": "http://arxiv.org/abs/2406.12602v2", "date": "2024-06-21", "relevancy": 1.8264, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.462}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.453}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcement-Learning%20based%20routing%20for%20packet-optical%20networks%20with%0A%20%20hybrid%20telemetry&body=Title%3A%20Reinforcement-Learning%20based%20routing%20for%20packet-optical%20networks%20with%0A%20%20hybrid%20telemetry%0AAuthor%3A%20A.%20L.%20Garc%C3%ADa%20Navarro%20and%20Nataliia%20Koneva%20and%20Alfonso%20S%C3%A1nchez-Maci%C3%A1n%20and%20Jos%C3%A9%20Alberto%20Hern%C3%A1ndez%20and%20%C3%93scar%20Gonz%C3%A1lez%20de%20Dios%20and%20J.%20M.%20Rivas-Moscoso%0AAbstract%3A%20%20%20This%20article%20provides%20a%20methodology%20and%20open-source%20implementation%20of%0AReinforcement%20Learning%20algorithms%20for%20finding%20optimal%20routes%20in%20a%0Apacket-optical%20network%20scenario.%20The%20algorithm%20uses%20measurements%20provided%20by%0Athe%20physical%20layer%20%28pre-FEC%20bit%20error%20rate%20and%20propagation%20delay%29%20and%20the%20link%0Alayer%20%28link%20load%29%20to%20configure%20a%20set%20of%20latency-based%20rewards%20and%20penalties%0Abased%20on%20such%20measurements.%20Then%2C%20the%20algorithm%20executes%20Q-learning%20based%20on%0Athis%20set%20of%20rewards%20for%20finding%20the%20optimal%20routing%20strategies.%20It%20is%20further%0Ashown%20that%20the%20algorithm%20dynamically%20adapts%20to%20changing%20network%20conditions%20by%0Are-calculating%20optimal%20policies%20upon%20either%20link%20load%20changes%20or%20link%0Adegradation%20as%20measured%20by%20pre-FEC%20BER.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12602v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcement-Learning%2520based%2520routing%2520for%2520packet-optical%2520networks%2520with%250A%2520%2520hybrid%2520telemetry%26entry.906535625%3DA.%2520L.%2520Garc%25C3%25ADa%2520Navarro%2520and%2520Nataliia%2520Koneva%2520and%2520Alfonso%2520S%25C3%25A1nchez-Maci%25C3%25A1n%2520and%2520Jos%25C3%25A9%2520Alberto%2520Hern%25C3%25A1ndez%2520and%2520%25C3%2593scar%2520Gonz%25C3%25A1lez%2520de%2520Dios%2520and%2520J.%2520M.%2520Rivas-Moscoso%26entry.1292438233%3D%2520%2520This%2520article%2520provides%2520a%2520methodology%2520and%2520open-source%2520implementation%2520of%250AReinforcement%2520Learning%2520algorithms%2520for%2520finding%2520optimal%2520routes%2520in%2520a%250Apacket-optical%2520network%2520scenario.%2520The%2520algorithm%2520uses%2520measurements%2520provided%2520by%250Athe%2520physical%2520layer%2520%2528pre-FEC%2520bit%2520error%2520rate%2520and%2520propagation%2520delay%2529%2520and%2520the%2520link%250Alayer%2520%2528link%2520load%2529%2520to%2520configure%2520a%2520set%2520of%2520latency-based%2520rewards%2520and%2520penalties%250Abased%2520on%2520such%2520measurements.%2520Then%252C%2520the%2520algorithm%2520executes%2520Q-learning%2520based%2520on%250Athis%2520set%2520of%2520rewards%2520for%2520finding%2520the%2520optimal%2520routing%2520strategies.%2520It%2520is%2520further%250Ashown%2520that%2520the%2520algorithm%2520dynamically%2520adapts%2520to%2520changing%2520network%2520conditions%2520by%250Are-calculating%2520optimal%2520policies%2520upon%2520either%2520link%2520load%2520changes%2520or%2520link%250Adegradation%2520as%2520measured%2520by%2520pre-FEC%2520BER.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12602v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcement-Learning%20based%20routing%20for%20packet-optical%20networks%20with%0A%20%20hybrid%20telemetry&entry.906535625=A.%20L.%20Garc%C3%ADa%20Navarro%20and%20Nataliia%20Koneva%20and%20Alfonso%20S%C3%A1nchez-Maci%C3%A1n%20and%20Jos%C3%A9%20Alberto%20Hern%C3%A1ndez%20and%20%C3%93scar%20Gonz%C3%A1lez%20de%20Dios%20and%20J.%20M.%20Rivas-Moscoso&entry.1292438233=%20%20This%20article%20provides%20a%20methodology%20and%20open-source%20implementation%20of%0AReinforcement%20Learning%20algorithms%20for%20finding%20optimal%20routes%20in%20a%0Apacket-optical%20network%20scenario.%20The%20algorithm%20uses%20measurements%20provided%20by%0Athe%20physical%20layer%20%28pre-FEC%20bit%20error%20rate%20and%20propagation%20delay%29%20and%20the%20link%0Alayer%20%28link%20load%29%20to%20configure%20a%20set%20of%20latency-based%20rewards%20and%20penalties%0Abased%20on%20such%20measurements.%20Then%2C%20the%20algorithm%20executes%20Q-learning%20based%20on%0Athis%20set%20of%20rewards%20for%20finding%20the%20optimal%20routing%20strategies.%20It%20is%20further%0Ashown%20that%20the%20algorithm%20dynamically%20adapts%20to%20changing%20network%20conditions%20by%0Are-calculating%20optimal%20policies%20upon%20either%20link%20load%20changes%20or%20link%0Adegradation%20as%20measured%20by%20pre-FEC%20BER.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12602v2&entry.124074799=Read"},
{"title": "Transferability of Graph Neural Networks using Graphon and Sampling\n  Theories", "author": "A. Martina Neuman and Jason J. Bramburger", "abstract": "  Graph neural networks (GNNs) have become powerful tools for processing\ngraph-based information in various domains. A desirable property of GNNs is\ntransferability, where a trained network can swap in information from a\ndifferent graph without retraining and retain its accuracy. A recent method of\ncapturing transferability of GNNs is through the use of graphons, which are\nsymmetric, measurable functions representing the limit of large dense graphs.\nIn this work, we contribute to the application of graphons to GNNs by\npresenting an explicit two-layer graphon neural network (WNN) architecture. We\nprove its ability to approximate bandlimited graphon signals within a specified\nerror tolerance using a minimal number of network weights. We then leverage\nthis result, to establish the transferability of an explicit two-layer GNN over\nall sufficiently large graphs in a convergent sequence. Our work addresses\ntransferability between both deterministic weighted graphs and simple random\ngraphs and overcomes issues related to the curse of dimensionality that arise\nin other GNN results. The proposed WNN and GNN architectures offer practical\nsolutions for handling graph data of varying sizes while maintaining\nperformance guarantees without extensive retraining.\n", "link": "http://arxiv.org/abs/2307.13206v2", "date": "2024-06-21", "relevancy": 1.8103, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4569}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4506}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transferability%20of%20Graph%20Neural%20Networks%20using%20Graphon%20and%20Sampling%0A%20%20Theories&body=Title%3A%20Transferability%20of%20Graph%20Neural%20Networks%20using%20Graphon%20and%20Sampling%0A%20%20Theories%0AAuthor%3A%20A.%20Martina%20Neuman%20and%20Jason%20J.%20Bramburger%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20become%20powerful%20tools%20for%20processing%0Agraph-based%20information%20in%20various%20domains.%20A%20desirable%20property%20of%20GNNs%20is%0Atransferability%2C%20where%20a%20trained%20network%20can%20swap%20in%20information%20from%20a%0Adifferent%20graph%20without%20retraining%20and%20retain%20its%20accuracy.%20A%20recent%20method%20of%0Acapturing%20transferability%20of%20GNNs%20is%20through%20the%20use%20of%20graphons%2C%20which%20are%0Asymmetric%2C%20measurable%20functions%20representing%20the%20limit%20of%20large%20dense%20graphs.%0AIn%20this%20work%2C%20we%20contribute%20to%20the%20application%20of%20graphons%20to%20GNNs%20by%0Apresenting%20an%20explicit%20two-layer%20graphon%20neural%20network%20%28WNN%29%20architecture.%20We%0Aprove%20its%20ability%20to%20approximate%20bandlimited%20graphon%20signals%20within%20a%20specified%0Aerror%20tolerance%20using%20a%20minimal%20number%20of%20network%20weights.%20We%20then%20leverage%0Athis%20result%2C%20to%20establish%20the%20transferability%20of%20an%20explicit%20two-layer%20GNN%20over%0Aall%20sufficiently%20large%20graphs%20in%20a%20convergent%20sequence.%20Our%20work%20addresses%0Atransferability%20between%20both%20deterministic%20weighted%20graphs%20and%20simple%20random%0Agraphs%20and%20overcomes%20issues%20related%20to%20the%20curse%20of%20dimensionality%20that%20arise%0Ain%20other%20GNN%20results.%20The%20proposed%20WNN%20and%20GNN%20architectures%20offer%20practical%0Asolutions%20for%20handling%20graph%20data%20of%20varying%20sizes%20while%20maintaining%0Aperformance%20guarantees%20without%20extensive%20retraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.13206v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransferability%2520of%2520Graph%2520Neural%2520Networks%2520using%2520Graphon%2520and%2520Sampling%250A%2520%2520Theories%26entry.906535625%3DA.%2520Martina%2520Neuman%2520and%2520Jason%2520J.%2520Bramburger%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520have%2520become%2520powerful%2520tools%2520for%2520processing%250Agraph-based%2520information%2520in%2520various%2520domains.%2520A%2520desirable%2520property%2520of%2520GNNs%2520is%250Atransferability%252C%2520where%2520a%2520trained%2520network%2520can%2520swap%2520in%2520information%2520from%2520a%250Adifferent%2520graph%2520without%2520retraining%2520and%2520retain%2520its%2520accuracy.%2520A%2520recent%2520method%2520of%250Acapturing%2520transferability%2520of%2520GNNs%2520is%2520through%2520the%2520use%2520of%2520graphons%252C%2520which%2520are%250Asymmetric%252C%2520measurable%2520functions%2520representing%2520the%2520limit%2520of%2520large%2520dense%2520graphs.%250AIn%2520this%2520work%252C%2520we%2520contribute%2520to%2520the%2520application%2520of%2520graphons%2520to%2520GNNs%2520by%250Apresenting%2520an%2520explicit%2520two-layer%2520graphon%2520neural%2520network%2520%2528WNN%2529%2520architecture.%2520We%250Aprove%2520its%2520ability%2520to%2520approximate%2520bandlimited%2520graphon%2520signals%2520within%2520a%2520specified%250Aerror%2520tolerance%2520using%2520a%2520minimal%2520number%2520of%2520network%2520weights.%2520We%2520then%2520leverage%250Athis%2520result%252C%2520to%2520establish%2520the%2520transferability%2520of%2520an%2520explicit%2520two-layer%2520GNN%2520over%250Aall%2520sufficiently%2520large%2520graphs%2520in%2520a%2520convergent%2520sequence.%2520Our%2520work%2520addresses%250Atransferability%2520between%2520both%2520deterministic%2520weighted%2520graphs%2520and%2520simple%2520random%250Agraphs%2520and%2520overcomes%2520issues%2520related%2520to%2520the%2520curse%2520of%2520dimensionality%2520that%2520arise%250Ain%2520other%2520GNN%2520results.%2520The%2520proposed%2520WNN%2520and%2520GNN%2520architectures%2520offer%2520practical%250Asolutions%2520for%2520handling%2520graph%2520data%2520of%2520varying%2520sizes%2520while%2520maintaining%250Aperformance%2520guarantees%2520without%2520extensive%2520retraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.13206v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transferability%20of%20Graph%20Neural%20Networks%20using%20Graphon%20and%20Sampling%0A%20%20Theories&entry.906535625=A.%20Martina%20Neuman%20and%20Jason%20J.%20Bramburger&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20become%20powerful%20tools%20for%20processing%0Agraph-based%20information%20in%20various%20domains.%20A%20desirable%20property%20of%20GNNs%20is%0Atransferability%2C%20where%20a%20trained%20network%20can%20swap%20in%20information%20from%20a%0Adifferent%20graph%20without%20retraining%20and%20retain%20its%20accuracy.%20A%20recent%20method%20of%0Acapturing%20transferability%20of%20GNNs%20is%20through%20the%20use%20of%20graphons%2C%20which%20are%0Asymmetric%2C%20measurable%20functions%20representing%20the%20limit%20of%20large%20dense%20graphs.%0AIn%20this%20work%2C%20we%20contribute%20to%20the%20application%20of%20graphons%20to%20GNNs%20by%0Apresenting%20an%20explicit%20two-layer%20graphon%20neural%20network%20%28WNN%29%20architecture.%20We%0Aprove%20its%20ability%20to%20approximate%20bandlimited%20graphon%20signals%20within%20a%20specified%0Aerror%20tolerance%20using%20a%20minimal%20number%20of%20network%20weights.%20We%20then%20leverage%0Athis%20result%2C%20to%20establish%20the%20transferability%20of%20an%20explicit%20two-layer%20GNN%20over%0Aall%20sufficiently%20large%20graphs%20in%20a%20convergent%20sequence.%20Our%20work%20addresses%0Atransferability%20between%20both%20deterministic%20weighted%20graphs%20and%20simple%20random%0Agraphs%20and%20overcomes%20issues%20related%20to%20the%20curse%20of%20dimensionality%20that%20arise%0Ain%20other%20GNN%20results.%20The%20proposed%20WNN%20and%20GNN%20architectures%20offer%20practical%0Asolutions%20for%20handling%20graph%20data%20of%20varying%20sizes%20while%20maintaining%0Aperformance%20guarantees%20without%20extensive%20retraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.13206v2&entry.124074799=Read"},
{"title": "Directly Fine-Tuning Diffusion Models on Differentiable Rewards", "author": "Kevin Clark and Paul Vicol and Kevin Swersky and David J Fleet", "abstract": "  We present Direct Reward Fine-Tuning (DRaFT), a simple and effective method\nfor fine-tuning diffusion models to maximize differentiable reward functions,\nsuch as scores from human preference models. We first show that it is possible\nto backpropagate the reward function gradient through the full sampling\nprocedure, and that doing so achieves strong performance on a variety of\nrewards, outperforming reinforcement learning-based approaches. We then propose\nmore efficient variants of DRaFT: DRaFT-K, which truncates backpropagation to\nonly the last K steps of sampling, and DRaFT-LV, which obtains lower-variance\ngradient estimates for the case when K=1. We show that our methods work well\nfor a variety of reward functions and can be used to substantially improve the\naesthetic quality of images generated by Stable Diffusion 1.4. Finally, we draw\nconnections between our approach and prior work, providing a unifying\nperspective on the design space of gradient-based fine-tuning algorithms.\n", "link": "http://arxiv.org/abs/2309.17400v2", "date": "2024-06-21", "relevancy": 1.7994, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6085}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5935}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5844}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Directly%20Fine-Tuning%20Diffusion%20Models%20on%20Differentiable%20Rewards&body=Title%3A%20Directly%20Fine-Tuning%20Diffusion%20Models%20on%20Differentiable%20Rewards%0AAuthor%3A%20Kevin%20Clark%20and%20Paul%20Vicol%20and%20Kevin%20Swersky%20and%20David%20J%20Fleet%0AAbstract%3A%20%20%20We%20present%20Direct%20Reward%20Fine-Tuning%20%28DRaFT%29%2C%20a%20simple%20and%20effective%20method%0Afor%20fine-tuning%20diffusion%20models%20to%20maximize%20differentiable%20reward%20functions%2C%0Asuch%20as%20scores%20from%20human%20preference%20models.%20We%20first%20show%20that%20it%20is%20possible%0Ato%20backpropagate%20the%20reward%20function%20gradient%20through%20the%20full%20sampling%0Aprocedure%2C%20and%20that%20doing%20so%20achieves%20strong%20performance%20on%20a%20variety%20of%0Arewards%2C%20outperforming%20reinforcement%20learning-based%20approaches.%20We%20then%20propose%0Amore%20efficient%20variants%20of%20DRaFT%3A%20DRaFT-K%2C%20which%20truncates%20backpropagation%20to%0Aonly%20the%20last%20K%20steps%20of%20sampling%2C%20and%20DRaFT-LV%2C%20which%20obtains%20lower-variance%0Agradient%20estimates%20for%20the%20case%20when%20K%3D1.%20We%20show%20that%20our%20methods%20work%20well%0Afor%20a%20variety%20of%20reward%20functions%20and%20can%20be%20used%20to%20substantially%20improve%20the%0Aaesthetic%20quality%20of%20images%20generated%20by%20Stable%20Diffusion%201.4.%20Finally%2C%20we%20draw%0Aconnections%20between%20our%20approach%20and%20prior%20work%2C%20providing%20a%20unifying%0Aperspective%20on%20the%20design%20space%20of%20gradient-based%20fine-tuning%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.17400v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDirectly%2520Fine-Tuning%2520Diffusion%2520Models%2520on%2520Differentiable%2520Rewards%26entry.906535625%3DKevin%2520Clark%2520and%2520Paul%2520Vicol%2520and%2520Kevin%2520Swersky%2520and%2520David%2520J%2520Fleet%26entry.1292438233%3D%2520%2520We%2520present%2520Direct%2520Reward%2520Fine-Tuning%2520%2528DRaFT%2529%252C%2520a%2520simple%2520and%2520effective%2520method%250Afor%2520fine-tuning%2520diffusion%2520models%2520to%2520maximize%2520differentiable%2520reward%2520functions%252C%250Asuch%2520as%2520scores%2520from%2520human%2520preference%2520models.%2520We%2520first%2520show%2520that%2520it%2520is%2520possible%250Ato%2520backpropagate%2520the%2520reward%2520function%2520gradient%2520through%2520the%2520full%2520sampling%250Aprocedure%252C%2520and%2520that%2520doing%2520so%2520achieves%2520strong%2520performance%2520on%2520a%2520variety%2520of%250Arewards%252C%2520outperforming%2520reinforcement%2520learning-based%2520approaches.%2520We%2520then%2520propose%250Amore%2520efficient%2520variants%2520of%2520DRaFT%253A%2520DRaFT-K%252C%2520which%2520truncates%2520backpropagation%2520to%250Aonly%2520the%2520last%2520K%2520steps%2520of%2520sampling%252C%2520and%2520DRaFT-LV%252C%2520which%2520obtains%2520lower-variance%250Agradient%2520estimates%2520for%2520the%2520case%2520when%2520K%253D1.%2520We%2520show%2520that%2520our%2520methods%2520work%2520well%250Afor%2520a%2520variety%2520of%2520reward%2520functions%2520and%2520can%2520be%2520used%2520to%2520substantially%2520improve%2520the%250Aaesthetic%2520quality%2520of%2520images%2520generated%2520by%2520Stable%2520Diffusion%25201.4.%2520Finally%252C%2520we%2520draw%250Aconnections%2520between%2520our%2520approach%2520and%2520prior%2520work%252C%2520providing%2520a%2520unifying%250Aperspective%2520on%2520the%2520design%2520space%2520of%2520gradient-based%2520fine-tuning%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.17400v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Directly%20Fine-Tuning%20Diffusion%20Models%20on%20Differentiable%20Rewards&entry.906535625=Kevin%20Clark%20and%20Paul%20Vicol%20and%20Kevin%20Swersky%20and%20David%20J%20Fleet&entry.1292438233=%20%20We%20present%20Direct%20Reward%20Fine-Tuning%20%28DRaFT%29%2C%20a%20simple%20and%20effective%20method%0Afor%20fine-tuning%20diffusion%20models%20to%20maximize%20differentiable%20reward%20functions%2C%0Asuch%20as%20scores%20from%20human%20preference%20models.%20We%20first%20show%20that%20it%20is%20possible%0Ato%20backpropagate%20the%20reward%20function%20gradient%20through%20the%20full%20sampling%0Aprocedure%2C%20and%20that%20doing%20so%20achieves%20strong%20performance%20on%20a%20variety%20of%0Arewards%2C%20outperforming%20reinforcement%20learning-based%20approaches.%20We%20then%20propose%0Amore%20efficient%20variants%20of%20DRaFT%3A%20DRaFT-K%2C%20which%20truncates%20backpropagation%20to%0Aonly%20the%20last%20K%20steps%20of%20sampling%2C%20and%20DRaFT-LV%2C%20which%20obtains%20lower-variance%0Agradient%20estimates%20for%20the%20case%20when%20K%3D1.%20We%20show%20that%20our%20methods%20work%20well%0Afor%20a%20variety%20of%20reward%20functions%20and%20can%20be%20used%20to%20substantially%20improve%20the%0Aaesthetic%20quality%20of%20images%20generated%20by%20Stable%20Diffusion%201.4.%20Finally%2C%20we%20draw%0Aconnections%20between%20our%20approach%20and%20prior%20work%2C%20providing%20a%20unifying%0Aperspective%20on%20the%20design%20space%20of%20gradient-based%20fine-tuning%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.17400v2&entry.124074799=Read"},
{"title": "Incentivizing High-Quality Content in Online Recommender Systems", "author": "Xinyan Hu and Meena Jagadeesan and Michael I. Jordan and Jacob Steinhardt", "abstract": "  In content recommender systems such as TikTok and YouTube, the platform's\nrecommendation algorithm shapes content producer incentives. Many platforms\nemploy online learning, which generates intertemporal incentives, since content\nproduced today affects recommendations of future content. We study the game\nbetween producers and analyze the content created at equilibrium. We show that\nstandard online learning algorithms, such as Hedge and EXP3, unfortunately\nincentivize producers to create low-quality content, where producers' effort\napproaches zero in the long run for typical learning rate schedules. Motivated\nby this negative result, we design learning algorithms that incentivize\nproducers to invest high effort and achieve high user welfare. At a conceptual\nlevel, our work illustrates the unintended impact that a platform's learning\nalgorithm can have on content quality and introduces algorithmic approaches to\nmitigating these effects.\n", "link": "http://arxiv.org/abs/2306.07479v3", "date": "2024-06-21", "relevancy": 1.7992, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4612}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4554}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4396}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Incentivizing%20High-Quality%20Content%20in%20Online%20Recommender%20Systems&body=Title%3A%20Incentivizing%20High-Quality%20Content%20in%20Online%20Recommender%20Systems%0AAuthor%3A%20Xinyan%20Hu%20and%20Meena%20Jagadeesan%20and%20Michael%20I.%20Jordan%20and%20Jacob%20Steinhardt%0AAbstract%3A%20%20%20In%20content%20recommender%20systems%20such%20as%20TikTok%20and%20YouTube%2C%20the%20platform%27s%0Arecommendation%20algorithm%20shapes%20content%20producer%20incentives.%20Many%20platforms%0Aemploy%20online%20learning%2C%20which%20generates%20intertemporal%20incentives%2C%20since%20content%0Aproduced%20today%20affects%20recommendations%20of%20future%20content.%20We%20study%20the%20game%0Abetween%20producers%20and%20analyze%20the%20content%20created%20at%20equilibrium.%20We%20show%20that%0Astandard%20online%20learning%20algorithms%2C%20such%20as%20Hedge%20and%20EXP3%2C%20unfortunately%0Aincentivize%20producers%20to%20create%20low-quality%20content%2C%20where%20producers%27%20effort%0Aapproaches%20zero%20in%20the%20long%20run%20for%20typical%20learning%20rate%20schedules.%20Motivated%0Aby%20this%20negative%20result%2C%20we%20design%20learning%20algorithms%20that%20incentivize%0Aproducers%20to%20invest%20high%20effort%20and%20achieve%20high%20user%20welfare.%20At%20a%20conceptual%0Alevel%2C%20our%20work%20illustrates%20the%20unintended%20impact%20that%20a%20platform%27s%20learning%0Aalgorithm%20can%20have%20on%20content%20quality%20and%20introduces%20algorithmic%20approaches%20to%0Amitigating%20these%20effects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.07479v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncentivizing%2520High-Quality%2520Content%2520in%2520Online%2520Recommender%2520Systems%26entry.906535625%3DXinyan%2520Hu%2520and%2520Meena%2520Jagadeesan%2520and%2520Michael%2520I.%2520Jordan%2520and%2520Jacob%2520Steinhardt%26entry.1292438233%3D%2520%2520In%2520content%2520recommender%2520systems%2520such%2520as%2520TikTok%2520and%2520YouTube%252C%2520the%2520platform%2527s%250Arecommendation%2520algorithm%2520shapes%2520content%2520producer%2520incentives.%2520Many%2520platforms%250Aemploy%2520online%2520learning%252C%2520which%2520generates%2520intertemporal%2520incentives%252C%2520since%2520content%250Aproduced%2520today%2520affects%2520recommendations%2520of%2520future%2520content.%2520We%2520study%2520the%2520game%250Abetween%2520producers%2520and%2520analyze%2520the%2520content%2520created%2520at%2520equilibrium.%2520We%2520show%2520that%250Astandard%2520online%2520learning%2520algorithms%252C%2520such%2520as%2520Hedge%2520and%2520EXP3%252C%2520unfortunately%250Aincentivize%2520producers%2520to%2520create%2520low-quality%2520content%252C%2520where%2520producers%2527%2520effort%250Aapproaches%2520zero%2520in%2520the%2520long%2520run%2520for%2520typical%2520learning%2520rate%2520schedules.%2520Motivated%250Aby%2520this%2520negative%2520result%252C%2520we%2520design%2520learning%2520algorithms%2520that%2520incentivize%250Aproducers%2520to%2520invest%2520high%2520effort%2520and%2520achieve%2520high%2520user%2520welfare.%2520At%2520a%2520conceptual%250Alevel%252C%2520our%2520work%2520illustrates%2520the%2520unintended%2520impact%2520that%2520a%2520platform%2527s%2520learning%250Aalgorithm%2520can%2520have%2520on%2520content%2520quality%2520and%2520introduces%2520algorithmic%2520approaches%2520to%250Amitigating%2520these%2520effects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.07479v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Incentivizing%20High-Quality%20Content%20in%20Online%20Recommender%20Systems&entry.906535625=Xinyan%20Hu%20and%20Meena%20Jagadeesan%20and%20Michael%20I.%20Jordan%20and%20Jacob%20Steinhardt&entry.1292438233=%20%20In%20content%20recommender%20systems%20such%20as%20TikTok%20and%20YouTube%2C%20the%20platform%27s%0Arecommendation%20algorithm%20shapes%20content%20producer%20incentives.%20Many%20platforms%0Aemploy%20online%20learning%2C%20which%20generates%20intertemporal%20incentives%2C%20since%20content%0Aproduced%20today%20affects%20recommendations%20of%20future%20content.%20We%20study%20the%20game%0Abetween%20producers%20and%20analyze%20the%20content%20created%20at%20equilibrium.%20We%20show%20that%0Astandard%20online%20learning%20algorithms%2C%20such%20as%20Hedge%20and%20EXP3%2C%20unfortunately%0Aincentivize%20producers%20to%20create%20low-quality%20content%2C%20where%20producers%27%20effort%0Aapproaches%20zero%20in%20the%20long%20run%20for%20typical%20learning%20rate%20schedules.%20Motivated%0Aby%20this%20negative%20result%2C%20we%20design%20learning%20algorithms%20that%20incentivize%0Aproducers%20to%20invest%20high%20effort%20and%20achieve%20high%20user%20welfare.%20At%20a%20conceptual%0Alevel%2C%20our%20work%20illustrates%20the%20unintended%20impact%20that%20a%20platform%27s%20learning%0Aalgorithm%20can%20have%20on%20content%20quality%20and%20introduces%20algorithmic%20approaches%20to%0Amitigating%20these%20effects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.07479v3&entry.124074799=Read"},
{"title": "A Wavelet Guided Attention Module for Skin Cancer Classification with\n  Gradient-based Feature Fusion", "author": "Ayush Roy and Sujan Sarkar and Sohom Ghosal and Dmitrii Kaplun and Asya Lyanova and Ram Sarkar", "abstract": "  Skin cancer is a highly dangerous type of cancer that requires an accurate\ndiagnosis from experienced physicians. To help physicians diagnose skin cancer\nmore efficiently, a computer-aided diagnosis (CAD) system can be very helpful.\nIn this paper, we propose a novel model, which uses a novel attention mechanism\nto pinpoint the differences in features across the spatial dimensions and\nsymmetry of the lesion, thereby focusing on the dissimilarities of various\nclasses based on symmetry, uniformity in texture and color, etc. Additionally,\nto take into account the variations in the boundaries of the lesions for\ndifferent classes, we employ a gradient-based fusion of wavelet and soft\nattention-aided features to extract boundary information of skin lesions. We\nhave tested our model on the multi-class and highly class-imbalanced dataset,\ncalled HAM10000, and achieved promising results, with a 91.17\\% F1-score and\n90.75\\% accuracy. The code is made available at:\nhttps://github.com/AyushRoy2001/WAGF-Fusion.\n", "link": "http://arxiv.org/abs/2406.15128v1", "date": "2024-06-21", "relevancy": 1.7921, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.456}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4474}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Wavelet%20Guided%20Attention%20Module%20for%20Skin%20Cancer%20Classification%20with%0A%20%20Gradient-based%20Feature%20Fusion&body=Title%3A%20A%20Wavelet%20Guided%20Attention%20Module%20for%20Skin%20Cancer%20Classification%20with%0A%20%20Gradient-based%20Feature%20Fusion%0AAuthor%3A%20Ayush%20Roy%20and%20Sujan%20Sarkar%20and%20Sohom%20Ghosal%20and%20Dmitrii%20Kaplun%20and%20Asya%20Lyanova%20and%20Ram%20Sarkar%0AAbstract%3A%20%20%20Skin%20cancer%20is%20a%20highly%20dangerous%20type%20of%20cancer%20that%20requires%20an%20accurate%0Adiagnosis%20from%20experienced%20physicians.%20To%20help%20physicians%20diagnose%20skin%20cancer%0Amore%20efficiently%2C%20a%20computer-aided%20diagnosis%20%28CAD%29%20system%20can%20be%20very%20helpful.%0AIn%20this%20paper%2C%20we%20propose%20a%20novel%20model%2C%20which%20uses%20a%20novel%20attention%20mechanism%0Ato%20pinpoint%20the%20differences%20in%20features%20across%20the%20spatial%20dimensions%20and%0Asymmetry%20of%20the%20lesion%2C%20thereby%20focusing%20on%20the%20dissimilarities%20of%20various%0Aclasses%20based%20on%20symmetry%2C%20uniformity%20in%20texture%20and%20color%2C%20etc.%20Additionally%2C%0Ato%20take%20into%20account%20the%20variations%20in%20the%20boundaries%20of%20the%20lesions%20for%0Adifferent%20classes%2C%20we%20employ%20a%20gradient-based%20fusion%20of%20wavelet%20and%20soft%0Aattention-aided%20features%20to%20extract%20boundary%20information%20of%20skin%20lesions.%20We%0Ahave%20tested%20our%20model%20on%20the%20multi-class%20and%20highly%20class-imbalanced%20dataset%2C%0Acalled%20HAM10000%2C%20and%20achieved%20promising%20results%2C%20with%20a%2091.17%5C%25%20F1-score%20and%0A90.75%5C%25%20accuracy.%20The%20code%20is%20made%20available%20at%3A%0Ahttps%3A//github.com/AyushRoy2001/WAGF-Fusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15128v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Wavelet%2520Guided%2520Attention%2520Module%2520for%2520Skin%2520Cancer%2520Classification%2520with%250A%2520%2520Gradient-based%2520Feature%2520Fusion%26entry.906535625%3DAyush%2520Roy%2520and%2520Sujan%2520Sarkar%2520and%2520Sohom%2520Ghosal%2520and%2520Dmitrii%2520Kaplun%2520and%2520Asya%2520Lyanova%2520and%2520Ram%2520Sarkar%26entry.1292438233%3D%2520%2520Skin%2520cancer%2520is%2520a%2520highly%2520dangerous%2520type%2520of%2520cancer%2520that%2520requires%2520an%2520accurate%250Adiagnosis%2520from%2520experienced%2520physicians.%2520To%2520help%2520physicians%2520diagnose%2520skin%2520cancer%250Amore%2520efficiently%252C%2520a%2520computer-aided%2520diagnosis%2520%2528CAD%2529%2520system%2520can%2520be%2520very%2520helpful.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520model%252C%2520which%2520uses%2520a%2520novel%2520attention%2520mechanism%250Ato%2520pinpoint%2520the%2520differences%2520in%2520features%2520across%2520the%2520spatial%2520dimensions%2520and%250Asymmetry%2520of%2520the%2520lesion%252C%2520thereby%2520focusing%2520on%2520the%2520dissimilarities%2520of%2520various%250Aclasses%2520based%2520on%2520symmetry%252C%2520uniformity%2520in%2520texture%2520and%2520color%252C%2520etc.%2520Additionally%252C%250Ato%2520take%2520into%2520account%2520the%2520variations%2520in%2520the%2520boundaries%2520of%2520the%2520lesions%2520for%250Adifferent%2520classes%252C%2520we%2520employ%2520a%2520gradient-based%2520fusion%2520of%2520wavelet%2520and%2520soft%250Aattention-aided%2520features%2520to%2520extract%2520boundary%2520information%2520of%2520skin%2520lesions.%2520We%250Ahave%2520tested%2520our%2520model%2520on%2520the%2520multi-class%2520and%2520highly%2520class-imbalanced%2520dataset%252C%250Acalled%2520HAM10000%252C%2520and%2520achieved%2520promising%2520results%252C%2520with%2520a%252091.17%255C%2525%2520F1-score%2520and%250A90.75%255C%2525%2520accuracy.%2520The%2520code%2520is%2520made%2520available%2520at%253A%250Ahttps%253A//github.com/AyushRoy2001/WAGF-Fusion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15128v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Wavelet%20Guided%20Attention%20Module%20for%20Skin%20Cancer%20Classification%20with%0A%20%20Gradient-based%20Feature%20Fusion&entry.906535625=Ayush%20Roy%20and%20Sujan%20Sarkar%20and%20Sohom%20Ghosal%20and%20Dmitrii%20Kaplun%20and%20Asya%20Lyanova%20and%20Ram%20Sarkar&entry.1292438233=%20%20Skin%20cancer%20is%20a%20highly%20dangerous%20type%20of%20cancer%20that%20requires%20an%20accurate%0Adiagnosis%20from%20experienced%20physicians.%20To%20help%20physicians%20diagnose%20skin%20cancer%0Amore%20efficiently%2C%20a%20computer-aided%20diagnosis%20%28CAD%29%20system%20can%20be%20very%20helpful.%0AIn%20this%20paper%2C%20we%20propose%20a%20novel%20model%2C%20which%20uses%20a%20novel%20attention%20mechanism%0Ato%20pinpoint%20the%20differences%20in%20features%20across%20the%20spatial%20dimensions%20and%0Asymmetry%20of%20the%20lesion%2C%20thereby%20focusing%20on%20the%20dissimilarities%20of%20various%0Aclasses%20based%20on%20symmetry%2C%20uniformity%20in%20texture%20and%20color%2C%20etc.%20Additionally%2C%0Ato%20take%20into%20account%20the%20variations%20in%20the%20boundaries%20of%20the%20lesions%20for%0Adifferent%20classes%2C%20we%20employ%20a%20gradient-based%20fusion%20of%20wavelet%20and%20soft%0Aattention-aided%20features%20to%20extract%20boundary%20information%20of%20skin%20lesions.%20We%0Ahave%20tested%20our%20model%20on%20the%20multi-class%20and%20highly%20class-imbalanced%20dataset%2C%0Acalled%20HAM10000%2C%20and%20achieved%20promising%20results%2C%20with%20a%2091.17%5C%25%20F1-score%20and%0A90.75%5C%25%20accuracy.%20The%20code%20is%20made%20available%20at%3A%0Ahttps%3A//github.com/AyushRoy2001/WAGF-Fusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15128v1&entry.124074799=Read"},
{"title": "How Effective is GPT-4 Turbo in Generating School-Level Questions from\n  Textbooks Based on Bloom's Revised Taxonomy?", "author": "Subhankar Maity and Aniket Deroy and Sudeshna Sarkar", "abstract": "  We evaluate the effectiveness of GPT-4 Turbo in generating educational\nquestions from NCERT textbooks in zero-shot mode. Our study highlights GPT-4\nTurbo's ability to generate questions that require higher-order thinking\nskills, especially at the \"understanding\" level according to Bloom's Revised\nTaxonomy. While we find a notable consistency between questions generated by\nGPT-4 Turbo and those assessed by humans in terms of complexity, there are\noccasional differences. Our evaluation also uncovers variations in how humans\nand machines evaluate question quality, with a trend inversely related to\nBloom's Revised Taxonomy levels. These findings suggest that while GPT-4 Turbo\nis a promising tool for educational question generation, its efficacy varies\nacross different cognitive levels, indicating a need for further refinement to\nfully meet educational standards.\n", "link": "http://arxiv.org/abs/2406.15211v1", "date": "2024-06-21", "relevancy": 1.7687, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4649}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4283}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Effective%20is%20GPT-4%20Turbo%20in%20Generating%20School-Level%20Questions%20from%0A%20%20Textbooks%20Based%20on%20Bloom%27s%20Revised%20Taxonomy%3F&body=Title%3A%20How%20Effective%20is%20GPT-4%20Turbo%20in%20Generating%20School-Level%20Questions%20from%0A%20%20Textbooks%20Based%20on%20Bloom%27s%20Revised%20Taxonomy%3F%0AAuthor%3A%20Subhankar%20Maity%20and%20Aniket%20Deroy%20and%20Sudeshna%20Sarkar%0AAbstract%3A%20%20%20We%20evaluate%20the%20effectiveness%20of%20GPT-4%20Turbo%20in%20generating%20educational%0Aquestions%20from%20NCERT%20textbooks%20in%20zero-shot%20mode.%20Our%20study%20highlights%20GPT-4%0ATurbo%27s%20ability%20to%20generate%20questions%20that%20require%20higher-order%20thinking%0Askills%2C%20especially%20at%20the%20%22understanding%22%20level%20according%20to%20Bloom%27s%20Revised%0ATaxonomy.%20While%20we%20find%20a%20notable%20consistency%20between%20questions%20generated%20by%0AGPT-4%20Turbo%20and%20those%20assessed%20by%20humans%20in%20terms%20of%20complexity%2C%20there%20are%0Aoccasional%20differences.%20Our%20evaluation%20also%20uncovers%20variations%20in%20how%20humans%0Aand%20machines%20evaluate%20question%20quality%2C%20with%20a%20trend%20inversely%20related%20to%0ABloom%27s%20Revised%20Taxonomy%20levels.%20These%20findings%20suggest%20that%20while%20GPT-4%20Turbo%0Ais%20a%20promising%20tool%20for%20educational%20question%20generation%2C%20its%20efficacy%20varies%0Aacross%20different%20cognitive%20levels%2C%20indicating%20a%20need%20for%20further%20refinement%20to%0Afully%20meet%20educational%20standards.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15211v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Effective%2520is%2520GPT-4%2520Turbo%2520in%2520Generating%2520School-Level%2520Questions%2520from%250A%2520%2520Textbooks%2520Based%2520on%2520Bloom%2527s%2520Revised%2520Taxonomy%253F%26entry.906535625%3DSubhankar%2520Maity%2520and%2520Aniket%2520Deroy%2520and%2520Sudeshna%2520Sarkar%26entry.1292438233%3D%2520%2520We%2520evaluate%2520the%2520effectiveness%2520of%2520GPT-4%2520Turbo%2520in%2520generating%2520educational%250Aquestions%2520from%2520NCERT%2520textbooks%2520in%2520zero-shot%2520mode.%2520Our%2520study%2520highlights%2520GPT-4%250ATurbo%2527s%2520ability%2520to%2520generate%2520questions%2520that%2520require%2520higher-order%2520thinking%250Askills%252C%2520especially%2520at%2520the%2520%2522understanding%2522%2520level%2520according%2520to%2520Bloom%2527s%2520Revised%250ATaxonomy.%2520While%2520we%2520find%2520a%2520notable%2520consistency%2520between%2520questions%2520generated%2520by%250AGPT-4%2520Turbo%2520and%2520those%2520assessed%2520by%2520humans%2520in%2520terms%2520of%2520complexity%252C%2520there%2520are%250Aoccasional%2520differences.%2520Our%2520evaluation%2520also%2520uncovers%2520variations%2520in%2520how%2520humans%250Aand%2520machines%2520evaluate%2520question%2520quality%252C%2520with%2520a%2520trend%2520inversely%2520related%2520to%250ABloom%2527s%2520Revised%2520Taxonomy%2520levels.%2520These%2520findings%2520suggest%2520that%2520while%2520GPT-4%2520Turbo%250Ais%2520a%2520promising%2520tool%2520for%2520educational%2520question%2520generation%252C%2520its%2520efficacy%2520varies%250Aacross%2520different%2520cognitive%2520levels%252C%2520indicating%2520a%2520need%2520for%2520further%2520refinement%2520to%250Afully%2520meet%2520educational%2520standards.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15211v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Effective%20is%20GPT-4%20Turbo%20in%20Generating%20School-Level%20Questions%20from%0A%20%20Textbooks%20Based%20on%20Bloom%27s%20Revised%20Taxonomy%3F&entry.906535625=Subhankar%20Maity%20and%20Aniket%20Deroy%20and%20Sudeshna%20Sarkar&entry.1292438233=%20%20We%20evaluate%20the%20effectiveness%20of%20GPT-4%20Turbo%20in%20generating%20educational%0Aquestions%20from%20NCERT%20textbooks%20in%20zero-shot%20mode.%20Our%20study%20highlights%20GPT-4%0ATurbo%27s%20ability%20to%20generate%20questions%20that%20require%20higher-order%20thinking%0Askills%2C%20especially%20at%20the%20%22understanding%22%20level%20according%20to%20Bloom%27s%20Revised%0ATaxonomy.%20While%20we%20find%20a%20notable%20consistency%20between%20questions%20generated%20by%0AGPT-4%20Turbo%20and%20those%20assessed%20by%20humans%20in%20terms%20of%20complexity%2C%20there%20are%0Aoccasional%20differences.%20Our%20evaluation%20also%20uncovers%20variations%20in%20how%20humans%0Aand%20machines%20evaluate%20question%20quality%2C%20with%20a%20trend%20inversely%20related%20to%0ABloom%27s%20Revised%20Taxonomy%20levels.%20These%20findings%20suggest%20that%20while%20GPT-4%20Turbo%0Ais%20a%20promising%20tool%20for%20educational%20question%20generation%2C%20its%20efficacy%20varies%0Aacross%20different%20cognitive%20levels%2C%20indicating%20a%20need%20for%20further%20refinement%20to%0Afully%20meet%20educational%20standards.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15211v1&entry.124074799=Read"},
{"title": "Sharp detection of low-dimensional structure in probability measures via\n  dimensional logarithmic Sobolev inequalities", "author": "Matthew T. C. Li and Tiangang Cui and Fengyi Li and Youssef Marzouk and Olivier Zahm", "abstract": "  Identifying low-dimensional structure in high-dimensional probability\nmeasures is an essential pre-processing step for efficient sampling. We\nintroduce a method for identifying and approximating a target measure $\\pi$ as\na perturbation of a given reference measure $\\mu$ along a few significant\ndirections of $\\mathbb{R}^{d}$. The reference measure can be a Gaussian or a\nnonlinear transformation of a Gaussian, as commonly arising in generative\nmodeling. Our method extends prior work on minimizing majorizations of the\nKullback--Leibler divergence to identify optimal approximations within this\nclass of measures. Our main contribution unveils a connection between the\n\\emph{dimensional} logarithmic Sobolev inequality (LSI) and approximations with\nthis ansatz. Specifically, when the target and reference are both Gaussian, we\nshow that minimizing the dimensional LSI is equivalent to minimizing the KL\ndivergence restricted to this ansatz. For general non-Gaussian measures, the\ndimensional LSI produces majorants that uniformly improve on previous majorants\nfor gradient-based dimension reduction. We further demonstrate the\napplicability of this analysis to the squared Hellinger distance, where\nanalogous reasoning shows that the dimensional Poincar\\'e inequality offers\nimproved bounds.\n", "link": "http://arxiv.org/abs/2406.13036v2", "date": "2024-06-21", "relevancy": 1.7578, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.442}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4403}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sharp%20detection%20of%20low-dimensional%20structure%20in%20probability%20measures%20via%0A%20%20dimensional%20logarithmic%20Sobolev%20inequalities&body=Title%3A%20Sharp%20detection%20of%20low-dimensional%20structure%20in%20probability%20measures%20via%0A%20%20dimensional%20logarithmic%20Sobolev%20inequalities%0AAuthor%3A%20Matthew%20T.%20C.%20Li%20and%20Tiangang%20Cui%20and%20Fengyi%20Li%20and%20Youssef%20Marzouk%20and%20Olivier%20Zahm%0AAbstract%3A%20%20%20Identifying%20low-dimensional%20structure%20in%20high-dimensional%20probability%0Ameasures%20is%20an%20essential%20pre-processing%20step%20for%20efficient%20sampling.%20We%0Aintroduce%20a%20method%20for%20identifying%20and%20approximating%20a%20target%20measure%20%24%5Cpi%24%20as%0Aa%20perturbation%20of%20a%20given%20reference%20measure%20%24%5Cmu%24%20along%20a%20few%20significant%0Adirections%20of%20%24%5Cmathbb%7BR%7D%5E%7Bd%7D%24.%20The%20reference%20measure%20can%20be%20a%20Gaussian%20or%20a%0Anonlinear%20transformation%20of%20a%20Gaussian%2C%20as%20commonly%20arising%20in%20generative%0Amodeling.%20Our%20method%20extends%20prior%20work%20on%20minimizing%20majorizations%20of%20the%0AKullback--Leibler%20divergence%20to%20identify%20optimal%20approximations%20within%20this%0Aclass%20of%20measures.%20Our%20main%20contribution%20unveils%20a%20connection%20between%20the%0A%5Cemph%7Bdimensional%7D%20logarithmic%20Sobolev%20inequality%20%28LSI%29%20and%20approximations%20with%0Athis%20ansatz.%20Specifically%2C%20when%20the%20target%20and%20reference%20are%20both%20Gaussian%2C%20we%0Ashow%20that%20minimizing%20the%20dimensional%20LSI%20is%20equivalent%20to%20minimizing%20the%20KL%0Adivergence%20restricted%20to%20this%20ansatz.%20For%20general%20non-Gaussian%20measures%2C%20the%0Adimensional%20LSI%20produces%20majorants%20that%20uniformly%20improve%20on%20previous%20majorants%0Afor%20gradient-based%20dimension%20reduction.%20We%20further%20demonstrate%20the%0Aapplicability%20of%20this%20analysis%20to%20the%20squared%20Hellinger%20distance%2C%20where%0Aanalogous%20reasoning%20shows%20that%20the%20dimensional%20Poincar%5C%27e%20inequality%20offers%0Aimproved%20bounds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.13036v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSharp%2520detection%2520of%2520low-dimensional%2520structure%2520in%2520probability%2520measures%2520via%250A%2520%2520dimensional%2520logarithmic%2520Sobolev%2520inequalities%26entry.906535625%3DMatthew%2520T.%2520C.%2520Li%2520and%2520Tiangang%2520Cui%2520and%2520Fengyi%2520Li%2520and%2520Youssef%2520Marzouk%2520and%2520Olivier%2520Zahm%26entry.1292438233%3D%2520%2520Identifying%2520low-dimensional%2520structure%2520in%2520high-dimensional%2520probability%250Ameasures%2520is%2520an%2520essential%2520pre-processing%2520step%2520for%2520efficient%2520sampling.%2520We%250Aintroduce%2520a%2520method%2520for%2520identifying%2520and%2520approximating%2520a%2520target%2520measure%2520%2524%255Cpi%2524%2520as%250Aa%2520perturbation%2520of%2520a%2520given%2520reference%2520measure%2520%2524%255Cmu%2524%2520along%2520a%2520few%2520significant%250Adirections%2520of%2520%2524%255Cmathbb%257BR%257D%255E%257Bd%257D%2524.%2520The%2520reference%2520measure%2520can%2520be%2520a%2520Gaussian%2520or%2520a%250Anonlinear%2520transformation%2520of%2520a%2520Gaussian%252C%2520as%2520commonly%2520arising%2520in%2520generative%250Amodeling.%2520Our%2520method%2520extends%2520prior%2520work%2520on%2520minimizing%2520majorizations%2520of%2520the%250AKullback--Leibler%2520divergence%2520to%2520identify%2520optimal%2520approximations%2520within%2520this%250Aclass%2520of%2520measures.%2520Our%2520main%2520contribution%2520unveils%2520a%2520connection%2520between%2520the%250A%255Cemph%257Bdimensional%257D%2520logarithmic%2520Sobolev%2520inequality%2520%2528LSI%2529%2520and%2520approximations%2520with%250Athis%2520ansatz.%2520Specifically%252C%2520when%2520the%2520target%2520and%2520reference%2520are%2520both%2520Gaussian%252C%2520we%250Ashow%2520that%2520minimizing%2520the%2520dimensional%2520LSI%2520is%2520equivalent%2520to%2520minimizing%2520the%2520KL%250Adivergence%2520restricted%2520to%2520this%2520ansatz.%2520For%2520general%2520non-Gaussian%2520measures%252C%2520the%250Adimensional%2520LSI%2520produces%2520majorants%2520that%2520uniformly%2520improve%2520on%2520previous%2520majorants%250Afor%2520gradient-based%2520dimension%2520reduction.%2520We%2520further%2520demonstrate%2520the%250Aapplicability%2520of%2520this%2520analysis%2520to%2520the%2520squared%2520Hellinger%2520distance%252C%2520where%250Aanalogous%2520reasoning%2520shows%2520that%2520the%2520dimensional%2520Poincar%255C%2527e%2520inequality%2520offers%250Aimproved%2520bounds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.13036v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sharp%20detection%20of%20low-dimensional%20structure%20in%20probability%20measures%20via%0A%20%20dimensional%20logarithmic%20Sobolev%20inequalities&entry.906535625=Matthew%20T.%20C.%20Li%20and%20Tiangang%20Cui%20and%20Fengyi%20Li%20and%20Youssef%20Marzouk%20and%20Olivier%20Zahm&entry.1292438233=%20%20Identifying%20low-dimensional%20structure%20in%20high-dimensional%20probability%0Ameasures%20is%20an%20essential%20pre-processing%20step%20for%20efficient%20sampling.%20We%0Aintroduce%20a%20method%20for%20identifying%20and%20approximating%20a%20target%20measure%20%24%5Cpi%24%20as%0Aa%20perturbation%20of%20a%20given%20reference%20measure%20%24%5Cmu%24%20along%20a%20few%20significant%0Adirections%20of%20%24%5Cmathbb%7BR%7D%5E%7Bd%7D%24.%20The%20reference%20measure%20can%20be%20a%20Gaussian%20or%20a%0Anonlinear%20transformation%20of%20a%20Gaussian%2C%20as%20commonly%20arising%20in%20generative%0Amodeling.%20Our%20method%20extends%20prior%20work%20on%20minimizing%20majorizations%20of%20the%0AKullback--Leibler%20divergence%20to%20identify%20optimal%20approximations%20within%20this%0Aclass%20of%20measures.%20Our%20main%20contribution%20unveils%20a%20connection%20between%20the%0A%5Cemph%7Bdimensional%7D%20logarithmic%20Sobolev%20inequality%20%28LSI%29%20and%20approximations%20with%0Athis%20ansatz.%20Specifically%2C%20when%20the%20target%20and%20reference%20are%20both%20Gaussian%2C%20we%0Ashow%20that%20minimizing%20the%20dimensional%20LSI%20is%20equivalent%20to%20minimizing%20the%20KL%0Adivergence%20restricted%20to%20this%20ansatz.%20For%20general%20non-Gaussian%20measures%2C%20the%0Adimensional%20LSI%20produces%20majorants%20that%20uniformly%20improve%20on%20previous%20majorants%0Afor%20gradient-based%20dimension%20reduction.%20We%20further%20demonstrate%20the%0Aapplicability%20of%20this%20analysis%20to%20the%20squared%20Hellinger%20distance%2C%20where%0Aanalogous%20reasoning%20shows%20that%20the%20dimensional%20Poincar%5C%27e%20inequality%20offers%0Aimproved%20bounds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.13036v2&entry.124074799=Read"},
{"title": "Online detection and infographic explanation of spam reviews with data\n  drift adaptation", "author": "Francisco de Arriba-P\u00e9rez and Silvia Garc\u00eda-M\u00e9ndez and F\u00e1tima Leal and Benedita Malheiro and J. C. Burguillo", "abstract": "  Spam reviews are a pervasive problem on online platforms due to its\nsignificant impact on reputation. However, research into spam detection in data\nstreams is scarce. Another concern lies in their need for transparency.\nConsequently, this paper addresses those problems by proposing an online\nsolution for identifying and explaining spam reviews, incorporating data drift\nadaptation. It integrates (i) incremental profiling, (ii) data drift detection\n& adaptation, and (iii) identification of spam reviews employing Machine\nLearning. The explainable mechanism displays a visual and textual prediction\nexplanation in a dashboard. The best results obtained reached up to 87 % spam\nF-measure.\n", "link": "http://arxiv.org/abs/2406.15038v1", "date": "2024-06-21", "relevancy": 1.7574, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4419}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4386}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4351}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20detection%20and%20infographic%20explanation%20of%20spam%20reviews%20with%20data%0A%20%20drift%20adaptation&body=Title%3A%20Online%20detection%20and%20infographic%20explanation%20of%20spam%20reviews%20with%20data%0A%20%20drift%20adaptation%0AAuthor%3A%20Francisco%20de%20Arriba-P%C3%A9rez%20and%20Silvia%20Garc%C3%ADa-M%C3%A9ndez%20and%20F%C3%A1tima%20Leal%20and%20Benedita%20Malheiro%20and%20J.%20C.%20Burguillo%0AAbstract%3A%20%20%20Spam%20reviews%20are%20a%20pervasive%20problem%20on%20online%20platforms%20due%20to%20its%0Asignificant%20impact%20on%20reputation.%20However%2C%20research%20into%20spam%20detection%20in%20data%0Astreams%20is%20scarce.%20Another%20concern%20lies%20in%20their%20need%20for%20transparency.%0AConsequently%2C%20this%20paper%20addresses%20those%20problems%20by%20proposing%20an%20online%0Asolution%20for%20identifying%20and%20explaining%20spam%20reviews%2C%20incorporating%20data%20drift%0Aadaptation.%20It%20integrates%20%28i%29%20incremental%20profiling%2C%20%28ii%29%20data%20drift%20detection%0A%26%20adaptation%2C%20and%20%28iii%29%20identification%20of%20spam%20reviews%20employing%20Machine%0ALearning.%20The%20explainable%20mechanism%20displays%20a%20visual%20and%20textual%20prediction%0Aexplanation%20in%20a%20dashboard.%20The%20best%20results%20obtained%20reached%20up%20to%2087%20%25%20spam%0AF-measure.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15038v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520detection%2520and%2520infographic%2520explanation%2520of%2520spam%2520reviews%2520with%2520data%250A%2520%2520drift%2520adaptation%26entry.906535625%3DFrancisco%2520de%2520Arriba-P%25C3%25A9rez%2520and%2520Silvia%2520Garc%25C3%25ADa-M%25C3%25A9ndez%2520and%2520F%25C3%25A1tima%2520Leal%2520and%2520Benedita%2520Malheiro%2520and%2520J.%2520C.%2520Burguillo%26entry.1292438233%3D%2520%2520Spam%2520reviews%2520are%2520a%2520pervasive%2520problem%2520on%2520online%2520platforms%2520due%2520to%2520its%250Asignificant%2520impact%2520on%2520reputation.%2520However%252C%2520research%2520into%2520spam%2520detection%2520in%2520data%250Astreams%2520is%2520scarce.%2520Another%2520concern%2520lies%2520in%2520their%2520need%2520for%2520transparency.%250AConsequently%252C%2520this%2520paper%2520addresses%2520those%2520problems%2520by%2520proposing%2520an%2520online%250Asolution%2520for%2520identifying%2520and%2520explaining%2520spam%2520reviews%252C%2520incorporating%2520data%2520drift%250Aadaptation.%2520It%2520integrates%2520%2528i%2529%2520incremental%2520profiling%252C%2520%2528ii%2529%2520data%2520drift%2520detection%250A%2526%2520adaptation%252C%2520and%2520%2528iii%2529%2520identification%2520of%2520spam%2520reviews%2520employing%2520Machine%250ALearning.%2520The%2520explainable%2520mechanism%2520displays%2520a%2520visual%2520and%2520textual%2520prediction%250Aexplanation%2520in%2520a%2520dashboard.%2520The%2520best%2520results%2520obtained%2520reached%2520up%2520to%252087%2520%2525%2520spam%250AF-measure.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15038v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20detection%20and%20infographic%20explanation%20of%20spam%20reviews%20with%20data%0A%20%20drift%20adaptation&entry.906535625=Francisco%20de%20Arriba-P%C3%A9rez%20and%20Silvia%20Garc%C3%ADa-M%C3%A9ndez%20and%20F%C3%A1tima%20Leal%20and%20Benedita%20Malheiro%20and%20J.%20C.%20Burguillo&entry.1292438233=%20%20Spam%20reviews%20are%20a%20pervasive%20problem%20on%20online%20platforms%20due%20to%20its%0Asignificant%20impact%20on%20reputation.%20However%2C%20research%20into%20spam%20detection%20in%20data%0Astreams%20is%20scarce.%20Another%20concern%20lies%20in%20their%20need%20for%20transparency.%0AConsequently%2C%20this%20paper%20addresses%20those%20problems%20by%20proposing%20an%20online%0Asolution%20for%20identifying%20and%20explaining%20spam%20reviews%2C%20incorporating%20data%20drift%0Aadaptation.%20It%20integrates%20%28i%29%20incremental%20profiling%2C%20%28ii%29%20data%20drift%20detection%0A%26%20adaptation%2C%20and%20%28iii%29%20identification%20of%20spam%20reviews%20employing%20Machine%0ALearning.%20The%20explainable%20mechanism%20displays%20a%20visual%20and%20textual%20prediction%0Aexplanation%20in%20a%20dashboard.%20The%20best%20results%20obtained%20reached%20up%20to%2087%20%25%20spam%0AF-measure.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15038v1&entry.124074799=Read"},
{"title": "A policy gradient approach for Finite Horizon Constrained Markov\n  Decision Processes", "author": "Soumyajit Guin and Shalabh Bhatnagar", "abstract": "  The infinite horizon setting is widely adopted for problems of reinforcement\nlearning (RL). These invariably result in stationary policies that are optimal.\nIn many situations, finite horizon control problems are of interest and for\nsuch problems, the optimal policies are time-varying in general. Another\nsetting that has become popular in recent times is of Constrained Reinforcement\nLearning, where the agent maximizes its rewards while it also aims to satisfy\nsome given constraint criteria. However, this setting has only been studied in\nthe context of infinite horizon MDPs where stationary policies are optimal. We\npresent an algorithm for constrained RL in the Finite Horizon Setting where the\nhorizon terminates after a fixed (finite) time. We use function approximation\nin our algorithm which is essential when the state and action spaces are large\nor continuous and use the policy gradient method to find the optimal policy.\nThe optimal policy that we obtain depends on the stage and so is non-stationary\nin general. To the best of our knowledge, our paper presents the first policy\ngradient algorithm for the finite horizon setting with constraints. We show the\nconvergence of our algorithm to a constrained optimal policy. We also compare\nand analyze the performance of our algorithm through experiments and show that\nour algorithm performs better than some other well known algorithms.\n", "link": "http://arxiv.org/abs/2210.04527v3", "date": "2024-06-21", "relevancy": 1.7552, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4522}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4373}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.435}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20policy%20gradient%20approach%20for%20Finite%20Horizon%20Constrained%20Markov%0A%20%20Decision%20Processes&body=Title%3A%20A%20policy%20gradient%20approach%20for%20Finite%20Horizon%20Constrained%20Markov%0A%20%20Decision%20Processes%0AAuthor%3A%20Soumyajit%20Guin%20and%20Shalabh%20Bhatnagar%0AAbstract%3A%20%20%20The%20infinite%20horizon%20setting%20is%20widely%20adopted%20for%20problems%20of%20reinforcement%0Alearning%20%28RL%29.%20These%20invariably%20result%20in%20stationary%20policies%20that%20are%20optimal.%0AIn%20many%20situations%2C%20finite%20horizon%20control%20problems%20are%20of%20interest%20and%20for%0Asuch%20problems%2C%20the%20optimal%20policies%20are%20time-varying%20in%20general.%20Another%0Asetting%20that%20has%20become%20popular%20in%20recent%20times%20is%20of%20Constrained%20Reinforcement%0ALearning%2C%20where%20the%20agent%20maximizes%20its%20rewards%20while%20it%20also%20aims%20to%20satisfy%0Asome%20given%20constraint%20criteria.%20However%2C%20this%20setting%20has%20only%20been%20studied%20in%0Athe%20context%20of%20infinite%20horizon%20MDPs%20where%20stationary%20policies%20are%20optimal.%20We%0Apresent%20an%20algorithm%20for%20constrained%20RL%20in%20the%20Finite%20Horizon%20Setting%20where%20the%0Ahorizon%20terminates%20after%20a%20fixed%20%28finite%29%20time.%20We%20use%20function%20approximation%0Ain%20our%20algorithm%20which%20is%20essential%20when%20the%20state%20and%20action%20spaces%20are%20large%0Aor%20continuous%20and%20use%20the%20policy%20gradient%20method%20to%20find%20the%20optimal%20policy.%0AThe%20optimal%20policy%20that%20we%20obtain%20depends%20on%20the%20stage%20and%20so%20is%20non-stationary%0Ain%20general.%20To%20the%20best%20of%20our%20knowledge%2C%20our%20paper%20presents%20the%20first%20policy%0Agradient%20algorithm%20for%20the%20finite%20horizon%20setting%20with%20constraints.%20We%20show%20the%0Aconvergence%20of%20our%20algorithm%20to%20a%20constrained%20optimal%20policy.%20We%20also%20compare%0Aand%20analyze%20the%20performance%20of%20our%20algorithm%20through%20experiments%20and%20show%20that%0Aour%20algorithm%20performs%20better%20than%20some%20other%20well%20known%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.04527v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520policy%2520gradient%2520approach%2520for%2520Finite%2520Horizon%2520Constrained%2520Markov%250A%2520%2520Decision%2520Processes%26entry.906535625%3DSoumyajit%2520Guin%2520and%2520Shalabh%2520Bhatnagar%26entry.1292438233%3D%2520%2520The%2520infinite%2520horizon%2520setting%2520is%2520widely%2520adopted%2520for%2520problems%2520of%2520reinforcement%250Alearning%2520%2528RL%2529.%2520These%2520invariably%2520result%2520in%2520stationary%2520policies%2520that%2520are%2520optimal.%250AIn%2520many%2520situations%252C%2520finite%2520horizon%2520control%2520problems%2520are%2520of%2520interest%2520and%2520for%250Asuch%2520problems%252C%2520the%2520optimal%2520policies%2520are%2520time-varying%2520in%2520general.%2520Another%250Asetting%2520that%2520has%2520become%2520popular%2520in%2520recent%2520times%2520is%2520of%2520Constrained%2520Reinforcement%250ALearning%252C%2520where%2520the%2520agent%2520maximizes%2520its%2520rewards%2520while%2520it%2520also%2520aims%2520to%2520satisfy%250Asome%2520given%2520constraint%2520criteria.%2520However%252C%2520this%2520setting%2520has%2520only%2520been%2520studied%2520in%250Athe%2520context%2520of%2520infinite%2520horizon%2520MDPs%2520where%2520stationary%2520policies%2520are%2520optimal.%2520We%250Apresent%2520an%2520algorithm%2520for%2520constrained%2520RL%2520in%2520the%2520Finite%2520Horizon%2520Setting%2520where%2520the%250Ahorizon%2520terminates%2520after%2520a%2520fixed%2520%2528finite%2529%2520time.%2520We%2520use%2520function%2520approximation%250Ain%2520our%2520algorithm%2520which%2520is%2520essential%2520when%2520the%2520state%2520and%2520action%2520spaces%2520are%2520large%250Aor%2520continuous%2520and%2520use%2520the%2520policy%2520gradient%2520method%2520to%2520find%2520the%2520optimal%2520policy.%250AThe%2520optimal%2520policy%2520that%2520we%2520obtain%2520depends%2520on%2520the%2520stage%2520and%2520so%2520is%2520non-stationary%250Ain%2520general.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520our%2520paper%2520presents%2520the%2520first%2520policy%250Agradient%2520algorithm%2520for%2520the%2520finite%2520horizon%2520setting%2520with%2520constraints.%2520We%2520show%2520the%250Aconvergence%2520of%2520our%2520algorithm%2520to%2520a%2520constrained%2520optimal%2520policy.%2520We%2520also%2520compare%250Aand%2520analyze%2520the%2520performance%2520of%2520our%2520algorithm%2520through%2520experiments%2520and%2520show%2520that%250Aour%2520algorithm%2520performs%2520better%2520than%2520some%2520other%2520well%2520known%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2210.04527v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20policy%20gradient%20approach%20for%20Finite%20Horizon%20Constrained%20Markov%0A%20%20Decision%20Processes&entry.906535625=Soumyajit%20Guin%20and%20Shalabh%20Bhatnagar&entry.1292438233=%20%20The%20infinite%20horizon%20setting%20is%20widely%20adopted%20for%20problems%20of%20reinforcement%0Alearning%20%28RL%29.%20These%20invariably%20result%20in%20stationary%20policies%20that%20are%20optimal.%0AIn%20many%20situations%2C%20finite%20horizon%20control%20problems%20are%20of%20interest%20and%20for%0Asuch%20problems%2C%20the%20optimal%20policies%20are%20time-varying%20in%20general.%20Another%0Asetting%20that%20has%20become%20popular%20in%20recent%20times%20is%20of%20Constrained%20Reinforcement%0ALearning%2C%20where%20the%20agent%20maximizes%20its%20rewards%20while%20it%20also%20aims%20to%20satisfy%0Asome%20given%20constraint%20criteria.%20However%2C%20this%20setting%20has%20only%20been%20studied%20in%0Athe%20context%20of%20infinite%20horizon%20MDPs%20where%20stationary%20policies%20are%20optimal.%20We%0Apresent%20an%20algorithm%20for%20constrained%20RL%20in%20the%20Finite%20Horizon%20Setting%20where%20the%0Ahorizon%20terminates%20after%20a%20fixed%20%28finite%29%20time.%20We%20use%20function%20approximation%0Ain%20our%20algorithm%20which%20is%20essential%20when%20the%20state%20and%20action%20spaces%20are%20large%0Aor%20continuous%20and%20use%20the%20policy%20gradient%20method%20to%20find%20the%20optimal%20policy.%0AThe%20optimal%20policy%20that%20we%20obtain%20depends%20on%20the%20stage%20and%20so%20is%20non-stationary%0Ain%20general.%20To%20the%20best%20of%20our%20knowledge%2C%20our%20paper%20presents%20the%20first%20policy%0Agradient%20algorithm%20for%20the%20finite%20horizon%20setting%20with%20constraints.%20We%20show%20the%0Aconvergence%20of%20our%20algorithm%20to%20a%20constrained%20optimal%20policy.%20We%20also%20compare%0Aand%20analyze%20the%20performance%20of%20our%20algorithm%20through%20experiments%20and%20show%20that%0Aour%20algorithm%20performs%20better%20than%20some%20other%20well%20known%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.04527v3&entry.124074799=Read"},
{"title": "Open Problem: Order Optimal Regret Bounds for Kernel-Based Reinforcement\n  Learning", "author": "Sattar Vakili", "abstract": "  Reinforcement Learning (RL) has shown great empirical success in various\napplication domains. The theoretical aspects of the problem have been\nextensively studied over past decades, particularly under tabular and linear\nMarkov Decision Process structures. Recently, non-linear function approximation\nusing kernel-based prediction has gained traction. This approach is\nparticularly interesting as it naturally extends the linear structure, and\nhelps explain the behavior of neural-network-based models at their infinite\nwidth limit. The analytical results however do not adequately address the\nperformance guarantees for this case. We will highlight this open problem,\noverview existing partial results, and discuss related challenges.\n", "link": "http://arxiv.org/abs/2406.15250v1", "date": "2024-06-21", "relevancy": 1.7472, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4721}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4301}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open%20Problem%3A%20Order%20Optimal%20Regret%20Bounds%20for%20Kernel-Based%20Reinforcement%0A%20%20Learning&body=Title%3A%20Open%20Problem%3A%20Order%20Optimal%20Regret%20Bounds%20for%20Kernel-Based%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Sattar%20Vakili%0AAbstract%3A%20%20%20Reinforcement%20Learning%20%28RL%29%20has%20shown%20great%20empirical%20success%20in%20various%0Aapplication%20domains.%20The%20theoretical%20aspects%20of%20the%20problem%20have%20been%0Aextensively%20studied%20over%20past%20decades%2C%20particularly%20under%20tabular%20and%20linear%0AMarkov%20Decision%20Process%20structures.%20Recently%2C%20non-linear%20function%20approximation%0Ausing%20kernel-based%20prediction%20has%20gained%20traction.%20This%20approach%20is%0Aparticularly%20interesting%20as%20it%20naturally%20extends%20the%20linear%20structure%2C%20and%0Ahelps%20explain%20the%20behavior%20of%20neural-network-based%20models%20at%20their%20infinite%0Awidth%20limit.%20The%20analytical%20results%20however%20do%20not%20adequately%20address%20the%0Aperformance%20guarantees%20for%20this%20case.%20We%20will%20highlight%20this%20open%20problem%2C%0Aoverview%20existing%20partial%20results%2C%20and%20discuss%20related%20challenges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15250v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen%2520Problem%253A%2520Order%2520Optimal%2520Regret%2520Bounds%2520for%2520Kernel-Based%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DSattar%2520Vakili%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520%2528RL%2529%2520has%2520shown%2520great%2520empirical%2520success%2520in%2520various%250Aapplication%2520domains.%2520The%2520theoretical%2520aspects%2520of%2520the%2520problem%2520have%2520been%250Aextensively%2520studied%2520over%2520past%2520decades%252C%2520particularly%2520under%2520tabular%2520and%2520linear%250AMarkov%2520Decision%2520Process%2520structures.%2520Recently%252C%2520non-linear%2520function%2520approximation%250Ausing%2520kernel-based%2520prediction%2520has%2520gained%2520traction.%2520This%2520approach%2520is%250Aparticularly%2520interesting%2520as%2520it%2520naturally%2520extends%2520the%2520linear%2520structure%252C%2520and%250Ahelps%2520explain%2520the%2520behavior%2520of%2520neural-network-based%2520models%2520at%2520their%2520infinite%250Awidth%2520limit.%2520The%2520analytical%2520results%2520however%2520do%2520not%2520adequately%2520address%2520the%250Aperformance%2520guarantees%2520for%2520this%2520case.%2520We%2520will%2520highlight%2520this%2520open%2520problem%252C%250Aoverview%2520existing%2520partial%2520results%252C%2520and%2520discuss%2520related%2520challenges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15250v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open%20Problem%3A%20Order%20Optimal%20Regret%20Bounds%20for%20Kernel-Based%20Reinforcement%0A%20%20Learning&entry.906535625=Sattar%20Vakili&entry.1292438233=%20%20Reinforcement%20Learning%20%28RL%29%20has%20shown%20great%20empirical%20success%20in%20various%0Aapplication%20domains.%20The%20theoretical%20aspects%20of%20the%20problem%20have%20been%0Aextensively%20studied%20over%20past%20decades%2C%20particularly%20under%20tabular%20and%20linear%0AMarkov%20Decision%20Process%20structures.%20Recently%2C%20non-linear%20function%20approximation%0Ausing%20kernel-based%20prediction%20has%20gained%20traction.%20This%20approach%20is%0Aparticularly%20interesting%20as%20it%20naturally%20extends%20the%20linear%20structure%2C%20and%0Ahelps%20explain%20the%20behavior%20of%20neural-network-based%20models%20at%20their%20infinite%0Awidth%20limit.%20The%20analytical%20results%20however%20do%20not%20adequately%20address%20the%0Aperformance%20guarantees%20for%20this%20case.%20We%20will%20highlight%20this%20open%20problem%2C%0Aoverview%20existing%20partial%20results%2C%20and%20discuss%20related%20challenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15250v1&entry.124074799=Read"},
{"title": "Random Pareto front surfaces", "author": "Ben Tu and Nikolas Kantas and Robert M. Lee and Behrang Shafei", "abstract": "  The goal of multi-objective optimisation is to identify the Pareto front\nsurface which is the set obtained by connecting the best trade-off points.\nTypically this surface is computed by evaluating the objectives at different\npoints and then interpolating between the subset of the best evaluated\ntrade-off points. In this work, we propose to parameterise the Pareto front\nsurface using polar coordinates. More precisely, we show that any Pareto front\nsurface can be equivalently represented using a scalar-valued length function\nwhich returns the projected length along any positive radial direction. We then\nuse this representation in order to rigorously develop the theory and\napplications of stochastic Pareto front surfaces. In particular, we derive many\nPareto front surface statistics of interest such as the expectation, covariance\nand quantiles. We then discuss how these can be used in practice within a\ndesign of experiments setting, where the goal is to both infer and use the\nPareto front surface distribution in order to make effective decisions. Our\nframework allows for clear uncertainty quantification and we also develop\nadvanced visualisation techniques for this purpose. Finally we discuss the\napplicability of our ideas within multivariate extreme value theory and\nillustrate our methodology in a variety of numerical examples, including a case\nstudy with a real-world air pollution data set.\n", "link": "http://arxiv.org/abs/2405.01404v2", "date": "2024-06-21", "relevancy": 1.7444, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.473}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4433}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4142}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Random%20Pareto%20front%20surfaces&body=Title%3A%20Random%20Pareto%20front%20surfaces%0AAuthor%3A%20Ben%20Tu%20and%20Nikolas%20Kantas%20and%20Robert%20M.%20Lee%20and%20Behrang%20Shafei%0AAbstract%3A%20%20%20The%20goal%20of%20multi-objective%20optimisation%20is%20to%20identify%20the%20Pareto%20front%0Asurface%20which%20is%20the%20set%20obtained%20by%20connecting%20the%20best%20trade-off%20points.%0ATypically%20this%20surface%20is%20computed%20by%20evaluating%20the%20objectives%20at%20different%0Apoints%20and%20then%20interpolating%20between%20the%20subset%20of%20the%20best%20evaluated%0Atrade-off%20points.%20In%20this%20work%2C%20we%20propose%20to%20parameterise%20the%20Pareto%20front%0Asurface%20using%20polar%20coordinates.%20More%20precisely%2C%20we%20show%20that%20any%20Pareto%20front%0Asurface%20can%20be%20equivalently%20represented%20using%20a%20scalar-valued%20length%20function%0Awhich%20returns%20the%20projected%20length%20along%20any%20positive%20radial%20direction.%20We%20then%0Ause%20this%20representation%20in%20order%20to%20rigorously%20develop%20the%20theory%20and%0Aapplications%20of%20stochastic%20Pareto%20front%20surfaces.%20In%20particular%2C%20we%20derive%20many%0APareto%20front%20surface%20statistics%20of%20interest%20such%20as%20the%20expectation%2C%20covariance%0Aand%20quantiles.%20We%20then%20discuss%20how%20these%20can%20be%20used%20in%20practice%20within%20a%0Adesign%20of%20experiments%20setting%2C%20where%20the%20goal%20is%20to%20both%20infer%20and%20use%20the%0APareto%20front%20surface%20distribution%20in%20order%20to%20make%20effective%20decisions.%20Our%0Aframework%20allows%20for%20clear%20uncertainty%20quantification%20and%20we%20also%20develop%0Aadvanced%20visualisation%20techniques%20for%20this%20purpose.%20Finally%20we%20discuss%20the%0Aapplicability%20of%20our%20ideas%20within%20multivariate%20extreme%20value%20theory%20and%0Aillustrate%20our%20methodology%20in%20a%20variety%20of%20numerical%20examples%2C%20including%20a%20case%0Astudy%20with%20a%20real-world%20air%20pollution%20data%20set.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01404v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRandom%2520Pareto%2520front%2520surfaces%26entry.906535625%3DBen%2520Tu%2520and%2520Nikolas%2520Kantas%2520and%2520Robert%2520M.%2520Lee%2520and%2520Behrang%2520Shafei%26entry.1292438233%3D%2520%2520The%2520goal%2520of%2520multi-objective%2520optimisation%2520is%2520to%2520identify%2520the%2520Pareto%2520front%250Asurface%2520which%2520is%2520the%2520set%2520obtained%2520by%2520connecting%2520the%2520best%2520trade-off%2520points.%250ATypically%2520this%2520surface%2520is%2520computed%2520by%2520evaluating%2520the%2520objectives%2520at%2520different%250Apoints%2520and%2520then%2520interpolating%2520between%2520the%2520subset%2520of%2520the%2520best%2520evaluated%250Atrade-off%2520points.%2520In%2520this%2520work%252C%2520we%2520propose%2520to%2520parameterise%2520the%2520Pareto%2520front%250Asurface%2520using%2520polar%2520coordinates.%2520More%2520precisely%252C%2520we%2520show%2520that%2520any%2520Pareto%2520front%250Asurface%2520can%2520be%2520equivalently%2520represented%2520using%2520a%2520scalar-valued%2520length%2520function%250Awhich%2520returns%2520the%2520projected%2520length%2520along%2520any%2520positive%2520radial%2520direction.%2520We%2520then%250Ause%2520this%2520representation%2520in%2520order%2520to%2520rigorously%2520develop%2520the%2520theory%2520and%250Aapplications%2520of%2520stochastic%2520Pareto%2520front%2520surfaces.%2520In%2520particular%252C%2520we%2520derive%2520many%250APareto%2520front%2520surface%2520statistics%2520of%2520interest%2520such%2520as%2520the%2520expectation%252C%2520covariance%250Aand%2520quantiles.%2520We%2520then%2520discuss%2520how%2520these%2520can%2520be%2520used%2520in%2520practice%2520within%2520a%250Adesign%2520of%2520experiments%2520setting%252C%2520where%2520the%2520goal%2520is%2520to%2520both%2520infer%2520and%2520use%2520the%250APareto%2520front%2520surface%2520distribution%2520in%2520order%2520to%2520make%2520effective%2520decisions.%2520Our%250Aframework%2520allows%2520for%2520clear%2520uncertainty%2520quantification%2520and%2520we%2520also%2520develop%250Aadvanced%2520visualisation%2520techniques%2520for%2520this%2520purpose.%2520Finally%2520we%2520discuss%2520the%250Aapplicability%2520of%2520our%2520ideas%2520within%2520multivariate%2520extreme%2520value%2520theory%2520and%250Aillustrate%2520our%2520methodology%2520in%2520a%2520variety%2520of%2520numerical%2520examples%252C%2520including%2520a%2520case%250Astudy%2520with%2520a%2520real-world%2520air%2520pollution%2520data%2520set.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01404v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Random%20Pareto%20front%20surfaces&entry.906535625=Ben%20Tu%20and%20Nikolas%20Kantas%20and%20Robert%20M.%20Lee%20and%20Behrang%20Shafei&entry.1292438233=%20%20The%20goal%20of%20multi-objective%20optimisation%20is%20to%20identify%20the%20Pareto%20front%0Asurface%20which%20is%20the%20set%20obtained%20by%20connecting%20the%20best%20trade-off%20points.%0ATypically%20this%20surface%20is%20computed%20by%20evaluating%20the%20objectives%20at%20different%0Apoints%20and%20then%20interpolating%20between%20the%20subset%20of%20the%20best%20evaluated%0Atrade-off%20points.%20In%20this%20work%2C%20we%20propose%20to%20parameterise%20the%20Pareto%20front%0Asurface%20using%20polar%20coordinates.%20More%20precisely%2C%20we%20show%20that%20any%20Pareto%20front%0Asurface%20can%20be%20equivalently%20represented%20using%20a%20scalar-valued%20length%20function%0Awhich%20returns%20the%20projected%20length%20along%20any%20positive%20radial%20direction.%20We%20then%0Ause%20this%20representation%20in%20order%20to%20rigorously%20develop%20the%20theory%20and%0Aapplications%20of%20stochastic%20Pareto%20front%20surfaces.%20In%20particular%2C%20we%20derive%20many%0APareto%20front%20surface%20statistics%20of%20interest%20such%20as%20the%20expectation%2C%20covariance%0Aand%20quantiles.%20We%20then%20discuss%20how%20these%20can%20be%20used%20in%20practice%20within%20a%0Adesign%20of%20experiments%20setting%2C%20where%20the%20goal%20is%20to%20both%20infer%20and%20use%20the%0APareto%20front%20surface%20distribution%20in%20order%20to%20make%20effective%20decisions.%20Our%0Aframework%20allows%20for%20clear%20uncertainty%20quantification%20and%20we%20also%20develop%0Aadvanced%20visualisation%20techniques%20for%20this%20purpose.%20Finally%20we%20discuss%20the%0Aapplicability%20of%20our%20ideas%20within%20multivariate%20extreme%20value%20theory%20and%0Aillustrate%20our%20methodology%20in%20a%20variety%20of%20numerical%20examples%2C%20including%20a%20case%0Astudy%20with%20a%20real-world%20air%20pollution%20data%20set.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01404v2&entry.124074799=Read"},
{"title": "Testing Calibration in Nearly-Linear Time", "author": "Lunjia Hu and Arun Jambulapati and Kevin Tian and Chutong Yang", "abstract": "  In the recent literature on machine learning and decision making, calibration\nhas emerged as a desirable and widely-studied statistical property of the\noutputs of binary prediction models. However, the algorithmic aspects of\nmeasuring model calibration have remained relatively less well-explored.\nMotivated by [BGHN23], which proposed a rigorous framework for measuring\ndistances to calibration, we initiate the algorithmic study of calibration\nthrough the lens of property testing. We define the problem of calibration\ntesting from samples where given $n$ draws from a distribution $\\mathcal{D}$ on\n$(predictions, binary outcomes)$, our goal is to distinguish between the case\nwhere $\\mathcal{D}$ is perfectly calibrated, and the case where $\\mathcal{D}$\nis $\\varepsilon$-far from calibration.\n  We make the simple observation that the empirical smooth calibration linear\nprogram can be reformulated as an instance of minimum-cost flow on a\nhighly-structured graph, and design an exact dynamic programming-based solver\nfor it which runs in time $O(n\\log^2(n))$, and solves the calibration testing\nproblem information-theoretically optimally in the same time. This improves\nupon state-of-the-art black-box linear program solvers requiring\n$\\Omega(n^\\omega)$ time, where $\\omega > 2$ is the exponent of matrix\nmultiplication. We also develop algorithms for tolerant variants of our testing\nproblem improving upon black-box linear program solvers, and give sample\ncomplexity lower bounds for alternative calibration measures to the one\nconsidered in this work. Finally, we present experiments showing the testing\nproblem we define faithfully captures standard notions of calibration, and that\nour algorithms scale efficiently to accommodate large sample sizes.\n", "link": "http://arxiv.org/abs/2402.13187v2", "date": "2024-06-21", "relevancy": 1.7375, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4748}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4345}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4181}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Testing%20Calibration%20in%20Nearly-Linear%20Time&body=Title%3A%20Testing%20Calibration%20in%20Nearly-Linear%20Time%0AAuthor%3A%20Lunjia%20Hu%20and%20Arun%20Jambulapati%20and%20Kevin%20Tian%20and%20Chutong%20Yang%0AAbstract%3A%20%20%20In%20the%20recent%20literature%20on%20machine%20learning%20and%20decision%20making%2C%20calibration%0Ahas%20emerged%20as%20a%20desirable%20and%20widely-studied%20statistical%20property%20of%20the%0Aoutputs%20of%20binary%20prediction%20models.%20However%2C%20the%20algorithmic%20aspects%20of%0Ameasuring%20model%20calibration%20have%20remained%20relatively%20less%20well-explored.%0AMotivated%20by%20%5BBGHN23%5D%2C%20which%20proposed%20a%20rigorous%20framework%20for%20measuring%0Adistances%20to%20calibration%2C%20we%20initiate%20the%20algorithmic%20study%20of%20calibration%0Athrough%20the%20lens%20of%20property%20testing.%20We%20define%20the%20problem%20of%20calibration%0Atesting%20from%20samples%20where%20given%20%24n%24%20draws%20from%20a%20distribution%20%24%5Cmathcal%7BD%7D%24%20on%0A%24%28predictions%2C%20binary%20outcomes%29%24%2C%20our%20goal%20is%20to%20distinguish%20between%20the%20case%0Awhere%20%24%5Cmathcal%7BD%7D%24%20is%20perfectly%20calibrated%2C%20and%20the%20case%20where%20%24%5Cmathcal%7BD%7D%24%0Ais%20%24%5Cvarepsilon%24-far%20from%20calibration.%0A%20%20We%20make%20the%20simple%20observation%20that%20the%20empirical%20smooth%20calibration%20linear%0Aprogram%20can%20be%20reformulated%20as%20an%20instance%20of%20minimum-cost%20flow%20on%20a%0Ahighly-structured%20graph%2C%20and%20design%20an%20exact%20dynamic%20programming-based%20solver%0Afor%20it%20which%20runs%20in%20time%20%24O%28n%5Clog%5E2%28n%29%29%24%2C%20and%20solves%20the%20calibration%20testing%0Aproblem%20information-theoretically%20optimally%20in%20the%20same%20time.%20This%20improves%0Aupon%20state-of-the-art%20black-box%20linear%20program%20solvers%20requiring%0A%24%5COmega%28n%5E%5Comega%29%24%20time%2C%20where%20%24%5Comega%20%3E%202%24%20is%20the%20exponent%20of%20matrix%0Amultiplication.%20We%20also%20develop%20algorithms%20for%20tolerant%20variants%20of%20our%20testing%0Aproblem%20improving%20upon%20black-box%20linear%20program%20solvers%2C%20and%20give%20sample%0Acomplexity%20lower%20bounds%20for%20alternative%20calibration%20measures%20to%20the%20one%0Aconsidered%20in%20this%20work.%20Finally%2C%20we%20present%20experiments%20showing%20the%20testing%0Aproblem%20we%20define%20faithfully%20captures%20standard%20notions%20of%20calibration%2C%20and%20that%0Aour%20algorithms%20scale%20efficiently%20to%20accommodate%20large%20sample%20sizes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.13187v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTesting%2520Calibration%2520in%2520Nearly-Linear%2520Time%26entry.906535625%3DLunjia%2520Hu%2520and%2520Arun%2520Jambulapati%2520and%2520Kevin%2520Tian%2520and%2520Chutong%2520Yang%26entry.1292438233%3D%2520%2520In%2520the%2520recent%2520literature%2520on%2520machine%2520learning%2520and%2520decision%2520making%252C%2520calibration%250Ahas%2520emerged%2520as%2520a%2520desirable%2520and%2520widely-studied%2520statistical%2520property%2520of%2520the%250Aoutputs%2520of%2520binary%2520prediction%2520models.%2520However%252C%2520the%2520algorithmic%2520aspects%2520of%250Ameasuring%2520model%2520calibration%2520have%2520remained%2520relatively%2520less%2520well-explored.%250AMotivated%2520by%2520%255BBGHN23%255D%252C%2520which%2520proposed%2520a%2520rigorous%2520framework%2520for%2520measuring%250Adistances%2520to%2520calibration%252C%2520we%2520initiate%2520the%2520algorithmic%2520study%2520of%2520calibration%250Athrough%2520the%2520lens%2520of%2520property%2520testing.%2520We%2520define%2520the%2520problem%2520of%2520calibration%250Atesting%2520from%2520samples%2520where%2520given%2520%2524n%2524%2520draws%2520from%2520a%2520distribution%2520%2524%255Cmathcal%257BD%257D%2524%2520on%250A%2524%2528predictions%252C%2520binary%2520outcomes%2529%2524%252C%2520our%2520goal%2520is%2520to%2520distinguish%2520between%2520the%2520case%250Awhere%2520%2524%255Cmathcal%257BD%257D%2524%2520is%2520perfectly%2520calibrated%252C%2520and%2520the%2520case%2520where%2520%2524%255Cmathcal%257BD%257D%2524%250Ais%2520%2524%255Cvarepsilon%2524-far%2520from%2520calibration.%250A%2520%2520We%2520make%2520the%2520simple%2520observation%2520that%2520the%2520empirical%2520smooth%2520calibration%2520linear%250Aprogram%2520can%2520be%2520reformulated%2520as%2520an%2520instance%2520of%2520minimum-cost%2520flow%2520on%2520a%250Ahighly-structured%2520graph%252C%2520and%2520design%2520an%2520exact%2520dynamic%2520programming-based%2520solver%250Afor%2520it%2520which%2520runs%2520in%2520time%2520%2524O%2528n%255Clog%255E2%2528n%2529%2529%2524%252C%2520and%2520solves%2520the%2520calibration%2520testing%250Aproblem%2520information-theoretically%2520optimally%2520in%2520the%2520same%2520time.%2520This%2520improves%250Aupon%2520state-of-the-art%2520black-box%2520linear%2520program%2520solvers%2520requiring%250A%2524%255COmega%2528n%255E%255Comega%2529%2524%2520time%252C%2520where%2520%2524%255Comega%2520%253E%25202%2524%2520is%2520the%2520exponent%2520of%2520matrix%250Amultiplication.%2520We%2520also%2520develop%2520algorithms%2520for%2520tolerant%2520variants%2520of%2520our%2520testing%250Aproblem%2520improving%2520upon%2520black-box%2520linear%2520program%2520solvers%252C%2520and%2520give%2520sample%250Acomplexity%2520lower%2520bounds%2520for%2520alternative%2520calibration%2520measures%2520to%2520the%2520one%250Aconsidered%2520in%2520this%2520work.%2520Finally%252C%2520we%2520present%2520experiments%2520showing%2520the%2520testing%250Aproblem%2520we%2520define%2520faithfully%2520captures%2520standard%2520notions%2520of%2520calibration%252C%2520and%2520that%250Aour%2520algorithms%2520scale%2520efficiently%2520to%2520accommodate%2520large%2520sample%2520sizes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.13187v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Testing%20Calibration%20in%20Nearly-Linear%20Time&entry.906535625=Lunjia%20Hu%20and%20Arun%20Jambulapati%20and%20Kevin%20Tian%20and%20Chutong%20Yang&entry.1292438233=%20%20In%20the%20recent%20literature%20on%20machine%20learning%20and%20decision%20making%2C%20calibration%0Ahas%20emerged%20as%20a%20desirable%20and%20widely-studied%20statistical%20property%20of%20the%0Aoutputs%20of%20binary%20prediction%20models.%20However%2C%20the%20algorithmic%20aspects%20of%0Ameasuring%20model%20calibration%20have%20remained%20relatively%20less%20well-explored.%0AMotivated%20by%20%5BBGHN23%5D%2C%20which%20proposed%20a%20rigorous%20framework%20for%20measuring%0Adistances%20to%20calibration%2C%20we%20initiate%20the%20algorithmic%20study%20of%20calibration%0Athrough%20the%20lens%20of%20property%20testing.%20We%20define%20the%20problem%20of%20calibration%0Atesting%20from%20samples%20where%20given%20%24n%24%20draws%20from%20a%20distribution%20%24%5Cmathcal%7BD%7D%24%20on%0A%24%28predictions%2C%20binary%20outcomes%29%24%2C%20our%20goal%20is%20to%20distinguish%20between%20the%20case%0Awhere%20%24%5Cmathcal%7BD%7D%24%20is%20perfectly%20calibrated%2C%20and%20the%20case%20where%20%24%5Cmathcal%7BD%7D%24%0Ais%20%24%5Cvarepsilon%24-far%20from%20calibration.%0A%20%20We%20make%20the%20simple%20observation%20that%20the%20empirical%20smooth%20calibration%20linear%0Aprogram%20can%20be%20reformulated%20as%20an%20instance%20of%20minimum-cost%20flow%20on%20a%0Ahighly-structured%20graph%2C%20and%20design%20an%20exact%20dynamic%20programming-based%20solver%0Afor%20it%20which%20runs%20in%20time%20%24O%28n%5Clog%5E2%28n%29%29%24%2C%20and%20solves%20the%20calibration%20testing%0Aproblem%20information-theoretically%20optimally%20in%20the%20same%20time.%20This%20improves%0Aupon%20state-of-the-art%20black-box%20linear%20program%20solvers%20requiring%0A%24%5COmega%28n%5E%5Comega%29%24%20time%2C%20where%20%24%5Comega%20%3E%202%24%20is%20the%20exponent%20of%20matrix%0Amultiplication.%20We%20also%20develop%20algorithms%20for%20tolerant%20variants%20of%20our%20testing%0Aproblem%20improving%20upon%20black-box%20linear%20program%20solvers%2C%20and%20give%20sample%0Acomplexity%20lower%20bounds%20for%20alternative%20calibration%20measures%20to%20the%20one%0Aconsidered%20in%20this%20work.%20Finally%2C%20we%20present%20experiments%20showing%20the%20testing%0Aproblem%20we%20define%20faithfully%20captures%20standard%20notions%20of%20calibration%2C%20and%20that%0Aour%20algorithms%20scale%20efficiently%20to%20accommodate%20large%20sample%20sizes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.13187v2&entry.124074799=Read"},
{"title": "Towards Probabilistic Clearance, Explanation and Optimization", "author": "Simon Kohaut and Benedict Flade and Devendra Singh Dhami and Julian Eggert and Kristian Kersting", "abstract": "  Employing Unmanned Aircraft Systems (UAS) beyond visual line of sight (BVLOS)\nis an endearing and challenging task. While UAS have the potential to\nsignificantly enhance today's logistics and emergency response capabilities,\nunmanned flying objects above the heads of unprotected pedestrians induce\nsimilarly significant safety risks. In this work, we make strides towards\nimproved safety and legal compliance in applying UAS in two ways. First, we\ndemonstrate navigation within the Probabilistic Mission Design (ProMis)\nframework. To this end, our approach translates Probabilistic Mission\nLandscapes (PML) into a navigation graph and derives a cost from the\nprobability of complying with all underlying constraints. Second, we introduce\nthe clearance, explanation, and optimization (CEO) cycle on top of ProMis by\nleveraging the declaratively encoded domain knowledge, legal requirements, and\nsafety assertions to guide the mission design process. Based on inaccurate,\ncrowd-sourced map data and a synthetic scenario, we illustrate the application\nand utility of our methods in UAS navigation.\n", "link": "http://arxiv.org/abs/2406.15088v1", "date": "2024-06-21", "relevancy": 1.7233, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6069}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5519}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5158}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Probabilistic%20Clearance%2C%20Explanation%20and%20Optimization&body=Title%3A%20Towards%20Probabilistic%20Clearance%2C%20Explanation%20and%20Optimization%0AAuthor%3A%20Simon%20Kohaut%20and%20Benedict%20Flade%20and%20Devendra%20Singh%20Dhami%20and%20Julian%20Eggert%20and%20Kristian%20Kersting%0AAbstract%3A%20%20%20Employing%20Unmanned%20Aircraft%20Systems%20%28UAS%29%20beyond%20visual%20line%20of%20sight%20%28BVLOS%29%0Ais%20an%20endearing%20and%20challenging%20task.%20While%20UAS%20have%20the%20potential%20to%0Asignificantly%20enhance%20today%27s%20logistics%20and%20emergency%20response%20capabilities%2C%0Aunmanned%20flying%20objects%20above%20the%20heads%20of%20unprotected%20pedestrians%20induce%0Asimilarly%20significant%20safety%20risks.%20In%20this%20work%2C%20we%20make%20strides%20towards%0Aimproved%20safety%20and%20legal%20compliance%20in%20applying%20UAS%20in%20two%20ways.%20First%2C%20we%0Ademonstrate%20navigation%20within%20the%20Probabilistic%20Mission%20Design%20%28ProMis%29%0Aframework.%20To%20this%20end%2C%20our%20approach%20translates%20Probabilistic%20Mission%0ALandscapes%20%28PML%29%20into%20a%20navigation%20graph%20and%20derives%20a%20cost%20from%20the%0Aprobability%20of%20complying%20with%20all%20underlying%20constraints.%20Second%2C%20we%20introduce%0Athe%20clearance%2C%20explanation%2C%20and%20optimization%20%28CEO%29%20cycle%20on%20top%20of%20ProMis%20by%0Aleveraging%20the%20declaratively%20encoded%20domain%20knowledge%2C%20legal%20requirements%2C%20and%0Asafety%20assertions%20to%20guide%20the%20mission%20design%20process.%20Based%20on%20inaccurate%2C%0Acrowd-sourced%20map%20data%20and%20a%20synthetic%20scenario%2C%20we%20illustrate%20the%20application%0Aand%20utility%20of%20our%20methods%20in%20UAS%20navigation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15088v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Probabilistic%2520Clearance%252C%2520Explanation%2520and%2520Optimization%26entry.906535625%3DSimon%2520Kohaut%2520and%2520Benedict%2520Flade%2520and%2520Devendra%2520Singh%2520Dhami%2520and%2520Julian%2520Eggert%2520and%2520Kristian%2520Kersting%26entry.1292438233%3D%2520%2520Employing%2520Unmanned%2520Aircraft%2520Systems%2520%2528UAS%2529%2520beyond%2520visual%2520line%2520of%2520sight%2520%2528BVLOS%2529%250Ais%2520an%2520endearing%2520and%2520challenging%2520task.%2520While%2520UAS%2520have%2520the%2520potential%2520to%250Asignificantly%2520enhance%2520today%2527s%2520logistics%2520and%2520emergency%2520response%2520capabilities%252C%250Aunmanned%2520flying%2520objects%2520above%2520the%2520heads%2520of%2520unprotected%2520pedestrians%2520induce%250Asimilarly%2520significant%2520safety%2520risks.%2520In%2520this%2520work%252C%2520we%2520make%2520strides%2520towards%250Aimproved%2520safety%2520and%2520legal%2520compliance%2520in%2520applying%2520UAS%2520in%2520two%2520ways.%2520First%252C%2520we%250Ademonstrate%2520navigation%2520within%2520the%2520Probabilistic%2520Mission%2520Design%2520%2528ProMis%2529%250Aframework.%2520To%2520this%2520end%252C%2520our%2520approach%2520translates%2520Probabilistic%2520Mission%250ALandscapes%2520%2528PML%2529%2520into%2520a%2520navigation%2520graph%2520and%2520derives%2520a%2520cost%2520from%2520the%250Aprobability%2520of%2520complying%2520with%2520all%2520underlying%2520constraints.%2520Second%252C%2520we%2520introduce%250Athe%2520clearance%252C%2520explanation%252C%2520and%2520optimization%2520%2528CEO%2529%2520cycle%2520on%2520top%2520of%2520ProMis%2520by%250Aleveraging%2520the%2520declaratively%2520encoded%2520domain%2520knowledge%252C%2520legal%2520requirements%252C%2520and%250Asafety%2520assertions%2520to%2520guide%2520the%2520mission%2520design%2520process.%2520Based%2520on%2520inaccurate%252C%250Acrowd-sourced%2520map%2520data%2520and%2520a%2520synthetic%2520scenario%252C%2520we%2520illustrate%2520the%2520application%250Aand%2520utility%2520of%2520our%2520methods%2520in%2520UAS%2520navigation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15088v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Probabilistic%20Clearance%2C%20Explanation%20and%20Optimization&entry.906535625=Simon%20Kohaut%20and%20Benedict%20Flade%20and%20Devendra%20Singh%20Dhami%20and%20Julian%20Eggert%20and%20Kristian%20Kersting&entry.1292438233=%20%20Employing%20Unmanned%20Aircraft%20Systems%20%28UAS%29%20beyond%20visual%20line%20of%20sight%20%28BVLOS%29%0Ais%20an%20endearing%20and%20challenging%20task.%20While%20UAS%20have%20the%20potential%20to%0Asignificantly%20enhance%20today%27s%20logistics%20and%20emergency%20response%20capabilities%2C%0Aunmanned%20flying%20objects%20above%20the%20heads%20of%20unprotected%20pedestrians%20induce%0Asimilarly%20significant%20safety%20risks.%20In%20this%20work%2C%20we%20make%20strides%20towards%0Aimproved%20safety%20and%20legal%20compliance%20in%20applying%20UAS%20in%20two%20ways.%20First%2C%20we%0Ademonstrate%20navigation%20within%20the%20Probabilistic%20Mission%20Design%20%28ProMis%29%0Aframework.%20To%20this%20end%2C%20our%20approach%20translates%20Probabilistic%20Mission%0ALandscapes%20%28PML%29%20into%20a%20navigation%20graph%20and%20derives%20a%20cost%20from%20the%0Aprobability%20of%20complying%20with%20all%20underlying%20constraints.%20Second%2C%20we%20introduce%0Athe%20clearance%2C%20explanation%2C%20and%20optimization%20%28CEO%29%20cycle%20on%20top%20of%20ProMis%20by%0Aleveraging%20the%20declaratively%20encoded%20domain%20knowledge%2C%20legal%20requirements%2C%20and%0Asafety%20assertions%20to%20guide%20the%20mission%20design%20process.%20Based%20on%20inaccurate%2C%0Acrowd-sourced%20map%20data%20and%20a%20synthetic%20scenario%2C%20we%20illustrate%20the%20application%0Aand%20utility%20of%20our%20methods%20in%20UAS%20navigation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15088v1&entry.124074799=Read"},
{"title": "Multimodal Task Vectors Enable Many-Shot Multimodal In-Context Learning", "author": "Brandon Huang and Chancharik Mitra and Assaf Arbelle and Leonid Karlinsky and Trevor Darrell and Roei Herzig", "abstract": "  The recent success of interleaved Large Multimodal Models (LMMs) in few-shot\nlearning suggests that in-context learning (ICL) with many examples can be\npromising for learning new tasks. However, this many-shot multimodal ICL\nsetting has one crucial problem: it is fundamentally limited by the model's\ncontext length set at pretraining. The problem is especially prominent in the\nmultimodal domain, which processes both text and images, requiring additional\ntokens. This motivates the need for a multimodal method to compress many shots\ninto fewer tokens without finetuning. In this work, we enable LMMs to perform\nmultimodal, many-shot in-context learning by leveraging Multimodal Task Vectors\n(MTV)--compact implicit representations of in-context examples compressed in\nthe model's attention heads. Specifically, we first demonstrate the existence\nof such MTV in LMMs and then leverage these extracted MTV to enable many-shot\nin-context learning for various vision-and-language tasks. Our experiments\nsuggest that MTV can scale in performance with the number of compressed shots\nand generalize to similar out-of-domain tasks without additional context length\nfor inference.\n", "link": "http://arxiv.org/abs/2406.15334v1", "date": "2024-06-21", "relevancy": 1.7132, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6079}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.536}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Task%20Vectors%20Enable%20Many-Shot%20Multimodal%20In-Context%20Learning&body=Title%3A%20Multimodal%20Task%20Vectors%20Enable%20Many-Shot%20Multimodal%20In-Context%20Learning%0AAuthor%3A%20Brandon%20Huang%20and%20Chancharik%20Mitra%20and%20Assaf%20Arbelle%20and%20Leonid%20Karlinsky%20and%20Trevor%20Darrell%20and%20Roei%20Herzig%0AAbstract%3A%20%20%20The%20recent%20success%20of%20interleaved%20Large%20Multimodal%20Models%20%28LMMs%29%20in%20few-shot%0Alearning%20suggests%20that%20in-context%20learning%20%28ICL%29%20with%20many%20examples%20can%20be%0Apromising%20for%20learning%20new%20tasks.%20However%2C%20this%20many-shot%20multimodal%20ICL%0Asetting%20has%20one%20crucial%20problem%3A%20it%20is%20fundamentally%20limited%20by%20the%20model%27s%0Acontext%20length%20set%20at%20pretraining.%20The%20problem%20is%20especially%20prominent%20in%20the%0Amultimodal%20domain%2C%20which%20processes%20both%20text%20and%20images%2C%20requiring%20additional%0Atokens.%20This%20motivates%20the%20need%20for%20a%20multimodal%20method%20to%20compress%20many%20shots%0Ainto%20fewer%20tokens%20without%20finetuning.%20In%20this%20work%2C%20we%20enable%20LMMs%20to%20perform%0Amultimodal%2C%20many-shot%20in-context%20learning%20by%20leveraging%20Multimodal%20Task%20Vectors%0A%28MTV%29--compact%20implicit%20representations%20of%20in-context%20examples%20compressed%20in%0Athe%20model%27s%20attention%20heads.%20Specifically%2C%20we%20first%20demonstrate%20the%20existence%0Aof%20such%20MTV%20in%20LMMs%20and%20then%20leverage%20these%20extracted%20MTV%20to%20enable%20many-shot%0Ain-context%20learning%20for%20various%20vision-and-language%20tasks.%20Our%20experiments%0Asuggest%20that%20MTV%20can%20scale%20in%20performance%20with%20the%20number%20of%20compressed%20shots%0Aand%20generalize%20to%20similar%20out-of-domain%20tasks%20without%20additional%20context%20length%0Afor%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15334v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Task%2520Vectors%2520Enable%2520Many-Shot%2520Multimodal%2520In-Context%2520Learning%26entry.906535625%3DBrandon%2520Huang%2520and%2520Chancharik%2520Mitra%2520and%2520Assaf%2520Arbelle%2520and%2520Leonid%2520Karlinsky%2520and%2520Trevor%2520Darrell%2520and%2520Roei%2520Herzig%26entry.1292438233%3D%2520%2520The%2520recent%2520success%2520of%2520interleaved%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%2520in%2520few-shot%250Alearning%2520suggests%2520that%2520in-context%2520learning%2520%2528ICL%2529%2520with%2520many%2520examples%2520can%2520be%250Apromising%2520for%2520learning%2520new%2520tasks.%2520However%252C%2520this%2520many-shot%2520multimodal%2520ICL%250Asetting%2520has%2520one%2520crucial%2520problem%253A%2520it%2520is%2520fundamentally%2520limited%2520by%2520the%2520model%2527s%250Acontext%2520length%2520set%2520at%2520pretraining.%2520The%2520problem%2520is%2520especially%2520prominent%2520in%2520the%250Amultimodal%2520domain%252C%2520which%2520processes%2520both%2520text%2520and%2520images%252C%2520requiring%2520additional%250Atokens.%2520This%2520motivates%2520the%2520need%2520for%2520a%2520multimodal%2520method%2520to%2520compress%2520many%2520shots%250Ainto%2520fewer%2520tokens%2520without%2520finetuning.%2520In%2520this%2520work%252C%2520we%2520enable%2520LMMs%2520to%2520perform%250Amultimodal%252C%2520many-shot%2520in-context%2520learning%2520by%2520leveraging%2520Multimodal%2520Task%2520Vectors%250A%2528MTV%2529--compact%2520implicit%2520representations%2520of%2520in-context%2520examples%2520compressed%2520in%250Athe%2520model%2527s%2520attention%2520heads.%2520Specifically%252C%2520we%2520first%2520demonstrate%2520the%2520existence%250Aof%2520such%2520MTV%2520in%2520LMMs%2520and%2520then%2520leverage%2520these%2520extracted%2520MTV%2520to%2520enable%2520many-shot%250Ain-context%2520learning%2520for%2520various%2520vision-and-language%2520tasks.%2520Our%2520experiments%250Asuggest%2520that%2520MTV%2520can%2520scale%2520in%2520performance%2520with%2520the%2520number%2520of%2520compressed%2520shots%250Aand%2520generalize%2520to%2520similar%2520out-of-domain%2520tasks%2520without%2520additional%2520context%2520length%250Afor%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15334v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Task%20Vectors%20Enable%20Many-Shot%20Multimodal%20In-Context%20Learning&entry.906535625=Brandon%20Huang%20and%20Chancharik%20Mitra%20and%20Assaf%20Arbelle%20and%20Leonid%20Karlinsky%20and%20Trevor%20Darrell%20and%20Roei%20Herzig&entry.1292438233=%20%20The%20recent%20success%20of%20interleaved%20Large%20Multimodal%20Models%20%28LMMs%29%20in%20few-shot%0Alearning%20suggests%20that%20in-context%20learning%20%28ICL%29%20with%20many%20examples%20can%20be%0Apromising%20for%20learning%20new%20tasks.%20However%2C%20this%20many-shot%20multimodal%20ICL%0Asetting%20has%20one%20crucial%20problem%3A%20it%20is%20fundamentally%20limited%20by%20the%20model%27s%0Acontext%20length%20set%20at%20pretraining.%20The%20problem%20is%20especially%20prominent%20in%20the%0Amultimodal%20domain%2C%20which%20processes%20both%20text%20and%20images%2C%20requiring%20additional%0Atokens.%20This%20motivates%20the%20need%20for%20a%20multimodal%20method%20to%20compress%20many%20shots%0Ainto%20fewer%20tokens%20without%20finetuning.%20In%20this%20work%2C%20we%20enable%20LMMs%20to%20perform%0Amultimodal%2C%20many-shot%20in-context%20learning%20by%20leveraging%20Multimodal%20Task%20Vectors%0A%28MTV%29--compact%20implicit%20representations%20of%20in-context%20examples%20compressed%20in%0Athe%20model%27s%20attention%20heads.%20Specifically%2C%20we%20first%20demonstrate%20the%20existence%0Aof%20such%20MTV%20in%20LMMs%20and%20then%20leverage%20these%20extracted%20MTV%20to%20enable%20many-shot%0Ain-context%20learning%20for%20various%20vision-and-language%20tasks.%20Our%20experiments%0Asuggest%20that%20MTV%20can%20scale%20in%20performance%20with%20the%20number%20of%20compressed%20shots%0Aand%20generalize%20to%20similar%20out-of-domain%20tasks%20without%20additional%20context%20length%0Afor%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15334v1&entry.124074799=Read"},
{"title": "Accelerating Complex Disease Treatment through Network Medicine and\n  GenAI: A Case Study on Drug Repurposing for Breast Cancer", "author": "Ahmed Abdeen Hamed and Tamer E. Fandy", "abstract": "  The objective of this research is to introduce a network specialized in\npredicting drugs that can be repurposed by investigating real-world evidence\nsources, such as clinical trials and biomedical literature. Specifically, it\naims to generate drug combination therapies for complex diseases (e.g., cancer,\nAlzheimer's). We present a multilayered network medicine approach, empowered by\na highly configured ChatGPT prompt engineering system, which is constructed on\nthe fly to extract drug mentions in clinical trials. Additionally, we introduce\na novel algorithm that connects real-world evidence with disease-specific\nsignaling pathways (e.g., KEGG database). This sheds light on the\nrepurposability of drugs if they are found to bind with one or more protein\nconstituents of a signaling pathway. To demonstrate, we instantiated the\nframework for breast cancer and found that, out of 46 breast cancer signaling\npathways, the framework identified 38 pathways that were covered by at least\ntwo drugs. This evidence signals the potential for combining those drugs.\nSpecifically, the most covered signaling pathway, ID hsa:2064, was covered by\n108 drugs, some of which can be combined. Conversely, the signaling pathway ID\nhsa:1499 was covered by only two drugs, indicating a significant gap for\nfurther research. Our network medicine framework, empowered by GenAI, shows\npromise in identifying drug combinations with a high degree of specificity,\nknowing the exact signaling pathways and proteins that serve as targets. It is\nnoteworthy that ChatGPT successfully accelerated the process of identifying\ndrug mentions in clinical trials, though further investigations are required to\ndetermine the relationships among the drug mentions.\n", "link": "http://arxiv.org/abs/2406.13106v2", "date": "2024-06-21", "relevancy": 1.7042, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4346}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4323}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4164}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Complex%20Disease%20Treatment%20through%20Network%20Medicine%20and%0A%20%20GenAI%3A%20A%20Case%20Study%20on%20Drug%20Repurposing%20for%20Breast%20Cancer&body=Title%3A%20Accelerating%20Complex%20Disease%20Treatment%20through%20Network%20Medicine%20and%0A%20%20GenAI%3A%20A%20Case%20Study%20on%20Drug%20Repurposing%20for%20Breast%20Cancer%0AAuthor%3A%20Ahmed%20Abdeen%20Hamed%20and%20Tamer%20E.%20Fandy%0AAbstract%3A%20%20%20The%20objective%20of%20this%20research%20is%20to%20introduce%20a%20network%20specialized%20in%0Apredicting%20drugs%20that%20can%20be%20repurposed%20by%20investigating%20real-world%20evidence%0Asources%2C%20such%20as%20clinical%20trials%20and%20biomedical%20literature.%20Specifically%2C%20it%0Aaims%20to%20generate%20drug%20combination%20therapies%20for%20complex%20diseases%20%28e.g.%2C%20cancer%2C%0AAlzheimer%27s%29.%20We%20present%20a%20multilayered%20network%20medicine%20approach%2C%20empowered%20by%0Aa%20highly%20configured%20ChatGPT%20prompt%20engineering%20system%2C%20which%20is%20constructed%20on%0Athe%20fly%20to%20extract%20drug%20mentions%20in%20clinical%20trials.%20Additionally%2C%20we%20introduce%0Aa%20novel%20algorithm%20that%20connects%20real-world%20evidence%20with%20disease-specific%0Asignaling%20pathways%20%28e.g.%2C%20KEGG%20database%29.%20This%20sheds%20light%20on%20the%0Arepurposability%20of%20drugs%20if%20they%20are%20found%20to%20bind%20with%20one%20or%20more%20protein%0Aconstituents%20of%20a%20signaling%20pathway.%20To%20demonstrate%2C%20we%20instantiated%20the%0Aframework%20for%20breast%20cancer%20and%20found%20that%2C%20out%20of%2046%20breast%20cancer%20signaling%0Apathways%2C%20the%20framework%20identified%2038%20pathways%20that%20were%20covered%20by%20at%20least%0Atwo%20drugs.%20This%20evidence%20signals%20the%20potential%20for%20combining%20those%20drugs.%0ASpecifically%2C%20the%20most%20covered%20signaling%20pathway%2C%20ID%20hsa%3A2064%2C%20was%20covered%20by%0A108%20drugs%2C%20some%20of%20which%20can%20be%20combined.%20Conversely%2C%20the%20signaling%20pathway%20ID%0Ahsa%3A1499%20was%20covered%20by%20only%20two%20drugs%2C%20indicating%20a%20significant%20gap%20for%0Afurther%20research.%20Our%20network%20medicine%20framework%2C%20empowered%20by%20GenAI%2C%20shows%0Apromise%20in%20identifying%20drug%20combinations%20with%20a%20high%20degree%20of%20specificity%2C%0Aknowing%20the%20exact%20signaling%20pathways%20and%20proteins%20that%20serve%20as%20targets.%20It%20is%0Anoteworthy%20that%20ChatGPT%20successfully%20accelerated%20the%20process%20of%20identifying%0Adrug%20mentions%20in%20clinical%20trials%2C%20though%20further%20investigations%20are%20required%20to%0Adetermine%20the%20relationships%20among%20the%20drug%20mentions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.13106v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520Complex%2520Disease%2520Treatment%2520through%2520Network%2520Medicine%2520and%250A%2520%2520GenAI%253A%2520A%2520Case%2520Study%2520on%2520Drug%2520Repurposing%2520for%2520Breast%2520Cancer%26entry.906535625%3DAhmed%2520Abdeen%2520Hamed%2520and%2520Tamer%2520E.%2520Fandy%26entry.1292438233%3D%2520%2520The%2520objective%2520of%2520this%2520research%2520is%2520to%2520introduce%2520a%2520network%2520specialized%2520in%250Apredicting%2520drugs%2520that%2520can%2520be%2520repurposed%2520by%2520investigating%2520real-world%2520evidence%250Asources%252C%2520such%2520as%2520clinical%2520trials%2520and%2520biomedical%2520literature.%2520Specifically%252C%2520it%250Aaims%2520to%2520generate%2520drug%2520combination%2520therapies%2520for%2520complex%2520diseases%2520%2528e.g.%252C%2520cancer%252C%250AAlzheimer%2527s%2529.%2520We%2520present%2520a%2520multilayered%2520network%2520medicine%2520approach%252C%2520empowered%2520by%250Aa%2520highly%2520configured%2520ChatGPT%2520prompt%2520engineering%2520system%252C%2520which%2520is%2520constructed%2520on%250Athe%2520fly%2520to%2520extract%2520drug%2520mentions%2520in%2520clinical%2520trials.%2520Additionally%252C%2520we%2520introduce%250Aa%2520novel%2520algorithm%2520that%2520connects%2520real-world%2520evidence%2520with%2520disease-specific%250Asignaling%2520pathways%2520%2528e.g.%252C%2520KEGG%2520database%2529.%2520This%2520sheds%2520light%2520on%2520the%250Arepurposability%2520of%2520drugs%2520if%2520they%2520are%2520found%2520to%2520bind%2520with%2520one%2520or%2520more%2520protein%250Aconstituents%2520of%2520a%2520signaling%2520pathway.%2520To%2520demonstrate%252C%2520we%2520instantiated%2520the%250Aframework%2520for%2520breast%2520cancer%2520and%2520found%2520that%252C%2520out%2520of%252046%2520breast%2520cancer%2520signaling%250Apathways%252C%2520the%2520framework%2520identified%252038%2520pathways%2520that%2520were%2520covered%2520by%2520at%2520least%250Atwo%2520drugs.%2520This%2520evidence%2520signals%2520the%2520potential%2520for%2520combining%2520those%2520drugs.%250ASpecifically%252C%2520the%2520most%2520covered%2520signaling%2520pathway%252C%2520ID%2520hsa%253A2064%252C%2520was%2520covered%2520by%250A108%2520drugs%252C%2520some%2520of%2520which%2520can%2520be%2520combined.%2520Conversely%252C%2520the%2520signaling%2520pathway%2520ID%250Ahsa%253A1499%2520was%2520covered%2520by%2520only%2520two%2520drugs%252C%2520indicating%2520a%2520significant%2520gap%2520for%250Afurther%2520research.%2520Our%2520network%2520medicine%2520framework%252C%2520empowered%2520by%2520GenAI%252C%2520shows%250Apromise%2520in%2520identifying%2520drug%2520combinations%2520with%2520a%2520high%2520degree%2520of%2520specificity%252C%250Aknowing%2520the%2520exact%2520signaling%2520pathways%2520and%2520proteins%2520that%2520serve%2520as%2520targets.%2520It%2520is%250Anoteworthy%2520that%2520ChatGPT%2520successfully%2520accelerated%2520the%2520process%2520of%2520identifying%250Adrug%2520mentions%2520in%2520clinical%2520trials%252C%2520though%2520further%2520investigations%2520are%2520required%2520to%250Adetermine%2520the%2520relationships%2520among%2520the%2520drug%2520mentions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.13106v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Complex%20Disease%20Treatment%20through%20Network%20Medicine%20and%0A%20%20GenAI%3A%20A%20Case%20Study%20on%20Drug%20Repurposing%20for%20Breast%20Cancer&entry.906535625=Ahmed%20Abdeen%20Hamed%20and%20Tamer%20E.%20Fandy&entry.1292438233=%20%20The%20objective%20of%20this%20research%20is%20to%20introduce%20a%20network%20specialized%20in%0Apredicting%20drugs%20that%20can%20be%20repurposed%20by%20investigating%20real-world%20evidence%0Asources%2C%20such%20as%20clinical%20trials%20and%20biomedical%20literature.%20Specifically%2C%20it%0Aaims%20to%20generate%20drug%20combination%20therapies%20for%20complex%20diseases%20%28e.g.%2C%20cancer%2C%0AAlzheimer%27s%29.%20We%20present%20a%20multilayered%20network%20medicine%20approach%2C%20empowered%20by%0Aa%20highly%20configured%20ChatGPT%20prompt%20engineering%20system%2C%20which%20is%20constructed%20on%0Athe%20fly%20to%20extract%20drug%20mentions%20in%20clinical%20trials.%20Additionally%2C%20we%20introduce%0Aa%20novel%20algorithm%20that%20connects%20real-world%20evidence%20with%20disease-specific%0Asignaling%20pathways%20%28e.g.%2C%20KEGG%20database%29.%20This%20sheds%20light%20on%20the%0Arepurposability%20of%20drugs%20if%20they%20are%20found%20to%20bind%20with%20one%20or%20more%20protein%0Aconstituents%20of%20a%20signaling%20pathway.%20To%20demonstrate%2C%20we%20instantiated%20the%0Aframework%20for%20breast%20cancer%20and%20found%20that%2C%20out%20of%2046%20breast%20cancer%20signaling%0Apathways%2C%20the%20framework%20identified%2038%20pathways%20that%20were%20covered%20by%20at%20least%0Atwo%20drugs.%20This%20evidence%20signals%20the%20potential%20for%20combining%20those%20drugs.%0ASpecifically%2C%20the%20most%20covered%20signaling%20pathway%2C%20ID%20hsa%3A2064%2C%20was%20covered%20by%0A108%20drugs%2C%20some%20of%20which%20can%20be%20combined.%20Conversely%2C%20the%20signaling%20pathway%20ID%0Ahsa%3A1499%20was%20covered%20by%20only%20two%20drugs%2C%20indicating%20a%20significant%20gap%20for%0Afurther%20research.%20Our%20network%20medicine%20framework%2C%20empowered%20by%20GenAI%2C%20shows%0Apromise%20in%20identifying%20drug%20combinations%20with%20a%20high%20degree%20of%20specificity%2C%0Aknowing%20the%20exact%20signaling%20pathways%20and%20proteins%20that%20serve%20as%20targets.%20It%20is%0Anoteworthy%20that%20ChatGPT%20successfully%20accelerated%20the%20process%20of%20identifying%0Adrug%20mentions%20in%20clinical%20trials%2C%20though%20further%20investigations%20are%20required%20to%0Adetermine%20the%20relationships%20among%20the%20drug%20mentions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.13106v2&entry.124074799=Read"},
{"title": "Chain-of-Thought Unfaithfulness as Disguised Accuracy", "author": "Oliver Bentham and Nathan Stringham and Ana Marasovi\u0107", "abstract": "  Understanding the extent to which Chain-of-Thought (CoT) generations align\nwith a large language model's (LLM) internal computations is critical for\ndeciding whether to trust an LLM's output. As a proxy for CoT faithfulness,\nLanham et al. (2023) propose a metric that measures a model's dependence on its\nCoT for producing an answer. Within a single family of proprietary models, they\nfind that LLMs exhibit a scaling-then-inverse-scaling relationship between\nmodel size and their measure of faithfulness, and that a 13 billion parameter\nmodel exhibits increased faithfulness compared to models ranging from 810\nmillion to 175 billion parameters in size. We evaluate whether these results\ngeneralize as a property of all LLMs. We replicate the experimental setup in\ntheir section focused on scaling experiments with three different families of\nmodels and, under specific conditions, successfully reproduce the scaling\ntrends for CoT faithfulness they report. However, after normalizing the metric\nto account for a model's bias toward certain answer choices, unfaithfulness\ndrops significantly for smaller less-capable models. This normalized\nfaithfulness metric is also strongly correlated ($R^2$=0.74) with accuracy,\nraising doubts about its validity for evaluating faithfulness.\n", "link": "http://arxiv.org/abs/2402.14897v3", "date": "2024-06-21", "relevancy": 1.6939, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4511}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4226}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chain-of-Thought%20Unfaithfulness%20as%20Disguised%20Accuracy&body=Title%3A%20Chain-of-Thought%20Unfaithfulness%20as%20Disguised%20Accuracy%0AAuthor%3A%20Oliver%20Bentham%20and%20Nathan%20Stringham%20and%20Ana%20Marasovi%C4%87%0AAbstract%3A%20%20%20Understanding%20the%20extent%20to%20which%20Chain-of-Thought%20%28CoT%29%20generations%20align%0Awith%20a%20large%20language%20model%27s%20%28LLM%29%20internal%20computations%20is%20critical%20for%0Adeciding%20whether%20to%20trust%20an%20LLM%27s%20output.%20As%20a%20proxy%20for%20CoT%20faithfulness%2C%0ALanham%20et%20al.%20%282023%29%20propose%20a%20metric%20that%20measures%20a%20model%27s%20dependence%20on%20its%0ACoT%20for%20producing%20an%20answer.%20Within%20a%20single%20family%20of%20proprietary%20models%2C%20they%0Afind%20that%20LLMs%20exhibit%20a%20scaling-then-inverse-scaling%20relationship%20between%0Amodel%20size%20and%20their%20measure%20of%20faithfulness%2C%20and%20that%20a%2013%20billion%20parameter%0Amodel%20exhibits%20increased%20faithfulness%20compared%20to%20models%20ranging%20from%20810%0Amillion%20to%20175%20billion%20parameters%20in%20size.%20We%20evaluate%20whether%20these%20results%0Ageneralize%20as%20a%20property%20of%20all%20LLMs.%20We%20replicate%20the%20experimental%20setup%20in%0Atheir%20section%20focused%20on%20scaling%20experiments%20with%20three%20different%20families%20of%0Amodels%20and%2C%20under%20specific%20conditions%2C%20successfully%20reproduce%20the%20scaling%0Atrends%20for%20CoT%20faithfulness%20they%20report.%20However%2C%20after%20normalizing%20the%20metric%0Ato%20account%20for%20a%20model%27s%20bias%20toward%20certain%20answer%20choices%2C%20unfaithfulness%0Adrops%20significantly%20for%20smaller%20less-capable%20models.%20This%20normalized%0Afaithfulness%20metric%20is%20also%20strongly%20correlated%20%28%24R%5E2%24%3D0.74%29%20with%20accuracy%2C%0Araising%20doubts%20about%20its%20validity%20for%20evaluating%20faithfulness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14897v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChain-of-Thought%2520Unfaithfulness%2520as%2520Disguised%2520Accuracy%26entry.906535625%3DOliver%2520Bentham%2520and%2520Nathan%2520Stringham%2520and%2520Ana%2520Marasovi%25C4%2587%26entry.1292438233%3D%2520%2520Understanding%2520the%2520extent%2520to%2520which%2520Chain-of-Thought%2520%2528CoT%2529%2520generations%2520align%250Awith%2520a%2520large%2520language%2520model%2527s%2520%2528LLM%2529%2520internal%2520computations%2520is%2520critical%2520for%250Adeciding%2520whether%2520to%2520trust%2520an%2520LLM%2527s%2520output.%2520As%2520a%2520proxy%2520for%2520CoT%2520faithfulness%252C%250ALanham%2520et%2520al.%2520%25282023%2529%2520propose%2520a%2520metric%2520that%2520measures%2520a%2520model%2527s%2520dependence%2520on%2520its%250ACoT%2520for%2520producing%2520an%2520answer.%2520Within%2520a%2520single%2520family%2520of%2520proprietary%2520models%252C%2520they%250Afind%2520that%2520LLMs%2520exhibit%2520a%2520scaling-then-inverse-scaling%2520relationship%2520between%250Amodel%2520size%2520and%2520their%2520measure%2520of%2520faithfulness%252C%2520and%2520that%2520a%252013%2520billion%2520parameter%250Amodel%2520exhibits%2520increased%2520faithfulness%2520compared%2520to%2520models%2520ranging%2520from%2520810%250Amillion%2520to%2520175%2520billion%2520parameters%2520in%2520size.%2520We%2520evaluate%2520whether%2520these%2520results%250Ageneralize%2520as%2520a%2520property%2520of%2520all%2520LLMs.%2520We%2520replicate%2520the%2520experimental%2520setup%2520in%250Atheir%2520section%2520focused%2520on%2520scaling%2520experiments%2520with%2520three%2520different%2520families%2520of%250Amodels%2520and%252C%2520under%2520specific%2520conditions%252C%2520successfully%2520reproduce%2520the%2520scaling%250Atrends%2520for%2520CoT%2520faithfulness%2520they%2520report.%2520However%252C%2520after%2520normalizing%2520the%2520metric%250Ato%2520account%2520for%2520a%2520model%2527s%2520bias%2520toward%2520certain%2520answer%2520choices%252C%2520unfaithfulness%250Adrops%2520significantly%2520for%2520smaller%2520less-capable%2520models.%2520This%2520normalized%250Afaithfulness%2520metric%2520is%2520also%2520strongly%2520correlated%2520%2528%2524R%255E2%2524%253D0.74%2529%2520with%2520accuracy%252C%250Araising%2520doubts%2520about%2520its%2520validity%2520for%2520evaluating%2520faithfulness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.14897v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chain-of-Thought%20Unfaithfulness%20as%20Disguised%20Accuracy&entry.906535625=Oliver%20Bentham%20and%20Nathan%20Stringham%20and%20Ana%20Marasovi%C4%87&entry.1292438233=%20%20Understanding%20the%20extent%20to%20which%20Chain-of-Thought%20%28CoT%29%20generations%20align%0Awith%20a%20large%20language%20model%27s%20%28LLM%29%20internal%20computations%20is%20critical%20for%0Adeciding%20whether%20to%20trust%20an%20LLM%27s%20output.%20As%20a%20proxy%20for%20CoT%20faithfulness%2C%0ALanham%20et%20al.%20%282023%29%20propose%20a%20metric%20that%20measures%20a%20model%27s%20dependence%20on%20its%0ACoT%20for%20producing%20an%20answer.%20Within%20a%20single%20family%20of%20proprietary%20models%2C%20they%0Afind%20that%20LLMs%20exhibit%20a%20scaling-then-inverse-scaling%20relationship%20between%0Amodel%20size%20and%20their%20measure%20of%20faithfulness%2C%20and%20that%20a%2013%20billion%20parameter%0Amodel%20exhibits%20increased%20faithfulness%20compared%20to%20models%20ranging%20from%20810%0Amillion%20to%20175%20billion%20parameters%20in%20size.%20We%20evaluate%20whether%20these%20results%0Ageneralize%20as%20a%20property%20of%20all%20LLMs.%20We%20replicate%20the%20experimental%20setup%20in%0Atheir%20section%20focused%20on%20scaling%20experiments%20with%20three%20different%20families%20of%0Amodels%20and%2C%20under%20specific%20conditions%2C%20successfully%20reproduce%20the%20scaling%0Atrends%20for%20CoT%20faithfulness%20they%20report.%20However%2C%20after%20normalizing%20the%20metric%0Ato%20account%20for%20a%20model%27s%20bias%20toward%20certain%20answer%20choices%2C%20unfaithfulness%0Adrops%20significantly%20for%20smaller%20less-capable%20models.%20This%20normalized%0Afaithfulness%20metric%20is%20also%20strongly%20correlated%20%28%24R%5E2%24%3D0.74%29%20with%20accuracy%2C%0Araising%20doubts%20about%20its%20validity%20for%20evaluating%20faithfulness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14897v3&entry.124074799=Read"},
{"title": "Using Neural Networks for Data Cleaning in Weather Datasets", "author": "Jack R. P. Hanslope and Laurence Aitchison", "abstract": "  In climate science, we often want to compare across different datasets.\nDifficulties can arise in doing this due to inevitable mismatches that arise\nbetween observational and reanalysis data, or even between different\nreanalyses. This misalignment can raise problems for any work that seeks to\nmake inferences about one dataset from another. We considered tropical cyclone\nlocation as an example task with one dataset providing atmospheric conditions\n(ERA5) and another providing storm tracks (IBTrACS). We found that while the\nexamples often aligned well, there were a considerable proportion (around 25%)\nwhich were not well aligned. We trained a neural network to map from the wind\nfield to the storm location; in this setting misalignment in the datasets\nappears as \"label noise\" (i.e. the labelled storm location does not correspond\nto the underlying wind field). We found that this neural network trained only\non the often noisy labels from IBTrACS had a denoising effect, and performed\nbetter than the IBTrACS labels themselves, as measured by human preferences.\nRemarkably, this even held true for training points, on which we might have\nexpected the network to overfit to the IBTrACS predictions.\n", "link": "http://arxiv.org/abs/2406.15027v1", "date": "2024-06-21", "relevancy": 1.6876, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4261}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4239}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4169}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Using%20Neural%20Networks%20for%20Data%20Cleaning%20in%20Weather%20Datasets&body=Title%3A%20Using%20Neural%20Networks%20for%20Data%20Cleaning%20in%20Weather%20Datasets%0AAuthor%3A%20Jack%20R.%20P.%20Hanslope%20and%20Laurence%20Aitchison%0AAbstract%3A%20%20%20In%20climate%20science%2C%20we%20often%20want%20to%20compare%20across%20different%20datasets.%0ADifficulties%20can%20arise%20in%20doing%20this%20due%20to%20inevitable%20mismatches%20that%20arise%0Abetween%20observational%20and%20reanalysis%20data%2C%20or%20even%20between%20different%0Areanalyses.%20This%20misalignment%20can%20raise%20problems%20for%20any%20work%20that%20seeks%20to%0Amake%20inferences%20about%20one%20dataset%20from%20another.%20We%20considered%20tropical%20cyclone%0Alocation%20as%20an%20example%20task%20with%20one%20dataset%20providing%20atmospheric%20conditions%0A%28ERA5%29%20and%20another%20providing%20storm%20tracks%20%28IBTrACS%29.%20We%20found%20that%20while%20the%0Aexamples%20often%20aligned%20well%2C%20there%20were%20a%20considerable%20proportion%20%28around%2025%25%29%0Awhich%20were%20not%20well%20aligned.%20We%20trained%20a%20neural%20network%20to%20map%20from%20the%20wind%0Afield%20to%20the%20storm%20location%3B%20in%20this%20setting%20misalignment%20in%20the%20datasets%0Aappears%20as%20%22label%20noise%22%20%28i.e.%20the%20labelled%20storm%20location%20does%20not%20correspond%0Ato%20the%20underlying%20wind%20field%29.%20We%20found%20that%20this%20neural%20network%20trained%20only%0Aon%20the%20often%20noisy%20labels%20from%20IBTrACS%20had%20a%20denoising%20effect%2C%20and%20performed%0Abetter%20than%20the%20IBTrACS%20labels%20themselves%2C%20as%20measured%20by%20human%20preferences.%0ARemarkably%2C%20this%20even%20held%20true%20for%20training%20points%2C%20on%20which%20we%20might%20have%0Aexpected%20the%20network%20to%20overfit%20to%20the%20IBTrACS%20predictions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15027v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUsing%2520Neural%2520Networks%2520for%2520Data%2520Cleaning%2520in%2520Weather%2520Datasets%26entry.906535625%3DJack%2520R.%2520P.%2520Hanslope%2520and%2520Laurence%2520Aitchison%26entry.1292438233%3D%2520%2520In%2520climate%2520science%252C%2520we%2520often%2520want%2520to%2520compare%2520across%2520different%2520datasets.%250ADifficulties%2520can%2520arise%2520in%2520doing%2520this%2520due%2520to%2520inevitable%2520mismatches%2520that%2520arise%250Abetween%2520observational%2520and%2520reanalysis%2520data%252C%2520or%2520even%2520between%2520different%250Areanalyses.%2520This%2520misalignment%2520can%2520raise%2520problems%2520for%2520any%2520work%2520that%2520seeks%2520to%250Amake%2520inferences%2520about%2520one%2520dataset%2520from%2520another.%2520We%2520considered%2520tropical%2520cyclone%250Alocation%2520as%2520an%2520example%2520task%2520with%2520one%2520dataset%2520providing%2520atmospheric%2520conditions%250A%2528ERA5%2529%2520and%2520another%2520providing%2520storm%2520tracks%2520%2528IBTrACS%2529.%2520We%2520found%2520that%2520while%2520the%250Aexamples%2520often%2520aligned%2520well%252C%2520there%2520were%2520a%2520considerable%2520proportion%2520%2528around%252025%2525%2529%250Awhich%2520were%2520not%2520well%2520aligned.%2520We%2520trained%2520a%2520neural%2520network%2520to%2520map%2520from%2520the%2520wind%250Afield%2520to%2520the%2520storm%2520location%253B%2520in%2520this%2520setting%2520misalignment%2520in%2520the%2520datasets%250Aappears%2520as%2520%2522label%2520noise%2522%2520%2528i.e.%2520the%2520labelled%2520storm%2520location%2520does%2520not%2520correspond%250Ato%2520the%2520underlying%2520wind%2520field%2529.%2520We%2520found%2520that%2520this%2520neural%2520network%2520trained%2520only%250Aon%2520the%2520often%2520noisy%2520labels%2520from%2520IBTrACS%2520had%2520a%2520denoising%2520effect%252C%2520and%2520performed%250Abetter%2520than%2520the%2520IBTrACS%2520labels%2520themselves%252C%2520as%2520measured%2520by%2520human%2520preferences.%250ARemarkably%252C%2520this%2520even%2520held%2520true%2520for%2520training%2520points%252C%2520on%2520which%2520we%2520might%2520have%250Aexpected%2520the%2520network%2520to%2520overfit%2520to%2520the%2520IBTrACS%2520predictions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15027v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20Neural%20Networks%20for%20Data%20Cleaning%20in%20Weather%20Datasets&entry.906535625=Jack%20R.%20P.%20Hanslope%20and%20Laurence%20Aitchison&entry.1292438233=%20%20In%20climate%20science%2C%20we%20often%20want%20to%20compare%20across%20different%20datasets.%0ADifficulties%20can%20arise%20in%20doing%20this%20due%20to%20inevitable%20mismatches%20that%20arise%0Abetween%20observational%20and%20reanalysis%20data%2C%20or%20even%20between%20different%0Areanalyses.%20This%20misalignment%20can%20raise%20problems%20for%20any%20work%20that%20seeks%20to%0Amake%20inferences%20about%20one%20dataset%20from%20another.%20We%20considered%20tropical%20cyclone%0Alocation%20as%20an%20example%20task%20with%20one%20dataset%20providing%20atmospheric%20conditions%0A%28ERA5%29%20and%20another%20providing%20storm%20tracks%20%28IBTrACS%29.%20We%20found%20that%20while%20the%0Aexamples%20often%20aligned%20well%2C%20there%20were%20a%20considerable%20proportion%20%28around%2025%25%29%0Awhich%20were%20not%20well%20aligned.%20We%20trained%20a%20neural%20network%20to%20map%20from%20the%20wind%0Afield%20to%20the%20storm%20location%3B%20in%20this%20setting%20misalignment%20in%20the%20datasets%0Aappears%20as%20%22label%20noise%22%20%28i.e.%20the%20labelled%20storm%20location%20does%20not%20correspond%0Ato%20the%20underlying%20wind%20field%29.%20We%20found%20that%20this%20neural%20network%20trained%20only%0Aon%20the%20often%20noisy%20labels%20from%20IBTrACS%20had%20a%20denoising%20effect%2C%20and%20performed%0Abetter%20than%20the%20IBTrACS%20labels%20themselves%2C%20as%20measured%20by%20human%20preferences.%0ARemarkably%2C%20this%20even%20held%20true%20for%20training%20points%2C%20on%20which%20we%20might%20have%0Aexpected%20the%20network%20to%20overfit%20to%20the%20IBTrACS%20predictions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15027v1&entry.124074799=Read"},
{"title": "Deep hybrid models: infer and plan in the real world", "author": "Matteo Priorelli and Ivilin Peev Stoianov", "abstract": "  Determining an optimal plan to accomplish a goal is a hard problem in\nrealistic scenarios, which often comprise dynamic and causal relationships\nbetween several entities. Although traditionally such problems have been\ntackled with optimal control and reinforcement learning, a recent\nbiologically-motivated proposal casts planning and control as an inference\nprocess. Among these new approaches, one is particularly promising: active\ninference. This new paradigm assumes that action and perception are two\ncomplementary aspects of life whereby the role of the former is to fulfill the\npredictions inferred by the latter. In this study, we present an effective\nsolution, based on active inference, to complex control tasks. The proposed\narchitecture exploits hybrid (discrete and continuous) processing to construct\na hierarchical and dynamic representation of the self and the environment,\nwhich is then used to produce a flexible plan consisting of subgoals at\ndifferent temporal scales. We evaluate this deep hybrid model on a non-trivial\ntask: reaching a moving object after having picked a moving tool. This study\nextends past work on planning as inference and advances an alternative\ndirection to optimal control and reinforcement learning.\n", "link": "http://arxiv.org/abs/2402.10088v2", "date": "2024-06-21", "relevancy": 1.6792, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6297}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5614}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20hybrid%20models%3A%20infer%20and%20plan%20in%20the%20real%20world&body=Title%3A%20Deep%20hybrid%20models%3A%20infer%20and%20plan%20in%20the%20real%20world%0AAuthor%3A%20Matteo%20Priorelli%20and%20Ivilin%20Peev%20Stoianov%0AAbstract%3A%20%20%20Determining%20an%20optimal%20plan%20to%20accomplish%20a%20goal%20is%20a%20hard%20problem%20in%0Arealistic%20scenarios%2C%20which%20often%20comprise%20dynamic%20and%20causal%20relationships%0Abetween%20several%20entities.%20Although%20traditionally%20such%20problems%20have%20been%0Atackled%20with%20optimal%20control%20and%20reinforcement%20learning%2C%20a%20recent%0Abiologically-motivated%20proposal%20casts%20planning%20and%20control%20as%20an%20inference%0Aprocess.%20Among%20these%20new%20approaches%2C%20one%20is%20particularly%20promising%3A%20active%0Ainference.%20This%20new%20paradigm%20assumes%20that%20action%20and%20perception%20are%20two%0Acomplementary%20aspects%20of%20life%20whereby%20the%20role%20of%20the%20former%20is%20to%20fulfill%20the%0Apredictions%20inferred%20by%20the%20latter.%20In%20this%20study%2C%20we%20present%20an%20effective%0Asolution%2C%20based%20on%20active%20inference%2C%20to%20complex%20control%20tasks.%20The%20proposed%0Aarchitecture%20exploits%20hybrid%20%28discrete%20and%20continuous%29%20processing%20to%20construct%0Aa%20hierarchical%20and%20dynamic%20representation%20of%20the%20self%20and%20the%20environment%2C%0Awhich%20is%20then%20used%20to%20produce%20a%20flexible%20plan%20consisting%20of%20subgoals%20at%0Adifferent%20temporal%20scales.%20We%20evaluate%20this%20deep%20hybrid%20model%20on%20a%20non-trivial%0Atask%3A%20reaching%20a%20moving%20object%20after%20having%20picked%20a%20moving%20tool.%20This%20study%0Aextends%20past%20work%20on%20planning%20as%20inference%20and%20advances%20an%20alternative%0Adirection%20to%20optimal%20control%20and%20reinforcement%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10088v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520hybrid%2520models%253A%2520infer%2520and%2520plan%2520in%2520the%2520real%2520world%26entry.906535625%3DMatteo%2520Priorelli%2520and%2520Ivilin%2520Peev%2520Stoianov%26entry.1292438233%3D%2520%2520Determining%2520an%2520optimal%2520plan%2520to%2520accomplish%2520a%2520goal%2520is%2520a%2520hard%2520problem%2520in%250Arealistic%2520scenarios%252C%2520which%2520often%2520comprise%2520dynamic%2520and%2520causal%2520relationships%250Abetween%2520several%2520entities.%2520Although%2520traditionally%2520such%2520problems%2520have%2520been%250Atackled%2520with%2520optimal%2520control%2520and%2520reinforcement%2520learning%252C%2520a%2520recent%250Abiologically-motivated%2520proposal%2520casts%2520planning%2520and%2520control%2520as%2520an%2520inference%250Aprocess.%2520Among%2520these%2520new%2520approaches%252C%2520one%2520is%2520particularly%2520promising%253A%2520active%250Ainference.%2520This%2520new%2520paradigm%2520assumes%2520that%2520action%2520and%2520perception%2520are%2520two%250Acomplementary%2520aspects%2520of%2520life%2520whereby%2520the%2520role%2520of%2520the%2520former%2520is%2520to%2520fulfill%2520the%250Apredictions%2520inferred%2520by%2520the%2520latter.%2520In%2520this%2520study%252C%2520we%2520present%2520an%2520effective%250Asolution%252C%2520based%2520on%2520active%2520inference%252C%2520to%2520complex%2520control%2520tasks.%2520The%2520proposed%250Aarchitecture%2520exploits%2520hybrid%2520%2528discrete%2520and%2520continuous%2529%2520processing%2520to%2520construct%250Aa%2520hierarchical%2520and%2520dynamic%2520representation%2520of%2520the%2520self%2520and%2520the%2520environment%252C%250Awhich%2520is%2520then%2520used%2520to%2520produce%2520a%2520flexible%2520plan%2520consisting%2520of%2520subgoals%2520at%250Adifferent%2520temporal%2520scales.%2520We%2520evaluate%2520this%2520deep%2520hybrid%2520model%2520on%2520a%2520non-trivial%250Atask%253A%2520reaching%2520a%2520moving%2520object%2520after%2520having%2520picked%2520a%2520moving%2520tool.%2520This%2520study%250Aextends%2520past%2520work%2520on%2520planning%2520as%2520inference%2520and%2520advances%2520an%2520alternative%250Adirection%2520to%2520optimal%2520control%2520and%2520reinforcement%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.10088v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20hybrid%20models%3A%20infer%20and%20plan%20in%20the%20real%20world&entry.906535625=Matteo%20Priorelli%20and%20Ivilin%20Peev%20Stoianov&entry.1292438233=%20%20Determining%20an%20optimal%20plan%20to%20accomplish%20a%20goal%20is%20a%20hard%20problem%20in%0Arealistic%20scenarios%2C%20which%20often%20comprise%20dynamic%20and%20causal%20relationships%0Abetween%20several%20entities.%20Although%20traditionally%20such%20problems%20have%20been%0Atackled%20with%20optimal%20control%20and%20reinforcement%20learning%2C%20a%20recent%0Abiologically-motivated%20proposal%20casts%20planning%20and%20control%20as%20an%20inference%0Aprocess.%20Among%20these%20new%20approaches%2C%20one%20is%20particularly%20promising%3A%20active%0Ainference.%20This%20new%20paradigm%20assumes%20that%20action%20and%20perception%20are%20two%0Acomplementary%20aspects%20of%20life%20whereby%20the%20role%20of%20the%20former%20is%20to%20fulfill%20the%0Apredictions%20inferred%20by%20the%20latter.%20In%20this%20study%2C%20we%20present%20an%20effective%0Asolution%2C%20based%20on%20active%20inference%2C%20to%20complex%20control%20tasks.%20The%20proposed%0Aarchitecture%20exploits%20hybrid%20%28discrete%20and%20continuous%29%20processing%20to%20construct%0Aa%20hierarchical%20and%20dynamic%20representation%20of%20the%20self%20and%20the%20environment%2C%0Awhich%20is%20then%20used%20to%20produce%20a%20flexible%20plan%20consisting%20of%20subgoals%20at%0Adifferent%20temporal%20scales.%20We%20evaluate%20this%20deep%20hybrid%20model%20on%20a%20non-trivial%0Atask%3A%20reaching%20a%20moving%20object%20after%20having%20picked%20a%20moving%20tool.%20This%20study%0Aextends%20past%20work%20on%20planning%20as%20inference%20and%20advances%20an%20alternative%0Adirection%20to%20optimal%20control%20and%20reinforcement%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10088v2&entry.124074799=Read"},
{"title": "LLIC: Large Receptive Field Transform Coding with Adaptive Weights for\n  Learned Image Compression", "author": "Wei Jiang and Peirong Ning and Jiayu Yang and Yongqi Zhai and Feng Gao and Ronggang Wang", "abstract": "  The effective receptive field (ERF) plays an important role in transform\ncoding, which determines how much redundancy can be removed during transform\nand how many spatial priors can be utilized to synthesize textures during\ninverse transform. Existing methods rely on stacks of small kernels, whose ERFs\nremain insufficiently large, or heavy non-local attention mechanisms, which\nlimit the potential of high-resolution image coding. To tackle this issue, we\npropose Large Receptive Field Transform Coding with Adaptive Weights for\nLearned Image Compression (LLIC). Specifically, for the first time in the\nlearned image compression community, we introduce a few large kernelbased\ndepth-wise convolutions to reduce more redundancy while maintaining modest\ncomplexity. Due to the wide range of image diversity, we further propose a\nmechanism to augment convolution adaptability through the self-conditioned\ngeneration of weights. The large kernels cooperate with non-linear embedding\nand gate mechanisms for better expressiveness and lighter pointwise\ninteractions. Our investigation extends to refined training methods that unlock\nthe full potential of these large kernels. Moreover, to promote more dynamic\ninter-channel interactions, we introduce an adaptive channel-wise bit\nallocation strategy that autonomously generates channel importance factors in a\nself-conditioned manner. To demonstrate the effectiveness of the proposed\ntransform coding, we align the entropy model to compare with existing transform\nmethods and obtain models LLIC-STF, LLIC-ELIC, and LLIC-TCM. Extensive\nexperiments demonstrate that our proposed LLIC models have significant\nimprovements over the corresponding baselines and reduce the BD-Rate by 9.49%,\n9.47%, 10.94% on Kodak over VTM-17.0 Intra, respectively. Our LLIC models\nachieve state-of-the-art performances and better trade-offs between performance\nand complexity.\n", "link": "http://arxiv.org/abs/2304.09571v9", "date": "2024-06-21", "relevancy": 1.6713, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5725}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.544}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5317}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLIC%3A%20Large%20Receptive%20Field%20Transform%20Coding%20with%20Adaptive%20Weights%20for%0A%20%20Learned%20Image%20Compression&body=Title%3A%20LLIC%3A%20Large%20Receptive%20Field%20Transform%20Coding%20with%20Adaptive%20Weights%20for%0A%20%20Learned%20Image%20Compression%0AAuthor%3A%20Wei%20Jiang%20and%20Peirong%20Ning%20and%20Jiayu%20Yang%20and%20Yongqi%20Zhai%20and%20Feng%20Gao%20and%20Ronggang%20Wang%0AAbstract%3A%20%20%20The%20effective%20receptive%20field%20%28ERF%29%20plays%20an%20important%20role%20in%20transform%0Acoding%2C%20which%20determines%20how%20much%20redundancy%20can%20be%20removed%20during%20transform%0Aand%20how%20many%20spatial%20priors%20can%20be%20utilized%20to%20synthesize%20textures%20during%0Ainverse%20transform.%20Existing%20methods%20rely%20on%20stacks%20of%20small%20kernels%2C%20whose%20ERFs%0Aremain%20insufficiently%20large%2C%20or%20heavy%20non-local%20attention%20mechanisms%2C%20which%0Alimit%20the%20potential%20of%20high-resolution%20image%20coding.%20To%20tackle%20this%20issue%2C%20we%0Apropose%20Large%20Receptive%20Field%20Transform%20Coding%20with%20Adaptive%20Weights%20for%0ALearned%20Image%20Compression%20%28LLIC%29.%20Specifically%2C%20for%20the%20first%20time%20in%20the%0Alearned%20image%20compression%20community%2C%20we%20introduce%20a%20few%20large%20kernelbased%0Adepth-wise%20convolutions%20to%20reduce%20more%20redundancy%20while%20maintaining%20modest%0Acomplexity.%20Due%20to%20the%20wide%20range%20of%20image%20diversity%2C%20we%20further%20propose%20a%0Amechanism%20to%20augment%20convolution%20adaptability%20through%20the%20self-conditioned%0Ageneration%20of%20weights.%20The%20large%20kernels%20cooperate%20with%20non-linear%20embedding%0Aand%20gate%20mechanisms%20for%20better%20expressiveness%20and%20lighter%20pointwise%0Ainteractions.%20Our%20investigation%20extends%20to%20refined%20training%20methods%20that%20unlock%0Athe%20full%20potential%20of%20these%20large%20kernels.%20Moreover%2C%20to%20promote%20more%20dynamic%0Ainter-channel%20interactions%2C%20we%20introduce%20an%20adaptive%20channel-wise%20bit%0Aallocation%20strategy%20that%20autonomously%20generates%20channel%20importance%20factors%20in%20a%0Aself-conditioned%20manner.%20To%20demonstrate%20the%20effectiveness%20of%20the%20proposed%0Atransform%20coding%2C%20we%20align%20the%20entropy%20model%20to%20compare%20with%20existing%20transform%0Amethods%20and%20obtain%20models%20LLIC-STF%2C%20LLIC-ELIC%2C%20and%20LLIC-TCM.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20proposed%20LLIC%20models%20have%20significant%0Aimprovements%20over%20the%20corresponding%20baselines%20and%20reduce%20the%20BD-Rate%20by%209.49%25%2C%0A9.47%25%2C%2010.94%25%20on%20Kodak%20over%20VTM-17.0%20Intra%2C%20respectively.%20Our%20LLIC%20models%0Aachieve%20state-of-the-art%20performances%20and%20better%20trade-offs%20between%20performance%0Aand%20complexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.09571v9%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLIC%253A%2520Large%2520Receptive%2520Field%2520Transform%2520Coding%2520with%2520Adaptive%2520Weights%2520for%250A%2520%2520Learned%2520Image%2520Compression%26entry.906535625%3DWei%2520Jiang%2520and%2520Peirong%2520Ning%2520and%2520Jiayu%2520Yang%2520and%2520Yongqi%2520Zhai%2520and%2520Feng%2520Gao%2520and%2520Ronggang%2520Wang%26entry.1292438233%3D%2520%2520The%2520effective%2520receptive%2520field%2520%2528ERF%2529%2520plays%2520an%2520important%2520role%2520in%2520transform%250Acoding%252C%2520which%2520determines%2520how%2520much%2520redundancy%2520can%2520be%2520removed%2520during%2520transform%250Aand%2520how%2520many%2520spatial%2520priors%2520can%2520be%2520utilized%2520to%2520synthesize%2520textures%2520during%250Ainverse%2520transform.%2520Existing%2520methods%2520rely%2520on%2520stacks%2520of%2520small%2520kernels%252C%2520whose%2520ERFs%250Aremain%2520insufficiently%2520large%252C%2520or%2520heavy%2520non-local%2520attention%2520mechanisms%252C%2520which%250Alimit%2520the%2520potential%2520of%2520high-resolution%2520image%2520coding.%2520To%2520tackle%2520this%2520issue%252C%2520we%250Apropose%2520Large%2520Receptive%2520Field%2520Transform%2520Coding%2520with%2520Adaptive%2520Weights%2520for%250ALearned%2520Image%2520Compression%2520%2528LLIC%2529.%2520Specifically%252C%2520for%2520the%2520first%2520time%2520in%2520the%250Alearned%2520image%2520compression%2520community%252C%2520we%2520introduce%2520a%2520few%2520large%2520kernelbased%250Adepth-wise%2520convolutions%2520to%2520reduce%2520more%2520redundancy%2520while%2520maintaining%2520modest%250Acomplexity.%2520Due%2520to%2520the%2520wide%2520range%2520of%2520image%2520diversity%252C%2520we%2520further%2520propose%2520a%250Amechanism%2520to%2520augment%2520convolution%2520adaptability%2520through%2520the%2520self-conditioned%250Ageneration%2520of%2520weights.%2520The%2520large%2520kernels%2520cooperate%2520with%2520non-linear%2520embedding%250Aand%2520gate%2520mechanisms%2520for%2520better%2520expressiveness%2520and%2520lighter%2520pointwise%250Ainteractions.%2520Our%2520investigation%2520extends%2520to%2520refined%2520training%2520methods%2520that%2520unlock%250Athe%2520full%2520potential%2520of%2520these%2520large%2520kernels.%2520Moreover%252C%2520to%2520promote%2520more%2520dynamic%250Ainter-channel%2520interactions%252C%2520we%2520introduce%2520an%2520adaptive%2520channel-wise%2520bit%250Aallocation%2520strategy%2520that%2520autonomously%2520generates%2520channel%2520importance%2520factors%2520in%2520a%250Aself-conditioned%2520manner.%2520To%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%250Atransform%2520coding%252C%2520we%2520align%2520the%2520entropy%2520model%2520to%2520compare%2520with%2520existing%2520transform%250Amethods%2520and%2520obtain%2520models%2520LLIC-STF%252C%2520LLIC-ELIC%252C%2520and%2520LLIC-TCM.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520proposed%2520LLIC%2520models%2520have%2520significant%250Aimprovements%2520over%2520the%2520corresponding%2520baselines%2520and%2520reduce%2520the%2520BD-Rate%2520by%25209.49%2525%252C%250A9.47%2525%252C%252010.94%2525%2520on%2520Kodak%2520over%2520VTM-17.0%2520Intra%252C%2520respectively.%2520Our%2520LLIC%2520models%250Aachieve%2520state-of-the-art%2520performances%2520and%2520better%2520trade-offs%2520between%2520performance%250Aand%2520complexity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.09571v9%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLIC%3A%20Large%20Receptive%20Field%20Transform%20Coding%20with%20Adaptive%20Weights%20for%0A%20%20Learned%20Image%20Compression&entry.906535625=Wei%20Jiang%20and%20Peirong%20Ning%20and%20Jiayu%20Yang%20and%20Yongqi%20Zhai%20and%20Feng%20Gao%20and%20Ronggang%20Wang&entry.1292438233=%20%20The%20effective%20receptive%20field%20%28ERF%29%20plays%20an%20important%20role%20in%20transform%0Acoding%2C%20which%20determines%20how%20much%20redundancy%20can%20be%20removed%20during%20transform%0Aand%20how%20many%20spatial%20priors%20can%20be%20utilized%20to%20synthesize%20textures%20during%0Ainverse%20transform.%20Existing%20methods%20rely%20on%20stacks%20of%20small%20kernels%2C%20whose%20ERFs%0Aremain%20insufficiently%20large%2C%20or%20heavy%20non-local%20attention%20mechanisms%2C%20which%0Alimit%20the%20potential%20of%20high-resolution%20image%20coding.%20To%20tackle%20this%20issue%2C%20we%0Apropose%20Large%20Receptive%20Field%20Transform%20Coding%20with%20Adaptive%20Weights%20for%0ALearned%20Image%20Compression%20%28LLIC%29.%20Specifically%2C%20for%20the%20first%20time%20in%20the%0Alearned%20image%20compression%20community%2C%20we%20introduce%20a%20few%20large%20kernelbased%0Adepth-wise%20convolutions%20to%20reduce%20more%20redundancy%20while%20maintaining%20modest%0Acomplexity.%20Due%20to%20the%20wide%20range%20of%20image%20diversity%2C%20we%20further%20propose%20a%0Amechanism%20to%20augment%20convolution%20adaptability%20through%20the%20self-conditioned%0Ageneration%20of%20weights.%20The%20large%20kernels%20cooperate%20with%20non-linear%20embedding%0Aand%20gate%20mechanisms%20for%20better%20expressiveness%20and%20lighter%20pointwise%0Ainteractions.%20Our%20investigation%20extends%20to%20refined%20training%20methods%20that%20unlock%0Athe%20full%20potential%20of%20these%20large%20kernels.%20Moreover%2C%20to%20promote%20more%20dynamic%0Ainter-channel%20interactions%2C%20we%20introduce%20an%20adaptive%20channel-wise%20bit%0Aallocation%20strategy%20that%20autonomously%20generates%20channel%20importance%20factors%20in%20a%0Aself-conditioned%20manner.%20To%20demonstrate%20the%20effectiveness%20of%20the%20proposed%0Atransform%20coding%2C%20we%20align%20the%20entropy%20model%20to%20compare%20with%20existing%20transform%0Amethods%20and%20obtain%20models%20LLIC-STF%2C%20LLIC-ELIC%2C%20and%20LLIC-TCM.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20proposed%20LLIC%20models%20have%20significant%0Aimprovements%20over%20the%20corresponding%20baselines%20and%20reduce%20the%20BD-Rate%20by%209.49%25%2C%0A9.47%25%2C%2010.94%25%20on%20Kodak%20over%20VTM-17.0%20Intra%2C%20respectively.%20Our%20LLIC%20models%0Aachieve%20state-of-the-art%20performances%20and%20better%20trade-offs%20between%20performance%0Aand%20complexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.09571v9&entry.124074799=Read"},
{"title": "HLQ: Fast and Efficient Backpropagation via Hadamard Low-rank\n  Quantization", "author": "Seonggon Kim and Eunhyeok Park", "abstract": "  With the rapid increase in model size and the growing importance of various\nfine-tuning applications, lightweight training has become crucial. Since the\nbackward pass is twice as expensive as the forward pass, optimizing\nbackpropagation is particularly important. However, modifications to this\nprocess can lead to suboptimal convergence, so training optimization should\nminimize perturbations, which is a highly challenging task. In this study, we\nintroduce a novel optimization strategy called Hadamard Low-rank Quantization\n(HLQ), focusing on reducing the cost of backpropagation in convolutional and\nlinear layers. We first analyze the sensitivity of gradient computation with\nrespect to activation and weight, and judiciously design the HLQ pipeline to\napply 4-bit Hadamard quantization to the activation gradient and Hadamard\nlow-rank approximation to the weight gradient. This combination was found to be\nthe best for maximizing benefits, and our extensive experiments demonstrate the\noutstanding performance of HLQ in both training from scratch and fine-tuning,\nachieving significant memory savings and acceleration on real GPUs with\nnegligible quality degradation.\n", "link": "http://arxiv.org/abs/2406.15102v1", "date": "2024-06-21", "relevancy": 1.6678, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5748}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5484}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HLQ%3A%20Fast%20and%20Efficient%20Backpropagation%20via%20Hadamard%20Low-rank%0A%20%20Quantization&body=Title%3A%20HLQ%3A%20Fast%20and%20Efficient%20Backpropagation%20via%20Hadamard%20Low-rank%0A%20%20Quantization%0AAuthor%3A%20Seonggon%20Kim%20and%20Eunhyeok%20Park%0AAbstract%3A%20%20%20With%20the%20rapid%20increase%20in%20model%20size%20and%20the%20growing%20importance%20of%20various%0Afine-tuning%20applications%2C%20lightweight%20training%20has%20become%20crucial.%20Since%20the%0Abackward%20pass%20is%20twice%20as%20expensive%20as%20the%20forward%20pass%2C%20optimizing%0Abackpropagation%20is%20particularly%20important.%20However%2C%20modifications%20to%20this%0Aprocess%20can%20lead%20to%20suboptimal%20convergence%2C%20so%20training%20optimization%20should%0Aminimize%20perturbations%2C%20which%20is%20a%20highly%20challenging%20task.%20In%20this%20study%2C%20we%0Aintroduce%20a%20novel%20optimization%20strategy%20called%20Hadamard%20Low-rank%20Quantization%0A%28HLQ%29%2C%20focusing%20on%20reducing%20the%20cost%20of%20backpropagation%20in%20convolutional%20and%0Alinear%20layers.%20We%20first%20analyze%20the%20sensitivity%20of%20gradient%20computation%20with%0Arespect%20to%20activation%20and%20weight%2C%20and%20judiciously%20design%20the%20HLQ%20pipeline%20to%0Aapply%204-bit%20Hadamard%20quantization%20to%20the%20activation%20gradient%20and%20Hadamard%0Alow-rank%20approximation%20to%20the%20weight%20gradient.%20This%20combination%20was%20found%20to%20be%0Athe%20best%20for%20maximizing%20benefits%2C%20and%20our%20extensive%20experiments%20demonstrate%20the%0Aoutstanding%20performance%20of%20HLQ%20in%20both%20training%20from%20scratch%20and%20fine-tuning%2C%0Aachieving%20significant%20memory%20savings%20and%20acceleration%20on%20real%20GPUs%20with%0Anegligible%20quality%20degradation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15102v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHLQ%253A%2520Fast%2520and%2520Efficient%2520Backpropagation%2520via%2520Hadamard%2520Low-rank%250A%2520%2520Quantization%26entry.906535625%3DSeonggon%2520Kim%2520and%2520Eunhyeok%2520Park%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520increase%2520in%2520model%2520size%2520and%2520the%2520growing%2520importance%2520of%2520various%250Afine-tuning%2520applications%252C%2520lightweight%2520training%2520has%2520become%2520crucial.%2520Since%2520the%250Abackward%2520pass%2520is%2520twice%2520as%2520expensive%2520as%2520the%2520forward%2520pass%252C%2520optimizing%250Abackpropagation%2520is%2520particularly%2520important.%2520However%252C%2520modifications%2520to%2520this%250Aprocess%2520can%2520lead%2520to%2520suboptimal%2520convergence%252C%2520so%2520training%2520optimization%2520should%250Aminimize%2520perturbations%252C%2520which%2520is%2520a%2520highly%2520challenging%2520task.%2520In%2520this%2520study%252C%2520we%250Aintroduce%2520a%2520novel%2520optimization%2520strategy%2520called%2520Hadamard%2520Low-rank%2520Quantization%250A%2528HLQ%2529%252C%2520focusing%2520on%2520reducing%2520the%2520cost%2520of%2520backpropagation%2520in%2520convolutional%2520and%250Alinear%2520layers.%2520We%2520first%2520analyze%2520the%2520sensitivity%2520of%2520gradient%2520computation%2520with%250Arespect%2520to%2520activation%2520and%2520weight%252C%2520and%2520judiciously%2520design%2520the%2520HLQ%2520pipeline%2520to%250Aapply%25204-bit%2520Hadamard%2520quantization%2520to%2520the%2520activation%2520gradient%2520and%2520Hadamard%250Alow-rank%2520approximation%2520to%2520the%2520weight%2520gradient.%2520This%2520combination%2520was%2520found%2520to%2520be%250Athe%2520best%2520for%2520maximizing%2520benefits%252C%2520and%2520our%2520extensive%2520experiments%2520demonstrate%2520the%250Aoutstanding%2520performance%2520of%2520HLQ%2520in%2520both%2520training%2520from%2520scratch%2520and%2520fine-tuning%252C%250Aachieving%2520significant%2520memory%2520savings%2520and%2520acceleration%2520on%2520real%2520GPUs%2520with%250Anegligible%2520quality%2520degradation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15102v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HLQ%3A%20Fast%20and%20Efficient%20Backpropagation%20via%20Hadamard%20Low-rank%0A%20%20Quantization&entry.906535625=Seonggon%20Kim%20and%20Eunhyeok%20Park&entry.1292438233=%20%20With%20the%20rapid%20increase%20in%20model%20size%20and%20the%20growing%20importance%20of%20various%0Afine-tuning%20applications%2C%20lightweight%20training%20has%20become%20crucial.%20Since%20the%0Abackward%20pass%20is%20twice%20as%20expensive%20as%20the%20forward%20pass%2C%20optimizing%0Abackpropagation%20is%20particularly%20important.%20However%2C%20modifications%20to%20this%0Aprocess%20can%20lead%20to%20suboptimal%20convergence%2C%20so%20training%20optimization%20should%0Aminimize%20perturbations%2C%20which%20is%20a%20highly%20challenging%20task.%20In%20this%20study%2C%20we%0Aintroduce%20a%20novel%20optimization%20strategy%20called%20Hadamard%20Low-rank%20Quantization%0A%28HLQ%29%2C%20focusing%20on%20reducing%20the%20cost%20of%20backpropagation%20in%20convolutional%20and%0Alinear%20layers.%20We%20first%20analyze%20the%20sensitivity%20of%20gradient%20computation%20with%0Arespect%20to%20activation%20and%20weight%2C%20and%20judiciously%20design%20the%20HLQ%20pipeline%20to%0Aapply%204-bit%20Hadamard%20quantization%20to%20the%20activation%20gradient%20and%20Hadamard%0Alow-rank%20approximation%20to%20the%20weight%20gradient.%20This%20combination%20was%20found%20to%20be%0Athe%20best%20for%20maximizing%20benefits%2C%20and%20our%20extensive%20experiments%20demonstrate%20the%0Aoutstanding%20performance%20of%20HLQ%20in%20both%20training%20from%20scratch%20and%20fine-tuning%2C%0Aachieving%20significant%20memory%20savings%20and%20acceleration%20on%20real%20GPUs%20with%0Anegligible%20quality%20degradation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15102v1&entry.124074799=Read"},
{"title": "FreeMotion: MoCap-Free Human Motion Synthesis with Multimodal Large\n  Language Models", "author": "Zhikai Zhang and Yitang Li and Haofeng Huang and Mingxian Lin and Li Yi", "abstract": "  Human motion synthesis is a fundamental task in computer animation. Despite\nrecent progress in this field utilizing deep learning and motion capture data,\nexisting methods are always limited to specific motion categories,\nenvironments, and styles. This poor generalizability can be partially\nattributed to the difficulty and expense of collecting large-scale and\nhigh-quality motion data. At the same time, foundation models trained with\ninternet-scale image and text data have demonstrated surprising world knowledge\nand reasoning ability for various downstream tasks. Utilizing these foundation\nmodels may help with human motion synthesis, which some recent works have\nsuperficially explored. However, these methods didn't fully unveil the\nfoundation models' potential for this task and only support several simple\nactions and environments. In this paper, we for the first time, without any\nmotion data, explore open-set human motion synthesis using natural language\ninstructions as user control signals based on MLLMs across any motion task and\nenvironment. Our framework can be split into two stages: 1) sequential keyframe\ngeneration by utilizing MLLMs as a keyframe designer and animator; 2) motion\nfilling between keyframes through interpolation and motion tracking. Our method\ncan achieve general human motion synthesis for many downstream tasks. The\npromising results demonstrate the worth of mocap-free human motion synthesis\naided by MLLMs and pave the way for future research.\n", "link": "http://arxiv.org/abs/2406.10740v2", "date": "2024-06-21", "relevancy": 1.666, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5706}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5592}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FreeMotion%3A%20MoCap-Free%20Human%20Motion%20Synthesis%20with%20Multimodal%20Large%0A%20%20Language%20Models&body=Title%3A%20FreeMotion%3A%20MoCap-Free%20Human%20Motion%20Synthesis%20with%20Multimodal%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Zhikai%20Zhang%20and%20Yitang%20Li%20and%20Haofeng%20Huang%20and%20Mingxian%20Lin%20and%20Li%20Yi%0AAbstract%3A%20%20%20Human%20motion%20synthesis%20is%20a%20fundamental%20task%20in%20computer%20animation.%20Despite%0Arecent%20progress%20in%20this%20field%20utilizing%20deep%20learning%20and%20motion%20capture%20data%2C%0Aexisting%20methods%20are%20always%20limited%20to%20specific%20motion%20categories%2C%0Aenvironments%2C%20and%20styles.%20This%20poor%20generalizability%20can%20be%20partially%0Aattributed%20to%20the%20difficulty%20and%20expense%20of%20collecting%20large-scale%20and%0Ahigh-quality%20motion%20data.%20At%20the%20same%20time%2C%20foundation%20models%20trained%20with%0Ainternet-scale%20image%20and%20text%20data%20have%20demonstrated%20surprising%20world%20knowledge%0Aand%20reasoning%20ability%20for%20various%20downstream%20tasks.%20Utilizing%20these%20foundation%0Amodels%20may%20help%20with%20human%20motion%20synthesis%2C%20which%20some%20recent%20works%20have%0Asuperficially%20explored.%20However%2C%20these%20methods%20didn%27t%20fully%20unveil%20the%0Afoundation%20models%27%20potential%20for%20this%20task%20and%20only%20support%20several%20simple%0Aactions%20and%20environments.%20In%20this%20paper%2C%20we%20for%20the%20first%20time%2C%20without%20any%0Amotion%20data%2C%20explore%20open-set%20human%20motion%20synthesis%20using%20natural%20language%0Ainstructions%20as%20user%20control%20signals%20based%20on%20MLLMs%20across%20any%20motion%20task%20and%0Aenvironment.%20Our%20framework%20can%20be%20split%20into%20two%20stages%3A%201%29%20sequential%20keyframe%0Ageneration%20by%20utilizing%20MLLMs%20as%20a%20keyframe%20designer%20and%20animator%3B%202%29%20motion%0Afilling%20between%20keyframes%20through%20interpolation%20and%20motion%20tracking.%20Our%20method%0Acan%20achieve%20general%20human%20motion%20synthesis%20for%20many%20downstream%20tasks.%20The%0Apromising%20results%20demonstrate%20the%20worth%20of%20mocap-free%20human%20motion%20synthesis%0Aaided%20by%20MLLMs%20and%20pave%20the%20way%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10740v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFreeMotion%253A%2520MoCap-Free%2520Human%2520Motion%2520Synthesis%2520with%2520Multimodal%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DZhikai%2520Zhang%2520and%2520Yitang%2520Li%2520and%2520Haofeng%2520Huang%2520and%2520Mingxian%2520Lin%2520and%2520Li%2520Yi%26entry.1292438233%3D%2520%2520Human%2520motion%2520synthesis%2520is%2520a%2520fundamental%2520task%2520in%2520computer%2520animation.%2520Despite%250Arecent%2520progress%2520in%2520this%2520field%2520utilizing%2520deep%2520learning%2520and%2520motion%2520capture%2520data%252C%250Aexisting%2520methods%2520are%2520always%2520limited%2520to%2520specific%2520motion%2520categories%252C%250Aenvironments%252C%2520and%2520styles.%2520This%2520poor%2520generalizability%2520can%2520be%2520partially%250Aattributed%2520to%2520the%2520difficulty%2520and%2520expense%2520of%2520collecting%2520large-scale%2520and%250Ahigh-quality%2520motion%2520data.%2520At%2520the%2520same%2520time%252C%2520foundation%2520models%2520trained%2520with%250Ainternet-scale%2520image%2520and%2520text%2520data%2520have%2520demonstrated%2520surprising%2520world%2520knowledge%250Aand%2520reasoning%2520ability%2520for%2520various%2520downstream%2520tasks.%2520Utilizing%2520these%2520foundation%250Amodels%2520may%2520help%2520with%2520human%2520motion%2520synthesis%252C%2520which%2520some%2520recent%2520works%2520have%250Asuperficially%2520explored.%2520However%252C%2520these%2520methods%2520didn%2527t%2520fully%2520unveil%2520the%250Afoundation%2520models%2527%2520potential%2520for%2520this%2520task%2520and%2520only%2520support%2520several%2520simple%250Aactions%2520and%2520environments.%2520In%2520this%2520paper%252C%2520we%2520for%2520the%2520first%2520time%252C%2520without%2520any%250Amotion%2520data%252C%2520explore%2520open-set%2520human%2520motion%2520synthesis%2520using%2520natural%2520language%250Ainstructions%2520as%2520user%2520control%2520signals%2520based%2520on%2520MLLMs%2520across%2520any%2520motion%2520task%2520and%250Aenvironment.%2520Our%2520framework%2520can%2520be%2520split%2520into%2520two%2520stages%253A%25201%2529%2520sequential%2520keyframe%250Ageneration%2520by%2520utilizing%2520MLLMs%2520as%2520a%2520keyframe%2520designer%2520and%2520animator%253B%25202%2529%2520motion%250Afilling%2520between%2520keyframes%2520through%2520interpolation%2520and%2520motion%2520tracking.%2520Our%2520method%250Acan%2520achieve%2520general%2520human%2520motion%2520synthesis%2520for%2520many%2520downstream%2520tasks.%2520The%250Apromising%2520results%2520demonstrate%2520the%2520worth%2520of%2520mocap-free%2520human%2520motion%2520synthesis%250Aaided%2520by%2520MLLMs%2520and%2520pave%2520the%2520way%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10740v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FreeMotion%3A%20MoCap-Free%20Human%20Motion%20Synthesis%20with%20Multimodal%20Large%0A%20%20Language%20Models&entry.906535625=Zhikai%20Zhang%20and%20Yitang%20Li%20and%20Haofeng%20Huang%20and%20Mingxian%20Lin%20and%20Li%20Yi&entry.1292438233=%20%20Human%20motion%20synthesis%20is%20a%20fundamental%20task%20in%20computer%20animation.%20Despite%0Arecent%20progress%20in%20this%20field%20utilizing%20deep%20learning%20and%20motion%20capture%20data%2C%0Aexisting%20methods%20are%20always%20limited%20to%20specific%20motion%20categories%2C%0Aenvironments%2C%20and%20styles.%20This%20poor%20generalizability%20can%20be%20partially%0Aattributed%20to%20the%20difficulty%20and%20expense%20of%20collecting%20large-scale%20and%0Ahigh-quality%20motion%20data.%20At%20the%20same%20time%2C%20foundation%20models%20trained%20with%0Ainternet-scale%20image%20and%20text%20data%20have%20demonstrated%20surprising%20world%20knowledge%0Aand%20reasoning%20ability%20for%20various%20downstream%20tasks.%20Utilizing%20these%20foundation%0Amodels%20may%20help%20with%20human%20motion%20synthesis%2C%20which%20some%20recent%20works%20have%0Asuperficially%20explored.%20However%2C%20these%20methods%20didn%27t%20fully%20unveil%20the%0Afoundation%20models%27%20potential%20for%20this%20task%20and%20only%20support%20several%20simple%0Aactions%20and%20environments.%20In%20this%20paper%2C%20we%20for%20the%20first%20time%2C%20without%20any%0Amotion%20data%2C%20explore%20open-set%20human%20motion%20synthesis%20using%20natural%20language%0Ainstructions%20as%20user%20control%20signals%20based%20on%20MLLMs%20across%20any%20motion%20task%20and%0Aenvironment.%20Our%20framework%20can%20be%20split%20into%20two%20stages%3A%201%29%20sequential%20keyframe%0Ageneration%20by%20utilizing%20MLLMs%20as%20a%20keyframe%20designer%20and%20animator%3B%202%29%20motion%0Afilling%20between%20keyframes%20through%20interpolation%20and%20motion%20tracking.%20Our%20method%0Acan%20achieve%20general%20human%20motion%20synthesis%20for%20many%20downstream%20tasks.%20The%0Apromising%20results%20demonstrate%20the%20worth%20of%20mocap-free%20human%20motion%20synthesis%0Aaided%20by%20MLLMs%20and%20pave%20the%20way%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10740v2&entry.124074799=Read"},
{"title": "Enhancing Human-Robot Collaborative Assembly in Manufacturing Systems\n  Using Large Language Models", "author": "Jonghan Lim and Sujani Patel and Alex Evans and John Pimley and Yifei Li and Ilya Kovalenko", "abstract": "  The development of human-robot collaboration has the ability to improve\nmanufacturing system performance by leveraging the unique strengths of both\nhumans and robots. On the shop floor, human operators contribute with their\nadaptability and flexibility in dynamic situations, while robots provide\nprecision and the ability to perform repetitive tasks. However, the\ncommunication gap between human operators and robots limits the collaboration\nand coordination of human-robot teams in manufacturing systems. Our research\npresents a human-robot collaborative assembly framework that utilizes a large\nlanguage model for enhancing communication in manufacturing environments. The\nframework facilitates human-robot communication by integrating voice commands\nthrough natural language for task management. A case study for an assembly task\ndemonstrates the framework's ability to process natural language inputs and\naddress real-time assembly challenges, emphasizing adaptability to language\nvariation and efficiency in error resolution. The results suggest that large\nlanguage models have the potential to improve human-robot interaction for\ncollaborative manufacturing assembly applications.\n", "link": "http://arxiv.org/abs/2406.01915v2", "date": "2024-06-21", "relevancy": 1.5544, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5408}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5143}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Human-Robot%20Collaborative%20Assembly%20in%20Manufacturing%20Systems%0A%20%20Using%20Large%20Language%20Models&body=Title%3A%20Enhancing%20Human-Robot%20Collaborative%20Assembly%20in%20Manufacturing%20Systems%0A%20%20Using%20Large%20Language%20Models%0AAuthor%3A%20Jonghan%20Lim%20and%20Sujani%20Patel%20and%20Alex%20Evans%20and%20John%20Pimley%20and%20Yifei%20Li%20and%20Ilya%20Kovalenko%0AAbstract%3A%20%20%20The%20development%20of%20human-robot%20collaboration%20has%20the%20ability%20to%20improve%0Amanufacturing%20system%20performance%20by%20leveraging%20the%20unique%20strengths%20of%20both%0Ahumans%20and%20robots.%20On%20the%20shop%20floor%2C%20human%20operators%20contribute%20with%20their%0Aadaptability%20and%20flexibility%20in%20dynamic%20situations%2C%20while%20robots%20provide%0Aprecision%20and%20the%20ability%20to%20perform%20repetitive%20tasks.%20However%2C%20the%0Acommunication%20gap%20between%20human%20operators%20and%20robots%20limits%20the%20collaboration%0Aand%20coordination%20of%20human-robot%20teams%20in%20manufacturing%20systems.%20Our%20research%0Apresents%20a%20human-robot%20collaborative%20assembly%20framework%20that%20utilizes%20a%20large%0Alanguage%20model%20for%20enhancing%20communication%20in%20manufacturing%20environments.%20The%0Aframework%20facilitates%20human-robot%20communication%20by%20integrating%20voice%20commands%0Athrough%20natural%20language%20for%20task%20management.%20A%20case%20study%20for%20an%20assembly%20task%0Ademonstrates%20the%20framework%27s%20ability%20to%20process%20natural%20language%20inputs%20and%0Aaddress%20real-time%20assembly%20challenges%2C%20emphasizing%20adaptability%20to%20language%0Avariation%20and%20efficiency%20in%20error%20resolution.%20The%20results%20suggest%20that%20large%0Alanguage%20models%20have%20the%20potential%20to%20improve%20human-robot%20interaction%20for%0Acollaborative%20manufacturing%20assembly%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.01915v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Human-Robot%2520Collaborative%2520Assembly%2520in%2520Manufacturing%2520Systems%250A%2520%2520Using%2520Large%2520Language%2520Models%26entry.906535625%3DJonghan%2520Lim%2520and%2520Sujani%2520Patel%2520and%2520Alex%2520Evans%2520and%2520John%2520Pimley%2520and%2520Yifei%2520Li%2520and%2520Ilya%2520Kovalenko%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520human-robot%2520collaboration%2520has%2520the%2520ability%2520to%2520improve%250Amanufacturing%2520system%2520performance%2520by%2520leveraging%2520the%2520unique%2520strengths%2520of%2520both%250Ahumans%2520and%2520robots.%2520On%2520the%2520shop%2520floor%252C%2520human%2520operators%2520contribute%2520with%2520their%250Aadaptability%2520and%2520flexibility%2520in%2520dynamic%2520situations%252C%2520while%2520robots%2520provide%250Aprecision%2520and%2520the%2520ability%2520to%2520perform%2520repetitive%2520tasks.%2520However%252C%2520the%250Acommunication%2520gap%2520between%2520human%2520operators%2520and%2520robots%2520limits%2520the%2520collaboration%250Aand%2520coordination%2520of%2520human-robot%2520teams%2520in%2520manufacturing%2520systems.%2520Our%2520research%250Apresents%2520a%2520human-robot%2520collaborative%2520assembly%2520framework%2520that%2520utilizes%2520a%2520large%250Alanguage%2520model%2520for%2520enhancing%2520communication%2520in%2520manufacturing%2520environments.%2520The%250Aframework%2520facilitates%2520human-robot%2520communication%2520by%2520integrating%2520voice%2520commands%250Athrough%2520natural%2520language%2520for%2520task%2520management.%2520A%2520case%2520study%2520for%2520an%2520assembly%2520task%250Ademonstrates%2520the%2520framework%2527s%2520ability%2520to%2520process%2520natural%2520language%2520inputs%2520and%250Aaddress%2520real-time%2520assembly%2520challenges%252C%2520emphasizing%2520adaptability%2520to%2520language%250Avariation%2520and%2520efficiency%2520in%2520error%2520resolution.%2520The%2520results%2520suggest%2520that%2520large%250Alanguage%2520models%2520have%2520the%2520potential%2520to%2520improve%2520human-robot%2520interaction%2520for%250Acollaborative%2520manufacturing%2520assembly%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01915v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Human-Robot%20Collaborative%20Assembly%20in%20Manufacturing%20Systems%0A%20%20Using%20Large%20Language%20Models&entry.906535625=Jonghan%20Lim%20and%20Sujani%20Patel%20and%20Alex%20Evans%20and%20John%20Pimley%20and%20Yifei%20Li%20and%20Ilya%20Kovalenko&entry.1292438233=%20%20The%20development%20of%20human-robot%20collaboration%20has%20the%20ability%20to%20improve%0Amanufacturing%20system%20performance%20by%20leveraging%20the%20unique%20strengths%20of%20both%0Ahumans%20and%20robots.%20On%20the%20shop%20floor%2C%20human%20operators%20contribute%20with%20their%0Aadaptability%20and%20flexibility%20in%20dynamic%20situations%2C%20while%20robots%20provide%0Aprecision%20and%20the%20ability%20to%20perform%20repetitive%20tasks.%20However%2C%20the%0Acommunication%20gap%20between%20human%20operators%20and%20robots%20limits%20the%20collaboration%0Aand%20coordination%20of%20human-robot%20teams%20in%20manufacturing%20systems.%20Our%20research%0Apresents%20a%20human-robot%20collaborative%20assembly%20framework%20that%20utilizes%20a%20large%0Alanguage%20model%20for%20enhancing%20communication%20in%20manufacturing%20environments.%20The%0Aframework%20facilitates%20human-robot%20communication%20by%20integrating%20voice%20commands%0Athrough%20natural%20language%20for%20task%20management.%20A%20case%20study%20for%20an%20assembly%20task%0Ademonstrates%20the%20framework%27s%20ability%20to%20process%20natural%20language%20inputs%20and%0Aaddress%20real-time%20assembly%20challenges%2C%20emphasizing%20adaptability%20to%20language%0Avariation%20and%20efficiency%20in%20error%20resolution.%20The%20results%20suggest%20that%20large%0Alanguage%20models%20have%20the%20potential%20to%20improve%20human-robot%20interaction%20for%0Acollaborative%20manufacturing%20assembly%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.01915v2&entry.124074799=Read"},
{"title": "KalMamba: Towards Efficient Probabilistic State Space Models for RL\n  under Uncertainty", "author": "Philipp Becker and Niklas Freymuth and Gerhard Neumann", "abstract": "  Probabilistic State Space Models (SSMs) are essential for Reinforcement\nLearning (RL) from high-dimensional, partial information as they provide\nconcise representations for control. Yet, they lack the computational\nefficiency of their recent deterministic counterparts such as S4 or Mamba. We\npropose KalMamba, an efficient architecture to learn representations for RL\nthat combines the strengths of probabilistic SSMs with the scalability of\ndeterministic SSMs. KalMamba leverages Mamba to learn the dynamics parameters\nof a linear Gaussian SSM in a latent space. Inference in this latent space\namounts to standard Kalman filtering and smoothing. We realize these operations\nusing parallel associative scanning, similar to Mamba, to obtain a principled,\nhighly efficient, and scalable probabilistic SSM. Our experiments show that\nKalMamba competes with state-of-the-art SSM approaches in RL while\nsignificantly improving computational efficiency, especially on longer\ninteraction sequences.\n", "link": "http://arxiv.org/abs/2406.15131v1", "date": "2024-06-21", "relevancy": 1.5685, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5826}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5127}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KalMamba%3A%20Towards%20Efficient%20Probabilistic%20State%20Space%20Models%20for%20RL%0A%20%20under%20Uncertainty&body=Title%3A%20KalMamba%3A%20Towards%20Efficient%20Probabilistic%20State%20Space%20Models%20for%20RL%0A%20%20under%20Uncertainty%0AAuthor%3A%20Philipp%20Becker%20and%20Niklas%20Freymuth%20and%20Gerhard%20Neumann%0AAbstract%3A%20%20%20Probabilistic%20State%20Space%20Models%20%28SSMs%29%20are%20essential%20for%20Reinforcement%0ALearning%20%28RL%29%20from%20high-dimensional%2C%20partial%20information%20as%20they%20provide%0Aconcise%20representations%20for%20control.%20Yet%2C%20they%20lack%20the%20computational%0Aefficiency%20of%20their%20recent%20deterministic%20counterparts%20such%20as%20S4%20or%20Mamba.%20We%0Apropose%20KalMamba%2C%20an%20efficient%20architecture%20to%20learn%20representations%20for%20RL%0Athat%20combines%20the%20strengths%20of%20probabilistic%20SSMs%20with%20the%20scalability%20of%0Adeterministic%20SSMs.%20KalMamba%20leverages%20Mamba%20to%20learn%20the%20dynamics%20parameters%0Aof%20a%20linear%20Gaussian%20SSM%20in%20a%20latent%20space.%20Inference%20in%20this%20latent%20space%0Aamounts%20to%20standard%20Kalman%20filtering%20and%20smoothing.%20We%20realize%20these%20operations%0Ausing%20parallel%20associative%20scanning%2C%20similar%20to%20Mamba%2C%20to%20obtain%20a%20principled%2C%0Ahighly%20efficient%2C%20and%20scalable%20probabilistic%20SSM.%20Our%20experiments%20show%20that%0AKalMamba%20competes%20with%20state-of-the-art%20SSM%20approaches%20in%20RL%20while%0Asignificantly%20improving%20computational%20efficiency%2C%20especially%20on%20longer%0Ainteraction%20sequences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15131v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKalMamba%253A%2520Towards%2520Efficient%2520Probabilistic%2520State%2520Space%2520Models%2520for%2520RL%250A%2520%2520under%2520Uncertainty%26entry.906535625%3DPhilipp%2520Becker%2520and%2520Niklas%2520Freymuth%2520and%2520Gerhard%2520Neumann%26entry.1292438233%3D%2520%2520Probabilistic%2520State%2520Space%2520Models%2520%2528SSMs%2529%2520are%2520essential%2520for%2520Reinforcement%250ALearning%2520%2528RL%2529%2520from%2520high-dimensional%252C%2520partial%2520information%2520as%2520they%2520provide%250Aconcise%2520representations%2520for%2520control.%2520Yet%252C%2520they%2520lack%2520the%2520computational%250Aefficiency%2520of%2520their%2520recent%2520deterministic%2520counterparts%2520such%2520as%2520S4%2520or%2520Mamba.%2520We%250Apropose%2520KalMamba%252C%2520an%2520efficient%2520architecture%2520to%2520learn%2520representations%2520for%2520RL%250Athat%2520combines%2520the%2520strengths%2520of%2520probabilistic%2520SSMs%2520with%2520the%2520scalability%2520of%250Adeterministic%2520SSMs.%2520KalMamba%2520leverages%2520Mamba%2520to%2520learn%2520the%2520dynamics%2520parameters%250Aof%2520a%2520linear%2520Gaussian%2520SSM%2520in%2520a%2520latent%2520space.%2520Inference%2520in%2520this%2520latent%2520space%250Aamounts%2520to%2520standard%2520Kalman%2520filtering%2520and%2520smoothing.%2520We%2520realize%2520these%2520operations%250Ausing%2520parallel%2520associative%2520scanning%252C%2520similar%2520to%2520Mamba%252C%2520to%2520obtain%2520a%2520principled%252C%250Ahighly%2520efficient%252C%2520and%2520scalable%2520probabilistic%2520SSM.%2520Our%2520experiments%2520show%2520that%250AKalMamba%2520competes%2520with%2520state-of-the-art%2520SSM%2520approaches%2520in%2520RL%2520while%250Asignificantly%2520improving%2520computational%2520efficiency%252C%2520especially%2520on%2520longer%250Ainteraction%2520sequences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15131v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KalMamba%3A%20Towards%20Efficient%20Probabilistic%20State%20Space%20Models%20for%20RL%0A%20%20under%20Uncertainty&entry.906535625=Philipp%20Becker%20and%20Niklas%20Freymuth%20and%20Gerhard%20Neumann&entry.1292438233=%20%20Probabilistic%20State%20Space%20Models%20%28SSMs%29%20are%20essential%20for%20Reinforcement%0ALearning%20%28RL%29%20from%20high-dimensional%2C%20partial%20information%20as%20they%20provide%0Aconcise%20representations%20for%20control.%20Yet%2C%20they%20lack%20the%20computational%0Aefficiency%20of%20their%20recent%20deterministic%20counterparts%20such%20as%20S4%20or%20Mamba.%20We%0Apropose%20KalMamba%2C%20an%20efficient%20architecture%20to%20learn%20representations%20for%20RL%0Athat%20combines%20the%20strengths%20of%20probabilistic%20SSMs%20with%20the%20scalability%20of%0Adeterministic%20SSMs.%20KalMamba%20leverages%20Mamba%20to%20learn%20the%20dynamics%20parameters%0Aof%20a%20linear%20Gaussian%20SSM%20in%20a%20latent%20space.%20Inference%20in%20this%20latent%20space%0Aamounts%20to%20standard%20Kalman%20filtering%20and%20smoothing.%20We%20realize%20these%20operations%0Ausing%20parallel%20associative%20scanning%2C%20similar%20to%20Mamba%2C%20to%20obtain%20a%20principled%2C%0Ahighly%20efficient%2C%20and%20scalable%20probabilistic%20SSM.%20Our%20experiments%20show%20that%0AKalMamba%20competes%20with%20state-of-the-art%20SSM%20approaches%20in%20RL%20while%0Asignificantly%20improving%20computational%20efficiency%2C%20especially%20on%20longer%0Ainteraction%20sequences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15131v1&entry.124074799=Read"},
{"title": "Causal Learning in Biomedical Applications", "author": "Petr Ry\u0161av\u00fd and Xiaoyu He and Jakub Mare\u010dek", "abstract": "  We present a benchmark for methods in causal learning. Specifically, we\nconsider training a rich class of causal models from time-series data, and we\nsuggest the use of the Krebs cycle and models of metabolism more broadly.\n", "link": "http://arxiv.org/abs/2406.15189v1", "date": "2024-06-21", "relevancy": 1.2727, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4639}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4197}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.3959}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causal%20Learning%20in%20Biomedical%20Applications&body=Title%3A%20Causal%20Learning%20in%20Biomedical%20Applications%0AAuthor%3A%20Petr%20Ry%C5%A1av%C3%BD%20and%20Xiaoyu%20He%20and%20Jakub%20Mare%C4%8Dek%0AAbstract%3A%20%20%20We%20present%20a%20benchmark%20for%20methods%20in%20causal%20learning.%20Specifically%2C%20we%0Aconsider%20training%20a%20rich%20class%20of%20causal%20models%20from%20time-series%20data%2C%20and%20we%0Asuggest%20the%20use%20of%20the%20Krebs%20cycle%20and%20models%20of%20metabolism%20more%20broadly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15189v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausal%2520Learning%2520in%2520Biomedical%2520Applications%26entry.906535625%3DPetr%2520Ry%25C5%25A1av%25C3%25BD%2520and%2520Xiaoyu%2520He%2520and%2520Jakub%2520Mare%25C4%258Dek%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520benchmark%2520for%2520methods%2520in%2520causal%2520learning.%2520Specifically%252C%2520we%250Aconsider%2520training%2520a%2520rich%2520class%2520of%2520causal%2520models%2520from%2520time-series%2520data%252C%2520and%2520we%250Asuggest%2520the%2520use%2520of%2520the%2520Krebs%2520cycle%2520and%2520models%2520of%2520metabolism%2520more%2520broadly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15189v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal%20Learning%20in%20Biomedical%20Applications&entry.906535625=Petr%20Ry%C5%A1av%C3%BD%20and%20Xiaoyu%20He%20and%20Jakub%20Mare%C4%8Dek&entry.1292438233=%20%20We%20present%20a%20benchmark%20for%20methods%20in%20causal%20learning.%20Specifically%2C%20we%0Aconsider%20training%20a%20rich%20class%20of%20causal%20models%20from%20time-series%20data%2C%20and%20we%0Asuggest%20the%20use%20of%20the%20Krebs%20cycle%20and%20models%20of%20metabolism%20more%20broadly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15189v1&entry.124074799=Read"},
{"title": "Large Language Model-Enabled Multi-Agent Manufacturing Systems", "author": "Jonghan Lim and Birgit Vogel-Heuser and Ilya Kovalenko", "abstract": "  Traditional manufacturing faces challenges adapting to dynamic environments\nand quickly responding to manufacturing changes. The use of multi-agent systems\nhas improved adaptability and coordination but requires further advancements in\nrapid human instruction comprehension, operational adaptability, and\ncoordination through natural language integration. Large language models like\nGPT-3.5 and GPT-4 enhance multi-agent manufacturing systems by enabling agents\nto communicate in natural language and interpret human instructions for\ndecision-making. This research introduces a novel framework where large\nlanguage models enhance the capabilities of agents in manufacturing, making\nthem more adaptable, and capable of processing context-specific instructions. A\ncase study demonstrates the practical application of this framework, showing\nhow agents can effectively communicate, understand tasks, and execute\nmanufacturing processes, including precise G-code allocation among agents. The\nfindings highlight the importance of continuous large language model\nintegration into multi-agent manufacturing systems and the development of\nsophisticated agent communication protocols for a more flexible manufacturing\nsystem.\n", "link": "http://arxiv.org/abs/2406.01893v2", "date": "2024-06-21", "relevancy": 1.509, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5122}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5014}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Model-Enabled%20Multi-Agent%20Manufacturing%20Systems&body=Title%3A%20Large%20Language%20Model-Enabled%20Multi-Agent%20Manufacturing%20Systems%0AAuthor%3A%20Jonghan%20Lim%20and%20Birgit%20Vogel-Heuser%20and%20Ilya%20Kovalenko%0AAbstract%3A%20%20%20Traditional%20manufacturing%20faces%20challenges%20adapting%20to%20dynamic%20environments%0Aand%20quickly%20responding%20to%20manufacturing%20changes.%20The%20use%20of%20multi-agent%20systems%0Ahas%20improved%20adaptability%20and%20coordination%20but%20requires%20further%20advancements%20in%0Arapid%20human%20instruction%20comprehension%2C%20operational%20adaptability%2C%20and%0Acoordination%20through%20natural%20language%20integration.%20Large%20language%20models%20like%0AGPT-3.5%20and%20GPT-4%20enhance%20multi-agent%20manufacturing%20systems%20by%20enabling%20agents%0Ato%20communicate%20in%20natural%20language%20and%20interpret%20human%20instructions%20for%0Adecision-making.%20This%20research%20introduces%20a%20novel%20framework%20where%20large%0Alanguage%20models%20enhance%20the%20capabilities%20of%20agents%20in%20manufacturing%2C%20making%0Athem%20more%20adaptable%2C%20and%20capable%20of%20processing%20context-specific%20instructions.%20A%0Acase%20study%20demonstrates%20the%20practical%20application%20of%20this%20framework%2C%20showing%0Ahow%20agents%20can%20effectively%20communicate%2C%20understand%20tasks%2C%20and%20execute%0Amanufacturing%20processes%2C%20including%20precise%20G-code%20allocation%20among%20agents.%20The%0Afindings%20highlight%20the%20importance%20of%20continuous%20large%20language%20model%0Aintegration%20into%20multi-agent%20manufacturing%20systems%20and%20the%20development%20of%0Asophisticated%20agent%20communication%20protocols%20for%20a%20more%20flexible%20manufacturing%0Asystem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.01893v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Model-Enabled%2520Multi-Agent%2520Manufacturing%2520Systems%26entry.906535625%3DJonghan%2520Lim%2520and%2520Birgit%2520Vogel-Heuser%2520and%2520Ilya%2520Kovalenko%26entry.1292438233%3D%2520%2520Traditional%2520manufacturing%2520faces%2520challenges%2520adapting%2520to%2520dynamic%2520environments%250Aand%2520quickly%2520responding%2520to%2520manufacturing%2520changes.%2520The%2520use%2520of%2520multi-agent%2520systems%250Ahas%2520improved%2520adaptability%2520and%2520coordination%2520but%2520requires%2520further%2520advancements%2520in%250Arapid%2520human%2520instruction%2520comprehension%252C%2520operational%2520adaptability%252C%2520and%250Acoordination%2520through%2520natural%2520language%2520integration.%2520Large%2520language%2520models%2520like%250AGPT-3.5%2520and%2520GPT-4%2520enhance%2520multi-agent%2520manufacturing%2520systems%2520by%2520enabling%2520agents%250Ato%2520communicate%2520in%2520natural%2520language%2520and%2520interpret%2520human%2520instructions%2520for%250Adecision-making.%2520This%2520research%2520introduces%2520a%2520novel%2520framework%2520where%2520large%250Alanguage%2520models%2520enhance%2520the%2520capabilities%2520of%2520agents%2520in%2520manufacturing%252C%2520making%250Athem%2520more%2520adaptable%252C%2520and%2520capable%2520of%2520processing%2520context-specific%2520instructions.%2520A%250Acase%2520study%2520demonstrates%2520the%2520practical%2520application%2520of%2520this%2520framework%252C%2520showing%250Ahow%2520agents%2520can%2520effectively%2520communicate%252C%2520understand%2520tasks%252C%2520and%2520execute%250Amanufacturing%2520processes%252C%2520including%2520precise%2520G-code%2520allocation%2520among%2520agents.%2520The%250Afindings%2520highlight%2520the%2520importance%2520of%2520continuous%2520large%2520language%2520model%250Aintegration%2520into%2520multi-agent%2520manufacturing%2520systems%2520and%2520the%2520development%2520of%250Asophisticated%2520agent%2520communication%2520protocols%2520for%2520a%2520more%2520flexible%2520manufacturing%250Asystem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01893v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Model-Enabled%20Multi-Agent%20Manufacturing%20Systems&entry.906535625=Jonghan%20Lim%20and%20Birgit%20Vogel-Heuser%20and%20Ilya%20Kovalenko&entry.1292438233=%20%20Traditional%20manufacturing%20faces%20challenges%20adapting%20to%20dynamic%20environments%0Aand%20quickly%20responding%20to%20manufacturing%20changes.%20The%20use%20of%20multi-agent%20systems%0Ahas%20improved%20adaptability%20and%20coordination%20but%20requires%20further%20advancements%20in%0Arapid%20human%20instruction%20comprehension%2C%20operational%20adaptability%2C%20and%0Acoordination%20through%20natural%20language%20integration.%20Large%20language%20models%20like%0AGPT-3.5%20and%20GPT-4%20enhance%20multi-agent%20manufacturing%20systems%20by%20enabling%20agents%0Ato%20communicate%20in%20natural%20language%20and%20interpret%20human%20instructions%20for%0Adecision-making.%20This%20research%20introduces%20a%20novel%20framework%20where%20large%0Alanguage%20models%20enhance%20the%20capabilities%20of%20agents%20in%20manufacturing%2C%20making%0Athem%20more%20adaptable%2C%20and%20capable%20of%20processing%20context-specific%20instructions.%20A%0Acase%20study%20demonstrates%20the%20practical%20application%20of%20this%20framework%2C%20showing%0Ahow%20agents%20can%20effectively%20communicate%2C%20understand%20tasks%2C%20and%20execute%0Amanufacturing%20processes%2C%20including%20precise%20G-code%20allocation%20among%20agents.%20The%0Afindings%20highlight%20the%20importance%20of%20continuous%20large%20language%20model%0Aintegration%20into%20multi-agent%20manufacturing%20systems%20and%20the%20development%20of%0Asophisticated%20agent%20communication%20protocols%20for%20a%20more%20flexible%20manufacturing%0Asystem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.01893v2&entry.124074799=Read"},
{"title": "Rethinking Remote Sensing Change Detection With A Mask View", "author": "Xiaowen Ma and Zhenkai Wu and Rongrong Lian and Wei Zhang and Siyang Song", "abstract": "  Remote sensing change detection aims to compare two or more images recorded\nfor the same area but taken at different time stamps to quantitatively and\nqualitatively assess changes in geographical entities and environmental\nfactors. Mainstream models usually built on pixel-by-pixel change detection\nparadigms, which cannot tolerate the diversity of changes due to complex scenes\nand variation in imaging conditions. To address this shortcoming, this paper\nrethinks the change detection with the mask view, and further proposes the\ncorresponding: 1) meta-architecture CDMask and 2) instance network\nCDMaskFormer. Components of CDMask include Siamese backbone, change extractor,\npixel decoder, transformer decoder and normalized detector, which ensures the\nproper functioning of the mask detection paradigm. Since the change query can\nbe adaptively updated based on the bi-temporal feature content, the proposed\nCDMask can adapt to different latent data distributions, thus accurately\nidentifying regions of interest changes in complex scenarios. Consequently, we\nfurther propose the instance network CDMaskFormer customized for the change\ndetection task, which includes: (i) a Spatial-temporal convolutional\nattention-based instantiated change extractor to capture spatio-temporal\ncontext simultaneously with lightweight operations; and (ii) a scene-guided\naxial attention-instantiated transformer decoder to extract more spatial\ndetails. State-of-the-art performance of CDMaskFormer is achieved on five\nbenchmark datasets with a satisfactory efficiency-accuracy trade-off. Code is\navailable at https://github.com/xwmaxwma/rschange.\n", "link": "http://arxiv.org/abs/2406.15320v1", "date": "2024-06-21", "relevancy": 1.5716, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5389}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5345}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5136}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Remote%20Sensing%20Change%20Detection%20With%20A%20Mask%20View&body=Title%3A%20Rethinking%20Remote%20Sensing%20Change%20Detection%20With%20A%20Mask%20View%0AAuthor%3A%20Xiaowen%20Ma%20and%20Zhenkai%20Wu%20and%20Rongrong%20Lian%20and%20Wei%20Zhang%20and%20Siyang%20Song%0AAbstract%3A%20%20%20Remote%20sensing%20change%20detection%20aims%20to%20compare%20two%20or%20more%20images%20recorded%0Afor%20the%20same%20area%20but%20taken%20at%20different%20time%20stamps%20to%20quantitatively%20and%0Aqualitatively%20assess%20changes%20in%20geographical%20entities%20and%20environmental%0Afactors.%20Mainstream%20models%20usually%20built%20on%20pixel-by-pixel%20change%20detection%0Aparadigms%2C%20which%20cannot%20tolerate%20the%20diversity%20of%20changes%20due%20to%20complex%20scenes%0Aand%20variation%20in%20imaging%20conditions.%20To%20address%20this%20shortcoming%2C%20this%20paper%0Arethinks%20the%20change%20detection%20with%20the%20mask%20view%2C%20and%20further%20proposes%20the%0Acorresponding%3A%201%29%20meta-architecture%20CDMask%20and%202%29%20instance%20network%0ACDMaskFormer.%20Components%20of%20CDMask%20include%20Siamese%20backbone%2C%20change%20extractor%2C%0Apixel%20decoder%2C%20transformer%20decoder%20and%20normalized%20detector%2C%20which%20ensures%20the%0Aproper%20functioning%20of%20the%20mask%20detection%20paradigm.%20Since%20the%20change%20query%20can%0Abe%20adaptively%20updated%20based%20on%20the%20bi-temporal%20feature%20content%2C%20the%20proposed%0ACDMask%20can%20adapt%20to%20different%20latent%20data%20distributions%2C%20thus%20accurately%0Aidentifying%20regions%20of%20interest%20changes%20in%20complex%20scenarios.%20Consequently%2C%20we%0Afurther%20propose%20the%20instance%20network%20CDMaskFormer%20customized%20for%20the%20change%0Adetection%20task%2C%20which%20includes%3A%20%28i%29%20a%20Spatial-temporal%20convolutional%0Aattention-based%20instantiated%20change%20extractor%20to%20capture%20spatio-temporal%0Acontext%20simultaneously%20with%20lightweight%20operations%3B%20and%20%28ii%29%20a%20scene-guided%0Aaxial%20attention-instantiated%20transformer%20decoder%20to%20extract%20more%20spatial%0Adetails.%20State-of-the-art%20performance%20of%20CDMaskFormer%20is%20achieved%20on%20five%0Abenchmark%20datasets%20with%20a%20satisfactory%20efficiency-accuracy%20trade-off.%20Code%20is%0Aavailable%20at%20https%3A//github.com/xwmaxwma/rschange.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15320v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Remote%2520Sensing%2520Change%2520Detection%2520With%2520A%2520Mask%2520View%26entry.906535625%3DXiaowen%2520Ma%2520and%2520Zhenkai%2520Wu%2520and%2520Rongrong%2520Lian%2520and%2520Wei%2520Zhang%2520and%2520Siyang%2520Song%26entry.1292438233%3D%2520%2520Remote%2520sensing%2520change%2520detection%2520aims%2520to%2520compare%2520two%2520or%2520more%2520images%2520recorded%250Afor%2520the%2520same%2520area%2520but%2520taken%2520at%2520different%2520time%2520stamps%2520to%2520quantitatively%2520and%250Aqualitatively%2520assess%2520changes%2520in%2520geographical%2520entities%2520and%2520environmental%250Afactors.%2520Mainstream%2520models%2520usually%2520built%2520on%2520pixel-by-pixel%2520change%2520detection%250Aparadigms%252C%2520which%2520cannot%2520tolerate%2520the%2520diversity%2520of%2520changes%2520due%2520to%2520complex%2520scenes%250Aand%2520variation%2520in%2520imaging%2520conditions.%2520To%2520address%2520this%2520shortcoming%252C%2520this%2520paper%250Arethinks%2520the%2520change%2520detection%2520with%2520the%2520mask%2520view%252C%2520and%2520further%2520proposes%2520the%250Acorresponding%253A%25201%2529%2520meta-architecture%2520CDMask%2520and%25202%2529%2520instance%2520network%250ACDMaskFormer.%2520Components%2520of%2520CDMask%2520include%2520Siamese%2520backbone%252C%2520change%2520extractor%252C%250Apixel%2520decoder%252C%2520transformer%2520decoder%2520and%2520normalized%2520detector%252C%2520which%2520ensures%2520the%250Aproper%2520functioning%2520of%2520the%2520mask%2520detection%2520paradigm.%2520Since%2520the%2520change%2520query%2520can%250Abe%2520adaptively%2520updated%2520based%2520on%2520the%2520bi-temporal%2520feature%2520content%252C%2520the%2520proposed%250ACDMask%2520can%2520adapt%2520to%2520different%2520latent%2520data%2520distributions%252C%2520thus%2520accurately%250Aidentifying%2520regions%2520of%2520interest%2520changes%2520in%2520complex%2520scenarios.%2520Consequently%252C%2520we%250Afurther%2520propose%2520the%2520instance%2520network%2520CDMaskFormer%2520customized%2520for%2520the%2520change%250Adetection%2520task%252C%2520which%2520includes%253A%2520%2528i%2529%2520a%2520Spatial-temporal%2520convolutional%250Aattention-based%2520instantiated%2520change%2520extractor%2520to%2520capture%2520spatio-temporal%250Acontext%2520simultaneously%2520with%2520lightweight%2520operations%253B%2520and%2520%2528ii%2529%2520a%2520scene-guided%250Aaxial%2520attention-instantiated%2520transformer%2520decoder%2520to%2520extract%2520more%2520spatial%250Adetails.%2520State-of-the-art%2520performance%2520of%2520CDMaskFormer%2520is%2520achieved%2520on%2520five%250Abenchmark%2520datasets%2520with%2520a%2520satisfactory%2520efficiency-accuracy%2520trade-off.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/xwmaxwma/rschange.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15320v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Remote%20Sensing%20Change%20Detection%20With%20A%20Mask%20View&entry.906535625=Xiaowen%20Ma%20and%20Zhenkai%20Wu%20and%20Rongrong%20Lian%20and%20Wei%20Zhang%20and%20Siyang%20Song&entry.1292438233=%20%20Remote%20sensing%20change%20detection%20aims%20to%20compare%20two%20or%20more%20images%20recorded%0Afor%20the%20same%20area%20but%20taken%20at%20different%20time%20stamps%20to%20quantitatively%20and%0Aqualitatively%20assess%20changes%20in%20geographical%20entities%20and%20environmental%0Afactors.%20Mainstream%20models%20usually%20built%20on%20pixel-by-pixel%20change%20detection%0Aparadigms%2C%20which%20cannot%20tolerate%20the%20diversity%20of%20changes%20due%20to%20complex%20scenes%0Aand%20variation%20in%20imaging%20conditions.%20To%20address%20this%20shortcoming%2C%20this%20paper%0Arethinks%20the%20change%20detection%20with%20the%20mask%20view%2C%20and%20further%20proposes%20the%0Acorresponding%3A%201%29%20meta-architecture%20CDMask%20and%202%29%20instance%20network%0ACDMaskFormer.%20Components%20of%20CDMask%20include%20Siamese%20backbone%2C%20change%20extractor%2C%0Apixel%20decoder%2C%20transformer%20decoder%20and%20normalized%20detector%2C%20which%20ensures%20the%0Aproper%20functioning%20of%20the%20mask%20detection%20paradigm.%20Since%20the%20change%20query%20can%0Abe%20adaptively%20updated%20based%20on%20the%20bi-temporal%20feature%20content%2C%20the%20proposed%0ACDMask%20can%20adapt%20to%20different%20latent%20data%20distributions%2C%20thus%20accurately%0Aidentifying%20regions%20of%20interest%20changes%20in%20complex%20scenarios.%20Consequently%2C%20we%0Afurther%20propose%20the%20instance%20network%20CDMaskFormer%20customized%20for%20the%20change%0Adetection%20task%2C%20which%20includes%3A%20%28i%29%20a%20Spatial-temporal%20convolutional%0Aattention-based%20instantiated%20change%20extractor%20to%20capture%20spatio-temporal%0Acontext%20simultaneously%20with%20lightweight%20operations%3B%20and%20%28ii%29%20a%20scene-guided%0Aaxial%20attention-instantiated%20transformer%20decoder%20to%20extract%20more%20spatial%0Adetails.%20State-of-the-art%20performance%20of%20CDMaskFormer%20is%20achieved%20on%20five%0Abenchmark%20datasets%20with%20a%20satisfactory%20efficiency-accuracy%20trade-off.%20Code%20is%0Aavailable%20at%20https%3A//github.com/xwmaxwma/rschange.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15320v1&entry.124074799=Read"},
{"title": "Keystroke Dynamics Against Academic Dishonesty in the Age of LLMs", "author": "Debnath Kundu and Atharva Mehta and Rajesh Kumar and Naman Lal and Avinash Anand and Apoorv Singh and Rajiv Ratn Shah", "abstract": "  The transition to online examinations and assignments raises significant\nconcerns about academic integrity. Traditional plagiarism detection systems\noften struggle to identify instances of intelligent cheating, particularly when\nstudents utilize advanced generative AI tools to craft their responses. This\nstudy proposes a keystroke dynamics-based method to differentiate between bona\nfide and assisted writing within academic contexts. To facilitate this, a\ndataset was developed to capture the keystroke patterns of individuals engaged\nin writing tasks, both with and without the assistance of generative AI. The\ndetector, trained using a modified TypeNet architecture, achieved accuracies\nranging from 74.98% to 85.72% in condition-specific scenarios and from 52.24%\nto 80.54% in condition-agnostic scenarios. The findings highlight significant\ndifferences in keystroke dynamics between genuine and assisted writing. The\noutcomes of this study enhance our understanding of how users interact with\ngenerative AI and have implications for improving the reliability of digital\neducational platforms.\n", "link": "http://arxiv.org/abs/2406.15335v1", "date": "2024-06-21", "relevancy": 1.3043, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4433}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4351}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Keystroke%20Dynamics%20Against%20Academic%20Dishonesty%20in%20the%20Age%20of%20LLMs&body=Title%3A%20Keystroke%20Dynamics%20Against%20Academic%20Dishonesty%20in%20the%20Age%20of%20LLMs%0AAuthor%3A%20Debnath%20Kundu%20and%20Atharva%20Mehta%20and%20Rajesh%20Kumar%20and%20Naman%20Lal%20and%20Avinash%20Anand%20and%20Apoorv%20Singh%20and%20Rajiv%20Ratn%20Shah%0AAbstract%3A%20%20%20The%20transition%20to%20online%20examinations%20and%20assignments%20raises%20significant%0Aconcerns%20about%20academic%20integrity.%20Traditional%20plagiarism%20detection%20systems%0Aoften%20struggle%20to%20identify%20instances%20of%20intelligent%20cheating%2C%20particularly%20when%0Astudents%20utilize%20advanced%20generative%20AI%20tools%20to%20craft%20their%20responses.%20This%0Astudy%20proposes%20a%20keystroke%20dynamics-based%20method%20to%20differentiate%20between%20bona%0Afide%20and%20assisted%20writing%20within%20academic%20contexts.%20To%20facilitate%20this%2C%20a%0Adataset%20was%20developed%20to%20capture%20the%20keystroke%20patterns%20of%20individuals%20engaged%0Ain%20writing%20tasks%2C%20both%20with%20and%20without%20the%20assistance%20of%20generative%20AI.%20The%0Adetector%2C%20trained%20using%20a%20modified%20TypeNet%20architecture%2C%20achieved%20accuracies%0Aranging%20from%2074.98%25%20to%2085.72%25%20in%20condition-specific%20scenarios%20and%20from%2052.24%25%0Ato%2080.54%25%20in%20condition-agnostic%20scenarios.%20The%20findings%20highlight%20significant%0Adifferences%20in%20keystroke%20dynamics%20between%20genuine%20and%20assisted%20writing.%20The%0Aoutcomes%20of%20this%20study%20enhance%20our%20understanding%20of%20how%20users%20interact%20with%0Agenerative%20AI%20and%20have%20implications%20for%20improving%20the%20reliability%20of%20digital%0Aeducational%20platforms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15335v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKeystroke%2520Dynamics%2520Against%2520Academic%2520Dishonesty%2520in%2520the%2520Age%2520of%2520LLMs%26entry.906535625%3DDebnath%2520Kundu%2520and%2520Atharva%2520Mehta%2520and%2520Rajesh%2520Kumar%2520and%2520Naman%2520Lal%2520and%2520Avinash%2520Anand%2520and%2520Apoorv%2520Singh%2520and%2520Rajiv%2520Ratn%2520Shah%26entry.1292438233%3D%2520%2520The%2520transition%2520to%2520online%2520examinations%2520and%2520assignments%2520raises%2520significant%250Aconcerns%2520about%2520academic%2520integrity.%2520Traditional%2520plagiarism%2520detection%2520systems%250Aoften%2520struggle%2520to%2520identify%2520instances%2520of%2520intelligent%2520cheating%252C%2520particularly%2520when%250Astudents%2520utilize%2520advanced%2520generative%2520AI%2520tools%2520to%2520craft%2520their%2520responses.%2520This%250Astudy%2520proposes%2520a%2520keystroke%2520dynamics-based%2520method%2520to%2520differentiate%2520between%2520bona%250Afide%2520and%2520assisted%2520writing%2520within%2520academic%2520contexts.%2520To%2520facilitate%2520this%252C%2520a%250Adataset%2520was%2520developed%2520to%2520capture%2520the%2520keystroke%2520patterns%2520of%2520individuals%2520engaged%250Ain%2520writing%2520tasks%252C%2520both%2520with%2520and%2520without%2520the%2520assistance%2520of%2520generative%2520AI.%2520The%250Adetector%252C%2520trained%2520using%2520a%2520modified%2520TypeNet%2520architecture%252C%2520achieved%2520accuracies%250Aranging%2520from%252074.98%2525%2520to%252085.72%2525%2520in%2520condition-specific%2520scenarios%2520and%2520from%252052.24%2525%250Ato%252080.54%2525%2520in%2520condition-agnostic%2520scenarios.%2520The%2520findings%2520highlight%2520significant%250Adifferences%2520in%2520keystroke%2520dynamics%2520between%2520genuine%2520and%2520assisted%2520writing.%2520The%250Aoutcomes%2520of%2520this%2520study%2520enhance%2520our%2520understanding%2520of%2520how%2520users%2520interact%2520with%250Agenerative%2520AI%2520and%2520have%2520implications%2520for%2520improving%2520the%2520reliability%2520of%2520digital%250Aeducational%2520platforms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15335v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Keystroke%20Dynamics%20Against%20Academic%20Dishonesty%20in%20the%20Age%20of%20LLMs&entry.906535625=Debnath%20Kundu%20and%20Atharva%20Mehta%20and%20Rajesh%20Kumar%20and%20Naman%20Lal%20and%20Avinash%20Anand%20and%20Apoorv%20Singh%20and%20Rajiv%20Ratn%20Shah&entry.1292438233=%20%20The%20transition%20to%20online%20examinations%20and%20assignments%20raises%20significant%0Aconcerns%20about%20academic%20integrity.%20Traditional%20plagiarism%20detection%20systems%0Aoften%20struggle%20to%20identify%20instances%20of%20intelligent%20cheating%2C%20particularly%20when%0Astudents%20utilize%20advanced%20generative%20AI%20tools%20to%20craft%20their%20responses.%20This%0Astudy%20proposes%20a%20keystroke%20dynamics-based%20method%20to%20differentiate%20between%20bona%0Afide%20and%20assisted%20writing%20within%20academic%20contexts.%20To%20facilitate%20this%2C%20a%0Adataset%20was%20developed%20to%20capture%20the%20keystroke%20patterns%20of%20individuals%20engaged%0Ain%20writing%20tasks%2C%20both%20with%20and%20without%20the%20assistance%20of%20generative%20AI.%20The%0Adetector%2C%20trained%20using%20a%20modified%20TypeNet%20architecture%2C%20achieved%20accuracies%0Aranging%20from%2074.98%25%20to%2085.72%25%20in%20condition-specific%20scenarios%20and%20from%2052.24%25%0Ato%2080.54%25%20in%20condition-agnostic%20scenarios.%20The%20findings%20highlight%20significant%0Adifferences%20in%20keystroke%20dynamics%20between%20genuine%20and%20assisted%20writing.%20The%0Aoutcomes%20of%20this%20study%20enhance%20our%20understanding%20of%20how%20users%20interact%20with%0Agenerative%20AI%20and%20have%20implications%20for%20improving%20the%20reliability%20of%20digital%0Aeducational%20platforms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15335v1&entry.124074799=Read"},
{"title": "Enhancing Idiomatic Representation in Multiple Languages via an Adaptive\n  Contrastive Triplet Loss", "author": "Wei He and Marco Idiart and Carolina Scarton and Aline Villavicencio", "abstract": "  Accurately modeling idiomatic or non-compositional language has been a\nlongstanding challenge in Natural Language Processing (NLP). This is partly\nbecause these expressions do not derive their meanings solely from their\nconstituent words, but also due to the scarcity of relevant data resources, and\ntheir impact on the performance of downstream tasks such as machine translation\nand simplification. In this paper we propose an approach to model idiomaticity\neffectively using a triplet loss that incorporates the asymmetric contribution\nof components words to an idiomatic meaning for training language models by\nusing adaptive contrastive learning and resampling miners to build an\nidiomatic-aware learning objective. Our proposed method is evaluated on a\nSemEval challenge and outperforms previous alternatives significantly in many\nmetrics.\n", "link": "http://arxiv.org/abs/2406.15175v1", "date": "2024-06-21", "relevancy": 1.5534, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5547}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4749}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Idiomatic%20Representation%20in%20Multiple%20Languages%20via%20an%20Adaptive%0A%20%20Contrastive%20Triplet%20Loss&body=Title%3A%20Enhancing%20Idiomatic%20Representation%20in%20Multiple%20Languages%20via%20an%20Adaptive%0A%20%20Contrastive%20Triplet%20Loss%0AAuthor%3A%20Wei%20He%20and%20Marco%20Idiart%20and%20Carolina%20Scarton%20and%20Aline%20Villavicencio%0AAbstract%3A%20%20%20Accurately%20modeling%20idiomatic%20or%20non-compositional%20language%20has%20been%20a%0Alongstanding%20challenge%20in%20Natural%20Language%20Processing%20%28NLP%29.%20This%20is%20partly%0Abecause%20these%20expressions%20do%20not%20derive%20their%20meanings%20solely%20from%20their%0Aconstituent%20words%2C%20but%20also%20due%20to%20the%20scarcity%20of%20relevant%20data%20resources%2C%20and%0Atheir%20impact%20on%20the%20performance%20of%20downstream%20tasks%20such%20as%20machine%20translation%0Aand%20simplification.%20In%20this%20paper%20we%20propose%20an%20approach%20to%20model%20idiomaticity%0Aeffectively%20using%20a%20triplet%20loss%20that%20incorporates%20the%20asymmetric%20contribution%0Aof%20components%20words%20to%20an%20idiomatic%20meaning%20for%20training%20language%20models%20by%0Ausing%20adaptive%20contrastive%20learning%20and%20resampling%20miners%20to%20build%20an%0Aidiomatic-aware%20learning%20objective.%20Our%20proposed%20method%20is%20evaluated%20on%20a%0ASemEval%20challenge%20and%20outperforms%20previous%20alternatives%20significantly%20in%20many%0Ametrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15175v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Idiomatic%2520Representation%2520in%2520Multiple%2520Languages%2520via%2520an%2520Adaptive%250A%2520%2520Contrastive%2520Triplet%2520Loss%26entry.906535625%3DWei%2520He%2520and%2520Marco%2520Idiart%2520and%2520Carolina%2520Scarton%2520and%2520Aline%2520Villavicencio%26entry.1292438233%3D%2520%2520Accurately%2520modeling%2520idiomatic%2520or%2520non-compositional%2520language%2520has%2520been%2520a%250Alongstanding%2520challenge%2520in%2520Natural%2520Language%2520Processing%2520%2528NLP%2529.%2520This%2520is%2520partly%250Abecause%2520these%2520expressions%2520do%2520not%2520derive%2520their%2520meanings%2520solely%2520from%2520their%250Aconstituent%2520words%252C%2520but%2520also%2520due%2520to%2520the%2520scarcity%2520of%2520relevant%2520data%2520resources%252C%2520and%250Atheir%2520impact%2520on%2520the%2520performance%2520of%2520downstream%2520tasks%2520such%2520as%2520machine%2520translation%250Aand%2520simplification.%2520In%2520this%2520paper%2520we%2520propose%2520an%2520approach%2520to%2520model%2520idiomaticity%250Aeffectively%2520using%2520a%2520triplet%2520loss%2520that%2520incorporates%2520the%2520asymmetric%2520contribution%250Aof%2520components%2520words%2520to%2520an%2520idiomatic%2520meaning%2520for%2520training%2520language%2520models%2520by%250Ausing%2520adaptive%2520contrastive%2520learning%2520and%2520resampling%2520miners%2520to%2520build%2520an%250Aidiomatic-aware%2520learning%2520objective.%2520Our%2520proposed%2520method%2520is%2520evaluated%2520on%2520a%250ASemEval%2520challenge%2520and%2520outperforms%2520previous%2520alternatives%2520significantly%2520in%2520many%250Ametrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15175v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Idiomatic%20Representation%20in%20Multiple%20Languages%20via%20an%20Adaptive%0A%20%20Contrastive%20Triplet%20Loss&entry.906535625=Wei%20He%20and%20Marco%20Idiart%20and%20Carolina%20Scarton%20and%20Aline%20Villavicencio&entry.1292438233=%20%20Accurately%20modeling%20idiomatic%20or%20non-compositional%20language%20has%20been%20a%0Alongstanding%20challenge%20in%20Natural%20Language%20Processing%20%28NLP%29.%20This%20is%20partly%0Abecause%20these%20expressions%20do%20not%20derive%20their%20meanings%20solely%20from%20their%0Aconstituent%20words%2C%20but%20also%20due%20to%20the%20scarcity%20of%20relevant%20data%20resources%2C%20and%0Atheir%20impact%20on%20the%20performance%20of%20downstream%20tasks%20such%20as%20machine%20translation%0Aand%20simplification.%20In%20this%20paper%20we%20propose%20an%20approach%20to%20model%20idiomaticity%0Aeffectively%20using%20a%20triplet%20loss%20that%20incorporates%20the%20asymmetric%20contribution%0Aof%20components%20words%20to%20an%20idiomatic%20meaning%20for%20training%20language%20models%20by%0Ausing%20adaptive%20contrastive%20learning%20and%20resampling%20miners%20to%20build%20an%0Aidiomatic-aware%20learning%20objective.%20Our%20proposed%20method%20is%20evaluated%20on%20a%0ASemEval%20challenge%20and%20outperforms%20previous%20alternatives%20significantly%20in%20many%0Ametrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15175v1&entry.124074799=Read"},
{"title": "MTUncertainty: Assessing the Need for Post-editing of Machine\n  Translation Outputs by Fine-tuning OpenAI LLMs", "author": "Serge Gladkoff and Lifeng Han and Gleb Erofeev and Irina Sorokina and Goran Nenadic", "abstract": "  Translation Quality Evaluation (TQE) is an essential step of the modern\ntranslation production process. TQE is critical in assessing both machine\ntranslation (MT) and human translation (HT) quality without reference\ntranslations. The ability to evaluate or even simply estimate the quality of\ntranslation automatically may open significant efficiency gains through process\noptimisation. This work examines whether the state-of-the-art large language\nmodels (LLMs) can be used for this purpose. We take OpenAI models as the best\nstate-of-the-art technology and approach TQE as a binary classification task.\nOn eight language pairs including English to Italian, German, French, Japanese,\nDutch, Portuguese, Turkish, and Chinese, our experimental results show that\nfine-tuned gpt3.5 can demonstrate good performance on translation quality\nprediction tasks, i.e. whether the translation needs to be edited. Another\nfinding is that simply increasing the sizes of LLMs does not lead to apparent\nbetter performances on this task by comparing the performance of three\ndifferent versions of OpenAI models: curie, davinci, and gpt3.5 with 13B, 175B,\nand 175B parameters, respectively.\n", "link": "http://arxiv.org/abs/2308.00158v6", "date": "2024-06-21", "relevancy": 1.4251, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5563}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.452}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MTUncertainty%3A%20Assessing%20the%20Need%20for%20Post-editing%20of%20Machine%0A%20%20Translation%20Outputs%20by%20Fine-tuning%20OpenAI%20LLMs&body=Title%3A%20MTUncertainty%3A%20Assessing%20the%20Need%20for%20Post-editing%20of%20Machine%0A%20%20Translation%20Outputs%20by%20Fine-tuning%20OpenAI%20LLMs%0AAuthor%3A%20Serge%20Gladkoff%20and%20Lifeng%20Han%20and%20Gleb%20Erofeev%20and%20Irina%20Sorokina%20and%20Goran%20Nenadic%0AAbstract%3A%20%20%20Translation%20Quality%20Evaluation%20%28TQE%29%20is%20an%20essential%20step%20of%20the%20modern%0Atranslation%20production%20process.%20TQE%20is%20critical%20in%20assessing%20both%20machine%0Atranslation%20%28MT%29%20and%20human%20translation%20%28HT%29%20quality%20without%20reference%0Atranslations.%20The%20ability%20to%20evaluate%20or%20even%20simply%20estimate%20the%20quality%20of%0Atranslation%20automatically%20may%20open%20significant%20efficiency%20gains%20through%20process%0Aoptimisation.%20This%20work%20examines%20whether%20the%20state-of-the-art%20large%20language%0Amodels%20%28LLMs%29%20can%20be%20used%20for%20this%20purpose.%20We%20take%20OpenAI%20models%20as%20the%20best%0Astate-of-the-art%20technology%20and%20approach%20TQE%20as%20a%20binary%20classification%20task.%0AOn%20eight%20language%20pairs%20including%20English%20to%20Italian%2C%20German%2C%20French%2C%20Japanese%2C%0ADutch%2C%20Portuguese%2C%20Turkish%2C%20and%20Chinese%2C%20our%20experimental%20results%20show%20that%0Afine-tuned%20gpt3.5%20can%20demonstrate%20good%20performance%20on%20translation%20quality%0Aprediction%20tasks%2C%20i.e.%20whether%20the%20translation%20needs%20to%20be%20edited.%20Another%0Afinding%20is%20that%20simply%20increasing%20the%20sizes%20of%20LLMs%20does%20not%20lead%20to%20apparent%0Abetter%20performances%20on%20this%20task%20by%20comparing%20the%20performance%20of%20three%0Adifferent%20versions%20of%20OpenAI%20models%3A%20curie%2C%20davinci%2C%20and%20gpt3.5%20with%2013B%2C%20175B%2C%0Aand%20175B%20parameters%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.00158v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMTUncertainty%253A%2520Assessing%2520the%2520Need%2520for%2520Post-editing%2520of%2520Machine%250A%2520%2520Translation%2520Outputs%2520by%2520Fine-tuning%2520OpenAI%2520LLMs%26entry.906535625%3DSerge%2520Gladkoff%2520and%2520Lifeng%2520Han%2520and%2520Gleb%2520Erofeev%2520and%2520Irina%2520Sorokina%2520and%2520Goran%2520Nenadic%26entry.1292438233%3D%2520%2520Translation%2520Quality%2520Evaluation%2520%2528TQE%2529%2520is%2520an%2520essential%2520step%2520of%2520the%2520modern%250Atranslation%2520production%2520process.%2520TQE%2520is%2520critical%2520in%2520assessing%2520both%2520machine%250Atranslation%2520%2528MT%2529%2520and%2520human%2520translation%2520%2528HT%2529%2520quality%2520without%2520reference%250Atranslations.%2520The%2520ability%2520to%2520evaluate%2520or%2520even%2520simply%2520estimate%2520the%2520quality%2520of%250Atranslation%2520automatically%2520may%2520open%2520significant%2520efficiency%2520gains%2520through%2520process%250Aoptimisation.%2520This%2520work%2520examines%2520whether%2520the%2520state-of-the-art%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520can%2520be%2520used%2520for%2520this%2520purpose.%2520We%2520take%2520OpenAI%2520models%2520as%2520the%2520best%250Astate-of-the-art%2520technology%2520and%2520approach%2520TQE%2520as%2520a%2520binary%2520classification%2520task.%250AOn%2520eight%2520language%2520pairs%2520including%2520English%2520to%2520Italian%252C%2520German%252C%2520French%252C%2520Japanese%252C%250ADutch%252C%2520Portuguese%252C%2520Turkish%252C%2520and%2520Chinese%252C%2520our%2520experimental%2520results%2520show%2520that%250Afine-tuned%2520gpt3.5%2520can%2520demonstrate%2520good%2520performance%2520on%2520translation%2520quality%250Aprediction%2520tasks%252C%2520i.e.%2520whether%2520the%2520translation%2520needs%2520to%2520be%2520edited.%2520Another%250Afinding%2520is%2520that%2520simply%2520increasing%2520the%2520sizes%2520of%2520LLMs%2520does%2520not%2520lead%2520to%2520apparent%250Abetter%2520performances%2520on%2520this%2520task%2520by%2520comparing%2520the%2520performance%2520of%2520three%250Adifferent%2520versions%2520of%2520OpenAI%2520models%253A%2520curie%252C%2520davinci%252C%2520and%2520gpt3.5%2520with%252013B%252C%2520175B%252C%250Aand%2520175B%2520parameters%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.00158v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MTUncertainty%3A%20Assessing%20the%20Need%20for%20Post-editing%20of%20Machine%0A%20%20Translation%20Outputs%20by%20Fine-tuning%20OpenAI%20LLMs&entry.906535625=Serge%20Gladkoff%20and%20Lifeng%20Han%20and%20Gleb%20Erofeev%20and%20Irina%20Sorokina%20and%20Goran%20Nenadic&entry.1292438233=%20%20Translation%20Quality%20Evaluation%20%28TQE%29%20is%20an%20essential%20step%20of%20the%20modern%0Atranslation%20production%20process.%20TQE%20is%20critical%20in%20assessing%20both%20machine%0Atranslation%20%28MT%29%20and%20human%20translation%20%28HT%29%20quality%20without%20reference%0Atranslations.%20The%20ability%20to%20evaluate%20or%20even%20simply%20estimate%20the%20quality%20of%0Atranslation%20automatically%20may%20open%20significant%20efficiency%20gains%20through%20process%0Aoptimisation.%20This%20work%20examines%20whether%20the%20state-of-the-art%20large%20language%0Amodels%20%28LLMs%29%20can%20be%20used%20for%20this%20purpose.%20We%20take%20OpenAI%20models%20as%20the%20best%0Astate-of-the-art%20technology%20and%20approach%20TQE%20as%20a%20binary%20classification%20task.%0AOn%20eight%20language%20pairs%20including%20English%20to%20Italian%2C%20German%2C%20French%2C%20Japanese%2C%0ADutch%2C%20Portuguese%2C%20Turkish%2C%20and%20Chinese%2C%20our%20experimental%20results%20show%20that%0Afine-tuned%20gpt3.5%20can%20demonstrate%20good%20performance%20on%20translation%20quality%0Aprediction%20tasks%2C%20i.e.%20whether%20the%20translation%20needs%20to%20be%20edited.%20Another%0Afinding%20is%20that%20simply%20increasing%20the%20sizes%20of%20LLMs%20does%20not%20lead%20to%20apparent%0Abetter%20performances%20on%20this%20task%20by%20comparing%20the%20performance%20of%20three%0Adifferent%20versions%20of%20OpenAI%20models%3A%20curie%2C%20davinci%2C%20and%20gpt3.5%20with%2013B%2C%20175B%2C%0Aand%20175B%20parameters%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.00158v6&entry.124074799=Read"},
{"title": "Explainable Online Unsupervised Anomaly Detection for Cyber-Physical\n  Systems via Causal Discovery from Time Series", "author": "Daniele Meli", "abstract": "  Online unsupervised detection of anomalies is crucial to guarantee the\ncorrect operation of cyber-physical systems and the safety of humans\ninteracting with them. State-of-the-art approaches based on deep learning via\nneural networks achieve outstanding performance at anomaly recognition,\nevaluating the discrepancy between a normal model of the system (with no\nanomalies) and the real-time stream of sensor time series. However, large\ntraining data and time are typically required, and explainability is still a\nchallenge to identify the root of the anomaly and implement predictive\nmaintainance. In this paper, we use causal discovery to learn a normal causal\ngraph of the system, and we evaluate the persistency of causal links during\nreal-time acquisition of sensor data to promptly detect anomalies. On two\nbenchmark anomaly detection datasets, we show that our method has higher\ntraining efficiency, outperforms the accuracy of state-of-the-art neural\narchitectures and correctly identifies the sources of $>10$ different\nanomalies. The code for experimental replication is at\nhttp://tinyurl.com/case24causal.\n", "link": "http://arxiv.org/abs/2404.09871v2", "date": "2024-06-21", "relevancy": 1.4071, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5086}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.458}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explainable%20Online%20Unsupervised%20Anomaly%20Detection%20for%20Cyber-Physical%0A%20%20Systems%20via%20Causal%20Discovery%20from%20Time%20Series&body=Title%3A%20Explainable%20Online%20Unsupervised%20Anomaly%20Detection%20for%20Cyber-Physical%0A%20%20Systems%20via%20Causal%20Discovery%20from%20Time%20Series%0AAuthor%3A%20Daniele%20Meli%0AAbstract%3A%20%20%20Online%20unsupervised%20detection%20of%20anomalies%20is%20crucial%20to%20guarantee%20the%0Acorrect%20operation%20of%20cyber-physical%20systems%20and%20the%20safety%20of%20humans%0Ainteracting%20with%20them.%20State-of-the-art%20approaches%20based%20on%20deep%20learning%20via%0Aneural%20networks%20achieve%20outstanding%20performance%20at%20anomaly%20recognition%2C%0Aevaluating%20the%20discrepancy%20between%20a%20normal%20model%20of%20the%20system%20%28with%20no%0Aanomalies%29%20and%20the%20real-time%20stream%20of%20sensor%20time%20series.%20However%2C%20large%0Atraining%20data%20and%20time%20are%20typically%20required%2C%20and%20explainability%20is%20still%20a%0Achallenge%20to%20identify%20the%20root%20of%20the%20anomaly%20and%20implement%20predictive%0Amaintainance.%20In%20this%20paper%2C%20we%20use%20causal%20discovery%20to%20learn%20a%20normal%20causal%0Agraph%20of%20the%20system%2C%20and%20we%20evaluate%20the%20persistency%20of%20causal%20links%20during%0Areal-time%20acquisition%20of%20sensor%20data%20to%20promptly%20detect%20anomalies.%20On%20two%0Abenchmark%20anomaly%20detection%20datasets%2C%20we%20show%20that%20our%20method%20has%20higher%0Atraining%20efficiency%2C%20outperforms%20the%20accuracy%20of%20state-of-the-art%20neural%0Aarchitectures%20and%20correctly%20identifies%20the%20sources%20of%20%24%3E10%24%20different%0Aanomalies.%20The%20code%20for%20experimental%20replication%20is%20at%0Ahttp%3A//tinyurl.com/case24causal.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09871v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplainable%2520Online%2520Unsupervised%2520Anomaly%2520Detection%2520for%2520Cyber-Physical%250A%2520%2520Systems%2520via%2520Causal%2520Discovery%2520from%2520Time%2520Series%26entry.906535625%3DDaniele%2520Meli%26entry.1292438233%3D%2520%2520Online%2520unsupervised%2520detection%2520of%2520anomalies%2520is%2520crucial%2520to%2520guarantee%2520the%250Acorrect%2520operation%2520of%2520cyber-physical%2520systems%2520and%2520the%2520safety%2520of%2520humans%250Ainteracting%2520with%2520them.%2520State-of-the-art%2520approaches%2520based%2520on%2520deep%2520learning%2520via%250Aneural%2520networks%2520achieve%2520outstanding%2520performance%2520at%2520anomaly%2520recognition%252C%250Aevaluating%2520the%2520discrepancy%2520between%2520a%2520normal%2520model%2520of%2520the%2520system%2520%2528with%2520no%250Aanomalies%2529%2520and%2520the%2520real-time%2520stream%2520of%2520sensor%2520time%2520series.%2520However%252C%2520large%250Atraining%2520data%2520and%2520time%2520are%2520typically%2520required%252C%2520and%2520explainability%2520is%2520still%2520a%250Achallenge%2520to%2520identify%2520the%2520root%2520of%2520the%2520anomaly%2520and%2520implement%2520predictive%250Amaintainance.%2520In%2520this%2520paper%252C%2520we%2520use%2520causal%2520discovery%2520to%2520learn%2520a%2520normal%2520causal%250Agraph%2520of%2520the%2520system%252C%2520and%2520we%2520evaluate%2520the%2520persistency%2520of%2520causal%2520links%2520during%250Areal-time%2520acquisition%2520of%2520sensor%2520data%2520to%2520promptly%2520detect%2520anomalies.%2520On%2520two%250Abenchmark%2520anomaly%2520detection%2520datasets%252C%2520we%2520show%2520that%2520our%2520method%2520has%2520higher%250Atraining%2520efficiency%252C%2520outperforms%2520the%2520accuracy%2520of%2520state-of-the-art%2520neural%250Aarchitectures%2520and%2520correctly%2520identifies%2520the%2520sources%2520of%2520%2524%253E10%2524%2520different%250Aanomalies.%2520The%2520code%2520for%2520experimental%2520replication%2520is%2520at%250Ahttp%253A//tinyurl.com/case24causal.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.09871v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explainable%20Online%20Unsupervised%20Anomaly%20Detection%20for%20Cyber-Physical%0A%20%20Systems%20via%20Causal%20Discovery%20from%20Time%20Series&entry.906535625=Daniele%20Meli&entry.1292438233=%20%20Online%20unsupervised%20detection%20of%20anomalies%20is%20crucial%20to%20guarantee%20the%0Acorrect%20operation%20of%20cyber-physical%20systems%20and%20the%20safety%20of%20humans%0Ainteracting%20with%20them.%20State-of-the-art%20approaches%20based%20on%20deep%20learning%20via%0Aneural%20networks%20achieve%20outstanding%20performance%20at%20anomaly%20recognition%2C%0Aevaluating%20the%20discrepancy%20between%20a%20normal%20model%20of%20the%20system%20%28with%20no%0Aanomalies%29%20and%20the%20real-time%20stream%20of%20sensor%20time%20series.%20However%2C%20large%0Atraining%20data%20and%20time%20are%20typically%20required%2C%20and%20explainability%20is%20still%20a%0Achallenge%20to%20identify%20the%20root%20of%20the%20anomaly%20and%20implement%20predictive%0Amaintainance.%20In%20this%20paper%2C%20we%20use%20causal%20discovery%20to%20learn%20a%20normal%20causal%0Agraph%20of%20the%20system%2C%20and%20we%20evaluate%20the%20persistency%20of%20causal%20links%20during%0Areal-time%20acquisition%20of%20sensor%20data%20to%20promptly%20detect%20anomalies.%20On%20two%0Abenchmark%20anomaly%20detection%20datasets%2C%20we%20show%20that%20our%20method%20has%20higher%0Atraining%20efficiency%2C%20outperforms%20the%20accuracy%20of%20state-of-the-art%20neural%0Aarchitectures%20and%20correctly%20identifies%20the%20sources%20of%20%24%3E10%24%20different%0Aanomalies.%20The%20code%20for%20experimental%20replication%20is%20at%0Ahttp%3A//tinyurl.com/case24causal.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09871v2&entry.124074799=Read"},
{"title": "Privacy Preserved Blood Glucose Level Cross-Prediction: An Asynchronous\n  Decentralized Federated Learning Approach", "author": "Chengzhe Piao and Taiyu Zhu and Yu Wang and Stephanie E Baldeweg and Paul Taylor and Pantelis Georgiou and Jiahao Sun and Jun Wang and Kezhi Li", "abstract": "  Newly diagnosed Type 1 Diabetes (T1D) patients often struggle to obtain\neffective Blood Glucose (BG) prediction models due to the lack of sufficient BG\ndata from Continuous Glucose Monitoring (CGM), presenting a significant \"cold\nstart\" problem in patient care. Utilizing population models to address this\nchallenge is a potential solution, but collecting patient data for training\npopulation models in a privacy-conscious manner is challenging, especially\ngiven that such data is often stored on personal devices. Considering the\nprivacy protection and addressing the \"cold start\" problem in diabetes care, we\npropose \"GluADFL\", blood Glucose prediction by Asynchronous Decentralized\nFederated Learning. We compared GluADFL with eight baseline methods using four\ndistinct T1D datasets, comprising 298 participants, which demonstrated its\nsuperior performance in accurately predicting BG levels for cross-patient\nanalysis. Furthermore, patients' data might be stored and shared across various\ncommunication networks in GluADFL, ranging from highly interconnected (e.g.,\nrandom, performs the best among others) to more structured topologies (e.g.,\ncluster and ring), suitable for various social networks. The asynchronous\ntraining framework supports flexible participation. By adjusting the ratios of\ninactive participants, we found it remains stable if less than 70% are\ninactive. Our results confirm that GluADFL offers a practical,\nprivacy-preserving solution for BG prediction in T1D, significantly enhancing\nthe quality of diabetes management.\n", "link": "http://arxiv.org/abs/2406.15346v1", "date": "2024-06-21", "relevancy": 1.3431, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4606}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4443}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Privacy%20Preserved%20Blood%20Glucose%20Level%20Cross-Prediction%3A%20An%20Asynchronous%0A%20%20Decentralized%20Federated%20Learning%20Approach&body=Title%3A%20Privacy%20Preserved%20Blood%20Glucose%20Level%20Cross-Prediction%3A%20An%20Asynchronous%0A%20%20Decentralized%20Federated%20Learning%20Approach%0AAuthor%3A%20Chengzhe%20Piao%20and%20Taiyu%20Zhu%20and%20Yu%20Wang%20and%20Stephanie%20E%20Baldeweg%20and%20Paul%20Taylor%20and%20Pantelis%20Georgiou%20and%20Jiahao%20Sun%20and%20Jun%20Wang%20and%20Kezhi%20Li%0AAbstract%3A%20%20%20Newly%20diagnosed%20Type%201%20Diabetes%20%28T1D%29%20patients%20often%20struggle%20to%20obtain%0Aeffective%20Blood%20Glucose%20%28BG%29%20prediction%20models%20due%20to%20the%20lack%20of%20sufficient%20BG%0Adata%20from%20Continuous%20Glucose%20Monitoring%20%28CGM%29%2C%20presenting%20a%20significant%20%22cold%0Astart%22%20problem%20in%20patient%20care.%20Utilizing%20population%20models%20to%20address%20this%0Achallenge%20is%20a%20potential%20solution%2C%20but%20collecting%20patient%20data%20for%20training%0Apopulation%20models%20in%20a%20privacy-conscious%20manner%20is%20challenging%2C%20especially%0Agiven%20that%20such%20data%20is%20often%20stored%20on%20personal%20devices.%20Considering%20the%0Aprivacy%20protection%20and%20addressing%20the%20%22cold%20start%22%20problem%20in%20diabetes%20care%2C%20we%0Apropose%20%22GluADFL%22%2C%20blood%20Glucose%20prediction%20by%20Asynchronous%20Decentralized%0AFederated%20Learning.%20We%20compared%20GluADFL%20with%20eight%20baseline%20methods%20using%20four%0Adistinct%20T1D%20datasets%2C%20comprising%20298%20participants%2C%20which%20demonstrated%20its%0Asuperior%20performance%20in%20accurately%20predicting%20BG%20levels%20for%20cross-patient%0Aanalysis.%20Furthermore%2C%20patients%27%20data%20might%20be%20stored%20and%20shared%20across%20various%0Acommunication%20networks%20in%20GluADFL%2C%20ranging%20from%20highly%20interconnected%20%28e.g.%2C%0Arandom%2C%20performs%20the%20best%20among%20others%29%20to%20more%20structured%20topologies%20%28e.g.%2C%0Acluster%20and%20ring%29%2C%20suitable%20for%20various%20social%20networks.%20The%20asynchronous%0Atraining%20framework%20supports%20flexible%20participation.%20By%20adjusting%20the%20ratios%20of%0Ainactive%20participants%2C%20we%20found%20it%20remains%20stable%20if%20less%20than%2070%25%20are%0Ainactive.%20Our%20results%20confirm%20that%20GluADFL%20offers%20a%20practical%2C%0Aprivacy-preserving%20solution%20for%20BG%20prediction%20in%20T1D%2C%20significantly%20enhancing%0Athe%20quality%20of%20diabetes%20management.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15346v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrivacy%2520Preserved%2520Blood%2520Glucose%2520Level%2520Cross-Prediction%253A%2520An%2520Asynchronous%250A%2520%2520Decentralized%2520Federated%2520Learning%2520Approach%26entry.906535625%3DChengzhe%2520Piao%2520and%2520Taiyu%2520Zhu%2520and%2520Yu%2520Wang%2520and%2520Stephanie%2520E%2520Baldeweg%2520and%2520Paul%2520Taylor%2520and%2520Pantelis%2520Georgiou%2520and%2520Jiahao%2520Sun%2520and%2520Jun%2520Wang%2520and%2520Kezhi%2520Li%26entry.1292438233%3D%2520%2520Newly%2520diagnosed%2520Type%25201%2520Diabetes%2520%2528T1D%2529%2520patients%2520often%2520struggle%2520to%2520obtain%250Aeffective%2520Blood%2520Glucose%2520%2528BG%2529%2520prediction%2520models%2520due%2520to%2520the%2520lack%2520of%2520sufficient%2520BG%250Adata%2520from%2520Continuous%2520Glucose%2520Monitoring%2520%2528CGM%2529%252C%2520presenting%2520a%2520significant%2520%2522cold%250Astart%2522%2520problem%2520in%2520patient%2520care.%2520Utilizing%2520population%2520models%2520to%2520address%2520this%250Achallenge%2520is%2520a%2520potential%2520solution%252C%2520but%2520collecting%2520patient%2520data%2520for%2520training%250Apopulation%2520models%2520in%2520a%2520privacy-conscious%2520manner%2520is%2520challenging%252C%2520especially%250Agiven%2520that%2520such%2520data%2520is%2520often%2520stored%2520on%2520personal%2520devices.%2520Considering%2520the%250Aprivacy%2520protection%2520and%2520addressing%2520the%2520%2522cold%2520start%2522%2520problem%2520in%2520diabetes%2520care%252C%2520we%250Apropose%2520%2522GluADFL%2522%252C%2520blood%2520Glucose%2520prediction%2520by%2520Asynchronous%2520Decentralized%250AFederated%2520Learning.%2520We%2520compared%2520GluADFL%2520with%2520eight%2520baseline%2520methods%2520using%2520four%250Adistinct%2520T1D%2520datasets%252C%2520comprising%2520298%2520participants%252C%2520which%2520demonstrated%2520its%250Asuperior%2520performance%2520in%2520accurately%2520predicting%2520BG%2520levels%2520for%2520cross-patient%250Aanalysis.%2520Furthermore%252C%2520patients%2527%2520data%2520might%2520be%2520stored%2520and%2520shared%2520across%2520various%250Acommunication%2520networks%2520in%2520GluADFL%252C%2520ranging%2520from%2520highly%2520interconnected%2520%2528e.g.%252C%250Arandom%252C%2520performs%2520the%2520best%2520among%2520others%2529%2520to%2520more%2520structured%2520topologies%2520%2528e.g.%252C%250Acluster%2520and%2520ring%2529%252C%2520suitable%2520for%2520various%2520social%2520networks.%2520The%2520asynchronous%250Atraining%2520framework%2520supports%2520flexible%2520participation.%2520By%2520adjusting%2520the%2520ratios%2520of%250Ainactive%2520participants%252C%2520we%2520found%2520it%2520remains%2520stable%2520if%2520less%2520than%252070%2525%2520are%250Ainactive.%2520Our%2520results%2520confirm%2520that%2520GluADFL%2520offers%2520a%2520practical%252C%250Aprivacy-preserving%2520solution%2520for%2520BG%2520prediction%2520in%2520T1D%252C%2520significantly%2520enhancing%250Athe%2520quality%2520of%2520diabetes%2520management.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15346v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Privacy%20Preserved%20Blood%20Glucose%20Level%20Cross-Prediction%3A%20An%20Asynchronous%0A%20%20Decentralized%20Federated%20Learning%20Approach&entry.906535625=Chengzhe%20Piao%20and%20Taiyu%20Zhu%20and%20Yu%20Wang%20and%20Stephanie%20E%20Baldeweg%20and%20Paul%20Taylor%20and%20Pantelis%20Georgiou%20and%20Jiahao%20Sun%20and%20Jun%20Wang%20and%20Kezhi%20Li&entry.1292438233=%20%20Newly%20diagnosed%20Type%201%20Diabetes%20%28T1D%29%20patients%20often%20struggle%20to%20obtain%0Aeffective%20Blood%20Glucose%20%28BG%29%20prediction%20models%20due%20to%20the%20lack%20of%20sufficient%20BG%0Adata%20from%20Continuous%20Glucose%20Monitoring%20%28CGM%29%2C%20presenting%20a%20significant%20%22cold%0Astart%22%20problem%20in%20patient%20care.%20Utilizing%20population%20models%20to%20address%20this%0Achallenge%20is%20a%20potential%20solution%2C%20but%20collecting%20patient%20data%20for%20training%0Apopulation%20models%20in%20a%20privacy-conscious%20manner%20is%20challenging%2C%20especially%0Agiven%20that%20such%20data%20is%20often%20stored%20on%20personal%20devices.%20Considering%20the%0Aprivacy%20protection%20and%20addressing%20the%20%22cold%20start%22%20problem%20in%20diabetes%20care%2C%20we%0Apropose%20%22GluADFL%22%2C%20blood%20Glucose%20prediction%20by%20Asynchronous%20Decentralized%0AFederated%20Learning.%20We%20compared%20GluADFL%20with%20eight%20baseline%20methods%20using%20four%0Adistinct%20T1D%20datasets%2C%20comprising%20298%20participants%2C%20which%20demonstrated%20its%0Asuperior%20performance%20in%20accurately%20predicting%20BG%20levels%20for%20cross-patient%0Aanalysis.%20Furthermore%2C%20patients%27%20data%20might%20be%20stored%20and%20shared%20across%20various%0Acommunication%20networks%20in%20GluADFL%2C%20ranging%20from%20highly%20interconnected%20%28e.g.%2C%0Arandom%2C%20performs%20the%20best%20among%20others%29%20to%20more%20structured%20topologies%20%28e.g.%2C%0Acluster%20and%20ring%29%2C%20suitable%20for%20various%20social%20networks.%20The%20asynchronous%0Atraining%20framework%20supports%20flexible%20participation.%20By%20adjusting%20the%20ratios%20of%0Ainactive%20participants%2C%20we%20found%20it%20remains%20stable%20if%20less%20than%2070%25%20are%0Ainactive.%20Our%20results%20confirm%20that%20GluADFL%20offers%20a%20practical%2C%0Aprivacy-preserving%20solution%20for%20BG%20prediction%20in%20T1D%2C%20significantly%20enhancing%0Athe%20quality%20of%20diabetes%20management.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15346v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


