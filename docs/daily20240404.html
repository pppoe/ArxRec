<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }

    </style>
  </head>
  <body>

    <header>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Unsupervised Occupancy Learning from Sparse Point Cloud", "author": "Amine Ouasfi and Adnane Boukhayma", "abstract": "  Implicit Neural Representations have gained prominence as a powerful\nframework for capturing complex data modalities, encompassing a wide range from\n3D shapes to images and audio. Within the realm of 3D shape representation,\nNeural Signed Distance Functions (SDF) have demonstrated remarkable potential\nin faithfully encoding intricate shape geometry. However, learning SDFs from 3D\npoint clouds in the absence of ground truth supervision remains a very\nchallenging task. In this paper, we propose a method to infer occupancy fields\ninstead of SDFs as they are easier to learn from sparse inputs. We leverage a\nmargin-based uncertainty measure to differentially sample from the decision\nboundary of the occupancy function and supervise the sampled boundary points\nusing the input point cloud. We further stabilize the optimization process at\nthe early stages of the training by biasing the occupancy function towards\nminimal entropy fields while maximizing its entropy at the input point cloud.\nThrough extensive experiments and evaluations, we illustrate the efficacy of\nour proposed method, highlighting its capacity to improve implicit shape\ninference with respect to baselines and the state-of-the-art using synthetic\nand real data.\n", "link": "http://arxiv.org/abs/2404.02759v1", "date": "2024-04-03", "relevancy": 3.0188, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6646}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5895}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5573}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Occupancy%20Learning%20from%20Sparse%20Point%20Cloud&body=Title%3A%20Unsupervised%20Occupancy%20Learning%20from%20Sparse%20Point%20Cloud%0AAuthor%3A%20Amine%20Ouasfi%20and%20Adnane%20Boukhayma%0AAbstract%3A%20%20%20Implicit%20Neural%20Representations%20have%20gained%20prominence%20as%20a%20powerful%0Aframework%20for%20capturing%20complex%20data%20modalities%2C%20encompassing%20a%20wide%20range%20from%0A3D%20shapes%20to%20images%20and%20audio.%20Within%20the%20realm%20of%203D%20shape%20representation%2C%0ANeural%20Signed%20Distance%20Functions%20%28SDF%29%20have%20demonstrated%20remarkable%20potential%0Ain%20faithfully%20encoding%20intricate%20shape%20geometry.%20However%2C%20learning%20SDFs%20from%203D%0Apoint%20clouds%20in%20the%20absence%20of%20ground%20truth%20supervision%20remains%20a%20very%0Achallenging%20task.%20In%20this%20paper%2C%20we%20propose%20a%20method%20to%20infer%20occupancy%20fields%0Ainstead%20of%20SDFs%20as%20they%20are%20easier%20to%20learn%20from%20sparse%20inputs.%20We%20leverage%20a%0Amargin-based%20uncertainty%20measure%20to%20differentially%20sample%20from%20the%20decision%0Aboundary%20of%20the%20occupancy%20function%20and%20supervise%20the%20sampled%20boundary%20points%0Ausing%20the%20input%20point%20cloud.%20We%20further%20stabilize%20the%20optimization%20process%20at%0Athe%20early%20stages%20of%20the%20training%20by%20biasing%20the%20occupancy%20function%20towards%0Aminimal%20entropy%20fields%20while%20maximizing%20its%20entropy%20at%20the%20input%20point%20cloud.%0AThrough%20extensive%20experiments%20and%20evaluations%2C%20we%20illustrate%20the%20efficacy%20of%0Aour%20proposed%20method%2C%20highlighting%20its%20capacity%20to%20improve%20implicit%20shape%0Ainference%20with%20respect%20to%20baselines%20and%20the%20state-of-the-art%20using%20synthetic%0Aand%20real%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02759v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Occupancy%20Learning%20from%20Sparse%20Point%20Cloud&entry.906535625=Amine%20Ouasfi%20and%20Adnane%20Boukhayma&entry.1292438233=%20%20Implicit%20Neural%20Representations%20have%20gained%20prominence%20as%20a%20powerful%0Aframework%20for%20capturing%20complex%20data%20modalities%2C%20encompassing%20a%20wide%20range%20from%0A3D%20shapes%20to%20images%20and%20audio.%20Within%20the%20realm%20of%203D%20shape%20representation%2C%0ANeural%20Signed%20Distance%20Functions%20%28SDF%29%20have%20demonstrated%20remarkable%20potential%0Ain%20faithfully%20encoding%20intricate%20shape%20geometry.%20However%2C%20learning%20SDFs%20from%203D%0Apoint%20clouds%20in%20the%20absence%20of%20ground%20truth%20supervision%20remains%20a%20very%0Achallenging%20task.%20In%20this%20paper%2C%20we%20propose%20a%20method%20to%20infer%20occupancy%20fields%0Ainstead%20of%20SDFs%20as%20they%20are%20easier%20to%20learn%20from%20sparse%20inputs.%20We%20leverage%20a%0Amargin-based%20uncertainty%20measure%20to%20differentially%20sample%20from%20the%20decision%0Aboundary%20of%20the%20occupancy%20function%20and%20supervise%20the%20sampled%20boundary%20points%0Ausing%20the%20input%20point%20cloud.%20We%20further%20stabilize%20the%20optimization%20process%20at%0Athe%20early%20stages%20of%20the%20training%20by%20biasing%20the%20occupancy%20function%20towards%0Aminimal%20entropy%20fields%20while%20maximizing%20its%20entropy%20at%20the%20input%20point%20cloud.%0AThrough%20extensive%20experiments%20and%20evaluations%2C%20we%20illustrate%20the%20efficacy%20of%0Aour%20proposed%20method%2C%20highlighting%20its%20capacity%20to%20improve%20implicit%20shape%0Ainference%20with%20respect%20to%20baselines%20and%20the%20state-of-the-art%20using%20synthetic%0Aand%20real%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02759v1&entry.124074799=Read"},
{"title": "GenN2N: Generative NeRF2NeRF Translation", "author": "Xiangyue Liu and Han Xue and Kunming Luo and Ping Tan and Li Yi", "abstract": "  We present GenN2N, a unified NeRF-to-NeRF translation framework for various\nNeRF translation tasks such as text-driven NeRF editing, colorization,\nsuper-resolution, inpainting, etc. Unlike previous methods designed for\nindividual translation tasks with task-specific schemes, GenN2N achieves all\nthese NeRF editing tasks by employing a plug-and-play image-to-image translator\nto perform editing in the 2D domain and lifting 2D edits into the 3D NeRF\nspace. Since the 3D consistency of 2D edits may not be assured, we propose to\nmodel the distribution of the underlying 3D edits through a generative model\nthat can cover all possible edited NeRFs. To model the distribution of 3D\nedited NeRFs from 2D edited images, we carefully design a VAE-GAN that encodes\nimages while decoding NeRFs. The latent space is trained to align with a\nGaussian distribution and the NeRFs are supervised through an adversarial loss\non its renderings. To ensure the latent code does not depend on 2D viewpoints\nbut truly reflects the 3D edits, we also regularize the latent code through a\ncontrastive learning scheme. Extensive experiments on various editing tasks\nshow GenN2N, as a universal framework, performs as well or better than\ntask-specific specialists while possessing flexible generative power. More\nresults on our project page: https://xiangyueliu.github.io/GenN2N/\n", "link": "http://arxiv.org/abs/2404.02788v1", "date": "2024-04-03", "relevancy": 2.8853, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5925}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5779}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5608}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GenN2N%3A%20Generative%20NeRF2NeRF%20Translation&body=Title%3A%20GenN2N%3A%20Generative%20NeRF2NeRF%20Translation%0AAuthor%3A%20Xiangyue%20Liu%20and%20Han%20Xue%20and%20Kunming%20Luo%20and%20Ping%20Tan%20and%20Li%20Yi%0AAbstract%3A%20%20%20We%20present%20GenN2N%2C%20a%20unified%20NeRF-to-NeRF%20translation%20framework%20for%20various%0ANeRF%20translation%20tasks%20such%20as%20text-driven%20NeRF%20editing%2C%20colorization%2C%0Asuper-resolution%2C%20inpainting%2C%20etc.%20Unlike%20previous%20methods%20designed%20for%0Aindividual%20translation%20tasks%20with%20task-specific%20schemes%2C%20GenN2N%20achieves%20all%0Athese%20NeRF%20editing%20tasks%20by%20employing%20a%20plug-and-play%20image-to-image%20translator%0Ato%20perform%20editing%20in%20the%202D%20domain%20and%20lifting%202D%20edits%20into%20the%203D%20NeRF%0Aspace.%20Since%20the%203D%20consistency%20of%202D%20edits%20may%20not%20be%20assured%2C%20we%20propose%20to%0Amodel%20the%20distribution%20of%20the%20underlying%203D%20edits%20through%20a%20generative%20model%0Athat%20can%20cover%20all%20possible%20edited%20NeRFs.%20To%20model%20the%20distribution%20of%203D%0Aedited%20NeRFs%20from%202D%20edited%20images%2C%20we%20carefully%20design%20a%20VAE-GAN%20that%20encodes%0Aimages%20while%20decoding%20NeRFs.%20The%20latent%20space%20is%20trained%20to%20align%20with%20a%0AGaussian%20distribution%20and%20the%20NeRFs%20are%20supervised%20through%20an%20adversarial%20loss%0Aon%20its%20renderings.%20To%20ensure%20the%20latent%20code%20does%20not%20depend%20on%202D%20viewpoints%0Abut%20truly%20reflects%20the%203D%20edits%2C%20we%20also%20regularize%20the%20latent%20code%20through%20a%0Acontrastive%20learning%20scheme.%20Extensive%20experiments%20on%20various%20editing%20tasks%0Ashow%20GenN2N%2C%20as%20a%20universal%20framework%2C%20performs%20as%20well%20or%20better%20than%0Atask-specific%20specialists%20while%20possessing%20flexible%20generative%20power.%20More%0Aresults%20on%20our%20project%20page%3A%20https%3A//xiangyueliu.github.io/GenN2N/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02788v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenN2N%3A%20Generative%20NeRF2NeRF%20Translation&entry.906535625=Xiangyue%20Liu%20and%20Han%20Xue%20and%20Kunming%20Luo%20and%20Ping%20Tan%20and%20Li%20Yi&entry.1292438233=%20%20We%20present%20GenN2N%2C%20a%20unified%20NeRF-to-NeRF%20translation%20framework%20for%20various%0ANeRF%20translation%20tasks%20such%20as%20text-driven%20NeRF%20editing%2C%20colorization%2C%0Asuper-resolution%2C%20inpainting%2C%20etc.%20Unlike%20previous%20methods%20designed%20for%0Aindividual%20translation%20tasks%20with%20task-specific%20schemes%2C%20GenN2N%20achieves%20all%0Athese%20NeRF%20editing%20tasks%20by%20employing%20a%20plug-and-play%20image-to-image%20translator%0Ato%20perform%20editing%20in%20the%202D%20domain%20and%20lifting%202D%20edits%20into%20the%203D%20NeRF%0Aspace.%20Since%20the%203D%20consistency%20of%202D%20edits%20may%20not%20be%20assured%2C%20we%20propose%20to%0Amodel%20the%20distribution%20of%20the%20underlying%203D%20edits%20through%20a%20generative%20model%0Athat%20can%20cover%20all%20possible%20edited%20NeRFs.%20To%20model%20the%20distribution%20of%203D%0Aedited%20NeRFs%20from%202D%20edited%20images%2C%20we%20carefully%20design%20a%20VAE-GAN%20that%20encodes%0Aimages%20while%20decoding%20NeRFs.%20The%20latent%20space%20is%20trained%20to%20align%20with%20a%0AGaussian%20distribution%20and%20the%20NeRFs%20are%20supervised%20through%20an%20adversarial%20loss%0Aon%20its%20renderings.%20To%20ensure%20the%20latent%20code%20does%20not%20depend%20on%202D%20viewpoints%0Abut%20truly%20reflects%20the%203D%20edits%2C%20we%20also%20regularize%20the%20latent%20code%20through%20a%0Acontrastive%20learning%20scheme.%20Extensive%20experiments%20on%20various%20editing%20tasks%0Ashow%20GenN2N%2C%20as%20a%20universal%20framework%2C%20performs%20as%20well%20or%20better%20than%0Atask-specific%20specialists%20while%20possessing%20flexible%20generative%20power.%20More%0Aresults%20on%20our%20project%20page%3A%20https%3A//xiangyueliu.github.io/GenN2N/%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02788v1&entry.124074799=Read"},
{"title": "Generative-Contrastive Heterogeneous Graph Neural Network", "author": "Yu Wang and Lei Sang and Yi Zhang and Yiwen Zhang", "abstract": "  Heterogeneous Graphs (HGs) can effectively model complex relationships in the\nreal world by multi-type nodes and edges. In recent years, inspired by\nself-supervised learning, contrastive Heterogeneous Graphs Neural Networks\n(HGNNs) have shown great potential by utilizing data augmentation and\ndiscriminators for downstream tasks. However, data augmentation is still\nlimited due to the discrete and abstract nature of graphs. To tackle the above\nlimitations, we propose a novel \\textit{Generative-Contrastive Heterogeneous\nGraph Neural Network (GC-HGNN)}. Specifically, we first propose a heterogeneous\ngraph generative learning enhanced contrastive paradigm. This paradigm\nincludes: 1) A contrastive view augmentation strategy by using masked\nautoencoder. 2) Position-aware and semantics-aware positive sample sampling\nstrategy for generate hard negative samples. 3) A hierarchical contrastive\nlearning strategy for capturing local and global information. Furthermore, the\nhierarchical contrastive learning and sampling strategies aim to constitute an\nenhanced discriminator under the generative-contrastive perspective. Finally,\nwe compare our model with seventeen baselines on eight real-world datasets. Our\nmodel outperforms the latest contrastive and generative baselines on node\nclassification and link prediction tasks. To reproduce our work, we have\nopen-sourced our code at https://github.com/xxx.\n", "link": "http://arxiv.org/abs/2404.02810v1", "date": "2024-04-03", "relevancy": 2.8089, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5968}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5506}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.538}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Generative-Contrastive%20Heterogeneous%20Graph%20Neural%20Network&body=Title%3A%20Generative-Contrastive%20Heterogeneous%20Graph%20Neural%20Network%0AAuthor%3A%20Yu%20Wang%20and%20Lei%20Sang%20and%20Yi%20Zhang%20and%20Yiwen%20Zhang%0AAbstract%3A%20%20%20Heterogeneous%20Graphs%20%28HGs%29%20can%20effectively%20model%20complex%20relationships%20in%20the%0Areal%20world%20by%20multi-type%20nodes%20and%20edges.%20In%20recent%20years%2C%20inspired%20by%0Aself-supervised%20learning%2C%20contrastive%20Heterogeneous%20Graphs%20Neural%20Networks%0A%28HGNNs%29%20have%20shown%20great%20potential%20by%20utilizing%20data%20augmentation%20and%0Adiscriminators%20for%20downstream%20tasks.%20However%2C%20data%20augmentation%20is%20still%0Alimited%20due%20to%20the%20discrete%20and%20abstract%20nature%20of%20graphs.%20To%20tackle%20the%20above%0Alimitations%2C%20we%20propose%20a%20novel%20%5Ctextit%7BGenerative-Contrastive%20Heterogeneous%0AGraph%20Neural%20Network%20%28GC-HGNN%29%7D.%20Specifically%2C%20we%20first%20propose%20a%20heterogeneous%0Agraph%20generative%20learning%20enhanced%20contrastive%20paradigm.%20This%20paradigm%0Aincludes%3A%201%29%20A%20contrastive%20view%20augmentation%20strategy%20by%20using%20masked%0Aautoencoder.%202%29%20Position-aware%20and%20semantics-aware%20positive%20sample%20sampling%0Astrategy%20for%20generate%20hard%20negative%20samples.%203%29%20A%20hierarchical%20contrastive%0Alearning%20strategy%20for%20capturing%20local%20and%20global%20information.%20Furthermore%2C%20the%0Ahierarchical%20contrastive%20learning%20and%20sampling%20strategies%20aim%20to%20constitute%20an%0Aenhanced%20discriminator%20under%20the%20generative-contrastive%20perspective.%20Finally%2C%0Awe%20compare%20our%20model%20with%20seventeen%20baselines%20on%20eight%20real-world%20datasets.%20Our%0Amodel%20outperforms%20the%20latest%20contrastive%20and%20generative%20baselines%20on%20node%0Aclassification%20and%20link%20prediction%20tasks.%20To%20reproduce%20our%20work%2C%20we%20have%0Aopen-sourced%20our%20code%20at%20https%3A//github.com/xxx.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02810v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative-Contrastive%20Heterogeneous%20Graph%20Neural%20Network&entry.906535625=Yu%20Wang%20and%20Lei%20Sang%20and%20Yi%20Zhang%20and%20Yiwen%20Zhang&entry.1292438233=%20%20Heterogeneous%20Graphs%20%28HGs%29%20can%20effectively%20model%20complex%20relationships%20in%20the%0Areal%20world%20by%20multi-type%20nodes%20and%20edges.%20In%20recent%20years%2C%20inspired%20by%0Aself-supervised%20learning%2C%20contrastive%20Heterogeneous%20Graphs%20Neural%20Networks%0A%28HGNNs%29%20have%20shown%20great%20potential%20by%20utilizing%20data%20augmentation%20and%0Adiscriminators%20for%20downstream%20tasks.%20However%2C%20data%20augmentation%20is%20still%0Alimited%20due%20to%20the%20discrete%20and%20abstract%20nature%20of%20graphs.%20To%20tackle%20the%20above%0Alimitations%2C%20we%20propose%20a%20novel%20%5Ctextit%7BGenerative-Contrastive%20Heterogeneous%0AGraph%20Neural%20Network%20%28GC-HGNN%29%7D.%20Specifically%2C%20we%20first%20propose%20a%20heterogeneous%0Agraph%20generative%20learning%20enhanced%20contrastive%20paradigm.%20This%20paradigm%0Aincludes%3A%201%29%20A%20contrastive%20view%20augmentation%20strategy%20by%20using%20masked%0Aautoencoder.%202%29%20Position-aware%20and%20semantics-aware%20positive%20sample%20sampling%0Astrategy%20for%20generate%20hard%20negative%20samples.%203%29%20A%20hierarchical%20contrastive%0Alearning%20strategy%20for%20capturing%20local%20and%20global%20information.%20Furthermore%2C%20the%0Ahierarchical%20contrastive%20learning%20and%20sampling%20strategies%20aim%20to%20constitute%20an%0Aenhanced%20discriminator%20under%20the%20generative-contrastive%20perspective.%20Finally%2C%0Awe%20compare%20our%20model%20with%20seventeen%20baselines%20on%20eight%20real-world%20datasets.%20Our%0Amodel%20outperforms%20the%20latest%20contrastive%20and%20generative%20baselines%20on%20node%0Aclassification%20and%20link%20prediction%20tasks.%20To%20reproduce%20our%20work%2C%20we%20have%0Aopen-sourced%20our%20code%20at%20https%3A//github.com/xxx.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02810v1&entry.124074799=Read"},
{"title": "Proper Laplacian Representation Learning", "author": "Diego Gomez and Michael Bowling and Marlos C. Machado", "abstract": "  The ability to learn good representations of states is essential for solving\nlarge reinforcement learning problems, where exploration, generalization, and\ntransfer are particularly challenging. The Laplacian representation is a\npromising approach to address these problems by inducing informative state\nencoding and intrinsic rewards for temporally-extended action discovery and\nreward shaping. To obtain the Laplacian representation one needs to compute the\neigensystem of the graph Laplacian, which is often approximated through\noptimization objectives compatible with deep learning approaches. These\napproximations, however, depend on hyperparameters that are impossible to tune\nefficiently, converge to arbitrary rotations of the desired eigenvectors, and\nare unable to accurately recover the corresponding eigenvalues. In this paper\nwe introduce a theoretically sound objective and corresponding optimization\nalgorithm for approximating the Laplacian representation. Our approach\nnaturally recovers both the true eigenvectors and eigenvalues while eliminating\nthe hyperparameter dependence of previous approximations. We provide\ntheoretical guarantees for our method and we show that those results translate\nempirically into robust learning across multiple environments.\n", "link": "http://arxiv.org/abs/2310.10833v2", "date": "2024-04-03", "relevancy": 2.7542, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6201}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5324}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Proper%20Laplacian%20Representation%20Learning&body=Title%3A%20Proper%20Laplacian%20Representation%20Learning%0AAuthor%3A%20Diego%20Gomez%20and%20Michael%20Bowling%20and%20Marlos%20C.%20Machado%0AAbstract%3A%20%20%20The%20ability%20to%20learn%20good%20representations%20of%20states%20is%20essential%20for%20solving%0Alarge%20reinforcement%20learning%20problems%2C%20where%20exploration%2C%20generalization%2C%20and%0Atransfer%20are%20particularly%20challenging.%20The%20Laplacian%20representation%20is%20a%0Apromising%20approach%20to%20address%20these%20problems%20by%20inducing%20informative%20state%0Aencoding%20and%20intrinsic%20rewards%20for%20temporally-extended%20action%20discovery%20and%0Areward%20shaping.%20To%20obtain%20the%20Laplacian%20representation%20one%20needs%20to%20compute%20the%0Aeigensystem%20of%20the%20graph%20Laplacian%2C%20which%20is%20often%20approximated%20through%0Aoptimization%20objectives%20compatible%20with%20deep%20learning%20approaches.%20These%0Aapproximations%2C%20however%2C%20depend%20on%20hyperparameters%20that%20are%20impossible%20to%20tune%0Aefficiently%2C%20converge%20to%20arbitrary%20rotations%20of%20the%20desired%20eigenvectors%2C%20and%0Aare%20unable%20to%20accurately%20recover%20the%20corresponding%20eigenvalues.%20In%20this%20paper%0Awe%20introduce%20a%20theoretically%20sound%20objective%20and%20corresponding%20optimization%0Aalgorithm%20for%20approximating%20the%20Laplacian%20representation.%20Our%20approach%0Anaturally%20recovers%20both%20the%20true%20eigenvectors%20and%20eigenvalues%20while%20eliminating%0Athe%20hyperparameter%20dependence%20of%20previous%20approximations.%20We%20provide%0Atheoretical%20guarantees%20for%20our%20method%20and%20we%20show%20that%20those%20results%20translate%0Aempirically%20into%20robust%20learning%20across%20multiple%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.10833v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Proper%20Laplacian%20Representation%20Learning&entry.906535625=Diego%20Gomez%20and%20Michael%20Bowling%20and%20Marlos%20C.%20Machado&entry.1292438233=%20%20The%20ability%20to%20learn%20good%20representations%20of%20states%20is%20essential%20for%20solving%0Alarge%20reinforcement%20learning%20problems%2C%20where%20exploration%2C%20generalization%2C%20and%0Atransfer%20are%20particularly%20challenging.%20The%20Laplacian%20representation%20is%20a%0Apromising%20approach%20to%20address%20these%20problems%20by%20inducing%20informative%20state%0Aencoding%20and%20intrinsic%20rewards%20for%20temporally-extended%20action%20discovery%20and%0Areward%20shaping.%20To%20obtain%20the%20Laplacian%20representation%20one%20needs%20to%20compute%20the%0Aeigensystem%20of%20the%20graph%20Laplacian%2C%20which%20is%20often%20approximated%20through%0Aoptimization%20objectives%20compatible%20with%20deep%20learning%20approaches.%20These%0Aapproximations%2C%20however%2C%20depend%20on%20hyperparameters%20that%20are%20impossible%20to%20tune%0Aefficiently%2C%20converge%20to%20arbitrary%20rotations%20of%20the%20desired%20eigenvectors%2C%20and%0Aare%20unable%20to%20accurately%20recover%20the%20corresponding%20eigenvalues.%20In%20this%20paper%0Awe%20introduce%20a%20theoretically%20sound%20objective%20and%20corresponding%20optimization%0Aalgorithm%20for%20approximating%20the%20Laplacian%20representation.%20Our%20approach%0Anaturally%20recovers%20both%20the%20true%20eigenvectors%20and%20eigenvalues%20while%20eliminating%0Athe%20hyperparameter%20dependence%20of%20previous%20approximations.%20We%20provide%0Atheoretical%20guarantees%20for%20our%20method%20and%20we%20show%20that%20those%20results%20translate%0Aempirically%20into%20robust%20learning%20across%20multiple%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.10833v2&entry.124074799=Read"},
{"title": "Towards Seamless Adaptation of Pre-trained Models for Visual Place\n  Recognition", "author": "Feng Lu and Lijun Zhang and Xiangyuan Lan and Shuting Dong and Yaowei Wang and Chun Yuan", "abstract": "  Recent studies show that vision models pre-trained in generic visual learning\ntasks with large-scale data can provide useful feature representations for a\nwide range of visual perception problems. However, few attempts have been made\nto exploit pre-trained foundation models in visual place recognition (VPR). Due\nto the inherent difference in training objectives and data between the tasks of\nmodel pre-training and VPR, how to bridge the gap and fully unleash the\ncapability of pre-trained models for VPR is still a key issue to address. To\nthis end, we propose a novel method to realize seamless adaptation of\npre-trained models for VPR. Specifically, to obtain both global and local\nfeatures that focus on salient landmarks for discriminating places, we design a\nhybrid adaptation method to achieve both global and local adaptation\nefficiently, in which only lightweight adapters are tuned without adjusting the\npre-trained model. Besides, to guide effective adaptation, we propose a mutual\nnearest neighbor local feature loss, which ensures proper dense local features\nare produced for local matching and avoids time-consuming spatial verification\nin re-ranking. Experimental results show that our method outperforms the\nstate-of-the-art methods with less training data and training time, and uses\nabout only 3% retrieval runtime of the two-stage VPR methods with RANSAC-based\nspatial verification. It ranks 1st on the MSLS challenge leaderboard (at the\ntime of submission). The code is released at\nhttps://github.com/Lu-Feng/SelaVPR.\n", "link": "http://arxiv.org/abs/2402.14505v3", "date": "2024-04-03", "relevancy": 2.7275, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5479}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5447}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5439}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20Seamless%20Adaptation%20of%20Pre-trained%20Models%20for%20Visual%20Place%0A%20%20Recognition&body=Title%3A%20Towards%20Seamless%20Adaptation%20of%20Pre-trained%20Models%20for%20Visual%20Place%0A%20%20Recognition%0AAuthor%3A%20Feng%20Lu%20and%20Lijun%20Zhang%20and%20Xiangyuan%20Lan%20and%20Shuting%20Dong%20and%20Yaowei%20Wang%20and%20Chun%20Yuan%0AAbstract%3A%20%20%20Recent%20studies%20show%20that%20vision%20models%20pre-trained%20in%20generic%20visual%20learning%0Atasks%20with%20large-scale%20data%20can%20provide%20useful%20feature%20representations%20for%20a%0Awide%20range%20of%20visual%20perception%20problems.%20However%2C%20few%20attempts%20have%20been%20made%0Ato%20exploit%20pre-trained%20foundation%20models%20in%20visual%20place%20recognition%20%28VPR%29.%20Due%0Ato%20the%20inherent%20difference%20in%20training%20objectives%20and%20data%20between%20the%20tasks%20of%0Amodel%20pre-training%20and%20VPR%2C%20how%20to%20bridge%20the%20gap%20and%20fully%20unleash%20the%0Acapability%20of%20pre-trained%20models%20for%20VPR%20is%20still%20a%20key%20issue%20to%20address.%20To%0Athis%20end%2C%20we%20propose%20a%20novel%20method%20to%20realize%20seamless%20adaptation%20of%0Apre-trained%20models%20for%20VPR.%20Specifically%2C%20to%20obtain%20both%20global%20and%20local%0Afeatures%20that%20focus%20on%20salient%20landmarks%20for%20discriminating%20places%2C%20we%20design%20a%0Ahybrid%20adaptation%20method%20to%20achieve%20both%20global%20and%20local%20adaptation%0Aefficiently%2C%20in%20which%20only%20lightweight%20adapters%20are%20tuned%20without%20adjusting%20the%0Apre-trained%20model.%20Besides%2C%20to%20guide%20effective%20adaptation%2C%20we%20propose%20a%20mutual%0Anearest%20neighbor%20local%20feature%20loss%2C%20which%20ensures%20proper%20dense%20local%20features%0Aare%20produced%20for%20local%20matching%20and%20avoids%20time-consuming%20spatial%20verification%0Ain%20re-ranking.%20Experimental%20results%20show%20that%20our%20method%20outperforms%20the%0Astate-of-the-art%20methods%20with%20less%20training%20data%20and%20training%20time%2C%20and%20uses%0Aabout%20only%203%25%20retrieval%20runtime%20of%20the%20two-stage%20VPR%20methods%20with%20RANSAC-based%0Aspatial%20verification.%20It%20ranks%201st%20on%20the%20MSLS%20challenge%20leaderboard%20%28at%20the%0Atime%20of%20submission%29.%20The%20code%20is%20released%20at%0Ahttps%3A//github.com/Lu-Feng/SelaVPR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14505v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Seamless%20Adaptation%20of%20Pre-trained%20Models%20for%20Visual%20Place%0A%20%20Recognition&entry.906535625=Feng%20Lu%20and%20Lijun%20Zhang%20and%20Xiangyuan%20Lan%20and%20Shuting%20Dong%20and%20Yaowei%20Wang%20and%20Chun%20Yuan&entry.1292438233=%20%20Recent%20studies%20show%20that%20vision%20models%20pre-trained%20in%20generic%20visual%20learning%0Atasks%20with%20large-scale%20data%20can%20provide%20useful%20feature%20representations%20for%20a%0Awide%20range%20of%20visual%20perception%20problems.%20However%2C%20few%20attempts%20have%20been%20made%0Ato%20exploit%20pre-trained%20foundation%20models%20in%20visual%20place%20recognition%20%28VPR%29.%20Due%0Ato%20the%20inherent%20difference%20in%20training%20objectives%20and%20data%20between%20the%20tasks%20of%0Amodel%20pre-training%20and%20VPR%2C%20how%20to%20bridge%20the%20gap%20and%20fully%20unleash%20the%0Acapability%20of%20pre-trained%20models%20for%20VPR%20is%20still%20a%20key%20issue%20to%20address.%20To%0Athis%20end%2C%20we%20propose%20a%20novel%20method%20to%20realize%20seamless%20adaptation%20of%0Apre-trained%20models%20for%20VPR.%20Specifically%2C%20to%20obtain%20both%20global%20and%20local%0Afeatures%20that%20focus%20on%20salient%20landmarks%20for%20discriminating%20places%2C%20we%20design%20a%0Ahybrid%20adaptation%20method%20to%20achieve%20both%20global%20and%20local%20adaptation%0Aefficiently%2C%20in%20which%20only%20lightweight%20adapters%20are%20tuned%20without%20adjusting%20the%0Apre-trained%20model.%20Besides%2C%20to%20guide%20effective%20adaptation%2C%20we%20propose%20a%20mutual%0Anearest%20neighbor%20local%20feature%20loss%2C%20which%20ensures%20proper%20dense%20local%20features%0Aare%20produced%20for%20local%20matching%20and%20avoids%20time-consuming%20spatial%20verification%0Ain%20re-ranking.%20Experimental%20results%20show%20that%20our%20method%20outperforms%20the%0Astate-of-the-art%20methods%20with%20less%20training%20data%20and%20training%20time%2C%20and%20uses%0Aabout%20only%203%25%20retrieval%20runtime%20of%20the%20two-stage%20VPR%20methods%20with%20RANSAC-based%0Aspatial%20verification.%20It%20ranks%201st%20on%20the%20MSLS%20challenge%20leaderboard%20%28at%20the%0Atime%20of%20submission%29.%20The%20code%20is%20released%20at%0Ahttps%3A//github.com/Lu-Feng/SelaVPR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14505v3&entry.124074799=Read"},
{"title": "MCL-GAN: Generative Adversarial Networks with Multiple Specialized\n  Discriminators", "author": "Jinyoung Choi and Bohyung Han", "abstract": "  We propose a framework of generative adversarial networks with multiple\ndiscriminators, which collaborate to represent a real dataset more effectively.\nOur approach facilitates learning a generator consistent with the underlying\ndata distribution based on real images and thus mitigates the chronic mode\ncollapse problem. From the inspiration of multiple choice learning, we guide\neach discriminator to have expertise in a subset of the entire data and allow\nthe generator to find reasonable correspondences between the latent and real\ndata spaces automatically without extra supervision for training examples.\nDespite the use of multiple discriminators, the backbone networks are shared\nacross the discriminators and the increase in training cost is marginal. We\ndemonstrate the effectiveness of our algorithm using multiple evaluation\nmetrics in the standard datasets for diverse tasks.\n", "link": "http://arxiv.org/abs/2107.07260v3", "date": "2024-04-03", "relevancy": 2.705, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5451}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5424}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5355}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MCL-GAN%3A%20Generative%20Adversarial%20Networks%20with%20Multiple%20Specialized%0A%20%20Discriminators&body=Title%3A%20MCL-GAN%3A%20Generative%20Adversarial%20Networks%20with%20Multiple%20Specialized%0A%20%20Discriminators%0AAuthor%3A%20Jinyoung%20Choi%20and%20Bohyung%20Han%0AAbstract%3A%20%20%20We%20propose%20a%20framework%20of%20generative%20adversarial%20networks%20with%20multiple%0Adiscriminators%2C%20which%20collaborate%20to%20represent%20a%20real%20dataset%20more%20effectively.%0AOur%20approach%20facilitates%20learning%20a%20generator%20consistent%20with%20the%20underlying%0Adata%20distribution%20based%20on%20real%20images%20and%20thus%20mitigates%20the%20chronic%20mode%0Acollapse%20problem.%20From%20the%20inspiration%20of%20multiple%20choice%20learning%2C%20we%20guide%0Aeach%20discriminator%20to%20have%20expertise%20in%20a%20subset%20of%20the%20entire%20data%20and%20allow%0Athe%20generator%20to%20find%20reasonable%20correspondences%20between%20the%20latent%20and%20real%0Adata%20spaces%20automatically%20without%20extra%20supervision%20for%20training%20examples.%0ADespite%20the%20use%20of%20multiple%20discriminators%2C%20the%20backbone%20networks%20are%20shared%0Aacross%20the%20discriminators%20and%20the%20increase%20in%20training%20cost%20is%20marginal.%20We%0Ademonstrate%20the%20effectiveness%20of%20our%20algorithm%20using%20multiple%20evaluation%0Ametrics%20in%20the%20standard%20datasets%20for%20diverse%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2107.07260v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MCL-GAN%3A%20Generative%20Adversarial%20Networks%20with%20Multiple%20Specialized%0A%20%20Discriminators&entry.906535625=Jinyoung%20Choi%20and%20Bohyung%20Han&entry.1292438233=%20%20We%20propose%20a%20framework%20of%20generative%20adversarial%20networks%20with%20multiple%0Adiscriminators%2C%20which%20collaborate%20to%20represent%20a%20real%20dataset%20more%20effectively.%0AOur%20approach%20facilitates%20learning%20a%20generator%20consistent%20with%20the%20underlying%0Adata%20distribution%20based%20on%20real%20images%20and%20thus%20mitigates%20the%20chronic%20mode%0Acollapse%20problem.%20From%20the%20inspiration%20of%20multiple%20choice%20learning%2C%20we%20guide%0Aeach%20discriminator%20to%20have%20expertise%20in%20a%20subset%20of%20the%20entire%20data%20and%20allow%0Athe%20generator%20to%20find%20reasonable%20correspondences%20between%20the%20latent%20and%20real%0Adata%20spaces%20automatically%20without%20extra%20supervision%20for%20training%20examples.%0ADespite%20the%20use%20of%20multiple%20discriminators%2C%20the%20backbone%20networks%20are%20shared%0Aacross%20the%20discriminators%20and%20the%20increase%20in%20training%20cost%20is%20marginal.%20We%0Ademonstrate%20the%20effectiveness%20of%20our%20algorithm%20using%20multiple%20evaluation%0Ametrics%20in%20the%20standard%20datasets%20for%20diverse%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2107.07260v3&entry.124074799=Read"},
{"title": "SIGMA: Scale-Invariant Global Sparse Shape Matching", "author": "Maolin Gao and Paul Roetzer and Marvin Eisenberger and Zorah L\u00e4hner and Michael Moeller and Daniel Cremers and Florian Bernard", "abstract": "  We propose a novel mixed-integer programming (MIP) formulation for generating\nprecise sparse correspondences for highly non-rigid shapes. To this end, we\nintroduce a projected Laplace-Beltrami operator (PLBO) which combines intrinsic\nand extrinsic geometric information to measure the deformation quality induced\nby predicted correspondences. We integrate the PLBO, together with an\norientation-aware regulariser, into a novel MIP formulation that can be solved\nto global optimality for many practical problems. In contrast to previous\nmethods, our approach is provably invariant to rigid transformations and global\nscaling, initialisation-free, has optimality guarantees, and scales to high\nresolution meshes with (empirically observed) linear time. We show\nstate-of-the-art results for sparse non-rigid matching on several challenging\n3D datasets, including data with inconsistent meshing, as well as applications\nin mesh-to-point-cloud matching.\n", "link": "http://arxiv.org/abs/2308.08393v2", "date": "2024-04-03", "relevancy": 2.6125, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5268}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5233}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5175}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SIGMA%3A%20Scale-Invariant%20Global%20Sparse%20Shape%20Matching&body=Title%3A%20SIGMA%3A%20Scale-Invariant%20Global%20Sparse%20Shape%20Matching%0AAuthor%3A%20Maolin%20Gao%20and%20Paul%20Roetzer%20and%20Marvin%20Eisenberger%20and%20Zorah%20L%C3%A4hner%20and%20Michael%20Moeller%20and%20Daniel%20Cremers%20and%20Florian%20Bernard%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20mixed-integer%20programming%20%28MIP%29%20formulation%20for%20generating%0Aprecise%20sparse%20correspondences%20for%20highly%20non-rigid%20shapes.%20To%20this%20end%2C%20we%0Aintroduce%20a%20projected%20Laplace-Beltrami%20operator%20%28PLBO%29%20which%20combines%20intrinsic%0Aand%20extrinsic%20geometric%20information%20to%20measure%20the%20deformation%20quality%20induced%0Aby%20predicted%20correspondences.%20We%20integrate%20the%20PLBO%2C%20together%20with%20an%0Aorientation-aware%20regulariser%2C%20into%20a%20novel%20MIP%20formulation%20that%20can%20be%20solved%0Ato%20global%20optimality%20for%20many%20practical%20problems.%20In%20contrast%20to%20previous%0Amethods%2C%20our%20approach%20is%20provably%20invariant%20to%20rigid%20transformations%20and%20global%0Ascaling%2C%20initialisation-free%2C%20has%20optimality%20guarantees%2C%20and%20scales%20to%20high%0Aresolution%20meshes%20with%20%28empirically%20observed%29%20linear%20time.%20We%20show%0Astate-of-the-art%20results%20for%20sparse%20non-rigid%20matching%20on%20several%20challenging%0A3D%20datasets%2C%20including%20data%20with%20inconsistent%20meshing%2C%20as%20well%20as%20applications%0Ain%20mesh-to-point-cloud%20matching.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.08393v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SIGMA%3A%20Scale-Invariant%20Global%20Sparse%20Shape%20Matching&entry.906535625=Maolin%20Gao%20and%20Paul%20Roetzer%20and%20Marvin%20Eisenberger%20and%20Zorah%20L%C3%A4hner%20and%20Michael%20Moeller%20and%20Daniel%20Cremers%20and%20Florian%20Bernard&entry.1292438233=%20%20We%20propose%20a%20novel%20mixed-integer%20programming%20%28MIP%29%20formulation%20for%20generating%0Aprecise%20sparse%20correspondences%20for%20highly%20non-rigid%20shapes.%20To%20this%20end%2C%20we%0Aintroduce%20a%20projected%20Laplace-Beltrami%20operator%20%28PLBO%29%20which%20combines%20intrinsic%0Aand%20extrinsic%20geometric%20information%20to%20measure%20the%20deformation%20quality%20induced%0Aby%20predicted%20correspondences.%20We%20integrate%20the%20PLBO%2C%20together%20with%20an%0Aorientation-aware%20regulariser%2C%20into%20a%20novel%20MIP%20formulation%20that%20can%20be%20solved%0Ato%20global%20optimality%20for%20many%20practical%20problems.%20In%20contrast%20to%20previous%0Amethods%2C%20our%20approach%20is%20provably%20invariant%20to%20rigid%20transformations%20and%20global%0Ascaling%2C%20initialisation-free%2C%20has%20optimality%20guarantees%2C%20and%20scales%20to%20high%0Aresolution%20meshes%20with%20%28empirically%20observed%29%20linear%20time.%20We%20show%0Astate-of-the-art%20results%20for%20sparse%20non-rigid%20matching%20on%20several%20challenging%0A3D%20datasets%2C%20including%20data%20with%20inconsistent%20meshing%2C%20as%20well%20as%20applications%0Ain%20mesh-to-point-cloud%20matching.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.08393v2&entry.124074799=Read"},
{"title": "End-To-End Self-tuning Self-supervised Time Series Anomaly Detection", "author": "Boje Deforce and Meng-Chieh Lee and Bart Baesens and Estefan\u00eda Serral Asensio and Jaemin Yoo and Leman Akoglu", "abstract": "  Time series anomaly detection (TSAD) finds many applications such as\nmonitoring environmental sensors, industry KPIs, patient biomarkers, etc. A\ntwo-fold challenge for TSAD is a versatile and unsupervised model that can\ndetect various different types of time series anomalies (spikes,\ndiscontinuities, trend shifts, etc.) without any labeled data. Modern neural\nnetworks have outstanding ability in modeling complex time series.\nSelf-supervised models in particular tackle unsupervised TSAD by transforming\nthe input via various augmentations to create pseudo anomalies for training.\nHowever, their performance is sensitive to the choice of augmentation, which is\nhard to choose in practice, while there exists no effort in the literature on\ndata augmentation tuning for TSAD without labels. Our work aims to fill this\ngap. We introduce TSAP for TSA \"on autoPilot\", which can (self-)tune\naugmentation hyperparameters end-to-end. It stands on two key components: a\ndifferentiable augmentation architecture and an unsupervised validation loss to\neffectively assess the alignment between augmentation type and anomaly type.\nCase studies show TSAP's ability to effectively select the (discrete)\naugmentation type and associated (continuous) hyperparameters. In turn, it\noutperforms established baselines, including SOTA self-supervised models, on\ndiverse TSAD tasks exhibiting different anomaly types.\n", "link": "http://arxiv.org/abs/2404.02865v1", "date": "2024-04-03", "relevancy": 2.587, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5692}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5146}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4684}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20End-To-End%20Self-tuning%20Self-supervised%20Time%20Series%20Anomaly%20Detection&body=Title%3A%20End-To-End%20Self-tuning%20Self-supervised%20Time%20Series%20Anomaly%20Detection%0AAuthor%3A%20Boje%20Deforce%20and%20Meng-Chieh%20Lee%20and%20Bart%20Baesens%20and%20Estefan%C3%ADa%20Serral%20Asensio%20and%20Jaemin%20Yoo%20and%20Leman%20Akoglu%0AAbstract%3A%20%20%20Time%20series%20anomaly%20detection%20%28TSAD%29%20finds%20many%20applications%20such%20as%0Amonitoring%20environmental%20sensors%2C%20industry%20KPIs%2C%20patient%20biomarkers%2C%20etc.%20A%0Atwo-fold%20challenge%20for%20TSAD%20is%20a%20versatile%20and%20unsupervised%20model%20that%20can%0Adetect%20various%20different%20types%20of%20time%20series%20anomalies%20%28spikes%2C%0Adiscontinuities%2C%20trend%20shifts%2C%20etc.%29%20without%20any%20labeled%20data.%20Modern%20neural%0Anetworks%20have%20outstanding%20ability%20in%20modeling%20complex%20time%20series.%0ASelf-supervised%20models%20in%20particular%20tackle%20unsupervised%20TSAD%20by%20transforming%0Athe%20input%20via%20various%20augmentations%20to%20create%20pseudo%20anomalies%20for%20training.%0AHowever%2C%20their%20performance%20is%20sensitive%20to%20the%20choice%20of%20augmentation%2C%20which%20is%0Ahard%20to%20choose%20in%20practice%2C%20while%20there%20exists%20no%20effort%20in%20the%20literature%20on%0Adata%20augmentation%20tuning%20for%20TSAD%20without%20labels.%20Our%20work%20aims%20to%20fill%20this%0Agap.%20We%20introduce%20TSAP%20for%20TSA%20%22on%20autoPilot%22%2C%20which%20can%20%28self-%29tune%0Aaugmentation%20hyperparameters%20end-to-end.%20It%20stands%20on%20two%20key%20components%3A%20a%0Adifferentiable%20augmentation%20architecture%20and%20an%20unsupervised%20validation%20loss%20to%0Aeffectively%20assess%20the%20alignment%20between%20augmentation%20type%20and%20anomaly%20type.%0ACase%20studies%20show%20TSAP%27s%20ability%20to%20effectively%20select%20the%20%28discrete%29%0Aaugmentation%20type%20and%20associated%20%28continuous%29%20hyperparameters.%20In%20turn%2C%20it%0Aoutperforms%20established%20baselines%2C%20including%20SOTA%20self-supervised%20models%2C%20on%0Adiverse%20TSAD%20tasks%20exhibiting%20different%20anomaly%20types.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02865v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=End-To-End%20Self-tuning%20Self-supervised%20Time%20Series%20Anomaly%20Detection&entry.906535625=Boje%20Deforce%20and%20Meng-Chieh%20Lee%20and%20Bart%20Baesens%20and%20Estefan%C3%ADa%20Serral%20Asensio%20and%20Jaemin%20Yoo%20and%20Leman%20Akoglu&entry.1292438233=%20%20Time%20series%20anomaly%20detection%20%28TSAD%29%20finds%20many%20applications%20such%20as%0Amonitoring%20environmental%20sensors%2C%20industry%20KPIs%2C%20patient%20biomarkers%2C%20etc.%20A%0Atwo-fold%20challenge%20for%20TSAD%20is%20a%20versatile%20and%20unsupervised%20model%20that%20can%0Adetect%20various%20different%20types%20of%20time%20series%20anomalies%20%28spikes%2C%0Adiscontinuities%2C%20trend%20shifts%2C%20etc.%29%20without%20any%20labeled%20data.%20Modern%20neural%0Anetworks%20have%20outstanding%20ability%20in%20modeling%20complex%20time%20series.%0ASelf-supervised%20models%20in%20particular%20tackle%20unsupervised%20TSAD%20by%20transforming%0Athe%20input%20via%20various%20augmentations%20to%20create%20pseudo%20anomalies%20for%20training.%0AHowever%2C%20their%20performance%20is%20sensitive%20to%20the%20choice%20of%20augmentation%2C%20which%20is%0Ahard%20to%20choose%20in%20practice%2C%20while%20there%20exists%20no%20effort%20in%20the%20literature%20on%0Adata%20augmentation%20tuning%20for%20TSAD%20without%20labels.%20Our%20work%20aims%20to%20fill%20this%0Agap.%20We%20introduce%20TSAP%20for%20TSA%20%22on%20autoPilot%22%2C%20which%20can%20%28self-%29tune%0Aaugmentation%20hyperparameters%20end-to-end.%20It%20stands%20on%20two%20key%20components%3A%20a%0Adifferentiable%20augmentation%20architecture%20and%20an%20unsupervised%20validation%20loss%20to%0Aeffectively%20assess%20the%20alignment%20between%20augmentation%20type%20and%20anomaly%20type.%0ACase%20studies%20show%20TSAP%27s%20ability%20to%20effectively%20select%20the%20%28discrete%29%0Aaugmentation%20type%20and%20associated%20%28continuous%29%20hyperparameters.%20In%20turn%2C%20it%0Aoutperforms%20established%20baselines%2C%20including%20SOTA%20self-supervised%20models%2C%20on%0Adiverse%20TSAD%20tasks%20exhibiting%20different%20anomaly%20types.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02865v1&entry.124074799=Read"},
{"title": "Model-agnostic Origin Attribution of Generated Images with Few-shot\n  Examples", "author": "Fengyuan Liu and Haochen Luo and Yiming Li and Philip Torr and Jindong Gu", "abstract": "  Recent progress in visual generative models enables the generation of\nhigh-quality images. To prevent the misuse of generated images, it is important\nto identify the origin model that generates them. In this work, we study the\norigin attribution of generated images in a practical setting where only a few\nimages generated by a source model are available and the source model cannot be\naccessed. The goal is to check if a given image is generated by the source\nmodel. We first formulate this problem as a few-shot one-class classification\ntask. To solve the task, we propose OCC-CLIP, a CLIP-based framework for\nfew-shot one-class classification, enabling the identification of an image's\nsource model, even among multiple candidates. Extensive experiments\ncorresponding to various generative models verify the effectiveness of our\nOCC-CLIP framework. Furthermore, an experiment based on the recently released\nDALL-E 3 API verifies the real-world applicability of our solution.\n", "link": "http://arxiv.org/abs/2404.02697v1", "date": "2024-04-03", "relevancy": 2.573, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5259}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5188}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4991}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Model-agnostic%20Origin%20Attribution%20of%20Generated%20Images%20with%20Few-shot%0A%20%20Examples&body=Title%3A%20Model-agnostic%20Origin%20Attribution%20of%20Generated%20Images%20with%20Few-shot%0A%20%20Examples%0AAuthor%3A%20Fengyuan%20Liu%20and%20Haochen%20Luo%20and%20Yiming%20Li%20and%20Philip%20Torr%20and%20Jindong%20Gu%0AAbstract%3A%20%20%20Recent%20progress%20in%20visual%20generative%20models%20enables%20the%20generation%20of%0Ahigh-quality%20images.%20To%20prevent%20the%20misuse%20of%20generated%20images%2C%20it%20is%20important%0Ato%20identify%20the%20origin%20model%20that%20generates%20them.%20In%20this%20work%2C%20we%20study%20the%0Aorigin%20attribution%20of%20generated%20images%20in%20a%20practical%20setting%20where%20only%20a%20few%0Aimages%20generated%20by%20a%20source%20model%20are%20available%20and%20the%20source%20model%20cannot%20be%0Aaccessed.%20The%20goal%20is%20to%20check%20if%20a%20given%20image%20is%20generated%20by%20the%20source%0Amodel.%20We%20first%20formulate%20this%20problem%20as%20a%20few-shot%20one-class%20classification%0Atask.%20To%20solve%20the%20task%2C%20we%20propose%20OCC-CLIP%2C%20a%20CLIP-based%20framework%20for%0Afew-shot%20one-class%20classification%2C%20enabling%20the%20identification%20of%20an%20image%27s%0Asource%20model%2C%20even%20among%20multiple%20candidates.%20Extensive%20experiments%0Acorresponding%20to%20various%20generative%20models%20verify%20the%20effectiveness%20of%20our%0AOCC-CLIP%20framework.%20Furthermore%2C%20an%20experiment%20based%20on%20the%20recently%20released%0ADALL-E%203%20API%20verifies%20the%20real-world%20applicability%20of%20our%20solution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02697v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model-agnostic%20Origin%20Attribution%20of%20Generated%20Images%20with%20Few-shot%0A%20%20Examples&entry.906535625=Fengyuan%20Liu%20and%20Haochen%20Luo%20and%20Yiming%20Li%20and%20Philip%20Torr%20and%20Jindong%20Gu&entry.1292438233=%20%20Recent%20progress%20in%20visual%20generative%20models%20enables%20the%20generation%20of%0Ahigh-quality%20images.%20To%20prevent%20the%20misuse%20of%20generated%20images%2C%20it%20is%20important%0Ato%20identify%20the%20origin%20model%20that%20generates%20them.%20In%20this%20work%2C%20we%20study%20the%0Aorigin%20attribution%20of%20generated%20images%20in%20a%20practical%20setting%20where%20only%20a%20few%0Aimages%20generated%20by%20a%20source%20model%20are%20available%20and%20the%20source%20model%20cannot%20be%0Aaccessed.%20The%20goal%20is%20to%20check%20if%20a%20given%20image%20is%20generated%20by%20the%20source%0Amodel.%20We%20first%20formulate%20this%20problem%20as%20a%20few-shot%20one-class%20classification%0Atask.%20To%20solve%20the%20task%2C%20we%20propose%20OCC-CLIP%2C%20a%20CLIP-based%20framework%20for%0Afew-shot%20one-class%20classification%2C%20enabling%20the%20identification%20of%20an%20image%27s%0Asource%20model%2C%20even%20among%20multiple%20candidates.%20Extensive%20experiments%0Acorresponding%20to%20various%20generative%20models%20verify%20the%20effectiveness%20of%20our%0AOCC-CLIP%20framework.%20Furthermore%2C%20an%20experiment%20based%20on%20the%20recently%20released%0ADALL-E%203%20API%20verifies%20the%20real-world%20applicability%20of%20our%20solution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02697v1&entry.124074799=Read"},
{"title": "Continual Learning of Numerous Tasks from Long-tail Distributions", "author": "Liwei Kang and Wee Sun Lee", "abstract": "  Continual learning, an important aspect of artificial intelligence and\nmachine learning research, focuses on developing models that learn and adapt to\nnew tasks while retaining previously acquired knowledge. Existing continual\nlearning algorithms usually involve a small number of tasks with uniform sizes\nand may not accurately represent real-world learning scenarios. In this paper,\nwe investigate the performance of continual learning algorithms with a large\nnumber of tasks drawn from a task distribution that is long-tail in terms of\ntask sizes. We design one synthetic dataset and two real-world continual\nlearning datasets to evaluate the performance of existing algorithms in such a\nsetting. Moreover, we study an overlooked factor in continual learning, the\noptimizer states, e.g. first and second moments in the Adam optimizer, and\ninvestigate how it can be used to improve continual learning performance. We\npropose a method that reuses the optimizer states in Adam by maintaining a\nweighted average of the second moments from previous tasks. We demonstrate that\nour method, compatible with most existing continual learning algorithms,\neffectively reduces forgetting with only a small amount of additional\ncomputational or memory costs, and provides further improvements on existing\ncontinual learning algorithms, particularly in a long-tail task sequence.\n", "link": "http://arxiv.org/abs/2404.02754v1", "date": "2024-04-03", "relevancy": 2.5684, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.525}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5144}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5017}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Continual%20Learning%20of%20Numerous%20Tasks%20from%20Long-tail%20Distributions&body=Title%3A%20Continual%20Learning%20of%20Numerous%20Tasks%20from%20Long-tail%20Distributions%0AAuthor%3A%20Liwei%20Kang%20and%20Wee%20Sun%20Lee%0AAbstract%3A%20%20%20Continual%20learning%2C%20an%20important%20aspect%20of%20artificial%20intelligence%20and%0Amachine%20learning%20research%2C%20focuses%20on%20developing%20models%20that%20learn%20and%20adapt%20to%0Anew%20tasks%20while%20retaining%20previously%20acquired%20knowledge.%20Existing%20continual%0Alearning%20algorithms%20usually%20involve%20a%20small%20number%20of%20tasks%20with%20uniform%20sizes%0Aand%20may%20not%20accurately%20represent%20real-world%20learning%20scenarios.%20In%20this%20paper%2C%0Awe%20investigate%20the%20performance%20of%20continual%20learning%20algorithms%20with%20a%20large%0Anumber%20of%20tasks%20drawn%20from%20a%20task%20distribution%20that%20is%20long-tail%20in%20terms%20of%0Atask%20sizes.%20We%20design%20one%20synthetic%20dataset%20and%20two%20real-world%20continual%0Alearning%20datasets%20to%20evaluate%20the%20performance%20of%20existing%20algorithms%20in%20such%20a%0Asetting.%20Moreover%2C%20we%20study%20an%20overlooked%20factor%20in%20continual%20learning%2C%20the%0Aoptimizer%20states%2C%20e.g.%20first%20and%20second%20moments%20in%20the%20Adam%20optimizer%2C%20and%0Ainvestigate%20how%20it%20can%20be%20used%20to%20improve%20continual%20learning%20performance.%20We%0Apropose%20a%20method%20that%20reuses%20the%20optimizer%20states%20in%20Adam%20by%20maintaining%20a%0Aweighted%20average%20of%20the%20second%20moments%20from%20previous%20tasks.%20We%20demonstrate%20that%0Aour%20method%2C%20compatible%20with%20most%20existing%20continual%20learning%20algorithms%2C%0Aeffectively%20reduces%20forgetting%20with%20only%20a%20small%20amount%20of%20additional%0Acomputational%20or%20memory%20costs%2C%20and%20provides%20further%20improvements%20on%20existing%0Acontinual%20learning%20algorithms%2C%20particularly%20in%20a%20long-tail%20task%20sequence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02754v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Learning%20of%20Numerous%20Tasks%20from%20Long-tail%20Distributions&entry.906535625=Liwei%20Kang%20and%20Wee%20Sun%20Lee&entry.1292438233=%20%20Continual%20learning%2C%20an%20important%20aspect%20of%20artificial%20intelligence%20and%0Amachine%20learning%20research%2C%20focuses%20on%20developing%20models%20that%20learn%20and%20adapt%20to%0Anew%20tasks%20while%20retaining%20previously%20acquired%20knowledge.%20Existing%20continual%0Alearning%20algorithms%20usually%20involve%20a%20small%20number%20of%20tasks%20with%20uniform%20sizes%0Aand%20may%20not%20accurately%20represent%20real-world%20learning%20scenarios.%20In%20this%20paper%2C%0Awe%20investigate%20the%20performance%20of%20continual%20learning%20algorithms%20with%20a%20large%0Anumber%20of%20tasks%20drawn%20from%20a%20task%20distribution%20that%20is%20long-tail%20in%20terms%20of%0Atask%20sizes.%20We%20design%20one%20synthetic%20dataset%20and%20two%20real-world%20continual%0Alearning%20datasets%20to%20evaluate%20the%20performance%20of%20existing%20algorithms%20in%20such%20a%0Asetting.%20Moreover%2C%20we%20study%20an%20overlooked%20factor%20in%20continual%20learning%2C%20the%0Aoptimizer%20states%2C%20e.g.%20first%20and%20second%20moments%20in%20the%20Adam%20optimizer%2C%20and%0Ainvestigate%20how%20it%20can%20be%20used%20to%20improve%20continual%20learning%20performance.%20We%0Apropose%20a%20method%20that%20reuses%20the%20optimizer%20states%20in%20Adam%20by%20maintaining%20a%0Aweighted%20average%20of%20the%20second%20moments%20from%20previous%20tasks.%20We%20demonstrate%20that%0Aour%20method%2C%20compatible%20with%20most%20existing%20continual%20learning%20algorithms%2C%0Aeffectively%20reduces%20forgetting%20with%20only%20a%20small%20amount%20of%20additional%0Acomputational%20or%20memory%20costs%2C%20and%20provides%20further%20improvements%20on%20existing%0Acontinual%20learning%20algorithms%2C%20particularly%20in%20a%20long-tail%20task%20sequence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02754v1&entry.124074799=Read"},
{"title": "ResNet with Integrated Convolutional Block Attention Module for Ship\n  Classification Using Transfer Learning on Optical Satellite Imagery", "author": "Ryan Donghan Kwon and Gangjoo Robin Nam and Jisoo Tak and Yeom Hyeok and Junseob Shin and Hyerin Cha and Kim Soo Bin", "abstract": "  This study proposes a novel transfer learning framework for effective ship\nclassification using high-resolution optical remote sensing satellite imagery.\nThe framework is based on the deep convolutional neural network model ResNet50\nand incorporates the Convolutional Block Attention Module (CBAM) to enhance\nperformance. CBAM enables the model to attend to salient features in the\nimages, allowing it to better discriminate between subtle differences between\nships and backgrounds. Furthermore, this study adopts a transfer learning\napproach tailored for accurately classifying diverse types of ships by\nfine-tuning a pre-trained model for the specific task. Experimental results\ndemonstrate the efficacy of the proposed framework in ship classification using\noptical remote sensing imagery, achieving a high classification accuracy of 94%\nacross 5 classes, outperforming existing methods. This research holds potential\napplications in maritime surveillance and management, illegal fishing\ndetection, and maritime traffic monitoring.\n", "link": "http://arxiv.org/abs/2404.02135v2", "date": "2024-04-03", "relevancy": 2.5671, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5332}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.512}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4951}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ResNet%20with%20Integrated%20Convolutional%20Block%20Attention%20Module%20for%20Ship%0A%20%20Classification%20Using%20Transfer%20Learning%20on%20Optical%20Satellite%20Imagery&body=Title%3A%20ResNet%20with%20Integrated%20Convolutional%20Block%20Attention%20Module%20for%20Ship%0A%20%20Classification%20Using%20Transfer%20Learning%20on%20Optical%20Satellite%20Imagery%0AAuthor%3A%20Ryan%20Donghan%20Kwon%20and%20Gangjoo%20Robin%20Nam%20and%20Jisoo%20Tak%20and%20Yeom%20Hyeok%20and%20Junseob%20Shin%20and%20Hyerin%20Cha%20and%20Kim%20Soo%20Bin%0AAbstract%3A%20%20%20This%20study%20proposes%20a%20novel%20transfer%20learning%20framework%20for%20effective%20ship%0Aclassification%20using%20high-resolution%20optical%20remote%20sensing%20satellite%20imagery.%0AThe%20framework%20is%20based%20on%20the%20deep%20convolutional%20neural%20network%20model%20ResNet50%0Aand%20incorporates%20the%20Convolutional%20Block%20Attention%20Module%20%28CBAM%29%20to%20enhance%0Aperformance.%20CBAM%20enables%20the%20model%20to%20attend%20to%20salient%20features%20in%20the%0Aimages%2C%20allowing%20it%20to%20better%20discriminate%20between%20subtle%20differences%20between%0Aships%20and%20backgrounds.%20Furthermore%2C%20this%20study%20adopts%20a%20transfer%20learning%0Aapproach%20tailored%20for%20accurately%20classifying%20diverse%20types%20of%20ships%20by%0Afine-tuning%20a%20pre-trained%20model%20for%20the%20specific%20task.%20Experimental%20results%0Ademonstrate%20the%20efficacy%20of%20the%20proposed%20framework%20in%20ship%20classification%20using%0Aoptical%20remote%20sensing%20imagery%2C%20achieving%20a%20high%20classification%20accuracy%20of%2094%25%0Aacross%205%20classes%2C%20outperforming%20existing%20methods.%20This%20research%20holds%20potential%0Aapplications%20in%20maritime%20surveillance%20and%20management%2C%20illegal%20fishing%0Adetection%2C%20and%20maritime%20traffic%20monitoring.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02135v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ResNet%20with%20Integrated%20Convolutional%20Block%20Attention%20Module%20for%20Ship%0A%20%20Classification%20Using%20Transfer%20Learning%20on%20Optical%20Satellite%20Imagery&entry.906535625=Ryan%20Donghan%20Kwon%20and%20Gangjoo%20Robin%20Nam%20and%20Jisoo%20Tak%20and%20Yeom%20Hyeok%20and%20Junseob%20Shin%20and%20Hyerin%20Cha%20and%20Kim%20Soo%20Bin&entry.1292438233=%20%20This%20study%20proposes%20a%20novel%20transfer%20learning%20framework%20for%20effective%20ship%0Aclassification%20using%20high-resolution%20optical%20remote%20sensing%20satellite%20imagery.%0AThe%20framework%20is%20based%20on%20the%20deep%20convolutional%20neural%20network%20model%20ResNet50%0Aand%20incorporates%20the%20Convolutional%20Block%20Attention%20Module%20%28CBAM%29%20to%20enhance%0Aperformance.%20CBAM%20enables%20the%20model%20to%20attend%20to%20salient%20features%20in%20the%0Aimages%2C%20allowing%20it%20to%20better%20discriminate%20between%20subtle%20differences%20between%0Aships%20and%20backgrounds.%20Furthermore%2C%20this%20study%20adopts%20a%20transfer%20learning%0Aapproach%20tailored%20for%20accurately%20classifying%20diverse%20types%20of%20ships%20by%0Afine-tuning%20a%20pre-trained%20model%20for%20the%20specific%20task.%20Experimental%20results%0Ademonstrate%20the%20efficacy%20of%20the%20proposed%20framework%20in%20ship%20classification%20using%0Aoptical%20remote%20sensing%20imagery%2C%20achieving%20a%20high%20classification%20accuracy%20of%2094%25%0Aacross%205%20classes%2C%20outperforming%20existing%20methods.%20This%20research%20holds%20potential%0Aapplications%20in%20maritime%20surveillance%20and%20management%2C%20illegal%20fishing%0Adetection%2C%20and%20maritime%20traffic%20monitoring.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02135v2&entry.124074799=Read"},
{"title": "Non-negative Subspace Feature Representation for Few-shot Learning in\n  Medical Imaging", "author": "Keqiang Fan and Xiaohao Cai and Mahesan Niranjan", "abstract": "  Unlike typical visual scene recognition domains, in which massive datasets\nare accessible to deep neural networks, medical image interpretations are often\nobstructed by the paucity of data. In this paper, we investigate the\neffectiveness of data-based few-shot learning in medical imaging by exploring\ndifferent data attribute representations in a low-dimensional space. We\nintroduce different types of non-negative matrix factorization (NMF) in\nfew-shot learning, addressing the data scarcity issue in medical image\nclassification. Extensive empirical studies are conducted in terms of\nvalidating the effectiveness of NMF, especially its supervised variants (e.g.,\ndiscriminative NMF, and supervised and constrained NMF with sparseness), and\nthe comparison with principal component analysis (PCA), i.e., the collaborative\nrepresentation-based dimensionality reduction technique derived from\neigenvectors. With 14 different datasets covering 11 distinct illness\ncategories, thorough experimental results and comparison with related\ntechniques demonstrate that NMF is a competitive alternative to PCA for\nfew-shot learning in medical imaging, and the supervised NMF algorithms are\nmore discriminative in the subspace with greater effectiveness. Furthermore, we\nshow that the part-based representation of NMF, especially its supervised\nvariants, is dramatically impactful in detecting lesion areas in medical\nimaging with limited samples.\n", "link": "http://arxiv.org/abs/2404.02656v1", "date": "2024-04-03", "relevancy": 2.5323, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.521}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5102}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4882}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Non-negative%20Subspace%20Feature%20Representation%20for%20Few-shot%20Learning%20in%0A%20%20Medical%20Imaging&body=Title%3A%20Non-negative%20Subspace%20Feature%20Representation%20for%20Few-shot%20Learning%20in%0A%20%20Medical%20Imaging%0AAuthor%3A%20Keqiang%20Fan%20and%20Xiaohao%20Cai%20and%20Mahesan%20Niranjan%0AAbstract%3A%20%20%20Unlike%20typical%20visual%20scene%20recognition%20domains%2C%20in%20which%20massive%20datasets%0Aare%20accessible%20to%20deep%20neural%20networks%2C%20medical%20image%20interpretations%20are%20often%0Aobstructed%20by%20the%20paucity%20of%20data.%20In%20this%20paper%2C%20we%20investigate%20the%0Aeffectiveness%20of%20data-based%20few-shot%20learning%20in%20medical%20imaging%20by%20exploring%0Adifferent%20data%20attribute%20representations%20in%20a%20low-dimensional%20space.%20We%0Aintroduce%20different%20types%20of%20non-negative%20matrix%20factorization%20%28NMF%29%20in%0Afew-shot%20learning%2C%20addressing%20the%20data%20scarcity%20issue%20in%20medical%20image%0Aclassification.%20Extensive%20empirical%20studies%20are%20conducted%20in%20terms%20of%0Avalidating%20the%20effectiveness%20of%20NMF%2C%20especially%20its%20supervised%20variants%20%28e.g.%2C%0Adiscriminative%20NMF%2C%20and%20supervised%20and%20constrained%20NMF%20with%20sparseness%29%2C%20and%0Athe%20comparison%20with%20principal%20component%20analysis%20%28PCA%29%2C%20i.e.%2C%20the%20collaborative%0Arepresentation-based%20dimensionality%20reduction%20technique%20derived%20from%0Aeigenvectors.%20With%2014%20different%20datasets%20covering%2011%20distinct%20illness%0Acategories%2C%20thorough%20experimental%20results%20and%20comparison%20with%20related%0Atechniques%20demonstrate%20that%20NMF%20is%20a%20competitive%20alternative%20to%20PCA%20for%0Afew-shot%20learning%20in%20medical%20imaging%2C%20and%20the%20supervised%20NMF%20algorithms%20are%0Amore%20discriminative%20in%20the%20subspace%20with%20greater%20effectiveness.%20Furthermore%2C%20we%0Ashow%20that%20the%20part-based%20representation%20of%20NMF%2C%20especially%20its%20supervised%0Avariants%2C%20is%20dramatically%20impactful%20in%20detecting%20lesion%20areas%20in%20medical%0Aimaging%20with%20limited%20samples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02656v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-negative%20Subspace%20Feature%20Representation%20for%20Few-shot%20Learning%20in%0A%20%20Medical%20Imaging&entry.906535625=Keqiang%20Fan%20and%20Xiaohao%20Cai%20and%20Mahesan%20Niranjan&entry.1292438233=%20%20Unlike%20typical%20visual%20scene%20recognition%20domains%2C%20in%20which%20massive%20datasets%0Aare%20accessible%20to%20deep%20neural%20networks%2C%20medical%20image%20interpretations%20are%20often%0Aobstructed%20by%20the%20paucity%20of%20data.%20In%20this%20paper%2C%20we%20investigate%20the%0Aeffectiveness%20of%20data-based%20few-shot%20learning%20in%20medical%20imaging%20by%20exploring%0Adifferent%20data%20attribute%20representations%20in%20a%20low-dimensional%20space.%20We%0Aintroduce%20different%20types%20of%20non-negative%20matrix%20factorization%20%28NMF%29%20in%0Afew-shot%20learning%2C%20addressing%20the%20data%20scarcity%20issue%20in%20medical%20image%0Aclassification.%20Extensive%20empirical%20studies%20are%20conducted%20in%20terms%20of%0Avalidating%20the%20effectiveness%20of%20NMF%2C%20especially%20its%20supervised%20variants%20%28e.g.%2C%0Adiscriminative%20NMF%2C%20and%20supervised%20and%20constrained%20NMF%20with%20sparseness%29%2C%20and%0Athe%20comparison%20with%20principal%20component%20analysis%20%28PCA%29%2C%20i.e.%2C%20the%20collaborative%0Arepresentation-based%20dimensionality%20reduction%20technique%20derived%20from%0Aeigenvectors.%20With%2014%20different%20datasets%20covering%2011%20distinct%20illness%0Acategories%2C%20thorough%20experimental%20results%20and%20comparison%20with%20related%0Atechniques%20demonstrate%20that%20NMF%20is%20a%20competitive%20alternative%20to%20PCA%20for%0Afew-shot%20learning%20in%20medical%20imaging%2C%20and%20the%20supervised%20NMF%20algorithms%20are%0Amore%20discriminative%20in%20the%20subspace%20with%20greater%20effectiveness.%20Furthermore%2C%20we%0Ashow%20that%20the%20part-based%20representation%20of%20NMF%2C%20especially%20its%20supervised%0Avariants%2C%20is%20dramatically%20impactful%20in%20detecting%20lesion%20areas%20in%20medical%0Aimaging%20with%20limited%20samples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02656v1&entry.124074799=Read"},
{"title": "FlightScope: A Deep Comprehensive Assessment of Aircraft Detection\n  Algorithms in Satellite Imagery", "author": "Safouane El Ghazouali and Arnaud Gucciardi and Nicola Venturi and Michael Rueegsegger and Umberto Michelucci", "abstract": "  Object detection in remotely sensed satellite pictures is fundamental in many\nfields such as biophysical, and environmental monitoring. While deep learning\nalgorithms are constantly evolving, they have been mostly implemented and\ntested on popular ground-based taken photos. This paper critically evaluates\nand compares a suite of advanced object detection algorithms customized for the\ntask of identifying aircraft within satellite imagery. Using the large\nHRPlanesV2 dataset, together with a rigorous validation with the GDIT dataset,\nthis research encompasses an array of methodologies including YOLO versions 5\nand 8, Faster RCNN, CenterNet, RetinaNet, RTMDet, and DETR, all trained from\nscratch. This exhaustive training and validation study reveal YOLOv5 as the\npreeminent model for the specific case of identifying airplanes from remote\nsensing data, showcasing high precision and adaptability across diverse imaging\nconditions. This research highlight the nuanced performance landscapes of these\nalgorithms, with YOLOv5 emerging as a robust solution for aerial object\ndetection, underlining its importance through superior mean average precision,\nRecall, and Intersection over Union scores. The findings described here\nunderscore the fundamental role of algorithm selection aligned with the\nspecific demands of satellite imagery analysis and extend a comprehensive\nframework to evaluate model efficacy. The benchmark toolkit and codes,\navailable via https://github.com/toelt-llc/FlightScope_Bench, aims to further\nexploration and innovation in the realm of remote sensing object detection,\npaving the way for improved analytical methodologies in satellite imagery\napplications.\n", "link": "http://arxiv.org/abs/2404.02877v1", "date": "2024-04-03", "relevancy": 2.5215, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5171}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4982}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4976}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FlightScope%3A%20A%20Deep%20Comprehensive%20Assessment%20of%20Aircraft%20Detection%0A%20%20Algorithms%20in%20Satellite%20Imagery&body=Title%3A%20FlightScope%3A%20A%20Deep%20Comprehensive%20Assessment%20of%20Aircraft%20Detection%0A%20%20Algorithms%20in%20Satellite%20Imagery%0AAuthor%3A%20Safouane%20El%20Ghazouali%20and%20Arnaud%20Gucciardi%20and%20Nicola%20Venturi%20and%20Michael%20Rueegsegger%20and%20Umberto%20Michelucci%0AAbstract%3A%20%20%20Object%20detection%20in%20remotely%20sensed%20satellite%20pictures%20is%20fundamental%20in%20many%0Afields%20such%20as%20biophysical%2C%20and%20environmental%20monitoring.%20While%20deep%20learning%0Aalgorithms%20are%20constantly%20evolving%2C%20they%20have%20been%20mostly%20implemented%20and%0Atested%20on%20popular%20ground-based%20taken%20photos.%20This%20paper%20critically%20evaluates%0Aand%20compares%20a%20suite%20of%20advanced%20object%20detection%20algorithms%20customized%20for%20the%0Atask%20of%20identifying%20aircraft%20within%20satellite%20imagery.%20Using%20the%20large%0AHRPlanesV2%20dataset%2C%20together%20with%20a%20rigorous%20validation%20with%20the%20GDIT%20dataset%2C%0Athis%20research%20encompasses%20an%20array%20of%20methodologies%20including%20YOLO%20versions%205%0Aand%208%2C%20Faster%20RCNN%2C%20CenterNet%2C%20RetinaNet%2C%20RTMDet%2C%20and%20DETR%2C%20all%20trained%20from%0Ascratch.%20This%20exhaustive%20training%20and%20validation%20study%20reveal%20YOLOv5%20as%20the%0Apreeminent%20model%20for%20the%20specific%20case%20of%20identifying%20airplanes%20from%20remote%0Asensing%20data%2C%20showcasing%20high%20precision%20and%20adaptability%20across%20diverse%20imaging%0Aconditions.%20This%20research%20highlight%20the%20nuanced%20performance%20landscapes%20of%20these%0Aalgorithms%2C%20with%20YOLOv5%20emerging%20as%20a%20robust%20solution%20for%20aerial%20object%0Adetection%2C%20underlining%20its%20importance%20through%20superior%20mean%20average%20precision%2C%0ARecall%2C%20and%20Intersection%20over%20Union%20scores.%20The%20findings%20described%20here%0Aunderscore%20the%20fundamental%20role%20of%20algorithm%20selection%20aligned%20with%20the%0Aspecific%20demands%20of%20satellite%20imagery%20analysis%20and%20extend%20a%20comprehensive%0Aframework%20to%20evaluate%20model%20efficacy.%20The%20benchmark%20toolkit%20and%20codes%2C%0Aavailable%20via%20https%3A//github.com/toelt-llc/FlightScope_Bench%2C%20aims%20to%20further%0Aexploration%20and%20innovation%20in%20the%20realm%20of%20remote%20sensing%20object%20detection%2C%0Apaving%20the%20way%20for%20improved%20analytical%20methodologies%20in%20satellite%20imagery%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02877v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlightScope%3A%20A%20Deep%20Comprehensive%20Assessment%20of%20Aircraft%20Detection%0A%20%20Algorithms%20in%20Satellite%20Imagery&entry.906535625=Safouane%20El%20Ghazouali%20and%20Arnaud%20Gucciardi%20and%20Nicola%20Venturi%20and%20Michael%20Rueegsegger%20and%20Umberto%20Michelucci&entry.1292438233=%20%20Object%20detection%20in%20remotely%20sensed%20satellite%20pictures%20is%20fundamental%20in%20many%0Afields%20such%20as%20biophysical%2C%20and%20environmental%20monitoring.%20While%20deep%20learning%0Aalgorithms%20are%20constantly%20evolving%2C%20they%20have%20been%20mostly%20implemented%20and%0Atested%20on%20popular%20ground-based%20taken%20photos.%20This%20paper%20critically%20evaluates%0Aand%20compares%20a%20suite%20of%20advanced%20object%20detection%20algorithms%20customized%20for%20the%0Atask%20of%20identifying%20aircraft%20within%20satellite%20imagery.%20Using%20the%20large%0AHRPlanesV2%20dataset%2C%20together%20with%20a%20rigorous%20validation%20with%20the%20GDIT%20dataset%2C%0Athis%20research%20encompasses%20an%20array%20of%20methodologies%20including%20YOLO%20versions%205%0Aand%208%2C%20Faster%20RCNN%2C%20CenterNet%2C%20RetinaNet%2C%20RTMDet%2C%20and%20DETR%2C%20all%20trained%20from%0Ascratch.%20This%20exhaustive%20training%20and%20validation%20study%20reveal%20YOLOv5%20as%20the%0Apreeminent%20model%20for%20the%20specific%20case%20of%20identifying%20airplanes%20from%20remote%0Asensing%20data%2C%20showcasing%20high%20precision%20and%20adaptability%20across%20diverse%20imaging%0Aconditions.%20This%20research%20highlight%20the%20nuanced%20performance%20landscapes%20of%20these%0Aalgorithms%2C%20with%20YOLOv5%20emerging%20as%20a%20robust%20solution%20for%20aerial%20object%0Adetection%2C%20underlining%20its%20importance%20through%20superior%20mean%20average%20precision%2C%0ARecall%2C%20and%20Intersection%20over%20Union%20scores.%20The%20findings%20described%20here%0Aunderscore%20the%20fundamental%20role%20of%20algorithm%20selection%20aligned%20with%20the%0Aspecific%20demands%20of%20satellite%20imagery%20analysis%20and%20extend%20a%20comprehensive%0Aframework%20to%20evaluate%20model%20efficacy.%20The%20benchmark%20toolkit%20and%20codes%2C%0Aavailable%20via%20https%3A//github.com/toelt-llc/FlightScope_Bench%2C%20aims%20to%20further%0Aexploration%20and%20innovation%20in%20the%20realm%20of%20remote%20sensing%20object%20detection%2C%0Apaving%20the%20way%20for%20improved%20analytical%20methodologies%20in%20satellite%20imagery%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02877v1&entry.124074799=Read"},
{"title": "Global and Local Prompts Cooperation via Optimal Transport for Federated\n  Learning", "author": "Hongxia Li and Wei Huang and Jingya Wang and Ye Shi", "abstract": "  Prompt learning in pretrained visual-language models has shown remarkable\nflexibility across various downstream tasks. Leveraging its inherent\nlightweight nature, recent research attempted to integrate the powerful\npretrained models into federated learning frameworks to simultaneously reduce\ncommunication costs and promote local training on insufficient data. Despite\nthese efforts, current federated prompt learning methods lack specialized\ndesigns to systematically address severe data heterogeneities, e.g., data\ndistribution with both label and feature shifts involved. To address this\nchallenge, we present Federated Prompts Cooperation via Optimal Transport\n(FedOTP), which introduces efficient collaborative prompt learning strategies\nto capture diverse category traits on a per-client basis. Specifically, for\neach client, we learn a global prompt to extract consensus knowledge among\nclients, and a local prompt to capture client-specific category\ncharacteristics. Unbalanced Optimal Transport is then employed to align local\nvisual features with these prompts, striking a balance between global consensus\nand local personalization. By relaxing one of the equality constraints, FedOTP\nenables prompts to focus solely on the core regions of image patches. Extensive\nexperiments on datasets with various types of heterogeneities have demonstrated\nthat our FedOTP outperforms the state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2403.00041v2", "date": "2024-04-03", "relevancy": 2.5103, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5029}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5027}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5006}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Global%20and%20Local%20Prompts%20Cooperation%20via%20Optimal%20Transport%20for%20Federated%0A%20%20Learning&body=Title%3A%20Global%20and%20Local%20Prompts%20Cooperation%20via%20Optimal%20Transport%20for%20Federated%0A%20%20Learning%0AAuthor%3A%20Hongxia%20Li%20and%20Wei%20Huang%20and%20Jingya%20Wang%20and%20Ye%20Shi%0AAbstract%3A%20%20%20Prompt%20learning%20in%20pretrained%20visual-language%20models%20has%20shown%20remarkable%0Aflexibility%20across%20various%20downstream%20tasks.%20Leveraging%20its%20inherent%0Alightweight%20nature%2C%20recent%20research%20attempted%20to%20integrate%20the%20powerful%0Apretrained%20models%20into%20federated%20learning%20frameworks%20to%20simultaneously%20reduce%0Acommunication%20costs%20and%20promote%20local%20training%20on%20insufficient%20data.%20Despite%0Athese%20efforts%2C%20current%20federated%20prompt%20learning%20methods%20lack%20specialized%0Adesigns%20to%20systematically%20address%20severe%20data%20heterogeneities%2C%20e.g.%2C%20data%0Adistribution%20with%20both%20label%20and%20feature%20shifts%20involved.%20To%20address%20this%0Achallenge%2C%20we%20present%20Federated%20Prompts%20Cooperation%20via%20Optimal%20Transport%0A%28FedOTP%29%2C%20which%20introduces%20efficient%20collaborative%20prompt%20learning%20strategies%0Ato%20capture%20diverse%20category%20traits%20on%20a%20per-client%20basis.%20Specifically%2C%20for%0Aeach%20client%2C%20we%20learn%20a%20global%20prompt%20to%20extract%20consensus%20knowledge%20among%0Aclients%2C%20and%20a%20local%20prompt%20to%20capture%20client-specific%20category%0Acharacteristics.%20Unbalanced%20Optimal%20Transport%20is%20then%20employed%20to%20align%20local%0Avisual%20features%20with%20these%20prompts%2C%20striking%20a%20balance%20between%20global%20consensus%0Aand%20local%20personalization.%20By%20relaxing%20one%20of%20the%20equality%20constraints%2C%20FedOTP%0Aenables%20prompts%20to%20focus%20solely%20on%20the%20core%20regions%20of%20image%20patches.%20Extensive%0Aexperiments%20on%20datasets%20with%20various%20types%20of%20heterogeneities%20have%20demonstrated%0Athat%20our%20FedOTP%20outperforms%20the%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.00041v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Global%20and%20Local%20Prompts%20Cooperation%20via%20Optimal%20Transport%20for%20Federated%0A%20%20Learning&entry.906535625=Hongxia%20Li%20and%20Wei%20Huang%20and%20Jingya%20Wang%20and%20Ye%20Shi&entry.1292438233=%20%20Prompt%20learning%20in%20pretrained%20visual-language%20models%20has%20shown%20remarkable%0Aflexibility%20across%20various%20downstream%20tasks.%20Leveraging%20its%20inherent%0Alightweight%20nature%2C%20recent%20research%20attempted%20to%20integrate%20the%20powerful%0Apretrained%20models%20into%20federated%20learning%20frameworks%20to%20simultaneously%20reduce%0Acommunication%20costs%20and%20promote%20local%20training%20on%20insufficient%20data.%20Despite%0Athese%20efforts%2C%20current%20federated%20prompt%20learning%20methods%20lack%20specialized%0Adesigns%20to%20systematically%20address%20severe%20data%20heterogeneities%2C%20e.g.%2C%20data%0Adistribution%20with%20both%20label%20and%20feature%20shifts%20involved.%20To%20address%20this%0Achallenge%2C%20we%20present%20Federated%20Prompts%20Cooperation%20via%20Optimal%20Transport%0A%28FedOTP%29%2C%20which%20introduces%20efficient%20collaborative%20prompt%20learning%20strategies%0Ato%20capture%20diverse%20category%20traits%20on%20a%20per-client%20basis.%20Specifically%2C%20for%0Aeach%20client%2C%20we%20learn%20a%20global%20prompt%20to%20extract%20consensus%20knowledge%20among%0Aclients%2C%20and%20a%20local%20prompt%20to%20capture%20client-specific%20category%0Acharacteristics.%20Unbalanced%20Optimal%20Transport%20is%20then%20employed%20to%20align%20local%0Avisual%20features%20with%20these%20prompts%2C%20striking%20a%20balance%20between%20global%20consensus%0Aand%20local%20personalization.%20By%20relaxing%20one%20of%20the%20equality%20constraints%2C%20FedOTP%0Aenables%20prompts%20to%20focus%20solely%20on%20the%20core%20regions%20of%20image%20patches.%20Extensive%0Aexperiments%20on%20datasets%20with%20various%20types%20of%20heterogeneities%20have%20demonstrated%0Athat%20our%20FedOTP%20outperforms%20the%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00041v2&entry.124074799=Read"},
{"title": "Total Selfie: Generating Full-Body Selfies", "author": "Bowei Chen and Brian Curless and Ira Kemelmacher-Shlizerman and Steven M. Seitz", "abstract": "  We present a method to generate full-body selfies from photographs originally\ntaken at arms length. Because self-captured photos are typically taken close\nup, they have limited field of view and exaggerated perspective that distorts\nfacial shapes. We instead seek to generate the photo some one else would take\nof you from a few feet away. Our approach takes as input four selfies of your\nface and body, a background image, and generates a full-body selfie in a\ndesired target pose. We introduce a novel diffusion-based approach to combine\nall of this information into high-quality, well-composed photos of you with the\ndesired pose and background.\n", "link": "http://arxiv.org/abs/2308.14740v2", "date": "2024-04-03", "relevancy": 2.4982, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5205}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4906}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4879}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Total%20Selfie%3A%20Generating%20Full-Body%20Selfies&body=Title%3A%20Total%20Selfie%3A%20Generating%20Full-Body%20Selfies%0AAuthor%3A%20Bowei%20Chen%20and%20Brian%20Curless%20and%20Ira%20Kemelmacher-Shlizerman%20and%20Steven%20M.%20Seitz%0AAbstract%3A%20%20%20We%20present%20a%20method%20to%20generate%20full-body%20selfies%20from%20photographs%20originally%0Ataken%20at%20arms%20length.%20Because%20self-captured%20photos%20are%20typically%20taken%20close%0Aup%2C%20they%20have%20limited%20field%20of%20view%20and%20exaggerated%20perspective%20that%20distorts%0Afacial%20shapes.%20We%20instead%20seek%20to%20generate%20the%20photo%20some%20one%20else%20would%20take%0Aof%20you%20from%20a%20few%20feet%20away.%20Our%20approach%20takes%20as%20input%20four%20selfies%20of%20your%0Aface%20and%20body%2C%20a%20background%20image%2C%20and%20generates%20a%20full-body%20selfie%20in%20a%0Adesired%20target%20pose.%20We%20introduce%20a%20novel%20diffusion-based%20approach%20to%20combine%0Aall%20of%20this%20information%20into%20high-quality%2C%20well-composed%20photos%20of%20you%20with%20the%0Adesired%20pose%20and%20background.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.14740v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Total%20Selfie%3A%20Generating%20Full-Body%20Selfies&entry.906535625=Bowei%20Chen%20and%20Brian%20Curless%20and%20Ira%20Kemelmacher-Shlizerman%20and%20Steven%20M.%20Seitz&entry.1292438233=%20%20We%20present%20a%20method%20to%20generate%20full-body%20selfies%20from%20photographs%20originally%0Ataken%20at%20arms%20length.%20Because%20self-captured%20photos%20are%20typically%20taken%20close%0Aup%2C%20they%20have%20limited%20field%20of%20view%20and%20exaggerated%20perspective%20that%20distorts%0Afacial%20shapes.%20We%20instead%20seek%20to%20generate%20the%20photo%20some%20one%20else%20would%20take%0Aof%20you%20from%20a%20few%20feet%20away.%20Our%20approach%20takes%20as%20input%20four%20selfies%20of%20your%0Aface%20and%20body%2C%20a%20background%20image%2C%20and%20generates%20a%20full-body%20selfie%20in%20a%0Adesired%20target%20pose.%20We%20introduce%20a%20novel%20diffusion-based%20approach%20to%20combine%0Aall%20of%20this%20information%20into%20high-quality%2C%20well-composed%20photos%20of%20you%20with%20the%0Adesired%20pose%20and%20background.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.14740v2&entry.124074799=Read"},
{"title": "Repurposing Diffusion-Based Image Generators for Monocular Depth\n  Estimation", "author": "Bingxin Ke and Anton Obukhov and Shengyu Huang and Nando Metzger and Rodrigo Caye Daudt and Konrad Schindler", "abstract": "  Monocular depth estimation is a fundamental computer vision task. Recovering\n3D depth from a single image is geometrically ill-posed and requires scene\nunderstanding, so it is not surprising that the rise of deep learning has led\nto a breakthrough. The impressive progress of monocular depth estimators has\nmirrored the growth in model capacity, from relatively modest CNNs to large\nTransformer architectures. Still, monocular depth estimators tend to struggle\nwhen presented with images with unfamiliar content and layout, since their\nknowledge of the visual world is restricted by the data seen during training,\nand challenged by zero-shot generalization to new domains. This motivates us to\nexplore whether the extensive priors captured in recent generative diffusion\nmodels can enable better, more generalizable depth estimation. We introduce\nMarigold, a method for affine-invariant monocular depth estimation that is\nderived from Stable Diffusion and retains its rich prior knowledge. The\nestimator can be fine-tuned in a couple of days on a single GPU using only\nsynthetic training data. It delivers state-of-the-art performance across a wide\nrange of datasets, including over 20% performance gains in specific cases.\nProject page: https://marigoldmonodepth.github.io.\n", "link": "http://arxiv.org/abs/2312.02145v2", "date": "2024-04-03", "relevancy": 2.4577, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6289}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6262}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5969}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Repurposing%20Diffusion-Based%20Image%20Generators%20for%20Monocular%20Depth%0A%20%20Estimation&body=Title%3A%20Repurposing%20Diffusion-Based%20Image%20Generators%20for%20Monocular%20Depth%0A%20%20Estimation%0AAuthor%3A%20Bingxin%20Ke%20and%20Anton%20Obukhov%20and%20Shengyu%20Huang%20and%20Nando%20Metzger%20and%20Rodrigo%20Caye%20Daudt%20and%20Konrad%20Schindler%0AAbstract%3A%20%20%20Monocular%20depth%20estimation%20is%20a%20fundamental%20computer%20vision%20task.%20Recovering%0A3D%20depth%20from%20a%20single%20image%20is%20geometrically%20ill-posed%20and%20requires%20scene%0Aunderstanding%2C%20so%20it%20is%20not%20surprising%20that%20the%20rise%20of%20deep%20learning%20has%20led%0Ato%20a%20breakthrough.%20The%20impressive%20progress%20of%20monocular%20depth%20estimators%20has%0Amirrored%20the%20growth%20in%20model%20capacity%2C%20from%20relatively%20modest%20CNNs%20to%20large%0ATransformer%20architectures.%20Still%2C%20monocular%20depth%20estimators%20tend%20to%20struggle%0Awhen%20presented%20with%20images%20with%20unfamiliar%20content%20and%20layout%2C%20since%20their%0Aknowledge%20of%20the%20visual%20world%20is%20restricted%20by%20the%20data%20seen%20during%20training%2C%0Aand%20challenged%20by%20zero-shot%20generalization%20to%20new%20domains.%20This%20motivates%20us%20to%0Aexplore%20whether%20the%20extensive%20priors%20captured%20in%20recent%20generative%20diffusion%0Amodels%20can%20enable%20better%2C%20more%20generalizable%20depth%20estimation.%20We%20introduce%0AMarigold%2C%20a%20method%20for%20affine-invariant%20monocular%20depth%20estimation%20that%20is%0Aderived%20from%20Stable%20Diffusion%20and%20retains%20its%20rich%20prior%20knowledge.%20The%0Aestimator%20can%20be%20fine-tuned%20in%20a%20couple%20of%20days%20on%20a%20single%20GPU%20using%20only%0Asynthetic%20training%20data.%20It%20delivers%20state-of-the-art%20performance%20across%20a%20wide%0Arange%20of%20datasets%2C%20including%20over%2020%25%20performance%20gains%20in%20specific%20cases.%0AProject%20page%3A%20https%3A//marigoldmonodepth.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02145v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Repurposing%20Diffusion-Based%20Image%20Generators%20for%20Monocular%20Depth%0A%20%20Estimation&entry.906535625=Bingxin%20Ke%20and%20Anton%20Obukhov%20and%20Shengyu%20Huang%20and%20Nando%20Metzger%20and%20Rodrigo%20Caye%20Daudt%20and%20Konrad%20Schindler&entry.1292438233=%20%20Monocular%20depth%20estimation%20is%20a%20fundamental%20computer%20vision%20task.%20Recovering%0A3D%20depth%20from%20a%20single%20image%20is%20geometrically%20ill-posed%20and%20requires%20scene%0Aunderstanding%2C%20so%20it%20is%20not%20surprising%20that%20the%20rise%20of%20deep%20learning%20has%20led%0Ato%20a%20breakthrough.%20The%20impressive%20progress%20of%20monocular%20depth%20estimators%20has%0Amirrored%20the%20growth%20in%20model%20capacity%2C%20from%20relatively%20modest%20CNNs%20to%20large%0ATransformer%20architectures.%20Still%2C%20monocular%20depth%20estimators%20tend%20to%20struggle%0Awhen%20presented%20with%20images%20with%20unfamiliar%20content%20and%20layout%2C%20since%20their%0Aknowledge%20of%20the%20visual%20world%20is%20restricted%20by%20the%20data%20seen%20during%20training%2C%0Aand%20challenged%20by%20zero-shot%20generalization%20to%20new%20domains.%20This%20motivates%20us%20to%0Aexplore%20whether%20the%20extensive%20priors%20captured%20in%20recent%20generative%20diffusion%0Amodels%20can%20enable%20better%2C%20more%20generalizable%20depth%20estimation.%20We%20introduce%0AMarigold%2C%20a%20method%20for%20affine-invariant%20monocular%20depth%20estimation%20that%20is%0Aderived%20from%20Stable%20Diffusion%20and%20retains%20its%20rich%20prior%20knowledge.%20The%0Aestimator%20can%20be%20fine-tuned%20in%20a%20couple%20of%20days%20on%20a%20single%20GPU%20using%20only%0Asynthetic%20training%20data.%20It%20delivers%20state-of-the-art%20performance%20across%20a%20wide%0Arange%20of%20datasets%2C%20including%20over%2020%25%20performance%20gains%20in%20specific%20cases.%0AProject%20page%3A%20https%3A//marigoldmonodepth.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02145v2&entry.124074799=Read"},
{"title": "Learnable Weight Initialization for Volumetric Medical Image\n  Segmentation", "author": "Shahina Kunhimon and Abdelrahman Shaker and Muzammal Naseer and Salman Khan and Fahad Shahbaz Khan", "abstract": "  Hybrid volumetric medical image segmentation models, combining the advantages\nof local convolution and global attention, have recently received considerable\nattention. While mainly focusing on architectural modifications, most existing\nhybrid approaches still use conventional data-independent weight initialization\nschemes which restrict their performance due to ignoring the inherent\nvolumetric nature of the medical data. To address this issue, we propose a\nlearnable weight initialization approach that utilizes the available medical\ntraining data to effectively learn the contextual and structural cues via the\nproposed self-supervised objectives. Our approach is easy to integrate into any\nhybrid model and requires no external training data. Experiments on multi-organ\nand lung cancer segmentation tasks demonstrate the effectiveness of our\napproach, leading to state-of-the-art segmentation performance. Our proposed\ndata-dependent initialization approach performs favorably as compared to the\nSwin-UNETR model pretrained using large-scale datasets on multi-organ\nsegmentation task. Our source code and models are available at:\nhttps://github.com/ShahinaKK/LWI-VMS.\n", "link": "http://arxiv.org/abs/2306.09320v4", "date": "2024-04-03", "relevancy": 2.3928, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.495}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4706}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4701}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learnable%20Weight%20Initialization%20for%20Volumetric%20Medical%20Image%0A%20%20Segmentation&body=Title%3A%20Learnable%20Weight%20Initialization%20for%20Volumetric%20Medical%20Image%0A%20%20Segmentation%0AAuthor%3A%20Shahina%20Kunhimon%20and%20Abdelrahman%20Shaker%20and%20Muzammal%20Naseer%20and%20Salman%20Khan%20and%20Fahad%20Shahbaz%20Khan%0AAbstract%3A%20%20%20Hybrid%20volumetric%20medical%20image%20segmentation%20models%2C%20combining%20the%20advantages%0Aof%20local%20convolution%20and%20global%20attention%2C%20have%20recently%20received%20considerable%0Aattention.%20While%20mainly%20focusing%20on%20architectural%20modifications%2C%20most%20existing%0Ahybrid%20approaches%20still%20use%20conventional%20data-independent%20weight%20initialization%0Aschemes%20which%20restrict%20their%20performance%20due%20to%20ignoring%20the%20inherent%0Avolumetric%20nature%20of%20the%20medical%20data.%20To%20address%20this%20issue%2C%20we%20propose%20a%0Alearnable%20weight%20initialization%20approach%20that%20utilizes%20the%20available%20medical%0Atraining%20data%20to%20effectively%20learn%20the%20contextual%20and%20structural%20cues%20via%20the%0Aproposed%20self-supervised%20objectives.%20Our%20approach%20is%20easy%20to%20integrate%20into%20any%0Ahybrid%20model%20and%20requires%20no%20external%20training%20data.%20Experiments%20on%20multi-organ%0Aand%20lung%20cancer%20segmentation%20tasks%20demonstrate%20the%20effectiveness%20of%20our%0Aapproach%2C%20leading%20to%20state-of-the-art%20segmentation%20performance.%20Our%20proposed%0Adata-dependent%20initialization%20approach%20performs%20favorably%20as%20compared%20to%20the%0ASwin-UNETR%20model%20pretrained%20using%20large-scale%20datasets%20on%20multi-organ%0Asegmentation%20task.%20Our%20source%20code%20and%20models%20are%20available%20at%3A%0Ahttps%3A//github.com/ShahinaKK/LWI-VMS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.09320v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learnable%20Weight%20Initialization%20for%20Volumetric%20Medical%20Image%0A%20%20Segmentation&entry.906535625=Shahina%20Kunhimon%20and%20Abdelrahman%20Shaker%20and%20Muzammal%20Naseer%20and%20Salman%20Khan%20and%20Fahad%20Shahbaz%20Khan&entry.1292438233=%20%20Hybrid%20volumetric%20medical%20image%20segmentation%20models%2C%20combining%20the%20advantages%0Aof%20local%20convolution%20and%20global%20attention%2C%20have%20recently%20received%20considerable%0Aattention.%20While%20mainly%20focusing%20on%20architectural%20modifications%2C%20most%20existing%0Ahybrid%20approaches%20still%20use%20conventional%20data-independent%20weight%20initialization%0Aschemes%20which%20restrict%20their%20performance%20due%20to%20ignoring%20the%20inherent%0Avolumetric%20nature%20of%20the%20medical%20data.%20To%20address%20this%20issue%2C%20we%20propose%20a%0Alearnable%20weight%20initialization%20approach%20that%20utilizes%20the%20available%20medical%0Atraining%20data%20to%20effectively%20learn%20the%20contextual%20and%20structural%20cues%20via%20the%0Aproposed%20self-supervised%20objectives.%20Our%20approach%20is%20easy%20to%20integrate%20into%20any%0Ahybrid%20model%20and%20requires%20no%20external%20training%20data.%20Experiments%20on%20multi-organ%0Aand%20lung%20cancer%20segmentation%20tasks%20demonstrate%20the%20effectiveness%20of%20our%0Aapproach%2C%20leading%20to%20state-of-the-art%20segmentation%20performance.%20Our%20proposed%0Adata-dependent%20initialization%20approach%20performs%20favorably%20as%20compared%20to%20the%0ASwin-UNETR%20model%20pretrained%20using%20large-scale%20datasets%20on%20multi-organ%0Asegmentation%20task.%20Our%20source%20code%20and%20models%20are%20available%20at%3A%0Ahttps%3A//github.com/ShahinaKK/LWI-VMS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.09320v4&entry.124074799=Read"},
{"title": "G3DR: Generative 3D Reconstruction in ImageNet", "author": "Pradyumna Reddy and Ismail Elezi and Jiankang Deng", "abstract": "  We introduce a novel 3D generative method, Generative 3D Reconstruction\n(G3DR) in ImageNet, capable of generating diverse and high-quality 3D objects\nfrom single images, addressing the limitations of existing methods. At the\nheart of our framework is a novel depth regularization technique that enables\nthe generation of scenes with high-geometric fidelity. G3DR also leverages a\npretrained language-vision model, such as CLIP, to enable reconstruction in\nnovel views and improve the visual realism of generations. Additionally, G3DR\ndesigns a simple but effective sampling procedure to further improve the\nquality of generations. G3DR offers diverse and efficient 3D asset generation\nbased on class or text conditioning. Despite its simplicity, G3DR is able to\nbeat state-of-theart methods, improving over them by up to 22% in perceptual\nmetrics and 90% in geometry scores, while needing only half of the training\ntime. Code is available at https://github.com/preddy5/G3DR\n", "link": "http://arxiv.org/abs/2403.00939v3", "date": "2024-04-03", "relevancy": 2.3741, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6026}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5943}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5691}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20G3DR%3A%20Generative%203D%20Reconstruction%20in%20ImageNet&body=Title%3A%20G3DR%3A%20Generative%203D%20Reconstruction%20in%20ImageNet%0AAuthor%3A%20Pradyumna%20Reddy%20and%20Ismail%20Elezi%20and%20Jiankang%20Deng%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%203D%20generative%20method%2C%20Generative%203D%20Reconstruction%0A%28G3DR%29%20in%20ImageNet%2C%20capable%20of%20generating%20diverse%20and%20high-quality%203D%20objects%0Afrom%20single%20images%2C%20addressing%20the%20limitations%20of%20existing%20methods.%20At%20the%0Aheart%20of%20our%20framework%20is%20a%20novel%20depth%20regularization%20technique%20that%20enables%0Athe%20generation%20of%20scenes%20with%20high-geometric%20fidelity.%20G3DR%20also%20leverages%20a%0Apretrained%20language-vision%20model%2C%20such%20as%20CLIP%2C%20to%20enable%20reconstruction%20in%0Anovel%20views%20and%20improve%20the%20visual%20realism%20of%20generations.%20Additionally%2C%20G3DR%0Adesigns%20a%20simple%20but%20effective%20sampling%20procedure%20to%20further%20improve%20the%0Aquality%20of%20generations.%20G3DR%20offers%20diverse%20and%20efficient%203D%20asset%20generation%0Abased%20on%20class%20or%20text%20conditioning.%20Despite%20its%20simplicity%2C%20G3DR%20is%20able%20to%0Abeat%20state-of-theart%20methods%2C%20improving%20over%20them%20by%20up%20to%2022%25%20in%20perceptual%0Ametrics%20and%2090%25%20in%20geometry%20scores%2C%20while%20needing%20only%20half%20of%20the%20training%0Atime.%20Code%20is%20available%20at%20https%3A//github.com/preddy5/G3DR%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.00939v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=G3DR%3A%20Generative%203D%20Reconstruction%20in%20ImageNet&entry.906535625=Pradyumna%20Reddy%20and%20Ismail%20Elezi%20and%20Jiankang%20Deng&entry.1292438233=%20%20We%20introduce%20a%20novel%203D%20generative%20method%2C%20Generative%203D%20Reconstruction%0A%28G3DR%29%20in%20ImageNet%2C%20capable%20of%20generating%20diverse%20and%20high-quality%203D%20objects%0Afrom%20single%20images%2C%20addressing%20the%20limitations%20of%20existing%20methods.%20At%20the%0Aheart%20of%20our%20framework%20is%20a%20novel%20depth%20regularization%20technique%20that%20enables%0Athe%20generation%20of%20scenes%20with%20high-geometric%20fidelity.%20G3DR%20also%20leverages%20a%0Apretrained%20language-vision%20model%2C%20such%20as%20CLIP%2C%20to%20enable%20reconstruction%20in%0Anovel%20views%20and%20improve%20the%20visual%20realism%20of%20generations.%20Additionally%2C%20G3DR%0Adesigns%20a%20simple%20but%20effective%20sampling%20procedure%20to%20further%20improve%20the%0Aquality%20of%20generations.%20G3DR%20offers%20diverse%20and%20efficient%203D%20asset%20generation%0Abased%20on%20class%20or%20text%20conditioning.%20Despite%20its%20simplicity%2C%20G3DR%20is%20able%20to%0Abeat%20state-of-theart%20methods%2C%20improving%20over%20them%20by%20up%20to%2022%25%20in%20perceptual%0Ametrics%20and%2090%25%20in%20geometry%20scores%2C%20while%20needing%20only%20half%20of%20the%20training%0Atime.%20Code%20is%20available%20at%20https%3A//github.com/preddy5/G3DR%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00939v3&entry.124074799=Read"},
{"title": "Think While You Write: Hypothesis Verification Promotes Faithful\n  Knowledge-to-Text Generation", "author": "Yifu Qiu and Varun Embar and Shay B. Cohen and Benjamin Han", "abstract": "  Knowledge-to-text generators often struggle to faithfully generate\ndescriptions for the input facts: they may produce hallucinations that\ncontradict the input, or describe facts not present in the input. To reduce\nhallucinations, we propose a decoding-only method, TWEAK (Think While\nEffectively Articulating Knowledge), which can be integrated with any generator\nwithout retraining. TWEAK treats the generated sequences at each decoding step\nand its future sequences as hypotheses, and ranks each generation candidate\nbased on the extent to which their hypotheses are supported by the input facts\nusing a Hypothesis Verification Model (HVM). We first demonstrate the\neffectiveness of TWEAK by using a Natural Language Inference (NLI) model as the\nHVM and report improved faithfulness with a minimal impact on the quality. We\nthen replace the NLI model with a task-specific HVM trained with a\nfirst-of-a-kind dataset, FATE (Fact-Aligned Textual Entailment), which pairs\ninput facts with their original and perturbed descriptions. We test TWEAK with\ntwo generators, and the best TWEAK variants improve on average for the two\nmodels by 2.24/7.17 points in faithfulness (FactKB) in in/out-of-distribution\nevaluations, respectively, and with only a 0.14/0.32-point decline in quality\n(BERTScore).\n", "link": "http://arxiv.org/abs/2311.09467v2", "date": "2024-04-03", "relevancy": 2.3591, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4774}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.472}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4661}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Think%20While%20You%20Write%3A%20Hypothesis%20Verification%20Promotes%20Faithful%0A%20%20Knowledge-to-Text%20Generation&body=Title%3A%20Think%20While%20You%20Write%3A%20Hypothesis%20Verification%20Promotes%20Faithful%0A%20%20Knowledge-to-Text%20Generation%0AAuthor%3A%20Yifu%20Qiu%20and%20Varun%20Embar%20and%20Shay%20B.%20Cohen%20and%20Benjamin%20Han%0AAbstract%3A%20%20%20Knowledge-to-text%20generators%20often%20struggle%20to%20faithfully%20generate%0Adescriptions%20for%20the%20input%20facts%3A%20they%20may%20produce%20hallucinations%20that%0Acontradict%20the%20input%2C%20or%20describe%20facts%20not%20present%20in%20the%20input.%20To%20reduce%0Ahallucinations%2C%20we%20propose%20a%20decoding-only%20method%2C%20TWEAK%20%28Think%20While%0AEffectively%20Articulating%20Knowledge%29%2C%20which%20can%20be%20integrated%20with%20any%20generator%0Awithout%20retraining.%20TWEAK%20treats%20the%20generated%20sequences%20at%20each%20decoding%20step%0Aand%20its%20future%20sequences%20as%20hypotheses%2C%20and%20ranks%20each%20generation%20candidate%0Abased%20on%20the%20extent%20to%20which%20their%20hypotheses%20are%20supported%20by%20the%20input%20facts%0Ausing%20a%20Hypothesis%20Verification%20Model%20%28HVM%29.%20We%20first%20demonstrate%20the%0Aeffectiveness%20of%20TWEAK%20by%20using%20a%20Natural%20Language%20Inference%20%28NLI%29%20model%20as%20the%0AHVM%20and%20report%20improved%20faithfulness%20with%20a%20minimal%20impact%20on%20the%20quality.%20We%0Athen%20replace%20the%20NLI%20model%20with%20a%20task-specific%20HVM%20trained%20with%20a%0Afirst-of-a-kind%20dataset%2C%20FATE%20%28Fact-Aligned%20Textual%20Entailment%29%2C%20which%20pairs%0Ainput%20facts%20with%20their%20original%20and%20perturbed%20descriptions.%20We%20test%20TWEAK%20with%0Atwo%20generators%2C%20and%20the%20best%20TWEAK%20variants%20improve%20on%20average%20for%20the%20two%0Amodels%20by%202.24/7.17%20points%20in%20faithfulness%20%28FactKB%29%20in%20in/out-of-distribution%0Aevaluations%2C%20respectively%2C%20and%20with%20only%20a%200.14/0.32-point%20decline%20in%20quality%0A%28BERTScore%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.09467v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Think%20While%20You%20Write%3A%20Hypothesis%20Verification%20Promotes%20Faithful%0A%20%20Knowledge-to-Text%20Generation&entry.906535625=Yifu%20Qiu%20and%20Varun%20Embar%20and%20Shay%20B.%20Cohen%20and%20Benjamin%20Han&entry.1292438233=%20%20Knowledge-to-text%20generators%20often%20struggle%20to%20faithfully%20generate%0Adescriptions%20for%20the%20input%20facts%3A%20they%20may%20produce%20hallucinations%20that%0Acontradict%20the%20input%2C%20or%20describe%20facts%20not%20present%20in%20the%20input.%20To%20reduce%0Ahallucinations%2C%20we%20propose%20a%20decoding-only%20method%2C%20TWEAK%20%28Think%20While%0AEffectively%20Articulating%20Knowledge%29%2C%20which%20can%20be%20integrated%20with%20any%20generator%0Awithout%20retraining.%20TWEAK%20treats%20the%20generated%20sequences%20at%20each%20decoding%20step%0Aand%20its%20future%20sequences%20as%20hypotheses%2C%20and%20ranks%20each%20generation%20candidate%0Abased%20on%20the%20extent%20to%20which%20their%20hypotheses%20are%20supported%20by%20the%20input%20facts%0Ausing%20a%20Hypothesis%20Verification%20Model%20%28HVM%29.%20We%20first%20demonstrate%20the%0Aeffectiveness%20of%20TWEAK%20by%20using%20a%20Natural%20Language%20Inference%20%28NLI%29%20model%20as%20the%0AHVM%20and%20report%20improved%20faithfulness%20with%20a%20minimal%20impact%20on%20the%20quality.%20We%0Athen%20replace%20the%20NLI%20model%20with%20a%20task-specific%20HVM%20trained%20with%20a%0Afirst-of-a-kind%20dataset%2C%20FATE%20%28Fact-Aligned%20Textual%20Entailment%29%2C%20which%20pairs%0Ainput%20facts%20with%20their%20original%20and%20perturbed%20descriptions.%20We%20test%20TWEAK%20with%0Atwo%20generators%2C%20and%20the%20best%20TWEAK%20variants%20improve%20on%20average%20for%20the%20two%0Amodels%20by%202.24/7.17%20points%20in%20faithfulness%20%28FactKB%29%20in%20in/out-of-distribution%0Aevaluations%2C%20respectively%2C%20and%20with%20only%20a%200.14/0.32-point%20decline%20in%20quality%0A%28BERTScore%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.09467v2&entry.124074799=Read"},
{"title": "A Satellite Band Selection Framework for Amazon Forest Deforestation\n  Detection Task", "author": "Eduardo Neto and Fabio A. Faria and Amanda A. S. de Oliveira and \u00c1lvaro L. Fazenda", "abstract": "  The conservation of tropical forests is a topic of significant social and\necological relevance due to their crucial role in the global ecosystem.\nUnfortunately, deforestation and degradation impact millions of hectares\nannually, necessitating government or private initiatives for effective forest\nmonitoring. This study introduces a novel framework that employs the Univariate\nMarginal Distribution Algorithm (UMDA) to select spectral bands from Landsat-8\nsatellite, optimizing the representation of deforested areas. This selection\nguides a semantic segmentation architecture, DeepLabv3+, enhancing its\nperformance. Experimental results revealed several band compositions that\nachieved superior balanced accuracy compared to commonly adopted combinations\nfor deforestation detection, utilizing segment classification via a Support\nVector Machine (SVM). Moreover, the optimal band compositions identified by the\nUMDA-based approach improved the performance of the DeepLabv3+ architecture,\nsurpassing state-of-the-art approaches compared in this study. The observation\nthat a few selected bands outperform the total contradicts the data-driven\nparadigm prevalent in the deep learning field. Therefore, this suggests an\nexception to the conventional wisdom that 'more is always better'.\n", "link": "http://arxiv.org/abs/2404.02659v1", "date": "2024-04-03", "relevancy": 2.3373, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4727}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4649}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4647}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Satellite%20Band%20Selection%20Framework%20for%20Amazon%20Forest%20Deforestation%0A%20%20Detection%20Task&body=Title%3A%20A%20Satellite%20Band%20Selection%20Framework%20for%20Amazon%20Forest%20Deforestation%0A%20%20Detection%20Task%0AAuthor%3A%20Eduardo%20Neto%20and%20Fabio%20A.%20Faria%20and%20Amanda%20A.%20S.%20de%20Oliveira%20and%20%C3%81lvaro%20L.%20Fazenda%0AAbstract%3A%20%20%20The%20conservation%20of%20tropical%20forests%20is%20a%20topic%20of%20significant%20social%20and%0Aecological%20relevance%20due%20to%20their%20crucial%20role%20in%20the%20global%20ecosystem.%0AUnfortunately%2C%20deforestation%20and%20degradation%20impact%20millions%20of%20hectares%0Aannually%2C%20necessitating%20government%20or%20private%20initiatives%20for%20effective%20forest%0Amonitoring.%20This%20study%20introduces%20a%20novel%20framework%20that%20employs%20the%20Univariate%0AMarginal%20Distribution%20Algorithm%20%28UMDA%29%20to%20select%20spectral%20bands%20from%20Landsat-8%0Asatellite%2C%20optimizing%20the%20representation%20of%20deforested%20areas.%20This%20selection%0Aguides%20a%20semantic%20segmentation%20architecture%2C%20DeepLabv3%2B%2C%20enhancing%20its%0Aperformance.%20Experimental%20results%20revealed%20several%20band%20compositions%20that%0Aachieved%20superior%20balanced%20accuracy%20compared%20to%20commonly%20adopted%20combinations%0Afor%20deforestation%20detection%2C%20utilizing%20segment%20classification%20via%20a%20Support%0AVector%20Machine%20%28SVM%29.%20Moreover%2C%20the%20optimal%20band%20compositions%20identified%20by%20the%0AUMDA-based%20approach%20improved%20the%20performance%20of%20the%20DeepLabv3%2B%20architecture%2C%0Asurpassing%20state-of-the-art%20approaches%20compared%20in%20this%20study.%20The%20observation%0Athat%20a%20few%20selected%20bands%20outperform%20the%20total%20contradicts%20the%20data-driven%0Aparadigm%20prevalent%20in%20the%20deep%20learning%20field.%20Therefore%2C%20this%20suggests%20an%0Aexception%20to%20the%20conventional%20wisdom%20that%20%27more%20is%20always%20better%27.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02659v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Satellite%20Band%20Selection%20Framework%20for%20Amazon%20Forest%20Deforestation%0A%20%20Detection%20Task&entry.906535625=Eduardo%20Neto%20and%20Fabio%20A.%20Faria%20and%20Amanda%20A.%20S.%20de%20Oliveira%20and%20%C3%81lvaro%20L.%20Fazenda&entry.1292438233=%20%20The%20conservation%20of%20tropical%20forests%20is%20a%20topic%20of%20significant%20social%20and%0Aecological%20relevance%20due%20to%20their%20crucial%20role%20in%20the%20global%20ecosystem.%0AUnfortunately%2C%20deforestation%20and%20degradation%20impact%20millions%20of%20hectares%0Aannually%2C%20necessitating%20government%20or%20private%20initiatives%20for%20effective%20forest%0Amonitoring.%20This%20study%20introduces%20a%20novel%20framework%20that%20employs%20the%20Univariate%0AMarginal%20Distribution%20Algorithm%20%28UMDA%29%20to%20select%20spectral%20bands%20from%20Landsat-8%0Asatellite%2C%20optimizing%20the%20representation%20of%20deforested%20areas.%20This%20selection%0Aguides%20a%20semantic%20segmentation%20architecture%2C%20DeepLabv3%2B%2C%20enhancing%20its%0Aperformance.%20Experimental%20results%20revealed%20several%20band%20compositions%20that%0Aachieved%20superior%20balanced%20accuracy%20compared%20to%20commonly%20adopted%20combinations%0Afor%20deforestation%20detection%2C%20utilizing%20segment%20classification%20via%20a%20Support%0AVector%20Machine%20%28SVM%29.%20Moreover%2C%20the%20optimal%20band%20compositions%20identified%20by%20the%0AUMDA-based%20approach%20improved%20the%20performance%20of%20the%20DeepLabv3%2B%20architecture%2C%0Asurpassing%20state-of-the-art%20approaches%20compared%20in%20this%20study.%20The%20observation%0Athat%20a%20few%20selected%20bands%20outperform%20the%20total%20contradicts%20the%20data-driven%0Aparadigm%20prevalent%20in%20the%20deep%20learning%20field.%20Therefore%2C%20this%20suggests%20an%0Aexception%20to%20the%20conventional%20wisdom%20that%20%27more%20is%20always%20better%27.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02659v1&entry.124074799=Read"},
{"title": "Independently Keypoint Learning for Small Object Semantic Correspondence", "author": "Hailong Jin and Huiying Li", "abstract": "  Semantic correspondence remains a challenging task for establishing\ncorrespondences between a pair of images with the same category or similar\nscenes due to the large intra-class appearance. In this paper, we introduce a\nnovel problem called 'Small Object Semantic Correspondence (SOSC).' This\nproblem is challenging due to the close proximity of keypoints associated with\nsmall objects, which results in the fusion of these respective features. It is\ndifficult to identify the corresponding key points of the fused features, and\nit is also difficult to be recognized. To address this challenge, we propose\nthe Keypoint Bounding box-centered Cropping (KBC) method, which aims to\nincrease the spatial separation between keypoints of small objects, thereby\nfacilitating independent learning of these keypoints. The KBC method is\nseamlessly integrated into our proposed inference pipeline and can be easily\nincorporated into other methodologies, resulting in significant performance\nenhancements. Additionally, we introduce a novel framework, named KBCNet, which\nserves as our baseline model. KBCNet comprises a Cross-Scale Feature Alignment\n(CSFA) module and an efficient 4D convolutional decoder. The CSFA module is\ndesigned to align multi-scale features, enriching keypoint representations by\nintegrating fine-grained features and deep semantic features. Meanwhile, the 4D\nconvolutional decoder, based on efficient 4D convolution, ensures efficiency\nand rapid convergence. To empirically validate the effectiveness of our\nproposed methodology, extensive experiments are conducted on three widely used\nbenchmarks: PF-PASCAL, PF-WILLOW, and SPair-71k. Our KBC method demonstrates a\nsubstantial performance improvement of 7.5\\% on the SPair-71K dataset,\nproviding compelling evidence of its efficacy.\n", "link": "http://arxiv.org/abs/2404.02678v1", "date": "2024-04-03", "relevancy": 2.294, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6044}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5527}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5482}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Independently%20Keypoint%20Learning%20for%20Small%20Object%20Semantic%20Correspondence&body=Title%3A%20Independently%20Keypoint%20Learning%20for%20Small%20Object%20Semantic%20Correspondence%0AAuthor%3A%20Hailong%20Jin%20and%20Huiying%20Li%0AAbstract%3A%20%20%20Semantic%20correspondence%20remains%20a%20challenging%20task%20for%20establishing%0Acorrespondences%20between%20a%20pair%20of%20images%20with%20the%20same%20category%20or%20similar%0Ascenes%20due%20to%20the%20large%20intra-class%20appearance.%20In%20this%20paper%2C%20we%20introduce%20a%0Anovel%20problem%20called%20%27Small%20Object%20Semantic%20Correspondence%20%28SOSC%29.%27%20This%0Aproblem%20is%20challenging%20due%20to%20the%20close%20proximity%20of%20keypoints%20associated%20with%0Asmall%20objects%2C%20which%20results%20in%20the%20fusion%20of%20these%20respective%20features.%20It%20is%0Adifficult%20to%20identify%20the%20corresponding%20key%20points%20of%20the%20fused%20features%2C%20and%0Ait%20is%20also%20difficult%20to%20be%20recognized.%20To%20address%20this%20challenge%2C%20we%20propose%0Athe%20Keypoint%20Bounding%20box-centered%20Cropping%20%28KBC%29%20method%2C%20which%20aims%20to%0Aincrease%20the%20spatial%20separation%20between%20keypoints%20of%20small%20objects%2C%20thereby%0Afacilitating%20independent%20learning%20of%20these%20keypoints.%20The%20KBC%20method%20is%0Aseamlessly%20integrated%20into%20our%20proposed%20inference%20pipeline%20and%20can%20be%20easily%0Aincorporated%20into%20other%20methodologies%2C%20resulting%20in%20significant%20performance%0Aenhancements.%20Additionally%2C%20we%20introduce%20a%20novel%20framework%2C%20named%20KBCNet%2C%20which%0Aserves%20as%20our%20baseline%20model.%20KBCNet%20comprises%20a%20Cross-Scale%20Feature%20Alignment%0A%28CSFA%29%20module%20and%20an%20efficient%204D%20convolutional%20decoder.%20The%20CSFA%20module%20is%0Adesigned%20to%20align%20multi-scale%20features%2C%20enriching%20keypoint%20representations%20by%0Aintegrating%20fine-grained%20features%20and%20deep%20semantic%20features.%20Meanwhile%2C%20the%204D%0Aconvolutional%20decoder%2C%20based%20on%20efficient%204D%20convolution%2C%20ensures%20efficiency%0Aand%20rapid%20convergence.%20To%20empirically%20validate%20the%20effectiveness%20of%20our%0Aproposed%20methodology%2C%20extensive%20experiments%20are%20conducted%20on%20three%20widely%20used%0Abenchmarks%3A%20PF-PASCAL%2C%20PF-WILLOW%2C%20and%20SPair-71k.%20Our%20KBC%20method%20demonstrates%20a%0Asubstantial%20performance%20improvement%20of%207.5%5C%25%20on%20the%20SPair-71K%20dataset%2C%0Aproviding%20compelling%20evidence%20of%20its%20efficacy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02678v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Independently%20Keypoint%20Learning%20for%20Small%20Object%20Semantic%20Correspondence&entry.906535625=Hailong%20Jin%20and%20Huiying%20Li&entry.1292438233=%20%20Semantic%20correspondence%20remains%20a%20challenging%20task%20for%20establishing%0Acorrespondences%20between%20a%20pair%20of%20images%20with%20the%20same%20category%20or%20similar%0Ascenes%20due%20to%20the%20large%20intra-class%20appearance.%20In%20this%20paper%2C%20we%20introduce%20a%0Anovel%20problem%20called%20%27Small%20Object%20Semantic%20Correspondence%20%28SOSC%29.%27%20This%0Aproblem%20is%20challenging%20due%20to%20the%20close%20proximity%20of%20keypoints%20associated%20with%0Asmall%20objects%2C%20which%20results%20in%20the%20fusion%20of%20these%20respective%20features.%20It%20is%0Adifficult%20to%20identify%20the%20corresponding%20key%20points%20of%20the%20fused%20features%2C%20and%0Ait%20is%20also%20difficult%20to%20be%20recognized.%20To%20address%20this%20challenge%2C%20we%20propose%0Athe%20Keypoint%20Bounding%20box-centered%20Cropping%20%28KBC%29%20method%2C%20which%20aims%20to%0Aincrease%20the%20spatial%20separation%20between%20keypoints%20of%20small%20objects%2C%20thereby%0Afacilitating%20independent%20learning%20of%20these%20keypoints.%20The%20KBC%20method%20is%0Aseamlessly%20integrated%20into%20our%20proposed%20inference%20pipeline%20and%20can%20be%20easily%0Aincorporated%20into%20other%20methodologies%2C%20resulting%20in%20significant%20performance%0Aenhancements.%20Additionally%2C%20we%20introduce%20a%20novel%20framework%2C%20named%20KBCNet%2C%20which%0Aserves%20as%20our%20baseline%20model.%20KBCNet%20comprises%20a%20Cross-Scale%20Feature%20Alignment%0A%28CSFA%29%20module%20and%20an%20efficient%204D%20convolutional%20decoder.%20The%20CSFA%20module%20is%0Adesigned%20to%20align%20multi-scale%20features%2C%20enriching%20keypoint%20representations%20by%0Aintegrating%20fine-grained%20features%20and%20deep%20semantic%20features.%20Meanwhile%2C%20the%204D%0Aconvolutional%20decoder%2C%20based%20on%20efficient%204D%20convolution%2C%20ensures%20efficiency%0Aand%20rapid%20convergence.%20To%20empirically%20validate%20the%20effectiveness%20of%20our%0Aproposed%20methodology%2C%20extensive%20experiments%20are%20conducted%20on%20three%20widely%20used%0Abenchmarks%3A%20PF-PASCAL%2C%20PF-WILLOW%2C%20and%20SPair-71k.%20Our%20KBC%20method%20demonstrates%20a%0Asubstantial%20performance%20improvement%20of%207.5%5C%25%20on%20the%20SPair-71K%20dataset%2C%0Aproviding%20compelling%20evidence%20of%20its%20efficacy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02678v1&entry.124074799=Read"},
{"title": "Semi-supervised Active Learning for Video Action Detection", "author": "Ayush Singh and Aayush J Rana and Akash Kumar and Shruti Vyas and Yogesh Singh Rawat", "abstract": "  In this work, we focus on label efficient learning for video action\ndetection. We develop a novel semi-supervised active learning approach which\nutilizes both labeled as well as unlabeled data along with informative sample\nselection for action detection. Video action detection requires spatio-temporal\nlocalization along with classification, which poses several challenges for both\nactive learning informative sample selection as well as semi-supervised\nlearning pseudo label generation. First, we propose NoiseAug, a simple\naugmentation strategy which effectively selects informative samples for video\naction detection. Next, we propose fft-attention, a novel technique based on\nhigh-pass filtering which enables effective utilization of pseudo label for SSL\nin video action detection by emphasizing on relevant activity region within a\nvideo. We evaluate the proposed approach on three different benchmark datasets,\nUCF-101-24, JHMDB-21, and Youtube-VOS. First, we demonstrate its effectiveness\non video action detection where the proposed approach outperforms prior works\nin semi-supervised and weakly-supervised learning along with several baseline\napproaches in both UCF101-24 and JHMDB-21. Next, we also show its effectiveness\non Youtube-VOS for video object segmentation demonstrating its generalization\ncapability for other dense prediction tasks in videos. The code and models is\npublicly available at:\n\\url{https://github.com/AKASH2907/semi-sup-active-learning}.\n", "link": "http://arxiv.org/abs/2312.07169v3", "date": "2024-04-03", "relevancy": 2.2887, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6388}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5382}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5192}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Semi-supervised%20Active%20Learning%20for%20Video%20Action%20Detection&body=Title%3A%20Semi-supervised%20Active%20Learning%20for%20Video%20Action%20Detection%0AAuthor%3A%20Ayush%20Singh%20and%20Aayush%20J%20Rana%20and%20Akash%20Kumar%20and%20Shruti%20Vyas%20and%20Yogesh%20Singh%20Rawat%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20focus%20on%20label%20efficient%20learning%20for%20video%20action%0Adetection.%20We%20develop%20a%20novel%20semi-supervised%20active%20learning%20approach%20which%0Autilizes%20both%20labeled%20as%20well%20as%20unlabeled%20data%20along%20with%20informative%20sample%0Aselection%20for%20action%20detection.%20Video%20action%20detection%20requires%20spatio-temporal%0Alocalization%20along%20with%20classification%2C%20which%20poses%20several%20challenges%20for%20both%0Aactive%20learning%20informative%20sample%20selection%20as%20well%20as%20semi-supervised%0Alearning%20pseudo%20label%20generation.%20First%2C%20we%20propose%20NoiseAug%2C%20a%20simple%0Aaugmentation%20strategy%20which%20effectively%20selects%20informative%20samples%20for%20video%0Aaction%20detection.%20Next%2C%20we%20propose%20fft-attention%2C%20a%20novel%20technique%20based%20on%0Ahigh-pass%20filtering%20which%20enables%20effective%20utilization%20of%20pseudo%20label%20for%20SSL%0Ain%20video%20action%20detection%20by%20emphasizing%20on%20relevant%20activity%20region%20within%20a%0Avideo.%20We%20evaluate%20the%20proposed%20approach%20on%20three%20different%20benchmark%20datasets%2C%0AUCF-101-24%2C%20JHMDB-21%2C%20and%20Youtube-VOS.%20First%2C%20we%20demonstrate%20its%20effectiveness%0Aon%20video%20action%20detection%20where%20the%20proposed%20approach%20outperforms%20prior%20works%0Ain%20semi-supervised%20and%20weakly-supervised%20learning%20along%20with%20several%20baseline%0Aapproaches%20in%20both%20UCF101-24%20and%20JHMDB-21.%20Next%2C%20we%20also%20show%20its%20effectiveness%0Aon%20Youtube-VOS%20for%20video%20object%20segmentation%20demonstrating%20its%20generalization%0Acapability%20for%20other%20dense%20prediction%20tasks%20in%20videos.%20The%20code%20and%20models%20is%0Apublicly%20available%20at%3A%0A%5Curl%7Bhttps%3A//github.com/AKASH2907/semi-sup-active-learning%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07169v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semi-supervised%20Active%20Learning%20for%20Video%20Action%20Detection&entry.906535625=Ayush%20Singh%20and%20Aayush%20J%20Rana%20and%20Akash%20Kumar%20and%20Shruti%20Vyas%20and%20Yogesh%20Singh%20Rawat&entry.1292438233=%20%20In%20this%20work%2C%20we%20focus%20on%20label%20efficient%20learning%20for%20video%20action%0Adetection.%20We%20develop%20a%20novel%20semi-supervised%20active%20learning%20approach%20which%0Autilizes%20both%20labeled%20as%20well%20as%20unlabeled%20data%20along%20with%20informative%20sample%0Aselection%20for%20action%20detection.%20Video%20action%20detection%20requires%20spatio-temporal%0Alocalization%20along%20with%20classification%2C%20which%20poses%20several%20challenges%20for%20both%0Aactive%20learning%20informative%20sample%20selection%20as%20well%20as%20semi-supervised%0Alearning%20pseudo%20label%20generation.%20First%2C%20we%20propose%20NoiseAug%2C%20a%20simple%0Aaugmentation%20strategy%20which%20effectively%20selects%20informative%20samples%20for%20video%0Aaction%20detection.%20Next%2C%20we%20propose%20fft-attention%2C%20a%20novel%20technique%20based%20on%0Ahigh-pass%20filtering%20which%20enables%20effective%20utilization%20of%20pseudo%20label%20for%20SSL%0Ain%20video%20action%20detection%20by%20emphasizing%20on%20relevant%20activity%20region%20within%20a%0Avideo.%20We%20evaluate%20the%20proposed%20approach%20on%20three%20different%20benchmark%20datasets%2C%0AUCF-101-24%2C%20JHMDB-21%2C%20and%20Youtube-VOS.%20First%2C%20we%20demonstrate%20its%20effectiveness%0Aon%20video%20action%20detection%20where%20the%20proposed%20approach%20outperforms%20prior%20works%0Ain%20semi-supervised%20and%20weakly-supervised%20learning%20along%20with%20several%20baseline%0Aapproaches%20in%20both%20UCF101-24%20and%20JHMDB-21.%20Next%2C%20we%20also%20show%20its%20effectiveness%0Aon%20Youtube-VOS%20for%20video%20object%20segmentation%20demonstrating%20its%20generalization%0Acapability%20for%20other%20dense%20prediction%20tasks%20in%20videos.%20The%20code%20and%20models%20is%0Apublicly%20available%20at%3A%0A%5Curl%7Bhttps%3A//github.com/AKASH2907/semi-sup-active-learning%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07169v3&entry.124074799=Read"},
{"title": "LeanVec: Searching vectors faster by making them fit", "author": "Mariano Tepper and Ishwar Singh Bhati and Cecilia Aguerrebere and Mark Hildebrand and Ted Willke", "abstract": "  Modern deep learning models have the ability to generate high-dimensional\nvectors whose similarity reflects semantic resemblance. Thus, similarity\nsearch, i.e., the operation of retrieving those vectors in a large collection\nthat are similar to a given query, has become a critical component of a wide\nrange of applications that demand highly accurate and timely answers. In this\nsetting, the high vector dimensionality puts similarity search systems under\ncompute and memory pressure, leading to subpar performance. Additionally,\ncross-modal retrieval tasks have become increasingly common, e.g., where a user\ninputs a text query to find the most relevant images for that query. However,\nthese queries often have different distributions than the database embeddings,\nmaking it challenging to achieve high accuracy. In this work, we present\nLeanVec, a framework that combines linear dimensionality reduction with vector\nquantization to accelerate similarity search on high-dimensional vectors while\nmaintaining accuracy. We present LeanVec variants for in-distribution (ID) and\nout-of-distribution (OOD) queries. LeanVec-ID yields accuracies on par with\nthose from recently introduced deep learning alternatives whose computational\noverhead precludes their usage in practice. LeanVec-OOD uses two novel\ntechniques for dimensionality reduction that consider the query and database\ndistributions to simultaneously boost the accuracy and the performance of the\nframework even further (even presenting competitive results when the query and\ndatabase distributions match). All in all, our extensive and varied\nexperimental results show that LeanVec produces state-of-the-art results, with\nup to 3.7x improvement in search throughput and up to 4.9x faster index build\ntime over the state of the art.\n", "link": "http://arxiv.org/abs/2312.16335v2", "date": "2024-04-03", "relevancy": 2.2858, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4601}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4567}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4547}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LeanVec%3A%20Searching%20vectors%20faster%20by%20making%20them%20fit&body=Title%3A%20LeanVec%3A%20Searching%20vectors%20faster%20by%20making%20them%20fit%0AAuthor%3A%20Mariano%20Tepper%20and%20Ishwar%20Singh%20Bhati%20and%20Cecilia%20Aguerrebere%20and%20Mark%20Hildebrand%20and%20Ted%20Willke%0AAbstract%3A%20%20%20Modern%20deep%20learning%20models%20have%20the%20ability%20to%20generate%20high-dimensional%0Avectors%20whose%20similarity%20reflects%20semantic%20resemblance.%20Thus%2C%20similarity%0Asearch%2C%20i.e.%2C%20the%20operation%20of%20retrieving%20those%20vectors%20in%20a%20large%20collection%0Athat%20are%20similar%20to%20a%20given%20query%2C%20has%20become%20a%20critical%20component%20of%20a%20wide%0Arange%20of%20applications%20that%20demand%20highly%20accurate%20and%20timely%20answers.%20In%20this%0Asetting%2C%20the%20high%20vector%20dimensionality%20puts%20similarity%20search%20systems%20under%0Acompute%20and%20memory%20pressure%2C%20leading%20to%20subpar%20performance.%20Additionally%2C%0Across-modal%20retrieval%20tasks%20have%20become%20increasingly%20common%2C%20e.g.%2C%20where%20a%20user%0Ainputs%20a%20text%20query%20to%20find%20the%20most%20relevant%20images%20for%20that%20query.%20However%2C%0Athese%20queries%20often%20have%20different%20distributions%20than%20the%20database%20embeddings%2C%0Amaking%20it%20challenging%20to%20achieve%20high%20accuracy.%20In%20this%20work%2C%20we%20present%0ALeanVec%2C%20a%20framework%20that%20combines%20linear%20dimensionality%20reduction%20with%20vector%0Aquantization%20to%20accelerate%20similarity%20search%20on%20high-dimensional%20vectors%20while%0Amaintaining%20accuracy.%20We%20present%20LeanVec%20variants%20for%20in-distribution%20%28ID%29%20and%0Aout-of-distribution%20%28OOD%29%20queries.%20LeanVec-ID%20yields%20accuracies%20on%20par%20with%0Athose%20from%20recently%20introduced%20deep%20learning%20alternatives%20whose%20computational%0Aoverhead%20precludes%20their%20usage%20in%20practice.%20LeanVec-OOD%20uses%20two%20novel%0Atechniques%20for%20dimensionality%20reduction%20that%20consider%20the%20query%20and%20database%0Adistributions%20to%20simultaneously%20boost%20the%20accuracy%20and%20the%20performance%20of%20the%0Aframework%20even%20further%20%28even%20presenting%20competitive%20results%20when%20the%20query%20and%0Adatabase%20distributions%20match%29.%20All%20in%20all%2C%20our%20extensive%20and%20varied%0Aexperimental%20results%20show%20that%20LeanVec%20produces%20state-of-the-art%20results%2C%20with%0Aup%20to%203.7x%20improvement%20in%20search%20throughput%20and%20up%20to%204.9x%20faster%20index%20build%0Atime%20over%20the%20state%20of%20the%20art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.16335v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LeanVec%3A%20Searching%20vectors%20faster%20by%20making%20them%20fit&entry.906535625=Mariano%20Tepper%20and%20Ishwar%20Singh%20Bhati%20and%20Cecilia%20Aguerrebere%20and%20Mark%20Hildebrand%20and%20Ted%20Willke&entry.1292438233=%20%20Modern%20deep%20learning%20models%20have%20the%20ability%20to%20generate%20high-dimensional%0Avectors%20whose%20similarity%20reflects%20semantic%20resemblance.%20Thus%2C%20similarity%0Asearch%2C%20i.e.%2C%20the%20operation%20of%20retrieving%20those%20vectors%20in%20a%20large%20collection%0Athat%20are%20similar%20to%20a%20given%20query%2C%20has%20become%20a%20critical%20component%20of%20a%20wide%0Arange%20of%20applications%20that%20demand%20highly%20accurate%20and%20timely%20answers.%20In%20this%0Asetting%2C%20the%20high%20vector%20dimensionality%20puts%20similarity%20search%20systems%20under%0Acompute%20and%20memory%20pressure%2C%20leading%20to%20subpar%20performance.%20Additionally%2C%0Across-modal%20retrieval%20tasks%20have%20become%20increasingly%20common%2C%20e.g.%2C%20where%20a%20user%0Ainputs%20a%20text%20query%20to%20find%20the%20most%20relevant%20images%20for%20that%20query.%20However%2C%0Athese%20queries%20often%20have%20different%20distributions%20than%20the%20database%20embeddings%2C%0Amaking%20it%20challenging%20to%20achieve%20high%20accuracy.%20In%20this%20work%2C%20we%20present%0ALeanVec%2C%20a%20framework%20that%20combines%20linear%20dimensionality%20reduction%20with%20vector%0Aquantization%20to%20accelerate%20similarity%20search%20on%20high-dimensional%20vectors%20while%0Amaintaining%20accuracy.%20We%20present%20LeanVec%20variants%20for%20in-distribution%20%28ID%29%20and%0Aout-of-distribution%20%28OOD%29%20queries.%20LeanVec-ID%20yields%20accuracies%20on%20par%20with%0Athose%20from%20recently%20introduced%20deep%20learning%20alternatives%20whose%20computational%0Aoverhead%20precludes%20their%20usage%20in%20practice.%20LeanVec-OOD%20uses%20two%20novel%0Atechniques%20for%20dimensionality%20reduction%20that%20consider%20the%20query%20and%20database%0Adistributions%20to%20simultaneously%20boost%20the%20accuracy%20and%20the%20performance%20of%20the%0Aframework%20even%20further%20%28even%20presenting%20competitive%20results%20when%20the%20query%20and%0Adatabase%20distributions%20match%29.%20All%20in%20all%2C%20our%20extensive%20and%20varied%0Aexperimental%20results%20show%20that%20LeanVec%20produces%20state-of-the-art%20results%2C%20with%0Aup%20to%203.7x%20improvement%20in%20search%20throughput%20and%20up%20to%204.9x%20faster%20index%20build%0Atime%20over%20the%20state%20of%20the%20art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.16335v2&entry.124074799=Read"},
{"title": "Trust Your $\\nabla$: Gradient-based Intervention Targeting for Causal\n  Discovery", "author": "Mateusz Olko and Micha\u0142 Zaj\u0105c and Aleksandra Nowak and Nino Scherrer and Yashas Annadani and Stefan Bauer and \u0141ukasz Kuci\u0144ski and Piotr Mi\u0142o\u015b", "abstract": "  Inferring causal structure from data is a challenging task of fundamental\nimportance in science. Observational data are often insufficient to identify a\nsystem's causal structure uniquely. While conducting interventions (i.e.,\nexperiments) can improve the identifiability, such samples are usually\nchallenging and expensive to obtain. Hence, experimental design approaches for\ncausal discovery aim to minimize the number of interventions by estimating the\nmost informative intervention target. In this work, we propose a novel\nGradient-based Intervention Targeting method, abbreviated GIT, that 'trusts'\nthe gradient estimator of a gradient-based causal discovery framework to\nprovide signals for the intervention acquisition function. We provide extensive\nexperiments in simulated and real-world datasets and demonstrate that GIT\nperforms on par with competitive baselines, surpassing them in the low-data\nregime.\n", "link": "http://arxiv.org/abs/2211.13715v5", "date": "2024-04-03", "relevancy": 2.2802, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4636}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4588}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4458}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Trust%20Your%20%24%5Cnabla%24%3A%20Gradient-based%20Intervention%20Targeting%20for%20Causal%0A%20%20Discovery&body=Title%3A%20Trust%20Your%20%24%5Cnabla%24%3A%20Gradient-based%20Intervention%20Targeting%20for%20Causal%0A%20%20Discovery%0AAuthor%3A%20Mateusz%20Olko%20and%20Micha%C5%82%20Zaj%C4%85c%20and%20Aleksandra%20Nowak%20and%20Nino%20Scherrer%20and%20Yashas%20Annadani%20and%20Stefan%20Bauer%20and%20%C5%81ukasz%20Kuci%C5%84ski%20and%20Piotr%20Mi%C5%82o%C5%9B%0AAbstract%3A%20%20%20Inferring%20causal%20structure%20from%20data%20is%20a%20challenging%20task%20of%20fundamental%0Aimportance%20in%20science.%20Observational%20data%20are%20often%20insufficient%20to%20identify%20a%0Asystem%27s%20causal%20structure%20uniquely.%20While%20conducting%20interventions%20%28i.e.%2C%0Aexperiments%29%20can%20improve%20the%20identifiability%2C%20such%20samples%20are%20usually%0Achallenging%20and%20expensive%20to%20obtain.%20Hence%2C%20experimental%20design%20approaches%20for%0Acausal%20discovery%20aim%20to%20minimize%20the%20number%20of%20interventions%20by%20estimating%20the%0Amost%20informative%20intervention%20target.%20In%20this%20work%2C%20we%20propose%20a%20novel%0AGradient-based%20Intervention%20Targeting%20method%2C%20abbreviated%20GIT%2C%20that%20%27trusts%27%0Athe%20gradient%20estimator%20of%20a%20gradient-based%20causal%20discovery%20framework%20to%0Aprovide%20signals%20for%20the%20intervention%20acquisition%20function.%20We%20provide%20extensive%0Aexperiments%20in%20simulated%20and%20real-world%20datasets%20and%20demonstrate%20that%20GIT%0Aperforms%20on%20par%20with%20competitive%20baselines%2C%20surpassing%20them%20in%20the%20low-data%0Aregime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.13715v5", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trust%20Your%20%24%5Cnabla%24%3A%20Gradient-based%20Intervention%20Targeting%20for%20Causal%0A%20%20Discovery&entry.906535625=Mateusz%20Olko%20and%20Micha%C5%82%20Zaj%C4%85c%20and%20Aleksandra%20Nowak%20and%20Nino%20Scherrer%20and%20Yashas%20Annadani%20and%20Stefan%20Bauer%20and%20%C5%81ukasz%20Kuci%C5%84ski%20and%20Piotr%20Mi%C5%82o%C5%9B&entry.1292438233=%20%20Inferring%20causal%20structure%20from%20data%20is%20a%20challenging%20task%20of%20fundamental%0Aimportance%20in%20science.%20Observational%20data%20are%20often%20insufficient%20to%20identify%20a%0Asystem%27s%20causal%20structure%20uniquely.%20While%20conducting%20interventions%20%28i.e.%2C%0Aexperiments%29%20can%20improve%20the%20identifiability%2C%20such%20samples%20are%20usually%0Achallenging%20and%20expensive%20to%20obtain.%20Hence%2C%20experimental%20design%20approaches%20for%0Acausal%20discovery%20aim%20to%20minimize%20the%20number%20of%20interventions%20by%20estimating%20the%0Amost%20informative%20intervention%20target.%20In%20this%20work%2C%20we%20propose%20a%20novel%0AGradient-based%20Intervention%20Targeting%20method%2C%20abbreviated%20GIT%2C%20that%20%27trusts%27%0Athe%20gradient%20estimator%20of%20a%20gradient-based%20causal%20discovery%20framework%20to%0Aprovide%20signals%20for%20the%20intervention%20acquisition%20function.%20We%20provide%20extensive%0Aexperiments%20in%20simulated%20and%20real-world%20datasets%20and%20demonstrate%20that%20GIT%0Aperforms%20on%20par%20with%20competitive%20baselines%2C%20surpassing%20them%20in%20the%20low-data%0Aregime.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.13715v5&entry.124074799=Read"},
{"title": "Three Heads Are Better Than One: Complementary Experts for Long-Tailed\n  Semi-supervised Learning", "author": "Chengcheng Ma and Ismail Elezi and Jiankang Deng and Weiming Dong and Changsheng Xu", "abstract": "  We address the challenging problem of Long-Tailed Semi-Supervised Learning\n(LTSSL) where labeled data exhibit imbalanced class distribution and unlabeled\ndata follow an unknown distribution. Unlike in balanced SSL, the generated\npseudo-labels are skewed towards head classes, intensifying the training bias.\nSuch a phenomenon is even amplified as more unlabeled data will be mislabeled\nas head classes when the class distribution of labeled and unlabeled datasets\nare mismatched. To solve this problem, we propose a novel method named\nComPlementary Experts (CPE). Specifically, we train multiple experts to model\nvarious class distributions, each of them yielding high-quality pseudo-labels\nwithin one form of class distribution. Besides, we introduce Classwise Batch\nNormalization for CPE to avoid performance degradation caused by feature\ndistribution mismatch between head and non-head classes. CPE achieves\nstate-of-the-art performances on CIFAR-10-LT, CIFAR-100-LT, and STL-10-LT\ndataset benchmarks. For instance, on CIFAR-10-LT, CPE improves test accuracy by\nover 2.22% compared to baselines. Code is available at\nhttps://github.com/machengcheng2016/CPE-LTSSL.\n", "link": "http://arxiv.org/abs/2312.15702v2", "date": "2024-04-03", "relevancy": 2.2458, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5897}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5487}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5226}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Three%20Heads%20Are%20Better%20Than%20One%3A%20Complementary%20Experts%20for%20Long-Tailed%0A%20%20Semi-supervised%20Learning&body=Title%3A%20Three%20Heads%20Are%20Better%20Than%20One%3A%20Complementary%20Experts%20for%20Long-Tailed%0A%20%20Semi-supervised%20Learning%0AAuthor%3A%20Chengcheng%20Ma%20and%20Ismail%20Elezi%20and%20Jiankang%20Deng%20and%20Weiming%20Dong%20and%20Changsheng%20Xu%0AAbstract%3A%20%20%20We%20address%20the%20challenging%20problem%20of%20Long-Tailed%20Semi-Supervised%20Learning%0A%28LTSSL%29%20where%20labeled%20data%20exhibit%20imbalanced%20class%20distribution%20and%20unlabeled%0Adata%20follow%20an%20unknown%20distribution.%20Unlike%20in%20balanced%20SSL%2C%20the%20generated%0Apseudo-labels%20are%20skewed%20towards%20head%20classes%2C%20intensifying%20the%20training%20bias.%0ASuch%20a%20phenomenon%20is%20even%20amplified%20as%20more%20unlabeled%20data%20will%20be%20mislabeled%0Aas%20head%20classes%20when%20the%20class%20distribution%20of%20labeled%20and%20unlabeled%20datasets%0Aare%20mismatched.%20To%20solve%20this%20problem%2C%20we%20propose%20a%20novel%20method%20named%0AComPlementary%20Experts%20%28CPE%29.%20Specifically%2C%20we%20train%20multiple%20experts%20to%20model%0Avarious%20class%20distributions%2C%20each%20of%20them%20yielding%20high-quality%20pseudo-labels%0Awithin%20one%20form%20of%20class%20distribution.%20Besides%2C%20we%20introduce%20Classwise%20Batch%0ANormalization%20for%20CPE%20to%20avoid%20performance%20degradation%20caused%20by%20feature%0Adistribution%20mismatch%20between%20head%20and%20non-head%20classes.%20CPE%20achieves%0Astate-of-the-art%20performances%20on%20CIFAR-10-LT%2C%20CIFAR-100-LT%2C%20and%20STL-10-LT%0Adataset%20benchmarks.%20For%20instance%2C%20on%20CIFAR-10-LT%2C%20CPE%20improves%20test%20accuracy%20by%0Aover%202.22%25%20compared%20to%20baselines.%20Code%20is%20available%20at%0Ahttps%3A//github.com/machengcheng2016/CPE-LTSSL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.15702v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Three%20Heads%20Are%20Better%20Than%20One%3A%20Complementary%20Experts%20for%20Long-Tailed%0A%20%20Semi-supervised%20Learning&entry.906535625=Chengcheng%20Ma%20and%20Ismail%20Elezi%20and%20Jiankang%20Deng%20and%20Weiming%20Dong%20and%20Changsheng%20Xu&entry.1292438233=%20%20We%20address%20the%20challenging%20problem%20of%20Long-Tailed%20Semi-Supervised%20Learning%0A%28LTSSL%29%20where%20labeled%20data%20exhibit%20imbalanced%20class%20distribution%20and%20unlabeled%0Adata%20follow%20an%20unknown%20distribution.%20Unlike%20in%20balanced%20SSL%2C%20the%20generated%0Apseudo-labels%20are%20skewed%20towards%20head%20classes%2C%20intensifying%20the%20training%20bias.%0ASuch%20a%20phenomenon%20is%20even%20amplified%20as%20more%20unlabeled%20data%20will%20be%20mislabeled%0Aas%20head%20classes%20when%20the%20class%20distribution%20of%20labeled%20and%20unlabeled%20datasets%0Aare%20mismatched.%20To%20solve%20this%20problem%2C%20we%20propose%20a%20novel%20method%20named%0AComPlementary%20Experts%20%28CPE%29.%20Specifically%2C%20we%20train%20multiple%20experts%20to%20model%0Avarious%20class%20distributions%2C%20each%20of%20them%20yielding%20high-quality%20pseudo-labels%0Awithin%20one%20form%20of%20class%20distribution.%20Besides%2C%20we%20introduce%20Classwise%20Batch%0ANormalization%20for%20CPE%20to%20avoid%20performance%20degradation%20caused%20by%20feature%0Adistribution%20mismatch%20between%20head%20and%20non-head%20classes.%20CPE%20achieves%0Astate-of-the-art%20performances%20on%20CIFAR-10-LT%2C%20CIFAR-100-LT%2C%20and%20STL-10-LT%0Adataset%20benchmarks.%20For%20instance%2C%20on%20CIFAR-10-LT%2C%20CPE%20improves%20test%20accuracy%20by%0Aover%202.22%25%20compared%20to%20baselines.%20Code%20is%20available%20at%0Ahttps%3A//github.com/machengcheng2016/CPE-LTSSL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.15702v2&entry.124074799=Read"},
{"title": "DeiT-LT Distillation Strikes Back for Vision Transformer Training on\n  Long-Tailed Datasets", "author": "Harsh Rangwani and Pradipto Mondal and Mayank Mishra and Ashish Ramayee Asokan and R. Venkatesh Babu", "abstract": "  Vision Transformer (ViT) has emerged as a prominent architecture for various\ncomputer vision tasks. In ViT, we divide the input image into patch tokens and\nprocess them through a stack of self attention blocks. However, unlike\nConvolutional Neural Networks (CNN), ViTs simple architecture has no\ninformative inductive bias (e.g., locality,etc. ). Due to this, ViT requires a\nlarge amount of data for pre-training. Various data efficient approaches (DeiT)\nhave been proposed to train ViT on balanced datasets effectively. However,\nlimited literature discusses the use of ViT for datasets with long-tailed\nimbalances. In this work, we introduce DeiT-LT to tackle the problem of\ntraining ViTs from scratch on long-tailed datasets. In DeiT-LT, we introduce an\nefficient and effective way of distillation from CNN via distillation DIST\ntoken by using out-of-distribution images and re-weighting the distillation\nloss to enhance focus on tail classes. This leads to the learning of local\nCNN-like features in early ViT blocks, improving generalization for tail\nclasses. Further, to mitigate overfitting, we propose distilling from a flat\nCNN teacher, which leads to learning low-rank generalizable features for DIST\ntokens across all ViT blocks. With the proposed DeiT-LT scheme, the\ndistillation DIST token becomes an expert on the tail classes, and the\nclassifier CLS token becomes an expert on the head classes. The experts help to\neffectively learn features corresponding to both the majority and minority\nclasses using a distinct set of tokens within the same ViT architecture. We\nshow the effectiveness of DeiT-LT for training ViT from scratch on datasets\nranging from small-scale CIFAR-10 LT to large-scale iNaturalist-2018.\n", "link": "http://arxiv.org/abs/2404.02900v1", "date": "2024-04-03", "relevancy": 2.2449, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5647}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5635}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5568}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DeiT-LT%20Distillation%20Strikes%20Back%20for%20Vision%20Transformer%20Training%20on%0A%20%20Long-Tailed%20Datasets&body=Title%3A%20DeiT-LT%20Distillation%20Strikes%20Back%20for%20Vision%20Transformer%20Training%20on%0A%20%20Long-Tailed%20Datasets%0AAuthor%3A%20Harsh%20Rangwani%20and%20Pradipto%20Mondal%20and%20Mayank%20Mishra%20and%20Ashish%20Ramayee%20Asokan%20and%20R.%20Venkatesh%20Babu%0AAbstract%3A%20%20%20Vision%20Transformer%20%28ViT%29%20has%20emerged%20as%20a%20prominent%20architecture%20for%20various%0Acomputer%20vision%20tasks.%20In%20ViT%2C%20we%20divide%20the%20input%20image%20into%20patch%20tokens%20and%0Aprocess%20them%20through%20a%20stack%20of%20self%20attention%20blocks.%20However%2C%20unlike%0AConvolutional%20Neural%20Networks%20%28CNN%29%2C%20ViTs%20simple%20architecture%20has%20no%0Ainformative%20inductive%20bias%20%28e.g.%2C%20locality%2Cetc.%20%29.%20Due%20to%20this%2C%20ViT%20requires%20a%0Alarge%20amount%20of%20data%20for%20pre-training.%20Various%20data%20efficient%20approaches%20%28DeiT%29%0Ahave%20been%20proposed%20to%20train%20ViT%20on%20balanced%20datasets%20effectively.%20However%2C%0Alimited%20literature%20discusses%20the%20use%20of%20ViT%20for%20datasets%20with%20long-tailed%0Aimbalances.%20In%20this%20work%2C%20we%20introduce%20DeiT-LT%20to%20tackle%20the%20problem%20of%0Atraining%20ViTs%20from%20scratch%20on%20long-tailed%20datasets.%20In%20DeiT-LT%2C%20we%20introduce%20an%0Aefficient%20and%20effective%20way%20of%20distillation%20from%20CNN%20via%20distillation%20DIST%0Atoken%20by%20using%20out-of-distribution%20images%20and%20re-weighting%20the%20distillation%0Aloss%20to%20enhance%20focus%20on%20tail%20classes.%20This%20leads%20to%20the%20learning%20of%20local%0ACNN-like%20features%20in%20early%20ViT%20blocks%2C%20improving%20generalization%20for%20tail%0Aclasses.%20Further%2C%20to%20mitigate%20overfitting%2C%20we%20propose%20distilling%20from%20a%20flat%0ACNN%20teacher%2C%20which%20leads%20to%20learning%20low-rank%20generalizable%20features%20for%20DIST%0Atokens%20across%20all%20ViT%20blocks.%20With%20the%20proposed%20DeiT-LT%20scheme%2C%20the%0Adistillation%20DIST%20token%20becomes%20an%20expert%20on%20the%20tail%20classes%2C%20and%20the%0Aclassifier%20CLS%20token%20becomes%20an%20expert%20on%20the%20head%20classes.%20The%20experts%20help%20to%0Aeffectively%20learn%20features%20corresponding%20to%20both%20the%20majority%20and%20minority%0Aclasses%20using%20a%20distinct%20set%20of%20tokens%20within%20the%20same%20ViT%20architecture.%20We%0Ashow%20the%20effectiveness%20of%20DeiT-LT%20for%20training%20ViT%20from%20scratch%20on%20datasets%0Aranging%20from%20small-scale%20CIFAR-10%20LT%20to%20large-scale%20iNaturalist-2018.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02900v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeiT-LT%20Distillation%20Strikes%20Back%20for%20Vision%20Transformer%20Training%20on%0A%20%20Long-Tailed%20Datasets&entry.906535625=Harsh%20Rangwani%20and%20Pradipto%20Mondal%20and%20Mayank%20Mishra%20and%20Ashish%20Ramayee%20Asokan%20and%20R.%20Venkatesh%20Babu&entry.1292438233=%20%20Vision%20Transformer%20%28ViT%29%20has%20emerged%20as%20a%20prominent%20architecture%20for%20various%0Acomputer%20vision%20tasks.%20In%20ViT%2C%20we%20divide%20the%20input%20image%20into%20patch%20tokens%20and%0Aprocess%20them%20through%20a%20stack%20of%20self%20attention%20blocks.%20However%2C%20unlike%0AConvolutional%20Neural%20Networks%20%28CNN%29%2C%20ViTs%20simple%20architecture%20has%20no%0Ainformative%20inductive%20bias%20%28e.g.%2C%20locality%2Cetc.%20%29.%20Due%20to%20this%2C%20ViT%20requires%20a%0Alarge%20amount%20of%20data%20for%20pre-training.%20Various%20data%20efficient%20approaches%20%28DeiT%29%0Ahave%20been%20proposed%20to%20train%20ViT%20on%20balanced%20datasets%20effectively.%20However%2C%0Alimited%20literature%20discusses%20the%20use%20of%20ViT%20for%20datasets%20with%20long-tailed%0Aimbalances.%20In%20this%20work%2C%20we%20introduce%20DeiT-LT%20to%20tackle%20the%20problem%20of%0Atraining%20ViTs%20from%20scratch%20on%20long-tailed%20datasets.%20In%20DeiT-LT%2C%20we%20introduce%20an%0Aefficient%20and%20effective%20way%20of%20distillation%20from%20CNN%20via%20distillation%20DIST%0Atoken%20by%20using%20out-of-distribution%20images%20and%20re-weighting%20the%20distillation%0Aloss%20to%20enhance%20focus%20on%20tail%20classes.%20This%20leads%20to%20the%20learning%20of%20local%0ACNN-like%20features%20in%20early%20ViT%20blocks%2C%20improving%20generalization%20for%20tail%0Aclasses.%20Further%2C%20to%20mitigate%20overfitting%2C%20we%20propose%20distilling%20from%20a%20flat%0ACNN%20teacher%2C%20which%20leads%20to%20learning%20low-rank%20generalizable%20features%20for%20DIST%0Atokens%20across%20all%20ViT%20blocks.%20With%20the%20proposed%20DeiT-LT%20scheme%2C%20the%0Adistillation%20DIST%20token%20becomes%20an%20expert%20on%20the%20tail%20classes%2C%20and%20the%0Aclassifier%20CLS%20token%20becomes%20an%20expert%20on%20the%20head%20classes.%20The%20experts%20help%20to%0Aeffectively%20learn%20features%20corresponding%20to%20both%20the%20majority%20and%20minority%0Aclasses%20using%20a%20distinct%20set%20of%20tokens%20within%20the%20same%20ViT%20architecture.%20We%0Ashow%20the%20effectiveness%20of%20DeiT-LT%20for%20training%20ViT%20from%20scratch%20on%20datasets%0Aranging%20from%20small-scale%20CIFAR-10%20LT%20to%20large-scale%20iNaturalist-2018.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02900v1&entry.124074799=Read"},
{"title": "Efficient and Scalable Graph Generation through Iterative Local\n  Expansion", "author": "Andreas Bergmeister and Karolis Martinkus and Nathana\u00ebl Perraudin and Roger Wattenhofer", "abstract": "  In the realm of generative models for graphs, extensive research has been\nconducted. However, most existing methods struggle with large graphs due to the\ncomplexity of representing the entire joint distribution across all node pairs\nand capturing both global and local graph structures simultaneously. To\novercome these issues, we introduce a method that generates a graph by\nprogressively expanding a single node to a target graph. In each step, nodes\nand edges are added in a localized manner through denoising diffusion, building\nfirst the global structure, and then refining the local details. The local\ngeneration avoids modeling the entire joint distribution over all node pairs,\nachieving substantial computational savings with subquadratic runtime relative\nto node count while maintaining high expressivity through multiscale\ngeneration. Our experiments show that our model achieves state-of-the-art\nperformance on well-established benchmark datasets while successfully scaling\nto graphs with at least 5000 nodes. Our method is also the first to\nsuccessfully extrapolate to graphs outside of the training distribution,\nshowcasing a much better generalization capability over existing methods.\n", "link": "http://arxiv.org/abs/2312.11529v3", "date": "2024-04-03", "relevancy": 2.2416, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5812}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5474}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5408}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Efficient%20and%20Scalable%20Graph%20Generation%20through%20Iterative%20Local%0A%20%20Expansion&body=Title%3A%20Efficient%20and%20Scalable%20Graph%20Generation%20through%20Iterative%20Local%0A%20%20Expansion%0AAuthor%3A%20Andreas%20Bergmeister%20and%20Karolis%20Martinkus%20and%20Nathana%C3%ABl%20Perraudin%20and%20Roger%20Wattenhofer%0AAbstract%3A%20%20%20In%20the%20realm%20of%20generative%20models%20for%20graphs%2C%20extensive%20research%20has%20been%0Aconducted.%20However%2C%20most%20existing%20methods%20struggle%20with%20large%20graphs%20due%20to%20the%0Acomplexity%20of%20representing%20the%20entire%20joint%20distribution%20across%20all%20node%20pairs%0Aand%20capturing%20both%20global%20and%20local%20graph%20structures%20simultaneously.%20To%0Aovercome%20these%20issues%2C%20we%20introduce%20a%20method%20that%20generates%20a%20graph%20by%0Aprogressively%20expanding%20a%20single%20node%20to%20a%20target%20graph.%20In%20each%20step%2C%20nodes%0Aand%20edges%20are%20added%20in%20a%20localized%20manner%20through%20denoising%20diffusion%2C%20building%0Afirst%20the%20global%20structure%2C%20and%20then%20refining%20the%20local%20details.%20The%20local%0Ageneration%20avoids%20modeling%20the%20entire%20joint%20distribution%20over%20all%20node%20pairs%2C%0Aachieving%20substantial%20computational%20savings%20with%20subquadratic%20runtime%20relative%0Ato%20node%20count%20while%20maintaining%20high%20expressivity%20through%20multiscale%0Ageneration.%20Our%20experiments%20show%20that%20our%20model%20achieves%20state-of-the-art%0Aperformance%20on%20well-established%20benchmark%20datasets%20while%20successfully%20scaling%0Ato%20graphs%20with%20at%20least%205000%20nodes.%20Our%20method%20is%20also%20the%20first%20to%0Asuccessfully%20extrapolate%20to%20graphs%20outside%20of%20the%20training%20distribution%2C%0Ashowcasing%20a%20much%20better%20generalization%20capability%20over%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.11529v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20and%20Scalable%20Graph%20Generation%20through%20Iterative%20Local%0A%20%20Expansion&entry.906535625=Andreas%20Bergmeister%20and%20Karolis%20Martinkus%20and%20Nathana%C3%ABl%20Perraudin%20and%20Roger%20Wattenhofer&entry.1292438233=%20%20In%20the%20realm%20of%20generative%20models%20for%20graphs%2C%20extensive%20research%20has%20been%0Aconducted.%20However%2C%20most%20existing%20methods%20struggle%20with%20large%20graphs%20due%20to%20the%0Acomplexity%20of%20representing%20the%20entire%20joint%20distribution%20across%20all%20node%20pairs%0Aand%20capturing%20both%20global%20and%20local%20graph%20structures%20simultaneously.%20To%0Aovercome%20these%20issues%2C%20we%20introduce%20a%20method%20that%20generates%20a%20graph%20by%0Aprogressively%20expanding%20a%20single%20node%20to%20a%20target%20graph.%20In%20each%20step%2C%20nodes%0Aand%20edges%20are%20added%20in%20a%20localized%20manner%20through%20denoising%20diffusion%2C%20building%0Afirst%20the%20global%20structure%2C%20and%20then%20refining%20the%20local%20details.%20The%20local%0Ageneration%20avoids%20modeling%20the%20entire%20joint%20distribution%20over%20all%20node%20pairs%2C%0Aachieving%20substantial%20computational%20savings%20with%20subquadratic%20runtime%20relative%0Ato%20node%20count%20while%20maintaining%20high%20expressivity%20through%20multiscale%0Ageneration.%20Our%20experiments%20show%20that%20our%20model%20achieves%20state-of-the-art%0Aperformance%20on%20well-established%20benchmark%20datasets%20while%20successfully%20scaling%0Ato%20graphs%20with%20at%20least%205000%20nodes.%20Our%20method%20is%20also%20the%20first%20to%0Asuccessfully%20extrapolate%20to%20graphs%20outside%20of%20the%20training%20distribution%2C%0Ashowcasing%20a%20much%20better%20generalization%20capability%20over%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.11529v3&entry.124074799=Read"},
{"title": "LYT-Net: Lightweight YUV Transformer-based Network for Low-Light Image\n  Enhancement", "author": "A. Brateanu and R. Balmez and A. Avram and C. Orhei", "abstract": "  In recent years, deep learning-based solutions have proven successful in the\ndomains of image enhancement. This paper introduces LYT-Net, or Lightweight YUV\nTransformer-based Network, as a novel approach for low-light image enhancement.\nThe proposed architecture, distinct from conventional Retinex-based models,\nleverages the YUV color space's natural separation of luminance (Y) and\nchrominance (U and V) to simplify the intricate task of disentangling light and\ncolor information in images. By utilizing the strengths of transformers, known\nfor their capability to capture long-range dependencies, LYT-Net ensures a\ncomprehensive contextual understanding of the image while maintaining reduced\nmodel complexity. By employing a novel hybrid loss function, our proposed\nmethod achieves state-of-the-art results on low-light image enhancement\ndatasets, all while being considerably more compact than its counterparts. The\nsource code and pre-trained models are available at\nhttps://github.com/albrateanu/LYT-Net\n", "link": "http://arxiv.org/abs/2401.15204v4", "date": "2024-04-03", "relevancy": 2.2318, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5737}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5602}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5494}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LYT-Net%3A%20Lightweight%20YUV%20Transformer-based%20Network%20for%20Low-Light%20Image%0A%20%20Enhancement&body=Title%3A%20LYT-Net%3A%20Lightweight%20YUV%20Transformer-based%20Network%20for%20Low-Light%20Image%0A%20%20Enhancement%0AAuthor%3A%20A.%20Brateanu%20and%20R.%20Balmez%20and%20A.%20Avram%20and%20C.%20Orhei%0AAbstract%3A%20%20%20In%20recent%20years%2C%20deep%20learning-based%20solutions%20have%20proven%20successful%20in%20the%0Adomains%20of%20image%20enhancement.%20This%20paper%20introduces%20LYT-Net%2C%20or%20Lightweight%20YUV%0ATransformer-based%20Network%2C%20as%20a%20novel%20approach%20for%20low-light%20image%20enhancement.%0AThe%20proposed%20architecture%2C%20distinct%20from%20conventional%20Retinex-based%20models%2C%0Aleverages%20the%20YUV%20color%20space%27s%20natural%20separation%20of%20luminance%20%28Y%29%20and%0Achrominance%20%28U%20and%20V%29%20to%20simplify%20the%20intricate%20task%20of%20disentangling%20light%20and%0Acolor%20information%20in%20images.%20By%20utilizing%20the%20strengths%20of%20transformers%2C%20known%0Afor%20their%20capability%20to%20capture%20long-range%20dependencies%2C%20LYT-Net%20ensures%20a%0Acomprehensive%20contextual%20understanding%20of%20the%20image%20while%20maintaining%20reduced%0Amodel%20complexity.%20By%20employing%20a%20novel%20hybrid%20loss%20function%2C%20our%20proposed%0Amethod%20achieves%20state-of-the-art%20results%20on%20low-light%20image%20enhancement%0Adatasets%2C%20all%20while%20being%20considerably%20more%20compact%20than%20its%20counterparts.%20The%0Asource%20code%20and%20pre-trained%20models%20are%20available%20at%0Ahttps%3A//github.com/albrateanu/LYT-Net%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.15204v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LYT-Net%3A%20Lightweight%20YUV%20Transformer-based%20Network%20for%20Low-Light%20Image%0A%20%20Enhancement&entry.906535625=A.%20Brateanu%20and%20R.%20Balmez%20and%20A.%20Avram%20and%20C.%20Orhei&entry.1292438233=%20%20In%20recent%20years%2C%20deep%20learning-based%20solutions%20have%20proven%20successful%20in%20the%0Adomains%20of%20image%20enhancement.%20This%20paper%20introduces%20LYT-Net%2C%20or%20Lightweight%20YUV%0ATransformer-based%20Network%2C%20as%20a%20novel%20approach%20for%20low-light%20image%20enhancement.%0AThe%20proposed%20architecture%2C%20distinct%20from%20conventional%20Retinex-based%20models%2C%0Aleverages%20the%20YUV%20color%20space%27s%20natural%20separation%20of%20luminance%20%28Y%29%20and%0Achrominance%20%28U%20and%20V%29%20to%20simplify%20the%20intricate%20task%20of%20disentangling%20light%20and%0Acolor%20information%20in%20images.%20By%20utilizing%20the%20strengths%20of%20transformers%2C%20known%0Afor%20their%20capability%20to%20capture%20long-range%20dependencies%2C%20LYT-Net%20ensures%20a%0Acomprehensive%20contextual%20understanding%20of%20the%20image%20while%20maintaining%20reduced%0Amodel%20complexity.%20By%20employing%20a%20novel%20hybrid%20loss%20function%2C%20our%20proposed%0Amethod%20achieves%20state-of-the-art%20results%20on%20low-light%20image%20enhancement%0Adatasets%2C%20all%20while%20being%20considerably%20more%20compact%20than%20its%20counterparts.%20The%0Asource%20code%20and%20pre-trained%20models%20are%20available%20at%0Ahttps%3A//github.com/albrateanu/LYT-Net%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.15204v4&entry.124074799=Read"},
{"title": "Design2Cloth: 3D Cloth Generation from 2D Masks", "author": "Jiali Zheng and Rolandos Alexandros Potamias and Stefanos Zafeiriou", "abstract": "  In recent years, there has been a significant shift in the field of digital\navatar research, towards modeling, animating and reconstructing clothed human\nrepresentations, as a key step towards creating realistic avatars. However,\ncurrent 3D cloth generation methods are garment specific or trained completely\non synthetic data, hence lacking fine details and realism. In this work, we\nmake a step towards automatic realistic garment design and propose\nDesign2Cloth, a high fidelity 3D generative model trained on a real world\ndataset from more than 2000 subject scans. To provide vital contribution to the\nfashion industry, we developed a user-friendly adversarial model capable of\ngenerating diverse and detailed clothes simply by drawing a 2D cloth mask.\nUnder a series of both qualitative and quantitative experiments, we showcase\nthat Design2Cloth outperforms current state-of-the-art cloth generative models\nby a large margin. In addition to the generative properties of our network, we\nshowcase that the proposed method can be used to achieve high quality\nreconstructions from single in-the-wild images and 3D scans. Dataset, code and\npre-trained model will become publicly available.\n", "link": "http://arxiv.org/abs/2404.02686v1", "date": "2024-04-03", "relevancy": 2.2315, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6497}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5526}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5265}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Design2Cloth%3A%203D%20Cloth%20Generation%20from%202D%20Masks&body=Title%3A%20Design2Cloth%3A%203D%20Cloth%20Generation%20from%202D%20Masks%0AAuthor%3A%20Jiali%20Zheng%20and%20Rolandos%20Alexandros%20Potamias%20and%20Stefanos%20Zafeiriou%0AAbstract%3A%20%20%20In%20recent%20years%2C%20there%20has%20been%20a%20significant%20shift%20in%20the%20field%20of%20digital%0Aavatar%20research%2C%20towards%20modeling%2C%20animating%20and%20reconstructing%20clothed%20human%0Arepresentations%2C%20as%20a%20key%20step%20towards%20creating%20realistic%20avatars.%20However%2C%0Acurrent%203D%20cloth%20generation%20methods%20are%20garment%20specific%20or%20trained%20completely%0Aon%20synthetic%20data%2C%20hence%20lacking%20fine%20details%20and%20realism.%20In%20this%20work%2C%20we%0Amake%20a%20step%20towards%20automatic%20realistic%20garment%20design%20and%20propose%0ADesign2Cloth%2C%20a%20high%20fidelity%203D%20generative%20model%20trained%20on%20a%20real%20world%0Adataset%20from%20more%20than%202000%20subject%20scans.%20To%20provide%20vital%20contribution%20to%20the%0Afashion%20industry%2C%20we%20developed%20a%20user-friendly%20adversarial%20model%20capable%20of%0Agenerating%20diverse%20and%20detailed%20clothes%20simply%20by%20drawing%20a%202D%20cloth%20mask.%0AUnder%20a%20series%20of%20both%20qualitative%20and%20quantitative%20experiments%2C%20we%20showcase%0Athat%20Design2Cloth%20outperforms%20current%20state-of-the-art%20cloth%20generative%20models%0Aby%20a%20large%20margin.%20In%20addition%20to%20the%20generative%20properties%20of%20our%20network%2C%20we%0Ashowcase%20that%20the%20proposed%20method%20can%20be%20used%20to%20achieve%20high%20quality%0Areconstructions%20from%20single%20in-the-wild%20images%20and%203D%20scans.%20Dataset%2C%20code%20and%0Apre-trained%20model%20will%20become%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02686v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Design2Cloth%3A%203D%20Cloth%20Generation%20from%202D%20Masks&entry.906535625=Jiali%20Zheng%20and%20Rolandos%20Alexandros%20Potamias%20and%20Stefanos%20Zafeiriou&entry.1292438233=%20%20In%20recent%20years%2C%20there%20has%20been%20a%20significant%20shift%20in%20the%20field%20of%20digital%0Aavatar%20research%2C%20towards%20modeling%2C%20animating%20and%20reconstructing%20clothed%20human%0Arepresentations%2C%20as%20a%20key%20step%20towards%20creating%20realistic%20avatars.%20However%2C%0Acurrent%203D%20cloth%20generation%20methods%20are%20garment%20specific%20or%20trained%20completely%0Aon%20synthetic%20data%2C%20hence%20lacking%20fine%20details%20and%20realism.%20In%20this%20work%2C%20we%0Amake%20a%20step%20towards%20automatic%20realistic%20garment%20design%20and%20propose%0ADesign2Cloth%2C%20a%20high%20fidelity%203D%20generative%20model%20trained%20on%20a%20real%20world%0Adataset%20from%20more%20than%202000%20subject%20scans.%20To%20provide%20vital%20contribution%20to%20the%0Afashion%20industry%2C%20we%20developed%20a%20user-friendly%20adversarial%20model%20capable%20of%0Agenerating%20diverse%20and%20detailed%20clothes%20simply%20by%20drawing%20a%202D%20cloth%20mask.%0AUnder%20a%20series%20of%20both%20qualitative%20and%20quantitative%20experiments%2C%20we%20showcase%0Athat%20Design2Cloth%20outperforms%20current%20state-of-the-art%20cloth%20generative%20models%0Aby%20a%20large%20margin.%20In%20addition%20to%20the%20generative%20properties%20of%20our%20network%2C%20we%0Ashowcase%20that%20the%20proposed%20method%20can%20be%20used%20to%20achieve%20high%20quality%0Areconstructions%20from%20single%20in-the-wild%20images%20and%203D%20scans.%20Dataset%2C%20code%20and%0Apre-trained%20model%20will%20become%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02686v1&entry.124074799=Read"},
{"title": "Vocabulary Attack to Hijack Large Language Model Applications", "author": "Patrick Levi and Christoph P. Neumann", "abstract": "  The fast advancements in Large Language Models (LLMs) are driving an\nincreasing number of applications. Together with the growing number of users,\nwe also see an increasing number of attackers who try to outsmart these\nsystems. They want the model to reveal confidential information, specific false\ninformation, or offensive behavior. To this end, they manipulate their\ninstructions for the LLM by inserting separators or rephrasing them\nsystematically until they reach their goal. Our approach is different. It\ninserts words from the model vocabulary. We find these words using an\noptimization procedure and embeddings from another LLM (attacker LLM). We prove\nour approach by goal hijacking two popular open-source LLMs from the Llama2 and\nthe Flan-T5 families, respectively. We present two main findings. First, our\napproach creates inconspicuous instructions and therefore it is hard to detect.\nFor many attack cases, we find that even a single word insertion is sufficient.\nSecond, we demonstrate that we can conduct our attack using a different model\nthan the target model to conduct our attack with.\n", "link": "http://arxiv.org/abs/2404.02637v1", "date": "2024-04-03", "relevancy": 2.2197, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4607}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4361}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4351}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Vocabulary%20Attack%20to%20Hijack%20Large%20Language%20Model%20Applications&body=Title%3A%20Vocabulary%20Attack%20to%20Hijack%20Large%20Language%20Model%20Applications%0AAuthor%3A%20Patrick%20Levi%20and%20Christoph%20P.%20Neumann%0AAbstract%3A%20%20%20The%20fast%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20are%20driving%20an%0Aincreasing%20number%20of%20applications.%20Together%20with%20the%20growing%20number%20of%20users%2C%0Awe%20also%20see%20an%20increasing%20number%20of%20attackers%20who%20try%20to%20outsmart%20these%0Asystems.%20They%20want%20the%20model%20to%20reveal%20confidential%20information%2C%20specific%20false%0Ainformation%2C%20or%20offensive%20behavior.%20To%20this%20end%2C%20they%20manipulate%20their%0Ainstructions%20for%20the%20LLM%20by%20inserting%20separators%20or%20rephrasing%20them%0Asystematically%20until%20they%20reach%20their%20goal.%20Our%20approach%20is%20different.%20It%0Ainserts%20words%20from%20the%20model%20vocabulary.%20We%20find%20these%20words%20using%20an%0Aoptimization%20procedure%20and%20embeddings%20from%20another%20LLM%20%28attacker%20LLM%29.%20We%20prove%0Aour%20approach%20by%20goal%20hijacking%20two%20popular%20open-source%20LLMs%20from%20the%20Llama2%20and%0Athe%20Flan-T5%20families%2C%20respectively.%20We%20present%20two%20main%20findings.%20First%2C%20our%0Aapproach%20creates%20inconspicuous%20instructions%20and%20therefore%20it%20is%20hard%20to%20detect.%0AFor%20many%20attack%20cases%2C%20we%20find%20that%20even%20a%20single%20word%20insertion%20is%20sufficient.%0ASecond%2C%20we%20demonstrate%20that%20we%20can%20conduct%20our%20attack%20using%20a%20different%20model%0Athan%20the%20target%20model%20to%20conduct%20our%20attack%20with.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02637v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vocabulary%20Attack%20to%20Hijack%20Large%20Language%20Model%20Applications&entry.906535625=Patrick%20Levi%20and%20Christoph%20P.%20Neumann&entry.1292438233=%20%20The%20fast%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20are%20driving%20an%0Aincreasing%20number%20of%20applications.%20Together%20with%20the%20growing%20number%20of%20users%2C%0Awe%20also%20see%20an%20increasing%20number%20of%20attackers%20who%20try%20to%20outsmart%20these%0Asystems.%20They%20want%20the%20model%20to%20reveal%20confidential%20information%2C%20specific%20false%0Ainformation%2C%20or%20offensive%20behavior.%20To%20this%20end%2C%20they%20manipulate%20their%0Ainstructions%20for%20the%20LLM%20by%20inserting%20separators%20or%20rephrasing%20them%0Asystematically%20until%20they%20reach%20their%20goal.%20Our%20approach%20is%20different.%20It%0Ainserts%20words%20from%20the%20model%20vocabulary.%20We%20find%20these%20words%20using%20an%0Aoptimization%20procedure%20and%20embeddings%20from%20another%20LLM%20%28attacker%20LLM%29.%20We%20prove%0Aour%20approach%20by%20goal%20hijacking%20two%20popular%20open-source%20LLMs%20from%20the%20Llama2%20and%0Athe%20Flan-T5%20families%2C%20respectively.%20We%20present%20two%20main%20findings.%20First%2C%20our%0Aapproach%20creates%20inconspicuous%20instructions%20and%20therefore%20it%20is%20hard%20to%20detect.%0AFor%20many%20attack%20cases%2C%20we%20find%20that%20even%20a%20single%20word%20insertion%20is%20sufficient.%0ASecond%2C%20we%20demonstrate%20that%20we%20can%20conduct%20our%20attack%20using%20a%20different%20model%0Athan%20the%20target%20model%20to%20conduct%20our%20attack%20with.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02637v1&entry.124074799=Read"},
{"title": "Task-conditioned adaptation of visual features in multi-task policy\n  learning", "author": "Pierre Marza and Laetitia Matignon and Olivier Simonin and Christian Wolf", "abstract": "  Successfully addressing a wide variety of tasks is a core ability of\nautonomous agents, requiring flexibly adapting the underlying decision-making\nstrategies and, as we argue in this work, also adapting the perception modules.\nAn analogical argument would be the human visual system, which uses top-down\nsignals to focus attention determined by the current task. Similarly, we adapt\npre-trained large vision models conditioned on specific downstream tasks in the\ncontext of multi-task policy learning. We introduce task-conditioned adapters\nthat do not require finetuning any pre-trained weights, combined with a single\npolicy trained with behavior cloning and capable of addressing multiple tasks.\nWe condition the visual adapters on task embeddings, which can be selected at\ninference if the task is known, or alternatively inferred from a set of example\ndemonstrations. To this end, we propose a new optimization-based estimator. We\nevaluate the method on a wide variety of tasks from the CortexBench benchmark\nand show that, compared to existing work, it can be addressed with a single\npolicy. In particular, we demonstrate that adapting visual features is a key\ndesign choice and that the method generalizes to unseen tasks given a few\ndemonstrations.\n", "link": "http://arxiv.org/abs/2402.07739v2", "date": "2024-04-03", "relevancy": 2.2044, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5814}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5707}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5194}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Task-conditioned%20adaptation%20of%20visual%20features%20in%20multi-task%20policy%0A%20%20learning&body=Title%3A%20Task-conditioned%20adaptation%20of%20visual%20features%20in%20multi-task%20policy%0A%20%20learning%0AAuthor%3A%20Pierre%20Marza%20and%20Laetitia%20Matignon%20and%20Olivier%20Simonin%20and%20Christian%20Wolf%0AAbstract%3A%20%20%20Successfully%20addressing%20a%20wide%20variety%20of%20tasks%20is%20a%20core%20ability%20of%0Aautonomous%20agents%2C%20requiring%20flexibly%20adapting%20the%20underlying%20decision-making%0Astrategies%20and%2C%20as%20we%20argue%20in%20this%20work%2C%20also%20adapting%20the%20perception%20modules.%0AAn%20analogical%20argument%20would%20be%20the%20human%20visual%20system%2C%20which%20uses%20top-down%0Asignals%20to%20focus%20attention%20determined%20by%20the%20current%20task.%20Similarly%2C%20we%20adapt%0Apre-trained%20large%20vision%20models%20conditioned%20on%20specific%20downstream%20tasks%20in%20the%0Acontext%20of%20multi-task%20policy%20learning.%20We%20introduce%20task-conditioned%20adapters%0Athat%20do%20not%20require%20finetuning%20any%20pre-trained%20weights%2C%20combined%20with%20a%20single%0Apolicy%20trained%20with%20behavior%20cloning%20and%20capable%20of%20addressing%20multiple%20tasks.%0AWe%20condition%20the%20visual%20adapters%20on%20task%20embeddings%2C%20which%20can%20be%20selected%20at%0Ainference%20if%20the%20task%20is%20known%2C%20or%20alternatively%20inferred%20from%20a%20set%20of%20example%0Ademonstrations.%20To%20this%20end%2C%20we%20propose%20a%20new%20optimization-based%20estimator.%20We%0Aevaluate%20the%20method%20on%20a%20wide%20variety%20of%20tasks%20from%20the%20CortexBench%20benchmark%0Aand%20show%20that%2C%20compared%20to%20existing%20work%2C%20it%20can%20be%20addressed%20with%20a%20single%0Apolicy.%20In%20particular%2C%20we%20demonstrate%20that%20adapting%20visual%20features%20is%20a%20key%0Adesign%20choice%20and%20that%20the%20method%20generalizes%20to%20unseen%20tasks%20given%20a%20few%0Ademonstrations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.07739v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Task-conditioned%20adaptation%20of%20visual%20features%20in%20multi-task%20policy%0A%20%20learning&entry.906535625=Pierre%20Marza%20and%20Laetitia%20Matignon%20and%20Olivier%20Simonin%20and%20Christian%20Wolf&entry.1292438233=%20%20Successfully%20addressing%20a%20wide%20variety%20of%20tasks%20is%20a%20core%20ability%20of%0Aautonomous%20agents%2C%20requiring%20flexibly%20adapting%20the%20underlying%20decision-making%0Astrategies%20and%2C%20as%20we%20argue%20in%20this%20work%2C%20also%20adapting%20the%20perception%20modules.%0AAn%20analogical%20argument%20would%20be%20the%20human%20visual%20system%2C%20which%20uses%20top-down%0Asignals%20to%20focus%20attention%20determined%20by%20the%20current%20task.%20Similarly%2C%20we%20adapt%0Apre-trained%20large%20vision%20models%20conditioned%20on%20specific%20downstream%20tasks%20in%20the%0Acontext%20of%20multi-task%20policy%20learning.%20We%20introduce%20task-conditioned%20adapters%0Athat%20do%20not%20require%20finetuning%20any%20pre-trained%20weights%2C%20combined%20with%20a%20single%0Apolicy%20trained%20with%20behavior%20cloning%20and%20capable%20of%20addressing%20multiple%20tasks.%0AWe%20condition%20the%20visual%20adapters%20on%20task%20embeddings%2C%20which%20can%20be%20selected%20at%0Ainference%20if%20the%20task%20is%20known%2C%20or%20alternatively%20inferred%20from%20a%20set%20of%20example%0Ademonstrations.%20To%20this%20end%2C%20we%20propose%20a%20new%20optimization-based%20estimator.%20We%0Aevaluate%20the%20method%20on%20a%20wide%20variety%20of%20tasks%20from%20the%20CortexBench%20benchmark%0Aand%20show%20that%2C%20compared%20to%20existing%20work%2C%20it%20can%20be%20addressed%20with%20a%20single%0Apolicy.%20In%20particular%2C%20we%20demonstrate%20that%20adapting%20visual%20features%20is%20a%20key%0Adesign%20choice%20and%20that%20the%20method%20generalizes%20to%20unseen%20tasks%20given%20a%20few%0Ademonstrations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.07739v2&entry.124074799=Read"},
{"title": "Deep Privacy Funnel Model: From a Discriminative to a Generative\n  Approach with an Application to Face Recognition", "author": "Behrooz Razeghi and Parsa Rahimi and S\u00e9bastien Marcel", "abstract": "  In this study, we apply the information-theoretic Privacy Funnel (PF) model\nto the domain of face recognition, developing a novel method for\nprivacy-preserving representation learning within an end-to-end training\nframework. Our approach addresses the trade-off between obfuscation and utility\nin data protection, quantified through logarithmic loss, also known as\nself-information loss. This research provides a foundational exploration into\nthe integration of information-theoretic privacy principles with representation\nlearning, focusing specifically on the face recognition systems. We\nparticularly highlight the adaptability of our framework with recent\nadvancements in face recognition networks, such as AdaFace and ArcFace. In\naddition, we introduce the Generative Privacy Funnel ($\\mathsf{GenPF}$) model,\na paradigm that extends beyond the traditional scope of the PF model, referred\nto as the Discriminative Privacy Funnel ($\\mathsf{DisPF}$). This\n$\\mathsf{GenPF}$ model brings new perspectives on data generation methods with\nestimation-theoretic and information-theoretic privacy guarantees.\nComplementing these developments, we also present the deep variational PF\n(DVPF) model. This model proposes a tractable variational bound for measuring\ninformation leakage, enhancing the understanding of privacy preservation\nchallenges in deep representation learning. The DVPF model, associated with\nboth $\\mathsf{DisPF}$ and $\\mathsf{GenPF}$ models, sheds light on connections\nwith various generative models such as Variational Autoencoders (VAEs),\nGenerative Adversarial Networks (GANs), and Diffusion models. Complementing our\ntheoretical contributions, we release a reproducible PyTorch package,\nfacilitating further exploration and application of these privacy-preserving\nmethodologies in face recognition systems.\n", "link": "http://arxiv.org/abs/2404.02696v1", "date": "2024-04-03", "relevancy": 2.1988, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5572}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5562}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5396}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Deep%20Privacy%20Funnel%20Model%3A%20From%20a%20Discriminative%20to%20a%20Generative%0A%20%20Approach%20with%20an%20Application%20to%20Face%20Recognition&body=Title%3A%20Deep%20Privacy%20Funnel%20Model%3A%20From%20a%20Discriminative%20to%20a%20Generative%0A%20%20Approach%20with%20an%20Application%20to%20Face%20Recognition%0AAuthor%3A%20Behrooz%20Razeghi%20and%20Parsa%20Rahimi%20and%20S%C3%A9bastien%20Marcel%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20apply%20the%20information-theoretic%20Privacy%20Funnel%20%28PF%29%20model%0Ato%20the%20domain%20of%20face%20recognition%2C%20developing%20a%20novel%20method%20for%0Aprivacy-preserving%20representation%20learning%20within%20an%20end-to-end%20training%0Aframework.%20Our%20approach%20addresses%20the%20trade-off%20between%20obfuscation%20and%20utility%0Ain%20data%20protection%2C%20quantified%20through%20logarithmic%20loss%2C%20also%20known%20as%0Aself-information%20loss.%20This%20research%20provides%20a%20foundational%20exploration%20into%0Athe%20integration%20of%20information-theoretic%20privacy%20principles%20with%20representation%0Alearning%2C%20focusing%20specifically%20on%20the%20face%20recognition%20systems.%20We%0Aparticularly%20highlight%20the%20adaptability%20of%20our%20framework%20with%20recent%0Aadvancements%20in%20face%20recognition%20networks%2C%20such%20as%20AdaFace%20and%20ArcFace.%20In%0Aaddition%2C%20we%20introduce%20the%20Generative%20Privacy%20Funnel%20%28%24%5Cmathsf%7BGenPF%7D%24%29%20model%2C%0Aa%20paradigm%20that%20extends%20beyond%20the%20traditional%20scope%20of%20the%20PF%20model%2C%20referred%0Ato%20as%20the%20Discriminative%20Privacy%20Funnel%20%28%24%5Cmathsf%7BDisPF%7D%24%29.%20This%0A%24%5Cmathsf%7BGenPF%7D%24%20model%20brings%20new%20perspectives%20on%20data%20generation%20methods%20with%0Aestimation-theoretic%20and%20information-theoretic%20privacy%20guarantees.%0AComplementing%20these%20developments%2C%20we%20also%20present%20the%20deep%20variational%20PF%0A%28DVPF%29%20model.%20This%20model%20proposes%20a%20tractable%20variational%20bound%20for%20measuring%0Ainformation%20leakage%2C%20enhancing%20the%20understanding%20of%20privacy%20preservation%0Achallenges%20in%20deep%20representation%20learning.%20The%20DVPF%20model%2C%20associated%20with%0Aboth%20%24%5Cmathsf%7BDisPF%7D%24%20and%20%24%5Cmathsf%7BGenPF%7D%24%20models%2C%20sheds%20light%20on%20connections%0Awith%20various%20generative%20models%20such%20as%20Variational%20Autoencoders%20%28VAEs%29%2C%0AGenerative%20Adversarial%20Networks%20%28GANs%29%2C%20and%20Diffusion%20models.%20Complementing%20our%0Atheoretical%20contributions%2C%20we%20release%20a%20reproducible%20PyTorch%20package%2C%0Afacilitating%20further%20exploration%20and%20application%20of%20these%20privacy-preserving%0Amethodologies%20in%20face%20recognition%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02696v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Privacy%20Funnel%20Model%3A%20From%20a%20Discriminative%20to%20a%20Generative%0A%20%20Approach%20with%20an%20Application%20to%20Face%20Recognition&entry.906535625=Behrooz%20Razeghi%20and%20Parsa%20Rahimi%20and%20S%C3%A9bastien%20Marcel&entry.1292438233=%20%20In%20this%20study%2C%20we%20apply%20the%20information-theoretic%20Privacy%20Funnel%20%28PF%29%20model%0Ato%20the%20domain%20of%20face%20recognition%2C%20developing%20a%20novel%20method%20for%0Aprivacy-preserving%20representation%20learning%20within%20an%20end-to-end%20training%0Aframework.%20Our%20approach%20addresses%20the%20trade-off%20between%20obfuscation%20and%20utility%0Ain%20data%20protection%2C%20quantified%20through%20logarithmic%20loss%2C%20also%20known%20as%0Aself-information%20loss.%20This%20research%20provides%20a%20foundational%20exploration%20into%0Athe%20integration%20of%20information-theoretic%20privacy%20principles%20with%20representation%0Alearning%2C%20focusing%20specifically%20on%20the%20face%20recognition%20systems.%20We%0Aparticularly%20highlight%20the%20adaptability%20of%20our%20framework%20with%20recent%0Aadvancements%20in%20face%20recognition%20networks%2C%20such%20as%20AdaFace%20and%20ArcFace.%20In%0Aaddition%2C%20we%20introduce%20the%20Generative%20Privacy%20Funnel%20%28%24%5Cmathsf%7BGenPF%7D%24%29%20model%2C%0Aa%20paradigm%20that%20extends%20beyond%20the%20traditional%20scope%20of%20the%20PF%20model%2C%20referred%0Ato%20as%20the%20Discriminative%20Privacy%20Funnel%20%28%24%5Cmathsf%7BDisPF%7D%24%29.%20This%0A%24%5Cmathsf%7BGenPF%7D%24%20model%20brings%20new%20perspectives%20on%20data%20generation%20methods%20with%0Aestimation-theoretic%20and%20information-theoretic%20privacy%20guarantees.%0AComplementing%20these%20developments%2C%20we%20also%20present%20the%20deep%20variational%20PF%0A%28DVPF%29%20model.%20This%20model%20proposes%20a%20tractable%20variational%20bound%20for%20measuring%0Ainformation%20leakage%2C%20enhancing%20the%20understanding%20of%20privacy%20preservation%0Achallenges%20in%20deep%20representation%20learning.%20The%20DVPF%20model%2C%20associated%20with%0Aboth%20%24%5Cmathsf%7BDisPF%7D%24%20and%20%24%5Cmathsf%7BGenPF%7D%24%20models%2C%20sheds%20light%20on%20connections%0Awith%20various%20generative%20models%20such%20as%20Variational%20Autoencoders%20%28VAEs%29%2C%0AGenerative%20Adversarial%20Networks%20%28GANs%29%2C%20and%20Diffusion%20models.%20Complementing%20our%0Atheoretical%20contributions%2C%20we%20release%20a%20reproducible%20PyTorch%20package%2C%0Afacilitating%20further%20exploration%20and%20application%20of%20these%20privacy-preserving%0Amethodologies%20in%20face%20recognition%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02696v1&entry.124074799=Read"},
{"title": "NEAT: Distilling 3D Wireframes from Neural Attraction Fields", "author": "Nan Xue and Bin Tan and Yuxi Xiao and Liang Dong and Gui-Song Xia and Tianfu Wu and Yujun Shen", "abstract": "  This paper studies the problem of structured 3D reconstruction using\nwireframes that consist of line segments and junctions, focusing on the\ncomputation of structured boundary geometries of scenes. Instead of leveraging\nmatching-based solutions from 2D wireframes (or line segments) for 3D wireframe\nreconstruction as done in prior arts, we present NEAT, a rendering-distilling\nformulation using neural fields to represent 3D line segments with 2D\nobservations, and bipartite matching for perceiving and distilling of a sparse\nset of 3D global junctions. The proposed {NEAT} enjoys the joint optimization\nof the neural fields and the global junctions from scratch, using\nview-dependent 2D observations without precomputed cross-view feature matching.\nComprehensive experiments on the DTU and BlendedMVS datasets demonstrate our\nNEAT's superiority over state-of-the-art alternatives for 3D wireframe\nreconstruction. Moreover, the distilled 3D global junctions by NEAT, are a\nbetter initialization than SfM points, for the recently-emerged 3D Gaussian\nSplatting for high-fidelity novel view synthesis using about 20 times fewer\ninitial 3D points. Project page: \\url{https://xuenan.net/neat}.\n", "link": "http://arxiv.org/abs/2307.10206v2", "date": "2024-04-03", "relevancy": 2.1931, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.589}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5222}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5117}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20NEAT%3A%20Distilling%203D%20Wireframes%20from%20Neural%20Attraction%20Fields&body=Title%3A%20NEAT%3A%20Distilling%203D%20Wireframes%20from%20Neural%20Attraction%20Fields%0AAuthor%3A%20Nan%20Xue%20and%20Bin%20Tan%20and%20Yuxi%20Xiao%20and%20Liang%20Dong%20and%20Gui-Song%20Xia%20and%20Tianfu%20Wu%20and%20Yujun%20Shen%0AAbstract%3A%20%20%20This%20paper%20studies%20the%20problem%20of%20structured%203D%20reconstruction%20using%0Awireframes%20that%20consist%20of%20line%20segments%20and%20junctions%2C%20focusing%20on%20the%0Acomputation%20of%20structured%20boundary%20geometries%20of%20scenes.%20Instead%20of%20leveraging%0Amatching-based%20solutions%20from%202D%20wireframes%20%28or%20line%20segments%29%20for%203D%20wireframe%0Areconstruction%20as%20done%20in%20prior%20arts%2C%20we%20present%20NEAT%2C%20a%20rendering-distilling%0Aformulation%20using%20neural%20fields%20to%20represent%203D%20line%20segments%20with%202D%0Aobservations%2C%20and%20bipartite%20matching%20for%20perceiving%20and%20distilling%20of%20a%20sparse%0Aset%20of%203D%20global%20junctions.%20The%20proposed%20%7BNEAT%7D%20enjoys%20the%20joint%20optimization%0Aof%20the%20neural%20fields%20and%20the%20global%20junctions%20from%20scratch%2C%20using%0Aview-dependent%202D%20observations%20without%20precomputed%20cross-view%20feature%20matching.%0AComprehensive%20experiments%20on%20the%20DTU%20and%20BlendedMVS%20datasets%20demonstrate%20our%0ANEAT%27s%20superiority%20over%20state-of-the-art%20alternatives%20for%203D%20wireframe%0Areconstruction.%20Moreover%2C%20the%20distilled%203D%20global%20junctions%20by%20NEAT%2C%20are%20a%0Abetter%20initialization%20than%20SfM%20points%2C%20for%20the%20recently-emerged%203D%20Gaussian%0ASplatting%20for%20high-fidelity%20novel%20view%20synthesis%20using%20about%2020%20times%20fewer%0Ainitial%203D%20points.%20Project%20page%3A%20%5Curl%7Bhttps%3A//xuenan.net/neat%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.10206v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NEAT%3A%20Distilling%203D%20Wireframes%20from%20Neural%20Attraction%20Fields&entry.906535625=Nan%20Xue%20and%20Bin%20Tan%20and%20Yuxi%20Xiao%20and%20Liang%20Dong%20and%20Gui-Song%20Xia%20and%20Tianfu%20Wu%20and%20Yujun%20Shen&entry.1292438233=%20%20This%20paper%20studies%20the%20problem%20of%20structured%203D%20reconstruction%20using%0Awireframes%20that%20consist%20of%20line%20segments%20and%20junctions%2C%20focusing%20on%20the%0Acomputation%20of%20structured%20boundary%20geometries%20of%20scenes.%20Instead%20of%20leveraging%0Amatching-based%20solutions%20from%202D%20wireframes%20%28or%20line%20segments%29%20for%203D%20wireframe%0Areconstruction%20as%20done%20in%20prior%20arts%2C%20we%20present%20NEAT%2C%20a%20rendering-distilling%0Aformulation%20using%20neural%20fields%20to%20represent%203D%20line%20segments%20with%202D%0Aobservations%2C%20and%20bipartite%20matching%20for%20perceiving%20and%20distilling%20of%20a%20sparse%0Aset%20of%203D%20global%20junctions.%20The%20proposed%20%7BNEAT%7D%20enjoys%20the%20joint%20optimization%0Aof%20the%20neural%20fields%20and%20the%20global%20junctions%20from%20scratch%2C%20using%0Aview-dependent%202D%20observations%20without%20precomputed%20cross-view%20feature%20matching.%0AComprehensive%20experiments%20on%20the%20DTU%20and%20BlendedMVS%20datasets%20demonstrate%20our%0ANEAT%27s%20superiority%20over%20state-of-the-art%20alternatives%20for%203D%20wireframe%0Areconstruction.%20Moreover%2C%20the%20distilled%203D%20global%20junctions%20by%20NEAT%2C%20are%20a%0Abetter%20initialization%20than%20SfM%20points%2C%20for%20the%20recently-emerged%203D%20Gaussian%0ASplatting%20for%20high-fidelity%20novel%20view%20synthesis%20using%20about%2020%20times%20fewer%0Ainitial%203D%20points.%20Project%20page%3A%20%5Curl%7Bhttps%3A//xuenan.net/neat%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.10206v2&entry.124074799=Read"},
{"title": "LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis", "author": "Zehan Zheng and Fan Lu and Weiyi Xue and Guang Chen and Changjun Jiang", "abstract": "  Although neural radiance fields (NeRFs) have achieved triumphs in image novel\nview synthesis (NVS), LiDAR NVS remains largely unexplored. Previous LiDAR NVS\nmethods employ a simple shift from image NVS methods while ignoring the dynamic\nnature and the large-scale reconstruction problem of LiDAR point clouds. In\nlight of this, we propose LiDAR4D, a differentiable LiDAR-only framework for\nnovel space-time LiDAR view synthesis. In consideration of the sparsity and\nlarge-scale characteristics, we design a 4D hybrid representation combined with\nmulti-planar and grid features to achieve effective reconstruction in a\ncoarse-to-fine manner. Furthermore, we introduce geometric constraints derived\nfrom point clouds to improve temporal consistency. For the realistic synthesis\nof LiDAR point clouds, we incorporate the global optimization of ray-drop\nprobability to preserve cross-region patterns. Extensive experiments on\nKITTI-360 and NuScenes datasets demonstrate the superiority of our method in\naccomplishing geometry-aware and time-consistent dynamic reconstruction. Codes\nare available at https://github.com/ispc-lab/LiDAR4D.\n", "link": "http://arxiv.org/abs/2404.02742v1", "date": "2024-04-03", "relevancy": 2.1922, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.559}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5421}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5356}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LiDAR4D%3A%20Dynamic%20Neural%20Fields%20for%20Novel%20Space-time%20View%20LiDAR%20Synthesis&body=Title%3A%20LiDAR4D%3A%20Dynamic%20Neural%20Fields%20for%20Novel%20Space-time%20View%20LiDAR%20Synthesis%0AAuthor%3A%20Zehan%20Zheng%20and%20Fan%20Lu%20and%20Weiyi%20Xue%20and%20Guang%20Chen%20and%20Changjun%20Jiang%0AAbstract%3A%20%20%20Although%20neural%20radiance%20fields%20%28NeRFs%29%20have%20achieved%20triumphs%20in%20image%20novel%0Aview%20synthesis%20%28NVS%29%2C%20LiDAR%20NVS%20remains%20largely%20unexplored.%20Previous%20LiDAR%20NVS%0Amethods%20employ%20a%20simple%20shift%20from%20image%20NVS%20methods%20while%20ignoring%20the%20dynamic%0Anature%20and%20the%20large-scale%20reconstruction%20problem%20of%20LiDAR%20point%20clouds.%20In%0Alight%20of%20this%2C%20we%20propose%20LiDAR4D%2C%20a%20differentiable%20LiDAR-only%20framework%20for%0Anovel%20space-time%20LiDAR%20view%20synthesis.%20In%20consideration%20of%20the%20sparsity%20and%0Alarge-scale%20characteristics%2C%20we%20design%20a%204D%20hybrid%20representation%20combined%20with%0Amulti-planar%20and%20grid%20features%20to%20achieve%20effective%20reconstruction%20in%20a%0Acoarse-to-fine%20manner.%20Furthermore%2C%20we%20introduce%20geometric%20constraints%20derived%0Afrom%20point%20clouds%20to%20improve%20temporal%20consistency.%20For%20the%20realistic%20synthesis%0Aof%20LiDAR%20point%20clouds%2C%20we%20incorporate%20the%20global%20optimization%20of%20ray-drop%0Aprobability%20to%20preserve%20cross-region%20patterns.%20Extensive%20experiments%20on%0AKITTI-360%20and%20NuScenes%20datasets%20demonstrate%20the%20superiority%20of%20our%20method%20in%0Aaccomplishing%20geometry-aware%20and%20time-consistent%20dynamic%20reconstruction.%20Codes%0Aare%20available%20at%20https%3A//github.com/ispc-lab/LiDAR4D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02742v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiDAR4D%3A%20Dynamic%20Neural%20Fields%20for%20Novel%20Space-time%20View%20LiDAR%20Synthesis&entry.906535625=Zehan%20Zheng%20and%20Fan%20Lu%20and%20Weiyi%20Xue%20and%20Guang%20Chen%20and%20Changjun%20Jiang&entry.1292438233=%20%20Although%20neural%20radiance%20fields%20%28NeRFs%29%20have%20achieved%20triumphs%20in%20image%20novel%0Aview%20synthesis%20%28NVS%29%2C%20LiDAR%20NVS%20remains%20largely%20unexplored.%20Previous%20LiDAR%20NVS%0Amethods%20employ%20a%20simple%20shift%20from%20image%20NVS%20methods%20while%20ignoring%20the%20dynamic%0Anature%20and%20the%20large-scale%20reconstruction%20problem%20of%20LiDAR%20point%20clouds.%20In%0Alight%20of%20this%2C%20we%20propose%20LiDAR4D%2C%20a%20differentiable%20LiDAR-only%20framework%20for%0Anovel%20space-time%20LiDAR%20view%20synthesis.%20In%20consideration%20of%20the%20sparsity%20and%0Alarge-scale%20characteristics%2C%20we%20design%20a%204D%20hybrid%20representation%20combined%20with%0Amulti-planar%20and%20grid%20features%20to%20achieve%20effective%20reconstruction%20in%20a%0Acoarse-to-fine%20manner.%20Furthermore%2C%20we%20introduce%20geometric%20constraints%20derived%0Afrom%20point%20clouds%20to%20improve%20temporal%20consistency.%20For%20the%20realistic%20synthesis%0Aof%20LiDAR%20point%20clouds%2C%20we%20incorporate%20the%20global%20optimization%20of%20ray-drop%0Aprobability%20to%20preserve%20cross-region%20patterns.%20Extensive%20experiments%20on%0AKITTI-360%20and%20NuScenes%20datasets%20demonstrate%20the%20superiority%20of%20our%20method%20in%0Aaccomplishing%20geometry-aware%20and%20time-consistent%20dynamic%20reconstruction.%20Codes%0Aare%20available%20at%20https%3A//github.com/ispc-lab/LiDAR4D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02742v1&entry.124074799=Read"},
{"title": "DIBS: Enhancing Dense Video Captioning with Unlabeled Videos via Pseudo\n  Boundary Enrichment and Online Refinement", "author": "Hao Wu and Huabin Liu and Yu Qiao and Xiao Sun", "abstract": "  We present Dive Into the BoundarieS (DIBS), a novel pretraining framework for\ndense video captioning (DVC), that elaborates on improving the quality of the\ngenerated event captions and their associated pseudo event boundaries from\nunlabeled videos. By leveraging the capabilities of diverse large language\nmodels (LLMs), we generate rich DVC-oriented caption candidates and optimize\nthe corresponding pseudo boundaries under several meticulously designed\nobjectives, considering diversity, event-centricity, temporal ordering, and\ncoherence. Moreover, we further introduce a novel online boundary refinement\nstrategy that iteratively improves the quality of pseudo boundaries during\ntraining. Comprehensive experiments have been conducted to examine the\neffectiveness of the proposed technique components. By leveraging a substantial\namount of unlabeled video data, such as HowTo100M, we achieve a remarkable\nadvancement on standard DVC datasets like YouCook2 and ActivityNet. We\noutperform the previous state-of-the-art Vid2Seq across a majority of metrics,\nachieving this with just 0.4% of the unlabeled video data used for pre-training\nby Vid2Seq.\n", "link": "http://arxiv.org/abs/2404.02755v1", "date": "2024-04-03", "relevancy": 2.1818, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5594}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.545}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5403}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DIBS%3A%20Enhancing%20Dense%20Video%20Captioning%20with%20Unlabeled%20Videos%20via%20Pseudo%0A%20%20Boundary%20Enrichment%20and%20Online%20Refinement&body=Title%3A%20DIBS%3A%20Enhancing%20Dense%20Video%20Captioning%20with%20Unlabeled%20Videos%20via%20Pseudo%0A%20%20Boundary%20Enrichment%20and%20Online%20Refinement%0AAuthor%3A%20Hao%20Wu%20and%20Huabin%20Liu%20and%20Yu%20Qiao%20and%20Xiao%20Sun%0AAbstract%3A%20%20%20We%20present%20Dive%20Into%20the%20BoundarieS%20%28DIBS%29%2C%20a%20novel%20pretraining%20framework%20for%0Adense%20video%20captioning%20%28DVC%29%2C%20that%20elaborates%20on%20improving%20the%20quality%20of%20the%0Agenerated%20event%20captions%20and%20their%20associated%20pseudo%20event%20boundaries%20from%0Aunlabeled%20videos.%20By%20leveraging%20the%20capabilities%20of%20diverse%20large%20language%0Amodels%20%28LLMs%29%2C%20we%20generate%20rich%20DVC-oriented%20caption%20candidates%20and%20optimize%0Athe%20corresponding%20pseudo%20boundaries%20under%20several%20meticulously%20designed%0Aobjectives%2C%20considering%20diversity%2C%20event-centricity%2C%20temporal%20ordering%2C%20and%0Acoherence.%20Moreover%2C%20we%20further%20introduce%20a%20novel%20online%20boundary%20refinement%0Astrategy%20that%20iteratively%20improves%20the%20quality%20of%20pseudo%20boundaries%20during%0Atraining.%20Comprehensive%20experiments%20have%20been%20conducted%20to%20examine%20the%0Aeffectiveness%20of%20the%20proposed%20technique%20components.%20By%20leveraging%20a%20substantial%0Aamount%20of%20unlabeled%20video%20data%2C%20such%20as%20HowTo100M%2C%20we%20achieve%20a%20remarkable%0Aadvancement%20on%20standard%20DVC%20datasets%20like%20YouCook2%20and%20ActivityNet.%20We%0Aoutperform%20the%20previous%20state-of-the-art%20Vid2Seq%20across%20a%20majority%20of%20metrics%2C%0Aachieving%20this%20with%20just%200.4%25%20of%20the%20unlabeled%20video%20data%20used%20for%20pre-training%0Aby%20Vid2Seq.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02755v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DIBS%3A%20Enhancing%20Dense%20Video%20Captioning%20with%20Unlabeled%20Videos%20via%20Pseudo%0A%20%20Boundary%20Enrichment%20and%20Online%20Refinement&entry.906535625=Hao%20Wu%20and%20Huabin%20Liu%20and%20Yu%20Qiao%20and%20Xiao%20Sun&entry.1292438233=%20%20We%20present%20Dive%20Into%20the%20BoundarieS%20%28DIBS%29%2C%20a%20novel%20pretraining%20framework%20for%0Adense%20video%20captioning%20%28DVC%29%2C%20that%20elaborates%20on%20improving%20the%20quality%20of%20the%0Agenerated%20event%20captions%20and%20their%20associated%20pseudo%20event%20boundaries%20from%0Aunlabeled%20videos.%20By%20leveraging%20the%20capabilities%20of%20diverse%20large%20language%0Amodels%20%28LLMs%29%2C%20we%20generate%20rich%20DVC-oriented%20caption%20candidates%20and%20optimize%0Athe%20corresponding%20pseudo%20boundaries%20under%20several%20meticulously%20designed%0Aobjectives%2C%20considering%20diversity%2C%20event-centricity%2C%20temporal%20ordering%2C%20and%0Acoherence.%20Moreover%2C%20we%20further%20introduce%20a%20novel%20online%20boundary%20refinement%0Astrategy%20that%20iteratively%20improves%20the%20quality%20of%20pseudo%20boundaries%20during%0Atraining.%20Comprehensive%20experiments%20have%20been%20conducted%20to%20examine%20the%0Aeffectiveness%20of%20the%20proposed%20technique%20components.%20By%20leveraging%20a%20substantial%0Aamount%20of%20unlabeled%20video%20data%2C%20such%20as%20HowTo100M%2C%20we%20achieve%20a%20remarkable%0Aadvancement%20on%20standard%20DVC%20datasets%20like%20YouCook2%20and%20ActivityNet.%20We%0Aoutperform%20the%20previous%20state-of-the-art%20Vid2Seq%20across%20a%20majority%20of%20metrics%2C%0Aachieving%20this%20with%20just%200.4%25%20of%20the%20unlabeled%20video%20data%20used%20for%20pre-training%0Aby%20Vid2Seq.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02755v1&entry.124074799=Read"},
{"title": "Deep Image Composition Meets Image Forgery", "author": "Eren Tahir and Mert Bal", "abstract": "  Image forgery is a topic that has been studied for many years. Before the\nbreakthrough of deep learning, forged images were detected using handcrafted\nfeatures that did not require training. These traditional methods failed to\nperform satisfactorily even on datasets much worse in quality than real-life\nimage manipulations. Advances in deep learning have impacted image forgery\ndetection as much as they have impacted other areas of computer vision and have\nimproved the state of the art. Deep learning models require large amounts of\nlabeled data for training. In the case of image forgery, labeled data at the\npixel level is a very important factor for the models to learn. None of the\nexisting datasets have sufficient size, realism and pixel-level labeling at the\nsame time. This is due to the high cost of producing and labeling quality\nimages. It can take hours for an image editing expert to manipulate just one\nimage. To bridge this gap, we automate data generation using image composition\ntechniques that are very related to image forgery. Unlike other automated data\ngeneration frameworks, we use state of the art image composition deep learning\nmodels to generate spliced images close to the quality of real-life\nmanipulations. Finally, we test the generated dataset on the SOTA image\nmanipulation detection model and show that its prediction performance is lower\ncompared to existing datasets, i.e. we produce realistic images that are more\ndifficult to detect. Dataset will be available at\nhttps://github.com/99eren99/DIS25k .\n", "link": "http://arxiv.org/abs/2404.02897v1", "date": "2024-04-03", "relevancy": 2.1575, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5555}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5284}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5266}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Deep%20Image%20Composition%20Meets%20Image%20Forgery&body=Title%3A%20Deep%20Image%20Composition%20Meets%20Image%20Forgery%0AAuthor%3A%20Eren%20Tahir%20and%20Mert%20Bal%0AAbstract%3A%20%20%20Image%20forgery%20is%20a%20topic%20that%20has%20been%20studied%20for%20many%20years.%20Before%20the%0Abreakthrough%20of%20deep%20learning%2C%20forged%20images%20were%20detected%20using%20handcrafted%0Afeatures%20that%20did%20not%20require%20training.%20These%20traditional%20methods%20failed%20to%0Aperform%20satisfactorily%20even%20on%20datasets%20much%20worse%20in%20quality%20than%20real-life%0Aimage%20manipulations.%20Advances%20in%20deep%20learning%20have%20impacted%20image%20forgery%0Adetection%20as%20much%20as%20they%20have%20impacted%20other%20areas%20of%20computer%20vision%20and%20have%0Aimproved%20the%20state%20of%20the%20art.%20Deep%20learning%20models%20require%20large%20amounts%20of%0Alabeled%20data%20for%20training.%20In%20the%20case%20of%20image%20forgery%2C%20labeled%20data%20at%20the%0Apixel%20level%20is%20a%20very%20important%20factor%20for%20the%20models%20to%20learn.%20None%20of%20the%0Aexisting%20datasets%20have%20sufficient%20size%2C%20realism%20and%20pixel-level%20labeling%20at%20the%0Asame%20time.%20This%20is%20due%20to%20the%20high%20cost%20of%20producing%20and%20labeling%20quality%0Aimages.%20It%20can%20take%20hours%20for%20an%20image%20editing%20expert%20to%20manipulate%20just%20one%0Aimage.%20To%20bridge%20this%20gap%2C%20we%20automate%20data%20generation%20using%20image%20composition%0Atechniques%20that%20are%20very%20related%20to%20image%20forgery.%20Unlike%20other%20automated%20data%0Ageneration%20frameworks%2C%20we%20use%20state%20of%20the%20art%20image%20composition%20deep%20learning%0Amodels%20to%20generate%20spliced%20images%20close%20to%20the%20quality%20of%20real-life%0Amanipulations.%20Finally%2C%20we%20test%20the%20generated%20dataset%20on%20the%20SOTA%20image%0Amanipulation%20detection%20model%20and%20show%20that%20its%20prediction%20performance%20is%20lower%0Acompared%20to%20existing%20datasets%2C%20i.e.%20we%20produce%20realistic%20images%20that%20are%20more%0Adifficult%20to%20detect.%20Dataset%20will%20be%20available%20at%0Ahttps%3A//github.com/99eren99/DIS25k%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02897v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Image%20Composition%20Meets%20Image%20Forgery&entry.906535625=Eren%20Tahir%20and%20Mert%20Bal&entry.1292438233=%20%20Image%20forgery%20is%20a%20topic%20that%20has%20been%20studied%20for%20many%20years.%20Before%20the%0Abreakthrough%20of%20deep%20learning%2C%20forged%20images%20were%20detected%20using%20handcrafted%0Afeatures%20that%20did%20not%20require%20training.%20These%20traditional%20methods%20failed%20to%0Aperform%20satisfactorily%20even%20on%20datasets%20much%20worse%20in%20quality%20than%20real-life%0Aimage%20manipulations.%20Advances%20in%20deep%20learning%20have%20impacted%20image%20forgery%0Adetection%20as%20much%20as%20they%20have%20impacted%20other%20areas%20of%20computer%20vision%20and%20have%0Aimproved%20the%20state%20of%20the%20art.%20Deep%20learning%20models%20require%20large%20amounts%20of%0Alabeled%20data%20for%20training.%20In%20the%20case%20of%20image%20forgery%2C%20labeled%20data%20at%20the%0Apixel%20level%20is%20a%20very%20important%20factor%20for%20the%20models%20to%20learn.%20None%20of%20the%0Aexisting%20datasets%20have%20sufficient%20size%2C%20realism%20and%20pixel-level%20labeling%20at%20the%0Asame%20time.%20This%20is%20due%20to%20the%20high%20cost%20of%20producing%20and%20labeling%20quality%0Aimages.%20It%20can%20take%20hours%20for%20an%20image%20editing%20expert%20to%20manipulate%20just%20one%0Aimage.%20To%20bridge%20this%20gap%2C%20we%20automate%20data%20generation%20using%20image%20composition%0Atechniques%20that%20are%20very%20related%20to%20image%20forgery.%20Unlike%20other%20automated%20data%0Ageneration%20frameworks%2C%20we%20use%20state%20of%20the%20art%20image%20composition%20deep%20learning%0Amodels%20to%20generate%20spliced%20images%20close%20to%20the%20quality%20of%20real-life%0Amanipulations.%20Finally%2C%20we%20test%20the%20generated%20dataset%20on%20the%20SOTA%20image%0Amanipulation%20detection%20model%20and%20show%20that%20its%20prediction%20performance%20is%20lower%0Acompared%20to%20existing%20datasets%2C%20i.e.%20we%20produce%20realistic%20images%20that%20are%20more%0Adifficult%20to%20detect.%20Dataset%20will%20be%20available%20at%0Ahttps%3A//github.com/99eren99/DIS25k%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02897v1&entry.124074799=Read"},
{"title": "Polynomial Graphical Lasso: Learning Edges from Gaussian\n  Graph-Stationary Signals", "author": "Andrei Buciulea and Jiaxi Ying and Antonio G. Marques and Daniel P. Palomar", "abstract": "  This paper introduces Polynomial Graphical Lasso (PGL), a new approach to\nlearning graph structures from nodal signals. Our key contribution lies in\nmodeling the signals as Gaussian and stationary on the graph, enabling the\ndevelopment of a graph-learning formulation that combines the strengths of\ngraphical lasso with a more encompassing model. Specifically, we assume that\nthe precision matrix can take any polynomial form of the sought graph, allowing\nfor increased flexibility in modeling nodal relationships. Given the resulting\ncomplexity and nonconvexity of the resulting optimization problem, we (i)\npropose a low-complexity algorithm that alternates between estimating the graph\nand precision matrices, and (ii) characterize its convergence. We evaluate the\nperformance of PGL through comprehensive numerical simulations using both\nsynthetic and real data, demonstrating its superiority over several\nalternatives. Overall, this approach presents a significant advancement in\ngraph learning and holds promise for various applications in graph-aware signal\nanalysis and beyond.\n", "link": "http://arxiv.org/abs/2404.02621v1", "date": "2024-04-03", "relevancy": 2.157, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.481}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4108}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4023}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Polynomial%20Graphical%20Lasso%3A%20Learning%20Edges%20from%20Gaussian%0A%20%20Graph-Stationary%20Signals&body=Title%3A%20Polynomial%20Graphical%20Lasso%3A%20Learning%20Edges%20from%20Gaussian%0A%20%20Graph-Stationary%20Signals%0AAuthor%3A%20Andrei%20Buciulea%20and%20Jiaxi%20Ying%20and%20Antonio%20G.%20Marques%20and%20Daniel%20P.%20Palomar%0AAbstract%3A%20%20%20This%20paper%20introduces%20Polynomial%20Graphical%20Lasso%20%28PGL%29%2C%20a%20new%20approach%20to%0Alearning%20graph%20structures%20from%20nodal%20signals.%20Our%20key%20contribution%20lies%20in%0Amodeling%20the%20signals%20as%20Gaussian%20and%20stationary%20on%20the%20graph%2C%20enabling%20the%0Adevelopment%20of%20a%20graph-learning%20formulation%20that%20combines%20the%20strengths%20of%0Agraphical%20lasso%20with%20a%20more%20encompassing%20model.%20Specifically%2C%20we%20assume%20that%0Athe%20precision%20matrix%20can%20take%20any%20polynomial%20form%20of%20the%20sought%20graph%2C%20allowing%0Afor%20increased%20flexibility%20in%20modeling%20nodal%20relationships.%20Given%20the%20resulting%0Acomplexity%20and%20nonconvexity%20of%20the%20resulting%20optimization%20problem%2C%20we%20%28i%29%0Apropose%20a%20low-complexity%20algorithm%20that%20alternates%20between%20estimating%20the%20graph%0Aand%20precision%20matrices%2C%20and%20%28ii%29%20characterize%20its%20convergence.%20We%20evaluate%20the%0Aperformance%20of%20PGL%20through%20comprehensive%20numerical%20simulations%20using%20both%0Asynthetic%20and%20real%20data%2C%20demonstrating%20its%20superiority%20over%20several%0Aalternatives.%20Overall%2C%20this%20approach%20presents%20a%20significant%20advancement%20in%0Agraph%20learning%20and%20holds%20promise%20for%20various%20applications%20in%20graph-aware%20signal%0Aanalysis%20and%20beyond.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02621v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Polynomial%20Graphical%20Lasso%3A%20Learning%20Edges%20from%20Gaussian%0A%20%20Graph-Stationary%20Signals&entry.906535625=Andrei%20Buciulea%20and%20Jiaxi%20Ying%20and%20Antonio%20G.%20Marques%20and%20Daniel%20P.%20Palomar&entry.1292438233=%20%20This%20paper%20introduces%20Polynomial%20Graphical%20Lasso%20%28PGL%29%2C%20a%20new%20approach%20to%0Alearning%20graph%20structures%20from%20nodal%20signals.%20Our%20key%20contribution%20lies%20in%0Amodeling%20the%20signals%20as%20Gaussian%20and%20stationary%20on%20the%20graph%2C%20enabling%20the%0Adevelopment%20of%20a%20graph-learning%20formulation%20that%20combines%20the%20strengths%20of%0Agraphical%20lasso%20with%20a%20more%20encompassing%20model.%20Specifically%2C%20we%20assume%20that%0Athe%20precision%20matrix%20can%20take%20any%20polynomial%20form%20of%20the%20sought%20graph%2C%20allowing%0Afor%20increased%20flexibility%20in%20modeling%20nodal%20relationships.%20Given%20the%20resulting%0Acomplexity%20and%20nonconvexity%20of%20the%20resulting%20optimization%20problem%2C%20we%20%28i%29%0Apropose%20a%20low-complexity%20algorithm%20that%20alternates%20between%20estimating%20the%20graph%0Aand%20precision%20matrices%2C%20and%20%28ii%29%20characterize%20its%20convergence.%20We%20evaluate%20the%0Aperformance%20of%20PGL%20through%20comprehensive%20numerical%20simulations%20using%20both%0Asynthetic%20and%20real%20data%2C%20demonstrating%20its%20superiority%20over%20several%0Aalternatives.%20Overall%2C%20this%20approach%20presents%20a%20significant%20advancement%20in%0Agraph%20learning%20and%20holds%20promise%20for%20various%20applications%20in%20graph-aware%20signal%0Aanalysis%20and%20beyond.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02621v1&entry.124074799=Read"},
{"title": "Structure-reinforced Transformer for Dynamic Graph Representation\n  Learning with Edge Temporal States", "author": "Shengxiang Hu and Guobing Zou and Song Yang and Shiyi Lin and Bofeng Zhang and Yixin Chen", "abstract": "  The burgeoning field of dynamic graph representation learning, fuelled by the\nincreasing demand for graph data analysis in real-world applications, poses\nboth enticing opportunities and formidable challenges. Despite the promising\nresults achieved by recent research leveraging recurrent neural networks (RNNs)\nand graph neural networks (GNNs), these approaches often fail to adequately\nconsider the impact of the edge temporal states on the strength of inter-node\nrelationships across different time slices, further overlooking the dynamic\nchanges in node features induced by fluctuations in relationship strength.\nFurthermore, the extraction of global structural features is hindered by the\ninherent over-smoothing drawback of GNNs, which in turn limits their overall\nperformance. In this paper, we introduce a novel dynamic graph representation\nlearning framework namely Recurrent Structure-reinforced Graph Transformer\n(RSGT), which initially models the temporal status of edges explicitly by\nutilizing different edge types and weights based on the differences between any\ntwo consecutive snapshots. In this manner, the varying edge temporal states are\nmapped as a part of the topological structure of the graph. Subsequently, a\nstructure-reinforced graph transformer is proposed to capture temporal node\nrepresentations that encoding both the graph topological structure and evolving\ndynamics,through a recurrent learning paradigm. Our experimental evaluations,\nconducted on four real-world datasets, underscore the superior performance of\nthe RSGT in the realm of discrete dynamic graph representation learning. The\nresults reveal that RSGT consistently surpasses competing methods in dynamic\nlink prediction tasks.\n", "link": "http://arxiv.org/abs/2304.10079v2", "date": "2024-04-03", "relevancy": 2.1561, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5565}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5425}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5285}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Structure-reinforced%20Transformer%20for%20Dynamic%20Graph%20Representation%0A%20%20Learning%20with%20Edge%20Temporal%20States&body=Title%3A%20Structure-reinforced%20Transformer%20for%20Dynamic%20Graph%20Representation%0A%20%20Learning%20with%20Edge%20Temporal%20States%0AAuthor%3A%20Shengxiang%20Hu%20and%20Guobing%20Zou%20and%20Song%20Yang%20and%20Shiyi%20Lin%20and%20Bofeng%20Zhang%20and%20Yixin%20Chen%0AAbstract%3A%20%20%20The%20burgeoning%20field%20of%20dynamic%20graph%20representation%20learning%2C%20fuelled%20by%20the%0Aincreasing%20demand%20for%20graph%20data%20analysis%20in%20real-world%20applications%2C%20poses%0Aboth%20enticing%20opportunities%20and%20formidable%20challenges.%20Despite%20the%20promising%0Aresults%20achieved%20by%20recent%20research%20leveraging%20recurrent%20neural%20networks%20%28RNNs%29%0Aand%20graph%20neural%20networks%20%28GNNs%29%2C%20these%20approaches%20often%20fail%20to%20adequately%0Aconsider%20the%20impact%20of%20the%20edge%20temporal%20states%20on%20the%20strength%20of%20inter-node%0Arelationships%20across%20different%20time%20slices%2C%20further%20overlooking%20the%20dynamic%0Achanges%20in%20node%20features%20induced%20by%20fluctuations%20in%20relationship%20strength.%0AFurthermore%2C%20the%20extraction%20of%20global%20structural%20features%20is%20hindered%20by%20the%0Ainherent%20over-smoothing%20drawback%20of%20GNNs%2C%20which%20in%20turn%20limits%20their%20overall%0Aperformance.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20dynamic%20graph%20representation%0Alearning%20framework%20namely%20Recurrent%20Structure-reinforced%20Graph%20Transformer%0A%28RSGT%29%2C%20which%20initially%20models%20the%20temporal%20status%20of%20edges%20explicitly%20by%0Autilizing%20different%20edge%20types%20and%20weights%20based%20on%20the%20differences%20between%20any%0Atwo%20consecutive%20snapshots.%20In%20this%20manner%2C%20the%20varying%20edge%20temporal%20states%20are%0Amapped%20as%20a%20part%20of%20the%20topological%20structure%20of%20the%20graph.%20Subsequently%2C%20a%0Astructure-reinforced%20graph%20transformer%20is%20proposed%20to%20capture%20temporal%20node%0Arepresentations%20that%20encoding%20both%20the%20graph%20topological%20structure%20and%20evolving%0Adynamics%2Cthrough%20a%20recurrent%20learning%20paradigm.%20Our%20experimental%20evaluations%2C%0Aconducted%20on%20four%20real-world%20datasets%2C%20underscore%20the%20superior%20performance%20of%0Athe%20RSGT%20in%20the%20realm%20of%20discrete%20dynamic%20graph%20representation%20learning.%20The%0Aresults%20reveal%20that%20RSGT%20consistently%20surpasses%20competing%20methods%20in%20dynamic%0Alink%20prediction%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.10079v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structure-reinforced%20Transformer%20for%20Dynamic%20Graph%20Representation%0A%20%20Learning%20with%20Edge%20Temporal%20States&entry.906535625=Shengxiang%20Hu%20and%20Guobing%20Zou%20and%20Song%20Yang%20and%20Shiyi%20Lin%20and%20Bofeng%20Zhang%20and%20Yixin%20Chen&entry.1292438233=%20%20The%20burgeoning%20field%20of%20dynamic%20graph%20representation%20learning%2C%20fuelled%20by%20the%0Aincreasing%20demand%20for%20graph%20data%20analysis%20in%20real-world%20applications%2C%20poses%0Aboth%20enticing%20opportunities%20and%20formidable%20challenges.%20Despite%20the%20promising%0Aresults%20achieved%20by%20recent%20research%20leveraging%20recurrent%20neural%20networks%20%28RNNs%29%0Aand%20graph%20neural%20networks%20%28GNNs%29%2C%20these%20approaches%20often%20fail%20to%20adequately%0Aconsider%20the%20impact%20of%20the%20edge%20temporal%20states%20on%20the%20strength%20of%20inter-node%0Arelationships%20across%20different%20time%20slices%2C%20further%20overlooking%20the%20dynamic%0Achanges%20in%20node%20features%20induced%20by%20fluctuations%20in%20relationship%20strength.%0AFurthermore%2C%20the%20extraction%20of%20global%20structural%20features%20is%20hindered%20by%20the%0Ainherent%20over-smoothing%20drawback%20of%20GNNs%2C%20which%20in%20turn%20limits%20their%20overall%0Aperformance.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20dynamic%20graph%20representation%0Alearning%20framework%20namely%20Recurrent%20Structure-reinforced%20Graph%20Transformer%0A%28RSGT%29%2C%20which%20initially%20models%20the%20temporal%20status%20of%20edges%20explicitly%20by%0Autilizing%20different%20edge%20types%20and%20weights%20based%20on%20the%20differences%20between%20any%0Atwo%20consecutive%20snapshots.%20In%20this%20manner%2C%20the%20varying%20edge%20temporal%20states%20are%0Amapped%20as%20a%20part%20of%20the%20topological%20structure%20of%20the%20graph.%20Subsequently%2C%20a%0Astructure-reinforced%20graph%20transformer%20is%20proposed%20to%20capture%20temporal%20node%0Arepresentations%20that%20encoding%20both%20the%20graph%20topological%20structure%20and%20evolving%0Adynamics%2Cthrough%20a%20recurrent%20learning%20paradigm.%20Our%20experimental%20evaluations%2C%0Aconducted%20on%20four%20real-world%20datasets%2C%20underscore%20the%20superior%20performance%20of%0Athe%20RSGT%20in%20the%20realm%20of%20discrete%20dynamic%20graph%20representation%20learning.%20The%0Aresults%20reveal%20that%20RSGT%20consistently%20surpasses%20competing%20methods%20in%20dynamic%0Alink%20prediction%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.10079v2&entry.124074799=Read"},
{"title": "On the Importance of Uncertainty in Decision-Making with Large Language\n  Models", "author": "Nicol\u00f2 Felicioni and Lucas Maystre and Sina Ghiassian and Kamil Ciosek", "abstract": "  We investigate the role of uncertainty in decision-making problems with\nnatural language as input. For such tasks, using Large Language Models as\nagents has become the norm. However, none of the recent approaches employ any\nadditional phase for estimating the uncertainty the agent has about the world\nduring the decision-making task. We focus on a fundamental decision-making\nframework with natural language as input, which is the one of contextual\nbandits, where the context information consists of text. As a representative of\nthe approaches with no uncertainty estimation, we consider an LLM bandit with a\ngreedy policy, which picks the action corresponding to the largest predicted\nreward. We compare this baseline to LLM bandits that make active use of\nuncertainty estimation by integrating the uncertainty in a Thompson Sampling\npolicy. We employ different techniques for uncertainty estimation, such as\nLaplace Approximation, Dropout, and Epinets. We empirically show on real-world\ndata that the greedy policy performs worse than the Thompson Sampling policies.\nThese findings suggest that, while overlooked in the LLM literature,\nuncertainty plays a fundamental role in bandit tasks with LLMs.\n", "link": "http://arxiv.org/abs/2404.02649v1", "date": "2024-04-03", "relevancy": 2.154, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5752}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5483}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.514}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20On%20the%20Importance%20of%20Uncertainty%20in%20Decision-Making%20with%20Large%20Language%0A%20%20Models&body=Title%3A%20On%20the%20Importance%20of%20Uncertainty%20in%20Decision-Making%20with%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Nicol%C3%B2%20Felicioni%20and%20Lucas%20Maystre%20and%20Sina%20Ghiassian%20and%20Kamil%20Ciosek%0AAbstract%3A%20%20%20We%20investigate%20the%20role%20of%20uncertainty%20in%20decision-making%20problems%20with%0Anatural%20language%20as%20input.%20For%20such%20tasks%2C%20using%20Large%20Language%20Models%20as%0Aagents%20has%20become%20the%20norm.%20However%2C%20none%20of%20the%20recent%20approaches%20employ%20any%0Aadditional%20phase%20for%20estimating%20the%20uncertainty%20the%20agent%20has%20about%20the%20world%0Aduring%20the%20decision-making%20task.%20We%20focus%20on%20a%20fundamental%20decision-making%0Aframework%20with%20natural%20language%20as%20input%2C%20which%20is%20the%20one%20of%20contextual%0Abandits%2C%20where%20the%20context%20information%20consists%20of%20text.%20As%20a%20representative%20of%0Athe%20approaches%20with%20no%20uncertainty%20estimation%2C%20we%20consider%20an%20LLM%20bandit%20with%20a%0Agreedy%20policy%2C%20which%20picks%20the%20action%20corresponding%20to%20the%20largest%20predicted%0Areward.%20We%20compare%20this%20baseline%20to%20LLM%20bandits%20that%20make%20active%20use%20of%0Auncertainty%20estimation%20by%20integrating%20the%20uncertainty%20in%20a%20Thompson%20Sampling%0Apolicy.%20We%20employ%20different%20techniques%20for%20uncertainty%20estimation%2C%20such%20as%0ALaplace%20Approximation%2C%20Dropout%2C%20and%20Epinets.%20We%20empirically%20show%20on%20real-world%0Adata%20that%20the%20greedy%20policy%20performs%20worse%20than%20the%20Thompson%20Sampling%20policies.%0AThese%20findings%20suggest%20that%2C%20while%20overlooked%20in%20the%20LLM%20literature%2C%0Auncertainty%20plays%20a%20fundamental%20role%20in%20bandit%20tasks%20with%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02649v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Importance%20of%20Uncertainty%20in%20Decision-Making%20with%20Large%20Language%0A%20%20Models&entry.906535625=Nicol%C3%B2%20Felicioni%20and%20Lucas%20Maystre%20and%20Sina%20Ghiassian%20and%20Kamil%20Ciosek&entry.1292438233=%20%20We%20investigate%20the%20role%20of%20uncertainty%20in%20decision-making%20problems%20with%0Anatural%20language%20as%20input.%20For%20such%20tasks%2C%20using%20Large%20Language%20Models%20as%0Aagents%20has%20become%20the%20norm.%20However%2C%20none%20of%20the%20recent%20approaches%20employ%20any%0Aadditional%20phase%20for%20estimating%20the%20uncertainty%20the%20agent%20has%20about%20the%20world%0Aduring%20the%20decision-making%20task.%20We%20focus%20on%20a%20fundamental%20decision-making%0Aframework%20with%20natural%20language%20as%20input%2C%20which%20is%20the%20one%20of%20contextual%0Abandits%2C%20where%20the%20context%20information%20consists%20of%20text.%20As%20a%20representative%20of%0Athe%20approaches%20with%20no%20uncertainty%20estimation%2C%20we%20consider%20an%20LLM%20bandit%20with%20a%0Agreedy%20policy%2C%20which%20picks%20the%20action%20corresponding%20to%20the%20largest%20predicted%0Areward.%20We%20compare%20this%20baseline%20to%20LLM%20bandits%20that%20make%20active%20use%20of%0Auncertainty%20estimation%20by%20integrating%20the%20uncertainty%20in%20a%20Thompson%20Sampling%0Apolicy.%20We%20employ%20different%20techniques%20for%20uncertainty%20estimation%2C%20such%20as%0ALaplace%20Approximation%2C%20Dropout%2C%20and%20Epinets.%20We%20empirically%20show%20on%20real-world%0Adata%20that%20the%20greedy%20policy%20performs%20worse%20than%20the%20Thompson%20Sampling%20policies.%0AThese%20findings%20suggest%20that%2C%20while%20overlooked%20in%20the%20LLM%20literature%2C%0Auncertainty%20plays%20a%20fundamental%20role%20in%20bandit%20tasks%20with%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02649v1&entry.124074799=Read"},
{"title": "CV-Attention UNet: Attention-based UNet for 3D Cerebrovascular\n  Segmentation of Enhanced TOF-MRA Images", "author": "Syed Farhan Abbas and Nguyen Thanh Duc and Yoonguu Song and Kyungwon Kim and Ekta Srivastava and Boreom Lee", "abstract": "  Due to the lack of automated methods, to diagnose cerebrovascular disease,\ntime-of-flight magnetic resonance angiography (TOF-MRA) is assessed visually,\nmaking it time-consuming. The commonly used encoder-decoder architectures for\ncerebrovascular segmentation utilize redundant features, eventually leading to\nthe extraction of low-level features multiple times. Additionally,\nconvolutional neural networks (CNNs) suffer from performance degradation when\nthe batch size is small, and deeper networks experience the vanishing gradient\nproblem. Methods: In this paper, we attempt to solve these limitations and\npropose the 3D cerebrovascular attention UNet method, named CV-AttentionUNet,\nfor precise extraction of brain vessel images. We proposed a sequence of\npreprocessing techniques followed by deeply supervised UNet to improve the\naccuracy of segmentation of the brain vessels leading to a stroke. To combine\nthe low and high semantics, we applied the attention mechanism. This mechanism\nfocuses on relevant associations and neglects irrelevant anatomical\ninformation. Furthermore, the inclusion of deep supervision incorporates\ndifferent levels of features that prove to be beneficial for network\nconvergence. Results: We demonstrate the efficiency of the proposed method by\ncross-validating with an unlabeled dataset, which was further labeled by us. We\nbelieve that the novelty of this algorithm lies in its ability to perform well\non both labeled and unlabeled data with image processing-based enhancement. The\nresults indicate that our method performed better than the existing\nstate-of-the-art methods on the TubeTK dataset. Conclusion: The proposed method\nwill help in accurate segmentation of cerebrovascular structure leading to\nstroke\n", "link": "http://arxiv.org/abs/2311.10224v2", "date": "2024-04-03", "relevancy": 2.1502, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5418}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5396}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5325}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CV-Attention%20UNet%3A%20Attention-based%20UNet%20for%203D%20Cerebrovascular%0A%20%20Segmentation%20of%20Enhanced%20TOF-MRA%20Images&body=Title%3A%20CV-Attention%20UNet%3A%20Attention-based%20UNet%20for%203D%20Cerebrovascular%0A%20%20Segmentation%20of%20Enhanced%20TOF-MRA%20Images%0AAuthor%3A%20Syed%20Farhan%20Abbas%20and%20Nguyen%20Thanh%20Duc%20and%20Yoonguu%20Song%20and%20Kyungwon%20Kim%20and%20Ekta%20Srivastava%20and%20Boreom%20Lee%0AAbstract%3A%20%20%20Due%20to%20the%20lack%20of%20automated%20methods%2C%20to%20diagnose%20cerebrovascular%20disease%2C%0Atime-of-flight%20magnetic%20resonance%20angiography%20%28TOF-MRA%29%20is%20assessed%20visually%2C%0Amaking%20it%20time-consuming.%20The%20commonly%20used%20encoder-decoder%20architectures%20for%0Acerebrovascular%20segmentation%20utilize%20redundant%20features%2C%20eventually%20leading%20to%0Athe%20extraction%20of%20low-level%20features%20multiple%20times.%20Additionally%2C%0Aconvolutional%20neural%20networks%20%28CNNs%29%20suffer%20from%20performance%20degradation%20when%0Athe%20batch%20size%20is%20small%2C%20and%20deeper%20networks%20experience%20the%20vanishing%20gradient%0Aproblem.%20Methods%3A%20In%20this%20paper%2C%20we%20attempt%20to%20solve%20these%20limitations%20and%0Apropose%20the%203D%20cerebrovascular%20attention%20UNet%20method%2C%20named%20CV-AttentionUNet%2C%0Afor%20precise%20extraction%20of%20brain%20vessel%20images.%20We%20proposed%20a%20sequence%20of%0Apreprocessing%20techniques%20followed%20by%20deeply%20supervised%20UNet%20to%20improve%20the%0Aaccuracy%20of%20segmentation%20of%20the%20brain%20vessels%20leading%20to%20a%20stroke.%20To%20combine%0Athe%20low%20and%20high%20semantics%2C%20we%20applied%20the%20attention%20mechanism.%20This%20mechanism%0Afocuses%20on%20relevant%20associations%20and%20neglects%20irrelevant%20anatomical%0Ainformation.%20Furthermore%2C%20the%20inclusion%20of%20deep%20supervision%20incorporates%0Adifferent%20levels%20of%20features%20that%20prove%20to%20be%20beneficial%20for%20network%0Aconvergence.%20Results%3A%20We%20demonstrate%20the%20efficiency%20of%20the%20proposed%20method%20by%0Across-validating%20with%20an%20unlabeled%20dataset%2C%20which%20was%20further%20labeled%20by%20us.%20We%0Abelieve%20that%20the%20novelty%20of%20this%20algorithm%20lies%20in%20its%20ability%20to%20perform%20well%0Aon%20both%20labeled%20and%20unlabeled%20data%20with%20image%20processing-based%20enhancement.%20The%0Aresults%20indicate%20that%20our%20method%20performed%20better%20than%20the%20existing%0Astate-of-the-art%20methods%20on%20the%20TubeTK%20dataset.%20Conclusion%3A%20The%20proposed%20method%0Awill%20help%20in%20accurate%20segmentation%20of%20cerebrovascular%20structure%20leading%20to%0Astroke%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.10224v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CV-Attention%20UNet%3A%20Attention-based%20UNet%20for%203D%20Cerebrovascular%0A%20%20Segmentation%20of%20Enhanced%20TOF-MRA%20Images&entry.906535625=Syed%20Farhan%20Abbas%20and%20Nguyen%20Thanh%20Duc%20and%20Yoonguu%20Song%20and%20Kyungwon%20Kim%20and%20Ekta%20Srivastava%20and%20Boreom%20Lee&entry.1292438233=%20%20Due%20to%20the%20lack%20of%20automated%20methods%2C%20to%20diagnose%20cerebrovascular%20disease%2C%0Atime-of-flight%20magnetic%20resonance%20angiography%20%28TOF-MRA%29%20is%20assessed%20visually%2C%0Amaking%20it%20time-consuming.%20The%20commonly%20used%20encoder-decoder%20architectures%20for%0Acerebrovascular%20segmentation%20utilize%20redundant%20features%2C%20eventually%20leading%20to%0Athe%20extraction%20of%20low-level%20features%20multiple%20times.%20Additionally%2C%0Aconvolutional%20neural%20networks%20%28CNNs%29%20suffer%20from%20performance%20degradation%20when%0Athe%20batch%20size%20is%20small%2C%20and%20deeper%20networks%20experience%20the%20vanishing%20gradient%0Aproblem.%20Methods%3A%20In%20this%20paper%2C%20we%20attempt%20to%20solve%20these%20limitations%20and%0Apropose%20the%203D%20cerebrovascular%20attention%20UNet%20method%2C%20named%20CV-AttentionUNet%2C%0Afor%20precise%20extraction%20of%20brain%20vessel%20images.%20We%20proposed%20a%20sequence%20of%0Apreprocessing%20techniques%20followed%20by%20deeply%20supervised%20UNet%20to%20improve%20the%0Aaccuracy%20of%20segmentation%20of%20the%20brain%20vessels%20leading%20to%20a%20stroke.%20To%20combine%0Athe%20low%20and%20high%20semantics%2C%20we%20applied%20the%20attention%20mechanism.%20This%20mechanism%0Afocuses%20on%20relevant%20associations%20and%20neglects%20irrelevant%20anatomical%0Ainformation.%20Furthermore%2C%20the%20inclusion%20of%20deep%20supervision%20incorporates%0Adifferent%20levels%20of%20features%20that%20prove%20to%20be%20beneficial%20for%20network%0Aconvergence.%20Results%3A%20We%20demonstrate%20the%20efficiency%20of%20the%20proposed%20method%20by%0Across-validating%20with%20an%20unlabeled%20dataset%2C%20which%20was%20further%20labeled%20by%20us.%20We%0Abelieve%20that%20the%20novelty%20of%20this%20algorithm%20lies%20in%20its%20ability%20to%20perform%20well%0Aon%20both%20labeled%20and%20unlabeled%20data%20with%20image%20processing-based%20enhancement.%20The%0Aresults%20indicate%20that%20our%20method%20performed%20better%20than%20the%20existing%0Astate-of-the-art%20methods%20on%20the%20TubeTK%20dataset.%20Conclusion%3A%20The%20proposed%20method%0Awill%20help%20in%20accurate%20segmentation%20of%20cerebrovascular%20structure%20leading%20to%0Astroke%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.10224v2&entry.124074799=Read"},
{"title": "Event Camera Demosaicing via Swin Transformer and Pixel-focus Loss", "author": "Yunfan Lu and Yijie Xu and Wenzong Ma and Weiyu Guo and Hui Xiong", "abstract": "  Recent research has highlighted improvements in high-quality imaging guided\nby event cameras, with most of these efforts concentrating on the RGB domain.\nHowever, these advancements frequently neglect the unique challenges introduced\nby the inherent flaws in the sensor design of event cameras in the RAW domain.\nSpecifically, this sensor design results in the partial loss of pixel values,\nposing new challenges for RAW domain processes like demosaicing. The challenge\nintensifies as most research in the RAW domain is based on the premise that\neach pixel contains a value, making the straightforward adaptation of these\nmethods to event camera demosaicing problematic. To end this, we present a\nSwin-Transformer-based backbone and a pixel-focus loss function for demosaicing\nwith missing pixel values in RAW domain processing. Our core motivation is to\nrefine a general and widely applicable foundational model from the RGB domain\nfor RAW domain processing, thereby broadening the model's applicability within\nthe entire imaging process. Our method harnesses multi-scale processing and\nspace-to-depth techniques to ensure efficiency and reduce computing complexity.\nWe also proposed the Pixel-focus Loss function for network fine-tuning to\nimprove network convergence based on our discovery of a long-tailed\ndistribution in training loss. Our method has undergone validation on the MIPI\nDemosaic Challenge dataset, with subsequent analytical experimentation\nconfirming its efficacy. All code and trained models are released here:\nhttps://github.com/yunfanLu/ev-demosaic\n", "link": "http://arxiv.org/abs/2404.02731v1", "date": "2024-04-03", "relevancy": 2.1468, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5861}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5303}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5234}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Event%20Camera%20Demosaicing%20via%20Swin%20Transformer%20and%20Pixel-focus%20Loss&body=Title%3A%20Event%20Camera%20Demosaicing%20via%20Swin%20Transformer%20and%20Pixel-focus%20Loss%0AAuthor%3A%20Yunfan%20Lu%20and%20Yijie%20Xu%20and%20Wenzong%20Ma%20and%20Weiyu%20Guo%20and%20Hui%20Xiong%0AAbstract%3A%20%20%20Recent%20research%20has%20highlighted%20improvements%20in%20high-quality%20imaging%20guided%0Aby%20event%20cameras%2C%20with%20most%20of%20these%20efforts%20concentrating%20on%20the%20RGB%20domain.%0AHowever%2C%20these%20advancements%20frequently%20neglect%20the%20unique%20challenges%20introduced%0Aby%20the%20inherent%20flaws%20in%20the%20sensor%20design%20of%20event%20cameras%20in%20the%20RAW%20domain.%0ASpecifically%2C%20this%20sensor%20design%20results%20in%20the%20partial%20loss%20of%20pixel%20values%2C%0Aposing%20new%20challenges%20for%20RAW%20domain%20processes%20like%20demosaicing.%20The%20challenge%0Aintensifies%20as%20most%20research%20in%20the%20RAW%20domain%20is%20based%20on%20the%20premise%20that%0Aeach%20pixel%20contains%20a%20value%2C%20making%20the%20straightforward%20adaptation%20of%20these%0Amethods%20to%20event%20camera%20demosaicing%20problematic.%20To%20end%20this%2C%20we%20present%20a%0ASwin-Transformer-based%20backbone%20and%20a%20pixel-focus%20loss%20function%20for%20demosaicing%0Awith%20missing%20pixel%20values%20in%20RAW%20domain%20processing.%20Our%20core%20motivation%20is%20to%0Arefine%20a%20general%20and%20widely%20applicable%20foundational%20model%20from%20the%20RGB%20domain%0Afor%20RAW%20domain%20processing%2C%20thereby%20broadening%20the%20model%27s%20applicability%20within%0Athe%20entire%20imaging%20process.%20Our%20method%20harnesses%20multi-scale%20processing%20and%0Aspace-to-depth%20techniques%20to%20ensure%20efficiency%20and%20reduce%20computing%20complexity.%0AWe%20also%20proposed%20the%20Pixel-focus%20Loss%20function%20for%20network%20fine-tuning%20to%0Aimprove%20network%20convergence%20based%20on%20our%20discovery%20of%20a%20long-tailed%0Adistribution%20in%20training%20loss.%20Our%20method%20has%20undergone%20validation%20on%20the%20MIPI%0ADemosaic%20Challenge%20dataset%2C%20with%20subsequent%20analytical%20experimentation%0Aconfirming%20its%20efficacy.%20All%20code%20and%20trained%20models%20are%20released%20here%3A%0Ahttps%3A//github.com/yunfanLu/ev-demosaic%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02731v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Event%20Camera%20Demosaicing%20via%20Swin%20Transformer%20and%20Pixel-focus%20Loss&entry.906535625=Yunfan%20Lu%20and%20Yijie%20Xu%20and%20Wenzong%20Ma%20and%20Weiyu%20Guo%20and%20Hui%20Xiong&entry.1292438233=%20%20Recent%20research%20has%20highlighted%20improvements%20in%20high-quality%20imaging%20guided%0Aby%20event%20cameras%2C%20with%20most%20of%20these%20efforts%20concentrating%20on%20the%20RGB%20domain.%0AHowever%2C%20these%20advancements%20frequently%20neglect%20the%20unique%20challenges%20introduced%0Aby%20the%20inherent%20flaws%20in%20the%20sensor%20design%20of%20event%20cameras%20in%20the%20RAW%20domain.%0ASpecifically%2C%20this%20sensor%20design%20results%20in%20the%20partial%20loss%20of%20pixel%20values%2C%0Aposing%20new%20challenges%20for%20RAW%20domain%20processes%20like%20demosaicing.%20The%20challenge%0Aintensifies%20as%20most%20research%20in%20the%20RAW%20domain%20is%20based%20on%20the%20premise%20that%0Aeach%20pixel%20contains%20a%20value%2C%20making%20the%20straightforward%20adaptation%20of%20these%0Amethods%20to%20event%20camera%20demosaicing%20problematic.%20To%20end%20this%2C%20we%20present%20a%0ASwin-Transformer-based%20backbone%20and%20a%20pixel-focus%20loss%20function%20for%20demosaicing%0Awith%20missing%20pixel%20values%20in%20RAW%20domain%20processing.%20Our%20core%20motivation%20is%20to%0Arefine%20a%20general%20and%20widely%20applicable%20foundational%20model%20from%20the%20RGB%20domain%0Afor%20RAW%20domain%20processing%2C%20thereby%20broadening%20the%20model%27s%20applicability%20within%0Athe%20entire%20imaging%20process.%20Our%20method%20harnesses%20multi-scale%20processing%20and%0Aspace-to-depth%20techniques%20to%20ensure%20efficiency%20and%20reduce%20computing%20complexity.%0AWe%20also%20proposed%20the%20Pixel-focus%20Loss%20function%20for%20network%20fine-tuning%20to%0Aimprove%20network%20convergence%20based%20on%20our%20discovery%20of%20a%20long-tailed%0Adistribution%20in%20training%20loss.%20Our%20method%20has%20undergone%20validation%20on%20the%20MIPI%0ADemosaic%20Challenge%20dataset%2C%20with%20subsequent%20analytical%20experimentation%0Aconfirming%20its%20efficacy.%20All%20code%20and%20trained%20models%20are%20released%20here%3A%0Ahttps%3A//github.com/yunfanLu/ev-demosaic%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02731v1&entry.124074799=Read"},
{"title": "SG-BEV: Satellite-Guided BEV Fusion for Cross-View Semantic Segmentation", "author": "Junyan Ye and Qiyan Luo and Jinhua Yu and Huaping Zhong and Zhimeng Zheng and Conghui He and Weijia Li", "abstract": "  This paper aims at achieving fine-grained building attribute segmentation in\na cross-view scenario, i.e., using satellite and street-view image pairs. The\nmain challenge lies in overcoming the significant perspective differences\nbetween street views and satellite views. In this work, we introduce SG-BEV, a\nnovel approach for satellite-guided BEV fusion for cross-view semantic\nsegmentation. To overcome the limitations of existing cross-view projection\nmethods in capturing the complete building facade features, we innovatively\nincorporate Bird's Eye View (BEV) method to establish a spatially explicit\nmapping of street-view features. Moreover, we fully leverage the advantages of\nmultiple perspectives by introducing a novel satellite-guided reprojection\nmodule, optimizing the uneven feature distribution issues associated with\ntraditional BEV methods. Our method demonstrates significant improvements on\nfour cross-view datasets collected from multiple cities, including New York,\nSan Francisco, and Boston. On average across these datasets, our method\nachieves an increase in mIOU by 10.13% and 5.21% compared with the\nstate-of-the-art satellite-based and cross-view methods. The code and datasets\nof this work will be released at https://github.com/yejy53/SG-BEV.\n", "link": "http://arxiv.org/abs/2404.02638v1", "date": "2024-04-03", "relevancy": 2.1384, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5708}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5494}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5053}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SG-BEV%3A%20Satellite-Guided%20BEV%20Fusion%20for%20Cross-View%20Semantic%20Segmentation&body=Title%3A%20SG-BEV%3A%20Satellite-Guided%20BEV%20Fusion%20for%20Cross-View%20Semantic%20Segmentation%0AAuthor%3A%20Junyan%20Ye%20and%20Qiyan%20Luo%20and%20Jinhua%20Yu%20and%20Huaping%20Zhong%20and%20Zhimeng%20Zheng%20and%20Conghui%20He%20and%20Weijia%20Li%0AAbstract%3A%20%20%20This%20paper%20aims%20at%20achieving%20fine-grained%20building%20attribute%20segmentation%20in%0Aa%20cross-view%20scenario%2C%20i.e.%2C%20using%20satellite%20and%20street-view%20image%20pairs.%20The%0Amain%20challenge%20lies%20in%20overcoming%20the%20significant%20perspective%20differences%0Abetween%20street%20views%20and%20satellite%20views.%20In%20this%20work%2C%20we%20introduce%20SG-BEV%2C%20a%0Anovel%20approach%20for%20satellite-guided%20BEV%20fusion%20for%20cross-view%20semantic%0Asegmentation.%20To%20overcome%20the%20limitations%20of%20existing%20cross-view%20projection%0Amethods%20in%20capturing%20the%20complete%20building%20facade%20features%2C%20we%20innovatively%0Aincorporate%20Bird%27s%20Eye%20View%20%28BEV%29%20method%20to%20establish%20a%20spatially%20explicit%0Amapping%20of%20street-view%20features.%20Moreover%2C%20we%20fully%20leverage%20the%20advantages%20of%0Amultiple%20perspectives%20by%20introducing%20a%20novel%20satellite-guided%20reprojection%0Amodule%2C%20optimizing%20the%20uneven%20feature%20distribution%20issues%20associated%20with%0Atraditional%20BEV%20methods.%20Our%20method%20demonstrates%20significant%20improvements%20on%0Afour%20cross-view%20datasets%20collected%20from%20multiple%20cities%2C%20including%20New%20York%2C%0ASan%20Francisco%2C%20and%20Boston.%20On%20average%20across%20these%20datasets%2C%20our%20method%0Aachieves%20an%20increase%20in%20mIOU%20by%2010.13%25%20and%205.21%25%20compared%20with%20the%0Astate-of-the-art%20satellite-based%20and%20cross-view%20methods.%20The%20code%20and%20datasets%0Aof%20this%20work%20will%20be%20released%20at%20https%3A//github.com/yejy53/SG-BEV.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02638v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SG-BEV%3A%20Satellite-Guided%20BEV%20Fusion%20for%20Cross-View%20Semantic%20Segmentation&entry.906535625=Junyan%20Ye%20and%20Qiyan%20Luo%20and%20Jinhua%20Yu%20and%20Huaping%20Zhong%20and%20Zhimeng%20Zheng%20and%20Conghui%20He%20and%20Weijia%20Li&entry.1292438233=%20%20This%20paper%20aims%20at%20achieving%20fine-grained%20building%20attribute%20segmentation%20in%0Aa%20cross-view%20scenario%2C%20i.e.%2C%20using%20satellite%20and%20street-view%20image%20pairs.%20The%0Amain%20challenge%20lies%20in%20overcoming%20the%20significant%20perspective%20differences%0Abetween%20street%20views%20and%20satellite%20views.%20In%20this%20work%2C%20we%20introduce%20SG-BEV%2C%20a%0Anovel%20approach%20for%20satellite-guided%20BEV%20fusion%20for%20cross-view%20semantic%0Asegmentation.%20To%20overcome%20the%20limitations%20of%20existing%20cross-view%20projection%0Amethods%20in%20capturing%20the%20complete%20building%20facade%20features%2C%20we%20innovatively%0Aincorporate%20Bird%27s%20Eye%20View%20%28BEV%29%20method%20to%20establish%20a%20spatially%20explicit%0Amapping%20of%20street-view%20features.%20Moreover%2C%20we%20fully%20leverage%20the%20advantages%20of%0Amultiple%20perspectives%20by%20introducing%20a%20novel%20satellite-guided%20reprojection%0Amodule%2C%20optimizing%20the%20uneven%20feature%20distribution%20issues%20associated%20with%0Atraditional%20BEV%20methods.%20Our%20method%20demonstrates%20significant%20improvements%20on%0Afour%20cross-view%20datasets%20collected%20from%20multiple%20cities%2C%20including%20New%20York%2C%0ASan%20Francisco%2C%20and%20Boston.%20On%20average%20across%20these%20datasets%2C%20our%20method%0Aachieves%20an%20increase%20in%20mIOU%20by%2010.13%25%20and%205.21%25%20compared%20with%20the%0Astate-of-the-art%20satellite-based%20and%20cross-view%20methods.%20The%20code%20and%20datasets%0Aof%20this%20work%20will%20be%20released%20at%20https%3A//github.com/yejy53/SG-BEV.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02638v1&entry.124074799=Read"},
{"title": "Dynamic LiDAR Re-simulation using Compositional Neural Fields", "author": "Hanfeng Wu and Xingxing Zuo and Stefan Leutenegger and Or Litany and Konrad Schindler and Shengyu Huang", "abstract": "  We introduce DyNFL, a novel neural field-based approach for high-fidelity\nre-simulation of LiDAR scans in dynamic driving scenes. DyNFL processes LiDAR\nmeasurements from dynamic environments, accompanied by bounding boxes of moving\nobjects, to construct an editable neural field. This field, comprising\nseparately reconstructed static background and dynamic objects, allows users to\nmodify viewpoints, adjust object positions, and seamlessly add or remove\nobjects in the re-simulated scene. A key innovation of our method is the neural\nfield composition technique, which effectively integrates reconstructed neural\nassets from various scenes through a ray drop test, accounting for occlusions\nand transparent surfaces. Our evaluation with both synthetic and real-world\nenvironments demonstrates that DyNFL substantially improves dynamic scene LiDAR\nsimulation, offering a combination of physical fidelity and flexible editing\ncapabilities.\n", "link": "http://arxiv.org/abs/2312.05247v2", "date": "2024-04-03", "relevancy": 2.1269, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5561}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.518}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5129}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Dynamic%20LiDAR%20Re-simulation%20using%20Compositional%20Neural%20Fields&body=Title%3A%20Dynamic%20LiDAR%20Re-simulation%20using%20Compositional%20Neural%20Fields%0AAuthor%3A%20Hanfeng%20Wu%20and%20Xingxing%20Zuo%20and%20Stefan%20Leutenegger%20and%20Or%20Litany%20and%20Konrad%20Schindler%20and%20Shengyu%20Huang%0AAbstract%3A%20%20%20We%20introduce%20DyNFL%2C%20a%20novel%20neural%20field-based%20approach%20for%20high-fidelity%0Are-simulation%20of%20LiDAR%20scans%20in%20dynamic%20driving%20scenes.%20DyNFL%20processes%20LiDAR%0Ameasurements%20from%20dynamic%20environments%2C%20accompanied%20by%20bounding%20boxes%20of%20moving%0Aobjects%2C%20to%20construct%20an%20editable%20neural%20field.%20This%20field%2C%20comprising%0Aseparately%20reconstructed%20static%20background%20and%20dynamic%20objects%2C%20allows%20users%20to%0Amodify%20viewpoints%2C%20adjust%20object%20positions%2C%20and%20seamlessly%20add%20or%20remove%0Aobjects%20in%20the%20re-simulated%20scene.%20A%20key%20innovation%20of%20our%20method%20is%20the%20neural%0Afield%20composition%20technique%2C%20which%20effectively%20integrates%20reconstructed%20neural%0Aassets%20from%20various%20scenes%20through%20a%20ray%20drop%20test%2C%20accounting%20for%20occlusions%0Aand%20transparent%20surfaces.%20Our%20evaluation%20with%20both%20synthetic%20and%20real-world%0Aenvironments%20demonstrates%20that%20DyNFL%20substantially%20improves%20dynamic%20scene%20LiDAR%0Asimulation%2C%20offering%20a%20combination%20of%20physical%20fidelity%20and%20flexible%20editing%0Acapabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.05247v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20LiDAR%20Re-simulation%20using%20Compositional%20Neural%20Fields&entry.906535625=Hanfeng%20Wu%20and%20Xingxing%20Zuo%20and%20Stefan%20Leutenegger%20and%20Or%20Litany%20and%20Konrad%20Schindler%20and%20Shengyu%20Huang&entry.1292438233=%20%20We%20introduce%20DyNFL%2C%20a%20novel%20neural%20field-based%20approach%20for%20high-fidelity%0Are-simulation%20of%20LiDAR%20scans%20in%20dynamic%20driving%20scenes.%20DyNFL%20processes%20LiDAR%0Ameasurements%20from%20dynamic%20environments%2C%20accompanied%20by%20bounding%20boxes%20of%20moving%0Aobjects%2C%20to%20construct%20an%20editable%20neural%20field.%20This%20field%2C%20comprising%0Aseparately%20reconstructed%20static%20background%20and%20dynamic%20objects%2C%20allows%20users%20to%0Amodify%20viewpoints%2C%20adjust%20object%20positions%2C%20and%20seamlessly%20add%20or%20remove%0Aobjects%20in%20the%20re-simulated%20scene.%20A%20key%20innovation%20of%20our%20method%20is%20the%20neural%0Afield%20composition%20technique%2C%20which%20effectively%20integrates%20reconstructed%20neural%0Aassets%20from%20various%20scenes%20through%20a%20ray%20drop%20test%2C%20accounting%20for%20occlusions%0Aand%20transparent%20surfaces.%20Our%20evaluation%20with%20both%20synthetic%20and%20real-world%0Aenvironments%20demonstrates%20that%20DyNFL%20substantially%20improves%20dynamic%20scene%20LiDAR%0Asimulation%2C%20offering%20a%20combination%20of%20physical%20fidelity%20and%20flexible%20editing%0Acapabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.05247v2&entry.124074799=Read"},
{"title": "Creating Ensembles of Classifiers through UMDA for Aerial Scene\n  Classification", "author": "Fabio A. Faria and Luiz H. Buris and Luis A. M. Pereira and F\u00e1bio A. M. Cappabianco", "abstract": "  Aerial scene classification, which aims to semantically label remote sensing\nimages in a set of predefined classes (e.g., agricultural, beach, and harbor),\nis a very challenging task in remote sensing due to high intra-class\nvariability and the different scales and orientations of the objects present in\nthe dataset images. In remote sensing area, the use of CNN architectures as an\nalternative solution is also a reality for scene classification tasks.\nGenerally, these CNNs are used to perform the traditional image classification\ntask. However, another less used way to classify remote sensing image might be\nthe one that uses deep metric learning (DML) approaches. In this sense, this\nwork proposes to employ six DML approaches for aerial scene classification\ntasks, analysing their behave with four different pre-trained CNNs as well as\ncombining them through the use of evolutionary computation algorithm (UMDA). In\nperformed experiments, it is possible to observe than DML approaches can\nachieve the best classification results when compared to traditional\npre-trained CNNs for three well-known remote sensing aerial scene datasets. In\naddition, the UMDA algorithm proved to be a promising strategy to combine DML\napproaches when there is diversity among them, managing to improve at least\n5.6% of accuracy in the classification results using almost 50\\% of the\navailable classifiers for the construction of the final ensemble of\nclassifiers.\n", "link": "http://arxiv.org/abs/2303.11389v2", "date": "2024-04-03", "relevancy": 2.126, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5579}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5348}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5177}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Creating%20Ensembles%20of%20Classifiers%20through%20UMDA%20for%20Aerial%20Scene%0A%20%20Classification&body=Title%3A%20Creating%20Ensembles%20of%20Classifiers%20through%20UMDA%20for%20Aerial%20Scene%0A%20%20Classification%0AAuthor%3A%20Fabio%20A.%20Faria%20and%20Luiz%20H.%20Buris%20and%20Luis%20A.%20M.%20Pereira%20and%20F%C3%A1bio%20A.%20M.%20Cappabianco%0AAbstract%3A%20%20%20Aerial%20scene%20classification%2C%20which%20aims%20to%20semantically%20label%20remote%20sensing%0Aimages%20in%20a%20set%20of%20predefined%20classes%20%28e.g.%2C%20agricultural%2C%20beach%2C%20and%20harbor%29%2C%0Ais%20a%20very%20challenging%20task%20in%20remote%20sensing%20due%20to%20high%20intra-class%0Avariability%20and%20the%20different%20scales%20and%20orientations%20of%20the%20objects%20present%20in%0Athe%20dataset%20images.%20In%20remote%20sensing%20area%2C%20the%20use%20of%20CNN%20architectures%20as%20an%0Aalternative%20solution%20is%20also%20a%20reality%20for%20scene%20classification%20tasks.%0AGenerally%2C%20these%20CNNs%20are%20used%20to%20perform%20the%20traditional%20image%20classification%0Atask.%20However%2C%20another%20less%20used%20way%20to%20classify%20remote%20sensing%20image%20might%20be%0Athe%20one%20that%20uses%20deep%20metric%20learning%20%28DML%29%20approaches.%20In%20this%20sense%2C%20this%0Awork%20proposes%20to%20employ%20six%20DML%20approaches%20for%20aerial%20scene%20classification%0Atasks%2C%20analysing%20their%20behave%20with%20four%20different%20pre-trained%20CNNs%20as%20well%20as%0Acombining%20them%20through%20the%20use%20of%20evolutionary%20computation%20algorithm%20%28UMDA%29.%20In%0Aperformed%20experiments%2C%20it%20is%20possible%20to%20observe%20than%20DML%20approaches%20can%0Aachieve%20the%20best%20classification%20results%20when%20compared%20to%20traditional%0Apre-trained%20CNNs%20for%20three%20well-known%20remote%20sensing%20aerial%20scene%20datasets.%20In%0Aaddition%2C%20the%20UMDA%20algorithm%20proved%20to%20be%20a%20promising%20strategy%20to%20combine%20DML%0Aapproaches%20when%20there%20is%20diversity%20among%20them%2C%20managing%20to%20improve%20at%20least%0A5.6%25%20of%20accuracy%20in%20the%20classification%20results%20using%20almost%2050%5C%25%20of%20the%0Aavailable%20classifiers%20for%20the%20construction%20of%20the%20final%20ensemble%20of%0Aclassifiers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.11389v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Creating%20Ensembles%20of%20Classifiers%20through%20UMDA%20for%20Aerial%20Scene%0A%20%20Classification&entry.906535625=Fabio%20A.%20Faria%20and%20Luiz%20H.%20Buris%20and%20Luis%20A.%20M.%20Pereira%20and%20F%C3%A1bio%20A.%20M.%20Cappabianco&entry.1292438233=%20%20Aerial%20scene%20classification%2C%20which%20aims%20to%20semantically%20label%20remote%20sensing%0Aimages%20in%20a%20set%20of%20predefined%20classes%20%28e.g.%2C%20agricultural%2C%20beach%2C%20and%20harbor%29%2C%0Ais%20a%20very%20challenging%20task%20in%20remote%20sensing%20due%20to%20high%20intra-class%0Avariability%20and%20the%20different%20scales%20and%20orientations%20of%20the%20objects%20present%20in%0Athe%20dataset%20images.%20In%20remote%20sensing%20area%2C%20the%20use%20of%20CNN%20architectures%20as%20an%0Aalternative%20solution%20is%20also%20a%20reality%20for%20scene%20classification%20tasks.%0AGenerally%2C%20these%20CNNs%20are%20used%20to%20perform%20the%20traditional%20image%20classification%0Atask.%20However%2C%20another%20less%20used%20way%20to%20classify%20remote%20sensing%20image%20might%20be%0Athe%20one%20that%20uses%20deep%20metric%20learning%20%28DML%29%20approaches.%20In%20this%20sense%2C%20this%0Awork%20proposes%20to%20employ%20six%20DML%20approaches%20for%20aerial%20scene%20classification%0Atasks%2C%20analysing%20their%20behave%20with%20four%20different%20pre-trained%20CNNs%20as%20well%20as%0Acombining%20them%20through%20the%20use%20of%20evolutionary%20computation%20algorithm%20%28UMDA%29.%20In%0Aperformed%20experiments%2C%20it%20is%20possible%20to%20observe%20than%20DML%20approaches%20can%0Aachieve%20the%20best%20classification%20results%20when%20compared%20to%20traditional%0Apre-trained%20CNNs%20for%20three%20well-known%20remote%20sensing%20aerial%20scene%20datasets.%20In%0Aaddition%2C%20the%20UMDA%20algorithm%20proved%20to%20be%20a%20promising%20strategy%20to%20combine%20DML%0Aapproaches%20when%20there%20is%20diversity%20among%20them%2C%20managing%20to%20improve%20at%20least%0A5.6%25%20of%20accuracy%20in%20the%20classification%20results%20using%20almost%2050%5C%25%20of%20the%0Aavailable%20classifiers%20for%20the%20construction%20of%20the%20final%20ensemble%20of%0Aclassifiers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.11389v2&entry.124074799=Read"},
{"title": "Leveraging Swarm Intelligence to Drive Autonomously: A Particle Swarm\n  Optimization based Approach to Motion Planning", "author": "Sven Ochs and Jens Doll and Marc Heinrich and Philip Sch\u00f6rner and Sebastian Klemm and Marc Ren\u00e9 Zofka and J. Marius Z\u00f6llner", "abstract": "  Motion planning is an essential part of autonomous mobile platforms. A good\npipeline should be modular enough to handle different vehicles, environments,\nand perception modules. The planning process has to cope with all the different\nmodalities and has to have a modular and flexible design. But most importantly,\nit has to be safe and robust. In this paper, we want to present our motion\nplanning pipeline with particle swarm optimization (PSO) at its core. This\nsolution is independent of the vehicle type and has a clear and\nsimple-to-implement interface for perception modules. Moreover, the approach\nstands out for being easily adaptable to new scenarios. Parallel calculation\nallows for fast planning cycles. Following the principles of PSO, the\ntrajectory planer first generates a swarm of initial trajectories that are\noptimized afterward. We present the underlying control space and inner\nworkings. Finally, the application to real-world automated driving is shown in\nthe evaluation with a deeper look at the modeling of the cost function. The\napproach is used in our automated shuttles that have already driven more than\n3.500 km safely and entirely autonomously in sub-urban everyday traffic.\n", "link": "http://arxiv.org/abs/2404.02644v1", "date": "2024-04-03", "relevancy": 2.1251, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5399}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5318}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5225}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Swarm%20Intelligence%20to%20Drive%20Autonomously%3A%20A%20Particle%20Swarm%0A%20%20Optimization%20based%20Approach%20to%20Motion%20Planning&body=Title%3A%20Leveraging%20Swarm%20Intelligence%20to%20Drive%20Autonomously%3A%20A%20Particle%20Swarm%0A%20%20Optimization%20based%20Approach%20to%20Motion%20Planning%0AAuthor%3A%20Sven%20Ochs%20and%20Jens%20Doll%20and%20Marc%20Heinrich%20and%20Philip%20Sch%C3%B6rner%20and%20Sebastian%20Klemm%20and%20Marc%20Ren%C3%A9%20Zofka%20and%20J.%20Marius%20Z%C3%B6llner%0AAbstract%3A%20%20%20Motion%20planning%20is%20an%20essential%20part%20of%20autonomous%20mobile%20platforms.%20A%20good%0Apipeline%20should%20be%20modular%20enough%20to%20handle%20different%20vehicles%2C%20environments%2C%0Aand%20perception%20modules.%20The%20planning%20process%20has%20to%20cope%20with%20all%20the%20different%0Amodalities%20and%20has%20to%20have%20a%20modular%20and%20flexible%20design.%20But%20most%20importantly%2C%0Ait%20has%20to%20be%20safe%20and%20robust.%20In%20this%20paper%2C%20we%20want%20to%20present%20our%20motion%0Aplanning%20pipeline%20with%20particle%20swarm%20optimization%20%28PSO%29%20at%20its%20core.%20This%0Asolution%20is%20independent%20of%20the%20vehicle%20type%20and%20has%20a%20clear%20and%0Asimple-to-implement%20interface%20for%20perception%20modules.%20Moreover%2C%20the%20approach%0Astands%20out%20for%20being%20easily%20adaptable%20to%20new%20scenarios.%20Parallel%20calculation%0Aallows%20for%20fast%20planning%20cycles.%20Following%20the%20principles%20of%20PSO%2C%20the%0Atrajectory%20planer%20first%20generates%20a%20swarm%20of%20initial%20trajectories%20that%20are%0Aoptimized%20afterward.%20We%20present%20the%20underlying%20control%20space%20and%20inner%0Aworkings.%20Finally%2C%20the%20application%20to%20real-world%20automated%20driving%20is%20shown%20in%0Athe%20evaluation%20with%20a%20deeper%20look%20at%20the%20modeling%20of%20the%20cost%20function.%20The%0Aapproach%20is%20used%20in%20our%20automated%20shuttles%20that%20have%20already%20driven%20more%20than%0A3.500%20km%20safely%20and%20entirely%20autonomously%20in%20sub-urban%20everyday%20traffic.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02644v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Swarm%20Intelligence%20to%20Drive%20Autonomously%3A%20A%20Particle%20Swarm%0A%20%20Optimization%20based%20Approach%20to%20Motion%20Planning&entry.906535625=Sven%20Ochs%20and%20Jens%20Doll%20and%20Marc%20Heinrich%20and%20Philip%20Sch%C3%B6rner%20and%20Sebastian%20Klemm%20and%20Marc%20Ren%C3%A9%20Zofka%20and%20J.%20Marius%20Z%C3%B6llner&entry.1292438233=%20%20Motion%20planning%20is%20an%20essential%20part%20of%20autonomous%20mobile%20platforms.%20A%20good%0Apipeline%20should%20be%20modular%20enough%20to%20handle%20different%20vehicles%2C%20environments%2C%0Aand%20perception%20modules.%20The%20planning%20process%20has%20to%20cope%20with%20all%20the%20different%0Amodalities%20and%20has%20to%20have%20a%20modular%20and%20flexible%20design.%20But%20most%20importantly%2C%0Ait%20has%20to%20be%20safe%20and%20robust.%20In%20this%20paper%2C%20we%20want%20to%20present%20our%20motion%0Aplanning%20pipeline%20with%20particle%20swarm%20optimization%20%28PSO%29%20at%20its%20core.%20This%0Asolution%20is%20independent%20of%20the%20vehicle%20type%20and%20has%20a%20clear%20and%0Asimple-to-implement%20interface%20for%20perception%20modules.%20Moreover%2C%20the%20approach%0Astands%20out%20for%20being%20easily%20adaptable%20to%20new%20scenarios.%20Parallel%20calculation%0Aallows%20for%20fast%20planning%20cycles.%20Following%20the%20principles%20of%20PSO%2C%20the%0Atrajectory%20planer%20first%20generates%20a%20swarm%20of%20initial%20trajectories%20that%20are%0Aoptimized%20afterward.%20We%20present%20the%20underlying%20control%20space%20and%20inner%0Aworkings.%20Finally%2C%20the%20application%20to%20real-world%20automated%20driving%20is%20shown%20in%0Athe%20evaluation%20with%20a%20deeper%20look%20at%20the%20modeling%20of%20the%20cost%20function.%20The%0Aapproach%20is%20used%20in%20our%20automated%20shuttles%20that%20have%20already%20driven%20more%20than%0A3.500%20km%20safely%20and%20entirely%20autonomously%20in%20sub-urban%20everyday%20traffic.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02644v1&entry.124074799=Read"},
{"title": "FreeZe: Training-free zero-shot 6D pose estimation with geometric and\n  vision foundation models", "author": "Andrea Caraffa and Davide Boscaini and Amir Hamza and Fabio Poiesi", "abstract": "  Estimating the 6D pose of objects unseen during training is highly desirable\nyet challenging. Zero-shot object 6D pose estimation methods address this\nchallenge by leveraging additional task-specific supervision provided by\nlarge-scale, photo-realistic synthetic datasets. However, their performance\nheavily depends on the quality and diversity of rendered data and they require\nextensive training. In this work, we show how to tackle the same task but\nwithout training on specific data. We propose FreeZe, a novel solution that\nharnesses the capabilities of pre-trained geometric and vision foundation\nmodels. FreeZe leverages 3D geometric descriptors learned from unrelated 3D\npoint clouds and 2D visual features learned from web-scale 2D images to\ngenerate discriminative 3D point-level descriptors. We then estimate the 6D\npose of unseen objects by 3D registration based on RANSAC. We also introduce a\nnovel algorithm to solve ambiguous cases due to geometrically symmetric objects\nthat is based on visual features. We comprehensively evaluate FreeZe across the\nseven core datasets of the BOP Benchmark, which include over a hundred 3D\nobjects and 20,000 images captured in various scenarios. FreeZe consistently\noutperforms all state-of-the-art approaches, including competitors extensively\ntrained on synthetic 6D pose estimation data. Code will be publicly available\nat https://andreacaraffa.github.io/freeze.\n", "link": "http://arxiv.org/abs/2312.00947v2", "date": "2024-04-03", "relevancy": 2.1237, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5425}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5233}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.521}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FreeZe%3A%20Training-free%20zero-shot%206D%20pose%20estimation%20with%20geometric%20and%0A%20%20vision%20foundation%20models&body=Title%3A%20FreeZe%3A%20Training-free%20zero-shot%206D%20pose%20estimation%20with%20geometric%20and%0A%20%20vision%20foundation%20models%0AAuthor%3A%20Andrea%20Caraffa%20and%20Davide%20Boscaini%20and%20Amir%20Hamza%20and%20Fabio%20Poiesi%0AAbstract%3A%20%20%20Estimating%20the%206D%20pose%20of%20objects%20unseen%20during%20training%20is%20highly%20desirable%0Ayet%20challenging.%20Zero-shot%20object%206D%20pose%20estimation%20methods%20address%20this%0Achallenge%20by%20leveraging%20additional%20task-specific%20supervision%20provided%20by%0Alarge-scale%2C%20photo-realistic%20synthetic%20datasets.%20However%2C%20their%20performance%0Aheavily%20depends%20on%20the%20quality%20and%20diversity%20of%20rendered%20data%20and%20they%20require%0Aextensive%20training.%20In%20this%20work%2C%20we%20show%20how%20to%20tackle%20the%20same%20task%20but%0Awithout%20training%20on%20specific%20data.%20We%20propose%20FreeZe%2C%20a%20novel%20solution%20that%0Aharnesses%20the%20capabilities%20of%20pre-trained%20geometric%20and%20vision%20foundation%0Amodels.%20FreeZe%20leverages%203D%20geometric%20descriptors%20learned%20from%20unrelated%203D%0Apoint%20clouds%20and%202D%20visual%20features%20learned%20from%20web-scale%202D%20images%20to%0Agenerate%20discriminative%203D%20point-level%20descriptors.%20We%20then%20estimate%20the%206D%0Apose%20of%20unseen%20objects%20by%203D%20registration%20based%20on%20RANSAC.%20We%20also%20introduce%20a%0Anovel%20algorithm%20to%20solve%20ambiguous%20cases%20due%20to%20geometrically%20symmetric%20objects%0Athat%20is%20based%20on%20visual%20features.%20We%20comprehensively%20evaluate%20FreeZe%20across%20the%0Aseven%20core%20datasets%20of%20the%20BOP%20Benchmark%2C%20which%20include%20over%20a%20hundred%203D%0Aobjects%20and%2020%2C000%20images%20captured%20in%20various%20scenarios.%20FreeZe%20consistently%0Aoutperforms%20all%20state-of-the-art%20approaches%2C%20including%20competitors%20extensively%0Atrained%20on%20synthetic%206D%20pose%20estimation%20data.%20Code%20will%20be%20publicly%20available%0Aat%20https%3A//andreacaraffa.github.io/freeze.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00947v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FreeZe%3A%20Training-free%20zero-shot%206D%20pose%20estimation%20with%20geometric%20and%0A%20%20vision%20foundation%20models&entry.906535625=Andrea%20Caraffa%20and%20Davide%20Boscaini%20and%20Amir%20Hamza%20and%20Fabio%20Poiesi&entry.1292438233=%20%20Estimating%20the%206D%20pose%20of%20objects%20unseen%20during%20training%20is%20highly%20desirable%0Ayet%20challenging.%20Zero-shot%20object%206D%20pose%20estimation%20methods%20address%20this%0Achallenge%20by%20leveraging%20additional%20task-specific%20supervision%20provided%20by%0Alarge-scale%2C%20photo-realistic%20synthetic%20datasets.%20However%2C%20their%20performance%0Aheavily%20depends%20on%20the%20quality%20and%20diversity%20of%20rendered%20data%20and%20they%20require%0Aextensive%20training.%20In%20this%20work%2C%20we%20show%20how%20to%20tackle%20the%20same%20task%20but%0Awithout%20training%20on%20specific%20data.%20We%20propose%20FreeZe%2C%20a%20novel%20solution%20that%0Aharnesses%20the%20capabilities%20of%20pre-trained%20geometric%20and%20vision%20foundation%0Amodels.%20FreeZe%20leverages%203D%20geometric%20descriptors%20learned%20from%20unrelated%203D%0Apoint%20clouds%20and%202D%20visual%20features%20learned%20from%20web-scale%202D%20images%20to%0Agenerate%20discriminative%203D%20point-level%20descriptors.%20We%20then%20estimate%20the%206D%0Apose%20of%20unseen%20objects%20by%203D%20registration%20based%20on%20RANSAC.%20We%20also%20introduce%20a%0Anovel%20algorithm%20to%20solve%20ambiguous%20cases%20due%20to%20geometrically%20symmetric%20objects%0Athat%20is%20based%20on%20visual%20features.%20We%20comprehensively%20evaluate%20FreeZe%20across%20the%0Aseven%20core%20datasets%20of%20the%20BOP%20Benchmark%2C%20which%20include%20over%20a%20hundred%203D%0Aobjects%20and%2020%2C000%20images%20captured%20in%20various%20scenarios.%20FreeZe%20consistently%0Aoutperforms%20all%20state-of-the-art%20approaches%2C%20including%20competitors%20extensively%0Atrained%20on%20synthetic%206D%20pose%20estimation%20data.%20Code%20will%20be%20publicly%20available%0Aat%20https%3A//andreacaraffa.github.io/freeze.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00947v2&entry.124074799=Read"},
{"title": "PoCo: Point Context Cluster for RGBD Indoor Place Recognition", "author": "Jing Liang and Zhuo Deng and Zheming Zhou and Omid Ghasemalizadeh and Dinesh Manocha and Min Sun and Cheng-Hao Kuo and Arnie Sen", "abstract": "  We present a novel end-to-end algorithm (PoCo) for the indoor RGB-D place\nrecognition task, aimed at identifying the most likely match for a given query\nframe within a reference database. The task presents inherent challenges\nattributed to the constrained field of view and limited range of perception\nsensors. We propose a new network architecture, which generalizes the recent\nContext of Clusters (CoCs) to extract global descriptors directly from the\nnoisy point clouds through end-to-end learning. Moreover, we develop the\narchitecture by integrating both color and geometric modalities into the point\nfeatures to enhance the global descriptor representation. We conducted\nevaluations on public datasets ScanNet-PR and ARKit with 807 and 5047\nscenarios, respectively. PoCo achieves SOTA performance: on ScanNet-PR, we\nachieve R@1 of 64.63%, a 5.7% improvement from the best-published result CGis\n(61.12%); on Arkit, we achieve R@1 of 45.12%, a 13.3% improvement from the\nbest-published result CGis (39.82%). In addition, PoCo shows higher efficiency\nthan CGis in inference time (1.75X-faster), and we demonstrate the\neffectiveness of PoCo in recognizing places within a real-world laboratory\nenvironment.\n", "link": "http://arxiv.org/abs/2404.02885v1", "date": "2024-04-03", "relevancy": 2.1232, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.539}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5385}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4909}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PoCo%3A%20Point%20Context%20Cluster%20for%20RGBD%20Indoor%20Place%20Recognition&body=Title%3A%20PoCo%3A%20Point%20Context%20Cluster%20for%20RGBD%20Indoor%20Place%20Recognition%0AAuthor%3A%20Jing%20Liang%20and%20Zhuo%20Deng%20and%20Zheming%20Zhou%20and%20Omid%20Ghasemalizadeh%20and%20Dinesh%20Manocha%20and%20Min%20Sun%20and%20Cheng-Hao%20Kuo%20and%20Arnie%20Sen%0AAbstract%3A%20%20%20We%20present%20a%20novel%20end-to-end%20algorithm%20%28PoCo%29%20for%20the%20indoor%20RGB-D%20place%0Arecognition%20task%2C%20aimed%20at%20identifying%20the%20most%20likely%20match%20for%20a%20given%20query%0Aframe%20within%20a%20reference%20database.%20The%20task%20presents%20inherent%20challenges%0Aattributed%20to%20the%20constrained%20field%20of%20view%20and%20limited%20range%20of%20perception%0Asensors.%20We%20propose%20a%20new%20network%20architecture%2C%20which%20generalizes%20the%20recent%0AContext%20of%20Clusters%20%28CoCs%29%20to%20extract%20global%20descriptors%20directly%20from%20the%0Anoisy%20point%20clouds%20through%20end-to-end%20learning.%20Moreover%2C%20we%20develop%20the%0Aarchitecture%20by%20integrating%20both%20color%20and%20geometric%20modalities%20into%20the%20point%0Afeatures%20to%20enhance%20the%20global%20descriptor%20representation.%20We%20conducted%0Aevaluations%20on%20public%20datasets%20ScanNet-PR%20and%20ARKit%20with%20807%20and%205047%0Ascenarios%2C%20respectively.%20PoCo%20achieves%20SOTA%20performance%3A%20on%20ScanNet-PR%2C%20we%0Aachieve%20R%401%20of%2064.63%25%2C%20a%205.7%25%20improvement%20from%20the%20best-published%20result%20CGis%0A%2861.12%25%29%3B%20on%20Arkit%2C%20we%20achieve%20R%401%20of%2045.12%25%2C%20a%2013.3%25%20improvement%20from%20the%0Abest-published%20result%20CGis%20%2839.82%25%29.%20In%20addition%2C%20PoCo%20shows%20higher%20efficiency%0Athan%20CGis%20in%20inference%20time%20%281.75X-faster%29%2C%20and%20we%20demonstrate%20the%0Aeffectiveness%20of%20PoCo%20in%20recognizing%20places%20within%20a%20real-world%20laboratory%0Aenvironment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02885v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PoCo%3A%20Point%20Context%20Cluster%20for%20RGBD%20Indoor%20Place%20Recognition&entry.906535625=Jing%20Liang%20and%20Zhuo%20Deng%20and%20Zheming%20Zhou%20and%20Omid%20Ghasemalizadeh%20and%20Dinesh%20Manocha%20and%20Min%20Sun%20and%20Cheng-Hao%20Kuo%20and%20Arnie%20Sen&entry.1292438233=%20%20We%20present%20a%20novel%20end-to-end%20algorithm%20%28PoCo%29%20for%20the%20indoor%20RGB-D%20place%0Arecognition%20task%2C%20aimed%20at%20identifying%20the%20most%20likely%20match%20for%20a%20given%20query%0Aframe%20within%20a%20reference%20database.%20The%20task%20presents%20inherent%20challenges%0Aattributed%20to%20the%20constrained%20field%20of%20view%20and%20limited%20range%20of%20perception%0Asensors.%20We%20propose%20a%20new%20network%20architecture%2C%20which%20generalizes%20the%20recent%0AContext%20of%20Clusters%20%28CoCs%29%20to%20extract%20global%20descriptors%20directly%20from%20the%0Anoisy%20point%20clouds%20through%20end-to-end%20learning.%20Moreover%2C%20we%20develop%20the%0Aarchitecture%20by%20integrating%20both%20color%20and%20geometric%20modalities%20into%20the%20point%0Afeatures%20to%20enhance%20the%20global%20descriptor%20representation.%20We%20conducted%0Aevaluations%20on%20public%20datasets%20ScanNet-PR%20and%20ARKit%20with%20807%20and%205047%0Ascenarios%2C%20respectively.%20PoCo%20achieves%20SOTA%20performance%3A%20on%20ScanNet-PR%2C%20we%0Aachieve%20R%401%20of%2064.63%25%2C%20a%205.7%25%20improvement%20from%20the%20best-published%20result%20CGis%0A%2861.12%25%29%3B%20on%20Arkit%2C%20we%20achieve%20R%401%20of%2045.12%25%2C%20a%2013.3%25%20improvement%20from%20the%0Abest-published%20result%20CGis%20%2839.82%25%29.%20In%20addition%2C%20PoCo%20shows%20higher%20efficiency%0Athan%20CGis%20in%20inference%20time%20%281.75X-faster%29%2C%20and%20we%20demonstrate%20the%0Aeffectiveness%20of%20PoCo%20in%20recognizing%20places%20within%20a%20real-world%20laboratory%0Aenvironment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02885v1&entry.124074799=Read"},
{"title": "ElasticLaneNet: An Efficient Geometry-Flexible Approach for Lane\n  Detection", "author": "Yaxin Feng and Yuan Lan and Luchan Zhang and Yang Xiang", "abstract": "  The task of lane detection involves identifying the boundaries of driving\nareas in real-time. Recognizing lanes with variable and complex geometric\nstructures remains a challenge. In this paper, we explore a novel and flexible\nway of implicit lanes representation named \\textit{Elastic Lane map (ELM)}, and\nintroduce an efficient physics-informed end-to-end lane detection framework,\nnamely, ElasticLaneNet (Elastic interaction energy-informed Lane detection\nNetwork). The approach considers predicted lanes as moving zero-contours on the\nflexibly shaped \\textit{ELM} that are attracted to the ground truth guided by\nan elastic interaction energy-loss function (EIE loss). Our framework well\nintegrates the global information and low-level features. The method performs\nwell in complex lane scenarios, including those with large curvature, weak\ngeometry features at intersections, complicated cross lanes, Y-shapes lanes,\ndense lanes, etc. We apply our approach on three datasets: SDLane, CULane, and\nTuSimple. The results demonstrate exceptional performance of our method, with\nthe state-of-the-art results on the structurally diverse SDLane, achieving\nF1-score of 89.51, Recall rate of 87.50, and Precision of 91.61 with fast\ninference speed.\n", "link": "http://arxiv.org/abs/2312.10389v2", "date": "2024-04-03", "relevancy": 2.1185, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5477}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.529}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.523}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ElasticLaneNet%3A%20An%20Efficient%20Geometry-Flexible%20Approach%20for%20Lane%0A%20%20Detection&body=Title%3A%20ElasticLaneNet%3A%20An%20Efficient%20Geometry-Flexible%20Approach%20for%20Lane%0A%20%20Detection%0AAuthor%3A%20Yaxin%20Feng%20and%20Yuan%20Lan%20and%20Luchan%20Zhang%20and%20Yang%20Xiang%0AAbstract%3A%20%20%20The%20task%20of%20lane%20detection%20involves%20identifying%20the%20boundaries%20of%20driving%0Aareas%20in%20real-time.%20Recognizing%20lanes%20with%20variable%20and%20complex%20geometric%0Astructures%20remains%20a%20challenge.%20In%20this%20paper%2C%20we%20explore%20a%20novel%20and%20flexible%0Away%20of%20implicit%20lanes%20representation%20named%20%5Ctextit%7BElastic%20Lane%20map%20%28ELM%29%7D%2C%20and%0Aintroduce%20an%20efficient%20physics-informed%20end-to-end%20lane%20detection%20framework%2C%0Anamely%2C%20ElasticLaneNet%20%28Elastic%20interaction%20energy-informed%20Lane%20detection%0ANetwork%29.%20The%20approach%20considers%20predicted%20lanes%20as%20moving%20zero-contours%20on%20the%0Aflexibly%20shaped%20%5Ctextit%7BELM%7D%20that%20are%20attracted%20to%20the%20ground%20truth%20guided%20by%0Aan%20elastic%20interaction%20energy-loss%20function%20%28EIE%20loss%29.%20Our%20framework%20well%0Aintegrates%20the%20global%20information%20and%20low-level%20features.%20The%20method%20performs%0Awell%20in%20complex%20lane%20scenarios%2C%20including%20those%20with%20large%20curvature%2C%20weak%0Ageometry%20features%20at%20intersections%2C%20complicated%20cross%20lanes%2C%20Y-shapes%20lanes%2C%0Adense%20lanes%2C%20etc.%20We%20apply%20our%20approach%20on%20three%20datasets%3A%20SDLane%2C%20CULane%2C%20and%0ATuSimple.%20The%20results%20demonstrate%20exceptional%20performance%20of%20our%20method%2C%20with%0Athe%20state-of-the-art%20results%20on%20the%20structurally%20diverse%20SDLane%2C%20achieving%0AF1-score%20of%2089.51%2C%20Recall%20rate%20of%2087.50%2C%20and%20Precision%20of%2091.61%20with%20fast%0Ainference%20speed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.10389v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ElasticLaneNet%3A%20An%20Efficient%20Geometry-Flexible%20Approach%20for%20Lane%0A%20%20Detection&entry.906535625=Yaxin%20Feng%20and%20Yuan%20Lan%20and%20Luchan%20Zhang%20and%20Yang%20Xiang&entry.1292438233=%20%20The%20task%20of%20lane%20detection%20involves%20identifying%20the%20boundaries%20of%20driving%0Aareas%20in%20real-time.%20Recognizing%20lanes%20with%20variable%20and%20complex%20geometric%0Astructures%20remains%20a%20challenge.%20In%20this%20paper%2C%20we%20explore%20a%20novel%20and%20flexible%0Away%20of%20implicit%20lanes%20representation%20named%20%5Ctextit%7BElastic%20Lane%20map%20%28ELM%29%7D%2C%20and%0Aintroduce%20an%20efficient%20physics-informed%20end-to-end%20lane%20detection%20framework%2C%0Anamely%2C%20ElasticLaneNet%20%28Elastic%20interaction%20energy-informed%20Lane%20detection%0ANetwork%29.%20The%20approach%20considers%20predicted%20lanes%20as%20moving%20zero-contours%20on%20the%0Aflexibly%20shaped%20%5Ctextit%7BELM%7D%20that%20are%20attracted%20to%20the%20ground%20truth%20guided%20by%0Aan%20elastic%20interaction%20energy-loss%20function%20%28EIE%20loss%29.%20Our%20framework%20well%0Aintegrates%20the%20global%20information%20and%20low-level%20features.%20The%20method%20performs%0Awell%20in%20complex%20lane%20scenarios%2C%20including%20those%20with%20large%20curvature%2C%20weak%0Ageometry%20features%20at%20intersections%2C%20complicated%20cross%20lanes%2C%20Y-shapes%20lanes%2C%0Adense%20lanes%2C%20etc.%20We%20apply%20our%20approach%20on%20three%20datasets%3A%20SDLane%2C%20CULane%2C%20and%0ATuSimple.%20The%20results%20demonstrate%20exceptional%20performance%20of%20our%20method%2C%20with%0Athe%20state-of-the-art%20results%20on%20the%20structurally%20diverse%20SDLane%2C%20achieving%0AF1-score%20of%2089.51%2C%20Recall%20rate%20of%2087.50%2C%20and%20Precision%20of%2091.61%20with%20fast%0Ainference%20speed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.10389v2&entry.124074799=Read"},
{"title": "Multi-Scale Spatial-Temporal Self-Attention Graph Convolutional Networks\n  for Skeleton-based Action Recognition", "author": "Ikuo Nakamura", "abstract": "  Skeleton-based gesture recognition methods have achieved high success using\nGraph Convolutional Network (GCN). In addition, context-dependent adaptive\ntopology as a neighborhood vertex information and attention mechanism leverages\na model to better represent actions. In this paper, we propose self-attention\nGCN hybrid model, Multi-Scale Spatial-Temporal self-attention (MSST)-GCN to\neffectively improve modeling ability to achieve state-of-the-art results on\nseveral datasets. We utilize spatial self-attention module with adaptive\ntopology to understand intra-frame interactions within a frame among different\nbody parts, and temporal self-attention module to examine correlations between\nframes of a node. These two are followed by multi-scale convolution network\nwith dilations, which not only captures the long-range temporal dependencies of\njoints but also the long-range spatial dependencies (i.e., long-distance\ndependencies) of node temporal behaviors. They are combined into high-level\nspatial-temporal representations and output the predicted action with the\nsoftmax classifier.\n", "link": "http://arxiv.org/abs/2404.02624v1", "date": "2024-04-03", "relevancy": 2.1135, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5604}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.507}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5018}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multi-Scale%20Spatial-Temporal%20Self-Attention%20Graph%20Convolutional%20Networks%0A%20%20for%20Skeleton-based%20Action%20Recognition&body=Title%3A%20Multi-Scale%20Spatial-Temporal%20Self-Attention%20Graph%20Convolutional%20Networks%0A%20%20for%20Skeleton-based%20Action%20Recognition%0AAuthor%3A%20Ikuo%20Nakamura%0AAbstract%3A%20%20%20Skeleton-based%20gesture%20recognition%20methods%20have%20achieved%20high%20success%20using%0AGraph%20Convolutional%20Network%20%28GCN%29.%20In%20addition%2C%20context-dependent%20adaptive%0Atopology%20as%20a%20neighborhood%20vertex%20information%20and%20attention%20mechanism%20leverages%0Aa%20model%20to%20better%20represent%20actions.%20In%20this%20paper%2C%20we%20propose%20self-attention%0AGCN%20hybrid%20model%2C%20Multi-Scale%20Spatial-Temporal%20self-attention%20%28MSST%29-GCN%20to%0Aeffectively%20improve%20modeling%20ability%20to%20achieve%20state-of-the-art%20results%20on%0Aseveral%20datasets.%20We%20utilize%20spatial%20self-attention%20module%20with%20adaptive%0Atopology%20to%20understand%20intra-frame%20interactions%20within%20a%20frame%20among%20different%0Abody%20parts%2C%20and%20temporal%20self-attention%20module%20to%20examine%20correlations%20between%0Aframes%20of%20a%20node.%20These%20two%20are%20followed%20by%20multi-scale%20convolution%20network%0Awith%20dilations%2C%20which%20not%20only%20captures%20the%20long-range%20temporal%20dependencies%20of%0Ajoints%20but%20also%20the%20long-range%20spatial%20dependencies%20%28i.e.%2C%20long-distance%0Adependencies%29%20of%20node%20temporal%20behaviors.%20They%20are%20combined%20into%20high-level%0Aspatial-temporal%20representations%20and%20output%20the%20predicted%20action%20with%20the%0Asoftmax%20classifier.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02624v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Scale%20Spatial-Temporal%20Self-Attention%20Graph%20Convolutional%20Networks%0A%20%20for%20Skeleton-based%20Action%20Recognition&entry.906535625=Ikuo%20Nakamura&entry.1292438233=%20%20Skeleton-based%20gesture%20recognition%20methods%20have%20achieved%20high%20success%20using%0AGraph%20Convolutional%20Network%20%28GCN%29.%20In%20addition%2C%20context-dependent%20adaptive%0Atopology%20as%20a%20neighborhood%20vertex%20information%20and%20attention%20mechanism%20leverages%0Aa%20model%20to%20better%20represent%20actions.%20In%20this%20paper%2C%20we%20propose%20self-attention%0AGCN%20hybrid%20model%2C%20Multi-Scale%20Spatial-Temporal%20self-attention%20%28MSST%29-GCN%20to%0Aeffectively%20improve%20modeling%20ability%20to%20achieve%20state-of-the-art%20results%20on%0Aseveral%20datasets.%20We%20utilize%20spatial%20self-attention%20module%20with%20adaptive%0Atopology%20to%20understand%20intra-frame%20interactions%20within%20a%20frame%20among%20different%0Abody%20parts%2C%20and%20temporal%20self-attention%20module%20to%20examine%20correlations%20between%0Aframes%20of%20a%20node.%20These%20two%20are%20followed%20by%20multi-scale%20convolution%20network%0Awith%20dilations%2C%20which%20not%20only%20captures%20the%20long-range%20temporal%20dependencies%20of%0Ajoints%20but%20also%20the%20long-range%20spatial%20dependencies%20%28i.e.%2C%20long-distance%0Adependencies%29%20of%20node%20temporal%20behaviors.%20They%20are%20combined%20into%20high-level%0Aspatial-temporal%20representations%20and%20output%20the%20predicted%20action%20with%20the%0Asoftmax%20classifier.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02624v1&entry.124074799=Read"},
{"title": "LLaFS: When Large Language Models Meet Few-Shot Segmentation", "author": "Lanyun Zhu and Tianrun Chen and Deyi Ji and Jieping Ye and Jun Liu", "abstract": "  This paper proposes LLaFS, the first attempt to leverage large language\nmodels (LLMs) in few-shot segmentation. In contrast to the conventional\nfew-shot segmentation methods that only rely on the limited and biased\ninformation from the annotated support images, LLaFS leverages the vast prior\nknowledge gained by LLM as an effective supplement and directly uses the LLM to\nsegment images in a few-shot manner. To enable the text-based LLM to handle\nimage-related tasks, we carefully design an input instruction that allows the\nLLM to produce segmentation results represented as polygons, and propose a\nregion-attribute table to simulate the human visual mechanism and provide\nmulti-modal guidance. We also synthesize pseudo samples and use curriculum\nlearning for pretraining to augment data and achieve better optimization. LLaFS\nachieves state-of-the-art results on multiple datasets, showing the potential\nof using LLMs for few-shot computer vision tasks.\n", "link": "http://arxiv.org/abs/2311.16926v5", "date": "2024-04-03", "relevancy": 2.0674, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5245}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5175}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4963}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LLaFS%3A%20When%20Large%20Language%20Models%20Meet%20Few-Shot%20Segmentation&body=Title%3A%20LLaFS%3A%20When%20Large%20Language%20Models%20Meet%20Few-Shot%20Segmentation%0AAuthor%3A%20Lanyun%20Zhu%20and%20Tianrun%20Chen%20and%20Deyi%20Ji%20and%20Jieping%20Ye%20and%20Jun%20Liu%0AAbstract%3A%20%20%20This%20paper%20proposes%20LLaFS%2C%20the%20first%20attempt%20to%20leverage%20large%20language%0Amodels%20%28LLMs%29%20in%20few-shot%20segmentation.%20In%20contrast%20to%20the%20conventional%0Afew-shot%20segmentation%20methods%20that%20only%20rely%20on%20the%20limited%20and%20biased%0Ainformation%20from%20the%20annotated%20support%20images%2C%20LLaFS%20leverages%20the%20vast%20prior%0Aknowledge%20gained%20by%20LLM%20as%20an%20effective%20supplement%20and%20directly%20uses%20the%20LLM%20to%0Asegment%20images%20in%20a%20few-shot%20manner.%20To%20enable%20the%20text-based%20LLM%20to%20handle%0Aimage-related%20tasks%2C%20we%20carefully%20design%20an%20input%20instruction%20that%20allows%20the%0ALLM%20to%20produce%20segmentation%20results%20represented%20as%20polygons%2C%20and%20propose%20a%0Aregion-attribute%20table%20to%20simulate%20the%20human%20visual%20mechanism%20and%20provide%0Amulti-modal%20guidance.%20We%20also%20synthesize%20pseudo%20samples%20and%20use%20curriculum%0Alearning%20for%20pretraining%20to%20augment%20data%20and%20achieve%20better%20optimization.%20LLaFS%0Aachieves%20state-of-the-art%20results%20on%20multiple%20datasets%2C%20showing%20the%20potential%0Aof%20using%20LLMs%20for%20few-shot%20computer%20vision%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.16926v5", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLaFS%3A%20When%20Large%20Language%20Models%20Meet%20Few-Shot%20Segmentation&entry.906535625=Lanyun%20Zhu%20and%20Tianrun%20Chen%20and%20Deyi%20Ji%20and%20Jieping%20Ye%20and%20Jun%20Liu&entry.1292438233=%20%20This%20paper%20proposes%20LLaFS%2C%20the%20first%20attempt%20to%20leverage%20large%20language%0Amodels%20%28LLMs%29%20in%20few-shot%20segmentation.%20In%20contrast%20to%20the%20conventional%0Afew-shot%20segmentation%20methods%20that%20only%20rely%20on%20the%20limited%20and%20biased%0Ainformation%20from%20the%20annotated%20support%20images%2C%20LLaFS%20leverages%20the%20vast%20prior%0Aknowledge%20gained%20by%20LLM%20as%20an%20effective%20supplement%20and%20directly%20uses%20the%20LLM%20to%0Asegment%20images%20in%20a%20few-shot%20manner.%20To%20enable%20the%20text-based%20LLM%20to%20handle%0Aimage-related%20tasks%2C%20we%20carefully%20design%20an%20input%20instruction%20that%20allows%20the%0ALLM%20to%20produce%20segmentation%20results%20represented%20as%20polygons%2C%20and%20propose%20a%0Aregion-attribute%20table%20to%20simulate%20the%20human%20visual%20mechanism%20and%20provide%0Amulti-modal%20guidance.%20We%20also%20synthesize%20pseudo%20samples%20and%20use%20curriculum%0Alearning%20for%20pretraining%20to%20augment%20data%20and%20achieve%20better%20optimization.%20LLaFS%0Aachieves%20state-of-the-art%20results%20on%20multiple%20datasets%2C%20showing%20the%20potential%0Aof%20using%20LLMs%20for%20few-shot%20computer%20vision%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.16926v5&entry.124074799=Read"},
{"title": "Isometric Multi-Shape Matching", "author": "Maolin Gao and Zorah L\u00e4hner and Johan Thunberg and Daniel Cremers and Florian Bernard", "abstract": "  Finding correspondences between shapes is a fundamental problem in computer\nvision and graphics, which is relevant for many applications, including 3D\nreconstruction, object tracking, and style transfer. The vast majority of\ncorrespondence methods aim to find a solution between pairs of shapes, even if\nmultiple instances of the same class are available. While isometries are often\nstudied in shape correspondence problems, they have not been considered\nexplicitly in the multi-matching setting. This paper closes this gap by\nproposing a novel optimisation formulation for isometric multi-shape matching.\nWe present a suitable optimisation algorithm for solving our formulation and\nprovide a convergence and complexity analysis. Our algorithm obtains\nmulti-matchings that are by construction provably cycle-consistent. We\ndemonstrate the superior performance of our method on various datasets and set\nthe new state-of-the-art in isometric multi-shape matching.\n", "link": "http://arxiv.org/abs/2012.02689v2", "date": "2024-04-03", "relevancy": 2.0633, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5406}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4984}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.498}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Isometric%20Multi-Shape%20Matching&body=Title%3A%20Isometric%20Multi-Shape%20Matching%0AAuthor%3A%20Maolin%20Gao%20and%20Zorah%20L%C3%A4hner%20and%20Johan%20Thunberg%20and%20Daniel%20Cremers%20and%20Florian%20Bernard%0AAbstract%3A%20%20%20Finding%20correspondences%20between%20shapes%20is%20a%20fundamental%20problem%20in%20computer%0Avision%20and%20graphics%2C%20which%20is%20relevant%20for%20many%20applications%2C%20including%203D%0Areconstruction%2C%20object%20tracking%2C%20and%20style%20transfer.%20The%20vast%20majority%20of%0Acorrespondence%20methods%20aim%20to%20find%20a%20solution%20between%20pairs%20of%20shapes%2C%20even%20if%0Amultiple%20instances%20of%20the%20same%20class%20are%20available.%20While%20isometries%20are%20often%0Astudied%20in%20shape%20correspondence%20problems%2C%20they%20have%20not%20been%20considered%0Aexplicitly%20in%20the%20multi-matching%20setting.%20This%20paper%20closes%20this%20gap%20by%0Aproposing%20a%20novel%20optimisation%20formulation%20for%20isometric%20multi-shape%20matching.%0AWe%20present%20a%20suitable%20optimisation%20algorithm%20for%20solving%20our%20formulation%20and%0Aprovide%20a%20convergence%20and%20complexity%20analysis.%20Our%20algorithm%20obtains%0Amulti-matchings%20that%20are%20by%20construction%20provably%20cycle-consistent.%20We%0Ademonstrate%20the%20superior%20performance%20of%20our%20method%20on%20various%20datasets%20and%20set%0Athe%20new%20state-of-the-art%20in%20isometric%20multi-shape%20matching.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2012.02689v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Isometric%20Multi-Shape%20Matching&entry.906535625=Maolin%20Gao%20and%20Zorah%20L%C3%A4hner%20and%20Johan%20Thunberg%20and%20Daniel%20Cremers%20and%20Florian%20Bernard&entry.1292438233=%20%20Finding%20correspondences%20between%20shapes%20is%20a%20fundamental%20problem%20in%20computer%0Avision%20and%20graphics%2C%20which%20is%20relevant%20for%20many%20applications%2C%20including%203D%0Areconstruction%2C%20object%20tracking%2C%20and%20style%20transfer.%20The%20vast%20majority%20of%0Acorrespondence%20methods%20aim%20to%20find%20a%20solution%20between%20pairs%20of%20shapes%2C%20even%20if%0Amultiple%20instances%20of%20the%20same%20class%20are%20available.%20While%20isometries%20are%20often%0Astudied%20in%20shape%20correspondence%20problems%2C%20they%20have%20not%20been%20considered%0Aexplicitly%20in%20the%20multi-matching%20setting.%20This%20paper%20closes%20this%20gap%20by%0Aproposing%20a%20novel%20optimisation%20formulation%20for%20isometric%20multi-shape%20matching.%0AWe%20present%20a%20suitable%20optimisation%20algorithm%20for%20solving%20our%20formulation%20and%0Aprovide%20a%20convergence%20and%20complexity%20analysis.%20Our%20algorithm%20obtains%0Amulti-matchings%20that%20are%20by%20construction%20provably%20cycle-consistent.%20We%0Ademonstrate%20the%20superior%20performance%20of%20our%20method%20on%20various%20datasets%20and%20set%0Athe%20new%20state-of-the-art%20in%20isometric%20multi-shape%20matching.%0A&entry.1838667208=http%3A//arxiv.org/abs/2012.02689v2&entry.124074799=Read"},
{"title": "Cross-Architecture Transfer Learning for Linear-Cost Inference\n  Transformers", "author": "Sehyun Choi", "abstract": "  Recently, multiple architectures has been proposed to improve the efficiency\nof the Transformer Language Models through changing the design of the\nself-attention block to have a linear-cost inference (LCI). A notable approach\nin this realm is the State-Space Machines (SSMs) architecture, which showed\non-par performance on language modeling tasks with the self-attention\ntransformers. However, such an architectural change requires a full pretraining\nof the weights from scratch, which incurs a huge cost to researchers and\npractitioners who want to use the new architectures. In the more traditional\nlinear attention works, it has been proposed to approximate full attention with\nlinear attention by swap-and-finetune framework. Motivated by this approach, we\npropose Cross-Architecture Transfer Learning (XATL), in which the weights of\nthe shared components between LCI and self-attention-based transformers, such\nas layernorms, MLPs, input/output embeddings, are directly transferred to the\nnew architecture from already pre-trained model parameters. We experimented the\nefficacy of the method on varying sizes and alternative attention architectures\nand show that \\methodabbr significantly reduces the training time up to 2.5x\ntimes and converges to a better minimum with up to 2.6% stronger model on the\nLM benchmarks within the same compute budget.\n", "link": "http://arxiv.org/abs/2404.02684v1", "date": "2024-04-03", "relevancy": 2.0632, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5403}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5224}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4994}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Cross-Architecture%20Transfer%20Learning%20for%20Linear-Cost%20Inference%0A%20%20Transformers&body=Title%3A%20Cross-Architecture%20Transfer%20Learning%20for%20Linear-Cost%20Inference%0A%20%20Transformers%0AAuthor%3A%20Sehyun%20Choi%0AAbstract%3A%20%20%20Recently%2C%20multiple%20architectures%20has%20been%20proposed%20to%20improve%20the%20efficiency%0Aof%20the%20Transformer%20Language%20Models%20through%20changing%20the%20design%20of%20the%0Aself-attention%20block%20to%20have%20a%20linear-cost%20inference%20%28LCI%29.%20A%20notable%20approach%0Ain%20this%20realm%20is%20the%20State-Space%20Machines%20%28SSMs%29%20architecture%2C%20which%20showed%0Aon-par%20performance%20on%20language%20modeling%20tasks%20with%20the%20self-attention%0Atransformers.%20However%2C%20such%20an%20architectural%20change%20requires%20a%20full%20pretraining%0Aof%20the%20weights%20from%20scratch%2C%20which%20incurs%20a%20huge%20cost%20to%20researchers%20and%0Apractitioners%20who%20want%20to%20use%20the%20new%20architectures.%20In%20the%20more%20traditional%0Alinear%20attention%20works%2C%20it%20has%20been%20proposed%20to%20approximate%20full%20attention%20with%0Alinear%20attention%20by%20swap-and-finetune%20framework.%20Motivated%20by%20this%20approach%2C%20we%0Apropose%20Cross-Architecture%20Transfer%20Learning%20%28XATL%29%2C%20in%20which%20the%20weights%20of%0Athe%20shared%20components%20between%20LCI%20and%20self-attention-based%20transformers%2C%20such%0Aas%20layernorms%2C%20MLPs%2C%20input/output%20embeddings%2C%20are%20directly%20transferred%20to%20the%0Anew%20architecture%20from%20already%20pre-trained%20model%20parameters.%20We%20experimented%20the%0Aefficacy%20of%20the%20method%20on%20varying%20sizes%20and%20alternative%20attention%20architectures%0Aand%20show%20that%20%5Cmethodabbr%20significantly%20reduces%20the%20training%20time%20up%20to%202.5x%0Atimes%20and%20converges%20to%20a%20better%20minimum%20with%20up%20to%202.6%25%20stronger%20model%20on%20the%0ALM%20benchmarks%20within%20the%20same%20compute%20budget.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02684v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Architecture%20Transfer%20Learning%20for%20Linear-Cost%20Inference%0A%20%20Transformers&entry.906535625=Sehyun%20Choi&entry.1292438233=%20%20Recently%2C%20multiple%20architectures%20has%20been%20proposed%20to%20improve%20the%20efficiency%0Aof%20the%20Transformer%20Language%20Models%20through%20changing%20the%20design%20of%20the%0Aself-attention%20block%20to%20have%20a%20linear-cost%20inference%20%28LCI%29.%20A%20notable%20approach%0Ain%20this%20realm%20is%20the%20State-Space%20Machines%20%28SSMs%29%20architecture%2C%20which%20showed%0Aon-par%20performance%20on%20language%20modeling%20tasks%20with%20the%20self-attention%0Atransformers.%20However%2C%20such%20an%20architectural%20change%20requires%20a%20full%20pretraining%0Aof%20the%20weights%20from%20scratch%2C%20which%20incurs%20a%20huge%20cost%20to%20researchers%20and%0Apractitioners%20who%20want%20to%20use%20the%20new%20architectures.%20In%20the%20more%20traditional%0Alinear%20attention%20works%2C%20it%20has%20been%20proposed%20to%20approximate%20full%20attention%20with%0Alinear%20attention%20by%20swap-and-finetune%20framework.%20Motivated%20by%20this%20approach%2C%20we%0Apropose%20Cross-Architecture%20Transfer%20Learning%20%28XATL%29%2C%20in%20which%20the%20weights%20of%0Athe%20shared%20components%20between%20LCI%20and%20self-attention-based%20transformers%2C%20such%0Aas%20layernorms%2C%20MLPs%2C%20input/output%20embeddings%2C%20are%20directly%20transferred%20to%20the%0Anew%20architecture%20from%20already%20pre-trained%20model%20parameters.%20We%20experimented%20the%0Aefficacy%20of%20the%20method%20on%20varying%20sizes%20and%20alternative%20attention%20architectures%0Aand%20show%20that%20%5Cmethodabbr%20significantly%20reduces%20the%20training%20time%20up%20to%202.5x%0Atimes%20and%20converges%20to%20a%20better%20minimum%20with%20up%20to%202.6%25%20stronger%20model%20on%20the%0ALM%20benchmarks%20within%20the%20same%20compute%20budget.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02684v1&entry.124074799=Read"},
{"title": "Towards detecting unanticipated bias in Large Language Models", "author": "Anna Kruspe", "abstract": "  Over the last year, Large Language Models (LLMs) like ChatGPT have become\nwidely available and have exhibited fairness issues similar to those in\nprevious machine learning systems. Current research is primarily focused on\nanalyzing and quantifying these biases in training data and their impact on the\ndecisions of these models, alongside developing mitigation strategies. This\nresearch largely targets well-known biases related to gender, race, ethnicity,\nand language. However, it is clear that LLMs are also affected by other, less\nobvious implicit biases. The complex and often opaque nature of these models\nmakes detecting such biases challenging, yet this is crucial due to their\npotential negative impact in various applications. In this paper, we explore\nnew avenues for detecting these unanticipated biases in LLMs, focusing\nspecifically on Uncertainty Quantification and Explainable AI methods. These\napproaches aim to assess the certainty of model decisions and to make the\ninternal decision-making processes of LLMs more transparent, thereby\nidentifying and understanding biases that are not immediately apparent. Through\nthis research, we aim to contribute to the development of fairer and more\ntransparent AI systems.\n", "link": "http://arxiv.org/abs/2404.02650v1", "date": "2024-04-03", "relevancy": 2.0557, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5596}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.509}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5006}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20detecting%20unanticipated%20bias%20in%20Large%20Language%20Models&body=Title%3A%20Towards%20detecting%20unanticipated%20bias%20in%20Large%20Language%20Models%0AAuthor%3A%20Anna%20Kruspe%0AAbstract%3A%20%20%20Over%20the%20last%20year%2C%20Large%20Language%20Models%20%28LLMs%29%20like%20ChatGPT%20have%20become%0Awidely%20available%20and%20have%20exhibited%20fairness%20issues%20similar%20to%20those%20in%0Aprevious%20machine%20learning%20systems.%20Current%20research%20is%20primarily%20focused%20on%0Aanalyzing%20and%20quantifying%20these%20biases%20in%20training%20data%20and%20their%20impact%20on%20the%0Adecisions%20of%20these%20models%2C%20alongside%20developing%20mitigation%20strategies.%20This%0Aresearch%20largely%20targets%20well-known%20biases%20related%20to%20gender%2C%20race%2C%20ethnicity%2C%0Aand%20language.%20However%2C%20it%20is%20clear%20that%20LLMs%20are%20also%20affected%20by%20other%2C%20less%0Aobvious%20implicit%20biases.%20The%20complex%20and%20often%20opaque%20nature%20of%20these%20models%0Amakes%20detecting%20such%20biases%20challenging%2C%20yet%20this%20is%20crucial%20due%20to%20their%0Apotential%20negative%20impact%20in%20various%20applications.%20In%20this%20paper%2C%20we%20explore%0Anew%20avenues%20for%20detecting%20these%20unanticipated%20biases%20in%20LLMs%2C%20focusing%0Aspecifically%20on%20Uncertainty%20Quantification%20and%20Explainable%20AI%20methods.%20These%0Aapproaches%20aim%20to%20assess%20the%20certainty%20of%20model%20decisions%20and%20to%20make%20the%0Ainternal%20decision-making%20processes%20of%20LLMs%20more%20transparent%2C%20thereby%0Aidentifying%20and%20understanding%20biases%20that%20are%20not%20immediately%20apparent.%20Through%0Athis%20research%2C%20we%20aim%20to%20contribute%20to%20the%20development%20of%20fairer%20and%20more%0Atransparent%20AI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02650v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20detecting%20unanticipated%20bias%20in%20Large%20Language%20Models&entry.906535625=Anna%20Kruspe&entry.1292438233=%20%20Over%20the%20last%20year%2C%20Large%20Language%20Models%20%28LLMs%29%20like%20ChatGPT%20have%20become%0Awidely%20available%20and%20have%20exhibited%20fairness%20issues%20similar%20to%20those%20in%0Aprevious%20machine%20learning%20systems.%20Current%20research%20is%20primarily%20focused%20on%0Aanalyzing%20and%20quantifying%20these%20biases%20in%20training%20data%20and%20their%20impact%20on%20the%0Adecisions%20of%20these%20models%2C%20alongside%20developing%20mitigation%20strategies.%20This%0Aresearch%20largely%20targets%20well-known%20biases%20related%20to%20gender%2C%20race%2C%20ethnicity%2C%0Aand%20language.%20However%2C%20it%20is%20clear%20that%20LLMs%20are%20also%20affected%20by%20other%2C%20less%0Aobvious%20implicit%20biases.%20The%20complex%20and%20often%20opaque%20nature%20of%20these%20models%0Amakes%20detecting%20such%20biases%20challenging%2C%20yet%20this%20is%20crucial%20due%20to%20their%0Apotential%20negative%20impact%20in%20various%20applications.%20In%20this%20paper%2C%20we%20explore%0Anew%20avenues%20for%20detecting%20these%20unanticipated%20biases%20in%20LLMs%2C%20focusing%0Aspecifically%20on%20Uncertainty%20Quantification%20and%20Explainable%20AI%20methods.%20These%0Aapproaches%20aim%20to%20assess%20the%20certainty%20of%20model%20decisions%20and%20to%20make%20the%0Ainternal%20decision-making%20processes%20of%20LLMs%20more%20transparent%2C%20thereby%0Aidentifying%20and%20understanding%20biases%20that%20are%20not%20immediately%20apparent.%20Through%0Athis%20research%2C%20we%20aim%20to%20contribute%20to%20the%20development%20of%20fairer%20and%20more%0Atransparent%20AI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02650v1&entry.124074799=Read"},
{"title": "Implicit Neural Representations for Breathing-compensated Volume\n  Reconstruction in Robotic Ultrasound", "author": "Yordanka Velikova and Mohammad Farid Azampour and Walter Simson and Marco Esposito and Nassir Navab", "abstract": "  Ultrasound (US) imaging is widely used in diagnosing and staging abdominal\ndiseases due to its lack of non-ionizing radiation and prevalent availability.\nHowever, significant inter-operator variability and inconsistent image\nacquisition hinder the widespread adoption of extensive screening programs.\nRobotic ultrasound systems have emerged as a promising solution, offering\nstandardized acquisition protocols and the possibility of automated\nacquisition. Additionally, these systems enable access to 3D data via robotic\ntracking, enhancing volumetric reconstruction for improved ultrasound\ninterpretation and precise disease diagnosis. However, the interpretability of\n3D US reconstruction of abdominal images can be affected by the patient's\nbreathing motion. This study introduces a method to compensate for breathing\nmotion in 3D US compounding by leveraging implicit neural representations. Our\napproach employs a robotic ultrasound system for automated screenings. To\ndemonstrate the method's effectiveness, we evaluate our proposed method for the\ndiagnosis and monitoring of abdominal aorta aneurysms as a representative use\ncase. Our experiments demonstrate that our proposed pipeline facilitates robust\nautomated robotic acquisition, mitigating artifacts from breathing motion, and\nyields smoother 3D reconstructions for enhanced screening and medical\ndiagnosis.\n", "link": "http://arxiv.org/abs/2311.04999v2", "date": "2024-04-03", "relevancy": 2.0551, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.516}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5132}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5095}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Implicit%20Neural%20Representations%20for%20Breathing-compensated%20Volume%0A%20%20Reconstruction%20in%20Robotic%20Ultrasound&body=Title%3A%20Implicit%20Neural%20Representations%20for%20Breathing-compensated%20Volume%0A%20%20Reconstruction%20in%20Robotic%20Ultrasound%0AAuthor%3A%20Yordanka%20Velikova%20and%20Mohammad%20Farid%20Azampour%20and%20Walter%20Simson%20and%20Marco%20Esposito%20and%20Nassir%20Navab%0AAbstract%3A%20%20%20Ultrasound%20%28US%29%20imaging%20is%20widely%20used%20in%20diagnosing%20and%20staging%20abdominal%0Adiseases%20due%20to%20its%20lack%20of%20non-ionizing%20radiation%20and%20prevalent%20availability.%0AHowever%2C%20significant%20inter-operator%20variability%20and%20inconsistent%20image%0Aacquisition%20hinder%20the%20widespread%20adoption%20of%20extensive%20screening%20programs.%0ARobotic%20ultrasound%20systems%20have%20emerged%20as%20a%20promising%20solution%2C%20offering%0Astandardized%20acquisition%20protocols%20and%20the%20possibility%20of%20automated%0Aacquisition.%20Additionally%2C%20these%20systems%20enable%20access%20to%203D%20data%20via%20robotic%0Atracking%2C%20enhancing%20volumetric%20reconstruction%20for%20improved%20ultrasound%0Ainterpretation%20and%20precise%20disease%20diagnosis.%20However%2C%20the%20interpretability%20of%0A3D%20US%20reconstruction%20of%20abdominal%20images%20can%20be%20affected%20by%20the%20patient%27s%0Abreathing%20motion.%20This%20study%20introduces%20a%20method%20to%20compensate%20for%20breathing%0Amotion%20in%203D%20US%20compounding%20by%20leveraging%20implicit%20neural%20representations.%20Our%0Aapproach%20employs%20a%20robotic%20ultrasound%20system%20for%20automated%20screenings.%20To%0Ademonstrate%20the%20method%27s%20effectiveness%2C%20we%20evaluate%20our%20proposed%20method%20for%20the%0Adiagnosis%20and%20monitoring%20of%20abdominal%20aorta%20aneurysms%20as%20a%20representative%20use%0Acase.%20Our%20experiments%20demonstrate%20that%20our%20proposed%20pipeline%20facilitates%20robust%0Aautomated%20robotic%20acquisition%2C%20mitigating%20artifacts%20from%20breathing%20motion%2C%20and%0Ayields%20smoother%203D%20reconstructions%20for%20enhanced%20screening%20and%20medical%0Adiagnosis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.04999v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implicit%20Neural%20Representations%20for%20Breathing-compensated%20Volume%0A%20%20Reconstruction%20in%20Robotic%20Ultrasound&entry.906535625=Yordanka%20Velikova%20and%20Mohammad%20Farid%20Azampour%20and%20Walter%20Simson%20and%20Marco%20Esposito%20and%20Nassir%20Navab&entry.1292438233=%20%20Ultrasound%20%28US%29%20imaging%20is%20widely%20used%20in%20diagnosing%20and%20staging%20abdominal%0Adiseases%20due%20to%20its%20lack%20of%20non-ionizing%20radiation%20and%20prevalent%20availability.%0AHowever%2C%20significant%20inter-operator%20variability%20and%20inconsistent%20image%0Aacquisition%20hinder%20the%20widespread%20adoption%20of%20extensive%20screening%20programs.%0ARobotic%20ultrasound%20systems%20have%20emerged%20as%20a%20promising%20solution%2C%20offering%0Astandardized%20acquisition%20protocols%20and%20the%20possibility%20of%20automated%0Aacquisition.%20Additionally%2C%20these%20systems%20enable%20access%20to%203D%20data%20via%20robotic%0Atracking%2C%20enhancing%20volumetric%20reconstruction%20for%20improved%20ultrasound%0Ainterpretation%20and%20precise%20disease%20diagnosis.%20However%2C%20the%20interpretability%20of%0A3D%20US%20reconstruction%20of%20abdominal%20images%20can%20be%20affected%20by%20the%20patient%27s%0Abreathing%20motion.%20This%20study%20introduces%20a%20method%20to%20compensate%20for%20breathing%0Amotion%20in%203D%20US%20compounding%20by%20leveraging%20implicit%20neural%20representations.%20Our%0Aapproach%20employs%20a%20robotic%20ultrasound%20system%20for%20automated%20screenings.%20To%0Ademonstrate%20the%20method%27s%20effectiveness%2C%20we%20evaluate%20our%20proposed%20method%20for%20the%0Adiagnosis%20and%20monitoring%20of%20abdominal%20aorta%20aneurysms%20as%20a%20representative%20use%0Acase.%20Our%20experiments%20demonstrate%20that%20our%20proposed%20pipeline%20facilitates%20robust%0Aautomated%20robotic%20acquisition%2C%20mitigating%20artifacts%20from%20breathing%20motion%2C%20and%0Ayields%20smoother%203D%20reconstructions%20for%20enhanced%20screening%20and%20medical%0Adiagnosis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.04999v2&entry.124074799=Read"},
{"title": "Understanding the Learning Dynamics of Alignment with Human Feedback", "author": "Shawn Im and Yixuan Li", "abstract": "  Aligning large language models (LLMs) with human intentions has become a\ncritical task for safely deploying models in real-world systems. While existing\nalignment approaches have seen empirical success, theoretically understanding\nhow these methods affect model behavior remains an open question. Our work\nprovides an initial attempt to theoretically analyze the learning dynamics of\nhuman preference alignment. We formally show how the distribution of preference\ndatasets influences the rate of model updates and provide rigorous guarantees\non the training accuracy. Our theory also reveals an intricate phenomenon where\nthe optimization is prone to prioritizing certain behaviors with higher\npreference distinguishability. We empirically validate our findings on\ncontemporary LLMs and alignment tasks, reinforcing our theoretical insights and\nshedding light on considerations for future alignment approaches. Disclaimer:\nThis paper contains potentially offensive text; reader discretion is advised.\n", "link": "http://arxiv.org/abs/2403.18742v2", "date": "2024-04-03", "relevancy": 2.0107, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5099}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5058}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4941}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Understanding%20the%20Learning%20Dynamics%20of%20Alignment%20with%20Human%20Feedback&body=Title%3A%20Understanding%20the%20Learning%20Dynamics%20of%20Alignment%20with%20Human%20Feedback%0AAuthor%3A%20Shawn%20Im%20and%20Yixuan%20Li%0AAbstract%3A%20%20%20Aligning%20large%20language%20models%20%28LLMs%29%20with%20human%20intentions%20has%20become%20a%0Acritical%20task%20for%20safely%20deploying%20models%20in%20real-world%20systems.%20While%20existing%0Aalignment%20approaches%20have%20seen%20empirical%20success%2C%20theoretically%20understanding%0Ahow%20these%20methods%20affect%20model%20behavior%20remains%20an%20open%20question.%20Our%20work%0Aprovides%20an%20initial%20attempt%20to%20theoretically%20analyze%20the%20learning%20dynamics%20of%0Ahuman%20preference%20alignment.%20We%20formally%20show%20how%20the%20distribution%20of%20preference%0Adatasets%20influences%20the%20rate%20of%20model%20updates%20and%20provide%20rigorous%20guarantees%0Aon%20the%20training%20accuracy.%20Our%20theory%20also%20reveals%20an%20intricate%20phenomenon%20where%0Athe%20optimization%20is%20prone%20to%20prioritizing%20certain%20behaviors%20with%20higher%0Apreference%20distinguishability.%20We%20empirically%20validate%20our%20findings%20on%0Acontemporary%20LLMs%20and%20alignment%20tasks%2C%20reinforcing%20our%20theoretical%20insights%20and%0Ashedding%20light%20on%20considerations%20for%20future%20alignment%20approaches.%20Disclaimer%3A%0AThis%20paper%20contains%20potentially%20offensive%20text%3B%20reader%20discretion%20is%20advised.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18742v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20the%20Learning%20Dynamics%20of%20Alignment%20with%20Human%20Feedback&entry.906535625=Shawn%20Im%20and%20Yixuan%20Li&entry.1292438233=%20%20Aligning%20large%20language%20models%20%28LLMs%29%20with%20human%20intentions%20has%20become%20a%0Acritical%20task%20for%20safely%20deploying%20models%20in%20real-world%20systems.%20While%20existing%0Aalignment%20approaches%20have%20seen%20empirical%20success%2C%20theoretically%20understanding%0Ahow%20these%20methods%20affect%20model%20behavior%20remains%20an%20open%20question.%20Our%20work%0Aprovides%20an%20initial%20attempt%20to%20theoretically%20analyze%20the%20learning%20dynamics%20of%0Ahuman%20preference%20alignment.%20We%20formally%20show%20how%20the%20distribution%20of%20preference%0Adatasets%20influences%20the%20rate%20of%20model%20updates%20and%20provide%20rigorous%20guarantees%0Aon%20the%20training%20accuracy.%20Our%20theory%20also%20reveals%20an%20intricate%20phenomenon%20where%0Athe%20optimization%20is%20prone%20to%20prioritizing%20certain%20behaviors%20with%20higher%0Apreference%20distinguishability.%20We%20empirically%20validate%20our%20findings%20on%0Acontemporary%20LLMs%20and%20alignment%20tasks%2C%20reinforcing%20our%20theoretical%20insights%20and%0Ashedding%20light%20on%20considerations%20for%20future%20alignment%20approaches.%20Disclaimer%3A%0AThis%20paper%20contains%20potentially%20offensive%20text%3B%20reader%20discretion%20is%20advised.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18742v2&entry.124074799=Read"},
{"title": "BAdam: A Memory Efficient Full Parameter Training Method for Large\n  Language Models", "author": "Qijun Luo and Hengxu Yu and Xiao Li", "abstract": "  This work presents BAdam, an optimizer that leverages the block coordinate\noptimization framework with Adam as the inner solver. BAdam offers a memory\nefficient approach to the full parameter finetuning of large language models\nand reduces running time of the backward process thanks to the chain rule\nproperty. Experimentally, we apply BAdam to instruction-tune the Llama 2-7B\nmodel on the Alpaca-GPT4 dataset using a single RTX3090-24GB GPU. The results\nindicate that BAdam exhibits superior convergence behavior in comparison to\nLoRA and LOMO. Furthermore, our downstream performance evaluation of the\ninstruction-tuned models using the MT-bench shows that BAdam modestly surpasses\nLoRA and more substantially outperforms LOMO. Finally, we compare BAdam with\nAdam on a medium-sized task, i.e., finetuning RoBERTa-large on the SuperGLUE\nbenchmark. The results demonstrate that BAdam is capable of narrowing the\nperformance gap with Adam. Our code is available at\nhttps://github.com/Ledzy/BAdam.\n", "link": "http://arxiv.org/abs/2404.02827v1", "date": "2024-04-03", "relevancy": 1.9999, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5079}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.504}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4699}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20BAdam%3A%20A%20Memory%20Efficient%20Full%20Parameter%20Training%20Method%20for%20Large%0A%20%20Language%20Models&body=Title%3A%20BAdam%3A%20A%20Memory%20Efficient%20Full%20Parameter%20Training%20Method%20for%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Qijun%20Luo%20and%20Hengxu%20Yu%20and%20Xiao%20Li%0AAbstract%3A%20%20%20This%20work%20presents%20BAdam%2C%20an%20optimizer%20that%20leverages%20the%20block%20coordinate%0Aoptimization%20framework%20with%20Adam%20as%20the%20inner%20solver.%20BAdam%20offers%20a%20memory%0Aefficient%20approach%20to%20the%20full%20parameter%20finetuning%20of%20large%20language%20models%0Aand%20reduces%20running%20time%20of%20the%20backward%20process%20thanks%20to%20the%20chain%20rule%0Aproperty.%20Experimentally%2C%20we%20apply%20BAdam%20to%20instruction-tune%20the%20Llama%202-7B%0Amodel%20on%20the%20Alpaca-GPT4%20dataset%20using%20a%20single%20RTX3090-24GB%20GPU.%20The%20results%0Aindicate%20that%20BAdam%20exhibits%20superior%20convergence%20behavior%20in%20comparison%20to%0ALoRA%20and%20LOMO.%20Furthermore%2C%20our%20downstream%20performance%20evaluation%20of%20the%0Ainstruction-tuned%20models%20using%20the%20MT-bench%20shows%20that%20BAdam%20modestly%20surpasses%0ALoRA%20and%20more%20substantially%20outperforms%20LOMO.%20Finally%2C%20we%20compare%20BAdam%20with%0AAdam%20on%20a%20medium-sized%20task%2C%20i.e.%2C%20finetuning%20RoBERTa-large%20on%20the%20SuperGLUE%0Abenchmark.%20The%20results%20demonstrate%20that%20BAdam%20is%20capable%20of%20narrowing%20the%0Aperformance%20gap%20with%20Adam.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Ledzy/BAdam.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02827v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BAdam%3A%20A%20Memory%20Efficient%20Full%20Parameter%20Training%20Method%20for%20Large%0A%20%20Language%20Models&entry.906535625=Qijun%20Luo%20and%20Hengxu%20Yu%20and%20Xiao%20Li&entry.1292438233=%20%20This%20work%20presents%20BAdam%2C%20an%20optimizer%20that%20leverages%20the%20block%20coordinate%0Aoptimization%20framework%20with%20Adam%20as%20the%20inner%20solver.%20BAdam%20offers%20a%20memory%0Aefficient%20approach%20to%20the%20full%20parameter%20finetuning%20of%20large%20language%20models%0Aand%20reduces%20running%20time%20of%20the%20backward%20process%20thanks%20to%20the%20chain%20rule%0Aproperty.%20Experimentally%2C%20we%20apply%20BAdam%20to%20instruction-tune%20the%20Llama%202-7B%0Amodel%20on%20the%20Alpaca-GPT4%20dataset%20using%20a%20single%20RTX3090-24GB%20GPU.%20The%20results%0Aindicate%20that%20BAdam%20exhibits%20superior%20convergence%20behavior%20in%20comparison%20to%0ALoRA%20and%20LOMO.%20Furthermore%2C%20our%20downstream%20performance%20evaluation%20of%20the%0Ainstruction-tuned%20models%20using%20the%20MT-bench%20shows%20that%20BAdam%20modestly%20surpasses%0ALoRA%20and%20more%20substantially%20outperforms%20LOMO.%20Finally%2C%20we%20compare%20BAdam%20with%0AAdam%20on%20a%20medium-sized%20task%2C%20i.e.%2C%20finetuning%20RoBERTa-large%20on%20the%20SuperGLUE%0Abenchmark.%20The%20results%20demonstrate%20that%20BAdam%20is%20capable%20of%20narrowing%20the%0Aperformance%20gap%20with%20Adam.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Ledzy/BAdam.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02827v1&entry.124074799=Read"},
{"title": "Enhancing Multi-Objective Optimization through Machine\n  Learning-Supported Multiphysics Simulation", "author": "Diego Botache and Jens Decke and Winfried Ripken and Abhinay Dornipati and Franz G\u00f6tz-Hahn and Mohamed Ayeb and Bernhard Sick", "abstract": "  This paper presents a methodological framework for training, self-optimising,\nand self-organising surrogate models to approximate and speed up multiobjective\noptimisation of technical systems based on multiphysics simulations. At the\nhand of two real-world datasets, we illustrate that surrogate models can be\ntrained on relatively small amounts of data to approximate the underlying\nsimulations accurately. Including explainable AI techniques allow for\nhighlighting feature relevancy or dependencies and supporting the possible\nextension of the used datasets. One of the datasets was created for this paper\nand is made publicly available for the broader scientific community. Extensive\nexperiments combine four machine learning and deep learning algorithms with an\nevolutionary optimisation algorithm. The performance of the combined training\nand optimisation pipeline is evaluated by verifying the generated\nPareto-optimal results using the ground truth simulations. The results from our\npipeline and a comprehensive evaluation strategy show the potential for\nefficiently acquiring solution candidates in multiobjective optimisation tasks\nby reducing the number of simulations and conserving a higher prediction\naccuracy, i.e., with a MAPE score under 5% for one of the presented use cases.\n", "link": "http://arxiv.org/abs/2309.13179v2", "date": "2024-04-03", "relevancy": 1.9931, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5653}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4856}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4841}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Multi-Objective%20Optimization%20through%20Machine%0A%20%20Learning-Supported%20Multiphysics%20Simulation&body=Title%3A%20Enhancing%20Multi-Objective%20Optimization%20through%20Machine%0A%20%20Learning-Supported%20Multiphysics%20Simulation%0AAuthor%3A%20Diego%20Botache%20and%20Jens%20Decke%20and%20Winfried%20Ripken%20and%20Abhinay%20Dornipati%20and%20Franz%20G%C3%B6tz-Hahn%20and%20Mohamed%20Ayeb%20and%20Bernhard%20Sick%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20methodological%20framework%20for%20training%2C%20self-optimising%2C%0Aand%20self-organising%20surrogate%20models%20to%20approximate%20and%20speed%20up%20multiobjective%0Aoptimisation%20of%20technical%20systems%20based%20on%20multiphysics%20simulations.%20At%20the%0Ahand%20of%20two%20real-world%20datasets%2C%20we%20illustrate%20that%20surrogate%20models%20can%20be%0Atrained%20on%20relatively%20small%20amounts%20of%20data%20to%20approximate%20the%20underlying%0Asimulations%20accurately.%20Including%20explainable%20AI%20techniques%20allow%20for%0Ahighlighting%20feature%20relevancy%20or%20dependencies%20and%20supporting%20the%20possible%0Aextension%20of%20the%20used%20datasets.%20One%20of%20the%20datasets%20was%20created%20for%20this%20paper%0Aand%20is%20made%20publicly%20available%20for%20the%20broader%20scientific%20community.%20Extensive%0Aexperiments%20combine%20four%20machine%20learning%20and%20deep%20learning%20algorithms%20with%20an%0Aevolutionary%20optimisation%20algorithm.%20The%20performance%20of%20the%20combined%20training%0Aand%20optimisation%20pipeline%20is%20evaluated%20by%20verifying%20the%20generated%0APareto-optimal%20results%20using%20the%20ground%20truth%20simulations.%20The%20results%20from%20our%0Apipeline%20and%20a%20comprehensive%20evaluation%20strategy%20show%20the%20potential%20for%0Aefficiently%20acquiring%20solution%20candidates%20in%20multiobjective%20optimisation%20tasks%0Aby%20reducing%20the%20number%20of%20simulations%20and%20conserving%20a%20higher%20prediction%0Aaccuracy%2C%20i.e.%2C%20with%20a%20MAPE%20score%20under%205%25%20for%20one%20of%20the%20presented%20use%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.13179v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Multi-Objective%20Optimization%20through%20Machine%0A%20%20Learning-Supported%20Multiphysics%20Simulation&entry.906535625=Diego%20Botache%20and%20Jens%20Decke%20and%20Winfried%20Ripken%20and%20Abhinay%20Dornipati%20and%20Franz%20G%C3%B6tz-Hahn%20and%20Mohamed%20Ayeb%20and%20Bernhard%20Sick&entry.1292438233=%20%20This%20paper%20presents%20a%20methodological%20framework%20for%20training%2C%20self-optimising%2C%0Aand%20self-organising%20surrogate%20models%20to%20approximate%20and%20speed%20up%20multiobjective%0Aoptimisation%20of%20technical%20systems%20based%20on%20multiphysics%20simulations.%20At%20the%0Ahand%20of%20two%20real-world%20datasets%2C%20we%20illustrate%20that%20surrogate%20models%20can%20be%0Atrained%20on%20relatively%20small%20amounts%20of%20data%20to%20approximate%20the%20underlying%0Asimulations%20accurately.%20Including%20explainable%20AI%20techniques%20allow%20for%0Ahighlighting%20feature%20relevancy%20or%20dependencies%20and%20supporting%20the%20possible%0Aextension%20of%20the%20used%20datasets.%20One%20of%20the%20datasets%20was%20created%20for%20this%20paper%0Aand%20is%20made%20publicly%20available%20for%20the%20broader%20scientific%20community.%20Extensive%0Aexperiments%20combine%20four%20machine%20learning%20and%20deep%20learning%20algorithms%20with%20an%0Aevolutionary%20optimisation%20algorithm.%20The%20performance%20of%20the%20combined%20training%0Aand%20optimisation%20pipeline%20is%20evaluated%20by%20verifying%20the%20generated%0APareto-optimal%20results%20using%20the%20ground%20truth%20simulations.%20The%20results%20from%20our%0Apipeline%20and%20a%20comprehensive%20evaluation%20strategy%20show%20the%20potential%20for%0Aefficiently%20acquiring%20solution%20candidates%20in%20multiobjective%20optimisation%20tasks%0Aby%20reducing%20the%20number%20of%20simulations%20and%20conserving%20a%20higher%20prediction%0Aaccuracy%2C%20i.e.%2C%20with%20a%20MAPE%20score%20under%205%25%20for%20one%20of%20the%20presented%20use%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.13179v2&entry.124074799=Read"},
{"title": "Domain Generalization through Meta-Learning: A Survey", "author": "Arsham Gholamzadeh Khoee and Yinan Yu and Robert Feldt", "abstract": "  Deep neural networks (DNNs) have revolutionized artificial intelligence but\noften lack performance when faced with out-of-distribution (OOD) data, a common\nscenario due to the inevitable domain shifts in real-world applications. This\nlimitation stems from the common assumption that training and testing data\nshare the same distribution-an assumption frequently violated in practice.\nDespite their effectiveness with large amounts of data and computational power,\nDNNs struggle with distributional shifts and limited labeled data, leading to\noverfitting and poor generalization across various tasks and domains.\nMeta-learning presents a promising approach by employing algorithms that\nacquire transferable knowledge across various tasks for fast adaptation,\neliminating the need to learn each task from scratch. This survey paper delves\ninto the realm of meta-learning with a focus on its contribution to domain\ngeneralization. We first clarify the concept of meta-learning for domain\ngeneralization and introduce a novel taxonomy based on the feature extraction\nstrategy and the classifier learning methodology, offering a granular view of\nmethodologies. Through an exhaustive review of existing methods and underlying\ntheories, we map out the fundamentals of the field. Our survey provides\npractical insights and an informed discussion on promising research directions,\npaving the way for future innovation in meta-learning for domain\ngeneralization.\n", "link": "http://arxiv.org/abs/2404.02785v1", "date": "2024-04-03", "relevancy": 1.9918, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5157}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4886}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.484}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Domain%20Generalization%20through%20Meta-Learning%3A%20A%20Survey&body=Title%3A%20Domain%20Generalization%20through%20Meta-Learning%3A%20A%20Survey%0AAuthor%3A%20Arsham%20Gholamzadeh%20Khoee%20and%20Yinan%20Yu%20and%20Robert%20Feldt%0AAbstract%3A%20%20%20Deep%20neural%20networks%20%28DNNs%29%20have%20revolutionized%20artificial%20intelligence%20but%0Aoften%20lack%20performance%20when%20faced%20with%20out-of-distribution%20%28OOD%29%20data%2C%20a%20common%0Ascenario%20due%20to%20the%20inevitable%20domain%20shifts%20in%20real-world%20applications.%20This%0Alimitation%20stems%20from%20the%20common%20assumption%20that%20training%20and%20testing%20data%0Ashare%20the%20same%20distribution-an%20assumption%20frequently%20violated%20in%20practice.%0ADespite%20their%20effectiveness%20with%20large%20amounts%20of%20data%20and%20computational%20power%2C%0ADNNs%20struggle%20with%20distributional%20shifts%20and%20limited%20labeled%20data%2C%20leading%20to%0Aoverfitting%20and%20poor%20generalization%20across%20various%20tasks%20and%20domains.%0AMeta-learning%20presents%20a%20promising%20approach%20by%20employing%20algorithms%20that%0Aacquire%20transferable%20knowledge%20across%20various%20tasks%20for%20fast%20adaptation%2C%0Aeliminating%20the%20need%20to%20learn%20each%20task%20from%20scratch.%20This%20survey%20paper%20delves%0Ainto%20the%20realm%20of%20meta-learning%20with%20a%20focus%20on%20its%20contribution%20to%20domain%0Ageneralization.%20We%20first%20clarify%20the%20concept%20of%20meta-learning%20for%20domain%0Ageneralization%20and%20introduce%20a%20novel%20taxonomy%20based%20on%20the%20feature%20extraction%0Astrategy%20and%20the%20classifier%20learning%20methodology%2C%20offering%20a%20granular%20view%20of%0Amethodologies.%20Through%20an%20exhaustive%20review%20of%20existing%20methods%20and%20underlying%0Atheories%2C%20we%20map%20out%20the%20fundamentals%20of%20the%20field.%20Our%20survey%20provides%0Apractical%20insights%20and%20an%20informed%20discussion%20on%20promising%20research%20directions%2C%0Apaving%20the%20way%20for%20future%20innovation%20in%20meta-learning%20for%20domain%0Ageneralization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02785v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain%20Generalization%20through%20Meta-Learning%3A%20A%20Survey&entry.906535625=Arsham%20Gholamzadeh%20Khoee%20and%20Yinan%20Yu%20and%20Robert%20Feldt&entry.1292438233=%20%20Deep%20neural%20networks%20%28DNNs%29%20have%20revolutionized%20artificial%20intelligence%20but%0Aoften%20lack%20performance%20when%20faced%20with%20out-of-distribution%20%28OOD%29%20data%2C%20a%20common%0Ascenario%20due%20to%20the%20inevitable%20domain%20shifts%20in%20real-world%20applications.%20This%0Alimitation%20stems%20from%20the%20common%20assumption%20that%20training%20and%20testing%20data%0Ashare%20the%20same%20distribution-an%20assumption%20frequently%20violated%20in%20practice.%0ADespite%20their%20effectiveness%20with%20large%20amounts%20of%20data%20and%20computational%20power%2C%0ADNNs%20struggle%20with%20distributional%20shifts%20and%20limited%20labeled%20data%2C%20leading%20to%0Aoverfitting%20and%20poor%20generalization%20across%20various%20tasks%20and%20domains.%0AMeta-learning%20presents%20a%20promising%20approach%20by%20employing%20algorithms%20that%0Aacquire%20transferable%20knowledge%20across%20various%20tasks%20for%20fast%20adaptation%2C%0Aeliminating%20the%20need%20to%20learn%20each%20task%20from%20scratch.%20This%20survey%20paper%20delves%0Ainto%20the%20realm%20of%20meta-learning%20with%20a%20focus%20on%20its%20contribution%20to%20domain%0Ageneralization.%20We%20first%20clarify%20the%20concept%20of%20meta-learning%20for%20domain%0Ageneralization%20and%20introduce%20a%20novel%20taxonomy%20based%20on%20the%20feature%20extraction%0Astrategy%20and%20the%20classifier%20learning%20methodology%2C%20offering%20a%20granular%20view%20of%0Amethodologies.%20Through%20an%20exhaustive%20review%20of%20existing%20methods%20and%20underlying%0Atheories%2C%20we%20map%20out%20the%20fundamentals%20of%20the%20field.%20Our%20survey%20provides%0Apractical%20insights%20and%20an%20informed%20discussion%20on%20promising%20research%20directions%2C%0Apaving%20the%20way%20for%20future%20innovation%20in%20meta-learning%20for%20domain%0Ageneralization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02785v1&entry.124074799=Read"},
{"title": "Adversarial Attacks and Dimensionality in Text Classifiers", "author": "Nandish Chattopadhyay and Atreya Goswami and Anupam Chattopadhyay", "abstract": "  Adversarial attacks on machine learning algorithms have been a key deterrent\nto the adoption of AI in many real-world use cases. They significantly\nundermine the ability of high-performance neural networks by forcing\nmisclassifications. These attacks introduce minute and structured perturbations\nor alterations in the test samples, imperceptible to human annotators in\ngeneral, but trained neural networks and other models are sensitive to it.\nHistorically, adversarial attacks have been first identified and studied in the\ndomain of image processing. In this paper, we study adversarial examples in the\nfield of natural language processing, specifically text classification tasks.\nWe investigate the reasons for adversarial vulnerability, particularly in\nrelation to the inherent dimensionality of the model. Our key finding is that\nthere is a very strong correlation between the embedding dimensionality of the\nadversarial samples and their effectiveness on models tuned with input samples\nwith same embedding dimension. We utilize this sensitivity to design an\nadversarial defense mechanism. We use ensemble models of varying inherent\ndimensionality to thwart the attacks. This is tested on multiple datasets for\nits efficacy in providing robustness. We also study the problem of measuring\nadversarial perturbation using different distance metrics. For all of the\naforementioned studies, we have run tests on multiple models with varying\ndimensionality and used a word-vector level adversarial attack to substantiate\nthe findings.\n", "link": "http://arxiv.org/abs/2404.02660v1", "date": "2024-04-03", "relevancy": 1.9906, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5204}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4926}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4533}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Attacks%20and%20Dimensionality%20in%20Text%20Classifiers&body=Title%3A%20Adversarial%20Attacks%20and%20Dimensionality%20in%20Text%20Classifiers%0AAuthor%3A%20Nandish%20Chattopadhyay%20and%20Atreya%20Goswami%20and%20Anupam%20Chattopadhyay%0AAbstract%3A%20%20%20Adversarial%20attacks%20on%20machine%20learning%20algorithms%20have%20been%20a%20key%20deterrent%0Ato%20the%20adoption%20of%20AI%20in%20many%20real-world%20use%20cases.%20They%20significantly%0Aundermine%20the%20ability%20of%20high-performance%20neural%20networks%20by%20forcing%0Amisclassifications.%20These%20attacks%20introduce%20minute%20and%20structured%20perturbations%0Aor%20alterations%20in%20the%20test%20samples%2C%20imperceptible%20to%20human%20annotators%20in%0Ageneral%2C%20but%20trained%20neural%20networks%20and%20other%20models%20are%20sensitive%20to%20it.%0AHistorically%2C%20adversarial%20attacks%20have%20been%20first%20identified%20and%20studied%20in%20the%0Adomain%20of%20image%20processing.%20In%20this%20paper%2C%20we%20study%20adversarial%20examples%20in%20the%0Afield%20of%20natural%20language%20processing%2C%20specifically%20text%20classification%20tasks.%0AWe%20investigate%20the%20reasons%20for%20adversarial%20vulnerability%2C%20particularly%20in%0Arelation%20to%20the%20inherent%20dimensionality%20of%20the%20model.%20Our%20key%20finding%20is%20that%0Athere%20is%20a%20very%20strong%20correlation%20between%20the%20embedding%20dimensionality%20of%20the%0Aadversarial%20samples%20and%20their%20effectiveness%20on%20models%20tuned%20with%20input%20samples%0Awith%20same%20embedding%20dimension.%20We%20utilize%20this%20sensitivity%20to%20design%20an%0Aadversarial%20defense%20mechanism.%20We%20use%20ensemble%20models%20of%20varying%20inherent%0Adimensionality%20to%20thwart%20the%20attacks.%20This%20is%20tested%20on%20multiple%20datasets%20for%0Aits%20efficacy%20in%20providing%20robustness.%20We%20also%20study%20the%20problem%20of%20measuring%0Aadversarial%20perturbation%20using%20different%20distance%20metrics.%20For%20all%20of%20the%0Aaforementioned%20studies%2C%20we%20have%20run%20tests%20on%20multiple%20models%20with%20varying%0Adimensionality%20and%20used%20a%20word-vector%20level%20adversarial%20attack%20to%20substantiate%0Athe%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02660v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Attacks%20and%20Dimensionality%20in%20Text%20Classifiers&entry.906535625=Nandish%20Chattopadhyay%20and%20Atreya%20Goswami%20and%20Anupam%20Chattopadhyay&entry.1292438233=%20%20Adversarial%20attacks%20on%20machine%20learning%20algorithms%20have%20been%20a%20key%20deterrent%0Ato%20the%20adoption%20of%20AI%20in%20many%20real-world%20use%20cases.%20They%20significantly%0Aundermine%20the%20ability%20of%20high-performance%20neural%20networks%20by%20forcing%0Amisclassifications.%20These%20attacks%20introduce%20minute%20and%20structured%20perturbations%0Aor%20alterations%20in%20the%20test%20samples%2C%20imperceptible%20to%20human%20annotators%20in%0Ageneral%2C%20but%20trained%20neural%20networks%20and%20other%20models%20are%20sensitive%20to%20it.%0AHistorically%2C%20adversarial%20attacks%20have%20been%20first%20identified%20and%20studied%20in%20the%0Adomain%20of%20image%20processing.%20In%20this%20paper%2C%20we%20study%20adversarial%20examples%20in%20the%0Afield%20of%20natural%20language%20processing%2C%20specifically%20text%20classification%20tasks.%0AWe%20investigate%20the%20reasons%20for%20adversarial%20vulnerability%2C%20particularly%20in%0Arelation%20to%20the%20inherent%20dimensionality%20of%20the%20model.%20Our%20key%20finding%20is%20that%0Athere%20is%20a%20very%20strong%20correlation%20between%20the%20embedding%20dimensionality%20of%20the%0Aadversarial%20samples%20and%20their%20effectiveness%20on%20models%20tuned%20with%20input%20samples%0Awith%20same%20embedding%20dimension.%20We%20utilize%20this%20sensitivity%20to%20design%20an%0Aadversarial%20defense%20mechanism.%20We%20use%20ensemble%20models%20of%20varying%20inherent%0Adimensionality%20to%20thwart%20the%20attacks.%20This%20is%20tested%20on%20multiple%20datasets%20for%0Aits%20efficacy%20in%20providing%20robustness.%20We%20also%20study%20the%20problem%20of%20measuring%0Aadversarial%20perturbation%20using%20different%20distance%20metrics.%20For%20all%20of%20the%0Aaforementioned%20studies%2C%20we%20have%20run%20tests%20on%20multiple%20models%20with%20varying%0Adimensionality%20and%20used%20a%20word-vector%20level%20adversarial%20attack%20to%20substantiate%0Athe%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02660v1&entry.124074799=Read"},
{"title": "RDumb: A simple approach that questions our progress in continual\n  test-time adaptation", "author": "Ori Press and Steffen Schneider and Matthias K\u00fcmmerer and Matthias Bethge", "abstract": "  Test-Time Adaptation (TTA) allows to update pre-trained models to changing\ndata distributions at deployment time. While early work tested these algorithms\nfor individual fixed distribution shifts, recent work proposed and applied\nmethods for continual adaptation over long timescales. To examine the reported\nprogress in the field, we propose the Continually Changing Corruptions (CCC)\nbenchmark to measure asymptotic performance of TTA techniques. We find that\neventually all but one state-of-the-art methods collapse and perform worse than\na non-adapting model, including models specifically proposed to be robust to\nperformance collapse. In addition, we introduce a simple baseline, \"RDumb\",\nthat periodically resets the model to its pretrained state. RDumb performs\nbetter or on par with the previously proposed state-of-the-art in all\nconsidered benchmarks. Our results show that previous TTA approaches are\nneither effective at regularizing adaptation to avoid collapse nor able to\noutperform a simplistic resetting strategy.\n", "link": "http://arxiv.org/abs/2306.05401v3", "date": "2024-04-03", "relevancy": 1.9866, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.512}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4876}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4807}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RDumb%3A%20A%20simple%20approach%20that%20questions%20our%20progress%20in%20continual%0A%20%20test-time%20adaptation&body=Title%3A%20RDumb%3A%20A%20simple%20approach%20that%20questions%20our%20progress%20in%20continual%0A%20%20test-time%20adaptation%0AAuthor%3A%20Ori%20Press%20and%20Steffen%20Schneider%20and%20Matthias%20K%C3%BCmmerer%20and%20Matthias%20Bethge%0AAbstract%3A%20%20%20Test-Time%20Adaptation%20%28TTA%29%20allows%20to%20update%20pre-trained%20models%20to%20changing%0Adata%20distributions%20at%20deployment%20time.%20While%20early%20work%20tested%20these%20algorithms%0Afor%20individual%20fixed%20distribution%20shifts%2C%20recent%20work%20proposed%20and%20applied%0Amethods%20for%20continual%20adaptation%20over%20long%20timescales.%20To%20examine%20the%20reported%0Aprogress%20in%20the%20field%2C%20we%20propose%20the%20Continually%20Changing%20Corruptions%20%28CCC%29%0Abenchmark%20to%20measure%20asymptotic%20performance%20of%20TTA%20techniques.%20We%20find%20that%0Aeventually%20all%20but%20one%20state-of-the-art%20methods%20collapse%20and%20perform%20worse%20than%0Aa%20non-adapting%20model%2C%20including%20models%20specifically%20proposed%20to%20be%20robust%20to%0Aperformance%20collapse.%20In%20addition%2C%20we%20introduce%20a%20simple%20baseline%2C%20%22RDumb%22%2C%0Athat%20periodically%20resets%20the%20model%20to%20its%20pretrained%20state.%20RDumb%20performs%0Abetter%20or%20on%20par%20with%20the%20previously%20proposed%20state-of-the-art%20in%20all%0Aconsidered%20benchmarks.%20Our%20results%20show%20that%20previous%20TTA%20approaches%20are%0Aneither%20effective%20at%20regularizing%20adaptation%20to%20avoid%20collapse%20nor%20able%20to%0Aoutperform%20a%20simplistic%20resetting%20strategy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.05401v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RDumb%3A%20A%20simple%20approach%20that%20questions%20our%20progress%20in%20continual%0A%20%20test-time%20adaptation&entry.906535625=Ori%20Press%20and%20Steffen%20Schneider%20and%20Matthias%20K%C3%BCmmerer%20and%20Matthias%20Bethge&entry.1292438233=%20%20Test-Time%20Adaptation%20%28TTA%29%20allows%20to%20update%20pre-trained%20models%20to%20changing%0Adata%20distributions%20at%20deployment%20time.%20While%20early%20work%20tested%20these%20algorithms%0Afor%20individual%20fixed%20distribution%20shifts%2C%20recent%20work%20proposed%20and%20applied%0Amethods%20for%20continual%20adaptation%20over%20long%20timescales.%20To%20examine%20the%20reported%0Aprogress%20in%20the%20field%2C%20we%20propose%20the%20Continually%20Changing%20Corruptions%20%28CCC%29%0Abenchmark%20to%20measure%20asymptotic%20performance%20of%20TTA%20techniques.%20We%20find%20that%0Aeventually%20all%20but%20one%20state-of-the-art%20methods%20collapse%20and%20perform%20worse%20than%0Aa%20non-adapting%20model%2C%20including%20models%20specifically%20proposed%20to%20be%20robust%20to%0Aperformance%20collapse.%20In%20addition%2C%20we%20introduce%20a%20simple%20baseline%2C%20%22RDumb%22%2C%0Athat%20periodically%20resets%20the%20model%20to%20its%20pretrained%20state.%20RDumb%20performs%0Abetter%20or%20on%20par%20with%20the%20previously%20proposed%20state-of-the-art%20in%20all%0Aconsidered%20benchmarks.%20Our%20results%20show%20that%20previous%20TTA%20approaches%20are%0Aneither%20effective%20at%20regularizing%20adaptation%20to%20avoid%20collapse%20nor%20able%20to%0Aoutperform%20a%20simplistic%20resetting%20strategy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.05401v3&entry.124074799=Read"},
{"title": "One Stack to Rule them All: To Drive Automated Vehicles, and Reach for\n  the 4th level", "author": "Sven Ochs and Jens Doll and Daniel Grimm and Tobias Fleck and Marc Heinrich and Stefan Orf and Albert Schotschneider and Helen Gremmelmaier and Rupert Polley and Svetlana Pavlitska and Maximilian Zipfl and Helen Schneider and Ferdinand M\u00fctsch and Daniel Bogdoll and Florian Kuhnt and Philip Sch\u00f6rner and Marc Ren\u00e9 Zofka and J. Marius Z\u00f6llner", "abstract": "  Most automated driving functions are designed for a specific task or vehicle.\nMost often, the underlying architecture is fixed to specific algorithms to\nincrease performance. Therefore, it is not possible to deploy new modules and\nalgorithms easily. In this paper, we present our automated driving stack which\ncombines both scalability and adaptability. Due to the modular design, our\nstack allows for a fast integration and testing of novel and state-of-the-art\nresearch approaches. Furthermore, it is flexible to be used for our different\ntesting vehicles, including modified EasyMile EZ10 shuttles and different\npassenger cars. These vehicles differ in multiple ways, e.g. sensor setups,\ncontrol systems, maximum speed, or steering angle limitations. Finally, our\nstack is deployed in real world environments, including passenger transport in\nurban areas. Our stack includes all components needed for operating an\nautonomous vehicle, including localization, perception, planning, controller,\nand additional safety modules. Our stack is developed, tested, and evaluated in\nreal world traffic in multiple test sites, including the Test Area Autonomous\nDriving Baden-W\\\"urttemberg.\n", "link": "http://arxiv.org/abs/2404.02645v1", "date": "2024-04-03", "relevancy": 1.9788, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.522}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4803}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4627}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20One%20Stack%20to%20Rule%20them%20All%3A%20To%20Drive%20Automated%20Vehicles%2C%20and%20Reach%20for%0A%20%20the%204th%20level&body=Title%3A%20One%20Stack%20to%20Rule%20them%20All%3A%20To%20Drive%20Automated%20Vehicles%2C%20and%20Reach%20for%0A%20%20the%204th%20level%0AAuthor%3A%20Sven%20Ochs%20and%20Jens%20Doll%20and%20Daniel%20Grimm%20and%20Tobias%20Fleck%20and%20Marc%20Heinrich%20and%20Stefan%20Orf%20and%20Albert%20Schotschneider%20and%20Helen%20Gremmelmaier%20and%20Rupert%20Polley%20and%20Svetlana%20Pavlitska%20and%20Maximilian%20Zipfl%20and%20Helen%20Schneider%20and%20Ferdinand%20M%C3%BCtsch%20and%20Daniel%20Bogdoll%20and%20Florian%20Kuhnt%20and%20Philip%20Sch%C3%B6rner%20and%20Marc%20Ren%C3%A9%20Zofka%20and%20J.%20Marius%20Z%C3%B6llner%0AAbstract%3A%20%20%20Most%20automated%20driving%20functions%20are%20designed%20for%20a%20specific%20task%20or%20vehicle.%0AMost%20often%2C%20the%20underlying%20architecture%20is%20fixed%20to%20specific%20algorithms%20to%0Aincrease%20performance.%20Therefore%2C%20it%20is%20not%20possible%20to%20deploy%20new%20modules%20and%0Aalgorithms%20easily.%20In%20this%20paper%2C%20we%20present%20our%20automated%20driving%20stack%20which%0Acombines%20both%20scalability%20and%20adaptability.%20Due%20to%20the%20modular%20design%2C%20our%0Astack%20allows%20for%20a%20fast%20integration%20and%20testing%20of%20novel%20and%20state-of-the-art%0Aresearch%20approaches.%20Furthermore%2C%20it%20is%20flexible%20to%20be%20used%20for%20our%20different%0Atesting%20vehicles%2C%20including%20modified%20EasyMile%20EZ10%20shuttles%20and%20different%0Apassenger%20cars.%20These%20vehicles%20differ%20in%20multiple%20ways%2C%20e.g.%20sensor%20setups%2C%0Acontrol%20systems%2C%20maximum%20speed%2C%20or%20steering%20angle%20limitations.%20Finally%2C%20our%0Astack%20is%20deployed%20in%20real%20world%20environments%2C%20including%20passenger%20transport%20in%0Aurban%20areas.%20Our%20stack%20includes%20all%20components%20needed%20for%20operating%20an%0Aautonomous%20vehicle%2C%20including%20localization%2C%20perception%2C%20planning%2C%20controller%2C%0Aand%20additional%20safety%20modules.%20Our%20stack%20is%20developed%2C%20tested%2C%20and%20evaluated%20in%0Areal%20world%20traffic%20in%20multiple%20test%20sites%2C%20including%20the%20Test%20Area%20Autonomous%0ADriving%20Baden-W%5C%22urttemberg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02645v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One%20Stack%20to%20Rule%20them%20All%3A%20To%20Drive%20Automated%20Vehicles%2C%20and%20Reach%20for%0A%20%20the%204th%20level&entry.906535625=Sven%20Ochs%20and%20Jens%20Doll%20and%20Daniel%20Grimm%20and%20Tobias%20Fleck%20and%20Marc%20Heinrich%20and%20Stefan%20Orf%20and%20Albert%20Schotschneider%20and%20Helen%20Gremmelmaier%20and%20Rupert%20Polley%20and%20Svetlana%20Pavlitska%20and%20Maximilian%20Zipfl%20and%20Helen%20Schneider%20and%20Ferdinand%20M%C3%BCtsch%20and%20Daniel%20Bogdoll%20and%20Florian%20Kuhnt%20and%20Philip%20Sch%C3%B6rner%20and%20Marc%20Ren%C3%A9%20Zofka%20and%20J.%20Marius%20Z%C3%B6llner&entry.1292438233=%20%20Most%20automated%20driving%20functions%20are%20designed%20for%20a%20specific%20task%20or%20vehicle.%0AMost%20often%2C%20the%20underlying%20architecture%20is%20fixed%20to%20specific%20algorithms%20to%0Aincrease%20performance.%20Therefore%2C%20it%20is%20not%20possible%20to%20deploy%20new%20modules%20and%0Aalgorithms%20easily.%20In%20this%20paper%2C%20we%20present%20our%20automated%20driving%20stack%20which%0Acombines%20both%20scalability%20and%20adaptability.%20Due%20to%20the%20modular%20design%2C%20our%0Astack%20allows%20for%20a%20fast%20integration%20and%20testing%20of%20novel%20and%20state-of-the-art%0Aresearch%20approaches.%20Furthermore%2C%20it%20is%20flexible%20to%20be%20used%20for%20our%20different%0Atesting%20vehicles%2C%20including%20modified%20EasyMile%20EZ10%20shuttles%20and%20different%0Apassenger%20cars.%20These%20vehicles%20differ%20in%20multiple%20ways%2C%20e.g.%20sensor%20setups%2C%0Acontrol%20systems%2C%20maximum%20speed%2C%20or%20steering%20angle%20limitations.%20Finally%2C%20our%0Astack%20is%20deployed%20in%20real%20world%20environments%2C%20including%20passenger%20transport%20in%0Aurban%20areas.%20Our%20stack%20includes%20all%20components%20needed%20for%20operating%20an%0Aautonomous%20vehicle%2C%20including%20localization%2C%20perception%2C%20planning%2C%20controller%2C%0Aand%20additional%20safety%20modules.%20Our%20stack%20is%20developed%2C%20tested%2C%20and%20evaluated%20in%0Areal%20world%20traffic%20in%20multiple%20test%20sites%2C%20including%20the%20Test%20Area%20Autonomous%0ADriving%20Baden-W%5C%22urttemberg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02645v1&entry.124074799=Read"},
{"title": "Attention is Naturally Sparse with Gaussian Distributed Input", "author": "Yichuan Deng and Zhao Song and Chiwun Yang", "abstract": "  The computational intensity of Large Language Models (LLMs) is a critical\nbottleneck, primarily due to the $O(n^2)$ complexity of the attention mechanism\nin transformer architectures. Addressing this, sparse attention emerges as a\nkey innovation, aiming to reduce computational load while maintaining model\nperformance. This study presents a rigorous theoretical analysis of the\nsparsity in attention scores within LLMs, particularly under the framework of\nGaussian inputs. By establishing a set of foundational assumptions and\nemploying a methodical theoretical approach, we unravel the intrinsic\ncharacteristics of attention score sparsity and its implications on\ncomputational efficiency. Our main contribution lies in providing a detailed\ntheoretical examination of how sparsity manifests in attention mechanisms,\noffering insights into the potential trade-offs between computational savings\nand model effectiveness. This work not only advances our understanding of\nsparse attention but also provides a scaffold for future research in optimizing\nthe computational frameworks of LLMs, paving the way for more scalable and\nefficient AI systems.\n", "link": "http://arxiv.org/abs/2404.02690v1", "date": "2024-04-03", "relevancy": 1.9719, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5339}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4718}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4605}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Attention%20is%20Naturally%20Sparse%20with%20Gaussian%20Distributed%20Input&body=Title%3A%20Attention%20is%20Naturally%20Sparse%20with%20Gaussian%20Distributed%20Input%0AAuthor%3A%20Yichuan%20Deng%20and%20Zhao%20Song%20and%20Chiwun%20Yang%0AAbstract%3A%20%20%20The%20computational%20intensity%20of%20Large%20Language%20Models%20%28LLMs%29%20is%20a%20critical%0Abottleneck%2C%20primarily%20due%20to%20the%20%24O%28n%5E2%29%24%20complexity%20of%20the%20attention%20mechanism%0Ain%20transformer%20architectures.%20Addressing%20this%2C%20sparse%20attention%20emerges%20as%20a%0Akey%20innovation%2C%20aiming%20to%20reduce%20computational%20load%20while%20maintaining%20model%0Aperformance.%20This%20study%20presents%20a%20rigorous%20theoretical%20analysis%20of%20the%0Asparsity%20in%20attention%20scores%20within%20LLMs%2C%20particularly%20under%20the%20framework%20of%0AGaussian%20inputs.%20By%20establishing%20a%20set%20of%20foundational%20assumptions%20and%0Aemploying%20a%20methodical%20theoretical%20approach%2C%20we%20unravel%20the%20intrinsic%0Acharacteristics%20of%20attention%20score%20sparsity%20and%20its%20implications%20on%0Acomputational%20efficiency.%20Our%20main%20contribution%20lies%20in%20providing%20a%20detailed%0Atheoretical%20examination%20of%20how%20sparsity%20manifests%20in%20attention%20mechanisms%2C%0Aoffering%20insights%20into%20the%20potential%20trade-offs%20between%20computational%20savings%0Aand%20model%20effectiveness.%20This%20work%20not%20only%20advances%20our%20understanding%20of%0Asparse%20attention%20but%20also%20provides%20a%20scaffold%20for%20future%20research%20in%20optimizing%0Athe%20computational%20frameworks%20of%20LLMs%2C%20paving%20the%20way%20for%20more%20scalable%20and%0Aefficient%20AI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02690v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention%20is%20Naturally%20Sparse%20with%20Gaussian%20Distributed%20Input&entry.906535625=Yichuan%20Deng%20and%20Zhao%20Song%20and%20Chiwun%20Yang&entry.1292438233=%20%20The%20computational%20intensity%20of%20Large%20Language%20Models%20%28LLMs%29%20is%20a%20critical%0Abottleneck%2C%20primarily%20due%20to%20the%20%24O%28n%5E2%29%24%20complexity%20of%20the%20attention%20mechanism%0Ain%20transformer%20architectures.%20Addressing%20this%2C%20sparse%20attention%20emerges%20as%20a%0Akey%20innovation%2C%20aiming%20to%20reduce%20computational%20load%20while%20maintaining%20model%0Aperformance.%20This%20study%20presents%20a%20rigorous%20theoretical%20analysis%20of%20the%0Asparsity%20in%20attention%20scores%20within%20LLMs%2C%20particularly%20under%20the%20framework%20of%0AGaussian%20inputs.%20By%20establishing%20a%20set%20of%20foundational%20assumptions%20and%0Aemploying%20a%20methodical%20theoretical%20approach%2C%20we%20unravel%20the%20intrinsic%0Acharacteristics%20of%20attention%20score%20sparsity%20and%20its%20implications%20on%0Acomputational%20efficiency.%20Our%20main%20contribution%20lies%20in%20providing%20a%20detailed%0Atheoretical%20examination%20of%20how%20sparsity%20manifests%20in%20attention%20mechanisms%2C%0Aoffering%20insights%20into%20the%20potential%20trade-offs%20between%20computational%20savings%0Aand%20model%20effectiveness.%20This%20work%20not%20only%20advances%20our%20understanding%20of%0Asparse%20attention%20but%20also%20provides%20a%20scaffold%20for%20future%20research%20in%20optimizing%0Athe%20computational%20frameworks%20of%20LLMs%2C%20paving%20the%20way%20for%20more%20scalable%20and%0Aefficient%20AI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02690v1&entry.124074799=Read"},
{"title": "eWand: A calibration framework for wide baseline frame-based and\n  event-based camera systems", "author": "Thomas Gossard and Andreas Ziegler and Levin Kolmar and Jonas Tebbe and Andreas Zell", "abstract": "  Accurate calibration is crucial for using multiple cameras to triangulate the\nposition of objects precisely. However, it is also a time-consuming process\nthat needs to be repeated for every displacement of the cameras. The standard\napproach is to use a printed pattern with known geometry to estimate the\nintrinsic and extrinsic parameters of the cameras. The same idea can be applied\nto event-based cameras, though it requires extra work. By using frame\nreconstruction from events, a printed pattern can be detected. A blinking\npattern can also be displayed on a screen. Then, the pattern can be directly\ndetected from the events. Such calibration methods can provide accurate\nintrinsic calibration for both frame- and event-based cameras. However, using\n2D patterns has several limitations for multi-camera extrinsic calibration,\nwith cameras possessing highly different points of view and a wide baseline.\nThe 2D pattern can only be detected from one direction and needs to be of\nsignificant size to compensate for its distance to the camera. This makes the\nextrinsic calibration time-consuming and cumbersome. To overcome these\nlimitations, we propose eWand, a new method that uses blinking LEDs inside\nopaque spheres instead of a printed or displayed pattern. Our method provides a\nfaster, easier-to-use extrinsic calibration approach that maintains high\naccuracy for both event- and frame-based cameras.\n", "link": "http://arxiv.org/abs/2309.12685v2", "date": "2024-04-03", "relevancy": 1.9621, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5029}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4941}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4508}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20eWand%3A%20A%20calibration%20framework%20for%20wide%20baseline%20frame-based%20and%0A%20%20event-based%20camera%20systems&body=Title%3A%20eWand%3A%20A%20calibration%20framework%20for%20wide%20baseline%20frame-based%20and%0A%20%20event-based%20camera%20systems%0AAuthor%3A%20Thomas%20Gossard%20and%20Andreas%20Ziegler%20and%20Levin%20Kolmar%20and%20Jonas%20Tebbe%20and%20Andreas%20Zell%0AAbstract%3A%20%20%20Accurate%20calibration%20is%20crucial%20for%20using%20multiple%20cameras%20to%20triangulate%20the%0Aposition%20of%20objects%20precisely.%20However%2C%20it%20is%20also%20a%20time-consuming%20process%0Athat%20needs%20to%20be%20repeated%20for%20every%20displacement%20of%20the%20cameras.%20The%20standard%0Aapproach%20is%20to%20use%20a%20printed%20pattern%20with%20known%20geometry%20to%20estimate%20the%0Aintrinsic%20and%20extrinsic%20parameters%20of%20the%20cameras.%20The%20same%20idea%20can%20be%20applied%0Ato%20event-based%20cameras%2C%20though%20it%20requires%20extra%20work.%20By%20using%20frame%0Areconstruction%20from%20events%2C%20a%20printed%20pattern%20can%20be%20detected.%20A%20blinking%0Apattern%20can%20also%20be%20displayed%20on%20a%20screen.%20Then%2C%20the%20pattern%20can%20be%20directly%0Adetected%20from%20the%20events.%20Such%20calibration%20methods%20can%20provide%20accurate%0Aintrinsic%20calibration%20for%20both%20frame-%20and%20event-based%20cameras.%20However%2C%20using%0A2D%20patterns%20has%20several%20limitations%20for%20multi-camera%20extrinsic%20calibration%2C%0Awith%20cameras%20possessing%20highly%20different%20points%20of%20view%20and%20a%20wide%20baseline.%0AThe%202D%20pattern%20can%20only%20be%20detected%20from%20one%20direction%20and%20needs%20to%20be%20of%0Asignificant%20size%20to%20compensate%20for%20its%20distance%20to%20the%20camera.%20This%20makes%20the%0Aextrinsic%20calibration%20time-consuming%20and%20cumbersome.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20eWand%2C%20a%20new%20method%20that%20uses%20blinking%20LEDs%20inside%0Aopaque%20spheres%20instead%20of%20a%20printed%20or%20displayed%20pattern.%20Our%20method%20provides%20a%0Afaster%2C%20easier-to-use%20extrinsic%20calibration%20approach%20that%20maintains%20high%0Aaccuracy%20for%20both%20event-%20and%20frame-based%20cameras.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.12685v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=eWand%3A%20A%20calibration%20framework%20for%20wide%20baseline%20frame-based%20and%0A%20%20event-based%20camera%20systems&entry.906535625=Thomas%20Gossard%20and%20Andreas%20Ziegler%20and%20Levin%20Kolmar%20and%20Jonas%20Tebbe%20and%20Andreas%20Zell&entry.1292438233=%20%20Accurate%20calibration%20is%20crucial%20for%20using%20multiple%20cameras%20to%20triangulate%20the%0Aposition%20of%20objects%20precisely.%20However%2C%20it%20is%20also%20a%20time-consuming%20process%0Athat%20needs%20to%20be%20repeated%20for%20every%20displacement%20of%20the%20cameras.%20The%20standard%0Aapproach%20is%20to%20use%20a%20printed%20pattern%20with%20known%20geometry%20to%20estimate%20the%0Aintrinsic%20and%20extrinsic%20parameters%20of%20the%20cameras.%20The%20same%20idea%20can%20be%20applied%0Ato%20event-based%20cameras%2C%20though%20it%20requires%20extra%20work.%20By%20using%20frame%0Areconstruction%20from%20events%2C%20a%20printed%20pattern%20can%20be%20detected.%20A%20blinking%0Apattern%20can%20also%20be%20displayed%20on%20a%20screen.%20Then%2C%20the%20pattern%20can%20be%20directly%0Adetected%20from%20the%20events.%20Such%20calibration%20methods%20can%20provide%20accurate%0Aintrinsic%20calibration%20for%20both%20frame-%20and%20event-based%20cameras.%20However%2C%20using%0A2D%20patterns%20has%20several%20limitations%20for%20multi-camera%20extrinsic%20calibration%2C%0Awith%20cameras%20possessing%20highly%20different%20points%20of%20view%20and%20a%20wide%20baseline.%0AThe%202D%20pattern%20can%20only%20be%20detected%20from%20one%20direction%20and%20needs%20to%20be%20of%0Asignificant%20size%20to%20compensate%20for%20its%20distance%20to%20the%20camera.%20This%20makes%20the%0Aextrinsic%20calibration%20time-consuming%20and%20cumbersome.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20eWand%2C%20a%20new%20method%20that%20uses%20blinking%20LEDs%20inside%0Aopaque%20spheres%20instead%20of%20a%20printed%20or%20displayed%20pattern.%20Our%20method%20provides%20a%0Afaster%2C%20easier-to-use%20extrinsic%20calibration%20approach%20that%20maintains%20high%0Aaccuracy%20for%20both%20event-%20and%20frame-based%20cameras.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.12685v2&entry.124074799=Read"},
{"title": "Off-Policy Correction For Multi-Agent Reinforcement Learning", "author": "Micha\u0142 Zawalski and B\u0142a\u017cej Osi\u0144ski and Henryk Michalewski and Piotr Mi\u0142o\u015b", "abstract": "  Multi-agent reinforcement learning (MARL) provides a framework for problems\ninvolving multiple interacting agents. Despite apparent similarity to the\nsingle-agent case, multi-agent problems are often harder to train and analyze\ntheoretically. In this work, we propose MA-Trace, a new on-policy actor-critic\nalgorithm, which extends V-Trace to the MARL setting. The key advantage of our\nalgorithm is its high scalability in a multi-worker setting. To this end,\nMA-Trace utilizes importance sampling as an off-policy correction method, which\nallows distributing the computations with no impact on the quality of training.\nFurthermore, our algorithm is theoretically grounded - we prove a fixed-point\ntheorem that guarantees convergence. We evaluate the algorithm extensively on\nthe StarCraft Multi-Agent Challenge, a standard benchmark for multi-agent\nalgorithms. MA-Trace achieves high performance on all its tasks and exceeds\nstate-of-the-art results on some of them.\n", "link": "http://arxiv.org/abs/2111.11229v3", "date": "2024-04-03", "relevancy": 1.9611, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5025}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4947}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.481}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Off-Policy%20Correction%20For%20Multi-Agent%20Reinforcement%20Learning&body=Title%3A%20Off-Policy%20Correction%20For%20Multi-Agent%20Reinforcement%20Learning%0AAuthor%3A%20Micha%C5%82%20Zawalski%20and%20B%C5%82a%C5%BCej%20Osi%C5%84ski%20and%20Henryk%20Michalewski%20and%20Piotr%20Mi%C5%82o%C5%9B%0AAbstract%3A%20%20%20Multi-agent%20reinforcement%20learning%20%28MARL%29%20provides%20a%20framework%20for%20problems%0Ainvolving%20multiple%20interacting%20agents.%20Despite%20apparent%20similarity%20to%20the%0Asingle-agent%20case%2C%20multi-agent%20problems%20are%20often%20harder%20to%20train%20and%20analyze%0Atheoretically.%20In%20this%20work%2C%20we%20propose%20MA-Trace%2C%20a%20new%20on-policy%20actor-critic%0Aalgorithm%2C%20which%20extends%20V-Trace%20to%20the%20MARL%20setting.%20The%20key%20advantage%20of%20our%0Aalgorithm%20is%20its%20high%20scalability%20in%20a%20multi-worker%20setting.%20To%20this%20end%2C%0AMA-Trace%20utilizes%20importance%20sampling%20as%20an%20off-policy%20correction%20method%2C%20which%0Aallows%20distributing%20the%20computations%20with%20no%20impact%20on%20the%20quality%20of%20training.%0AFurthermore%2C%20our%20algorithm%20is%20theoretically%20grounded%20-%20we%20prove%20a%20fixed-point%0Atheorem%20that%20guarantees%20convergence.%20We%20evaluate%20the%20algorithm%20extensively%20on%0Athe%20StarCraft%20Multi-Agent%20Challenge%2C%20a%20standard%20benchmark%20for%20multi-agent%0Aalgorithms.%20MA-Trace%20achieves%20high%20performance%20on%20all%20its%20tasks%20and%20exceeds%0Astate-of-the-art%20results%20on%20some%20of%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2111.11229v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Off-Policy%20Correction%20For%20Multi-Agent%20Reinforcement%20Learning&entry.906535625=Micha%C5%82%20Zawalski%20and%20B%C5%82a%C5%BCej%20Osi%C5%84ski%20and%20Henryk%20Michalewski%20and%20Piotr%20Mi%C5%82o%C5%9B&entry.1292438233=%20%20Multi-agent%20reinforcement%20learning%20%28MARL%29%20provides%20a%20framework%20for%20problems%0Ainvolving%20multiple%20interacting%20agents.%20Despite%20apparent%20similarity%20to%20the%0Asingle-agent%20case%2C%20multi-agent%20problems%20are%20often%20harder%20to%20train%20and%20analyze%0Atheoretically.%20In%20this%20work%2C%20we%20propose%20MA-Trace%2C%20a%20new%20on-policy%20actor-critic%0Aalgorithm%2C%20which%20extends%20V-Trace%20to%20the%20MARL%20setting.%20The%20key%20advantage%20of%20our%0Aalgorithm%20is%20its%20high%20scalability%20in%20a%20multi-worker%20setting.%20To%20this%20end%2C%0AMA-Trace%20utilizes%20importance%20sampling%20as%20an%20off-policy%20correction%20method%2C%20which%0Aallows%20distributing%20the%20computations%20with%20no%20impact%20on%20the%20quality%20of%20training.%0AFurthermore%2C%20our%20algorithm%20is%20theoretically%20grounded%20-%20we%20prove%20a%20fixed-point%0Atheorem%20that%20guarantees%20convergence.%20We%20evaluate%20the%20algorithm%20extensively%20on%0Athe%20StarCraft%20Multi-Agent%20Challenge%2C%20a%20standard%20benchmark%20for%20multi-agent%0Aalgorithms.%20MA-Trace%20achieves%20high%20performance%20on%20all%20its%20tasks%20and%20exceeds%0Astate-of-the-art%20results%20on%20some%20of%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2111.11229v3&entry.124074799=Read"},
{"title": "Empowering Biomedical Discovery with AI Agents", "author": "Shanghua Gao and Ada Fang and Yepeng Huang and Valentina Giunchiglia and Ayush Noori and Jonathan Richard Schwarz and Yasha Ektefaie and Jovana Kondic and Marinka Zitnik", "abstract": "  We envision 'AI scientists' as systems capable of skeptical learning and\nreasoning that empower biomedical research through collaborative agents that\nintegrate machine learning tools with experimental platforms. Rather than\ntaking humans out of the discovery process, biomedical AI agents combine human\ncreativity and expertise with AI's ability to analyze large datasets, navigate\nhypothesis spaces, and execute repetitive tasks. AI agents are proficient in a\nvariety of tasks, including self-assessment and planning of discovery\nworkflows. These agents use large language models and generative models to\nfeature structured memory for continual learning and use machine learning tools\nto incorporate scientific knowledge, biological principles, and theories. AI\nagents can impact areas ranging from hybrid cell simulation, programmable\ncontrol of phenotypes, and the design of cellular circuits to the development\nof new therapies.\n", "link": "http://arxiv.org/abs/2404.02831v1", "date": "2024-04-03", "relevancy": 1.9469, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4906}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4849}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4816}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Empowering%20Biomedical%20Discovery%20with%20AI%20Agents&body=Title%3A%20Empowering%20Biomedical%20Discovery%20with%20AI%20Agents%0AAuthor%3A%20Shanghua%20Gao%20and%20Ada%20Fang%20and%20Yepeng%20Huang%20and%20Valentina%20Giunchiglia%20and%20Ayush%20Noori%20and%20Jonathan%20Richard%20Schwarz%20and%20Yasha%20Ektefaie%20and%20Jovana%20Kondic%20and%20Marinka%20Zitnik%0AAbstract%3A%20%20%20We%20envision%20%27AI%20scientists%27%20as%20systems%20capable%20of%20skeptical%20learning%20and%0Areasoning%20that%20empower%20biomedical%20research%20through%20collaborative%20agents%20that%0Aintegrate%20machine%20learning%20tools%20with%20experimental%20platforms.%20Rather%20than%0Ataking%20humans%20out%20of%20the%20discovery%20process%2C%20biomedical%20AI%20agents%20combine%20human%0Acreativity%20and%20expertise%20with%20AI%27s%20ability%20to%20analyze%20large%20datasets%2C%20navigate%0Ahypothesis%20spaces%2C%20and%20execute%20repetitive%20tasks.%20AI%20agents%20are%20proficient%20in%20a%0Avariety%20of%20tasks%2C%20including%20self-assessment%20and%20planning%20of%20discovery%0Aworkflows.%20These%20agents%20use%20large%20language%20models%20and%20generative%20models%20to%0Afeature%20structured%20memory%20for%20continual%20learning%20and%20use%20machine%20learning%20tools%0Ato%20incorporate%20scientific%20knowledge%2C%20biological%20principles%2C%20and%20theories.%20AI%0Aagents%20can%20impact%20areas%20ranging%20from%20hybrid%20cell%20simulation%2C%20programmable%0Acontrol%20of%20phenotypes%2C%20and%20the%20design%20of%20cellular%20circuits%20to%20the%20development%0Aof%20new%20therapies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02831v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Empowering%20Biomedical%20Discovery%20with%20AI%20Agents&entry.906535625=Shanghua%20Gao%20and%20Ada%20Fang%20and%20Yepeng%20Huang%20and%20Valentina%20Giunchiglia%20and%20Ayush%20Noori%20and%20Jonathan%20Richard%20Schwarz%20and%20Yasha%20Ektefaie%20and%20Jovana%20Kondic%20and%20Marinka%20Zitnik&entry.1292438233=%20%20We%20envision%20%27AI%20scientists%27%20as%20systems%20capable%20of%20skeptical%20learning%20and%0Areasoning%20that%20empower%20biomedical%20research%20through%20collaborative%20agents%20that%0Aintegrate%20machine%20learning%20tools%20with%20experimental%20platforms.%20Rather%20than%0Ataking%20humans%20out%20of%20the%20discovery%20process%2C%20biomedical%20AI%20agents%20combine%20human%0Acreativity%20and%20expertise%20with%20AI%27s%20ability%20to%20analyze%20large%20datasets%2C%20navigate%0Ahypothesis%20spaces%2C%20and%20execute%20repetitive%20tasks.%20AI%20agents%20are%20proficient%20in%20a%0Avariety%20of%20tasks%2C%20including%20self-assessment%20and%20planning%20of%20discovery%0Aworkflows.%20These%20agents%20use%20large%20language%20models%20and%20generative%20models%20to%0Afeature%20structured%20memory%20for%20continual%20learning%20and%20use%20machine%20learning%20tools%0Ato%20incorporate%20scientific%20knowledge%2C%20biological%20principles%2C%20and%20theories.%20AI%0Aagents%20can%20impact%20areas%20ranging%20from%20hybrid%20cell%20simulation%2C%20programmable%0Acontrol%20of%20phenotypes%2C%20and%20the%20design%20of%20cellular%20circuits%20to%20the%20development%0Aof%20new%20therapies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02831v1&entry.124074799=Read"},
{"title": "Unleashing the Potential of Large Language Models for Predictive Tabular\n  Tasks in Data Science", "author": "Yazheng Yang and Yuqi Wang and Sankalok Sen and Lei Li and Qi Liu", "abstract": "  In the domain of data science, the predictive tasks of classification,\nregression, and imputation of missing values are commonly encountered\nchallenges associated with tabular data. This research endeavors to apply Large\nLanguage Models (LLMs) towards addressing these predictive tasks. Despite their\nproficiency in comprehending natural language, LLMs fall short in dealing with\nstructured tabular data. This limitation stems from their lacking exposure to\nthe intricacies of tabular data during their foundational training. Our\nresearch aims to mitigate this gap by compiling a comprehensive corpus of\ntables annotated with instructions and executing large-scale training of\nLlama-2 on this enriched dataset. Furthermore, we investigate the practical\napplication of applying the trained model to zero-shot prediction, few-shot\nprediction, and in-context learning scenarios. Through extensive experiments,\nour methodology has shown significant improvements over existing benchmarks.\nThese advancements highlight the efficacy of tailoring LLM training to solve\ntable-related problems in data science, thereby establishing a new benchmark in\nthe utilization of LLMs for enhancing tabular intelligence.\n", "link": "http://arxiv.org/abs/2403.20208v2", "date": "2024-04-03", "relevancy": 1.9442, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.496}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.48}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4761}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Unleashing%20the%20Potential%20of%20Large%20Language%20Models%20for%20Predictive%20Tabular%0A%20%20Tasks%20in%20Data%20Science&body=Title%3A%20Unleashing%20the%20Potential%20of%20Large%20Language%20Models%20for%20Predictive%20Tabular%0A%20%20Tasks%20in%20Data%20Science%0AAuthor%3A%20Yazheng%20Yang%20and%20Yuqi%20Wang%20and%20Sankalok%20Sen%20and%20Lei%20Li%20and%20Qi%20Liu%0AAbstract%3A%20%20%20In%20the%20domain%20of%20data%20science%2C%20the%20predictive%20tasks%20of%20classification%2C%0Aregression%2C%20and%20imputation%20of%20missing%20values%20are%20commonly%20encountered%0Achallenges%20associated%20with%20tabular%20data.%20This%20research%20endeavors%20to%20apply%20Large%0ALanguage%20Models%20%28LLMs%29%20towards%20addressing%20these%20predictive%20tasks.%20Despite%20their%0Aproficiency%20in%20comprehending%20natural%20language%2C%20LLMs%20fall%20short%20in%20dealing%20with%0Astructured%20tabular%20data.%20This%20limitation%20stems%20from%20their%20lacking%20exposure%20to%0Athe%20intricacies%20of%20tabular%20data%20during%20their%20foundational%20training.%20Our%0Aresearch%20aims%20to%20mitigate%20this%20gap%20by%20compiling%20a%20comprehensive%20corpus%20of%0Atables%20annotated%20with%20instructions%20and%20executing%20large-scale%20training%20of%0ALlama-2%20on%20this%20enriched%20dataset.%20Furthermore%2C%20we%20investigate%20the%20practical%0Aapplication%20of%20applying%20the%20trained%20model%20to%20zero-shot%20prediction%2C%20few-shot%0Aprediction%2C%20and%20in-context%20learning%20scenarios.%20Through%20extensive%20experiments%2C%0Aour%20methodology%20has%20shown%20significant%20improvements%20over%20existing%20benchmarks.%0AThese%20advancements%20highlight%20the%20efficacy%20of%20tailoring%20LLM%20training%20to%20solve%0Atable-related%20problems%20in%20data%20science%2C%20thereby%20establishing%20a%20new%20benchmark%20in%0Athe%20utilization%20of%20LLMs%20for%20enhancing%20tabular%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20208v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unleashing%20the%20Potential%20of%20Large%20Language%20Models%20for%20Predictive%20Tabular%0A%20%20Tasks%20in%20Data%20Science&entry.906535625=Yazheng%20Yang%20and%20Yuqi%20Wang%20and%20Sankalok%20Sen%20and%20Lei%20Li%20and%20Qi%20Liu&entry.1292438233=%20%20In%20the%20domain%20of%20data%20science%2C%20the%20predictive%20tasks%20of%20classification%2C%0Aregression%2C%20and%20imputation%20of%20missing%20values%20are%20commonly%20encountered%0Achallenges%20associated%20with%20tabular%20data.%20This%20research%20endeavors%20to%20apply%20Large%0ALanguage%20Models%20%28LLMs%29%20towards%20addressing%20these%20predictive%20tasks.%20Despite%20their%0Aproficiency%20in%20comprehending%20natural%20language%2C%20LLMs%20fall%20short%20in%20dealing%20with%0Astructured%20tabular%20data.%20This%20limitation%20stems%20from%20their%20lacking%20exposure%20to%0Athe%20intricacies%20of%20tabular%20data%20during%20their%20foundational%20training.%20Our%0Aresearch%20aims%20to%20mitigate%20this%20gap%20by%20compiling%20a%20comprehensive%20corpus%20of%0Atables%20annotated%20with%20instructions%20and%20executing%20large-scale%20training%20of%0ALlama-2%20on%20this%20enriched%20dataset.%20Furthermore%2C%20we%20investigate%20the%20practical%0Aapplication%20of%20applying%20the%20trained%20model%20to%20zero-shot%20prediction%2C%20few-shot%0Aprediction%2C%20and%20in-context%20learning%20scenarios.%20Through%20extensive%20experiments%2C%0Aour%20methodology%20has%20shown%20significant%20improvements%20over%20existing%20benchmarks.%0AThese%20advancements%20highlight%20the%20efficacy%20of%20tailoring%20LLM%20training%20to%20solve%0Atable-related%20problems%20in%20data%20science%2C%20thereby%20establishing%20a%20new%20benchmark%20in%0Athe%20utilization%20of%20LLMs%20for%20enhancing%20tabular%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20208v2&entry.124074799=Read"},
{"title": "MODNO: Multi Operator Learning With Distributed Neural Operators", "author": "Zecheng Zhang", "abstract": "  The study of operator learning involves the utilization of neural networks to\napproximate operators. Traditionally, the focus has been on single-operator\nlearning (SOL). However, recent advances have rapidly expanded this to include\nthe approximation of multiple operators using foundation models equipped with\nmillions or billions of trainable parameters, leading to the research of\nmulti-operator learning (MOL). In this paper, we present a novel distributed\ntraining approach aimed at enabling a single neural operator with significantly\nfewer parameters to effectively tackle multi-operator learning challenges, all\nwithout incurring additional average costs. Our method is applicable to various\nChen-Chen-type neural operators, such as Deep Operator Neural Networks (DON).\nThe core idea is to independently learn the output basis functions for each\noperator using its dedicated data, while simultaneously centralizing the\nlearning of the input function encoding shared by all operators using the\nentire dataset. Through a systematic study of five numerical examples, we\ncompare the accuracy and cost of training a single neural operator for each\noperator independently versus training a MOL model using our proposed method.\nOur results demonstrate enhanced efficiency and satisfactory accuracy.\nMoreover, our approach illustrates that some operators with limited data can be\nmore effectively constructed with the aid of data from analogous operators\nthrough MOL learning. This highlights another MOL's potential to bolster\noperator learning.\n", "link": "http://arxiv.org/abs/2404.02892v1", "date": "2024-04-03", "relevancy": 1.9291, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4907}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4864}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4722}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MODNO%3A%20Multi%20Operator%20Learning%20With%20Distributed%20Neural%20Operators&body=Title%3A%20MODNO%3A%20Multi%20Operator%20Learning%20With%20Distributed%20Neural%20Operators%0AAuthor%3A%20Zecheng%20Zhang%0AAbstract%3A%20%20%20The%20study%20of%20operator%20learning%20involves%20the%20utilization%20of%20neural%20networks%20to%0Aapproximate%20operators.%20Traditionally%2C%20the%20focus%20has%20been%20on%20single-operator%0Alearning%20%28SOL%29.%20However%2C%20recent%20advances%20have%20rapidly%20expanded%20this%20to%20include%0Athe%20approximation%20of%20multiple%20operators%20using%20foundation%20models%20equipped%20with%0Amillions%20or%20billions%20of%20trainable%20parameters%2C%20leading%20to%20the%20research%20of%0Amulti-operator%20learning%20%28MOL%29.%20In%20this%20paper%2C%20we%20present%20a%20novel%20distributed%0Atraining%20approach%20aimed%20at%20enabling%20a%20single%20neural%20operator%20with%20significantly%0Afewer%20parameters%20to%20effectively%20tackle%20multi-operator%20learning%20challenges%2C%20all%0Awithout%20incurring%20additional%20average%20costs.%20Our%20method%20is%20applicable%20to%20various%0AChen-Chen-type%20neural%20operators%2C%20such%20as%20Deep%20Operator%20Neural%20Networks%20%28DON%29.%0AThe%20core%20idea%20is%20to%20independently%20learn%20the%20output%20basis%20functions%20for%20each%0Aoperator%20using%20its%20dedicated%20data%2C%20while%20simultaneously%20centralizing%20the%0Alearning%20of%20the%20input%20function%20encoding%20shared%20by%20all%20operators%20using%20the%0Aentire%20dataset.%20Through%20a%20systematic%20study%20of%20five%20numerical%20examples%2C%20we%0Acompare%20the%20accuracy%20and%20cost%20of%20training%20a%20single%20neural%20operator%20for%20each%0Aoperator%20independently%20versus%20training%20a%20MOL%20model%20using%20our%20proposed%20method.%0AOur%20results%20demonstrate%20enhanced%20efficiency%20and%20satisfactory%20accuracy.%0AMoreover%2C%20our%20approach%20illustrates%20that%20some%20operators%20with%20limited%20data%20can%20be%0Amore%20effectively%20constructed%20with%20the%20aid%20of%20data%20from%20analogous%20operators%0Athrough%20MOL%20learning.%20This%20highlights%20another%20MOL%27s%20potential%20to%20bolster%0Aoperator%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02892v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MODNO%3A%20Multi%20Operator%20Learning%20With%20Distributed%20Neural%20Operators&entry.906535625=Zecheng%20Zhang&entry.1292438233=%20%20The%20study%20of%20operator%20learning%20involves%20the%20utilization%20of%20neural%20networks%20to%0Aapproximate%20operators.%20Traditionally%2C%20the%20focus%20has%20been%20on%20single-operator%0Alearning%20%28SOL%29.%20However%2C%20recent%20advances%20have%20rapidly%20expanded%20this%20to%20include%0Athe%20approximation%20of%20multiple%20operators%20using%20foundation%20models%20equipped%20with%0Amillions%20or%20billions%20of%20trainable%20parameters%2C%20leading%20to%20the%20research%20of%0Amulti-operator%20learning%20%28MOL%29.%20In%20this%20paper%2C%20we%20present%20a%20novel%20distributed%0Atraining%20approach%20aimed%20at%20enabling%20a%20single%20neural%20operator%20with%20significantly%0Afewer%20parameters%20to%20effectively%20tackle%20multi-operator%20learning%20challenges%2C%20all%0Awithout%20incurring%20additional%20average%20costs.%20Our%20method%20is%20applicable%20to%20various%0AChen-Chen-type%20neural%20operators%2C%20such%20as%20Deep%20Operator%20Neural%20Networks%20%28DON%29.%0AThe%20core%20idea%20is%20to%20independently%20learn%20the%20output%20basis%20functions%20for%20each%0Aoperator%20using%20its%20dedicated%20data%2C%20while%20simultaneously%20centralizing%20the%0Alearning%20of%20the%20input%20function%20encoding%20shared%20by%20all%20operators%20using%20the%0Aentire%20dataset.%20Through%20a%20systematic%20study%20of%20five%20numerical%20examples%2C%20we%0Acompare%20the%20accuracy%20and%20cost%20of%20training%20a%20single%20neural%20operator%20for%20each%0Aoperator%20independently%20versus%20training%20a%20MOL%20model%20using%20our%20proposed%20method.%0AOur%20results%20demonstrate%20enhanced%20efficiency%20and%20satisfactory%20accuracy.%0AMoreover%2C%20our%20approach%20illustrates%20that%20some%20operators%20with%20limited%20data%20can%20be%0Amore%20effectively%20constructed%20with%20the%20aid%20of%20data%20from%20analogous%20operators%0Athrough%20MOL%20learning.%20This%20highlights%20another%20MOL%27s%20potential%20to%20bolster%0Aoperator%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02892v1&entry.124074799=Read"},
{"title": "Steganographic Passport: An Owner and User Verifiable Credential for\n  Deep Model IP Protection Without Retraining", "author": "Qi Cui and Ruohan Meng and Chaohui Xu and Chip-Hong Chang", "abstract": "  Ensuring the legal usage of deep models is crucial to promoting trustable,\naccountable, and responsible artificial intelligence innovation. Current\npassport-based methods that obfuscate model functionality for license-to-use\nand ownership verifications suffer from capacity and quality constraints, as\nthey require retraining the owner model for new users. They are also vulnerable\nto advanced Expanded Residual Block ambiguity attacks. We propose\nSteganographic Passport, which uses an invertible steganographic network to\ndecouple license-to-use from ownership verification by hiding the user's\nidentity images into the owner-side passport and recovering them from their\nrespective user-side passports. An irreversible and collision-resistant hash\nfunction is used to avoid exposing the owner-side passport from the derived\nuser-side passports and increase the uniqueness of the model signature. To\nsafeguard both the passport and model's weights against advanced ambiguity\nattacks, an activation-level obfuscation is proposed for the verification\nbranch of the owner's model. By jointly training the verification and\ndeployment branches, their weights become tightly coupled. The proposed method\nsupports agile licensing of deep models by providing a strong ownership proof\nand license accountability without requiring a separate model retraining for\nthe admission of every new user. Experiment results show that our\nSteganographic Passport outperforms other passport-based deep model protection\nmethods in robustness against various known attacks.\n", "link": "http://arxiv.org/abs/2404.02889v1", "date": "2024-04-03", "relevancy": 1.9291, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5074}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4652}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4623}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Steganographic%20Passport%3A%20An%20Owner%20and%20User%20Verifiable%20Credential%20for%0A%20%20Deep%20Model%20IP%20Protection%20Without%20Retraining&body=Title%3A%20Steganographic%20Passport%3A%20An%20Owner%20and%20User%20Verifiable%20Credential%20for%0A%20%20Deep%20Model%20IP%20Protection%20Without%20Retraining%0AAuthor%3A%20Qi%20Cui%20and%20Ruohan%20Meng%20and%20Chaohui%20Xu%20and%20Chip-Hong%20Chang%0AAbstract%3A%20%20%20Ensuring%20the%20legal%20usage%20of%20deep%20models%20is%20crucial%20to%20promoting%20trustable%2C%0Aaccountable%2C%20and%20responsible%20artificial%20intelligence%20innovation.%20Current%0Apassport-based%20methods%20that%20obfuscate%20model%20functionality%20for%20license-to-use%0Aand%20ownership%20verifications%20suffer%20from%20capacity%20and%20quality%20constraints%2C%20as%0Athey%20require%20retraining%20the%20owner%20model%20for%20new%20users.%20They%20are%20also%20vulnerable%0Ato%20advanced%20Expanded%20Residual%20Block%20ambiguity%20attacks.%20We%20propose%0ASteganographic%20Passport%2C%20which%20uses%20an%20invertible%20steganographic%20network%20to%0Adecouple%20license-to-use%20from%20ownership%20verification%20by%20hiding%20the%20user%27s%0Aidentity%20images%20into%20the%20owner-side%20passport%20and%20recovering%20them%20from%20their%0Arespective%20user-side%20passports.%20An%20irreversible%20and%20collision-resistant%20hash%0Afunction%20is%20used%20to%20avoid%20exposing%20the%20owner-side%20passport%20from%20the%20derived%0Auser-side%20passports%20and%20increase%20the%20uniqueness%20of%20the%20model%20signature.%20To%0Asafeguard%20both%20the%20passport%20and%20model%27s%20weights%20against%20advanced%20ambiguity%0Aattacks%2C%20an%20activation-level%20obfuscation%20is%20proposed%20for%20the%20verification%0Abranch%20of%20the%20owner%27s%20model.%20By%20jointly%20training%20the%20verification%20and%0Adeployment%20branches%2C%20their%20weights%20become%20tightly%20coupled.%20The%20proposed%20method%0Asupports%20agile%20licensing%20of%20deep%20models%20by%20providing%20a%20strong%20ownership%20proof%0Aand%20license%20accountability%20without%20requiring%20a%20separate%20model%20retraining%20for%0Athe%20admission%20of%20every%20new%20user.%20Experiment%20results%20show%20that%20our%0ASteganographic%20Passport%20outperforms%20other%20passport-based%20deep%20model%20protection%0Amethods%20in%20robustness%20against%20various%20known%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02889v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Steganographic%20Passport%3A%20An%20Owner%20and%20User%20Verifiable%20Credential%20for%0A%20%20Deep%20Model%20IP%20Protection%20Without%20Retraining&entry.906535625=Qi%20Cui%20and%20Ruohan%20Meng%20and%20Chaohui%20Xu%20and%20Chip-Hong%20Chang&entry.1292438233=%20%20Ensuring%20the%20legal%20usage%20of%20deep%20models%20is%20crucial%20to%20promoting%20trustable%2C%0Aaccountable%2C%20and%20responsible%20artificial%20intelligence%20innovation.%20Current%0Apassport-based%20methods%20that%20obfuscate%20model%20functionality%20for%20license-to-use%0Aand%20ownership%20verifications%20suffer%20from%20capacity%20and%20quality%20constraints%2C%20as%0Athey%20require%20retraining%20the%20owner%20model%20for%20new%20users.%20They%20are%20also%20vulnerable%0Ato%20advanced%20Expanded%20Residual%20Block%20ambiguity%20attacks.%20We%20propose%0ASteganographic%20Passport%2C%20which%20uses%20an%20invertible%20steganographic%20network%20to%0Adecouple%20license-to-use%20from%20ownership%20verification%20by%20hiding%20the%20user%27s%0Aidentity%20images%20into%20the%20owner-side%20passport%20and%20recovering%20them%20from%20their%0Arespective%20user-side%20passports.%20An%20irreversible%20and%20collision-resistant%20hash%0Afunction%20is%20used%20to%20avoid%20exposing%20the%20owner-side%20passport%20from%20the%20derived%0Auser-side%20passports%20and%20increase%20the%20uniqueness%20of%20the%20model%20signature.%20To%0Asafeguard%20both%20the%20passport%20and%20model%27s%20weights%20against%20advanced%20ambiguity%0Aattacks%2C%20an%20activation-level%20obfuscation%20is%20proposed%20for%20the%20verification%0Abranch%20of%20the%20owner%27s%20model.%20By%20jointly%20training%20the%20verification%20and%0Adeployment%20branches%2C%20their%20weights%20become%20tightly%20coupled.%20The%20proposed%20method%0Asupports%20agile%20licensing%20of%20deep%20models%20by%20providing%20a%20strong%20ownership%20proof%0Aand%20license%20accountability%20without%20requiring%20a%20separate%20model%20retraining%20for%0Athe%20admission%20of%20every%20new%20user.%20Experiment%20results%20show%20that%20our%0ASteganographic%20Passport%20outperforms%20other%20passport-based%20deep%20model%20protection%0Amethods%20in%20robustness%20against%20various%20known%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02889v1&entry.124074799=Read"},
{"title": "On the Scalability of Diffusion-based Text-to-Image Generation", "author": "Hao Li and Yang Zou and Ying Wang and Orchid Majumder and Yusheng Xie and R. Manmatha and Ashwin Swaminathan and Zhuowen Tu and Stefano Ermon and Stefano Soatto", "abstract": "  Scaling up model and data size has been quite successful for the evolution of\nLLMs. However, the scaling law for the diffusion based text-to-image (T2I)\nmodels is not fully explored. It is also unclear how to efficiently scale the\nmodel for better performance at reduced cost. The different training settings\nand expensive training cost make a fair model comparison extremely difficult.\nIn this work, we empirically study the scaling properties of diffusion based\nT2I models by performing extensive and rigours ablations on scaling both\ndenoising backbones and training set, including training scaled UNet and\nTransformer variants ranging from 0.4B to 4B parameters on datasets upto 600M\nimages. For model scaling, we find the location and amount of cross attention\ndistinguishes the performance of existing UNet designs. And increasing the\ntransformer blocks is more parameter-efficient for improving text-image\nalignment than increasing channel numbers. We then identify an efficient UNet\nvariant, which is 45% smaller and 28% faster than SDXL's UNet. On the data\nscaling side, we show the quality and diversity of the training set matters\nmore than simply dataset size. Increasing caption density and diversity\nimproves text-image alignment performance and the learning efficiency. Finally,\nwe provide scaling functions to predict the text-image alignment performance as\nfunctions of the scale of model size, compute and dataset size.\n", "link": "http://arxiv.org/abs/2404.02883v1", "date": "2024-04-03", "relevancy": 1.925, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.7117}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6257}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6201}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20On%20the%20Scalability%20of%20Diffusion-based%20Text-to-Image%20Generation&body=Title%3A%20On%20the%20Scalability%20of%20Diffusion-based%20Text-to-Image%20Generation%0AAuthor%3A%20Hao%20Li%20and%20Yang%20Zou%20and%20Ying%20Wang%20and%20Orchid%20Majumder%20and%20Yusheng%20Xie%20and%20R.%20Manmatha%20and%20Ashwin%20Swaminathan%20and%20Zhuowen%20Tu%20and%20Stefano%20Ermon%20and%20Stefano%20Soatto%0AAbstract%3A%20%20%20Scaling%20up%20model%20and%20data%20size%20has%20been%20quite%20successful%20for%20the%20evolution%20of%0ALLMs.%20However%2C%20the%20scaling%20law%20for%20the%20diffusion%20based%20text-to-image%20%28T2I%29%0Amodels%20is%20not%20fully%20explored.%20It%20is%20also%20unclear%20how%20to%20efficiently%20scale%20the%0Amodel%20for%20better%20performance%20at%20reduced%20cost.%20The%20different%20training%20settings%0Aand%20expensive%20training%20cost%20make%20a%20fair%20model%20comparison%20extremely%20difficult.%0AIn%20this%20work%2C%20we%20empirically%20study%20the%20scaling%20properties%20of%20diffusion%20based%0AT2I%20models%20by%20performing%20extensive%20and%20rigours%20ablations%20on%20scaling%20both%0Adenoising%20backbones%20and%20training%20set%2C%20including%20training%20scaled%20UNet%20and%0ATransformer%20variants%20ranging%20from%200.4B%20to%204B%20parameters%20on%20datasets%20upto%20600M%0Aimages.%20For%20model%20scaling%2C%20we%20find%20the%20location%20and%20amount%20of%20cross%20attention%0Adistinguishes%20the%20performance%20of%20existing%20UNet%20designs.%20And%20increasing%20the%0Atransformer%20blocks%20is%20more%20parameter-efficient%20for%20improving%20text-image%0Aalignment%20than%20increasing%20channel%20numbers.%20We%20then%20identify%20an%20efficient%20UNet%0Avariant%2C%20which%20is%2045%25%20smaller%20and%2028%25%20faster%20than%20SDXL%27s%20UNet.%20On%20the%20data%0Ascaling%20side%2C%20we%20show%20the%20quality%20and%20diversity%20of%20the%20training%20set%20matters%0Amore%20than%20simply%20dataset%20size.%20Increasing%20caption%20density%20and%20diversity%0Aimproves%20text-image%20alignment%20performance%20and%20the%20learning%20efficiency.%20Finally%2C%0Awe%20provide%20scaling%20functions%20to%20predict%20the%20text-image%20alignment%20performance%20as%0Afunctions%20of%20the%20scale%20of%20model%20size%2C%20compute%20and%20dataset%20size.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02883v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Scalability%20of%20Diffusion-based%20Text-to-Image%20Generation&entry.906535625=Hao%20Li%20and%20Yang%20Zou%20and%20Ying%20Wang%20and%20Orchid%20Majumder%20and%20Yusheng%20Xie%20and%20R.%20Manmatha%20and%20Ashwin%20Swaminathan%20and%20Zhuowen%20Tu%20and%20Stefano%20Ermon%20and%20Stefano%20Soatto&entry.1292438233=%20%20Scaling%20up%20model%20and%20data%20size%20has%20been%20quite%20successful%20for%20the%20evolution%20of%0ALLMs.%20However%2C%20the%20scaling%20law%20for%20the%20diffusion%20based%20text-to-image%20%28T2I%29%0Amodels%20is%20not%20fully%20explored.%20It%20is%20also%20unclear%20how%20to%20efficiently%20scale%20the%0Amodel%20for%20better%20performance%20at%20reduced%20cost.%20The%20different%20training%20settings%0Aand%20expensive%20training%20cost%20make%20a%20fair%20model%20comparison%20extremely%20difficult.%0AIn%20this%20work%2C%20we%20empirically%20study%20the%20scaling%20properties%20of%20diffusion%20based%0AT2I%20models%20by%20performing%20extensive%20and%20rigours%20ablations%20on%20scaling%20both%0Adenoising%20backbones%20and%20training%20set%2C%20including%20training%20scaled%20UNet%20and%0ATransformer%20variants%20ranging%20from%200.4B%20to%204B%20parameters%20on%20datasets%20upto%20600M%0Aimages.%20For%20model%20scaling%2C%20we%20find%20the%20location%20and%20amount%20of%20cross%20attention%0Adistinguishes%20the%20performance%20of%20existing%20UNet%20designs.%20And%20increasing%20the%0Atransformer%20blocks%20is%20more%20parameter-efficient%20for%20improving%20text-image%0Aalignment%20than%20increasing%20channel%20numbers.%20We%20then%20identify%20an%20efficient%20UNet%0Avariant%2C%20which%20is%2045%25%20smaller%20and%2028%25%20faster%20than%20SDXL%27s%20UNet.%20On%20the%20data%0Ascaling%20side%2C%20we%20show%20the%20quality%20and%20diversity%20of%20the%20training%20set%20matters%0Amore%20than%20simply%20dataset%20size.%20Increasing%20caption%20density%20and%20diversity%0Aimproves%20text-image%20alignment%20performance%20and%20the%20learning%20efficiency.%20Finally%2C%0Awe%20provide%20scaling%20functions%20to%20predict%20the%20text-image%20alignment%20performance%20as%0Afunctions%20of%20the%20scale%20of%20model%20size%2C%20compute%20and%20dataset%20size.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02883v1&entry.124074799=Read"},
{"title": "An evaluation of CFEAR Radar Odometry", "author": "Daniel Adolfsson and Maximilian Hilger", "abstract": "  This article describes the method CFEAR Radar odometry, submitted to a\ncompetition at the Radar in Robotics workshop, ICRA 20241. CFEAR is an\nefficient and accurate method for spinning 2D radar odometry that generalizes\nwell across environments. This article presents an overview of the odometry\npipeline with new experiments on the public Boreas dataset. We show that a\nreal-time capable configuration of CFEAR - with its original parameter set -\nyields surprisingly low drift in the Boreas dataset. Additionally, we discuss\nan improved implementation and solving strategy that enables the most accurate\nconfiguration to run in real-time with improved robustness, reaching as low as\n0.61% translation drift at a frame rate of 68 Hz. A recent release of the\nsource code is available to the community\nhttps://github.com/dan11003/CFEAR_Radarodometry_code_public, and we publish the\nevaluation from this article on https://github.com/dan11003/cfear_2024_workshop\n", "link": "http://arxiv.org/abs/2404.01781v2", "date": "2024-04-03", "relevancy": 1.9045, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5048}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4832}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4576}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20An%20evaluation%20of%20CFEAR%20Radar%20Odometry&body=Title%3A%20An%20evaluation%20of%20CFEAR%20Radar%20Odometry%0AAuthor%3A%20Daniel%20Adolfsson%20and%20Maximilian%20Hilger%0AAbstract%3A%20%20%20This%20article%20describes%20the%20method%20CFEAR%20Radar%20odometry%2C%20submitted%20to%20a%0Acompetition%20at%20the%20Radar%20in%20Robotics%20workshop%2C%20ICRA%2020241.%20CFEAR%20is%20an%0Aefficient%20and%20accurate%20method%20for%20spinning%202D%20radar%20odometry%20that%20generalizes%0Awell%20across%20environments.%20This%20article%20presents%20an%20overview%20of%20the%20odometry%0Apipeline%20with%20new%20experiments%20on%20the%20public%20Boreas%20dataset.%20We%20show%20that%20a%0Areal-time%20capable%20configuration%20of%20CFEAR%20-%20with%20its%20original%20parameter%20set%20-%0Ayields%20surprisingly%20low%20drift%20in%20the%20Boreas%20dataset.%20Additionally%2C%20we%20discuss%0Aan%20improved%20implementation%20and%20solving%20strategy%20that%20enables%20the%20most%20accurate%0Aconfiguration%20to%20run%20in%20real-time%20with%20improved%20robustness%2C%20reaching%20as%20low%20as%0A0.61%25%20translation%20drift%20at%20a%20frame%20rate%20of%2068%20Hz.%20A%20recent%20release%20of%20the%0Asource%20code%20is%20available%20to%20the%20community%0Ahttps%3A//github.com/dan11003/CFEAR_Radarodometry_code_public%2C%20and%20we%20publish%20the%0Aevaluation%20from%20this%20article%20on%20https%3A//github.com/dan11003/cfear_2024_workshop%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01781v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20evaluation%20of%20CFEAR%20Radar%20Odometry&entry.906535625=Daniel%20Adolfsson%20and%20Maximilian%20Hilger&entry.1292438233=%20%20This%20article%20describes%20the%20method%20CFEAR%20Radar%20odometry%2C%20submitted%20to%20a%0Acompetition%20at%20the%20Radar%20in%20Robotics%20workshop%2C%20ICRA%2020241.%20CFEAR%20is%20an%0Aefficient%20and%20accurate%20method%20for%20spinning%202D%20radar%20odometry%20that%20generalizes%0Awell%20across%20environments.%20This%20article%20presents%20an%20overview%20of%20the%20odometry%0Apipeline%20with%20new%20experiments%20on%20the%20public%20Boreas%20dataset.%20We%20show%20that%20a%0Areal-time%20capable%20configuration%20of%20CFEAR%20-%20with%20its%20original%20parameter%20set%20-%0Ayields%20surprisingly%20low%20drift%20in%20the%20Boreas%20dataset.%20Additionally%2C%20we%20discuss%0Aan%20improved%20implementation%20and%20solving%20strategy%20that%20enables%20the%20most%20accurate%0Aconfiguration%20to%20run%20in%20real-time%20with%20improved%20robustness%2C%20reaching%20as%20low%20as%0A0.61%25%20translation%20drift%20at%20a%20frame%20rate%20of%2068%20Hz.%20A%20recent%20release%20of%20the%0Asource%20code%20is%20available%20to%20the%20community%0Ahttps%3A//github.com/dan11003/CFEAR_Radarodometry_code_public%2C%20and%20we%20publish%20the%0Aevaluation%20from%20this%20article%20on%20https%3A//github.com/dan11003/cfear_2024_workshop%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01781v2&entry.124074799=Read"},
{"title": "On Few-Shot Prompting for Controllable Question-Answer Generation in\n  Narrative Comprehension", "author": "Bernardo Leite and Henrique Lopes Cardoso", "abstract": "  Question Generation aims to automatically generate questions based on a given\ninput provided as context. A controllable question generation scheme focuses on\ngenerating questions with specific attributes, allowing better control. In this\nstudy, we propose a few-shot prompting strategy for controlling the generation\nof question-answer pairs from children's narrative texts. We aim to control two\nattributes: the question's explicitness and underlying narrative elements. With\nempirical evaluation, we show the effectiveness of controlling the generation\nprocess by employing few-shot prompting side by side with a reference model.\nOur experiments highlight instances where the few-shot strategy surpasses the\nreference model, particularly in scenarios such as semantic closeness\nevaluation and the diversity and coherency of question-answer pairs. However,\nthese improvements are not always statistically significant. The code is\npublicly available at github.com/bernardoleite/few-shot-prompting-qg-control.\n", "link": "http://arxiv.org/abs/2404.02800v1", "date": "2024-04-03", "relevancy": 1.9045, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5081}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4556}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4524}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20On%20Few-Shot%20Prompting%20for%20Controllable%20Question-Answer%20Generation%20in%0A%20%20Narrative%20Comprehension&body=Title%3A%20On%20Few-Shot%20Prompting%20for%20Controllable%20Question-Answer%20Generation%20in%0A%20%20Narrative%20Comprehension%0AAuthor%3A%20Bernardo%20Leite%20and%20Henrique%20Lopes%20Cardoso%0AAbstract%3A%20%20%20Question%20Generation%20aims%20to%20automatically%20generate%20questions%20based%20on%20a%20given%0Ainput%20provided%20as%20context.%20A%20controllable%20question%20generation%20scheme%20focuses%20on%0Agenerating%20questions%20with%20specific%20attributes%2C%20allowing%20better%20control.%20In%20this%0Astudy%2C%20we%20propose%20a%20few-shot%20prompting%20strategy%20for%20controlling%20the%20generation%0Aof%20question-answer%20pairs%20from%20children%27s%20narrative%20texts.%20We%20aim%20to%20control%20two%0Aattributes%3A%20the%20question%27s%20explicitness%20and%20underlying%20narrative%20elements.%20With%0Aempirical%20evaluation%2C%20we%20show%20the%20effectiveness%20of%20controlling%20the%20generation%0Aprocess%20by%20employing%20few-shot%20prompting%20side%20by%20side%20with%20a%20reference%20model.%0AOur%20experiments%20highlight%20instances%20where%20the%20few-shot%20strategy%20surpasses%20the%0Areference%20model%2C%20particularly%20in%20scenarios%20such%20as%20semantic%20closeness%0Aevaluation%20and%20the%20diversity%20and%20coherency%20of%20question-answer%20pairs.%20However%2C%0Athese%20improvements%20are%20not%20always%20statistically%20significant.%20The%20code%20is%0Apublicly%20available%20at%20github.com/bernardoleite/few-shot-prompting-qg-control.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02800v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Few-Shot%20Prompting%20for%20Controllable%20Question-Answer%20Generation%20in%0A%20%20Narrative%20Comprehension&entry.906535625=Bernardo%20Leite%20and%20Henrique%20Lopes%20Cardoso&entry.1292438233=%20%20Question%20Generation%20aims%20to%20automatically%20generate%20questions%20based%20on%20a%20given%0Ainput%20provided%20as%20context.%20A%20controllable%20question%20generation%20scheme%20focuses%20on%0Agenerating%20questions%20with%20specific%20attributes%2C%20allowing%20better%20control.%20In%20this%0Astudy%2C%20we%20propose%20a%20few-shot%20prompting%20strategy%20for%20controlling%20the%20generation%0Aof%20question-answer%20pairs%20from%20children%27s%20narrative%20texts.%20We%20aim%20to%20control%20two%0Aattributes%3A%20the%20question%27s%20explicitness%20and%20underlying%20narrative%20elements.%20With%0Aempirical%20evaluation%2C%20we%20show%20the%20effectiveness%20of%20controlling%20the%20generation%0Aprocess%20by%20employing%20few-shot%20prompting%20side%20by%20side%20with%20a%20reference%20model.%0AOur%20experiments%20highlight%20instances%20where%20the%20few-shot%20strategy%20surpasses%20the%0Areference%20model%2C%20particularly%20in%20scenarios%20such%20as%20semantic%20closeness%0Aevaluation%20and%20the%20diversity%20and%20coherency%20of%20question-answer%20pairs.%20However%2C%0Athese%20improvements%20are%20not%20always%20statistically%20significant.%20The%20code%20is%0Apublicly%20available%20at%20github.com/bernardoleite/few-shot-prompting-qg-control.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02800v1&entry.124074799=Read"},
{"title": "Emulated Disalignment: Safety Alignment for Large Language Models May\n  Backfire!", "author": "Zhanhui Zhou and Jie Liu and Zhichen Dong and Jiaheng Liu and Chao Yang and Wanli Ouyang and Yu Qiao", "abstract": "  Large language models (LLMs) need to undergo safety alignment to ensure safe\nconversations with humans. However, this paper introduces an inference-time\nattack method, demonstrating that safety alignment can be easily reversed to\nproduce harmful language models without additional training. Specifically, this\nreversal is achieved by contrasting the output token distribution of a\nsafety-aligned language model (e.g., Llama-2-chat) against its pre-trained\nversion (e.g., Llama-2) so that the token predictions are shifted towards the\nopposite direction of alignment. We name this method emulated disalignment (ED)\nbecause it uses pure sampling to provably emulate (or \"approximate\") the result\nof fine-tuning the pre-trained model to minimize a safety reward. Our\nexperiments with ED across three evaluation datasets and four model families\n(Llama-1, Llama-2, Mistral, and Alpaca) show that ED doubles the harmfulness of\npre-trained models and outperforms strong baselines, achieving the highest\nharmful rate in 43 out of 48 evaluation subsets by a large margin. Eventually,\ngiven ED's need for language model output token distributions, which\nparticularly compromises open-source models, our findings highlight the\nimportance of reevaluating the practice of open-sourcing language models even\nafter safety alignment.\n", "link": "http://arxiv.org/abs/2402.12343v3", "date": "2024-04-03", "relevancy": 1.9033, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5303}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.466}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4638}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Emulated%20Disalignment%3A%20Safety%20Alignment%20for%20Large%20Language%20Models%20May%0A%20%20Backfire%21&body=Title%3A%20Emulated%20Disalignment%3A%20Safety%20Alignment%20for%20Large%20Language%20Models%20May%0A%20%20Backfire%21%0AAuthor%3A%20Zhanhui%20Zhou%20and%20Jie%20Liu%20and%20Zhichen%20Dong%20and%20Jiaheng%20Liu%20and%20Chao%20Yang%20and%20Wanli%20Ouyang%20and%20Yu%20Qiao%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20need%20to%20undergo%20safety%20alignment%20to%20ensure%20safe%0Aconversations%20with%20humans.%20However%2C%20this%20paper%20introduces%20an%20inference-time%0Aattack%20method%2C%20demonstrating%20that%20safety%20alignment%20can%20be%20easily%20reversed%20to%0Aproduce%20harmful%20language%20models%20without%20additional%20training.%20Specifically%2C%20this%0Areversal%20is%20achieved%20by%20contrasting%20the%20output%20token%20distribution%20of%20a%0Asafety-aligned%20language%20model%20%28e.g.%2C%20Llama-2-chat%29%20against%20its%20pre-trained%0Aversion%20%28e.g.%2C%20Llama-2%29%20so%20that%20the%20token%20predictions%20are%20shifted%20towards%20the%0Aopposite%20direction%20of%20alignment.%20We%20name%20this%20method%20emulated%20disalignment%20%28ED%29%0Abecause%20it%20uses%20pure%20sampling%20to%20provably%20emulate%20%28or%20%22approximate%22%29%20the%20result%0Aof%20fine-tuning%20the%20pre-trained%20model%20to%20minimize%20a%20safety%20reward.%20Our%0Aexperiments%20with%20ED%20across%20three%20evaluation%20datasets%20and%20four%20model%20families%0A%28Llama-1%2C%20Llama-2%2C%20Mistral%2C%20and%20Alpaca%29%20show%20that%20ED%20doubles%20the%20harmfulness%20of%0Apre-trained%20models%20and%20outperforms%20strong%20baselines%2C%20achieving%20the%20highest%0Aharmful%20rate%20in%2043%20out%20of%2048%20evaluation%20subsets%20by%20a%20large%20margin.%20Eventually%2C%0Agiven%20ED%27s%20need%20for%20language%20model%20output%20token%20distributions%2C%20which%0Aparticularly%20compromises%20open-source%20models%2C%20our%20findings%20highlight%20the%0Aimportance%20of%20reevaluating%20the%20practice%20of%20open-sourcing%20language%20models%20even%0Aafter%20safety%20alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12343v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emulated%20Disalignment%3A%20Safety%20Alignment%20for%20Large%20Language%20Models%20May%0A%20%20Backfire%21&entry.906535625=Zhanhui%20Zhou%20and%20Jie%20Liu%20and%20Zhichen%20Dong%20and%20Jiaheng%20Liu%20and%20Chao%20Yang%20and%20Wanli%20Ouyang%20and%20Yu%20Qiao&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20need%20to%20undergo%20safety%20alignment%20to%20ensure%20safe%0Aconversations%20with%20humans.%20However%2C%20this%20paper%20introduces%20an%20inference-time%0Aattack%20method%2C%20demonstrating%20that%20safety%20alignment%20can%20be%20easily%20reversed%20to%0Aproduce%20harmful%20language%20models%20without%20additional%20training.%20Specifically%2C%20this%0Areversal%20is%20achieved%20by%20contrasting%20the%20output%20token%20distribution%20of%20a%0Asafety-aligned%20language%20model%20%28e.g.%2C%20Llama-2-chat%29%20against%20its%20pre-trained%0Aversion%20%28e.g.%2C%20Llama-2%29%20so%20that%20the%20token%20predictions%20are%20shifted%20towards%20the%0Aopposite%20direction%20of%20alignment.%20We%20name%20this%20method%20emulated%20disalignment%20%28ED%29%0Abecause%20it%20uses%20pure%20sampling%20to%20provably%20emulate%20%28or%20%22approximate%22%29%20the%20result%0Aof%20fine-tuning%20the%20pre-trained%20model%20to%20minimize%20a%20safety%20reward.%20Our%0Aexperiments%20with%20ED%20across%20three%20evaluation%20datasets%20and%20four%20model%20families%0A%28Llama-1%2C%20Llama-2%2C%20Mistral%2C%20and%20Alpaca%29%20show%20that%20ED%20doubles%20the%20harmfulness%20of%0Apre-trained%20models%20and%20outperforms%20strong%20baselines%2C%20achieving%20the%20highest%0Aharmful%20rate%20in%2043%20out%20of%2048%20evaluation%20subsets%20by%20a%20large%20margin.%20Eventually%2C%0Agiven%20ED%27s%20need%20for%20language%20model%20output%20token%20distributions%2C%20which%0Aparticularly%20compromises%20open-source%20models%2C%20our%20findings%20highlight%20the%0Aimportance%20of%20reevaluating%20the%20practice%20of%20open-sourcing%20language%20models%20even%0Aafter%20safety%20alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12343v3&entry.124074799=Read"},
{"title": "Privacy-Aware Semantic Cache for Large Language Models", "author": "Waris Gill and Mohamed Elidrisi and Pallavi Kalapatapu and Ali Anwar and Muhammad Ali Gulzar", "abstract": "  Large Language Models (LLMs) like ChatGPT and Llama2 have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries, leading to unacceptable false hit-and-miss rates.\n  This paper introduces MeanCache, a user-centric semantic cache for LLMs that\nidentifies semantically similar queries to determine cache hit or miss. Using\nMeanCache, the response to a user's semantically similar query can be retrieved\nfrom a local cache rather than re-querying the LLM, thus reducing costs,\nservice provider load, and environmental impact. Existing caching solutions for\nLLMs raise privacy and scalability concerns and perform wasteful query\nrequests. MeanCache leverages Federated Learning (FL) to collaboratively train\na query similarity model across LLM users without violating privacy. By placing\na local cache in each user's device and using FL, MeanCache reduces the latency\nand costs and enhances model performance, resulting in lower false hit rates.\nMeanCache compresses the embedding dimensions to minimize cache storage and\nalso finds the optimal cosine similarity threshold. Our experiments benchmarked\nagainst the state-of-the-art caching method, reveal that MeanCache attains an\napproximately 17% higher F-score and a 20% increase in precision during\nsemantic cache hit-and-miss decisions. It also reduces the storage requirement\nby 83% and accelerates semantic cache hit-and-miss decisions by 11%.\n", "link": "http://arxiv.org/abs/2403.02694v2", "date": "2024-04-03", "relevancy": 1.9014, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4813}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4786}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4697}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Privacy-Aware%20Semantic%20Cache%20for%20Large%20Language%20Models&body=Title%3A%20Privacy-Aware%20Semantic%20Cache%20for%20Large%20Language%20Models%0AAuthor%3A%20Waris%20Gill%20and%20Mohamed%20Elidrisi%20and%20Pallavi%20Kalapatapu%20and%20Ali%20Anwar%20and%20Muhammad%20Ali%20Gulzar%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20like%20ChatGPT%20and%20Llama2%20have%20revolutionized%0Anatural%20language%20processing%20and%20search%20engine%20dynamics.%20However%2C%20these%20models%0Aincur%20exceptionally%20high%20computational%20costs.%20For%20instance%2C%20GPT-3%20consists%20of%0A175%20billion%20parameters%20where%20inference%20demands%20billions%20of%20floating-point%0Aoperations.%20Caching%20is%20a%20natural%20solution%20to%20reduce%20LLM%20inference%20costs%20on%0Arepeated%20queries%20which%20constitute%20about%2031%25%20of%20the%20total%20queries.%20However%2C%0Aexisting%20caching%20methods%20are%20incapable%20of%20finding%20semantic%20similarities%20among%0ALLM%20queries%2C%20leading%20to%20unacceptable%20false%20hit-and-miss%20rates.%0A%20%20This%20paper%20introduces%20MeanCache%2C%20a%20user-centric%20semantic%20cache%20for%20LLMs%20that%0Aidentifies%20semantically%20similar%20queries%20to%20determine%20cache%20hit%20or%20miss.%20Using%0AMeanCache%2C%20the%20response%20to%20a%20user%27s%20semantically%20similar%20query%20can%20be%20retrieved%0Afrom%20a%20local%20cache%20rather%20than%20re-querying%20the%20LLM%2C%20thus%20reducing%20costs%2C%0Aservice%20provider%20load%2C%20and%20environmental%20impact.%20Existing%20caching%20solutions%20for%0ALLMs%20raise%20privacy%20and%20scalability%20concerns%20and%20perform%20wasteful%20query%0Arequests.%20MeanCache%20leverages%20Federated%20Learning%20%28FL%29%20to%20collaboratively%20train%0Aa%20query%20similarity%20model%20across%20LLM%20users%20without%20violating%20privacy.%20By%20placing%0Aa%20local%20cache%20in%20each%20user%27s%20device%20and%20using%20FL%2C%20MeanCache%20reduces%20the%20latency%0Aand%20costs%20and%20enhances%20model%20performance%2C%20resulting%20in%20lower%20false%20hit%20rates.%0AMeanCache%20compresses%20the%20embedding%20dimensions%20to%20minimize%20cache%20storage%20and%0Aalso%20finds%20the%20optimal%20cosine%20similarity%20threshold.%20Our%20experiments%20benchmarked%0Aagainst%20the%20state-of-the-art%20caching%20method%2C%20reveal%20that%20MeanCache%20attains%20an%0Aapproximately%2017%25%20higher%20F-score%20and%20a%2020%25%20increase%20in%20precision%20during%0Asemantic%20cache%20hit-and-miss%20decisions.%20It%20also%20reduces%20the%20storage%20requirement%0Aby%2083%25%20and%20accelerates%20semantic%20cache%20hit-and-miss%20decisions%20by%2011%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.02694v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Privacy-Aware%20Semantic%20Cache%20for%20Large%20Language%20Models&entry.906535625=Waris%20Gill%20and%20Mohamed%20Elidrisi%20and%20Pallavi%20Kalapatapu%20and%20Ali%20Anwar%20and%20Muhammad%20Ali%20Gulzar&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20like%20ChatGPT%20and%20Llama2%20have%20revolutionized%0Anatural%20language%20processing%20and%20search%20engine%20dynamics.%20However%2C%20these%20models%0Aincur%20exceptionally%20high%20computational%20costs.%20For%20instance%2C%20GPT-3%20consists%20of%0A175%20billion%20parameters%20where%20inference%20demands%20billions%20of%20floating-point%0Aoperations.%20Caching%20is%20a%20natural%20solution%20to%20reduce%20LLM%20inference%20costs%20on%0Arepeated%20queries%20which%20constitute%20about%2031%25%20of%20the%20total%20queries.%20However%2C%0Aexisting%20caching%20methods%20are%20incapable%20of%20finding%20semantic%20similarities%20among%0ALLM%20queries%2C%20leading%20to%20unacceptable%20false%20hit-and-miss%20rates.%0A%20%20This%20paper%20introduces%20MeanCache%2C%20a%20user-centric%20semantic%20cache%20for%20LLMs%20that%0Aidentifies%20semantically%20similar%20queries%20to%20determine%20cache%20hit%20or%20miss.%20Using%0AMeanCache%2C%20the%20response%20to%20a%20user%27s%20semantically%20similar%20query%20can%20be%20retrieved%0Afrom%20a%20local%20cache%20rather%20than%20re-querying%20the%20LLM%2C%20thus%20reducing%20costs%2C%0Aservice%20provider%20load%2C%20and%20environmental%20impact.%20Existing%20caching%20solutions%20for%0ALLMs%20raise%20privacy%20and%20scalability%20concerns%20and%20perform%20wasteful%20query%0Arequests.%20MeanCache%20leverages%20Federated%20Learning%20%28FL%29%20to%20collaboratively%20train%0Aa%20query%20similarity%20model%20across%20LLM%20users%20without%20violating%20privacy.%20By%20placing%0Aa%20local%20cache%20in%20each%20user%27s%20device%20and%20using%20FL%2C%20MeanCache%20reduces%20the%20latency%0Aand%20costs%20and%20enhances%20model%20performance%2C%20resulting%20in%20lower%20false%20hit%20rates.%0AMeanCache%20compresses%20the%20embedding%20dimensions%20to%20minimize%20cache%20storage%20and%0Aalso%20finds%20the%20optimal%20cosine%20similarity%20threshold.%20Our%20experiments%20benchmarked%0Aagainst%20the%20state-of-the-art%20caching%20method%2C%20reveal%20that%20MeanCache%20attains%20an%0Aapproximately%2017%25%20higher%20F-score%20and%20a%2020%25%20increase%20in%20precision%20during%0Asemantic%20cache%20hit-and-miss%20decisions.%20It%20also%20reduces%20the%20storage%20requirement%0Aby%2083%25%20and%20accelerates%20semantic%20cache%20hit-and-miss%20decisions%20by%2011%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02694v2&entry.124074799=Read"},
{"title": "Refining Myocardial Infarction Detection: A Novel Multi-Modal Composite\n  Kernel Strategy in One-Class Classification", "author": "Muhammad Uzair Zahid and Aysen Degerli and Fahad Sohrab and Serkan Kiranyaz and Tahir Hamid and Rashid Mazhar and Moncef Gabbouj", "abstract": "  Early detection of myocardial infarction (MI), a critical condition arising\nfrom coronary artery disease (CAD), is vital to prevent further myocardial\ndamage. This study introduces a novel method for early MI detection using a\none-class classification (OCC) algorithm in echocardiography. Our study\novercomes the challenge of limited echocardiography data availability by\nadopting a novel approach based on Multi-modal Subspace Support Vector Data\nDescription. The proposed technique involves a specialized MI detection\nframework employing multi-view echocardiography incorporating a composite\nkernel in the non-linear projection trick, fusing Gaussian and Laplacian\nsigmoid functions. Additionally, we enhance the update strategy of the\nprojection matrices by adapting maximization for both or one of the modalities\nin the optimization process. Our method boosts MI detection capability by\nefficiently transforming features extracted from echocardiography data into an\noptimized lower-dimensional subspace. The OCC model trained specifically on\ntarget class instances from the comprehensive HMC-QU dataset that includes\nmultiple echocardiography views indicates a marked improvement in MI detection\naccuracy. Our findings reveal that our proposed multi-view approach achieves a\ngeometric mean of 71.24%, signifying a substantial advancement in\nechocardiography-based MI diagnosis and offering more precise and efficient\ndiagnostic tools.\n", "link": "http://arxiv.org/abs/2402.06530v2", "date": "2024-04-03", "relevancy": 1.8996, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.485}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4846}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4611}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Refining%20Myocardial%20Infarction%20Detection%3A%20A%20Novel%20Multi-Modal%20Composite%0A%20%20Kernel%20Strategy%20in%20One-Class%20Classification&body=Title%3A%20Refining%20Myocardial%20Infarction%20Detection%3A%20A%20Novel%20Multi-Modal%20Composite%0A%20%20Kernel%20Strategy%20in%20One-Class%20Classification%0AAuthor%3A%20Muhammad%20Uzair%20Zahid%20and%20Aysen%20Degerli%20and%20Fahad%20Sohrab%20and%20Serkan%20Kiranyaz%20and%20Tahir%20Hamid%20and%20Rashid%20Mazhar%20and%20Moncef%20Gabbouj%0AAbstract%3A%20%20%20Early%20detection%20of%20myocardial%20infarction%20%28MI%29%2C%20a%20critical%20condition%20arising%0Afrom%20coronary%20artery%20disease%20%28CAD%29%2C%20is%20vital%20to%20prevent%20further%20myocardial%0Adamage.%20This%20study%20introduces%20a%20novel%20method%20for%20early%20MI%20detection%20using%20a%0Aone-class%20classification%20%28OCC%29%20algorithm%20in%20echocardiography.%20Our%20study%0Aovercomes%20the%20challenge%20of%20limited%20echocardiography%20data%20availability%20by%0Aadopting%20a%20novel%20approach%20based%20on%20Multi-modal%20Subspace%20Support%20Vector%20Data%0ADescription.%20The%20proposed%20technique%20involves%20a%20specialized%20MI%20detection%0Aframework%20employing%20multi-view%20echocardiography%20incorporating%20a%20composite%0Akernel%20in%20the%20non-linear%20projection%20trick%2C%20fusing%20Gaussian%20and%20Laplacian%0Asigmoid%20functions.%20Additionally%2C%20we%20enhance%20the%20update%20strategy%20of%20the%0Aprojection%20matrices%20by%20adapting%20maximization%20for%20both%20or%20one%20of%20the%20modalities%0Ain%20the%20optimization%20process.%20Our%20method%20boosts%20MI%20detection%20capability%20by%0Aefficiently%20transforming%20features%20extracted%20from%20echocardiography%20data%20into%20an%0Aoptimized%20lower-dimensional%20subspace.%20The%20OCC%20model%20trained%20specifically%20on%0Atarget%20class%20instances%20from%20the%20comprehensive%20HMC-QU%20dataset%20that%20includes%0Amultiple%20echocardiography%20views%20indicates%20a%20marked%20improvement%20in%20MI%20detection%0Aaccuracy.%20Our%20findings%20reveal%20that%20our%20proposed%20multi-view%20approach%20achieves%20a%0Ageometric%20mean%20of%2071.24%25%2C%20signifying%20a%20substantial%20advancement%20in%0Aechocardiography-based%20MI%20diagnosis%20and%20offering%20more%20precise%20and%20efficient%0Adiagnostic%20tools.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.06530v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Refining%20Myocardial%20Infarction%20Detection%3A%20A%20Novel%20Multi-Modal%20Composite%0A%20%20Kernel%20Strategy%20in%20One-Class%20Classification&entry.906535625=Muhammad%20Uzair%20Zahid%20and%20Aysen%20Degerli%20and%20Fahad%20Sohrab%20and%20Serkan%20Kiranyaz%20and%20Tahir%20Hamid%20and%20Rashid%20Mazhar%20and%20Moncef%20Gabbouj&entry.1292438233=%20%20Early%20detection%20of%20myocardial%20infarction%20%28MI%29%2C%20a%20critical%20condition%20arising%0Afrom%20coronary%20artery%20disease%20%28CAD%29%2C%20is%20vital%20to%20prevent%20further%20myocardial%0Adamage.%20This%20study%20introduces%20a%20novel%20method%20for%20early%20MI%20detection%20using%20a%0Aone-class%20classification%20%28OCC%29%20algorithm%20in%20echocardiography.%20Our%20study%0Aovercomes%20the%20challenge%20of%20limited%20echocardiography%20data%20availability%20by%0Aadopting%20a%20novel%20approach%20based%20on%20Multi-modal%20Subspace%20Support%20Vector%20Data%0ADescription.%20The%20proposed%20technique%20involves%20a%20specialized%20MI%20detection%0Aframework%20employing%20multi-view%20echocardiography%20incorporating%20a%20composite%0Akernel%20in%20the%20non-linear%20projection%20trick%2C%20fusing%20Gaussian%20and%20Laplacian%0Asigmoid%20functions.%20Additionally%2C%20we%20enhance%20the%20update%20strategy%20of%20the%0Aprojection%20matrices%20by%20adapting%20maximization%20for%20both%20or%20one%20of%20the%20modalities%0Ain%20the%20optimization%20process.%20Our%20method%20boosts%20MI%20detection%20capability%20by%0Aefficiently%20transforming%20features%20extracted%20from%20echocardiography%20data%20into%20an%0Aoptimized%20lower-dimensional%20subspace.%20The%20OCC%20model%20trained%20specifically%20on%0Atarget%20class%20instances%20from%20the%20comprehensive%20HMC-QU%20dataset%20that%20includes%0Amultiple%20echocardiography%20views%20indicates%20a%20marked%20improvement%20in%20MI%20detection%0Aaccuracy.%20Our%20findings%20reveal%20that%20our%20proposed%20multi-view%20approach%20achieves%20a%0Ageometric%20mean%20of%2071.24%25%2C%20signifying%20a%20substantial%20advancement%20in%0Aechocardiography-based%20MI%20diagnosis%20and%20offering%20more%20precise%20and%20efficient%0Adiagnostic%20tools.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.06530v2&entry.124074799=Read"},
{"title": "Channel-Feedback-Free Transmission for Downlink FD-RAN: A Radio Map\n  based Complex-valued Precoding Network Approach", "author": "Jiwei Zhao and Jiacheng Chen and Zeyu Sun and Yuhang Shi and Haibo Zhou and  Xuemin and  Shen", "abstract": "  As the demand for high-quality services proliferates, an innovative network\narchitecture, the fully-decoupled RAN (FD-RAN), has emerged for more flexible\nspectrum resource utilization and lower network costs. However, with the\ndecoupling of uplink base stations and downlink base stations in FD-RAN, the\ntraditional transmission mechanism, which relies on real-time channel feedback,\nis not suitable as the receiver is not able to feedback accurate and timely\nchannel state information to the transmitter. This paper proposes a novel\ntransmission scheme without relying on physical layer channel feedback.\nSpecifically, we design a radio map based complex-valued precoding\nnetwork~(RMCPNet) model, which outputs the base station precoding based on user\nlocation. RMCPNet comprises multiple subnets, with each subnet responsible for\nextracting unique modal features from diverse input modalities. Furthermore,\nthe multi-modal embeddings derived from these distinct subnets are integrated\nwithin the information fusion layer, culminating in a unified representation.\nWe also develop a specific RMCPNet training algorithm that employs the negative\nspectral efficiency as the loss function. We evaluate the performance of the\nproposed scheme on the public DeepMIMO dataset and show that RMCPNet can\nachieve 16\\% and 76\\% performance improvements over the conventional\nreal-valued neural network and statistical codebook approach, respectively.\n", "link": "http://arxiv.org/abs/2312.02184v2", "date": "2024-04-03", "relevancy": 1.8947, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4908}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4689}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4585}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Channel-Feedback-Free%20Transmission%20for%20Downlink%20FD-RAN%3A%20A%20Radio%20Map%0A%20%20based%20Complex-valued%20Precoding%20Network%20Approach&body=Title%3A%20Channel-Feedback-Free%20Transmission%20for%20Downlink%20FD-RAN%3A%20A%20Radio%20Map%0A%20%20based%20Complex-valued%20Precoding%20Network%20Approach%0AAuthor%3A%20Jiwei%20Zhao%20and%20Jiacheng%20Chen%20and%20Zeyu%20Sun%20and%20Yuhang%20Shi%20and%20Haibo%20Zhou%20and%20%20Xuemin%20and%20%20Shen%0AAbstract%3A%20%20%20As%20the%20demand%20for%20high-quality%20services%20proliferates%2C%20an%20innovative%20network%0Aarchitecture%2C%20the%20fully-decoupled%20RAN%20%28FD-RAN%29%2C%20has%20emerged%20for%20more%20flexible%0Aspectrum%20resource%20utilization%20and%20lower%20network%20costs.%20However%2C%20with%20the%0Adecoupling%20of%20uplink%20base%20stations%20and%20downlink%20base%20stations%20in%20FD-RAN%2C%20the%0Atraditional%20transmission%20mechanism%2C%20which%20relies%20on%20real-time%20channel%20feedback%2C%0Ais%20not%20suitable%20as%20the%20receiver%20is%20not%20able%20to%20feedback%20accurate%20and%20timely%0Achannel%20state%20information%20to%20the%20transmitter.%20This%20paper%20proposes%20a%20novel%0Atransmission%20scheme%20without%20relying%20on%20physical%20layer%20channel%20feedback.%0ASpecifically%2C%20we%20design%20a%20radio%20map%20based%20complex-valued%20precoding%0Anetwork~%28RMCPNet%29%20model%2C%20which%20outputs%20the%20base%20station%20precoding%20based%20on%20user%0Alocation.%20RMCPNet%20comprises%20multiple%20subnets%2C%20with%20each%20subnet%20responsible%20for%0Aextracting%20unique%20modal%20features%20from%20diverse%20input%20modalities.%20Furthermore%2C%0Athe%20multi-modal%20embeddings%20derived%20from%20these%20distinct%20subnets%20are%20integrated%0Awithin%20the%20information%20fusion%20layer%2C%20culminating%20in%20a%20unified%20representation.%0AWe%20also%20develop%20a%20specific%20RMCPNet%20training%20algorithm%20that%20employs%20the%20negative%0Aspectral%20efficiency%20as%20the%20loss%20function.%20We%20evaluate%20the%20performance%20of%20the%0Aproposed%20scheme%20on%20the%20public%20DeepMIMO%20dataset%20and%20show%20that%20RMCPNet%20can%0Aachieve%2016%5C%25%20and%2076%5C%25%20performance%20improvements%20over%20the%20conventional%0Areal-valued%20neural%20network%20and%20statistical%20codebook%20approach%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02184v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Channel-Feedback-Free%20Transmission%20for%20Downlink%20FD-RAN%3A%20A%20Radio%20Map%0A%20%20based%20Complex-valued%20Precoding%20Network%20Approach&entry.906535625=Jiwei%20Zhao%20and%20Jiacheng%20Chen%20and%20Zeyu%20Sun%20and%20Yuhang%20Shi%20and%20Haibo%20Zhou%20and%20%20Xuemin%20and%20%20Shen&entry.1292438233=%20%20As%20the%20demand%20for%20high-quality%20services%20proliferates%2C%20an%20innovative%20network%0Aarchitecture%2C%20the%20fully-decoupled%20RAN%20%28FD-RAN%29%2C%20has%20emerged%20for%20more%20flexible%0Aspectrum%20resource%20utilization%20and%20lower%20network%20costs.%20However%2C%20with%20the%0Adecoupling%20of%20uplink%20base%20stations%20and%20downlink%20base%20stations%20in%20FD-RAN%2C%20the%0Atraditional%20transmission%20mechanism%2C%20which%20relies%20on%20real-time%20channel%20feedback%2C%0Ais%20not%20suitable%20as%20the%20receiver%20is%20not%20able%20to%20feedback%20accurate%20and%20timely%0Achannel%20state%20information%20to%20the%20transmitter.%20This%20paper%20proposes%20a%20novel%0Atransmission%20scheme%20without%20relying%20on%20physical%20layer%20channel%20feedback.%0ASpecifically%2C%20we%20design%20a%20radio%20map%20based%20complex-valued%20precoding%0Anetwork~%28RMCPNet%29%20model%2C%20which%20outputs%20the%20base%20station%20precoding%20based%20on%20user%0Alocation.%20RMCPNet%20comprises%20multiple%20subnets%2C%20with%20each%20subnet%20responsible%20for%0Aextracting%20unique%20modal%20features%20from%20diverse%20input%20modalities.%20Furthermore%2C%0Athe%20multi-modal%20embeddings%20derived%20from%20these%20distinct%20subnets%20are%20integrated%0Awithin%20the%20information%20fusion%20layer%2C%20culminating%20in%20a%20unified%20representation.%0AWe%20also%20develop%20a%20specific%20RMCPNet%20training%20algorithm%20that%20employs%20the%20negative%0Aspectral%20efficiency%20as%20the%20loss%20function.%20We%20evaluate%20the%20performance%20of%20the%0Aproposed%20scheme%20on%20the%20public%20DeepMIMO%20dataset%20and%20show%20that%20RMCPNet%20can%0Aachieve%2016%5C%25%20and%2076%5C%25%20performance%20improvements%20over%20the%20conventional%0Areal-valued%20neural%20network%20and%20statistical%20codebook%20approach%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02184v2&entry.124074799=Read"},
{"title": "Guarantees of confidentiality via Hammersley-Chapman-Robbins bounds", "author": "Kamalika Chaudhuri and Chuan Guo and Laurens van der Maaten and Saeed Mahloujifar and Mark Tygert", "abstract": "  Protecting privacy during inference with deep neural networks is possible by\nadding noise to the activations in the last layers prior to the final\nclassifiers or other task-specific layers. The activations in such layers are\nknown as \"features\" (or, less commonly, as \"embeddings\" or \"feature\nembeddings\"). The added noise helps prevent reconstruction of the inputs from\nthe noisy features. Lower bounding the variance of every possible unbiased\nestimator of the inputs quantifies the confidentiality arising from such added\nnoise. Convenient, computationally tractable bounds are available from classic\ninequalities of Hammersley and of Chapman and Robbins -- the HCR bounds.\nNumerical experiments indicate that the HCR bounds are on the precipice of\nbeing effectual for small neural nets with the data sets, \"MNIST\" and\n\"CIFAR-10,\" which contain 10 classes each for image classification. The HCR\nbounds appear to be insufficient on their own to guarantee confidentiality of\nthe inputs to inference with standard deep neural nets, \"ResNet-18\" and\n\"Swin-T,\" pre-trained on the data set, \"ImageNet-1000,\" which contains 1000\nclasses. Supplementing the addition of noise to features with other methods for\nproviding confidentiality may be warranted in the case of ImageNet. In all\ncases, the results reported here limit consideration to amounts of added noise\nthat incur little degradation in the accuracy of classification from the noisy\nfeatures. Thus, the added noise enhances confidentiality without much reduction\nin the accuracy on the task of image classification.\n", "link": "http://arxiv.org/abs/2404.02866v1", "date": "2024-04-03", "relevancy": 1.8924, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4911}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4656}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4468}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Guarantees%20of%20confidentiality%20via%20Hammersley-Chapman-Robbins%20bounds&body=Title%3A%20Guarantees%20of%20confidentiality%20via%20Hammersley-Chapman-Robbins%20bounds%0AAuthor%3A%20Kamalika%20Chaudhuri%20and%20Chuan%20Guo%20and%20Laurens%20van%20der%20Maaten%20and%20Saeed%20Mahloujifar%20and%20Mark%20Tygert%0AAbstract%3A%20%20%20Protecting%20privacy%20during%20inference%20with%20deep%20neural%20networks%20is%20possible%20by%0Aadding%20noise%20to%20the%20activations%20in%20the%20last%20layers%20prior%20to%20the%20final%0Aclassifiers%20or%20other%20task-specific%20layers.%20The%20activations%20in%20such%20layers%20are%0Aknown%20as%20%22features%22%20%28or%2C%20less%20commonly%2C%20as%20%22embeddings%22%20or%20%22feature%0Aembeddings%22%29.%20The%20added%20noise%20helps%20prevent%20reconstruction%20of%20the%20inputs%20from%0Athe%20noisy%20features.%20Lower%20bounding%20the%20variance%20of%20every%20possible%20unbiased%0Aestimator%20of%20the%20inputs%20quantifies%20the%20confidentiality%20arising%20from%20such%20added%0Anoise.%20Convenient%2C%20computationally%20tractable%20bounds%20are%20available%20from%20classic%0Ainequalities%20of%20Hammersley%20and%20of%20Chapman%20and%20Robbins%20--%20the%20HCR%20bounds.%0ANumerical%20experiments%20indicate%20that%20the%20HCR%20bounds%20are%20on%20the%20precipice%20of%0Abeing%20effectual%20for%20small%20neural%20nets%20with%20the%20data%20sets%2C%20%22MNIST%22%20and%0A%22CIFAR-10%2C%22%20which%20contain%2010%20classes%20each%20for%20image%20classification.%20The%20HCR%0Abounds%20appear%20to%20be%20insufficient%20on%20their%20own%20to%20guarantee%20confidentiality%20of%0Athe%20inputs%20to%20inference%20with%20standard%20deep%20neural%20nets%2C%20%22ResNet-18%22%20and%0A%22Swin-T%2C%22%20pre-trained%20on%20the%20data%20set%2C%20%22ImageNet-1000%2C%22%20which%20contains%201000%0Aclasses.%20Supplementing%20the%20addition%20of%20noise%20to%20features%20with%20other%20methods%20for%0Aproviding%20confidentiality%20may%20be%20warranted%20in%20the%20case%20of%20ImageNet.%20In%20all%0Acases%2C%20the%20results%20reported%20here%20limit%20consideration%20to%20amounts%20of%20added%20noise%0Athat%20incur%20little%20degradation%20in%20the%20accuracy%20of%20classification%20from%20the%20noisy%0Afeatures.%20Thus%2C%20the%20added%20noise%20enhances%20confidentiality%20without%20much%20reduction%0Ain%20the%20accuracy%20on%20the%20task%20of%20image%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02866v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guarantees%20of%20confidentiality%20via%20Hammersley-Chapman-Robbins%20bounds&entry.906535625=Kamalika%20Chaudhuri%20and%20Chuan%20Guo%20and%20Laurens%20van%20der%20Maaten%20and%20Saeed%20Mahloujifar%20and%20Mark%20Tygert&entry.1292438233=%20%20Protecting%20privacy%20during%20inference%20with%20deep%20neural%20networks%20is%20possible%20by%0Aadding%20noise%20to%20the%20activations%20in%20the%20last%20layers%20prior%20to%20the%20final%0Aclassifiers%20or%20other%20task-specific%20layers.%20The%20activations%20in%20such%20layers%20are%0Aknown%20as%20%22features%22%20%28or%2C%20less%20commonly%2C%20as%20%22embeddings%22%20or%20%22feature%0Aembeddings%22%29.%20The%20added%20noise%20helps%20prevent%20reconstruction%20of%20the%20inputs%20from%0Athe%20noisy%20features.%20Lower%20bounding%20the%20variance%20of%20every%20possible%20unbiased%0Aestimator%20of%20the%20inputs%20quantifies%20the%20confidentiality%20arising%20from%20such%20added%0Anoise.%20Convenient%2C%20computationally%20tractable%20bounds%20are%20available%20from%20classic%0Ainequalities%20of%20Hammersley%20and%20of%20Chapman%20and%20Robbins%20--%20the%20HCR%20bounds.%0ANumerical%20experiments%20indicate%20that%20the%20HCR%20bounds%20are%20on%20the%20precipice%20of%0Abeing%20effectual%20for%20small%20neural%20nets%20with%20the%20data%20sets%2C%20%22MNIST%22%20and%0A%22CIFAR-10%2C%22%20which%20contain%2010%20classes%20each%20for%20image%20classification.%20The%20HCR%0Abounds%20appear%20to%20be%20insufficient%20on%20their%20own%20to%20guarantee%20confidentiality%20of%0Athe%20inputs%20to%20inference%20with%20standard%20deep%20neural%20nets%2C%20%22ResNet-18%22%20and%0A%22Swin-T%2C%22%20pre-trained%20on%20the%20data%20set%2C%20%22ImageNet-1000%2C%22%20which%20contains%201000%0Aclasses.%20Supplementing%20the%20addition%20of%20noise%20to%20features%20with%20other%20methods%20for%0Aproviding%20confidentiality%20may%20be%20warranted%20in%20the%20case%20of%20ImageNet.%20In%20all%0Acases%2C%20the%20results%20reported%20here%20limit%20consideration%20to%20amounts%20of%20added%20noise%0Athat%20incur%20little%20degradation%20in%20the%20accuracy%20of%20classification%20from%20the%20noisy%0Afeatures.%20Thus%2C%20the%20added%20noise%20enhances%20confidentiality%20without%20much%20reduction%0Ain%20the%20accuracy%20on%20the%20task%20of%20image%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02866v1&entry.124074799=Read"},
{"title": "Gegenbauer Graph Neural Networks for Time-varying Signal Reconstruction", "author": "Jhon A. Castro-Correa and Jhony H. Giraldo and Mohsen Badiey and Fragkiskos D. Malliaros", "abstract": "  Reconstructing time-varying graph signals (or graph time-series imputation)\nis a critical problem in machine learning and signal processing with broad\napplications, ranging from missing data imputation in sensor networks to\ntime-series forecasting. Accurately capturing the spatio-temporal information\ninherent in these signals is crucial for effectively addressing these tasks.\nHowever, existing approaches relying on smoothness assumptions of temporal\ndifferences and simple convex optimization techniques have inherent\nlimitations. To address these challenges, we propose a novel approach that\nincorporates a learning module to enhance the accuracy of the downstream task.\nTo this end, we introduce the Gegenbauer-based graph convolutional (GegenConv)\noperator, which is a generalization of the conventional Chebyshev graph\nconvolution by leveraging the theory of Gegenbauer polynomials. By deviating\nfrom traditional convex problems, we expand the complexity of the model and\noffer a more accurate solution for recovering time-varying graph signals.\nBuilding upon GegenConv, we design the Gegenbauer-based time Graph Neural\nNetwork (GegenGNN) architecture, which adopts an encoder-decoder structure.\nLikewise, our approach also utilizes a dedicated loss function that\nincorporates a mean squared error component alongside Sobolev smoothness\nregularization. This combination enables GegenGNN to capture both the fidelity\nto ground truth and the underlying smoothness properties of the signals,\nenhancing the reconstruction performance. We conduct extensive experiments on\nreal datasets to evaluate the effectiveness of our proposed approach. The\nexperimental results demonstrate that GegenGNN outperforms state-of-the-art\nmethods, showcasing its superior capability in recovering time-varying graph\nsignals.\n", "link": "http://arxiv.org/abs/2403.19800v2", "date": "2024-04-03", "relevancy": 1.8753, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4729}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4661}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4658}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Gegenbauer%20Graph%20Neural%20Networks%20for%20Time-varying%20Signal%20Reconstruction&body=Title%3A%20Gegenbauer%20Graph%20Neural%20Networks%20for%20Time-varying%20Signal%20Reconstruction%0AAuthor%3A%20Jhon%20A.%20Castro-Correa%20and%20Jhony%20H.%20Giraldo%20and%20Mohsen%20Badiey%20and%20Fragkiskos%20D.%20Malliaros%0AAbstract%3A%20%20%20Reconstructing%20time-varying%20graph%20signals%20%28or%20graph%20time-series%20imputation%29%0Ais%20a%20critical%20problem%20in%20machine%20learning%20and%20signal%20processing%20with%20broad%0Aapplications%2C%20ranging%20from%20missing%20data%20imputation%20in%20sensor%20networks%20to%0Atime-series%20forecasting.%20Accurately%20capturing%20the%20spatio-temporal%20information%0Ainherent%20in%20these%20signals%20is%20crucial%20for%20effectively%20addressing%20these%20tasks.%0AHowever%2C%20existing%20approaches%20relying%20on%20smoothness%20assumptions%20of%20temporal%0Adifferences%20and%20simple%20convex%20optimization%20techniques%20have%20inherent%0Alimitations.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20approach%20that%0Aincorporates%20a%20learning%20module%20to%20enhance%20the%20accuracy%20of%20the%20downstream%20task.%0ATo%20this%20end%2C%20we%20introduce%20the%20Gegenbauer-based%20graph%20convolutional%20%28GegenConv%29%0Aoperator%2C%20which%20is%20a%20generalization%20of%20the%20conventional%20Chebyshev%20graph%0Aconvolution%20by%20leveraging%20the%20theory%20of%20Gegenbauer%20polynomials.%20By%20deviating%0Afrom%20traditional%20convex%20problems%2C%20we%20expand%20the%20complexity%20of%20the%20model%20and%0Aoffer%20a%20more%20accurate%20solution%20for%20recovering%20time-varying%20graph%20signals.%0ABuilding%20upon%20GegenConv%2C%20we%20design%20the%20Gegenbauer-based%20time%20Graph%20Neural%0ANetwork%20%28GegenGNN%29%20architecture%2C%20which%20adopts%20an%20encoder-decoder%20structure.%0ALikewise%2C%20our%20approach%20also%20utilizes%20a%20dedicated%20loss%20function%20that%0Aincorporates%20a%20mean%20squared%20error%20component%20alongside%20Sobolev%20smoothness%0Aregularization.%20This%20combination%20enables%20GegenGNN%20to%20capture%20both%20the%20fidelity%0Ato%20ground%20truth%20and%20the%20underlying%20smoothness%20properties%20of%20the%20signals%2C%0Aenhancing%20the%20reconstruction%20performance.%20We%20conduct%20extensive%20experiments%20on%0Areal%20datasets%20to%20evaluate%20the%20effectiveness%20of%20our%20proposed%20approach.%20The%0Aexperimental%20results%20demonstrate%20that%20GegenGNN%20outperforms%20state-of-the-art%0Amethods%2C%20showcasing%20its%20superior%20capability%20in%20recovering%20time-varying%20graph%0Asignals.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19800v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gegenbauer%20Graph%20Neural%20Networks%20for%20Time-varying%20Signal%20Reconstruction&entry.906535625=Jhon%20A.%20Castro-Correa%20and%20Jhony%20H.%20Giraldo%20and%20Mohsen%20Badiey%20and%20Fragkiskos%20D.%20Malliaros&entry.1292438233=%20%20Reconstructing%20time-varying%20graph%20signals%20%28or%20graph%20time-series%20imputation%29%0Ais%20a%20critical%20problem%20in%20machine%20learning%20and%20signal%20processing%20with%20broad%0Aapplications%2C%20ranging%20from%20missing%20data%20imputation%20in%20sensor%20networks%20to%0Atime-series%20forecasting.%20Accurately%20capturing%20the%20spatio-temporal%20information%0Ainherent%20in%20these%20signals%20is%20crucial%20for%20effectively%20addressing%20these%20tasks.%0AHowever%2C%20existing%20approaches%20relying%20on%20smoothness%20assumptions%20of%20temporal%0Adifferences%20and%20simple%20convex%20optimization%20techniques%20have%20inherent%0Alimitations.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20approach%20that%0Aincorporates%20a%20learning%20module%20to%20enhance%20the%20accuracy%20of%20the%20downstream%20task.%0ATo%20this%20end%2C%20we%20introduce%20the%20Gegenbauer-based%20graph%20convolutional%20%28GegenConv%29%0Aoperator%2C%20which%20is%20a%20generalization%20of%20the%20conventional%20Chebyshev%20graph%0Aconvolution%20by%20leveraging%20the%20theory%20of%20Gegenbauer%20polynomials.%20By%20deviating%0Afrom%20traditional%20convex%20problems%2C%20we%20expand%20the%20complexity%20of%20the%20model%20and%0Aoffer%20a%20more%20accurate%20solution%20for%20recovering%20time-varying%20graph%20signals.%0ABuilding%20upon%20GegenConv%2C%20we%20design%20the%20Gegenbauer-based%20time%20Graph%20Neural%0ANetwork%20%28GegenGNN%29%20architecture%2C%20which%20adopts%20an%20encoder-decoder%20structure.%0ALikewise%2C%20our%20approach%20also%20utilizes%20a%20dedicated%20loss%20function%20that%0Aincorporates%20a%20mean%20squared%20error%20component%20alongside%20Sobolev%20smoothness%0Aregularization.%20This%20combination%20enables%20GegenGNN%20to%20capture%20both%20the%20fidelity%0Ato%20ground%20truth%20and%20the%20underlying%20smoothness%20properties%20of%20the%20signals%2C%0Aenhancing%20the%20reconstruction%20performance.%20We%20conduct%20extensive%20experiments%20on%0Areal%20datasets%20to%20evaluate%20the%20effectiveness%20of%20our%20proposed%20approach.%20The%0Aexperimental%20results%20demonstrate%20that%20GegenGNN%20outperforms%20state-of-the-art%0Amethods%2C%20showcasing%20its%20superior%20capability%20in%20recovering%20time-varying%20graph%0Asignals.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19800v2&entry.124074799=Read"},
{"title": "AddSR: Accelerating Diffusion-based Blind Super-Resolution with\n  Adversarial Diffusion Distillation", "author": "Rui Xie and Ying Tai and Kai Zhang and Zhenyu Zhang and Jun Zhou and Jian Yang", "abstract": "  Blind super-resolution methods based on stable diffusion showcase formidable\ngenerative capabilities in reconstructing clear high-resolution images with\nintricate details from low-resolution inputs. However, their practical\napplicability is often hampered by poor efficiency, stemming from the\nrequirement of thousands or hundreds of sampling steps. Inspired by the\nefficient text-to-image approach adversarial diffusion distillation (ADD), we\ndesign AddSR to address this issue by incorporating the ideas of both\ndistillation and ControlNet. Specifically, we first propose a prediction-based\nself-refinement strategy to provide high-frequency information in the student\nmodel output with marginal additional time cost. Furthermore, we refine the\ntraining process by employing HR images, rather than LR images, to regulate the\nteacher model, providing a more robust constraint for distillation. Second, we\nintroduce a timestep-adapting loss to address the perception-distortion\nimbalance problem introduced by ADD. Extensive experiments demonstrate our\nAddSR generates better restoration results, while achieving faster speed than\nprevious SD-based state-of-the-art models (e.g., 7x faster than SeeSR).\n", "link": "http://arxiv.org/abs/2404.01717v2", "date": "2024-04-03", "relevancy": 1.8741, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6566}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6176}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6105}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AddSR%3A%20Accelerating%20Diffusion-based%20Blind%20Super-Resolution%20with%0A%20%20Adversarial%20Diffusion%20Distillation&body=Title%3A%20AddSR%3A%20Accelerating%20Diffusion-based%20Blind%20Super-Resolution%20with%0A%20%20Adversarial%20Diffusion%20Distillation%0AAuthor%3A%20Rui%20Xie%20and%20Ying%20Tai%20and%20Kai%20Zhang%20and%20Zhenyu%20Zhang%20and%20Jun%20Zhou%20and%20Jian%20Yang%0AAbstract%3A%20%20%20Blind%20super-resolution%20methods%20based%20on%20stable%20diffusion%20showcase%20formidable%0Agenerative%20capabilities%20in%20reconstructing%20clear%20high-resolution%20images%20with%0Aintricate%20details%20from%20low-resolution%20inputs.%20However%2C%20their%20practical%0Aapplicability%20is%20often%20hampered%20by%20poor%20efficiency%2C%20stemming%20from%20the%0Arequirement%20of%20thousands%20or%20hundreds%20of%20sampling%20steps.%20Inspired%20by%20the%0Aefficient%20text-to-image%20approach%20adversarial%20diffusion%20distillation%20%28ADD%29%2C%20we%0Adesign%20AddSR%20to%20address%20this%20issue%20by%20incorporating%20the%20ideas%20of%20both%0Adistillation%20and%20ControlNet.%20Specifically%2C%20we%20first%20propose%20a%20prediction-based%0Aself-refinement%20strategy%20to%20provide%20high-frequency%20information%20in%20the%20student%0Amodel%20output%20with%20marginal%20additional%20time%20cost.%20Furthermore%2C%20we%20refine%20the%0Atraining%20process%20by%20employing%20HR%20images%2C%20rather%20than%20LR%20images%2C%20to%20regulate%20the%0Ateacher%20model%2C%20providing%20a%20more%20robust%20constraint%20for%20distillation.%20Second%2C%20we%0Aintroduce%20a%20timestep-adapting%20loss%20to%20address%20the%20perception-distortion%0Aimbalance%20problem%20introduced%20by%20ADD.%20Extensive%20experiments%20demonstrate%20our%0AAddSR%20generates%20better%20restoration%20results%2C%20while%20achieving%20faster%20speed%20than%0Aprevious%20SD-based%20state-of-the-art%20models%20%28e.g.%2C%207x%20faster%20than%20SeeSR%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01717v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AddSR%3A%20Accelerating%20Diffusion-based%20Blind%20Super-Resolution%20with%0A%20%20Adversarial%20Diffusion%20Distillation&entry.906535625=Rui%20Xie%20and%20Ying%20Tai%20and%20Kai%20Zhang%20and%20Zhenyu%20Zhang%20and%20Jun%20Zhou%20and%20Jian%20Yang&entry.1292438233=%20%20Blind%20super-resolution%20methods%20based%20on%20stable%20diffusion%20showcase%20formidable%0Agenerative%20capabilities%20in%20reconstructing%20clear%20high-resolution%20images%20with%0Aintricate%20details%20from%20low-resolution%20inputs.%20However%2C%20their%20practical%0Aapplicability%20is%20often%20hampered%20by%20poor%20efficiency%2C%20stemming%20from%20the%0Arequirement%20of%20thousands%20or%20hundreds%20of%20sampling%20steps.%20Inspired%20by%20the%0Aefficient%20text-to-image%20approach%20adversarial%20diffusion%20distillation%20%28ADD%29%2C%20we%0Adesign%20AddSR%20to%20address%20this%20issue%20by%20incorporating%20the%20ideas%20of%20both%0Adistillation%20and%20ControlNet.%20Specifically%2C%20we%20first%20propose%20a%20prediction-based%0Aself-refinement%20strategy%20to%20provide%20high-frequency%20information%20in%20the%20student%0Amodel%20output%20with%20marginal%20additional%20time%20cost.%20Furthermore%2C%20we%20refine%20the%0Atraining%20process%20by%20employing%20HR%20images%2C%20rather%20than%20LR%20images%2C%20to%20regulate%20the%0Ateacher%20model%2C%20providing%20a%20more%20robust%20constraint%20for%20distillation.%20Second%2C%20we%0Aintroduce%20a%20timestep-adapting%20loss%20to%20address%20the%20perception-distortion%0Aimbalance%20problem%20introduced%20by%20ADD.%20Extensive%20experiments%20demonstrate%20our%0AAddSR%20generates%20better%20restoration%20results%2C%20while%20achieving%20faster%20speed%20than%0Aprevious%20SD-based%20state-of-the-art%20models%20%28e.g.%2C%207x%20faster%20than%20SeeSR%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01717v2&entry.124074799=Read"},
{"title": "Conifer: Improving Complex Constrained Instruction-Following Ability of\n  Large Language Models", "author": "Haoran Sun and Lixin Liu and Junjie Li and Fengyu Wang and Baohua Dong and Ran Lin and Ruohui Huang", "abstract": "  The ability of large language models (LLMs) to follow instructions is crucial\nto real-world applications. Despite recent advances, several studies have\nhighlighted that LLMs struggle when faced with challenging instructions,\nespecially those that include complex constraints, hindering their\neffectiveness in various tasks. To address this challenge, we introduce\nConifer, a novel instruction tuning dataset, designed to enhance LLMs to follow\nmulti-level instructions with complex constraints. Utilizing GPT-4, we curate\nthe dataset by a series of LLM-driven refinement processes to ensure high\nquality. We also propose a progressive learning scheme that emphasizes an\neasy-to-hard progression, and learning from process feedback. Models trained\nwith Conifer exhibit remarkable improvements in instruction-following\nabilities, especially for instructions with complex constraints. On several\ninstruction-following benchmarks, our 7B model outperforms the state-of-the-art\nopen-source 7B models, even exceeds the performance of models 10 times larger\non certain metrics. All the code and Conifer dataset are available at\nhttps://www.github.com/ConiferLM/Conifer.\n", "link": "http://arxiv.org/abs/2404.02823v1", "date": "2024-04-03", "relevancy": 1.8721, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4836}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4723}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4575}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Conifer%3A%20Improving%20Complex%20Constrained%20Instruction-Following%20Ability%20of%0A%20%20Large%20Language%20Models&body=Title%3A%20Conifer%3A%20Improving%20Complex%20Constrained%20Instruction-Following%20Ability%20of%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Haoran%20Sun%20and%20Lixin%20Liu%20and%20Junjie%20Li%20and%20Fengyu%20Wang%20and%20Baohua%20Dong%20and%20Ran%20Lin%20and%20Ruohui%20Huang%0AAbstract%3A%20%20%20The%20ability%20of%20large%20language%20models%20%28LLMs%29%20to%20follow%20instructions%20is%20crucial%0Ato%20real-world%20applications.%20Despite%20recent%20advances%2C%20several%20studies%20have%0Ahighlighted%20that%20LLMs%20struggle%20when%20faced%20with%20challenging%20instructions%2C%0Aespecially%20those%20that%20include%20complex%20constraints%2C%20hindering%20their%0Aeffectiveness%20in%20various%20tasks.%20To%20address%20this%20challenge%2C%20we%20introduce%0AConifer%2C%20a%20novel%20instruction%20tuning%20dataset%2C%20designed%20to%20enhance%20LLMs%20to%20follow%0Amulti-level%20instructions%20with%20complex%20constraints.%20Utilizing%20GPT-4%2C%20we%20curate%0Athe%20dataset%20by%20a%20series%20of%20LLM-driven%20refinement%20processes%20to%20ensure%20high%0Aquality.%20We%20also%20propose%20a%20progressive%20learning%20scheme%20that%20emphasizes%20an%0Aeasy-to-hard%20progression%2C%20and%20learning%20from%20process%20feedback.%20Models%20trained%0Awith%20Conifer%20exhibit%20remarkable%20improvements%20in%20instruction-following%0Aabilities%2C%20especially%20for%20instructions%20with%20complex%20constraints.%20On%20several%0Ainstruction-following%20benchmarks%2C%20our%207B%20model%20outperforms%20the%20state-of-the-art%0Aopen-source%207B%20models%2C%20even%20exceeds%20the%20performance%20of%20models%2010%20times%20larger%0Aon%20certain%20metrics.%20All%20the%20code%20and%20Conifer%20dataset%20are%20available%20at%0Ahttps%3A//www.github.com/ConiferLM/Conifer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02823v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conifer%3A%20Improving%20Complex%20Constrained%20Instruction-Following%20Ability%20of%0A%20%20Large%20Language%20Models&entry.906535625=Haoran%20Sun%20and%20Lixin%20Liu%20and%20Junjie%20Li%20and%20Fengyu%20Wang%20and%20Baohua%20Dong%20and%20Ran%20Lin%20and%20Ruohui%20Huang&entry.1292438233=%20%20The%20ability%20of%20large%20language%20models%20%28LLMs%29%20to%20follow%20instructions%20is%20crucial%0Ato%20real-world%20applications.%20Despite%20recent%20advances%2C%20several%20studies%20have%0Ahighlighted%20that%20LLMs%20struggle%20when%20faced%20with%20challenging%20instructions%2C%0Aespecially%20those%20that%20include%20complex%20constraints%2C%20hindering%20their%0Aeffectiveness%20in%20various%20tasks.%20To%20address%20this%20challenge%2C%20we%20introduce%0AConifer%2C%20a%20novel%20instruction%20tuning%20dataset%2C%20designed%20to%20enhance%20LLMs%20to%20follow%0Amulti-level%20instructions%20with%20complex%20constraints.%20Utilizing%20GPT-4%2C%20we%20curate%0Athe%20dataset%20by%20a%20series%20of%20LLM-driven%20refinement%20processes%20to%20ensure%20high%0Aquality.%20We%20also%20propose%20a%20progressive%20learning%20scheme%20that%20emphasizes%20an%0Aeasy-to-hard%20progression%2C%20and%20learning%20from%20process%20feedback.%20Models%20trained%0Awith%20Conifer%20exhibit%20remarkable%20improvements%20in%20instruction-following%0Aabilities%2C%20especially%20for%20instructions%20with%20complex%20constraints.%20On%20several%0Ainstruction-following%20benchmarks%2C%20our%207B%20model%20outperforms%20the%20state-of-the-art%0Aopen-source%207B%20models%2C%20even%20exceeds%20the%20performance%20of%20models%2010%20times%20larger%0Aon%20certain%20metrics.%20All%20the%20code%20and%20Conifer%20dataset%20are%20available%20at%0Ahttps%3A//www.github.com/ConiferLM/Conifer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02823v1&entry.124074799=Read"},
{"title": "CamemBERT-bio: Leveraging Continual Pre-training for Cost-Effective\n  Models on French Biomedical Data", "author": "Rian Touchent and Laurent Romary and Eric de la Clergerie", "abstract": "  Clinical data in hospitals are increasingly accessible for research through\nclinical data warehouses. However these documents are unstructured and it is\ntherefore necessary to extract information from medical reports to conduct\nclinical studies. Transfer learning with BERT-like models such as CamemBERT has\nallowed major advances for French, especially for named entity recognition.\nHowever, these models are trained for plain language and are less efficient on\nbiomedical data. Addressing this gap, we introduce CamemBERT-bio, a dedicated\nFrench biomedical model derived from a new public French biomedical dataset.\nThrough continual pre-training of the original CamemBERT, CamemBERT-bio\nachieves an improvement of 2.54 points of F1-score on average across various\nbiomedical named entity recognition tasks, reinforcing the potential of\ncontinual pre-training as an equally proficient yet less computationally\nintensive alternative to training from scratch. Additionally, we highlight the\nimportance of using a standard evaluation protocol that provides a clear view\nof the current state-of-the-art for French biomedical models.\n", "link": "http://arxiv.org/abs/2306.15550v3", "date": "2024-04-03", "relevancy": 1.8542, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4837}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4506}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4486}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CamemBERT-bio%3A%20Leveraging%20Continual%20Pre-training%20for%20Cost-Effective%0A%20%20Models%20on%20French%20Biomedical%20Data&body=Title%3A%20CamemBERT-bio%3A%20Leveraging%20Continual%20Pre-training%20for%20Cost-Effective%0A%20%20Models%20on%20French%20Biomedical%20Data%0AAuthor%3A%20Rian%20Touchent%20and%20Laurent%20Romary%20and%20Eric%20de%20la%20Clergerie%0AAbstract%3A%20%20%20Clinical%20data%20in%20hospitals%20are%20increasingly%20accessible%20for%20research%20through%0Aclinical%20data%20warehouses.%20However%20these%20documents%20are%20unstructured%20and%20it%20is%0Atherefore%20necessary%20to%20extract%20information%20from%20medical%20reports%20to%20conduct%0Aclinical%20studies.%20Transfer%20learning%20with%20BERT-like%20models%20such%20as%20CamemBERT%20has%0Aallowed%20major%20advances%20for%20French%2C%20especially%20for%20named%20entity%20recognition.%0AHowever%2C%20these%20models%20are%20trained%20for%20plain%20language%20and%20are%20less%20efficient%20on%0Abiomedical%20data.%20Addressing%20this%20gap%2C%20we%20introduce%20CamemBERT-bio%2C%20a%20dedicated%0AFrench%20biomedical%20model%20derived%20from%20a%20new%20public%20French%20biomedical%20dataset.%0AThrough%20continual%20pre-training%20of%20the%20original%20CamemBERT%2C%20CamemBERT-bio%0Aachieves%20an%20improvement%20of%202.54%20points%20of%20F1-score%20on%20average%20across%20various%0Abiomedical%20named%20entity%20recognition%20tasks%2C%20reinforcing%20the%20potential%20of%0Acontinual%20pre-training%20as%20an%20equally%20proficient%20yet%20less%20computationally%0Aintensive%20alternative%20to%20training%20from%20scratch.%20Additionally%2C%20we%20highlight%20the%0Aimportance%20of%20using%20a%20standard%20evaluation%20protocol%20that%20provides%20a%20clear%20view%0Aof%20the%20current%20state-of-the-art%20for%20French%20biomedical%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.15550v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CamemBERT-bio%3A%20Leveraging%20Continual%20Pre-training%20for%20Cost-Effective%0A%20%20Models%20on%20French%20Biomedical%20Data&entry.906535625=Rian%20Touchent%20and%20Laurent%20Romary%20and%20Eric%20de%20la%20Clergerie&entry.1292438233=%20%20Clinical%20data%20in%20hospitals%20are%20increasingly%20accessible%20for%20research%20through%0Aclinical%20data%20warehouses.%20However%20these%20documents%20are%20unstructured%20and%20it%20is%0Atherefore%20necessary%20to%20extract%20information%20from%20medical%20reports%20to%20conduct%0Aclinical%20studies.%20Transfer%20learning%20with%20BERT-like%20models%20such%20as%20CamemBERT%20has%0Aallowed%20major%20advances%20for%20French%2C%20especially%20for%20named%20entity%20recognition.%0AHowever%2C%20these%20models%20are%20trained%20for%20plain%20language%20and%20are%20less%20efficient%20on%0Abiomedical%20data.%20Addressing%20this%20gap%2C%20we%20introduce%20CamemBERT-bio%2C%20a%20dedicated%0AFrench%20biomedical%20model%20derived%20from%20a%20new%20public%20French%20biomedical%20dataset.%0AThrough%20continual%20pre-training%20of%20the%20original%20CamemBERT%2C%20CamemBERT-bio%0Aachieves%20an%20improvement%20of%202.54%20points%20of%20F1-score%20on%20average%20across%20various%0Abiomedical%20named%20entity%20recognition%20tasks%2C%20reinforcing%20the%20potential%20of%0Acontinual%20pre-training%20as%20an%20equally%20proficient%20yet%20less%20computationally%0Aintensive%20alternative%20to%20training%20from%20scratch.%20Additionally%2C%20we%20highlight%20the%0Aimportance%20of%20using%20a%20standard%20evaluation%20protocol%20that%20provides%20a%20clear%20view%0Aof%20the%20current%20state-of-the-art%20for%20French%20biomedical%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.15550v3&entry.124074799=Read"},
{"title": "AGFSync: Leveraging AI-Generated Feedback for Preference Optimization in\n  Text-to-Image Generation", "author": "Jingkun An and Yinghao Zhu and Zongjian Li and Haoran Feng and Bohua Chen and Yemin Shi and Chengwei Pan", "abstract": "  Text-to-Image (T2I) diffusion models have achieved remarkable success in\nimage generation. Despite their progress, challenges remain in both\nprompt-following ability, image quality and lack of high-quality datasets,\nwhich are essential for refining these models. As acquiring labeled data is\ncostly, we introduce AGFSync, a framework that enhances T2I diffusion models\nthrough Direct Preference Optimization (DPO) in a fully AI-driven approach.\nAGFSync utilizes Vision-Language Models (VLM) to assess image quality across\nstyle, coherence, and aesthetics, generating feedback data within an AI-driven\nloop. By applying AGFSync to leading T2I models such as SD v1.4, v1.5, and\nSDXL, our extensive experiments on the TIFA dataset demonstrate notable\nimprovements in VQA scores, aesthetic evaluations, and performance on the HPSv2\nbenchmark, consistently outperforming the base models. AGFSync's method of\nrefining T2I diffusion models paves the way for scalable alignment techniques.\n", "link": "http://arxiv.org/abs/2403.13352v3", "date": "2024-04-03", "relevancy": 1.845, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6348}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6134}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6077}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AGFSync%3A%20Leveraging%20AI-Generated%20Feedback%20for%20Preference%20Optimization%20in%0A%20%20Text-to-Image%20Generation&body=Title%3A%20AGFSync%3A%20Leveraging%20AI-Generated%20Feedback%20for%20Preference%20Optimization%20in%0A%20%20Text-to-Image%20Generation%0AAuthor%3A%20Jingkun%20An%20and%20Yinghao%20Zhu%20and%20Zongjian%20Li%20and%20Haoran%20Feng%20and%20Bohua%20Chen%20and%20Yemin%20Shi%20and%20Chengwei%20Pan%0AAbstract%3A%20%20%20Text-to-Image%20%28T2I%29%20diffusion%20models%20have%20achieved%20remarkable%20success%20in%0Aimage%20generation.%20Despite%20their%20progress%2C%20challenges%20remain%20in%20both%0Aprompt-following%20ability%2C%20image%20quality%20and%20lack%20of%20high-quality%20datasets%2C%0Awhich%20are%20essential%20for%20refining%20these%20models.%20As%20acquiring%20labeled%20data%20is%0Acostly%2C%20we%20introduce%20AGFSync%2C%20a%20framework%20that%20enhances%20T2I%20diffusion%20models%0Athrough%20Direct%20Preference%20Optimization%20%28DPO%29%20in%20a%20fully%20AI-driven%20approach.%0AAGFSync%20utilizes%20Vision-Language%20Models%20%28VLM%29%20to%20assess%20image%20quality%20across%0Astyle%2C%20coherence%2C%20and%20aesthetics%2C%20generating%20feedback%20data%20within%20an%20AI-driven%0Aloop.%20By%20applying%20AGFSync%20to%20leading%20T2I%20models%20such%20as%20SD%20v1.4%2C%20v1.5%2C%20and%0ASDXL%2C%20our%20extensive%20experiments%20on%20the%20TIFA%20dataset%20demonstrate%20notable%0Aimprovements%20in%20VQA%20scores%2C%20aesthetic%20evaluations%2C%20and%20performance%20on%20the%20HPSv2%0Abenchmark%2C%20consistently%20outperforming%20the%20base%20models.%20AGFSync%27s%20method%20of%0Arefining%20T2I%20diffusion%20models%20paves%20the%20way%20for%20scalable%20alignment%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13352v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AGFSync%3A%20Leveraging%20AI-Generated%20Feedback%20for%20Preference%20Optimization%20in%0A%20%20Text-to-Image%20Generation&entry.906535625=Jingkun%20An%20and%20Yinghao%20Zhu%20and%20Zongjian%20Li%20and%20Haoran%20Feng%20and%20Bohua%20Chen%20and%20Yemin%20Shi%20and%20Chengwei%20Pan&entry.1292438233=%20%20Text-to-Image%20%28T2I%29%20diffusion%20models%20have%20achieved%20remarkable%20success%20in%0Aimage%20generation.%20Despite%20their%20progress%2C%20challenges%20remain%20in%20both%0Aprompt-following%20ability%2C%20image%20quality%20and%20lack%20of%20high-quality%20datasets%2C%0Awhich%20are%20essential%20for%20refining%20these%20models.%20As%20acquiring%20labeled%20data%20is%0Acostly%2C%20we%20introduce%20AGFSync%2C%20a%20framework%20that%20enhances%20T2I%20diffusion%20models%0Athrough%20Direct%20Preference%20Optimization%20%28DPO%29%20in%20a%20fully%20AI-driven%20approach.%0AAGFSync%20utilizes%20Vision-Language%20Models%20%28VLM%29%20to%20assess%20image%20quality%20across%0Astyle%2C%20coherence%2C%20and%20aesthetics%2C%20generating%20feedback%20data%20within%20an%20AI-driven%0Aloop.%20By%20applying%20AGFSync%20to%20leading%20T2I%20models%20such%20as%20SD%20v1.4%2C%20v1.5%2C%20and%0ASDXL%2C%20our%20extensive%20experiments%20on%20the%20TIFA%20dataset%20demonstrate%20notable%0Aimprovements%20in%20VQA%20scores%2C%20aesthetic%20evaluations%2C%20and%20performance%20on%20the%20HPSv2%0Abenchmark%2C%20consistently%20outperforming%20the%20base%20models.%20AGFSync%27s%20method%20of%0Arefining%20T2I%20diffusion%20models%20paves%20the%20way%20for%20scalable%20alignment%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13352v3&entry.124074799=Read"},
{"title": "Your Student is Better Than Expected: Adaptive Teacher-Student\n  Collaboration for Text-Conditional Diffusion Models", "author": "Nikita Starodubcev and Artem Fedorov and Artem Babenko and Dmitry Baranchuk", "abstract": "  Knowledge distillation methods have recently shown to be a promising\ndirection to speedup the synthesis of large-scale diffusion models by requiring\nonly a few inference steps. While several powerful distillation methods were\nrecently proposed, the overall quality of student samples is typically lower\ncompared to the teacher ones, which hinders their practical usage. In this\nwork, we investigate the relative quality of samples produced by the teacher\ntext-to-image diffusion model and its distilled student version. As our main\nempirical finding, we discover that a noticeable portion of student samples\nexhibit superior fidelity compared to the teacher ones, despite the\n\"approximate\" nature of the student. Based on this finding, we propose an\nadaptive collaboration between student and teacher diffusion models for\neffective text-to-image synthesis. Specifically, the distilled model produces\nthe initial sample, and then an oracle decides whether it needs further\nimprovements with a slow teacher model. Extensive experiments demonstrate that\nthe designed pipeline surpasses state-of-the-art text-to-image alternatives for\nvarious inference budgets in terms of human preference. Furthermore, the\nproposed approach can be naturally used in popular applications such as\ntext-guided image editing and controllable generation.\n", "link": "http://arxiv.org/abs/2312.10835v3", "date": "2024-04-03", "relevancy": 1.8425, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6331}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6297}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6004}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Your%20Student%20is%20Better%20Than%20Expected%3A%20Adaptive%20Teacher-Student%0A%20%20Collaboration%20for%20Text-Conditional%20Diffusion%20Models&body=Title%3A%20Your%20Student%20is%20Better%20Than%20Expected%3A%20Adaptive%20Teacher-Student%0A%20%20Collaboration%20for%20Text-Conditional%20Diffusion%20Models%0AAuthor%3A%20Nikita%20Starodubcev%20and%20Artem%20Fedorov%20and%20Artem%20Babenko%20and%20Dmitry%20Baranchuk%0AAbstract%3A%20%20%20Knowledge%20distillation%20methods%20have%20recently%20shown%20to%20be%20a%20promising%0Adirection%20to%20speedup%20the%20synthesis%20of%20large-scale%20diffusion%20models%20by%20requiring%0Aonly%20a%20few%20inference%20steps.%20While%20several%20powerful%20distillation%20methods%20were%0Arecently%20proposed%2C%20the%20overall%20quality%20of%20student%20samples%20is%20typically%20lower%0Acompared%20to%20the%20teacher%20ones%2C%20which%20hinders%20their%20practical%20usage.%20In%20this%0Awork%2C%20we%20investigate%20the%20relative%20quality%20of%20samples%20produced%20by%20the%20teacher%0Atext-to-image%20diffusion%20model%20and%20its%20distilled%20student%20version.%20As%20our%20main%0Aempirical%20finding%2C%20we%20discover%20that%20a%20noticeable%20portion%20of%20student%20samples%0Aexhibit%20superior%20fidelity%20compared%20to%20the%20teacher%20ones%2C%20despite%20the%0A%22approximate%22%20nature%20of%20the%20student.%20Based%20on%20this%20finding%2C%20we%20propose%20an%0Aadaptive%20collaboration%20between%20student%20and%20teacher%20diffusion%20models%20for%0Aeffective%20text-to-image%20synthesis.%20Specifically%2C%20the%20distilled%20model%20produces%0Athe%20initial%20sample%2C%20and%20then%20an%20oracle%20decides%20whether%20it%20needs%20further%0Aimprovements%20with%20a%20slow%20teacher%20model.%20Extensive%20experiments%20demonstrate%20that%0Athe%20designed%20pipeline%20surpasses%20state-of-the-art%20text-to-image%20alternatives%20for%0Avarious%20inference%20budgets%20in%20terms%20of%20human%20preference.%20Furthermore%2C%20the%0Aproposed%20approach%20can%20be%20naturally%20used%20in%20popular%20applications%20such%20as%0Atext-guided%20image%20editing%20and%20controllable%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.10835v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Your%20Student%20is%20Better%20Than%20Expected%3A%20Adaptive%20Teacher-Student%0A%20%20Collaboration%20for%20Text-Conditional%20Diffusion%20Models&entry.906535625=Nikita%20Starodubcev%20and%20Artem%20Fedorov%20and%20Artem%20Babenko%20and%20Dmitry%20Baranchuk&entry.1292438233=%20%20Knowledge%20distillation%20methods%20have%20recently%20shown%20to%20be%20a%20promising%0Adirection%20to%20speedup%20the%20synthesis%20of%20large-scale%20diffusion%20models%20by%20requiring%0Aonly%20a%20few%20inference%20steps.%20While%20several%20powerful%20distillation%20methods%20were%0Arecently%20proposed%2C%20the%20overall%20quality%20of%20student%20samples%20is%20typically%20lower%0Acompared%20to%20the%20teacher%20ones%2C%20which%20hinders%20their%20practical%20usage.%20In%20this%0Awork%2C%20we%20investigate%20the%20relative%20quality%20of%20samples%20produced%20by%20the%20teacher%0Atext-to-image%20diffusion%20model%20and%20its%20distilled%20student%20version.%20As%20our%20main%0Aempirical%20finding%2C%20we%20discover%20that%20a%20noticeable%20portion%20of%20student%20samples%0Aexhibit%20superior%20fidelity%20compared%20to%20the%20teacher%20ones%2C%20despite%20the%0A%22approximate%22%20nature%20of%20the%20student.%20Based%20on%20this%20finding%2C%20we%20propose%20an%0Aadaptive%20collaboration%20between%20student%20and%20teacher%20diffusion%20models%20for%0Aeffective%20text-to-image%20synthesis.%20Specifically%2C%20the%20distilled%20model%20produces%0Athe%20initial%20sample%2C%20and%20then%20an%20oracle%20decides%20whether%20it%20needs%20further%0Aimprovements%20with%20a%20slow%20teacher%20model.%20Extensive%20experiments%20demonstrate%20that%0Athe%20designed%20pipeline%20surpasses%20state-of-the-art%20text-to-image%20alternatives%20for%0Avarious%20inference%20budgets%20in%20terms%20of%20human%20preference.%20Furthermore%2C%20the%0Aproposed%20approach%20can%20be%20naturally%20used%20in%20popular%20applications%20such%20as%0Atext-guided%20image%20editing%20and%20controllable%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.10835v3&entry.124074799=Read"},
{"title": "Learning Sequence Attractors in Recurrent Networks with Hidden Neurons", "author": "Yao Lu and Si Wu", "abstract": "  The brain is targeted for processing temporal sequence information. It\nremains largely unclear how the brain learns to store and retrieve sequence\nmemories. Here, we study how recurrent networks of binary neurons learn\nsequence attractors to store predefined pattern sequences and retrieve them\nrobustly. We show that to store arbitrary pattern sequences, it is necessary\nfor the network to include hidden neurons even though their role in displaying\nsequence memories is indirect. We develop a local learning algorithm to learn\nsequence attractors in the networks with hidden neurons. The algorithm is\nproven to converge and lead to sequence attractors. We demonstrate that the\nnetwork model can store and retrieve sequences robustly on synthetic and\nreal-world datasets. We hope that this study provides new insights in\nunderstanding sequence memory and temporal information processing in the brain.\n", "link": "http://arxiv.org/abs/2404.02729v1", "date": "2024-04-03", "relevancy": 1.8386, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4658}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.465}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4308}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20Sequence%20Attractors%20in%20Recurrent%20Networks%20with%20Hidden%20Neurons&body=Title%3A%20Learning%20Sequence%20Attractors%20in%20Recurrent%20Networks%20with%20Hidden%20Neurons%0AAuthor%3A%20Yao%20Lu%20and%20Si%20Wu%0AAbstract%3A%20%20%20The%20brain%20is%20targeted%20for%20processing%20temporal%20sequence%20information.%20It%0Aremains%20largely%20unclear%20how%20the%20brain%20learns%20to%20store%20and%20retrieve%20sequence%0Amemories.%20Here%2C%20we%20study%20how%20recurrent%20networks%20of%20binary%20neurons%20learn%0Asequence%20attractors%20to%20store%20predefined%20pattern%20sequences%20and%20retrieve%20them%0Arobustly.%20We%20show%20that%20to%20store%20arbitrary%20pattern%20sequences%2C%20it%20is%20necessary%0Afor%20the%20network%20to%20include%20hidden%20neurons%20even%20though%20their%20role%20in%20displaying%0Asequence%20memories%20is%20indirect.%20We%20develop%20a%20local%20learning%20algorithm%20to%20learn%0Asequence%20attractors%20in%20the%20networks%20with%20hidden%20neurons.%20The%20algorithm%20is%0Aproven%20to%20converge%20and%20lead%20to%20sequence%20attractors.%20We%20demonstrate%20that%20the%0Anetwork%20model%20can%20store%20and%20retrieve%20sequences%20robustly%20on%20synthetic%20and%0Areal-world%20datasets.%20We%20hope%20that%20this%20study%20provides%20new%20insights%20in%0Aunderstanding%20sequence%20memory%20and%20temporal%20information%20processing%20in%20the%20brain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02729v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Sequence%20Attractors%20in%20Recurrent%20Networks%20with%20Hidden%20Neurons&entry.906535625=Yao%20Lu%20and%20Si%20Wu&entry.1292438233=%20%20The%20brain%20is%20targeted%20for%20processing%20temporal%20sequence%20information.%20It%0Aremains%20largely%20unclear%20how%20the%20brain%20learns%20to%20store%20and%20retrieve%20sequence%0Amemories.%20Here%2C%20we%20study%20how%20recurrent%20networks%20of%20binary%20neurons%20learn%0Asequence%20attractors%20to%20store%20predefined%20pattern%20sequences%20and%20retrieve%20them%0Arobustly.%20We%20show%20that%20to%20store%20arbitrary%20pattern%20sequences%2C%20it%20is%20necessary%0Afor%20the%20network%20to%20include%20hidden%20neurons%20even%20though%20their%20role%20in%20displaying%0Asequence%20memories%20is%20indirect.%20We%20develop%20a%20local%20learning%20algorithm%20to%20learn%0Asequence%20attractors%20in%20the%20networks%20with%20hidden%20neurons.%20The%20algorithm%20is%0Aproven%20to%20converge%20and%20lead%20to%20sequence%20attractors.%20We%20demonstrate%20that%20the%0Anetwork%20model%20can%20store%20and%20retrieve%20sequences%20robustly%20on%20synthetic%20and%0Areal-world%20datasets.%20We%20hope%20that%20this%20study%20provides%20new%20insights%20in%0Aunderstanding%20sequence%20memory%20and%20temporal%20information%20processing%20in%20the%20brain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02729v1&entry.124074799=Read"},
{"title": "Cross-Attention Makes Inference Cumbersome in Text-to-Image Diffusion\n  Models", "author": "Wentian Zhang and Haozhe Liu and Jinheng Xie and Francesco Faccio and Mike Zheng Shou and J\u00fcrgen Schmidhuber", "abstract": "  This study explores the role of cross-attention during inference in\ntext-conditional diffusion models. We find that cross-attention outputs\nconverge to a fixed point after few inference steps. Accordingly, the time\npoint of convergence naturally divides the entire inference process into two\nstages: an initial semantics-planning stage, during which, the model relies on\ncross-attention to plan text-oriented visual semantics, and a subsequent\nfidelity-improving stage, during which the model tries to generate images from\npreviously planned semantics. Surprisingly, ignoring text conditions in the\nfidelity-improving stage not only reduces computation complexity, but also\nmaintains model performance. This yields a simple and training-free method\ncalled TGATE for efficient generation, which caches the cross-attention output\nonce it converges and keeps it fixed during the remaining inference steps. Our\nempirical study on the MS-COCO validation set confirms its effectiveness. The\nsource code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE.\n", "link": "http://arxiv.org/abs/2404.02747v1", "date": "2024-04-03", "relevancy": 1.8323, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6261}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6096}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5736}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Cross-Attention%20Makes%20Inference%20Cumbersome%20in%20Text-to-Image%20Diffusion%0A%20%20Models&body=Title%3A%20Cross-Attention%20Makes%20Inference%20Cumbersome%20in%20Text-to-Image%20Diffusion%0A%20%20Models%0AAuthor%3A%20Wentian%20Zhang%20and%20Haozhe%20Liu%20and%20Jinheng%20Xie%20and%20Francesco%20Faccio%20and%20Mike%20Zheng%20Shou%20and%20J%C3%BCrgen%20Schmidhuber%0AAbstract%3A%20%20%20This%20study%20explores%20the%20role%20of%20cross-attention%20during%20inference%20in%0Atext-conditional%20diffusion%20models.%20We%20find%20that%20cross-attention%20outputs%0Aconverge%20to%20a%20fixed%20point%20after%20few%20inference%20steps.%20Accordingly%2C%20the%20time%0Apoint%20of%20convergence%20naturally%20divides%20the%20entire%20inference%20process%20into%20two%0Astages%3A%20an%20initial%20semantics-planning%20stage%2C%20during%20which%2C%20the%20model%20relies%20on%0Across-attention%20to%20plan%20text-oriented%20visual%20semantics%2C%20and%20a%20subsequent%0Afidelity-improving%20stage%2C%20during%20which%20the%20model%20tries%20to%20generate%20images%20from%0Apreviously%20planned%20semantics.%20Surprisingly%2C%20ignoring%20text%20conditions%20in%20the%0Afidelity-improving%20stage%20not%20only%20reduces%20computation%20complexity%2C%20but%20also%0Amaintains%20model%20performance.%20This%20yields%20a%20simple%20and%20training-free%20method%0Acalled%20TGATE%20for%20efficient%20generation%2C%20which%20caches%20the%20cross-attention%20output%0Aonce%20it%20converges%20and%20keeps%20it%20fixed%20during%20the%20remaining%20inference%20steps.%20Our%0Aempirical%20study%20on%20the%20MS-COCO%20validation%20set%20confirms%20its%20effectiveness.%20The%0Asource%20code%20of%20TGATE%20is%20available%20at%20https%3A//github.com/HaozheLiu-ST/T-GATE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02747v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Attention%20Makes%20Inference%20Cumbersome%20in%20Text-to-Image%20Diffusion%0A%20%20Models&entry.906535625=Wentian%20Zhang%20and%20Haozhe%20Liu%20and%20Jinheng%20Xie%20and%20Francesco%20Faccio%20and%20Mike%20Zheng%20Shou%20and%20J%C3%BCrgen%20Schmidhuber&entry.1292438233=%20%20This%20study%20explores%20the%20role%20of%20cross-attention%20during%20inference%20in%0Atext-conditional%20diffusion%20models.%20We%20find%20that%20cross-attention%20outputs%0Aconverge%20to%20a%20fixed%20point%20after%20few%20inference%20steps.%20Accordingly%2C%20the%20time%0Apoint%20of%20convergence%20naturally%20divides%20the%20entire%20inference%20process%20into%20two%0Astages%3A%20an%20initial%20semantics-planning%20stage%2C%20during%20which%2C%20the%20model%20relies%20on%0Across-attention%20to%20plan%20text-oriented%20visual%20semantics%2C%20and%20a%20subsequent%0Afidelity-improving%20stage%2C%20during%20which%20the%20model%20tries%20to%20generate%20images%20from%0Apreviously%20planned%20semantics.%20Surprisingly%2C%20ignoring%20text%20conditions%20in%20the%0Afidelity-improving%20stage%20not%20only%20reduces%20computation%20complexity%2C%20but%20also%0Amaintains%20model%20performance.%20This%20yields%20a%20simple%20and%20training-free%20method%0Acalled%20TGATE%20for%20efficient%20generation%2C%20which%20caches%20the%20cross-attention%20output%0Aonce%20it%20converges%20and%20keeps%20it%20fixed%20during%20the%20remaining%20inference%20steps.%20Our%0Aempirical%20study%20on%20the%20MS-COCO%20validation%20set%20confirms%20its%20effectiveness.%20The%0Asource%20code%20of%20TGATE%20is%20available%20at%20https%3A//github.com/HaozheLiu-ST/T-GATE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02747v1&entry.124074799=Read"},
{"title": "A Universal Deep Neural Network for Signal Detection in Wireless\n  Communication Systems", "author": "Khalid Albagami and Nguyen Van Huynh and Geoffrey Ye Li", "abstract": "  Recently, deep learning (DL) has been emerging as a promising approach for\nchannel estimation and signal detection in wireless communications. The\nmajority of the existing studies investigating the use of DL techniques in this\ndomain focus on analysing channel impulse responses that are generated from\nonly one channel distribution such as additive white Gaussian channel noise and\nRayleigh channels. In practice, to cope with the dynamic nature of the wireless\nchannel, DL methods must be re-trained on newly non-aged collected data which\nis costly, inefficient, and impractical. To tackle this challenge, this paper\nproposes a novel universal deep neural network (Uni-DNN) that can achieve high\ndetection performance in various wireless environments without retraining the\nmodel. In particular, our proposed Uni-DNN model consists of a wireless channel\nclassifier and a signal detector which are constructed by using DNNs. The\nwireless channel classifier enables the signal detector to generalise and\nperform optimally for multiple wireless channel distributions. In addition, to\nfurther improve the signal detection performance of the proposed model,\nconvolutional neural network is employed. Extensive simulations using the\northogonal frequency division multiplexing scheme demonstrate that the bit\nerror rate performance of our proposed solution can outperform conventional\nDL-based approaches as well as least square and minimum mean square error\nchannel estimators in practical low pilot density scenarios.\n", "link": "http://arxiv.org/abs/2404.02648v1", "date": "2024-04-03", "relevancy": 1.8266, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4624}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4612}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4498}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Universal%20Deep%20Neural%20Network%20for%20Signal%20Detection%20in%20Wireless%0A%20%20Communication%20Systems&body=Title%3A%20A%20Universal%20Deep%20Neural%20Network%20for%20Signal%20Detection%20in%20Wireless%0A%20%20Communication%20Systems%0AAuthor%3A%20Khalid%20Albagami%20and%20Nguyen%20Van%20Huynh%20and%20Geoffrey%20Ye%20Li%0AAbstract%3A%20%20%20Recently%2C%20deep%20learning%20%28DL%29%20has%20been%20emerging%20as%20a%20promising%20approach%20for%0Achannel%20estimation%20and%20signal%20detection%20in%20wireless%20communications.%20The%0Amajority%20of%20the%20existing%20studies%20investigating%20the%20use%20of%20DL%20techniques%20in%20this%0Adomain%20focus%20on%20analysing%20channel%20impulse%20responses%20that%20are%20generated%20from%0Aonly%20one%20channel%20distribution%20such%20as%20additive%20white%20Gaussian%20channel%20noise%20and%0ARayleigh%20channels.%20In%20practice%2C%20to%20cope%20with%20the%20dynamic%20nature%20of%20the%20wireless%0Achannel%2C%20DL%20methods%20must%20be%20re-trained%20on%20newly%20non-aged%20collected%20data%20which%0Ais%20costly%2C%20inefficient%2C%20and%20impractical.%20To%20tackle%20this%20challenge%2C%20this%20paper%0Aproposes%20a%20novel%20universal%20deep%20neural%20network%20%28Uni-DNN%29%20that%20can%20achieve%20high%0Adetection%20performance%20in%20various%20wireless%20environments%20without%20retraining%20the%0Amodel.%20In%20particular%2C%20our%20proposed%20Uni-DNN%20model%20consists%20of%20a%20wireless%20channel%0Aclassifier%20and%20a%20signal%20detector%20which%20are%20constructed%20by%20using%20DNNs.%20The%0Awireless%20channel%20classifier%20enables%20the%20signal%20detector%20to%20generalise%20and%0Aperform%20optimally%20for%20multiple%20wireless%20channel%20distributions.%20In%20addition%2C%20to%0Afurther%20improve%20the%20signal%20detection%20performance%20of%20the%20proposed%20model%2C%0Aconvolutional%20neural%20network%20is%20employed.%20Extensive%20simulations%20using%20the%0Aorthogonal%20frequency%20division%20multiplexing%20scheme%20demonstrate%20that%20the%20bit%0Aerror%20rate%20performance%20of%20our%20proposed%20solution%20can%20outperform%20conventional%0ADL-based%20approaches%20as%20well%20as%20least%20square%20and%20minimum%20mean%20square%20error%0Achannel%20estimators%20in%20practical%20low%20pilot%20density%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02648v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Universal%20Deep%20Neural%20Network%20for%20Signal%20Detection%20in%20Wireless%0A%20%20Communication%20Systems&entry.906535625=Khalid%20Albagami%20and%20Nguyen%20Van%20Huynh%20and%20Geoffrey%20Ye%20Li&entry.1292438233=%20%20Recently%2C%20deep%20learning%20%28DL%29%20has%20been%20emerging%20as%20a%20promising%20approach%20for%0Achannel%20estimation%20and%20signal%20detection%20in%20wireless%20communications.%20The%0Amajority%20of%20the%20existing%20studies%20investigating%20the%20use%20of%20DL%20techniques%20in%20this%0Adomain%20focus%20on%20analysing%20channel%20impulse%20responses%20that%20are%20generated%20from%0Aonly%20one%20channel%20distribution%20such%20as%20additive%20white%20Gaussian%20channel%20noise%20and%0ARayleigh%20channels.%20In%20practice%2C%20to%20cope%20with%20the%20dynamic%20nature%20of%20the%20wireless%0Achannel%2C%20DL%20methods%20must%20be%20re-trained%20on%20newly%20non-aged%20collected%20data%20which%0Ais%20costly%2C%20inefficient%2C%20and%20impractical.%20To%20tackle%20this%20challenge%2C%20this%20paper%0Aproposes%20a%20novel%20universal%20deep%20neural%20network%20%28Uni-DNN%29%20that%20can%20achieve%20high%0Adetection%20performance%20in%20various%20wireless%20environments%20without%20retraining%20the%0Amodel.%20In%20particular%2C%20our%20proposed%20Uni-DNN%20model%20consists%20of%20a%20wireless%20channel%0Aclassifier%20and%20a%20signal%20detector%20which%20are%20constructed%20by%20using%20DNNs.%20The%0Awireless%20channel%20classifier%20enables%20the%20signal%20detector%20to%20generalise%20and%0Aperform%20optimally%20for%20multiple%20wireless%20channel%20distributions.%20In%20addition%2C%20to%0Afurther%20improve%20the%20signal%20detection%20performance%20of%20the%20proposed%20model%2C%0Aconvolutional%20neural%20network%20is%20employed.%20Extensive%20simulations%20using%20the%0Aorthogonal%20frequency%20division%20multiplexing%20scheme%20demonstrate%20that%20the%20bit%0Aerror%20rate%20performance%20of%20our%20proposed%20solution%20can%20outperform%20conventional%0ADL-based%20approaches%20as%20well%20as%20least%20square%20and%20minimum%20mean%20square%20error%0Achannel%20estimators%20in%20practical%20low%20pilot%20density%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02648v1&entry.124074799=Read"},
{"title": "Quaternion recurrent neural network with real-time recurrent learning\n  and maximum correntropy criterion", "author": "Pauline Bourigault and Dongpo Xu and Danilo P. Mandic", "abstract": "  We develop a robust quaternion recurrent neural network (QRNN) for real-time\nprocessing of 3D and 4D data with outliers. This is achieved by combining the\nreal-time recurrent learning (RTRL) algorithm and the maximum correntropy\ncriterion (MCC) as a loss function. While both the mean square error and\nmaximum correntropy criterion are viable cost functions, it is shown that the\nnon-quadratic maximum correntropy loss function is less sensitive to outliers,\nmaking it suitable for applications with multidimensional noisy or uncertain\ndata. Both algorithms are derived based on the novel generalised HR (GHR)\ncalculus, which allows for the differentiation of real functions of quaternion\nvariables and offers the product and chain rules, thus enabling elegant and\ncompact derivations. Simulation results in the context of motion prediction of\nchest internal markers for lung cancer radiotherapy, which includes regular and\nirregular breathing sequences, support the analysis.\n", "link": "http://arxiv.org/abs/2402.14227v2", "date": "2024-04-03", "relevancy": 1.8195, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4936}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4521}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4422}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Quaternion%20recurrent%20neural%20network%20with%20real-time%20recurrent%20learning%0A%20%20and%20maximum%20correntropy%20criterion&body=Title%3A%20Quaternion%20recurrent%20neural%20network%20with%20real-time%20recurrent%20learning%0A%20%20and%20maximum%20correntropy%20criterion%0AAuthor%3A%20Pauline%20Bourigault%20and%20Dongpo%20Xu%20and%20Danilo%20P.%20Mandic%0AAbstract%3A%20%20%20We%20develop%20a%20robust%20quaternion%20recurrent%20neural%20network%20%28QRNN%29%20for%20real-time%0Aprocessing%20of%203D%20and%204D%20data%20with%20outliers.%20This%20is%20achieved%20by%20combining%20the%0Areal-time%20recurrent%20learning%20%28RTRL%29%20algorithm%20and%20the%20maximum%20correntropy%0Acriterion%20%28MCC%29%20as%20a%20loss%20function.%20While%20both%20the%20mean%20square%20error%20and%0Amaximum%20correntropy%20criterion%20are%20viable%20cost%20functions%2C%20it%20is%20shown%20that%20the%0Anon-quadratic%20maximum%20correntropy%20loss%20function%20is%20less%20sensitive%20to%20outliers%2C%0Amaking%20it%20suitable%20for%20applications%20with%20multidimensional%20noisy%20or%20uncertain%0Adata.%20Both%20algorithms%20are%20derived%20based%20on%20the%20novel%20generalised%20HR%20%28GHR%29%0Acalculus%2C%20which%20allows%20for%20the%20differentiation%20of%20real%20functions%20of%20quaternion%0Avariables%20and%20offers%20the%20product%20and%20chain%20rules%2C%20thus%20enabling%20elegant%20and%0Acompact%20derivations.%20Simulation%20results%20in%20the%20context%20of%20motion%20prediction%20of%0Achest%20internal%20markers%20for%20lung%20cancer%20radiotherapy%2C%20which%20includes%20regular%20and%0Airregular%20breathing%20sequences%2C%20support%20the%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14227v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quaternion%20recurrent%20neural%20network%20with%20real-time%20recurrent%20learning%0A%20%20and%20maximum%20correntropy%20criterion&entry.906535625=Pauline%20Bourigault%20and%20Dongpo%20Xu%20and%20Danilo%20P.%20Mandic&entry.1292438233=%20%20We%20develop%20a%20robust%20quaternion%20recurrent%20neural%20network%20%28QRNN%29%20for%20real-time%0Aprocessing%20of%203D%20and%204D%20data%20with%20outliers.%20This%20is%20achieved%20by%20combining%20the%0Areal-time%20recurrent%20learning%20%28RTRL%29%20algorithm%20and%20the%20maximum%20correntropy%0Acriterion%20%28MCC%29%20as%20a%20loss%20function.%20While%20both%20the%20mean%20square%20error%20and%0Amaximum%20correntropy%20criterion%20are%20viable%20cost%20functions%2C%20it%20is%20shown%20that%20the%0Anon-quadratic%20maximum%20correntropy%20loss%20function%20is%20less%20sensitive%20to%20outliers%2C%0Amaking%20it%20suitable%20for%20applications%20with%20multidimensional%20noisy%20or%20uncertain%0Adata.%20Both%20algorithms%20are%20derived%20based%20on%20the%20novel%20generalised%20HR%20%28GHR%29%0Acalculus%2C%20which%20allows%20for%20the%20differentiation%20of%20real%20functions%20of%20quaternion%0Avariables%20and%20offers%20the%20product%20and%20chain%20rules%2C%20thus%20enabling%20elegant%20and%0Acompact%20derivations.%20Simulation%20results%20in%20the%20context%20of%20motion%20prediction%20of%0Achest%20internal%20markers%20for%20lung%20cancer%20radiotherapy%2C%20which%20includes%20regular%20and%0Airregular%20breathing%20sequences%2C%20support%20the%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14227v2&entry.124074799=Read"},
{"title": "On-line conformalized neural networks ensembles for probabilistic\n  forecasting of day-ahead electricity prices", "author": "Alessandro Brusaferri and Andrea Ballarino and Luigi Grossi and Fabrizio Laurini", "abstract": "  Probabilistic electricity price forecasting (PEPF) is subject of increasing\ninterest, following the demand for proper quantification of prediction\nuncertainty, to support the operation in complex power markets with increasing\nshare of renewable generation. Distributional neural networks ensembles have\nbeen recently shown to outperform state of the art PEPF benchmarks. Still, they\nrequire critical reliability enhancements, as fail to pass the coverage tests\nat various steps on the prediction horizon. In this work, we propose a novel\napproach to PEPF, extending the state of the art neural networks ensembles\nbased methods through conformal inference based techniques, deployed within an\non-line recalibration procedure. Experiments have been conducted on multiple\nmarket regions, achieving day-ahead forecasts with improved hourly coverage and\nstable probabilistic scores.\n", "link": "http://arxiv.org/abs/2404.02722v1", "date": "2024-04-03", "relevancy": 1.8147, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.463}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4475}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4468}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20On-line%20conformalized%20neural%20networks%20ensembles%20for%20probabilistic%0A%20%20forecasting%20of%20day-ahead%20electricity%20prices&body=Title%3A%20On-line%20conformalized%20neural%20networks%20ensembles%20for%20probabilistic%0A%20%20forecasting%20of%20day-ahead%20electricity%20prices%0AAuthor%3A%20Alessandro%20Brusaferri%20and%20Andrea%20Ballarino%20and%20Luigi%20Grossi%20and%20Fabrizio%20Laurini%0AAbstract%3A%20%20%20Probabilistic%20electricity%20price%20forecasting%20%28PEPF%29%20is%20subject%20of%20increasing%0Ainterest%2C%20following%20the%20demand%20for%20proper%20quantification%20of%20prediction%0Auncertainty%2C%20to%20support%20the%20operation%20in%20complex%20power%20markets%20with%20increasing%0Ashare%20of%20renewable%20generation.%20Distributional%20neural%20networks%20ensembles%20have%0Abeen%20recently%20shown%20to%20outperform%20state%20of%20the%20art%20PEPF%20benchmarks.%20Still%2C%20they%0Arequire%20critical%20reliability%20enhancements%2C%20as%20fail%20to%20pass%20the%20coverage%20tests%0Aat%20various%20steps%20on%20the%20prediction%20horizon.%20In%20this%20work%2C%20we%20propose%20a%20novel%0Aapproach%20to%20PEPF%2C%20extending%20the%20state%20of%20the%20art%20neural%20networks%20ensembles%0Abased%20methods%20through%20conformal%20inference%20based%20techniques%2C%20deployed%20within%20an%0Aon-line%20recalibration%20procedure.%20Experiments%20have%20been%20conducted%20on%20multiple%0Amarket%20regions%2C%20achieving%20day-ahead%20forecasts%20with%20improved%20hourly%20coverage%20and%0Astable%20probabilistic%20scores.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02722v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On-line%20conformalized%20neural%20networks%20ensembles%20for%20probabilistic%0A%20%20forecasting%20of%20day-ahead%20electricity%20prices&entry.906535625=Alessandro%20Brusaferri%20and%20Andrea%20Ballarino%20and%20Luigi%20Grossi%20and%20Fabrizio%20Laurini&entry.1292438233=%20%20Probabilistic%20electricity%20price%20forecasting%20%28PEPF%29%20is%20subject%20of%20increasing%0Ainterest%2C%20following%20the%20demand%20for%20proper%20quantification%20of%20prediction%0Auncertainty%2C%20to%20support%20the%20operation%20in%20complex%20power%20markets%20with%20increasing%0Ashare%20of%20renewable%20generation.%20Distributional%20neural%20networks%20ensembles%20have%0Abeen%20recently%20shown%20to%20outperform%20state%20of%20the%20art%20PEPF%20benchmarks.%20Still%2C%20they%0Arequire%20critical%20reliability%20enhancements%2C%20as%20fail%20to%20pass%20the%20coverage%20tests%0Aat%20various%20steps%20on%20the%20prediction%20horizon.%20In%20this%20work%2C%20we%20propose%20a%20novel%0Aapproach%20to%20PEPF%2C%20extending%20the%20state%20of%20the%20art%20neural%20networks%20ensembles%0Abased%20methods%20through%20conformal%20inference%20based%20techniques%2C%20deployed%20within%20an%0Aon-line%20recalibration%20procedure.%20Experiments%20have%20been%20conducted%20on%20multiple%0Amarket%20regions%2C%20achieving%20day-ahead%20forecasts%20with%20improved%20hourly%20coverage%20and%0Astable%20probabilistic%20scores.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02722v1&entry.124074799=Read"},
{"title": "SHIELD: A regularization technique for eXplainable Artificial\n  Intelligence", "author": "Iv\u00e1n Sevillano-Garc\u00eda and Juli\u00e1n Luengo and Francisco Herrera", "abstract": "  As Artificial Intelligence systems become integral across domains, the demand\nfor explainability grows. While the effort by the scientific community is\nfocused on obtaining a better explanation for the model, it is important not to\nignore the potential of this explanation process to improve training as well.\nWhile existing efforts primarily focus on generating and evaluating\nexplanations for black-box models, there remains a critical gap in directly\nenhancing models through these evaluations. This paper introduces SHIELD\n(Selective Hidden Input Evaluation for Learning Dynamics), a regularization\ntechnique for explainable artificial intelligence designed to improve model\nquality by concealing portions of input data and assessing the resulting\ndiscrepancy in predictions. In contrast to conventional approaches, SHIELD\nregularization seamlessly integrates into the objective function, enhancing\nmodel explainability while also improving performance. Experimental validation\non benchmark datasets underscores SHIELD's effectiveness in improving\nArtificial Intelligence model explainability and overall performance. This\nestablishes SHIELD regularization as a promising pathway for developing\ntransparent and reliable Artificial Intelligence regularization techniques.\n", "link": "http://arxiv.org/abs/2404.02611v1", "date": "2024-04-03", "relevancy": 1.8132, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4787}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4631}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4333}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SHIELD%3A%20A%20regularization%20technique%20for%20eXplainable%20Artificial%0A%20%20Intelligence&body=Title%3A%20SHIELD%3A%20A%20regularization%20technique%20for%20eXplainable%20Artificial%0A%20%20Intelligence%0AAuthor%3A%20Iv%C3%A1n%20Sevillano-Garc%C3%ADa%20and%20Juli%C3%A1n%20Luengo%20and%20Francisco%20Herrera%0AAbstract%3A%20%20%20As%20Artificial%20Intelligence%20systems%20become%20integral%20across%20domains%2C%20the%20demand%0Afor%20explainability%20grows.%20While%20the%20effort%20by%20the%20scientific%20community%20is%0Afocused%20on%20obtaining%20a%20better%20explanation%20for%20the%20model%2C%20it%20is%20important%20not%20to%0Aignore%20the%20potential%20of%20this%20explanation%20process%20to%20improve%20training%20as%20well.%0AWhile%20existing%20efforts%20primarily%20focus%20on%20generating%20and%20evaluating%0Aexplanations%20for%20black-box%20models%2C%20there%20remains%20a%20critical%20gap%20in%20directly%0Aenhancing%20models%20through%20these%20evaluations.%20This%20paper%20introduces%20SHIELD%0A%28Selective%20Hidden%20Input%20Evaluation%20for%20Learning%20Dynamics%29%2C%20a%20regularization%0Atechnique%20for%20explainable%20artificial%20intelligence%20designed%20to%20improve%20model%0Aquality%20by%20concealing%20portions%20of%20input%20data%20and%20assessing%20the%20resulting%0Adiscrepancy%20in%20predictions.%20In%20contrast%20to%20conventional%20approaches%2C%20SHIELD%0Aregularization%20seamlessly%20integrates%20into%20the%20objective%20function%2C%20enhancing%0Amodel%20explainability%20while%20also%20improving%20performance.%20Experimental%20validation%0Aon%20benchmark%20datasets%20underscores%20SHIELD%27s%20effectiveness%20in%20improving%0AArtificial%20Intelligence%20model%20explainability%20and%20overall%20performance.%20This%0Aestablishes%20SHIELD%20regularization%20as%20a%20promising%20pathway%20for%20developing%0Atransparent%20and%20reliable%20Artificial%20Intelligence%20regularization%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02611v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SHIELD%3A%20A%20regularization%20technique%20for%20eXplainable%20Artificial%0A%20%20Intelligence&entry.906535625=Iv%C3%A1n%20Sevillano-Garc%C3%ADa%20and%20Juli%C3%A1n%20Luengo%20and%20Francisco%20Herrera&entry.1292438233=%20%20As%20Artificial%20Intelligence%20systems%20become%20integral%20across%20domains%2C%20the%20demand%0Afor%20explainability%20grows.%20While%20the%20effort%20by%20the%20scientific%20community%20is%0Afocused%20on%20obtaining%20a%20better%20explanation%20for%20the%20model%2C%20it%20is%20important%20not%20to%0Aignore%20the%20potential%20of%20this%20explanation%20process%20to%20improve%20training%20as%20well.%0AWhile%20existing%20efforts%20primarily%20focus%20on%20generating%20and%20evaluating%0Aexplanations%20for%20black-box%20models%2C%20there%20remains%20a%20critical%20gap%20in%20directly%0Aenhancing%20models%20through%20these%20evaluations.%20This%20paper%20introduces%20SHIELD%0A%28Selective%20Hidden%20Input%20Evaluation%20for%20Learning%20Dynamics%29%2C%20a%20regularization%0Atechnique%20for%20explainable%20artificial%20intelligence%20designed%20to%20improve%20model%0Aquality%20by%20concealing%20portions%20of%20input%20data%20and%20assessing%20the%20resulting%0Adiscrepancy%20in%20predictions.%20In%20contrast%20to%20conventional%20approaches%2C%20SHIELD%0Aregularization%20seamlessly%20integrates%20into%20the%20objective%20function%2C%20enhancing%0Amodel%20explainability%20while%20also%20improving%20performance.%20Experimental%20validation%0Aon%20benchmark%20datasets%20underscores%20SHIELD%27s%20effectiveness%20in%20improving%0AArtificial%20Intelligence%20model%20explainability%20and%20overall%20performance.%20This%0Aestablishes%20SHIELD%20regularization%20as%20a%20promising%20pathway%20for%20developing%0Atransparent%20and%20reliable%20Artificial%20Intelligence%20regularization%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02611v1&entry.124074799=Read"},
{"title": "Linear Attention Sequence Parallelism", "author": "Weigao Sun and Zhen Qin and Dong Li and Xuyang Shen and Yu Qiao and Yiran Zhong", "abstract": "  Sequence Parallel (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single GPU. However, existing SP\nmethods do not take advantage of linear attention features, resulting in\nsub-optimal parallelism efficiency and usability for linear attention-based\nlanguage models. In this paper, we introduce Linear Attention Sequence Parallel\n(LASP), an efficient SP method tailored to linear attention-based language\nmodels. Specifically, we design an efficient point-to-point communication\nmechanism to leverage the right-product kernel trick of linear attention, which\nsharply decreases the communication overhead of SP. We also enhance the\npractical efficiency of LASP by performing kernel fusion and intermediate state\ncaching, making the implementation of LASP hardware-friendly on GPU clusters.\nFurthermore, we meticulously ensure the compatibility of sequence-level LASP\nwith all types of batch-level data parallel methods, which is vital for\ndistributed training on large clusters with long sequences and large batches.\nWe conduct extensive experiments on two linear attention-based models with\nvarying sequence lengths and GPU cluster sizes. LASP scales sequence length up\nto 4096K using 128 A100 80G GPUs on 1B models, which is 8 times longer than\nexisting SP methods while being significantly faster. The code is available at\nhttps://github.com/OpenNLPLab/LASP.\n", "link": "http://arxiv.org/abs/2404.02882v1", "date": "2024-04-03", "relevancy": 1.8015, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4603}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4449}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4427}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Linear%20Attention%20Sequence%20Parallelism&body=Title%3A%20Linear%20Attention%20Sequence%20Parallelism%0AAuthor%3A%20Weigao%20Sun%20and%20Zhen%20Qin%20and%20Dong%20Li%20and%20Xuyang%20Shen%20and%20Yu%20Qiao%20and%20Yiran%20Zhong%0AAbstract%3A%20%20%20Sequence%20Parallel%20%28SP%29%20serves%20as%20a%20prevalent%20strategy%20to%20handle%20long%0Asequences%20that%20exceed%20the%20memory%20limit%20of%20a%20single%20GPU.%20However%2C%20existing%20SP%0Amethods%20do%20not%20take%20advantage%20of%20linear%20attention%20features%2C%20resulting%20in%0Asub-optimal%20parallelism%20efficiency%20and%20usability%20for%20linear%20attention-based%0Alanguage%20models.%20In%20this%20paper%2C%20we%20introduce%20Linear%20Attention%20Sequence%20Parallel%0A%28LASP%29%2C%20an%20efficient%20SP%20method%20tailored%20to%20linear%20attention-based%20language%0Amodels.%20Specifically%2C%20we%20design%20an%20efficient%20point-to-point%20communication%0Amechanism%20to%20leverage%20the%20right-product%20kernel%20trick%20of%20linear%20attention%2C%20which%0Asharply%20decreases%20the%20communication%20overhead%20of%20SP.%20We%20also%20enhance%20the%0Apractical%20efficiency%20of%20LASP%20by%20performing%20kernel%20fusion%20and%20intermediate%20state%0Acaching%2C%20making%20the%20implementation%20of%20LASP%20hardware-friendly%20on%20GPU%20clusters.%0AFurthermore%2C%20we%20meticulously%20ensure%20the%20compatibility%20of%20sequence-level%20LASP%0Awith%20all%20types%20of%20batch-level%20data%20parallel%20methods%2C%20which%20is%20vital%20for%0Adistributed%20training%20on%20large%20clusters%20with%20long%20sequences%20and%20large%20batches.%0AWe%20conduct%20extensive%20experiments%20on%20two%20linear%20attention-based%20models%20with%0Avarying%20sequence%20lengths%20and%20GPU%20cluster%20sizes.%20LASP%20scales%20sequence%20length%20up%0Ato%204096K%20using%20128%20A100%2080G%20GPUs%20on%201B%20models%2C%20which%20is%208%20times%20longer%20than%0Aexisting%20SP%20methods%20while%20being%20significantly%20faster.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/OpenNLPLab/LASP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02882v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Linear%20Attention%20Sequence%20Parallelism&entry.906535625=Weigao%20Sun%20and%20Zhen%20Qin%20and%20Dong%20Li%20and%20Xuyang%20Shen%20and%20Yu%20Qiao%20and%20Yiran%20Zhong&entry.1292438233=%20%20Sequence%20Parallel%20%28SP%29%20serves%20as%20a%20prevalent%20strategy%20to%20handle%20long%0Asequences%20that%20exceed%20the%20memory%20limit%20of%20a%20single%20GPU.%20However%2C%20existing%20SP%0Amethods%20do%20not%20take%20advantage%20of%20linear%20attention%20features%2C%20resulting%20in%0Asub-optimal%20parallelism%20efficiency%20and%20usability%20for%20linear%20attention-based%0Alanguage%20models.%20In%20this%20paper%2C%20we%20introduce%20Linear%20Attention%20Sequence%20Parallel%0A%28LASP%29%2C%20an%20efficient%20SP%20method%20tailored%20to%20linear%20attention-based%20language%0Amodels.%20Specifically%2C%20we%20design%20an%20efficient%20point-to-point%20communication%0Amechanism%20to%20leverage%20the%20right-product%20kernel%20trick%20of%20linear%20attention%2C%20which%0Asharply%20decreases%20the%20communication%20overhead%20of%20SP.%20We%20also%20enhance%20the%0Apractical%20efficiency%20of%20LASP%20by%20performing%20kernel%20fusion%20and%20intermediate%20state%0Acaching%2C%20making%20the%20implementation%20of%20LASP%20hardware-friendly%20on%20GPU%20clusters.%0AFurthermore%2C%20we%20meticulously%20ensure%20the%20compatibility%20of%20sequence-level%20LASP%0Awith%20all%20types%20of%20batch-level%20data%20parallel%20methods%2C%20which%20is%20vital%20for%0Adistributed%20training%20on%20large%20clusters%20with%20long%20sequences%20and%20large%20batches.%0AWe%20conduct%20extensive%20experiments%20on%20two%20linear%20attention-based%20models%20with%0Avarying%20sequence%20lengths%20and%20GPU%20cluster%20sizes.%20LASP%20scales%20sequence%20length%20up%0Ato%204096K%20using%20128%20A100%2080G%20GPUs%20on%201B%20models%2C%20which%20is%208%20times%20longer%20than%0Aexisting%20SP%20methods%20while%20being%20significantly%20faster.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/OpenNLPLab/LASP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02882v1&entry.124074799=Read"},
{"title": "AQuA -- Combining Experts' and Non-Experts' Views To Assess Deliberation\n  Quality in Online Discussions Using LLMs", "author": "Maike Behrendt and Stefan Sylvius Wagner and Marc Ziegele and Lena Wilms and Anke Stoll and Dominique Heinbach and Stefan Harmeling", "abstract": "  Measuring the quality of contributions in political online discussions is\ncrucial in deliberation research and computer science. Research has identified\nvarious indicators to assess online discussion quality, and with deep learning\nadvancements, automating these measures has become feasible. While some studies\nfocus on analyzing specific quality indicators, a comprehensive quality score\nincorporating various deliberative aspects is often preferred. In this work, we\nintroduce AQuA, an additive score that calculates a unified deliberative\nquality score from multiple indices for each discussion post. Unlike other\nsingular scores, AQuA preserves information on the deliberative aspects present\nin comments, enhancing model transparency. We develop adapter models for 20\ndeliberative indices, and calculate correlation coefficients between experts'\nannotations and the perceived deliberativeness by non-experts to weigh the\nindividual indices into a single deliberative score. We demonstrate that the\nAQuA score can be computed easily from pre-trained adapters and aligns well\nwith annotations on other datasets that have not be seen during training. The\nanalysis of experts' vs. non-experts' annotations confirms theoretical findings\nin the social science literature.\n", "link": "http://arxiv.org/abs/2404.02761v1", "date": "2024-04-03", "relevancy": 1.7998, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.489}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.444}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4402}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AQuA%20--%20Combining%20Experts%27%20and%20Non-Experts%27%20Views%20To%20Assess%20Deliberation%0A%20%20Quality%20in%20Online%20Discussions%20Using%20LLMs&body=Title%3A%20AQuA%20--%20Combining%20Experts%27%20and%20Non-Experts%27%20Views%20To%20Assess%20Deliberation%0A%20%20Quality%20in%20Online%20Discussions%20Using%20LLMs%0AAuthor%3A%20Maike%20Behrendt%20and%20Stefan%20Sylvius%20Wagner%20and%20Marc%20Ziegele%20and%20Lena%20Wilms%20and%20Anke%20Stoll%20and%20Dominique%20Heinbach%20and%20Stefan%20Harmeling%0AAbstract%3A%20%20%20Measuring%20the%20quality%20of%20contributions%20in%20political%20online%20discussions%20is%0Acrucial%20in%20deliberation%20research%20and%20computer%20science.%20Research%20has%20identified%0Avarious%20indicators%20to%20assess%20online%20discussion%20quality%2C%20and%20with%20deep%20learning%0Aadvancements%2C%20automating%20these%20measures%20has%20become%20feasible.%20While%20some%20studies%0Afocus%20on%20analyzing%20specific%20quality%20indicators%2C%20a%20comprehensive%20quality%20score%0Aincorporating%20various%20deliberative%20aspects%20is%20often%20preferred.%20In%20this%20work%2C%20we%0Aintroduce%20AQuA%2C%20an%20additive%20score%20that%20calculates%20a%20unified%20deliberative%0Aquality%20score%20from%20multiple%20indices%20for%20each%20discussion%20post.%20Unlike%20other%0Asingular%20scores%2C%20AQuA%20preserves%20information%20on%20the%20deliberative%20aspects%20present%0Ain%20comments%2C%20enhancing%20model%20transparency.%20We%20develop%20adapter%20models%20for%2020%0Adeliberative%20indices%2C%20and%20calculate%20correlation%20coefficients%20between%20experts%27%0Aannotations%20and%20the%20perceived%20deliberativeness%20by%20non-experts%20to%20weigh%20the%0Aindividual%20indices%20into%20a%20single%20deliberative%20score.%20We%20demonstrate%20that%20the%0AAQuA%20score%20can%20be%20computed%20easily%20from%20pre-trained%20adapters%20and%20aligns%20well%0Awith%20annotations%20on%20other%20datasets%20that%20have%20not%20be%20seen%20during%20training.%20The%0Aanalysis%20of%20experts%27%20vs.%20non-experts%27%20annotations%20confirms%20theoretical%20findings%0Ain%20the%20social%20science%20literature.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02761v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AQuA%20--%20Combining%20Experts%27%20and%20Non-Experts%27%20Views%20To%20Assess%20Deliberation%0A%20%20Quality%20in%20Online%20Discussions%20Using%20LLMs&entry.906535625=Maike%20Behrendt%20and%20Stefan%20Sylvius%20Wagner%20and%20Marc%20Ziegele%20and%20Lena%20Wilms%20and%20Anke%20Stoll%20and%20Dominique%20Heinbach%20and%20Stefan%20Harmeling&entry.1292438233=%20%20Measuring%20the%20quality%20of%20contributions%20in%20political%20online%20discussions%20is%0Acrucial%20in%20deliberation%20research%20and%20computer%20science.%20Research%20has%20identified%0Avarious%20indicators%20to%20assess%20online%20discussion%20quality%2C%20and%20with%20deep%20learning%0Aadvancements%2C%20automating%20these%20measures%20has%20become%20feasible.%20While%20some%20studies%0Afocus%20on%20analyzing%20specific%20quality%20indicators%2C%20a%20comprehensive%20quality%20score%0Aincorporating%20various%20deliberative%20aspects%20is%20often%20preferred.%20In%20this%20work%2C%20we%0Aintroduce%20AQuA%2C%20an%20additive%20score%20that%20calculates%20a%20unified%20deliberative%0Aquality%20score%20from%20multiple%20indices%20for%20each%20discussion%20post.%20Unlike%20other%0Asingular%20scores%2C%20AQuA%20preserves%20information%20on%20the%20deliberative%20aspects%20present%0Ain%20comments%2C%20enhancing%20model%20transparency.%20We%20develop%20adapter%20models%20for%2020%0Adeliberative%20indices%2C%20and%20calculate%20correlation%20coefficients%20between%20experts%27%0Aannotations%20and%20the%20perceived%20deliberativeness%20by%20non-experts%20to%20weigh%20the%0Aindividual%20indices%20into%20a%20single%20deliberative%20score.%20We%20demonstrate%20that%20the%0AAQuA%20score%20can%20be%20computed%20easily%20from%20pre-trained%20adapters%20and%20aligns%20well%0Awith%20annotations%20on%20other%20datasets%20that%20have%20not%20be%20seen%20during%20training.%20The%0Aanalysis%20of%20experts%27%20vs.%20non-experts%27%20annotations%20confirms%20theoretical%20findings%0Ain%20the%20social%20science%20literature.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02761v1&entry.124074799=Read"},
{"title": "Utilizing Maximum Mean Discrepancy Barycenter for Propagating the\n  Uncertainty of Value Functions in Reinforcement Learning", "author": "Srinjoy Roy and Swagatam Das", "abstract": "  Accounting for the uncertainty of value functions boosts exploration in\nReinforcement Learning (RL). Our work introduces Maximum Mean Discrepancy\nQ-Learning (MMD-QL) to improve Wasserstein Q-Learning (WQL) for uncertainty\npropagation during Temporal Difference (TD) updates. MMD-QL uses the MMD\nbarycenter for this purpose, as MMD provides a tighter estimate of closeness\nbetween probability measures than the Wasserstein distance. Firstly, we\nestablish that MMD-QL is Probably Approximately Correct in MDP (PAC-MDP) under\nthe average loss metric. Concerning the accumulated rewards, experiments on\ntabular environments show that MMD-QL outperforms WQL and other algorithms.\nSecondly, we incorporate deep networks into MMD-QL to create MMD Q-Network\n(MMD-QN). Making reasonable assumptions, we analyze the convergence rates of\nMMD-QN using function approximation. Empirical results on challenging Atari\ngames demonstrate that MMD-QN performs well compared to benchmark deep RL\nalgorithms, highlighting its effectiveness in handling large state-action\nspaces.\n", "link": "http://arxiv.org/abs/2404.00686v2", "date": "2024-04-03", "relevancy": 1.5514, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5556}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5139}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.503}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Utilizing%20Maximum%20Mean%20Discrepancy%20Barycenter%20for%20Propagating%20the%0A%20%20Uncertainty%20of%20Value%20Functions%20in%20Reinforcement%20Learning&body=Title%3A%20Utilizing%20Maximum%20Mean%20Discrepancy%20Barycenter%20for%20Propagating%20the%0A%20%20Uncertainty%20of%20Value%20Functions%20in%20Reinforcement%20Learning%0AAuthor%3A%20Srinjoy%20Roy%20and%20Swagatam%20Das%0AAbstract%3A%20%20%20Accounting%20for%20the%20uncertainty%20of%20value%20functions%20boosts%20exploration%20in%0AReinforcement%20Learning%20%28RL%29.%20Our%20work%20introduces%20Maximum%20Mean%20Discrepancy%0AQ-Learning%20%28MMD-QL%29%20to%20improve%20Wasserstein%20Q-Learning%20%28WQL%29%20for%20uncertainty%0Apropagation%20during%20Temporal%20Difference%20%28TD%29%20updates.%20MMD-QL%20uses%20the%20MMD%0Abarycenter%20for%20this%20purpose%2C%20as%20MMD%20provides%20a%20tighter%20estimate%20of%20closeness%0Abetween%20probability%20measures%20than%20the%20Wasserstein%20distance.%20Firstly%2C%20we%0Aestablish%20that%20MMD-QL%20is%20Probably%20Approximately%20Correct%20in%20MDP%20%28PAC-MDP%29%20under%0Athe%20average%20loss%20metric.%20Concerning%20the%20accumulated%20rewards%2C%20experiments%20on%0Atabular%20environments%20show%20that%20MMD-QL%20outperforms%20WQL%20and%20other%20algorithms.%0ASecondly%2C%20we%20incorporate%20deep%20networks%20into%20MMD-QL%20to%20create%20MMD%20Q-Network%0A%28MMD-QN%29.%20Making%20reasonable%20assumptions%2C%20we%20analyze%20the%20convergence%20rates%20of%0AMMD-QN%20using%20function%20approximation.%20Empirical%20results%20on%20challenging%20Atari%0Agames%20demonstrate%20that%20MMD-QN%20performs%20well%20compared%20to%20benchmark%20deep%20RL%0Aalgorithms%2C%20highlighting%20its%20effectiveness%20in%20handling%20large%20state-action%0Aspaces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.00686v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Utilizing%20Maximum%20Mean%20Discrepancy%20Barycenter%20for%20Propagating%20the%0A%20%20Uncertainty%20of%20Value%20Functions%20in%20Reinforcement%20Learning&entry.906535625=Srinjoy%20Roy%20and%20Swagatam%20Das&entry.1292438233=%20%20Accounting%20for%20the%20uncertainty%20of%20value%20functions%20boosts%20exploration%20in%0AReinforcement%20Learning%20%28RL%29.%20Our%20work%20introduces%20Maximum%20Mean%20Discrepancy%0AQ-Learning%20%28MMD-QL%29%20to%20improve%20Wasserstein%20Q-Learning%20%28WQL%29%20for%20uncertainty%0Apropagation%20during%20Temporal%20Difference%20%28TD%29%20updates.%20MMD-QL%20uses%20the%20MMD%0Abarycenter%20for%20this%20purpose%2C%20as%20MMD%20provides%20a%20tighter%20estimate%20of%20closeness%0Abetween%20probability%20measures%20than%20the%20Wasserstein%20distance.%20Firstly%2C%20we%0Aestablish%20that%20MMD-QL%20is%20Probably%20Approximately%20Correct%20in%20MDP%20%28PAC-MDP%29%20under%0Athe%20average%20loss%20metric.%20Concerning%20the%20accumulated%20rewards%2C%20experiments%20on%0Atabular%20environments%20show%20that%20MMD-QL%20outperforms%20WQL%20and%20other%20algorithms.%0ASecondly%2C%20we%20incorporate%20deep%20networks%20into%20MMD-QL%20to%20create%20MMD%20Q-Network%0A%28MMD-QN%29.%20Making%20reasonable%20assumptions%2C%20we%20analyze%20the%20convergence%20rates%20of%0AMMD-QN%20using%20function%20approximation.%20Empirical%20results%20on%20challenging%20Atari%0Agames%20demonstrate%20that%20MMD-QN%20performs%20well%20compared%20to%20benchmark%20deep%20RL%0Aalgorithms%2C%20highlighting%20its%20effectiveness%20in%20handling%20large%20state-action%0Aspaces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.00686v2&entry.124074799=Read"},
{"title": "Psychometric Predictive Power of Large Language Models", "author": "Tatsuki Kuribayashi and Yohei Oseki and Timothy Baldwin", "abstract": "  Instruction tuning aligns the response of large language models (LLMs) with\nhuman preferences. Despite such efforts in human--LLM alignment, we report\nthat, interestingly, instruction tuning does not always make LLMs human-like\nfrom a cognitive modeling perspective. More specifically, next-word\nprobabilities estimated by instruction-tuned LLMs are often worse at simulating\nhuman reading behavior than those estimated by base LLMs. In addition, we\nexplore prompting methodologies in simulating human reading behavior with LLMs.\nOur results show that prompts reflecting a particular linguistic hypothesis\nimprove PPP but are still inferior to PPP from small base models. These\nfindings highlight that recent advancements in LLMs, i.e., instruction tuning\nand prompting, do not offer better estimates than direct probability\nmeasurements from base LLMs in cognitive modeling. In other words, our\nexperiments highlight that pure next-word probability remains a strong\npredictor for human reading behavior, even in the age of LLMs.\n", "link": "http://arxiv.org/abs/2311.07484v2", "date": "2024-04-03", "relevancy": 1.3309, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4626}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.441}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4371}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Psychometric%20Predictive%20Power%20of%20Large%20Language%20Models&body=Title%3A%20Psychometric%20Predictive%20Power%20of%20Large%20Language%20Models%0AAuthor%3A%20Tatsuki%20Kuribayashi%20and%20Yohei%20Oseki%20and%20Timothy%20Baldwin%0AAbstract%3A%20%20%20Instruction%20tuning%20aligns%20the%20response%20of%20large%20language%20models%20%28LLMs%29%20with%0Ahuman%20preferences.%20Despite%20such%20efforts%20in%20human--LLM%20alignment%2C%20we%20report%0Athat%2C%20interestingly%2C%20instruction%20tuning%20does%20not%20always%20make%20LLMs%20human-like%0Afrom%20a%20cognitive%20modeling%20perspective.%20More%20specifically%2C%20next-word%0Aprobabilities%20estimated%20by%20instruction-tuned%20LLMs%20are%20often%20worse%20at%20simulating%0Ahuman%20reading%20behavior%20than%20those%20estimated%20by%20base%20LLMs.%20In%20addition%2C%20we%0Aexplore%20prompting%20methodologies%20in%20simulating%20human%20reading%20behavior%20with%20LLMs.%0AOur%20results%20show%20that%20prompts%20reflecting%20a%20particular%20linguistic%20hypothesis%0Aimprove%20PPP%20but%20are%20still%20inferior%20to%20PPP%20from%20small%20base%20models.%20These%0Afindings%20highlight%20that%20recent%20advancements%20in%20LLMs%2C%20i.e.%2C%20instruction%20tuning%0Aand%20prompting%2C%20do%20not%20offer%20better%20estimates%20than%20direct%20probability%0Ameasurements%20from%20base%20LLMs%20in%20cognitive%20modeling.%20In%20other%20words%2C%20our%0Aexperiments%20highlight%20that%20pure%20next-word%20probability%20remains%20a%20strong%0Apredictor%20for%20human%20reading%20behavior%2C%20even%20in%20the%20age%20of%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.07484v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Psychometric%20Predictive%20Power%20of%20Large%20Language%20Models&entry.906535625=Tatsuki%20Kuribayashi%20and%20Yohei%20Oseki%20and%20Timothy%20Baldwin&entry.1292438233=%20%20Instruction%20tuning%20aligns%20the%20response%20of%20large%20language%20models%20%28LLMs%29%20with%0Ahuman%20preferences.%20Despite%20such%20efforts%20in%20human--LLM%20alignment%2C%20we%20report%0Athat%2C%20interestingly%2C%20instruction%20tuning%20does%20not%20always%20make%20LLMs%20human-like%0Afrom%20a%20cognitive%20modeling%20perspective.%20More%20specifically%2C%20next-word%0Aprobabilities%20estimated%20by%20instruction-tuned%20LLMs%20are%20often%20worse%20at%20simulating%0Ahuman%20reading%20behavior%20than%20those%20estimated%20by%20base%20LLMs.%20In%20addition%2C%20we%0Aexplore%20prompting%20methodologies%20in%20simulating%20human%20reading%20behavior%20with%20LLMs.%0AOur%20results%20show%20that%20prompts%20reflecting%20a%20particular%20linguistic%20hypothesis%0Aimprove%20PPP%20but%20are%20still%20inferior%20to%20PPP%20from%20small%20base%20models.%20These%0Afindings%20highlight%20that%20recent%20advancements%20in%20LLMs%2C%20i.e.%2C%20instruction%20tuning%0Aand%20prompting%2C%20do%20not%20offer%20better%20estimates%20than%20direct%20probability%0Ameasurements%20from%20base%20LLMs%20in%20cognitive%20modeling.%20In%20other%20words%2C%20our%0Aexperiments%20highlight%20that%20pure%20next-word%20probability%20remains%20a%20strong%0Apredictor%20for%20human%20reading%20behavior%2C%20even%20in%20the%20age%20of%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.07484v2&entry.124074799=Read"},
{"title": "Planning for Robust Open-loop Pushing: Exploiting Quasi-static Belief\n  Dynamics and Contact-informed Optimization", "author": "Julius Jankowski and Lara Bruderm\u00fcller and Nick Hawes and Sylvain Calinon", "abstract": "  Non-prehensile manipulation such as pushing is typically subject to\nuncertain, non-smooth dynamics. However, modeling the uncertainty of the\ndynamics typically results in intractable belief dynamics, making\ndata-efficient planning under uncertainty difficult. This article focuses on\nthe problem of efficiently generating robust open-loop pushing plans. First, we\ninvestigate how the belief over object configurations propagates through\nquasi-static contact dynamics. We exploit the simplified dynamics to predict\nthe variance of the object configuration without sampling from a perturbation\ndistribution. In a sampling-based trajectory optimization algorithm, the gain\nof the variance is constrained in order to enforce robustness of the plan.\nSecond, we propose an informed trajectory sampling mechanism for drawing robot\ntrajectories that are likely to make contact with the object. This sampling\nmechanism is shown to significantly improve chances of finding robust\nsolutions, especially when making-and-breaking contacts is required. We\ndemonstrate that the proposed approach is able to synthesize bi-manual pushing\ntrajectories, resulting in successful long-horizon pushing maneuvers without\nexteroceptive feedback such as vision or tactile feedback.\n", "link": "http://arxiv.org/abs/2404.02795v1", "date": "2024-04-03", "relevancy": 1.6781, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5912}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5655}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5442}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Planning%20for%20Robust%20Open-loop%20Pushing%3A%20Exploiting%20Quasi-static%20Belief%0A%20%20Dynamics%20and%20Contact-informed%20Optimization&body=Title%3A%20Planning%20for%20Robust%20Open-loop%20Pushing%3A%20Exploiting%20Quasi-static%20Belief%0A%20%20Dynamics%20and%20Contact-informed%20Optimization%0AAuthor%3A%20Julius%20Jankowski%20and%20Lara%20Bruderm%C3%BCller%20and%20Nick%20Hawes%20and%20Sylvain%20Calinon%0AAbstract%3A%20%20%20Non-prehensile%20manipulation%20such%20as%20pushing%20is%20typically%20subject%20to%0Auncertain%2C%20non-smooth%20dynamics.%20However%2C%20modeling%20the%20uncertainty%20of%20the%0Adynamics%20typically%20results%20in%20intractable%20belief%20dynamics%2C%20making%0Adata-efficient%20planning%20under%20uncertainty%20difficult.%20This%20article%20focuses%20on%0Athe%20problem%20of%20efficiently%20generating%20robust%20open-loop%20pushing%20plans.%20First%2C%20we%0Ainvestigate%20how%20the%20belief%20over%20object%20configurations%20propagates%20through%0Aquasi-static%20contact%20dynamics.%20We%20exploit%20the%20simplified%20dynamics%20to%20predict%0Athe%20variance%20of%20the%20object%20configuration%20without%20sampling%20from%20a%20perturbation%0Adistribution.%20In%20a%20sampling-based%20trajectory%20optimization%20algorithm%2C%20the%20gain%0Aof%20the%20variance%20is%20constrained%20in%20order%20to%20enforce%20robustness%20of%20the%20plan.%0ASecond%2C%20we%20propose%20an%20informed%20trajectory%20sampling%20mechanism%20for%20drawing%20robot%0Atrajectories%20that%20are%20likely%20to%20make%20contact%20with%20the%20object.%20This%20sampling%0Amechanism%20is%20shown%20to%20significantly%20improve%20chances%20of%20finding%20robust%0Asolutions%2C%20especially%20when%20making-and-breaking%20contacts%20is%20required.%20We%0Ademonstrate%20that%20the%20proposed%20approach%20is%20able%20to%20synthesize%20bi-manual%20pushing%0Atrajectories%2C%20resulting%20in%20successful%20long-horizon%20pushing%20maneuvers%20without%0Aexteroceptive%20feedback%20such%20as%20vision%20or%20tactile%20feedback.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02795v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Planning%20for%20Robust%20Open-loop%20Pushing%3A%20Exploiting%20Quasi-static%20Belief%0A%20%20Dynamics%20and%20Contact-informed%20Optimization&entry.906535625=Julius%20Jankowski%20and%20Lara%20Bruderm%C3%BCller%20and%20Nick%20Hawes%20and%20Sylvain%20Calinon&entry.1292438233=%20%20Non-prehensile%20manipulation%20such%20as%20pushing%20is%20typically%20subject%20to%0Auncertain%2C%20non-smooth%20dynamics.%20However%2C%20modeling%20the%20uncertainty%20of%20the%0Adynamics%20typically%20results%20in%20intractable%20belief%20dynamics%2C%20making%0Adata-efficient%20planning%20under%20uncertainty%20difficult.%20This%20article%20focuses%20on%0Athe%20problem%20of%20efficiently%20generating%20robust%20open-loop%20pushing%20plans.%20First%2C%20we%0Ainvestigate%20how%20the%20belief%20over%20object%20configurations%20propagates%20through%0Aquasi-static%20contact%20dynamics.%20We%20exploit%20the%20simplified%20dynamics%20to%20predict%0Athe%20variance%20of%20the%20object%20configuration%20without%20sampling%20from%20a%20perturbation%0Adistribution.%20In%20a%20sampling-based%20trajectory%20optimization%20algorithm%2C%20the%20gain%0Aof%20the%20variance%20is%20constrained%20in%20order%20to%20enforce%20robustness%20of%20the%20plan.%0ASecond%2C%20we%20propose%20an%20informed%20trajectory%20sampling%20mechanism%20for%20drawing%20robot%0Atrajectories%20that%20are%20likely%20to%20make%20contact%20with%20the%20object.%20This%20sampling%0Amechanism%20is%20shown%20to%20significantly%20improve%20chances%20of%20finding%20robust%0Asolutions%2C%20especially%20when%20making-and-breaking%20contacts%20is%20required.%20We%0Ademonstrate%20that%20the%20proposed%20approach%20is%20able%20to%20synthesize%20bi-manual%20pushing%0Atrajectories%2C%20resulting%20in%20successful%20long-horizon%20pushing%20maneuvers%20without%0Aexteroceptive%20feedback%20such%20as%20vision%20or%20tactile%20feedback.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02795v1&entry.124074799=Read"},
{"title": "3DStyleGLIP: Part-Tailored Text-Guided 3D Neural Stylization", "author": "SeungJeh Chung and JooHyun Park and Hyewon Kan and HyeongYeop Kang", "abstract": "  3D stylization, which entails the application of specific styles to\nthree-dimensional objects, holds significant commercial potential as it enables\nthe creation of diverse 3D objects with distinct moods and styles, tailored to\nspecific demands of different scenes. With recent advancements in text-driven\nmethods and artificial intelligence, the stylization process is increasingly\nintuitive and automated, thereby diminishing the reliance on manual labor and\nexpertise. However, existing methods have predominantly focused on holistic\nstylization, thereby leaving the application of styles to individual components\nof a 3D object unexplored. In response, we introduce 3DStyleGLIP, a novel\nframework specifically designed for text-driven, part-tailored 3D stylization.\nGiven a 3D mesh and a text prompt, 3DStyleGLIP leverages the vision-language\nembedding space of the Grounded Language-Image Pre-training (GLIP) model to\nlocalize the individual parts of the 3D mesh and modify their colors and local\ngeometries to align them with the desired styles specified in the text prompt.\n3DStyleGLIP is effectively trained for 3D stylization tasks through a\npart-level style loss working in GLIP's embedding space, supplemented by two\ncomplementary learning techniques. Extensive experimental validation confirms\nthat our method achieves significant part-wise stylization capabilities,\ndemonstrating promising potential in advancing the field of 3D stylization.\n", "link": "http://arxiv.org/abs/2404.02634v1", "date": "2024-04-03", "relevancy": 1.6688, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5615}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5573}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5422}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%203DStyleGLIP%3A%20Part-Tailored%20Text-Guided%203D%20Neural%20Stylization&body=Title%3A%203DStyleGLIP%3A%20Part-Tailored%20Text-Guided%203D%20Neural%20Stylization%0AAuthor%3A%20SeungJeh%20Chung%20and%20JooHyun%20Park%20and%20Hyewon%20Kan%20and%20HyeongYeop%20Kang%0AAbstract%3A%20%20%203D%20stylization%2C%20which%20entails%20the%20application%20of%20specific%20styles%20to%0Athree-dimensional%20objects%2C%20holds%20significant%20commercial%20potential%20as%20it%20enables%0Athe%20creation%20of%20diverse%203D%20objects%20with%20distinct%20moods%20and%20styles%2C%20tailored%20to%0Aspecific%20demands%20of%20different%20scenes.%20With%20recent%20advancements%20in%20text-driven%0Amethods%20and%20artificial%20intelligence%2C%20the%20stylization%20process%20is%20increasingly%0Aintuitive%20and%20automated%2C%20thereby%20diminishing%20the%20reliance%20on%20manual%20labor%20and%0Aexpertise.%20However%2C%20existing%20methods%20have%20predominantly%20focused%20on%20holistic%0Astylization%2C%20thereby%20leaving%20the%20application%20of%20styles%20to%20individual%20components%0Aof%20a%203D%20object%20unexplored.%20In%20response%2C%20we%20introduce%203DStyleGLIP%2C%20a%20novel%0Aframework%20specifically%20designed%20for%20text-driven%2C%20part-tailored%203D%20stylization.%0AGiven%20a%203D%20mesh%20and%20a%20text%20prompt%2C%203DStyleGLIP%20leverages%20the%20vision-language%0Aembedding%20space%20of%20the%20Grounded%20Language-Image%20Pre-training%20%28GLIP%29%20model%20to%0Alocalize%20the%20individual%20parts%20of%20the%203D%20mesh%20and%20modify%20their%20colors%20and%20local%0Ageometries%20to%20align%20them%20with%20the%20desired%20styles%20specified%20in%20the%20text%20prompt.%0A3DStyleGLIP%20is%20effectively%20trained%20for%203D%20stylization%20tasks%20through%20a%0Apart-level%20style%20loss%20working%20in%20GLIP%27s%20embedding%20space%2C%20supplemented%20by%20two%0Acomplementary%20learning%20techniques.%20Extensive%20experimental%20validation%20confirms%0Athat%20our%20method%20achieves%20significant%20part-wise%20stylization%20capabilities%2C%0Ademonstrating%20promising%20potential%20in%20advancing%20the%20field%20of%203D%20stylization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02634v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DStyleGLIP%3A%20Part-Tailored%20Text-Guided%203D%20Neural%20Stylization&entry.906535625=SeungJeh%20Chung%20and%20JooHyun%20Park%20and%20Hyewon%20Kan%20and%20HyeongYeop%20Kang&entry.1292438233=%20%203D%20stylization%2C%20which%20entails%20the%20application%20of%20specific%20styles%20to%0Athree-dimensional%20objects%2C%20holds%20significant%20commercial%20potential%20as%20it%20enables%0Athe%20creation%20of%20diverse%203D%20objects%20with%20distinct%20moods%20and%20styles%2C%20tailored%20to%0Aspecific%20demands%20of%20different%20scenes.%20With%20recent%20advancements%20in%20text-driven%0Amethods%20and%20artificial%20intelligence%2C%20the%20stylization%20process%20is%20increasingly%0Aintuitive%20and%20automated%2C%20thereby%20diminishing%20the%20reliance%20on%20manual%20labor%20and%0Aexpertise.%20However%2C%20existing%20methods%20have%20predominantly%20focused%20on%20holistic%0Astylization%2C%20thereby%20leaving%20the%20application%20of%20styles%20to%20individual%20components%0Aof%20a%203D%20object%20unexplored.%20In%20response%2C%20we%20introduce%203DStyleGLIP%2C%20a%20novel%0Aframework%20specifically%20designed%20for%20text-driven%2C%20part-tailored%203D%20stylization.%0AGiven%20a%203D%20mesh%20and%20a%20text%20prompt%2C%203DStyleGLIP%20leverages%20the%20vision-language%0Aembedding%20space%20of%20the%20Grounded%20Language-Image%20Pre-training%20%28GLIP%29%20model%20to%0Alocalize%20the%20individual%20parts%20of%20the%203D%20mesh%20and%20modify%20their%20colors%20and%20local%0Ageometries%20to%20align%20them%20with%20the%20desired%20styles%20specified%20in%20the%20text%20prompt.%0A3DStyleGLIP%20is%20effectively%20trained%20for%203D%20stylization%20tasks%20through%20a%0Apart-level%20style%20loss%20working%20in%20GLIP%27s%20embedding%20space%2C%20supplemented%20by%20two%0Acomplementary%20learning%20techniques.%20Extensive%20experimental%20validation%20confirms%0Athat%20our%20method%20achieves%20significant%20part-wise%20stylization%20capabilities%2C%0Ademonstrating%20promising%20potential%20in%20advancing%20the%20field%20of%203D%20stylization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02634v1&entry.124074799=Read"},
{"title": "Identifying Climate Targets in National Laws and Policies using Machine\n  Learning", "author": "Matyas Juhasz and Tina Marchand and Roshan Melwani and Kalyan Dutia and Sarah Goodenough and Harrison Pim and Henry Franks", "abstract": "  Quantified policy targets are a fundamental element of climate policy,\ntypically characterised by domain-specific and technical language. Current\nmethods for curating comprehensive views of global climate policy targets\nentail significant manual effort. At present there are few scalable methods for\nextracting climate targets from national laws or policies, which limits\npolicymakers' and researchers' ability to (1) assess private and public sector\nalignment with global goals and (2) inform policy decisions. In this paper we\npresent an approach for extracting mentions of climate targets from national\nlaws and policies. We create an expert-annotated dataset identifying three\ncategories of target ('Net Zero', 'Reduction' and 'Other' (e.g. renewable\nenergy targets)) and train a classifier to reliably identify them in text. We\ninvestigate bias and equity impacts related to our model and identify specific\nyears and country names as problematic features. Finally, we investigate the\ncharacteristics of the dataset produced by running this classifier on the\nClimate Policy Radar (CPR) dataset of global national climate laws and policies\nand UNFCCC submissions, highlighting the potential of automated and scalable\ndata collection for existing climate policy databases and supporting further\nresearch. Our work represents a significant upgrade in the accessibility of\nthese key climate policy elements for policymakers and researchers. We publish\nour model at\n\\url{https://huggingface.co/ClimatePolicyRadar/national-climate-targets} and\nrelated dataset at\n\\url{https://huggingface.co/datasets/ClimatePolicyRadar/national-climate-targets}.\n", "link": "http://arxiv.org/abs/2404.02822v1", "date": "2024-04-03", "relevancy": 1.3541, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4586}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4527}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.448}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Identifying%20Climate%20Targets%20in%20National%20Laws%20and%20Policies%20using%20Machine%0A%20%20Learning&body=Title%3A%20Identifying%20Climate%20Targets%20in%20National%20Laws%20and%20Policies%20using%20Machine%0A%20%20Learning%0AAuthor%3A%20Matyas%20Juhasz%20and%20Tina%20Marchand%20and%20Roshan%20Melwani%20and%20Kalyan%20Dutia%20and%20Sarah%20Goodenough%20and%20Harrison%20Pim%20and%20Henry%20Franks%0AAbstract%3A%20%20%20Quantified%20policy%20targets%20are%20a%20fundamental%20element%20of%20climate%20policy%2C%0Atypically%20characterised%20by%20domain-specific%20and%20technical%20language.%20Current%0Amethods%20for%20curating%20comprehensive%20views%20of%20global%20climate%20policy%20targets%0Aentail%20significant%20manual%20effort.%20At%20present%20there%20are%20few%20scalable%20methods%20for%0Aextracting%20climate%20targets%20from%20national%20laws%20or%20policies%2C%20which%20limits%0Apolicymakers%27%20and%20researchers%27%20ability%20to%20%281%29%20assess%20private%20and%20public%20sector%0Aalignment%20with%20global%20goals%20and%20%282%29%20inform%20policy%20decisions.%20In%20this%20paper%20we%0Apresent%20an%20approach%20for%20extracting%20mentions%20of%20climate%20targets%20from%20national%0Alaws%20and%20policies.%20We%20create%20an%20expert-annotated%20dataset%20identifying%20three%0Acategories%20of%20target%20%28%27Net%20Zero%27%2C%20%27Reduction%27%20and%20%27Other%27%20%28e.g.%20renewable%0Aenergy%20targets%29%29%20and%20train%20a%20classifier%20to%20reliably%20identify%20them%20in%20text.%20We%0Ainvestigate%20bias%20and%20equity%20impacts%20related%20to%20our%20model%20and%20identify%20specific%0Ayears%20and%20country%20names%20as%20problematic%20features.%20Finally%2C%20we%20investigate%20the%0Acharacteristics%20of%20the%20dataset%20produced%20by%20running%20this%20classifier%20on%20the%0AClimate%20Policy%20Radar%20%28CPR%29%20dataset%20of%20global%20national%20climate%20laws%20and%20policies%0Aand%20UNFCCC%20submissions%2C%20highlighting%20the%20potential%20of%20automated%20and%20scalable%0Adata%20collection%20for%20existing%20climate%20policy%20databases%20and%20supporting%20further%0Aresearch.%20Our%20work%20represents%20a%20significant%20upgrade%20in%20the%20accessibility%20of%0Athese%20key%20climate%20policy%20elements%20for%20policymakers%20and%20researchers.%20We%20publish%0Aour%20model%20at%0A%5Curl%7Bhttps%3A//huggingface.co/ClimatePolicyRadar/national-climate-targets%7D%20and%0Arelated%20dataset%20at%0A%5Curl%7Bhttps%3A//huggingface.co/datasets/ClimatePolicyRadar/national-climate-targets%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02822v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identifying%20Climate%20Targets%20in%20National%20Laws%20and%20Policies%20using%20Machine%0A%20%20Learning&entry.906535625=Matyas%20Juhasz%20and%20Tina%20Marchand%20and%20Roshan%20Melwani%20and%20Kalyan%20Dutia%20and%20Sarah%20Goodenough%20and%20Harrison%20Pim%20and%20Henry%20Franks&entry.1292438233=%20%20Quantified%20policy%20targets%20are%20a%20fundamental%20element%20of%20climate%20policy%2C%0Atypically%20characterised%20by%20domain-specific%20and%20technical%20language.%20Current%0Amethods%20for%20curating%20comprehensive%20views%20of%20global%20climate%20policy%20targets%0Aentail%20significant%20manual%20effort.%20At%20present%20there%20are%20few%20scalable%20methods%20for%0Aextracting%20climate%20targets%20from%20national%20laws%20or%20policies%2C%20which%20limits%0Apolicymakers%27%20and%20researchers%27%20ability%20to%20%281%29%20assess%20private%20and%20public%20sector%0Aalignment%20with%20global%20goals%20and%20%282%29%20inform%20policy%20decisions.%20In%20this%20paper%20we%0Apresent%20an%20approach%20for%20extracting%20mentions%20of%20climate%20targets%20from%20national%0Alaws%20and%20policies.%20We%20create%20an%20expert-annotated%20dataset%20identifying%20three%0Acategories%20of%20target%20%28%27Net%20Zero%27%2C%20%27Reduction%27%20and%20%27Other%27%20%28e.g.%20renewable%0Aenergy%20targets%29%29%20and%20train%20a%20classifier%20to%20reliably%20identify%20them%20in%20text.%20We%0Ainvestigate%20bias%20and%20equity%20impacts%20related%20to%20our%20model%20and%20identify%20specific%0Ayears%20and%20country%20names%20as%20problematic%20features.%20Finally%2C%20we%20investigate%20the%0Acharacteristics%20of%20the%20dataset%20produced%20by%20running%20this%20classifier%20on%20the%0AClimate%20Policy%20Radar%20%28CPR%29%20dataset%20of%20global%20national%20climate%20laws%20and%20policies%0Aand%20UNFCCC%20submissions%2C%20highlighting%20the%20potential%20of%20automated%20and%20scalable%0Adata%20collection%20for%20existing%20climate%20policy%20databases%20and%20supporting%20further%0Aresearch.%20Our%20work%20represents%20a%20significant%20upgrade%20in%20the%20accessibility%20of%0Athese%20key%20climate%20policy%20elements%20for%20policymakers%20and%20researchers.%20We%20publish%0Aour%20model%20at%0A%5Curl%7Bhttps%3A//huggingface.co/ClimatePolicyRadar/national-climate-targets%7D%20and%0Arelated%20dataset%20at%0A%5Curl%7Bhttps%3A//huggingface.co/datasets/ClimatePolicyRadar/national-climate-targets%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02822v1&entry.124074799=Read"},
{"title": "Unsupervised Learning of Effective Actions in Robotics", "author": "Marko Zaric and Jakob Hollenstein and Justus Piater and Erwan Renaudo", "abstract": "  Learning actions that are relevant to decision-making and can be executed\neffectively is a key problem in autonomous robotics. Current state-of-the-art\naction representations in robotics lack proper effect-driven learning of the\nrobot's actions. Although successful in solving manipulation tasks, deep\nlearning methods also lack this ability, in addition to their high cost in\nterms of memory or training data. In this paper, we propose an unsupervised\nalgorithm to discretize a continuous motion space and generate \"action\nprototypes\", each producing different effects in the environment. After an\nexploration phase, the algorithm automatically builds a representation of the\neffects and groups motions into action prototypes, where motions more likely to\nproduce an effect are represented more than those that lead to negligible\nchanges. We evaluate our method on a simulated stair-climbing reinforcement\nlearning task, and the preliminary results show that our effect driven\ndiscretization outperforms uniformly and randomly sampled discretizations in\nconvergence speed and maximum reward.\n", "link": "http://arxiv.org/abs/2404.02728v1", "date": "2024-04-03", "relevancy": 1.6881, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6407}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5661}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5302}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Learning%20of%20Effective%20Actions%20in%20Robotics&body=Title%3A%20Unsupervised%20Learning%20of%20Effective%20Actions%20in%20Robotics%0AAuthor%3A%20Marko%20Zaric%20and%20Jakob%20Hollenstein%20and%20Justus%20Piater%20and%20Erwan%20Renaudo%0AAbstract%3A%20%20%20Learning%20actions%20that%20are%20relevant%20to%20decision-making%20and%20can%20be%20executed%0Aeffectively%20is%20a%20key%20problem%20in%20autonomous%20robotics.%20Current%20state-of-the-art%0Aaction%20representations%20in%20robotics%20lack%20proper%20effect-driven%20learning%20of%20the%0Arobot%27s%20actions.%20Although%20successful%20in%20solving%20manipulation%20tasks%2C%20deep%0Alearning%20methods%20also%20lack%20this%20ability%2C%20in%20addition%20to%20their%20high%20cost%20in%0Aterms%20of%20memory%20or%20training%20data.%20In%20this%20paper%2C%20we%20propose%20an%20unsupervised%0Aalgorithm%20to%20discretize%20a%20continuous%20motion%20space%20and%20generate%20%22action%0Aprototypes%22%2C%20each%20producing%20different%20effects%20in%20the%20environment.%20After%20an%0Aexploration%20phase%2C%20the%20algorithm%20automatically%20builds%20a%20representation%20of%20the%0Aeffects%20and%20groups%20motions%20into%20action%20prototypes%2C%20where%20motions%20more%20likely%20to%0Aproduce%20an%20effect%20are%20represented%20more%20than%20those%20that%20lead%20to%20negligible%0Achanges.%20We%20evaluate%20our%20method%20on%20a%20simulated%20stair-climbing%20reinforcement%0Alearning%20task%2C%20and%20the%20preliminary%20results%20show%20that%20our%20effect%20driven%0Adiscretization%20outperforms%20uniformly%20and%20randomly%20sampled%20discretizations%20in%0Aconvergence%20speed%20and%20maximum%20reward.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02728v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Learning%20of%20Effective%20Actions%20in%20Robotics&entry.906535625=Marko%20Zaric%20and%20Jakob%20Hollenstein%20and%20Justus%20Piater%20and%20Erwan%20Renaudo&entry.1292438233=%20%20Learning%20actions%20that%20are%20relevant%20to%20decision-making%20and%20can%20be%20executed%0Aeffectively%20is%20a%20key%20problem%20in%20autonomous%20robotics.%20Current%20state-of-the-art%0Aaction%20representations%20in%20robotics%20lack%20proper%20effect-driven%20learning%20of%20the%0Arobot%27s%20actions.%20Although%20successful%20in%20solving%20manipulation%20tasks%2C%20deep%0Alearning%20methods%20also%20lack%20this%20ability%2C%20in%20addition%20to%20their%20high%20cost%20in%0Aterms%20of%20memory%20or%20training%20data.%20In%20this%20paper%2C%20we%20propose%20an%20unsupervised%0Aalgorithm%20to%20discretize%20a%20continuous%20motion%20space%20and%20generate%20%22action%0Aprototypes%22%2C%20each%20producing%20different%20effects%20in%20the%20environment.%20After%20an%0Aexploration%20phase%2C%20the%20algorithm%20automatically%20builds%20a%20representation%20of%20the%0Aeffects%20and%20groups%20motions%20into%20action%20prototypes%2C%20where%20motions%20more%20likely%20to%0Aproduce%20an%20effect%20are%20represented%20more%20than%20those%20that%20lead%20to%20negligible%0Achanges.%20We%20evaluate%20our%20method%20on%20a%20simulated%20stair-climbing%20reinforcement%0Alearning%20task%2C%20and%20the%20preliminary%20results%20show%20that%20our%20effect%20driven%0Adiscretization%20outperforms%20uniformly%20and%20randomly%20sampled%20discretizations%20in%0Aconvergence%20speed%20and%20maximum%20reward.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02728v1&entry.124074799=Read"},
{"title": "Reinforcement Learning in Categorical Cybernetics", "author": "Jules Hedges and Riu Rodr\u00edguez Sakamoto", "abstract": "  We show that several major algorithms of reinforcement learning (RL) fit into\nthe framework of categorical cybernetics, that is to say, parametrised\nbidirectional processes. We build on our previous work in which we show that\nvalue iteration can be represented by precomposition with a certain optic. The\noutline of the main construction in this paper is: (1) We extend the Bellman\noperators to parametrised optics that apply to action-value functions and\ndepend on a sample. (2) We apply a representable contravariant functor,\nobtaining a parametrised function that applies the Bellman iteration. (3) This\nparametrised function becomes the backward pass of another parametrised optic\nthat represents the model, which interacts with an environment via an agent.\nThus, parametrised optics appear in two different ways in our construction,\nwith one becoming part of the other. As we show, many of the major classes of\nalgorithms in RL can be seen as different extremal cases of this general setup:\ndynamic programming, Monte Carlo methods, temporal difference learning, and\ndeep RL. We see this as strong evidence that this approach is a natural one and\nbelieve that it will be a fruitful way to think about RL in the future.\n", "link": "http://arxiv.org/abs/2404.02688v1", "date": "2024-04-03", "relevancy": 1.3631, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5121}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4388}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4357}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Reinforcement%20Learning%20in%20Categorical%20Cybernetics&body=Title%3A%20Reinforcement%20Learning%20in%20Categorical%20Cybernetics%0AAuthor%3A%20Jules%20Hedges%20and%20Riu%20Rodr%C3%ADguez%20Sakamoto%0AAbstract%3A%20%20%20We%20show%20that%20several%20major%20algorithms%20of%20reinforcement%20learning%20%28RL%29%20fit%20into%0Athe%20framework%20of%20categorical%20cybernetics%2C%20that%20is%20to%20say%2C%20parametrised%0Abidirectional%20processes.%20We%20build%20on%20our%20previous%20work%20in%20which%20we%20show%20that%0Avalue%20iteration%20can%20be%20represented%20by%20precomposition%20with%20a%20certain%20optic.%20The%0Aoutline%20of%20the%20main%20construction%20in%20this%20paper%20is%3A%20%281%29%20We%20extend%20the%20Bellman%0Aoperators%20to%20parametrised%20optics%20that%20apply%20to%20action-value%20functions%20and%0Adepend%20on%20a%20sample.%20%282%29%20We%20apply%20a%20representable%20contravariant%20functor%2C%0Aobtaining%20a%20parametrised%20function%20that%20applies%20the%20Bellman%20iteration.%20%283%29%20This%0Aparametrised%20function%20becomes%20the%20backward%20pass%20of%20another%20parametrised%20optic%0Athat%20represents%20the%20model%2C%20which%20interacts%20with%20an%20environment%20via%20an%20agent.%0AThus%2C%20parametrised%20optics%20appear%20in%20two%20different%20ways%20in%20our%20construction%2C%0Awith%20one%20becoming%20part%20of%20the%20other.%20As%20we%20show%2C%20many%20of%20the%20major%20classes%20of%0Aalgorithms%20in%20RL%20can%20be%20seen%20as%20different%20extremal%20cases%20of%20this%20general%20setup%3A%0Adynamic%20programming%2C%20Monte%20Carlo%20methods%2C%20temporal%20difference%20learning%2C%20and%0Adeep%20RL.%20We%20see%20this%20as%20strong%20evidence%20that%20this%20approach%20is%20a%20natural%20one%20and%0Abelieve%20that%20it%20will%20be%20a%20fruitful%20way%20to%20think%20about%20RL%20in%20the%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02688v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcement%20Learning%20in%20Categorical%20Cybernetics&entry.906535625=Jules%20Hedges%20and%20Riu%20Rodr%C3%ADguez%20Sakamoto&entry.1292438233=%20%20We%20show%20that%20several%20major%20algorithms%20of%20reinforcement%20learning%20%28RL%29%20fit%20into%0Athe%20framework%20of%20categorical%20cybernetics%2C%20that%20is%20to%20say%2C%20parametrised%0Abidirectional%20processes.%20We%20build%20on%20our%20previous%20work%20in%20which%20we%20show%20that%0Avalue%20iteration%20can%20be%20represented%20by%20precomposition%20with%20a%20certain%20optic.%20The%0Aoutline%20of%20the%20main%20construction%20in%20this%20paper%20is%3A%20%281%29%20We%20extend%20the%20Bellman%0Aoperators%20to%20parametrised%20optics%20that%20apply%20to%20action-value%20functions%20and%0Adepend%20on%20a%20sample.%20%282%29%20We%20apply%20a%20representable%20contravariant%20functor%2C%0Aobtaining%20a%20parametrised%20function%20that%20applies%20the%20Bellman%20iteration.%20%283%29%20This%0Aparametrised%20function%20becomes%20the%20backward%20pass%20of%20another%20parametrised%20optic%0Athat%20represents%20the%20model%2C%20which%20interacts%20with%20an%20environment%20via%20an%20agent.%0AThus%2C%20parametrised%20optics%20appear%20in%20two%20different%20ways%20in%20our%20construction%2C%0Awith%20one%20becoming%20part%20of%20the%20other.%20As%20we%20show%2C%20many%20of%20the%20major%20classes%20of%0Aalgorithms%20in%20RL%20can%20be%20seen%20as%20different%20extremal%20cases%20of%20this%20general%20setup%3A%0Adynamic%20programming%2C%20Monte%20Carlo%20methods%2C%20temporal%20difference%20learning%2C%20and%0Adeep%20RL.%20We%20see%20this%20as%20strong%20evidence%20that%20this%20approach%20is%20a%20natural%20one%20and%0Abelieve%20that%20it%20will%20be%20a%20fruitful%20way%20to%20think%20about%20RL%20in%20the%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02688v1&entry.124074799=Read"},
{"title": "An Optimization Framework to Personalize Passive Cardiac Mechanics", "author": "Lei Shi and Ian Chen and Hiroo Takayama and Vijay Vedula", "abstract": "  Personalized cardiac mechanics modeling is a powerful tool for understanding\nthe biomechanics of cardiac function in health and disease and assisting in\ntreatment planning. However, current models are limited to using medical images\nacquired at a single cardiac phase, often limiting their applicability for\nprocessing dynamic image acquisitions. This study introduces an inverse finite\nelement analysis (iFEA) framework to estimate the passive mechanical properties\nof cardiac tissue using time-dependent medical image data. The iFEA framework\nrelies on a novel nested optimization scheme, in which the outer iterations\nutilize a traditional optimization method to best approximate material\nparameters that fit image data, while the inner iterations employ an augmented\nSellier's algorithm to estimate the stress-free reference configuration. With a\nfocus on characterizing the passive mechanical behavior, the framework employs\nstructurally based anisotropic hyperelastic constitutive models and\nphysiologically relevant boundary conditions to simulate myocardial mechanics.\nWe use a stabilized variational multiscale formulation for solving the\ngoverning nonlinear elastodynamics equations, verified for cardiac mechanics\napplications. The framework is tested in myocardium models of biventricle and\nleft atrium derived from cardiac phase-resolved computed tomographic (CT)\nimages of a healthy subject and three patients with hypertrophic obstructive\ncardiomyopathy (HOCM). The impact of the choice of optimization methods and\nother numerical settings, including fiber direction parameters, mesh size,\ninitial parameters for optimization, and perturbations to optimal material\nparameters, is assessed using a rigorous sensitivity analysis. The performance\nof the current iFEA is compared against an assumed power-law-based\npressure-volume relation, typically used for single-phase image acquisition.\n", "link": "http://arxiv.org/abs/2404.02807v1", "date": "2024-04-03", "relevancy": 1.4246, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4796}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4788}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.459}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20An%20Optimization%20Framework%20to%20Personalize%20Passive%20Cardiac%20Mechanics&body=Title%3A%20An%20Optimization%20Framework%20to%20Personalize%20Passive%20Cardiac%20Mechanics%0AAuthor%3A%20Lei%20Shi%20and%20Ian%20Chen%20and%20Hiroo%20Takayama%20and%20Vijay%20Vedula%0AAbstract%3A%20%20%20Personalized%20cardiac%20mechanics%20modeling%20is%20a%20powerful%20tool%20for%20understanding%0Athe%20biomechanics%20of%20cardiac%20function%20in%20health%20and%20disease%20and%20assisting%20in%0Atreatment%20planning.%20However%2C%20current%20models%20are%20limited%20to%20using%20medical%20images%0Aacquired%20at%20a%20single%20cardiac%20phase%2C%20often%20limiting%20their%20applicability%20for%0Aprocessing%20dynamic%20image%20acquisitions.%20This%20study%20introduces%20an%20inverse%20finite%0Aelement%20analysis%20%28iFEA%29%20framework%20to%20estimate%20the%20passive%20mechanical%20properties%0Aof%20cardiac%20tissue%20using%20time-dependent%20medical%20image%20data.%20The%20iFEA%20framework%0Arelies%20on%20a%20novel%20nested%20optimization%20scheme%2C%20in%20which%20the%20outer%20iterations%0Autilize%20a%20traditional%20optimization%20method%20to%20best%20approximate%20material%0Aparameters%20that%20fit%20image%20data%2C%20while%20the%20inner%20iterations%20employ%20an%20augmented%0ASellier%27s%20algorithm%20to%20estimate%20the%20stress-free%20reference%20configuration.%20With%20a%0Afocus%20on%20characterizing%20the%20passive%20mechanical%20behavior%2C%20the%20framework%20employs%0Astructurally%20based%20anisotropic%20hyperelastic%20constitutive%20models%20and%0Aphysiologically%20relevant%20boundary%20conditions%20to%20simulate%20myocardial%20mechanics.%0AWe%20use%20a%20stabilized%20variational%20multiscale%20formulation%20for%20solving%20the%0Agoverning%20nonlinear%20elastodynamics%20equations%2C%20verified%20for%20cardiac%20mechanics%0Aapplications.%20The%20framework%20is%20tested%20in%20myocardium%20models%20of%20biventricle%20and%0Aleft%20atrium%20derived%20from%20cardiac%20phase-resolved%20computed%20tomographic%20%28CT%29%0Aimages%20of%20a%20healthy%20subject%20and%20three%20patients%20with%20hypertrophic%20obstructive%0Acardiomyopathy%20%28HOCM%29.%20The%20impact%20of%20the%20choice%20of%20optimization%20methods%20and%0Aother%20numerical%20settings%2C%20including%20fiber%20direction%20parameters%2C%20mesh%20size%2C%0Ainitial%20parameters%20for%20optimization%2C%20and%20perturbations%20to%20optimal%20material%0Aparameters%2C%20is%20assessed%20using%20a%20rigorous%20sensitivity%20analysis.%20The%20performance%0Aof%20the%20current%20iFEA%20is%20compared%20against%20an%20assumed%20power-law-based%0Apressure-volume%20relation%2C%20typically%20used%20for%20single-phase%20image%20acquisition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02807v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Optimization%20Framework%20to%20Personalize%20Passive%20Cardiac%20Mechanics&entry.906535625=Lei%20Shi%20and%20Ian%20Chen%20and%20Hiroo%20Takayama%20and%20Vijay%20Vedula&entry.1292438233=%20%20Personalized%20cardiac%20mechanics%20modeling%20is%20a%20powerful%20tool%20for%20understanding%0Athe%20biomechanics%20of%20cardiac%20function%20in%20health%20and%20disease%20and%20assisting%20in%0Atreatment%20planning.%20However%2C%20current%20models%20are%20limited%20to%20using%20medical%20images%0Aacquired%20at%20a%20single%20cardiac%20phase%2C%20often%20limiting%20their%20applicability%20for%0Aprocessing%20dynamic%20image%20acquisitions.%20This%20study%20introduces%20an%20inverse%20finite%0Aelement%20analysis%20%28iFEA%29%20framework%20to%20estimate%20the%20passive%20mechanical%20properties%0Aof%20cardiac%20tissue%20using%20time-dependent%20medical%20image%20data.%20The%20iFEA%20framework%0Arelies%20on%20a%20novel%20nested%20optimization%20scheme%2C%20in%20which%20the%20outer%20iterations%0Autilize%20a%20traditional%20optimization%20method%20to%20best%20approximate%20material%0Aparameters%20that%20fit%20image%20data%2C%20while%20the%20inner%20iterations%20employ%20an%20augmented%0ASellier%27s%20algorithm%20to%20estimate%20the%20stress-free%20reference%20configuration.%20With%20a%0Afocus%20on%20characterizing%20the%20passive%20mechanical%20behavior%2C%20the%20framework%20employs%0Astructurally%20based%20anisotropic%20hyperelastic%20constitutive%20models%20and%0Aphysiologically%20relevant%20boundary%20conditions%20to%20simulate%20myocardial%20mechanics.%0AWe%20use%20a%20stabilized%20variational%20multiscale%20formulation%20for%20solving%20the%0Agoverning%20nonlinear%20elastodynamics%20equations%2C%20verified%20for%20cardiac%20mechanics%0Aapplications.%20The%20framework%20is%20tested%20in%20myocardium%20models%20of%20biventricle%20and%0Aleft%20atrium%20derived%20from%20cardiac%20phase-resolved%20computed%20tomographic%20%28CT%29%0Aimages%20of%20a%20healthy%20subject%20and%20three%20patients%20with%20hypertrophic%20obstructive%0Acardiomyopathy%20%28HOCM%29.%20The%20impact%20of%20the%20choice%20of%20optimization%20methods%20and%0Aother%20numerical%20settings%2C%20including%20fiber%20direction%20parameters%2C%20mesh%20size%2C%0Ainitial%20parameters%20for%20optimization%2C%20and%20perturbations%20to%20optimal%20material%0Aparameters%2C%20is%20assessed%20using%20a%20rigorous%20sensitivity%20analysis.%20The%20performance%0Aof%20the%20current%20iFEA%20is%20compared%20against%20an%20assumed%20power-law-based%0Apressure-volume%20relation%2C%20typically%20used%20for%20single-phase%20image%20acquisition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02807v1&entry.124074799=Read"},
{"title": "Integrating Explanations in Learning LTL Specifications from\n  Demonstrations", "author": "Ashutosh Gupta and John Komp and Abhay Singh Rajput and Krishna Shankaranarayanan and Ashutosh Trivedi and Namrita Varshney", "abstract": "  This paper investigates whether recent advances in Large Language Models\n(LLMs) can assist in translating human explanations into a format that can\nrobustly support learning Linear Temporal Logic (LTL) from demonstrations. Both\nLLMs and optimization-based methods can extract LTL specifications from\ndemonstrations; however, they have distinct limitations. LLMs can quickly\ngenerate solutions and incorporate human explanations, but their lack of\nconsistency and reliability hampers their applicability in safety-critical\ndomains. On the other hand, optimization-based methods do provide formal\nguarantees but cannot process natural language explanations and face\nscalability challenges. We present a principled approach to combining LLMs and\noptimization-based methods to faithfully translate human explanations and\ndemonstrations into LTL specifications. We have implemented a tool called\nJanaka based on our approach. Our experiments demonstrate the effectiveness of\ncombining explanations with demonstrations in learning LTL specifications\nthrough several case studies.\n", "link": "http://arxiv.org/abs/2404.02872v1", "date": "2024-04-03", "relevancy": 0.954, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4822}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4818}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.467}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Integrating%20Explanations%20in%20Learning%20LTL%20Specifications%20from%0A%20%20Demonstrations&body=Title%3A%20Integrating%20Explanations%20in%20Learning%20LTL%20Specifications%20from%0A%20%20Demonstrations%0AAuthor%3A%20Ashutosh%20Gupta%20and%20John%20Komp%20and%20Abhay%20Singh%20Rajput%20and%20Krishna%20Shankaranarayanan%20and%20Ashutosh%20Trivedi%20and%20Namrita%20Varshney%0AAbstract%3A%20%20%20This%20paper%20investigates%20whether%20recent%20advances%20in%20Large%20Language%20Models%0A%28LLMs%29%20can%20assist%20in%20translating%20human%20explanations%20into%20a%20format%20that%20can%0Arobustly%20support%20learning%20Linear%20Temporal%20Logic%20%28LTL%29%20from%20demonstrations.%20Both%0ALLMs%20and%20optimization-based%20methods%20can%20extract%20LTL%20specifications%20from%0Ademonstrations%3B%20however%2C%20they%20have%20distinct%20limitations.%20LLMs%20can%20quickly%0Agenerate%20solutions%20and%20incorporate%20human%20explanations%2C%20but%20their%20lack%20of%0Aconsistency%20and%20reliability%20hampers%20their%20applicability%20in%20safety-critical%0Adomains.%20On%20the%20other%20hand%2C%20optimization-based%20methods%20do%20provide%20formal%0Aguarantees%20but%20cannot%20process%20natural%20language%20explanations%20and%20face%0Ascalability%20challenges.%20We%20present%20a%20principled%20approach%20to%20combining%20LLMs%20and%0Aoptimization-based%20methods%20to%20faithfully%20translate%20human%20explanations%20and%0Ademonstrations%20into%20LTL%20specifications.%20We%20have%20implemented%20a%20tool%20called%0AJanaka%20based%20on%20our%20approach.%20Our%20experiments%20demonstrate%20the%20effectiveness%20of%0Acombining%20explanations%20with%20demonstrations%20in%20learning%20LTL%20specifications%0Athrough%20several%20case%20studies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02872v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20Explanations%20in%20Learning%20LTL%20Specifications%20from%0A%20%20Demonstrations&entry.906535625=Ashutosh%20Gupta%20and%20John%20Komp%20and%20Abhay%20Singh%20Rajput%20and%20Krishna%20Shankaranarayanan%20and%20Ashutosh%20Trivedi%20and%20Namrita%20Varshney&entry.1292438233=%20%20This%20paper%20investigates%20whether%20recent%20advances%20in%20Large%20Language%20Models%0A%28LLMs%29%20can%20assist%20in%20translating%20human%20explanations%20into%20a%20format%20that%20can%0Arobustly%20support%20learning%20Linear%20Temporal%20Logic%20%28LTL%29%20from%20demonstrations.%20Both%0ALLMs%20and%20optimization-based%20methods%20can%20extract%20LTL%20specifications%20from%0Ademonstrations%3B%20however%2C%20they%20have%20distinct%20limitations.%20LLMs%20can%20quickly%0Agenerate%20solutions%20and%20incorporate%20human%20explanations%2C%20but%20their%20lack%20of%0Aconsistency%20and%20reliability%20hampers%20their%20applicability%20in%20safety-critical%0Adomains.%20On%20the%20other%20hand%2C%20optimization-based%20methods%20do%20provide%20formal%0Aguarantees%20but%20cannot%20process%20natural%20language%20explanations%20and%20face%0Ascalability%20challenges.%20We%20present%20a%20principled%20approach%20to%20combining%20LLMs%20and%0Aoptimization-based%20methods%20to%20faithfully%20translate%20human%20explanations%20and%0Ademonstrations%20into%20LTL%20specifications.%20We%20have%20implemented%20a%20tool%20called%0AJanaka%20based%20on%20our%20approach.%20Our%20experiments%20demonstrate%20the%20effectiveness%20of%0Acombining%20explanations%20with%20demonstrations%20in%20learning%20LTL%20specifications%0Athrough%20several%20case%20studies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02872v1&entry.124074799=Read"},
{"title": "Financial Risk Management on a Neutral Atom Quantum Processor", "author": "Lucas Leclerc and Luis Ortiz-Guitierrez and Sebastian Grijalva and Boris Albrecht and Julia R. K. Cline and Vincent E. Elfving and Adrien Signoles and Lo\u00efc Henriet and Gianni Del Bimbo and Usman Ayub Sheikh and Maitree Shah and Luc Andrea and Faysal Ishtiaq and Andoni Duarte and Samuel Mugel and Irene Caceres and Michel Kurek and Roman Orus and Achraf Seddik and Oumaima Hammammi and Hacene Isselnane and Didier M'tamon", "abstract": "  Machine Learning models capable of handling the large datasets collected in\nthe financial world can often become black boxes expensive to run. The quantum\ncomputing paradigm suggests new optimization techniques, that combined with\nclassical algorithms, may deliver competitive, faster and more interpretable\nmodels. In this work we propose a quantum-enhanced machine learning solution\nfor the prediction of credit rating downgrades, also known as fallen-angels\nforecasting in the financial risk management field. We implement this solution\non a neutral atom Quantum Processing Unit with up to 60 qubits on a real-life\ndataset. We report competitive performances against the state-of-the-art Random\nForest benchmark whilst our model achieves better interpretability and\ncomparable training times. We examine how to improve performance in the\nnear-term validating our ideas with Tensor Networks-based numerical\nsimulations.\n", "link": "http://arxiv.org/abs/2212.03223v2", "date": "2024-04-03", "relevancy": 1.3918, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4766}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4762}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4539}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Financial%20Risk%20Management%20on%20a%20Neutral%20Atom%20Quantum%20Processor&body=Title%3A%20Financial%20Risk%20Management%20on%20a%20Neutral%20Atom%20Quantum%20Processor%0AAuthor%3A%20Lucas%20Leclerc%20and%20Luis%20Ortiz-Guitierrez%20and%20Sebastian%20Grijalva%20and%20Boris%20Albrecht%20and%20Julia%20R.%20K.%20Cline%20and%20Vincent%20E.%20Elfving%20and%20Adrien%20Signoles%20and%20Lo%C3%AFc%20Henriet%20and%20Gianni%20Del%20Bimbo%20and%20Usman%20Ayub%20Sheikh%20and%20Maitree%20Shah%20and%20Luc%20Andrea%20and%20Faysal%20Ishtiaq%20and%20Andoni%20Duarte%20and%20Samuel%20Mugel%20and%20Irene%20Caceres%20and%20Michel%20Kurek%20and%20Roman%20Orus%20and%20Achraf%20Seddik%20and%20Oumaima%20Hammammi%20and%20Hacene%20Isselnane%20and%20Didier%20M%27tamon%0AAbstract%3A%20%20%20Machine%20Learning%20models%20capable%20of%20handling%20the%20large%20datasets%20collected%20in%0Athe%20financial%20world%20can%20often%20become%20black%20boxes%20expensive%20to%20run.%20The%20quantum%0Acomputing%20paradigm%20suggests%20new%20optimization%20techniques%2C%20that%20combined%20with%0Aclassical%20algorithms%2C%20may%20deliver%20competitive%2C%20faster%20and%20more%20interpretable%0Amodels.%20In%20this%20work%20we%20propose%20a%20quantum-enhanced%20machine%20learning%20solution%0Afor%20the%20prediction%20of%20credit%20rating%20downgrades%2C%20also%20known%20as%20fallen-angels%0Aforecasting%20in%20the%20financial%20risk%20management%20field.%20We%20implement%20this%20solution%0Aon%20a%20neutral%20atom%20Quantum%20Processing%20Unit%20with%20up%20to%2060%20qubits%20on%20a%20real-life%0Adataset.%20We%20report%20competitive%20performances%20against%20the%20state-of-the-art%20Random%0AForest%20benchmark%20whilst%20our%20model%20achieves%20better%20interpretability%20and%0Acomparable%20training%20times.%20We%20examine%20how%20to%20improve%20performance%20in%20the%0Anear-term%20validating%20our%20ideas%20with%20Tensor%20Networks-based%20numerical%0Asimulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.03223v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Financial%20Risk%20Management%20on%20a%20Neutral%20Atom%20Quantum%20Processor&entry.906535625=Lucas%20Leclerc%20and%20Luis%20Ortiz-Guitierrez%20and%20Sebastian%20Grijalva%20and%20Boris%20Albrecht%20and%20Julia%20R.%20K.%20Cline%20and%20Vincent%20E.%20Elfving%20and%20Adrien%20Signoles%20and%20Lo%C3%AFc%20Henriet%20and%20Gianni%20Del%20Bimbo%20and%20Usman%20Ayub%20Sheikh%20and%20Maitree%20Shah%20and%20Luc%20Andrea%20and%20Faysal%20Ishtiaq%20and%20Andoni%20Duarte%20and%20Samuel%20Mugel%20and%20Irene%20Caceres%20and%20Michel%20Kurek%20and%20Roman%20Orus%20and%20Achraf%20Seddik%20and%20Oumaima%20Hammammi%20and%20Hacene%20Isselnane%20and%20Didier%20M%27tamon&entry.1292438233=%20%20Machine%20Learning%20models%20capable%20of%20handling%20the%20large%20datasets%20collected%20in%0Athe%20financial%20world%20can%20often%20become%20black%20boxes%20expensive%20to%20run.%20The%20quantum%0Acomputing%20paradigm%20suggests%20new%20optimization%20techniques%2C%20that%20combined%20with%0Aclassical%20algorithms%2C%20may%20deliver%20competitive%2C%20faster%20and%20more%20interpretable%0Amodels.%20In%20this%20work%20we%20propose%20a%20quantum-enhanced%20machine%20learning%20solution%0Afor%20the%20prediction%20of%20credit%20rating%20downgrades%2C%20also%20known%20as%20fallen-angels%0Aforecasting%20in%20the%20financial%20risk%20management%20field.%20We%20implement%20this%20solution%0Aon%20a%20neutral%20atom%20Quantum%20Processing%20Unit%20with%20up%20to%2060%20qubits%20on%20a%20real-life%0Adataset.%20We%20report%20competitive%20performances%20against%20the%20state-of-the-art%20Random%0AForest%20benchmark%20whilst%20our%20model%20achieves%20better%20interpretability%20and%0Acomparable%20training%20times.%20We%20examine%20how%20to%20improve%20performance%20in%20the%0Anear-term%20validating%20our%20ideas%20with%20Tensor%20Networks-based%20numerical%0Asimulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.03223v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


