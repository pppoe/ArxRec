<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250505.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "3D Vision-Language Gaussian Splatting", "author": "Qucheng Peng and Benjamin Planche and Zhongpai Gao and Meng Zheng and Anwesa Choudhuri and Terrence Chen and Chen Chen and Ziyan Wu", "abstract": "  Recent advancements in 3D reconstruction methods and vision-language models\nhave propelled the development of multi-modal 3D scene understanding, which has\nvital applications in robotics, autonomous driving, and virtual/augmented\nreality. However, current multi-modal scene understanding approaches have\nnaively embedded semantic representations into 3D reconstruction methods\nwithout striking a balance between visual and language modalities, which leads\nto unsatisfying semantic rasterization of translucent or reflective objects, as\nwell as over-fitting on color modality. To alleviate these limitations, we\npropose a solution that adequately handles the distinct visual and semantic\nmodalities, i.e., a 3D vision-language Gaussian splatting model for scene\nunderstanding, to put emphasis on the representation learning of language\nmodality. We propose a novel cross-modal rasterizer, using modality fusion\nalong with a smoothed semantic indicator for enhancing semantic rasterization.\nWe also employ a camera-view blending technique to improve semantic consistency\nbetween existing and synthesized views, thereby effectively mitigating\nover-fitting. Extensive experiments demonstrate that our method achieves\nstate-of-the-art performance in open-vocabulary semantic segmentation,\nsurpassing existing methods by a significant margin.\n", "link": "http://arxiv.org/abs/2410.07577v2", "date": "2025-05-05", "relevancy": 3.3539, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.71}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6528}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Vision-Language%20Gaussian%20Splatting&body=Title%3A%203D%20Vision-Language%20Gaussian%20Splatting%0AAuthor%3A%20Qucheng%20Peng%20and%20Benjamin%20Planche%20and%20Zhongpai%20Gao%20and%20Meng%20Zheng%20and%20Anwesa%20Choudhuri%20and%20Terrence%20Chen%20and%20Chen%20Chen%20and%20Ziyan%20Wu%0AAbstract%3A%20%20%20Recent%20advancements%20in%203D%20reconstruction%20methods%20and%20vision-language%20models%0Ahave%20propelled%20the%20development%20of%20multi-modal%203D%20scene%20understanding%2C%20which%20has%0Avital%20applications%20in%20robotics%2C%20autonomous%20driving%2C%20and%20virtual/augmented%0Areality.%20However%2C%20current%20multi-modal%20scene%20understanding%20approaches%20have%0Anaively%20embedded%20semantic%20representations%20into%203D%20reconstruction%20methods%0Awithout%20striking%20a%20balance%20between%20visual%20and%20language%20modalities%2C%20which%20leads%0Ato%20unsatisfying%20semantic%20rasterization%20of%20translucent%20or%20reflective%20objects%2C%20as%0Awell%20as%20over-fitting%20on%20color%20modality.%20To%20alleviate%20these%20limitations%2C%20we%0Apropose%20a%20solution%20that%20adequately%20handles%20the%20distinct%20visual%20and%20semantic%0Amodalities%2C%20i.e.%2C%20a%203D%20vision-language%20Gaussian%20splatting%20model%20for%20scene%0Aunderstanding%2C%20to%20put%20emphasis%20on%20the%20representation%20learning%20of%20language%0Amodality.%20We%20propose%20a%20novel%20cross-modal%20rasterizer%2C%20using%20modality%20fusion%0Aalong%20with%20a%20smoothed%20semantic%20indicator%20for%20enhancing%20semantic%20rasterization.%0AWe%20also%20employ%20a%20camera-view%20blending%20technique%20to%20improve%20semantic%20consistency%0Abetween%20existing%20and%20synthesized%20views%2C%20thereby%20effectively%20mitigating%0Aover-fitting.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20achieves%0Astate-of-the-art%20performance%20in%20open-vocabulary%20semantic%20segmentation%2C%0Asurpassing%20existing%20methods%20by%20a%20significant%20margin.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07577v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Vision-Language%2520Gaussian%2520Splatting%26entry.906535625%3DQucheng%2520Peng%2520and%2520Benjamin%2520Planche%2520and%2520Zhongpai%2520Gao%2520and%2520Meng%2520Zheng%2520and%2520Anwesa%2520Choudhuri%2520and%2520Terrence%2520Chen%2520and%2520Chen%2520Chen%2520and%2520Ziyan%2520Wu%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%25203D%2520reconstruction%2520methods%2520and%2520vision-language%2520models%250Ahave%2520propelled%2520the%2520development%2520of%2520multi-modal%25203D%2520scene%2520understanding%252C%2520which%2520has%250Avital%2520applications%2520in%2520robotics%252C%2520autonomous%2520driving%252C%2520and%2520virtual/augmented%250Areality.%2520However%252C%2520current%2520multi-modal%2520scene%2520understanding%2520approaches%2520have%250Anaively%2520embedded%2520semantic%2520representations%2520into%25203D%2520reconstruction%2520methods%250Awithout%2520striking%2520a%2520balance%2520between%2520visual%2520and%2520language%2520modalities%252C%2520which%2520leads%250Ato%2520unsatisfying%2520semantic%2520rasterization%2520of%2520translucent%2520or%2520reflective%2520objects%252C%2520as%250Awell%2520as%2520over-fitting%2520on%2520color%2520modality.%2520To%2520alleviate%2520these%2520limitations%252C%2520we%250Apropose%2520a%2520solution%2520that%2520adequately%2520handles%2520the%2520distinct%2520visual%2520and%2520semantic%250Amodalities%252C%2520i.e.%252C%2520a%25203D%2520vision-language%2520Gaussian%2520splatting%2520model%2520for%2520scene%250Aunderstanding%252C%2520to%2520put%2520emphasis%2520on%2520the%2520representation%2520learning%2520of%2520language%250Amodality.%2520We%2520propose%2520a%2520novel%2520cross-modal%2520rasterizer%252C%2520using%2520modality%2520fusion%250Aalong%2520with%2520a%2520smoothed%2520semantic%2520indicator%2520for%2520enhancing%2520semantic%2520rasterization.%250AWe%2520also%2520employ%2520a%2520camera-view%2520blending%2520technique%2520to%2520improve%2520semantic%2520consistency%250Abetween%2520existing%2520and%2520synthesized%2520views%252C%2520thereby%2520effectively%2520mitigating%250Aover-fitting.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520achieves%250Astate-of-the-art%2520performance%2520in%2520open-vocabulary%2520semantic%2520segmentation%252C%250Asurpassing%2520existing%2520methods%2520by%2520a%2520significant%2520margin.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07577v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Vision-Language%20Gaussian%20Splatting&entry.906535625=Qucheng%20Peng%20and%20Benjamin%20Planche%20and%20Zhongpai%20Gao%20and%20Meng%20Zheng%20and%20Anwesa%20Choudhuri%20and%20Terrence%20Chen%20and%20Chen%20Chen%20and%20Ziyan%20Wu&entry.1292438233=%20%20Recent%20advancements%20in%203D%20reconstruction%20methods%20and%20vision-language%20models%0Ahave%20propelled%20the%20development%20of%20multi-modal%203D%20scene%20understanding%2C%20which%20has%0Avital%20applications%20in%20robotics%2C%20autonomous%20driving%2C%20and%20virtual/augmented%0Areality.%20However%2C%20current%20multi-modal%20scene%20understanding%20approaches%20have%0Anaively%20embedded%20semantic%20representations%20into%203D%20reconstruction%20methods%0Awithout%20striking%20a%20balance%20between%20visual%20and%20language%20modalities%2C%20which%20leads%0Ato%20unsatisfying%20semantic%20rasterization%20of%20translucent%20or%20reflective%20objects%2C%20as%0Awell%20as%20over-fitting%20on%20color%20modality.%20To%20alleviate%20these%20limitations%2C%20we%0Apropose%20a%20solution%20that%20adequately%20handles%20the%20distinct%20visual%20and%20semantic%0Amodalities%2C%20i.e.%2C%20a%203D%20vision-language%20Gaussian%20splatting%20model%20for%20scene%0Aunderstanding%2C%20to%20put%20emphasis%20on%20the%20representation%20learning%20of%20language%0Amodality.%20We%20propose%20a%20novel%20cross-modal%20rasterizer%2C%20using%20modality%20fusion%0Aalong%20with%20a%20smoothed%20semantic%20indicator%20for%20enhancing%20semantic%20rasterization.%0AWe%20also%20employ%20a%20camera-view%20blending%20technique%20to%20improve%20semantic%20consistency%0Abetween%20existing%20and%20synthesized%20views%2C%20thereby%20effectively%20mitigating%0Aover-fitting.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20achieves%0Astate-of-the-art%20performance%20in%20open-vocabulary%20semantic%20segmentation%2C%0Asurpassing%20existing%20methods%20by%20a%20significant%20margin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07577v2&entry.124074799=Read"},
{"title": "Unsupervised Deep Learning-based Keypoint Localization Estimating\n  Descriptor Matching Performance", "author": "David Rivas-Villar and \u00c1lvaro S. Hervella and Jos\u00e9 Rouco and Jorge Novo", "abstract": "  Retinal image registration, particularly for color fundus images, is a\nchallenging yet essential task with diverse clinical applications. Existing\nregistration methods for color fundus images typically rely on keypoints and\ndescriptors for alignment; however, a significant limitation is their reliance\non labeled data, which is particularly scarce in the medical domain.\n  In this work, we present a novel unsupervised registration pipeline that\nentirely eliminates the need for labeled data. Our approach is based on the\nprinciple that locations with distinctive descriptors constitute reliable\nkeypoints. This fully inverts the conventional state-of-the-art approach,\nconditioning the detector on the descriptor rather than the opposite.\n  First, we propose an innovative descriptor learning method that operates\nwithout keypoint detection or any labels, generating descriptors for arbitrary\nlocations in retinal images. Next, we introduce a novel, label-free keypoint\ndetector network which works by estimating descriptor performance directly from\nthe input image.\n  We validate our method through a comprehensive evaluation on four hold-out\ndatasets, demonstrating that our unsupervised descriptor outperforms\nstate-of-the-art supervised descriptors and that our unsupervised detector\nsignificantly outperforms existing unsupervised detection methods. Finally, our\nfull registration pipeline achieves performance comparable to the leading\nsupervised methods, while not employing any labeled data. Additionally, the\nlabel-free nature and design of our method enable direct adaptation to other\ndomains and modalities.\n", "link": "http://arxiv.org/abs/2505.02779v1", "date": "2025-05-05", "relevancy": 3.0279, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6825}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5735}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Deep%20Learning-based%20Keypoint%20Localization%20Estimating%0A%20%20Descriptor%20Matching%20Performance&body=Title%3A%20Unsupervised%20Deep%20Learning-based%20Keypoint%20Localization%20Estimating%0A%20%20Descriptor%20Matching%20Performance%0AAuthor%3A%20David%20Rivas-Villar%20and%20%C3%81lvaro%20S.%20Hervella%20and%20Jos%C3%A9%20Rouco%20and%20Jorge%20Novo%0AAbstract%3A%20%20%20Retinal%20image%20registration%2C%20particularly%20for%20color%20fundus%20images%2C%20is%20a%0Achallenging%20yet%20essential%20task%20with%20diverse%20clinical%20applications.%20Existing%0Aregistration%20methods%20for%20color%20fundus%20images%20typically%20rely%20on%20keypoints%20and%0Adescriptors%20for%20alignment%3B%20however%2C%20a%20significant%20limitation%20is%20their%20reliance%0Aon%20labeled%20data%2C%20which%20is%20particularly%20scarce%20in%20the%20medical%20domain.%0A%20%20In%20this%20work%2C%20we%20present%20a%20novel%20unsupervised%20registration%20pipeline%20that%0Aentirely%20eliminates%20the%20need%20for%20labeled%20data.%20Our%20approach%20is%20based%20on%20the%0Aprinciple%20that%20locations%20with%20distinctive%20descriptors%20constitute%20reliable%0Akeypoints.%20This%20fully%20inverts%20the%20conventional%20state-of-the-art%20approach%2C%0Aconditioning%20the%20detector%20on%20the%20descriptor%20rather%20than%20the%20opposite.%0A%20%20First%2C%20we%20propose%20an%20innovative%20descriptor%20learning%20method%20that%20operates%0Awithout%20keypoint%20detection%20or%20any%20labels%2C%20generating%20descriptors%20for%20arbitrary%0Alocations%20in%20retinal%20images.%20Next%2C%20we%20introduce%20a%20novel%2C%20label-free%20keypoint%0Adetector%20network%20which%20works%20by%20estimating%20descriptor%20performance%20directly%20from%0Athe%20input%20image.%0A%20%20We%20validate%20our%20method%20through%20a%20comprehensive%20evaluation%20on%20four%20hold-out%0Adatasets%2C%20demonstrating%20that%20our%20unsupervised%20descriptor%20outperforms%0Astate-of-the-art%20supervised%20descriptors%20and%20that%20our%20unsupervised%20detector%0Asignificantly%20outperforms%20existing%20unsupervised%20detection%20methods.%20Finally%2C%20our%0Afull%20registration%20pipeline%20achieves%20performance%20comparable%20to%20the%20leading%0Asupervised%20methods%2C%20while%20not%20employing%20any%20labeled%20data.%20Additionally%2C%20the%0Alabel-free%20nature%20and%20design%20of%20our%20method%20enable%20direct%20adaptation%20to%20other%0Adomains%20and%20modalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02779v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Deep%2520Learning-based%2520Keypoint%2520Localization%2520Estimating%250A%2520%2520Descriptor%2520Matching%2520Performance%26entry.906535625%3DDavid%2520Rivas-Villar%2520and%2520%25C3%2581lvaro%2520S.%2520Hervella%2520and%2520Jos%25C3%25A9%2520Rouco%2520and%2520Jorge%2520Novo%26entry.1292438233%3D%2520%2520Retinal%2520image%2520registration%252C%2520particularly%2520for%2520color%2520fundus%2520images%252C%2520is%2520a%250Achallenging%2520yet%2520essential%2520task%2520with%2520diverse%2520clinical%2520applications.%2520Existing%250Aregistration%2520methods%2520for%2520color%2520fundus%2520images%2520typically%2520rely%2520on%2520keypoints%2520and%250Adescriptors%2520for%2520alignment%253B%2520however%252C%2520a%2520significant%2520limitation%2520is%2520their%2520reliance%250Aon%2520labeled%2520data%252C%2520which%2520is%2520particularly%2520scarce%2520in%2520the%2520medical%2520domain.%250A%2520%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520novel%2520unsupervised%2520registration%2520pipeline%2520that%250Aentirely%2520eliminates%2520the%2520need%2520for%2520labeled%2520data.%2520Our%2520approach%2520is%2520based%2520on%2520the%250Aprinciple%2520that%2520locations%2520with%2520distinctive%2520descriptors%2520constitute%2520reliable%250Akeypoints.%2520This%2520fully%2520inverts%2520the%2520conventional%2520state-of-the-art%2520approach%252C%250Aconditioning%2520the%2520detector%2520on%2520the%2520descriptor%2520rather%2520than%2520the%2520opposite.%250A%2520%2520First%252C%2520we%2520propose%2520an%2520innovative%2520descriptor%2520learning%2520method%2520that%2520operates%250Awithout%2520keypoint%2520detection%2520or%2520any%2520labels%252C%2520generating%2520descriptors%2520for%2520arbitrary%250Alocations%2520in%2520retinal%2520images.%2520Next%252C%2520we%2520introduce%2520a%2520novel%252C%2520label-free%2520keypoint%250Adetector%2520network%2520which%2520works%2520by%2520estimating%2520descriptor%2520performance%2520directly%2520from%250Athe%2520input%2520image.%250A%2520%2520We%2520validate%2520our%2520method%2520through%2520a%2520comprehensive%2520evaluation%2520on%2520four%2520hold-out%250Adatasets%252C%2520demonstrating%2520that%2520our%2520unsupervised%2520descriptor%2520outperforms%250Astate-of-the-art%2520supervised%2520descriptors%2520and%2520that%2520our%2520unsupervised%2520detector%250Asignificantly%2520outperforms%2520existing%2520unsupervised%2520detection%2520methods.%2520Finally%252C%2520our%250Afull%2520registration%2520pipeline%2520achieves%2520performance%2520comparable%2520to%2520the%2520leading%250Asupervised%2520methods%252C%2520while%2520not%2520employing%2520any%2520labeled%2520data.%2520Additionally%252C%2520the%250Alabel-free%2520nature%2520and%2520design%2520of%2520our%2520method%2520enable%2520direct%2520adaptation%2520to%2520other%250Adomains%2520and%2520modalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02779v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Deep%20Learning-based%20Keypoint%20Localization%20Estimating%0A%20%20Descriptor%20Matching%20Performance&entry.906535625=David%20Rivas-Villar%20and%20%C3%81lvaro%20S.%20Hervella%20and%20Jos%C3%A9%20Rouco%20and%20Jorge%20Novo&entry.1292438233=%20%20Retinal%20image%20registration%2C%20particularly%20for%20color%20fundus%20images%2C%20is%20a%0Achallenging%20yet%20essential%20task%20with%20diverse%20clinical%20applications.%20Existing%0Aregistration%20methods%20for%20color%20fundus%20images%20typically%20rely%20on%20keypoints%20and%0Adescriptors%20for%20alignment%3B%20however%2C%20a%20significant%20limitation%20is%20their%20reliance%0Aon%20labeled%20data%2C%20which%20is%20particularly%20scarce%20in%20the%20medical%20domain.%0A%20%20In%20this%20work%2C%20we%20present%20a%20novel%20unsupervised%20registration%20pipeline%20that%0Aentirely%20eliminates%20the%20need%20for%20labeled%20data.%20Our%20approach%20is%20based%20on%20the%0Aprinciple%20that%20locations%20with%20distinctive%20descriptors%20constitute%20reliable%0Akeypoints.%20This%20fully%20inverts%20the%20conventional%20state-of-the-art%20approach%2C%0Aconditioning%20the%20detector%20on%20the%20descriptor%20rather%20than%20the%20opposite.%0A%20%20First%2C%20we%20propose%20an%20innovative%20descriptor%20learning%20method%20that%20operates%0Awithout%20keypoint%20detection%20or%20any%20labels%2C%20generating%20descriptors%20for%20arbitrary%0Alocations%20in%20retinal%20images.%20Next%2C%20we%20introduce%20a%20novel%2C%20label-free%20keypoint%0Adetector%20network%20which%20works%20by%20estimating%20descriptor%20performance%20directly%20from%0Athe%20input%20image.%0A%20%20We%20validate%20our%20method%20through%20a%20comprehensive%20evaluation%20on%20four%20hold-out%0Adatasets%2C%20demonstrating%20that%20our%20unsupervised%20descriptor%20outperforms%0Astate-of-the-art%20supervised%20descriptors%20and%20that%20our%20unsupervised%20detector%0Asignificantly%20outperforms%20existing%20unsupervised%20detection%20methods.%20Finally%2C%20our%0Afull%20registration%20pipeline%20achieves%20performance%20comparable%20to%20the%20leading%0Asupervised%20methods%2C%20while%20not%20employing%20any%20labeled%20data.%20Additionally%2C%20the%0Alabel-free%20nature%20and%20design%20of%20our%20method%20enable%20direct%20adaptation%20to%20other%0Adomains%20and%20modalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02779v1&entry.124074799=Read"},
{"title": "Interpretable Dynamic Graph Neural Networks for Small Occluded Object\n  Detection and Tracking", "author": "Shahriar Soudeep and Md Abrar Jahin and M. F. Mridha", "abstract": "  The detection and tracking of small, occluded objects such as pedestrians,\ncyclists, and motorbikes pose significant challenges for traffic surveillance\nsystems because of their erratic movement, frequent occlusion, and poor\nvisibility in dynamic urban environments. Traditional methods like YOLO11,\nwhile proficient in spatial feature extraction for precise detection, often\nstruggle with these small and dynamically moving objects, particularly in\nhandling real-time data updates and resource efficiency. This paper introduces\nDGNN-YOLO, a novel framework that integrates dynamic graph neural networks\n(DGNNs) with YOLO11 to address these limitations. Unlike standard GNNs, DGNNs\nare chosen for their superior ability to dynamically update graph structures in\nreal-time, which enables adaptive and robust tracking of objects in highly\nvariable urban traffic scenarios. This framework constructs and regularly\nupdates its graph representations, capturing objects as nodes and their\ninteractions as edges, thus effectively responding to rapidly changing\nconditions. Additionally, DGNN-YOLO incorporates Grad-CAM, Grad-CAM++, and\nEigen-CAM visualization techniques to enhance interpretability and foster\ntrust, offering insights into the model's decision-making process. Extensive\nexperiments validate the framework's performance, achieving a precision of\n0.8382, recall of 0.6875, and mAP@0.5:0.95 of 0.6476, significantly\noutperforming existing methods. This study offers a scalable and interpretable\nsolution for real-time traffic surveillance and significantly advances\nintelligent transportation systems' capabilities by addressing the critical\nchallenge of detecting and tracking small, occluded objects.\n", "link": "http://arxiv.org/abs/2411.17251v8", "date": "2025-05-05", "relevancy": 2.8589, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5915}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5636}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20Dynamic%20Graph%20Neural%20Networks%20for%20Small%20Occluded%20Object%0A%20%20Detection%20and%20Tracking&body=Title%3A%20Interpretable%20Dynamic%20Graph%20Neural%20Networks%20for%20Small%20Occluded%20Object%0A%20%20Detection%20and%20Tracking%0AAuthor%3A%20Shahriar%20Soudeep%20and%20Md%20Abrar%20Jahin%20and%20M.%20F.%20Mridha%0AAbstract%3A%20%20%20The%20detection%20and%20tracking%20of%20small%2C%20occluded%20objects%20such%20as%20pedestrians%2C%0Acyclists%2C%20and%20motorbikes%20pose%20significant%20challenges%20for%20traffic%20surveillance%0Asystems%20because%20of%20their%20erratic%20movement%2C%20frequent%20occlusion%2C%20and%20poor%0Avisibility%20in%20dynamic%20urban%20environments.%20Traditional%20methods%20like%20YOLO11%2C%0Awhile%20proficient%20in%20spatial%20feature%20extraction%20for%20precise%20detection%2C%20often%0Astruggle%20with%20these%20small%20and%20dynamically%20moving%20objects%2C%20particularly%20in%0Ahandling%20real-time%20data%20updates%20and%20resource%20efficiency.%20This%20paper%20introduces%0ADGNN-YOLO%2C%20a%20novel%20framework%20that%20integrates%20dynamic%20graph%20neural%20networks%0A%28DGNNs%29%20with%20YOLO11%20to%20address%20these%20limitations.%20Unlike%20standard%20GNNs%2C%20DGNNs%0Aare%20chosen%20for%20their%20superior%20ability%20to%20dynamically%20update%20graph%20structures%20in%0Areal-time%2C%20which%20enables%20adaptive%20and%20robust%20tracking%20of%20objects%20in%20highly%0Avariable%20urban%20traffic%20scenarios.%20This%20framework%20constructs%20and%20regularly%0Aupdates%20its%20graph%20representations%2C%20capturing%20objects%20as%20nodes%20and%20their%0Ainteractions%20as%20edges%2C%20thus%20effectively%20responding%20to%20rapidly%20changing%0Aconditions.%20Additionally%2C%20DGNN-YOLO%20incorporates%20Grad-CAM%2C%20Grad-CAM%2B%2B%2C%20and%0AEigen-CAM%20visualization%20techniques%20to%20enhance%20interpretability%20and%20foster%0Atrust%2C%20offering%20insights%20into%20the%20model%27s%20decision-making%20process.%20Extensive%0Aexperiments%20validate%20the%20framework%27s%20performance%2C%20achieving%20a%20precision%20of%0A0.8382%2C%20recall%20of%200.6875%2C%20and%20mAP%400.5%3A0.95%20of%200.6476%2C%20significantly%0Aoutperforming%20existing%20methods.%20This%20study%20offers%20a%20scalable%20and%20interpretable%0Asolution%20for%20real-time%20traffic%20surveillance%20and%20significantly%20advances%0Aintelligent%20transportation%20systems%27%20capabilities%20by%20addressing%20the%20critical%0Achallenge%20of%20detecting%20and%20tracking%20small%2C%20occluded%20objects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17251v8%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520Dynamic%2520Graph%2520Neural%2520Networks%2520for%2520Small%2520Occluded%2520Object%250A%2520%2520Detection%2520and%2520Tracking%26entry.906535625%3DShahriar%2520Soudeep%2520and%2520Md%2520Abrar%2520Jahin%2520and%2520M.%2520F.%2520Mridha%26entry.1292438233%3D%2520%2520The%2520detection%2520and%2520tracking%2520of%2520small%252C%2520occluded%2520objects%2520such%2520as%2520pedestrians%252C%250Acyclists%252C%2520and%2520motorbikes%2520pose%2520significant%2520challenges%2520for%2520traffic%2520surveillance%250Asystems%2520because%2520of%2520their%2520erratic%2520movement%252C%2520frequent%2520occlusion%252C%2520and%2520poor%250Avisibility%2520in%2520dynamic%2520urban%2520environments.%2520Traditional%2520methods%2520like%2520YOLO11%252C%250Awhile%2520proficient%2520in%2520spatial%2520feature%2520extraction%2520for%2520precise%2520detection%252C%2520often%250Astruggle%2520with%2520these%2520small%2520and%2520dynamically%2520moving%2520objects%252C%2520particularly%2520in%250Ahandling%2520real-time%2520data%2520updates%2520and%2520resource%2520efficiency.%2520This%2520paper%2520introduces%250ADGNN-YOLO%252C%2520a%2520novel%2520framework%2520that%2520integrates%2520dynamic%2520graph%2520neural%2520networks%250A%2528DGNNs%2529%2520with%2520YOLO11%2520to%2520address%2520these%2520limitations.%2520Unlike%2520standard%2520GNNs%252C%2520DGNNs%250Aare%2520chosen%2520for%2520their%2520superior%2520ability%2520to%2520dynamically%2520update%2520graph%2520structures%2520in%250Areal-time%252C%2520which%2520enables%2520adaptive%2520and%2520robust%2520tracking%2520of%2520objects%2520in%2520highly%250Avariable%2520urban%2520traffic%2520scenarios.%2520This%2520framework%2520constructs%2520and%2520regularly%250Aupdates%2520its%2520graph%2520representations%252C%2520capturing%2520objects%2520as%2520nodes%2520and%2520their%250Ainteractions%2520as%2520edges%252C%2520thus%2520effectively%2520responding%2520to%2520rapidly%2520changing%250Aconditions.%2520Additionally%252C%2520DGNN-YOLO%2520incorporates%2520Grad-CAM%252C%2520Grad-CAM%252B%252B%252C%2520and%250AEigen-CAM%2520visualization%2520techniques%2520to%2520enhance%2520interpretability%2520and%2520foster%250Atrust%252C%2520offering%2520insights%2520into%2520the%2520model%2527s%2520decision-making%2520process.%2520Extensive%250Aexperiments%2520validate%2520the%2520framework%2527s%2520performance%252C%2520achieving%2520a%2520precision%2520of%250A0.8382%252C%2520recall%2520of%25200.6875%252C%2520and%2520mAP%25400.5%253A0.95%2520of%25200.6476%252C%2520significantly%250Aoutperforming%2520existing%2520methods.%2520This%2520study%2520offers%2520a%2520scalable%2520and%2520interpretable%250Asolution%2520for%2520real-time%2520traffic%2520surveillance%2520and%2520significantly%2520advances%250Aintelligent%2520transportation%2520systems%2527%2520capabilities%2520by%2520addressing%2520the%2520critical%250Achallenge%2520of%2520detecting%2520and%2520tracking%2520small%252C%2520occluded%2520objects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17251v8%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20Dynamic%20Graph%20Neural%20Networks%20for%20Small%20Occluded%20Object%0A%20%20Detection%20and%20Tracking&entry.906535625=Shahriar%20Soudeep%20and%20Md%20Abrar%20Jahin%20and%20M.%20F.%20Mridha&entry.1292438233=%20%20The%20detection%20and%20tracking%20of%20small%2C%20occluded%20objects%20such%20as%20pedestrians%2C%0Acyclists%2C%20and%20motorbikes%20pose%20significant%20challenges%20for%20traffic%20surveillance%0Asystems%20because%20of%20their%20erratic%20movement%2C%20frequent%20occlusion%2C%20and%20poor%0Avisibility%20in%20dynamic%20urban%20environments.%20Traditional%20methods%20like%20YOLO11%2C%0Awhile%20proficient%20in%20spatial%20feature%20extraction%20for%20precise%20detection%2C%20often%0Astruggle%20with%20these%20small%20and%20dynamically%20moving%20objects%2C%20particularly%20in%0Ahandling%20real-time%20data%20updates%20and%20resource%20efficiency.%20This%20paper%20introduces%0ADGNN-YOLO%2C%20a%20novel%20framework%20that%20integrates%20dynamic%20graph%20neural%20networks%0A%28DGNNs%29%20with%20YOLO11%20to%20address%20these%20limitations.%20Unlike%20standard%20GNNs%2C%20DGNNs%0Aare%20chosen%20for%20their%20superior%20ability%20to%20dynamically%20update%20graph%20structures%20in%0Areal-time%2C%20which%20enables%20adaptive%20and%20robust%20tracking%20of%20objects%20in%20highly%0Avariable%20urban%20traffic%20scenarios.%20This%20framework%20constructs%20and%20regularly%0Aupdates%20its%20graph%20representations%2C%20capturing%20objects%20as%20nodes%20and%20their%0Ainteractions%20as%20edges%2C%20thus%20effectively%20responding%20to%20rapidly%20changing%0Aconditions.%20Additionally%2C%20DGNN-YOLO%20incorporates%20Grad-CAM%2C%20Grad-CAM%2B%2B%2C%20and%0AEigen-CAM%20visualization%20techniques%20to%20enhance%20interpretability%20and%20foster%0Atrust%2C%20offering%20insights%20into%20the%20model%27s%20decision-making%20process.%20Extensive%0Aexperiments%20validate%20the%20framework%27s%20performance%2C%20achieving%20a%20precision%20of%0A0.8382%2C%20recall%20of%200.6875%2C%20and%20mAP%400.5%3A0.95%20of%200.6476%2C%20significantly%0Aoutperforming%20existing%20methods.%20This%20study%20offers%20a%20scalable%20and%20interpretable%0Asolution%20for%20real-time%20traffic%20surveillance%20and%20significantly%20advances%0Aintelligent%20transportation%20systems%27%20capabilities%20by%20addressing%20the%20critical%0Achallenge%20of%20detecting%20and%20tracking%20small%2C%20occluded%20objects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17251v8&entry.124074799=Read"},
{"title": "SiMHand: Mining Similar Hands for Large-Scale 3D Hand Pose Pre-training", "author": "Nie Lin and Takehiko Ohkawa and Yifei Huang and Mingfang Zhang and Minjie Cai and Ming Li and Ryosuke Furuta and Yoichi Sato", "abstract": "  We present a framework for pre-training of 3D hand pose estimation from\nin-the-wild hand images sharing with similar hand characteristics, dubbed\nSimHand. Pre-training with large-scale images achieves promising results in\nvarious tasks, but prior methods for 3D hand pose pre-training have not fully\nutilized the potential of diverse hand images accessible from in-the-wild\nvideos. To facilitate scalable pre-training, we first prepare an extensive pool\nof hand images from in-the-wild videos and design our pre-training method with\ncontrastive learning. Specifically, we collect over 2.0M hand images from\nrecent human-centric videos, such as 100DOH and Ego4D. To extract\ndiscriminative information from these images, we focus on the similarity of\nhands: pairs of non-identical samples with similar hand poses. We then propose\na novel contrastive learning method that embeds similar hand pairs closer in\nthe feature space. Our method not only learns from similar samples but also\nadaptively weights the contrastive learning loss based on inter-sample\ndistance, leading to additional performance gains. Our experiments demonstrate\nthat our method outperforms conventional contrastive learning approaches that\nproduce positive pairs sorely from a single image with data augmentation. We\nachieve significant improvements over the state-of-the-art method (PeCLR) in\nvarious datasets, with gains of 15% on FreiHand, 10% on DexYCB, and 4% on\nAssemblyHands.\n  Our code is available at https://github.com/ut-vision/SiMHand.\n", "link": "http://arxiv.org/abs/2502.15251v3", "date": "2025-05-05", "relevancy": 2.8464, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5792}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.565}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5637}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SiMHand%3A%20Mining%20Similar%20Hands%20for%20Large-Scale%203D%20Hand%20Pose%20Pre-training&body=Title%3A%20SiMHand%3A%20Mining%20Similar%20Hands%20for%20Large-Scale%203D%20Hand%20Pose%20Pre-training%0AAuthor%3A%20Nie%20Lin%20and%20Takehiko%20Ohkawa%20and%20Yifei%20Huang%20and%20Mingfang%20Zhang%20and%20Minjie%20Cai%20and%20Ming%20Li%20and%20Ryosuke%20Furuta%20and%20Yoichi%20Sato%0AAbstract%3A%20%20%20We%20present%20a%20framework%20for%20pre-training%20of%203D%20hand%20pose%20estimation%20from%0Ain-the-wild%20hand%20images%20sharing%20with%20similar%20hand%20characteristics%2C%20dubbed%0ASimHand.%20Pre-training%20with%20large-scale%20images%20achieves%20promising%20results%20in%0Avarious%20tasks%2C%20but%20prior%20methods%20for%203D%20hand%20pose%20pre-training%20have%20not%20fully%0Autilized%20the%20potential%20of%20diverse%20hand%20images%20accessible%20from%20in-the-wild%0Avideos.%20To%20facilitate%20scalable%20pre-training%2C%20we%20first%20prepare%20an%20extensive%20pool%0Aof%20hand%20images%20from%20in-the-wild%20videos%20and%20design%20our%20pre-training%20method%20with%0Acontrastive%20learning.%20Specifically%2C%20we%20collect%20over%202.0M%20hand%20images%20from%0Arecent%20human-centric%20videos%2C%20such%20as%20100DOH%20and%20Ego4D.%20To%20extract%0Adiscriminative%20information%20from%20these%20images%2C%20we%20focus%20on%20the%20similarity%20of%0Ahands%3A%20pairs%20of%20non-identical%20samples%20with%20similar%20hand%20poses.%20We%20then%20propose%0Aa%20novel%20contrastive%20learning%20method%20that%20embeds%20similar%20hand%20pairs%20closer%20in%0Athe%20feature%20space.%20Our%20method%20not%20only%20learns%20from%20similar%20samples%20but%20also%0Aadaptively%20weights%20the%20contrastive%20learning%20loss%20based%20on%20inter-sample%0Adistance%2C%20leading%20to%20additional%20performance%20gains.%20Our%20experiments%20demonstrate%0Athat%20our%20method%20outperforms%20conventional%20contrastive%20learning%20approaches%20that%0Aproduce%20positive%20pairs%20sorely%20from%20a%20single%20image%20with%20data%20augmentation.%20We%0Aachieve%20significant%20improvements%20over%20the%20state-of-the-art%20method%20%28PeCLR%29%20in%0Avarious%20datasets%2C%20with%20gains%20of%2015%25%20on%20FreiHand%2C%2010%25%20on%20DexYCB%2C%20and%204%25%20on%0AAssemblyHands.%0A%20%20Our%20code%20is%20available%20at%20https%3A//github.com/ut-vision/SiMHand.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15251v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSiMHand%253A%2520Mining%2520Similar%2520Hands%2520for%2520Large-Scale%25203D%2520Hand%2520Pose%2520Pre-training%26entry.906535625%3DNie%2520Lin%2520and%2520Takehiko%2520Ohkawa%2520and%2520Yifei%2520Huang%2520and%2520Mingfang%2520Zhang%2520and%2520Minjie%2520Cai%2520and%2520Ming%2520Li%2520and%2520Ryosuke%2520Furuta%2520and%2520Yoichi%2520Sato%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520framework%2520for%2520pre-training%2520of%25203D%2520hand%2520pose%2520estimation%2520from%250Ain-the-wild%2520hand%2520images%2520sharing%2520with%2520similar%2520hand%2520characteristics%252C%2520dubbed%250ASimHand.%2520Pre-training%2520with%2520large-scale%2520images%2520achieves%2520promising%2520results%2520in%250Avarious%2520tasks%252C%2520but%2520prior%2520methods%2520for%25203D%2520hand%2520pose%2520pre-training%2520have%2520not%2520fully%250Autilized%2520the%2520potential%2520of%2520diverse%2520hand%2520images%2520accessible%2520from%2520in-the-wild%250Avideos.%2520To%2520facilitate%2520scalable%2520pre-training%252C%2520we%2520first%2520prepare%2520an%2520extensive%2520pool%250Aof%2520hand%2520images%2520from%2520in-the-wild%2520videos%2520and%2520design%2520our%2520pre-training%2520method%2520with%250Acontrastive%2520learning.%2520Specifically%252C%2520we%2520collect%2520over%25202.0M%2520hand%2520images%2520from%250Arecent%2520human-centric%2520videos%252C%2520such%2520as%2520100DOH%2520and%2520Ego4D.%2520To%2520extract%250Adiscriminative%2520information%2520from%2520these%2520images%252C%2520we%2520focus%2520on%2520the%2520similarity%2520of%250Ahands%253A%2520pairs%2520of%2520non-identical%2520samples%2520with%2520similar%2520hand%2520poses.%2520We%2520then%2520propose%250Aa%2520novel%2520contrastive%2520learning%2520method%2520that%2520embeds%2520similar%2520hand%2520pairs%2520closer%2520in%250Athe%2520feature%2520space.%2520Our%2520method%2520not%2520only%2520learns%2520from%2520similar%2520samples%2520but%2520also%250Aadaptively%2520weights%2520the%2520contrastive%2520learning%2520loss%2520based%2520on%2520inter-sample%250Adistance%252C%2520leading%2520to%2520additional%2520performance%2520gains.%2520Our%2520experiments%2520demonstrate%250Athat%2520our%2520method%2520outperforms%2520conventional%2520contrastive%2520learning%2520approaches%2520that%250Aproduce%2520positive%2520pairs%2520sorely%2520from%2520a%2520single%2520image%2520with%2520data%2520augmentation.%2520We%250Aachieve%2520significant%2520improvements%2520over%2520the%2520state-of-the-art%2520method%2520%2528PeCLR%2529%2520in%250Avarious%2520datasets%252C%2520with%2520gains%2520of%252015%2525%2520on%2520FreiHand%252C%252010%2525%2520on%2520DexYCB%252C%2520and%25204%2525%2520on%250AAssemblyHands.%250A%2520%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/ut-vision/SiMHand.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15251v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SiMHand%3A%20Mining%20Similar%20Hands%20for%20Large-Scale%203D%20Hand%20Pose%20Pre-training&entry.906535625=Nie%20Lin%20and%20Takehiko%20Ohkawa%20and%20Yifei%20Huang%20and%20Mingfang%20Zhang%20and%20Minjie%20Cai%20and%20Ming%20Li%20and%20Ryosuke%20Furuta%20and%20Yoichi%20Sato&entry.1292438233=%20%20We%20present%20a%20framework%20for%20pre-training%20of%203D%20hand%20pose%20estimation%20from%0Ain-the-wild%20hand%20images%20sharing%20with%20similar%20hand%20characteristics%2C%20dubbed%0ASimHand.%20Pre-training%20with%20large-scale%20images%20achieves%20promising%20results%20in%0Avarious%20tasks%2C%20but%20prior%20methods%20for%203D%20hand%20pose%20pre-training%20have%20not%20fully%0Autilized%20the%20potential%20of%20diverse%20hand%20images%20accessible%20from%20in-the-wild%0Avideos.%20To%20facilitate%20scalable%20pre-training%2C%20we%20first%20prepare%20an%20extensive%20pool%0Aof%20hand%20images%20from%20in-the-wild%20videos%20and%20design%20our%20pre-training%20method%20with%0Acontrastive%20learning.%20Specifically%2C%20we%20collect%20over%202.0M%20hand%20images%20from%0Arecent%20human-centric%20videos%2C%20such%20as%20100DOH%20and%20Ego4D.%20To%20extract%0Adiscriminative%20information%20from%20these%20images%2C%20we%20focus%20on%20the%20similarity%20of%0Ahands%3A%20pairs%20of%20non-identical%20samples%20with%20similar%20hand%20poses.%20We%20then%20propose%0Aa%20novel%20contrastive%20learning%20method%20that%20embeds%20similar%20hand%20pairs%20closer%20in%0Athe%20feature%20space.%20Our%20method%20not%20only%20learns%20from%20similar%20samples%20but%20also%0Aadaptively%20weights%20the%20contrastive%20learning%20loss%20based%20on%20inter-sample%0Adistance%2C%20leading%20to%20additional%20performance%20gains.%20Our%20experiments%20demonstrate%0Athat%20our%20method%20outperforms%20conventional%20contrastive%20learning%20approaches%20that%0Aproduce%20positive%20pairs%20sorely%20from%20a%20single%20image%20with%20data%20augmentation.%20We%0Aachieve%20significant%20improvements%20over%20the%20state-of-the-art%20method%20%28PeCLR%29%20in%0Avarious%20datasets%2C%20with%20gains%20of%2015%25%20on%20FreiHand%2C%2010%25%20on%20DexYCB%2C%20and%204%25%20on%0AAssemblyHands.%0A%20%20Our%20code%20is%20available%20at%20https%3A//github.com/ut-vision/SiMHand.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15251v3&entry.124074799=Read"},
{"title": "DeepSparse: A Foundation Model for Sparse-View CBCT Reconstruction", "author": "Yiqun Lin and Hualiang Wang and Jixiang Chen and Jiewen Yang and Jiarong Guo and Xiaomeng Li", "abstract": "  Cone-beam computed tomography (CBCT) is a critical 3D imaging technology in\nthe medical field, while the high radiation exposure required for high-quality\nimaging raises significant concerns, particularly for vulnerable populations.\nSparse-view reconstruction reduces radiation by using fewer X-ray projections\nwhile maintaining image quality, yet existing methods face challenges such as\nhigh computational demands and poor generalizability to different datasets. To\novercome these limitations, we propose DeepSparse, the first foundation model\nfor sparse-view CBCT reconstruction, featuring DiCE (Dual-Dimensional\nCross-Scale Embedding), a novel network that integrates multi-view 2D features\nand multi-scale 3D features. Additionally, we introduce the HyViP (Hybrid View\nSampling Pretraining) framework, which pretrains the model on large datasets\nwith both sparse-view and dense-view projections, and a two-step finetuning\nstrategy to adapt and refine the model for new datasets. Extensive experiments\nand ablation studies demonstrate that our proposed DeepSparse achieves superior\nreconstruction quality compared to state-of-the-art methods, paving the way for\nsafer and more efficient CBCT imaging.\n", "link": "http://arxiv.org/abs/2505.02628v1", "date": "2025-05-05", "relevancy": 2.8406, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5736}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5736}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepSparse%3A%20A%20Foundation%20Model%20for%20Sparse-View%20CBCT%20Reconstruction&body=Title%3A%20DeepSparse%3A%20A%20Foundation%20Model%20for%20Sparse-View%20CBCT%20Reconstruction%0AAuthor%3A%20Yiqun%20Lin%20and%20Hualiang%20Wang%20and%20Jixiang%20Chen%20and%20Jiewen%20Yang%20and%20Jiarong%20Guo%20and%20Xiaomeng%20Li%0AAbstract%3A%20%20%20Cone-beam%20computed%20tomography%20%28CBCT%29%20is%20a%20critical%203D%20imaging%20technology%20in%0Athe%20medical%20field%2C%20while%20the%20high%20radiation%20exposure%20required%20for%20high-quality%0Aimaging%20raises%20significant%20concerns%2C%20particularly%20for%20vulnerable%20populations.%0ASparse-view%20reconstruction%20reduces%20radiation%20by%20using%20fewer%20X-ray%20projections%0Awhile%20maintaining%20image%20quality%2C%20yet%20existing%20methods%20face%20challenges%20such%20as%0Ahigh%20computational%20demands%20and%20poor%20generalizability%20to%20different%20datasets.%20To%0Aovercome%20these%20limitations%2C%20we%20propose%20DeepSparse%2C%20the%20first%20foundation%20model%0Afor%20sparse-view%20CBCT%20reconstruction%2C%20featuring%20DiCE%20%28Dual-Dimensional%0ACross-Scale%20Embedding%29%2C%20a%20novel%20network%20that%20integrates%20multi-view%202D%20features%0Aand%20multi-scale%203D%20features.%20Additionally%2C%20we%20introduce%20the%20HyViP%20%28Hybrid%20View%0ASampling%20Pretraining%29%20framework%2C%20which%20pretrains%20the%20model%20on%20large%20datasets%0Awith%20both%20sparse-view%20and%20dense-view%20projections%2C%20and%20a%20two-step%20finetuning%0Astrategy%20to%20adapt%20and%20refine%20the%20model%20for%20new%20datasets.%20Extensive%20experiments%0Aand%20ablation%20studies%20demonstrate%20that%20our%20proposed%20DeepSparse%20achieves%20superior%0Areconstruction%20quality%20compared%20to%20state-of-the-art%20methods%2C%20paving%20the%20way%20for%0Asafer%20and%20more%20efficient%20CBCT%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02628v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepSparse%253A%2520A%2520Foundation%2520Model%2520for%2520Sparse-View%2520CBCT%2520Reconstruction%26entry.906535625%3DYiqun%2520Lin%2520and%2520Hualiang%2520Wang%2520and%2520Jixiang%2520Chen%2520and%2520Jiewen%2520Yang%2520and%2520Jiarong%2520Guo%2520and%2520Xiaomeng%2520Li%26entry.1292438233%3D%2520%2520Cone-beam%2520computed%2520tomography%2520%2528CBCT%2529%2520is%2520a%2520critical%25203D%2520imaging%2520technology%2520in%250Athe%2520medical%2520field%252C%2520while%2520the%2520high%2520radiation%2520exposure%2520required%2520for%2520high-quality%250Aimaging%2520raises%2520significant%2520concerns%252C%2520particularly%2520for%2520vulnerable%2520populations.%250ASparse-view%2520reconstruction%2520reduces%2520radiation%2520by%2520using%2520fewer%2520X-ray%2520projections%250Awhile%2520maintaining%2520image%2520quality%252C%2520yet%2520existing%2520methods%2520face%2520challenges%2520such%2520as%250Ahigh%2520computational%2520demands%2520and%2520poor%2520generalizability%2520to%2520different%2520datasets.%2520To%250Aovercome%2520these%2520limitations%252C%2520we%2520propose%2520DeepSparse%252C%2520the%2520first%2520foundation%2520model%250Afor%2520sparse-view%2520CBCT%2520reconstruction%252C%2520featuring%2520DiCE%2520%2528Dual-Dimensional%250ACross-Scale%2520Embedding%2529%252C%2520a%2520novel%2520network%2520that%2520integrates%2520multi-view%25202D%2520features%250Aand%2520multi-scale%25203D%2520features.%2520Additionally%252C%2520we%2520introduce%2520the%2520HyViP%2520%2528Hybrid%2520View%250ASampling%2520Pretraining%2529%2520framework%252C%2520which%2520pretrains%2520the%2520model%2520on%2520large%2520datasets%250Awith%2520both%2520sparse-view%2520and%2520dense-view%2520projections%252C%2520and%2520a%2520two-step%2520finetuning%250Astrategy%2520to%2520adapt%2520and%2520refine%2520the%2520model%2520for%2520new%2520datasets.%2520Extensive%2520experiments%250Aand%2520ablation%2520studies%2520demonstrate%2520that%2520our%2520proposed%2520DeepSparse%2520achieves%2520superior%250Areconstruction%2520quality%2520compared%2520to%2520state-of-the-art%2520methods%252C%2520paving%2520the%2520way%2520for%250Asafer%2520and%2520more%2520efficient%2520CBCT%2520imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02628v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepSparse%3A%20A%20Foundation%20Model%20for%20Sparse-View%20CBCT%20Reconstruction&entry.906535625=Yiqun%20Lin%20and%20Hualiang%20Wang%20and%20Jixiang%20Chen%20and%20Jiewen%20Yang%20and%20Jiarong%20Guo%20and%20Xiaomeng%20Li&entry.1292438233=%20%20Cone-beam%20computed%20tomography%20%28CBCT%29%20is%20a%20critical%203D%20imaging%20technology%20in%0Athe%20medical%20field%2C%20while%20the%20high%20radiation%20exposure%20required%20for%20high-quality%0Aimaging%20raises%20significant%20concerns%2C%20particularly%20for%20vulnerable%20populations.%0ASparse-view%20reconstruction%20reduces%20radiation%20by%20using%20fewer%20X-ray%20projections%0Awhile%20maintaining%20image%20quality%2C%20yet%20existing%20methods%20face%20challenges%20such%20as%0Ahigh%20computational%20demands%20and%20poor%20generalizability%20to%20different%20datasets.%20To%0Aovercome%20these%20limitations%2C%20we%20propose%20DeepSparse%2C%20the%20first%20foundation%20model%0Afor%20sparse-view%20CBCT%20reconstruction%2C%20featuring%20DiCE%20%28Dual-Dimensional%0ACross-Scale%20Embedding%29%2C%20a%20novel%20network%20that%20integrates%20multi-view%202D%20features%0Aand%20multi-scale%203D%20features.%20Additionally%2C%20we%20introduce%20the%20HyViP%20%28Hybrid%20View%0ASampling%20Pretraining%29%20framework%2C%20which%20pretrains%20the%20model%20on%20large%20datasets%0Awith%20both%20sparse-view%20and%20dense-view%20projections%2C%20and%20a%20two-step%20finetuning%0Astrategy%20to%20adapt%20and%20refine%20the%20model%20for%20new%20datasets.%20Extensive%20experiments%0Aand%20ablation%20studies%20demonstrate%20that%20our%20proposed%20DeepSparse%20achieves%20superior%0Areconstruction%20quality%20compared%20to%20state-of-the-art%20methods%2C%20paving%20the%20way%20for%0Asafer%20and%20more%20efficient%20CBCT%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02628v1&entry.124074799=Read"},
{"title": "CaRe-Ego: Contact-aware Relationship Modeling for Egocentric Interactive\n  Hand-object Segmentation", "author": "Yuejiao Su and Yi Wang and Lap-Pui Chau", "abstract": "  Egocentric Interactive hand-object segmentation (EgoIHOS) requires the\nsegmentation of hands and interacting objects in egocentric images, which is\ncrucial for understanding human behavior in assistive systems. Previous methods\ntypically recognize hands and interacting objects as distinct semantic\ncategories based solely on visual features, or simply use hand predictions as\nauxiliary cues for object segmentation. Despite the promising progress achieved\nby these methods, they fail to adequately model the interactive relationships\nbetween hands and objects while ignoring the coupled physical relationships\namong object categories, ultimately constraining their segmentation\nperformance. To make up for the shortcomings of existing methods, we propose a\nnovel method called CaRe-Ego that achieves state-of-the-art performance by\nemphasizing the contact between hands and objects from two aspects. First, we\nintroduce a Hand-guided Object Feature Enhancer (HOFE) to establish the\nhand-object interactive relationships to extract more contact-relevant and\ndiscriminative object features. Second, we design the Contact-centric Object\nDecoupling Strategy (CODS) to explicitly model and disentangle coupling\nrelationships among object categories, thereby emphasizing contact-aware\nfeature learning. Experiments on various in-domain and out-of-domain test sets\nshow that Care-Ego significantly outperforms existing methods with robust\ngeneralization capability. Codes are publicly available at\nhttps://github.com/yuggiehk/CaRe-Ego/.\n", "link": "http://arxiv.org/abs/2407.05576v3", "date": "2025-05-05", "relevancy": 2.8385, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5841}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5691}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CaRe-Ego%3A%20Contact-aware%20Relationship%20Modeling%20for%20Egocentric%20Interactive%0A%20%20Hand-object%20Segmentation&body=Title%3A%20CaRe-Ego%3A%20Contact-aware%20Relationship%20Modeling%20for%20Egocentric%20Interactive%0A%20%20Hand-object%20Segmentation%0AAuthor%3A%20Yuejiao%20Su%20and%20Yi%20Wang%20and%20Lap-Pui%20Chau%0AAbstract%3A%20%20%20Egocentric%20Interactive%20hand-object%20segmentation%20%28EgoIHOS%29%20requires%20the%0Asegmentation%20of%20hands%20and%20interacting%20objects%20in%20egocentric%20images%2C%20which%20is%0Acrucial%20for%20understanding%20human%20behavior%20in%20assistive%20systems.%20Previous%20methods%0Atypically%20recognize%20hands%20and%20interacting%20objects%20as%20distinct%20semantic%0Acategories%20based%20solely%20on%20visual%20features%2C%20or%20simply%20use%20hand%20predictions%20as%0Aauxiliary%20cues%20for%20object%20segmentation.%20Despite%20the%20promising%20progress%20achieved%0Aby%20these%20methods%2C%20they%20fail%20to%20adequately%20model%20the%20interactive%20relationships%0Abetween%20hands%20and%20objects%20while%20ignoring%20the%20coupled%20physical%20relationships%0Aamong%20object%20categories%2C%20ultimately%20constraining%20their%20segmentation%0Aperformance.%20To%20make%20up%20for%20the%20shortcomings%20of%20existing%20methods%2C%20we%20propose%20a%0Anovel%20method%20called%20CaRe-Ego%20that%20achieves%20state-of-the-art%20performance%20by%0Aemphasizing%20the%20contact%20between%20hands%20and%20objects%20from%20two%20aspects.%20First%2C%20we%0Aintroduce%20a%20Hand-guided%20Object%20Feature%20Enhancer%20%28HOFE%29%20to%20establish%20the%0Ahand-object%20interactive%20relationships%20to%20extract%20more%20contact-relevant%20and%0Adiscriminative%20object%20features.%20Second%2C%20we%20design%20the%20Contact-centric%20Object%0ADecoupling%20Strategy%20%28CODS%29%20to%20explicitly%20model%20and%20disentangle%20coupling%0Arelationships%20among%20object%20categories%2C%20thereby%20emphasizing%20contact-aware%0Afeature%20learning.%20Experiments%20on%20various%20in-domain%20and%20out-of-domain%20test%20sets%0Ashow%20that%20Care-Ego%20significantly%20outperforms%20existing%20methods%20with%20robust%0Ageneralization%20capability.%20Codes%20are%20publicly%20available%20at%0Ahttps%3A//github.com/yuggiehk/CaRe-Ego/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05576v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCaRe-Ego%253A%2520Contact-aware%2520Relationship%2520Modeling%2520for%2520Egocentric%2520Interactive%250A%2520%2520Hand-object%2520Segmentation%26entry.906535625%3DYuejiao%2520Su%2520and%2520Yi%2520Wang%2520and%2520Lap-Pui%2520Chau%26entry.1292438233%3D%2520%2520Egocentric%2520Interactive%2520hand-object%2520segmentation%2520%2528EgoIHOS%2529%2520requires%2520the%250Asegmentation%2520of%2520hands%2520and%2520interacting%2520objects%2520in%2520egocentric%2520images%252C%2520which%2520is%250Acrucial%2520for%2520understanding%2520human%2520behavior%2520in%2520assistive%2520systems.%2520Previous%2520methods%250Atypically%2520recognize%2520hands%2520and%2520interacting%2520objects%2520as%2520distinct%2520semantic%250Acategories%2520based%2520solely%2520on%2520visual%2520features%252C%2520or%2520simply%2520use%2520hand%2520predictions%2520as%250Aauxiliary%2520cues%2520for%2520object%2520segmentation.%2520Despite%2520the%2520promising%2520progress%2520achieved%250Aby%2520these%2520methods%252C%2520they%2520fail%2520to%2520adequately%2520model%2520the%2520interactive%2520relationships%250Abetween%2520hands%2520and%2520objects%2520while%2520ignoring%2520the%2520coupled%2520physical%2520relationships%250Aamong%2520object%2520categories%252C%2520ultimately%2520constraining%2520their%2520segmentation%250Aperformance.%2520To%2520make%2520up%2520for%2520the%2520shortcomings%2520of%2520existing%2520methods%252C%2520we%2520propose%2520a%250Anovel%2520method%2520called%2520CaRe-Ego%2520that%2520achieves%2520state-of-the-art%2520performance%2520by%250Aemphasizing%2520the%2520contact%2520between%2520hands%2520and%2520objects%2520from%2520two%2520aspects.%2520First%252C%2520we%250Aintroduce%2520a%2520Hand-guided%2520Object%2520Feature%2520Enhancer%2520%2528HOFE%2529%2520to%2520establish%2520the%250Ahand-object%2520interactive%2520relationships%2520to%2520extract%2520more%2520contact-relevant%2520and%250Adiscriminative%2520object%2520features.%2520Second%252C%2520we%2520design%2520the%2520Contact-centric%2520Object%250ADecoupling%2520Strategy%2520%2528CODS%2529%2520to%2520explicitly%2520model%2520and%2520disentangle%2520coupling%250Arelationships%2520among%2520object%2520categories%252C%2520thereby%2520emphasizing%2520contact-aware%250Afeature%2520learning.%2520Experiments%2520on%2520various%2520in-domain%2520and%2520out-of-domain%2520test%2520sets%250Ashow%2520that%2520Care-Ego%2520significantly%2520outperforms%2520existing%2520methods%2520with%2520robust%250Ageneralization%2520capability.%2520Codes%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/yuggiehk/CaRe-Ego/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05576v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CaRe-Ego%3A%20Contact-aware%20Relationship%20Modeling%20for%20Egocentric%20Interactive%0A%20%20Hand-object%20Segmentation&entry.906535625=Yuejiao%20Su%20and%20Yi%20Wang%20and%20Lap-Pui%20Chau&entry.1292438233=%20%20Egocentric%20Interactive%20hand-object%20segmentation%20%28EgoIHOS%29%20requires%20the%0Asegmentation%20of%20hands%20and%20interacting%20objects%20in%20egocentric%20images%2C%20which%20is%0Acrucial%20for%20understanding%20human%20behavior%20in%20assistive%20systems.%20Previous%20methods%0Atypically%20recognize%20hands%20and%20interacting%20objects%20as%20distinct%20semantic%0Acategories%20based%20solely%20on%20visual%20features%2C%20or%20simply%20use%20hand%20predictions%20as%0Aauxiliary%20cues%20for%20object%20segmentation.%20Despite%20the%20promising%20progress%20achieved%0Aby%20these%20methods%2C%20they%20fail%20to%20adequately%20model%20the%20interactive%20relationships%0Abetween%20hands%20and%20objects%20while%20ignoring%20the%20coupled%20physical%20relationships%0Aamong%20object%20categories%2C%20ultimately%20constraining%20their%20segmentation%0Aperformance.%20To%20make%20up%20for%20the%20shortcomings%20of%20existing%20methods%2C%20we%20propose%20a%0Anovel%20method%20called%20CaRe-Ego%20that%20achieves%20state-of-the-art%20performance%20by%0Aemphasizing%20the%20contact%20between%20hands%20and%20objects%20from%20two%20aspects.%20First%2C%20we%0Aintroduce%20a%20Hand-guided%20Object%20Feature%20Enhancer%20%28HOFE%29%20to%20establish%20the%0Ahand-object%20interactive%20relationships%20to%20extract%20more%20contact-relevant%20and%0Adiscriminative%20object%20features.%20Second%2C%20we%20design%20the%20Contact-centric%20Object%0ADecoupling%20Strategy%20%28CODS%29%20to%20explicitly%20model%20and%20disentangle%20coupling%0Arelationships%20among%20object%20categories%2C%20thereby%20emphasizing%20contact-aware%0Afeature%20learning.%20Experiments%20on%20various%20in-domain%20and%20out-of-domain%20test%20sets%0Ashow%20that%20Care-Ego%20significantly%20outperforms%20existing%20methods%20with%20robust%0Ageneralization%20capability.%20Codes%20are%20publicly%20available%20at%0Ahttps%3A//github.com/yuggiehk/CaRe-Ego/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05576v3&entry.124074799=Read"},
{"title": "LISAT: Language-Instructed Segmentation Assistant for Satellite Imagery", "author": "Jerome Quenum and Wen-Han Hsieh and Tsung-Han Wu and Ritwik Gupta and Trevor Darrell and David M. Chan", "abstract": "  Segmentation models can recognize a pre-defined set of objects in images.\nHowever, models that can reason over complex user queries that implicitly refer\nto multiple objects of interest are still in their infancy. Recent advances in\nreasoning segmentation--generating segmentation masks from complex, implicit\nquery text--demonstrate that vision-language models can operate across an open\ndomain and produce reasonable outputs. However, our experiments show that such\nmodels struggle with complex remote-sensing imagery. In this work, we introduce\nLISAt, a vision-language model designed to describe complex remote-sensing\nscenes, answer questions about them, and segment objects of interest. We\ntrained LISAt on a new curated geospatial reasoning-segmentation dataset, GRES,\nwith 27,615 annotations over 9,205 images, and a multimodal pretraining\ndataset, PreGRES, containing over 1 million question-answer pairs. LISAt\noutperforms existing geospatial foundation models such as RS-GPT4V by over\n10.04 % (BLEU-4) on remote-sensing description tasks, and surpasses\nstate-of-the-art open-domain models on reasoning segmentation tasks by 143.36 %\n(gIoU). Our model, datasets, and code are available at\nhttps://lisat-bair.github.io/LISAt/\n", "link": "http://arxiv.org/abs/2505.02829v1", "date": "2025-05-05", "relevancy": 2.8154, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5842}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5842}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5209}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LISAT%3A%20Language-Instructed%20Segmentation%20Assistant%20for%20Satellite%20Imagery&body=Title%3A%20LISAT%3A%20Language-Instructed%20Segmentation%20Assistant%20for%20Satellite%20Imagery%0AAuthor%3A%20Jerome%20Quenum%20and%20Wen-Han%20Hsieh%20and%20Tsung-Han%20Wu%20and%20Ritwik%20Gupta%20and%20Trevor%20Darrell%20and%20David%20M.%20Chan%0AAbstract%3A%20%20%20Segmentation%20models%20can%20recognize%20a%20pre-defined%20set%20of%20objects%20in%20images.%0AHowever%2C%20models%20that%20can%20reason%20over%20complex%20user%20queries%20that%20implicitly%20refer%0Ato%20multiple%20objects%20of%20interest%20are%20still%20in%20their%20infancy.%20Recent%20advances%20in%0Areasoning%20segmentation--generating%20segmentation%20masks%20from%20complex%2C%20implicit%0Aquery%20text--demonstrate%20that%20vision-language%20models%20can%20operate%20across%20an%20open%0Adomain%20and%20produce%20reasonable%20outputs.%20However%2C%20our%20experiments%20show%20that%20such%0Amodels%20struggle%20with%20complex%20remote-sensing%20imagery.%20In%20this%20work%2C%20we%20introduce%0ALISAt%2C%20a%20vision-language%20model%20designed%20to%20describe%20complex%20remote-sensing%0Ascenes%2C%20answer%20questions%20about%20them%2C%20and%20segment%20objects%20of%20interest.%20We%0Atrained%20LISAt%20on%20a%20new%20curated%20geospatial%20reasoning-segmentation%20dataset%2C%20GRES%2C%0Awith%2027%2C615%20annotations%20over%209%2C205%20images%2C%20and%20a%20multimodal%20pretraining%0Adataset%2C%20PreGRES%2C%20containing%20over%201%20million%20question-answer%20pairs.%20LISAt%0Aoutperforms%20existing%20geospatial%20foundation%20models%20such%20as%20RS-GPT4V%20by%20over%0A10.04%20%25%20%28BLEU-4%29%20on%20remote-sensing%20description%20tasks%2C%20and%20surpasses%0Astate-of-the-art%20open-domain%20models%20on%20reasoning%20segmentation%20tasks%20by%20143.36%20%25%0A%28gIoU%29.%20Our%20model%2C%20datasets%2C%20and%20code%20are%20available%20at%0Ahttps%3A//lisat-bair.github.io/LISAt/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02829v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLISAT%253A%2520Language-Instructed%2520Segmentation%2520Assistant%2520for%2520Satellite%2520Imagery%26entry.906535625%3DJerome%2520Quenum%2520and%2520Wen-Han%2520Hsieh%2520and%2520Tsung-Han%2520Wu%2520and%2520Ritwik%2520Gupta%2520and%2520Trevor%2520Darrell%2520and%2520David%2520M.%2520Chan%26entry.1292438233%3D%2520%2520Segmentation%2520models%2520can%2520recognize%2520a%2520pre-defined%2520set%2520of%2520objects%2520in%2520images.%250AHowever%252C%2520models%2520that%2520can%2520reason%2520over%2520complex%2520user%2520queries%2520that%2520implicitly%2520refer%250Ato%2520multiple%2520objects%2520of%2520interest%2520are%2520still%2520in%2520their%2520infancy.%2520Recent%2520advances%2520in%250Areasoning%2520segmentation--generating%2520segmentation%2520masks%2520from%2520complex%252C%2520implicit%250Aquery%2520text--demonstrate%2520that%2520vision-language%2520models%2520can%2520operate%2520across%2520an%2520open%250Adomain%2520and%2520produce%2520reasonable%2520outputs.%2520However%252C%2520our%2520experiments%2520show%2520that%2520such%250Amodels%2520struggle%2520with%2520complex%2520remote-sensing%2520imagery.%2520In%2520this%2520work%252C%2520we%2520introduce%250ALISAt%252C%2520a%2520vision-language%2520model%2520designed%2520to%2520describe%2520complex%2520remote-sensing%250Ascenes%252C%2520answer%2520questions%2520about%2520them%252C%2520and%2520segment%2520objects%2520of%2520interest.%2520We%250Atrained%2520LISAt%2520on%2520a%2520new%2520curated%2520geospatial%2520reasoning-segmentation%2520dataset%252C%2520GRES%252C%250Awith%252027%252C615%2520annotations%2520over%25209%252C205%2520images%252C%2520and%2520a%2520multimodal%2520pretraining%250Adataset%252C%2520PreGRES%252C%2520containing%2520over%25201%2520million%2520question-answer%2520pairs.%2520LISAt%250Aoutperforms%2520existing%2520geospatial%2520foundation%2520models%2520such%2520as%2520RS-GPT4V%2520by%2520over%250A10.04%2520%2525%2520%2528BLEU-4%2529%2520on%2520remote-sensing%2520description%2520tasks%252C%2520and%2520surpasses%250Astate-of-the-art%2520open-domain%2520models%2520on%2520reasoning%2520segmentation%2520tasks%2520by%2520143.36%2520%2525%250A%2528gIoU%2529.%2520Our%2520model%252C%2520datasets%252C%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//lisat-bair.github.io/LISAt/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02829v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LISAT%3A%20Language-Instructed%20Segmentation%20Assistant%20for%20Satellite%20Imagery&entry.906535625=Jerome%20Quenum%20and%20Wen-Han%20Hsieh%20and%20Tsung-Han%20Wu%20and%20Ritwik%20Gupta%20and%20Trevor%20Darrell%20and%20David%20M.%20Chan&entry.1292438233=%20%20Segmentation%20models%20can%20recognize%20a%20pre-defined%20set%20of%20objects%20in%20images.%0AHowever%2C%20models%20that%20can%20reason%20over%20complex%20user%20queries%20that%20implicitly%20refer%0Ato%20multiple%20objects%20of%20interest%20are%20still%20in%20their%20infancy.%20Recent%20advances%20in%0Areasoning%20segmentation--generating%20segmentation%20masks%20from%20complex%2C%20implicit%0Aquery%20text--demonstrate%20that%20vision-language%20models%20can%20operate%20across%20an%20open%0Adomain%20and%20produce%20reasonable%20outputs.%20However%2C%20our%20experiments%20show%20that%20such%0Amodels%20struggle%20with%20complex%20remote-sensing%20imagery.%20In%20this%20work%2C%20we%20introduce%0ALISAt%2C%20a%20vision-language%20model%20designed%20to%20describe%20complex%20remote-sensing%0Ascenes%2C%20answer%20questions%20about%20them%2C%20and%20segment%20objects%20of%20interest.%20We%0Atrained%20LISAt%20on%20a%20new%20curated%20geospatial%20reasoning-segmentation%20dataset%2C%20GRES%2C%0Awith%2027%2C615%20annotations%20over%209%2C205%20images%2C%20and%20a%20multimodal%20pretraining%0Adataset%2C%20PreGRES%2C%20containing%20over%201%20million%20question-answer%20pairs.%20LISAt%0Aoutperforms%20existing%20geospatial%20foundation%20models%20such%20as%20RS-GPT4V%20by%20over%0A10.04%20%25%20%28BLEU-4%29%20on%20remote-sensing%20description%20tasks%2C%20and%20surpasses%0Astate-of-the-art%20open-domain%20models%20on%20reasoning%20segmentation%20tasks%20by%20143.36%20%25%0A%28gIoU%29.%20Our%20model%2C%20datasets%2C%20and%20code%20are%20available%20at%0Ahttps%3A//lisat-bair.github.io/LISAt/%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02829v1&entry.124074799=Read"},
{"title": "Sim2Real in endoscopy segmentation with a novel structure aware image\n  translation", "author": "Clara Tomasini and Luis Riazuelo and Ana C. Murillo", "abstract": "  Automatic segmentation of anatomical landmarks in endoscopic images can\nprovide assistance to doctors and surgeons for diagnosis, treatments or medical\ntraining. However, obtaining the annotations required to train commonly used\nsupervised learning methods is a tedious and difficult task, in particular for\nreal images. While ground truth annotations are easier to obtain for synthetic\ndata, models trained on such data often do not generalize well to real data.\nGenerative approaches can add realistic texture to it, but face difficulties to\nmaintain the structure of the original scene. The main contribution in this\nwork is a novel image translation model that adds realistic texture to\nsimulated endoscopic images while keeping the key scene layout information. Our\napproach produces realistic images in different endoscopy scenarios. We\ndemonstrate these images can effectively be used to successfully train a model\nfor a challenging end task without any real labeled data. In particular, we\ndemonstrate our approach for the task of fold segmentation in colonoscopy\nimages. Folds are key anatomical landmarks that can occlude parts of the colon\nmucosa and possible polyps. Our approach generates realistic images maintaining\nthe shape and location of the original folds, after the\nimage-style-translation, better than existing methods. We run experiments both\non a novel simulated dataset for fold segmentation, and real data from the\nEndoMapper (EM) dataset. All our new generated data and new EM metadata is\nbeing released to facilitate further research, as no public benchmark is\ncurrently available for the task of fold segmentation.\n", "link": "http://arxiv.org/abs/2505.02654v1", "date": "2025-05-05", "relevancy": 2.7782, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5613}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5528}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sim2Real%20in%20endoscopy%20segmentation%20with%20a%20novel%20structure%20aware%20image%0A%20%20translation&body=Title%3A%20Sim2Real%20in%20endoscopy%20segmentation%20with%20a%20novel%20structure%20aware%20image%0A%20%20translation%0AAuthor%3A%20Clara%20Tomasini%20and%20Luis%20Riazuelo%20and%20Ana%20C.%20Murillo%0AAbstract%3A%20%20%20Automatic%20segmentation%20of%20anatomical%20landmarks%20in%20endoscopic%20images%20can%0Aprovide%20assistance%20to%20doctors%20and%20surgeons%20for%20diagnosis%2C%20treatments%20or%20medical%0Atraining.%20However%2C%20obtaining%20the%20annotations%20required%20to%20train%20commonly%20used%0Asupervised%20learning%20methods%20is%20a%20tedious%20and%20difficult%20task%2C%20in%20particular%20for%0Areal%20images.%20While%20ground%20truth%20annotations%20are%20easier%20to%20obtain%20for%20synthetic%0Adata%2C%20models%20trained%20on%20such%20data%20often%20do%20not%20generalize%20well%20to%20real%20data.%0AGenerative%20approaches%20can%20add%20realistic%20texture%20to%20it%2C%20but%20face%20difficulties%20to%0Amaintain%20the%20structure%20of%20the%20original%20scene.%20The%20main%20contribution%20in%20this%0Awork%20is%20a%20novel%20image%20translation%20model%20that%20adds%20realistic%20texture%20to%0Asimulated%20endoscopic%20images%20while%20keeping%20the%20key%20scene%20layout%20information.%20Our%0Aapproach%20produces%20realistic%20images%20in%20different%20endoscopy%20scenarios.%20We%0Ademonstrate%20these%20images%20can%20effectively%20be%20used%20to%20successfully%20train%20a%20model%0Afor%20a%20challenging%20end%20task%20without%20any%20real%20labeled%20data.%20In%20particular%2C%20we%0Ademonstrate%20our%20approach%20for%20the%20task%20of%20fold%20segmentation%20in%20colonoscopy%0Aimages.%20Folds%20are%20key%20anatomical%20landmarks%20that%20can%20occlude%20parts%20of%20the%20colon%0Amucosa%20and%20possible%20polyps.%20Our%20approach%20generates%20realistic%20images%20maintaining%0Athe%20shape%20and%20location%20of%20the%20original%20folds%2C%20after%20the%0Aimage-style-translation%2C%20better%20than%20existing%20methods.%20We%20run%20experiments%20both%0Aon%20a%20novel%20simulated%20dataset%20for%20fold%20segmentation%2C%20and%20real%20data%20from%20the%0AEndoMapper%20%28EM%29%20dataset.%20All%20our%20new%20generated%20data%20and%20new%20EM%20metadata%20is%0Abeing%20released%20to%20facilitate%20further%20research%2C%20as%20no%20public%20benchmark%20is%0Acurrently%20available%20for%20the%20task%20of%20fold%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02654v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSim2Real%2520in%2520endoscopy%2520segmentation%2520with%2520a%2520novel%2520structure%2520aware%2520image%250A%2520%2520translation%26entry.906535625%3DClara%2520Tomasini%2520and%2520Luis%2520Riazuelo%2520and%2520Ana%2520C.%2520Murillo%26entry.1292438233%3D%2520%2520Automatic%2520segmentation%2520of%2520anatomical%2520landmarks%2520in%2520endoscopic%2520images%2520can%250Aprovide%2520assistance%2520to%2520doctors%2520and%2520surgeons%2520for%2520diagnosis%252C%2520treatments%2520or%2520medical%250Atraining.%2520However%252C%2520obtaining%2520the%2520annotations%2520required%2520to%2520train%2520commonly%2520used%250Asupervised%2520learning%2520methods%2520is%2520a%2520tedious%2520and%2520difficult%2520task%252C%2520in%2520particular%2520for%250Areal%2520images.%2520While%2520ground%2520truth%2520annotations%2520are%2520easier%2520to%2520obtain%2520for%2520synthetic%250Adata%252C%2520models%2520trained%2520on%2520such%2520data%2520often%2520do%2520not%2520generalize%2520well%2520to%2520real%2520data.%250AGenerative%2520approaches%2520can%2520add%2520realistic%2520texture%2520to%2520it%252C%2520but%2520face%2520difficulties%2520to%250Amaintain%2520the%2520structure%2520of%2520the%2520original%2520scene.%2520The%2520main%2520contribution%2520in%2520this%250Awork%2520is%2520a%2520novel%2520image%2520translation%2520model%2520that%2520adds%2520realistic%2520texture%2520to%250Asimulated%2520endoscopic%2520images%2520while%2520keeping%2520the%2520key%2520scene%2520layout%2520information.%2520Our%250Aapproach%2520produces%2520realistic%2520images%2520in%2520different%2520endoscopy%2520scenarios.%2520We%250Ademonstrate%2520these%2520images%2520can%2520effectively%2520be%2520used%2520to%2520successfully%2520train%2520a%2520model%250Afor%2520a%2520challenging%2520end%2520task%2520without%2520any%2520real%2520labeled%2520data.%2520In%2520particular%252C%2520we%250Ademonstrate%2520our%2520approach%2520for%2520the%2520task%2520of%2520fold%2520segmentation%2520in%2520colonoscopy%250Aimages.%2520Folds%2520are%2520key%2520anatomical%2520landmarks%2520that%2520can%2520occlude%2520parts%2520of%2520the%2520colon%250Amucosa%2520and%2520possible%2520polyps.%2520Our%2520approach%2520generates%2520realistic%2520images%2520maintaining%250Athe%2520shape%2520and%2520location%2520of%2520the%2520original%2520folds%252C%2520after%2520the%250Aimage-style-translation%252C%2520better%2520than%2520existing%2520methods.%2520We%2520run%2520experiments%2520both%250Aon%2520a%2520novel%2520simulated%2520dataset%2520for%2520fold%2520segmentation%252C%2520and%2520real%2520data%2520from%2520the%250AEndoMapper%2520%2528EM%2529%2520dataset.%2520All%2520our%2520new%2520generated%2520data%2520and%2520new%2520EM%2520metadata%2520is%250Abeing%2520released%2520to%2520facilitate%2520further%2520research%252C%2520as%2520no%2520public%2520benchmark%2520is%250Acurrently%2520available%2520for%2520the%2520task%2520of%2520fold%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02654v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sim2Real%20in%20endoscopy%20segmentation%20with%20a%20novel%20structure%20aware%20image%0A%20%20translation&entry.906535625=Clara%20Tomasini%20and%20Luis%20Riazuelo%20and%20Ana%20C.%20Murillo&entry.1292438233=%20%20Automatic%20segmentation%20of%20anatomical%20landmarks%20in%20endoscopic%20images%20can%0Aprovide%20assistance%20to%20doctors%20and%20surgeons%20for%20diagnosis%2C%20treatments%20or%20medical%0Atraining.%20However%2C%20obtaining%20the%20annotations%20required%20to%20train%20commonly%20used%0Asupervised%20learning%20methods%20is%20a%20tedious%20and%20difficult%20task%2C%20in%20particular%20for%0Areal%20images.%20While%20ground%20truth%20annotations%20are%20easier%20to%20obtain%20for%20synthetic%0Adata%2C%20models%20trained%20on%20such%20data%20often%20do%20not%20generalize%20well%20to%20real%20data.%0AGenerative%20approaches%20can%20add%20realistic%20texture%20to%20it%2C%20but%20face%20difficulties%20to%0Amaintain%20the%20structure%20of%20the%20original%20scene.%20The%20main%20contribution%20in%20this%0Awork%20is%20a%20novel%20image%20translation%20model%20that%20adds%20realistic%20texture%20to%0Asimulated%20endoscopic%20images%20while%20keeping%20the%20key%20scene%20layout%20information.%20Our%0Aapproach%20produces%20realistic%20images%20in%20different%20endoscopy%20scenarios.%20We%0Ademonstrate%20these%20images%20can%20effectively%20be%20used%20to%20successfully%20train%20a%20model%0Afor%20a%20challenging%20end%20task%20without%20any%20real%20labeled%20data.%20In%20particular%2C%20we%0Ademonstrate%20our%20approach%20for%20the%20task%20of%20fold%20segmentation%20in%20colonoscopy%0Aimages.%20Folds%20are%20key%20anatomical%20landmarks%20that%20can%20occlude%20parts%20of%20the%20colon%0Amucosa%20and%20possible%20polyps.%20Our%20approach%20generates%20realistic%20images%20maintaining%0Athe%20shape%20and%20location%20of%20the%20original%20folds%2C%20after%20the%0Aimage-style-translation%2C%20better%20than%20existing%20methods.%20We%20run%20experiments%20both%0Aon%20a%20novel%20simulated%20dataset%20for%20fold%20segmentation%2C%20and%20real%20data%20from%20the%0AEndoMapper%20%28EM%29%20dataset.%20All%20our%20new%20generated%20data%20and%20new%20EM%20metadata%20is%0Abeing%20released%20to%20facilitate%20further%20research%2C%20as%20no%20public%20benchmark%20is%0Acurrently%20available%20for%20the%20task%20of%20fold%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02654v1&entry.124074799=Read"},
{"title": "Dance of Fireworks: An Interactive Broadcast Gymnastics Training System\n  Based on Pose Estimation", "author": "Haotian Chen and Ziyu Liu and Xi Cheng and Chuangqi Li", "abstract": "  This study introduces Dance of Fireworks, an interactive system designed to\ncombat sedentary health risks by enhancing engagement in radio calisthenics.\nLeveraging mobile device cameras and lightweight pose estimation\n(PoseNet/TensorFlow Lite), the system extracts body keypoints, computes joint\nangles, and compares them with standardized motions to deliver real-time\ncorrective feedback. To incentivize participation, it dynamically maps users'\nmovements (such as joint angles and velocity) to customizable fireworks\nanimations, rewarding improved accuracy with richer visual effects. Experiments\ninvolving 136 participants demonstrated a significant reduction in average\njoint angle errors from 21.3 degrees to 9.8 degrees (p < 0.01) over four\nsessions, with 93.4 percent of users affirming its exercise-promoting efficacy\nand 85.4 percent praising its entertainment value. The system operates without\npredefined motion templates or specialised hardware, enabling seamless\nintegration into office environments. Future enhancements will focus on\nimproving pose recognition accuracy, reducing latency, and adding features such\nas multiplayer interaction and music synchronisation. This work presents a\ncost-effective, engaging solution to promote physical activity in sedentary\npopulations.\n", "link": "http://arxiv.org/abs/2505.02690v1", "date": "2025-05-05", "relevancy": 2.7287, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5742}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5361}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5269}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dance%20of%20Fireworks%3A%20An%20Interactive%20Broadcast%20Gymnastics%20Training%20System%0A%20%20Based%20on%20Pose%20Estimation&body=Title%3A%20Dance%20of%20Fireworks%3A%20An%20Interactive%20Broadcast%20Gymnastics%20Training%20System%0A%20%20Based%20on%20Pose%20Estimation%0AAuthor%3A%20Haotian%20Chen%20and%20Ziyu%20Liu%20and%20Xi%20Cheng%20and%20Chuangqi%20Li%0AAbstract%3A%20%20%20This%20study%20introduces%20Dance%20of%20Fireworks%2C%20an%20interactive%20system%20designed%20to%0Acombat%20sedentary%20health%20risks%20by%20enhancing%20engagement%20in%20radio%20calisthenics.%0ALeveraging%20mobile%20device%20cameras%20and%20lightweight%20pose%20estimation%0A%28PoseNet/TensorFlow%20Lite%29%2C%20the%20system%20extracts%20body%20keypoints%2C%20computes%20joint%0Aangles%2C%20and%20compares%20them%20with%20standardized%20motions%20to%20deliver%20real-time%0Acorrective%20feedback.%20To%20incentivize%20participation%2C%20it%20dynamically%20maps%20users%27%0Amovements%20%28such%20as%20joint%20angles%20and%20velocity%29%20to%20customizable%20fireworks%0Aanimations%2C%20rewarding%20improved%20accuracy%20with%20richer%20visual%20effects.%20Experiments%0Ainvolving%20136%20participants%20demonstrated%20a%20significant%20reduction%20in%20average%0Ajoint%20angle%20errors%20from%2021.3%20degrees%20to%209.8%20degrees%20%28p%20%3C%200.01%29%20over%20four%0Asessions%2C%20with%2093.4%20percent%20of%20users%20affirming%20its%20exercise-promoting%20efficacy%0Aand%2085.4%20percent%20praising%20its%20entertainment%20value.%20The%20system%20operates%20without%0Apredefined%20motion%20templates%20or%20specialised%20hardware%2C%20enabling%20seamless%0Aintegration%20into%20office%20environments.%20Future%20enhancements%20will%20focus%20on%0Aimproving%20pose%20recognition%20accuracy%2C%20reducing%20latency%2C%20and%20adding%20features%20such%0Aas%20multiplayer%20interaction%20and%20music%20synchronisation.%20This%20work%20presents%20a%0Acost-effective%2C%20engaging%20solution%20to%20promote%20physical%20activity%20in%20sedentary%0Apopulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02690v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDance%2520of%2520Fireworks%253A%2520An%2520Interactive%2520Broadcast%2520Gymnastics%2520Training%2520System%250A%2520%2520Based%2520on%2520Pose%2520Estimation%26entry.906535625%3DHaotian%2520Chen%2520and%2520Ziyu%2520Liu%2520and%2520Xi%2520Cheng%2520and%2520Chuangqi%2520Li%26entry.1292438233%3D%2520%2520This%2520study%2520introduces%2520Dance%2520of%2520Fireworks%252C%2520an%2520interactive%2520system%2520designed%2520to%250Acombat%2520sedentary%2520health%2520risks%2520by%2520enhancing%2520engagement%2520in%2520radio%2520calisthenics.%250ALeveraging%2520mobile%2520device%2520cameras%2520and%2520lightweight%2520pose%2520estimation%250A%2528PoseNet/TensorFlow%2520Lite%2529%252C%2520the%2520system%2520extracts%2520body%2520keypoints%252C%2520computes%2520joint%250Aangles%252C%2520and%2520compares%2520them%2520with%2520standardized%2520motions%2520to%2520deliver%2520real-time%250Acorrective%2520feedback.%2520To%2520incentivize%2520participation%252C%2520it%2520dynamically%2520maps%2520users%2527%250Amovements%2520%2528such%2520as%2520joint%2520angles%2520and%2520velocity%2529%2520to%2520customizable%2520fireworks%250Aanimations%252C%2520rewarding%2520improved%2520accuracy%2520with%2520richer%2520visual%2520effects.%2520Experiments%250Ainvolving%2520136%2520participants%2520demonstrated%2520a%2520significant%2520reduction%2520in%2520average%250Ajoint%2520angle%2520errors%2520from%252021.3%2520degrees%2520to%25209.8%2520degrees%2520%2528p%2520%253C%25200.01%2529%2520over%2520four%250Asessions%252C%2520with%252093.4%2520percent%2520of%2520users%2520affirming%2520its%2520exercise-promoting%2520efficacy%250Aand%252085.4%2520percent%2520praising%2520its%2520entertainment%2520value.%2520The%2520system%2520operates%2520without%250Apredefined%2520motion%2520templates%2520or%2520specialised%2520hardware%252C%2520enabling%2520seamless%250Aintegration%2520into%2520office%2520environments.%2520Future%2520enhancements%2520will%2520focus%2520on%250Aimproving%2520pose%2520recognition%2520accuracy%252C%2520reducing%2520latency%252C%2520and%2520adding%2520features%2520such%250Aas%2520multiplayer%2520interaction%2520and%2520music%2520synchronisation.%2520This%2520work%2520presents%2520a%250Acost-effective%252C%2520engaging%2520solution%2520to%2520promote%2520physical%2520activity%2520in%2520sedentary%250Apopulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02690v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dance%20of%20Fireworks%3A%20An%20Interactive%20Broadcast%20Gymnastics%20Training%20System%0A%20%20Based%20on%20Pose%20Estimation&entry.906535625=Haotian%20Chen%20and%20Ziyu%20Liu%20and%20Xi%20Cheng%20and%20Chuangqi%20Li&entry.1292438233=%20%20This%20study%20introduces%20Dance%20of%20Fireworks%2C%20an%20interactive%20system%20designed%20to%0Acombat%20sedentary%20health%20risks%20by%20enhancing%20engagement%20in%20radio%20calisthenics.%0ALeveraging%20mobile%20device%20cameras%20and%20lightweight%20pose%20estimation%0A%28PoseNet/TensorFlow%20Lite%29%2C%20the%20system%20extracts%20body%20keypoints%2C%20computes%20joint%0Aangles%2C%20and%20compares%20them%20with%20standardized%20motions%20to%20deliver%20real-time%0Acorrective%20feedback.%20To%20incentivize%20participation%2C%20it%20dynamically%20maps%20users%27%0Amovements%20%28such%20as%20joint%20angles%20and%20velocity%29%20to%20customizable%20fireworks%0Aanimations%2C%20rewarding%20improved%20accuracy%20with%20richer%20visual%20effects.%20Experiments%0Ainvolving%20136%20participants%20demonstrated%20a%20significant%20reduction%20in%20average%0Ajoint%20angle%20errors%20from%2021.3%20degrees%20to%209.8%20degrees%20%28p%20%3C%200.01%29%20over%20four%0Asessions%2C%20with%2093.4%20percent%20of%20users%20affirming%20its%20exercise-promoting%20efficacy%0Aand%2085.4%20percent%20praising%20its%20entertainment%20value.%20The%20system%20operates%20without%0Apredefined%20motion%20templates%20or%20specialised%20hardware%2C%20enabling%20seamless%0Aintegration%20into%20office%20environments.%20Future%20enhancements%20will%20focus%20on%0Aimproving%20pose%20recognition%20accuracy%2C%20reducing%20latency%2C%20and%20adding%20features%20such%0Aas%20multiplayer%20interaction%20and%20music%20synchronisation.%20This%20work%20presents%20a%0Acost-effective%2C%20engaging%20solution%20to%20promote%20physical%20activity%20in%20sedentary%0Apopulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02690v1&entry.124074799=Read"},
{"title": "Scenethesis: A Language and Vision Agentic Framework for 3D Scene\n  Generation", "author": "Lu Ling and Chen-Hsuan Lin and Tsung-Yi Lin and Yifan Ding and Yu Zeng and Yichen Sheng and Yunhao Ge and Ming-Yu Liu and Aniket Bera and Zhaoshuo Li", "abstract": "  Synthesizing interactive 3D scenes from text is essential for gaming, virtual\nreality, and embodied AI. However, existing methods face several challenges.\nLearning-based approaches depend on small-scale indoor datasets, limiting the\nscene diversity and layout complexity. While large language models (LLMs) can\nleverage diverse text-domain knowledge, they struggle with spatial realism,\noften producing unnatural object placements that fail to respect common sense.\nOur key insight is that vision perception can bridge this gap by providing\nrealistic spatial guidance that LLMs lack. To this end, we introduce\nScenethesis, a training-free agentic framework that integrates LLM-based scene\nplanning with vision-guided layout refinement. Given a text prompt, Scenethesis\nfirst employs an LLM to draft a coarse layout. A vision module then refines it\nby generating an image guidance and extracting scene structure to capture\ninter-object relations. Next, an optimization module iteratively enforces\naccurate pose alignment and physical plausibility, preventing artifacts like\nobject penetration and instability. Finally, a judge module verifies spatial\ncoherence. Comprehensive experiments show that Scenethesis generates diverse,\nrealistic, and physically plausible 3D interactive scenes, making it valuable\nfor virtual content creation, simulation environments, and embodied AI\nresearch.\n", "link": "http://arxiv.org/abs/2505.02836v1", "date": "2025-05-05", "relevancy": 2.7228, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6891}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6891}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scenethesis%3A%20A%20Language%20and%20Vision%20Agentic%20Framework%20for%203D%20Scene%0A%20%20Generation&body=Title%3A%20Scenethesis%3A%20A%20Language%20and%20Vision%20Agentic%20Framework%20for%203D%20Scene%0A%20%20Generation%0AAuthor%3A%20Lu%20Ling%20and%20Chen-Hsuan%20Lin%20and%20Tsung-Yi%20Lin%20and%20Yifan%20Ding%20and%20Yu%20Zeng%20and%20Yichen%20Sheng%20and%20Yunhao%20Ge%20and%20Ming-Yu%20Liu%20and%20Aniket%20Bera%20and%20Zhaoshuo%20Li%0AAbstract%3A%20%20%20Synthesizing%20interactive%203D%20scenes%20from%20text%20is%20essential%20for%20gaming%2C%20virtual%0Areality%2C%20and%20embodied%20AI.%20However%2C%20existing%20methods%20face%20several%20challenges.%0ALearning-based%20approaches%20depend%20on%20small-scale%20indoor%20datasets%2C%20limiting%20the%0Ascene%20diversity%20and%20layout%20complexity.%20While%20large%20language%20models%20%28LLMs%29%20can%0Aleverage%20diverse%20text-domain%20knowledge%2C%20they%20struggle%20with%20spatial%20realism%2C%0Aoften%20producing%20unnatural%20object%20placements%20that%20fail%20to%20respect%20common%20sense.%0AOur%20key%20insight%20is%20that%20vision%20perception%20can%20bridge%20this%20gap%20by%20providing%0Arealistic%20spatial%20guidance%20that%20LLMs%20lack.%20To%20this%20end%2C%20we%20introduce%0AScenethesis%2C%20a%20training-free%20agentic%20framework%20that%20integrates%20LLM-based%20scene%0Aplanning%20with%20vision-guided%20layout%20refinement.%20Given%20a%20text%20prompt%2C%20Scenethesis%0Afirst%20employs%20an%20LLM%20to%20draft%20a%20coarse%20layout.%20A%20vision%20module%20then%20refines%20it%0Aby%20generating%20an%20image%20guidance%20and%20extracting%20scene%20structure%20to%20capture%0Ainter-object%20relations.%20Next%2C%20an%20optimization%20module%20iteratively%20enforces%0Aaccurate%20pose%20alignment%20and%20physical%20plausibility%2C%20preventing%20artifacts%20like%0Aobject%20penetration%20and%20instability.%20Finally%2C%20a%20judge%20module%20verifies%20spatial%0Acoherence.%20Comprehensive%20experiments%20show%20that%20Scenethesis%20generates%20diverse%2C%0Arealistic%2C%20and%20physically%20plausible%203D%20interactive%20scenes%2C%20making%20it%20valuable%0Afor%20virtual%20content%20creation%2C%20simulation%20environments%2C%20and%20embodied%20AI%0Aresearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02836v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScenethesis%253A%2520A%2520Language%2520and%2520Vision%2520Agentic%2520Framework%2520for%25203D%2520Scene%250A%2520%2520Generation%26entry.906535625%3DLu%2520Ling%2520and%2520Chen-Hsuan%2520Lin%2520and%2520Tsung-Yi%2520Lin%2520and%2520Yifan%2520Ding%2520and%2520Yu%2520Zeng%2520and%2520Yichen%2520Sheng%2520and%2520Yunhao%2520Ge%2520and%2520Ming-Yu%2520Liu%2520and%2520Aniket%2520Bera%2520and%2520Zhaoshuo%2520Li%26entry.1292438233%3D%2520%2520Synthesizing%2520interactive%25203D%2520scenes%2520from%2520text%2520is%2520essential%2520for%2520gaming%252C%2520virtual%250Areality%252C%2520and%2520embodied%2520AI.%2520However%252C%2520existing%2520methods%2520face%2520several%2520challenges.%250ALearning-based%2520approaches%2520depend%2520on%2520small-scale%2520indoor%2520datasets%252C%2520limiting%2520the%250Ascene%2520diversity%2520and%2520layout%2520complexity.%2520While%2520large%2520language%2520models%2520%2528LLMs%2529%2520can%250Aleverage%2520diverse%2520text-domain%2520knowledge%252C%2520they%2520struggle%2520with%2520spatial%2520realism%252C%250Aoften%2520producing%2520unnatural%2520object%2520placements%2520that%2520fail%2520to%2520respect%2520common%2520sense.%250AOur%2520key%2520insight%2520is%2520that%2520vision%2520perception%2520can%2520bridge%2520this%2520gap%2520by%2520providing%250Arealistic%2520spatial%2520guidance%2520that%2520LLMs%2520lack.%2520To%2520this%2520end%252C%2520we%2520introduce%250AScenethesis%252C%2520a%2520training-free%2520agentic%2520framework%2520that%2520integrates%2520LLM-based%2520scene%250Aplanning%2520with%2520vision-guided%2520layout%2520refinement.%2520Given%2520a%2520text%2520prompt%252C%2520Scenethesis%250Afirst%2520employs%2520an%2520LLM%2520to%2520draft%2520a%2520coarse%2520layout.%2520A%2520vision%2520module%2520then%2520refines%2520it%250Aby%2520generating%2520an%2520image%2520guidance%2520and%2520extracting%2520scene%2520structure%2520to%2520capture%250Ainter-object%2520relations.%2520Next%252C%2520an%2520optimization%2520module%2520iteratively%2520enforces%250Aaccurate%2520pose%2520alignment%2520and%2520physical%2520plausibility%252C%2520preventing%2520artifacts%2520like%250Aobject%2520penetration%2520and%2520instability.%2520Finally%252C%2520a%2520judge%2520module%2520verifies%2520spatial%250Acoherence.%2520Comprehensive%2520experiments%2520show%2520that%2520Scenethesis%2520generates%2520diverse%252C%250Arealistic%252C%2520and%2520physically%2520plausible%25203D%2520interactive%2520scenes%252C%2520making%2520it%2520valuable%250Afor%2520virtual%2520content%2520creation%252C%2520simulation%2520environments%252C%2520and%2520embodied%2520AI%250Aresearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02836v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scenethesis%3A%20A%20Language%20and%20Vision%20Agentic%20Framework%20for%203D%20Scene%0A%20%20Generation&entry.906535625=Lu%20Ling%20and%20Chen-Hsuan%20Lin%20and%20Tsung-Yi%20Lin%20and%20Yifan%20Ding%20and%20Yu%20Zeng%20and%20Yichen%20Sheng%20and%20Yunhao%20Ge%20and%20Ming-Yu%20Liu%20and%20Aniket%20Bera%20and%20Zhaoshuo%20Li&entry.1292438233=%20%20Synthesizing%20interactive%203D%20scenes%20from%20text%20is%20essential%20for%20gaming%2C%20virtual%0Areality%2C%20and%20embodied%20AI.%20However%2C%20existing%20methods%20face%20several%20challenges.%0ALearning-based%20approaches%20depend%20on%20small-scale%20indoor%20datasets%2C%20limiting%20the%0Ascene%20diversity%20and%20layout%20complexity.%20While%20large%20language%20models%20%28LLMs%29%20can%0Aleverage%20diverse%20text-domain%20knowledge%2C%20they%20struggle%20with%20spatial%20realism%2C%0Aoften%20producing%20unnatural%20object%20placements%20that%20fail%20to%20respect%20common%20sense.%0AOur%20key%20insight%20is%20that%20vision%20perception%20can%20bridge%20this%20gap%20by%20providing%0Arealistic%20spatial%20guidance%20that%20LLMs%20lack.%20To%20this%20end%2C%20we%20introduce%0AScenethesis%2C%20a%20training-free%20agentic%20framework%20that%20integrates%20LLM-based%20scene%0Aplanning%20with%20vision-guided%20layout%20refinement.%20Given%20a%20text%20prompt%2C%20Scenethesis%0Afirst%20employs%20an%20LLM%20to%20draft%20a%20coarse%20layout.%20A%20vision%20module%20then%20refines%20it%0Aby%20generating%20an%20image%20guidance%20and%20extracting%20scene%20structure%20to%20capture%0Ainter-object%20relations.%20Next%2C%20an%20optimization%20module%20iteratively%20enforces%0Aaccurate%20pose%20alignment%20and%20physical%20plausibility%2C%20preventing%20artifacts%20like%0Aobject%20penetration%20and%20instability.%20Finally%2C%20a%20judge%20module%20verifies%20spatial%0Acoherence.%20Comprehensive%20experiments%20show%20that%20Scenethesis%20generates%20diverse%2C%0Arealistic%2C%20and%20physically%20plausible%203D%20interactive%20scenes%2C%20making%20it%20valuable%0Afor%20virtual%20content%20creation%2C%20simulation%20environments%2C%20and%20embodied%20AI%0Aresearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02836v1&entry.124074799=Read"},
{"title": "AC-LIO: Towards Asymptotic and Consistent Convergence in LiDAR-Inertial\n  Odometry", "author": "Tianxiang Zhang and Xuanxuan Zhang and Wenlei Fan and Xin Xia and Huai Yu and Lin Wang and You Li", "abstract": "  Existing LiDAR-Inertial Odometry (LIO) methods typically utilize the prior\nstate trajectory derived from the IMU integration to compensate for the motion\ndistortion within LiDAR frames. However, discrepancies between the prior and\nactual trajectory can lead to residual distortions that compromise the\nconsistency of the LiDAR frame with its corresponding geometric environment.\nThis imbalance may result in pointcloud registration becoming trapped in local\noptima, thereby exacerbating drift during long-term and large-scale\nlocalization. To address the issue, we propose a novel asymptotically and\nconsistently converging LIO framework dubbed AC-LIO. Our key idea is to back\npropagate current update term based on the prior state chain, and\nasymptotically compensate for the residual distortion during iteration.\nMoreover, considering the weak correlation between previous error and current\ndistortion, we establish convergence criteria based on the pointcloud\nconstraints to regulate the backpropagation. This method of guiding asymptotic\ndistortion compensation using convergence criteria subtly enhances the\nconsistent convergence of pointcloud registration, futher improving the\naccuracy and robustness of LIO system. Extensive experiments demonstrate that\nour AC-LIO framework significantly promotes consistent convergence in state\nestimation compared to prior arts, with about 30.4% reduction in average RMSE\nover the second best result, leading to marked improvements in the accuracy of\nlong-term and large-scale localization and mapping.\n", "link": "http://arxiv.org/abs/2412.05873v2", "date": "2025-05-05", "relevancy": 2.7147, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5894}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5346}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5048}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AC-LIO%3A%20Towards%20Asymptotic%20and%20Consistent%20Convergence%20in%20LiDAR-Inertial%0A%20%20Odometry&body=Title%3A%20AC-LIO%3A%20Towards%20Asymptotic%20and%20Consistent%20Convergence%20in%20LiDAR-Inertial%0A%20%20Odometry%0AAuthor%3A%20Tianxiang%20Zhang%20and%20Xuanxuan%20Zhang%20and%20Wenlei%20Fan%20and%20Xin%20Xia%20and%20Huai%20Yu%20and%20Lin%20Wang%20and%20You%20Li%0AAbstract%3A%20%20%20Existing%20LiDAR-Inertial%20Odometry%20%28LIO%29%20methods%20typically%20utilize%20the%20prior%0Astate%20trajectory%20derived%20from%20the%20IMU%20integration%20to%20compensate%20for%20the%20motion%0Adistortion%20within%20LiDAR%20frames.%20However%2C%20discrepancies%20between%20the%20prior%20and%0Aactual%20trajectory%20can%20lead%20to%20residual%20distortions%20that%20compromise%20the%0Aconsistency%20of%20the%20LiDAR%20frame%20with%20its%20corresponding%20geometric%20environment.%0AThis%20imbalance%20may%20result%20in%20pointcloud%20registration%20becoming%20trapped%20in%20local%0Aoptima%2C%20thereby%20exacerbating%20drift%20during%20long-term%20and%20large-scale%0Alocalization.%20To%20address%20the%20issue%2C%20we%20propose%20a%20novel%20asymptotically%20and%0Aconsistently%20converging%20LIO%20framework%20dubbed%20AC-LIO.%20Our%20key%20idea%20is%20to%20back%0Apropagate%20current%20update%20term%20based%20on%20the%20prior%20state%20chain%2C%20and%0Aasymptotically%20compensate%20for%20the%20residual%20distortion%20during%20iteration.%0AMoreover%2C%20considering%20the%20weak%20correlation%20between%20previous%20error%20and%20current%0Adistortion%2C%20we%20establish%20convergence%20criteria%20based%20on%20the%20pointcloud%0Aconstraints%20to%20regulate%20the%20backpropagation.%20This%20method%20of%20guiding%20asymptotic%0Adistortion%20compensation%20using%20convergence%20criteria%20subtly%20enhances%20the%0Aconsistent%20convergence%20of%20pointcloud%20registration%2C%20futher%20improving%20the%0Aaccuracy%20and%20robustness%20of%20LIO%20system.%20Extensive%20experiments%20demonstrate%20that%0Aour%20AC-LIO%20framework%20significantly%20promotes%20consistent%20convergence%20in%20state%0Aestimation%20compared%20to%20prior%20arts%2C%20with%20about%2030.4%25%20reduction%20in%20average%20RMSE%0Aover%20the%20second%20best%20result%2C%20leading%20to%20marked%20improvements%20in%20the%20accuracy%20of%0Along-term%20and%20large-scale%20localization%20and%20mapping.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.05873v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAC-LIO%253A%2520Towards%2520Asymptotic%2520and%2520Consistent%2520Convergence%2520in%2520LiDAR-Inertial%250A%2520%2520Odometry%26entry.906535625%3DTianxiang%2520Zhang%2520and%2520Xuanxuan%2520Zhang%2520and%2520Wenlei%2520Fan%2520and%2520Xin%2520Xia%2520and%2520Huai%2520Yu%2520and%2520Lin%2520Wang%2520and%2520You%2520Li%26entry.1292438233%3D%2520%2520Existing%2520LiDAR-Inertial%2520Odometry%2520%2528LIO%2529%2520methods%2520typically%2520utilize%2520the%2520prior%250Astate%2520trajectory%2520derived%2520from%2520the%2520IMU%2520integration%2520to%2520compensate%2520for%2520the%2520motion%250Adistortion%2520within%2520LiDAR%2520frames.%2520However%252C%2520discrepancies%2520between%2520the%2520prior%2520and%250Aactual%2520trajectory%2520can%2520lead%2520to%2520residual%2520distortions%2520that%2520compromise%2520the%250Aconsistency%2520of%2520the%2520LiDAR%2520frame%2520with%2520its%2520corresponding%2520geometric%2520environment.%250AThis%2520imbalance%2520may%2520result%2520in%2520pointcloud%2520registration%2520becoming%2520trapped%2520in%2520local%250Aoptima%252C%2520thereby%2520exacerbating%2520drift%2520during%2520long-term%2520and%2520large-scale%250Alocalization.%2520To%2520address%2520the%2520issue%252C%2520we%2520propose%2520a%2520novel%2520asymptotically%2520and%250Aconsistently%2520converging%2520LIO%2520framework%2520dubbed%2520AC-LIO.%2520Our%2520key%2520idea%2520is%2520to%2520back%250Apropagate%2520current%2520update%2520term%2520based%2520on%2520the%2520prior%2520state%2520chain%252C%2520and%250Aasymptotically%2520compensate%2520for%2520the%2520residual%2520distortion%2520during%2520iteration.%250AMoreover%252C%2520considering%2520the%2520weak%2520correlation%2520between%2520previous%2520error%2520and%2520current%250Adistortion%252C%2520we%2520establish%2520convergence%2520criteria%2520based%2520on%2520the%2520pointcloud%250Aconstraints%2520to%2520regulate%2520the%2520backpropagation.%2520This%2520method%2520of%2520guiding%2520asymptotic%250Adistortion%2520compensation%2520using%2520convergence%2520criteria%2520subtly%2520enhances%2520the%250Aconsistent%2520convergence%2520of%2520pointcloud%2520registration%252C%2520futher%2520improving%2520the%250Aaccuracy%2520and%2520robustness%2520of%2520LIO%2520system.%2520Extensive%2520experiments%2520demonstrate%2520that%250Aour%2520AC-LIO%2520framework%2520significantly%2520promotes%2520consistent%2520convergence%2520in%2520state%250Aestimation%2520compared%2520to%2520prior%2520arts%252C%2520with%2520about%252030.4%2525%2520reduction%2520in%2520average%2520RMSE%250Aover%2520the%2520second%2520best%2520result%252C%2520leading%2520to%2520marked%2520improvements%2520in%2520the%2520accuracy%2520of%250Along-term%2520and%2520large-scale%2520localization%2520and%2520mapping.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.05873v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AC-LIO%3A%20Towards%20Asymptotic%20and%20Consistent%20Convergence%20in%20LiDAR-Inertial%0A%20%20Odometry&entry.906535625=Tianxiang%20Zhang%20and%20Xuanxuan%20Zhang%20and%20Wenlei%20Fan%20and%20Xin%20Xia%20and%20Huai%20Yu%20and%20Lin%20Wang%20and%20You%20Li&entry.1292438233=%20%20Existing%20LiDAR-Inertial%20Odometry%20%28LIO%29%20methods%20typically%20utilize%20the%20prior%0Astate%20trajectory%20derived%20from%20the%20IMU%20integration%20to%20compensate%20for%20the%20motion%0Adistortion%20within%20LiDAR%20frames.%20However%2C%20discrepancies%20between%20the%20prior%20and%0Aactual%20trajectory%20can%20lead%20to%20residual%20distortions%20that%20compromise%20the%0Aconsistency%20of%20the%20LiDAR%20frame%20with%20its%20corresponding%20geometric%20environment.%0AThis%20imbalance%20may%20result%20in%20pointcloud%20registration%20becoming%20trapped%20in%20local%0Aoptima%2C%20thereby%20exacerbating%20drift%20during%20long-term%20and%20large-scale%0Alocalization.%20To%20address%20the%20issue%2C%20we%20propose%20a%20novel%20asymptotically%20and%0Aconsistently%20converging%20LIO%20framework%20dubbed%20AC-LIO.%20Our%20key%20idea%20is%20to%20back%0Apropagate%20current%20update%20term%20based%20on%20the%20prior%20state%20chain%2C%20and%0Aasymptotically%20compensate%20for%20the%20residual%20distortion%20during%20iteration.%0AMoreover%2C%20considering%20the%20weak%20correlation%20between%20previous%20error%20and%20current%0Adistortion%2C%20we%20establish%20convergence%20criteria%20based%20on%20the%20pointcloud%0Aconstraints%20to%20regulate%20the%20backpropagation.%20This%20method%20of%20guiding%20asymptotic%0Adistortion%20compensation%20using%20convergence%20criteria%20subtly%20enhances%20the%0Aconsistent%20convergence%20of%20pointcloud%20registration%2C%20futher%20improving%20the%0Aaccuracy%20and%20robustness%20of%20LIO%20system.%20Extensive%20experiments%20demonstrate%20that%0Aour%20AC-LIO%20framework%20significantly%20promotes%20consistent%20convergence%20in%20state%0Aestimation%20compared%20to%20prior%20arts%2C%20with%20about%2030.4%25%20reduction%20in%20average%20RMSE%0Aover%20the%20second%20best%20result%2C%20leading%20to%20marked%20improvements%20in%20the%20accuracy%20of%0Along-term%20and%20large-scale%20localization%20and%20mapping.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.05873v2&entry.124074799=Read"},
{"title": "Joint-Embedding Masked Autoencoder for Self-supervised Learning of\n  Dynamic Functional Connectivity from the Human Brain", "author": "Jungwon Choi and Hyungi Lee and Byung-Hoon Kim and Juho Lee", "abstract": "  Graph Neural Networks (GNNs) have shown promise in learning dynamic\nfunctional connectivity for distinguishing phenotypes from human brain\nnetworks. However, obtaining extensive labeled clinical data for training is\noften resource-intensive, making practical application difficult. Leveraging\nunlabeled data thus becomes crucial for representation learning in a\nlabel-scarce setting. Although generative self-supervised learning techniques,\nespecially masked autoencoders, have shown promising results in representation\nlearning in various domains, their application to dynamic graphs for dynamic\nfunctional connectivity remains underexplored, facing challenges in capturing\nhigh-level semantic representations. Here, we introduce the Spatio-Temporal\nJoint Embedding Masked Autoencoder (ST-JEMA), drawing inspiration from the\nJoint Embedding Predictive Architecture (JEPA) in computer vision. ST-JEMA\nemploys a JEPA-inspired strategy for reconstructing dynamic graphs, which\nenables the learning of higher-level semantic representations considering\ntemporal perspectives, addressing the challenges in fMRI data representation\nlearning. Utilizing the large-scale UK Biobank dataset for self-supervised\nlearning, ST-JEMA shows exceptional representation learning performance on\ndynamic functional connectivity demonstrating superiority over previous methods\nin predicting phenotypes and psychiatric diagnoses across eight benchmark fMRI\ndatasets even with limited samples and effectiveness of temporal reconstruction\non missing data scenarios. These findings highlight the potential of our\napproach as a robust representation learning method for leveraging label-scarce\nfMRI data.\n", "link": "http://arxiv.org/abs/2403.06432v2", "date": "2025-05-05", "relevancy": 2.7072, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5554}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5475}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5215}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Joint-Embedding%20Masked%20Autoencoder%20for%20Self-supervised%20Learning%20of%0A%20%20Dynamic%20Functional%20Connectivity%20from%20the%20Human%20Brain&body=Title%3A%20Joint-Embedding%20Masked%20Autoencoder%20for%20Self-supervised%20Learning%20of%0A%20%20Dynamic%20Functional%20Connectivity%20from%20the%20Human%20Brain%0AAuthor%3A%20Jungwon%20Choi%20and%20Hyungi%20Lee%20and%20Byung-Hoon%20Kim%20and%20Juho%20Lee%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20shown%20promise%20in%20learning%20dynamic%0Afunctional%20connectivity%20for%20distinguishing%20phenotypes%20from%20human%20brain%0Anetworks.%20However%2C%20obtaining%20extensive%20labeled%20clinical%20data%20for%20training%20is%0Aoften%20resource-intensive%2C%20making%20practical%20application%20difficult.%20Leveraging%0Aunlabeled%20data%20thus%20becomes%20crucial%20for%20representation%20learning%20in%20a%0Alabel-scarce%20setting.%20Although%20generative%20self-supervised%20learning%20techniques%2C%0Aespecially%20masked%20autoencoders%2C%20have%20shown%20promising%20results%20in%20representation%0Alearning%20in%20various%20domains%2C%20their%20application%20to%20dynamic%20graphs%20for%20dynamic%0Afunctional%20connectivity%20remains%20underexplored%2C%20facing%20challenges%20in%20capturing%0Ahigh-level%20semantic%20representations.%20Here%2C%20we%20introduce%20the%20Spatio-Temporal%0AJoint%20Embedding%20Masked%20Autoencoder%20%28ST-JEMA%29%2C%20drawing%20inspiration%20from%20the%0AJoint%20Embedding%20Predictive%20Architecture%20%28JEPA%29%20in%20computer%20vision.%20ST-JEMA%0Aemploys%20a%20JEPA-inspired%20strategy%20for%20reconstructing%20dynamic%20graphs%2C%20which%0Aenables%20the%20learning%20of%20higher-level%20semantic%20representations%20considering%0Atemporal%20perspectives%2C%20addressing%20the%20challenges%20in%20fMRI%20data%20representation%0Alearning.%20Utilizing%20the%20large-scale%20UK%20Biobank%20dataset%20for%20self-supervised%0Alearning%2C%20ST-JEMA%20shows%20exceptional%20representation%20learning%20performance%20on%0Adynamic%20functional%20connectivity%20demonstrating%20superiority%20over%20previous%20methods%0Ain%20predicting%20phenotypes%20and%20psychiatric%20diagnoses%20across%20eight%20benchmark%20fMRI%0Adatasets%20even%20with%20limited%20samples%20and%20effectiveness%20of%20temporal%20reconstruction%0Aon%20missing%20data%20scenarios.%20These%20findings%20highlight%20the%20potential%20of%20our%0Aapproach%20as%20a%20robust%20representation%20learning%20method%20for%20leveraging%20label-scarce%0AfMRI%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06432v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoint-Embedding%2520Masked%2520Autoencoder%2520for%2520Self-supervised%2520Learning%2520of%250A%2520%2520Dynamic%2520Functional%2520Connectivity%2520from%2520the%2520Human%2520Brain%26entry.906535625%3DJungwon%2520Choi%2520and%2520Hyungi%2520Lee%2520and%2520Byung-Hoon%2520Kim%2520and%2520Juho%2520Lee%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520shown%2520promise%2520in%2520learning%2520dynamic%250Afunctional%2520connectivity%2520for%2520distinguishing%2520phenotypes%2520from%2520human%2520brain%250Anetworks.%2520However%252C%2520obtaining%2520extensive%2520labeled%2520clinical%2520data%2520for%2520training%2520is%250Aoften%2520resource-intensive%252C%2520making%2520practical%2520application%2520difficult.%2520Leveraging%250Aunlabeled%2520data%2520thus%2520becomes%2520crucial%2520for%2520representation%2520learning%2520in%2520a%250Alabel-scarce%2520setting.%2520Although%2520generative%2520self-supervised%2520learning%2520techniques%252C%250Aespecially%2520masked%2520autoencoders%252C%2520have%2520shown%2520promising%2520results%2520in%2520representation%250Alearning%2520in%2520various%2520domains%252C%2520their%2520application%2520to%2520dynamic%2520graphs%2520for%2520dynamic%250Afunctional%2520connectivity%2520remains%2520underexplored%252C%2520facing%2520challenges%2520in%2520capturing%250Ahigh-level%2520semantic%2520representations.%2520Here%252C%2520we%2520introduce%2520the%2520Spatio-Temporal%250AJoint%2520Embedding%2520Masked%2520Autoencoder%2520%2528ST-JEMA%2529%252C%2520drawing%2520inspiration%2520from%2520the%250AJoint%2520Embedding%2520Predictive%2520Architecture%2520%2528JEPA%2529%2520in%2520computer%2520vision.%2520ST-JEMA%250Aemploys%2520a%2520JEPA-inspired%2520strategy%2520for%2520reconstructing%2520dynamic%2520graphs%252C%2520which%250Aenables%2520the%2520learning%2520of%2520higher-level%2520semantic%2520representations%2520considering%250Atemporal%2520perspectives%252C%2520addressing%2520the%2520challenges%2520in%2520fMRI%2520data%2520representation%250Alearning.%2520Utilizing%2520the%2520large-scale%2520UK%2520Biobank%2520dataset%2520for%2520self-supervised%250Alearning%252C%2520ST-JEMA%2520shows%2520exceptional%2520representation%2520learning%2520performance%2520on%250Adynamic%2520functional%2520connectivity%2520demonstrating%2520superiority%2520over%2520previous%2520methods%250Ain%2520predicting%2520phenotypes%2520and%2520psychiatric%2520diagnoses%2520across%2520eight%2520benchmark%2520fMRI%250Adatasets%2520even%2520with%2520limited%2520samples%2520and%2520effectiveness%2520of%2520temporal%2520reconstruction%250Aon%2520missing%2520data%2520scenarios.%2520These%2520findings%2520highlight%2520the%2520potential%2520of%2520our%250Aapproach%2520as%2520a%2520robust%2520representation%2520learning%2520method%2520for%2520leveraging%2520label-scarce%250AfMRI%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.06432v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint-Embedding%20Masked%20Autoencoder%20for%20Self-supervised%20Learning%20of%0A%20%20Dynamic%20Functional%20Connectivity%20from%20the%20Human%20Brain&entry.906535625=Jungwon%20Choi%20and%20Hyungi%20Lee%20and%20Byung-Hoon%20Kim%20and%20Juho%20Lee&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20shown%20promise%20in%20learning%20dynamic%0Afunctional%20connectivity%20for%20distinguishing%20phenotypes%20from%20human%20brain%0Anetworks.%20However%2C%20obtaining%20extensive%20labeled%20clinical%20data%20for%20training%20is%0Aoften%20resource-intensive%2C%20making%20practical%20application%20difficult.%20Leveraging%0Aunlabeled%20data%20thus%20becomes%20crucial%20for%20representation%20learning%20in%20a%0Alabel-scarce%20setting.%20Although%20generative%20self-supervised%20learning%20techniques%2C%0Aespecially%20masked%20autoencoders%2C%20have%20shown%20promising%20results%20in%20representation%0Alearning%20in%20various%20domains%2C%20their%20application%20to%20dynamic%20graphs%20for%20dynamic%0Afunctional%20connectivity%20remains%20underexplored%2C%20facing%20challenges%20in%20capturing%0Ahigh-level%20semantic%20representations.%20Here%2C%20we%20introduce%20the%20Spatio-Temporal%0AJoint%20Embedding%20Masked%20Autoencoder%20%28ST-JEMA%29%2C%20drawing%20inspiration%20from%20the%0AJoint%20Embedding%20Predictive%20Architecture%20%28JEPA%29%20in%20computer%20vision.%20ST-JEMA%0Aemploys%20a%20JEPA-inspired%20strategy%20for%20reconstructing%20dynamic%20graphs%2C%20which%0Aenables%20the%20learning%20of%20higher-level%20semantic%20representations%20considering%0Atemporal%20perspectives%2C%20addressing%20the%20challenges%20in%20fMRI%20data%20representation%0Alearning.%20Utilizing%20the%20large-scale%20UK%20Biobank%20dataset%20for%20self-supervised%0Alearning%2C%20ST-JEMA%20shows%20exceptional%20representation%20learning%20performance%20on%0Adynamic%20functional%20connectivity%20demonstrating%20superiority%20over%20previous%20methods%0Ain%20predicting%20phenotypes%20and%20psychiatric%20diagnoses%20across%20eight%20benchmark%20fMRI%0Adatasets%20even%20with%20limited%20samples%20and%20effectiveness%20of%20temporal%20reconstruction%0Aon%20missing%20data%20scenarios.%20These%20findings%20highlight%20the%20potential%20of%20our%0Aapproach%20as%20a%20robust%20representation%20learning%20method%20for%20leveraging%20label-scarce%0AfMRI%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06432v2&entry.124074799=Read"},
{"title": "AOR: Anatomical Ontology-Guided Reasoning for Medical Large Multimodal\n  Model in Chest X-Ray Interpretation", "author": "Qingqiu Li and Zihang Cui and Seongsu Bae and Jilan Xu and Runtian Yuan and Yuejie Zhang and Rui Feng and Quanli Shen and Xiaobo Zhang and Junjun He and Shujun Wang", "abstract": "  Chest X-rays (CXRs) are the most frequently performed imaging examinations in\nclinical settings. Recent advancements in Large Multimodal Models (LMMs) have\nenabled automated CXR interpretation, enhancing diagnostic accuracy and\nefficiency. However, despite their strong visual understanding, current Medical\nLMMs (MLMMs) still face two major challenges: (1) Insufficient region-level\nunderstanding and interaction, and (2) Limited accuracy and interpretability\ndue to single-step reasoning. In this paper, we empower MLMMs with\nanatomy-centric reasoning capabilities to enhance their interactivity and\nexplainability. Specifically, we first propose an Anatomical Ontology-Guided\nReasoning (AOR) framework, which centers on cross-modal region-level\ninformation to facilitate multi-step reasoning. Next, under the guidance of\nexpert physicians, we develop AOR-Instruction, a large instruction dataset for\nMLMMs training. Our experiments demonstrate AOR's superior performance in both\nVQA and report generation tasks.\n", "link": "http://arxiv.org/abs/2505.02830v1", "date": "2025-05-05", "relevancy": 2.6914, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5431}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5431}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5286}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AOR%3A%20Anatomical%20Ontology-Guided%20Reasoning%20for%20Medical%20Large%20Multimodal%0A%20%20Model%20in%20Chest%20X-Ray%20Interpretation&body=Title%3A%20AOR%3A%20Anatomical%20Ontology-Guided%20Reasoning%20for%20Medical%20Large%20Multimodal%0A%20%20Model%20in%20Chest%20X-Ray%20Interpretation%0AAuthor%3A%20Qingqiu%20Li%20and%20Zihang%20Cui%20and%20Seongsu%20Bae%20and%20Jilan%20Xu%20and%20Runtian%20Yuan%20and%20Yuejie%20Zhang%20and%20Rui%20Feng%20and%20Quanli%20Shen%20and%20Xiaobo%20Zhang%20and%20Junjun%20He%20and%20Shujun%20Wang%0AAbstract%3A%20%20%20Chest%20X-rays%20%28CXRs%29%20are%20the%20most%20frequently%20performed%20imaging%20examinations%20in%0Aclinical%20settings.%20Recent%20advancements%20in%20Large%20Multimodal%20Models%20%28LMMs%29%20have%0Aenabled%20automated%20CXR%20interpretation%2C%20enhancing%20diagnostic%20accuracy%20and%0Aefficiency.%20However%2C%20despite%20their%20strong%20visual%20understanding%2C%20current%20Medical%0ALMMs%20%28MLMMs%29%20still%20face%20two%20major%20challenges%3A%20%281%29%20Insufficient%20region-level%0Aunderstanding%20and%20interaction%2C%20and%20%282%29%20Limited%20accuracy%20and%20interpretability%0Adue%20to%20single-step%20reasoning.%20In%20this%20paper%2C%20we%20empower%20MLMMs%20with%0Aanatomy-centric%20reasoning%20capabilities%20to%20enhance%20their%20interactivity%20and%0Aexplainability.%20Specifically%2C%20we%20first%20propose%20an%20Anatomical%20Ontology-Guided%0AReasoning%20%28AOR%29%20framework%2C%20which%20centers%20on%20cross-modal%20region-level%0Ainformation%20to%20facilitate%20multi-step%20reasoning.%20Next%2C%20under%20the%20guidance%20of%0Aexpert%20physicians%2C%20we%20develop%20AOR-Instruction%2C%20a%20large%20instruction%20dataset%20for%0AMLMMs%20training.%20Our%20experiments%20demonstrate%20AOR%27s%20superior%20performance%20in%20both%0AVQA%20and%20report%20generation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02830v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAOR%253A%2520Anatomical%2520Ontology-Guided%2520Reasoning%2520for%2520Medical%2520Large%2520Multimodal%250A%2520%2520Model%2520in%2520Chest%2520X-Ray%2520Interpretation%26entry.906535625%3DQingqiu%2520Li%2520and%2520Zihang%2520Cui%2520and%2520Seongsu%2520Bae%2520and%2520Jilan%2520Xu%2520and%2520Runtian%2520Yuan%2520and%2520Yuejie%2520Zhang%2520and%2520Rui%2520Feng%2520and%2520Quanli%2520Shen%2520and%2520Xiaobo%2520Zhang%2520and%2520Junjun%2520He%2520and%2520Shujun%2520Wang%26entry.1292438233%3D%2520%2520Chest%2520X-rays%2520%2528CXRs%2529%2520are%2520the%2520most%2520frequently%2520performed%2520imaging%2520examinations%2520in%250Aclinical%2520settings.%2520Recent%2520advancements%2520in%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%2520have%250Aenabled%2520automated%2520CXR%2520interpretation%252C%2520enhancing%2520diagnostic%2520accuracy%2520and%250Aefficiency.%2520However%252C%2520despite%2520their%2520strong%2520visual%2520understanding%252C%2520current%2520Medical%250ALMMs%2520%2528MLMMs%2529%2520still%2520face%2520two%2520major%2520challenges%253A%2520%25281%2529%2520Insufficient%2520region-level%250Aunderstanding%2520and%2520interaction%252C%2520and%2520%25282%2529%2520Limited%2520accuracy%2520and%2520interpretability%250Adue%2520to%2520single-step%2520reasoning.%2520In%2520this%2520paper%252C%2520we%2520empower%2520MLMMs%2520with%250Aanatomy-centric%2520reasoning%2520capabilities%2520to%2520enhance%2520their%2520interactivity%2520and%250Aexplainability.%2520Specifically%252C%2520we%2520first%2520propose%2520an%2520Anatomical%2520Ontology-Guided%250AReasoning%2520%2528AOR%2529%2520framework%252C%2520which%2520centers%2520on%2520cross-modal%2520region-level%250Ainformation%2520to%2520facilitate%2520multi-step%2520reasoning.%2520Next%252C%2520under%2520the%2520guidance%2520of%250Aexpert%2520physicians%252C%2520we%2520develop%2520AOR-Instruction%252C%2520a%2520large%2520instruction%2520dataset%2520for%250AMLMMs%2520training.%2520Our%2520experiments%2520demonstrate%2520AOR%2527s%2520superior%2520performance%2520in%2520both%250AVQA%2520and%2520report%2520generation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02830v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AOR%3A%20Anatomical%20Ontology-Guided%20Reasoning%20for%20Medical%20Large%20Multimodal%0A%20%20Model%20in%20Chest%20X-Ray%20Interpretation&entry.906535625=Qingqiu%20Li%20and%20Zihang%20Cui%20and%20Seongsu%20Bae%20and%20Jilan%20Xu%20and%20Runtian%20Yuan%20and%20Yuejie%20Zhang%20and%20Rui%20Feng%20and%20Quanli%20Shen%20and%20Xiaobo%20Zhang%20and%20Junjun%20He%20and%20Shujun%20Wang&entry.1292438233=%20%20Chest%20X-rays%20%28CXRs%29%20are%20the%20most%20frequently%20performed%20imaging%20examinations%20in%0Aclinical%20settings.%20Recent%20advancements%20in%20Large%20Multimodal%20Models%20%28LMMs%29%20have%0Aenabled%20automated%20CXR%20interpretation%2C%20enhancing%20diagnostic%20accuracy%20and%0Aefficiency.%20However%2C%20despite%20their%20strong%20visual%20understanding%2C%20current%20Medical%0ALMMs%20%28MLMMs%29%20still%20face%20two%20major%20challenges%3A%20%281%29%20Insufficient%20region-level%0Aunderstanding%20and%20interaction%2C%20and%20%282%29%20Limited%20accuracy%20and%20interpretability%0Adue%20to%20single-step%20reasoning.%20In%20this%20paper%2C%20we%20empower%20MLMMs%20with%0Aanatomy-centric%20reasoning%20capabilities%20to%20enhance%20their%20interactivity%20and%0Aexplainability.%20Specifically%2C%20we%20first%20propose%20an%20Anatomical%20Ontology-Guided%0AReasoning%20%28AOR%29%20framework%2C%20which%20centers%20on%20cross-modal%20region-level%0Ainformation%20to%20facilitate%20multi-step%20reasoning.%20Next%2C%20under%20the%20guidance%20of%0Aexpert%20physicians%2C%20we%20develop%20AOR-Instruction%2C%20a%20large%20instruction%20dataset%20for%0AMLMMs%20training.%20Our%20experiments%20demonstrate%20AOR%27s%20superior%20performance%20in%20both%0AVQA%20and%20report%20generation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02830v1&entry.124074799=Read"},
{"title": "Voila: Voice-Language Foundation Models for Real-Time Autonomous\n  Interaction and Voice Role-Play", "author": "Yemin Shi and Yu Shu and Siwei Dong and Guangyi Liu and Jaward Sesay and Jingwen Li and Zhiting Hu", "abstract": "  A voice AI agent that blends seamlessly into daily life would interact with\nhumans in an autonomous, real-time, and emotionally expressive manner. Rather\nthan merely reacting to commands, it would continuously listen, reason, and\nrespond proactively, fostering fluid, dynamic, and emotionally resonant\ninteractions. We introduce Voila, a family of large voice-language foundation\nmodels that make a step towards this vision. Voila moves beyond traditional\npipeline systems by adopting a new end-to-end architecture that enables\nfull-duplex, low-latency conversations while preserving rich vocal nuances such\nas tone, rhythm, and emotion. It achieves a response latency of just 195\nmilliseconds, surpassing the average human response time. Its hierarchical\nmulti-scale Transformer integrates the reasoning capabilities of large language\nmodels (LLMs) with powerful acoustic modeling, enabling natural, persona-aware\nvoice generation -- where users can simply write text instructions to define\nthe speaker's identity, tone, and other characteristics. Moreover, Voila\nsupports over one million pre-built voices and efficient customization of new\nones from brief audio samples as short as 10 seconds. Beyond spoken dialogue,\nVoila is designed as a unified model for a wide range of voice-based\napplications, including automatic speech recognition (ASR), Text-to-Speech\n(TTS), and, with minimal adaptation, multilingual speech translation. Voila is\nfully open-sourced to support open research and accelerate progress toward\nnext-generation human-machine interactions.\n", "link": "http://arxiv.org/abs/2505.02707v1", "date": "2025-05-05", "relevancy": 2.6803, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5425}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5425}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5232}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Voila%3A%20Voice-Language%20Foundation%20Models%20for%20Real-Time%20Autonomous%0A%20%20Interaction%20and%20Voice%20Role-Play&body=Title%3A%20Voila%3A%20Voice-Language%20Foundation%20Models%20for%20Real-Time%20Autonomous%0A%20%20Interaction%20and%20Voice%20Role-Play%0AAuthor%3A%20Yemin%20Shi%20and%20Yu%20Shu%20and%20Siwei%20Dong%20and%20Guangyi%20Liu%20and%20Jaward%20Sesay%20and%20Jingwen%20Li%20and%20Zhiting%20Hu%0AAbstract%3A%20%20%20A%20voice%20AI%20agent%20that%20blends%20seamlessly%20into%20daily%20life%20would%20interact%20with%0Ahumans%20in%20an%20autonomous%2C%20real-time%2C%20and%20emotionally%20expressive%20manner.%20Rather%0Athan%20merely%20reacting%20to%20commands%2C%20it%20would%20continuously%20listen%2C%20reason%2C%20and%0Arespond%20proactively%2C%20fostering%20fluid%2C%20dynamic%2C%20and%20emotionally%20resonant%0Ainteractions.%20We%20introduce%20Voila%2C%20a%20family%20of%20large%20voice-language%20foundation%0Amodels%20that%20make%20a%20step%20towards%20this%20vision.%20Voila%20moves%20beyond%20traditional%0Apipeline%20systems%20by%20adopting%20a%20new%20end-to-end%20architecture%20that%20enables%0Afull-duplex%2C%20low-latency%20conversations%20while%20preserving%20rich%20vocal%20nuances%20such%0Aas%20tone%2C%20rhythm%2C%20and%20emotion.%20It%20achieves%20a%20response%20latency%20of%20just%20195%0Amilliseconds%2C%20surpassing%20the%20average%20human%20response%20time.%20Its%20hierarchical%0Amulti-scale%20Transformer%20integrates%20the%20reasoning%20capabilities%20of%20large%20language%0Amodels%20%28LLMs%29%20with%20powerful%20acoustic%20modeling%2C%20enabling%20natural%2C%20persona-aware%0Avoice%20generation%20--%20where%20users%20can%20simply%20write%20text%20instructions%20to%20define%0Athe%20speaker%27s%20identity%2C%20tone%2C%20and%20other%20characteristics.%20Moreover%2C%20Voila%0Asupports%20over%20one%20million%20pre-built%20voices%20and%20efficient%20customization%20of%20new%0Aones%20from%20brief%20audio%20samples%20as%20short%20as%2010%20seconds.%20Beyond%20spoken%20dialogue%2C%0AVoila%20is%20designed%20as%20a%20unified%20model%20for%20a%20wide%20range%20of%20voice-based%0Aapplications%2C%20including%20automatic%20speech%20recognition%20%28ASR%29%2C%20Text-to-Speech%0A%28TTS%29%2C%20and%2C%20with%20minimal%20adaptation%2C%20multilingual%20speech%20translation.%20Voila%20is%0Afully%20open-sourced%20to%20support%20open%20research%20and%20accelerate%20progress%20toward%0Anext-generation%20human-machine%20interactions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02707v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVoila%253A%2520Voice-Language%2520Foundation%2520Models%2520for%2520Real-Time%2520Autonomous%250A%2520%2520Interaction%2520and%2520Voice%2520Role-Play%26entry.906535625%3DYemin%2520Shi%2520and%2520Yu%2520Shu%2520and%2520Siwei%2520Dong%2520and%2520Guangyi%2520Liu%2520and%2520Jaward%2520Sesay%2520and%2520Jingwen%2520Li%2520and%2520Zhiting%2520Hu%26entry.1292438233%3D%2520%2520A%2520voice%2520AI%2520agent%2520that%2520blends%2520seamlessly%2520into%2520daily%2520life%2520would%2520interact%2520with%250Ahumans%2520in%2520an%2520autonomous%252C%2520real-time%252C%2520and%2520emotionally%2520expressive%2520manner.%2520Rather%250Athan%2520merely%2520reacting%2520to%2520commands%252C%2520it%2520would%2520continuously%2520listen%252C%2520reason%252C%2520and%250Arespond%2520proactively%252C%2520fostering%2520fluid%252C%2520dynamic%252C%2520and%2520emotionally%2520resonant%250Ainteractions.%2520We%2520introduce%2520Voila%252C%2520a%2520family%2520of%2520large%2520voice-language%2520foundation%250Amodels%2520that%2520make%2520a%2520step%2520towards%2520this%2520vision.%2520Voila%2520moves%2520beyond%2520traditional%250Apipeline%2520systems%2520by%2520adopting%2520a%2520new%2520end-to-end%2520architecture%2520that%2520enables%250Afull-duplex%252C%2520low-latency%2520conversations%2520while%2520preserving%2520rich%2520vocal%2520nuances%2520such%250Aas%2520tone%252C%2520rhythm%252C%2520and%2520emotion.%2520It%2520achieves%2520a%2520response%2520latency%2520of%2520just%2520195%250Amilliseconds%252C%2520surpassing%2520the%2520average%2520human%2520response%2520time.%2520Its%2520hierarchical%250Amulti-scale%2520Transformer%2520integrates%2520the%2520reasoning%2520capabilities%2520of%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520with%2520powerful%2520acoustic%2520modeling%252C%2520enabling%2520natural%252C%2520persona-aware%250Avoice%2520generation%2520--%2520where%2520users%2520can%2520simply%2520write%2520text%2520instructions%2520to%2520define%250Athe%2520speaker%2527s%2520identity%252C%2520tone%252C%2520and%2520other%2520characteristics.%2520Moreover%252C%2520Voila%250Asupports%2520over%2520one%2520million%2520pre-built%2520voices%2520and%2520efficient%2520customization%2520of%2520new%250Aones%2520from%2520brief%2520audio%2520samples%2520as%2520short%2520as%252010%2520seconds.%2520Beyond%2520spoken%2520dialogue%252C%250AVoila%2520is%2520designed%2520as%2520a%2520unified%2520model%2520for%2520a%2520wide%2520range%2520of%2520voice-based%250Aapplications%252C%2520including%2520automatic%2520speech%2520recognition%2520%2528ASR%2529%252C%2520Text-to-Speech%250A%2528TTS%2529%252C%2520and%252C%2520with%2520minimal%2520adaptation%252C%2520multilingual%2520speech%2520translation.%2520Voila%2520is%250Afully%2520open-sourced%2520to%2520support%2520open%2520research%2520and%2520accelerate%2520progress%2520toward%250Anext-generation%2520human-machine%2520interactions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02707v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Voila%3A%20Voice-Language%20Foundation%20Models%20for%20Real-Time%20Autonomous%0A%20%20Interaction%20and%20Voice%20Role-Play&entry.906535625=Yemin%20Shi%20and%20Yu%20Shu%20and%20Siwei%20Dong%20and%20Guangyi%20Liu%20and%20Jaward%20Sesay%20and%20Jingwen%20Li%20and%20Zhiting%20Hu&entry.1292438233=%20%20A%20voice%20AI%20agent%20that%20blends%20seamlessly%20into%20daily%20life%20would%20interact%20with%0Ahumans%20in%20an%20autonomous%2C%20real-time%2C%20and%20emotionally%20expressive%20manner.%20Rather%0Athan%20merely%20reacting%20to%20commands%2C%20it%20would%20continuously%20listen%2C%20reason%2C%20and%0Arespond%20proactively%2C%20fostering%20fluid%2C%20dynamic%2C%20and%20emotionally%20resonant%0Ainteractions.%20We%20introduce%20Voila%2C%20a%20family%20of%20large%20voice-language%20foundation%0Amodels%20that%20make%20a%20step%20towards%20this%20vision.%20Voila%20moves%20beyond%20traditional%0Apipeline%20systems%20by%20adopting%20a%20new%20end-to-end%20architecture%20that%20enables%0Afull-duplex%2C%20low-latency%20conversations%20while%20preserving%20rich%20vocal%20nuances%20such%0Aas%20tone%2C%20rhythm%2C%20and%20emotion.%20It%20achieves%20a%20response%20latency%20of%20just%20195%0Amilliseconds%2C%20surpassing%20the%20average%20human%20response%20time.%20Its%20hierarchical%0Amulti-scale%20Transformer%20integrates%20the%20reasoning%20capabilities%20of%20large%20language%0Amodels%20%28LLMs%29%20with%20powerful%20acoustic%20modeling%2C%20enabling%20natural%2C%20persona-aware%0Avoice%20generation%20--%20where%20users%20can%20simply%20write%20text%20instructions%20to%20define%0Athe%20speaker%27s%20identity%2C%20tone%2C%20and%20other%20characteristics.%20Moreover%2C%20Voila%0Asupports%20over%20one%20million%20pre-built%20voices%20and%20efficient%20customization%20of%20new%0Aones%20from%20brief%20audio%20samples%20as%20short%20as%2010%20seconds.%20Beyond%20spoken%20dialogue%2C%0AVoila%20is%20designed%20as%20a%20unified%20model%20for%20a%20wide%20range%20of%20voice-based%0Aapplications%2C%20including%20automatic%20speech%20recognition%20%28ASR%29%2C%20Text-to-Speech%0A%28TTS%29%2C%20and%2C%20with%20minimal%20adaptation%2C%20multilingual%20speech%20translation.%20Voila%20is%0Afully%20open-sourced%20to%20support%20open%20research%20and%20accelerate%20progress%20toward%0Anext-generation%20human-machine%20interactions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02707v1&entry.124074799=Read"},
{"title": "Beyond the Monitor: Mixed Reality Visualization and AI for Enhanced\n  Digital Pathology Workflow", "author": "Jai Prakash Veerla and Partha Sai Guttikonda and Helen H. Shang and Mohammad Sadegh Nasr and Cesar Torres and Jacob M. Luber", "abstract": "  Pathologists rely on gigapixel whole-slide images (WSIs) to diagnose diseases\nlike cancer, yet current digital pathology tools hinder diagnosis. The immense\nscale of WSIs, often exceeding 100,000 X 100,000 pixels, clashes with the\nlimited views traditional monitors offer. This mismatch forces constant panning\nand zooming, increasing pathologist cognitive load, causing diagnostic fatigue,\nand slowing pathologists' adoption of digital methods. PathVis, our\nmixed-reality visualization platform for Apple Vision Pro, addresses these\nchallenges. It transforms the pathologist's interaction with data, replacing\ncumbersome mouse-and-monitor navigation with intuitive exploration using\nnatural hand gestures, eye gaze, and voice commands in an immersive workspace.\nPathVis integrates AI to enhance diagnosis. An AI-driven search function\ninstantly retrieves and displays the top five similar patient cases\nside-by-side, improving diagnostic precision and efficiency through rapid\ncomparison. Additionally, a multimodal conversational AI assistant offers\nreal-time image interpretation support and aids collaboration among\npathologists across multiple Apple devices. By merging the directness of\ntraditional pathology with advanced mixed-reality visualization and AI, PathVis\nimproves diagnostic workflows, reduces cognitive strain, and makes pathology\npractice more effective and engaging. The PathVis source code and a demo video\nare publicly available at: https://github.com/jaiprakash1824/Path_Vis\n", "link": "http://arxiv.org/abs/2505.02780v1", "date": "2025-05-05", "relevancy": 2.6777, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5468}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5468}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20the%20Monitor%3A%20Mixed%20Reality%20Visualization%20and%20AI%20for%20Enhanced%0A%20%20Digital%20Pathology%20Workflow&body=Title%3A%20Beyond%20the%20Monitor%3A%20Mixed%20Reality%20Visualization%20and%20AI%20for%20Enhanced%0A%20%20Digital%20Pathology%20Workflow%0AAuthor%3A%20Jai%20Prakash%20Veerla%20and%20Partha%20Sai%20Guttikonda%20and%20Helen%20H.%20Shang%20and%20Mohammad%20Sadegh%20Nasr%20and%20Cesar%20Torres%20and%20Jacob%20M.%20Luber%0AAbstract%3A%20%20%20Pathologists%20rely%20on%20gigapixel%20whole-slide%20images%20%28WSIs%29%20to%20diagnose%20diseases%0Alike%20cancer%2C%20yet%20current%20digital%20pathology%20tools%20hinder%20diagnosis.%20The%20immense%0Ascale%20of%20WSIs%2C%20often%20exceeding%20100%2C000%20X%20100%2C000%20pixels%2C%20clashes%20with%20the%0Alimited%20views%20traditional%20monitors%20offer.%20This%20mismatch%20forces%20constant%20panning%0Aand%20zooming%2C%20increasing%20pathologist%20cognitive%20load%2C%20causing%20diagnostic%20fatigue%2C%0Aand%20slowing%20pathologists%27%20adoption%20of%20digital%20methods.%20PathVis%2C%20our%0Amixed-reality%20visualization%20platform%20for%20Apple%20Vision%20Pro%2C%20addresses%20these%0Achallenges.%20It%20transforms%20the%20pathologist%27s%20interaction%20with%20data%2C%20replacing%0Acumbersome%20mouse-and-monitor%20navigation%20with%20intuitive%20exploration%20using%0Anatural%20hand%20gestures%2C%20eye%20gaze%2C%20and%20voice%20commands%20in%20an%20immersive%20workspace.%0APathVis%20integrates%20AI%20to%20enhance%20diagnosis.%20An%20AI-driven%20search%20function%0Ainstantly%20retrieves%20and%20displays%20the%20top%20five%20similar%20patient%20cases%0Aside-by-side%2C%20improving%20diagnostic%20precision%20and%20efficiency%20through%20rapid%0Acomparison.%20Additionally%2C%20a%20multimodal%20conversational%20AI%20assistant%20offers%0Areal-time%20image%20interpretation%20support%20and%20aids%20collaboration%20among%0Apathologists%20across%20multiple%20Apple%20devices.%20By%20merging%20the%20directness%20of%0Atraditional%20pathology%20with%20advanced%20mixed-reality%20visualization%20and%20AI%2C%20PathVis%0Aimproves%20diagnostic%20workflows%2C%20reduces%20cognitive%20strain%2C%20and%20makes%20pathology%0Apractice%20more%20effective%20and%20engaging.%20The%20PathVis%20source%20code%20and%20a%20demo%20video%0Aare%20publicly%20available%20at%3A%20https%3A//github.com/jaiprakash1824/Path_Vis%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02780v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520the%2520Monitor%253A%2520Mixed%2520Reality%2520Visualization%2520and%2520AI%2520for%2520Enhanced%250A%2520%2520Digital%2520Pathology%2520Workflow%26entry.906535625%3DJai%2520Prakash%2520Veerla%2520and%2520Partha%2520Sai%2520Guttikonda%2520and%2520Helen%2520H.%2520Shang%2520and%2520Mohammad%2520Sadegh%2520Nasr%2520and%2520Cesar%2520Torres%2520and%2520Jacob%2520M.%2520Luber%26entry.1292438233%3D%2520%2520Pathologists%2520rely%2520on%2520gigapixel%2520whole-slide%2520images%2520%2528WSIs%2529%2520to%2520diagnose%2520diseases%250Alike%2520cancer%252C%2520yet%2520current%2520digital%2520pathology%2520tools%2520hinder%2520diagnosis.%2520The%2520immense%250Ascale%2520of%2520WSIs%252C%2520often%2520exceeding%2520100%252C000%2520X%2520100%252C000%2520pixels%252C%2520clashes%2520with%2520the%250Alimited%2520views%2520traditional%2520monitors%2520offer.%2520This%2520mismatch%2520forces%2520constant%2520panning%250Aand%2520zooming%252C%2520increasing%2520pathologist%2520cognitive%2520load%252C%2520causing%2520diagnostic%2520fatigue%252C%250Aand%2520slowing%2520pathologists%2527%2520adoption%2520of%2520digital%2520methods.%2520PathVis%252C%2520our%250Amixed-reality%2520visualization%2520platform%2520for%2520Apple%2520Vision%2520Pro%252C%2520addresses%2520these%250Achallenges.%2520It%2520transforms%2520the%2520pathologist%2527s%2520interaction%2520with%2520data%252C%2520replacing%250Acumbersome%2520mouse-and-monitor%2520navigation%2520with%2520intuitive%2520exploration%2520using%250Anatural%2520hand%2520gestures%252C%2520eye%2520gaze%252C%2520and%2520voice%2520commands%2520in%2520an%2520immersive%2520workspace.%250APathVis%2520integrates%2520AI%2520to%2520enhance%2520diagnosis.%2520An%2520AI-driven%2520search%2520function%250Ainstantly%2520retrieves%2520and%2520displays%2520the%2520top%2520five%2520similar%2520patient%2520cases%250Aside-by-side%252C%2520improving%2520diagnostic%2520precision%2520and%2520efficiency%2520through%2520rapid%250Acomparison.%2520Additionally%252C%2520a%2520multimodal%2520conversational%2520AI%2520assistant%2520offers%250Areal-time%2520image%2520interpretation%2520support%2520and%2520aids%2520collaboration%2520among%250Apathologists%2520across%2520multiple%2520Apple%2520devices.%2520By%2520merging%2520the%2520directness%2520of%250Atraditional%2520pathology%2520with%2520advanced%2520mixed-reality%2520visualization%2520and%2520AI%252C%2520PathVis%250Aimproves%2520diagnostic%2520workflows%252C%2520reduces%2520cognitive%2520strain%252C%2520and%2520makes%2520pathology%250Apractice%2520more%2520effective%2520and%2520engaging.%2520The%2520PathVis%2520source%2520code%2520and%2520a%2520demo%2520video%250Aare%2520publicly%2520available%2520at%253A%2520https%253A//github.com/jaiprakash1824/Path_Vis%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02780v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20the%20Monitor%3A%20Mixed%20Reality%20Visualization%20and%20AI%20for%20Enhanced%0A%20%20Digital%20Pathology%20Workflow&entry.906535625=Jai%20Prakash%20Veerla%20and%20Partha%20Sai%20Guttikonda%20and%20Helen%20H.%20Shang%20and%20Mohammad%20Sadegh%20Nasr%20and%20Cesar%20Torres%20and%20Jacob%20M.%20Luber&entry.1292438233=%20%20Pathologists%20rely%20on%20gigapixel%20whole-slide%20images%20%28WSIs%29%20to%20diagnose%20diseases%0Alike%20cancer%2C%20yet%20current%20digital%20pathology%20tools%20hinder%20diagnosis.%20The%20immense%0Ascale%20of%20WSIs%2C%20often%20exceeding%20100%2C000%20X%20100%2C000%20pixels%2C%20clashes%20with%20the%0Alimited%20views%20traditional%20monitors%20offer.%20This%20mismatch%20forces%20constant%20panning%0Aand%20zooming%2C%20increasing%20pathologist%20cognitive%20load%2C%20causing%20diagnostic%20fatigue%2C%0Aand%20slowing%20pathologists%27%20adoption%20of%20digital%20methods.%20PathVis%2C%20our%0Amixed-reality%20visualization%20platform%20for%20Apple%20Vision%20Pro%2C%20addresses%20these%0Achallenges.%20It%20transforms%20the%20pathologist%27s%20interaction%20with%20data%2C%20replacing%0Acumbersome%20mouse-and-monitor%20navigation%20with%20intuitive%20exploration%20using%0Anatural%20hand%20gestures%2C%20eye%20gaze%2C%20and%20voice%20commands%20in%20an%20immersive%20workspace.%0APathVis%20integrates%20AI%20to%20enhance%20diagnosis.%20An%20AI-driven%20search%20function%0Ainstantly%20retrieves%20and%20displays%20the%20top%20five%20similar%20patient%20cases%0Aside-by-side%2C%20improving%20diagnostic%20precision%20and%20efficiency%20through%20rapid%0Acomparison.%20Additionally%2C%20a%20multimodal%20conversational%20AI%20assistant%20offers%0Areal-time%20image%20interpretation%20support%20and%20aids%20collaboration%20among%0Apathologists%20across%20multiple%20Apple%20devices.%20By%20merging%20the%20directness%20of%0Atraditional%20pathology%20with%20advanced%20mixed-reality%20visualization%20and%20AI%2C%20PathVis%0Aimproves%20diagnostic%20workflows%2C%20reduces%20cognitive%20strain%2C%20and%20makes%20pathology%0Apractice%20more%20effective%20and%20engaging.%20The%20PathVis%20source%20code%20and%20a%20demo%20video%0Aare%20publicly%20available%20at%3A%20https%3A//github.com/jaiprakash1824/Path_Vis%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02780v1&entry.124074799=Read"},
{"title": "Database-Agnostic Gait Enrollment using SetTransformers", "author": "Nicoleta Basoc and Adrian Cosma and Andy C\u01cetrun\u01ce and Emilian R\u01cedoi", "abstract": "  Gait recognition has emerged as a powerful tool for unobtrusive and\nlong-range identity analysis, with growing relevance in surveillance and\nmonitoring applications. Although recent advances in deep learning and\nlarge-scale datasets have enabled highly accurate recognition under closed-set\nconditions, real-world deployment demands open-set gait enrollment, which means\ndetermining whether a new gait sample corresponds to a known identity or\nrepresents a previously unseen individual. In this work, we introduce a\ntransformer-based framework for open-set gait enrollment that is both\ndataset-agnostic and recognition-architecture-agnostic. Our method leverages a\nSetTransformer to make enrollment decisions based on the embedding of a probe\nsample and a context set drawn from the gallery, without requiring\ntask-specific thresholds or retraining for new environments. By decoupling\nenrollment from the main recognition pipeline, our model is generalized across\ndifferent datasets, gallery sizes, and identity distributions. We propose an\nevaluation protocol that uses existing datasets in different ratios of\nidentities and walks per identity. We instantiate our method using\nskeleton-based gait representations and evaluate it on two benchmark datasets\n(CASIA-B and PsyMo), using embeddings from three state-of-the-art recognition\nmodels (GaitGraph, GaitFormer, and GaitPT). We show that our method is\nflexible, is able to accurately perform enrollment in different scenarios, and\nscales better with data compared to traditional approaches. We will make the\ncode and dataset scenarios publicly available.\n", "link": "http://arxiv.org/abs/2505.02815v1", "date": "2025-05-05", "relevancy": 2.6702, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5677}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5323}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Database-Agnostic%20Gait%20Enrollment%20using%20SetTransformers&body=Title%3A%20Database-Agnostic%20Gait%20Enrollment%20using%20SetTransformers%0AAuthor%3A%20Nicoleta%20Basoc%20and%20Adrian%20Cosma%20and%20Andy%20C%C7%8Etrun%C7%8E%20and%20Emilian%20R%C7%8Edoi%0AAbstract%3A%20%20%20Gait%20recognition%20has%20emerged%20as%20a%20powerful%20tool%20for%20unobtrusive%20and%0Along-range%20identity%20analysis%2C%20with%20growing%20relevance%20in%20surveillance%20and%0Amonitoring%20applications.%20Although%20recent%20advances%20in%20deep%20learning%20and%0Alarge-scale%20datasets%20have%20enabled%20highly%20accurate%20recognition%20under%20closed-set%0Aconditions%2C%20real-world%20deployment%20demands%20open-set%20gait%20enrollment%2C%20which%20means%0Adetermining%20whether%20a%20new%20gait%20sample%20corresponds%20to%20a%20known%20identity%20or%0Arepresents%20a%20previously%20unseen%20individual.%20In%20this%20work%2C%20we%20introduce%20a%0Atransformer-based%20framework%20for%20open-set%20gait%20enrollment%20that%20is%20both%0Adataset-agnostic%20and%20recognition-architecture-agnostic.%20Our%20method%20leverages%20a%0ASetTransformer%20to%20make%20enrollment%20decisions%20based%20on%20the%20embedding%20of%20a%20probe%0Asample%20and%20a%20context%20set%20drawn%20from%20the%20gallery%2C%20without%20requiring%0Atask-specific%20thresholds%20or%20retraining%20for%20new%20environments.%20By%20decoupling%0Aenrollment%20from%20the%20main%20recognition%20pipeline%2C%20our%20model%20is%20generalized%20across%0Adifferent%20datasets%2C%20gallery%20sizes%2C%20and%20identity%20distributions.%20We%20propose%20an%0Aevaluation%20protocol%20that%20uses%20existing%20datasets%20in%20different%20ratios%20of%0Aidentities%20and%20walks%20per%20identity.%20We%20instantiate%20our%20method%20using%0Askeleton-based%20gait%20representations%20and%20evaluate%20it%20on%20two%20benchmark%20datasets%0A%28CASIA-B%20and%20PsyMo%29%2C%20using%20embeddings%20from%20three%20state-of-the-art%20recognition%0Amodels%20%28GaitGraph%2C%20GaitFormer%2C%20and%20GaitPT%29.%20We%20show%20that%20our%20method%20is%0Aflexible%2C%20is%20able%20to%20accurately%20perform%20enrollment%20in%20different%20scenarios%2C%20and%0Ascales%20better%20with%20data%20compared%20to%20traditional%20approaches.%20We%20will%20make%20the%0Acode%20and%20dataset%20scenarios%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02815v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDatabase-Agnostic%2520Gait%2520Enrollment%2520using%2520SetTransformers%26entry.906535625%3DNicoleta%2520Basoc%2520and%2520Adrian%2520Cosma%2520and%2520Andy%2520C%25C7%258Etrun%25C7%258E%2520and%2520Emilian%2520R%25C7%258Edoi%26entry.1292438233%3D%2520%2520Gait%2520recognition%2520has%2520emerged%2520as%2520a%2520powerful%2520tool%2520for%2520unobtrusive%2520and%250Along-range%2520identity%2520analysis%252C%2520with%2520growing%2520relevance%2520in%2520surveillance%2520and%250Amonitoring%2520applications.%2520Although%2520recent%2520advances%2520in%2520deep%2520learning%2520and%250Alarge-scale%2520datasets%2520have%2520enabled%2520highly%2520accurate%2520recognition%2520under%2520closed-set%250Aconditions%252C%2520real-world%2520deployment%2520demands%2520open-set%2520gait%2520enrollment%252C%2520which%2520means%250Adetermining%2520whether%2520a%2520new%2520gait%2520sample%2520corresponds%2520to%2520a%2520known%2520identity%2520or%250Arepresents%2520a%2520previously%2520unseen%2520individual.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%250Atransformer-based%2520framework%2520for%2520open-set%2520gait%2520enrollment%2520that%2520is%2520both%250Adataset-agnostic%2520and%2520recognition-architecture-agnostic.%2520Our%2520method%2520leverages%2520a%250ASetTransformer%2520to%2520make%2520enrollment%2520decisions%2520based%2520on%2520the%2520embedding%2520of%2520a%2520probe%250Asample%2520and%2520a%2520context%2520set%2520drawn%2520from%2520the%2520gallery%252C%2520without%2520requiring%250Atask-specific%2520thresholds%2520or%2520retraining%2520for%2520new%2520environments.%2520By%2520decoupling%250Aenrollment%2520from%2520the%2520main%2520recognition%2520pipeline%252C%2520our%2520model%2520is%2520generalized%2520across%250Adifferent%2520datasets%252C%2520gallery%2520sizes%252C%2520and%2520identity%2520distributions.%2520We%2520propose%2520an%250Aevaluation%2520protocol%2520that%2520uses%2520existing%2520datasets%2520in%2520different%2520ratios%2520of%250Aidentities%2520and%2520walks%2520per%2520identity.%2520We%2520instantiate%2520our%2520method%2520using%250Askeleton-based%2520gait%2520representations%2520and%2520evaluate%2520it%2520on%2520two%2520benchmark%2520datasets%250A%2528CASIA-B%2520and%2520PsyMo%2529%252C%2520using%2520embeddings%2520from%2520three%2520state-of-the-art%2520recognition%250Amodels%2520%2528GaitGraph%252C%2520GaitFormer%252C%2520and%2520GaitPT%2529.%2520We%2520show%2520that%2520our%2520method%2520is%250Aflexible%252C%2520is%2520able%2520to%2520accurately%2520perform%2520enrollment%2520in%2520different%2520scenarios%252C%2520and%250Ascales%2520better%2520with%2520data%2520compared%2520to%2520traditional%2520approaches.%2520We%2520will%2520make%2520the%250Acode%2520and%2520dataset%2520scenarios%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02815v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Database-Agnostic%20Gait%20Enrollment%20using%20SetTransformers&entry.906535625=Nicoleta%20Basoc%20and%20Adrian%20Cosma%20and%20Andy%20C%C7%8Etrun%C7%8E%20and%20Emilian%20R%C7%8Edoi&entry.1292438233=%20%20Gait%20recognition%20has%20emerged%20as%20a%20powerful%20tool%20for%20unobtrusive%20and%0Along-range%20identity%20analysis%2C%20with%20growing%20relevance%20in%20surveillance%20and%0Amonitoring%20applications.%20Although%20recent%20advances%20in%20deep%20learning%20and%0Alarge-scale%20datasets%20have%20enabled%20highly%20accurate%20recognition%20under%20closed-set%0Aconditions%2C%20real-world%20deployment%20demands%20open-set%20gait%20enrollment%2C%20which%20means%0Adetermining%20whether%20a%20new%20gait%20sample%20corresponds%20to%20a%20known%20identity%20or%0Arepresents%20a%20previously%20unseen%20individual.%20In%20this%20work%2C%20we%20introduce%20a%0Atransformer-based%20framework%20for%20open-set%20gait%20enrollment%20that%20is%20both%0Adataset-agnostic%20and%20recognition-architecture-agnostic.%20Our%20method%20leverages%20a%0ASetTransformer%20to%20make%20enrollment%20decisions%20based%20on%20the%20embedding%20of%20a%20probe%0Asample%20and%20a%20context%20set%20drawn%20from%20the%20gallery%2C%20without%20requiring%0Atask-specific%20thresholds%20or%20retraining%20for%20new%20environments.%20By%20decoupling%0Aenrollment%20from%20the%20main%20recognition%20pipeline%2C%20our%20model%20is%20generalized%20across%0Adifferent%20datasets%2C%20gallery%20sizes%2C%20and%20identity%20distributions.%20We%20propose%20an%0Aevaluation%20protocol%20that%20uses%20existing%20datasets%20in%20different%20ratios%20of%0Aidentities%20and%20walks%20per%20identity.%20We%20instantiate%20our%20method%20using%0Askeleton-based%20gait%20representations%20and%20evaluate%20it%20on%20two%20benchmark%20datasets%0A%28CASIA-B%20and%20PsyMo%29%2C%20using%20embeddings%20from%20three%20state-of-the-art%20recognition%0Amodels%20%28GaitGraph%2C%20GaitFormer%2C%20and%20GaitPT%29.%20We%20show%20that%20our%20method%20is%0Aflexible%2C%20is%20able%20to%20accurately%20perform%20enrollment%20in%20different%20scenarios%2C%20and%0Ascales%20better%20with%20data%20compared%20to%20traditional%20approaches.%20We%20will%20make%20the%0Acode%20and%20dataset%20scenarios%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02815v1&entry.124074799=Read"},
{"title": "landmarker: a Toolkit for Anatomical Landmark Localization in 2D/3D\n  Images", "author": "Jef Jonkers and Luc Duchateau and Glenn Van Wallendael and Sofie Van Hoecke", "abstract": "  Anatomical landmark localization in 2D/3D images is a critical task in\nmedical imaging. Although many general-purpose tools exist for landmark\nlocalization in classical computer vision tasks, such as pose estimation, they\nlack the specialized features and modularity necessary for anatomical landmark\nlocalization applications in the medical domain. Therefore, we introduce\nlandmarker, a Python package built on PyTorch. The package provides a\ncomprehensive, flexible toolkit for developing and evaluating landmark\nlocalization algorithms, supporting a range of methodologies, including static\nand adaptive heatmap regression. landmarker enhances the accuracy of landmark\nidentification, streamlines research and development processes, and supports\nvarious image formats and preprocessing pipelines. Its modular design allows\nusers to customize and extend the toolkit for specific datasets and\napplications, accelerating innovation in medical imaging. landmarker addresses\na critical need for precision and customization in landmark localization tasks\nnot adequately met by existing general-purpose pose estimation tools.\n", "link": "http://arxiv.org/abs/2501.10098v2", "date": "2025-05-05", "relevancy": 2.6509, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.551}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5204}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5192}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20landmarker%3A%20a%20Toolkit%20for%20Anatomical%20Landmark%20Localization%20in%202D/3D%0A%20%20Images&body=Title%3A%20landmarker%3A%20a%20Toolkit%20for%20Anatomical%20Landmark%20Localization%20in%202D/3D%0A%20%20Images%0AAuthor%3A%20Jef%20Jonkers%20and%20Luc%20Duchateau%20and%20Glenn%20Van%20Wallendael%20and%20Sofie%20Van%20Hoecke%0AAbstract%3A%20%20%20Anatomical%20landmark%20localization%20in%202D/3D%20images%20is%20a%20critical%20task%20in%0Amedical%20imaging.%20Although%20many%20general-purpose%20tools%20exist%20for%20landmark%0Alocalization%20in%20classical%20computer%20vision%20tasks%2C%20such%20as%20pose%20estimation%2C%20they%0Alack%20the%20specialized%20features%20and%20modularity%20necessary%20for%20anatomical%20landmark%0Alocalization%20applications%20in%20the%20medical%20domain.%20Therefore%2C%20we%20introduce%0Alandmarker%2C%20a%20Python%20package%20built%20on%20PyTorch.%20The%20package%20provides%20a%0Acomprehensive%2C%20flexible%20toolkit%20for%20developing%20and%20evaluating%20landmark%0Alocalization%20algorithms%2C%20supporting%20a%20range%20of%20methodologies%2C%20including%20static%0Aand%20adaptive%20heatmap%20regression.%20landmarker%20enhances%20the%20accuracy%20of%20landmark%0Aidentification%2C%20streamlines%20research%20and%20development%20processes%2C%20and%20supports%0Avarious%20image%20formats%20and%20preprocessing%20pipelines.%20Its%20modular%20design%20allows%0Ausers%20to%20customize%20and%20extend%20the%20toolkit%20for%20specific%20datasets%20and%0Aapplications%2C%20accelerating%20innovation%20in%20medical%20imaging.%20landmarker%20addresses%0Aa%20critical%20need%20for%20precision%20and%20customization%20in%20landmark%20localization%20tasks%0Anot%20adequately%20met%20by%20existing%20general-purpose%20pose%20estimation%20tools.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10098v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dlandmarker%253A%2520a%2520Toolkit%2520for%2520Anatomical%2520Landmark%2520Localization%2520in%25202D/3D%250A%2520%2520Images%26entry.906535625%3DJef%2520Jonkers%2520and%2520Luc%2520Duchateau%2520and%2520Glenn%2520Van%2520Wallendael%2520and%2520Sofie%2520Van%2520Hoecke%26entry.1292438233%3D%2520%2520Anatomical%2520landmark%2520localization%2520in%25202D/3D%2520images%2520is%2520a%2520critical%2520task%2520in%250Amedical%2520imaging.%2520Although%2520many%2520general-purpose%2520tools%2520exist%2520for%2520landmark%250Alocalization%2520in%2520classical%2520computer%2520vision%2520tasks%252C%2520such%2520as%2520pose%2520estimation%252C%2520they%250Alack%2520the%2520specialized%2520features%2520and%2520modularity%2520necessary%2520for%2520anatomical%2520landmark%250Alocalization%2520applications%2520in%2520the%2520medical%2520domain.%2520Therefore%252C%2520we%2520introduce%250Alandmarker%252C%2520a%2520Python%2520package%2520built%2520on%2520PyTorch.%2520The%2520package%2520provides%2520a%250Acomprehensive%252C%2520flexible%2520toolkit%2520for%2520developing%2520and%2520evaluating%2520landmark%250Alocalization%2520algorithms%252C%2520supporting%2520a%2520range%2520of%2520methodologies%252C%2520including%2520static%250Aand%2520adaptive%2520heatmap%2520regression.%2520landmarker%2520enhances%2520the%2520accuracy%2520of%2520landmark%250Aidentification%252C%2520streamlines%2520research%2520and%2520development%2520processes%252C%2520and%2520supports%250Avarious%2520image%2520formats%2520and%2520preprocessing%2520pipelines.%2520Its%2520modular%2520design%2520allows%250Ausers%2520to%2520customize%2520and%2520extend%2520the%2520toolkit%2520for%2520specific%2520datasets%2520and%250Aapplications%252C%2520accelerating%2520innovation%2520in%2520medical%2520imaging.%2520landmarker%2520addresses%250Aa%2520critical%2520need%2520for%2520precision%2520and%2520customization%2520in%2520landmark%2520localization%2520tasks%250Anot%2520adequately%2520met%2520by%2520existing%2520general-purpose%2520pose%2520estimation%2520tools.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10098v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=landmarker%3A%20a%20Toolkit%20for%20Anatomical%20Landmark%20Localization%20in%202D/3D%0A%20%20Images&entry.906535625=Jef%20Jonkers%20and%20Luc%20Duchateau%20and%20Glenn%20Van%20Wallendael%20and%20Sofie%20Van%20Hoecke&entry.1292438233=%20%20Anatomical%20landmark%20localization%20in%202D/3D%20images%20is%20a%20critical%20task%20in%0Amedical%20imaging.%20Although%20many%20general-purpose%20tools%20exist%20for%20landmark%0Alocalization%20in%20classical%20computer%20vision%20tasks%2C%20such%20as%20pose%20estimation%2C%20they%0Alack%20the%20specialized%20features%20and%20modularity%20necessary%20for%20anatomical%20landmark%0Alocalization%20applications%20in%20the%20medical%20domain.%20Therefore%2C%20we%20introduce%0Alandmarker%2C%20a%20Python%20package%20built%20on%20PyTorch.%20The%20package%20provides%20a%0Acomprehensive%2C%20flexible%20toolkit%20for%20developing%20and%20evaluating%20landmark%0Alocalization%20algorithms%2C%20supporting%20a%20range%20of%20methodologies%2C%20including%20static%0Aand%20adaptive%20heatmap%20regression.%20landmarker%20enhances%20the%20accuracy%20of%20landmark%0Aidentification%2C%20streamlines%20research%20and%20development%20processes%2C%20and%20supports%0Avarious%20image%20formats%20and%20preprocessing%20pipelines.%20Its%20modular%20design%20allows%0Ausers%20to%20customize%20and%20extend%20the%20toolkit%20for%20specific%20datasets%20and%0Aapplications%2C%20accelerating%20innovation%20in%20medical%20imaging.%20landmarker%20addresses%0Aa%20critical%20need%20for%20precision%20and%20customization%20in%20landmark%20localization%20tasks%0Anot%20adequately%20met%20by%20existing%20general-purpose%20pose%20estimation%20tools.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10098v2&entry.124074799=Read"},
{"title": "Using Knowledge Graphs to harvest datasets for efficient CLIP model\n  training", "author": "Simon Ging and Sebastian Walter and Jelena Bratuli\u0107 and Johannes Dienert and Hannah Bast and Thomas Brox", "abstract": "  Training high-quality CLIP models typically requires enormous datasets, which\nlimits the development of domain-specific models -- especially in areas that\neven the largest CLIP models do not cover well -- and drives up training costs.\nThis poses challenges for scientific research that needs fine-grained control\nover the training procedure of CLIP models. In this work, we show that by\nemploying smart web search strategies enhanced with knowledge graphs, a robust\nCLIP model can be trained from scratch with considerably less data.\nSpecifically, we demonstrate that an expert foundation model for living\norganisms can be built using just 10M images. Moreover, we introduce EntityNet,\na dataset comprising 33M images paired with 46M text descriptions, which\nenables the training of a generic CLIP model in significantly reduced time.\n", "link": "http://arxiv.org/abs/2505.02746v1", "date": "2025-05-05", "relevancy": 2.5205, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5043}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.504}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Using%20Knowledge%20Graphs%20to%20harvest%20datasets%20for%20efficient%20CLIP%20model%0A%20%20training&body=Title%3A%20Using%20Knowledge%20Graphs%20to%20harvest%20datasets%20for%20efficient%20CLIP%20model%0A%20%20training%0AAuthor%3A%20Simon%20Ging%20and%20Sebastian%20Walter%20and%20Jelena%20Bratuli%C4%87%20and%20Johannes%20Dienert%20and%20Hannah%20Bast%20and%20Thomas%20Brox%0AAbstract%3A%20%20%20Training%20high-quality%20CLIP%20models%20typically%20requires%20enormous%20datasets%2C%20which%0Alimits%20the%20development%20of%20domain-specific%20models%20--%20especially%20in%20areas%20that%0Aeven%20the%20largest%20CLIP%20models%20do%20not%20cover%20well%20--%20and%20drives%20up%20training%20costs.%0AThis%20poses%20challenges%20for%20scientific%20research%20that%20needs%20fine-grained%20control%0Aover%20the%20training%20procedure%20of%20CLIP%20models.%20In%20this%20work%2C%20we%20show%20that%20by%0Aemploying%20smart%20web%20search%20strategies%20enhanced%20with%20knowledge%20graphs%2C%20a%20robust%0ACLIP%20model%20can%20be%20trained%20from%20scratch%20with%20considerably%20less%20data.%0ASpecifically%2C%20we%20demonstrate%20that%20an%20expert%20foundation%20model%20for%20living%0Aorganisms%20can%20be%20built%20using%20just%2010M%20images.%20Moreover%2C%20we%20introduce%20EntityNet%2C%0Aa%20dataset%20comprising%2033M%20images%20paired%20with%2046M%20text%20descriptions%2C%20which%0Aenables%20the%20training%20of%20a%20generic%20CLIP%20model%20in%20significantly%20reduced%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02746v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUsing%2520Knowledge%2520Graphs%2520to%2520harvest%2520datasets%2520for%2520efficient%2520CLIP%2520model%250A%2520%2520training%26entry.906535625%3DSimon%2520Ging%2520and%2520Sebastian%2520Walter%2520and%2520Jelena%2520Bratuli%25C4%2587%2520and%2520Johannes%2520Dienert%2520and%2520Hannah%2520Bast%2520and%2520Thomas%2520Brox%26entry.1292438233%3D%2520%2520Training%2520high-quality%2520CLIP%2520models%2520typically%2520requires%2520enormous%2520datasets%252C%2520which%250Alimits%2520the%2520development%2520of%2520domain-specific%2520models%2520--%2520especially%2520in%2520areas%2520that%250Aeven%2520the%2520largest%2520CLIP%2520models%2520do%2520not%2520cover%2520well%2520--%2520and%2520drives%2520up%2520training%2520costs.%250AThis%2520poses%2520challenges%2520for%2520scientific%2520research%2520that%2520needs%2520fine-grained%2520control%250Aover%2520the%2520training%2520procedure%2520of%2520CLIP%2520models.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520by%250Aemploying%2520smart%2520web%2520search%2520strategies%2520enhanced%2520with%2520knowledge%2520graphs%252C%2520a%2520robust%250ACLIP%2520model%2520can%2520be%2520trained%2520from%2520scratch%2520with%2520considerably%2520less%2520data.%250ASpecifically%252C%2520we%2520demonstrate%2520that%2520an%2520expert%2520foundation%2520model%2520for%2520living%250Aorganisms%2520can%2520be%2520built%2520using%2520just%252010M%2520images.%2520Moreover%252C%2520we%2520introduce%2520EntityNet%252C%250Aa%2520dataset%2520comprising%252033M%2520images%2520paired%2520with%252046M%2520text%2520descriptions%252C%2520which%250Aenables%2520the%2520training%2520of%2520a%2520generic%2520CLIP%2520model%2520in%2520significantly%2520reduced%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02746v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20Knowledge%20Graphs%20to%20harvest%20datasets%20for%20efficient%20CLIP%20model%0A%20%20training&entry.906535625=Simon%20Ging%20and%20Sebastian%20Walter%20and%20Jelena%20Bratuli%C4%87%20and%20Johannes%20Dienert%20and%20Hannah%20Bast%20and%20Thomas%20Brox&entry.1292438233=%20%20Training%20high-quality%20CLIP%20models%20typically%20requires%20enormous%20datasets%2C%20which%0Alimits%20the%20development%20of%20domain-specific%20models%20--%20especially%20in%20areas%20that%0Aeven%20the%20largest%20CLIP%20models%20do%20not%20cover%20well%20--%20and%20drives%20up%20training%20costs.%0AThis%20poses%20challenges%20for%20scientific%20research%20that%20needs%20fine-grained%20control%0Aover%20the%20training%20procedure%20of%20CLIP%20models.%20In%20this%20work%2C%20we%20show%20that%20by%0Aemploying%20smart%20web%20search%20strategies%20enhanced%20with%20knowledge%20graphs%2C%20a%20robust%0ACLIP%20model%20can%20be%20trained%20from%20scratch%20with%20considerably%20less%20data.%0ASpecifically%2C%20we%20demonstrate%20that%20an%20expert%20foundation%20model%20for%20living%0Aorganisms%20can%20be%20built%20using%20just%2010M%20images.%20Moreover%2C%20we%20introduce%20EntityNet%2C%0Aa%20dataset%20comprising%2033M%20images%20paired%20with%2046M%20text%20descriptions%2C%20which%0Aenables%20the%20training%20of%20a%20generic%20CLIP%20model%20in%20significantly%20reduced%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02746v1&entry.124074799=Read"},
{"title": "Helping Large Language Models Protect Themselves: An Enhanced Filtering\n  and Summarization System", "author": "Sheikh Samit Muhaimin and Spyridon Mastorakis", "abstract": "  The recent growth in the use of Large Language Models has made them\nvulnerable to sophisticated adversarial assaults, manipulative prompts, and\nencoded malicious inputs. Existing countermeasures frequently necessitate\nretraining models, which is computationally costly and impracticable for\ndeployment. Without the need for retraining or fine-tuning, this study presents\na unique defense paradigm that allows LLMs to recognize, filter, and defend\nagainst adversarial or malicious inputs on their own. There are two main parts\nto the suggested framework: (1) A prompt filtering module that uses\nsophisticated Natural Language Processing (NLP) techniques, including zero-shot\nclassification, keyword analysis, and encoded content detection (e.g. base64,\nhexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and\n(2) A summarization module that processes and summarizes adversarial research\nliterature to give the LLM context-aware defense knowledge. This approach\nstrengthens LLMs' resistance to adversarial exploitation by fusing text\nextraction, summarization, and harmful prompt analysis. According to\nexperimental results, this integrated technique has a 98.71% success rate in\nidentifying harmful patterns, manipulative language structures, and encoded\nprompts. By employing a modest amount of adversarial research literature as\ncontext, the methodology also allows the model to react correctly to harmful\ninputs with a larger percentage of jailbreak resistance and refusal rate. While\nmaintaining the quality of LLM responses, the framework dramatically increases\nLLM's resistance to hostile misuse, demonstrating its efficacy as a quick and\neasy substitute for time-consuming, retraining-based defenses.\n", "link": "http://arxiv.org/abs/2505.01315v2", "date": "2025-05-05", "relevancy": 2.5019, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5106}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5106}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.48}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Helping%20Large%20Language%20Models%20Protect%20Themselves%3A%20An%20Enhanced%20Filtering%0A%20%20and%20Summarization%20System&body=Title%3A%20Helping%20Large%20Language%20Models%20Protect%20Themselves%3A%20An%20Enhanced%20Filtering%0A%20%20and%20Summarization%20System%0AAuthor%3A%20Sheikh%20Samit%20Muhaimin%20and%20Spyridon%20Mastorakis%0AAbstract%3A%20%20%20The%20recent%20growth%20in%20the%20use%20of%20Large%20Language%20Models%20has%20made%20them%0Avulnerable%20to%20sophisticated%20adversarial%20assaults%2C%20manipulative%20prompts%2C%20and%0Aencoded%20malicious%20inputs.%20Existing%20countermeasures%20frequently%20necessitate%0Aretraining%20models%2C%20which%20is%20computationally%20costly%20and%20impracticable%20for%0Adeployment.%20Without%20the%20need%20for%20retraining%20or%20fine-tuning%2C%20this%20study%20presents%0Aa%20unique%20defense%20paradigm%20that%20allows%20LLMs%20to%20recognize%2C%20filter%2C%20and%20defend%0Aagainst%20adversarial%20or%20malicious%20inputs%20on%20their%20own.%20There%20are%20two%20main%20parts%0Ato%20the%20suggested%20framework%3A%20%281%29%20A%20prompt%20filtering%20module%20that%20uses%0Asophisticated%20Natural%20Language%20Processing%20%28NLP%29%20techniques%2C%20including%20zero-shot%0Aclassification%2C%20keyword%20analysis%2C%20and%20encoded%20content%20detection%20%28e.g.%20base64%2C%0Ahexadecimal%2C%20URL%20encoding%29%2C%20to%20detect%2C%20decode%2C%20and%20classify%20harmful%20inputs%3B%20and%0A%282%29%20A%20summarization%20module%20that%20processes%20and%20summarizes%20adversarial%20research%0Aliterature%20to%20give%20the%20LLM%20context-aware%20defense%20knowledge.%20This%20approach%0Astrengthens%20LLMs%27%20resistance%20to%20adversarial%20exploitation%20by%20fusing%20text%0Aextraction%2C%20summarization%2C%20and%20harmful%20prompt%20analysis.%20According%20to%0Aexperimental%20results%2C%20this%20integrated%20technique%20has%20a%2098.71%25%20success%20rate%20in%0Aidentifying%20harmful%20patterns%2C%20manipulative%20language%20structures%2C%20and%20encoded%0Aprompts.%20By%20employing%20a%20modest%20amount%20of%20adversarial%20research%20literature%20as%0Acontext%2C%20the%20methodology%20also%20allows%20the%20model%20to%20react%20correctly%20to%20harmful%0Ainputs%20with%20a%20larger%20percentage%20of%20jailbreak%20resistance%20and%20refusal%20rate.%20While%0Amaintaining%20the%20quality%20of%20LLM%20responses%2C%20the%20framework%20dramatically%20increases%0ALLM%27s%20resistance%20to%20hostile%20misuse%2C%20demonstrating%20its%20efficacy%20as%20a%20quick%20and%0Aeasy%20substitute%20for%20time-consuming%2C%20retraining-based%20defenses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01315v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHelping%2520Large%2520Language%2520Models%2520Protect%2520Themselves%253A%2520An%2520Enhanced%2520Filtering%250A%2520%2520and%2520Summarization%2520System%26entry.906535625%3DSheikh%2520Samit%2520Muhaimin%2520and%2520Spyridon%2520Mastorakis%26entry.1292438233%3D%2520%2520The%2520recent%2520growth%2520in%2520the%2520use%2520of%2520Large%2520Language%2520Models%2520has%2520made%2520them%250Avulnerable%2520to%2520sophisticated%2520adversarial%2520assaults%252C%2520manipulative%2520prompts%252C%2520and%250Aencoded%2520malicious%2520inputs.%2520Existing%2520countermeasures%2520frequently%2520necessitate%250Aretraining%2520models%252C%2520which%2520is%2520computationally%2520costly%2520and%2520impracticable%2520for%250Adeployment.%2520Without%2520the%2520need%2520for%2520retraining%2520or%2520fine-tuning%252C%2520this%2520study%2520presents%250Aa%2520unique%2520defense%2520paradigm%2520that%2520allows%2520LLMs%2520to%2520recognize%252C%2520filter%252C%2520and%2520defend%250Aagainst%2520adversarial%2520or%2520malicious%2520inputs%2520on%2520their%2520own.%2520There%2520are%2520two%2520main%2520parts%250Ato%2520the%2520suggested%2520framework%253A%2520%25281%2529%2520A%2520prompt%2520filtering%2520module%2520that%2520uses%250Asophisticated%2520Natural%2520Language%2520Processing%2520%2528NLP%2529%2520techniques%252C%2520including%2520zero-shot%250Aclassification%252C%2520keyword%2520analysis%252C%2520and%2520encoded%2520content%2520detection%2520%2528e.g.%2520base64%252C%250Ahexadecimal%252C%2520URL%2520encoding%2529%252C%2520to%2520detect%252C%2520decode%252C%2520and%2520classify%2520harmful%2520inputs%253B%2520and%250A%25282%2529%2520A%2520summarization%2520module%2520that%2520processes%2520and%2520summarizes%2520adversarial%2520research%250Aliterature%2520to%2520give%2520the%2520LLM%2520context-aware%2520defense%2520knowledge.%2520This%2520approach%250Astrengthens%2520LLMs%2527%2520resistance%2520to%2520adversarial%2520exploitation%2520by%2520fusing%2520text%250Aextraction%252C%2520summarization%252C%2520and%2520harmful%2520prompt%2520analysis.%2520According%2520to%250Aexperimental%2520results%252C%2520this%2520integrated%2520technique%2520has%2520a%252098.71%2525%2520success%2520rate%2520in%250Aidentifying%2520harmful%2520patterns%252C%2520manipulative%2520language%2520structures%252C%2520and%2520encoded%250Aprompts.%2520By%2520employing%2520a%2520modest%2520amount%2520of%2520adversarial%2520research%2520literature%2520as%250Acontext%252C%2520the%2520methodology%2520also%2520allows%2520the%2520model%2520to%2520react%2520correctly%2520to%2520harmful%250Ainputs%2520with%2520a%2520larger%2520percentage%2520of%2520jailbreak%2520resistance%2520and%2520refusal%2520rate.%2520While%250Amaintaining%2520the%2520quality%2520of%2520LLM%2520responses%252C%2520the%2520framework%2520dramatically%2520increases%250ALLM%2527s%2520resistance%2520to%2520hostile%2520misuse%252C%2520demonstrating%2520its%2520efficacy%2520as%2520a%2520quick%2520and%250Aeasy%2520substitute%2520for%2520time-consuming%252C%2520retraining-based%2520defenses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01315v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Helping%20Large%20Language%20Models%20Protect%20Themselves%3A%20An%20Enhanced%20Filtering%0A%20%20and%20Summarization%20System&entry.906535625=Sheikh%20Samit%20Muhaimin%20and%20Spyridon%20Mastorakis&entry.1292438233=%20%20The%20recent%20growth%20in%20the%20use%20of%20Large%20Language%20Models%20has%20made%20them%0Avulnerable%20to%20sophisticated%20adversarial%20assaults%2C%20manipulative%20prompts%2C%20and%0Aencoded%20malicious%20inputs.%20Existing%20countermeasures%20frequently%20necessitate%0Aretraining%20models%2C%20which%20is%20computationally%20costly%20and%20impracticable%20for%0Adeployment.%20Without%20the%20need%20for%20retraining%20or%20fine-tuning%2C%20this%20study%20presents%0Aa%20unique%20defense%20paradigm%20that%20allows%20LLMs%20to%20recognize%2C%20filter%2C%20and%20defend%0Aagainst%20adversarial%20or%20malicious%20inputs%20on%20their%20own.%20There%20are%20two%20main%20parts%0Ato%20the%20suggested%20framework%3A%20%281%29%20A%20prompt%20filtering%20module%20that%20uses%0Asophisticated%20Natural%20Language%20Processing%20%28NLP%29%20techniques%2C%20including%20zero-shot%0Aclassification%2C%20keyword%20analysis%2C%20and%20encoded%20content%20detection%20%28e.g.%20base64%2C%0Ahexadecimal%2C%20URL%20encoding%29%2C%20to%20detect%2C%20decode%2C%20and%20classify%20harmful%20inputs%3B%20and%0A%282%29%20A%20summarization%20module%20that%20processes%20and%20summarizes%20adversarial%20research%0Aliterature%20to%20give%20the%20LLM%20context-aware%20defense%20knowledge.%20This%20approach%0Astrengthens%20LLMs%27%20resistance%20to%20adversarial%20exploitation%20by%20fusing%20text%0Aextraction%2C%20summarization%2C%20and%20harmful%20prompt%20analysis.%20According%20to%0Aexperimental%20results%2C%20this%20integrated%20technique%20has%20a%2098.71%25%20success%20rate%20in%0Aidentifying%20harmful%20patterns%2C%20manipulative%20language%20structures%2C%20and%20encoded%0Aprompts.%20By%20employing%20a%20modest%20amount%20of%20adversarial%20research%20literature%20as%0Acontext%2C%20the%20methodology%20also%20allows%20the%20model%20to%20react%20correctly%20to%20harmful%0Ainputs%20with%20a%20larger%20percentage%20of%20jailbreak%20resistance%20and%20refusal%20rate.%20While%0Amaintaining%20the%20quality%20of%20LLM%20responses%2C%20the%20framework%20dramatically%20increases%0ALLM%27s%20resistance%20to%20hostile%20misuse%2C%20demonstrating%20its%20efficacy%20as%20a%20quick%20and%0Aeasy%20substitute%20for%20time-consuming%2C%20retraining-based%20defenses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01315v2&entry.124074799=Read"},
{"title": "A Rate-Quality Model for Learned Video Coding", "author": "Sang NguyenQuang and Cheng-Wei Chen and Xiem HoangVan and Wen-Hsiao Peng", "abstract": "  Learned video coding (LVC) has recently achieved superior coding performance.\nIn this paper, we model the rate-quality (R-Q) relationship for learned video\ncoding by a parametric function. We learn a neural network, termed RQNet, to\ncharacterize the relationship between the bitrate and quality level according\nto video content and coding context. The predicted (R,Q) results are further\nintegrated with those from previously coded frames using the least-squares\nmethod to determine the parameters of our R-Q model on-the-fly. Compared to the\nconventional approaches, our method accurately estimates the R-Q relationship,\nenabling the online adaptation of model parameters to enhance both flexibility\nand precision. Experimental results show that our R-Q model achieves\nsignificantly smaller bitrate deviations than the baseline method on commonly\nused datasets with minimal additional complexity.\n", "link": "http://arxiv.org/abs/2505.02720v1", "date": "2025-05-05", "relevancy": 2.4529, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5004}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4857}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4857}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Rate-Quality%20Model%20for%20Learned%20Video%20Coding&body=Title%3A%20A%20Rate-Quality%20Model%20for%20Learned%20Video%20Coding%0AAuthor%3A%20Sang%20NguyenQuang%20and%20Cheng-Wei%20Chen%20and%20Xiem%20HoangVan%20and%20Wen-Hsiao%20Peng%0AAbstract%3A%20%20%20Learned%20video%20coding%20%28LVC%29%20has%20recently%20achieved%20superior%20coding%20performance.%0AIn%20this%20paper%2C%20we%20model%20the%20rate-quality%20%28R-Q%29%20relationship%20for%20learned%20video%0Acoding%20by%20a%20parametric%20function.%20We%20learn%20a%20neural%20network%2C%20termed%20RQNet%2C%20to%0Acharacterize%20the%20relationship%20between%20the%20bitrate%20and%20quality%20level%20according%0Ato%20video%20content%20and%20coding%20context.%20The%20predicted%20%28R%2CQ%29%20results%20are%20further%0Aintegrated%20with%20those%20from%20previously%20coded%20frames%20using%20the%20least-squares%0Amethod%20to%20determine%20the%20parameters%20of%20our%20R-Q%20model%20on-the-fly.%20Compared%20to%20the%0Aconventional%20approaches%2C%20our%20method%20accurately%20estimates%20the%20R-Q%20relationship%2C%0Aenabling%20the%20online%20adaptation%20of%20model%20parameters%20to%20enhance%20both%20flexibility%0Aand%20precision.%20Experimental%20results%20show%20that%20our%20R-Q%20model%20achieves%0Asignificantly%20smaller%20bitrate%20deviations%20than%20the%20baseline%20method%20on%20commonly%0Aused%20datasets%20with%20minimal%20additional%20complexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02720v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Rate-Quality%2520Model%2520for%2520Learned%2520Video%2520Coding%26entry.906535625%3DSang%2520NguyenQuang%2520and%2520Cheng-Wei%2520Chen%2520and%2520Xiem%2520HoangVan%2520and%2520Wen-Hsiao%2520Peng%26entry.1292438233%3D%2520%2520Learned%2520video%2520coding%2520%2528LVC%2529%2520has%2520recently%2520achieved%2520superior%2520coding%2520performance.%250AIn%2520this%2520paper%252C%2520we%2520model%2520the%2520rate-quality%2520%2528R-Q%2529%2520relationship%2520for%2520learned%2520video%250Acoding%2520by%2520a%2520parametric%2520function.%2520We%2520learn%2520a%2520neural%2520network%252C%2520termed%2520RQNet%252C%2520to%250Acharacterize%2520the%2520relationship%2520between%2520the%2520bitrate%2520and%2520quality%2520level%2520according%250Ato%2520video%2520content%2520and%2520coding%2520context.%2520The%2520predicted%2520%2528R%252CQ%2529%2520results%2520are%2520further%250Aintegrated%2520with%2520those%2520from%2520previously%2520coded%2520frames%2520using%2520the%2520least-squares%250Amethod%2520to%2520determine%2520the%2520parameters%2520of%2520our%2520R-Q%2520model%2520on-the-fly.%2520Compared%2520to%2520the%250Aconventional%2520approaches%252C%2520our%2520method%2520accurately%2520estimates%2520the%2520R-Q%2520relationship%252C%250Aenabling%2520the%2520online%2520adaptation%2520of%2520model%2520parameters%2520to%2520enhance%2520both%2520flexibility%250Aand%2520precision.%2520Experimental%2520results%2520show%2520that%2520our%2520R-Q%2520model%2520achieves%250Asignificantly%2520smaller%2520bitrate%2520deviations%2520than%2520the%2520baseline%2520method%2520on%2520commonly%250Aused%2520datasets%2520with%2520minimal%2520additional%2520complexity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02720v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Rate-Quality%20Model%20for%20Learned%20Video%20Coding&entry.906535625=Sang%20NguyenQuang%20and%20Cheng-Wei%20Chen%20and%20Xiem%20HoangVan%20and%20Wen-Hsiao%20Peng&entry.1292438233=%20%20Learned%20video%20coding%20%28LVC%29%20has%20recently%20achieved%20superior%20coding%20performance.%0AIn%20this%20paper%2C%20we%20model%20the%20rate-quality%20%28R-Q%29%20relationship%20for%20learned%20video%0Acoding%20by%20a%20parametric%20function.%20We%20learn%20a%20neural%20network%2C%20termed%20RQNet%2C%20to%0Acharacterize%20the%20relationship%20between%20the%20bitrate%20and%20quality%20level%20according%0Ato%20video%20content%20and%20coding%20context.%20The%20predicted%20%28R%2CQ%29%20results%20are%20further%0Aintegrated%20with%20those%20from%20previously%20coded%20frames%20using%20the%20least-squares%0Amethod%20to%20determine%20the%20parameters%20of%20our%20R-Q%20model%20on-the-fly.%20Compared%20to%20the%0Aconventional%20approaches%2C%20our%20method%20accurately%20estimates%20the%20R-Q%20relationship%2C%0Aenabling%20the%20online%20adaptation%20of%20model%20parameters%20to%20enhance%20both%20flexibility%0Aand%20precision.%20Experimental%20results%20show%20that%20our%20R-Q%20model%20achieves%0Asignificantly%20smaller%20bitrate%20deviations%20than%20the%20baseline%20method%20on%20commonly%0Aused%20datasets%20with%20minimal%20additional%20complexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02720v1&entry.124074799=Read"},
{"title": "Grasp the Graph (GtG) 2.0: Ensemble of GNNs for High-Precision Grasp\n  Pose Detection in Clutter", "author": "Ali Rashidi Moghadam and Sayedmohammadreza Rastegari and Mehdi Tale Masouleh and Ahmad Kalhor", "abstract": "  Grasp pose detection in cluttered, real-world environments remains a\nsignificant challenge due to noisy and incomplete sensory data combined with\ncomplex object geometries. This paper introduces Grasp the Graph 2.0 (GtG 2.0)\nmethod, a lightweight yet highly effective hypothesis-and-test robotics\ngrasping framework which leverages an ensemble of Graph Neural Networks for\nefficient geometric reasoning from point cloud data. Building on the success of\nGtG 1.0, which demonstrated the potential of Graph Neural Networks for grasp\ndetection but was limited by assumptions of complete, noise-free point clouds\nand 4-Dof grasping, GtG 2.0 employs a conventional Grasp Pose Generator to\nefficiently produce 7-Dof grasp candidates. Candidates are assessed with an\nensemble Graph Neural Network model which includes points within the gripper\njaws (inside points) and surrounding contextual points (outside points). This\nimproved representation boosts grasp detection performance over previous\nmethods using the same generator. GtG 2.0 shows up to a 35% improvement in\nAverage Precision on the GraspNet-1Billion benchmark compared to\nhypothesis-and-test and Graph Neural Network-based methods, ranking it among\nthe top three frameworks. Experiments with a 3-Dof Delta Parallel robot and\nKinect-v1 camera show a success rate of 91% and a clutter completion rate of\n100%, demonstrating its flexibility and reliability.\n", "link": "http://arxiv.org/abs/2505.02664v1", "date": "2025-05-05", "relevancy": 2.4374, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6742}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5641}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grasp%20the%20Graph%20%28GtG%29%202.0%3A%20Ensemble%20of%20GNNs%20for%20High-Precision%20Grasp%0A%20%20Pose%20Detection%20in%20Clutter&body=Title%3A%20Grasp%20the%20Graph%20%28GtG%29%202.0%3A%20Ensemble%20of%20GNNs%20for%20High-Precision%20Grasp%0A%20%20Pose%20Detection%20in%20Clutter%0AAuthor%3A%20Ali%20Rashidi%20Moghadam%20and%20Sayedmohammadreza%20Rastegari%20and%20Mehdi%20Tale%20Masouleh%20and%20Ahmad%20Kalhor%0AAbstract%3A%20%20%20Grasp%20pose%20detection%20in%20cluttered%2C%20real-world%20environments%20remains%20a%0Asignificant%20challenge%20due%20to%20noisy%20and%20incomplete%20sensory%20data%20combined%20with%0Acomplex%20object%20geometries.%20This%20paper%20introduces%20Grasp%20the%20Graph%202.0%20%28GtG%202.0%29%0Amethod%2C%20a%20lightweight%20yet%20highly%20effective%20hypothesis-and-test%20robotics%0Agrasping%20framework%20which%20leverages%20an%20ensemble%20of%20Graph%20Neural%20Networks%20for%0Aefficient%20geometric%20reasoning%20from%20point%20cloud%20data.%20Building%20on%20the%20success%20of%0AGtG%201.0%2C%20which%20demonstrated%20the%20potential%20of%20Graph%20Neural%20Networks%20for%20grasp%0Adetection%20but%20was%20limited%20by%20assumptions%20of%20complete%2C%20noise-free%20point%20clouds%0Aand%204-Dof%20grasping%2C%20GtG%202.0%20employs%20a%20conventional%20Grasp%20Pose%20Generator%20to%0Aefficiently%20produce%207-Dof%20grasp%20candidates.%20Candidates%20are%20assessed%20with%20an%0Aensemble%20Graph%20Neural%20Network%20model%20which%20includes%20points%20within%20the%20gripper%0Ajaws%20%28inside%20points%29%20and%20surrounding%20contextual%20points%20%28outside%20points%29.%20This%0Aimproved%20representation%20boosts%20grasp%20detection%20performance%20over%20previous%0Amethods%20using%20the%20same%20generator.%20GtG%202.0%20shows%20up%20to%20a%2035%25%20improvement%20in%0AAverage%20Precision%20on%20the%20GraspNet-1Billion%20benchmark%20compared%20to%0Ahypothesis-and-test%20and%20Graph%20Neural%20Network-based%20methods%2C%20ranking%20it%20among%0Athe%20top%20three%20frameworks.%20Experiments%20with%20a%203-Dof%20Delta%20Parallel%20robot%20and%0AKinect-v1%20camera%20show%20a%20success%20rate%20of%2091%25%20and%20a%20clutter%20completion%20rate%20of%0A100%25%2C%20demonstrating%20its%20flexibility%20and%20reliability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02664v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrasp%2520the%2520Graph%2520%2528GtG%2529%25202.0%253A%2520Ensemble%2520of%2520GNNs%2520for%2520High-Precision%2520Grasp%250A%2520%2520Pose%2520Detection%2520in%2520Clutter%26entry.906535625%3DAli%2520Rashidi%2520Moghadam%2520and%2520Sayedmohammadreza%2520Rastegari%2520and%2520Mehdi%2520Tale%2520Masouleh%2520and%2520Ahmad%2520Kalhor%26entry.1292438233%3D%2520%2520Grasp%2520pose%2520detection%2520in%2520cluttered%252C%2520real-world%2520environments%2520remains%2520a%250Asignificant%2520challenge%2520due%2520to%2520noisy%2520and%2520incomplete%2520sensory%2520data%2520combined%2520with%250Acomplex%2520object%2520geometries.%2520This%2520paper%2520introduces%2520Grasp%2520the%2520Graph%25202.0%2520%2528GtG%25202.0%2529%250Amethod%252C%2520a%2520lightweight%2520yet%2520highly%2520effective%2520hypothesis-and-test%2520robotics%250Agrasping%2520framework%2520which%2520leverages%2520an%2520ensemble%2520of%2520Graph%2520Neural%2520Networks%2520for%250Aefficient%2520geometric%2520reasoning%2520from%2520point%2520cloud%2520data.%2520Building%2520on%2520the%2520success%2520of%250AGtG%25201.0%252C%2520which%2520demonstrated%2520the%2520potential%2520of%2520Graph%2520Neural%2520Networks%2520for%2520grasp%250Adetection%2520but%2520was%2520limited%2520by%2520assumptions%2520of%2520complete%252C%2520noise-free%2520point%2520clouds%250Aand%25204-Dof%2520grasping%252C%2520GtG%25202.0%2520employs%2520a%2520conventional%2520Grasp%2520Pose%2520Generator%2520to%250Aefficiently%2520produce%25207-Dof%2520grasp%2520candidates.%2520Candidates%2520are%2520assessed%2520with%2520an%250Aensemble%2520Graph%2520Neural%2520Network%2520model%2520which%2520includes%2520points%2520within%2520the%2520gripper%250Ajaws%2520%2528inside%2520points%2529%2520and%2520surrounding%2520contextual%2520points%2520%2528outside%2520points%2529.%2520This%250Aimproved%2520representation%2520boosts%2520grasp%2520detection%2520performance%2520over%2520previous%250Amethods%2520using%2520the%2520same%2520generator.%2520GtG%25202.0%2520shows%2520up%2520to%2520a%252035%2525%2520improvement%2520in%250AAverage%2520Precision%2520on%2520the%2520GraspNet-1Billion%2520benchmark%2520compared%2520to%250Ahypothesis-and-test%2520and%2520Graph%2520Neural%2520Network-based%2520methods%252C%2520ranking%2520it%2520among%250Athe%2520top%2520three%2520frameworks.%2520Experiments%2520with%2520a%25203-Dof%2520Delta%2520Parallel%2520robot%2520and%250AKinect-v1%2520camera%2520show%2520a%2520success%2520rate%2520of%252091%2525%2520and%2520a%2520clutter%2520completion%2520rate%2520of%250A100%2525%252C%2520demonstrating%2520its%2520flexibility%2520and%2520reliability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02664v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grasp%20the%20Graph%20%28GtG%29%202.0%3A%20Ensemble%20of%20GNNs%20for%20High-Precision%20Grasp%0A%20%20Pose%20Detection%20in%20Clutter&entry.906535625=Ali%20Rashidi%20Moghadam%20and%20Sayedmohammadreza%20Rastegari%20and%20Mehdi%20Tale%20Masouleh%20and%20Ahmad%20Kalhor&entry.1292438233=%20%20Grasp%20pose%20detection%20in%20cluttered%2C%20real-world%20environments%20remains%20a%0Asignificant%20challenge%20due%20to%20noisy%20and%20incomplete%20sensory%20data%20combined%20with%0Acomplex%20object%20geometries.%20This%20paper%20introduces%20Grasp%20the%20Graph%202.0%20%28GtG%202.0%29%0Amethod%2C%20a%20lightweight%20yet%20highly%20effective%20hypothesis-and-test%20robotics%0Agrasping%20framework%20which%20leverages%20an%20ensemble%20of%20Graph%20Neural%20Networks%20for%0Aefficient%20geometric%20reasoning%20from%20point%20cloud%20data.%20Building%20on%20the%20success%20of%0AGtG%201.0%2C%20which%20demonstrated%20the%20potential%20of%20Graph%20Neural%20Networks%20for%20grasp%0Adetection%20but%20was%20limited%20by%20assumptions%20of%20complete%2C%20noise-free%20point%20clouds%0Aand%204-Dof%20grasping%2C%20GtG%202.0%20employs%20a%20conventional%20Grasp%20Pose%20Generator%20to%0Aefficiently%20produce%207-Dof%20grasp%20candidates.%20Candidates%20are%20assessed%20with%20an%0Aensemble%20Graph%20Neural%20Network%20model%20which%20includes%20points%20within%20the%20gripper%0Ajaws%20%28inside%20points%29%20and%20surrounding%20contextual%20points%20%28outside%20points%29.%20This%0Aimproved%20representation%20boosts%20grasp%20detection%20performance%20over%20previous%0Amethods%20using%20the%20same%20generator.%20GtG%202.0%20shows%20up%20to%20a%2035%25%20improvement%20in%0AAverage%20Precision%20on%20the%20GraspNet-1Billion%20benchmark%20compared%20to%0Ahypothesis-and-test%20and%20Graph%20Neural%20Network-based%20methods%2C%20ranking%20it%20among%0Athe%20top%20three%20frameworks.%20Experiments%20with%20a%203-Dof%20Delta%20Parallel%20robot%20and%0AKinect-v1%20camera%20show%20a%20success%20rate%20of%2091%25%20and%20a%20clutter%20completion%20rate%20of%0A100%25%2C%20demonstrating%20its%20flexibility%20and%20reliability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02664v1&entry.124074799=Read"},
{"title": "tPARAFAC2: Tracking evolving patterns in (incomplete) temporal data", "author": "Christos Chatzis and Carla Schenker and Max Pfeffer and Evrim Acar", "abstract": "  Tensor factorizations have been widely used for the task of uncovering\npatterns in various domains. Often, the input is time-evolving, shifting the\ngoal to tracking the evolution of the underlying patterns instead. To adapt to\nthis more complex setting, existing methods incorporate temporal regularization\nbut they either have overly constrained structural requirements or lack\nuniqueness which is crucial for interpretation. In this paper, in order to\ncapture the underlying evolving patterns, we introduce t(emporal)PARAFAC2,\nwhich utilizes temporal smoothness regularization on the evolving factors.\nPreviously, Alternating Optimization (AO) and Alternating Direction Method of\nMultipliers (ADMM)-based algorithmic approach has been introduced to fit the\nPARAFAC2 model to fully observed data. In this paper, we extend this\nalgorithmic framework to the case of partially observed data and use it to fit\nthe tPARAFAC2 model to complete and incomplete datasets with the goal of\nrevealing evolving patterns. Our numerical experiments on simulated datasets\ndemonstrate that tPARAFAC2 can extract the underlying evolving patterns more\naccurately compared to the state-of-the-art in the presence of high amounts of\nnoise and missing data. Using two real datasets, we also demonstrate the\neffectiveness of the algorithmic approach in terms of handling missing data and\ntPARAFAC2 model in terms of revealing evolving patterns. The paper provides an\nextensive comparison of different approaches for handling missing data within\nthe proposed framework, and discusses both the advantages and limitations of\ntPARAFAC2 model.\n", "link": "http://arxiv.org/abs/2407.01356v2", "date": "2025-05-05", "relevancy": 2.4056, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4971}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4769}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4693}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20tPARAFAC2%3A%20Tracking%20evolving%20patterns%20in%20%28incomplete%29%20temporal%20data&body=Title%3A%20tPARAFAC2%3A%20Tracking%20evolving%20patterns%20in%20%28incomplete%29%20temporal%20data%0AAuthor%3A%20Christos%20Chatzis%20and%20Carla%20Schenker%20and%20Max%20Pfeffer%20and%20Evrim%20Acar%0AAbstract%3A%20%20%20Tensor%20factorizations%20have%20been%20widely%20used%20for%20the%20task%20of%20uncovering%0Apatterns%20in%20various%20domains.%20Often%2C%20the%20input%20is%20time-evolving%2C%20shifting%20the%0Agoal%20to%20tracking%20the%20evolution%20of%20the%20underlying%20patterns%20instead.%20To%20adapt%20to%0Athis%20more%20complex%20setting%2C%20existing%20methods%20incorporate%20temporal%20regularization%0Abut%20they%20either%20have%20overly%20constrained%20structural%20requirements%20or%20lack%0Auniqueness%20which%20is%20crucial%20for%20interpretation.%20In%20this%20paper%2C%20in%20order%20to%0Acapture%20the%20underlying%20evolving%20patterns%2C%20we%20introduce%20t%28emporal%29PARAFAC2%2C%0Awhich%20utilizes%20temporal%20smoothness%20regularization%20on%20the%20evolving%20factors.%0APreviously%2C%20Alternating%20Optimization%20%28AO%29%20and%20Alternating%20Direction%20Method%20of%0AMultipliers%20%28ADMM%29-based%20algorithmic%20approach%20has%20been%20introduced%20to%20fit%20the%0APARAFAC2%20model%20to%20fully%20observed%20data.%20In%20this%20paper%2C%20we%20extend%20this%0Aalgorithmic%20framework%20to%20the%20case%20of%20partially%20observed%20data%20and%20use%20it%20to%20fit%0Athe%20tPARAFAC2%20model%20to%20complete%20and%20incomplete%20datasets%20with%20the%20goal%20of%0Arevealing%20evolving%20patterns.%20Our%20numerical%20experiments%20on%20simulated%20datasets%0Ademonstrate%20that%20tPARAFAC2%20can%20extract%20the%20underlying%20evolving%20patterns%20more%0Aaccurately%20compared%20to%20the%20state-of-the-art%20in%20the%20presence%20of%20high%20amounts%20of%0Anoise%20and%20missing%20data.%20Using%20two%20real%20datasets%2C%20we%20also%20demonstrate%20the%0Aeffectiveness%20of%20the%20algorithmic%20approach%20in%20terms%20of%20handling%20missing%20data%20and%0AtPARAFAC2%20model%20in%20terms%20of%20revealing%20evolving%20patterns.%20The%20paper%20provides%20an%0Aextensive%20comparison%20of%20different%20approaches%20for%20handling%20missing%20data%20within%0Athe%20proposed%20framework%2C%20and%20discusses%20both%20the%20advantages%20and%20limitations%20of%0AtPARAFAC2%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.01356v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DtPARAFAC2%253A%2520Tracking%2520evolving%2520patterns%2520in%2520%2528incomplete%2529%2520temporal%2520data%26entry.906535625%3DChristos%2520Chatzis%2520and%2520Carla%2520Schenker%2520and%2520Max%2520Pfeffer%2520and%2520Evrim%2520Acar%26entry.1292438233%3D%2520%2520Tensor%2520factorizations%2520have%2520been%2520widely%2520used%2520for%2520the%2520task%2520of%2520uncovering%250Apatterns%2520in%2520various%2520domains.%2520Often%252C%2520the%2520input%2520is%2520time-evolving%252C%2520shifting%2520the%250Agoal%2520to%2520tracking%2520the%2520evolution%2520of%2520the%2520underlying%2520patterns%2520instead.%2520To%2520adapt%2520to%250Athis%2520more%2520complex%2520setting%252C%2520existing%2520methods%2520incorporate%2520temporal%2520regularization%250Abut%2520they%2520either%2520have%2520overly%2520constrained%2520structural%2520requirements%2520or%2520lack%250Auniqueness%2520which%2520is%2520crucial%2520for%2520interpretation.%2520In%2520this%2520paper%252C%2520in%2520order%2520to%250Acapture%2520the%2520underlying%2520evolving%2520patterns%252C%2520we%2520introduce%2520t%2528emporal%2529PARAFAC2%252C%250Awhich%2520utilizes%2520temporal%2520smoothness%2520regularization%2520on%2520the%2520evolving%2520factors.%250APreviously%252C%2520Alternating%2520Optimization%2520%2528AO%2529%2520and%2520Alternating%2520Direction%2520Method%2520of%250AMultipliers%2520%2528ADMM%2529-based%2520algorithmic%2520approach%2520has%2520been%2520introduced%2520to%2520fit%2520the%250APARAFAC2%2520model%2520to%2520fully%2520observed%2520data.%2520In%2520this%2520paper%252C%2520we%2520extend%2520this%250Aalgorithmic%2520framework%2520to%2520the%2520case%2520of%2520partially%2520observed%2520data%2520and%2520use%2520it%2520to%2520fit%250Athe%2520tPARAFAC2%2520model%2520to%2520complete%2520and%2520incomplete%2520datasets%2520with%2520the%2520goal%2520of%250Arevealing%2520evolving%2520patterns.%2520Our%2520numerical%2520experiments%2520on%2520simulated%2520datasets%250Ademonstrate%2520that%2520tPARAFAC2%2520can%2520extract%2520the%2520underlying%2520evolving%2520patterns%2520more%250Aaccurately%2520compared%2520to%2520the%2520state-of-the-art%2520in%2520the%2520presence%2520of%2520high%2520amounts%2520of%250Anoise%2520and%2520missing%2520data.%2520Using%2520two%2520real%2520datasets%252C%2520we%2520also%2520demonstrate%2520the%250Aeffectiveness%2520of%2520the%2520algorithmic%2520approach%2520in%2520terms%2520of%2520handling%2520missing%2520data%2520and%250AtPARAFAC2%2520model%2520in%2520terms%2520of%2520revealing%2520evolving%2520patterns.%2520The%2520paper%2520provides%2520an%250Aextensive%2520comparison%2520of%2520different%2520approaches%2520for%2520handling%2520missing%2520data%2520within%250Athe%2520proposed%2520framework%252C%2520and%2520discusses%2520both%2520the%2520advantages%2520and%2520limitations%2520of%250AtPARAFAC2%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.01356v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=tPARAFAC2%3A%20Tracking%20evolving%20patterns%20in%20%28incomplete%29%20temporal%20data&entry.906535625=Christos%20Chatzis%20and%20Carla%20Schenker%20and%20Max%20Pfeffer%20and%20Evrim%20Acar&entry.1292438233=%20%20Tensor%20factorizations%20have%20been%20widely%20used%20for%20the%20task%20of%20uncovering%0Apatterns%20in%20various%20domains.%20Often%2C%20the%20input%20is%20time-evolving%2C%20shifting%20the%0Agoal%20to%20tracking%20the%20evolution%20of%20the%20underlying%20patterns%20instead.%20To%20adapt%20to%0Athis%20more%20complex%20setting%2C%20existing%20methods%20incorporate%20temporal%20regularization%0Abut%20they%20either%20have%20overly%20constrained%20structural%20requirements%20or%20lack%0Auniqueness%20which%20is%20crucial%20for%20interpretation.%20In%20this%20paper%2C%20in%20order%20to%0Acapture%20the%20underlying%20evolving%20patterns%2C%20we%20introduce%20t%28emporal%29PARAFAC2%2C%0Awhich%20utilizes%20temporal%20smoothness%20regularization%20on%20the%20evolving%20factors.%0APreviously%2C%20Alternating%20Optimization%20%28AO%29%20and%20Alternating%20Direction%20Method%20of%0AMultipliers%20%28ADMM%29-based%20algorithmic%20approach%20has%20been%20introduced%20to%20fit%20the%0APARAFAC2%20model%20to%20fully%20observed%20data.%20In%20this%20paper%2C%20we%20extend%20this%0Aalgorithmic%20framework%20to%20the%20case%20of%20partially%20observed%20data%20and%20use%20it%20to%20fit%0Athe%20tPARAFAC2%20model%20to%20complete%20and%20incomplete%20datasets%20with%20the%20goal%20of%0Arevealing%20evolving%20patterns.%20Our%20numerical%20experiments%20on%20simulated%20datasets%0Ademonstrate%20that%20tPARAFAC2%20can%20extract%20the%20underlying%20evolving%20patterns%20more%0Aaccurately%20compared%20to%20the%20state-of-the-art%20in%20the%20presence%20of%20high%20amounts%20of%0Anoise%20and%20missing%20data.%20Using%20two%20real%20datasets%2C%20we%20also%20demonstrate%20the%0Aeffectiveness%20of%20the%20algorithmic%20approach%20in%20terms%20of%20handling%20missing%20data%20and%0AtPARAFAC2%20model%20in%20terms%20of%20revealing%20evolving%20patterns.%20The%20paper%20provides%20an%0Aextensive%20comparison%20of%20different%20approaches%20for%20handling%20missing%20data%20within%0Athe%20proposed%20framework%2C%20and%20discusses%20both%20the%20advantages%20and%20limitations%20of%0AtPARAFAC2%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.01356v2&entry.124074799=Read"},
{"title": "Token-Efficient RL for LLM Reasoning", "author": "Alan Lee and Harry Tong", "abstract": "  We propose reinforcement learning (RL) strategies tailored for reasoning in\nlarge language models (LLMs) under strict memory and compute limits, with a\nparticular focus on compatibility with LoRA fine-tuning. Rather than relying on\nfull-sequence updates or separate critic networks, we design critic-free\nmethods that operate on a small, informative subset of output tokens to reduce\nmemory usage and stabilize training. We introduce S-GRPO, a stochastic variant\nof Group Relative Policy Optimization, and T-SPMO, a token-level prefix\nmatching approach for fine-grained credit assignment. Applied to Qwen2-1.5B,\nour methods raise accuracy on the SVAMP benchmark from 46% to over 70% and show\nstrong performance on multi-digit multiplication. Surprisingly, full-token GRPO\nunder LoRA fails to improve over the base model, suggesting that selective\ntoken-level optimization may act as an implicit regularizer in low-parameter\ntraining regimes.\n", "link": "http://arxiv.org/abs/2504.20834v2", "date": "2025-05-05", "relevancy": 2.351, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5132}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4487}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Token-Efficient%20RL%20for%20LLM%20Reasoning&body=Title%3A%20Token-Efficient%20RL%20for%20LLM%20Reasoning%0AAuthor%3A%20Alan%20Lee%20and%20Harry%20Tong%0AAbstract%3A%20%20%20We%20propose%20reinforcement%20learning%20%28RL%29%20strategies%20tailored%20for%20reasoning%20in%0Alarge%20language%20models%20%28LLMs%29%20under%20strict%20memory%20and%20compute%20limits%2C%20with%20a%0Aparticular%20focus%20on%20compatibility%20with%20LoRA%20fine-tuning.%20Rather%20than%20relying%20on%0Afull-sequence%20updates%20or%20separate%20critic%20networks%2C%20we%20design%20critic-free%0Amethods%20that%20operate%20on%20a%20small%2C%20informative%20subset%20of%20output%20tokens%20to%20reduce%0Amemory%20usage%20and%20stabilize%20training.%20We%20introduce%20S-GRPO%2C%20a%20stochastic%20variant%0Aof%20Group%20Relative%20Policy%20Optimization%2C%20and%20T-SPMO%2C%20a%20token-level%20prefix%0Amatching%20approach%20for%20fine-grained%20credit%20assignment.%20Applied%20to%20Qwen2-1.5B%2C%0Aour%20methods%20raise%20accuracy%20on%20the%20SVAMP%20benchmark%20from%2046%25%20to%20over%2070%25%20and%20show%0Astrong%20performance%20on%20multi-digit%20multiplication.%20Surprisingly%2C%20full-token%20GRPO%0Aunder%20LoRA%20fails%20to%20improve%20over%20the%20base%20model%2C%20suggesting%20that%20selective%0Atoken-level%20optimization%20may%20act%20as%20an%20implicit%20regularizer%20in%20low-parameter%0Atraining%20regimes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20834v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToken-Efficient%2520RL%2520for%2520LLM%2520Reasoning%26entry.906535625%3DAlan%2520Lee%2520and%2520Harry%2520Tong%26entry.1292438233%3D%2520%2520We%2520propose%2520reinforcement%2520learning%2520%2528RL%2529%2520strategies%2520tailored%2520for%2520reasoning%2520in%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520under%2520strict%2520memory%2520and%2520compute%2520limits%252C%2520with%2520a%250Aparticular%2520focus%2520on%2520compatibility%2520with%2520LoRA%2520fine-tuning.%2520Rather%2520than%2520relying%2520on%250Afull-sequence%2520updates%2520or%2520separate%2520critic%2520networks%252C%2520we%2520design%2520critic-free%250Amethods%2520that%2520operate%2520on%2520a%2520small%252C%2520informative%2520subset%2520of%2520output%2520tokens%2520to%2520reduce%250Amemory%2520usage%2520and%2520stabilize%2520training.%2520We%2520introduce%2520S-GRPO%252C%2520a%2520stochastic%2520variant%250Aof%2520Group%2520Relative%2520Policy%2520Optimization%252C%2520and%2520T-SPMO%252C%2520a%2520token-level%2520prefix%250Amatching%2520approach%2520for%2520fine-grained%2520credit%2520assignment.%2520Applied%2520to%2520Qwen2-1.5B%252C%250Aour%2520methods%2520raise%2520accuracy%2520on%2520the%2520SVAMP%2520benchmark%2520from%252046%2525%2520to%2520over%252070%2525%2520and%2520show%250Astrong%2520performance%2520on%2520multi-digit%2520multiplication.%2520Surprisingly%252C%2520full-token%2520GRPO%250Aunder%2520LoRA%2520fails%2520to%2520improve%2520over%2520the%2520base%2520model%252C%2520suggesting%2520that%2520selective%250Atoken-level%2520optimization%2520may%2520act%2520as%2520an%2520implicit%2520regularizer%2520in%2520low-parameter%250Atraining%2520regimes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20834v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Token-Efficient%20RL%20for%20LLM%20Reasoning&entry.906535625=Alan%20Lee%20and%20Harry%20Tong&entry.1292438233=%20%20We%20propose%20reinforcement%20learning%20%28RL%29%20strategies%20tailored%20for%20reasoning%20in%0Alarge%20language%20models%20%28LLMs%29%20under%20strict%20memory%20and%20compute%20limits%2C%20with%20a%0Aparticular%20focus%20on%20compatibility%20with%20LoRA%20fine-tuning.%20Rather%20than%20relying%20on%0Afull-sequence%20updates%20or%20separate%20critic%20networks%2C%20we%20design%20critic-free%0Amethods%20that%20operate%20on%20a%20small%2C%20informative%20subset%20of%20output%20tokens%20to%20reduce%0Amemory%20usage%20and%20stabilize%20training.%20We%20introduce%20S-GRPO%2C%20a%20stochastic%20variant%0Aof%20Group%20Relative%20Policy%20Optimization%2C%20and%20T-SPMO%2C%20a%20token-level%20prefix%0Amatching%20approach%20for%20fine-grained%20credit%20assignment.%20Applied%20to%20Qwen2-1.5B%2C%0Aour%20methods%20raise%20accuracy%20on%20the%20SVAMP%20benchmark%20from%2046%25%20to%20over%2070%25%20and%20show%0Astrong%20performance%20on%20multi-digit%20multiplication.%20Surprisingly%2C%20full-token%20GRPO%0Aunder%20LoRA%20fails%20to%20improve%20over%20the%20base%20model%2C%20suggesting%20that%20selective%0Atoken-level%20optimization%20may%20act%20as%20an%20implicit%20regularizer%20in%20low-parameter%0Atraining%20regimes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20834v2&entry.124074799=Read"},
{"title": "Visually-Guided Linguistic Disambiguation for Monocular Depth Scale\n  Recovery", "author": "Bojin Wu and Jing Chen", "abstract": "  We propose a robust method for monocular depth scale recovery. Monocular\ndepth estimation can be divided into two main directions: (1) relative depth\nestimation, which provides normalized or inverse depth without scale\ninformation, and (2) metric depth estimation, which involves recovering depth\nwith absolute scale. To obtain absolute scale information for practical\ndownstream tasks, utilizing textual information to recover the scale of a\nrelative depth map is a highly promising approach. However, since a single\nimage can have multiple descriptions from different perspectives or with\nvarying styles, it has been shown that different textual descriptions can\nsignificantly affect the scale recovery process. To address this issue, our\nmethod, VGLD, stabilizes the influence of textual information by incorporating\nhigh-level semantic information from the corresponding image alongside the\ntextual description. This approach resolves textual ambiguities and robustly\noutputs a set of linear transformation parameters (scalars) that can be\nglobally applied to the relative depth map, ultimately generating depth\npredictions with metric-scale accuracy. We validate our method across several\npopular relative depth models(MiDas, DepthAnything), using both indoor scenes\n(NYUv2) and outdoor scenes (KITTI). Our results demonstrate that VGLD functions\nas a universal alignment module when trained on multiple datasets, achieving\nstrong performance even in zero-shot scenarios. Code is available at:\nhttps://github.com/pakinwu/VGLD.\n", "link": "http://arxiv.org/abs/2505.02704v1", "date": "2025-05-05", "relevancy": 2.3439, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.595}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5885}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5798}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visually-Guided%20Linguistic%20Disambiguation%20for%20Monocular%20Depth%20Scale%0A%20%20Recovery&body=Title%3A%20Visually-Guided%20Linguistic%20Disambiguation%20for%20Monocular%20Depth%20Scale%0A%20%20Recovery%0AAuthor%3A%20Bojin%20Wu%20and%20Jing%20Chen%0AAbstract%3A%20%20%20We%20propose%20a%20robust%20method%20for%20monocular%20depth%20scale%20recovery.%20Monocular%0Adepth%20estimation%20can%20be%20divided%20into%20two%20main%20directions%3A%20%281%29%20relative%20depth%0Aestimation%2C%20which%20provides%20normalized%20or%20inverse%20depth%20without%20scale%0Ainformation%2C%20and%20%282%29%20metric%20depth%20estimation%2C%20which%20involves%20recovering%20depth%0Awith%20absolute%20scale.%20To%20obtain%20absolute%20scale%20information%20for%20practical%0Adownstream%20tasks%2C%20utilizing%20textual%20information%20to%20recover%20the%20scale%20of%20a%0Arelative%20depth%20map%20is%20a%20highly%20promising%20approach.%20However%2C%20since%20a%20single%0Aimage%20can%20have%20multiple%20descriptions%20from%20different%20perspectives%20or%20with%0Avarying%20styles%2C%20it%20has%20been%20shown%20that%20different%20textual%20descriptions%20can%0Asignificantly%20affect%20the%20scale%20recovery%20process.%20To%20address%20this%20issue%2C%20our%0Amethod%2C%20VGLD%2C%20stabilizes%20the%20influence%20of%20textual%20information%20by%20incorporating%0Ahigh-level%20semantic%20information%20from%20the%20corresponding%20image%20alongside%20the%0Atextual%20description.%20This%20approach%20resolves%20textual%20ambiguities%20and%20robustly%0Aoutputs%20a%20set%20of%20linear%20transformation%20parameters%20%28scalars%29%20that%20can%20be%0Aglobally%20applied%20to%20the%20relative%20depth%20map%2C%20ultimately%20generating%20depth%0Apredictions%20with%20metric-scale%20accuracy.%20We%20validate%20our%20method%20across%20several%0Apopular%20relative%20depth%20models%28MiDas%2C%20DepthAnything%29%2C%20using%20both%20indoor%20scenes%0A%28NYUv2%29%20and%20outdoor%20scenes%20%28KITTI%29.%20Our%20results%20demonstrate%20that%20VGLD%20functions%0Aas%20a%20universal%20alignment%20module%20when%20trained%20on%20multiple%20datasets%2C%20achieving%0Astrong%20performance%20even%20in%20zero-shot%20scenarios.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/pakinwu/VGLD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02704v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisually-Guided%2520Linguistic%2520Disambiguation%2520for%2520Monocular%2520Depth%2520Scale%250A%2520%2520Recovery%26entry.906535625%3DBojin%2520Wu%2520and%2520Jing%2520Chen%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520robust%2520method%2520for%2520monocular%2520depth%2520scale%2520recovery.%2520Monocular%250Adepth%2520estimation%2520can%2520be%2520divided%2520into%2520two%2520main%2520directions%253A%2520%25281%2529%2520relative%2520depth%250Aestimation%252C%2520which%2520provides%2520normalized%2520or%2520inverse%2520depth%2520without%2520scale%250Ainformation%252C%2520and%2520%25282%2529%2520metric%2520depth%2520estimation%252C%2520which%2520involves%2520recovering%2520depth%250Awith%2520absolute%2520scale.%2520To%2520obtain%2520absolute%2520scale%2520information%2520for%2520practical%250Adownstream%2520tasks%252C%2520utilizing%2520textual%2520information%2520to%2520recover%2520the%2520scale%2520of%2520a%250Arelative%2520depth%2520map%2520is%2520a%2520highly%2520promising%2520approach.%2520However%252C%2520since%2520a%2520single%250Aimage%2520can%2520have%2520multiple%2520descriptions%2520from%2520different%2520perspectives%2520or%2520with%250Avarying%2520styles%252C%2520it%2520has%2520been%2520shown%2520that%2520different%2520textual%2520descriptions%2520can%250Asignificantly%2520affect%2520the%2520scale%2520recovery%2520process.%2520To%2520address%2520this%2520issue%252C%2520our%250Amethod%252C%2520VGLD%252C%2520stabilizes%2520the%2520influence%2520of%2520textual%2520information%2520by%2520incorporating%250Ahigh-level%2520semantic%2520information%2520from%2520the%2520corresponding%2520image%2520alongside%2520the%250Atextual%2520description.%2520This%2520approach%2520resolves%2520textual%2520ambiguities%2520and%2520robustly%250Aoutputs%2520a%2520set%2520of%2520linear%2520transformation%2520parameters%2520%2528scalars%2529%2520that%2520can%2520be%250Aglobally%2520applied%2520to%2520the%2520relative%2520depth%2520map%252C%2520ultimately%2520generating%2520depth%250Apredictions%2520with%2520metric-scale%2520accuracy.%2520We%2520validate%2520our%2520method%2520across%2520several%250Apopular%2520relative%2520depth%2520models%2528MiDas%252C%2520DepthAnything%2529%252C%2520using%2520both%2520indoor%2520scenes%250A%2528NYUv2%2529%2520and%2520outdoor%2520scenes%2520%2528KITTI%2529.%2520Our%2520results%2520demonstrate%2520that%2520VGLD%2520functions%250Aas%2520a%2520universal%2520alignment%2520module%2520when%2520trained%2520on%2520multiple%2520datasets%252C%2520achieving%250Astrong%2520performance%2520even%2520in%2520zero-shot%2520scenarios.%2520Code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/pakinwu/VGLD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02704v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visually-Guided%20Linguistic%20Disambiguation%20for%20Monocular%20Depth%20Scale%0A%20%20Recovery&entry.906535625=Bojin%20Wu%20and%20Jing%20Chen&entry.1292438233=%20%20We%20propose%20a%20robust%20method%20for%20monocular%20depth%20scale%20recovery.%20Monocular%0Adepth%20estimation%20can%20be%20divided%20into%20two%20main%20directions%3A%20%281%29%20relative%20depth%0Aestimation%2C%20which%20provides%20normalized%20or%20inverse%20depth%20without%20scale%0Ainformation%2C%20and%20%282%29%20metric%20depth%20estimation%2C%20which%20involves%20recovering%20depth%0Awith%20absolute%20scale.%20To%20obtain%20absolute%20scale%20information%20for%20practical%0Adownstream%20tasks%2C%20utilizing%20textual%20information%20to%20recover%20the%20scale%20of%20a%0Arelative%20depth%20map%20is%20a%20highly%20promising%20approach.%20However%2C%20since%20a%20single%0Aimage%20can%20have%20multiple%20descriptions%20from%20different%20perspectives%20or%20with%0Avarying%20styles%2C%20it%20has%20been%20shown%20that%20different%20textual%20descriptions%20can%0Asignificantly%20affect%20the%20scale%20recovery%20process.%20To%20address%20this%20issue%2C%20our%0Amethod%2C%20VGLD%2C%20stabilizes%20the%20influence%20of%20textual%20information%20by%20incorporating%0Ahigh-level%20semantic%20information%20from%20the%20corresponding%20image%20alongside%20the%0Atextual%20description.%20This%20approach%20resolves%20textual%20ambiguities%20and%20robustly%0Aoutputs%20a%20set%20of%20linear%20transformation%20parameters%20%28scalars%29%20that%20can%20be%0Aglobally%20applied%20to%20the%20relative%20depth%20map%2C%20ultimately%20generating%20depth%0Apredictions%20with%20metric-scale%20accuracy.%20We%20validate%20our%20method%20across%20several%0Apopular%20relative%20depth%20models%28MiDas%2C%20DepthAnything%29%2C%20using%20both%20indoor%20scenes%0A%28NYUv2%29%20and%20outdoor%20scenes%20%28KITTI%29.%20Our%20results%20demonstrate%20that%20VGLD%20functions%0Aas%20a%20universal%20alignment%20module%20when%20trained%20on%20multiple%20datasets%2C%20achieving%0Astrong%20performance%20even%20in%20zero-shot%20scenarios.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/pakinwu/VGLD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02704v1&entry.124074799=Read"},
{"title": "HSplitLoRA: A Heterogeneous Split Parameter-Efficient Fine-Tuning\n  Framework for Large Language Models", "author": "Zheng Lin and Yuxin Zhang and Zhe Chen and Zihan Fang and Xianhao Chen and Praneeth Vepakomma and Wei Ni and Jun Luo and Yue Gao", "abstract": "  Recently, large language models (LLMs) have achieved remarkable\nbreakthroughs, revolutionizing the natural language processing domain and\nbeyond. Due to immense parameter sizes, fine-tuning these models with private\ndata for diverse downstream tasks has become mainstream. Though federated\nlearning (FL) offers a promising solution for fine-tuning LLMs without sharing\nraw data, substantial computing costs hinder its democratization. Moreover, in\nreal-world scenarios, private client devices often possess heterogeneous\ncomputing resources, further complicating LLM fine-tuning. To combat these\nchallenges, we propose HSplitLoRA, a heterogeneous parameter-efficient\nfine-tuning (PEFT) framework built on split learning (SL) and low-rank\nadaptation (LoRA) fine-tuning, for efficiently fine-tuning LLMs on\nheterogeneous client devices. HSplitLoRA first identifies important weights\nbased on their contributions to LLM training. It then dynamically configures\nthe decomposition ranks of LoRA adapters for selected weights and determines\nthe model split point according to varying computing budgets of client devices.\nFinally, a noise-free adapter aggregation mechanism is devised to support\nheterogeneous adapter aggregation without introducing noise. Extensive\nexperiments demonstrate that HSplitLoRA outperforms state-of-the-art benchmarks\nin training accuracy and convergence speed.\n", "link": "http://arxiv.org/abs/2505.02795v1", "date": "2025-05-05", "relevancy": 2.3376, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4745}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4697}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HSplitLoRA%3A%20A%20Heterogeneous%20Split%20Parameter-Efficient%20Fine-Tuning%0A%20%20Framework%20for%20Large%20Language%20Models&body=Title%3A%20HSplitLoRA%3A%20A%20Heterogeneous%20Split%20Parameter-Efficient%20Fine-Tuning%0A%20%20Framework%20for%20Large%20Language%20Models%0AAuthor%3A%20Zheng%20Lin%20and%20Yuxin%20Zhang%20and%20Zhe%20Chen%20and%20Zihan%20Fang%20and%20Xianhao%20Chen%20and%20Praneeth%20Vepakomma%20and%20Wei%20Ni%20and%20Jun%20Luo%20and%20Yue%20Gao%0AAbstract%3A%20%20%20Recently%2C%20large%20language%20models%20%28LLMs%29%20have%20achieved%20remarkable%0Abreakthroughs%2C%20revolutionizing%20the%20natural%20language%20processing%20domain%20and%0Abeyond.%20Due%20to%20immense%20parameter%20sizes%2C%20fine-tuning%20these%20models%20with%20private%0Adata%20for%20diverse%20downstream%20tasks%20has%20become%20mainstream.%20Though%20federated%0Alearning%20%28FL%29%20offers%20a%20promising%20solution%20for%20fine-tuning%20LLMs%20without%20sharing%0Araw%20data%2C%20substantial%20computing%20costs%20hinder%20its%20democratization.%20Moreover%2C%20in%0Areal-world%20scenarios%2C%20private%20client%20devices%20often%20possess%20heterogeneous%0Acomputing%20resources%2C%20further%20complicating%20LLM%20fine-tuning.%20To%20combat%20these%0Achallenges%2C%20we%20propose%20HSplitLoRA%2C%20a%20heterogeneous%20parameter-efficient%0Afine-tuning%20%28PEFT%29%20framework%20built%20on%20split%20learning%20%28SL%29%20and%20low-rank%0Aadaptation%20%28LoRA%29%20fine-tuning%2C%20for%20efficiently%20fine-tuning%20LLMs%20on%0Aheterogeneous%20client%20devices.%20HSplitLoRA%20first%20identifies%20important%20weights%0Abased%20on%20their%20contributions%20to%20LLM%20training.%20It%20then%20dynamically%20configures%0Athe%20decomposition%20ranks%20of%20LoRA%20adapters%20for%20selected%20weights%20and%20determines%0Athe%20model%20split%20point%20according%20to%20varying%20computing%20budgets%20of%20client%20devices.%0AFinally%2C%20a%20noise-free%20adapter%20aggregation%20mechanism%20is%20devised%20to%20support%0Aheterogeneous%20adapter%20aggregation%20without%20introducing%20noise.%20Extensive%0Aexperiments%20demonstrate%20that%20HSplitLoRA%20outperforms%20state-of-the-art%20benchmarks%0Ain%20training%20accuracy%20and%20convergence%20speed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02795v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHSplitLoRA%253A%2520A%2520Heterogeneous%2520Split%2520Parameter-Efficient%2520Fine-Tuning%250A%2520%2520Framework%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DZheng%2520Lin%2520and%2520Yuxin%2520Zhang%2520and%2520Zhe%2520Chen%2520and%2520Zihan%2520Fang%2520and%2520Xianhao%2520Chen%2520and%2520Praneeth%2520Vepakomma%2520and%2520Wei%2520Ni%2520and%2520Jun%2520Luo%2520and%2520Yue%2520Gao%26entry.1292438233%3D%2520%2520Recently%252C%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520achieved%2520remarkable%250Abreakthroughs%252C%2520revolutionizing%2520the%2520natural%2520language%2520processing%2520domain%2520and%250Abeyond.%2520Due%2520to%2520immense%2520parameter%2520sizes%252C%2520fine-tuning%2520these%2520models%2520with%2520private%250Adata%2520for%2520diverse%2520downstream%2520tasks%2520has%2520become%2520mainstream.%2520Though%2520federated%250Alearning%2520%2528FL%2529%2520offers%2520a%2520promising%2520solution%2520for%2520fine-tuning%2520LLMs%2520without%2520sharing%250Araw%2520data%252C%2520substantial%2520computing%2520costs%2520hinder%2520its%2520democratization.%2520Moreover%252C%2520in%250Areal-world%2520scenarios%252C%2520private%2520client%2520devices%2520often%2520possess%2520heterogeneous%250Acomputing%2520resources%252C%2520further%2520complicating%2520LLM%2520fine-tuning.%2520To%2520combat%2520these%250Achallenges%252C%2520we%2520propose%2520HSplitLoRA%252C%2520a%2520heterogeneous%2520parameter-efficient%250Afine-tuning%2520%2528PEFT%2529%2520framework%2520built%2520on%2520split%2520learning%2520%2528SL%2529%2520and%2520low-rank%250Aadaptation%2520%2528LoRA%2529%2520fine-tuning%252C%2520for%2520efficiently%2520fine-tuning%2520LLMs%2520on%250Aheterogeneous%2520client%2520devices.%2520HSplitLoRA%2520first%2520identifies%2520important%2520weights%250Abased%2520on%2520their%2520contributions%2520to%2520LLM%2520training.%2520It%2520then%2520dynamically%2520configures%250Athe%2520decomposition%2520ranks%2520of%2520LoRA%2520adapters%2520for%2520selected%2520weights%2520and%2520determines%250Athe%2520model%2520split%2520point%2520according%2520to%2520varying%2520computing%2520budgets%2520of%2520client%2520devices.%250AFinally%252C%2520a%2520noise-free%2520adapter%2520aggregation%2520mechanism%2520is%2520devised%2520to%2520support%250Aheterogeneous%2520adapter%2520aggregation%2520without%2520introducing%2520noise.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520HSplitLoRA%2520outperforms%2520state-of-the-art%2520benchmarks%250Ain%2520training%2520accuracy%2520and%2520convergence%2520speed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02795v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HSplitLoRA%3A%20A%20Heterogeneous%20Split%20Parameter-Efficient%20Fine-Tuning%0A%20%20Framework%20for%20Large%20Language%20Models&entry.906535625=Zheng%20Lin%20and%20Yuxin%20Zhang%20and%20Zhe%20Chen%20and%20Zihan%20Fang%20and%20Xianhao%20Chen%20and%20Praneeth%20Vepakomma%20and%20Wei%20Ni%20and%20Jun%20Luo%20and%20Yue%20Gao&entry.1292438233=%20%20Recently%2C%20large%20language%20models%20%28LLMs%29%20have%20achieved%20remarkable%0Abreakthroughs%2C%20revolutionizing%20the%20natural%20language%20processing%20domain%20and%0Abeyond.%20Due%20to%20immense%20parameter%20sizes%2C%20fine-tuning%20these%20models%20with%20private%0Adata%20for%20diverse%20downstream%20tasks%20has%20become%20mainstream.%20Though%20federated%0Alearning%20%28FL%29%20offers%20a%20promising%20solution%20for%20fine-tuning%20LLMs%20without%20sharing%0Araw%20data%2C%20substantial%20computing%20costs%20hinder%20its%20democratization.%20Moreover%2C%20in%0Areal-world%20scenarios%2C%20private%20client%20devices%20often%20possess%20heterogeneous%0Acomputing%20resources%2C%20further%20complicating%20LLM%20fine-tuning.%20To%20combat%20these%0Achallenges%2C%20we%20propose%20HSplitLoRA%2C%20a%20heterogeneous%20parameter-efficient%0Afine-tuning%20%28PEFT%29%20framework%20built%20on%20split%20learning%20%28SL%29%20and%20low-rank%0Aadaptation%20%28LoRA%29%20fine-tuning%2C%20for%20efficiently%20fine-tuning%20LLMs%20on%0Aheterogeneous%20client%20devices.%20HSplitLoRA%20first%20identifies%20important%20weights%0Abased%20on%20their%20contributions%20to%20LLM%20training.%20It%20then%20dynamically%20configures%0Athe%20decomposition%20ranks%20of%20LoRA%20adapters%20for%20selected%20weights%20and%20determines%0Athe%20model%20split%20point%20according%20to%20varying%20computing%20budgets%20of%20client%20devices.%0AFinally%2C%20a%20noise-free%20adapter%20aggregation%20mechanism%20is%20devised%20to%20support%0Aheterogeneous%20adapter%20aggregation%20without%20introducing%20noise.%20Extensive%0Aexperiments%20demonstrate%20that%20HSplitLoRA%20outperforms%20state-of-the-art%20benchmarks%0Ain%20training%20accuracy%20and%20convergence%20speed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02795v1&entry.124074799=Read"},
{"title": "Unsupervised training of keypoint-agnostic descriptors for flexible\n  retinal image registration", "author": "David Rivas-Villar and \u00c1lvaro S. Hervella and Jos\u00e9 Rouco and Jorge Novo", "abstract": "  Current color fundus image registration approaches are limited, among other\nthings, by the lack of labeled data, which is even more significant in the\nmedical domain, motivating the use of unsupervised learning. Therefore, in this\nwork, we develop a novel unsupervised descriptor learning method that does not\nrely on keypoint detection. This enables the resulting descriptor network to be\nagnostic to the keypoint detector used during the registration inference.\n  To validate this approach, we perform an extensive and comprehensive\ncomparison on the reference public retinal image registration dataset.\nAdditionally, we test our method with multiple keypoint detectors of varied\nnature, even proposing some novel ones. Our results demonstrate that the\nproposed approach offers accurate registration, not incurring in any\nperformance loss versus supervised methods. Additionally, it demonstrates\naccurate performance regardless of the keypoint detector used. Thus, this work\nrepresents a notable step towards leveraging unsupervised learning in the\nmedical domain.\n", "link": "http://arxiv.org/abs/2505.02787v1", "date": "2025-05-05", "relevancy": 2.3376, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6144}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5742}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20training%20of%20keypoint-agnostic%20descriptors%20for%20flexible%0A%20%20retinal%20image%20registration&body=Title%3A%20Unsupervised%20training%20of%20keypoint-agnostic%20descriptors%20for%20flexible%0A%20%20retinal%20image%20registration%0AAuthor%3A%20David%20Rivas-Villar%20and%20%C3%81lvaro%20S.%20Hervella%20and%20Jos%C3%A9%20Rouco%20and%20Jorge%20Novo%0AAbstract%3A%20%20%20Current%20color%20fundus%20image%20registration%20approaches%20are%20limited%2C%20among%20other%0Athings%2C%20by%20the%20lack%20of%20labeled%20data%2C%20which%20is%20even%20more%20significant%20in%20the%0Amedical%20domain%2C%20motivating%20the%20use%20of%20unsupervised%20learning.%20Therefore%2C%20in%20this%0Awork%2C%20we%20develop%20a%20novel%20unsupervised%20descriptor%20learning%20method%20that%20does%20not%0Arely%20on%20keypoint%20detection.%20This%20enables%20the%20resulting%20descriptor%20network%20to%20be%0Aagnostic%20to%20the%20keypoint%20detector%20used%20during%20the%20registration%20inference.%0A%20%20To%20validate%20this%20approach%2C%20we%20perform%20an%20extensive%20and%20comprehensive%0Acomparison%20on%20the%20reference%20public%20retinal%20image%20registration%20dataset.%0AAdditionally%2C%20we%20test%20our%20method%20with%20multiple%20keypoint%20detectors%20of%20varied%0Anature%2C%20even%20proposing%20some%20novel%20ones.%20Our%20results%20demonstrate%20that%20the%0Aproposed%20approach%20offers%20accurate%20registration%2C%20not%20incurring%20in%20any%0Aperformance%20loss%20versus%20supervised%20methods.%20Additionally%2C%20it%20demonstrates%0Aaccurate%20performance%20regardless%20of%20the%20keypoint%20detector%20used.%20Thus%2C%20this%20work%0Arepresents%20a%20notable%20step%20towards%20leveraging%20unsupervised%20learning%20in%20the%0Amedical%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02787v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520training%2520of%2520keypoint-agnostic%2520descriptors%2520for%2520flexible%250A%2520%2520retinal%2520image%2520registration%26entry.906535625%3DDavid%2520Rivas-Villar%2520and%2520%25C3%2581lvaro%2520S.%2520Hervella%2520and%2520Jos%25C3%25A9%2520Rouco%2520and%2520Jorge%2520Novo%26entry.1292438233%3D%2520%2520Current%2520color%2520fundus%2520image%2520registration%2520approaches%2520are%2520limited%252C%2520among%2520other%250Athings%252C%2520by%2520the%2520lack%2520of%2520labeled%2520data%252C%2520which%2520is%2520even%2520more%2520significant%2520in%2520the%250Amedical%2520domain%252C%2520motivating%2520the%2520use%2520of%2520unsupervised%2520learning.%2520Therefore%252C%2520in%2520this%250Awork%252C%2520we%2520develop%2520a%2520novel%2520unsupervised%2520descriptor%2520learning%2520method%2520that%2520does%2520not%250Arely%2520on%2520keypoint%2520detection.%2520This%2520enables%2520the%2520resulting%2520descriptor%2520network%2520to%2520be%250Aagnostic%2520to%2520the%2520keypoint%2520detector%2520used%2520during%2520the%2520registration%2520inference.%250A%2520%2520To%2520validate%2520this%2520approach%252C%2520we%2520perform%2520an%2520extensive%2520and%2520comprehensive%250Acomparison%2520on%2520the%2520reference%2520public%2520retinal%2520image%2520registration%2520dataset.%250AAdditionally%252C%2520we%2520test%2520our%2520method%2520with%2520multiple%2520keypoint%2520detectors%2520of%2520varied%250Anature%252C%2520even%2520proposing%2520some%2520novel%2520ones.%2520Our%2520results%2520demonstrate%2520that%2520the%250Aproposed%2520approach%2520offers%2520accurate%2520registration%252C%2520not%2520incurring%2520in%2520any%250Aperformance%2520loss%2520versus%2520supervised%2520methods.%2520Additionally%252C%2520it%2520demonstrates%250Aaccurate%2520performance%2520regardless%2520of%2520the%2520keypoint%2520detector%2520used.%2520Thus%252C%2520this%2520work%250Arepresents%2520a%2520notable%2520step%2520towards%2520leveraging%2520unsupervised%2520learning%2520in%2520the%250Amedical%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02787v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20training%20of%20keypoint-agnostic%20descriptors%20for%20flexible%0A%20%20retinal%20image%20registration&entry.906535625=David%20Rivas-Villar%20and%20%C3%81lvaro%20S.%20Hervella%20and%20Jos%C3%A9%20Rouco%20and%20Jorge%20Novo&entry.1292438233=%20%20Current%20color%20fundus%20image%20registration%20approaches%20are%20limited%2C%20among%20other%0Athings%2C%20by%20the%20lack%20of%20labeled%20data%2C%20which%20is%20even%20more%20significant%20in%20the%0Amedical%20domain%2C%20motivating%20the%20use%20of%20unsupervised%20learning.%20Therefore%2C%20in%20this%0Awork%2C%20we%20develop%20a%20novel%20unsupervised%20descriptor%20learning%20method%20that%20does%20not%0Arely%20on%20keypoint%20detection.%20This%20enables%20the%20resulting%20descriptor%20network%20to%20be%0Aagnostic%20to%20the%20keypoint%20detector%20used%20during%20the%20registration%20inference.%0A%20%20To%20validate%20this%20approach%2C%20we%20perform%20an%20extensive%20and%20comprehensive%0Acomparison%20on%20the%20reference%20public%20retinal%20image%20registration%20dataset.%0AAdditionally%2C%20we%20test%20our%20method%20with%20multiple%20keypoint%20detectors%20of%20varied%0Anature%2C%20even%20proposing%20some%20novel%20ones.%20Our%20results%20demonstrate%20that%20the%0Aproposed%20approach%20offers%20accurate%20registration%2C%20not%20incurring%20in%20any%0Aperformance%20loss%20versus%20supervised%20methods.%20Additionally%2C%20it%20demonstrates%0Aaccurate%20performance%20regardless%20of%20the%20keypoint%20detector%20used.%20Thus%2C%20this%20work%0Arepresents%20a%20notable%20step%20towards%20leveraging%20unsupervised%20learning%20in%20the%0Amedical%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02787v1&entry.124074799=Read"},
{"title": "LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive\n  Streaming Speech Synthesis", "author": "Qingkai Fang and Yan Zhou and Shoutao Guo and Shaolei Zhang and Yang Feng", "abstract": "  Real-time, intelligent, and natural speech interaction is an essential part\nof the next-generation human-computer interaction. Recent advancements have\nshowcased the potential of building intelligent spoken chatbots based on large\nlanguage models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of\nspeech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable\nof achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built\nupon the Qwen2.5 series models, integrating a speech encoder and an\nautoregressive streaming speech decoder. Despite being trained on only 200K\nmulti-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong\nperformance on several spoken question answering and speech instruction\nfollowing benchmarks, surpassing previous state-of-the-art SpeechLMs like\nGLM-4-Voice, which was trained on millions of hours of speech data.\n", "link": "http://arxiv.org/abs/2505.02625v1", "date": "2025-05-05", "relevancy": 2.3239, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.47}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.47}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLaMA-Omni2%3A%20LLM-based%20Real-time%20Spoken%20Chatbot%20with%20Autoregressive%0A%20%20Streaming%20Speech%20Synthesis&body=Title%3A%20LLaMA-Omni2%3A%20LLM-based%20Real-time%20Spoken%20Chatbot%20with%20Autoregressive%0A%20%20Streaming%20Speech%20Synthesis%0AAuthor%3A%20Qingkai%20Fang%20and%20Yan%20Zhou%20and%20Shoutao%20Guo%20and%20Shaolei%20Zhang%20and%20Yang%20Feng%0AAbstract%3A%20%20%20Real-time%2C%20intelligent%2C%20and%20natural%20speech%20interaction%20is%20an%20essential%20part%0Aof%20the%20next-generation%20human-computer%20interaction.%20Recent%20advancements%20have%0Ashowcased%20the%20potential%20of%20building%20intelligent%20spoken%20chatbots%20based%20on%20large%0Alanguage%20models%20%28LLMs%29.%20In%20this%20paper%2C%20we%20introduce%20LLaMA-Omni%202%2C%20a%20series%20of%0Aspeech%20language%20models%20%28SpeechLMs%29%20ranging%20from%200.5B%20to%2014B%20parameters%2C%20capable%0Aof%20achieving%20high-quality%20real-time%20speech%20interaction.%20LLaMA-Omni%202%20is%20built%0Aupon%20the%20Qwen2.5%20series%20models%2C%20integrating%20a%20speech%20encoder%20and%20an%0Aautoregressive%20streaming%20speech%20decoder.%20Despite%20being%20trained%20on%20only%20200K%0Amulti-turn%20speech%20dialogue%20samples%2C%20LLaMA-Omni%202%20demonstrates%20strong%0Aperformance%20on%20several%20spoken%20question%20answering%20and%20speech%20instruction%0Afollowing%20benchmarks%2C%20surpassing%20previous%20state-of-the-art%20SpeechLMs%20like%0AGLM-4-Voice%2C%20which%20was%20trained%20on%20millions%20of%20hours%20of%20speech%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02625v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLaMA-Omni2%253A%2520LLM-based%2520Real-time%2520Spoken%2520Chatbot%2520with%2520Autoregressive%250A%2520%2520Streaming%2520Speech%2520Synthesis%26entry.906535625%3DQingkai%2520Fang%2520and%2520Yan%2520Zhou%2520and%2520Shoutao%2520Guo%2520and%2520Shaolei%2520Zhang%2520and%2520Yang%2520Feng%26entry.1292438233%3D%2520%2520Real-time%252C%2520intelligent%252C%2520and%2520natural%2520speech%2520interaction%2520is%2520an%2520essential%2520part%250Aof%2520the%2520next-generation%2520human-computer%2520interaction.%2520Recent%2520advancements%2520have%250Ashowcased%2520the%2520potential%2520of%2520building%2520intelligent%2520spoken%2520chatbots%2520based%2520on%2520large%250Alanguage%2520models%2520%2528LLMs%2529.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520LLaMA-Omni%25202%252C%2520a%2520series%2520of%250Aspeech%2520language%2520models%2520%2528SpeechLMs%2529%2520ranging%2520from%25200.5B%2520to%252014B%2520parameters%252C%2520capable%250Aof%2520achieving%2520high-quality%2520real-time%2520speech%2520interaction.%2520LLaMA-Omni%25202%2520is%2520built%250Aupon%2520the%2520Qwen2.5%2520series%2520models%252C%2520integrating%2520a%2520speech%2520encoder%2520and%2520an%250Aautoregressive%2520streaming%2520speech%2520decoder.%2520Despite%2520being%2520trained%2520on%2520only%2520200K%250Amulti-turn%2520speech%2520dialogue%2520samples%252C%2520LLaMA-Omni%25202%2520demonstrates%2520strong%250Aperformance%2520on%2520several%2520spoken%2520question%2520answering%2520and%2520speech%2520instruction%250Afollowing%2520benchmarks%252C%2520surpassing%2520previous%2520state-of-the-art%2520SpeechLMs%2520like%250AGLM-4-Voice%252C%2520which%2520was%2520trained%2520on%2520millions%2520of%2520hours%2520of%2520speech%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02625v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLaMA-Omni2%3A%20LLM-based%20Real-time%20Spoken%20Chatbot%20with%20Autoregressive%0A%20%20Streaming%20Speech%20Synthesis&entry.906535625=Qingkai%20Fang%20and%20Yan%20Zhou%20and%20Shoutao%20Guo%20and%20Shaolei%20Zhang%20and%20Yang%20Feng&entry.1292438233=%20%20Real-time%2C%20intelligent%2C%20and%20natural%20speech%20interaction%20is%20an%20essential%20part%0Aof%20the%20next-generation%20human-computer%20interaction.%20Recent%20advancements%20have%0Ashowcased%20the%20potential%20of%20building%20intelligent%20spoken%20chatbots%20based%20on%20large%0Alanguage%20models%20%28LLMs%29.%20In%20this%20paper%2C%20we%20introduce%20LLaMA-Omni%202%2C%20a%20series%20of%0Aspeech%20language%20models%20%28SpeechLMs%29%20ranging%20from%200.5B%20to%2014B%20parameters%2C%20capable%0Aof%20achieving%20high-quality%20real-time%20speech%20interaction.%20LLaMA-Omni%202%20is%20built%0Aupon%20the%20Qwen2.5%20series%20models%2C%20integrating%20a%20speech%20encoder%20and%20an%0Aautoregressive%20streaming%20speech%20decoder.%20Despite%20being%20trained%20on%20only%20200K%0Amulti-turn%20speech%20dialogue%20samples%2C%20LLaMA-Omni%202%20demonstrates%20strong%0Aperformance%20on%20several%20spoken%20question%20answering%20and%20speech%20instruction%0Afollowing%20benchmarks%2C%20surpassing%20previous%20state-of-the-art%20SpeechLMs%20like%0AGLM-4-Voice%2C%20which%20was%20trained%20on%20millions%20of%20hours%20of%20speech%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02625v1&entry.124074799=Read"},
{"title": "Marker-Based Extrinsic Calibration Method for Accurate Multi-Camera 3D\n  Reconstruction", "author": "Nahuel Garcia-D'Urso and Bernabe Sanchez-Sos and Jorge Azorin-Lopez and Andres Fuster-Guillo and Antonio Macia-Lillo and Higinio Mora-Mora", "abstract": "  Accurate 3D reconstruction using multi-camera RGB-D systems critically\ndepends on precise extrinsic calibration to achieve proper alignment between\ncaptured views. In this paper, we introduce an iterative extrinsic calibration\nmethod that leverages the geometric constraints provided by a three-dimensional\nmarker to significantly improve calibration accuracy. Our proposed approach\nsystematically segments and refines marker planes through clustering,\nregression analysis, and iterative reassignment techniques, ensuring robust\ngeometric correspondence across camera views. We validate our method\ncomprehensively in both controlled environments and practical real-world\nsettings within the Tech4Diet project, aimed at modeling the physical\nprogression of patients undergoing nutritional treatments. Experimental results\ndemonstrate substantial reductions in alignment errors, facilitating accurate\nand reliable 3D reconstructions.\n", "link": "http://arxiv.org/abs/2505.02539v1", "date": "2025-05-05", "relevancy": 2.3135, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5951}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5836}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5664}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Marker-Based%20Extrinsic%20Calibration%20Method%20for%20Accurate%20Multi-Camera%203D%0A%20%20Reconstruction&body=Title%3A%20Marker-Based%20Extrinsic%20Calibration%20Method%20for%20Accurate%20Multi-Camera%203D%0A%20%20Reconstruction%0AAuthor%3A%20Nahuel%20Garcia-D%27Urso%20and%20Bernabe%20Sanchez-Sos%20and%20Jorge%20Azorin-Lopez%20and%20Andres%20Fuster-Guillo%20and%20Antonio%20Macia-Lillo%20and%20Higinio%20Mora-Mora%0AAbstract%3A%20%20%20Accurate%203D%20reconstruction%20using%20multi-camera%20RGB-D%20systems%20critically%0Adepends%20on%20precise%20extrinsic%20calibration%20to%20achieve%20proper%20alignment%20between%0Acaptured%20views.%20In%20this%20paper%2C%20we%20introduce%20an%20iterative%20extrinsic%20calibration%0Amethod%20that%20leverages%20the%20geometric%20constraints%20provided%20by%20a%20three-dimensional%0Amarker%20to%20significantly%20improve%20calibration%20accuracy.%20Our%20proposed%20approach%0Asystematically%20segments%20and%20refines%20marker%20planes%20through%20clustering%2C%0Aregression%20analysis%2C%20and%20iterative%20reassignment%20techniques%2C%20ensuring%20robust%0Ageometric%20correspondence%20across%20camera%20views.%20We%20validate%20our%20method%0Acomprehensively%20in%20both%20controlled%20environments%20and%20practical%20real-world%0Asettings%20within%20the%20Tech4Diet%20project%2C%20aimed%20at%20modeling%20the%20physical%0Aprogression%20of%20patients%20undergoing%20nutritional%20treatments.%20Experimental%20results%0Ademonstrate%20substantial%20reductions%20in%20alignment%20errors%2C%20facilitating%20accurate%0Aand%20reliable%203D%20reconstructions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02539v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMarker-Based%2520Extrinsic%2520Calibration%2520Method%2520for%2520Accurate%2520Multi-Camera%25203D%250A%2520%2520Reconstruction%26entry.906535625%3DNahuel%2520Garcia-D%2527Urso%2520and%2520Bernabe%2520Sanchez-Sos%2520and%2520Jorge%2520Azorin-Lopez%2520and%2520Andres%2520Fuster-Guillo%2520and%2520Antonio%2520Macia-Lillo%2520and%2520Higinio%2520Mora-Mora%26entry.1292438233%3D%2520%2520Accurate%25203D%2520reconstruction%2520using%2520multi-camera%2520RGB-D%2520systems%2520critically%250Adepends%2520on%2520precise%2520extrinsic%2520calibration%2520to%2520achieve%2520proper%2520alignment%2520between%250Acaptured%2520views.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520an%2520iterative%2520extrinsic%2520calibration%250Amethod%2520that%2520leverages%2520the%2520geometric%2520constraints%2520provided%2520by%2520a%2520three-dimensional%250Amarker%2520to%2520significantly%2520improve%2520calibration%2520accuracy.%2520Our%2520proposed%2520approach%250Asystematically%2520segments%2520and%2520refines%2520marker%2520planes%2520through%2520clustering%252C%250Aregression%2520analysis%252C%2520and%2520iterative%2520reassignment%2520techniques%252C%2520ensuring%2520robust%250Ageometric%2520correspondence%2520across%2520camera%2520views.%2520We%2520validate%2520our%2520method%250Acomprehensively%2520in%2520both%2520controlled%2520environments%2520and%2520practical%2520real-world%250Asettings%2520within%2520the%2520Tech4Diet%2520project%252C%2520aimed%2520at%2520modeling%2520the%2520physical%250Aprogression%2520of%2520patients%2520undergoing%2520nutritional%2520treatments.%2520Experimental%2520results%250Ademonstrate%2520substantial%2520reductions%2520in%2520alignment%2520errors%252C%2520facilitating%2520accurate%250Aand%2520reliable%25203D%2520reconstructions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02539v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Marker-Based%20Extrinsic%20Calibration%20Method%20for%20Accurate%20Multi-Camera%203D%0A%20%20Reconstruction&entry.906535625=Nahuel%20Garcia-D%27Urso%20and%20Bernabe%20Sanchez-Sos%20and%20Jorge%20Azorin-Lopez%20and%20Andres%20Fuster-Guillo%20and%20Antonio%20Macia-Lillo%20and%20Higinio%20Mora-Mora&entry.1292438233=%20%20Accurate%203D%20reconstruction%20using%20multi-camera%20RGB-D%20systems%20critically%0Adepends%20on%20precise%20extrinsic%20calibration%20to%20achieve%20proper%20alignment%20between%0Acaptured%20views.%20In%20this%20paper%2C%20we%20introduce%20an%20iterative%20extrinsic%20calibration%0Amethod%20that%20leverages%20the%20geometric%20constraints%20provided%20by%20a%20three-dimensional%0Amarker%20to%20significantly%20improve%20calibration%20accuracy.%20Our%20proposed%20approach%0Asystematically%20segments%20and%20refines%20marker%20planes%20through%20clustering%2C%0Aregression%20analysis%2C%20and%20iterative%20reassignment%20techniques%2C%20ensuring%20robust%0Ageometric%20correspondence%20across%20camera%20views.%20We%20validate%20our%20method%0Acomprehensively%20in%20both%20controlled%20environments%20and%20practical%20real-world%0Asettings%20within%20the%20Tech4Diet%20project%2C%20aimed%20at%20modeling%20the%20physical%0Aprogression%20of%20patients%20undergoing%20nutritional%20treatments.%20Experimental%20results%0Ademonstrate%20substantial%20reductions%20in%20alignment%20errors%2C%20facilitating%20accurate%0Aand%20reliable%203D%20reconstructions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02539v1&entry.124074799=Read"},
{"title": "High and Low Resolution Tradeoffs in Roadside Multimodal Sensing", "author": "Shaozu Ding and Yihong Tang and Marco De Vincenzi and Dajiang Suo", "abstract": "  Balancing cost and performance is crucial when choosing high- versus\nlow-resolution point-cloud roadside sensors. For example, LiDAR delivers dense\npoint cloud, while 4D millimeter-wave radar, though spatially sparser, embeds\nvelocity cues that help distinguish objects and come at a lower price.\nUnfortunately, the sensor placement strategies will influence point cloud\ndensity and distribution across the coverage area. Compounding the first\nchallenge is the fact that different sensor mixtures often demand distinct\nneural network architectures to maximize their complementary strengths. Without\nan evaluation framework that establishes a benchmark for comparison, it is\nimprudent to make claims regarding whether marginal gains result from higher\nresolution and new sensing modalities or from the algorithms. We present an\nex-ante evaluation that addresses the two challenges. First, we realized a\nsimulation tool that builds on integer programming to automatically compare\ndifferent sensor placement strategies against coverage and cost jointly.\nAdditionally, inspired by human multi-sensory integration, we propose a modular\nframework to assess whether reductions in spatial resolution can be compensated\nby informational richness in detecting traffic participants. Extensive\nexperimental testing on the proposed framework shows that fusing\nvelocity-encoded radar with low-resolution LiDAR yields marked gains (14\npercent AP for pedestrians and an overall mAP improvement of 1.5 percent across\nsix categories) at lower cost than high-resolution LiDAR alone. Notably, these\nmarked gains hold regardless of the specific deep neural modules employed in\nour frame. The result challenges the prevailing assumption that high resolution\nare always superior to low-resolution alternatives.\n", "link": "http://arxiv.org/abs/2410.01250v2", "date": "2025-05-05", "relevancy": 2.2992, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5976}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.59}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High%20and%20Low%20Resolution%20Tradeoffs%20in%20Roadside%20Multimodal%20Sensing&body=Title%3A%20High%20and%20Low%20Resolution%20Tradeoffs%20in%20Roadside%20Multimodal%20Sensing%0AAuthor%3A%20Shaozu%20Ding%20and%20Yihong%20Tang%20and%20Marco%20De%20Vincenzi%20and%20Dajiang%20Suo%0AAbstract%3A%20%20%20Balancing%20cost%20and%20performance%20is%20crucial%20when%20choosing%20high-%20versus%0Alow-resolution%20point-cloud%20roadside%20sensors.%20For%20example%2C%20LiDAR%20delivers%20dense%0Apoint%20cloud%2C%20while%204D%20millimeter-wave%20radar%2C%20though%20spatially%20sparser%2C%20embeds%0Avelocity%20cues%20that%20help%20distinguish%20objects%20and%20come%20at%20a%20lower%20price.%0AUnfortunately%2C%20the%20sensor%20placement%20strategies%20will%20influence%20point%20cloud%0Adensity%20and%20distribution%20across%20the%20coverage%20area.%20Compounding%20the%20first%0Achallenge%20is%20the%20fact%20that%20different%20sensor%20mixtures%20often%20demand%20distinct%0Aneural%20network%20architectures%20to%20maximize%20their%20complementary%20strengths.%20Without%0Aan%20evaluation%20framework%20that%20establishes%20a%20benchmark%20for%20comparison%2C%20it%20is%0Aimprudent%20to%20make%20claims%20regarding%20whether%20marginal%20gains%20result%20from%20higher%0Aresolution%20and%20new%20sensing%20modalities%20or%20from%20the%20algorithms.%20We%20present%20an%0Aex-ante%20evaluation%20that%20addresses%20the%20two%20challenges.%20First%2C%20we%20realized%20a%0Asimulation%20tool%20that%20builds%20on%20integer%20programming%20to%20automatically%20compare%0Adifferent%20sensor%20placement%20strategies%20against%20coverage%20and%20cost%20jointly.%0AAdditionally%2C%20inspired%20by%20human%20multi-sensory%20integration%2C%20we%20propose%20a%20modular%0Aframework%20to%20assess%20whether%20reductions%20in%20spatial%20resolution%20can%20be%20compensated%0Aby%20informational%20richness%20in%20detecting%20traffic%20participants.%20Extensive%0Aexperimental%20testing%20on%20the%20proposed%20framework%20shows%20that%20fusing%0Avelocity-encoded%20radar%20with%20low-resolution%20LiDAR%20yields%20marked%20gains%20%2814%0Apercent%20AP%20for%20pedestrians%20and%20an%20overall%20mAP%20improvement%20of%201.5%20percent%20across%0Asix%20categories%29%20at%20lower%20cost%20than%20high-resolution%20LiDAR%20alone.%20Notably%2C%20these%0Amarked%20gains%20hold%20regardless%20of%20the%20specific%20deep%20neural%20modules%20employed%20in%0Aour%20frame.%20The%20result%20challenges%20the%20prevailing%20assumption%20that%20high%20resolution%0Aare%20always%20superior%20to%20low-resolution%20alternatives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01250v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh%2520and%2520Low%2520Resolution%2520Tradeoffs%2520in%2520Roadside%2520Multimodal%2520Sensing%26entry.906535625%3DShaozu%2520Ding%2520and%2520Yihong%2520Tang%2520and%2520Marco%2520De%2520Vincenzi%2520and%2520Dajiang%2520Suo%26entry.1292438233%3D%2520%2520Balancing%2520cost%2520and%2520performance%2520is%2520crucial%2520when%2520choosing%2520high-%2520versus%250Alow-resolution%2520point-cloud%2520roadside%2520sensors.%2520For%2520example%252C%2520LiDAR%2520delivers%2520dense%250Apoint%2520cloud%252C%2520while%25204D%2520millimeter-wave%2520radar%252C%2520though%2520spatially%2520sparser%252C%2520embeds%250Avelocity%2520cues%2520that%2520help%2520distinguish%2520objects%2520and%2520come%2520at%2520a%2520lower%2520price.%250AUnfortunately%252C%2520the%2520sensor%2520placement%2520strategies%2520will%2520influence%2520point%2520cloud%250Adensity%2520and%2520distribution%2520across%2520the%2520coverage%2520area.%2520Compounding%2520the%2520first%250Achallenge%2520is%2520the%2520fact%2520that%2520different%2520sensor%2520mixtures%2520often%2520demand%2520distinct%250Aneural%2520network%2520architectures%2520to%2520maximize%2520their%2520complementary%2520strengths.%2520Without%250Aan%2520evaluation%2520framework%2520that%2520establishes%2520a%2520benchmark%2520for%2520comparison%252C%2520it%2520is%250Aimprudent%2520to%2520make%2520claims%2520regarding%2520whether%2520marginal%2520gains%2520result%2520from%2520higher%250Aresolution%2520and%2520new%2520sensing%2520modalities%2520or%2520from%2520the%2520algorithms.%2520We%2520present%2520an%250Aex-ante%2520evaluation%2520that%2520addresses%2520the%2520two%2520challenges.%2520First%252C%2520we%2520realized%2520a%250Asimulation%2520tool%2520that%2520builds%2520on%2520integer%2520programming%2520to%2520automatically%2520compare%250Adifferent%2520sensor%2520placement%2520strategies%2520against%2520coverage%2520and%2520cost%2520jointly.%250AAdditionally%252C%2520inspired%2520by%2520human%2520multi-sensory%2520integration%252C%2520we%2520propose%2520a%2520modular%250Aframework%2520to%2520assess%2520whether%2520reductions%2520in%2520spatial%2520resolution%2520can%2520be%2520compensated%250Aby%2520informational%2520richness%2520in%2520detecting%2520traffic%2520participants.%2520Extensive%250Aexperimental%2520testing%2520on%2520the%2520proposed%2520framework%2520shows%2520that%2520fusing%250Avelocity-encoded%2520radar%2520with%2520low-resolution%2520LiDAR%2520yields%2520marked%2520gains%2520%252814%250Apercent%2520AP%2520for%2520pedestrians%2520and%2520an%2520overall%2520mAP%2520improvement%2520of%25201.5%2520percent%2520across%250Asix%2520categories%2529%2520at%2520lower%2520cost%2520than%2520high-resolution%2520LiDAR%2520alone.%2520Notably%252C%2520these%250Amarked%2520gains%2520hold%2520regardless%2520of%2520the%2520specific%2520deep%2520neural%2520modules%2520employed%2520in%250Aour%2520frame.%2520The%2520result%2520challenges%2520the%2520prevailing%2520assumption%2520that%2520high%2520resolution%250Aare%2520always%2520superior%2520to%2520low-resolution%2520alternatives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01250v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High%20and%20Low%20Resolution%20Tradeoffs%20in%20Roadside%20Multimodal%20Sensing&entry.906535625=Shaozu%20Ding%20and%20Yihong%20Tang%20and%20Marco%20De%20Vincenzi%20and%20Dajiang%20Suo&entry.1292438233=%20%20Balancing%20cost%20and%20performance%20is%20crucial%20when%20choosing%20high-%20versus%0Alow-resolution%20point-cloud%20roadside%20sensors.%20For%20example%2C%20LiDAR%20delivers%20dense%0Apoint%20cloud%2C%20while%204D%20millimeter-wave%20radar%2C%20though%20spatially%20sparser%2C%20embeds%0Avelocity%20cues%20that%20help%20distinguish%20objects%20and%20come%20at%20a%20lower%20price.%0AUnfortunately%2C%20the%20sensor%20placement%20strategies%20will%20influence%20point%20cloud%0Adensity%20and%20distribution%20across%20the%20coverage%20area.%20Compounding%20the%20first%0Achallenge%20is%20the%20fact%20that%20different%20sensor%20mixtures%20often%20demand%20distinct%0Aneural%20network%20architectures%20to%20maximize%20their%20complementary%20strengths.%20Without%0Aan%20evaluation%20framework%20that%20establishes%20a%20benchmark%20for%20comparison%2C%20it%20is%0Aimprudent%20to%20make%20claims%20regarding%20whether%20marginal%20gains%20result%20from%20higher%0Aresolution%20and%20new%20sensing%20modalities%20or%20from%20the%20algorithms.%20We%20present%20an%0Aex-ante%20evaluation%20that%20addresses%20the%20two%20challenges.%20First%2C%20we%20realized%20a%0Asimulation%20tool%20that%20builds%20on%20integer%20programming%20to%20automatically%20compare%0Adifferent%20sensor%20placement%20strategies%20against%20coverage%20and%20cost%20jointly.%0AAdditionally%2C%20inspired%20by%20human%20multi-sensory%20integration%2C%20we%20propose%20a%20modular%0Aframework%20to%20assess%20whether%20reductions%20in%20spatial%20resolution%20can%20be%20compensated%0Aby%20informational%20richness%20in%20detecting%20traffic%20participants.%20Extensive%0Aexperimental%20testing%20on%20the%20proposed%20framework%20shows%20that%20fusing%0Avelocity-encoded%20radar%20with%20low-resolution%20LiDAR%20yields%20marked%20gains%20%2814%0Apercent%20AP%20for%20pedestrians%20and%20an%20overall%20mAP%20improvement%20of%201.5%20percent%20across%0Asix%20categories%29%20at%20lower%20cost%20than%20high-resolution%20LiDAR%20alone.%20Notably%2C%20these%0Amarked%20gains%20hold%20regardless%20of%20the%20specific%20deep%20neural%20modules%20employed%20in%0Aour%20frame.%20The%20result%20challenges%20the%20prevailing%20assumption%20that%20high%20resolution%0Aare%20always%20superior%20to%20low-resolution%20alternatives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01250v2&entry.124074799=Read"},
{"title": "FissionVAE: Federated Non-IID Image Generation with Latent Space and\n  Decoder Decomposition", "author": "Chen Hu and Hanchi Ren and Jingjing Deng and Xianghua Xie and Xiaoke Ma", "abstract": "  Federated learning is a machine learning paradigm that enables decentralized\nclients to collaboratively learn a shared model while keeping all the training\ndata local. While considerable research has focused on federated image\ngeneration, particularly Generative Adversarial Networks, Variational\nAutoencoders have received less attention. In this paper, we address the\nchallenges of non-IID (independently and identically distributed) data\nenvironments featuring multiple groups of images of different types. Non-IID\ndata distributions can lead to difficulties in maintaining a consistent latent\nspace and can also result in local generators with disparate texture features\nbeing blended during aggregation. We thereby introduce FissionVAE that\ndecouples the latent space and constructs decoder branches tailored to\nindividual client groups. This method allows for customized learning that\naligns with the unique data distributions of each group. Additionally, we\nincorporate hierarchical VAEs and demonstrate the use of heterogeneous decoder\narchitectures within FissionVAE. We also explore strategies for setting the\nlatent prior distributions to enhance the decoupling process. To evaluate our\napproach, we assemble two composite datasets: the first combines MNIST and\nFashionMNIST; the second comprises RGB datasets of cartoon and human faces,\nwild animals, marine vessels, and remote sensing images. Our experiments\ndemonstrate that FissionVAE greatly improves generation quality on these\ndatasets compared to baseline federated VAE models.\n", "link": "http://arxiv.org/abs/2408.17090v2", "date": "2025-05-05", "relevancy": 2.2966, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5902}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5672}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FissionVAE%3A%20Federated%20Non-IID%20Image%20Generation%20with%20Latent%20Space%20and%0A%20%20Decoder%20Decomposition&body=Title%3A%20FissionVAE%3A%20Federated%20Non-IID%20Image%20Generation%20with%20Latent%20Space%20and%0A%20%20Decoder%20Decomposition%0AAuthor%3A%20Chen%20Hu%20and%20Hanchi%20Ren%20and%20Jingjing%20Deng%20and%20Xianghua%20Xie%20and%20Xiaoke%20Ma%0AAbstract%3A%20%20%20Federated%20learning%20is%20a%20machine%20learning%20paradigm%20that%20enables%20decentralized%0Aclients%20to%20collaboratively%20learn%20a%20shared%20model%20while%20keeping%20all%20the%20training%0Adata%20local.%20While%20considerable%20research%20has%20focused%20on%20federated%20image%0Ageneration%2C%20particularly%20Generative%20Adversarial%20Networks%2C%20Variational%0AAutoencoders%20have%20received%20less%20attention.%20In%20this%20paper%2C%20we%20address%20the%0Achallenges%20of%20non-IID%20%28independently%20and%20identically%20distributed%29%20data%0Aenvironments%20featuring%20multiple%20groups%20of%20images%20of%20different%20types.%20Non-IID%0Adata%20distributions%20can%20lead%20to%20difficulties%20in%20maintaining%20a%20consistent%20latent%0Aspace%20and%20can%20also%20result%20in%20local%20generators%20with%20disparate%20texture%20features%0Abeing%20blended%20during%20aggregation.%20We%20thereby%20introduce%20FissionVAE%20that%0Adecouples%20the%20latent%20space%20and%20constructs%20decoder%20branches%20tailored%20to%0Aindividual%20client%20groups.%20This%20method%20allows%20for%20customized%20learning%20that%0Aaligns%20with%20the%20unique%20data%20distributions%20of%20each%20group.%20Additionally%2C%20we%0Aincorporate%20hierarchical%20VAEs%20and%20demonstrate%20the%20use%20of%20heterogeneous%20decoder%0Aarchitectures%20within%20FissionVAE.%20We%20also%20explore%20strategies%20for%20setting%20the%0Alatent%20prior%20distributions%20to%20enhance%20the%20decoupling%20process.%20To%20evaluate%20our%0Aapproach%2C%20we%20assemble%20two%20composite%20datasets%3A%20the%20first%20combines%20MNIST%20and%0AFashionMNIST%3B%20the%20second%20comprises%20RGB%20datasets%20of%20cartoon%20and%20human%20faces%2C%0Awild%20animals%2C%20marine%20vessels%2C%20and%20remote%20sensing%20images.%20Our%20experiments%0Ademonstrate%20that%20FissionVAE%20greatly%20improves%20generation%20quality%20on%20these%0Adatasets%20compared%20to%20baseline%20federated%20VAE%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17090v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFissionVAE%253A%2520Federated%2520Non-IID%2520Image%2520Generation%2520with%2520Latent%2520Space%2520and%250A%2520%2520Decoder%2520Decomposition%26entry.906535625%3DChen%2520Hu%2520and%2520Hanchi%2520Ren%2520and%2520Jingjing%2520Deng%2520and%2520Xianghua%2520Xie%2520and%2520Xiaoke%2520Ma%26entry.1292438233%3D%2520%2520Federated%2520learning%2520is%2520a%2520machine%2520learning%2520paradigm%2520that%2520enables%2520decentralized%250Aclients%2520to%2520collaboratively%2520learn%2520a%2520shared%2520model%2520while%2520keeping%2520all%2520the%2520training%250Adata%2520local.%2520While%2520considerable%2520research%2520has%2520focused%2520on%2520federated%2520image%250Ageneration%252C%2520particularly%2520Generative%2520Adversarial%2520Networks%252C%2520Variational%250AAutoencoders%2520have%2520received%2520less%2520attention.%2520In%2520this%2520paper%252C%2520we%2520address%2520the%250Achallenges%2520of%2520non-IID%2520%2528independently%2520and%2520identically%2520distributed%2529%2520data%250Aenvironments%2520featuring%2520multiple%2520groups%2520of%2520images%2520of%2520different%2520types.%2520Non-IID%250Adata%2520distributions%2520can%2520lead%2520to%2520difficulties%2520in%2520maintaining%2520a%2520consistent%2520latent%250Aspace%2520and%2520can%2520also%2520result%2520in%2520local%2520generators%2520with%2520disparate%2520texture%2520features%250Abeing%2520blended%2520during%2520aggregation.%2520We%2520thereby%2520introduce%2520FissionVAE%2520that%250Adecouples%2520the%2520latent%2520space%2520and%2520constructs%2520decoder%2520branches%2520tailored%2520to%250Aindividual%2520client%2520groups.%2520This%2520method%2520allows%2520for%2520customized%2520learning%2520that%250Aaligns%2520with%2520the%2520unique%2520data%2520distributions%2520of%2520each%2520group.%2520Additionally%252C%2520we%250Aincorporate%2520hierarchical%2520VAEs%2520and%2520demonstrate%2520the%2520use%2520of%2520heterogeneous%2520decoder%250Aarchitectures%2520within%2520FissionVAE.%2520We%2520also%2520explore%2520strategies%2520for%2520setting%2520the%250Alatent%2520prior%2520distributions%2520to%2520enhance%2520the%2520decoupling%2520process.%2520To%2520evaluate%2520our%250Aapproach%252C%2520we%2520assemble%2520two%2520composite%2520datasets%253A%2520the%2520first%2520combines%2520MNIST%2520and%250AFashionMNIST%253B%2520the%2520second%2520comprises%2520RGB%2520datasets%2520of%2520cartoon%2520and%2520human%2520faces%252C%250Awild%2520animals%252C%2520marine%2520vessels%252C%2520and%2520remote%2520sensing%2520images.%2520Our%2520experiments%250Ademonstrate%2520that%2520FissionVAE%2520greatly%2520improves%2520generation%2520quality%2520on%2520these%250Adatasets%2520compared%2520to%2520baseline%2520federated%2520VAE%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17090v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FissionVAE%3A%20Federated%20Non-IID%20Image%20Generation%20with%20Latent%20Space%20and%0A%20%20Decoder%20Decomposition&entry.906535625=Chen%20Hu%20and%20Hanchi%20Ren%20and%20Jingjing%20Deng%20and%20Xianghua%20Xie%20and%20Xiaoke%20Ma&entry.1292438233=%20%20Federated%20learning%20is%20a%20machine%20learning%20paradigm%20that%20enables%20decentralized%0Aclients%20to%20collaboratively%20learn%20a%20shared%20model%20while%20keeping%20all%20the%20training%0Adata%20local.%20While%20considerable%20research%20has%20focused%20on%20federated%20image%0Ageneration%2C%20particularly%20Generative%20Adversarial%20Networks%2C%20Variational%0AAutoencoders%20have%20received%20less%20attention.%20In%20this%20paper%2C%20we%20address%20the%0Achallenges%20of%20non-IID%20%28independently%20and%20identically%20distributed%29%20data%0Aenvironments%20featuring%20multiple%20groups%20of%20images%20of%20different%20types.%20Non-IID%0Adata%20distributions%20can%20lead%20to%20difficulties%20in%20maintaining%20a%20consistent%20latent%0Aspace%20and%20can%20also%20result%20in%20local%20generators%20with%20disparate%20texture%20features%0Abeing%20blended%20during%20aggregation.%20We%20thereby%20introduce%20FissionVAE%20that%0Adecouples%20the%20latent%20space%20and%20constructs%20decoder%20branches%20tailored%20to%0Aindividual%20client%20groups.%20This%20method%20allows%20for%20customized%20learning%20that%0Aaligns%20with%20the%20unique%20data%20distributions%20of%20each%20group.%20Additionally%2C%20we%0Aincorporate%20hierarchical%20VAEs%20and%20demonstrate%20the%20use%20of%20heterogeneous%20decoder%0Aarchitectures%20within%20FissionVAE.%20We%20also%20explore%20strategies%20for%20setting%20the%0Alatent%20prior%20distributions%20to%20enhance%20the%20decoupling%20process.%20To%20evaluate%20our%0Aapproach%2C%20we%20assemble%20two%20composite%20datasets%3A%20the%20first%20combines%20MNIST%20and%0AFashionMNIST%3B%20the%20second%20comprises%20RGB%20datasets%20of%20cartoon%20and%20human%20faces%2C%0Awild%20animals%2C%20marine%20vessels%2C%20and%20remote%20sensing%20images.%20Our%20experiments%0Ademonstrate%20that%20FissionVAE%20greatly%20improves%20generation%20quality%20on%20these%0Adatasets%20compared%20to%20baseline%20federated%20VAE%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17090v2&entry.124074799=Read"},
{"title": "Towards Quantifying the Hessian Structure of Neural Networks", "author": "Zhaorui Dong and Yushun Zhang and Zhi-Quan Luo and Jianfeng Yao and Ruoyu Sun", "abstract": "  Empirical studies reported that the Hessian matrix of neural networks (NNs)\nexhibits a near-block-diagonal structure, yet its theoretical foundation\nremains unclear. In this work, we reveal two forces that shape the Hessian\nstructure: a ``static force'' rooted in the architecture design, and a\n``dynamic force'' arisen from training. We then provide a rigorous theoretical\nanalysis of ``static force'' at random initialization. We study linear models\nand 1-hidden-layer networks with the mean-square (MSE) loss and the\nCross-Entropy (CE) loss for classification tasks. By leveraging random matrix\ntheory, we compare the limit distributions of the diagonal and off-diagonal\nHessian blocks and find that the block-diagonal structure arises as $C\n\\rightarrow \\infty$, where $C$ denotes the number of classes. Our findings\nreveal that $C$ is a primary driver of the near-block-diagonal structure. These\nresults may shed new light on the Hessian structure of large language models\n(LLMs), which typically operate with a large $C$ exceeding $10^4$ or $10^5$.\n", "link": "http://arxiv.org/abs/2505.02809v1", "date": "2025-05-05", "relevancy": 2.2905, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5005}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4371}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Quantifying%20the%20Hessian%20Structure%20of%20Neural%20Networks&body=Title%3A%20Towards%20Quantifying%20the%20Hessian%20Structure%20of%20Neural%20Networks%0AAuthor%3A%20Zhaorui%20Dong%20and%20Yushun%20Zhang%20and%20Zhi-Quan%20Luo%20and%20Jianfeng%20Yao%20and%20Ruoyu%20Sun%0AAbstract%3A%20%20%20Empirical%20studies%20reported%20that%20the%20Hessian%20matrix%20of%20neural%20networks%20%28NNs%29%0Aexhibits%20a%20near-block-diagonal%20structure%2C%20yet%20its%20theoretical%20foundation%0Aremains%20unclear.%20In%20this%20work%2C%20we%20reveal%20two%20forces%20that%20shape%20the%20Hessian%0Astructure%3A%20a%20%60%60static%20force%27%27%20rooted%20in%20the%20architecture%20design%2C%20and%20a%0A%60%60dynamic%20force%27%27%20arisen%20from%20training.%20We%20then%20provide%20a%20rigorous%20theoretical%0Aanalysis%20of%20%60%60static%20force%27%27%20at%20random%20initialization.%20We%20study%20linear%20models%0Aand%201-hidden-layer%20networks%20with%20the%20mean-square%20%28MSE%29%20loss%20and%20the%0ACross-Entropy%20%28CE%29%20loss%20for%20classification%20tasks.%20By%20leveraging%20random%20matrix%0Atheory%2C%20we%20compare%20the%20limit%20distributions%20of%20the%20diagonal%20and%20off-diagonal%0AHessian%20blocks%20and%20find%20that%20the%20block-diagonal%20structure%20arises%20as%20%24C%0A%5Crightarrow%20%5Cinfty%24%2C%20where%20%24C%24%20denotes%20the%20number%20of%20classes.%20Our%20findings%0Areveal%20that%20%24C%24%20is%20a%20primary%20driver%20of%20the%20near-block-diagonal%20structure.%20These%0Aresults%20may%20shed%20new%20light%20on%20the%20Hessian%20structure%20of%20large%20language%20models%0A%28LLMs%29%2C%20which%20typically%20operate%20with%20a%20large%20%24C%24%20exceeding%20%2410%5E4%24%20or%20%2410%5E5%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02809v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Quantifying%2520the%2520Hessian%2520Structure%2520of%2520Neural%2520Networks%26entry.906535625%3DZhaorui%2520Dong%2520and%2520Yushun%2520Zhang%2520and%2520Zhi-Quan%2520Luo%2520and%2520Jianfeng%2520Yao%2520and%2520Ruoyu%2520Sun%26entry.1292438233%3D%2520%2520Empirical%2520studies%2520reported%2520that%2520the%2520Hessian%2520matrix%2520of%2520neural%2520networks%2520%2528NNs%2529%250Aexhibits%2520a%2520near-block-diagonal%2520structure%252C%2520yet%2520its%2520theoretical%2520foundation%250Aremains%2520unclear.%2520In%2520this%2520work%252C%2520we%2520reveal%2520two%2520forces%2520that%2520shape%2520the%2520Hessian%250Astructure%253A%2520a%2520%2560%2560static%2520force%2527%2527%2520rooted%2520in%2520the%2520architecture%2520design%252C%2520and%2520a%250A%2560%2560dynamic%2520force%2527%2527%2520arisen%2520from%2520training.%2520We%2520then%2520provide%2520a%2520rigorous%2520theoretical%250Aanalysis%2520of%2520%2560%2560static%2520force%2527%2527%2520at%2520random%2520initialization.%2520We%2520study%2520linear%2520models%250Aand%25201-hidden-layer%2520networks%2520with%2520the%2520mean-square%2520%2528MSE%2529%2520loss%2520and%2520the%250ACross-Entropy%2520%2528CE%2529%2520loss%2520for%2520classification%2520tasks.%2520By%2520leveraging%2520random%2520matrix%250Atheory%252C%2520we%2520compare%2520the%2520limit%2520distributions%2520of%2520the%2520diagonal%2520and%2520off-diagonal%250AHessian%2520blocks%2520and%2520find%2520that%2520the%2520block-diagonal%2520structure%2520arises%2520as%2520%2524C%250A%255Crightarrow%2520%255Cinfty%2524%252C%2520where%2520%2524C%2524%2520denotes%2520the%2520number%2520of%2520classes.%2520Our%2520findings%250Areveal%2520that%2520%2524C%2524%2520is%2520a%2520primary%2520driver%2520of%2520the%2520near-block-diagonal%2520structure.%2520These%250Aresults%2520may%2520shed%2520new%2520light%2520on%2520the%2520Hessian%2520structure%2520of%2520large%2520language%2520models%250A%2528LLMs%2529%252C%2520which%2520typically%2520operate%2520with%2520a%2520large%2520%2524C%2524%2520exceeding%2520%252410%255E4%2524%2520or%2520%252410%255E5%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02809v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Quantifying%20the%20Hessian%20Structure%20of%20Neural%20Networks&entry.906535625=Zhaorui%20Dong%20and%20Yushun%20Zhang%20and%20Zhi-Quan%20Luo%20and%20Jianfeng%20Yao%20and%20Ruoyu%20Sun&entry.1292438233=%20%20Empirical%20studies%20reported%20that%20the%20Hessian%20matrix%20of%20neural%20networks%20%28NNs%29%0Aexhibits%20a%20near-block-diagonal%20structure%2C%20yet%20its%20theoretical%20foundation%0Aremains%20unclear.%20In%20this%20work%2C%20we%20reveal%20two%20forces%20that%20shape%20the%20Hessian%0Astructure%3A%20a%20%60%60static%20force%27%27%20rooted%20in%20the%20architecture%20design%2C%20and%20a%0A%60%60dynamic%20force%27%27%20arisen%20from%20training.%20We%20then%20provide%20a%20rigorous%20theoretical%0Aanalysis%20of%20%60%60static%20force%27%27%20at%20random%20initialization.%20We%20study%20linear%20models%0Aand%201-hidden-layer%20networks%20with%20the%20mean-square%20%28MSE%29%20loss%20and%20the%0ACross-Entropy%20%28CE%29%20loss%20for%20classification%20tasks.%20By%20leveraging%20random%20matrix%0Atheory%2C%20we%20compare%20the%20limit%20distributions%20of%20the%20diagonal%20and%20off-diagonal%0AHessian%20blocks%20and%20find%20that%20the%20block-diagonal%20structure%20arises%20as%20%24C%0A%5Crightarrow%20%5Cinfty%24%2C%20where%20%24C%24%20denotes%20the%20number%20of%20classes.%20Our%20findings%0Areveal%20that%20%24C%24%20is%20a%20primary%20driver%20of%20the%20near-block-diagonal%20structure.%20These%0Aresults%20may%20shed%20new%20light%20on%20the%20Hessian%20structure%20of%20large%20language%20models%0A%28LLMs%29%2C%20which%20typically%20operate%20with%20a%20large%20%24C%24%20exceeding%20%2410%5E4%24%20or%20%2410%5E5%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02809v1&entry.124074799=Read"},
{"title": "LMME3DHF: Benchmarking and Evaluating Multimodal 3D Human Face\n  Generation with LMMs", "author": "Woo Yi Yang and Jiarui Wang and Sijing Wu and Huiyu Duan and Yuxin Zhu and Liu Yang and Kang Fu and Guangtao Zhai and Xiongkuo Min", "abstract": "  The rapid advancement in generative artificial intelligence have enabled the\ncreation of 3D human faces (HFs) for applications including media production,\nvirtual reality, security, healthcare, and game development, etc. However,\nassessing the quality and realism of these AI-generated 3D human faces remains\na significant challenge due to the subjective nature of human perception and\ninnate perceptual sensitivity to facial features. To this end, we conduct a\ncomprehensive study on the quality assessment of AI-generated 3D human faces.\nWe first introduce Gen3DHF, a large-scale benchmark comprising 2,000 videos of\nAI-Generated 3D Human Faces along with 4,000 Mean Opinion Scores (MOS)\ncollected across two dimensions, i.e., quality and authenticity, 2,000\ndistortion-aware saliency maps and distortion descriptions. Based on Gen3DHF,\nwe propose LMME3DHF, a Large Multimodal Model (LMM)-based metric for Evaluating\n3DHF capable of quality and authenticity score prediction, distortion-aware\nvisual question answering, and distortion-aware saliency prediction.\nExperimental results show that LMME3DHF achieves state-of-the-art performance,\nsurpassing existing methods in both accurately predicting quality scores for\nAI-generated 3D human faces and effectively identifying distortion-aware\nsalient regions and distortion types, while maintaining strong alignment with\nhuman perceptual judgments. Both the Gen3DHF database and the LMME3DHF will be\nreleased upon the publication.\n", "link": "http://arxiv.org/abs/2504.20466v2", "date": "2025-05-05", "relevancy": 2.2905, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.587}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5714}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LMME3DHF%3A%20Benchmarking%20and%20Evaluating%20Multimodal%203D%20Human%20Face%0A%20%20Generation%20with%20LMMs&body=Title%3A%20LMME3DHF%3A%20Benchmarking%20and%20Evaluating%20Multimodal%203D%20Human%20Face%0A%20%20Generation%20with%20LMMs%0AAuthor%3A%20Woo%20Yi%20Yang%20and%20Jiarui%20Wang%20and%20Sijing%20Wu%20and%20Huiyu%20Duan%20and%20Yuxin%20Zhu%20and%20Liu%20Yang%20and%20Kang%20Fu%20and%20Guangtao%20Zhai%20and%20Xiongkuo%20Min%0AAbstract%3A%20%20%20The%20rapid%20advancement%20in%20generative%20artificial%20intelligence%20have%20enabled%20the%0Acreation%20of%203D%20human%20faces%20%28HFs%29%20for%20applications%20including%20media%20production%2C%0Avirtual%20reality%2C%20security%2C%20healthcare%2C%20and%20game%20development%2C%20etc.%20However%2C%0Aassessing%20the%20quality%20and%20realism%20of%20these%20AI-generated%203D%20human%20faces%20remains%0Aa%20significant%20challenge%20due%20to%20the%20subjective%20nature%20of%20human%20perception%20and%0Ainnate%20perceptual%20sensitivity%20to%20facial%20features.%20To%20this%20end%2C%20we%20conduct%20a%0Acomprehensive%20study%20on%20the%20quality%20assessment%20of%20AI-generated%203D%20human%20faces.%0AWe%20first%20introduce%20Gen3DHF%2C%20a%20large-scale%20benchmark%20comprising%202%2C000%20videos%20of%0AAI-Generated%203D%20Human%20Faces%20along%20with%204%2C000%20Mean%20Opinion%20Scores%20%28MOS%29%0Acollected%20across%20two%20dimensions%2C%20i.e.%2C%20quality%20and%20authenticity%2C%202%2C000%0Adistortion-aware%20saliency%20maps%20and%20distortion%20descriptions.%20Based%20on%20Gen3DHF%2C%0Awe%20propose%20LMME3DHF%2C%20a%20Large%20Multimodal%20Model%20%28LMM%29-based%20metric%20for%20Evaluating%0A3DHF%20capable%20of%20quality%20and%20authenticity%20score%20prediction%2C%20distortion-aware%0Avisual%20question%20answering%2C%20and%20distortion-aware%20saliency%20prediction.%0AExperimental%20results%20show%20that%20LMME3DHF%20achieves%20state-of-the-art%20performance%2C%0Asurpassing%20existing%20methods%20in%20both%20accurately%20predicting%20quality%20scores%20for%0AAI-generated%203D%20human%20faces%20and%20effectively%20identifying%20distortion-aware%0Asalient%20regions%20and%20distortion%20types%2C%20while%20maintaining%20strong%20alignment%20with%0Ahuman%20perceptual%20judgments.%20Both%20the%20Gen3DHF%20database%20and%20the%20LMME3DHF%20will%20be%0Areleased%20upon%20the%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20466v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLMME3DHF%253A%2520Benchmarking%2520and%2520Evaluating%2520Multimodal%25203D%2520Human%2520Face%250A%2520%2520Generation%2520with%2520LMMs%26entry.906535625%3DWoo%2520Yi%2520Yang%2520and%2520Jiarui%2520Wang%2520and%2520Sijing%2520Wu%2520and%2520Huiyu%2520Duan%2520and%2520Yuxin%2520Zhu%2520and%2520Liu%2520Yang%2520and%2520Kang%2520Fu%2520and%2520Guangtao%2520Zhai%2520and%2520Xiongkuo%2520Min%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520in%2520generative%2520artificial%2520intelligence%2520have%2520enabled%2520the%250Acreation%2520of%25203D%2520human%2520faces%2520%2528HFs%2529%2520for%2520applications%2520including%2520media%2520production%252C%250Avirtual%2520reality%252C%2520security%252C%2520healthcare%252C%2520and%2520game%2520development%252C%2520etc.%2520However%252C%250Aassessing%2520the%2520quality%2520and%2520realism%2520of%2520these%2520AI-generated%25203D%2520human%2520faces%2520remains%250Aa%2520significant%2520challenge%2520due%2520to%2520the%2520subjective%2520nature%2520of%2520human%2520perception%2520and%250Ainnate%2520perceptual%2520sensitivity%2520to%2520facial%2520features.%2520To%2520this%2520end%252C%2520we%2520conduct%2520a%250Acomprehensive%2520study%2520on%2520the%2520quality%2520assessment%2520of%2520AI-generated%25203D%2520human%2520faces.%250AWe%2520first%2520introduce%2520Gen3DHF%252C%2520a%2520large-scale%2520benchmark%2520comprising%25202%252C000%2520videos%2520of%250AAI-Generated%25203D%2520Human%2520Faces%2520along%2520with%25204%252C000%2520Mean%2520Opinion%2520Scores%2520%2528MOS%2529%250Acollected%2520across%2520two%2520dimensions%252C%2520i.e.%252C%2520quality%2520and%2520authenticity%252C%25202%252C000%250Adistortion-aware%2520saliency%2520maps%2520and%2520distortion%2520descriptions.%2520Based%2520on%2520Gen3DHF%252C%250Awe%2520propose%2520LMME3DHF%252C%2520a%2520Large%2520Multimodal%2520Model%2520%2528LMM%2529-based%2520metric%2520for%2520Evaluating%250A3DHF%2520capable%2520of%2520quality%2520and%2520authenticity%2520score%2520prediction%252C%2520distortion-aware%250Avisual%2520question%2520answering%252C%2520and%2520distortion-aware%2520saliency%2520prediction.%250AExperimental%2520results%2520show%2520that%2520LMME3DHF%2520achieves%2520state-of-the-art%2520performance%252C%250Asurpassing%2520existing%2520methods%2520in%2520both%2520accurately%2520predicting%2520quality%2520scores%2520for%250AAI-generated%25203D%2520human%2520faces%2520and%2520effectively%2520identifying%2520distortion-aware%250Asalient%2520regions%2520and%2520distortion%2520types%252C%2520while%2520maintaining%2520strong%2520alignment%2520with%250Ahuman%2520perceptual%2520judgments.%2520Both%2520the%2520Gen3DHF%2520database%2520and%2520the%2520LMME3DHF%2520will%2520be%250Areleased%2520upon%2520the%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20466v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LMME3DHF%3A%20Benchmarking%20and%20Evaluating%20Multimodal%203D%20Human%20Face%0A%20%20Generation%20with%20LMMs&entry.906535625=Woo%20Yi%20Yang%20and%20Jiarui%20Wang%20and%20Sijing%20Wu%20and%20Huiyu%20Duan%20and%20Yuxin%20Zhu%20and%20Liu%20Yang%20and%20Kang%20Fu%20and%20Guangtao%20Zhai%20and%20Xiongkuo%20Min&entry.1292438233=%20%20The%20rapid%20advancement%20in%20generative%20artificial%20intelligence%20have%20enabled%20the%0Acreation%20of%203D%20human%20faces%20%28HFs%29%20for%20applications%20including%20media%20production%2C%0Avirtual%20reality%2C%20security%2C%20healthcare%2C%20and%20game%20development%2C%20etc.%20However%2C%0Aassessing%20the%20quality%20and%20realism%20of%20these%20AI-generated%203D%20human%20faces%20remains%0Aa%20significant%20challenge%20due%20to%20the%20subjective%20nature%20of%20human%20perception%20and%0Ainnate%20perceptual%20sensitivity%20to%20facial%20features.%20To%20this%20end%2C%20we%20conduct%20a%0Acomprehensive%20study%20on%20the%20quality%20assessment%20of%20AI-generated%203D%20human%20faces.%0AWe%20first%20introduce%20Gen3DHF%2C%20a%20large-scale%20benchmark%20comprising%202%2C000%20videos%20of%0AAI-Generated%203D%20Human%20Faces%20along%20with%204%2C000%20Mean%20Opinion%20Scores%20%28MOS%29%0Acollected%20across%20two%20dimensions%2C%20i.e.%2C%20quality%20and%20authenticity%2C%202%2C000%0Adistortion-aware%20saliency%20maps%20and%20distortion%20descriptions.%20Based%20on%20Gen3DHF%2C%0Awe%20propose%20LMME3DHF%2C%20a%20Large%20Multimodal%20Model%20%28LMM%29-based%20metric%20for%20Evaluating%0A3DHF%20capable%20of%20quality%20and%20authenticity%20score%20prediction%2C%20distortion-aware%0Avisual%20question%20answering%2C%20and%20distortion-aware%20saliency%20prediction.%0AExperimental%20results%20show%20that%20LMME3DHF%20achieves%20state-of-the-art%20performance%2C%0Asurpassing%20existing%20methods%20in%20both%20accurately%20predicting%20quality%20scores%20for%0AAI-generated%203D%20human%20faces%20and%20effectively%20identifying%20distortion-aware%0Asalient%20regions%20and%20distortion%20types%2C%20while%20maintaining%20strong%20alignment%20with%0Ahuman%20perceptual%20judgments.%20Both%20the%20Gen3DHF%20database%20and%20the%20LMME3DHF%20will%20be%0Areleased%20upon%20the%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20466v2&entry.124074799=Read"},
{"title": "LiDAR-Inertial SLAM-Based Navigation and Safety-Oriented AI-Driven\n  Control System for Skid-Steer Robots", "author": "Mehdi Heydari Shahna and Eemil Haaparanta and Pauli Mustalahti and Jouni Mattila", "abstract": "  Integrating artificial intelligence (AI) and stochastic technologies into the\nmobile robot navigation and control (MRNC) framework while adhering to rigorous\nsafety standards presents significant challenges. To address these challenges,\nthis paper proposes a comprehensively integrated MRNC framework for skid-steer\nwheeled mobile robots (SSWMRs), in which all components are actively engaged in\nreal-time execution. The framework comprises: 1) a LiDAR-inertial simultaneous\nlocalization and mapping (SLAM) algorithm for estimating the current pose of\nthe robot within the built map; 2) an effective path-following control system\nfor generating desired linear and angular velocity commands based on the\ncurrent pose and the desired pose; 3) inverse kinematics for transferring\nlinear and angular velocity commands into left and right side velocity\ncommands; and 4) a robust AI-driven (RAID) control system incorporating a\nradial basis function network (RBFN) with a new adaptive algorithm to enforce\nin-wheel actuation systems to track each side motion commands. To further meet\nsafety requirements, the proposed RAID control within the MRNC framework of the\nSSWMR constrains AI-generated tracking performance within predefined overshoot\nand steady-state error limits, while ensuring robustness and system stability\nby compensating for modeling errors, unknown RBF weights, and external forces.\nExperimental results verify the proposed MRNC framework performance for a 4,836\nkg SSWMR operating on soft terrain.\n", "link": "http://arxiv.org/abs/2505.02598v1", "date": "2025-05-05", "relevancy": 2.2882, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6114}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5796}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5297}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiDAR-Inertial%20SLAM-Based%20Navigation%20and%20Safety-Oriented%20AI-Driven%0A%20%20Control%20System%20for%20Skid-Steer%20Robots&body=Title%3A%20LiDAR-Inertial%20SLAM-Based%20Navigation%20and%20Safety-Oriented%20AI-Driven%0A%20%20Control%20System%20for%20Skid-Steer%20Robots%0AAuthor%3A%20Mehdi%20Heydari%20Shahna%20and%20Eemil%20Haaparanta%20and%20Pauli%20Mustalahti%20and%20Jouni%20Mattila%0AAbstract%3A%20%20%20Integrating%20artificial%20intelligence%20%28AI%29%20and%20stochastic%20technologies%20into%20the%0Amobile%20robot%20navigation%20and%20control%20%28MRNC%29%20framework%20while%20adhering%20to%20rigorous%0Asafety%20standards%20presents%20significant%20challenges.%20To%20address%20these%20challenges%2C%0Athis%20paper%20proposes%20a%20comprehensively%20integrated%20MRNC%20framework%20for%20skid-steer%0Awheeled%20mobile%20robots%20%28SSWMRs%29%2C%20in%20which%20all%20components%20are%20actively%20engaged%20in%0Areal-time%20execution.%20The%20framework%20comprises%3A%201%29%20a%20LiDAR-inertial%20simultaneous%0Alocalization%20and%20mapping%20%28SLAM%29%20algorithm%20for%20estimating%20the%20current%20pose%20of%0Athe%20robot%20within%20the%20built%20map%3B%202%29%20an%20effective%20path-following%20control%20system%0Afor%20generating%20desired%20linear%20and%20angular%20velocity%20commands%20based%20on%20the%0Acurrent%20pose%20and%20the%20desired%20pose%3B%203%29%20inverse%20kinematics%20for%20transferring%0Alinear%20and%20angular%20velocity%20commands%20into%20left%20and%20right%20side%20velocity%0Acommands%3B%20and%204%29%20a%20robust%20AI-driven%20%28RAID%29%20control%20system%20incorporating%20a%0Aradial%20basis%20function%20network%20%28RBFN%29%20with%20a%20new%20adaptive%20algorithm%20to%20enforce%0Ain-wheel%20actuation%20systems%20to%20track%20each%20side%20motion%20commands.%20To%20further%20meet%0Asafety%20requirements%2C%20the%20proposed%20RAID%20control%20within%20the%20MRNC%20framework%20of%20the%0ASSWMR%20constrains%20AI-generated%20tracking%20performance%20within%20predefined%20overshoot%0Aand%20steady-state%20error%20limits%2C%20while%20ensuring%20robustness%20and%20system%20stability%0Aby%20compensating%20for%20modeling%20errors%2C%20unknown%20RBF%20weights%2C%20and%20external%20forces.%0AExperimental%20results%20verify%20the%20proposed%20MRNC%20framework%20performance%20for%20a%204%2C836%0Akg%20SSWMR%20operating%20on%20soft%20terrain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02598v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiDAR-Inertial%2520SLAM-Based%2520Navigation%2520and%2520Safety-Oriented%2520AI-Driven%250A%2520%2520Control%2520System%2520for%2520Skid-Steer%2520Robots%26entry.906535625%3DMehdi%2520Heydari%2520Shahna%2520and%2520Eemil%2520Haaparanta%2520and%2520Pauli%2520Mustalahti%2520and%2520Jouni%2520Mattila%26entry.1292438233%3D%2520%2520Integrating%2520artificial%2520intelligence%2520%2528AI%2529%2520and%2520stochastic%2520technologies%2520into%2520the%250Amobile%2520robot%2520navigation%2520and%2520control%2520%2528MRNC%2529%2520framework%2520while%2520adhering%2520to%2520rigorous%250Asafety%2520standards%2520presents%2520significant%2520challenges.%2520To%2520address%2520these%2520challenges%252C%250Athis%2520paper%2520proposes%2520a%2520comprehensively%2520integrated%2520MRNC%2520framework%2520for%2520skid-steer%250Awheeled%2520mobile%2520robots%2520%2528SSWMRs%2529%252C%2520in%2520which%2520all%2520components%2520are%2520actively%2520engaged%2520in%250Areal-time%2520execution.%2520The%2520framework%2520comprises%253A%25201%2529%2520a%2520LiDAR-inertial%2520simultaneous%250Alocalization%2520and%2520mapping%2520%2528SLAM%2529%2520algorithm%2520for%2520estimating%2520the%2520current%2520pose%2520of%250Athe%2520robot%2520within%2520the%2520built%2520map%253B%25202%2529%2520an%2520effective%2520path-following%2520control%2520system%250Afor%2520generating%2520desired%2520linear%2520and%2520angular%2520velocity%2520commands%2520based%2520on%2520the%250Acurrent%2520pose%2520and%2520the%2520desired%2520pose%253B%25203%2529%2520inverse%2520kinematics%2520for%2520transferring%250Alinear%2520and%2520angular%2520velocity%2520commands%2520into%2520left%2520and%2520right%2520side%2520velocity%250Acommands%253B%2520and%25204%2529%2520a%2520robust%2520AI-driven%2520%2528RAID%2529%2520control%2520system%2520incorporating%2520a%250Aradial%2520basis%2520function%2520network%2520%2528RBFN%2529%2520with%2520a%2520new%2520adaptive%2520algorithm%2520to%2520enforce%250Ain-wheel%2520actuation%2520systems%2520to%2520track%2520each%2520side%2520motion%2520commands.%2520To%2520further%2520meet%250Asafety%2520requirements%252C%2520the%2520proposed%2520RAID%2520control%2520within%2520the%2520MRNC%2520framework%2520of%2520the%250ASSWMR%2520constrains%2520AI-generated%2520tracking%2520performance%2520within%2520predefined%2520overshoot%250Aand%2520steady-state%2520error%2520limits%252C%2520while%2520ensuring%2520robustness%2520and%2520system%2520stability%250Aby%2520compensating%2520for%2520modeling%2520errors%252C%2520unknown%2520RBF%2520weights%252C%2520and%2520external%2520forces.%250AExperimental%2520results%2520verify%2520the%2520proposed%2520MRNC%2520framework%2520performance%2520for%2520a%25204%252C836%250Akg%2520SSWMR%2520operating%2520on%2520soft%2520terrain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02598v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiDAR-Inertial%20SLAM-Based%20Navigation%20and%20Safety-Oriented%20AI-Driven%0A%20%20Control%20System%20for%20Skid-Steer%20Robots&entry.906535625=Mehdi%20Heydari%20Shahna%20and%20Eemil%20Haaparanta%20and%20Pauli%20Mustalahti%20and%20Jouni%20Mattila&entry.1292438233=%20%20Integrating%20artificial%20intelligence%20%28AI%29%20and%20stochastic%20technologies%20into%20the%0Amobile%20robot%20navigation%20and%20control%20%28MRNC%29%20framework%20while%20adhering%20to%20rigorous%0Asafety%20standards%20presents%20significant%20challenges.%20To%20address%20these%20challenges%2C%0Athis%20paper%20proposes%20a%20comprehensively%20integrated%20MRNC%20framework%20for%20skid-steer%0Awheeled%20mobile%20robots%20%28SSWMRs%29%2C%20in%20which%20all%20components%20are%20actively%20engaged%20in%0Areal-time%20execution.%20The%20framework%20comprises%3A%201%29%20a%20LiDAR-inertial%20simultaneous%0Alocalization%20and%20mapping%20%28SLAM%29%20algorithm%20for%20estimating%20the%20current%20pose%20of%0Athe%20robot%20within%20the%20built%20map%3B%202%29%20an%20effective%20path-following%20control%20system%0Afor%20generating%20desired%20linear%20and%20angular%20velocity%20commands%20based%20on%20the%0Acurrent%20pose%20and%20the%20desired%20pose%3B%203%29%20inverse%20kinematics%20for%20transferring%0Alinear%20and%20angular%20velocity%20commands%20into%20left%20and%20right%20side%20velocity%0Acommands%3B%20and%204%29%20a%20robust%20AI-driven%20%28RAID%29%20control%20system%20incorporating%20a%0Aradial%20basis%20function%20network%20%28RBFN%29%20with%20a%20new%20adaptive%20algorithm%20to%20enforce%0Ain-wheel%20actuation%20systems%20to%20track%20each%20side%20motion%20commands.%20To%20further%20meet%0Asafety%20requirements%2C%20the%20proposed%20RAID%20control%20within%20the%20MRNC%20framework%20of%20the%0ASSWMR%20constrains%20AI-generated%20tracking%20performance%20within%20predefined%20overshoot%0Aand%20steady-state%20error%20limits%2C%20while%20ensuring%20robustness%20and%20system%20stability%0Aby%20compensating%20for%20modeling%20errors%2C%20unknown%20RBF%20weights%2C%20and%20external%20forces.%0AExperimental%20results%20verify%20the%20proposed%20MRNC%20framework%20performance%20for%20a%204%2C836%0Akg%20SSWMR%20operating%20on%20soft%20terrain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02598v1&entry.124074799=Read"},
{"title": "Generative Trajectory Stitching through Diffusion Composition", "author": "Yunhao Luo and Utkarsh A. Mishra and Yilun Du and Danfei Xu", "abstract": "  Effective trajectory stitching for long-horizon planning is a significant\nchallenge in robotic decision-making. While diffusion models have shown promise\nin planning, they are limited to solving tasks similar to those seen in their\ntraining data. We propose CompDiffuser, a novel generative approach that can\nsolve new tasks by learning to compositionally stitch together shorter\ntrajectory chunks from previously seen tasks. Our key insight is modeling the\ntrajectory distribution by subdividing it into overlapping chunks and learning\ntheir conditional relationships through a single bidirectional diffusion model.\nThis allows information to propagate between segments during generation,\nensuring physically consistent connections. We conduct experiments on benchmark\ntasks of various difficulties, covering different environment sizes, agent\nstate dimension, trajectory types, training data quality, and show that\nCompDiffuser significantly outperforms existing methods.\n", "link": "http://arxiv.org/abs/2503.05153v2", "date": "2025-05-05", "relevancy": 2.2868, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6189}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5735}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Trajectory%20Stitching%20through%20Diffusion%20Composition&body=Title%3A%20Generative%20Trajectory%20Stitching%20through%20Diffusion%20Composition%0AAuthor%3A%20Yunhao%20Luo%20and%20Utkarsh%20A.%20Mishra%20and%20Yilun%20Du%20and%20Danfei%20Xu%0AAbstract%3A%20%20%20Effective%20trajectory%20stitching%20for%20long-horizon%20planning%20is%20a%20significant%0Achallenge%20in%20robotic%20decision-making.%20While%20diffusion%20models%20have%20shown%20promise%0Ain%20planning%2C%20they%20are%20limited%20to%20solving%20tasks%20similar%20to%20those%20seen%20in%20their%0Atraining%20data.%20We%20propose%20CompDiffuser%2C%20a%20novel%20generative%20approach%20that%20can%0Asolve%20new%20tasks%20by%20learning%20to%20compositionally%20stitch%20together%20shorter%0Atrajectory%20chunks%20from%20previously%20seen%20tasks.%20Our%20key%20insight%20is%20modeling%20the%0Atrajectory%20distribution%20by%20subdividing%20it%20into%20overlapping%20chunks%20and%20learning%0Atheir%20conditional%20relationships%20through%20a%20single%20bidirectional%20diffusion%20model.%0AThis%20allows%20information%20to%20propagate%20between%20segments%20during%20generation%2C%0Aensuring%20physically%20consistent%20connections.%20We%20conduct%20experiments%20on%20benchmark%0Atasks%20of%20various%20difficulties%2C%20covering%20different%20environment%20sizes%2C%20agent%0Astate%20dimension%2C%20trajectory%20types%2C%20training%20data%20quality%2C%20and%20show%20that%0ACompDiffuser%20significantly%20outperforms%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.05153v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Trajectory%2520Stitching%2520through%2520Diffusion%2520Composition%26entry.906535625%3DYunhao%2520Luo%2520and%2520Utkarsh%2520A.%2520Mishra%2520and%2520Yilun%2520Du%2520and%2520Danfei%2520Xu%26entry.1292438233%3D%2520%2520Effective%2520trajectory%2520stitching%2520for%2520long-horizon%2520planning%2520is%2520a%2520significant%250Achallenge%2520in%2520robotic%2520decision-making.%2520While%2520diffusion%2520models%2520have%2520shown%2520promise%250Ain%2520planning%252C%2520they%2520are%2520limited%2520to%2520solving%2520tasks%2520similar%2520to%2520those%2520seen%2520in%2520their%250Atraining%2520data.%2520We%2520propose%2520CompDiffuser%252C%2520a%2520novel%2520generative%2520approach%2520that%2520can%250Asolve%2520new%2520tasks%2520by%2520learning%2520to%2520compositionally%2520stitch%2520together%2520shorter%250Atrajectory%2520chunks%2520from%2520previously%2520seen%2520tasks.%2520Our%2520key%2520insight%2520is%2520modeling%2520the%250Atrajectory%2520distribution%2520by%2520subdividing%2520it%2520into%2520overlapping%2520chunks%2520and%2520learning%250Atheir%2520conditional%2520relationships%2520through%2520a%2520single%2520bidirectional%2520diffusion%2520model.%250AThis%2520allows%2520information%2520to%2520propagate%2520between%2520segments%2520during%2520generation%252C%250Aensuring%2520physically%2520consistent%2520connections.%2520We%2520conduct%2520experiments%2520on%2520benchmark%250Atasks%2520of%2520various%2520difficulties%252C%2520covering%2520different%2520environment%2520sizes%252C%2520agent%250Astate%2520dimension%252C%2520trajectory%2520types%252C%2520training%2520data%2520quality%252C%2520and%2520show%2520that%250ACompDiffuser%2520significantly%2520outperforms%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.05153v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Trajectory%20Stitching%20through%20Diffusion%20Composition&entry.906535625=Yunhao%20Luo%20and%20Utkarsh%20A.%20Mishra%20and%20Yilun%20Du%20and%20Danfei%20Xu&entry.1292438233=%20%20Effective%20trajectory%20stitching%20for%20long-horizon%20planning%20is%20a%20significant%0Achallenge%20in%20robotic%20decision-making.%20While%20diffusion%20models%20have%20shown%20promise%0Ain%20planning%2C%20they%20are%20limited%20to%20solving%20tasks%20similar%20to%20those%20seen%20in%20their%0Atraining%20data.%20We%20propose%20CompDiffuser%2C%20a%20novel%20generative%20approach%20that%20can%0Asolve%20new%20tasks%20by%20learning%20to%20compositionally%20stitch%20together%20shorter%0Atrajectory%20chunks%20from%20previously%20seen%20tasks.%20Our%20key%20insight%20is%20modeling%20the%0Atrajectory%20distribution%20by%20subdividing%20it%20into%20overlapping%20chunks%20and%20learning%0Atheir%20conditional%20relationships%20through%20a%20single%20bidirectional%20diffusion%20model.%0AThis%20allows%20information%20to%20propagate%20between%20segments%20during%20generation%2C%0Aensuring%20physically%20consistent%20connections.%20We%20conduct%20experiments%20on%20benchmark%0Atasks%20of%20various%20difficulties%2C%20covering%20different%20environment%20sizes%2C%20agent%0Astate%20dimension%2C%20trajectory%20types%2C%20training%20data%20quality%2C%20and%20show%20that%0ACompDiffuser%20significantly%20outperforms%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.05153v2&entry.124074799=Read"},
{"title": "Robust Duality Learning for Unsupervised Visible-Infrared Person\n  Re-Identfication", "author": "Yongxiang Li and Yuan Sun and Yang Qin and Dezhong Peng and Xi Peng and Peng Hu", "abstract": "  Unsupervised visible-infrared person re-identification (UVI-ReID) aims to\nretrieve pedestrian images across different modalities without costly\nannotations, but faces challenges due to the modality gap and lack of\nsupervision. Existing methods often adopt self-training with\nclustering-generated pseudo-labels but implicitly assume these labels are\nalways correct. In practice, however, this assumption fails due to inevitable\npseudo-label noise, which hinders model learning. To address this, we introduce\na new learning paradigm that explicitly considers Pseudo-Label Noise (PLN),\ncharacterized by three key challenges: noise overfitting, error accumulation,\nand noisy cluster correspondence. To this end, we propose a novel Robust\nDuality Learning framework (RoDE) for UVI-ReID to mitigate the effects of noisy\npseudo-labels. First, to combat noise overfitting, a Robust Adaptive Learning\nmechanism (RAL) is proposed to dynamically emphasize clean samples while\ndown-weighting noisy ones. Second, to alleviate error accumulation-where the\nmodel reinforces its own mistakes-RoDE employs dual distinct models that are\nalternately trained using pseudo-labels from each other, encouraging diversity\nand preventing collapse. However, this dual-model strategy introduces\nmisalignment between clusters across models and modalities, creating noisy\ncluster correspondence. To resolve this, we introduce Cluster Consistency\nMatching (CCM), which aligns clusters across models and modalities by measuring\ncross-cluster similarity. Extensive experiments on three benchmarks demonstrate\nthe effectiveness of RoDE.\n", "link": "http://arxiv.org/abs/2505.02549v1", "date": "2025-05-05", "relevancy": 2.2796, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5826}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5688}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Duality%20Learning%20for%20Unsupervised%20Visible-Infrared%20Person%0A%20%20Re-Identfication&body=Title%3A%20Robust%20Duality%20Learning%20for%20Unsupervised%20Visible-Infrared%20Person%0A%20%20Re-Identfication%0AAuthor%3A%20Yongxiang%20Li%20and%20Yuan%20Sun%20and%20Yang%20Qin%20and%20Dezhong%20Peng%20and%20Xi%20Peng%20and%20Peng%20Hu%0AAbstract%3A%20%20%20Unsupervised%20visible-infrared%20person%20re-identification%20%28UVI-ReID%29%20aims%20to%0Aretrieve%20pedestrian%20images%20across%20different%20modalities%20without%20costly%0Aannotations%2C%20but%20faces%20challenges%20due%20to%20the%20modality%20gap%20and%20lack%20of%0Asupervision.%20Existing%20methods%20often%20adopt%20self-training%20with%0Aclustering-generated%20pseudo-labels%20but%20implicitly%20assume%20these%20labels%20are%0Aalways%20correct.%20In%20practice%2C%20however%2C%20this%20assumption%20fails%20due%20to%20inevitable%0Apseudo-label%20noise%2C%20which%20hinders%20model%20learning.%20To%20address%20this%2C%20we%20introduce%0Aa%20new%20learning%20paradigm%20that%20explicitly%20considers%20Pseudo-Label%20Noise%20%28PLN%29%2C%0Acharacterized%20by%20three%20key%20challenges%3A%20noise%20overfitting%2C%20error%20accumulation%2C%0Aand%20noisy%20cluster%20correspondence.%20To%20this%20end%2C%20we%20propose%20a%20novel%20Robust%0ADuality%20Learning%20framework%20%28RoDE%29%20for%20UVI-ReID%20to%20mitigate%20the%20effects%20of%20noisy%0Apseudo-labels.%20First%2C%20to%20combat%20noise%20overfitting%2C%20a%20Robust%20Adaptive%20Learning%0Amechanism%20%28RAL%29%20is%20proposed%20to%20dynamically%20emphasize%20clean%20samples%20while%0Adown-weighting%20noisy%20ones.%20Second%2C%20to%20alleviate%20error%20accumulation-where%20the%0Amodel%20reinforces%20its%20own%20mistakes-RoDE%20employs%20dual%20distinct%20models%20that%20are%0Aalternately%20trained%20using%20pseudo-labels%20from%20each%20other%2C%20encouraging%20diversity%0Aand%20preventing%20collapse.%20However%2C%20this%20dual-model%20strategy%20introduces%0Amisalignment%20between%20clusters%20across%20models%20and%20modalities%2C%20creating%20noisy%0Acluster%20correspondence.%20To%20resolve%20this%2C%20we%20introduce%20Cluster%20Consistency%0AMatching%20%28CCM%29%2C%20which%20aligns%20clusters%20across%20models%20and%20modalities%20by%20measuring%0Across-cluster%20similarity.%20Extensive%20experiments%20on%20three%20benchmarks%20demonstrate%0Athe%20effectiveness%20of%20RoDE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02549v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Duality%2520Learning%2520for%2520Unsupervised%2520Visible-Infrared%2520Person%250A%2520%2520Re-Identfication%26entry.906535625%3DYongxiang%2520Li%2520and%2520Yuan%2520Sun%2520and%2520Yang%2520Qin%2520and%2520Dezhong%2520Peng%2520and%2520Xi%2520Peng%2520and%2520Peng%2520Hu%26entry.1292438233%3D%2520%2520Unsupervised%2520visible-infrared%2520person%2520re-identification%2520%2528UVI-ReID%2529%2520aims%2520to%250Aretrieve%2520pedestrian%2520images%2520across%2520different%2520modalities%2520without%2520costly%250Aannotations%252C%2520but%2520faces%2520challenges%2520due%2520to%2520the%2520modality%2520gap%2520and%2520lack%2520of%250Asupervision.%2520Existing%2520methods%2520often%2520adopt%2520self-training%2520with%250Aclustering-generated%2520pseudo-labels%2520but%2520implicitly%2520assume%2520these%2520labels%2520are%250Aalways%2520correct.%2520In%2520practice%252C%2520however%252C%2520this%2520assumption%2520fails%2520due%2520to%2520inevitable%250Apseudo-label%2520noise%252C%2520which%2520hinders%2520model%2520learning.%2520To%2520address%2520this%252C%2520we%2520introduce%250Aa%2520new%2520learning%2520paradigm%2520that%2520explicitly%2520considers%2520Pseudo-Label%2520Noise%2520%2528PLN%2529%252C%250Acharacterized%2520by%2520three%2520key%2520challenges%253A%2520noise%2520overfitting%252C%2520error%2520accumulation%252C%250Aand%2520noisy%2520cluster%2520correspondence.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520novel%2520Robust%250ADuality%2520Learning%2520framework%2520%2528RoDE%2529%2520for%2520UVI-ReID%2520to%2520mitigate%2520the%2520effects%2520of%2520noisy%250Apseudo-labels.%2520First%252C%2520to%2520combat%2520noise%2520overfitting%252C%2520a%2520Robust%2520Adaptive%2520Learning%250Amechanism%2520%2528RAL%2529%2520is%2520proposed%2520to%2520dynamically%2520emphasize%2520clean%2520samples%2520while%250Adown-weighting%2520noisy%2520ones.%2520Second%252C%2520to%2520alleviate%2520error%2520accumulation-where%2520the%250Amodel%2520reinforces%2520its%2520own%2520mistakes-RoDE%2520employs%2520dual%2520distinct%2520models%2520that%2520are%250Aalternately%2520trained%2520using%2520pseudo-labels%2520from%2520each%2520other%252C%2520encouraging%2520diversity%250Aand%2520preventing%2520collapse.%2520However%252C%2520this%2520dual-model%2520strategy%2520introduces%250Amisalignment%2520between%2520clusters%2520across%2520models%2520and%2520modalities%252C%2520creating%2520noisy%250Acluster%2520correspondence.%2520To%2520resolve%2520this%252C%2520we%2520introduce%2520Cluster%2520Consistency%250AMatching%2520%2528CCM%2529%252C%2520which%2520aligns%2520clusters%2520across%2520models%2520and%2520modalities%2520by%2520measuring%250Across-cluster%2520similarity.%2520Extensive%2520experiments%2520on%2520three%2520benchmarks%2520demonstrate%250Athe%2520effectiveness%2520of%2520RoDE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02549v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Duality%20Learning%20for%20Unsupervised%20Visible-Infrared%20Person%0A%20%20Re-Identfication&entry.906535625=Yongxiang%20Li%20and%20Yuan%20Sun%20and%20Yang%20Qin%20and%20Dezhong%20Peng%20and%20Xi%20Peng%20and%20Peng%20Hu&entry.1292438233=%20%20Unsupervised%20visible-infrared%20person%20re-identification%20%28UVI-ReID%29%20aims%20to%0Aretrieve%20pedestrian%20images%20across%20different%20modalities%20without%20costly%0Aannotations%2C%20but%20faces%20challenges%20due%20to%20the%20modality%20gap%20and%20lack%20of%0Asupervision.%20Existing%20methods%20often%20adopt%20self-training%20with%0Aclustering-generated%20pseudo-labels%20but%20implicitly%20assume%20these%20labels%20are%0Aalways%20correct.%20In%20practice%2C%20however%2C%20this%20assumption%20fails%20due%20to%20inevitable%0Apseudo-label%20noise%2C%20which%20hinders%20model%20learning.%20To%20address%20this%2C%20we%20introduce%0Aa%20new%20learning%20paradigm%20that%20explicitly%20considers%20Pseudo-Label%20Noise%20%28PLN%29%2C%0Acharacterized%20by%20three%20key%20challenges%3A%20noise%20overfitting%2C%20error%20accumulation%2C%0Aand%20noisy%20cluster%20correspondence.%20To%20this%20end%2C%20we%20propose%20a%20novel%20Robust%0ADuality%20Learning%20framework%20%28RoDE%29%20for%20UVI-ReID%20to%20mitigate%20the%20effects%20of%20noisy%0Apseudo-labels.%20First%2C%20to%20combat%20noise%20overfitting%2C%20a%20Robust%20Adaptive%20Learning%0Amechanism%20%28RAL%29%20is%20proposed%20to%20dynamically%20emphasize%20clean%20samples%20while%0Adown-weighting%20noisy%20ones.%20Second%2C%20to%20alleviate%20error%20accumulation-where%20the%0Amodel%20reinforces%20its%20own%20mistakes-RoDE%20employs%20dual%20distinct%20models%20that%20are%0Aalternately%20trained%20using%20pseudo-labels%20from%20each%20other%2C%20encouraging%20diversity%0Aand%20preventing%20collapse.%20However%2C%20this%20dual-model%20strategy%20introduces%0Amisalignment%20between%20clusters%20across%20models%20and%20modalities%2C%20creating%20noisy%0Acluster%20correspondence.%20To%20resolve%20this%2C%20we%20introduce%20Cluster%20Consistency%0AMatching%20%28CCM%29%2C%20which%20aligns%20clusters%20across%20models%20and%20modalities%20by%20measuring%0Across-cluster%20similarity.%20Extensive%20experiments%20on%20three%20benchmarks%20demonstrate%0Athe%20effectiveness%20of%20RoDE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02549v1&entry.124074799=Read"},
{"title": "DPNet: Dynamic Pooling Network for Tiny Object Detection", "author": "Luqi Gong and Haotian Chen and Yikun Chen and Tianliang Yao and Chao Li and Shuai Zhao and Guangjie Han", "abstract": "  In unmanned aerial systems, especially in complex environments, accurately\ndetecting tiny objects is crucial. Resizing images is a common strategy to\nimprove detection accuracy, particularly for small objects. However, simply\nenlarging images significantly increases computational costs and the number of\nnegative samples, severely degrading detection performance and limiting its\napplicability. This paper proposes a Dynamic Pooling Network (DPNet) for tiny\nobject detection to mitigate these issues. DPNet employs a flexible\ndown-sampling strategy by introducing a factor (df) to relax the fixed\ndownsampling process of the feature map to an adjustable one. Furthermore, we\ndesign a lightweight predictor to predict df for each input image, which is\nused to decrease the resolution of feature maps in the backbone. Thus, we\nachieve input-aware downsampling. We also design an Adaptive Normalization\nModule (ANM) to make a unified detector compatible with different dfs. A\nguidance loss supervises the predictor's training. DPNet dynamically allocates\ncomputing resources to trade off between detection accuracy and efficiency.\nExperiments on the TinyCOCO and TinyPerson datasets show that DPNet can save\nover 35% and 25% GFLOPs, respectively, while maintaining comparable detection\nperformance. The code will be made publicly available.\n", "link": "http://arxiv.org/abs/2505.02797v1", "date": "2025-05-05", "relevancy": 2.2623, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5739}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5683}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DPNet%3A%20Dynamic%20Pooling%20Network%20for%20Tiny%20Object%20Detection&body=Title%3A%20DPNet%3A%20Dynamic%20Pooling%20Network%20for%20Tiny%20Object%20Detection%0AAuthor%3A%20Luqi%20Gong%20and%20Haotian%20Chen%20and%20Yikun%20Chen%20and%20Tianliang%20Yao%20and%20Chao%20Li%20and%20Shuai%20Zhao%20and%20Guangjie%20Han%0AAbstract%3A%20%20%20In%20unmanned%20aerial%20systems%2C%20especially%20in%20complex%20environments%2C%20accurately%0Adetecting%20tiny%20objects%20is%20crucial.%20Resizing%20images%20is%20a%20common%20strategy%20to%0Aimprove%20detection%20accuracy%2C%20particularly%20for%20small%20objects.%20However%2C%20simply%0Aenlarging%20images%20significantly%20increases%20computational%20costs%20and%20the%20number%20of%0Anegative%20samples%2C%20severely%20degrading%20detection%20performance%20and%20limiting%20its%0Aapplicability.%20This%20paper%20proposes%20a%20Dynamic%20Pooling%20Network%20%28DPNet%29%20for%20tiny%0Aobject%20detection%20to%20mitigate%20these%20issues.%20DPNet%20employs%20a%20flexible%0Adown-sampling%20strategy%20by%20introducing%20a%20factor%20%28df%29%20to%20relax%20the%20fixed%0Adownsampling%20process%20of%20the%20feature%20map%20to%20an%20adjustable%20one.%20Furthermore%2C%20we%0Adesign%20a%20lightweight%20predictor%20to%20predict%20df%20for%20each%20input%20image%2C%20which%20is%0Aused%20to%20decrease%20the%20resolution%20of%20feature%20maps%20in%20the%20backbone.%20Thus%2C%20we%0Aachieve%20input-aware%20downsampling.%20We%20also%20design%20an%20Adaptive%20Normalization%0AModule%20%28ANM%29%20to%20make%20a%20unified%20detector%20compatible%20with%20different%20dfs.%20A%0Aguidance%20loss%20supervises%20the%20predictor%27s%20training.%20DPNet%20dynamically%20allocates%0Acomputing%20resources%20to%20trade%20off%20between%20detection%20accuracy%20and%20efficiency.%0AExperiments%20on%20the%20TinyCOCO%20and%20TinyPerson%20datasets%20show%20that%20DPNet%20can%20save%0Aover%2035%25%20and%2025%25%20GFLOPs%2C%20respectively%2C%20while%20maintaining%20comparable%20detection%0Aperformance.%20The%20code%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02797v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDPNet%253A%2520Dynamic%2520Pooling%2520Network%2520for%2520Tiny%2520Object%2520Detection%26entry.906535625%3DLuqi%2520Gong%2520and%2520Haotian%2520Chen%2520and%2520Yikun%2520Chen%2520and%2520Tianliang%2520Yao%2520and%2520Chao%2520Li%2520and%2520Shuai%2520Zhao%2520and%2520Guangjie%2520Han%26entry.1292438233%3D%2520%2520In%2520unmanned%2520aerial%2520systems%252C%2520especially%2520in%2520complex%2520environments%252C%2520accurately%250Adetecting%2520tiny%2520objects%2520is%2520crucial.%2520Resizing%2520images%2520is%2520a%2520common%2520strategy%2520to%250Aimprove%2520detection%2520accuracy%252C%2520particularly%2520for%2520small%2520objects.%2520However%252C%2520simply%250Aenlarging%2520images%2520significantly%2520increases%2520computational%2520costs%2520and%2520the%2520number%2520of%250Anegative%2520samples%252C%2520severely%2520degrading%2520detection%2520performance%2520and%2520limiting%2520its%250Aapplicability.%2520This%2520paper%2520proposes%2520a%2520Dynamic%2520Pooling%2520Network%2520%2528DPNet%2529%2520for%2520tiny%250Aobject%2520detection%2520to%2520mitigate%2520these%2520issues.%2520DPNet%2520employs%2520a%2520flexible%250Adown-sampling%2520strategy%2520by%2520introducing%2520a%2520factor%2520%2528df%2529%2520to%2520relax%2520the%2520fixed%250Adownsampling%2520process%2520of%2520the%2520feature%2520map%2520to%2520an%2520adjustable%2520one.%2520Furthermore%252C%2520we%250Adesign%2520a%2520lightweight%2520predictor%2520to%2520predict%2520df%2520for%2520each%2520input%2520image%252C%2520which%2520is%250Aused%2520to%2520decrease%2520the%2520resolution%2520of%2520feature%2520maps%2520in%2520the%2520backbone.%2520Thus%252C%2520we%250Aachieve%2520input-aware%2520downsampling.%2520We%2520also%2520design%2520an%2520Adaptive%2520Normalization%250AModule%2520%2528ANM%2529%2520to%2520make%2520a%2520unified%2520detector%2520compatible%2520with%2520different%2520dfs.%2520A%250Aguidance%2520loss%2520supervises%2520the%2520predictor%2527s%2520training.%2520DPNet%2520dynamically%2520allocates%250Acomputing%2520resources%2520to%2520trade%2520off%2520between%2520detection%2520accuracy%2520and%2520efficiency.%250AExperiments%2520on%2520the%2520TinyCOCO%2520and%2520TinyPerson%2520datasets%2520show%2520that%2520DPNet%2520can%2520save%250Aover%252035%2525%2520and%252025%2525%2520GFLOPs%252C%2520respectively%252C%2520while%2520maintaining%2520comparable%2520detection%250Aperformance.%2520The%2520code%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02797v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DPNet%3A%20Dynamic%20Pooling%20Network%20for%20Tiny%20Object%20Detection&entry.906535625=Luqi%20Gong%20and%20Haotian%20Chen%20and%20Yikun%20Chen%20and%20Tianliang%20Yao%20and%20Chao%20Li%20and%20Shuai%20Zhao%20and%20Guangjie%20Han&entry.1292438233=%20%20In%20unmanned%20aerial%20systems%2C%20especially%20in%20complex%20environments%2C%20accurately%0Adetecting%20tiny%20objects%20is%20crucial.%20Resizing%20images%20is%20a%20common%20strategy%20to%0Aimprove%20detection%20accuracy%2C%20particularly%20for%20small%20objects.%20However%2C%20simply%0Aenlarging%20images%20significantly%20increases%20computational%20costs%20and%20the%20number%20of%0Anegative%20samples%2C%20severely%20degrading%20detection%20performance%20and%20limiting%20its%0Aapplicability.%20This%20paper%20proposes%20a%20Dynamic%20Pooling%20Network%20%28DPNet%29%20for%20tiny%0Aobject%20detection%20to%20mitigate%20these%20issues.%20DPNet%20employs%20a%20flexible%0Adown-sampling%20strategy%20by%20introducing%20a%20factor%20%28df%29%20to%20relax%20the%20fixed%0Adownsampling%20process%20of%20the%20feature%20map%20to%20an%20adjustable%20one.%20Furthermore%2C%20we%0Adesign%20a%20lightweight%20predictor%20to%20predict%20df%20for%20each%20input%20image%2C%20which%20is%0Aused%20to%20decrease%20the%20resolution%20of%20feature%20maps%20in%20the%20backbone.%20Thus%2C%20we%0Aachieve%20input-aware%20downsampling.%20We%20also%20design%20an%20Adaptive%20Normalization%0AModule%20%28ANM%29%20to%20make%20a%20unified%20detector%20compatible%20with%20different%20dfs.%20A%0Aguidance%20loss%20supervises%20the%20predictor%27s%20training.%20DPNet%20dynamically%20allocates%0Acomputing%20resources%20to%20trade%20off%20between%20detection%20accuracy%20and%20efficiency.%0AExperiments%20on%20the%20TinyCOCO%20and%20TinyPerson%20datasets%20show%20that%20DPNet%20can%20save%0Aover%2035%25%20and%2025%25%20GFLOPs%2C%20respectively%2C%20while%20maintaining%20comparable%20detection%0Aperformance.%20The%20code%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02797v1&entry.124074799=Read"},
{"title": "TWIST: Teleoperated Whole-Body Imitation System", "author": "Yanjie Ze and Zixuan Chen and Jo\u00e3o Pedro Ara\u00fajo and Zi-ang Cao and Xue Bin Peng and Jiajun Wu and C. Karen Liu", "abstract": "  Teleoperating humanoid robots in a whole-body manner marks a fundamental step\ntoward developing general-purpose robotic intelligence, with human motion\nproviding an ideal interface for controlling all degrees of freedom. Yet, most\ncurrent humanoid teleoperation systems fall short of enabling coordinated\nwhole-body behavior, typically limiting themselves to isolated locomotion or\nmanipulation tasks. We present the Teleoperated Whole-Body Imitation System\n(TWIST), a system for humanoid teleoperation through whole-body motion\nimitation. We first generate reference motion clips by retargeting human motion\ncapture data to the humanoid robot. We then develop a robust, adaptive, and\nresponsive whole-body controller using a combination of reinforcement learning\nand behavior cloning (RL+BC). Through systematic analysis, we demonstrate how\nincorporating privileged future motion frames and real-world motion capture\n(MoCap) data improves tracking accuracy. TWIST enables real-world humanoid\nrobots to achieve unprecedented, versatile, and coordinated whole-body motor\nskills--spanning whole-body manipulation, legged manipulation, locomotion, and\nexpressive movement--using a single unified neural network controller. Our\nproject website: https://humanoid-teleop.github.io\n", "link": "http://arxiv.org/abs/2505.02833v1", "date": "2025-05-05", "relevancy": 2.2533, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5864}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.551}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TWIST%3A%20Teleoperated%20Whole-Body%20Imitation%20System&body=Title%3A%20TWIST%3A%20Teleoperated%20Whole-Body%20Imitation%20System%0AAuthor%3A%20Yanjie%20Ze%20and%20Zixuan%20Chen%20and%20Jo%C3%A3o%20Pedro%20Ara%C3%BAjo%20and%20Zi-ang%20Cao%20and%20Xue%20Bin%20Peng%20and%20Jiajun%20Wu%20and%20C.%20Karen%20Liu%0AAbstract%3A%20%20%20Teleoperating%20humanoid%20robots%20in%20a%20whole-body%20manner%20marks%20a%20fundamental%20step%0Atoward%20developing%20general-purpose%20robotic%20intelligence%2C%20with%20human%20motion%0Aproviding%20an%20ideal%20interface%20for%20controlling%20all%20degrees%20of%20freedom.%20Yet%2C%20most%0Acurrent%20humanoid%20teleoperation%20systems%20fall%20short%20of%20enabling%20coordinated%0Awhole-body%20behavior%2C%20typically%20limiting%20themselves%20to%20isolated%20locomotion%20or%0Amanipulation%20tasks.%20We%20present%20the%20Teleoperated%20Whole-Body%20Imitation%20System%0A%28TWIST%29%2C%20a%20system%20for%20humanoid%20teleoperation%20through%20whole-body%20motion%0Aimitation.%20We%20first%20generate%20reference%20motion%20clips%20by%20retargeting%20human%20motion%0Acapture%20data%20to%20the%20humanoid%20robot.%20We%20then%20develop%20a%20robust%2C%20adaptive%2C%20and%0Aresponsive%20whole-body%20controller%20using%20a%20combination%20of%20reinforcement%20learning%0Aand%20behavior%20cloning%20%28RL%2BBC%29.%20Through%20systematic%20analysis%2C%20we%20demonstrate%20how%0Aincorporating%20privileged%20future%20motion%20frames%20and%20real-world%20motion%20capture%0A%28MoCap%29%20data%20improves%20tracking%20accuracy.%20TWIST%20enables%20real-world%20humanoid%0Arobots%20to%20achieve%20unprecedented%2C%20versatile%2C%20and%20coordinated%20whole-body%20motor%0Askills--spanning%20whole-body%20manipulation%2C%20legged%20manipulation%2C%20locomotion%2C%20and%0Aexpressive%20movement--using%20a%20single%20unified%20neural%20network%20controller.%20Our%0Aproject%20website%3A%20https%3A//humanoid-teleop.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02833v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTWIST%253A%2520Teleoperated%2520Whole-Body%2520Imitation%2520System%26entry.906535625%3DYanjie%2520Ze%2520and%2520Zixuan%2520Chen%2520and%2520Jo%25C3%25A3o%2520Pedro%2520Ara%25C3%25BAjo%2520and%2520Zi-ang%2520Cao%2520and%2520Xue%2520Bin%2520Peng%2520and%2520Jiajun%2520Wu%2520and%2520C.%2520Karen%2520Liu%26entry.1292438233%3D%2520%2520Teleoperating%2520humanoid%2520robots%2520in%2520a%2520whole-body%2520manner%2520marks%2520a%2520fundamental%2520step%250Atoward%2520developing%2520general-purpose%2520robotic%2520intelligence%252C%2520with%2520human%2520motion%250Aproviding%2520an%2520ideal%2520interface%2520for%2520controlling%2520all%2520degrees%2520of%2520freedom.%2520Yet%252C%2520most%250Acurrent%2520humanoid%2520teleoperation%2520systems%2520fall%2520short%2520of%2520enabling%2520coordinated%250Awhole-body%2520behavior%252C%2520typically%2520limiting%2520themselves%2520to%2520isolated%2520locomotion%2520or%250Amanipulation%2520tasks.%2520We%2520present%2520the%2520Teleoperated%2520Whole-Body%2520Imitation%2520System%250A%2528TWIST%2529%252C%2520a%2520system%2520for%2520humanoid%2520teleoperation%2520through%2520whole-body%2520motion%250Aimitation.%2520We%2520first%2520generate%2520reference%2520motion%2520clips%2520by%2520retargeting%2520human%2520motion%250Acapture%2520data%2520to%2520the%2520humanoid%2520robot.%2520We%2520then%2520develop%2520a%2520robust%252C%2520adaptive%252C%2520and%250Aresponsive%2520whole-body%2520controller%2520using%2520a%2520combination%2520of%2520reinforcement%2520learning%250Aand%2520behavior%2520cloning%2520%2528RL%252BBC%2529.%2520Through%2520systematic%2520analysis%252C%2520we%2520demonstrate%2520how%250Aincorporating%2520privileged%2520future%2520motion%2520frames%2520and%2520real-world%2520motion%2520capture%250A%2528MoCap%2529%2520data%2520improves%2520tracking%2520accuracy.%2520TWIST%2520enables%2520real-world%2520humanoid%250Arobots%2520to%2520achieve%2520unprecedented%252C%2520versatile%252C%2520and%2520coordinated%2520whole-body%2520motor%250Askills--spanning%2520whole-body%2520manipulation%252C%2520legged%2520manipulation%252C%2520locomotion%252C%2520and%250Aexpressive%2520movement--using%2520a%2520single%2520unified%2520neural%2520network%2520controller.%2520Our%250Aproject%2520website%253A%2520https%253A//humanoid-teleop.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02833v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TWIST%3A%20Teleoperated%20Whole-Body%20Imitation%20System&entry.906535625=Yanjie%20Ze%20and%20Zixuan%20Chen%20and%20Jo%C3%A3o%20Pedro%20Ara%C3%BAjo%20and%20Zi-ang%20Cao%20and%20Xue%20Bin%20Peng%20and%20Jiajun%20Wu%20and%20C.%20Karen%20Liu&entry.1292438233=%20%20Teleoperating%20humanoid%20robots%20in%20a%20whole-body%20manner%20marks%20a%20fundamental%20step%0Atoward%20developing%20general-purpose%20robotic%20intelligence%2C%20with%20human%20motion%0Aproviding%20an%20ideal%20interface%20for%20controlling%20all%20degrees%20of%20freedom.%20Yet%2C%20most%0Acurrent%20humanoid%20teleoperation%20systems%20fall%20short%20of%20enabling%20coordinated%0Awhole-body%20behavior%2C%20typically%20limiting%20themselves%20to%20isolated%20locomotion%20or%0Amanipulation%20tasks.%20We%20present%20the%20Teleoperated%20Whole-Body%20Imitation%20System%0A%28TWIST%29%2C%20a%20system%20for%20humanoid%20teleoperation%20through%20whole-body%20motion%0Aimitation.%20We%20first%20generate%20reference%20motion%20clips%20by%20retargeting%20human%20motion%0Acapture%20data%20to%20the%20humanoid%20robot.%20We%20then%20develop%20a%20robust%2C%20adaptive%2C%20and%0Aresponsive%20whole-body%20controller%20using%20a%20combination%20of%20reinforcement%20learning%0Aand%20behavior%20cloning%20%28RL%2BBC%29.%20Through%20systematic%20analysis%2C%20we%20demonstrate%20how%0Aincorporating%20privileged%20future%20motion%20frames%20and%20real-world%20motion%20capture%0A%28MoCap%29%20data%20improves%20tracking%20accuracy.%20TWIST%20enables%20real-world%20humanoid%0Arobots%20to%20achieve%20unprecedented%2C%20versatile%2C%20and%20coordinated%20whole-body%20motor%0Askills--spanning%20whole-body%20manipulation%2C%20legged%20manipulation%2C%20locomotion%2C%20and%0Aexpressive%20movement--using%20a%20single%20unified%20neural%20network%20controller.%20Our%0Aproject%20website%3A%20https%3A//humanoid-teleop.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02833v1&entry.124074799=Read"},
{"title": "Towards Application-Specific Evaluation of Vision Models: Case Studies\n  in Ecology and Biology", "author": "Alex Hoi Hang Chan and Otto Brookes and Urs Waldmann and Hemal Naik and Iain D. Couzin and Majid Mirmehdi and No\u00ebl Adiko Houa and Emmanuelle Normand and Christophe Boesch and Lukas Boesch and Mimi Arandjelovic and Hjalmar K\u00fchl and Tilo Burghardt and Fumihiro Kano", "abstract": "  Computer vision methods have demonstrated considerable potential to\nstreamline ecological and biological workflows, with a growing number of\ndatasets and models becoming available to the research community. However,\nthese resources focus predominantly on evaluation using machine learning\nmetrics, with relatively little emphasis on how their application impacts\ndownstream analysis. We argue that models should be evaluated using\napplication-specific metrics that directly represent model performance in the\ncontext of its final use case. To support this argument, we present two\ndisparate case studies: (1) estimating chimpanzee abundance and density with\ncamera trap distance sampling when using a video-based behaviour classifier and\n(2) estimating head rotation in pigeons using a 3D posture estimator. We show\nthat even models with strong machine learning performance (e.g., 87% mAP) can\nyield data that leads to discrepancies in abundance estimates compared to\nexpert-derived data. Similarly, the highest-performing models for posture\nestimation do not produce the most accurate inferences of gaze direction in\npigeons. Motivated by these findings, we call for researchers to integrate\napplication-specific metrics in ecological/biological datasets, allowing for\nmodels to be benchmarked in the context of their downstream application and to\nfacilitate better integration of models into application workflows.\n", "link": "http://arxiv.org/abs/2505.02825v1", "date": "2025-05-05", "relevancy": 2.2251, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.557}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.557}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Application-Specific%20Evaluation%20of%20Vision%20Models%3A%20Case%20Studies%0A%20%20in%20Ecology%20and%20Biology&body=Title%3A%20Towards%20Application-Specific%20Evaluation%20of%20Vision%20Models%3A%20Case%20Studies%0A%20%20in%20Ecology%20and%20Biology%0AAuthor%3A%20Alex%20Hoi%20Hang%20Chan%20and%20Otto%20Brookes%20and%20Urs%20Waldmann%20and%20Hemal%20Naik%20and%20Iain%20D.%20Couzin%20and%20Majid%20Mirmehdi%20and%20No%C3%ABl%20Adiko%20Houa%20and%20Emmanuelle%20Normand%20and%20Christophe%20Boesch%20and%20Lukas%20Boesch%20and%20Mimi%20Arandjelovic%20and%20Hjalmar%20K%C3%BChl%20and%20Tilo%20Burghardt%20and%20Fumihiro%20Kano%0AAbstract%3A%20%20%20Computer%20vision%20methods%20have%20demonstrated%20considerable%20potential%20to%0Astreamline%20ecological%20and%20biological%20workflows%2C%20with%20a%20growing%20number%20of%0Adatasets%20and%20models%20becoming%20available%20to%20the%20research%20community.%20However%2C%0Athese%20resources%20focus%20predominantly%20on%20evaluation%20using%20machine%20learning%0Ametrics%2C%20with%20relatively%20little%20emphasis%20on%20how%20their%20application%20impacts%0Adownstream%20analysis.%20We%20argue%20that%20models%20should%20be%20evaluated%20using%0Aapplication-specific%20metrics%20that%20directly%20represent%20model%20performance%20in%20the%0Acontext%20of%20its%20final%20use%20case.%20To%20support%20this%20argument%2C%20we%20present%20two%0Adisparate%20case%20studies%3A%20%281%29%20estimating%20chimpanzee%20abundance%20and%20density%20with%0Acamera%20trap%20distance%20sampling%20when%20using%20a%20video-based%20behaviour%20classifier%20and%0A%282%29%20estimating%20head%20rotation%20in%20pigeons%20using%20a%203D%20posture%20estimator.%20We%20show%0Athat%20even%20models%20with%20strong%20machine%20learning%20performance%20%28e.g.%2C%2087%25%20mAP%29%20can%0Ayield%20data%20that%20leads%20to%20discrepancies%20in%20abundance%20estimates%20compared%20to%0Aexpert-derived%20data.%20Similarly%2C%20the%20highest-performing%20models%20for%20posture%0Aestimation%20do%20not%20produce%20the%20most%20accurate%20inferences%20of%20gaze%20direction%20in%0Apigeons.%20Motivated%20by%20these%20findings%2C%20we%20call%20for%20researchers%20to%20integrate%0Aapplication-specific%20metrics%20in%20ecological/biological%20datasets%2C%20allowing%20for%0Amodels%20to%20be%20benchmarked%20in%20the%20context%20of%20their%20downstream%20application%20and%20to%0Afacilitate%20better%20integration%20of%20models%20into%20application%20workflows.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02825v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Application-Specific%2520Evaluation%2520of%2520Vision%2520Models%253A%2520Case%2520Studies%250A%2520%2520in%2520Ecology%2520and%2520Biology%26entry.906535625%3DAlex%2520Hoi%2520Hang%2520Chan%2520and%2520Otto%2520Brookes%2520and%2520Urs%2520Waldmann%2520and%2520Hemal%2520Naik%2520and%2520Iain%2520D.%2520Couzin%2520and%2520Majid%2520Mirmehdi%2520and%2520No%25C3%25ABl%2520Adiko%2520Houa%2520and%2520Emmanuelle%2520Normand%2520and%2520Christophe%2520Boesch%2520and%2520Lukas%2520Boesch%2520and%2520Mimi%2520Arandjelovic%2520and%2520Hjalmar%2520K%25C3%25BChl%2520and%2520Tilo%2520Burghardt%2520and%2520Fumihiro%2520Kano%26entry.1292438233%3D%2520%2520Computer%2520vision%2520methods%2520have%2520demonstrated%2520considerable%2520potential%2520to%250Astreamline%2520ecological%2520and%2520biological%2520workflows%252C%2520with%2520a%2520growing%2520number%2520of%250Adatasets%2520and%2520models%2520becoming%2520available%2520to%2520the%2520research%2520community.%2520However%252C%250Athese%2520resources%2520focus%2520predominantly%2520on%2520evaluation%2520using%2520machine%2520learning%250Ametrics%252C%2520with%2520relatively%2520little%2520emphasis%2520on%2520how%2520their%2520application%2520impacts%250Adownstream%2520analysis.%2520We%2520argue%2520that%2520models%2520should%2520be%2520evaluated%2520using%250Aapplication-specific%2520metrics%2520that%2520directly%2520represent%2520model%2520performance%2520in%2520the%250Acontext%2520of%2520its%2520final%2520use%2520case.%2520To%2520support%2520this%2520argument%252C%2520we%2520present%2520two%250Adisparate%2520case%2520studies%253A%2520%25281%2529%2520estimating%2520chimpanzee%2520abundance%2520and%2520density%2520with%250Acamera%2520trap%2520distance%2520sampling%2520when%2520using%2520a%2520video-based%2520behaviour%2520classifier%2520and%250A%25282%2529%2520estimating%2520head%2520rotation%2520in%2520pigeons%2520using%2520a%25203D%2520posture%2520estimator.%2520We%2520show%250Athat%2520even%2520models%2520with%2520strong%2520machine%2520learning%2520performance%2520%2528e.g.%252C%252087%2525%2520mAP%2529%2520can%250Ayield%2520data%2520that%2520leads%2520to%2520discrepancies%2520in%2520abundance%2520estimates%2520compared%2520to%250Aexpert-derived%2520data.%2520Similarly%252C%2520the%2520highest-performing%2520models%2520for%2520posture%250Aestimation%2520do%2520not%2520produce%2520the%2520most%2520accurate%2520inferences%2520of%2520gaze%2520direction%2520in%250Apigeons.%2520Motivated%2520by%2520these%2520findings%252C%2520we%2520call%2520for%2520researchers%2520to%2520integrate%250Aapplication-specific%2520metrics%2520in%2520ecological/biological%2520datasets%252C%2520allowing%2520for%250Amodels%2520to%2520be%2520benchmarked%2520in%2520the%2520context%2520of%2520their%2520downstream%2520application%2520and%2520to%250Afacilitate%2520better%2520integration%2520of%2520models%2520into%2520application%2520workflows.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02825v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Application-Specific%20Evaluation%20of%20Vision%20Models%3A%20Case%20Studies%0A%20%20in%20Ecology%20and%20Biology&entry.906535625=Alex%20Hoi%20Hang%20Chan%20and%20Otto%20Brookes%20and%20Urs%20Waldmann%20and%20Hemal%20Naik%20and%20Iain%20D.%20Couzin%20and%20Majid%20Mirmehdi%20and%20No%C3%ABl%20Adiko%20Houa%20and%20Emmanuelle%20Normand%20and%20Christophe%20Boesch%20and%20Lukas%20Boesch%20and%20Mimi%20Arandjelovic%20and%20Hjalmar%20K%C3%BChl%20and%20Tilo%20Burghardt%20and%20Fumihiro%20Kano&entry.1292438233=%20%20Computer%20vision%20methods%20have%20demonstrated%20considerable%20potential%20to%0Astreamline%20ecological%20and%20biological%20workflows%2C%20with%20a%20growing%20number%20of%0Adatasets%20and%20models%20becoming%20available%20to%20the%20research%20community.%20However%2C%0Athese%20resources%20focus%20predominantly%20on%20evaluation%20using%20machine%20learning%0Ametrics%2C%20with%20relatively%20little%20emphasis%20on%20how%20their%20application%20impacts%0Adownstream%20analysis.%20We%20argue%20that%20models%20should%20be%20evaluated%20using%0Aapplication-specific%20metrics%20that%20directly%20represent%20model%20performance%20in%20the%0Acontext%20of%20its%20final%20use%20case.%20To%20support%20this%20argument%2C%20we%20present%20two%0Adisparate%20case%20studies%3A%20%281%29%20estimating%20chimpanzee%20abundance%20and%20density%20with%0Acamera%20trap%20distance%20sampling%20when%20using%20a%20video-based%20behaviour%20classifier%20and%0A%282%29%20estimating%20head%20rotation%20in%20pigeons%20using%20a%203D%20posture%20estimator.%20We%20show%0Athat%20even%20models%20with%20strong%20machine%20learning%20performance%20%28e.g.%2C%2087%25%20mAP%29%20can%0Ayield%20data%20that%20leads%20to%20discrepancies%20in%20abundance%20estimates%20compared%20to%0Aexpert-derived%20data.%20Similarly%2C%20the%20highest-performing%20models%20for%20posture%0Aestimation%20do%20not%20produce%20the%20most%20accurate%20inferences%20of%20gaze%20direction%20in%0Apigeons.%20Motivated%20by%20these%20findings%2C%20we%20call%20for%20researchers%20to%20integrate%0Aapplication-specific%20metrics%20in%20ecological/biological%20datasets%2C%20allowing%20for%0Amodels%20to%20be%20benchmarked%20in%20the%20context%20of%20their%20downstream%20application%20and%20to%0Afacilitate%20better%20integration%20of%20models%20into%20application%20workflows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02825v1&entry.124074799=Read"},
{"title": "Enhancing person re-identification via Uncertainty Feature Fusion Method\n  and Auto-weighted Measure Combination", "author": "Quang-Huy Che and Le-Chuong Nguyen and Duc-Tuan Luu and Vinh-Tiep Nguyen", "abstract": "  Person re-identification (Re-ID) is a challenging task that involves\nidentifying the same person across different camera views in surveillance\nsystems. Current methods usually rely on features from single-camera views,\nwhich can be limiting when dealing with multiple cameras and challenges such as\nchanging viewpoints and occlusions. In this paper, a new approach is introduced\nthat enhances the capability of ReID models through the Uncertain Feature\nFusion Method (UFFM) and Auto-weighted Measure Combination (AMC). UFFM\ngenerates multi-view features using features extracted independently from\nmultiple images to mitigate view bias. However, relying only on similarity\nbased on multi-view features is limited because these features ignore the\ndetails represented in single-view features. Therefore, we propose the AMC\nmethod to generate a more robust similarity measure by combining various\nmeasures. Our method significantly improves Rank@1 accuracy and Mean Average\nPrecision (mAP) when evaluated on person re-identification datasets. Combined\nwith the BoT Baseline on challenging datasets, we achieve impressive results,\nwith a 7.9% improvement in Rank@1 and a 12.1% improvement in mAP on the MSMT17\ndataset. On the Occluded-DukeMTMC dataset, our method increases Rank@1 by 22.0%\nand mAP by 18.4%. Code is available:\nhttps://github.com/chequanghuy/Enhancing-Person-Re-Identification-via-UFFM-and-AMC\n", "link": "http://arxiv.org/abs/2405.01101v5", "date": "2025-05-05", "relevancy": 2.2193, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5767}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5704}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5267}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20person%20re-identification%20via%20Uncertainty%20Feature%20Fusion%20Method%0A%20%20and%20Auto-weighted%20Measure%20Combination&body=Title%3A%20Enhancing%20person%20re-identification%20via%20Uncertainty%20Feature%20Fusion%20Method%0A%20%20and%20Auto-weighted%20Measure%20Combination%0AAuthor%3A%20Quang-Huy%20Che%20and%20Le-Chuong%20Nguyen%20and%20Duc-Tuan%20Luu%20and%20Vinh-Tiep%20Nguyen%0AAbstract%3A%20%20%20Person%20re-identification%20%28Re-ID%29%20is%20a%20challenging%20task%20that%20involves%0Aidentifying%20the%20same%20person%20across%20different%20camera%20views%20in%20surveillance%0Asystems.%20Current%20methods%20usually%20rely%20on%20features%20from%20single-camera%20views%2C%0Awhich%20can%20be%20limiting%20when%20dealing%20with%20multiple%20cameras%20and%20challenges%20such%20as%0Achanging%20viewpoints%20and%20occlusions.%20In%20this%20paper%2C%20a%20new%20approach%20is%20introduced%0Athat%20enhances%20the%20capability%20of%20ReID%20models%20through%20the%20Uncertain%20Feature%0AFusion%20Method%20%28UFFM%29%20and%20Auto-weighted%20Measure%20Combination%20%28AMC%29.%20UFFM%0Agenerates%20multi-view%20features%20using%20features%20extracted%20independently%20from%0Amultiple%20images%20to%20mitigate%20view%20bias.%20However%2C%20relying%20only%20on%20similarity%0Abased%20on%20multi-view%20features%20is%20limited%20because%20these%20features%20ignore%20the%0Adetails%20represented%20in%20single-view%20features.%20Therefore%2C%20we%20propose%20the%20AMC%0Amethod%20to%20generate%20a%20more%20robust%20similarity%20measure%20by%20combining%20various%0Ameasures.%20Our%20method%20significantly%20improves%20Rank%401%20accuracy%20and%20Mean%20Average%0APrecision%20%28mAP%29%20when%20evaluated%20on%20person%20re-identification%20datasets.%20Combined%0Awith%20the%20BoT%20Baseline%20on%20challenging%20datasets%2C%20we%20achieve%20impressive%20results%2C%0Awith%20a%207.9%25%20improvement%20in%20Rank%401%20and%20a%2012.1%25%20improvement%20in%20mAP%20on%20the%20MSMT17%0Adataset.%20On%20the%20Occluded-DukeMTMC%20dataset%2C%20our%20method%20increases%20Rank%401%20by%2022.0%25%0Aand%20mAP%20by%2018.4%25.%20Code%20is%20available%3A%0Ahttps%3A//github.com/chequanghuy/Enhancing-Person-Re-Identification-via-UFFM-and-AMC%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01101v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520person%2520re-identification%2520via%2520Uncertainty%2520Feature%2520Fusion%2520Method%250A%2520%2520and%2520Auto-weighted%2520Measure%2520Combination%26entry.906535625%3DQuang-Huy%2520Che%2520and%2520Le-Chuong%2520Nguyen%2520and%2520Duc-Tuan%2520Luu%2520and%2520Vinh-Tiep%2520Nguyen%26entry.1292438233%3D%2520%2520Person%2520re-identification%2520%2528Re-ID%2529%2520is%2520a%2520challenging%2520task%2520that%2520involves%250Aidentifying%2520the%2520same%2520person%2520across%2520different%2520camera%2520views%2520in%2520surveillance%250Asystems.%2520Current%2520methods%2520usually%2520rely%2520on%2520features%2520from%2520single-camera%2520views%252C%250Awhich%2520can%2520be%2520limiting%2520when%2520dealing%2520with%2520multiple%2520cameras%2520and%2520challenges%2520such%2520as%250Achanging%2520viewpoints%2520and%2520occlusions.%2520In%2520this%2520paper%252C%2520a%2520new%2520approach%2520is%2520introduced%250Athat%2520enhances%2520the%2520capability%2520of%2520ReID%2520models%2520through%2520the%2520Uncertain%2520Feature%250AFusion%2520Method%2520%2528UFFM%2529%2520and%2520Auto-weighted%2520Measure%2520Combination%2520%2528AMC%2529.%2520UFFM%250Agenerates%2520multi-view%2520features%2520using%2520features%2520extracted%2520independently%2520from%250Amultiple%2520images%2520to%2520mitigate%2520view%2520bias.%2520However%252C%2520relying%2520only%2520on%2520similarity%250Abased%2520on%2520multi-view%2520features%2520is%2520limited%2520because%2520these%2520features%2520ignore%2520the%250Adetails%2520represented%2520in%2520single-view%2520features.%2520Therefore%252C%2520we%2520propose%2520the%2520AMC%250Amethod%2520to%2520generate%2520a%2520more%2520robust%2520similarity%2520measure%2520by%2520combining%2520various%250Ameasures.%2520Our%2520method%2520significantly%2520improves%2520Rank%25401%2520accuracy%2520and%2520Mean%2520Average%250APrecision%2520%2528mAP%2529%2520when%2520evaluated%2520on%2520person%2520re-identification%2520datasets.%2520Combined%250Awith%2520the%2520BoT%2520Baseline%2520on%2520challenging%2520datasets%252C%2520we%2520achieve%2520impressive%2520results%252C%250Awith%2520a%25207.9%2525%2520improvement%2520in%2520Rank%25401%2520and%2520a%252012.1%2525%2520improvement%2520in%2520mAP%2520on%2520the%2520MSMT17%250Adataset.%2520On%2520the%2520Occluded-DukeMTMC%2520dataset%252C%2520our%2520method%2520increases%2520Rank%25401%2520by%252022.0%2525%250Aand%2520mAP%2520by%252018.4%2525.%2520Code%2520is%2520available%253A%250Ahttps%253A//github.com/chequanghuy/Enhancing-Person-Re-Identification-via-UFFM-and-AMC%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01101v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20person%20re-identification%20via%20Uncertainty%20Feature%20Fusion%20Method%0A%20%20and%20Auto-weighted%20Measure%20Combination&entry.906535625=Quang-Huy%20Che%20and%20Le-Chuong%20Nguyen%20and%20Duc-Tuan%20Luu%20and%20Vinh-Tiep%20Nguyen&entry.1292438233=%20%20Person%20re-identification%20%28Re-ID%29%20is%20a%20challenging%20task%20that%20involves%0Aidentifying%20the%20same%20person%20across%20different%20camera%20views%20in%20surveillance%0Asystems.%20Current%20methods%20usually%20rely%20on%20features%20from%20single-camera%20views%2C%0Awhich%20can%20be%20limiting%20when%20dealing%20with%20multiple%20cameras%20and%20challenges%20such%20as%0Achanging%20viewpoints%20and%20occlusions.%20In%20this%20paper%2C%20a%20new%20approach%20is%20introduced%0Athat%20enhances%20the%20capability%20of%20ReID%20models%20through%20the%20Uncertain%20Feature%0AFusion%20Method%20%28UFFM%29%20and%20Auto-weighted%20Measure%20Combination%20%28AMC%29.%20UFFM%0Agenerates%20multi-view%20features%20using%20features%20extracted%20independently%20from%0Amultiple%20images%20to%20mitigate%20view%20bias.%20However%2C%20relying%20only%20on%20similarity%0Abased%20on%20multi-view%20features%20is%20limited%20because%20these%20features%20ignore%20the%0Adetails%20represented%20in%20single-view%20features.%20Therefore%2C%20we%20propose%20the%20AMC%0Amethod%20to%20generate%20a%20more%20robust%20similarity%20measure%20by%20combining%20various%0Ameasures.%20Our%20method%20significantly%20improves%20Rank%401%20accuracy%20and%20Mean%20Average%0APrecision%20%28mAP%29%20when%20evaluated%20on%20person%20re-identification%20datasets.%20Combined%0Awith%20the%20BoT%20Baseline%20on%20challenging%20datasets%2C%20we%20achieve%20impressive%20results%2C%0Awith%20a%207.9%25%20improvement%20in%20Rank%401%20and%20a%2012.1%25%20improvement%20in%20mAP%20on%20the%20MSMT17%0Adataset.%20On%20the%20Occluded-DukeMTMC%20dataset%2C%20our%20method%20increases%20Rank%401%20by%2022.0%25%0Aand%20mAP%20by%2018.4%25.%20Code%20is%20available%3A%0Ahttps%3A//github.com/chequanghuy/Enhancing-Person-Re-Identification-via-UFFM-and-AMC%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01101v5&entry.124074799=Read"},
{"title": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated\n  Agent-Human Interplay", "author": "Akshara Prabhakar and Zuxin Liu and Ming Zhu and Jianguo Zhang and Tulika Awalgaonkar and Shiyu Wang and Zhiwei Liu and Haolin Chen and Thai Hoang and Juan Carlos Niebles and Shelby Heinecke and Weiran Yao and Huan Wang and Silvio Savarese and Caiming Xiong", "abstract": "  Training effective AI agents for multi-turn interactions requires\nhigh-quality data that captures realistic human-agent dynamics, yet such data\nis scarce and expensive to collect manually. We introduce APIGen-MT, a\ntwo-phase framework that generates verifiable and diverse multi-turn agent\ndata. In the first phase, our agentic pipeline produces detailed task\nblueprints with ground-truth actions, leveraging a committee of LLM reviewers\nand iterative feedback loops. These blueprints are then transformed into\ncomplete interaction trajectories through simulated human-agent interplay. We\ntrain a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B\nto 70B parameters. Our models outperform frontier models such as GPT-4o and\nClaude 3.5 on $\\tau$-bench and BFCL benchmarks, with the smaller models\nsurpassing their larger counterparts, particularly in multi-turn settings,\nwhile maintaining superior consistency across multiple trials. Comprehensive\nexperiments demonstrate that our verified blueprint-to-details approach yields\nhigh-quality training data, enabling the development of more reliable,\nefficient, and capable agents. We open-source 5K synthetic data trajectories\nand the trained xLAM-2-fc-r models to advance research in AI agents.\n  Models at\nhttps://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4;\nDataset at https://huggingface.co/datasets/Salesforce/APIGen-MT-5k and Website\nat https://apigen-mt.github.io\n", "link": "http://arxiv.org/abs/2504.03601v3", "date": "2025-05-05", "relevancy": 2.2167, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5774}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5409}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20APIGen-MT%3A%20Agentic%20Pipeline%20for%20Multi-Turn%20Data%20Generation%20via%20Simulated%0A%20%20Agent-Human%20Interplay&body=Title%3A%20APIGen-MT%3A%20Agentic%20Pipeline%20for%20Multi-Turn%20Data%20Generation%20via%20Simulated%0A%20%20Agent-Human%20Interplay%0AAuthor%3A%20Akshara%20Prabhakar%20and%20Zuxin%20Liu%20and%20Ming%20Zhu%20and%20Jianguo%20Zhang%20and%20Tulika%20Awalgaonkar%20and%20Shiyu%20Wang%20and%20Zhiwei%20Liu%20and%20Haolin%20Chen%20and%20Thai%20Hoang%20and%20Juan%20Carlos%20Niebles%20and%20Shelby%20Heinecke%20and%20Weiran%20Yao%20and%20Huan%20Wang%20and%20Silvio%20Savarese%20and%20Caiming%20Xiong%0AAbstract%3A%20%20%20Training%20effective%20AI%20agents%20for%20multi-turn%20interactions%20requires%0Ahigh-quality%20data%20that%20captures%20realistic%20human-agent%20dynamics%2C%20yet%20such%20data%0Ais%20scarce%20and%20expensive%20to%20collect%20manually.%20We%20introduce%20APIGen-MT%2C%20a%0Atwo-phase%20framework%20that%20generates%20verifiable%20and%20diverse%20multi-turn%20agent%0Adata.%20In%20the%20first%20phase%2C%20our%20agentic%20pipeline%20produces%20detailed%20task%0Ablueprints%20with%20ground-truth%20actions%2C%20leveraging%20a%20committee%20of%20LLM%20reviewers%0Aand%20iterative%20feedback%20loops.%20These%20blueprints%20are%20then%20transformed%20into%0Acomplete%20interaction%20trajectories%20through%20simulated%20human-agent%20interplay.%20We%0Atrain%20a%20family%20of%20models%20--%20the%20xLAM-2-fc-r%20series%20with%20sizes%20ranging%20from%201B%0Ato%2070B%20parameters.%20Our%20models%20outperform%20frontier%20models%20such%20as%20GPT-4o%20and%0AClaude%203.5%20on%20%24%5Ctau%24-bench%20and%20BFCL%20benchmarks%2C%20with%20the%20smaller%20models%0Asurpassing%20their%20larger%20counterparts%2C%20particularly%20in%20multi-turn%20settings%2C%0Awhile%20maintaining%20superior%20consistency%20across%20multiple%20trials.%20Comprehensive%0Aexperiments%20demonstrate%20that%20our%20verified%20blueprint-to-details%20approach%20yields%0Ahigh-quality%20training%20data%2C%20enabling%20the%20development%20of%20more%20reliable%2C%0Aefficient%2C%20and%20capable%20agents.%20We%20open-source%205K%20synthetic%20data%20trajectories%0Aand%20the%20trained%20xLAM-2-fc-r%20models%20to%20advance%20research%20in%20AI%20agents.%0A%20%20Models%20at%0Ahttps%3A//huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4%3B%0ADataset%20at%20https%3A//huggingface.co/datasets/Salesforce/APIGen-MT-5k%20and%20Website%0Aat%20https%3A//apigen-mt.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.03601v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAPIGen-MT%253A%2520Agentic%2520Pipeline%2520for%2520Multi-Turn%2520Data%2520Generation%2520via%2520Simulated%250A%2520%2520Agent-Human%2520Interplay%26entry.906535625%3DAkshara%2520Prabhakar%2520and%2520Zuxin%2520Liu%2520and%2520Ming%2520Zhu%2520and%2520Jianguo%2520Zhang%2520and%2520Tulika%2520Awalgaonkar%2520and%2520Shiyu%2520Wang%2520and%2520Zhiwei%2520Liu%2520and%2520Haolin%2520Chen%2520and%2520Thai%2520Hoang%2520and%2520Juan%2520Carlos%2520Niebles%2520and%2520Shelby%2520Heinecke%2520and%2520Weiran%2520Yao%2520and%2520Huan%2520Wang%2520and%2520Silvio%2520Savarese%2520and%2520Caiming%2520Xiong%26entry.1292438233%3D%2520%2520Training%2520effective%2520AI%2520agents%2520for%2520multi-turn%2520interactions%2520requires%250Ahigh-quality%2520data%2520that%2520captures%2520realistic%2520human-agent%2520dynamics%252C%2520yet%2520such%2520data%250Ais%2520scarce%2520and%2520expensive%2520to%2520collect%2520manually.%2520We%2520introduce%2520APIGen-MT%252C%2520a%250Atwo-phase%2520framework%2520that%2520generates%2520verifiable%2520and%2520diverse%2520multi-turn%2520agent%250Adata.%2520In%2520the%2520first%2520phase%252C%2520our%2520agentic%2520pipeline%2520produces%2520detailed%2520task%250Ablueprints%2520with%2520ground-truth%2520actions%252C%2520leveraging%2520a%2520committee%2520of%2520LLM%2520reviewers%250Aand%2520iterative%2520feedback%2520loops.%2520These%2520blueprints%2520are%2520then%2520transformed%2520into%250Acomplete%2520interaction%2520trajectories%2520through%2520simulated%2520human-agent%2520interplay.%2520We%250Atrain%2520a%2520family%2520of%2520models%2520--%2520the%2520xLAM-2-fc-r%2520series%2520with%2520sizes%2520ranging%2520from%25201B%250Ato%252070B%2520parameters.%2520Our%2520models%2520outperform%2520frontier%2520models%2520such%2520as%2520GPT-4o%2520and%250AClaude%25203.5%2520on%2520%2524%255Ctau%2524-bench%2520and%2520BFCL%2520benchmarks%252C%2520with%2520the%2520smaller%2520models%250Asurpassing%2520their%2520larger%2520counterparts%252C%2520particularly%2520in%2520multi-turn%2520settings%252C%250Awhile%2520maintaining%2520superior%2520consistency%2520across%2520multiple%2520trials.%2520Comprehensive%250Aexperiments%2520demonstrate%2520that%2520our%2520verified%2520blueprint-to-details%2520approach%2520yields%250Ahigh-quality%2520training%2520data%252C%2520enabling%2520the%2520development%2520of%2520more%2520reliable%252C%250Aefficient%252C%2520and%2520capable%2520agents.%2520We%2520open-source%25205K%2520synthetic%2520data%2520trajectories%250Aand%2520the%2520trained%2520xLAM-2-fc-r%2520models%2520to%2520advance%2520research%2520in%2520AI%2520agents.%250A%2520%2520Models%2520at%250Ahttps%253A//huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4%253B%250ADataset%2520at%2520https%253A//huggingface.co/datasets/Salesforce/APIGen-MT-5k%2520and%2520Website%250Aat%2520https%253A//apigen-mt.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.03601v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=APIGen-MT%3A%20Agentic%20Pipeline%20for%20Multi-Turn%20Data%20Generation%20via%20Simulated%0A%20%20Agent-Human%20Interplay&entry.906535625=Akshara%20Prabhakar%20and%20Zuxin%20Liu%20and%20Ming%20Zhu%20and%20Jianguo%20Zhang%20and%20Tulika%20Awalgaonkar%20and%20Shiyu%20Wang%20and%20Zhiwei%20Liu%20and%20Haolin%20Chen%20and%20Thai%20Hoang%20and%20Juan%20Carlos%20Niebles%20and%20Shelby%20Heinecke%20and%20Weiran%20Yao%20and%20Huan%20Wang%20and%20Silvio%20Savarese%20and%20Caiming%20Xiong&entry.1292438233=%20%20Training%20effective%20AI%20agents%20for%20multi-turn%20interactions%20requires%0Ahigh-quality%20data%20that%20captures%20realistic%20human-agent%20dynamics%2C%20yet%20such%20data%0Ais%20scarce%20and%20expensive%20to%20collect%20manually.%20We%20introduce%20APIGen-MT%2C%20a%0Atwo-phase%20framework%20that%20generates%20verifiable%20and%20diverse%20multi-turn%20agent%0Adata.%20In%20the%20first%20phase%2C%20our%20agentic%20pipeline%20produces%20detailed%20task%0Ablueprints%20with%20ground-truth%20actions%2C%20leveraging%20a%20committee%20of%20LLM%20reviewers%0Aand%20iterative%20feedback%20loops.%20These%20blueprints%20are%20then%20transformed%20into%0Acomplete%20interaction%20trajectories%20through%20simulated%20human-agent%20interplay.%20We%0Atrain%20a%20family%20of%20models%20--%20the%20xLAM-2-fc-r%20series%20with%20sizes%20ranging%20from%201B%0Ato%2070B%20parameters.%20Our%20models%20outperform%20frontier%20models%20such%20as%20GPT-4o%20and%0AClaude%203.5%20on%20%24%5Ctau%24-bench%20and%20BFCL%20benchmarks%2C%20with%20the%20smaller%20models%0Asurpassing%20their%20larger%20counterparts%2C%20particularly%20in%20multi-turn%20settings%2C%0Awhile%20maintaining%20superior%20consistency%20across%20multiple%20trials.%20Comprehensive%0Aexperiments%20demonstrate%20that%20our%20verified%20blueprint-to-details%20approach%20yields%0Ahigh-quality%20training%20data%2C%20enabling%20the%20development%20of%20more%20reliable%2C%0Aefficient%2C%20and%20capable%20agents.%20We%20open-source%205K%20synthetic%20data%20trajectories%0Aand%20the%20trained%20xLAM-2-fc-r%20models%20to%20advance%20research%20in%20AI%20agents.%0A%20%20Models%20at%0Ahttps%3A//huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4%3B%0ADataset%20at%20https%3A//huggingface.co/datasets/Salesforce/APIGen-MT-5k%20and%20Website%0Aat%20https%3A//apigen-mt.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.03601v3&entry.124074799=Read"},
{"title": "Large Language Models Understanding: an Inherent Ambiguity Barrier", "author": "Daniel N. Nissani", "abstract": "  A lively ongoing debate is taking place, since the extraordinary emergence of\nLarge Language Models (LLMs) with regards to their capability to understand the\nworld and capture the meaning of the dialogues in which they are involved.\nArguments and counter-arguments have been proposed based upon thought\nexperiments, anecdotal conversations between LLMs and humans, statistical\nlinguistic analysis, philosophical considerations, and more. In this brief\npaper we present a counter-argument based upon a thought experiment and\nsemi-formal considerations leading to an inherent ambiguity barrier which\nprevents LLMs from having any understanding of what their amazingly fluent\ndialogues mean.\n", "link": "http://arxiv.org/abs/2505.00654v2", "date": "2025-05-05", "relevancy": 2.2161, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5655}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5655}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4969}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20Understanding%3A%20an%20Inherent%20Ambiguity%20Barrier&body=Title%3A%20Large%20Language%20Models%20Understanding%3A%20an%20Inherent%20Ambiguity%20Barrier%0AAuthor%3A%20Daniel%20N.%20Nissani%0AAbstract%3A%20%20%20A%20lively%20ongoing%20debate%20is%20taking%20place%2C%20since%20the%20extraordinary%20emergence%20of%0ALarge%20Language%20Models%20%28LLMs%29%20with%20regards%20to%20their%20capability%20to%20understand%20the%0Aworld%20and%20capture%20the%20meaning%20of%20the%20dialogues%20in%20which%20they%20are%20involved.%0AArguments%20and%20counter-arguments%20have%20been%20proposed%20based%20upon%20thought%0Aexperiments%2C%20anecdotal%20conversations%20between%20LLMs%20and%20humans%2C%20statistical%0Alinguistic%20analysis%2C%20philosophical%20considerations%2C%20and%20more.%20In%20this%20brief%0Apaper%20we%20present%20a%20counter-argument%20based%20upon%20a%20thought%20experiment%20and%0Asemi-formal%20considerations%20leading%20to%20an%20inherent%20ambiguity%20barrier%20which%0Aprevents%20LLMs%20from%20having%20any%20understanding%20of%20what%20their%20amazingly%20fluent%0Adialogues%20mean.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00654v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520Understanding%253A%2520an%2520Inherent%2520Ambiguity%2520Barrier%26entry.906535625%3DDaniel%2520N.%2520Nissani%26entry.1292438233%3D%2520%2520A%2520lively%2520ongoing%2520debate%2520is%2520taking%2520place%252C%2520since%2520the%2520extraordinary%2520emergence%2520of%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%2520with%2520regards%2520to%2520their%2520capability%2520to%2520understand%2520the%250Aworld%2520and%2520capture%2520the%2520meaning%2520of%2520the%2520dialogues%2520in%2520which%2520they%2520are%2520involved.%250AArguments%2520and%2520counter-arguments%2520have%2520been%2520proposed%2520based%2520upon%2520thought%250Aexperiments%252C%2520anecdotal%2520conversations%2520between%2520LLMs%2520and%2520humans%252C%2520statistical%250Alinguistic%2520analysis%252C%2520philosophical%2520considerations%252C%2520and%2520more.%2520In%2520this%2520brief%250Apaper%2520we%2520present%2520a%2520counter-argument%2520based%2520upon%2520a%2520thought%2520experiment%2520and%250Asemi-formal%2520considerations%2520leading%2520to%2520an%2520inherent%2520ambiguity%2520barrier%2520which%250Aprevents%2520LLMs%2520from%2520having%2520any%2520understanding%2520of%2520what%2520their%2520amazingly%2520fluent%250Adialogues%2520mean.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00654v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20Understanding%3A%20an%20Inherent%20Ambiguity%20Barrier&entry.906535625=Daniel%20N.%20Nissani&entry.1292438233=%20%20A%20lively%20ongoing%20debate%20is%20taking%20place%2C%20since%20the%20extraordinary%20emergence%20of%0ALarge%20Language%20Models%20%28LLMs%29%20with%20regards%20to%20their%20capability%20to%20understand%20the%0Aworld%20and%20capture%20the%20meaning%20of%20the%20dialogues%20in%20which%20they%20are%20involved.%0AArguments%20and%20counter-arguments%20have%20been%20proposed%20based%20upon%20thought%0Aexperiments%2C%20anecdotal%20conversations%20between%20LLMs%20and%20humans%2C%20statistical%0Alinguistic%20analysis%2C%20philosophical%20considerations%2C%20and%20more.%20In%20this%20brief%0Apaper%20we%20present%20a%20counter-argument%20based%20upon%20a%20thought%20experiment%20and%0Asemi-formal%20considerations%20leading%20to%20an%20inherent%20ambiguity%20barrier%20which%0Aprevents%20LLMs%20from%20having%20any%20understanding%20of%20what%20their%20amazingly%20fluent%0Adialogues%20mean.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00654v2&entry.124074799=Read"},
{"title": "DELTA: Dense Depth from Events and LiDAR using Transformer's Attention", "author": "Vincent Brebion and Julien Moreau and Franck Davoine", "abstract": "  Event cameras and LiDARs provide complementary yet distinct data:\nrespectively, asynchronous detections of changes in lighting versus sparse but\naccurate depth information at a fixed rate. To this day, few works have\nexplored the combination of these two modalities. In this article, we propose a\nnovel neural-network-based method for fusing event and LiDAR data in order to\nestimate dense depth maps. Our architecture, DELTA, exploits the concepts of\nself- and cross-attention to model the spatial and temporal relations within\nand between the event and LiDAR data. Following a thorough evaluation, we\ndemonstrate that DELTA sets a new state of the art in the event-based depth\nestimation problem, and that it is able to reduce the errors up to four times\nfor close ranges compared to the previous SOTA.\n", "link": "http://arxiv.org/abs/2505.02593v1", "date": "2025-05-05", "relevancy": 2.2121, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5854}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.548}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DELTA%3A%20Dense%20Depth%20from%20Events%20and%20LiDAR%20using%20Transformer%27s%20Attention&body=Title%3A%20DELTA%3A%20Dense%20Depth%20from%20Events%20and%20LiDAR%20using%20Transformer%27s%20Attention%0AAuthor%3A%20Vincent%20Brebion%20and%20Julien%20Moreau%20and%20Franck%20Davoine%0AAbstract%3A%20%20%20Event%20cameras%20and%20LiDARs%20provide%20complementary%20yet%20distinct%20data%3A%0Arespectively%2C%20asynchronous%20detections%20of%20changes%20in%20lighting%20versus%20sparse%20but%0Aaccurate%20depth%20information%20at%20a%20fixed%20rate.%20To%20this%20day%2C%20few%20works%20have%0Aexplored%20the%20combination%20of%20these%20two%20modalities.%20In%20this%20article%2C%20we%20propose%20a%0Anovel%20neural-network-based%20method%20for%20fusing%20event%20and%20LiDAR%20data%20in%20order%20to%0Aestimate%20dense%20depth%20maps.%20Our%20architecture%2C%20DELTA%2C%20exploits%20the%20concepts%20of%0Aself-%20and%20cross-attention%20to%20model%20the%20spatial%20and%20temporal%20relations%20within%0Aand%20between%20the%20event%20and%20LiDAR%20data.%20Following%20a%20thorough%20evaluation%2C%20we%0Ademonstrate%20that%20DELTA%20sets%20a%20new%20state%20of%20the%20art%20in%20the%20event-based%20depth%0Aestimation%20problem%2C%20and%20that%20it%20is%20able%20to%20reduce%20the%20errors%20up%20to%20four%20times%0Afor%20close%20ranges%20compared%20to%20the%20previous%20SOTA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02593v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDELTA%253A%2520Dense%2520Depth%2520from%2520Events%2520and%2520LiDAR%2520using%2520Transformer%2527s%2520Attention%26entry.906535625%3DVincent%2520Brebion%2520and%2520Julien%2520Moreau%2520and%2520Franck%2520Davoine%26entry.1292438233%3D%2520%2520Event%2520cameras%2520and%2520LiDARs%2520provide%2520complementary%2520yet%2520distinct%2520data%253A%250Arespectively%252C%2520asynchronous%2520detections%2520of%2520changes%2520in%2520lighting%2520versus%2520sparse%2520but%250Aaccurate%2520depth%2520information%2520at%2520a%2520fixed%2520rate.%2520To%2520this%2520day%252C%2520few%2520works%2520have%250Aexplored%2520the%2520combination%2520of%2520these%2520two%2520modalities.%2520In%2520this%2520article%252C%2520we%2520propose%2520a%250Anovel%2520neural-network-based%2520method%2520for%2520fusing%2520event%2520and%2520LiDAR%2520data%2520in%2520order%2520to%250Aestimate%2520dense%2520depth%2520maps.%2520Our%2520architecture%252C%2520DELTA%252C%2520exploits%2520the%2520concepts%2520of%250Aself-%2520and%2520cross-attention%2520to%2520model%2520the%2520spatial%2520and%2520temporal%2520relations%2520within%250Aand%2520between%2520the%2520event%2520and%2520LiDAR%2520data.%2520Following%2520a%2520thorough%2520evaluation%252C%2520we%250Ademonstrate%2520that%2520DELTA%2520sets%2520a%2520new%2520state%2520of%2520the%2520art%2520in%2520the%2520event-based%2520depth%250Aestimation%2520problem%252C%2520and%2520that%2520it%2520is%2520able%2520to%2520reduce%2520the%2520errors%2520up%2520to%2520four%2520times%250Afor%2520close%2520ranges%2520compared%2520to%2520the%2520previous%2520SOTA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02593v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DELTA%3A%20Dense%20Depth%20from%20Events%20and%20LiDAR%20using%20Transformer%27s%20Attention&entry.906535625=Vincent%20Brebion%20and%20Julien%20Moreau%20and%20Franck%20Davoine&entry.1292438233=%20%20Event%20cameras%20and%20LiDARs%20provide%20complementary%20yet%20distinct%20data%3A%0Arespectively%2C%20asynchronous%20detections%20of%20changes%20in%20lighting%20versus%20sparse%20but%0Aaccurate%20depth%20information%20at%20a%20fixed%20rate.%20To%20this%20day%2C%20few%20works%20have%0Aexplored%20the%20combination%20of%20these%20two%20modalities.%20In%20this%20article%2C%20we%20propose%20a%0Anovel%20neural-network-based%20method%20for%20fusing%20event%20and%20LiDAR%20data%20in%20order%20to%0Aestimate%20dense%20depth%20maps.%20Our%20architecture%2C%20DELTA%2C%20exploits%20the%20concepts%20of%0Aself-%20and%20cross-attention%20to%20model%20the%20spatial%20and%20temporal%20relations%20within%0Aand%20between%20the%20event%20and%20LiDAR%20data.%20Following%20a%20thorough%20evaluation%2C%20we%0Ademonstrate%20that%20DELTA%20sets%20a%20new%20state%20of%20the%20art%20in%20the%20event-based%20depth%0Aestimation%20problem%2C%20and%20that%20it%20is%20able%20to%20reduce%20the%20errors%20up%20to%20four%20times%0Afor%20close%20ranges%20compared%20to%20the%20previous%20SOTA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02593v1&entry.124074799=Read"},
{"title": "Less is More: Efficient Weight Farcasting with 1-Layer Neural Network", "author": "Xiao Shou and Debarun Bhattacharjya and Yanna Ding and Chen Zhao and Rui Li and Jianxi Gao", "abstract": "  Addressing the computational challenges inherent in training large-scale deep\nneural networks remains a critical endeavor in contemporary machine learning\nresearch. While previous efforts have focused on enhancing training efficiency\nthrough techniques such as gradient descent with momentum, learning rate\nscheduling, and weight regularization, the demand for further innovation\ncontinues to burgeon as model sizes keep expanding. In this study, we introduce\na novel framework which diverges from conventional approaches by leveraging\nlong-term time series forecasting techniques. Our method capitalizes solely on\ninitial and final weight values, offering a streamlined alternative for complex\nmodel architectures. We also introduce a novel regularizer that is tailored to\nenhance the forecasting performance of our approach. Empirical evaluations\nconducted on synthetic weight sequences and real-world deep learning\narchitectures, including the prominent large language model DistilBERT,\ndemonstrate the superiority of our method in terms of forecasting accuracy and\ncomputational efficiency. Notably, our framework showcases improved performance\nwhile requiring minimal additional computational overhead, thus presenting a\npromising avenue for accelerating the training process across diverse tasks and\narchitectures.\n", "link": "http://arxiv.org/abs/2505.02714v1", "date": "2025-05-05", "relevancy": 2.2106, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5569}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5545}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Less%20is%20More%3A%20Efficient%20Weight%20Farcasting%20with%201-Layer%20Neural%20Network&body=Title%3A%20Less%20is%20More%3A%20Efficient%20Weight%20Farcasting%20with%201-Layer%20Neural%20Network%0AAuthor%3A%20Xiao%20Shou%20and%20Debarun%20Bhattacharjya%20and%20Yanna%20Ding%20and%20Chen%20Zhao%20and%20Rui%20Li%20and%20Jianxi%20Gao%0AAbstract%3A%20%20%20Addressing%20the%20computational%20challenges%20inherent%20in%20training%20large-scale%20deep%0Aneural%20networks%20remains%20a%20critical%20endeavor%20in%20contemporary%20machine%20learning%0Aresearch.%20While%20previous%20efforts%20have%20focused%20on%20enhancing%20training%20efficiency%0Athrough%20techniques%20such%20as%20gradient%20descent%20with%20momentum%2C%20learning%20rate%0Ascheduling%2C%20and%20weight%20regularization%2C%20the%20demand%20for%20further%20innovation%0Acontinues%20to%20burgeon%20as%20model%20sizes%20keep%20expanding.%20In%20this%20study%2C%20we%20introduce%0Aa%20novel%20framework%20which%20diverges%20from%20conventional%20approaches%20by%20leveraging%0Along-term%20time%20series%20forecasting%20techniques.%20Our%20method%20capitalizes%20solely%20on%0Ainitial%20and%20final%20weight%20values%2C%20offering%20a%20streamlined%20alternative%20for%20complex%0Amodel%20architectures.%20We%20also%20introduce%20a%20novel%20regularizer%20that%20is%20tailored%20to%0Aenhance%20the%20forecasting%20performance%20of%20our%20approach.%20Empirical%20evaluations%0Aconducted%20on%20synthetic%20weight%20sequences%20and%20real-world%20deep%20learning%0Aarchitectures%2C%20including%20the%20prominent%20large%20language%20model%20DistilBERT%2C%0Ademonstrate%20the%20superiority%20of%20our%20method%20in%20terms%20of%20forecasting%20accuracy%20and%0Acomputational%20efficiency.%20Notably%2C%20our%20framework%20showcases%20improved%20performance%0Awhile%20requiring%20minimal%20additional%20computational%20overhead%2C%20thus%20presenting%20a%0Apromising%20avenue%20for%20accelerating%20the%20training%20process%20across%20diverse%20tasks%20and%0Aarchitectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02714v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLess%2520is%2520More%253A%2520Efficient%2520Weight%2520Farcasting%2520with%25201-Layer%2520Neural%2520Network%26entry.906535625%3DXiao%2520Shou%2520and%2520Debarun%2520Bhattacharjya%2520and%2520Yanna%2520Ding%2520and%2520Chen%2520Zhao%2520and%2520Rui%2520Li%2520and%2520Jianxi%2520Gao%26entry.1292438233%3D%2520%2520Addressing%2520the%2520computational%2520challenges%2520inherent%2520in%2520training%2520large-scale%2520deep%250Aneural%2520networks%2520remains%2520a%2520critical%2520endeavor%2520in%2520contemporary%2520machine%2520learning%250Aresearch.%2520While%2520previous%2520efforts%2520have%2520focused%2520on%2520enhancing%2520training%2520efficiency%250Athrough%2520techniques%2520such%2520as%2520gradient%2520descent%2520with%2520momentum%252C%2520learning%2520rate%250Ascheduling%252C%2520and%2520weight%2520regularization%252C%2520the%2520demand%2520for%2520further%2520innovation%250Acontinues%2520to%2520burgeon%2520as%2520model%2520sizes%2520keep%2520expanding.%2520In%2520this%2520study%252C%2520we%2520introduce%250Aa%2520novel%2520framework%2520which%2520diverges%2520from%2520conventional%2520approaches%2520by%2520leveraging%250Along-term%2520time%2520series%2520forecasting%2520techniques.%2520Our%2520method%2520capitalizes%2520solely%2520on%250Ainitial%2520and%2520final%2520weight%2520values%252C%2520offering%2520a%2520streamlined%2520alternative%2520for%2520complex%250Amodel%2520architectures.%2520We%2520also%2520introduce%2520a%2520novel%2520regularizer%2520that%2520is%2520tailored%2520to%250Aenhance%2520the%2520forecasting%2520performance%2520of%2520our%2520approach.%2520Empirical%2520evaluations%250Aconducted%2520on%2520synthetic%2520weight%2520sequences%2520and%2520real-world%2520deep%2520learning%250Aarchitectures%252C%2520including%2520the%2520prominent%2520large%2520language%2520model%2520DistilBERT%252C%250Ademonstrate%2520the%2520superiority%2520of%2520our%2520method%2520in%2520terms%2520of%2520forecasting%2520accuracy%2520and%250Acomputational%2520efficiency.%2520Notably%252C%2520our%2520framework%2520showcases%2520improved%2520performance%250Awhile%2520requiring%2520minimal%2520additional%2520computational%2520overhead%252C%2520thus%2520presenting%2520a%250Apromising%2520avenue%2520for%2520accelerating%2520the%2520training%2520process%2520across%2520diverse%2520tasks%2520and%250Aarchitectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02714v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Less%20is%20More%3A%20Efficient%20Weight%20Farcasting%20with%201-Layer%20Neural%20Network&entry.906535625=Xiao%20Shou%20and%20Debarun%20Bhattacharjya%20and%20Yanna%20Ding%20and%20Chen%20Zhao%20and%20Rui%20Li%20and%20Jianxi%20Gao&entry.1292438233=%20%20Addressing%20the%20computational%20challenges%20inherent%20in%20training%20large-scale%20deep%0Aneural%20networks%20remains%20a%20critical%20endeavor%20in%20contemporary%20machine%20learning%0Aresearch.%20While%20previous%20efforts%20have%20focused%20on%20enhancing%20training%20efficiency%0Athrough%20techniques%20such%20as%20gradient%20descent%20with%20momentum%2C%20learning%20rate%0Ascheduling%2C%20and%20weight%20regularization%2C%20the%20demand%20for%20further%20innovation%0Acontinues%20to%20burgeon%20as%20model%20sizes%20keep%20expanding.%20In%20this%20study%2C%20we%20introduce%0Aa%20novel%20framework%20which%20diverges%20from%20conventional%20approaches%20by%20leveraging%0Along-term%20time%20series%20forecasting%20techniques.%20Our%20method%20capitalizes%20solely%20on%0Ainitial%20and%20final%20weight%20values%2C%20offering%20a%20streamlined%20alternative%20for%20complex%0Amodel%20architectures.%20We%20also%20introduce%20a%20novel%20regularizer%20that%20is%20tailored%20to%0Aenhance%20the%20forecasting%20performance%20of%20our%20approach.%20Empirical%20evaluations%0Aconducted%20on%20synthetic%20weight%20sequences%20and%20real-world%20deep%20learning%0Aarchitectures%2C%20including%20the%20prominent%20large%20language%20model%20DistilBERT%2C%0Ademonstrate%20the%20superiority%20of%20our%20method%20in%20terms%20of%20forecasting%20accuracy%20and%0Acomputational%20efficiency.%20Notably%2C%20our%20framework%20showcases%20improved%20performance%0Awhile%20requiring%20minimal%20additional%20computational%20overhead%2C%20thus%20presenting%20a%0Apromising%20avenue%20for%20accelerating%20the%20training%20process%20across%20diverse%20tasks%20and%0Aarchitectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02714v1&entry.124074799=Read"},
{"title": "Geometric Knowledge-Guided Localized Global Distribution Alignment for\n  Federated Learning", "author": "Yanbiao Ma and Wei Dai and Wenke Huang and Jiayi Chen", "abstract": "  Data heterogeneity in federated learning, characterized by a significant\nmisalignment between local and global distributions, leads to divergent local\noptimization directions and hinders global model training. Existing studies\nmainly focus on optimizing local updates or global aggregation, but these\nindirect approaches demonstrate instability when handling highly heterogeneous\ndata distributions, especially in scenarios where label skew and domain skew\ncoexist. To address this, we propose a geometry-guided data generation method\nthat centers on simulating the global embedding distribution locally. We first\nintroduce the concept of the geometric shape of an embedding distribution and\nthen address the challenge of obtaining global geometric shapes under privacy\nconstraints. Subsequently, we propose GGEUR, which leverages global geometric\nshapes to guide the generation of new samples, enabling a closer approximation\nto the ideal global distribution. In single-domain scenarios, we augment\nsamples based on global geometric shapes to enhance model generalization; in\nmulti-domain scenarios, we further employ class prototypes to simulate the\nglobal distribution across domains. Extensive experimental results demonstrate\nthat our method significantly enhances the performance of existing approaches\nin handling highly heterogeneous data, including scenarios with label skew,\ndomain skew, and their coexistence. Code published at:\nhttps://github.com/WeiDai-David/2025CVPR_GGEUR\n", "link": "http://arxiv.org/abs/2503.06457v2", "date": "2025-05-05", "relevancy": 2.205, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5635}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5435}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometric%20Knowledge-Guided%20Localized%20Global%20Distribution%20Alignment%20for%0A%20%20Federated%20Learning&body=Title%3A%20Geometric%20Knowledge-Guided%20Localized%20Global%20Distribution%20Alignment%20for%0A%20%20Federated%20Learning%0AAuthor%3A%20Yanbiao%20Ma%20and%20Wei%20Dai%20and%20Wenke%20Huang%20and%20Jiayi%20Chen%0AAbstract%3A%20%20%20Data%20heterogeneity%20in%20federated%20learning%2C%20characterized%20by%20a%20significant%0Amisalignment%20between%20local%20and%20global%20distributions%2C%20leads%20to%20divergent%20local%0Aoptimization%20directions%20and%20hinders%20global%20model%20training.%20Existing%20studies%0Amainly%20focus%20on%20optimizing%20local%20updates%20or%20global%20aggregation%2C%20but%20these%0Aindirect%20approaches%20demonstrate%20instability%20when%20handling%20highly%20heterogeneous%0Adata%20distributions%2C%20especially%20in%20scenarios%20where%20label%20skew%20and%20domain%20skew%0Acoexist.%20To%20address%20this%2C%20we%20propose%20a%20geometry-guided%20data%20generation%20method%0Athat%20centers%20on%20simulating%20the%20global%20embedding%20distribution%20locally.%20We%20first%0Aintroduce%20the%20concept%20of%20the%20geometric%20shape%20of%20an%20embedding%20distribution%20and%0Athen%20address%20the%20challenge%20of%20obtaining%20global%20geometric%20shapes%20under%20privacy%0Aconstraints.%20Subsequently%2C%20we%20propose%20GGEUR%2C%20which%20leverages%20global%20geometric%0Ashapes%20to%20guide%20the%20generation%20of%20new%20samples%2C%20enabling%20a%20closer%20approximation%0Ato%20the%20ideal%20global%20distribution.%20In%20single-domain%20scenarios%2C%20we%20augment%0Asamples%20based%20on%20global%20geometric%20shapes%20to%20enhance%20model%20generalization%3B%20in%0Amulti-domain%20scenarios%2C%20we%20further%20employ%20class%20prototypes%20to%20simulate%20the%0Aglobal%20distribution%20across%20domains.%20Extensive%20experimental%20results%20demonstrate%0Athat%20our%20method%20significantly%20enhances%20the%20performance%20of%20existing%20approaches%0Ain%20handling%20highly%20heterogeneous%20data%2C%20including%20scenarios%20with%20label%20skew%2C%0Adomain%20skew%2C%20and%20their%20coexistence.%20Code%20published%20at%3A%0Ahttps%3A//github.com/WeiDai-David/2025CVPR_GGEUR%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.06457v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometric%2520Knowledge-Guided%2520Localized%2520Global%2520Distribution%2520Alignment%2520for%250A%2520%2520Federated%2520Learning%26entry.906535625%3DYanbiao%2520Ma%2520and%2520Wei%2520Dai%2520and%2520Wenke%2520Huang%2520and%2520Jiayi%2520Chen%26entry.1292438233%3D%2520%2520Data%2520heterogeneity%2520in%2520federated%2520learning%252C%2520characterized%2520by%2520a%2520significant%250Amisalignment%2520between%2520local%2520and%2520global%2520distributions%252C%2520leads%2520to%2520divergent%2520local%250Aoptimization%2520directions%2520and%2520hinders%2520global%2520model%2520training.%2520Existing%2520studies%250Amainly%2520focus%2520on%2520optimizing%2520local%2520updates%2520or%2520global%2520aggregation%252C%2520but%2520these%250Aindirect%2520approaches%2520demonstrate%2520instability%2520when%2520handling%2520highly%2520heterogeneous%250Adata%2520distributions%252C%2520especially%2520in%2520scenarios%2520where%2520label%2520skew%2520and%2520domain%2520skew%250Acoexist.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520geometry-guided%2520data%2520generation%2520method%250Athat%2520centers%2520on%2520simulating%2520the%2520global%2520embedding%2520distribution%2520locally.%2520We%2520first%250Aintroduce%2520the%2520concept%2520of%2520the%2520geometric%2520shape%2520of%2520an%2520embedding%2520distribution%2520and%250Athen%2520address%2520the%2520challenge%2520of%2520obtaining%2520global%2520geometric%2520shapes%2520under%2520privacy%250Aconstraints.%2520Subsequently%252C%2520we%2520propose%2520GGEUR%252C%2520which%2520leverages%2520global%2520geometric%250Ashapes%2520to%2520guide%2520the%2520generation%2520of%2520new%2520samples%252C%2520enabling%2520a%2520closer%2520approximation%250Ato%2520the%2520ideal%2520global%2520distribution.%2520In%2520single-domain%2520scenarios%252C%2520we%2520augment%250Asamples%2520based%2520on%2520global%2520geometric%2520shapes%2520to%2520enhance%2520model%2520generalization%253B%2520in%250Amulti-domain%2520scenarios%252C%2520we%2520further%2520employ%2520class%2520prototypes%2520to%2520simulate%2520the%250Aglobal%2520distribution%2520across%2520domains.%2520Extensive%2520experimental%2520results%2520demonstrate%250Athat%2520our%2520method%2520significantly%2520enhances%2520the%2520performance%2520of%2520existing%2520approaches%250Ain%2520handling%2520highly%2520heterogeneous%2520data%252C%2520including%2520scenarios%2520with%2520label%2520skew%252C%250Adomain%2520skew%252C%2520and%2520their%2520coexistence.%2520Code%2520published%2520at%253A%250Ahttps%253A//github.com/WeiDai-David/2025CVPR_GGEUR%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.06457v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometric%20Knowledge-Guided%20Localized%20Global%20Distribution%20Alignment%20for%0A%20%20Federated%20Learning&entry.906535625=Yanbiao%20Ma%20and%20Wei%20Dai%20and%20Wenke%20Huang%20and%20Jiayi%20Chen&entry.1292438233=%20%20Data%20heterogeneity%20in%20federated%20learning%2C%20characterized%20by%20a%20significant%0Amisalignment%20between%20local%20and%20global%20distributions%2C%20leads%20to%20divergent%20local%0Aoptimization%20directions%20and%20hinders%20global%20model%20training.%20Existing%20studies%0Amainly%20focus%20on%20optimizing%20local%20updates%20or%20global%20aggregation%2C%20but%20these%0Aindirect%20approaches%20demonstrate%20instability%20when%20handling%20highly%20heterogeneous%0Adata%20distributions%2C%20especially%20in%20scenarios%20where%20label%20skew%20and%20domain%20skew%0Acoexist.%20To%20address%20this%2C%20we%20propose%20a%20geometry-guided%20data%20generation%20method%0Athat%20centers%20on%20simulating%20the%20global%20embedding%20distribution%20locally.%20We%20first%0Aintroduce%20the%20concept%20of%20the%20geometric%20shape%20of%20an%20embedding%20distribution%20and%0Athen%20address%20the%20challenge%20of%20obtaining%20global%20geometric%20shapes%20under%20privacy%0Aconstraints.%20Subsequently%2C%20we%20propose%20GGEUR%2C%20which%20leverages%20global%20geometric%0Ashapes%20to%20guide%20the%20generation%20of%20new%20samples%2C%20enabling%20a%20closer%20approximation%0Ato%20the%20ideal%20global%20distribution.%20In%20single-domain%20scenarios%2C%20we%20augment%0Asamples%20based%20on%20global%20geometric%20shapes%20to%20enhance%20model%20generalization%3B%20in%0Amulti-domain%20scenarios%2C%20we%20further%20employ%20class%20prototypes%20to%20simulate%20the%0Aglobal%20distribution%20across%20domains.%20Extensive%20experimental%20results%20demonstrate%0Athat%20our%20method%20significantly%20enhances%20the%20performance%20of%20existing%20approaches%0Ain%20handling%20highly%20heterogeneous%20data%2C%20including%20scenarios%20with%20label%20skew%2C%0Adomain%20skew%2C%20and%20their%20coexistence.%20Code%20published%20at%3A%0Ahttps%3A//github.com/WeiDai-David/2025CVPR_GGEUR%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.06457v2&entry.124074799=Read"},
{"title": "Inverse Dynamics Trajectory Optimization for Contact-Implicit Model\n  Predictive Control", "author": "Vince Kurtz and Alejandro Castro and Aykut \u00d6zg\u00fcn \u00d6nol and Hai Lin", "abstract": "  Robots must make and break contact with the environment to perform useful\ntasks, but planning and control through contact remains a formidable challenge.\nIn this work, we achieve real-time contact-implicit model predictive control\nwith a surprisingly simple method: inverse dynamics trajectory optimization.\nWhile trajectory optimization with inverse dynamics is not new, we introduce a\nseries of incremental innovations that collectively enable fast model\npredictive control on a variety of challenging manipulation and locomotion\ntasks. We implement these innovations in an open-source solver and present\nsimulation examples to support the effectiveness of the proposed approach.\nAdditionally, we demonstrate contact-implicit model predictive control on\nhardware at over 100 Hz for a 20-degree-of-freedom bi-manual manipulation task.\nVideo and code are available at https://idto.github.io.\n", "link": "http://arxiv.org/abs/2309.01813v3", "date": "2025-05-05", "relevancy": 2.2024, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5696}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5682}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inverse%20Dynamics%20Trajectory%20Optimization%20for%20Contact-Implicit%20Model%0A%20%20Predictive%20Control&body=Title%3A%20Inverse%20Dynamics%20Trajectory%20Optimization%20for%20Contact-Implicit%20Model%0A%20%20Predictive%20Control%0AAuthor%3A%20Vince%20Kurtz%20and%20Alejandro%20Castro%20and%20Aykut%20%C3%96zg%C3%BCn%20%C3%96nol%20and%20Hai%20Lin%0AAbstract%3A%20%20%20Robots%20must%20make%20and%20break%20contact%20with%20the%20environment%20to%20perform%20useful%0Atasks%2C%20but%20planning%20and%20control%20through%20contact%20remains%20a%20formidable%20challenge.%0AIn%20this%20work%2C%20we%20achieve%20real-time%20contact-implicit%20model%20predictive%20control%0Awith%20a%20surprisingly%20simple%20method%3A%20inverse%20dynamics%20trajectory%20optimization.%0AWhile%20trajectory%20optimization%20with%20inverse%20dynamics%20is%20not%20new%2C%20we%20introduce%20a%0Aseries%20of%20incremental%20innovations%20that%20collectively%20enable%20fast%20model%0Apredictive%20control%20on%20a%20variety%20of%20challenging%20manipulation%20and%20locomotion%0Atasks.%20We%20implement%20these%20innovations%20in%20an%20open-source%20solver%20and%20present%0Asimulation%20examples%20to%20support%20the%20effectiveness%20of%20the%20proposed%20approach.%0AAdditionally%2C%20we%20demonstrate%20contact-implicit%20model%20predictive%20control%20on%0Ahardware%20at%20over%20100%20Hz%20for%20a%2020-degree-of-freedom%20bi-manual%20manipulation%20task.%0AVideo%20and%20code%20are%20available%20at%20https%3A//idto.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.01813v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInverse%2520Dynamics%2520Trajectory%2520Optimization%2520for%2520Contact-Implicit%2520Model%250A%2520%2520Predictive%2520Control%26entry.906535625%3DVince%2520Kurtz%2520and%2520Alejandro%2520Castro%2520and%2520Aykut%2520%25C3%2596zg%25C3%25BCn%2520%25C3%2596nol%2520and%2520Hai%2520Lin%26entry.1292438233%3D%2520%2520Robots%2520must%2520make%2520and%2520break%2520contact%2520with%2520the%2520environment%2520to%2520perform%2520useful%250Atasks%252C%2520but%2520planning%2520and%2520control%2520through%2520contact%2520remains%2520a%2520formidable%2520challenge.%250AIn%2520this%2520work%252C%2520we%2520achieve%2520real-time%2520contact-implicit%2520model%2520predictive%2520control%250Awith%2520a%2520surprisingly%2520simple%2520method%253A%2520inverse%2520dynamics%2520trajectory%2520optimization.%250AWhile%2520trajectory%2520optimization%2520with%2520inverse%2520dynamics%2520is%2520not%2520new%252C%2520we%2520introduce%2520a%250Aseries%2520of%2520incremental%2520innovations%2520that%2520collectively%2520enable%2520fast%2520model%250Apredictive%2520control%2520on%2520a%2520variety%2520of%2520challenging%2520manipulation%2520and%2520locomotion%250Atasks.%2520We%2520implement%2520these%2520innovations%2520in%2520an%2520open-source%2520solver%2520and%2520present%250Asimulation%2520examples%2520to%2520support%2520the%2520effectiveness%2520of%2520the%2520proposed%2520approach.%250AAdditionally%252C%2520we%2520demonstrate%2520contact-implicit%2520model%2520predictive%2520control%2520on%250Ahardware%2520at%2520over%2520100%2520Hz%2520for%2520a%252020-degree-of-freedom%2520bi-manual%2520manipulation%2520task.%250AVideo%2520and%2520code%2520are%2520available%2520at%2520https%253A//idto.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.01813v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inverse%20Dynamics%20Trajectory%20Optimization%20for%20Contact-Implicit%20Model%0A%20%20Predictive%20Control&entry.906535625=Vince%20Kurtz%20and%20Alejandro%20Castro%20and%20Aykut%20%C3%96zg%C3%BCn%20%C3%96nol%20and%20Hai%20Lin&entry.1292438233=%20%20Robots%20must%20make%20and%20break%20contact%20with%20the%20environment%20to%20perform%20useful%0Atasks%2C%20but%20planning%20and%20control%20through%20contact%20remains%20a%20formidable%20challenge.%0AIn%20this%20work%2C%20we%20achieve%20real-time%20contact-implicit%20model%20predictive%20control%0Awith%20a%20surprisingly%20simple%20method%3A%20inverse%20dynamics%20trajectory%20optimization.%0AWhile%20trajectory%20optimization%20with%20inverse%20dynamics%20is%20not%20new%2C%20we%20introduce%20a%0Aseries%20of%20incremental%20innovations%20that%20collectively%20enable%20fast%20model%0Apredictive%20control%20on%20a%20variety%20of%20challenging%20manipulation%20and%20locomotion%0Atasks.%20We%20implement%20these%20innovations%20in%20an%20open-source%20solver%20and%20present%0Asimulation%20examples%20to%20support%20the%20effectiveness%20of%20the%20proposed%20approach.%0AAdditionally%2C%20we%20demonstrate%20contact-implicit%20model%20predictive%20control%20on%0Ahardware%20at%20over%20100%20Hz%20for%20a%2020-degree-of-freedom%20bi-manual%20manipulation%20task.%0AVideo%20and%20code%20are%20available%20at%20https%3A//idto.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.01813v3&entry.124074799=Read"},
{"title": "Active Data Curation Effectively Distills Large-Scale Multimodal Models", "author": "Vishaal Udandarao and Nikhil Parthasarathy and Muhammad Ferjad Naeem and Talfan Evans and Samuel Albanie and Federico Tombari and Yongqin Xian and Alessio Tonioni and Olivier J. H\u00e9naff", "abstract": "  Knowledge distillation (KD) is the de facto standard for compressing\nlarge-scale models into smaller ones. Prior works have explored ever more\ncomplex KD strategies involving different objective functions,\nteacher-ensembles, and weight inheritance. In this work we explore an\nalternative, yet simple approach -- active data curation as effective\ndistillation for contrastive multimodal pretraining. Our simple online batch\nselection method, ACID, outperforms strong KD baselines across various model-,\ndata- and compute-configurations. Further, we find such an active data curation\nstrategy to in fact be complementary to standard KD, and can be effectively\ncombined to train highly performant inference-efficient models. Our simple and\nscalable pretraining framework, ACED, achieves state-of-the-art results across\n27 zero-shot classification and retrieval tasks with upto 11% less inference\nFLOPs. We further demonstrate that our ACED models yield strong vision-encoders\nfor training generative multimodal models in the LiT-Decoder setting,\noutperforming larger vision encoders for image-captioning and visual\nquestion-answering tasks.\n", "link": "http://arxiv.org/abs/2411.18674v2", "date": "2025-05-05", "relevancy": 2.1874, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5547}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5413}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Data%20Curation%20Effectively%20Distills%20Large-Scale%20Multimodal%20Models&body=Title%3A%20Active%20Data%20Curation%20Effectively%20Distills%20Large-Scale%20Multimodal%20Models%0AAuthor%3A%20Vishaal%20Udandarao%20and%20Nikhil%20Parthasarathy%20and%20Muhammad%20Ferjad%20Naeem%20and%20Talfan%20Evans%20and%20Samuel%20Albanie%20and%20Federico%20Tombari%20and%20Yongqin%20Xian%20and%20Alessio%20Tonioni%20and%20Olivier%20J.%20H%C3%A9naff%0AAbstract%3A%20%20%20Knowledge%20distillation%20%28KD%29%20is%20the%20de%20facto%20standard%20for%20compressing%0Alarge-scale%20models%20into%20smaller%20ones.%20Prior%20works%20have%20explored%20ever%20more%0Acomplex%20KD%20strategies%20involving%20different%20objective%20functions%2C%0Ateacher-ensembles%2C%20and%20weight%20inheritance.%20In%20this%20work%20we%20explore%20an%0Aalternative%2C%20yet%20simple%20approach%20--%20active%20data%20curation%20as%20effective%0Adistillation%20for%20contrastive%20multimodal%20pretraining.%20Our%20simple%20online%20batch%0Aselection%20method%2C%20ACID%2C%20outperforms%20strong%20KD%20baselines%20across%20various%20model-%2C%0Adata-%20and%20compute-configurations.%20Further%2C%20we%20find%20such%20an%20active%20data%20curation%0Astrategy%20to%20in%20fact%20be%20complementary%20to%20standard%20KD%2C%20and%20can%20be%20effectively%0Acombined%20to%20train%20highly%20performant%20inference-efficient%20models.%20Our%20simple%20and%0Ascalable%20pretraining%20framework%2C%20ACED%2C%20achieves%20state-of-the-art%20results%20across%0A27%20zero-shot%20classification%20and%20retrieval%20tasks%20with%20upto%2011%25%20less%20inference%0AFLOPs.%20We%20further%20demonstrate%20that%20our%20ACED%20models%20yield%20strong%20vision-encoders%0Afor%20training%20generative%20multimodal%20models%20in%20the%20LiT-Decoder%20setting%2C%0Aoutperforming%20larger%20vision%20encoders%20for%20image-captioning%20and%20visual%0Aquestion-answering%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18674v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Data%2520Curation%2520Effectively%2520Distills%2520Large-Scale%2520Multimodal%2520Models%26entry.906535625%3DVishaal%2520Udandarao%2520and%2520Nikhil%2520Parthasarathy%2520and%2520Muhammad%2520Ferjad%2520Naeem%2520and%2520Talfan%2520Evans%2520and%2520Samuel%2520Albanie%2520and%2520Federico%2520Tombari%2520and%2520Yongqin%2520Xian%2520and%2520Alessio%2520Tonioni%2520and%2520Olivier%2520J.%2520H%25C3%25A9naff%26entry.1292438233%3D%2520%2520Knowledge%2520distillation%2520%2528KD%2529%2520is%2520the%2520de%2520facto%2520standard%2520for%2520compressing%250Alarge-scale%2520models%2520into%2520smaller%2520ones.%2520Prior%2520works%2520have%2520explored%2520ever%2520more%250Acomplex%2520KD%2520strategies%2520involving%2520different%2520objective%2520functions%252C%250Ateacher-ensembles%252C%2520and%2520weight%2520inheritance.%2520In%2520this%2520work%2520we%2520explore%2520an%250Aalternative%252C%2520yet%2520simple%2520approach%2520--%2520active%2520data%2520curation%2520as%2520effective%250Adistillation%2520for%2520contrastive%2520multimodal%2520pretraining.%2520Our%2520simple%2520online%2520batch%250Aselection%2520method%252C%2520ACID%252C%2520outperforms%2520strong%2520KD%2520baselines%2520across%2520various%2520model-%252C%250Adata-%2520and%2520compute-configurations.%2520Further%252C%2520we%2520find%2520such%2520an%2520active%2520data%2520curation%250Astrategy%2520to%2520in%2520fact%2520be%2520complementary%2520to%2520standard%2520KD%252C%2520and%2520can%2520be%2520effectively%250Acombined%2520to%2520train%2520highly%2520performant%2520inference-efficient%2520models.%2520Our%2520simple%2520and%250Ascalable%2520pretraining%2520framework%252C%2520ACED%252C%2520achieves%2520state-of-the-art%2520results%2520across%250A27%2520zero-shot%2520classification%2520and%2520retrieval%2520tasks%2520with%2520upto%252011%2525%2520less%2520inference%250AFLOPs.%2520We%2520further%2520demonstrate%2520that%2520our%2520ACED%2520models%2520yield%2520strong%2520vision-encoders%250Afor%2520training%2520generative%2520multimodal%2520models%2520in%2520the%2520LiT-Decoder%2520setting%252C%250Aoutperforming%2520larger%2520vision%2520encoders%2520for%2520image-captioning%2520and%2520visual%250Aquestion-answering%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18674v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Data%20Curation%20Effectively%20Distills%20Large-Scale%20Multimodal%20Models&entry.906535625=Vishaal%20Udandarao%20and%20Nikhil%20Parthasarathy%20and%20Muhammad%20Ferjad%20Naeem%20and%20Talfan%20Evans%20and%20Samuel%20Albanie%20and%20Federico%20Tombari%20and%20Yongqin%20Xian%20and%20Alessio%20Tonioni%20and%20Olivier%20J.%20H%C3%A9naff&entry.1292438233=%20%20Knowledge%20distillation%20%28KD%29%20is%20the%20de%20facto%20standard%20for%20compressing%0Alarge-scale%20models%20into%20smaller%20ones.%20Prior%20works%20have%20explored%20ever%20more%0Acomplex%20KD%20strategies%20involving%20different%20objective%20functions%2C%0Ateacher-ensembles%2C%20and%20weight%20inheritance.%20In%20this%20work%20we%20explore%20an%0Aalternative%2C%20yet%20simple%20approach%20--%20active%20data%20curation%20as%20effective%0Adistillation%20for%20contrastive%20multimodal%20pretraining.%20Our%20simple%20online%20batch%0Aselection%20method%2C%20ACID%2C%20outperforms%20strong%20KD%20baselines%20across%20various%20model-%2C%0Adata-%20and%20compute-configurations.%20Further%2C%20we%20find%20such%20an%20active%20data%20curation%0Astrategy%20to%20in%20fact%20be%20complementary%20to%20standard%20KD%2C%20and%20can%20be%20effectively%0Acombined%20to%20train%20highly%20performant%20inference-efficient%20models.%20Our%20simple%20and%0Ascalable%20pretraining%20framework%2C%20ACED%2C%20achieves%20state-of-the-art%20results%20across%0A27%20zero-shot%20classification%20and%20retrieval%20tasks%20with%20upto%2011%25%20less%20inference%0AFLOPs.%20We%20further%20demonstrate%20that%20our%20ACED%20models%20yield%20strong%20vision-encoders%0Afor%20training%20generative%20multimodal%20models%20in%20the%20LiT-Decoder%20setting%2C%0Aoutperforming%20larger%20vision%20encoders%20for%20image-captioning%20and%20visual%0Aquestion-answering%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18674v2&entry.124074799=Read"},
{"title": "SAM2MOT: A Novel Paradigm of Multi-Object Tracking by Segmentation", "author": "Junjie Jiang and Zelin Wang and Manqi Zhao and Yin Li and DongSheng Jiang", "abstract": "  Segment Anything 2 (SAM2) enables robust single-object tracking using\nsegmentation. To extend this to multi-object tracking (MOT), we propose\nSAM2MOT, introducing a novel Tracking by Segmentation paradigm. Unlike Tracking\nby Detection or Tracking by Query, SAM2MOT directly generates tracking boxes\nfrom segmentation masks, reducing reliance on detection accuracy. SAM2MOT has\ntwo key advantages: zero-shot generalization, allowing it to work across\ndatasets without fine-tuning, and strong object association, inherited from\nSAM2. To further improve performance, we integrate a trajectory manager system\nfor precise object addition and removal, and a cross-object interaction module\nto handle occlusions. Experiments on DanceTrack, UAVDT, and BDD100K show\nstate-of-the-art results. Notably, SAM2MOT outperforms existing methods on\nDanceTrack by +2.1 HOTA and +4.5 IDF1, highlighting its effectiveness in MOT.\nCode is available at https://github.com/TripleJoy/SAM2MOT.\n", "link": "http://arxiv.org/abs/2504.04519v3", "date": "2025-05-05", "relevancy": 2.171, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5625}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5613}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAM2MOT%3A%20A%20Novel%20Paradigm%20of%20Multi-Object%20Tracking%20by%20Segmentation&body=Title%3A%20SAM2MOT%3A%20A%20Novel%20Paradigm%20of%20Multi-Object%20Tracking%20by%20Segmentation%0AAuthor%3A%20Junjie%20Jiang%20and%20Zelin%20Wang%20and%20Manqi%20Zhao%20and%20Yin%20Li%20and%20DongSheng%20Jiang%0AAbstract%3A%20%20%20Segment%20Anything%202%20%28SAM2%29%20enables%20robust%20single-object%20tracking%20using%0Asegmentation.%20To%20extend%20this%20to%20multi-object%20tracking%20%28MOT%29%2C%20we%20propose%0ASAM2MOT%2C%20introducing%20a%20novel%20Tracking%20by%20Segmentation%20paradigm.%20Unlike%20Tracking%0Aby%20Detection%20or%20Tracking%20by%20Query%2C%20SAM2MOT%20directly%20generates%20tracking%20boxes%0Afrom%20segmentation%20masks%2C%20reducing%20reliance%20on%20detection%20accuracy.%20SAM2MOT%20has%0Atwo%20key%20advantages%3A%20zero-shot%20generalization%2C%20allowing%20it%20to%20work%20across%0Adatasets%20without%20fine-tuning%2C%20and%20strong%20object%20association%2C%20inherited%20from%0ASAM2.%20To%20further%20improve%20performance%2C%20we%20integrate%20a%20trajectory%20manager%20system%0Afor%20precise%20object%20addition%20and%20removal%2C%20and%20a%20cross-object%20interaction%20module%0Ato%20handle%20occlusions.%20Experiments%20on%20DanceTrack%2C%20UAVDT%2C%20and%20BDD100K%20show%0Astate-of-the-art%20results.%20Notably%2C%20SAM2MOT%20outperforms%20existing%20methods%20on%0ADanceTrack%20by%20%2B2.1%20HOTA%20and%20%2B4.5%20IDF1%2C%20highlighting%20its%20effectiveness%20in%20MOT.%0ACode%20is%20available%20at%20https%3A//github.com/TripleJoy/SAM2MOT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.04519v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAM2MOT%253A%2520A%2520Novel%2520Paradigm%2520of%2520Multi-Object%2520Tracking%2520by%2520Segmentation%26entry.906535625%3DJunjie%2520Jiang%2520and%2520Zelin%2520Wang%2520and%2520Manqi%2520Zhao%2520and%2520Yin%2520Li%2520and%2520DongSheng%2520Jiang%26entry.1292438233%3D%2520%2520Segment%2520Anything%25202%2520%2528SAM2%2529%2520enables%2520robust%2520single-object%2520tracking%2520using%250Asegmentation.%2520To%2520extend%2520this%2520to%2520multi-object%2520tracking%2520%2528MOT%2529%252C%2520we%2520propose%250ASAM2MOT%252C%2520introducing%2520a%2520novel%2520Tracking%2520by%2520Segmentation%2520paradigm.%2520Unlike%2520Tracking%250Aby%2520Detection%2520or%2520Tracking%2520by%2520Query%252C%2520SAM2MOT%2520directly%2520generates%2520tracking%2520boxes%250Afrom%2520segmentation%2520masks%252C%2520reducing%2520reliance%2520on%2520detection%2520accuracy.%2520SAM2MOT%2520has%250Atwo%2520key%2520advantages%253A%2520zero-shot%2520generalization%252C%2520allowing%2520it%2520to%2520work%2520across%250Adatasets%2520without%2520fine-tuning%252C%2520and%2520strong%2520object%2520association%252C%2520inherited%2520from%250ASAM2.%2520To%2520further%2520improve%2520performance%252C%2520we%2520integrate%2520a%2520trajectory%2520manager%2520system%250Afor%2520precise%2520object%2520addition%2520and%2520removal%252C%2520and%2520a%2520cross-object%2520interaction%2520module%250Ato%2520handle%2520occlusions.%2520Experiments%2520on%2520DanceTrack%252C%2520UAVDT%252C%2520and%2520BDD100K%2520show%250Astate-of-the-art%2520results.%2520Notably%252C%2520SAM2MOT%2520outperforms%2520existing%2520methods%2520on%250ADanceTrack%2520by%2520%252B2.1%2520HOTA%2520and%2520%252B4.5%2520IDF1%252C%2520highlighting%2520its%2520effectiveness%2520in%2520MOT.%250ACode%2520is%2520available%2520at%2520https%253A//github.com/TripleJoy/SAM2MOT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.04519v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAM2MOT%3A%20A%20Novel%20Paradigm%20of%20Multi-Object%20Tracking%20by%20Segmentation&entry.906535625=Junjie%20Jiang%20and%20Zelin%20Wang%20and%20Manqi%20Zhao%20and%20Yin%20Li%20and%20DongSheng%20Jiang&entry.1292438233=%20%20Segment%20Anything%202%20%28SAM2%29%20enables%20robust%20single-object%20tracking%20using%0Asegmentation.%20To%20extend%20this%20to%20multi-object%20tracking%20%28MOT%29%2C%20we%20propose%0ASAM2MOT%2C%20introducing%20a%20novel%20Tracking%20by%20Segmentation%20paradigm.%20Unlike%20Tracking%0Aby%20Detection%20or%20Tracking%20by%20Query%2C%20SAM2MOT%20directly%20generates%20tracking%20boxes%0Afrom%20segmentation%20masks%2C%20reducing%20reliance%20on%20detection%20accuracy.%20SAM2MOT%20has%0Atwo%20key%20advantages%3A%20zero-shot%20generalization%2C%20allowing%20it%20to%20work%20across%0Adatasets%20without%20fine-tuning%2C%20and%20strong%20object%20association%2C%20inherited%20from%0ASAM2.%20To%20further%20improve%20performance%2C%20we%20integrate%20a%20trajectory%20manager%20system%0Afor%20precise%20object%20addition%20and%20removal%2C%20and%20a%20cross-object%20interaction%20module%0Ato%20handle%20occlusions.%20Experiments%20on%20DanceTrack%2C%20UAVDT%2C%20and%20BDD100K%20show%0Astate-of-the-art%20results.%20Notably%2C%20SAM2MOT%20outperforms%20existing%20methods%20on%0ADanceTrack%20by%20%2B2.1%20HOTA%20and%20%2B4.5%20IDF1%2C%20highlighting%20its%20effectiveness%20in%20MOT.%0ACode%20is%20available%20at%20https%3A//github.com/TripleJoy/SAM2MOT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.04519v3&entry.124074799=Read"},
{"title": "FolAI: Synchronized Foley Sound Generation with Semantic and Temporal\n  Alignment", "author": "Riccardo Fosco Gramaccioni and Christian Marinoni and Emilian Postolache and Marco Comunit\u00e0 and Luca Cosmo and Joshua D. Reiss and Danilo Comminiello", "abstract": "  Traditional sound design workflows rely on manual alignment of audio events\nto visual cues, as in Foley sound design, where everyday actions like footsteps\nor object interactions are recreated to match the on-screen motion. This\nprocess is time-consuming, difficult to scale, and lacks automation tools that\npreserve creative intent. Despite recent advances in vision-to-audio\ngeneration, producing temporally coherent and semantically controllable sound\neffects from video remains a major challenge. To address these limitations, we\nintroduce FolAI, a two-stage generative framework that decouples the when and\nthe what of sound synthesis, i.e., the temporal structure extraction and the\nsemantically guided generation, respectively. In the first stage, we estimate a\nsmooth control signal from the video that captures the motion intensity and\nrhythmic structure over time, serving as a temporal scaffold for the audio. In\nthe second stage, a diffusion-based generative model produces sound effects\nconditioned both on this temporal envelope and on high-level semantic\nembeddings, provided by the user, that define the desired auditory content\n(e.g., material or action type). This modular design enables precise control\nover both timing and timbre, streamlining repetitive tasks while preserving\ncreative flexibility in professional Foley workflows. Results on diverse visual\ncontexts, such as footstep generation and action-specific sonorization,\ndemonstrate that our model reliably produces audio that is temporally aligned\nwith visual motion, semantically consistent with user intent, and perceptually\nrealistic. These findings highlight the potential of FolAI as a controllable\nand modular solution for scalable, high-quality Foley sound synthesis in\nprofessional and interactive settings. Supplementary materials are accessible\non our dedicated demo page at https://ispamm.github.io/FolAI.\n", "link": "http://arxiv.org/abs/2412.15023v3", "date": "2025-05-05", "relevancy": 2.1585, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5591}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5268}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5253}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FolAI%3A%20Synchronized%20Foley%20Sound%20Generation%20with%20Semantic%20and%20Temporal%0A%20%20Alignment&body=Title%3A%20FolAI%3A%20Synchronized%20Foley%20Sound%20Generation%20with%20Semantic%20and%20Temporal%0A%20%20Alignment%0AAuthor%3A%20Riccardo%20Fosco%20Gramaccioni%20and%20Christian%20Marinoni%20and%20Emilian%20Postolache%20and%20Marco%20Comunit%C3%A0%20and%20Luca%20Cosmo%20and%20Joshua%20D.%20Reiss%20and%20Danilo%20Comminiello%0AAbstract%3A%20%20%20Traditional%20sound%20design%20workflows%20rely%20on%20manual%20alignment%20of%20audio%20events%0Ato%20visual%20cues%2C%20as%20in%20Foley%20sound%20design%2C%20where%20everyday%20actions%20like%20footsteps%0Aor%20object%20interactions%20are%20recreated%20to%20match%20the%20on-screen%20motion.%20This%0Aprocess%20is%20time-consuming%2C%20difficult%20to%20scale%2C%20and%20lacks%20automation%20tools%20that%0Apreserve%20creative%20intent.%20Despite%20recent%20advances%20in%20vision-to-audio%0Ageneration%2C%20producing%20temporally%20coherent%20and%20semantically%20controllable%20sound%0Aeffects%20from%20video%20remains%20a%20major%20challenge.%20To%20address%20these%20limitations%2C%20we%0Aintroduce%20FolAI%2C%20a%20two-stage%20generative%20framework%20that%20decouples%20the%20when%20and%0Athe%20what%20of%20sound%20synthesis%2C%20i.e.%2C%20the%20temporal%20structure%20extraction%20and%20the%0Asemantically%20guided%20generation%2C%20respectively.%20In%20the%20first%20stage%2C%20we%20estimate%20a%0Asmooth%20control%20signal%20from%20the%20video%20that%20captures%20the%20motion%20intensity%20and%0Arhythmic%20structure%20over%20time%2C%20serving%20as%20a%20temporal%20scaffold%20for%20the%20audio.%20In%0Athe%20second%20stage%2C%20a%20diffusion-based%20generative%20model%20produces%20sound%20effects%0Aconditioned%20both%20on%20this%20temporal%20envelope%20and%20on%20high-level%20semantic%0Aembeddings%2C%20provided%20by%20the%20user%2C%20that%20define%20the%20desired%20auditory%20content%0A%28e.g.%2C%20material%20or%20action%20type%29.%20This%20modular%20design%20enables%20precise%20control%0Aover%20both%20timing%20and%20timbre%2C%20streamlining%20repetitive%20tasks%20while%20preserving%0Acreative%20flexibility%20in%20professional%20Foley%20workflows.%20Results%20on%20diverse%20visual%0Acontexts%2C%20such%20as%20footstep%20generation%20and%20action-specific%20sonorization%2C%0Ademonstrate%20that%20our%20model%20reliably%20produces%20audio%20that%20is%20temporally%20aligned%0Awith%20visual%20motion%2C%20semantically%20consistent%20with%20user%20intent%2C%20and%20perceptually%0Arealistic.%20These%20findings%20highlight%20the%20potential%20of%20FolAI%20as%20a%20controllable%0Aand%20modular%20solution%20for%20scalable%2C%20high-quality%20Foley%20sound%20synthesis%20in%0Aprofessional%20and%20interactive%20settings.%20Supplementary%20materials%20are%20accessible%0Aon%20our%20dedicated%20demo%20page%20at%20https%3A//ispamm.github.io/FolAI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15023v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFolAI%253A%2520Synchronized%2520Foley%2520Sound%2520Generation%2520with%2520Semantic%2520and%2520Temporal%250A%2520%2520Alignment%26entry.906535625%3DRiccardo%2520Fosco%2520Gramaccioni%2520and%2520Christian%2520Marinoni%2520and%2520Emilian%2520Postolache%2520and%2520Marco%2520Comunit%25C3%25A0%2520and%2520Luca%2520Cosmo%2520and%2520Joshua%2520D.%2520Reiss%2520and%2520Danilo%2520Comminiello%26entry.1292438233%3D%2520%2520Traditional%2520sound%2520design%2520workflows%2520rely%2520on%2520manual%2520alignment%2520of%2520audio%2520events%250Ato%2520visual%2520cues%252C%2520as%2520in%2520Foley%2520sound%2520design%252C%2520where%2520everyday%2520actions%2520like%2520footsteps%250Aor%2520object%2520interactions%2520are%2520recreated%2520to%2520match%2520the%2520on-screen%2520motion.%2520This%250Aprocess%2520is%2520time-consuming%252C%2520difficult%2520to%2520scale%252C%2520and%2520lacks%2520automation%2520tools%2520that%250Apreserve%2520creative%2520intent.%2520Despite%2520recent%2520advances%2520in%2520vision-to-audio%250Ageneration%252C%2520producing%2520temporally%2520coherent%2520and%2520semantically%2520controllable%2520sound%250Aeffects%2520from%2520video%2520remains%2520a%2520major%2520challenge.%2520To%2520address%2520these%2520limitations%252C%2520we%250Aintroduce%2520FolAI%252C%2520a%2520two-stage%2520generative%2520framework%2520that%2520decouples%2520the%2520when%2520and%250Athe%2520what%2520of%2520sound%2520synthesis%252C%2520i.e.%252C%2520the%2520temporal%2520structure%2520extraction%2520and%2520the%250Asemantically%2520guided%2520generation%252C%2520respectively.%2520In%2520the%2520first%2520stage%252C%2520we%2520estimate%2520a%250Asmooth%2520control%2520signal%2520from%2520the%2520video%2520that%2520captures%2520the%2520motion%2520intensity%2520and%250Arhythmic%2520structure%2520over%2520time%252C%2520serving%2520as%2520a%2520temporal%2520scaffold%2520for%2520the%2520audio.%2520In%250Athe%2520second%2520stage%252C%2520a%2520diffusion-based%2520generative%2520model%2520produces%2520sound%2520effects%250Aconditioned%2520both%2520on%2520this%2520temporal%2520envelope%2520and%2520on%2520high-level%2520semantic%250Aembeddings%252C%2520provided%2520by%2520the%2520user%252C%2520that%2520define%2520the%2520desired%2520auditory%2520content%250A%2528e.g.%252C%2520material%2520or%2520action%2520type%2529.%2520This%2520modular%2520design%2520enables%2520precise%2520control%250Aover%2520both%2520timing%2520and%2520timbre%252C%2520streamlining%2520repetitive%2520tasks%2520while%2520preserving%250Acreative%2520flexibility%2520in%2520professional%2520Foley%2520workflows.%2520Results%2520on%2520diverse%2520visual%250Acontexts%252C%2520such%2520as%2520footstep%2520generation%2520and%2520action-specific%2520sonorization%252C%250Ademonstrate%2520that%2520our%2520model%2520reliably%2520produces%2520audio%2520that%2520is%2520temporally%2520aligned%250Awith%2520visual%2520motion%252C%2520semantically%2520consistent%2520with%2520user%2520intent%252C%2520and%2520perceptually%250Arealistic.%2520These%2520findings%2520highlight%2520the%2520potential%2520of%2520FolAI%2520as%2520a%2520controllable%250Aand%2520modular%2520solution%2520for%2520scalable%252C%2520high-quality%2520Foley%2520sound%2520synthesis%2520in%250Aprofessional%2520and%2520interactive%2520settings.%2520Supplementary%2520materials%2520are%2520accessible%250Aon%2520our%2520dedicated%2520demo%2520page%2520at%2520https%253A//ispamm.github.io/FolAI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15023v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FolAI%3A%20Synchronized%20Foley%20Sound%20Generation%20with%20Semantic%20and%20Temporal%0A%20%20Alignment&entry.906535625=Riccardo%20Fosco%20Gramaccioni%20and%20Christian%20Marinoni%20and%20Emilian%20Postolache%20and%20Marco%20Comunit%C3%A0%20and%20Luca%20Cosmo%20and%20Joshua%20D.%20Reiss%20and%20Danilo%20Comminiello&entry.1292438233=%20%20Traditional%20sound%20design%20workflows%20rely%20on%20manual%20alignment%20of%20audio%20events%0Ato%20visual%20cues%2C%20as%20in%20Foley%20sound%20design%2C%20where%20everyday%20actions%20like%20footsteps%0Aor%20object%20interactions%20are%20recreated%20to%20match%20the%20on-screen%20motion.%20This%0Aprocess%20is%20time-consuming%2C%20difficult%20to%20scale%2C%20and%20lacks%20automation%20tools%20that%0Apreserve%20creative%20intent.%20Despite%20recent%20advances%20in%20vision-to-audio%0Ageneration%2C%20producing%20temporally%20coherent%20and%20semantically%20controllable%20sound%0Aeffects%20from%20video%20remains%20a%20major%20challenge.%20To%20address%20these%20limitations%2C%20we%0Aintroduce%20FolAI%2C%20a%20two-stage%20generative%20framework%20that%20decouples%20the%20when%20and%0Athe%20what%20of%20sound%20synthesis%2C%20i.e.%2C%20the%20temporal%20structure%20extraction%20and%20the%0Asemantically%20guided%20generation%2C%20respectively.%20In%20the%20first%20stage%2C%20we%20estimate%20a%0Asmooth%20control%20signal%20from%20the%20video%20that%20captures%20the%20motion%20intensity%20and%0Arhythmic%20structure%20over%20time%2C%20serving%20as%20a%20temporal%20scaffold%20for%20the%20audio.%20In%0Athe%20second%20stage%2C%20a%20diffusion-based%20generative%20model%20produces%20sound%20effects%0Aconditioned%20both%20on%20this%20temporal%20envelope%20and%20on%20high-level%20semantic%0Aembeddings%2C%20provided%20by%20the%20user%2C%20that%20define%20the%20desired%20auditory%20content%0A%28e.g.%2C%20material%20or%20action%20type%29.%20This%20modular%20design%20enables%20precise%20control%0Aover%20both%20timing%20and%20timbre%2C%20streamlining%20repetitive%20tasks%20while%20preserving%0Acreative%20flexibility%20in%20professional%20Foley%20workflows.%20Results%20on%20diverse%20visual%0Acontexts%2C%20such%20as%20footstep%20generation%20and%20action-specific%20sonorization%2C%0Ademonstrate%20that%20our%20model%20reliably%20produces%20audio%20that%20is%20temporally%20aligned%0Awith%20visual%20motion%2C%20semantically%20consistent%20with%20user%20intent%2C%20and%20perceptually%0Arealistic.%20These%20findings%20highlight%20the%20potential%20of%20FolAI%20as%20a%20controllable%0Aand%20modular%20solution%20for%20scalable%2C%20high-quality%20Foley%20sound%20synthesis%20in%0Aprofessional%20and%20interactive%20settings.%20Supplementary%20materials%20are%20accessible%0Aon%20our%20dedicated%20demo%20page%20at%20https%3A//ispamm.github.io/FolAI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15023v3&entry.124074799=Read"},
{"title": "CHOSEN: Contrastive Hypothesis Selection for Multi-View Depth Refinement", "author": "Di Qiu and Yinda Zhang and Thabo Beeler and Vladimir Tankovich and Christian H\u00e4ne and Sean Fanello and Christoph Rhemann and Sergio Orts Escolano", "abstract": "  We propose CHOSEN, a simple yet flexible, robust and effective multi-view\ndepth refinement framework. It can be employed in any existing multi-view\nstereo pipeline, with straightforward generalization capability for different\nmulti-view capture systems such as camera relative positioning and lenses.\nGiven an initial depth estimation, CHOSEN iteratively re-samples and selects\nthe best hypotheses, and automatically adapts to different metric or intrinsic\nscales determined by the capture system. The key to our approach is the\napplication of contrastive learning in an appropriate solution space and a\ncarefully designed hypothesis feature, based on which positive and negative\nhypotheses can be effectively distinguished. Integrated in a simple baseline\nmulti-view stereo pipeline, CHOSEN delivers impressive quality in terms of\ndepth and normal accuracy compared to many current deep learning based\nmulti-view stereo pipelines.\n", "link": "http://arxiv.org/abs/2404.02225v2", "date": "2025-05-05", "relevancy": 2.1546, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5738}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5316}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5316}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CHOSEN%3A%20Contrastive%20Hypothesis%20Selection%20for%20Multi-View%20Depth%20Refinement&body=Title%3A%20CHOSEN%3A%20Contrastive%20Hypothesis%20Selection%20for%20Multi-View%20Depth%20Refinement%0AAuthor%3A%20Di%20Qiu%20and%20Yinda%20Zhang%20and%20Thabo%20Beeler%20and%20Vladimir%20Tankovich%20and%20Christian%20H%C3%A4ne%20and%20Sean%20Fanello%20and%20Christoph%20Rhemann%20and%20Sergio%20Orts%20Escolano%0AAbstract%3A%20%20%20We%20propose%20CHOSEN%2C%20a%20simple%20yet%20flexible%2C%20robust%20and%20effective%20multi-view%0Adepth%20refinement%20framework.%20It%20can%20be%20employed%20in%20any%20existing%20multi-view%0Astereo%20pipeline%2C%20with%20straightforward%20generalization%20capability%20for%20different%0Amulti-view%20capture%20systems%20such%20as%20camera%20relative%20positioning%20and%20lenses.%0AGiven%20an%20initial%20depth%20estimation%2C%20CHOSEN%20iteratively%20re-samples%20and%20selects%0Athe%20best%20hypotheses%2C%20and%20automatically%20adapts%20to%20different%20metric%20or%20intrinsic%0Ascales%20determined%20by%20the%20capture%20system.%20The%20key%20to%20our%20approach%20is%20the%0Aapplication%20of%20contrastive%20learning%20in%20an%20appropriate%20solution%20space%20and%20a%0Acarefully%20designed%20hypothesis%20feature%2C%20based%20on%20which%20positive%20and%20negative%0Ahypotheses%20can%20be%20effectively%20distinguished.%20Integrated%20in%20a%20simple%20baseline%0Amulti-view%20stereo%20pipeline%2C%20CHOSEN%20delivers%20impressive%20quality%20in%20terms%20of%0Adepth%20and%20normal%20accuracy%20compared%20to%20many%20current%20deep%20learning%20based%0Amulti-view%20stereo%20pipelines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02225v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCHOSEN%253A%2520Contrastive%2520Hypothesis%2520Selection%2520for%2520Multi-View%2520Depth%2520Refinement%26entry.906535625%3DDi%2520Qiu%2520and%2520Yinda%2520Zhang%2520and%2520Thabo%2520Beeler%2520and%2520Vladimir%2520Tankovich%2520and%2520Christian%2520H%25C3%25A4ne%2520and%2520Sean%2520Fanello%2520and%2520Christoph%2520Rhemann%2520and%2520Sergio%2520Orts%2520Escolano%26entry.1292438233%3D%2520%2520We%2520propose%2520CHOSEN%252C%2520a%2520simple%2520yet%2520flexible%252C%2520robust%2520and%2520effective%2520multi-view%250Adepth%2520refinement%2520framework.%2520It%2520can%2520be%2520employed%2520in%2520any%2520existing%2520multi-view%250Astereo%2520pipeline%252C%2520with%2520straightforward%2520generalization%2520capability%2520for%2520different%250Amulti-view%2520capture%2520systems%2520such%2520as%2520camera%2520relative%2520positioning%2520and%2520lenses.%250AGiven%2520an%2520initial%2520depth%2520estimation%252C%2520CHOSEN%2520iteratively%2520re-samples%2520and%2520selects%250Athe%2520best%2520hypotheses%252C%2520and%2520automatically%2520adapts%2520to%2520different%2520metric%2520or%2520intrinsic%250Ascales%2520determined%2520by%2520the%2520capture%2520system.%2520The%2520key%2520to%2520our%2520approach%2520is%2520the%250Aapplication%2520of%2520contrastive%2520learning%2520in%2520an%2520appropriate%2520solution%2520space%2520and%2520a%250Acarefully%2520designed%2520hypothesis%2520feature%252C%2520based%2520on%2520which%2520positive%2520and%2520negative%250Ahypotheses%2520can%2520be%2520effectively%2520distinguished.%2520Integrated%2520in%2520a%2520simple%2520baseline%250Amulti-view%2520stereo%2520pipeline%252C%2520CHOSEN%2520delivers%2520impressive%2520quality%2520in%2520terms%2520of%250Adepth%2520and%2520normal%2520accuracy%2520compared%2520to%2520many%2520current%2520deep%2520learning%2520based%250Amulti-view%2520stereo%2520pipelines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.02225v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CHOSEN%3A%20Contrastive%20Hypothesis%20Selection%20for%20Multi-View%20Depth%20Refinement&entry.906535625=Di%20Qiu%20and%20Yinda%20Zhang%20and%20Thabo%20Beeler%20and%20Vladimir%20Tankovich%20and%20Christian%20H%C3%A4ne%20and%20Sean%20Fanello%20and%20Christoph%20Rhemann%20and%20Sergio%20Orts%20Escolano&entry.1292438233=%20%20We%20propose%20CHOSEN%2C%20a%20simple%20yet%20flexible%2C%20robust%20and%20effective%20multi-view%0Adepth%20refinement%20framework.%20It%20can%20be%20employed%20in%20any%20existing%20multi-view%0Astereo%20pipeline%2C%20with%20straightforward%20generalization%20capability%20for%20different%0Amulti-view%20capture%20systems%20such%20as%20camera%20relative%20positioning%20and%20lenses.%0AGiven%20an%20initial%20depth%20estimation%2C%20CHOSEN%20iteratively%20re-samples%20and%20selects%0Athe%20best%20hypotheses%2C%20and%20automatically%20adapts%20to%20different%20metric%20or%20intrinsic%0Ascales%20determined%20by%20the%20capture%20system.%20The%20key%20to%20our%20approach%20is%20the%0Aapplication%20of%20contrastive%20learning%20in%20an%20appropriate%20solution%20space%20and%20a%0Acarefully%20designed%20hypothesis%20feature%2C%20based%20on%20which%20positive%20and%20negative%0Ahypotheses%20can%20be%20effectively%20distinguished.%20Integrated%20in%20a%20simple%20baseline%0Amulti-view%20stereo%20pipeline%2C%20CHOSEN%20delivers%20impressive%20quality%20in%20terms%20of%0Adepth%20and%20normal%20accuracy%20compared%20to%20many%20current%20deep%20learning%20based%0Amulti-view%20stereo%20pipelines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02225v2&entry.124074799=Read"},
{"title": "VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models\n  with Reinforcement Learning", "author": "Haozhe Wang and Chao Qu and Zuming Huang and Wei Chu and Fangzhen Lin and Wenhu Chen", "abstract": "  Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated\ngreat potential in solving challenging problems through explicit reflection.\nThey significantly outperform the best fast-thinking models, such as GPT-4o, on\nvarious math and science benchmarks. However, their multimodal reasoning\ncapabilities remain on par with fast-thinking models. For instance, GPT-o1's\nperformance on benchmarks like MathVista, MathVerse, and MathVision is similar\nto fast-thinking models. In this paper, we aim to enhance the slow-thinking\ncapabilities of vision-language models using reinforcement learning (without\nrelying on distillation) to advance the state of the art. First, we adapt the\nGRPO algorithm with a novel technique called Selective Sample Replay (SSR) to\naddress the vanishing advantages problem. While this approach yields strong\nperformance, the resulting RL-trained models exhibit limited self-reflection or\nself-verification. To further encourage slow-thinking, we introduce Forced\nRethinking, which appends a rethinking trigger token to the end of rollouts in\nRL training, explicitly enforcing a self-reflection reasoning step. By\ncombining these two techniques, our model, VL-Rethinker, advances\nstate-of-the-art scores on MathVista, MathVerse to achieve 80.4%, 63.5%\nrespectively. VL-Rethinker also achieves open-source SoTA on multi-disciplinary\nbenchmarks such as MathVision, MMMU-Pro, EMMA, and MEGA-Bench, narrowing the\ngap with OpenAI-o1. Our empirical results show the effectiveness of our\napproaches.\n", "link": "http://arxiv.org/abs/2504.08837v2", "date": "2025-05-05", "relevancy": 2.1521, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5406}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5406}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5251}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VL-Rethinker%3A%20Incentivizing%20Self-Reflection%20of%20Vision-Language%20Models%0A%20%20with%20Reinforcement%20Learning&body=Title%3A%20VL-Rethinker%3A%20Incentivizing%20Self-Reflection%20of%20Vision-Language%20Models%0A%20%20with%20Reinforcement%20Learning%0AAuthor%3A%20Haozhe%20Wang%20and%20Chao%20Qu%20and%20Zuming%20Huang%20and%20Wei%20Chu%20and%20Fangzhen%20Lin%20and%20Wenhu%20Chen%0AAbstract%3A%20%20%20Recently%2C%20slow-thinking%20systems%20like%20GPT-o1%20and%20DeepSeek-R1%20have%20demonstrated%0Agreat%20potential%20in%20solving%20challenging%20problems%20through%20explicit%20reflection.%0AThey%20significantly%20outperform%20the%20best%20fast-thinking%20models%2C%20such%20as%20GPT-4o%2C%20on%0Avarious%20math%20and%20science%20benchmarks.%20However%2C%20their%20multimodal%20reasoning%0Acapabilities%20remain%20on%20par%20with%20fast-thinking%20models.%20For%20instance%2C%20GPT-o1%27s%0Aperformance%20on%20benchmarks%20like%20MathVista%2C%20MathVerse%2C%20and%20MathVision%20is%20similar%0Ato%20fast-thinking%20models.%20In%20this%20paper%2C%20we%20aim%20to%20enhance%20the%20slow-thinking%0Acapabilities%20of%20vision-language%20models%20using%20reinforcement%20learning%20%28without%0Arelying%20on%20distillation%29%20to%20advance%20the%20state%20of%20the%20art.%20First%2C%20we%20adapt%20the%0AGRPO%20algorithm%20with%20a%20novel%20technique%20called%20Selective%20Sample%20Replay%20%28SSR%29%20to%0Aaddress%20the%20vanishing%20advantages%20problem.%20While%20this%20approach%20yields%20strong%0Aperformance%2C%20the%20resulting%20RL-trained%20models%20exhibit%20limited%20self-reflection%20or%0Aself-verification.%20To%20further%20encourage%20slow-thinking%2C%20we%20introduce%20Forced%0ARethinking%2C%20which%20appends%20a%20rethinking%20trigger%20token%20to%20the%20end%20of%20rollouts%20in%0ARL%20training%2C%20explicitly%20enforcing%20a%20self-reflection%20reasoning%20step.%20By%0Acombining%20these%20two%20techniques%2C%20our%20model%2C%20VL-Rethinker%2C%20advances%0Astate-of-the-art%20scores%20on%20MathVista%2C%20MathVerse%20to%20achieve%2080.4%25%2C%2063.5%25%0Arespectively.%20VL-Rethinker%20also%20achieves%20open-source%20SoTA%20on%20multi-disciplinary%0Abenchmarks%20such%20as%20MathVision%2C%20MMMU-Pro%2C%20EMMA%2C%20and%20MEGA-Bench%2C%20narrowing%20the%0Agap%20with%20OpenAI-o1.%20Our%20empirical%20results%20show%20the%20effectiveness%20of%20our%0Aapproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.08837v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVL-Rethinker%253A%2520Incentivizing%2520Self-Reflection%2520of%2520Vision-Language%2520Models%250A%2520%2520with%2520Reinforcement%2520Learning%26entry.906535625%3DHaozhe%2520Wang%2520and%2520Chao%2520Qu%2520and%2520Zuming%2520Huang%2520and%2520Wei%2520Chu%2520and%2520Fangzhen%2520Lin%2520and%2520Wenhu%2520Chen%26entry.1292438233%3D%2520%2520Recently%252C%2520slow-thinking%2520systems%2520like%2520GPT-o1%2520and%2520DeepSeek-R1%2520have%2520demonstrated%250Agreat%2520potential%2520in%2520solving%2520challenging%2520problems%2520through%2520explicit%2520reflection.%250AThey%2520significantly%2520outperform%2520the%2520best%2520fast-thinking%2520models%252C%2520such%2520as%2520GPT-4o%252C%2520on%250Avarious%2520math%2520and%2520science%2520benchmarks.%2520However%252C%2520their%2520multimodal%2520reasoning%250Acapabilities%2520remain%2520on%2520par%2520with%2520fast-thinking%2520models.%2520For%2520instance%252C%2520GPT-o1%2527s%250Aperformance%2520on%2520benchmarks%2520like%2520MathVista%252C%2520MathVerse%252C%2520and%2520MathVision%2520is%2520similar%250Ato%2520fast-thinking%2520models.%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520enhance%2520the%2520slow-thinking%250Acapabilities%2520of%2520vision-language%2520models%2520using%2520reinforcement%2520learning%2520%2528without%250Arelying%2520on%2520distillation%2529%2520to%2520advance%2520the%2520state%2520of%2520the%2520art.%2520First%252C%2520we%2520adapt%2520the%250AGRPO%2520algorithm%2520with%2520a%2520novel%2520technique%2520called%2520Selective%2520Sample%2520Replay%2520%2528SSR%2529%2520to%250Aaddress%2520the%2520vanishing%2520advantages%2520problem.%2520While%2520this%2520approach%2520yields%2520strong%250Aperformance%252C%2520the%2520resulting%2520RL-trained%2520models%2520exhibit%2520limited%2520self-reflection%2520or%250Aself-verification.%2520To%2520further%2520encourage%2520slow-thinking%252C%2520we%2520introduce%2520Forced%250ARethinking%252C%2520which%2520appends%2520a%2520rethinking%2520trigger%2520token%2520to%2520the%2520end%2520of%2520rollouts%2520in%250ARL%2520training%252C%2520explicitly%2520enforcing%2520a%2520self-reflection%2520reasoning%2520step.%2520By%250Acombining%2520these%2520two%2520techniques%252C%2520our%2520model%252C%2520VL-Rethinker%252C%2520advances%250Astate-of-the-art%2520scores%2520on%2520MathVista%252C%2520MathVerse%2520to%2520achieve%252080.4%2525%252C%252063.5%2525%250Arespectively.%2520VL-Rethinker%2520also%2520achieves%2520open-source%2520SoTA%2520on%2520multi-disciplinary%250Abenchmarks%2520such%2520as%2520MathVision%252C%2520MMMU-Pro%252C%2520EMMA%252C%2520and%2520MEGA-Bench%252C%2520narrowing%2520the%250Agap%2520with%2520OpenAI-o1.%2520Our%2520empirical%2520results%2520show%2520the%2520effectiveness%2520of%2520our%250Aapproaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.08837v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VL-Rethinker%3A%20Incentivizing%20Self-Reflection%20of%20Vision-Language%20Models%0A%20%20with%20Reinforcement%20Learning&entry.906535625=Haozhe%20Wang%20and%20Chao%20Qu%20and%20Zuming%20Huang%20and%20Wei%20Chu%20and%20Fangzhen%20Lin%20and%20Wenhu%20Chen&entry.1292438233=%20%20Recently%2C%20slow-thinking%20systems%20like%20GPT-o1%20and%20DeepSeek-R1%20have%20demonstrated%0Agreat%20potential%20in%20solving%20challenging%20problems%20through%20explicit%20reflection.%0AThey%20significantly%20outperform%20the%20best%20fast-thinking%20models%2C%20such%20as%20GPT-4o%2C%20on%0Avarious%20math%20and%20science%20benchmarks.%20However%2C%20their%20multimodal%20reasoning%0Acapabilities%20remain%20on%20par%20with%20fast-thinking%20models.%20For%20instance%2C%20GPT-o1%27s%0Aperformance%20on%20benchmarks%20like%20MathVista%2C%20MathVerse%2C%20and%20MathVision%20is%20similar%0Ato%20fast-thinking%20models.%20In%20this%20paper%2C%20we%20aim%20to%20enhance%20the%20slow-thinking%0Acapabilities%20of%20vision-language%20models%20using%20reinforcement%20learning%20%28without%0Arelying%20on%20distillation%29%20to%20advance%20the%20state%20of%20the%20art.%20First%2C%20we%20adapt%20the%0AGRPO%20algorithm%20with%20a%20novel%20technique%20called%20Selective%20Sample%20Replay%20%28SSR%29%20to%0Aaddress%20the%20vanishing%20advantages%20problem.%20While%20this%20approach%20yields%20strong%0Aperformance%2C%20the%20resulting%20RL-trained%20models%20exhibit%20limited%20self-reflection%20or%0Aself-verification.%20To%20further%20encourage%20slow-thinking%2C%20we%20introduce%20Forced%0ARethinking%2C%20which%20appends%20a%20rethinking%20trigger%20token%20to%20the%20end%20of%20rollouts%20in%0ARL%20training%2C%20explicitly%20enforcing%20a%20self-reflection%20reasoning%20step.%20By%0Acombining%20these%20two%20techniques%2C%20our%20model%2C%20VL-Rethinker%2C%20advances%0Astate-of-the-art%20scores%20on%20MathVista%2C%20MathVerse%20to%20achieve%2080.4%25%2C%2063.5%25%0Arespectively.%20VL-Rethinker%20also%20achieves%20open-source%20SoTA%20on%20multi-disciplinary%0Abenchmarks%20such%20as%20MathVision%2C%20MMMU-Pro%2C%20EMMA%2C%20and%20MEGA-Bench%2C%20narrowing%20the%0Agap%20with%20OpenAI-o1.%20Our%20empirical%20results%20show%20the%20effectiveness%20of%20our%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.08837v2&entry.124074799=Read"},
{"title": "Advancing Generalizable Tumor Segmentation with Anomaly-Aware\n  Open-Vocabulary Attention Maps and Frozen Foundation Diffusion Models", "author": "Yankai Jiang and Peng Zhang and Donglin Yang and Yuan Tian and Hai Lin and Xiaosong Wang", "abstract": "  We explore Generalizable Tumor Segmentation, aiming to train a single model\nfor zero-shot tumor segmentation across diverse anatomical regions. Existing\nmethods face limitations related to segmentation quality, scalability, and the\nrange of applicable imaging modalities. In this paper, we uncover the potential\nof the internal representations within frozen medical foundation diffusion\nmodels as highly efficient zero-shot learners for tumor segmentation by\nintroducing a novel framework named DiffuGTS. DiffuGTS creates anomaly-aware\nopen-vocabulary attention maps based on text prompts to enable generalizable\nanomaly segmentation without being restricted by a predefined training category\nlist. To further improve and refine anomaly segmentation masks, DiffuGTS\nleverages the diffusion model, transforming pathological regions into\nhigh-quality pseudo-healthy counterparts through latent space inpainting, and\napplies a novel pixel-level and feature-level residual learning approach,\nresulting in segmentation masks with significantly enhanced quality and\ngeneralization. Comprehensive experiments on four datasets and seven tumor\ncategories demonstrate the superior performance of our method, surpassing\ncurrent state-of-the-art models across multiple zero-shot settings. Codes are\navailable at https://github.com/Yankai96/DiffuGTS.\n", "link": "http://arxiv.org/abs/2505.02753v1", "date": "2025-05-05", "relevancy": 2.1414, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5426}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5339}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5286}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Generalizable%20Tumor%20Segmentation%20with%20Anomaly-Aware%0A%20%20Open-Vocabulary%20Attention%20Maps%20and%20Frozen%20Foundation%20Diffusion%20Models&body=Title%3A%20Advancing%20Generalizable%20Tumor%20Segmentation%20with%20Anomaly-Aware%0A%20%20Open-Vocabulary%20Attention%20Maps%20and%20Frozen%20Foundation%20Diffusion%20Models%0AAuthor%3A%20Yankai%20Jiang%20and%20Peng%20Zhang%20and%20Donglin%20Yang%20and%20Yuan%20Tian%20and%20Hai%20Lin%20and%20Xiaosong%20Wang%0AAbstract%3A%20%20%20We%20explore%20Generalizable%20Tumor%20Segmentation%2C%20aiming%20to%20train%20a%20single%20model%0Afor%20zero-shot%20tumor%20segmentation%20across%20diverse%20anatomical%20regions.%20Existing%0Amethods%20face%20limitations%20related%20to%20segmentation%20quality%2C%20scalability%2C%20and%20the%0Arange%20of%20applicable%20imaging%20modalities.%20In%20this%20paper%2C%20we%20uncover%20the%20potential%0Aof%20the%20internal%20representations%20within%20frozen%20medical%20foundation%20diffusion%0Amodels%20as%20highly%20efficient%20zero-shot%20learners%20for%20tumor%20segmentation%20by%0Aintroducing%20a%20novel%20framework%20named%20DiffuGTS.%20DiffuGTS%20creates%20anomaly-aware%0Aopen-vocabulary%20attention%20maps%20based%20on%20text%20prompts%20to%20enable%20generalizable%0Aanomaly%20segmentation%20without%20being%20restricted%20by%20a%20predefined%20training%20category%0Alist.%20To%20further%20improve%20and%20refine%20anomaly%20segmentation%20masks%2C%20DiffuGTS%0Aleverages%20the%20diffusion%20model%2C%20transforming%20pathological%20regions%20into%0Ahigh-quality%20pseudo-healthy%20counterparts%20through%20latent%20space%20inpainting%2C%20and%0Aapplies%20a%20novel%20pixel-level%20and%20feature-level%20residual%20learning%20approach%2C%0Aresulting%20in%20segmentation%20masks%20with%20significantly%20enhanced%20quality%20and%0Ageneralization.%20Comprehensive%20experiments%20on%20four%20datasets%20and%20seven%20tumor%0Acategories%20demonstrate%20the%20superior%20performance%20of%20our%20method%2C%20surpassing%0Acurrent%20state-of-the-art%20models%20across%20multiple%20zero-shot%20settings.%20Codes%20are%0Aavailable%20at%20https%3A//github.com/Yankai96/DiffuGTS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02753v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Generalizable%2520Tumor%2520Segmentation%2520with%2520Anomaly-Aware%250A%2520%2520Open-Vocabulary%2520Attention%2520Maps%2520and%2520Frozen%2520Foundation%2520Diffusion%2520Models%26entry.906535625%3DYankai%2520Jiang%2520and%2520Peng%2520Zhang%2520and%2520Donglin%2520Yang%2520and%2520Yuan%2520Tian%2520and%2520Hai%2520Lin%2520and%2520Xiaosong%2520Wang%26entry.1292438233%3D%2520%2520We%2520explore%2520Generalizable%2520Tumor%2520Segmentation%252C%2520aiming%2520to%2520train%2520a%2520single%2520model%250Afor%2520zero-shot%2520tumor%2520segmentation%2520across%2520diverse%2520anatomical%2520regions.%2520Existing%250Amethods%2520face%2520limitations%2520related%2520to%2520segmentation%2520quality%252C%2520scalability%252C%2520and%2520the%250Arange%2520of%2520applicable%2520imaging%2520modalities.%2520In%2520this%2520paper%252C%2520we%2520uncover%2520the%2520potential%250Aof%2520the%2520internal%2520representations%2520within%2520frozen%2520medical%2520foundation%2520diffusion%250Amodels%2520as%2520highly%2520efficient%2520zero-shot%2520learners%2520for%2520tumor%2520segmentation%2520by%250Aintroducing%2520a%2520novel%2520framework%2520named%2520DiffuGTS.%2520DiffuGTS%2520creates%2520anomaly-aware%250Aopen-vocabulary%2520attention%2520maps%2520based%2520on%2520text%2520prompts%2520to%2520enable%2520generalizable%250Aanomaly%2520segmentation%2520without%2520being%2520restricted%2520by%2520a%2520predefined%2520training%2520category%250Alist.%2520To%2520further%2520improve%2520and%2520refine%2520anomaly%2520segmentation%2520masks%252C%2520DiffuGTS%250Aleverages%2520the%2520diffusion%2520model%252C%2520transforming%2520pathological%2520regions%2520into%250Ahigh-quality%2520pseudo-healthy%2520counterparts%2520through%2520latent%2520space%2520inpainting%252C%2520and%250Aapplies%2520a%2520novel%2520pixel-level%2520and%2520feature-level%2520residual%2520learning%2520approach%252C%250Aresulting%2520in%2520segmentation%2520masks%2520with%2520significantly%2520enhanced%2520quality%2520and%250Ageneralization.%2520Comprehensive%2520experiments%2520on%2520four%2520datasets%2520and%2520seven%2520tumor%250Acategories%2520demonstrate%2520the%2520superior%2520performance%2520of%2520our%2520method%252C%2520surpassing%250Acurrent%2520state-of-the-art%2520models%2520across%2520multiple%2520zero-shot%2520settings.%2520Codes%2520are%250Aavailable%2520at%2520https%253A//github.com/Yankai96/DiffuGTS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02753v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Generalizable%20Tumor%20Segmentation%20with%20Anomaly-Aware%0A%20%20Open-Vocabulary%20Attention%20Maps%20and%20Frozen%20Foundation%20Diffusion%20Models&entry.906535625=Yankai%20Jiang%20and%20Peng%20Zhang%20and%20Donglin%20Yang%20and%20Yuan%20Tian%20and%20Hai%20Lin%20and%20Xiaosong%20Wang&entry.1292438233=%20%20We%20explore%20Generalizable%20Tumor%20Segmentation%2C%20aiming%20to%20train%20a%20single%20model%0Afor%20zero-shot%20tumor%20segmentation%20across%20diverse%20anatomical%20regions.%20Existing%0Amethods%20face%20limitations%20related%20to%20segmentation%20quality%2C%20scalability%2C%20and%20the%0Arange%20of%20applicable%20imaging%20modalities.%20In%20this%20paper%2C%20we%20uncover%20the%20potential%0Aof%20the%20internal%20representations%20within%20frozen%20medical%20foundation%20diffusion%0Amodels%20as%20highly%20efficient%20zero-shot%20learners%20for%20tumor%20segmentation%20by%0Aintroducing%20a%20novel%20framework%20named%20DiffuGTS.%20DiffuGTS%20creates%20anomaly-aware%0Aopen-vocabulary%20attention%20maps%20based%20on%20text%20prompts%20to%20enable%20generalizable%0Aanomaly%20segmentation%20without%20being%20restricted%20by%20a%20predefined%20training%20category%0Alist.%20To%20further%20improve%20and%20refine%20anomaly%20segmentation%20masks%2C%20DiffuGTS%0Aleverages%20the%20diffusion%20model%2C%20transforming%20pathological%20regions%20into%0Ahigh-quality%20pseudo-healthy%20counterparts%20through%20latent%20space%20inpainting%2C%20and%0Aapplies%20a%20novel%20pixel-level%20and%20feature-level%20residual%20learning%20approach%2C%0Aresulting%20in%20segmentation%20masks%20with%20significantly%20enhanced%20quality%20and%0Ageneralization.%20Comprehensive%20experiments%20on%20four%20datasets%20and%20seven%20tumor%0Acategories%20demonstrate%20the%20superior%20performance%20of%20our%20method%2C%20surpassing%0Acurrent%20state-of-the-art%20models%20across%20multiple%20zero-shot%20settings.%20Codes%20are%0Aavailable%20at%20https%3A//github.com/Yankai96/DiffuGTS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02753v1&entry.124074799=Read"},
{"title": "RobSurv: Vector Quantization-Based Multi-Modal Learning for Robust\n  Cancer Survival Prediction", "author": "Aiman Farooq and Azad Singh and Deepak Mishra and Santanu Chaudhury", "abstract": "  Cancer survival prediction using multi-modal medical imaging presents a\ncritical challenge in oncology, mainly due to the vulnerability of deep\nlearning models to noise and protocol variations across imaging centers.\nCurrent approaches struggle to extract consistent features from heterogeneous\nCT and PET images, limiting their clinical applicability. We address these\nchallenges by introducing RobSurv, a robust deep-learning framework that\nleverages vector quantization for resilient multi-modal feature learning. The\nkey innovation of our approach lies in its dual-path architecture: one path\nmaps continuous imaging features to learned discrete codebooks for\nnoise-resistant representation, while the parallel path preserves fine-grained\ndetails through continuous feature processing. This dual representation is\nintegrated through a novel patch-wise fusion mechanism that maintains local\nspatial relationships while capturing global context via Transformer-based\nprocessing. In extensive evaluations across three diverse datasets (HECKTOR,\nH\\&N1, and NSCLC Radiogenomics), RobSurv demonstrates superior performance,\nachieving concordance index of 0.771, 0.742, and 0.734 respectively -\nsignificantly outperforming existing methods. Most notably, our model maintains\nrobust performance even under severe noise conditions, with performance\ndegradation of only 3.8-4.5\\% compared to 8-12\\% in baseline methods. These\nresults, combined with strong generalization across different cancer types and\nimaging protocols, establish RobSurv as a promising solution for reliable\nclinical prognosis that can enhance treatment planning and patient care.\n", "link": "http://arxiv.org/abs/2505.02529v1", "date": "2025-05-05", "relevancy": 2.1322, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5551}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5328}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5244}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RobSurv%3A%20Vector%20Quantization-Based%20Multi-Modal%20Learning%20for%20Robust%0A%20%20Cancer%20Survival%20Prediction&body=Title%3A%20RobSurv%3A%20Vector%20Quantization-Based%20Multi-Modal%20Learning%20for%20Robust%0A%20%20Cancer%20Survival%20Prediction%0AAuthor%3A%20Aiman%20Farooq%20and%20Azad%20Singh%20and%20Deepak%20Mishra%20and%20Santanu%20Chaudhury%0AAbstract%3A%20%20%20Cancer%20survival%20prediction%20using%20multi-modal%20medical%20imaging%20presents%20a%0Acritical%20challenge%20in%20oncology%2C%20mainly%20due%20to%20the%20vulnerability%20of%20deep%0Alearning%20models%20to%20noise%20and%20protocol%20variations%20across%20imaging%20centers.%0ACurrent%20approaches%20struggle%20to%20extract%20consistent%20features%20from%20heterogeneous%0ACT%20and%20PET%20images%2C%20limiting%20their%20clinical%20applicability.%20We%20address%20these%0Achallenges%20by%20introducing%20RobSurv%2C%20a%20robust%20deep-learning%20framework%20that%0Aleverages%20vector%20quantization%20for%20resilient%20multi-modal%20feature%20learning.%20The%0Akey%20innovation%20of%20our%20approach%20lies%20in%20its%20dual-path%20architecture%3A%20one%20path%0Amaps%20continuous%20imaging%20features%20to%20learned%20discrete%20codebooks%20for%0Anoise-resistant%20representation%2C%20while%20the%20parallel%20path%20preserves%20fine-grained%0Adetails%20through%20continuous%20feature%20processing.%20This%20dual%20representation%20is%0Aintegrated%20through%20a%20novel%20patch-wise%20fusion%20mechanism%20that%20maintains%20local%0Aspatial%20relationships%20while%20capturing%20global%20context%20via%20Transformer-based%0Aprocessing.%20In%20extensive%20evaluations%20across%20three%20diverse%20datasets%20%28HECKTOR%2C%0AH%5C%26N1%2C%20and%20NSCLC%20Radiogenomics%29%2C%20RobSurv%20demonstrates%20superior%20performance%2C%0Aachieving%20concordance%20index%20of%200.771%2C%200.742%2C%20and%200.734%20respectively%20-%0Asignificantly%20outperforming%20existing%20methods.%20Most%20notably%2C%20our%20model%20maintains%0Arobust%20performance%20even%20under%20severe%20noise%20conditions%2C%20with%20performance%0Adegradation%20of%20only%203.8-4.5%5C%25%20compared%20to%208-12%5C%25%20in%20baseline%20methods.%20These%0Aresults%2C%20combined%20with%20strong%20generalization%20across%20different%20cancer%20types%20and%0Aimaging%20protocols%2C%20establish%20RobSurv%20as%20a%20promising%20solution%20for%20reliable%0Aclinical%20prognosis%20that%20can%20enhance%20treatment%20planning%20and%20patient%20care.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02529v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobSurv%253A%2520Vector%2520Quantization-Based%2520Multi-Modal%2520Learning%2520for%2520Robust%250A%2520%2520Cancer%2520Survival%2520Prediction%26entry.906535625%3DAiman%2520Farooq%2520and%2520Azad%2520Singh%2520and%2520Deepak%2520Mishra%2520and%2520Santanu%2520Chaudhury%26entry.1292438233%3D%2520%2520Cancer%2520survival%2520prediction%2520using%2520multi-modal%2520medical%2520imaging%2520presents%2520a%250Acritical%2520challenge%2520in%2520oncology%252C%2520mainly%2520due%2520to%2520the%2520vulnerability%2520of%2520deep%250Alearning%2520models%2520to%2520noise%2520and%2520protocol%2520variations%2520across%2520imaging%2520centers.%250ACurrent%2520approaches%2520struggle%2520to%2520extract%2520consistent%2520features%2520from%2520heterogeneous%250ACT%2520and%2520PET%2520images%252C%2520limiting%2520their%2520clinical%2520applicability.%2520We%2520address%2520these%250Achallenges%2520by%2520introducing%2520RobSurv%252C%2520a%2520robust%2520deep-learning%2520framework%2520that%250Aleverages%2520vector%2520quantization%2520for%2520resilient%2520multi-modal%2520feature%2520learning.%2520The%250Akey%2520innovation%2520of%2520our%2520approach%2520lies%2520in%2520its%2520dual-path%2520architecture%253A%2520one%2520path%250Amaps%2520continuous%2520imaging%2520features%2520to%2520learned%2520discrete%2520codebooks%2520for%250Anoise-resistant%2520representation%252C%2520while%2520the%2520parallel%2520path%2520preserves%2520fine-grained%250Adetails%2520through%2520continuous%2520feature%2520processing.%2520This%2520dual%2520representation%2520is%250Aintegrated%2520through%2520a%2520novel%2520patch-wise%2520fusion%2520mechanism%2520that%2520maintains%2520local%250Aspatial%2520relationships%2520while%2520capturing%2520global%2520context%2520via%2520Transformer-based%250Aprocessing.%2520In%2520extensive%2520evaluations%2520across%2520three%2520diverse%2520datasets%2520%2528HECKTOR%252C%250AH%255C%2526N1%252C%2520and%2520NSCLC%2520Radiogenomics%2529%252C%2520RobSurv%2520demonstrates%2520superior%2520performance%252C%250Aachieving%2520concordance%2520index%2520of%25200.771%252C%25200.742%252C%2520and%25200.734%2520respectively%2520-%250Asignificantly%2520outperforming%2520existing%2520methods.%2520Most%2520notably%252C%2520our%2520model%2520maintains%250Arobust%2520performance%2520even%2520under%2520severe%2520noise%2520conditions%252C%2520with%2520performance%250Adegradation%2520of%2520only%25203.8-4.5%255C%2525%2520compared%2520to%25208-12%255C%2525%2520in%2520baseline%2520methods.%2520These%250Aresults%252C%2520combined%2520with%2520strong%2520generalization%2520across%2520different%2520cancer%2520types%2520and%250Aimaging%2520protocols%252C%2520establish%2520RobSurv%2520as%2520a%2520promising%2520solution%2520for%2520reliable%250Aclinical%2520prognosis%2520that%2520can%2520enhance%2520treatment%2520planning%2520and%2520patient%2520care.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02529v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RobSurv%3A%20Vector%20Quantization-Based%20Multi-Modal%20Learning%20for%20Robust%0A%20%20Cancer%20Survival%20Prediction&entry.906535625=Aiman%20Farooq%20and%20Azad%20Singh%20and%20Deepak%20Mishra%20and%20Santanu%20Chaudhury&entry.1292438233=%20%20Cancer%20survival%20prediction%20using%20multi-modal%20medical%20imaging%20presents%20a%0Acritical%20challenge%20in%20oncology%2C%20mainly%20due%20to%20the%20vulnerability%20of%20deep%0Alearning%20models%20to%20noise%20and%20protocol%20variations%20across%20imaging%20centers.%0ACurrent%20approaches%20struggle%20to%20extract%20consistent%20features%20from%20heterogeneous%0ACT%20and%20PET%20images%2C%20limiting%20their%20clinical%20applicability.%20We%20address%20these%0Achallenges%20by%20introducing%20RobSurv%2C%20a%20robust%20deep-learning%20framework%20that%0Aleverages%20vector%20quantization%20for%20resilient%20multi-modal%20feature%20learning.%20The%0Akey%20innovation%20of%20our%20approach%20lies%20in%20its%20dual-path%20architecture%3A%20one%20path%0Amaps%20continuous%20imaging%20features%20to%20learned%20discrete%20codebooks%20for%0Anoise-resistant%20representation%2C%20while%20the%20parallel%20path%20preserves%20fine-grained%0Adetails%20through%20continuous%20feature%20processing.%20This%20dual%20representation%20is%0Aintegrated%20through%20a%20novel%20patch-wise%20fusion%20mechanism%20that%20maintains%20local%0Aspatial%20relationships%20while%20capturing%20global%20context%20via%20Transformer-based%0Aprocessing.%20In%20extensive%20evaluations%20across%20three%20diverse%20datasets%20%28HECKTOR%2C%0AH%5C%26N1%2C%20and%20NSCLC%20Radiogenomics%29%2C%20RobSurv%20demonstrates%20superior%20performance%2C%0Aachieving%20concordance%20index%20of%200.771%2C%200.742%2C%20and%200.734%20respectively%20-%0Asignificantly%20outperforming%20existing%20methods.%20Most%20notably%2C%20our%20model%20maintains%0Arobust%20performance%20even%20under%20severe%20noise%20conditions%2C%20with%20performance%0Adegradation%20of%20only%203.8-4.5%5C%25%20compared%20to%208-12%5C%25%20in%20baseline%20methods.%20These%0Aresults%2C%20combined%20with%20strong%20generalization%20across%20different%20cancer%20types%20and%0Aimaging%20protocols%2C%20establish%20RobSurv%20as%20a%20promising%20solution%20for%20reliable%0Aclinical%20prognosis%20that%20can%20enhance%20treatment%20planning%20and%20patient%20care.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02529v1&entry.124074799=Read"},
{"title": "Structure Causal Models and LLMs Integration in Medical Visual Question\n  Answering", "author": "Zibo Xu and Qiang Li and Weizhi Nie and Weijie Wang and Anan Liu", "abstract": "  Medical Visual Question Answering (MedVQA) aims to answer medical questions\naccording to medical images. However, the complexity of medical data leads to\nconfounders that are difficult to observe, so bias between images and questions\nis inevitable. Such cross-modal bias makes it challenging to infer medically\nmeaningful answers. In this work, we propose a causal inference framework for\nthe MedVQA task, which effectively eliminates the relative confounding effect\nbetween the image and the question to ensure the precision of the\nquestion-answering (QA) session. We are the first to introduce a novel causal\ngraph structure that represents the interaction between visual and textual\nelements, explicitly capturing how different questions influence visual\nfeatures. During optimization, we apply the mutual information to discover\nspurious correlations and propose a multi-variable resampling front-door\nadjustment method to eliminate the relative confounding effect, which aims to\nalign features based on their true causal relevance to the question-answering\ntask. In addition, we also introduce a prompt strategy that combines multiple\nprompt forms to improve the model's ability to understand complex medical data\nand answer accurately. Extensive experiments on three MedVQA datasets\ndemonstrate that 1) our method significantly improves the accuracy of MedVQA,\nand 2) our method achieves true causal correlations in the face of complex\nmedical data.\n", "link": "http://arxiv.org/abs/2505.02703v1", "date": "2025-05-05", "relevancy": 2.1296, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.536}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.536}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structure%20Causal%20Models%20and%20LLMs%20Integration%20in%20Medical%20Visual%20Question%0A%20%20Answering&body=Title%3A%20Structure%20Causal%20Models%20and%20LLMs%20Integration%20in%20Medical%20Visual%20Question%0A%20%20Answering%0AAuthor%3A%20Zibo%20Xu%20and%20Qiang%20Li%20and%20Weizhi%20Nie%20and%20Weijie%20Wang%20and%20Anan%20Liu%0AAbstract%3A%20%20%20Medical%20Visual%20Question%20Answering%20%28MedVQA%29%20aims%20to%20answer%20medical%20questions%0Aaccording%20to%20medical%20images.%20However%2C%20the%20complexity%20of%20medical%20data%20leads%20to%0Aconfounders%20that%20are%20difficult%20to%20observe%2C%20so%20bias%20between%20images%20and%20questions%0Ais%20inevitable.%20Such%20cross-modal%20bias%20makes%20it%20challenging%20to%20infer%20medically%0Ameaningful%20answers.%20In%20this%20work%2C%20we%20propose%20a%20causal%20inference%20framework%20for%0Athe%20MedVQA%20task%2C%20which%20effectively%20eliminates%20the%20relative%20confounding%20effect%0Abetween%20the%20image%20and%20the%20question%20to%20ensure%20the%20precision%20of%20the%0Aquestion-answering%20%28QA%29%20session.%20We%20are%20the%20first%20to%20introduce%20a%20novel%20causal%0Agraph%20structure%20that%20represents%20the%20interaction%20between%20visual%20and%20textual%0Aelements%2C%20explicitly%20capturing%20how%20different%20questions%20influence%20visual%0Afeatures.%20During%20optimization%2C%20we%20apply%20the%20mutual%20information%20to%20discover%0Aspurious%20correlations%20and%20propose%20a%20multi-variable%20resampling%20front-door%0Aadjustment%20method%20to%20eliminate%20the%20relative%20confounding%20effect%2C%20which%20aims%20to%0Aalign%20features%20based%20on%20their%20true%20causal%20relevance%20to%20the%20question-answering%0Atask.%20In%20addition%2C%20we%20also%20introduce%20a%20prompt%20strategy%20that%20combines%20multiple%0Aprompt%20forms%20to%20improve%20the%20model%27s%20ability%20to%20understand%20complex%20medical%20data%0Aand%20answer%20accurately.%20Extensive%20experiments%20on%20three%20MedVQA%20datasets%0Ademonstrate%20that%201%29%20our%20method%20significantly%20improves%20the%20accuracy%20of%20MedVQA%2C%0Aand%202%29%20our%20method%20achieves%20true%20causal%20correlations%20in%20the%20face%20of%20complex%0Amedical%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02703v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructure%2520Causal%2520Models%2520and%2520LLMs%2520Integration%2520in%2520Medical%2520Visual%2520Question%250A%2520%2520Answering%26entry.906535625%3DZibo%2520Xu%2520and%2520Qiang%2520Li%2520and%2520Weizhi%2520Nie%2520and%2520Weijie%2520Wang%2520and%2520Anan%2520Liu%26entry.1292438233%3D%2520%2520Medical%2520Visual%2520Question%2520Answering%2520%2528MedVQA%2529%2520aims%2520to%2520answer%2520medical%2520questions%250Aaccording%2520to%2520medical%2520images.%2520However%252C%2520the%2520complexity%2520of%2520medical%2520data%2520leads%2520to%250Aconfounders%2520that%2520are%2520difficult%2520to%2520observe%252C%2520so%2520bias%2520between%2520images%2520and%2520questions%250Ais%2520inevitable.%2520Such%2520cross-modal%2520bias%2520makes%2520it%2520challenging%2520to%2520infer%2520medically%250Ameaningful%2520answers.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520causal%2520inference%2520framework%2520for%250Athe%2520MedVQA%2520task%252C%2520which%2520effectively%2520eliminates%2520the%2520relative%2520confounding%2520effect%250Abetween%2520the%2520image%2520and%2520the%2520question%2520to%2520ensure%2520the%2520precision%2520of%2520the%250Aquestion-answering%2520%2528QA%2529%2520session.%2520We%2520are%2520the%2520first%2520to%2520introduce%2520a%2520novel%2520causal%250Agraph%2520structure%2520that%2520represents%2520the%2520interaction%2520between%2520visual%2520and%2520textual%250Aelements%252C%2520explicitly%2520capturing%2520how%2520different%2520questions%2520influence%2520visual%250Afeatures.%2520During%2520optimization%252C%2520we%2520apply%2520the%2520mutual%2520information%2520to%2520discover%250Aspurious%2520correlations%2520and%2520propose%2520a%2520multi-variable%2520resampling%2520front-door%250Aadjustment%2520method%2520to%2520eliminate%2520the%2520relative%2520confounding%2520effect%252C%2520which%2520aims%2520to%250Aalign%2520features%2520based%2520on%2520their%2520true%2520causal%2520relevance%2520to%2520the%2520question-answering%250Atask.%2520In%2520addition%252C%2520we%2520also%2520introduce%2520a%2520prompt%2520strategy%2520that%2520combines%2520multiple%250Aprompt%2520forms%2520to%2520improve%2520the%2520model%2527s%2520ability%2520to%2520understand%2520complex%2520medical%2520data%250Aand%2520answer%2520accurately.%2520Extensive%2520experiments%2520on%2520three%2520MedVQA%2520datasets%250Ademonstrate%2520that%25201%2529%2520our%2520method%2520significantly%2520improves%2520the%2520accuracy%2520of%2520MedVQA%252C%250Aand%25202%2529%2520our%2520method%2520achieves%2520true%2520causal%2520correlations%2520in%2520the%2520face%2520of%2520complex%250Amedical%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02703v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structure%20Causal%20Models%20and%20LLMs%20Integration%20in%20Medical%20Visual%20Question%0A%20%20Answering&entry.906535625=Zibo%20Xu%20and%20Qiang%20Li%20and%20Weizhi%20Nie%20and%20Weijie%20Wang%20and%20Anan%20Liu&entry.1292438233=%20%20Medical%20Visual%20Question%20Answering%20%28MedVQA%29%20aims%20to%20answer%20medical%20questions%0Aaccording%20to%20medical%20images.%20However%2C%20the%20complexity%20of%20medical%20data%20leads%20to%0Aconfounders%20that%20are%20difficult%20to%20observe%2C%20so%20bias%20between%20images%20and%20questions%0Ais%20inevitable.%20Such%20cross-modal%20bias%20makes%20it%20challenging%20to%20infer%20medically%0Ameaningful%20answers.%20In%20this%20work%2C%20we%20propose%20a%20causal%20inference%20framework%20for%0Athe%20MedVQA%20task%2C%20which%20effectively%20eliminates%20the%20relative%20confounding%20effect%0Abetween%20the%20image%20and%20the%20question%20to%20ensure%20the%20precision%20of%20the%0Aquestion-answering%20%28QA%29%20session.%20We%20are%20the%20first%20to%20introduce%20a%20novel%20causal%0Agraph%20structure%20that%20represents%20the%20interaction%20between%20visual%20and%20textual%0Aelements%2C%20explicitly%20capturing%20how%20different%20questions%20influence%20visual%0Afeatures.%20During%20optimization%2C%20we%20apply%20the%20mutual%20information%20to%20discover%0Aspurious%20correlations%20and%20propose%20a%20multi-variable%20resampling%20front-door%0Aadjustment%20method%20to%20eliminate%20the%20relative%20confounding%20effect%2C%20which%20aims%20to%0Aalign%20features%20based%20on%20their%20true%20causal%20relevance%20to%20the%20question-answering%0Atask.%20In%20addition%2C%20we%20also%20introduce%20a%20prompt%20strategy%20that%20combines%20multiple%0Aprompt%20forms%20to%20improve%20the%20model%27s%20ability%20to%20understand%20complex%20medical%20data%0Aand%20answer%20accurately.%20Extensive%20experiments%20on%20three%20MedVQA%20datasets%0Ademonstrate%20that%201%29%20our%20method%20significantly%20improves%20the%20accuracy%20of%20MedVQA%2C%0Aand%202%29%20our%20method%20achieves%20true%20causal%20correlations%20in%20the%20face%20of%20complex%0Amedical%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02703v1&entry.124074799=Read"},
{"title": "Learning and Online Replication of Grasp Forces from Electromyography\n  Signals for Prosthetic Finger Control", "author": "Robin Arbaud and Elisa Motta and Marco Domenico Avaro and Stefano Picinich and Marta Lorenzini and Arash Ajoudani", "abstract": "  Partial hand amputations significantly affect the physical and psychosocial\nwell-being of individuals, yet intuitive control of externally powered\nprostheses remains an open challenge. To address this gap, we developed a\nforce-controlled prosthetic finger activated by electromyography (EMG) signals.\nThe prototype, constructed around a wrist brace, functions as a supernumerary\nfinger placed near the index, allowing for early-stage evaluation on unimpaired\nsubjects. A neural network-based model was then implemented to estimate\nfingertip forces from EMG inputs, allowing for online adjustment of the\nprosthetic finger grip strength. The force estimation model was validated\nthrough experiments with ten participants, demonstrating its effectiveness in\npredicting forces. Additionally, online trials with four users wearing the\nprosthesis exhibited precise control over the device. Our findings highlight\nthe potential of using EMG-based force estimation to enhance the functionality\nof prosthetic fingers.\n", "link": "http://arxiv.org/abs/2505.02574v1", "date": "2025-05-05", "relevancy": 2.1026, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5352}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5222}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5105}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20and%20Online%20Replication%20of%20Grasp%20Forces%20from%20Electromyography%0A%20%20Signals%20for%20Prosthetic%20Finger%20Control&body=Title%3A%20Learning%20and%20Online%20Replication%20of%20Grasp%20Forces%20from%20Electromyography%0A%20%20Signals%20for%20Prosthetic%20Finger%20Control%0AAuthor%3A%20Robin%20Arbaud%20and%20Elisa%20Motta%20and%20Marco%20Domenico%20Avaro%20and%20Stefano%20Picinich%20and%20Marta%20Lorenzini%20and%20Arash%20Ajoudani%0AAbstract%3A%20%20%20Partial%20hand%20amputations%20significantly%20affect%20the%20physical%20and%20psychosocial%0Awell-being%20of%20individuals%2C%20yet%20intuitive%20control%20of%20externally%20powered%0Aprostheses%20remains%20an%20open%20challenge.%20To%20address%20this%20gap%2C%20we%20developed%20a%0Aforce-controlled%20prosthetic%20finger%20activated%20by%20electromyography%20%28EMG%29%20signals.%0AThe%20prototype%2C%20constructed%20around%20a%20wrist%20brace%2C%20functions%20as%20a%20supernumerary%0Afinger%20placed%20near%20the%20index%2C%20allowing%20for%20early-stage%20evaluation%20on%20unimpaired%0Asubjects.%20A%20neural%20network-based%20model%20was%20then%20implemented%20to%20estimate%0Afingertip%20forces%20from%20EMG%20inputs%2C%20allowing%20for%20online%20adjustment%20of%20the%0Aprosthetic%20finger%20grip%20strength.%20The%20force%20estimation%20model%20was%20validated%0Athrough%20experiments%20with%20ten%20participants%2C%20demonstrating%20its%20effectiveness%20in%0Apredicting%20forces.%20Additionally%2C%20online%20trials%20with%20four%20users%20wearing%20the%0Aprosthesis%20exhibited%20precise%20control%20over%20the%20device.%20Our%20findings%20highlight%0Athe%20potential%20of%20using%20EMG-based%20force%20estimation%20to%20enhance%20the%20functionality%0Aof%20prosthetic%20fingers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02574v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520and%2520Online%2520Replication%2520of%2520Grasp%2520Forces%2520from%2520Electromyography%250A%2520%2520Signals%2520for%2520Prosthetic%2520Finger%2520Control%26entry.906535625%3DRobin%2520Arbaud%2520and%2520Elisa%2520Motta%2520and%2520Marco%2520Domenico%2520Avaro%2520and%2520Stefano%2520Picinich%2520and%2520Marta%2520Lorenzini%2520and%2520Arash%2520Ajoudani%26entry.1292438233%3D%2520%2520Partial%2520hand%2520amputations%2520significantly%2520affect%2520the%2520physical%2520and%2520psychosocial%250Awell-being%2520of%2520individuals%252C%2520yet%2520intuitive%2520control%2520of%2520externally%2520powered%250Aprostheses%2520remains%2520an%2520open%2520challenge.%2520To%2520address%2520this%2520gap%252C%2520we%2520developed%2520a%250Aforce-controlled%2520prosthetic%2520finger%2520activated%2520by%2520electromyography%2520%2528EMG%2529%2520signals.%250AThe%2520prototype%252C%2520constructed%2520around%2520a%2520wrist%2520brace%252C%2520functions%2520as%2520a%2520supernumerary%250Afinger%2520placed%2520near%2520the%2520index%252C%2520allowing%2520for%2520early-stage%2520evaluation%2520on%2520unimpaired%250Asubjects.%2520A%2520neural%2520network-based%2520model%2520was%2520then%2520implemented%2520to%2520estimate%250Afingertip%2520forces%2520from%2520EMG%2520inputs%252C%2520allowing%2520for%2520online%2520adjustment%2520of%2520the%250Aprosthetic%2520finger%2520grip%2520strength.%2520The%2520force%2520estimation%2520model%2520was%2520validated%250Athrough%2520experiments%2520with%2520ten%2520participants%252C%2520demonstrating%2520its%2520effectiveness%2520in%250Apredicting%2520forces.%2520Additionally%252C%2520online%2520trials%2520with%2520four%2520users%2520wearing%2520the%250Aprosthesis%2520exhibited%2520precise%2520control%2520over%2520the%2520device.%2520Our%2520findings%2520highlight%250Athe%2520potential%2520of%2520using%2520EMG-based%2520force%2520estimation%2520to%2520enhance%2520the%2520functionality%250Aof%2520prosthetic%2520fingers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02574v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20and%20Online%20Replication%20of%20Grasp%20Forces%20from%20Electromyography%0A%20%20Signals%20for%20Prosthetic%20Finger%20Control&entry.906535625=Robin%20Arbaud%20and%20Elisa%20Motta%20and%20Marco%20Domenico%20Avaro%20and%20Stefano%20Picinich%20and%20Marta%20Lorenzini%20and%20Arash%20Ajoudani&entry.1292438233=%20%20Partial%20hand%20amputations%20significantly%20affect%20the%20physical%20and%20psychosocial%0Awell-being%20of%20individuals%2C%20yet%20intuitive%20control%20of%20externally%20powered%0Aprostheses%20remains%20an%20open%20challenge.%20To%20address%20this%20gap%2C%20we%20developed%20a%0Aforce-controlled%20prosthetic%20finger%20activated%20by%20electromyography%20%28EMG%29%20signals.%0AThe%20prototype%2C%20constructed%20around%20a%20wrist%20brace%2C%20functions%20as%20a%20supernumerary%0Afinger%20placed%20near%20the%20index%2C%20allowing%20for%20early-stage%20evaluation%20on%20unimpaired%0Asubjects.%20A%20neural%20network-based%20model%20was%20then%20implemented%20to%20estimate%0Afingertip%20forces%20from%20EMG%20inputs%2C%20allowing%20for%20online%20adjustment%20of%20the%0Aprosthetic%20finger%20grip%20strength.%20The%20force%20estimation%20model%20was%20validated%0Athrough%20experiments%20with%20ten%20participants%2C%20demonstrating%20its%20effectiveness%20in%0Apredicting%20forces.%20Additionally%2C%20online%20trials%20with%20four%20users%20wearing%20the%0Aprosthesis%20exhibited%20precise%20control%20over%20the%20device.%20Our%20findings%20highlight%0Athe%20potential%20of%20using%20EMG-based%20force%20estimation%20to%20enhance%20the%20functionality%0Aof%20prosthetic%20fingers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02574v1&entry.124074799=Read"},
{"title": "Activation Space Interventions Can Be Transferred Between Large Language\n  Models", "author": "Narmeen Oozeer and Dhruv Nathawani and Nirmalendu Prakash and Michael Lan and Abir Harrasse and Amirali Abdullah", "abstract": "  The study of representation universality in AI models reveals growing\nconvergence across domains, modalities, and architectures. However, the\npractical applications of representation universality remain largely\nunexplored. We bridge this gap by demonstrating that safety interventions can\nbe transferred between models through learned mappings of their shared\nactivation spaces. We demonstrate this approach on two well-established AI\nsafety tasks: backdoor removal and refusal of harmful prompts, showing\nsuccessful transfer of steering vectors that alter the models' outputs in a\npredictable way. Additionally, we propose a new task, \\textit{corrupted\ncapabilities}, where models are fine-tuned to embed knowledge tied to a\nbackdoor. This tests their ability to separate useful skills from backdoors,\nreflecting real-world challenges. Extensive experiments across Llama, Qwen and\nGemma model families show that our method enables using smaller models to\nefficiently align larger ones. Furthermore, we demonstrate that autoencoder\nmappings between base and fine-tuned models can serve as reliable ``lightweight\nsafety switches\", allowing dynamic toggling between model behaviors.\n", "link": "http://arxiv.org/abs/2503.04429v2", "date": "2025-05-05", "relevancy": 2.0922, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5359}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5166}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5128}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Activation%20Space%20Interventions%20Can%20Be%20Transferred%20Between%20Large%20Language%0A%20%20Models&body=Title%3A%20Activation%20Space%20Interventions%20Can%20Be%20Transferred%20Between%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Narmeen%20Oozeer%20and%20Dhruv%20Nathawani%20and%20Nirmalendu%20Prakash%20and%20Michael%20Lan%20and%20Abir%20Harrasse%20and%20Amirali%20Abdullah%0AAbstract%3A%20%20%20The%20study%20of%20representation%20universality%20in%20AI%20models%20reveals%20growing%0Aconvergence%20across%20domains%2C%20modalities%2C%20and%20architectures.%20However%2C%20the%0Apractical%20applications%20of%20representation%20universality%20remain%20largely%0Aunexplored.%20We%20bridge%20this%20gap%20by%20demonstrating%20that%20safety%20interventions%20can%0Abe%20transferred%20between%20models%20through%20learned%20mappings%20of%20their%20shared%0Aactivation%20spaces.%20We%20demonstrate%20this%20approach%20on%20two%20well-established%20AI%0Asafety%20tasks%3A%20backdoor%20removal%20and%20refusal%20of%20harmful%20prompts%2C%20showing%0Asuccessful%20transfer%20of%20steering%20vectors%20that%20alter%20the%20models%27%20outputs%20in%20a%0Apredictable%20way.%20Additionally%2C%20we%20propose%20a%20new%20task%2C%20%5Ctextit%7Bcorrupted%0Acapabilities%7D%2C%20where%20models%20are%20fine-tuned%20to%20embed%20knowledge%20tied%20to%20a%0Abackdoor.%20This%20tests%20their%20ability%20to%20separate%20useful%20skills%20from%20backdoors%2C%0Areflecting%20real-world%20challenges.%20Extensive%20experiments%20across%20Llama%2C%20Qwen%20and%0AGemma%20model%20families%20show%20that%20our%20method%20enables%20using%20smaller%20models%20to%0Aefficiently%20align%20larger%20ones.%20Furthermore%2C%20we%20demonstrate%20that%20autoencoder%0Amappings%20between%20base%20and%20fine-tuned%20models%20can%20serve%20as%20reliable%20%60%60lightweight%0Asafety%20switches%22%2C%20allowing%20dynamic%20toggling%20between%20model%20behaviors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.04429v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActivation%2520Space%2520Interventions%2520Can%2520Be%2520Transferred%2520Between%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DNarmeen%2520Oozeer%2520and%2520Dhruv%2520Nathawani%2520and%2520Nirmalendu%2520Prakash%2520and%2520Michael%2520Lan%2520and%2520Abir%2520Harrasse%2520and%2520Amirali%2520Abdullah%26entry.1292438233%3D%2520%2520The%2520study%2520of%2520representation%2520universality%2520in%2520AI%2520models%2520reveals%2520growing%250Aconvergence%2520across%2520domains%252C%2520modalities%252C%2520and%2520architectures.%2520However%252C%2520the%250Apractical%2520applications%2520of%2520representation%2520universality%2520remain%2520largely%250Aunexplored.%2520We%2520bridge%2520this%2520gap%2520by%2520demonstrating%2520that%2520safety%2520interventions%2520can%250Abe%2520transferred%2520between%2520models%2520through%2520learned%2520mappings%2520of%2520their%2520shared%250Aactivation%2520spaces.%2520We%2520demonstrate%2520this%2520approach%2520on%2520two%2520well-established%2520AI%250Asafety%2520tasks%253A%2520backdoor%2520removal%2520and%2520refusal%2520of%2520harmful%2520prompts%252C%2520showing%250Asuccessful%2520transfer%2520of%2520steering%2520vectors%2520that%2520alter%2520the%2520models%2527%2520outputs%2520in%2520a%250Apredictable%2520way.%2520Additionally%252C%2520we%2520propose%2520a%2520new%2520task%252C%2520%255Ctextit%257Bcorrupted%250Acapabilities%257D%252C%2520where%2520models%2520are%2520fine-tuned%2520to%2520embed%2520knowledge%2520tied%2520to%2520a%250Abackdoor.%2520This%2520tests%2520their%2520ability%2520to%2520separate%2520useful%2520skills%2520from%2520backdoors%252C%250Areflecting%2520real-world%2520challenges.%2520Extensive%2520experiments%2520across%2520Llama%252C%2520Qwen%2520and%250AGemma%2520model%2520families%2520show%2520that%2520our%2520method%2520enables%2520using%2520smaller%2520models%2520to%250Aefficiently%2520align%2520larger%2520ones.%2520Furthermore%252C%2520we%2520demonstrate%2520that%2520autoencoder%250Amappings%2520between%2520base%2520and%2520fine-tuned%2520models%2520can%2520serve%2520as%2520reliable%2520%2560%2560lightweight%250Asafety%2520switches%2522%252C%2520allowing%2520dynamic%2520toggling%2520between%2520model%2520behaviors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.04429v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Activation%20Space%20Interventions%20Can%20Be%20Transferred%20Between%20Large%20Language%0A%20%20Models&entry.906535625=Narmeen%20Oozeer%20and%20Dhruv%20Nathawani%20and%20Nirmalendu%20Prakash%20and%20Michael%20Lan%20and%20Abir%20Harrasse%20and%20Amirali%20Abdullah&entry.1292438233=%20%20The%20study%20of%20representation%20universality%20in%20AI%20models%20reveals%20growing%0Aconvergence%20across%20domains%2C%20modalities%2C%20and%20architectures.%20However%2C%20the%0Apractical%20applications%20of%20representation%20universality%20remain%20largely%0Aunexplored.%20We%20bridge%20this%20gap%20by%20demonstrating%20that%20safety%20interventions%20can%0Abe%20transferred%20between%20models%20through%20learned%20mappings%20of%20their%20shared%0Aactivation%20spaces.%20We%20demonstrate%20this%20approach%20on%20two%20well-established%20AI%0Asafety%20tasks%3A%20backdoor%20removal%20and%20refusal%20of%20harmful%20prompts%2C%20showing%0Asuccessful%20transfer%20of%20steering%20vectors%20that%20alter%20the%20models%27%20outputs%20in%20a%0Apredictable%20way.%20Additionally%2C%20we%20propose%20a%20new%20task%2C%20%5Ctextit%7Bcorrupted%0Acapabilities%7D%2C%20where%20models%20are%20fine-tuned%20to%20embed%20knowledge%20tied%20to%20a%0Abackdoor.%20This%20tests%20their%20ability%20to%20separate%20useful%20skills%20from%20backdoors%2C%0Areflecting%20real-world%20challenges.%20Extensive%20experiments%20across%20Llama%2C%20Qwen%20and%0AGemma%20model%20families%20show%20that%20our%20method%20enables%20using%20smaller%20models%20to%0Aefficiently%20align%20larger%20ones.%20Furthermore%2C%20we%20demonstrate%20that%20autoencoder%0Amappings%20between%20base%20and%20fine-tuned%20models%20can%20serve%20as%20reliable%20%60%60lightweight%0Asafety%20switches%22%2C%20allowing%20dynamic%20toggling%20between%20model%20behaviors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.04429v2&entry.124074799=Read"},
{"title": "Cooperative Bayesian and variance networks disentangle aleatoric and\n  epistemic uncertainties", "author": "Jiaxiang Yi and Miguel A. Bessa", "abstract": "  Real-world data contains aleatoric uncertainty - irreducible noise arising\nfrom imperfect measurements or from incomplete knowledge about the data\ngeneration process. Mean variance estimation (MVE) networks can learn this type\nof uncertainty but require ad-hoc regularization strategies to avoid\noverfitting and are unable to predict epistemic uncertainty (model\nuncertainty). Conversely, Bayesian neural networks predict epistemic\nuncertainty but are notoriously difficult to train due to the approximate\nnature of Bayesian inference. We propose to cooperatively train a variance\nnetwork with a Bayesian neural network and demonstrate that the resulting model\ndisentangles aleatoric and epistemic uncertainties while improving the mean\nestimation. We demonstrate the effectiveness and scalability of this method\nacross a diverse range of datasets, including a time-dependent heteroscedastic\nregression dataset we created where the aleatoric uncertainty is known. The\nproposed method is straightforward to implement, robust, and adaptable to\nvarious model architectures.\n", "link": "http://arxiv.org/abs/2505.02743v1", "date": "2025-05-05", "relevancy": 2.0699, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5514}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5171}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cooperative%20Bayesian%20and%20variance%20networks%20disentangle%20aleatoric%20and%0A%20%20epistemic%20uncertainties&body=Title%3A%20Cooperative%20Bayesian%20and%20variance%20networks%20disentangle%20aleatoric%20and%0A%20%20epistemic%20uncertainties%0AAuthor%3A%20Jiaxiang%20Yi%20and%20Miguel%20A.%20Bessa%0AAbstract%3A%20%20%20Real-world%20data%20contains%20aleatoric%20uncertainty%20-%20irreducible%20noise%20arising%0Afrom%20imperfect%20measurements%20or%20from%20incomplete%20knowledge%20about%20the%20data%0Ageneration%20process.%20Mean%20variance%20estimation%20%28MVE%29%20networks%20can%20learn%20this%20type%0Aof%20uncertainty%20but%20require%20ad-hoc%20regularization%20strategies%20to%20avoid%0Aoverfitting%20and%20are%20unable%20to%20predict%20epistemic%20uncertainty%20%28model%0Auncertainty%29.%20Conversely%2C%20Bayesian%20neural%20networks%20predict%20epistemic%0Auncertainty%20but%20are%20notoriously%20difficult%20to%20train%20due%20to%20the%20approximate%0Anature%20of%20Bayesian%20inference.%20We%20propose%20to%20cooperatively%20train%20a%20variance%0Anetwork%20with%20a%20Bayesian%20neural%20network%20and%20demonstrate%20that%20the%20resulting%20model%0Adisentangles%20aleatoric%20and%20epistemic%20uncertainties%20while%20improving%20the%20mean%0Aestimation.%20We%20demonstrate%20the%20effectiveness%20and%20scalability%20of%20this%20method%0Aacross%20a%20diverse%20range%20of%20datasets%2C%20including%20a%20time-dependent%20heteroscedastic%0Aregression%20dataset%20we%20created%20where%20the%20aleatoric%20uncertainty%20is%20known.%20The%0Aproposed%20method%20is%20straightforward%20to%20implement%2C%20robust%2C%20and%20adaptable%20to%0Avarious%20model%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02743v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCooperative%2520Bayesian%2520and%2520variance%2520networks%2520disentangle%2520aleatoric%2520and%250A%2520%2520epistemic%2520uncertainties%26entry.906535625%3DJiaxiang%2520Yi%2520and%2520Miguel%2520A.%2520Bessa%26entry.1292438233%3D%2520%2520Real-world%2520data%2520contains%2520aleatoric%2520uncertainty%2520-%2520irreducible%2520noise%2520arising%250Afrom%2520imperfect%2520measurements%2520or%2520from%2520incomplete%2520knowledge%2520about%2520the%2520data%250Ageneration%2520process.%2520Mean%2520variance%2520estimation%2520%2528MVE%2529%2520networks%2520can%2520learn%2520this%2520type%250Aof%2520uncertainty%2520but%2520require%2520ad-hoc%2520regularization%2520strategies%2520to%2520avoid%250Aoverfitting%2520and%2520are%2520unable%2520to%2520predict%2520epistemic%2520uncertainty%2520%2528model%250Auncertainty%2529.%2520Conversely%252C%2520Bayesian%2520neural%2520networks%2520predict%2520epistemic%250Auncertainty%2520but%2520are%2520notoriously%2520difficult%2520to%2520train%2520due%2520to%2520the%2520approximate%250Anature%2520of%2520Bayesian%2520inference.%2520We%2520propose%2520to%2520cooperatively%2520train%2520a%2520variance%250Anetwork%2520with%2520a%2520Bayesian%2520neural%2520network%2520and%2520demonstrate%2520that%2520the%2520resulting%2520model%250Adisentangles%2520aleatoric%2520and%2520epistemic%2520uncertainties%2520while%2520improving%2520the%2520mean%250Aestimation.%2520We%2520demonstrate%2520the%2520effectiveness%2520and%2520scalability%2520of%2520this%2520method%250Aacross%2520a%2520diverse%2520range%2520of%2520datasets%252C%2520including%2520a%2520time-dependent%2520heteroscedastic%250Aregression%2520dataset%2520we%2520created%2520where%2520the%2520aleatoric%2520uncertainty%2520is%2520known.%2520The%250Aproposed%2520method%2520is%2520straightforward%2520to%2520implement%252C%2520robust%252C%2520and%2520adaptable%2520to%250Avarious%2520model%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02743v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cooperative%20Bayesian%20and%20variance%20networks%20disentangle%20aleatoric%20and%0A%20%20epistemic%20uncertainties&entry.906535625=Jiaxiang%20Yi%20and%20Miguel%20A.%20Bessa&entry.1292438233=%20%20Real-world%20data%20contains%20aleatoric%20uncertainty%20-%20irreducible%20noise%20arising%0Afrom%20imperfect%20measurements%20or%20from%20incomplete%20knowledge%20about%20the%20data%0Ageneration%20process.%20Mean%20variance%20estimation%20%28MVE%29%20networks%20can%20learn%20this%20type%0Aof%20uncertainty%20but%20require%20ad-hoc%20regularization%20strategies%20to%20avoid%0Aoverfitting%20and%20are%20unable%20to%20predict%20epistemic%20uncertainty%20%28model%0Auncertainty%29.%20Conversely%2C%20Bayesian%20neural%20networks%20predict%20epistemic%0Auncertainty%20but%20are%20notoriously%20difficult%20to%20train%20due%20to%20the%20approximate%0Anature%20of%20Bayesian%20inference.%20We%20propose%20to%20cooperatively%20train%20a%20variance%0Anetwork%20with%20a%20Bayesian%20neural%20network%20and%20demonstrate%20that%20the%20resulting%20model%0Adisentangles%20aleatoric%20and%20epistemic%20uncertainties%20while%20improving%20the%20mean%0Aestimation.%20We%20demonstrate%20the%20effectiveness%20and%20scalability%20of%20this%20method%0Aacross%20a%20diverse%20range%20of%20datasets%2C%20including%20a%20time-dependent%20heteroscedastic%0Aregression%20dataset%20we%20created%20where%20the%20aleatoric%20uncertainty%20is%20known.%20The%0Aproposed%20method%20is%20straightforward%20to%20implement%2C%20robust%2C%20and%20adaptable%20to%0Avarious%20model%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02743v1&entry.124074799=Read"},
{"title": "Adaptive Anomaly Detection in Network Flows with Low-Rank Tensor\n  Decompositions and Deep Unrolling", "author": "Lukas Schynol and Marius Pesavento", "abstract": "  Anomaly detection (AD) is increasingly recognized as a key component for\nensuring the resilience of future communication systems. While deep learning\nhas shown state-of-the-art AD performance, its application in critical systems\nis hindered by concerns regarding training data efficiency, domain adaptation\nand interpretability. This work considers AD in network flows using incomplete\nmeasurements, leveraging a robust tensor decomposition approach and deep\nunrolling techniques to address these challenges. We first propose a novel\nblock-successive convex approximation algorithm based on a regularized\nmodel-fitting objective where the normal flows are modeled as low-rank tensors\nand anomalies as sparse. An augmentation of the objective is introduced to\ndecrease the computational cost. We apply deep unrolling to derive a novel deep\nnetwork architecture based on our proposed algorithm, treating the\nregularization parameters as learnable weights. Inspired by Bayesian\napproaches, we extend the model architecture to perform online adaptation to\nper-flow and per-time-step statistics, improving AD performance while\nmaintaining a low parameter count and preserving the problem's permutation\nequivariances. To optimize the deep network weights for detection performance,\nwe employ a homotopy optimization approach based on an efficient approximation\nof the area under the receiver operating characteristic curve. Extensive\nexperiments on synthetic and real-world data demonstrate that our proposed deep\nnetwork architecture exhibits a high training data efficiency, outperforms\nreference methods, and adapts seamlessly to varying network topologies.\n", "link": "http://arxiv.org/abs/2409.11529v2", "date": "2025-05-05", "relevancy": 2.0309, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5163}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5087}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Anomaly%20Detection%20in%20Network%20Flows%20with%20Low-Rank%20Tensor%0A%20%20Decompositions%20and%20Deep%20Unrolling&body=Title%3A%20Adaptive%20Anomaly%20Detection%20in%20Network%20Flows%20with%20Low-Rank%20Tensor%0A%20%20Decompositions%20and%20Deep%20Unrolling%0AAuthor%3A%20Lukas%20Schynol%20and%20Marius%20Pesavento%0AAbstract%3A%20%20%20Anomaly%20detection%20%28AD%29%20is%20increasingly%20recognized%20as%20a%20key%20component%20for%0Aensuring%20the%20resilience%20of%20future%20communication%20systems.%20While%20deep%20learning%0Ahas%20shown%20state-of-the-art%20AD%20performance%2C%20its%20application%20in%20critical%20systems%0Ais%20hindered%20by%20concerns%20regarding%20training%20data%20efficiency%2C%20domain%20adaptation%0Aand%20interpretability.%20This%20work%20considers%20AD%20in%20network%20flows%20using%20incomplete%0Ameasurements%2C%20leveraging%20a%20robust%20tensor%20decomposition%20approach%20and%20deep%0Aunrolling%20techniques%20to%20address%20these%20challenges.%20We%20first%20propose%20a%20novel%0Ablock-successive%20convex%20approximation%20algorithm%20based%20on%20a%20regularized%0Amodel-fitting%20objective%20where%20the%20normal%20flows%20are%20modeled%20as%20low-rank%20tensors%0Aand%20anomalies%20as%20sparse.%20An%20augmentation%20of%20the%20objective%20is%20introduced%20to%0Adecrease%20the%20computational%20cost.%20We%20apply%20deep%20unrolling%20to%20derive%20a%20novel%20deep%0Anetwork%20architecture%20based%20on%20our%20proposed%20algorithm%2C%20treating%20the%0Aregularization%20parameters%20as%20learnable%20weights.%20Inspired%20by%20Bayesian%0Aapproaches%2C%20we%20extend%20the%20model%20architecture%20to%20perform%20online%20adaptation%20to%0Aper-flow%20and%20per-time-step%20statistics%2C%20improving%20AD%20performance%20while%0Amaintaining%20a%20low%20parameter%20count%20and%20preserving%20the%20problem%27s%20permutation%0Aequivariances.%20To%20optimize%20the%20deep%20network%20weights%20for%20detection%20performance%2C%0Awe%20employ%20a%20homotopy%20optimization%20approach%20based%20on%20an%20efficient%20approximation%0Aof%20the%20area%20under%20the%20receiver%20operating%20characteristic%20curve.%20Extensive%0Aexperiments%20on%20synthetic%20and%20real-world%20data%20demonstrate%20that%20our%20proposed%20deep%0Anetwork%20architecture%20exhibits%20a%20high%20training%20data%20efficiency%2C%20outperforms%0Areference%20methods%2C%20and%20adapts%20seamlessly%20to%20varying%20network%20topologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11529v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Anomaly%2520Detection%2520in%2520Network%2520Flows%2520with%2520Low-Rank%2520Tensor%250A%2520%2520Decompositions%2520and%2520Deep%2520Unrolling%26entry.906535625%3DLukas%2520Schynol%2520and%2520Marius%2520Pesavento%26entry.1292438233%3D%2520%2520Anomaly%2520detection%2520%2528AD%2529%2520is%2520increasingly%2520recognized%2520as%2520a%2520key%2520component%2520for%250Aensuring%2520the%2520resilience%2520of%2520future%2520communication%2520systems.%2520While%2520deep%2520learning%250Ahas%2520shown%2520state-of-the-art%2520AD%2520performance%252C%2520its%2520application%2520in%2520critical%2520systems%250Ais%2520hindered%2520by%2520concerns%2520regarding%2520training%2520data%2520efficiency%252C%2520domain%2520adaptation%250Aand%2520interpretability.%2520This%2520work%2520considers%2520AD%2520in%2520network%2520flows%2520using%2520incomplete%250Ameasurements%252C%2520leveraging%2520a%2520robust%2520tensor%2520decomposition%2520approach%2520and%2520deep%250Aunrolling%2520techniques%2520to%2520address%2520these%2520challenges.%2520We%2520first%2520propose%2520a%2520novel%250Ablock-successive%2520convex%2520approximation%2520algorithm%2520based%2520on%2520a%2520regularized%250Amodel-fitting%2520objective%2520where%2520the%2520normal%2520flows%2520are%2520modeled%2520as%2520low-rank%2520tensors%250Aand%2520anomalies%2520as%2520sparse.%2520An%2520augmentation%2520of%2520the%2520objective%2520is%2520introduced%2520to%250Adecrease%2520the%2520computational%2520cost.%2520We%2520apply%2520deep%2520unrolling%2520to%2520derive%2520a%2520novel%2520deep%250Anetwork%2520architecture%2520based%2520on%2520our%2520proposed%2520algorithm%252C%2520treating%2520the%250Aregularization%2520parameters%2520as%2520learnable%2520weights.%2520Inspired%2520by%2520Bayesian%250Aapproaches%252C%2520we%2520extend%2520the%2520model%2520architecture%2520to%2520perform%2520online%2520adaptation%2520to%250Aper-flow%2520and%2520per-time-step%2520statistics%252C%2520improving%2520AD%2520performance%2520while%250Amaintaining%2520a%2520low%2520parameter%2520count%2520and%2520preserving%2520the%2520problem%2527s%2520permutation%250Aequivariances.%2520To%2520optimize%2520the%2520deep%2520network%2520weights%2520for%2520detection%2520performance%252C%250Awe%2520employ%2520a%2520homotopy%2520optimization%2520approach%2520based%2520on%2520an%2520efficient%2520approximation%250Aof%2520the%2520area%2520under%2520the%2520receiver%2520operating%2520characteristic%2520curve.%2520Extensive%250Aexperiments%2520on%2520synthetic%2520and%2520real-world%2520data%2520demonstrate%2520that%2520our%2520proposed%2520deep%250Anetwork%2520architecture%2520exhibits%2520a%2520high%2520training%2520data%2520efficiency%252C%2520outperforms%250Areference%2520methods%252C%2520and%2520adapts%2520seamlessly%2520to%2520varying%2520network%2520topologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11529v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Anomaly%20Detection%20in%20Network%20Flows%20with%20Low-Rank%20Tensor%0A%20%20Decompositions%20and%20Deep%20Unrolling&entry.906535625=Lukas%20Schynol%20and%20Marius%20Pesavento&entry.1292438233=%20%20Anomaly%20detection%20%28AD%29%20is%20increasingly%20recognized%20as%20a%20key%20component%20for%0Aensuring%20the%20resilience%20of%20future%20communication%20systems.%20While%20deep%20learning%0Ahas%20shown%20state-of-the-art%20AD%20performance%2C%20its%20application%20in%20critical%20systems%0Ais%20hindered%20by%20concerns%20regarding%20training%20data%20efficiency%2C%20domain%20adaptation%0Aand%20interpretability.%20This%20work%20considers%20AD%20in%20network%20flows%20using%20incomplete%0Ameasurements%2C%20leveraging%20a%20robust%20tensor%20decomposition%20approach%20and%20deep%0Aunrolling%20techniques%20to%20address%20these%20challenges.%20We%20first%20propose%20a%20novel%0Ablock-successive%20convex%20approximation%20algorithm%20based%20on%20a%20regularized%0Amodel-fitting%20objective%20where%20the%20normal%20flows%20are%20modeled%20as%20low-rank%20tensors%0Aand%20anomalies%20as%20sparse.%20An%20augmentation%20of%20the%20objective%20is%20introduced%20to%0Adecrease%20the%20computational%20cost.%20We%20apply%20deep%20unrolling%20to%20derive%20a%20novel%20deep%0Anetwork%20architecture%20based%20on%20our%20proposed%20algorithm%2C%20treating%20the%0Aregularization%20parameters%20as%20learnable%20weights.%20Inspired%20by%20Bayesian%0Aapproaches%2C%20we%20extend%20the%20model%20architecture%20to%20perform%20online%20adaptation%20to%0Aper-flow%20and%20per-time-step%20statistics%2C%20improving%20AD%20performance%20while%0Amaintaining%20a%20low%20parameter%20count%20and%20preserving%20the%20problem%27s%20permutation%0Aequivariances.%20To%20optimize%20the%20deep%20network%20weights%20for%20detection%20performance%2C%0Awe%20employ%20a%20homotopy%20optimization%20approach%20based%20on%20an%20efficient%20approximation%0Aof%20the%20area%20under%20the%20receiver%20operating%20characteristic%20curve.%20Extensive%0Aexperiments%20on%20synthetic%20and%20real-world%20data%20demonstrate%20that%20our%20proposed%20deep%0Anetwork%20architecture%20exhibits%20a%20high%20training%20data%20efficiency%2C%20outperforms%0Areference%20methods%2C%20and%20adapts%20seamlessly%20to%20varying%20network%20topologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11529v2&entry.124074799=Read"},
{"title": "Towards Cross-Modality Modeling for Time Series Analytics: A Survey in\n  the LLM Era", "author": "Chenxi Liu and Shaowen Zhou and Qianxiong Xu and Hao Miao and Cheng Long and Ziyue Li and Rui Zhao", "abstract": "  The proliferation of edge devices has generated an unprecedented volume of\ntime series data across different domains, motivating various well-customized\nmethods. Recently, Large Language Models (LLMs) have emerged as a new paradigm\nfor time series analytics by leveraging the shared sequential nature of textual\ndata and time series. However, a fundamental cross-modality gap between time\nseries and LLMs exists, as LLMs are pre-trained on textual corpora and are not\ninherently optimized for time series. Many recent proposals are designed to\naddress this issue. In this survey, we provide an up-to-date overview of\nLLMs-based cross-modality modeling for time series analytics. We first\nintroduce a taxonomy that classifies existing approaches into four groups based\non the type of textual data employed for time series modeling. We then\nsummarize key cross-modality strategies, e.g., alignment and fusion, and\ndiscuss their applications across a range of downstream tasks. Furthermore, we\nconduct experiments on multimodal datasets from different application domains\nto investigate effective combinations of textual data and cross-modality\nstrategies for enhancing time series analytics. Finally, we suggest several\npromising directions for future research. This survey is designed for a range\nof professionals, researchers, and practitioners interested in LLM-based time\nseries modeling.\n", "link": "http://arxiv.org/abs/2505.02583v1", "date": "2025-05-05", "relevancy": 2.0233, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5189}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4996}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4952}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Cross-Modality%20Modeling%20for%20Time%20Series%20Analytics%3A%20A%20Survey%20in%0A%20%20the%20LLM%20Era&body=Title%3A%20Towards%20Cross-Modality%20Modeling%20for%20Time%20Series%20Analytics%3A%20A%20Survey%20in%0A%20%20the%20LLM%20Era%0AAuthor%3A%20Chenxi%20Liu%20and%20Shaowen%20Zhou%20and%20Qianxiong%20Xu%20and%20Hao%20Miao%20and%20Cheng%20Long%20and%20Ziyue%20Li%20and%20Rui%20Zhao%0AAbstract%3A%20%20%20The%20proliferation%20of%20edge%20devices%20has%20generated%20an%20unprecedented%20volume%20of%0Atime%20series%20data%20across%20different%20domains%2C%20motivating%20various%20well-customized%0Amethods.%20Recently%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20emerged%20as%20a%20new%20paradigm%0Afor%20time%20series%20analytics%20by%20leveraging%20the%20shared%20sequential%20nature%20of%20textual%0Adata%20and%20time%20series.%20However%2C%20a%20fundamental%20cross-modality%20gap%20between%20time%0Aseries%20and%20LLMs%20exists%2C%20as%20LLMs%20are%20pre-trained%20on%20textual%20corpora%20and%20are%20not%0Ainherently%20optimized%20for%20time%20series.%20Many%20recent%20proposals%20are%20designed%20to%0Aaddress%20this%20issue.%20In%20this%20survey%2C%20we%20provide%20an%20up-to-date%20overview%20of%0ALLMs-based%20cross-modality%20modeling%20for%20time%20series%20analytics.%20We%20first%0Aintroduce%20a%20taxonomy%20that%20classifies%20existing%20approaches%20into%20four%20groups%20based%0Aon%20the%20type%20of%20textual%20data%20employed%20for%20time%20series%20modeling.%20We%20then%0Asummarize%20key%20cross-modality%20strategies%2C%20e.g.%2C%20alignment%20and%20fusion%2C%20and%0Adiscuss%20their%20applications%20across%20a%20range%20of%20downstream%20tasks.%20Furthermore%2C%20we%0Aconduct%20experiments%20on%20multimodal%20datasets%20from%20different%20application%20domains%0Ato%20investigate%20effective%20combinations%20of%20textual%20data%20and%20cross-modality%0Astrategies%20for%20enhancing%20time%20series%20analytics.%20Finally%2C%20we%20suggest%20several%0Apromising%20directions%20for%20future%20research.%20This%20survey%20is%20designed%20for%20a%20range%0Aof%20professionals%2C%20researchers%2C%20and%20practitioners%20interested%20in%20LLM-based%20time%0Aseries%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02583v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Cross-Modality%2520Modeling%2520for%2520Time%2520Series%2520Analytics%253A%2520A%2520Survey%2520in%250A%2520%2520the%2520LLM%2520Era%26entry.906535625%3DChenxi%2520Liu%2520and%2520Shaowen%2520Zhou%2520and%2520Qianxiong%2520Xu%2520and%2520Hao%2520Miao%2520and%2520Cheng%2520Long%2520and%2520Ziyue%2520Li%2520and%2520Rui%2520Zhao%26entry.1292438233%3D%2520%2520The%2520proliferation%2520of%2520edge%2520devices%2520has%2520generated%2520an%2520unprecedented%2520volume%2520of%250Atime%2520series%2520data%2520across%2520different%2520domains%252C%2520motivating%2520various%2520well-customized%250Amethods.%2520Recently%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520emerged%2520as%2520a%2520new%2520paradigm%250Afor%2520time%2520series%2520analytics%2520by%2520leveraging%2520the%2520shared%2520sequential%2520nature%2520of%2520textual%250Adata%2520and%2520time%2520series.%2520However%252C%2520a%2520fundamental%2520cross-modality%2520gap%2520between%2520time%250Aseries%2520and%2520LLMs%2520exists%252C%2520as%2520LLMs%2520are%2520pre-trained%2520on%2520textual%2520corpora%2520and%2520are%2520not%250Ainherently%2520optimized%2520for%2520time%2520series.%2520Many%2520recent%2520proposals%2520are%2520designed%2520to%250Aaddress%2520this%2520issue.%2520In%2520this%2520survey%252C%2520we%2520provide%2520an%2520up-to-date%2520overview%2520of%250ALLMs-based%2520cross-modality%2520modeling%2520for%2520time%2520series%2520analytics.%2520We%2520first%250Aintroduce%2520a%2520taxonomy%2520that%2520classifies%2520existing%2520approaches%2520into%2520four%2520groups%2520based%250Aon%2520the%2520type%2520of%2520textual%2520data%2520employed%2520for%2520time%2520series%2520modeling.%2520We%2520then%250Asummarize%2520key%2520cross-modality%2520strategies%252C%2520e.g.%252C%2520alignment%2520and%2520fusion%252C%2520and%250Adiscuss%2520their%2520applications%2520across%2520a%2520range%2520of%2520downstream%2520tasks.%2520Furthermore%252C%2520we%250Aconduct%2520experiments%2520on%2520multimodal%2520datasets%2520from%2520different%2520application%2520domains%250Ato%2520investigate%2520effective%2520combinations%2520of%2520textual%2520data%2520and%2520cross-modality%250Astrategies%2520for%2520enhancing%2520time%2520series%2520analytics.%2520Finally%252C%2520we%2520suggest%2520several%250Apromising%2520directions%2520for%2520future%2520research.%2520This%2520survey%2520is%2520designed%2520for%2520a%2520range%250Aof%2520professionals%252C%2520researchers%252C%2520and%2520practitioners%2520interested%2520in%2520LLM-based%2520time%250Aseries%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02583v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Cross-Modality%20Modeling%20for%20Time%20Series%20Analytics%3A%20A%20Survey%20in%0A%20%20the%20LLM%20Era&entry.906535625=Chenxi%20Liu%20and%20Shaowen%20Zhou%20and%20Qianxiong%20Xu%20and%20Hao%20Miao%20and%20Cheng%20Long%20and%20Ziyue%20Li%20and%20Rui%20Zhao&entry.1292438233=%20%20The%20proliferation%20of%20edge%20devices%20has%20generated%20an%20unprecedented%20volume%20of%0Atime%20series%20data%20across%20different%20domains%2C%20motivating%20various%20well-customized%0Amethods.%20Recently%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20emerged%20as%20a%20new%20paradigm%0Afor%20time%20series%20analytics%20by%20leveraging%20the%20shared%20sequential%20nature%20of%20textual%0Adata%20and%20time%20series.%20However%2C%20a%20fundamental%20cross-modality%20gap%20between%20time%0Aseries%20and%20LLMs%20exists%2C%20as%20LLMs%20are%20pre-trained%20on%20textual%20corpora%20and%20are%20not%0Ainherently%20optimized%20for%20time%20series.%20Many%20recent%20proposals%20are%20designed%20to%0Aaddress%20this%20issue.%20In%20this%20survey%2C%20we%20provide%20an%20up-to-date%20overview%20of%0ALLMs-based%20cross-modality%20modeling%20for%20time%20series%20analytics.%20We%20first%0Aintroduce%20a%20taxonomy%20that%20classifies%20existing%20approaches%20into%20four%20groups%20based%0Aon%20the%20type%20of%20textual%20data%20employed%20for%20time%20series%20modeling.%20We%20then%0Asummarize%20key%20cross-modality%20strategies%2C%20e.g.%2C%20alignment%20and%20fusion%2C%20and%0Adiscuss%20their%20applications%20across%20a%20range%20of%20downstream%20tasks.%20Furthermore%2C%20we%0Aconduct%20experiments%20on%20multimodal%20datasets%20from%20different%20application%20domains%0Ato%20investigate%20effective%20combinations%20of%20textual%20data%20and%20cross-modality%0Astrategies%20for%20enhancing%20time%20series%20analytics.%20Finally%2C%20we%20suggest%20several%0Apromising%20directions%20for%20future%20research.%20This%20survey%20is%20designed%20for%20a%20range%0Aof%20professionals%2C%20researchers%2C%20and%20practitioners%20interested%20in%20LLM-based%20time%0Aseries%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02583v1&entry.124074799=Read"},
{"title": "Un-Straightening Generative AI: How Queer Artists Surface and Challenge\n  the Normativity of Generative AI Models", "author": "Jordan Taylor and Joel Mire and Franchesca Spektor and Alicia DeVrio and Maarten Sap and Haiyi Zhu and Sarah Fox", "abstract": "  Queer people are often discussed as targets of bias, harm, or discrimination\nin research on generative AI. However, the specific ways that queer people\nengage with generative AI, and thus possible uses that support queer people,\nhave yet to be explored. We conducted a workshop study with 13 queer artists,\nduring which we gave participants access to GPT-4 and DALL-E 3 and facilitated\ngroup sensemaking activities. We found our participants struggled to use these\nmodels due to various normative values embedded in their designs, such as\nhyper-positivity and anti-sexuality. We describe various strategies our\nparticipants developed to overcome these models' limitations and how,\nnevertheless, our participants found value in these highly-normative\ntechnologies. Drawing on queer feminist theory, we discuss implications for the\nconceptualization of \"state-of-the-art\" models and consider how FAccT\nresearchers might support queer alternatives.\n", "link": "http://arxiv.org/abs/2503.09805v2", "date": "2025-05-05", "relevancy": 2.016, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5273}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5095}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4892}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Un-Straightening%20Generative%20AI%3A%20How%20Queer%20Artists%20Surface%20and%20Challenge%0A%20%20the%20Normativity%20of%20Generative%20AI%20Models&body=Title%3A%20Un-Straightening%20Generative%20AI%3A%20How%20Queer%20Artists%20Surface%20and%20Challenge%0A%20%20the%20Normativity%20of%20Generative%20AI%20Models%0AAuthor%3A%20Jordan%20Taylor%20and%20Joel%20Mire%20and%20Franchesca%20Spektor%20and%20Alicia%20DeVrio%20and%20Maarten%20Sap%20and%20Haiyi%20Zhu%20and%20Sarah%20Fox%0AAbstract%3A%20%20%20Queer%20people%20are%20often%20discussed%20as%20targets%20of%20bias%2C%20harm%2C%20or%20discrimination%0Ain%20research%20on%20generative%20AI.%20However%2C%20the%20specific%20ways%20that%20queer%20people%0Aengage%20with%20generative%20AI%2C%20and%20thus%20possible%20uses%20that%20support%20queer%20people%2C%0Ahave%20yet%20to%20be%20explored.%20We%20conducted%20a%20workshop%20study%20with%2013%20queer%20artists%2C%0Aduring%20which%20we%20gave%20participants%20access%20to%20GPT-4%20and%20DALL-E%203%20and%20facilitated%0Agroup%20sensemaking%20activities.%20We%20found%20our%20participants%20struggled%20to%20use%20these%0Amodels%20due%20to%20various%20normative%20values%20embedded%20in%20their%20designs%2C%20such%20as%0Ahyper-positivity%20and%20anti-sexuality.%20We%20describe%20various%20strategies%20our%0Aparticipants%20developed%20to%20overcome%20these%20models%27%20limitations%20and%20how%2C%0Anevertheless%2C%20our%20participants%20found%20value%20in%20these%20highly-normative%0Atechnologies.%20Drawing%20on%20queer%20feminist%20theory%2C%20we%20discuss%20implications%20for%20the%0Aconceptualization%20of%20%22state-of-the-art%22%20models%20and%20consider%20how%20FAccT%0Aresearchers%20might%20support%20queer%20alternatives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.09805v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUn-Straightening%2520Generative%2520AI%253A%2520How%2520Queer%2520Artists%2520Surface%2520and%2520Challenge%250A%2520%2520the%2520Normativity%2520of%2520Generative%2520AI%2520Models%26entry.906535625%3DJordan%2520Taylor%2520and%2520Joel%2520Mire%2520and%2520Franchesca%2520Spektor%2520and%2520Alicia%2520DeVrio%2520and%2520Maarten%2520Sap%2520and%2520Haiyi%2520Zhu%2520and%2520Sarah%2520Fox%26entry.1292438233%3D%2520%2520Queer%2520people%2520are%2520often%2520discussed%2520as%2520targets%2520of%2520bias%252C%2520harm%252C%2520or%2520discrimination%250Ain%2520research%2520on%2520generative%2520AI.%2520However%252C%2520the%2520specific%2520ways%2520that%2520queer%2520people%250Aengage%2520with%2520generative%2520AI%252C%2520and%2520thus%2520possible%2520uses%2520that%2520support%2520queer%2520people%252C%250Ahave%2520yet%2520to%2520be%2520explored.%2520We%2520conducted%2520a%2520workshop%2520study%2520with%252013%2520queer%2520artists%252C%250Aduring%2520which%2520we%2520gave%2520participants%2520access%2520to%2520GPT-4%2520and%2520DALL-E%25203%2520and%2520facilitated%250Agroup%2520sensemaking%2520activities.%2520We%2520found%2520our%2520participants%2520struggled%2520to%2520use%2520these%250Amodels%2520due%2520to%2520various%2520normative%2520values%2520embedded%2520in%2520their%2520designs%252C%2520such%2520as%250Ahyper-positivity%2520and%2520anti-sexuality.%2520We%2520describe%2520various%2520strategies%2520our%250Aparticipants%2520developed%2520to%2520overcome%2520these%2520models%2527%2520limitations%2520and%2520how%252C%250Anevertheless%252C%2520our%2520participants%2520found%2520value%2520in%2520these%2520highly-normative%250Atechnologies.%2520Drawing%2520on%2520queer%2520feminist%2520theory%252C%2520we%2520discuss%2520implications%2520for%2520the%250Aconceptualization%2520of%2520%2522state-of-the-art%2522%2520models%2520and%2520consider%2520how%2520FAccT%250Aresearchers%2520might%2520support%2520queer%2520alternatives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.09805v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Un-Straightening%20Generative%20AI%3A%20How%20Queer%20Artists%20Surface%20and%20Challenge%0A%20%20the%20Normativity%20of%20Generative%20AI%20Models&entry.906535625=Jordan%20Taylor%20and%20Joel%20Mire%20and%20Franchesca%20Spektor%20and%20Alicia%20DeVrio%20and%20Maarten%20Sap%20and%20Haiyi%20Zhu%20and%20Sarah%20Fox&entry.1292438233=%20%20Queer%20people%20are%20often%20discussed%20as%20targets%20of%20bias%2C%20harm%2C%20or%20discrimination%0Ain%20research%20on%20generative%20AI.%20However%2C%20the%20specific%20ways%20that%20queer%20people%0Aengage%20with%20generative%20AI%2C%20and%20thus%20possible%20uses%20that%20support%20queer%20people%2C%0Ahave%20yet%20to%20be%20explored.%20We%20conducted%20a%20workshop%20study%20with%2013%20queer%20artists%2C%0Aduring%20which%20we%20gave%20participants%20access%20to%20GPT-4%20and%20DALL-E%203%20and%20facilitated%0Agroup%20sensemaking%20activities.%20We%20found%20our%20participants%20struggled%20to%20use%20these%0Amodels%20due%20to%20various%20normative%20values%20embedded%20in%20their%20designs%2C%20such%20as%0Ahyper-positivity%20and%20anti-sexuality.%20We%20describe%20various%20strategies%20our%0Aparticipants%20developed%20to%20overcome%20these%20models%27%20limitations%20and%20how%2C%0Anevertheless%2C%20our%20participants%20found%20value%20in%20these%20highly-normative%0Atechnologies.%20Drawing%20on%20queer%20feminist%20theory%2C%20we%20discuss%20implications%20for%20the%0Aconceptualization%20of%20%22state-of-the-art%22%20models%20and%20consider%20how%20FAccT%0Aresearchers%20might%20support%20queer%20alternatives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.09805v2&entry.124074799=Read"},
{"title": "EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and\n  Flexible LLM Fine-Tuning", "author": "Lingxiao Kong and Cong Yang and Susanne Neufang and Oya Deniz Beyan and Zeyd Boukhers", "abstract": "  Recent advances in reinforcement learning (RL) for large language model (LLM)\nfine-tuning show promise in addressing multi-objective tasks but still face\nsignificant challenges, including complex objective balancing, low training\nefficiency, poor scalability, and limited explainability. Leveraging ensemble\nlearning principles, we introduce an Ensemble Multi-Objective RL (EMORL)\nframework that fine-tunes multiple models with individual objectives while\noptimizing their aggregation after the training to improve efficiency and\nflexibility. Our method is the first to aggregate the last hidden states of\nindividual models, incorporating contextual information from multiple\nobjectives. This approach is supported by a hierarchical grid search algorithm\nthat identifies optimal weighted combinations. We evaluate EMORL on counselor\nreflection generation tasks, using text-scoring LLMs to evaluate the\ngenerations and provide rewards during RL fine-tuning. Through comprehensive\nexperiments on the PAIR and Psych8k datasets, we demonstrate the advantages of\nEMORL against existing baselines: significantly lower and more stable training\nconsumption ($17,529\\pm 1,650$ data points and $6,573\\pm 147.43$ seconds),\nimproved scalability and explainability, and comparable performance across\nmultiple objectives.\n", "link": "http://arxiv.org/abs/2505.02579v1", "date": "2025-05-05", "relevancy": 2.0089, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5414}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5037}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EMORL%3A%20Ensemble%20Multi-Objective%20Reinforcement%20Learning%20for%20Efficient%20and%0A%20%20Flexible%20LLM%20Fine-Tuning&body=Title%3A%20EMORL%3A%20Ensemble%20Multi-Objective%20Reinforcement%20Learning%20for%20Efficient%20and%0A%20%20Flexible%20LLM%20Fine-Tuning%0AAuthor%3A%20Lingxiao%20Kong%20and%20Cong%20Yang%20and%20Susanne%20Neufang%20and%20Oya%20Deniz%20Beyan%20and%20Zeyd%20Boukhers%0AAbstract%3A%20%20%20Recent%20advances%20in%20reinforcement%20learning%20%28RL%29%20for%20large%20language%20model%20%28LLM%29%0Afine-tuning%20show%20promise%20in%20addressing%20multi-objective%20tasks%20but%20still%20face%0Asignificant%20challenges%2C%20including%20complex%20objective%20balancing%2C%20low%20training%0Aefficiency%2C%20poor%20scalability%2C%20and%20limited%20explainability.%20Leveraging%20ensemble%0Alearning%20principles%2C%20we%20introduce%20an%20Ensemble%20Multi-Objective%20RL%20%28EMORL%29%0Aframework%20that%20fine-tunes%20multiple%20models%20with%20individual%20objectives%20while%0Aoptimizing%20their%20aggregation%20after%20the%20training%20to%20improve%20efficiency%20and%0Aflexibility.%20Our%20method%20is%20the%20first%20to%20aggregate%20the%20last%20hidden%20states%20of%0Aindividual%20models%2C%20incorporating%20contextual%20information%20from%20multiple%0Aobjectives.%20This%20approach%20is%20supported%20by%20a%20hierarchical%20grid%20search%20algorithm%0Athat%20identifies%20optimal%20weighted%20combinations.%20We%20evaluate%20EMORL%20on%20counselor%0Areflection%20generation%20tasks%2C%20using%20text-scoring%20LLMs%20to%20evaluate%20the%0Agenerations%20and%20provide%20rewards%20during%20RL%20fine-tuning.%20Through%20comprehensive%0Aexperiments%20on%20the%20PAIR%20and%20Psych8k%20datasets%2C%20we%20demonstrate%20the%20advantages%20of%0AEMORL%20against%20existing%20baselines%3A%20significantly%20lower%20and%20more%20stable%20training%0Aconsumption%20%28%2417%2C529%5Cpm%201%2C650%24%20data%20points%20and%20%246%2C573%5Cpm%20147.43%24%20seconds%29%2C%0Aimproved%20scalability%20and%20explainability%2C%20and%20comparable%20performance%20across%0Amultiple%20objectives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02579v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEMORL%253A%2520Ensemble%2520Multi-Objective%2520Reinforcement%2520Learning%2520for%2520Efficient%2520and%250A%2520%2520Flexible%2520LLM%2520Fine-Tuning%26entry.906535625%3DLingxiao%2520Kong%2520and%2520Cong%2520Yang%2520and%2520Susanne%2520Neufang%2520and%2520Oya%2520Deniz%2520Beyan%2520and%2520Zeyd%2520Boukhers%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520reinforcement%2520learning%2520%2528RL%2529%2520for%2520large%2520language%2520model%2520%2528LLM%2529%250Afine-tuning%2520show%2520promise%2520in%2520addressing%2520multi-objective%2520tasks%2520but%2520still%2520face%250Asignificant%2520challenges%252C%2520including%2520complex%2520objective%2520balancing%252C%2520low%2520training%250Aefficiency%252C%2520poor%2520scalability%252C%2520and%2520limited%2520explainability.%2520Leveraging%2520ensemble%250Alearning%2520principles%252C%2520we%2520introduce%2520an%2520Ensemble%2520Multi-Objective%2520RL%2520%2528EMORL%2529%250Aframework%2520that%2520fine-tunes%2520multiple%2520models%2520with%2520individual%2520objectives%2520while%250Aoptimizing%2520their%2520aggregation%2520after%2520the%2520training%2520to%2520improve%2520efficiency%2520and%250Aflexibility.%2520Our%2520method%2520is%2520the%2520first%2520to%2520aggregate%2520the%2520last%2520hidden%2520states%2520of%250Aindividual%2520models%252C%2520incorporating%2520contextual%2520information%2520from%2520multiple%250Aobjectives.%2520This%2520approach%2520is%2520supported%2520by%2520a%2520hierarchical%2520grid%2520search%2520algorithm%250Athat%2520identifies%2520optimal%2520weighted%2520combinations.%2520We%2520evaluate%2520EMORL%2520on%2520counselor%250Areflection%2520generation%2520tasks%252C%2520using%2520text-scoring%2520LLMs%2520to%2520evaluate%2520the%250Agenerations%2520and%2520provide%2520rewards%2520during%2520RL%2520fine-tuning.%2520Through%2520comprehensive%250Aexperiments%2520on%2520the%2520PAIR%2520and%2520Psych8k%2520datasets%252C%2520we%2520demonstrate%2520the%2520advantages%2520of%250AEMORL%2520against%2520existing%2520baselines%253A%2520significantly%2520lower%2520and%2520more%2520stable%2520training%250Aconsumption%2520%2528%252417%252C529%255Cpm%25201%252C650%2524%2520data%2520points%2520and%2520%25246%252C573%255Cpm%2520147.43%2524%2520seconds%2529%252C%250Aimproved%2520scalability%2520and%2520explainability%252C%2520and%2520comparable%2520performance%2520across%250Amultiple%2520objectives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02579v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EMORL%3A%20Ensemble%20Multi-Objective%20Reinforcement%20Learning%20for%20Efficient%20and%0A%20%20Flexible%20LLM%20Fine-Tuning&entry.906535625=Lingxiao%20Kong%20and%20Cong%20Yang%20and%20Susanne%20Neufang%20and%20Oya%20Deniz%20Beyan%20and%20Zeyd%20Boukhers&entry.1292438233=%20%20Recent%20advances%20in%20reinforcement%20learning%20%28RL%29%20for%20large%20language%20model%20%28LLM%29%0Afine-tuning%20show%20promise%20in%20addressing%20multi-objective%20tasks%20but%20still%20face%0Asignificant%20challenges%2C%20including%20complex%20objective%20balancing%2C%20low%20training%0Aefficiency%2C%20poor%20scalability%2C%20and%20limited%20explainability.%20Leveraging%20ensemble%0Alearning%20principles%2C%20we%20introduce%20an%20Ensemble%20Multi-Objective%20RL%20%28EMORL%29%0Aframework%20that%20fine-tunes%20multiple%20models%20with%20individual%20objectives%20while%0Aoptimizing%20their%20aggregation%20after%20the%20training%20to%20improve%20efficiency%20and%0Aflexibility.%20Our%20method%20is%20the%20first%20to%20aggregate%20the%20last%20hidden%20states%20of%0Aindividual%20models%2C%20incorporating%20contextual%20information%20from%20multiple%0Aobjectives.%20This%20approach%20is%20supported%20by%20a%20hierarchical%20grid%20search%20algorithm%0Athat%20identifies%20optimal%20weighted%20combinations.%20We%20evaluate%20EMORL%20on%20counselor%0Areflection%20generation%20tasks%2C%20using%20text-scoring%20LLMs%20to%20evaluate%20the%0Agenerations%20and%20provide%20rewards%20during%20RL%20fine-tuning.%20Through%20comprehensive%0Aexperiments%20on%20the%20PAIR%20and%20Psych8k%20datasets%2C%20we%20demonstrate%20the%20advantages%20of%0AEMORL%20against%20existing%20baselines%3A%20significantly%20lower%20and%20more%20stable%20training%0Aconsumption%20%28%2417%2C529%5Cpm%201%2C650%24%20data%20points%20and%20%246%2C573%5Cpm%20147.43%24%20seconds%29%2C%0Aimproved%20scalability%20and%20explainability%2C%20and%20comparable%20performance%20across%0Amultiple%20objectives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02579v1&entry.124074799=Read"},
{"title": "Energy-Efficient Flying LoRa Gateways: A Multi-Agent Reinforcement\n  Learning Approach", "author": "Abdullahi Isa Ahmed and Jamal Bentahar and El Mehdi Amhoud", "abstract": "  As next-generation Internet of Things (NG-IoT) networks continue to grow, the\nnumber of connected devices is rapidly increasing, along with their energy\ndemands. This creates challenges for resource management and sustainability.\nEnergy-efficient communication, particularly for power-limited IoT devices, is\ntherefore a key research focus. In this paper, we deployed flying LoRa gateways\nmounted on unmanned aerial vehicles (UAVs) to collect data from LoRa end\ndevices and transmit it to a central server. Our primary objective is to\nmaximize the global system energy efficiency of wireless LoRa networks by joint\noptimization of transmission power, spreading factor, bandwidth, and user\nassociation. To solve this challenging problem, we model the problem as a\npartially observable Markov decision process (POMDP), where each flying LoRa GW\nacts as a learning agent using a cooperative multi-agent reinforcement learning\n(MARL). Simulation results demonstrate that our proposed method, based on the\nmulti-agent proximal policy optimization algorithm, significantly improves the\nglobal system energy efficiency and surpasses the popular MARL and other\nconventional schemes.\n", "link": "http://arxiv.org/abs/2502.03377v3", "date": "2025-05-05", "relevancy": 2.0074, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.535}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5051}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4675}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Energy-Efficient%20Flying%20LoRa%20Gateways%3A%20A%20Multi-Agent%20Reinforcement%0A%20%20Learning%20Approach&body=Title%3A%20Energy-Efficient%20Flying%20LoRa%20Gateways%3A%20A%20Multi-Agent%20Reinforcement%0A%20%20Learning%20Approach%0AAuthor%3A%20Abdullahi%20Isa%20Ahmed%20and%20Jamal%20Bentahar%20and%20El%20Mehdi%20Amhoud%0AAbstract%3A%20%20%20As%20next-generation%20Internet%20of%20Things%20%28NG-IoT%29%20networks%20continue%20to%20grow%2C%20the%0Anumber%20of%20connected%20devices%20is%20rapidly%20increasing%2C%20along%20with%20their%20energy%0Ademands.%20This%20creates%20challenges%20for%20resource%20management%20and%20sustainability.%0AEnergy-efficient%20communication%2C%20particularly%20for%20power-limited%20IoT%20devices%2C%20is%0Atherefore%20a%20key%20research%20focus.%20In%20this%20paper%2C%20we%20deployed%20flying%20LoRa%20gateways%0Amounted%20on%20unmanned%20aerial%20vehicles%20%28UAVs%29%20to%20collect%20data%20from%20LoRa%20end%0Adevices%20and%20transmit%20it%20to%20a%20central%20server.%20Our%20primary%20objective%20is%20to%0Amaximize%20the%20global%20system%20energy%20efficiency%20of%20wireless%20LoRa%20networks%20by%20joint%0Aoptimization%20of%20transmission%20power%2C%20spreading%20factor%2C%20bandwidth%2C%20and%20user%0Aassociation.%20To%20solve%20this%20challenging%20problem%2C%20we%20model%20the%20problem%20as%20a%0Apartially%20observable%20Markov%20decision%20process%20%28POMDP%29%2C%20where%20each%20flying%20LoRa%20GW%0Aacts%20as%20a%20learning%20agent%20using%20a%20cooperative%20multi-agent%20reinforcement%20learning%0A%28MARL%29.%20Simulation%20results%20demonstrate%20that%20our%20proposed%20method%2C%20based%20on%20the%0Amulti-agent%20proximal%20policy%20optimization%20algorithm%2C%20significantly%20improves%20the%0Aglobal%20system%20energy%20efficiency%20and%20surpasses%20the%20popular%20MARL%20and%20other%0Aconventional%20schemes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03377v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnergy-Efficient%2520Flying%2520LoRa%2520Gateways%253A%2520A%2520Multi-Agent%2520Reinforcement%250A%2520%2520Learning%2520Approach%26entry.906535625%3DAbdullahi%2520Isa%2520Ahmed%2520and%2520Jamal%2520Bentahar%2520and%2520El%2520Mehdi%2520Amhoud%26entry.1292438233%3D%2520%2520As%2520next-generation%2520Internet%2520of%2520Things%2520%2528NG-IoT%2529%2520networks%2520continue%2520to%2520grow%252C%2520the%250Anumber%2520of%2520connected%2520devices%2520is%2520rapidly%2520increasing%252C%2520along%2520with%2520their%2520energy%250Ademands.%2520This%2520creates%2520challenges%2520for%2520resource%2520management%2520and%2520sustainability.%250AEnergy-efficient%2520communication%252C%2520particularly%2520for%2520power-limited%2520IoT%2520devices%252C%2520is%250Atherefore%2520a%2520key%2520research%2520focus.%2520In%2520this%2520paper%252C%2520we%2520deployed%2520flying%2520LoRa%2520gateways%250Amounted%2520on%2520unmanned%2520aerial%2520vehicles%2520%2528UAVs%2529%2520to%2520collect%2520data%2520from%2520LoRa%2520end%250Adevices%2520and%2520transmit%2520it%2520to%2520a%2520central%2520server.%2520Our%2520primary%2520objective%2520is%2520to%250Amaximize%2520the%2520global%2520system%2520energy%2520efficiency%2520of%2520wireless%2520LoRa%2520networks%2520by%2520joint%250Aoptimization%2520of%2520transmission%2520power%252C%2520spreading%2520factor%252C%2520bandwidth%252C%2520and%2520user%250Aassociation.%2520To%2520solve%2520this%2520challenging%2520problem%252C%2520we%2520model%2520the%2520problem%2520as%2520a%250Apartially%2520observable%2520Markov%2520decision%2520process%2520%2528POMDP%2529%252C%2520where%2520each%2520flying%2520LoRa%2520GW%250Aacts%2520as%2520a%2520learning%2520agent%2520using%2520a%2520cooperative%2520multi-agent%2520reinforcement%2520learning%250A%2528MARL%2529.%2520Simulation%2520results%2520demonstrate%2520that%2520our%2520proposed%2520method%252C%2520based%2520on%2520the%250Amulti-agent%2520proximal%2520policy%2520optimization%2520algorithm%252C%2520significantly%2520improves%2520the%250Aglobal%2520system%2520energy%2520efficiency%2520and%2520surpasses%2520the%2520popular%2520MARL%2520and%2520other%250Aconventional%2520schemes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03377v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Energy-Efficient%20Flying%20LoRa%20Gateways%3A%20A%20Multi-Agent%20Reinforcement%0A%20%20Learning%20Approach&entry.906535625=Abdullahi%20Isa%20Ahmed%20and%20Jamal%20Bentahar%20and%20El%20Mehdi%20Amhoud&entry.1292438233=%20%20As%20next-generation%20Internet%20of%20Things%20%28NG-IoT%29%20networks%20continue%20to%20grow%2C%20the%0Anumber%20of%20connected%20devices%20is%20rapidly%20increasing%2C%20along%20with%20their%20energy%0Ademands.%20This%20creates%20challenges%20for%20resource%20management%20and%20sustainability.%0AEnergy-efficient%20communication%2C%20particularly%20for%20power-limited%20IoT%20devices%2C%20is%0Atherefore%20a%20key%20research%20focus.%20In%20this%20paper%2C%20we%20deployed%20flying%20LoRa%20gateways%0Amounted%20on%20unmanned%20aerial%20vehicles%20%28UAVs%29%20to%20collect%20data%20from%20LoRa%20end%0Adevices%20and%20transmit%20it%20to%20a%20central%20server.%20Our%20primary%20objective%20is%20to%0Amaximize%20the%20global%20system%20energy%20efficiency%20of%20wireless%20LoRa%20networks%20by%20joint%0Aoptimization%20of%20transmission%20power%2C%20spreading%20factor%2C%20bandwidth%2C%20and%20user%0Aassociation.%20To%20solve%20this%20challenging%20problem%2C%20we%20model%20the%20problem%20as%20a%0Apartially%20observable%20Markov%20decision%20process%20%28POMDP%29%2C%20where%20each%20flying%20LoRa%20GW%0Aacts%20as%20a%20learning%20agent%20using%20a%20cooperative%20multi-agent%20reinforcement%20learning%0A%28MARL%29.%20Simulation%20results%20demonstrate%20that%20our%20proposed%20method%2C%20based%20on%20the%0Amulti-agent%20proximal%20policy%20optimization%20algorithm%2C%20significantly%20improves%20the%0Aglobal%20system%20energy%20efficiency%20and%20surpasses%20the%20popular%20MARL%20and%20other%0Aconventional%20schemes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03377v3&entry.124074799=Read"},
{"title": "R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement\n  Learning", "author": "Yi-Fan Zhang and Xingyu Lu and Xiao Hu and Chaoyou Fu and Bin Wen and Tianke Zhang and Changyi Liu and Kaiyu Jiang and Kaibing Chen and Kaiyu Tang and Haojie Ding and Jiankang Chen and Fan Yang and Zhang Zhang and Tingting Gao and Liang Wang", "abstract": "  Multimodal Reward Models (MRMs) play a crucial role in enhancing the\nperformance of Multimodal Large Language Models (MLLMs). While recent\nadvancements have primarily focused on improving the model structure and\ntraining data of MRMs, there has been limited exploration into the\neffectiveness of long-term reasoning capabilities for reward modeling and how\nto activate these capabilities in MRMs. In this paper, we explore how\nReinforcement Learning (RL) can be used to improve reward modeling.\nSpecifically, we reformulate the reward modeling problem as a rule-based RL\ntask. However, we observe that directly applying existing RL algorithms, such\nas Reinforce++, to reward modeling often leads to training instability or even\ncollapse due to the inherent limitations of these algorithms. To address this\nissue, we propose the StableReinforce algorithm, which refines the training\nloss, advantage estimation strategy, and reward design of existing RL methods.\nThese refinements result in more stable training dynamics and superior\nperformance. To facilitate MRM training, we collect 200K preference data from\ndiverse datasets. Our reward model, R1-Reward, trained using the\nStableReinforce algorithm on this dataset, significantly improves performance\non multimodal reward modeling benchmarks. Compared to previous SOTA models,\nR1-Reward achieves a $8.4\\%$ improvement on the VL Reward-Bench and a $14.3\\%$\nimprovement on the Multimodal Reward Bench. Moreover, with more inference\ncompute, R1-Reward's performance is further enhanced, highlighting the\npotential of RL algorithms in optimizing MRMs.\n", "link": "http://arxiv.org/abs/2505.02835v1", "date": "2025-05-05", "relevancy": 2.0035, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5338}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5141}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20R1-Reward%3A%20Training%20Multimodal%20Reward%20Model%20Through%20Stable%20Reinforcement%0A%20%20Learning&body=Title%3A%20R1-Reward%3A%20Training%20Multimodal%20Reward%20Model%20Through%20Stable%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Yi-Fan%20Zhang%20and%20Xingyu%20Lu%20and%20Xiao%20Hu%20and%20Chaoyou%20Fu%20and%20Bin%20Wen%20and%20Tianke%20Zhang%20and%20Changyi%20Liu%20and%20Kaiyu%20Jiang%20and%20Kaibing%20Chen%20and%20Kaiyu%20Tang%20and%20Haojie%20Ding%20and%20Jiankang%20Chen%20and%20Fan%20Yang%20and%20Zhang%20Zhang%20and%20Tingting%20Gao%20and%20Liang%20Wang%0AAbstract%3A%20%20%20Multimodal%20Reward%20Models%20%28MRMs%29%20play%20a%20crucial%20role%20in%20enhancing%20the%0Aperformance%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29.%20While%20recent%0Aadvancements%20have%20primarily%20focused%20on%20improving%20the%20model%20structure%20and%0Atraining%20data%20of%20MRMs%2C%20there%20has%20been%20limited%20exploration%20into%20the%0Aeffectiveness%20of%20long-term%20reasoning%20capabilities%20for%20reward%20modeling%20and%20how%0Ato%20activate%20these%20capabilities%20in%20MRMs.%20In%20this%20paper%2C%20we%20explore%20how%0AReinforcement%20Learning%20%28RL%29%20can%20be%20used%20to%20improve%20reward%20modeling.%0ASpecifically%2C%20we%20reformulate%20the%20reward%20modeling%20problem%20as%20a%20rule-based%20RL%0Atask.%20However%2C%20we%20observe%20that%20directly%20applying%20existing%20RL%20algorithms%2C%20such%0Aas%20Reinforce%2B%2B%2C%20to%20reward%20modeling%20often%20leads%20to%20training%20instability%20or%20even%0Acollapse%20due%20to%20the%20inherent%20limitations%20of%20these%20algorithms.%20To%20address%20this%0Aissue%2C%20we%20propose%20the%20StableReinforce%20algorithm%2C%20which%20refines%20the%20training%0Aloss%2C%20advantage%20estimation%20strategy%2C%20and%20reward%20design%20of%20existing%20RL%20methods.%0AThese%20refinements%20result%20in%20more%20stable%20training%20dynamics%20and%20superior%0Aperformance.%20To%20facilitate%20MRM%20training%2C%20we%20collect%20200K%20preference%20data%20from%0Adiverse%20datasets.%20Our%20reward%20model%2C%20R1-Reward%2C%20trained%20using%20the%0AStableReinforce%20algorithm%20on%20this%20dataset%2C%20significantly%20improves%20performance%0Aon%20multimodal%20reward%20modeling%20benchmarks.%20Compared%20to%20previous%20SOTA%20models%2C%0AR1-Reward%20achieves%20a%20%248.4%5C%25%24%20improvement%20on%20the%20VL%20Reward-Bench%20and%20a%20%2414.3%5C%25%24%0Aimprovement%20on%20the%20Multimodal%20Reward%20Bench.%20Moreover%2C%20with%20more%20inference%0Acompute%2C%20R1-Reward%27s%20performance%20is%20further%20enhanced%2C%20highlighting%20the%0Apotential%20of%20RL%20algorithms%20in%20optimizing%20MRMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02835v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DR1-Reward%253A%2520Training%2520Multimodal%2520Reward%2520Model%2520Through%2520Stable%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DYi-Fan%2520Zhang%2520and%2520Xingyu%2520Lu%2520and%2520Xiao%2520Hu%2520and%2520Chaoyou%2520Fu%2520and%2520Bin%2520Wen%2520and%2520Tianke%2520Zhang%2520and%2520Changyi%2520Liu%2520and%2520Kaiyu%2520Jiang%2520and%2520Kaibing%2520Chen%2520and%2520Kaiyu%2520Tang%2520and%2520Haojie%2520Ding%2520and%2520Jiankang%2520Chen%2520and%2520Fan%2520Yang%2520and%2520Zhang%2520Zhang%2520and%2520Tingting%2520Gao%2520and%2520Liang%2520Wang%26entry.1292438233%3D%2520%2520Multimodal%2520Reward%2520Models%2520%2528MRMs%2529%2520play%2520a%2520crucial%2520role%2520in%2520enhancing%2520the%250Aperformance%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529.%2520While%2520recent%250Aadvancements%2520have%2520primarily%2520focused%2520on%2520improving%2520the%2520model%2520structure%2520and%250Atraining%2520data%2520of%2520MRMs%252C%2520there%2520has%2520been%2520limited%2520exploration%2520into%2520the%250Aeffectiveness%2520of%2520long-term%2520reasoning%2520capabilities%2520for%2520reward%2520modeling%2520and%2520how%250Ato%2520activate%2520these%2520capabilities%2520in%2520MRMs.%2520In%2520this%2520paper%252C%2520we%2520explore%2520how%250AReinforcement%2520Learning%2520%2528RL%2529%2520can%2520be%2520used%2520to%2520improve%2520reward%2520modeling.%250ASpecifically%252C%2520we%2520reformulate%2520the%2520reward%2520modeling%2520problem%2520as%2520a%2520rule-based%2520RL%250Atask.%2520However%252C%2520we%2520observe%2520that%2520directly%2520applying%2520existing%2520RL%2520algorithms%252C%2520such%250Aas%2520Reinforce%252B%252B%252C%2520to%2520reward%2520modeling%2520often%2520leads%2520to%2520training%2520instability%2520or%2520even%250Acollapse%2520due%2520to%2520the%2520inherent%2520limitations%2520of%2520these%2520algorithms.%2520To%2520address%2520this%250Aissue%252C%2520we%2520propose%2520the%2520StableReinforce%2520algorithm%252C%2520which%2520refines%2520the%2520training%250Aloss%252C%2520advantage%2520estimation%2520strategy%252C%2520and%2520reward%2520design%2520of%2520existing%2520RL%2520methods.%250AThese%2520refinements%2520result%2520in%2520more%2520stable%2520training%2520dynamics%2520and%2520superior%250Aperformance.%2520To%2520facilitate%2520MRM%2520training%252C%2520we%2520collect%2520200K%2520preference%2520data%2520from%250Adiverse%2520datasets.%2520Our%2520reward%2520model%252C%2520R1-Reward%252C%2520trained%2520using%2520the%250AStableReinforce%2520algorithm%2520on%2520this%2520dataset%252C%2520significantly%2520improves%2520performance%250Aon%2520multimodal%2520reward%2520modeling%2520benchmarks.%2520Compared%2520to%2520previous%2520SOTA%2520models%252C%250AR1-Reward%2520achieves%2520a%2520%25248.4%255C%2525%2524%2520improvement%2520on%2520the%2520VL%2520Reward-Bench%2520and%2520a%2520%252414.3%255C%2525%2524%250Aimprovement%2520on%2520the%2520Multimodal%2520Reward%2520Bench.%2520Moreover%252C%2520with%2520more%2520inference%250Acompute%252C%2520R1-Reward%2527s%2520performance%2520is%2520further%2520enhanced%252C%2520highlighting%2520the%250Apotential%2520of%2520RL%2520algorithms%2520in%2520optimizing%2520MRMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02835v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=R1-Reward%3A%20Training%20Multimodal%20Reward%20Model%20Through%20Stable%20Reinforcement%0A%20%20Learning&entry.906535625=Yi-Fan%20Zhang%20and%20Xingyu%20Lu%20and%20Xiao%20Hu%20and%20Chaoyou%20Fu%20and%20Bin%20Wen%20and%20Tianke%20Zhang%20and%20Changyi%20Liu%20and%20Kaiyu%20Jiang%20and%20Kaibing%20Chen%20and%20Kaiyu%20Tang%20and%20Haojie%20Ding%20and%20Jiankang%20Chen%20and%20Fan%20Yang%20and%20Zhang%20Zhang%20and%20Tingting%20Gao%20and%20Liang%20Wang&entry.1292438233=%20%20Multimodal%20Reward%20Models%20%28MRMs%29%20play%20a%20crucial%20role%20in%20enhancing%20the%0Aperformance%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29.%20While%20recent%0Aadvancements%20have%20primarily%20focused%20on%20improving%20the%20model%20structure%20and%0Atraining%20data%20of%20MRMs%2C%20there%20has%20been%20limited%20exploration%20into%20the%0Aeffectiveness%20of%20long-term%20reasoning%20capabilities%20for%20reward%20modeling%20and%20how%0Ato%20activate%20these%20capabilities%20in%20MRMs.%20In%20this%20paper%2C%20we%20explore%20how%0AReinforcement%20Learning%20%28RL%29%20can%20be%20used%20to%20improve%20reward%20modeling.%0ASpecifically%2C%20we%20reformulate%20the%20reward%20modeling%20problem%20as%20a%20rule-based%20RL%0Atask.%20However%2C%20we%20observe%20that%20directly%20applying%20existing%20RL%20algorithms%2C%20such%0Aas%20Reinforce%2B%2B%2C%20to%20reward%20modeling%20often%20leads%20to%20training%20instability%20or%20even%0Acollapse%20due%20to%20the%20inherent%20limitations%20of%20these%20algorithms.%20To%20address%20this%0Aissue%2C%20we%20propose%20the%20StableReinforce%20algorithm%2C%20which%20refines%20the%20training%0Aloss%2C%20advantage%20estimation%20strategy%2C%20and%20reward%20design%20of%20existing%20RL%20methods.%0AThese%20refinements%20result%20in%20more%20stable%20training%20dynamics%20and%20superior%0Aperformance.%20To%20facilitate%20MRM%20training%2C%20we%20collect%20200K%20preference%20data%20from%0Adiverse%20datasets.%20Our%20reward%20model%2C%20R1-Reward%2C%20trained%20using%20the%0AStableReinforce%20algorithm%20on%20this%20dataset%2C%20significantly%20improves%20performance%0Aon%20multimodal%20reward%20modeling%20benchmarks.%20Compared%20to%20previous%20SOTA%20models%2C%0AR1-Reward%20achieves%20a%20%248.4%5C%25%24%20improvement%20on%20the%20VL%20Reward-Bench%20and%20a%20%2414.3%5C%25%24%0Aimprovement%20on%20the%20Multimodal%20Reward%20Bench.%20Moreover%2C%20with%20more%20inference%0Acompute%2C%20R1-Reward%27s%20performance%20is%20further%20enhanced%2C%20highlighting%20the%0Apotential%20of%20RL%20algorithms%20in%20optimizing%20MRMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02835v1&entry.124074799=Read"},
{"title": "Backpropagation through space, time, and the brain", "author": "Benjamin Ellenberger and Paul Haider and Jakob Jordan and Kevin Max and Ismael Jaras and Laura Kriener and Federico Benitez and Mihai A. Petrovici", "abstract": "  How physical networks of neurons, bound by spatio-temporal locality\nconstraints, can perform efficient credit assignment, remains, to a large\nextent, an open question. In machine learning, the answer is almost universally\ngiven by the error backpropagation algorithm, through both space and time.\nHowever, this algorithm is well-known to rely on biologically implausible\nassumptions, in particular with respect to spatio-temporal (non-)locality.\nAlternative forward-propagation models such as real-time recurrent learning\nonly partially solve the locality problem, but only at the cost of scaling, due\nto prohibitive storage requirements. We introduce Generalized Latent\nEquilibrium (GLE), a computational framework for fully local spatio-temporal\ncredit assignment in physical, dynamical networks of neurons. We start by\ndefining an energy based on neuron-local mismatches, from which we derive both\nneuronal dynamics via stationarity and parameter dynamics via gradient descent.\nThe resulting dynamics can be interpreted as a real-time, biologically\nplausible approximation of backpropagation through space and time in deep\ncortical networks with continuous-time neuronal dynamics and continuously\nactive, local synaptic plasticity. In particular, GLE exploits the morphology\nof dendritic trees to enable more complex information storage and processing in\nsingle neurons, as well as the ability of biological neurons to phase-shift\ntheir output rate with respect to their membrane potential, which is essential\nin both directions of information propagation. For the forward computation, it\nenables the mapping of time-continuous inputs to neuronal space, effectively\nperforming a spatio-temporal convolution. For the backward computation, it\npermits the temporal inversion of feedback signals, which consequently\napproximate the adjoint variables necessary for useful parameter updates.\n", "link": "http://arxiv.org/abs/2403.16933v3", "date": "2025-05-05", "relevancy": 2.0034, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.531}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5093}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Backpropagation%20through%20space%2C%20time%2C%20and%20the%20brain&body=Title%3A%20Backpropagation%20through%20space%2C%20time%2C%20and%20the%20brain%0AAuthor%3A%20Benjamin%20Ellenberger%20and%20Paul%20Haider%20and%20Jakob%20Jordan%20and%20Kevin%20Max%20and%20Ismael%20Jaras%20and%20Laura%20Kriener%20and%20Federico%20Benitez%20and%20Mihai%20A.%20Petrovici%0AAbstract%3A%20%20%20How%20physical%20networks%20of%20neurons%2C%20bound%20by%20spatio-temporal%20locality%0Aconstraints%2C%20can%20perform%20efficient%20credit%20assignment%2C%20remains%2C%20to%20a%20large%0Aextent%2C%20an%20open%20question.%20In%20machine%20learning%2C%20the%20answer%20is%20almost%20universally%0Agiven%20by%20the%20error%20backpropagation%20algorithm%2C%20through%20both%20space%20and%20time.%0AHowever%2C%20this%20algorithm%20is%20well-known%20to%20rely%20on%20biologically%20implausible%0Aassumptions%2C%20in%20particular%20with%20respect%20to%20spatio-temporal%20%28non-%29locality.%0AAlternative%20forward-propagation%20models%20such%20as%20real-time%20recurrent%20learning%0Aonly%20partially%20solve%20the%20locality%20problem%2C%20but%20only%20at%20the%20cost%20of%20scaling%2C%20due%0Ato%20prohibitive%20storage%20requirements.%20We%20introduce%20Generalized%20Latent%0AEquilibrium%20%28GLE%29%2C%20a%20computational%20framework%20for%20fully%20local%20spatio-temporal%0Acredit%20assignment%20in%20physical%2C%20dynamical%20networks%20of%20neurons.%20We%20start%20by%0Adefining%20an%20energy%20based%20on%20neuron-local%20mismatches%2C%20from%20which%20we%20derive%20both%0Aneuronal%20dynamics%20via%20stationarity%20and%20parameter%20dynamics%20via%20gradient%20descent.%0AThe%20resulting%20dynamics%20can%20be%20interpreted%20as%20a%20real-time%2C%20biologically%0Aplausible%20approximation%20of%20backpropagation%20through%20space%20and%20time%20in%20deep%0Acortical%20networks%20with%20continuous-time%20neuronal%20dynamics%20and%20continuously%0Aactive%2C%20local%20synaptic%20plasticity.%20In%20particular%2C%20GLE%20exploits%20the%20morphology%0Aof%20dendritic%20trees%20to%20enable%20more%20complex%20information%20storage%20and%20processing%20in%0Asingle%20neurons%2C%20as%20well%20as%20the%20ability%20of%20biological%20neurons%20to%20phase-shift%0Atheir%20output%20rate%20with%20respect%20to%20their%20membrane%20potential%2C%20which%20is%20essential%0Ain%20both%20directions%20of%20information%20propagation.%20For%20the%20forward%20computation%2C%20it%0Aenables%20the%20mapping%20of%20time-continuous%20inputs%20to%20neuronal%20space%2C%20effectively%0Aperforming%20a%20spatio-temporal%20convolution.%20For%20the%20backward%20computation%2C%20it%0Apermits%20the%20temporal%20inversion%20of%20feedback%20signals%2C%20which%20consequently%0Aapproximate%20the%20adjoint%20variables%20necessary%20for%20useful%20parameter%20updates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16933v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBackpropagation%2520through%2520space%252C%2520time%252C%2520and%2520the%2520brain%26entry.906535625%3DBenjamin%2520Ellenberger%2520and%2520Paul%2520Haider%2520and%2520Jakob%2520Jordan%2520and%2520Kevin%2520Max%2520and%2520Ismael%2520Jaras%2520and%2520Laura%2520Kriener%2520and%2520Federico%2520Benitez%2520and%2520Mihai%2520A.%2520Petrovici%26entry.1292438233%3D%2520%2520How%2520physical%2520networks%2520of%2520neurons%252C%2520bound%2520by%2520spatio-temporal%2520locality%250Aconstraints%252C%2520can%2520perform%2520efficient%2520credit%2520assignment%252C%2520remains%252C%2520to%2520a%2520large%250Aextent%252C%2520an%2520open%2520question.%2520In%2520machine%2520learning%252C%2520the%2520answer%2520is%2520almost%2520universally%250Agiven%2520by%2520the%2520error%2520backpropagation%2520algorithm%252C%2520through%2520both%2520space%2520and%2520time.%250AHowever%252C%2520this%2520algorithm%2520is%2520well-known%2520to%2520rely%2520on%2520biologically%2520implausible%250Aassumptions%252C%2520in%2520particular%2520with%2520respect%2520to%2520spatio-temporal%2520%2528non-%2529locality.%250AAlternative%2520forward-propagation%2520models%2520such%2520as%2520real-time%2520recurrent%2520learning%250Aonly%2520partially%2520solve%2520the%2520locality%2520problem%252C%2520but%2520only%2520at%2520the%2520cost%2520of%2520scaling%252C%2520due%250Ato%2520prohibitive%2520storage%2520requirements.%2520We%2520introduce%2520Generalized%2520Latent%250AEquilibrium%2520%2528GLE%2529%252C%2520a%2520computational%2520framework%2520for%2520fully%2520local%2520spatio-temporal%250Acredit%2520assignment%2520in%2520physical%252C%2520dynamical%2520networks%2520of%2520neurons.%2520We%2520start%2520by%250Adefining%2520an%2520energy%2520based%2520on%2520neuron-local%2520mismatches%252C%2520from%2520which%2520we%2520derive%2520both%250Aneuronal%2520dynamics%2520via%2520stationarity%2520and%2520parameter%2520dynamics%2520via%2520gradient%2520descent.%250AThe%2520resulting%2520dynamics%2520can%2520be%2520interpreted%2520as%2520a%2520real-time%252C%2520biologically%250Aplausible%2520approximation%2520of%2520backpropagation%2520through%2520space%2520and%2520time%2520in%2520deep%250Acortical%2520networks%2520with%2520continuous-time%2520neuronal%2520dynamics%2520and%2520continuously%250Aactive%252C%2520local%2520synaptic%2520plasticity.%2520In%2520particular%252C%2520GLE%2520exploits%2520the%2520morphology%250Aof%2520dendritic%2520trees%2520to%2520enable%2520more%2520complex%2520information%2520storage%2520and%2520processing%2520in%250Asingle%2520neurons%252C%2520as%2520well%2520as%2520the%2520ability%2520of%2520biological%2520neurons%2520to%2520phase-shift%250Atheir%2520output%2520rate%2520with%2520respect%2520to%2520their%2520membrane%2520potential%252C%2520which%2520is%2520essential%250Ain%2520both%2520directions%2520of%2520information%2520propagation.%2520For%2520the%2520forward%2520computation%252C%2520it%250Aenables%2520the%2520mapping%2520of%2520time-continuous%2520inputs%2520to%2520neuronal%2520space%252C%2520effectively%250Aperforming%2520a%2520spatio-temporal%2520convolution.%2520For%2520the%2520backward%2520computation%252C%2520it%250Apermits%2520the%2520temporal%2520inversion%2520of%2520feedback%2520signals%252C%2520which%2520consequently%250Aapproximate%2520the%2520adjoint%2520variables%2520necessary%2520for%2520useful%2520parameter%2520updates.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16933v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Backpropagation%20through%20space%2C%20time%2C%20and%20the%20brain&entry.906535625=Benjamin%20Ellenberger%20and%20Paul%20Haider%20and%20Jakob%20Jordan%20and%20Kevin%20Max%20and%20Ismael%20Jaras%20and%20Laura%20Kriener%20and%20Federico%20Benitez%20and%20Mihai%20A.%20Petrovici&entry.1292438233=%20%20How%20physical%20networks%20of%20neurons%2C%20bound%20by%20spatio-temporal%20locality%0Aconstraints%2C%20can%20perform%20efficient%20credit%20assignment%2C%20remains%2C%20to%20a%20large%0Aextent%2C%20an%20open%20question.%20In%20machine%20learning%2C%20the%20answer%20is%20almost%20universally%0Agiven%20by%20the%20error%20backpropagation%20algorithm%2C%20through%20both%20space%20and%20time.%0AHowever%2C%20this%20algorithm%20is%20well-known%20to%20rely%20on%20biologically%20implausible%0Aassumptions%2C%20in%20particular%20with%20respect%20to%20spatio-temporal%20%28non-%29locality.%0AAlternative%20forward-propagation%20models%20such%20as%20real-time%20recurrent%20learning%0Aonly%20partially%20solve%20the%20locality%20problem%2C%20but%20only%20at%20the%20cost%20of%20scaling%2C%20due%0Ato%20prohibitive%20storage%20requirements.%20We%20introduce%20Generalized%20Latent%0AEquilibrium%20%28GLE%29%2C%20a%20computational%20framework%20for%20fully%20local%20spatio-temporal%0Acredit%20assignment%20in%20physical%2C%20dynamical%20networks%20of%20neurons.%20We%20start%20by%0Adefining%20an%20energy%20based%20on%20neuron-local%20mismatches%2C%20from%20which%20we%20derive%20both%0Aneuronal%20dynamics%20via%20stationarity%20and%20parameter%20dynamics%20via%20gradient%20descent.%0AThe%20resulting%20dynamics%20can%20be%20interpreted%20as%20a%20real-time%2C%20biologically%0Aplausible%20approximation%20of%20backpropagation%20through%20space%20and%20time%20in%20deep%0Acortical%20networks%20with%20continuous-time%20neuronal%20dynamics%20and%20continuously%0Aactive%2C%20local%20synaptic%20plasticity.%20In%20particular%2C%20GLE%20exploits%20the%20morphology%0Aof%20dendritic%20trees%20to%20enable%20more%20complex%20information%20storage%20and%20processing%20in%0Asingle%20neurons%2C%20as%20well%20as%20the%20ability%20of%20biological%20neurons%20to%20phase-shift%0Atheir%20output%20rate%20with%20respect%20to%20their%20membrane%20potential%2C%20which%20is%20essential%0Ain%20both%20directions%20of%20information%20propagation.%20For%20the%20forward%20computation%2C%20it%0Aenables%20the%20mapping%20of%20time-continuous%20inputs%20to%20neuronal%20space%2C%20effectively%0Aperforming%20a%20spatio-temporal%20convolution.%20For%20the%20backward%20computation%2C%20it%0Apermits%20the%20temporal%20inversion%20of%20feedback%20signals%2C%20which%20consequently%0Aapproximate%20the%20adjoint%20variables%20necessary%20for%20useful%20parameter%20updates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16933v3&entry.124074799=Read"},
{"title": "Rethinking Federated Graph Learning: A Data Condensation Perspective", "author": "Hao Zhang and Xunkai Li and Yinlin Zhu and Lianglin Hu", "abstract": "  Federated graph learning is a widely recognized technique that promotes\ncollaborative training of graph neural networks (GNNs) by multi-client\ngraphs.However, existing approaches heavily rely on the communication of model\nparameters or gradients for federated optimization and fail to adequately\naddress the data heterogeneity introduced by intricate and diverse graph\ndistributions. Although some methods attempt to share additional messages among\nthe server and clients to improve federated convergence during communication,\nthey introduce significant privacy risks and increase communication overhead.\nTo address these issues, we introduce the concept of a condensed graph as a\nnovel optimization carrier to address FGL data heterogeneity and propose a new\nFGL paradigm called FedGM. Specifically, we utilize a generalized condensation\ngraph consensus to aggregate comprehensive knowledge from distributed graphs,\nwhile minimizing communication costs and privacy risks through a single\ntransmission of the condensed data. Extensive experiments on six public\ndatasets consistently demonstrate the superiority of FedGM over\nstate-of-the-art baselines, highlighting its potential for a novel FGL\nparadigm.\n", "link": "http://arxiv.org/abs/2505.02573v1", "date": "2025-05-05", "relevancy": 2.0026, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5128}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5066}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4861}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Federated%20Graph%20Learning%3A%20A%20Data%20Condensation%20Perspective&body=Title%3A%20Rethinking%20Federated%20Graph%20Learning%3A%20A%20Data%20Condensation%20Perspective%0AAuthor%3A%20Hao%20Zhang%20and%20Xunkai%20Li%20and%20Yinlin%20Zhu%20and%20Lianglin%20Hu%0AAbstract%3A%20%20%20Federated%20graph%20learning%20is%20a%20widely%20recognized%20technique%20that%20promotes%0Acollaborative%20training%20of%20graph%20neural%20networks%20%28GNNs%29%20by%20multi-client%0Agraphs.However%2C%20existing%20approaches%20heavily%20rely%20on%20the%20communication%20of%20model%0Aparameters%20or%20gradients%20for%20federated%20optimization%20and%20fail%20to%20adequately%0Aaddress%20the%20data%20heterogeneity%20introduced%20by%20intricate%20and%20diverse%20graph%0Adistributions.%20Although%20some%20methods%20attempt%20to%20share%20additional%20messages%20among%0Athe%20server%20and%20clients%20to%20improve%20federated%20convergence%20during%20communication%2C%0Athey%20introduce%20significant%20privacy%20risks%20and%20increase%20communication%20overhead.%0ATo%20address%20these%20issues%2C%20we%20introduce%20the%20concept%20of%20a%20condensed%20graph%20as%20a%0Anovel%20optimization%20carrier%20to%20address%20FGL%20data%20heterogeneity%20and%20propose%20a%20new%0AFGL%20paradigm%20called%20FedGM.%20Specifically%2C%20we%20utilize%20a%20generalized%20condensation%0Agraph%20consensus%20to%20aggregate%20comprehensive%20knowledge%20from%20distributed%20graphs%2C%0Awhile%20minimizing%20communication%20costs%20and%20privacy%20risks%20through%20a%20single%0Atransmission%20of%20the%20condensed%20data.%20Extensive%20experiments%20on%20six%20public%0Adatasets%20consistently%20demonstrate%20the%20superiority%20of%20FedGM%20over%0Astate-of-the-art%20baselines%2C%20highlighting%20its%20potential%20for%20a%20novel%20FGL%0Aparadigm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02573v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Federated%2520Graph%2520Learning%253A%2520A%2520Data%2520Condensation%2520Perspective%26entry.906535625%3DHao%2520Zhang%2520and%2520Xunkai%2520Li%2520and%2520Yinlin%2520Zhu%2520and%2520Lianglin%2520Hu%26entry.1292438233%3D%2520%2520Federated%2520graph%2520learning%2520is%2520a%2520widely%2520recognized%2520technique%2520that%2520promotes%250Acollaborative%2520training%2520of%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520by%2520multi-client%250Agraphs.However%252C%2520existing%2520approaches%2520heavily%2520rely%2520on%2520the%2520communication%2520of%2520model%250Aparameters%2520or%2520gradients%2520for%2520federated%2520optimization%2520and%2520fail%2520to%2520adequately%250Aaddress%2520the%2520data%2520heterogeneity%2520introduced%2520by%2520intricate%2520and%2520diverse%2520graph%250Adistributions.%2520Although%2520some%2520methods%2520attempt%2520to%2520share%2520additional%2520messages%2520among%250Athe%2520server%2520and%2520clients%2520to%2520improve%2520federated%2520convergence%2520during%2520communication%252C%250Athey%2520introduce%2520significant%2520privacy%2520risks%2520and%2520increase%2520communication%2520overhead.%250ATo%2520address%2520these%2520issues%252C%2520we%2520introduce%2520the%2520concept%2520of%2520a%2520condensed%2520graph%2520as%2520a%250Anovel%2520optimization%2520carrier%2520to%2520address%2520FGL%2520data%2520heterogeneity%2520and%2520propose%2520a%2520new%250AFGL%2520paradigm%2520called%2520FedGM.%2520Specifically%252C%2520we%2520utilize%2520a%2520generalized%2520condensation%250Agraph%2520consensus%2520to%2520aggregate%2520comprehensive%2520knowledge%2520from%2520distributed%2520graphs%252C%250Awhile%2520minimizing%2520communication%2520costs%2520and%2520privacy%2520risks%2520through%2520a%2520single%250Atransmission%2520of%2520the%2520condensed%2520data.%2520Extensive%2520experiments%2520on%2520six%2520public%250Adatasets%2520consistently%2520demonstrate%2520the%2520superiority%2520of%2520FedGM%2520over%250Astate-of-the-art%2520baselines%252C%2520highlighting%2520its%2520potential%2520for%2520a%2520novel%2520FGL%250Aparadigm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02573v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Federated%20Graph%20Learning%3A%20A%20Data%20Condensation%20Perspective&entry.906535625=Hao%20Zhang%20and%20Xunkai%20Li%20and%20Yinlin%20Zhu%20and%20Lianglin%20Hu&entry.1292438233=%20%20Federated%20graph%20learning%20is%20a%20widely%20recognized%20technique%20that%20promotes%0Acollaborative%20training%20of%20graph%20neural%20networks%20%28GNNs%29%20by%20multi-client%0Agraphs.However%2C%20existing%20approaches%20heavily%20rely%20on%20the%20communication%20of%20model%0Aparameters%20or%20gradients%20for%20federated%20optimization%20and%20fail%20to%20adequately%0Aaddress%20the%20data%20heterogeneity%20introduced%20by%20intricate%20and%20diverse%20graph%0Adistributions.%20Although%20some%20methods%20attempt%20to%20share%20additional%20messages%20among%0Athe%20server%20and%20clients%20to%20improve%20federated%20convergence%20during%20communication%2C%0Athey%20introduce%20significant%20privacy%20risks%20and%20increase%20communication%20overhead.%0ATo%20address%20these%20issues%2C%20we%20introduce%20the%20concept%20of%20a%20condensed%20graph%20as%20a%0Anovel%20optimization%20carrier%20to%20address%20FGL%20data%20heterogeneity%20and%20propose%20a%20new%0AFGL%20paradigm%20called%20FedGM.%20Specifically%2C%20we%20utilize%20a%20generalized%20condensation%0Agraph%20consensus%20to%20aggregate%20comprehensive%20knowledge%20from%20distributed%20graphs%2C%0Awhile%20minimizing%20communication%20costs%20and%20privacy%20risks%20through%20a%20single%0Atransmission%20of%20the%20condensed%20data.%20Extensive%20experiments%20on%20six%20public%0Adatasets%20consistently%20demonstrate%20the%20superiority%20of%20FedGM%20over%0Astate-of-the-art%20baselines%2C%20highlighting%20its%20potential%20for%20a%20novel%20FGL%0Aparadigm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02573v1&entry.124074799=Read"},
{"title": "Smoothing of Headland Path Edges and Headland-to-Mainfield Lane\n  Transitions Based on a Spatial Domain Transformation and Linear Programming", "author": "Mogens Plessen", "abstract": "  Within the context of in-field path planning and under the assumption of\nnonholonomic vehicle models this paper addresses two tasks: smoothing of\nheadland path edges and smoothing of headland-to-mainfield lane transitions.\nBoth tasks are solved by a two-step hierarchical algorithm. The first step\ndiffers for the two tasks generating either a piecewise-affine or a Dubins\nreference path. The second step leverages a transformation of vehicle dynamics\nfrom the time domain into the spatial domain and linear programming. Benefits\nsuch as a hyperparameter-free objective function and spatial constraints useful\nfor area coverage gaps avoidance and precision path planning are discussed. The\nmethod, which is a deterministic optimisation-based method, is evaluated on 5\nreal-world fields solving 19 instances of the first task and 84 instances of\nthe second task.\n", "link": "http://arxiv.org/abs/2407.05979v3", "date": "2025-05-05", "relevancy": 2.0002, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5199}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5043}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4879}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Smoothing%20of%20Headland%20Path%20Edges%20and%20Headland-to-Mainfield%20Lane%0A%20%20Transitions%20Based%20on%20a%20Spatial%20Domain%20Transformation%20and%20Linear%20Programming&body=Title%3A%20Smoothing%20of%20Headland%20Path%20Edges%20and%20Headland-to-Mainfield%20Lane%0A%20%20Transitions%20Based%20on%20a%20Spatial%20Domain%20Transformation%20and%20Linear%20Programming%0AAuthor%3A%20Mogens%20Plessen%0AAbstract%3A%20%20%20Within%20the%20context%20of%20in-field%20path%20planning%20and%20under%20the%20assumption%20of%0Anonholonomic%20vehicle%20models%20this%20paper%20addresses%20two%20tasks%3A%20smoothing%20of%0Aheadland%20path%20edges%20and%20smoothing%20of%20headland-to-mainfield%20lane%20transitions.%0ABoth%20tasks%20are%20solved%20by%20a%20two-step%20hierarchical%20algorithm.%20The%20first%20step%0Adiffers%20for%20the%20two%20tasks%20generating%20either%20a%20piecewise-affine%20or%20a%20Dubins%0Areference%20path.%20The%20second%20step%20leverages%20a%20transformation%20of%20vehicle%20dynamics%0Afrom%20the%20time%20domain%20into%20the%20spatial%20domain%20and%20linear%20programming.%20Benefits%0Asuch%20as%20a%20hyperparameter-free%20objective%20function%20and%20spatial%20constraints%20useful%0Afor%20area%20coverage%20gaps%20avoidance%20and%20precision%20path%20planning%20are%20discussed.%20The%0Amethod%2C%20which%20is%20a%20deterministic%20optimisation-based%20method%2C%20is%20evaluated%20on%205%0Areal-world%20fields%20solving%2019%20instances%20of%20the%20first%20task%20and%2084%20instances%20of%0Athe%20second%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05979v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmoothing%2520of%2520Headland%2520Path%2520Edges%2520and%2520Headland-to-Mainfield%2520Lane%250A%2520%2520Transitions%2520Based%2520on%2520a%2520Spatial%2520Domain%2520Transformation%2520and%2520Linear%2520Programming%26entry.906535625%3DMogens%2520Plessen%26entry.1292438233%3D%2520%2520Within%2520the%2520context%2520of%2520in-field%2520path%2520planning%2520and%2520under%2520the%2520assumption%2520of%250Anonholonomic%2520vehicle%2520models%2520this%2520paper%2520addresses%2520two%2520tasks%253A%2520smoothing%2520of%250Aheadland%2520path%2520edges%2520and%2520smoothing%2520of%2520headland-to-mainfield%2520lane%2520transitions.%250ABoth%2520tasks%2520are%2520solved%2520by%2520a%2520two-step%2520hierarchical%2520algorithm.%2520The%2520first%2520step%250Adiffers%2520for%2520the%2520two%2520tasks%2520generating%2520either%2520a%2520piecewise-affine%2520or%2520a%2520Dubins%250Areference%2520path.%2520The%2520second%2520step%2520leverages%2520a%2520transformation%2520of%2520vehicle%2520dynamics%250Afrom%2520the%2520time%2520domain%2520into%2520the%2520spatial%2520domain%2520and%2520linear%2520programming.%2520Benefits%250Asuch%2520as%2520a%2520hyperparameter-free%2520objective%2520function%2520and%2520spatial%2520constraints%2520useful%250Afor%2520area%2520coverage%2520gaps%2520avoidance%2520and%2520precision%2520path%2520planning%2520are%2520discussed.%2520The%250Amethod%252C%2520which%2520is%2520a%2520deterministic%2520optimisation-based%2520method%252C%2520is%2520evaluated%2520on%25205%250Areal-world%2520fields%2520solving%252019%2520instances%2520of%2520the%2520first%2520task%2520and%252084%2520instances%2520of%250Athe%2520second%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05979v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Smoothing%20of%20Headland%20Path%20Edges%20and%20Headland-to-Mainfield%20Lane%0A%20%20Transitions%20Based%20on%20a%20Spatial%20Domain%20Transformation%20and%20Linear%20Programming&entry.906535625=Mogens%20Plessen&entry.1292438233=%20%20Within%20the%20context%20of%20in-field%20path%20planning%20and%20under%20the%20assumption%20of%0Anonholonomic%20vehicle%20models%20this%20paper%20addresses%20two%20tasks%3A%20smoothing%20of%0Aheadland%20path%20edges%20and%20smoothing%20of%20headland-to-mainfield%20lane%20transitions.%0ABoth%20tasks%20are%20solved%20by%20a%20two-step%20hierarchical%20algorithm.%20The%20first%20step%0Adiffers%20for%20the%20two%20tasks%20generating%20either%20a%20piecewise-affine%20or%20a%20Dubins%0Areference%20path.%20The%20second%20step%20leverages%20a%20transformation%20of%20vehicle%20dynamics%0Afrom%20the%20time%20domain%20into%20the%20spatial%20domain%20and%20linear%20programming.%20Benefits%0Asuch%20as%20a%20hyperparameter-free%20objective%20function%20and%20spatial%20constraints%20useful%0Afor%20area%20coverage%20gaps%20avoidance%20and%20precision%20path%20planning%20are%20discussed.%20The%0Amethod%2C%20which%20is%20a%20deterministic%20optimisation-based%20method%2C%20is%20evaluated%20on%205%0Areal-world%20fields%20solving%2019%20instances%20of%20the%20first%20task%20and%2084%20instances%20of%0Athe%20second%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05979v3&entry.124074799=Read"},
{"title": "A Survey of Slow Thinking-based Reasoning LLMs using Reinforced Learning\n  and Inference-time Scaling Law", "author": "Qianjun Pan and Wenkai Ji and Yuyang Ding and Junsong Li and Shilian Chen and Junyi Wang and Jie Zhou and Qin Chen and Min Zhang and Yulan Wu and Liang He", "abstract": "  This survey explores recent advancements in reasoning large language models\n(LLMs) designed to mimic \"slow thinking\" - a reasoning process inspired by\nhuman cognition, as described in Kahneman's Thinking, Fast and Slow. These\nmodels, like OpenAI's o1, focus on scaling computational resources dynamically\nduring complex tasks, such as math reasoning, visual reasoning, medical\ndiagnosis, and multi-agent debates. We present the development of reasoning\nLLMs and list their key technologies. By synthesizing over 100 studies, it\ncharts a path toward LLMs that combine human-like deep thinking with scalable\nefficiency for reasoning. The review breaks down methods into three categories:\n(1) test-time scaling dynamically adjusts computation based on task complexity\nvia search and sampling, dynamic verification; (2) reinforced learning refines\ndecision-making through iterative improvement leveraging policy networks,\nreward models, and self-evolution strategies; and (3) slow-thinking frameworks\n(e.g., long CoT, hierarchical processes) that structure problem-solving with\nmanageable steps. The survey highlights the challenges and further directions\nof this domain. Understanding and advancing the reasoning abilities of LLMs is\ncrucial for unlocking their full potential in real-world applications, from\nscientific discovery to decision support systems.\n", "link": "http://arxiv.org/abs/2505.02665v1", "date": "2025-05-05", "relevancy": 2.0, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5035}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5035}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20of%20Slow%20Thinking-based%20Reasoning%20LLMs%20using%20Reinforced%20Learning%0A%20%20and%20Inference-time%20Scaling%20Law&body=Title%3A%20A%20Survey%20of%20Slow%20Thinking-based%20Reasoning%20LLMs%20using%20Reinforced%20Learning%0A%20%20and%20Inference-time%20Scaling%20Law%0AAuthor%3A%20Qianjun%20Pan%20and%20Wenkai%20Ji%20and%20Yuyang%20Ding%20and%20Junsong%20Li%20and%20Shilian%20Chen%20and%20Junyi%20Wang%20and%20Jie%20Zhou%20and%20Qin%20Chen%20and%20Min%20Zhang%20and%20Yulan%20Wu%20and%20Liang%20He%0AAbstract%3A%20%20%20This%20survey%20explores%20recent%20advancements%20in%20reasoning%20large%20language%20models%0A%28LLMs%29%20designed%20to%20mimic%20%22slow%20thinking%22%20-%20a%20reasoning%20process%20inspired%20by%0Ahuman%20cognition%2C%20as%20described%20in%20Kahneman%27s%20Thinking%2C%20Fast%20and%20Slow.%20These%0Amodels%2C%20like%20OpenAI%27s%20o1%2C%20focus%20on%20scaling%20computational%20resources%20dynamically%0Aduring%20complex%20tasks%2C%20such%20as%20math%20reasoning%2C%20visual%20reasoning%2C%20medical%0Adiagnosis%2C%20and%20multi-agent%20debates.%20We%20present%20the%20development%20of%20reasoning%0ALLMs%20and%20list%20their%20key%20technologies.%20By%20synthesizing%20over%20100%20studies%2C%20it%0Acharts%20a%20path%20toward%20LLMs%20that%20combine%20human-like%20deep%20thinking%20with%20scalable%0Aefficiency%20for%20reasoning.%20The%20review%20breaks%20down%20methods%20into%20three%20categories%3A%0A%281%29%20test-time%20scaling%20dynamically%20adjusts%20computation%20based%20on%20task%20complexity%0Avia%20search%20and%20sampling%2C%20dynamic%20verification%3B%20%282%29%20reinforced%20learning%20refines%0Adecision-making%20through%20iterative%20improvement%20leveraging%20policy%20networks%2C%0Areward%20models%2C%20and%20self-evolution%20strategies%3B%20and%20%283%29%20slow-thinking%20frameworks%0A%28e.g.%2C%20long%20CoT%2C%20hierarchical%20processes%29%20that%20structure%20problem-solving%20with%0Amanageable%20steps.%20The%20survey%20highlights%20the%20challenges%20and%20further%20directions%0Aof%20this%20domain.%20Understanding%20and%20advancing%20the%20reasoning%20abilities%20of%20LLMs%20is%0Acrucial%20for%20unlocking%20their%20full%20potential%20in%20real-world%20applications%2C%20from%0Ascientific%20discovery%20to%20decision%20support%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02665v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520of%2520Slow%2520Thinking-based%2520Reasoning%2520LLMs%2520using%2520Reinforced%2520Learning%250A%2520%2520and%2520Inference-time%2520Scaling%2520Law%26entry.906535625%3DQianjun%2520Pan%2520and%2520Wenkai%2520Ji%2520and%2520Yuyang%2520Ding%2520and%2520Junsong%2520Li%2520and%2520Shilian%2520Chen%2520and%2520Junyi%2520Wang%2520and%2520Jie%2520Zhou%2520and%2520Qin%2520Chen%2520and%2520Min%2520Zhang%2520and%2520Yulan%2520Wu%2520and%2520Liang%2520He%26entry.1292438233%3D%2520%2520This%2520survey%2520explores%2520recent%2520advancements%2520in%2520reasoning%2520large%2520language%2520models%250A%2528LLMs%2529%2520designed%2520to%2520mimic%2520%2522slow%2520thinking%2522%2520-%2520a%2520reasoning%2520process%2520inspired%2520by%250Ahuman%2520cognition%252C%2520as%2520described%2520in%2520Kahneman%2527s%2520Thinking%252C%2520Fast%2520and%2520Slow.%2520These%250Amodels%252C%2520like%2520OpenAI%2527s%2520o1%252C%2520focus%2520on%2520scaling%2520computational%2520resources%2520dynamically%250Aduring%2520complex%2520tasks%252C%2520such%2520as%2520math%2520reasoning%252C%2520visual%2520reasoning%252C%2520medical%250Adiagnosis%252C%2520and%2520multi-agent%2520debates.%2520We%2520present%2520the%2520development%2520of%2520reasoning%250ALLMs%2520and%2520list%2520their%2520key%2520technologies.%2520By%2520synthesizing%2520over%2520100%2520studies%252C%2520it%250Acharts%2520a%2520path%2520toward%2520LLMs%2520that%2520combine%2520human-like%2520deep%2520thinking%2520with%2520scalable%250Aefficiency%2520for%2520reasoning.%2520The%2520review%2520breaks%2520down%2520methods%2520into%2520three%2520categories%253A%250A%25281%2529%2520test-time%2520scaling%2520dynamically%2520adjusts%2520computation%2520based%2520on%2520task%2520complexity%250Avia%2520search%2520and%2520sampling%252C%2520dynamic%2520verification%253B%2520%25282%2529%2520reinforced%2520learning%2520refines%250Adecision-making%2520through%2520iterative%2520improvement%2520leveraging%2520policy%2520networks%252C%250Areward%2520models%252C%2520and%2520self-evolution%2520strategies%253B%2520and%2520%25283%2529%2520slow-thinking%2520frameworks%250A%2528e.g.%252C%2520long%2520CoT%252C%2520hierarchical%2520processes%2529%2520that%2520structure%2520problem-solving%2520with%250Amanageable%2520steps.%2520The%2520survey%2520highlights%2520the%2520challenges%2520and%2520further%2520directions%250Aof%2520this%2520domain.%2520Understanding%2520and%2520advancing%2520the%2520reasoning%2520abilities%2520of%2520LLMs%2520is%250Acrucial%2520for%2520unlocking%2520their%2520full%2520potential%2520in%2520real-world%2520applications%252C%2520from%250Ascientific%2520discovery%2520to%2520decision%2520support%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02665v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20of%20Slow%20Thinking-based%20Reasoning%20LLMs%20using%20Reinforced%20Learning%0A%20%20and%20Inference-time%20Scaling%20Law&entry.906535625=Qianjun%20Pan%20and%20Wenkai%20Ji%20and%20Yuyang%20Ding%20and%20Junsong%20Li%20and%20Shilian%20Chen%20and%20Junyi%20Wang%20and%20Jie%20Zhou%20and%20Qin%20Chen%20and%20Min%20Zhang%20and%20Yulan%20Wu%20and%20Liang%20He&entry.1292438233=%20%20This%20survey%20explores%20recent%20advancements%20in%20reasoning%20large%20language%20models%0A%28LLMs%29%20designed%20to%20mimic%20%22slow%20thinking%22%20-%20a%20reasoning%20process%20inspired%20by%0Ahuman%20cognition%2C%20as%20described%20in%20Kahneman%27s%20Thinking%2C%20Fast%20and%20Slow.%20These%0Amodels%2C%20like%20OpenAI%27s%20o1%2C%20focus%20on%20scaling%20computational%20resources%20dynamically%0Aduring%20complex%20tasks%2C%20such%20as%20math%20reasoning%2C%20visual%20reasoning%2C%20medical%0Adiagnosis%2C%20and%20multi-agent%20debates.%20We%20present%20the%20development%20of%20reasoning%0ALLMs%20and%20list%20their%20key%20technologies.%20By%20synthesizing%20over%20100%20studies%2C%20it%0Acharts%20a%20path%20toward%20LLMs%20that%20combine%20human-like%20deep%20thinking%20with%20scalable%0Aefficiency%20for%20reasoning.%20The%20review%20breaks%20down%20methods%20into%20three%20categories%3A%0A%281%29%20test-time%20scaling%20dynamically%20adjusts%20computation%20based%20on%20task%20complexity%0Avia%20search%20and%20sampling%2C%20dynamic%20verification%3B%20%282%29%20reinforced%20learning%20refines%0Adecision-making%20through%20iterative%20improvement%20leveraging%20policy%20networks%2C%0Areward%20models%2C%20and%20self-evolution%20strategies%3B%20and%20%283%29%20slow-thinking%20frameworks%0A%28e.g.%2C%20long%20CoT%2C%20hierarchical%20processes%29%20that%20structure%20problem-solving%20with%0Amanageable%20steps.%20The%20survey%20highlights%20the%20challenges%20and%20further%20directions%0Aof%20this%20domain.%20Understanding%20and%20advancing%20the%20reasoning%20abilities%20of%20LLMs%20is%0Acrucial%20for%20unlocking%20their%20full%20potential%20in%20real-world%20applications%2C%20from%0Ascientific%20discovery%20to%20decision%20support%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02665v1&entry.124074799=Read"},
{"title": "Graph Neural Network-Based Reinforcement Learning for Controlling\n  Biological Networks: The GATTACA Framework", "author": "Andrzej Mizera and Jakub Zarzycki", "abstract": "  Cellular reprogramming, the artificial transformation of one cell type into\nanother, has been attracting increasing research attention due to its\ntherapeutic potential for complex diseases. However, discovering reprogramming\nstrategies through classical wet-lab experiments is hindered by lengthy time\ncommitments and high costs. In this study, we explore the use of deep\nreinforcement learning (DRL) to control Boolean network models of complex\nbiological systems, such as gene regulatory networks and signalling pathway\nnetworks. We formulate a novel control problem for Boolean network models under\nthe asynchronous update mode in the context of cellular reprogramming. To\nfacilitate scalability, we consider our previously introduced concept of a\npseudo-attractor and we improve our procedure for effective identification of\npseudo-attractor states. Finally, we devise a computational framework to solve\nthe control problem. To leverage the structure of biological systems, we\nincorporate graph neural networks with graph convolutions into the artificial\nneural network approximator for the action-value function learned by the DRL\nagent. Experiments on a number of large real-world biological networks from\nliterature demonstrate the scalability and effectiveness of our approach.\n", "link": "http://arxiv.org/abs/2505.02712v1", "date": "2025-05-05", "relevancy": 1.9982, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.512}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5108}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4826}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Neural%20Network-Based%20Reinforcement%20Learning%20for%20Controlling%0A%20%20Biological%20Networks%3A%20The%20GATTACA%20Framework&body=Title%3A%20Graph%20Neural%20Network-Based%20Reinforcement%20Learning%20for%20Controlling%0A%20%20Biological%20Networks%3A%20The%20GATTACA%20Framework%0AAuthor%3A%20Andrzej%20Mizera%20and%20Jakub%20Zarzycki%0AAbstract%3A%20%20%20Cellular%20reprogramming%2C%20the%20artificial%20transformation%20of%20one%20cell%20type%20into%0Aanother%2C%20has%20been%20attracting%20increasing%20research%20attention%20due%20to%20its%0Atherapeutic%20potential%20for%20complex%20diseases.%20However%2C%20discovering%20reprogramming%0Astrategies%20through%20classical%20wet-lab%20experiments%20is%20hindered%20by%20lengthy%20time%0Acommitments%20and%20high%20costs.%20In%20this%20study%2C%20we%20explore%20the%20use%20of%20deep%0Areinforcement%20learning%20%28DRL%29%20to%20control%20Boolean%20network%20models%20of%20complex%0Abiological%20systems%2C%20such%20as%20gene%20regulatory%20networks%20and%20signalling%20pathway%0Anetworks.%20We%20formulate%20a%20novel%20control%20problem%20for%20Boolean%20network%20models%20under%0Athe%20asynchronous%20update%20mode%20in%20the%20context%20of%20cellular%20reprogramming.%20To%0Afacilitate%20scalability%2C%20we%20consider%20our%20previously%20introduced%20concept%20of%20a%0Apseudo-attractor%20and%20we%20improve%20our%20procedure%20for%20effective%20identification%20of%0Apseudo-attractor%20states.%20Finally%2C%20we%20devise%20a%20computational%20framework%20to%20solve%0Athe%20control%20problem.%20To%20leverage%20the%20structure%20of%20biological%20systems%2C%20we%0Aincorporate%20graph%20neural%20networks%20with%20graph%20convolutions%20into%20the%20artificial%0Aneural%20network%20approximator%20for%20the%20action-value%20function%20learned%20by%20the%20DRL%0Aagent.%20Experiments%20on%20a%20number%20of%20large%20real-world%20biological%20networks%20from%0Aliterature%20demonstrate%20the%20scalability%20and%20effectiveness%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02712v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Neural%2520Network-Based%2520Reinforcement%2520Learning%2520for%2520Controlling%250A%2520%2520Biological%2520Networks%253A%2520The%2520GATTACA%2520Framework%26entry.906535625%3DAndrzej%2520Mizera%2520and%2520Jakub%2520Zarzycki%26entry.1292438233%3D%2520%2520Cellular%2520reprogramming%252C%2520the%2520artificial%2520transformation%2520of%2520one%2520cell%2520type%2520into%250Aanother%252C%2520has%2520been%2520attracting%2520increasing%2520research%2520attention%2520due%2520to%2520its%250Atherapeutic%2520potential%2520for%2520complex%2520diseases.%2520However%252C%2520discovering%2520reprogramming%250Astrategies%2520through%2520classical%2520wet-lab%2520experiments%2520is%2520hindered%2520by%2520lengthy%2520time%250Acommitments%2520and%2520high%2520costs.%2520In%2520this%2520study%252C%2520we%2520explore%2520the%2520use%2520of%2520deep%250Areinforcement%2520learning%2520%2528DRL%2529%2520to%2520control%2520Boolean%2520network%2520models%2520of%2520complex%250Abiological%2520systems%252C%2520such%2520as%2520gene%2520regulatory%2520networks%2520and%2520signalling%2520pathway%250Anetworks.%2520We%2520formulate%2520a%2520novel%2520control%2520problem%2520for%2520Boolean%2520network%2520models%2520under%250Athe%2520asynchronous%2520update%2520mode%2520in%2520the%2520context%2520of%2520cellular%2520reprogramming.%2520To%250Afacilitate%2520scalability%252C%2520we%2520consider%2520our%2520previously%2520introduced%2520concept%2520of%2520a%250Apseudo-attractor%2520and%2520we%2520improve%2520our%2520procedure%2520for%2520effective%2520identification%2520of%250Apseudo-attractor%2520states.%2520Finally%252C%2520we%2520devise%2520a%2520computational%2520framework%2520to%2520solve%250Athe%2520control%2520problem.%2520To%2520leverage%2520the%2520structure%2520of%2520biological%2520systems%252C%2520we%250Aincorporate%2520graph%2520neural%2520networks%2520with%2520graph%2520convolutions%2520into%2520the%2520artificial%250Aneural%2520network%2520approximator%2520for%2520the%2520action-value%2520function%2520learned%2520by%2520the%2520DRL%250Aagent.%2520Experiments%2520on%2520a%2520number%2520of%2520large%2520real-world%2520biological%2520networks%2520from%250Aliterature%2520demonstrate%2520the%2520scalability%2520and%2520effectiveness%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02712v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Neural%20Network-Based%20Reinforcement%20Learning%20for%20Controlling%0A%20%20Biological%20Networks%3A%20The%20GATTACA%20Framework&entry.906535625=Andrzej%20Mizera%20and%20Jakub%20Zarzycki&entry.1292438233=%20%20Cellular%20reprogramming%2C%20the%20artificial%20transformation%20of%20one%20cell%20type%20into%0Aanother%2C%20has%20been%20attracting%20increasing%20research%20attention%20due%20to%20its%0Atherapeutic%20potential%20for%20complex%20diseases.%20However%2C%20discovering%20reprogramming%0Astrategies%20through%20classical%20wet-lab%20experiments%20is%20hindered%20by%20lengthy%20time%0Acommitments%20and%20high%20costs.%20In%20this%20study%2C%20we%20explore%20the%20use%20of%20deep%0Areinforcement%20learning%20%28DRL%29%20to%20control%20Boolean%20network%20models%20of%20complex%0Abiological%20systems%2C%20such%20as%20gene%20regulatory%20networks%20and%20signalling%20pathway%0Anetworks.%20We%20formulate%20a%20novel%20control%20problem%20for%20Boolean%20network%20models%20under%0Athe%20asynchronous%20update%20mode%20in%20the%20context%20of%20cellular%20reprogramming.%20To%0Afacilitate%20scalability%2C%20we%20consider%20our%20previously%20introduced%20concept%20of%20a%0Apseudo-attractor%20and%20we%20improve%20our%20procedure%20for%20effective%20identification%20of%0Apseudo-attractor%20states.%20Finally%2C%20we%20devise%20a%20computational%20framework%20to%20solve%0Athe%20control%20problem.%20To%20leverage%20the%20structure%20of%20biological%20systems%2C%20we%0Aincorporate%20graph%20neural%20networks%20with%20graph%20convolutions%20into%20the%20artificial%0Aneural%20network%20approximator%20for%20the%20action-value%20function%20learned%20by%20the%20DRL%0Aagent.%20Experiments%20on%20a%20number%20of%20large%20real-world%20biological%20networks%20from%0Aliterature%20demonstrate%20the%20scalability%20and%20effectiveness%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02712v1&entry.124074799=Read"},
{"title": "The Effectiveness of Large Language Models in Transforming Unstructured\n  Text to Standardized Formats", "author": "William Brach and Kristi\u00e1n Ko\u0161\u0165\u00e1l and Michal Ries", "abstract": "  The exponential growth of unstructured text data presents a fundamental\nchallenge in modern data management and information retrieval. While Large\nLanguage Models (LLMs) have shown remarkable capabilities in natural language\nprocessing, their potential to transform unstructured text into standardized,\nstructured formats remains largely unexplored - a capability that could\nrevolutionize data processing workflows across industries. This study breaks\nnew ground by systematically evaluating LLMs' ability to convert unstructured\nrecipe text into the structured Cooklang format. Through comprehensive testing\nof four models (GPT-4o, GPT-4o-mini, Llama3.1:70b, and Llama3.1:8b), an\ninnovative evaluation approach is introduced that combines traditional metrics\n(WER, ROUGE-L, TER) with specialized metrics for semantic element\nidentification. Our experiments reveal that GPT-4o with few-shot prompting\nachieves breakthrough performance (ROUGE-L: 0.9722, WER: 0.0730), demonstrating\nfor the first time that LLMs can reliably transform domain-specific\nunstructured text into structured formats without extensive training. Although\nmodel performance generally scales with size, we uncover surprising potential\nin smaller models like Llama3.1:8b for optimization through targeted\nfine-tuning. These findings open new possibilities for automated structured\ndata generation across various domains, from medical records to technical\ndocumentation, potentially transforming the way organizations process and\nutilize unstructured information.\n", "link": "http://arxiv.org/abs/2503.02650v2", "date": "2025-05-05", "relevancy": 1.9961, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5004}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5004}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4924}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Effectiveness%20of%20Large%20Language%20Models%20in%20Transforming%20Unstructured%0A%20%20Text%20to%20Standardized%20Formats&body=Title%3A%20The%20Effectiveness%20of%20Large%20Language%20Models%20in%20Transforming%20Unstructured%0A%20%20Text%20to%20Standardized%20Formats%0AAuthor%3A%20William%20Brach%20and%20Kristi%C3%A1n%20Ko%C5%A1%C5%A5%C3%A1l%20and%20Michal%20Ries%0AAbstract%3A%20%20%20The%20exponential%20growth%20of%20unstructured%20text%20data%20presents%20a%20fundamental%0Achallenge%20in%20modern%20data%20management%20and%20information%20retrieval.%20While%20Large%0ALanguage%20Models%20%28LLMs%29%20have%20shown%20remarkable%20capabilities%20in%20natural%20language%0Aprocessing%2C%20their%20potential%20to%20transform%20unstructured%20text%20into%20standardized%2C%0Astructured%20formats%20remains%20largely%20unexplored%20-%20a%20capability%20that%20could%0Arevolutionize%20data%20processing%20workflows%20across%20industries.%20This%20study%20breaks%0Anew%20ground%20by%20systematically%20evaluating%20LLMs%27%20ability%20to%20convert%20unstructured%0Arecipe%20text%20into%20the%20structured%20Cooklang%20format.%20Through%20comprehensive%20testing%0Aof%20four%20models%20%28GPT-4o%2C%20GPT-4o-mini%2C%20Llama3.1%3A70b%2C%20and%20Llama3.1%3A8b%29%2C%20an%0Ainnovative%20evaluation%20approach%20is%20introduced%20that%20combines%20traditional%20metrics%0A%28WER%2C%20ROUGE-L%2C%20TER%29%20with%20specialized%20metrics%20for%20semantic%20element%0Aidentification.%20Our%20experiments%20reveal%20that%20GPT-4o%20with%20few-shot%20prompting%0Aachieves%20breakthrough%20performance%20%28ROUGE-L%3A%200.9722%2C%20WER%3A%200.0730%29%2C%20demonstrating%0Afor%20the%20first%20time%20that%20LLMs%20can%20reliably%20transform%20domain-specific%0Aunstructured%20text%20into%20structured%20formats%20without%20extensive%20training.%20Although%0Amodel%20performance%20generally%20scales%20with%20size%2C%20we%20uncover%20surprising%20potential%0Ain%20smaller%20models%20like%20Llama3.1%3A8b%20for%20optimization%20through%20targeted%0Afine-tuning.%20These%20findings%20open%20new%20possibilities%20for%20automated%20structured%0Adata%20generation%20across%20various%20domains%2C%20from%20medical%20records%20to%20technical%0Adocumentation%2C%20potentially%20transforming%20the%20way%20organizations%20process%20and%0Autilize%20unstructured%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.02650v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Effectiveness%2520of%2520Large%2520Language%2520Models%2520in%2520Transforming%2520Unstructured%250A%2520%2520Text%2520to%2520Standardized%2520Formats%26entry.906535625%3DWilliam%2520Brach%2520and%2520Kristi%25C3%25A1n%2520Ko%25C5%25A1%25C5%25A5%25C3%25A1l%2520and%2520Michal%2520Ries%26entry.1292438233%3D%2520%2520The%2520exponential%2520growth%2520of%2520unstructured%2520text%2520data%2520presents%2520a%2520fundamental%250Achallenge%2520in%2520modern%2520data%2520management%2520and%2520information%2520retrieval.%2520While%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520have%2520shown%2520remarkable%2520capabilities%2520in%2520natural%2520language%250Aprocessing%252C%2520their%2520potential%2520to%2520transform%2520unstructured%2520text%2520into%2520standardized%252C%250Astructured%2520formats%2520remains%2520largely%2520unexplored%2520-%2520a%2520capability%2520that%2520could%250Arevolutionize%2520data%2520processing%2520workflows%2520across%2520industries.%2520This%2520study%2520breaks%250Anew%2520ground%2520by%2520systematically%2520evaluating%2520LLMs%2527%2520ability%2520to%2520convert%2520unstructured%250Arecipe%2520text%2520into%2520the%2520structured%2520Cooklang%2520format.%2520Through%2520comprehensive%2520testing%250Aof%2520four%2520models%2520%2528GPT-4o%252C%2520GPT-4o-mini%252C%2520Llama3.1%253A70b%252C%2520and%2520Llama3.1%253A8b%2529%252C%2520an%250Ainnovative%2520evaluation%2520approach%2520is%2520introduced%2520that%2520combines%2520traditional%2520metrics%250A%2528WER%252C%2520ROUGE-L%252C%2520TER%2529%2520with%2520specialized%2520metrics%2520for%2520semantic%2520element%250Aidentification.%2520Our%2520experiments%2520reveal%2520that%2520GPT-4o%2520with%2520few-shot%2520prompting%250Aachieves%2520breakthrough%2520performance%2520%2528ROUGE-L%253A%25200.9722%252C%2520WER%253A%25200.0730%2529%252C%2520demonstrating%250Afor%2520the%2520first%2520time%2520that%2520LLMs%2520can%2520reliably%2520transform%2520domain-specific%250Aunstructured%2520text%2520into%2520structured%2520formats%2520without%2520extensive%2520training.%2520Although%250Amodel%2520performance%2520generally%2520scales%2520with%2520size%252C%2520we%2520uncover%2520surprising%2520potential%250Ain%2520smaller%2520models%2520like%2520Llama3.1%253A8b%2520for%2520optimization%2520through%2520targeted%250Afine-tuning.%2520These%2520findings%2520open%2520new%2520possibilities%2520for%2520automated%2520structured%250Adata%2520generation%2520across%2520various%2520domains%252C%2520from%2520medical%2520records%2520to%2520technical%250Adocumentation%252C%2520potentially%2520transforming%2520the%2520way%2520organizations%2520process%2520and%250Autilize%2520unstructured%2520information.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.02650v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Effectiveness%20of%20Large%20Language%20Models%20in%20Transforming%20Unstructured%0A%20%20Text%20to%20Standardized%20Formats&entry.906535625=William%20Brach%20and%20Kristi%C3%A1n%20Ko%C5%A1%C5%A5%C3%A1l%20and%20Michal%20Ries&entry.1292438233=%20%20The%20exponential%20growth%20of%20unstructured%20text%20data%20presents%20a%20fundamental%0Achallenge%20in%20modern%20data%20management%20and%20information%20retrieval.%20While%20Large%0ALanguage%20Models%20%28LLMs%29%20have%20shown%20remarkable%20capabilities%20in%20natural%20language%0Aprocessing%2C%20their%20potential%20to%20transform%20unstructured%20text%20into%20standardized%2C%0Astructured%20formats%20remains%20largely%20unexplored%20-%20a%20capability%20that%20could%0Arevolutionize%20data%20processing%20workflows%20across%20industries.%20This%20study%20breaks%0Anew%20ground%20by%20systematically%20evaluating%20LLMs%27%20ability%20to%20convert%20unstructured%0Arecipe%20text%20into%20the%20structured%20Cooklang%20format.%20Through%20comprehensive%20testing%0Aof%20four%20models%20%28GPT-4o%2C%20GPT-4o-mini%2C%20Llama3.1%3A70b%2C%20and%20Llama3.1%3A8b%29%2C%20an%0Ainnovative%20evaluation%20approach%20is%20introduced%20that%20combines%20traditional%20metrics%0A%28WER%2C%20ROUGE-L%2C%20TER%29%20with%20specialized%20metrics%20for%20semantic%20element%0Aidentification.%20Our%20experiments%20reveal%20that%20GPT-4o%20with%20few-shot%20prompting%0Aachieves%20breakthrough%20performance%20%28ROUGE-L%3A%200.9722%2C%20WER%3A%200.0730%29%2C%20demonstrating%0Afor%20the%20first%20time%20that%20LLMs%20can%20reliably%20transform%20domain-specific%0Aunstructured%20text%20into%20structured%20formats%20without%20extensive%20training.%20Although%0Amodel%20performance%20generally%20scales%20with%20size%2C%20we%20uncover%20surprising%20potential%0Ain%20smaller%20models%20like%20Llama3.1%3A8b%20for%20optimization%20through%20targeted%0Afine-tuning.%20These%20findings%20open%20new%20possibilities%20for%20automated%20structured%0Adata%20generation%20across%20various%20domains%2C%20from%20medical%20records%20to%20technical%0Adocumentation%2C%20potentially%20transforming%20the%20way%20organizations%20process%20and%0Autilize%20unstructured%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.02650v2&entry.124074799=Read"},
{"title": "From Human Judgements to Predictive Models: Unravelling Acceptability in\n  Code-Mixed Sentences", "author": "Prashant Kodali and Anmol Goel and Likhith Asapu and Vamshi Krishna Bonagiri and Anirudh Govil and Monojit Choudhury and Ponnurangam Kumaraguru and Manish Shrivastava", "abstract": "  Current computational approaches for analysing or generating code-mixed\nsentences do not explicitly model ``naturalness'' or ``acceptability'' of\ncode-mixed sentences, but rely on training corpora to reflect distribution of\nacceptable code-mixed sentences. Modelling human judgement for the\nacceptability of code-mixed text can help in distinguishing natural code-mixed\ntext and enable quality-controlled generation of code-mixed text. To this end,\nwe construct Cline - a dataset containing human acceptability judgements for\nEnglish-Hindi~(en-hi) code-mixed text. Cline is the largest of its kind with\n16,642 sentences, consisting of samples sourced from two sources: synthetically\ngenerated code-mixed text and samples collected from online social media. Our\nanalysis establishes that popular code-mixing metrics such as CMI, Number of\nSwitch Points, Burstines, which are used to filter/curate/compare code-mixed\ncorpora have low correlation with human acceptability judgements, underlining\nthe necessity of our dataset. Experiments using Cline demonstrate that simple\nMultilayer Perceptron (MLP) models when trained solely using code-mixing\nmetrics as features are outperformed by fine-tuned pre-trained Multilingual\nLarge Language Models (MLLMs). Specifically, among Encoder models XLM-Roberta\nand Bernice outperform IndicBERT across different configurations. Among\nEncoder-Decoder models, mBART performs better than mT5, however Encoder-Decoder\nmodels are not able to outperform Encoder-only models. Decoder-only models\nperform the best when compared to all other MLLMS, with Llama 3.2 - 3B models\noutperforming similarly sized Qwen, Phi models. Comparison with zero and\nfewshot capabilitites of ChatGPT show that MLLMs fine-tuned on larger data\noutperform ChatGPT, providing scope for improvement in code-mixed tasks.\nZero-shot transfer from En-Hi to En-Te acceptability judgments are better than\nrandom baselines.\n", "link": "http://arxiv.org/abs/2405.05572v2", "date": "2025-05-05", "relevancy": 1.9898, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5036}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4962}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Human%20Judgements%20to%20Predictive%20Models%3A%20Unravelling%20Acceptability%20in%0A%20%20Code-Mixed%20Sentences&body=Title%3A%20From%20Human%20Judgements%20to%20Predictive%20Models%3A%20Unravelling%20Acceptability%20in%0A%20%20Code-Mixed%20Sentences%0AAuthor%3A%20Prashant%20Kodali%20and%20Anmol%20Goel%20and%20Likhith%20Asapu%20and%20Vamshi%20Krishna%20Bonagiri%20and%20Anirudh%20Govil%20and%20Monojit%20Choudhury%20and%20Ponnurangam%20Kumaraguru%20and%20Manish%20Shrivastava%0AAbstract%3A%20%20%20Current%20computational%20approaches%20for%20analysing%20or%20generating%20code-mixed%0Asentences%20do%20not%20explicitly%20model%20%60%60naturalness%27%27%20or%20%60%60acceptability%27%27%20of%0Acode-mixed%20sentences%2C%20but%20rely%20on%20training%20corpora%20to%20reflect%20distribution%20of%0Aacceptable%20code-mixed%20sentences.%20Modelling%20human%20judgement%20for%20the%0Aacceptability%20of%20code-mixed%20text%20can%20help%20in%20distinguishing%20natural%20code-mixed%0Atext%20and%20enable%20quality-controlled%20generation%20of%20code-mixed%20text.%20To%20this%20end%2C%0Awe%20construct%20Cline%20-%20a%20dataset%20containing%20human%20acceptability%20judgements%20for%0AEnglish-Hindi~%28en-hi%29%20code-mixed%20text.%20Cline%20is%20the%20largest%20of%20its%20kind%20with%0A16%2C642%20sentences%2C%20consisting%20of%20samples%20sourced%20from%20two%20sources%3A%20synthetically%0Agenerated%20code-mixed%20text%20and%20samples%20collected%20from%20online%20social%20media.%20Our%0Aanalysis%20establishes%20that%20popular%20code-mixing%20metrics%20such%20as%20CMI%2C%20Number%20of%0ASwitch%20Points%2C%20Burstines%2C%20which%20are%20used%20to%20filter/curate/compare%20code-mixed%0Acorpora%20have%20low%20correlation%20with%20human%20acceptability%20judgements%2C%20underlining%0Athe%20necessity%20of%20our%20dataset.%20Experiments%20using%20Cline%20demonstrate%20that%20simple%0AMultilayer%20Perceptron%20%28MLP%29%20models%20when%20trained%20solely%20using%20code-mixing%0Ametrics%20as%20features%20are%20outperformed%20by%20fine-tuned%20pre-trained%20Multilingual%0ALarge%20Language%20Models%20%28MLLMs%29.%20Specifically%2C%20among%20Encoder%20models%20XLM-Roberta%0Aand%20Bernice%20outperform%20IndicBERT%20across%20different%20configurations.%20Among%0AEncoder-Decoder%20models%2C%20mBART%20performs%20better%20than%20mT5%2C%20however%20Encoder-Decoder%0Amodels%20are%20not%20able%20to%20outperform%20Encoder-only%20models.%20Decoder-only%20models%0Aperform%20the%20best%20when%20compared%20to%20all%20other%20MLLMS%2C%20with%20Llama%203.2%20-%203B%20models%0Aoutperforming%20similarly%20sized%20Qwen%2C%20Phi%20models.%20Comparison%20with%20zero%20and%0Afewshot%20capabilitites%20of%20ChatGPT%20show%20that%20MLLMs%20fine-tuned%20on%20larger%20data%0Aoutperform%20ChatGPT%2C%20providing%20scope%20for%20improvement%20in%20code-mixed%20tasks.%0AZero-shot%20transfer%20from%20En-Hi%20to%20En-Te%20acceptability%20judgments%20are%20better%20than%0Arandom%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05572v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Human%2520Judgements%2520to%2520Predictive%2520Models%253A%2520Unravelling%2520Acceptability%2520in%250A%2520%2520Code-Mixed%2520Sentences%26entry.906535625%3DPrashant%2520Kodali%2520and%2520Anmol%2520Goel%2520and%2520Likhith%2520Asapu%2520and%2520Vamshi%2520Krishna%2520Bonagiri%2520and%2520Anirudh%2520Govil%2520and%2520Monojit%2520Choudhury%2520and%2520Ponnurangam%2520Kumaraguru%2520and%2520Manish%2520Shrivastava%26entry.1292438233%3D%2520%2520Current%2520computational%2520approaches%2520for%2520analysing%2520or%2520generating%2520code-mixed%250Asentences%2520do%2520not%2520explicitly%2520model%2520%2560%2560naturalness%2527%2527%2520or%2520%2560%2560acceptability%2527%2527%2520of%250Acode-mixed%2520sentences%252C%2520but%2520rely%2520on%2520training%2520corpora%2520to%2520reflect%2520distribution%2520of%250Aacceptable%2520code-mixed%2520sentences.%2520Modelling%2520human%2520judgement%2520for%2520the%250Aacceptability%2520of%2520code-mixed%2520text%2520can%2520help%2520in%2520distinguishing%2520natural%2520code-mixed%250Atext%2520and%2520enable%2520quality-controlled%2520generation%2520of%2520code-mixed%2520text.%2520To%2520this%2520end%252C%250Awe%2520construct%2520Cline%2520-%2520a%2520dataset%2520containing%2520human%2520acceptability%2520judgements%2520for%250AEnglish-Hindi~%2528en-hi%2529%2520code-mixed%2520text.%2520Cline%2520is%2520the%2520largest%2520of%2520its%2520kind%2520with%250A16%252C642%2520sentences%252C%2520consisting%2520of%2520samples%2520sourced%2520from%2520two%2520sources%253A%2520synthetically%250Agenerated%2520code-mixed%2520text%2520and%2520samples%2520collected%2520from%2520online%2520social%2520media.%2520Our%250Aanalysis%2520establishes%2520that%2520popular%2520code-mixing%2520metrics%2520such%2520as%2520CMI%252C%2520Number%2520of%250ASwitch%2520Points%252C%2520Burstines%252C%2520which%2520are%2520used%2520to%2520filter/curate/compare%2520code-mixed%250Acorpora%2520have%2520low%2520correlation%2520with%2520human%2520acceptability%2520judgements%252C%2520underlining%250Athe%2520necessity%2520of%2520our%2520dataset.%2520Experiments%2520using%2520Cline%2520demonstrate%2520that%2520simple%250AMultilayer%2520Perceptron%2520%2528MLP%2529%2520models%2520when%2520trained%2520solely%2520using%2520code-mixing%250Ametrics%2520as%2520features%2520are%2520outperformed%2520by%2520fine-tuned%2520pre-trained%2520Multilingual%250ALarge%2520Language%2520Models%2520%2528MLLMs%2529.%2520Specifically%252C%2520among%2520Encoder%2520models%2520XLM-Roberta%250Aand%2520Bernice%2520outperform%2520IndicBERT%2520across%2520different%2520configurations.%2520Among%250AEncoder-Decoder%2520models%252C%2520mBART%2520performs%2520better%2520than%2520mT5%252C%2520however%2520Encoder-Decoder%250Amodels%2520are%2520not%2520able%2520to%2520outperform%2520Encoder-only%2520models.%2520Decoder-only%2520models%250Aperform%2520the%2520best%2520when%2520compared%2520to%2520all%2520other%2520MLLMS%252C%2520with%2520Llama%25203.2%2520-%25203B%2520models%250Aoutperforming%2520similarly%2520sized%2520Qwen%252C%2520Phi%2520models.%2520Comparison%2520with%2520zero%2520and%250Afewshot%2520capabilitites%2520of%2520ChatGPT%2520show%2520that%2520MLLMs%2520fine-tuned%2520on%2520larger%2520data%250Aoutperform%2520ChatGPT%252C%2520providing%2520scope%2520for%2520improvement%2520in%2520code-mixed%2520tasks.%250AZero-shot%2520transfer%2520from%2520En-Hi%2520to%2520En-Te%2520acceptability%2520judgments%2520are%2520better%2520than%250Arandom%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05572v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Human%20Judgements%20to%20Predictive%20Models%3A%20Unravelling%20Acceptability%20in%0A%20%20Code-Mixed%20Sentences&entry.906535625=Prashant%20Kodali%20and%20Anmol%20Goel%20and%20Likhith%20Asapu%20and%20Vamshi%20Krishna%20Bonagiri%20and%20Anirudh%20Govil%20and%20Monojit%20Choudhury%20and%20Ponnurangam%20Kumaraguru%20and%20Manish%20Shrivastava&entry.1292438233=%20%20Current%20computational%20approaches%20for%20analysing%20or%20generating%20code-mixed%0Asentences%20do%20not%20explicitly%20model%20%60%60naturalness%27%27%20or%20%60%60acceptability%27%27%20of%0Acode-mixed%20sentences%2C%20but%20rely%20on%20training%20corpora%20to%20reflect%20distribution%20of%0Aacceptable%20code-mixed%20sentences.%20Modelling%20human%20judgement%20for%20the%0Aacceptability%20of%20code-mixed%20text%20can%20help%20in%20distinguishing%20natural%20code-mixed%0Atext%20and%20enable%20quality-controlled%20generation%20of%20code-mixed%20text.%20To%20this%20end%2C%0Awe%20construct%20Cline%20-%20a%20dataset%20containing%20human%20acceptability%20judgements%20for%0AEnglish-Hindi~%28en-hi%29%20code-mixed%20text.%20Cline%20is%20the%20largest%20of%20its%20kind%20with%0A16%2C642%20sentences%2C%20consisting%20of%20samples%20sourced%20from%20two%20sources%3A%20synthetically%0Agenerated%20code-mixed%20text%20and%20samples%20collected%20from%20online%20social%20media.%20Our%0Aanalysis%20establishes%20that%20popular%20code-mixing%20metrics%20such%20as%20CMI%2C%20Number%20of%0ASwitch%20Points%2C%20Burstines%2C%20which%20are%20used%20to%20filter/curate/compare%20code-mixed%0Acorpora%20have%20low%20correlation%20with%20human%20acceptability%20judgements%2C%20underlining%0Athe%20necessity%20of%20our%20dataset.%20Experiments%20using%20Cline%20demonstrate%20that%20simple%0AMultilayer%20Perceptron%20%28MLP%29%20models%20when%20trained%20solely%20using%20code-mixing%0Ametrics%20as%20features%20are%20outperformed%20by%20fine-tuned%20pre-trained%20Multilingual%0ALarge%20Language%20Models%20%28MLLMs%29.%20Specifically%2C%20among%20Encoder%20models%20XLM-Roberta%0Aand%20Bernice%20outperform%20IndicBERT%20across%20different%20configurations.%20Among%0AEncoder-Decoder%20models%2C%20mBART%20performs%20better%20than%20mT5%2C%20however%20Encoder-Decoder%0Amodels%20are%20not%20able%20to%20outperform%20Encoder-only%20models.%20Decoder-only%20models%0Aperform%20the%20best%20when%20compared%20to%20all%20other%20MLLMS%2C%20with%20Llama%203.2%20-%203B%20models%0Aoutperforming%20similarly%20sized%20Qwen%2C%20Phi%20models.%20Comparison%20with%20zero%20and%0Afewshot%20capabilitites%20of%20ChatGPT%20show%20that%20MLLMs%20fine-tuned%20on%20larger%20data%0Aoutperform%20ChatGPT%2C%20providing%20scope%20for%20improvement%20in%20code-mixed%20tasks.%0AZero-shot%20transfer%20from%20En-Hi%20to%20En-Te%20acceptability%20judgments%20are%20better%20than%0Arandom%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05572v2&entry.124074799=Read"},
{"title": "SDA-GRIN for Adaptive Spatial-Temporal Multivariate Time Series\n  Imputation", "author": "Amir Eskandari and Aman Anand and Drishti Sharma and Farhana Zulkernine", "abstract": "  In various applications, the multivariate time series often suffers from\nmissing data. This issue can significantly disrupt systems that rely on the\ndata. Spatial and temporal dependencies can be leveraged to impute the missing\nsamples. Existing imputation methods often ignore dynamic changes in spatial\ndependencies. We propose a Spatial Dynamic Aware Graph Recurrent Imputation\nNetwork (SDA-GRIN) which is capable of capturing dynamic changes in spatial\ndependencies.SDA-GRIN leverages a multi-head attention mechanism to adapt graph\nstructures with time. SDA-GRIN models multivariate time series as a sequence of\ntemporal graphs and uses a recurrent message-passing architecture for\nimputation. We evaluate SDA-GRIN on four real-world datasets: SDA-GRIN improves\nMSE by 9.51% for the AQI and 9.40% for AQI-36. On the PEMS-BAY dataset, it\nachieves a 1.94% improvement in MSE. Detailed ablation study demonstrates the\neffect of window sizes and missing data on the performance of the method.\nProject page:https://ameskandari.github.io/sda-grin/\n", "link": "http://arxiv.org/abs/2410.03954v2", "date": "2025-05-05", "relevancy": 1.9747, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5002}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4929}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SDA-GRIN%20for%20Adaptive%20Spatial-Temporal%20Multivariate%20Time%20Series%0A%20%20Imputation&body=Title%3A%20SDA-GRIN%20for%20Adaptive%20Spatial-Temporal%20Multivariate%20Time%20Series%0A%20%20Imputation%0AAuthor%3A%20Amir%20Eskandari%20and%20Aman%20Anand%20and%20Drishti%20Sharma%20and%20Farhana%20Zulkernine%0AAbstract%3A%20%20%20In%20various%20applications%2C%20the%20multivariate%20time%20series%20often%20suffers%20from%0Amissing%20data.%20This%20issue%20can%20significantly%20disrupt%20systems%20that%20rely%20on%20the%0Adata.%20Spatial%20and%20temporal%20dependencies%20can%20be%20leveraged%20to%20impute%20the%20missing%0Asamples.%20Existing%20imputation%20methods%20often%20ignore%20dynamic%20changes%20in%20spatial%0Adependencies.%20We%20propose%20a%20Spatial%20Dynamic%20Aware%20Graph%20Recurrent%20Imputation%0ANetwork%20%28SDA-GRIN%29%20which%20is%20capable%20of%20capturing%20dynamic%20changes%20in%20spatial%0Adependencies.SDA-GRIN%20leverages%20a%20multi-head%20attention%20mechanism%20to%20adapt%20graph%0Astructures%20with%20time.%20SDA-GRIN%20models%20multivariate%20time%20series%20as%20a%20sequence%20of%0Atemporal%20graphs%20and%20uses%20a%20recurrent%20message-passing%20architecture%20for%0Aimputation.%20We%20evaluate%20SDA-GRIN%20on%20four%20real-world%20datasets%3A%20SDA-GRIN%20improves%0AMSE%20by%209.51%25%20for%20the%20AQI%20and%209.40%25%20for%20AQI-36.%20On%20the%20PEMS-BAY%20dataset%2C%20it%0Aachieves%20a%201.94%25%20improvement%20in%20MSE.%20Detailed%20ablation%20study%20demonstrates%20the%0Aeffect%20of%20window%20sizes%20and%20missing%20data%20on%20the%20performance%20of%20the%20method.%0AProject%20page%3Ahttps%3A//ameskandari.github.io/sda-grin/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03954v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSDA-GRIN%2520for%2520Adaptive%2520Spatial-Temporal%2520Multivariate%2520Time%2520Series%250A%2520%2520Imputation%26entry.906535625%3DAmir%2520Eskandari%2520and%2520Aman%2520Anand%2520and%2520Drishti%2520Sharma%2520and%2520Farhana%2520Zulkernine%26entry.1292438233%3D%2520%2520In%2520various%2520applications%252C%2520the%2520multivariate%2520time%2520series%2520often%2520suffers%2520from%250Amissing%2520data.%2520This%2520issue%2520can%2520significantly%2520disrupt%2520systems%2520that%2520rely%2520on%2520the%250Adata.%2520Spatial%2520and%2520temporal%2520dependencies%2520can%2520be%2520leveraged%2520to%2520impute%2520the%2520missing%250Asamples.%2520Existing%2520imputation%2520methods%2520often%2520ignore%2520dynamic%2520changes%2520in%2520spatial%250Adependencies.%2520We%2520propose%2520a%2520Spatial%2520Dynamic%2520Aware%2520Graph%2520Recurrent%2520Imputation%250ANetwork%2520%2528SDA-GRIN%2529%2520which%2520is%2520capable%2520of%2520capturing%2520dynamic%2520changes%2520in%2520spatial%250Adependencies.SDA-GRIN%2520leverages%2520a%2520multi-head%2520attention%2520mechanism%2520to%2520adapt%2520graph%250Astructures%2520with%2520time.%2520SDA-GRIN%2520models%2520multivariate%2520time%2520series%2520as%2520a%2520sequence%2520of%250Atemporal%2520graphs%2520and%2520uses%2520a%2520recurrent%2520message-passing%2520architecture%2520for%250Aimputation.%2520We%2520evaluate%2520SDA-GRIN%2520on%2520four%2520real-world%2520datasets%253A%2520SDA-GRIN%2520improves%250AMSE%2520by%25209.51%2525%2520for%2520the%2520AQI%2520and%25209.40%2525%2520for%2520AQI-36.%2520On%2520the%2520PEMS-BAY%2520dataset%252C%2520it%250Aachieves%2520a%25201.94%2525%2520improvement%2520in%2520MSE.%2520Detailed%2520ablation%2520study%2520demonstrates%2520the%250Aeffect%2520of%2520window%2520sizes%2520and%2520missing%2520data%2520on%2520the%2520performance%2520of%2520the%2520method.%250AProject%2520page%253Ahttps%253A//ameskandari.github.io/sda-grin/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03954v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SDA-GRIN%20for%20Adaptive%20Spatial-Temporal%20Multivariate%20Time%20Series%0A%20%20Imputation&entry.906535625=Amir%20Eskandari%20and%20Aman%20Anand%20and%20Drishti%20Sharma%20and%20Farhana%20Zulkernine&entry.1292438233=%20%20In%20various%20applications%2C%20the%20multivariate%20time%20series%20often%20suffers%20from%0Amissing%20data.%20This%20issue%20can%20significantly%20disrupt%20systems%20that%20rely%20on%20the%0Adata.%20Spatial%20and%20temporal%20dependencies%20can%20be%20leveraged%20to%20impute%20the%20missing%0Asamples.%20Existing%20imputation%20methods%20often%20ignore%20dynamic%20changes%20in%20spatial%0Adependencies.%20We%20propose%20a%20Spatial%20Dynamic%20Aware%20Graph%20Recurrent%20Imputation%0ANetwork%20%28SDA-GRIN%29%20which%20is%20capable%20of%20capturing%20dynamic%20changes%20in%20spatial%0Adependencies.SDA-GRIN%20leverages%20a%20multi-head%20attention%20mechanism%20to%20adapt%20graph%0Astructures%20with%20time.%20SDA-GRIN%20models%20multivariate%20time%20series%20as%20a%20sequence%20of%0Atemporal%20graphs%20and%20uses%20a%20recurrent%20message-passing%20architecture%20for%0Aimputation.%20We%20evaluate%20SDA-GRIN%20on%20four%20real-world%20datasets%3A%20SDA-GRIN%20improves%0AMSE%20by%209.51%25%20for%20the%20AQI%20and%209.40%25%20for%20AQI-36.%20On%20the%20PEMS-BAY%20dataset%2C%20it%0Aachieves%20a%201.94%25%20improvement%20in%20MSE.%20Detailed%20ablation%20study%20demonstrates%20the%0Aeffect%20of%20window%20sizes%20and%20missing%20data%20on%20the%20performance%20of%20the%20method.%0AProject%20page%3Ahttps%3A//ameskandari.github.io/sda-grin/%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03954v2&entry.124074799=Read"},
{"title": "Advances in Automated Fetal Brain MRI Segmentation and Biometry:\n  Insights from the FeTA 2024 Challenge", "author": "Vladyslav Zalevskyi and Thomas Sanchez and Misha Kaandorp and Margaux Roulet and Diego Fajardo-Rojas and Liu Li and Jana Hutter and Hongwei Bran Li and Matthew Barkovich and Hui Ji and Luca Wilhelmi and Aline D\u00e4ndliker and C\u00e9line Steger and M\u00e9riam Koob and Yvan Gomez and Anton Jakov\u010di\u0107 and Melita Klai\u0107 and Ana Ad\u017ei\u0107 and Pavel Markovi\u0107 and Gracia Grabari\u0107 and Milan Rados and Jordina Aviles Verdera and Gregor Kasprian and Gregor Dovjak and Raphael Gaubert-Rachm\u00fchl and Maurice Aschwanden and Qi Zeng and Davood Karimi and Denis Peruzzo and Tommaso Ciceri and Giorgio Longari and Rachika E. Hamadache and Amina Bouzid and Xavier Llad\u00f3 and Simone Chiarella and Gerard Mart\u00ed-Juan and Miguel \u00c1ngel Gonz\u00e1lez Ballester and Marco Castellaro and Marco Pinamonti and Valentina Visani and Robin Cremese and Ke\u00efn Sam and Fleur Gaudfernau and Param Ahir and Mehul Parikh and Maximilian Zenk and Michael Baumgartner and Klaus Maier-Hein and Li Tianhong and Yang Hong and Zhao Longfei and Domen Preloznik and \u017diga \u0160piclin and Jae Won Choi and Muyang Li and Jia Fu and Guotai Wang and Jingwen Jiang and Lyuyang Tong and Bo Du and Andrea Gondova and Sungmin You and Kiho Im and Abdul Qayyum and Moona Mazher and Steven A Niederer and Maya Yanko and Bella Specktor-Fadida and Dafna Ben Bashat and Andras Jakab and Roxane Licandro and Kelly Payette and Meritxell Bach Cuadra", "abstract": "  Accurate fetal brain tissue segmentation and biometric analysis are essential\nfor studying brain development in utero. The FeTA Challenge 2024 advanced\nautomated fetal brain MRI analysis by introducing biometry prediction as a new\ntask alongside tissue segmentation. For the first time, our diverse\nmulti-centric test set included data from a new low-field (0.55T) MRI dataset.\nEvaluation metrics were also expanded to include the topology-specific Euler\ncharacteristic difference (ED). Sixteen teams submitted segmentation methods,\nmost of which performed consistently across both high- and low-field scans.\nHowever, longitudinal trends indicate that segmentation accuracy may be\nreaching a plateau, with results now approaching inter-rater variability. The\nED metric uncovered topological differences that were missed by conventional\nmetrics, while the low-field dataset achieved the highest segmentation scores,\nhighlighting the potential of affordable imaging systems when paired with\nhigh-quality reconstruction. Seven teams participated in the biometry task, but\nmost methods failed to outperform a simple baseline that predicted measurements\nbased solely on gestational age, underscoring the challenge of extracting\nreliable biometric estimates from image data alone. Domain shift analysis\nidentified image quality as the most significant factor affecting model\ngeneralization, with super-resolution pipelines also playing a substantial\nrole. Other factors, such as gestational age, pathology, and acquisition site,\nhad smaller, though still measurable, effects. Overall, FeTA 2024 offers a\ncomprehensive benchmark for multi-class segmentation and biometry estimation in\nfetal brain MRI, underscoring the need for data-centric approaches, improved\ntopological evaluation, and greater dataset diversity to enable clinically\nrobust and generalizable AI tools.\n", "link": "http://arxiv.org/abs/2505.02784v1", "date": "2025-05-05", "relevancy": 1.9676, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.496}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4897}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advances%20in%20Automated%20Fetal%20Brain%20MRI%20Segmentation%20and%20Biometry%3A%0A%20%20Insights%20from%20the%20FeTA%202024%20Challenge&body=Title%3A%20Advances%20in%20Automated%20Fetal%20Brain%20MRI%20Segmentation%20and%20Biometry%3A%0A%20%20Insights%20from%20the%20FeTA%202024%20Challenge%0AAuthor%3A%20Vladyslav%20Zalevskyi%20and%20Thomas%20Sanchez%20and%20Misha%20Kaandorp%20and%20Margaux%20Roulet%20and%20Diego%20Fajardo-Rojas%20and%20Liu%20Li%20and%20Jana%20Hutter%20and%20Hongwei%20Bran%20Li%20and%20Matthew%20Barkovich%20and%20Hui%20Ji%20and%20Luca%20Wilhelmi%20and%20Aline%20D%C3%A4ndliker%20and%20C%C3%A9line%20Steger%20and%20M%C3%A9riam%20Koob%20and%20Yvan%20Gomez%20and%20Anton%20Jakov%C4%8Di%C4%87%20and%20Melita%20Klai%C4%87%20and%20Ana%20Ad%C5%BEi%C4%87%20and%20Pavel%20Markovi%C4%87%20and%20Gracia%20Grabari%C4%87%20and%20Milan%20Rados%20and%20Jordina%20Aviles%20Verdera%20and%20Gregor%20Kasprian%20and%20Gregor%20Dovjak%20and%20Raphael%20Gaubert-Rachm%C3%BChl%20and%20Maurice%20Aschwanden%20and%20Qi%20Zeng%20and%20Davood%20Karimi%20and%20Denis%20Peruzzo%20and%20Tommaso%20Ciceri%20and%20Giorgio%20Longari%20and%20Rachika%20E.%20Hamadache%20and%20Amina%20Bouzid%20and%20Xavier%20Llad%C3%B3%20and%20Simone%20Chiarella%20and%20Gerard%20Mart%C3%AD-Juan%20and%20Miguel%20%C3%81ngel%20Gonz%C3%A1lez%20Ballester%20and%20Marco%20Castellaro%20and%20Marco%20Pinamonti%20and%20Valentina%20Visani%20and%20Robin%20Cremese%20and%20Ke%C3%AFn%20Sam%20and%20Fleur%20Gaudfernau%20and%20Param%20Ahir%20and%20Mehul%20Parikh%20and%20Maximilian%20Zenk%20and%20Michael%20Baumgartner%20and%20Klaus%20Maier-Hein%20and%20Li%20Tianhong%20and%20Yang%20Hong%20and%20Zhao%20Longfei%20and%20Domen%20Preloznik%20and%20%C5%BDiga%20%C5%A0piclin%20and%20Jae%20Won%20Choi%20and%20Muyang%20Li%20and%20Jia%20Fu%20and%20Guotai%20Wang%20and%20Jingwen%20Jiang%20and%20Lyuyang%20Tong%20and%20Bo%20Du%20and%20Andrea%20Gondova%20and%20Sungmin%20You%20and%20Kiho%20Im%20and%20Abdul%20Qayyum%20and%20Moona%20Mazher%20and%20Steven%20A%20Niederer%20and%20Maya%20Yanko%20and%20Bella%20Specktor-Fadida%20and%20Dafna%20Ben%20Bashat%20and%20Andras%20Jakab%20and%20Roxane%20Licandro%20and%20Kelly%20Payette%20and%20Meritxell%20Bach%20Cuadra%0AAbstract%3A%20%20%20Accurate%20fetal%20brain%20tissue%20segmentation%20and%20biometric%20analysis%20are%20essential%0Afor%20studying%20brain%20development%20in%20utero.%20The%20FeTA%20Challenge%202024%20advanced%0Aautomated%20fetal%20brain%20MRI%20analysis%20by%20introducing%20biometry%20prediction%20as%20a%20new%0Atask%20alongside%20tissue%20segmentation.%20For%20the%20first%20time%2C%20our%20diverse%0Amulti-centric%20test%20set%20included%20data%20from%20a%20new%20low-field%20%280.55T%29%20MRI%20dataset.%0AEvaluation%20metrics%20were%20also%20expanded%20to%20include%20the%20topology-specific%20Euler%0Acharacteristic%20difference%20%28ED%29.%20Sixteen%20teams%20submitted%20segmentation%20methods%2C%0Amost%20of%20which%20performed%20consistently%20across%20both%20high-%20and%20low-field%20scans.%0AHowever%2C%20longitudinal%20trends%20indicate%20that%20segmentation%20accuracy%20may%20be%0Areaching%20a%20plateau%2C%20with%20results%20now%20approaching%20inter-rater%20variability.%20The%0AED%20metric%20uncovered%20topological%20differences%20that%20were%20missed%20by%20conventional%0Ametrics%2C%20while%20the%20low-field%20dataset%20achieved%20the%20highest%20segmentation%20scores%2C%0Ahighlighting%20the%20potential%20of%20affordable%20imaging%20systems%20when%20paired%20with%0Ahigh-quality%20reconstruction.%20Seven%20teams%20participated%20in%20the%20biometry%20task%2C%20but%0Amost%20methods%20failed%20to%20outperform%20a%20simple%20baseline%20that%20predicted%20measurements%0Abased%20solely%20on%20gestational%20age%2C%20underscoring%20the%20challenge%20of%20extracting%0Areliable%20biometric%20estimates%20from%20image%20data%20alone.%20Domain%20shift%20analysis%0Aidentified%20image%20quality%20as%20the%20most%20significant%20factor%20affecting%20model%0Ageneralization%2C%20with%20super-resolution%20pipelines%20also%20playing%20a%20substantial%0Arole.%20Other%20factors%2C%20such%20as%20gestational%20age%2C%20pathology%2C%20and%20acquisition%20site%2C%0Ahad%20smaller%2C%20though%20still%20measurable%2C%20effects.%20Overall%2C%20FeTA%202024%20offers%20a%0Acomprehensive%20benchmark%20for%20multi-class%20segmentation%20and%20biometry%20estimation%20in%0Afetal%20brain%20MRI%2C%20underscoring%20the%20need%20for%20data-centric%20approaches%2C%20improved%0Atopological%20evaluation%2C%20and%20greater%20dataset%20diversity%20to%20enable%20clinically%0Arobust%20and%20generalizable%20AI%20tools.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02784v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvances%2520in%2520Automated%2520Fetal%2520Brain%2520MRI%2520Segmentation%2520and%2520Biometry%253A%250A%2520%2520Insights%2520from%2520the%2520FeTA%25202024%2520Challenge%26entry.906535625%3DVladyslav%2520Zalevskyi%2520and%2520Thomas%2520Sanchez%2520and%2520Misha%2520Kaandorp%2520and%2520Margaux%2520Roulet%2520and%2520Diego%2520Fajardo-Rojas%2520and%2520Liu%2520Li%2520and%2520Jana%2520Hutter%2520and%2520Hongwei%2520Bran%2520Li%2520and%2520Matthew%2520Barkovich%2520and%2520Hui%2520Ji%2520and%2520Luca%2520Wilhelmi%2520and%2520Aline%2520D%25C3%25A4ndliker%2520and%2520C%25C3%25A9line%2520Steger%2520and%2520M%25C3%25A9riam%2520Koob%2520and%2520Yvan%2520Gomez%2520and%2520Anton%2520Jakov%25C4%258Di%25C4%2587%2520and%2520Melita%2520Klai%25C4%2587%2520and%2520Ana%2520Ad%25C5%25BEi%25C4%2587%2520and%2520Pavel%2520Markovi%25C4%2587%2520and%2520Gracia%2520Grabari%25C4%2587%2520and%2520Milan%2520Rados%2520and%2520Jordina%2520Aviles%2520Verdera%2520and%2520Gregor%2520Kasprian%2520and%2520Gregor%2520Dovjak%2520and%2520Raphael%2520Gaubert-Rachm%25C3%25BChl%2520and%2520Maurice%2520Aschwanden%2520and%2520Qi%2520Zeng%2520and%2520Davood%2520Karimi%2520and%2520Denis%2520Peruzzo%2520and%2520Tommaso%2520Ciceri%2520and%2520Giorgio%2520Longari%2520and%2520Rachika%2520E.%2520Hamadache%2520and%2520Amina%2520Bouzid%2520and%2520Xavier%2520Llad%25C3%25B3%2520and%2520Simone%2520Chiarella%2520and%2520Gerard%2520Mart%25C3%25AD-Juan%2520and%2520Miguel%2520%25C3%2581ngel%2520Gonz%25C3%25A1lez%2520Ballester%2520and%2520Marco%2520Castellaro%2520and%2520Marco%2520Pinamonti%2520and%2520Valentina%2520Visani%2520and%2520Robin%2520Cremese%2520and%2520Ke%25C3%25AFn%2520Sam%2520and%2520Fleur%2520Gaudfernau%2520and%2520Param%2520Ahir%2520and%2520Mehul%2520Parikh%2520and%2520Maximilian%2520Zenk%2520and%2520Michael%2520Baumgartner%2520and%2520Klaus%2520Maier-Hein%2520and%2520Li%2520Tianhong%2520and%2520Yang%2520Hong%2520and%2520Zhao%2520Longfei%2520and%2520Domen%2520Preloznik%2520and%2520%25C5%25BDiga%2520%25C5%25A0piclin%2520and%2520Jae%2520Won%2520Choi%2520and%2520Muyang%2520Li%2520and%2520Jia%2520Fu%2520and%2520Guotai%2520Wang%2520and%2520Jingwen%2520Jiang%2520and%2520Lyuyang%2520Tong%2520and%2520Bo%2520Du%2520and%2520Andrea%2520Gondova%2520and%2520Sungmin%2520You%2520and%2520Kiho%2520Im%2520and%2520Abdul%2520Qayyum%2520and%2520Moona%2520Mazher%2520and%2520Steven%2520A%2520Niederer%2520and%2520Maya%2520Yanko%2520and%2520Bella%2520Specktor-Fadida%2520and%2520Dafna%2520Ben%2520Bashat%2520and%2520Andras%2520Jakab%2520and%2520Roxane%2520Licandro%2520and%2520Kelly%2520Payette%2520and%2520Meritxell%2520Bach%2520Cuadra%26entry.1292438233%3D%2520%2520Accurate%2520fetal%2520brain%2520tissue%2520segmentation%2520and%2520biometric%2520analysis%2520are%2520essential%250Afor%2520studying%2520brain%2520development%2520in%2520utero.%2520The%2520FeTA%2520Challenge%25202024%2520advanced%250Aautomated%2520fetal%2520brain%2520MRI%2520analysis%2520by%2520introducing%2520biometry%2520prediction%2520as%2520a%2520new%250Atask%2520alongside%2520tissue%2520segmentation.%2520For%2520the%2520first%2520time%252C%2520our%2520diverse%250Amulti-centric%2520test%2520set%2520included%2520data%2520from%2520a%2520new%2520low-field%2520%25280.55T%2529%2520MRI%2520dataset.%250AEvaluation%2520metrics%2520were%2520also%2520expanded%2520to%2520include%2520the%2520topology-specific%2520Euler%250Acharacteristic%2520difference%2520%2528ED%2529.%2520Sixteen%2520teams%2520submitted%2520segmentation%2520methods%252C%250Amost%2520of%2520which%2520performed%2520consistently%2520across%2520both%2520high-%2520and%2520low-field%2520scans.%250AHowever%252C%2520longitudinal%2520trends%2520indicate%2520that%2520segmentation%2520accuracy%2520may%2520be%250Areaching%2520a%2520plateau%252C%2520with%2520results%2520now%2520approaching%2520inter-rater%2520variability.%2520The%250AED%2520metric%2520uncovered%2520topological%2520differences%2520that%2520were%2520missed%2520by%2520conventional%250Ametrics%252C%2520while%2520the%2520low-field%2520dataset%2520achieved%2520the%2520highest%2520segmentation%2520scores%252C%250Ahighlighting%2520the%2520potential%2520of%2520affordable%2520imaging%2520systems%2520when%2520paired%2520with%250Ahigh-quality%2520reconstruction.%2520Seven%2520teams%2520participated%2520in%2520the%2520biometry%2520task%252C%2520but%250Amost%2520methods%2520failed%2520to%2520outperform%2520a%2520simple%2520baseline%2520that%2520predicted%2520measurements%250Abased%2520solely%2520on%2520gestational%2520age%252C%2520underscoring%2520the%2520challenge%2520of%2520extracting%250Areliable%2520biometric%2520estimates%2520from%2520image%2520data%2520alone.%2520Domain%2520shift%2520analysis%250Aidentified%2520image%2520quality%2520as%2520the%2520most%2520significant%2520factor%2520affecting%2520model%250Ageneralization%252C%2520with%2520super-resolution%2520pipelines%2520also%2520playing%2520a%2520substantial%250Arole.%2520Other%2520factors%252C%2520such%2520as%2520gestational%2520age%252C%2520pathology%252C%2520and%2520acquisition%2520site%252C%250Ahad%2520smaller%252C%2520though%2520still%2520measurable%252C%2520effects.%2520Overall%252C%2520FeTA%25202024%2520offers%2520a%250Acomprehensive%2520benchmark%2520for%2520multi-class%2520segmentation%2520and%2520biometry%2520estimation%2520in%250Afetal%2520brain%2520MRI%252C%2520underscoring%2520the%2520need%2520for%2520data-centric%2520approaches%252C%2520improved%250Atopological%2520evaluation%252C%2520and%2520greater%2520dataset%2520diversity%2520to%2520enable%2520clinically%250Arobust%2520and%2520generalizable%2520AI%2520tools.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02784v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advances%20in%20Automated%20Fetal%20Brain%20MRI%20Segmentation%20and%20Biometry%3A%0A%20%20Insights%20from%20the%20FeTA%202024%20Challenge&entry.906535625=Vladyslav%20Zalevskyi%20and%20Thomas%20Sanchez%20and%20Misha%20Kaandorp%20and%20Margaux%20Roulet%20and%20Diego%20Fajardo-Rojas%20and%20Liu%20Li%20and%20Jana%20Hutter%20and%20Hongwei%20Bran%20Li%20and%20Matthew%20Barkovich%20and%20Hui%20Ji%20and%20Luca%20Wilhelmi%20and%20Aline%20D%C3%A4ndliker%20and%20C%C3%A9line%20Steger%20and%20M%C3%A9riam%20Koob%20and%20Yvan%20Gomez%20and%20Anton%20Jakov%C4%8Di%C4%87%20and%20Melita%20Klai%C4%87%20and%20Ana%20Ad%C5%BEi%C4%87%20and%20Pavel%20Markovi%C4%87%20and%20Gracia%20Grabari%C4%87%20and%20Milan%20Rados%20and%20Jordina%20Aviles%20Verdera%20and%20Gregor%20Kasprian%20and%20Gregor%20Dovjak%20and%20Raphael%20Gaubert-Rachm%C3%BChl%20and%20Maurice%20Aschwanden%20and%20Qi%20Zeng%20and%20Davood%20Karimi%20and%20Denis%20Peruzzo%20and%20Tommaso%20Ciceri%20and%20Giorgio%20Longari%20and%20Rachika%20E.%20Hamadache%20and%20Amina%20Bouzid%20and%20Xavier%20Llad%C3%B3%20and%20Simone%20Chiarella%20and%20Gerard%20Mart%C3%AD-Juan%20and%20Miguel%20%C3%81ngel%20Gonz%C3%A1lez%20Ballester%20and%20Marco%20Castellaro%20and%20Marco%20Pinamonti%20and%20Valentina%20Visani%20and%20Robin%20Cremese%20and%20Ke%C3%AFn%20Sam%20and%20Fleur%20Gaudfernau%20and%20Param%20Ahir%20and%20Mehul%20Parikh%20and%20Maximilian%20Zenk%20and%20Michael%20Baumgartner%20and%20Klaus%20Maier-Hein%20and%20Li%20Tianhong%20and%20Yang%20Hong%20and%20Zhao%20Longfei%20and%20Domen%20Preloznik%20and%20%C5%BDiga%20%C5%A0piclin%20and%20Jae%20Won%20Choi%20and%20Muyang%20Li%20and%20Jia%20Fu%20and%20Guotai%20Wang%20and%20Jingwen%20Jiang%20and%20Lyuyang%20Tong%20and%20Bo%20Du%20and%20Andrea%20Gondova%20and%20Sungmin%20You%20and%20Kiho%20Im%20and%20Abdul%20Qayyum%20and%20Moona%20Mazher%20and%20Steven%20A%20Niederer%20and%20Maya%20Yanko%20and%20Bella%20Specktor-Fadida%20and%20Dafna%20Ben%20Bashat%20and%20Andras%20Jakab%20and%20Roxane%20Licandro%20and%20Kelly%20Payette%20and%20Meritxell%20Bach%20Cuadra&entry.1292438233=%20%20Accurate%20fetal%20brain%20tissue%20segmentation%20and%20biometric%20analysis%20are%20essential%0Afor%20studying%20brain%20development%20in%20utero.%20The%20FeTA%20Challenge%202024%20advanced%0Aautomated%20fetal%20brain%20MRI%20analysis%20by%20introducing%20biometry%20prediction%20as%20a%20new%0Atask%20alongside%20tissue%20segmentation.%20For%20the%20first%20time%2C%20our%20diverse%0Amulti-centric%20test%20set%20included%20data%20from%20a%20new%20low-field%20%280.55T%29%20MRI%20dataset.%0AEvaluation%20metrics%20were%20also%20expanded%20to%20include%20the%20topology-specific%20Euler%0Acharacteristic%20difference%20%28ED%29.%20Sixteen%20teams%20submitted%20segmentation%20methods%2C%0Amost%20of%20which%20performed%20consistently%20across%20both%20high-%20and%20low-field%20scans.%0AHowever%2C%20longitudinal%20trends%20indicate%20that%20segmentation%20accuracy%20may%20be%0Areaching%20a%20plateau%2C%20with%20results%20now%20approaching%20inter-rater%20variability.%20The%0AED%20metric%20uncovered%20topological%20differences%20that%20were%20missed%20by%20conventional%0Ametrics%2C%20while%20the%20low-field%20dataset%20achieved%20the%20highest%20segmentation%20scores%2C%0Ahighlighting%20the%20potential%20of%20affordable%20imaging%20systems%20when%20paired%20with%0Ahigh-quality%20reconstruction.%20Seven%20teams%20participated%20in%20the%20biometry%20task%2C%20but%0Amost%20methods%20failed%20to%20outperform%20a%20simple%20baseline%20that%20predicted%20measurements%0Abased%20solely%20on%20gestational%20age%2C%20underscoring%20the%20challenge%20of%20extracting%0Areliable%20biometric%20estimates%20from%20image%20data%20alone.%20Domain%20shift%20analysis%0Aidentified%20image%20quality%20as%20the%20most%20significant%20factor%20affecting%20model%0Ageneralization%2C%20with%20super-resolution%20pipelines%20also%20playing%20a%20substantial%0Arole.%20Other%20factors%2C%20such%20as%20gestational%20age%2C%20pathology%2C%20and%20acquisition%20site%2C%0Ahad%20smaller%2C%20though%20still%20measurable%2C%20effects.%20Overall%2C%20FeTA%202024%20offers%20a%0Acomprehensive%20benchmark%20for%20multi-class%20segmentation%20and%20biometry%20estimation%20in%0Afetal%20brain%20MRI%2C%20underscoring%20the%20need%20for%20data-centric%20approaches%2C%20improved%0Atopological%20evaluation%2C%20and%20greater%20dataset%20diversity%20to%20enable%20clinically%0Arobust%20and%20generalizable%20AI%20tools.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02784v1&entry.124074799=Read"},
{"title": "YARE-GAN: Yet Another Resting State EEG-GAN", "author": "Yeganeh Farahzadi and Morteza Ansarinia and Zoltan Kekecs", "abstract": "  In this study, we implement a Wasserstein GAN with Gradient Penalty (WGAN-GP)\nto generate multi-channel resting-state EEG data and assess the quality of the\nsynthesized signals through both visual and feature-based evaluations. Our\nresults indicate that the model effectively captures the statistical and\nspectral characteristics of real EEG data, although challenges remain in\nreplicating high-frequency oscillations in the frontal region. Additionally, we\ndemonstrate that the Critic's learned representations can be reused for gender\nclassification task, achieving an out-of-sample accuracy, significantly better\nthan a shuffled-label baseline and a model trained directly on EEG data. These\nfindings suggest that generative models can serve not only as EEG data\ngenerators but also as unsupervised feature extractors, reducing the need for\nmanual feature engineering. This study highlights the potential of GAN-based\nunsupervised learning for EEG analysis, suggesting avenues for more\ndata-efficient deep learning applications in neuroscience.\n", "link": "http://arxiv.org/abs/2503.02636v3", "date": "2025-05-05", "relevancy": 1.9658, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.508}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4833}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20YARE-GAN%3A%20Yet%20Another%20Resting%20State%20EEG-GAN&body=Title%3A%20YARE-GAN%3A%20Yet%20Another%20Resting%20State%20EEG-GAN%0AAuthor%3A%20Yeganeh%20Farahzadi%20and%20Morteza%20Ansarinia%20and%20Zoltan%20Kekecs%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20implement%20a%20Wasserstein%20GAN%20with%20Gradient%20Penalty%20%28WGAN-GP%29%0Ato%20generate%20multi-channel%20resting-state%20EEG%20data%20and%20assess%20the%20quality%20of%20the%0Asynthesized%20signals%20through%20both%20visual%20and%20feature-based%20evaluations.%20Our%0Aresults%20indicate%20that%20the%20model%20effectively%20captures%20the%20statistical%20and%0Aspectral%20characteristics%20of%20real%20EEG%20data%2C%20although%20challenges%20remain%20in%0Areplicating%20high-frequency%20oscillations%20in%20the%20frontal%20region.%20Additionally%2C%20we%0Ademonstrate%20that%20the%20Critic%27s%20learned%20representations%20can%20be%20reused%20for%20gender%0Aclassification%20task%2C%20achieving%20an%20out-of-sample%20accuracy%2C%20significantly%20better%0Athan%20a%20shuffled-label%20baseline%20and%20a%20model%20trained%20directly%20on%20EEG%20data.%20These%0Afindings%20suggest%20that%20generative%20models%20can%20serve%20not%20only%20as%20EEG%20data%0Agenerators%20but%20also%20as%20unsupervised%20feature%20extractors%2C%20reducing%20the%20need%20for%0Amanual%20feature%20engineering.%20This%20study%20highlights%20the%20potential%20of%20GAN-based%0Aunsupervised%20learning%20for%20EEG%20analysis%2C%20suggesting%20avenues%20for%20more%0Adata-efficient%20deep%20learning%20applications%20in%20neuroscience.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.02636v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYARE-GAN%253A%2520Yet%2520Another%2520Resting%2520State%2520EEG-GAN%26entry.906535625%3DYeganeh%2520Farahzadi%2520and%2520Morteza%2520Ansarinia%2520and%2520Zoltan%2520Kekecs%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520implement%2520a%2520Wasserstein%2520GAN%2520with%2520Gradient%2520Penalty%2520%2528WGAN-GP%2529%250Ato%2520generate%2520multi-channel%2520resting-state%2520EEG%2520data%2520and%2520assess%2520the%2520quality%2520of%2520the%250Asynthesized%2520signals%2520through%2520both%2520visual%2520and%2520feature-based%2520evaluations.%2520Our%250Aresults%2520indicate%2520that%2520the%2520model%2520effectively%2520captures%2520the%2520statistical%2520and%250Aspectral%2520characteristics%2520of%2520real%2520EEG%2520data%252C%2520although%2520challenges%2520remain%2520in%250Areplicating%2520high-frequency%2520oscillations%2520in%2520the%2520frontal%2520region.%2520Additionally%252C%2520we%250Ademonstrate%2520that%2520the%2520Critic%2527s%2520learned%2520representations%2520can%2520be%2520reused%2520for%2520gender%250Aclassification%2520task%252C%2520achieving%2520an%2520out-of-sample%2520accuracy%252C%2520significantly%2520better%250Athan%2520a%2520shuffled-label%2520baseline%2520and%2520a%2520model%2520trained%2520directly%2520on%2520EEG%2520data.%2520These%250Afindings%2520suggest%2520that%2520generative%2520models%2520can%2520serve%2520not%2520only%2520as%2520EEG%2520data%250Agenerators%2520but%2520also%2520as%2520unsupervised%2520feature%2520extractors%252C%2520reducing%2520the%2520need%2520for%250Amanual%2520feature%2520engineering.%2520This%2520study%2520highlights%2520the%2520potential%2520of%2520GAN-based%250Aunsupervised%2520learning%2520for%2520EEG%2520analysis%252C%2520suggesting%2520avenues%2520for%2520more%250Adata-efficient%2520deep%2520learning%2520applications%2520in%2520neuroscience.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.02636v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=YARE-GAN%3A%20Yet%20Another%20Resting%20State%20EEG-GAN&entry.906535625=Yeganeh%20Farahzadi%20and%20Morteza%20Ansarinia%20and%20Zoltan%20Kekecs&entry.1292438233=%20%20In%20this%20study%2C%20we%20implement%20a%20Wasserstein%20GAN%20with%20Gradient%20Penalty%20%28WGAN-GP%29%0Ato%20generate%20multi-channel%20resting-state%20EEG%20data%20and%20assess%20the%20quality%20of%20the%0Asynthesized%20signals%20through%20both%20visual%20and%20feature-based%20evaluations.%20Our%0Aresults%20indicate%20that%20the%20model%20effectively%20captures%20the%20statistical%20and%0Aspectral%20characteristics%20of%20real%20EEG%20data%2C%20although%20challenges%20remain%20in%0Areplicating%20high-frequency%20oscillations%20in%20the%20frontal%20region.%20Additionally%2C%20we%0Ademonstrate%20that%20the%20Critic%27s%20learned%20representations%20can%20be%20reused%20for%20gender%0Aclassification%20task%2C%20achieving%20an%20out-of-sample%20accuracy%2C%20significantly%20better%0Athan%20a%20shuffled-label%20baseline%20and%20a%20model%20trained%20directly%20on%20EEG%20data.%20These%0Afindings%20suggest%20that%20generative%20models%20can%20serve%20not%20only%20as%20EEG%20data%0Agenerators%20but%20also%20as%20unsupervised%20feature%20extractors%2C%20reducing%20the%20need%20for%0Amanual%20feature%20engineering.%20This%20study%20highlights%20the%20potential%20of%20GAN-based%0Aunsupervised%20learning%20for%20EEG%20analysis%2C%20suggesting%20avenues%20for%20more%0Adata-efficient%20deep%20learning%20applications%20in%20neuroscience.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.02636v3&entry.124074799=Read"},
{"title": "Spatio-Tempora Metric-Semantic Mapping for Persistent Orchard\n  Monitoring: Method and Dataset", "author": "Jiuzhou Lei and Ankit Prabhu and Xu Liu and Fernando Cladera and Mehrad Mortazavi and Reza Ehsani and Pratik Chaudhari and Vijay Kumar", "abstract": "  Monitoring orchards at the individual tree or fruit level throughout the\ngrowth season is crucial for plant phenotyping and horticultural resource\noptimization, such as chemical use and yield estimation. We present a 4D\nspatio-temporal metric-semantic mapping system that integrates multi-session\nmeasurements to track fruit growth over time. Our approach combines a LiDAR-RGB\nfusion module for 3D fruit localization with a 4D fruit association method\nleveraging positional, visual, and topology information for improved data\nassociation precision. Evaluated on real orchard data, our method achieves a\n96.9% fruit counting accuracy for 1,790 apples across 60 trees, a mean fruit\nsize estimation error of 1.1 cm, and a 23.7% improvement in 4D data association\nprecision over baselines. We publicly release a multimodal dataset covering\nfive fruit species across their growth seasons.\nhttps://4d-metric-semantic-mapping.org/\n", "link": "http://arxiv.org/abs/2409.19786v2", "date": "2025-05-05", "relevancy": 1.9611, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.526}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4908}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4754}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatio-Tempora%20Metric-Semantic%20Mapping%20for%20Persistent%20Orchard%0A%20%20Monitoring%3A%20Method%20and%20Dataset&body=Title%3A%20Spatio-Tempora%20Metric-Semantic%20Mapping%20for%20Persistent%20Orchard%0A%20%20Monitoring%3A%20Method%20and%20Dataset%0AAuthor%3A%20Jiuzhou%20Lei%20and%20Ankit%20Prabhu%20and%20Xu%20Liu%20and%20Fernando%20Cladera%20and%20Mehrad%20Mortazavi%20and%20Reza%20Ehsani%20and%20Pratik%20Chaudhari%20and%20Vijay%20Kumar%0AAbstract%3A%20%20%20Monitoring%20orchards%20at%20the%20individual%20tree%20or%20fruit%20level%20throughout%20the%0Agrowth%20season%20is%20crucial%20for%20plant%20phenotyping%20and%20horticultural%20resource%0Aoptimization%2C%20such%20as%20chemical%20use%20and%20yield%20estimation.%20We%20present%20a%204D%0Aspatio-temporal%20metric-semantic%20mapping%20system%20that%20integrates%20multi-session%0Ameasurements%20to%20track%20fruit%20growth%20over%20time.%20Our%20approach%20combines%20a%20LiDAR-RGB%0Afusion%20module%20for%203D%20fruit%20localization%20with%20a%204D%20fruit%20association%20method%0Aleveraging%20positional%2C%20visual%2C%20and%20topology%20information%20for%20improved%20data%0Aassociation%20precision.%20Evaluated%20on%20real%20orchard%20data%2C%20our%20method%20achieves%20a%0A96.9%25%20fruit%20counting%20accuracy%20for%201%2C790%20apples%20across%2060%20trees%2C%20a%20mean%20fruit%0Asize%20estimation%20error%20of%201.1%20cm%2C%20and%20a%2023.7%25%20improvement%20in%204D%20data%20association%0Aprecision%20over%20baselines.%20We%20publicly%20release%20a%20multimodal%20dataset%20covering%0Afive%20fruit%20species%20across%20their%20growth%20seasons.%0Ahttps%3A//4d-metric-semantic-mapping.org/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.19786v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatio-Tempora%2520Metric-Semantic%2520Mapping%2520for%2520Persistent%2520Orchard%250A%2520%2520Monitoring%253A%2520Method%2520and%2520Dataset%26entry.906535625%3DJiuzhou%2520Lei%2520and%2520Ankit%2520Prabhu%2520and%2520Xu%2520Liu%2520and%2520Fernando%2520Cladera%2520and%2520Mehrad%2520Mortazavi%2520and%2520Reza%2520Ehsani%2520and%2520Pratik%2520Chaudhari%2520and%2520Vijay%2520Kumar%26entry.1292438233%3D%2520%2520Monitoring%2520orchards%2520at%2520the%2520individual%2520tree%2520or%2520fruit%2520level%2520throughout%2520the%250Agrowth%2520season%2520is%2520crucial%2520for%2520plant%2520phenotyping%2520and%2520horticultural%2520resource%250Aoptimization%252C%2520such%2520as%2520chemical%2520use%2520and%2520yield%2520estimation.%2520We%2520present%2520a%25204D%250Aspatio-temporal%2520metric-semantic%2520mapping%2520system%2520that%2520integrates%2520multi-session%250Ameasurements%2520to%2520track%2520fruit%2520growth%2520over%2520time.%2520Our%2520approach%2520combines%2520a%2520LiDAR-RGB%250Afusion%2520module%2520for%25203D%2520fruit%2520localization%2520with%2520a%25204D%2520fruit%2520association%2520method%250Aleveraging%2520positional%252C%2520visual%252C%2520and%2520topology%2520information%2520for%2520improved%2520data%250Aassociation%2520precision.%2520Evaluated%2520on%2520real%2520orchard%2520data%252C%2520our%2520method%2520achieves%2520a%250A96.9%2525%2520fruit%2520counting%2520accuracy%2520for%25201%252C790%2520apples%2520across%252060%2520trees%252C%2520a%2520mean%2520fruit%250Asize%2520estimation%2520error%2520of%25201.1%2520cm%252C%2520and%2520a%252023.7%2525%2520improvement%2520in%25204D%2520data%2520association%250Aprecision%2520over%2520baselines.%2520We%2520publicly%2520release%2520a%2520multimodal%2520dataset%2520covering%250Afive%2520fruit%2520species%2520across%2520their%2520growth%2520seasons.%250Ahttps%253A//4d-metric-semantic-mapping.org/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.19786v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatio-Tempora%20Metric-Semantic%20Mapping%20for%20Persistent%20Orchard%0A%20%20Monitoring%3A%20Method%20and%20Dataset&entry.906535625=Jiuzhou%20Lei%20and%20Ankit%20Prabhu%20and%20Xu%20Liu%20and%20Fernando%20Cladera%20and%20Mehrad%20Mortazavi%20and%20Reza%20Ehsani%20and%20Pratik%20Chaudhari%20and%20Vijay%20Kumar&entry.1292438233=%20%20Monitoring%20orchards%20at%20the%20individual%20tree%20or%20fruit%20level%20throughout%20the%0Agrowth%20season%20is%20crucial%20for%20plant%20phenotyping%20and%20horticultural%20resource%0Aoptimization%2C%20such%20as%20chemical%20use%20and%20yield%20estimation.%20We%20present%20a%204D%0Aspatio-temporal%20metric-semantic%20mapping%20system%20that%20integrates%20multi-session%0Ameasurements%20to%20track%20fruit%20growth%20over%20time.%20Our%20approach%20combines%20a%20LiDAR-RGB%0Afusion%20module%20for%203D%20fruit%20localization%20with%20a%204D%20fruit%20association%20method%0Aleveraging%20positional%2C%20visual%2C%20and%20topology%20information%20for%20improved%20data%0Aassociation%20precision.%20Evaluated%20on%20real%20orchard%20data%2C%20our%20method%20achieves%20a%0A96.9%25%20fruit%20counting%20accuracy%20for%201%2C790%20apples%20across%2060%20trees%2C%20a%20mean%20fruit%0Asize%20estimation%20error%20of%201.1%20cm%2C%20and%20a%2023.7%25%20improvement%20in%204D%20data%20association%0Aprecision%20over%20baselines.%20We%20publicly%20release%20a%20multimodal%20dataset%20covering%0Afive%20fruit%20species%20across%20their%20growth%20seasons.%0Ahttps%3A//4d-metric-semantic-mapping.org/%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.19786v2&entry.124074799=Read"},
{"title": "FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language\n  Models", "author": "Zhouliang Yu and Ruotian Peng and Keyi Ding and Yizhe Li and Zhongyuan Peng and Minghao Liu and Yifan Zhang and Zheng Yuan and Huajian Xin and Wenhao Huang and Yandong Wen and Ge Zhang and Weiyang Liu", "abstract": "  Formal mathematical reasoning remains a critical challenge for artificial\nintelligence, hindered by limitations of existing benchmarks in scope and\nscale. To address this, we present FormalMATH, a large-scale Lean4 benchmark\ncomprising 5,560 formally verified problems spanning from high-school Olympiad\nchallenges to undergraduate-level theorems across diverse domains (e.g.,\nalgebra, applied mathematics, calculus, number theory, and discrete\nmathematics). To mitigate the inefficiency of manual formalization, we\nintroduce a novel human-in-the-loop autoformalization pipeline that integrates:\n(1) specialized large language models (LLMs) for statement autoformalization,\n(2) multi-LLM semantic verification, and (3) negation-based disproof filtering\nstrategies using off-the-shelf LLM-based provers. This approach reduces expert\nannotation costs by retaining 72.09% of statements before manual verification\nwhile ensuring fidelity to the original natural-language problems. Our\nevaluation of state-of-the-art LLM-based theorem provers reveals significant\nlimitations: even the strongest models achieve only 16.46% success rate under\npractical sampling budgets, exhibiting pronounced domain bias (e.g., excelling\nin algebra but failing in calculus) and over-reliance on simplified automation\ntactics. Notably, we identify a counterintuitive inverse relationship between\nnatural-language solution guidance and proof success in chain-of-thought\nreasoning scenarios, suggesting that human-written informal reasoning\nintroduces noise rather than clarity in the formal reasoning settings. We\nbelieve that FormalMATH provides a robust benchmark for benchmarking formal\nmathematical reasoning.\n", "link": "http://arxiv.org/abs/2505.02735v1", "date": "2025-05-05", "relevancy": 1.9586, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4947}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4947}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4647}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FormalMATH%3A%20Benchmarking%20Formal%20Mathematical%20Reasoning%20of%20Large%20Language%0A%20%20Models&body=Title%3A%20FormalMATH%3A%20Benchmarking%20Formal%20Mathematical%20Reasoning%20of%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Zhouliang%20Yu%20and%20Ruotian%20Peng%20and%20Keyi%20Ding%20and%20Yizhe%20Li%20and%20Zhongyuan%20Peng%20and%20Minghao%20Liu%20and%20Yifan%20Zhang%20and%20Zheng%20Yuan%20and%20Huajian%20Xin%20and%20Wenhao%20Huang%20and%20Yandong%20Wen%20and%20Ge%20Zhang%20and%20Weiyang%20Liu%0AAbstract%3A%20%20%20Formal%20mathematical%20reasoning%20remains%20a%20critical%20challenge%20for%20artificial%0Aintelligence%2C%20hindered%20by%20limitations%20of%20existing%20benchmarks%20in%20scope%20and%0Ascale.%20To%20address%20this%2C%20we%20present%20FormalMATH%2C%20a%20large-scale%20Lean4%20benchmark%0Acomprising%205%2C560%20formally%20verified%20problems%20spanning%20from%20high-school%20Olympiad%0Achallenges%20to%20undergraduate-level%20theorems%20across%20diverse%20domains%20%28e.g.%2C%0Aalgebra%2C%20applied%20mathematics%2C%20calculus%2C%20number%20theory%2C%20and%20discrete%0Amathematics%29.%20To%20mitigate%20the%20inefficiency%20of%20manual%20formalization%2C%20we%0Aintroduce%20a%20novel%20human-in-the-loop%20autoformalization%20pipeline%20that%20integrates%3A%0A%281%29%20specialized%20large%20language%20models%20%28LLMs%29%20for%20statement%20autoformalization%2C%0A%282%29%20multi-LLM%20semantic%20verification%2C%20and%20%283%29%20negation-based%20disproof%20filtering%0Astrategies%20using%20off-the-shelf%20LLM-based%20provers.%20This%20approach%20reduces%20expert%0Aannotation%20costs%20by%20retaining%2072.09%25%20of%20statements%20before%20manual%20verification%0Awhile%20ensuring%20fidelity%20to%20the%20original%20natural-language%20problems.%20Our%0Aevaluation%20of%20state-of-the-art%20LLM-based%20theorem%20provers%20reveals%20significant%0Alimitations%3A%20even%20the%20strongest%20models%20achieve%20only%2016.46%25%20success%20rate%20under%0Apractical%20sampling%20budgets%2C%20exhibiting%20pronounced%20domain%20bias%20%28e.g.%2C%20excelling%0Ain%20algebra%20but%20failing%20in%20calculus%29%20and%20over-reliance%20on%20simplified%20automation%0Atactics.%20Notably%2C%20we%20identify%20a%20counterintuitive%20inverse%20relationship%20between%0Anatural-language%20solution%20guidance%20and%20proof%20success%20in%20chain-of-thought%0Areasoning%20scenarios%2C%20suggesting%20that%20human-written%20informal%20reasoning%0Aintroduces%20noise%20rather%20than%20clarity%20in%20the%20formal%20reasoning%20settings.%20We%0Abelieve%20that%20FormalMATH%20provides%20a%20robust%20benchmark%20for%20benchmarking%20formal%0Amathematical%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02735v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFormalMATH%253A%2520Benchmarking%2520Formal%2520Mathematical%2520Reasoning%2520of%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DZhouliang%2520Yu%2520and%2520Ruotian%2520Peng%2520and%2520Keyi%2520Ding%2520and%2520Yizhe%2520Li%2520and%2520Zhongyuan%2520Peng%2520and%2520Minghao%2520Liu%2520and%2520Yifan%2520Zhang%2520and%2520Zheng%2520Yuan%2520and%2520Huajian%2520Xin%2520and%2520Wenhao%2520Huang%2520and%2520Yandong%2520Wen%2520and%2520Ge%2520Zhang%2520and%2520Weiyang%2520Liu%26entry.1292438233%3D%2520%2520Formal%2520mathematical%2520reasoning%2520remains%2520a%2520critical%2520challenge%2520for%2520artificial%250Aintelligence%252C%2520hindered%2520by%2520limitations%2520of%2520existing%2520benchmarks%2520in%2520scope%2520and%250Ascale.%2520To%2520address%2520this%252C%2520we%2520present%2520FormalMATH%252C%2520a%2520large-scale%2520Lean4%2520benchmark%250Acomprising%25205%252C560%2520formally%2520verified%2520problems%2520spanning%2520from%2520high-school%2520Olympiad%250Achallenges%2520to%2520undergraduate-level%2520theorems%2520across%2520diverse%2520domains%2520%2528e.g.%252C%250Aalgebra%252C%2520applied%2520mathematics%252C%2520calculus%252C%2520number%2520theory%252C%2520and%2520discrete%250Amathematics%2529.%2520To%2520mitigate%2520the%2520inefficiency%2520of%2520manual%2520formalization%252C%2520we%250Aintroduce%2520a%2520novel%2520human-in-the-loop%2520autoformalization%2520pipeline%2520that%2520integrates%253A%250A%25281%2529%2520specialized%2520large%2520language%2520models%2520%2528LLMs%2529%2520for%2520statement%2520autoformalization%252C%250A%25282%2529%2520multi-LLM%2520semantic%2520verification%252C%2520and%2520%25283%2529%2520negation-based%2520disproof%2520filtering%250Astrategies%2520using%2520off-the-shelf%2520LLM-based%2520provers.%2520This%2520approach%2520reduces%2520expert%250Aannotation%2520costs%2520by%2520retaining%252072.09%2525%2520of%2520statements%2520before%2520manual%2520verification%250Awhile%2520ensuring%2520fidelity%2520to%2520the%2520original%2520natural-language%2520problems.%2520Our%250Aevaluation%2520of%2520state-of-the-art%2520LLM-based%2520theorem%2520provers%2520reveals%2520significant%250Alimitations%253A%2520even%2520the%2520strongest%2520models%2520achieve%2520only%252016.46%2525%2520success%2520rate%2520under%250Apractical%2520sampling%2520budgets%252C%2520exhibiting%2520pronounced%2520domain%2520bias%2520%2528e.g.%252C%2520excelling%250Ain%2520algebra%2520but%2520failing%2520in%2520calculus%2529%2520and%2520over-reliance%2520on%2520simplified%2520automation%250Atactics.%2520Notably%252C%2520we%2520identify%2520a%2520counterintuitive%2520inverse%2520relationship%2520between%250Anatural-language%2520solution%2520guidance%2520and%2520proof%2520success%2520in%2520chain-of-thought%250Areasoning%2520scenarios%252C%2520suggesting%2520that%2520human-written%2520informal%2520reasoning%250Aintroduces%2520noise%2520rather%2520than%2520clarity%2520in%2520the%2520formal%2520reasoning%2520settings.%2520We%250Abelieve%2520that%2520FormalMATH%2520provides%2520a%2520robust%2520benchmark%2520for%2520benchmarking%2520formal%250Amathematical%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02735v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FormalMATH%3A%20Benchmarking%20Formal%20Mathematical%20Reasoning%20of%20Large%20Language%0A%20%20Models&entry.906535625=Zhouliang%20Yu%20and%20Ruotian%20Peng%20and%20Keyi%20Ding%20and%20Yizhe%20Li%20and%20Zhongyuan%20Peng%20and%20Minghao%20Liu%20and%20Yifan%20Zhang%20and%20Zheng%20Yuan%20and%20Huajian%20Xin%20and%20Wenhao%20Huang%20and%20Yandong%20Wen%20and%20Ge%20Zhang%20and%20Weiyang%20Liu&entry.1292438233=%20%20Formal%20mathematical%20reasoning%20remains%20a%20critical%20challenge%20for%20artificial%0Aintelligence%2C%20hindered%20by%20limitations%20of%20existing%20benchmarks%20in%20scope%20and%0Ascale.%20To%20address%20this%2C%20we%20present%20FormalMATH%2C%20a%20large-scale%20Lean4%20benchmark%0Acomprising%205%2C560%20formally%20verified%20problems%20spanning%20from%20high-school%20Olympiad%0Achallenges%20to%20undergraduate-level%20theorems%20across%20diverse%20domains%20%28e.g.%2C%0Aalgebra%2C%20applied%20mathematics%2C%20calculus%2C%20number%20theory%2C%20and%20discrete%0Amathematics%29.%20To%20mitigate%20the%20inefficiency%20of%20manual%20formalization%2C%20we%0Aintroduce%20a%20novel%20human-in-the-loop%20autoformalization%20pipeline%20that%20integrates%3A%0A%281%29%20specialized%20large%20language%20models%20%28LLMs%29%20for%20statement%20autoformalization%2C%0A%282%29%20multi-LLM%20semantic%20verification%2C%20and%20%283%29%20negation-based%20disproof%20filtering%0Astrategies%20using%20off-the-shelf%20LLM-based%20provers.%20This%20approach%20reduces%20expert%0Aannotation%20costs%20by%20retaining%2072.09%25%20of%20statements%20before%20manual%20verification%0Awhile%20ensuring%20fidelity%20to%20the%20original%20natural-language%20problems.%20Our%0Aevaluation%20of%20state-of-the-art%20LLM-based%20theorem%20provers%20reveals%20significant%0Alimitations%3A%20even%20the%20strongest%20models%20achieve%20only%2016.46%25%20success%20rate%20under%0Apractical%20sampling%20budgets%2C%20exhibiting%20pronounced%20domain%20bias%20%28e.g.%2C%20excelling%0Ain%20algebra%20but%20failing%20in%20calculus%29%20and%20over-reliance%20on%20simplified%20automation%0Atactics.%20Notably%2C%20we%20identify%20a%20counterintuitive%20inverse%20relationship%20between%0Anatural-language%20solution%20guidance%20and%20proof%20success%20in%20chain-of-thought%0Areasoning%20scenarios%2C%20suggesting%20that%20human-written%20informal%20reasoning%0Aintroduces%20noise%20rather%20than%20clarity%20in%20the%20formal%20reasoning%20settings.%20We%0Abelieve%20that%20FormalMATH%20provides%20a%20robust%20benchmark%20for%20benchmarking%20formal%0Amathematical%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02735v1&entry.124074799=Read"},
{"title": "Low-Loss Space in Neural Networks is Continuous and Fully Connected", "author": "Yongding Tian and Zaid Al-Ars and Maksim Kitsak and Peter Hofstee", "abstract": "  Visualizations of the loss landscape in neural networks suggest that minima\nare isolated points. However, both theoretical and empirical studies indicate\nthat it is possible to connect two different minima with a path consisting of\nintermediate points that also have low loss. In this study, we propose a new\nalgorithm which investigates low-loss paths in the full parameter space, not\nonly between two minima. Our experiments on LeNet5, ResNet18, and Compact\nConvolutional Transformer architectures consistently demonstrate the existence\nof such continuous paths in the parameter space. These results suggest that the\nlow-loss region is a fully connected and continuous space in the parameter\nspace. Our findings provide theoretical insight into neural network\nover-parameterization, highlighting that parameters collectively define a\nhigh-dimensional low-loss space, implying parameter redundancy exists only\nwithin individual models and not throughout the entire low-loss space.\nAdditionally, our work also provides new visualization methods and\nopportunities to improve model generalization by exploring the low-loss space\nthat is closer to the origin.\n", "link": "http://arxiv.org/abs/2505.02604v1", "date": "2025-05-05", "relevancy": 1.947, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4961}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4803}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4799}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-Loss%20Space%20in%20Neural%20Networks%20is%20Continuous%20and%20Fully%20Connected&body=Title%3A%20Low-Loss%20Space%20in%20Neural%20Networks%20is%20Continuous%20and%20Fully%20Connected%0AAuthor%3A%20Yongding%20Tian%20and%20Zaid%20Al-Ars%20and%20Maksim%20Kitsak%20and%20Peter%20Hofstee%0AAbstract%3A%20%20%20Visualizations%20of%20the%20loss%20landscape%20in%20neural%20networks%20suggest%20that%20minima%0Aare%20isolated%20points.%20However%2C%20both%20theoretical%20and%20empirical%20studies%20indicate%0Athat%20it%20is%20possible%20to%20connect%20two%20different%20minima%20with%20a%20path%20consisting%20of%0Aintermediate%20points%20that%20also%20have%20low%20loss.%20In%20this%20study%2C%20we%20propose%20a%20new%0Aalgorithm%20which%20investigates%20low-loss%20paths%20in%20the%20full%20parameter%20space%2C%20not%0Aonly%20between%20two%20minima.%20Our%20experiments%20on%20LeNet5%2C%20ResNet18%2C%20and%20Compact%0AConvolutional%20Transformer%20architectures%20consistently%20demonstrate%20the%20existence%0Aof%20such%20continuous%20paths%20in%20the%20parameter%20space.%20These%20results%20suggest%20that%20the%0Alow-loss%20region%20is%20a%20fully%20connected%20and%20continuous%20space%20in%20the%20parameter%0Aspace.%20Our%20findings%20provide%20theoretical%20insight%20into%20neural%20network%0Aover-parameterization%2C%20highlighting%20that%20parameters%20collectively%20define%20a%0Ahigh-dimensional%20low-loss%20space%2C%20implying%20parameter%20redundancy%20exists%20only%0Awithin%20individual%20models%20and%20not%20throughout%20the%20entire%20low-loss%20space.%0AAdditionally%2C%20our%20work%20also%20provides%20new%20visualization%20methods%20and%0Aopportunities%20to%20improve%20model%20generalization%20by%20exploring%20the%20low-loss%20space%0Athat%20is%20closer%20to%20the%20origin.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02604v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-Loss%2520Space%2520in%2520Neural%2520Networks%2520is%2520Continuous%2520and%2520Fully%2520Connected%26entry.906535625%3DYongding%2520Tian%2520and%2520Zaid%2520Al-Ars%2520and%2520Maksim%2520Kitsak%2520and%2520Peter%2520Hofstee%26entry.1292438233%3D%2520%2520Visualizations%2520of%2520the%2520loss%2520landscape%2520in%2520neural%2520networks%2520suggest%2520that%2520minima%250Aare%2520isolated%2520points.%2520However%252C%2520both%2520theoretical%2520and%2520empirical%2520studies%2520indicate%250Athat%2520it%2520is%2520possible%2520to%2520connect%2520two%2520different%2520minima%2520with%2520a%2520path%2520consisting%2520of%250Aintermediate%2520points%2520that%2520also%2520have%2520low%2520loss.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520new%250Aalgorithm%2520which%2520investigates%2520low-loss%2520paths%2520in%2520the%2520full%2520parameter%2520space%252C%2520not%250Aonly%2520between%2520two%2520minima.%2520Our%2520experiments%2520on%2520LeNet5%252C%2520ResNet18%252C%2520and%2520Compact%250AConvolutional%2520Transformer%2520architectures%2520consistently%2520demonstrate%2520the%2520existence%250Aof%2520such%2520continuous%2520paths%2520in%2520the%2520parameter%2520space.%2520These%2520results%2520suggest%2520that%2520the%250Alow-loss%2520region%2520is%2520a%2520fully%2520connected%2520and%2520continuous%2520space%2520in%2520the%2520parameter%250Aspace.%2520Our%2520findings%2520provide%2520theoretical%2520insight%2520into%2520neural%2520network%250Aover-parameterization%252C%2520highlighting%2520that%2520parameters%2520collectively%2520define%2520a%250Ahigh-dimensional%2520low-loss%2520space%252C%2520implying%2520parameter%2520redundancy%2520exists%2520only%250Awithin%2520individual%2520models%2520and%2520not%2520throughout%2520the%2520entire%2520low-loss%2520space.%250AAdditionally%252C%2520our%2520work%2520also%2520provides%2520new%2520visualization%2520methods%2520and%250Aopportunities%2520to%2520improve%2520model%2520generalization%2520by%2520exploring%2520the%2520low-loss%2520space%250Athat%2520is%2520closer%2520to%2520the%2520origin.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02604v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Loss%20Space%20in%20Neural%20Networks%20is%20Continuous%20and%20Fully%20Connected&entry.906535625=Yongding%20Tian%20and%20Zaid%20Al-Ars%20and%20Maksim%20Kitsak%20and%20Peter%20Hofstee&entry.1292438233=%20%20Visualizations%20of%20the%20loss%20landscape%20in%20neural%20networks%20suggest%20that%20minima%0Aare%20isolated%20points.%20However%2C%20both%20theoretical%20and%20empirical%20studies%20indicate%0Athat%20it%20is%20possible%20to%20connect%20two%20different%20minima%20with%20a%20path%20consisting%20of%0Aintermediate%20points%20that%20also%20have%20low%20loss.%20In%20this%20study%2C%20we%20propose%20a%20new%0Aalgorithm%20which%20investigates%20low-loss%20paths%20in%20the%20full%20parameter%20space%2C%20not%0Aonly%20between%20two%20minima.%20Our%20experiments%20on%20LeNet5%2C%20ResNet18%2C%20and%20Compact%0AConvolutional%20Transformer%20architectures%20consistently%20demonstrate%20the%20existence%0Aof%20such%20continuous%20paths%20in%20the%20parameter%20space.%20These%20results%20suggest%20that%20the%0Alow-loss%20region%20is%20a%20fully%20connected%20and%20continuous%20space%20in%20the%20parameter%0Aspace.%20Our%20findings%20provide%20theoretical%20insight%20into%20neural%20network%0Aover-parameterization%2C%20highlighting%20that%20parameters%20collectively%20define%20a%0Ahigh-dimensional%20low-loss%20space%2C%20implying%20parameter%20redundancy%20exists%20only%0Awithin%20individual%20models%20and%20not%20throughout%20the%20entire%20low-loss%20space.%0AAdditionally%2C%20our%20work%20also%20provides%20new%20visualization%20methods%20and%0Aopportunities%20to%20improve%20model%20generalization%20by%20exploring%20the%20low-loss%20space%0Athat%20is%20closer%20to%20the%20origin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02604v1&entry.124074799=Read"},
{"title": "Knowledge Graphs for Enhancing Large Language Models in Entity\n  Disambiguation", "author": "Pons Gerard and Bilalli Besim and Queralt Anna", "abstract": "  Recent advances in Large Language Models (LLMs) have positioned them as a\nprominent solution for Natural Language Processing tasks. Notably, they can\napproach these problems in a zero or few-shot manner, thereby eliminating the\nneed for training or fine-tuning task-specific models. However, LLMs face some\nchallenges, including hallucination and the presence of outdated knowledge or\nmissing information from specific domains in the training data. These problems\ncannot be easily solved by retraining the models with new data as it is a\ntime-consuming and expensive process. To mitigate these issues, Knowledge\nGraphs (KGs) have been proposed as a structured external source of information\nto enrich LLMs. With this idea, in this work we use KGs to enhance LLMs for\nzero-shot Entity Disambiguation (ED). For that purpose, we leverage the\nhierarchical representation of the entities' classes in a KG to gradually prune\nthe candidate space as well as the entities' descriptions to enrich the input\nprompt with additional factual knowledge. Our evaluation on popular ED datasets\nshows that the proposed method outperforms non-enhanced and description-only\nenhanced LLMs, and has a higher degree of adaptability than task-specific\nmodels. Furthermore, we conduct an error analysis and discuss the impact of the\nleveraged KG's semantic expressivity on the ED performance.\n", "link": "http://arxiv.org/abs/2505.02737v1", "date": "2025-05-05", "relevancy": 1.9458, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5413}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4755}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge%20Graphs%20for%20Enhancing%20Large%20Language%20Models%20in%20Entity%0A%20%20Disambiguation&body=Title%3A%20Knowledge%20Graphs%20for%20Enhancing%20Large%20Language%20Models%20in%20Entity%0A%20%20Disambiguation%0AAuthor%3A%20Pons%20Gerard%20and%20Bilalli%20Besim%20and%20Queralt%20Anna%0AAbstract%3A%20%20%20Recent%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20positioned%20them%20as%20a%0Aprominent%20solution%20for%20Natural%20Language%20Processing%20tasks.%20Notably%2C%20they%20can%0Aapproach%20these%20problems%20in%20a%20zero%20or%20few-shot%20manner%2C%20thereby%20eliminating%20the%0Aneed%20for%20training%20or%20fine-tuning%20task-specific%20models.%20However%2C%20LLMs%20face%20some%0Achallenges%2C%20including%20hallucination%20and%20the%20presence%20of%20outdated%20knowledge%20or%0Amissing%20information%20from%20specific%20domains%20in%20the%20training%20data.%20These%20problems%0Acannot%20be%20easily%20solved%20by%20retraining%20the%20models%20with%20new%20data%20as%20it%20is%20a%0Atime-consuming%20and%20expensive%20process.%20To%20mitigate%20these%20issues%2C%20Knowledge%0AGraphs%20%28KGs%29%20have%20been%20proposed%20as%20a%20structured%20external%20source%20of%20information%0Ato%20enrich%20LLMs.%20With%20this%20idea%2C%20in%20this%20work%20we%20use%20KGs%20to%20enhance%20LLMs%20for%0Azero-shot%20Entity%20Disambiguation%20%28ED%29.%20For%20that%20purpose%2C%20we%20leverage%20the%0Ahierarchical%20representation%20of%20the%20entities%27%20classes%20in%20a%20KG%20to%20gradually%20prune%0Athe%20candidate%20space%20as%20well%20as%20the%20entities%27%20descriptions%20to%20enrich%20the%20input%0Aprompt%20with%20additional%20factual%20knowledge.%20Our%20evaluation%20on%20popular%20ED%20datasets%0Ashows%20that%20the%20proposed%20method%20outperforms%20non-enhanced%20and%20description-only%0Aenhanced%20LLMs%2C%20and%20has%20a%20higher%20degree%20of%20adaptability%20than%20task-specific%0Amodels.%20Furthermore%2C%20we%20conduct%20an%20error%20analysis%20and%20discuss%20the%20impact%20of%20the%0Aleveraged%20KG%27s%20semantic%20expressivity%20on%20the%20ED%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02737v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge%2520Graphs%2520for%2520Enhancing%2520Large%2520Language%2520Models%2520in%2520Entity%250A%2520%2520Disambiguation%26entry.906535625%3DPons%2520Gerard%2520and%2520Bilalli%2520Besim%2520and%2520Queralt%2520Anna%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520positioned%2520them%2520as%2520a%250Aprominent%2520solution%2520for%2520Natural%2520Language%2520Processing%2520tasks.%2520Notably%252C%2520they%2520can%250Aapproach%2520these%2520problems%2520in%2520a%2520zero%2520or%2520few-shot%2520manner%252C%2520thereby%2520eliminating%2520the%250Aneed%2520for%2520training%2520or%2520fine-tuning%2520task-specific%2520models.%2520However%252C%2520LLMs%2520face%2520some%250Achallenges%252C%2520including%2520hallucination%2520and%2520the%2520presence%2520of%2520outdated%2520knowledge%2520or%250Amissing%2520information%2520from%2520specific%2520domains%2520in%2520the%2520training%2520data.%2520These%2520problems%250Acannot%2520be%2520easily%2520solved%2520by%2520retraining%2520the%2520models%2520with%2520new%2520data%2520as%2520it%2520is%2520a%250Atime-consuming%2520and%2520expensive%2520process.%2520To%2520mitigate%2520these%2520issues%252C%2520Knowledge%250AGraphs%2520%2528KGs%2529%2520have%2520been%2520proposed%2520as%2520a%2520structured%2520external%2520source%2520of%2520information%250Ato%2520enrich%2520LLMs.%2520With%2520this%2520idea%252C%2520in%2520this%2520work%2520we%2520use%2520KGs%2520to%2520enhance%2520LLMs%2520for%250Azero-shot%2520Entity%2520Disambiguation%2520%2528ED%2529.%2520For%2520that%2520purpose%252C%2520we%2520leverage%2520the%250Ahierarchical%2520representation%2520of%2520the%2520entities%2527%2520classes%2520in%2520a%2520KG%2520to%2520gradually%2520prune%250Athe%2520candidate%2520space%2520as%2520well%2520as%2520the%2520entities%2527%2520descriptions%2520to%2520enrich%2520the%2520input%250Aprompt%2520with%2520additional%2520factual%2520knowledge.%2520Our%2520evaluation%2520on%2520popular%2520ED%2520datasets%250Ashows%2520that%2520the%2520proposed%2520method%2520outperforms%2520non-enhanced%2520and%2520description-only%250Aenhanced%2520LLMs%252C%2520and%2520has%2520a%2520higher%2520degree%2520of%2520adaptability%2520than%2520task-specific%250Amodels.%2520Furthermore%252C%2520we%2520conduct%2520an%2520error%2520analysis%2520and%2520discuss%2520the%2520impact%2520of%2520the%250Aleveraged%2520KG%2527s%2520semantic%2520expressivity%2520on%2520the%2520ED%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02737v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge%20Graphs%20for%20Enhancing%20Large%20Language%20Models%20in%20Entity%0A%20%20Disambiguation&entry.906535625=Pons%20Gerard%20and%20Bilalli%20Besim%20and%20Queralt%20Anna&entry.1292438233=%20%20Recent%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20positioned%20them%20as%20a%0Aprominent%20solution%20for%20Natural%20Language%20Processing%20tasks.%20Notably%2C%20they%20can%0Aapproach%20these%20problems%20in%20a%20zero%20or%20few-shot%20manner%2C%20thereby%20eliminating%20the%0Aneed%20for%20training%20or%20fine-tuning%20task-specific%20models.%20However%2C%20LLMs%20face%20some%0Achallenges%2C%20including%20hallucination%20and%20the%20presence%20of%20outdated%20knowledge%20or%0Amissing%20information%20from%20specific%20domains%20in%20the%20training%20data.%20These%20problems%0Acannot%20be%20easily%20solved%20by%20retraining%20the%20models%20with%20new%20data%20as%20it%20is%20a%0Atime-consuming%20and%20expensive%20process.%20To%20mitigate%20these%20issues%2C%20Knowledge%0AGraphs%20%28KGs%29%20have%20been%20proposed%20as%20a%20structured%20external%20source%20of%20information%0Ato%20enrich%20LLMs.%20With%20this%20idea%2C%20in%20this%20work%20we%20use%20KGs%20to%20enhance%20LLMs%20for%0Azero-shot%20Entity%20Disambiguation%20%28ED%29.%20For%20that%20purpose%2C%20we%20leverage%20the%0Ahierarchical%20representation%20of%20the%20entities%27%20classes%20in%20a%20KG%20to%20gradually%20prune%0Athe%20candidate%20space%20as%20well%20as%20the%20entities%27%20descriptions%20to%20enrich%20the%20input%0Aprompt%20with%20additional%20factual%20knowledge.%20Our%20evaluation%20on%20popular%20ED%20datasets%0Ashows%20that%20the%20proposed%20method%20outperforms%20non-enhanced%20and%20description-only%0Aenhanced%20LLMs%2C%20and%20has%20a%20higher%20degree%20of%20adaptability%20than%20task-specific%0Amodels.%20Furthermore%2C%20we%20conduct%20an%20error%20analysis%20and%20discuss%20the%20impact%20of%20the%0Aleveraged%20KG%27s%20semantic%20expressivity%20on%20the%20ED%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02737v1&entry.124074799=Read"},
{"title": "Recursive Decomposition with Dependencies for Generic Divide-and-Conquer\n  Reasoning", "author": "Sergio Hern\u00e1ndez-Guti\u00e9rrez and Minttu Alakuijala and Alexander V. Nikitin and Pekka Marttinen", "abstract": "  Reasoning tasks are crucial in many domains, especially in science and\nengineering. Although large language models (LLMs) have made progress in\nreasoning tasks using techniques such as chain-of-thought and least-to-most\nprompting, these approaches still do not effectively scale to complex problems\nin either their performance or execution time. Moreover, they often require\nadditional supervision for each new task, such as in-context examples. In this\nwork, we introduce Recursive Decomposition with Dependencies (RDD), a scalable\ndivide-and-conquer method for solving reasoning problems that requires less\nsupervision than prior approaches. Our method can be directly applied to a new\nproblem class even in the absence of any task-specific guidance. Furthermore,\nRDD supports sub-task dependencies, allowing for ordered execution of\nsub-tasks, as well as an error recovery mechanism that can correct mistakes\nmade in previous steps. We evaluate our approach on two benchmarks with six\ndifficulty levels each and in two in-context settings: one with task-specific\nexamples and one without. Our results demonstrate that RDD outperforms other\nmethods in a compute-matched setting as task complexity increases, while also\nbeing more computationally efficient.\n", "link": "http://arxiv.org/abs/2505.02576v1", "date": "2025-05-05", "relevancy": 1.9405, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.502}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4817}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recursive%20Decomposition%20with%20Dependencies%20for%20Generic%20Divide-and-Conquer%0A%20%20Reasoning&body=Title%3A%20Recursive%20Decomposition%20with%20Dependencies%20for%20Generic%20Divide-and-Conquer%0A%20%20Reasoning%0AAuthor%3A%20Sergio%20Hern%C3%A1ndez-Guti%C3%A9rrez%20and%20Minttu%20Alakuijala%20and%20Alexander%20V.%20Nikitin%20and%20Pekka%20Marttinen%0AAbstract%3A%20%20%20Reasoning%20tasks%20are%20crucial%20in%20many%20domains%2C%20especially%20in%20science%20and%0Aengineering.%20Although%20large%20language%20models%20%28LLMs%29%20have%20made%20progress%20in%0Areasoning%20tasks%20using%20techniques%20such%20as%20chain-of-thought%20and%20least-to-most%0Aprompting%2C%20these%20approaches%20still%20do%20not%20effectively%20scale%20to%20complex%20problems%0Ain%20either%20their%20performance%20or%20execution%20time.%20Moreover%2C%20they%20often%20require%0Aadditional%20supervision%20for%20each%20new%20task%2C%20such%20as%20in-context%20examples.%20In%20this%0Awork%2C%20we%20introduce%20Recursive%20Decomposition%20with%20Dependencies%20%28RDD%29%2C%20a%20scalable%0Adivide-and-conquer%20method%20for%20solving%20reasoning%20problems%20that%20requires%20less%0Asupervision%20than%20prior%20approaches.%20Our%20method%20can%20be%20directly%20applied%20to%20a%20new%0Aproblem%20class%20even%20in%20the%20absence%20of%20any%20task-specific%20guidance.%20Furthermore%2C%0ARDD%20supports%20sub-task%20dependencies%2C%20allowing%20for%20ordered%20execution%20of%0Asub-tasks%2C%20as%20well%20as%20an%20error%20recovery%20mechanism%20that%20can%20correct%20mistakes%0Amade%20in%20previous%20steps.%20We%20evaluate%20our%20approach%20on%20two%20benchmarks%20with%20six%0Adifficulty%20levels%20each%20and%20in%20two%20in-context%20settings%3A%20one%20with%20task-specific%0Aexamples%20and%20one%20without.%20Our%20results%20demonstrate%20that%20RDD%20outperforms%20other%0Amethods%20in%20a%20compute-matched%20setting%20as%20task%20complexity%20increases%2C%20while%20also%0Abeing%20more%20computationally%20efficient.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02576v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecursive%2520Decomposition%2520with%2520Dependencies%2520for%2520Generic%2520Divide-and-Conquer%250A%2520%2520Reasoning%26entry.906535625%3DSergio%2520Hern%25C3%25A1ndez-Guti%25C3%25A9rrez%2520and%2520Minttu%2520Alakuijala%2520and%2520Alexander%2520V.%2520Nikitin%2520and%2520Pekka%2520Marttinen%26entry.1292438233%3D%2520%2520Reasoning%2520tasks%2520are%2520crucial%2520in%2520many%2520domains%252C%2520especially%2520in%2520science%2520and%250Aengineering.%2520Although%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520made%2520progress%2520in%250Areasoning%2520tasks%2520using%2520techniques%2520such%2520as%2520chain-of-thought%2520and%2520least-to-most%250Aprompting%252C%2520these%2520approaches%2520still%2520do%2520not%2520effectively%2520scale%2520to%2520complex%2520problems%250Ain%2520either%2520their%2520performance%2520or%2520execution%2520time.%2520Moreover%252C%2520they%2520often%2520require%250Aadditional%2520supervision%2520for%2520each%2520new%2520task%252C%2520such%2520as%2520in-context%2520examples.%2520In%2520this%250Awork%252C%2520we%2520introduce%2520Recursive%2520Decomposition%2520with%2520Dependencies%2520%2528RDD%2529%252C%2520a%2520scalable%250Adivide-and-conquer%2520method%2520for%2520solving%2520reasoning%2520problems%2520that%2520requires%2520less%250Asupervision%2520than%2520prior%2520approaches.%2520Our%2520method%2520can%2520be%2520directly%2520applied%2520to%2520a%2520new%250Aproblem%2520class%2520even%2520in%2520the%2520absence%2520of%2520any%2520task-specific%2520guidance.%2520Furthermore%252C%250ARDD%2520supports%2520sub-task%2520dependencies%252C%2520allowing%2520for%2520ordered%2520execution%2520of%250Asub-tasks%252C%2520as%2520well%2520as%2520an%2520error%2520recovery%2520mechanism%2520that%2520can%2520correct%2520mistakes%250Amade%2520in%2520previous%2520steps.%2520We%2520evaluate%2520our%2520approach%2520on%2520two%2520benchmarks%2520with%2520six%250Adifficulty%2520levels%2520each%2520and%2520in%2520two%2520in-context%2520settings%253A%2520one%2520with%2520task-specific%250Aexamples%2520and%2520one%2520without.%2520Our%2520results%2520demonstrate%2520that%2520RDD%2520outperforms%2520other%250Amethods%2520in%2520a%2520compute-matched%2520setting%2520as%2520task%2520complexity%2520increases%252C%2520while%2520also%250Abeing%2520more%2520computationally%2520efficient.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02576v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recursive%20Decomposition%20with%20Dependencies%20for%20Generic%20Divide-and-Conquer%0A%20%20Reasoning&entry.906535625=Sergio%20Hern%C3%A1ndez-Guti%C3%A9rrez%20and%20Minttu%20Alakuijala%20and%20Alexander%20V.%20Nikitin%20and%20Pekka%20Marttinen&entry.1292438233=%20%20Reasoning%20tasks%20are%20crucial%20in%20many%20domains%2C%20especially%20in%20science%20and%0Aengineering.%20Although%20large%20language%20models%20%28LLMs%29%20have%20made%20progress%20in%0Areasoning%20tasks%20using%20techniques%20such%20as%20chain-of-thought%20and%20least-to-most%0Aprompting%2C%20these%20approaches%20still%20do%20not%20effectively%20scale%20to%20complex%20problems%0Ain%20either%20their%20performance%20or%20execution%20time.%20Moreover%2C%20they%20often%20require%0Aadditional%20supervision%20for%20each%20new%20task%2C%20such%20as%20in-context%20examples.%20In%20this%0Awork%2C%20we%20introduce%20Recursive%20Decomposition%20with%20Dependencies%20%28RDD%29%2C%20a%20scalable%0Adivide-and-conquer%20method%20for%20solving%20reasoning%20problems%20that%20requires%20less%0Asupervision%20than%20prior%20approaches.%20Our%20method%20can%20be%20directly%20applied%20to%20a%20new%0Aproblem%20class%20even%20in%20the%20absence%20of%20any%20task-specific%20guidance.%20Furthermore%2C%0ARDD%20supports%20sub-task%20dependencies%2C%20allowing%20for%20ordered%20execution%20of%0Asub-tasks%2C%20as%20well%20as%20an%20error%20recovery%20mechanism%20that%20can%20correct%20mistakes%0Amade%20in%20previous%20steps.%20We%20evaluate%20our%20approach%20on%20two%20benchmarks%20with%20six%0Adifficulty%20levels%20each%20and%20in%20two%20in-context%20settings%3A%20one%20with%20task-specific%0Aexamples%20and%20one%20without.%20Our%20results%20demonstrate%20that%20RDD%20outperforms%20other%0Amethods%20in%20a%20compute-matched%20setting%20as%20task%20complexity%20increases%2C%20while%20also%0Abeing%20more%20computationally%20efficient.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02576v1&entry.124074799=Read"},
{"title": "Aerodynamic and structural airfoil shape optimisation via Transfer\n  Learning-enhanced Deep Reinforcement Learning", "author": "David Ramos and Lucas Lacasa and Eusebio Valero and Gonzalo Rubio", "abstract": "  The main objective of this paper is to introduce a transfer\nlearning-enhanced, multi-objective, deep reinforcement learning (DRL)\nmethodology that is able to optimise the geometry of any airfoil based on\nconcomitant aerodynamic and structural criteria. To showcase the method, we aim\nto maximise the lift-to-drag ratio $C_L/C_D$ while preserving the structural\nintegrity of the airfoil -- as modelled by its maximum thickness -- and train\nthe DRL agent using a list of different transfer learning (TL) strategies. The\nperformance of the DRL agent is compared with Particle Swarm Optimisation\n(PSO), a traditional gradient-free optimisation method. Results indicate that\nDRL agents are able to perform multi-objective shape optimisation, that the DRL\napproach outperforms PSO in terms of computational efficiency and shape\noptimisation performance, and that the TL-enhanced DRL agent achieves\nperformance comparable to the DRL one, while further saving substantial\ncomputational resources.\n", "link": "http://arxiv.org/abs/2505.02634v1", "date": "2025-05-05", "relevancy": 1.9399, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4922}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4876}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aerodynamic%20and%20structural%20airfoil%20shape%20optimisation%20via%20Transfer%0A%20%20Learning-enhanced%20Deep%20Reinforcement%20Learning&body=Title%3A%20Aerodynamic%20and%20structural%20airfoil%20shape%20optimisation%20via%20Transfer%0A%20%20Learning-enhanced%20Deep%20Reinforcement%20Learning%0AAuthor%3A%20David%20Ramos%20and%20Lucas%20Lacasa%20and%20Eusebio%20Valero%20and%20Gonzalo%20Rubio%0AAbstract%3A%20%20%20The%20main%20objective%20of%20this%20paper%20is%20to%20introduce%20a%20transfer%0Alearning-enhanced%2C%20multi-objective%2C%20deep%20reinforcement%20learning%20%28DRL%29%0Amethodology%20that%20is%20able%20to%20optimise%20the%20geometry%20of%20any%20airfoil%20based%20on%0Aconcomitant%20aerodynamic%20and%20structural%20criteria.%20To%20showcase%20the%20method%2C%20we%20aim%0Ato%20maximise%20the%20lift-to-drag%20ratio%20%24C_L/C_D%24%20while%20preserving%20the%20structural%0Aintegrity%20of%20the%20airfoil%20--%20as%20modelled%20by%20its%20maximum%20thickness%20--%20and%20train%0Athe%20DRL%20agent%20using%20a%20list%20of%20different%20transfer%20learning%20%28TL%29%20strategies.%20The%0Aperformance%20of%20the%20DRL%20agent%20is%20compared%20with%20Particle%20Swarm%20Optimisation%0A%28PSO%29%2C%20a%20traditional%20gradient-free%20optimisation%20method.%20Results%20indicate%20that%0ADRL%20agents%20are%20able%20to%20perform%20multi-objective%20shape%20optimisation%2C%20that%20the%20DRL%0Aapproach%20outperforms%20PSO%20in%20terms%20of%20computational%20efficiency%20and%20shape%0Aoptimisation%20performance%2C%20and%20that%20the%20TL-enhanced%20DRL%20agent%20achieves%0Aperformance%20comparable%20to%20the%20DRL%20one%2C%20while%20further%20saving%20substantial%0Acomputational%20resources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02634v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAerodynamic%2520and%2520structural%2520airfoil%2520shape%2520optimisation%2520via%2520Transfer%250A%2520%2520Learning-enhanced%2520Deep%2520Reinforcement%2520Learning%26entry.906535625%3DDavid%2520Ramos%2520and%2520Lucas%2520Lacasa%2520and%2520Eusebio%2520Valero%2520and%2520Gonzalo%2520Rubio%26entry.1292438233%3D%2520%2520The%2520main%2520objective%2520of%2520this%2520paper%2520is%2520to%2520introduce%2520a%2520transfer%250Alearning-enhanced%252C%2520multi-objective%252C%2520deep%2520reinforcement%2520learning%2520%2528DRL%2529%250Amethodology%2520that%2520is%2520able%2520to%2520optimise%2520the%2520geometry%2520of%2520any%2520airfoil%2520based%2520on%250Aconcomitant%2520aerodynamic%2520and%2520structural%2520criteria.%2520To%2520showcase%2520the%2520method%252C%2520we%2520aim%250Ato%2520maximise%2520the%2520lift-to-drag%2520ratio%2520%2524C_L/C_D%2524%2520while%2520preserving%2520the%2520structural%250Aintegrity%2520of%2520the%2520airfoil%2520--%2520as%2520modelled%2520by%2520its%2520maximum%2520thickness%2520--%2520and%2520train%250Athe%2520DRL%2520agent%2520using%2520a%2520list%2520of%2520different%2520transfer%2520learning%2520%2528TL%2529%2520strategies.%2520The%250Aperformance%2520of%2520the%2520DRL%2520agent%2520is%2520compared%2520with%2520Particle%2520Swarm%2520Optimisation%250A%2528PSO%2529%252C%2520a%2520traditional%2520gradient-free%2520optimisation%2520method.%2520Results%2520indicate%2520that%250ADRL%2520agents%2520are%2520able%2520to%2520perform%2520multi-objective%2520shape%2520optimisation%252C%2520that%2520the%2520DRL%250Aapproach%2520outperforms%2520PSO%2520in%2520terms%2520of%2520computational%2520efficiency%2520and%2520shape%250Aoptimisation%2520performance%252C%2520and%2520that%2520the%2520TL-enhanced%2520DRL%2520agent%2520achieves%250Aperformance%2520comparable%2520to%2520the%2520DRL%2520one%252C%2520while%2520further%2520saving%2520substantial%250Acomputational%2520resources.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02634v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aerodynamic%20and%20structural%20airfoil%20shape%20optimisation%20via%20Transfer%0A%20%20Learning-enhanced%20Deep%20Reinforcement%20Learning&entry.906535625=David%20Ramos%20and%20Lucas%20Lacasa%20and%20Eusebio%20Valero%20and%20Gonzalo%20Rubio&entry.1292438233=%20%20The%20main%20objective%20of%20this%20paper%20is%20to%20introduce%20a%20transfer%0Alearning-enhanced%2C%20multi-objective%2C%20deep%20reinforcement%20learning%20%28DRL%29%0Amethodology%20that%20is%20able%20to%20optimise%20the%20geometry%20of%20any%20airfoil%20based%20on%0Aconcomitant%20aerodynamic%20and%20structural%20criteria.%20To%20showcase%20the%20method%2C%20we%20aim%0Ato%20maximise%20the%20lift-to-drag%20ratio%20%24C_L/C_D%24%20while%20preserving%20the%20structural%0Aintegrity%20of%20the%20airfoil%20--%20as%20modelled%20by%20its%20maximum%20thickness%20--%20and%20train%0Athe%20DRL%20agent%20using%20a%20list%20of%20different%20transfer%20learning%20%28TL%29%20strategies.%20The%0Aperformance%20of%20the%20DRL%20agent%20is%20compared%20with%20Particle%20Swarm%20Optimisation%0A%28PSO%29%2C%20a%20traditional%20gradient-free%20optimisation%20method.%20Results%20indicate%20that%0ADRL%20agents%20are%20able%20to%20perform%20multi-objective%20shape%20optimisation%2C%20that%20the%20DRL%0Aapproach%20outperforms%20PSO%20in%20terms%20of%20computational%20efficiency%20and%20shape%0Aoptimisation%20performance%2C%20and%20that%20the%20TL-enhanced%20DRL%20agent%20achieves%0Aperformance%20comparable%20to%20the%20DRL%20one%2C%20while%20further%20saving%20substantial%0Acomputational%20resources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02634v1&entry.124074799=Read"},
{"title": "RobustDexGrasp: Robust Dexterous Grasping of General Objects", "author": "Hui Zhang and Zijian Wu and Linyi Huang and Sammy Christen and Jie Song", "abstract": "  The ability to robustly grasp a variety of objects is essential for dexterous\nrobots. In this paper, we present a framework for zero-shot dynamic dexterous\ngrasping using single-view visual inputs, designed to be resilient to various\ndisturbances. Our approach utilizes a hand-centric object shape representation\nbased on dynamic distance vectors between finger joints and object surfaces.\nThis representation captures the local shape around potential contact regions\nrather than focusing on detailed global object geometry, thereby enhancing\ngeneralization to shape variations and uncertainties. To address perception\nlimitations, we integrate a privileged teacher policy with a mixed curriculum\nlearning approach, allowing the student policy to effectively distill grasping\ncapabilities and explore for adaptation to disturbances. Trained in simulation,\nour method achieves success rates of 97.0% across 247,786 simulated objects and\n94.6% across 512 real objects, demonstrating remarkable generalization.\nQuantitative and qualitative results validate the robustness of our policy\nagainst various disturbances.\n", "link": "http://arxiv.org/abs/2504.05287v2", "date": "2025-05-05", "relevancy": 1.9349, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6991}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6055}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RobustDexGrasp%3A%20Robust%20Dexterous%20Grasping%20of%20General%20Objects&body=Title%3A%20RobustDexGrasp%3A%20Robust%20Dexterous%20Grasping%20of%20General%20Objects%0AAuthor%3A%20Hui%20Zhang%20and%20Zijian%20Wu%20and%20Linyi%20Huang%20and%20Sammy%20Christen%20and%20Jie%20Song%0AAbstract%3A%20%20%20The%20ability%20to%20robustly%20grasp%20a%20variety%20of%20objects%20is%20essential%20for%20dexterous%0Arobots.%20In%20this%20paper%2C%20we%20present%20a%20framework%20for%20zero-shot%20dynamic%20dexterous%0Agrasping%20using%20single-view%20visual%20inputs%2C%20designed%20to%20be%20resilient%20to%20various%0Adisturbances.%20Our%20approach%20utilizes%20a%20hand-centric%20object%20shape%20representation%0Abased%20on%20dynamic%20distance%20vectors%20between%20finger%20joints%20and%20object%20surfaces.%0AThis%20representation%20captures%20the%20local%20shape%20around%20potential%20contact%20regions%0Arather%20than%20focusing%20on%20detailed%20global%20object%20geometry%2C%20thereby%20enhancing%0Ageneralization%20to%20shape%20variations%20and%20uncertainties.%20To%20address%20perception%0Alimitations%2C%20we%20integrate%20a%20privileged%20teacher%20policy%20with%20a%20mixed%20curriculum%0Alearning%20approach%2C%20allowing%20the%20student%20policy%20to%20effectively%20distill%20grasping%0Acapabilities%20and%20explore%20for%20adaptation%20to%20disturbances.%20Trained%20in%20simulation%2C%0Aour%20method%20achieves%20success%20rates%20of%2097.0%25%20across%20247%2C786%20simulated%20objects%20and%0A94.6%25%20across%20512%20real%20objects%2C%20demonstrating%20remarkable%20generalization.%0AQuantitative%20and%20qualitative%20results%20validate%20the%20robustness%20of%20our%20policy%0Aagainst%20various%20disturbances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.05287v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobustDexGrasp%253A%2520Robust%2520Dexterous%2520Grasping%2520of%2520General%2520Objects%26entry.906535625%3DHui%2520Zhang%2520and%2520Zijian%2520Wu%2520and%2520Linyi%2520Huang%2520and%2520Sammy%2520Christen%2520and%2520Jie%2520Song%26entry.1292438233%3D%2520%2520The%2520ability%2520to%2520robustly%2520grasp%2520a%2520variety%2520of%2520objects%2520is%2520essential%2520for%2520dexterous%250Arobots.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520framework%2520for%2520zero-shot%2520dynamic%2520dexterous%250Agrasping%2520using%2520single-view%2520visual%2520inputs%252C%2520designed%2520to%2520be%2520resilient%2520to%2520various%250Adisturbances.%2520Our%2520approach%2520utilizes%2520a%2520hand-centric%2520object%2520shape%2520representation%250Abased%2520on%2520dynamic%2520distance%2520vectors%2520between%2520finger%2520joints%2520and%2520object%2520surfaces.%250AThis%2520representation%2520captures%2520the%2520local%2520shape%2520around%2520potential%2520contact%2520regions%250Arather%2520than%2520focusing%2520on%2520detailed%2520global%2520object%2520geometry%252C%2520thereby%2520enhancing%250Ageneralization%2520to%2520shape%2520variations%2520and%2520uncertainties.%2520To%2520address%2520perception%250Alimitations%252C%2520we%2520integrate%2520a%2520privileged%2520teacher%2520policy%2520with%2520a%2520mixed%2520curriculum%250Alearning%2520approach%252C%2520allowing%2520the%2520student%2520policy%2520to%2520effectively%2520distill%2520grasping%250Acapabilities%2520and%2520explore%2520for%2520adaptation%2520to%2520disturbances.%2520Trained%2520in%2520simulation%252C%250Aour%2520method%2520achieves%2520success%2520rates%2520of%252097.0%2525%2520across%2520247%252C786%2520simulated%2520objects%2520and%250A94.6%2525%2520across%2520512%2520real%2520objects%252C%2520demonstrating%2520remarkable%2520generalization.%250AQuantitative%2520and%2520qualitative%2520results%2520validate%2520the%2520robustness%2520of%2520our%2520policy%250Aagainst%2520various%2520disturbances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.05287v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RobustDexGrasp%3A%20Robust%20Dexterous%20Grasping%20of%20General%20Objects&entry.906535625=Hui%20Zhang%20and%20Zijian%20Wu%20and%20Linyi%20Huang%20and%20Sammy%20Christen%20and%20Jie%20Song&entry.1292438233=%20%20The%20ability%20to%20robustly%20grasp%20a%20variety%20of%20objects%20is%20essential%20for%20dexterous%0Arobots.%20In%20this%20paper%2C%20we%20present%20a%20framework%20for%20zero-shot%20dynamic%20dexterous%0Agrasping%20using%20single-view%20visual%20inputs%2C%20designed%20to%20be%20resilient%20to%20various%0Adisturbances.%20Our%20approach%20utilizes%20a%20hand-centric%20object%20shape%20representation%0Abased%20on%20dynamic%20distance%20vectors%20between%20finger%20joints%20and%20object%20surfaces.%0AThis%20representation%20captures%20the%20local%20shape%20around%20potential%20contact%20regions%0Arather%20than%20focusing%20on%20detailed%20global%20object%20geometry%2C%20thereby%20enhancing%0Ageneralization%20to%20shape%20variations%20and%20uncertainties.%20To%20address%20perception%0Alimitations%2C%20we%20integrate%20a%20privileged%20teacher%20policy%20with%20a%20mixed%20curriculum%0Alearning%20approach%2C%20allowing%20the%20student%20policy%20to%20effectively%20distill%20grasping%0Acapabilities%20and%20explore%20for%20adaptation%20to%20disturbances.%20Trained%20in%20simulation%2C%0Aour%20method%20achieves%20success%20rates%20of%2097.0%25%20across%20247%2C786%20simulated%20objects%20and%0A94.6%25%20across%20512%20real%20objects%2C%20demonstrating%20remarkable%20generalization.%0AQuantitative%20and%20qualitative%20results%20validate%20the%20robustness%20of%20our%20policy%0Aagainst%20various%20disturbances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.05287v2&entry.124074799=Read"},
{"title": "Hard-Constrained Neural Networks with Universal Approximation Guarantees", "author": "Youngjae Min and Navid Azizan", "abstract": "  Incorporating prior knowledge or specifications of input-output relationships\ninto machine learning models has gained significant attention, as it enhances\ngeneralization from limited data and leads to conforming outputs. However, most\nexisting approaches use soft constraints by penalizing violations through\nregularization, which offers no guarantee of constraint satisfaction--an\nessential requirement in safety-critical applications. On the other hand,\nimposing hard constraints on neural networks may hinder their representational\npower, adversely affecting performance. To address this, we propose HardNet, a\npractical framework for constructing neural networks that inherently satisfy\nhard constraints without sacrificing model capacity. Unlike approaches that\nmodify outputs only at inference time, HardNet enables end-to-end training with\nhard constraint guarantees, leading to improved performance. To the best of our\nknowledge, HardNet is the first method with an efficient forward pass to\nenforce more than one input-dependent inequality constraint. It allows\nunconstrained optimization of the network parameters using standard algorithms\nby appending a differentiable closed-form enforcement layer to the network's\noutput. Furthermore, we show that HardNet retains the universal approximation\ncapabilities of neural networks. We demonstrate the versatility and\neffectiveness of HardNet across various applications: learning with piecewise\nconstraints, learning optimization solvers, optimizing control policies in\nsafety-critical systems, and learning safe decision logic for aircraft systems.\n", "link": "http://arxiv.org/abs/2410.10807v2", "date": "2025-05-05", "relevancy": 1.9336, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5078}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4756}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hard-Constrained%20Neural%20Networks%20with%20Universal%20Approximation%20Guarantees&body=Title%3A%20Hard-Constrained%20Neural%20Networks%20with%20Universal%20Approximation%20Guarantees%0AAuthor%3A%20Youngjae%20Min%20and%20Navid%20Azizan%0AAbstract%3A%20%20%20Incorporating%20prior%20knowledge%20or%20specifications%20of%20input-output%20relationships%0Ainto%20machine%20learning%20models%20has%20gained%20significant%20attention%2C%20as%20it%20enhances%0Ageneralization%20from%20limited%20data%20and%20leads%20to%20conforming%20outputs.%20However%2C%20most%0Aexisting%20approaches%20use%20soft%20constraints%20by%20penalizing%20violations%20through%0Aregularization%2C%20which%20offers%20no%20guarantee%20of%20constraint%20satisfaction--an%0Aessential%20requirement%20in%20safety-critical%20applications.%20On%20the%20other%20hand%2C%0Aimposing%20hard%20constraints%20on%20neural%20networks%20may%20hinder%20their%20representational%0Apower%2C%20adversely%20affecting%20performance.%20To%20address%20this%2C%20we%20propose%20HardNet%2C%20a%0Apractical%20framework%20for%20constructing%20neural%20networks%20that%20inherently%20satisfy%0Ahard%20constraints%20without%20sacrificing%20model%20capacity.%20Unlike%20approaches%20that%0Amodify%20outputs%20only%20at%20inference%20time%2C%20HardNet%20enables%20end-to-end%20training%20with%0Ahard%20constraint%20guarantees%2C%20leading%20to%20improved%20performance.%20To%20the%20best%20of%20our%0Aknowledge%2C%20HardNet%20is%20the%20first%20method%20with%20an%20efficient%20forward%20pass%20to%0Aenforce%20more%20than%20one%20input-dependent%20inequality%20constraint.%20It%20allows%0Aunconstrained%20optimization%20of%20the%20network%20parameters%20using%20standard%20algorithms%0Aby%20appending%20a%20differentiable%20closed-form%20enforcement%20layer%20to%20the%20network%27s%0Aoutput.%20Furthermore%2C%20we%20show%20that%20HardNet%20retains%20the%20universal%20approximation%0Acapabilities%20of%20neural%20networks.%20We%20demonstrate%20the%20versatility%20and%0Aeffectiveness%20of%20HardNet%20across%20various%20applications%3A%20learning%20with%20piecewise%0Aconstraints%2C%20learning%20optimization%20solvers%2C%20optimizing%20control%20policies%20in%0Asafety-critical%20systems%2C%20and%20learning%20safe%20decision%20logic%20for%20aircraft%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10807v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHard-Constrained%2520Neural%2520Networks%2520with%2520Universal%2520Approximation%2520Guarantees%26entry.906535625%3DYoungjae%2520Min%2520and%2520Navid%2520Azizan%26entry.1292438233%3D%2520%2520Incorporating%2520prior%2520knowledge%2520or%2520specifications%2520of%2520input-output%2520relationships%250Ainto%2520machine%2520learning%2520models%2520has%2520gained%2520significant%2520attention%252C%2520as%2520it%2520enhances%250Ageneralization%2520from%2520limited%2520data%2520and%2520leads%2520to%2520conforming%2520outputs.%2520However%252C%2520most%250Aexisting%2520approaches%2520use%2520soft%2520constraints%2520by%2520penalizing%2520violations%2520through%250Aregularization%252C%2520which%2520offers%2520no%2520guarantee%2520of%2520constraint%2520satisfaction--an%250Aessential%2520requirement%2520in%2520safety-critical%2520applications.%2520On%2520the%2520other%2520hand%252C%250Aimposing%2520hard%2520constraints%2520on%2520neural%2520networks%2520may%2520hinder%2520their%2520representational%250Apower%252C%2520adversely%2520affecting%2520performance.%2520To%2520address%2520this%252C%2520we%2520propose%2520HardNet%252C%2520a%250Apractical%2520framework%2520for%2520constructing%2520neural%2520networks%2520that%2520inherently%2520satisfy%250Ahard%2520constraints%2520without%2520sacrificing%2520model%2520capacity.%2520Unlike%2520approaches%2520that%250Amodify%2520outputs%2520only%2520at%2520inference%2520time%252C%2520HardNet%2520enables%2520end-to-end%2520training%2520with%250Ahard%2520constraint%2520guarantees%252C%2520leading%2520to%2520improved%2520performance.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520HardNet%2520is%2520the%2520first%2520method%2520with%2520an%2520efficient%2520forward%2520pass%2520to%250Aenforce%2520more%2520than%2520one%2520input-dependent%2520inequality%2520constraint.%2520It%2520allows%250Aunconstrained%2520optimization%2520of%2520the%2520network%2520parameters%2520using%2520standard%2520algorithms%250Aby%2520appending%2520a%2520differentiable%2520closed-form%2520enforcement%2520layer%2520to%2520the%2520network%2527s%250Aoutput.%2520Furthermore%252C%2520we%2520show%2520that%2520HardNet%2520retains%2520the%2520universal%2520approximation%250Acapabilities%2520of%2520neural%2520networks.%2520We%2520demonstrate%2520the%2520versatility%2520and%250Aeffectiveness%2520of%2520HardNet%2520across%2520various%2520applications%253A%2520learning%2520with%2520piecewise%250Aconstraints%252C%2520learning%2520optimization%2520solvers%252C%2520optimizing%2520control%2520policies%2520in%250Asafety-critical%2520systems%252C%2520and%2520learning%2520safe%2520decision%2520logic%2520for%2520aircraft%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10807v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hard-Constrained%20Neural%20Networks%20with%20Universal%20Approximation%20Guarantees&entry.906535625=Youngjae%20Min%20and%20Navid%20Azizan&entry.1292438233=%20%20Incorporating%20prior%20knowledge%20or%20specifications%20of%20input-output%20relationships%0Ainto%20machine%20learning%20models%20has%20gained%20significant%20attention%2C%20as%20it%20enhances%0Ageneralization%20from%20limited%20data%20and%20leads%20to%20conforming%20outputs.%20However%2C%20most%0Aexisting%20approaches%20use%20soft%20constraints%20by%20penalizing%20violations%20through%0Aregularization%2C%20which%20offers%20no%20guarantee%20of%20constraint%20satisfaction--an%0Aessential%20requirement%20in%20safety-critical%20applications.%20On%20the%20other%20hand%2C%0Aimposing%20hard%20constraints%20on%20neural%20networks%20may%20hinder%20their%20representational%0Apower%2C%20adversely%20affecting%20performance.%20To%20address%20this%2C%20we%20propose%20HardNet%2C%20a%0Apractical%20framework%20for%20constructing%20neural%20networks%20that%20inherently%20satisfy%0Ahard%20constraints%20without%20sacrificing%20model%20capacity.%20Unlike%20approaches%20that%0Amodify%20outputs%20only%20at%20inference%20time%2C%20HardNet%20enables%20end-to-end%20training%20with%0Ahard%20constraint%20guarantees%2C%20leading%20to%20improved%20performance.%20To%20the%20best%20of%20our%0Aknowledge%2C%20HardNet%20is%20the%20first%20method%20with%20an%20efficient%20forward%20pass%20to%0Aenforce%20more%20than%20one%20input-dependent%20inequality%20constraint.%20It%20allows%0Aunconstrained%20optimization%20of%20the%20network%20parameters%20using%20standard%20algorithms%0Aby%20appending%20a%20differentiable%20closed-form%20enforcement%20layer%20to%20the%20network%27s%0Aoutput.%20Furthermore%2C%20we%20show%20that%20HardNet%20retains%20the%20universal%20approximation%0Acapabilities%20of%20neural%20networks.%20We%20demonstrate%20the%20versatility%20and%0Aeffectiveness%20of%20HardNet%20across%20various%20applications%3A%20learning%20with%20piecewise%0Aconstraints%2C%20learning%20optimization%20solvers%2C%20optimizing%20control%20policies%20in%0Asafety-critical%20systems%2C%20and%20learning%20safe%20decision%20logic%20for%20aircraft%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10807v2&entry.124074799=Read"},
{"title": "Lazy But Effective: Collaborative Personalized Federated Learning with\n  Heterogeneous Data", "author": "Ljubomir Rokvic and Panayiotis Danassis and Boi Faltings", "abstract": "  In Federated Learning, heterogeneity in client data distributions often means\nthat a single global model does not have the best performance for individual\nclients. Consider for example training a next-word prediction model for\nkeyboards: user-specific language patterns due to demographics (dialect, age,\netc.), language proficiency, and writing style result in a highly non-IID\ndataset across clients. Other examples are medical images taken with different\nmachines, or driving data from different vehicle types. To address this, we\npropose a simple yet effective personalized federated learning framework\n(pFedLIA) that utilizes a computationally efficient influence approximation,\ncalled `Lazy Influence', to cluster clients in a distributed manner before\nmodel aggregation. Within each cluster, data owners collaborate to jointly\ntrain a model that captures the specific data patterns of the clients. Our\nmethod has been shown to successfully recover the global model's performance\ndrop due to the non-IID-ness in various synthetic and real-world settings,\nspecifically a next-word prediction task on the Nordic languages as well as\nseveral benchmark tasks. It matches the performance of a hypothetical Oracle\nclustering, and significantly improves on existing baselines, e.g., an\nimprovement of 17% on CIFAR100.\n", "link": "http://arxiv.org/abs/2505.02540v1", "date": "2025-05-05", "relevancy": 1.9304, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4834}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4824}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4811}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lazy%20But%20Effective%3A%20Collaborative%20Personalized%20Federated%20Learning%20with%0A%20%20Heterogeneous%20Data&body=Title%3A%20Lazy%20But%20Effective%3A%20Collaborative%20Personalized%20Federated%20Learning%20with%0A%20%20Heterogeneous%20Data%0AAuthor%3A%20Ljubomir%20Rokvic%20and%20Panayiotis%20Danassis%20and%20Boi%20Faltings%0AAbstract%3A%20%20%20In%20Federated%20Learning%2C%20heterogeneity%20in%20client%20data%20distributions%20often%20means%0Athat%20a%20single%20global%20model%20does%20not%20have%20the%20best%20performance%20for%20individual%0Aclients.%20Consider%20for%20example%20training%20a%20next-word%20prediction%20model%20for%0Akeyboards%3A%20user-specific%20language%20patterns%20due%20to%20demographics%20%28dialect%2C%20age%2C%0Aetc.%29%2C%20language%20proficiency%2C%20and%20writing%20style%20result%20in%20a%20highly%20non-IID%0Adataset%20across%20clients.%20Other%20examples%20are%20medical%20images%20taken%20with%20different%0Amachines%2C%20or%20driving%20data%20from%20different%20vehicle%20types.%20To%20address%20this%2C%20we%0Apropose%20a%20simple%20yet%20effective%20personalized%20federated%20learning%20framework%0A%28pFedLIA%29%20that%20utilizes%20a%20computationally%20efficient%20influence%20approximation%2C%0Acalled%20%60Lazy%20Influence%27%2C%20to%20cluster%20clients%20in%20a%20distributed%20manner%20before%0Amodel%20aggregation.%20Within%20each%20cluster%2C%20data%20owners%20collaborate%20to%20jointly%0Atrain%20a%20model%20that%20captures%20the%20specific%20data%20patterns%20of%20the%20clients.%20Our%0Amethod%20has%20been%20shown%20to%20successfully%20recover%20the%20global%20model%27s%20performance%0Adrop%20due%20to%20the%20non-IID-ness%20in%20various%20synthetic%20and%20real-world%20settings%2C%0Aspecifically%20a%20next-word%20prediction%20task%20on%20the%20Nordic%20languages%20as%20well%20as%0Aseveral%20benchmark%20tasks.%20It%20matches%20the%20performance%20of%20a%20hypothetical%20Oracle%0Aclustering%2C%20and%20significantly%20improves%20on%20existing%20baselines%2C%20e.g.%2C%20an%0Aimprovement%20of%2017%25%20on%20CIFAR100.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02540v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLazy%2520But%2520Effective%253A%2520Collaborative%2520Personalized%2520Federated%2520Learning%2520with%250A%2520%2520Heterogeneous%2520Data%26entry.906535625%3DLjubomir%2520Rokvic%2520and%2520Panayiotis%2520Danassis%2520and%2520Boi%2520Faltings%26entry.1292438233%3D%2520%2520In%2520Federated%2520Learning%252C%2520heterogeneity%2520in%2520client%2520data%2520distributions%2520often%2520means%250Athat%2520a%2520single%2520global%2520model%2520does%2520not%2520have%2520the%2520best%2520performance%2520for%2520individual%250Aclients.%2520Consider%2520for%2520example%2520training%2520a%2520next-word%2520prediction%2520model%2520for%250Akeyboards%253A%2520user-specific%2520language%2520patterns%2520due%2520to%2520demographics%2520%2528dialect%252C%2520age%252C%250Aetc.%2529%252C%2520language%2520proficiency%252C%2520and%2520writing%2520style%2520result%2520in%2520a%2520highly%2520non-IID%250Adataset%2520across%2520clients.%2520Other%2520examples%2520are%2520medical%2520images%2520taken%2520with%2520different%250Amachines%252C%2520or%2520driving%2520data%2520from%2520different%2520vehicle%2520types.%2520To%2520address%2520this%252C%2520we%250Apropose%2520a%2520simple%2520yet%2520effective%2520personalized%2520federated%2520learning%2520framework%250A%2528pFedLIA%2529%2520that%2520utilizes%2520a%2520computationally%2520efficient%2520influence%2520approximation%252C%250Acalled%2520%2560Lazy%2520Influence%2527%252C%2520to%2520cluster%2520clients%2520in%2520a%2520distributed%2520manner%2520before%250Amodel%2520aggregation.%2520Within%2520each%2520cluster%252C%2520data%2520owners%2520collaborate%2520to%2520jointly%250Atrain%2520a%2520model%2520that%2520captures%2520the%2520specific%2520data%2520patterns%2520of%2520the%2520clients.%2520Our%250Amethod%2520has%2520been%2520shown%2520to%2520successfully%2520recover%2520the%2520global%2520model%2527s%2520performance%250Adrop%2520due%2520to%2520the%2520non-IID-ness%2520in%2520various%2520synthetic%2520and%2520real-world%2520settings%252C%250Aspecifically%2520a%2520next-word%2520prediction%2520task%2520on%2520the%2520Nordic%2520languages%2520as%2520well%2520as%250Aseveral%2520benchmark%2520tasks.%2520It%2520matches%2520the%2520performance%2520of%2520a%2520hypothetical%2520Oracle%250Aclustering%252C%2520and%2520significantly%2520improves%2520on%2520existing%2520baselines%252C%2520e.g.%252C%2520an%250Aimprovement%2520of%252017%2525%2520on%2520CIFAR100.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02540v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lazy%20But%20Effective%3A%20Collaborative%20Personalized%20Federated%20Learning%20with%0A%20%20Heterogeneous%20Data&entry.906535625=Ljubomir%20Rokvic%20and%20Panayiotis%20Danassis%20and%20Boi%20Faltings&entry.1292438233=%20%20In%20Federated%20Learning%2C%20heterogeneity%20in%20client%20data%20distributions%20often%20means%0Athat%20a%20single%20global%20model%20does%20not%20have%20the%20best%20performance%20for%20individual%0Aclients.%20Consider%20for%20example%20training%20a%20next-word%20prediction%20model%20for%0Akeyboards%3A%20user-specific%20language%20patterns%20due%20to%20demographics%20%28dialect%2C%20age%2C%0Aetc.%29%2C%20language%20proficiency%2C%20and%20writing%20style%20result%20in%20a%20highly%20non-IID%0Adataset%20across%20clients.%20Other%20examples%20are%20medical%20images%20taken%20with%20different%0Amachines%2C%20or%20driving%20data%20from%20different%20vehicle%20types.%20To%20address%20this%2C%20we%0Apropose%20a%20simple%20yet%20effective%20personalized%20federated%20learning%20framework%0A%28pFedLIA%29%20that%20utilizes%20a%20computationally%20efficient%20influence%20approximation%2C%0Acalled%20%60Lazy%20Influence%27%2C%20to%20cluster%20clients%20in%20a%20distributed%20manner%20before%0Amodel%20aggregation.%20Within%20each%20cluster%2C%20data%20owners%20collaborate%20to%20jointly%0Atrain%20a%20model%20that%20captures%20the%20specific%20data%20patterns%20of%20the%20clients.%20Our%0Amethod%20has%20been%20shown%20to%20successfully%20recover%20the%20global%20model%27s%20performance%0Adrop%20due%20to%20the%20non-IID-ness%20in%20various%20synthetic%20and%20real-world%20settings%2C%0Aspecifically%20a%20next-word%20prediction%20task%20on%20the%20Nordic%20languages%20as%20well%20as%0Aseveral%20benchmark%20tasks.%20It%20matches%20the%20performance%20of%20a%20hypothetical%20Oracle%0Aclustering%2C%20and%20significantly%20improves%20on%20existing%20baselines%2C%20e.g.%2C%20an%0Aimprovement%20of%2017%25%20on%20CIFAR100.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02540v1&entry.124074799=Read"},
{"title": "Mirror Mean-Field Langevin Dynamics", "author": "Anming Gu and Juno Kim", "abstract": "  The mean-field Langevin dynamics (MFLD) minimizes an entropy-regularized\nnonlinear convex functional on the Wasserstein space over $\\mathbb{R}^d$, and\nhas gained attention recently as a model for the gradient descent dynamics of\ninteracting particle systems such as infinite-width two-layer neural networks.\nHowever, many problems of interest have constrained domains, which are not\nsolved by existing mean-field algorithms due to the global diffusion term. We\nstudy the optimization of probability measures constrained to a convex subset\nof $\\mathbb{R}^d$ by proposing the \\emph{mirror mean-field Langevin dynamics}\n(MMFLD), an extension of MFLD to the mirror Langevin framework. We obtain\nlinear convergence guarantees for the continuous MMFLD via a uniform\nlog-Sobolev inequality, and uniform-in-time propagation of chaos results for\nits time- and particle-discretized counterpart.\n", "link": "http://arxiv.org/abs/2505.02621v1", "date": "2025-05-05", "relevancy": 1.9283, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4942}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4807}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mirror%20Mean-Field%20Langevin%20Dynamics&body=Title%3A%20Mirror%20Mean-Field%20Langevin%20Dynamics%0AAuthor%3A%20Anming%20Gu%20and%20Juno%20Kim%0AAbstract%3A%20%20%20The%20mean-field%20Langevin%20dynamics%20%28MFLD%29%20minimizes%20an%20entropy-regularized%0Anonlinear%20convex%20functional%20on%20the%20Wasserstein%20space%20over%20%24%5Cmathbb%7BR%7D%5Ed%24%2C%20and%0Ahas%20gained%20attention%20recently%20as%20a%20model%20for%20the%20gradient%20descent%20dynamics%20of%0Ainteracting%20particle%20systems%20such%20as%20infinite-width%20two-layer%20neural%20networks.%0AHowever%2C%20many%20problems%20of%20interest%20have%20constrained%20domains%2C%20which%20are%20not%0Asolved%20by%20existing%20mean-field%20algorithms%20due%20to%20the%20global%20diffusion%20term.%20We%0Astudy%20the%20optimization%20of%20probability%20measures%20constrained%20to%20a%20convex%20subset%0Aof%20%24%5Cmathbb%7BR%7D%5Ed%24%20by%20proposing%20the%20%5Cemph%7Bmirror%20mean-field%20Langevin%20dynamics%7D%0A%28MMFLD%29%2C%20an%20extension%20of%20MFLD%20to%20the%20mirror%20Langevin%20framework.%20We%20obtain%0Alinear%20convergence%20guarantees%20for%20the%20continuous%20MMFLD%20via%20a%20uniform%0Alog-Sobolev%20inequality%2C%20and%20uniform-in-time%20propagation%20of%20chaos%20results%20for%0Aits%20time-%20and%20particle-discretized%20counterpart.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02621v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMirror%2520Mean-Field%2520Langevin%2520Dynamics%26entry.906535625%3DAnming%2520Gu%2520and%2520Juno%2520Kim%26entry.1292438233%3D%2520%2520The%2520mean-field%2520Langevin%2520dynamics%2520%2528MFLD%2529%2520minimizes%2520an%2520entropy-regularized%250Anonlinear%2520convex%2520functional%2520on%2520the%2520Wasserstein%2520space%2520over%2520%2524%255Cmathbb%257BR%257D%255Ed%2524%252C%2520and%250Ahas%2520gained%2520attention%2520recently%2520as%2520a%2520model%2520for%2520the%2520gradient%2520descent%2520dynamics%2520of%250Ainteracting%2520particle%2520systems%2520such%2520as%2520infinite-width%2520two-layer%2520neural%2520networks.%250AHowever%252C%2520many%2520problems%2520of%2520interest%2520have%2520constrained%2520domains%252C%2520which%2520are%2520not%250Asolved%2520by%2520existing%2520mean-field%2520algorithms%2520due%2520to%2520the%2520global%2520diffusion%2520term.%2520We%250Astudy%2520the%2520optimization%2520of%2520probability%2520measures%2520constrained%2520to%2520a%2520convex%2520subset%250Aof%2520%2524%255Cmathbb%257BR%257D%255Ed%2524%2520by%2520proposing%2520the%2520%255Cemph%257Bmirror%2520mean-field%2520Langevin%2520dynamics%257D%250A%2528MMFLD%2529%252C%2520an%2520extension%2520of%2520MFLD%2520to%2520the%2520mirror%2520Langevin%2520framework.%2520We%2520obtain%250Alinear%2520convergence%2520guarantees%2520for%2520the%2520continuous%2520MMFLD%2520via%2520a%2520uniform%250Alog-Sobolev%2520inequality%252C%2520and%2520uniform-in-time%2520propagation%2520of%2520chaos%2520results%2520for%250Aits%2520time-%2520and%2520particle-discretized%2520counterpart.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02621v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mirror%20Mean-Field%20Langevin%20Dynamics&entry.906535625=Anming%20Gu%20and%20Juno%20Kim&entry.1292438233=%20%20The%20mean-field%20Langevin%20dynamics%20%28MFLD%29%20minimizes%20an%20entropy-regularized%0Anonlinear%20convex%20functional%20on%20the%20Wasserstein%20space%20over%20%24%5Cmathbb%7BR%7D%5Ed%24%2C%20and%0Ahas%20gained%20attention%20recently%20as%20a%20model%20for%20the%20gradient%20descent%20dynamics%20of%0Ainteracting%20particle%20systems%20such%20as%20infinite-width%20two-layer%20neural%20networks.%0AHowever%2C%20many%20problems%20of%20interest%20have%20constrained%20domains%2C%20which%20are%20not%0Asolved%20by%20existing%20mean-field%20algorithms%20due%20to%20the%20global%20diffusion%20term.%20We%0Astudy%20the%20optimization%20of%20probability%20measures%20constrained%20to%20a%20convex%20subset%0Aof%20%24%5Cmathbb%7BR%7D%5Ed%24%20by%20proposing%20the%20%5Cemph%7Bmirror%20mean-field%20Langevin%20dynamics%7D%0A%28MMFLD%29%2C%20an%20extension%20of%20MFLD%20to%20the%20mirror%20Langevin%20framework.%20We%20obtain%0Alinear%20convergence%20guarantees%20for%20the%20continuous%20MMFLD%20via%20a%20uniform%0Alog-Sobolev%20inequality%2C%20and%20uniform-in-time%20propagation%20of%20chaos%20results%20for%0Aits%20time-%20and%20particle-discretized%20counterpart.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02621v1&entry.124074799=Read"},
{"title": "Platelet enumeration in dense aggregates", "author": "H. Martin Gillis and Yogeshwar Shendye and Paul Hollensen and Alan Fine and Thomas Trappenberg", "abstract": "  Identifying and counting blood components such as red blood cells, various\ntypes of white blood cells, and platelets is a critical task for healthcare\npractitioners. Deep learning approaches, particularly convolutional neural\nnetworks (CNNs) using supervised learning strategies, have shown considerable\nsuccess for such tasks. However, CNN based architectures such as U-Net, often\nstruggles to accurately identify platelets due to their sizes and high\nvariability of features. To address these challenges, researchers have commonly\nemployed strategies such as class weighted loss functions, which have\ndemonstrated some success. However, this does not address the more significant\nchallenge of platelet variability in size and tendency to form aggregates and\nassociations with other blood components. In this study, we explored an\nalternative approach by investigating the role of convolutional kernels in\nmitigating these issues. We also assigned separate classes to singular\nplatelets and platelet aggregates and performed semantic segmentation using\nvarious U-Net architectures for identifying platelets. We then evaluated and\ncompared two common methods (pixel area method and connected component\nanalysis) for counting platelets and proposed an alternative approach\nspecialized for single platelets and platelet aggregates. Our experiments\nprovided results that showed significant improvements in the identification of\nplatelets, highlighting the importance of optimizing convolutional operations\nand class designations. We show that the common practice of pixel area-based\ncounting often over estimate platelet counts, whereas the proposed method\npresented in this work offers significant improvements. We discuss in detail\nabout these methods from segmentation masks.\n", "link": "http://arxiv.org/abs/2505.02751v1", "date": "2025-05-05", "relevancy": 1.9231, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5089}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4645}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Platelet%20enumeration%20in%20dense%20aggregates&body=Title%3A%20Platelet%20enumeration%20in%20dense%20aggregates%0AAuthor%3A%20H.%20Martin%20Gillis%20and%20Yogeshwar%20Shendye%20and%20Paul%20Hollensen%20and%20Alan%20Fine%20and%20Thomas%20Trappenberg%0AAbstract%3A%20%20%20Identifying%20and%20counting%20blood%20components%20such%20as%20red%20blood%20cells%2C%20various%0Atypes%20of%20white%20blood%20cells%2C%20and%20platelets%20is%20a%20critical%20task%20for%20healthcare%0Apractitioners.%20Deep%20learning%20approaches%2C%20particularly%20convolutional%20neural%0Anetworks%20%28CNNs%29%20using%20supervised%20learning%20strategies%2C%20have%20shown%20considerable%0Asuccess%20for%20such%20tasks.%20However%2C%20CNN%20based%20architectures%20such%20as%20U-Net%2C%20often%0Astruggles%20to%20accurately%20identify%20platelets%20due%20to%20their%20sizes%20and%20high%0Avariability%20of%20features.%20To%20address%20these%20challenges%2C%20researchers%20have%20commonly%0Aemployed%20strategies%20such%20as%20class%20weighted%20loss%20functions%2C%20which%20have%0Ademonstrated%20some%20success.%20However%2C%20this%20does%20not%20address%20the%20more%20significant%0Achallenge%20of%20platelet%20variability%20in%20size%20and%20tendency%20to%20form%20aggregates%20and%0Aassociations%20with%20other%20blood%20components.%20In%20this%20study%2C%20we%20explored%20an%0Aalternative%20approach%20by%20investigating%20the%20role%20of%20convolutional%20kernels%20in%0Amitigating%20these%20issues.%20We%20also%20assigned%20separate%20classes%20to%20singular%0Aplatelets%20and%20platelet%20aggregates%20and%20performed%20semantic%20segmentation%20using%0Avarious%20U-Net%20architectures%20for%20identifying%20platelets.%20We%20then%20evaluated%20and%0Acompared%20two%20common%20methods%20%28pixel%20area%20method%20and%20connected%20component%0Aanalysis%29%20for%20counting%20platelets%20and%20proposed%20an%20alternative%20approach%0Aspecialized%20for%20single%20platelets%20and%20platelet%20aggregates.%20Our%20experiments%0Aprovided%20results%20that%20showed%20significant%20improvements%20in%20the%20identification%20of%0Aplatelets%2C%20highlighting%20the%20importance%20of%20optimizing%20convolutional%20operations%0Aand%20class%20designations.%20We%20show%20that%20the%20common%20practice%20of%20pixel%20area-based%0Acounting%20often%20over%20estimate%20platelet%20counts%2C%20whereas%20the%20proposed%20method%0Apresented%20in%20this%20work%20offers%20significant%20improvements.%20We%20discuss%20in%20detail%0Aabout%20these%20methods%20from%20segmentation%20masks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02751v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPlatelet%2520enumeration%2520in%2520dense%2520aggregates%26entry.906535625%3DH.%2520Martin%2520Gillis%2520and%2520Yogeshwar%2520Shendye%2520and%2520Paul%2520Hollensen%2520and%2520Alan%2520Fine%2520and%2520Thomas%2520Trappenberg%26entry.1292438233%3D%2520%2520Identifying%2520and%2520counting%2520blood%2520components%2520such%2520as%2520red%2520blood%2520cells%252C%2520various%250Atypes%2520of%2520white%2520blood%2520cells%252C%2520and%2520platelets%2520is%2520a%2520critical%2520task%2520for%2520healthcare%250Apractitioners.%2520Deep%2520learning%2520approaches%252C%2520particularly%2520convolutional%2520neural%250Anetworks%2520%2528CNNs%2529%2520using%2520supervised%2520learning%2520strategies%252C%2520have%2520shown%2520considerable%250Asuccess%2520for%2520such%2520tasks.%2520However%252C%2520CNN%2520based%2520architectures%2520such%2520as%2520U-Net%252C%2520often%250Astruggles%2520to%2520accurately%2520identify%2520platelets%2520due%2520to%2520their%2520sizes%2520and%2520high%250Avariability%2520of%2520features.%2520To%2520address%2520these%2520challenges%252C%2520researchers%2520have%2520commonly%250Aemployed%2520strategies%2520such%2520as%2520class%2520weighted%2520loss%2520functions%252C%2520which%2520have%250Ademonstrated%2520some%2520success.%2520However%252C%2520this%2520does%2520not%2520address%2520the%2520more%2520significant%250Achallenge%2520of%2520platelet%2520variability%2520in%2520size%2520and%2520tendency%2520to%2520form%2520aggregates%2520and%250Aassociations%2520with%2520other%2520blood%2520components.%2520In%2520this%2520study%252C%2520we%2520explored%2520an%250Aalternative%2520approach%2520by%2520investigating%2520the%2520role%2520of%2520convolutional%2520kernels%2520in%250Amitigating%2520these%2520issues.%2520We%2520also%2520assigned%2520separate%2520classes%2520to%2520singular%250Aplatelets%2520and%2520platelet%2520aggregates%2520and%2520performed%2520semantic%2520segmentation%2520using%250Avarious%2520U-Net%2520architectures%2520for%2520identifying%2520platelets.%2520We%2520then%2520evaluated%2520and%250Acompared%2520two%2520common%2520methods%2520%2528pixel%2520area%2520method%2520and%2520connected%2520component%250Aanalysis%2529%2520for%2520counting%2520platelets%2520and%2520proposed%2520an%2520alternative%2520approach%250Aspecialized%2520for%2520single%2520platelets%2520and%2520platelet%2520aggregates.%2520Our%2520experiments%250Aprovided%2520results%2520that%2520showed%2520significant%2520improvements%2520in%2520the%2520identification%2520of%250Aplatelets%252C%2520highlighting%2520the%2520importance%2520of%2520optimizing%2520convolutional%2520operations%250Aand%2520class%2520designations.%2520We%2520show%2520that%2520the%2520common%2520practice%2520of%2520pixel%2520area-based%250Acounting%2520often%2520over%2520estimate%2520platelet%2520counts%252C%2520whereas%2520the%2520proposed%2520method%250Apresented%2520in%2520this%2520work%2520offers%2520significant%2520improvements.%2520We%2520discuss%2520in%2520detail%250Aabout%2520these%2520methods%2520from%2520segmentation%2520masks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02751v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Platelet%20enumeration%20in%20dense%20aggregates&entry.906535625=H.%20Martin%20Gillis%20and%20Yogeshwar%20Shendye%20and%20Paul%20Hollensen%20and%20Alan%20Fine%20and%20Thomas%20Trappenberg&entry.1292438233=%20%20Identifying%20and%20counting%20blood%20components%20such%20as%20red%20blood%20cells%2C%20various%0Atypes%20of%20white%20blood%20cells%2C%20and%20platelets%20is%20a%20critical%20task%20for%20healthcare%0Apractitioners.%20Deep%20learning%20approaches%2C%20particularly%20convolutional%20neural%0Anetworks%20%28CNNs%29%20using%20supervised%20learning%20strategies%2C%20have%20shown%20considerable%0Asuccess%20for%20such%20tasks.%20However%2C%20CNN%20based%20architectures%20such%20as%20U-Net%2C%20often%0Astruggles%20to%20accurately%20identify%20platelets%20due%20to%20their%20sizes%20and%20high%0Avariability%20of%20features.%20To%20address%20these%20challenges%2C%20researchers%20have%20commonly%0Aemployed%20strategies%20such%20as%20class%20weighted%20loss%20functions%2C%20which%20have%0Ademonstrated%20some%20success.%20However%2C%20this%20does%20not%20address%20the%20more%20significant%0Achallenge%20of%20platelet%20variability%20in%20size%20and%20tendency%20to%20form%20aggregates%20and%0Aassociations%20with%20other%20blood%20components.%20In%20this%20study%2C%20we%20explored%20an%0Aalternative%20approach%20by%20investigating%20the%20role%20of%20convolutional%20kernels%20in%0Amitigating%20these%20issues.%20We%20also%20assigned%20separate%20classes%20to%20singular%0Aplatelets%20and%20platelet%20aggregates%20and%20performed%20semantic%20segmentation%20using%0Avarious%20U-Net%20architectures%20for%20identifying%20platelets.%20We%20then%20evaluated%20and%0Acompared%20two%20common%20methods%20%28pixel%20area%20method%20and%20connected%20component%0Aanalysis%29%20for%20counting%20platelets%20and%20proposed%20an%20alternative%20approach%0Aspecialized%20for%20single%20platelets%20and%20platelet%20aggregates.%20Our%20experiments%0Aprovided%20results%20that%20showed%20significant%20improvements%20in%20the%20identification%20of%0Aplatelets%2C%20highlighting%20the%20importance%20of%20optimizing%20convolutional%20operations%0Aand%20class%20designations.%20We%20show%20that%20the%20common%20practice%20of%20pixel%20area-based%0Acounting%20often%20over%20estimate%20platelet%20counts%2C%20whereas%20the%20proposed%20method%0Apresented%20in%20this%20work%20offers%20significant%20improvements.%20We%20discuss%20in%20detail%0Aabout%20these%20methods%20from%20segmentation%20masks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02751v1&entry.124074799=Read"},
{"title": "Technical Report: Evaluating Goal Drift in Language Model Agents", "author": "Rauno Arike and Elizabeth Donoway and Henning Bartsch and Marius Hobbhahn", "abstract": "  As language models (LMs) are increasingly deployed as autonomous agents,\ntheir robust adherence to human-assigned objectives becomes crucial for safe\noperation. When these agents operate independently for extended periods without\nhuman oversight, even initially well-specified goals may gradually shift.\nDetecting and measuring goal drift - an agent's tendency to deviate from its\noriginal objective over time - presents significant challenges, as goals can\nshift gradually, causing only subtle behavioral changes. This paper proposes a\nnovel approach to analyzing goal drift in LM agents. In our experiments, agents\nare first explicitly given a goal through their system prompt, then exposed to\ncompeting objectives through environmental pressures. We demonstrate that while\nthe best-performing agent (a scaffolded version of Claude 3.5 Sonnet) maintains\nnearly perfect goal adherence for more than 100,000 tokens in our most\ndifficult evaluation setting, all evaluated models exhibit some degree of goal\ndrift. We also find that goal drift correlates with models' increasing\nsusceptibility to pattern-matching behaviors as the context length grows.\n", "link": "http://arxiv.org/abs/2505.02709v1", "date": "2025-05-05", "relevancy": 1.9089, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4874}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4734}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4685}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Technical%20Report%3A%20Evaluating%20Goal%20Drift%20in%20Language%20Model%20Agents&body=Title%3A%20Technical%20Report%3A%20Evaluating%20Goal%20Drift%20in%20Language%20Model%20Agents%0AAuthor%3A%20Rauno%20Arike%20and%20Elizabeth%20Donoway%20and%20Henning%20Bartsch%20and%20Marius%20Hobbhahn%0AAbstract%3A%20%20%20As%20language%20models%20%28LMs%29%20are%20increasingly%20deployed%20as%20autonomous%20agents%2C%0Atheir%20robust%20adherence%20to%20human-assigned%20objectives%20becomes%20crucial%20for%20safe%0Aoperation.%20When%20these%20agents%20operate%20independently%20for%20extended%20periods%20without%0Ahuman%20oversight%2C%20even%20initially%20well-specified%20goals%20may%20gradually%20shift.%0ADetecting%20and%20measuring%20goal%20drift%20-%20an%20agent%27s%20tendency%20to%20deviate%20from%20its%0Aoriginal%20objective%20over%20time%20-%20presents%20significant%20challenges%2C%20as%20goals%20can%0Ashift%20gradually%2C%20causing%20only%20subtle%20behavioral%20changes.%20This%20paper%20proposes%20a%0Anovel%20approach%20to%20analyzing%20goal%20drift%20in%20LM%20agents.%20In%20our%20experiments%2C%20agents%0Aare%20first%20explicitly%20given%20a%20goal%20through%20their%20system%20prompt%2C%20then%20exposed%20to%0Acompeting%20objectives%20through%20environmental%20pressures.%20We%20demonstrate%20that%20while%0Athe%20best-performing%20agent%20%28a%20scaffolded%20version%20of%20Claude%203.5%20Sonnet%29%20maintains%0Anearly%20perfect%20goal%20adherence%20for%20more%20than%20100%2C000%20tokens%20in%20our%20most%0Adifficult%20evaluation%20setting%2C%20all%20evaluated%20models%20exhibit%20some%20degree%20of%20goal%0Adrift.%20We%20also%20find%20that%20goal%20drift%20correlates%20with%20models%27%20increasing%0Asusceptibility%20to%20pattern-matching%20behaviors%20as%20the%20context%20length%20grows.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02709v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTechnical%2520Report%253A%2520Evaluating%2520Goal%2520Drift%2520in%2520Language%2520Model%2520Agents%26entry.906535625%3DRauno%2520Arike%2520and%2520Elizabeth%2520Donoway%2520and%2520Henning%2520Bartsch%2520and%2520Marius%2520Hobbhahn%26entry.1292438233%3D%2520%2520As%2520language%2520models%2520%2528LMs%2529%2520are%2520increasingly%2520deployed%2520as%2520autonomous%2520agents%252C%250Atheir%2520robust%2520adherence%2520to%2520human-assigned%2520objectives%2520becomes%2520crucial%2520for%2520safe%250Aoperation.%2520When%2520these%2520agents%2520operate%2520independently%2520for%2520extended%2520periods%2520without%250Ahuman%2520oversight%252C%2520even%2520initially%2520well-specified%2520goals%2520may%2520gradually%2520shift.%250ADetecting%2520and%2520measuring%2520goal%2520drift%2520-%2520an%2520agent%2527s%2520tendency%2520to%2520deviate%2520from%2520its%250Aoriginal%2520objective%2520over%2520time%2520-%2520presents%2520significant%2520challenges%252C%2520as%2520goals%2520can%250Ashift%2520gradually%252C%2520causing%2520only%2520subtle%2520behavioral%2520changes.%2520This%2520paper%2520proposes%2520a%250Anovel%2520approach%2520to%2520analyzing%2520goal%2520drift%2520in%2520LM%2520agents.%2520In%2520our%2520experiments%252C%2520agents%250Aare%2520first%2520explicitly%2520given%2520a%2520goal%2520through%2520their%2520system%2520prompt%252C%2520then%2520exposed%2520to%250Acompeting%2520objectives%2520through%2520environmental%2520pressures.%2520We%2520demonstrate%2520that%2520while%250Athe%2520best-performing%2520agent%2520%2528a%2520scaffolded%2520version%2520of%2520Claude%25203.5%2520Sonnet%2529%2520maintains%250Anearly%2520perfect%2520goal%2520adherence%2520for%2520more%2520than%2520100%252C000%2520tokens%2520in%2520our%2520most%250Adifficult%2520evaluation%2520setting%252C%2520all%2520evaluated%2520models%2520exhibit%2520some%2520degree%2520of%2520goal%250Adrift.%2520We%2520also%2520find%2520that%2520goal%2520drift%2520correlates%2520with%2520models%2527%2520increasing%250Asusceptibility%2520to%2520pattern-matching%2520behaviors%2520as%2520the%2520context%2520length%2520grows.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02709v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Technical%20Report%3A%20Evaluating%20Goal%20Drift%20in%20Language%20Model%20Agents&entry.906535625=Rauno%20Arike%20and%20Elizabeth%20Donoway%20and%20Henning%20Bartsch%20and%20Marius%20Hobbhahn&entry.1292438233=%20%20As%20language%20models%20%28LMs%29%20are%20increasingly%20deployed%20as%20autonomous%20agents%2C%0Atheir%20robust%20adherence%20to%20human-assigned%20objectives%20becomes%20crucial%20for%20safe%0Aoperation.%20When%20these%20agents%20operate%20independently%20for%20extended%20periods%20without%0Ahuman%20oversight%2C%20even%20initially%20well-specified%20goals%20may%20gradually%20shift.%0ADetecting%20and%20measuring%20goal%20drift%20-%20an%20agent%27s%20tendency%20to%20deviate%20from%20its%0Aoriginal%20objective%20over%20time%20-%20presents%20significant%20challenges%2C%20as%20goals%20can%0Ashift%20gradually%2C%20causing%20only%20subtle%20behavioral%20changes.%20This%20paper%20proposes%20a%0Anovel%20approach%20to%20analyzing%20goal%20drift%20in%20LM%20agents.%20In%20our%20experiments%2C%20agents%0Aare%20first%20explicitly%20given%20a%20goal%20through%20their%20system%20prompt%2C%20then%20exposed%20to%0Acompeting%20objectives%20through%20environmental%20pressures.%20We%20demonstrate%20that%20while%0Athe%20best-performing%20agent%20%28a%20scaffolded%20version%20of%20Claude%203.5%20Sonnet%29%20maintains%0Anearly%20perfect%20goal%20adherence%20for%20more%20than%20100%2C000%20tokens%20in%20our%20most%0Adifficult%20evaluation%20setting%2C%20all%20evaluated%20models%20exhibit%20some%20degree%20of%20goal%0Adrift.%20We%20also%20find%20that%20goal%20drift%20correlates%20with%20models%27%20increasing%0Asusceptibility%20to%20pattern-matching%20behaviors%20as%20the%20context%20length%20grows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02709v1&entry.124074799=Read"},
{"title": "Bielik v3 Small: Technical Report", "author": "Krzysztof Ociepa and \u0141ukasz Flis and Remigiusz Kinas and Krzysztof Wr\u00f3bel and Adrian Gwo\u017adziej", "abstract": "  We introduce Bielik v3, a series of parameter-efficient generative text\nmodels (1.5B and 4.5B) optimized for Polish language processing. These models\ndemonstrate that smaller, well-optimized architectures can achieve performance\ncomparable to much larger counterparts while requiring substantially fewer\ncomputational resources. Our approach incorporates several key innovations: a\ncustom Polish tokenizer (APT4) that significantly improves token efficiency,\nWeighted Instruction Cross-Entropy Loss to balance learning across instruction\ntypes, and Adaptive Learning Rate that dynamically adjusts based on training\nprogress. Trained on a meticulously curated corpus of 292 billion tokens\nspanning 303 million documents, these models excel across multiple benchmarks,\nincluding the Open PL LLM Leaderboard, Complex Polish Text Understanding\nBenchmark, Polish EQ-Bench, and Polish Medical Leaderboard. The 4.5B parameter\nmodel achieves results competitive with models 2-3 times its size, while the\n1.5B model delivers strong performance despite its extremely compact profile.\nThese advances establish new benchmarks for parameter-efficient language\nmodeling in less-represented languages, making high-quality Polish language AI\nmore accessible for resource-constrained applications.\n", "link": "http://arxiv.org/abs/2505.02550v1", "date": "2025-05-05", "relevancy": 1.9034, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4779}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4779}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bielik%20v3%20Small%3A%20Technical%20Report&body=Title%3A%20Bielik%20v3%20Small%3A%20Technical%20Report%0AAuthor%3A%20Krzysztof%20Ociepa%20and%20%C5%81ukasz%20Flis%20and%20Remigiusz%20Kinas%20and%20Krzysztof%20Wr%C3%B3bel%20and%20Adrian%20Gwo%C5%BAdziej%0AAbstract%3A%20%20%20We%20introduce%20Bielik%20v3%2C%20a%20series%20of%20parameter-efficient%20generative%20text%0Amodels%20%281.5B%20and%204.5B%29%20optimized%20for%20Polish%20language%20processing.%20These%20models%0Ademonstrate%20that%20smaller%2C%20well-optimized%20architectures%20can%20achieve%20performance%0Acomparable%20to%20much%20larger%20counterparts%20while%20requiring%20substantially%20fewer%0Acomputational%20resources.%20Our%20approach%20incorporates%20several%20key%20innovations%3A%20a%0Acustom%20Polish%20tokenizer%20%28APT4%29%20that%20significantly%20improves%20token%20efficiency%2C%0AWeighted%20Instruction%20Cross-Entropy%20Loss%20to%20balance%20learning%20across%20instruction%0Atypes%2C%20and%20Adaptive%20Learning%20Rate%20that%20dynamically%20adjusts%20based%20on%20training%0Aprogress.%20Trained%20on%20a%20meticulously%20curated%20corpus%20of%20292%20billion%20tokens%0Aspanning%20303%20million%20documents%2C%20these%20models%20excel%20across%20multiple%20benchmarks%2C%0Aincluding%20the%20Open%20PL%20LLM%20Leaderboard%2C%20Complex%20Polish%20Text%20Understanding%0ABenchmark%2C%20Polish%20EQ-Bench%2C%20and%20Polish%20Medical%20Leaderboard.%20The%204.5B%20parameter%0Amodel%20achieves%20results%20competitive%20with%20models%202-3%20times%20its%20size%2C%20while%20the%0A1.5B%20model%20delivers%20strong%20performance%20despite%20its%20extremely%20compact%20profile.%0AThese%20advances%20establish%20new%20benchmarks%20for%20parameter-efficient%20language%0Amodeling%20in%20less-represented%20languages%2C%20making%20high-quality%20Polish%20language%20AI%0Amore%20accessible%20for%20resource-constrained%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02550v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBielik%2520v3%2520Small%253A%2520Technical%2520Report%26entry.906535625%3DKrzysztof%2520Ociepa%2520and%2520%25C5%2581ukasz%2520Flis%2520and%2520Remigiusz%2520Kinas%2520and%2520Krzysztof%2520Wr%25C3%25B3bel%2520and%2520Adrian%2520Gwo%25C5%25BAdziej%26entry.1292438233%3D%2520%2520We%2520introduce%2520Bielik%2520v3%252C%2520a%2520series%2520of%2520parameter-efficient%2520generative%2520text%250Amodels%2520%25281.5B%2520and%25204.5B%2529%2520optimized%2520for%2520Polish%2520language%2520processing.%2520These%2520models%250Ademonstrate%2520that%2520smaller%252C%2520well-optimized%2520architectures%2520can%2520achieve%2520performance%250Acomparable%2520to%2520much%2520larger%2520counterparts%2520while%2520requiring%2520substantially%2520fewer%250Acomputational%2520resources.%2520Our%2520approach%2520incorporates%2520several%2520key%2520innovations%253A%2520a%250Acustom%2520Polish%2520tokenizer%2520%2528APT4%2529%2520that%2520significantly%2520improves%2520token%2520efficiency%252C%250AWeighted%2520Instruction%2520Cross-Entropy%2520Loss%2520to%2520balance%2520learning%2520across%2520instruction%250Atypes%252C%2520and%2520Adaptive%2520Learning%2520Rate%2520that%2520dynamically%2520adjusts%2520based%2520on%2520training%250Aprogress.%2520Trained%2520on%2520a%2520meticulously%2520curated%2520corpus%2520of%2520292%2520billion%2520tokens%250Aspanning%2520303%2520million%2520documents%252C%2520these%2520models%2520excel%2520across%2520multiple%2520benchmarks%252C%250Aincluding%2520the%2520Open%2520PL%2520LLM%2520Leaderboard%252C%2520Complex%2520Polish%2520Text%2520Understanding%250ABenchmark%252C%2520Polish%2520EQ-Bench%252C%2520and%2520Polish%2520Medical%2520Leaderboard.%2520The%25204.5B%2520parameter%250Amodel%2520achieves%2520results%2520competitive%2520with%2520models%25202-3%2520times%2520its%2520size%252C%2520while%2520the%250A1.5B%2520model%2520delivers%2520strong%2520performance%2520despite%2520its%2520extremely%2520compact%2520profile.%250AThese%2520advances%2520establish%2520new%2520benchmarks%2520for%2520parameter-efficient%2520language%250Amodeling%2520in%2520less-represented%2520languages%252C%2520making%2520high-quality%2520Polish%2520language%2520AI%250Amore%2520accessible%2520for%2520resource-constrained%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02550v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bielik%20v3%20Small%3A%20Technical%20Report&entry.906535625=Krzysztof%20Ociepa%20and%20%C5%81ukasz%20Flis%20and%20Remigiusz%20Kinas%20and%20Krzysztof%20Wr%C3%B3bel%20and%20Adrian%20Gwo%C5%BAdziej&entry.1292438233=%20%20We%20introduce%20Bielik%20v3%2C%20a%20series%20of%20parameter-efficient%20generative%20text%0Amodels%20%281.5B%20and%204.5B%29%20optimized%20for%20Polish%20language%20processing.%20These%20models%0Ademonstrate%20that%20smaller%2C%20well-optimized%20architectures%20can%20achieve%20performance%0Acomparable%20to%20much%20larger%20counterparts%20while%20requiring%20substantially%20fewer%0Acomputational%20resources.%20Our%20approach%20incorporates%20several%20key%20innovations%3A%20a%0Acustom%20Polish%20tokenizer%20%28APT4%29%20that%20significantly%20improves%20token%20efficiency%2C%0AWeighted%20Instruction%20Cross-Entropy%20Loss%20to%20balance%20learning%20across%20instruction%0Atypes%2C%20and%20Adaptive%20Learning%20Rate%20that%20dynamically%20adjusts%20based%20on%20training%0Aprogress.%20Trained%20on%20a%20meticulously%20curated%20corpus%20of%20292%20billion%20tokens%0Aspanning%20303%20million%20documents%2C%20these%20models%20excel%20across%20multiple%20benchmarks%2C%0Aincluding%20the%20Open%20PL%20LLM%20Leaderboard%2C%20Complex%20Polish%20Text%20Understanding%0ABenchmark%2C%20Polish%20EQ-Bench%2C%20and%20Polish%20Medical%20Leaderboard.%20The%204.5B%20parameter%0Amodel%20achieves%20results%20competitive%20with%20models%202-3%20times%20its%20size%2C%20while%20the%0A1.5B%20model%20delivers%20strong%20performance%20despite%20its%20extremely%20compact%20profile.%0AThese%20advances%20establish%20new%20benchmarks%20for%20parameter-efficient%20language%0Amodeling%20in%20less-represented%20languages%2C%20making%20high-quality%20Polish%20language%20AI%0Amore%20accessible%20for%20resource-constrained%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02550v1&entry.124074799=Read"},
{"title": "Stabilizing and Solving Unique Continuation Problems by Parameterizing\n  Data and Learning Finite Element Solution Operators", "author": "Erik Burman and Mats G. Larson and Karl Larsson and Carl Lundholm", "abstract": "  We consider an inverse problem involving the reconstruction of the solution\nto a nonlinear partial differential equation (PDE) with unknown boundary\nconditions. Instead of direct boundary data, we are provided with a large\ndataset of boundary observations for typical solutions (collective data) and a\nbulk measurement of a specific realization. To leverage this collective data,\nwe first compress the boundary data using proper orthogonal decomposition (POD)\nin a linear expansion. Next, we identify a possible nonlinear low-dimensional\nstructure in the expansion coefficients using an autoencoder, which provides a\nparametrization of the dataset in a lower-dimensional latent space. We then\ntrain an operator network to map the expansion coefficients representing the\nboundary data to the finite element (FE) solution of the PDE. Finally, we\nconnect the autoencoder's decoder to the operator network which enables us to\nsolve the inverse problem by optimizing a data-fitting term over the latent\nspace. We analyze the underlying stabilized finite element method (FEM) in the\nlinear setting and establish an optimal error estimate in the $H^1$-norm. The\nnonlinear problem is then studied numerically, demonstrating the effectiveness\nof our approach.\n", "link": "http://arxiv.org/abs/2412.04409v3", "date": "2025-05-05", "relevancy": 1.9012, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4867}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4731}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4647}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stabilizing%20and%20Solving%20Unique%20Continuation%20Problems%20by%20Parameterizing%0A%20%20Data%20and%20Learning%20Finite%20Element%20Solution%20Operators&body=Title%3A%20Stabilizing%20and%20Solving%20Unique%20Continuation%20Problems%20by%20Parameterizing%0A%20%20Data%20and%20Learning%20Finite%20Element%20Solution%20Operators%0AAuthor%3A%20Erik%20Burman%20and%20Mats%20G.%20Larson%20and%20Karl%20Larsson%20and%20Carl%20Lundholm%0AAbstract%3A%20%20%20We%20consider%20an%20inverse%20problem%20involving%20the%20reconstruction%20of%20the%20solution%0Ato%20a%20nonlinear%20partial%20differential%20equation%20%28PDE%29%20with%20unknown%20boundary%0Aconditions.%20Instead%20of%20direct%20boundary%20data%2C%20we%20are%20provided%20with%20a%20large%0Adataset%20of%20boundary%20observations%20for%20typical%20solutions%20%28collective%20data%29%20and%20a%0Abulk%20measurement%20of%20a%20specific%20realization.%20To%20leverage%20this%20collective%20data%2C%0Awe%20first%20compress%20the%20boundary%20data%20using%20proper%20orthogonal%20decomposition%20%28POD%29%0Ain%20a%20linear%20expansion.%20Next%2C%20we%20identify%20a%20possible%20nonlinear%20low-dimensional%0Astructure%20in%20the%20expansion%20coefficients%20using%20an%20autoencoder%2C%20which%20provides%20a%0Aparametrization%20of%20the%20dataset%20in%20a%20lower-dimensional%20latent%20space.%20We%20then%0Atrain%20an%20operator%20network%20to%20map%20the%20expansion%20coefficients%20representing%20the%0Aboundary%20data%20to%20the%20finite%20element%20%28FE%29%20solution%20of%20the%20PDE.%20Finally%2C%20we%0Aconnect%20the%20autoencoder%27s%20decoder%20to%20the%20operator%20network%20which%20enables%20us%20to%0Asolve%20the%20inverse%20problem%20by%20optimizing%20a%20data-fitting%20term%20over%20the%20latent%0Aspace.%20We%20analyze%20the%20underlying%20stabilized%20finite%20element%20method%20%28FEM%29%20in%20the%0Alinear%20setting%20and%20establish%20an%20optimal%20error%20estimate%20in%20the%20%24H%5E1%24-norm.%20The%0Anonlinear%20problem%20is%20then%20studied%20numerically%2C%20demonstrating%20the%20effectiveness%0Aof%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.04409v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStabilizing%2520and%2520Solving%2520Unique%2520Continuation%2520Problems%2520by%2520Parameterizing%250A%2520%2520Data%2520and%2520Learning%2520Finite%2520Element%2520Solution%2520Operators%26entry.906535625%3DErik%2520Burman%2520and%2520Mats%2520G.%2520Larson%2520and%2520Karl%2520Larsson%2520and%2520Carl%2520Lundholm%26entry.1292438233%3D%2520%2520We%2520consider%2520an%2520inverse%2520problem%2520involving%2520the%2520reconstruction%2520of%2520the%2520solution%250Ato%2520a%2520nonlinear%2520partial%2520differential%2520equation%2520%2528PDE%2529%2520with%2520unknown%2520boundary%250Aconditions.%2520Instead%2520of%2520direct%2520boundary%2520data%252C%2520we%2520are%2520provided%2520with%2520a%2520large%250Adataset%2520of%2520boundary%2520observations%2520for%2520typical%2520solutions%2520%2528collective%2520data%2529%2520and%2520a%250Abulk%2520measurement%2520of%2520a%2520specific%2520realization.%2520To%2520leverage%2520this%2520collective%2520data%252C%250Awe%2520first%2520compress%2520the%2520boundary%2520data%2520using%2520proper%2520orthogonal%2520decomposition%2520%2528POD%2529%250Ain%2520a%2520linear%2520expansion.%2520Next%252C%2520we%2520identify%2520a%2520possible%2520nonlinear%2520low-dimensional%250Astructure%2520in%2520the%2520expansion%2520coefficients%2520using%2520an%2520autoencoder%252C%2520which%2520provides%2520a%250Aparametrization%2520of%2520the%2520dataset%2520in%2520a%2520lower-dimensional%2520latent%2520space.%2520We%2520then%250Atrain%2520an%2520operator%2520network%2520to%2520map%2520the%2520expansion%2520coefficients%2520representing%2520the%250Aboundary%2520data%2520to%2520the%2520finite%2520element%2520%2528FE%2529%2520solution%2520of%2520the%2520PDE.%2520Finally%252C%2520we%250Aconnect%2520the%2520autoencoder%2527s%2520decoder%2520to%2520the%2520operator%2520network%2520which%2520enables%2520us%2520to%250Asolve%2520the%2520inverse%2520problem%2520by%2520optimizing%2520a%2520data-fitting%2520term%2520over%2520the%2520latent%250Aspace.%2520We%2520analyze%2520the%2520underlying%2520stabilized%2520finite%2520element%2520method%2520%2528FEM%2529%2520in%2520the%250Alinear%2520setting%2520and%2520establish%2520an%2520optimal%2520error%2520estimate%2520in%2520the%2520%2524H%255E1%2524-norm.%2520The%250Anonlinear%2520problem%2520is%2520then%2520studied%2520numerically%252C%2520demonstrating%2520the%2520effectiveness%250Aof%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.04409v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stabilizing%20and%20Solving%20Unique%20Continuation%20Problems%20by%20Parameterizing%0A%20%20Data%20and%20Learning%20Finite%20Element%20Solution%20Operators&entry.906535625=Erik%20Burman%20and%20Mats%20G.%20Larson%20and%20Karl%20Larsson%20and%20Carl%20Lundholm&entry.1292438233=%20%20We%20consider%20an%20inverse%20problem%20involving%20the%20reconstruction%20of%20the%20solution%0Ato%20a%20nonlinear%20partial%20differential%20equation%20%28PDE%29%20with%20unknown%20boundary%0Aconditions.%20Instead%20of%20direct%20boundary%20data%2C%20we%20are%20provided%20with%20a%20large%0Adataset%20of%20boundary%20observations%20for%20typical%20solutions%20%28collective%20data%29%20and%20a%0Abulk%20measurement%20of%20a%20specific%20realization.%20To%20leverage%20this%20collective%20data%2C%0Awe%20first%20compress%20the%20boundary%20data%20using%20proper%20orthogonal%20decomposition%20%28POD%29%0Ain%20a%20linear%20expansion.%20Next%2C%20we%20identify%20a%20possible%20nonlinear%20low-dimensional%0Astructure%20in%20the%20expansion%20coefficients%20using%20an%20autoencoder%2C%20which%20provides%20a%0Aparametrization%20of%20the%20dataset%20in%20a%20lower-dimensional%20latent%20space.%20We%20then%0Atrain%20an%20operator%20network%20to%20map%20the%20expansion%20coefficients%20representing%20the%0Aboundary%20data%20to%20the%20finite%20element%20%28FE%29%20solution%20of%20the%20PDE.%20Finally%2C%20we%0Aconnect%20the%20autoencoder%27s%20decoder%20to%20the%20operator%20network%20which%20enables%20us%20to%0Asolve%20the%20inverse%20problem%20by%20optimizing%20a%20data-fitting%20term%20over%20the%20latent%0Aspace.%20We%20analyze%20the%20underlying%20stabilized%20finite%20element%20method%20%28FEM%29%20in%20the%0Alinear%20setting%20and%20establish%20an%20optimal%20error%20estimate%20in%20the%20%24H%5E1%24-norm.%20The%0Anonlinear%20problem%20is%20then%20studied%20numerically%2C%20demonstrating%20the%20effectiveness%0Aof%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.04409v3&entry.124074799=Read"},
{"title": "Bye-bye, Bluebook? Automating Legal Procedure with Large Language Models", "author": "Matthew Dahl", "abstract": "  Legal practice requires careful adherence to procedural rules. In the United\nStates, few are more complex than those found in The Bluebook: A Uniform System\nof Citation. Compliance with this system's 500+ pages of byzantine formatting\ninstructions is the raison d'etre of thousands of student law review editors\nand the bete noire of lawyers everywhere. To evaluate whether large language\nmodels (LLMs) are able to adhere to the procedures of such a complicated\nsystem, we construct an original dataset of 866 Bluebook tasks and test\nflagship LLMs from OpenAI, Anthropic, Google, Meta, and DeepSeek. We show (1)\nthat these models produce fully compliant Bluebook citations only 69%-74% of\nthe time and (2) that in-context learning on the Bluebook's underlying system\nof rules raises accuracy only to 77%. These results caution against using\noff-the-shelf LLMs to automate aspects of the law where fidelity to procedure\nis paramount.\n", "link": "http://arxiv.org/abs/2505.02763v1", "date": "2025-05-05", "relevancy": 1.8962, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4743}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4743}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bye-bye%2C%20Bluebook%3F%20Automating%20Legal%20Procedure%20with%20Large%20Language%20Models&body=Title%3A%20Bye-bye%2C%20Bluebook%3F%20Automating%20Legal%20Procedure%20with%20Large%20Language%20Models%0AAuthor%3A%20Matthew%20Dahl%0AAbstract%3A%20%20%20Legal%20practice%20requires%20careful%20adherence%20to%20procedural%20rules.%20In%20the%20United%0AStates%2C%20few%20are%20more%20complex%20than%20those%20found%20in%20The%20Bluebook%3A%20A%20Uniform%20System%0Aof%20Citation.%20Compliance%20with%20this%20system%27s%20500%2B%20pages%20of%20byzantine%20formatting%0Ainstructions%20is%20the%20raison%20d%27etre%20of%20thousands%20of%20student%20law%20review%20editors%0Aand%20the%20bete%20noire%20of%20lawyers%20everywhere.%20To%20evaluate%20whether%20large%20language%0Amodels%20%28LLMs%29%20are%20able%20to%20adhere%20to%20the%20procedures%20of%20such%20a%20complicated%0Asystem%2C%20we%20construct%20an%20original%20dataset%20of%20866%20Bluebook%20tasks%20and%20test%0Aflagship%20LLMs%20from%20OpenAI%2C%20Anthropic%2C%20Google%2C%20Meta%2C%20and%20DeepSeek.%20We%20show%20%281%29%0Athat%20these%20models%20produce%20fully%20compliant%20Bluebook%20citations%20only%2069%25-74%25%20of%0Athe%20time%20and%20%282%29%20that%20in-context%20learning%20on%20the%20Bluebook%27s%20underlying%20system%0Aof%20rules%20raises%20accuracy%20only%20to%2077%25.%20These%20results%20caution%20against%20using%0Aoff-the-shelf%20LLMs%20to%20automate%20aspects%20of%20the%20law%20where%20fidelity%20to%20procedure%0Ais%20paramount.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02763v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBye-bye%252C%2520Bluebook%253F%2520Automating%2520Legal%2520Procedure%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DMatthew%2520Dahl%26entry.1292438233%3D%2520%2520Legal%2520practice%2520requires%2520careful%2520adherence%2520to%2520procedural%2520rules.%2520In%2520the%2520United%250AStates%252C%2520few%2520are%2520more%2520complex%2520than%2520those%2520found%2520in%2520The%2520Bluebook%253A%2520A%2520Uniform%2520System%250Aof%2520Citation.%2520Compliance%2520with%2520this%2520system%2527s%2520500%252B%2520pages%2520of%2520byzantine%2520formatting%250Ainstructions%2520is%2520the%2520raison%2520d%2527etre%2520of%2520thousands%2520of%2520student%2520law%2520review%2520editors%250Aand%2520the%2520bete%2520noire%2520of%2520lawyers%2520everywhere.%2520To%2520evaluate%2520whether%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520are%2520able%2520to%2520adhere%2520to%2520the%2520procedures%2520of%2520such%2520a%2520complicated%250Asystem%252C%2520we%2520construct%2520an%2520original%2520dataset%2520of%2520866%2520Bluebook%2520tasks%2520and%2520test%250Aflagship%2520LLMs%2520from%2520OpenAI%252C%2520Anthropic%252C%2520Google%252C%2520Meta%252C%2520and%2520DeepSeek.%2520We%2520show%2520%25281%2529%250Athat%2520these%2520models%2520produce%2520fully%2520compliant%2520Bluebook%2520citations%2520only%252069%2525-74%2525%2520of%250Athe%2520time%2520and%2520%25282%2529%2520that%2520in-context%2520learning%2520on%2520the%2520Bluebook%2527s%2520underlying%2520system%250Aof%2520rules%2520raises%2520accuracy%2520only%2520to%252077%2525.%2520These%2520results%2520caution%2520against%2520using%250Aoff-the-shelf%2520LLMs%2520to%2520automate%2520aspects%2520of%2520the%2520law%2520where%2520fidelity%2520to%2520procedure%250Ais%2520paramount.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02763v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bye-bye%2C%20Bluebook%3F%20Automating%20Legal%20Procedure%20with%20Large%20Language%20Models&entry.906535625=Matthew%20Dahl&entry.1292438233=%20%20Legal%20practice%20requires%20careful%20adherence%20to%20procedural%20rules.%20In%20the%20United%0AStates%2C%20few%20are%20more%20complex%20than%20those%20found%20in%20The%20Bluebook%3A%20A%20Uniform%20System%0Aof%20Citation.%20Compliance%20with%20this%20system%27s%20500%2B%20pages%20of%20byzantine%20formatting%0Ainstructions%20is%20the%20raison%20d%27etre%20of%20thousands%20of%20student%20law%20review%20editors%0Aand%20the%20bete%20noire%20of%20lawyers%20everywhere.%20To%20evaluate%20whether%20large%20language%0Amodels%20%28LLMs%29%20are%20able%20to%20adhere%20to%20the%20procedures%20of%20such%20a%20complicated%0Asystem%2C%20we%20construct%20an%20original%20dataset%20of%20866%20Bluebook%20tasks%20and%20test%0Aflagship%20LLMs%20from%20OpenAI%2C%20Anthropic%2C%20Google%2C%20Meta%2C%20and%20DeepSeek.%20We%20show%20%281%29%0Athat%20these%20models%20produce%20fully%20compliant%20Bluebook%20citations%20only%2069%25-74%25%20of%0Athe%20time%20and%20%282%29%20that%20in-context%20learning%20on%20the%20Bluebook%27s%20underlying%20system%0Aof%20rules%20raises%20accuracy%20only%20to%2077%25.%20These%20results%20caution%20against%20using%0Aoff-the-shelf%20LLMs%20to%20automate%20aspects%20of%20the%20law%20where%20fidelity%20to%20procedure%0Ais%20paramount.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02763v1&entry.124074799=Read"},
{"title": "Advancing Constrained Monotonic Neural Networks: Achieving Universal\n  Approximation Beyond Bounded Activations", "author": "Davide Sartor and Alberto Sinigaglia and Gian Antonio Susto", "abstract": "  Conventional techniques for imposing monotonicity in MLPs by construction\ninvolve the use of non-negative weight constraints and bounded activation\nfunctions, which pose well-known optimization challenges. In this work, we\ngeneralize previous theoretical results, showing that MLPs with non-negative\nweight constraint and activations that saturate on alternating sides are\nuniversal approximators for monotonic functions. Additionally, we show an\nequivalence between the saturation side in the activations and the sign of the\nweight constraint. This connection allows us to prove that MLPs with convex\nmonotone activations and non-positive constrained weights also qualify as\nuniversal approximators, in contrast to their non-negative constrained\ncounterparts. Our results provide theoretical grounding to the empirical\neffectiveness observed in previous works while leading to possible\narchitectural simplification. Moreover, to further alleviate the optimization\ndifficulties, we propose an alternative formulation that allows the network to\nadjust its activations according to the sign of the weights. This eliminates\nthe requirement for weight reparameterization, easing initialization and\nimproving training stability. Experimental evaluation reinforces the validity\nof the theoretical results, showing that our novel approach compares favourably\nto traditional monotonic architectures.\n", "link": "http://arxiv.org/abs/2505.02537v1", "date": "2025-05-05", "relevancy": 1.8824, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4775}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4673}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4617}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Constrained%20Monotonic%20Neural%20Networks%3A%20Achieving%20Universal%0A%20%20Approximation%20Beyond%20Bounded%20Activations&body=Title%3A%20Advancing%20Constrained%20Monotonic%20Neural%20Networks%3A%20Achieving%20Universal%0A%20%20Approximation%20Beyond%20Bounded%20Activations%0AAuthor%3A%20Davide%20Sartor%20and%20Alberto%20Sinigaglia%20and%20Gian%20Antonio%20Susto%0AAbstract%3A%20%20%20Conventional%20techniques%20for%20imposing%20monotonicity%20in%20MLPs%20by%20construction%0Ainvolve%20the%20use%20of%20non-negative%20weight%20constraints%20and%20bounded%20activation%0Afunctions%2C%20which%20pose%20well-known%20optimization%20challenges.%20In%20this%20work%2C%20we%0Ageneralize%20previous%20theoretical%20results%2C%20showing%20that%20MLPs%20with%20non-negative%0Aweight%20constraint%20and%20activations%20that%20saturate%20on%20alternating%20sides%20are%0Auniversal%20approximators%20for%20monotonic%20functions.%20Additionally%2C%20we%20show%20an%0Aequivalence%20between%20the%20saturation%20side%20in%20the%20activations%20and%20the%20sign%20of%20the%0Aweight%20constraint.%20This%20connection%20allows%20us%20to%20prove%20that%20MLPs%20with%20convex%0Amonotone%20activations%20and%20non-positive%20constrained%20weights%20also%20qualify%20as%0Auniversal%20approximators%2C%20in%20contrast%20to%20their%20non-negative%20constrained%0Acounterparts.%20Our%20results%20provide%20theoretical%20grounding%20to%20the%20empirical%0Aeffectiveness%20observed%20in%20previous%20works%20while%20leading%20to%20possible%0Aarchitectural%20simplification.%20Moreover%2C%20to%20further%20alleviate%20the%20optimization%0Adifficulties%2C%20we%20propose%20an%20alternative%20formulation%20that%20allows%20the%20network%20to%0Aadjust%20its%20activations%20according%20to%20the%20sign%20of%20the%20weights.%20This%20eliminates%0Athe%20requirement%20for%20weight%20reparameterization%2C%20easing%20initialization%20and%0Aimproving%20training%20stability.%20Experimental%20evaluation%20reinforces%20the%20validity%0Aof%20the%20theoretical%20results%2C%20showing%20that%20our%20novel%20approach%20compares%20favourably%0Ato%20traditional%20monotonic%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02537v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Constrained%2520Monotonic%2520Neural%2520Networks%253A%2520Achieving%2520Universal%250A%2520%2520Approximation%2520Beyond%2520Bounded%2520Activations%26entry.906535625%3DDavide%2520Sartor%2520and%2520Alberto%2520Sinigaglia%2520and%2520Gian%2520Antonio%2520Susto%26entry.1292438233%3D%2520%2520Conventional%2520techniques%2520for%2520imposing%2520monotonicity%2520in%2520MLPs%2520by%2520construction%250Ainvolve%2520the%2520use%2520of%2520non-negative%2520weight%2520constraints%2520and%2520bounded%2520activation%250Afunctions%252C%2520which%2520pose%2520well-known%2520optimization%2520challenges.%2520In%2520this%2520work%252C%2520we%250Ageneralize%2520previous%2520theoretical%2520results%252C%2520showing%2520that%2520MLPs%2520with%2520non-negative%250Aweight%2520constraint%2520and%2520activations%2520that%2520saturate%2520on%2520alternating%2520sides%2520are%250Auniversal%2520approximators%2520for%2520monotonic%2520functions.%2520Additionally%252C%2520we%2520show%2520an%250Aequivalence%2520between%2520the%2520saturation%2520side%2520in%2520the%2520activations%2520and%2520the%2520sign%2520of%2520the%250Aweight%2520constraint.%2520This%2520connection%2520allows%2520us%2520to%2520prove%2520that%2520MLPs%2520with%2520convex%250Amonotone%2520activations%2520and%2520non-positive%2520constrained%2520weights%2520also%2520qualify%2520as%250Auniversal%2520approximators%252C%2520in%2520contrast%2520to%2520their%2520non-negative%2520constrained%250Acounterparts.%2520Our%2520results%2520provide%2520theoretical%2520grounding%2520to%2520the%2520empirical%250Aeffectiveness%2520observed%2520in%2520previous%2520works%2520while%2520leading%2520to%2520possible%250Aarchitectural%2520simplification.%2520Moreover%252C%2520to%2520further%2520alleviate%2520the%2520optimization%250Adifficulties%252C%2520we%2520propose%2520an%2520alternative%2520formulation%2520that%2520allows%2520the%2520network%2520to%250Aadjust%2520its%2520activations%2520according%2520to%2520the%2520sign%2520of%2520the%2520weights.%2520This%2520eliminates%250Athe%2520requirement%2520for%2520weight%2520reparameterization%252C%2520easing%2520initialization%2520and%250Aimproving%2520training%2520stability.%2520Experimental%2520evaluation%2520reinforces%2520the%2520validity%250Aof%2520the%2520theoretical%2520results%252C%2520showing%2520that%2520our%2520novel%2520approach%2520compares%2520favourably%250Ato%2520traditional%2520monotonic%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02537v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Constrained%20Monotonic%20Neural%20Networks%3A%20Achieving%20Universal%0A%20%20Approximation%20Beyond%20Bounded%20Activations&entry.906535625=Davide%20Sartor%20and%20Alberto%20Sinigaglia%20and%20Gian%20Antonio%20Susto&entry.1292438233=%20%20Conventional%20techniques%20for%20imposing%20monotonicity%20in%20MLPs%20by%20construction%0Ainvolve%20the%20use%20of%20non-negative%20weight%20constraints%20and%20bounded%20activation%0Afunctions%2C%20which%20pose%20well-known%20optimization%20challenges.%20In%20this%20work%2C%20we%0Ageneralize%20previous%20theoretical%20results%2C%20showing%20that%20MLPs%20with%20non-negative%0Aweight%20constraint%20and%20activations%20that%20saturate%20on%20alternating%20sides%20are%0Auniversal%20approximators%20for%20monotonic%20functions.%20Additionally%2C%20we%20show%20an%0Aequivalence%20between%20the%20saturation%20side%20in%20the%20activations%20and%20the%20sign%20of%20the%0Aweight%20constraint.%20This%20connection%20allows%20us%20to%20prove%20that%20MLPs%20with%20convex%0Amonotone%20activations%20and%20non-positive%20constrained%20weights%20also%20qualify%20as%0Auniversal%20approximators%2C%20in%20contrast%20to%20their%20non-negative%20constrained%0Acounterparts.%20Our%20results%20provide%20theoretical%20grounding%20to%20the%20empirical%0Aeffectiveness%20observed%20in%20previous%20works%20while%20leading%20to%20possible%0Aarchitectural%20simplification.%20Moreover%2C%20to%20further%20alleviate%20the%20optimization%0Adifficulties%2C%20we%20propose%20an%20alternative%20formulation%20that%20allows%20the%20network%20to%0Aadjust%20its%20activations%20according%20to%20the%20sign%20of%20the%20weights.%20This%20eliminates%0Athe%20requirement%20for%20weight%20reparameterization%2C%20easing%20initialization%20and%0Aimproving%20training%20stability.%20Experimental%20evaluation%20reinforces%20the%20validity%0Aof%20the%20theoretical%20results%2C%20showing%20that%20our%20novel%20approach%20compares%20favourably%0Ato%20traditional%20monotonic%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02537v1&entry.124074799=Read"},
{"title": "Enhancing LLMs' Clinical Reasoning with Real-World Data from a\n  Nationwide Sepsis Registry", "author": "Junu Kim and Chaeeun Shim and Sungjin Park and Su Yeon Lee and Gee Young Suh and Chae-Man Lim and Seong Jin Choi and Song Mi Moon and Kyoung-Ho Song and Eu Suk Kim and Hong Bin Kim and Sejoong Kim and Chami Im and Dong-Wan Kang and Yong Soo Kim and Hee-Joon Bae and Sung Yoon Lim and Han-Gil Jeong and Edward Choi", "abstract": "  Although large language models (LLMs) have demonstrated impressive reasoning\ncapabilities across general domains, their effectiveness in real-world clinical\npractice remains limited. This is likely due to their insufficient exposure to\nreal-world clinical data during training, as such data is typically not\nincluded due to privacy concerns. To address this, we propose enhancing the\nclinical reasoning capabilities of LLMs by leveraging real-world clinical data.\nWe constructed reasoning-intensive questions from a nationwide sepsis registry\nand fine-tuned Phi-4 on these questions using reinforcement learning, resulting\nin C-Reason. C-Reason exhibited strong clinical reasoning capabilities on the\nin-domain test set, as evidenced by both quantitative metrics and expert\nevaluations. Furthermore, its enhanced reasoning capabilities generalized to a\nsepsis dataset involving different tasks and patient cohorts, an open-ended\nconsultations on antibiotics use task, and other diseases. Future research\nshould focus on training LLMs with large-scale, multi-disease clinical datasets\nto develop more powerful, general-purpose clinical reasoning models.\n", "link": "http://arxiv.org/abs/2505.02722v1", "date": "2025-05-05", "relevancy": 1.8815, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4726}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4726}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20LLMs%27%20Clinical%20Reasoning%20with%20Real-World%20Data%20from%20a%0A%20%20Nationwide%20Sepsis%20Registry&body=Title%3A%20Enhancing%20LLMs%27%20Clinical%20Reasoning%20with%20Real-World%20Data%20from%20a%0A%20%20Nationwide%20Sepsis%20Registry%0AAuthor%3A%20Junu%20Kim%20and%20Chaeeun%20Shim%20and%20Sungjin%20Park%20and%20Su%20Yeon%20Lee%20and%20Gee%20Young%20Suh%20and%20Chae-Man%20Lim%20and%20Seong%20Jin%20Choi%20and%20Song%20Mi%20Moon%20and%20Kyoung-Ho%20Song%20and%20Eu%20Suk%20Kim%20and%20Hong%20Bin%20Kim%20and%20Sejoong%20Kim%20and%20Chami%20Im%20and%20Dong-Wan%20Kang%20and%20Yong%20Soo%20Kim%20and%20Hee-Joon%20Bae%20and%20Sung%20Yoon%20Lim%20and%20Han-Gil%20Jeong%20and%20Edward%20Choi%0AAbstract%3A%20%20%20Although%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20impressive%20reasoning%0Acapabilities%20across%20general%20domains%2C%20their%20effectiveness%20in%20real-world%20clinical%0Apractice%20remains%20limited.%20This%20is%20likely%20due%20to%20their%20insufficient%20exposure%20to%0Areal-world%20clinical%20data%20during%20training%2C%20as%20such%20data%20is%20typically%20not%0Aincluded%20due%20to%20privacy%20concerns.%20To%20address%20this%2C%20we%20propose%20enhancing%20the%0Aclinical%20reasoning%20capabilities%20of%20LLMs%20by%20leveraging%20real-world%20clinical%20data.%0AWe%20constructed%20reasoning-intensive%20questions%20from%20a%20nationwide%20sepsis%20registry%0Aand%20fine-tuned%20Phi-4%20on%20these%20questions%20using%20reinforcement%20learning%2C%20resulting%0Ain%20C-Reason.%20C-Reason%20exhibited%20strong%20clinical%20reasoning%20capabilities%20on%20the%0Ain-domain%20test%20set%2C%20as%20evidenced%20by%20both%20quantitative%20metrics%20and%20expert%0Aevaluations.%20Furthermore%2C%20its%20enhanced%20reasoning%20capabilities%20generalized%20to%20a%0Asepsis%20dataset%20involving%20different%20tasks%20and%20patient%20cohorts%2C%20an%20open-ended%0Aconsultations%20on%20antibiotics%20use%20task%2C%20and%20other%20diseases.%20Future%20research%0Ashould%20focus%20on%20training%20LLMs%20with%20large-scale%2C%20multi-disease%20clinical%20datasets%0Ato%20develop%20more%20powerful%2C%20general-purpose%20clinical%20reasoning%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02722v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520LLMs%2527%2520Clinical%2520Reasoning%2520with%2520Real-World%2520Data%2520from%2520a%250A%2520%2520Nationwide%2520Sepsis%2520Registry%26entry.906535625%3DJunu%2520Kim%2520and%2520Chaeeun%2520Shim%2520and%2520Sungjin%2520Park%2520and%2520Su%2520Yeon%2520Lee%2520and%2520Gee%2520Young%2520Suh%2520and%2520Chae-Man%2520Lim%2520and%2520Seong%2520Jin%2520Choi%2520and%2520Song%2520Mi%2520Moon%2520and%2520Kyoung-Ho%2520Song%2520and%2520Eu%2520Suk%2520Kim%2520and%2520Hong%2520Bin%2520Kim%2520and%2520Sejoong%2520Kim%2520and%2520Chami%2520Im%2520and%2520Dong-Wan%2520Kang%2520and%2520Yong%2520Soo%2520Kim%2520and%2520Hee-Joon%2520Bae%2520and%2520Sung%2520Yoon%2520Lim%2520and%2520Han-Gil%2520Jeong%2520and%2520Edward%2520Choi%26entry.1292438233%3D%2520%2520Although%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520impressive%2520reasoning%250Acapabilities%2520across%2520general%2520domains%252C%2520their%2520effectiveness%2520in%2520real-world%2520clinical%250Apractice%2520remains%2520limited.%2520This%2520is%2520likely%2520due%2520to%2520their%2520insufficient%2520exposure%2520to%250Areal-world%2520clinical%2520data%2520during%2520training%252C%2520as%2520such%2520data%2520is%2520typically%2520not%250Aincluded%2520due%2520to%2520privacy%2520concerns.%2520To%2520address%2520this%252C%2520we%2520propose%2520enhancing%2520the%250Aclinical%2520reasoning%2520capabilities%2520of%2520LLMs%2520by%2520leveraging%2520real-world%2520clinical%2520data.%250AWe%2520constructed%2520reasoning-intensive%2520questions%2520from%2520a%2520nationwide%2520sepsis%2520registry%250Aand%2520fine-tuned%2520Phi-4%2520on%2520these%2520questions%2520using%2520reinforcement%2520learning%252C%2520resulting%250Ain%2520C-Reason.%2520C-Reason%2520exhibited%2520strong%2520clinical%2520reasoning%2520capabilities%2520on%2520the%250Ain-domain%2520test%2520set%252C%2520as%2520evidenced%2520by%2520both%2520quantitative%2520metrics%2520and%2520expert%250Aevaluations.%2520Furthermore%252C%2520its%2520enhanced%2520reasoning%2520capabilities%2520generalized%2520to%2520a%250Asepsis%2520dataset%2520involving%2520different%2520tasks%2520and%2520patient%2520cohorts%252C%2520an%2520open-ended%250Aconsultations%2520on%2520antibiotics%2520use%2520task%252C%2520and%2520other%2520diseases.%2520Future%2520research%250Ashould%2520focus%2520on%2520training%2520LLMs%2520with%2520large-scale%252C%2520multi-disease%2520clinical%2520datasets%250Ato%2520develop%2520more%2520powerful%252C%2520general-purpose%2520clinical%2520reasoning%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02722v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20LLMs%27%20Clinical%20Reasoning%20with%20Real-World%20Data%20from%20a%0A%20%20Nationwide%20Sepsis%20Registry&entry.906535625=Junu%20Kim%20and%20Chaeeun%20Shim%20and%20Sungjin%20Park%20and%20Su%20Yeon%20Lee%20and%20Gee%20Young%20Suh%20and%20Chae-Man%20Lim%20and%20Seong%20Jin%20Choi%20and%20Song%20Mi%20Moon%20and%20Kyoung-Ho%20Song%20and%20Eu%20Suk%20Kim%20and%20Hong%20Bin%20Kim%20and%20Sejoong%20Kim%20and%20Chami%20Im%20and%20Dong-Wan%20Kang%20and%20Yong%20Soo%20Kim%20and%20Hee-Joon%20Bae%20and%20Sung%20Yoon%20Lim%20and%20Han-Gil%20Jeong%20and%20Edward%20Choi&entry.1292438233=%20%20Although%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20impressive%20reasoning%0Acapabilities%20across%20general%20domains%2C%20their%20effectiveness%20in%20real-world%20clinical%0Apractice%20remains%20limited.%20This%20is%20likely%20due%20to%20their%20insufficient%20exposure%20to%0Areal-world%20clinical%20data%20during%20training%2C%20as%20such%20data%20is%20typically%20not%0Aincluded%20due%20to%20privacy%20concerns.%20To%20address%20this%2C%20we%20propose%20enhancing%20the%0Aclinical%20reasoning%20capabilities%20of%20LLMs%20by%20leveraging%20real-world%20clinical%20data.%0AWe%20constructed%20reasoning-intensive%20questions%20from%20a%20nationwide%20sepsis%20registry%0Aand%20fine-tuned%20Phi-4%20on%20these%20questions%20using%20reinforcement%20learning%2C%20resulting%0Ain%20C-Reason.%20C-Reason%20exhibited%20strong%20clinical%20reasoning%20capabilities%20on%20the%0Ain-domain%20test%20set%2C%20as%20evidenced%20by%20both%20quantitative%20metrics%20and%20expert%0Aevaluations.%20Furthermore%2C%20its%20enhanced%20reasoning%20capabilities%20generalized%20to%20a%0Asepsis%20dataset%20involving%20different%20tasks%20and%20patient%20cohorts%2C%20an%20open-ended%0Aconsultations%20on%20antibiotics%20use%20task%2C%20and%20other%20diseases.%20Future%20research%0Ashould%20focus%20on%20training%20LLMs%20with%20large-scale%2C%20multi-disease%20clinical%20datasets%0Ato%20develop%20more%20powerful%2C%20general-purpose%20clinical%20reasoning%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02722v1&entry.124074799=Read"},
{"title": "Physics-Informed Weakly Supervised Learning for Interatomic Potentials", "author": "Makoto Takamoto and Viktor Zaverkin and Mathias Niepert", "abstract": "  Machine learning plays an increasingly important role in computational\nchemistry and materials science, complementing computationally intensive ab\ninitio and first-principles methods. Despite their utility, machine-learning\nmodels often lack generalization capability and robustness during atomistic\nsimulations, yielding unphysical energy and force predictions that hinder their\nreal-world applications. We address this challenge by introducing a\nphysics-informed, weakly supervised approach for training machine-learned\ninteratomic potentials (MLIPs). We introduce two novel loss functions,\nextrapolating the potential energy via a Taylor expansion and using the concept\nof conservative forces. Our approach improves the accuracy of MLIPs applied to\ntraining tasks with sparse training data sets and reduces the need for\npre-training computationally demanding models with large data sets.\nParticularly, we perform extensive experiments demonstrating reduced energy and\nforce errors -- often lower by a factor of two -- for various baseline models\nand benchmark data sets. Moreover, we demonstrate improved robustness during MD\nsimulations of the MLIP models trained with the proposed weakly supervised\nloss. Finally, our approach improves the fine-tuning of foundation models on\nsparse, highly accurate ab initio data. An implementation of our method and\nscripts for executing experiments are available at\nhttps://github.com/nec-research/PICPS-ML4Sci.\n", "link": "http://arxiv.org/abs/2408.05215v2", "date": "2025-05-05", "relevancy": 1.8617, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4778}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4702}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics-Informed%20Weakly%20Supervised%20Learning%20for%20Interatomic%20Potentials&body=Title%3A%20Physics-Informed%20Weakly%20Supervised%20Learning%20for%20Interatomic%20Potentials%0AAuthor%3A%20Makoto%20Takamoto%20and%20Viktor%20Zaverkin%20and%20Mathias%20Niepert%0AAbstract%3A%20%20%20Machine%20learning%20plays%20an%20increasingly%20important%20role%20in%20computational%0Achemistry%20and%20materials%20science%2C%20complementing%20computationally%20intensive%20ab%0Ainitio%20and%20first-principles%20methods.%20Despite%20their%20utility%2C%20machine-learning%0Amodels%20often%20lack%20generalization%20capability%20and%20robustness%20during%20atomistic%0Asimulations%2C%20yielding%20unphysical%20energy%20and%20force%20predictions%20that%20hinder%20their%0Areal-world%20applications.%20We%20address%20this%20challenge%20by%20introducing%20a%0Aphysics-informed%2C%20weakly%20supervised%20approach%20for%20training%20machine-learned%0Ainteratomic%20potentials%20%28MLIPs%29.%20We%20introduce%20two%20novel%20loss%20functions%2C%0Aextrapolating%20the%20potential%20energy%20via%20a%20Taylor%20expansion%20and%20using%20the%20concept%0Aof%20conservative%20forces.%20Our%20approach%20improves%20the%20accuracy%20of%20MLIPs%20applied%20to%0Atraining%20tasks%20with%20sparse%20training%20data%20sets%20and%20reduces%20the%20need%20for%0Apre-training%20computationally%20demanding%20models%20with%20large%20data%20sets.%0AParticularly%2C%20we%20perform%20extensive%20experiments%20demonstrating%20reduced%20energy%20and%0Aforce%20errors%20--%20often%20lower%20by%20a%20factor%20of%20two%20--%20for%20various%20baseline%20models%0Aand%20benchmark%20data%20sets.%20Moreover%2C%20we%20demonstrate%20improved%20robustness%20during%20MD%0Asimulations%20of%20the%20MLIP%20models%20trained%20with%20the%20proposed%20weakly%20supervised%0Aloss.%20Finally%2C%20our%20approach%20improves%20the%20fine-tuning%20of%20foundation%20models%20on%0Asparse%2C%20highly%20accurate%20ab%20initio%20data.%20An%20implementation%20of%20our%20method%20and%0Ascripts%20for%20executing%20experiments%20are%20available%20at%0Ahttps%3A//github.com/nec-research/PICPS-ML4Sci.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05215v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics-Informed%2520Weakly%2520Supervised%2520Learning%2520for%2520Interatomic%2520Potentials%26entry.906535625%3DMakoto%2520Takamoto%2520and%2520Viktor%2520Zaverkin%2520and%2520Mathias%2520Niepert%26entry.1292438233%3D%2520%2520Machine%2520learning%2520plays%2520an%2520increasingly%2520important%2520role%2520in%2520computational%250Achemistry%2520and%2520materials%2520science%252C%2520complementing%2520computationally%2520intensive%2520ab%250Ainitio%2520and%2520first-principles%2520methods.%2520Despite%2520their%2520utility%252C%2520machine-learning%250Amodels%2520often%2520lack%2520generalization%2520capability%2520and%2520robustness%2520during%2520atomistic%250Asimulations%252C%2520yielding%2520unphysical%2520energy%2520and%2520force%2520predictions%2520that%2520hinder%2520their%250Areal-world%2520applications.%2520We%2520address%2520this%2520challenge%2520by%2520introducing%2520a%250Aphysics-informed%252C%2520weakly%2520supervised%2520approach%2520for%2520training%2520machine-learned%250Ainteratomic%2520potentials%2520%2528MLIPs%2529.%2520We%2520introduce%2520two%2520novel%2520loss%2520functions%252C%250Aextrapolating%2520the%2520potential%2520energy%2520via%2520a%2520Taylor%2520expansion%2520and%2520using%2520the%2520concept%250Aof%2520conservative%2520forces.%2520Our%2520approach%2520improves%2520the%2520accuracy%2520of%2520MLIPs%2520applied%2520to%250Atraining%2520tasks%2520with%2520sparse%2520training%2520data%2520sets%2520and%2520reduces%2520the%2520need%2520for%250Apre-training%2520computationally%2520demanding%2520models%2520with%2520large%2520data%2520sets.%250AParticularly%252C%2520we%2520perform%2520extensive%2520experiments%2520demonstrating%2520reduced%2520energy%2520and%250Aforce%2520errors%2520--%2520often%2520lower%2520by%2520a%2520factor%2520of%2520two%2520--%2520for%2520various%2520baseline%2520models%250Aand%2520benchmark%2520data%2520sets.%2520Moreover%252C%2520we%2520demonstrate%2520improved%2520robustness%2520during%2520MD%250Asimulations%2520of%2520the%2520MLIP%2520models%2520trained%2520with%2520the%2520proposed%2520weakly%2520supervised%250Aloss.%2520Finally%252C%2520our%2520approach%2520improves%2520the%2520fine-tuning%2520of%2520foundation%2520models%2520on%250Asparse%252C%2520highly%2520accurate%2520ab%2520initio%2520data.%2520An%2520implementation%2520of%2520our%2520method%2520and%250Ascripts%2520for%2520executing%2520experiments%2520are%2520available%2520at%250Ahttps%253A//github.com/nec-research/PICPS-ML4Sci.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05215v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-Informed%20Weakly%20Supervised%20Learning%20for%20Interatomic%20Potentials&entry.906535625=Makoto%20Takamoto%20and%20Viktor%20Zaverkin%20and%20Mathias%20Niepert&entry.1292438233=%20%20Machine%20learning%20plays%20an%20increasingly%20important%20role%20in%20computational%0Achemistry%20and%20materials%20science%2C%20complementing%20computationally%20intensive%20ab%0Ainitio%20and%20first-principles%20methods.%20Despite%20their%20utility%2C%20machine-learning%0Amodels%20often%20lack%20generalization%20capability%20and%20robustness%20during%20atomistic%0Asimulations%2C%20yielding%20unphysical%20energy%20and%20force%20predictions%20that%20hinder%20their%0Areal-world%20applications.%20We%20address%20this%20challenge%20by%20introducing%20a%0Aphysics-informed%2C%20weakly%20supervised%20approach%20for%20training%20machine-learned%0Ainteratomic%20potentials%20%28MLIPs%29.%20We%20introduce%20two%20novel%20loss%20functions%2C%0Aextrapolating%20the%20potential%20energy%20via%20a%20Taylor%20expansion%20and%20using%20the%20concept%0Aof%20conservative%20forces.%20Our%20approach%20improves%20the%20accuracy%20of%20MLIPs%20applied%20to%0Atraining%20tasks%20with%20sparse%20training%20data%20sets%20and%20reduces%20the%20need%20for%0Apre-training%20computationally%20demanding%20models%20with%20large%20data%20sets.%0AParticularly%2C%20we%20perform%20extensive%20experiments%20demonstrating%20reduced%20energy%20and%0Aforce%20errors%20--%20often%20lower%20by%20a%20factor%20of%20two%20--%20for%20various%20baseline%20models%0Aand%20benchmark%20data%20sets.%20Moreover%2C%20we%20demonstrate%20improved%20robustness%20during%20MD%0Asimulations%20of%20the%20MLIP%20models%20trained%20with%20the%20proposed%20weakly%20supervised%0Aloss.%20Finally%2C%20our%20approach%20improves%20the%20fine-tuning%20of%20foundation%20models%20on%0Asparse%2C%20highly%20accurate%20ab%20initio%20data.%20An%20implementation%20of%20our%20method%20and%0Ascripts%20for%20executing%20experiments%20are%20available%20at%0Ahttps%3A//github.com/nec-research/PICPS-ML4Sci.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05215v2&entry.124074799=Read"},
{"title": "Towards Dataset Copyright Evasion Attack against Personalized\n  Text-to-Image Diffusion Models", "author": "Kuofeng Gao and Yufei Zhu and Yiming Li and Jiawang Bai and Yong Yang and Zhifeng Li and Shu-Tao Xia", "abstract": "  Text-to-image (T2I) diffusion models have rapidly advanced, enabling\nhigh-quality image generation conditioned on textual prompts. However, the\ngrowing trend of fine-tuning pre-trained models for personalization raises\nserious concerns about unauthorized dataset usage. To combat this, dataset\nownership verification (DOV) has emerged as a solution, embedding watermarks\ninto the fine-tuning datasets using backdoor techniques. These watermarks\nremain inactive under benign samples but produce owner-specified outputs when\ntriggered. Despite the promise of DOV for T2I diffusion models, its robustness\nagainst copyright evasion attacks (CEA) remains unexplored. In this paper, we\nexplore how attackers can bypass these mechanisms through CEA, allowing models\nto circumvent watermarks even when trained on watermarked datasets. We propose\nthe first copyright evasion attack (i.e., CEAT2I) specifically designed to\nundermine DOV in T2I diffusion models. Concretely, our CEAT2I comprises three\nstages: watermarked sample detection, trigger identification, and efficient\nwatermark mitigation. A key insight driving our approach is that T2I models\nexhibit faster convergence on watermarked samples during the fine-tuning,\nevident through intermediate feature deviation. Leveraging this, CEAT2I can\nreliably detect the watermarked samples. Then, we iteratively ablate tokens\nfrom the prompts of detected watermarked samples and monitor shifts in\nintermediate features to pinpoint the exact trigger tokens. Finally, we adopt a\nclosed-form concept erasure method to remove the injected watermark. Extensive\nexperiments show that our CEAT2I effectively evades DOV mechanisms while\npreserving model performance.\n", "link": "http://arxiv.org/abs/2505.02824v1", "date": "2025-05-05", "relevancy": 1.7363, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5896}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5803}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.55}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Dataset%20Copyright%20Evasion%20Attack%20against%20Personalized%0A%20%20Text-to-Image%20Diffusion%20Models&body=Title%3A%20Towards%20Dataset%20Copyright%20Evasion%20Attack%20against%20Personalized%0A%20%20Text-to-Image%20Diffusion%20Models%0AAuthor%3A%20Kuofeng%20Gao%20and%20Yufei%20Zhu%20and%20Yiming%20Li%20and%20Jiawang%20Bai%20and%20Yong%20Yang%20and%20Zhifeng%20Li%20and%20Shu-Tao%20Xia%0AAbstract%3A%20%20%20Text-to-image%20%28T2I%29%20diffusion%20models%20have%20rapidly%20advanced%2C%20enabling%0Ahigh-quality%20image%20generation%20conditioned%20on%20textual%20prompts.%20However%2C%20the%0Agrowing%20trend%20of%20fine-tuning%20pre-trained%20models%20for%20personalization%20raises%0Aserious%20concerns%20about%20unauthorized%20dataset%20usage.%20To%20combat%20this%2C%20dataset%0Aownership%20verification%20%28DOV%29%20has%20emerged%20as%20a%20solution%2C%20embedding%20watermarks%0Ainto%20the%20fine-tuning%20datasets%20using%20backdoor%20techniques.%20These%20watermarks%0Aremain%20inactive%20under%20benign%20samples%20but%20produce%20owner-specified%20outputs%20when%0Atriggered.%20Despite%20the%20promise%20of%20DOV%20for%20T2I%20diffusion%20models%2C%20its%20robustness%0Aagainst%20copyright%20evasion%20attacks%20%28CEA%29%20remains%20unexplored.%20In%20this%20paper%2C%20we%0Aexplore%20how%20attackers%20can%20bypass%20these%20mechanisms%20through%20CEA%2C%20allowing%20models%0Ato%20circumvent%20watermarks%20even%20when%20trained%20on%20watermarked%20datasets.%20We%20propose%0Athe%20first%20copyright%20evasion%20attack%20%28i.e.%2C%20CEAT2I%29%20specifically%20designed%20to%0Aundermine%20DOV%20in%20T2I%20diffusion%20models.%20Concretely%2C%20our%20CEAT2I%20comprises%20three%0Astages%3A%20watermarked%20sample%20detection%2C%20trigger%20identification%2C%20and%20efficient%0Awatermark%20mitigation.%20A%20key%20insight%20driving%20our%20approach%20is%20that%20T2I%20models%0Aexhibit%20faster%20convergence%20on%20watermarked%20samples%20during%20the%20fine-tuning%2C%0Aevident%20through%20intermediate%20feature%20deviation.%20Leveraging%20this%2C%20CEAT2I%20can%0Areliably%20detect%20the%20watermarked%20samples.%20Then%2C%20we%20iteratively%20ablate%20tokens%0Afrom%20the%20prompts%20of%20detected%20watermarked%20samples%20and%20monitor%20shifts%20in%0Aintermediate%20features%20to%20pinpoint%20the%20exact%20trigger%20tokens.%20Finally%2C%20we%20adopt%20a%0Aclosed-form%20concept%20erasure%20method%20to%20remove%20the%20injected%20watermark.%20Extensive%0Aexperiments%20show%20that%20our%20CEAT2I%20effectively%20evades%20DOV%20mechanisms%20while%0Apreserving%20model%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02824v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Dataset%2520Copyright%2520Evasion%2520Attack%2520against%2520Personalized%250A%2520%2520Text-to-Image%2520Diffusion%2520Models%26entry.906535625%3DKuofeng%2520Gao%2520and%2520Yufei%2520Zhu%2520and%2520Yiming%2520Li%2520and%2520Jiawang%2520Bai%2520and%2520Yong%2520Yang%2520and%2520Zhifeng%2520Li%2520and%2520Shu-Tao%2520Xia%26entry.1292438233%3D%2520%2520Text-to-image%2520%2528T2I%2529%2520diffusion%2520models%2520have%2520rapidly%2520advanced%252C%2520enabling%250Ahigh-quality%2520image%2520generation%2520conditioned%2520on%2520textual%2520prompts.%2520However%252C%2520the%250Agrowing%2520trend%2520of%2520fine-tuning%2520pre-trained%2520models%2520for%2520personalization%2520raises%250Aserious%2520concerns%2520about%2520unauthorized%2520dataset%2520usage.%2520To%2520combat%2520this%252C%2520dataset%250Aownership%2520verification%2520%2528DOV%2529%2520has%2520emerged%2520as%2520a%2520solution%252C%2520embedding%2520watermarks%250Ainto%2520the%2520fine-tuning%2520datasets%2520using%2520backdoor%2520techniques.%2520These%2520watermarks%250Aremain%2520inactive%2520under%2520benign%2520samples%2520but%2520produce%2520owner-specified%2520outputs%2520when%250Atriggered.%2520Despite%2520the%2520promise%2520of%2520DOV%2520for%2520T2I%2520diffusion%2520models%252C%2520its%2520robustness%250Aagainst%2520copyright%2520evasion%2520attacks%2520%2528CEA%2529%2520remains%2520unexplored.%2520In%2520this%2520paper%252C%2520we%250Aexplore%2520how%2520attackers%2520can%2520bypass%2520these%2520mechanisms%2520through%2520CEA%252C%2520allowing%2520models%250Ato%2520circumvent%2520watermarks%2520even%2520when%2520trained%2520on%2520watermarked%2520datasets.%2520We%2520propose%250Athe%2520first%2520copyright%2520evasion%2520attack%2520%2528i.e.%252C%2520CEAT2I%2529%2520specifically%2520designed%2520to%250Aundermine%2520DOV%2520in%2520T2I%2520diffusion%2520models.%2520Concretely%252C%2520our%2520CEAT2I%2520comprises%2520three%250Astages%253A%2520watermarked%2520sample%2520detection%252C%2520trigger%2520identification%252C%2520and%2520efficient%250Awatermark%2520mitigation.%2520A%2520key%2520insight%2520driving%2520our%2520approach%2520is%2520that%2520T2I%2520models%250Aexhibit%2520faster%2520convergence%2520on%2520watermarked%2520samples%2520during%2520the%2520fine-tuning%252C%250Aevident%2520through%2520intermediate%2520feature%2520deviation.%2520Leveraging%2520this%252C%2520CEAT2I%2520can%250Areliably%2520detect%2520the%2520watermarked%2520samples.%2520Then%252C%2520we%2520iteratively%2520ablate%2520tokens%250Afrom%2520the%2520prompts%2520of%2520detected%2520watermarked%2520samples%2520and%2520monitor%2520shifts%2520in%250Aintermediate%2520features%2520to%2520pinpoint%2520the%2520exact%2520trigger%2520tokens.%2520Finally%252C%2520we%2520adopt%2520a%250Aclosed-form%2520concept%2520erasure%2520method%2520to%2520remove%2520the%2520injected%2520watermark.%2520Extensive%250Aexperiments%2520show%2520that%2520our%2520CEAT2I%2520effectively%2520evades%2520DOV%2520mechanisms%2520while%250Apreserving%2520model%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02824v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Dataset%20Copyright%20Evasion%20Attack%20against%20Personalized%0A%20%20Text-to-Image%20Diffusion%20Models&entry.906535625=Kuofeng%20Gao%20and%20Yufei%20Zhu%20and%20Yiming%20Li%20and%20Jiawang%20Bai%20and%20Yong%20Yang%20and%20Zhifeng%20Li%20and%20Shu-Tao%20Xia&entry.1292438233=%20%20Text-to-image%20%28T2I%29%20diffusion%20models%20have%20rapidly%20advanced%2C%20enabling%0Ahigh-quality%20image%20generation%20conditioned%20on%20textual%20prompts.%20However%2C%20the%0Agrowing%20trend%20of%20fine-tuning%20pre-trained%20models%20for%20personalization%20raises%0Aserious%20concerns%20about%20unauthorized%20dataset%20usage.%20To%20combat%20this%2C%20dataset%0Aownership%20verification%20%28DOV%29%20has%20emerged%20as%20a%20solution%2C%20embedding%20watermarks%0Ainto%20the%20fine-tuning%20datasets%20using%20backdoor%20techniques.%20These%20watermarks%0Aremain%20inactive%20under%20benign%20samples%20but%20produce%20owner-specified%20outputs%20when%0Atriggered.%20Despite%20the%20promise%20of%20DOV%20for%20T2I%20diffusion%20models%2C%20its%20robustness%0Aagainst%20copyright%20evasion%20attacks%20%28CEA%29%20remains%20unexplored.%20In%20this%20paper%2C%20we%0Aexplore%20how%20attackers%20can%20bypass%20these%20mechanisms%20through%20CEA%2C%20allowing%20models%0Ato%20circumvent%20watermarks%20even%20when%20trained%20on%20watermarked%20datasets.%20We%20propose%0Athe%20first%20copyright%20evasion%20attack%20%28i.e.%2C%20CEAT2I%29%20specifically%20designed%20to%0Aundermine%20DOV%20in%20T2I%20diffusion%20models.%20Concretely%2C%20our%20CEAT2I%20comprises%20three%0Astages%3A%20watermarked%20sample%20detection%2C%20trigger%20identification%2C%20and%20efficient%0Awatermark%20mitigation.%20A%20key%20insight%20driving%20our%20approach%20is%20that%20T2I%20models%0Aexhibit%20faster%20convergence%20on%20watermarked%20samples%20during%20the%20fine-tuning%2C%0Aevident%20through%20intermediate%20feature%20deviation.%20Leveraging%20this%2C%20CEAT2I%20can%0Areliably%20detect%20the%20watermarked%20samples.%20Then%2C%20we%20iteratively%20ablate%20tokens%0Afrom%20the%20prompts%20of%20detected%20watermarked%20samples%20and%20monitor%20shifts%20in%0Aintermediate%20features%20to%20pinpoint%20the%20exact%20trigger%20tokens.%20Finally%2C%20we%20adopt%20a%0Aclosed-form%20concept%20erasure%20method%20to%20remove%20the%20injected%20watermark.%20Extensive%0Aexperiments%20show%20that%20our%20CEAT2I%20effectively%20evades%20DOV%20mechanisms%20while%0Apreserving%20model%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02824v1&entry.124074799=Read"},
{"title": "Unified Multimodal Understanding and Generation Models: Advances,\n  Challenges, and Opportunities", "author": "Xinjie Zhang and Jintao Guo and Shanshan Zhao and Minghao Fu and Lunhao Duan and Guo-Hua Wang and Qing-Guo Chen and Zhao Xu and Weihua Luo and Kaifu Zhang", "abstract": "  Recent years have seen remarkable progress in both multimodal understanding\nmodels and image generation models. Despite their respective successes, these\ntwo domains have evolved independently, leading to distinct architectural\nparadigms: While autoregressive-based architectures have dominated multimodal\nunderstanding, diffusion-based models have become the cornerstone of image\ngeneration. Recently, there has been growing interest in developing unified\nframeworks that integrate these tasks. The emergence of GPT-4o's new\ncapabilities exemplifies this trend, highlighting the potential for\nunification. However, the architectural differences between the two domains\npose significant challenges. To provide a clear overview of current efforts\ntoward unification, we present a comprehensive survey aimed at guiding future\nresearch. First, we introduce the foundational concepts and recent advancements\nin multimodal understanding and text-to-image generation models. Next, we\nreview existing unified models, categorizing them into three main architectural\nparadigms: diffusion-based, autoregressive-based, and hybrid approaches that\nfuse autoregressive and diffusion mechanisms. For each category, we analyze the\nstructural designs and innovations introduced by related works. Additionally,\nwe compile datasets and benchmarks tailored for unified models, offering\nresources for future exploration. Finally, we discuss the key challenges facing\nthis nascent field, including tokenization strategy, cross-modal attention, and\ndata. As this area is still in its early stages, we anticipate rapid\nadvancements and will regularly update this survey. Our goal is to inspire\nfurther research and provide a valuable reference for the community. The\nreferences associated with this survey will be available on GitHub soon.\n", "link": "http://arxiv.org/abs/2505.02567v1", "date": "2025-05-05", "relevancy": 1.752, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6032}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5866}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20Multimodal%20Understanding%20and%20Generation%20Models%3A%20Advances%2C%0A%20%20Challenges%2C%20and%20Opportunities&body=Title%3A%20Unified%20Multimodal%20Understanding%20and%20Generation%20Models%3A%20Advances%2C%0A%20%20Challenges%2C%20and%20Opportunities%0AAuthor%3A%20Xinjie%20Zhang%20and%20Jintao%20Guo%20and%20Shanshan%20Zhao%20and%20Minghao%20Fu%20and%20Lunhao%20Duan%20and%20Guo-Hua%20Wang%20and%20Qing-Guo%20Chen%20and%20Zhao%20Xu%20and%20Weihua%20Luo%20and%20Kaifu%20Zhang%0AAbstract%3A%20%20%20Recent%20years%20have%20seen%20remarkable%20progress%20in%20both%20multimodal%20understanding%0Amodels%20and%20image%20generation%20models.%20Despite%20their%20respective%20successes%2C%20these%0Atwo%20domains%20have%20evolved%20independently%2C%20leading%20to%20distinct%20architectural%0Aparadigms%3A%20While%20autoregressive-based%20architectures%20have%20dominated%20multimodal%0Aunderstanding%2C%20diffusion-based%20models%20have%20become%20the%20cornerstone%20of%20image%0Ageneration.%20Recently%2C%20there%20has%20been%20growing%20interest%20in%20developing%20unified%0Aframeworks%20that%20integrate%20these%20tasks.%20The%20emergence%20of%20GPT-4o%27s%20new%0Acapabilities%20exemplifies%20this%20trend%2C%20highlighting%20the%20potential%20for%0Aunification.%20However%2C%20the%20architectural%20differences%20between%20the%20two%20domains%0Apose%20significant%20challenges.%20To%20provide%20a%20clear%20overview%20of%20current%20efforts%0Atoward%20unification%2C%20we%20present%20a%20comprehensive%20survey%20aimed%20at%20guiding%20future%0Aresearch.%20First%2C%20we%20introduce%20the%20foundational%20concepts%20and%20recent%20advancements%0Ain%20multimodal%20understanding%20and%20text-to-image%20generation%20models.%20Next%2C%20we%0Areview%20existing%20unified%20models%2C%20categorizing%20them%20into%20three%20main%20architectural%0Aparadigms%3A%20diffusion-based%2C%20autoregressive-based%2C%20and%20hybrid%20approaches%20that%0Afuse%20autoregressive%20and%20diffusion%20mechanisms.%20For%20each%20category%2C%20we%20analyze%20the%0Astructural%20designs%20and%20innovations%20introduced%20by%20related%20works.%20Additionally%2C%0Awe%20compile%20datasets%20and%20benchmarks%20tailored%20for%20unified%20models%2C%20offering%0Aresources%20for%20future%20exploration.%20Finally%2C%20we%20discuss%20the%20key%20challenges%20facing%0Athis%20nascent%20field%2C%20including%20tokenization%20strategy%2C%20cross-modal%20attention%2C%20and%0Adata.%20As%20this%20area%20is%20still%20in%20its%20early%20stages%2C%20we%20anticipate%20rapid%0Aadvancements%20and%20will%20regularly%20update%20this%20survey.%20Our%20goal%20is%20to%20inspire%0Afurther%20research%20and%20provide%20a%20valuable%20reference%20for%20the%20community.%20The%0Areferences%20associated%20with%20this%20survey%20will%20be%20available%20on%20GitHub%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02567v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520Multimodal%2520Understanding%2520and%2520Generation%2520Models%253A%2520Advances%252C%250A%2520%2520Challenges%252C%2520and%2520Opportunities%26entry.906535625%3DXinjie%2520Zhang%2520and%2520Jintao%2520Guo%2520and%2520Shanshan%2520Zhao%2520and%2520Minghao%2520Fu%2520and%2520Lunhao%2520Duan%2520and%2520Guo-Hua%2520Wang%2520and%2520Qing-Guo%2520Chen%2520and%2520Zhao%2520Xu%2520and%2520Weihua%2520Luo%2520and%2520Kaifu%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520years%2520have%2520seen%2520remarkable%2520progress%2520in%2520both%2520multimodal%2520understanding%250Amodels%2520and%2520image%2520generation%2520models.%2520Despite%2520their%2520respective%2520successes%252C%2520these%250Atwo%2520domains%2520have%2520evolved%2520independently%252C%2520leading%2520to%2520distinct%2520architectural%250Aparadigms%253A%2520While%2520autoregressive-based%2520architectures%2520have%2520dominated%2520multimodal%250Aunderstanding%252C%2520diffusion-based%2520models%2520have%2520become%2520the%2520cornerstone%2520of%2520image%250Ageneration.%2520Recently%252C%2520there%2520has%2520been%2520growing%2520interest%2520in%2520developing%2520unified%250Aframeworks%2520that%2520integrate%2520these%2520tasks.%2520The%2520emergence%2520of%2520GPT-4o%2527s%2520new%250Acapabilities%2520exemplifies%2520this%2520trend%252C%2520highlighting%2520the%2520potential%2520for%250Aunification.%2520However%252C%2520the%2520architectural%2520differences%2520between%2520the%2520two%2520domains%250Apose%2520significant%2520challenges.%2520To%2520provide%2520a%2520clear%2520overview%2520of%2520current%2520efforts%250Atoward%2520unification%252C%2520we%2520present%2520a%2520comprehensive%2520survey%2520aimed%2520at%2520guiding%2520future%250Aresearch.%2520First%252C%2520we%2520introduce%2520the%2520foundational%2520concepts%2520and%2520recent%2520advancements%250Ain%2520multimodal%2520understanding%2520and%2520text-to-image%2520generation%2520models.%2520Next%252C%2520we%250Areview%2520existing%2520unified%2520models%252C%2520categorizing%2520them%2520into%2520three%2520main%2520architectural%250Aparadigms%253A%2520diffusion-based%252C%2520autoregressive-based%252C%2520and%2520hybrid%2520approaches%2520that%250Afuse%2520autoregressive%2520and%2520diffusion%2520mechanisms.%2520For%2520each%2520category%252C%2520we%2520analyze%2520the%250Astructural%2520designs%2520and%2520innovations%2520introduced%2520by%2520related%2520works.%2520Additionally%252C%250Awe%2520compile%2520datasets%2520and%2520benchmarks%2520tailored%2520for%2520unified%2520models%252C%2520offering%250Aresources%2520for%2520future%2520exploration.%2520Finally%252C%2520we%2520discuss%2520the%2520key%2520challenges%2520facing%250Athis%2520nascent%2520field%252C%2520including%2520tokenization%2520strategy%252C%2520cross-modal%2520attention%252C%2520and%250Adata.%2520As%2520this%2520area%2520is%2520still%2520in%2520its%2520early%2520stages%252C%2520we%2520anticipate%2520rapid%250Aadvancements%2520and%2520will%2520regularly%2520update%2520this%2520survey.%2520Our%2520goal%2520is%2520to%2520inspire%250Afurther%2520research%2520and%2520provide%2520a%2520valuable%2520reference%2520for%2520the%2520community.%2520The%250Areferences%2520associated%2520with%2520this%2520survey%2520will%2520be%2520available%2520on%2520GitHub%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02567v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Multimodal%20Understanding%20and%20Generation%20Models%3A%20Advances%2C%0A%20%20Challenges%2C%20and%20Opportunities&entry.906535625=Xinjie%20Zhang%20and%20Jintao%20Guo%20and%20Shanshan%20Zhao%20and%20Minghao%20Fu%20and%20Lunhao%20Duan%20and%20Guo-Hua%20Wang%20and%20Qing-Guo%20Chen%20and%20Zhao%20Xu%20and%20Weihua%20Luo%20and%20Kaifu%20Zhang&entry.1292438233=%20%20Recent%20years%20have%20seen%20remarkable%20progress%20in%20both%20multimodal%20understanding%0Amodels%20and%20image%20generation%20models.%20Despite%20their%20respective%20successes%2C%20these%0Atwo%20domains%20have%20evolved%20independently%2C%20leading%20to%20distinct%20architectural%0Aparadigms%3A%20While%20autoregressive-based%20architectures%20have%20dominated%20multimodal%0Aunderstanding%2C%20diffusion-based%20models%20have%20become%20the%20cornerstone%20of%20image%0Ageneration.%20Recently%2C%20there%20has%20been%20growing%20interest%20in%20developing%20unified%0Aframeworks%20that%20integrate%20these%20tasks.%20The%20emergence%20of%20GPT-4o%27s%20new%0Acapabilities%20exemplifies%20this%20trend%2C%20highlighting%20the%20potential%20for%0Aunification.%20However%2C%20the%20architectural%20differences%20between%20the%20two%20domains%0Apose%20significant%20challenges.%20To%20provide%20a%20clear%20overview%20of%20current%20efforts%0Atoward%20unification%2C%20we%20present%20a%20comprehensive%20survey%20aimed%20at%20guiding%20future%0Aresearch.%20First%2C%20we%20introduce%20the%20foundational%20concepts%20and%20recent%20advancements%0Ain%20multimodal%20understanding%20and%20text-to-image%20generation%20models.%20Next%2C%20we%0Areview%20existing%20unified%20models%2C%20categorizing%20them%20into%20three%20main%20architectural%0Aparadigms%3A%20diffusion-based%2C%20autoregressive-based%2C%20and%20hybrid%20approaches%20that%0Afuse%20autoregressive%20and%20diffusion%20mechanisms.%20For%20each%20category%2C%20we%20analyze%20the%0Astructural%20designs%20and%20innovations%20introduced%20by%20related%20works.%20Additionally%2C%0Awe%20compile%20datasets%20and%20benchmarks%20tailored%20for%20unified%20models%2C%20offering%0Aresources%20for%20future%20exploration.%20Finally%2C%20we%20discuss%20the%20key%20challenges%20facing%0Athis%20nascent%20field%2C%20including%20tokenization%20strategy%2C%20cross-modal%20attention%2C%20and%0Adata.%20As%20this%20area%20is%20still%20in%20its%20early%20stages%2C%20we%20anticipate%20rapid%0Aadvancements%20and%20will%20regularly%20update%20this%20survey.%20Our%20goal%20is%20to%20inspire%0Afurther%20research%20and%20provide%20a%20valuable%20reference%20for%20the%20community.%20The%0Areferences%20associated%20with%20this%20survey%20will%20be%20available%20on%20GitHub%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02567v1&entry.124074799=Read"},
{"title": "Observability conditions for neural state-space models with eigenvalues\n  and their roots of unity", "author": "Andrew Gracyk", "abstract": "  We operate through the lens of ordinary differential equations and control\ntheory to study the concept of observability in the context of neural\nstate-space models and the Mamba architecture. We develop strategies to enforce\nobservability, which are tailored to a learning context, specifically where the\nhidden states are learnable at initial time, in conjunction to over its\ncontinuum, and high-dimensional. We also highlight our methods emphasize\neigenvalues, roots of unity, or both. Our methods effectuate computational\nefficiency when enforcing observability, sometimes at great scale. We formulate\nobservability conditions in machine learning based on classical control theory\nand discuss their computational complexity. Our nontrivial results are\nfivefold. We discuss observability through the use of permutations in neural\napplications with learnable matrices without high precision. We present two\nresults built upon the Fourier transform that effect observability with high\nprobability up to the randomness in the learning. These results are worked with\nthe interplay of representations in Fourier space and their eigenstructure,\nnonlinear mappings, and the observability matrix. We present a result for Mamba\nthat is similar to a Hautus-type condition, but instead employs an argument\nusing a Vandermonde matrix instead of eigenvectors. Our final result is a\nshared-parameter construction of the Mamba system, which is computationally\nefficient in high exponentiation. We develop a training algorithm with this\ncoupling, showing it satisfies a Robbins-Monro condition under certain\northogonality, while a more classical training procedure fails to satisfy a\ncontraction with high Lipschitz constant.\n", "link": "http://arxiv.org/abs/2504.15758v2", "date": "2025-05-05", "relevancy": 1.4244, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5069}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4682}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4646}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Observability%20conditions%20for%20neural%20state-space%20models%20with%20eigenvalues%0A%20%20and%20their%20roots%20of%20unity&body=Title%3A%20Observability%20conditions%20for%20neural%20state-space%20models%20with%20eigenvalues%0A%20%20and%20their%20roots%20of%20unity%0AAuthor%3A%20Andrew%20Gracyk%0AAbstract%3A%20%20%20We%20operate%20through%20the%20lens%20of%20ordinary%20differential%20equations%20and%20control%0Atheory%20to%20study%20the%20concept%20of%20observability%20in%20the%20context%20of%20neural%0Astate-space%20models%20and%20the%20Mamba%20architecture.%20We%20develop%20strategies%20to%20enforce%0Aobservability%2C%20which%20are%20tailored%20to%20a%20learning%20context%2C%20specifically%20where%20the%0Ahidden%20states%20are%20learnable%20at%20initial%20time%2C%20in%20conjunction%20to%20over%20its%0Acontinuum%2C%20and%20high-dimensional.%20We%20also%20highlight%20our%20methods%20emphasize%0Aeigenvalues%2C%20roots%20of%20unity%2C%20or%20both.%20Our%20methods%20effectuate%20computational%0Aefficiency%20when%20enforcing%20observability%2C%20sometimes%20at%20great%20scale.%20We%20formulate%0Aobservability%20conditions%20in%20machine%20learning%20based%20on%20classical%20control%20theory%0Aand%20discuss%20their%20computational%20complexity.%20Our%20nontrivial%20results%20are%0Afivefold.%20We%20discuss%20observability%20through%20the%20use%20of%20permutations%20in%20neural%0Aapplications%20with%20learnable%20matrices%20without%20high%20precision.%20We%20present%20two%0Aresults%20built%20upon%20the%20Fourier%20transform%20that%20effect%20observability%20with%20high%0Aprobability%20up%20to%20the%20randomness%20in%20the%20learning.%20These%20results%20are%20worked%20with%0Athe%20interplay%20of%20representations%20in%20Fourier%20space%20and%20their%20eigenstructure%2C%0Anonlinear%20mappings%2C%20and%20the%20observability%20matrix.%20We%20present%20a%20result%20for%20Mamba%0Athat%20is%20similar%20to%20a%20Hautus-type%20condition%2C%20but%20instead%20employs%20an%20argument%0Ausing%20a%20Vandermonde%20matrix%20instead%20of%20eigenvectors.%20Our%20final%20result%20is%20a%0Ashared-parameter%20construction%20of%20the%20Mamba%20system%2C%20which%20is%20computationally%0Aefficient%20in%20high%20exponentiation.%20We%20develop%20a%20training%20algorithm%20with%20this%0Acoupling%2C%20showing%20it%20satisfies%20a%20Robbins-Monro%20condition%20under%20certain%0Aorthogonality%2C%20while%20a%20more%20classical%20training%20procedure%20fails%20to%20satisfy%20a%0Acontraction%20with%20high%20Lipschitz%20constant.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15758v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObservability%2520conditions%2520for%2520neural%2520state-space%2520models%2520with%2520eigenvalues%250A%2520%2520and%2520their%2520roots%2520of%2520unity%26entry.906535625%3DAndrew%2520Gracyk%26entry.1292438233%3D%2520%2520We%2520operate%2520through%2520the%2520lens%2520of%2520ordinary%2520differential%2520equations%2520and%2520control%250Atheory%2520to%2520study%2520the%2520concept%2520of%2520observability%2520in%2520the%2520context%2520of%2520neural%250Astate-space%2520models%2520and%2520the%2520Mamba%2520architecture.%2520We%2520develop%2520strategies%2520to%2520enforce%250Aobservability%252C%2520which%2520are%2520tailored%2520to%2520a%2520learning%2520context%252C%2520specifically%2520where%2520the%250Ahidden%2520states%2520are%2520learnable%2520at%2520initial%2520time%252C%2520in%2520conjunction%2520to%2520over%2520its%250Acontinuum%252C%2520and%2520high-dimensional.%2520We%2520also%2520highlight%2520our%2520methods%2520emphasize%250Aeigenvalues%252C%2520roots%2520of%2520unity%252C%2520or%2520both.%2520Our%2520methods%2520effectuate%2520computational%250Aefficiency%2520when%2520enforcing%2520observability%252C%2520sometimes%2520at%2520great%2520scale.%2520We%2520formulate%250Aobservability%2520conditions%2520in%2520machine%2520learning%2520based%2520on%2520classical%2520control%2520theory%250Aand%2520discuss%2520their%2520computational%2520complexity.%2520Our%2520nontrivial%2520results%2520are%250Afivefold.%2520We%2520discuss%2520observability%2520through%2520the%2520use%2520of%2520permutations%2520in%2520neural%250Aapplications%2520with%2520learnable%2520matrices%2520without%2520high%2520precision.%2520We%2520present%2520two%250Aresults%2520built%2520upon%2520the%2520Fourier%2520transform%2520that%2520effect%2520observability%2520with%2520high%250Aprobability%2520up%2520to%2520the%2520randomness%2520in%2520the%2520learning.%2520These%2520results%2520are%2520worked%2520with%250Athe%2520interplay%2520of%2520representations%2520in%2520Fourier%2520space%2520and%2520their%2520eigenstructure%252C%250Anonlinear%2520mappings%252C%2520and%2520the%2520observability%2520matrix.%2520We%2520present%2520a%2520result%2520for%2520Mamba%250Athat%2520is%2520similar%2520to%2520a%2520Hautus-type%2520condition%252C%2520but%2520instead%2520employs%2520an%2520argument%250Ausing%2520a%2520Vandermonde%2520matrix%2520instead%2520of%2520eigenvectors.%2520Our%2520final%2520result%2520is%2520a%250Ashared-parameter%2520construction%2520of%2520the%2520Mamba%2520system%252C%2520which%2520is%2520computationally%250Aefficient%2520in%2520high%2520exponentiation.%2520We%2520develop%2520a%2520training%2520algorithm%2520with%2520this%250Acoupling%252C%2520showing%2520it%2520satisfies%2520a%2520Robbins-Monro%2520condition%2520under%2520certain%250Aorthogonality%252C%2520while%2520a%2520more%2520classical%2520training%2520procedure%2520fails%2520to%2520satisfy%2520a%250Acontraction%2520with%2520high%2520Lipschitz%2520constant.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15758v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Observability%20conditions%20for%20neural%20state-space%20models%20with%20eigenvalues%0A%20%20and%20their%20roots%20of%20unity&entry.906535625=Andrew%20Gracyk&entry.1292438233=%20%20We%20operate%20through%20the%20lens%20of%20ordinary%20differential%20equations%20and%20control%0Atheory%20to%20study%20the%20concept%20of%20observability%20in%20the%20context%20of%20neural%0Astate-space%20models%20and%20the%20Mamba%20architecture.%20We%20develop%20strategies%20to%20enforce%0Aobservability%2C%20which%20are%20tailored%20to%20a%20learning%20context%2C%20specifically%20where%20the%0Ahidden%20states%20are%20learnable%20at%20initial%20time%2C%20in%20conjunction%20to%20over%20its%0Acontinuum%2C%20and%20high-dimensional.%20We%20also%20highlight%20our%20methods%20emphasize%0Aeigenvalues%2C%20roots%20of%20unity%2C%20or%20both.%20Our%20methods%20effectuate%20computational%0Aefficiency%20when%20enforcing%20observability%2C%20sometimes%20at%20great%20scale.%20We%20formulate%0Aobservability%20conditions%20in%20machine%20learning%20based%20on%20classical%20control%20theory%0Aand%20discuss%20their%20computational%20complexity.%20Our%20nontrivial%20results%20are%0Afivefold.%20We%20discuss%20observability%20through%20the%20use%20of%20permutations%20in%20neural%0Aapplications%20with%20learnable%20matrices%20without%20high%20precision.%20We%20present%20two%0Aresults%20built%20upon%20the%20Fourier%20transform%20that%20effect%20observability%20with%20high%0Aprobability%20up%20to%20the%20randomness%20in%20the%20learning.%20These%20results%20are%20worked%20with%0Athe%20interplay%20of%20representations%20in%20Fourier%20space%20and%20their%20eigenstructure%2C%0Anonlinear%20mappings%2C%20and%20the%20observability%20matrix.%20We%20present%20a%20result%20for%20Mamba%0Athat%20is%20similar%20to%20a%20Hautus-type%20condition%2C%20but%20instead%20employs%20an%20argument%0Ausing%20a%20Vandermonde%20matrix%20instead%20of%20eigenvectors.%20Our%20final%20result%20is%20a%0Ashared-parameter%20construction%20of%20the%20Mamba%20system%2C%20which%20is%20computationally%0Aefficient%20in%20high%20exponentiation.%20We%20develop%20a%20training%20algorithm%20with%20this%0Acoupling%2C%20showing%20it%20satisfies%20a%20Robbins-Monro%20condition%20under%20certain%0Aorthogonality%2C%20while%20a%20more%20classical%20training%20procedure%20fails%20to%20satisfy%20a%0Acontraction%20with%20high%20Lipschitz%20constant.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15758v2&entry.124074799=Read"},
{"title": "PAC Learning is just Bipartite Matching (Sort of)", "author": "Shaddin Dughmi", "abstract": "  The main goal of this article is to convince you, the reader, that supervised\nlearning in the Probably Approximately Correct (PAC) model is closely related\nto -- of all things -- bipartite matching! En-route from PAC learning to\nbipartite matching, I will overview a particular transductive model of\nlearning, and associated one-inclusion graphs, which can be viewed as a\ngeneralization of some of the hat puzzles that are popular in recreational\nmathematics. Whereas this transductive model is far from new, it has recently\nseen a resurgence of interest as a tool for tackling deep questions in learning\ntheory. A secondary purpose of this article could be as a (biased) tutorial on\nthe connections between the PAC and transductive models of learning.\n", "link": "http://arxiv.org/abs/2502.00607v3", "date": "2025-05-05", "relevancy": 1.8409, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4666}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.46}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PAC%20Learning%20is%20just%20Bipartite%20Matching%20%28Sort%20of%29&body=Title%3A%20PAC%20Learning%20is%20just%20Bipartite%20Matching%20%28Sort%20of%29%0AAuthor%3A%20Shaddin%20Dughmi%0AAbstract%3A%20%20%20The%20main%20goal%20of%20this%20article%20is%20to%20convince%20you%2C%20the%20reader%2C%20that%20supervised%0Alearning%20in%20the%20Probably%20Approximately%20Correct%20%28PAC%29%20model%20is%20closely%20related%0Ato%20--%20of%20all%20things%20--%20bipartite%20matching%21%20En-route%20from%20PAC%20learning%20to%0Abipartite%20matching%2C%20I%20will%20overview%20a%20particular%20transductive%20model%20of%0Alearning%2C%20and%20associated%20one-inclusion%20graphs%2C%20which%20can%20be%20viewed%20as%20a%0Ageneralization%20of%20some%20of%20the%20hat%20puzzles%20that%20are%20popular%20in%20recreational%0Amathematics.%20Whereas%20this%20transductive%20model%20is%20far%20from%20new%2C%20it%20has%20recently%0Aseen%20a%20resurgence%20of%20interest%20as%20a%20tool%20for%20tackling%20deep%20questions%20in%20learning%0Atheory.%20A%20secondary%20purpose%20of%20this%20article%20could%20be%20as%20a%20%28biased%29%20tutorial%20on%0Athe%20connections%20between%20the%20PAC%20and%20transductive%20models%20of%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.00607v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPAC%2520Learning%2520is%2520just%2520Bipartite%2520Matching%2520%2528Sort%2520of%2529%26entry.906535625%3DShaddin%2520Dughmi%26entry.1292438233%3D%2520%2520The%2520main%2520goal%2520of%2520this%2520article%2520is%2520to%2520convince%2520you%252C%2520the%2520reader%252C%2520that%2520supervised%250Alearning%2520in%2520the%2520Probably%2520Approximately%2520Correct%2520%2528PAC%2529%2520model%2520is%2520closely%2520related%250Ato%2520--%2520of%2520all%2520things%2520--%2520bipartite%2520matching%2521%2520En-route%2520from%2520PAC%2520learning%2520to%250Abipartite%2520matching%252C%2520I%2520will%2520overview%2520a%2520particular%2520transductive%2520model%2520of%250Alearning%252C%2520and%2520associated%2520one-inclusion%2520graphs%252C%2520which%2520can%2520be%2520viewed%2520as%2520a%250Ageneralization%2520of%2520some%2520of%2520the%2520hat%2520puzzles%2520that%2520are%2520popular%2520in%2520recreational%250Amathematics.%2520Whereas%2520this%2520transductive%2520model%2520is%2520far%2520from%2520new%252C%2520it%2520has%2520recently%250Aseen%2520a%2520resurgence%2520of%2520interest%2520as%2520a%2520tool%2520for%2520tackling%2520deep%2520questions%2520in%2520learning%250Atheory.%2520A%2520secondary%2520purpose%2520of%2520this%2520article%2520could%2520be%2520as%2520a%2520%2528biased%2529%2520tutorial%2520on%250Athe%2520connections%2520between%2520the%2520PAC%2520and%2520transductive%2520models%2520of%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.00607v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PAC%20Learning%20is%20just%20Bipartite%20Matching%20%28Sort%20of%29&entry.906535625=Shaddin%20Dughmi&entry.1292438233=%20%20The%20main%20goal%20of%20this%20article%20is%20to%20convince%20you%2C%20the%20reader%2C%20that%20supervised%0Alearning%20in%20the%20Probably%20Approximately%20Correct%20%28PAC%29%20model%20is%20closely%20related%0Ato%20--%20of%20all%20things%20--%20bipartite%20matching%21%20En-route%20from%20PAC%20learning%20to%0Abipartite%20matching%2C%20I%20will%20overview%20a%20particular%20transductive%20model%20of%0Alearning%2C%20and%20associated%20one-inclusion%20graphs%2C%20which%20can%20be%20viewed%20as%20a%0Ageneralization%20of%20some%20of%20the%20hat%20puzzles%20that%20are%20popular%20in%20recreational%0Amathematics.%20Whereas%20this%20transductive%20model%20is%20far%20from%20new%2C%20it%20has%20recently%0Aseen%20a%20resurgence%20of%20interest%20as%20a%20tool%20for%20tackling%20deep%20questions%20in%20learning%0Atheory.%20A%20secondary%20purpose%20of%20this%20article%20could%20be%20as%20a%20%28biased%29%20tutorial%20on%0Athe%20connections%20between%20the%20PAC%20and%20transductive%20models%20of%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.00607v3&entry.124074799=Read"},
{"title": "M3-Jepa: Multimodal Alignment via Multi-directional MoE based on the\n  JEPA framework", "author": "Hongyang Lei and Xiaolong Cheng and Dan Wang and Kun Fan and Qi Qin and Huazhen Huang and Yetao Wu and Qingqing Gu and Zhonglin Jiang and Yong Chen and Luo Ji", "abstract": "  Current multimodal alignment strategies primarily use single or unified\nmodality encoders, while optimizing the alignment on the original token space.\nSuch a framework is easy to implement and incorporate with the pretrained\nknowledge, but might result in information bias. To deal with such issues, the\njoint encoding predictive architecture (JEPA) learns the alignment loss on the\nlatent space, with a predictor to convert the input encoding to the output\nlatent space. However, the application of JEPA in multimodal scenarios is\nlimited so far. In this paper, we introduce M3-Jepa, a scalable multimodal\nalignment framework, with the predictor implemented by a multi-directional\nmixture of experts (MoE). We demonstrate the framework can maximize the mutual\ninformation with information theory derivations, by alternating the\noptimization between different uni-directional tasks. By thoroughly designed\nexperiments, we show that M3-Jepa can obtain state-of-the-art performance on\ndifferent modalities and tasks, generalize to unseen datasets and domains, and\nis computationally efficient in training and inference. Our study indicates\nthat M3-Jepa might provide a new paradigm to self-supervised learning and\nopen-world modeling.\n", "link": "http://arxiv.org/abs/2409.05929v4", "date": "2025-05-05", "relevancy": 1.6907, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5744}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5503}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20M3-Jepa%3A%20Multimodal%20Alignment%20via%20Multi-directional%20MoE%20based%20on%20the%0A%20%20JEPA%20framework&body=Title%3A%20M3-Jepa%3A%20Multimodal%20Alignment%20via%20Multi-directional%20MoE%20based%20on%20the%0A%20%20JEPA%20framework%0AAuthor%3A%20Hongyang%20Lei%20and%20Xiaolong%20Cheng%20and%20Dan%20Wang%20and%20Kun%20Fan%20and%20Qi%20Qin%20and%20Huazhen%20Huang%20and%20Yetao%20Wu%20and%20Qingqing%20Gu%20and%20Zhonglin%20Jiang%20and%20Yong%20Chen%20and%20Luo%20Ji%0AAbstract%3A%20%20%20Current%20multimodal%20alignment%20strategies%20primarily%20use%20single%20or%20unified%0Amodality%20encoders%2C%20while%20optimizing%20the%20alignment%20on%20the%20original%20token%20space.%0ASuch%20a%20framework%20is%20easy%20to%20implement%20and%20incorporate%20with%20the%20pretrained%0Aknowledge%2C%20but%20might%20result%20in%20information%20bias.%20To%20deal%20with%20such%20issues%2C%20the%0Ajoint%20encoding%20predictive%20architecture%20%28JEPA%29%20learns%20the%20alignment%20loss%20on%20the%0Alatent%20space%2C%20with%20a%20predictor%20to%20convert%20the%20input%20encoding%20to%20the%20output%0Alatent%20space.%20However%2C%20the%20application%20of%20JEPA%20in%20multimodal%20scenarios%20is%0Alimited%20so%20far.%20In%20this%20paper%2C%20we%20introduce%20M3-Jepa%2C%20a%20scalable%20multimodal%0Aalignment%20framework%2C%20with%20the%20predictor%20implemented%20by%20a%20multi-directional%0Amixture%20of%20experts%20%28MoE%29.%20We%20demonstrate%20the%20framework%20can%20maximize%20the%20mutual%0Ainformation%20with%20information%20theory%20derivations%2C%20by%20alternating%20the%0Aoptimization%20between%20different%20uni-directional%20tasks.%20By%20thoroughly%20designed%0Aexperiments%2C%20we%20show%20that%20M3-Jepa%20can%20obtain%20state-of-the-art%20performance%20on%0Adifferent%20modalities%20and%20tasks%2C%20generalize%20to%20unseen%20datasets%20and%20domains%2C%20and%0Ais%20computationally%20efficient%20in%20training%20and%20inference.%20Our%20study%20indicates%0Athat%20M3-Jepa%20might%20provide%20a%20new%20paradigm%20to%20self-supervised%20learning%20and%0Aopen-world%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05929v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DM3-Jepa%253A%2520Multimodal%2520Alignment%2520via%2520Multi-directional%2520MoE%2520based%2520on%2520the%250A%2520%2520JEPA%2520framework%26entry.906535625%3DHongyang%2520Lei%2520and%2520Xiaolong%2520Cheng%2520and%2520Dan%2520Wang%2520and%2520Kun%2520Fan%2520and%2520Qi%2520Qin%2520and%2520Huazhen%2520Huang%2520and%2520Yetao%2520Wu%2520and%2520Qingqing%2520Gu%2520and%2520Zhonglin%2520Jiang%2520and%2520Yong%2520Chen%2520and%2520Luo%2520Ji%26entry.1292438233%3D%2520%2520Current%2520multimodal%2520alignment%2520strategies%2520primarily%2520use%2520single%2520or%2520unified%250Amodality%2520encoders%252C%2520while%2520optimizing%2520the%2520alignment%2520on%2520the%2520original%2520token%2520space.%250ASuch%2520a%2520framework%2520is%2520easy%2520to%2520implement%2520and%2520incorporate%2520with%2520the%2520pretrained%250Aknowledge%252C%2520but%2520might%2520result%2520in%2520information%2520bias.%2520To%2520deal%2520with%2520such%2520issues%252C%2520the%250Ajoint%2520encoding%2520predictive%2520architecture%2520%2528JEPA%2529%2520learns%2520the%2520alignment%2520loss%2520on%2520the%250Alatent%2520space%252C%2520with%2520a%2520predictor%2520to%2520convert%2520the%2520input%2520encoding%2520to%2520the%2520output%250Alatent%2520space.%2520However%252C%2520the%2520application%2520of%2520JEPA%2520in%2520multimodal%2520scenarios%2520is%250Alimited%2520so%2520far.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520M3-Jepa%252C%2520a%2520scalable%2520multimodal%250Aalignment%2520framework%252C%2520with%2520the%2520predictor%2520implemented%2520by%2520a%2520multi-directional%250Amixture%2520of%2520experts%2520%2528MoE%2529.%2520We%2520demonstrate%2520the%2520framework%2520can%2520maximize%2520the%2520mutual%250Ainformation%2520with%2520information%2520theory%2520derivations%252C%2520by%2520alternating%2520the%250Aoptimization%2520between%2520different%2520uni-directional%2520tasks.%2520By%2520thoroughly%2520designed%250Aexperiments%252C%2520we%2520show%2520that%2520M3-Jepa%2520can%2520obtain%2520state-of-the-art%2520performance%2520on%250Adifferent%2520modalities%2520and%2520tasks%252C%2520generalize%2520to%2520unseen%2520datasets%2520and%2520domains%252C%2520and%250Ais%2520computationally%2520efficient%2520in%2520training%2520and%2520inference.%2520Our%2520study%2520indicates%250Athat%2520M3-Jepa%2520might%2520provide%2520a%2520new%2520paradigm%2520to%2520self-supervised%2520learning%2520and%250Aopen-world%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05929v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=M3-Jepa%3A%20Multimodal%20Alignment%20via%20Multi-directional%20MoE%20based%20on%20the%0A%20%20JEPA%20framework&entry.906535625=Hongyang%20Lei%20and%20Xiaolong%20Cheng%20and%20Dan%20Wang%20and%20Kun%20Fan%20and%20Qi%20Qin%20and%20Huazhen%20Huang%20and%20Yetao%20Wu%20and%20Qingqing%20Gu%20and%20Zhonglin%20Jiang%20and%20Yong%20Chen%20and%20Luo%20Ji&entry.1292438233=%20%20Current%20multimodal%20alignment%20strategies%20primarily%20use%20single%20or%20unified%0Amodality%20encoders%2C%20while%20optimizing%20the%20alignment%20on%20the%20original%20token%20space.%0ASuch%20a%20framework%20is%20easy%20to%20implement%20and%20incorporate%20with%20the%20pretrained%0Aknowledge%2C%20but%20might%20result%20in%20information%20bias.%20To%20deal%20with%20such%20issues%2C%20the%0Ajoint%20encoding%20predictive%20architecture%20%28JEPA%29%20learns%20the%20alignment%20loss%20on%20the%0Alatent%20space%2C%20with%20a%20predictor%20to%20convert%20the%20input%20encoding%20to%20the%20output%0Alatent%20space.%20However%2C%20the%20application%20of%20JEPA%20in%20multimodal%20scenarios%20is%0Alimited%20so%20far.%20In%20this%20paper%2C%20we%20introduce%20M3-Jepa%2C%20a%20scalable%20multimodal%0Aalignment%20framework%2C%20with%20the%20predictor%20implemented%20by%20a%20multi-directional%0Amixture%20of%20experts%20%28MoE%29.%20We%20demonstrate%20the%20framework%20can%20maximize%20the%20mutual%0Ainformation%20with%20information%20theory%20derivations%2C%20by%20alternating%20the%0Aoptimization%20between%20different%20uni-directional%20tasks.%20By%20thoroughly%20designed%0Aexperiments%2C%20we%20show%20that%20M3-Jepa%20can%20obtain%20state-of-the-art%20performance%20on%0Adifferent%20modalities%20and%20tasks%2C%20generalize%20to%20unseen%20datasets%20and%20domains%2C%20and%0Ais%20computationally%20efficient%20in%20training%20and%20inference.%20Our%20study%20indicates%0Athat%20M3-Jepa%20might%20provide%20a%20new%20paradigm%20to%20self-supervised%20learning%20and%0Aopen-world%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05929v4&entry.124074799=Read"},
{"title": "Adaptive Bidding Policies for First-Price Auctions with Budget\n  Constraints under Non-stationarity", "author": "Yige Wang and Jiashuo Jiang", "abstract": "  We study how a budget-constrained bidder should learn to adaptively bid in\nrepeated first-price auctions to maximize her cumulative payoff. This problem\narose due to an industry-wide shift from second-price auctions to first-price\nauctions in display advertising recently, which renders truthful bidding (i.e.,\nalways bidding one's private value) no longer optimal. We propose a simple\ndual-gradient-descent-based bidding policy that maintains a dual variable for\nbudget constraint as the bidder consumes her budget. In analysis, we consider\ntwo settings regarding the bidder's knowledge of her private values in the\nfuture: (i) an uninformative setting where all the distributional knowledge\n(can be non-stationary) is entirely unknown to the bidder, and (ii) an\ninformative setting where a prediction of the budget allocation in advance. We\ncharacterize the performance loss (or regret) relative to an optimal policy\nwith complete information on the stochasticity. For uninformative setting, We\nshow that the regret is \\tilde{O}(\\sqrt{T}) plus a variation term that reflects\nthe non-stationarity of the value distributions, and this is of optimal order.\nWe then show that we can get rid of the variation term with the help of the\nprediction; specifically, the regret is \\tilde{O}(\\sqrt{T}) plus the prediction\nerror term in the informative setting.\n", "link": "http://arxiv.org/abs/2505.02796v1", "date": "2025-05-05", "relevancy": 1.6347, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4312}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4084}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Bidding%20Policies%20for%20First-Price%20Auctions%20with%20Budget%0A%20%20Constraints%20under%20Non-stationarity&body=Title%3A%20Adaptive%20Bidding%20Policies%20for%20First-Price%20Auctions%20with%20Budget%0A%20%20Constraints%20under%20Non-stationarity%0AAuthor%3A%20Yige%20Wang%20and%20Jiashuo%20Jiang%0AAbstract%3A%20%20%20We%20study%20how%20a%20budget-constrained%20bidder%20should%20learn%20to%20adaptively%20bid%20in%0Arepeated%20first-price%20auctions%20to%20maximize%20her%20cumulative%20payoff.%20This%20problem%0Aarose%20due%20to%20an%20industry-wide%20shift%20from%20second-price%20auctions%20to%20first-price%0Aauctions%20in%20display%20advertising%20recently%2C%20which%20renders%20truthful%20bidding%20%28i.e.%2C%0Aalways%20bidding%20one%27s%20private%20value%29%20no%20longer%20optimal.%20We%20propose%20a%20simple%0Adual-gradient-descent-based%20bidding%20policy%20that%20maintains%20a%20dual%20variable%20for%0Abudget%20constraint%20as%20the%20bidder%20consumes%20her%20budget.%20In%20analysis%2C%20we%20consider%0Atwo%20settings%20regarding%20the%20bidder%27s%20knowledge%20of%20her%20private%20values%20in%20the%0Afuture%3A%20%28i%29%20an%20uninformative%20setting%20where%20all%20the%20distributional%20knowledge%0A%28can%20be%20non-stationary%29%20is%20entirely%20unknown%20to%20the%20bidder%2C%20and%20%28ii%29%20an%0Ainformative%20setting%20where%20a%20prediction%20of%20the%20budget%20allocation%20in%20advance.%20We%0Acharacterize%20the%20performance%20loss%20%28or%20regret%29%20relative%20to%20an%20optimal%20policy%0Awith%20complete%20information%20on%20the%20stochasticity.%20For%20uninformative%20setting%2C%20We%0Ashow%20that%20the%20regret%20is%20%5Ctilde%7BO%7D%28%5Csqrt%7BT%7D%29%20plus%20a%20variation%20term%20that%20reflects%0Athe%20non-stationarity%20of%20the%20value%20distributions%2C%20and%20this%20is%20of%20optimal%20order.%0AWe%20then%20show%20that%20we%20can%20get%20rid%20of%20the%20variation%20term%20with%20the%20help%20of%20the%0Aprediction%3B%20specifically%2C%20the%20regret%20is%20%5Ctilde%7BO%7D%28%5Csqrt%7BT%7D%29%20plus%20the%20prediction%0Aerror%20term%20in%20the%20informative%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02796v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Bidding%2520Policies%2520for%2520First-Price%2520Auctions%2520with%2520Budget%250A%2520%2520Constraints%2520under%2520Non-stationarity%26entry.906535625%3DYige%2520Wang%2520and%2520Jiashuo%2520Jiang%26entry.1292438233%3D%2520%2520We%2520study%2520how%2520a%2520budget-constrained%2520bidder%2520should%2520learn%2520to%2520adaptively%2520bid%2520in%250Arepeated%2520first-price%2520auctions%2520to%2520maximize%2520her%2520cumulative%2520payoff.%2520This%2520problem%250Aarose%2520due%2520to%2520an%2520industry-wide%2520shift%2520from%2520second-price%2520auctions%2520to%2520first-price%250Aauctions%2520in%2520display%2520advertising%2520recently%252C%2520which%2520renders%2520truthful%2520bidding%2520%2528i.e.%252C%250Aalways%2520bidding%2520one%2527s%2520private%2520value%2529%2520no%2520longer%2520optimal.%2520We%2520propose%2520a%2520simple%250Adual-gradient-descent-based%2520bidding%2520policy%2520that%2520maintains%2520a%2520dual%2520variable%2520for%250Abudget%2520constraint%2520as%2520the%2520bidder%2520consumes%2520her%2520budget.%2520In%2520analysis%252C%2520we%2520consider%250Atwo%2520settings%2520regarding%2520the%2520bidder%2527s%2520knowledge%2520of%2520her%2520private%2520values%2520in%2520the%250Afuture%253A%2520%2528i%2529%2520an%2520uninformative%2520setting%2520where%2520all%2520the%2520distributional%2520knowledge%250A%2528can%2520be%2520non-stationary%2529%2520is%2520entirely%2520unknown%2520to%2520the%2520bidder%252C%2520and%2520%2528ii%2529%2520an%250Ainformative%2520setting%2520where%2520a%2520prediction%2520of%2520the%2520budget%2520allocation%2520in%2520advance.%2520We%250Acharacterize%2520the%2520performance%2520loss%2520%2528or%2520regret%2529%2520relative%2520to%2520an%2520optimal%2520policy%250Awith%2520complete%2520information%2520on%2520the%2520stochasticity.%2520For%2520uninformative%2520setting%252C%2520We%250Ashow%2520that%2520the%2520regret%2520is%2520%255Ctilde%257BO%257D%2528%255Csqrt%257BT%257D%2529%2520plus%2520a%2520variation%2520term%2520that%2520reflects%250Athe%2520non-stationarity%2520of%2520the%2520value%2520distributions%252C%2520and%2520this%2520is%2520of%2520optimal%2520order.%250AWe%2520then%2520show%2520that%2520we%2520can%2520get%2520rid%2520of%2520the%2520variation%2520term%2520with%2520the%2520help%2520of%2520the%250Aprediction%253B%2520specifically%252C%2520the%2520regret%2520is%2520%255Ctilde%257BO%257D%2528%255Csqrt%257BT%257D%2529%2520plus%2520the%2520prediction%250Aerror%2520term%2520in%2520the%2520informative%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02796v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Bidding%20Policies%20for%20First-Price%20Auctions%20with%20Budget%0A%20%20Constraints%20under%20Non-stationarity&entry.906535625=Yige%20Wang%20and%20Jiashuo%20Jiang&entry.1292438233=%20%20We%20study%20how%20a%20budget-constrained%20bidder%20should%20learn%20to%20adaptively%20bid%20in%0Arepeated%20first-price%20auctions%20to%20maximize%20her%20cumulative%20payoff.%20This%20problem%0Aarose%20due%20to%20an%20industry-wide%20shift%20from%20second-price%20auctions%20to%20first-price%0Aauctions%20in%20display%20advertising%20recently%2C%20which%20renders%20truthful%20bidding%20%28i.e.%2C%0Aalways%20bidding%20one%27s%20private%20value%29%20no%20longer%20optimal.%20We%20propose%20a%20simple%0Adual-gradient-descent-based%20bidding%20policy%20that%20maintains%20a%20dual%20variable%20for%0Abudget%20constraint%20as%20the%20bidder%20consumes%20her%20budget.%20In%20analysis%2C%20we%20consider%0Atwo%20settings%20regarding%20the%20bidder%27s%20knowledge%20of%20her%20private%20values%20in%20the%0Afuture%3A%20%28i%29%20an%20uninformative%20setting%20where%20all%20the%20distributional%20knowledge%0A%28can%20be%20non-stationary%29%20is%20entirely%20unknown%20to%20the%20bidder%2C%20and%20%28ii%29%20an%0Ainformative%20setting%20where%20a%20prediction%20of%20the%20budget%20allocation%20in%20advance.%20We%0Acharacterize%20the%20performance%20loss%20%28or%20regret%29%20relative%20to%20an%20optimal%20policy%0Awith%20complete%20information%20on%20the%20stochasticity.%20For%20uninformative%20setting%2C%20We%0Ashow%20that%20the%20regret%20is%20%5Ctilde%7BO%7D%28%5Csqrt%7BT%7D%29%20plus%20a%20variation%20term%20that%20reflects%0Athe%20non-stationarity%20of%20the%20value%20distributions%2C%20and%20this%20is%20of%20optimal%20order.%0AWe%20then%20show%20that%20we%20can%20get%20rid%20of%20the%20variation%20term%20with%20the%20help%20of%20the%0Aprediction%3B%20specifically%2C%20the%20regret%20is%20%5Ctilde%7BO%7D%28%5Csqrt%7BT%7D%29%20plus%20the%20prediction%0Aerror%20term%20in%20the%20informative%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02796v1&entry.124074799=Read"},
{"title": "Agentic Neurodivergence as a Contingent Solution to the AI Alignment\n  Problem", "author": "Alberto Hern\u00e1ndez-Espinosa and Felipe S. Abrah\u00e3o and Olaf Witkowski and Hector Zenil", "abstract": "  The AI alignment problem, which focusses on ensuring that artificial\nintelligence (AI), including AGI and ASI, systems act according to human\nvalues, presents profound challenges. With the progression from narrow AI to\nArtificial General Intelligence (AGI) and Superintelligence, fears about\ncontrol and existential risk have escalated. This paper demonstrates that\nachieving complete alignment is inherently unattainable due to mathematical\nprinciples rooted in the foundations of predicate logic and computability, in\nparticular Turing's computational universality, G\\\"odel's incompleteness and\nChaitin's randomness. Instead, we argue that embracing AI misalignment or\nagent's `neurodivergence' as a contingent strategy, defined as fostering a\ndynamic ecosystem of competing, partially aligned agents, is a possible only\nviable path to mitigate risks. Through mathematical proofs and an experimental\ndesign, we explore how misalignment may serve and should be promoted as a\ncounterbalancing mechanism to team up with whichever agents are most aligned AI\nto human values, ensuring that no single system dominates destructively. The\nmain premise of our contribution is that misalignment is inevitable because\nfull AI-human alignment is a mathematical impossibility from Turing-complete\nsystems which we also prove in this paper, a feature then inherited to AGI and\nASI systems. We introduce and test `change-of-opinion' attacks based on this\nkind of perturbation and intervention analysis to study how agents may\nneutralise friendly or unfriendly AIs through cooperation, competition or\nmalice.\n", "link": "http://arxiv.org/abs/2505.02581v1", "date": "2025-05-05", "relevancy": 1.7162, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4342}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4326}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4234}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agentic%20Neurodivergence%20as%20a%20Contingent%20Solution%20to%20the%20AI%20Alignment%0A%20%20Problem&body=Title%3A%20Agentic%20Neurodivergence%20as%20a%20Contingent%20Solution%20to%20the%20AI%20Alignment%0A%20%20Problem%0AAuthor%3A%20Alberto%20Hern%C3%A1ndez-Espinosa%20and%20Felipe%20S.%20Abrah%C3%A3o%20and%20Olaf%20Witkowski%20and%20Hector%20Zenil%0AAbstract%3A%20%20%20The%20AI%20alignment%20problem%2C%20which%20focusses%20on%20ensuring%20that%20artificial%0Aintelligence%20%28AI%29%2C%20including%20AGI%20and%20ASI%2C%20systems%20act%20according%20to%20human%0Avalues%2C%20presents%20profound%20challenges.%20With%20the%20progression%20from%20narrow%20AI%20to%0AArtificial%20General%20Intelligence%20%28AGI%29%20and%20Superintelligence%2C%20fears%20about%0Acontrol%20and%20existential%20risk%20have%20escalated.%20This%20paper%20demonstrates%20that%0Aachieving%20complete%20alignment%20is%20inherently%20unattainable%20due%20to%20mathematical%0Aprinciples%20rooted%20in%20the%20foundations%20of%20predicate%20logic%20and%20computability%2C%20in%0Aparticular%20Turing%27s%20computational%20universality%2C%20G%5C%22odel%27s%20incompleteness%20and%0AChaitin%27s%20randomness.%20Instead%2C%20we%20argue%20that%20embracing%20AI%20misalignment%20or%0Aagent%27s%20%60neurodivergence%27%20as%20a%20contingent%20strategy%2C%20defined%20as%20fostering%20a%0Adynamic%20ecosystem%20of%20competing%2C%20partially%20aligned%20agents%2C%20is%20a%20possible%20only%0Aviable%20path%20to%20mitigate%20risks.%20Through%20mathematical%20proofs%20and%20an%20experimental%0Adesign%2C%20we%20explore%20how%20misalignment%20may%20serve%20and%20should%20be%20promoted%20as%20a%0Acounterbalancing%20mechanism%20to%20team%20up%20with%20whichever%20agents%20are%20most%20aligned%20AI%0Ato%20human%20values%2C%20ensuring%20that%20no%20single%20system%20dominates%20destructively.%20The%0Amain%20premise%20of%20our%20contribution%20is%20that%20misalignment%20is%20inevitable%20because%0Afull%20AI-human%20alignment%20is%20a%20mathematical%20impossibility%20from%20Turing-complete%0Asystems%20which%20we%20also%20prove%20in%20this%20paper%2C%20a%20feature%20then%20inherited%20to%20AGI%20and%0AASI%20systems.%20We%20introduce%20and%20test%20%60change-of-opinion%27%20attacks%20based%20on%20this%0Akind%20of%20perturbation%20and%20intervention%20analysis%20to%20study%20how%20agents%20may%0Aneutralise%20friendly%20or%20unfriendly%20AIs%20through%20cooperation%2C%20competition%20or%0Amalice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02581v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentic%2520Neurodivergence%2520as%2520a%2520Contingent%2520Solution%2520to%2520the%2520AI%2520Alignment%250A%2520%2520Problem%26entry.906535625%3DAlberto%2520Hern%25C3%25A1ndez-Espinosa%2520and%2520Felipe%2520S.%2520Abrah%25C3%25A3o%2520and%2520Olaf%2520Witkowski%2520and%2520Hector%2520Zenil%26entry.1292438233%3D%2520%2520The%2520AI%2520alignment%2520problem%252C%2520which%2520focusses%2520on%2520ensuring%2520that%2520artificial%250Aintelligence%2520%2528AI%2529%252C%2520including%2520AGI%2520and%2520ASI%252C%2520systems%2520act%2520according%2520to%2520human%250Avalues%252C%2520presents%2520profound%2520challenges.%2520With%2520the%2520progression%2520from%2520narrow%2520AI%2520to%250AArtificial%2520General%2520Intelligence%2520%2528AGI%2529%2520and%2520Superintelligence%252C%2520fears%2520about%250Acontrol%2520and%2520existential%2520risk%2520have%2520escalated.%2520This%2520paper%2520demonstrates%2520that%250Aachieving%2520complete%2520alignment%2520is%2520inherently%2520unattainable%2520due%2520to%2520mathematical%250Aprinciples%2520rooted%2520in%2520the%2520foundations%2520of%2520predicate%2520logic%2520and%2520computability%252C%2520in%250Aparticular%2520Turing%2527s%2520computational%2520universality%252C%2520G%255C%2522odel%2527s%2520incompleteness%2520and%250AChaitin%2527s%2520randomness.%2520Instead%252C%2520we%2520argue%2520that%2520embracing%2520AI%2520misalignment%2520or%250Aagent%2527s%2520%2560neurodivergence%2527%2520as%2520a%2520contingent%2520strategy%252C%2520defined%2520as%2520fostering%2520a%250Adynamic%2520ecosystem%2520of%2520competing%252C%2520partially%2520aligned%2520agents%252C%2520is%2520a%2520possible%2520only%250Aviable%2520path%2520to%2520mitigate%2520risks.%2520Through%2520mathematical%2520proofs%2520and%2520an%2520experimental%250Adesign%252C%2520we%2520explore%2520how%2520misalignment%2520may%2520serve%2520and%2520should%2520be%2520promoted%2520as%2520a%250Acounterbalancing%2520mechanism%2520to%2520team%2520up%2520with%2520whichever%2520agents%2520are%2520most%2520aligned%2520AI%250Ato%2520human%2520values%252C%2520ensuring%2520that%2520no%2520single%2520system%2520dominates%2520destructively.%2520The%250Amain%2520premise%2520of%2520our%2520contribution%2520is%2520that%2520misalignment%2520is%2520inevitable%2520because%250Afull%2520AI-human%2520alignment%2520is%2520a%2520mathematical%2520impossibility%2520from%2520Turing-complete%250Asystems%2520which%2520we%2520also%2520prove%2520in%2520this%2520paper%252C%2520a%2520feature%2520then%2520inherited%2520to%2520AGI%2520and%250AASI%2520systems.%2520We%2520introduce%2520and%2520test%2520%2560change-of-opinion%2527%2520attacks%2520based%2520on%2520this%250Akind%2520of%2520perturbation%2520and%2520intervention%2520analysis%2520to%2520study%2520how%2520agents%2520may%250Aneutralise%2520friendly%2520or%2520unfriendly%2520AIs%2520through%2520cooperation%252C%2520competition%2520or%250Amalice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02581v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agentic%20Neurodivergence%20as%20a%20Contingent%20Solution%20to%20the%20AI%20Alignment%0A%20%20Problem&entry.906535625=Alberto%20Hern%C3%A1ndez-Espinosa%20and%20Felipe%20S.%20Abrah%C3%A3o%20and%20Olaf%20Witkowski%20and%20Hector%20Zenil&entry.1292438233=%20%20The%20AI%20alignment%20problem%2C%20which%20focusses%20on%20ensuring%20that%20artificial%0Aintelligence%20%28AI%29%2C%20including%20AGI%20and%20ASI%2C%20systems%20act%20according%20to%20human%0Avalues%2C%20presents%20profound%20challenges.%20With%20the%20progression%20from%20narrow%20AI%20to%0AArtificial%20General%20Intelligence%20%28AGI%29%20and%20Superintelligence%2C%20fears%20about%0Acontrol%20and%20existential%20risk%20have%20escalated.%20This%20paper%20demonstrates%20that%0Aachieving%20complete%20alignment%20is%20inherently%20unattainable%20due%20to%20mathematical%0Aprinciples%20rooted%20in%20the%20foundations%20of%20predicate%20logic%20and%20computability%2C%20in%0Aparticular%20Turing%27s%20computational%20universality%2C%20G%5C%22odel%27s%20incompleteness%20and%0AChaitin%27s%20randomness.%20Instead%2C%20we%20argue%20that%20embracing%20AI%20misalignment%20or%0Aagent%27s%20%60neurodivergence%27%20as%20a%20contingent%20strategy%2C%20defined%20as%20fostering%20a%0Adynamic%20ecosystem%20of%20competing%2C%20partially%20aligned%20agents%2C%20is%20a%20possible%20only%0Aviable%20path%20to%20mitigate%20risks.%20Through%20mathematical%20proofs%20and%20an%20experimental%0Adesign%2C%20we%20explore%20how%20misalignment%20may%20serve%20and%20should%20be%20promoted%20as%20a%0Acounterbalancing%20mechanism%20to%20team%20up%20with%20whichever%20agents%20are%20most%20aligned%20AI%0Ato%20human%20values%2C%20ensuring%20that%20no%20single%20system%20dominates%20destructively.%20The%0Amain%20premise%20of%20our%20contribution%20is%20that%20misalignment%20is%20inevitable%20because%0Afull%20AI-human%20alignment%20is%20a%20mathematical%20impossibility%20from%20Turing-complete%0Asystems%20which%20we%20also%20prove%20in%20this%20paper%2C%20a%20feature%20then%20inherited%20to%20AGI%20and%0AASI%20systems.%20We%20introduce%20and%20test%20%60change-of-opinion%27%20attacks%20based%20on%20this%0Akind%20of%20perturbation%20and%20intervention%20analysis%20to%20study%20how%20agents%20may%0Aneutralise%20friendly%20or%20unfriendly%20AIs%20through%20cooperation%2C%20competition%20or%0Amalice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02581v1&entry.124074799=Read"},
{"title": "Giving Simulated Cells a Voice: Evolving Prompt-to-Intervention Models\n  for Cellular Control", "author": "Nam H. Le and Patrick Erikson and Yanbo Zhang and Michael Levin and Josh Bongard", "abstract": "  Guiding biological systems toward desired states, such as morphogenetic\noutcomes, remains a fundamental challenge with far-reaching implications for\nmedicine and synthetic biology. While large language models (LLMs) have enabled\nnatural language as an interface for interpretable control in AI systems, their\nuse as mediators for steering biological or cellular dynamics remains largely\nunexplored.\n  In this work, we present a functional pipeline that translates natural\nlanguage prompts into spatial vector fields capable of directing simulated\ncellular collectives. Our approach combines a large language model with an\nevolvable neural controller (Prompt-to-Intervention, or P2I), optimized via\nevolutionary strategies to generate behaviors such as clustering or scattering\nin a simulated 2D environment.\n  We demonstrate that even with constrained vocabulary and simplified cell\nmodels, evolved P2I networks can successfully align cellular dynamics with\nuser-defined goals expressed in plain language. This work offers a complete\nloop from language input to simulated bioelectric-like intervention to\nbehavioral output, providing a foundation for future systems capable of natural\nlanguage-driven cellular control.\n", "link": "http://arxiv.org/abs/2505.02766v1", "date": "2025-05-05", "relevancy": 1.49, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4997}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4973}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Giving%20Simulated%20Cells%20a%20Voice%3A%20Evolving%20Prompt-to-Intervention%20Models%0A%20%20for%20Cellular%20Control&body=Title%3A%20Giving%20Simulated%20Cells%20a%20Voice%3A%20Evolving%20Prompt-to-Intervention%20Models%0A%20%20for%20Cellular%20Control%0AAuthor%3A%20Nam%20H.%20Le%20and%20Patrick%20Erikson%20and%20Yanbo%20Zhang%20and%20Michael%20Levin%20and%20Josh%20Bongard%0AAbstract%3A%20%20%20Guiding%20biological%20systems%20toward%20desired%20states%2C%20such%20as%20morphogenetic%0Aoutcomes%2C%20remains%20a%20fundamental%20challenge%20with%20far-reaching%20implications%20for%0Amedicine%20and%20synthetic%20biology.%20While%20large%20language%20models%20%28LLMs%29%20have%20enabled%0Anatural%20language%20as%20an%20interface%20for%20interpretable%20control%20in%20AI%20systems%2C%20their%0Ause%20as%20mediators%20for%20steering%20biological%20or%20cellular%20dynamics%20remains%20largely%0Aunexplored.%0A%20%20In%20this%20work%2C%20we%20present%20a%20functional%20pipeline%20that%20translates%20natural%0Alanguage%20prompts%20into%20spatial%20vector%20fields%20capable%20of%20directing%20simulated%0Acellular%20collectives.%20Our%20approach%20combines%20a%20large%20language%20model%20with%20an%0Aevolvable%20neural%20controller%20%28Prompt-to-Intervention%2C%20or%20P2I%29%2C%20optimized%20via%0Aevolutionary%20strategies%20to%20generate%20behaviors%20such%20as%20clustering%20or%20scattering%0Ain%20a%20simulated%202D%20environment.%0A%20%20We%20demonstrate%20that%20even%20with%20constrained%20vocabulary%20and%20simplified%20cell%0Amodels%2C%20evolved%20P2I%20networks%20can%20successfully%20align%20cellular%20dynamics%20with%0Auser-defined%20goals%20expressed%20in%20plain%20language.%20This%20work%20offers%20a%20complete%0Aloop%20from%20language%20input%20to%20simulated%20bioelectric-like%20intervention%20to%0Abehavioral%20output%2C%20providing%20a%20foundation%20for%20future%20systems%20capable%20of%20natural%0Alanguage-driven%20cellular%20control.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02766v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGiving%2520Simulated%2520Cells%2520a%2520Voice%253A%2520Evolving%2520Prompt-to-Intervention%2520Models%250A%2520%2520for%2520Cellular%2520Control%26entry.906535625%3DNam%2520H.%2520Le%2520and%2520Patrick%2520Erikson%2520and%2520Yanbo%2520Zhang%2520and%2520Michael%2520Levin%2520and%2520Josh%2520Bongard%26entry.1292438233%3D%2520%2520Guiding%2520biological%2520systems%2520toward%2520desired%2520states%252C%2520such%2520as%2520morphogenetic%250Aoutcomes%252C%2520remains%2520a%2520fundamental%2520challenge%2520with%2520far-reaching%2520implications%2520for%250Amedicine%2520and%2520synthetic%2520biology.%2520While%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520enabled%250Anatural%2520language%2520as%2520an%2520interface%2520for%2520interpretable%2520control%2520in%2520AI%2520systems%252C%2520their%250Ause%2520as%2520mediators%2520for%2520steering%2520biological%2520or%2520cellular%2520dynamics%2520remains%2520largely%250Aunexplored.%250A%2520%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520functional%2520pipeline%2520that%2520translates%2520natural%250Alanguage%2520prompts%2520into%2520spatial%2520vector%2520fields%2520capable%2520of%2520directing%2520simulated%250Acellular%2520collectives.%2520Our%2520approach%2520combines%2520a%2520large%2520language%2520model%2520with%2520an%250Aevolvable%2520neural%2520controller%2520%2528Prompt-to-Intervention%252C%2520or%2520P2I%2529%252C%2520optimized%2520via%250Aevolutionary%2520strategies%2520to%2520generate%2520behaviors%2520such%2520as%2520clustering%2520or%2520scattering%250Ain%2520a%2520simulated%25202D%2520environment.%250A%2520%2520We%2520demonstrate%2520that%2520even%2520with%2520constrained%2520vocabulary%2520and%2520simplified%2520cell%250Amodels%252C%2520evolved%2520P2I%2520networks%2520can%2520successfully%2520align%2520cellular%2520dynamics%2520with%250Auser-defined%2520goals%2520expressed%2520in%2520plain%2520language.%2520This%2520work%2520offers%2520a%2520complete%250Aloop%2520from%2520language%2520input%2520to%2520simulated%2520bioelectric-like%2520intervention%2520to%250Abehavioral%2520output%252C%2520providing%2520a%2520foundation%2520for%2520future%2520systems%2520capable%2520of%2520natural%250Alanguage-driven%2520cellular%2520control.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02766v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Giving%20Simulated%20Cells%20a%20Voice%3A%20Evolving%20Prompt-to-Intervention%20Models%0A%20%20for%20Cellular%20Control&entry.906535625=Nam%20H.%20Le%20and%20Patrick%20Erikson%20and%20Yanbo%20Zhang%20and%20Michael%20Levin%20and%20Josh%20Bongard&entry.1292438233=%20%20Guiding%20biological%20systems%20toward%20desired%20states%2C%20such%20as%20morphogenetic%0Aoutcomes%2C%20remains%20a%20fundamental%20challenge%20with%20far-reaching%20implications%20for%0Amedicine%20and%20synthetic%20biology.%20While%20large%20language%20models%20%28LLMs%29%20have%20enabled%0Anatural%20language%20as%20an%20interface%20for%20interpretable%20control%20in%20AI%20systems%2C%20their%0Ause%20as%20mediators%20for%20steering%20biological%20or%20cellular%20dynamics%20remains%20largely%0Aunexplored.%0A%20%20In%20this%20work%2C%20we%20present%20a%20functional%20pipeline%20that%20translates%20natural%0Alanguage%20prompts%20into%20spatial%20vector%20fields%20capable%20of%20directing%20simulated%0Acellular%20collectives.%20Our%20approach%20combines%20a%20large%20language%20model%20with%20an%0Aevolvable%20neural%20controller%20%28Prompt-to-Intervention%2C%20or%20P2I%29%2C%20optimized%20via%0Aevolutionary%20strategies%20to%20generate%20behaviors%20such%20as%20clustering%20or%20scattering%0Ain%20a%20simulated%202D%20environment.%0A%20%20We%20demonstrate%20that%20even%20with%20constrained%20vocabulary%20and%20simplified%20cell%0Amodels%2C%20evolved%20P2I%20networks%20can%20successfully%20align%20cellular%20dynamics%20with%0Auser-defined%20goals%20expressed%20in%20plain%20language.%20This%20work%20offers%20a%20complete%0Aloop%20from%20language%20input%20to%20simulated%20bioelectric-like%20intervention%20to%0Abehavioral%20output%2C%20providing%20a%20foundation%20for%20future%20systems%20capable%20of%20natural%0Alanguage-driven%20cellular%20control.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02766v1&entry.124074799=Read"},
{"title": "A distance function for stochastic matrices", "author": "Antony R. Lee and Peter Tino and Iain Bruce Styles", "abstract": "  Motivated by information geometry, a distance function on the space of\nstochastic matrices is advocated. Starting with sequences of Markov chains the\nBhattacharyya angle is advocated as the natural tool for comparing both short\nand long term Markov chain runs. Bounds on the convergence of the distance and\nmixing times are derived. Guided by the desire to compare different Markov\nchain models, especially in the setting of healthcare processes, a new distance\nfunction on the space of stochastic matrices is presented. It is a true\ndistance measure which has a closed form and is efficient to implement for\nnumerical evaluation. In the case of ergodic Markov chains, it is shown that\nconsidering either the Bhattacharyya angle on Markov sequences or the new\nstochastic matrix distance leads to the same distance between models.\n", "link": "http://arxiv.org/abs/2410.12689v2", "date": "2025-05-05", "relevancy": 1.6719, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4366}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4311}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.3975}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20distance%20function%20for%20stochastic%20matrices&body=Title%3A%20A%20distance%20function%20for%20stochastic%20matrices%0AAuthor%3A%20Antony%20R.%20Lee%20and%20Peter%20Tino%20and%20Iain%20Bruce%20Styles%0AAbstract%3A%20%20%20Motivated%20by%20information%20geometry%2C%20a%20distance%20function%20on%20the%20space%20of%0Astochastic%20matrices%20is%20advocated.%20Starting%20with%20sequences%20of%20Markov%20chains%20the%0ABhattacharyya%20angle%20is%20advocated%20as%20the%20natural%20tool%20for%20comparing%20both%20short%0Aand%20long%20term%20Markov%20chain%20runs.%20Bounds%20on%20the%20convergence%20of%20the%20distance%20and%0Amixing%20times%20are%20derived.%20Guided%20by%20the%20desire%20to%20compare%20different%20Markov%0Achain%20models%2C%20especially%20in%20the%20setting%20of%20healthcare%20processes%2C%20a%20new%20distance%0Afunction%20on%20the%20space%20of%20stochastic%20matrices%20is%20presented.%20It%20is%20a%20true%0Adistance%20measure%20which%20has%20a%20closed%20form%20and%20is%20efficient%20to%20implement%20for%0Anumerical%20evaluation.%20In%20the%20case%20of%20ergodic%20Markov%20chains%2C%20it%20is%20shown%20that%0Aconsidering%20either%20the%20Bhattacharyya%20angle%20on%20Markov%20sequences%20or%20the%20new%0Astochastic%20matrix%20distance%20leads%20to%20the%20same%20distance%20between%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12689v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520distance%2520function%2520for%2520stochastic%2520matrices%26entry.906535625%3DAntony%2520R.%2520Lee%2520and%2520Peter%2520Tino%2520and%2520Iain%2520Bruce%2520Styles%26entry.1292438233%3D%2520%2520Motivated%2520by%2520information%2520geometry%252C%2520a%2520distance%2520function%2520on%2520the%2520space%2520of%250Astochastic%2520matrices%2520is%2520advocated.%2520Starting%2520with%2520sequences%2520of%2520Markov%2520chains%2520the%250ABhattacharyya%2520angle%2520is%2520advocated%2520as%2520the%2520natural%2520tool%2520for%2520comparing%2520both%2520short%250Aand%2520long%2520term%2520Markov%2520chain%2520runs.%2520Bounds%2520on%2520the%2520convergence%2520of%2520the%2520distance%2520and%250Amixing%2520times%2520are%2520derived.%2520Guided%2520by%2520the%2520desire%2520to%2520compare%2520different%2520Markov%250Achain%2520models%252C%2520especially%2520in%2520the%2520setting%2520of%2520healthcare%2520processes%252C%2520a%2520new%2520distance%250Afunction%2520on%2520the%2520space%2520of%2520stochastic%2520matrices%2520is%2520presented.%2520It%2520is%2520a%2520true%250Adistance%2520measure%2520which%2520has%2520a%2520closed%2520form%2520and%2520is%2520efficient%2520to%2520implement%2520for%250Anumerical%2520evaluation.%2520In%2520the%2520case%2520of%2520ergodic%2520Markov%2520chains%252C%2520it%2520is%2520shown%2520that%250Aconsidering%2520either%2520the%2520Bhattacharyya%2520angle%2520on%2520Markov%2520sequences%2520or%2520the%2520new%250Astochastic%2520matrix%2520distance%2520leads%2520to%2520the%2520same%2520distance%2520between%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12689v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20distance%20function%20for%20stochastic%20matrices&entry.906535625=Antony%20R.%20Lee%20and%20Peter%20Tino%20and%20Iain%20Bruce%20Styles&entry.1292438233=%20%20Motivated%20by%20information%20geometry%2C%20a%20distance%20function%20on%20the%20space%20of%0Astochastic%20matrices%20is%20advocated.%20Starting%20with%20sequences%20of%20Markov%20chains%20the%0ABhattacharyya%20angle%20is%20advocated%20as%20the%20natural%20tool%20for%20comparing%20both%20short%0Aand%20long%20term%20Markov%20chain%20runs.%20Bounds%20on%20the%20convergence%20of%20the%20distance%20and%0Amixing%20times%20are%20derived.%20Guided%20by%20the%20desire%20to%20compare%20different%20Markov%0Achain%20models%2C%20especially%20in%20the%20setting%20of%20healthcare%20processes%2C%20a%20new%20distance%0Afunction%20on%20the%20space%20of%20stochastic%20matrices%20is%20presented.%20It%20is%20a%20true%0Adistance%20measure%20which%20has%20a%20closed%20form%20and%20is%20efficient%20to%20implement%20for%0Anumerical%20evaluation.%20In%20the%20case%20of%20ergodic%20Markov%20chains%2C%20it%20is%20shown%20that%0Aconsidering%20either%20the%20Bhattacharyya%20angle%20on%20Markov%20sequences%20or%20the%20new%0Astochastic%20matrix%20distance%20leads%20to%20the%20same%20distance%20between%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12689v2&entry.124074799=Read"},
{"title": "BrushEdit: All-In-One Image Inpainting and Editing", "author": "Yaowei Li and Yuxuan Bian and Xuan Ju and Zhaoyang Zhang and Junhao Zhuang and Ying Shan and Yuexian Zou and Qiang Xu", "abstract": "  Image editing has advanced significantly with the development of diffusion\nmodels using both inversion-based and instruction-based methods. However,\ncurrent inversion-based approaches struggle with big modifications (e.g.,\nadding or removing objects) due to the structured nature of inversion noise,\nwhich hinders substantial changes. Meanwhile, instruction-based methods often\nconstrain users to black-box operations, limiting direct interaction for\nspecifying editing regions and intensity. To address these limitations, we\npropose BrushEdit, a novel inpainting-based instruction-guided image editing\nparadigm, which leverages multimodal large language models (MLLMs) and image\ninpainting models to enable autonomous, user-friendly, and interactive\nfree-form instruction editing. Specifically, we devise a system enabling\nfree-form instruction editing by integrating MLLMs and a dual-branch image\ninpainting model in an agent-cooperative framework to perform editing category\nclassification, main object identification, mask acquisition, and editing area\ninpainting. Extensive experiments show that our framework effectively combines\nMLLMs and inpainting models, achieving superior performance across seven\nmetrics including mask region preservation and editing effect coherence.\n", "link": "http://arxiv.org/abs/2412.10316v3", "date": "2025-05-05", "relevancy": 1.7117, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5934}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5866}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BrushEdit%3A%20All-In-One%20Image%20Inpainting%20and%20Editing&body=Title%3A%20BrushEdit%3A%20All-In-One%20Image%20Inpainting%20and%20Editing%0AAuthor%3A%20Yaowei%20Li%20and%20Yuxuan%20Bian%20and%20Xuan%20Ju%20and%20Zhaoyang%20Zhang%20and%20Junhao%20Zhuang%20and%20Ying%20Shan%20and%20Yuexian%20Zou%20and%20Qiang%20Xu%0AAbstract%3A%20%20%20Image%20editing%20has%20advanced%20significantly%20with%20the%20development%20of%20diffusion%0Amodels%20using%20both%20inversion-based%20and%20instruction-based%20methods.%20However%2C%0Acurrent%20inversion-based%20approaches%20struggle%20with%20big%20modifications%20%28e.g.%2C%0Aadding%20or%20removing%20objects%29%20due%20to%20the%20structured%20nature%20of%20inversion%20noise%2C%0Awhich%20hinders%20substantial%20changes.%20Meanwhile%2C%20instruction-based%20methods%20often%0Aconstrain%20users%20to%20black-box%20operations%2C%20limiting%20direct%20interaction%20for%0Aspecifying%20editing%20regions%20and%20intensity.%20To%20address%20these%20limitations%2C%20we%0Apropose%20BrushEdit%2C%20a%20novel%20inpainting-based%20instruction-guided%20image%20editing%0Aparadigm%2C%20which%20leverages%20multimodal%20large%20language%20models%20%28MLLMs%29%20and%20image%0Ainpainting%20models%20to%20enable%20autonomous%2C%20user-friendly%2C%20and%20interactive%0Afree-form%20instruction%20editing.%20Specifically%2C%20we%20devise%20a%20system%20enabling%0Afree-form%20instruction%20editing%20by%20integrating%20MLLMs%20and%20a%20dual-branch%20image%0Ainpainting%20model%20in%20an%20agent-cooperative%20framework%20to%20perform%20editing%20category%0Aclassification%2C%20main%20object%20identification%2C%20mask%20acquisition%2C%20and%20editing%20area%0Ainpainting.%20Extensive%20experiments%20show%20that%20our%20framework%20effectively%20combines%0AMLLMs%20and%20inpainting%20models%2C%20achieving%20superior%20performance%20across%20seven%0Ametrics%20including%20mask%20region%20preservation%20and%20editing%20effect%20coherence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10316v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrushEdit%253A%2520All-In-One%2520Image%2520Inpainting%2520and%2520Editing%26entry.906535625%3DYaowei%2520Li%2520and%2520Yuxuan%2520Bian%2520and%2520Xuan%2520Ju%2520and%2520Zhaoyang%2520Zhang%2520and%2520Junhao%2520Zhuang%2520and%2520Ying%2520Shan%2520and%2520Yuexian%2520Zou%2520and%2520Qiang%2520Xu%26entry.1292438233%3D%2520%2520Image%2520editing%2520has%2520advanced%2520significantly%2520with%2520the%2520development%2520of%2520diffusion%250Amodels%2520using%2520both%2520inversion-based%2520and%2520instruction-based%2520methods.%2520However%252C%250Acurrent%2520inversion-based%2520approaches%2520struggle%2520with%2520big%2520modifications%2520%2528e.g.%252C%250Aadding%2520or%2520removing%2520objects%2529%2520due%2520to%2520the%2520structured%2520nature%2520of%2520inversion%2520noise%252C%250Awhich%2520hinders%2520substantial%2520changes.%2520Meanwhile%252C%2520instruction-based%2520methods%2520often%250Aconstrain%2520users%2520to%2520black-box%2520operations%252C%2520limiting%2520direct%2520interaction%2520for%250Aspecifying%2520editing%2520regions%2520and%2520intensity.%2520To%2520address%2520these%2520limitations%252C%2520we%250Apropose%2520BrushEdit%252C%2520a%2520novel%2520inpainting-based%2520instruction-guided%2520image%2520editing%250Aparadigm%252C%2520which%2520leverages%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520and%2520image%250Ainpainting%2520models%2520to%2520enable%2520autonomous%252C%2520user-friendly%252C%2520and%2520interactive%250Afree-form%2520instruction%2520editing.%2520Specifically%252C%2520we%2520devise%2520a%2520system%2520enabling%250Afree-form%2520instruction%2520editing%2520by%2520integrating%2520MLLMs%2520and%2520a%2520dual-branch%2520image%250Ainpainting%2520model%2520in%2520an%2520agent-cooperative%2520framework%2520to%2520perform%2520editing%2520category%250Aclassification%252C%2520main%2520object%2520identification%252C%2520mask%2520acquisition%252C%2520and%2520editing%2520area%250Ainpainting.%2520Extensive%2520experiments%2520show%2520that%2520our%2520framework%2520effectively%2520combines%250AMLLMs%2520and%2520inpainting%2520models%252C%2520achieving%2520superior%2520performance%2520across%2520seven%250Ametrics%2520including%2520mask%2520region%2520preservation%2520and%2520editing%2520effect%2520coherence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10316v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BrushEdit%3A%20All-In-One%20Image%20Inpainting%20and%20Editing&entry.906535625=Yaowei%20Li%20and%20Yuxuan%20Bian%20and%20Xuan%20Ju%20and%20Zhaoyang%20Zhang%20and%20Junhao%20Zhuang%20and%20Ying%20Shan%20and%20Yuexian%20Zou%20and%20Qiang%20Xu&entry.1292438233=%20%20Image%20editing%20has%20advanced%20significantly%20with%20the%20development%20of%20diffusion%0Amodels%20using%20both%20inversion-based%20and%20instruction-based%20methods.%20However%2C%0Acurrent%20inversion-based%20approaches%20struggle%20with%20big%20modifications%20%28e.g.%2C%0Aadding%20or%20removing%20objects%29%20due%20to%20the%20structured%20nature%20of%20inversion%20noise%2C%0Awhich%20hinders%20substantial%20changes.%20Meanwhile%2C%20instruction-based%20methods%20often%0Aconstrain%20users%20to%20black-box%20operations%2C%20limiting%20direct%20interaction%20for%0Aspecifying%20editing%20regions%20and%20intensity.%20To%20address%20these%20limitations%2C%20we%0Apropose%20BrushEdit%2C%20a%20novel%20inpainting-based%20instruction-guided%20image%20editing%0Aparadigm%2C%20which%20leverages%20multimodal%20large%20language%20models%20%28MLLMs%29%20and%20image%0Ainpainting%20models%20to%20enable%20autonomous%2C%20user-friendly%2C%20and%20interactive%0Afree-form%20instruction%20editing.%20Specifically%2C%20we%20devise%20a%20system%20enabling%0Afree-form%20instruction%20editing%20by%20integrating%20MLLMs%20and%20a%20dual-branch%20image%0Ainpainting%20model%20in%20an%20agent-cooperative%20framework%20to%20perform%20editing%20category%0Aclassification%2C%20main%20object%20identification%2C%20mask%20acquisition%2C%20and%20editing%20area%0Ainpainting.%20Extensive%20experiments%20show%20that%20our%20framework%20effectively%20combines%0AMLLMs%20and%20inpainting%20models%2C%20achieving%20superior%20performance%20across%20seven%0Ametrics%20including%20mask%20region%20preservation%20and%20editing%20effect%20coherence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10316v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


