<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251022.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Optimized 3D Gaussian Splatting using Coarse-to-Fine Image Frequency\n  Modulation", "author": "Umar Farooq and Jean-Yves Guillemaut and Adrian Hilton and Marco Volino", "abstract": "  The field of Novel View Synthesis has been revolutionized by 3D Gaussian\nSplatting (3DGS), which enables high-quality scene reconstruction that can be\nrendered in real-time. 3DGS-based techniques typically suffer from high GPU\nmemory and disk storage requirements which limits their practical application\non consumer-grade devices. We propose Opti3DGS, a novel frequency-modulated\ncoarse-to-fine optimization framework that aims to minimize the number of\nGaussian primitives used to represent a scene, thus reducing memory and storage\ndemands. Opti3DGS leverages image frequency modulation, initially enforcing a\ncoarse scene representation and progressively refining it by modulating\nfrequency details in the training images. On the baseline 3DGS, we demonstrate\nan average reduction of 62% in Gaussians, a 40% reduction in the training GPU\nmemory requirements and a 20% reduction in optimization time without\nsacrificing the visual quality. Furthermore, we show that our method integrates\nseamlessly with many 3DGS-based techniques, consistently reducing the number of\nGaussian primitives while maintaining, and often improving, visual quality.\nAdditionally, Opti3DGS inherently produces a level-of-detail scene\nrepresentation at no extra cost, a natural byproduct of the optimization\npipeline. Results and code will be made publicly available.\n", "link": "http://arxiv.org/abs/2503.14475v2", "date": "2025-10-22", "relevancy": 3.4012, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7056}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6721}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimized%203D%20Gaussian%20Splatting%20using%20Coarse-to-Fine%20Image%20Frequency%0A%20%20Modulation&body=Title%3A%20Optimized%203D%20Gaussian%20Splatting%20using%20Coarse-to-Fine%20Image%20Frequency%0A%20%20Modulation%0AAuthor%3A%20Umar%20Farooq%20and%20Jean-Yves%20Guillemaut%20and%20Adrian%20Hilton%20and%20Marco%20Volino%0AAbstract%3A%20%20%20The%20field%20of%20Novel%20View%20Synthesis%20has%20been%20revolutionized%20by%203D%20Gaussian%0ASplatting%20%283DGS%29%2C%20which%20enables%20high-quality%20scene%20reconstruction%20that%20can%20be%0Arendered%20in%20real-time.%203DGS-based%20techniques%20typically%20suffer%20from%20high%20GPU%0Amemory%20and%20disk%20storage%20requirements%20which%20limits%20their%20practical%20application%0Aon%20consumer-grade%20devices.%20We%20propose%20Opti3DGS%2C%20a%20novel%20frequency-modulated%0Acoarse-to-fine%20optimization%20framework%20that%20aims%20to%20minimize%20the%20number%20of%0AGaussian%20primitives%20used%20to%20represent%20a%20scene%2C%20thus%20reducing%20memory%20and%20storage%0Ademands.%20Opti3DGS%20leverages%20image%20frequency%20modulation%2C%20initially%20enforcing%20a%0Acoarse%20scene%20representation%20and%20progressively%20refining%20it%20by%20modulating%0Afrequency%20details%20in%20the%20training%20images.%20On%20the%20baseline%203DGS%2C%20we%20demonstrate%0Aan%20average%20reduction%20of%2062%25%20in%20Gaussians%2C%20a%2040%25%20reduction%20in%20the%20training%20GPU%0Amemory%20requirements%20and%20a%2020%25%20reduction%20in%20optimization%20time%20without%0Asacrificing%20the%20visual%20quality.%20Furthermore%2C%20we%20show%20that%20our%20method%20integrates%0Aseamlessly%20with%20many%203DGS-based%20techniques%2C%20consistently%20reducing%20the%20number%20of%0AGaussian%20primitives%20while%20maintaining%2C%20and%20often%20improving%2C%20visual%20quality.%0AAdditionally%2C%20Opti3DGS%20inherently%20produces%20a%20level-of-detail%20scene%0Arepresentation%20at%20no%20extra%20cost%2C%20a%20natural%20byproduct%20of%20the%20optimization%0Apipeline.%20Results%20and%20code%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.14475v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimized%25203D%2520Gaussian%2520Splatting%2520using%2520Coarse-to-Fine%2520Image%2520Frequency%250A%2520%2520Modulation%26entry.906535625%3DUmar%2520Farooq%2520and%2520Jean-Yves%2520Guillemaut%2520and%2520Adrian%2520Hilton%2520and%2520Marco%2520Volino%26entry.1292438233%3D%2520%2520The%2520field%2520of%2520Novel%2520View%2520Synthesis%2520has%2520been%2520revolutionized%2520by%25203D%2520Gaussian%250ASplatting%2520%25283DGS%2529%252C%2520which%2520enables%2520high-quality%2520scene%2520reconstruction%2520that%2520can%2520be%250Arendered%2520in%2520real-time.%25203DGS-based%2520techniques%2520typically%2520suffer%2520from%2520high%2520GPU%250Amemory%2520and%2520disk%2520storage%2520requirements%2520which%2520limits%2520their%2520practical%2520application%250Aon%2520consumer-grade%2520devices.%2520We%2520propose%2520Opti3DGS%252C%2520a%2520novel%2520frequency-modulated%250Acoarse-to-fine%2520optimization%2520framework%2520that%2520aims%2520to%2520minimize%2520the%2520number%2520of%250AGaussian%2520primitives%2520used%2520to%2520represent%2520a%2520scene%252C%2520thus%2520reducing%2520memory%2520and%2520storage%250Ademands.%2520Opti3DGS%2520leverages%2520image%2520frequency%2520modulation%252C%2520initially%2520enforcing%2520a%250Acoarse%2520scene%2520representation%2520and%2520progressively%2520refining%2520it%2520by%2520modulating%250Afrequency%2520details%2520in%2520the%2520training%2520images.%2520On%2520the%2520baseline%25203DGS%252C%2520we%2520demonstrate%250Aan%2520average%2520reduction%2520of%252062%2525%2520in%2520Gaussians%252C%2520a%252040%2525%2520reduction%2520in%2520the%2520training%2520GPU%250Amemory%2520requirements%2520and%2520a%252020%2525%2520reduction%2520in%2520optimization%2520time%2520without%250Asacrificing%2520the%2520visual%2520quality.%2520Furthermore%252C%2520we%2520show%2520that%2520our%2520method%2520integrates%250Aseamlessly%2520with%2520many%25203DGS-based%2520techniques%252C%2520consistently%2520reducing%2520the%2520number%2520of%250AGaussian%2520primitives%2520while%2520maintaining%252C%2520and%2520often%2520improving%252C%2520visual%2520quality.%250AAdditionally%252C%2520Opti3DGS%2520inherently%2520produces%2520a%2520level-of-detail%2520scene%250Arepresentation%2520at%2520no%2520extra%2520cost%252C%2520a%2520natural%2520byproduct%2520of%2520the%2520optimization%250Apipeline.%2520Results%2520and%2520code%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.14475v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimized%203D%20Gaussian%20Splatting%20using%20Coarse-to-Fine%20Image%20Frequency%0A%20%20Modulation&entry.906535625=Umar%20Farooq%20and%20Jean-Yves%20Guillemaut%20and%20Adrian%20Hilton%20and%20Marco%20Volino&entry.1292438233=%20%20The%20field%20of%20Novel%20View%20Synthesis%20has%20been%20revolutionized%20by%203D%20Gaussian%0ASplatting%20%283DGS%29%2C%20which%20enables%20high-quality%20scene%20reconstruction%20that%20can%20be%0Arendered%20in%20real-time.%203DGS-based%20techniques%20typically%20suffer%20from%20high%20GPU%0Amemory%20and%20disk%20storage%20requirements%20which%20limits%20their%20practical%20application%0Aon%20consumer-grade%20devices.%20We%20propose%20Opti3DGS%2C%20a%20novel%20frequency-modulated%0Acoarse-to-fine%20optimization%20framework%20that%20aims%20to%20minimize%20the%20number%20of%0AGaussian%20primitives%20used%20to%20represent%20a%20scene%2C%20thus%20reducing%20memory%20and%20storage%0Ademands.%20Opti3DGS%20leverages%20image%20frequency%20modulation%2C%20initially%20enforcing%20a%0Acoarse%20scene%20representation%20and%20progressively%20refining%20it%20by%20modulating%0Afrequency%20details%20in%20the%20training%20images.%20On%20the%20baseline%203DGS%2C%20we%20demonstrate%0Aan%20average%20reduction%20of%2062%25%20in%20Gaussians%2C%20a%2040%25%20reduction%20in%20the%20training%20GPU%0Amemory%20requirements%20and%20a%2020%25%20reduction%20in%20optimization%20time%20without%0Asacrificing%20the%20visual%20quality.%20Furthermore%2C%20we%20show%20that%20our%20method%20integrates%0Aseamlessly%20with%20many%203DGS-based%20techniques%2C%20consistently%20reducing%20the%20number%20of%0AGaussian%20primitives%20while%20maintaining%2C%20and%20often%20improving%2C%20visual%20quality.%0AAdditionally%2C%20Opti3DGS%20inherently%20produces%20a%20level-of-detail%20scene%0Arepresentation%20at%20no%20extra%20cost%2C%20a%20natural%20byproduct%20of%20the%20optimization%0Apipeline.%20Results%20and%20code%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.14475v2&entry.124074799=Read"},
{"title": "Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision\n  Geometry Priors", "author": "Duo Zheng and Shijia Huang and Yanyang Li and Liwei Wang", "abstract": "  Previous research has investigated the application of Multimodal Large\nLanguage Models (MLLMs) in understanding 3D scenes by interpreting them as\nvideos. These approaches generally depend on comprehensive 3D data inputs, such\nas point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research,\nwe advance this field by enhancing the capability of MLLMs to understand and\nreason in 3D spaces directly from video data, without the need for additional\n3D input. We propose a novel and efficient method called the Video-3D Geometry\nLarge Language Model (VG LLM). Our approach utilizes a 3D visual geometry\nencoder to extract 3D prior information from video sequences. This information\nis then integrated with visual tokens and input into the MLLM. Extensive\nexperiments have shown that our method has achieved substantial improvements in\nvarious tasks related to 3D scene understanding and spatial reasoning, all\ndirectly learned from video sources. Impressively, our 4B model, which does not\nrely on explicit 3D data inputs, achieves competitive results compared to\nexisting state-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the\nVSI-Bench evaluations.\n", "link": "http://arxiv.org/abs/2505.24625v3", "date": "2025-10-22", "relevancy": 3.277, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.674}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.674}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20Videos%20for%203D%20World%3A%20Enhancing%20MLLMs%20with%203D%20Vision%0A%20%20Geometry%20Priors&body=Title%3A%20Learning%20from%20Videos%20for%203D%20World%3A%20Enhancing%20MLLMs%20with%203D%20Vision%0A%20%20Geometry%20Priors%0AAuthor%3A%20Duo%20Zheng%20and%20Shijia%20Huang%20and%20Yanyang%20Li%20and%20Liwei%20Wang%0AAbstract%3A%20%20%20Previous%20research%20has%20investigated%20the%20application%20of%20Multimodal%20Large%0ALanguage%20Models%20%28MLLMs%29%20in%20understanding%203D%20scenes%20by%20interpreting%20them%20as%0Avideos.%20These%20approaches%20generally%20depend%20on%20comprehensive%203D%20data%20inputs%2C%20such%0Aas%20point%20clouds%20or%20reconstructed%20Bird%27s-Eye%20View%20%28BEV%29%20maps.%20In%20our%20research%2C%0Awe%20advance%20this%20field%20by%20enhancing%20the%20capability%20of%20MLLMs%20to%20understand%20and%0Areason%20in%203D%20spaces%20directly%20from%20video%20data%2C%20without%20the%20need%20for%20additional%0A3D%20input.%20We%20propose%20a%20novel%20and%20efficient%20method%20called%20the%20Video-3D%20Geometry%0ALarge%20Language%20Model%20%28VG%20LLM%29.%20Our%20approach%20utilizes%20a%203D%20visual%20geometry%0Aencoder%20to%20extract%203D%20prior%20information%20from%20video%20sequences.%20This%20information%0Ais%20then%20integrated%20with%20visual%20tokens%20and%20input%20into%20the%20MLLM.%20Extensive%0Aexperiments%20have%20shown%20that%20our%20method%20has%20achieved%20substantial%20improvements%20in%0Avarious%20tasks%20related%20to%203D%20scene%20understanding%20and%20spatial%20reasoning%2C%20all%0Adirectly%20learned%20from%20video%20sources.%20Impressively%2C%20our%204B%20model%2C%20which%20does%20not%0Arely%20on%20explicit%203D%20data%20inputs%2C%20achieves%20competitive%20results%20compared%20to%0Aexisting%20state-of-the-art%20methods%2C%20and%20even%20surpasses%20the%20Gemini-1.5-Pro%20in%20the%0AVSI-Bench%20evaluations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24625v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520from%2520Videos%2520for%25203D%2520World%253A%2520Enhancing%2520MLLMs%2520with%25203D%2520Vision%250A%2520%2520Geometry%2520Priors%26entry.906535625%3DDuo%2520Zheng%2520and%2520Shijia%2520Huang%2520and%2520Yanyang%2520Li%2520and%2520Liwei%2520Wang%26entry.1292438233%3D%2520%2520Previous%2520research%2520has%2520investigated%2520the%2520application%2520of%2520Multimodal%2520Large%250ALanguage%2520Models%2520%2528MLLMs%2529%2520in%2520understanding%25203D%2520scenes%2520by%2520interpreting%2520them%2520as%250Avideos.%2520These%2520approaches%2520generally%2520depend%2520on%2520comprehensive%25203D%2520data%2520inputs%252C%2520such%250Aas%2520point%2520clouds%2520or%2520reconstructed%2520Bird%2527s-Eye%2520View%2520%2528BEV%2529%2520maps.%2520In%2520our%2520research%252C%250Awe%2520advance%2520this%2520field%2520by%2520enhancing%2520the%2520capability%2520of%2520MLLMs%2520to%2520understand%2520and%250Areason%2520in%25203D%2520spaces%2520directly%2520from%2520video%2520data%252C%2520without%2520the%2520need%2520for%2520additional%250A3D%2520input.%2520We%2520propose%2520a%2520novel%2520and%2520efficient%2520method%2520called%2520the%2520Video-3D%2520Geometry%250ALarge%2520Language%2520Model%2520%2528VG%2520LLM%2529.%2520Our%2520approach%2520utilizes%2520a%25203D%2520visual%2520geometry%250Aencoder%2520to%2520extract%25203D%2520prior%2520information%2520from%2520video%2520sequences.%2520This%2520information%250Ais%2520then%2520integrated%2520with%2520visual%2520tokens%2520and%2520input%2520into%2520the%2520MLLM.%2520Extensive%250Aexperiments%2520have%2520shown%2520that%2520our%2520method%2520has%2520achieved%2520substantial%2520improvements%2520in%250Avarious%2520tasks%2520related%2520to%25203D%2520scene%2520understanding%2520and%2520spatial%2520reasoning%252C%2520all%250Adirectly%2520learned%2520from%2520video%2520sources.%2520Impressively%252C%2520our%25204B%2520model%252C%2520which%2520does%2520not%250Arely%2520on%2520explicit%25203D%2520data%2520inputs%252C%2520achieves%2520competitive%2520results%2520compared%2520to%250Aexisting%2520state-of-the-art%2520methods%252C%2520and%2520even%2520surpasses%2520the%2520Gemini-1.5-Pro%2520in%2520the%250AVSI-Bench%2520evaluations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24625v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20Videos%20for%203D%20World%3A%20Enhancing%20MLLMs%20with%203D%20Vision%0A%20%20Geometry%20Priors&entry.906535625=Duo%20Zheng%20and%20Shijia%20Huang%20and%20Yanyang%20Li%20and%20Liwei%20Wang&entry.1292438233=%20%20Previous%20research%20has%20investigated%20the%20application%20of%20Multimodal%20Large%0ALanguage%20Models%20%28MLLMs%29%20in%20understanding%203D%20scenes%20by%20interpreting%20them%20as%0Avideos.%20These%20approaches%20generally%20depend%20on%20comprehensive%203D%20data%20inputs%2C%20such%0Aas%20point%20clouds%20or%20reconstructed%20Bird%27s-Eye%20View%20%28BEV%29%20maps.%20In%20our%20research%2C%0Awe%20advance%20this%20field%20by%20enhancing%20the%20capability%20of%20MLLMs%20to%20understand%20and%0Areason%20in%203D%20spaces%20directly%20from%20video%20data%2C%20without%20the%20need%20for%20additional%0A3D%20input.%20We%20propose%20a%20novel%20and%20efficient%20method%20called%20the%20Video-3D%20Geometry%0ALarge%20Language%20Model%20%28VG%20LLM%29.%20Our%20approach%20utilizes%20a%203D%20visual%20geometry%0Aencoder%20to%20extract%203D%20prior%20information%20from%20video%20sequences.%20This%20information%0Ais%20then%20integrated%20with%20visual%20tokens%20and%20input%20into%20the%20MLLM.%20Extensive%0Aexperiments%20have%20shown%20that%20our%20method%20has%20achieved%20substantial%20improvements%20in%0Avarious%20tasks%20related%20to%203D%20scene%20understanding%20and%20spatial%20reasoning%2C%20all%0Adirectly%20learned%20from%20video%20sources.%20Impressively%2C%20our%204B%20model%2C%20which%20does%20not%0Arely%20on%20explicit%203D%20data%20inputs%2C%20achieves%20competitive%20results%20compared%20to%0Aexisting%20state-of-the-art%20methods%2C%20and%20even%20surpasses%20the%20Gemini-1.5-Pro%20in%20the%0AVSI-Bench%20evaluations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24625v3&entry.124074799=Read"},
{"title": "Discretized Gaussian Representation for Tomographic Reconstruction", "author": "Shaokai Wu and Yuxiang Lu and Yapan Guo and Wei Ji and Suizhi Huang and Fengyu Yang and Shalayiding Sirejiding and Qichen He and Jing Tong and Yanbiao Ji and Yue Ding and Hongtao Lu", "abstract": "  Computed Tomography (CT) enables detailed cross-sectional imaging but\ncontinues to face challenges in balancing reconstruction quality and\ncomputational efficiency. While deep learning-based methods have significantly\nimproved image quality and noise reduction, they typically require large-scale\ntraining data and intensive computation. Recent advances in scene\nreconstruction, such as Neural Radiance Fields and 3D Gaussian Splatting, offer\nalternative perspectives but are not well-suited for direct volumetric CT\nreconstruction. In this work, we propose Discretized Gaussian Representation\n(DGR), a novel framework that reconstructs the 3D volume directly using a set\nof discretized Gaussian functions in an end-to-end manner. To further enhance\nefficiency, we introduce Fast Volume Reconstruction, a highly parallelized\ntechnique that aggregates Gaussian contributions into the voxel grid with\nminimal overhead. Extensive experiments on both real-world and synthetic\ndatasets demonstrate that DGR achieves superior reconstruction quality and\nruntime performance across various CT reconstruction scenarios. Our code is\npublicly available at https://github.com/wskingdom/DGR.\n", "link": "http://arxiv.org/abs/2411.04844v4", "date": "2025-10-22", "relevancy": 3.249, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.696}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6386}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6148}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discretized%20Gaussian%20Representation%20for%20Tomographic%20Reconstruction&body=Title%3A%20Discretized%20Gaussian%20Representation%20for%20Tomographic%20Reconstruction%0AAuthor%3A%20Shaokai%20Wu%20and%20Yuxiang%20Lu%20and%20Yapan%20Guo%20and%20Wei%20Ji%20and%20Suizhi%20Huang%20and%20Fengyu%20Yang%20and%20Shalayiding%20Sirejiding%20and%20Qichen%20He%20and%20Jing%20Tong%20and%20Yanbiao%20Ji%20and%20Yue%20Ding%20and%20Hongtao%20Lu%0AAbstract%3A%20%20%20Computed%20Tomography%20%28CT%29%20enables%20detailed%20cross-sectional%20imaging%20but%0Acontinues%20to%20face%20challenges%20in%20balancing%20reconstruction%20quality%20and%0Acomputational%20efficiency.%20While%20deep%20learning-based%20methods%20have%20significantly%0Aimproved%20image%20quality%20and%20noise%20reduction%2C%20they%20typically%20require%20large-scale%0Atraining%20data%20and%20intensive%20computation.%20Recent%20advances%20in%20scene%0Areconstruction%2C%20such%20as%20Neural%20Radiance%20Fields%20and%203D%20Gaussian%20Splatting%2C%20offer%0Aalternative%20perspectives%20but%20are%20not%20well-suited%20for%20direct%20volumetric%20CT%0Areconstruction.%20In%20this%20work%2C%20we%20propose%20Discretized%20Gaussian%20Representation%0A%28DGR%29%2C%20a%20novel%20framework%20that%20reconstructs%20the%203D%20volume%20directly%20using%20a%20set%0Aof%20discretized%20Gaussian%20functions%20in%20an%20end-to-end%20manner.%20To%20further%20enhance%0Aefficiency%2C%20we%20introduce%20Fast%20Volume%20Reconstruction%2C%20a%20highly%20parallelized%0Atechnique%20that%20aggregates%20Gaussian%20contributions%20into%20the%20voxel%20grid%20with%0Aminimal%20overhead.%20Extensive%20experiments%20on%20both%20real-world%20and%20synthetic%0Adatasets%20demonstrate%20that%20DGR%20achieves%20superior%20reconstruction%20quality%20and%0Aruntime%20performance%20across%20various%20CT%20reconstruction%20scenarios.%20Our%20code%20is%0Apublicly%20available%20at%20https%3A//github.com/wskingdom/DGR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04844v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscretized%2520Gaussian%2520Representation%2520for%2520Tomographic%2520Reconstruction%26entry.906535625%3DShaokai%2520Wu%2520and%2520Yuxiang%2520Lu%2520and%2520Yapan%2520Guo%2520and%2520Wei%2520Ji%2520and%2520Suizhi%2520Huang%2520and%2520Fengyu%2520Yang%2520and%2520Shalayiding%2520Sirejiding%2520and%2520Qichen%2520He%2520and%2520Jing%2520Tong%2520and%2520Yanbiao%2520Ji%2520and%2520Yue%2520Ding%2520and%2520Hongtao%2520Lu%26entry.1292438233%3D%2520%2520Computed%2520Tomography%2520%2528CT%2529%2520enables%2520detailed%2520cross-sectional%2520imaging%2520but%250Acontinues%2520to%2520face%2520challenges%2520in%2520balancing%2520reconstruction%2520quality%2520and%250Acomputational%2520efficiency.%2520While%2520deep%2520learning-based%2520methods%2520have%2520significantly%250Aimproved%2520image%2520quality%2520and%2520noise%2520reduction%252C%2520they%2520typically%2520require%2520large-scale%250Atraining%2520data%2520and%2520intensive%2520computation.%2520Recent%2520advances%2520in%2520scene%250Areconstruction%252C%2520such%2520as%2520Neural%2520Radiance%2520Fields%2520and%25203D%2520Gaussian%2520Splatting%252C%2520offer%250Aalternative%2520perspectives%2520but%2520are%2520not%2520well-suited%2520for%2520direct%2520volumetric%2520CT%250Areconstruction.%2520In%2520this%2520work%252C%2520we%2520propose%2520Discretized%2520Gaussian%2520Representation%250A%2528DGR%2529%252C%2520a%2520novel%2520framework%2520that%2520reconstructs%2520the%25203D%2520volume%2520directly%2520using%2520a%2520set%250Aof%2520discretized%2520Gaussian%2520functions%2520in%2520an%2520end-to-end%2520manner.%2520To%2520further%2520enhance%250Aefficiency%252C%2520we%2520introduce%2520Fast%2520Volume%2520Reconstruction%252C%2520a%2520highly%2520parallelized%250Atechnique%2520that%2520aggregates%2520Gaussian%2520contributions%2520into%2520the%2520voxel%2520grid%2520with%250Aminimal%2520overhead.%2520Extensive%2520experiments%2520on%2520both%2520real-world%2520and%2520synthetic%250Adatasets%2520demonstrate%2520that%2520DGR%2520achieves%2520superior%2520reconstruction%2520quality%2520and%250Aruntime%2520performance%2520across%2520various%2520CT%2520reconstruction%2520scenarios.%2520Our%2520code%2520is%250Apublicly%2520available%2520at%2520https%253A//github.com/wskingdom/DGR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04844v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discretized%20Gaussian%20Representation%20for%20Tomographic%20Reconstruction&entry.906535625=Shaokai%20Wu%20and%20Yuxiang%20Lu%20and%20Yapan%20Guo%20and%20Wei%20Ji%20and%20Suizhi%20Huang%20and%20Fengyu%20Yang%20and%20Shalayiding%20Sirejiding%20and%20Qichen%20He%20and%20Jing%20Tong%20and%20Yanbiao%20Ji%20and%20Yue%20Ding%20and%20Hongtao%20Lu&entry.1292438233=%20%20Computed%20Tomography%20%28CT%29%20enables%20detailed%20cross-sectional%20imaging%20but%0Acontinues%20to%20face%20challenges%20in%20balancing%20reconstruction%20quality%20and%0Acomputational%20efficiency.%20While%20deep%20learning-based%20methods%20have%20significantly%0Aimproved%20image%20quality%20and%20noise%20reduction%2C%20they%20typically%20require%20large-scale%0Atraining%20data%20and%20intensive%20computation.%20Recent%20advances%20in%20scene%0Areconstruction%2C%20such%20as%20Neural%20Radiance%20Fields%20and%203D%20Gaussian%20Splatting%2C%20offer%0Aalternative%20perspectives%20but%20are%20not%20well-suited%20for%20direct%20volumetric%20CT%0Areconstruction.%20In%20this%20work%2C%20we%20propose%20Discretized%20Gaussian%20Representation%0A%28DGR%29%2C%20a%20novel%20framework%20that%20reconstructs%20the%203D%20volume%20directly%20using%20a%20set%0Aof%20discretized%20Gaussian%20functions%20in%20an%20end-to-end%20manner.%20To%20further%20enhance%0Aefficiency%2C%20we%20introduce%20Fast%20Volume%20Reconstruction%2C%20a%20highly%20parallelized%0Atechnique%20that%20aggregates%20Gaussian%20contributions%20into%20the%20voxel%20grid%20with%0Aminimal%20overhead.%20Extensive%20experiments%20on%20both%20real-world%20and%20synthetic%0Adatasets%20demonstrate%20that%20DGR%20achieves%20superior%20reconstruction%20quality%20and%0Aruntime%20performance%20across%20various%20CT%20reconstruction%20scenarios.%20Our%20code%20is%0Apublicly%20available%20at%20https%3A//github.com/wskingdom/DGR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04844v4&entry.124074799=Read"},
{"title": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond\n  Semantic Dependency Constraints", "author": "Meiqi Wu and Jiashu Zhu and Xiaokun Feng and Chubin Chen and Chen Zhu and Bingze Song and Fangyuan Mao and Jiahong Wu and Xiangxiang Chu and Kaiqi Huang", "abstract": "  Video generation models have achieved remarkable progress, particularly\nexcelling in realistic scenarios; however, their performance degrades notably\nin imaginative scenarios. These prompts often involve rarely co-occurring\nconcepts with long-distance semantic relationships, falling outside training\ndistributions. Existing methods typically apply test-time scaling for improving\nvideo quality, but their fixed search spaces and static reward designs limit\nadaptability to imaginative scenarios. To fill this gap, we propose\nImagerySearch, a prompt-guided adaptive test-time search strategy that\ndynamically adjusts both the inference search space and reward function\naccording to semantic relationships in the prompt. This enables more coherent\nand visually plausible videos in challenging imaginative settings. To evaluate\nprogress in this direction, we introduce LDT-Bench, the first dedicated\nbenchmark for long-distance semantic prompts, consisting of 2,839 diverse\nconcept pairs and an automated protocol for assessing creative generation\ncapabilities. Extensive experiments show that ImagerySearch consistently\noutperforms strong video generation baselines and existing test-time scaling\napproaches on LDT-Bench, and achieves competitive improvements on VBench,\ndemonstrating its effectiveness across diverse prompt types. We will release\nLDT-Bench and code to facilitate future research on imaginative video\ngeneration.\n", "link": "http://arxiv.org/abs/2510.14847v2", "date": "2025-10-22", "relevancy": 2.9973, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6067}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5959}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5958}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ImagerySearch%3A%20Adaptive%20Test-Time%20Search%20for%20Video%20Generation%20Beyond%0A%20%20Semantic%20Dependency%20Constraints&body=Title%3A%20ImagerySearch%3A%20Adaptive%20Test-Time%20Search%20for%20Video%20Generation%20Beyond%0A%20%20Semantic%20Dependency%20Constraints%0AAuthor%3A%20Meiqi%20Wu%20and%20Jiashu%20Zhu%20and%20Xiaokun%20Feng%20and%20Chubin%20Chen%20and%20Chen%20Zhu%20and%20Bingze%20Song%20and%20Fangyuan%20Mao%20and%20Jiahong%20Wu%20and%20Xiangxiang%20Chu%20and%20Kaiqi%20Huang%0AAbstract%3A%20%20%20Video%20generation%20models%20have%20achieved%20remarkable%20progress%2C%20particularly%0Aexcelling%20in%20realistic%20scenarios%3B%20however%2C%20their%20performance%20degrades%20notably%0Ain%20imaginative%20scenarios.%20These%20prompts%20often%20involve%20rarely%20co-occurring%0Aconcepts%20with%20long-distance%20semantic%20relationships%2C%20falling%20outside%20training%0Adistributions.%20Existing%20methods%20typically%20apply%20test-time%20scaling%20for%20improving%0Avideo%20quality%2C%20but%20their%20fixed%20search%20spaces%20and%20static%20reward%20designs%20limit%0Aadaptability%20to%20imaginative%20scenarios.%20To%20fill%20this%20gap%2C%20we%20propose%0AImagerySearch%2C%20a%20prompt-guided%20adaptive%20test-time%20search%20strategy%20that%0Adynamically%20adjusts%20both%20the%20inference%20search%20space%20and%20reward%20function%0Aaccording%20to%20semantic%20relationships%20in%20the%20prompt.%20This%20enables%20more%20coherent%0Aand%20visually%20plausible%20videos%20in%20challenging%20imaginative%20settings.%20To%20evaluate%0Aprogress%20in%20this%20direction%2C%20we%20introduce%20LDT-Bench%2C%20the%20first%20dedicated%0Abenchmark%20for%20long-distance%20semantic%20prompts%2C%20consisting%20of%202%2C839%20diverse%0Aconcept%20pairs%20and%20an%20automated%20protocol%20for%20assessing%20creative%20generation%0Acapabilities.%20Extensive%20experiments%20show%20that%20ImagerySearch%20consistently%0Aoutperforms%20strong%20video%20generation%20baselines%20and%20existing%20test-time%20scaling%0Aapproaches%20on%20LDT-Bench%2C%20and%20achieves%20competitive%20improvements%20on%20VBench%2C%0Ademonstrating%20its%20effectiveness%20across%20diverse%20prompt%20types.%20We%20will%20release%0ALDT-Bench%20and%20code%20to%20facilitate%20future%20research%20on%20imaginative%20video%0Ageneration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14847v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImagerySearch%253A%2520Adaptive%2520Test-Time%2520Search%2520for%2520Video%2520Generation%2520Beyond%250A%2520%2520Semantic%2520Dependency%2520Constraints%26entry.906535625%3DMeiqi%2520Wu%2520and%2520Jiashu%2520Zhu%2520and%2520Xiaokun%2520Feng%2520and%2520Chubin%2520Chen%2520and%2520Chen%2520Zhu%2520and%2520Bingze%2520Song%2520and%2520Fangyuan%2520Mao%2520and%2520Jiahong%2520Wu%2520and%2520Xiangxiang%2520Chu%2520and%2520Kaiqi%2520Huang%26entry.1292438233%3D%2520%2520Video%2520generation%2520models%2520have%2520achieved%2520remarkable%2520progress%252C%2520particularly%250Aexcelling%2520in%2520realistic%2520scenarios%253B%2520however%252C%2520their%2520performance%2520degrades%2520notably%250Ain%2520imaginative%2520scenarios.%2520These%2520prompts%2520often%2520involve%2520rarely%2520co-occurring%250Aconcepts%2520with%2520long-distance%2520semantic%2520relationships%252C%2520falling%2520outside%2520training%250Adistributions.%2520Existing%2520methods%2520typically%2520apply%2520test-time%2520scaling%2520for%2520improving%250Avideo%2520quality%252C%2520but%2520their%2520fixed%2520search%2520spaces%2520and%2520static%2520reward%2520designs%2520limit%250Aadaptability%2520to%2520imaginative%2520scenarios.%2520To%2520fill%2520this%2520gap%252C%2520we%2520propose%250AImagerySearch%252C%2520a%2520prompt-guided%2520adaptive%2520test-time%2520search%2520strategy%2520that%250Adynamically%2520adjusts%2520both%2520the%2520inference%2520search%2520space%2520and%2520reward%2520function%250Aaccording%2520to%2520semantic%2520relationships%2520in%2520the%2520prompt.%2520This%2520enables%2520more%2520coherent%250Aand%2520visually%2520plausible%2520videos%2520in%2520challenging%2520imaginative%2520settings.%2520To%2520evaluate%250Aprogress%2520in%2520this%2520direction%252C%2520we%2520introduce%2520LDT-Bench%252C%2520the%2520first%2520dedicated%250Abenchmark%2520for%2520long-distance%2520semantic%2520prompts%252C%2520consisting%2520of%25202%252C839%2520diverse%250Aconcept%2520pairs%2520and%2520an%2520automated%2520protocol%2520for%2520assessing%2520creative%2520generation%250Acapabilities.%2520Extensive%2520experiments%2520show%2520that%2520ImagerySearch%2520consistently%250Aoutperforms%2520strong%2520video%2520generation%2520baselines%2520and%2520existing%2520test-time%2520scaling%250Aapproaches%2520on%2520LDT-Bench%252C%2520and%2520achieves%2520competitive%2520improvements%2520on%2520VBench%252C%250Ademonstrating%2520its%2520effectiveness%2520across%2520diverse%2520prompt%2520types.%2520We%2520will%2520release%250ALDT-Bench%2520and%2520code%2520to%2520facilitate%2520future%2520research%2520on%2520imaginative%2520video%250Ageneration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14847v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ImagerySearch%3A%20Adaptive%20Test-Time%20Search%20for%20Video%20Generation%20Beyond%0A%20%20Semantic%20Dependency%20Constraints&entry.906535625=Meiqi%20Wu%20and%20Jiashu%20Zhu%20and%20Xiaokun%20Feng%20and%20Chubin%20Chen%20and%20Chen%20Zhu%20and%20Bingze%20Song%20and%20Fangyuan%20Mao%20and%20Jiahong%20Wu%20and%20Xiangxiang%20Chu%20and%20Kaiqi%20Huang&entry.1292438233=%20%20Video%20generation%20models%20have%20achieved%20remarkable%20progress%2C%20particularly%0Aexcelling%20in%20realistic%20scenarios%3B%20however%2C%20their%20performance%20degrades%20notably%0Ain%20imaginative%20scenarios.%20These%20prompts%20often%20involve%20rarely%20co-occurring%0Aconcepts%20with%20long-distance%20semantic%20relationships%2C%20falling%20outside%20training%0Adistributions.%20Existing%20methods%20typically%20apply%20test-time%20scaling%20for%20improving%0Avideo%20quality%2C%20but%20their%20fixed%20search%20spaces%20and%20static%20reward%20designs%20limit%0Aadaptability%20to%20imaginative%20scenarios.%20To%20fill%20this%20gap%2C%20we%20propose%0AImagerySearch%2C%20a%20prompt-guided%20adaptive%20test-time%20search%20strategy%20that%0Adynamically%20adjusts%20both%20the%20inference%20search%20space%20and%20reward%20function%0Aaccording%20to%20semantic%20relationships%20in%20the%20prompt.%20This%20enables%20more%20coherent%0Aand%20visually%20plausible%20videos%20in%20challenging%20imaginative%20settings.%20To%20evaluate%0Aprogress%20in%20this%20direction%2C%20we%20introduce%20LDT-Bench%2C%20the%20first%20dedicated%0Abenchmark%20for%20long-distance%20semantic%20prompts%2C%20consisting%20of%202%2C839%20diverse%0Aconcept%20pairs%20and%20an%20automated%20protocol%20for%20assessing%20creative%20generation%0Acapabilities.%20Extensive%20experiments%20show%20that%20ImagerySearch%20consistently%0Aoutperforms%20strong%20video%20generation%20baselines%20and%20existing%20test-time%20scaling%0Aapproaches%20on%20LDT-Bench%2C%20and%20achieves%20competitive%20improvements%20on%20VBench%2C%0Ademonstrating%20its%20effectiveness%20across%20diverse%20prompt%20types.%20We%20will%20release%0ALDT-Bench%20and%20code%20to%20facilitate%20future%20research%20on%20imaginative%20video%0Ageneration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14847v2&entry.124074799=Read"},
{"title": "3D Visual Illusion Depth Estimation", "author": "Chengtang Yao and Zhidan Liu and Jiaxi Zeng and Lidong Yu and Yuwei Wu and Yunde Jia", "abstract": "  3D visual illusion is a perceptual phenomenon where a two-dimensional plane\nis manipulated to simulate three-dimensional spatial relationships, making a\nflat artwork or object look three-dimensional in the human visual system. In\nthis paper, we reveal that the machine visual system is also seriously fooled\nby 3D visual illusions, including monocular and binocular depth estimation. In\norder to explore and analyze the impact of 3D visual illusion on depth\nestimation, we collect a large dataset containing almost 3k scenes and 200k\nimages to train and evaluate SOTA monocular and binocular depth estimation\nmethods. We also propose a 3D visual illusion depth estimation framework that\nuses common sense from the vision language model to adaptively fuse depth from\nbinocular disparity and monocular depth. Experiments show that SOTA monocular,\nbinocular, and multi-view depth estimation approaches are all fooled by various\n3D visual illusions, while our method achieves SOTA performance.\n", "link": "http://arxiv.org/abs/2505.13061v4", "date": "2025-10-22", "relevancy": 2.8701, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5792}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5792}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5636}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Visual%20Illusion%20Depth%20Estimation&body=Title%3A%203D%20Visual%20Illusion%20Depth%20Estimation%0AAuthor%3A%20Chengtang%20Yao%20and%20Zhidan%20Liu%20and%20Jiaxi%20Zeng%20and%20Lidong%20Yu%20and%20Yuwei%20Wu%20and%20Yunde%20Jia%0AAbstract%3A%20%20%203D%20visual%20illusion%20is%20a%20perceptual%20phenomenon%20where%20a%20two-dimensional%20plane%0Ais%20manipulated%20to%20simulate%20three-dimensional%20spatial%20relationships%2C%20making%20a%0Aflat%20artwork%20or%20object%20look%20three-dimensional%20in%20the%20human%20visual%20system.%20In%0Athis%20paper%2C%20we%20reveal%20that%20the%20machine%20visual%20system%20is%20also%20seriously%20fooled%0Aby%203D%20visual%20illusions%2C%20including%20monocular%20and%20binocular%20depth%20estimation.%20In%0Aorder%20to%20explore%20and%20analyze%20the%20impact%20of%203D%20visual%20illusion%20on%20depth%0Aestimation%2C%20we%20collect%20a%20large%20dataset%20containing%20almost%203k%20scenes%20and%20200k%0Aimages%20to%20train%20and%20evaluate%20SOTA%20monocular%20and%20binocular%20depth%20estimation%0Amethods.%20We%20also%20propose%20a%203D%20visual%20illusion%20depth%20estimation%20framework%20that%0Auses%20common%20sense%20from%20the%20vision%20language%20model%20to%20adaptively%20fuse%20depth%20from%0Abinocular%20disparity%20and%20monocular%20depth.%20Experiments%20show%20that%20SOTA%20monocular%2C%0Abinocular%2C%20and%20multi-view%20depth%20estimation%20approaches%20are%20all%20fooled%20by%20various%0A3D%20visual%20illusions%2C%20while%20our%20method%20achieves%20SOTA%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13061v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Visual%2520Illusion%2520Depth%2520Estimation%26entry.906535625%3DChengtang%2520Yao%2520and%2520Zhidan%2520Liu%2520and%2520Jiaxi%2520Zeng%2520and%2520Lidong%2520Yu%2520and%2520Yuwei%2520Wu%2520and%2520Yunde%2520Jia%26entry.1292438233%3D%2520%25203D%2520visual%2520illusion%2520is%2520a%2520perceptual%2520phenomenon%2520where%2520a%2520two-dimensional%2520plane%250Ais%2520manipulated%2520to%2520simulate%2520three-dimensional%2520spatial%2520relationships%252C%2520making%2520a%250Aflat%2520artwork%2520or%2520object%2520look%2520three-dimensional%2520in%2520the%2520human%2520visual%2520system.%2520In%250Athis%2520paper%252C%2520we%2520reveal%2520that%2520the%2520machine%2520visual%2520system%2520is%2520also%2520seriously%2520fooled%250Aby%25203D%2520visual%2520illusions%252C%2520including%2520monocular%2520and%2520binocular%2520depth%2520estimation.%2520In%250Aorder%2520to%2520explore%2520and%2520analyze%2520the%2520impact%2520of%25203D%2520visual%2520illusion%2520on%2520depth%250Aestimation%252C%2520we%2520collect%2520a%2520large%2520dataset%2520containing%2520almost%25203k%2520scenes%2520and%2520200k%250Aimages%2520to%2520train%2520and%2520evaluate%2520SOTA%2520monocular%2520and%2520binocular%2520depth%2520estimation%250Amethods.%2520We%2520also%2520propose%2520a%25203D%2520visual%2520illusion%2520depth%2520estimation%2520framework%2520that%250Auses%2520common%2520sense%2520from%2520the%2520vision%2520language%2520model%2520to%2520adaptively%2520fuse%2520depth%2520from%250Abinocular%2520disparity%2520and%2520monocular%2520depth.%2520Experiments%2520show%2520that%2520SOTA%2520monocular%252C%250Abinocular%252C%2520and%2520multi-view%2520depth%2520estimation%2520approaches%2520are%2520all%2520fooled%2520by%2520various%250A3D%2520visual%2520illusions%252C%2520while%2520our%2520method%2520achieves%2520SOTA%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13061v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Visual%20Illusion%20Depth%20Estimation&entry.906535625=Chengtang%20Yao%20and%20Zhidan%20Liu%20and%20Jiaxi%20Zeng%20and%20Lidong%20Yu%20and%20Yuwei%20Wu%20and%20Yunde%20Jia&entry.1292438233=%20%203D%20visual%20illusion%20is%20a%20perceptual%20phenomenon%20where%20a%20two-dimensional%20plane%0Ais%20manipulated%20to%20simulate%20three-dimensional%20spatial%20relationships%2C%20making%20a%0Aflat%20artwork%20or%20object%20look%20three-dimensional%20in%20the%20human%20visual%20system.%20In%0Athis%20paper%2C%20we%20reveal%20that%20the%20machine%20visual%20system%20is%20also%20seriously%20fooled%0Aby%203D%20visual%20illusions%2C%20including%20monocular%20and%20binocular%20depth%20estimation.%20In%0Aorder%20to%20explore%20and%20analyze%20the%20impact%20of%203D%20visual%20illusion%20on%20depth%0Aestimation%2C%20we%20collect%20a%20large%20dataset%20containing%20almost%203k%20scenes%20and%20200k%0Aimages%20to%20train%20and%20evaluate%20SOTA%20monocular%20and%20binocular%20depth%20estimation%0Amethods.%20We%20also%20propose%20a%203D%20visual%20illusion%20depth%20estimation%20framework%20that%0Auses%20common%20sense%20from%20the%20vision%20language%20model%20to%20adaptively%20fuse%20depth%20from%0Abinocular%20disparity%20and%20monocular%20depth.%20Experiments%20show%20that%20SOTA%20monocular%2C%0Abinocular%2C%20and%20multi-view%20depth%20estimation%20approaches%20are%20all%20fooled%20by%20various%0A3D%20visual%20illusions%2C%20while%20our%20method%20achieves%20SOTA%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13061v4&entry.124074799=Read"},
{"title": "XBench: A Comprehensive Benchmark for Visual-Language Explanations in\n  Chest Radiography", "author": "Haozhe Luo and Shelley Zixin Shu and Ziyu Zhou and Sebastian Otalora and Mauricio Reyes", "abstract": "  Vision-language models (VLMs) have recently shown remarkable zero-shot\nperformance in medical image understanding, yet their grounding ability, the\nextent to which textual concepts align with visual evidence, remains\nunderexplored. In the medical domain, however, reliable grounding is essential\nfor interpretability and clinical adoption. In this work, we present the first\nsystematic benchmark for evaluating cross-modal interpretability in chest\nX-rays across seven CLIP-style VLM variants. We generate visual explanations\nusing cross-attention and similarity-based localization maps, and\nquantitatively assess their alignment with radiologist-annotated regions across\nmultiple pathologies. Our analysis reveals that: (1) while all VLM variants\ndemonstrate reasonable localization for large and well-defined pathologies,\ntheir performance substantially degrades for small or diffuse lesions; (2)\nmodels that are pretrained on chest X-ray-specific datasets exhibit improved\nalignment compared to those trained on general-domain data. (3) The overall\nrecognition ability and grounding ability of the model are strongly correlated.\nThese findings underscore that current VLMs, despite their strong recognition\nability, still fall short in clinically reliable grounding, highlighting the\nneed for targeted interpretability benchmarks before deployment in medical\npractice. XBench code is available at\nhttps://github.com/Roypic/Benchmarkingattention\n", "link": "http://arxiv.org/abs/2510.19599v1", "date": "2025-10-22", "relevancy": 2.8533, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5969}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5969}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XBench%3A%20A%20Comprehensive%20Benchmark%20for%20Visual-Language%20Explanations%20in%0A%20%20Chest%20Radiography&body=Title%3A%20XBench%3A%20A%20Comprehensive%20Benchmark%20for%20Visual-Language%20Explanations%20in%0A%20%20Chest%20Radiography%0AAuthor%3A%20Haozhe%20Luo%20and%20Shelley%20Zixin%20Shu%20and%20Ziyu%20Zhou%20and%20Sebastian%20Otalora%20and%20Mauricio%20Reyes%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20have%20recently%20shown%20remarkable%20zero-shot%0Aperformance%20in%20medical%20image%20understanding%2C%20yet%20their%20grounding%20ability%2C%20the%0Aextent%20to%20which%20textual%20concepts%20align%20with%20visual%20evidence%2C%20remains%0Aunderexplored.%20In%20the%20medical%20domain%2C%20however%2C%20reliable%20grounding%20is%20essential%0Afor%20interpretability%20and%20clinical%20adoption.%20In%20this%20work%2C%20we%20present%20the%20first%0Asystematic%20benchmark%20for%20evaluating%20cross-modal%20interpretability%20in%20chest%0AX-rays%20across%20seven%20CLIP-style%20VLM%20variants.%20We%20generate%20visual%20explanations%0Ausing%20cross-attention%20and%20similarity-based%20localization%20maps%2C%20and%0Aquantitatively%20assess%20their%20alignment%20with%20radiologist-annotated%20regions%20across%0Amultiple%20pathologies.%20Our%20analysis%20reveals%20that%3A%20%281%29%20while%20all%20VLM%20variants%0Ademonstrate%20reasonable%20localization%20for%20large%20and%20well-defined%20pathologies%2C%0Atheir%20performance%20substantially%20degrades%20for%20small%20or%20diffuse%20lesions%3B%20%282%29%0Amodels%20that%20are%20pretrained%20on%20chest%20X-ray-specific%20datasets%20exhibit%20improved%0Aalignment%20compared%20to%20those%20trained%20on%20general-domain%20data.%20%283%29%20The%20overall%0Arecognition%20ability%20and%20grounding%20ability%20of%20the%20model%20are%20strongly%20correlated.%0AThese%20findings%20underscore%20that%20current%20VLMs%2C%20despite%20their%20strong%20recognition%0Aability%2C%20still%20fall%20short%20in%20clinically%20reliable%20grounding%2C%20highlighting%20the%0Aneed%20for%20targeted%20interpretability%20benchmarks%20before%20deployment%20in%20medical%0Apractice.%20XBench%20code%20is%20available%20at%0Ahttps%3A//github.com/Roypic/Benchmarkingattention%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19599v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXBench%253A%2520A%2520Comprehensive%2520Benchmark%2520for%2520Visual-Language%2520Explanations%2520in%250A%2520%2520Chest%2520Radiography%26entry.906535625%3DHaozhe%2520Luo%2520and%2520Shelley%2520Zixin%2520Shu%2520and%2520Ziyu%2520Zhou%2520and%2520Sebastian%2520Otalora%2520and%2520Mauricio%2520Reyes%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520have%2520recently%2520shown%2520remarkable%2520zero-shot%250Aperformance%2520in%2520medical%2520image%2520understanding%252C%2520yet%2520their%2520grounding%2520ability%252C%2520the%250Aextent%2520to%2520which%2520textual%2520concepts%2520align%2520with%2520visual%2520evidence%252C%2520remains%250Aunderexplored.%2520In%2520the%2520medical%2520domain%252C%2520however%252C%2520reliable%2520grounding%2520is%2520essential%250Afor%2520interpretability%2520and%2520clinical%2520adoption.%2520In%2520this%2520work%252C%2520we%2520present%2520the%2520first%250Asystematic%2520benchmark%2520for%2520evaluating%2520cross-modal%2520interpretability%2520in%2520chest%250AX-rays%2520across%2520seven%2520CLIP-style%2520VLM%2520variants.%2520We%2520generate%2520visual%2520explanations%250Ausing%2520cross-attention%2520and%2520similarity-based%2520localization%2520maps%252C%2520and%250Aquantitatively%2520assess%2520their%2520alignment%2520with%2520radiologist-annotated%2520regions%2520across%250Amultiple%2520pathologies.%2520Our%2520analysis%2520reveals%2520that%253A%2520%25281%2529%2520while%2520all%2520VLM%2520variants%250Ademonstrate%2520reasonable%2520localization%2520for%2520large%2520and%2520well-defined%2520pathologies%252C%250Atheir%2520performance%2520substantially%2520degrades%2520for%2520small%2520or%2520diffuse%2520lesions%253B%2520%25282%2529%250Amodels%2520that%2520are%2520pretrained%2520on%2520chest%2520X-ray-specific%2520datasets%2520exhibit%2520improved%250Aalignment%2520compared%2520to%2520those%2520trained%2520on%2520general-domain%2520data.%2520%25283%2529%2520The%2520overall%250Arecognition%2520ability%2520and%2520grounding%2520ability%2520of%2520the%2520model%2520are%2520strongly%2520correlated.%250AThese%2520findings%2520underscore%2520that%2520current%2520VLMs%252C%2520despite%2520their%2520strong%2520recognition%250Aability%252C%2520still%2520fall%2520short%2520in%2520clinically%2520reliable%2520grounding%252C%2520highlighting%2520the%250Aneed%2520for%2520targeted%2520interpretability%2520benchmarks%2520before%2520deployment%2520in%2520medical%250Apractice.%2520XBench%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Roypic/Benchmarkingattention%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19599v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XBench%3A%20A%20Comprehensive%20Benchmark%20for%20Visual-Language%20Explanations%20in%0A%20%20Chest%20Radiography&entry.906535625=Haozhe%20Luo%20and%20Shelley%20Zixin%20Shu%20and%20Ziyu%20Zhou%20and%20Sebastian%20Otalora%20and%20Mauricio%20Reyes&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20have%20recently%20shown%20remarkable%20zero-shot%0Aperformance%20in%20medical%20image%20understanding%2C%20yet%20their%20grounding%20ability%2C%20the%0Aextent%20to%20which%20textual%20concepts%20align%20with%20visual%20evidence%2C%20remains%0Aunderexplored.%20In%20the%20medical%20domain%2C%20however%2C%20reliable%20grounding%20is%20essential%0Afor%20interpretability%20and%20clinical%20adoption.%20In%20this%20work%2C%20we%20present%20the%20first%0Asystematic%20benchmark%20for%20evaluating%20cross-modal%20interpretability%20in%20chest%0AX-rays%20across%20seven%20CLIP-style%20VLM%20variants.%20We%20generate%20visual%20explanations%0Ausing%20cross-attention%20and%20similarity-based%20localization%20maps%2C%20and%0Aquantitatively%20assess%20their%20alignment%20with%20radiologist-annotated%20regions%20across%0Amultiple%20pathologies.%20Our%20analysis%20reveals%20that%3A%20%281%29%20while%20all%20VLM%20variants%0Ademonstrate%20reasonable%20localization%20for%20large%20and%20well-defined%20pathologies%2C%0Atheir%20performance%20substantially%20degrades%20for%20small%20or%20diffuse%20lesions%3B%20%282%29%0Amodels%20that%20are%20pretrained%20on%20chest%20X-ray-specific%20datasets%20exhibit%20improved%0Aalignment%20compared%20to%20those%20trained%20on%20general-domain%20data.%20%283%29%20The%20overall%0Arecognition%20ability%20and%20grounding%20ability%20of%20the%20model%20are%20strongly%20correlated.%0AThese%20findings%20underscore%20that%20current%20VLMs%2C%20despite%20their%20strong%20recognition%0Aability%2C%20still%20fall%20short%20in%20clinically%20reliable%20grounding%2C%20highlighting%20the%0Aneed%20for%20targeted%20interpretability%20benchmarks%20before%20deployment%20in%20medical%0Apractice.%20XBench%20code%20is%20available%20at%0Ahttps%3A//github.com/Roypic/Benchmarkingattention%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19599v1&entry.124074799=Read"},
{"title": "WikiVideo: Article Generation from Multiple Videos", "author": "Alexander Martin and Reno Kriz and William Gantt Walden and Kate Sanders and Hannah Recknor and Eugene Yang and Francis Ferraro and Benjamin Van Durme", "abstract": "  We introduce the task of grounded article generation with the goal of\ncreating a Wikipedia-style article from multiple diverse videos about\nreal-world events -- from natural disasters to political elections -- where all\nthe information in the article is supported by video evidence. Videos are\nintuitive sources for retrieval-augmented generation (RAG), but most\ncontemporary RAG workflows focus heavily on text while existing methods for\nvideo-based summarization focus on low-level scene understanding rather than\nhigh-level event semantics. To close this gap, we introduce WikiVideo, a\nbenchmark consisting of expert-written articles and densely annotated videos\nthat provide evidence for articles' claims, facilitating the integration of\nvideo into RAG pipelines and enabling the creation of in-depth content that is\ngrounded in multimodal sources. We further propose Collaborative Article\nGeneration (CAG), a novel interactive method for article creation from multiple\nvideos. CAG leverages an iterative interaction between an r1-style reasoning\nmodel and a VideoLLM to draw higher-level inferences about the target event\nthan is possible with VideoLLMs alone, which fixate on low-level visual\nfeatures. We benchmark state-of-the-art VideoLLMs and CAG in both oracle\nretrieval and RAG settings and find that CAG consistently outperforms\nalternative methods, while suggesting intriguing avenues for future work.\n", "link": "http://arxiv.org/abs/2504.00939v2", "date": "2025-10-22", "relevancy": 2.8451, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.604}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5547}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WikiVideo%3A%20Article%20Generation%20from%20Multiple%20Videos&body=Title%3A%20WikiVideo%3A%20Article%20Generation%20from%20Multiple%20Videos%0AAuthor%3A%20Alexander%20Martin%20and%20Reno%20Kriz%20and%20William%20Gantt%20Walden%20and%20Kate%20Sanders%20and%20Hannah%20Recknor%20and%20Eugene%20Yang%20and%20Francis%20Ferraro%20and%20Benjamin%20Van%20Durme%0AAbstract%3A%20%20%20We%20introduce%20the%20task%20of%20grounded%20article%20generation%20with%20the%20goal%20of%0Acreating%20a%20Wikipedia-style%20article%20from%20multiple%20diverse%20videos%20about%0Areal-world%20events%20--%20from%20natural%20disasters%20to%20political%20elections%20--%20where%20all%0Athe%20information%20in%20the%20article%20is%20supported%20by%20video%20evidence.%20Videos%20are%0Aintuitive%20sources%20for%20retrieval-augmented%20generation%20%28RAG%29%2C%20but%20most%0Acontemporary%20RAG%20workflows%20focus%20heavily%20on%20text%20while%20existing%20methods%20for%0Avideo-based%20summarization%20focus%20on%20low-level%20scene%20understanding%20rather%20than%0Ahigh-level%20event%20semantics.%20To%20close%20this%20gap%2C%20we%20introduce%20WikiVideo%2C%20a%0Abenchmark%20consisting%20of%20expert-written%20articles%20and%20densely%20annotated%20videos%0Athat%20provide%20evidence%20for%20articles%27%20claims%2C%20facilitating%20the%20integration%20of%0Avideo%20into%20RAG%20pipelines%20and%20enabling%20the%20creation%20of%20in-depth%20content%20that%20is%0Agrounded%20in%20multimodal%20sources.%20We%20further%20propose%20Collaborative%20Article%0AGeneration%20%28CAG%29%2C%20a%20novel%20interactive%20method%20for%20article%20creation%20from%20multiple%0Avideos.%20CAG%20leverages%20an%20iterative%20interaction%20between%20an%20r1-style%20reasoning%0Amodel%20and%20a%20VideoLLM%20to%20draw%20higher-level%20inferences%20about%20the%20target%20event%0Athan%20is%20possible%20with%20VideoLLMs%20alone%2C%20which%20fixate%20on%20low-level%20visual%0Afeatures.%20We%20benchmark%20state-of-the-art%20VideoLLMs%20and%20CAG%20in%20both%20oracle%0Aretrieval%20and%20RAG%20settings%20and%20find%20that%20CAG%20consistently%20outperforms%0Aalternative%20methods%2C%20while%20suggesting%20intriguing%20avenues%20for%20future%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.00939v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWikiVideo%253A%2520Article%2520Generation%2520from%2520Multiple%2520Videos%26entry.906535625%3DAlexander%2520Martin%2520and%2520Reno%2520Kriz%2520and%2520William%2520Gantt%2520Walden%2520and%2520Kate%2520Sanders%2520and%2520Hannah%2520Recknor%2520and%2520Eugene%2520Yang%2520and%2520Francis%2520Ferraro%2520and%2520Benjamin%2520Van%2520Durme%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520task%2520of%2520grounded%2520article%2520generation%2520with%2520the%2520goal%2520of%250Acreating%2520a%2520Wikipedia-style%2520article%2520from%2520multiple%2520diverse%2520videos%2520about%250Areal-world%2520events%2520--%2520from%2520natural%2520disasters%2520to%2520political%2520elections%2520--%2520where%2520all%250Athe%2520information%2520in%2520the%2520article%2520is%2520supported%2520by%2520video%2520evidence.%2520Videos%2520are%250Aintuitive%2520sources%2520for%2520retrieval-augmented%2520generation%2520%2528RAG%2529%252C%2520but%2520most%250Acontemporary%2520RAG%2520workflows%2520focus%2520heavily%2520on%2520text%2520while%2520existing%2520methods%2520for%250Avideo-based%2520summarization%2520focus%2520on%2520low-level%2520scene%2520understanding%2520rather%2520than%250Ahigh-level%2520event%2520semantics.%2520To%2520close%2520this%2520gap%252C%2520we%2520introduce%2520WikiVideo%252C%2520a%250Abenchmark%2520consisting%2520of%2520expert-written%2520articles%2520and%2520densely%2520annotated%2520videos%250Athat%2520provide%2520evidence%2520for%2520articles%2527%2520claims%252C%2520facilitating%2520the%2520integration%2520of%250Avideo%2520into%2520RAG%2520pipelines%2520and%2520enabling%2520the%2520creation%2520of%2520in-depth%2520content%2520that%2520is%250Agrounded%2520in%2520multimodal%2520sources.%2520We%2520further%2520propose%2520Collaborative%2520Article%250AGeneration%2520%2528CAG%2529%252C%2520a%2520novel%2520interactive%2520method%2520for%2520article%2520creation%2520from%2520multiple%250Avideos.%2520CAG%2520leverages%2520an%2520iterative%2520interaction%2520between%2520an%2520r1-style%2520reasoning%250Amodel%2520and%2520a%2520VideoLLM%2520to%2520draw%2520higher-level%2520inferences%2520about%2520the%2520target%2520event%250Athan%2520is%2520possible%2520with%2520VideoLLMs%2520alone%252C%2520which%2520fixate%2520on%2520low-level%2520visual%250Afeatures.%2520We%2520benchmark%2520state-of-the-art%2520VideoLLMs%2520and%2520CAG%2520in%2520both%2520oracle%250Aretrieval%2520and%2520RAG%2520settings%2520and%2520find%2520that%2520CAG%2520consistently%2520outperforms%250Aalternative%2520methods%252C%2520while%2520suggesting%2520intriguing%2520avenues%2520for%2520future%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.00939v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WikiVideo%3A%20Article%20Generation%20from%20Multiple%20Videos&entry.906535625=Alexander%20Martin%20and%20Reno%20Kriz%20and%20William%20Gantt%20Walden%20and%20Kate%20Sanders%20and%20Hannah%20Recknor%20and%20Eugene%20Yang%20and%20Francis%20Ferraro%20and%20Benjamin%20Van%20Durme&entry.1292438233=%20%20We%20introduce%20the%20task%20of%20grounded%20article%20generation%20with%20the%20goal%20of%0Acreating%20a%20Wikipedia-style%20article%20from%20multiple%20diverse%20videos%20about%0Areal-world%20events%20--%20from%20natural%20disasters%20to%20political%20elections%20--%20where%20all%0Athe%20information%20in%20the%20article%20is%20supported%20by%20video%20evidence.%20Videos%20are%0Aintuitive%20sources%20for%20retrieval-augmented%20generation%20%28RAG%29%2C%20but%20most%0Acontemporary%20RAG%20workflows%20focus%20heavily%20on%20text%20while%20existing%20methods%20for%0Avideo-based%20summarization%20focus%20on%20low-level%20scene%20understanding%20rather%20than%0Ahigh-level%20event%20semantics.%20To%20close%20this%20gap%2C%20we%20introduce%20WikiVideo%2C%20a%0Abenchmark%20consisting%20of%20expert-written%20articles%20and%20densely%20annotated%20videos%0Athat%20provide%20evidence%20for%20articles%27%20claims%2C%20facilitating%20the%20integration%20of%0Avideo%20into%20RAG%20pipelines%20and%20enabling%20the%20creation%20of%20in-depth%20content%20that%20is%0Agrounded%20in%20multimodal%20sources.%20We%20further%20propose%20Collaborative%20Article%0AGeneration%20%28CAG%29%2C%20a%20novel%20interactive%20method%20for%20article%20creation%20from%20multiple%0Avideos.%20CAG%20leverages%20an%20iterative%20interaction%20between%20an%20r1-style%20reasoning%0Amodel%20and%20a%20VideoLLM%20to%20draw%20higher-level%20inferences%20about%20the%20target%20event%0Athan%20is%20possible%20with%20VideoLLMs%20alone%2C%20which%20fixate%20on%20low-level%20visual%0Afeatures.%20We%20benchmark%20state-of-the-art%20VideoLLMs%20and%20CAG%20in%20both%20oracle%0Aretrieval%20and%20RAG%20settings%20and%20find%20that%20CAG%20consistently%20outperforms%0Aalternative%20methods%2C%20while%20suggesting%20intriguing%20avenues%20for%20future%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.00939v2&entry.124074799=Read"},
{"title": "Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning\n  Segmentation", "author": "Su Ho Han and Jeongseok Hyun and Pilhyeon Lee and Minho Shim and Dongyoon Wee and Seon Joo Kim", "abstract": "  Multimodal large language models (MLLMs) demonstrate strong video\nunderstanding by attending to visual tokens relevant to textual queries. To\ndirectly adapt this for localization in a training-free manner, we cast video\nreasoning segmentation as a video QA task and extract attention maps via\nrollout mechanism. However, raw attention maps are noisy and poorly aligned\nwith object regions. We propose Decomposed Attention Fusion (DecAF), which\nrefines these maps through two mechanisms: (1) contrastive object-background\nfusion and (2) complementary video-frame fusion. This method suppresses\nirrelevant activations and enhances object-focused cues, enabling direct\nconversion of attention maps into coarse segmentation masks. In addition, we\nintroduce attention-guided SAM2 prompting for obtaining fine-grained masks.\nUnlike existing methods that jointly train MLLMs with SAM, our method operates\nentirely without retraining. DecAF outperforms training-free methods and\nachieves performance comparable to training-based methods on both referring and\nreasoning VOS benchmarks. The code will be available at\nhttps://github.com/HYUNJS/DecAF.\n", "link": "http://arxiv.org/abs/2510.19592v1", "date": "2025-10-22", "relevancy": 2.8386, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5689}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5689}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decomposed%20Attention%20Fusion%20in%20MLLMs%20for%20Training-Free%20Video%20Reasoning%0A%20%20Segmentation&body=Title%3A%20Decomposed%20Attention%20Fusion%20in%20MLLMs%20for%20Training-Free%20Video%20Reasoning%0A%20%20Segmentation%0AAuthor%3A%20Su%20Ho%20Han%20and%20Jeongseok%20Hyun%20and%20Pilhyeon%20Lee%20and%20Minho%20Shim%20and%20Dongyoon%20Wee%20and%20Seon%20Joo%20Kim%0AAbstract%3A%20%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20demonstrate%20strong%20video%0Aunderstanding%20by%20attending%20to%20visual%20tokens%20relevant%20to%20textual%20queries.%20To%0Adirectly%20adapt%20this%20for%20localization%20in%20a%20training-free%20manner%2C%20we%20cast%20video%0Areasoning%20segmentation%20as%20a%20video%20QA%20task%20and%20extract%20attention%20maps%20via%0Arollout%20mechanism.%20However%2C%20raw%20attention%20maps%20are%20noisy%20and%20poorly%20aligned%0Awith%20object%20regions.%20We%20propose%20Decomposed%20Attention%20Fusion%20%28DecAF%29%2C%20which%0Arefines%20these%20maps%20through%20two%20mechanisms%3A%20%281%29%20contrastive%20object-background%0Afusion%20and%20%282%29%20complementary%20video-frame%20fusion.%20This%20method%20suppresses%0Airrelevant%20activations%20and%20enhances%20object-focused%20cues%2C%20enabling%20direct%0Aconversion%20of%20attention%20maps%20into%20coarse%20segmentation%20masks.%20In%20addition%2C%20we%0Aintroduce%20attention-guided%20SAM2%20prompting%20for%20obtaining%20fine-grained%20masks.%0AUnlike%20existing%20methods%20that%20jointly%20train%20MLLMs%20with%20SAM%2C%20our%20method%20operates%0Aentirely%20without%20retraining.%20DecAF%20outperforms%20training-free%20methods%20and%0Aachieves%20performance%20comparable%20to%20training-based%20methods%20on%20both%20referring%20and%0Areasoning%20VOS%20benchmarks.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/HYUNJS/DecAF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19592v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecomposed%2520Attention%2520Fusion%2520in%2520MLLMs%2520for%2520Training-Free%2520Video%2520Reasoning%250A%2520%2520Segmentation%26entry.906535625%3DSu%2520Ho%2520Han%2520and%2520Jeongseok%2520Hyun%2520and%2520Pilhyeon%2520Lee%2520and%2520Minho%2520Shim%2520and%2520Dongyoon%2520Wee%2520and%2520Seon%2520Joo%2520Kim%26entry.1292438233%3D%2520%2520Multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520demonstrate%2520strong%2520video%250Aunderstanding%2520by%2520attending%2520to%2520visual%2520tokens%2520relevant%2520to%2520textual%2520queries.%2520To%250Adirectly%2520adapt%2520this%2520for%2520localization%2520in%2520a%2520training-free%2520manner%252C%2520we%2520cast%2520video%250Areasoning%2520segmentation%2520as%2520a%2520video%2520QA%2520task%2520and%2520extract%2520attention%2520maps%2520via%250Arollout%2520mechanism.%2520However%252C%2520raw%2520attention%2520maps%2520are%2520noisy%2520and%2520poorly%2520aligned%250Awith%2520object%2520regions.%2520We%2520propose%2520Decomposed%2520Attention%2520Fusion%2520%2528DecAF%2529%252C%2520which%250Arefines%2520these%2520maps%2520through%2520two%2520mechanisms%253A%2520%25281%2529%2520contrastive%2520object-background%250Afusion%2520and%2520%25282%2529%2520complementary%2520video-frame%2520fusion.%2520This%2520method%2520suppresses%250Airrelevant%2520activations%2520and%2520enhances%2520object-focused%2520cues%252C%2520enabling%2520direct%250Aconversion%2520of%2520attention%2520maps%2520into%2520coarse%2520segmentation%2520masks.%2520In%2520addition%252C%2520we%250Aintroduce%2520attention-guided%2520SAM2%2520prompting%2520for%2520obtaining%2520fine-grained%2520masks.%250AUnlike%2520existing%2520methods%2520that%2520jointly%2520train%2520MLLMs%2520with%2520SAM%252C%2520our%2520method%2520operates%250Aentirely%2520without%2520retraining.%2520DecAF%2520outperforms%2520training-free%2520methods%2520and%250Aachieves%2520performance%2520comparable%2520to%2520training-based%2520methods%2520on%2520both%2520referring%2520and%250Areasoning%2520VOS%2520benchmarks.%2520The%2520code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/HYUNJS/DecAF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19592v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decomposed%20Attention%20Fusion%20in%20MLLMs%20for%20Training-Free%20Video%20Reasoning%0A%20%20Segmentation&entry.906535625=Su%20Ho%20Han%20and%20Jeongseok%20Hyun%20and%20Pilhyeon%20Lee%20and%20Minho%20Shim%20and%20Dongyoon%20Wee%20and%20Seon%20Joo%20Kim&entry.1292438233=%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20demonstrate%20strong%20video%0Aunderstanding%20by%20attending%20to%20visual%20tokens%20relevant%20to%20textual%20queries.%20To%0Adirectly%20adapt%20this%20for%20localization%20in%20a%20training-free%20manner%2C%20we%20cast%20video%0Areasoning%20segmentation%20as%20a%20video%20QA%20task%20and%20extract%20attention%20maps%20via%0Arollout%20mechanism.%20However%2C%20raw%20attention%20maps%20are%20noisy%20and%20poorly%20aligned%0Awith%20object%20regions.%20We%20propose%20Decomposed%20Attention%20Fusion%20%28DecAF%29%2C%20which%0Arefines%20these%20maps%20through%20two%20mechanisms%3A%20%281%29%20contrastive%20object-background%0Afusion%20and%20%282%29%20complementary%20video-frame%20fusion.%20This%20method%20suppresses%0Airrelevant%20activations%20and%20enhances%20object-focused%20cues%2C%20enabling%20direct%0Aconversion%20of%20attention%20maps%20into%20coarse%20segmentation%20masks.%20In%20addition%2C%20we%0Aintroduce%20attention-guided%20SAM2%20prompting%20for%20obtaining%20fine-grained%20masks.%0AUnlike%20existing%20methods%20that%20jointly%20train%20MLLMs%20with%20SAM%2C%20our%20method%20operates%0Aentirely%20without%20retraining.%20DecAF%20outperforms%20training-free%20methods%20and%0Aachieves%20performance%20comparable%20to%20training-based%20methods%20on%20both%20referring%20and%0Areasoning%20VOS%20benchmarks.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/HYUNJS/DecAF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19592v1&entry.124074799=Read"},
{"title": "3D-GSRD: 3D Molecular Graph Auto-Encoder with Selective Re-mask Decoding", "author": "Chang Wu and Zhiyuan Liu and Wen Shu and Liang Wang and Yanchen Luo and Wenqiang Lei and Yatao Bian and Junfeng Fang and Xiang Wang", "abstract": "  Masked graph modeling (MGM) is a promising approach for molecular\nrepresentation learning (MRL).However, extending the success of re-mask\ndecoding from 2D to 3D MGM is non-trivial, primarily due to two conflicting\nchallenges: avoiding 2D structure leakage to the decoder, while still providing\nsufficient 2D context for reconstructing re-masked atoms. To address these\nchallenges, we propose 3D-GSRD: a 3D Molecular Graph Auto-Encoder with\nSelective Re-mask Decoding. The core innovation of 3D-GSRD lies in its\nSelective Re-mask Decoding(SRD), which re-masks only 3D-relevant information\nfrom encoder representations while preserving the 2D graph structures. This SRD\nis synergistically integrated with a 3D Relational-Transformer(3D-ReTrans)\nencoder alongside a structure-independent decoder. We analyze that SRD,\ncombined with the structure-independent decoder, enhances the encoder's role in\nMRL. Extensive experiments show that 3D-GSRD achieves strong downstream\nperformance, setting a new state-of-the-art on 7 out of 8 targets in the widely\nused MD17 molecular property prediction benchmark. The code is released at\nhttps://github.com/WuChang0124/3D-GSRD.\n", "link": "http://arxiv.org/abs/2510.16780v2", "date": "2025-10-22", "relevancy": 2.8335, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5862}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5604}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D-GSRD%3A%203D%20Molecular%20Graph%20Auto-Encoder%20with%20Selective%20Re-mask%20Decoding&body=Title%3A%203D-GSRD%3A%203D%20Molecular%20Graph%20Auto-Encoder%20with%20Selective%20Re-mask%20Decoding%0AAuthor%3A%20Chang%20Wu%20and%20Zhiyuan%20Liu%20and%20Wen%20Shu%20and%20Liang%20Wang%20and%20Yanchen%20Luo%20and%20Wenqiang%20Lei%20and%20Yatao%20Bian%20and%20Junfeng%20Fang%20and%20Xiang%20Wang%0AAbstract%3A%20%20%20Masked%20graph%20modeling%20%28MGM%29%20is%20a%20promising%20approach%20for%20molecular%0Arepresentation%20learning%20%28MRL%29.However%2C%20extending%20the%20success%20of%20re-mask%0Adecoding%20from%202D%20to%203D%20MGM%20is%20non-trivial%2C%20primarily%20due%20to%20two%20conflicting%0Achallenges%3A%20avoiding%202D%20structure%20leakage%20to%20the%20decoder%2C%20while%20still%20providing%0Asufficient%202D%20context%20for%20reconstructing%20re-masked%20atoms.%20To%20address%20these%0Achallenges%2C%20we%20propose%203D-GSRD%3A%20a%203D%20Molecular%20Graph%20Auto-Encoder%20with%0ASelective%20Re-mask%20Decoding.%20The%20core%20innovation%20of%203D-GSRD%20lies%20in%20its%0ASelective%20Re-mask%20Decoding%28SRD%29%2C%20which%20re-masks%20only%203D-relevant%20information%0Afrom%20encoder%20representations%20while%20preserving%20the%202D%20graph%20structures.%20This%20SRD%0Ais%20synergistically%20integrated%20with%20a%203D%20Relational-Transformer%283D-ReTrans%29%0Aencoder%20alongside%20a%20structure-independent%20decoder.%20We%20analyze%20that%20SRD%2C%0Acombined%20with%20the%20structure-independent%20decoder%2C%20enhances%20the%20encoder%27s%20role%20in%0AMRL.%20Extensive%20experiments%20show%20that%203D-GSRD%20achieves%20strong%20downstream%0Aperformance%2C%20setting%20a%20new%20state-of-the-art%20on%207%20out%20of%208%20targets%20in%20the%20widely%0Aused%20MD17%20molecular%20property%20prediction%20benchmark.%20The%20code%20is%20released%20at%0Ahttps%3A//github.com/WuChang0124/3D-GSRD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.16780v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D-GSRD%253A%25203D%2520Molecular%2520Graph%2520Auto-Encoder%2520with%2520Selective%2520Re-mask%2520Decoding%26entry.906535625%3DChang%2520Wu%2520and%2520Zhiyuan%2520Liu%2520and%2520Wen%2520Shu%2520and%2520Liang%2520Wang%2520and%2520Yanchen%2520Luo%2520and%2520Wenqiang%2520Lei%2520and%2520Yatao%2520Bian%2520and%2520Junfeng%2520Fang%2520and%2520Xiang%2520Wang%26entry.1292438233%3D%2520%2520Masked%2520graph%2520modeling%2520%2528MGM%2529%2520is%2520a%2520promising%2520approach%2520for%2520molecular%250Arepresentation%2520learning%2520%2528MRL%2529.However%252C%2520extending%2520the%2520success%2520of%2520re-mask%250Adecoding%2520from%25202D%2520to%25203D%2520MGM%2520is%2520non-trivial%252C%2520primarily%2520due%2520to%2520two%2520conflicting%250Achallenges%253A%2520avoiding%25202D%2520structure%2520leakage%2520to%2520the%2520decoder%252C%2520while%2520still%2520providing%250Asufficient%25202D%2520context%2520for%2520reconstructing%2520re-masked%2520atoms.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%25203D-GSRD%253A%2520a%25203D%2520Molecular%2520Graph%2520Auto-Encoder%2520with%250ASelective%2520Re-mask%2520Decoding.%2520The%2520core%2520innovation%2520of%25203D-GSRD%2520lies%2520in%2520its%250ASelective%2520Re-mask%2520Decoding%2528SRD%2529%252C%2520which%2520re-masks%2520only%25203D-relevant%2520information%250Afrom%2520encoder%2520representations%2520while%2520preserving%2520the%25202D%2520graph%2520structures.%2520This%2520SRD%250Ais%2520synergistically%2520integrated%2520with%2520a%25203D%2520Relational-Transformer%25283D-ReTrans%2529%250Aencoder%2520alongside%2520a%2520structure-independent%2520decoder.%2520We%2520analyze%2520that%2520SRD%252C%250Acombined%2520with%2520the%2520structure-independent%2520decoder%252C%2520enhances%2520the%2520encoder%2527s%2520role%2520in%250AMRL.%2520Extensive%2520experiments%2520show%2520that%25203D-GSRD%2520achieves%2520strong%2520downstream%250Aperformance%252C%2520setting%2520a%2520new%2520state-of-the-art%2520on%25207%2520out%2520of%25208%2520targets%2520in%2520the%2520widely%250Aused%2520MD17%2520molecular%2520property%2520prediction%2520benchmark.%2520The%2520code%2520is%2520released%2520at%250Ahttps%253A//github.com/WuChang0124/3D-GSRD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.16780v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D-GSRD%3A%203D%20Molecular%20Graph%20Auto-Encoder%20with%20Selective%20Re-mask%20Decoding&entry.906535625=Chang%20Wu%20and%20Zhiyuan%20Liu%20and%20Wen%20Shu%20and%20Liang%20Wang%20and%20Yanchen%20Luo%20and%20Wenqiang%20Lei%20and%20Yatao%20Bian%20and%20Junfeng%20Fang%20and%20Xiang%20Wang&entry.1292438233=%20%20Masked%20graph%20modeling%20%28MGM%29%20is%20a%20promising%20approach%20for%20molecular%0Arepresentation%20learning%20%28MRL%29.However%2C%20extending%20the%20success%20of%20re-mask%0Adecoding%20from%202D%20to%203D%20MGM%20is%20non-trivial%2C%20primarily%20due%20to%20two%20conflicting%0Achallenges%3A%20avoiding%202D%20structure%20leakage%20to%20the%20decoder%2C%20while%20still%20providing%0Asufficient%202D%20context%20for%20reconstructing%20re-masked%20atoms.%20To%20address%20these%0Achallenges%2C%20we%20propose%203D-GSRD%3A%20a%203D%20Molecular%20Graph%20Auto-Encoder%20with%0ASelective%20Re-mask%20Decoding.%20The%20core%20innovation%20of%203D-GSRD%20lies%20in%20its%0ASelective%20Re-mask%20Decoding%28SRD%29%2C%20which%20re-masks%20only%203D-relevant%20information%0Afrom%20encoder%20representations%20while%20preserving%20the%202D%20graph%20structures.%20This%20SRD%0Ais%20synergistically%20integrated%20with%20a%203D%20Relational-Transformer%283D-ReTrans%29%0Aencoder%20alongside%20a%20structure-independent%20decoder.%20We%20analyze%20that%20SRD%2C%0Acombined%20with%20the%20structure-independent%20decoder%2C%20enhances%20the%20encoder%27s%20role%20in%0AMRL.%20Extensive%20experiments%20show%20that%203D-GSRD%20achieves%20strong%20downstream%0Aperformance%2C%20setting%20a%20new%20state-of-the-art%20on%207%20out%20of%208%20targets%20in%20the%20widely%0Aused%20MD17%20molecular%20property%20prediction%20benchmark.%20The%20code%20is%20released%20at%0Ahttps%3A//github.com/WuChang0124/3D-GSRD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.16780v2&entry.124074799=Read"},
{"title": "Fast Marker Detection for UV-Based Visual Relative Localisation in Agile\n  UAV Swarms", "author": "Vojt\u011bch Vrba and Viktor Walter and Petr \u0160t\u011bp\u00e1n and Martin Saska", "abstract": "  A novel approach for the fast onboard detection of isolated markers for\nvisual relative localisation of multiple teammates in agile UAV swarms is\nintroduced in this paper. As the detection forms a key component of real-time\nlocalisation systems, a three-fold innovation is presented, consisting of an\noptimised procedure for CPUs, a GPU shader program, and a functionally\nequivalent FPGA streaming architecture. For the proposed CPU and GPU solutions,\nthe mean processing time per pixel of input camera frames was accelerated by\ntwo to three orders of magnitude compared to the state of the art. For the\nlocalisation task, the proposed FPGA architecture offered the most significant\noverall acceleration by minimising the total delay from camera exposure to\ndetection results. Additionally, the proposed solutions were evaluated on\nvarious 32-bit and 64-bit embedded platforms to demonstrate their efficiency,\nas well as their feasibility for applications using low-end UAVs and MAVs.\nThus, it has become a crucial enabling technology for agile UAV swarming.\n", "link": "http://arxiv.org/abs/2510.19663v1", "date": "2025-10-22", "relevancy": 2.7104, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5647}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5552}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Marker%20Detection%20for%20UV-Based%20Visual%20Relative%20Localisation%20in%20Agile%0A%20%20UAV%20Swarms&body=Title%3A%20Fast%20Marker%20Detection%20for%20UV-Based%20Visual%20Relative%20Localisation%20in%20Agile%0A%20%20UAV%20Swarms%0AAuthor%3A%20Vojt%C4%9Bch%20Vrba%20and%20Viktor%20Walter%20and%20Petr%20%C5%A0t%C4%9Bp%C3%A1n%20and%20Martin%20Saska%0AAbstract%3A%20%20%20A%20novel%20approach%20for%20the%20fast%20onboard%20detection%20of%20isolated%20markers%20for%0Avisual%20relative%20localisation%20of%20multiple%20teammates%20in%20agile%20UAV%20swarms%20is%0Aintroduced%20in%20this%20paper.%20As%20the%20detection%20forms%20a%20key%20component%20of%20real-time%0Alocalisation%20systems%2C%20a%20three-fold%20innovation%20is%20presented%2C%20consisting%20of%20an%0Aoptimised%20procedure%20for%20CPUs%2C%20a%20GPU%20shader%20program%2C%20and%20a%20functionally%0Aequivalent%20FPGA%20streaming%20architecture.%20For%20the%20proposed%20CPU%20and%20GPU%20solutions%2C%0Athe%20mean%20processing%20time%20per%20pixel%20of%20input%20camera%20frames%20was%20accelerated%20by%0Atwo%20to%20three%20orders%20of%20magnitude%20compared%20to%20the%20state%20of%20the%20art.%20For%20the%0Alocalisation%20task%2C%20the%20proposed%20FPGA%20architecture%20offered%20the%20most%20significant%0Aoverall%20acceleration%20by%20minimising%20the%20total%20delay%20from%20camera%20exposure%20to%0Adetection%20results.%20Additionally%2C%20the%20proposed%20solutions%20were%20evaluated%20on%0Avarious%2032-bit%20and%2064-bit%20embedded%20platforms%20to%20demonstrate%20their%20efficiency%2C%0Aas%20well%20as%20their%20feasibility%20for%20applications%20using%20low-end%20UAVs%20and%20MAVs.%0AThus%2C%20it%20has%20become%20a%20crucial%20enabling%20technology%20for%20agile%20UAV%20swarming.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19663v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Marker%2520Detection%2520for%2520UV-Based%2520Visual%2520Relative%2520Localisation%2520in%2520Agile%250A%2520%2520UAV%2520Swarms%26entry.906535625%3DVojt%25C4%259Bch%2520Vrba%2520and%2520Viktor%2520Walter%2520and%2520Petr%2520%25C5%25A0t%25C4%259Bp%25C3%25A1n%2520and%2520Martin%2520Saska%26entry.1292438233%3D%2520%2520A%2520novel%2520approach%2520for%2520the%2520fast%2520onboard%2520detection%2520of%2520isolated%2520markers%2520for%250Avisual%2520relative%2520localisation%2520of%2520multiple%2520teammates%2520in%2520agile%2520UAV%2520swarms%2520is%250Aintroduced%2520in%2520this%2520paper.%2520As%2520the%2520detection%2520forms%2520a%2520key%2520component%2520of%2520real-time%250Alocalisation%2520systems%252C%2520a%2520three-fold%2520innovation%2520is%2520presented%252C%2520consisting%2520of%2520an%250Aoptimised%2520procedure%2520for%2520CPUs%252C%2520a%2520GPU%2520shader%2520program%252C%2520and%2520a%2520functionally%250Aequivalent%2520FPGA%2520streaming%2520architecture.%2520For%2520the%2520proposed%2520CPU%2520and%2520GPU%2520solutions%252C%250Athe%2520mean%2520processing%2520time%2520per%2520pixel%2520of%2520input%2520camera%2520frames%2520was%2520accelerated%2520by%250Atwo%2520to%2520three%2520orders%2520of%2520magnitude%2520compared%2520to%2520the%2520state%2520of%2520the%2520art.%2520For%2520the%250Alocalisation%2520task%252C%2520the%2520proposed%2520FPGA%2520architecture%2520offered%2520the%2520most%2520significant%250Aoverall%2520acceleration%2520by%2520minimising%2520the%2520total%2520delay%2520from%2520camera%2520exposure%2520to%250Adetection%2520results.%2520Additionally%252C%2520the%2520proposed%2520solutions%2520were%2520evaluated%2520on%250Avarious%252032-bit%2520and%252064-bit%2520embedded%2520platforms%2520to%2520demonstrate%2520their%2520efficiency%252C%250Aas%2520well%2520as%2520their%2520feasibility%2520for%2520applications%2520using%2520low-end%2520UAVs%2520and%2520MAVs.%250AThus%252C%2520it%2520has%2520become%2520a%2520crucial%2520enabling%2520technology%2520for%2520agile%2520UAV%2520swarming.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19663v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Marker%20Detection%20for%20UV-Based%20Visual%20Relative%20Localisation%20in%20Agile%0A%20%20UAV%20Swarms&entry.906535625=Vojt%C4%9Bch%20Vrba%20and%20Viktor%20Walter%20and%20Petr%20%C5%A0t%C4%9Bp%C3%A1n%20and%20Martin%20Saska&entry.1292438233=%20%20A%20novel%20approach%20for%20the%20fast%20onboard%20detection%20of%20isolated%20markers%20for%0Avisual%20relative%20localisation%20of%20multiple%20teammates%20in%20agile%20UAV%20swarms%20is%0Aintroduced%20in%20this%20paper.%20As%20the%20detection%20forms%20a%20key%20component%20of%20real-time%0Alocalisation%20systems%2C%20a%20three-fold%20innovation%20is%20presented%2C%20consisting%20of%20an%0Aoptimised%20procedure%20for%20CPUs%2C%20a%20GPU%20shader%20program%2C%20and%20a%20functionally%0Aequivalent%20FPGA%20streaming%20architecture.%20For%20the%20proposed%20CPU%20and%20GPU%20solutions%2C%0Athe%20mean%20processing%20time%20per%20pixel%20of%20input%20camera%20frames%20was%20accelerated%20by%0Atwo%20to%20three%20orders%20of%20magnitude%20compared%20to%20the%20state%20of%20the%20art.%20For%20the%0Alocalisation%20task%2C%20the%20proposed%20FPGA%20architecture%20offered%20the%20most%20significant%0Aoverall%20acceleration%20by%20minimising%20the%20total%20delay%20from%20camera%20exposure%20to%0Adetection%20results.%20Additionally%2C%20the%20proposed%20solutions%20were%20evaluated%20on%0Avarious%2032-bit%20and%2064-bit%20embedded%20platforms%20to%20demonstrate%20their%20efficiency%2C%0Aas%20well%20as%20their%20feasibility%20for%20applications%20using%20low-end%20UAVs%20and%20MAVs.%0AThus%2C%20it%20has%20become%20a%20crucial%20enabling%20technology%20for%20agile%20UAV%20swarming.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19663v1&entry.124074799=Read"},
{"title": "Rethinking Backbone Design for Lightweight 3D Object Detection in LiDAR", "author": "Adwait Chandorkar and Hasan Tercan and Tobias Meisen", "abstract": "  Recent advancements in LiDAR-based 3D object detection have significantly\naccelerated progress toward the realization of fully autonomous driving in\nreal-world environments. Despite achieving high detection performance, most of\nthe approaches still rely on a VGG-based or ResNet-based backbone for feature\nexploration, which increases the model complexity. Lightweight backbone design\nis well-explored for 2D object detection, but research on 3D object detection\nstill remains limited. In this work, we introduce Dense Backbone, a lightweight\nbackbone that combines the benefits of high processing speed, lightweight\narchitecture, and robust detection accuracy. We adapt multiple SoTA 3d object\ndetectors, such as PillarNet, with our backbone and show that with our\nbackbone, these models retain most of their detection capability at a\nsignificantly reduced computational cost. To our knowledge, this is the first\ndense-layer-based backbone tailored specifically for 3D object detection from\npoint cloud data. DensePillarNet, our adaptation of PillarNet, achieves a 29%\nreduction in model parameters and a 28% reduction in latency with just a 2%\ndrop in detection accuracy on the nuScenes test set. Furthermore, Dense\nBackbone's plug-and-play design allows straightforward integration into\nexisting architectures, requiring no modifications to other network components.\n", "link": "http://arxiv.org/abs/2508.00744v2", "date": "2025-10-22", "relevancy": 2.6529, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5457}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5248}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Backbone%20Design%20for%20Lightweight%203D%20Object%20Detection%20in%20LiDAR&body=Title%3A%20Rethinking%20Backbone%20Design%20for%20Lightweight%203D%20Object%20Detection%20in%20LiDAR%0AAuthor%3A%20Adwait%20Chandorkar%20and%20Hasan%20Tercan%20and%20Tobias%20Meisen%0AAbstract%3A%20%20%20Recent%20advancements%20in%20LiDAR-based%203D%20object%20detection%20have%20significantly%0Aaccelerated%20progress%20toward%20the%20realization%20of%20fully%20autonomous%20driving%20in%0Areal-world%20environments.%20Despite%20achieving%20high%20detection%20performance%2C%20most%20of%0Athe%20approaches%20still%20rely%20on%20a%20VGG-based%20or%20ResNet-based%20backbone%20for%20feature%0Aexploration%2C%20which%20increases%20the%20model%20complexity.%20Lightweight%20backbone%20design%0Ais%20well-explored%20for%202D%20object%20detection%2C%20but%20research%20on%203D%20object%20detection%0Astill%20remains%20limited.%20In%20this%20work%2C%20we%20introduce%20Dense%20Backbone%2C%20a%20lightweight%0Abackbone%20that%20combines%20the%20benefits%20of%20high%20processing%20speed%2C%20lightweight%0Aarchitecture%2C%20and%20robust%20detection%20accuracy.%20We%20adapt%20multiple%20SoTA%203d%20object%0Adetectors%2C%20such%20as%20PillarNet%2C%20with%20our%20backbone%20and%20show%20that%20with%20our%0Abackbone%2C%20these%20models%20retain%20most%20of%20their%20detection%20capability%20at%20a%0Asignificantly%20reduced%20computational%20cost.%20To%20our%20knowledge%2C%20this%20is%20the%20first%0Adense-layer-based%20backbone%20tailored%20specifically%20for%203D%20object%20detection%20from%0Apoint%20cloud%20data.%20DensePillarNet%2C%20our%20adaptation%20of%20PillarNet%2C%20achieves%20a%2029%25%0Areduction%20in%20model%20parameters%20and%20a%2028%25%20reduction%20in%20latency%20with%20just%20a%202%25%0Adrop%20in%20detection%20accuracy%20on%20the%20nuScenes%20test%20set.%20Furthermore%2C%20Dense%0ABackbone%27s%20plug-and-play%20design%20allows%20straightforward%20integration%20into%0Aexisting%20architectures%2C%20requiring%20no%20modifications%20to%20other%20network%20components.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00744v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Backbone%2520Design%2520for%2520Lightweight%25203D%2520Object%2520Detection%2520in%2520LiDAR%26entry.906535625%3DAdwait%2520Chandorkar%2520and%2520Hasan%2520Tercan%2520and%2520Tobias%2520Meisen%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520LiDAR-based%25203D%2520object%2520detection%2520have%2520significantly%250Aaccelerated%2520progress%2520toward%2520the%2520realization%2520of%2520fully%2520autonomous%2520driving%2520in%250Areal-world%2520environments.%2520Despite%2520achieving%2520high%2520detection%2520performance%252C%2520most%2520of%250Athe%2520approaches%2520still%2520rely%2520on%2520a%2520VGG-based%2520or%2520ResNet-based%2520backbone%2520for%2520feature%250Aexploration%252C%2520which%2520increases%2520the%2520model%2520complexity.%2520Lightweight%2520backbone%2520design%250Ais%2520well-explored%2520for%25202D%2520object%2520detection%252C%2520but%2520research%2520on%25203D%2520object%2520detection%250Astill%2520remains%2520limited.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Dense%2520Backbone%252C%2520a%2520lightweight%250Abackbone%2520that%2520combines%2520the%2520benefits%2520of%2520high%2520processing%2520speed%252C%2520lightweight%250Aarchitecture%252C%2520and%2520robust%2520detection%2520accuracy.%2520We%2520adapt%2520multiple%2520SoTA%25203d%2520object%250Adetectors%252C%2520such%2520as%2520PillarNet%252C%2520with%2520our%2520backbone%2520and%2520show%2520that%2520with%2520our%250Abackbone%252C%2520these%2520models%2520retain%2520most%2520of%2520their%2520detection%2520capability%2520at%2520a%250Asignificantly%2520reduced%2520computational%2520cost.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%250Adense-layer-based%2520backbone%2520tailored%2520specifically%2520for%25203D%2520object%2520detection%2520from%250Apoint%2520cloud%2520data.%2520DensePillarNet%252C%2520our%2520adaptation%2520of%2520PillarNet%252C%2520achieves%2520a%252029%2525%250Areduction%2520in%2520model%2520parameters%2520and%2520a%252028%2525%2520reduction%2520in%2520latency%2520with%2520just%2520a%25202%2525%250Adrop%2520in%2520detection%2520accuracy%2520on%2520the%2520nuScenes%2520test%2520set.%2520Furthermore%252C%2520Dense%250ABackbone%2527s%2520plug-and-play%2520design%2520allows%2520straightforward%2520integration%2520into%250Aexisting%2520architectures%252C%2520requiring%2520no%2520modifications%2520to%2520other%2520network%2520components.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00744v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Backbone%20Design%20for%20Lightweight%203D%20Object%20Detection%20in%20LiDAR&entry.906535625=Adwait%20Chandorkar%20and%20Hasan%20Tercan%20and%20Tobias%20Meisen&entry.1292438233=%20%20Recent%20advancements%20in%20LiDAR-based%203D%20object%20detection%20have%20significantly%0Aaccelerated%20progress%20toward%20the%20realization%20of%20fully%20autonomous%20driving%20in%0Areal-world%20environments.%20Despite%20achieving%20high%20detection%20performance%2C%20most%20of%0Athe%20approaches%20still%20rely%20on%20a%20VGG-based%20or%20ResNet-based%20backbone%20for%20feature%0Aexploration%2C%20which%20increases%20the%20model%20complexity.%20Lightweight%20backbone%20design%0Ais%20well-explored%20for%202D%20object%20detection%2C%20but%20research%20on%203D%20object%20detection%0Astill%20remains%20limited.%20In%20this%20work%2C%20we%20introduce%20Dense%20Backbone%2C%20a%20lightweight%0Abackbone%20that%20combines%20the%20benefits%20of%20high%20processing%20speed%2C%20lightweight%0Aarchitecture%2C%20and%20robust%20detection%20accuracy.%20We%20adapt%20multiple%20SoTA%203d%20object%0Adetectors%2C%20such%20as%20PillarNet%2C%20with%20our%20backbone%20and%20show%20that%20with%20our%0Abackbone%2C%20these%20models%20retain%20most%20of%20their%20detection%20capability%20at%20a%0Asignificantly%20reduced%20computational%20cost.%20To%20our%20knowledge%2C%20this%20is%20the%20first%0Adense-layer-based%20backbone%20tailored%20specifically%20for%203D%20object%20detection%20from%0Apoint%20cloud%20data.%20DensePillarNet%2C%20our%20adaptation%20of%20PillarNet%2C%20achieves%20a%2029%25%0Areduction%20in%20model%20parameters%20and%20a%2028%25%20reduction%20in%20latency%20with%20just%20a%202%25%0Adrop%20in%20detection%20accuracy%20on%20the%20nuScenes%20test%20set.%20Furthermore%2C%20Dense%0ABackbone%27s%20plug-and-play%20design%20allows%20straightforward%20integration%20into%0Aexisting%20architectures%2C%20requiring%20no%20modifications%20to%20other%20network%20components.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00744v2&entry.124074799=Read"},
{"title": "The Coverage Principle: How Pre-Training Enables Post-Training", "author": "Fan Chen and Audrey Huang and Noah Golowich and Sadhika Malladi and Adam Block and Jordan T. Ash and Akshay Krishnamurthy and Dylan J. Foster", "abstract": "  Language models demonstrate remarkable abilities when pre-trained on large\ntext corpora and fine-tuned for specific tasks, but how and why pre-training\nshapes the success of the final model remains poorly understood. Notably,\nalthough pre-training success is often quantified by cross-entropy loss,\ncross-entropy can be a poor predictor of downstream performance. Instead, we\nprovide a theoretical perspective on this relationship through the lens of\n\\emph{coverage}, which quantifies the probability mass the pre-trained model\nplaces on high-quality responses and which is necessary and sufficient for\npost-training and test-time scaling methods such as Best-of-N to succeed. Our\nmain results develop an understanding of \\emph{the coverage principle}, a\nphenomenon whereby next-token prediction (more generally, maximum likelihood)\nimplicitly optimizes toward a model with good coverage. In particular, we\nuncover a mechanism that explains the power of coverage in predicting\ndownstream performance: \\emph{coverage generalizes faster than cross-entropy},\navoiding spurious dependence on problem-dependent parameters such as the\nsequence length. We also study practical algorithmic interventions with\nprovable benefits for improving coverage, including (i) model/checkpoint\nselection procedures, (ii) gradient normalization schemes, and (iii) test-time\ndecoding strategies.\n", "link": "http://arxiv.org/abs/2510.15020v2", "date": "2025-10-22", "relevancy": 2.6452, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5477}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5477}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4917}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Coverage%20Principle%3A%20How%20Pre-Training%20Enables%20Post-Training&body=Title%3A%20The%20Coverage%20Principle%3A%20How%20Pre-Training%20Enables%20Post-Training%0AAuthor%3A%20Fan%20Chen%20and%20Audrey%20Huang%20and%20Noah%20Golowich%20and%20Sadhika%20Malladi%20and%20Adam%20Block%20and%20Jordan%20T.%20Ash%20and%20Akshay%20Krishnamurthy%20and%20Dylan%20J.%20Foster%0AAbstract%3A%20%20%20Language%20models%20demonstrate%20remarkable%20abilities%20when%20pre-trained%20on%20large%0Atext%20corpora%20and%20fine-tuned%20for%20specific%20tasks%2C%20but%20how%20and%20why%20pre-training%0Ashapes%20the%20success%20of%20the%20final%20model%20remains%20poorly%20understood.%20Notably%2C%0Aalthough%20pre-training%20success%20is%20often%20quantified%20by%20cross-entropy%20loss%2C%0Across-entropy%20can%20be%20a%20poor%20predictor%20of%20downstream%20performance.%20Instead%2C%20we%0Aprovide%20a%20theoretical%20perspective%20on%20this%20relationship%20through%20the%20lens%20of%0A%5Cemph%7Bcoverage%7D%2C%20which%20quantifies%20the%20probability%20mass%20the%20pre-trained%20model%0Aplaces%20on%20high-quality%20responses%20and%20which%20is%20necessary%20and%20sufficient%20for%0Apost-training%20and%20test-time%20scaling%20methods%20such%20as%20Best-of-N%20to%20succeed.%20Our%0Amain%20results%20develop%20an%20understanding%20of%20%5Cemph%7Bthe%20coverage%20principle%7D%2C%20a%0Aphenomenon%20whereby%20next-token%20prediction%20%28more%20generally%2C%20maximum%20likelihood%29%0Aimplicitly%20optimizes%20toward%20a%20model%20with%20good%20coverage.%20In%20particular%2C%20we%0Auncover%20a%20mechanism%20that%20explains%20the%20power%20of%20coverage%20in%20predicting%0Adownstream%20performance%3A%20%5Cemph%7Bcoverage%20generalizes%20faster%20than%20cross-entropy%7D%2C%0Aavoiding%20spurious%20dependence%20on%20problem-dependent%20parameters%20such%20as%20the%0Asequence%20length.%20We%20also%20study%20practical%20algorithmic%20interventions%20with%0Aprovable%20benefits%20for%20improving%20coverage%2C%20including%20%28i%29%20model/checkpoint%0Aselection%20procedures%2C%20%28ii%29%20gradient%20normalization%20schemes%2C%20and%20%28iii%29%20test-time%0Adecoding%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15020v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Coverage%2520Principle%253A%2520How%2520Pre-Training%2520Enables%2520Post-Training%26entry.906535625%3DFan%2520Chen%2520and%2520Audrey%2520Huang%2520and%2520Noah%2520Golowich%2520and%2520Sadhika%2520Malladi%2520and%2520Adam%2520Block%2520and%2520Jordan%2520T.%2520Ash%2520and%2520Akshay%2520Krishnamurthy%2520and%2520Dylan%2520J.%2520Foster%26entry.1292438233%3D%2520%2520Language%2520models%2520demonstrate%2520remarkable%2520abilities%2520when%2520pre-trained%2520on%2520large%250Atext%2520corpora%2520and%2520fine-tuned%2520for%2520specific%2520tasks%252C%2520but%2520how%2520and%2520why%2520pre-training%250Ashapes%2520the%2520success%2520of%2520the%2520final%2520model%2520remains%2520poorly%2520understood.%2520Notably%252C%250Aalthough%2520pre-training%2520success%2520is%2520often%2520quantified%2520by%2520cross-entropy%2520loss%252C%250Across-entropy%2520can%2520be%2520a%2520poor%2520predictor%2520of%2520downstream%2520performance.%2520Instead%252C%2520we%250Aprovide%2520a%2520theoretical%2520perspective%2520on%2520this%2520relationship%2520through%2520the%2520lens%2520of%250A%255Cemph%257Bcoverage%257D%252C%2520which%2520quantifies%2520the%2520probability%2520mass%2520the%2520pre-trained%2520model%250Aplaces%2520on%2520high-quality%2520responses%2520and%2520which%2520is%2520necessary%2520and%2520sufficient%2520for%250Apost-training%2520and%2520test-time%2520scaling%2520methods%2520such%2520as%2520Best-of-N%2520to%2520succeed.%2520Our%250Amain%2520results%2520develop%2520an%2520understanding%2520of%2520%255Cemph%257Bthe%2520coverage%2520principle%257D%252C%2520a%250Aphenomenon%2520whereby%2520next-token%2520prediction%2520%2528more%2520generally%252C%2520maximum%2520likelihood%2529%250Aimplicitly%2520optimizes%2520toward%2520a%2520model%2520with%2520good%2520coverage.%2520In%2520particular%252C%2520we%250Auncover%2520a%2520mechanism%2520that%2520explains%2520the%2520power%2520of%2520coverage%2520in%2520predicting%250Adownstream%2520performance%253A%2520%255Cemph%257Bcoverage%2520generalizes%2520faster%2520than%2520cross-entropy%257D%252C%250Aavoiding%2520spurious%2520dependence%2520on%2520problem-dependent%2520parameters%2520such%2520as%2520the%250Asequence%2520length.%2520We%2520also%2520study%2520practical%2520algorithmic%2520interventions%2520with%250Aprovable%2520benefits%2520for%2520improving%2520coverage%252C%2520including%2520%2528i%2529%2520model/checkpoint%250Aselection%2520procedures%252C%2520%2528ii%2529%2520gradient%2520normalization%2520schemes%252C%2520and%2520%2528iii%2529%2520test-time%250Adecoding%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15020v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Coverage%20Principle%3A%20How%20Pre-Training%20Enables%20Post-Training&entry.906535625=Fan%20Chen%20and%20Audrey%20Huang%20and%20Noah%20Golowich%20and%20Sadhika%20Malladi%20and%20Adam%20Block%20and%20Jordan%20T.%20Ash%20and%20Akshay%20Krishnamurthy%20and%20Dylan%20J.%20Foster&entry.1292438233=%20%20Language%20models%20demonstrate%20remarkable%20abilities%20when%20pre-trained%20on%20large%0Atext%20corpora%20and%20fine-tuned%20for%20specific%20tasks%2C%20but%20how%20and%20why%20pre-training%0Ashapes%20the%20success%20of%20the%20final%20model%20remains%20poorly%20understood.%20Notably%2C%0Aalthough%20pre-training%20success%20is%20often%20quantified%20by%20cross-entropy%20loss%2C%0Across-entropy%20can%20be%20a%20poor%20predictor%20of%20downstream%20performance.%20Instead%2C%20we%0Aprovide%20a%20theoretical%20perspective%20on%20this%20relationship%20through%20the%20lens%20of%0A%5Cemph%7Bcoverage%7D%2C%20which%20quantifies%20the%20probability%20mass%20the%20pre-trained%20model%0Aplaces%20on%20high-quality%20responses%20and%20which%20is%20necessary%20and%20sufficient%20for%0Apost-training%20and%20test-time%20scaling%20methods%20such%20as%20Best-of-N%20to%20succeed.%20Our%0Amain%20results%20develop%20an%20understanding%20of%20%5Cemph%7Bthe%20coverage%20principle%7D%2C%20a%0Aphenomenon%20whereby%20next-token%20prediction%20%28more%20generally%2C%20maximum%20likelihood%29%0Aimplicitly%20optimizes%20toward%20a%20model%20with%20good%20coverage.%20In%20particular%2C%20we%0Auncover%20a%20mechanism%20that%20explains%20the%20power%20of%20coverage%20in%20predicting%0Adownstream%20performance%3A%20%5Cemph%7Bcoverage%20generalizes%20faster%20than%20cross-entropy%7D%2C%0Aavoiding%20spurious%20dependence%20on%20problem-dependent%20parameters%20such%20as%20the%0Asequence%20length.%20We%20also%20study%20practical%20algorithmic%20interventions%20with%0Aprovable%20benefits%20for%20improving%20coverage%2C%20including%20%28i%29%20model/checkpoint%0Aselection%20procedures%2C%20%28ii%29%20gradient%20normalization%20schemes%2C%20and%20%28iii%29%20test-time%0Adecoding%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15020v2&entry.124074799=Read"},
{"title": "CNeuroMod-THINGS, a densely-sampled fMRI dataset for visual neuroscience", "author": "Marie St-Laurent and Basile Pinsard and Oliver Contier and Elizabeth DuPre and Katja Seeliger and Valentina Borghesani and Julie A. Boyle and Lune Bellec and Martin N. Hebart", "abstract": "  Data-hungry neuro-AI modelling requires ever larger neuroimaging datasets.\nCNeuroMod-THINGS meets this need by capturing neural representations for a wide\nset of semantic concepts using well-characterized images in a new\ndensely-sampled, large-scale fMRI dataset. Importantly, CNeuroMod-THINGS\nexploits synergies between two existing projects: the THINGS initiative\n(THINGS) and the Courtois Project on Neural Modelling (CNeuroMod). THINGS has\ndeveloped a common set of thoroughly annotated images broadly sampling natural\nand man-made objects which is used to acquire a growing collection of\nlarge-scale multimodal neural responses. Meanwhile, CNeuroMod is acquiring\nhundreds of hours of fMRI data from a core set of participants during\ncontrolled and naturalistic tasks, including visual tasks like movie watching\nand videogame playing. For CNeuroMod-THINGS, four CNeuroMod participants each\ncompleted 33-36 sessions of a continuous recognition paradigm using\napproximately 4000 images from the THINGS stimulus set spanning 720 categories.\nWe report behavioural and neuroimaging metrics that showcase the quality of the\ndata. By bridging together large existing resources, CNeuroMod-THINGS expands\nour capacity to model broad slices of the human visual experience.\n", "link": "http://arxiv.org/abs/2507.09024v4", "date": "2025-10-22", "relevancy": 2.5773, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5213}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5213}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CNeuroMod-THINGS%2C%20a%20densely-sampled%20fMRI%20dataset%20for%20visual%20neuroscience&body=Title%3A%20CNeuroMod-THINGS%2C%20a%20densely-sampled%20fMRI%20dataset%20for%20visual%20neuroscience%0AAuthor%3A%20Marie%20St-Laurent%20and%20Basile%20Pinsard%20and%20Oliver%20Contier%20and%20Elizabeth%20DuPre%20and%20Katja%20Seeliger%20and%20Valentina%20Borghesani%20and%20Julie%20A.%20Boyle%20and%20Lune%20Bellec%20and%20Martin%20N.%20Hebart%0AAbstract%3A%20%20%20Data-hungry%20neuro-AI%20modelling%20requires%20ever%20larger%20neuroimaging%20datasets.%0ACNeuroMod-THINGS%20meets%20this%20need%20by%20capturing%20neural%20representations%20for%20a%20wide%0Aset%20of%20semantic%20concepts%20using%20well-characterized%20images%20in%20a%20new%0Adensely-sampled%2C%20large-scale%20fMRI%20dataset.%20Importantly%2C%20CNeuroMod-THINGS%0Aexploits%20synergies%20between%20two%20existing%20projects%3A%20the%20THINGS%20initiative%0A%28THINGS%29%20and%20the%20Courtois%20Project%20on%20Neural%20Modelling%20%28CNeuroMod%29.%20THINGS%20has%0Adeveloped%20a%20common%20set%20of%20thoroughly%20annotated%20images%20broadly%20sampling%20natural%0Aand%20man-made%20objects%20which%20is%20used%20to%20acquire%20a%20growing%20collection%20of%0Alarge-scale%20multimodal%20neural%20responses.%20Meanwhile%2C%20CNeuroMod%20is%20acquiring%0Ahundreds%20of%20hours%20of%20fMRI%20data%20from%20a%20core%20set%20of%20participants%20during%0Acontrolled%20and%20naturalistic%20tasks%2C%20including%20visual%20tasks%20like%20movie%20watching%0Aand%20videogame%20playing.%20For%20CNeuroMod-THINGS%2C%20four%20CNeuroMod%20participants%20each%0Acompleted%2033-36%20sessions%20of%20a%20continuous%20recognition%20paradigm%20using%0Aapproximately%204000%20images%20from%20the%20THINGS%20stimulus%20set%20spanning%20720%20categories.%0AWe%20report%20behavioural%20and%20neuroimaging%20metrics%20that%20showcase%20the%20quality%20of%20the%0Adata.%20By%20bridging%20together%20large%20existing%20resources%2C%20CNeuroMod-THINGS%20expands%0Aour%20capacity%20to%20model%20broad%20slices%20of%20the%20human%20visual%20experience.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.09024v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCNeuroMod-THINGS%252C%2520a%2520densely-sampled%2520fMRI%2520dataset%2520for%2520visual%2520neuroscience%26entry.906535625%3DMarie%2520St-Laurent%2520and%2520Basile%2520Pinsard%2520and%2520Oliver%2520Contier%2520and%2520Elizabeth%2520DuPre%2520and%2520Katja%2520Seeliger%2520and%2520Valentina%2520Borghesani%2520and%2520Julie%2520A.%2520Boyle%2520and%2520Lune%2520Bellec%2520and%2520Martin%2520N.%2520Hebart%26entry.1292438233%3D%2520%2520Data-hungry%2520neuro-AI%2520modelling%2520requires%2520ever%2520larger%2520neuroimaging%2520datasets.%250ACNeuroMod-THINGS%2520meets%2520this%2520need%2520by%2520capturing%2520neural%2520representations%2520for%2520a%2520wide%250Aset%2520of%2520semantic%2520concepts%2520using%2520well-characterized%2520images%2520in%2520a%2520new%250Adensely-sampled%252C%2520large-scale%2520fMRI%2520dataset.%2520Importantly%252C%2520CNeuroMod-THINGS%250Aexploits%2520synergies%2520between%2520two%2520existing%2520projects%253A%2520the%2520THINGS%2520initiative%250A%2528THINGS%2529%2520and%2520the%2520Courtois%2520Project%2520on%2520Neural%2520Modelling%2520%2528CNeuroMod%2529.%2520THINGS%2520has%250Adeveloped%2520a%2520common%2520set%2520of%2520thoroughly%2520annotated%2520images%2520broadly%2520sampling%2520natural%250Aand%2520man-made%2520objects%2520which%2520is%2520used%2520to%2520acquire%2520a%2520growing%2520collection%2520of%250Alarge-scale%2520multimodal%2520neural%2520responses.%2520Meanwhile%252C%2520CNeuroMod%2520is%2520acquiring%250Ahundreds%2520of%2520hours%2520of%2520fMRI%2520data%2520from%2520a%2520core%2520set%2520of%2520participants%2520during%250Acontrolled%2520and%2520naturalistic%2520tasks%252C%2520including%2520visual%2520tasks%2520like%2520movie%2520watching%250Aand%2520videogame%2520playing.%2520For%2520CNeuroMod-THINGS%252C%2520four%2520CNeuroMod%2520participants%2520each%250Acompleted%252033-36%2520sessions%2520of%2520a%2520continuous%2520recognition%2520paradigm%2520using%250Aapproximately%25204000%2520images%2520from%2520the%2520THINGS%2520stimulus%2520set%2520spanning%2520720%2520categories.%250AWe%2520report%2520behavioural%2520and%2520neuroimaging%2520metrics%2520that%2520showcase%2520the%2520quality%2520of%2520the%250Adata.%2520By%2520bridging%2520together%2520large%2520existing%2520resources%252C%2520CNeuroMod-THINGS%2520expands%250Aour%2520capacity%2520to%2520model%2520broad%2520slices%2520of%2520the%2520human%2520visual%2520experience.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.09024v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CNeuroMod-THINGS%2C%20a%20densely-sampled%20fMRI%20dataset%20for%20visual%20neuroscience&entry.906535625=Marie%20St-Laurent%20and%20Basile%20Pinsard%20and%20Oliver%20Contier%20and%20Elizabeth%20DuPre%20and%20Katja%20Seeliger%20and%20Valentina%20Borghesani%20and%20Julie%20A.%20Boyle%20and%20Lune%20Bellec%20and%20Martin%20N.%20Hebart&entry.1292438233=%20%20Data-hungry%20neuro-AI%20modelling%20requires%20ever%20larger%20neuroimaging%20datasets.%0ACNeuroMod-THINGS%20meets%20this%20need%20by%20capturing%20neural%20representations%20for%20a%20wide%0Aset%20of%20semantic%20concepts%20using%20well-characterized%20images%20in%20a%20new%0Adensely-sampled%2C%20large-scale%20fMRI%20dataset.%20Importantly%2C%20CNeuroMod-THINGS%0Aexploits%20synergies%20between%20two%20existing%20projects%3A%20the%20THINGS%20initiative%0A%28THINGS%29%20and%20the%20Courtois%20Project%20on%20Neural%20Modelling%20%28CNeuroMod%29.%20THINGS%20has%0Adeveloped%20a%20common%20set%20of%20thoroughly%20annotated%20images%20broadly%20sampling%20natural%0Aand%20man-made%20objects%20which%20is%20used%20to%20acquire%20a%20growing%20collection%20of%0Alarge-scale%20multimodal%20neural%20responses.%20Meanwhile%2C%20CNeuroMod%20is%20acquiring%0Ahundreds%20of%20hours%20of%20fMRI%20data%20from%20a%20core%20set%20of%20participants%20during%0Acontrolled%20and%20naturalistic%20tasks%2C%20including%20visual%20tasks%20like%20movie%20watching%0Aand%20videogame%20playing.%20For%20CNeuroMod-THINGS%2C%20four%20CNeuroMod%20participants%20each%0Acompleted%2033-36%20sessions%20of%20a%20continuous%20recognition%20paradigm%20using%0Aapproximately%204000%20images%20from%20the%20THINGS%20stimulus%20set%20spanning%20720%20categories.%0AWe%20report%20behavioural%20and%20neuroimaging%20metrics%20that%20showcase%20the%20quality%20of%20the%0Adata.%20By%20bridging%20together%20large%20existing%20resources%2C%20CNeuroMod-THINGS%20expands%0Aour%20capacity%20to%20model%20broad%20slices%20of%20the%20human%20visual%20experience.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.09024v4&entry.124074799=Read"},
{"title": "Do Prompts Reshape Representations? An Empirical Study of Prompting\n  Effects on Embeddings", "author": "Cesar Gonzalez-Gutierrez and Dirk Hovy", "abstract": "  Prompting is a common approach for leveraging LMs in zero-shot settings.\nHowever, the underlying mechanisms that enable LMs to perform diverse tasks\nwithout task-specific supervision remain poorly understood. Studying the\nrelationship between prompting and the quality of internal representations can\nshed light on how pre-trained embeddings may support in-context task solving.\nIn this empirical study, we conduct a series of probing experiments on prompt\nembeddings, analyzing various combinations of prompt templates for zero-shot\nclassification. Our findings show that while prompting affects the quality of\nrepresentations, these changes do not consistently correlate with the relevance\nof the prompts to the target task. This result challenges the assumption that\nmore relevant prompts necessarily lead to better representations. We further\nanalyze potential factors that may contribute to this unexpected behavior.\n", "link": "http://arxiv.org/abs/2510.19694v1", "date": "2025-10-22", "relevancy": 2.5224, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5209}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5209}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Prompts%20Reshape%20Representations%3F%20An%20Empirical%20Study%20of%20Prompting%0A%20%20Effects%20on%20Embeddings&body=Title%3A%20Do%20Prompts%20Reshape%20Representations%3F%20An%20Empirical%20Study%20of%20Prompting%0A%20%20Effects%20on%20Embeddings%0AAuthor%3A%20Cesar%20Gonzalez-Gutierrez%20and%20Dirk%20Hovy%0AAbstract%3A%20%20%20Prompting%20is%20a%20common%20approach%20for%20leveraging%20LMs%20in%20zero-shot%20settings.%0AHowever%2C%20the%20underlying%20mechanisms%20that%20enable%20LMs%20to%20perform%20diverse%20tasks%0Awithout%20task-specific%20supervision%20remain%20poorly%20understood.%20Studying%20the%0Arelationship%20between%20prompting%20and%20the%20quality%20of%20internal%20representations%20can%0Ashed%20light%20on%20how%20pre-trained%20embeddings%20may%20support%20in-context%20task%20solving.%0AIn%20this%20empirical%20study%2C%20we%20conduct%20a%20series%20of%20probing%20experiments%20on%20prompt%0Aembeddings%2C%20analyzing%20various%20combinations%20of%20prompt%20templates%20for%20zero-shot%0Aclassification.%20Our%20findings%20show%20that%20while%20prompting%20affects%20the%20quality%20of%0Arepresentations%2C%20these%20changes%20do%20not%20consistently%20correlate%20with%20the%20relevance%0Aof%20the%20prompts%20to%20the%20target%20task.%20This%20result%20challenges%20the%20assumption%20that%0Amore%20relevant%20prompts%20necessarily%20lead%20to%20better%20representations.%20We%20further%0Aanalyze%20potential%20factors%20that%20may%20contribute%20to%20this%20unexpected%20behavior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19694v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Prompts%2520Reshape%2520Representations%253F%2520An%2520Empirical%2520Study%2520of%2520Prompting%250A%2520%2520Effects%2520on%2520Embeddings%26entry.906535625%3DCesar%2520Gonzalez-Gutierrez%2520and%2520Dirk%2520Hovy%26entry.1292438233%3D%2520%2520Prompting%2520is%2520a%2520common%2520approach%2520for%2520leveraging%2520LMs%2520in%2520zero-shot%2520settings.%250AHowever%252C%2520the%2520underlying%2520mechanisms%2520that%2520enable%2520LMs%2520to%2520perform%2520diverse%2520tasks%250Awithout%2520task-specific%2520supervision%2520remain%2520poorly%2520understood.%2520Studying%2520the%250Arelationship%2520between%2520prompting%2520and%2520the%2520quality%2520of%2520internal%2520representations%2520can%250Ashed%2520light%2520on%2520how%2520pre-trained%2520embeddings%2520may%2520support%2520in-context%2520task%2520solving.%250AIn%2520this%2520empirical%2520study%252C%2520we%2520conduct%2520a%2520series%2520of%2520probing%2520experiments%2520on%2520prompt%250Aembeddings%252C%2520analyzing%2520various%2520combinations%2520of%2520prompt%2520templates%2520for%2520zero-shot%250Aclassification.%2520Our%2520findings%2520show%2520that%2520while%2520prompting%2520affects%2520the%2520quality%2520of%250Arepresentations%252C%2520these%2520changes%2520do%2520not%2520consistently%2520correlate%2520with%2520the%2520relevance%250Aof%2520the%2520prompts%2520to%2520the%2520target%2520task.%2520This%2520result%2520challenges%2520the%2520assumption%2520that%250Amore%2520relevant%2520prompts%2520necessarily%2520lead%2520to%2520better%2520representations.%2520We%2520further%250Aanalyze%2520potential%2520factors%2520that%2520may%2520contribute%2520to%2520this%2520unexpected%2520behavior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19694v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Prompts%20Reshape%20Representations%3F%20An%20Empirical%20Study%20of%20Prompting%0A%20%20Effects%20on%20Embeddings&entry.906535625=Cesar%20Gonzalez-Gutierrez%20and%20Dirk%20Hovy&entry.1292438233=%20%20Prompting%20is%20a%20common%20approach%20for%20leveraging%20LMs%20in%20zero-shot%20settings.%0AHowever%2C%20the%20underlying%20mechanisms%20that%20enable%20LMs%20to%20perform%20diverse%20tasks%0Awithout%20task-specific%20supervision%20remain%20poorly%20understood.%20Studying%20the%0Arelationship%20between%20prompting%20and%20the%20quality%20of%20internal%20representations%20can%0Ashed%20light%20on%20how%20pre-trained%20embeddings%20may%20support%20in-context%20task%20solving.%0AIn%20this%20empirical%20study%2C%20we%20conduct%20a%20series%20of%20probing%20experiments%20on%20prompt%0Aembeddings%2C%20analyzing%20various%20combinations%20of%20prompt%20templates%20for%20zero-shot%0Aclassification.%20Our%20findings%20show%20that%20while%20prompting%20affects%20the%20quality%20of%0Arepresentations%2C%20these%20changes%20do%20not%20consistently%20correlate%20with%20the%20relevance%0Aof%20the%20prompts%20to%20the%20target%20task.%20This%20result%20challenges%20the%20assumption%20that%0Amore%20relevant%20prompts%20necessarily%20lead%20to%20better%20representations.%20We%20further%0Aanalyze%20potential%20factors%20that%20may%20contribute%20to%20this%20unexpected%20behavior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19694v1&entry.124074799=Read"},
{"title": "Learning to Learn with Contrastive Meta-Objective", "author": "Shiguang Wu and Yaqing Wang and Yatao Bian and Quanming Yao", "abstract": "  Meta-learning enables learning systems to adapt quickly to new tasks, similar\nto humans. Different meta-learning approaches all work under/with the\nmini-batch episodic training framework. Such framework naturally gives the\ninformation about task identity, which can serve as additional supervision for\nmeta-training to improve generalizability. We propose to exploit task identity\nas additional supervision in meta-training, inspired by the alignment and\ndiscrimination ability which is is intrinsic in human's fast learning. This is\nachieved by contrasting what meta-learners learn, i.e., model representations.\nThe proposed ConML is evaluating and optimizing the contrastive meta-objective\nunder a problem- and learner-agnostic meta-training framework. We demonstrate\nthat ConML integrates seamlessly with existing meta-learners, as well as\nin-context learning models, and brings significant boost in performance with\nsmall implementation cost.\n", "link": "http://arxiv.org/abs/2410.05975v4", "date": "2025-10-22", "relevancy": 2.5102, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5333}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4867}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4861}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Learn%20with%20Contrastive%20Meta-Objective&body=Title%3A%20Learning%20to%20Learn%20with%20Contrastive%20Meta-Objective%0AAuthor%3A%20Shiguang%20Wu%20and%20Yaqing%20Wang%20and%20Yatao%20Bian%20and%20Quanming%20Yao%0AAbstract%3A%20%20%20Meta-learning%20enables%20learning%20systems%20to%20adapt%20quickly%20to%20new%20tasks%2C%20similar%0Ato%20humans.%20Different%20meta-learning%20approaches%20all%20work%20under/with%20the%0Amini-batch%20episodic%20training%20framework.%20Such%20framework%20naturally%20gives%20the%0Ainformation%20about%20task%20identity%2C%20which%20can%20serve%20as%20additional%20supervision%20for%0Ameta-training%20to%20improve%20generalizability.%20We%20propose%20to%20exploit%20task%20identity%0Aas%20additional%20supervision%20in%20meta-training%2C%20inspired%20by%20the%20alignment%20and%0Adiscrimination%20ability%20which%20is%20is%20intrinsic%20in%20human%27s%20fast%20learning.%20This%20is%0Aachieved%20by%20contrasting%20what%20meta-learners%20learn%2C%20i.e.%2C%20model%20representations.%0AThe%20proposed%20ConML%20is%20evaluating%20and%20optimizing%20the%20contrastive%20meta-objective%0Aunder%20a%20problem-%20and%20learner-agnostic%20meta-training%20framework.%20We%20demonstrate%0Athat%20ConML%20integrates%20seamlessly%20with%20existing%20meta-learners%2C%20as%20well%20as%0Ain-context%20learning%20models%2C%20and%20brings%20significant%20boost%20in%20performance%20with%0Asmall%20implementation%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05975v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Learn%2520with%2520Contrastive%2520Meta-Objective%26entry.906535625%3DShiguang%2520Wu%2520and%2520Yaqing%2520Wang%2520and%2520Yatao%2520Bian%2520and%2520Quanming%2520Yao%26entry.1292438233%3D%2520%2520Meta-learning%2520enables%2520learning%2520systems%2520to%2520adapt%2520quickly%2520to%2520new%2520tasks%252C%2520similar%250Ato%2520humans.%2520Different%2520meta-learning%2520approaches%2520all%2520work%2520under/with%2520the%250Amini-batch%2520episodic%2520training%2520framework.%2520Such%2520framework%2520naturally%2520gives%2520the%250Ainformation%2520about%2520task%2520identity%252C%2520which%2520can%2520serve%2520as%2520additional%2520supervision%2520for%250Ameta-training%2520to%2520improve%2520generalizability.%2520We%2520propose%2520to%2520exploit%2520task%2520identity%250Aas%2520additional%2520supervision%2520in%2520meta-training%252C%2520inspired%2520by%2520the%2520alignment%2520and%250Adiscrimination%2520ability%2520which%2520is%2520is%2520intrinsic%2520in%2520human%2527s%2520fast%2520learning.%2520This%2520is%250Aachieved%2520by%2520contrasting%2520what%2520meta-learners%2520learn%252C%2520i.e.%252C%2520model%2520representations.%250AThe%2520proposed%2520ConML%2520is%2520evaluating%2520and%2520optimizing%2520the%2520contrastive%2520meta-objective%250Aunder%2520a%2520problem-%2520and%2520learner-agnostic%2520meta-training%2520framework.%2520We%2520demonstrate%250Athat%2520ConML%2520integrates%2520seamlessly%2520with%2520existing%2520meta-learners%252C%2520as%2520well%2520as%250Ain-context%2520learning%2520models%252C%2520and%2520brings%2520significant%2520boost%2520in%2520performance%2520with%250Asmall%2520implementation%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05975v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Learn%20with%20Contrastive%20Meta-Objective&entry.906535625=Shiguang%20Wu%20and%20Yaqing%20Wang%20and%20Yatao%20Bian%20and%20Quanming%20Yao&entry.1292438233=%20%20Meta-learning%20enables%20learning%20systems%20to%20adapt%20quickly%20to%20new%20tasks%2C%20similar%0Ato%20humans.%20Different%20meta-learning%20approaches%20all%20work%20under/with%20the%0Amini-batch%20episodic%20training%20framework.%20Such%20framework%20naturally%20gives%20the%0Ainformation%20about%20task%20identity%2C%20which%20can%20serve%20as%20additional%20supervision%20for%0Ameta-training%20to%20improve%20generalizability.%20We%20propose%20to%20exploit%20task%20identity%0Aas%20additional%20supervision%20in%20meta-training%2C%20inspired%20by%20the%20alignment%20and%0Adiscrimination%20ability%20which%20is%20is%20intrinsic%20in%20human%27s%20fast%20learning.%20This%20is%0Aachieved%20by%20contrasting%20what%20meta-learners%20learn%2C%20i.e.%2C%20model%20representations.%0AThe%20proposed%20ConML%20is%20evaluating%20and%20optimizing%20the%20contrastive%20meta-objective%0Aunder%20a%20problem-%20and%20learner-agnostic%20meta-training%20framework.%20We%20demonstrate%0Athat%20ConML%20integrates%20seamlessly%20with%20existing%20meta-learners%2C%20as%20well%20as%0Ain-context%20learning%20models%2C%20and%20brings%20significant%20boost%20in%20performance%20with%0Asmall%20implementation%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05975v4&entry.124074799=Read"},
{"title": "Graph Few-Shot Learning via Adaptive Spectrum Experts and Cross-Set\n  Distribution Calibration", "author": "Yonghao Liu and Yajun Wang and Chunli Guo and Wei Pang and Ximing Li and Fausto Giunchiglia and Xiaoyue Feng and Renchu Guan", "abstract": "  Graph few-shot learning has attracted increasing attention due to its ability\nto rapidly adapt models to new tasks with only limited labeled nodes. Despite\nthe remarkable progress made by existing graph few-shot learning methods,\nseveral key limitations remain. First, most current approaches rely on\npredefined and unified graph filters (e.g., low-pass or high-pass filters) to\nglobally enhance or suppress node frequency signals. Such fixed spectral\noperations fail to account for the heterogeneity of local topological\nstructures inherent in real-world graphs. Moreover, these methods often assume\nthat the support and query sets are drawn from the same distribution. However,\nunder few-shot conditions, the limited labeled data in the support set may not\nsufficiently capture the complex distribution of the query set, leading to\nsuboptimal generalization. To address these challenges, we propose GRACE, a\nnovel Graph few-shot leaRning framework that integrates Adaptive spectrum\nexperts with Cross-sEt distribution calibration techniques. Theoretically, the\nproposed approach enhances model generalization by adapting to both local\nstructural variations and cross-set distribution calibration. Empirically,\nGRACE consistently outperforms state-of-the-art baselines across a wide range\nof experimental settings. Our code can be found here.\n", "link": "http://arxiv.org/abs/2510.12140v2", "date": "2025-10-22", "relevancy": 2.4955, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5116}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5082}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4775}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Few-Shot%20Learning%20via%20Adaptive%20Spectrum%20Experts%20and%20Cross-Set%0A%20%20Distribution%20Calibration&body=Title%3A%20Graph%20Few-Shot%20Learning%20via%20Adaptive%20Spectrum%20Experts%20and%20Cross-Set%0A%20%20Distribution%20Calibration%0AAuthor%3A%20Yonghao%20Liu%20and%20Yajun%20Wang%20and%20Chunli%20Guo%20and%20Wei%20Pang%20and%20Ximing%20Li%20and%20Fausto%20Giunchiglia%20and%20Xiaoyue%20Feng%20and%20Renchu%20Guan%0AAbstract%3A%20%20%20Graph%20few-shot%20learning%20has%20attracted%20increasing%20attention%20due%20to%20its%20ability%0Ato%20rapidly%20adapt%20models%20to%20new%20tasks%20with%20only%20limited%20labeled%20nodes.%20Despite%0Athe%20remarkable%20progress%20made%20by%20existing%20graph%20few-shot%20learning%20methods%2C%0Aseveral%20key%20limitations%20remain.%20First%2C%20most%20current%20approaches%20rely%20on%0Apredefined%20and%20unified%20graph%20filters%20%28e.g.%2C%20low-pass%20or%20high-pass%20filters%29%20to%0Aglobally%20enhance%20or%20suppress%20node%20frequency%20signals.%20Such%20fixed%20spectral%0Aoperations%20fail%20to%20account%20for%20the%20heterogeneity%20of%20local%20topological%0Astructures%20inherent%20in%20real-world%20graphs.%20Moreover%2C%20these%20methods%20often%20assume%0Athat%20the%20support%20and%20query%20sets%20are%20drawn%20from%20the%20same%20distribution.%20However%2C%0Aunder%20few-shot%20conditions%2C%20the%20limited%20labeled%20data%20in%20the%20support%20set%20may%20not%0Asufficiently%20capture%20the%20complex%20distribution%20of%20the%20query%20set%2C%20leading%20to%0Asuboptimal%20generalization.%20To%20address%20these%20challenges%2C%20we%20propose%20GRACE%2C%20a%0Anovel%20Graph%20few-shot%20leaRning%20framework%20that%20integrates%20Adaptive%20spectrum%0Aexperts%20with%20Cross-sEt%20distribution%20calibration%20techniques.%20Theoretically%2C%20the%0Aproposed%20approach%20enhances%20model%20generalization%20by%20adapting%20to%20both%20local%0Astructural%20variations%20and%20cross-set%20distribution%20calibration.%20Empirically%2C%0AGRACE%20consistently%20outperforms%20state-of-the-art%20baselines%20across%20a%20wide%20range%0Aof%20experimental%20settings.%20Our%20code%20can%20be%20found%20here.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12140v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Few-Shot%2520Learning%2520via%2520Adaptive%2520Spectrum%2520Experts%2520and%2520Cross-Set%250A%2520%2520Distribution%2520Calibration%26entry.906535625%3DYonghao%2520Liu%2520and%2520Yajun%2520Wang%2520and%2520Chunli%2520Guo%2520and%2520Wei%2520Pang%2520and%2520Ximing%2520Li%2520and%2520Fausto%2520Giunchiglia%2520and%2520Xiaoyue%2520Feng%2520and%2520Renchu%2520Guan%26entry.1292438233%3D%2520%2520Graph%2520few-shot%2520learning%2520has%2520attracted%2520increasing%2520attention%2520due%2520to%2520its%2520ability%250Ato%2520rapidly%2520adapt%2520models%2520to%2520new%2520tasks%2520with%2520only%2520limited%2520labeled%2520nodes.%2520Despite%250Athe%2520remarkable%2520progress%2520made%2520by%2520existing%2520graph%2520few-shot%2520learning%2520methods%252C%250Aseveral%2520key%2520limitations%2520remain.%2520First%252C%2520most%2520current%2520approaches%2520rely%2520on%250Apredefined%2520and%2520unified%2520graph%2520filters%2520%2528e.g.%252C%2520low-pass%2520or%2520high-pass%2520filters%2529%2520to%250Aglobally%2520enhance%2520or%2520suppress%2520node%2520frequency%2520signals.%2520Such%2520fixed%2520spectral%250Aoperations%2520fail%2520to%2520account%2520for%2520the%2520heterogeneity%2520of%2520local%2520topological%250Astructures%2520inherent%2520in%2520real-world%2520graphs.%2520Moreover%252C%2520these%2520methods%2520often%2520assume%250Athat%2520the%2520support%2520and%2520query%2520sets%2520are%2520drawn%2520from%2520the%2520same%2520distribution.%2520However%252C%250Aunder%2520few-shot%2520conditions%252C%2520the%2520limited%2520labeled%2520data%2520in%2520the%2520support%2520set%2520may%2520not%250Asufficiently%2520capture%2520the%2520complex%2520distribution%2520of%2520the%2520query%2520set%252C%2520leading%2520to%250Asuboptimal%2520generalization.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520GRACE%252C%2520a%250Anovel%2520Graph%2520few-shot%2520leaRning%2520framework%2520that%2520integrates%2520Adaptive%2520spectrum%250Aexperts%2520with%2520Cross-sEt%2520distribution%2520calibration%2520techniques.%2520Theoretically%252C%2520the%250Aproposed%2520approach%2520enhances%2520model%2520generalization%2520by%2520adapting%2520to%2520both%2520local%250Astructural%2520variations%2520and%2520cross-set%2520distribution%2520calibration.%2520Empirically%252C%250AGRACE%2520consistently%2520outperforms%2520state-of-the-art%2520baselines%2520across%2520a%2520wide%2520range%250Aof%2520experimental%2520settings.%2520Our%2520code%2520can%2520be%2520found%2520here.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12140v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Few-Shot%20Learning%20via%20Adaptive%20Spectrum%20Experts%20and%20Cross-Set%0A%20%20Distribution%20Calibration&entry.906535625=Yonghao%20Liu%20and%20Yajun%20Wang%20and%20Chunli%20Guo%20and%20Wei%20Pang%20and%20Ximing%20Li%20and%20Fausto%20Giunchiglia%20and%20Xiaoyue%20Feng%20and%20Renchu%20Guan&entry.1292438233=%20%20Graph%20few-shot%20learning%20has%20attracted%20increasing%20attention%20due%20to%20its%20ability%0Ato%20rapidly%20adapt%20models%20to%20new%20tasks%20with%20only%20limited%20labeled%20nodes.%20Despite%0Athe%20remarkable%20progress%20made%20by%20existing%20graph%20few-shot%20learning%20methods%2C%0Aseveral%20key%20limitations%20remain.%20First%2C%20most%20current%20approaches%20rely%20on%0Apredefined%20and%20unified%20graph%20filters%20%28e.g.%2C%20low-pass%20or%20high-pass%20filters%29%20to%0Aglobally%20enhance%20or%20suppress%20node%20frequency%20signals.%20Such%20fixed%20spectral%0Aoperations%20fail%20to%20account%20for%20the%20heterogeneity%20of%20local%20topological%0Astructures%20inherent%20in%20real-world%20graphs.%20Moreover%2C%20these%20methods%20often%20assume%0Athat%20the%20support%20and%20query%20sets%20are%20drawn%20from%20the%20same%20distribution.%20However%2C%0Aunder%20few-shot%20conditions%2C%20the%20limited%20labeled%20data%20in%20the%20support%20set%20may%20not%0Asufficiently%20capture%20the%20complex%20distribution%20of%20the%20query%20set%2C%20leading%20to%0Asuboptimal%20generalization.%20To%20address%20these%20challenges%2C%20we%20propose%20GRACE%2C%20a%0Anovel%20Graph%20few-shot%20leaRning%20framework%20that%20integrates%20Adaptive%20spectrum%0Aexperts%20with%20Cross-sEt%20distribution%20calibration%20techniques.%20Theoretically%2C%20the%0Aproposed%20approach%20enhances%20model%20generalization%20by%20adapting%20to%20both%20local%0Astructural%20variations%20and%20cross-set%20distribution%20calibration.%20Empirically%2C%0AGRACE%20consistently%20outperforms%20state-of-the-art%20baselines%20across%20a%20wide%20range%0Aof%20experimental%20settings.%20Our%20code%20can%20be%20found%20here.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12140v2&entry.124074799=Read"},
{"title": "OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation", "author": "Guowei Xu and Yuxuan Bian and Ailing Zeng and Mingyi Shi and Shaoli Huang and Wen Li and Lixin Duan and Qiang Xu", "abstract": "  This paper introduces OmniMotion-X, a versatile multimodal framework for\nwhole-body human motion generation, leveraging an autoregressive diffusion\ntransformer in a unified sequence-to-sequence manner. OmniMotion-X efficiently\nsupports diverse multimodal tasks, including text-to-motion, music-to-dance,\nspeech-to-gesture, and global spatial-temporal control scenarios (e.g., motion\nprediction, in-betweening, completion, and joint/trajectory-guided synthesis),\nas well as flexible combinations of these tasks. Specifically, we propose the\nuse of reference motion as a novel conditioning signal, substantially enhancing\nthe consistency of generated content, style, and temporal dynamics crucial for\nrealistic animations. To handle multimodal conflicts, we introduce a\nprogressive weak-to-strong mixed-condition training strategy. To enable\nhigh-quality multimodal training, we construct OmniMoCap-X, the largest unified\nmultimodal motion dataset to date, integrating 28 publicly available MoCap\nsources across 10 distinct tasks, standardized to the SMPL-X format at 30 fps.\nTo ensure detailed and consistent annotations, we render sequences into videos\nand use GPT-4o to automatically generate structured and hierarchical captions,\ncapturing both low-level actions and high-level semantics. Extensive\nexperimental evaluations confirm that OmniMotion-X significantly surpasses\nexisting methods, demonstrating state-of-the-art performance across multiple\nmultimodal tasks and enabling the interactive generation of realistic,\ncoherent, and controllable long-duration motions.\n", "link": "http://arxiv.org/abs/2510.19789v1", "date": "2025-10-22", "relevancy": 2.4781, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6579}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6126}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5839}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniMotion-X%3A%20Versatile%20Multimodal%20Whole-Body%20Motion%20Generation&body=Title%3A%20OmniMotion-X%3A%20Versatile%20Multimodal%20Whole-Body%20Motion%20Generation%0AAuthor%3A%20Guowei%20Xu%20and%20Yuxuan%20Bian%20and%20Ailing%20Zeng%20and%20Mingyi%20Shi%20and%20Shaoli%20Huang%20and%20Wen%20Li%20and%20Lixin%20Duan%20and%20Qiang%20Xu%0AAbstract%3A%20%20%20This%20paper%20introduces%20OmniMotion-X%2C%20a%20versatile%20multimodal%20framework%20for%0Awhole-body%20human%20motion%20generation%2C%20leveraging%20an%20autoregressive%20diffusion%0Atransformer%20in%20a%20unified%20sequence-to-sequence%20manner.%20OmniMotion-X%20efficiently%0Asupports%20diverse%20multimodal%20tasks%2C%20including%20text-to-motion%2C%20music-to-dance%2C%0Aspeech-to-gesture%2C%20and%20global%20spatial-temporal%20control%20scenarios%20%28e.g.%2C%20motion%0Aprediction%2C%20in-betweening%2C%20completion%2C%20and%20joint/trajectory-guided%20synthesis%29%2C%0Aas%20well%20as%20flexible%20combinations%20of%20these%20tasks.%20Specifically%2C%20we%20propose%20the%0Ause%20of%20reference%20motion%20as%20a%20novel%20conditioning%20signal%2C%20substantially%20enhancing%0Athe%20consistency%20of%20generated%20content%2C%20style%2C%20and%20temporal%20dynamics%20crucial%20for%0Arealistic%20animations.%20To%20handle%20multimodal%20conflicts%2C%20we%20introduce%20a%0Aprogressive%20weak-to-strong%20mixed-condition%20training%20strategy.%20To%20enable%0Ahigh-quality%20multimodal%20training%2C%20we%20construct%20OmniMoCap-X%2C%20the%20largest%20unified%0Amultimodal%20motion%20dataset%20to%20date%2C%20integrating%2028%20publicly%20available%20MoCap%0Asources%20across%2010%20distinct%20tasks%2C%20standardized%20to%20the%20SMPL-X%20format%20at%2030%20fps.%0ATo%20ensure%20detailed%20and%20consistent%20annotations%2C%20we%20render%20sequences%20into%20videos%0Aand%20use%20GPT-4o%20to%20automatically%20generate%20structured%20and%20hierarchical%20captions%2C%0Acapturing%20both%20low-level%20actions%20and%20high-level%20semantics.%20Extensive%0Aexperimental%20evaluations%20confirm%20that%20OmniMotion-X%20significantly%20surpasses%0Aexisting%20methods%2C%20demonstrating%20state-of-the-art%20performance%20across%20multiple%0Amultimodal%20tasks%20and%20enabling%20the%20interactive%20generation%20of%20realistic%2C%0Acoherent%2C%20and%20controllable%20long-duration%20motions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19789v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniMotion-X%253A%2520Versatile%2520Multimodal%2520Whole-Body%2520Motion%2520Generation%26entry.906535625%3DGuowei%2520Xu%2520and%2520Yuxuan%2520Bian%2520and%2520Ailing%2520Zeng%2520and%2520Mingyi%2520Shi%2520and%2520Shaoli%2520Huang%2520and%2520Wen%2520Li%2520and%2520Lixin%2520Duan%2520and%2520Qiang%2520Xu%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520OmniMotion-X%252C%2520a%2520versatile%2520multimodal%2520framework%2520for%250Awhole-body%2520human%2520motion%2520generation%252C%2520leveraging%2520an%2520autoregressive%2520diffusion%250Atransformer%2520in%2520a%2520unified%2520sequence-to-sequence%2520manner.%2520OmniMotion-X%2520efficiently%250Asupports%2520diverse%2520multimodal%2520tasks%252C%2520including%2520text-to-motion%252C%2520music-to-dance%252C%250Aspeech-to-gesture%252C%2520and%2520global%2520spatial-temporal%2520control%2520scenarios%2520%2528e.g.%252C%2520motion%250Aprediction%252C%2520in-betweening%252C%2520completion%252C%2520and%2520joint/trajectory-guided%2520synthesis%2529%252C%250Aas%2520well%2520as%2520flexible%2520combinations%2520of%2520these%2520tasks.%2520Specifically%252C%2520we%2520propose%2520the%250Ause%2520of%2520reference%2520motion%2520as%2520a%2520novel%2520conditioning%2520signal%252C%2520substantially%2520enhancing%250Athe%2520consistency%2520of%2520generated%2520content%252C%2520style%252C%2520and%2520temporal%2520dynamics%2520crucial%2520for%250Arealistic%2520animations.%2520To%2520handle%2520multimodal%2520conflicts%252C%2520we%2520introduce%2520a%250Aprogressive%2520weak-to-strong%2520mixed-condition%2520training%2520strategy.%2520To%2520enable%250Ahigh-quality%2520multimodal%2520training%252C%2520we%2520construct%2520OmniMoCap-X%252C%2520the%2520largest%2520unified%250Amultimodal%2520motion%2520dataset%2520to%2520date%252C%2520integrating%252028%2520publicly%2520available%2520MoCap%250Asources%2520across%252010%2520distinct%2520tasks%252C%2520standardized%2520to%2520the%2520SMPL-X%2520format%2520at%252030%2520fps.%250ATo%2520ensure%2520detailed%2520and%2520consistent%2520annotations%252C%2520we%2520render%2520sequences%2520into%2520videos%250Aand%2520use%2520GPT-4o%2520to%2520automatically%2520generate%2520structured%2520and%2520hierarchical%2520captions%252C%250Acapturing%2520both%2520low-level%2520actions%2520and%2520high-level%2520semantics.%2520Extensive%250Aexperimental%2520evaluations%2520confirm%2520that%2520OmniMotion-X%2520significantly%2520surpasses%250Aexisting%2520methods%252C%2520demonstrating%2520state-of-the-art%2520performance%2520across%2520multiple%250Amultimodal%2520tasks%2520and%2520enabling%2520the%2520interactive%2520generation%2520of%2520realistic%252C%250Acoherent%252C%2520and%2520controllable%2520long-duration%2520motions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19789v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniMotion-X%3A%20Versatile%20Multimodal%20Whole-Body%20Motion%20Generation&entry.906535625=Guowei%20Xu%20and%20Yuxuan%20Bian%20and%20Ailing%20Zeng%20and%20Mingyi%20Shi%20and%20Shaoli%20Huang%20and%20Wen%20Li%20and%20Lixin%20Duan%20and%20Qiang%20Xu&entry.1292438233=%20%20This%20paper%20introduces%20OmniMotion-X%2C%20a%20versatile%20multimodal%20framework%20for%0Awhole-body%20human%20motion%20generation%2C%20leveraging%20an%20autoregressive%20diffusion%0Atransformer%20in%20a%20unified%20sequence-to-sequence%20manner.%20OmniMotion-X%20efficiently%0Asupports%20diverse%20multimodal%20tasks%2C%20including%20text-to-motion%2C%20music-to-dance%2C%0Aspeech-to-gesture%2C%20and%20global%20spatial-temporal%20control%20scenarios%20%28e.g.%2C%20motion%0Aprediction%2C%20in-betweening%2C%20completion%2C%20and%20joint/trajectory-guided%20synthesis%29%2C%0Aas%20well%20as%20flexible%20combinations%20of%20these%20tasks.%20Specifically%2C%20we%20propose%20the%0Ause%20of%20reference%20motion%20as%20a%20novel%20conditioning%20signal%2C%20substantially%20enhancing%0Athe%20consistency%20of%20generated%20content%2C%20style%2C%20and%20temporal%20dynamics%20crucial%20for%0Arealistic%20animations.%20To%20handle%20multimodal%20conflicts%2C%20we%20introduce%20a%0Aprogressive%20weak-to-strong%20mixed-condition%20training%20strategy.%20To%20enable%0Ahigh-quality%20multimodal%20training%2C%20we%20construct%20OmniMoCap-X%2C%20the%20largest%20unified%0Amultimodal%20motion%20dataset%20to%20date%2C%20integrating%2028%20publicly%20available%20MoCap%0Asources%20across%2010%20distinct%20tasks%2C%20standardized%20to%20the%20SMPL-X%20format%20at%2030%20fps.%0ATo%20ensure%20detailed%20and%20consistent%20annotations%2C%20we%20render%20sequences%20into%20videos%0Aand%20use%20GPT-4o%20to%20automatically%20generate%20structured%20and%20hierarchical%20captions%2C%0Acapturing%20both%20low-level%20actions%20and%20high-level%20semantics.%20Extensive%0Aexperimental%20evaluations%20confirm%20that%20OmniMotion-X%20significantly%20surpasses%0Aexisting%20methods%2C%20demonstrating%20state-of-the-art%20performance%20across%20multiple%0Amultimodal%20tasks%20and%20enabling%20the%20interactive%20generation%20of%20realistic%2C%0Acoherent%2C%20and%20controllable%20long-duration%20motions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19789v1&entry.124074799=Read"},
{"title": "Phase-driven Domain Generalizable Learning for Nonstationary Time Series", "author": "Payal Mohapatra and Lixu Wang and Qi Zhu", "abstract": "  Pattern recognition is a fundamental task in continuous sensing applications,\nbut real-world scenarios often experience distribution shifts that necessitate\nlearning generalizable representations for such tasks. This challenge is\nexacerbated with time-series data, which also exhibit inherent\nnonstationarity--variations in statistical and spectral properties over time.\nIn this work, we offer a fresh perspective on learning generalizable\nrepresentations for time-series classification by considering the phase\ninformation of a signal as an approximate proxy for nonstationarity and propose\na phase-driven generalizable representation learning framework for time-series\nclassification, PhASER. It consists of three key elements: 1) Hilbert\ntransform-based augmentation, which diversifies nonstationarity while\npreserving task-specific discriminatory semantics, 2) separate magnitude-phase\nencoding, viewing time-varying magnitude and phase as independent modalities,\nand 3) phase-residual feature broadcasting, integrating 2D phase features with\na residual connection to the 1D signal representation, providing inherent\nregularization to improve distribution-invariant learning. Extensive\nevaluations on five datasets from sleep-stage classification, human activity\nrecognition, and gesture recognition against 13 state-of-the-art baseline\nmethods demonstrate that PhASER consistently outperforms the best baselines by\nan average of 5% and up to 11% in some cases. Additionally, the principles of\nPhASER can be broadly applied to enhance the generalizability of existing\ntime-series representation learning models.\n", "link": "http://arxiv.org/abs/2402.05960v2", "date": "2025-10-22", "relevancy": 2.4688, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5024}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.498}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4809}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Phase-driven%20Domain%20Generalizable%20Learning%20for%20Nonstationary%20Time%20Series&body=Title%3A%20Phase-driven%20Domain%20Generalizable%20Learning%20for%20Nonstationary%20Time%20Series%0AAuthor%3A%20Payal%20Mohapatra%20and%20Lixu%20Wang%20and%20Qi%20Zhu%0AAbstract%3A%20%20%20Pattern%20recognition%20is%20a%20fundamental%20task%20in%20continuous%20sensing%20applications%2C%0Abut%20real-world%20scenarios%20often%20experience%20distribution%20shifts%20that%20necessitate%0Alearning%20generalizable%20representations%20for%20such%20tasks.%20This%20challenge%20is%0Aexacerbated%20with%20time-series%20data%2C%20which%20also%20exhibit%20inherent%0Anonstationarity--variations%20in%20statistical%20and%20spectral%20properties%20over%20time.%0AIn%20this%20work%2C%20we%20offer%20a%20fresh%20perspective%20on%20learning%20generalizable%0Arepresentations%20for%20time-series%20classification%20by%20considering%20the%20phase%0Ainformation%20of%20a%20signal%20as%20an%20approximate%20proxy%20for%20nonstationarity%20and%20propose%0Aa%20phase-driven%20generalizable%20representation%20learning%20framework%20for%20time-series%0Aclassification%2C%20PhASER.%20It%20consists%20of%20three%20key%20elements%3A%201%29%20Hilbert%0Atransform-based%20augmentation%2C%20which%20diversifies%20nonstationarity%20while%0Apreserving%20task-specific%20discriminatory%20semantics%2C%202%29%20separate%20magnitude-phase%0Aencoding%2C%20viewing%20time-varying%20magnitude%20and%20phase%20as%20independent%20modalities%2C%0Aand%203%29%20phase-residual%20feature%20broadcasting%2C%20integrating%202D%20phase%20features%20with%0Aa%20residual%20connection%20to%20the%201D%20signal%20representation%2C%20providing%20inherent%0Aregularization%20to%20improve%20distribution-invariant%20learning.%20Extensive%0Aevaluations%20on%20five%20datasets%20from%20sleep-stage%20classification%2C%20human%20activity%0Arecognition%2C%20and%20gesture%20recognition%20against%2013%20state-of-the-art%20baseline%0Amethods%20demonstrate%20that%20PhASER%20consistently%20outperforms%20the%20best%20baselines%20by%0Aan%20average%20of%205%25%20and%20up%20to%2011%25%20in%20some%20cases.%20Additionally%2C%20the%20principles%20of%0APhASER%20can%20be%20broadly%20applied%20to%20enhance%20the%20generalizability%20of%20existing%0Atime-series%20representation%20learning%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.05960v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhase-driven%2520Domain%2520Generalizable%2520Learning%2520for%2520Nonstationary%2520Time%2520Series%26entry.906535625%3DPayal%2520Mohapatra%2520and%2520Lixu%2520Wang%2520and%2520Qi%2520Zhu%26entry.1292438233%3D%2520%2520Pattern%2520recognition%2520is%2520a%2520fundamental%2520task%2520in%2520continuous%2520sensing%2520applications%252C%250Abut%2520real-world%2520scenarios%2520often%2520experience%2520distribution%2520shifts%2520that%2520necessitate%250Alearning%2520generalizable%2520representations%2520for%2520such%2520tasks.%2520This%2520challenge%2520is%250Aexacerbated%2520with%2520time-series%2520data%252C%2520which%2520also%2520exhibit%2520inherent%250Anonstationarity--variations%2520in%2520statistical%2520and%2520spectral%2520properties%2520over%2520time.%250AIn%2520this%2520work%252C%2520we%2520offer%2520a%2520fresh%2520perspective%2520on%2520learning%2520generalizable%250Arepresentations%2520for%2520time-series%2520classification%2520by%2520considering%2520the%2520phase%250Ainformation%2520of%2520a%2520signal%2520as%2520an%2520approximate%2520proxy%2520for%2520nonstationarity%2520and%2520propose%250Aa%2520phase-driven%2520generalizable%2520representation%2520learning%2520framework%2520for%2520time-series%250Aclassification%252C%2520PhASER.%2520It%2520consists%2520of%2520three%2520key%2520elements%253A%25201%2529%2520Hilbert%250Atransform-based%2520augmentation%252C%2520which%2520diversifies%2520nonstationarity%2520while%250Apreserving%2520task-specific%2520discriminatory%2520semantics%252C%25202%2529%2520separate%2520magnitude-phase%250Aencoding%252C%2520viewing%2520time-varying%2520magnitude%2520and%2520phase%2520as%2520independent%2520modalities%252C%250Aand%25203%2529%2520phase-residual%2520feature%2520broadcasting%252C%2520integrating%25202D%2520phase%2520features%2520with%250Aa%2520residual%2520connection%2520to%2520the%25201D%2520signal%2520representation%252C%2520providing%2520inherent%250Aregularization%2520to%2520improve%2520distribution-invariant%2520learning.%2520Extensive%250Aevaluations%2520on%2520five%2520datasets%2520from%2520sleep-stage%2520classification%252C%2520human%2520activity%250Arecognition%252C%2520and%2520gesture%2520recognition%2520against%252013%2520state-of-the-art%2520baseline%250Amethods%2520demonstrate%2520that%2520PhASER%2520consistently%2520outperforms%2520the%2520best%2520baselines%2520by%250Aan%2520average%2520of%25205%2525%2520and%2520up%2520to%252011%2525%2520in%2520some%2520cases.%2520Additionally%252C%2520the%2520principles%2520of%250APhASER%2520can%2520be%2520broadly%2520applied%2520to%2520enhance%2520the%2520generalizability%2520of%2520existing%250Atime-series%2520representation%2520learning%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.05960v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Phase-driven%20Domain%20Generalizable%20Learning%20for%20Nonstationary%20Time%20Series&entry.906535625=Payal%20Mohapatra%20and%20Lixu%20Wang%20and%20Qi%20Zhu&entry.1292438233=%20%20Pattern%20recognition%20is%20a%20fundamental%20task%20in%20continuous%20sensing%20applications%2C%0Abut%20real-world%20scenarios%20often%20experience%20distribution%20shifts%20that%20necessitate%0Alearning%20generalizable%20representations%20for%20such%20tasks.%20This%20challenge%20is%0Aexacerbated%20with%20time-series%20data%2C%20which%20also%20exhibit%20inherent%0Anonstationarity--variations%20in%20statistical%20and%20spectral%20properties%20over%20time.%0AIn%20this%20work%2C%20we%20offer%20a%20fresh%20perspective%20on%20learning%20generalizable%0Arepresentations%20for%20time-series%20classification%20by%20considering%20the%20phase%0Ainformation%20of%20a%20signal%20as%20an%20approximate%20proxy%20for%20nonstationarity%20and%20propose%0Aa%20phase-driven%20generalizable%20representation%20learning%20framework%20for%20time-series%0Aclassification%2C%20PhASER.%20It%20consists%20of%20three%20key%20elements%3A%201%29%20Hilbert%0Atransform-based%20augmentation%2C%20which%20diversifies%20nonstationarity%20while%0Apreserving%20task-specific%20discriminatory%20semantics%2C%202%29%20separate%20magnitude-phase%0Aencoding%2C%20viewing%20time-varying%20magnitude%20and%20phase%20as%20independent%20modalities%2C%0Aand%203%29%20phase-residual%20feature%20broadcasting%2C%20integrating%202D%20phase%20features%20with%0Aa%20residual%20connection%20to%20the%201D%20signal%20representation%2C%20providing%20inherent%0Aregularization%20to%20improve%20distribution-invariant%20learning.%20Extensive%0Aevaluations%20on%20five%20datasets%20from%20sleep-stage%20classification%2C%20human%20activity%0Arecognition%2C%20and%20gesture%20recognition%20against%2013%20state-of-the-art%20baseline%0Amethods%20demonstrate%20that%20PhASER%20consistently%20outperforms%20the%20best%20baselines%20by%0Aan%20average%20of%205%25%20and%20up%20to%2011%25%20in%20some%20cases.%20Additionally%2C%20the%20principles%20of%0APhASER%20can%20be%20broadly%20applied%20to%20enhance%20the%20generalizability%20of%20existing%0Atime-series%20representation%20learning%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.05960v2&entry.124074799=Read"},
{"title": "Hubble: a Model Suite to Advance the Study of LLM Memorization", "author": "Johnny Tian-Zheng Wei and Ameya Godbole and Mohammad Aflah Khan and Ryan Wang and Xiaoyuan Zhu and James Flemings and Nitya Kashyap and Krishna P. Gummadi and Willie Neiswanger and Robin Jia", "abstract": "  We present Hubble, a suite of fully open-source large language models (LLMs)\nfor the scientific study of LLM memorization. Hubble models come in standard\nand perturbed variants: standard models are pretrained on a large English\ncorpus, and perturbed models are trained in the same way but with controlled\ninsertion of text (e.g., book passages, biographies, and test sets) designed to\nemulate key memorization risks. Our core release includes 8 models -- standard\nand perturbed models with 1B or 8B parameters, pretrained on 100B or 500B\ntokens -- establishing that memorization risks are determined by the frequency\nof sensitive data relative to size of the training corpus (i.e., a password\nappearing once in a smaller corpus is memorized better than the same password\nin a larger corpus). Our release also includes 6 perturbed models with text\ninserted at different pretraining phases, showing that sensitive data without\ncontinued exposure can be forgotten. These findings suggest two best practices\nfor addressing memorization risks: to dilute sensitive data by increasing the\nsize of the training corpus, and to order sensitive data to appear earlier in\ntraining. Beyond these general empirical findings, Hubble enables a broad range\nof memorization research; for example, analyzing the biographies reveals how\nreadily different types of private information are memorized. We also\ndemonstrate that the randomized insertions in Hubble make it an ideal testbed\nfor membership inference and machine unlearning, and invite the community to\nfurther explore, benchmark, and build upon our work.\n", "link": "http://arxiv.org/abs/2510.19811v1", "date": "2025-10-22", "relevancy": 2.4636, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4973}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4973}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4836}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hubble%3A%20a%20Model%20Suite%20to%20Advance%20the%20Study%20of%20LLM%20Memorization&body=Title%3A%20Hubble%3A%20a%20Model%20Suite%20to%20Advance%20the%20Study%20of%20LLM%20Memorization%0AAuthor%3A%20Johnny%20Tian-Zheng%20Wei%20and%20Ameya%20Godbole%20and%20Mohammad%20Aflah%20Khan%20and%20Ryan%20Wang%20and%20Xiaoyuan%20Zhu%20and%20James%20Flemings%20and%20Nitya%20Kashyap%20and%20Krishna%20P.%20Gummadi%20and%20Willie%20Neiswanger%20and%20Robin%20Jia%0AAbstract%3A%20%20%20We%20present%20Hubble%2C%20a%20suite%20of%20fully%20open-source%20large%20language%20models%20%28LLMs%29%0Afor%20the%20scientific%20study%20of%20LLM%20memorization.%20Hubble%20models%20come%20in%20standard%0Aand%20perturbed%20variants%3A%20standard%20models%20are%20pretrained%20on%20a%20large%20English%0Acorpus%2C%20and%20perturbed%20models%20are%20trained%20in%20the%20same%20way%20but%20with%20controlled%0Ainsertion%20of%20text%20%28e.g.%2C%20book%20passages%2C%20biographies%2C%20and%20test%20sets%29%20designed%20to%0Aemulate%20key%20memorization%20risks.%20Our%20core%20release%20includes%208%20models%20--%20standard%0Aand%20perturbed%20models%20with%201B%20or%208B%20parameters%2C%20pretrained%20on%20100B%20or%20500B%0Atokens%20--%20establishing%20that%20memorization%20risks%20are%20determined%20by%20the%20frequency%0Aof%20sensitive%20data%20relative%20to%20size%20of%20the%20training%20corpus%20%28i.e.%2C%20a%20password%0Aappearing%20once%20in%20a%20smaller%20corpus%20is%20memorized%20better%20than%20the%20same%20password%0Ain%20a%20larger%20corpus%29.%20Our%20release%20also%20includes%206%20perturbed%20models%20with%20text%0Ainserted%20at%20different%20pretraining%20phases%2C%20showing%20that%20sensitive%20data%20without%0Acontinued%20exposure%20can%20be%20forgotten.%20These%20findings%20suggest%20two%20best%20practices%0Afor%20addressing%20memorization%20risks%3A%20to%20dilute%20sensitive%20data%20by%20increasing%20the%0Asize%20of%20the%20training%20corpus%2C%20and%20to%20order%20sensitive%20data%20to%20appear%20earlier%20in%0Atraining.%20Beyond%20these%20general%20empirical%20findings%2C%20Hubble%20enables%20a%20broad%20range%0Aof%20memorization%20research%3B%20for%20example%2C%20analyzing%20the%20biographies%20reveals%20how%0Areadily%20different%20types%20of%20private%20information%20are%20memorized.%20We%20also%0Ademonstrate%20that%20the%20randomized%20insertions%20in%20Hubble%20make%20it%20an%20ideal%20testbed%0Afor%20membership%20inference%20and%20machine%20unlearning%2C%20and%20invite%20the%20community%20to%0Afurther%20explore%2C%20benchmark%2C%20and%20build%20upon%20our%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19811v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHubble%253A%2520a%2520Model%2520Suite%2520to%2520Advance%2520the%2520Study%2520of%2520LLM%2520Memorization%26entry.906535625%3DJohnny%2520Tian-Zheng%2520Wei%2520and%2520Ameya%2520Godbole%2520and%2520Mohammad%2520Aflah%2520Khan%2520and%2520Ryan%2520Wang%2520and%2520Xiaoyuan%2520Zhu%2520and%2520James%2520Flemings%2520and%2520Nitya%2520Kashyap%2520and%2520Krishna%2520P.%2520Gummadi%2520and%2520Willie%2520Neiswanger%2520and%2520Robin%2520Jia%26entry.1292438233%3D%2520%2520We%2520present%2520Hubble%252C%2520a%2520suite%2520of%2520fully%2520open-source%2520large%2520language%2520models%2520%2528LLMs%2529%250Afor%2520the%2520scientific%2520study%2520of%2520LLM%2520memorization.%2520Hubble%2520models%2520come%2520in%2520standard%250Aand%2520perturbed%2520variants%253A%2520standard%2520models%2520are%2520pretrained%2520on%2520a%2520large%2520English%250Acorpus%252C%2520and%2520perturbed%2520models%2520are%2520trained%2520in%2520the%2520same%2520way%2520but%2520with%2520controlled%250Ainsertion%2520of%2520text%2520%2528e.g.%252C%2520book%2520passages%252C%2520biographies%252C%2520and%2520test%2520sets%2529%2520designed%2520to%250Aemulate%2520key%2520memorization%2520risks.%2520Our%2520core%2520release%2520includes%25208%2520models%2520--%2520standard%250Aand%2520perturbed%2520models%2520with%25201B%2520or%25208B%2520parameters%252C%2520pretrained%2520on%2520100B%2520or%2520500B%250Atokens%2520--%2520establishing%2520that%2520memorization%2520risks%2520are%2520determined%2520by%2520the%2520frequency%250Aof%2520sensitive%2520data%2520relative%2520to%2520size%2520of%2520the%2520training%2520corpus%2520%2528i.e.%252C%2520a%2520password%250Aappearing%2520once%2520in%2520a%2520smaller%2520corpus%2520is%2520memorized%2520better%2520than%2520the%2520same%2520password%250Ain%2520a%2520larger%2520corpus%2529.%2520Our%2520release%2520also%2520includes%25206%2520perturbed%2520models%2520with%2520text%250Ainserted%2520at%2520different%2520pretraining%2520phases%252C%2520showing%2520that%2520sensitive%2520data%2520without%250Acontinued%2520exposure%2520can%2520be%2520forgotten.%2520These%2520findings%2520suggest%2520two%2520best%2520practices%250Afor%2520addressing%2520memorization%2520risks%253A%2520to%2520dilute%2520sensitive%2520data%2520by%2520increasing%2520the%250Asize%2520of%2520the%2520training%2520corpus%252C%2520and%2520to%2520order%2520sensitive%2520data%2520to%2520appear%2520earlier%2520in%250Atraining.%2520Beyond%2520these%2520general%2520empirical%2520findings%252C%2520Hubble%2520enables%2520a%2520broad%2520range%250Aof%2520memorization%2520research%253B%2520for%2520example%252C%2520analyzing%2520the%2520biographies%2520reveals%2520how%250Areadily%2520different%2520types%2520of%2520private%2520information%2520are%2520memorized.%2520We%2520also%250Ademonstrate%2520that%2520the%2520randomized%2520insertions%2520in%2520Hubble%2520make%2520it%2520an%2520ideal%2520testbed%250Afor%2520membership%2520inference%2520and%2520machine%2520unlearning%252C%2520and%2520invite%2520the%2520community%2520to%250Afurther%2520explore%252C%2520benchmark%252C%2520and%2520build%2520upon%2520our%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19811v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hubble%3A%20a%20Model%20Suite%20to%20Advance%20the%20Study%20of%20LLM%20Memorization&entry.906535625=Johnny%20Tian-Zheng%20Wei%20and%20Ameya%20Godbole%20and%20Mohammad%20Aflah%20Khan%20and%20Ryan%20Wang%20and%20Xiaoyuan%20Zhu%20and%20James%20Flemings%20and%20Nitya%20Kashyap%20and%20Krishna%20P.%20Gummadi%20and%20Willie%20Neiswanger%20and%20Robin%20Jia&entry.1292438233=%20%20We%20present%20Hubble%2C%20a%20suite%20of%20fully%20open-source%20large%20language%20models%20%28LLMs%29%0Afor%20the%20scientific%20study%20of%20LLM%20memorization.%20Hubble%20models%20come%20in%20standard%0Aand%20perturbed%20variants%3A%20standard%20models%20are%20pretrained%20on%20a%20large%20English%0Acorpus%2C%20and%20perturbed%20models%20are%20trained%20in%20the%20same%20way%20but%20with%20controlled%0Ainsertion%20of%20text%20%28e.g.%2C%20book%20passages%2C%20biographies%2C%20and%20test%20sets%29%20designed%20to%0Aemulate%20key%20memorization%20risks.%20Our%20core%20release%20includes%208%20models%20--%20standard%0Aand%20perturbed%20models%20with%201B%20or%208B%20parameters%2C%20pretrained%20on%20100B%20or%20500B%0Atokens%20--%20establishing%20that%20memorization%20risks%20are%20determined%20by%20the%20frequency%0Aof%20sensitive%20data%20relative%20to%20size%20of%20the%20training%20corpus%20%28i.e.%2C%20a%20password%0Aappearing%20once%20in%20a%20smaller%20corpus%20is%20memorized%20better%20than%20the%20same%20password%0Ain%20a%20larger%20corpus%29.%20Our%20release%20also%20includes%206%20perturbed%20models%20with%20text%0Ainserted%20at%20different%20pretraining%20phases%2C%20showing%20that%20sensitive%20data%20without%0Acontinued%20exposure%20can%20be%20forgotten.%20These%20findings%20suggest%20two%20best%20practices%0Afor%20addressing%20memorization%20risks%3A%20to%20dilute%20sensitive%20data%20by%20increasing%20the%0Asize%20of%20the%20training%20corpus%2C%20and%20to%20order%20sensitive%20data%20to%20appear%20earlier%20in%0Atraining.%20Beyond%20these%20general%20empirical%20findings%2C%20Hubble%20enables%20a%20broad%20range%0Aof%20memorization%20research%3B%20for%20example%2C%20analyzing%20the%20biographies%20reveals%20how%0Areadily%20different%20types%20of%20private%20information%20are%20memorized.%20We%20also%0Ademonstrate%20that%20the%20randomized%20insertions%20in%20Hubble%20make%20it%20an%20ideal%20testbed%0Afor%20membership%20inference%20and%20machine%20unlearning%2C%20and%20invite%20the%20community%20to%0Afurther%20explore%2C%20benchmark%2C%20and%20build%20upon%20our%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19811v1&entry.124074799=Read"},
{"title": "GaLLoP: Gradient-based Sparse Learning on Low-Magnitude Parameters", "author": "Anand Choudhary and Yasser Sula\u0131man and Lukas Mauch and Ghouthi Boukli Hacene and Fabien Cardinaux and Antoine Bosselut", "abstract": "  Sparse fine-tuning techniques adapt LLMs to downstream tasks by only tuning a\nsparse subset of model parameters. However, the effectiveness of sparse\nadaptation depends on optimally selecting the model parameters to be\nfine-tuned. In this work, we introduce a novel sparse fine-tuning technique\nnamed GaLLoP: Gradient-based Sparse Learning on Low-Magnitude Parameters, which\nfine-tunes only those model parameters which have the largest gradient\nmagnitudes on downstream tasks and the smallest pre-trained magnitudes,\nintuitively prioritizing parameters that are highly task-relevant, but\nminimally disruptive to pre-trained knowledge. Our experimentation with LLaMA3\n8B and Gemma 2B as base models shows that GaLLoP consistently improves or\nmatches the in-distribution as well as out-of-distribution performance obtained\nvia the usage of other leading parameter-efficient fine-tuning techniques,\nincluding LoRA, DoRA, and SAFT. Our analysis demonstrates that GaLLoP mitigates\ncatastrophic forgetting and memorization of task data, as important pre-trained\nparameters remain unchanged, and stabilizes performance relative to other\nfine-tuning techniques, robustly generalizing across most random seeds.\n", "link": "http://arxiv.org/abs/2510.19778v1", "date": "2025-10-22", "relevancy": 2.4614, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5014}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4878}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaLLoP%3A%20Gradient-based%20Sparse%20Learning%20on%20Low-Magnitude%20Parameters&body=Title%3A%20GaLLoP%3A%20Gradient-based%20Sparse%20Learning%20on%20Low-Magnitude%20Parameters%0AAuthor%3A%20Anand%20Choudhary%20and%20Yasser%20Sula%C4%B1man%20and%20Lukas%20Mauch%20and%20Ghouthi%20Boukli%20Hacene%20and%20Fabien%20Cardinaux%20and%20Antoine%20Bosselut%0AAbstract%3A%20%20%20Sparse%20fine-tuning%20techniques%20adapt%20LLMs%20to%20downstream%20tasks%20by%20only%20tuning%20a%0Asparse%20subset%20of%20model%20parameters.%20However%2C%20the%20effectiveness%20of%20sparse%0Aadaptation%20depends%20on%20optimally%20selecting%20the%20model%20parameters%20to%20be%0Afine-tuned.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20sparse%20fine-tuning%20technique%0Anamed%20GaLLoP%3A%20Gradient-based%20Sparse%20Learning%20on%20Low-Magnitude%20Parameters%2C%20which%0Afine-tunes%20only%20those%20model%20parameters%20which%20have%20the%20largest%20gradient%0Amagnitudes%20on%20downstream%20tasks%20and%20the%20smallest%20pre-trained%20magnitudes%2C%0Aintuitively%20prioritizing%20parameters%20that%20are%20highly%20task-relevant%2C%20but%0Aminimally%20disruptive%20to%20pre-trained%20knowledge.%20Our%20experimentation%20with%20LLaMA3%0A8B%20and%20Gemma%202B%20as%20base%20models%20shows%20that%20GaLLoP%20consistently%20improves%20or%0Amatches%20the%20in-distribution%20as%20well%20as%20out-of-distribution%20performance%20obtained%0Avia%20the%20usage%20of%20other%20leading%20parameter-efficient%20fine-tuning%20techniques%2C%0Aincluding%20LoRA%2C%20DoRA%2C%20and%20SAFT.%20Our%20analysis%20demonstrates%20that%20GaLLoP%20mitigates%0Acatastrophic%20forgetting%20and%20memorization%20of%20task%20data%2C%20as%20important%20pre-trained%0Aparameters%20remain%20unchanged%2C%20and%20stabilizes%20performance%20relative%20to%20other%0Afine-tuning%20techniques%2C%20robustly%20generalizing%20across%20most%20random%20seeds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19778v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaLLoP%253A%2520Gradient-based%2520Sparse%2520Learning%2520on%2520Low-Magnitude%2520Parameters%26entry.906535625%3DAnand%2520Choudhary%2520and%2520Yasser%2520Sula%25C4%25B1man%2520and%2520Lukas%2520Mauch%2520and%2520Ghouthi%2520Boukli%2520Hacene%2520and%2520Fabien%2520Cardinaux%2520and%2520Antoine%2520Bosselut%26entry.1292438233%3D%2520%2520Sparse%2520fine-tuning%2520techniques%2520adapt%2520LLMs%2520to%2520downstream%2520tasks%2520by%2520only%2520tuning%2520a%250Asparse%2520subset%2520of%2520model%2520parameters.%2520However%252C%2520the%2520effectiveness%2520of%2520sparse%250Aadaptation%2520depends%2520on%2520optimally%2520selecting%2520the%2520model%2520parameters%2520to%2520be%250Afine-tuned.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520sparse%2520fine-tuning%2520technique%250Anamed%2520GaLLoP%253A%2520Gradient-based%2520Sparse%2520Learning%2520on%2520Low-Magnitude%2520Parameters%252C%2520which%250Afine-tunes%2520only%2520those%2520model%2520parameters%2520which%2520have%2520the%2520largest%2520gradient%250Amagnitudes%2520on%2520downstream%2520tasks%2520and%2520the%2520smallest%2520pre-trained%2520magnitudes%252C%250Aintuitively%2520prioritizing%2520parameters%2520that%2520are%2520highly%2520task-relevant%252C%2520but%250Aminimally%2520disruptive%2520to%2520pre-trained%2520knowledge.%2520Our%2520experimentation%2520with%2520LLaMA3%250A8B%2520and%2520Gemma%25202B%2520as%2520base%2520models%2520shows%2520that%2520GaLLoP%2520consistently%2520improves%2520or%250Amatches%2520the%2520in-distribution%2520as%2520well%2520as%2520out-of-distribution%2520performance%2520obtained%250Avia%2520the%2520usage%2520of%2520other%2520leading%2520parameter-efficient%2520fine-tuning%2520techniques%252C%250Aincluding%2520LoRA%252C%2520DoRA%252C%2520and%2520SAFT.%2520Our%2520analysis%2520demonstrates%2520that%2520GaLLoP%2520mitigates%250Acatastrophic%2520forgetting%2520and%2520memorization%2520of%2520task%2520data%252C%2520as%2520important%2520pre-trained%250Aparameters%2520remain%2520unchanged%252C%2520and%2520stabilizes%2520performance%2520relative%2520to%2520other%250Afine-tuning%2520techniques%252C%2520robustly%2520generalizing%2520across%2520most%2520random%2520seeds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19778v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaLLoP%3A%20Gradient-based%20Sparse%20Learning%20on%20Low-Magnitude%20Parameters&entry.906535625=Anand%20Choudhary%20and%20Yasser%20Sula%C4%B1man%20and%20Lukas%20Mauch%20and%20Ghouthi%20Boukli%20Hacene%20and%20Fabien%20Cardinaux%20and%20Antoine%20Bosselut&entry.1292438233=%20%20Sparse%20fine-tuning%20techniques%20adapt%20LLMs%20to%20downstream%20tasks%20by%20only%20tuning%20a%0Asparse%20subset%20of%20model%20parameters.%20However%2C%20the%20effectiveness%20of%20sparse%0Aadaptation%20depends%20on%20optimally%20selecting%20the%20model%20parameters%20to%20be%0Afine-tuned.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20sparse%20fine-tuning%20technique%0Anamed%20GaLLoP%3A%20Gradient-based%20Sparse%20Learning%20on%20Low-Magnitude%20Parameters%2C%20which%0Afine-tunes%20only%20those%20model%20parameters%20which%20have%20the%20largest%20gradient%0Amagnitudes%20on%20downstream%20tasks%20and%20the%20smallest%20pre-trained%20magnitudes%2C%0Aintuitively%20prioritizing%20parameters%20that%20are%20highly%20task-relevant%2C%20but%0Aminimally%20disruptive%20to%20pre-trained%20knowledge.%20Our%20experimentation%20with%20LLaMA3%0A8B%20and%20Gemma%202B%20as%20base%20models%20shows%20that%20GaLLoP%20consistently%20improves%20or%0Amatches%20the%20in-distribution%20as%20well%20as%20out-of-distribution%20performance%20obtained%0Avia%20the%20usage%20of%20other%20leading%20parameter-efficient%20fine-tuning%20techniques%2C%0Aincluding%20LoRA%2C%20DoRA%2C%20and%20SAFT.%20Our%20analysis%20demonstrates%20that%20GaLLoP%20mitigates%0Acatastrophic%20forgetting%20and%20memorization%20of%20task%20data%2C%20as%20important%20pre-trained%0Aparameters%20remain%20unchanged%2C%20and%20stabilizes%20performance%20relative%20to%20other%0Afine-tuning%20techniques%2C%20robustly%20generalizing%20across%20most%20random%20seeds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19778v1&entry.124074799=Read"},
{"title": "AtomSurf : Surface Representation for Learning on Protein Structures", "author": "Vincent Mallet and Souhaib Attaiki and Yangyang Miao and Bruno Correia and Maks Ovsjanikov", "abstract": "  While there has been significant progress in evaluating and comparing\ndifferent representations for learning on protein data, the role of\nsurface-based learning approaches remains not well-understood. In particular,\nthere is a lack of direct and fair benchmark comparison between the best\navailable surface-based learning methods against alternative representations\nsuch as graphs. Moreover, the few existing surface-based approaches either use\nsurface information in isolation or, at best, perform global pooling between\nsurface and graph-based architectures.\n  In this work, we fill this gap by first adapting a state-of-the-art surface\nencoder for protein learning tasks. We then perform a direct and fair\ncomparison of the resulting method against alternative approaches within the\nAtom3D benchmark, highlighting the limitations of pure surface-based learning.\nFinally, we propose an integrated approach, which allows learned feature\nsharing between graphs and surface representations on the level of nodes and\nvertices across all layers.\n  We demonstrate that the resulting architecture achieves state-of-the-art\nresults on all tasks in the Atom3D benchmark, while adhering to the strict\nbenchmark protocol, as well as more broadly on binding site identification and\nbinding pocket classification. Furthermore, we use coarsened surfaces and\noptimize our approach for efficiency, making our tool competitive in training\nand inference time with existing techniques. Code can be found online:\nhttps://github.com/Vincentx15/atomsurf\n", "link": "http://arxiv.org/abs/2309.16519v4", "date": "2025-10-22", "relevancy": 2.4256, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4914}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.482}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AtomSurf%20%3A%20Surface%20Representation%20for%20Learning%20on%20Protein%20Structures&body=Title%3A%20AtomSurf%20%3A%20Surface%20Representation%20for%20Learning%20on%20Protein%20Structures%0AAuthor%3A%20Vincent%20Mallet%20and%20Souhaib%20Attaiki%20and%20Yangyang%20Miao%20and%20Bruno%20Correia%20and%20Maks%20Ovsjanikov%0AAbstract%3A%20%20%20While%20there%20has%20been%20significant%20progress%20in%20evaluating%20and%20comparing%0Adifferent%20representations%20for%20learning%20on%20protein%20data%2C%20the%20role%20of%0Asurface-based%20learning%20approaches%20remains%20not%20well-understood.%20In%20particular%2C%0Athere%20is%20a%20lack%20of%20direct%20and%20fair%20benchmark%20comparison%20between%20the%20best%0Aavailable%20surface-based%20learning%20methods%20against%20alternative%20representations%0Asuch%20as%20graphs.%20Moreover%2C%20the%20few%20existing%20surface-based%20approaches%20either%20use%0Asurface%20information%20in%20isolation%20or%2C%20at%20best%2C%20perform%20global%20pooling%20between%0Asurface%20and%20graph-based%20architectures.%0A%20%20In%20this%20work%2C%20we%20fill%20this%20gap%20by%20first%20adapting%20a%20state-of-the-art%20surface%0Aencoder%20for%20protein%20learning%20tasks.%20We%20then%20perform%20a%20direct%20and%20fair%0Acomparison%20of%20the%20resulting%20method%20against%20alternative%20approaches%20within%20the%0AAtom3D%20benchmark%2C%20highlighting%20the%20limitations%20of%20pure%20surface-based%20learning.%0AFinally%2C%20we%20propose%20an%20integrated%20approach%2C%20which%20allows%20learned%20feature%0Asharing%20between%20graphs%20and%20surface%20representations%20on%20the%20level%20of%20nodes%20and%0Avertices%20across%20all%20layers.%0A%20%20We%20demonstrate%20that%20the%20resulting%20architecture%20achieves%20state-of-the-art%0Aresults%20on%20all%20tasks%20in%20the%20Atom3D%20benchmark%2C%20while%20adhering%20to%20the%20strict%0Abenchmark%20protocol%2C%20as%20well%20as%20more%20broadly%20on%20binding%20site%20identification%20and%0Abinding%20pocket%20classification.%20Furthermore%2C%20we%20use%20coarsened%20surfaces%20and%0Aoptimize%20our%20approach%20for%20efficiency%2C%20making%20our%20tool%20competitive%20in%20training%0Aand%20inference%20time%20with%20existing%20techniques.%20Code%20can%20be%20found%20online%3A%0Ahttps%3A//github.com/Vincentx15/atomsurf%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.16519v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAtomSurf%2520%253A%2520Surface%2520Representation%2520for%2520Learning%2520on%2520Protein%2520Structures%26entry.906535625%3DVincent%2520Mallet%2520and%2520Souhaib%2520Attaiki%2520and%2520Yangyang%2520Miao%2520and%2520Bruno%2520Correia%2520and%2520Maks%2520Ovsjanikov%26entry.1292438233%3D%2520%2520While%2520there%2520has%2520been%2520significant%2520progress%2520in%2520evaluating%2520and%2520comparing%250Adifferent%2520representations%2520for%2520learning%2520on%2520protein%2520data%252C%2520the%2520role%2520of%250Asurface-based%2520learning%2520approaches%2520remains%2520not%2520well-understood.%2520In%2520particular%252C%250Athere%2520is%2520a%2520lack%2520of%2520direct%2520and%2520fair%2520benchmark%2520comparison%2520between%2520the%2520best%250Aavailable%2520surface-based%2520learning%2520methods%2520against%2520alternative%2520representations%250Asuch%2520as%2520graphs.%2520Moreover%252C%2520the%2520few%2520existing%2520surface-based%2520approaches%2520either%2520use%250Asurface%2520information%2520in%2520isolation%2520or%252C%2520at%2520best%252C%2520perform%2520global%2520pooling%2520between%250Asurface%2520and%2520graph-based%2520architectures.%250A%2520%2520In%2520this%2520work%252C%2520we%2520fill%2520this%2520gap%2520by%2520first%2520adapting%2520a%2520state-of-the-art%2520surface%250Aencoder%2520for%2520protein%2520learning%2520tasks.%2520We%2520then%2520perform%2520a%2520direct%2520and%2520fair%250Acomparison%2520of%2520the%2520resulting%2520method%2520against%2520alternative%2520approaches%2520within%2520the%250AAtom3D%2520benchmark%252C%2520highlighting%2520the%2520limitations%2520of%2520pure%2520surface-based%2520learning.%250AFinally%252C%2520we%2520propose%2520an%2520integrated%2520approach%252C%2520which%2520allows%2520learned%2520feature%250Asharing%2520between%2520graphs%2520and%2520surface%2520representations%2520on%2520the%2520level%2520of%2520nodes%2520and%250Avertices%2520across%2520all%2520layers.%250A%2520%2520We%2520demonstrate%2520that%2520the%2520resulting%2520architecture%2520achieves%2520state-of-the-art%250Aresults%2520on%2520all%2520tasks%2520in%2520the%2520Atom3D%2520benchmark%252C%2520while%2520adhering%2520to%2520the%2520strict%250Abenchmark%2520protocol%252C%2520as%2520well%2520as%2520more%2520broadly%2520on%2520binding%2520site%2520identification%2520and%250Abinding%2520pocket%2520classification.%2520Furthermore%252C%2520we%2520use%2520coarsened%2520surfaces%2520and%250Aoptimize%2520our%2520approach%2520for%2520efficiency%252C%2520making%2520our%2520tool%2520competitive%2520in%2520training%250Aand%2520inference%2520time%2520with%2520existing%2520techniques.%2520Code%2520can%2520be%2520found%2520online%253A%250Ahttps%253A//github.com/Vincentx15/atomsurf%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.16519v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AtomSurf%20%3A%20Surface%20Representation%20for%20Learning%20on%20Protein%20Structures&entry.906535625=Vincent%20Mallet%20and%20Souhaib%20Attaiki%20and%20Yangyang%20Miao%20and%20Bruno%20Correia%20and%20Maks%20Ovsjanikov&entry.1292438233=%20%20While%20there%20has%20been%20significant%20progress%20in%20evaluating%20and%20comparing%0Adifferent%20representations%20for%20learning%20on%20protein%20data%2C%20the%20role%20of%0Asurface-based%20learning%20approaches%20remains%20not%20well-understood.%20In%20particular%2C%0Athere%20is%20a%20lack%20of%20direct%20and%20fair%20benchmark%20comparison%20between%20the%20best%0Aavailable%20surface-based%20learning%20methods%20against%20alternative%20representations%0Asuch%20as%20graphs.%20Moreover%2C%20the%20few%20existing%20surface-based%20approaches%20either%20use%0Asurface%20information%20in%20isolation%20or%2C%20at%20best%2C%20perform%20global%20pooling%20between%0Asurface%20and%20graph-based%20architectures.%0A%20%20In%20this%20work%2C%20we%20fill%20this%20gap%20by%20first%20adapting%20a%20state-of-the-art%20surface%0Aencoder%20for%20protein%20learning%20tasks.%20We%20then%20perform%20a%20direct%20and%20fair%0Acomparison%20of%20the%20resulting%20method%20against%20alternative%20approaches%20within%20the%0AAtom3D%20benchmark%2C%20highlighting%20the%20limitations%20of%20pure%20surface-based%20learning.%0AFinally%2C%20we%20propose%20an%20integrated%20approach%2C%20which%20allows%20learned%20feature%0Asharing%20between%20graphs%20and%20surface%20representations%20on%20the%20level%20of%20nodes%20and%0Avertices%20across%20all%20layers.%0A%20%20We%20demonstrate%20that%20the%20resulting%20architecture%20achieves%20state-of-the-art%0Aresults%20on%20all%20tasks%20in%20the%20Atom3D%20benchmark%2C%20while%20adhering%20to%20the%20strict%0Abenchmark%20protocol%2C%20as%20well%20as%20more%20broadly%20on%20binding%20site%20identification%20and%0Abinding%20pocket%20classification.%20Furthermore%2C%20we%20use%20coarsened%20surfaces%20and%0Aoptimize%20our%20approach%20for%20efficiency%2C%20making%20our%20tool%20competitive%20in%20training%0Aand%20inference%20time%20with%20existing%20techniques.%20Code%20can%20be%20found%20online%3A%0Ahttps%3A//github.com/Vincentx15/atomsurf%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.16519v4&entry.124074799=Read"},
{"title": "I Spy With My Model's Eye: Visual Search as a Behavioural Test for MLLMs", "author": "John Burden and Jonathan Prunty and Ben Slater and Matthieu Tehenan and Greg Davis and Lucy Cheke", "abstract": "  Multimodal large language models (MLLMs) achieve strong performance on\nvision-language tasks, yet their visual processing is opaque. Most black-box\nevaluations measure task accuracy, but reveal little about underlying\nmechanisms. Drawing on cognitive psychology, we adapt classic visual search\nparadigms -- originally developed to study human perception -- to test whether\nMLLMs exhibit the ``pop-out'' effect, where salient visual features are\ndetected independently of distractor set size. Using controlled experiments\ntargeting colour, size and lighting features, we find that advanced MLLMs\nexhibit human-like pop-out effects in colour or size-based disjunctive (single\nfeature) search, as well as capacity limits for conjunctive (multiple feature)\nsearch. We also find evidence to suggest that MLLMs, like humans, incorporate\nnatural scene priors such as lighting direction into object representations. We\nreinforce our findings using targeted fine-tuning and mechanistic\ninterpretability analyses. Our work shows how visual search can serve as a\ncognitively grounded diagnostic tool for evaluating perceptual capabilities in\nMLLMs.\n", "link": "http://arxiv.org/abs/2510.19678v1", "date": "2025-10-22", "relevancy": 2.4037, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6109}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6109}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20I%20Spy%20With%20My%20Model%27s%20Eye%3A%20Visual%20Search%20as%20a%20Behavioural%20Test%20for%20MLLMs&body=Title%3A%20I%20Spy%20With%20My%20Model%27s%20Eye%3A%20Visual%20Search%20as%20a%20Behavioural%20Test%20for%20MLLMs%0AAuthor%3A%20John%20Burden%20and%20Jonathan%20Prunty%20and%20Ben%20Slater%20and%20Matthieu%20Tehenan%20and%20Greg%20Davis%20and%20Lucy%20Cheke%0AAbstract%3A%20%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20achieve%20strong%20performance%20on%0Avision-language%20tasks%2C%20yet%20their%20visual%20processing%20is%20opaque.%20Most%20black-box%0Aevaluations%20measure%20task%20accuracy%2C%20but%20reveal%20little%20about%20underlying%0Amechanisms.%20Drawing%20on%20cognitive%20psychology%2C%20we%20adapt%20classic%20visual%20search%0Aparadigms%20--%20originally%20developed%20to%20study%20human%20perception%20--%20to%20test%20whether%0AMLLMs%20exhibit%20the%20%60%60pop-out%27%27%20effect%2C%20where%20salient%20visual%20features%20are%0Adetected%20independently%20of%20distractor%20set%20size.%20Using%20controlled%20experiments%0Atargeting%20colour%2C%20size%20and%20lighting%20features%2C%20we%20find%20that%20advanced%20MLLMs%0Aexhibit%20human-like%20pop-out%20effects%20in%20colour%20or%20size-based%20disjunctive%20%28single%0Afeature%29%20search%2C%20as%20well%20as%20capacity%20limits%20for%20conjunctive%20%28multiple%20feature%29%0Asearch.%20We%20also%20find%20evidence%20to%20suggest%20that%20MLLMs%2C%20like%20humans%2C%20incorporate%0Anatural%20scene%20priors%20such%20as%20lighting%20direction%20into%20object%20representations.%20We%0Areinforce%20our%20findings%20using%20targeted%20fine-tuning%20and%20mechanistic%0Ainterpretability%20analyses.%20Our%20work%20shows%20how%20visual%20search%20can%20serve%20as%20a%0Acognitively%20grounded%20diagnostic%20tool%20for%20evaluating%20perceptual%20capabilities%20in%0AMLLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19678v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DI%2520Spy%2520With%2520My%2520Model%2527s%2520Eye%253A%2520Visual%2520Search%2520as%2520a%2520Behavioural%2520Test%2520for%2520MLLMs%26entry.906535625%3DJohn%2520Burden%2520and%2520Jonathan%2520Prunty%2520and%2520Ben%2520Slater%2520and%2520Matthieu%2520Tehenan%2520and%2520Greg%2520Davis%2520and%2520Lucy%2520Cheke%26entry.1292438233%3D%2520%2520Multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520achieve%2520strong%2520performance%2520on%250Avision-language%2520tasks%252C%2520yet%2520their%2520visual%2520processing%2520is%2520opaque.%2520Most%2520black-box%250Aevaluations%2520measure%2520task%2520accuracy%252C%2520but%2520reveal%2520little%2520about%2520underlying%250Amechanisms.%2520Drawing%2520on%2520cognitive%2520psychology%252C%2520we%2520adapt%2520classic%2520visual%2520search%250Aparadigms%2520--%2520originally%2520developed%2520to%2520study%2520human%2520perception%2520--%2520to%2520test%2520whether%250AMLLMs%2520exhibit%2520the%2520%2560%2560pop-out%2527%2527%2520effect%252C%2520where%2520salient%2520visual%2520features%2520are%250Adetected%2520independently%2520of%2520distractor%2520set%2520size.%2520Using%2520controlled%2520experiments%250Atargeting%2520colour%252C%2520size%2520and%2520lighting%2520features%252C%2520we%2520find%2520that%2520advanced%2520MLLMs%250Aexhibit%2520human-like%2520pop-out%2520effects%2520in%2520colour%2520or%2520size-based%2520disjunctive%2520%2528single%250Afeature%2529%2520search%252C%2520as%2520well%2520as%2520capacity%2520limits%2520for%2520conjunctive%2520%2528multiple%2520feature%2529%250Asearch.%2520We%2520also%2520find%2520evidence%2520to%2520suggest%2520that%2520MLLMs%252C%2520like%2520humans%252C%2520incorporate%250Anatural%2520scene%2520priors%2520such%2520as%2520lighting%2520direction%2520into%2520object%2520representations.%2520We%250Areinforce%2520our%2520findings%2520using%2520targeted%2520fine-tuning%2520and%2520mechanistic%250Ainterpretability%2520analyses.%2520Our%2520work%2520shows%2520how%2520visual%2520search%2520can%2520serve%2520as%2520a%250Acognitively%2520grounded%2520diagnostic%2520tool%2520for%2520evaluating%2520perceptual%2520capabilities%2520in%250AMLLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19678v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=I%20Spy%20With%20My%20Model%27s%20Eye%3A%20Visual%20Search%20as%20a%20Behavioural%20Test%20for%20MLLMs&entry.906535625=John%20Burden%20and%20Jonathan%20Prunty%20and%20Ben%20Slater%20and%20Matthieu%20Tehenan%20and%20Greg%20Davis%20and%20Lucy%20Cheke&entry.1292438233=%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20achieve%20strong%20performance%20on%0Avision-language%20tasks%2C%20yet%20their%20visual%20processing%20is%20opaque.%20Most%20black-box%0Aevaluations%20measure%20task%20accuracy%2C%20but%20reveal%20little%20about%20underlying%0Amechanisms.%20Drawing%20on%20cognitive%20psychology%2C%20we%20adapt%20classic%20visual%20search%0Aparadigms%20--%20originally%20developed%20to%20study%20human%20perception%20--%20to%20test%20whether%0AMLLMs%20exhibit%20the%20%60%60pop-out%27%27%20effect%2C%20where%20salient%20visual%20features%20are%0Adetected%20independently%20of%20distractor%20set%20size.%20Using%20controlled%20experiments%0Atargeting%20colour%2C%20size%20and%20lighting%20features%2C%20we%20find%20that%20advanced%20MLLMs%0Aexhibit%20human-like%20pop-out%20effects%20in%20colour%20or%20size-based%20disjunctive%20%28single%0Afeature%29%20search%2C%20as%20well%20as%20capacity%20limits%20for%20conjunctive%20%28multiple%20feature%29%0Asearch.%20We%20also%20find%20evidence%20to%20suggest%20that%20MLLMs%2C%20like%20humans%2C%20incorporate%0Anatural%20scene%20priors%20such%20as%20lighting%20direction%20into%20object%20representations.%20We%0Areinforce%20our%20findings%20using%20targeted%20fine-tuning%20and%20mechanistic%0Ainterpretability%20analyses.%20Our%20work%20shows%20how%20visual%20search%20can%20serve%20as%20a%0Acognitively%20grounded%20diagnostic%20tool%20for%20evaluating%20perceptual%20capabilities%20in%0AMLLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19678v1&entry.124074799=Read"},
{"title": "Zhyper: Factorized Hypernetworks for Conditioned LLM Fine-Tuning", "author": "M. H. I. Abdalla and Zhipin Wang and Christian Frey and Steffen Eger and Josif Grabocka", "abstract": "  Large Language Model (LLM) conditioning refers to instructing an LLM to\ngenerate content in accordance with the norms and values of a specific culture,\nbeliefs of a particular political orientation, or any desired text-specified\nsemantic conditioning. Unfortunately, prompt engineering does not ensure that\nLLMs behave in accordance with a desired conditioning due to the inductive bias\nof the pre-training and alignment datasets. Prior works have focused on\nfine-tuning LLMs by directly conditioning the LoRA weights; however, such\nmethods introduce a large number of parameters. As a remedy, we propose Zhyper,\na parameter-efficient factorized hypernetwork framework that generates\ncontext-aware LoRA adapters from textual descriptions. Experiments on multiple\nbenchmarks show that Zhyper achieves competitive performance with up to 26x\nfewer parameters than the state-of-the-art baselines. Furthermore, we extend\nZhyper to cultural alignment, demonstrating improved generalization to\nout-of-domain settings and a better capturing of fine-grained contextual\nvalues.\n", "link": "http://arxiv.org/abs/2510.19733v1", "date": "2025-10-22", "relevancy": 2.3598, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4751}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4751}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zhyper%3A%20Factorized%20Hypernetworks%20for%20Conditioned%20LLM%20Fine-Tuning&body=Title%3A%20Zhyper%3A%20Factorized%20Hypernetworks%20for%20Conditioned%20LLM%20Fine-Tuning%0AAuthor%3A%20M.%20H.%20I.%20Abdalla%20and%20Zhipin%20Wang%20and%20Christian%20Frey%20and%20Steffen%20Eger%20and%20Josif%20Grabocka%0AAbstract%3A%20%20%20Large%20Language%20Model%20%28LLM%29%20conditioning%20refers%20to%20instructing%20an%20LLM%20to%0Agenerate%20content%20in%20accordance%20with%20the%20norms%20and%20values%20of%20a%20specific%20culture%2C%0Abeliefs%20of%20a%20particular%20political%20orientation%2C%20or%20any%20desired%20text-specified%0Asemantic%20conditioning.%20Unfortunately%2C%20prompt%20engineering%20does%20not%20ensure%20that%0ALLMs%20behave%20in%20accordance%20with%20a%20desired%20conditioning%20due%20to%20the%20inductive%20bias%0Aof%20the%20pre-training%20and%20alignment%20datasets.%20Prior%20works%20have%20focused%20on%0Afine-tuning%20LLMs%20by%20directly%20conditioning%20the%20LoRA%20weights%3B%20however%2C%20such%0Amethods%20introduce%20a%20large%20number%20of%20parameters.%20As%20a%20remedy%2C%20we%20propose%20Zhyper%2C%0Aa%20parameter-efficient%20factorized%20hypernetwork%20framework%20that%20generates%0Acontext-aware%20LoRA%20adapters%20from%20textual%20descriptions.%20Experiments%20on%20multiple%0Abenchmarks%20show%20that%20Zhyper%20achieves%20competitive%20performance%20with%20up%20to%2026x%0Afewer%20parameters%20than%20the%20state-of-the-art%20baselines.%20Furthermore%2C%20we%20extend%0AZhyper%20to%20cultural%20alignment%2C%20demonstrating%20improved%20generalization%20to%0Aout-of-domain%20settings%20and%20a%20better%20capturing%20of%20fine-grained%20contextual%0Avalues.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19733v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZhyper%253A%2520Factorized%2520Hypernetworks%2520for%2520Conditioned%2520LLM%2520Fine-Tuning%26entry.906535625%3DM.%2520H.%2520I.%2520Abdalla%2520and%2520Zhipin%2520Wang%2520and%2520Christian%2520Frey%2520and%2520Steffen%2520Eger%2520and%2520Josif%2520Grabocka%26entry.1292438233%3D%2520%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520conditioning%2520refers%2520to%2520instructing%2520an%2520LLM%2520to%250Agenerate%2520content%2520in%2520accordance%2520with%2520the%2520norms%2520and%2520values%2520of%2520a%2520specific%2520culture%252C%250Abeliefs%2520of%2520a%2520particular%2520political%2520orientation%252C%2520or%2520any%2520desired%2520text-specified%250Asemantic%2520conditioning.%2520Unfortunately%252C%2520prompt%2520engineering%2520does%2520not%2520ensure%2520that%250ALLMs%2520behave%2520in%2520accordance%2520with%2520a%2520desired%2520conditioning%2520due%2520to%2520the%2520inductive%2520bias%250Aof%2520the%2520pre-training%2520and%2520alignment%2520datasets.%2520Prior%2520works%2520have%2520focused%2520on%250Afine-tuning%2520LLMs%2520by%2520directly%2520conditioning%2520the%2520LoRA%2520weights%253B%2520however%252C%2520such%250Amethods%2520introduce%2520a%2520large%2520number%2520of%2520parameters.%2520As%2520a%2520remedy%252C%2520we%2520propose%2520Zhyper%252C%250Aa%2520parameter-efficient%2520factorized%2520hypernetwork%2520framework%2520that%2520generates%250Acontext-aware%2520LoRA%2520adapters%2520from%2520textual%2520descriptions.%2520Experiments%2520on%2520multiple%250Abenchmarks%2520show%2520that%2520Zhyper%2520achieves%2520competitive%2520performance%2520with%2520up%2520to%252026x%250Afewer%2520parameters%2520than%2520the%2520state-of-the-art%2520baselines.%2520Furthermore%252C%2520we%2520extend%250AZhyper%2520to%2520cultural%2520alignment%252C%2520demonstrating%2520improved%2520generalization%2520to%250Aout-of-domain%2520settings%2520and%2520a%2520better%2520capturing%2520of%2520fine-grained%2520contextual%250Avalues.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19733v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zhyper%3A%20Factorized%20Hypernetworks%20for%20Conditioned%20LLM%20Fine-Tuning&entry.906535625=M.%20H.%20I.%20Abdalla%20and%20Zhipin%20Wang%20and%20Christian%20Frey%20and%20Steffen%20Eger%20and%20Josif%20Grabocka&entry.1292438233=%20%20Large%20Language%20Model%20%28LLM%29%20conditioning%20refers%20to%20instructing%20an%20LLM%20to%0Agenerate%20content%20in%20accordance%20with%20the%20norms%20and%20values%20of%20a%20specific%20culture%2C%0Abeliefs%20of%20a%20particular%20political%20orientation%2C%20or%20any%20desired%20text-specified%0Asemantic%20conditioning.%20Unfortunately%2C%20prompt%20engineering%20does%20not%20ensure%20that%0ALLMs%20behave%20in%20accordance%20with%20a%20desired%20conditioning%20due%20to%20the%20inductive%20bias%0Aof%20the%20pre-training%20and%20alignment%20datasets.%20Prior%20works%20have%20focused%20on%0Afine-tuning%20LLMs%20by%20directly%20conditioning%20the%20LoRA%20weights%3B%20however%2C%20such%0Amethods%20introduce%20a%20large%20number%20of%20parameters.%20As%20a%20remedy%2C%20we%20propose%20Zhyper%2C%0Aa%20parameter-efficient%20factorized%20hypernetwork%20framework%20that%20generates%0Acontext-aware%20LoRA%20adapters%20from%20textual%20descriptions.%20Experiments%20on%20multiple%0Abenchmarks%20show%20that%20Zhyper%20achieves%20competitive%20performance%20with%20up%20to%2026x%0Afewer%20parameters%20than%20the%20state-of-the-art%20baselines.%20Furthermore%2C%20we%20extend%0AZhyper%20to%20cultural%20alignment%2C%20demonstrating%20improved%20generalization%20to%0Aout-of-domain%20settings%20and%20a%20better%20capturing%20of%20fine-grained%20contextual%0Avalues.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19733v1&entry.124074799=Read"},
{"title": "Latent Space Factorization in LoRA", "author": "Shashi Kumar and Yacouba Kaloga and John Mitros and Petr Motlicek and Ina Kodrasi", "abstract": "  Low-rank adaptation (LoRA) is a widely used method for parameter-efficient\nfinetuning. However, existing LoRA variants lack mechanisms to explicitly\ndisambiguate task-relevant information within the learned low-rank subspace,\npotentially limiting downstream performance. We propose Factorized Variational\nAutoencoder LoRA (FVAE-LoRA), which leverages a VAE to learn two distinct\nlatent spaces. Our novel Evidence Lower Bound formulation explicitly promotes\nfactorization between the latent spaces, dedicating one latent space to\ntask-salient features and the other to residual information. Extensive\nexperiments on text, audio, and image tasks demonstrate that FVAE-LoRA\nconsistently outperforms standard LoRA. Moreover, spurious correlation\nevaluations confirm that FVAE-LoRA better isolates task-relevant signals,\nleading to improved robustness under distribution shifts. Our code is publicly\navailable at: https://github.com/idiap/FVAE-LoRA\n", "link": "http://arxiv.org/abs/2510.19640v1", "date": "2025-10-22", "relevancy": 2.3506, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4733}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4733}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4638}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latent%20Space%20Factorization%20in%20LoRA&body=Title%3A%20Latent%20Space%20Factorization%20in%20LoRA%0AAuthor%3A%20Shashi%20Kumar%20and%20Yacouba%20Kaloga%20and%20John%20Mitros%20and%20Petr%20Motlicek%20and%20Ina%20Kodrasi%0AAbstract%3A%20%20%20Low-rank%20adaptation%20%28LoRA%29%20is%20a%20widely%20used%20method%20for%20parameter-efficient%0Afinetuning.%20However%2C%20existing%20LoRA%20variants%20lack%20mechanisms%20to%20explicitly%0Adisambiguate%20task-relevant%20information%20within%20the%20learned%20low-rank%20subspace%2C%0Apotentially%20limiting%20downstream%20performance.%20We%20propose%20Factorized%20Variational%0AAutoencoder%20LoRA%20%28FVAE-LoRA%29%2C%20which%20leverages%20a%20VAE%20to%20learn%20two%20distinct%0Alatent%20spaces.%20Our%20novel%20Evidence%20Lower%20Bound%20formulation%20explicitly%20promotes%0Afactorization%20between%20the%20latent%20spaces%2C%20dedicating%20one%20latent%20space%20to%0Atask-salient%20features%20and%20the%20other%20to%20residual%20information.%20Extensive%0Aexperiments%20on%20text%2C%20audio%2C%20and%20image%20tasks%20demonstrate%20that%20FVAE-LoRA%0Aconsistently%20outperforms%20standard%20LoRA.%20Moreover%2C%20spurious%20correlation%0Aevaluations%20confirm%20that%20FVAE-LoRA%20better%20isolates%20task-relevant%20signals%2C%0Aleading%20to%20improved%20robustness%20under%20distribution%20shifts.%20Our%20code%20is%20publicly%0Aavailable%20at%3A%20https%3A//github.com/idiap/FVAE-LoRA%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19640v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatent%2520Space%2520Factorization%2520in%2520LoRA%26entry.906535625%3DShashi%2520Kumar%2520and%2520Yacouba%2520Kaloga%2520and%2520John%2520Mitros%2520and%2520Petr%2520Motlicek%2520and%2520Ina%2520Kodrasi%26entry.1292438233%3D%2520%2520Low-rank%2520adaptation%2520%2528LoRA%2529%2520is%2520a%2520widely%2520used%2520method%2520for%2520parameter-efficient%250Afinetuning.%2520However%252C%2520existing%2520LoRA%2520variants%2520lack%2520mechanisms%2520to%2520explicitly%250Adisambiguate%2520task-relevant%2520information%2520within%2520the%2520learned%2520low-rank%2520subspace%252C%250Apotentially%2520limiting%2520downstream%2520performance.%2520We%2520propose%2520Factorized%2520Variational%250AAutoencoder%2520LoRA%2520%2528FVAE-LoRA%2529%252C%2520which%2520leverages%2520a%2520VAE%2520to%2520learn%2520two%2520distinct%250Alatent%2520spaces.%2520Our%2520novel%2520Evidence%2520Lower%2520Bound%2520formulation%2520explicitly%2520promotes%250Afactorization%2520between%2520the%2520latent%2520spaces%252C%2520dedicating%2520one%2520latent%2520space%2520to%250Atask-salient%2520features%2520and%2520the%2520other%2520to%2520residual%2520information.%2520Extensive%250Aexperiments%2520on%2520text%252C%2520audio%252C%2520and%2520image%2520tasks%2520demonstrate%2520that%2520FVAE-LoRA%250Aconsistently%2520outperforms%2520standard%2520LoRA.%2520Moreover%252C%2520spurious%2520correlation%250Aevaluations%2520confirm%2520that%2520FVAE-LoRA%2520better%2520isolates%2520task-relevant%2520signals%252C%250Aleading%2520to%2520improved%2520robustness%2520under%2520distribution%2520shifts.%2520Our%2520code%2520is%2520publicly%250Aavailable%2520at%253A%2520https%253A//github.com/idiap/FVAE-LoRA%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19640v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent%20Space%20Factorization%20in%20LoRA&entry.906535625=Shashi%20Kumar%20and%20Yacouba%20Kaloga%20and%20John%20Mitros%20and%20Petr%20Motlicek%20and%20Ina%20Kodrasi&entry.1292438233=%20%20Low-rank%20adaptation%20%28LoRA%29%20is%20a%20widely%20used%20method%20for%20parameter-efficient%0Afinetuning.%20However%2C%20existing%20LoRA%20variants%20lack%20mechanisms%20to%20explicitly%0Adisambiguate%20task-relevant%20information%20within%20the%20learned%20low-rank%20subspace%2C%0Apotentially%20limiting%20downstream%20performance.%20We%20propose%20Factorized%20Variational%0AAutoencoder%20LoRA%20%28FVAE-LoRA%29%2C%20which%20leverages%20a%20VAE%20to%20learn%20two%20distinct%0Alatent%20spaces.%20Our%20novel%20Evidence%20Lower%20Bound%20formulation%20explicitly%20promotes%0Afactorization%20between%20the%20latent%20spaces%2C%20dedicating%20one%20latent%20space%20to%0Atask-salient%20features%20and%20the%20other%20to%20residual%20information.%20Extensive%0Aexperiments%20on%20text%2C%20audio%2C%20and%20image%20tasks%20demonstrate%20that%20FVAE-LoRA%0Aconsistently%20outperforms%20standard%20LoRA.%20Moreover%2C%20spurious%20correlation%0Aevaluations%20confirm%20that%20FVAE-LoRA%20better%20isolates%20task-relevant%20signals%2C%0Aleading%20to%20improved%20robustness%20under%20distribution%20shifts.%20Our%20code%20is%20publicly%0Aavailable%20at%3A%20https%3A//github.com/idiap/FVAE-LoRA%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19640v1&entry.124074799=Read"},
{"title": "Serverless GPU Architecture for Enterprise HR Analytics: A\n  Production-Scale BDaaS Implementation", "author": "Guilin Zhang and Wulan Guo and Ziqi Tan and Srinivas Vippagunta and Suchitra Raman and Shreeshankar Chatterjee and Ju Lin and Shang Liu and Mary Schladenhauffen and Jeffrey Luo and Hailong Jiang", "abstract": "  Industrial and government organizations increasingly depend on data-driven\nanalytics for workforce, finance, and regulated decision processes, where\ntimeliness, cost efficiency, and compliance are critical. Distributed\nframeworks such as Spark and Flink remain effective for massive-scale batch or\nstreaming analytics but introduce coordination complexity and auditing\noverheads that misalign with moderate-scale, latency-sensitive inference.\nMeanwhile, cloud providers now offer serverless GPUs, and models such as TabNet\nenable interpretable tabular ML, motivating new deployment blueprints for\nregulated environments. In this paper, we present a production-oriented Big\nData as a Service (BDaaS) blueprint that integrates a single-node serverless\nGPU runtime with TabNet. The design leverages GPU acceleration for throughput,\nserverless elasticity for cost reduction, and feature-mask interpretability for\nIL4/FIPS compliance. We conduct benchmarks on the HR, Adult, and BLS datasets,\ncomparing our approach against Spark and CPU baselines. Our results show that\nGPU pipelines achieve up to 4.5x higher throughput, 98x lower latency, and 90%\nlower cost per 1K inferences compared to Spark baselines, while compliance\nmechanisms add only ~5.7 ms latency with p99 < 22 ms. Interpretability remains\nstable under peak load, ensuring reliable auditability. Taken together, these\nfindings provide a compliance-aware benchmark, a reproducible Helm-packaged\nblueprint, and a decision framework that demonstrate the practicality of\nsecure, interpretable, and cost-efficient serverless GPU analytics for\nregulated enterprise and government settings.\n", "link": "http://arxiv.org/abs/2510.19689v1", "date": "2025-10-22", "relevancy": 2.3466, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.477}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.467}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Serverless%20GPU%20Architecture%20for%20Enterprise%20HR%20Analytics%3A%20A%0A%20%20Production-Scale%20BDaaS%20Implementation&body=Title%3A%20Serverless%20GPU%20Architecture%20for%20Enterprise%20HR%20Analytics%3A%20A%0A%20%20Production-Scale%20BDaaS%20Implementation%0AAuthor%3A%20Guilin%20Zhang%20and%20Wulan%20Guo%20and%20Ziqi%20Tan%20and%20Srinivas%20Vippagunta%20and%20Suchitra%20Raman%20and%20Shreeshankar%20Chatterjee%20and%20Ju%20Lin%20and%20Shang%20Liu%20and%20Mary%20Schladenhauffen%20and%20Jeffrey%20Luo%20and%20Hailong%20Jiang%0AAbstract%3A%20%20%20Industrial%20and%20government%20organizations%20increasingly%20depend%20on%20data-driven%0Aanalytics%20for%20workforce%2C%20finance%2C%20and%20regulated%20decision%20processes%2C%20where%0Atimeliness%2C%20cost%20efficiency%2C%20and%20compliance%20are%20critical.%20Distributed%0Aframeworks%20such%20as%20Spark%20and%20Flink%20remain%20effective%20for%20massive-scale%20batch%20or%0Astreaming%20analytics%20but%20introduce%20coordination%20complexity%20and%20auditing%0Aoverheads%20that%20misalign%20with%20moderate-scale%2C%20latency-sensitive%20inference.%0AMeanwhile%2C%20cloud%20providers%20now%20offer%20serverless%20GPUs%2C%20and%20models%20such%20as%20TabNet%0Aenable%20interpretable%20tabular%20ML%2C%20motivating%20new%20deployment%20blueprints%20for%0Aregulated%20environments.%20In%20this%20paper%2C%20we%20present%20a%20production-oriented%20Big%0AData%20as%20a%20Service%20%28BDaaS%29%20blueprint%20that%20integrates%20a%20single-node%20serverless%0AGPU%20runtime%20with%20TabNet.%20The%20design%20leverages%20GPU%20acceleration%20for%20throughput%2C%0Aserverless%20elasticity%20for%20cost%20reduction%2C%20and%20feature-mask%20interpretability%20for%0AIL4/FIPS%20compliance.%20We%20conduct%20benchmarks%20on%20the%20HR%2C%20Adult%2C%20and%20BLS%20datasets%2C%0Acomparing%20our%20approach%20against%20Spark%20and%20CPU%20baselines.%20Our%20results%20show%20that%0AGPU%20pipelines%20achieve%20up%20to%204.5x%20higher%20throughput%2C%2098x%20lower%20latency%2C%20and%2090%25%0Alower%20cost%20per%201K%20inferences%20compared%20to%20Spark%20baselines%2C%20while%20compliance%0Amechanisms%20add%20only%20~5.7%20ms%20latency%20with%20p99%20%3C%2022%20ms.%20Interpretability%20remains%0Astable%20under%20peak%20load%2C%20ensuring%20reliable%20auditability.%20Taken%20together%2C%20these%0Afindings%20provide%20a%20compliance-aware%20benchmark%2C%20a%20reproducible%20Helm-packaged%0Ablueprint%2C%20and%20a%20decision%20framework%20that%20demonstrate%20the%20practicality%20of%0Asecure%2C%20interpretable%2C%20and%20cost-efficient%20serverless%20GPU%20analytics%20for%0Aregulated%20enterprise%20and%20government%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19689v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DServerless%2520GPU%2520Architecture%2520for%2520Enterprise%2520HR%2520Analytics%253A%2520A%250A%2520%2520Production-Scale%2520BDaaS%2520Implementation%26entry.906535625%3DGuilin%2520Zhang%2520and%2520Wulan%2520Guo%2520and%2520Ziqi%2520Tan%2520and%2520Srinivas%2520Vippagunta%2520and%2520Suchitra%2520Raman%2520and%2520Shreeshankar%2520Chatterjee%2520and%2520Ju%2520Lin%2520and%2520Shang%2520Liu%2520and%2520Mary%2520Schladenhauffen%2520and%2520Jeffrey%2520Luo%2520and%2520Hailong%2520Jiang%26entry.1292438233%3D%2520%2520Industrial%2520and%2520government%2520organizations%2520increasingly%2520depend%2520on%2520data-driven%250Aanalytics%2520for%2520workforce%252C%2520finance%252C%2520and%2520regulated%2520decision%2520processes%252C%2520where%250Atimeliness%252C%2520cost%2520efficiency%252C%2520and%2520compliance%2520are%2520critical.%2520Distributed%250Aframeworks%2520such%2520as%2520Spark%2520and%2520Flink%2520remain%2520effective%2520for%2520massive-scale%2520batch%2520or%250Astreaming%2520analytics%2520but%2520introduce%2520coordination%2520complexity%2520and%2520auditing%250Aoverheads%2520that%2520misalign%2520with%2520moderate-scale%252C%2520latency-sensitive%2520inference.%250AMeanwhile%252C%2520cloud%2520providers%2520now%2520offer%2520serverless%2520GPUs%252C%2520and%2520models%2520such%2520as%2520TabNet%250Aenable%2520interpretable%2520tabular%2520ML%252C%2520motivating%2520new%2520deployment%2520blueprints%2520for%250Aregulated%2520environments.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520production-oriented%2520Big%250AData%2520as%2520a%2520Service%2520%2528BDaaS%2529%2520blueprint%2520that%2520integrates%2520a%2520single-node%2520serverless%250AGPU%2520runtime%2520with%2520TabNet.%2520The%2520design%2520leverages%2520GPU%2520acceleration%2520for%2520throughput%252C%250Aserverless%2520elasticity%2520for%2520cost%2520reduction%252C%2520and%2520feature-mask%2520interpretability%2520for%250AIL4/FIPS%2520compliance.%2520We%2520conduct%2520benchmarks%2520on%2520the%2520HR%252C%2520Adult%252C%2520and%2520BLS%2520datasets%252C%250Acomparing%2520our%2520approach%2520against%2520Spark%2520and%2520CPU%2520baselines.%2520Our%2520results%2520show%2520that%250AGPU%2520pipelines%2520achieve%2520up%2520to%25204.5x%2520higher%2520throughput%252C%252098x%2520lower%2520latency%252C%2520and%252090%2525%250Alower%2520cost%2520per%25201K%2520inferences%2520compared%2520to%2520Spark%2520baselines%252C%2520while%2520compliance%250Amechanisms%2520add%2520only%2520~5.7%2520ms%2520latency%2520with%2520p99%2520%253C%252022%2520ms.%2520Interpretability%2520remains%250Astable%2520under%2520peak%2520load%252C%2520ensuring%2520reliable%2520auditability.%2520Taken%2520together%252C%2520these%250Afindings%2520provide%2520a%2520compliance-aware%2520benchmark%252C%2520a%2520reproducible%2520Helm-packaged%250Ablueprint%252C%2520and%2520a%2520decision%2520framework%2520that%2520demonstrate%2520the%2520practicality%2520of%250Asecure%252C%2520interpretable%252C%2520and%2520cost-efficient%2520serverless%2520GPU%2520analytics%2520for%250Aregulated%2520enterprise%2520and%2520government%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19689v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Serverless%20GPU%20Architecture%20for%20Enterprise%20HR%20Analytics%3A%20A%0A%20%20Production-Scale%20BDaaS%20Implementation&entry.906535625=Guilin%20Zhang%20and%20Wulan%20Guo%20and%20Ziqi%20Tan%20and%20Srinivas%20Vippagunta%20and%20Suchitra%20Raman%20and%20Shreeshankar%20Chatterjee%20and%20Ju%20Lin%20and%20Shang%20Liu%20and%20Mary%20Schladenhauffen%20and%20Jeffrey%20Luo%20and%20Hailong%20Jiang&entry.1292438233=%20%20Industrial%20and%20government%20organizations%20increasingly%20depend%20on%20data-driven%0Aanalytics%20for%20workforce%2C%20finance%2C%20and%20regulated%20decision%20processes%2C%20where%0Atimeliness%2C%20cost%20efficiency%2C%20and%20compliance%20are%20critical.%20Distributed%0Aframeworks%20such%20as%20Spark%20and%20Flink%20remain%20effective%20for%20massive-scale%20batch%20or%0Astreaming%20analytics%20but%20introduce%20coordination%20complexity%20and%20auditing%0Aoverheads%20that%20misalign%20with%20moderate-scale%2C%20latency-sensitive%20inference.%0AMeanwhile%2C%20cloud%20providers%20now%20offer%20serverless%20GPUs%2C%20and%20models%20such%20as%20TabNet%0Aenable%20interpretable%20tabular%20ML%2C%20motivating%20new%20deployment%20blueprints%20for%0Aregulated%20environments.%20In%20this%20paper%2C%20we%20present%20a%20production-oriented%20Big%0AData%20as%20a%20Service%20%28BDaaS%29%20blueprint%20that%20integrates%20a%20single-node%20serverless%0AGPU%20runtime%20with%20TabNet.%20The%20design%20leverages%20GPU%20acceleration%20for%20throughput%2C%0Aserverless%20elasticity%20for%20cost%20reduction%2C%20and%20feature-mask%20interpretability%20for%0AIL4/FIPS%20compliance.%20We%20conduct%20benchmarks%20on%20the%20HR%2C%20Adult%2C%20and%20BLS%20datasets%2C%0Acomparing%20our%20approach%20against%20Spark%20and%20CPU%20baselines.%20Our%20results%20show%20that%0AGPU%20pipelines%20achieve%20up%20to%204.5x%20higher%20throughput%2C%2098x%20lower%20latency%2C%20and%2090%25%0Alower%20cost%20per%201K%20inferences%20compared%20to%20Spark%20baselines%2C%20while%20compliance%0Amechanisms%20add%20only%20~5.7%20ms%20latency%20with%20p99%20%3C%2022%20ms.%20Interpretability%20remains%0Astable%20under%20peak%20load%2C%20ensuring%20reliable%20auditability.%20Taken%20together%2C%20these%0Afindings%20provide%20a%20compliance-aware%20benchmark%2C%20a%20reproducible%20Helm-packaged%0Ablueprint%2C%20and%20a%20decision%20framework%20that%20demonstrate%20the%20practicality%20of%0Asecure%2C%20interpretable%2C%20and%20cost-efficient%20serverless%20GPU%20analytics%20for%0Aregulated%20enterprise%20and%20government%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19689v1&entry.124074799=Read"},
{"title": "Class-Aware Prototype Learning with Negative Contrast for Test-Time\n  Adaptation of Vision-Language Models", "author": "Xiaozhen Qiao and Jingkai Zhao and Yuqiu Jiang and Xianda Guo and Zhe Sun and Hongyuan Zhang and Xuelong Li", "abstract": "  Vision-Language Models (VLMs) demonstrate impressive zero-shot generalization\nthrough large-scale image-text pretraining, yet their performance can drop once\nthe deployment distribution diverges from the training distribution. To address\nthis, Test-Time Adaptation (TTA) methods update models using unlabeled target\ndata. However, existing approaches often ignore two key challenges: prototype\ndegradation in long-tailed distributions and confusion between semantically\nsimilar classes. To tackle these issues, we propose \\textbf{C}lass-Aware\n\\textbf{P}rototype \\textbf{L}earning with \\textbf{N}egative\n\\textbf{C}ontrast(\\textbf{CPL-NC}), a lightweight TTA framework designed\nspecifically for VLMs to enhance generalization under distribution shifts.\nCPL-NC introduces a \\textit{Class-Aware Prototype Cache} Module that\ndynamically adjusts per-class capacity based on test-time frequency and\nactivation history, with a rejuvenation mechanism for inactive classes to\nretain rare-category knowledge. Additionally, a \\textit{Negative Contrastive\nLearning} Mechanism identifies and constrains hard visual-textual negatives to\nimprove class separability. The framework employs asymmetric optimization,\nrefining only textual prototypes while anchoring on stable visual features.\nExperiments on 15 benchmarks show that CPL-NC consistently outperforms prior\nTTA methods across both ResNet-50 and ViT-B/16 backbones.\n", "link": "http://arxiv.org/abs/2510.19802v1", "date": "2025-10-22", "relevancy": 2.3201, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6112}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5677}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Class-Aware%20Prototype%20Learning%20with%20Negative%20Contrast%20for%20Test-Time%0A%20%20Adaptation%20of%20Vision-Language%20Models&body=Title%3A%20Class-Aware%20Prototype%20Learning%20with%20Negative%20Contrast%20for%20Test-Time%0A%20%20Adaptation%20of%20Vision-Language%20Models%0AAuthor%3A%20Xiaozhen%20Qiao%20and%20Jingkai%20Zhao%20and%20Yuqiu%20Jiang%20and%20Xianda%20Guo%20and%20Zhe%20Sun%20and%20Hongyuan%20Zhang%20and%20Xuelong%20Li%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20demonstrate%20impressive%20zero-shot%20generalization%0Athrough%20large-scale%20image-text%20pretraining%2C%20yet%20their%20performance%20can%20drop%20once%0Athe%20deployment%20distribution%20diverges%20from%20the%20training%20distribution.%20To%20address%0Athis%2C%20Test-Time%20Adaptation%20%28TTA%29%20methods%20update%20models%20using%20unlabeled%20target%0Adata.%20However%2C%20existing%20approaches%20often%20ignore%20two%20key%20challenges%3A%20prototype%0Adegradation%20in%20long-tailed%20distributions%20and%20confusion%20between%20semantically%0Asimilar%20classes.%20To%20tackle%20these%20issues%2C%20we%20propose%20%5Ctextbf%7BC%7Dlass-Aware%0A%5Ctextbf%7BP%7Drototype%20%5Ctextbf%7BL%7Dearning%20with%20%5Ctextbf%7BN%7Degative%0A%5Ctextbf%7BC%7Dontrast%28%5Ctextbf%7BCPL-NC%7D%29%2C%20a%20lightweight%20TTA%20framework%20designed%0Aspecifically%20for%20VLMs%20to%20enhance%20generalization%20under%20distribution%20shifts.%0ACPL-NC%20introduces%20a%20%5Ctextit%7BClass-Aware%20Prototype%20Cache%7D%20Module%20that%0Adynamically%20adjusts%20per-class%20capacity%20based%20on%20test-time%20frequency%20and%0Aactivation%20history%2C%20with%20a%20rejuvenation%20mechanism%20for%20inactive%20classes%20to%0Aretain%20rare-category%20knowledge.%20Additionally%2C%20a%20%5Ctextit%7BNegative%20Contrastive%0ALearning%7D%20Mechanism%20identifies%20and%20constrains%20hard%20visual-textual%20negatives%20to%0Aimprove%20class%20separability.%20The%20framework%20employs%20asymmetric%20optimization%2C%0Arefining%20only%20textual%20prototypes%20while%20anchoring%20on%20stable%20visual%20features.%0AExperiments%20on%2015%20benchmarks%20show%20that%20CPL-NC%20consistently%20outperforms%20prior%0ATTA%20methods%20across%20both%20ResNet-50%20and%20ViT-B/16%20backbones.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19802v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClass-Aware%2520Prototype%2520Learning%2520with%2520Negative%2520Contrast%2520for%2520Test-Time%250A%2520%2520Adaptation%2520of%2520Vision-Language%2520Models%26entry.906535625%3DXiaozhen%2520Qiao%2520and%2520Jingkai%2520Zhao%2520and%2520Yuqiu%2520Jiang%2520and%2520Xianda%2520Guo%2520and%2520Zhe%2520Sun%2520and%2520Hongyuan%2520Zhang%2520and%2520Xuelong%2520Li%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520demonstrate%2520impressive%2520zero-shot%2520generalization%250Athrough%2520large-scale%2520image-text%2520pretraining%252C%2520yet%2520their%2520performance%2520can%2520drop%2520once%250Athe%2520deployment%2520distribution%2520diverges%2520from%2520the%2520training%2520distribution.%2520To%2520address%250Athis%252C%2520Test-Time%2520Adaptation%2520%2528TTA%2529%2520methods%2520update%2520models%2520using%2520unlabeled%2520target%250Adata.%2520However%252C%2520existing%2520approaches%2520often%2520ignore%2520two%2520key%2520challenges%253A%2520prototype%250Adegradation%2520in%2520long-tailed%2520distributions%2520and%2520confusion%2520between%2520semantically%250Asimilar%2520classes.%2520To%2520tackle%2520these%2520issues%252C%2520we%2520propose%2520%255Ctextbf%257BC%257Dlass-Aware%250A%255Ctextbf%257BP%257Drototype%2520%255Ctextbf%257BL%257Dearning%2520with%2520%255Ctextbf%257BN%257Degative%250A%255Ctextbf%257BC%257Dontrast%2528%255Ctextbf%257BCPL-NC%257D%2529%252C%2520a%2520lightweight%2520TTA%2520framework%2520designed%250Aspecifically%2520for%2520VLMs%2520to%2520enhance%2520generalization%2520under%2520distribution%2520shifts.%250ACPL-NC%2520introduces%2520a%2520%255Ctextit%257BClass-Aware%2520Prototype%2520Cache%257D%2520Module%2520that%250Adynamically%2520adjusts%2520per-class%2520capacity%2520based%2520on%2520test-time%2520frequency%2520and%250Aactivation%2520history%252C%2520with%2520a%2520rejuvenation%2520mechanism%2520for%2520inactive%2520classes%2520to%250Aretain%2520rare-category%2520knowledge.%2520Additionally%252C%2520a%2520%255Ctextit%257BNegative%2520Contrastive%250ALearning%257D%2520Mechanism%2520identifies%2520and%2520constrains%2520hard%2520visual-textual%2520negatives%2520to%250Aimprove%2520class%2520separability.%2520The%2520framework%2520employs%2520asymmetric%2520optimization%252C%250Arefining%2520only%2520textual%2520prototypes%2520while%2520anchoring%2520on%2520stable%2520visual%2520features.%250AExperiments%2520on%252015%2520benchmarks%2520show%2520that%2520CPL-NC%2520consistently%2520outperforms%2520prior%250ATTA%2520methods%2520across%2520both%2520ResNet-50%2520and%2520ViT-B/16%2520backbones.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19802v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Class-Aware%20Prototype%20Learning%20with%20Negative%20Contrast%20for%20Test-Time%0A%20%20Adaptation%20of%20Vision-Language%20Models&entry.906535625=Xiaozhen%20Qiao%20and%20Jingkai%20Zhao%20and%20Yuqiu%20Jiang%20and%20Xianda%20Guo%20and%20Zhe%20Sun%20and%20Hongyuan%20Zhang%20and%20Xuelong%20Li&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20demonstrate%20impressive%20zero-shot%20generalization%0Athrough%20large-scale%20image-text%20pretraining%2C%20yet%20their%20performance%20can%20drop%20once%0Athe%20deployment%20distribution%20diverges%20from%20the%20training%20distribution.%20To%20address%0Athis%2C%20Test-Time%20Adaptation%20%28TTA%29%20methods%20update%20models%20using%20unlabeled%20target%0Adata.%20However%2C%20existing%20approaches%20often%20ignore%20two%20key%20challenges%3A%20prototype%0Adegradation%20in%20long-tailed%20distributions%20and%20confusion%20between%20semantically%0Asimilar%20classes.%20To%20tackle%20these%20issues%2C%20we%20propose%20%5Ctextbf%7BC%7Dlass-Aware%0A%5Ctextbf%7BP%7Drototype%20%5Ctextbf%7BL%7Dearning%20with%20%5Ctextbf%7BN%7Degative%0A%5Ctextbf%7BC%7Dontrast%28%5Ctextbf%7BCPL-NC%7D%29%2C%20a%20lightweight%20TTA%20framework%20designed%0Aspecifically%20for%20VLMs%20to%20enhance%20generalization%20under%20distribution%20shifts.%0ACPL-NC%20introduces%20a%20%5Ctextit%7BClass-Aware%20Prototype%20Cache%7D%20Module%20that%0Adynamically%20adjusts%20per-class%20capacity%20based%20on%20test-time%20frequency%20and%0Aactivation%20history%2C%20with%20a%20rejuvenation%20mechanism%20for%20inactive%20classes%20to%0Aretain%20rare-category%20knowledge.%20Additionally%2C%20a%20%5Ctextit%7BNegative%20Contrastive%0ALearning%7D%20Mechanism%20identifies%20and%20constrains%20hard%20visual-textual%20negatives%20to%0Aimprove%20class%20separability.%20The%20framework%20employs%20asymmetric%20optimization%2C%0Arefining%20only%20textual%20prototypes%20while%20anchoring%20on%20stable%20visual%20features.%0AExperiments%20on%2015%20benchmarks%20show%20that%20CPL-NC%20consistently%20outperforms%20prior%0ATTA%20methods%20across%20both%20ResNet-50%20and%20ViT-B/16%20backbones.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19802v1&entry.124074799=Read"},
{"title": "LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision\n  Language Navigation in Continuous Environments", "author": "Hongyu Ding and Ziming Xu and Yudong Fang and You Wu and Zixuan Chen and Jieqi Shi and Jing Huo and Yifan Zhang and Yang Gao", "abstract": "  Zero-shot Vision-and-Language Navigation in Continuous Environments (VLN-CE)\nrequires an agent to navigate unseen environments based on natural language\ninstructions without any prior training. Current methods face a critical\ntrade-off: either rely on environment-specific waypoint predictors that limit\nscene generalization, or underutilize the reasoning capabilities of large\nmodels during navigation. We introduce LaViRA, a simple yet effective zero-shot\nframework that addresses this dilemma by decomposing action into a\ncoarse-to-fine hierarchy: Language Action for high-level planning, Vision\nAction for perceptual grounding, and Robot Action for robust navigation. This\nmodular decomposition allows us to leverage the distinct strengths of different\nscales of Multimodal Large Language Models (MLLMs) at each stage, creating a\nsystem that is powerful in its reasoning, grounding and practical control.\nLaViRA significantly outperforms existing state-of-the-art methods on the\nVLN-CE benchmark, demonstrating superior generalization capabilities in unseen\nenvironments, while maintaining transparency and efficiency for real-world\ndeployment.\n", "link": "http://arxiv.org/abs/2510.19655v1", "date": "2025-10-22", "relevancy": 2.2933, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.576}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.576}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LaViRA%3A%20Language-Vision-Robot%20Actions%20Translation%20for%20Zero-Shot%20Vision%0A%20%20Language%20Navigation%20in%20Continuous%20Environments&body=Title%3A%20LaViRA%3A%20Language-Vision-Robot%20Actions%20Translation%20for%20Zero-Shot%20Vision%0A%20%20Language%20Navigation%20in%20Continuous%20Environments%0AAuthor%3A%20Hongyu%20Ding%20and%20Ziming%20Xu%20and%20Yudong%20Fang%20and%20You%20Wu%20and%20Zixuan%20Chen%20and%20Jieqi%20Shi%20and%20Jing%20Huo%20and%20Yifan%20Zhang%20and%20Yang%20Gao%0AAbstract%3A%20%20%20Zero-shot%20Vision-and-Language%20Navigation%20in%20Continuous%20Environments%20%28VLN-CE%29%0Arequires%20an%20agent%20to%20navigate%20unseen%20environments%20based%20on%20natural%20language%0Ainstructions%20without%20any%20prior%20training.%20Current%20methods%20face%20a%20critical%0Atrade-off%3A%20either%20rely%20on%20environment-specific%20waypoint%20predictors%20that%20limit%0Ascene%20generalization%2C%20or%20underutilize%20the%20reasoning%20capabilities%20of%20large%0Amodels%20during%20navigation.%20We%20introduce%20LaViRA%2C%20a%20simple%20yet%20effective%20zero-shot%0Aframework%20that%20addresses%20this%20dilemma%20by%20decomposing%20action%20into%20a%0Acoarse-to-fine%20hierarchy%3A%20Language%20Action%20for%20high-level%20planning%2C%20Vision%0AAction%20for%20perceptual%20grounding%2C%20and%20Robot%20Action%20for%20robust%20navigation.%20This%0Amodular%20decomposition%20allows%20us%20to%20leverage%20the%20distinct%20strengths%20of%20different%0Ascales%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20at%20each%20stage%2C%20creating%20a%0Asystem%20that%20is%20powerful%20in%20its%20reasoning%2C%20grounding%20and%20practical%20control.%0ALaViRA%20significantly%20outperforms%20existing%20state-of-the-art%20methods%20on%20the%0AVLN-CE%20benchmark%2C%20demonstrating%20superior%20generalization%20capabilities%20in%20unseen%0Aenvironments%2C%20while%20maintaining%20transparency%20and%20efficiency%20for%20real-world%0Adeployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19655v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLaViRA%253A%2520Language-Vision-Robot%2520Actions%2520Translation%2520for%2520Zero-Shot%2520Vision%250A%2520%2520Language%2520Navigation%2520in%2520Continuous%2520Environments%26entry.906535625%3DHongyu%2520Ding%2520and%2520Ziming%2520Xu%2520and%2520Yudong%2520Fang%2520and%2520You%2520Wu%2520and%2520Zixuan%2520Chen%2520and%2520Jieqi%2520Shi%2520and%2520Jing%2520Huo%2520and%2520Yifan%2520Zhang%2520and%2520Yang%2520Gao%26entry.1292438233%3D%2520%2520Zero-shot%2520Vision-and-Language%2520Navigation%2520in%2520Continuous%2520Environments%2520%2528VLN-CE%2529%250Arequires%2520an%2520agent%2520to%2520navigate%2520unseen%2520environments%2520based%2520on%2520natural%2520language%250Ainstructions%2520without%2520any%2520prior%2520training.%2520Current%2520methods%2520face%2520a%2520critical%250Atrade-off%253A%2520either%2520rely%2520on%2520environment-specific%2520waypoint%2520predictors%2520that%2520limit%250Ascene%2520generalization%252C%2520or%2520underutilize%2520the%2520reasoning%2520capabilities%2520of%2520large%250Amodels%2520during%2520navigation.%2520We%2520introduce%2520LaViRA%252C%2520a%2520simple%2520yet%2520effective%2520zero-shot%250Aframework%2520that%2520addresses%2520this%2520dilemma%2520by%2520decomposing%2520action%2520into%2520a%250Acoarse-to-fine%2520hierarchy%253A%2520Language%2520Action%2520for%2520high-level%2520planning%252C%2520Vision%250AAction%2520for%2520perceptual%2520grounding%252C%2520and%2520Robot%2520Action%2520for%2520robust%2520navigation.%2520This%250Amodular%2520decomposition%2520allows%2520us%2520to%2520leverage%2520the%2520distinct%2520strengths%2520of%2520different%250Ascales%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520at%2520each%2520stage%252C%2520creating%2520a%250Asystem%2520that%2520is%2520powerful%2520in%2520its%2520reasoning%252C%2520grounding%2520and%2520practical%2520control.%250ALaViRA%2520significantly%2520outperforms%2520existing%2520state-of-the-art%2520methods%2520on%2520the%250AVLN-CE%2520benchmark%252C%2520demonstrating%2520superior%2520generalization%2520capabilities%2520in%2520unseen%250Aenvironments%252C%2520while%2520maintaining%2520transparency%2520and%2520efficiency%2520for%2520real-world%250Adeployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19655v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LaViRA%3A%20Language-Vision-Robot%20Actions%20Translation%20for%20Zero-Shot%20Vision%0A%20%20Language%20Navigation%20in%20Continuous%20Environments&entry.906535625=Hongyu%20Ding%20and%20Ziming%20Xu%20and%20Yudong%20Fang%20and%20You%20Wu%20and%20Zixuan%20Chen%20and%20Jieqi%20Shi%20and%20Jing%20Huo%20and%20Yifan%20Zhang%20and%20Yang%20Gao&entry.1292438233=%20%20Zero-shot%20Vision-and-Language%20Navigation%20in%20Continuous%20Environments%20%28VLN-CE%29%0Arequires%20an%20agent%20to%20navigate%20unseen%20environments%20based%20on%20natural%20language%0Ainstructions%20without%20any%20prior%20training.%20Current%20methods%20face%20a%20critical%0Atrade-off%3A%20either%20rely%20on%20environment-specific%20waypoint%20predictors%20that%20limit%0Ascene%20generalization%2C%20or%20underutilize%20the%20reasoning%20capabilities%20of%20large%0Amodels%20during%20navigation.%20We%20introduce%20LaViRA%2C%20a%20simple%20yet%20effective%20zero-shot%0Aframework%20that%20addresses%20this%20dilemma%20by%20decomposing%20action%20into%20a%0Acoarse-to-fine%20hierarchy%3A%20Language%20Action%20for%20high-level%20planning%2C%20Vision%0AAction%20for%20perceptual%20grounding%2C%20and%20Robot%20Action%20for%20robust%20navigation.%20This%0Amodular%20decomposition%20allows%20us%20to%20leverage%20the%20distinct%20strengths%20of%20different%0Ascales%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20at%20each%20stage%2C%20creating%20a%0Asystem%20that%20is%20powerful%20in%20its%20reasoning%2C%20grounding%20and%20practical%20control.%0ALaViRA%20significantly%20outperforms%20existing%20state-of-the-art%20methods%20on%20the%0AVLN-CE%20benchmark%2C%20demonstrating%20superior%20generalization%20capabilities%20in%20unseen%0Aenvironments%2C%20while%20maintaining%20transparency%20and%20efficiency%20for%20real-world%0Adeployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19655v1&entry.124074799=Read"},
{"title": "On Controlled Change: Generative AI's Impact on Professional Authority\n  in Journalism", "author": "Tom\u00e1s Dodds and Wang Ngai Yeung and Claudia Mellado and Mathias-Felipe de Lima-Santos", "abstract": "  Using (generative) artificial intelligence tools and systems in journalism is\nexpected to increase journalists' production rates, transform newsrooms'\neconomic models, and further personalize the audience's news consumption\npractices. Since its release in 2022, OpenAI's ChatGPT and other large language\nmodels have raised the alarms inside news organizations, not only for bringing\nnew challenges to news reporting and fact-checking but also for what these\ntechnologies would mean for journalists' professional authority in journalism.\nThis paper examines how journalists in Dutch media manage the integration of AI\ntechnologies into their daily routines. Drawing from 13 interviews with\neditors, journalists, and innovation managers in different news outlets and\nmedia companies, we propose the concept of controlled change. as a heuristic to\nexplain how journalists are proactively setting guidelines, experimenting with\nAI tools, and identifying their limitations and capabilities. Using\nprofessional authority as a theoretical framework, we argue that journalists\nanticipate and integrate AI technologies in a supervised manner and identify\nthree primary mechanisms through which journalists manage this integration: (1)\ndeveloping adaptive guidelines that align AI use with ethical codes, (2)\nexperimenting with AI technologies to determine their necessity and fit, and\n(3) critically assessing the capabilities and limitations of AI systems.\n", "link": "http://arxiv.org/abs/2510.19792v1", "date": "2025-10-22", "relevancy": 2.2784, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4694}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.456}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Controlled%20Change%3A%20Generative%20AI%27s%20Impact%20on%20Professional%20Authority%0A%20%20in%20Journalism&body=Title%3A%20On%20Controlled%20Change%3A%20Generative%20AI%27s%20Impact%20on%20Professional%20Authority%0A%20%20in%20Journalism%0AAuthor%3A%20Tom%C3%A1s%20Dodds%20and%20Wang%20Ngai%20Yeung%20and%20Claudia%20Mellado%20and%20Mathias-Felipe%20de%20Lima-Santos%0AAbstract%3A%20%20%20Using%20%28generative%29%20artificial%20intelligence%20tools%20and%20systems%20in%20journalism%20is%0Aexpected%20to%20increase%20journalists%27%20production%20rates%2C%20transform%20newsrooms%27%0Aeconomic%20models%2C%20and%20further%20personalize%20the%20audience%27s%20news%20consumption%0Apractices.%20Since%20its%20release%20in%202022%2C%20OpenAI%27s%20ChatGPT%20and%20other%20large%20language%0Amodels%20have%20raised%20the%20alarms%20inside%20news%20organizations%2C%20not%20only%20for%20bringing%0Anew%20challenges%20to%20news%20reporting%20and%20fact-checking%20but%20also%20for%20what%20these%0Atechnologies%20would%20mean%20for%20journalists%27%20professional%20authority%20in%20journalism.%0AThis%20paper%20examines%20how%20journalists%20in%20Dutch%20media%20manage%20the%20integration%20of%20AI%0Atechnologies%20into%20their%20daily%20routines.%20Drawing%20from%2013%20interviews%20with%0Aeditors%2C%20journalists%2C%20and%20innovation%20managers%20in%20different%20news%20outlets%20and%0Amedia%20companies%2C%20we%20propose%20the%20concept%20of%20controlled%20change.%20as%20a%20heuristic%20to%0Aexplain%20how%20journalists%20are%20proactively%20setting%20guidelines%2C%20experimenting%20with%0AAI%20tools%2C%20and%20identifying%20their%20limitations%20and%20capabilities.%20Using%0Aprofessional%20authority%20as%20a%20theoretical%20framework%2C%20we%20argue%20that%20journalists%0Aanticipate%20and%20integrate%20AI%20technologies%20in%20a%20supervised%20manner%20and%20identify%0Athree%20primary%20mechanisms%20through%20which%20journalists%20manage%20this%20integration%3A%20%281%29%0Adeveloping%20adaptive%20guidelines%20that%20align%20AI%20use%20with%20ethical%20codes%2C%20%282%29%0Aexperimenting%20with%20AI%20technologies%20to%20determine%20their%20necessity%20and%20fit%2C%20and%0A%283%29%20critically%20assessing%20the%20capabilities%20and%20limitations%20of%20AI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19792v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Controlled%2520Change%253A%2520Generative%2520AI%2527s%2520Impact%2520on%2520Professional%2520Authority%250A%2520%2520in%2520Journalism%26entry.906535625%3DTom%25C3%25A1s%2520Dodds%2520and%2520Wang%2520Ngai%2520Yeung%2520and%2520Claudia%2520Mellado%2520and%2520Mathias-Felipe%2520de%2520Lima-Santos%26entry.1292438233%3D%2520%2520Using%2520%2528generative%2529%2520artificial%2520intelligence%2520tools%2520and%2520systems%2520in%2520journalism%2520is%250Aexpected%2520to%2520increase%2520journalists%2527%2520production%2520rates%252C%2520transform%2520newsrooms%2527%250Aeconomic%2520models%252C%2520and%2520further%2520personalize%2520the%2520audience%2527s%2520news%2520consumption%250Apractices.%2520Since%2520its%2520release%2520in%25202022%252C%2520OpenAI%2527s%2520ChatGPT%2520and%2520other%2520large%2520language%250Amodels%2520have%2520raised%2520the%2520alarms%2520inside%2520news%2520organizations%252C%2520not%2520only%2520for%2520bringing%250Anew%2520challenges%2520to%2520news%2520reporting%2520and%2520fact-checking%2520but%2520also%2520for%2520what%2520these%250Atechnologies%2520would%2520mean%2520for%2520journalists%2527%2520professional%2520authority%2520in%2520journalism.%250AThis%2520paper%2520examines%2520how%2520journalists%2520in%2520Dutch%2520media%2520manage%2520the%2520integration%2520of%2520AI%250Atechnologies%2520into%2520their%2520daily%2520routines.%2520Drawing%2520from%252013%2520interviews%2520with%250Aeditors%252C%2520journalists%252C%2520and%2520innovation%2520managers%2520in%2520different%2520news%2520outlets%2520and%250Amedia%2520companies%252C%2520we%2520propose%2520the%2520concept%2520of%2520controlled%2520change.%2520as%2520a%2520heuristic%2520to%250Aexplain%2520how%2520journalists%2520are%2520proactively%2520setting%2520guidelines%252C%2520experimenting%2520with%250AAI%2520tools%252C%2520and%2520identifying%2520their%2520limitations%2520and%2520capabilities.%2520Using%250Aprofessional%2520authority%2520as%2520a%2520theoretical%2520framework%252C%2520we%2520argue%2520that%2520journalists%250Aanticipate%2520and%2520integrate%2520AI%2520technologies%2520in%2520a%2520supervised%2520manner%2520and%2520identify%250Athree%2520primary%2520mechanisms%2520through%2520which%2520journalists%2520manage%2520this%2520integration%253A%2520%25281%2529%250Adeveloping%2520adaptive%2520guidelines%2520that%2520align%2520AI%2520use%2520with%2520ethical%2520codes%252C%2520%25282%2529%250Aexperimenting%2520with%2520AI%2520technologies%2520to%2520determine%2520their%2520necessity%2520and%2520fit%252C%2520and%250A%25283%2529%2520critically%2520assessing%2520the%2520capabilities%2520and%2520limitations%2520of%2520AI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19792v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Controlled%20Change%3A%20Generative%20AI%27s%20Impact%20on%20Professional%20Authority%0A%20%20in%20Journalism&entry.906535625=Tom%C3%A1s%20Dodds%20and%20Wang%20Ngai%20Yeung%20and%20Claudia%20Mellado%20and%20Mathias-Felipe%20de%20Lima-Santos&entry.1292438233=%20%20Using%20%28generative%29%20artificial%20intelligence%20tools%20and%20systems%20in%20journalism%20is%0Aexpected%20to%20increase%20journalists%27%20production%20rates%2C%20transform%20newsrooms%27%0Aeconomic%20models%2C%20and%20further%20personalize%20the%20audience%27s%20news%20consumption%0Apractices.%20Since%20its%20release%20in%202022%2C%20OpenAI%27s%20ChatGPT%20and%20other%20large%20language%0Amodels%20have%20raised%20the%20alarms%20inside%20news%20organizations%2C%20not%20only%20for%20bringing%0Anew%20challenges%20to%20news%20reporting%20and%20fact-checking%20but%20also%20for%20what%20these%0Atechnologies%20would%20mean%20for%20journalists%27%20professional%20authority%20in%20journalism.%0AThis%20paper%20examines%20how%20journalists%20in%20Dutch%20media%20manage%20the%20integration%20of%20AI%0Atechnologies%20into%20their%20daily%20routines.%20Drawing%20from%2013%20interviews%20with%0Aeditors%2C%20journalists%2C%20and%20innovation%20managers%20in%20different%20news%20outlets%20and%0Amedia%20companies%2C%20we%20propose%20the%20concept%20of%20controlled%20change.%20as%20a%20heuristic%20to%0Aexplain%20how%20journalists%20are%20proactively%20setting%20guidelines%2C%20experimenting%20with%0AAI%20tools%2C%20and%20identifying%20their%20limitations%20and%20capabilities.%20Using%0Aprofessional%20authority%20as%20a%20theoretical%20framework%2C%20we%20argue%20that%20journalists%0Aanticipate%20and%20integrate%20AI%20technologies%20in%20a%20supervised%20manner%20and%20identify%0Athree%20primary%20mechanisms%20through%20which%20journalists%20manage%20this%20integration%3A%20%281%29%0Adeveloping%20adaptive%20guidelines%20that%20align%20AI%20use%20with%20ethical%20codes%2C%20%282%29%0Aexperimenting%20with%20AI%20technologies%20to%20determine%20their%20necessity%20and%20fit%2C%20and%0A%283%29%20critically%20assessing%20the%20capabilities%20and%20limitations%20of%20AI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19792v1&entry.124074799=Read"},
{"title": "Study of Training Dynamics for Memory-Constrained Fine-Tuning", "author": "A\u00ebl Qu\u00e9lennec and Nour Hezbri and Pavlo Mozharovskyi and Van-Tam Nguyen and Enzo Tartaglione", "abstract": "  Memory-efficient training of deep neural networks has become increasingly\nimportant as models grow larger while deployment environments impose strict\nresource constraints. We propose TraDy, a novel transfer learning scheme\nleveraging two key insights: layer importance for updates is\narchitecture-dependent and determinable a priori, while dynamic stochastic\nchannel selection provides superior gradient approximation compared to static\napproaches. We introduce a dynamic channel selection approach that\nstochastically resamples channels between epochs within preselected layers.\nExtensive experiments demonstrate TraDy achieves state-of-the-art performance\nacross various downstream tasks and architectures while maintaining strict\nmemory constraints, achieving up to 99% activation sparsity, 95% weight\nderivative sparsity, and 97% reduction in FLOPs for weight derivative\ncomputation.\n", "link": "http://arxiv.org/abs/2510.19675v1", "date": "2025-10-22", "relevancy": 2.2747, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5767}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5744}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5341}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Study%20of%20Training%20Dynamics%20for%20Memory-Constrained%20Fine-Tuning&body=Title%3A%20Study%20of%20Training%20Dynamics%20for%20Memory-Constrained%20Fine-Tuning%0AAuthor%3A%20A%C3%ABl%20Qu%C3%A9lennec%20and%20Nour%20Hezbri%20and%20Pavlo%20Mozharovskyi%20and%20Van-Tam%20Nguyen%20and%20Enzo%20Tartaglione%0AAbstract%3A%20%20%20Memory-efficient%20training%20of%20deep%20neural%20networks%20has%20become%20increasingly%0Aimportant%20as%20models%20grow%20larger%20while%20deployment%20environments%20impose%20strict%0Aresource%20constraints.%20We%20propose%20TraDy%2C%20a%20novel%20transfer%20learning%20scheme%0Aleveraging%20two%20key%20insights%3A%20layer%20importance%20for%20updates%20is%0Aarchitecture-dependent%20and%20determinable%20a%20priori%2C%20while%20dynamic%20stochastic%0Achannel%20selection%20provides%20superior%20gradient%20approximation%20compared%20to%20static%0Aapproaches.%20We%20introduce%20a%20dynamic%20channel%20selection%20approach%20that%0Astochastically%20resamples%20channels%20between%20epochs%20within%20preselected%20layers.%0AExtensive%20experiments%20demonstrate%20TraDy%20achieves%20state-of-the-art%20performance%0Aacross%20various%20downstream%20tasks%20and%20architectures%20while%20maintaining%20strict%0Amemory%20constraints%2C%20achieving%20up%20to%2099%25%20activation%20sparsity%2C%2095%25%20weight%0Aderivative%20sparsity%2C%20and%2097%25%20reduction%20in%20FLOPs%20for%20weight%20derivative%0Acomputation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19675v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStudy%2520of%2520Training%2520Dynamics%2520for%2520Memory-Constrained%2520Fine-Tuning%26entry.906535625%3DA%25C3%25ABl%2520Qu%25C3%25A9lennec%2520and%2520Nour%2520Hezbri%2520and%2520Pavlo%2520Mozharovskyi%2520and%2520Van-Tam%2520Nguyen%2520and%2520Enzo%2520Tartaglione%26entry.1292438233%3D%2520%2520Memory-efficient%2520training%2520of%2520deep%2520neural%2520networks%2520has%2520become%2520increasingly%250Aimportant%2520as%2520models%2520grow%2520larger%2520while%2520deployment%2520environments%2520impose%2520strict%250Aresource%2520constraints.%2520We%2520propose%2520TraDy%252C%2520a%2520novel%2520transfer%2520learning%2520scheme%250Aleveraging%2520two%2520key%2520insights%253A%2520layer%2520importance%2520for%2520updates%2520is%250Aarchitecture-dependent%2520and%2520determinable%2520a%2520priori%252C%2520while%2520dynamic%2520stochastic%250Achannel%2520selection%2520provides%2520superior%2520gradient%2520approximation%2520compared%2520to%2520static%250Aapproaches.%2520We%2520introduce%2520a%2520dynamic%2520channel%2520selection%2520approach%2520that%250Astochastically%2520resamples%2520channels%2520between%2520epochs%2520within%2520preselected%2520layers.%250AExtensive%2520experiments%2520demonstrate%2520TraDy%2520achieves%2520state-of-the-art%2520performance%250Aacross%2520various%2520downstream%2520tasks%2520and%2520architectures%2520while%2520maintaining%2520strict%250Amemory%2520constraints%252C%2520achieving%2520up%2520to%252099%2525%2520activation%2520sparsity%252C%252095%2525%2520weight%250Aderivative%2520sparsity%252C%2520and%252097%2525%2520reduction%2520in%2520FLOPs%2520for%2520weight%2520derivative%250Acomputation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19675v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Study%20of%20Training%20Dynamics%20for%20Memory-Constrained%20Fine-Tuning&entry.906535625=A%C3%ABl%20Qu%C3%A9lennec%20and%20Nour%20Hezbri%20and%20Pavlo%20Mozharovskyi%20and%20Van-Tam%20Nguyen%20and%20Enzo%20Tartaglione&entry.1292438233=%20%20Memory-efficient%20training%20of%20deep%20neural%20networks%20has%20become%20increasingly%0Aimportant%20as%20models%20grow%20larger%20while%20deployment%20environments%20impose%20strict%0Aresource%20constraints.%20We%20propose%20TraDy%2C%20a%20novel%20transfer%20learning%20scheme%0Aleveraging%20two%20key%20insights%3A%20layer%20importance%20for%20updates%20is%0Aarchitecture-dependent%20and%20determinable%20a%20priori%2C%20while%20dynamic%20stochastic%0Achannel%20selection%20provides%20superior%20gradient%20approximation%20compared%20to%20static%0Aapproaches.%20We%20introduce%20a%20dynamic%20channel%20selection%20approach%20that%0Astochastically%20resamples%20channels%20between%20epochs%20within%20preselected%20layers.%0AExtensive%20experiments%20demonstrate%20TraDy%20achieves%20state-of-the-art%20performance%0Aacross%20various%20downstream%20tasks%20and%20architectures%20while%20maintaining%20strict%0Amemory%20constraints%2C%20achieving%20up%20to%2099%25%20activation%20sparsity%2C%2095%25%20weight%0Aderivative%20sparsity%2C%20and%2097%25%20reduction%20in%20FLOPs%20for%20weight%20derivative%0Acomputation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19675v1&entry.124074799=Read"},
{"title": "What Foundation Models can Bring for Robot Learning in Manipulation : A\n  Survey", "author": "Dingzhe Li and Yixiang Jin and Yuhao Sun and Yong A and Hongze Yu and Jun Shi and Xiaoshuai Hao and Peng Hao and Huaping Liu and Xiang Li and Xinde Li and Fuchun Sun and Jianwei Zhang and Bin Fang", "abstract": "  The realization of universal robots is an ultimate goal of researchers.\nHowever, a key hurdle in achieving this goal lies in the robots' ability to\nmanipulate objects in their unstructured surrounding environments according to\ndifferent tasks. The learning-based approach is considered an effective way to\naddress generalization. The impressive performance of foundation models in the\nfields of computer vision and natural language suggests the potential of\nembedding foundation models into manipulation tasks as a viable path toward\nachieving general manipulation capability. However, we believe achieving\ngeneral manipulation capability requires an overarching framework akin to auto\ndriving. This framework should encompass multiple functional modules, with\ndifferent foundation models assuming distinct roles in facilitating general\nmanipulation capability. This survey focuses on the contributions of foundation\nmodels to robot learning for manipulation. We propose a comprehensive framework\nand detail how foundation models can address challenges in each module of the\nframework. What's more, we examine current approaches, outline challenges,\nsuggest future research directions, and identify potential risks associated\nwith integrating foundation models into this domain.\n", "link": "http://arxiv.org/abs/2404.18201v6", "date": "2025-10-22", "relevancy": 2.2534, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5651}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5651}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Foundation%20Models%20can%20Bring%20for%20Robot%20Learning%20in%20Manipulation%20%3A%20A%0A%20%20Survey&body=Title%3A%20What%20Foundation%20Models%20can%20Bring%20for%20Robot%20Learning%20in%20Manipulation%20%3A%20A%0A%20%20Survey%0AAuthor%3A%20Dingzhe%20Li%20and%20Yixiang%20Jin%20and%20Yuhao%20Sun%20and%20Yong%20A%20and%20Hongze%20Yu%20and%20Jun%20Shi%20and%20Xiaoshuai%20Hao%20and%20Peng%20Hao%20and%20Huaping%20Liu%20and%20Xiang%20Li%20and%20Xinde%20Li%20and%20Fuchun%20Sun%20and%20Jianwei%20Zhang%20and%20Bin%20Fang%0AAbstract%3A%20%20%20The%20realization%20of%20universal%20robots%20is%20an%20ultimate%20goal%20of%20researchers.%0AHowever%2C%20a%20key%20hurdle%20in%20achieving%20this%20goal%20lies%20in%20the%20robots%27%20ability%20to%0Amanipulate%20objects%20in%20their%20unstructured%20surrounding%20environments%20according%20to%0Adifferent%20tasks.%20The%20learning-based%20approach%20is%20considered%20an%20effective%20way%20to%0Aaddress%20generalization.%20The%20impressive%20performance%20of%20foundation%20models%20in%20the%0Afields%20of%20computer%20vision%20and%20natural%20language%20suggests%20the%20potential%20of%0Aembedding%20foundation%20models%20into%20manipulation%20tasks%20as%20a%20viable%20path%20toward%0Aachieving%20general%20manipulation%20capability.%20However%2C%20we%20believe%20achieving%0Ageneral%20manipulation%20capability%20requires%20an%20overarching%20framework%20akin%20to%20auto%0Adriving.%20This%20framework%20should%20encompass%20multiple%20functional%20modules%2C%20with%0Adifferent%20foundation%20models%20assuming%20distinct%20roles%20in%20facilitating%20general%0Amanipulation%20capability.%20This%20survey%20focuses%20on%20the%20contributions%20of%20foundation%0Amodels%20to%20robot%20learning%20for%20manipulation.%20We%20propose%20a%20comprehensive%20framework%0Aand%20detail%20how%20foundation%20models%20can%20address%20challenges%20in%20each%20module%20of%20the%0Aframework.%20What%27s%20more%2C%20we%20examine%20current%20approaches%2C%20outline%20challenges%2C%0Asuggest%20future%20research%20directions%2C%20and%20identify%20potential%20risks%20associated%0Awith%20integrating%20foundation%20models%20into%20this%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18201v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Foundation%2520Models%2520can%2520Bring%2520for%2520Robot%2520Learning%2520in%2520Manipulation%2520%253A%2520A%250A%2520%2520Survey%26entry.906535625%3DDingzhe%2520Li%2520and%2520Yixiang%2520Jin%2520and%2520Yuhao%2520Sun%2520and%2520Yong%2520A%2520and%2520Hongze%2520Yu%2520and%2520Jun%2520Shi%2520and%2520Xiaoshuai%2520Hao%2520and%2520Peng%2520Hao%2520and%2520Huaping%2520Liu%2520and%2520Xiang%2520Li%2520and%2520Xinde%2520Li%2520and%2520Fuchun%2520Sun%2520and%2520Jianwei%2520Zhang%2520and%2520Bin%2520Fang%26entry.1292438233%3D%2520%2520The%2520realization%2520of%2520universal%2520robots%2520is%2520an%2520ultimate%2520goal%2520of%2520researchers.%250AHowever%252C%2520a%2520key%2520hurdle%2520in%2520achieving%2520this%2520goal%2520lies%2520in%2520the%2520robots%2527%2520ability%2520to%250Amanipulate%2520objects%2520in%2520their%2520unstructured%2520surrounding%2520environments%2520according%2520to%250Adifferent%2520tasks.%2520The%2520learning-based%2520approach%2520is%2520considered%2520an%2520effective%2520way%2520to%250Aaddress%2520generalization.%2520The%2520impressive%2520performance%2520of%2520foundation%2520models%2520in%2520the%250Afields%2520of%2520computer%2520vision%2520and%2520natural%2520language%2520suggests%2520the%2520potential%2520of%250Aembedding%2520foundation%2520models%2520into%2520manipulation%2520tasks%2520as%2520a%2520viable%2520path%2520toward%250Aachieving%2520general%2520manipulation%2520capability.%2520However%252C%2520we%2520believe%2520achieving%250Ageneral%2520manipulation%2520capability%2520requires%2520an%2520overarching%2520framework%2520akin%2520to%2520auto%250Adriving.%2520This%2520framework%2520should%2520encompass%2520multiple%2520functional%2520modules%252C%2520with%250Adifferent%2520foundation%2520models%2520assuming%2520distinct%2520roles%2520in%2520facilitating%2520general%250Amanipulation%2520capability.%2520This%2520survey%2520focuses%2520on%2520the%2520contributions%2520of%2520foundation%250Amodels%2520to%2520robot%2520learning%2520for%2520manipulation.%2520We%2520propose%2520a%2520comprehensive%2520framework%250Aand%2520detail%2520how%2520foundation%2520models%2520can%2520address%2520challenges%2520in%2520each%2520module%2520of%2520the%250Aframework.%2520What%2527s%2520more%252C%2520we%2520examine%2520current%2520approaches%252C%2520outline%2520challenges%252C%250Asuggest%2520future%2520research%2520directions%252C%2520and%2520identify%2520potential%2520risks%2520associated%250Awith%2520integrating%2520foundation%2520models%2520into%2520this%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.18201v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Foundation%20Models%20can%20Bring%20for%20Robot%20Learning%20in%20Manipulation%20%3A%20A%0A%20%20Survey&entry.906535625=Dingzhe%20Li%20and%20Yixiang%20Jin%20and%20Yuhao%20Sun%20and%20Yong%20A%20and%20Hongze%20Yu%20and%20Jun%20Shi%20and%20Xiaoshuai%20Hao%20and%20Peng%20Hao%20and%20Huaping%20Liu%20and%20Xiang%20Li%20and%20Xinde%20Li%20and%20Fuchun%20Sun%20and%20Jianwei%20Zhang%20and%20Bin%20Fang&entry.1292438233=%20%20The%20realization%20of%20universal%20robots%20is%20an%20ultimate%20goal%20of%20researchers.%0AHowever%2C%20a%20key%20hurdle%20in%20achieving%20this%20goal%20lies%20in%20the%20robots%27%20ability%20to%0Amanipulate%20objects%20in%20their%20unstructured%20surrounding%20environments%20according%20to%0Adifferent%20tasks.%20The%20learning-based%20approach%20is%20considered%20an%20effective%20way%20to%0Aaddress%20generalization.%20The%20impressive%20performance%20of%20foundation%20models%20in%20the%0Afields%20of%20computer%20vision%20and%20natural%20language%20suggests%20the%20potential%20of%0Aembedding%20foundation%20models%20into%20manipulation%20tasks%20as%20a%20viable%20path%20toward%0Aachieving%20general%20manipulation%20capability.%20However%2C%20we%20believe%20achieving%0Ageneral%20manipulation%20capability%20requires%20an%20overarching%20framework%20akin%20to%20auto%0Adriving.%20This%20framework%20should%20encompass%20multiple%20functional%20modules%2C%20with%0Adifferent%20foundation%20models%20assuming%20distinct%20roles%20in%20facilitating%20general%0Amanipulation%20capability.%20This%20survey%20focuses%20on%20the%20contributions%20of%20foundation%0Amodels%20to%20robot%20learning%20for%20manipulation.%20We%20propose%20a%20comprehensive%20framework%0Aand%20detail%20how%20foundation%20models%20can%20address%20challenges%20in%20each%20module%20of%20the%0Aframework.%20What%27s%20more%2C%20we%20examine%20current%20approaches%2C%20outline%20challenges%2C%0Asuggest%20future%20research%20directions%2C%20and%20identify%20potential%20risks%20associated%0Awith%20integrating%20foundation%20models%20into%20this%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18201v6&entry.124074799=Read"},
{"title": "QoQ-Med: Building Multimodal Clinical Foundation Models with\n  Domain-Aware GRPO Training", "author": "Wei Dai and Peilin Chen and Chanakya Ekbote and Paul Pu Liang", "abstract": "  Clinical decision-making routinely demands reasoning over heterogeneous data,\nyet existing multimodal language models (MLLMs) remain largely vision-centric\nand fail to generalize across clinical specialties. To bridge this gap, we\nintroduce QoQ-Med-7B/32B, the first open generalist clinical foundation model\nthat jointly reasons across medical images, time-series signals, and text\nreports. QoQ-Med is trained with Domain-aware Relative Policy Optimization\n(DRPO), a novel reinforcement-learning objective that hierarchically scales\nnormalized rewards according to domain rarity and modality difficulty,\nmitigating performance imbalance caused by skewed clinical data distributions.\nTrained on 2.61 million instruction tuning pairs spanning 9 clinical domains,\nwe show that DRPO training boosts diagnostic performance by 43% in macro-F1 on\naverage across all visual domains as compared to other critic-free training\nmethods like GRPO. Furthermore, with QoQ-Med trained on intensive segmentation\ndata, it is able to highlight salient regions related to the diagnosis, with an\nIoU 10x higher than open models while reaching the performance of OpenAI\no4-mini. To foster reproducibility and downstream research, we release (i) the\nfull model weights, (ii) the modular training pipeline, and (iii) all\nintermediate reasoning traces at https://github.com/DDVD233/QoQ_Med.\n", "link": "http://arxiv.org/abs/2506.00711v2", "date": "2025-10-22", "relevancy": 2.2384, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5623}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5623}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QoQ-Med%3A%20Building%20Multimodal%20Clinical%20Foundation%20Models%20with%0A%20%20Domain-Aware%20GRPO%20Training&body=Title%3A%20QoQ-Med%3A%20Building%20Multimodal%20Clinical%20Foundation%20Models%20with%0A%20%20Domain-Aware%20GRPO%20Training%0AAuthor%3A%20Wei%20Dai%20and%20Peilin%20Chen%20and%20Chanakya%20Ekbote%20and%20Paul%20Pu%20Liang%0AAbstract%3A%20%20%20Clinical%20decision-making%20routinely%20demands%20reasoning%20over%20heterogeneous%20data%2C%0Ayet%20existing%20multimodal%20language%20models%20%28MLLMs%29%20remain%20largely%20vision-centric%0Aand%20fail%20to%20generalize%20across%20clinical%20specialties.%20To%20bridge%20this%20gap%2C%20we%0Aintroduce%20QoQ-Med-7B/32B%2C%20the%20first%20open%20generalist%20clinical%20foundation%20model%0Athat%20jointly%20reasons%20across%20medical%20images%2C%20time-series%20signals%2C%20and%20text%0Areports.%20QoQ-Med%20is%20trained%20with%20Domain-aware%20Relative%20Policy%20Optimization%0A%28DRPO%29%2C%20a%20novel%20reinforcement-learning%20objective%20that%20hierarchically%20scales%0Anormalized%20rewards%20according%20to%20domain%20rarity%20and%20modality%20difficulty%2C%0Amitigating%20performance%20imbalance%20caused%20by%20skewed%20clinical%20data%20distributions.%0ATrained%20on%202.61%20million%20instruction%20tuning%20pairs%20spanning%209%20clinical%20domains%2C%0Awe%20show%20that%20DRPO%20training%20boosts%20diagnostic%20performance%20by%2043%25%20in%20macro-F1%20on%0Aaverage%20across%20all%20visual%20domains%20as%20compared%20to%20other%20critic-free%20training%0Amethods%20like%20GRPO.%20Furthermore%2C%20with%20QoQ-Med%20trained%20on%20intensive%20segmentation%0Adata%2C%20it%20is%20able%20to%20highlight%20salient%20regions%20related%20to%20the%20diagnosis%2C%20with%20an%0AIoU%2010x%20higher%20than%20open%20models%20while%20reaching%20the%20performance%20of%20OpenAI%0Ao4-mini.%20To%20foster%20reproducibility%20and%20downstream%20research%2C%20we%20release%20%28i%29%20the%0Afull%20model%20weights%2C%20%28ii%29%20the%20modular%20training%20pipeline%2C%20and%20%28iii%29%20all%0Aintermediate%20reasoning%20traces%20at%20https%3A//github.com/DDVD233/QoQ_Med.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.00711v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQoQ-Med%253A%2520Building%2520Multimodal%2520Clinical%2520Foundation%2520Models%2520with%250A%2520%2520Domain-Aware%2520GRPO%2520Training%26entry.906535625%3DWei%2520Dai%2520and%2520Peilin%2520Chen%2520and%2520Chanakya%2520Ekbote%2520and%2520Paul%2520Pu%2520Liang%26entry.1292438233%3D%2520%2520Clinical%2520decision-making%2520routinely%2520demands%2520reasoning%2520over%2520heterogeneous%2520data%252C%250Ayet%2520existing%2520multimodal%2520language%2520models%2520%2528MLLMs%2529%2520remain%2520largely%2520vision-centric%250Aand%2520fail%2520to%2520generalize%2520across%2520clinical%2520specialties.%2520To%2520bridge%2520this%2520gap%252C%2520we%250Aintroduce%2520QoQ-Med-7B/32B%252C%2520the%2520first%2520open%2520generalist%2520clinical%2520foundation%2520model%250Athat%2520jointly%2520reasons%2520across%2520medical%2520images%252C%2520time-series%2520signals%252C%2520and%2520text%250Areports.%2520QoQ-Med%2520is%2520trained%2520with%2520Domain-aware%2520Relative%2520Policy%2520Optimization%250A%2528DRPO%2529%252C%2520a%2520novel%2520reinforcement-learning%2520objective%2520that%2520hierarchically%2520scales%250Anormalized%2520rewards%2520according%2520to%2520domain%2520rarity%2520and%2520modality%2520difficulty%252C%250Amitigating%2520performance%2520imbalance%2520caused%2520by%2520skewed%2520clinical%2520data%2520distributions.%250ATrained%2520on%25202.61%2520million%2520instruction%2520tuning%2520pairs%2520spanning%25209%2520clinical%2520domains%252C%250Awe%2520show%2520that%2520DRPO%2520training%2520boosts%2520diagnostic%2520performance%2520by%252043%2525%2520in%2520macro-F1%2520on%250Aaverage%2520across%2520all%2520visual%2520domains%2520as%2520compared%2520to%2520other%2520critic-free%2520training%250Amethods%2520like%2520GRPO.%2520Furthermore%252C%2520with%2520QoQ-Med%2520trained%2520on%2520intensive%2520segmentation%250Adata%252C%2520it%2520is%2520able%2520to%2520highlight%2520salient%2520regions%2520related%2520to%2520the%2520diagnosis%252C%2520with%2520an%250AIoU%252010x%2520higher%2520than%2520open%2520models%2520while%2520reaching%2520the%2520performance%2520of%2520OpenAI%250Ao4-mini.%2520To%2520foster%2520reproducibility%2520and%2520downstream%2520research%252C%2520we%2520release%2520%2528i%2529%2520the%250Afull%2520model%2520weights%252C%2520%2528ii%2529%2520the%2520modular%2520training%2520pipeline%252C%2520and%2520%2528iii%2529%2520all%250Aintermediate%2520reasoning%2520traces%2520at%2520https%253A//github.com/DDVD233/QoQ_Med.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.00711v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QoQ-Med%3A%20Building%20Multimodal%20Clinical%20Foundation%20Models%20with%0A%20%20Domain-Aware%20GRPO%20Training&entry.906535625=Wei%20Dai%20and%20Peilin%20Chen%20and%20Chanakya%20Ekbote%20and%20Paul%20Pu%20Liang&entry.1292438233=%20%20Clinical%20decision-making%20routinely%20demands%20reasoning%20over%20heterogeneous%20data%2C%0Ayet%20existing%20multimodal%20language%20models%20%28MLLMs%29%20remain%20largely%20vision-centric%0Aand%20fail%20to%20generalize%20across%20clinical%20specialties.%20To%20bridge%20this%20gap%2C%20we%0Aintroduce%20QoQ-Med-7B/32B%2C%20the%20first%20open%20generalist%20clinical%20foundation%20model%0Athat%20jointly%20reasons%20across%20medical%20images%2C%20time-series%20signals%2C%20and%20text%0Areports.%20QoQ-Med%20is%20trained%20with%20Domain-aware%20Relative%20Policy%20Optimization%0A%28DRPO%29%2C%20a%20novel%20reinforcement-learning%20objective%20that%20hierarchically%20scales%0Anormalized%20rewards%20according%20to%20domain%20rarity%20and%20modality%20difficulty%2C%0Amitigating%20performance%20imbalance%20caused%20by%20skewed%20clinical%20data%20distributions.%0ATrained%20on%202.61%20million%20instruction%20tuning%20pairs%20spanning%209%20clinical%20domains%2C%0Awe%20show%20that%20DRPO%20training%20boosts%20diagnostic%20performance%20by%2043%25%20in%20macro-F1%20on%0Aaverage%20across%20all%20visual%20domains%20as%20compared%20to%20other%20critic-free%20training%0Amethods%20like%20GRPO.%20Furthermore%2C%20with%20QoQ-Med%20trained%20on%20intensive%20segmentation%0Adata%2C%20it%20is%20able%20to%20highlight%20salient%20regions%20related%20to%20the%20diagnosis%2C%20with%20an%0AIoU%2010x%20higher%20than%20open%20models%20while%20reaching%20the%20performance%20of%20OpenAI%0Ao4-mini.%20To%20foster%20reproducibility%20and%20downstream%20research%2C%20we%20release%20%28i%29%20the%0Afull%20model%20weights%2C%20%28ii%29%20the%20modular%20training%20pipeline%2C%20and%20%28iii%29%20all%0Aintermediate%20reasoning%20traces%20at%20https%3A//github.com/DDVD233/QoQ_Med.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.00711v2&entry.124074799=Read"},
{"title": "Test-time Prompt Intervention", "author": "Chenxu Yang and Qingyi Si and Mz Dai and Dingyu Yao and Mingyu Zheng and Minghui Chen and Zheng Lin and Weiping Wang", "abstract": "  Test-time compute has led to remarkable success in the large language model\n(LLM) community, particularly for complex tasks, where longer chains of thought\n(CoTs) are generated to enhance reasoning capabilities. However, growing\nevidence reveals that such reasoning models often produce CoTs plagued by\nexcessive redundancy, including unnecessary verification steps and repetitive\nreasoning shifts. The root cause lies in post-training of them that overly rely\non outcome reward paradigms, as the data of process reward paradigms, which\nregulate intermediate reasoning steps, is difficult to construct at scale. To\naddress this, we propose PI, a novel framework for Test-time Prompt\nIntervention. PI provides an interface to dynamically guide and regulate\nreasoning paths during inference through timely (When module) and proper (How\nmodule) interventions and post-intervention sampling (Which module). This\nallows human problem-solving expertise and cognitive science principles to be\nseamlessly integrated into LLMs' reasoning processes, enhancing controllability\nand interpretability. Extensive experiments across multiple models and datasets\ndemonstrate that PI significantly shortens CoTs while reducing hallucination,\nyielding more concise and reliable reasoning.\n", "link": "http://arxiv.org/abs/2508.02511v2", "date": "2025-10-22", "relevancy": 2.2284, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4461}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4461}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Test-time%20Prompt%20Intervention&body=Title%3A%20Test-time%20Prompt%20Intervention%0AAuthor%3A%20Chenxu%20Yang%20and%20Qingyi%20Si%20and%20Mz%20Dai%20and%20Dingyu%20Yao%20and%20Mingyu%20Zheng%20and%20Minghui%20Chen%20and%20Zheng%20Lin%20and%20Weiping%20Wang%0AAbstract%3A%20%20%20Test-time%20compute%20has%20led%20to%20remarkable%20success%20in%20the%20large%20language%20model%0A%28LLM%29%20community%2C%20particularly%20for%20complex%20tasks%2C%20where%20longer%20chains%20of%20thought%0A%28CoTs%29%20are%20generated%20to%20enhance%20reasoning%20capabilities.%20However%2C%20growing%0Aevidence%20reveals%20that%20such%20reasoning%20models%20often%20produce%20CoTs%20plagued%20by%0Aexcessive%20redundancy%2C%20including%20unnecessary%20verification%20steps%20and%20repetitive%0Areasoning%20shifts.%20The%20root%20cause%20lies%20in%20post-training%20of%20them%20that%20overly%20rely%0Aon%20outcome%20reward%20paradigms%2C%20as%20the%20data%20of%20process%20reward%20paradigms%2C%20which%0Aregulate%20intermediate%20reasoning%20steps%2C%20is%20difficult%20to%20construct%20at%20scale.%20To%0Aaddress%20this%2C%20we%20propose%20PI%2C%20a%20novel%20framework%20for%20Test-time%20Prompt%0AIntervention.%20PI%20provides%20an%20interface%20to%20dynamically%20guide%20and%20regulate%0Areasoning%20paths%20during%20inference%20through%20timely%20%28When%20module%29%20and%20proper%20%28How%0Amodule%29%20interventions%20and%20post-intervention%20sampling%20%28Which%20module%29.%20This%0Aallows%20human%20problem-solving%20expertise%20and%20cognitive%20science%20principles%20to%20be%0Aseamlessly%20integrated%20into%20LLMs%27%20reasoning%20processes%2C%20enhancing%20controllability%0Aand%20interpretability.%20Extensive%20experiments%20across%20multiple%20models%20and%20datasets%0Ademonstrate%20that%20PI%20significantly%20shortens%20CoTs%20while%20reducing%20hallucination%2C%0Ayielding%20more%20concise%20and%20reliable%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02511v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTest-time%2520Prompt%2520Intervention%26entry.906535625%3DChenxu%2520Yang%2520and%2520Qingyi%2520Si%2520and%2520Mz%2520Dai%2520and%2520Dingyu%2520Yao%2520and%2520Mingyu%2520Zheng%2520and%2520Minghui%2520Chen%2520and%2520Zheng%2520Lin%2520and%2520Weiping%2520Wang%26entry.1292438233%3D%2520%2520Test-time%2520compute%2520has%2520led%2520to%2520remarkable%2520success%2520in%2520the%2520large%2520language%2520model%250A%2528LLM%2529%2520community%252C%2520particularly%2520for%2520complex%2520tasks%252C%2520where%2520longer%2520chains%2520of%2520thought%250A%2528CoTs%2529%2520are%2520generated%2520to%2520enhance%2520reasoning%2520capabilities.%2520However%252C%2520growing%250Aevidence%2520reveals%2520that%2520such%2520reasoning%2520models%2520often%2520produce%2520CoTs%2520plagued%2520by%250Aexcessive%2520redundancy%252C%2520including%2520unnecessary%2520verification%2520steps%2520and%2520repetitive%250Areasoning%2520shifts.%2520The%2520root%2520cause%2520lies%2520in%2520post-training%2520of%2520them%2520that%2520overly%2520rely%250Aon%2520outcome%2520reward%2520paradigms%252C%2520as%2520the%2520data%2520of%2520process%2520reward%2520paradigms%252C%2520which%250Aregulate%2520intermediate%2520reasoning%2520steps%252C%2520is%2520difficult%2520to%2520construct%2520at%2520scale.%2520To%250Aaddress%2520this%252C%2520we%2520propose%2520PI%252C%2520a%2520novel%2520framework%2520for%2520Test-time%2520Prompt%250AIntervention.%2520PI%2520provides%2520an%2520interface%2520to%2520dynamically%2520guide%2520and%2520regulate%250Areasoning%2520paths%2520during%2520inference%2520through%2520timely%2520%2528When%2520module%2529%2520and%2520proper%2520%2528How%250Amodule%2529%2520interventions%2520and%2520post-intervention%2520sampling%2520%2528Which%2520module%2529.%2520This%250Aallows%2520human%2520problem-solving%2520expertise%2520and%2520cognitive%2520science%2520principles%2520to%2520be%250Aseamlessly%2520integrated%2520into%2520LLMs%2527%2520reasoning%2520processes%252C%2520enhancing%2520controllability%250Aand%2520interpretability.%2520Extensive%2520experiments%2520across%2520multiple%2520models%2520and%2520datasets%250Ademonstrate%2520that%2520PI%2520significantly%2520shortens%2520CoTs%2520while%2520reducing%2520hallucination%252C%250Ayielding%2520more%2520concise%2520and%2520reliable%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02511v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Test-time%20Prompt%20Intervention&entry.906535625=Chenxu%20Yang%20and%20Qingyi%20Si%20and%20Mz%20Dai%20and%20Dingyu%20Yao%20and%20Mingyu%20Zheng%20and%20Minghui%20Chen%20and%20Zheng%20Lin%20and%20Weiping%20Wang&entry.1292438233=%20%20Test-time%20compute%20has%20led%20to%20remarkable%20success%20in%20the%20large%20language%20model%0A%28LLM%29%20community%2C%20particularly%20for%20complex%20tasks%2C%20where%20longer%20chains%20of%20thought%0A%28CoTs%29%20are%20generated%20to%20enhance%20reasoning%20capabilities.%20However%2C%20growing%0Aevidence%20reveals%20that%20such%20reasoning%20models%20often%20produce%20CoTs%20plagued%20by%0Aexcessive%20redundancy%2C%20including%20unnecessary%20verification%20steps%20and%20repetitive%0Areasoning%20shifts.%20The%20root%20cause%20lies%20in%20post-training%20of%20them%20that%20overly%20rely%0Aon%20outcome%20reward%20paradigms%2C%20as%20the%20data%20of%20process%20reward%20paradigms%2C%20which%0Aregulate%20intermediate%20reasoning%20steps%2C%20is%20difficult%20to%20construct%20at%20scale.%20To%0Aaddress%20this%2C%20we%20propose%20PI%2C%20a%20novel%20framework%20for%20Test-time%20Prompt%0AIntervention.%20PI%20provides%20an%20interface%20to%20dynamically%20guide%20and%20regulate%0Areasoning%20paths%20during%20inference%20through%20timely%20%28When%20module%29%20and%20proper%20%28How%0Amodule%29%20interventions%20and%20post-intervention%20sampling%20%28Which%20module%29.%20This%0Aallows%20human%20problem-solving%20expertise%20and%20cognitive%20science%20principles%20to%20be%0Aseamlessly%20integrated%20into%20LLMs%27%20reasoning%20processes%2C%20enhancing%20controllability%0Aand%20interpretability.%20Extensive%20experiments%20across%20multiple%20models%20and%20datasets%0Ademonstrate%20that%20PI%20significantly%20shortens%20CoTs%20while%20reducing%20hallucination%2C%0Ayielding%20more%20concise%20and%20reliable%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02511v2&entry.124074799=Read"},
{"title": "Video-R1: Reinforcing Video Reasoning in MLLMs", "author": "Kaituo Feng and Kaixiong Gong and Bohao Li and Zonghao Guo and Yibing Wang and Tianshuo Peng and Junfei Wu and Xiaoying Zhang and Benyou Wang and Xiangyu Yue", "abstract": "  Inspired by DeepSeek-R1's success in eliciting reasoning abilities through\nrule-based reinforcement learning (RL), we introduce Video-R1 as the first\nattempt to systematically explore the R1 paradigm for incentivizing video\nreasoning within multimodal large language models (MLLMs). However, directly\napplying RL training with the GRPO algorithm to video reasoning presents two\nprimary challenges: (i) a lack of temporal modeling for video reasoning, and\n(ii) the scarcity of high-quality video-reasoning data. To address these\nissues, we first propose the T-GRPO algorithm, which encourages models to\nutilize temporal information in videos for reasoning. Additionally, instead of\nrelying solely on video data, we incorporate high-quality image-reasoning data\ninto the training process. We have constructed two datasets: Video-R1-CoT-165k\nfor SFT cold start and Video-R1-260k for RL training, both comprising image and\nvideo data. Experimental results demonstrate that Video-R1 achieves significant\nimprovements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as\nwell as on general video benchmarks including MVBench and TempCompass, etc.\nNotably, Video-R1-7B attains a 37.1% accuracy on video spatial reasoning\nbenchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All\ncode, models, and data are released in: https://github.com/tulerfeng/Video-R1.\n", "link": "http://arxiv.org/abs/2503.21776v4", "date": "2025-10-22", "relevancy": 2.2118, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5565}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5565}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5352}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video-R1%3A%20Reinforcing%20Video%20Reasoning%20in%20MLLMs&body=Title%3A%20Video-R1%3A%20Reinforcing%20Video%20Reasoning%20in%20MLLMs%0AAuthor%3A%20Kaituo%20Feng%20and%20Kaixiong%20Gong%20and%20Bohao%20Li%20and%20Zonghao%20Guo%20and%20Yibing%20Wang%20and%20Tianshuo%20Peng%20and%20Junfei%20Wu%20and%20Xiaoying%20Zhang%20and%20Benyou%20Wang%20and%20Xiangyu%20Yue%0AAbstract%3A%20%20%20Inspired%20by%20DeepSeek-R1%27s%20success%20in%20eliciting%20reasoning%20abilities%20through%0Arule-based%20reinforcement%20learning%20%28RL%29%2C%20we%20introduce%20Video-R1%20as%20the%20first%0Aattempt%20to%20systematically%20explore%20the%20R1%20paradigm%20for%20incentivizing%20video%0Areasoning%20within%20multimodal%20large%20language%20models%20%28MLLMs%29.%20However%2C%20directly%0Aapplying%20RL%20training%20with%20the%20GRPO%20algorithm%20to%20video%20reasoning%20presents%20two%0Aprimary%20challenges%3A%20%28i%29%20a%20lack%20of%20temporal%20modeling%20for%20video%20reasoning%2C%20and%0A%28ii%29%20the%20scarcity%20of%20high-quality%20video-reasoning%20data.%20To%20address%20these%0Aissues%2C%20we%20first%20propose%20the%20T-GRPO%20algorithm%2C%20which%20encourages%20models%20to%0Autilize%20temporal%20information%20in%20videos%20for%20reasoning.%20Additionally%2C%20instead%20of%0Arelying%20solely%20on%20video%20data%2C%20we%20incorporate%20high-quality%20image-reasoning%20data%0Ainto%20the%20training%20process.%20We%20have%20constructed%20two%20datasets%3A%20Video-R1-CoT-165k%0Afor%20SFT%20cold%20start%20and%20Video-R1-260k%20for%20RL%20training%2C%20both%20comprising%20image%20and%0Avideo%20data.%20Experimental%20results%20demonstrate%20that%20Video-R1%20achieves%20significant%0Aimprovements%20on%20video%20reasoning%20benchmarks%20such%20as%20VideoMMMU%20and%20VSI-Bench%2C%20as%0Awell%20as%20on%20general%20video%20benchmarks%20including%20MVBench%20and%20TempCompass%2C%20etc.%0ANotably%2C%20Video-R1-7B%20attains%20a%2037.1%25%20accuracy%20on%20video%20spatial%20reasoning%0Abenchmark%20VSI-bench%2C%20surpassing%20the%20commercial%20proprietary%20model%20GPT-4o.%20All%0Acode%2C%20models%2C%20and%20data%20are%20released%20in%3A%20https%3A//github.com/tulerfeng/Video-R1.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.21776v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo-R1%253A%2520Reinforcing%2520Video%2520Reasoning%2520in%2520MLLMs%26entry.906535625%3DKaituo%2520Feng%2520and%2520Kaixiong%2520Gong%2520and%2520Bohao%2520Li%2520and%2520Zonghao%2520Guo%2520and%2520Yibing%2520Wang%2520and%2520Tianshuo%2520Peng%2520and%2520Junfei%2520Wu%2520and%2520Xiaoying%2520Zhang%2520and%2520Benyou%2520Wang%2520and%2520Xiangyu%2520Yue%26entry.1292438233%3D%2520%2520Inspired%2520by%2520DeepSeek-R1%2527s%2520success%2520in%2520eliciting%2520reasoning%2520abilities%2520through%250Arule-based%2520reinforcement%2520learning%2520%2528RL%2529%252C%2520we%2520introduce%2520Video-R1%2520as%2520the%2520first%250Aattempt%2520to%2520systematically%2520explore%2520the%2520R1%2520paradigm%2520for%2520incentivizing%2520video%250Areasoning%2520within%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529.%2520However%252C%2520directly%250Aapplying%2520RL%2520training%2520with%2520the%2520GRPO%2520algorithm%2520to%2520video%2520reasoning%2520presents%2520two%250Aprimary%2520challenges%253A%2520%2528i%2529%2520a%2520lack%2520of%2520temporal%2520modeling%2520for%2520video%2520reasoning%252C%2520and%250A%2528ii%2529%2520the%2520scarcity%2520of%2520high-quality%2520video-reasoning%2520data.%2520To%2520address%2520these%250Aissues%252C%2520we%2520first%2520propose%2520the%2520T-GRPO%2520algorithm%252C%2520which%2520encourages%2520models%2520to%250Autilize%2520temporal%2520information%2520in%2520videos%2520for%2520reasoning.%2520Additionally%252C%2520instead%2520of%250Arelying%2520solely%2520on%2520video%2520data%252C%2520we%2520incorporate%2520high-quality%2520image-reasoning%2520data%250Ainto%2520the%2520training%2520process.%2520We%2520have%2520constructed%2520two%2520datasets%253A%2520Video-R1-CoT-165k%250Afor%2520SFT%2520cold%2520start%2520and%2520Video-R1-260k%2520for%2520RL%2520training%252C%2520both%2520comprising%2520image%2520and%250Avideo%2520data.%2520Experimental%2520results%2520demonstrate%2520that%2520Video-R1%2520achieves%2520significant%250Aimprovements%2520on%2520video%2520reasoning%2520benchmarks%2520such%2520as%2520VideoMMMU%2520and%2520VSI-Bench%252C%2520as%250Awell%2520as%2520on%2520general%2520video%2520benchmarks%2520including%2520MVBench%2520and%2520TempCompass%252C%2520etc.%250ANotably%252C%2520Video-R1-7B%2520attains%2520a%252037.1%2525%2520accuracy%2520on%2520video%2520spatial%2520reasoning%250Abenchmark%2520VSI-bench%252C%2520surpassing%2520the%2520commercial%2520proprietary%2520model%2520GPT-4o.%2520All%250Acode%252C%2520models%252C%2520and%2520data%2520are%2520released%2520in%253A%2520https%253A//github.com/tulerfeng/Video-R1.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.21776v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video-R1%3A%20Reinforcing%20Video%20Reasoning%20in%20MLLMs&entry.906535625=Kaituo%20Feng%20and%20Kaixiong%20Gong%20and%20Bohao%20Li%20and%20Zonghao%20Guo%20and%20Yibing%20Wang%20and%20Tianshuo%20Peng%20and%20Junfei%20Wu%20and%20Xiaoying%20Zhang%20and%20Benyou%20Wang%20and%20Xiangyu%20Yue&entry.1292438233=%20%20Inspired%20by%20DeepSeek-R1%27s%20success%20in%20eliciting%20reasoning%20abilities%20through%0Arule-based%20reinforcement%20learning%20%28RL%29%2C%20we%20introduce%20Video-R1%20as%20the%20first%0Aattempt%20to%20systematically%20explore%20the%20R1%20paradigm%20for%20incentivizing%20video%0Areasoning%20within%20multimodal%20large%20language%20models%20%28MLLMs%29.%20However%2C%20directly%0Aapplying%20RL%20training%20with%20the%20GRPO%20algorithm%20to%20video%20reasoning%20presents%20two%0Aprimary%20challenges%3A%20%28i%29%20a%20lack%20of%20temporal%20modeling%20for%20video%20reasoning%2C%20and%0A%28ii%29%20the%20scarcity%20of%20high-quality%20video-reasoning%20data.%20To%20address%20these%0Aissues%2C%20we%20first%20propose%20the%20T-GRPO%20algorithm%2C%20which%20encourages%20models%20to%0Autilize%20temporal%20information%20in%20videos%20for%20reasoning.%20Additionally%2C%20instead%20of%0Arelying%20solely%20on%20video%20data%2C%20we%20incorporate%20high-quality%20image-reasoning%20data%0Ainto%20the%20training%20process.%20We%20have%20constructed%20two%20datasets%3A%20Video-R1-CoT-165k%0Afor%20SFT%20cold%20start%20and%20Video-R1-260k%20for%20RL%20training%2C%20both%20comprising%20image%20and%0Avideo%20data.%20Experimental%20results%20demonstrate%20that%20Video-R1%20achieves%20significant%0Aimprovements%20on%20video%20reasoning%20benchmarks%20such%20as%20VideoMMMU%20and%20VSI-Bench%2C%20as%0Awell%20as%20on%20general%20video%20benchmarks%20including%20MVBench%20and%20TempCompass%2C%20etc.%0ANotably%2C%20Video-R1-7B%20attains%20a%2037.1%25%20accuracy%20on%20video%20spatial%20reasoning%0Abenchmark%20VSI-bench%2C%20surpassing%20the%20commercial%20proprietary%20model%20GPT-4o.%20All%0Acode%2C%20models%2C%20and%20data%20are%20released%20in%3A%20https%3A//github.com/tulerfeng/Video-R1.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.21776v4&entry.124074799=Read"},
{"title": "Environment Inference for Learning Generalizable Dynamical System", "author": "Shixuan Liu and Yue He and Haotian Wang and Wenjing Yang and Yunfei Wang and Peng Cui and Zhong Liu", "abstract": "  Data-driven methods offer efficient and robust solutions for analyzing\ncomplex dynamical systems but rely on the assumption of I.I.D. data, driving\nthe development of generalization techniques for handling environmental\ndifferences. These techniques, however, are limited by their dependence on\nenvironment labels, which are often unavailable during training due to data\nacquisition challenges, privacy concerns, and environmental variability,\nparticularly in large public datasets and privacy-sensitive domains. In\nresponse, we propose DynaInfer, a novel method that infers environment\nspecifications by analyzing prediction errors from fixed neural networks within\neach training round, enabling environment assignments directly from data. We\nprove our algorithm effectively solves the alternating optimization problem in\nunlabeled scenarios and validate it through extensive experiments across\ndiverse dynamical systems. Results show that DynaInfer outperforms existing\nenvironment assignment techniques, converges rapidly to true labels, and even\nachieves superior performance when environment labels are available.\n", "link": "http://arxiv.org/abs/2510.19784v1", "date": "2025-10-22", "relevancy": 2.2108, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5758}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5383}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Environment%20Inference%20for%20Learning%20Generalizable%20Dynamical%20System&body=Title%3A%20Environment%20Inference%20for%20Learning%20Generalizable%20Dynamical%20System%0AAuthor%3A%20Shixuan%20Liu%20and%20Yue%20He%20and%20Haotian%20Wang%20and%20Wenjing%20Yang%20and%20Yunfei%20Wang%20and%20Peng%20Cui%20and%20Zhong%20Liu%0AAbstract%3A%20%20%20Data-driven%20methods%20offer%20efficient%20and%20robust%20solutions%20for%20analyzing%0Acomplex%20dynamical%20systems%20but%20rely%20on%20the%20assumption%20of%20I.I.D.%20data%2C%20driving%0Athe%20development%20of%20generalization%20techniques%20for%20handling%20environmental%0Adifferences.%20These%20techniques%2C%20however%2C%20are%20limited%20by%20their%20dependence%20on%0Aenvironment%20labels%2C%20which%20are%20often%20unavailable%20during%20training%20due%20to%20data%0Aacquisition%20challenges%2C%20privacy%20concerns%2C%20and%20environmental%20variability%2C%0Aparticularly%20in%20large%20public%20datasets%20and%20privacy-sensitive%20domains.%20In%0Aresponse%2C%20we%20propose%20DynaInfer%2C%20a%20novel%20method%20that%20infers%20environment%0Aspecifications%20by%20analyzing%20prediction%20errors%20from%20fixed%20neural%20networks%20within%0Aeach%20training%20round%2C%20enabling%20environment%20assignments%20directly%20from%20data.%20We%0Aprove%20our%20algorithm%20effectively%20solves%20the%20alternating%20optimization%20problem%20in%0Aunlabeled%20scenarios%20and%20validate%20it%20through%20extensive%20experiments%20across%0Adiverse%20dynamical%20systems.%20Results%20show%20that%20DynaInfer%20outperforms%20existing%0Aenvironment%20assignment%20techniques%2C%20converges%20rapidly%20to%20true%20labels%2C%20and%20even%0Aachieves%20superior%20performance%20when%20environment%20labels%20are%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19784v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnvironment%2520Inference%2520for%2520Learning%2520Generalizable%2520Dynamical%2520System%26entry.906535625%3DShixuan%2520Liu%2520and%2520Yue%2520He%2520and%2520Haotian%2520Wang%2520and%2520Wenjing%2520Yang%2520and%2520Yunfei%2520Wang%2520and%2520Peng%2520Cui%2520and%2520Zhong%2520Liu%26entry.1292438233%3D%2520%2520Data-driven%2520methods%2520offer%2520efficient%2520and%2520robust%2520solutions%2520for%2520analyzing%250Acomplex%2520dynamical%2520systems%2520but%2520rely%2520on%2520the%2520assumption%2520of%2520I.I.D.%2520data%252C%2520driving%250Athe%2520development%2520of%2520generalization%2520techniques%2520for%2520handling%2520environmental%250Adifferences.%2520These%2520techniques%252C%2520however%252C%2520are%2520limited%2520by%2520their%2520dependence%2520on%250Aenvironment%2520labels%252C%2520which%2520are%2520often%2520unavailable%2520during%2520training%2520due%2520to%2520data%250Aacquisition%2520challenges%252C%2520privacy%2520concerns%252C%2520and%2520environmental%2520variability%252C%250Aparticularly%2520in%2520large%2520public%2520datasets%2520and%2520privacy-sensitive%2520domains.%2520In%250Aresponse%252C%2520we%2520propose%2520DynaInfer%252C%2520a%2520novel%2520method%2520that%2520infers%2520environment%250Aspecifications%2520by%2520analyzing%2520prediction%2520errors%2520from%2520fixed%2520neural%2520networks%2520within%250Aeach%2520training%2520round%252C%2520enabling%2520environment%2520assignments%2520directly%2520from%2520data.%2520We%250Aprove%2520our%2520algorithm%2520effectively%2520solves%2520the%2520alternating%2520optimization%2520problem%2520in%250Aunlabeled%2520scenarios%2520and%2520validate%2520it%2520through%2520extensive%2520experiments%2520across%250Adiverse%2520dynamical%2520systems.%2520Results%2520show%2520that%2520DynaInfer%2520outperforms%2520existing%250Aenvironment%2520assignment%2520techniques%252C%2520converges%2520rapidly%2520to%2520true%2520labels%252C%2520and%2520even%250Aachieves%2520superior%2520performance%2520when%2520environment%2520labels%2520are%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19784v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Environment%20Inference%20for%20Learning%20Generalizable%20Dynamical%20System&entry.906535625=Shixuan%20Liu%20and%20Yue%20He%20and%20Haotian%20Wang%20and%20Wenjing%20Yang%20and%20Yunfei%20Wang%20and%20Peng%20Cui%20and%20Zhong%20Liu&entry.1292438233=%20%20Data-driven%20methods%20offer%20efficient%20and%20robust%20solutions%20for%20analyzing%0Acomplex%20dynamical%20systems%20but%20rely%20on%20the%20assumption%20of%20I.I.D.%20data%2C%20driving%0Athe%20development%20of%20generalization%20techniques%20for%20handling%20environmental%0Adifferences.%20These%20techniques%2C%20however%2C%20are%20limited%20by%20their%20dependence%20on%0Aenvironment%20labels%2C%20which%20are%20often%20unavailable%20during%20training%20due%20to%20data%0Aacquisition%20challenges%2C%20privacy%20concerns%2C%20and%20environmental%20variability%2C%0Aparticularly%20in%20large%20public%20datasets%20and%20privacy-sensitive%20domains.%20In%0Aresponse%2C%20we%20propose%20DynaInfer%2C%20a%20novel%20method%20that%20infers%20environment%0Aspecifications%20by%20analyzing%20prediction%20errors%20from%20fixed%20neural%20networks%20within%0Aeach%20training%20round%2C%20enabling%20environment%20assignments%20directly%20from%20data.%20We%0Aprove%20our%20algorithm%20effectively%20solves%20the%20alternating%20optimization%20problem%20in%0Aunlabeled%20scenarios%20and%20validate%20it%20through%20extensive%20experiments%20across%0Adiverse%20dynamical%20systems.%20Results%20show%20that%20DynaInfer%20outperforms%20existing%0Aenvironment%20assignment%20techniques%2C%20converges%20rapidly%20to%20true%20labels%2C%20and%20even%0Aachieves%20superior%20performance%20when%20environment%20labels%20are%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19784v1&entry.124074799=Read"},
{"title": "Base Models Know How to Reason, Thinking Models Learn When", "author": "Constantin Venhoff and Iv\u00e1n Arcuschin and Philip Torr and Arthur Conmy and Neel Nanda", "abstract": "  Why do thinking language models like DeepSeek R1 outperform their base\ncounterparts? Despite consistent performance gains, it remains unclear to what\nextent thinking models learn entirely new reasoning capabilities or repurpose\npre-existing base model ones. In this work, we propose a hybrid model where we\nactivate reasoning mechanisms in base models at the right time to elicit\nthinking-model-level reasoning chains, implying that thinking models exploit\nalready existing capabilities. To ground our analysis, we introduce an\nunsupervised, bottom-up approach for uncovering human-interpretable reasoning\nbehaviors in thinking models. This approach provides an unbiased method to\ndiscover reasoning behaviors without imposing manual or LLM-derived\nassumptions. Across three base and four thinking models, using GSM8K and\nMATH500, our hybrid model recovers up to 91% of the performance gap to thinking\nmodels without any weight updates while steering only 12% of tokens.\nConcretely, our empirical setup provides a simple, causal way to test the\neffectiveness of existing reasoning mechanisms in base models by invoking them\ndirectly and measuring the resulting task performance. More broadly, these\nresults reframe our understanding of how thinking models are trained:\npre-training is when models acquire most of their reasoning mechanisms, and\npost-training teaches efficient deployment of these mechanisms at the right\ntime, enabling efficient use of their inference-time compute.\n", "link": "http://arxiv.org/abs/2510.07364v3", "date": "2025-10-22", "relevancy": 2.2079, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5597}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5597}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5134}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Base%20Models%20Know%20How%20to%20Reason%2C%20Thinking%20Models%20Learn%20When&body=Title%3A%20Base%20Models%20Know%20How%20to%20Reason%2C%20Thinking%20Models%20Learn%20When%0AAuthor%3A%20Constantin%20Venhoff%20and%20Iv%C3%A1n%20Arcuschin%20and%20Philip%20Torr%20and%20Arthur%20Conmy%20and%20Neel%20Nanda%0AAbstract%3A%20%20%20Why%20do%20thinking%20language%20models%20like%20DeepSeek%20R1%20outperform%20their%20base%0Acounterparts%3F%20Despite%20consistent%20performance%20gains%2C%20it%20remains%20unclear%20to%20what%0Aextent%20thinking%20models%20learn%20entirely%20new%20reasoning%20capabilities%20or%20repurpose%0Apre-existing%20base%20model%20ones.%20In%20this%20work%2C%20we%20propose%20a%20hybrid%20model%20where%20we%0Aactivate%20reasoning%20mechanisms%20in%20base%20models%20at%20the%20right%20time%20to%20elicit%0Athinking-model-level%20reasoning%20chains%2C%20implying%20that%20thinking%20models%20exploit%0Aalready%20existing%20capabilities.%20To%20ground%20our%20analysis%2C%20we%20introduce%20an%0Aunsupervised%2C%20bottom-up%20approach%20for%20uncovering%20human-interpretable%20reasoning%0Abehaviors%20in%20thinking%20models.%20This%20approach%20provides%20an%20unbiased%20method%20to%0Adiscover%20reasoning%20behaviors%20without%20imposing%20manual%20or%20LLM-derived%0Aassumptions.%20Across%20three%20base%20and%20four%20thinking%20models%2C%20using%20GSM8K%20and%0AMATH500%2C%20our%20hybrid%20model%20recovers%20up%20to%2091%25%20of%20the%20performance%20gap%20to%20thinking%0Amodels%20without%20any%20weight%20updates%20while%20steering%20only%2012%25%20of%20tokens.%0AConcretely%2C%20our%20empirical%20setup%20provides%20a%20simple%2C%20causal%20way%20to%20test%20the%0Aeffectiveness%20of%20existing%20reasoning%20mechanisms%20in%20base%20models%20by%20invoking%20them%0Adirectly%20and%20measuring%20the%20resulting%20task%20performance.%20More%20broadly%2C%20these%0Aresults%20reframe%20our%20understanding%20of%20how%20thinking%20models%20are%20trained%3A%0Apre-training%20is%20when%20models%20acquire%20most%20of%20their%20reasoning%20mechanisms%2C%20and%0Apost-training%20teaches%20efficient%20deployment%20of%20these%20mechanisms%20at%20the%20right%0Atime%2C%20enabling%20efficient%20use%20of%20their%20inference-time%20compute.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07364v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBase%2520Models%2520Know%2520How%2520to%2520Reason%252C%2520Thinking%2520Models%2520Learn%2520When%26entry.906535625%3DConstantin%2520Venhoff%2520and%2520Iv%25C3%25A1n%2520Arcuschin%2520and%2520Philip%2520Torr%2520and%2520Arthur%2520Conmy%2520and%2520Neel%2520Nanda%26entry.1292438233%3D%2520%2520Why%2520do%2520thinking%2520language%2520models%2520like%2520DeepSeek%2520R1%2520outperform%2520their%2520base%250Acounterparts%253F%2520Despite%2520consistent%2520performance%2520gains%252C%2520it%2520remains%2520unclear%2520to%2520what%250Aextent%2520thinking%2520models%2520learn%2520entirely%2520new%2520reasoning%2520capabilities%2520or%2520repurpose%250Apre-existing%2520base%2520model%2520ones.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520hybrid%2520model%2520where%2520we%250Aactivate%2520reasoning%2520mechanisms%2520in%2520base%2520models%2520at%2520the%2520right%2520time%2520to%2520elicit%250Athinking-model-level%2520reasoning%2520chains%252C%2520implying%2520that%2520thinking%2520models%2520exploit%250Aalready%2520existing%2520capabilities.%2520To%2520ground%2520our%2520analysis%252C%2520we%2520introduce%2520an%250Aunsupervised%252C%2520bottom-up%2520approach%2520for%2520uncovering%2520human-interpretable%2520reasoning%250Abehaviors%2520in%2520thinking%2520models.%2520This%2520approach%2520provides%2520an%2520unbiased%2520method%2520to%250Adiscover%2520reasoning%2520behaviors%2520without%2520imposing%2520manual%2520or%2520LLM-derived%250Aassumptions.%2520Across%2520three%2520base%2520and%2520four%2520thinking%2520models%252C%2520using%2520GSM8K%2520and%250AMATH500%252C%2520our%2520hybrid%2520model%2520recovers%2520up%2520to%252091%2525%2520of%2520the%2520performance%2520gap%2520to%2520thinking%250Amodels%2520without%2520any%2520weight%2520updates%2520while%2520steering%2520only%252012%2525%2520of%2520tokens.%250AConcretely%252C%2520our%2520empirical%2520setup%2520provides%2520a%2520simple%252C%2520causal%2520way%2520to%2520test%2520the%250Aeffectiveness%2520of%2520existing%2520reasoning%2520mechanisms%2520in%2520base%2520models%2520by%2520invoking%2520them%250Adirectly%2520and%2520measuring%2520the%2520resulting%2520task%2520performance.%2520More%2520broadly%252C%2520these%250Aresults%2520reframe%2520our%2520understanding%2520of%2520how%2520thinking%2520models%2520are%2520trained%253A%250Apre-training%2520is%2520when%2520models%2520acquire%2520most%2520of%2520their%2520reasoning%2520mechanisms%252C%2520and%250Apost-training%2520teaches%2520efficient%2520deployment%2520of%2520these%2520mechanisms%2520at%2520the%2520right%250Atime%252C%2520enabling%2520efficient%2520use%2520of%2520their%2520inference-time%2520compute.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07364v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Base%20Models%20Know%20How%20to%20Reason%2C%20Thinking%20Models%20Learn%20When&entry.906535625=Constantin%20Venhoff%20and%20Iv%C3%A1n%20Arcuschin%20and%20Philip%20Torr%20and%20Arthur%20Conmy%20and%20Neel%20Nanda&entry.1292438233=%20%20Why%20do%20thinking%20language%20models%20like%20DeepSeek%20R1%20outperform%20their%20base%0Acounterparts%3F%20Despite%20consistent%20performance%20gains%2C%20it%20remains%20unclear%20to%20what%0Aextent%20thinking%20models%20learn%20entirely%20new%20reasoning%20capabilities%20or%20repurpose%0Apre-existing%20base%20model%20ones.%20In%20this%20work%2C%20we%20propose%20a%20hybrid%20model%20where%20we%0Aactivate%20reasoning%20mechanisms%20in%20base%20models%20at%20the%20right%20time%20to%20elicit%0Athinking-model-level%20reasoning%20chains%2C%20implying%20that%20thinking%20models%20exploit%0Aalready%20existing%20capabilities.%20To%20ground%20our%20analysis%2C%20we%20introduce%20an%0Aunsupervised%2C%20bottom-up%20approach%20for%20uncovering%20human-interpretable%20reasoning%0Abehaviors%20in%20thinking%20models.%20This%20approach%20provides%20an%20unbiased%20method%20to%0Adiscover%20reasoning%20behaviors%20without%20imposing%20manual%20or%20LLM-derived%0Aassumptions.%20Across%20three%20base%20and%20four%20thinking%20models%2C%20using%20GSM8K%20and%0AMATH500%2C%20our%20hybrid%20model%20recovers%20up%20to%2091%25%20of%20the%20performance%20gap%20to%20thinking%0Amodels%20without%20any%20weight%20updates%20while%20steering%20only%2012%25%20of%20tokens.%0AConcretely%2C%20our%20empirical%20setup%20provides%20a%20simple%2C%20causal%20way%20to%20test%20the%0Aeffectiveness%20of%20existing%20reasoning%20mechanisms%20in%20base%20models%20by%20invoking%20them%0Adirectly%20and%20measuring%20the%20resulting%20task%20performance.%20More%20broadly%2C%20these%0Aresults%20reframe%20our%20understanding%20of%20how%20thinking%20models%20are%20trained%3A%0Apre-training%20is%20when%20models%20acquire%20most%20of%20their%20reasoning%20mechanisms%2C%20and%0Apost-training%20teaches%20efficient%20deployment%20of%20these%20mechanisms%20at%20the%20right%0Atime%2C%20enabling%20efficient%20use%20of%20their%20inference-time%20compute.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07364v3&entry.124074799=Read"},
{"title": "Curvilinear Structure-preserving Unpaired Cross-domain Medical Image\n  Translation", "author": "Zihao Chen and Yi Zhou and Xudong Jiang and Li Chen and Leopold Schmetterer and Bingyao Tan and Jun Cheng", "abstract": "  Unpaired image-to-image translation has emerged as a crucial technique in\nmedical imaging, enabling cross-modality synthesis, domain adaptation, and data\naugmentation without costly paired datasets. Yet, existing approaches often\ndistort fine curvilinear structures, such as microvasculature, undermining both\ndiagnostic reliability and quantitative analysis. This limitation is\nconsequential in ophthalmic and vascular imaging, where subtle morphological\nchanges carry significant clinical meaning. We propose Curvilinear\nStructure-preserving Translation (CST), a general framework that explicitly\npreserves fine curvilinear structures during unpaired translation by\nintegrating structure consistency into the training. Specifically, CST augments\nbaseline models with a curvilinear extraction module for topological\nsupervision. It can be seamlessly incorporated into existing methods. We\nintegrate it into CycleGAN and UNSB as two representative backbones.\nComprehensive evaluation across three imaging modalities: optical coherence\ntomography angiography, color fundus and X-ray coronary angiography\ndemonstrates that CST improves translation fidelity and achieves\nstate-of-the-art performance. By reinforcing geometric integrity in learned\nmappings, CST establishes a principled pathway toward curvilinear\nstructure-aware cross-domain translation in medical imaging.\n", "link": "http://arxiv.org/abs/2510.19679v1", "date": "2025-10-22", "relevancy": 2.1949, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.562}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5402}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Curvilinear%20Structure-preserving%20Unpaired%20Cross-domain%20Medical%20Image%0A%20%20Translation&body=Title%3A%20Curvilinear%20Structure-preserving%20Unpaired%20Cross-domain%20Medical%20Image%0A%20%20Translation%0AAuthor%3A%20Zihao%20Chen%20and%20Yi%20Zhou%20and%20Xudong%20Jiang%20and%20Li%20Chen%20and%20Leopold%20Schmetterer%20and%20Bingyao%20Tan%20and%20Jun%20Cheng%0AAbstract%3A%20%20%20Unpaired%20image-to-image%20translation%20has%20emerged%20as%20a%20crucial%20technique%20in%0Amedical%20imaging%2C%20enabling%20cross-modality%20synthesis%2C%20domain%20adaptation%2C%20and%20data%0Aaugmentation%20without%20costly%20paired%20datasets.%20Yet%2C%20existing%20approaches%20often%0Adistort%20fine%20curvilinear%20structures%2C%20such%20as%20microvasculature%2C%20undermining%20both%0Adiagnostic%20reliability%20and%20quantitative%20analysis.%20This%20limitation%20is%0Aconsequential%20in%20ophthalmic%20and%20vascular%20imaging%2C%20where%20subtle%20morphological%0Achanges%20carry%20significant%20clinical%20meaning.%20We%20propose%20Curvilinear%0AStructure-preserving%20Translation%20%28CST%29%2C%20a%20general%20framework%20that%20explicitly%0Apreserves%20fine%20curvilinear%20structures%20during%20unpaired%20translation%20by%0Aintegrating%20structure%20consistency%20into%20the%20training.%20Specifically%2C%20CST%20augments%0Abaseline%20models%20with%20a%20curvilinear%20extraction%20module%20for%20topological%0Asupervision.%20It%20can%20be%20seamlessly%20incorporated%20into%20existing%20methods.%20We%0Aintegrate%20it%20into%20CycleGAN%20and%20UNSB%20as%20two%20representative%20backbones.%0AComprehensive%20evaluation%20across%20three%20imaging%20modalities%3A%20optical%20coherence%0Atomography%20angiography%2C%20color%20fundus%20and%20X-ray%20coronary%20angiography%0Ademonstrates%20that%20CST%20improves%20translation%20fidelity%20and%20achieves%0Astate-of-the-art%20performance.%20By%20reinforcing%20geometric%20integrity%20in%20learned%0Amappings%2C%20CST%20establishes%20a%20principled%20pathway%20toward%20curvilinear%0Astructure-aware%20cross-domain%20translation%20in%20medical%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19679v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCurvilinear%2520Structure-preserving%2520Unpaired%2520Cross-domain%2520Medical%2520Image%250A%2520%2520Translation%26entry.906535625%3DZihao%2520Chen%2520and%2520Yi%2520Zhou%2520and%2520Xudong%2520Jiang%2520and%2520Li%2520Chen%2520and%2520Leopold%2520Schmetterer%2520and%2520Bingyao%2520Tan%2520and%2520Jun%2520Cheng%26entry.1292438233%3D%2520%2520Unpaired%2520image-to-image%2520translation%2520has%2520emerged%2520as%2520a%2520crucial%2520technique%2520in%250Amedical%2520imaging%252C%2520enabling%2520cross-modality%2520synthesis%252C%2520domain%2520adaptation%252C%2520and%2520data%250Aaugmentation%2520without%2520costly%2520paired%2520datasets.%2520Yet%252C%2520existing%2520approaches%2520often%250Adistort%2520fine%2520curvilinear%2520structures%252C%2520such%2520as%2520microvasculature%252C%2520undermining%2520both%250Adiagnostic%2520reliability%2520and%2520quantitative%2520analysis.%2520This%2520limitation%2520is%250Aconsequential%2520in%2520ophthalmic%2520and%2520vascular%2520imaging%252C%2520where%2520subtle%2520morphological%250Achanges%2520carry%2520significant%2520clinical%2520meaning.%2520We%2520propose%2520Curvilinear%250AStructure-preserving%2520Translation%2520%2528CST%2529%252C%2520a%2520general%2520framework%2520that%2520explicitly%250Apreserves%2520fine%2520curvilinear%2520structures%2520during%2520unpaired%2520translation%2520by%250Aintegrating%2520structure%2520consistency%2520into%2520the%2520training.%2520Specifically%252C%2520CST%2520augments%250Abaseline%2520models%2520with%2520a%2520curvilinear%2520extraction%2520module%2520for%2520topological%250Asupervision.%2520It%2520can%2520be%2520seamlessly%2520incorporated%2520into%2520existing%2520methods.%2520We%250Aintegrate%2520it%2520into%2520CycleGAN%2520and%2520UNSB%2520as%2520two%2520representative%2520backbones.%250AComprehensive%2520evaluation%2520across%2520three%2520imaging%2520modalities%253A%2520optical%2520coherence%250Atomography%2520angiography%252C%2520color%2520fundus%2520and%2520X-ray%2520coronary%2520angiography%250Ademonstrates%2520that%2520CST%2520improves%2520translation%2520fidelity%2520and%2520achieves%250Astate-of-the-art%2520performance.%2520By%2520reinforcing%2520geometric%2520integrity%2520in%2520learned%250Amappings%252C%2520CST%2520establishes%2520a%2520principled%2520pathway%2520toward%2520curvilinear%250Astructure-aware%2520cross-domain%2520translation%2520in%2520medical%2520imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19679v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Curvilinear%20Structure-preserving%20Unpaired%20Cross-domain%20Medical%20Image%0A%20%20Translation&entry.906535625=Zihao%20Chen%20and%20Yi%20Zhou%20and%20Xudong%20Jiang%20and%20Li%20Chen%20and%20Leopold%20Schmetterer%20and%20Bingyao%20Tan%20and%20Jun%20Cheng&entry.1292438233=%20%20Unpaired%20image-to-image%20translation%20has%20emerged%20as%20a%20crucial%20technique%20in%0Amedical%20imaging%2C%20enabling%20cross-modality%20synthesis%2C%20domain%20adaptation%2C%20and%20data%0Aaugmentation%20without%20costly%20paired%20datasets.%20Yet%2C%20existing%20approaches%20often%0Adistort%20fine%20curvilinear%20structures%2C%20such%20as%20microvasculature%2C%20undermining%20both%0Adiagnostic%20reliability%20and%20quantitative%20analysis.%20This%20limitation%20is%0Aconsequential%20in%20ophthalmic%20and%20vascular%20imaging%2C%20where%20subtle%20morphological%0Achanges%20carry%20significant%20clinical%20meaning.%20We%20propose%20Curvilinear%0AStructure-preserving%20Translation%20%28CST%29%2C%20a%20general%20framework%20that%20explicitly%0Apreserves%20fine%20curvilinear%20structures%20during%20unpaired%20translation%20by%0Aintegrating%20structure%20consistency%20into%20the%20training.%20Specifically%2C%20CST%20augments%0Abaseline%20models%20with%20a%20curvilinear%20extraction%20module%20for%20topological%0Asupervision.%20It%20can%20be%20seamlessly%20incorporated%20into%20existing%20methods.%20We%0Aintegrate%20it%20into%20CycleGAN%20and%20UNSB%20as%20two%20representative%20backbones.%0AComprehensive%20evaluation%20across%20three%20imaging%20modalities%3A%20optical%20coherence%0Atomography%20angiography%2C%20color%20fundus%20and%20X-ray%20coronary%20angiography%0Ademonstrates%20that%20CST%20improves%20translation%20fidelity%20and%20achieves%0Astate-of-the-art%20performance.%20By%20reinforcing%20geometric%20integrity%20in%20learned%0Amappings%2C%20CST%20establishes%20a%20principled%20pathway%20toward%20curvilinear%0Astructure-aware%20cross-domain%20translation%20in%20medical%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19679v1&entry.124074799=Read"},
{"title": "An Efficient Local Search Approach for Polarized Community Discovery in\n  Signed Networks", "author": "Linus Aronsson and Morteza Haghir Chehreghani", "abstract": "  Signed networks, where edges are labeled as positive or negative to represent\nfriendly or antagonistic interactions, provide a natural framework for\nanalyzing polarization, trust, and conflict in social systems. Detecting\nmeaningful group structures in such networks is crucial for understanding\nonline discourse, political divisions, and trust dynamics. A key challenge is\nto identify communities that are internally cohesive and externally\nantagonistic, while allowing for neutral or unaligned vertices. In this paper,\nwe propose a method for identifying $k$ polarized communities that addresses a\nmajor limitation of prior methods: their tendency to produce highly\nsize-imbalanced solutions. We introduce a novel optimization objective that\navoids such imbalance. In addition, it is well known that approximation\nalgorithms based on local search are highly effective for clustering signed\nnetworks when neutral vertices are not allowed. We build on this idea and\ndesign the first local search algorithm that extends to the setting with\nneutral vertices while scaling to large networks. By connecting our approach to\nblock-coordinate Frank-Wolfe optimization, we prove a linear convergence rate,\nenabled by the structure of our objective. Experiments on real-world and\nsynthetic datasets demonstrate that our method consistently outperforms\nstate-of-the-art baselines in solution quality, while remaining competitive in\ncomputational efficiency.\n", "link": "http://arxiv.org/abs/2502.02197v3", "date": "2025-10-22", "relevancy": 2.194, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4442}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4418}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Efficient%20Local%20Search%20Approach%20for%20Polarized%20Community%20Discovery%20in%0A%20%20Signed%20Networks&body=Title%3A%20An%20Efficient%20Local%20Search%20Approach%20for%20Polarized%20Community%20Discovery%20in%0A%20%20Signed%20Networks%0AAuthor%3A%20Linus%20Aronsson%20and%20Morteza%20Haghir%20Chehreghani%0AAbstract%3A%20%20%20Signed%20networks%2C%20where%20edges%20are%20labeled%20as%20positive%20or%20negative%20to%20represent%0Afriendly%20or%20antagonistic%20interactions%2C%20provide%20a%20natural%20framework%20for%0Aanalyzing%20polarization%2C%20trust%2C%20and%20conflict%20in%20social%20systems.%20Detecting%0Ameaningful%20group%20structures%20in%20such%20networks%20is%20crucial%20for%20understanding%0Aonline%20discourse%2C%20political%20divisions%2C%20and%20trust%20dynamics.%20A%20key%20challenge%20is%0Ato%20identify%20communities%20that%20are%20internally%20cohesive%20and%20externally%0Aantagonistic%2C%20while%20allowing%20for%20neutral%20or%20unaligned%20vertices.%20In%20this%20paper%2C%0Awe%20propose%20a%20method%20for%20identifying%20%24k%24%20polarized%20communities%20that%20addresses%20a%0Amajor%20limitation%20of%20prior%20methods%3A%20their%20tendency%20to%20produce%20highly%0Asize-imbalanced%20solutions.%20We%20introduce%20a%20novel%20optimization%20objective%20that%0Aavoids%20such%20imbalance.%20In%20addition%2C%20it%20is%20well%20known%20that%20approximation%0Aalgorithms%20based%20on%20local%20search%20are%20highly%20effective%20for%20clustering%20signed%0Anetworks%20when%20neutral%20vertices%20are%20not%20allowed.%20We%20build%20on%20this%20idea%20and%0Adesign%20the%20first%20local%20search%20algorithm%20that%20extends%20to%20the%20setting%20with%0Aneutral%20vertices%20while%20scaling%20to%20large%20networks.%20By%20connecting%20our%20approach%20to%0Ablock-coordinate%20Frank-Wolfe%20optimization%2C%20we%20prove%20a%20linear%20convergence%20rate%2C%0Aenabled%20by%20the%20structure%20of%20our%20objective.%20Experiments%20on%20real-world%20and%0Asynthetic%20datasets%20demonstrate%20that%20our%20method%20consistently%20outperforms%0Astate-of-the-art%20baselines%20in%20solution%20quality%2C%20while%20remaining%20competitive%20in%0Acomputational%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02197v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Efficient%2520Local%2520Search%2520Approach%2520for%2520Polarized%2520Community%2520Discovery%2520in%250A%2520%2520Signed%2520Networks%26entry.906535625%3DLinus%2520Aronsson%2520and%2520Morteza%2520Haghir%2520Chehreghani%26entry.1292438233%3D%2520%2520Signed%2520networks%252C%2520where%2520edges%2520are%2520labeled%2520as%2520positive%2520or%2520negative%2520to%2520represent%250Afriendly%2520or%2520antagonistic%2520interactions%252C%2520provide%2520a%2520natural%2520framework%2520for%250Aanalyzing%2520polarization%252C%2520trust%252C%2520and%2520conflict%2520in%2520social%2520systems.%2520Detecting%250Ameaningful%2520group%2520structures%2520in%2520such%2520networks%2520is%2520crucial%2520for%2520understanding%250Aonline%2520discourse%252C%2520political%2520divisions%252C%2520and%2520trust%2520dynamics.%2520A%2520key%2520challenge%2520is%250Ato%2520identify%2520communities%2520that%2520are%2520internally%2520cohesive%2520and%2520externally%250Aantagonistic%252C%2520while%2520allowing%2520for%2520neutral%2520or%2520unaligned%2520vertices.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520a%2520method%2520for%2520identifying%2520%2524k%2524%2520polarized%2520communities%2520that%2520addresses%2520a%250Amajor%2520limitation%2520of%2520prior%2520methods%253A%2520their%2520tendency%2520to%2520produce%2520highly%250Asize-imbalanced%2520solutions.%2520We%2520introduce%2520a%2520novel%2520optimization%2520objective%2520that%250Aavoids%2520such%2520imbalance.%2520In%2520addition%252C%2520it%2520is%2520well%2520known%2520that%2520approximation%250Aalgorithms%2520based%2520on%2520local%2520search%2520are%2520highly%2520effective%2520for%2520clustering%2520signed%250Anetworks%2520when%2520neutral%2520vertices%2520are%2520not%2520allowed.%2520We%2520build%2520on%2520this%2520idea%2520and%250Adesign%2520the%2520first%2520local%2520search%2520algorithm%2520that%2520extends%2520to%2520the%2520setting%2520with%250Aneutral%2520vertices%2520while%2520scaling%2520to%2520large%2520networks.%2520By%2520connecting%2520our%2520approach%2520to%250Ablock-coordinate%2520Frank-Wolfe%2520optimization%252C%2520we%2520prove%2520a%2520linear%2520convergence%2520rate%252C%250Aenabled%2520by%2520the%2520structure%2520of%2520our%2520objective.%2520Experiments%2520on%2520real-world%2520and%250Asynthetic%2520datasets%2520demonstrate%2520that%2520our%2520method%2520consistently%2520outperforms%250Astate-of-the-art%2520baselines%2520in%2520solution%2520quality%252C%2520while%2520remaining%2520competitive%2520in%250Acomputational%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02197v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Efficient%20Local%20Search%20Approach%20for%20Polarized%20Community%20Discovery%20in%0A%20%20Signed%20Networks&entry.906535625=Linus%20Aronsson%20and%20Morteza%20Haghir%20Chehreghani&entry.1292438233=%20%20Signed%20networks%2C%20where%20edges%20are%20labeled%20as%20positive%20or%20negative%20to%20represent%0Afriendly%20or%20antagonistic%20interactions%2C%20provide%20a%20natural%20framework%20for%0Aanalyzing%20polarization%2C%20trust%2C%20and%20conflict%20in%20social%20systems.%20Detecting%0Ameaningful%20group%20structures%20in%20such%20networks%20is%20crucial%20for%20understanding%0Aonline%20discourse%2C%20political%20divisions%2C%20and%20trust%20dynamics.%20A%20key%20challenge%20is%0Ato%20identify%20communities%20that%20are%20internally%20cohesive%20and%20externally%0Aantagonistic%2C%20while%20allowing%20for%20neutral%20or%20unaligned%20vertices.%20In%20this%20paper%2C%0Awe%20propose%20a%20method%20for%20identifying%20%24k%24%20polarized%20communities%20that%20addresses%20a%0Amajor%20limitation%20of%20prior%20methods%3A%20their%20tendency%20to%20produce%20highly%0Asize-imbalanced%20solutions.%20We%20introduce%20a%20novel%20optimization%20objective%20that%0Aavoids%20such%20imbalance.%20In%20addition%2C%20it%20is%20well%20known%20that%20approximation%0Aalgorithms%20based%20on%20local%20search%20are%20highly%20effective%20for%20clustering%20signed%0Anetworks%20when%20neutral%20vertices%20are%20not%20allowed.%20We%20build%20on%20this%20idea%20and%0Adesign%20the%20first%20local%20search%20algorithm%20that%20extends%20to%20the%20setting%20with%0Aneutral%20vertices%20while%20scaling%20to%20large%20networks.%20By%20connecting%20our%20approach%20to%0Ablock-coordinate%20Frank-Wolfe%20optimization%2C%20we%20prove%20a%20linear%20convergence%20rate%2C%0Aenabled%20by%20the%20structure%20of%20our%20objective.%20Experiments%20on%20real-world%20and%0Asynthetic%20datasets%20demonstrate%20that%20our%20method%20consistently%20outperforms%0Astate-of-the-art%20baselines%20in%20solution%20quality%2C%20while%20remaining%20competitive%20in%0Acomputational%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02197v3&entry.124074799=Read"},
{"title": "MedReason-R1: Learning to Reason for CT Diagnosis with Reinforcement\n  Learning and Local Zoom", "author": "Yifan Li and Fenghe Tang and Yingtai Li and Shaohua Kevin Zhou", "abstract": "  General-purpose large Vision-Language Models (VLMs) demonstrate strong\ncapabilities in generating detailed descriptions for natural images. However,\ntheir performance in the medical domain remains suboptimal, even for relatively\nstraightforward tasks, primarily due to the lack of large-scale, high-quality,\nspecialized medical imaging datasets and the neglect of the diagnostic process\nthat progresses from coarse to fine-grained. To address the first issue, we\nconstruct the CT-RATE-VQA dataset, which has 84K QA pairs. For the second\nissue, we propose MedReason-R1, a medical VLM with explicit reasoning process\nfor disease diagnosis. MedReason-R1 incorporates a novel strategy that embeds\nzoom-in disease region-of-interest areas into the image, highlighting the\ncrucial role of both global localization and disease-specific details in\nenhancing the model's diagnostic performance. Furthermore, we introduce the\nGRPO reinforcement learning framework to MedReason-R1, which enables effective\nreasoning without relying on costly manual annotations. Compared to recent\ngeneral-purpose and medical VLMs, MedReason-R1 achieves state-of-the-art\nperformance in CT disease diagnosis while retaining generalization. The code,\ncheckpoints, and dataset are available at:\nhttps://github.com/Leevan001/MedReason-R1\n", "link": "http://arxiv.org/abs/2510.19626v1", "date": "2025-10-22", "relevancy": 2.1918, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5511}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5511}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5324}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MedReason-R1%3A%20Learning%20to%20Reason%20for%20CT%20Diagnosis%20with%20Reinforcement%0A%20%20Learning%20and%20Local%20Zoom&body=Title%3A%20MedReason-R1%3A%20Learning%20to%20Reason%20for%20CT%20Diagnosis%20with%20Reinforcement%0A%20%20Learning%20and%20Local%20Zoom%0AAuthor%3A%20Yifan%20Li%20and%20Fenghe%20Tang%20and%20Yingtai%20Li%20and%20Shaohua%20Kevin%20Zhou%0AAbstract%3A%20%20%20General-purpose%20large%20Vision-Language%20Models%20%28VLMs%29%20demonstrate%20strong%0Acapabilities%20in%20generating%20detailed%20descriptions%20for%20natural%20images.%20However%2C%0Atheir%20performance%20in%20the%20medical%20domain%20remains%20suboptimal%2C%20even%20for%20relatively%0Astraightforward%20tasks%2C%20primarily%20due%20to%20the%20lack%20of%20large-scale%2C%20high-quality%2C%0Aspecialized%20medical%20imaging%20datasets%20and%20the%20neglect%20of%20the%20diagnostic%20process%0Athat%20progresses%20from%20coarse%20to%20fine-grained.%20To%20address%20the%20first%20issue%2C%20we%0Aconstruct%20the%20CT-RATE-VQA%20dataset%2C%20which%20has%2084K%20QA%20pairs.%20For%20the%20second%0Aissue%2C%20we%20propose%20MedReason-R1%2C%20a%20medical%20VLM%20with%20explicit%20reasoning%20process%0Afor%20disease%20diagnosis.%20MedReason-R1%20incorporates%20a%20novel%20strategy%20that%20embeds%0Azoom-in%20disease%20region-of-interest%20areas%20into%20the%20image%2C%20highlighting%20the%0Acrucial%20role%20of%20both%20global%20localization%20and%20disease-specific%20details%20in%0Aenhancing%20the%20model%27s%20diagnostic%20performance.%20Furthermore%2C%20we%20introduce%20the%0AGRPO%20reinforcement%20learning%20framework%20to%20MedReason-R1%2C%20which%20enables%20effective%0Areasoning%20without%20relying%20on%20costly%20manual%20annotations.%20Compared%20to%20recent%0Ageneral-purpose%20and%20medical%20VLMs%2C%20MedReason-R1%20achieves%20state-of-the-art%0Aperformance%20in%20CT%20disease%20diagnosis%20while%20retaining%20generalization.%20The%20code%2C%0Acheckpoints%2C%20and%20dataset%20are%20available%20at%3A%0Ahttps%3A//github.com/Leevan001/MedReason-R1%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19626v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedReason-R1%253A%2520Learning%2520to%2520Reason%2520for%2520CT%2520Diagnosis%2520with%2520Reinforcement%250A%2520%2520Learning%2520and%2520Local%2520Zoom%26entry.906535625%3DYifan%2520Li%2520and%2520Fenghe%2520Tang%2520and%2520Yingtai%2520Li%2520and%2520Shaohua%2520Kevin%2520Zhou%26entry.1292438233%3D%2520%2520General-purpose%2520large%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520demonstrate%2520strong%250Acapabilities%2520in%2520generating%2520detailed%2520descriptions%2520for%2520natural%2520images.%2520However%252C%250Atheir%2520performance%2520in%2520the%2520medical%2520domain%2520remains%2520suboptimal%252C%2520even%2520for%2520relatively%250Astraightforward%2520tasks%252C%2520primarily%2520due%2520to%2520the%2520lack%2520of%2520large-scale%252C%2520high-quality%252C%250Aspecialized%2520medical%2520imaging%2520datasets%2520and%2520the%2520neglect%2520of%2520the%2520diagnostic%2520process%250Athat%2520progresses%2520from%2520coarse%2520to%2520fine-grained.%2520To%2520address%2520the%2520first%2520issue%252C%2520we%250Aconstruct%2520the%2520CT-RATE-VQA%2520dataset%252C%2520which%2520has%252084K%2520QA%2520pairs.%2520For%2520the%2520second%250Aissue%252C%2520we%2520propose%2520MedReason-R1%252C%2520a%2520medical%2520VLM%2520with%2520explicit%2520reasoning%2520process%250Afor%2520disease%2520diagnosis.%2520MedReason-R1%2520incorporates%2520a%2520novel%2520strategy%2520that%2520embeds%250Azoom-in%2520disease%2520region-of-interest%2520areas%2520into%2520the%2520image%252C%2520highlighting%2520the%250Acrucial%2520role%2520of%2520both%2520global%2520localization%2520and%2520disease-specific%2520details%2520in%250Aenhancing%2520the%2520model%2527s%2520diagnostic%2520performance.%2520Furthermore%252C%2520we%2520introduce%2520the%250AGRPO%2520reinforcement%2520learning%2520framework%2520to%2520MedReason-R1%252C%2520which%2520enables%2520effective%250Areasoning%2520without%2520relying%2520on%2520costly%2520manual%2520annotations.%2520Compared%2520to%2520recent%250Ageneral-purpose%2520and%2520medical%2520VLMs%252C%2520MedReason-R1%2520achieves%2520state-of-the-art%250Aperformance%2520in%2520CT%2520disease%2520diagnosis%2520while%2520retaining%2520generalization.%2520The%2520code%252C%250Acheckpoints%252C%2520and%2520dataset%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/Leevan001/MedReason-R1%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19626v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedReason-R1%3A%20Learning%20to%20Reason%20for%20CT%20Diagnosis%20with%20Reinforcement%0A%20%20Learning%20and%20Local%20Zoom&entry.906535625=Yifan%20Li%20and%20Fenghe%20Tang%20and%20Yingtai%20Li%20and%20Shaohua%20Kevin%20Zhou&entry.1292438233=%20%20General-purpose%20large%20Vision-Language%20Models%20%28VLMs%29%20demonstrate%20strong%0Acapabilities%20in%20generating%20detailed%20descriptions%20for%20natural%20images.%20However%2C%0Atheir%20performance%20in%20the%20medical%20domain%20remains%20suboptimal%2C%20even%20for%20relatively%0Astraightforward%20tasks%2C%20primarily%20due%20to%20the%20lack%20of%20large-scale%2C%20high-quality%2C%0Aspecialized%20medical%20imaging%20datasets%20and%20the%20neglect%20of%20the%20diagnostic%20process%0Athat%20progresses%20from%20coarse%20to%20fine-grained.%20To%20address%20the%20first%20issue%2C%20we%0Aconstruct%20the%20CT-RATE-VQA%20dataset%2C%20which%20has%2084K%20QA%20pairs.%20For%20the%20second%0Aissue%2C%20we%20propose%20MedReason-R1%2C%20a%20medical%20VLM%20with%20explicit%20reasoning%20process%0Afor%20disease%20diagnosis.%20MedReason-R1%20incorporates%20a%20novel%20strategy%20that%20embeds%0Azoom-in%20disease%20region-of-interest%20areas%20into%20the%20image%2C%20highlighting%20the%0Acrucial%20role%20of%20both%20global%20localization%20and%20disease-specific%20details%20in%0Aenhancing%20the%20model%27s%20diagnostic%20performance.%20Furthermore%2C%20we%20introduce%20the%0AGRPO%20reinforcement%20learning%20framework%20to%20MedReason-R1%2C%20which%20enables%20effective%0Areasoning%20without%20relying%20on%20costly%20manual%20annotations.%20Compared%20to%20recent%0Ageneral-purpose%20and%20medical%20VLMs%2C%20MedReason-R1%20achieves%20state-of-the-art%0Aperformance%20in%20CT%20disease%20diagnosis%20while%20retaining%20generalization.%20The%20code%2C%0Acheckpoints%2C%20and%20dataset%20are%20available%20at%3A%0Ahttps%3A//github.com/Leevan001/MedReason-R1%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19626v1&entry.124074799=Read"},
{"title": "Benchmarking World-Model Learning", "author": "Archana Warrier and Dat Nyugen and Michelangelo Naim and Moksh Jain and Yichao Liang and Karen Schroeder and Cambridge Yang and Joshua B. Tenenbaum and Sebastian Vollmer and Kevin Ellis and Zenna Tavares", "abstract": "  Model-learning agents should gather information to learn world models that\nsupport many downstream tasks and inferences, such as predicting unobserved\nstates, estimating near- and far-term consequences of actions, planning action\nsequences, and detecting changes in dynamics. Current methods for learning and\nevaluating world models diverge from this goal: training and evaluation are\nanchored to next-frame prediction, and success is scored by reward maximization\nin the same environment. We propose WorldTest, a protocol to evaluate\nmodel-learning agents that separates reward-free interaction from a scored test\nphase in a different but related environment. WorldTest is\nopen-ended$\\unicode{x2014}$models should support many different tasks unknown\nahead of time$\\unicode{x2014}$and agnostic to model representation, allowing\ncomparison across approaches. We instantiated WorldTest with AutumnBench, a\nsuite of 43 interactive grid-world environments and 129 tasks across three\nfamilies: masked-frame prediction, planning, and predicting changes to the\ncausal dynamics. We compared 517 human participants and three frontier models\non AutumnBench. We found that humans outperform the models, and scaling compute\nimproves performance only in some environments but not others. WorldTest\nprovides a novel template$\\unicode{x2014}$reward-free exploration, derived\ntests, and behavior-based scoring$\\unicode{x2014}$to evaluate what agents learn\nabout environment dynamics, and AutumnBench exposes significant headroom in\nworld-model learning.\n", "link": "http://arxiv.org/abs/2510.19788v1", "date": "2025-10-22", "relevancy": 2.1794, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5582}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.542}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20World-Model%20Learning&body=Title%3A%20Benchmarking%20World-Model%20Learning%0AAuthor%3A%20Archana%20Warrier%20and%20Dat%20Nyugen%20and%20Michelangelo%20Naim%20and%20Moksh%20Jain%20and%20Yichao%20Liang%20and%20Karen%20Schroeder%20and%20Cambridge%20Yang%20and%20Joshua%20B.%20Tenenbaum%20and%20Sebastian%20Vollmer%20and%20Kevin%20Ellis%20and%20Zenna%20Tavares%0AAbstract%3A%20%20%20Model-learning%20agents%20should%20gather%20information%20to%20learn%20world%20models%20that%0Asupport%20many%20downstream%20tasks%20and%20inferences%2C%20such%20as%20predicting%20unobserved%0Astates%2C%20estimating%20near-%20and%20far-term%20consequences%20of%20actions%2C%20planning%20action%0Asequences%2C%20and%20detecting%20changes%20in%20dynamics.%20Current%20methods%20for%20learning%20and%0Aevaluating%20world%20models%20diverge%20from%20this%20goal%3A%20training%20and%20evaluation%20are%0Aanchored%20to%20next-frame%20prediction%2C%20and%20success%20is%20scored%20by%20reward%20maximization%0Ain%20the%20same%20environment.%20We%20propose%20WorldTest%2C%20a%20protocol%20to%20evaluate%0Amodel-learning%20agents%20that%20separates%20reward-free%20interaction%20from%20a%20scored%20test%0Aphase%20in%20a%20different%20but%20related%20environment.%20WorldTest%20is%0Aopen-ended%24%5Cunicode%7Bx2014%7D%24models%20should%20support%20many%20different%20tasks%20unknown%0Aahead%20of%20time%24%5Cunicode%7Bx2014%7D%24and%20agnostic%20to%20model%20representation%2C%20allowing%0Acomparison%20across%20approaches.%20We%20instantiated%20WorldTest%20with%20AutumnBench%2C%20a%0Asuite%20of%2043%20interactive%20grid-world%20environments%20and%20129%20tasks%20across%20three%0Afamilies%3A%20masked-frame%20prediction%2C%20planning%2C%20and%20predicting%20changes%20to%20the%0Acausal%20dynamics.%20We%20compared%20517%20human%20participants%20and%20three%20frontier%20models%0Aon%20AutumnBench.%20We%20found%20that%20humans%20outperform%20the%20models%2C%20and%20scaling%20compute%0Aimproves%20performance%20only%20in%20some%20environments%20but%20not%20others.%20WorldTest%0Aprovides%20a%20novel%20template%24%5Cunicode%7Bx2014%7D%24reward-free%20exploration%2C%20derived%0Atests%2C%20and%20behavior-based%20scoring%24%5Cunicode%7Bx2014%7D%24to%20evaluate%20what%20agents%20learn%0Aabout%20environment%20dynamics%2C%20and%20AutumnBench%20exposes%20significant%20headroom%20in%0Aworld-model%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19788v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520World-Model%2520Learning%26entry.906535625%3DArchana%2520Warrier%2520and%2520Dat%2520Nyugen%2520and%2520Michelangelo%2520Naim%2520and%2520Moksh%2520Jain%2520and%2520Yichao%2520Liang%2520and%2520Karen%2520Schroeder%2520and%2520Cambridge%2520Yang%2520and%2520Joshua%2520B.%2520Tenenbaum%2520and%2520Sebastian%2520Vollmer%2520and%2520Kevin%2520Ellis%2520and%2520Zenna%2520Tavares%26entry.1292438233%3D%2520%2520Model-learning%2520agents%2520should%2520gather%2520information%2520to%2520learn%2520world%2520models%2520that%250Asupport%2520many%2520downstream%2520tasks%2520and%2520inferences%252C%2520such%2520as%2520predicting%2520unobserved%250Astates%252C%2520estimating%2520near-%2520and%2520far-term%2520consequences%2520of%2520actions%252C%2520planning%2520action%250Asequences%252C%2520and%2520detecting%2520changes%2520in%2520dynamics.%2520Current%2520methods%2520for%2520learning%2520and%250Aevaluating%2520world%2520models%2520diverge%2520from%2520this%2520goal%253A%2520training%2520and%2520evaluation%2520are%250Aanchored%2520to%2520next-frame%2520prediction%252C%2520and%2520success%2520is%2520scored%2520by%2520reward%2520maximization%250Ain%2520the%2520same%2520environment.%2520We%2520propose%2520WorldTest%252C%2520a%2520protocol%2520to%2520evaluate%250Amodel-learning%2520agents%2520that%2520separates%2520reward-free%2520interaction%2520from%2520a%2520scored%2520test%250Aphase%2520in%2520a%2520different%2520but%2520related%2520environment.%2520WorldTest%2520is%250Aopen-ended%2524%255Cunicode%257Bx2014%257D%2524models%2520should%2520support%2520many%2520different%2520tasks%2520unknown%250Aahead%2520of%2520time%2524%255Cunicode%257Bx2014%257D%2524and%2520agnostic%2520to%2520model%2520representation%252C%2520allowing%250Acomparison%2520across%2520approaches.%2520We%2520instantiated%2520WorldTest%2520with%2520AutumnBench%252C%2520a%250Asuite%2520of%252043%2520interactive%2520grid-world%2520environments%2520and%2520129%2520tasks%2520across%2520three%250Afamilies%253A%2520masked-frame%2520prediction%252C%2520planning%252C%2520and%2520predicting%2520changes%2520to%2520the%250Acausal%2520dynamics.%2520We%2520compared%2520517%2520human%2520participants%2520and%2520three%2520frontier%2520models%250Aon%2520AutumnBench.%2520We%2520found%2520that%2520humans%2520outperform%2520the%2520models%252C%2520and%2520scaling%2520compute%250Aimproves%2520performance%2520only%2520in%2520some%2520environments%2520but%2520not%2520others.%2520WorldTest%250Aprovides%2520a%2520novel%2520template%2524%255Cunicode%257Bx2014%257D%2524reward-free%2520exploration%252C%2520derived%250Atests%252C%2520and%2520behavior-based%2520scoring%2524%255Cunicode%257Bx2014%257D%2524to%2520evaluate%2520what%2520agents%2520learn%250Aabout%2520environment%2520dynamics%252C%2520and%2520AutumnBench%2520exposes%2520significant%2520headroom%2520in%250Aworld-model%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19788v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20World-Model%20Learning&entry.906535625=Archana%20Warrier%20and%20Dat%20Nyugen%20and%20Michelangelo%20Naim%20and%20Moksh%20Jain%20and%20Yichao%20Liang%20and%20Karen%20Schroeder%20and%20Cambridge%20Yang%20and%20Joshua%20B.%20Tenenbaum%20and%20Sebastian%20Vollmer%20and%20Kevin%20Ellis%20and%20Zenna%20Tavares&entry.1292438233=%20%20Model-learning%20agents%20should%20gather%20information%20to%20learn%20world%20models%20that%0Asupport%20many%20downstream%20tasks%20and%20inferences%2C%20such%20as%20predicting%20unobserved%0Astates%2C%20estimating%20near-%20and%20far-term%20consequences%20of%20actions%2C%20planning%20action%0Asequences%2C%20and%20detecting%20changes%20in%20dynamics.%20Current%20methods%20for%20learning%20and%0Aevaluating%20world%20models%20diverge%20from%20this%20goal%3A%20training%20and%20evaluation%20are%0Aanchored%20to%20next-frame%20prediction%2C%20and%20success%20is%20scored%20by%20reward%20maximization%0Ain%20the%20same%20environment.%20We%20propose%20WorldTest%2C%20a%20protocol%20to%20evaluate%0Amodel-learning%20agents%20that%20separates%20reward-free%20interaction%20from%20a%20scored%20test%0Aphase%20in%20a%20different%20but%20related%20environment.%20WorldTest%20is%0Aopen-ended%24%5Cunicode%7Bx2014%7D%24models%20should%20support%20many%20different%20tasks%20unknown%0Aahead%20of%20time%24%5Cunicode%7Bx2014%7D%24and%20agnostic%20to%20model%20representation%2C%20allowing%0Acomparison%20across%20approaches.%20We%20instantiated%20WorldTest%20with%20AutumnBench%2C%20a%0Asuite%20of%2043%20interactive%20grid-world%20environments%20and%20129%20tasks%20across%20three%0Afamilies%3A%20masked-frame%20prediction%2C%20planning%2C%20and%20predicting%20changes%20to%20the%0Acausal%20dynamics.%20We%20compared%20517%20human%20participants%20and%20three%20frontier%20models%0Aon%20AutumnBench.%20We%20found%20that%20humans%20outperform%20the%20models%2C%20and%20scaling%20compute%0Aimproves%20performance%20only%20in%20some%20environments%20but%20not%20others.%20WorldTest%0Aprovides%20a%20novel%20template%24%5Cunicode%7Bx2014%7D%24reward-free%20exploration%2C%20derived%0Atests%2C%20and%20behavior-based%20scoring%24%5Cunicode%7Bx2014%7D%24to%20evaluate%20what%20agents%20learn%0Aabout%20environment%20dynamics%2C%20and%20AutumnBench%20exposes%20significant%20headroom%20in%0Aworld-model%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19788v1&entry.124074799=Read"},
{"title": "Adaptive Distribution-aware Quantization for Mixed-Precision Neural\n  Networks", "author": "Shaohang Jia and Zhiyong Huang and Zhi Yu and Mingyang Hou and Shuai Miao and Han Yang", "abstract": "  Quantization-Aware Training (QAT) is a critical technique for deploying deep\nneural networks on resource-constrained devices. However, existing methods\noften face two major challenges: the highly non-uniform distribution of\nactivations and the static, mismatched codebooks used in weight quantization.\nTo address these challenges, we propose Adaptive Distribution-aware\nQuantization (ADQ), a mixed-precision quantization framework that employs a\ndifferentiated strategy. The core of ADQ is a novel adaptive weight\nquantization scheme comprising three key innovations: (1) a quantile-based\ninitialization method that constructs a codebook closely aligned with the\ninitial weight distribution; (2) an online codebook adaptation mechanism based\non Exponential Moving Average (EMA) to dynamically track distributional shifts;\nand (3) a sensitivity-informed strategy for mixed-precision allocation. For\nactivations, we integrate a hardware-friendly non-uniform-to-uniform mapping\nscheme. Comprehensive experiments validate the effectiveness of our method. On\nImageNet, ADQ enables a ResNet-18 to achieve 71.512% Top-1 accuracy with an\naverage bit-width of only 2.81 bits, outperforming state-of-the-art methods\nunder comparable conditions. Furthermore, detailed ablation studies on CIFAR-10\nsystematically demonstrate the individual contributions of each innovative\ncomponent, validating the rationale and effectiveness of our design.\n", "link": "http://arxiv.org/abs/2510.19760v1", "date": "2025-10-22", "relevancy": 2.1787, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.557}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5383}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5299}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Distribution-aware%20Quantization%20for%20Mixed-Precision%20Neural%0A%20%20Networks&body=Title%3A%20Adaptive%20Distribution-aware%20Quantization%20for%20Mixed-Precision%20Neural%0A%20%20Networks%0AAuthor%3A%20Shaohang%20Jia%20and%20Zhiyong%20Huang%20and%20Zhi%20Yu%20and%20Mingyang%20Hou%20and%20Shuai%20Miao%20and%20Han%20Yang%0AAbstract%3A%20%20%20Quantization-Aware%20Training%20%28QAT%29%20is%20a%20critical%20technique%20for%20deploying%20deep%0Aneural%20networks%20on%20resource-constrained%20devices.%20However%2C%20existing%20methods%0Aoften%20face%20two%20major%20challenges%3A%20the%20highly%20non-uniform%20distribution%20of%0Aactivations%20and%20the%20static%2C%20mismatched%20codebooks%20used%20in%20weight%20quantization.%0ATo%20address%20these%20challenges%2C%20we%20propose%20Adaptive%20Distribution-aware%0AQuantization%20%28ADQ%29%2C%20a%20mixed-precision%20quantization%20framework%20that%20employs%20a%0Adifferentiated%20strategy.%20The%20core%20of%20ADQ%20is%20a%20novel%20adaptive%20weight%0Aquantization%20scheme%20comprising%20three%20key%20innovations%3A%20%281%29%20a%20quantile-based%0Ainitialization%20method%20that%20constructs%20a%20codebook%20closely%20aligned%20with%20the%0Ainitial%20weight%20distribution%3B%20%282%29%20an%20online%20codebook%20adaptation%20mechanism%20based%0Aon%20Exponential%20Moving%20Average%20%28EMA%29%20to%20dynamically%20track%20distributional%20shifts%3B%0Aand%20%283%29%20a%20sensitivity-informed%20strategy%20for%20mixed-precision%20allocation.%20For%0Aactivations%2C%20we%20integrate%20a%20hardware-friendly%20non-uniform-to-uniform%20mapping%0Ascheme.%20Comprehensive%20experiments%20validate%20the%20effectiveness%20of%20our%20method.%20On%0AImageNet%2C%20ADQ%20enables%20a%20ResNet-18%20to%20achieve%2071.512%25%20Top-1%20accuracy%20with%20an%0Aaverage%20bit-width%20of%20only%202.81%20bits%2C%20outperforming%20state-of-the-art%20methods%0Aunder%20comparable%20conditions.%20Furthermore%2C%20detailed%20ablation%20studies%20on%20CIFAR-10%0Asystematically%20demonstrate%20the%20individual%20contributions%20of%20each%20innovative%0Acomponent%2C%20validating%20the%20rationale%20and%20effectiveness%20of%20our%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19760v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Distribution-aware%2520Quantization%2520for%2520Mixed-Precision%2520Neural%250A%2520%2520Networks%26entry.906535625%3DShaohang%2520Jia%2520and%2520Zhiyong%2520Huang%2520and%2520Zhi%2520Yu%2520and%2520Mingyang%2520Hou%2520and%2520Shuai%2520Miao%2520and%2520Han%2520Yang%26entry.1292438233%3D%2520%2520Quantization-Aware%2520Training%2520%2528QAT%2529%2520is%2520a%2520critical%2520technique%2520for%2520deploying%2520deep%250Aneural%2520networks%2520on%2520resource-constrained%2520devices.%2520However%252C%2520existing%2520methods%250Aoften%2520face%2520two%2520major%2520challenges%253A%2520the%2520highly%2520non-uniform%2520distribution%2520of%250Aactivations%2520and%2520the%2520static%252C%2520mismatched%2520codebooks%2520used%2520in%2520weight%2520quantization.%250ATo%2520address%2520these%2520challenges%252C%2520we%2520propose%2520Adaptive%2520Distribution-aware%250AQuantization%2520%2528ADQ%2529%252C%2520a%2520mixed-precision%2520quantization%2520framework%2520that%2520employs%2520a%250Adifferentiated%2520strategy.%2520The%2520core%2520of%2520ADQ%2520is%2520a%2520novel%2520adaptive%2520weight%250Aquantization%2520scheme%2520comprising%2520three%2520key%2520innovations%253A%2520%25281%2529%2520a%2520quantile-based%250Ainitialization%2520method%2520that%2520constructs%2520a%2520codebook%2520closely%2520aligned%2520with%2520the%250Ainitial%2520weight%2520distribution%253B%2520%25282%2529%2520an%2520online%2520codebook%2520adaptation%2520mechanism%2520based%250Aon%2520Exponential%2520Moving%2520Average%2520%2528EMA%2529%2520to%2520dynamically%2520track%2520distributional%2520shifts%253B%250Aand%2520%25283%2529%2520a%2520sensitivity-informed%2520strategy%2520for%2520mixed-precision%2520allocation.%2520For%250Aactivations%252C%2520we%2520integrate%2520a%2520hardware-friendly%2520non-uniform-to-uniform%2520mapping%250Ascheme.%2520Comprehensive%2520experiments%2520validate%2520the%2520effectiveness%2520of%2520our%2520method.%2520On%250AImageNet%252C%2520ADQ%2520enables%2520a%2520ResNet-18%2520to%2520achieve%252071.512%2525%2520Top-1%2520accuracy%2520with%2520an%250Aaverage%2520bit-width%2520of%2520only%25202.81%2520bits%252C%2520outperforming%2520state-of-the-art%2520methods%250Aunder%2520comparable%2520conditions.%2520Furthermore%252C%2520detailed%2520ablation%2520studies%2520on%2520CIFAR-10%250Asystematically%2520demonstrate%2520the%2520individual%2520contributions%2520of%2520each%2520innovative%250Acomponent%252C%2520validating%2520the%2520rationale%2520and%2520effectiveness%2520of%2520our%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19760v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Distribution-aware%20Quantization%20for%20Mixed-Precision%20Neural%0A%20%20Networks&entry.906535625=Shaohang%20Jia%20and%20Zhiyong%20Huang%20and%20Zhi%20Yu%20and%20Mingyang%20Hou%20and%20Shuai%20Miao%20and%20Han%20Yang&entry.1292438233=%20%20Quantization-Aware%20Training%20%28QAT%29%20is%20a%20critical%20technique%20for%20deploying%20deep%0Aneural%20networks%20on%20resource-constrained%20devices.%20However%2C%20existing%20methods%0Aoften%20face%20two%20major%20challenges%3A%20the%20highly%20non-uniform%20distribution%20of%0Aactivations%20and%20the%20static%2C%20mismatched%20codebooks%20used%20in%20weight%20quantization.%0ATo%20address%20these%20challenges%2C%20we%20propose%20Adaptive%20Distribution-aware%0AQuantization%20%28ADQ%29%2C%20a%20mixed-precision%20quantization%20framework%20that%20employs%20a%0Adifferentiated%20strategy.%20The%20core%20of%20ADQ%20is%20a%20novel%20adaptive%20weight%0Aquantization%20scheme%20comprising%20three%20key%20innovations%3A%20%281%29%20a%20quantile-based%0Ainitialization%20method%20that%20constructs%20a%20codebook%20closely%20aligned%20with%20the%0Ainitial%20weight%20distribution%3B%20%282%29%20an%20online%20codebook%20adaptation%20mechanism%20based%0Aon%20Exponential%20Moving%20Average%20%28EMA%29%20to%20dynamically%20track%20distributional%20shifts%3B%0Aand%20%283%29%20a%20sensitivity-informed%20strategy%20for%20mixed-precision%20allocation.%20For%0Aactivations%2C%20we%20integrate%20a%20hardware-friendly%20non-uniform-to-uniform%20mapping%0Ascheme.%20Comprehensive%20experiments%20validate%20the%20effectiveness%20of%20our%20method.%20On%0AImageNet%2C%20ADQ%20enables%20a%20ResNet-18%20to%20achieve%2071.512%25%20Top-1%20accuracy%20with%20an%0Aaverage%20bit-width%20of%20only%202.81%20bits%2C%20outperforming%20state-of-the-art%20methods%0Aunder%20comparable%20conditions.%20Furthermore%2C%20detailed%20ablation%20studies%20on%20CIFAR-10%0Asystematically%20demonstrate%20the%20individual%20contributions%20of%20each%20innovative%0Acomponent%2C%20validating%20the%20rationale%20and%20effectiveness%20of%20our%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19760v1&entry.124074799=Read"},
{"title": "Unraveling Emotions with Pre-Trained Models", "author": "Alejandro Paj\u00f3n-Sanmart\u00edn and Francisco De Arriba-P\u00e9rez and Silvia Garc\u00eda-M\u00e9ndez and F\u00e1tima Leal and Benedita Malheiro and Juan Carlos Burguillo-Rial", "abstract": "  Transformer models have significantly advanced the field of emotion\nrecognition. However, there are still open challenges when exploring open-ended\nqueries for Large Language Models (LLMs). Although current models offer good\nresults, automatic emotion analysis in open texts presents significant\nchallenges, such as contextual ambiguity, linguistic variability, and\ndifficulty interpreting complex emotional expressions. These limitations make\nthe direct application of generalist models difficult. Accordingly, this work\ncompares the effectiveness of fine-tuning and prompt engineering in emotion\ndetection in three distinct scenarios: (i) performance of fine-tuned\npre-trained models and general-purpose LLMs using simple prompts; (ii)\neffectiveness of different emotion prompt designs with LLMs; and (iii) impact\nof emotion grouping techniques on these models. Experimental tests attain\nmetrics above 70% with a fine-tuned pre-trained model for emotion recognition.\nMoreover, the findings highlight that LLMs require structured prompt\nengineering and emotion grouping to enhance their performance. These\nadvancements improve sentiment analysis, human-computer interaction, and\nunderstanding of user behavior across various domains.\n", "link": "http://arxiv.org/abs/2510.19668v1", "date": "2025-10-22", "relevancy": 2.1784, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5497}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5497}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5191}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unraveling%20Emotions%20with%20Pre-Trained%20Models&body=Title%3A%20Unraveling%20Emotions%20with%20Pre-Trained%20Models%0AAuthor%3A%20Alejandro%20Paj%C3%B3n-Sanmart%C3%ADn%20and%20Francisco%20De%20Arriba-P%C3%A9rez%20and%20Silvia%20Garc%C3%ADa-M%C3%A9ndez%20and%20F%C3%A1tima%20Leal%20and%20Benedita%20Malheiro%20and%20Juan%20Carlos%20Burguillo-Rial%0AAbstract%3A%20%20%20Transformer%20models%20have%20significantly%20advanced%20the%20field%20of%20emotion%0Arecognition.%20However%2C%20there%20are%20still%20open%20challenges%20when%20exploring%20open-ended%0Aqueries%20for%20Large%20Language%20Models%20%28LLMs%29.%20Although%20current%20models%20offer%20good%0Aresults%2C%20automatic%20emotion%20analysis%20in%20open%20texts%20presents%20significant%0Achallenges%2C%20such%20as%20contextual%20ambiguity%2C%20linguistic%20variability%2C%20and%0Adifficulty%20interpreting%20complex%20emotional%20expressions.%20These%20limitations%20make%0Athe%20direct%20application%20of%20generalist%20models%20difficult.%20Accordingly%2C%20this%20work%0Acompares%20the%20effectiveness%20of%20fine-tuning%20and%20prompt%20engineering%20in%20emotion%0Adetection%20in%20three%20distinct%20scenarios%3A%20%28i%29%20performance%20of%20fine-tuned%0Apre-trained%20models%20and%20general-purpose%20LLMs%20using%20simple%20prompts%3B%20%28ii%29%0Aeffectiveness%20of%20different%20emotion%20prompt%20designs%20with%20LLMs%3B%20and%20%28iii%29%20impact%0Aof%20emotion%20grouping%20techniques%20on%20these%20models.%20Experimental%20tests%20attain%0Ametrics%20above%2070%25%20with%20a%20fine-tuned%20pre-trained%20model%20for%20emotion%20recognition.%0AMoreover%2C%20the%20findings%20highlight%20that%20LLMs%20require%20structured%20prompt%0Aengineering%20and%20emotion%20grouping%20to%20enhance%20their%20performance.%20These%0Aadvancements%20improve%20sentiment%20analysis%2C%20human-computer%20interaction%2C%20and%0Aunderstanding%20of%20user%20behavior%20across%20various%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19668v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnraveling%2520Emotions%2520with%2520Pre-Trained%2520Models%26entry.906535625%3DAlejandro%2520Paj%25C3%25B3n-Sanmart%25C3%25ADn%2520and%2520Francisco%2520De%2520Arriba-P%25C3%25A9rez%2520and%2520Silvia%2520Garc%25C3%25ADa-M%25C3%25A9ndez%2520and%2520F%25C3%25A1tima%2520Leal%2520and%2520Benedita%2520Malheiro%2520and%2520Juan%2520Carlos%2520Burguillo-Rial%26entry.1292438233%3D%2520%2520Transformer%2520models%2520have%2520significantly%2520advanced%2520the%2520field%2520of%2520emotion%250Arecognition.%2520However%252C%2520there%2520are%2520still%2520open%2520challenges%2520when%2520exploring%2520open-ended%250Aqueries%2520for%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520Although%2520current%2520models%2520offer%2520good%250Aresults%252C%2520automatic%2520emotion%2520analysis%2520in%2520open%2520texts%2520presents%2520significant%250Achallenges%252C%2520such%2520as%2520contextual%2520ambiguity%252C%2520linguistic%2520variability%252C%2520and%250Adifficulty%2520interpreting%2520complex%2520emotional%2520expressions.%2520These%2520limitations%2520make%250Athe%2520direct%2520application%2520of%2520generalist%2520models%2520difficult.%2520Accordingly%252C%2520this%2520work%250Acompares%2520the%2520effectiveness%2520of%2520fine-tuning%2520and%2520prompt%2520engineering%2520in%2520emotion%250Adetection%2520in%2520three%2520distinct%2520scenarios%253A%2520%2528i%2529%2520performance%2520of%2520fine-tuned%250Apre-trained%2520models%2520and%2520general-purpose%2520LLMs%2520using%2520simple%2520prompts%253B%2520%2528ii%2529%250Aeffectiveness%2520of%2520different%2520emotion%2520prompt%2520designs%2520with%2520LLMs%253B%2520and%2520%2528iii%2529%2520impact%250Aof%2520emotion%2520grouping%2520techniques%2520on%2520these%2520models.%2520Experimental%2520tests%2520attain%250Ametrics%2520above%252070%2525%2520with%2520a%2520fine-tuned%2520pre-trained%2520model%2520for%2520emotion%2520recognition.%250AMoreover%252C%2520the%2520findings%2520highlight%2520that%2520LLMs%2520require%2520structured%2520prompt%250Aengineering%2520and%2520emotion%2520grouping%2520to%2520enhance%2520their%2520performance.%2520These%250Aadvancements%2520improve%2520sentiment%2520analysis%252C%2520human-computer%2520interaction%252C%2520and%250Aunderstanding%2520of%2520user%2520behavior%2520across%2520various%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19668v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unraveling%20Emotions%20with%20Pre-Trained%20Models&entry.906535625=Alejandro%20Paj%C3%B3n-Sanmart%C3%ADn%20and%20Francisco%20De%20Arriba-P%C3%A9rez%20and%20Silvia%20Garc%C3%ADa-M%C3%A9ndez%20and%20F%C3%A1tima%20Leal%20and%20Benedita%20Malheiro%20and%20Juan%20Carlos%20Burguillo-Rial&entry.1292438233=%20%20Transformer%20models%20have%20significantly%20advanced%20the%20field%20of%20emotion%0Arecognition.%20However%2C%20there%20are%20still%20open%20challenges%20when%20exploring%20open-ended%0Aqueries%20for%20Large%20Language%20Models%20%28LLMs%29.%20Although%20current%20models%20offer%20good%0Aresults%2C%20automatic%20emotion%20analysis%20in%20open%20texts%20presents%20significant%0Achallenges%2C%20such%20as%20contextual%20ambiguity%2C%20linguistic%20variability%2C%20and%0Adifficulty%20interpreting%20complex%20emotional%20expressions.%20These%20limitations%20make%0Athe%20direct%20application%20of%20generalist%20models%20difficult.%20Accordingly%2C%20this%20work%0Acompares%20the%20effectiveness%20of%20fine-tuning%20and%20prompt%20engineering%20in%20emotion%0Adetection%20in%20three%20distinct%20scenarios%3A%20%28i%29%20performance%20of%20fine-tuned%0Apre-trained%20models%20and%20general-purpose%20LLMs%20using%20simple%20prompts%3B%20%28ii%29%0Aeffectiveness%20of%20different%20emotion%20prompt%20designs%20with%20LLMs%3B%20and%20%28iii%29%20impact%0Aof%20emotion%20grouping%20techniques%20on%20these%20models.%20Experimental%20tests%20attain%0Ametrics%20above%2070%25%20with%20a%20fine-tuned%20pre-trained%20model%20for%20emotion%20recognition.%0AMoreover%2C%20the%20findings%20highlight%20that%20LLMs%20require%20structured%20prompt%0Aengineering%20and%20emotion%20grouping%20to%20enhance%20their%20performance.%20These%0Aadvancements%20improve%20sentiment%20analysis%2C%20human-computer%20interaction%2C%20and%0Aunderstanding%20of%20user%20behavior%20across%20various%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19668v1&entry.124074799=Read"},
{"title": "Memo: Training Memory-Efficient Embodied Agents with Reinforcement\n  Learning", "author": "Gunshi Gupta and Karmesh Yadav and Zsolt Kira and Yarin Gal and Rahaf Aljundi", "abstract": "  To enable embodied agents to operate effectively over extended timeframes, it\nis crucial to develop models that form and access memories to stay\ncontextualized in their environment. In the current paradigm of training\ntransformer-based policies for embodied sequential decision-making tasks,\nvisual inputs often overwhelm the context limits of transformers, while humans\ncan maintain and utilize a lifetime of experience compressed as memories.\nSignificant compression is possible in principle, as much of the input is\nirrelevant and can be abstracted. However, existing approaches predominantly\nfocus on either recurrent models with fixed-size memory or transformers with\nfull-context reliance. In this work, we propose Memo, a transformer-based\narchitecture and training recipe for reinforcement learning (RL) on\nmemory-intensive, long-horizon tasks. Memo incorporates the creation and\nretrieval of memory by interleaving periodic summarization tokens with the\ninputs of a model during training. We demonstrate Memo's effectiveness on a\ngridworld meta-RL benchmark and a multi-object navigation task in\nphoto-realistic indoor settings. Memo outperforms naive long-context\ntransformer baselines while being more compute and storage efficient.\nAdditionally, Memo generalizes better to longer contexts at inference time and\nremains robust in streaming settings, where historical context must be\ntruncated to fit inference constraints.\n", "link": "http://arxiv.org/abs/2510.19732v1", "date": "2025-10-22", "relevancy": 2.172, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5521}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5417}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5407}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Memo%3A%20Training%20Memory-Efficient%20Embodied%20Agents%20with%20Reinforcement%0A%20%20Learning&body=Title%3A%20Memo%3A%20Training%20Memory-Efficient%20Embodied%20Agents%20with%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Gunshi%20Gupta%20and%20Karmesh%20Yadav%20and%20Zsolt%20Kira%20and%20Yarin%20Gal%20and%20Rahaf%20Aljundi%0AAbstract%3A%20%20%20To%20enable%20embodied%20agents%20to%20operate%20effectively%20over%20extended%20timeframes%2C%20it%0Ais%20crucial%20to%20develop%20models%20that%20form%20and%20access%20memories%20to%20stay%0Acontextualized%20in%20their%20environment.%20In%20the%20current%20paradigm%20of%20training%0Atransformer-based%20policies%20for%20embodied%20sequential%20decision-making%20tasks%2C%0Avisual%20inputs%20often%20overwhelm%20the%20context%20limits%20of%20transformers%2C%20while%20humans%0Acan%20maintain%20and%20utilize%20a%20lifetime%20of%20experience%20compressed%20as%20memories.%0ASignificant%20compression%20is%20possible%20in%20principle%2C%20as%20much%20of%20the%20input%20is%0Airrelevant%20and%20can%20be%20abstracted.%20However%2C%20existing%20approaches%20predominantly%0Afocus%20on%20either%20recurrent%20models%20with%20fixed-size%20memory%20or%20transformers%20with%0Afull-context%20reliance.%20In%20this%20work%2C%20we%20propose%20Memo%2C%20a%20transformer-based%0Aarchitecture%20and%20training%20recipe%20for%20reinforcement%20learning%20%28RL%29%20on%0Amemory-intensive%2C%20long-horizon%20tasks.%20Memo%20incorporates%20the%20creation%20and%0Aretrieval%20of%20memory%20by%20interleaving%20periodic%20summarization%20tokens%20with%20the%0Ainputs%20of%20a%20model%20during%20training.%20We%20demonstrate%20Memo%27s%20effectiveness%20on%20a%0Agridworld%20meta-RL%20benchmark%20and%20a%20multi-object%20navigation%20task%20in%0Aphoto-realistic%20indoor%20settings.%20Memo%20outperforms%20naive%20long-context%0Atransformer%20baselines%20while%20being%20more%20compute%20and%20storage%20efficient.%0AAdditionally%2C%20Memo%20generalizes%20better%20to%20longer%20contexts%20at%20inference%20time%20and%0Aremains%20robust%20in%20streaming%20settings%2C%20where%20historical%20context%20must%20be%0Atruncated%20to%20fit%20inference%20constraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19732v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemo%253A%2520Training%2520Memory-Efficient%2520Embodied%2520Agents%2520with%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DGunshi%2520Gupta%2520and%2520Karmesh%2520Yadav%2520and%2520Zsolt%2520Kira%2520and%2520Yarin%2520Gal%2520and%2520Rahaf%2520Aljundi%26entry.1292438233%3D%2520%2520To%2520enable%2520embodied%2520agents%2520to%2520operate%2520effectively%2520over%2520extended%2520timeframes%252C%2520it%250Ais%2520crucial%2520to%2520develop%2520models%2520that%2520form%2520and%2520access%2520memories%2520to%2520stay%250Acontextualized%2520in%2520their%2520environment.%2520In%2520the%2520current%2520paradigm%2520of%2520training%250Atransformer-based%2520policies%2520for%2520embodied%2520sequential%2520decision-making%2520tasks%252C%250Avisual%2520inputs%2520often%2520overwhelm%2520the%2520context%2520limits%2520of%2520transformers%252C%2520while%2520humans%250Acan%2520maintain%2520and%2520utilize%2520a%2520lifetime%2520of%2520experience%2520compressed%2520as%2520memories.%250ASignificant%2520compression%2520is%2520possible%2520in%2520principle%252C%2520as%2520much%2520of%2520the%2520input%2520is%250Airrelevant%2520and%2520can%2520be%2520abstracted.%2520However%252C%2520existing%2520approaches%2520predominantly%250Afocus%2520on%2520either%2520recurrent%2520models%2520with%2520fixed-size%2520memory%2520or%2520transformers%2520with%250Afull-context%2520reliance.%2520In%2520this%2520work%252C%2520we%2520propose%2520Memo%252C%2520a%2520transformer-based%250Aarchitecture%2520and%2520training%2520recipe%2520for%2520reinforcement%2520learning%2520%2528RL%2529%2520on%250Amemory-intensive%252C%2520long-horizon%2520tasks.%2520Memo%2520incorporates%2520the%2520creation%2520and%250Aretrieval%2520of%2520memory%2520by%2520interleaving%2520periodic%2520summarization%2520tokens%2520with%2520the%250Ainputs%2520of%2520a%2520model%2520during%2520training.%2520We%2520demonstrate%2520Memo%2527s%2520effectiveness%2520on%2520a%250Agridworld%2520meta-RL%2520benchmark%2520and%2520a%2520multi-object%2520navigation%2520task%2520in%250Aphoto-realistic%2520indoor%2520settings.%2520Memo%2520outperforms%2520naive%2520long-context%250Atransformer%2520baselines%2520while%2520being%2520more%2520compute%2520and%2520storage%2520efficient.%250AAdditionally%252C%2520Memo%2520generalizes%2520better%2520to%2520longer%2520contexts%2520at%2520inference%2520time%2520and%250Aremains%2520robust%2520in%2520streaming%2520settings%252C%2520where%2520historical%2520context%2520must%2520be%250Atruncated%2520to%2520fit%2520inference%2520constraints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19732v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memo%3A%20Training%20Memory-Efficient%20Embodied%20Agents%20with%20Reinforcement%0A%20%20Learning&entry.906535625=Gunshi%20Gupta%20and%20Karmesh%20Yadav%20and%20Zsolt%20Kira%20and%20Yarin%20Gal%20and%20Rahaf%20Aljundi&entry.1292438233=%20%20To%20enable%20embodied%20agents%20to%20operate%20effectively%20over%20extended%20timeframes%2C%20it%0Ais%20crucial%20to%20develop%20models%20that%20form%20and%20access%20memories%20to%20stay%0Acontextualized%20in%20their%20environment.%20In%20the%20current%20paradigm%20of%20training%0Atransformer-based%20policies%20for%20embodied%20sequential%20decision-making%20tasks%2C%0Avisual%20inputs%20often%20overwhelm%20the%20context%20limits%20of%20transformers%2C%20while%20humans%0Acan%20maintain%20and%20utilize%20a%20lifetime%20of%20experience%20compressed%20as%20memories.%0ASignificant%20compression%20is%20possible%20in%20principle%2C%20as%20much%20of%20the%20input%20is%0Airrelevant%20and%20can%20be%20abstracted.%20However%2C%20existing%20approaches%20predominantly%0Afocus%20on%20either%20recurrent%20models%20with%20fixed-size%20memory%20or%20transformers%20with%0Afull-context%20reliance.%20In%20this%20work%2C%20we%20propose%20Memo%2C%20a%20transformer-based%0Aarchitecture%20and%20training%20recipe%20for%20reinforcement%20learning%20%28RL%29%20on%0Amemory-intensive%2C%20long-horizon%20tasks.%20Memo%20incorporates%20the%20creation%20and%0Aretrieval%20of%20memory%20by%20interleaving%20periodic%20summarization%20tokens%20with%20the%0Ainputs%20of%20a%20model%20during%20training.%20We%20demonstrate%20Memo%27s%20effectiveness%20on%20a%0Agridworld%20meta-RL%20benchmark%20and%20a%20multi-object%20navigation%20task%20in%0Aphoto-realistic%20indoor%20settings.%20Memo%20outperforms%20naive%20long-context%0Atransformer%20baselines%20while%20being%20more%20compute%20and%20storage%20efficient.%0AAdditionally%2C%20Memo%20generalizes%20better%20to%20longer%20contexts%20at%20inference%20time%20and%0Aremains%20robust%20in%20streaming%20settings%2C%20where%20historical%20context%20must%20be%0Atruncated%20to%20fit%20inference%20constraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19732v1&entry.124074799=Read"},
{"title": "Pay Attention to Small Weights", "author": "Chao Zhou and Tom Jacobs and Advait Gadhikar and Rebekka Burkholz", "abstract": "  Finetuning large pretrained neural networks is known to be\nresource-intensive, both in terms of memory and computational cost. To mitigate\nthis, a common approach is to restrict training to a subset of the model\nparameters. By analyzing the relationship between gradients and weights during\nfinetuning, we observe a notable pattern: large gradients are often associated\nwith small-magnitude weights. This correlation is more pronounced in finetuning\nsettings than in training from scratch. Motivated by this observation, we\npropose NANOADAM, which dynamically updates only the small-magnitude weights\nduring finetuning and offers several practical advantages: first, this\ncriterion is gradient-free -- the parameter subset can be determined without\ngradient computation; second, it preserves large-magnitude weights, which are\nlikely to encode critical features learned during pretraining, thereby reducing\nthe risk of catastrophic forgetting; thirdly, it permits the use of larger\nlearning rates and consistently leads to better generalization performance in\nexperiments. We demonstrate this for both NLP and vision tasks.\n", "link": "http://arxiv.org/abs/2506.21374v2", "date": "2025-10-22", "relevancy": 2.1713, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5574}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5507}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pay%20Attention%20to%20Small%20Weights&body=Title%3A%20Pay%20Attention%20to%20Small%20Weights%0AAuthor%3A%20Chao%20Zhou%20and%20Tom%20Jacobs%20and%20Advait%20Gadhikar%20and%20Rebekka%20Burkholz%0AAbstract%3A%20%20%20Finetuning%20large%20pretrained%20neural%20networks%20is%20known%20to%20be%0Aresource-intensive%2C%20both%20in%20terms%20of%20memory%20and%20computational%20cost.%20To%20mitigate%0Athis%2C%20a%20common%20approach%20is%20to%20restrict%20training%20to%20a%20subset%20of%20the%20model%0Aparameters.%20By%20analyzing%20the%20relationship%20between%20gradients%20and%20weights%20during%0Afinetuning%2C%20we%20observe%20a%20notable%20pattern%3A%20large%20gradients%20are%20often%20associated%0Awith%20small-magnitude%20weights.%20This%20correlation%20is%20more%20pronounced%20in%20finetuning%0Asettings%20than%20in%20training%20from%20scratch.%20Motivated%20by%20this%20observation%2C%20we%0Apropose%20NANOADAM%2C%20which%20dynamically%20updates%20only%20the%20small-magnitude%20weights%0Aduring%20finetuning%20and%20offers%20several%20practical%20advantages%3A%20first%2C%20this%0Acriterion%20is%20gradient-free%20--%20the%20parameter%20subset%20can%20be%20determined%20without%0Agradient%20computation%3B%20second%2C%20it%20preserves%20large-magnitude%20weights%2C%20which%20are%0Alikely%20to%20encode%20critical%20features%20learned%20during%20pretraining%2C%20thereby%20reducing%0Athe%20risk%20of%20catastrophic%20forgetting%3B%20thirdly%2C%20it%20permits%20the%20use%20of%20larger%0Alearning%20rates%20and%20consistently%20leads%20to%20better%20generalization%20performance%20in%0Aexperiments.%20We%20demonstrate%20this%20for%20both%20NLP%20and%20vision%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.21374v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPay%2520Attention%2520to%2520Small%2520Weights%26entry.906535625%3DChao%2520Zhou%2520and%2520Tom%2520Jacobs%2520and%2520Advait%2520Gadhikar%2520and%2520Rebekka%2520Burkholz%26entry.1292438233%3D%2520%2520Finetuning%2520large%2520pretrained%2520neural%2520networks%2520is%2520known%2520to%2520be%250Aresource-intensive%252C%2520both%2520in%2520terms%2520of%2520memory%2520and%2520computational%2520cost.%2520To%2520mitigate%250Athis%252C%2520a%2520common%2520approach%2520is%2520to%2520restrict%2520training%2520to%2520a%2520subset%2520of%2520the%2520model%250Aparameters.%2520By%2520analyzing%2520the%2520relationship%2520between%2520gradients%2520and%2520weights%2520during%250Afinetuning%252C%2520we%2520observe%2520a%2520notable%2520pattern%253A%2520large%2520gradients%2520are%2520often%2520associated%250Awith%2520small-magnitude%2520weights.%2520This%2520correlation%2520is%2520more%2520pronounced%2520in%2520finetuning%250Asettings%2520than%2520in%2520training%2520from%2520scratch.%2520Motivated%2520by%2520this%2520observation%252C%2520we%250Apropose%2520NANOADAM%252C%2520which%2520dynamically%2520updates%2520only%2520the%2520small-magnitude%2520weights%250Aduring%2520finetuning%2520and%2520offers%2520several%2520practical%2520advantages%253A%2520first%252C%2520this%250Acriterion%2520is%2520gradient-free%2520--%2520the%2520parameter%2520subset%2520can%2520be%2520determined%2520without%250Agradient%2520computation%253B%2520second%252C%2520it%2520preserves%2520large-magnitude%2520weights%252C%2520which%2520are%250Alikely%2520to%2520encode%2520critical%2520features%2520learned%2520during%2520pretraining%252C%2520thereby%2520reducing%250Athe%2520risk%2520of%2520catastrophic%2520forgetting%253B%2520thirdly%252C%2520it%2520permits%2520the%2520use%2520of%2520larger%250Alearning%2520rates%2520and%2520consistently%2520leads%2520to%2520better%2520generalization%2520performance%2520in%250Aexperiments.%2520We%2520demonstrate%2520this%2520for%2520both%2520NLP%2520and%2520vision%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.21374v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pay%20Attention%20to%20Small%20Weights&entry.906535625=Chao%20Zhou%20and%20Tom%20Jacobs%20and%20Advait%20Gadhikar%20and%20Rebekka%20Burkholz&entry.1292438233=%20%20Finetuning%20large%20pretrained%20neural%20networks%20is%20known%20to%20be%0Aresource-intensive%2C%20both%20in%20terms%20of%20memory%20and%20computational%20cost.%20To%20mitigate%0Athis%2C%20a%20common%20approach%20is%20to%20restrict%20training%20to%20a%20subset%20of%20the%20model%0Aparameters.%20By%20analyzing%20the%20relationship%20between%20gradients%20and%20weights%20during%0Afinetuning%2C%20we%20observe%20a%20notable%20pattern%3A%20large%20gradients%20are%20often%20associated%0Awith%20small-magnitude%20weights.%20This%20correlation%20is%20more%20pronounced%20in%20finetuning%0Asettings%20than%20in%20training%20from%20scratch.%20Motivated%20by%20this%20observation%2C%20we%0Apropose%20NANOADAM%2C%20which%20dynamically%20updates%20only%20the%20small-magnitude%20weights%0Aduring%20finetuning%20and%20offers%20several%20practical%20advantages%3A%20first%2C%20this%0Acriterion%20is%20gradient-free%20--%20the%20parameter%20subset%20can%20be%20determined%20without%0Agradient%20computation%3B%20second%2C%20it%20preserves%20large-magnitude%20weights%2C%20which%20are%0Alikely%20to%20encode%20critical%20features%20learned%20during%20pretraining%2C%20thereby%20reducing%0Athe%20risk%20of%20catastrophic%20forgetting%3B%20thirdly%2C%20it%20permits%20the%20use%20of%20larger%0Alearning%20rates%20and%20consistently%20leads%20to%20better%20generalization%20performance%20in%0Aexperiments.%20We%20demonstrate%20this%20for%20both%20NLP%20and%20vision%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.21374v2&entry.124074799=Read"},
{"title": "Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization", "author": "Yuanli Wu and Long Zhang and Yue Du and Bin Li", "abstract": "  We propose a rubric-guided, pseudo-labeled, and prompt-driven zero-shot video\nsummarization framework that bridges large language models with structured\nsemantic reasoning. A small subset of human annotations is converted into\nhigh-confidence pseudo labels and organized into dataset-adaptive rubrics\ndefining clear evaluation dimensions such as thematic relevance, action detail,\nand narrative progression. During inference, boundary scenes, including the\nopening and closing segments, are scored independently based on their own\ndescriptions, while intermediate scenes incorporate concise summaries of\nadjacent segments to assess narrative continuity and redundancy. This design\nenables the language model to balance local salience with global coherence\nwithout any parameter tuning. Across three benchmarks, the proposed method\nachieves stable and competitive results, with F1 scores of 57.58 on SumMe,\n63.05 on TVSum, and 53.79 on QFVS, surpassing zero-shot baselines by +0.85,\n+0.84, and +0.37, respectively. These outcomes demonstrate that rubric-guided\npseudo labeling combined with contextual prompting effectively stabilizes\nLLM-based scoring and establishes a general, interpretable, and training-free\nparadigm for both generic and query-focused video summarization.\n", "link": "http://arxiv.org/abs/2510.17501v3", "date": "2025-10-22", "relevancy": 2.163, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5471}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5471}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5089}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Context-Aware%20Pseudo-Label%20Scoring%20for%20Zero-Shot%20Video%20Summarization&body=Title%3A%20Context-Aware%20Pseudo-Label%20Scoring%20for%20Zero-Shot%20Video%20Summarization%0AAuthor%3A%20Yuanli%20Wu%20and%20Long%20Zhang%20and%20Yue%20Du%20and%20Bin%20Li%0AAbstract%3A%20%20%20We%20propose%20a%20rubric-guided%2C%20pseudo-labeled%2C%20and%20prompt-driven%20zero-shot%20video%0Asummarization%20framework%20that%20bridges%20large%20language%20models%20with%20structured%0Asemantic%20reasoning.%20A%20small%20subset%20of%20human%20annotations%20is%20converted%20into%0Ahigh-confidence%20pseudo%20labels%20and%20organized%20into%20dataset-adaptive%20rubrics%0Adefining%20clear%20evaluation%20dimensions%20such%20as%20thematic%20relevance%2C%20action%20detail%2C%0Aand%20narrative%20progression.%20During%20inference%2C%20boundary%20scenes%2C%20including%20the%0Aopening%20and%20closing%20segments%2C%20are%20scored%20independently%20based%20on%20their%20own%0Adescriptions%2C%20while%20intermediate%20scenes%20incorporate%20concise%20summaries%20of%0Aadjacent%20segments%20to%20assess%20narrative%20continuity%20and%20redundancy.%20This%20design%0Aenables%20the%20language%20model%20to%20balance%20local%20salience%20with%20global%20coherence%0Awithout%20any%20parameter%20tuning.%20Across%20three%20benchmarks%2C%20the%20proposed%20method%0Aachieves%20stable%20and%20competitive%20results%2C%20with%20F1%20scores%20of%2057.58%20on%20SumMe%2C%0A63.05%20on%20TVSum%2C%20and%2053.79%20on%20QFVS%2C%20surpassing%20zero-shot%20baselines%20by%20%2B0.85%2C%0A%2B0.84%2C%20and%20%2B0.37%2C%20respectively.%20These%20outcomes%20demonstrate%20that%20rubric-guided%0Apseudo%20labeling%20combined%20with%20contextual%20prompting%20effectively%20stabilizes%0ALLM-based%20scoring%20and%20establishes%20a%20general%2C%20interpretable%2C%20and%20training-free%0Aparadigm%20for%20both%20generic%20and%20query-focused%20video%20summarization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17501v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContext-Aware%2520Pseudo-Label%2520Scoring%2520for%2520Zero-Shot%2520Video%2520Summarization%26entry.906535625%3DYuanli%2520Wu%2520and%2520Long%2520Zhang%2520and%2520Yue%2520Du%2520and%2520Bin%2520Li%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520rubric-guided%252C%2520pseudo-labeled%252C%2520and%2520prompt-driven%2520zero-shot%2520video%250Asummarization%2520framework%2520that%2520bridges%2520large%2520language%2520models%2520with%2520structured%250Asemantic%2520reasoning.%2520A%2520small%2520subset%2520of%2520human%2520annotations%2520is%2520converted%2520into%250Ahigh-confidence%2520pseudo%2520labels%2520and%2520organized%2520into%2520dataset-adaptive%2520rubrics%250Adefining%2520clear%2520evaluation%2520dimensions%2520such%2520as%2520thematic%2520relevance%252C%2520action%2520detail%252C%250Aand%2520narrative%2520progression.%2520During%2520inference%252C%2520boundary%2520scenes%252C%2520including%2520the%250Aopening%2520and%2520closing%2520segments%252C%2520are%2520scored%2520independently%2520based%2520on%2520their%2520own%250Adescriptions%252C%2520while%2520intermediate%2520scenes%2520incorporate%2520concise%2520summaries%2520of%250Aadjacent%2520segments%2520to%2520assess%2520narrative%2520continuity%2520and%2520redundancy.%2520This%2520design%250Aenables%2520the%2520language%2520model%2520to%2520balance%2520local%2520salience%2520with%2520global%2520coherence%250Awithout%2520any%2520parameter%2520tuning.%2520Across%2520three%2520benchmarks%252C%2520the%2520proposed%2520method%250Aachieves%2520stable%2520and%2520competitive%2520results%252C%2520with%2520F1%2520scores%2520of%252057.58%2520on%2520SumMe%252C%250A63.05%2520on%2520TVSum%252C%2520and%252053.79%2520on%2520QFVS%252C%2520surpassing%2520zero-shot%2520baselines%2520by%2520%252B0.85%252C%250A%252B0.84%252C%2520and%2520%252B0.37%252C%2520respectively.%2520These%2520outcomes%2520demonstrate%2520that%2520rubric-guided%250Apseudo%2520labeling%2520combined%2520with%2520contextual%2520prompting%2520effectively%2520stabilizes%250ALLM-based%2520scoring%2520and%2520establishes%2520a%2520general%252C%2520interpretable%252C%2520and%2520training-free%250Aparadigm%2520for%2520both%2520generic%2520and%2520query-focused%2520video%2520summarization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17501v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context-Aware%20Pseudo-Label%20Scoring%20for%20Zero-Shot%20Video%20Summarization&entry.906535625=Yuanli%20Wu%20and%20Long%20Zhang%20and%20Yue%20Du%20and%20Bin%20Li&entry.1292438233=%20%20We%20propose%20a%20rubric-guided%2C%20pseudo-labeled%2C%20and%20prompt-driven%20zero-shot%20video%0Asummarization%20framework%20that%20bridges%20large%20language%20models%20with%20structured%0Asemantic%20reasoning.%20A%20small%20subset%20of%20human%20annotations%20is%20converted%20into%0Ahigh-confidence%20pseudo%20labels%20and%20organized%20into%20dataset-adaptive%20rubrics%0Adefining%20clear%20evaluation%20dimensions%20such%20as%20thematic%20relevance%2C%20action%20detail%2C%0Aand%20narrative%20progression.%20During%20inference%2C%20boundary%20scenes%2C%20including%20the%0Aopening%20and%20closing%20segments%2C%20are%20scored%20independently%20based%20on%20their%20own%0Adescriptions%2C%20while%20intermediate%20scenes%20incorporate%20concise%20summaries%20of%0Aadjacent%20segments%20to%20assess%20narrative%20continuity%20and%20redundancy.%20This%20design%0Aenables%20the%20language%20model%20to%20balance%20local%20salience%20with%20global%20coherence%0Awithout%20any%20parameter%20tuning.%20Across%20three%20benchmarks%2C%20the%20proposed%20method%0Aachieves%20stable%20and%20competitive%20results%2C%20with%20F1%20scores%20of%2057.58%20on%20SumMe%2C%0A63.05%20on%20TVSum%2C%20and%2053.79%20on%20QFVS%2C%20surpassing%20zero-shot%20baselines%20by%20%2B0.85%2C%0A%2B0.84%2C%20and%20%2B0.37%2C%20respectively.%20These%20outcomes%20demonstrate%20that%20rubric-guided%0Apseudo%20labeling%20combined%20with%20contextual%20prompting%20effectively%20stabilizes%0ALLM-based%20scoring%20and%20establishes%20a%20general%2C%20interpretable%2C%20and%20training-free%0Aparadigm%20for%20both%20generic%20and%20query-focused%20video%20summarization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17501v3&entry.124074799=Read"},
{"title": "Diffusion-Based Hierarchical Graph Neural Networks for Simulating\n  Nonlinear Solid Mechanics", "author": "Tobias W\u00fcrth and Niklas Freymuth and Gerhard Neumann and Luise K\u00e4rger", "abstract": "  Graph-based learned simulators have emerged as a promising approach for\nsimulating physical systems on unstructured meshes, offering speed and\ngeneralization across diverse geometries. However, they often struggle with\ncapturing global phenomena, such as bending or long-range correlations usually\noccurring in solid mechanics, and suffer from error accumulation over long\nrollouts due to their reliance on local message passing and direct next-step\nprediction. We address these limitations by introducing the Rolling\nDiffusion-Batched Inference Network (ROBIN), a novel learned simulator that\nintegrates two key innovations: (i) Rolling Diffusion-Batched Inference (ROBI),\na parallelized inference scheme that amortizes the cost of diffusion-based\nrefinement across physical time steps by overlapping denoising steps across a\ntemporal window. (ii) A Hierarchical Graph Neural Network built on algebraic\nmultigrid coarsening, enabling multiscale message passing across different mesh\nresolutions. This architecture, implemented via Algebraic-hierarchical Message\nPassing Networks, captures both fine-scale local dynamics and global structural\neffects critical for phenomena like beam bending or multi-body contact. We\nvalidate ROBIN on challenging 2D and 3D solid mechanics benchmarks involving\ngeometric, material, and contact nonlinearities. ROBIN achieves\nstate-of-the-art accuracy on all tasks, substantially outperforming existing\nnext-step learned simulators while reducing inference time by up to an order of\nmagnitude compared to standard diffusion simulators.\n", "link": "http://arxiv.org/abs/2506.06045v2", "date": "2025-10-22", "relevancy": 2.1623, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5693}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5429}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion-Based%20Hierarchical%20Graph%20Neural%20Networks%20for%20Simulating%0A%20%20Nonlinear%20Solid%20Mechanics&body=Title%3A%20Diffusion-Based%20Hierarchical%20Graph%20Neural%20Networks%20for%20Simulating%0A%20%20Nonlinear%20Solid%20Mechanics%0AAuthor%3A%20Tobias%20W%C3%BCrth%20and%20Niklas%20Freymuth%20and%20Gerhard%20Neumann%20and%20Luise%20K%C3%A4rger%0AAbstract%3A%20%20%20Graph-based%20learned%20simulators%20have%20emerged%20as%20a%20promising%20approach%20for%0Asimulating%20physical%20systems%20on%20unstructured%20meshes%2C%20offering%20speed%20and%0Ageneralization%20across%20diverse%20geometries.%20However%2C%20they%20often%20struggle%20with%0Acapturing%20global%20phenomena%2C%20such%20as%20bending%20or%20long-range%20correlations%20usually%0Aoccurring%20in%20solid%20mechanics%2C%20and%20suffer%20from%20error%20accumulation%20over%20long%0Arollouts%20due%20to%20their%20reliance%20on%20local%20message%20passing%20and%20direct%20next-step%0Aprediction.%20We%20address%20these%20limitations%20by%20introducing%20the%20Rolling%0ADiffusion-Batched%20Inference%20Network%20%28ROBIN%29%2C%20a%20novel%20learned%20simulator%20that%0Aintegrates%20two%20key%20innovations%3A%20%28i%29%20Rolling%20Diffusion-Batched%20Inference%20%28ROBI%29%2C%0Aa%20parallelized%20inference%20scheme%20that%20amortizes%20the%20cost%20of%20diffusion-based%0Arefinement%20across%20physical%20time%20steps%20by%20overlapping%20denoising%20steps%20across%20a%0Atemporal%20window.%20%28ii%29%20A%20Hierarchical%20Graph%20Neural%20Network%20built%20on%20algebraic%0Amultigrid%20coarsening%2C%20enabling%20multiscale%20message%20passing%20across%20different%20mesh%0Aresolutions.%20This%20architecture%2C%20implemented%20via%20Algebraic-hierarchical%20Message%0APassing%20Networks%2C%20captures%20both%20fine-scale%20local%20dynamics%20and%20global%20structural%0Aeffects%20critical%20for%20phenomena%20like%20beam%20bending%20or%20multi-body%20contact.%20We%0Avalidate%20ROBIN%20on%20challenging%202D%20and%203D%20solid%20mechanics%20benchmarks%20involving%0Ageometric%2C%20material%2C%20and%20contact%20nonlinearities.%20ROBIN%20achieves%0Astate-of-the-art%20accuracy%20on%20all%20tasks%2C%20substantially%20outperforming%20existing%0Anext-step%20learned%20simulators%20while%20reducing%20inference%20time%20by%20up%20to%20an%20order%20of%0Amagnitude%20compared%20to%20standard%20diffusion%20simulators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06045v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion-Based%2520Hierarchical%2520Graph%2520Neural%2520Networks%2520for%2520Simulating%250A%2520%2520Nonlinear%2520Solid%2520Mechanics%26entry.906535625%3DTobias%2520W%25C3%25BCrth%2520and%2520Niklas%2520Freymuth%2520and%2520Gerhard%2520Neumann%2520and%2520Luise%2520K%25C3%25A4rger%26entry.1292438233%3D%2520%2520Graph-based%2520learned%2520simulators%2520have%2520emerged%2520as%2520a%2520promising%2520approach%2520for%250Asimulating%2520physical%2520systems%2520on%2520unstructured%2520meshes%252C%2520offering%2520speed%2520and%250Ageneralization%2520across%2520diverse%2520geometries.%2520However%252C%2520they%2520often%2520struggle%2520with%250Acapturing%2520global%2520phenomena%252C%2520such%2520as%2520bending%2520or%2520long-range%2520correlations%2520usually%250Aoccurring%2520in%2520solid%2520mechanics%252C%2520and%2520suffer%2520from%2520error%2520accumulation%2520over%2520long%250Arollouts%2520due%2520to%2520their%2520reliance%2520on%2520local%2520message%2520passing%2520and%2520direct%2520next-step%250Aprediction.%2520We%2520address%2520these%2520limitations%2520by%2520introducing%2520the%2520Rolling%250ADiffusion-Batched%2520Inference%2520Network%2520%2528ROBIN%2529%252C%2520a%2520novel%2520learned%2520simulator%2520that%250Aintegrates%2520two%2520key%2520innovations%253A%2520%2528i%2529%2520Rolling%2520Diffusion-Batched%2520Inference%2520%2528ROBI%2529%252C%250Aa%2520parallelized%2520inference%2520scheme%2520that%2520amortizes%2520the%2520cost%2520of%2520diffusion-based%250Arefinement%2520across%2520physical%2520time%2520steps%2520by%2520overlapping%2520denoising%2520steps%2520across%2520a%250Atemporal%2520window.%2520%2528ii%2529%2520A%2520Hierarchical%2520Graph%2520Neural%2520Network%2520built%2520on%2520algebraic%250Amultigrid%2520coarsening%252C%2520enabling%2520multiscale%2520message%2520passing%2520across%2520different%2520mesh%250Aresolutions.%2520This%2520architecture%252C%2520implemented%2520via%2520Algebraic-hierarchical%2520Message%250APassing%2520Networks%252C%2520captures%2520both%2520fine-scale%2520local%2520dynamics%2520and%2520global%2520structural%250Aeffects%2520critical%2520for%2520phenomena%2520like%2520beam%2520bending%2520or%2520multi-body%2520contact.%2520We%250Avalidate%2520ROBIN%2520on%2520challenging%25202D%2520and%25203D%2520solid%2520mechanics%2520benchmarks%2520involving%250Ageometric%252C%2520material%252C%2520and%2520contact%2520nonlinearities.%2520ROBIN%2520achieves%250Astate-of-the-art%2520accuracy%2520on%2520all%2520tasks%252C%2520substantially%2520outperforming%2520existing%250Anext-step%2520learned%2520simulators%2520while%2520reducing%2520inference%2520time%2520by%2520up%2520to%2520an%2520order%2520of%250Amagnitude%2520compared%2520to%2520standard%2520diffusion%2520simulators.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06045v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion-Based%20Hierarchical%20Graph%20Neural%20Networks%20for%20Simulating%0A%20%20Nonlinear%20Solid%20Mechanics&entry.906535625=Tobias%20W%C3%BCrth%20and%20Niklas%20Freymuth%20and%20Gerhard%20Neumann%20and%20Luise%20K%C3%A4rger&entry.1292438233=%20%20Graph-based%20learned%20simulators%20have%20emerged%20as%20a%20promising%20approach%20for%0Asimulating%20physical%20systems%20on%20unstructured%20meshes%2C%20offering%20speed%20and%0Ageneralization%20across%20diverse%20geometries.%20However%2C%20they%20often%20struggle%20with%0Acapturing%20global%20phenomena%2C%20such%20as%20bending%20or%20long-range%20correlations%20usually%0Aoccurring%20in%20solid%20mechanics%2C%20and%20suffer%20from%20error%20accumulation%20over%20long%0Arollouts%20due%20to%20their%20reliance%20on%20local%20message%20passing%20and%20direct%20next-step%0Aprediction.%20We%20address%20these%20limitations%20by%20introducing%20the%20Rolling%0ADiffusion-Batched%20Inference%20Network%20%28ROBIN%29%2C%20a%20novel%20learned%20simulator%20that%0Aintegrates%20two%20key%20innovations%3A%20%28i%29%20Rolling%20Diffusion-Batched%20Inference%20%28ROBI%29%2C%0Aa%20parallelized%20inference%20scheme%20that%20amortizes%20the%20cost%20of%20diffusion-based%0Arefinement%20across%20physical%20time%20steps%20by%20overlapping%20denoising%20steps%20across%20a%0Atemporal%20window.%20%28ii%29%20A%20Hierarchical%20Graph%20Neural%20Network%20built%20on%20algebraic%0Amultigrid%20coarsening%2C%20enabling%20multiscale%20message%20passing%20across%20different%20mesh%0Aresolutions.%20This%20architecture%2C%20implemented%20via%20Algebraic-hierarchical%20Message%0APassing%20Networks%2C%20captures%20both%20fine-scale%20local%20dynamics%20and%20global%20structural%0Aeffects%20critical%20for%20phenomena%20like%20beam%20bending%20or%20multi-body%20contact.%20We%0Avalidate%20ROBIN%20on%20challenging%202D%20and%203D%20solid%20mechanics%20benchmarks%20involving%0Ageometric%2C%20material%2C%20and%20contact%20nonlinearities.%20ROBIN%20achieves%0Astate-of-the-art%20accuracy%20on%20all%20tasks%2C%20substantially%20outperforming%20existing%0Anext-step%20learned%20simulators%20while%20reducing%20inference%20time%20by%20up%20to%20an%20order%20of%0Amagnitude%20compared%20to%20standard%20diffusion%20simulators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06045v2&entry.124074799=Read"},
{"title": "dInfer: An Efficient Inference Framework for Diffusion Language Models", "author": "Yuxin Ma and Lun Du and Lanning Wei and Kun Chen and Qian Xu and Kangyu Wang and Guofeng Feng and Guoshan Lu and Lin Liu and Xiaojing Qi and Xinyuan Zhang and Zhen Tao and Haibo Feng and Ziyun Jiang and Ying Xu and Zenan Huang and Yihong Zhuang and Haokai Xu and Jiaqi Hu and Zhenzhong Lan and Junbo Zhao and Jianguo Li and Da Zheng", "abstract": "  Diffusion-based large language models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs, leveraging denoising-based generation\nto enable inherent parallelism. Even more and more open-sourced dLLM models\nemerge, yet their widespread adoption remains constrained by the lack of a\nstandardized and efficient inference framework. We present dInfer, an efficient\nand extensible framework for dLLM inference. dInfer decomposes the inference\npipeline into four modular components--model, diffusion iteration manager,\ndecoding strategy, and KV-cache manager--and integrates novel algorithms for\neach component alongside system-level optimizations. Through this combination\nof algorithmic innovations and system enhancements, dInfer achieves substantial\nefficiency gains without compromising output quality on LLaDA-MoE. At batch\nsize 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800\ntokens per second across six benchmarks on $8\\times$ H800 GPUs. Compared to\nprior systems, dInfer delivers a $10\\times$ speedup over Fast-dLLM while\nmaintaining similar model performance. Even compared to the AR model (with a\ncomparable number of activation parameters and performance) QWen2.5-3B, which\nis highly optimized with the latest vLLM inference engine, dInfer still\ndelivers a $2$-$3\\times$ speedup. The implementation of dInfer is open-sourced\nat https://github.com/inclusionAI/dInfer.\n", "link": "http://arxiv.org/abs/2510.08666v3", "date": "2025-10-22", "relevancy": 2.1359, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6012}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5205}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5205}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20dInfer%3A%20An%20Efficient%20Inference%20Framework%20for%20Diffusion%20Language%20Models&body=Title%3A%20dInfer%3A%20An%20Efficient%20Inference%20Framework%20for%20Diffusion%20Language%20Models%0AAuthor%3A%20Yuxin%20Ma%20and%20Lun%20Du%20and%20Lanning%20Wei%20and%20Kun%20Chen%20and%20Qian%20Xu%20and%20Kangyu%20Wang%20and%20Guofeng%20Feng%20and%20Guoshan%20Lu%20and%20Lin%20Liu%20and%20Xiaojing%20Qi%20and%20Xinyuan%20Zhang%20and%20Zhen%20Tao%20and%20Haibo%20Feng%20and%20Ziyun%20Jiang%20and%20Ying%20Xu%20and%20Zenan%20Huang%20and%20Yihong%20Zhuang%20and%20Haokai%20Xu%20and%20Jiaqi%20Hu%20and%20Zhenzhong%20Lan%20and%20Junbo%20Zhao%20and%20Jianguo%20Li%20and%20Da%20Zheng%0AAbstract%3A%20%20%20Diffusion-based%20large%20language%20models%20%28dLLMs%29%20have%20emerged%20as%20a%20promising%0Aalternative%20to%20autoregressive%20%28AR%29%20LLMs%2C%20leveraging%20denoising-based%20generation%0Ato%20enable%20inherent%20parallelism.%20Even%20more%20and%20more%20open-sourced%20dLLM%20models%0Aemerge%2C%20yet%20their%20widespread%20adoption%20remains%20constrained%20by%20the%20lack%20of%20a%0Astandardized%20and%20efficient%20inference%20framework.%20We%20present%20dInfer%2C%20an%20efficient%0Aand%20extensible%20framework%20for%20dLLM%20inference.%20dInfer%20decomposes%20the%20inference%0Apipeline%20into%20four%20modular%20components--model%2C%20diffusion%20iteration%20manager%2C%0Adecoding%20strategy%2C%20and%20KV-cache%20manager--and%20integrates%20novel%20algorithms%20for%0Aeach%20component%20alongside%20system-level%20optimizations.%20Through%20this%20combination%0Aof%20algorithmic%20innovations%20and%20system%20enhancements%2C%20dInfer%20achieves%20substantial%0Aefficiency%20gains%20without%20compromising%20output%20quality%20on%20LLaDA-MoE.%20At%20batch%0Asize%201%2C%20it%20surpasses%201%2C100%20tokens%20per%20second%20on%20HumanEval%20and%20averages%20over%20800%0Atokens%20per%20second%20across%20six%20benchmarks%20on%20%248%5Ctimes%24%20H800%20GPUs.%20Compared%20to%0Aprior%20systems%2C%20dInfer%20delivers%20a%20%2410%5Ctimes%24%20speedup%20over%20Fast-dLLM%20while%0Amaintaining%20similar%20model%20performance.%20Even%20compared%20to%20the%20AR%20model%20%28with%20a%0Acomparable%20number%20of%20activation%20parameters%20and%20performance%29%20QWen2.5-3B%2C%20which%0Ais%20highly%20optimized%20with%20the%20latest%20vLLM%20inference%20engine%2C%20dInfer%20still%0Adelivers%20a%20%242%24-%243%5Ctimes%24%20speedup.%20The%20implementation%20of%20dInfer%20is%20open-sourced%0Aat%20https%3A//github.com/inclusionAI/dInfer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.08666v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DdInfer%253A%2520An%2520Efficient%2520Inference%2520Framework%2520for%2520Diffusion%2520Language%2520Models%26entry.906535625%3DYuxin%2520Ma%2520and%2520Lun%2520Du%2520and%2520Lanning%2520Wei%2520and%2520Kun%2520Chen%2520and%2520Qian%2520Xu%2520and%2520Kangyu%2520Wang%2520and%2520Guofeng%2520Feng%2520and%2520Guoshan%2520Lu%2520and%2520Lin%2520Liu%2520and%2520Xiaojing%2520Qi%2520and%2520Xinyuan%2520Zhang%2520and%2520Zhen%2520Tao%2520and%2520Haibo%2520Feng%2520and%2520Ziyun%2520Jiang%2520and%2520Ying%2520Xu%2520and%2520Zenan%2520Huang%2520and%2520Yihong%2520Zhuang%2520and%2520Haokai%2520Xu%2520and%2520Jiaqi%2520Hu%2520and%2520Zhenzhong%2520Lan%2520and%2520Junbo%2520Zhao%2520and%2520Jianguo%2520Li%2520and%2520Da%2520Zheng%26entry.1292438233%3D%2520%2520Diffusion-based%2520large%2520language%2520models%2520%2528dLLMs%2529%2520have%2520emerged%2520as%2520a%2520promising%250Aalternative%2520to%2520autoregressive%2520%2528AR%2529%2520LLMs%252C%2520leveraging%2520denoising-based%2520generation%250Ato%2520enable%2520inherent%2520parallelism.%2520Even%2520more%2520and%2520more%2520open-sourced%2520dLLM%2520models%250Aemerge%252C%2520yet%2520their%2520widespread%2520adoption%2520remains%2520constrained%2520by%2520the%2520lack%2520of%2520a%250Astandardized%2520and%2520efficient%2520inference%2520framework.%2520We%2520present%2520dInfer%252C%2520an%2520efficient%250Aand%2520extensible%2520framework%2520for%2520dLLM%2520inference.%2520dInfer%2520decomposes%2520the%2520inference%250Apipeline%2520into%2520four%2520modular%2520components--model%252C%2520diffusion%2520iteration%2520manager%252C%250Adecoding%2520strategy%252C%2520and%2520KV-cache%2520manager--and%2520integrates%2520novel%2520algorithms%2520for%250Aeach%2520component%2520alongside%2520system-level%2520optimizations.%2520Through%2520this%2520combination%250Aof%2520algorithmic%2520innovations%2520and%2520system%2520enhancements%252C%2520dInfer%2520achieves%2520substantial%250Aefficiency%2520gains%2520without%2520compromising%2520output%2520quality%2520on%2520LLaDA-MoE.%2520At%2520batch%250Asize%25201%252C%2520it%2520surpasses%25201%252C100%2520tokens%2520per%2520second%2520on%2520HumanEval%2520and%2520averages%2520over%2520800%250Atokens%2520per%2520second%2520across%2520six%2520benchmarks%2520on%2520%25248%255Ctimes%2524%2520H800%2520GPUs.%2520Compared%2520to%250Aprior%2520systems%252C%2520dInfer%2520delivers%2520a%2520%252410%255Ctimes%2524%2520speedup%2520over%2520Fast-dLLM%2520while%250Amaintaining%2520similar%2520model%2520performance.%2520Even%2520compared%2520to%2520the%2520AR%2520model%2520%2528with%2520a%250Acomparable%2520number%2520of%2520activation%2520parameters%2520and%2520performance%2529%2520QWen2.5-3B%252C%2520which%250Ais%2520highly%2520optimized%2520with%2520the%2520latest%2520vLLM%2520inference%2520engine%252C%2520dInfer%2520still%250Adelivers%2520a%2520%25242%2524-%25243%255Ctimes%2524%2520speedup.%2520The%2520implementation%2520of%2520dInfer%2520is%2520open-sourced%250Aat%2520https%253A//github.com/inclusionAI/dInfer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.08666v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=dInfer%3A%20An%20Efficient%20Inference%20Framework%20for%20Diffusion%20Language%20Models&entry.906535625=Yuxin%20Ma%20and%20Lun%20Du%20and%20Lanning%20Wei%20and%20Kun%20Chen%20and%20Qian%20Xu%20and%20Kangyu%20Wang%20and%20Guofeng%20Feng%20and%20Guoshan%20Lu%20and%20Lin%20Liu%20and%20Xiaojing%20Qi%20and%20Xinyuan%20Zhang%20and%20Zhen%20Tao%20and%20Haibo%20Feng%20and%20Ziyun%20Jiang%20and%20Ying%20Xu%20and%20Zenan%20Huang%20and%20Yihong%20Zhuang%20and%20Haokai%20Xu%20and%20Jiaqi%20Hu%20and%20Zhenzhong%20Lan%20and%20Junbo%20Zhao%20and%20Jianguo%20Li%20and%20Da%20Zheng&entry.1292438233=%20%20Diffusion-based%20large%20language%20models%20%28dLLMs%29%20have%20emerged%20as%20a%20promising%0Aalternative%20to%20autoregressive%20%28AR%29%20LLMs%2C%20leveraging%20denoising-based%20generation%0Ato%20enable%20inherent%20parallelism.%20Even%20more%20and%20more%20open-sourced%20dLLM%20models%0Aemerge%2C%20yet%20their%20widespread%20adoption%20remains%20constrained%20by%20the%20lack%20of%20a%0Astandardized%20and%20efficient%20inference%20framework.%20We%20present%20dInfer%2C%20an%20efficient%0Aand%20extensible%20framework%20for%20dLLM%20inference.%20dInfer%20decomposes%20the%20inference%0Apipeline%20into%20four%20modular%20components--model%2C%20diffusion%20iteration%20manager%2C%0Adecoding%20strategy%2C%20and%20KV-cache%20manager--and%20integrates%20novel%20algorithms%20for%0Aeach%20component%20alongside%20system-level%20optimizations.%20Through%20this%20combination%0Aof%20algorithmic%20innovations%20and%20system%20enhancements%2C%20dInfer%20achieves%20substantial%0Aefficiency%20gains%20without%20compromising%20output%20quality%20on%20LLaDA-MoE.%20At%20batch%0Asize%201%2C%20it%20surpasses%201%2C100%20tokens%20per%20second%20on%20HumanEval%20and%20averages%20over%20800%0Atokens%20per%20second%20across%20six%20benchmarks%20on%20%248%5Ctimes%24%20H800%20GPUs.%20Compared%20to%0Aprior%20systems%2C%20dInfer%20delivers%20a%20%2410%5Ctimes%24%20speedup%20over%20Fast-dLLM%20while%0Amaintaining%20similar%20model%20performance.%20Even%20compared%20to%20the%20AR%20model%20%28with%20a%0Acomparable%20number%20of%20activation%20parameters%20and%20performance%29%20QWen2.5-3B%2C%20which%0Ais%20highly%20optimized%20with%20the%20latest%20vLLM%20inference%20engine%2C%20dInfer%20still%0Adelivers%20a%20%242%24-%243%5Ctimes%24%20speedup.%20The%20implementation%20of%20dInfer%20is%20open-sourced%0Aat%20https%3A//github.com/inclusionAI/dInfer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.08666v3&entry.124074799=Read"},
{"title": "gLSTM: Mitigating Over-Squashing by Increasing Storage Capacity", "author": "Hugh Blayney and \u00c1lvaro Arroyo and Xiaowen Dong and Michael M. Bronstein", "abstract": "  Graph Neural Networks (GNNs) leverage the graph structure to transmit\ninformation between nodes, typically through the message-passing mechanism.\nWhile these models have found a wide variety of applications, they are known to\nsuffer from over-squashing, where information from a large receptive field of\nnode representations is collapsed into a single fixed sized vector, resulting\nin an information bottleneck. In this paper, we re-examine the over-squashing\nphenomenon through the lens of model storage and retrieval capacity, which we\ndefine as the amount of information that can be stored in a node's\nrepresentation for later use. We study some of the limitations of existing\ntasks used to measure over-squashing and introduce a new synthetic task to\ndemonstrate that an information bottleneck can saturate this capacity.\nFurthermore, we adapt ideas from the sequence modeling literature on\nassociative memories, fast weight programmers, and the xLSTM model to develop a\nnovel GNN architecture with improved capacity. We demonstrate strong\nperformance of this architecture both on our capacity synthetic task, as well\nas a range of real-world graph benchmarks.\n", "link": "http://arxiv.org/abs/2510.08450v2", "date": "2025-10-22", "relevancy": 2.132, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5511}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5203}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5196}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20gLSTM%3A%20Mitigating%20Over-Squashing%20by%20Increasing%20Storage%20Capacity&body=Title%3A%20gLSTM%3A%20Mitigating%20Over-Squashing%20by%20Increasing%20Storage%20Capacity%0AAuthor%3A%20Hugh%20Blayney%20and%20%C3%81lvaro%20Arroyo%20and%20Xiaowen%20Dong%20and%20Michael%20M.%20Bronstein%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20leverage%20the%20graph%20structure%20to%20transmit%0Ainformation%20between%20nodes%2C%20typically%20through%20the%20message-passing%20mechanism.%0AWhile%20these%20models%20have%20found%20a%20wide%20variety%20of%20applications%2C%20they%20are%20known%20to%0Asuffer%20from%20over-squashing%2C%20where%20information%20from%20a%20large%20receptive%20field%20of%0Anode%20representations%20is%20collapsed%20into%20a%20single%20fixed%20sized%20vector%2C%20resulting%0Ain%20an%20information%20bottleneck.%20In%20this%20paper%2C%20we%20re-examine%20the%20over-squashing%0Aphenomenon%20through%20the%20lens%20of%20model%20storage%20and%20retrieval%20capacity%2C%20which%20we%0Adefine%20as%20the%20amount%20of%20information%20that%20can%20be%20stored%20in%20a%20node%27s%0Arepresentation%20for%20later%20use.%20We%20study%20some%20of%20the%20limitations%20of%20existing%0Atasks%20used%20to%20measure%20over-squashing%20and%20introduce%20a%20new%20synthetic%20task%20to%0Ademonstrate%20that%20an%20information%20bottleneck%20can%20saturate%20this%20capacity.%0AFurthermore%2C%20we%20adapt%20ideas%20from%20the%20sequence%20modeling%20literature%20on%0Aassociative%20memories%2C%20fast%20weight%20programmers%2C%20and%20the%20xLSTM%20model%20to%20develop%20a%0Anovel%20GNN%20architecture%20with%20improved%20capacity.%20We%20demonstrate%20strong%0Aperformance%20of%20this%20architecture%20both%20on%20our%20capacity%20synthetic%20task%2C%20as%20well%0Aas%20a%20range%20of%20real-world%20graph%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.08450v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DgLSTM%253A%2520Mitigating%2520Over-Squashing%2520by%2520Increasing%2520Storage%2520Capacity%26entry.906535625%3DHugh%2520Blayney%2520and%2520%25C3%2581lvaro%2520Arroyo%2520and%2520Xiaowen%2520Dong%2520and%2520Michael%2520M.%2520Bronstein%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520leverage%2520the%2520graph%2520structure%2520to%2520transmit%250Ainformation%2520between%2520nodes%252C%2520typically%2520through%2520the%2520message-passing%2520mechanism.%250AWhile%2520these%2520models%2520have%2520found%2520a%2520wide%2520variety%2520of%2520applications%252C%2520they%2520are%2520known%2520to%250Asuffer%2520from%2520over-squashing%252C%2520where%2520information%2520from%2520a%2520large%2520receptive%2520field%2520of%250Anode%2520representations%2520is%2520collapsed%2520into%2520a%2520single%2520fixed%2520sized%2520vector%252C%2520resulting%250Ain%2520an%2520information%2520bottleneck.%2520In%2520this%2520paper%252C%2520we%2520re-examine%2520the%2520over-squashing%250Aphenomenon%2520through%2520the%2520lens%2520of%2520model%2520storage%2520and%2520retrieval%2520capacity%252C%2520which%2520we%250Adefine%2520as%2520the%2520amount%2520of%2520information%2520that%2520can%2520be%2520stored%2520in%2520a%2520node%2527s%250Arepresentation%2520for%2520later%2520use.%2520We%2520study%2520some%2520of%2520the%2520limitations%2520of%2520existing%250Atasks%2520used%2520to%2520measure%2520over-squashing%2520and%2520introduce%2520a%2520new%2520synthetic%2520task%2520to%250Ademonstrate%2520that%2520an%2520information%2520bottleneck%2520can%2520saturate%2520this%2520capacity.%250AFurthermore%252C%2520we%2520adapt%2520ideas%2520from%2520the%2520sequence%2520modeling%2520literature%2520on%250Aassociative%2520memories%252C%2520fast%2520weight%2520programmers%252C%2520and%2520the%2520xLSTM%2520model%2520to%2520develop%2520a%250Anovel%2520GNN%2520architecture%2520with%2520improved%2520capacity.%2520We%2520demonstrate%2520strong%250Aperformance%2520of%2520this%2520architecture%2520both%2520on%2520our%2520capacity%2520synthetic%2520task%252C%2520as%2520well%250Aas%2520a%2520range%2520of%2520real-world%2520graph%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.08450v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=gLSTM%3A%20Mitigating%20Over-Squashing%20by%20Increasing%20Storage%20Capacity&entry.906535625=Hugh%20Blayney%20and%20%C3%81lvaro%20Arroyo%20and%20Xiaowen%20Dong%20and%20Michael%20M.%20Bronstein&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20leverage%20the%20graph%20structure%20to%20transmit%0Ainformation%20between%20nodes%2C%20typically%20through%20the%20message-passing%20mechanism.%0AWhile%20these%20models%20have%20found%20a%20wide%20variety%20of%20applications%2C%20they%20are%20known%20to%0Asuffer%20from%20over-squashing%2C%20where%20information%20from%20a%20large%20receptive%20field%20of%0Anode%20representations%20is%20collapsed%20into%20a%20single%20fixed%20sized%20vector%2C%20resulting%0Ain%20an%20information%20bottleneck.%20In%20this%20paper%2C%20we%20re-examine%20the%20over-squashing%0Aphenomenon%20through%20the%20lens%20of%20model%20storage%20and%20retrieval%20capacity%2C%20which%20we%0Adefine%20as%20the%20amount%20of%20information%20that%20can%20be%20stored%20in%20a%20node%27s%0Arepresentation%20for%20later%20use.%20We%20study%20some%20of%20the%20limitations%20of%20existing%0Atasks%20used%20to%20measure%20over-squashing%20and%20introduce%20a%20new%20synthetic%20task%20to%0Ademonstrate%20that%20an%20information%20bottleneck%20can%20saturate%20this%20capacity.%0AFurthermore%2C%20we%20adapt%20ideas%20from%20the%20sequence%20modeling%20literature%20on%0Aassociative%20memories%2C%20fast%20weight%20programmers%2C%20and%20the%20xLSTM%20model%20to%20develop%20a%0Anovel%20GNN%20architecture%20with%20improved%20capacity.%20We%20demonstrate%20strong%0Aperformance%20of%20this%20architecture%20both%20on%20our%20capacity%20synthetic%20task%2C%20as%20well%0Aas%20a%20range%20of%20real-world%20graph%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.08450v2&entry.124074799=Read"},
{"title": "Beyond sparse denoising in frames: minimax estimation with a scattering\n  transform", "author": "Nathana\u00ebl Cuvelle--Magar and St\u00e9phane Mallat", "abstract": "  A considerable amount of research in harmonic analysis has been devoted to\nnon-linear estimators of signals contaminated by additive Gaussian noise. They\nare implemented by thresholding coefficients in a frame, which provide a sparse\nsignal representation, or by minimising their $\\ell^1$ norm. However, sparse\nestimators in frames are not sufficiently rich to adapt to complex signal\nregularities. For cartoon images whose edges are piecewise $\\bf C^\\alpha$\ncurves, wavelet, curvelet and Xlet frames are suboptimal if the Lipschitz\nexponent $\\alpha \\leq 2$ is an unknown parameter. Deep convolutional neural\nnetworks have recently obtained much better numerical results, which reach the\nminimax asymptotic bounds for all $\\alpha$. Wavelet scattering coefficients\nhave been introduced as simplified convolutional neural network models. They\nare computed by transforming the modulus of wavelet coefficients with a second\nwavelet transform. We introduce a denoising estimator by jointly minimising and\nmaximising the $\\ell^1$ norms of different subsets of scattering coefficients.\nWe prove that these $\\ell^1$ norms capture different types of geometric image\nregularity. Numerical experiments show that this denoising estimator reaches\nthe minimax asymptotic bound for cartoon images for all Lipschitz exponents\n$\\alpha \\leq 2$. We state this numerical result as a mathematical conjecture.\nIt provides a different harmonic analysis approach to suppress noise from\nsignals, and to specify the geometric regularity of functions. It also opens a\nmathematical bridge between harmonic analysis and denoising estimators with\ndeep convolutional network.\n", "link": "http://arxiv.org/abs/2510.19612v1", "date": "2025-10-22", "relevancy": 2.1156, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5423}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5214}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20sparse%20denoising%20in%20frames%3A%20minimax%20estimation%20with%20a%20scattering%0A%20%20transform&body=Title%3A%20Beyond%20sparse%20denoising%20in%20frames%3A%20minimax%20estimation%20with%20a%20scattering%0A%20%20transform%0AAuthor%3A%20Nathana%C3%ABl%20Cuvelle--Magar%20and%20St%C3%A9phane%20Mallat%0AAbstract%3A%20%20%20A%20considerable%20amount%20of%20research%20in%20harmonic%20analysis%20has%20been%20devoted%20to%0Anon-linear%20estimators%20of%20signals%20contaminated%20by%20additive%20Gaussian%20noise.%20They%0Aare%20implemented%20by%20thresholding%20coefficients%20in%20a%20frame%2C%20which%20provide%20a%20sparse%0Asignal%20representation%2C%20or%20by%20minimising%20their%20%24%5Cell%5E1%24%20norm.%20However%2C%20sparse%0Aestimators%20in%20frames%20are%20not%20sufficiently%20rich%20to%20adapt%20to%20complex%20signal%0Aregularities.%20For%20cartoon%20images%20whose%20edges%20are%20piecewise%20%24%5Cbf%20C%5E%5Calpha%24%0Acurves%2C%20wavelet%2C%20curvelet%20and%20Xlet%20frames%20are%20suboptimal%20if%20the%20Lipschitz%0Aexponent%20%24%5Calpha%20%5Cleq%202%24%20is%20an%20unknown%20parameter.%20Deep%20convolutional%20neural%0Anetworks%20have%20recently%20obtained%20much%20better%20numerical%20results%2C%20which%20reach%20the%0Aminimax%20asymptotic%20bounds%20for%20all%20%24%5Calpha%24.%20Wavelet%20scattering%20coefficients%0Ahave%20been%20introduced%20as%20simplified%20convolutional%20neural%20network%20models.%20They%0Aare%20computed%20by%20transforming%20the%20modulus%20of%20wavelet%20coefficients%20with%20a%20second%0Awavelet%20transform.%20We%20introduce%20a%20denoising%20estimator%20by%20jointly%20minimising%20and%0Amaximising%20the%20%24%5Cell%5E1%24%20norms%20of%20different%20subsets%20of%20scattering%20coefficients.%0AWe%20prove%20that%20these%20%24%5Cell%5E1%24%20norms%20capture%20different%20types%20of%20geometric%20image%0Aregularity.%20Numerical%20experiments%20show%20that%20this%20denoising%20estimator%20reaches%0Athe%20minimax%20asymptotic%20bound%20for%20cartoon%20images%20for%20all%20Lipschitz%20exponents%0A%24%5Calpha%20%5Cleq%202%24.%20We%20state%20this%20numerical%20result%20as%20a%20mathematical%20conjecture.%0AIt%20provides%20a%20different%20harmonic%20analysis%20approach%20to%20suppress%20noise%20from%0Asignals%2C%20and%20to%20specify%20the%20geometric%20regularity%20of%20functions.%20It%20also%20opens%20a%0Amathematical%20bridge%20between%20harmonic%20analysis%20and%20denoising%20estimators%20with%0Adeep%20convolutional%20network.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19612v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520sparse%2520denoising%2520in%2520frames%253A%2520minimax%2520estimation%2520with%2520a%2520scattering%250A%2520%2520transform%26entry.906535625%3DNathana%25C3%25ABl%2520Cuvelle--Magar%2520and%2520St%25C3%25A9phane%2520Mallat%26entry.1292438233%3D%2520%2520A%2520considerable%2520amount%2520of%2520research%2520in%2520harmonic%2520analysis%2520has%2520been%2520devoted%2520to%250Anon-linear%2520estimators%2520of%2520signals%2520contaminated%2520by%2520additive%2520Gaussian%2520noise.%2520They%250Aare%2520implemented%2520by%2520thresholding%2520coefficients%2520in%2520a%2520frame%252C%2520which%2520provide%2520a%2520sparse%250Asignal%2520representation%252C%2520or%2520by%2520minimising%2520their%2520%2524%255Cell%255E1%2524%2520norm.%2520However%252C%2520sparse%250Aestimators%2520in%2520frames%2520are%2520not%2520sufficiently%2520rich%2520to%2520adapt%2520to%2520complex%2520signal%250Aregularities.%2520For%2520cartoon%2520images%2520whose%2520edges%2520are%2520piecewise%2520%2524%255Cbf%2520C%255E%255Calpha%2524%250Acurves%252C%2520wavelet%252C%2520curvelet%2520and%2520Xlet%2520frames%2520are%2520suboptimal%2520if%2520the%2520Lipschitz%250Aexponent%2520%2524%255Calpha%2520%255Cleq%25202%2524%2520is%2520an%2520unknown%2520parameter.%2520Deep%2520convolutional%2520neural%250Anetworks%2520have%2520recently%2520obtained%2520much%2520better%2520numerical%2520results%252C%2520which%2520reach%2520the%250Aminimax%2520asymptotic%2520bounds%2520for%2520all%2520%2524%255Calpha%2524.%2520Wavelet%2520scattering%2520coefficients%250Ahave%2520been%2520introduced%2520as%2520simplified%2520convolutional%2520neural%2520network%2520models.%2520They%250Aare%2520computed%2520by%2520transforming%2520the%2520modulus%2520of%2520wavelet%2520coefficients%2520with%2520a%2520second%250Awavelet%2520transform.%2520We%2520introduce%2520a%2520denoising%2520estimator%2520by%2520jointly%2520minimising%2520and%250Amaximising%2520the%2520%2524%255Cell%255E1%2524%2520norms%2520of%2520different%2520subsets%2520of%2520scattering%2520coefficients.%250AWe%2520prove%2520that%2520these%2520%2524%255Cell%255E1%2524%2520norms%2520capture%2520different%2520types%2520of%2520geometric%2520image%250Aregularity.%2520Numerical%2520experiments%2520show%2520that%2520this%2520denoising%2520estimator%2520reaches%250Athe%2520minimax%2520asymptotic%2520bound%2520for%2520cartoon%2520images%2520for%2520all%2520Lipschitz%2520exponents%250A%2524%255Calpha%2520%255Cleq%25202%2524.%2520We%2520state%2520this%2520numerical%2520result%2520as%2520a%2520mathematical%2520conjecture.%250AIt%2520provides%2520a%2520different%2520harmonic%2520analysis%2520approach%2520to%2520suppress%2520noise%2520from%250Asignals%252C%2520and%2520to%2520specify%2520the%2520geometric%2520regularity%2520of%2520functions.%2520It%2520also%2520opens%2520a%250Amathematical%2520bridge%2520between%2520harmonic%2520analysis%2520and%2520denoising%2520estimators%2520with%250Adeep%2520convolutional%2520network.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19612v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20sparse%20denoising%20in%20frames%3A%20minimax%20estimation%20with%20a%20scattering%0A%20%20transform&entry.906535625=Nathana%C3%ABl%20Cuvelle--Magar%20and%20St%C3%A9phane%20Mallat&entry.1292438233=%20%20A%20considerable%20amount%20of%20research%20in%20harmonic%20analysis%20has%20been%20devoted%20to%0Anon-linear%20estimators%20of%20signals%20contaminated%20by%20additive%20Gaussian%20noise.%20They%0Aare%20implemented%20by%20thresholding%20coefficients%20in%20a%20frame%2C%20which%20provide%20a%20sparse%0Asignal%20representation%2C%20or%20by%20minimising%20their%20%24%5Cell%5E1%24%20norm.%20However%2C%20sparse%0Aestimators%20in%20frames%20are%20not%20sufficiently%20rich%20to%20adapt%20to%20complex%20signal%0Aregularities.%20For%20cartoon%20images%20whose%20edges%20are%20piecewise%20%24%5Cbf%20C%5E%5Calpha%24%0Acurves%2C%20wavelet%2C%20curvelet%20and%20Xlet%20frames%20are%20suboptimal%20if%20the%20Lipschitz%0Aexponent%20%24%5Calpha%20%5Cleq%202%24%20is%20an%20unknown%20parameter.%20Deep%20convolutional%20neural%0Anetworks%20have%20recently%20obtained%20much%20better%20numerical%20results%2C%20which%20reach%20the%0Aminimax%20asymptotic%20bounds%20for%20all%20%24%5Calpha%24.%20Wavelet%20scattering%20coefficients%0Ahave%20been%20introduced%20as%20simplified%20convolutional%20neural%20network%20models.%20They%0Aare%20computed%20by%20transforming%20the%20modulus%20of%20wavelet%20coefficients%20with%20a%20second%0Awavelet%20transform.%20We%20introduce%20a%20denoising%20estimator%20by%20jointly%20minimising%20and%0Amaximising%20the%20%24%5Cell%5E1%24%20norms%20of%20different%20subsets%20of%20scattering%20coefficients.%0AWe%20prove%20that%20these%20%24%5Cell%5E1%24%20norms%20capture%20different%20types%20of%20geometric%20image%0Aregularity.%20Numerical%20experiments%20show%20that%20this%20denoising%20estimator%20reaches%0Athe%20minimax%20asymptotic%20bound%20for%20cartoon%20images%20for%20all%20Lipschitz%20exponents%0A%24%5Calpha%20%5Cleq%202%24.%20We%20state%20this%20numerical%20result%20as%20a%20mathematical%20conjecture.%0AIt%20provides%20a%20different%20harmonic%20analysis%20approach%20to%20suppress%20noise%20from%0Asignals%2C%20and%20to%20specify%20the%20geometric%20regularity%20of%20functions.%20It%20also%20opens%20a%0Amathematical%20bridge%20between%20harmonic%20analysis%20and%20denoising%20estimators%20with%0Adeep%20convolutional%20network.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19612v1&entry.124074799=Read"},
{"title": "AdaSPEC: Selective Knowledge Distillation for Efficient Speculative\n  Decoders", "author": "Yuezhou Hu and Jiaxin Guo and Xinyu Feng and Tuo Zhao", "abstract": "  Speculative Decoding (SD) accelerates large language model inference by\nemploying a small draft model to generate predictions, which are then verified\nby a larger target model. The effectiveness of SD hinges on the alignment\nbetween these models, which is typically enhanced by Knowledge Distillation\n(KD). However, conventional KD methods aim to minimize the KL divergence\nbetween the draft and target models across all tokens, a goal that is\nmisaligned with the true objective of SD, which is to maximize token acceptance\nrate. Therefore, draft models often struggle to fully assimilate the target\nmodel's knowledge due to capacity constraints, leading to suboptimal\nperformance. To address this challenge, we propose AdaSPEC, a novel method that\nincorporates selective token filtering into the KD process. AdaSPEC utilizes a\nreference model to identify and filter out difficult-to-fit tokens, enabling\nthe distillation of a draft model that better aligns with the target model on\nsimpler tokens. This approach improves the overall token acceptance rate\nwithout compromising generation quality. We evaluate AdaSPEC across diverse\ntasks, including arithmetic reasoning, instruction-following, coding, and\nsummarization, using model configurations of 31M/1.4B and 350M/2.7B parameters.\nOur results demonstrate that AdaSPEC consistently outperforms the\nstate-of-the-art DistillSpec method, achieving higher acceptance rates across\nall tasks (up to 15\\%). The code is publicly available at\nhttps://github.com/yuezhouhu/adaspec.\n", "link": "http://arxiv.org/abs/2510.19779v1", "date": "2025-10-22", "relevancy": 2.1018, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5544}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.544}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaSPEC%3A%20Selective%20Knowledge%20Distillation%20for%20Efficient%20Speculative%0A%20%20Decoders&body=Title%3A%20AdaSPEC%3A%20Selective%20Knowledge%20Distillation%20for%20Efficient%20Speculative%0A%20%20Decoders%0AAuthor%3A%20Yuezhou%20Hu%20and%20Jiaxin%20Guo%20and%20Xinyu%20Feng%20and%20Tuo%20Zhao%0AAbstract%3A%20%20%20Speculative%20Decoding%20%28SD%29%20accelerates%20large%20language%20model%20inference%20by%0Aemploying%20a%20small%20draft%20model%20to%20generate%20predictions%2C%20which%20are%20then%20verified%0Aby%20a%20larger%20target%20model.%20The%20effectiveness%20of%20SD%20hinges%20on%20the%20alignment%0Abetween%20these%20models%2C%20which%20is%20typically%20enhanced%20by%20Knowledge%20Distillation%0A%28KD%29.%20However%2C%20conventional%20KD%20methods%20aim%20to%20minimize%20the%20KL%20divergence%0Abetween%20the%20draft%20and%20target%20models%20across%20all%20tokens%2C%20a%20goal%20that%20is%0Amisaligned%20with%20the%20true%20objective%20of%20SD%2C%20which%20is%20to%20maximize%20token%20acceptance%0Arate.%20Therefore%2C%20draft%20models%20often%20struggle%20to%20fully%20assimilate%20the%20target%0Amodel%27s%20knowledge%20due%20to%20capacity%20constraints%2C%20leading%20to%20suboptimal%0Aperformance.%20To%20address%20this%20challenge%2C%20we%20propose%20AdaSPEC%2C%20a%20novel%20method%20that%0Aincorporates%20selective%20token%20filtering%20into%20the%20KD%20process.%20AdaSPEC%20utilizes%20a%0Areference%20model%20to%20identify%20and%20filter%20out%20difficult-to-fit%20tokens%2C%20enabling%0Athe%20distillation%20of%20a%20draft%20model%20that%20better%20aligns%20with%20the%20target%20model%20on%0Asimpler%20tokens.%20This%20approach%20improves%20the%20overall%20token%20acceptance%20rate%0Awithout%20compromising%20generation%20quality.%20We%20evaluate%20AdaSPEC%20across%20diverse%0Atasks%2C%20including%20arithmetic%20reasoning%2C%20instruction-following%2C%20coding%2C%20and%0Asummarization%2C%20using%20model%20configurations%20of%2031M/1.4B%20and%20350M/2.7B%20parameters.%0AOur%20results%20demonstrate%20that%20AdaSPEC%20consistently%20outperforms%20the%0Astate-of-the-art%20DistillSpec%20method%2C%20achieving%20higher%20acceptance%20rates%20across%0Aall%20tasks%20%28up%20to%2015%5C%25%29.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/yuezhouhu/adaspec.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19779v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaSPEC%253A%2520Selective%2520Knowledge%2520Distillation%2520for%2520Efficient%2520Speculative%250A%2520%2520Decoders%26entry.906535625%3DYuezhou%2520Hu%2520and%2520Jiaxin%2520Guo%2520and%2520Xinyu%2520Feng%2520and%2520Tuo%2520Zhao%26entry.1292438233%3D%2520%2520Speculative%2520Decoding%2520%2528SD%2529%2520accelerates%2520large%2520language%2520model%2520inference%2520by%250Aemploying%2520a%2520small%2520draft%2520model%2520to%2520generate%2520predictions%252C%2520which%2520are%2520then%2520verified%250Aby%2520a%2520larger%2520target%2520model.%2520The%2520effectiveness%2520of%2520SD%2520hinges%2520on%2520the%2520alignment%250Abetween%2520these%2520models%252C%2520which%2520is%2520typically%2520enhanced%2520by%2520Knowledge%2520Distillation%250A%2528KD%2529.%2520However%252C%2520conventional%2520KD%2520methods%2520aim%2520to%2520minimize%2520the%2520KL%2520divergence%250Abetween%2520the%2520draft%2520and%2520target%2520models%2520across%2520all%2520tokens%252C%2520a%2520goal%2520that%2520is%250Amisaligned%2520with%2520the%2520true%2520objective%2520of%2520SD%252C%2520which%2520is%2520to%2520maximize%2520token%2520acceptance%250Arate.%2520Therefore%252C%2520draft%2520models%2520often%2520struggle%2520to%2520fully%2520assimilate%2520the%2520target%250Amodel%2527s%2520knowledge%2520due%2520to%2520capacity%2520constraints%252C%2520leading%2520to%2520suboptimal%250Aperformance.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520AdaSPEC%252C%2520a%2520novel%2520method%2520that%250Aincorporates%2520selective%2520token%2520filtering%2520into%2520the%2520KD%2520process.%2520AdaSPEC%2520utilizes%2520a%250Areference%2520model%2520to%2520identify%2520and%2520filter%2520out%2520difficult-to-fit%2520tokens%252C%2520enabling%250Athe%2520distillation%2520of%2520a%2520draft%2520model%2520that%2520better%2520aligns%2520with%2520the%2520target%2520model%2520on%250Asimpler%2520tokens.%2520This%2520approach%2520improves%2520the%2520overall%2520token%2520acceptance%2520rate%250Awithout%2520compromising%2520generation%2520quality.%2520We%2520evaluate%2520AdaSPEC%2520across%2520diverse%250Atasks%252C%2520including%2520arithmetic%2520reasoning%252C%2520instruction-following%252C%2520coding%252C%2520and%250Asummarization%252C%2520using%2520model%2520configurations%2520of%252031M/1.4B%2520and%2520350M/2.7B%2520parameters.%250AOur%2520results%2520demonstrate%2520that%2520AdaSPEC%2520consistently%2520outperforms%2520the%250Astate-of-the-art%2520DistillSpec%2520method%252C%2520achieving%2520higher%2520acceptance%2520rates%2520across%250Aall%2520tasks%2520%2528up%2520to%252015%255C%2525%2529.%2520The%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/yuezhouhu/adaspec.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19779v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaSPEC%3A%20Selective%20Knowledge%20Distillation%20for%20Efficient%20Speculative%0A%20%20Decoders&entry.906535625=Yuezhou%20Hu%20and%20Jiaxin%20Guo%20and%20Xinyu%20Feng%20and%20Tuo%20Zhao&entry.1292438233=%20%20Speculative%20Decoding%20%28SD%29%20accelerates%20large%20language%20model%20inference%20by%0Aemploying%20a%20small%20draft%20model%20to%20generate%20predictions%2C%20which%20are%20then%20verified%0Aby%20a%20larger%20target%20model.%20The%20effectiveness%20of%20SD%20hinges%20on%20the%20alignment%0Abetween%20these%20models%2C%20which%20is%20typically%20enhanced%20by%20Knowledge%20Distillation%0A%28KD%29.%20However%2C%20conventional%20KD%20methods%20aim%20to%20minimize%20the%20KL%20divergence%0Abetween%20the%20draft%20and%20target%20models%20across%20all%20tokens%2C%20a%20goal%20that%20is%0Amisaligned%20with%20the%20true%20objective%20of%20SD%2C%20which%20is%20to%20maximize%20token%20acceptance%0Arate.%20Therefore%2C%20draft%20models%20often%20struggle%20to%20fully%20assimilate%20the%20target%0Amodel%27s%20knowledge%20due%20to%20capacity%20constraints%2C%20leading%20to%20suboptimal%0Aperformance.%20To%20address%20this%20challenge%2C%20we%20propose%20AdaSPEC%2C%20a%20novel%20method%20that%0Aincorporates%20selective%20token%20filtering%20into%20the%20KD%20process.%20AdaSPEC%20utilizes%20a%0Areference%20model%20to%20identify%20and%20filter%20out%20difficult-to-fit%20tokens%2C%20enabling%0Athe%20distillation%20of%20a%20draft%20model%20that%20better%20aligns%20with%20the%20target%20model%20on%0Asimpler%20tokens.%20This%20approach%20improves%20the%20overall%20token%20acceptance%20rate%0Awithout%20compromising%20generation%20quality.%20We%20evaluate%20AdaSPEC%20across%20diverse%0Atasks%2C%20including%20arithmetic%20reasoning%2C%20instruction-following%2C%20coding%2C%20and%0Asummarization%2C%20using%20model%20configurations%20of%2031M/1.4B%20and%20350M/2.7B%20parameters.%0AOur%20results%20demonstrate%20that%20AdaSPEC%20consistently%20outperforms%20the%0Astate-of-the-art%20DistillSpec%20method%2C%20achieving%20higher%20acceptance%20rates%20across%0Aall%20tasks%20%28up%20to%2015%5C%25%29.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/yuezhouhu/adaspec.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19779v1&entry.124074799=Read"},
{"title": "Blackbox Model Provenance via Palimpsestic Membership Inference", "author": "Rohith Kuditipudi and Jing Huang and Sally Zhu and Diyi Yang and Christopher Potts and Percy Liang", "abstract": "  Suppose Alice trains an open-weight language model and Bob uses a blackbox\nderivative of Alice's model to produce text. Can Alice prove that Bob is using\nher model, either by querying Bob's derivative model (query setting) or from\nthe text alone (observational setting)? We formulate this question as an\nindependence testing problem--in which the null hypothesis is that Bob's model\nor text is independent of Alice's randomized training run--and investigate it\nthrough the lens of palimpsestic memorization in language models: models are\nmore likely to memorize data seen later in training, so we can test whether Bob\nis using Alice's model using test statistics that capture correlation between\nBob's model or text and the ordering of training examples in Alice's training\nrun. If Alice has randomly shuffled her training data, then any significant\ncorrelation amounts to exactly quantifiable statistical evidence against the\nnull hypothesis, regardless of the composition of Alice's training data. In the\nquery setting, we directly estimate (via prompting) the likelihood Bob's model\ngives to Alice's training examples and order; we correlate the likelihoods of\nover 40 fine-tunes of various Pythia and OLMo base models ranging from 1B to\n12B parameters with the base model's training data order, achieving a p-value\non the order of at most 1e-8 in all but six cases. In the observational\nsetting, we try two approaches based on estimating 1) the likelihood of Bob's\ntext overlapping with spans of Alice's training examples and 2) the likelihood\nof Bob's text with respect to different versions of Alice's model we obtain by\nrepeating the last phase (e.g., 1%) of her training run on reshuffled data. The\nsecond approach can reliably distinguish Bob's text from as little as a few\nhundred tokens; the first does not involve any retraining but requires many\nmore tokens (several hundred thousand) to achieve high power.\n", "link": "http://arxiv.org/abs/2510.19796v1", "date": "2025-10-22", "relevancy": 2.0814, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5256}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5256}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Blackbox%20Model%20Provenance%20via%20Palimpsestic%20Membership%20Inference&body=Title%3A%20Blackbox%20Model%20Provenance%20via%20Palimpsestic%20Membership%20Inference%0AAuthor%3A%20Rohith%20Kuditipudi%20and%20Jing%20Huang%20and%20Sally%20Zhu%20and%20Diyi%20Yang%20and%20Christopher%20Potts%20and%20Percy%20Liang%0AAbstract%3A%20%20%20Suppose%20Alice%20trains%20an%20open-weight%20language%20model%20and%20Bob%20uses%20a%20blackbox%0Aderivative%20of%20Alice%27s%20model%20to%20produce%20text.%20Can%20Alice%20prove%20that%20Bob%20is%20using%0Aher%20model%2C%20either%20by%20querying%20Bob%27s%20derivative%20model%20%28query%20setting%29%20or%20from%0Athe%20text%20alone%20%28observational%20setting%29%3F%20We%20formulate%20this%20question%20as%20an%0Aindependence%20testing%20problem--in%20which%20the%20null%20hypothesis%20is%20that%20Bob%27s%20model%0Aor%20text%20is%20independent%20of%20Alice%27s%20randomized%20training%20run--and%20investigate%20it%0Athrough%20the%20lens%20of%20palimpsestic%20memorization%20in%20language%20models%3A%20models%20are%0Amore%20likely%20to%20memorize%20data%20seen%20later%20in%20training%2C%20so%20we%20can%20test%20whether%20Bob%0Ais%20using%20Alice%27s%20model%20using%20test%20statistics%20that%20capture%20correlation%20between%0ABob%27s%20model%20or%20text%20and%20the%20ordering%20of%20training%20examples%20in%20Alice%27s%20training%0Arun.%20If%20Alice%20has%20randomly%20shuffled%20her%20training%20data%2C%20then%20any%20significant%0Acorrelation%20amounts%20to%20exactly%20quantifiable%20statistical%20evidence%20against%20the%0Anull%20hypothesis%2C%20regardless%20of%20the%20composition%20of%20Alice%27s%20training%20data.%20In%20the%0Aquery%20setting%2C%20we%20directly%20estimate%20%28via%20prompting%29%20the%20likelihood%20Bob%27s%20model%0Agives%20to%20Alice%27s%20training%20examples%20and%20order%3B%20we%20correlate%20the%20likelihoods%20of%0Aover%2040%20fine-tunes%20of%20various%20Pythia%20and%20OLMo%20base%20models%20ranging%20from%201B%20to%0A12B%20parameters%20with%20the%20base%20model%27s%20training%20data%20order%2C%20achieving%20a%20p-value%0Aon%20the%20order%20of%20at%20most%201e-8%20in%20all%20but%20six%20cases.%20In%20the%20observational%0Asetting%2C%20we%20try%20two%20approaches%20based%20on%20estimating%201%29%20the%20likelihood%20of%20Bob%27s%0Atext%20overlapping%20with%20spans%20of%20Alice%27s%20training%20examples%20and%202%29%20the%20likelihood%0Aof%20Bob%27s%20text%20with%20respect%20to%20different%20versions%20of%20Alice%27s%20model%20we%20obtain%20by%0Arepeating%20the%20last%20phase%20%28e.g.%2C%201%25%29%20of%20her%20training%20run%20on%20reshuffled%20data.%20The%0Asecond%20approach%20can%20reliably%20distinguish%20Bob%27s%20text%20from%20as%20little%20as%20a%20few%0Ahundred%20tokens%3B%20the%20first%20does%20not%20involve%20any%20retraining%20but%20requires%20many%0Amore%20tokens%20%28several%20hundred%20thousand%29%20to%20achieve%20high%20power.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19796v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBlackbox%2520Model%2520Provenance%2520via%2520Palimpsestic%2520Membership%2520Inference%26entry.906535625%3DRohith%2520Kuditipudi%2520and%2520Jing%2520Huang%2520and%2520Sally%2520Zhu%2520and%2520Diyi%2520Yang%2520and%2520Christopher%2520Potts%2520and%2520Percy%2520Liang%26entry.1292438233%3D%2520%2520Suppose%2520Alice%2520trains%2520an%2520open-weight%2520language%2520model%2520and%2520Bob%2520uses%2520a%2520blackbox%250Aderivative%2520of%2520Alice%2527s%2520model%2520to%2520produce%2520text.%2520Can%2520Alice%2520prove%2520that%2520Bob%2520is%2520using%250Aher%2520model%252C%2520either%2520by%2520querying%2520Bob%2527s%2520derivative%2520model%2520%2528query%2520setting%2529%2520or%2520from%250Athe%2520text%2520alone%2520%2528observational%2520setting%2529%253F%2520We%2520formulate%2520this%2520question%2520as%2520an%250Aindependence%2520testing%2520problem--in%2520which%2520the%2520null%2520hypothesis%2520is%2520that%2520Bob%2527s%2520model%250Aor%2520text%2520is%2520independent%2520of%2520Alice%2527s%2520randomized%2520training%2520run--and%2520investigate%2520it%250Athrough%2520the%2520lens%2520of%2520palimpsestic%2520memorization%2520in%2520language%2520models%253A%2520models%2520are%250Amore%2520likely%2520to%2520memorize%2520data%2520seen%2520later%2520in%2520training%252C%2520so%2520we%2520can%2520test%2520whether%2520Bob%250Ais%2520using%2520Alice%2527s%2520model%2520using%2520test%2520statistics%2520that%2520capture%2520correlation%2520between%250ABob%2527s%2520model%2520or%2520text%2520and%2520the%2520ordering%2520of%2520training%2520examples%2520in%2520Alice%2527s%2520training%250Arun.%2520If%2520Alice%2520has%2520randomly%2520shuffled%2520her%2520training%2520data%252C%2520then%2520any%2520significant%250Acorrelation%2520amounts%2520to%2520exactly%2520quantifiable%2520statistical%2520evidence%2520against%2520the%250Anull%2520hypothesis%252C%2520regardless%2520of%2520the%2520composition%2520of%2520Alice%2527s%2520training%2520data.%2520In%2520the%250Aquery%2520setting%252C%2520we%2520directly%2520estimate%2520%2528via%2520prompting%2529%2520the%2520likelihood%2520Bob%2527s%2520model%250Agives%2520to%2520Alice%2527s%2520training%2520examples%2520and%2520order%253B%2520we%2520correlate%2520the%2520likelihoods%2520of%250Aover%252040%2520fine-tunes%2520of%2520various%2520Pythia%2520and%2520OLMo%2520base%2520models%2520ranging%2520from%25201B%2520to%250A12B%2520parameters%2520with%2520the%2520base%2520model%2527s%2520training%2520data%2520order%252C%2520achieving%2520a%2520p-value%250Aon%2520the%2520order%2520of%2520at%2520most%25201e-8%2520in%2520all%2520but%2520six%2520cases.%2520In%2520the%2520observational%250Asetting%252C%2520we%2520try%2520two%2520approaches%2520based%2520on%2520estimating%25201%2529%2520the%2520likelihood%2520of%2520Bob%2527s%250Atext%2520overlapping%2520with%2520spans%2520of%2520Alice%2527s%2520training%2520examples%2520and%25202%2529%2520the%2520likelihood%250Aof%2520Bob%2527s%2520text%2520with%2520respect%2520to%2520different%2520versions%2520of%2520Alice%2527s%2520model%2520we%2520obtain%2520by%250Arepeating%2520the%2520last%2520phase%2520%2528e.g.%252C%25201%2525%2529%2520of%2520her%2520training%2520run%2520on%2520reshuffled%2520data.%2520The%250Asecond%2520approach%2520can%2520reliably%2520distinguish%2520Bob%2527s%2520text%2520from%2520as%2520little%2520as%2520a%2520few%250Ahundred%2520tokens%253B%2520the%2520first%2520does%2520not%2520involve%2520any%2520retraining%2520but%2520requires%2520many%250Amore%2520tokens%2520%2528several%2520hundred%2520thousand%2529%2520to%2520achieve%2520high%2520power.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19796v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Blackbox%20Model%20Provenance%20via%20Palimpsestic%20Membership%20Inference&entry.906535625=Rohith%20Kuditipudi%20and%20Jing%20Huang%20and%20Sally%20Zhu%20and%20Diyi%20Yang%20and%20Christopher%20Potts%20and%20Percy%20Liang&entry.1292438233=%20%20Suppose%20Alice%20trains%20an%20open-weight%20language%20model%20and%20Bob%20uses%20a%20blackbox%0Aderivative%20of%20Alice%27s%20model%20to%20produce%20text.%20Can%20Alice%20prove%20that%20Bob%20is%20using%0Aher%20model%2C%20either%20by%20querying%20Bob%27s%20derivative%20model%20%28query%20setting%29%20or%20from%0Athe%20text%20alone%20%28observational%20setting%29%3F%20We%20formulate%20this%20question%20as%20an%0Aindependence%20testing%20problem--in%20which%20the%20null%20hypothesis%20is%20that%20Bob%27s%20model%0Aor%20text%20is%20independent%20of%20Alice%27s%20randomized%20training%20run--and%20investigate%20it%0Athrough%20the%20lens%20of%20palimpsestic%20memorization%20in%20language%20models%3A%20models%20are%0Amore%20likely%20to%20memorize%20data%20seen%20later%20in%20training%2C%20so%20we%20can%20test%20whether%20Bob%0Ais%20using%20Alice%27s%20model%20using%20test%20statistics%20that%20capture%20correlation%20between%0ABob%27s%20model%20or%20text%20and%20the%20ordering%20of%20training%20examples%20in%20Alice%27s%20training%0Arun.%20If%20Alice%20has%20randomly%20shuffled%20her%20training%20data%2C%20then%20any%20significant%0Acorrelation%20amounts%20to%20exactly%20quantifiable%20statistical%20evidence%20against%20the%0Anull%20hypothesis%2C%20regardless%20of%20the%20composition%20of%20Alice%27s%20training%20data.%20In%20the%0Aquery%20setting%2C%20we%20directly%20estimate%20%28via%20prompting%29%20the%20likelihood%20Bob%27s%20model%0Agives%20to%20Alice%27s%20training%20examples%20and%20order%3B%20we%20correlate%20the%20likelihoods%20of%0Aover%2040%20fine-tunes%20of%20various%20Pythia%20and%20OLMo%20base%20models%20ranging%20from%201B%20to%0A12B%20parameters%20with%20the%20base%20model%27s%20training%20data%20order%2C%20achieving%20a%20p-value%0Aon%20the%20order%20of%20at%20most%201e-8%20in%20all%20but%20six%20cases.%20In%20the%20observational%0Asetting%2C%20we%20try%20two%20approaches%20based%20on%20estimating%201%29%20the%20likelihood%20of%20Bob%27s%0Atext%20overlapping%20with%20spans%20of%20Alice%27s%20training%20examples%20and%202%29%20the%20likelihood%0Aof%20Bob%27s%20text%20with%20respect%20to%20different%20versions%20of%20Alice%27s%20model%20we%20obtain%20by%0Arepeating%20the%20last%20phase%20%28e.g.%2C%201%25%29%20of%20her%20training%20run%20on%20reshuffled%20data.%20The%0Asecond%20approach%20can%20reliably%20distinguish%20Bob%27s%20text%20from%20as%20little%20as%20a%20few%0Ahundred%20tokens%3B%20the%20first%20does%20not%20involve%20any%20retraining%20but%20requires%20many%0Amore%20tokens%20%28several%20hundred%20thousand%29%20to%20achieve%20high%20power.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19796v1&entry.124074799=Read"},
{"title": "Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing", "author": "Yusu Qian and Eli Bocek-Rivele and Liangchen Song and Jialing Tong and Yinfei Yang and Jiasen Lu and Wenze Hu and Zhe Gan", "abstract": "  Recent advances in multimodal models have demonstrated remarkable text-guided\nimage editing capabilities, with systems like GPT-4o and Nano-Banana setting\nnew benchmarks. However, the research community's progress remains constrained\nby the absence of large-scale, high-quality, and openly accessible datasets\nbuilt from real images. We introduce Pico-Banana-400K, a comprehensive\n400K-image dataset for instruction-based image editing. Our dataset is\nconstructed by leveraging Nano-Banana to generate diverse edit pairs from real\nphotographs in the OpenImages collection. What distinguishes Pico-Banana-400K\nfrom previous synthetic datasets is our systematic approach to quality and\ndiversity. We employ a fine-grained image editing taxonomy to ensure\ncomprehensive coverage of edit types while maintaining precise content\npreservation and instruction faithfulness through MLLM-based quality scoring\nand careful curation. Beyond single turn editing, Pico-Banana-400K enables\nresearch into complex editing scenarios. The dataset includes three specialized\nsubsets: (1) a 72K-example multi-turn collection for studying sequential\nediting, reasoning, and planning across consecutive modifications; (2) a\n56K-example preference subset for alignment research and reward model training;\nand (3) paired long-short editing instructions for developing instruction\nrewriting and summarization capabilities. By providing this large-scale,\nhigh-quality, and task-rich resource, Pico-Banana-400K establishes a robust\nfoundation for training and benchmarking the next generation of text-guided\nimage editing models.\n", "link": "http://arxiv.org/abs/2510.19808v1", "date": "2025-10-22", "relevancy": 2.0734, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5437}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5361}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4904}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pico-Banana-400K%3A%20A%20Large-Scale%20Dataset%20for%20Text-Guided%20Image%20Editing&body=Title%3A%20Pico-Banana-400K%3A%20A%20Large-Scale%20Dataset%20for%20Text-Guided%20Image%20Editing%0AAuthor%3A%20Yusu%20Qian%20and%20Eli%20Bocek-Rivele%20and%20Liangchen%20Song%20and%20Jialing%20Tong%20and%20Yinfei%20Yang%20and%20Jiasen%20Lu%20and%20Wenze%20Hu%20and%20Zhe%20Gan%0AAbstract%3A%20%20%20Recent%20advances%20in%20multimodal%20models%20have%20demonstrated%20remarkable%20text-guided%0Aimage%20editing%20capabilities%2C%20with%20systems%20like%20GPT-4o%20and%20Nano-Banana%20setting%0Anew%20benchmarks.%20However%2C%20the%20research%20community%27s%20progress%20remains%20constrained%0Aby%20the%20absence%20of%20large-scale%2C%20high-quality%2C%20and%20openly%20accessible%20datasets%0Abuilt%20from%20real%20images.%20We%20introduce%20Pico-Banana-400K%2C%20a%20comprehensive%0A400K-image%20dataset%20for%20instruction-based%20image%20editing.%20Our%20dataset%20is%0Aconstructed%20by%20leveraging%20Nano-Banana%20to%20generate%20diverse%20edit%20pairs%20from%20real%0Aphotographs%20in%20the%20OpenImages%20collection.%20What%20distinguishes%20Pico-Banana-400K%0Afrom%20previous%20synthetic%20datasets%20is%20our%20systematic%20approach%20to%20quality%20and%0Adiversity.%20We%20employ%20a%20fine-grained%20image%20editing%20taxonomy%20to%20ensure%0Acomprehensive%20coverage%20of%20edit%20types%20while%20maintaining%20precise%20content%0Apreservation%20and%20instruction%20faithfulness%20through%20MLLM-based%20quality%20scoring%0Aand%20careful%20curation.%20Beyond%20single%20turn%20editing%2C%20Pico-Banana-400K%20enables%0Aresearch%20into%20complex%20editing%20scenarios.%20The%20dataset%20includes%20three%20specialized%0Asubsets%3A%20%281%29%20a%2072K-example%20multi-turn%20collection%20for%20studying%20sequential%0Aediting%2C%20reasoning%2C%20and%20planning%20across%20consecutive%20modifications%3B%20%282%29%20a%0A56K-example%20preference%20subset%20for%20alignment%20research%20and%20reward%20model%20training%3B%0Aand%20%283%29%20paired%20long-short%20editing%20instructions%20for%20developing%20instruction%0Arewriting%20and%20summarization%20capabilities.%20By%20providing%20this%20large-scale%2C%0Ahigh-quality%2C%20and%20task-rich%20resource%2C%20Pico-Banana-400K%20establishes%20a%20robust%0Afoundation%20for%20training%20and%20benchmarking%20the%20next%20generation%20of%20text-guided%0Aimage%20editing%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19808v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPico-Banana-400K%253A%2520A%2520Large-Scale%2520Dataset%2520for%2520Text-Guided%2520Image%2520Editing%26entry.906535625%3DYusu%2520Qian%2520and%2520Eli%2520Bocek-Rivele%2520and%2520Liangchen%2520Song%2520and%2520Jialing%2520Tong%2520and%2520Yinfei%2520Yang%2520and%2520Jiasen%2520Lu%2520and%2520Wenze%2520Hu%2520and%2520Zhe%2520Gan%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520multimodal%2520models%2520have%2520demonstrated%2520remarkable%2520text-guided%250Aimage%2520editing%2520capabilities%252C%2520with%2520systems%2520like%2520GPT-4o%2520and%2520Nano-Banana%2520setting%250Anew%2520benchmarks.%2520However%252C%2520the%2520research%2520community%2527s%2520progress%2520remains%2520constrained%250Aby%2520the%2520absence%2520of%2520large-scale%252C%2520high-quality%252C%2520and%2520openly%2520accessible%2520datasets%250Abuilt%2520from%2520real%2520images.%2520We%2520introduce%2520Pico-Banana-400K%252C%2520a%2520comprehensive%250A400K-image%2520dataset%2520for%2520instruction-based%2520image%2520editing.%2520Our%2520dataset%2520is%250Aconstructed%2520by%2520leveraging%2520Nano-Banana%2520to%2520generate%2520diverse%2520edit%2520pairs%2520from%2520real%250Aphotographs%2520in%2520the%2520OpenImages%2520collection.%2520What%2520distinguishes%2520Pico-Banana-400K%250Afrom%2520previous%2520synthetic%2520datasets%2520is%2520our%2520systematic%2520approach%2520to%2520quality%2520and%250Adiversity.%2520We%2520employ%2520a%2520fine-grained%2520image%2520editing%2520taxonomy%2520to%2520ensure%250Acomprehensive%2520coverage%2520of%2520edit%2520types%2520while%2520maintaining%2520precise%2520content%250Apreservation%2520and%2520instruction%2520faithfulness%2520through%2520MLLM-based%2520quality%2520scoring%250Aand%2520careful%2520curation.%2520Beyond%2520single%2520turn%2520editing%252C%2520Pico-Banana-400K%2520enables%250Aresearch%2520into%2520complex%2520editing%2520scenarios.%2520The%2520dataset%2520includes%2520three%2520specialized%250Asubsets%253A%2520%25281%2529%2520a%252072K-example%2520multi-turn%2520collection%2520for%2520studying%2520sequential%250Aediting%252C%2520reasoning%252C%2520and%2520planning%2520across%2520consecutive%2520modifications%253B%2520%25282%2529%2520a%250A56K-example%2520preference%2520subset%2520for%2520alignment%2520research%2520and%2520reward%2520model%2520training%253B%250Aand%2520%25283%2529%2520paired%2520long-short%2520editing%2520instructions%2520for%2520developing%2520instruction%250Arewriting%2520and%2520summarization%2520capabilities.%2520By%2520providing%2520this%2520large-scale%252C%250Ahigh-quality%252C%2520and%2520task-rich%2520resource%252C%2520Pico-Banana-400K%2520establishes%2520a%2520robust%250Afoundation%2520for%2520training%2520and%2520benchmarking%2520the%2520next%2520generation%2520of%2520text-guided%250Aimage%2520editing%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19808v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pico-Banana-400K%3A%20A%20Large-Scale%20Dataset%20for%20Text-Guided%20Image%20Editing&entry.906535625=Yusu%20Qian%20and%20Eli%20Bocek-Rivele%20and%20Liangchen%20Song%20and%20Jialing%20Tong%20and%20Yinfei%20Yang%20and%20Jiasen%20Lu%20and%20Wenze%20Hu%20and%20Zhe%20Gan&entry.1292438233=%20%20Recent%20advances%20in%20multimodal%20models%20have%20demonstrated%20remarkable%20text-guided%0Aimage%20editing%20capabilities%2C%20with%20systems%20like%20GPT-4o%20and%20Nano-Banana%20setting%0Anew%20benchmarks.%20However%2C%20the%20research%20community%27s%20progress%20remains%20constrained%0Aby%20the%20absence%20of%20large-scale%2C%20high-quality%2C%20and%20openly%20accessible%20datasets%0Abuilt%20from%20real%20images.%20We%20introduce%20Pico-Banana-400K%2C%20a%20comprehensive%0A400K-image%20dataset%20for%20instruction-based%20image%20editing.%20Our%20dataset%20is%0Aconstructed%20by%20leveraging%20Nano-Banana%20to%20generate%20diverse%20edit%20pairs%20from%20real%0Aphotographs%20in%20the%20OpenImages%20collection.%20What%20distinguishes%20Pico-Banana-400K%0Afrom%20previous%20synthetic%20datasets%20is%20our%20systematic%20approach%20to%20quality%20and%0Adiversity.%20We%20employ%20a%20fine-grained%20image%20editing%20taxonomy%20to%20ensure%0Acomprehensive%20coverage%20of%20edit%20types%20while%20maintaining%20precise%20content%0Apreservation%20and%20instruction%20faithfulness%20through%20MLLM-based%20quality%20scoring%0Aand%20careful%20curation.%20Beyond%20single%20turn%20editing%2C%20Pico-Banana-400K%20enables%0Aresearch%20into%20complex%20editing%20scenarios.%20The%20dataset%20includes%20three%20specialized%0Asubsets%3A%20%281%29%20a%2072K-example%20multi-turn%20collection%20for%20studying%20sequential%0Aediting%2C%20reasoning%2C%20and%20planning%20across%20consecutive%20modifications%3B%20%282%29%20a%0A56K-example%20preference%20subset%20for%20alignment%20research%20and%20reward%20model%20training%3B%0Aand%20%283%29%20paired%20long-short%20editing%20instructions%20for%20developing%20instruction%0Arewriting%20and%20summarization%20capabilities.%20By%20providing%20this%20large-scale%2C%0Ahigh-quality%2C%20and%20task-rich%20resource%2C%20Pico-Banana-400K%20establishes%20a%20robust%0Afoundation%20for%20training%20and%20benchmarking%20the%20next%20generation%20of%20text-guided%0Aimage%20editing%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19808v1&entry.124074799=Read"},
{"title": "Augmenting Moment Retrieval: Zero-Dependency Two-Stage Learning", "author": "Zhengxuan Wei and Jiajin Tang and Sibei Yang", "abstract": "  Existing Moment Retrieval methods face three critical bottlenecks: (1) data\nscarcity forces models into shallow keyword-feature associations; (2) boundary\nambiguity in transition regions between adjacent events; (3) insufficient\ndiscrimination of fine-grained semantics (e.g., distinguishing ``kicking\" vs.\n``throwing\" a ball). In this paper, we propose a zero-external-dependency\nAugmented Moment Retrieval framework, AMR, designed to overcome local optima\ncaused by insufficient data annotations and the lack of robust boundary and\nsemantic discrimination capabilities. AMR is built upon two key insights: (1)\nit resolves ambiguous boundary information and semantic confusion in existing\nannotations without additional data (avoiding costly manual labeling), and (2)\nit preserves boundary and semantic discriminative capabilities enhanced by\ntraining while generalizing to real-world scenarios, significantly improving\nperformance. Furthermore, we propose a two-stage training framework with\ncold-start and distillation adaptation. The cold-start stage employs curriculum\nlearning on augmented data to build foundational boundary/semantic awareness.\nThe distillation stage introduces dual query sets: Original Queries maintain\nDETR-based localization using frozen Base Queries from the cold-start model,\nwhile Active Queries dynamically adapt to real-data distributions. A\ncross-stage distillation loss enforces consistency between Original and Base\nQueries, preventing knowledge forgetting while enabling real-world\ngeneralization. Experiments on multiple benchmarks show that AMR achieves\nimproved performance over prior state-of-the-art approaches.\n", "link": "http://arxiv.org/abs/2510.19622v1", "date": "2025-10-22", "relevancy": 2.0645, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5444}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5143}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Augmenting%20Moment%20Retrieval%3A%20Zero-Dependency%20Two-Stage%20Learning&body=Title%3A%20Augmenting%20Moment%20Retrieval%3A%20Zero-Dependency%20Two-Stage%20Learning%0AAuthor%3A%20Zhengxuan%20Wei%20and%20Jiajin%20Tang%20and%20Sibei%20Yang%0AAbstract%3A%20%20%20Existing%20Moment%20Retrieval%20methods%20face%20three%20critical%20bottlenecks%3A%20%281%29%20data%0Ascarcity%20forces%20models%20into%20shallow%20keyword-feature%20associations%3B%20%282%29%20boundary%0Aambiguity%20in%20transition%20regions%20between%20adjacent%20events%3B%20%283%29%20insufficient%0Adiscrimination%20of%20fine-grained%20semantics%20%28e.g.%2C%20distinguishing%20%60%60kicking%22%20vs.%0A%60%60throwing%22%20a%20ball%29.%20In%20this%20paper%2C%20we%20propose%20a%20zero-external-dependency%0AAugmented%20Moment%20Retrieval%20framework%2C%20AMR%2C%20designed%20to%20overcome%20local%20optima%0Acaused%20by%20insufficient%20data%20annotations%20and%20the%20lack%20of%20robust%20boundary%20and%0Asemantic%20discrimination%20capabilities.%20AMR%20is%20built%20upon%20two%20key%20insights%3A%20%281%29%0Ait%20resolves%20ambiguous%20boundary%20information%20and%20semantic%20confusion%20in%20existing%0Aannotations%20without%20additional%20data%20%28avoiding%20costly%20manual%20labeling%29%2C%20and%20%282%29%0Ait%20preserves%20boundary%20and%20semantic%20discriminative%20capabilities%20enhanced%20by%0Atraining%20while%20generalizing%20to%20real-world%20scenarios%2C%20significantly%20improving%0Aperformance.%20Furthermore%2C%20we%20propose%20a%20two-stage%20training%20framework%20with%0Acold-start%20and%20distillation%20adaptation.%20The%20cold-start%20stage%20employs%20curriculum%0Alearning%20on%20augmented%20data%20to%20build%20foundational%20boundary/semantic%20awareness.%0AThe%20distillation%20stage%20introduces%20dual%20query%20sets%3A%20Original%20Queries%20maintain%0ADETR-based%20localization%20using%20frozen%20Base%20Queries%20from%20the%20cold-start%20model%2C%0Awhile%20Active%20Queries%20dynamically%20adapt%20to%20real-data%20distributions.%20A%0Across-stage%20distillation%20loss%20enforces%20consistency%20between%20Original%20and%20Base%0AQueries%2C%20preventing%20knowledge%20forgetting%20while%20enabling%20real-world%0Ageneralization.%20Experiments%20on%20multiple%20benchmarks%20show%20that%20AMR%20achieves%0Aimproved%20performance%20over%20prior%20state-of-the-art%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19622v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAugmenting%2520Moment%2520Retrieval%253A%2520Zero-Dependency%2520Two-Stage%2520Learning%26entry.906535625%3DZhengxuan%2520Wei%2520and%2520Jiajin%2520Tang%2520and%2520Sibei%2520Yang%26entry.1292438233%3D%2520%2520Existing%2520Moment%2520Retrieval%2520methods%2520face%2520three%2520critical%2520bottlenecks%253A%2520%25281%2529%2520data%250Ascarcity%2520forces%2520models%2520into%2520shallow%2520keyword-feature%2520associations%253B%2520%25282%2529%2520boundary%250Aambiguity%2520in%2520transition%2520regions%2520between%2520adjacent%2520events%253B%2520%25283%2529%2520insufficient%250Adiscrimination%2520of%2520fine-grained%2520semantics%2520%2528e.g.%252C%2520distinguishing%2520%2560%2560kicking%2522%2520vs.%250A%2560%2560throwing%2522%2520a%2520ball%2529.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520zero-external-dependency%250AAugmented%2520Moment%2520Retrieval%2520framework%252C%2520AMR%252C%2520designed%2520to%2520overcome%2520local%2520optima%250Acaused%2520by%2520insufficient%2520data%2520annotations%2520and%2520the%2520lack%2520of%2520robust%2520boundary%2520and%250Asemantic%2520discrimination%2520capabilities.%2520AMR%2520is%2520built%2520upon%2520two%2520key%2520insights%253A%2520%25281%2529%250Ait%2520resolves%2520ambiguous%2520boundary%2520information%2520and%2520semantic%2520confusion%2520in%2520existing%250Aannotations%2520without%2520additional%2520data%2520%2528avoiding%2520costly%2520manual%2520labeling%2529%252C%2520and%2520%25282%2529%250Ait%2520preserves%2520boundary%2520and%2520semantic%2520discriminative%2520capabilities%2520enhanced%2520by%250Atraining%2520while%2520generalizing%2520to%2520real-world%2520scenarios%252C%2520significantly%2520improving%250Aperformance.%2520Furthermore%252C%2520we%2520propose%2520a%2520two-stage%2520training%2520framework%2520with%250Acold-start%2520and%2520distillation%2520adaptation.%2520The%2520cold-start%2520stage%2520employs%2520curriculum%250Alearning%2520on%2520augmented%2520data%2520to%2520build%2520foundational%2520boundary/semantic%2520awareness.%250AThe%2520distillation%2520stage%2520introduces%2520dual%2520query%2520sets%253A%2520Original%2520Queries%2520maintain%250ADETR-based%2520localization%2520using%2520frozen%2520Base%2520Queries%2520from%2520the%2520cold-start%2520model%252C%250Awhile%2520Active%2520Queries%2520dynamically%2520adapt%2520to%2520real-data%2520distributions.%2520A%250Across-stage%2520distillation%2520loss%2520enforces%2520consistency%2520between%2520Original%2520and%2520Base%250AQueries%252C%2520preventing%2520knowledge%2520forgetting%2520while%2520enabling%2520real-world%250Ageneralization.%2520Experiments%2520on%2520multiple%2520benchmarks%2520show%2520that%2520AMR%2520achieves%250Aimproved%2520performance%2520over%2520prior%2520state-of-the-art%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19622v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Augmenting%20Moment%20Retrieval%3A%20Zero-Dependency%20Two-Stage%20Learning&entry.906535625=Zhengxuan%20Wei%20and%20Jiajin%20Tang%20and%20Sibei%20Yang&entry.1292438233=%20%20Existing%20Moment%20Retrieval%20methods%20face%20three%20critical%20bottlenecks%3A%20%281%29%20data%0Ascarcity%20forces%20models%20into%20shallow%20keyword-feature%20associations%3B%20%282%29%20boundary%0Aambiguity%20in%20transition%20regions%20between%20adjacent%20events%3B%20%283%29%20insufficient%0Adiscrimination%20of%20fine-grained%20semantics%20%28e.g.%2C%20distinguishing%20%60%60kicking%22%20vs.%0A%60%60throwing%22%20a%20ball%29.%20In%20this%20paper%2C%20we%20propose%20a%20zero-external-dependency%0AAugmented%20Moment%20Retrieval%20framework%2C%20AMR%2C%20designed%20to%20overcome%20local%20optima%0Acaused%20by%20insufficient%20data%20annotations%20and%20the%20lack%20of%20robust%20boundary%20and%0Asemantic%20discrimination%20capabilities.%20AMR%20is%20built%20upon%20two%20key%20insights%3A%20%281%29%0Ait%20resolves%20ambiguous%20boundary%20information%20and%20semantic%20confusion%20in%20existing%0Aannotations%20without%20additional%20data%20%28avoiding%20costly%20manual%20labeling%29%2C%20and%20%282%29%0Ait%20preserves%20boundary%20and%20semantic%20discriminative%20capabilities%20enhanced%20by%0Atraining%20while%20generalizing%20to%20real-world%20scenarios%2C%20significantly%20improving%0Aperformance.%20Furthermore%2C%20we%20propose%20a%20two-stage%20training%20framework%20with%0Acold-start%20and%20distillation%20adaptation.%20The%20cold-start%20stage%20employs%20curriculum%0Alearning%20on%20augmented%20data%20to%20build%20foundational%20boundary/semantic%20awareness.%0AThe%20distillation%20stage%20introduces%20dual%20query%20sets%3A%20Original%20Queries%20maintain%0ADETR-based%20localization%20using%20frozen%20Base%20Queries%20from%20the%20cold-start%20model%2C%0Awhile%20Active%20Queries%20dynamically%20adapt%20to%20real-data%20distributions.%20A%0Across-stage%20distillation%20loss%20enforces%20consistency%20between%20Original%20and%20Base%0AQueries%2C%20preventing%20knowledge%20forgetting%20while%20enabling%20real-world%0Ageneralization.%20Experiments%20on%20multiple%20benchmarks%20show%20that%20AMR%20achieves%0Aimproved%20performance%20over%20prior%20state-of-the-art%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19622v1&entry.124074799=Read"},
{"title": "LASeR: Learning to Adaptively Select Reward Models with Multi-Armed\n  Bandits", "author": "Duy Nguyen and Archiki Prasad and Elias Stengel-Eskin and Mohit Bansal", "abstract": "  Reward Models (RMs) are crucial to aligning large language models (LLMs), but\nthe degree to which an RM specialized to one task (e.g. writing) generalizes to\nnew tasks (e.g. math) is often not known a priori, often making using only one\nfixed RM to train LLMs suboptimal. However, optimizing LLMs with multiple RMs\nsimultaneously can incur a prohibitively high computational cost and lead to\nconflicting signals from different RMs that may degrade performance. To address\nthese challenges, we introduce LASeR (Learning to Adaptively Select Rewards),\nwhich frames reward model selection as a multi-armed bandit problem,\nefficiently and iteratively training LLMs using multiple RMs by selecting the\nmost well-suited RM for each instance. On commonsense and math reasoning tasks,\nwe show that LASeR boosts iterative LLM training, improving the absolute\naverage accuracy of Llama-3-8B over three datasets by 2.67% over an ensemble of\nRM scores while also showing superior efficiency (e.g., a 2x speedup).\nMoreover, on WildChat (open-ended instruction-following tasks), LASeR leads to\na 72.69% AlpacaEval win rate over the RM score ensemble baseline. Extending to\nlong-context generation, LASeR improves by 2.96 F1 points (avg.) on\nsingle-document QA tasks and 2.97 F1 points on few-shot learning over the RM\nscore ensemble baseline with best-of-n sampling.\n", "link": "http://arxiv.org/abs/2410.01735v3", "date": "2025-10-22", "relevancy": 2.056, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5303}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5082}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5001}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LASeR%3A%20Learning%20to%20Adaptively%20Select%20Reward%20Models%20with%20Multi-Armed%0A%20%20Bandits&body=Title%3A%20LASeR%3A%20Learning%20to%20Adaptively%20Select%20Reward%20Models%20with%20Multi-Armed%0A%20%20Bandits%0AAuthor%3A%20Duy%20Nguyen%20and%20Archiki%20Prasad%20and%20Elias%20Stengel-Eskin%20and%20Mohit%20Bansal%0AAbstract%3A%20%20%20Reward%20Models%20%28RMs%29%20are%20crucial%20to%20aligning%20large%20language%20models%20%28LLMs%29%2C%20but%0Athe%20degree%20to%20which%20an%20RM%20specialized%20to%20one%20task%20%28e.g.%20writing%29%20generalizes%20to%0Anew%20tasks%20%28e.g.%20math%29%20is%20often%20not%20known%20a%20priori%2C%20often%20making%20using%20only%20one%0Afixed%20RM%20to%20train%20LLMs%20suboptimal.%20However%2C%20optimizing%20LLMs%20with%20multiple%20RMs%0Asimultaneously%20can%20incur%20a%20prohibitively%20high%20computational%20cost%20and%20lead%20to%0Aconflicting%20signals%20from%20different%20RMs%20that%20may%20degrade%20performance.%20To%20address%0Athese%20challenges%2C%20we%20introduce%20LASeR%20%28Learning%20to%20Adaptively%20Select%20Rewards%29%2C%0Awhich%20frames%20reward%20model%20selection%20as%20a%20multi-armed%20bandit%20problem%2C%0Aefficiently%20and%20iteratively%20training%20LLMs%20using%20multiple%20RMs%20by%20selecting%20the%0Amost%20well-suited%20RM%20for%20each%20instance.%20On%20commonsense%20and%20math%20reasoning%20tasks%2C%0Awe%20show%20that%20LASeR%20boosts%20iterative%20LLM%20training%2C%20improving%20the%20absolute%0Aaverage%20accuracy%20of%20Llama-3-8B%20over%20three%20datasets%20by%202.67%25%20over%20an%20ensemble%20of%0ARM%20scores%20while%20also%20showing%20superior%20efficiency%20%28e.g.%2C%20a%202x%20speedup%29.%0AMoreover%2C%20on%20WildChat%20%28open-ended%20instruction-following%20tasks%29%2C%20LASeR%20leads%20to%0Aa%2072.69%25%20AlpacaEval%20win%20rate%20over%20the%20RM%20score%20ensemble%20baseline.%20Extending%20to%0Along-context%20generation%2C%20LASeR%20improves%20by%202.96%20F1%20points%20%28avg.%29%20on%0Asingle-document%20QA%20tasks%20and%202.97%20F1%20points%20on%20few-shot%20learning%20over%20the%20RM%0Ascore%20ensemble%20baseline%20with%20best-of-n%20sampling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01735v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLASeR%253A%2520Learning%2520to%2520Adaptively%2520Select%2520Reward%2520Models%2520with%2520Multi-Armed%250A%2520%2520Bandits%26entry.906535625%3DDuy%2520Nguyen%2520and%2520Archiki%2520Prasad%2520and%2520Elias%2520Stengel-Eskin%2520and%2520Mohit%2520Bansal%26entry.1292438233%3D%2520%2520Reward%2520Models%2520%2528RMs%2529%2520are%2520crucial%2520to%2520aligning%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520but%250Athe%2520degree%2520to%2520which%2520an%2520RM%2520specialized%2520to%2520one%2520task%2520%2528e.g.%2520writing%2529%2520generalizes%2520to%250Anew%2520tasks%2520%2528e.g.%2520math%2529%2520is%2520often%2520not%2520known%2520a%2520priori%252C%2520often%2520making%2520using%2520only%2520one%250Afixed%2520RM%2520to%2520train%2520LLMs%2520suboptimal.%2520However%252C%2520optimizing%2520LLMs%2520with%2520multiple%2520RMs%250Asimultaneously%2520can%2520incur%2520a%2520prohibitively%2520high%2520computational%2520cost%2520and%2520lead%2520to%250Aconflicting%2520signals%2520from%2520different%2520RMs%2520that%2520may%2520degrade%2520performance.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520introduce%2520LASeR%2520%2528Learning%2520to%2520Adaptively%2520Select%2520Rewards%2529%252C%250Awhich%2520frames%2520reward%2520model%2520selection%2520as%2520a%2520multi-armed%2520bandit%2520problem%252C%250Aefficiently%2520and%2520iteratively%2520training%2520LLMs%2520using%2520multiple%2520RMs%2520by%2520selecting%2520the%250Amost%2520well-suited%2520RM%2520for%2520each%2520instance.%2520On%2520commonsense%2520and%2520math%2520reasoning%2520tasks%252C%250Awe%2520show%2520that%2520LASeR%2520boosts%2520iterative%2520LLM%2520training%252C%2520improving%2520the%2520absolute%250Aaverage%2520accuracy%2520of%2520Llama-3-8B%2520over%2520three%2520datasets%2520by%25202.67%2525%2520over%2520an%2520ensemble%2520of%250ARM%2520scores%2520while%2520also%2520showing%2520superior%2520efficiency%2520%2528e.g.%252C%2520a%25202x%2520speedup%2529.%250AMoreover%252C%2520on%2520WildChat%2520%2528open-ended%2520instruction-following%2520tasks%2529%252C%2520LASeR%2520leads%2520to%250Aa%252072.69%2525%2520AlpacaEval%2520win%2520rate%2520over%2520the%2520RM%2520score%2520ensemble%2520baseline.%2520Extending%2520to%250Along-context%2520generation%252C%2520LASeR%2520improves%2520by%25202.96%2520F1%2520points%2520%2528avg.%2529%2520on%250Asingle-document%2520QA%2520tasks%2520and%25202.97%2520F1%2520points%2520on%2520few-shot%2520learning%2520over%2520the%2520RM%250Ascore%2520ensemble%2520baseline%2520with%2520best-of-n%2520sampling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01735v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LASeR%3A%20Learning%20to%20Adaptively%20Select%20Reward%20Models%20with%20Multi-Armed%0A%20%20Bandits&entry.906535625=Duy%20Nguyen%20and%20Archiki%20Prasad%20and%20Elias%20Stengel-Eskin%20and%20Mohit%20Bansal&entry.1292438233=%20%20Reward%20Models%20%28RMs%29%20are%20crucial%20to%20aligning%20large%20language%20models%20%28LLMs%29%2C%20but%0Athe%20degree%20to%20which%20an%20RM%20specialized%20to%20one%20task%20%28e.g.%20writing%29%20generalizes%20to%0Anew%20tasks%20%28e.g.%20math%29%20is%20often%20not%20known%20a%20priori%2C%20often%20making%20using%20only%20one%0Afixed%20RM%20to%20train%20LLMs%20suboptimal.%20However%2C%20optimizing%20LLMs%20with%20multiple%20RMs%0Asimultaneously%20can%20incur%20a%20prohibitively%20high%20computational%20cost%20and%20lead%20to%0Aconflicting%20signals%20from%20different%20RMs%20that%20may%20degrade%20performance.%20To%20address%0Athese%20challenges%2C%20we%20introduce%20LASeR%20%28Learning%20to%20Adaptively%20Select%20Rewards%29%2C%0Awhich%20frames%20reward%20model%20selection%20as%20a%20multi-armed%20bandit%20problem%2C%0Aefficiently%20and%20iteratively%20training%20LLMs%20using%20multiple%20RMs%20by%20selecting%20the%0Amost%20well-suited%20RM%20for%20each%20instance.%20On%20commonsense%20and%20math%20reasoning%20tasks%2C%0Awe%20show%20that%20LASeR%20boosts%20iterative%20LLM%20training%2C%20improving%20the%20absolute%0Aaverage%20accuracy%20of%20Llama-3-8B%20over%20three%20datasets%20by%202.67%25%20over%20an%20ensemble%20of%0ARM%20scores%20while%20also%20showing%20superior%20efficiency%20%28e.g.%2C%20a%202x%20speedup%29.%0AMoreover%2C%20on%20WildChat%20%28open-ended%20instruction-following%20tasks%29%2C%20LASeR%20leads%20to%0Aa%2072.69%25%20AlpacaEval%20win%20rate%20over%20the%20RM%20score%20ensemble%20baseline.%20Extending%20to%0Along-context%20generation%2C%20LASeR%20improves%20by%202.96%20F1%20points%20%28avg.%29%20on%0Asingle-document%20QA%20tasks%20and%202.97%20F1%20points%20on%20few-shot%20learning%20over%20the%20RM%0Ascore%20ensemble%20baseline%20with%20best-of-n%20sampling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01735v3&entry.124074799=Read"},
{"title": "Explainable Face Presentation Attack Detection via Ensemble-CAM", "author": "Rashik Shadman and M G Sarwar Murshed and Faraz Hussain", "abstract": "  Presentation attacks represent a critical security threat where adversaries\nuse fake biometric data, such as face, fingerprint, or iris images, to gain\nunauthorized access to protected systems. Various presentation attack detection\n(PAD) systems have been designed leveraging deep learning (DL) models to\nmitigate this type of threat. Despite their effectiveness, most of the DL\nmodels function as black boxes - their decisions are opaque to their users. The\npurpose of explainability techniques is to provide detailed information about\nthe reason behind the behavior or decision of DL models. In particular, visual\nexplanation is necessary to better understand the decisions or predictions of\nDL-based PAD systems and determine the key regions due to which a biometric\nimage is considered real or fake by the system. In this work, a novel\ntechnique, Ensemble-CAM, is proposed for providing visual explanations for the\ndecisions made by deep learning-based face PAD systems. Our goal is to improve\nDL-based face PAD systems by providing a better understanding of their\nbehavior. Our provided visual explanations will enhance the transparency and\ntrustworthiness of DL-based face PAD systems.\n", "link": "http://arxiv.org/abs/2510.19695v1", "date": "2025-10-22", "relevancy": 2.0525, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5164}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5115}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5105}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explainable%20Face%20Presentation%20Attack%20Detection%20via%20Ensemble-CAM&body=Title%3A%20Explainable%20Face%20Presentation%20Attack%20Detection%20via%20Ensemble-CAM%0AAuthor%3A%20Rashik%20Shadman%20and%20M%20G%20Sarwar%20Murshed%20and%20Faraz%20Hussain%0AAbstract%3A%20%20%20Presentation%20attacks%20represent%20a%20critical%20security%20threat%20where%20adversaries%0Ause%20fake%20biometric%20data%2C%20such%20as%20face%2C%20fingerprint%2C%20or%20iris%20images%2C%20to%20gain%0Aunauthorized%20access%20to%20protected%20systems.%20Various%20presentation%20attack%20detection%0A%28PAD%29%20systems%20have%20been%20designed%20leveraging%20deep%20learning%20%28DL%29%20models%20to%0Amitigate%20this%20type%20of%20threat.%20Despite%20their%20effectiveness%2C%20most%20of%20the%20DL%0Amodels%20function%20as%20black%20boxes%20-%20their%20decisions%20are%20opaque%20to%20their%20users.%20The%0Apurpose%20of%20explainability%20techniques%20is%20to%20provide%20detailed%20information%20about%0Athe%20reason%20behind%20the%20behavior%20or%20decision%20of%20DL%20models.%20In%20particular%2C%20visual%0Aexplanation%20is%20necessary%20to%20better%20understand%20the%20decisions%20or%20predictions%20of%0ADL-based%20PAD%20systems%20and%20determine%20the%20key%20regions%20due%20to%20which%20a%20biometric%0Aimage%20is%20considered%20real%20or%20fake%20by%20the%20system.%20In%20this%20work%2C%20a%20novel%0Atechnique%2C%20Ensemble-CAM%2C%20is%20proposed%20for%20providing%20visual%20explanations%20for%20the%0Adecisions%20made%20by%20deep%20learning-based%20face%20PAD%20systems.%20Our%20goal%20is%20to%20improve%0ADL-based%20face%20PAD%20systems%20by%20providing%20a%20better%20understanding%20of%20their%0Abehavior.%20Our%20provided%20visual%20explanations%20will%20enhance%20the%20transparency%20and%0Atrustworthiness%20of%20DL-based%20face%20PAD%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19695v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplainable%2520Face%2520Presentation%2520Attack%2520Detection%2520via%2520Ensemble-CAM%26entry.906535625%3DRashik%2520Shadman%2520and%2520M%2520G%2520Sarwar%2520Murshed%2520and%2520Faraz%2520Hussain%26entry.1292438233%3D%2520%2520Presentation%2520attacks%2520represent%2520a%2520critical%2520security%2520threat%2520where%2520adversaries%250Ause%2520fake%2520biometric%2520data%252C%2520such%2520as%2520face%252C%2520fingerprint%252C%2520or%2520iris%2520images%252C%2520to%2520gain%250Aunauthorized%2520access%2520to%2520protected%2520systems.%2520Various%2520presentation%2520attack%2520detection%250A%2528PAD%2529%2520systems%2520have%2520been%2520designed%2520leveraging%2520deep%2520learning%2520%2528DL%2529%2520models%2520to%250Amitigate%2520this%2520type%2520of%2520threat.%2520Despite%2520their%2520effectiveness%252C%2520most%2520of%2520the%2520DL%250Amodels%2520function%2520as%2520black%2520boxes%2520-%2520their%2520decisions%2520are%2520opaque%2520to%2520their%2520users.%2520The%250Apurpose%2520of%2520explainability%2520techniques%2520is%2520to%2520provide%2520detailed%2520information%2520about%250Athe%2520reason%2520behind%2520the%2520behavior%2520or%2520decision%2520of%2520DL%2520models.%2520In%2520particular%252C%2520visual%250Aexplanation%2520is%2520necessary%2520to%2520better%2520understand%2520the%2520decisions%2520or%2520predictions%2520of%250ADL-based%2520PAD%2520systems%2520and%2520determine%2520the%2520key%2520regions%2520due%2520to%2520which%2520a%2520biometric%250Aimage%2520is%2520considered%2520real%2520or%2520fake%2520by%2520the%2520system.%2520In%2520this%2520work%252C%2520a%2520novel%250Atechnique%252C%2520Ensemble-CAM%252C%2520is%2520proposed%2520for%2520providing%2520visual%2520explanations%2520for%2520the%250Adecisions%2520made%2520by%2520deep%2520learning-based%2520face%2520PAD%2520systems.%2520Our%2520goal%2520is%2520to%2520improve%250ADL-based%2520face%2520PAD%2520systems%2520by%2520providing%2520a%2520better%2520understanding%2520of%2520their%250Abehavior.%2520Our%2520provided%2520visual%2520explanations%2520will%2520enhance%2520the%2520transparency%2520and%250Atrustworthiness%2520of%2520DL-based%2520face%2520PAD%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19695v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explainable%20Face%20Presentation%20Attack%20Detection%20via%20Ensemble-CAM&entry.906535625=Rashik%20Shadman%20and%20M%20G%20Sarwar%20Murshed%20and%20Faraz%20Hussain&entry.1292438233=%20%20Presentation%20attacks%20represent%20a%20critical%20security%20threat%20where%20adversaries%0Ause%20fake%20biometric%20data%2C%20such%20as%20face%2C%20fingerprint%2C%20or%20iris%20images%2C%20to%20gain%0Aunauthorized%20access%20to%20protected%20systems.%20Various%20presentation%20attack%20detection%0A%28PAD%29%20systems%20have%20been%20designed%20leveraging%20deep%20learning%20%28DL%29%20models%20to%0Amitigate%20this%20type%20of%20threat.%20Despite%20their%20effectiveness%2C%20most%20of%20the%20DL%0Amodels%20function%20as%20black%20boxes%20-%20their%20decisions%20are%20opaque%20to%20their%20users.%20The%0Apurpose%20of%20explainability%20techniques%20is%20to%20provide%20detailed%20information%20about%0Athe%20reason%20behind%20the%20behavior%20or%20decision%20of%20DL%20models.%20In%20particular%2C%20visual%0Aexplanation%20is%20necessary%20to%20better%20understand%20the%20decisions%20or%20predictions%20of%0ADL-based%20PAD%20systems%20and%20determine%20the%20key%20regions%20due%20to%20which%20a%20biometric%0Aimage%20is%20considered%20real%20or%20fake%20by%20the%20system.%20In%20this%20work%2C%20a%20novel%0Atechnique%2C%20Ensemble-CAM%2C%20is%20proposed%20for%20providing%20visual%20explanations%20for%20the%0Adecisions%20made%20by%20deep%20learning-based%20face%20PAD%20systems.%20Our%20goal%20is%20to%20improve%0ADL-based%20face%20PAD%20systems%20by%20providing%20a%20better%20understanding%20of%20their%0Abehavior.%20Our%20provided%20visual%20explanations%20will%20enhance%20the%20transparency%20and%0Atrustworthiness%20of%20DL-based%20face%20PAD%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19695v1&entry.124074799=Read"},
{"title": "IM-Chat: A Multi-agent LLM Framework Integrating Tool-Calling and\n  Diffusion Modeling for Knowledge Transfer in Injection Molding Industry", "author": "Junhyeong Lee and Joon-Young Kim and Heekyu Kim and Inhyo Lee and Seunghwa Ryu", "abstract": "  The injection molding industry faces critical challenges in preserving and\ntransferring field knowledge, particularly as experienced workers retire and\nmultilingual barriers hinder effective communication. This study introduces\nIM-Chat, a multi-agent framework based on large language models (LLMs),\ndesigned to facilitate knowledge transfer in injection molding. IM-Chat\nintegrates both limited documented knowledge (e.g., troubleshooting tables,\nmanuals) and extensive field data modeled through a data-driven process\ncondition generator that infers optimal manufacturing settings from\nenvironmental inputs such as temperature and humidity, enabling robust and\ncontext-aware task resolution. By adopting a retrieval-augmented generation\n(RAG) strategy and tool-calling agents within a modular architecture, IM-Chat\nensures adaptability without the need for fine-tuning. Performance was assessed\nacross 100 single-tool and 60 hybrid tasks for GPT-4o, GPT-4o-mini, and\nGPT-3.5-turbo by domain experts using a 10-point rubric focused on relevance\nand correctness, and was further supplemented by automated evaluation using\nGPT-4o guided by a domain-adapted instruction prompt. The evaluation results\nindicate that more capable models tend to achieve higher accuracy, particularly\nin complex, tool-integrated scenarios. In addition, compared with the\nfine-tuned single-agent LLM, IM-Chat demonstrated superior accuracy,\nparticularly in quantitative reasoning, and greater scalability in handling\nmultiple information sources. Overall, these findings demonstrate the viability\nof multi-agent LLM systems for industrial knowledge workflows and establish\nIM-Chat as a scalable and generalizable approach to AI-assisted decision\nsupport in manufacturing.\n", "link": "http://arxiv.org/abs/2507.15268v2", "date": "2025-10-22", "relevancy": 2.052, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5354}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5072}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IM-Chat%3A%20A%20Multi-agent%20LLM%20Framework%20Integrating%20Tool-Calling%20and%0A%20%20Diffusion%20Modeling%20for%20Knowledge%20Transfer%20in%20Injection%20Molding%20Industry&body=Title%3A%20IM-Chat%3A%20A%20Multi-agent%20LLM%20Framework%20Integrating%20Tool-Calling%20and%0A%20%20Diffusion%20Modeling%20for%20Knowledge%20Transfer%20in%20Injection%20Molding%20Industry%0AAuthor%3A%20Junhyeong%20Lee%20and%20Joon-Young%20Kim%20and%20Heekyu%20Kim%20and%20Inhyo%20Lee%20and%20Seunghwa%20Ryu%0AAbstract%3A%20%20%20The%20injection%20molding%20industry%20faces%20critical%20challenges%20in%20preserving%20and%0Atransferring%20field%20knowledge%2C%20particularly%20as%20experienced%20workers%20retire%20and%0Amultilingual%20barriers%20hinder%20effective%20communication.%20This%20study%20introduces%0AIM-Chat%2C%20a%20multi-agent%20framework%20based%20on%20large%20language%20models%20%28LLMs%29%2C%0Adesigned%20to%20facilitate%20knowledge%20transfer%20in%20injection%20molding.%20IM-Chat%0Aintegrates%20both%20limited%20documented%20knowledge%20%28e.g.%2C%20troubleshooting%20tables%2C%0Amanuals%29%20and%20extensive%20field%20data%20modeled%20through%20a%20data-driven%20process%0Acondition%20generator%20that%20infers%20optimal%20manufacturing%20settings%20from%0Aenvironmental%20inputs%20such%20as%20temperature%20and%20humidity%2C%20enabling%20robust%20and%0Acontext-aware%20task%20resolution.%20By%20adopting%20a%20retrieval-augmented%20generation%0A%28RAG%29%20strategy%20and%20tool-calling%20agents%20within%20a%20modular%20architecture%2C%20IM-Chat%0Aensures%20adaptability%20without%20the%20need%20for%20fine-tuning.%20Performance%20was%20assessed%0Aacross%20100%20single-tool%20and%2060%20hybrid%20tasks%20for%20GPT-4o%2C%20GPT-4o-mini%2C%20and%0AGPT-3.5-turbo%20by%20domain%20experts%20using%20a%2010-point%20rubric%20focused%20on%20relevance%0Aand%20correctness%2C%20and%20was%20further%20supplemented%20by%20automated%20evaluation%20using%0AGPT-4o%20guided%20by%20a%20domain-adapted%20instruction%20prompt.%20The%20evaluation%20results%0Aindicate%20that%20more%20capable%20models%20tend%20to%20achieve%20higher%20accuracy%2C%20particularly%0Ain%20complex%2C%20tool-integrated%20scenarios.%20In%20addition%2C%20compared%20with%20the%0Afine-tuned%20single-agent%20LLM%2C%20IM-Chat%20demonstrated%20superior%20accuracy%2C%0Aparticularly%20in%20quantitative%20reasoning%2C%20and%20greater%20scalability%20in%20handling%0Amultiple%20information%20sources.%20Overall%2C%20these%20findings%20demonstrate%20the%20viability%0Aof%20multi-agent%20LLM%20systems%20for%20industrial%20knowledge%20workflows%20and%20establish%0AIM-Chat%20as%20a%20scalable%20and%20generalizable%20approach%20to%20AI-assisted%20decision%0Asupport%20in%20manufacturing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15268v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIM-Chat%253A%2520A%2520Multi-agent%2520LLM%2520Framework%2520Integrating%2520Tool-Calling%2520and%250A%2520%2520Diffusion%2520Modeling%2520for%2520Knowledge%2520Transfer%2520in%2520Injection%2520Molding%2520Industry%26entry.906535625%3DJunhyeong%2520Lee%2520and%2520Joon-Young%2520Kim%2520and%2520Heekyu%2520Kim%2520and%2520Inhyo%2520Lee%2520and%2520Seunghwa%2520Ryu%26entry.1292438233%3D%2520%2520The%2520injection%2520molding%2520industry%2520faces%2520critical%2520challenges%2520in%2520preserving%2520and%250Atransferring%2520field%2520knowledge%252C%2520particularly%2520as%2520experienced%2520workers%2520retire%2520and%250Amultilingual%2520barriers%2520hinder%2520effective%2520communication.%2520This%2520study%2520introduces%250AIM-Chat%252C%2520a%2520multi-agent%2520framework%2520based%2520on%2520large%2520language%2520models%2520%2528LLMs%2529%252C%250Adesigned%2520to%2520facilitate%2520knowledge%2520transfer%2520in%2520injection%2520molding.%2520IM-Chat%250Aintegrates%2520both%2520limited%2520documented%2520knowledge%2520%2528e.g.%252C%2520troubleshooting%2520tables%252C%250Amanuals%2529%2520and%2520extensive%2520field%2520data%2520modeled%2520through%2520a%2520data-driven%2520process%250Acondition%2520generator%2520that%2520infers%2520optimal%2520manufacturing%2520settings%2520from%250Aenvironmental%2520inputs%2520such%2520as%2520temperature%2520and%2520humidity%252C%2520enabling%2520robust%2520and%250Acontext-aware%2520task%2520resolution.%2520By%2520adopting%2520a%2520retrieval-augmented%2520generation%250A%2528RAG%2529%2520strategy%2520and%2520tool-calling%2520agents%2520within%2520a%2520modular%2520architecture%252C%2520IM-Chat%250Aensures%2520adaptability%2520without%2520the%2520need%2520for%2520fine-tuning.%2520Performance%2520was%2520assessed%250Aacross%2520100%2520single-tool%2520and%252060%2520hybrid%2520tasks%2520for%2520GPT-4o%252C%2520GPT-4o-mini%252C%2520and%250AGPT-3.5-turbo%2520by%2520domain%2520experts%2520using%2520a%252010-point%2520rubric%2520focused%2520on%2520relevance%250Aand%2520correctness%252C%2520and%2520was%2520further%2520supplemented%2520by%2520automated%2520evaluation%2520using%250AGPT-4o%2520guided%2520by%2520a%2520domain-adapted%2520instruction%2520prompt.%2520The%2520evaluation%2520results%250Aindicate%2520that%2520more%2520capable%2520models%2520tend%2520to%2520achieve%2520higher%2520accuracy%252C%2520particularly%250Ain%2520complex%252C%2520tool-integrated%2520scenarios.%2520In%2520addition%252C%2520compared%2520with%2520the%250Afine-tuned%2520single-agent%2520LLM%252C%2520IM-Chat%2520demonstrated%2520superior%2520accuracy%252C%250Aparticularly%2520in%2520quantitative%2520reasoning%252C%2520and%2520greater%2520scalability%2520in%2520handling%250Amultiple%2520information%2520sources.%2520Overall%252C%2520these%2520findings%2520demonstrate%2520the%2520viability%250Aof%2520multi-agent%2520LLM%2520systems%2520for%2520industrial%2520knowledge%2520workflows%2520and%2520establish%250AIM-Chat%2520as%2520a%2520scalable%2520and%2520generalizable%2520approach%2520to%2520AI-assisted%2520decision%250Asupport%2520in%2520manufacturing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15268v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IM-Chat%3A%20A%20Multi-agent%20LLM%20Framework%20Integrating%20Tool-Calling%20and%0A%20%20Diffusion%20Modeling%20for%20Knowledge%20Transfer%20in%20Injection%20Molding%20Industry&entry.906535625=Junhyeong%20Lee%20and%20Joon-Young%20Kim%20and%20Heekyu%20Kim%20and%20Inhyo%20Lee%20and%20Seunghwa%20Ryu&entry.1292438233=%20%20The%20injection%20molding%20industry%20faces%20critical%20challenges%20in%20preserving%20and%0Atransferring%20field%20knowledge%2C%20particularly%20as%20experienced%20workers%20retire%20and%0Amultilingual%20barriers%20hinder%20effective%20communication.%20This%20study%20introduces%0AIM-Chat%2C%20a%20multi-agent%20framework%20based%20on%20large%20language%20models%20%28LLMs%29%2C%0Adesigned%20to%20facilitate%20knowledge%20transfer%20in%20injection%20molding.%20IM-Chat%0Aintegrates%20both%20limited%20documented%20knowledge%20%28e.g.%2C%20troubleshooting%20tables%2C%0Amanuals%29%20and%20extensive%20field%20data%20modeled%20through%20a%20data-driven%20process%0Acondition%20generator%20that%20infers%20optimal%20manufacturing%20settings%20from%0Aenvironmental%20inputs%20such%20as%20temperature%20and%20humidity%2C%20enabling%20robust%20and%0Acontext-aware%20task%20resolution.%20By%20adopting%20a%20retrieval-augmented%20generation%0A%28RAG%29%20strategy%20and%20tool-calling%20agents%20within%20a%20modular%20architecture%2C%20IM-Chat%0Aensures%20adaptability%20without%20the%20need%20for%20fine-tuning.%20Performance%20was%20assessed%0Aacross%20100%20single-tool%20and%2060%20hybrid%20tasks%20for%20GPT-4o%2C%20GPT-4o-mini%2C%20and%0AGPT-3.5-turbo%20by%20domain%20experts%20using%20a%2010-point%20rubric%20focused%20on%20relevance%0Aand%20correctness%2C%20and%20was%20further%20supplemented%20by%20automated%20evaluation%20using%0AGPT-4o%20guided%20by%20a%20domain-adapted%20instruction%20prompt.%20The%20evaluation%20results%0Aindicate%20that%20more%20capable%20models%20tend%20to%20achieve%20higher%20accuracy%2C%20particularly%0Ain%20complex%2C%20tool-integrated%20scenarios.%20In%20addition%2C%20compared%20with%20the%0Afine-tuned%20single-agent%20LLM%2C%20IM-Chat%20demonstrated%20superior%20accuracy%2C%0Aparticularly%20in%20quantitative%20reasoning%2C%20and%20greater%20scalability%20in%20handling%0Amultiple%20information%20sources.%20Overall%2C%20these%20findings%20demonstrate%20the%20viability%0Aof%20multi-agent%20LLM%20systems%20for%20industrial%20knowledge%20workflows%20and%20establish%0AIM-Chat%20as%20a%20scalable%20and%20generalizable%20approach%20to%20AI-assisted%20decision%0Asupport%20in%20manufacturing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15268v2&entry.124074799=Read"},
{"title": "Integrating Transparent Models, LLMs, and Practitioner-in-the-Loop: A\n  Case of Nonprofit Program Evaluation", "author": "Ji Ma and Albert Casella", "abstract": "  Public and nonprofit organizations often hesitate to adopt AI tools because\nmost models are opaque even though standard approaches typically analyze\naggregate patterns rather than offering actionable, case-level guidance. This\nstudy tests a practitioner-in-the-loop workflow that pairs transparent\ndecision-tree models with large language models (LLMs) to improve predictive\naccuracy, interpretability, and the generation of practical insights. Using\ndata from an ongoing college-success program, we build interpretable decision\ntrees to surface key predictors. We then provide each tree's structure to an\nLLM, enabling it to reproduce case-level predictions grounded in the\ntransparent models. Practitioners participate throughout feature engineering,\nmodel design, explanation review, and usability assessment, ensuring that field\nexpertise informs the analysis at every stage. Results show that integrating\ntransparent models, LLMs, and practitioner input yields accurate, trustworthy,\nand actionable case-level evaluations, offering a viable pathway for\nresponsible AI adoption in the public and nonprofit sectors.\n", "link": "http://arxiv.org/abs/2510.19799v1", "date": "2025-10-22", "relevancy": 2.0341, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5111}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5111}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrating%20Transparent%20Models%2C%20LLMs%2C%20and%20Practitioner-in-the-Loop%3A%20A%0A%20%20Case%20of%20Nonprofit%20Program%20Evaluation&body=Title%3A%20Integrating%20Transparent%20Models%2C%20LLMs%2C%20and%20Practitioner-in-the-Loop%3A%20A%0A%20%20Case%20of%20Nonprofit%20Program%20Evaluation%0AAuthor%3A%20Ji%20Ma%20and%20Albert%20Casella%0AAbstract%3A%20%20%20Public%20and%20nonprofit%20organizations%20often%20hesitate%20to%20adopt%20AI%20tools%20because%0Amost%20models%20are%20opaque%20even%20though%20standard%20approaches%20typically%20analyze%0Aaggregate%20patterns%20rather%20than%20offering%20actionable%2C%20case-level%20guidance.%20This%0Astudy%20tests%20a%20practitioner-in-the-loop%20workflow%20that%20pairs%20transparent%0Adecision-tree%20models%20with%20large%20language%20models%20%28LLMs%29%20to%20improve%20predictive%0Aaccuracy%2C%20interpretability%2C%20and%20the%20generation%20of%20practical%20insights.%20Using%0Adata%20from%20an%20ongoing%20college-success%20program%2C%20we%20build%20interpretable%20decision%0Atrees%20to%20surface%20key%20predictors.%20We%20then%20provide%20each%20tree%27s%20structure%20to%20an%0ALLM%2C%20enabling%20it%20to%20reproduce%20case-level%20predictions%20grounded%20in%20the%0Atransparent%20models.%20Practitioners%20participate%20throughout%20feature%20engineering%2C%0Amodel%20design%2C%20explanation%20review%2C%20and%20usability%20assessment%2C%20ensuring%20that%20field%0Aexpertise%20informs%20the%20analysis%20at%20every%20stage.%20Results%20show%20that%20integrating%0Atransparent%20models%2C%20LLMs%2C%20and%20practitioner%20input%20yields%20accurate%2C%20trustworthy%2C%0Aand%20actionable%20case-level%20evaluations%2C%20offering%20a%20viable%20pathway%20for%0Aresponsible%20AI%20adoption%20in%20the%20public%20and%20nonprofit%20sectors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19799v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrating%2520Transparent%2520Models%252C%2520LLMs%252C%2520and%2520Practitioner-in-the-Loop%253A%2520A%250A%2520%2520Case%2520of%2520Nonprofit%2520Program%2520Evaluation%26entry.906535625%3DJi%2520Ma%2520and%2520Albert%2520Casella%26entry.1292438233%3D%2520%2520Public%2520and%2520nonprofit%2520organizations%2520often%2520hesitate%2520to%2520adopt%2520AI%2520tools%2520because%250Amost%2520models%2520are%2520opaque%2520even%2520though%2520standard%2520approaches%2520typically%2520analyze%250Aaggregate%2520patterns%2520rather%2520than%2520offering%2520actionable%252C%2520case-level%2520guidance.%2520This%250Astudy%2520tests%2520a%2520practitioner-in-the-loop%2520workflow%2520that%2520pairs%2520transparent%250Adecision-tree%2520models%2520with%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520improve%2520predictive%250Aaccuracy%252C%2520interpretability%252C%2520and%2520the%2520generation%2520of%2520practical%2520insights.%2520Using%250Adata%2520from%2520an%2520ongoing%2520college-success%2520program%252C%2520we%2520build%2520interpretable%2520decision%250Atrees%2520to%2520surface%2520key%2520predictors.%2520We%2520then%2520provide%2520each%2520tree%2527s%2520structure%2520to%2520an%250ALLM%252C%2520enabling%2520it%2520to%2520reproduce%2520case-level%2520predictions%2520grounded%2520in%2520the%250Atransparent%2520models.%2520Practitioners%2520participate%2520throughout%2520feature%2520engineering%252C%250Amodel%2520design%252C%2520explanation%2520review%252C%2520and%2520usability%2520assessment%252C%2520ensuring%2520that%2520field%250Aexpertise%2520informs%2520the%2520analysis%2520at%2520every%2520stage.%2520Results%2520show%2520that%2520integrating%250Atransparent%2520models%252C%2520LLMs%252C%2520and%2520practitioner%2520input%2520yields%2520accurate%252C%2520trustworthy%252C%250Aand%2520actionable%2520case-level%2520evaluations%252C%2520offering%2520a%2520viable%2520pathway%2520for%250Aresponsible%2520AI%2520adoption%2520in%2520the%2520public%2520and%2520nonprofit%2520sectors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19799v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20Transparent%20Models%2C%20LLMs%2C%20and%20Practitioner-in-the-Loop%3A%20A%0A%20%20Case%20of%20Nonprofit%20Program%20Evaluation&entry.906535625=Ji%20Ma%20and%20Albert%20Casella&entry.1292438233=%20%20Public%20and%20nonprofit%20organizations%20often%20hesitate%20to%20adopt%20AI%20tools%20because%0Amost%20models%20are%20opaque%20even%20though%20standard%20approaches%20typically%20analyze%0Aaggregate%20patterns%20rather%20than%20offering%20actionable%2C%20case-level%20guidance.%20This%0Astudy%20tests%20a%20practitioner-in-the-loop%20workflow%20that%20pairs%20transparent%0Adecision-tree%20models%20with%20large%20language%20models%20%28LLMs%29%20to%20improve%20predictive%0Aaccuracy%2C%20interpretability%2C%20and%20the%20generation%20of%20practical%20insights.%20Using%0Adata%20from%20an%20ongoing%20college-success%20program%2C%20we%20build%20interpretable%20decision%0Atrees%20to%20surface%20key%20predictors.%20We%20then%20provide%20each%20tree%27s%20structure%20to%20an%0ALLM%2C%20enabling%20it%20to%20reproduce%20case-level%20predictions%20grounded%20in%20the%0Atransparent%20models.%20Practitioners%20participate%20throughout%20feature%20engineering%2C%0Amodel%20design%2C%20explanation%20review%2C%20and%20usability%20assessment%2C%20ensuring%20that%20field%0Aexpertise%20informs%20the%20analysis%20at%20every%20stage.%20Results%20show%20that%20integrating%0Atransparent%20models%2C%20LLMs%2C%20and%20practitioner%20input%20yields%20accurate%2C%20trustworthy%2C%0Aand%20actionable%20case-level%20evaluations%2C%20offering%20a%20viable%20pathway%20for%0Aresponsible%20AI%20adoption%20in%20the%20public%20and%20nonprofit%20sectors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19799v1&entry.124074799=Read"},
{"title": "RLIE: Rule Generation with Logistic Regression, Iterative Refinement,\n  and Evaluation for Large Language Models", "author": "Yang Yang and Hua XU and Zhangyi Hu and Yutao Yue", "abstract": "  Large Language Models (LLMs) can propose rules in natural language,\nsidestepping the need for a predefined predicate space in traditional rule\nlearning. Yet many LLM-based approaches ignore interactions among rules, and\nthe opportunity to couple LLMs with probabilistic rule learning for robust\ninference remains underexplored. We present RLIE, a unified framework that\nintegrates LLMs with probabilistic modeling to learn a set of weighted rules.\nRLIE has four stages: (1) Rule generation, where an LLM proposes and filters\ncandidates; (2) Logistic regression, which learns probabilistic weights for\nglobal selection and calibration; (3) Iterative refinement, which updates the\nrule set using prediction errors; and (4) Evaluation, which compares the\nweighted rule set as a direct classifier with methods that inject rules into an\nLLM. We evaluate multiple inference strategies on real-world datasets. Applying\nrules directly with their learned weights yields superior performance, whereas\nprompting LLMs with the rules, weights, and logistic-model outputs surprisingly\ndegrades accuracy. This supports the view that LLMs excel at semantic\ngeneration and interpretation but are less reliable for precise probabilistic\nintegration. RLIE clarifies the potential and limitations of LLMs for inductive\nreasoning and couples them with classic probabilistic rule combination methods\nto enable more reliable neuro-symbolic reasoning.\n", "link": "http://arxiv.org/abs/2510.19698v1", "date": "2025-10-22", "relevancy": 2.0264, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5169}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5135}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RLIE%3A%20Rule%20Generation%20with%20Logistic%20Regression%2C%20Iterative%20Refinement%2C%0A%20%20and%20Evaluation%20for%20Large%20Language%20Models&body=Title%3A%20RLIE%3A%20Rule%20Generation%20with%20Logistic%20Regression%2C%20Iterative%20Refinement%2C%0A%20%20and%20Evaluation%20for%20Large%20Language%20Models%0AAuthor%3A%20Yang%20Yang%20and%20Hua%20XU%20and%20Zhangyi%20Hu%20and%20Yutao%20Yue%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20can%20propose%20rules%20in%20natural%20language%2C%0Asidestepping%20the%20need%20for%20a%20predefined%20predicate%20space%20in%20traditional%20rule%0Alearning.%20Yet%20many%20LLM-based%20approaches%20ignore%20interactions%20among%20rules%2C%20and%0Athe%20opportunity%20to%20couple%20LLMs%20with%20probabilistic%20rule%20learning%20for%20robust%0Ainference%20remains%20underexplored.%20We%20present%20RLIE%2C%20a%20unified%20framework%20that%0Aintegrates%20LLMs%20with%20probabilistic%20modeling%20to%20learn%20a%20set%20of%20weighted%20rules.%0ARLIE%20has%20four%20stages%3A%20%281%29%20Rule%20generation%2C%20where%20an%20LLM%20proposes%20and%20filters%0Acandidates%3B%20%282%29%20Logistic%20regression%2C%20which%20learns%20probabilistic%20weights%20for%0Aglobal%20selection%20and%20calibration%3B%20%283%29%20Iterative%20refinement%2C%20which%20updates%20the%0Arule%20set%20using%20prediction%20errors%3B%20and%20%284%29%20Evaluation%2C%20which%20compares%20the%0Aweighted%20rule%20set%20as%20a%20direct%20classifier%20with%20methods%20that%20inject%20rules%20into%20an%0ALLM.%20We%20evaluate%20multiple%20inference%20strategies%20on%20real-world%20datasets.%20Applying%0Arules%20directly%20with%20their%20learned%20weights%20yields%20superior%20performance%2C%20whereas%0Aprompting%20LLMs%20with%20the%20rules%2C%20weights%2C%20and%20logistic-model%20outputs%20surprisingly%0Adegrades%20accuracy.%20This%20supports%20the%20view%20that%20LLMs%20excel%20at%20semantic%0Ageneration%20and%20interpretation%20but%20are%20less%20reliable%20for%20precise%20probabilistic%0Aintegration.%20RLIE%20clarifies%20the%20potential%20and%20limitations%20of%20LLMs%20for%20inductive%0Areasoning%20and%20couples%20them%20with%20classic%20probabilistic%20rule%20combination%20methods%0Ato%20enable%20more%20reliable%20neuro-symbolic%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19698v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRLIE%253A%2520Rule%2520Generation%2520with%2520Logistic%2520Regression%252C%2520Iterative%2520Refinement%252C%250A%2520%2520and%2520Evaluation%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DYang%2520Yang%2520and%2520Hua%2520XU%2520and%2520Zhangyi%2520Hu%2520and%2520Yutao%2520Yue%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520can%2520propose%2520rules%2520in%2520natural%2520language%252C%250Asidestepping%2520the%2520need%2520for%2520a%2520predefined%2520predicate%2520space%2520in%2520traditional%2520rule%250Alearning.%2520Yet%2520many%2520LLM-based%2520approaches%2520ignore%2520interactions%2520among%2520rules%252C%2520and%250Athe%2520opportunity%2520to%2520couple%2520LLMs%2520with%2520probabilistic%2520rule%2520learning%2520for%2520robust%250Ainference%2520remains%2520underexplored.%2520We%2520present%2520RLIE%252C%2520a%2520unified%2520framework%2520that%250Aintegrates%2520LLMs%2520with%2520probabilistic%2520modeling%2520to%2520learn%2520a%2520set%2520of%2520weighted%2520rules.%250ARLIE%2520has%2520four%2520stages%253A%2520%25281%2529%2520Rule%2520generation%252C%2520where%2520an%2520LLM%2520proposes%2520and%2520filters%250Acandidates%253B%2520%25282%2529%2520Logistic%2520regression%252C%2520which%2520learns%2520probabilistic%2520weights%2520for%250Aglobal%2520selection%2520and%2520calibration%253B%2520%25283%2529%2520Iterative%2520refinement%252C%2520which%2520updates%2520the%250Arule%2520set%2520using%2520prediction%2520errors%253B%2520and%2520%25284%2529%2520Evaluation%252C%2520which%2520compares%2520the%250Aweighted%2520rule%2520set%2520as%2520a%2520direct%2520classifier%2520with%2520methods%2520that%2520inject%2520rules%2520into%2520an%250ALLM.%2520We%2520evaluate%2520multiple%2520inference%2520strategies%2520on%2520real-world%2520datasets.%2520Applying%250Arules%2520directly%2520with%2520their%2520learned%2520weights%2520yields%2520superior%2520performance%252C%2520whereas%250Aprompting%2520LLMs%2520with%2520the%2520rules%252C%2520weights%252C%2520and%2520logistic-model%2520outputs%2520surprisingly%250Adegrades%2520accuracy.%2520This%2520supports%2520the%2520view%2520that%2520LLMs%2520excel%2520at%2520semantic%250Ageneration%2520and%2520interpretation%2520but%2520are%2520less%2520reliable%2520for%2520precise%2520probabilistic%250Aintegration.%2520RLIE%2520clarifies%2520the%2520potential%2520and%2520limitations%2520of%2520LLMs%2520for%2520inductive%250Areasoning%2520and%2520couples%2520them%2520with%2520classic%2520probabilistic%2520rule%2520combination%2520methods%250Ato%2520enable%2520more%2520reliable%2520neuro-symbolic%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19698v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RLIE%3A%20Rule%20Generation%20with%20Logistic%20Regression%2C%20Iterative%20Refinement%2C%0A%20%20and%20Evaluation%20for%20Large%20Language%20Models&entry.906535625=Yang%20Yang%20and%20Hua%20XU%20and%20Zhangyi%20Hu%20and%20Yutao%20Yue&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20can%20propose%20rules%20in%20natural%20language%2C%0Asidestepping%20the%20need%20for%20a%20predefined%20predicate%20space%20in%20traditional%20rule%0Alearning.%20Yet%20many%20LLM-based%20approaches%20ignore%20interactions%20among%20rules%2C%20and%0Athe%20opportunity%20to%20couple%20LLMs%20with%20probabilistic%20rule%20learning%20for%20robust%0Ainference%20remains%20underexplored.%20We%20present%20RLIE%2C%20a%20unified%20framework%20that%0Aintegrates%20LLMs%20with%20probabilistic%20modeling%20to%20learn%20a%20set%20of%20weighted%20rules.%0ARLIE%20has%20four%20stages%3A%20%281%29%20Rule%20generation%2C%20where%20an%20LLM%20proposes%20and%20filters%0Acandidates%3B%20%282%29%20Logistic%20regression%2C%20which%20learns%20probabilistic%20weights%20for%0Aglobal%20selection%20and%20calibration%3B%20%283%29%20Iterative%20refinement%2C%20which%20updates%20the%0Arule%20set%20using%20prediction%20errors%3B%20and%20%284%29%20Evaluation%2C%20which%20compares%20the%0Aweighted%20rule%20set%20as%20a%20direct%20classifier%20with%20methods%20that%20inject%20rules%20into%20an%0ALLM.%20We%20evaluate%20multiple%20inference%20strategies%20on%20real-world%20datasets.%20Applying%0Arules%20directly%20with%20their%20learned%20weights%20yields%20superior%20performance%2C%20whereas%0Aprompting%20LLMs%20with%20the%20rules%2C%20weights%2C%20and%20logistic-model%20outputs%20surprisingly%0Adegrades%20accuracy.%20This%20supports%20the%20view%20that%20LLMs%20excel%20at%20semantic%0Ageneration%20and%20interpretation%20but%20are%20less%20reliable%20for%20precise%20probabilistic%0Aintegration.%20RLIE%20clarifies%20the%20potential%20and%20limitations%20of%20LLMs%20for%20inductive%0Areasoning%20and%20couples%20them%20with%20classic%20probabilistic%20rule%20combination%20methods%0Ato%20enable%20more%20reliable%20neuro-symbolic%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19698v1&entry.124074799=Read"},
{"title": "Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement\n  Learning for General LLM Reasoning", "author": "Yang Zhou and Sunzhu Li and Shunyu Liu and Wenkai Fang and Kongcheng Zhang and Jiale Zhao and Jingwen Yang and Yihe Zhou and Jianwei Lv and Tongya Zheng and Hengtong Lu and Wei Chen and Yan Xie and Mingli Song", "abstract": "  Recent advances in Large Language Models (LLMs) have underscored the\npotential of Reinforcement Learning (RL) to facilitate the emergence of\nreasoning capabilities. Despite the encouraging results, a fundamental dilemma\npersists as RL improvement relies on learning from high-quality samples, yet\nthe exploration for such samples remains bounded by the inherent limitations of\nLLMs. This, in effect, creates an undesirable cycle in which what cannot be\nexplored cannot be learned. In this work, we propose Rubric-Scaffolded\nReinforcement Learning (RuscaRL), a novel instructional scaffolding framework\ndesigned to break the exploration bottleneck for general LLM reasoning.\nSpecifically, RuscaRL introduces checklist-style rubrics as (1) explicit\nscaffolding for exploration during rollout generation, where different rubrics\nare provided as external guidance within task instructions to steer diverse\nhigh-quality responses. This guidance is gradually decayed over time,\nencouraging the model to internalize the underlying reasoning patterns; (2)\nverifiable rewards for exploitation during model training, where we can obtain\nrobust LLM-as-a-Judge scores using rubrics as references, enabling effective RL\non general reasoning tasks. Extensive experiments demonstrate the superiority\nof the proposed RuscaRL across various benchmarks, effectively expanding\nreasoning boundaries under the Best-of-N evaluation. Notably, RuscaRL\nsignificantly boosts Qwen2.5-7B-Instruct from 23.6 to 50.3 on HealthBench-500,\nsurpassing GPT-4.1. Furthermore, our fine-tuned variant on\nQwen3-30B-A3B-Instruct achieves 61.1 on HealthBench-500, outperforming leading\nLLMs including OpenAI-o3. Our code is available at\nhttps://github.com/IANNXANG/RuscaRL.\n", "link": "http://arxiv.org/abs/2508.16949v5", "date": "2025-10-22", "relevancy": 2.0258, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5319}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5013}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Breaking%20the%20Exploration%20Bottleneck%3A%20Rubric-Scaffolded%20Reinforcement%0A%20%20Learning%20for%20General%20LLM%20Reasoning&body=Title%3A%20Breaking%20the%20Exploration%20Bottleneck%3A%20Rubric-Scaffolded%20Reinforcement%0A%20%20Learning%20for%20General%20LLM%20Reasoning%0AAuthor%3A%20Yang%20Zhou%20and%20Sunzhu%20Li%20and%20Shunyu%20Liu%20and%20Wenkai%20Fang%20and%20Kongcheng%20Zhang%20and%20Jiale%20Zhao%20and%20Jingwen%20Yang%20and%20Yihe%20Zhou%20and%20Jianwei%20Lv%20and%20Tongya%20Zheng%20and%20Hengtong%20Lu%20and%20Wei%20Chen%20and%20Yan%20Xie%20and%20Mingli%20Song%0AAbstract%3A%20%20%20Recent%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20underscored%20the%0Apotential%20of%20Reinforcement%20Learning%20%28RL%29%20to%20facilitate%20the%20emergence%20of%0Areasoning%20capabilities.%20Despite%20the%20encouraging%20results%2C%20a%20fundamental%20dilemma%0Apersists%20as%20RL%20improvement%20relies%20on%20learning%20from%20high-quality%20samples%2C%20yet%0Athe%20exploration%20for%20such%20samples%20remains%20bounded%20by%20the%20inherent%20limitations%20of%0ALLMs.%20This%2C%20in%20effect%2C%20creates%20an%20undesirable%20cycle%20in%20which%20what%20cannot%20be%0Aexplored%20cannot%20be%20learned.%20In%20this%20work%2C%20we%20propose%20Rubric-Scaffolded%0AReinforcement%20Learning%20%28RuscaRL%29%2C%20a%20novel%20instructional%20scaffolding%20framework%0Adesigned%20to%20break%20the%20exploration%20bottleneck%20for%20general%20LLM%20reasoning.%0ASpecifically%2C%20RuscaRL%20introduces%20checklist-style%20rubrics%20as%20%281%29%20explicit%0Ascaffolding%20for%20exploration%20during%20rollout%20generation%2C%20where%20different%20rubrics%0Aare%20provided%20as%20external%20guidance%20within%20task%20instructions%20to%20steer%20diverse%0Ahigh-quality%20responses.%20This%20guidance%20is%20gradually%20decayed%20over%20time%2C%0Aencouraging%20the%20model%20to%20internalize%20the%20underlying%20reasoning%20patterns%3B%20%282%29%0Averifiable%20rewards%20for%20exploitation%20during%20model%20training%2C%20where%20we%20can%20obtain%0Arobust%20LLM-as-a-Judge%20scores%20using%20rubrics%20as%20references%2C%20enabling%20effective%20RL%0Aon%20general%20reasoning%20tasks.%20Extensive%20experiments%20demonstrate%20the%20superiority%0Aof%20the%20proposed%20RuscaRL%20across%20various%20benchmarks%2C%20effectively%20expanding%0Areasoning%20boundaries%20under%20the%20Best-of-N%20evaluation.%20Notably%2C%20RuscaRL%0Asignificantly%20boosts%20Qwen2.5-7B-Instruct%20from%2023.6%20to%2050.3%20on%20HealthBench-500%2C%0Asurpassing%20GPT-4.1.%20Furthermore%2C%20our%20fine-tuned%20variant%20on%0AQwen3-30B-A3B-Instruct%20achieves%2061.1%20on%20HealthBench-500%2C%20outperforming%20leading%0ALLMs%20including%20OpenAI-o3.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/IANNXANG/RuscaRL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16949v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreaking%2520the%2520Exploration%2520Bottleneck%253A%2520Rubric-Scaffolded%2520Reinforcement%250A%2520%2520Learning%2520for%2520General%2520LLM%2520Reasoning%26entry.906535625%3DYang%2520Zhou%2520and%2520Sunzhu%2520Li%2520and%2520Shunyu%2520Liu%2520and%2520Wenkai%2520Fang%2520and%2520Kongcheng%2520Zhang%2520and%2520Jiale%2520Zhao%2520and%2520Jingwen%2520Yang%2520and%2520Yihe%2520Zhou%2520and%2520Jianwei%2520Lv%2520and%2520Tongya%2520Zheng%2520and%2520Hengtong%2520Lu%2520and%2520Wei%2520Chen%2520and%2520Yan%2520Xie%2520and%2520Mingli%2520Song%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520underscored%2520the%250Apotential%2520of%2520Reinforcement%2520Learning%2520%2528RL%2529%2520to%2520facilitate%2520the%2520emergence%2520of%250Areasoning%2520capabilities.%2520Despite%2520the%2520encouraging%2520results%252C%2520a%2520fundamental%2520dilemma%250Apersists%2520as%2520RL%2520improvement%2520relies%2520on%2520learning%2520from%2520high-quality%2520samples%252C%2520yet%250Athe%2520exploration%2520for%2520such%2520samples%2520remains%2520bounded%2520by%2520the%2520inherent%2520limitations%2520of%250ALLMs.%2520This%252C%2520in%2520effect%252C%2520creates%2520an%2520undesirable%2520cycle%2520in%2520which%2520what%2520cannot%2520be%250Aexplored%2520cannot%2520be%2520learned.%2520In%2520this%2520work%252C%2520we%2520propose%2520Rubric-Scaffolded%250AReinforcement%2520Learning%2520%2528RuscaRL%2529%252C%2520a%2520novel%2520instructional%2520scaffolding%2520framework%250Adesigned%2520to%2520break%2520the%2520exploration%2520bottleneck%2520for%2520general%2520LLM%2520reasoning.%250ASpecifically%252C%2520RuscaRL%2520introduces%2520checklist-style%2520rubrics%2520as%2520%25281%2529%2520explicit%250Ascaffolding%2520for%2520exploration%2520during%2520rollout%2520generation%252C%2520where%2520different%2520rubrics%250Aare%2520provided%2520as%2520external%2520guidance%2520within%2520task%2520instructions%2520to%2520steer%2520diverse%250Ahigh-quality%2520responses.%2520This%2520guidance%2520is%2520gradually%2520decayed%2520over%2520time%252C%250Aencouraging%2520the%2520model%2520to%2520internalize%2520the%2520underlying%2520reasoning%2520patterns%253B%2520%25282%2529%250Averifiable%2520rewards%2520for%2520exploitation%2520during%2520model%2520training%252C%2520where%2520we%2520can%2520obtain%250Arobust%2520LLM-as-a-Judge%2520scores%2520using%2520rubrics%2520as%2520references%252C%2520enabling%2520effective%2520RL%250Aon%2520general%2520reasoning%2520tasks.%2520Extensive%2520experiments%2520demonstrate%2520the%2520superiority%250Aof%2520the%2520proposed%2520RuscaRL%2520across%2520various%2520benchmarks%252C%2520effectively%2520expanding%250Areasoning%2520boundaries%2520under%2520the%2520Best-of-N%2520evaluation.%2520Notably%252C%2520RuscaRL%250Asignificantly%2520boosts%2520Qwen2.5-7B-Instruct%2520from%252023.6%2520to%252050.3%2520on%2520HealthBench-500%252C%250Asurpassing%2520GPT-4.1.%2520Furthermore%252C%2520our%2520fine-tuned%2520variant%2520on%250AQwen3-30B-A3B-Instruct%2520achieves%252061.1%2520on%2520HealthBench-500%252C%2520outperforming%2520leading%250ALLMs%2520including%2520OpenAI-o3.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/IANNXANG/RuscaRL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16949v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breaking%20the%20Exploration%20Bottleneck%3A%20Rubric-Scaffolded%20Reinforcement%0A%20%20Learning%20for%20General%20LLM%20Reasoning&entry.906535625=Yang%20Zhou%20and%20Sunzhu%20Li%20and%20Shunyu%20Liu%20and%20Wenkai%20Fang%20and%20Kongcheng%20Zhang%20and%20Jiale%20Zhao%20and%20Jingwen%20Yang%20and%20Yihe%20Zhou%20and%20Jianwei%20Lv%20and%20Tongya%20Zheng%20and%20Hengtong%20Lu%20and%20Wei%20Chen%20and%20Yan%20Xie%20and%20Mingli%20Song&entry.1292438233=%20%20Recent%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20underscored%20the%0Apotential%20of%20Reinforcement%20Learning%20%28RL%29%20to%20facilitate%20the%20emergence%20of%0Areasoning%20capabilities.%20Despite%20the%20encouraging%20results%2C%20a%20fundamental%20dilemma%0Apersists%20as%20RL%20improvement%20relies%20on%20learning%20from%20high-quality%20samples%2C%20yet%0Athe%20exploration%20for%20such%20samples%20remains%20bounded%20by%20the%20inherent%20limitations%20of%0ALLMs.%20This%2C%20in%20effect%2C%20creates%20an%20undesirable%20cycle%20in%20which%20what%20cannot%20be%0Aexplored%20cannot%20be%20learned.%20In%20this%20work%2C%20we%20propose%20Rubric-Scaffolded%0AReinforcement%20Learning%20%28RuscaRL%29%2C%20a%20novel%20instructional%20scaffolding%20framework%0Adesigned%20to%20break%20the%20exploration%20bottleneck%20for%20general%20LLM%20reasoning.%0ASpecifically%2C%20RuscaRL%20introduces%20checklist-style%20rubrics%20as%20%281%29%20explicit%0Ascaffolding%20for%20exploration%20during%20rollout%20generation%2C%20where%20different%20rubrics%0Aare%20provided%20as%20external%20guidance%20within%20task%20instructions%20to%20steer%20diverse%0Ahigh-quality%20responses.%20This%20guidance%20is%20gradually%20decayed%20over%20time%2C%0Aencouraging%20the%20model%20to%20internalize%20the%20underlying%20reasoning%20patterns%3B%20%282%29%0Averifiable%20rewards%20for%20exploitation%20during%20model%20training%2C%20where%20we%20can%20obtain%0Arobust%20LLM-as-a-Judge%20scores%20using%20rubrics%20as%20references%2C%20enabling%20effective%20RL%0Aon%20general%20reasoning%20tasks.%20Extensive%20experiments%20demonstrate%20the%20superiority%0Aof%20the%20proposed%20RuscaRL%20across%20various%20benchmarks%2C%20effectively%20expanding%0Areasoning%20boundaries%20under%20the%20Best-of-N%20evaluation.%20Notably%2C%20RuscaRL%0Asignificantly%20boosts%20Qwen2.5-7B-Instruct%20from%2023.6%20to%2050.3%20on%20HealthBench-500%2C%0Asurpassing%20GPT-4.1.%20Furthermore%2C%20our%20fine-tuned%20variant%20on%0AQwen3-30B-A3B-Instruct%20achieves%2061.1%20on%20HealthBench-500%2C%20outperforming%20leading%0ALLMs%20including%20OpenAI-o3.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/IANNXANG/RuscaRL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16949v5&entry.124074799=Read"},
{"title": "Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement\n  Learning for General LLM Reasoning", "author": "Yang Zhou and Sunzhu Li and Shunyu Liu and Wenkai Fang and Kongcheng Zhang and Jiale Zhao and Jingwen Yang and Yihe Zhou and Jianwei Lv and Tongya Zheng and Hengtong Lu and Wei Chen and Yan Xie and Mingli Song", "abstract": "  Recent advances in Large Language Models (LLMs) have underscored the\npotential of Reinforcement Learning (RL) to facilitate the emergence of\nreasoning capabilities. Despite the encouraging results, a fundamental dilemma\npersists as RL improvement relies on learning from high-quality samples, yet\nthe exploration for such samples remains bounded by the inherent limitations of\nLLMs. This, in effect, creates an undesirable cycle in which what cannot be\nexplored cannot be learned. In this work, we propose Rubric-Scaffolded\nReinforcement Learning (RuscaRL), a novel instructional scaffolding framework\ndesigned to break the exploration bottleneck for general LLM reasoning.\nSpecifically, RuscaRL introduces checklist-style rubrics as (1) explicit\nscaffolding for exploration during rollout generation, where different rubrics\nare provided as external guidance within task instructions to steer diverse\nhigh-quality responses. This guidance is gradually decayed over time,\nencouraging the model to internalize the underlying reasoning patterns; (2)\nverifiable rewards for exploitation during model training, where we can obtain\nrobust LLM-as-a-Judge scores using rubrics as references, enabling effective RL\non general reasoning tasks. Extensive experiments demonstrate the superiority\nof the proposed RuscaRL across various benchmarks, effectively expanding\nreasoning boundaries under the Best-of-N evaluation. Notably, RuscaRL\nsignificantly boosts Qwen2.5-7B-Instruct from 23.6 to 50.3 on HealthBench-500,\nsurpassing GPT-4.1. Furthermore, our fine-tuned variant on\nQwen3-30B-A3B-Instruct achieves 61.1 on HealthBench-500, outperforming leading\nLLMs including OpenAI-o3. Our code is available at\nhttps://github.com/IANNXANG/RuscaRL.\n", "link": "http://arxiv.org/abs/2508.16949v5", "date": "2025-10-22", "relevancy": 2.0257, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5319}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5013}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Breaking%20the%20Exploration%20Bottleneck%3A%20Rubric-Scaffolded%20Reinforcement%0A%20%20Learning%20for%20General%20LLM%20Reasoning&body=Title%3A%20Breaking%20the%20Exploration%20Bottleneck%3A%20Rubric-Scaffolded%20Reinforcement%0A%20%20Learning%20for%20General%20LLM%20Reasoning%0AAuthor%3A%20Yang%20Zhou%20and%20Sunzhu%20Li%20and%20Shunyu%20Liu%20and%20Wenkai%20Fang%20and%20Kongcheng%20Zhang%20and%20Jiale%20Zhao%20and%20Jingwen%20Yang%20and%20Yihe%20Zhou%20and%20Jianwei%20Lv%20and%20Tongya%20Zheng%20and%20Hengtong%20Lu%20and%20Wei%20Chen%20and%20Yan%20Xie%20and%20Mingli%20Song%0AAbstract%3A%20%20%20Recent%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20underscored%20the%0Apotential%20of%20Reinforcement%20Learning%20%28RL%29%20to%20facilitate%20the%20emergence%20of%0Areasoning%20capabilities.%20Despite%20the%20encouraging%20results%2C%20a%20fundamental%20dilemma%0Apersists%20as%20RL%20improvement%20relies%20on%20learning%20from%20high-quality%20samples%2C%20yet%0Athe%20exploration%20for%20such%20samples%20remains%20bounded%20by%20the%20inherent%20limitations%20of%0ALLMs.%20This%2C%20in%20effect%2C%20creates%20an%20undesirable%20cycle%20in%20which%20what%20cannot%20be%0Aexplored%20cannot%20be%20learned.%20In%20this%20work%2C%20we%20propose%20Rubric-Scaffolded%0AReinforcement%20Learning%20%28RuscaRL%29%2C%20a%20novel%20instructional%20scaffolding%20framework%0Adesigned%20to%20break%20the%20exploration%20bottleneck%20for%20general%20LLM%20reasoning.%0ASpecifically%2C%20RuscaRL%20introduces%20checklist-style%20rubrics%20as%20%281%29%20explicit%0Ascaffolding%20for%20exploration%20during%20rollout%20generation%2C%20where%20different%20rubrics%0Aare%20provided%20as%20external%20guidance%20within%20task%20instructions%20to%20steer%20diverse%0Ahigh-quality%20responses.%20This%20guidance%20is%20gradually%20decayed%20over%20time%2C%0Aencouraging%20the%20model%20to%20internalize%20the%20underlying%20reasoning%20patterns%3B%20%282%29%0Averifiable%20rewards%20for%20exploitation%20during%20model%20training%2C%20where%20we%20can%20obtain%0Arobust%20LLM-as-a-Judge%20scores%20using%20rubrics%20as%20references%2C%20enabling%20effective%20RL%0Aon%20general%20reasoning%20tasks.%20Extensive%20experiments%20demonstrate%20the%20superiority%0Aof%20the%20proposed%20RuscaRL%20across%20various%20benchmarks%2C%20effectively%20expanding%0Areasoning%20boundaries%20under%20the%20Best-of-N%20evaluation.%20Notably%2C%20RuscaRL%0Asignificantly%20boosts%20Qwen2.5-7B-Instruct%20from%2023.6%20to%2050.3%20on%20HealthBench-500%2C%0Asurpassing%20GPT-4.1.%20Furthermore%2C%20our%20fine-tuned%20variant%20on%0AQwen3-30B-A3B-Instruct%20achieves%2061.1%20on%20HealthBench-500%2C%20outperforming%20leading%0ALLMs%20including%20OpenAI-o3.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/IANNXANG/RuscaRL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16949v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreaking%2520the%2520Exploration%2520Bottleneck%253A%2520Rubric-Scaffolded%2520Reinforcement%250A%2520%2520Learning%2520for%2520General%2520LLM%2520Reasoning%26entry.906535625%3DYang%2520Zhou%2520and%2520Sunzhu%2520Li%2520and%2520Shunyu%2520Liu%2520and%2520Wenkai%2520Fang%2520and%2520Kongcheng%2520Zhang%2520and%2520Jiale%2520Zhao%2520and%2520Jingwen%2520Yang%2520and%2520Yihe%2520Zhou%2520and%2520Jianwei%2520Lv%2520and%2520Tongya%2520Zheng%2520and%2520Hengtong%2520Lu%2520and%2520Wei%2520Chen%2520and%2520Yan%2520Xie%2520and%2520Mingli%2520Song%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520underscored%2520the%250Apotential%2520of%2520Reinforcement%2520Learning%2520%2528RL%2529%2520to%2520facilitate%2520the%2520emergence%2520of%250Areasoning%2520capabilities.%2520Despite%2520the%2520encouraging%2520results%252C%2520a%2520fundamental%2520dilemma%250Apersists%2520as%2520RL%2520improvement%2520relies%2520on%2520learning%2520from%2520high-quality%2520samples%252C%2520yet%250Athe%2520exploration%2520for%2520such%2520samples%2520remains%2520bounded%2520by%2520the%2520inherent%2520limitations%2520of%250ALLMs.%2520This%252C%2520in%2520effect%252C%2520creates%2520an%2520undesirable%2520cycle%2520in%2520which%2520what%2520cannot%2520be%250Aexplored%2520cannot%2520be%2520learned.%2520In%2520this%2520work%252C%2520we%2520propose%2520Rubric-Scaffolded%250AReinforcement%2520Learning%2520%2528RuscaRL%2529%252C%2520a%2520novel%2520instructional%2520scaffolding%2520framework%250Adesigned%2520to%2520break%2520the%2520exploration%2520bottleneck%2520for%2520general%2520LLM%2520reasoning.%250ASpecifically%252C%2520RuscaRL%2520introduces%2520checklist-style%2520rubrics%2520as%2520%25281%2529%2520explicit%250Ascaffolding%2520for%2520exploration%2520during%2520rollout%2520generation%252C%2520where%2520different%2520rubrics%250Aare%2520provided%2520as%2520external%2520guidance%2520within%2520task%2520instructions%2520to%2520steer%2520diverse%250Ahigh-quality%2520responses.%2520This%2520guidance%2520is%2520gradually%2520decayed%2520over%2520time%252C%250Aencouraging%2520the%2520model%2520to%2520internalize%2520the%2520underlying%2520reasoning%2520patterns%253B%2520%25282%2529%250Averifiable%2520rewards%2520for%2520exploitation%2520during%2520model%2520training%252C%2520where%2520we%2520can%2520obtain%250Arobust%2520LLM-as-a-Judge%2520scores%2520using%2520rubrics%2520as%2520references%252C%2520enabling%2520effective%2520RL%250Aon%2520general%2520reasoning%2520tasks.%2520Extensive%2520experiments%2520demonstrate%2520the%2520superiority%250Aof%2520the%2520proposed%2520RuscaRL%2520across%2520various%2520benchmarks%252C%2520effectively%2520expanding%250Areasoning%2520boundaries%2520under%2520the%2520Best-of-N%2520evaluation.%2520Notably%252C%2520RuscaRL%250Asignificantly%2520boosts%2520Qwen2.5-7B-Instruct%2520from%252023.6%2520to%252050.3%2520on%2520HealthBench-500%252C%250Asurpassing%2520GPT-4.1.%2520Furthermore%252C%2520our%2520fine-tuned%2520variant%2520on%250AQwen3-30B-A3B-Instruct%2520achieves%252061.1%2520on%2520HealthBench-500%252C%2520outperforming%2520leading%250ALLMs%2520including%2520OpenAI-o3.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/IANNXANG/RuscaRL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16949v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breaking%20the%20Exploration%20Bottleneck%3A%20Rubric-Scaffolded%20Reinforcement%0A%20%20Learning%20for%20General%20LLM%20Reasoning&entry.906535625=Yang%20Zhou%20and%20Sunzhu%20Li%20and%20Shunyu%20Liu%20and%20Wenkai%20Fang%20and%20Kongcheng%20Zhang%20and%20Jiale%20Zhao%20and%20Jingwen%20Yang%20and%20Yihe%20Zhou%20and%20Jianwei%20Lv%20and%20Tongya%20Zheng%20and%20Hengtong%20Lu%20and%20Wei%20Chen%20and%20Yan%20Xie%20and%20Mingli%20Song&entry.1292438233=%20%20Recent%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20underscored%20the%0Apotential%20of%20Reinforcement%20Learning%20%28RL%29%20to%20facilitate%20the%20emergence%20of%0Areasoning%20capabilities.%20Despite%20the%20encouraging%20results%2C%20a%20fundamental%20dilemma%0Apersists%20as%20RL%20improvement%20relies%20on%20learning%20from%20high-quality%20samples%2C%20yet%0Athe%20exploration%20for%20such%20samples%20remains%20bounded%20by%20the%20inherent%20limitations%20of%0ALLMs.%20This%2C%20in%20effect%2C%20creates%20an%20undesirable%20cycle%20in%20which%20what%20cannot%20be%0Aexplored%20cannot%20be%20learned.%20In%20this%20work%2C%20we%20propose%20Rubric-Scaffolded%0AReinforcement%20Learning%20%28RuscaRL%29%2C%20a%20novel%20instructional%20scaffolding%20framework%0Adesigned%20to%20break%20the%20exploration%20bottleneck%20for%20general%20LLM%20reasoning.%0ASpecifically%2C%20RuscaRL%20introduces%20checklist-style%20rubrics%20as%20%281%29%20explicit%0Ascaffolding%20for%20exploration%20during%20rollout%20generation%2C%20where%20different%20rubrics%0Aare%20provided%20as%20external%20guidance%20within%20task%20instructions%20to%20steer%20diverse%0Ahigh-quality%20responses.%20This%20guidance%20is%20gradually%20decayed%20over%20time%2C%0Aencouraging%20the%20model%20to%20internalize%20the%20underlying%20reasoning%20patterns%3B%20%282%29%0Averifiable%20rewards%20for%20exploitation%20during%20model%20training%2C%20where%20we%20can%20obtain%0Arobust%20LLM-as-a-Judge%20scores%20using%20rubrics%20as%20references%2C%20enabling%20effective%20RL%0Aon%20general%20reasoning%20tasks.%20Extensive%20experiments%20demonstrate%20the%20superiority%0Aof%20the%20proposed%20RuscaRL%20across%20various%20benchmarks%2C%20effectively%20expanding%0Areasoning%20boundaries%20under%20the%20Best-of-N%20evaluation.%20Notably%2C%20RuscaRL%0Asignificantly%20boosts%20Qwen2.5-7B-Instruct%20from%2023.6%20to%2050.3%20on%20HealthBench-500%2C%0Asurpassing%20GPT-4.1.%20Furthermore%2C%20our%20fine-tuned%20variant%20on%0AQwen3-30B-A3B-Instruct%20achieves%2061.1%20on%20HealthBench-500%2C%20outperforming%20leading%0ALLMs%20including%20OpenAI-o3.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/IANNXANG/RuscaRL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16949v5&entry.124074799=Read"},
{"title": "Unveiling Transformer Perception by Exploring Input Manifolds", "author": "Alessandro Benfenati and Alfio Ferrara and Alessio Marta and Davide Riva and Elisabetta Rocchetti", "abstract": "  This paper introduces a general method for the exploration of equivalence\nclasses in the input space of Transformer models. The proposed approach is\nbased on sound mathematical theory which describes the internal layers of a\nTransformer architecture as sequential deformations of the input manifold.\nUsing eigendecomposition of the pullback of the distance metric defined on the\noutput space through the Jacobian of the model, we are able to reconstruct\nequivalence classes in the input space and navigate across them. Our method\nenables two complementary exploration procedures: the first retrieves input\ninstances that produce the same class probability distribution as the original\ninstance-thus identifying elements within the same equivalence class-while the\nsecond discovers instances that yield a different class probability\ndistribution, effectively navigating toward distinct equivalence classes.\nFinally, we demonstrate how the retrieved instances can be meaningfully\ninterpreted by projecting their embeddings back into a human-readable format.\n", "link": "http://arxiv.org/abs/2410.06019v2", "date": "2025-10-22", "relevancy": 2.0213, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5616}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.495}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4931}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unveiling%20Transformer%20Perception%20by%20Exploring%20Input%20Manifolds&body=Title%3A%20Unveiling%20Transformer%20Perception%20by%20Exploring%20Input%20Manifolds%0AAuthor%3A%20Alessandro%20Benfenati%20and%20Alfio%20Ferrara%20and%20Alessio%20Marta%20and%20Davide%20Riva%20and%20Elisabetta%20Rocchetti%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20general%20method%20for%20the%20exploration%20of%20equivalence%0Aclasses%20in%20the%20input%20space%20of%20Transformer%20models.%20The%20proposed%20approach%20is%0Abased%20on%20sound%20mathematical%20theory%20which%20describes%20the%20internal%20layers%20of%20a%0ATransformer%20architecture%20as%20sequential%20deformations%20of%20the%20input%20manifold.%0AUsing%20eigendecomposition%20of%20the%20pullback%20of%20the%20distance%20metric%20defined%20on%20the%0Aoutput%20space%20through%20the%20Jacobian%20of%20the%20model%2C%20we%20are%20able%20to%20reconstruct%0Aequivalence%20classes%20in%20the%20input%20space%20and%20navigate%20across%20them.%20Our%20method%0Aenables%20two%20complementary%20exploration%20procedures%3A%20the%20first%20retrieves%20input%0Ainstances%20that%20produce%20the%20same%20class%20probability%20distribution%20as%20the%20original%0Ainstance-thus%20identifying%20elements%20within%20the%20same%20equivalence%20class-while%20the%0Asecond%20discovers%20instances%20that%20yield%20a%20different%20class%20probability%0Adistribution%2C%20effectively%20navigating%20toward%20distinct%20equivalence%20classes.%0AFinally%2C%20we%20demonstrate%20how%20the%20retrieved%20instances%20can%20be%20meaningfully%0Ainterpreted%20by%20projecting%20their%20embeddings%20back%20into%20a%20human-readable%20format.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06019v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnveiling%2520Transformer%2520Perception%2520by%2520Exploring%2520Input%2520Manifolds%26entry.906535625%3DAlessandro%2520Benfenati%2520and%2520Alfio%2520Ferrara%2520and%2520Alessio%2520Marta%2520and%2520Davide%2520Riva%2520and%2520Elisabetta%2520Rocchetti%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520general%2520method%2520for%2520the%2520exploration%2520of%2520equivalence%250Aclasses%2520in%2520the%2520input%2520space%2520of%2520Transformer%2520models.%2520The%2520proposed%2520approach%2520is%250Abased%2520on%2520sound%2520mathematical%2520theory%2520which%2520describes%2520the%2520internal%2520layers%2520of%2520a%250ATransformer%2520architecture%2520as%2520sequential%2520deformations%2520of%2520the%2520input%2520manifold.%250AUsing%2520eigendecomposition%2520of%2520the%2520pullback%2520of%2520the%2520distance%2520metric%2520defined%2520on%2520the%250Aoutput%2520space%2520through%2520the%2520Jacobian%2520of%2520the%2520model%252C%2520we%2520are%2520able%2520to%2520reconstruct%250Aequivalence%2520classes%2520in%2520the%2520input%2520space%2520and%2520navigate%2520across%2520them.%2520Our%2520method%250Aenables%2520two%2520complementary%2520exploration%2520procedures%253A%2520the%2520first%2520retrieves%2520input%250Ainstances%2520that%2520produce%2520the%2520same%2520class%2520probability%2520distribution%2520as%2520the%2520original%250Ainstance-thus%2520identifying%2520elements%2520within%2520the%2520same%2520equivalence%2520class-while%2520the%250Asecond%2520discovers%2520instances%2520that%2520yield%2520a%2520different%2520class%2520probability%250Adistribution%252C%2520effectively%2520navigating%2520toward%2520distinct%2520equivalence%2520classes.%250AFinally%252C%2520we%2520demonstrate%2520how%2520the%2520retrieved%2520instances%2520can%2520be%2520meaningfully%250Ainterpreted%2520by%2520projecting%2520their%2520embeddings%2520back%2520into%2520a%2520human-readable%2520format.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06019v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20Transformer%20Perception%20by%20Exploring%20Input%20Manifolds&entry.906535625=Alessandro%20Benfenati%20and%20Alfio%20Ferrara%20and%20Alessio%20Marta%20and%20Davide%20Riva%20and%20Elisabetta%20Rocchetti&entry.1292438233=%20%20This%20paper%20introduces%20a%20general%20method%20for%20the%20exploration%20of%20equivalence%0Aclasses%20in%20the%20input%20space%20of%20Transformer%20models.%20The%20proposed%20approach%20is%0Abased%20on%20sound%20mathematical%20theory%20which%20describes%20the%20internal%20layers%20of%20a%0ATransformer%20architecture%20as%20sequential%20deformations%20of%20the%20input%20manifold.%0AUsing%20eigendecomposition%20of%20the%20pullback%20of%20the%20distance%20metric%20defined%20on%20the%0Aoutput%20space%20through%20the%20Jacobian%20of%20the%20model%2C%20we%20are%20able%20to%20reconstruct%0Aequivalence%20classes%20in%20the%20input%20space%20and%20navigate%20across%20them.%20Our%20method%0Aenables%20two%20complementary%20exploration%20procedures%3A%20the%20first%20retrieves%20input%0Ainstances%20that%20produce%20the%20same%20class%20probability%20distribution%20as%20the%20original%0Ainstance-thus%20identifying%20elements%20within%20the%20same%20equivalence%20class-while%20the%0Asecond%20discovers%20instances%20that%20yield%20a%20different%20class%20probability%0Adistribution%2C%20effectively%20navigating%20toward%20distinct%20equivalence%20classes.%0AFinally%2C%20we%20demonstrate%20how%20the%20retrieved%20instances%20can%20be%20meaningfully%0Ainterpreted%20by%20projecting%20their%20embeddings%20back%20into%20a%20human-readable%20format.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06019v2&entry.124074799=Read"},
{"title": "Transformers are almost optimal metalearners for linear classification", "author": "Roey Magen and Gal Vardi", "abstract": "  Transformers have demonstrated impressive in-context learning (ICL)\ncapabilities, raising the question of whether they can serve as metalearners\nthat adapt to new tasks using only a small number of in-context examples,\nwithout any further training. While recent theoretical work has studied\ntransformers' ability to perform ICL, most of these analyses do not address the\nformal metalearning setting, where the objective is to solve a collection of\nrelated tasks more efficiently than would be possible by solving each task\nindividually. In this paper, we provide the first theoretical analysis showing\nthat a simplified transformer architecture trained via gradient descent can act\nas a near-optimal metalearner in a linear classification setting. We consider a\nnatural family of tasks where each task corresponds to a class-conditional\nGaussian mixture model, with the mean vectors lying in a shared $k$-dimensional\nsubspace of $R^d$. After training on a sufficient number of such tasks, we show\nthat the transformer can generalize to a new task using only $O(k / R^4)$\nin-context examples, where $R$ denotes the signal strength at test time. This\nperformance (almost) matches that of an optimal learner that knows exactly the\nshared subspace and significantly outperforms any learner that only has access\nto the in-context data, which requires $\\Omega(d / R^4)$ examples to\ngeneralize. Importantly, our bounds on the number of training tasks and\nexamples per task needed to achieve this result are independent of the ambient\ndimension $d$.\n", "link": "http://arxiv.org/abs/2510.19797v1", "date": "2025-10-22", "relevancy": 2.0079, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5226}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5157}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transformers%20are%20almost%20optimal%20metalearners%20for%20linear%20classification&body=Title%3A%20Transformers%20are%20almost%20optimal%20metalearners%20for%20linear%20classification%0AAuthor%3A%20Roey%20Magen%20and%20Gal%20Vardi%0AAbstract%3A%20%20%20Transformers%20have%20demonstrated%20impressive%20in-context%20learning%20%28ICL%29%0Acapabilities%2C%20raising%20the%20question%20of%20whether%20they%20can%20serve%20as%20metalearners%0Athat%20adapt%20to%20new%20tasks%20using%20only%20a%20small%20number%20of%20in-context%20examples%2C%0Awithout%20any%20further%20training.%20While%20recent%20theoretical%20work%20has%20studied%0Atransformers%27%20ability%20to%20perform%20ICL%2C%20most%20of%20these%20analyses%20do%20not%20address%20the%0Aformal%20metalearning%20setting%2C%20where%20the%20objective%20is%20to%20solve%20a%20collection%20of%0Arelated%20tasks%20more%20efficiently%20than%20would%20be%20possible%20by%20solving%20each%20task%0Aindividually.%20In%20this%20paper%2C%20we%20provide%20the%20first%20theoretical%20analysis%20showing%0Athat%20a%20simplified%20transformer%20architecture%20trained%20via%20gradient%20descent%20can%20act%0Aas%20a%20near-optimal%20metalearner%20in%20a%20linear%20classification%20setting.%20We%20consider%20a%0Anatural%20family%20of%20tasks%20where%20each%20task%20corresponds%20to%20a%20class-conditional%0AGaussian%20mixture%20model%2C%20with%20the%20mean%20vectors%20lying%20in%20a%20shared%20%24k%24-dimensional%0Asubspace%20of%20%24R%5Ed%24.%20After%20training%20on%20a%20sufficient%20number%20of%20such%20tasks%2C%20we%20show%0Athat%20the%20transformer%20can%20generalize%20to%20a%20new%20task%20using%20only%20%24O%28k%20/%20R%5E4%29%24%0Ain-context%20examples%2C%20where%20%24R%24%20denotes%20the%20signal%20strength%20at%20test%20time.%20This%0Aperformance%20%28almost%29%20matches%20that%20of%20an%20optimal%20learner%20that%20knows%20exactly%20the%0Ashared%20subspace%20and%20significantly%20outperforms%20any%20learner%20that%20only%20has%20access%0Ato%20the%20in-context%20data%2C%20which%20requires%20%24%5COmega%28d%20/%20R%5E4%29%24%20examples%20to%0Ageneralize.%20Importantly%2C%20our%20bounds%20on%20the%20number%20of%20training%20tasks%20and%0Aexamples%20per%20task%20needed%20to%20achieve%20this%20result%20are%20independent%20of%20the%20ambient%0Adimension%20%24d%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19797v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransformers%2520are%2520almost%2520optimal%2520metalearners%2520for%2520linear%2520classification%26entry.906535625%3DRoey%2520Magen%2520and%2520Gal%2520Vardi%26entry.1292438233%3D%2520%2520Transformers%2520have%2520demonstrated%2520impressive%2520in-context%2520learning%2520%2528ICL%2529%250Acapabilities%252C%2520raising%2520the%2520question%2520of%2520whether%2520they%2520can%2520serve%2520as%2520metalearners%250Athat%2520adapt%2520to%2520new%2520tasks%2520using%2520only%2520a%2520small%2520number%2520of%2520in-context%2520examples%252C%250Awithout%2520any%2520further%2520training.%2520While%2520recent%2520theoretical%2520work%2520has%2520studied%250Atransformers%2527%2520ability%2520to%2520perform%2520ICL%252C%2520most%2520of%2520these%2520analyses%2520do%2520not%2520address%2520the%250Aformal%2520metalearning%2520setting%252C%2520where%2520the%2520objective%2520is%2520to%2520solve%2520a%2520collection%2520of%250Arelated%2520tasks%2520more%2520efficiently%2520than%2520would%2520be%2520possible%2520by%2520solving%2520each%2520task%250Aindividually.%2520In%2520this%2520paper%252C%2520we%2520provide%2520the%2520first%2520theoretical%2520analysis%2520showing%250Athat%2520a%2520simplified%2520transformer%2520architecture%2520trained%2520via%2520gradient%2520descent%2520can%2520act%250Aas%2520a%2520near-optimal%2520metalearner%2520in%2520a%2520linear%2520classification%2520setting.%2520We%2520consider%2520a%250Anatural%2520family%2520of%2520tasks%2520where%2520each%2520task%2520corresponds%2520to%2520a%2520class-conditional%250AGaussian%2520mixture%2520model%252C%2520with%2520the%2520mean%2520vectors%2520lying%2520in%2520a%2520shared%2520%2524k%2524-dimensional%250Asubspace%2520of%2520%2524R%255Ed%2524.%2520After%2520training%2520on%2520a%2520sufficient%2520number%2520of%2520such%2520tasks%252C%2520we%2520show%250Athat%2520the%2520transformer%2520can%2520generalize%2520to%2520a%2520new%2520task%2520using%2520only%2520%2524O%2528k%2520/%2520R%255E4%2529%2524%250Ain-context%2520examples%252C%2520where%2520%2524R%2524%2520denotes%2520the%2520signal%2520strength%2520at%2520test%2520time.%2520This%250Aperformance%2520%2528almost%2529%2520matches%2520that%2520of%2520an%2520optimal%2520learner%2520that%2520knows%2520exactly%2520the%250Ashared%2520subspace%2520and%2520significantly%2520outperforms%2520any%2520learner%2520that%2520only%2520has%2520access%250Ato%2520the%2520in-context%2520data%252C%2520which%2520requires%2520%2524%255COmega%2528d%2520/%2520R%255E4%2529%2524%2520examples%2520to%250Ageneralize.%2520Importantly%252C%2520our%2520bounds%2520on%2520the%2520number%2520of%2520training%2520tasks%2520and%250Aexamples%2520per%2520task%2520needed%2520to%2520achieve%2520this%2520result%2520are%2520independent%2520of%2520the%2520ambient%250Adimension%2520%2524d%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19797v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformers%20are%20almost%20optimal%20metalearners%20for%20linear%20classification&entry.906535625=Roey%20Magen%20and%20Gal%20Vardi&entry.1292438233=%20%20Transformers%20have%20demonstrated%20impressive%20in-context%20learning%20%28ICL%29%0Acapabilities%2C%20raising%20the%20question%20of%20whether%20they%20can%20serve%20as%20metalearners%0Athat%20adapt%20to%20new%20tasks%20using%20only%20a%20small%20number%20of%20in-context%20examples%2C%0Awithout%20any%20further%20training.%20While%20recent%20theoretical%20work%20has%20studied%0Atransformers%27%20ability%20to%20perform%20ICL%2C%20most%20of%20these%20analyses%20do%20not%20address%20the%0Aformal%20metalearning%20setting%2C%20where%20the%20objective%20is%20to%20solve%20a%20collection%20of%0Arelated%20tasks%20more%20efficiently%20than%20would%20be%20possible%20by%20solving%20each%20task%0Aindividually.%20In%20this%20paper%2C%20we%20provide%20the%20first%20theoretical%20analysis%20showing%0Athat%20a%20simplified%20transformer%20architecture%20trained%20via%20gradient%20descent%20can%20act%0Aas%20a%20near-optimal%20metalearner%20in%20a%20linear%20classification%20setting.%20We%20consider%20a%0Anatural%20family%20of%20tasks%20where%20each%20task%20corresponds%20to%20a%20class-conditional%0AGaussian%20mixture%20model%2C%20with%20the%20mean%20vectors%20lying%20in%20a%20shared%20%24k%24-dimensional%0Asubspace%20of%20%24R%5Ed%24.%20After%20training%20on%20a%20sufficient%20number%20of%20such%20tasks%2C%20we%20show%0Athat%20the%20transformer%20can%20generalize%20to%20a%20new%20task%20using%20only%20%24O%28k%20/%20R%5E4%29%24%0Ain-context%20examples%2C%20where%20%24R%24%20denotes%20the%20signal%20strength%20at%20test%20time.%20This%0Aperformance%20%28almost%29%20matches%20that%20of%20an%20optimal%20learner%20that%20knows%20exactly%20the%0Ashared%20subspace%20and%20significantly%20outperforms%20any%20learner%20that%20only%20has%20access%0Ato%20the%20in-context%20data%2C%20which%20requires%20%24%5COmega%28d%20/%20R%5E4%29%24%20examples%20to%0Ageneralize.%20Importantly%2C%20our%20bounds%20on%20the%20number%20of%20training%20tasks%20and%0Aexamples%20per%20task%20needed%20to%20achieve%20this%20result%20are%20independent%20of%20the%20ambient%0Adimension%20%24d%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19797v1&entry.124074799=Read"},
{"title": "SmartSwitch: Advancing LLM Reasoning by Overcoming Underthinking via\n  Promoting Deeper Thought Exploration", "author": "Xichen Zhang and Sitong Wu and Haoru Tan and Shaozuo Yu and Yinghao Zhu and Ziyi He and Jiaya Jia", "abstract": "  The long chain-of-thought (LongCoT) capability is central to the recent\nbreakthroughs achieved by large language models in complex reasoning tasks.\nHowever, the accompanying issue of ''underthinking'', where models exhibit\nshallow reasoning by frequently switching thoughts without sufficient\nexploration, limits both performance and token efficiency. To address this\nproblem, we propose a simple yet effective reasoning strategy: the SmartSwitch\ninference framework. This framework can be easily integrated into any large\nlanguage model as a plug-and-play solution, continuously monitoring the model's\nreasoning process to detect underthinking and guide it toward deeper\nexploration of promising but overlooked thoughts. Specifically, the perception\nmodule identifies points where thoughts switch and evaluates the potential of\nthe preceding thought using an off-the-shelf process reward model (PRM). If a\nhigh-potential thought is found to be prematurely abandoned, the intervention\nmodule interrupts the ongoing inference, backtracks to the point before the\nswitch, and inserts a \"deepening prompt\" to encourage further exploration along\nthat promising path. Extensive experiments on challenging mathematical\nreasoning benchmarks demonstrate that our method significantly enhances the\nperformance of various large language models of different sizes.\n", "link": "http://arxiv.org/abs/2510.19767v1", "date": "2025-10-22", "relevancy": 2.0078, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5036}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5036}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SmartSwitch%3A%20Advancing%20LLM%20Reasoning%20by%20Overcoming%20Underthinking%20via%0A%20%20Promoting%20Deeper%20Thought%20Exploration&body=Title%3A%20SmartSwitch%3A%20Advancing%20LLM%20Reasoning%20by%20Overcoming%20Underthinking%20via%0A%20%20Promoting%20Deeper%20Thought%20Exploration%0AAuthor%3A%20Xichen%20Zhang%20and%20Sitong%20Wu%20and%20Haoru%20Tan%20and%20Shaozuo%20Yu%20and%20Yinghao%20Zhu%20and%20Ziyi%20He%20and%20Jiaya%20Jia%0AAbstract%3A%20%20%20The%20long%20chain-of-thought%20%28LongCoT%29%20capability%20is%20central%20to%20the%20recent%0Abreakthroughs%20achieved%20by%20large%20language%20models%20in%20complex%20reasoning%20tasks.%0AHowever%2C%20the%20accompanying%20issue%20of%20%27%27underthinking%27%27%2C%20where%20models%20exhibit%0Ashallow%20reasoning%20by%20frequently%20switching%20thoughts%20without%20sufficient%0Aexploration%2C%20limits%20both%20performance%20and%20token%20efficiency.%20To%20address%20this%0Aproblem%2C%20we%20propose%20a%20simple%20yet%20effective%20reasoning%20strategy%3A%20the%20SmartSwitch%0Ainference%20framework.%20This%20framework%20can%20be%20easily%20integrated%20into%20any%20large%0Alanguage%20model%20as%20a%20plug-and-play%20solution%2C%20continuously%20monitoring%20the%20model%27s%0Areasoning%20process%20to%20detect%20underthinking%20and%20guide%20it%20toward%20deeper%0Aexploration%20of%20promising%20but%20overlooked%20thoughts.%20Specifically%2C%20the%20perception%0Amodule%20identifies%20points%20where%20thoughts%20switch%20and%20evaluates%20the%20potential%20of%0Athe%20preceding%20thought%20using%20an%20off-the-shelf%20process%20reward%20model%20%28PRM%29.%20If%20a%0Ahigh-potential%20thought%20is%20found%20to%20be%20prematurely%20abandoned%2C%20the%20intervention%0Amodule%20interrupts%20the%20ongoing%20inference%2C%20backtracks%20to%20the%20point%20before%20the%0Aswitch%2C%20and%20inserts%20a%20%22deepening%20prompt%22%20to%20encourage%20further%20exploration%20along%0Athat%20promising%20path.%20Extensive%20experiments%20on%20challenging%20mathematical%0Areasoning%20benchmarks%20demonstrate%20that%20our%20method%20significantly%20enhances%20the%0Aperformance%20of%20various%20large%20language%20models%20of%20different%20sizes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19767v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmartSwitch%253A%2520Advancing%2520LLM%2520Reasoning%2520by%2520Overcoming%2520Underthinking%2520via%250A%2520%2520Promoting%2520Deeper%2520Thought%2520Exploration%26entry.906535625%3DXichen%2520Zhang%2520and%2520Sitong%2520Wu%2520and%2520Haoru%2520Tan%2520and%2520Shaozuo%2520Yu%2520and%2520Yinghao%2520Zhu%2520and%2520Ziyi%2520He%2520and%2520Jiaya%2520Jia%26entry.1292438233%3D%2520%2520The%2520long%2520chain-of-thought%2520%2528LongCoT%2529%2520capability%2520is%2520central%2520to%2520the%2520recent%250Abreakthroughs%2520achieved%2520by%2520large%2520language%2520models%2520in%2520complex%2520reasoning%2520tasks.%250AHowever%252C%2520the%2520accompanying%2520issue%2520of%2520%2527%2527underthinking%2527%2527%252C%2520where%2520models%2520exhibit%250Ashallow%2520reasoning%2520by%2520frequently%2520switching%2520thoughts%2520without%2520sufficient%250Aexploration%252C%2520limits%2520both%2520performance%2520and%2520token%2520efficiency.%2520To%2520address%2520this%250Aproblem%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%2520reasoning%2520strategy%253A%2520the%2520SmartSwitch%250Ainference%2520framework.%2520This%2520framework%2520can%2520be%2520easily%2520integrated%2520into%2520any%2520large%250Alanguage%2520model%2520as%2520a%2520plug-and-play%2520solution%252C%2520continuously%2520monitoring%2520the%2520model%2527s%250Areasoning%2520process%2520to%2520detect%2520underthinking%2520and%2520guide%2520it%2520toward%2520deeper%250Aexploration%2520of%2520promising%2520but%2520overlooked%2520thoughts.%2520Specifically%252C%2520the%2520perception%250Amodule%2520identifies%2520points%2520where%2520thoughts%2520switch%2520and%2520evaluates%2520the%2520potential%2520of%250Athe%2520preceding%2520thought%2520using%2520an%2520off-the-shelf%2520process%2520reward%2520model%2520%2528PRM%2529.%2520If%2520a%250Ahigh-potential%2520thought%2520is%2520found%2520to%2520be%2520prematurely%2520abandoned%252C%2520the%2520intervention%250Amodule%2520interrupts%2520the%2520ongoing%2520inference%252C%2520backtracks%2520to%2520the%2520point%2520before%2520the%250Aswitch%252C%2520and%2520inserts%2520a%2520%2522deepening%2520prompt%2522%2520to%2520encourage%2520further%2520exploration%2520along%250Athat%2520promising%2520path.%2520Extensive%2520experiments%2520on%2520challenging%2520mathematical%250Areasoning%2520benchmarks%2520demonstrate%2520that%2520our%2520method%2520significantly%2520enhances%2520the%250Aperformance%2520of%2520various%2520large%2520language%2520models%2520of%2520different%2520sizes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19767v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SmartSwitch%3A%20Advancing%20LLM%20Reasoning%20by%20Overcoming%20Underthinking%20via%0A%20%20Promoting%20Deeper%20Thought%20Exploration&entry.906535625=Xichen%20Zhang%20and%20Sitong%20Wu%20and%20Haoru%20Tan%20and%20Shaozuo%20Yu%20and%20Yinghao%20Zhu%20and%20Ziyi%20He%20and%20Jiaya%20Jia&entry.1292438233=%20%20The%20long%20chain-of-thought%20%28LongCoT%29%20capability%20is%20central%20to%20the%20recent%0Abreakthroughs%20achieved%20by%20large%20language%20models%20in%20complex%20reasoning%20tasks.%0AHowever%2C%20the%20accompanying%20issue%20of%20%27%27underthinking%27%27%2C%20where%20models%20exhibit%0Ashallow%20reasoning%20by%20frequently%20switching%20thoughts%20without%20sufficient%0Aexploration%2C%20limits%20both%20performance%20and%20token%20efficiency.%20To%20address%20this%0Aproblem%2C%20we%20propose%20a%20simple%20yet%20effective%20reasoning%20strategy%3A%20the%20SmartSwitch%0Ainference%20framework.%20This%20framework%20can%20be%20easily%20integrated%20into%20any%20large%0Alanguage%20model%20as%20a%20plug-and-play%20solution%2C%20continuously%20monitoring%20the%20model%27s%0Areasoning%20process%20to%20detect%20underthinking%20and%20guide%20it%20toward%20deeper%0Aexploration%20of%20promising%20but%20overlooked%20thoughts.%20Specifically%2C%20the%20perception%0Amodule%20identifies%20points%20where%20thoughts%20switch%20and%20evaluates%20the%20potential%20of%0Athe%20preceding%20thought%20using%20an%20off-the-shelf%20process%20reward%20model%20%28PRM%29.%20If%20a%0Ahigh-potential%20thought%20is%20found%20to%20be%20prematurely%20abandoned%2C%20the%20intervention%0Amodule%20interrupts%20the%20ongoing%20inference%2C%20backtracks%20to%20the%20point%20before%20the%0Aswitch%2C%20and%20inserts%20a%20%22deepening%20prompt%22%20to%20encourage%20further%20exploration%20along%0Athat%20promising%20path.%20Extensive%20experiments%20on%20challenging%20mathematical%0Areasoning%20benchmarks%20demonstrate%20that%20our%20method%20significantly%20enhances%20the%0Aperformance%20of%20various%20large%20language%20models%20of%20different%20sizes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19767v1&entry.124074799=Read"},
{"title": "GeoBenchX: Benchmarking LLMs in Agent Solving Multistep Geospatial Tasks", "author": "Varvara Krechetova and Denis Kochedykov", "abstract": "  This paper establishes a benchmark for evaluating tool-calling capabilities\nof large language models (LLMs) on multi-step geospatial tasks relevant to\ncommercial GIS practitioners. We assess eight commercial LLMs (Claude Sonnet\n3.5 and 4, Claude Haiku 3.5, Gemini 2.0 Flash, Gemini 2.5 Pro Preview, GPT-4o,\nGPT-4.1 and o4-mini) using a simple tool-calling agent equipped with 23\ngeospatial functions. Our benchmark comprises tasks in four categories of\nincreasing complexity, with both solvable and intentionally unsolvable tasks to\ntest rejection accuracy. We develop a LLM-as-Judge evaluation framework to\ncompare agent solutions against reference solutions. Results show o4-mini and\nClaude 3.5 Sonnet achieve the best overall performance, OpenAI's GPT-4.1,\nGPT-4o and Google's Gemini 2.5 Pro Preview do not fall far behind, but the last\ntwo are more efficient in identifying unsolvable tasks. Claude Sonnet 4, due\nits preference to provide any solution rather than reject a task, proved to be\nless accurate. We observe significant differences in token usage, with\nAnthropic models consuming more tokens than competitors. Common errors include\nmisunderstanding geometrical relationships, relying on outdated knowledge, and\ninefficient data manipulation. The resulting benchmark set, evaluation\nframework, and data generation pipeline are released as open-source resources\n(available at https://github.com/Solirinai/GeoBenchX), providing one more\nstandardized method for the ongoing evaluation of LLMs for GeoAI.\n", "link": "http://arxiv.org/abs/2503.18129v2", "date": "2025-10-22", "relevancy": 2.0059, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5055}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5055}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4816}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoBenchX%3A%20Benchmarking%20LLMs%20in%20Agent%20Solving%20Multistep%20Geospatial%20Tasks&body=Title%3A%20GeoBenchX%3A%20Benchmarking%20LLMs%20in%20Agent%20Solving%20Multistep%20Geospatial%20Tasks%0AAuthor%3A%20Varvara%20Krechetova%20and%20Denis%20Kochedykov%0AAbstract%3A%20%20%20This%20paper%20establishes%20a%20benchmark%20for%20evaluating%20tool-calling%20capabilities%0Aof%20large%20language%20models%20%28LLMs%29%20on%20multi-step%20geospatial%20tasks%20relevant%20to%0Acommercial%20GIS%20practitioners.%20We%20assess%20eight%20commercial%20LLMs%20%28Claude%20Sonnet%0A3.5%20and%204%2C%20Claude%20Haiku%203.5%2C%20Gemini%202.0%20Flash%2C%20Gemini%202.5%20Pro%20Preview%2C%20GPT-4o%2C%0AGPT-4.1%20and%20o4-mini%29%20using%20a%20simple%20tool-calling%20agent%20equipped%20with%2023%0Ageospatial%20functions.%20Our%20benchmark%20comprises%20tasks%20in%20four%20categories%20of%0Aincreasing%20complexity%2C%20with%20both%20solvable%20and%20intentionally%20unsolvable%20tasks%20to%0Atest%20rejection%20accuracy.%20We%20develop%20a%20LLM-as-Judge%20evaluation%20framework%20to%0Acompare%20agent%20solutions%20against%20reference%20solutions.%20Results%20show%20o4-mini%20and%0AClaude%203.5%20Sonnet%20achieve%20the%20best%20overall%20performance%2C%20OpenAI%27s%20GPT-4.1%2C%0AGPT-4o%20and%20Google%27s%20Gemini%202.5%20Pro%20Preview%20do%20not%20fall%20far%20behind%2C%20but%20the%20last%0Atwo%20are%20more%20efficient%20in%20identifying%20unsolvable%20tasks.%20Claude%20Sonnet%204%2C%20due%0Aits%20preference%20to%20provide%20any%20solution%20rather%20than%20reject%20a%20task%2C%20proved%20to%20be%0Aless%20accurate.%20We%20observe%20significant%20differences%20in%20token%20usage%2C%20with%0AAnthropic%20models%20consuming%20more%20tokens%20than%20competitors.%20Common%20errors%20include%0Amisunderstanding%20geometrical%20relationships%2C%20relying%20on%20outdated%20knowledge%2C%20and%0Ainefficient%20data%20manipulation.%20The%20resulting%20benchmark%20set%2C%20evaluation%0Aframework%2C%20and%20data%20generation%20pipeline%20are%20released%20as%20open-source%20resources%0A%28available%20at%20https%3A//github.com/Solirinai/GeoBenchX%29%2C%20providing%20one%20more%0Astandardized%20method%20for%20the%20ongoing%20evaluation%20of%20LLMs%20for%20GeoAI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.18129v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoBenchX%253A%2520Benchmarking%2520LLMs%2520in%2520Agent%2520Solving%2520Multistep%2520Geospatial%2520Tasks%26entry.906535625%3DVarvara%2520Krechetova%2520and%2520Denis%2520Kochedykov%26entry.1292438233%3D%2520%2520This%2520paper%2520establishes%2520a%2520benchmark%2520for%2520evaluating%2520tool-calling%2520capabilities%250Aof%2520large%2520language%2520models%2520%2528LLMs%2529%2520on%2520multi-step%2520geospatial%2520tasks%2520relevant%2520to%250Acommercial%2520GIS%2520practitioners.%2520We%2520assess%2520eight%2520commercial%2520LLMs%2520%2528Claude%2520Sonnet%250A3.5%2520and%25204%252C%2520Claude%2520Haiku%25203.5%252C%2520Gemini%25202.0%2520Flash%252C%2520Gemini%25202.5%2520Pro%2520Preview%252C%2520GPT-4o%252C%250AGPT-4.1%2520and%2520o4-mini%2529%2520using%2520a%2520simple%2520tool-calling%2520agent%2520equipped%2520with%252023%250Ageospatial%2520functions.%2520Our%2520benchmark%2520comprises%2520tasks%2520in%2520four%2520categories%2520of%250Aincreasing%2520complexity%252C%2520with%2520both%2520solvable%2520and%2520intentionally%2520unsolvable%2520tasks%2520to%250Atest%2520rejection%2520accuracy.%2520We%2520develop%2520a%2520LLM-as-Judge%2520evaluation%2520framework%2520to%250Acompare%2520agent%2520solutions%2520against%2520reference%2520solutions.%2520Results%2520show%2520o4-mini%2520and%250AClaude%25203.5%2520Sonnet%2520achieve%2520the%2520best%2520overall%2520performance%252C%2520OpenAI%2527s%2520GPT-4.1%252C%250AGPT-4o%2520and%2520Google%2527s%2520Gemini%25202.5%2520Pro%2520Preview%2520do%2520not%2520fall%2520far%2520behind%252C%2520but%2520the%2520last%250Atwo%2520are%2520more%2520efficient%2520in%2520identifying%2520unsolvable%2520tasks.%2520Claude%2520Sonnet%25204%252C%2520due%250Aits%2520preference%2520to%2520provide%2520any%2520solution%2520rather%2520than%2520reject%2520a%2520task%252C%2520proved%2520to%2520be%250Aless%2520accurate.%2520We%2520observe%2520significant%2520differences%2520in%2520token%2520usage%252C%2520with%250AAnthropic%2520models%2520consuming%2520more%2520tokens%2520than%2520competitors.%2520Common%2520errors%2520include%250Amisunderstanding%2520geometrical%2520relationships%252C%2520relying%2520on%2520outdated%2520knowledge%252C%2520and%250Ainefficient%2520data%2520manipulation.%2520The%2520resulting%2520benchmark%2520set%252C%2520evaluation%250Aframework%252C%2520and%2520data%2520generation%2520pipeline%2520are%2520released%2520as%2520open-source%2520resources%250A%2528available%2520at%2520https%253A//github.com/Solirinai/GeoBenchX%2529%252C%2520providing%2520one%2520more%250Astandardized%2520method%2520for%2520the%2520ongoing%2520evaluation%2520of%2520LLMs%2520for%2520GeoAI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.18129v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoBenchX%3A%20Benchmarking%20LLMs%20in%20Agent%20Solving%20Multistep%20Geospatial%20Tasks&entry.906535625=Varvara%20Krechetova%20and%20Denis%20Kochedykov&entry.1292438233=%20%20This%20paper%20establishes%20a%20benchmark%20for%20evaluating%20tool-calling%20capabilities%0Aof%20large%20language%20models%20%28LLMs%29%20on%20multi-step%20geospatial%20tasks%20relevant%20to%0Acommercial%20GIS%20practitioners.%20We%20assess%20eight%20commercial%20LLMs%20%28Claude%20Sonnet%0A3.5%20and%204%2C%20Claude%20Haiku%203.5%2C%20Gemini%202.0%20Flash%2C%20Gemini%202.5%20Pro%20Preview%2C%20GPT-4o%2C%0AGPT-4.1%20and%20o4-mini%29%20using%20a%20simple%20tool-calling%20agent%20equipped%20with%2023%0Ageospatial%20functions.%20Our%20benchmark%20comprises%20tasks%20in%20four%20categories%20of%0Aincreasing%20complexity%2C%20with%20both%20solvable%20and%20intentionally%20unsolvable%20tasks%20to%0Atest%20rejection%20accuracy.%20We%20develop%20a%20LLM-as-Judge%20evaluation%20framework%20to%0Acompare%20agent%20solutions%20against%20reference%20solutions.%20Results%20show%20o4-mini%20and%0AClaude%203.5%20Sonnet%20achieve%20the%20best%20overall%20performance%2C%20OpenAI%27s%20GPT-4.1%2C%0AGPT-4o%20and%20Google%27s%20Gemini%202.5%20Pro%20Preview%20do%20not%20fall%20far%20behind%2C%20but%20the%20last%0Atwo%20are%20more%20efficient%20in%20identifying%20unsolvable%20tasks.%20Claude%20Sonnet%204%2C%20due%0Aits%20preference%20to%20provide%20any%20solution%20rather%20than%20reject%20a%20task%2C%20proved%20to%20be%0Aless%20accurate.%20We%20observe%20significant%20differences%20in%20token%20usage%2C%20with%0AAnthropic%20models%20consuming%20more%20tokens%20than%20competitors.%20Common%20errors%20include%0Amisunderstanding%20geometrical%20relationships%2C%20relying%20on%20outdated%20knowledge%2C%20and%0Ainefficient%20data%20manipulation.%20The%20resulting%20benchmark%20set%2C%20evaluation%0Aframework%2C%20and%20data%20generation%20pipeline%20are%20released%20as%20open-source%20resources%0A%28available%20at%20https%3A//github.com/Solirinai/GeoBenchX%29%2C%20providing%20one%20more%0Astandardized%20method%20for%20the%20ongoing%20evaluation%20of%20LLMs%20for%20GeoAI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.18129v2&entry.124074799=Read"},
{"title": "Beyond Reactivity: Measuring Proactive Problem Solving in LLM Agents", "author": "Gil Pasternak and Dheeraj Rajagopal and Julia White and Dhruv Atreja and Matthew Thomas and George Hurn-Maloney and Ash Lewis", "abstract": "  LLM-based agents are increasingly moving towards proactivity: rather than\nawaiting instruction, they exercise agency to anticipate user needs and solve\nthem autonomously. However, evaluating proactivity is challenging; current\nbenchmarks are constrained to localized context, limiting their ability to test\nreasoning across sources and longer time horizons. To address this gap, we\npresent PROBE (Proactive Resolution Of BottlEnecks). PROBE decomposes\nproactivity as a pipeline of three core capabilities: (1) searching for\nunspecified issues, (2) identifying specific bottlenecks, and (3) executing\nappropriate resolutions. We apply PROBE to evaluate leading LLMs and popular\nagentic frameworks, showing that even state-of-the-art models struggle to solve\nthis benchmark. Computing our consistent measurements across frontier LLMs and\nagents, we find that the best end-to-end performance of 40% is achieved by both\nGPT-5 and Claude Opus-4.1. Additionally, we demonstrate the relative\ncapabilities of each model and analyze mutual failure modes. Our results\nhighlight the current limitations of autonomous action in agentic systems, and\nexpose promising future research directions.\n", "link": "http://arxiv.org/abs/2510.19771v1", "date": "2025-10-22", "relevancy": 2.0045, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5394}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4935}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Reactivity%3A%20Measuring%20Proactive%20Problem%20Solving%20in%20LLM%20Agents&body=Title%3A%20Beyond%20Reactivity%3A%20Measuring%20Proactive%20Problem%20Solving%20in%20LLM%20Agents%0AAuthor%3A%20Gil%20Pasternak%20and%20Dheeraj%20Rajagopal%20and%20Julia%20White%20and%20Dhruv%20Atreja%20and%20Matthew%20Thomas%20and%20George%20Hurn-Maloney%20and%20Ash%20Lewis%0AAbstract%3A%20%20%20LLM-based%20agents%20are%20increasingly%20moving%20towards%20proactivity%3A%20rather%20than%0Aawaiting%20instruction%2C%20they%20exercise%20agency%20to%20anticipate%20user%20needs%20and%20solve%0Athem%20autonomously.%20However%2C%20evaluating%20proactivity%20is%20challenging%3B%20current%0Abenchmarks%20are%20constrained%20to%20localized%20context%2C%20limiting%20their%20ability%20to%20test%0Areasoning%20across%20sources%20and%20longer%20time%20horizons.%20To%20address%20this%20gap%2C%20we%0Apresent%20PROBE%20%28Proactive%20Resolution%20Of%20BottlEnecks%29.%20PROBE%20decomposes%0Aproactivity%20as%20a%20pipeline%20of%20three%20core%20capabilities%3A%20%281%29%20searching%20for%0Aunspecified%20issues%2C%20%282%29%20identifying%20specific%20bottlenecks%2C%20and%20%283%29%20executing%0Aappropriate%20resolutions.%20We%20apply%20PROBE%20to%20evaluate%20leading%20LLMs%20and%20popular%0Aagentic%20frameworks%2C%20showing%20that%20even%20state-of-the-art%20models%20struggle%20to%20solve%0Athis%20benchmark.%20Computing%20our%20consistent%20measurements%20across%20frontier%20LLMs%20and%0Aagents%2C%20we%20find%20that%20the%20best%20end-to-end%20performance%20of%2040%25%20is%20achieved%20by%20both%0AGPT-5%20and%20Claude%20Opus-4.1.%20Additionally%2C%20we%20demonstrate%20the%20relative%0Acapabilities%20of%20each%20model%20and%20analyze%20mutual%20failure%20modes.%20Our%20results%0Ahighlight%20the%20current%20limitations%20of%20autonomous%20action%20in%20agentic%20systems%2C%20and%0Aexpose%20promising%20future%20research%20directions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19771v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Reactivity%253A%2520Measuring%2520Proactive%2520Problem%2520Solving%2520in%2520LLM%2520Agents%26entry.906535625%3DGil%2520Pasternak%2520and%2520Dheeraj%2520Rajagopal%2520and%2520Julia%2520White%2520and%2520Dhruv%2520Atreja%2520and%2520Matthew%2520Thomas%2520and%2520George%2520Hurn-Maloney%2520and%2520Ash%2520Lewis%26entry.1292438233%3D%2520%2520LLM-based%2520agents%2520are%2520increasingly%2520moving%2520towards%2520proactivity%253A%2520rather%2520than%250Aawaiting%2520instruction%252C%2520they%2520exercise%2520agency%2520to%2520anticipate%2520user%2520needs%2520and%2520solve%250Athem%2520autonomously.%2520However%252C%2520evaluating%2520proactivity%2520is%2520challenging%253B%2520current%250Abenchmarks%2520are%2520constrained%2520to%2520localized%2520context%252C%2520limiting%2520their%2520ability%2520to%2520test%250Areasoning%2520across%2520sources%2520and%2520longer%2520time%2520horizons.%2520To%2520address%2520this%2520gap%252C%2520we%250Apresent%2520PROBE%2520%2528Proactive%2520Resolution%2520Of%2520BottlEnecks%2529.%2520PROBE%2520decomposes%250Aproactivity%2520as%2520a%2520pipeline%2520of%2520three%2520core%2520capabilities%253A%2520%25281%2529%2520searching%2520for%250Aunspecified%2520issues%252C%2520%25282%2529%2520identifying%2520specific%2520bottlenecks%252C%2520and%2520%25283%2529%2520executing%250Aappropriate%2520resolutions.%2520We%2520apply%2520PROBE%2520to%2520evaluate%2520leading%2520LLMs%2520and%2520popular%250Aagentic%2520frameworks%252C%2520showing%2520that%2520even%2520state-of-the-art%2520models%2520struggle%2520to%2520solve%250Athis%2520benchmark.%2520Computing%2520our%2520consistent%2520measurements%2520across%2520frontier%2520LLMs%2520and%250Aagents%252C%2520we%2520find%2520that%2520the%2520best%2520end-to-end%2520performance%2520of%252040%2525%2520is%2520achieved%2520by%2520both%250AGPT-5%2520and%2520Claude%2520Opus-4.1.%2520Additionally%252C%2520we%2520demonstrate%2520the%2520relative%250Acapabilities%2520of%2520each%2520model%2520and%2520analyze%2520mutual%2520failure%2520modes.%2520Our%2520results%250Ahighlight%2520the%2520current%2520limitations%2520of%2520autonomous%2520action%2520in%2520agentic%2520systems%252C%2520and%250Aexpose%2520promising%2520future%2520research%2520directions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19771v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Reactivity%3A%20Measuring%20Proactive%20Problem%20Solving%20in%20LLM%20Agents&entry.906535625=Gil%20Pasternak%20and%20Dheeraj%20Rajagopal%20and%20Julia%20White%20and%20Dhruv%20Atreja%20and%20Matthew%20Thomas%20and%20George%20Hurn-Maloney%20and%20Ash%20Lewis&entry.1292438233=%20%20LLM-based%20agents%20are%20increasingly%20moving%20towards%20proactivity%3A%20rather%20than%0Aawaiting%20instruction%2C%20they%20exercise%20agency%20to%20anticipate%20user%20needs%20and%20solve%0Athem%20autonomously.%20However%2C%20evaluating%20proactivity%20is%20challenging%3B%20current%0Abenchmarks%20are%20constrained%20to%20localized%20context%2C%20limiting%20their%20ability%20to%20test%0Areasoning%20across%20sources%20and%20longer%20time%20horizons.%20To%20address%20this%20gap%2C%20we%0Apresent%20PROBE%20%28Proactive%20Resolution%20Of%20BottlEnecks%29.%20PROBE%20decomposes%0Aproactivity%20as%20a%20pipeline%20of%20three%20core%20capabilities%3A%20%281%29%20searching%20for%0Aunspecified%20issues%2C%20%282%29%20identifying%20specific%20bottlenecks%2C%20and%20%283%29%20executing%0Aappropriate%20resolutions.%20We%20apply%20PROBE%20to%20evaluate%20leading%20LLMs%20and%20popular%0Aagentic%20frameworks%2C%20showing%20that%20even%20state-of-the-art%20models%20struggle%20to%20solve%0Athis%20benchmark.%20Computing%20our%20consistent%20measurements%20across%20frontier%20LLMs%20and%0Aagents%2C%20we%20find%20that%20the%20best%20end-to-end%20performance%20of%2040%25%20is%20achieved%20by%20both%0AGPT-5%20and%20Claude%20Opus-4.1.%20Additionally%2C%20we%20demonstrate%20the%20relative%0Acapabilities%20of%20each%20model%20and%20analyze%20mutual%20failure%20modes.%20Our%20results%0Ahighlight%20the%20current%20limitations%20of%20autonomous%20action%20in%20agentic%20systems%2C%20and%0Aexpose%20promising%20future%20research%20directions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19771v1&entry.124074799=Read"},
{"title": "TinySQL: A Progressive Text-to-SQL Dataset for Mechanistic\n  Interpretability Research", "author": "Abir Harrasse and Philip Quirke and Clement Neo and Dhruv Nathawani and Luke Marks and Amir Abdullah", "abstract": "  Mechanistic interpretability research faces a gap between analyzing simple\ncircuits in toy tasks and discovering features in large models. To bridge this\ngap, we propose text-to-SQL generation as an ideal task to study, as it\ncombines the formal structure of toy tasks with real-world complexity. We\nintroduce TinySQL, a synthetic dataset, progressing from basic to advanced SQL\noperations, and train models ranging from 33M to 1B parameters to establish a\ncomprehensive testbed for interpretability. We apply multiple complementary\ninterpretability techniques, including Edge Attribution Patching and Sparse\nAutoencoders, to identify minimal circuits and components supporting SQL\ngeneration. We compare circuits for different SQL subskills, evaluating their\nminimality, reliability, and identifiability. Finally, we conduct a layerwise\nlogit lens analysis to reveal how models compose SQL queries across layers:\nfrom intent recognition to schema resolution to structured generation. Our work\nprovides a robust framework for probing and comparing interpretability methods\nin a structured, progressively complex setting.\n", "link": "http://arxiv.org/abs/2503.12730v5", "date": "2025-10-22", "relevancy": 1.9872, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5068}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4923}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TinySQL%3A%20A%20Progressive%20Text-to-SQL%20Dataset%20for%20Mechanistic%0A%20%20Interpretability%20Research&body=Title%3A%20TinySQL%3A%20A%20Progressive%20Text-to-SQL%20Dataset%20for%20Mechanistic%0A%20%20Interpretability%20Research%0AAuthor%3A%20Abir%20Harrasse%20and%20Philip%20Quirke%20and%20Clement%20Neo%20and%20Dhruv%20Nathawani%20and%20Luke%20Marks%20and%20Amir%20Abdullah%0AAbstract%3A%20%20%20Mechanistic%20interpretability%20research%20faces%20a%20gap%20between%20analyzing%20simple%0Acircuits%20in%20toy%20tasks%20and%20discovering%20features%20in%20large%20models.%20To%20bridge%20this%0Agap%2C%20we%20propose%20text-to-SQL%20generation%20as%20an%20ideal%20task%20to%20study%2C%20as%20it%0Acombines%20the%20formal%20structure%20of%20toy%20tasks%20with%20real-world%20complexity.%20We%0Aintroduce%20TinySQL%2C%20a%20synthetic%20dataset%2C%20progressing%20from%20basic%20to%20advanced%20SQL%0Aoperations%2C%20and%20train%20models%20ranging%20from%2033M%20to%201B%20parameters%20to%20establish%20a%0Acomprehensive%20testbed%20for%20interpretability.%20We%20apply%20multiple%20complementary%0Ainterpretability%20techniques%2C%20including%20Edge%20Attribution%20Patching%20and%20Sparse%0AAutoencoders%2C%20to%20identify%20minimal%20circuits%20and%20components%20supporting%20SQL%0Ageneration.%20We%20compare%20circuits%20for%20different%20SQL%20subskills%2C%20evaluating%20their%0Aminimality%2C%20reliability%2C%20and%20identifiability.%20Finally%2C%20we%20conduct%20a%20layerwise%0Alogit%20lens%20analysis%20to%20reveal%20how%20models%20compose%20SQL%20queries%20across%20layers%3A%0Afrom%20intent%20recognition%20to%20schema%20resolution%20to%20structured%20generation.%20Our%20work%0Aprovides%20a%20robust%20framework%20for%20probing%20and%20comparing%20interpretability%20methods%0Ain%20a%20structured%2C%20progressively%20complex%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.12730v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTinySQL%253A%2520A%2520Progressive%2520Text-to-SQL%2520Dataset%2520for%2520Mechanistic%250A%2520%2520Interpretability%2520Research%26entry.906535625%3DAbir%2520Harrasse%2520and%2520Philip%2520Quirke%2520and%2520Clement%2520Neo%2520and%2520Dhruv%2520Nathawani%2520and%2520Luke%2520Marks%2520and%2520Amir%2520Abdullah%26entry.1292438233%3D%2520%2520Mechanistic%2520interpretability%2520research%2520faces%2520a%2520gap%2520between%2520analyzing%2520simple%250Acircuits%2520in%2520toy%2520tasks%2520and%2520discovering%2520features%2520in%2520large%2520models.%2520To%2520bridge%2520this%250Agap%252C%2520we%2520propose%2520text-to-SQL%2520generation%2520as%2520an%2520ideal%2520task%2520to%2520study%252C%2520as%2520it%250Acombines%2520the%2520formal%2520structure%2520of%2520toy%2520tasks%2520with%2520real-world%2520complexity.%2520We%250Aintroduce%2520TinySQL%252C%2520a%2520synthetic%2520dataset%252C%2520progressing%2520from%2520basic%2520to%2520advanced%2520SQL%250Aoperations%252C%2520and%2520train%2520models%2520ranging%2520from%252033M%2520to%25201B%2520parameters%2520to%2520establish%2520a%250Acomprehensive%2520testbed%2520for%2520interpretability.%2520We%2520apply%2520multiple%2520complementary%250Ainterpretability%2520techniques%252C%2520including%2520Edge%2520Attribution%2520Patching%2520and%2520Sparse%250AAutoencoders%252C%2520to%2520identify%2520minimal%2520circuits%2520and%2520components%2520supporting%2520SQL%250Ageneration.%2520We%2520compare%2520circuits%2520for%2520different%2520SQL%2520subskills%252C%2520evaluating%2520their%250Aminimality%252C%2520reliability%252C%2520and%2520identifiability.%2520Finally%252C%2520we%2520conduct%2520a%2520layerwise%250Alogit%2520lens%2520analysis%2520to%2520reveal%2520how%2520models%2520compose%2520SQL%2520queries%2520across%2520layers%253A%250Afrom%2520intent%2520recognition%2520to%2520schema%2520resolution%2520to%2520structured%2520generation.%2520Our%2520work%250Aprovides%2520a%2520robust%2520framework%2520for%2520probing%2520and%2520comparing%2520interpretability%2520methods%250Ain%2520a%2520structured%252C%2520progressively%2520complex%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.12730v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TinySQL%3A%20A%20Progressive%20Text-to-SQL%20Dataset%20for%20Mechanistic%0A%20%20Interpretability%20Research&entry.906535625=Abir%20Harrasse%20and%20Philip%20Quirke%20and%20Clement%20Neo%20and%20Dhruv%20Nathawani%20and%20Luke%20Marks%20and%20Amir%20Abdullah&entry.1292438233=%20%20Mechanistic%20interpretability%20research%20faces%20a%20gap%20between%20analyzing%20simple%0Acircuits%20in%20toy%20tasks%20and%20discovering%20features%20in%20large%20models.%20To%20bridge%20this%0Agap%2C%20we%20propose%20text-to-SQL%20generation%20as%20an%20ideal%20task%20to%20study%2C%20as%20it%0Acombines%20the%20formal%20structure%20of%20toy%20tasks%20with%20real-world%20complexity.%20We%0Aintroduce%20TinySQL%2C%20a%20synthetic%20dataset%2C%20progressing%20from%20basic%20to%20advanced%20SQL%0Aoperations%2C%20and%20train%20models%20ranging%20from%2033M%20to%201B%20parameters%20to%20establish%20a%0Acomprehensive%20testbed%20for%20interpretability.%20We%20apply%20multiple%20complementary%0Ainterpretability%20techniques%2C%20including%20Edge%20Attribution%20Patching%20and%20Sparse%0AAutoencoders%2C%20to%20identify%20minimal%20circuits%20and%20components%20supporting%20SQL%0Ageneration.%20We%20compare%20circuits%20for%20different%20SQL%20subskills%2C%20evaluating%20their%0Aminimality%2C%20reliability%2C%20and%20identifiability.%20Finally%2C%20we%20conduct%20a%20layerwise%0Alogit%20lens%20analysis%20to%20reveal%20how%20models%20compose%20SQL%20queries%20across%20layers%3A%0Afrom%20intent%20recognition%20to%20schema%20resolution%20to%20structured%20generation.%20Our%20work%0Aprovides%20a%20robust%20framework%20for%20probing%20and%20comparing%20interpretability%20methods%0Ain%20a%20structured%2C%20progressively%20complex%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.12730v5&entry.124074799=Read"},
{"title": "Preconditioned Norms: A Unified Framework for Steepest Descent,\n  Quasi-Newton and Adaptive Methods", "author": "Andrey Veprikov and Arman Bolatov and Samuel Horv\u00e1th and Aleksandr Beznosikov and Martin Tak\u00e1\u010d and Slavomir Hanzely", "abstract": "  Optimization lies at the core of modern deep learning, yet existing methods\noften face a fundamental trade-off between adapting to problem geometry and\nleveraging curvature utilization. Steepest descent algorithms adapt to\ndifferent geometries through norm choices but remain strictly first-order,\nwhereas quasi-Newton and adaptive optimizers incorporate curvature information\nbut are restricted to Frobenius geometry, limiting their applicability across\ndiverse architectures. In this work, we propose a unified framework\ngeneralizing steepest descent, quasi-Newton methods, and adaptive methods\nthrough the novel notion of preconditioned matrix norms. This abstraction\nreveals that widely used optimizers such as SGD and Adam, as well as more\nadvanced approaches like Muon and KL-Shampoo, and recent hybrids including SOAP\nand SPlus, all emerge as special cases of the same principle. Within this\nframework, we provide the first systematic treatment of affine and scale\ninvariance in the matrix-parameterized setting, establishing necessary and\nsufficient conditions under generalized norms. Building on this foundation, we\nintroduce two new methods, $\\texttt{MuAdam}$ and $\\texttt{MuAdam-SANIA}$, which\ncombine the spectral geometry of Muon with Adam-style preconditioning. Our\nexperiments demonstrate that these optimizers are competitive with, and in some\ncases outperform, existing state-of-the-art methods. Our code is available at\nhttps://github.com/brain-lab-research/LIB/tree/quasi_descent\n", "link": "http://arxiv.org/abs/2510.10777v2", "date": "2025-10-22", "relevancy": 1.985, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5086}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4954}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4921}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Preconditioned%20Norms%3A%20A%20Unified%20Framework%20for%20Steepest%20Descent%2C%0A%20%20Quasi-Newton%20and%20Adaptive%20Methods&body=Title%3A%20Preconditioned%20Norms%3A%20A%20Unified%20Framework%20for%20Steepest%20Descent%2C%0A%20%20Quasi-Newton%20and%20Adaptive%20Methods%0AAuthor%3A%20Andrey%20Veprikov%20and%20Arman%20Bolatov%20and%20Samuel%20Horv%C3%A1th%20and%20Aleksandr%20Beznosikov%20and%20Martin%20Tak%C3%A1%C4%8D%20and%20Slavomir%20Hanzely%0AAbstract%3A%20%20%20Optimization%20lies%20at%20the%20core%20of%20modern%20deep%20learning%2C%20yet%20existing%20methods%0Aoften%20face%20a%20fundamental%20trade-off%20between%20adapting%20to%20problem%20geometry%20and%0Aleveraging%20curvature%20utilization.%20Steepest%20descent%20algorithms%20adapt%20to%0Adifferent%20geometries%20through%20norm%20choices%20but%20remain%20strictly%20first-order%2C%0Awhereas%20quasi-Newton%20and%20adaptive%20optimizers%20incorporate%20curvature%20information%0Abut%20are%20restricted%20to%20Frobenius%20geometry%2C%20limiting%20their%20applicability%20across%0Adiverse%20architectures.%20In%20this%20work%2C%20we%20propose%20a%20unified%20framework%0Ageneralizing%20steepest%20descent%2C%20quasi-Newton%20methods%2C%20and%20adaptive%20methods%0Athrough%20the%20novel%20notion%20of%20preconditioned%20matrix%20norms.%20This%20abstraction%0Areveals%20that%20widely%20used%20optimizers%20such%20as%20SGD%20and%20Adam%2C%20as%20well%20as%20more%0Aadvanced%20approaches%20like%20Muon%20and%20KL-Shampoo%2C%20and%20recent%20hybrids%20including%20SOAP%0Aand%20SPlus%2C%20all%20emerge%20as%20special%20cases%20of%20the%20same%20principle.%20Within%20this%0Aframework%2C%20we%20provide%20the%20first%20systematic%20treatment%20of%20affine%20and%20scale%0Ainvariance%20in%20the%20matrix-parameterized%20setting%2C%20establishing%20necessary%20and%0Asufficient%20conditions%20under%20generalized%20norms.%20Building%20on%20this%20foundation%2C%20we%0Aintroduce%20two%20new%20methods%2C%20%24%5Ctexttt%7BMuAdam%7D%24%20and%20%24%5Ctexttt%7BMuAdam-SANIA%7D%24%2C%20which%0Acombine%20the%20spectral%20geometry%20of%20Muon%20with%20Adam-style%20preconditioning.%20Our%0Aexperiments%20demonstrate%20that%20these%20optimizers%20are%20competitive%20with%2C%20and%20in%20some%0Acases%20outperform%2C%20existing%20state-of-the-art%20methods.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/brain-lab-research/LIB/tree/quasi_descent%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.10777v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPreconditioned%2520Norms%253A%2520A%2520Unified%2520Framework%2520for%2520Steepest%2520Descent%252C%250A%2520%2520Quasi-Newton%2520and%2520Adaptive%2520Methods%26entry.906535625%3DAndrey%2520Veprikov%2520and%2520Arman%2520Bolatov%2520and%2520Samuel%2520Horv%25C3%25A1th%2520and%2520Aleksandr%2520Beznosikov%2520and%2520Martin%2520Tak%25C3%25A1%25C4%258D%2520and%2520Slavomir%2520Hanzely%26entry.1292438233%3D%2520%2520Optimization%2520lies%2520at%2520the%2520core%2520of%2520modern%2520deep%2520learning%252C%2520yet%2520existing%2520methods%250Aoften%2520face%2520a%2520fundamental%2520trade-off%2520between%2520adapting%2520to%2520problem%2520geometry%2520and%250Aleveraging%2520curvature%2520utilization.%2520Steepest%2520descent%2520algorithms%2520adapt%2520to%250Adifferent%2520geometries%2520through%2520norm%2520choices%2520but%2520remain%2520strictly%2520first-order%252C%250Awhereas%2520quasi-Newton%2520and%2520adaptive%2520optimizers%2520incorporate%2520curvature%2520information%250Abut%2520are%2520restricted%2520to%2520Frobenius%2520geometry%252C%2520limiting%2520their%2520applicability%2520across%250Adiverse%2520architectures.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520unified%2520framework%250Ageneralizing%2520steepest%2520descent%252C%2520quasi-Newton%2520methods%252C%2520and%2520adaptive%2520methods%250Athrough%2520the%2520novel%2520notion%2520of%2520preconditioned%2520matrix%2520norms.%2520This%2520abstraction%250Areveals%2520that%2520widely%2520used%2520optimizers%2520such%2520as%2520SGD%2520and%2520Adam%252C%2520as%2520well%2520as%2520more%250Aadvanced%2520approaches%2520like%2520Muon%2520and%2520KL-Shampoo%252C%2520and%2520recent%2520hybrids%2520including%2520SOAP%250Aand%2520SPlus%252C%2520all%2520emerge%2520as%2520special%2520cases%2520of%2520the%2520same%2520principle.%2520Within%2520this%250Aframework%252C%2520we%2520provide%2520the%2520first%2520systematic%2520treatment%2520of%2520affine%2520and%2520scale%250Ainvariance%2520in%2520the%2520matrix-parameterized%2520setting%252C%2520establishing%2520necessary%2520and%250Asufficient%2520conditions%2520under%2520generalized%2520norms.%2520Building%2520on%2520this%2520foundation%252C%2520we%250Aintroduce%2520two%2520new%2520methods%252C%2520%2524%255Ctexttt%257BMuAdam%257D%2524%2520and%2520%2524%255Ctexttt%257BMuAdam-SANIA%257D%2524%252C%2520which%250Acombine%2520the%2520spectral%2520geometry%2520of%2520Muon%2520with%2520Adam-style%2520preconditioning.%2520Our%250Aexperiments%2520demonstrate%2520that%2520these%2520optimizers%2520are%2520competitive%2520with%252C%2520and%2520in%2520some%250Acases%2520outperform%252C%2520existing%2520state-of-the-art%2520methods.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/brain-lab-research/LIB/tree/quasi_descent%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.10777v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Preconditioned%20Norms%3A%20A%20Unified%20Framework%20for%20Steepest%20Descent%2C%0A%20%20Quasi-Newton%20and%20Adaptive%20Methods&entry.906535625=Andrey%20Veprikov%20and%20Arman%20Bolatov%20and%20Samuel%20Horv%C3%A1th%20and%20Aleksandr%20Beznosikov%20and%20Martin%20Tak%C3%A1%C4%8D%20and%20Slavomir%20Hanzely&entry.1292438233=%20%20Optimization%20lies%20at%20the%20core%20of%20modern%20deep%20learning%2C%20yet%20existing%20methods%0Aoften%20face%20a%20fundamental%20trade-off%20between%20adapting%20to%20problem%20geometry%20and%0Aleveraging%20curvature%20utilization.%20Steepest%20descent%20algorithms%20adapt%20to%0Adifferent%20geometries%20through%20norm%20choices%20but%20remain%20strictly%20first-order%2C%0Awhereas%20quasi-Newton%20and%20adaptive%20optimizers%20incorporate%20curvature%20information%0Abut%20are%20restricted%20to%20Frobenius%20geometry%2C%20limiting%20their%20applicability%20across%0Adiverse%20architectures.%20In%20this%20work%2C%20we%20propose%20a%20unified%20framework%0Ageneralizing%20steepest%20descent%2C%20quasi-Newton%20methods%2C%20and%20adaptive%20methods%0Athrough%20the%20novel%20notion%20of%20preconditioned%20matrix%20norms.%20This%20abstraction%0Areveals%20that%20widely%20used%20optimizers%20such%20as%20SGD%20and%20Adam%2C%20as%20well%20as%20more%0Aadvanced%20approaches%20like%20Muon%20and%20KL-Shampoo%2C%20and%20recent%20hybrids%20including%20SOAP%0Aand%20SPlus%2C%20all%20emerge%20as%20special%20cases%20of%20the%20same%20principle.%20Within%20this%0Aframework%2C%20we%20provide%20the%20first%20systematic%20treatment%20of%20affine%20and%20scale%0Ainvariance%20in%20the%20matrix-parameterized%20setting%2C%20establishing%20necessary%20and%0Asufficient%20conditions%20under%20generalized%20norms.%20Building%20on%20this%20foundation%2C%20we%0Aintroduce%20two%20new%20methods%2C%20%24%5Ctexttt%7BMuAdam%7D%24%20and%20%24%5Ctexttt%7BMuAdam-SANIA%7D%24%2C%20which%0Acombine%20the%20spectral%20geometry%20of%20Muon%20with%20Adam-style%20preconditioning.%20Our%0Aexperiments%20demonstrate%20that%20these%20optimizers%20are%20competitive%20with%2C%20and%20in%20some%0Acases%20outperform%2C%20existing%20state-of-the-art%20methods.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/brain-lab-research/LIB/tree/quasi_descent%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.10777v2&entry.124074799=Read"},
{"title": "Unlearned but Not Forgotten: Data Extraction after Exact Unlearning in\n  LLM", "author": "Xiaoyu Wu and Yifei Pang and Terrance Liu and Zhiwei Steven Wu", "abstract": "  Large Language Models are typically trained on datasets collected from the\nweb, which may inadvertently contain harmful or sensitive personal information.\nTo address growing privacy concerns, unlearning methods have been proposed to\nremove the influence of specific data from trained models. Of these, exact\nunlearning -- which retrains the model from scratch without the target data --\nis widely regarded the gold standard for mitigating privacy risks in\ndeployment. In this paper, we revisit this assumption in a practical deployment\nsetting where both the pre- and post-unlearning logits API are exposed, such as\nin open-weight scenarios. Targeting this setting, we introduce a novel data\nextraction attack that leverages signals from the pre-unlearning model to guide\nthe post-unlearning model, uncovering patterns that reflect the removed data\ndistribution. Combining model guidance with a token filtering strategy, our\nattack significantly improves extraction success rates -- doubling performance\nin some cases -- across common benchmarks such as MUSE, TOFU, and WMDP.\nFurthermore, we demonstrate our attack's effectiveness on a simulated medical\ndiagnosis dataset to highlight real-world privacy risks associated with exact\nunlearning. In light of our findings, which suggest that unlearning may, in a\ncontradictory way, increase the risk of privacy leakage during real-world\ndeployments, we advocate for evaluation of unlearning methods to consider\nbroader threat models that account not only for post-unlearning models but also\nfor adversarial access to prior checkpoints. Code is publicly available at:\nhttps://github.com/Nicholas0228/unlearned_data_extraction_llm.\n", "link": "http://arxiv.org/abs/2505.24379v3", "date": "2025-10-22", "relevancy": 1.9763, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5664}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4834}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unlearned%20but%20Not%20Forgotten%3A%20Data%20Extraction%20after%20Exact%20Unlearning%20in%0A%20%20LLM&body=Title%3A%20Unlearned%20but%20Not%20Forgotten%3A%20Data%20Extraction%20after%20Exact%20Unlearning%20in%0A%20%20LLM%0AAuthor%3A%20Xiaoyu%20Wu%20and%20Yifei%20Pang%20and%20Terrance%20Liu%20and%20Zhiwei%20Steven%20Wu%0AAbstract%3A%20%20%20Large%20Language%20Models%20are%20typically%20trained%20on%20datasets%20collected%20from%20the%0Aweb%2C%20which%20may%20inadvertently%20contain%20harmful%20or%20sensitive%20personal%20information.%0ATo%20address%20growing%20privacy%20concerns%2C%20unlearning%20methods%20have%20been%20proposed%20to%0Aremove%20the%20influence%20of%20specific%20data%20from%20trained%20models.%20Of%20these%2C%20exact%0Aunlearning%20--%20which%20retrains%20the%20model%20from%20scratch%20without%20the%20target%20data%20--%0Ais%20widely%20regarded%20the%20gold%20standard%20for%20mitigating%20privacy%20risks%20in%0Adeployment.%20In%20this%20paper%2C%20we%20revisit%20this%20assumption%20in%20a%20practical%20deployment%0Asetting%20where%20both%20the%20pre-%20and%20post-unlearning%20logits%20API%20are%20exposed%2C%20such%20as%0Ain%20open-weight%20scenarios.%20Targeting%20this%20setting%2C%20we%20introduce%20a%20novel%20data%0Aextraction%20attack%20that%20leverages%20signals%20from%20the%20pre-unlearning%20model%20to%20guide%0Athe%20post-unlearning%20model%2C%20uncovering%20patterns%20that%20reflect%20the%20removed%20data%0Adistribution.%20Combining%20model%20guidance%20with%20a%20token%20filtering%20strategy%2C%20our%0Aattack%20significantly%20improves%20extraction%20success%20rates%20--%20doubling%20performance%0Ain%20some%20cases%20--%20across%20common%20benchmarks%20such%20as%20MUSE%2C%20TOFU%2C%20and%20WMDP.%0AFurthermore%2C%20we%20demonstrate%20our%20attack%27s%20effectiveness%20on%20a%20simulated%20medical%0Adiagnosis%20dataset%20to%20highlight%20real-world%20privacy%20risks%20associated%20with%20exact%0Aunlearning.%20In%20light%20of%20our%20findings%2C%20which%20suggest%20that%20unlearning%20may%2C%20in%20a%0Acontradictory%20way%2C%20increase%20the%20risk%20of%20privacy%20leakage%20during%20real-world%0Adeployments%2C%20we%20advocate%20for%20evaluation%20of%20unlearning%20methods%20to%20consider%0Abroader%20threat%20models%20that%20account%20not%20only%20for%20post-unlearning%20models%20but%20also%0Afor%20adversarial%20access%20to%20prior%20checkpoints.%20Code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/Nicholas0228/unlearned_data_extraction_llm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24379v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnlearned%2520but%2520Not%2520Forgotten%253A%2520Data%2520Extraction%2520after%2520Exact%2520Unlearning%2520in%250A%2520%2520LLM%26entry.906535625%3DXiaoyu%2520Wu%2520and%2520Yifei%2520Pang%2520and%2520Terrance%2520Liu%2520and%2520Zhiwei%2520Steven%2520Wu%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520are%2520typically%2520trained%2520on%2520datasets%2520collected%2520from%2520the%250Aweb%252C%2520which%2520may%2520inadvertently%2520contain%2520harmful%2520or%2520sensitive%2520personal%2520information.%250ATo%2520address%2520growing%2520privacy%2520concerns%252C%2520unlearning%2520methods%2520have%2520been%2520proposed%2520to%250Aremove%2520the%2520influence%2520of%2520specific%2520data%2520from%2520trained%2520models.%2520Of%2520these%252C%2520exact%250Aunlearning%2520--%2520which%2520retrains%2520the%2520model%2520from%2520scratch%2520without%2520the%2520target%2520data%2520--%250Ais%2520widely%2520regarded%2520the%2520gold%2520standard%2520for%2520mitigating%2520privacy%2520risks%2520in%250Adeployment.%2520In%2520this%2520paper%252C%2520we%2520revisit%2520this%2520assumption%2520in%2520a%2520practical%2520deployment%250Asetting%2520where%2520both%2520the%2520pre-%2520and%2520post-unlearning%2520logits%2520API%2520are%2520exposed%252C%2520such%2520as%250Ain%2520open-weight%2520scenarios.%2520Targeting%2520this%2520setting%252C%2520we%2520introduce%2520a%2520novel%2520data%250Aextraction%2520attack%2520that%2520leverages%2520signals%2520from%2520the%2520pre-unlearning%2520model%2520to%2520guide%250Athe%2520post-unlearning%2520model%252C%2520uncovering%2520patterns%2520that%2520reflect%2520the%2520removed%2520data%250Adistribution.%2520Combining%2520model%2520guidance%2520with%2520a%2520token%2520filtering%2520strategy%252C%2520our%250Aattack%2520significantly%2520improves%2520extraction%2520success%2520rates%2520--%2520doubling%2520performance%250Ain%2520some%2520cases%2520--%2520across%2520common%2520benchmarks%2520such%2520as%2520MUSE%252C%2520TOFU%252C%2520and%2520WMDP.%250AFurthermore%252C%2520we%2520demonstrate%2520our%2520attack%2527s%2520effectiveness%2520on%2520a%2520simulated%2520medical%250Adiagnosis%2520dataset%2520to%2520highlight%2520real-world%2520privacy%2520risks%2520associated%2520with%2520exact%250Aunlearning.%2520In%2520light%2520of%2520our%2520findings%252C%2520which%2520suggest%2520that%2520unlearning%2520may%252C%2520in%2520a%250Acontradictory%2520way%252C%2520increase%2520the%2520risk%2520of%2520privacy%2520leakage%2520during%2520real-world%250Adeployments%252C%2520we%2520advocate%2520for%2520evaluation%2520of%2520unlearning%2520methods%2520to%2520consider%250Abroader%2520threat%2520models%2520that%2520account%2520not%2520only%2520for%2520post-unlearning%2520models%2520but%2520also%250Afor%2520adversarial%2520access%2520to%2520prior%2520checkpoints.%2520Code%2520is%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/Nicholas0228/unlearned_data_extraction_llm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24379v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unlearned%20but%20Not%20Forgotten%3A%20Data%20Extraction%20after%20Exact%20Unlearning%20in%0A%20%20LLM&entry.906535625=Xiaoyu%20Wu%20and%20Yifei%20Pang%20and%20Terrance%20Liu%20and%20Zhiwei%20Steven%20Wu&entry.1292438233=%20%20Large%20Language%20Models%20are%20typically%20trained%20on%20datasets%20collected%20from%20the%0Aweb%2C%20which%20may%20inadvertently%20contain%20harmful%20or%20sensitive%20personal%20information.%0ATo%20address%20growing%20privacy%20concerns%2C%20unlearning%20methods%20have%20been%20proposed%20to%0Aremove%20the%20influence%20of%20specific%20data%20from%20trained%20models.%20Of%20these%2C%20exact%0Aunlearning%20--%20which%20retrains%20the%20model%20from%20scratch%20without%20the%20target%20data%20--%0Ais%20widely%20regarded%20the%20gold%20standard%20for%20mitigating%20privacy%20risks%20in%0Adeployment.%20In%20this%20paper%2C%20we%20revisit%20this%20assumption%20in%20a%20practical%20deployment%0Asetting%20where%20both%20the%20pre-%20and%20post-unlearning%20logits%20API%20are%20exposed%2C%20such%20as%0Ain%20open-weight%20scenarios.%20Targeting%20this%20setting%2C%20we%20introduce%20a%20novel%20data%0Aextraction%20attack%20that%20leverages%20signals%20from%20the%20pre-unlearning%20model%20to%20guide%0Athe%20post-unlearning%20model%2C%20uncovering%20patterns%20that%20reflect%20the%20removed%20data%0Adistribution.%20Combining%20model%20guidance%20with%20a%20token%20filtering%20strategy%2C%20our%0Aattack%20significantly%20improves%20extraction%20success%20rates%20--%20doubling%20performance%0Ain%20some%20cases%20--%20across%20common%20benchmarks%20such%20as%20MUSE%2C%20TOFU%2C%20and%20WMDP.%0AFurthermore%2C%20we%20demonstrate%20our%20attack%27s%20effectiveness%20on%20a%20simulated%20medical%0Adiagnosis%20dataset%20to%20highlight%20real-world%20privacy%20risks%20associated%20with%20exact%0Aunlearning.%20In%20light%20of%20our%20findings%2C%20which%20suggest%20that%20unlearning%20may%2C%20in%20a%0Acontradictory%20way%2C%20increase%20the%20risk%20of%20privacy%20leakage%20during%20real-world%0Adeployments%2C%20we%20advocate%20for%20evaluation%20of%20unlearning%20methods%20to%20consider%0Abroader%20threat%20models%20that%20account%20not%20only%20for%20post-unlearning%20models%20but%20also%0Afor%20adversarial%20access%20to%20prior%20checkpoints.%20Code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/Nicholas0228/unlearned_data_extraction_llm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24379v3&entry.124074799=Read"},
{"title": "InfiFPO: Implicit Model Fusion via Preference Optimization in Large\n  Language Models", "author": "Yanggan Gu and Yuanyi Wang and Zhaoyi Yan and Yiming Zhang and Qi Zhou and Fei Wu and Hongxia Yang", "abstract": "  Model fusion combines multiple Large Language Models (LLMs) with different\nstrengths into a more powerful, integrated model through lightweight training\nmethods. Existing works on model fusion focus primarily on supervised\nfine-tuning (SFT), leaving preference alignment (PA) --a critical phase for\nenhancing LLM performance--largely unexplored. The current few fusion methods\non PA phase, like WRPO, simplify the process by utilizing only response outputs\nfrom source models while discarding their probability information. To address\nthis limitation, we propose InfiFPO, a preference optimization method for\nimplicit model fusion. InfiFPO replaces the reference model in Direct\nPreference Optimization (DPO) with a fused source model that synthesizes\nmulti-source probabilities at the sequence level, circumventing complex\nvocabulary alignment challenges in previous works and meanwhile maintaining the\nprobability information. By introducing probability clipping and max-margin\nfusion strategies, InfiFPO enables the pivot model to align with human\npreferences while effectively distilling knowledge from source models.\nComprehensive experiments on 11 widely-used benchmarks demonstrate that InfiFPO\nconsistently outperforms existing model fusion and preference optimization\nmethods. When using Phi-4 as the pivot model, InfiFPO improve its average\nperformance from 79.95 to 83.33 on 11 benchmarks, significantly improving its\ncapabilities in mathematics, coding, and reasoning tasks.\n", "link": "http://arxiv.org/abs/2505.13878v2", "date": "2025-10-22", "relevancy": 1.9726, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4961}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4961}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InfiFPO%3A%20Implicit%20Model%20Fusion%20via%20Preference%20Optimization%20in%20Large%0A%20%20Language%20Models&body=Title%3A%20InfiFPO%3A%20Implicit%20Model%20Fusion%20via%20Preference%20Optimization%20in%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Yanggan%20Gu%20and%20Yuanyi%20Wang%20and%20Zhaoyi%20Yan%20and%20Yiming%20Zhang%20and%20Qi%20Zhou%20and%20Fei%20Wu%20and%20Hongxia%20Yang%0AAbstract%3A%20%20%20Model%20fusion%20combines%20multiple%20Large%20Language%20Models%20%28LLMs%29%20with%20different%0Astrengths%20into%20a%20more%20powerful%2C%20integrated%20model%20through%20lightweight%20training%0Amethods.%20Existing%20works%20on%20model%20fusion%20focus%20primarily%20on%20supervised%0Afine-tuning%20%28SFT%29%2C%20leaving%20preference%20alignment%20%28PA%29%20--a%20critical%20phase%20for%0Aenhancing%20LLM%20performance--largely%20unexplored.%20The%20current%20few%20fusion%20methods%0Aon%20PA%20phase%2C%20like%20WRPO%2C%20simplify%20the%20process%20by%20utilizing%20only%20response%20outputs%0Afrom%20source%20models%20while%20discarding%20their%20probability%20information.%20To%20address%0Athis%20limitation%2C%20we%20propose%20InfiFPO%2C%20a%20preference%20optimization%20method%20for%0Aimplicit%20model%20fusion.%20InfiFPO%20replaces%20the%20reference%20model%20in%20Direct%0APreference%20Optimization%20%28DPO%29%20with%20a%20fused%20source%20model%20that%20synthesizes%0Amulti-source%20probabilities%20at%20the%20sequence%20level%2C%20circumventing%20complex%0Avocabulary%20alignment%20challenges%20in%20previous%20works%20and%20meanwhile%20maintaining%20the%0Aprobability%20information.%20By%20introducing%20probability%20clipping%20and%20max-margin%0Afusion%20strategies%2C%20InfiFPO%20enables%20the%20pivot%20model%20to%20align%20with%20human%0Apreferences%20while%20effectively%20distilling%20knowledge%20from%20source%20models.%0AComprehensive%20experiments%20on%2011%20widely-used%20benchmarks%20demonstrate%20that%20InfiFPO%0Aconsistently%20outperforms%20existing%20model%20fusion%20and%20preference%20optimization%0Amethods.%20When%20using%20Phi-4%20as%20the%20pivot%20model%2C%20InfiFPO%20improve%20its%20average%0Aperformance%20from%2079.95%20to%2083.33%20on%2011%20benchmarks%2C%20significantly%20improving%20its%0Acapabilities%20in%20mathematics%2C%20coding%2C%20and%20reasoning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13878v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfiFPO%253A%2520Implicit%2520Model%2520Fusion%2520via%2520Preference%2520Optimization%2520in%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DYanggan%2520Gu%2520and%2520Yuanyi%2520Wang%2520and%2520Zhaoyi%2520Yan%2520and%2520Yiming%2520Zhang%2520and%2520Qi%2520Zhou%2520and%2520Fei%2520Wu%2520and%2520Hongxia%2520Yang%26entry.1292438233%3D%2520%2520Model%2520fusion%2520combines%2520multiple%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520with%2520different%250Astrengths%2520into%2520a%2520more%2520powerful%252C%2520integrated%2520model%2520through%2520lightweight%2520training%250Amethods.%2520Existing%2520works%2520on%2520model%2520fusion%2520focus%2520primarily%2520on%2520supervised%250Afine-tuning%2520%2528SFT%2529%252C%2520leaving%2520preference%2520alignment%2520%2528PA%2529%2520--a%2520critical%2520phase%2520for%250Aenhancing%2520LLM%2520performance--largely%2520unexplored.%2520The%2520current%2520few%2520fusion%2520methods%250Aon%2520PA%2520phase%252C%2520like%2520WRPO%252C%2520simplify%2520the%2520process%2520by%2520utilizing%2520only%2520response%2520outputs%250Afrom%2520source%2520models%2520while%2520discarding%2520their%2520probability%2520information.%2520To%2520address%250Athis%2520limitation%252C%2520we%2520propose%2520InfiFPO%252C%2520a%2520preference%2520optimization%2520method%2520for%250Aimplicit%2520model%2520fusion.%2520InfiFPO%2520replaces%2520the%2520reference%2520model%2520in%2520Direct%250APreference%2520Optimization%2520%2528DPO%2529%2520with%2520a%2520fused%2520source%2520model%2520that%2520synthesizes%250Amulti-source%2520probabilities%2520at%2520the%2520sequence%2520level%252C%2520circumventing%2520complex%250Avocabulary%2520alignment%2520challenges%2520in%2520previous%2520works%2520and%2520meanwhile%2520maintaining%2520the%250Aprobability%2520information.%2520By%2520introducing%2520probability%2520clipping%2520and%2520max-margin%250Afusion%2520strategies%252C%2520InfiFPO%2520enables%2520the%2520pivot%2520model%2520to%2520align%2520with%2520human%250Apreferences%2520while%2520effectively%2520distilling%2520knowledge%2520from%2520source%2520models.%250AComprehensive%2520experiments%2520on%252011%2520widely-used%2520benchmarks%2520demonstrate%2520that%2520InfiFPO%250Aconsistently%2520outperforms%2520existing%2520model%2520fusion%2520and%2520preference%2520optimization%250Amethods.%2520When%2520using%2520Phi-4%2520as%2520the%2520pivot%2520model%252C%2520InfiFPO%2520improve%2520its%2520average%250Aperformance%2520from%252079.95%2520to%252083.33%2520on%252011%2520benchmarks%252C%2520significantly%2520improving%2520its%250Acapabilities%2520in%2520mathematics%252C%2520coding%252C%2520and%2520reasoning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13878v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InfiFPO%3A%20Implicit%20Model%20Fusion%20via%20Preference%20Optimization%20in%20Large%0A%20%20Language%20Models&entry.906535625=Yanggan%20Gu%20and%20Yuanyi%20Wang%20and%20Zhaoyi%20Yan%20and%20Yiming%20Zhang%20and%20Qi%20Zhou%20and%20Fei%20Wu%20and%20Hongxia%20Yang&entry.1292438233=%20%20Model%20fusion%20combines%20multiple%20Large%20Language%20Models%20%28LLMs%29%20with%20different%0Astrengths%20into%20a%20more%20powerful%2C%20integrated%20model%20through%20lightweight%20training%0Amethods.%20Existing%20works%20on%20model%20fusion%20focus%20primarily%20on%20supervised%0Afine-tuning%20%28SFT%29%2C%20leaving%20preference%20alignment%20%28PA%29%20--a%20critical%20phase%20for%0Aenhancing%20LLM%20performance--largely%20unexplored.%20The%20current%20few%20fusion%20methods%0Aon%20PA%20phase%2C%20like%20WRPO%2C%20simplify%20the%20process%20by%20utilizing%20only%20response%20outputs%0Afrom%20source%20models%20while%20discarding%20their%20probability%20information.%20To%20address%0Athis%20limitation%2C%20we%20propose%20InfiFPO%2C%20a%20preference%20optimization%20method%20for%0Aimplicit%20model%20fusion.%20InfiFPO%20replaces%20the%20reference%20model%20in%20Direct%0APreference%20Optimization%20%28DPO%29%20with%20a%20fused%20source%20model%20that%20synthesizes%0Amulti-source%20probabilities%20at%20the%20sequence%20level%2C%20circumventing%20complex%0Avocabulary%20alignment%20challenges%20in%20previous%20works%20and%20meanwhile%20maintaining%20the%0Aprobability%20information.%20By%20introducing%20probability%20clipping%20and%20max-margin%0Afusion%20strategies%2C%20InfiFPO%20enables%20the%20pivot%20model%20to%20align%20with%20human%0Apreferences%20while%20effectively%20distilling%20knowledge%20from%20source%20models.%0AComprehensive%20experiments%20on%2011%20widely-used%20benchmarks%20demonstrate%20that%20InfiFPO%0Aconsistently%20outperforms%20existing%20model%20fusion%20and%20preference%20optimization%0Amethods.%20When%20using%20Phi-4%20as%20the%20pivot%20model%2C%20InfiFPO%20improve%20its%20average%0Aperformance%20from%2079.95%20to%2083.33%20on%2011%20benchmarks%2C%20significantly%20improving%20its%0Acapabilities%20in%20mathematics%2C%20coding%2C%20and%20reasoning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13878v2&entry.124074799=Read"},
{"title": "A recursive Bayesian neural network for constitutive modeling of sands\n  under monotonic and cyclic loading", "author": "Toiba Noor and Soban Nasir Lone and G. V. Ramana and Rajdip Nayek", "abstract": "  In geotechnical engineering, constitutive models are central to capturing\nsoil behavior across diverse drainage conditions, stress paths,and loading\nhistories. While data driven deep learning (DL) approaches have shown promise\nas alternatives to traditional constitutive formulations, their deployment\nrequires models that are both accurate and capable of quantifying predictive\nuncertainty. This study introduces a recursive Bayesian neural network (rBNN)\nframework that unifies temporal sequence learning with generalized Bayesian\ninference to achieve both predictive accuracy and rigorous uncertainty\nquantification. A key innovation is the incorporation of a sliding window\nrecursive structure that enables the model to effectively capture path\ndependent soil responses under monotonic and cyclic loading. By treating\nnetwork parameters as random variables and inferring their posterior\ndistributions via generalized variational inference, the rBNN produces well\ncalibrated confidence intervals alongside point predictions.The framework is\nvalidated against four datasets spanning both simulated and experimental\ntriaxial tests: monotonic loading using a Hardening Soil model simulation and\n28 CD tests on Baskarp sand, and cyclic loading using an exponential\nconstitutive simulation of CD CU tests and 37 experimental cyclic CU tests on\nOttawa F65 sand. This progression from monotonic to cyclic and from simulated\nto experimental data demonstrates the adaptability of the proposed approach\nacross varying levels of data fidelity and complexity. Comparative analyses\nwith LSTM, Encoder Decoder,and GRU architectures highlight that rBNN not only\nachieves competitive predictive accuracy but also provides reliable confidence\nintervals.\n", "link": "http://arxiv.org/abs/2501.10088v2", "date": "2025-10-22", "relevancy": 1.9567, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5584}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4831}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4676}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20recursive%20Bayesian%20neural%20network%20for%20constitutive%20modeling%20of%20sands%0A%20%20under%20monotonic%20and%20cyclic%20loading&body=Title%3A%20A%20recursive%20Bayesian%20neural%20network%20for%20constitutive%20modeling%20of%20sands%0A%20%20under%20monotonic%20and%20cyclic%20loading%0AAuthor%3A%20Toiba%20Noor%20and%20Soban%20Nasir%20Lone%20and%20G.%20V.%20Ramana%20and%20Rajdip%20Nayek%0AAbstract%3A%20%20%20In%20geotechnical%20engineering%2C%20constitutive%20models%20are%20central%20to%20capturing%0Asoil%20behavior%20across%20diverse%20drainage%20conditions%2C%20stress%20paths%2Cand%20loading%0Ahistories.%20While%20data%20driven%20deep%20learning%20%28DL%29%20approaches%20have%20shown%20promise%0Aas%20alternatives%20to%20traditional%20constitutive%20formulations%2C%20their%20deployment%0Arequires%20models%20that%20are%20both%20accurate%20and%20capable%20of%20quantifying%20predictive%0Auncertainty.%20This%20study%20introduces%20a%20recursive%20Bayesian%20neural%20network%20%28rBNN%29%0Aframework%20that%20unifies%20temporal%20sequence%20learning%20with%20generalized%20Bayesian%0Ainference%20to%20achieve%20both%20predictive%20accuracy%20and%20rigorous%20uncertainty%0Aquantification.%20A%20key%20innovation%20is%20the%20incorporation%20of%20a%20sliding%20window%0Arecursive%20structure%20that%20enables%20the%20model%20to%20effectively%20capture%20path%0Adependent%20soil%20responses%20under%20monotonic%20and%20cyclic%20loading.%20By%20treating%0Anetwork%20parameters%20as%20random%20variables%20and%20inferring%20their%20posterior%0Adistributions%20via%20generalized%20variational%20inference%2C%20the%20rBNN%20produces%20well%0Acalibrated%20confidence%20intervals%20alongside%20point%20predictions.The%20framework%20is%0Avalidated%20against%20four%20datasets%20spanning%20both%20simulated%20and%20experimental%0Atriaxial%20tests%3A%20monotonic%20loading%20using%20a%20Hardening%20Soil%20model%20simulation%20and%0A28%20CD%20tests%20on%20Baskarp%20sand%2C%20and%20cyclic%20loading%20using%20an%20exponential%0Aconstitutive%20simulation%20of%20CD%20CU%20tests%20and%2037%20experimental%20cyclic%20CU%20tests%20on%0AOttawa%20F65%20sand.%20This%20progression%20from%20monotonic%20to%20cyclic%20and%20from%20simulated%0Ato%20experimental%20data%20demonstrates%20the%20adaptability%20of%20the%20proposed%20approach%0Aacross%20varying%20levels%20of%20data%20fidelity%20and%20complexity.%20Comparative%20analyses%0Awith%20LSTM%2C%20Encoder%20Decoder%2Cand%20GRU%20architectures%20highlight%20that%20rBNN%20not%20only%0Aachieves%20competitive%20predictive%20accuracy%20but%20also%20provides%20reliable%20confidence%0Aintervals.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10088v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520recursive%2520Bayesian%2520neural%2520network%2520for%2520constitutive%2520modeling%2520of%2520sands%250A%2520%2520under%2520monotonic%2520and%2520cyclic%2520loading%26entry.906535625%3DToiba%2520Noor%2520and%2520Soban%2520Nasir%2520Lone%2520and%2520G.%2520V.%2520Ramana%2520and%2520Rajdip%2520Nayek%26entry.1292438233%3D%2520%2520In%2520geotechnical%2520engineering%252C%2520constitutive%2520models%2520are%2520central%2520to%2520capturing%250Asoil%2520behavior%2520across%2520diverse%2520drainage%2520conditions%252C%2520stress%2520paths%252Cand%2520loading%250Ahistories.%2520While%2520data%2520driven%2520deep%2520learning%2520%2528DL%2529%2520approaches%2520have%2520shown%2520promise%250Aas%2520alternatives%2520to%2520traditional%2520constitutive%2520formulations%252C%2520their%2520deployment%250Arequires%2520models%2520that%2520are%2520both%2520accurate%2520and%2520capable%2520of%2520quantifying%2520predictive%250Auncertainty.%2520This%2520study%2520introduces%2520a%2520recursive%2520Bayesian%2520neural%2520network%2520%2528rBNN%2529%250Aframework%2520that%2520unifies%2520temporal%2520sequence%2520learning%2520with%2520generalized%2520Bayesian%250Ainference%2520to%2520achieve%2520both%2520predictive%2520accuracy%2520and%2520rigorous%2520uncertainty%250Aquantification.%2520A%2520key%2520innovation%2520is%2520the%2520incorporation%2520of%2520a%2520sliding%2520window%250Arecursive%2520structure%2520that%2520enables%2520the%2520model%2520to%2520effectively%2520capture%2520path%250Adependent%2520soil%2520responses%2520under%2520monotonic%2520and%2520cyclic%2520loading.%2520By%2520treating%250Anetwork%2520parameters%2520as%2520random%2520variables%2520and%2520inferring%2520their%2520posterior%250Adistributions%2520via%2520generalized%2520variational%2520inference%252C%2520the%2520rBNN%2520produces%2520well%250Acalibrated%2520confidence%2520intervals%2520alongside%2520point%2520predictions.The%2520framework%2520is%250Avalidated%2520against%2520four%2520datasets%2520spanning%2520both%2520simulated%2520and%2520experimental%250Atriaxial%2520tests%253A%2520monotonic%2520loading%2520using%2520a%2520Hardening%2520Soil%2520model%2520simulation%2520and%250A28%2520CD%2520tests%2520on%2520Baskarp%2520sand%252C%2520and%2520cyclic%2520loading%2520using%2520an%2520exponential%250Aconstitutive%2520simulation%2520of%2520CD%2520CU%2520tests%2520and%252037%2520experimental%2520cyclic%2520CU%2520tests%2520on%250AOttawa%2520F65%2520sand.%2520This%2520progression%2520from%2520monotonic%2520to%2520cyclic%2520and%2520from%2520simulated%250Ato%2520experimental%2520data%2520demonstrates%2520the%2520adaptability%2520of%2520the%2520proposed%2520approach%250Aacross%2520varying%2520levels%2520of%2520data%2520fidelity%2520and%2520complexity.%2520Comparative%2520analyses%250Awith%2520LSTM%252C%2520Encoder%2520Decoder%252Cand%2520GRU%2520architectures%2520highlight%2520that%2520rBNN%2520not%2520only%250Aachieves%2520competitive%2520predictive%2520accuracy%2520but%2520also%2520provides%2520reliable%2520confidence%250Aintervals.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10088v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20recursive%20Bayesian%20neural%20network%20for%20constitutive%20modeling%20of%20sands%0A%20%20under%20monotonic%20and%20cyclic%20loading&entry.906535625=Toiba%20Noor%20and%20Soban%20Nasir%20Lone%20and%20G.%20V.%20Ramana%20and%20Rajdip%20Nayek&entry.1292438233=%20%20In%20geotechnical%20engineering%2C%20constitutive%20models%20are%20central%20to%20capturing%0Asoil%20behavior%20across%20diverse%20drainage%20conditions%2C%20stress%20paths%2Cand%20loading%0Ahistories.%20While%20data%20driven%20deep%20learning%20%28DL%29%20approaches%20have%20shown%20promise%0Aas%20alternatives%20to%20traditional%20constitutive%20formulations%2C%20their%20deployment%0Arequires%20models%20that%20are%20both%20accurate%20and%20capable%20of%20quantifying%20predictive%0Auncertainty.%20This%20study%20introduces%20a%20recursive%20Bayesian%20neural%20network%20%28rBNN%29%0Aframework%20that%20unifies%20temporal%20sequence%20learning%20with%20generalized%20Bayesian%0Ainference%20to%20achieve%20both%20predictive%20accuracy%20and%20rigorous%20uncertainty%0Aquantification.%20A%20key%20innovation%20is%20the%20incorporation%20of%20a%20sliding%20window%0Arecursive%20structure%20that%20enables%20the%20model%20to%20effectively%20capture%20path%0Adependent%20soil%20responses%20under%20monotonic%20and%20cyclic%20loading.%20By%20treating%0Anetwork%20parameters%20as%20random%20variables%20and%20inferring%20their%20posterior%0Adistributions%20via%20generalized%20variational%20inference%2C%20the%20rBNN%20produces%20well%0Acalibrated%20confidence%20intervals%20alongside%20point%20predictions.The%20framework%20is%0Avalidated%20against%20four%20datasets%20spanning%20both%20simulated%20and%20experimental%0Atriaxial%20tests%3A%20monotonic%20loading%20using%20a%20Hardening%20Soil%20model%20simulation%20and%0A28%20CD%20tests%20on%20Baskarp%20sand%2C%20and%20cyclic%20loading%20using%20an%20exponential%0Aconstitutive%20simulation%20of%20CD%20CU%20tests%20and%2037%20experimental%20cyclic%20CU%20tests%20on%0AOttawa%20F65%20sand.%20This%20progression%20from%20monotonic%20to%20cyclic%20and%20from%20simulated%0Ato%20experimental%20data%20demonstrates%20the%20adaptability%20of%20the%20proposed%20approach%0Aacross%20varying%20levels%20of%20data%20fidelity%20and%20complexity.%20Comparative%20analyses%0Awith%20LSTM%2C%20Encoder%20Decoder%2Cand%20GRU%20architectures%20highlight%20that%20rBNN%20not%20only%0Aachieves%20competitive%20predictive%20accuracy%20but%20also%20provides%20reliable%20confidence%0Aintervals.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10088v2&entry.124074799=Read"},
{"title": "HSCodeComp: A Realistic and Expert-level Benchmark for Deep Search\n  Agents in Hierarchical Rule Application", "author": "Yiqian Yang and Tian Lan and Qianghuai Jia and Li Zhu and Hui Jiang and Hang Zhu and Longyue Wang and Weihua Luo and Kaifu Zhang", "abstract": "  Effective deep search agents must not only access open-domain and\ndomain-specific knowledge but also apply complex rules-such as legal clauses,\nmedical manuals and tariff rules. These rules often feature vague boundaries\nand implicit logic relationships, making precise application challenging for\nagents. However, this critical capability is largely overlooked by current\nagent benchmarks.\n  To fill this gap, we introduce HSCodeComp, the first realistic, expert-level\ne-commerce benchmark designed to evaluate deep search agents in hierarchical\nrule application. In this task, the deep reasoning process of agents is guided\nby these rules to predict 10-digit Harmonized System Code (HSCode) of products\nwith noisy but realistic descriptions. These codes, established by the World\nCustoms Organization, are vital for global supply chain efficiency. Built from\nreal-world data collected from large-scale e-commerce platforms, our proposed\nHSCodeComp comprises 632 product entries spanning diverse product categories,\nwith these HSCodes annotated by several human experts.\n  Extensive experimental results on several state-of-the-art LLMs, open-source,\nand closed-source agents reveal a huge performance gap: best agent achieves\nonly 46.8% 10-digit accuracy, far below human experts at 95.0%. Besides,\ndetailed analysis demonstrates the challenges of hierarchical rule application,\nand test-time scaling fails to improve performance further.\n", "link": "http://arxiv.org/abs/2510.19631v1", "date": "2025-10-22", "relevancy": 1.949, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5028}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4841}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4841}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HSCodeComp%3A%20A%20Realistic%20and%20Expert-level%20Benchmark%20for%20Deep%20Search%0A%20%20Agents%20in%20Hierarchical%20Rule%20Application&body=Title%3A%20HSCodeComp%3A%20A%20Realistic%20and%20Expert-level%20Benchmark%20for%20Deep%20Search%0A%20%20Agents%20in%20Hierarchical%20Rule%20Application%0AAuthor%3A%20Yiqian%20Yang%20and%20Tian%20Lan%20and%20Qianghuai%20Jia%20and%20Li%20Zhu%20and%20Hui%20Jiang%20and%20Hang%20Zhu%20and%20Longyue%20Wang%20and%20Weihua%20Luo%20and%20Kaifu%20Zhang%0AAbstract%3A%20%20%20Effective%20deep%20search%20agents%20must%20not%20only%20access%20open-domain%20and%0Adomain-specific%20knowledge%20but%20also%20apply%20complex%20rules-such%20as%20legal%20clauses%2C%0Amedical%20manuals%20and%20tariff%20rules.%20These%20rules%20often%20feature%20vague%20boundaries%0Aand%20implicit%20logic%20relationships%2C%20making%20precise%20application%20challenging%20for%0Aagents.%20However%2C%20this%20critical%20capability%20is%20largely%20overlooked%20by%20current%0Aagent%20benchmarks.%0A%20%20To%20fill%20this%20gap%2C%20we%20introduce%20HSCodeComp%2C%20the%20first%20realistic%2C%20expert-level%0Ae-commerce%20benchmark%20designed%20to%20evaluate%20deep%20search%20agents%20in%20hierarchical%0Arule%20application.%20In%20this%20task%2C%20the%20deep%20reasoning%20process%20of%20agents%20is%20guided%0Aby%20these%20rules%20to%20predict%2010-digit%20Harmonized%20System%20Code%20%28HSCode%29%20of%20products%0Awith%20noisy%20but%20realistic%20descriptions.%20These%20codes%2C%20established%20by%20the%20World%0ACustoms%20Organization%2C%20are%20vital%20for%20global%20supply%20chain%20efficiency.%20Built%20from%0Areal-world%20data%20collected%20from%20large-scale%20e-commerce%20platforms%2C%20our%20proposed%0AHSCodeComp%20comprises%20632%20product%20entries%20spanning%20diverse%20product%20categories%2C%0Awith%20these%20HSCodes%20annotated%20by%20several%20human%20experts.%0A%20%20Extensive%20experimental%20results%20on%20several%20state-of-the-art%20LLMs%2C%20open-source%2C%0Aand%20closed-source%20agents%20reveal%20a%20huge%20performance%20gap%3A%20best%20agent%20achieves%0Aonly%2046.8%25%2010-digit%20accuracy%2C%20far%20below%20human%20experts%20at%2095.0%25.%20Besides%2C%0Adetailed%20analysis%20demonstrates%20the%20challenges%20of%20hierarchical%20rule%20application%2C%0Aand%20test-time%20scaling%20fails%20to%20improve%20performance%20further.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19631v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHSCodeComp%253A%2520A%2520Realistic%2520and%2520Expert-level%2520Benchmark%2520for%2520Deep%2520Search%250A%2520%2520Agents%2520in%2520Hierarchical%2520Rule%2520Application%26entry.906535625%3DYiqian%2520Yang%2520and%2520Tian%2520Lan%2520and%2520Qianghuai%2520Jia%2520and%2520Li%2520Zhu%2520and%2520Hui%2520Jiang%2520and%2520Hang%2520Zhu%2520and%2520Longyue%2520Wang%2520and%2520Weihua%2520Luo%2520and%2520Kaifu%2520Zhang%26entry.1292438233%3D%2520%2520Effective%2520deep%2520search%2520agents%2520must%2520not%2520only%2520access%2520open-domain%2520and%250Adomain-specific%2520knowledge%2520but%2520also%2520apply%2520complex%2520rules-such%2520as%2520legal%2520clauses%252C%250Amedical%2520manuals%2520and%2520tariff%2520rules.%2520These%2520rules%2520often%2520feature%2520vague%2520boundaries%250Aand%2520implicit%2520logic%2520relationships%252C%2520making%2520precise%2520application%2520challenging%2520for%250Aagents.%2520However%252C%2520this%2520critical%2520capability%2520is%2520largely%2520overlooked%2520by%2520current%250Aagent%2520benchmarks.%250A%2520%2520To%2520fill%2520this%2520gap%252C%2520we%2520introduce%2520HSCodeComp%252C%2520the%2520first%2520realistic%252C%2520expert-level%250Ae-commerce%2520benchmark%2520designed%2520to%2520evaluate%2520deep%2520search%2520agents%2520in%2520hierarchical%250Arule%2520application.%2520In%2520this%2520task%252C%2520the%2520deep%2520reasoning%2520process%2520of%2520agents%2520is%2520guided%250Aby%2520these%2520rules%2520to%2520predict%252010-digit%2520Harmonized%2520System%2520Code%2520%2528HSCode%2529%2520of%2520products%250Awith%2520noisy%2520but%2520realistic%2520descriptions.%2520These%2520codes%252C%2520established%2520by%2520the%2520World%250ACustoms%2520Organization%252C%2520are%2520vital%2520for%2520global%2520supply%2520chain%2520efficiency.%2520Built%2520from%250Areal-world%2520data%2520collected%2520from%2520large-scale%2520e-commerce%2520platforms%252C%2520our%2520proposed%250AHSCodeComp%2520comprises%2520632%2520product%2520entries%2520spanning%2520diverse%2520product%2520categories%252C%250Awith%2520these%2520HSCodes%2520annotated%2520by%2520several%2520human%2520experts.%250A%2520%2520Extensive%2520experimental%2520results%2520on%2520several%2520state-of-the-art%2520LLMs%252C%2520open-source%252C%250Aand%2520closed-source%2520agents%2520reveal%2520a%2520huge%2520performance%2520gap%253A%2520best%2520agent%2520achieves%250Aonly%252046.8%2525%252010-digit%2520accuracy%252C%2520far%2520below%2520human%2520experts%2520at%252095.0%2525.%2520Besides%252C%250Adetailed%2520analysis%2520demonstrates%2520the%2520challenges%2520of%2520hierarchical%2520rule%2520application%252C%250Aand%2520test-time%2520scaling%2520fails%2520to%2520improve%2520performance%2520further.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19631v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HSCodeComp%3A%20A%20Realistic%20and%20Expert-level%20Benchmark%20for%20Deep%20Search%0A%20%20Agents%20in%20Hierarchical%20Rule%20Application&entry.906535625=Yiqian%20Yang%20and%20Tian%20Lan%20and%20Qianghuai%20Jia%20and%20Li%20Zhu%20and%20Hui%20Jiang%20and%20Hang%20Zhu%20and%20Longyue%20Wang%20and%20Weihua%20Luo%20and%20Kaifu%20Zhang&entry.1292438233=%20%20Effective%20deep%20search%20agents%20must%20not%20only%20access%20open-domain%20and%0Adomain-specific%20knowledge%20but%20also%20apply%20complex%20rules-such%20as%20legal%20clauses%2C%0Amedical%20manuals%20and%20tariff%20rules.%20These%20rules%20often%20feature%20vague%20boundaries%0Aand%20implicit%20logic%20relationships%2C%20making%20precise%20application%20challenging%20for%0Aagents.%20However%2C%20this%20critical%20capability%20is%20largely%20overlooked%20by%20current%0Aagent%20benchmarks.%0A%20%20To%20fill%20this%20gap%2C%20we%20introduce%20HSCodeComp%2C%20the%20first%20realistic%2C%20expert-level%0Ae-commerce%20benchmark%20designed%20to%20evaluate%20deep%20search%20agents%20in%20hierarchical%0Arule%20application.%20In%20this%20task%2C%20the%20deep%20reasoning%20process%20of%20agents%20is%20guided%0Aby%20these%20rules%20to%20predict%2010-digit%20Harmonized%20System%20Code%20%28HSCode%29%20of%20products%0Awith%20noisy%20but%20realistic%20descriptions.%20These%20codes%2C%20established%20by%20the%20World%0ACustoms%20Organization%2C%20are%20vital%20for%20global%20supply%20chain%20efficiency.%20Built%20from%0Areal-world%20data%20collected%20from%20large-scale%20e-commerce%20platforms%2C%20our%20proposed%0AHSCodeComp%20comprises%20632%20product%20entries%20spanning%20diverse%20product%20categories%2C%0Awith%20these%20HSCodes%20annotated%20by%20several%20human%20experts.%0A%20%20Extensive%20experimental%20results%20on%20several%20state-of-the-art%20LLMs%2C%20open-source%2C%0Aand%20closed-source%20agents%20reveal%20a%20huge%20performance%20gap%3A%20best%20agent%20achieves%0Aonly%2046.8%25%2010-digit%20accuracy%2C%20far%20below%20human%20experts%20at%2095.0%25.%20Besides%2C%0Adetailed%20analysis%20demonstrates%20the%20challenges%20of%20hierarchical%20rule%20application%2C%0Aand%20test-time%20scaling%20fails%20to%20improve%20performance%20further.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19631v1&entry.124074799=Read"},
{"title": "AdaptGrad: Adaptive Sampling to Reduce Noise", "author": "Linjiang Zhou and Chao Ma and Zepeng Wang and Libing Wu and Xiaochuan Shi", "abstract": "  Gradient Smoothing is an efficient approach to reducing noise in\ngradient-based model explanation method. SmoothGrad adds Gaussian noise to\nmitigate much of these noise. However, the crucial hyper-parameter in this\nmethod, the variance $\\sigma$ of Gaussian noise, is set manually or with\nheuristic approach. However, it results in the smoothed gradients still\ncontaining a certain amount of noise. In this paper, we aim to interpret\nSmoothGrad as a corollary of convolution, thereby re-understanding the gradient\nnoise and the role of $\\sigma$ from the perspective of confidence level.\nFurthermore, we propose an adaptive gradient smoothing method, AdaptGrad, based\non these insights. Through comprehensive experiments, both qualitative and\nquantitative results demonstrate that AdaptGrad could effectively reduce almost\nall the noise in vanilla gradients compared with baselines methods. AdaptGrad\nis simple and universal, making it applicable for enhancing gradient-based\ninterpretability methods for better visualization.\n", "link": "http://arxiv.org/abs/2410.07711v2", "date": "2025-10-22", "relevancy": 1.9356, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4942}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4842}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4795}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaptGrad%3A%20Adaptive%20Sampling%20to%20Reduce%20Noise&body=Title%3A%20AdaptGrad%3A%20Adaptive%20Sampling%20to%20Reduce%20Noise%0AAuthor%3A%20Linjiang%20Zhou%20and%20Chao%20Ma%20and%20Zepeng%20Wang%20and%20Libing%20Wu%20and%20Xiaochuan%20Shi%0AAbstract%3A%20%20%20Gradient%20Smoothing%20is%20an%20efficient%20approach%20to%20reducing%20noise%20in%0Agradient-based%20model%20explanation%20method.%20SmoothGrad%20adds%20Gaussian%20noise%20to%0Amitigate%20much%20of%20these%20noise.%20However%2C%20the%20crucial%20hyper-parameter%20in%20this%0Amethod%2C%20the%20variance%20%24%5Csigma%24%20of%20Gaussian%20noise%2C%20is%20set%20manually%20or%20with%0Aheuristic%20approach.%20However%2C%20it%20results%20in%20the%20smoothed%20gradients%20still%0Acontaining%20a%20certain%20amount%20of%20noise.%20In%20this%20paper%2C%20we%20aim%20to%20interpret%0ASmoothGrad%20as%20a%20corollary%20of%20convolution%2C%20thereby%20re-understanding%20the%20gradient%0Anoise%20and%20the%20role%20of%20%24%5Csigma%24%20from%20the%20perspective%20of%20confidence%20level.%0AFurthermore%2C%20we%20propose%20an%20adaptive%20gradient%20smoothing%20method%2C%20AdaptGrad%2C%20based%0Aon%20these%20insights.%20Through%20comprehensive%20experiments%2C%20both%20qualitative%20and%0Aquantitative%20results%20demonstrate%20that%20AdaptGrad%20could%20effectively%20reduce%20almost%0Aall%20the%20noise%20in%20vanilla%20gradients%20compared%20with%20baselines%20methods.%20AdaptGrad%0Ais%20simple%20and%20universal%2C%20making%20it%20applicable%20for%20enhancing%20gradient-based%0Ainterpretability%20methods%20for%20better%20visualization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07711v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptGrad%253A%2520Adaptive%2520Sampling%2520to%2520Reduce%2520Noise%26entry.906535625%3DLinjiang%2520Zhou%2520and%2520Chao%2520Ma%2520and%2520Zepeng%2520Wang%2520and%2520Libing%2520Wu%2520and%2520Xiaochuan%2520Shi%26entry.1292438233%3D%2520%2520Gradient%2520Smoothing%2520is%2520an%2520efficient%2520approach%2520to%2520reducing%2520noise%2520in%250Agradient-based%2520model%2520explanation%2520method.%2520SmoothGrad%2520adds%2520Gaussian%2520noise%2520to%250Amitigate%2520much%2520of%2520these%2520noise.%2520However%252C%2520the%2520crucial%2520hyper-parameter%2520in%2520this%250Amethod%252C%2520the%2520variance%2520%2524%255Csigma%2524%2520of%2520Gaussian%2520noise%252C%2520is%2520set%2520manually%2520or%2520with%250Aheuristic%2520approach.%2520However%252C%2520it%2520results%2520in%2520the%2520smoothed%2520gradients%2520still%250Acontaining%2520a%2520certain%2520amount%2520of%2520noise.%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520interpret%250ASmoothGrad%2520as%2520a%2520corollary%2520of%2520convolution%252C%2520thereby%2520re-understanding%2520the%2520gradient%250Anoise%2520and%2520the%2520role%2520of%2520%2524%255Csigma%2524%2520from%2520the%2520perspective%2520of%2520confidence%2520level.%250AFurthermore%252C%2520we%2520propose%2520an%2520adaptive%2520gradient%2520smoothing%2520method%252C%2520AdaptGrad%252C%2520based%250Aon%2520these%2520insights.%2520Through%2520comprehensive%2520experiments%252C%2520both%2520qualitative%2520and%250Aquantitative%2520results%2520demonstrate%2520that%2520AdaptGrad%2520could%2520effectively%2520reduce%2520almost%250Aall%2520the%2520noise%2520in%2520vanilla%2520gradients%2520compared%2520with%2520baselines%2520methods.%2520AdaptGrad%250Ais%2520simple%2520and%2520universal%252C%2520making%2520it%2520applicable%2520for%2520enhancing%2520gradient-based%250Ainterpretability%2520methods%2520for%2520better%2520visualization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07711v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaptGrad%3A%20Adaptive%20Sampling%20to%20Reduce%20Noise&entry.906535625=Linjiang%20Zhou%20and%20Chao%20Ma%20and%20Zepeng%20Wang%20and%20Libing%20Wu%20and%20Xiaochuan%20Shi&entry.1292438233=%20%20Gradient%20Smoothing%20is%20an%20efficient%20approach%20to%20reducing%20noise%20in%0Agradient-based%20model%20explanation%20method.%20SmoothGrad%20adds%20Gaussian%20noise%20to%0Amitigate%20much%20of%20these%20noise.%20However%2C%20the%20crucial%20hyper-parameter%20in%20this%0Amethod%2C%20the%20variance%20%24%5Csigma%24%20of%20Gaussian%20noise%2C%20is%20set%20manually%20or%20with%0Aheuristic%20approach.%20However%2C%20it%20results%20in%20the%20smoothed%20gradients%20still%0Acontaining%20a%20certain%20amount%20of%20noise.%20In%20this%20paper%2C%20we%20aim%20to%20interpret%0ASmoothGrad%20as%20a%20corollary%20of%20convolution%2C%20thereby%20re-understanding%20the%20gradient%0Anoise%20and%20the%20role%20of%20%24%5Csigma%24%20from%20the%20perspective%20of%20confidence%20level.%0AFurthermore%2C%20we%20propose%20an%20adaptive%20gradient%20smoothing%20method%2C%20AdaptGrad%2C%20based%0Aon%20these%20insights.%20Through%20comprehensive%20experiments%2C%20both%20qualitative%20and%0Aquantitative%20results%20demonstrate%20that%20AdaptGrad%20could%20effectively%20reduce%20almost%0Aall%20the%20noise%20in%20vanilla%20gradients%20compared%20with%20baselines%20methods.%20AdaptGrad%0Ais%20simple%20and%20universal%2C%20making%20it%20applicable%20for%20enhancing%20gradient-based%0Ainterpretability%20methods%20for%20better%20visualization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07711v2&entry.124074799=Read"},
{"title": "The Open Syndrome Definition", "author": "Ana Paula Gomes Ferreira and Aleksandar An\u017eel and Izabel Oliva Marcilio de Souza and Helen Hughes and Alex J Elliot and Jude Dzevela Kong and Madlen Schranz and Alexander Ullrich and Georges Hattab", "abstract": "  Case definitions are essential for effectively communicating public health\nthreats. However, the absence of a standardized, machine-readable format poses\nsignificant challenges to interoperability, epidemiological research, the\nexchange of qualitative data, and the effective application of computational\nanalysis methods, including artificial intelligence (AI). This complicates\ncomparisons and collaborations across organizations and regions, limits data\nintegration, and hinders technological innovation in public health. To address\nthese issues, we propose the first open, machine-readable format for\nrepresenting case and syndrome definitions. Additionally, we introduce the\nfirst comprehensive dataset of standardized case definitions and tools to\nconvert existing human-readable definitions into machine-readable formats. We\nalso provide an accessible online platform for browsing, analyzing, and\ncontributing new definitions, available at https://opensyndrome.org. The Open\nSyndrome Definition format enables consistent, scalable use of case definitions\nacross systems, unlocking AI's potential to strengthen public health\npreparedness and response. The source code for the format can be found at\nhttps://github.com/OpenSyndrome/schema under the MIT license.\n", "link": "http://arxiv.org/abs/2509.25434v2", "date": "2025-10-22", "relevancy": 1.934, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.3936}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3834}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3834}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Open%20Syndrome%20Definition&body=Title%3A%20The%20Open%20Syndrome%20Definition%0AAuthor%3A%20Ana%20Paula%20Gomes%20Ferreira%20and%20Aleksandar%20An%C5%BEel%20and%20Izabel%20Oliva%20Marcilio%20de%20Souza%20and%20Helen%20Hughes%20and%20Alex%20J%20Elliot%20and%20Jude%20Dzevela%20Kong%20and%20Madlen%20Schranz%20and%20Alexander%20Ullrich%20and%20Georges%20Hattab%0AAbstract%3A%20%20%20Case%20definitions%20are%20essential%20for%20effectively%20communicating%20public%20health%0Athreats.%20However%2C%20the%20absence%20of%20a%20standardized%2C%20machine-readable%20format%20poses%0Asignificant%20challenges%20to%20interoperability%2C%20epidemiological%20research%2C%20the%0Aexchange%20of%20qualitative%20data%2C%20and%20the%20effective%20application%20of%20computational%0Aanalysis%20methods%2C%20including%20artificial%20intelligence%20%28AI%29.%20This%20complicates%0Acomparisons%20and%20collaborations%20across%20organizations%20and%20regions%2C%20limits%20data%0Aintegration%2C%20and%20hinders%20technological%20innovation%20in%20public%20health.%20To%20address%0Athese%20issues%2C%20we%20propose%20the%20first%20open%2C%20machine-readable%20format%20for%0Arepresenting%20case%20and%20syndrome%20definitions.%20Additionally%2C%20we%20introduce%20the%0Afirst%20comprehensive%20dataset%20of%20standardized%20case%20definitions%20and%20tools%20to%0Aconvert%20existing%20human-readable%20definitions%20into%20machine-readable%20formats.%20We%0Aalso%20provide%20an%20accessible%20online%20platform%20for%20browsing%2C%20analyzing%2C%20and%0Acontributing%20new%20definitions%2C%20available%20at%20https%3A//opensyndrome.org.%20The%20Open%0ASyndrome%20Definition%20format%20enables%20consistent%2C%20scalable%20use%20of%20case%20definitions%0Aacross%20systems%2C%20unlocking%20AI%27s%20potential%20to%20strengthen%20public%20health%0Apreparedness%20and%20response.%20The%20source%20code%20for%20the%20format%20can%20be%20found%20at%0Ahttps%3A//github.com/OpenSyndrome/schema%20under%20the%20MIT%20license.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25434v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Open%2520Syndrome%2520Definition%26entry.906535625%3DAna%2520Paula%2520Gomes%2520Ferreira%2520and%2520Aleksandar%2520An%25C5%25BEel%2520and%2520Izabel%2520Oliva%2520Marcilio%2520de%2520Souza%2520and%2520Helen%2520Hughes%2520and%2520Alex%2520J%2520Elliot%2520and%2520Jude%2520Dzevela%2520Kong%2520and%2520Madlen%2520Schranz%2520and%2520Alexander%2520Ullrich%2520and%2520Georges%2520Hattab%26entry.1292438233%3D%2520%2520Case%2520definitions%2520are%2520essential%2520for%2520effectively%2520communicating%2520public%2520health%250Athreats.%2520However%252C%2520the%2520absence%2520of%2520a%2520standardized%252C%2520machine-readable%2520format%2520poses%250Asignificant%2520challenges%2520to%2520interoperability%252C%2520epidemiological%2520research%252C%2520the%250Aexchange%2520of%2520qualitative%2520data%252C%2520and%2520the%2520effective%2520application%2520of%2520computational%250Aanalysis%2520methods%252C%2520including%2520artificial%2520intelligence%2520%2528AI%2529.%2520This%2520complicates%250Acomparisons%2520and%2520collaborations%2520across%2520organizations%2520and%2520regions%252C%2520limits%2520data%250Aintegration%252C%2520and%2520hinders%2520technological%2520innovation%2520in%2520public%2520health.%2520To%2520address%250Athese%2520issues%252C%2520we%2520propose%2520the%2520first%2520open%252C%2520machine-readable%2520format%2520for%250Arepresenting%2520case%2520and%2520syndrome%2520definitions.%2520Additionally%252C%2520we%2520introduce%2520the%250Afirst%2520comprehensive%2520dataset%2520of%2520standardized%2520case%2520definitions%2520and%2520tools%2520to%250Aconvert%2520existing%2520human-readable%2520definitions%2520into%2520machine-readable%2520formats.%2520We%250Aalso%2520provide%2520an%2520accessible%2520online%2520platform%2520for%2520browsing%252C%2520analyzing%252C%2520and%250Acontributing%2520new%2520definitions%252C%2520available%2520at%2520https%253A//opensyndrome.org.%2520The%2520Open%250ASyndrome%2520Definition%2520format%2520enables%2520consistent%252C%2520scalable%2520use%2520of%2520case%2520definitions%250Aacross%2520systems%252C%2520unlocking%2520AI%2527s%2520potential%2520to%2520strengthen%2520public%2520health%250Apreparedness%2520and%2520response.%2520The%2520source%2520code%2520for%2520the%2520format%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/OpenSyndrome/schema%2520under%2520the%2520MIT%2520license.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25434v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Open%20Syndrome%20Definition&entry.906535625=Ana%20Paula%20Gomes%20Ferreira%20and%20Aleksandar%20An%C5%BEel%20and%20Izabel%20Oliva%20Marcilio%20de%20Souza%20and%20Helen%20Hughes%20and%20Alex%20J%20Elliot%20and%20Jude%20Dzevela%20Kong%20and%20Madlen%20Schranz%20and%20Alexander%20Ullrich%20and%20Georges%20Hattab&entry.1292438233=%20%20Case%20definitions%20are%20essential%20for%20effectively%20communicating%20public%20health%0Athreats.%20However%2C%20the%20absence%20of%20a%20standardized%2C%20machine-readable%20format%20poses%0Asignificant%20challenges%20to%20interoperability%2C%20epidemiological%20research%2C%20the%0Aexchange%20of%20qualitative%20data%2C%20and%20the%20effective%20application%20of%20computational%0Aanalysis%20methods%2C%20including%20artificial%20intelligence%20%28AI%29.%20This%20complicates%0Acomparisons%20and%20collaborations%20across%20organizations%20and%20regions%2C%20limits%20data%0Aintegration%2C%20and%20hinders%20technological%20innovation%20in%20public%20health.%20To%20address%0Athese%20issues%2C%20we%20propose%20the%20first%20open%2C%20machine-readable%20format%20for%0Arepresenting%20case%20and%20syndrome%20definitions.%20Additionally%2C%20we%20introduce%20the%0Afirst%20comprehensive%20dataset%20of%20standardized%20case%20definitions%20and%20tools%20to%0Aconvert%20existing%20human-readable%20definitions%20into%20machine-readable%20formats.%20We%0Aalso%20provide%20an%20accessible%20online%20platform%20for%20browsing%2C%20analyzing%2C%20and%0Acontributing%20new%20definitions%2C%20available%20at%20https%3A//opensyndrome.org.%20The%20Open%0ASyndrome%20Definition%20format%20enables%20consistent%2C%20scalable%20use%20of%20case%20definitions%0Aacross%20systems%2C%20unlocking%20AI%27s%20potential%20to%20strengthen%20public%20health%0Apreparedness%20and%20response.%20The%20source%20code%20for%20the%20format%20can%20be%20found%20at%0Ahttps%3A//github.com/OpenSyndrome/schema%20under%20the%20MIT%20license.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25434v2&entry.124074799=Read"},
{"title": "Are Large Language Models Sensitive to the Motives Behind Communication?", "author": "Addison J. Wu and Ryan Liu and Kerem Oktar and Theodore R. Sumers and Thomas L. Griffiths", "abstract": "  Human communication is motivated: people speak, write, and create content\nwith a particular communicative intent in mind. As a result, information that\nlarge language models (LLMs) and AI agents process is inherently framed by\nhumans' intentions and incentives. People are adept at navigating such nuanced\ninformation: we routinely identify benevolent or self-serving motives in order\nto decide what statements to trust. For LLMs to be effective in the real world,\nthey too must critically evaluate content by factoring in the motivations of\nthe source -- for instance, weighing the credibility of claims made in a sales\npitch. In this paper, we undertake a comprehensive study of whether LLMs have\nthis capacity for motivational vigilance. We first employ controlled\nexperiments from cognitive science to verify that LLMs' behavior is consistent\nwith rational models of learning from motivated testimony, and find they\nsuccessfully discount information from biased sources in a human-like manner.\nWe then extend our evaluation to sponsored online adverts, a more naturalistic\nreflection of LLM agents' information ecosystems. In these settings, we find\nthat LLMs' inferences do not track the rational models' predictions nearly as\nclosely -- partly due to additional information that distracts them from\nvigilance-relevant considerations. However, a simple steering intervention that\nboosts the salience of intentions and incentives substantially increases the\ncorrespondence between LLMs and the rational model. These results suggest that\nLLMs possess a basic sensitivity to the motivations of others, but generalizing\nto novel real-world settings will require further improvements to these models.\n", "link": "http://arxiv.org/abs/2510.19687v1", "date": "2025-10-22", "relevancy": 1.9285, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4838}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4838}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Large%20Language%20Models%20Sensitive%20to%20the%20Motives%20Behind%20Communication%3F&body=Title%3A%20Are%20Large%20Language%20Models%20Sensitive%20to%20the%20Motives%20Behind%20Communication%3F%0AAuthor%3A%20Addison%20J.%20Wu%20and%20Ryan%20Liu%20and%20Kerem%20Oktar%20and%20Theodore%20R.%20Sumers%20and%20Thomas%20L.%20Griffiths%0AAbstract%3A%20%20%20Human%20communication%20is%20motivated%3A%20people%20speak%2C%20write%2C%20and%20create%20content%0Awith%20a%20particular%20communicative%20intent%20in%20mind.%20As%20a%20result%2C%20information%20that%0Alarge%20language%20models%20%28LLMs%29%20and%20AI%20agents%20process%20is%20inherently%20framed%20by%0Ahumans%27%20intentions%20and%20incentives.%20People%20are%20adept%20at%20navigating%20such%20nuanced%0Ainformation%3A%20we%20routinely%20identify%20benevolent%20or%20self-serving%20motives%20in%20order%0Ato%20decide%20what%20statements%20to%20trust.%20For%20LLMs%20to%20be%20effective%20in%20the%20real%20world%2C%0Athey%20too%20must%20critically%20evaluate%20content%20by%20factoring%20in%20the%20motivations%20of%0Athe%20source%20--%20for%20instance%2C%20weighing%20the%20credibility%20of%20claims%20made%20in%20a%20sales%0Apitch.%20In%20this%20paper%2C%20we%20undertake%20a%20comprehensive%20study%20of%20whether%20LLMs%20have%0Athis%20capacity%20for%20motivational%20vigilance.%20We%20first%20employ%20controlled%0Aexperiments%20from%20cognitive%20science%20to%20verify%20that%20LLMs%27%20behavior%20is%20consistent%0Awith%20rational%20models%20of%20learning%20from%20motivated%20testimony%2C%20and%20find%20they%0Asuccessfully%20discount%20information%20from%20biased%20sources%20in%20a%20human-like%20manner.%0AWe%20then%20extend%20our%20evaluation%20to%20sponsored%20online%20adverts%2C%20a%20more%20naturalistic%0Areflection%20of%20LLM%20agents%27%20information%20ecosystems.%20In%20these%20settings%2C%20we%20find%0Athat%20LLMs%27%20inferences%20do%20not%20track%20the%20rational%20models%27%20predictions%20nearly%20as%0Aclosely%20--%20partly%20due%20to%20additional%20information%20that%20distracts%20them%20from%0Avigilance-relevant%20considerations.%20However%2C%20a%20simple%20steering%20intervention%20that%0Aboosts%20the%20salience%20of%20intentions%20and%20incentives%20substantially%20increases%20the%0Acorrespondence%20between%20LLMs%20and%20the%20rational%20model.%20These%20results%20suggest%20that%0ALLMs%20possess%20a%20basic%20sensitivity%20to%20the%20motivations%20of%20others%2C%20but%20generalizing%0Ato%20novel%20real-world%20settings%20will%20require%20further%20improvements%20to%20these%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19687v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Large%2520Language%2520Models%2520Sensitive%2520to%2520the%2520Motives%2520Behind%2520Communication%253F%26entry.906535625%3DAddison%2520J.%2520Wu%2520and%2520Ryan%2520Liu%2520and%2520Kerem%2520Oktar%2520and%2520Theodore%2520R.%2520Sumers%2520and%2520Thomas%2520L.%2520Griffiths%26entry.1292438233%3D%2520%2520Human%2520communication%2520is%2520motivated%253A%2520people%2520speak%252C%2520write%252C%2520and%2520create%2520content%250Awith%2520a%2520particular%2520communicative%2520intent%2520in%2520mind.%2520As%2520a%2520result%252C%2520information%2520that%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520and%2520AI%2520agents%2520process%2520is%2520inherently%2520framed%2520by%250Ahumans%2527%2520intentions%2520and%2520incentives.%2520People%2520are%2520adept%2520at%2520navigating%2520such%2520nuanced%250Ainformation%253A%2520we%2520routinely%2520identify%2520benevolent%2520or%2520self-serving%2520motives%2520in%2520order%250Ato%2520decide%2520what%2520statements%2520to%2520trust.%2520For%2520LLMs%2520to%2520be%2520effective%2520in%2520the%2520real%2520world%252C%250Athey%2520too%2520must%2520critically%2520evaluate%2520content%2520by%2520factoring%2520in%2520the%2520motivations%2520of%250Athe%2520source%2520--%2520for%2520instance%252C%2520weighing%2520the%2520credibility%2520of%2520claims%2520made%2520in%2520a%2520sales%250Apitch.%2520In%2520this%2520paper%252C%2520we%2520undertake%2520a%2520comprehensive%2520study%2520of%2520whether%2520LLMs%2520have%250Athis%2520capacity%2520for%2520motivational%2520vigilance.%2520We%2520first%2520employ%2520controlled%250Aexperiments%2520from%2520cognitive%2520science%2520to%2520verify%2520that%2520LLMs%2527%2520behavior%2520is%2520consistent%250Awith%2520rational%2520models%2520of%2520learning%2520from%2520motivated%2520testimony%252C%2520and%2520find%2520they%250Asuccessfully%2520discount%2520information%2520from%2520biased%2520sources%2520in%2520a%2520human-like%2520manner.%250AWe%2520then%2520extend%2520our%2520evaluation%2520to%2520sponsored%2520online%2520adverts%252C%2520a%2520more%2520naturalistic%250Areflection%2520of%2520LLM%2520agents%2527%2520information%2520ecosystems.%2520In%2520these%2520settings%252C%2520we%2520find%250Athat%2520LLMs%2527%2520inferences%2520do%2520not%2520track%2520the%2520rational%2520models%2527%2520predictions%2520nearly%2520as%250Aclosely%2520--%2520partly%2520due%2520to%2520additional%2520information%2520that%2520distracts%2520them%2520from%250Avigilance-relevant%2520considerations.%2520However%252C%2520a%2520simple%2520steering%2520intervention%2520that%250Aboosts%2520the%2520salience%2520of%2520intentions%2520and%2520incentives%2520substantially%2520increases%2520the%250Acorrespondence%2520between%2520LLMs%2520and%2520the%2520rational%2520model.%2520These%2520results%2520suggest%2520that%250ALLMs%2520possess%2520a%2520basic%2520sensitivity%2520to%2520the%2520motivations%2520of%2520others%252C%2520but%2520generalizing%250Ato%2520novel%2520real-world%2520settings%2520will%2520require%2520further%2520improvements%2520to%2520these%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19687v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Large%20Language%20Models%20Sensitive%20to%20the%20Motives%20Behind%20Communication%3F&entry.906535625=Addison%20J.%20Wu%20and%20Ryan%20Liu%20and%20Kerem%20Oktar%20and%20Theodore%20R.%20Sumers%20and%20Thomas%20L.%20Griffiths&entry.1292438233=%20%20Human%20communication%20is%20motivated%3A%20people%20speak%2C%20write%2C%20and%20create%20content%0Awith%20a%20particular%20communicative%20intent%20in%20mind.%20As%20a%20result%2C%20information%20that%0Alarge%20language%20models%20%28LLMs%29%20and%20AI%20agents%20process%20is%20inherently%20framed%20by%0Ahumans%27%20intentions%20and%20incentives.%20People%20are%20adept%20at%20navigating%20such%20nuanced%0Ainformation%3A%20we%20routinely%20identify%20benevolent%20or%20self-serving%20motives%20in%20order%0Ato%20decide%20what%20statements%20to%20trust.%20For%20LLMs%20to%20be%20effective%20in%20the%20real%20world%2C%0Athey%20too%20must%20critically%20evaluate%20content%20by%20factoring%20in%20the%20motivations%20of%0Athe%20source%20--%20for%20instance%2C%20weighing%20the%20credibility%20of%20claims%20made%20in%20a%20sales%0Apitch.%20In%20this%20paper%2C%20we%20undertake%20a%20comprehensive%20study%20of%20whether%20LLMs%20have%0Athis%20capacity%20for%20motivational%20vigilance.%20We%20first%20employ%20controlled%0Aexperiments%20from%20cognitive%20science%20to%20verify%20that%20LLMs%27%20behavior%20is%20consistent%0Awith%20rational%20models%20of%20learning%20from%20motivated%20testimony%2C%20and%20find%20they%0Asuccessfully%20discount%20information%20from%20biased%20sources%20in%20a%20human-like%20manner.%0AWe%20then%20extend%20our%20evaluation%20to%20sponsored%20online%20adverts%2C%20a%20more%20naturalistic%0Areflection%20of%20LLM%20agents%27%20information%20ecosystems.%20In%20these%20settings%2C%20we%20find%0Athat%20LLMs%27%20inferences%20do%20not%20track%20the%20rational%20models%27%20predictions%20nearly%20as%0Aclosely%20--%20partly%20due%20to%20additional%20information%20that%20distracts%20them%20from%0Avigilance-relevant%20considerations.%20However%2C%20a%20simple%20steering%20intervention%20that%0Aboosts%20the%20salience%20of%20intentions%20and%20incentives%20substantially%20increases%20the%0Acorrespondence%20between%20LLMs%20and%20the%20rational%20model.%20These%20results%20suggest%20that%0ALLMs%20possess%20a%20basic%20sensitivity%20to%20the%20motivations%20of%20others%2C%20but%20generalizing%0Ato%20novel%20real-world%20settings%20will%20require%20further%20improvements%20to%20these%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19687v1&entry.124074799=Read"},
{"title": "LoRA vs Full Fine-tuning: An Illusion of Equivalence", "author": "Reece Shuttleworth and Jacob Andreas and Antonio Torralba and Pratyusha Sharma", "abstract": "  Fine-tuning is a crucial paradigm for adapting pre-trained large language\nmodels to downstream tasks. Recently, methods like Low-Rank Adaptation (LoRA)\nhave been shown to effectively fine-tune LLMs with an extreme reduction in\ntrainable parameters. But, \\emph{are their learned solutions really\nequivalent?} We study how LoRA and full-finetuning change pre-trained models by\nanalyzing the model's weight matrices through the lens of their spectral\nproperties. We find that LoRA and full fine-tuning yield weight matrices whose\nsingular value decompositions exhibit very different structure: weight matrices\ntrained with LoRA have new, high-ranking singular vectors, which we call\n\\emph{intruder dimensions}, while those trained with full fine-tuning do not.\nFurther, we extend the finding that LoRA forgets less than full fine-tuning and\nfind its forgetting is vastly localized to the intruder dimension -- by\ncausally intervening on the intruder dimensions by changing their associated\nsingular values post-fine-tuning, we show that they cause forgetting. Moreover,\nscaling them down significantly improves modeling of the pre-training\ndistribution with a minimal drop in downstream task performance. Given this, we\nshould expect accumulating intruder dimensions to be harmful and lead to more\nforgetting. This will be amplified during continual learning because of\nsequentially fine-tuning, and we show that LoRA models do accumulate intruder\ndimensions here tend to perform worse in this setting, emphasizing the\npracticality of our findings.\n", "link": "http://arxiv.org/abs/2410.21228v3", "date": "2025-10-22", "relevancy": 1.9145, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4829}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4778}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoRA%20vs%20Full%20Fine-tuning%3A%20An%20Illusion%20of%20Equivalence&body=Title%3A%20LoRA%20vs%20Full%20Fine-tuning%3A%20An%20Illusion%20of%20Equivalence%0AAuthor%3A%20Reece%20Shuttleworth%20and%20Jacob%20Andreas%20and%20Antonio%20Torralba%20and%20Pratyusha%20Sharma%0AAbstract%3A%20%20%20Fine-tuning%20is%20a%20crucial%20paradigm%20for%20adapting%20pre-trained%20large%20language%0Amodels%20to%20downstream%20tasks.%20Recently%2C%20methods%20like%20Low-Rank%20Adaptation%20%28LoRA%29%0Ahave%20been%20shown%20to%20effectively%20fine-tune%20LLMs%20with%20an%20extreme%20reduction%20in%0Atrainable%20parameters.%20But%2C%20%5Cemph%7Bare%20their%20learned%20solutions%20really%0Aequivalent%3F%7D%20We%20study%20how%20LoRA%20and%20full-finetuning%20change%20pre-trained%20models%20by%0Aanalyzing%20the%20model%27s%20weight%20matrices%20through%20the%20lens%20of%20their%20spectral%0Aproperties.%20We%20find%20that%20LoRA%20and%20full%20fine-tuning%20yield%20weight%20matrices%20whose%0Asingular%20value%20decompositions%20exhibit%20very%20different%20structure%3A%20weight%20matrices%0Atrained%20with%20LoRA%20have%20new%2C%20high-ranking%20singular%20vectors%2C%20which%20we%20call%0A%5Cemph%7Bintruder%20dimensions%7D%2C%20while%20those%20trained%20with%20full%20fine-tuning%20do%20not.%0AFurther%2C%20we%20extend%20the%20finding%20that%20LoRA%20forgets%20less%20than%20full%20fine-tuning%20and%0Afind%20its%20forgetting%20is%20vastly%20localized%20to%20the%20intruder%20dimension%20--%20by%0Acausally%20intervening%20on%20the%20intruder%20dimensions%20by%20changing%20their%20associated%0Asingular%20values%20post-fine-tuning%2C%20we%20show%20that%20they%20cause%20forgetting.%20Moreover%2C%0Ascaling%20them%20down%20significantly%20improves%20modeling%20of%20the%20pre-training%0Adistribution%20with%20a%20minimal%20drop%20in%20downstream%20task%20performance.%20Given%20this%2C%20we%0Ashould%20expect%20accumulating%20intruder%20dimensions%20to%20be%20harmful%20and%20lead%20to%20more%0Aforgetting.%20This%20will%20be%20amplified%20during%20continual%20learning%20because%20of%0Asequentially%20fine-tuning%2C%20and%20we%20show%20that%20LoRA%20models%20do%20accumulate%20intruder%0Adimensions%20here%20tend%20to%20perform%20worse%20in%20this%20setting%2C%20emphasizing%20the%0Apracticality%20of%20our%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21228v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoRA%2520vs%2520Full%2520Fine-tuning%253A%2520An%2520Illusion%2520of%2520Equivalence%26entry.906535625%3DReece%2520Shuttleworth%2520and%2520Jacob%2520Andreas%2520and%2520Antonio%2520Torralba%2520and%2520Pratyusha%2520Sharma%26entry.1292438233%3D%2520%2520Fine-tuning%2520is%2520a%2520crucial%2520paradigm%2520for%2520adapting%2520pre-trained%2520large%2520language%250Amodels%2520to%2520downstream%2520tasks.%2520Recently%252C%2520methods%2520like%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%250Ahave%2520been%2520shown%2520to%2520effectively%2520fine-tune%2520LLMs%2520with%2520an%2520extreme%2520reduction%2520in%250Atrainable%2520parameters.%2520But%252C%2520%255Cemph%257Bare%2520their%2520learned%2520solutions%2520really%250Aequivalent%253F%257D%2520We%2520study%2520how%2520LoRA%2520and%2520full-finetuning%2520change%2520pre-trained%2520models%2520by%250Aanalyzing%2520the%2520model%2527s%2520weight%2520matrices%2520through%2520the%2520lens%2520of%2520their%2520spectral%250Aproperties.%2520We%2520find%2520that%2520LoRA%2520and%2520full%2520fine-tuning%2520yield%2520weight%2520matrices%2520whose%250Asingular%2520value%2520decompositions%2520exhibit%2520very%2520different%2520structure%253A%2520weight%2520matrices%250Atrained%2520with%2520LoRA%2520have%2520new%252C%2520high-ranking%2520singular%2520vectors%252C%2520which%2520we%2520call%250A%255Cemph%257Bintruder%2520dimensions%257D%252C%2520while%2520those%2520trained%2520with%2520full%2520fine-tuning%2520do%2520not.%250AFurther%252C%2520we%2520extend%2520the%2520finding%2520that%2520LoRA%2520forgets%2520less%2520than%2520full%2520fine-tuning%2520and%250Afind%2520its%2520forgetting%2520is%2520vastly%2520localized%2520to%2520the%2520intruder%2520dimension%2520--%2520by%250Acausally%2520intervening%2520on%2520the%2520intruder%2520dimensions%2520by%2520changing%2520their%2520associated%250Asingular%2520values%2520post-fine-tuning%252C%2520we%2520show%2520that%2520they%2520cause%2520forgetting.%2520Moreover%252C%250Ascaling%2520them%2520down%2520significantly%2520improves%2520modeling%2520of%2520the%2520pre-training%250Adistribution%2520with%2520a%2520minimal%2520drop%2520in%2520downstream%2520task%2520performance.%2520Given%2520this%252C%2520we%250Ashould%2520expect%2520accumulating%2520intruder%2520dimensions%2520to%2520be%2520harmful%2520and%2520lead%2520to%2520more%250Aforgetting.%2520This%2520will%2520be%2520amplified%2520during%2520continual%2520learning%2520because%2520of%250Asequentially%2520fine-tuning%252C%2520and%2520we%2520show%2520that%2520LoRA%2520models%2520do%2520accumulate%2520intruder%250Adimensions%2520here%2520tend%2520to%2520perform%2520worse%2520in%2520this%2520setting%252C%2520emphasizing%2520the%250Apracticality%2520of%2520our%2520findings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21228v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoRA%20vs%20Full%20Fine-tuning%3A%20An%20Illusion%20of%20Equivalence&entry.906535625=Reece%20Shuttleworth%20and%20Jacob%20Andreas%20and%20Antonio%20Torralba%20and%20Pratyusha%20Sharma&entry.1292438233=%20%20Fine-tuning%20is%20a%20crucial%20paradigm%20for%20adapting%20pre-trained%20large%20language%0Amodels%20to%20downstream%20tasks.%20Recently%2C%20methods%20like%20Low-Rank%20Adaptation%20%28LoRA%29%0Ahave%20been%20shown%20to%20effectively%20fine-tune%20LLMs%20with%20an%20extreme%20reduction%20in%0Atrainable%20parameters.%20But%2C%20%5Cemph%7Bare%20their%20learned%20solutions%20really%0Aequivalent%3F%7D%20We%20study%20how%20LoRA%20and%20full-finetuning%20change%20pre-trained%20models%20by%0Aanalyzing%20the%20model%27s%20weight%20matrices%20through%20the%20lens%20of%20their%20spectral%0Aproperties.%20We%20find%20that%20LoRA%20and%20full%20fine-tuning%20yield%20weight%20matrices%20whose%0Asingular%20value%20decompositions%20exhibit%20very%20different%20structure%3A%20weight%20matrices%0Atrained%20with%20LoRA%20have%20new%2C%20high-ranking%20singular%20vectors%2C%20which%20we%20call%0A%5Cemph%7Bintruder%20dimensions%7D%2C%20while%20those%20trained%20with%20full%20fine-tuning%20do%20not.%0AFurther%2C%20we%20extend%20the%20finding%20that%20LoRA%20forgets%20less%20than%20full%20fine-tuning%20and%0Afind%20its%20forgetting%20is%20vastly%20localized%20to%20the%20intruder%20dimension%20--%20by%0Acausally%20intervening%20on%20the%20intruder%20dimensions%20by%20changing%20their%20associated%0Asingular%20values%20post-fine-tuning%2C%20we%20show%20that%20they%20cause%20forgetting.%20Moreover%2C%0Ascaling%20them%20down%20significantly%20improves%20modeling%20of%20the%20pre-training%0Adistribution%20with%20a%20minimal%20drop%20in%20downstream%20task%20performance.%20Given%20this%2C%20we%0Ashould%20expect%20accumulating%20intruder%20dimensions%20to%20be%20harmful%20and%20lead%20to%20more%0Aforgetting.%20This%20will%20be%20amplified%20during%20continual%20learning%20because%20of%0Asequentially%20fine-tuning%2C%20and%20we%20show%20that%20LoRA%20models%20do%20accumulate%20intruder%0Adimensions%20here%20tend%20to%20perform%20worse%20in%20this%20setting%2C%20emphasizing%20the%0Apracticality%20of%20our%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21228v3&entry.124074799=Read"},
{"title": "Towards Context-Aware Domain Generalization: Understanding the Benefits\n  and Limits of Marginal Transfer Learning", "author": "Jens M\u00fcller and Lars K\u00fchmichel and Martin Rohbeck and Stefan T. Radev and Ullrich K\u00f6the", "abstract": "  In this work, we analyze the conditions under which information about the\ncontext of an input $X$ can improve the predictions of deep learning models in\nnew domains. Following work in marginal transfer learning in Domain\nGeneralization (DG), we formalize the notion of context as a\npermutation-invariant representation of a set of data points that originate\nfrom the same domain as the input itself. We offer a theoretical analysis of\nthe conditions under which this approach can, in principle, yield benefits, and\nformulate two necessary criteria that can be easily verified in practice.\nAdditionally, we contribute insights into the kind of distribution shifts for\nwhich the marginal transfer learning approach promises robustness. Empirical\nanalysis shows that our criteria are effective in discerning both favorable and\nunfavorable scenarios. Finally, we demonstrate that we can reliably detect\nscenarios where a model is tasked with unwarranted extrapolation in\nout-of-distribution (OOD) domains, identifying potential failure cases.\nConsequently, we showcase a method to select between the most predictive and\nthe most robust model, circumventing the well-known trade-off between\npredictive performance and robustness.\n", "link": "http://arxiv.org/abs/2312.10107v3", "date": "2025-10-22", "relevancy": 1.9115, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.492}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4772}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Context-Aware%20Domain%20Generalization%3A%20Understanding%20the%20Benefits%0A%20%20and%20Limits%20of%20Marginal%20Transfer%20Learning&body=Title%3A%20Towards%20Context-Aware%20Domain%20Generalization%3A%20Understanding%20the%20Benefits%0A%20%20and%20Limits%20of%20Marginal%20Transfer%20Learning%0AAuthor%3A%20Jens%20M%C3%BCller%20and%20Lars%20K%C3%BChmichel%20and%20Martin%20Rohbeck%20and%20Stefan%20T.%20Radev%20and%20Ullrich%20K%C3%B6the%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20analyze%20the%20conditions%20under%20which%20information%20about%20the%0Acontext%20of%20an%20input%20%24X%24%20can%20improve%20the%20predictions%20of%20deep%20learning%20models%20in%0Anew%20domains.%20Following%20work%20in%20marginal%20transfer%20learning%20in%20Domain%0AGeneralization%20%28DG%29%2C%20we%20formalize%20the%20notion%20of%20context%20as%20a%0Apermutation-invariant%20representation%20of%20a%20set%20of%20data%20points%20that%20originate%0Afrom%20the%20same%20domain%20as%20the%20input%20itself.%20We%20offer%20a%20theoretical%20analysis%20of%0Athe%20conditions%20under%20which%20this%20approach%20can%2C%20in%20principle%2C%20yield%20benefits%2C%20and%0Aformulate%20two%20necessary%20criteria%20that%20can%20be%20easily%20verified%20in%20practice.%0AAdditionally%2C%20we%20contribute%20insights%20into%20the%20kind%20of%20distribution%20shifts%20for%0Awhich%20the%20marginal%20transfer%20learning%20approach%20promises%20robustness.%20Empirical%0Aanalysis%20shows%20that%20our%20criteria%20are%20effective%20in%20discerning%20both%20favorable%20and%0Aunfavorable%20scenarios.%20Finally%2C%20we%20demonstrate%20that%20we%20can%20reliably%20detect%0Ascenarios%20where%20a%20model%20is%20tasked%20with%20unwarranted%20extrapolation%20in%0Aout-of-distribution%20%28OOD%29%20domains%2C%20identifying%20potential%20failure%20cases.%0AConsequently%2C%20we%20showcase%20a%20method%20to%20select%20between%20the%20most%20predictive%20and%0Athe%20most%20robust%20model%2C%20circumventing%20the%20well-known%20trade-off%20between%0Apredictive%20performance%20and%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.10107v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Context-Aware%2520Domain%2520Generalization%253A%2520Understanding%2520the%2520Benefits%250A%2520%2520and%2520Limits%2520of%2520Marginal%2520Transfer%2520Learning%26entry.906535625%3DJens%2520M%25C3%25BCller%2520and%2520Lars%2520K%25C3%25BChmichel%2520and%2520Martin%2520Rohbeck%2520and%2520Stefan%2520T.%2520Radev%2520and%2520Ullrich%2520K%25C3%25B6the%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520analyze%2520the%2520conditions%2520under%2520which%2520information%2520about%2520the%250Acontext%2520of%2520an%2520input%2520%2524X%2524%2520can%2520improve%2520the%2520predictions%2520of%2520deep%2520learning%2520models%2520in%250Anew%2520domains.%2520Following%2520work%2520in%2520marginal%2520transfer%2520learning%2520in%2520Domain%250AGeneralization%2520%2528DG%2529%252C%2520we%2520formalize%2520the%2520notion%2520of%2520context%2520as%2520a%250Apermutation-invariant%2520representation%2520of%2520a%2520set%2520of%2520data%2520points%2520that%2520originate%250Afrom%2520the%2520same%2520domain%2520as%2520the%2520input%2520itself.%2520We%2520offer%2520a%2520theoretical%2520analysis%2520of%250Athe%2520conditions%2520under%2520which%2520this%2520approach%2520can%252C%2520in%2520principle%252C%2520yield%2520benefits%252C%2520and%250Aformulate%2520two%2520necessary%2520criteria%2520that%2520can%2520be%2520easily%2520verified%2520in%2520practice.%250AAdditionally%252C%2520we%2520contribute%2520insights%2520into%2520the%2520kind%2520of%2520distribution%2520shifts%2520for%250Awhich%2520the%2520marginal%2520transfer%2520learning%2520approach%2520promises%2520robustness.%2520Empirical%250Aanalysis%2520shows%2520that%2520our%2520criteria%2520are%2520effective%2520in%2520discerning%2520both%2520favorable%2520and%250Aunfavorable%2520scenarios.%2520Finally%252C%2520we%2520demonstrate%2520that%2520we%2520can%2520reliably%2520detect%250Ascenarios%2520where%2520a%2520model%2520is%2520tasked%2520with%2520unwarranted%2520extrapolation%2520in%250Aout-of-distribution%2520%2528OOD%2529%2520domains%252C%2520identifying%2520potential%2520failure%2520cases.%250AConsequently%252C%2520we%2520showcase%2520a%2520method%2520to%2520select%2520between%2520the%2520most%2520predictive%2520and%250Athe%2520most%2520robust%2520model%252C%2520circumventing%2520the%2520well-known%2520trade-off%2520between%250Apredictive%2520performance%2520and%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.10107v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Context-Aware%20Domain%20Generalization%3A%20Understanding%20the%20Benefits%0A%20%20and%20Limits%20of%20Marginal%20Transfer%20Learning&entry.906535625=Jens%20M%C3%BCller%20and%20Lars%20K%C3%BChmichel%20and%20Martin%20Rohbeck%20and%20Stefan%20T.%20Radev%20and%20Ullrich%20K%C3%B6the&entry.1292438233=%20%20In%20this%20work%2C%20we%20analyze%20the%20conditions%20under%20which%20information%20about%20the%0Acontext%20of%20an%20input%20%24X%24%20can%20improve%20the%20predictions%20of%20deep%20learning%20models%20in%0Anew%20domains.%20Following%20work%20in%20marginal%20transfer%20learning%20in%20Domain%0AGeneralization%20%28DG%29%2C%20we%20formalize%20the%20notion%20of%20context%20as%20a%0Apermutation-invariant%20representation%20of%20a%20set%20of%20data%20points%20that%20originate%0Afrom%20the%20same%20domain%20as%20the%20input%20itself.%20We%20offer%20a%20theoretical%20analysis%20of%0Athe%20conditions%20under%20which%20this%20approach%20can%2C%20in%20principle%2C%20yield%20benefits%2C%20and%0Aformulate%20two%20necessary%20criteria%20that%20can%20be%20easily%20verified%20in%20practice.%0AAdditionally%2C%20we%20contribute%20insights%20into%20the%20kind%20of%20distribution%20shifts%20for%0Awhich%20the%20marginal%20transfer%20learning%20approach%20promises%20robustness.%20Empirical%0Aanalysis%20shows%20that%20our%20criteria%20are%20effective%20in%20discerning%20both%20favorable%20and%0Aunfavorable%20scenarios.%20Finally%2C%20we%20demonstrate%20that%20we%20can%20reliably%20detect%0Ascenarios%20where%20a%20model%20is%20tasked%20with%20unwarranted%20extrapolation%20in%0Aout-of-distribution%20%28OOD%29%20domains%2C%20identifying%20potential%20failure%20cases.%0AConsequently%2C%20we%20showcase%20a%20method%20to%20select%20between%20the%20most%20predictive%20and%0Athe%20most%20robust%20model%2C%20circumventing%20the%20well-known%20trade-off%20between%0Apredictive%20performance%20and%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.10107v3&entry.124074799=Read"},
{"title": "Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1", "author": "Qianli Ma and Siyu Wang and Yilin Chen and Yinhao Tang and Yixiang Yang and Chang Guo and Bingjie Gao and Zhening Xing and Yanan Sun and Zhipeng Zhang", "abstract": "  In the quest for scientific progress, communicating research is as vital as\nthe discovery itself. Yet, researchers are often sidetracked by the manual,\nrepetitive chore of building project webpages to make their dense papers\naccessible. While automation has tackled static slides and posters, the\ndynamic, interactive nature of webpages has remained an unaddressed challenge.\nTo bridge this gap, we reframe the problem, arguing that the solution lies not\nin a single command, but in a collaborative, hierarchical process. We introduce\n$\\textbf{AutoPage}$, a novel multi-agent system that embodies this philosophy.\nAutoPage deconstructs paper-to-page creation into a coarse-to-fine pipeline\nfrom narrative planning to multimodal content generation and interactive\nrendering. To combat AI hallucination, dedicated \"Checker\" agents verify each\nstep against the source paper, while optional human checkpoints ensure the\nfinal product aligns perfectly with the author's vision, transforming the\nsystem from a mere tool into a powerful collaborative assistant. To rigorously\nvalidate our approach, we also construct $\\textbf{PageBench}$, the first\nbenchmark for this new task. Experiments show AutoPage not only generates\nhigh-quality, visually appealing pages but does so with remarkable efficiency\nin under 15 minutes for less than \\$0.1. Code and dataset will be released at\n$\\href{https://mqleet.github.io/AutoPage_ProjectPage/}{Webpage}$.\n", "link": "http://arxiv.org/abs/2510.19600v1", "date": "2025-10-22", "relevancy": 1.9107, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4916}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4854}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human-Agent%20Collaborative%20Paper-to-Page%20Crafting%20for%20Under%20%240.1&body=Title%3A%20Human-Agent%20Collaborative%20Paper-to-Page%20Crafting%20for%20Under%20%240.1%0AAuthor%3A%20Qianli%20Ma%20and%20Siyu%20Wang%20and%20Yilin%20Chen%20and%20Yinhao%20Tang%20and%20Yixiang%20Yang%20and%20Chang%20Guo%20and%20Bingjie%20Gao%20and%20Zhening%20Xing%20and%20Yanan%20Sun%20and%20Zhipeng%20Zhang%0AAbstract%3A%20%20%20In%20the%20quest%20for%20scientific%20progress%2C%20communicating%20research%20is%20as%20vital%20as%0Athe%20discovery%20itself.%20Yet%2C%20researchers%20are%20often%20sidetracked%20by%20the%20manual%2C%0Arepetitive%20chore%20of%20building%20project%20webpages%20to%20make%20their%20dense%20papers%0Aaccessible.%20While%20automation%20has%20tackled%20static%20slides%20and%20posters%2C%20the%0Adynamic%2C%20interactive%20nature%20of%20webpages%20has%20remained%20an%20unaddressed%20challenge.%0ATo%20bridge%20this%20gap%2C%20we%20reframe%20the%20problem%2C%20arguing%20that%20the%20solution%20lies%20not%0Ain%20a%20single%20command%2C%20but%20in%20a%20collaborative%2C%20hierarchical%20process.%20We%20introduce%0A%24%5Ctextbf%7BAutoPage%7D%24%2C%20a%20novel%20multi-agent%20system%20that%20embodies%20this%20philosophy.%0AAutoPage%20deconstructs%20paper-to-page%20creation%20into%20a%20coarse-to-fine%20pipeline%0Afrom%20narrative%20planning%20to%20multimodal%20content%20generation%20and%20interactive%0Arendering.%20To%20combat%20AI%20hallucination%2C%20dedicated%20%22Checker%22%20agents%20verify%20each%0Astep%20against%20the%20source%20paper%2C%20while%20optional%20human%20checkpoints%20ensure%20the%0Afinal%20product%20aligns%20perfectly%20with%20the%20author%27s%20vision%2C%20transforming%20the%0Asystem%20from%20a%20mere%20tool%20into%20a%20powerful%20collaborative%20assistant.%20To%20rigorously%0Avalidate%20our%20approach%2C%20we%20also%20construct%20%24%5Ctextbf%7BPageBench%7D%24%2C%20the%20first%0Abenchmark%20for%20this%20new%20task.%20Experiments%20show%20AutoPage%20not%20only%20generates%0Ahigh-quality%2C%20visually%20appealing%20pages%20but%20does%20so%20with%20remarkable%20efficiency%0Ain%20under%2015%20minutes%20for%20less%20than%20%5C%240.1.%20Code%20and%20dataset%20will%20be%20released%20at%0A%24%5Chref%7Bhttps%3A//mqleet.github.io/AutoPage_ProjectPage/%7D%7BWebpage%7D%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19600v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman-Agent%2520Collaborative%2520Paper-to-Page%2520Crafting%2520for%2520Under%2520%25240.1%26entry.906535625%3DQianli%2520Ma%2520and%2520Siyu%2520Wang%2520and%2520Yilin%2520Chen%2520and%2520Yinhao%2520Tang%2520and%2520Yixiang%2520Yang%2520and%2520Chang%2520Guo%2520and%2520Bingjie%2520Gao%2520and%2520Zhening%2520Xing%2520and%2520Yanan%2520Sun%2520and%2520Zhipeng%2520Zhang%26entry.1292438233%3D%2520%2520In%2520the%2520quest%2520for%2520scientific%2520progress%252C%2520communicating%2520research%2520is%2520as%2520vital%2520as%250Athe%2520discovery%2520itself.%2520Yet%252C%2520researchers%2520are%2520often%2520sidetracked%2520by%2520the%2520manual%252C%250Arepetitive%2520chore%2520of%2520building%2520project%2520webpages%2520to%2520make%2520their%2520dense%2520papers%250Aaccessible.%2520While%2520automation%2520has%2520tackled%2520static%2520slides%2520and%2520posters%252C%2520the%250Adynamic%252C%2520interactive%2520nature%2520of%2520webpages%2520has%2520remained%2520an%2520unaddressed%2520challenge.%250ATo%2520bridge%2520this%2520gap%252C%2520we%2520reframe%2520the%2520problem%252C%2520arguing%2520that%2520the%2520solution%2520lies%2520not%250Ain%2520a%2520single%2520command%252C%2520but%2520in%2520a%2520collaborative%252C%2520hierarchical%2520process.%2520We%2520introduce%250A%2524%255Ctextbf%257BAutoPage%257D%2524%252C%2520a%2520novel%2520multi-agent%2520system%2520that%2520embodies%2520this%2520philosophy.%250AAutoPage%2520deconstructs%2520paper-to-page%2520creation%2520into%2520a%2520coarse-to-fine%2520pipeline%250Afrom%2520narrative%2520planning%2520to%2520multimodal%2520content%2520generation%2520and%2520interactive%250Arendering.%2520To%2520combat%2520AI%2520hallucination%252C%2520dedicated%2520%2522Checker%2522%2520agents%2520verify%2520each%250Astep%2520against%2520the%2520source%2520paper%252C%2520while%2520optional%2520human%2520checkpoints%2520ensure%2520the%250Afinal%2520product%2520aligns%2520perfectly%2520with%2520the%2520author%2527s%2520vision%252C%2520transforming%2520the%250Asystem%2520from%2520a%2520mere%2520tool%2520into%2520a%2520powerful%2520collaborative%2520assistant.%2520To%2520rigorously%250Avalidate%2520our%2520approach%252C%2520we%2520also%2520construct%2520%2524%255Ctextbf%257BPageBench%257D%2524%252C%2520the%2520first%250Abenchmark%2520for%2520this%2520new%2520task.%2520Experiments%2520show%2520AutoPage%2520not%2520only%2520generates%250Ahigh-quality%252C%2520visually%2520appealing%2520pages%2520but%2520does%2520so%2520with%2520remarkable%2520efficiency%250Ain%2520under%252015%2520minutes%2520for%2520less%2520than%2520%255C%25240.1.%2520Code%2520and%2520dataset%2520will%2520be%2520released%2520at%250A%2524%255Chref%257Bhttps%253A//mqleet.github.io/AutoPage_ProjectPage/%257D%257BWebpage%257D%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19600v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human-Agent%20Collaborative%20Paper-to-Page%20Crafting%20for%20Under%20%240.1&entry.906535625=Qianli%20Ma%20and%20Siyu%20Wang%20and%20Yilin%20Chen%20and%20Yinhao%20Tang%20and%20Yixiang%20Yang%20and%20Chang%20Guo%20and%20Bingjie%20Gao%20and%20Zhening%20Xing%20and%20Yanan%20Sun%20and%20Zhipeng%20Zhang&entry.1292438233=%20%20In%20the%20quest%20for%20scientific%20progress%2C%20communicating%20research%20is%20as%20vital%20as%0Athe%20discovery%20itself.%20Yet%2C%20researchers%20are%20often%20sidetracked%20by%20the%20manual%2C%0Arepetitive%20chore%20of%20building%20project%20webpages%20to%20make%20their%20dense%20papers%0Aaccessible.%20While%20automation%20has%20tackled%20static%20slides%20and%20posters%2C%20the%0Adynamic%2C%20interactive%20nature%20of%20webpages%20has%20remained%20an%20unaddressed%20challenge.%0ATo%20bridge%20this%20gap%2C%20we%20reframe%20the%20problem%2C%20arguing%20that%20the%20solution%20lies%20not%0Ain%20a%20single%20command%2C%20but%20in%20a%20collaborative%2C%20hierarchical%20process.%20We%20introduce%0A%24%5Ctextbf%7BAutoPage%7D%24%2C%20a%20novel%20multi-agent%20system%20that%20embodies%20this%20philosophy.%0AAutoPage%20deconstructs%20paper-to-page%20creation%20into%20a%20coarse-to-fine%20pipeline%0Afrom%20narrative%20planning%20to%20multimodal%20content%20generation%20and%20interactive%0Arendering.%20To%20combat%20AI%20hallucination%2C%20dedicated%20%22Checker%22%20agents%20verify%20each%0Astep%20against%20the%20source%20paper%2C%20while%20optional%20human%20checkpoints%20ensure%20the%0Afinal%20product%20aligns%20perfectly%20with%20the%20author%27s%20vision%2C%20transforming%20the%0Asystem%20from%20a%20mere%20tool%20into%20a%20powerful%20collaborative%20assistant.%20To%20rigorously%0Avalidate%20our%20approach%2C%20we%20also%20construct%20%24%5Ctextbf%7BPageBench%7D%24%2C%20the%20first%0Abenchmark%20for%20this%20new%20task.%20Experiments%20show%20AutoPage%20not%20only%20generates%0Ahigh-quality%2C%20visually%20appealing%20pages%20but%20does%20so%20with%20remarkable%20efficiency%0Ain%20under%2015%20minutes%20for%20less%20than%20%5C%240.1.%20Code%20and%20dataset%20will%20be%20released%20at%0A%24%5Chref%7Bhttps%3A//mqleet.github.io/AutoPage_ProjectPage/%7D%7BWebpage%7D%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19600v1&entry.124074799=Read"},
{"title": "QiMeng-MuPa: Mutual-Supervised Learning for Sequential-to-Parallel Code\n  Translation", "author": "Changxin Ke and Rui Zhang and Shuo Wang and Li Ding and Guangli Li and Yuanbo Wen and Shuoming Zhang and Ruiyuan Xu and Jin Qin and Jiaming Guo and Chenxi Wang and Ling Li and Qi Guo and Yunji Chen", "abstract": "  The rise of GPU-based high-performance computing (HPC) has driven the\nwidespread adoption of parallel programming models such as CUDA. Yet, the\ninherent complexity of parallel programming creates a demand for the automated\nsequential-to-parallel approaches. However, data scarcity poses a significant\nchallenge for machine learning-based sequential-to-parallel code translation.\nAlthough recent back-translation methods show promise, they still fail to\nensure functional equivalence in the translated code. In this paper, we propose\n\\textbf{QiMeng-MuPa}, a novel \\textbf{Mu}tual-Supervised Learning framework for\nSequential-to-\\textbf{Pa}rallel code translation, to address the functional\nequivalence issue. QiMeng-MuPa consists of two models, a Translator and a\nTester. Through an iterative loop consisting of Co-verify and Co-evolve steps,\nthe Translator and the Tester mutually generate data for each other and improve\ncollectively. The Tester generates unit tests to verify and filter functionally\nequivalent translated code, thereby evolving the Translator, while the\nTranslator generates translated code as augmented input to evolve the Tester.\nExperimental results demonstrate that QiMeng-MuPa significantly enhances the\nperformance of the base models: when applied to Qwen2.5-Coder, it not only\nimproves Pass@1 by up to 28.91% and boosts Tester performance by 68.90%, but\nalso outperforms the previous state-of-the-art method CodeRosetta by 1.56 and\n6.92 in BLEU and CodeBLEU scores, while achieving performance comparable to\nDeepSeek-R1 and GPT-4.1. Our code is available at\nhttps://github.com/kcxain/mupa.\n", "link": "http://arxiv.org/abs/2506.11153v2", "date": "2025-10-22", "relevancy": 1.9058, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4825}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4739}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4678}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QiMeng-MuPa%3A%20Mutual-Supervised%20Learning%20for%20Sequential-to-Parallel%20Code%0A%20%20Translation&body=Title%3A%20QiMeng-MuPa%3A%20Mutual-Supervised%20Learning%20for%20Sequential-to-Parallel%20Code%0A%20%20Translation%0AAuthor%3A%20Changxin%20Ke%20and%20Rui%20Zhang%20and%20Shuo%20Wang%20and%20Li%20Ding%20and%20Guangli%20Li%20and%20Yuanbo%20Wen%20and%20Shuoming%20Zhang%20and%20Ruiyuan%20Xu%20and%20Jin%20Qin%20and%20Jiaming%20Guo%20and%20Chenxi%20Wang%20and%20Ling%20Li%20and%20Qi%20Guo%20and%20Yunji%20Chen%0AAbstract%3A%20%20%20The%20rise%20of%20GPU-based%20high-performance%20computing%20%28HPC%29%20has%20driven%20the%0Awidespread%20adoption%20of%20parallel%20programming%20models%20such%20as%20CUDA.%20Yet%2C%20the%0Ainherent%20complexity%20of%20parallel%20programming%20creates%20a%20demand%20for%20the%20automated%0Asequential-to-parallel%20approaches.%20However%2C%20data%20scarcity%20poses%20a%20significant%0Achallenge%20for%20machine%20learning-based%20sequential-to-parallel%20code%20translation.%0AAlthough%20recent%20back-translation%20methods%20show%20promise%2C%20they%20still%20fail%20to%0Aensure%20functional%20equivalence%20in%20the%20translated%20code.%20In%20this%20paper%2C%20we%20propose%0A%5Ctextbf%7BQiMeng-MuPa%7D%2C%20a%20novel%20%5Ctextbf%7BMu%7Dtual-Supervised%20Learning%20framework%20for%0ASequential-to-%5Ctextbf%7BPa%7Drallel%20code%20translation%2C%20to%20address%20the%20functional%0Aequivalence%20issue.%20QiMeng-MuPa%20consists%20of%20two%20models%2C%20a%20Translator%20and%20a%0ATester.%20Through%20an%20iterative%20loop%20consisting%20of%20Co-verify%20and%20Co-evolve%20steps%2C%0Athe%20Translator%20and%20the%20Tester%20mutually%20generate%20data%20for%20each%20other%20and%20improve%0Acollectively.%20The%20Tester%20generates%20unit%20tests%20to%20verify%20and%20filter%20functionally%0Aequivalent%20translated%20code%2C%20thereby%20evolving%20the%20Translator%2C%20while%20the%0ATranslator%20generates%20translated%20code%20as%20augmented%20input%20to%20evolve%20the%20Tester.%0AExperimental%20results%20demonstrate%20that%20QiMeng-MuPa%20significantly%20enhances%20the%0Aperformance%20of%20the%20base%20models%3A%20when%20applied%20to%20Qwen2.5-Coder%2C%20it%20not%20only%0Aimproves%20Pass%401%20by%20up%20to%2028.91%25%20and%20boosts%20Tester%20performance%20by%2068.90%25%2C%20but%0Aalso%20outperforms%20the%20previous%20state-of-the-art%20method%20CodeRosetta%20by%201.56%20and%0A6.92%20in%20BLEU%20and%20CodeBLEU%20scores%2C%20while%20achieving%20performance%20comparable%20to%0ADeepSeek-R1%20and%20GPT-4.1.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/kcxain/mupa.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11153v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQiMeng-MuPa%253A%2520Mutual-Supervised%2520Learning%2520for%2520Sequential-to-Parallel%2520Code%250A%2520%2520Translation%26entry.906535625%3DChangxin%2520Ke%2520and%2520Rui%2520Zhang%2520and%2520Shuo%2520Wang%2520and%2520Li%2520Ding%2520and%2520Guangli%2520Li%2520and%2520Yuanbo%2520Wen%2520and%2520Shuoming%2520Zhang%2520and%2520Ruiyuan%2520Xu%2520and%2520Jin%2520Qin%2520and%2520Jiaming%2520Guo%2520and%2520Chenxi%2520Wang%2520and%2520Ling%2520Li%2520and%2520Qi%2520Guo%2520and%2520Yunji%2520Chen%26entry.1292438233%3D%2520%2520The%2520rise%2520of%2520GPU-based%2520high-performance%2520computing%2520%2528HPC%2529%2520has%2520driven%2520the%250Awidespread%2520adoption%2520of%2520parallel%2520programming%2520models%2520such%2520as%2520CUDA.%2520Yet%252C%2520the%250Ainherent%2520complexity%2520of%2520parallel%2520programming%2520creates%2520a%2520demand%2520for%2520the%2520automated%250Asequential-to-parallel%2520approaches.%2520However%252C%2520data%2520scarcity%2520poses%2520a%2520significant%250Achallenge%2520for%2520machine%2520learning-based%2520sequential-to-parallel%2520code%2520translation.%250AAlthough%2520recent%2520back-translation%2520methods%2520show%2520promise%252C%2520they%2520still%2520fail%2520to%250Aensure%2520functional%2520equivalence%2520in%2520the%2520translated%2520code.%2520In%2520this%2520paper%252C%2520we%2520propose%250A%255Ctextbf%257BQiMeng-MuPa%257D%252C%2520a%2520novel%2520%255Ctextbf%257BMu%257Dtual-Supervised%2520Learning%2520framework%2520for%250ASequential-to-%255Ctextbf%257BPa%257Drallel%2520code%2520translation%252C%2520to%2520address%2520the%2520functional%250Aequivalence%2520issue.%2520QiMeng-MuPa%2520consists%2520of%2520two%2520models%252C%2520a%2520Translator%2520and%2520a%250ATester.%2520Through%2520an%2520iterative%2520loop%2520consisting%2520of%2520Co-verify%2520and%2520Co-evolve%2520steps%252C%250Athe%2520Translator%2520and%2520the%2520Tester%2520mutually%2520generate%2520data%2520for%2520each%2520other%2520and%2520improve%250Acollectively.%2520The%2520Tester%2520generates%2520unit%2520tests%2520to%2520verify%2520and%2520filter%2520functionally%250Aequivalent%2520translated%2520code%252C%2520thereby%2520evolving%2520the%2520Translator%252C%2520while%2520the%250ATranslator%2520generates%2520translated%2520code%2520as%2520augmented%2520input%2520to%2520evolve%2520the%2520Tester.%250AExperimental%2520results%2520demonstrate%2520that%2520QiMeng-MuPa%2520significantly%2520enhances%2520the%250Aperformance%2520of%2520the%2520base%2520models%253A%2520when%2520applied%2520to%2520Qwen2.5-Coder%252C%2520it%2520not%2520only%250Aimproves%2520Pass%25401%2520by%2520up%2520to%252028.91%2525%2520and%2520boosts%2520Tester%2520performance%2520by%252068.90%2525%252C%2520but%250Aalso%2520outperforms%2520the%2520previous%2520state-of-the-art%2520method%2520CodeRosetta%2520by%25201.56%2520and%250A6.92%2520in%2520BLEU%2520and%2520CodeBLEU%2520scores%252C%2520while%2520achieving%2520performance%2520comparable%2520to%250ADeepSeek-R1%2520and%2520GPT-4.1.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/kcxain/mupa.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11153v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QiMeng-MuPa%3A%20Mutual-Supervised%20Learning%20for%20Sequential-to-Parallel%20Code%0A%20%20Translation&entry.906535625=Changxin%20Ke%20and%20Rui%20Zhang%20and%20Shuo%20Wang%20and%20Li%20Ding%20and%20Guangli%20Li%20and%20Yuanbo%20Wen%20and%20Shuoming%20Zhang%20and%20Ruiyuan%20Xu%20and%20Jin%20Qin%20and%20Jiaming%20Guo%20and%20Chenxi%20Wang%20and%20Ling%20Li%20and%20Qi%20Guo%20and%20Yunji%20Chen&entry.1292438233=%20%20The%20rise%20of%20GPU-based%20high-performance%20computing%20%28HPC%29%20has%20driven%20the%0Awidespread%20adoption%20of%20parallel%20programming%20models%20such%20as%20CUDA.%20Yet%2C%20the%0Ainherent%20complexity%20of%20parallel%20programming%20creates%20a%20demand%20for%20the%20automated%0Asequential-to-parallel%20approaches.%20However%2C%20data%20scarcity%20poses%20a%20significant%0Achallenge%20for%20machine%20learning-based%20sequential-to-parallel%20code%20translation.%0AAlthough%20recent%20back-translation%20methods%20show%20promise%2C%20they%20still%20fail%20to%0Aensure%20functional%20equivalence%20in%20the%20translated%20code.%20In%20this%20paper%2C%20we%20propose%0A%5Ctextbf%7BQiMeng-MuPa%7D%2C%20a%20novel%20%5Ctextbf%7BMu%7Dtual-Supervised%20Learning%20framework%20for%0ASequential-to-%5Ctextbf%7BPa%7Drallel%20code%20translation%2C%20to%20address%20the%20functional%0Aequivalence%20issue.%20QiMeng-MuPa%20consists%20of%20two%20models%2C%20a%20Translator%20and%20a%0ATester.%20Through%20an%20iterative%20loop%20consisting%20of%20Co-verify%20and%20Co-evolve%20steps%2C%0Athe%20Translator%20and%20the%20Tester%20mutually%20generate%20data%20for%20each%20other%20and%20improve%0Acollectively.%20The%20Tester%20generates%20unit%20tests%20to%20verify%20and%20filter%20functionally%0Aequivalent%20translated%20code%2C%20thereby%20evolving%20the%20Translator%2C%20while%20the%0ATranslator%20generates%20translated%20code%20as%20augmented%20input%20to%20evolve%20the%20Tester.%0AExperimental%20results%20demonstrate%20that%20QiMeng-MuPa%20significantly%20enhances%20the%0Aperformance%20of%20the%20base%20models%3A%20when%20applied%20to%20Qwen2.5-Coder%2C%20it%20not%20only%0Aimproves%20Pass%401%20by%20up%20to%2028.91%25%20and%20boosts%20Tester%20performance%20by%2068.90%25%2C%20but%0Aalso%20outperforms%20the%20previous%20state-of-the-art%20method%20CodeRosetta%20by%201.56%20and%0A6.92%20in%20BLEU%20and%20CodeBLEU%20scores%2C%20while%20achieving%20performance%20comparable%20to%0ADeepSeek-R1%20and%20GPT-4.1.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/kcxain/mupa.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11153v2&entry.124074799=Read"},
{"title": "SEMPO: Lightweight Foundation Models for Time Series Forecasting", "author": "Hui He and Kun Yi and Yuanchi Ma and Qi Zhang and Zhendong Niu and Guansong Pang", "abstract": "  The recent boom of large pre-trained models witnesses remarkable success in\ndeveloping foundation models (FMs) for time series forecasting. Despite\nimpressive performance across diverse downstream forecasting tasks, existing\ntime series FMs possess massive network architectures and require substantial\npre-training on large-scale datasets, which significantly hinders their\ndeployment in resource-constrained environments. In response to this growing\ntension between versatility and affordability, we propose SEMPO, a novel\nlightweight foundation model that requires pretraining on relatively\nsmall-scale data, yet exhibits strong general time series forecasting.\nConcretely, SEMPO comprises two key modules: 1) energy-aware SpEctral\ndecomposition module, that substantially improves the utilization of\npre-training data by modeling not only the high-energy frequency signals but\nalso the low-energy yet informative frequency signals that are ignored in\ncurrent methods; and 2) Mixture-of-PrOmpts enabled Transformer, that learns\nheterogeneous temporal patterns through small dataset-specific prompts and\nadaptively routes time series tokens to prompt-based experts for\nparameter-efficient model adaptation across different datasets and domains.\nEquipped with these modules, SEMPO significantly reduces both pre-training data\nscale and model size, while achieving strong generalization. Extensive\nexperiments on two large-scale benchmarks covering 16 datasets demonstrate the\nsuperior performance of SEMPO in both zero-shot and few-shot forecasting\nscenarios compared with state-of-the-art methods. Code and data are available\nat https://github.com/mala-lab/SEMPO.\n", "link": "http://arxiv.org/abs/2510.19710v1", "date": "2025-10-22", "relevancy": 1.8985, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4801}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4741}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4693}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SEMPO%3A%20Lightweight%20Foundation%20Models%20for%20Time%20Series%20Forecasting&body=Title%3A%20SEMPO%3A%20Lightweight%20Foundation%20Models%20for%20Time%20Series%20Forecasting%0AAuthor%3A%20Hui%20He%20and%20Kun%20Yi%20and%20Yuanchi%20Ma%20and%20Qi%20Zhang%20and%20Zhendong%20Niu%20and%20Guansong%20Pang%0AAbstract%3A%20%20%20The%20recent%20boom%20of%20large%20pre-trained%20models%20witnesses%20remarkable%20success%20in%0Adeveloping%20foundation%20models%20%28FMs%29%20for%20time%20series%20forecasting.%20Despite%0Aimpressive%20performance%20across%20diverse%20downstream%20forecasting%20tasks%2C%20existing%0Atime%20series%20FMs%20possess%20massive%20network%20architectures%20and%20require%20substantial%0Apre-training%20on%20large-scale%20datasets%2C%20which%20significantly%20hinders%20their%0Adeployment%20in%20resource-constrained%20environments.%20In%20response%20to%20this%20growing%0Atension%20between%20versatility%20and%20affordability%2C%20we%20propose%20SEMPO%2C%20a%20novel%0Alightweight%20foundation%20model%20that%20requires%20pretraining%20on%20relatively%0Asmall-scale%20data%2C%20yet%20exhibits%20strong%20general%20time%20series%20forecasting.%0AConcretely%2C%20SEMPO%20comprises%20two%20key%20modules%3A%201%29%20energy-aware%20SpEctral%0Adecomposition%20module%2C%20that%20substantially%20improves%20the%20utilization%20of%0Apre-training%20data%20by%20modeling%20not%20only%20the%20high-energy%20frequency%20signals%20but%0Aalso%20the%20low-energy%20yet%20informative%20frequency%20signals%20that%20are%20ignored%20in%0Acurrent%20methods%3B%20and%202%29%20Mixture-of-PrOmpts%20enabled%20Transformer%2C%20that%20learns%0Aheterogeneous%20temporal%20patterns%20through%20small%20dataset-specific%20prompts%20and%0Aadaptively%20routes%20time%20series%20tokens%20to%20prompt-based%20experts%20for%0Aparameter-efficient%20model%20adaptation%20across%20different%20datasets%20and%20domains.%0AEquipped%20with%20these%20modules%2C%20SEMPO%20significantly%20reduces%20both%20pre-training%20data%0Ascale%20and%20model%20size%2C%20while%20achieving%20strong%20generalization.%20Extensive%0Aexperiments%20on%20two%20large-scale%20benchmarks%20covering%2016%20datasets%20demonstrate%20the%0Asuperior%20performance%20of%20SEMPO%20in%20both%20zero-shot%20and%20few-shot%20forecasting%0Ascenarios%20compared%20with%20state-of-the-art%20methods.%20Code%20and%20data%20are%20available%0Aat%20https%3A//github.com/mala-lab/SEMPO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19710v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSEMPO%253A%2520Lightweight%2520Foundation%2520Models%2520for%2520Time%2520Series%2520Forecasting%26entry.906535625%3DHui%2520He%2520and%2520Kun%2520Yi%2520and%2520Yuanchi%2520Ma%2520and%2520Qi%2520Zhang%2520and%2520Zhendong%2520Niu%2520and%2520Guansong%2520Pang%26entry.1292438233%3D%2520%2520The%2520recent%2520boom%2520of%2520large%2520pre-trained%2520models%2520witnesses%2520remarkable%2520success%2520in%250Adeveloping%2520foundation%2520models%2520%2528FMs%2529%2520for%2520time%2520series%2520forecasting.%2520Despite%250Aimpressive%2520performance%2520across%2520diverse%2520downstream%2520forecasting%2520tasks%252C%2520existing%250Atime%2520series%2520FMs%2520possess%2520massive%2520network%2520architectures%2520and%2520require%2520substantial%250Apre-training%2520on%2520large-scale%2520datasets%252C%2520which%2520significantly%2520hinders%2520their%250Adeployment%2520in%2520resource-constrained%2520environments.%2520In%2520response%2520to%2520this%2520growing%250Atension%2520between%2520versatility%2520and%2520affordability%252C%2520we%2520propose%2520SEMPO%252C%2520a%2520novel%250Alightweight%2520foundation%2520model%2520that%2520requires%2520pretraining%2520on%2520relatively%250Asmall-scale%2520data%252C%2520yet%2520exhibits%2520strong%2520general%2520time%2520series%2520forecasting.%250AConcretely%252C%2520SEMPO%2520comprises%2520two%2520key%2520modules%253A%25201%2529%2520energy-aware%2520SpEctral%250Adecomposition%2520module%252C%2520that%2520substantially%2520improves%2520the%2520utilization%2520of%250Apre-training%2520data%2520by%2520modeling%2520not%2520only%2520the%2520high-energy%2520frequency%2520signals%2520but%250Aalso%2520the%2520low-energy%2520yet%2520informative%2520frequency%2520signals%2520that%2520are%2520ignored%2520in%250Acurrent%2520methods%253B%2520and%25202%2529%2520Mixture-of-PrOmpts%2520enabled%2520Transformer%252C%2520that%2520learns%250Aheterogeneous%2520temporal%2520patterns%2520through%2520small%2520dataset-specific%2520prompts%2520and%250Aadaptively%2520routes%2520time%2520series%2520tokens%2520to%2520prompt-based%2520experts%2520for%250Aparameter-efficient%2520model%2520adaptation%2520across%2520different%2520datasets%2520and%2520domains.%250AEquipped%2520with%2520these%2520modules%252C%2520SEMPO%2520significantly%2520reduces%2520both%2520pre-training%2520data%250Ascale%2520and%2520model%2520size%252C%2520while%2520achieving%2520strong%2520generalization.%2520Extensive%250Aexperiments%2520on%2520two%2520large-scale%2520benchmarks%2520covering%252016%2520datasets%2520demonstrate%2520the%250Asuperior%2520performance%2520of%2520SEMPO%2520in%2520both%2520zero-shot%2520and%2520few-shot%2520forecasting%250Ascenarios%2520compared%2520with%2520state-of-the-art%2520methods.%2520Code%2520and%2520data%2520are%2520available%250Aat%2520https%253A//github.com/mala-lab/SEMPO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19710v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SEMPO%3A%20Lightweight%20Foundation%20Models%20for%20Time%20Series%20Forecasting&entry.906535625=Hui%20He%20and%20Kun%20Yi%20and%20Yuanchi%20Ma%20and%20Qi%20Zhang%20and%20Zhendong%20Niu%20and%20Guansong%20Pang&entry.1292438233=%20%20The%20recent%20boom%20of%20large%20pre-trained%20models%20witnesses%20remarkable%20success%20in%0Adeveloping%20foundation%20models%20%28FMs%29%20for%20time%20series%20forecasting.%20Despite%0Aimpressive%20performance%20across%20diverse%20downstream%20forecasting%20tasks%2C%20existing%0Atime%20series%20FMs%20possess%20massive%20network%20architectures%20and%20require%20substantial%0Apre-training%20on%20large-scale%20datasets%2C%20which%20significantly%20hinders%20their%0Adeployment%20in%20resource-constrained%20environments.%20In%20response%20to%20this%20growing%0Atension%20between%20versatility%20and%20affordability%2C%20we%20propose%20SEMPO%2C%20a%20novel%0Alightweight%20foundation%20model%20that%20requires%20pretraining%20on%20relatively%0Asmall-scale%20data%2C%20yet%20exhibits%20strong%20general%20time%20series%20forecasting.%0AConcretely%2C%20SEMPO%20comprises%20two%20key%20modules%3A%201%29%20energy-aware%20SpEctral%0Adecomposition%20module%2C%20that%20substantially%20improves%20the%20utilization%20of%0Apre-training%20data%20by%20modeling%20not%20only%20the%20high-energy%20frequency%20signals%20but%0Aalso%20the%20low-energy%20yet%20informative%20frequency%20signals%20that%20are%20ignored%20in%0Acurrent%20methods%3B%20and%202%29%20Mixture-of-PrOmpts%20enabled%20Transformer%2C%20that%20learns%0Aheterogeneous%20temporal%20patterns%20through%20small%20dataset-specific%20prompts%20and%0Aadaptively%20routes%20time%20series%20tokens%20to%20prompt-based%20experts%20for%0Aparameter-efficient%20model%20adaptation%20across%20different%20datasets%20and%20domains.%0AEquipped%20with%20these%20modules%2C%20SEMPO%20significantly%20reduces%20both%20pre-training%20data%0Ascale%20and%20model%20size%2C%20while%20achieving%20strong%20generalization.%20Extensive%0Aexperiments%20on%20two%20large-scale%20benchmarks%20covering%2016%20datasets%20demonstrate%20the%0Asuperior%20performance%20of%20SEMPO%20in%20both%20zero-shot%20and%20few-shot%20forecasting%0Ascenarios%20compared%20with%20state-of-the-art%20methods.%20Code%20and%20data%20are%20available%0Aat%20https%3A//github.com/mala-lab/SEMPO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19710v1&entry.124074799=Read"},
{"title": "Smoothed Distance Kernels for MMDs and Applications in Wasserstein\n  Gradient Flows", "author": "Nicolaj Rux and Michael Quellmalz and Gabriele Steidl", "abstract": "  Negative distance kernels $K(x,y) := - \\|x-y\\|$ were used in the definition\nof maximum mean discrepancies (MMDs) in statistics and lead to favorable\nnumerical results in various applications. In particular, so-called slicing\ntechniques for handling high-dimensional kernel summations profit from the\nsimple parameter-free structure of the distance kernel. However, due to its\nnon-smoothness in $x=y$, most of the classical theoretical results, e.g. on\nWasserstein gradient flows of the corresponding MMD functional do not longer\nhold true. In this paper, we propose a new kernel which keeps the favorable\nproperties of the negative distance kernel as being conditionally positive\ndefinite of order one with a nearly linear increase towards infinity and a\nsimple slicing structure, but is Lipschitz differentiable now. Our construction\nis based on a simple 1D smoothing procedure of the absolute value function\nfollowed by a Riemann-Liouville fractional integral transform. Numerical\nresults demonstrate that the new kernel performs similarly well as the negative\ndistance kernel in gradient descent methods, but now with theoretical\nguarantees.\n", "link": "http://arxiv.org/abs/2504.07820v2", "date": "2025-10-22", "relevancy": 1.8788, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4784}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4744}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Smoothed%20Distance%20Kernels%20for%20MMDs%20and%20Applications%20in%20Wasserstein%0A%20%20Gradient%20Flows&body=Title%3A%20Smoothed%20Distance%20Kernels%20for%20MMDs%20and%20Applications%20in%20Wasserstein%0A%20%20Gradient%20Flows%0AAuthor%3A%20Nicolaj%20Rux%20and%20Michael%20Quellmalz%20and%20Gabriele%20Steidl%0AAbstract%3A%20%20%20Negative%20distance%20kernels%20%24K%28x%2Cy%29%20%3A%3D%20-%20%5C%7Cx-y%5C%7C%24%20were%20used%20in%20the%20definition%0Aof%20maximum%20mean%20discrepancies%20%28MMDs%29%20in%20statistics%20and%20lead%20to%20favorable%0Anumerical%20results%20in%20various%20applications.%20In%20particular%2C%20so-called%20slicing%0Atechniques%20for%20handling%20high-dimensional%20kernel%20summations%20profit%20from%20the%0Asimple%20parameter-free%20structure%20of%20the%20distance%20kernel.%20However%2C%20due%20to%20its%0Anon-smoothness%20in%20%24x%3Dy%24%2C%20most%20of%20the%20classical%20theoretical%20results%2C%20e.g.%20on%0AWasserstein%20gradient%20flows%20of%20the%20corresponding%20MMD%20functional%20do%20not%20longer%0Ahold%20true.%20In%20this%20paper%2C%20we%20propose%20a%20new%20kernel%20which%20keeps%20the%20favorable%0Aproperties%20of%20the%20negative%20distance%20kernel%20as%20being%20conditionally%20positive%0Adefinite%20of%20order%20one%20with%20a%20nearly%20linear%20increase%20towards%20infinity%20and%20a%0Asimple%20slicing%20structure%2C%20but%20is%20Lipschitz%20differentiable%20now.%20Our%20construction%0Ais%20based%20on%20a%20simple%201D%20smoothing%20procedure%20of%20the%20absolute%20value%20function%0Afollowed%20by%20a%20Riemann-Liouville%20fractional%20integral%20transform.%20Numerical%0Aresults%20demonstrate%20that%20the%20new%20kernel%20performs%20similarly%20well%20as%20the%20negative%0Adistance%20kernel%20in%20gradient%20descent%20methods%2C%20but%20now%20with%20theoretical%0Aguarantees.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.07820v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmoothed%2520Distance%2520Kernels%2520for%2520MMDs%2520and%2520Applications%2520in%2520Wasserstein%250A%2520%2520Gradient%2520Flows%26entry.906535625%3DNicolaj%2520Rux%2520and%2520Michael%2520Quellmalz%2520and%2520Gabriele%2520Steidl%26entry.1292438233%3D%2520%2520Negative%2520distance%2520kernels%2520%2524K%2528x%252Cy%2529%2520%253A%253D%2520-%2520%255C%257Cx-y%255C%257C%2524%2520were%2520used%2520in%2520the%2520definition%250Aof%2520maximum%2520mean%2520discrepancies%2520%2528MMDs%2529%2520in%2520statistics%2520and%2520lead%2520to%2520favorable%250Anumerical%2520results%2520in%2520various%2520applications.%2520In%2520particular%252C%2520so-called%2520slicing%250Atechniques%2520for%2520handling%2520high-dimensional%2520kernel%2520summations%2520profit%2520from%2520the%250Asimple%2520parameter-free%2520structure%2520of%2520the%2520distance%2520kernel.%2520However%252C%2520due%2520to%2520its%250Anon-smoothness%2520in%2520%2524x%253Dy%2524%252C%2520most%2520of%2520the%2520classical%2520theoretical%2520results%252C%2520e.g.%2520on%250AWasserstein%2520gradient%2520flows%2520of%2520the%2520corresponding%2520MMD%2520functional%2520do%2520not%2520longer%250Ahold%2520true.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520kernel%2520which%2520keeps%2520the%2520favorable%250Aproperties%2520of%2520the%2520negative%2520distance%2520kernel%2520as%2520being%2520conditionally%2520positive%250Adefinite%2520of%2520order%2520one%2520with%2520a%2520nearly%2520linear%2520increase%2520towards%2520infinity%2520and%2520a%250Asimple%2520slicing%2520structure%252C%2520but%2520is%2520Lipschitz%2520differentiable%2520now.%2520Our%2520construction%250Ais%2520based%2520on%2520a%2520simple%25201D%2520smoothing%2520procedure%2520of%2520the%2520absolute%2520value%2520function%250Afollowed%2520by%2520a%2520Riemann-Liouville%2520fractional%2520integral%2520transform.%2520Numerical%250Aresults%2520demonstrate%2520that%2520the%2520new%2520kernel%2520performs%2520similarly%2520well%2520as%2520the%2520negative%250Adistance%2520kernel%2520in%2520gradient%2520descent%2520methods%252C%2520but%2520now%2520with%2520theoretical%250Aguarantees.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.07820v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Smoothed%20Distance%20Kernels%20for%20MMDs%20and%20Applications%20in%20Wasserstein%0A%20%20Gradient%20Flows&entry.906535625=Nicolaj%20Rux%20and%20Michael%20Quellmalz%20and%20Gabriele%20Steidl&entry.1292438233=%20%20Negative%20distance%20kernels%20%24K%28x%2Cy%29%20%3A%3D%20-%20%5C%7Cx-y%5C%7C%24%20were%20used%20in%20the%20definition%0Aof%20maximum%20mean%20discrepancies%20%28MMDs%29%20in%20statistics%20and%20lead%20to%20favorable%0Anumerical%20results%20in%20various%20applications.%20In%20particular%2C%20so-called%20slicing%0Atechniques%20for%20handling%20high-dimensional%20kernel%20summations%20profit%20from%20the%0Asimple%20parameter-free%20structure%20of%20the%20distance%20kernel.%20However%2C%20due%20to%20its%0Anon-smoothness%20in%20%24x%3Dy%24%2C%20most%20of%20the%20classical%20theoretical%20results%2C%20e.g.%20on%0AWasserstein%20gradient%20flows%20of%20the%20corresponding%20MMD%20functional%20do%20not%20longer%0Ahold%20true.%20In%20this%20paper%2C%20we%20propose%20a%20new%20kernel%20which%20keeps%20the%20favorable%0Aproperties%20of%20the%20negative%20distance%20kernel%20as%20being%20conditionally%20positive%0Adefinite%20of%20order%20one%20with%20a%20nearly%20linear%20increase%20towards%20infinity%20and%20a%0Asimple%20slicing%20structure%2C%20but%20is%20Lipschitz%20differentiable%20now.%20Our%20construction%0Ais%20based%20on%20a%20simple%201D%20smoothing%20procedure%20of%20the%20absolute%20value%20function%0Afollowed%20by%20a%20Riemann-Liouville%20fractional%20integral%20transform.%20Numerical%0Aresults%20demonstrate%20that%20the%20new%20kernel%20performs%20similarly%20well%20as%20the%20negative%0Adistance%20kernel%20in%20gradient%20descent%20methods%2C%20but%20now%20with%20theoretical%0Aguarantees.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.07820v2&entry.124074799=Read"},
{"title": "When Do Transformers Learn Heuristics for Graph Connectivity?", "author": "Qilin Ye and Deqing Fu and Robin Jia and Vatsal Sharan", "abstract": "  Transformers often fail to learn generalizable algorithms, instead relying on\nbrittle heuristics. Using graph connectivity as a testbed, we explain this\nphenomenon both theoretically and empirically. We consider a simplified\nTransformer architecture, the disentangled Transformer, and prove that an\n$L$-layer model has capacity to solve for graphs with diameters up to exactly\n$3^L$, implementing an algorithm equivalent to computing powers of the\nadjacency matrix. We analyze the training-dynamics, and show that the learned\nstrategy hinges on whether most training instances are within this model\ncapacity. Within-capacity graphs (diameter $\\leq 3^L$) drive the learning of a\ncorrect algorithmic solution while beyond-capacity graphs drive the learning of\na simple heuristic based on node degrees. Finally, we empirically demonstrate\nthat restricting training data within a model's capacity leads to both standard\nand disentangled transformers learning the exact algorithm rather than the\ndegree-based heuristic.\n", "link": "http://arxiv.org/abs/2510.19753v1", "date": "2025-10-22", "relevancy": 1.8634, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5122}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4623}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Do%20Transformers%20Learn%20Heuristics%20for%20Graph%20Connectivity%3F&body=Title%3A%20When%20Do%20Transformers%20Learn%20Heuristics%20for%20Graph%20Connectivity%3F%0AAuthor%3A%20Qilin%20Ye%20and%20Deqing%20Fu%20and%20Robin%20Jia%20and%20Vatsal%20Sharan%0AAbstract%3A%20%20%20Transformers%20often%20fail%20to%20learn%20generalizable%20algorithms%2C%20instead%20relying%20on%0Abrittle%20heuristics.%20Using%20graph%20connectivity%20as%20a%20testbed%2C%20we%20explain%20this%0Aphenomenon%20both%20theoretically%20and%20empirically.%20We%20consider%20a%20simplified%0ATransformer%20architecture%2C%20the%20disentangled%20Transformer%2C%20and%20prove%20that%20an%0A%24L%24-layer%20model%20has%20capacity%20to%20solve%20for%20graphs%20with%20diameters%20up%20to%20exactly%0A%243%5EL%24%2C%20implementing%20an%20algorithm%20equivalent%20to%20computing%20powers%20of%20the%0Aadjacency%20matrix.%20We%20analyze%20the%20training-dynamics%2C%20and%20show%20that%20the%20learned%0Astrategy%20hinges%20on%20whether%20most%20training%20instances%20are%20within%20this%20model%0Acapacity.%20Within-capacity%20graphs%20%28diameter%20%24%5Cleq%203%5EL%24%29%20drive%20the%20learning%20of%20a%0Acorrect%20algorithmic%20solution%20while%20beyond-capacity%20graphs%20drive%20the%20learning%20of%0Aa%20simple%20heuristic%20based%20on%20node%20degrees.%20Finally%2C%20we%20empirically%20demonstrate%0Athat%20restricting%20training%20data%20within%20a%20model%27s%20capacity%20leads%20to%20both%20standard%0Aand%20disentangled%20transformers%20learning%20the%20exact%20algorithm%20rather%20than%20the%0Adegree-based%20heuristic.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19753v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Do%2520Transformers%2520Learn%2520Heuristics%2520for%2520Graph%2520Connectivity%253F%26entry.906535625%3DQilin%2520Ye%2520and%2520Deqing%2520Fu%2520and%2520Robin%2520Jia%2520and%2520Vatsal%2520Sharan%26entry.1292438233%3D%2520%2520Transformers%2520often%2520fail%2520to%2520learn%2520generalizable%2520algorithms%252C%2520instead%2520relying%2520on%250Abrittle%2520heuristics.%2520Using%2520graph%2520connectivity%2520as%2520a%2520testbed%252C%2520we%2520explain%2520this%250Aphenomenon%2520both%2520theoretically%2520and%2520empirically.%2520We%2520consider%2520a%2520simplified%250ATransformer%2520architecture%252C%2520the%2520disentangled%2520Transformer%252C%2520and%2520prove%2520that%2520an%250A%2524L%2524-layer%2520model%2520has%2520capacity%2520to%2520solve%2520for%2520graphs%2520with%2520diameters%2520up%2520to%2520exactly%250A%25243%255EL%2524%252C%2520implementing%2520an%2520algorithm%2520equivalent%2520to%2520computing%2520powers%2520of%2520the%250Aadjacency%2520matrix.%2520We%2520analyze%2520the%2520training-dynamics%252C%2520and%2520show%2520that%2520the%2520learned%250Astrategy%2520hinges%2520on%2520whether%2520most%2520training%2520instances%2520are%2520within%2520this%2520model%250Acapacity.%2520Within-capacity%2520graphs%2520%2528diameter%2520%2524%255Cleq%25203%255EL%2524%2529%2520drive%2520the%2520learning%2520of%2520a%250Acorrect%2520algorithmic%2520solution%2520while%2520beyond-capacity%2520graphs%2520drive%2520the%2520learning%2520of%250Aa%2520simple%2520heuristic%2520based%2520on%2520node%2520degrees.%2520Finally%252C%2520we%2520empirically%2520demonstrate%250Athat%2520restricting%2520training%2520data%2520within%2520a%2520model%2527s%2520capacity%2520leads%2520to%2520both%2520standard%250Aand%2520disentangled%2520transformers%2520learning%2520the%2520exact%2520algorithm%2520rather%2520than%2520the%250Adegree-based%2520heuristic.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19753v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Do%20Transformers%20Learn%20Heuristics%20for%20Graph%20Connectivity%3F&entry.906535625=Qilin%20Ye%20and%20Deqing%20Fu%20and%20Robin%20Jia%20and%20Vatsal%20Sharan&entry.1292438233=%20%20Transformers%20often%20fail%20to%20learn%20generalizable%20algorithms%2C%20instead%20relying%20on%0Abrittle%20heuristics.%20Using%20graph%20connectivity%20as%20a%20testbed%2C%20we%20explain%20this%0Aphenomenon%20both%20theoretically%20and%20empirically.%20We%20consider%20a%20simplified%0ATransformer%20architecture%2C%20the%20disentangled%20Transformer%2C%20and%20prove%20that%20an%0A%24L%24-layer%20model%20has%20capacity%20to%20solve%20for%20graphs%20with%20diameters%20up%20to%20exactly%0A%243%5EL%24%2C%20implementing%20an%20algorithm%20equivalent%20to%20computing%20powers%20of%20the%0Aadjacency%20matrix.%20We%20analyze%20the%20training-dynamics%2C%20and%20show%20that%20the%20learned%0Astrategy%20hinges%20on%20whether%20most%20training%20instances%20are%20within%20this%20model%0Acapacity.%20Within-capacity%20graphs%20%28diameter%20%24%5Cleq%203%5EL%24%29%20drive%20the%20learning%20of%20a%0Acorrect%20algorithmic%20solution%20while%20beyond-capacity%20graphs%20drive%20the%20learning%20of%0Aa%20simple%20heuristic%20based%20on%20node%20degrees.%20Finally%2C%20we%20empirically%20demonstrate%0Athat%20restricting%20training%20data%20within%20a%20model%27s%20capacity%20leads%20to%20both%20standard%0Aand%20disentangled%20transformers%20learning%20the%20exact%20algorithm%20rather%20than%20the%0Adegree-based%20heuristic.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19753v1&entry.124074799=Read"},
{"title": "Bridging Earth and Space: A Survey on HAPS for Non-Terrestrial Networks", "author": "G. Svistunov and A. Akhtarshenas and D. L\u00f3pez-P\u00e9rez and M. Giordani and G. Geraci and H. Yanikomeroglu", "abstract": "  HAPS are emerging as key enablers in the evolution of 6G wireless networks,\nbridging terrestrial and non-terrestrial infrastructures. Operating in the\nstratosphere, HAPS can provide wide-area coverage, low-latency,\nenergy-efficient broadband communications with flexible deployment options for\ndiverse applications. This survey delivers a comprehensive overview of HAPS use\ncases, technologies, and integration strategies within the 6G ecosystem. The\nroles of HAPS in extending connectivity to underserved regions, supporting\ndynamic backhauling, enabling massive IoT, and delivering reliable low-latency\ncommunications for autonomous and immersive services are discussed. The paper\nreviews state-of-the-art architectures for terrestrial and non-terrestrial\nnetwork integration, highlights recent field trials. Furthermore, key enabling\ntechnologies such as channel modeling, AI-driven resource allocation,\ninterference control, mobility management, and energy-efficient communications\nare examined. The paper also outlines open research challenges. By addressing\nexisting gaps in the literature, this survey positions HAPS as a foundational\ncomponent of globally integrated, resilient, and sustainable 6G networks.\n", "link": "http://arxiv.org/abs/2510.19731v1", "date": "2025-10-22", "relevancy": 1.8571, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3775}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.3707}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.3661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Earth%20and%20Space%3A%20A%20Survey%20on%20HAPS%20for%20Non-Terrestrial%20Networks&body=Title%3A%20Bridging%20Earth%20and%20Space%3A%20A%20Survey%20on%20HAPS%20for%20Non-Terrestrial%20Networks%0AAuthor%3A%20G.%20Svistunov%20and%20A.%20Akhtarshenas%20and%20D.%20L%C3%B3pez-P%C3%A9rez%20and%20M.%20Giordani%20and%20G.%20Geraci%20and%20H.%20Yanikomeroglu%0AAbstract%3A%20%20%20HAPS%20are%20emerging%20as%20key%20enablers%20in%20the%20evolution%20of%206G%20wireless%20networks%2C%0Abridging%20terrestrial%20and%20non-terrestrial%20infrastructures.%20Operating%20in%20the%0Astratosphere%2C%20HAPS%20can%20provide%20wide-area%20coverage%2C%20low-latency%2C%0Aenergy-efficient%20broadband%20communications%20with%20flexible%20deployment%20options%20for%0Adiverse%20applications.%20This%20survey%20delivers%20a%20comprehensive%20overview%20of%20HAPS%20use%0Acases%2C%20technologies%2C%20and%20integration%20strategies%20within%20the%206G%20ecosystem.%20The%0Aroles%20of%20HAPS%20in%20extending%20connectivity%20to%20underserved%20regions%2C%20supporting%0Adynamic%20backhauling%2C%20enabling%20massive%20IoT%2C%20and%20delivering%20reliable%20low-latency%0Acommunications%20for%20autonomous%20and%20immersive%20services%20are%20discussed.%20The%20paper%0Areviews%20state-of-the-art%20architectures%20for%20terrestrial%20and%20non-terrestrial%0Anetwork%20integration%2C%20highlights%20recent%20field%20trials.%20Furthermore%2C%20key%20enabling%0Atechnologies%20such%20as%20channel%20modeling%2C%20AI-driven%20resource%20allocation%2C%0Ainterference%20control%2C%20mobility%20management%2C%20and%20energy-efficient%20communications%0Aare%20examined.%20The%20paper%20also%20outlines%20open%20research%20challenges.%20By%20addressing%0Aexisting%20gaps%20in%20the%20literature%2C%20this%20survey%20positions%20HAPS%20as%20a%20foundational%0Acomponent%20of%20globally%20integrated%2C%20resilient%2C%20and%20sustainable%206G%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19731v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Earth%2520and%2520Space%253A%2520A%2520Survey%2520on%2520HAPS%2520for%2520Non-Terrestrial%2520Networks%26entry.906535625%3DG.%2520Svistunov%2520and%2520A.%2520Akhtarshenas%2520and%2520D.%2520L%25C3%25B3pez-P%25C3%25A9rez%2520and%2520M.%2520Giordani%2520and%2520G.%2520Geraci%2520and%2520H.%2520Yanikomeroglu%26entry.1292438233%3D%2520%2520HAPS%2520are%2520emerging%2520as%2520key%2520enablers%2520in%2520the%2520evolution%2520of%25206G%2520wireless%2520networks%252C%250Abridging%2520terrestrial%2520and%2520non-terrestrial%2520infrastructures.%2520Operating%2520in%2520the%250Astratosphere%252C%2520HAPS%2520can%2520provide%2520wide-area%2520coverage%252C%2520low-latency%252C%250Aenergy-efficient%2520broadband%2520communications%2520with%2520flexible%2520deployment%2520options%2520for%250Adiverse%2520applications.%2520This%2520survey%2520delivers%2520a%2520comprehensive%2520overview%2520of%2520HAPS%2520use%250Acases%252C%2520technologies%252C%2520and%2520integration%2520strategies%2520within%2520the%25206G%2520ecosystem.%2520The%250Aroles%2520of%2520HAPS%2520in%2520extending%2520connectivity%2520to%2520underserved%2520regions%252C%2520supporting%250Adynamic%2520backhauling%252C%2520enabling%2520massive%2520IoT%252C%2520and%2520delivering%2520reliable%2520low-latency%250Acommunications%2520for%2520autonomous%2520and%2520immersive%2520services%2520are%2520discussed.%2520The%2520paper%250Areviews%2520state-of-the-art%2520architectures%2520for%2520terrestrial%2520and%2520non-terrestrial%250Anetwork%2520integration%252C%2520highlights%2520recent%2520field%2520trials.%2520Furthermore%252C%2520key%2520enabling%250Atechnologies%2520such%2520as%2520channel%2520modeling%252C%2520AI-driven%2520resource%2520allocation%252C%250Ainterference%2520control%252C%2520mobility%2520management%252C%2520and%2520energy-efficient%2520communications%250Aare%2520examined.%2520The%2520paper%2520also%2520outlines%2520open%2520research%2520challenges.%2520By%2520addressing%250Aexisting%2520gaps%2520in%2520the%2520literature%252C%2520this%2520survey%2520positions%2520HAPS%2520as%2520a%2520foundational%250Acomponent%2520of%2520globally%2520integrated%252C%2520resilient%252C%2520and%2520sustainable%25206G%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19731v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Earth%20and%20Space%3A%20A%20Survey%20on%20HAPS%20for%20Non-Terrestrial%20Networks&entry.906535625=G.%20Svistunov%20and%20A.%20Akhtarshenas%20and%20D.%20L%C3%B3pez-P%C3%A9rez%20and%20M.%20Giordani%20and%20G.%20Geraci%20and%20H.%20Yanikomeroglu&entry.1292438233=%20%20HAPS%20are%20emerging%20as%20key%20enablers%20in%20the%20evolution%20of%206G%20wireless%20networks%2C%0Abridging%20terrestrial%20and%20non-terrestrial%20infrastructures.%20Operating%20in%20the%0Astratosphere%2C%20HAPS%20can%20provide%20wide-area%20coverage%2C%20low-latency%2C%0Aenergy-efficient%20broadband%20communications%20with%20flexible%20deployment%20options%20for%0Adiverse%20applications.%20This%20survey%20delivers%20a%20comprehensive%20overview%20of%20HAPS%20use%0Acases%2C%20technologies%2C%20and%20integration%20strategies%20within%20the%206G%20ecosystem.%20The%0Aroles%20of%20HAPS%20in%20extending%20connectivity%20to%20underserved%20regions%2C%20supporting%0Adynamic%20backhauling%2C%20enabling%20massive%20IoT%2C%20and%20delivering%20reliable%20low-latency%0Acommunications%20for%20autonomous%20and%20immersive%20services%20are%20discussed.%20The%20paper%0Areviews%20state-of-the-art%20architectures%20for%20terrestrial%20and%20non-terrestrial%0Anetwork%20integration%2C%20highlights%20recent%20field%20trials.%20Furthermore%2C%20key%20enabling%0Atechnologies%20such%20as%20channel%20modeling%2C%20AI-driven%20resource%20allocation%2C%0Ainterference%20control%2C%20mobility%20management%2C%20and%20energy-efficient%20communications%0Aare%20examined.%20The%20paper%20also%20outlines%20open%20research%20challenges.%20By%20addressing%0Aexisting%20gaps%20in%20the%20literature%2C%20this%20survey%20positions%20HAPS%20as%20a%20foundational%0Acomponent%20of%20globally%20integrated%2C%20resilient%2C%20and%20sustainable%206G%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19731v1&entry.124074799=Read"},
{"title": "OmniNWM: Omniscient Driving Navigation World Models", "author": "Bohan Li and Zhuang Ma and Dalong Du and Baorui Peng and Zhujin Liang and Zhenqiang Liu and Chao Ma and Yueming Jin and Hao Zhao and Wenjun Zeng and Xin Jin", "abstract": "  Autonomous driving world models are expected to work effectively across three\ncore dimensions: state, action, and reward. Existing models, however, are\ntypically restricted to limited state modalities, short video sequences,\nimprecise action control, and a lack of reward awareness. In this paper, we\nintroduce OmniNWM, an omniscient panoramic navigation world model that\naddresses all three dimensions within a unified framework. For state, OmniNWM\njointly generates panoramic videos of RGB, semantics, metric depth, and 3D\noccupancy. A flexible forcing strategy enables high-quality long-horizon\nauto-regressive generation. For action, we introduce a normalized panoramic\nPlucker ray-map representation that encodes input trajectories into pixel-level\nsignals, enabling highly precise and generalizable control over panoramic video\ngeneration. Regarding reward, we move beyond learning reward functions with\nexternal image-based models: instead, we leverage the generated 3D occupancy to\ndirectly define rule-based dense rewards for driving compliance and safety.\nExtensive experiments demonstrate that OmniNWM achieves state-of-the-art\nperformance in video generation, control accuracy, and long-horizon stability,\nwhile providing a reliable closed-loop evaluation framework through\noccupancy-grounded rewards. Project page is available at\nhttps://github.com/Arlo0o/OmniNWM.\n", "link": "http://arxiv.org/abs/2510.18313v2", "date": "2025-10-22", "relevancy": 1.853, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.668}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6071}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniNWM%3A%20Omniscient%20Driving%20Navigation%20World%20Models&body=Title%3A%20OmniNWM%3A%20Omniscient%20Driving%20Navigation%20World%20Models%0AAuthor%3A%20Bohan%20Li%20and%20Zhuang%20Ma%20and%20Dalong%20Du%20and%20Baorui%20Peng%20and%20Zhujin%20Liang%20and%20Zhenqiang%20Liu%20and%20Chao%20Ma%20and%20Yueming%20Jin%20and%20Hao%20Zhao%20and%20Wenjun%20Zeng%20and%20Xin%20Jin%0AAbstract%3A%20%20%20Autonomous%20driving%20world%20models%20are%20expected%20to%20work%20effectively%20across%20three%0Acore%20dimensions%3A%20state%2C%20action%2C%20and%20reward.%20Existing%20models%2C%20however%2C%20are%0Atypically%20restricted%20to%20limited%20state%20modalities%2C%20short%20video%20sequences%2C%0Aimprecise%20action%20control%2C%20and%20a%20lack%20of%20reward%20awareness.%20In%20this%20paper%2C%20we%0Aintroduce%20OmniNWM%2C%20an%20omniscient%20panoramic%20navigation%20world%20model%20that%0Aaddresses%20all%20three%20dimensions%20within%20a%20unified%20framework.%20For%20state%2C%20OmniNWM%0Ajointly%20generates%20panoramic%20videos%20of%20RGB%2C%20semantics%2C%20metric%20depth%2C%20and%203D%0Aoccupancy.%20A%20flexible%20forcing%20strategy%20enables%20high-quality%20long-horizon%0Aauto-regressive%20generation.%20For%20action%2C%20we%20introduce%20a%20normalized%20panoramic%0APlucker%20ray-map%20representation%20that%20encodes%20input%20trajectories%20into%20pixel-level%0Asignals%2C%20enabling%20highly%20precise%20and%20generalizable%20control%20over%20panoramic%20video%0Ageneration.%20Regarding%20reward%2C%20we%20move%20beyond%20learning%20reward%20functions%20with%0Aexternal%20image-based%20models%3A%20instead%2C%20we%20leverage%20the%20generated%203D%20occupancy%20to%0Adirectly%20define%20rule-based%20dense%20rewards%20for%20driving%20compliance%20and%20safety.%0AExtensive%20experiments%20demonstrate%20that%20OmniNWM%20achieves%20state-of-the-art%0Aperformance%20in%20video%20generation%2C%20control%20accuracy%2C%20and%20long-horizon%20stability%2C%0Awhile%20providing%20a%20reliable%20closed-loop%20evaluation%20framework%20through%0Aoccupancy-grounded%20rewards.%20Project%20page%20is%20available%20at%0Ahttps%3A//github.com/Arlo0o/OmniNWM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18313v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniNWM%253A%2520Omniscient%2520Driving%2520Navigation%2520World%2520Models%26entry.906535625%3DBohan%2520Li%2520and%2520Zhuang%2520Ma%2520and%2520Dalong%2520Du%2520and%2520Baorui%2520Peng%2520and%2520Zhujin%2520Liang%2520and%2520Zhenqiang%2520Liu%2520and%2520Chao%2520Ma%2520and%2520Yueming%2520Jin%2520and%2520Hao%2520Zhao%2520and%2520Wenjun%2520Zeng%2520and%2520Xin%2520Jin%26entry.1292438233%3D%2520%2520Autonomous%2520driving%2520world%2520models%2520are%2520expected%2520to%2520work%2520effectively%2520across%2520three%250Acore%2520dimensions%253A%2520state%252C%2520action%252C%2520and%2520reward.%2520Existing%2520models%252C%2520however%252C%2520are%250Atypically%2520restricted%2520to%2520limited%2520state%2520modalities%252C%2520short%2520video%2520sequences%252C%250Aimprecise%2520action%2520control%252C%2520and%2520a%2520lack%2520of%2520reward%2520awareness.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520OmniNWM%252C%2520an%2520omniscient%2520panoramic%2520navigation%2520world%2520model%2520that%250Aaddresses%2520all%2520three%2520dimensions%2520within%2520a%2520unified%2520framework.%2520For%2520state%252C%2520OmniNWM%250Ajointly%2520generates%2520panoramic%2520videos%2520of%2520RGB%252C%2520semantics%252C%2520metric%2520depth%252C%2520and%25203D%250Aoccupancy.%2520A%2520flexible%2520forcing%2520strategy%2520enables%2520high-quality%2520long-horizon%250Aauto-regressive%2520generation.%2520For%2520action%252C%2520we%2520introduce%2520a%2520normalized%2520panoramic%250APlucker%2520ray-map%2520representation%2520that%2520encodes%2520input%2520trajectories%2520into%2520pixel-level%250Asignals%252C%2520enabling%2520highly%2520precise%2520and%2520generalizable%2520control%2520over%2520panoramic%2520video%250Ageneration.%2520Regarding%2520reward%252C%2520we%2520move%2520beyond%2520learning%2520reward%2520functions%2520with%250Aexternal%2520image-based%2520models%253A%2520instead%252C%2520we%2520leverage%2520the%2520generated%25203D%2520occupancy%2520to%250Adirectly%2520define%2520rule-based%2520dense%2520rewards%2520for%2520driving%2520compliance%2520and%2520safety.%250AExtensive%2520experiments%2520demonstrate%2520that%2520OmniNWM%2520achieves%2520state-of-the-art%250Aperformance%2520in%2520video%2520generation%252C%2520control%2520accuracy%252C%2520and%2520long-horizon%2520stability%252C%250Awhile%2520providing%2520a%2520reliable%2520closed-loop%2520evaluation%2520framework%2520through%250Aoccupancy-grounded%2520rewards.%2520Project%2520page%2520is%2520available%2520at%250Ahttps%253A//github.com/Arlo0o/OmniNWM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18313v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniNWM%3A%20Omniscient%20Driving%20Navigation%20World%20Models&entry.906535625=Bohan%20Li%20and%20Zhuang%20Ma%20and%20Dalong%20Du%20and%20Baorui%20Peng%20and%20Zhujin%20Liang%20and%20Zhenqiang%20Liu%20and%20Chao%20Ma%20and%20Yueming%20Jin%20and%20Hao%20Zhao%20and%20Wenjun%20Zeng%20and%20Xin%20Jin&entry.1292438233=%20%20Autonomous%20driving%20world%20models%20are%20expected%20to%20work%20effectively%20across%20three%0Acore%20dimensions%3A%20state%2C%20action%2C%20and%20reward.%20Existing%20models%2C%20however%2C%20are%0Atypically%20restricted%20to%20limited%20state%20modalities%2C%20short%20video%20sequences%2C%0Aimprecise%20action%20control%2C%20and%20a%20lack%20of%20reward%20awareness.%20In%20this%20paper%2C%20we%0Aintroduce%20OmniNWM%2C%20an%20omniscient%20panoramic%20navigation%20world%20model%20that%0Aaddresses%20all%20three%20dimensions%20within%20a%20unified%20framework.%20For%20state%2C%20OmniNWM%0Ajointly%20generates%20panoramic%20videos%20of%20RGB%2C%20semantics%2C%20metric%20depth%2C%20and%203D%0Aoccupancy.%20A%20flexible%20forcing%20strategy%20enables%20high-quality%20long-horizon%0Aauto-regressive%20generation.%20For%20action%2C%20we%20introduce%20a%20normalized%20panoramic%0APlucker%20ray-map%20representation%20that%20encodes%20input%20trajectories%20into%20pixel-level%0Asignals%2C%20enabling%20highly%20precise%20and%20generalizable%20control%20over%20panoramic%20video%0Ageneration.%20Regarding%20reward%2C%20we%20move%20beyond%20learning%20reward%20functions%20with%0Aexternal%20image-based%20models%3A%20instead%2C%20we%20leverage%20the%20generated%203D%20occupancy%20to%0Adirectly%20define%20rule-based%20dense%20rewards%20for%20driving%20compliance%20and%20safety.%0AExtensive%20experiments%20demonstrate%20that%20OmniNWM%20achieves%20state-of-the-art%0Aperformance%20in%20video%20generation%2C%20control%20accuracy%2C%20and%20long-horizon%20stability%2C%0Awhile%20providing%20a%20reliable%20closed-loop%20evaluation%20framework%20through%0Aoccupancy-grounded%20rewards.%20Project%20page%20is%20available%20at%0Ahttps%3A//github.com/Arlo0o/OmniNWM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18313v2&entry.124074799=Read"},
{"title": "Exploring the Effect of DNN Depth on Adversarial Attacks in Network\n  Intrusion Detection Systems", "author": "Mohamed ElShehaby and Ashraf Matrawy", "abstract": "  Adversarial attacks pose significant challenges to Machine Learning (ML)\nsystems and especially Deep Neural Networks (DNNs) by subtly manipulating\ninputs to induce incorrect predictions. This paper investigates whether\nincreasing the layer depth of deep neural networks affects their robustness\nagainst adversarial attacks in the Network Intrusion Detection System (NIDS)\ndomain. We compare the adversarial robustness of various deep neural networks\nacross both \\ac{NIDS} and computer vision domains (the latter being widely used\nin adversarial attack experiments). Our experimental results reveal that in the\nNIDS domain, adding more layers does not necessarily improve their performance,\nyet it may actually significantly degrade their robustness against adversarial\nattacks. Conversely, in the computer vision domain, adding more layers exhibits\na more modest impact on robustness. These findings can guide the development of\nrobust neural networks for (NIDS) applications and highlight the unique\ncharacteristics of network security domains within the (ML) landscape.\n", "link": "http://arxiv.org/abs/2510.19761v1", "date": "2025-10-22", "relevancy": 1.8448, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4866}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4473}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4414}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Effect%20of%20DNN%20Depth%20on%20Adversarial%20Attacks%20in%20Network%0A%20%20Intrusion%20Detection%20Systems&body=Title%3A%20Exploring%20the%20Effect%20of%20DNN%20Depth%20on%20Adversarial%20Attacks%20in%20Network%0A%20%20Intrusion%20Detection%20Systems%0AAuthor%3A%20Mohamed%20ElShehaby%20and%20Ashraf%20Matrawy%0AAbstract%3A%20%20%20Adversarial%20attacks%20pose%20significant%20challenges%20to%20Machine%20Learning%20%28ML%29%0Asystems%20and%20especially%20Deep%20Neural%20Networks%20%28DNNs%29%20by%20subtly%20manipulating%0Ainputs%20to%20induce%20incorrect%20predictions.%20This%20paper%20investigates%20whether%0Aincreasing%20the%20layer%20depth%20of%20deep%20neural%20networks%20affects%20their%20robustness%0Aagainst%20adversarial%20attacks%20in%20the%20Network%20Intrusion%20Detection%20System%20%28NIDS%29%0Adomain.%20We%20compare%20the%20adversarial%20robustness%20of%20various%20deep%20neural%20networks%0Aacross%20both%20%5Cac%7BNIDS%7D%20and%20computer%20vision%20domains%20%28the%20latter%20being%20widely%20used%0Ain%20adversarial%20attack%20experiments%29.%20Our%20experimental%20results%20reveal%20that%20in%20the%0ANIDS%20domain%2C%20adding%20more%20layers%20does%20not%20necessarily%20improve%20their%20performance%2C%0Ayet%20it%20may%20actually%20significantly%20degrade%20their%20robustness%20against%20adversarial%0Aattacks.%20Conversely%2C%20in%20the%20computer%20vision%20domain%2C%20adding%20more%20layers%20exhibits%0Aa%20more%20modest%20impact%20on%20robustness.%20These%20findings%20can%20guide%20the%20development%20of%0Arobust%20neural%20networks%20for%20%28NIDS%29%20applications%20and%20highlight%20the%20unique%0Acharacteristics%20of%20network%20security%20domains%20within%20the%20%28ML%29%20landscape.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19761v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Effect%2520of%2520DNN%2520Depth%2520on%2520Adversarial%2520Attacks%2520in%2520Network%250A%2520%2520Intrusion%2520Detection%2520Systems%26entry.906535625%3DMohamed%2520ElShehaby%2520and%2520Ashraf%2520Matrawy%26entry.1292438233%3D%2520%2520Adversarial%2520attacks%2520pose%2520significant%2520challenges%2520to%2520Machine%2520Learning%2520%2528ML%2529%250Asystems%2520and%2520especially%2520Deep%2520Neural%2520Networks%2520%2528DNNs%2529%2520by%2520subtly%2520manipulating%250Ainputs%2520to%2520induce%2520incorrect%2520predictions.%2520This%2520paper%2520investigates%2520whether%250Aincreasing%2520the%2520layer%2520depth%2520of%2520deep%2520neural%2520networks%2520affects%2520their%2520robustness%250Aagainst%2520adversarial%2520attacks%2520in%2520the%2520Network%2520Intrusion%2520Detection%2520System%2520%2528NIDS%2529%250Adomain.%2520We%2520compare%2520the%2520adversarial%2520robustness%2520of%2520various%2520deep%2520neural%2520networks%250Aacross%2520both%2520%255Cac%257BNIDS%257D%2520and%2520computer%2520vision%2520domains%2520%2528the%2520latter%2520being%2520widely%2520used%250Ain%2520adversarial%2520attack%2520experiments%2529.%2520Our%2520experimental%2520results%2520reveal%2520that%2520in%2520the%250ANIDS%2520domain%252C%2520adding%2520more%2520layers%2520does%2520not%2520necessarily%2520improve%2520their%2520performance%252C%250Ayet%2520it%2520may%2520actually%2520significantly%2520degrade%2520their%2520robustness%2520against%2520adversarial%250Aattacks.%2520Conversely%252C%2520in%2520the%2520computer%2520vision%2520domain%252C%2520adding%2520more%2520layers%2520exhibits%250Aa%2520more%2520modest%2520impact%2520on%2520robustness.%2520These%2520findings%2520can%2520guide%2520the%2520development%2520of%250Arobust%2520neural%2520networks%2520for%2520%2528NIDS%2529%2520applications%2520and%2520highlight%2520the%2520unique%250Acharacteristics%2520of%2520network%2520security%2520domains%2520within%2520the%2520%2528ML%2529%2520landscape.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19761v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Effect%20of%20DNN%20Depth%20on%20Adversarial%20Attacks%20in%20Network%0A%20%20Intrusion%20Detection%20Systems&entry.906535625=Mohamed%20ElShehaby%20and%20Ashraf%20Matrawy&entry.1292438233=%20%20Adversarial%20attacks%20pose%20significant%20challenges%20to%20Machine%20Learning%20%28ML%29%0Asystems%20and%20especially%20Deep%20Neural%20Networks%20%28DNNs%29%20by%20subtly%20manipulating%0Ainputs%20to%20induce%20incorrect%20predictions.%20This%20paper%20investigates%20whether%0Aincreasing%20the%20layer%20depth%20of%20deep%20neural%20networks%20affects%20their%20robustness%0Aagainst%20adversarial%20attacks%20in%20the%20Network%20Intrusion%20Detection%20System%20%28NIDS%29%0Adomain.%20We%20compare%20the%20adversarial%20robustness%20of%20various%20deep%20neural%20networks%0Aacross%20both%20%5Cac%7BNIDS%7D%20and%20computer%20vision%20domains%20%28the%20latter%20being%20widely%20used%0Ain%20adversarial%20attack%20experiments%29.%20Our%20experimental%20results%20reveal%20that%20in%20the%0ANIDS%20domain%2C%20adding%20more%20layers%20does%20not%20necessarily%20improve%20their%20performance%2C%0Ayet%20it%20may%20actually%20significantly%20degrade%20their%20robustness%20against%20adversarial%0Aattacks.%20Conversely%2C%20in%20the%20computer%20vision%20domain%2C%20adding%20more%20layers%20exhibits%0Aa%20more%20modest%20impact%20on%20robustness.%20These%20findings%20can%20guide%20the%20development%20of%0Arobust%20neural%20networks%20for%20%28NIDS%29%20applications%20and%20highlight%20the%20unique%0Acharacteristics%20of%20network%20security%20domains%20within%20the%20%28ML%29%20landscape.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19761v1&entry.124074799=Read"},
{"title": "Learning Linear Attention in Polynomial Time", "author": "Morris Yau and Ekin Aky\u00fcrek and Jiayuan Mao and Joshua B. Tenenbaum and Stefanie Jegelka and Jacob Andreas", "abstract": "  Previous research has explored the computational expressivity of Transformer\nmodels in simulating Boolean circuits or Turing machines. However, the\nlearnability of these simulators from observational data has remained an open\nquestion. Our study addresses this gap by providing the first polynomial-time\nlearnability results (specifically strong, agnostic PAC learning) for\nsingle-layer Transformers with linear attention. We show that linear attention\nmay be viewed as a linear predictor in a suitably defined RKHS. As a\nconsequence, the problem of learning any linear transformer may be converted\ninto the problem of learning an ordinary linear predictor in an expanded\nfeature space, and any such predictor may be converted back into a multiheaded\nlinear transformer. Moving to generalization, we show how to efficiently\nidentify training datasets for which every empirical risk minimizer is\nequivalent (up to trivial symmetries) to the linear Transformer that generated\nthe data, thereby guaranteeing the learned model will correctly generalize\nacross all inputs. Finally, we provide examples of computations expressible via\nlinear attention and therefore polynomial-time learnable, including associative\nmemories, finite automata, and a class of Universal Turing Machine (UTMs) with\npolynomially bounded computation histories. We empirically validate our\ntheoretical findings on three tasks: learning random linear attention networks,\nkey--value associations, and learning to execute finite automata. Our findings\nbridge a critical gap between theoretical expressivity and learnability of\nTransformers, and show that flexible and general models of computation are\nefficiently learnable.\n", "link": "http://arxiv.org/abs/2410.10101v3", "date": "2025-10-22", "relevancy": 1.8406, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4729}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.459}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Linear%20Attention%20in%20Polynomial%20Time&body=Title%3A%20Learning%20Linear%20Attention%20in%20Polynomial%20Time%0AAuthor%3A%20Morris%20Yau%20and%20Ekin%20Aky%C3%BCrek%20and%20Jiayuan%20Mao%20and%20Joshua%20B.%20Tenenbaum%20and%20Stefanie%20Jegelka%20and%20Jacob%20Andreas%0AAbstract%3A%20%20%20Previous%20research%20has%20explored%20the%20computational%20expressivity%20of%20Transformer%0Amodels%20in%20simulating%20Boolean%20circuits%20or%20Turing%20machines.%20However%2C%20the%0Alearnability%20of%20these%20simulators%20from%20observational%20data%20has%20remained%20an%20open%0Aquestion.%20Our%20study%20addresses%20this%20gap%20by%20providing%20the%20first%20polynomial-time%0Alearnability%20results%20%28specifically%20strong%2C%20agnostic%20PAC%20learning%29%20for%0Asingle-layer%20Transformers%20with%20linear%20attention.%20We%20show%20that%20linear%20attention%0Amay%20be%20viewed%20as%20a%20linear%20predictor%20in%20a%20suitably%20defined%20RKHS.%20As%20a%0Aconsequence%2C%20the%20problem%20of%20learning%20any%20linear%20transformer%20may%20be%20converted%0Ainto%20the%20problem%20of%20learning%20an%20ordinary%20linear%20predictor%20in%20an%20expanded%0Afeature%20space%2C%20and%20any%20such%20predictor%20may%20be%20converted%20back%20into%20a%20multiheaded%0Alinear%20transformer.%20Moving%20to%20generalization%2C%20we%20show%20how%20to%20efficiently%0Aidentify%20training%20datasets%20for%20which%20every%20empirical%20risk%20minimizer%20is%0Aequivalent%20%28up%20to%20trivial%20symmetries%29%20to%20the%20linear%20Transformer%20that%20generated%0Athe%20data%2C%20thereby%20guaranteeing%20the%20learned%20model%20will%20correctly%20generalize%0Aacross%20all%20inputs.%20Finally%2C%20we%20provide%20examples%20of%20computations%20expressible%20via%0Alinear%20attention%20and%20therefore%20polynomial-time%20learnable%2C%20including%20associative%0Amemories%2C%20finite%20automata%2C%20and%20a%20class%20of%20Universal%20Turing%20Machine%20%28UTMs%29%20with%0Apolynomially%20bounded%20computation%20histories.%20We%20empirically%20validate%20our%0Atheoretical%20findings%20on%20three%20tasks%3A%20learning%20random%20linear%20attention%20networks%2C%0Akey--value%20associations%2C%20and%20learning%20to%20execute%20finite%20automata.%20Our%20findings%0Abridge%20a%20critical%20gap%20between%20theoretical%20expressivity%20and%20learnability%20of%0ATransformers%2C%20and%20show%20that%20flexible%20and%20general%20models%20of%20computation%20are%0Aefficiently%20learnable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10101v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Linear%2520Attention%2520in%2520Polynomial%2520Time%26entry.906535625%3DMorris%2520Yau%2520and%2520Ekin%2520Aky%25C3%25BCrek%2520and%2520Jiayuan%2520Mao%2520and%2520Joshua%2520B.%2520Tenenbaum%2520and%2520Stefanie%2520Jegelka%2520and%2520Jacob%2520Andreas%26entry.1292438233%3D%2520%2520Previous%2520research%2520has%2520explored%2520the%2520computational%2520expressivity%2520of%2520Transformer%250Amodels%2520in%2520simulating%2520Boolean%2520circuits%2520or%2520Turing%2520machines.%2520However%252C%2520the%250Alearnability%2520of%2520these%2520simulators%2520from%2520observational%2520data%2520has%2520remained%2520an%2520open%250Aquestion.%2520Our%2520study%2520addresses%2520this%2520gap%2520by%2520providing%2520the%2520first%2520polynomial-time%250Alearnability%2520results%2520%2528specifically%2520strong%252C%2520agnostic%2520PAC%2520learning%2529%2520for%250Asingle-layer%2520Transformers%2520with%2520linear%2520attention.%2520We%2520show%2520that%2520linear%2520attention%250Amay%2520be%2520viewed%2520as%2520a%2520linear%2520predictor%2520in%2520a%2520suitably%2520defined%2520RKHS.%2520As%2520a%250Aconsequence%252C%2520the%2520problem%2520of%2520learning%2520any%2520linear%2520transformer%2520may%2520be%2520converted%250Ainto%2520the%2520problem%2520of%2520learning%2520an%2520ordinary%2520linear%2520predictor%2520in%2520an%2520expanded%250Afeature%2520space%252C%2520and%2520any%2520such%2520predictor%2520may%2520be%2520converted%2520back%2520into%2520a%2520multiheaded%250Alinear%2520transformer.%2520Moving%2520to%2520generalization%252C%2520we%2520show%2520how%2520to%2520efficiently%250Aidentify%2520training%2520datasets%2520for%2520which%2520every%2520empirical%2520risk%2520minimizer%2520is%250Aequivalent%2520%2528up%2520to%2520trivial%2520symmetries%2529%2520to%2520the%2520linear%2520Transformer%2520that%2520generated%250Athe%2520data%252C%2520thereby%2520guaranteeing%2520the%2520learned%2520model%2520will%2520correctly%2520generalize%250Aacross%2520all%2520inputs.%2520Finally%252C%2520we%2520provide%2520examples%2520of%2520computations%2520expressible%2520via%250Alinear%2520attention%2520and%2520therefore%2520polynomial-time%2520learnable%252C%2520including%2520associative%250Amemories%252C%2520finite%2520automata%252C%2520and%2520a%2520class%2520of%2520Universal%2520Turing%2520Machine%2520%2528UTMs%2529%2520with%250Apolynomially%2520bounded%2520computation%2520histories.%2520We%2520empirically%2520validate%2520our%250Atheoretical%2520findings%2520on%2520three%2520tasks%253A%2520learning%2520random%2520linear%2520attention%2520networks%252C%250Akey--value%2520associations%252C%2520and%2520learning%2520to%2520execute%2520finite%2520automata.%2520Our%2520findings%250Abridge%2520a%2520critical%2520gap%2520between%2520theoretical%2520expressivity%2520and%2520learnability%2520of%250ATransformers%252C%2520and%2520show%2520that%2520flexible%2520and%2520general%2520models%2520of%2520computation%2520are%250Aefficiently%2520learnable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10101v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Linear%20Attention%20in%20Polynomial%20Time&entry.906535625=Morris%20Yau%20and%20Ekin%20Aky%C3%BCrek%20and%20Jiayuan%20Mao%20and%20Joshua%20B.%20Tenenbaum%20and%20Stefanie%20Jegelka%20and%20Jacob%20Andreas&entry.1292438233=%20%20Previous%20research%20has%20explored%20the%20computational%20expressivity%20of%20Transformer%0Amodels%20in%20simulating%20Boolean%20circuits%20or%20Turing%20machines.%20However%2C%20the%0Alearnability%20of%20these%20simulators%20from%20observational%20data%20has%20remained%20an%20open%0Aquestion.%20Our%20study%20addresses%20this%20gap%20by%20providing%20the%20first%20polynomial-time%0Alearnability%20results%20%28specifically%20strong%2C%20agnostic%20PAC%20learning%29%20for%0Asingle-layer%20Transformers%20with%20linear%20attention.%20We%20show%20that%20linear%20attention%0Amay%20be%20viewed%20as%20a%20linear%20predictor%20in%20a%20suitably%20defined%20RKHS.%20As%20a%0Aconsequence%2C%20the%20problem%20of%20learning%20any%20linear%20transformer%20may%20be%20converted%0Ainto%20the%20problem%20of%20learning%20an%20ordinary%20linear%20predictor%20in%20an%20expanded%0Afeature%20space%2C%20and%20any%20such%20predictor%20may%20be%20converted%20back%20into%20a%20multiheaded%0Alinear%20transformer.%20Moving%20to%20generalization%2C%20we%20show%20how%20to%20efficiently%0Aidentify%20training%20datasets%20for%20which%20every%20empirical%20risk%20minimizer%20is%0Aequivalent%20%28up%20to%20trivial%20symmetries%29%20to%20the%20linear%20Transformer%20that%20generated%0Athe%20data%2C%20thereby%20guaranteeing%20the%20learned%20model%20will%20correctly%20generalize%0Aacross%20all%20inputs.%20Finally%2C%20we%20provide%20examples%20of%20computations%20expressible%20via%0Alinear%20attention%20and%20therefore%20polynomial-time%20learnable%2C%20including%20associative%0Amemories%2C%20finite%20automata%2C%20and%20a%20class%20of%20Universal%20Turing%20Machine%20%28UTMs%29%20with%0Apolynomially%20bounded%20computation%20histories.%20We%20empirically%20validate%20our%0Atheoretical%20findings%20on%20three%20tasks%3A%20learning%20random%20linear%20attention%20networks%2C%0Akey--value%20associations%2C%20and%20learning%20to%20execute%20finite%20automata.%20Our%20findings%0Abridge%20a%20critical%20gap%20between%20theoretical%20expressivity%20and%20learnability%20of%0ATransformers%2C%20and%20show%20that%20flexible%20and%20general%20models%20of%20computation%20are%0Aefficiently%20learnable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10101v3&entry.124074799=Read"},
{"title": "Masked Generative Priors Improve World Models Sequence Modelling\n  Capabilities", "author": "Cristian Meo and Mircea Lica and Zarif Ikram and Akihiro Nakano and Vedant Shah and Aniket Rajiv Didolkar and Dianbo Liu and Anirudh Goyal and Justin Dauwels", "abstract": "  Deep Reinforcement Learning (RL) has become the leading approach for creating\nartificial agents in complex environments. Model-based approaches, which are RL\nmethods with world models that predict environment dynamics, are among the most\npromising directions for improving data efficiency, forming a critical step\ntoward bridging the gap between research and real-world deployment. In\nparticular, world models enhance sample efficiency by learning in imagination,\nwhich involves training a generative sequence model of the environment in a\nself-supervised manner. Recently, Masked Generative Modelling has emerged as a\nmore efficient and superior inductive bias for modelling and generating token\nsequences. Building on the Efficient Stochastic Transformer-based World Models\n(STORM) architecture, we replace the traditional MLP prior with a Masked\nGenerative Prior (e.g., MaskGIT Prior) and introduce GIT-STORM. We evaluate our\nmodel on two downstream tasks: reinforcement learning and video prediction.\nGIT-STORM demonstrates substantial performance gains in RL tasks on the Atari\n100k benchmark. Moreover, we apply Transformer-based World Models to continuous\naction environments for the first time, addressing a significant gap in prior\nresearch. To achieve this, we employ a state mixer function that integrates\nlatent state representations with actions, enabling our model to handle\ncontinuous control tasks. We validate this approach through qualitative and\nquantitative analyses on the DeepMind Control Suite, showcasing the\neffectiveness of Transformer-based World Models in this new domain. Our results\nhighlight the versatility and efficacy of the MaskGIT dynamics prior, paving\nthe way for more accurate world models and effective RL policies.\n", "link": "http://arxiv.org/abs/2410.07836v6", "date": "2025-10-22", "relevancy": 1.8383, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6453}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5726}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Masked%20Generative%20Priors%20Improve%20World%20Models%20Sequence%20Modelling%0A%20%20Capabilities&body=Title%3A%20Masked%20Generative%20Priors%20Improve%20World%20Models%20Sequence%20Modelling%0A%20%20Capabilities%0AAuthor%3A%20Cristian%20Meo%20and%20Mircea%20Lica%20and%20Zarif%20Ikram%20and%20Akihiro%20Nakano%20and%20Vedant%20Shah%20and%20Aniket%20Rajiv%20Didolkar%20and%20Dianbo%20Liu%20and%20Anirudh%20Goyal%20and%20Justin%20Dauwels%0AAbstract%3A%20%20%20Deep%20Reinforcement%20Learning%20%28RL%29%20has%20become%20the%20leading%20approach%20for%20creating%0Aartificial%20agents%20in%20complex%20environments.%20Model-based%20approaches%2C%20which%20are%20RL%0Amethods%20with%20world%20models%20that%20predict%20environment%20dynamics%2C%20are%20among%20the%20most%0Apromising%20directions%20for%20improving%20data%20efficiency%2C%20forming%20a%20critical%20step%0Atoward%20bridging%20the%20gap%20between%20research%20and%20real-world%20deployment.%20In%0Aparticular%2C%20world%20models%20enhance%20sample%20efficiency%20by%20learning%20in%20imagination%2C%0Awhich%20involves%20training%20a%20generative%20sequence%20model%20of%20the%20environment%20in%20a%0Aself-supervised%20manner.%20Recently%2C%20Masked%20Generative%20Modelling%20has%20emerged%20as%20a%0Amore%20efficient%20and%20superior%20inductive%20bias%20for%20modelling%20and%20generating%20token%0Asequences.%20Building%20on%20the%20Efficient%20Stochastic%20Transformer-based%20World%20Models%0A%28STORM%29%20architecture%2C%20we%20replace%20the%20traditional%20MLP%20prior%20with%20a%20Masked%0AGenerative%20Prior%20%28e.g.%2C%20MaskGIT%20Prior%29%20and%20introduce%20GIT-STORM.%20We%20evaluate%20our%0Amodel%20on%20two%20downstream%20tasks%3A%20reinforcement%20learning%20and%20video%20prediction.%0AGIT-STORM%20demonstrates%20substantial%20performance%20gains%20in%20RL%20tasks%20on%20the%20Atari%0A100k%20benchmark.%20Moreover%2C%20we%20apply%20Transformer-based%20World%20Models%20to%20continuous%0Aaction%20environments%20for%20the%20first%20time%2C%20addressing%20a%20significant%20gap%20in%20prior%0Aresearch.%20To%20achieve%20this%2C%20we%20employ%20a%20state%20mixer%20function%20that%20integrates%0Alatent%20state%20representations%20with%20actions%2C%20enabling%20our%20model%20to%20handle%0Acontinuous%20control%20tasks.%20We%20validate%20this%20approach%20through%20qualitative%20and%0Aquantitative%20analyses%20on%20the%20DeepMind%20Control%20Suite%2C%20showcasing%20the%0Aeffectiveness%20of%20Transformer-based%20World%20Models%20in%20this%20new%20domain.%20Our%20results%0Ahighlight%20the%20versatility%20and%20efficacy%20of%20the%20MaskGIT%20dynamics%20prior%2C%20paving%0Athe%20way%20for%20more%20accurate%20world%20models%20and%20effective%20RL%20policies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07836v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMasked%2520Generative%2520Priors%2520Improve%2520World%2520Models%2520Sequence%2520Modelling%250A%2520%2520Capabilities%26entry.906535625%3DCristian%2520Meo%2520and%2520Mircea%2520Lica%2520and%2520Zarif%2520Ikram%2520and%2520Akihiro%2520Nakano%2520and%2520Vedant%2520Shah%2520and%2520Aniket%2520Rajiv%2520Didolkar%2520and%2520Dianbo%2520Liu%2520and%2520Anirudh%2520Goyal%2520and%2520Justin%2520Dauwels%26entry.1292438233%3D%2520%2520Deep%2520Reinforcement%2520Learning%2520%2528RL%2529%2520has%2520become%2520the%2520leading%2520approach%2520for%2520creating%250Aartificial%2520agents%2520in%2520complex%2520environments.%2520Model-based%2520approaches%252C%2520which%2520are%2520RL%250Amethods%2520with%2520world%2520models%2520that%2520predict%2520environment%2520dynamics%252C%2520are%2520among%2520the%2520most%250Apromising%2520directions%2520for%2520improving%2520data%2520efficiency%252C%2520forming%2520a%2520critical%2520step%250Atoward%2520bridging%2520the%2520gap%2520between%2520research%2520and%2520real-world%2520deployment.%2520In%250Aparticular%252C%2520world%2520models%2520enhance%2520sample%2520efficiency%2520by%2520learning%2520in%2520imagination%252C%250Awhich%2520involves%2520training%2520a%2520generative%2520sequence%2520model%2520of%2520the%2520environment%2520in%2520a%250Aself-supervised%2520manner.%2520Recently%252C%2520Masked%2520Generative%2520Modelling%2520has%2520emerged%2520as%2520a%250Amore%2520efficient%2520and%2520superior%2520inductive%2520bias%2520for%2520modelling%2520and%2520generating%2520token%250Asequences.%2520Building%2520on%2520the%2520Efficient%2520Stochastic%2520Transformer-based%2520World%2520Models%250A%2528STORM%2529%2520architecture%252C%2520we%2520replace%2520the%2520traditional%2520MLP%2520prior%2520with%2520a%2520Masked%250AGenerative%2520Prior%2520%2528e.g.%252C%2520MaskGIT%2520Prior%2529%2520and%2520introduce%2520GIT-STORM.%2520We%2520evaluate%2520our%250Amodel%2520on%2520two%2520downstream%2520tasks%253A%2520reinforcement%2520learning%2520and%2520video%2520prediction.%250AGIT-STORM%2520demonstrates%2520substantial%2520performance%2520gains%2520in%2520RL%2520tasks%2520on%2520the%2520Atari%250A100k%2520benchmark.%2520Moreover%252C%2520we%2520apply%2520Transformer-based%2520World%2520Models%2520to%2520continuous%250Aaction%2520environments%2520for%2520the%2520first%2520time%252C%2520addressing%2520a%2520significant%2520gap%2520in%2520prior%250Aresearch.%2520To%2520achieve%2520this%252C%2520we%2520employ%2520a%2520state%2520mixer%2520function%2520that%2520integrates%250Alatent%2520state%2520representations%2520with%2520actions%252C%2520enabling%2520our%2520model%2520to%2520handle%250Acontinuous%2520control%2520tasks.%2520We%2520validate%2520this%2520approach%2520through%2520qualitative%2520and%250Aquantitative%2520analyses%2520on%2520the%2520DeepMind%2520Control%2520Suite%252C%2520showcasing%2520the%250Aeffectiveness%2520of%2520Transformer-based%2520World%2520Models%2520in%2520this%2520new%2520domain.%2520Our%2520results%250Ahighlight%2520the%2520versatility%2520and%2520efficacy%2520of%2520the%2520MaskGIT%2520dynamics%2520prior%252C%2520paving%250Athe%2520way%2520for%2520more%2520accurate%2520world%2520models%2520and%2520effective%2520RL%2520policies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07836v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Masked%20Generative%20Priors%20Improve%20World%20Models%20Sequence%20Modelling%0A%20%20Capabilities&entry.906535625=Cristian%20Meo%20and%20Mircea%20Lica%20and%20Zarif%20Ikram%20and%20Akihiro%20Nakano%20and%20Vedant%20Shah%20and%20Aniket%20Rajiv%20Didolkar%20and%20Dianbo%20Liu%20and%20Anirudh%20Goyal%20and%20Justin%20Dauwels&entry.1292438233=%20%20Deep%20Reinforcement%20Learning%20%28RL%29%20has%20become%20the%20leading%20approach%20for%20creating%0Aartificial%20agents%20in%20complex%20environments.%20Model-based%20approaches%2C%20which%20are%20RL%0Amethods%20with%20world%20models%20that%20predict%20environment%20dynamics%2C%20are%20among%20the%20most%0Apromising%20directions%20for%20improving%20data%20efficiency%2C%20forming%20a%20critical%20step%0Atoward%20bridging%20the%20gap%20between%20research%20and%20real-world%20deployment.%20In%0Aparticular%2C%20world%20models%20enhance%20sample%20efficiency%20by%20learning%20in%20imagination%2C%0Awhich%20involves%20training%20a%20generative%20sequence%20model%20of%20the%20environment%20in%20a%0Aself-supervised%20manner.%20Recently%2C%20Masked%20Generative%20Modelling%20has%20emerged%20as%20a%0Amore%20efficient%20and%20superior%20inductive%20bias%20for%20modelling%20and%20generating%20token%0Asequences.%20Building%20on%20the%20Efficient%20Stochastic%20Transformer-based%20World%20Models%0A%28STORM%29%20architecture%2C%20we%20replace%20the%20traditional%20MLP%20prior%20with%20a%20Masked%0AGenerative%20Prior%20%28e.g.%2C%20MaskGIT%20Prior%29%20and%20introduce%20GIT-STORM.%20We%20evaluate%20our%0Amodel%20on%20two%20downstream%20tasks%3A%20reinforcement%20learning%20and%20video%20prediction.%0AGIT-STORM%20demonstrates%20substantial%20performance%20gains%20in%20RL%20tasks%20on%20the%20Atari%0A100k%20benchmark.%20Moreover%2C%20we%20apply%20Transformer-based%20World%20Models%20to%20continuous%0Aaction%20environments%20for%20the%20first%20time%2C%20addressing%20a%20significant%20gap%20in%20prior%0Aresearch.%20To%20achieve%20this%2C%20we%20employ%20a%20state%20mixer%20function%20that%20integrates%0Alatent%20state%20representations%20with%20actions%2C%20enabling%20our%20model%20to%20handle%0Acontinuous%20control%20tasks.%20We%20validate%20this%20approach%20through%20qualitative%20and%0Aquantitative%20analyses%20on%20the%20DeepMind%20Control%20Suite%2C%20showcasing%20the%0Aeffectiveness%20of%20Transformer-based%20World%20Models%20in%20this%20new%20domain.%20Our%20results%0Ahighlight%20the%20versatility%20and%20efficacy%20of%20the%20MaskGIT%20dynamics%20prior%2C%20paving%0Athe%20way%20for%20more%20accurate%20world%20models%20and%20effective%20RL%20policies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07836v6&entry.124074799=Read"},
{"title": "Training-Free Constrained Generation With Stable Diffusion Models", "author": "Stefano Zampini and Jacob K. Christopher and Luca Oneto and Davide Anguita and Ferdinando Fioretto", "abstract": "  Stable diffusion models represent the state-of-the-art in data synthesis\nacross diverse domains and hold transformative potential for applications in\nscience and engineering, e.g., by facilitating the discovery of novel solutions\nand simulating systems that are computationally intractable to model\nexplicitly. While there is increasing effort to incorporate physics-based\nconstraints into generative models, existing techniques are either limited in\ntheir applicability to latent diffusion frameworks or lack the capability to\nstrictly enforce domain-specific constraints. To address this limitation this\npaper proposes a novel integration of stable diffusion models with constrained\noptimization frameworks, enabling the generation of outputs satisfying\nstringent physical and functional requirements. The effectiveness of this\napproach is demonstrated through material design experiments requiring\nadherence to precise morphometric properties, challenging inverse design tasks\ninvolving the generation of materials inducing specific stress-strain\nresponses, and copyright-constrained content generation tasks. All code has\nbeen released at\nhttps://github.com/RAISELab-atUVA/Constrained-Stable-Diffusion.\n", "link": "http://arxiv.org/abs/2502.05625v4", "date": "2025-10-22", "relevancy": 1.8371, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6517}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6142}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5959}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training-Free%20Constrained%20Generation%20With%20Stable%20Diffusion%20Models&body=Title%3A%20Training-Free%20Constrained%20Generation%20With%20Stable%20Diffusion%20Models%0AAuthor%3A%20Stefano%20Zampini%20and%20Jacob%20K.%20Christopher%20and%20Luca%20Oneto%20and%20Davide%20Anguita%20and%20Ferdinando%20Fioretto%0AAbstract%3A%20%20%20Stable%20diffusion%20models%20represent%20the%20state-of-the-art%20in%20data%20synthesis%0Aacross%20diverse%20domains%20and%20hold%20transformative%20potential%20for%20applications%20in%0Ascience%20and%20engineering%2C%20e.g.%2C%20by%20facilitating%20the%20discovery%20of%20novel%20solutions%0Aand%20simulating%20systems%20that%20are%20computationally%20intractable%20to%20model%0Aexplicitly.%20While%20there%20is%20increasing%20effort%20to%20incorporate%20physics-based%0Aconstraints%20into%20generative%20models%2C%20existing%20techniques%20are%20either%20limited%20in%0Atheir%20applicability%20to%20latent%20diffusion%20frameworks%20or%20lack%20the%20capability%20to%0Astrictly%20enforce%20domain-specific%20constraints.%20To%20address%20this%20limitation%20this%0Apaper%20proposes%20a%20novel%20integration%20of%20stable%20diffusion%20models%20with%20constrained%0Aoptimization%20frameworks%2C%20enabling%20the%20generation%20of%20outputs%20satisfying%0Astringent%20physical%20and%20functional%20requirements.%20The%20effectiveness%20of%20this%0Aapproach%20is%20demonstrated%20through%20material%20design%20experiments%20requiring%0Aadherence%20to%20precise%20morphometric%20properties%2C%20challenging%20inverse%20design%20tasks%0Ainvolving%20the%20generation%20of%20materials%20inducing%20specific%20stress-strain%0Aresponses%2C%20and%20copyright-constrained%20content%20generation%20tasks.%20All%20code%20has%0Abeen%20released%20at%0Ahttps%3A//github.com/RAISELab-atUVA/Constrained-Stable-Diffusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05625v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining-Free%2520Constrained%2520Generation%2520With%2520Stable%2520Diffusion%2520Models%26entry.906535625%3DStefano%2520Zampini%2520and%2520Jacob%2520K.%2520Christopher%2520and%2520Luca%2520Oneto%2520and%2520Davide%2520Anguita%2520and%2520Ferdinando%2520Fioretto%26entry.1292438233%3D%2520%2520Stable%2520diffusion%2520models%2520represent%2520the%2520state-of-the-art%2520in%2520data%2520synthesis%250Aacross%2520diverse%2520domains%2520and%2520hold%2520transformative%2520potential%2520for%2520applications%2520in%250Ascience%2520and%2520engineering%252C%2520e.g.%252C%2520by%2520facilitating%2520the%2520discovery%2520of%2520novel%2520solutions%250Aand%2520simulating%2520systems%2520that%2520are%2520computationally%2520intractable%2520to%2520model%250Aexplicitly.%2520While%2520there%2520is%2520increasing%2520effort%2520to%2520incorporate%2520physics-based%250Aconstraints%2520into%2520generative%2520models%252C%2520existing%2520techniques%2520are%2520either%2520limited%2520in%250Atheir%2520applicability%2520to%2520latent%2520diffusion%2520frameworks%2520or%2520lack%2520the%2520capability%2520to%250Astrictly%2520enforce%2520domain-specific%2520constraints.%2520To%2520address%2520this%2520limitation%2520this%250Apaper%2520proposes%2520a%2520novel%2520integration%2520of%2520stable%2520diffusion%2520models%2520with%2520constrained%250Aoptimization%2520frameworks%252C%2520enabling%2520the%2520generation%2520of%2520outputs%2520satisfying%250Astringent%2520physical%2520and%2520functional%2520requirements.%2520The%2520effectiveness%2520of%2520this%250Aapproach%2520is%2520demonstrated%2520through%2520material%2520design%2520experiments%2520requiring%250Aadherence%2520to%2520precise%2520morphometric%2520properties%252C%2520challenging%2520inverse%2520design%2520tasks%250Ainvolving%2520the%2520generation%2520of%2520materials%2520inducing%2520specific%2520stress-strain%250Aresponses%252C%2520and%2520copyright-constrained%2520content%2520generation%2520tasks.%2520All%2520code%2520has%250Abeen%2520released%2520at%250Ahttps%253A//github.com/RAISELab-atUVA/Constrained-Stable-Diffusion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05625v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training-Free%20Constrained%20Generation%20With%20Stable%20Diffusion%20Models&entry.906535625=Stefano%20Zampini%20and%20Jacob%20K.%20Christopher%20and%20Luca%20Oneto%20and%20Davide%20Anguita%20and%20Ferdinando%20Fioretto&entry.1292438233=%20%20Stable%20diffusion%20models%20represent%20the%20state-of-the-art%20in%20data%20synthesis%0Aacross%20diverse%20domains%20and%20hold%20transformative%20potential%20for%20applications%20in%0Ascience%20and%20engineering%2C%20e.g.%2C%20by%20facilitating%20the%20discovery%20of%20novel%20solutions%0Aand%20simulating%20systems%20that%20are%20computationally%20intractable%20to%20model%0Aexplicitly.%20While%20there%20is%20increasing%20effort%20to%20incorporate%20physics-based%0Aconstraints%20into%20generative%20models%2C%20existing%20techniques%20are%20either%20limited%20in%0Atheir%20applicability%20to%20latent%20diffusion%20frameworks%20or%20lack%20the%20capability%20to%0Astrictly%20enforce%20domain-specific%20constraints.%20To%20address%20this%20limitation%20this%0Apaper%20proposes%20a%20novel%20integration%20of%20stable%20diffusion%20models%20with%20constrained%0Aoptimization%20frameworks%2C%20enabling%20the%20generation%20of%20outputs%20satisfying%0Astringent%20physical%20and%20functional%20requirements.%20The%20effectiveness%20of%20this%0Aapproach%20is%20demonstrated%20through%20material%20design%20experiments%20requiring%0Aadherence%20to%20precise%20morphometric%20properties%2C%20challenging%20inverse%20design%20tasks%0Ainvolving%20the%20generation%20of%20materials%20inducing%20specific%20stress-strain%0Aresponses%2C%20and%20copyright-constrained%20content%20generation%20tasks.%20All%20code%20has%0Abeen%20released%20at%0Ahttps%3A//github.com/RAISELab-atUVA/Constrained-Stable-Diffusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05625v4&entry.124074799=Read"},
{"title": "Estimating Long-term Heterogeneous Dose-response Curve: Generalization\n  Bound Leveraging Optimal Transport Weights", "author": "Zeqin Yang and Weilin Chen and Ruichu Cai and Yuguang Yan and Zhifeng Hao and Zhipeng Yu and Zhichao Zou and Jixing Xu and Zhen Peng and Jiecheng Guo", "abstract": "  Long-term treatment effect estimation is a significant but challenging\nproblem in many applications. Existing methods rely on ideal assumptions, such\nas no unobserved confounders or binary treatment, to estimate long-term average\ntreatment effects. However, in numerous real-world applications, these\nassumptions could be violated, and average treatment effects are insufficient\nfor personalized decision-making. In this paper, we address a more general\nproblem of estimating long-term Heterogeneous Dose-Response Curve (HDRC) while\naccounting for unobserved confounders and continuous treatment. Specifically,\nto remove the unobserved confounders in the long-term observational data, we\nintroduce an optimal transport weighting framework to align the long-term\nobservational data to an auxiliary short-term experimental data. Furthermore,\nto accurately predict the heterogeneous effects of continuous treatment, we\nestablish a generalization bound on counterfactual prediction error by\nleveraging the reweighted distribution induced by optimal transport. Finally,\nwe develop a long-term HDRC estimator building upon the above theoretical\nfoundations. Extensive experiments on synthetic and semi-synthetic datasets\ndemonstrate the effectiveness of our approach.\n", "link": "http://arxiv.org/abs/2406.19195v3", "date": "2025-10-22", "relevancy": 1.8307, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4642}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4552}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Estimating%20Long-term%20Heterogeneous%20Dose-response%20Curve%3A%20Generalization%0A%20%20Bound%20Leveraging%20Optimal%20Transport%20Weights&body=Title%3A%20Estimating%20Long-term%20Heterogeneous%20Dose-response%20Curve%3A%20Generalization%0A%20%20Bound%20Leveraging%20Optimal%20Transport%20Weights%0AAuthor%3A%20Zeqin%20Yang%20and%20Weilin%20Chen%20and%20Ruichu%20Cai%20and%20Yuguang%20Yan%20and%20Zhifeng%20Hao%20and%20Zhipeng%20Yu%20and%20Zhichao%20Zou%20and%20Jixing%20Xu%20and%20Zhen%20Peng%20and%20Jiecheng%20Guo%0AAbstract%3A%20%20%20Long-term%20treatment%20effect%20estimation%20is%20a%20significant%20but%20challenging%0Aproblem%20in%20many%20applications.%20Existing%20methods%20rely%20on%20ideal%20assumptions%2C%20such%0Aas%20no%20unobserved%20confounders%20or%20binary%20treatment%2C%20to%20estimate%20long-term%20average%0Atreatment%20effects.%20However%2C%20in%20numerous%20real-world%20applications%2C%20these%0Aassumptions%20could%20be%20violated%2C%20and%20average%20treatment%20effects%20are%20insufficient%0Afor%20personalized%20decision-making.%20In%20this%20paper%2C%20we%20address%20a%20more%20general%0Aproblem%20of%20estimating%20long-term%20Heterogeneous%20Dose-Response%20Curve%20%28HDRC%29%20while%0Aaccounting%20for%20unobserved%20confounders%20and%20continuous%20treatment.%20Specifically%2C%0Ato%20remove%20the%20unobserved%20confounders%20in%20the%20long-term%20observational%20data%2C%20we%0Aintroduce%20an%20optimal%20transport%20weighting%20framework%20to%20align%20the%20long-term%0Aobservational%20data%20to%20an%20auxiliary%20short-term%20experimental%20data.%20Furthermore%2C%0Ato%20accurately%20predict%20the%20heterogeneous%20effects%20of%20continuous%20treatment%2C%20we%0Aestablish%20a%20generalization%20bound%20on%20counterfactual%20prediction%20error%20by%0Aleveraging%20the%20reweighted%20distribution%20induced%20by%20optimal%20transport.%20Finally%2C%0Awe%20develop%20a%20long-term%20HDRC%20estimator%20building%20upon%20the%20above%20theoretical%0Afoundations.%20Extensive%20experiments%20on%20synthetic%20and%20semi-synthetic%20datasets%0Ademonstrate%20the%20effectiveness%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19195v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEstimating%2520Long-term%2520Heterogeneous%2520Dose-response%2520Curve%253A%2520Generalization%250A%2520%2520Bound%2520Leveraging%2520Optimal%2520Transport%2520Weights%26entry.906535625%3DZeqin%2520Yang%2520and%2520Weilin%2520Chen%2520and%2520Ruichu%2520Cai%2520and%2520Yuguang%2520Yan%2520and%2520Zhifeng%2520Hao%2520and%2520Zhipeng%2520Yu%2520and%2520Zhichao%2520Zou%2520and%2520Jixing%2520Xu%2520and%2520Zhen%2520Peng%2520and%2520Jiecheng%2520Guo%26entry.1292438233%3D%2520%2520Long-term%2520treatment%2520effect%2520estimation%2520is%2520a%2520significant%2520but%2520challenging%250Aproblem%2520in%2520many%2520applications.%2520Existing%2520methods%2520rely%2520on%2520ideal%2520assumptions%252C%2520such%250Aas%2520no%2520unobserved%2520confounders%2520or%2520binary%2520treatment%252C%2520to%2520estimate%2520long-term%2520average%250Atreatment%2520effects.%2520However%252C%2520in%2520numerous%2520real-world%2520applications%252C%2520these%250Aassumptions%2520could%2520be%2520violated%252C%2520and%2520average%2520treatment%2520effects%2520are%2520insufficient%250Afor%2520personalized%2520decision-making.%2520In%2520this%2520paper%252C%2520we%2520address%2520a%2520more%2520general%250Aproblem%2520of%2520estimating%2520long-term%2520Heterogeneous%2520Dose-Response%2520Curve%2520%2528HDRC%2529%2520while%250Aaccounting%2520for%2520unobserved%2520confounders%2520and%2520continuous%2520treatment.%2520Specifically%252C%250Ato%2520remove%2520the%2520unobserved%2520confounders%2520in%2520the%2520long-term%2520observational%2520data%252C%2520we%250Aintroduce%2520an%2520optimal%2520transport%2520weighting%2520framework%2520to%2520align%2520the%2520long-term%250Aobservational%2520data%2520to%2520an%2520auxiliary%2520short-term%2520experimental%2520data.%2520Furthermore%252C%250Ato%2520accurately%2520predict%2520the%2520heterogeneous%2520effects%2520of%2520continuous%2520treatment%252C%2520we%250Aestablish%2520a%2520generalization%2520bound%2520on%2520counterfactual%2520prediction%2520error%2520by%250Aleveraging%2520the%2520reweighted%2520distribution%2520induced%2520by%2520optimal%2520transport.%2520Finally%252C%250Awe%2520develop%2520a%2520long-term%2520HDRC%2520estimator%2520building%2520upon%2520the%2520above%2520theoretical%250Afoundations.%2520Extensive%2520experiments%2520on%2520synthetic%2520and%2520semi-synthetic%2520datasets%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19195v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Estimating%20Long-term%20Heterogeneous%20Dose-response%20Curve%3A%20Generalization%0A%20%20Bound%20Leveraging%20Optimal%20Transport%20Weights&entry.906535625=Zeqin%20Yang%20and%20Weilin%20Chen%20and%20Ruichu%20Cai%20and%20Yuguang%20Yan%20and%20Zhifeng%20Hao%20and%20Zhipeng%20Yu%20and%20Zhichao%20Zou%20and%20Jixing%20Xu%20and%20Zhen%20Peng%20and%20Jiecheng%20Guo&entry.1292438233=%20%20Long-term%20treatment%20effect%20estimation%20is%20a%20significant%20but%20challenging%0Aproblem%20in%20many%20applications.%20Existing%20methods%20rely%20on%20ideal%20assumptions%2C%20such%0Aas%20no%20unobserved%20confounders%20or%20binary%20treatment%2C%20to%20estimate%20long-term%20average%0Atreatment%20effects.%20However%2C%20in%20numerous%20real-world%20applications%2C%20these%0Aassumptions%20could%20be%20violated%2C%20and%20average%20treatment%20effects%20are%20insufficient%0Afor%20personalized%20decision-making.%20In%20this%20paper%2C%20we%20address%20a%20more%20general%0Aproblem%20of%20estimating%20long-term%20Heterogeneous%20Dose-Response%20Curve%20%28HDRC%29%20while%0Aaccounting%20for%20unobserved%20confounders%20and%20continuous%20treatment.%20Specifically%2C%0Ato%20remove%20the%20unobserved%20confounders%20in%20the%20long-term%20observational%20data%2C%20we%0Aintroduce%20an%20optimal%20transport%20weighting%20framework%20to%20align%20the%20long-term%0Aobservational%20data%20to%20an%20auxiliary%20short-term%20experimental%20data.%20Furthermore%2C%0Ato%20accurately%20predict%20the%20heterogeneous%20effects%20of%20continuous%20treatment%2C%20we%0Aestablish%20a%20generalization%20bound%20on%20counterfactual%20prediction%20error%20by%0Aleveraging%20the%20reweighted%20distribution%20induced%20by%20optimal%20transport.%20Finally%2C%0Awe%20develop%20a%20long-term%20HDRC%20estimator%20building%20upon%20the%20above%20theoretical%0Afoundations.%20Extensive%20experiments%20on%20synthetic%20and%20semi-synthetic%20datasets%0Ademonstrate%20the%20effectiveness%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19195v3&entry.124074799=Read"},
{"title": "What Expressivity Theory Misses: Message Passing Complexity for GNNs", "author": "Niklas Kemper and Tom Wollschl\u00e4ger and Stephan G\u00fcnnemann", "abstract": "  Expressivity theory, characterizing which graphs a GNN can distinguish, has\nbecome the predominant framework for analyzing GNNs, with new models striving\nfor higher expressivity. However, we argue that this focus is misguided: First,\nhigher expressivity is not necessary for most real-world tasks as these tasks\nrarely require expressivity beyond the basic WL test. Second, expressivity\ntheory's binary characterization and idealized assumptions fail to reflect\nGNNs' practical capabilities. To overcome these limitations, we propose Message\nPassing Complexity (MPC): a continuous measure that quantifies the difficulty\nfor a GNN architecture to solve a given task through message passing. MPC\ncaptures practical limitations like over-squashing while preserving the\ntheoretical impossibility results from expressivity theory, effectively\nnarrowing the gap between theory and practice. Through extensive validation on\nfundamental GNN tasks, we show that MPC's theoretical predictions correlate\nwith empirical performance, successfully explaining architectural successes and\nfailures. Thereby, MPC advances beyond expressivity theory to provide a more\npowerful and nuanced framework for understanding and improving GNN\narchitectures.\n", "link": "http://arxiv.org/abs/2509.01254v2", "date": "2025-10-22", "relevancy": 1.828, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.472}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.455}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Expressivity%20Theory%20Misses%3A%20Message%20Passing%20Complexity%20for%20GNNs&body=Title%3A%20What%20Expressivity%20Theory%20Misses%3A%20Message%20Passing%20Complexity%20for%20GNNs%0AAuthor%3A%20Niklas%20Kemper%20and%20Tom%20Wollschl%C3%A4ger%20and%20Stephan%20G%C3%BCnnemann%0AAbstract%3A%20%20%20Expressivity%20theory%2C%20characterizing%20which%20graphs%20a%20GNN%20can%20distinguish%2C%20has%0Abecome%20the%20predominant%20framework%20for%20analyzing%20GNNs%2C%20with%20new%20models%20striving%0Afor%20higher%20expressivity.%20However%2C%20we%20argue%20that%20this%20focus%20is%20misguided%3A%20First%2C%0Ahigher%20expressivity%20is%20not%20necessary%20for%20most%20real-world%20tasks%20as%20these%20tasks%0Ararely%20require%20expressivity%20beyond%20the%20basic%20WL%20test.%20Second%2C%20expressivity%0Atheory%27s%20binary%20characterization%20and%20idealized%20assumptions%20fail%20to%20reflect%0AGNNs%27%20practical%20capabilities.%20To%20overcome%20these%20limitations%2C%20we%20propose%20Message%0APassing%20Complexity%20%28MPC%29%3A%20a%20continuous%20measure%20that%20quantifies%20the%20difficulty%0Afor%20a%20GNN%20architecture%20to%20solve%20a%20given%20task%20through%20message%20passing.%20MPC%0Acaptures%20practical%20limitations%20like%20over-squashing%20while%20preserving%20the%0Atheoretical%20impossibility%20results%20from%20expressivity%20theory%2C%20effectively%0Anarrowing%20the%20gap%20between%20theory%20and%20practice.%20Through%20extensive%20validation%20on%0Afundamental%20GNN%20tasks%2C%20we%20show%20that%20MPC%27s%20theoretical%20predictions%20correlate%0Awith%20empirical%20performance%2C%20successfully%20explaining%20architectural%20successes%20and%0Afailures.%20Thereby%2C%20MPC%20advances%20beyond%20expressivity%20theory%20to%20provide%20a%20more%0Apowerful%20and%20nuanced%20framework%20for%20understanding%20and%20improving%20GNN%0Aarchitectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.01254v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Expressivity%2520Theory%2520Misses%253A%2520Message%2520Passing%2520Complexity%2520for%2520GNNs%26entry.906535625%3DNiklas%2520Kemper%2520and%2520Tom%2520Wollschl%25C3%25A4ger%2520and%2520Stephan%2520G%25C3%25BCnnemann%26entry.1292438233%3D%2520%2520Expressivity%2520theory%252C%2520characterizing%2520which%2520graphs%2520a%2520GNN%2520can%2520distinguish%252C%2520has%250Abecome%2520the%2520predominant%2520framework%2520for%2520analyzing%2520GNNs%252C%2520with%2520new%2520models%2520striving%250Afor%2520higher%2520expressivity.%2520However%252C%2520we%2520argue%2520that%2520this%2520focus%2520is%2520misguided%253A%2520First%252C%250Ahigher%2520expressivity%2520is%2520not%2520necessary%2520for%2520most%2520real-world%2520tasks%2520as%2520these%2520tasks%250Ararely%2520require%2520expressivity%2520beyond%2520the%2520basic%2520WL%2520test.%2520Second%252C%2520expressivity%250Atheory%2527s%2520binary%2520characterization%2520and%2520idealized%2520assumptions%2520fail%2520to%2520reflect%250AGNNs%2527%2520practical%2520capabilities.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520Message%250APassing%2520Complexity%2520%2528MPC%2529%253A%2520a%2520continuous%2520measure%2520that%2520quantifies%2520the%2520difficulty%250Afor%2520a%2520GNN%2520architecture%2520to%2520solve%2520a%2520given%2520task%2520through%2520message%2520passing.%2520MPC%250Acaptures%2520practical%2520limitations%2520like%2520over-squashing%2520while%2520preserving%2520the%250Atheoretical%2520impossibility%2520results%2520from%2520expressivity%2520theory%252C%2520effectively%250Anarrowing%2520the%2520gap%2520between%2520theory%2520and%2520practice.%2520Through%2520extensive%2520validation%2520on%250Afundamental%2520GNN%2520tasks%252C%2520we%2520show%2520that%2520MPC%2527s%2520theoretical%2520predictions%2520correlate%250Awith%2520empirical%2520performance%252C%2520successfully%2520explaining%2520architectural%2520successes%2520and%250Afailures.%2520Thereby%252C%2520MPC%2520advances%2520beyond%2520expressivity%2520theory%2520to%2520provide%2520a%2520more%250Apowerful%2520and%2520nuanced%2520framework%2520for%2520understanding%2520and%2520improving%2520GNN%250Aarchitectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.01254v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Expressivity%20Theory%20Misses%3A%20Message%20Passing%20Complexity%20for%20GNNs&entry.906535625=Niklas%20Kemper%20and%20Tom%20Wollschl%C3%A4ger%20and%20Stephan%20G%C3%BCnnemann&entry.1292438233=%20%20Expressivity%20theory%2C%20characterizing%20which%20graphs%20a%20GNN%20can%20distinguish%2C%20has%0Abecome%20the%20predominant%20framework%20for%20analyzing%20GNNs%2C%20with%20new%20models%20striving%0Afor%20higher%20expressivity.%20However%2C%20we%20argue%20that%20this%20focus%20is%20misguided%3A%20First%2C%0Ahigher%20expressivity%20is%20not%20necessary%20for%20most%20real-world%20tasks%20as%20these%20tasks%0Ararely%20require%20expressivity%20beyond%20the%20basic%20WL%20test.%20Second%2C%20expressivity%0Atheory%27s%20binary%20characterization%20and%20idealized%20assumptions%20fail%20to%20reflect%0AGNNs%27%20practical%20capabilities.%20To%20overcome%20these%20limitations%2C%20we%20propose%20Message%0APassing%20Complexity%20%28MPC%29%3A%20a%20continuous%20measure%20that%20quantifies%20the%20difficulty%0Afor%20a%20GNN%20architecture%20to%20solve%20a%20given%20task%20through%20message%20passing.%20MPC%0Acaptures%20practical%20limitations%20like%20over-squashing%20while%20preserving%20the%0Atheoretical%20impossibility%20results%20from%20expressivity%20theory%2C%20effectively%0Anarrowing%20the%20gap%20between%20theory%20and%20practice.%20Through%20extensive%20validation%20on%0Afundamental%20GNN%20tasks%2C%20we%20show%20that%20MPC%27s%20theoretical%20predictions%20correlate%0Awith%20empirical%20performance%2C%20successfully%20explaining%20architectural%20successes%20and%0Afailures.%20Thereby%2C%20MPC%20advances%20beyond%20expressivity%20theory%20to%20provide%20a%20more%0Apowerful%20and%20nuanced%20framework%20for%20understanding%20and%20improving%20GNN%0Aarchitectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.01254v2&entry.124074799=Read"},
{"title": "PRING: Rethinking Protein-Protein Interaction Prediction from Pairs to\n  Graphs", "author": "Xinzhe Zheng and Hao Du and Fanding Xu and Jinzhe Li and Zhiyuan Liu and Wenkang Wang and Tao Chen and Wanli Ouyang and Stan Z. Li and Yan Lu and Nanqing Dong and Yang Zhang", "abstract": "  Deep learning-based computational methods have achieved promising results in\npredicting protein-protein interactions (PPIs). However, existing benchmarks\npredominantly focus on isolated pairwise evaluations, overlooking a model's\ncapability to reconstruct biologically meaningful PPI networks, which is\ncrucial for biology research. To address this gap, we introduce PRING, the\nfirst comprehensive benchmark that evaluates protein-protein interaction\nprediction from a graph-level perspective. PRING curates a high-quality,\nmulti-species PPI network dataset comprising 21,484 proteins and 186,818\ninteractions, with well-designed strategies to address both data redundancy and\nleakage. Building on this golden-standard dataset, we establish two\ncomplementary evaluation paradigms: (1) topology-oriented tasks, which assess\nintra and cross-species PPI network construction, and (2) function-oriented\ntasks, including protein complex pathway prediction, GO module analysis, and\nessential protein justification. These evaluations not only reflect the model's\ncapability to understand the network topology but also facilitate protein\nfunction annotation, biological module detection, and even disease mechanism\nanalysis. Extensive experiments on four representative model categories,\nconsisting of sequence similarity-based, naive sequence-based, protein language\nmodel-based, and structure-based approaches, demonstrate that current PPI\nmodels have potential limitations in recovering both structural and functional\nproperties of PPI networks, highlighting the gap in supporting real-world\nbiological applications. We believe PRING provides a reliable platform to guide\nthe development of more effective PPI prediction models for the community. The\ndataset and source code of PRING are available at\nhttps://github.com/SophieSarceau/PRING.\n", "link": "http://arxiv.org/abs/2507.05101v2", "date": "2025-10-22", "relevancy": 1.8229, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4682}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4532}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PRING%3A%20Rethinking%20Protein-Protein%20Interaction%20Prediction%20from%20Pairs%20to%0A%20%20Graphs&body=Title%3A%20PRING%3A%20Rethinking%20Protein-Protein%20Interaction%20Prediction%20from%20Pairs%20to%0A%20%20Graphs%0AAuthor%3A%20Xinzhe%20Zheng%20and%20Hao%20Du%20and%20Fanding%20Xu%20and%20Jinzhe%20Li%20and%20Zhiyuan%20Liu%20and%20Wenkang%20Wang%20and%20Tao%20Chen%20and%20Wanli%20Ouyang%20and%20Stan%20Z.%20Li%20and%20Yan%20Lu%20and%20Nanqing%20Dong%20and%20Yang%20Zhang%0AAbstract%3A%20%20%20Deep%20learning-based%20computational%20methods%20have%20achieved%20promising%20results%20in%0Apredicting%20protein-protein%20interactions%20%28PPIs%29.%20However%2C%20existing%20benchmarks%0Apredominantly%20focus%20on%20isolated%20pairwise%20evaluations%2C%20overlooking%20a%20model%27s%0Acapability%20to%20reconstruct%20biologically%20meaningful%20PPI%20networks%2C%20which%20is%0Acrucial%20for%20biology%20research.%20To%20address%20this%20gap%2C%20we%20introduce%20PRING%2C%20the%0Afirst%20comprehensive%20benchmark%20that%20evaluates%20protein-protein%20interaction%0Aprediction%20from%20a%20graph-level%20perspective.%20PRING%20curates%20a%20high-quality%2C%0Amulti-species%20PPI%20network%20dataset%20comprising%2021%2C484%20proteins%20and%20186%2C818%0Ainteractions%2C%20with%20well-designed%20strategies%20to%20address%20both%20data%20redundancy%20and%0Aleakage.%20Building%20on%20this%20golden-standard%20dataset%2C%20we%20establish%20two%0Acomplementary%20evaluation%20paradigms%3A%20%281%29%20topology-oriented%20tasks%2C%20which%20assess%0Aintra%20and%20cross-species%20PPI%20network%20construction%2C%20and%20%282%29%20function-oriented%0Atasks%2C%20including%20protein%20complex%20pathway%20prediction%2C%20GO%20module%20analysis%2C%20and%0Aessential%20protein%20justification.%20These%20evaluations%20not%20only%20reflect%20the%20model%27s%0Acapability%20to%20understand%20the%20network%20topology%20but%20also%20facilitate%20protein%0Afunction%20annotation%2C%20biological%20module%20detection%2C%20and%20even%20disease%20mechanism%0Aanalysis.%20Extensive%20experiments%20on%20four%20representative%20model%20categories%2C%0Aconsisting%20of%20sequence%20similarity-based%2C%20naive%20sequence-based%2C%20protein%20language%0Amodel-based%2C%20and%20structure-based%20approaches%2C%20demonstrate%20that%20current%20PPI%0Amodels%20have%20potential%20limitations%20in%20recovering%20both%20structural%20and%20functional%0Aproperties%20of%20PPI%20networks%2C%20highlighting%20the%20gap%20in%20supporting%20real-world%0Abiological%20applications.%20We%20believe%20PRING%20provides%20a%20reliable%20platform%20to%20guide%0Athe%20development%20of%20more%20effective%20PPI%20prediction%20models%20for%20the%20community.%20The%0Adataset%20and%20source%20code%20of%20PRING%20are%20available%20at%0Ahttps%3A//github.com/SophieSarceau/PRING.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05101v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPRING%253A%2520Rethinking%2520Protein-Protein%2520Interaction%2520Prediction%2520from%2520Pairs%2520to%250A%2520%2520Graphs%26entry.906535625%3DXinzhe%2520Zheng%2520and%2520Hao%2520Du%2520and%2520Fanding%2520Xu%2520and%2520Jinzhe%2520Li%2520and%2520Zhiyuan%2520Liu%2520and%2520Wenkang%2520Wang%2520and%2520Tao%2520Chen%2520and%2520Wanli%2520Ouyang%2520and%2520Stan%2520Z.%2520Li%2520and%2520Yan%2520Lu%2520and%2520Nanqing%2520Dong%2520and%2520Yang%2520Zhang%26entry.1292438233%3D%2520%2520Deep%2520learning-based%2520computational%2520methods%2520have%2520achieved%2520promising%2520results%2520in%250Apredicting%2520protein-protein%2520interactions%2520%2528PPIs%2529.%2520However%252C%2520existing%2520benchmarks%250Apredominantly%2520focus%2520on%2520isolated%2520pairwise%2520evaluations%252C%2520overlooking%2520a%2520model%2527s%250Acapability%2520to%2520reconstruct%2520biologically%2520meaningful%2520PPI%2520networks%252C%2520which%2520is%250Acrucial%2520for%2520biology%2520research.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520PRING%252C%2520the%250Afirst%2520comprehensive%2520benchmark%2520that%2520evaluates%2520protein-protein%2520interaction%250Aprediction%2520from%2520a%2520graph-level%2520perspective.%2520PRING%2520curates%2520a%2520high-quality%252C%250Amulti-species%2520PPI%2520network%2520dataset%2520comprising%252021%252C484%2520proteins%2520and%2520186%252C818%250Ainteractions%252C%2520with%2520well-designed%2520strategies%2520to%2520address%2520both%2520data%2520redundancy%2520and%250Aleakage.%2520Building%2520on%2520this%2520golden-standard%2520dataset%252C%2520we%2520establish%2520two%250Acomplementary%2520evaluation%2520paradigms%253A%2520%25281%2529%2520topology-oriented%2520tasks%252C%2520which%2520assess%250Aintra%2520and%2520cross-species%2520PPI%2520network%2520construction%252C%2520and%2520%25282%2529%2520function-oriented%250Atasks%252C%2520including%2520protein%2520complex%2520pathway%2520prediction%252C%2520GO%2520module%2520analysis%252C%2520and%250Aessential%2520protein%2520justification.%2520These%2520evaluations%2520not%2520only%2520reflect%2520the%2520model%2527s%250Acapability%2520to%2520understand%2520the%2520network%2520topology%2520but%2520also%2520facilitate%2520protein%250Afunction%2520annotation%252C%2520biological%2520module%2520detection%252C%2520and%2520even%2520disease%2520mechanism%250Aanalysis.%2520Extensive%2520experiments%2520on%2520four%2520representative%2520model%2520categories%252C%250Aconsisting%2520of%2520sequence%2520similarity-based%252C%2520naive%2520sequence-based%252C%2520protein%2520language%250Amodel-based%252C%2520and%2520structure-based%2520approaches%252C%2520demonstrate%2520that%2520current%2520PPI%250Amodels%2520have%2520potential%2520limitations%2520in%2520recovering%2520both%2520structural%2520and%2520functional%250Aproperties%2520of%2520PPI%2520networks%252C%2520highlighting%2520the%2520gap%2520in%2520supporting%2520real-world%250Abiological%2520applications.%2520We%2520believe%2520PRING%2520provides%2520a%2520reliable%2520platform%2520to%2520guide%250Athe%2520development%2520of%2520more%2520effective%2520PPI%2520prediction%2520models%2520for%2520the%2520community.%2520The%250Adataset%2520and%2520source%2520code%2520of%2520PRING%2520are%2520available%2520at%250Ahttps%253A//github.com/SophieSarceau/PRING.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05101v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PRING%3A%20Rethinking%20Protein-Protein%20Interaction%20Prediction%20from%20Pairs%20to%0A%20%20Graphs&entry.906535625=Xinzhe%20Zheng%20and%20Hao%20Du%20and%20Fanding%20Xu%20and%20Jinzhe%20Li%20and%20Zhiyuan%20Liu%20and%20Wenkang%20Wang%20and%20Tao%20Chen%20and%20Wanli%20Ouyang%20and%20Stan%20Z.%20Li%20and%20Yan%20Lu%20and%20Nanqing%20Dong%20and%20Yang%20Zhang&entry.1292438233=%20%20Deep%20learning-based%20computational%20methods%20have%20achieved%20promising%20results%20in%0Apredicting%20protein-protein%20interactions%20%28PPIs%29.%20However%2C%20existing%20benchmarks%0Apredominantly%20focus%20on%20isolated%20pairwise%20evaluations%2C%20overlooking%20a%20model%27s%0Acapability%20to%20reconstruct%20biologically%20meaningful%20PPI%20networks%2C%20which%20is%0Acrucial%20for%20biology%20research.%20To%20address%20this%20gap%2C%20we%20introduce%20PRING%2C%20the%0Afirst%20comprehensive%20benchmark%20that%20evaluates%20protein-protein%20interaction%0Aprediction%20from%20a%20graph-level%20perspective.%20PRING%20curates%20a%20high-quality%2C%0Amulti-species%20PPI%20network%20dataset%20comprising%2021%2C484%20proteins%20and%20186%2C818%0Ainteractions%2C%20with%20well-designed%20strategies%20to%20address%20both%20data%20redundancy%20and%0Aleakage.%20Building%20on%20this%20golden-standard%20dataset%2C%20we%20establish%20two%0Acomplementary%20evaluation%20paradigms%3A%20%281%29%20topology-oriented%20tasks%2C%20which%20assess%0Aintra%20and%20cross-species%20PPI%20network%20construction%2C%20and%20%282%29%20function-oriented%0Atasks%2C%20including%20protein%20complex%20pathway%20prediction%2C%20GO%20module%20analysis%2C%20and%0Aessential%20protein%20justification.%20These%20evaluations%20not%20only%20reflect%20the%20model%27s%0Acapability%20to%20understand%20the%20network%20topology%20but%20also%20facilitate%20protein%0Afunction%20annotation%2C%20biological%20module%20detection%2C%20and%20even%20disease%20mechanism%0Aanalysis.%20Extensive%20experiments%20on%20four%20representative%20model%20categories%2C%0Aconsisting%20of%20sequence%20similarity-based%2C%20naive%20sequence-based%2C%20protein%20language%0Amodel-based%2C%20and%20structure-based%20approaches%2C%20demonstrate%20that%20current%20PPI%0Amodels%20have%20potential%20limitations%20in%20recovering%20both%20structural%20and%20functional%0Aproperties%20of%20PPI%20networks%2C%20highlighting%20the%20gap%20in%20supporting%20real-world%0Abiological%20applications.%20We%20believe%20PRING%20provides%20a%20reliable%20platform%20to%20guide%0Athe%20development%20of%20more%20effective%20PPI%20prediction%20models%20for%20the%20community.%20The%0Adataset%20and%20source%20code%20of%20PRING%20are%20available%20at%0Ahttps%3A//github.com/SophieSarceau/PRING.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05101v2&entry.124074799=Read"},
{"title": "Policy Learning with Abstention", "author": "Ayush Sawarni and Jikai Jin and Justin Whitehouse and Vasilis Syrgkanis", "abstract": "  Policy learning algorithms are widely used in areas such as personalized\nmedicine and advertising to develop individualized treatment regimes. However,\nmost methods force a decision even when predictions are uncertain, which is\nrisky in high-stakes settings. We study policy learning with abstention, where\na policy may defer to a safe default or an expert. When a policy abstains, it\nreceives a small additive reward on top of the value of a random guess. We\npropose a two-stage learner that first identifies a set of near-optimal\npolicies and then constructs an abstention rule from their disagreements. We\nestablish fast O(1/n)-type regret guarantees when propensities are known, and\nextend these guarantees to the unknown-propensity case via a doubly robust (DR)\nobjective. We further show that abstention is a versatile tool with direct\napplications to other core problems in policy learning: it yields improved\nguarantees under margin conditions without the common realizability assumption,\nconnects to distributionally robust policy learning by hedging against small\ndata shifts, and supports safe policy improvement by ensuring improvement over\na baseline policy with high probability.\n", "link": "http://arxiv.org/abs/2510.19672v1", "date": "2025-10-22", "relevancy": 1.8214, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4688}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4514}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4435}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Policy%20Learning%20with%20Abstention&body=Title%3A%20Policy%20Learning%20with%20Abstention%0AAuthor%3A%20Ayush%20Sawarni%20and%20Jikai%20Jin%20and%20Justin%20Whitehouse%20and%20Vasilis%20Syrgkanis%0AAbstract%3A%20%20%20Policy%20learning%20algorithms%20are%20widely%20used%20in%20areas%20such%20as%20personalized%0Amedicine%20and%20advertising%20to%20develop%20individualized%20treatment%20regimes.%20However%2C%0Amost%20methods%20force%20a%20decision%20even%20when%20predictions%20are%20uncertain%2C%20which%20is%0Arisky%20in%20high-stakes%20settings.%20We%20study%20policy%20learning%20with%20abstention%2C%20where%0Aa%20policy%20may%20defer%20to%20a%20safe%20default%20or%20an%20expert.%20When%20a%20policy%20abstains%2C%20it%0Areceives%20a%20small%20additive%20reward%20on%20top%20of%20the%20value%20of%20a%20random%20guess.%20We%0Apropose%20a%20two-stage%20learner%20that%20first%20identifies%20a%20set%20of%20near-optimal%0Apolicies%20and%20then%20constructs%20an%20abstention%20rule%20from%20their%20disagreements.%20We%0Aestablish%20fast%20O%281/n%29-type%20regret%20guarantees%20when%20propensities%20are%20known%2C%20and%0Aextend%20these%20guarantees%20to%20the%20unknown-propensity%20case%20via%20a%20doubly%20robust%20%28DR%29%0Aobjective.%20We%20further%20show%20that%20abstention%20is%20a%20versatile%20tool%20with%20direct%0Aapplications%20to%20other%20core%20problems%20in%20policy%20learning%3A%20it%20yields%20improved%0Aguarantees%20under%20margin%20conditions%20without%20the%20common%20realizability%20assumption%2C%0Aconnects%20to%20distributionally%20robust%20policy%20learning%20by%20hedging%20against%20small%0Adata%20shifts%2C%20and%20supports%20safe%20policy%20improvement%20by%20ensuring%20improvement%20over%0Aa%20baseline%20policy%20with%20high%20probability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19672v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPolicy%2520Learning%2520with%2520Abstention%26entry.906535625%3DAyush%2520Sawarni%2520and%2520Jikai%2520Jin%2520and%2520Justin%2520Whitehouse%2520and%2520Vasilis%2520Syrgkanis%26entry.1292438233%3D%2520%2520Policy%2520learning%2520algorithms%2520are%2520widely%2520used%2520in%2520areas%2520such%2520as%2520personalized%250Amedicine%2520and%2520advertising%2520to%2520develop%2520individualized%2520treatment%2520regimes.%2520However%252C%250Amost%2520methods%2520force%2520a%2520decision%2520even%2520when%2520predictions%2520are%2520uncertain%252C%2520which%2520is%250Arisky%2520in%2520high-stakes%2520settings.%2520We%2520study%2520policy%2520learning%2520with%2520abstention%252C%2520where%250Aa%2520policy%2520may%2520defer%2520to%2520a%2520safe%2520default%2520or%2520an%2520expert.%2520When%2520a%2520policy%2520abstains%252C%2520it%250Areceives%2520a%2520small%2520additive%2520reward%2520on%2520top%2520of%2520the%2520value%2520of%2520a%2520random%2520guess.%2520We%250Apropose%2520a%2520two-stage%2520learner%2520that%2520first%2520identifies%2520a%2520set%2520of%2520near-optimal%250Apolicies%2520and%2520then%2520constructs%2520an%2520abstention%2520rule%2520from%2520their%2520disagreements.%2520We%250Aestablish%2520fast%2520O%25281/n%2529-type%2520regret%2520guarantees%2520when%2520propensities%2520are%2520known%252C%2520and%250Aextend%2520these%2520guarantees%2520to%2520the%2520unknown-propensity%2520case%2520via%2520a%2520doubly%2520robust%2520%2528DR%2529%250Aobjective.%2520We%2520further%2520show%2520that%2520abstention%2520is%2520a%2520versatile%2520tool%2520with%2520direct%250Aapplications%2520to%2520other%2520core%2520problems%2520in%2520policy%2520learning%253A%2520it%2520yields%2520improved%250Aguarantees%2520under%2520margin%2520conditions%2520without%2520the%2520common%2520realizability%2520assumption%252C%250Aconnects%2520to%2520distributionally%2520robust%2520policy%2520learning%2520by%2520hedging%2520against%2520small%250Adata%2520shifts%252C%2520and%2520supports%2520safe%2520policy%2520improvement%2520by%2520ensuring%2520improvement%2520over%250Aa%2520baseline%2520policy%2520with%2520high%2520probability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19672v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Policy%20Learning%20with%20Abstention&entry.906535625=Ayush%20Sawarni%20and%20Jikai%20Jin%20and%20Justin%20Whitehouse%20and%20Vasilis%20Syrgkanis&entry.1292438233=%20%20Policy%20learning%20algorithms%20are%20widely%20used%20in%20areas%20such%20as%20personalized%0Amedicine%20and%20advertising%20to%20develop%20individualized%20treatment%20regimes.%20However%2C%0Amost%20methods%20force%20a%20decision%20even%20when%20predictions%20are%20uncertain%2C%20which%20is%0Arisky%20in%20high-stakes%20settings.%20We%20study%20policy%20learning%20with%20abstention%2C%20where%0Aa%20policy%20may%20defer%20to%20a%20safe%20default%20or%20an%20expert.%20When%20a%20policy%20abstains%2C%20it%0Areceives%20a%20small%20additive%20reward%20on%20top%20of%20the%20value%20of%20a%20random%20guess.%20We%0Apropose%20a%20two-stage%20learner%20that%20first%20identifies%20a%20set%20of%20near-optimal%0Apolicies%20and%20then%20constructs%20an%20abstention%20rule%20from%20their%20disagreements.%20We%0Aestablish%20fast%20O%281/n%29-type%20regret%20guarantees%20when%20propensities%20are%20known%2C%20and%0Aextend%20these%20guarantees%20to%20the%20unknown-propensity%20case%20via%20a%20doubly%20robust%20%28DR%29%0Aobjective.%20We%20further%20show%20that%20abstention%20is%20a%20versatile%20tool%20with%20direct%0Aapplications%20to%20other%20core%20problems%20in%20policy%20learning%3A%20it%20yields%20improved%0Aguarantees%20under%20margin%20conditions%20without%20the%20common%20realizability%20assumption%2C%0Aconnects%20to%20distributionally%20robust%20policy%20learning%20by%20hedging%20against%20small%0Adata%20shifts%2C%20and%20supports%20safe%20policy%20improvement%20by%20ensuring%20improvement%20over%0Aa%20baseline%20policy%20with%20high%20probability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19672v1&entry.124074799=Read"},
{"title": "LyTimeT: Towards Robust and Interpretable State-Variable Discovery", "author": "Kuai Yu and Crystal Su and Xiang Liu and Judah Goldfeder and Mingyuan Shao and Hod Lipson", "abstract": "  Extracting the true dynamical variables of a system from high-dimensional\nvideo is challenging due to distracting visual factors such as background\nmotion, occlusions, and texture changes. We propose LyTimeT, a two-phase\nframework for interpretable variable extraction that learns robust and stable\nlatent representations of dynamical systems. In Phase 1, LyTimeT employs a\nspatio-temporal TimeSformer-based autoencoder that uses global attention to\nfocus on dynamically relevant regions while suppressing nuisance variation,\nenabling distraction-robust latent state learning and accurate long-horizon\nvideo prediction. In Phase 2, we probe the learned latent space, select the\nmost physically meaningful dimensions using linear correlation analysis, and\nrefine the transition dynamics with a Lyapunov-based stability regularizer to\nenforce contraction and reduce error accumulation during roll-outs. Experiments\non five synthetic benchmarks and four real-world dynamical systems, including\nchaotic phenomena, show that LyTimeT achieves mutual information and intrinsic\ndimension estimates closest to ground truth, remains invariant under background\nperturbations, and delivers the lowest analytical mean squared error among\nCNN-based (TIDE) and transformer-only baselines. Our results demonstrate that\ncombining spatio-temporal attention with stability constraints yields\npredictive models that are not only accurate but also physically interpretable.\n", "link": "http://arxiv.org/abs/2510.19716v1", "date": "2025-10-22", "relevancy": 1.672, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5817}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.563}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LyTimeT%3A%20Towards%20Robust%20and%20Interpretable%20State-Variable%20Discovery&body=Title%3A%20LyTimeT%3A%20Towards%20Robust%20and%20Interpretable%20State-Variable%20Discovery%0AAuthor%3A%20Kuai%20Yu%20and%20Crystal%20Su%20and%20Xiang%20Liu%20and%20Judah%20Goldfeder%20and%20Mingyuan%20Shao%20and%20Hod%20Lipson%0AAbstract%3A%20%20%20Extracting%20the%20true%20dynamical%20variables%20of%20a%20system%20from%20high-dimensional%0Avideo%20is%20challenging%20due%20to%20distracting%20visual%20factors%20such%20as%20background%0Amotion%2C%20occlusions%2C%20and%20texture%20changes.%20We%20propose%20LyTimeT%2C%20a%20two-phase%0Aframework%20for%20interpretable%20variable%20extraction%20that%20learns%20robust%20and%20stable%0Alatent%20representations%20of%20dynamical%20systems.%20In%20Phase%201%2C%20LyTimeT%20employs%20a%0Aspatio-temporal%20TimeSformer-based%20autoencoder%20that%20uses%20global%20attention%20to%0Afocus%20on%20dynamically%20relevant%20regions%20while%20suppressing%20nuisance%20variation%2C%0Aenabling%20distraction-robust%20latent%20state%20learning%20and%20accurate%20long-horizon%0Avideo%20prediction.%20In%20Phase%202%2C%20we%20probe%20the%20learned%20latent%20space%2C%20select%20the%0Amost%20physically%20meaningful%20dimensions%20using%20linear%20correlation%20analysis%2C%20and%0Arefine%20the%20transition%20dynamics%20with%20a%20Lyapunov-based%20stability%20regularizer%20to%0Aenforce%20contraction%20and%20reduce%20error%20accumulation%20during%20roll-outs.%20Experiments%0Aon%20five%20synthetic%20benchmarks%20and%20four%20real-world%20dynamical%20systems%2C%20including%0Achaotic%20phenomena%2C%20show%20that%20LyTimeT%20achieves%20mutual%20information%20and%20intrinsic%0Adimension%20estimates%20closest%20to%20ground%20truth%2C%20remains%20invariant%20under%20background%0Aperturbations%2C%20and%20delivers%20the%20lowest%20analytical%20mean%20squared%20error%20among%0ACNN-based%20%28TIDE%29%20and%20transformer-only%20baselines.%20Our%20results%20demonstrate%20that%0Acombining%20spatio-temporal%20attention%20with%20stability%20constraints%20yields%0Apredictive%20models%20that%20are%20not%20only%20accurate%20but%20also%20physically%20interpretable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19716v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLyTimeT%253A%2520Towards%2520Robust%2520and%2520Interpretable%2520State-Variable%2520Discovery%26entry.906535625%3DKuai%2520Yu%2520and%2520Crystal%2520Su%2520and%2520Xiang%2520Liu%2520and%2520Judah%2520Goldfeder%2520and%2520Mingyuan%2520Shao%2520and%2520Hod%2520Lipson%26entry.1292438233%3D%2520%2520Extracting%2520the%2520true%2520dynamical%2520variables%2520of%2520a%2520system%2520from%2520high-dimensional%250Avideo%2520is%2520challenging%2520due%2520to%2520distracting%2520visual%2520factors%2520such%2520as%2520background%250Amotion%252C%2520occlusions%252C%2520and%2520texture%2520changes.%2520We%2520propose%2520LyTimeT%252C%2520a%2520two-phase%250Aframework%2520for%2520interpretable%2520variable%2520extraction%2520that%2520learns%2520robust%2520and%2520stable%250Alatent%2520representations%2520of%2520dynamical%2520systems.%2520In%2520Phase%25201%252C%2520LyTimeT%2520employs%2520a%250Aspatio-temporal%2520TimeSformer-based%2520autoencoder%2520that%2520uses%2520global%2520attention%2520to%250Afocus%2520on%2520dynamically%2520relevant%2520regions%2520while%2520suppressing%2520nuisance%2520variation%252C%250Aenabling%2520distraction-robust%2520latent%2520state%2520learning%2520and%2520accurate%2520long-horizon%250Avideo%2520prediction.%2520In%2520Phase%25202%252C%2520we%2520probe%2520the%2520learned%2520latent%2520space%252C%2520select%2520the%250Amost%2520physically%2520meaningful%2520dimensions%2520using%2520linear%2520correlation%2520analysis%252C%2520and%250Arefine%2520the%2520transition%2520dynamics%2520with%2520a%2520Lyapunov-based%2520stability%2520regularizer%2520to%250Aenforce%2520contraction%2520and%2520reduce%2520error%2520accumulation%2520during%2520roll-outs.%2520Experiments%250Aon%2520five%2520synthetic%2520benchmarks%2520and%2520four%2520real-world%2520dynamical%2520systems%252C%2520including%250Achaotic%2520phenomena%252C%2520show%2520that%2520LyTimeT%2520achieves%2520mutual%2520information%2520and%2520intrinsic%250Adimension%2520estimates%2520closest%2520to%2520ground%2520truth%252C%2520remains%2520invariant%2520under%2520background%250Aperturbations%252C%2520and%2520delivers%2520the%2520lowest%2520analytical%2520mean%2520squared%2520error%2520among%250ACNN-based%2520%2528TIDE%2529%2520and%2520transformer-only%2520baselines.%2520Our%2520results%2520demonstrate%2520that%250Acombining%2520spatio-temporal%2520attention%2520with%2520stability%2520constraints%2520yields%250Apredictive%2520models%2520that%2520are%2520not%2520only%2520accurate%2520but%2520also%2520physically%2520interpretable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19716v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LyTimeT%3A%20Towards%20Robust%20and%20Interpretable%20State-Variable%20Discovery&entry.906535625=Kuai%20Yu%20and%20Crystal%20Su%20and%20Xiang%20Liu%20and%20Judah%20Goldfeder%20and%20Mingyuan%20Shao%20and%20Hod%20Lipson&entry.1292438233=%20%20Extracting%20the%20true%20dynamical%20variables%20of%20a%20system%20from%20high-dimensional%0Avideo%20is%20challenging%20due%20to%20distracting%20visual%20factors%20such%20as%20background%0Amotion%2C%20occlusions%2C%20and%20texture%20changes.%20We%20propose%20LyTimeT%2C%20a%20two-phase%0Aframework%20for%20interpretable%20variable%20extraction%20that%20learns%20robust%20and%20stable%0Alatent%20representations%20of%20dynamical%20systems.%20In%20Phase%201%2C%20LyTimeT%20employs%20a%0Aspatio-temporal%20TimeSformer-based%20autoencoder%20that%20uses%20global%20attention%20to%0Afocus%20on%20dynamically%20relevant%20regions%20while%20suppressing%20nuisance%20variation%2C%0Aenabling%20distraction-robust%20latent%20state%20learning%20and%20accurate%20long-horizon%0Avideo%20prediction.%20In%20Phase%202%2C%20we%20probe%20the%20learned%20latent%20space%2C%20select%20the%0Amost%20physically%20meaningful%20dimensions%20using%20linear%20correlation%20analysis%2C%20and%0Arefine%20the%20transition%20dynamics%20with%20a%20Lyapunov-based%20stability%20regularizer%20to%0Aenforce%20contraction%20and%20reduce%20error%20accumulation%20during%20roll-outs.%20Experiments%0Aon%20five%20synthetic%20benchmarks%20and%20four%20real-world%20dynamical%20systems%2C%20including%0Achaotic%20phenomena%2C%20show%20that%20LyTimeT%20achieves%20mutual%20information%20and%20intrinsic%0Adimension%20estimates%20closest%20to%20ground%20truth%2C%20remains%20invariant%20under%20background%0Aperturbations%2C%20and%20delivers%20the%20lowest%20analytical%20mean%20squared%20error%20among%0ACNN-based%20%28TIDE%29%20and%20transformer-only%20baselines.%20Our%20results%20demonstrate%20that%0Acombining%20spatio-temporal%20attention%20with%20stability%20constraints%20yields%0Apredictive%20models%20that%20are%20not%20only%20accurate%20but%20also%20physically%20interpretable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19716v1&entry.124074799=Read"},
{"title": "BlockGPT: Spatio-Temporal Modelling of Rainfall via Frame-Level\n  Autoregression", "author": "Cristian Meo and Varun Sarathchandran and Avijit Majhi and Shao Hung and Carlo Saccardi and Ruben Imhoff and Roberto Deidda and Remko Uijlenhoet and Justin Dauwels", "abstract": "  Predicting precipitation maps is a highly complex spatiotemporal modeling\ntask, critical for mitigating the impacts of extreme weather events. Short-term\nprecipitation forecasting, or nowcasting, requires models that are not only\naccurate but also computationally efficient for real-time applications. Current\nmethods, such as token-based autoregressive models, often suffer from flawed\ninductive biases and slow inference, while diffusion models can be\ncomputationally intensive. To address these limitations, we introduce BlockGPT,\na generative autoregressive transformer using batched tokenization (Block)\nmethod that predicts full two-dimensional fields (frames) at each time step.\nConceived as a model-agnostic paradigm for video prediction, BlockGPT\nfactorizes space-time by using self-attention within each frame and causal\nattention across frames; in this work, we instantiate it for precipitation\nnowcasting. We evaluate BlockGPT on two precipitation datasets, viz. KNMI\n(Netherlands) and SEVIR (U.S.), comparing it to state-of-the-art baselines\nincluding token-based (NowcastingGPT) and diffusion-based (DiffCast+Phydnet)\nmodels. The results show that BlockGPT achieves superior accuracy, event\nlocalization as measured by categorical metrics, and inference speeds up to 31x\nfaster than comparable baselines.\n", "link": "http://arxiv.org/abs/2510.06293v2", "date": "2025-10-22", "relevancy": 1.0256, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5178}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5176}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BlockGPT%3A%20Spatio-Temporal%20Modelling%20of%20Rainfall%20via%20Frame-Level%0A%20%20Autoregression&body=Title%3A%20BlockGPT%3A%20Spatio-Temporal%20Modelling%20of%20Rainfall%20via%20Frame-Level%0A%20%20Autoregression%0AAuthor%3A%20Cristian%20Meo%20and%20Varun%20Sarathchandran%20and%20Avijit%20Majhi%20and%20Shao%20Hung%20and%20Carlo%20Saccardi%20and%20Ruben%20Imhoff%20and%20Roberto%20Deidda%20and%20Remko%20Uijlenhoet%20and%20Justin%20Dauwels%0AAbstract%3A%20%20%20Predicting%20precipitation%20maps%20is%20a%20highly%20complex%20spatiotemporal%20modeling%0Atask%2C%20critical%20for%20mitigating%20the%20impacts%20of%20extreme%20weather%20events.%20Short-term%0Aprecipitation%20forecasting%2C%20or%20nowcasting%2C%20requires%20models%20that%20are%20not%20only%0Aaccurate%20but%20also%20computationally%20efficient%20for%20real-time%20applications.%20Current%0Amethods%2C%20such%20as%20token-based%20autoregressive%20models%2C%20often%20suffer%20from%20flawed%0Ainductive%20biases%20and%20slow%20inference%2C%20while%20diffusion%20models%20can%20be%0Acomputationally%20intensive.%20To%20address%20these%20limitations%2C%20we%20introduce%20BlockGPT%2C%0Aa%20generative%20autoregressive%20transformer%20using%20batched%20tokenization%20%28Block%29%0Amethod%20that%20predicts%20full%20two-dimensional%20fields%20%28frames%29%20at%20each%20time%20step.%0AConceived%20as%20a%20model-agnostic%20paradigm%20for%20video%20prediction%2C%20BlockGPT%0Afactorizes%20space-time%20by%20using%20self-attention%20within%20each%20frame%20and%20causal%0Aattention%20across%20frames%3B%20in%20this%20work%2C%20we%20instantiate%20it%20for%20precipitation%0Anowcasting.%20We%20evaluate%20BlockGPT%20on%20two%20precipitation%20datasets%2C%20viz.%20KNMI%0A%28Netherlands%29%20and%20SEVIR%20%28U.S.%29%2C%20comparing%20it%20to%20state-of-the-art%20baselines%0Aincluding%20token-based%20%28NowcastingGPT%29%20and%20diffusion-based%20%28DiffCast%2BPhydnet%29%0Amodels.%20The%20results%20show%20that%20BlockGPT%20achieves%20superior%20accuracy%2C%20event%0Alocalization%20as%20measured%20by%20categorical%20metrics%2C%20and%20inference%20speeds%20up%20to%2031x%0Afaster%20than%20comparable%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.06293v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBlockGPT%253A%2520Spatio-Temporal%2520Modelling%2520of%2520Rainfall%2520via%2520Frame-Level%250A%2520%2520Autoregression%26entry.906535625%3DCristian%2520Meo%2520and%2520Varun%2520Sarathchandran%2520and%2520Avijit%2520Majhi%2520and%2520Shao%2520Hung%2520and%2520Carlo%2520Saccardi%2520and%2520Ruben%2520Imhoff%2520and%2520Roberto%2520Deidda%2520and%2520Remko%2520Uijlenhoet%2520and%2520Justin%2520Dauwels%26entry.1292438233%3D%2520%2520Predicting%2520precipitation%2520maps%2520is%2520a%2520highly%2520complex%2520spatiotemporal%2520modeling%250Atask%252C%2520critical%2520for%2520mitigating%2520the%2520impacts%2520of%2520extreme%2520weather%2520events.%2520Short-term%250Aprecipitation%2520forecasting%252C%2520or%2520nowcasting%252C%2520requires%2520models%2520that%2520are%2520not%2520only%250Aaccurate%2520but%2520also%2520computationally%2520efficient%2520for%2520real-time%2520applications.%2520Current%250Amethods%252C%2520such%2520as%2520token-based%2520autoregressive%2520models%252C%2520often%2520suffer%2520from%2520flawed%250Ainductive%2520biases%2520and%2520slow%2520inference%252C%2520while%2520diffusion%2520models%2520can%2520be%250Acomputationally%2520intensive.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520BlockGPT%252C%250Aa%2520generative%2520autoregressive%2520transformer%2520using%2520batched%2520tokenization%2520%2528Block%2529%250Amethod%2520that%2520predicts%2520full%2520two-dimensional%2520fields%2520%2528frames%2529%2520at%2520each%2520time%2520step.%250AConceived%2520as%2520a%2520model-agnostic%2520paradigm%2520for%2520video%2520prediction%252C%2520BlockGPT%250Afactorizes%2520space-time%2520by%2520using%2520self-attention%2520within%2520each%2520frame%2520and%2520causal%250Aattention%2520across%2520frames%253B%2520in%2520this%2520work%252C%2520we%2520instantiate%2520it%2520for%2520precipitation%250Anowcasting.%2520We%2520evaluate%2520BlockGPT%2520on%2520two%2520precipitation%2520datasets%252C%2520viz.%2520KNMI%250A%2528Netherlands%2529%2520and%2520SEVIR%2520%2528U.S.%2529%252C%2520comparing%2520it%2520to%2520state-of-the-art%2520baselines%250Aincluding%2520token-based%2520%2528NowcastingGPT%2529%2520and%2520diffusion-based%2520%2528DiffCast%252BPhydnet%2529%250Amodels.%2520The%2520results%2520show%2520that%2520BlockGPT%2520achieves%2520superior%2520accuracy%252C%2520event%250Alocalization%2520as%2520measured%2520by%2520categorical%2520metrics%252C%2520and%2520inference%2520speeds%2520up%2520to%252031x%250Afaster%2520than%2520comparable%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06293v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BlockGPT%3A%20Spatio-Temporal%20Modelling%20of%20Rainfall%20via%20Frame-Level%0A%20%20Autoregression&entry.906535625=Cristian%20Meo%20and%20Varun%20Sarathchandran%20and%20Avijit%20Majhi%20and%20Shao%20Hung%20and%20Carlo%20Saccardi%20and%20Ruben%20Imhoff%20and%20Roberto%20Deidda%20and%20Remko%20Uijlenhoet%20and%20Justin%20Dauwels&entry.1292438233=%20%20Predicting%20precipitation%20maps%20is%20a%20highly%20complex%20spatiotemporal%20modeling%0Atask%2C%20critical%20for%20mitigating%20the%20impacts%20of%20extreme%20weather%20events.%20Short-term%0Aprecipitation%20forecasting%2C%20or%20nowcasting%2C%20requires%20models%20that%20are%20not%20only%0Aaccurate%20but%20also%20computationally%20efficient%20for%20real-time%20applications.%20Current%0Amethods%2C%20such%20as%20token-based%20autoregressive%20models%2C%20often%20suffer%20from%20flawed%0Ainductive%20biases%20and%20slow%20inference%2C%20while%20diffusion%20models%20can%20be%0Acomputationally%20intensive.%20To%20address%20these%20limitations%2C%20we%20introduce%20BlockGPT%2C%0Aa%20generative%20autoregressive%20transformer%20using%20batched%20tokenization%20%28Block%29%0Amethod%20that%20predicts%20full%20two-dimensional%20fields%20%28frames%29%20at%20each%20time%20step.%0AConceived%20as%20a%20model-agnostic%20paradigm%20for%20video%20prediction%2C%20BlockGPT%0Afactorizes%20space-time%20by%20using%20self-attention%20within%20each%20frame%20and%20causal%0Aattention%20across%20frames%3B%20in%20this%20work%2C%20we%20instantiate%20it%20for%20precipitation%0Anowcasting.%20We%20evaluate%20BlockGPT%20on%20two%20precipitation%20datasets%2C%20viz.%20KNMI%0A%28Netherlands%29%20and%20SEVIR%20%28U.S.%29%2C%20comparing%20it%20to%20state-of-the-art%20baselines%0Aincluding%20token-based%20%28NowcastingGPT%29%20and%20diffusion-based%20%28DiffCast%2BPhydnet%29%0Amodels.%20The%20results%20show%20that%20BlockGPT%20achieves%20superior%20accuracy%2C%20event%0Alocalization%20as%20measured%20by%20categorical%20metrics%2C%20and%20inference%20speeds%20up%20to%2031x%0Afaster%20than%20comparable%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.06293v2&entry.124074799=Read"},
{"title": "AttentionSwarm: Reinforcement Learning with Attention Control Barier\n  Function for Crazyflie Drones in Dynamic Environments", "author": "Grik Tadevosyan and Valerii Serpiva and Aleksey Fedoseev and Roohan Ahmed Khan and Demetros Aschu and Faryal Batool and Nickolay Efanov and Artem Mikhaylov and Dzmitry Tsetserukou", "abstract": "  We introduce AttentionSwarm, a novel benchmark designed to evaluate safe and\nefficient swarm control in a dynamic drone racing scenario. Central to our\napproach is the Attention Model-Based Control Barrier Function (CBF) framework,\nwhich integrates attention mechanisms with safety-critical control theory to\nenable real-time collision avoidance and trajectory optimization. This\nframework dynamically prioritizes critical obstacles and agents in the swarm's\nvicinity using attention weights, while CBFs formally guarantee safety by\nenforcing collision-free constraints. The AttentionSwarm algorithm was\ndeveloped and evaluated using a swarm of Crazyflie 2.1 micro quadrotors, which\nwere tested indoors with the Vicon motion capture system to ensure precise\nlocalization and control. Experimental results show that our system achieves a\n95-100% collision-free navigation rate in a dynamic multi-agent drone racing\nenvironment, underscoring its effectiveness and robustness in real-world\nscenarios. This work offers a promising foundation for safe, high-speed\nmulti-robot applications in logistics, inspection, and racing.\n", "link": "http://arxiv.org/abs/2503.07376v2", "date": "2025-10-22", "relevancy": 1.5227, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5104}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5077}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AttentionSwarm%3A%20Reinforcement%20Learning%20with%20Attention%20Control%20Barier%0A%20%20Function%20for%20Crazyflie%20Drones%20in%20Dynamic%20Environments&body=Title%3A%20AttentionSwarm%3A%20Reinforcement%20Learning%20with%20Attention%20Control%20Barier%0A%20%20Function%20for%20Crazyflie%20Drones%20in%20Dynamic%20Environments%0AAuthor%3A%20Grik%20Tadevosyan%20and%20Valerii%20Serpiva%20and%20Aleksey%20Fedoseev%20and%20Roohan%20Ahmed%20Khan%20and%20Demetros%20Aschu%20and%20Faryal%20Batool%20and%20Nickolay%20Efanov%20and%20Artem%20Mikhaylov%20and%20Dzmitry%20Tsetserukou%0AAbstract%3A%20%20%20We%20introduce%20AttentionSwarm%2C%20a%20novel%20benchmark%20designed%20to%20evaluate%20safe%20and%0Aefficient%20swarm%20control%20in%20a%20dynamic%20drone%20racing%20scenario.%20Central%20to%20our%0Aapproach%20is%20the%20Attention%20Model-Based%20Control%20Barrier%20Function%20%28CBF%29%20framework%2C%0Awhich%20integrates%20attention%20mechanisms%20with%20safety-critical%20control%20theory%20to%0Aenable%20real-time%20collision%20avoidance%20and%20trajectory%20optimization.%20This%0Aframework%20dynamically%20prioritizes%20critical%20obstacles%20and%20agents%20in%20the%20swarm%27s%0Avicinity%20using%20attention%20weights%2C%20while%20CBFs%20formally%20guarantee%20safety%20by%0Aenforcing%20collision-free%20constraints.%20The%20AttentionSwarm%20algorithm%20was%0Adeveloped%20and%20evaluated%20using%20a%20swarm%20of%20Crazyflie%202.1%20micro%20quadrotors%2C%20which%0Awere%20tested%20indoors%20with%20the%20Vicon%20motion%20capture%20system%20to%20ensure%20precise%0Alocalization%20and%20control.%20Experimental%20results%20show%20that%20our%20system%20achieves%20a%0A95-100%25%20collision-free%20navigation%20rate%20in%20a%20dynamic%20multi-agent%20drone%20racing%0Aenvironment%2C%20underscoring%20its%20effectiveness%20and%20robustness%20in%20real-world%0Ascenarios.%20This%20work%20offers%20a%20promising%20foundation%20for%20safe%2C%20high-speed%0Amulti-robot%20applications%20in%20logistics%2C%20inspection%2C%20and%20racing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.07376v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttentionSwarm%253A%2520Reinforcement%2520Learning%2520with%2520Attention%2520Control%2520Barier%250A%2520%2520Function%2520for%2520Crazyflie%2520Drones%2520in%2520Dynamic%2520Environments%26entry.906535625%3DGrik%2520Tadevosyan%2520and%2520Valerii%2520Serpiva%2520and%2520Aleksey%2520Fedoseev%2520and%2520Roohan%2520Ahmed%2520Khan%2520and%2520Demetros%2520Aschu%2520and%2520Faryal%2520Batool%2520and%2520Nickolay%2520Efanov%2520and%2520Artem%2520Mikhaylov%2520and%2520Dzmitry%2520Tsetserukou%26entry.1292438233%3D%2520%2520We%2520introduce%2520AttentionSwarm%252C%2520a%2520novel%2520benchmark%2520designed%2520to%2520evaluate%2520safe%2520and%250Aefficient%2520swarm%2520control%2520in%2520a%2520dynamic%2520drone%2520racing%2520scenario.%2520Central%2520to%2520our%250Aapproach%2520is%2520the%2520Attention%2520Model-Based%2520Control%2520Barrier%2520Function%2520%2528CBF%2529%2520framework%252C%250Awhich%2520integrates%2520attention%2520mechanisms%2520with%2520safety-critical%2520control%2520theory%2520to%250Aenable%2520real-time%2520collision%2520avoidance%2520and%2520trajectory%2520optimization.%2520This%250Aframework%2520dynamically%2520prioritizes%2520critical%2520obstacles%2520and%2520agents%2520in%2520the%2520swarm%2527s%250Avicinity%2520using%2520attention%2520weights%252C%2520while%2520CBFs%2520formally%2520guarantee%2520safety%2520by%250Aenforcing%2520collision-free%2520constraints.%2520The%2520AttentionSwarm%2520algorithm%2520was%250Adeveloped%2520and%2520evaluated%2520using%2520a%2520swarm%2520of%2520Crazyflie%25202.1%2520micro%2520quadrotors%252C%2520which%250Awere%2520tested%2520indoors%2520with%2520the%2520Vicon%2520motion%2520capture%2520system%2520to%2520ensure%2520precise%250Alocalization%2520and%2520control.%2520Experimental%2520results%2520show%2520that%2520our%2520system%2520achieves%2520a%250A95-100%2525%2520collision-free%2520navigation%2520rate%2520in%2520a%2520dynamic%2520multi-agent%2520drone%2520racing%250Aenvironment%252C%2520underscoring%2520its%2520effectiveness%2520and%2520robustness%2520in%2520real-world%250Ascenarios.%2520This%2520work%2520offers%2520a%2520promising%2520foundation%2520for%2520safe%252C%2520high-speed%250Amulti-robot%2520applications%2520in%2520logistics%252C%2520inspection%252C%2520and%2520racing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.07376v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AttentionSwarm%3A%20Reinforcement%20Learning%20with%20Attention%20Control%20Barier%0A%20%20Function%20for%20Crazyflie%20Drones%20in%20Dynamic%20Environments&entry.906535625=Grik%20Tadevosyan%20and%20Valerii%20Serpiva%20and%20Aleksey%20Fedoseev%20and%20Roohan%20Ahmed%20Khan%20and%20Demetros%20Aschu%20and%20Faryal%20Batool%20and%20Nickolay%20Efanov%20and%20Artem%20Mikhaylov%20and%20Dzmitry%20Tsetserukou&entry.1292438233=%20%20We%20introduce%20AttentionSwarm%2C%20a%20novel%20benchmark%20designed%20to%20evaluate%20safe%20and%0Aefficient%20swarm%20control%20in%20a%20dynamic%20drone%20racing%20scenario.%20Central%20to%20our%0Aapproach%20is%20the%20Attention%20Model-Based%20Control%20Barrier%20Function%20%28CBF%29%20framework%2C%0Awhich%20integrates%20attention%20mechanisms%20with%20safety-critical%20control%20theory%20to%0Aenable%20real-time%20collision%20avoidance%20and%20trajectory%20optimization.%20This%0Aframework%20dynamically%20prioritizes%20critical%20obstacles%20and%20agents%20in%20the%20swarm%27s%0Avicinity%20using%20attention%20weights%2C%20while%20CBFs%20formally%20guarantee%20safety%20by%0Aenforcing%20collision-free%20constraints.%20The%20AttentionSwarm%20algorithm%20was%0Adeveloped%20and%20evaluated%20using%20a%20swarm%20of%20Crazyflie%202.1%20micro%20quadrotors%2C%20which%0Awere%20tested%20indoors%20with%20the%20Vicon%20motion%20capture%20system%20to%20ensure%20precise%0Alocalization%20and%20control.%20Experimental%20results%20show%20that%20our%20system%20achieves%20a%0A95-100%25%20collision-free%20navigation%20rate%20in%20a%20dynamic%20multi-agent%20drone%20racing%0Aenvironment%2C%20underscoring%20its%20effectiveness%20and%20robustness%20in%20real-world%0Ascenarios.%20This%20work%20offers%20a%20promising%20foundation%20for%20safe%2C%20high-speed%0Amulti-robot%20applications%20in%20logistics%2C%20inspection%2C%20and%20racing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.07376v2&entry.124074799=Read"},
{"title": "AgentSense: LLMs Empower Generalizable and Explainable Web-Based\n  Participatory Urban Sensing", "author": "Xusen Guo and Mingxing Peng and Xixuan Hao and Xingchen Zou and Qiongyan Wang and Sijie Ruan and Yuxuan Liang", "abstract": "  Web-based participatory urban sensing has emerged as a vital approach for\nmodern urban management by leveraging mobile individuals as distributed\nsensors. However, existing urban sensing systems struggle with limited\ngeneralization across diverse urban scenarios and poor interpretability in\ndecision-making. In this work, we introduce AgentSense, a hybrid, training-free\nframework that integrates large language models (LLMs) into participatory urban\nsensing through a multi-agent evolution system. AgentSense initially employs\nclassical planner to generate baseline solutions and then iteratively refines\nthem to adapt sensing task assignments to dynamic urban conditions and\nheterogeneous worker preferences, while producing natural language explanations\nthat enhance transparency and trust. Extensive experiments across two\nlarge-scale mobility datasets and seven types of dynamic disturbances\ndemonstrate that AgentSense offers distinct advantages in adaptivity and\nexplainability over traditional methods. Furthermore, compared to single-agent\nLLM baselines, our approach outperforms in both performance and robustness,\nwhile delivering more reasonable and transparent explanations. These results\nposition AgentSense as a significant advancement towards deploying adaptive and\nexplainable urban sensing systems on the web.\n", "link": "http://arxiv.org/abs/2510.19661v1", "date": "2025-10-22", "relevancy": 1.7288, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5992}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5734}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AgentSense%3A%20LLMs%20Empower%20Generalizable%20and%20Explainable%20Web-Based%0A%20%20Participatory%20Urban%20Sensing&body=Title%3A%20AgentSense%3A%20LLMs%20Empower%20Generalizable%20and%20Explainable%20Web-Based%0A%20%20Participatory%20Urban%20Sensing%0AAuthor%3A%20Xusen%20Guo%20and%20Mingxing%20Peng%20and%20Xixuan%20Hao%20and%20Xingchen%20Zou%20and%20Qiongyan%20Wang%20and%20Sijie%20Ruan%20and%20Yuxuan%20Liang%0AAbstract%3A%20%20%20Web-based%20participatory%20urban%20sensing%20has%20emerged%20as%20a%20vital%20approach%20for%0Amodern%20urban%20management%20by%20leveraging%20mobile%20individuals%20as%20distributed%0Asensors.%20However%2C%20existing%20urban%20sensing%20systems%20struggle%20with%20limited%0Ageneralization%20across%20diverse%20urban%20scenarios%20and%20poor%20interpretability%20in%0Adecision-making.%20In%20this%20work%2C%20we%20introduce%20AgentSense%2C%20a%20hybrid%2C%20training-free%0Aframework%20that%20integrates%20large%20language%20models%20%28LLMs%29%20into%20participatory%20urban%0Asensing%20through%20a%20multi-agent%20evolution%20system.%20AgentSense%20initially%20employs%0Aclassical%20planner%20to%20generate%20baseline%20solutions%20and%20then%20iteratively%20refines%0Athem%20to%20adapt%20sensing%20task%20assignments%20to%20dynamic%20urban%20conditions%20and%0Aheterogeneous%20worker%20preferences%2C%20while%20producing%20natural%20language%20explanations%0Athat%20enhance%20transparency%20and%20trust.%20Extensive%20experiments%20across%20two%0Alarge-scale%20mobility%20datasets%20and%20seven%20types%20of%20dynamic%20disturbances%0Ademonstrate%20that%20AgentSense%20offers%20distinct%20advantages%20in%20adaptivity%20and%0Aexplainability%20over%20traditional%20methods.%20Furthermore%2C%20compared%20to%20single-agent%0ALLM%20baselines%2C%20our%20approach%20outperforms%20in%20both%20performance%20and%20robustness%2C%0Awhile%20delivering%20more%20reasonable%20and%20transparent%20explanations.%20These%20results%0Aposition%20AgentSense%20as%20a%20significant%20advancement%20towards%20deploying%20adaptive%20and%0Aexplainable%20urban%20sensing%20systems%20on%20the%20web.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19661v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentSense%253A%2520LLMs%2520Empower%2520Generalizable%2520and%2520Explainable%2520Web-Based%250A%2520%2520Participatory%2520Urban%2520Sensing%26entry.906535625%3DXusen%2520Guo%2520and%2520Mingxing%2520Peng%2520and%2520Xixuan%2520Hao%2520and%2520Xingchen%2520Zou%2520and%2520Qiongyan%2520Wang%2520and%2520Sijie%2520Ruan%2520and%2520Yuxuan%2520Liang%26entry.1292438233%3D%2520%2520Web-based%2520participatory%2520urban%2520sensing%2520has%2520emerged%2520as%2520a%2520vital%2520approach%2520for%250Amodern%2520urban%2520management%2520by%2520leveraging%2520mobile%2520individuals%2520as%2520distributed%250Asensors.%2520However%252C%2520existing%2520urban%2520sensing%2520systems%2520struggle%2520with%2520limited%250Ageneralization%2520across%2520diverse%2520urban%2520scenarios%2520and%2520poor%2520interpretability%2520in%250Adecision-making.%2520In%2520this%2520work%252C%2520we%2520introduce%2520AgentSense%252C%2520a%2520hybrid%252C%2520training-free%250Aframework%2520that%2520integrates%2520large%2520language%2520models%2520%2528LLMs%2529%2520into%2520participatory%2520urban%250Asensing%2520through%2520a%2520multi-agent%2520evolution%2520system.%2520AgentSense%2520initially%2520employs%250Aclassical%2520planner%2520to%2520generate%2520baseline%2520solutions%2520and%2520then%2520iteratively%2520refines%250Athem%2520to%2520adapt%2520sensing%2520task%2520assignments%2520to%2520dynamic%2520urban%2520conditions%2520and%250Aheterogeneous%2520worker%2520preferences%252C%2520while%2520producing%2520natural%2520language%2520explanations%250Athat%2520enhance%2520transparency%2520and%2520trust.%2520Extensive%2520experiments%2520across%2520two%250Alarge-scale%2520mobility%2520datasets%2520and%2520seven%2520types%2520of%2520dynamic%2520disturbances%250Ademonstrate%2520that%2520AgentSense%2520offers%2520distinct%2520advantages%2520in%2520adaptivity%2520and%250Aexplainability%2520over%2520traditional%2520methods.%2520Furthermore%252C%2520compared%2520to%2520single-agent%250ALLM%2520baselines%252C%2520our%2520approach%2520outperforms%2520in%2520both%2520performance%2520and%2520robustness%252C%250Awhile%2520delivering%2520more%2520reasonable%2520and%2520transparent%2520explanations.%2520These%2520results%250Aposition%2520AgentSense%2520as%2520a%2520significant%2520advancement%2520towards%2520deploying%2520adaptive%2520and%250Aexplainable%2520urban%2520sensing%2520systems%2520on%2520the%2520web.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19661v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AgentSense%3A%20LLMs%20Empower%20Generalizable%20and%20Explainable%20Web-Based%0A%20%20Participatory%20Urban%20Sensing&entry.906535625=Xusen%20Guo%20and%20Mingxing%20Peng%20and%20Xixuan%20Hao%20and%20Xingchen%20Zou%20and%20Qiongyan%20Wang%20and%20Sijie%20Ruan%20and%20Yuxuan%20Liang&entry.1292438233=%20%20Web-based%20participatory%20urban%20sensing%20has%20emerged%20as%20a%20vital%20approach%20for%0Amodern%20urban%20management%20by%20leveraging%20mobile%20individuals%20as%20distributed%0Asensors.%20However%2C%20existing%20urban%20sensing%20systems%20struggle%20with%20limited%0Ageneralization%20across%20diverse%20urban%20scenarios%20and%20poor%20interpretability%20in%0Adecision-making.%20In%20this%20work%2C%20we%20introduce%20AgentSense%2C%20a%20hybrid%2C%20training-free%0Aframework%20that%20integrates%20large%20language%20models%20%28LLMs%29%20into%20participatory%20urban%0Asensing%20through%20a%20multi-agent%20evolution%20system.%20AgentSense%20initially%20employs%0Aclassical%20planner%20to%20generate%20baseline%20solutions%20and%20then%20iteratively%20refines%0Athem%20to%20adapt%20sensing%20task%20assignments%20to%20dynamic%20urban%20conditions%20and%0Aheterogeneous%20worker%20preferences%2C%20while%20producing%20natural%20language%20explanations%0Athat%20enhance%20transparency%20and%20trust.%20Extensive%20experiments%20across%20two%0Alarge-scale%20mobility%20datasets%20and%20seven%20types%20of%20dynamic%20disturbances%0Ademonstrate%20that%20AgentSense%20offers%20distinct%20advantages%20in%20adaptivity%20and%0Aexplainability%20over%20traditional%20methods.%20Furthermore%2C%20compared%20to%20single-agent%0ALLM%20baselines%2C%20our%20approach%20outperforms%20in%20both%20performance%20and%20robustness%2C%0Awhile%20delivering%20more%20reasonable%20and%20transparent%20explanations.%20These%20results%0Aposition%20AgentSense%20as%20a%20significant%20advancement%20towards%20deploying%20adaptive%20and%0Aexplainable%20urban%20sensing%20systems%20on%20the%20web.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19661v1&entry.124074799=Read"},
{"title": "Bootstrap Sampling Rate Greater than 1.0 May Improve Random Forest\n  Performance", "author": "Stanis\u0142aw Ka\u017amierczak and Jacek Ma\u0144dziuk", "abstract": "  Random forests (RFs) utilize bootstrap sampling to generate individual\ntraining sets for each component tree by sampling with replacement, with the\nsample size typically equal to that of the original training set ($N$).\nPrevious research indicates that drawing fewer than $N$ observations can also\nyield satisfactory results. The ratio of the number of observations in each\nbootstrap sample to the total number of training instances is referred to as\nthe bootstrap rate (BR). Sampling more than $N$ observations (BR $>$ 1.0) has\nbeen explored only to a limited extent and has generally been considered\nineffective. In this paper, we revisit this setup using 36 diverse datasets,\nevaluating BR values ranging from 1.2 to 5.0. Contrary to previous findings, we\nshow that higher BR values can lead to statistically significant improvements\nin classification accuracy compared to standard settings (BR $\\leq$ 1.0).\nFurthermore, we analyze how BR affects the leaf structure of decision trees\nwithin the RF and investigate factors influencing the optimal BR. Our results\nindicate that the optimal BR is primarily determined by the characteristics of\nthe data set rather than the RF hyperparameters.\n", "link": "http://arxiv.org/abs/2410.04297v2", "date": "2025-10-22", "relevancy": 1.7567, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4638}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4352}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bootstrap%20Sampling%20Rate%20Greater%20than%201.0%20May%20Improve%20Random%20Forest%0A%20%20Performance&body=Title%3A%20Bootstrap%20Sampling%20Rate%20Greater%20than%201.0%20May%20Improve%20Random%20Forest%0A%20%20Performance%0AAuthor%3A%20Stanis%C5%82aw%20Ka%C5%BAmierczak%20and%20Jacek%20Ma%C5%84dziuk%0AAbstract%3A%20%20%20Random%20forests%20%28RFs%29%20utilize%20bootstrap%20sampling%20to%20generate%20individual%0Atraining%20sets%20for%20each%20component%20tree%20by%20sampling%20with%20replacement%2C%20with%20the%0Asample%20size%20typically%20equal%20to%20that%20of%20the%20original%20training%20set%20%28%24N%24%29.%0APrevious%20research%20indicates%20that%20drawing%20fewer%20than%20%24N%24%20observations%20can%20also%0Ayield%20satisfactory%20results.%20The%20ratio%20of%20the%20number%20of%20observations%20in%20each%0Abootstrap%20sample%20to%20the%20total%20number%20of%20training%20instances%20is%20referred%20to%20as%0Athe%20bootstrap%20rate%20%28BR%29.%20Sampling%20more%20than%20%24N%24%20observations%20%28BR%20%24%3E%24%201.0%29%20has%0Abeen%20explored%20only%20to%20a%20limited%20extent%20and%20has%20generally%20been%20considered%0Aineffective.%20In%20this%20paper%2C%20we%20revisit%20this%20setup%20using%2036%20diverse%20datasets%2C%0Aevaluating%20BR%20values%20ranging%20from%201.2%20to%205.0.%20Contrary%20to%20previous%20findings%2C%20we%0Ashow%20that%20higher%20BR%20values%20can%20lead%20to%20statistically%20significant%20improvements%0Ain%20classification%20accuracy%20compared%20to%20standard%20settings%20%28BR%20%24%5Cleq%24%201.0%29.%0AFurthermore%2C%20we%20analyze%20how%20BR%20affects%20the%20leaf%20structure%20of%20decision%20trees%0Awithin%20the%20RF%20and%20investigate%20factors%20influencing%20the%20optimal%20BR.%20Our%20results%0Aindicate%20that%20the%20optimal%20BR%20is%20primarily%20determined%20by%20the%20characteristics%20of%0Athe%20data%20set%20rather%20than%20the%20RF%20hyperparameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.04297v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBootstrap%2520Sampling%2520Rate%2520Greater%2520than%25201.0%2520May%2520Improve%2520Random%2520Forest%250A%2520%2520Performance%26entry.906535625%3DStanis%25C5%2582aw%2520Ka%25C5%25BAmierczak%2520and%2520Jacek%2520Ma%25C5%2584dziuk%26entry.1292438233%3D%2520%2520Random%2520forests%2520%2528RFs%2529%2520utilize%2520bootstrap%2520sampling%2520to%2520generate%2520individual%250Atraining%2520sets%2520for%2520each%2520component%2520tree%2520by%2520sampling%2520with%2520replacement%252C%2520with%2520the%250Asample%2520size%2520typically%2520equal%2520to%2520that%2520of%2520the%2520original%2520training%2520set%2520%2528%2524N%2524%2529.%250APrevious%2520research%2520indicates%2520that%2520drawing%2520fewer%2520than%2520%2524N%2524%2520observations%2520can%2520also%250Ayield%2520satisfactory%2520results.%2520The%2520ratio%2520of%2520the%2520number%2520of%2520observations%2520in%2520each%250Abootstrap%2520sample%2520to%2520the%2520total%2520number%2520of%2520training%2520instances%2520is%2520referred%2520to%2520as%250Athe%2520bootstrap%2520rate%2520%2528BR%2529.%2520Sampling%2520more%2520than%2520%2524N%2524%2520observations%2520%2528BR%2520%2524%253E%2524%25201.0%2529%2520has%250Abeen%2520explored%2520only%2520to%2520a%2520limited%2520extent%2520and%2520has%2520generally%2520been%2520considered%250Aineffective.%2520In%2520this%2520paper%252C%2520we%2520revisit%2520this%2520setup%2520using%252036%2520diverse%2520datasets%252C%250Aevaluating%2520BR%2520values%2520ranging%2520from%25201.2%2520to%25205.0.%2520Contrary%2520to%2520previous%2520findings%252C%2520we%250Ashow%2520that%2520higher%2520BR%2520values%2520can%2520lead%2520to%2520statistically%2520significant%2520improvements%250Ain%2520classification%2520accuracy%2520compared%2520to%2520standard%2520settings%2520%2528BR%2520%2524%255Cleq%2524%25201.0%2529.%250AFurthermore%252C%2520we%2520analyze%2520how%2520BR%2520affects%2520the%2520leaf%2520structure%2520of%2520decision%2520trees%250Awithin%2520the%2520RF%2520and%2520investigate%2520factors%2520influencing%2520the%2520optimal%2520BR.%2520Our%2520results%250Aindicate%2520that%2520the%2520optimal%2520BR%2520is%2520primarily%2520determined%2520by%2520the%2520characteristics%2520of%250Athe%2520data%2520set%2520rather%2520than%2520the%2520RF%2520hyperparameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.04297v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bootstrap%20Sampling%20Rate%20Greater%20than%201.0%20May%20Improve%20Random%20Forest%0A%20%20Performance&entry.906535625=Stanis%C5%82aw%20Ka%C5%BAmierczak%20and%20Jacek%20Ma%C5%84dziuk&entry.1292438233=%20%20Random%20forests%20%28RFs%29%20utilize%20bootstrap%20sampling%20to%20generate%20individual%0Atraining%20sets%20for%20each%20component%20tree%20by%20sampling%20with%20replacement%2C%20with%20the%0Asample%20size%20typically%20equal%20to%20that%20of%20the%20original%20training%20set%20%28%24N%24%29.%0APrevious%20research%20indicates%20that%20drawing%20fewer%20than%20%24N%24%20observations%20can%20also%0Ayield%20satisfactory%20results.%20The%20ratio%20of%20the%20number%20of%20observations%20in%20each%0Abootstrap%20sample%20to%20the%20total%20number%20of%20training%20instances%20is%20referred%20to%20as%0Athe%20bootstrap%20rate%20%28BR%29.%20Sampling%20more%20than%20%24N%24%20observations%20%28BR%20%24%3E%24%201.0%29%20has%0Abeen%20explored%20only%20to%20a%20limited%20extent%20and%20has%20generally%20been%20considered%0Aineffective.%20In%20this%20paper%2C%20we%20revisit%20this%20setup%20using%2036%20diverse%20datasets%2C%0Aevaluating%20BR%20values%20ranging%20from%201.2%20to%205.0.%20Contrary%20to%20previous%20findings%2C%20we%0Ashow%20that%20higher%20BR%20values%20can%20lead%20to%20statistically%20significant%20improvements%0Ain%20classification%20accuracy%20compared%20to%20standard%20settings%20%28BR%20%24%5Cleq%24%201.0%29.%0AFurthermore%2C%20we%20analyze%20how%20BR%20affects%20the%20leaf%20structure%20of%20decision%20trees%0Awithin%20the%20RF%20and%20investigate%20factors%20influencing%20the%20optimal%20BR.%20Our%20results%0Aindicate%20that%20the%20optimal%20BR%20is%20primarily%20determined%20by%20the%20characteristics%20of%0Athe%20data%20set%20rather%20than%20the%20RF%20hyperparameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.04297v2&entry.124074799=Read"},
{"title": "Overlap-weighted orthogonal meta-learner for treatment effect estimation\n  over time", "author": "Konstantin Hess and Dennis Frauen and Mihaela van der Schaar and Stefan Feuerriegel", "abstract": "  Estimating heterogeneous treatment effects (HTEs) in time-varying settings is\nparticularly challenging, as the probability of observing certain treatment\nsequences decreases exponentially with longer prediction horizons. Thus, the\nobserved data contain little support for many plausible treatment sequences,\nwhich creates severe overlap problems. Existing meta-learners for the\ntime-varying setting typically assume adequate treatment overlap, and thus\nsuffer from exploding estimation variance when the overlap is low. To address\nthis problem, we introduce a novel overlap-weighted orthogonal (WO)\nmeta-learner for estimating HTEs that targets regions in the observed data with\nhigh probability of receiving the interventional treatment sequences. This\noffers a fully data-driven approach through which our WO-learner can counteract\ninstabilities as in existing meta-learners and thus obtain more reliable HTE\nestimates. Methodologically, we develop a novel Neyman-orthogonal population\nrisk function that minimizes the overlap-weighted oracle risk. We show that our\nWO-learner has the favorable property of Neyman-orthogonality, meaning that it\nis robust against misspecification in the nuisance functions. Further, our\nWO-learner is fully model-agnostic and can be applied to any machine learning\nmodel. Through extensive experiments with both transformer and LSTM backbones,\nwe demonstrate the benefits of our novel WO-learner.\n", "link": "http://arxiv.org/abs/2510.19643v1", "date": "2025-10-22", "relevancy": 1.7483, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4423}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4394}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4309}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Overlap-weighted%20orthogonal%20meta-learner%20for%20treatment%20effect%20estimation%0A%20%20over%20time&body=Title%3A%20Overlap-weighted%20orthogonal%20meta-learner%20for%20treatment%20effect%20estimation%0A%20%20over%20time%0AAuthor%3A%20Konstantin%20Hess%20and%20Dennis%20Frauen%20and%20Mihaela%20van%20der%20Schaar%20and%20Stefan%20Feuerriegel%0AAbstract%3A%20%20%20Estimating%20heterogeneous%20treatment%20effects%20%28HTEs%29%20in%20time-varying%20settings%20is%0Aparticularly%20challenging%2C%20as%20the%20probability%20of%20observing%20certain%20treatment%0Asequences%20decreases%20exponentially%20with%20longer%20prediction%20horizons.%20Thus%2C%20the%0Aobserved%20data%20contain%20little%20support%20for%20many%20plausible%20treatment%20sequences%2C%0Awhich%20creates%20severe%20overlap%20problems.%20Existing%20meta-learners%20for%20the%0Atime-varying%20setting%20typically%20assume%20adequate%20treatment%20overlap%2C%20and%20thus%0Asuffer%20from%20exploding%20estimation%20variance%20when%20the%20overlap%20is%20low.%20To%20address%0Athis%20problem%2C%20we%20introduce%20a%20novel%20overlap-weighted%20orthogonal%20%28WO%29%0Ameta-learner%20for%20estimating%20HTEs%20that%20targets%20regions%20in%20the%20observed%20data%20with%0Ahigh%20probability%20of%20receiving%20the%20interventional%20treatment%20sequences.%20This%0Aoffers%20a%20fully%20data-driven%20approach%20through%20which%20our%20WO-learner%20can%20counteract%0Ainstabilities%20as%20in%20existing%20meta-learners%20and%20thus%20obtain%20more%20reliable%20HTE%0Aestimates.%20Methodologically%2C%20we%20develop%20a%20novel%20Neyman-orthogonal%20population%0Arisk%20function%20that%20minimizes%20the%20overlap-weighted%20oracle%20risk.%20We%20show%20that%20our%0AWO-learner%20has%20the%20favorable%20property%20of%20Neyman-orthogonality%2C%20meaning%20that%20it%0Ais%20robust%20against%20misspecification%20in%20the%20nuisance%20functions.%20Further%2C%20our%0AWO-learner%20is%20fully%20model-agnostic%20and%20can%20be%20applied%20to%20any%20machine%20learning%0Amodel.%20Through%20extensive%20experiments%20with%20both%20transformer%20and%20LSTM%20backbones%2C%0Awe%20demonstrate%20the%20benefits%20of%20our%20novel%20WO-learner.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19643v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOverlap-weighted%2520orthogonal%2520meta-learner%2520for%2520treatment%2520effect%2520estimation%250A%2520%2520over%2520time%26entry.906535625%3DKonstantin%2520Hess%2520and%2520Dennis%2520Frauen%2520and%2520Mihaela%2520van%2520der%2520Schaar%2520and%2520Stefan%2520Feuerriegel%26entry.1292438233%3D%2520%2520Estimating%2520heterogeneous%2520treatment%2520effects%2520%2528HTEs%2529%2520in%2520time-varying%2520settings%2520is%250Aparticularly%2520challenging%252C%2520as%2520the%2520probability%2520of%2520observing%2520certain%2520treatment%250Asequences%2520decreases%2520exponentially%2520with%2520longer%2520prediction%2520horizons.%2520Thus%252C%2520the%250Aobserved%2520data%2520contain%2520little%2520support%2520for%2520many%2520plausible%2520treatment%2520sequences%252C%250Awhich%2520creates%2520severe%2520overlap%2520problems.%2520Existing%2520meta-learners%2520for%2520the%250Atime-varying%2520setting%2520typically%2520assume%2520adequate%2520treatment%2520overlap%252C%2520and%2520thus%250Asuffer%2520from%2520exploding%2520estimation%2520variance%2520when%2520the%2520overlap%2520is%2520low.%2520To%2520address%250Athis%2520problem%252C%2520we%2520introduce%2520a%2520novel%2520overlap-weighted%2520orthogonal%2520%2528WO%2529%250Ameta-learner%2520for%2520estimating%2520HTEs%2520that%2520targets%2520regions%2520in%2520the%2520observed%2520data%2520with%250Ahigh%2520probability%2520of%2520receiving%2520the%2520interventional%2520treatment%2520sequences.%2520This%250Aoffers%2520a%2520fully%2520data-driven%2520approach%2520through%2520which%2520our%2520WO-learner%2520can%2520counteract%250Ainstabilities%2520as%2520in%2520existing%2520meta-learners%2520and%2520thus%2520obtain%2520more%2520reliable%2520HTE%250Aestimates.%2520Methodologically%252C%2520we%2520develop%2520a%2520novel%2520Neyman-orthogonal%2520population%250Arisk%2520function%2520that%2520minimizes%2520the%2520overlap-weighted%2520oracle%2520risk.%2520We%2520show%2520that%2520our%250AWO-learner%2520has%2520the%2520favorable%2520property%2520of%2520Neyman-orthogonality%252C%2520meaning%2520that%2520it%250Ais%2520robust%2520against%2520misspecification%2520in%2520the%2520nuisance%2520functions.%2520Further%252C%2520our%250AWO-learner%2520is%2520fully%2520model-agnostic%2520and%2520can%2520be%2520applied%2520to%2520any%2520machine%2520learning%250Amodel.%2520Through%2520extensive%2520experiments%2520with%2520both%2520transformer%2520and%2520LSTM%2520backbones%252C%250Awe%2520demonstrate%2520the%2520benefits%2520of%2520our%2520novel%2520WO-learner.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19643v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Overlap-weighted%20orthogonal%20meta-learner%20for%20treatment%20effect%20estimation%0A%20%20over%20time&entry.906535625=Konstantin%20Hess%20and%20Dennis%20Frauen%20and%20Mihaela%20van%20der%20Schaar%20and%20Stefan%20Feuerriegel&entry.1292438233=%20%20Estimating%20heterogeneous%20treatment%20effects%20%28HTEs%29%20in%20time-varying%20settings%20is%0Aparticularly%20challenging%2C%20as%20the%20probability%20of%20observing%20certain%20treatment%0Asequences%20decreases%20exponentially%20with%20longer%20prediction%20horizons.%20Thus%2C%20the%0Aobserved%20data%20contain%20little%20support%20for%20many%20plausible%20treatment%20sequences%2C%0Awhich%20creates%20severe%20overlap%20problems.%20Existing%20meta-learners%20for%20the%0Atime-varying%20setting%20typically%20assume%20adequate%20treatment%20overlap%2C%20and%20thus%0Asuffer%20from%20exploding%20estimation%20variance%20when%20the%20overlap%20is%20low.%20To%20address%0Athis%20problem%2C%20we%20introduce%20a%20novel%20overlap-weighted%20orthogonal%20%28WO%29%0Ameta-learner%20for%20estimating%20HTEs%20that%20targets%20regions%20in%20the%20observed%20data%20with%0Ahigh%20probability%20of%20receiving%20the%20interventional%20treatment%20sequences.%20This%0Aoffers%20a%20fully%20data-driven%20approach%20through%20which%20our%20WO-learner%20can%20counteract%0Ainstabilities%20as%20in%20existing%20meta-learners%20and%20thus%20obtain%20more%20reliable%20HTE%0Aestimates.%20Methodologically%2C%20we%20develop%20a%20novel%20Neyman-orthogonal%20population%0Arisk%20function%20that%20minimizes%20the%20overlap-weighted%20oracle%20risk.%20We%20show%20that%20our%0AWO-learner%20has%20the%20favorable%20property%20of%20Neyman-orthogonality%2C%20meaning%20that%20it%0Ais%20robust%20against%20misspecification%20in%20the%20nuisance%20functions.%20Further%2C%20our%0AWO-learner%20is%20fully%20model-agnostic%20and%20can%20be%20applied%20to%20any%20machine%20learning%0Amodel.%20Through%20extensive%20experiments%20with%20both%20transformer%20and%20LSTM%20backbones%2C%0Awe%20demonstrate%20the%20benefits%20of%20our%20novel%20WO-learner.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19643v1&entry.124074799=Read"},
{"title": "SparseWorld: A Flexible, Adaptive, and Efficient 4D Occupancy World\n  Model Powered by Sparse and Dynamic Queries", "author": "Chenxu Dang and Haiyan Liu and Guangjun Bao and Pei An and Xinyue Tang and An Pan and Jie Ma and Bingchuan Sun and Yan Wang", "abstract": "  Semantic occupancy has emerged as a powerful representation in world models\nfor its ability to capture rich spatial semantics. However, most existing\noccupancy world models rely on static and fixed embeddings or grids, which\ninherently limit the flexibility of perception. Moreover, their \"in-place\nclassification\" over grids exhibits a potential misalignment with the dynamic\nand continuous nature of real scenarios.In this paper, we propose SparseWorld,\na novel 4D occupancy world model that is flexible, adaptive, and efficient,\npowered by sparse and dynamic queries. We propose a Range-Adaptive Perception\nmodule, in which learnable queries are modulated by the ego vehicle states and\nenriched with temporal-spatial associations to enable extended-range\nperception. To effectively capture the dynamics of the scene, we design a\nState-Conditioned Forecasting module, which replaces classification-based\nforecasting with regression-guided formulation, precisely aligning the dynamic\nqueries with the continuity of the 4D environment. In addition, We specifically\ndevise a Temporal-Aware Self-Scheduling training strategy to enable smooth and\nefficient training. Extensive experiments demonstrate that SparseWorld achieves\nstate-of-the-art performance across perception, forecasting, and planning\ntasks. Comprehensive visualizations and ablation studies further validate the\nadvantages of SparseWorld in terms of flexibility, adaptability, and\nefficiency. The code is available at https://github.com/MSunDYY/SparseWorld.\n", "link": "http://arxiv.org/abs/2510.17482v2", "date": "2025-10-22", "relevancy": 1.6887, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6109}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5688}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SparseWorld%3A%20A%20Flexible%2C%20Adaptive%2C%20and%20Efficient%204D%20Occupancy%20World%0A%20%20Model%20Powered%20by%20Sparse%20and%20Dynamic%20Queries&body=Title%3A%20SparseWorld%3A%20A%20Flexible%2C%20Adaptive%2C%20and%20Efficient%204D%20Occupancy%20World%0A%20%20Model%20Powered%20by%20Sparse%20and%20Dynamic%20Queries%0AAuthor%3A%20Chenxu%20Dang%20and%20Haiyan%20Liu%20and%20Guangjun%20Bao%20and%20Pei%20An%20and%20Xinyue%20Tang%20and%20An%20Pan%20and%20Jie%20Ma%20and%20Bingchuan%20Sun%20and%20Yan%20Wang%0AAbstract%3A%20%20%20Semantic%20occupancy%20has%20emerged%20as%20a%20powerful%20representation%20in%20world%20models%0Afor%20its%20ability%20to%20capture%20rich%20spatial%20semantics.%20However%2C%20most%20existing%0Aoccupancy%20world%20models%20rely%20on%20static%20and%20fixed%20embeddings%20or%20grids%2C%20which%0Ainherently%20limit%20the%20flexibility%20of%20perception.%20Moreover%2C%20their%20%22in-place%0Aclassification%22%20over%20grids%20exhibits%20a%20potential%20misalignment%20with%20the%20dynamic%0Aand%20continuous%20nature%20of%20real%20scenarios.In%20this%20paper%2C%20we%20propose%20SparseWorld%2C%0Aa%20novel%204D%20occupancy%20world%20model%20that%20is%20flexible%2C%20adaptive%2C%20and%20efficient%2C%0Apowered%20by%20sparse%20and%20dynamic%20queries.%20We%20propose%20a%20Range-Adaptive%20Perception%0Amodule%2C%20in%20which%20learnable%20queries%20are%20modulated%20by%20the%20ego%20vehicle%20states%20and%0Aenriched%20with%20temporal-spatial%20associations%20to%20enable%20extended-range%0Aperception.%20To%20effectively%20capture%20the%20dynamics%20of%20the%20scene%2C%20we%20design%20a%0AState-Conditioned%20Forecasting%20module%2C%20which%20replaces%20classification-based%0Aforecasting%20with%20regression-guided%20formulation%2C%20precisely%20aligning%20the%20dynamic%0Aqueries%20with%20the%20continuity%20of%20the%204D%20environment.%20In%20addition%2C%20We%20specifically%0Adevise%20a%20Temporal-Aware%20Self-Scheduling%20training%20strategy%20to%20enable%20smooth%20and%0Aefficient%20training.%20Extensive%20experiments%20demonstrate%20that%20SparseWorld%20achieves%0Astate-of-the-art%20performance%20across%20perception%2C%20forecasting%2C%20and%20planning%0Atasks.%20Comprehensive%20visualizations%20and%20ablation%20studies%20further%20validate%20the%0Aadvantages%20of%20SparseWorld%20in%20terms%20of%20flexibility%2C%20adaptability%2C%20and%0Aefficiency.%20The%20code%20is%20available%20at%20https%3A//github.com/MSunDYY/SparseWorld.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17482v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparseWorld%253A%2520A%2520Flexible%252C%2520Adaptive%252C%2520and%2520Efficient%25204D%2520Occupancy%2520World%250A%2520%2520Model%2520Powered%2520by%2520Sparse%2520and%2520Dynamic%2520Queries%26entry.906535625%3DChenxu%2520Dang%2520and%2520Haiyan%2520Liu%2520and%2520Guangjun%2520Bao%2520and%2520Pei%2520An%2520and%2520Xinyue%2520Tang%2520and%2520An%2520Pan%2520and%2520Jie%2520Ma%2520and%2520Bingchuan%2520Sun%2520and%2520Yan%2520Wang%26entry.1292438233%3D%2520%2520Semantic%2520occupancy%2520has%2520emerged%2520as%2520a%2520powerful%2520representation%2520in%2520world%2520models%250Afor%2520its%2520ability%2520to%2520capture%2520rich%2520spatial%2520semantics.%2520However%252C%2520most%2520existing%250Aoccupancy%2520world%2520models%2520rely%2520on%2520static%2520and%2520fixed%2520embeddings%2520or%2520grids%252C%2520which%250Ainherently%2520limit%2520the%2520flexibility%2520of%2520perception.%2520Moreover%252C%2520their%2520%2522in-place%250Aclassification%2522%2520over%2520grids%2520exhibits%2520a%2520potential%2520misalignment%2520with%2520the%2520dynamic%250Aand%2520continuous%2520nature%2520of%2520real%2520scenarios.In%2520this%2520paper%252C%2520we%2520propose%2520SparseWorld%252C%250Aa%2520novel%25204D%2520occupancy%2520world%2520model%2520that%2520is%2520flexible%252C%2520adaptive%252C%2520and%2520efficient%252C%250Apowered%2520by%2520sparse%2520and%2520dynamic%2520queries.%2520We%2520propose%2520a%2520Range-Adaptive%2520Perception%250Amodule%252C%2520in%2520which%2520learnable%2520queries%2520are%2520modulated%2520by%2520the%2520ego%2520vehicle%2520states%2520and%250Aenriched%2520with%2520temporal-spatial%2520associations%2520to%2520enable%2520extended-range%250Aperception.%2520To%2520effectively%2520capture%2520the%2520dynamics%2520of%2520the%2520scene%252C%2520we%2520design%2520a%250AState-Conditioned%2520Forecasting%2520module%252C%2520which%2520replaces%2520classification-based%250Aforecasting%2520with%2520regression-guided%2520formulation%252C%2520precisely%2520aligning%2520the%2520dynamic%250Aqueries%2520with%2520the%2520continuity%2520of%2520the%25204D%2520environment.%2520In%2520addition%252C%2520We%2520specifically%250Adevise%2520a%2520Temporal-Aware%2520Self-Scheduling%2520training%2520strategy%2520to%2520enable%2520smooth%2520and%250Aefficient%2520training.%2520Extensive%2520experiments%2520demonstrate%2520that%2520SparseWorld%2520achieves%250Astate-of-the-art%2520performance%2520across%2520perception%252C%2520forecasting%252C%2520and%2520planning%250Atasks.%2520Comprehensive%2520visualizations%2520and%2520ablation%2520studies%2520further%2520validate%2520the%250Aadvantages%2520of%2520SparseWorld%2520in%2520terms%2520of%2520flexibility%252C%2520adaptability%252C%2520and%250Aefficiency.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/MSunDYY/SparseWorld.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17482v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SparseWorld%3A%20A%20Flexible%2C%20Adaptive%2C%20and%20Efficient%204D%20Occupancy%20World%0A%20%20Model%20Powered%20by%20Sparse%20and%20Dynamic%20Queries&entry.906535625=Chenxu%20Dang%20and%20Haiyan%20Liu%20and%20Guangjun%20Bao%20and%20Pei%20An%20and%20Xinyue%20Tang%20and%20An%20Pan%20and%20Jie%20Ma%20and%20Bingchuan%20Sun%20and%20Yan%20Wang&entry.1292438233=%20%20Semantic%20occupancy%20has%20emerged%20as%20a%20powerful%20representation%20in%20world%20models%0Afor%20its%20ability%20to%20capture%20rich%20spatial%20semantics.%20However%2C%20most%20existing%0Aoccupancy%20world%20models%20rely%20on%20static%20and%20fixed%20embeddings%20or%20grids%2C%20which%0Ainherently%20limit%20the%20flexibility%20of%20perception.%20Moreover%2C%20their%20%22in-place%0Aclassification%22%20over%20grids%20exhibits%20a%20potential%20misalignment%20with%20the%20dynamic%0Aand%20continuous%20nature%20of%20real%20scenarios.In%20this%20paper%2C%20we%20propose%20SparseWorld%2C%0Aa%20novel%204D%20occupancy%20world%20model%20that%20is%20flexible%2C%20adaptive%2C%20and%20efficient%2C%0Apowered%20by%20sparse%20and%20dynamic%20queries.%20We%20propose%20a%20Range-Adaptive%20Perception%0Amodule%2C%20in%20which%20learnable%20queries%20are%20modulated%20by%20the%20ego%20vehicle%20states%20and%0Aenriched%20with%20temporal-spatial%20associations%20to%20enable%20extended-range%0Aperception.%20To%20effectively%20capture%20the%20dynamics%20of%20the%20scene%2C%20we%20design%20a%0AState-Conditioned%20Forecasting%20module%2C%20which%20replaces%20classification-based%0Aforecasting%20with%20regression-guided%20formulation%2C%20precisely%20aligning%20the%20dynamic%0Aqueries%20with%20the%20continuity%20of%20the%204D%20environment.%20In%20addition%2C%20We%20specifically%0Adevise%20a%20Temporal-Aware%20Self-Scheduling%20training%20strategy%20to%20enable%20smooth%20and%0Aefficient%20training.%20Extensive%20experiments%20demonstrate%20that%20SparseWorld%20achieves%0Astate-of-the-art%20performance%20across%20perception%2C%20forecasting%2C%20and%20planning%0Atasks.%20Comprehensive%20visualizations%20and%20ablation%20studies%20further%20validate%20the%0Aadvantages%20of%20SparseWorld%20in%20terms%20of%20flexibility%2C%20adaptability%2C%20and%0Aefficiency.%20The%20code%20is%20available%20at%20https%3A//github.com/MSunDYY/SparseWorld.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17482v2&entry.124074799=Read"},
{"title": "Follow the STARs: Dynamic $\u03c9$-Regular Shielding of Learned Policies", "author": "Ashwani Anand and Satya Prakash Nayak and Ritam Raha and Anne-Kathrin Schmuck", "abstract": "  This paper presents a novel dynamic post-shielding framework that enforces\nthe full class of $\\omega$-regular correctness properties over pre-computed\nprobabilistic policies. This constitutes a paradigm shift from the predominant\nsetting of safety-shielding -- i.e., ensuring that nothing bad ever happens --\nto a shielding process that additionally enforces liveness -- i.e., ensures\nthat something good eventually happens. At the core, our method uses\nStrategy-Template-based Adaptive Runtime Shields (STARs), which leverage\npermissive strategy templates to enable post-shielding with minimal\ninterference. As its main feature, STARs introduce a mechanism to dynamically\ncontrol interference, allowing a tunable enforcement parameter to balance\nformal obligations and task-specific behavior at runtime. This allows to\ntrigger more aggressive enforcement when needed, while allowing for optimized\npolicy choices otherwise. In addition, STARs support runtime adaptation to\nchanging specifications or actuator failures, making them especially suited for\ncyber-physical applications. We evaluate STARs on a mobile robot benchmark to\ndemonstrate their controllable interference when enforcing (incrementally\nupdated) $\\omega$-regular correctness properties over learned probabilistic\npolicies.\n", "link": "http://arxiv.org/abs/2505.14689v3", "date": "2025-10-22", "relevancy": 1.3257, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4628}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4452}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4127}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Follow%20the%20STARs%3A%20Dynamic%20%24%CF%89%24-Regular%20Shielding%20of%20Learned%20Policies&body=Title%3A%20Follow%20the%20STARs%3A%20Dynamic%20%24%CF%89%24-Regular%20Shielding%20of%20Learned%20Policies%0AAuthor%3A%20Ashwani%20Anand%20and%20Satya%20Prakash%20Nayak%20and%20Ritam%20Raha%20and%20Anne-Kathrin%20Schmuck%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20dynamic%20post-shielding%20framework%20that%20enforces%0Athe%20full%20class%20of%20%24%5Comega%24-regular%20correctness%20properties%20over%20pre-computed%0Aprobabilistic%20policies.%20This%20constitutes%20a%20paradigm%20shift%20from%20the%20predominant%0Asetting%20of%20safety-shielding%20--%20i.e.%2C%20ensuring%20that%20nothing%20bad%20ever%20happens%20--%0Ato%20a%20shielding%20process%20that%20additionally%20enforces%20liveness%20--%20i.e.%2C%20ensures%0Athat%20something%20good%20eventually%20happens.%20At%20the%20core%2C%20our%20method%20uses%0AStrategy-Template-based%20Adaptive%20Runtime%20Shields%20%28STARs%29%2C%20which%20leverage%0Apermissive%20strategy%20templates%20to%20enable%20post-shielding%20with%20minimal%0Ainterference.%20As%20its%20main%20feature%2C%20STARs%20introduce%20a%20mechanism%20to%20dynamically%0Acontrol%20interference%2C%20allowing%20a%20tunable%20enforcement%20parameter%20to%20balance%0Aformal%20obligations%20and%20task-specific%20behavior%20at%20runtime.%20This%20allows%20to%0Atrigger%20more%20aggressive%20enforcement%20when%20needed%2C%20while%20allowing%20for%20optimized%0Apolicy%20choices%20otherwise.%20In%20addition%2C%20STARs%20support%20runtime%20adaptation%20to%0Achanging%20specifications%20or%20actuator%20failures%2C%20making%20them%20especially%20suited%20for%0Acyber-physical%20applications.%20We%20evaluate%20STARs%20on%20a%20mobile%20robot%20benchmark%20to%0Ademonstrate%20their%20controllable%20interference%20when%20enforcing%20%28incrementally%0Aupdated%29%20%24%5Comega%24-regular%20correctness%20properties%20over%20learned%20probabilistic%0Apolicies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14689v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFollow%2520the%2520STARs%253A%2520Dynamic%2520%2524%25CF%2589%2524-Regular%2520Shielding%2520of%2520Learned%2520Policies%26entry.906535625%3DAshwani%2520Anand%2520and%2520Satya%2520Prakash%2520Nayak%2520and%2520Ritam%2520Raha%2520and%2520Anne-Kathrin%2520Schmuck%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520dynamic%2520post-shielding%2520framework%2520that%2520enforces%250Athe%2520full%2520class%2520of%2520%2524%255Comega%2524-regular%2520correctness%2520properties%2520over%2520pre-computed%250Aprobabilistic%2520policies.%2520This%2520constitutes%2520a%2520paradigm%2520shift%2520from%2520the%2520predominant%250Asetting%2520of%2520safety-shielding%2520--%2520i.e.%252C%2520ensuring%2520that%2520nothing%2520bad%2520ever%2520happens%2520--%250Ato%2520a%2520shielding%2520process%2520that%2520additionally%2520enforces%2520liveness%2520--%2520i.e.%252C%2520ensures%250Athat%2520something%2520good%2520eventually%2520happens.%2520At%2520the%2520core%252C%2520our%2520method%2520uses%250AStrategy-Template-based%2520Adaptive%2520Runtime%2520Shields%2520%2528STARs%2529%252C%2520which%2520leverage%250Apermissive%2520strategy%2520templates%2520to%2520enable%2520post-shielding%2520with%2520minimal%250Ainterference.%2520As%2520its%2520main%2520feature%252C%2520STARs%2520introduce%2520a%2520mechanism%2520to%2520dynamically%250Acontrol%2520interference%252C%2520allowing%2520a%2520tunable%2520enforcement%2520parameter%2520to%2520balance%250Aformal%2520obligations%2520and%2520task-specific%2520behavior%2520at%2520runtime.%2520This%2520allows%2520to%250Atrigger%2520more%2520aggressive%2520enforcement%2520when%2520needed%252C%2520while%2520allowing%2520for%2520optimized%250Apolicy%2520choices%2520otherwise.%2520In%2520addition%252C%2520STARs%2520support%2520runtime%2520adaptation%2520to%250Achanging%2520specifications%2520or%2520actuator%2520failures%252C%2520making%2520them%2520especially%2520suited%2520for%250Acyber-physical%2520applications.%2520We%2520evaluate%2520STARs%2520on%2520a%2520mobile%2520robot%2520benchmark%2520to%250Ademonstrate%2520their%2520controllable%2520interference%2520when%2520enforcing%2520%2528incrementally%250Aupdated%2529%2520%2524%255Comega%2524-regular%2520correctness%2520properties%2520over%2520learned%2520probabilistic%250Apolicies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14689v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Follow%20the%20STARs%3A%20Dynamic%20%24%CF%89%24-Regular%20Shielding%20of%20Learned%20Policies&entry.906535625=Ashwani%20Anand%20and%20Satya%20Prakash%20Nayak%20and%20Ritam%20Raha%20and%20Anne-Kathrin%20Schmuck&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20dynamic%20post-shielding%20framework%20that%20enforces%0Athe%20full%20class%20of%20%24%5Comega%24-regular%20correctness%20properties%20over%20pre-computed%0Aprobabilistic%20policies.%20This%20constitutes%20a%20paradigm%20shift%20from%20the%20predominant%0Asetting%20of%20safety-shielding%20--%20i.e.%2C%20ensuring%20that%20nothing%20bad%20ever%20happens%20--%0Ato%20a%20shielding%20process%20that%20additionally%20enforces%20liveness%20--%20i.e.%2C%20ensures%0Athat%20something%20good%20eventually%20happens.%20At%20the%20core%2C%20our%20method%20uses%0AStrategy-Template-based%20Adaptive%20Runtime%20Shields%20%28STARs%29%2C%20which%20leverage%0Apermissive%20strategy%20templates%20to%20enable%20post-shielding%20with%20minimal%0Ainterference.%20As%20its%20main%20feature%2C%20STARs%20introduce%20a%20mechanism%20to%20dynamically%0Acontrol%20interference%2C%20allowing%20a%20tunable%20enforcement%20parameter%20to%20balance%0Aformal%20obligations%20and%20task-specific%20behavior%20at%20runtime.%20This%20allows%20to%0Atrigger%20more%20aggressive%20enforcement%20when%20needed%2C%20while%20allowing%20for%20optimized%0Apolicy%20choices%20otherwise.%20In%20addition%2C%20STARs%20support%20runtime%20adaptation%20to%0Achanging%20specifications%20or%20actuator%20failures%2C%20making%20them%20especially%20suited%20for%0Acyber-physical%20applications.%20We%20evaluate%20STARs%20on%20a%20mobile%20robot%20benchmark%20to%0Ademonstrate%20their%20controllable%20interference%20when%20enforcing%20%28incrementally%0Aupdated%29%20%24%5Comega%24-regular%20correctness%20properties%20over%20learned%20probabilistic%0Apolicies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14689v3&entry.124074799=Read"},
{"title": "Scaf-GRPO: Scaffolded Group Relative Policy Optimization for Enhancing\n  LLM Reasoning", "author": "Xichen Zhang and Sitong Wu and Yinghao Zhu and Haoru Tan and Shaozuo Yu and Ziyi He and Jiaya Jia", "abstract": "  Reinforcement learning from verifiable rewards has emerged as a powerful\ntechnique for enhancing the complex reasoning abilities of Large Language\nModels (LLMs). However, these methods are fundamentally constrained by the\n''learning cliff'' phenomenon: when faced with problems far beyond their\ncurrent capabilities, models consistently fail, yielding a persistent\nzero-reward signal. In policy optimization algorithms like GRPO, this collapses\nthe advantage calculation to zero, rendering these difficult problems invisible\nto the learning gradient and stalling progress. To overcome this, we introduce\nScaf-GRPO (Scaffolded Group Relative Policy Optimization), a progressive\ntraining framework that strategically provides minimal guidance only when a\nmodel's independent learning has plateaued. The framework first diagnoses\nlearning stagnation and then intervenes by injecting tiered in-prompt hints,\nranging from abstract concepts to concrete steps, enabling the model to\nconstruct a valid solution by itself. Extensive experiments on challenging\nmathematics benchmarks demonstrate Scaf-GRPO's effectiveness, boosting the\npass@1 score of the Qwen2.5-Math-7B model on the AIME24 benchmark by a relative\n44.3% over a vanilla GRPO baseline. This result demonstrates our framework\nprovides a robust and effective methodology for unlocking a model's ability to\nsolve problems previously beyond its reach, a critical step towards extending\nthe frontier of autonomous reasoning in LLM.\n", "link": "http://arxiv.org/abs/2510.19807v1", "date": "2025-10-22", "relevancy": 1.5028, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5365}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4908}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaf-GRPO%3A%20Scaffolded%20Group%20Relative%20Policy%20Optimization%20for%20Enhancing%0A%20%20LLM%20Reasoning&body=Title%3A%20Scaf-GRPO%3A%20Scaffolded%20Group%20Relative%20Policy%20Optimization%20for%20Enhancing%0A%20%20LLM%20Reasoning%0AAuthor%3A%20Xichen%20Zhang%20and%20Sitong%20Wu%20and%20Yinghao%20Zhu%20and%20Haoru%20Tan%20and%20Shaozuo%20Yu%20and%20Ziyi%20He%20and%20Jiaya%20Jia%0AAbstract%3A%20%20%20Reinforcement%20learning%20from%20verifiable%20rewards%20has%20emerged%20as%20a%20powerful%0Atechnique%20for%20enhancing%20the%20complex%20reasoning%20abilities%20of%20Large%20Language%0AModels%20%28LLMs%29.%20However%2C%20these%20methods%20are%20fundamentally%20constrained%20by%20the%0A%27%27learning%20cliff%27%27%20phenomenon%3A%20when%20faced%20with%20problems%20far%20beyond%20their%0Acurrent%20capabilities%2C%20models%20consistently%20fail%2C%20yielding%20a%20persistent%0Azero-reward%20signal.%20In%20policy%20optimization%20algorithms%20like%20GRPO%2C%20this%20collapses%0Athe%20advantage%20calculation%20to%20zero%2C%20rendering%20these%20difficult%20problems%20invisible%0Ato%20the%20learning%20gradient%20and%20stalling%20progress.%20To%20overcome%20this%2C%20we%20introduce%0AScaf-GRPO%20%28Scaffolded%20Group%20Relative%20Policy%20Optimization%29%2C%20a%20progressive%0Atraining%20framework%20that%20strategically%20provides%20minimal%20guidance%20only%20when%20a%0Amodel%27s%20independent%20learning%20has%20plateaued.%20The%20framework%20first%20diagnoses%0Alearning%20stagnation%20and%20then%20intervenes%20by%20injecting%20tiered%20in-prompt%20hints%2C%0Aranging%20from%20abstract%20concepts%20to%20concrete%20steps%2C%20enabling%20the%20model%20to%0Aconstruct%20a%20valid%20solution%20by%20itself.%20Extensive%20experiments%20on%20challenging%0Amathematics%20benchmarks%20demonstrate%20Scaf-GRPO%27s%20effectiveness%2C%20boosting%20the%0Apass%401%20score%20of%20the%20Qwen2.5-Math-7B%20model%20on%20the%20AIME24%20benchmark%20by%20a%20relative%0A44.3%25%20over%20a%20vanilla%20GRPO%20baseline.%20This%20result%20demonstrates%20our%20framework%0Aprovides%20a%20robust%20and%20effective%20methodology%20for%20unlocking%20a%20model%27s%20ability%20to%0Asolve%20problems%20previously%20beyond%20its%20reach%2C%20a%20critical%20step%20towards%20extending%0Athe%20frontier%20of%20autonomous%20reasoning%20in%20LLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19807v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaf-GRPO%253A%2520Scaffolded%2520Group%2520Relative%2520Policy%2520Optimization%2520for%2520Enhancing%250A%2520%2520LLM%2520Reasoning%26entry.906535625%3DXichen%2520Zhang%2520and%2520Sitong%2520Wu%2520and%2520Yinghao%2520Zhu%2520and%2520Haoru%2520Tan%2520and%2520Shaozuo%2520Yu%2520and%2520Ziyi%2520He%2520and%2520Jiaya%2520Jia%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520from%2520verifiable%2520rewards%2520has%2520emerged%2520as%2520a%2520powerful%250Atechnique%2520for%2520enhancing%2520the%2520complex%2520reasoning%2520abilities%2520of%2520Large%2520Language%250AModels%2520%2528LLMs%2529.%2520However%252C%2520these%2520methods%2520are%2520fundamentally%2520constrained%2520by%2520the%250A%2527%2527learning%2520cliff%2527%2527%2520phenomenon%253A%2520when%2520faced%2520with%2520problems%2520far%2520beyond%2520their%250Acurrent%2520capabilities%252C%2520models%2520consistently%2520fail%252C%2520yielding%2520a%2520persistent%250Azero-reward%2520signal.%2520In%2520policy%2520optimization%2520algorithms%2520like%2520GRPO%252C%2520this%2520collapses%250Athe%2520advantage%2520calculation%2520to%2520zero%252C%2520rendering%2520these%2520difficult%2520problems%2520invisible%250Ato%2520the%2520learning%2520gradient%2520and%2520stalling%2520progress.%2520To%2520overcome%2520this%252C%2520we%2520introduce%250AScaf-GRPO%2520%2528Scaffolded%2520Group%2520Relative%2520Policy%2520Optimization%2529%252C%2520a%2520progressive%250Atraining%2520framework%2520that%2520strategically%2520provides%2520minimal%2520guidance%2520only%2520when%2520a%250Amodel%2527s%2520independent%2520learning%2520has%2520plateaued.%2520The%2520framework%2520first%2520diagnoses%250Alearning%2520stagnation%2520and%2520then%2520intervenes%2520by%2520injecting%2520tiered%2520in-prompt%2520hints%252C%250Aranging%2520from%2520abstract%2520concepts%2520to%2520concrete%2520steps%252C%2520enabling%2520the%2520model%2520to%250Aconstruct%2520a%2520valid%2520solution%2520by%2520itself.%2520Extensive%2520experiments%2520on%2520challenging%250Amathematics%2520benchmarks%2520demonstrate%2520Scaf-GRPO%2527s%2520effectiveness%252C%2520boosting%2520the%250Apass%25401%2520score%2520of%2520the%2520Qwen2.5-Math-7B%2520model%2520on%2520the%2520AIME24%2520benchmark%2520by%2520a%2520relative%250A44.3%2525%2520over%2520a%2520vanilla%2520GRPO%2520baseline.%2520This%2520result%2520demonstrates%2520our%2520framework%250Aprovides%2520a%2520robust%2520and%2520effective%2520methodology%2520for%2520unlocking%2520a%2520model%2527s%2520ability%2520to%250Asolve%2520problems%2520previously%2520beyond%2520its%2520reach%252C%2520a%2520critical%2520step%2520towards%2520extending%250Athe%2520frontier%2520of%2520autonomous%2520reasoning%2520in%2520LLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19807v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaf-GRPO%3A%20Scaffolded%20Group%20Relative%20Policy%20Optimization%20for%20Enhancing%0A%20%20LLM%20Reasoning&entry.906535625=Xichen%20Zhang%20and%20Sitong%20Wu%20and%20Yinghao%20Zhu%20and%20Haoru%20Tan%20and%20Shaozuo%20Yu%20and%20Ziyi%20He%20and%20Jiaya%20Jia&entry.1292438233=%20%20Reinforcement%20learning%20from%20verifiable%20rewards%20has%20emerged%20as%20a%20powerful%0Atechnique%20for%20enhancing%20the%20complex%20reasoning%20abilities%20of%20Large%20Language%0AModels%20%28LLMs%29.%20However%2C%20these%20methods%20are%20fundamentally%20constrained%20by%20the%0A%27%27learning%20cliff%27%27%20phenomenon%3A%20when%20faced%20with%20problems%20far%20beyond%20their%0Acurrent%20capabilities%2C%20models%20consistently%20fail%2C%20yielding%20a%20persistent%0Azero-reward%20signal.%20In%20policy%20optimization%20algorithms%20like%20GRPO%2C%20this%20collapses%0Athe%20advantage%20calculation%20to%20zero%2C%20rendering%20these%20difficult%20problems%20invisible%0Ato%20the%20learning%20gradient%20and%20stalling%20progress.%20To%20overcome%20this%2C%20we%20introduce%0AScaf-GRPO%20%28Scaffolded%20Group%20Relative%20Policy%20Optimization%29%2C%20a%20progressive%0Atraining%20framework%20that%20strategically%20provides%20minimal%20guidance%20only%20when%20a%0Amodel%27s%20independent%20learning%20has%20plateaued.%20The%20framework%20first%20diagnoses%0Alearning%20stagnation%20and%20then%20intervenes%20by%20injecting%20tiered%20in-prompt%20hints%2C%0Aranging%20from%20abstract%20concepts%20to%20concrete%20steps%2C%20enabling%20the%20model%20to%0Aconstruct%20a%20valid%20solution%20by%20itself.%20Extensive%20experiments%20on%20challenging%0Amathematics%20benchmarks%20demonstrate%20Scaf-GRPO%27s%20effectiveness%2C%20boosting%20the%0Apass%401%20score%20of%20the%20Qwen2.5-Math-7B%20model%20on%20the%20AIME24%20benchmark%20by%20a%20relative%0A44.3%25%20over%20a%20vanilla%20GRPO%20baseline.%20This%20result%20demonstrates%20our%20framework%0Aprovides%20a%20robust%20and%20effective%20methodology%20for%20unlocking%20a%20model%27s%20ability%20to%0Asolve%20problems%20previously%20beyond%20its%20reach%2C%20a%20critical%20step%20towards%20extending%0Athe%20frontier%20of%20autonomous%20reasoning%20in%20LLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19807v1&entry.124074799=Read"},
{"title": "A Graph Engine for Guitar Chord-Tone Soloing Education", "author": "Matthew Keating and Michael Casey", "abstract": "  We present a graph-based engine for computing chord tone soloing suggestions\nfor guitar students. Chord tone soloing is a fundamental practice for\nimprovising over a chord progression, where the instrumentalist uses only the\nnotes contained in the current chord. This practice is a building block for all\nadvanced jazz guitar theory but is difficult to learn and practice. First, we\ndiscuss methods for generating chord-tone arpeggios. Next, we construct a\nweighted graph where each node represents a chord tone arpeggio for a chord in\nthe progression. Then, we calculate the edge weight between each consecutive\nchord's nodes in terms of optimal transition tones. We then find the shortest\npath through this graph and reconstruct a chord-tone soloing line. Finally, we\ndiscuss a user-friendly system to handle input and output to this engine for\nguitar students to practice chord tone soloing.\n", "link": "http://arxiv.org/abs/2510.19666v1", "date": "2025-10-22", "relevancy": 1.1741, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.3988}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3972}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Graph%20Engine%20for%20Guitar%20Chord-Tone%20Soloing%20Education&body=Title%3A%20A%20Graph%20Engine%20for%20Guitar%20Chord-Tone%20Soloing%20Education%0AAuthor%3A%20Matthew%20Keating%20and%20Michael%20Casey%0AAbstract%3A%20%20%20We%20present%20a%20graph-based%20engine%20for%20computing%20chord%20tone%20soloing%20suggestions%0Afor%20guitar%20students.%20Chord%20tone%20soloing%20is%20a%20fundamental%20practice%20for%0Aimprovising%20over%20a%20chord%20progression%2C%20where%20the%20instrumentalist%20uses%20only%20the%0Anotes%20contained%20in%20the%20current%20chord.%20This%20practice%20is%20a%20building%20block%20for%20all%0Aadvanced%20jazz%20guitar%20theory%20but%20is%20difficult%20to%20learn%20and%20practice.%20First%2C%20we%0Adiscuss%20methods%20for%20generating%20chord-tone%20arpeggios.%20Next%2C%20we%20construct%20a%0Aweighted%20graph%20where%20each%20node%20represents%20a%20chord%20tone%20arpeggio%20for%20a%20chord%20in%0Athe%20progression.%20Then%2C%20we%20calculate%20the%20edge%20weight%20between%20each%20consecutive%0Achord%27s%20nodes%20in%20terms%20of%20optimal%20transition%20tones.%20We%20then%20find%20the%20shortest%0Apath%20through%20this%20graph%20and%20reconstruct%20a%20chord-tone%20soloing%20line.%20Finally%2C%20we%0Adiscuss%20a%20user-friendly%20system%20to%20handle%20input%20and%20output%20to%20this%20engine%20for%0Aguitar%20students%20to%20practice%20chord%20tone%20soloing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.19666v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Graph%2520Engine%2520for%2520Guitar%2520Chord-Tone%2520Soloing%2520Education%26entry.906535625%3DMatthew%2520Keating%2520and%2520Michael%2520Casey%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520graph-based%2520engine%2520for%2520computing%2520chord%2520tone%2520soloing%2520suggestions%250Afor%2520guitar%2520students.%2520Chord%2520tone%2520soloing%2520is%2520a%2520fundamental%2520practice%2520for%250Aimprovising%2520over%2520a%2520chord%2520progression%252C%2520where%2520the%2520instrumentalist%2520uses%2520only%2520the%250Anotes%2520contained%2520in%2520the%2520current%2520chord.%2520This%2520practice%2520is%2520a%2520building%2520block%2520for%2520all%250Aadvanced%2520jazz%2520guitar%2520theory%2520but%2520is%2520difficult%2520to%2520learn%2520and%2520practice.%2520First%252C%2520we%250Adiscuss%2520methods%2520for%2520generating%2520chord-tone%2520arpeggios.%2520Next%252C%2520we%2520construct%2520a%250Aweighted%2520graph%2520where%2520each%2520node%2520represents%2520a%2520chord%2520tone%2520arpeggio%2520for%2520a%2520chord%2520in%250Athe%2520progression.%2520Then%252C%2520we%2520calculate%2520the%2520edge%2520weight%2520between%2520each%2520consecutive%250Achord%2527s%2520nodes%2520in%2520terms%2520of%2520optimal%2520transition%2520tones.%2520We%2520then%2520find%2520the%2520shortest%250Apath%2520through%2520this%2520graph%2520and%2520reconstruct%2520a%2520chord-tone%2520soloing%2520line.%2520Finally%252C%2520we%250Adiscuss%2520a%2520user-friendly%2520system%2520to%2520handle%2520input%2520and%2520output%2520to%2520this%2520engine%2520for%250Aguitar%2520students%2520to%2520practice%2520chord%2520tone%2520soloing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19666v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Graph%20Engine%20for%20Guitar%20Chord-Tone%20Soloing%20Education&entry.906535625=Matthew%20Keating%20and%20Michael%20Casey&entry.1292438233=%20%20We%20present%20a%20graph-based%20engine%20for%20computing%20chord%20tone%20soloing%20suggestions%0Afor%20guitar%20students.%20Chord%20tone%20soloing%20is%20a%20fundamental%20practice%20for%0Aimprovising%20over%20a%20chord%20progression%2C%20where%20the%20instrumentalist%20uses%20only%20the%0Anotes%20contained%20in%20the%20current%20chord.%20This%20practice%20is%20a%20building%20block%20for%20all%0Aadvanced%20jazz%20guitar%20theory%20but%20is%20difficult%20to%20learn%20and%20practice.%20First%2C%20we%0Adiscuss%20methods%20for%20generating%20chord-tone%20arpeggios.%20Next%2C%20we%20construct%20a%0Aweighted%20graph%20where%20each%20node%20represents%20a%20chord%20tone%20arpeggio%20for%20a%20chord%20in%0Athe%20progression.%20Then%2C%20we%20calculate%20the%20edge%20weight%20between%20each%20consecutive%0Achord%27s%20nodes%20in%20terms%20of%20optimal%20transition%20tones.%20We%20then%20find%20the%20shortest%0Apath%20through%20this%20graph%20and%20reconstruct%20a%20chord-tone%20soloing%20line.%20Finally%2C%20we%0Adiscuss%20a%20user-friendly%20system%20to%20handle%20input%20and%20output%20to%20this%20engine%20for%0Aguitar%20students%20to%20practice%20chord%20tone%20soloing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.19666v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


