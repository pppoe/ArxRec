<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250824.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing,\n  and Generation", "author": "Shuting He and Peilin Ji and Yitong Yang and Changshuo Wang and Jiayi Ji and Yinglin Wang and Henghui Ding", "abstract": "  3D Gaussian Splatting (3DGS) has recently emerged as a powerful alternative\nto Neural Radiance Fields (NeRF) for 3D scene representation, offering\nhigh-fidelity photorealistic rendering with real-time performance. Beyond novel\nview synthesis, the explicit and compact nature of 3DGS enables a wide range of\ndownstream applications that require geometric and semantic understanding. This\nsurvey provides a comprehensive overview of recent progress in 3DGS\napplications. It first introduces 2D foundation models that support semantic\nunderstanding and control in 3DGS applications, followed by a review of\nNeRF-based methods that inform their 3DGS counterparts. We then categorize 3DGS\napplications into segmentation, editing, generation, and other functional\ntasks. For each, we summarize representative methods, supervision strategies,\nand learning paradigms, highlighting shared design principles and emerging\ntrends. Commonly used datasets and evaluation protocols are also summarized,\nalong with comparative analyses of recent methods across public benchmarks. To\nsupport ongoing research and development, a continually updated repository of\npapers, code, and resources is maintained at\nhttps://github.com/heshuting555/Awesome-3DGS-Applications.\n", "link": "http://arxiv.org/abs/2508.09977v2", "date": "2025-08-22", "relevancy": 3.4332, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7128}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6752}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.672}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%203D%20Gaussian%20Splatting%20Applications%3A%20Segmentation%2C%20Editing%2C%0A%20%20and%20Generation&body=Title%3A%20A%20Survey%20on%203D%20Gaussian%20Splatting%20Applications%3A%20Segmentation%2C%20Editing%2C%0A%20%20and%20Generation%0AAuthor%3A%20Shuting%20He%20and%20Peilin%20Ji%20and%20Yitong%20Yang%20and%20Changshuo%20Wang%20and%20Jiayi%20Ji%20and%20Yinglin%20Wang%20and%20Henghui%20Ding%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20recently%20emerged%20as%20a%20powerful%20alternative%0Ato%20Neural%20Radiance%20Fields%20%28NeRF%29%20for%203D%20scene%20representation%2C%20offering%0Ahigh-fidelity%20photorealistic%20rendering%20with%20real-time%20performance.%20Beyond%20novel%0Aview%20synthesis%2C%20the%20explicit%20and%20compact%20nature%20of%203DGS%20enables%20a%20wide%20range%20of%0Adownstream%20applications%20that%20require%20geometric%20and%20semantic%20understanding.%20This%0Asurvey%20provides%20a%20comprehensive%20overview%20of%20recent%20progress%20in%203DGS%0Aapplications.%20It%20first%20introduces%202D%20foundation%20models%20that%20support%20semantic%0Aunderstanding%20and%20control%20in%203DGS%20applications%2C%20followed%20by%20a%20review%20of%0ANeRF-based%20methods%20that%20inform%20their%203DGS%20counterparts.%20We%20then%20categorize%203DGS%0Aapplications%20into%20segmentation%2C%20editing%2C%20generation%2C%20and%20other%20functional%0Atasks.%20For%20each%2C%20we%20summarize%20representative%20methods%2C%20supervision%20strategies%2C%0Aand%20learning%20paradigms%2C%20highlighting%20shared%20design%20principles%20and%20emerging%0Atrends.%20Commonly%20used%20datasets%20and%20evaluation%20protocols%20are%20also%20summarized%2C%0Aalong%20with%20comparative%20analyses%20of%20recent%20methods%20across%20public%20benchmarks.%20To%0Asupport%20ongoing%20research%20and%20development%2C%20a%20continually%20updated%20repository%20of%0Apapers%2C%20code%2C%20and%20resources%20is%20maintained%20at%0Ahttps%3A//github.com/heshuting555/Awesome-3DGS-Applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09977v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%25203D%2520Gaussian%2520Splatting%2520Applications%253A%2520Segmentation%252C%2520Editing%252C%250A%2520%2520and%2520Generation%26entry.906535625%3DShuting%2520He%2520and%2520Peilin%2520Ji%2520and%2520Yitong%2520Yang%2520and%2520Changshuo%2520Wang%2520and%2520Jiayi%2520Ji%2520and%2520Yinglin%2520Wang%2520and%2520Henghui%2520Ding%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520recently%2520emerged%2520as%2520a%2520powerful%2520alternative%250Ato%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520for%25203D%2520scene%2520representation%252C%2520offering%250Ahigh-fidelity%2520photorealistic%2520rendering%2520with%2520real-time%2520performance.%2520Beyond%2520novel%250Aview%2520synthesis%252C%2520the%2520explicit%2520and%2520compact%2520nature%2520of%25203DGS%2520enables%2520a%2520wide%2520range%2520of%250Adownstream%2520applications%2520that%2520require%2520geometric%2520and%2520semantic%2520understanding.%2520This%250Asurvey%2520provides%2520a%2520comprehensive%2520overview%2520of%2520recent%2520progress%2520in%25203DGS%250Aapplications.%2520It%2520first%2520introduces%25202D%2520foundation%2520models%2520that%2520support%2520semantic%250Aunderstanding%2520and%2520control%2520in%25203DGS%2520applications%252C%2520followed%2520by%2520a%2520review%2520of%250ANeRF-based%2520methods%2520that%2520inform%2520their%25203DGS%2520counterparts.%2520We%2520then%2520categorize%25203DGS%250Aapplications%2520into%2520segmentation%252C%2520editing%252C%2520generation%252C%2520and%2520other%2520functional%250Atasks.%2520For%2520each%252C%2520we%2520summarize%2520representative%2520methods%252C%2520supervision%2520strategies%252C%250Aand%2520learning%2520paradigms%252C%2520highlighting%2520shared%2520design%2520principles%2520and%2520emerging%250Atrends.%2520Commonly%2520used%2520datasets%2520and%2520evaluation%2520protocols%2520are%2520also%2520summarized%252C%250Aalong%2520with%2520comparative%2520analyses%2520of%2520recent%2520methods%2520across%2520public%2520benchmarks.%2520To%250Asupport%2520ongoing%2520research%2520and%2520development%252C%2520a%2520continually%2520updated%2520repository%2520of%250Apapers%252C%2520code%252C%2520and%2520resources%2520is%2520maintained%2520at%250Ahttps%253A//github.com/heshuting555/Awesome-3DGS-Applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09977v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%203D%20Gaussian%20Splatting%20Applications%3A%20Segmentation%2C%20Editing%2C%0A%20%20and%20Generation&entry.906535625=Shuting%20He%20and%20Peilin%20Ji%20and%20Yitong%20Yang%20and%20Changshuo%20Wang%20and%20Jiayi%20Ji%20and%20Yinglin%20Wang%20and%20Henghui%20Ding&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20recently%20emerged%20as%20a%20powerful%20alternative%0Ato%20Neural%20Radiance%20Fields%20%28NeRF%29%20for%203D%20scene%20representation%2C%20offering%0Ahigh-fidelity%20photorealistic%20rendering%20with%20real-time%20performance.%20Beyond%20novel%0Aview%20synthesis%2C%20the%20explicit%20and%20compact%20nature%20of%203DGS%20enables%20a%20wide%20range%20of%0Adownstream%20applications%20that%20require%20geometric%20and%20semantic%20understanding.%20This%0Asurvey%20provides%20a%20comprehensive%20overview%20of%20recent%20progress%20in%203DGS%0Aapplications.%20It%20first%20introduces%202D%20foundation%20models%20that%20support%20semantic%0Aunderstanding%20and%20control%20in%203DGS%20applications%2C%20followed%20by%20a%20review%20of%0ANeRF-based%20methods%20that%20inform%20their%203DGS%20counterparts.%20We%20then%20categorize%203DGS%0Aapplications%20into%20segmentation%2C%20editing%2C%20generation%2C%20and%20other%20functional%0Atasks.%20For%20each%2C%20we%20summarize%20representative%20methods%2C%20supervision%20strategies%2C%0Aand%20learning%20paradigms%2C%20highlighting%20shared%20design%20principles%20and%20emerging%0Atrends.%20Commonly%20used%20datasets%20and%20evaluation%20protocols%20are%20also%20summarized%2C%0Aalong%20with%20comparative%20analyses%20of%20recent%20methods%20across%20public%20benchmarks.%20To%0Asupport%20ongoing%20research%20and%20development%2C%20a%20continually%20updated%20repository%20of%0Apapers%2C%20code%2C%20and%20resources%20is%20maintained%20at%0Ahttps%3A//github.com/heshuting555/Awesome-3DGS-Applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09977v2&entry.124074799=Read"},
{"title": "A Survey on 3D Gaussian Splatting", "author": "Guikun Chen and Wenguan Wang", "abstract": "  3D Gaussian splatting (GS) has emerged as a transformative technique in\nexplicit radiance field and computer graphics. This innovative approach,\ncharacterized by the use of millions of learnable 3D Gaussians, represents a\nsignificant departure from mainstream neural radiance field approaches, which\npredominantly use implicit, coordinate-based models to map spatial coordinates\nto pixel values. 3D GS, with its explicit scene representation and\ndifferentiable rendering algorithm, not only promises real-time rendering\ncapability but also introduces unprecedented levels of editability. This\npositions 3D GS as a potential game-changer for the next generation of 3D\nreconstruction and representation. In the present paper, we provide the first\nsystematic overview of the recent developments and critical contributions in\nthe domain of 3D GS. We begin with a detailed exploration of the underlying\nprinciples and the driving forces behind the emergence of 3D GS, laying the\ngroundwork for understanding its significance. A focal point of our discussion\nis the practical applicability of 3D GS. By enabling unprecedented rendering\nspeed, 3D GS opens up a plethora of applications, ranging from virtual reality\nto interactive media and beyond. This is complemented by a comparative analysis\nof leading 3D GS models, evaluated across various benchmark tasks to highlight\ntheir performance and practical utility. The survey concludes by identifying\ncurrent challenges and suggesting potential avenues for future research.\nThrough this survey, we aim to provide a valuable resource for both newcomers\nand seasoned researchers, fostering further exploration and advancement in\nexplicit radiance field.\n", "link": "http://arxiv.org/abs/2401.03890v7", "date": "2025-08-22", "relevancy": 3.4072, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.713}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6738}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%203D%20Gaussian%20Splatting&body=Title%3A%20A%20Survey%20on%203D%20Gaussian%20Splatting%0AAuthor%3A%20Guikun%20Chen%20and%20Wenguan%20Wang%0AAbstract%3A%20%20%203D%20Gaussian%20splatting%20%28GS%29%20has%20emerged%20as%20a%20transformative%20technique%20in%0Aexplicit%20radiance%20field%20and%20computer%20graphics.%20This%20innovative%20approach%2C%0Acharacterized%20by%20the%20use%20of%20millions%20of%20learnable%203D%20Gaussians%2C%20represents%20a%0Asignificant%20departure%20from%20mainstream%20neural%20radiance%20field%20approaches%2C%20which%0Apredominantly%20use%20implicit%2C%20coordinate-based%20models%20to%20map%20spatial%20coordinates%0Ato%20pixel%20values.%203D%20GS%2C%20with%20its%20explicit%20scene%20representation%20and%0Adifferentiable%20rendering%20algorithm%2C%20not%20only%20promises%20real-time%20rendering%0Acapability%20but%20also%20introduces%20unprecedented%20levels%20of%20editability.%20This%0Apositions%203D%20GS%20as%20a%20potential%20game-changer%20for%20the%20next%20generation%20of%203D%0Areconstruction%20and%20representation.%20In%20the%20present%20paper%2C%20we%20provide%20the%20first%0Asystematic%20overview%20of%20the%20recent%20developments%20and%20critical%20contributions%20in%0Athe%20domain%20of%203D%20GS.%20We%20begin%20with%20a%20detailed%20exploration%20of%20the%20underlying%0Aprinciples%20and%20the%20driving%20forces%20behind%20the%20emergence%20of%203D%20GS%2C%20laying%20the%0Agroundwork%20for%20understanding%20its%20significance.%20A%20focal%20point%20of%20our%20discussion%0Ais%20the%20practical%20applicability%20of%203D%20GS.%20By%20enabling%20unprecedented%20rendering%0Aspeed%2C%203D%20GS%20opens%20up%20a%20plethora%20of%20applications%2C%20ranging%20from%20virtual%20reality%0Ato%20interactive%20media%20and%20beyond.%20This%20is%20complemented%20by%20a%20comparative%20analysis%0Aof%20leading%203D%20GS%20models%2C%20evaluated%20across%20various%20benchmark%20tasks%20to%20highlight%0Atheir%20performance%20and%20practical%20utility.%20The%20survey%20concludes%20by%20identifying%0Acurrent%20challenges%20and%20suggesting%20potential%20avenues%20for%20future%20research.%0AThrough%20this%20survey%2C%20we%20aim%20to%20provide%20a%20valuable%20resource%20for%20both%20newcomers%0Aand%20seasoned%20researchers%2C%20fostering%20further%20exploration%20and%20advancement%20in%0Aexplicit%20radiance%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.03890v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DGuikun%2520Chen%2520and%2520Wenguan%2520Wang%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520splatting%2520%2528GS%2529%2520has%2520emerged%2520as%2520a%2520transformative%2520technique%2520in%250Aexplicit%2520radiance%2520field%2520and%2520computer%2520graphics.%2520This%2520innovative%2520approach%252C%250Acharacterized%2520by%2520the%2520use%2520of%2520millions%2520of%2520learnable%25203D%2520Gaussians%252C%2520represents%2520a%250Asignificant%2520departure%2520from%2520mainstream%2520neural%2520radiance%2520field%2520approaches%252C%2520which%250Apredominantly%2520use%2520implicit%252C%2520coordinate-based%2520models%2520to%2520map%2520spatial%2520coordinates%250Ato%2520pixel%2520values.%25203D%2520GS%252C%2520with%2520its%2520explicit%2520scene%2520representation%2520and%250Adifferentiable%2520rendering%2520algorithm%252C%2520not%2520only%2520promises%2520real-time%2520rendering%250Acapability%2520but%2520also%2520introduces%2520unprecedented%2520levels%2520of%2520editability.%2520This%250Apositions%25203D%2520GS%2520as%2520a%2520potential%2520game-changer%2520for%2520the%2520next%2520generation%2520of%25203D%250Areconstruction%2520and%2520representation.%2520In%2520the%2520present%2520paper%252C%2520we%2520provide%2520the%2520first%250Asystematic%2520overview%2520of%2520the%2520recent%2520developments%2520and%2520critical%2520contributions%2520in%250Athe%2520domain%2520of%25203D%2520GS.%2520We%2520begin%2520with%2520a%2520detailed%2520exploration%2520of%2520the%2520underlying%250Aprinciples%2520and%2520the%2520driving%2520forces%2520behind%2520the%2520emergence%2520of%25203D%2520GS%252C%2520laying%2520the%250Agroundwork%2520for%2520understanding%2520its%2520significance.%2520A%2520focal%2520point%2520of%2520our%2520discussion%250Ais%2520the%2520practical%2520applicability%2520of%25203D%2520GS.%2520By%2520enabling%2520unprecedented%2520rendering%250Aspeed%252C%25203D%2520GS%2520opens%2520up%2520a%2520plethora%2520of%2520applications%252C%2520ranging%2520from%2520virtual%2520reality%250Ato%2520interactive%2520media%2520and%2520beyond.%2520This%2520is%2520complemented%2520by%2520a%2520comparative%2520analysis%250Aof%2520leading%25203D%2520GS%2520models%252C%2520evaluated%2520across%2520various%2520benchmark%2520tasks%2520to%2520highlight%250Atheir%2520performance%2520and%2520practical%2520utility.%2520The%2520survey%2520concludes%2520by%2520identifying%250Acurrent%2520challenges%2520and%2520suggesting%2520potential%2520avenues%2520for%2520future%2520research.%250AThrough%2520this%2520survey%252C%2520we%2520aim%2520to%2520provide%2520a%2520valuable%2520resource%2520for%2520both%2520newcomers%250Aand%2520seasoned%2520researchers%252C%2520fostering%2520further%2520exploration%2520and%2520advancement%2520in%250Aexplicit%2520radiance%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.03890v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%203D%20Gaussian%20Splatting&entry.906535625=Guikun%20Chen%20and%20Wenguan%20Wang&entry.1292438233=%20%203D%20Gaussian%20splatting%20%28GS%29%20has%20emerged%20as%20a%20transformative%20technique%20in%0Aexplicit%20radiance%20field%20and%20computer%20graphics.%20This%20innovative%20approach%2C%0Acharacterized%20by%20the%20use%20of%20millions%20of%20learnable%203D%20Gaussians%2C%20represents%20a%0Asignificant%20departure%20from%20mainstream%20neural%20radiance%20field%20approaches%2C%20which%0Apredominantly%20use%20implicit%2C%20coordinate-based%20models%20to%20map%20spatial%20coordinates%0Ato%20pixel%20values.%203D%20GS%2C%20with%20its%20explicit%20scene%20representation%20and%0Adifferentiable%20rendering%20algorithm%2C%20not%20only%20promises%20real-time%20rendering%0Acapability%20but%20also%20introduces%20unprecedented%20levels%20of%20editability.%20This%0Apositions%203D%20GS%20as%20a%20potential%20game-changer%20for%20the%20next%20generation%20of%203D%0Areconstruction%20and%20representation.%20In%20the%20present%20paper%2C%20we%20provide%20the%20first%0Asystematic%20overview%20of%20the%20recent%20developments%20and%20critical%20contributions%20in%0Athe%20domain%20of%203D%20GS.%20We%20begin%20with%20a%20detailed%20exploration%20of%20the%20underlying%0Aprinciples%20and%20the%20driving%20forces%20behind%20the%20emergence%20of%203D%20GS%2C%20laying%20the%0Agroundwork%20for%20understanding%20its%20significance.%20A%20focal%20point%20of%20our%20discussion%0Ais%20the%20practical%20applicability%20of%203D%20GS.%20By%20enabling%20unprecedented%20rendering%0Aspeed%2C%203D%20GS%20opens%20up%20a%20plethora%20of%20applications%2C%20ranging%20from%20virtual%20reality%0Ato%20interactive%20media%20and%20beyond.%20This%20is%20complemented%20by%20a%20comparative%20analysis%0Aof%20leading%203D%20GS%20models%2C%20evaluated%20across%20various%20benchmark%20tasks%20to%20highlight%0Atheir%20performance%20and%20practical%20utility.%20The%20survey%20concludes%20by%20identifying%0Acurrent%20challenges%20and%20suggesting%20potential%20avenues%20for%20future%20research.%0AThrough%20this%20survey%2C%20we%20aim%20to%20provide%20a%20valuable%20resource%20for%20both%20newcomers%0Aand%20seasoned%20researchers%2C%20fostering%20further%20exploration%20and%20advancement%20in%0Aexplicit%20radiance%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.03890v7&entry.124074799=Read"},
{"title": "Arbitrary-Scale 3D Gaussian Super-Resolution", "author": "Huimin Zeng and Yue Bai and Yun Fu", "abstract": "  Existing 3D Gaussian Splatting (3DGS) super-resolution methods typically\nperform high-resolution (HR) rendering of fixed scale factors, making them\nimpractical for resource-limited scenarios. Directly rendering arbitrary-scale\nHR views with vanilla 3DGS introduces aliasing artifacts due to the lack of\nscale-aware rendering ability, while adding a post-processing upsampler for\n3DGS complicates the framework and reduces rendering efficiency. To tackle\nthese issues, we build an integrated framework that incorporates scale-aware\nrendering, generative prior-guided optimization, and progressive\nsuper-resolving to enable 3D Gaussian super-resolution of arbitrary scale\nfactors with a single 3D model. Notably, our approach supports both integer and\nnon-integer scale rendering to provide more flexibility. Extensive experiments\ndemonstrate the effectiveness of our model in rendering high-quality\narbitrary-scale HR views (6.59 dB PSNR gain over 3DGS) with a single model. It\npreserves structural consistency with LR views and across different scales,\nwhile maintaining real-time rendering speed (85 FPS at 1080p).\n", "link": "http://arxiv.org/abs/2508.16467v1", "date": "2025-08-22", "relevancy": 3.2406, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7089}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6294}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Arbitrary-Scale%203D%20Gaussian%20Super-Resolution&body=Title%3A%20Arbitrary-Scale%203D%20Gaussian%20Super-Resolution%0AAuthor%3A%20Huimin%20Zeng%20and%20Yue%20Bai%20and%20Yun%20Fu%0AAbstract%3A%20%20%20Existing%203D%20Gaussian%20Splatting%20%283DGS%29%20super-resolution%20methods%20typically%0Aperform%20high-resolution%20%28HR%29%20rendering%20of%20fixed%20scale%20factors%2C%20making%20them%0Aimpractical%20for%20resource-limited%20scenarios.%20Directly%20rendering%20arbitrary-scale%0AHR%20views%20with%20vanilla%203DGS%20introduces%20aliasing%20artifacts%20due%20to%20the%20lack%20of%0Ascale-aware%20rendering%20ability%2C%20while%20adding%20a%20post-processing%20upsampler%20for%0A3DGS%20complicates%20the%20framework%20and%20reduces%20rendering%20efficiency.%20To%20tackle%0Athese%20issues%2C%20we%20build%20an%20integrated%20framework%20that%20incorporates%20scale-aware%0Arendering%2C%20generative%20prior-guided%20optimization%2C%20and%20progressive%0Asuper-resolving%20to%20enable%203D%20Gaussian%20super-resolution%20of%20arbitrary%20scale%0Afactors%20with%20a%20single%203D%20model.%20Notably%2C%20our%20approach%20supports%20both%20integer%20and%0Anon-integer%20scale%20rendering%20to%20provide%20more%20flexibility.%20Extensive%20experiments%0Ademonstrate%20the%20effectiveness%20of%20our%20model%20in%20rendering%20high-quality%0Aarbitrary-scale%20HR%20views%20%286.59%20dB%20PSNR%20gain%20over%203DGS%29%20with%20a%20single%20model.%20It%0Apreserves%20structural%20consistency%20with%20LR%20views%20and%20across%20different%20scales%2C%0Awhile%20maintaining%20real-time%20rendering%20speed%20%2885%20FPS%20at%201080p%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16467v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArbitrary-Scale%25203D%2520Gaussian%2520Super-Resolution%26entry.906535625%3DHuimin%2520Zeng%2520and%2520Yue%2520Bai%2520and%2520Yun%2520Fu%26entry.1292438233%3D%2520%2520Existing%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520super-resolution%2520methods%2520typically%250Aperform%2520high-resolution%2520%2528HR%2529%2520rendering%2520of%2520fixed%2520scale%2520factors%252C%2520making%2520them%250Aimpractical%2520for%2520resource-limited%2520scenarios.%2520Directly%2520rendering%2520arbitrary-scale%250AHR%2520views%2520with%2520vanilla%25203DGS%2520introduces%2520aliasing%2520artifacts%2520due%2520to%2520the%2520lack%2520of%250Ascale-aware%2520rendering%2520ability%252C%2520while%2520adding%2520a%2520post-processing%2520upsampler%2520for%250A3DGS%2520complicates%2520the%2520framework%2520and%2520reduces%2520rendering%2520efficiency.%2520To%2520tackle%250Athese%2520issues%252C%2520we%2520build%2520an%2520integrated%2520framework%2520that%2520incorporates%2520scale-aware%250Arendering%252C%2520generative%2520prior-guided%2520optimization%252C%2520and%2520progressive%250Asuper-resolving%2520to%2520enable%25203D%2520Gaussian%2520super-resolution%2520of%2520arbitrary%2520scale%250Afactors%2520with%2520a%2520single%25203D%2520model.%2520Notably%252C%2520our%2520approach%2520supports%2520both%2520integer%2520and%250Anon-integer%2520scale%2520rendering%2520to%2520provide%2520more%2520flexibility.%2520Extensive%2520experiments%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520model%2520in%2520rendering%2520high-quality%250Aarbitrary-scale%2520HR%2520views%2520%25286.59%2520dB%2520PSNR%2520gain%2520over%25203DGS%2529%2520with%2520a%2520single%2520model.%2520It%250Apreserves%2520structural%2520consistency%2520with%2520LR%2520views%2520and%2520across%2520different%2520scales%252C%250Awhile%2520maintaining%2520real-time%2520rendering%2520speed%2520%252885%2520FPS%2520at%25201080p%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16467v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Arbitrary-Scale%203D%20Gaussian%20Super-Resolution&entry.906535625=Huimin%20Zeng%20and%20Yue%20Bai%20and%20Yun%20Fu&entry.1292438233=%20%20Existing%203D%20Gaussian%20Splatting%20%283DGS%29%20super-resolution%20methods%20typically%0Aperform%20high-resolution%20%28HR%29%20rendering%20of%20fixed%20scale%20factors%2C%20making%20them%0Aimpractical%20for%20resource-limited%20scenarios.%20Directly%20rendering%20arbitrary-scale%0AHR%20views%20with%20vanilla%203DGS%20introduces%20aliasing%20artifacts%20due%20to%20the%20lack%20of%0Ascale-aware%20rendering%20ability%2C%20while%20adding%20a%20post-processing%20upsampler%20for%0A3DGS%20complicates%20the%20framework%20and%20reduces%20rendering%20efficiency.%20To%20tackle%0Athese%20issues%2C%20we%20build%20an%20integrated%20framework%20that%20incorporates%20scale-aware%0Arendering%2C%20generative%20prior-guided%20optimization%2C%20and%20progressive%0Asuper-resolving%20to%20enable%203D%20Gaussian%20super-resolution%20of%20arbitrary%20scale%0Afactors%20with%20a%20single%203D%20model.%20Notably%2C%20our%20approach%20supports%20both%20integer%20and%0Anon-integer%20scale%20rendering%20to%20provide%20more%20flexibility.%20Extensive%20experiments%0Ademonstrate%20the%20effectiveness%20of%20our%20model%20in%20rendering%20high-quality%0Aarbitrary-scale%20HR%20views%20%286.59%20dB%20PSNR%20gain%20over%203DGS%29%20with%20a%20single%20model.%20It%0Apreserves%20structural%20consistency%20with%20LR%20views%20and%20across%20different%20scales%2C%0Awhile%20maintaining%20real-time%20rendering%20speed%20%2885%20FPS%20at%201080p%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16467v1&entry.124074799=Read"},
{"title": "GPL-SLAM: A Laser SLAM Framework with Gaussian Process Based Extended\n  Landmarks", "author": "Ali Emre Balc\u0131 and Erhan Ege Keyvan and Emre \u00d6zkan", "abstract": "  We present a novel Simultaneous Localization and Mapping (SLAM) method that\nemploys Gaussian Process (GP) based landmark (object) representations. Instead\nof conventional grid maps or point cloud registration, we model the environment\non a per object basis using GP based contour representations. These contours\nare updated online through a recursive scheme, enabling efficient memory usage.\nThe SLAM problem is formulated within a fully Bayesian framework, allowing\njoint inference over the robot pose and object based map. This representation\nprovides semantic information such as the number of objects and their areas,\nwhile also supporting probabilistic measurement to object associations.\nFurthermore, the GP based contours yield confidence bounds on object shapes,\noffering valuable information for downstream tasks like safe navigation and\nexploration. We validate our method on synthetic and real world experiments,\nand show that it delivers accurate localization and mapping performance across\ndiverse structured environments.\n", "link": "http://arxiv.org/abs/2508.16459v1", "date": "2025-08-22", "relevancy": 3.1573, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6911}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6226}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5807}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GPL-SLAM%3A%20A%20Laser%20SLAM%20Framework%20with%20Gaussian%20Process%20Based%20Extended%0A%20%20Landmarks&body=Title%3A%20GPL-SLAM%3A%20A%20Laser%20SLAM%20Framework%20with%20Gaussian%20Process%20Based%20Extended%0A%20%20Landmarks%0AAuthor%3A%20Ali%20Emre%20Balc%C4%B1%20and%20Erhan%20Ege%20Keyvan%20and%20Emre%20%C3%96zkan%0AAbstract%3A%20%20%20We%20present%20a%20novel%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20method%20that%0Aemploys%20Gaussian%20Process%20%28GP%29%20based%20landmark%20%28object%29%20representations.%20Instead%0Aof%20conventional%20grid%20maps%20or%20point%20cloud%20registration%2C%20we%20model%20the%20environment%0Aon%20a%20per%20object%20basis%20using%20GP%20based%20contour%20representations.%20These%20contours%0Aare%20updated%20online%20through%20a%20recursive%20scheme%2C%20enabling%20efficient%20memory%20usage.%0AThe%20SLAM%20problem%20is%20formulated%20within%20a%20fully%20Bayesian%20framework%2C%20allowing%0Ajoint%20inference%20over%20the%20robot%20pose%20and%20object%20based%20map.%20This%20representation%0Aprovides%20semantic%20information%20such%20as%20the%20number%20of%20objects%20and%20their%20areas%2C%0Awhile%20also%20supporting%20probabilistic%20measurement%20to%20object%20associations.%0AFurthermore%2C%20the%20GP%20based%20contours%20yield%20confidence%20bounds%20on%20object%20shapes%2C%0Aoffering%20valuable%20information%20for%20downstream%20tasks%20like%20safe%20navigation%20and%0Aexploration.%20We%20validate%20our%20method%20on%20synthetic%20and%20real%20world%20experiments%2C%0Aand%20show%20that%20it%20delivers%20accurate%20localization%20and%20mapping%20performance%20across%0Adiverse%20structured%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16459v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGPL-SLAM%253A%2520A%2520Laser%2520SLAM%2520Framework%2520with%2520Gaussian%2520Process%2520Based%2520Extended%250A%2520%2520Landmarks%26entry.906535625%3DAli%2520Emre%2520Balc%25C4%25B1%2520and%2520Erhan%2520Ege%2520Keyvan%2520and%2520Emre%2520%25C3%2596zkan%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520Simultaneous%2520Localization%2520and%2520Mapping%2520%2528SLAM%2529%2520method%2520that%250Aemploys%2520Gaussian%2520Process%2520%2528GP%2529%2520based%2520landmark%2520%2528object%2529%2520representations.%2520Instead%250Aof%2520conventional%2520grid%2520maps%2520or%2520point%2520cloud%2520registration%252C%2520we%2520model%2520the%2520environment%250Aon%2520a%2520per%2520object%2520basis%2520using%2520GP%2520based%2520contour%2520representations.%2520These%2520contours%250Aare%2520updated%2520online%2520through%2520a%2520recursive%2520scheme%252C%2520enabling%2520efficient%2520memory%2520usage.%250AThe%2520SLAM%2520problem%2520is%2520formulated%2520within%2520a%2520fully%2520Bayesian%2520framework%252C%2520allowing%250Ajoint%2520inference%2520over%2520the%2520robot%2520pose%2520and%2520object%2520based%2520map.%2520This%2520representation%250Aprovides%2520semantic%2520information%2520such%2520as%2520the%2520number%2520of%2520objects%2520and%2520their%2520areas%252C%250Awhile%2520also%2520supporting%2520probabilistic%2520measurement%2520to%2520object%2520associations.%250AFurthermore%252C%2520the%2520GP%2520based%2520contours%2520yield%2520confidence%2520bounds%2520on%2520object%2520shapes%252C%250Aoffering%2520valuable%2520information%2520for%2520downstream%2520tasks%2520like%2520safe%2520navigation%2520and%250Aexploration.%2520We%2520validate%2520our%2520method%2520on%2520synthetic%2520and%2520real%2520world%2520experiments%252C%250Aand%2520show%2520that%2520it%2520delivers%2520accurate%2520localization%2520and%2520mapping%2520performance%2520across%250Adiverse%2520structured%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16459v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GPL-SLAM%3A%20A%20Laser%20SLAM%20Framework%20with%20Gaussian%20Process%20Based%20Extended%0A%20%20Landmarks&entry.906535625=Ali%20Emre%20Balc%C4%B1%20and%20Erhan%20Ege%20Keyvan%20and%20Emre%20%C3%96zkan&entry.1292438233=%20%20We%20present%20a%20novel%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20method%20that%0Aemploys%20Gaussian%20Process%20%28GP%29%20based%20landmark%20%28object%29%20representations.%20Instead%0Aof%20conventional%20grid%20maps%20or%20point%20cloud%20registration%2C%20we%20model%20the%20environment%0Aon%20a%20per%20object%20basis%20using%20GP%20based%20contour%20representations.%20These%20contours%0Aare%20updated%20online%20through%20a%20recursive%20scheme%2C%20enabling%20efficient%20memory%20usage.%0AThe%20SLAM%20problem%20is%20formulated%20within%20a%20fully%20Bayesian%20framework%2C%20allowing%0Ajoint%20inference%20over%20the%20robot%20pose%20and%20object%20based%20map.%20This%20representation%0Aprovides%20semantic%20information%20such%20as%20the%20number%20of%20objects%20and%20their%20areas%2C%0Awhile%20also%20supporting%20probabilistic%20measurement%20to%20object%20associations.%0AFurthermore%2C%20the%20GP%20based%20contours%20yield%20confidence%20bounds%20on%20object%20shapes%2C%0Aoffering%20valuable%20information%20for%20downstream%20tasks%20like%20safe%20navigation%20and%0Aexploration.%20We%20validate%20our%20method%20on%20synthetic%20and%20real%20world%20experiments%2C%0Aand%20show%20that%20it%20delivers%20accurate%20localization%20and%20mapping%20performance%20across%0Adiverse%20structured%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16459v1&entry.124074799=Read"},
{"title": "Audio2Face-3D: Audio-driven Realistic Facial Animation For Digital\n  Avatars", "author": " NVIDIA and  : and Chaeyeon Chung and Ilya Fedorov and Michael Huang and Aleksey Karmanov and Dmitry Korobchenko and Roger Ribera and Yeongho Seol", "abstract": "  Audio-driven facial animation presents an effective solution for animating\ndigital avatars. In this paper, we detail the technical aspects of NVIDIA\nAudio2Face-3D, including data acquisition, network architecture, retargeting\nmethodology, evaluation metrics, and use cases. Audio2Face-3D system enables\nreal-time interaction between human users and interactive avatars, facilitating\nfacial animation authoring for game characters. To assist digital avatar\ncreators and game developers in generating realistic facial animations, we have\nopen-sourced Audio2Face-3D networks, SDK, training framework, and example\ndataset.\n", "link": "http://arxiv.org/abs/2508.16401v1", "date": "2025-08-22", "relevancy": 3.1262, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6465}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6465}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5828}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Audio2Face-3D%3A%20Audio-driven%20Realistic%20Facial%20Animation%20For%20Digital%0A%20%20Avatars&body=Title%3A%20Audio2Face-3D%3A%20Audio-driven%20Realistic%20Facial%20Animation%20For%20Digital%0A%20%20Avatars%0AAuthor%3A%20%20NVIDIA%20and%20%20%3A%20and%20Chaeyeon%20Chung%20and%20Ilya%20Fedorov%20and%20Michael%20Huang%20and%20Aleksey%20Karmanov%20and%20Dmitry%20Korobchenko%20and%20Roger%20Ribera%20and%20Yeongho%20Seol%0AAbstract%3A%20%20%20Audio-driven%20facial%20animation%20presents%20an%20effective%20solution%20for%20animating%0Adigital%20avatars.%20In%20this%20paper%2C%20we%20detail%20the%20technical%20aspects%20of%20NVIDIA%0AAudio2Face-3D%2C%20including%20data%20acquisition%2C%20network%20architecture%2C%20retargeting%0Amethodology%2C%20evaluation%20metrics%2C%20and%20use%20cases.%20Audio2Face-3D%20system%20enables%0Areal-time%20interaction%20between%20human%20users%20and%20interactive%20avatars%2C%20facilitating%0Afacial%20animation%20authoring%20for%20game%20characters.%20To%20assist%20digital%20avatar%0Acreators%20and%20game%20developers%20in%20generating%20realistic%20facial%20animations%2C%20we%20have%0Aopen-sourced%20Audio2Face-3D%20networks%2C%20SDK%2C%20training%20framework%2C%20and%20example%0Adataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16401v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAudio2Face-3D%253A%2520Audio-driven%2520Realistic%2520Facial%2520Animation%2520For%2520Digital%250A%2520%2520Avatars%26entry.906535625%3D%2520NVIDIA%2520and%2520%2520%253A%2520and%2520Chaeyeon%2520Chung%2520and%2520Ilya%2520Fedorov%2520and%2520Michael%2520Huang%2520and%2520Aleksey%2520Karmanov%2520and%2520Dmitry%2520Korobchenko%2520and%2520Roger%2520Ribera%2520and%2520Yeongho%2520Seol%26entry.1292438233%3D%2520%2520Audio-driven%2520facial%2520animation%2520presents%2520an%2520effective%2520solution%2520for%2520animating%250Adigital%2520avatars.%2520In%2520this%2520paper%252C%2520we%2520detail%2520the%2520technical%2520aspects%2520of%2520NVIDIA%250AAudio2Face-3D%252C%2520including%2520data%2520acquisition%252C%2520network%2520architecture%252C%2520retargeting%250Amethodology%252C%2520evaluation%2520metrics%252C%2520and%2520use%2520cases.%2520Audio2Face-3D%2520system%2520enables%250Areal-time%2520interaction%2520between%2520human%2520users%2520and%2520interactive%2520avatars%252C%2520facilitating%250Afacial%2520animation%2520authoring%2520for%2520game%2520characters.%2520To%2520assist%2520digital%2520avatar%250Acreators%2520and%2520game%2520developers%2520in%2520generating%2520realistic%2520facial%2520animations%252C%2520we%2520have%250Aopen-sourced%2520Audio2Face-3D%2520networks%252C%2520SDK%252C%2520training%2520framework%252C%2520and%2520example%250Adataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16401v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Audio2Face-3D%3A%20Audio-driven%20Realistic%20Facial%20Animation%20For%20Digital%0A%20%20Avatars&entry.906535625=%20NVIDIA%20and%20%20%3A%20and%20Chaeyeon%20Chung%20and%20Ilya%20Fedorov%20and%20Michael%20Huang%20and%20Aleksey%20Karmanov%20and%20Dmitry%20Korobchenko%20and%20Roger%20Ribera%20and%20Yeongho%20Seol&entry.1292438233=%20%20Audio-driven%20facial%20animation%20presents%20an%20effective%20solution%20for%20animating%0Adigital%20avatars.%20In%20this%20paper%2C%20we%20detail%20the%20technical%20aspects%20of%20NVIDIA%0AAudio2Face-3D%2C%20including%20data%20acquisition%2C%20network%20architecture%2C%20retargeting%0Amethodology%2C%20evaluation%20metrics%2C%20and%20use%20cases.%20Audio2Face-3D%20system%20enables%0Areal-time%20interaction%20between%20human%20users%20and%20interactive%20avatars%2C%20facilitating%0Afacial%20animation%20authoring%20for%20game%20characters.%20To%20assist%20digital%20avatar%0Acreators%20and%20game%20developers%20in%20generating%20realistic%20facial%20animations%2C%20we%20have%0Aopen-sourced%20Audio2Face-3D%20networks%2C%20SDK%2C%20training%20framework%2C%20and%20example%0Adataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16401v1&entry.124074799=Read"},
{"title": "HAMSt3R: Human-Aware Multi-view Stereo 3D Reconstruction", "author": "Sara Rojas and Matthieu Armando and Bernard Ghamen and Philippe Weinzaepfel and Vincent Leroy and Gregory Rogez", "abstract": "  Recovering the 3D geometry of a scene from a sparse set of uncalibrated\nimages is a long-standing problem in computer vision. While recent\nlearning-based approaches such as DUSt3R and MASt3R have demonstrated\nimpressive results by directly predicting dense scene geometry, they are\nprimarily trained on outdoor scenes with static environments and struggle to\nhandle human-centric scenarios. In this work, we introduce HAMSt3R, an\nextension of MASt3R for joint human and scene 3D reconstruction from sparse,\nuncalibrated multi-view images. First, we exploit DUNE, a strong image encoder\nobtained by distilling, among others, the encoders from MASt3R and from a\nstate-of-the-art Human Mesh Recovery (HMR) model, multi-HMR, for a better\nunderstanding of scene geometry and human bodies. Our method then incorporates\nadditional network heads to segment people, estimate dense correspondences via\nDensePose, and predict depth in human-centric environments, enabling a more\ncomprehensive 3D reconstruction. By leveraging the outputs of our different\nheads, HAMSt3R produces a dense point map enriched with human semantic\ninformation in 3D. Unlike existing methods that rely on complex optimization\npipelines, our approach is fully feed-forward and efficient, making it suitable\nfor real-world applications. We evaluate our model on EgoHumans and EgoExo4D,\ntwo challenging benchmarks con taining diverse human-centric scenarios.\nAdditionally, we validate its generalization to traditional multi-view stereo\nand multi-view pose regression tasks. Our results demonstrate that our method\ncan reconstruct humans effectively while preserving strong performance in\ngeneral 3D reconstruction tasks, bridging the gap between human and scene\nunderstanding in 3D vision.\n", "link": "http://arxiv.org/abs/2508.16433v1", "date": "2025-08-22", "relevancy": 3.1175, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6471}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6138}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6097}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HAMSt3R%3A%20Human-Aware%20Multi-view%20Stereo%203D%20Reconstruction&body=Title%3A%20HAMSt3R%3A%20Human-Aware%20Multi-view%20Stereo%203D%20Reconstruction%0AAuthor%3A%20Sara%20Rojas%20and%20Matthieu%20Armando%20and%20Bernard%20Ghamen%20and%20Philippe%20Weinzaepfel%20and%20Vincent%20Leroy%20and%20Gregory%20Rogez%0AAbstract%3A%20%20%20Recovering%20the%203D%20geometry%20of%20a%20scene%20from%20a%20sparse%20set%20of%20uncalibrated%0Aimages%20is%20a%20long-standing%20problem%20in%20computer%20vision.%20While%20recent%0Alearning-based%20approaches%20such%20as%20DUSt3R%20and%20MASt3R%20have%20demonstrated%0Aimpressive%20results%20by%20directly%20predicting%20dense%20scene%20geometry%2C%20they%20are%0Aprimarily%20trained%20on%20outdoor%20scenes%20with%20static%20environments%20and%20struggle%20to%0Ahandle%20human-centric%20scenarios.%20In%20this%20work%2C%20we%20introduce%20HAMSt3R%2C%20an%0Aextension%20of%20MASt3R%20for%20joint%20human%20and%20scene%203D%20reconstruction%20from%20sparse%2C%0Auncalibrated%20multi-view%20images.%20First%2C%20we%20exploit%20DUNE%2C%20a%20strong%20image%20encoder%0Aobtained%20by%20distilling%2C%20among%20others%2C%20the%20encoders%20from%20MASt3R%20and%20from%20a%0Astate-of-the-art%20Human%20Mesh%20Recovery%20%28HMR%29%20model%2C%20multi-HMR%2C%20for%20a%20better%0Aunderstanding%20of%20scene%20geometry%20and%20human%20bodies.%20Our%20method%20then%20incorporates%0Aadditional%20network%20heads%20to%20segment%20people%2C%20estimate%20dense%20correspondences%20via%0ADensePose%2C%20and%20predict%20depth%20in%20human-centric%20environments%2C%20enabling%20a%20more%0Acomprehensive%203D%20reconstruction.%20By%20leveraging%20the%20outputs%20of%20our%20different%0Aheads%2C%20HAMSt3R%20produces%20a%20dense%20point%20map%20enriched%20with%20human%20semantic%0Ainformation%20in%203D.%20Unlike%20existing%20methods%20that%20rely%20on%20complex%20optimization%0Apipelines%2C%20our%20approach%20is%20fully%20feed-forward%20and%20efficient%2C%20making%20it%20suitable%0Afor%20real-world%20applications.%20We%20evaluate%20our%20model%20on%20EgoHumans%20and%20EgoExo4D%2C%0Atwo%20challenging%20benchmarks%20con%20taining%20diverse%20human-centric%20scenarios.%0AAdditionally%2C%20we%20validate%20its%20generalization%20to%20traditional%20multi-view%20stereo%0Aand%20multi-view%20pose%20regression%20tasks.%20Our%20results%20demonstrate%20that%20our%20method%0Acan%20reconstruct%20humans%20effectively%20while%20preserving%20strong%20performance%20in%0Ageneral%203D%20reconstruction%20tasks%2C%20bridging%20the%20gap%20between%20human%20and%20scene%0Aunderstanding%20in%203D%20vision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16433v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHAMSt3R%253A%2520Human-Aware%2520Multi-view%2520Stereo%25203D%2520Reconstruction%26entry.906535625%3DSara%2520Rojas%2520and%2520Matthieu%2520Armando%2520and%2520Bernard%2520Ghamen%2520and%2520Philippe%2520Weinzaepfel%2520and%2520Vincent%2520Leroy%2520and%2520Gregory%2520Rogez%26entry.1292438233%3D%2520%2520Recovering%2520the%25203D%2520geometry%2520of%2520a%2520scene%2520from%2520a%2520sparse%2520set%2520of%2520uncalibrated%250Aimages%2520is%2520a%2520long-standing%2520problem%2520in%2520computer%2520vision.%2520While%2520recent%250Alearning-based%2520approaches%2520such%2520as%2520DUSt3R%2520and%2520MASt3R%2520have%2520demonstrated%250Aimpressive%2520results%2520by%2520directly%2520predicting%2520dense%2520scene%2520geometry%252C%2520they%2520are%250Aprimarily%2520trained%2520on%2520outdoor%2520scenes%2520with%2520static%2520environments%2520and%2520struggle%2520to%250Ahandle%2520human-centric%2520scenarios.%2520In%2520this%2520work%252C%2520we%2520introduce%2520HAMSt3R%252C%2520an%250Aextension%2520of%2520MASt3R%2520for%2520joint%2520human%2520and%2520scene%25203D%2520reconstruction%2520from%2520sparse%252C%250Auncalibrated%2520multi-view%2520images.%2520First%252C%2520we%2520exploit%2520DUNE%252C%2520a%2520strong%2520image%2520encoder%250Aobtained%2520by%2520distilling%252C%2520among%2520others%252C%2520the%2520encoders%2520from%2520MASt3R%2520and%2520from%2520a%250Astate-of-the-art%2520Human%2520Mesh%2520Recovery%2520%2528HMR%2529%2520model%252C%2520multi-HMR%252C%2520for%2520a%2520better%250Aunderstanding%2520of%2520scene%2520geometry%2520and%2520human%2520bodies.%2520Our%2520method%2520then%2520incorporates%250Aadditional%2520network%2520heads%2520to%2520segment%2520people%252C%2520estimate%2520dense%2520correspondences%2520via%250ADensePose%252C%2520and%2520predict%2520depth%2520in%2520human-centric%2520environments%252C%2520enabling%2520a%2520more%250Acomprehensive%25203D%2520reconstruction.%2520By%2520leveraging%2520the%2520outputs%2520of%2520our%2520different%250Aheads%252C%2520HAMSt3R%2520produces%2520a%2520dense%2520point%2520map%2520enriched%2520with%2520human%2520semantic%250Ainformation%2520in%25203D.%2520Unlike%2520existing%2520methods%2520that%2520rely%2520on%2520complex%2520optimization%250Apipelines%252C%2520our%2520approach%2520is%2520fully%2520feed-forward%2520and%2520efficient%252C%2520making%2520it%2520suitable%250Afor%2520real-world%2520applications.%2520We%2520evaluate%2520our%2520model%2520on%2520EgoHumans%2520and%2520EgoExo4D%252C%250Atwo%2520challenging%2520benchmarks%2520con%2520taining%2520diverse%2520human-centric%2520scenarios.%250AAdditionally%252C%2520we%2520validate%2520its%2520generalization%2520to%2520traditional%2520multi-view%2520stereo%250Aand%2520multi-view%2520pose%2520regression%2520tasks.%2520Our%2520results%2520demonstrate%2520that%2520our%2520method%250Acan%2520reconstruct%2520humans%2520effectively%2520while%2520preserving%2520strong%2520performance%2520in%250Ageneral%25203D%2520reconstruction%2520tasks%252C%2520bridging%2520the%2520gap%2520between%2520human%2520and%2520scene%250Aunderstanding%2520in%25203D%2520vision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16433v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HAMSt3R%3A%20Human-Aware%20Multi-view%20Stereo%203D%20Reconstruction&entry.906535625=Sara%20Rojas%20and%20Matthieu%20Armando%20and%20Bernard%20Ghamen%20and%20Philippe%20Weinzaepfel%20and%20Vincent%20Leroy%20and%20Gregory%20Rogez&entry.1292438233=%20%20Recovering%20the%203D%20geometry%20of%20a%20scene%20from%20a%20sparse%20set%20of%20uncalibrated%0Aimages%20is%20a%20long-standing%20problem%20in%20computer%20vision.%20While%20recent%0Alearning-based%20approaches%20such%20as%20DUSt3R%20and%20MASt3R%20have%20demonstrated%0Aimpressive%20results%20by%20directly%20predicting%20dense%20scene%20geometry%2C%20they%20are%0Aprimarily%20trained%20on%20outdoor%20scenes%20with%20static%20environments%20and%20struggle%20to%0Ahandle%20human-centric%20scenarios.%20In%20this%20work%2C%20we%20introduce%20HAMSt3R%2C%20an%0Aextension%20of%20MASt3R%20for%20joint%20human%20and%20scene%203D%20reconstruction%20from%20sparse%2C%0Auncalibrated%20multi-view%20images.%20First%2C%20we%20exploit%20DUNE%2C%20a%20strong%20image%20encoder%0Aobtained%20by%20distilling%2C%20among%20others%2C%20the%20encoders%20from%20MASt3R%20and%20from%20a%0Astate-of-the-art%20Human%20Mesh%20Recovery%20%28HMR%29%20model%2C%20multi-HMR%2C%20for%20a%20better%0Aunderstanding%20of%20scene%20geometry%20and%20human%20bodies.%20Our%20method%20then%20incorporates%0Aadditional%20network%20heads%20to%20segment%20people%2C%20estimate%20dense%20correspondences%20via%0ADensePose%2C%20and%20predict%20depth%20in%20human-centric%20environments%2C%20enabling%20a%20more%0Acomprehensive%203D%20reconstruction.%20By%20leveraging%20the%20outputs%20of%20our%20different%0Aheads%2C%20HAMSt3R%20produces%20a%20dense%20point%20map%20enriched%20with%20human%20semantic%0Ainformation%20in%203D.%20Unlike%20existing%20methods%20that%20rely%20on%20complex%20optimization%0Apipelines%2C%20our%20approach%20is%20fully%20feed-forward%20and%20efficient%2C%20making%20it%20suitable%0Afor%20real-world%20applications.%20We%20evaluate%20our%20model%20on%20EgoHumans%20and%20EgoExo4D%2C%0Atwo%20challenging%20benchmarks%20con%20taining%20diverse%20human-centric%20scenarios.%0AAdditionally%2C%20we%20validate%20its%20generalization%20to%20traditional%20multi-view%20stereo%0Aand%20multi-view%20pose%20regression%20tasks.%20Our%20results%20demonstrate%20that%20our%20method%0Acan%20reconstruct%20humans%20effectively%20while%20preserving%20strong%20performance%20in%0Ageneral%203D%20reconstruction%20tasks%2C%20bridging%20the%20gap%20between%20human%20and%20scene%0Aunderstanding%20in%203D%20vision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16433v1&entry.124074799=Read"},
{"title": "Modular Embedding Recomposition for Incremental Learning", "author": "Aniello Panariello and Emanuele Frascaroli and Pietro Buzzega and Lorenzo Bonicelli and Angelo Porrello and Simone Calderara", "abstract": "  The advent of pre-trained Vision-Language Models (VLMs) has significantly\ntransformed Continual Learning (CL), mainly due to their zero-shot\nclassification abilities. Such proficiency makes VLMs well-suited for\nreal-world applications, enabling robust performance on novel unseen classes\nwithout requiring adaptation. However, fine-tuning remains essential when\ndownstream tasks deviate significantly from the pre-training domain. Prior CL\napproaches primarily focus on preserving the zero-shot capabilities of VLMs\nduring incremental fine-tuning on a downstream task. We take a step further by\ndevising an approach that transforms preservation into enhancement of the\nzero-shot capabilities of VLMs. Our approach, named MoDular Embedding\nRecomposition (MoDER), introduces a modular framework that trains multiple\ntextual experts, each specialized in a single seen class, and stores them in a\nfoundational hub. At inference time, for each unseen class, we query the hub\nand compose the retrieved experts to synthesize a refined prototype that\nimproves classification. We show the effectiveness of our method across two\npopular zero-shot incremental protocols, Class-IL and MTIL, comprising a total\nof 14 datasets. The codebase is available at\nhttps://github.com/aimagelab/mammoth.\n", "link": "http://arxiv.org/abs/2508.16463v1", "date": "2025-08-22", "relevancy": 2.9426, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6198}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5729}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modular%20Embedding%20Recomposition%20for%20Incremental%20Learning&body=Title%3A%20Modular%20Embedding%20Recomposition%20for%20Incremental%20Learning%0AAuthor%3A%20Aniello%20Panariello%20and%20Emanuele%20Frascaroli%20and%20Pietro%20Buzzega%20and%20Lorenzo%20Bonicelli%20and%20Angelo%20Porrello%20and%20Simone%20Calderara%0AAbstract%3A%20%20%20The%20advent%20of%20pre-trained%20Vision-Language%20Models%20%28VLMs%29%20has%20significantly%0Atransformed%20Continual%20Learning%20%28CL%29%2C%20mainly%20due%20to%20their%20zero-shot%0Aclassification%20abilities.%20Such%20proficiency%20makes%20VLMs%20well-suited%20for%0Areal-world%20applications%2C%20enabling%20robust%20performance%20on%20novel%20unseen%20classes%0Awithout%20requiring%20adaptation.%20However%2C%20fine-tuning%20remains%20essential%20when%0Adownstream%20tasks%20deviate%20significantly%20from%20the%20pre-training%20domain.%20Prior%20CL%0Aapproaches%20primarily%20focus%20on%20preserving%20the%20zero-shot%20capabilities%20of%20VLMs%0Aduring%20incremental%20fine-tuning%20on%20a%20downstream%20task.%20We%20take%20a%20step%20further%20by%0Adevising%20an%20approach%20that%20transforms%20preservation%20into%20enhancement%20of%20the%0Azero-shot%20capabilities%20of%20VLMs.%20Our%20approach%2C%20named%20MoDular%20Embedding%0ARecomposition%20%28MoDER%29%2C%20introduces%20a%20modular%20framework%20that%20trains%20multiple%0Atextual%20experts%2C%20each%20specialized%20in%20a%20single%20seen%20class%2C%20and%20stores%20them%20in%20a%0Afoundational%20hub.%20At%20inference%20time%2C%20for%20each%20unseen%20class%2C%20we%20query%20the%20hub%0Aand%20compose%20the%20retrieved%20experts%20to%20synthesize%20a%20refined%20prototype%20that%0Aimproves%20classification.%20We%20show%20the%20effectiveness%20of%20our%20method%20across%20two%0Apopular%20zero-shot%20incremental%20protocols%2C%20Class-IL%20and%20MTIL%2C%20comprising%20a%20total%0Aof%2014%20datasets.%20The%20codebase%20is%20available%20at%0Ahttps%3A//github.com/aimagelab/mammoth.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16463v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModular%2520Embedding%2520Recomposition%2520for%2520Incremental%2520Learning%26entry.906535625%3DAniello%2520Panariello%2520and%2520Emanuele%2520Frascaroli%2520and%2520Pietro%2520Buzzega%2520and%2520Lorenzo%2520Bonicelli%2520and%2520Angelo%2520Porrello%2520and%2520Simone%2520Calderara%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520pre-trained%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520has%2520significantly%250Atransformed%2520Continual%2520Learning%2520%2528CL%2529%252C%2520mainly%2520due%2520to%2520their%2520zero-shot%250Aclassification%2520abilities.%2520Such%2520proficiency%2520makes%2520VLMs%2520well-suited%2520for%250Areal-world%2520applications%252C%2520enabling%2520robust%2520performance%2520on%2520novel%2520unseen%2520classes%250Awithout%2520requiring%2520adaptation.%2520However%252C%2520fine-tuning%2520remains%2520essential%2520when%250Adownstream%2520tasks%2520deviate%2520significantly%2520from%2520the%2520pre-training%2520domain.%2520Prior%2520CL%250Aapproaches%2520primarily%2520focus%2520on%2520preserving%2520the%2520zero-shot%2520capabilities%2520of%2520VLMs%250Aduring%2520incremental%2520fine-tuning%2520on%2520a%2520downstream%2520task.%2520We%2520take%2520a%2520step%2520further%2520by%250Adevising%2520an%2520approach%2520that%2520transforms%2520preservation%2520into%2520enhancement%2520of%2520the%250Azero-shot%2520capabilities%2520of%2520VLMs.%2520Our%2520approach%252C%2520named%2520MoDular%2520Embedding%250ARecomposition%2520%2528MoDER%2529%252C%2520introduces%2520a%2520modular%2520framework%2520that%2520trains%2520multiple%250Atextual%2520experts%252C%2520each%2520specialized%2520in%2520a%2520single%2520seen%2520class%252C%2520and%2520stores%2520them%2520in%2520a%250Afoundational%2520hub.%2520At%2520inference%2520time%252C%2520for%2520each%2520unseen%2520class%252C%2520we%2520query%2520the%2520hub%250Aand%2520compose%2520the%2520retrieved%2520experts%2520to%2520synthesize%2520a%2520refined%2520prototype%2520that%250Aimproves%2520classification.%2520We%2520show%2520the%2520effectiveness%2520of%2520our%2520method%2520across%2520two%250Apopular%2520zero-shot%2520incremental%2520protocols%252C%2520Class-IL%2520and%2520MTIL%252C%2520comprising%2520a%2520total%250Aof%252014%2520datasets.%2520The%2520codebase%2520is%2520available%2520at%250Ahttps%253A//github.com/aimagelab/mammoth.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16463v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modular%20Embedding%20Recomposition%20for%20Incremental%20Learning&entry.906535625=Aniello%20Panariello%20and%20Emanuele%20Frascaroli%20and%20Pietro%20Buzzega%20and%20Lorenzo%20Bonicelli%20and%20Angelo%20Porrello%20and%20Simone%20Calderara&entry.1292438233=%20%20The%20advent%20of%20pre-trained%20Vision-Language%20Models%20%28VLMs%29%20has%20significantly%0Atransformed%20Continual%20Learning%20%28CL%29%2C%20mainly%20due%20to%20their%20zero-shot%0Aclassification%20abilities.%20Such%20proficiency%20makes%20VLMs%20well-suited%20for%0Areal-world%20applications%2C%20enabling%20robust%20performance%20on%20novel%20unseen%20classes%0Awithout%20requiring%20adaptation.%20However%2C%20fine-tuning%20remains%20essential%20when%0Adownstream%20tasks%20deviate%20significantly%20from%20the%20pre-training%20domain.%20Prior%20CL%0Aapproaches%20primarily%20focus%20on%20preserving%20the%20zero-shot%20capabilities%20of%20VLMs%0Aduring%20incremental%20fine-tuning%20on%20a%20downstream%20task.%20We%20take%20a%20step%20further%20by%0Adevising%20an%20approach%20that%20transforms%20preservation%20into%20enhancement%20of%20the%0Azero-shot%20capabilities%20of%20VLMs.%20Our%20approach%2C%20named%20MoDular%20Embedding%0ARecomposition%20%28MoDER%29%2C%20introduces%20a%20modular%20framework%20that%20trains%20multiple%0Atextual%20experts%2C%20each%20specialized%20in%20a%20single%20seen%20class%2C%20and%20stores%20them%20in%20a%0Afoundational%20hub.%20At%20inference%20time%2C%20for%20each%20unseen%20class%2C%20we%20query%20the%20hub%0Aand%20compose%20the%20retrieved%20experts%20to%20synthesize%20a%20refined%20prototype%20that%0Aimproves%20classification.%20We%20show%20the%20effectiveness%20of%20our%20method%20across%20two%0Apopular%20zero-shot%20incremental%20protocols%2C%20Class-IL%20and%20MTIL%2C%20comprising%20a%20total%0Aof%2014%20datasets.%20The%20codebase%20is%20available%20at%0Ahttps%3A//github.com/aimagelab/mammoth.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16463v1&entry.124074799=Read"},
{"title": "HOSt3R: Keypoint-free Hand-Object 3D Reconstruction from RGB images", "author": "Anilkumar Swamy and Vincent Leroy and Philippe Weinzaepfel and Jean-S\u00e9bastien Franco and Gr\u00e9gory Rogez", "abstract": "  Hand-object 3D reconstruction has become increasingly important for\napplications in human-robot interaction and immersive AR/VR experiences. A\ncommon approach for object-agnostic hand-object reconstruction from RGB\nsequences involves a two-stage pipeline: hand-object 3D tracking followed by\nmulti-view 3D reconstruction. However, existing methods rely on keypoint\ndetection techniques, such as Structure from Motion (SfM) and hand-keypoint\noptimization, which struggle with diverse object geometries, weak textures, and\nmutual hand-object occlusions, limiting scalability and generalization. As a\nkey enabler to generic and seamless, non-intrusive applicability, we propose in\nthis work a robust, keypoint detector-free approach to estimating hand-object\n3D transformations from monocular motion video/images. We further integrate\nthis with a multi-view reconstruction pipeline to accurately recover\nhand-object 3D shape. Our method, named HOSt3R, is unconstrained, does not rely\non pre-scanned object templates or camera intrinsics, and reaches\nstate-of-the-art performance for the tasks of object-agnostic hand-object 3D\ntransformation and shape estimation on the SHOWMe benchmark. We also experiment\non sequences from the HO3D dataset, demonstrating generalization to unseen\nobject categories.\n", "link": "http://arxiv.org/abs/2508.16465v1", "date": "2025-08-22", "relevancy": 2.9042, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5878}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5778}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HOSt3R%3A%20Keypoint-free%20Hand-Object%203D%20Reconstruction%20from%20RGB%20images&body=Title%3A%20HOSt3R%3A%20Keypoint-free%20Hand-Object%203D%20Reconstruction%20from%20RGB%20images%0AAuthor%3A%20Anilkumar%20Swamy%20and%20Vincent%20Leroy%20and%20Philippe%20Weinzaepfel%20and%20Jean-S%C3%A9bastien%20Franco%20and%20Gr%C3%A9gory%20Rogez%0AAbstract%3A%20%20%20Hand-object%203D%20reconstruction%20has%20become%20increasingly%20important%20for%0Aapplications%20in%20human-robot%20interaction%20and%20immersive%20AR/VR%20experiences.%20A%0Acommon%20approach%20for%20object-agnostic%20hand-object%20reconstruction%20from%20RGB%0Asequences%20involves%20a%20two-stage%20pipeline%3A%20hand-object%203D%20tracking%20followed%20by%0Amulti-view%203D%20reconstruction.%20However%2C%20existing%20methods%20rely%20on%20keypoint%0Adetection%20techniques%2C%20such%20as%20Structure%20from%20Motion%20%28SfM%29%20and%20hand-keypoint%0Aoptimization%2C%20which%20struggle%20with%20diverse%20object%20geometries%2C%20weak%20textures%2C%20and%0Amutual%20hand-object%20occlusions%2C%20limiting%20scalability%20and%20generalization.%20As%20a%0Akey%20enabler%20to%20generic%20and%20seamless%2C%20non-intrusive%20applicability%2C%20we%20propose%20in%0Athis%20work%20a%20robust%2C%20keypoint%20detector-free%20approach%20to%20estimating%20hand-object%0A3D%20transformations%20from%20monocular%20motion%20video/images.%20We%20further%20integrate%0Athis%20with%20a%20multi-view%20reconstruction%20pipeline%20to%20accurately%20recover%0Ahand-object%203D%20shape.%20Our%20method%2C%20named%20HOSt3R%2C%20is%20unconstrained%2C%20does%20not%20rely%0Aon%20pre-scanned%20object%20templates%20or%20camera%20intrinsics%2C%20and%20reaches%0Astate-of-the-art%20performance%20for%20the%20tasks%20of%20object-agnostic%20hand-object%203D%0Atransformation%20and%20shape%20estimation%20on%20the%20SHOWMe%20benchmark.%20We%20also%20experiment%0Aon%20sequences%20from%20the%20HO3D%20dataset%2C%20demonstrating%20generalization%20to%20unseen%0Aobject%20categories.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16465v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHOSt3R%253A%2520Keypoint-free%2520Hand-Object%25203D%2520Reconstruction%2520from%2520RGB%2520images%26entry.906535625%3DAnilkumar%2520Swamy%2520and%2520Vincent%2520Leroy%2520and%2520Philippe%2520Weinzaepfel%2520and%2520Jean-S%25C3%25A9bastien%2520Franco%2520and%2520Gr%25C3%25A9gory%2520Rogez%26entry.1292438233%3D%2520%2520Hand-object%25203D%2520reconstruction%2520has%2520become%2520increasingly%2520important%2520for%250Aapplications%2520in%2520human-robot%2520interaction%2520and%2520immersive%2520AR/VR%2520experiences.%2520A%250Acommon%2520approach%2520for%2520object-agnostic%2520hand-object%2520reconstruction%2520from%2520RGB%250Asequences%2520involves%2520a%2520two-stage%2520pipeline%253A%2520hand-object%25203D%2520tracking%2520followed%2520by%250Amulti-view%25203D%2520reconstruction.%2520However%252C%2520existing%2520methods%2520rely%2520on%2520keypoint%250Adetection%2520techniques%252C%2520such%2520as%2520Structure%2520from%2520Motion%2520%2528SfM%2529%2520and%2520hand-keypoint%250Aoptimization%252C%2520which%2520struggle%2520with%2520diverse%2520object%2520geometries%252C%2520weak%2520textures%252C%2520and%250Amutual%2520hand-object%2520occlusions%252C%2520limiting%2520scalability%2520and%2520generalization.%2520As%2520a%250Akey%2520enabler%2520to%2520generic%2520and%2520seamless%252C%2520non-intrusive%2520applicability%252C%2520we%2520propose%2520in%250Athis%2520work%2520a%2520robust%252C%2520keypoint%2520detector-free%2520approach%2520to%2520estimating%2520hand-object%250A3D%2520transformations%2520from%2520monocular%2520motion%2520video/images.%2520We%2520further%2520integrate%250Athis%2520with%2520a%2520multi-view%2520reconstruction%2520pipeline%2520to%2520accurately%2520recover%250Ahand-object%25203D%2520shape.%2520Our%2520method%252C%2520named%2520HOSt3R%252C%2520is%2520unconstrained%252C%2520does%2520not%2520rely%250Aon%2520pre-scanned%2520object%2520templates%2520or%2520camera%2520intrinsics%252C%2520and%2520reaches%250Astate-of-the-art%2520performance%2520for%2520the%2520tasks%2520of%2520object-agnostic%2520hand-object%25203D%250Atransformation%2520and%2520shape%2520estimation%2520on%2520the%2520SHOWMe%2520benchmark.%2520We%2520also%2520experiment%250Aon%2520sequences%2520from%2520the%2520HO3D%2520dataset%252C%2520demonstrating%2520generalization%2520to%2520unseen%250Aobject%2520categories.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16465v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HOSt3R%3A%20Keypoint-free%20Hand-Object%203D%20Reconstruction%20from%20RGB%20images&entry.906535625=Anilkumar%20Swamy%20and%20Vincent%20Leroy%20and%20Philippe%20Weinzaepfel%20and%20Jean-S%C3%A9bastien%20Franco%20and%20Gr%C3%A9gory%20Rogez&entry.1292438233=%20%20Hand-object%203D%20reconstruction%20has%20become%20increasingly%20important%20for%0Aapplications%20in%20human-robot%20interaction%20and%20immersive%20AR/VR%20experiences.%20A%0Acommon%20approach%20for%20object-agnostic%20hand-object%20reconstruction%20from%20RGB%0Asequences%20involves%20a%20two-stage%20pipeline%3A%20hand-object%203D%20tracking%20followed%20by%0Amulti-view%203D%20reconstruction.%20However%2C%20existing%20methods%20rely%20on%20keypoint%0Adetection%20techniques%2C%20such%20as%20Structure%20from%20Motion%20%28SfM%29%20and%20hand-keypoint%0Aoptimization%2C%20which%20struggle%20with%20diverse%20object%20geometries%2C%20weak%20textures%2C%20and%0Amutual%20hand-object%20occlusions%2C%20limiting%20scalability%20and%20generalization.%20As%20a%0Akey%20enabler%20to%20generic%20and%20seamless%2C%20non-intrusive%20applicability%2C%20we%20propose%20in%0Athis%20work%20a%20robust%2C%20keypoint%20detector-free%20approach%20to%20estimating%20hand-object%0A3D%20transformations%20from%20monocular%20motion%20video/images.%20We%20further%20integrate%0Athis%20with%20a%20multi-view%20reconstruction%20pipeline%20to%20accurately%20recover%0Ahand-object%203D%20shape.%20Our%20method%2C%20named%20HOSt3R%2C%20is%20unconstrained%2C%20does%20not%20rely%0Aon%20pre-scanned%20object%20templates%20or%20camera%20intrinsics%2C%20and%20reaches%0Astate-of-the-art%20performance%20for%20the%20tasks%20of%20object-agnostic%20hand-object%203D%0Atransformation%20and%20shape%20estimation%20on%20the%20SHOWMe%20benchmark.%20We%20also%20experiment%0Aon%20sequences%20from%20the%20HO3D%20dataset%2C%20demonstrating%20generalization%20to%20unseen%0Aobject%20categories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16465v1&entry.124074799=Read"},
{"title": "MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds", "author": "Bingquan Dai and Li Ray Luo and Qihong Tang and Jie Wang and Xinyu Lian and Hao Xu and Minghan Qin and Xudong Xu and Bo Dai and Haoqian Wang and Zhaoyang Lyu and Jiangmiao Pang", "abstract": "  Reconstructing 3D objects into editable programs is pivotal for applications\nlike reverse engineering and shape editing. However, existing methods often\nrely on limited domain-specific languages (DSLs) and small-scale datasets,\nrestricting their ability to model complex geometries and structures. To\naddress these challenges, we introduce MeshCoder, a novel framework that\nreconstructs complex 3D objects from point clouds into editable Blender Python\nscripts. We develop a comprehensive set of expressive Blender Python APIs\ncapable of synthesizing intricate geometries. Leveraging these APIs, we\nconstruct a large-scale paired object-code dataset, where the code for each\nobject is decomposed into distinct semantic parts. Subsequently, we train a\nmultimodal large language model (LLM) that translates 3D point cloud into\nexecutable Blender Python scripts. Our approach not only achieves superior\nperformance in shape-to-code reconstruction tasks but also facilitates\nintuitive geometric and topological editing through convenient code\nmodifications. Furthermore, our code-based representation enhances the\nreasoning capabilities of LLMs in 3D shape understanding tasks. Together, these\ncontributions establish MeshCoder as a powerful and flexible solution for\nprogrammatic 3D shape reconstruction and understanding. The project homepage is\navailable at \\href{https://daibingquan.github.io/MeshCoder}{this link}.\n", "link": "http://arxiv.org/abs/2508.14879v2", "date": "2025-08-22", "relevancy": 2.8834, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6246}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5585}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MeshCoder%3A%20LLM-Powered%20Structured%20Mesh%20Code%20Generation%20from%20Point%20Clouds&body=Title%3A%20MeshCoder%3A%20LLM-Powered%20Structured%20Mesh%20Code%20Generation%20from%20Point%20Clouds%0AAuthor%3A%20Bingquan%20Dai%20and%20Li%20Ray%20Luo%20and%20Qihong%20Tang%20and%20Jie%20Wang%20and%20Xinyu%20Lian%20and%20Hao%20Xu%20and%20Minghan%20Qin%20and%20Xudong%20Xu%20and%20Bo%20Dai%20and%20Haoqian%20Wang%20and%20Zhaoyang%20Lyu%20and%20Jiangmiao%20Pang%0AAbstract%3A%20%20%20Reconstructing%203D%20objects%20into%20editable%20programs%20is%20pivotal%20for%20applications%0Alike%20reverse%20engineering%20and%20shape%20editing.%20However%2C%20existing%20methods%20often%0Arely%20on%20limited%20domain-specific%20languages%20%28DSLs%29%20and%20small-scale%20datasets%2C%0Arestricting%20their%20ability%20to%20model%20complex%20geometries%20and%20structures.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20MeshCoder%2C%20a%20novel%20framework%20that%0Areconstructs%20complex%203D%20objects%20from%20point%20clouds%20into%20editable%20Blender%20Python%0Ascripts.%20We%20develop%20a%20comprehensive%20set%20of%20expressive%20Blender%20Python%20APIs%0Acapable%20of%20synthesizing%20intricate%20geometries.%20Leveraging%20these%20APIs%2C%20we%0Aconstruct%20a%20large-scale%20paired%20object-code%20dataset%2C%20where%20the%20code%20for%20each%0Aobject%20is%20decomposed%20into%20distinct%20semantic%20parts.%20Subsequently%2C%20we%20train%20a%0Amultimodal%20large%20language%20model%20%28LLM%29%20that%20translates%203D%20point%20cloud%20into%0Aexecutable%20Blender%20Python%20scripts.%20Our%20approach%20not%20only%20achieves%20superior%0Aperformance%20in%20shape-to-code%20reconstruction%20tasks%20but%20also%20facilitates%0Aintuitive%20geometric%20and%20topological%20editing%20through%20convenient%20code%0Amodifications.%20Furthermore%2C%20our%20code-based%20representation%20enhances%20the%0Areasoning%20capabilities%20of%20LLMs%20in%203D%20shape%20understanding%20tasks.%20Together%2C%20these%0Acontributions%20establish%20MeshCoder%20as%20a%20powerful%20and%20flexible%20solution%20for%0Aprogrammatic%203D%20shape%20reconstruction%20and%20understanding.%20The%20project%20homepage%20is%0Aavailable%20at%20%5Chref%7Bhttps%3A//daibingquan.github.io/MeshCoder%7D%7Bthis%20link%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14879v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeshCoder%253A%2520LLM-Powered%2520Structured%2520Mesh%2520Code%2520Generation%2520from%2520Point%2520Clouds%26entry.906535625%3DBingquan%2520Dai%2520and%2520Li%2520Ray%2520Luo%2520and%2520Qihong%2520Tang%2520and%2520Jie%2520Wang%2520and%2520Xinyu%2520Lian%2520and%2520Hao%2520Xu%2520and%2520Minghan%2520Qin%2520and%2520Xudong%2520Xu%2520and%2520Bo%2520Dai%2520and%2520Haoqian%2520Wang%2520and%2520Zhaoyang%2520Lyu%2520and%2520Jiangmiao%2520Pang%26entry.1292438233%3D%2520%2520Reconstructing%25203D%2520objects%2520into%2520editable%2520programs%2520is%2520pivotal%2520for%2520applications%250Alike%2520reverse%2520engineering%2520and%2520shape%2520editing.%2520However%252C%2520existing%2520methods%2520often%250Arely%2520on%2520limited%2520domain-specific%2520languages%2520%2528DSLs%2529%2520and%2520small-scale%2520datasets%252C%250Arestricting%2520their%2520ability%2520to%2520model%2520complex%2520geometries%2520and%2520structures.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520introduce%2520MeshCoder%252C%2520a%2520novel%2520framework%2520that%250Areconstructs%2520complex%25203D%2520objects%2520from%2520point%2520clouds%2520into%2520editable%2520Blender%2520Python%250Ascripts.%2520We%2520develop%2520a%2520comprehensive%2520set%2520of%2520expressive%2520Blender%2520Python%2520APIs%250Acapable%2520of%2520synthesizing%2520intricate%2520geometries.%2520Leveraging%2520these%2520APIs%252C%2520we%250Aconstruct%2520a%2520large-scale%2520paired%2520object-code%2520dataset%252C%2520where%2520the%2520code%2520for%2520each%250Aobject%2520is%2520decomposed%2520into%2520distinct%2520semantic%2520parts.%2520Subsequently%252C%2520we%2520train%2520a%250Amultimodal%2520large%2520language%2520model%2520%2528LLM%2529%2520that%2520translates%25203D%2520point%2520cloud%2520into%250Aexecutable%2520Blender%2520Python%2520scripts.%2520Our%2520approach%2520not%2520only%2520achieves%2520superior%250Aperformance%2520in%2520shape-to-code%2520reconstruction%2520tasks%2520but%2520also%2520facilitates%250Aintuitive%2520geometric%2520and%2520topological%2520editing%2520through%2520convenient%2520code%250Amodifications.%2520Furthermore%252C%2520our%2520code-based%2520representation%2520enhances%2520the%250Areasoning%2520capabilities%2520of%2520LLMs%2520in%25203D%2520shape%2520understanding%2520tasks.%2520Together%252C%2520these%250Acontributions%2520establish%2520MeshCoder%2520as%2520a%2520powerful%2520and%2520flexible%2520solution%2520for%250Aprogrammatic%25203D%2520shape%2520reconstruction%2520and%2520understanding.%2520The%2520project%2520homepage%2520is%250Aavailable%2520at%2520%255Chref%257Bhttps%253A//daibingquan.github.io/MeshCoder%257D%257Bthis%2520link%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14879v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MeshCoder%3A%20LLM-Powered%20Structured%20Mesh%20Code%20Generation%20from%20Point%20Clouds&entry.906535625=Bingquan%20Dai%20and%20Li%20Ray%20Luo%20and%20Qihong%20Tang%20and%20Jie%20Wang%20and%20Xinyu%20Lian%20and%20Hao%20Xu%20and%20Minghan%20Qin%20and%20Xudong%20Xu%20and%20Bo%20Dai%20and%20Haoqian%20Wang%20and%20Zhaoyang%20Lyu%20and%20Jiangmiao%20Pang&entry.1292438233=%20%20Reconstructing%203D%20objects%20into%20editable%20programs%20is%20pivotal%20for%20applications%0Alike%20reverse%20engineering%20and%20shape%20editing.%20However%2C%20existing%20methods%20often%0Arely%20on%20limited%20domain-specific%20languages%20%28DSLs%29%20and%20small-scale%20datasets%2C%0Arestricting%20their%20ability%20to%20model%20complex%20geometries%20and%20structures.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20MeshCoder%2C%20a%20novel%20framework%20that%0Areconstructs%20complex%203D%20objects%20from%20point%20clouds%20into%20editable%20Blender%20Python%0Ascripts.%20We%20develop%20a%20comprehensive%20set%20of%20expressive%20Blender%20Python%20APIs%0Acapable%20of%20synthesizing%20intricate%20geometries.%20Leveraging%20these%20APIs%2C%20we%0Aconstruct%20a%20large-scale%20paired%20object-code%20dataset%2C%20where%20the%20code%20for%20each%0Aobject%20is%20decomposed%20into%20distinct%20semantic%20parts.%20Subsequently%2C%20we%20train%20a%0Amultimodal%20large%20language%20model%20%28LLM%29%20that%20translates%203D%20point%20cloud%20into%0Aexecutable%20Blender%20Python%20scripts.%20Our%20approach%20not%20only%20achieves%20superior%0Aperformance%20in%20shape-to-code%20reconstruction%20tasks%20but%20also%20facilitates%0Aintuitive%20geometric%20and%20topological%20editing%20through%20convenient%20code%0Amodifications.%20Furthermore%2C%20our%20code-based%20representation%20enhances%20the%0Areasoning%20capabilities%20of%20LLMs%20in%203D%20shape%20understanding%20tasks.%20Together%2C%20these%0Acontributions%20establish%20MeshCoder%20as%20a%20powerful%20and%20flexible%20solution%20for%0Aprogrammatic%203D%20shape%20reconstruction%20and%20understanding.%20The%20project%20homepage%20is%0Aavailable%20at%20%5Chref%7Bhttps%3A//daibingquan.github.io/MeshCoder%7D%7Bthis%20link%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14879v2&entry.124074799=Read"},
{"title": "Zero-Shot Skeleton-based Action Recognition with Dual Visual-Text\n  Alignment", "author": "Jidong Kuang and Hongsong Wang and Chaolei Han and Yang Zhang and Jie Gui", "abstract": "  Zero-shot action recognition, which addresses the issue of scalability and\ngeneralization in action recognition and allows the models to adapt to new and\nunseen actions dynamically, is an important research topic in computer vision\ncommunities. The key to zero-shot action recognition lies in aligning visual\nfeatures with semantic vectors representing action categories. Most existing\nmethods either directly project visual features onto the semantic space of text\ncategory or learn a shared embedding space between the two modalities. However,\na direct projection cannot accurately align the two modalities, and learning\nrobust and discriminative embedding space between visual and text\nrepresentations is often difficult. To address these issues, we introduce Dual\nVisual-Text Alignment (DVTA) for skeleton-based zero-shot action recognition.\nThe DVTA consists of two alignment modules--Direct Alignment (DA) and Augmented\nAlignment (AA)--along with a designed Semantic Description Enhancement (SDE).\nThe DA module maps the skeleton features to the semantic space through a\nspecially designed visual projector, followed by the SDE, which is based on\ncross-attention to enhance the connection between skeleton and text, thereby\nreducing the gap between modalities. The AA module further strengthens the\nlearning of the embedding space by utilizing deep metric learning to learn the\nsimilarity between skeleton and text. Our approach achieves state-of-the-art\nperformances on several popular zero-shot skeleton-based action recognition\nbenchmarks. The code is available at: https://github.com/jidongkuang/DVTA.\n", "link": "http://arxiv.org/abs/2409.14336v2", "date": "2025-08-22", "relevancy": 2.8796, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6081}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5714}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Skeleton-based%20Action%20Recognition%20with%20Dual%20Visual-Text%0A%20%20Alignment&body=Title%3A%20Zero-Shot%20Skeleton-based%20Action%20Recognition%20with%20Dual%20Visual-Text%0A%20%20Alignment%0AAuthor%3A%20Jidong%20Kuang%20and%20Hongsong%20Wang%20and%20Chaolei%20Han%20and%20Yang%20Zhang%20and%20Jie%20Gui%0AAbstract%3A%20%20%20Zero-shot%20action%20recognition%2C%20which%20addresses%20the%20issue%20of%20scalability%20and%0Ageneralization%20in%20action%20recognition%20and%20allows%20the%20models%20to%20adapt%20to%20new%20and%0Aunseen%20actions%20dynamically%2C%20is%20an%20important%20research%20topic%20in%20computer%20vision%0Acommunities.%20The%20key%20to%20zero-shot%20action%20recognition%20lies%20in%20aligning%20visual%0Afeatures%20with%20semantic%20vectors%20representing%20action%20categories.%20Most%20existing%0Amethods%20either%20directly%20project%20visual%20features%20onto%20the%20semantic%20space%20of%20text%0Acategory%20or%20learn%20a%20shared%20embedding%20space%20between%20the%20two%20modalities.%20However%2C%0Aa%20direct%20projection%20cannot%20accurately%20align%20the%20two%20modalities%2C%20and%20learning%0Arobust%20and%20discriminative%20embedding%20space%20between%20visual%20and%20text%0Arepresentations%20is%20often%20difficult.%20To%20address%20these%20issues%2C%20we%20introduce%20Dual%0AVisual-Text%20Alignment%20%28DVTA%29%20for%20skeleton-based%20zero-shot%20action%20recognition.%0AThe%20DVTA%20consists%20of%20two%20alignment%20modules--Direct%20Alignment%20%28DA%29%20and%20Augmented%0AAlignment%20%28AA%29--along%20with%20a%20designed%20Semantic%20Description%20Enhancement%20%28SDE%29.%0AThe%20DA%20module%20maps%20the%20skeleton%20features%20to%20the%20semantic%20space%20through%20a%0Aspecially%20designed%20visual%20projector%2C%20followed%20by%20the%20SDE%2C%20which%20is%20based%20on%0Across-attention%20to%20enhance%20the%20connection%20between%20skeleton%20and%20text%2C%20thereby%0Areducing%20the%20gap%20between%20modalities.%20The%20AA%20module%20further%20strengthens%20the%0Alearning%20of%20the%20embedding%20space%20by%20utilizing%20deep%20metric%20learning%20to%20learn%20the%0Asimilarity%20between%20skeleton%20and%20text.%20Our%20approach%20achieves%20state-of-the-art%0Aperformances%20on%20several%20popular%20zero-shot%20skeleton-based%20action%20recognition%0Abenchmarks.%20The%20code%20is%20available%20at%3A%20https%3A//github.com/jidongkuang/DVTA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.14336v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Skeleton-based%2520Action%2520Recognition%2520with%2520Dual%2520Visual-Text%250A%2520%2520Alignment%26entry.906535625%3DJidong%2520Kuang%2520and%2520Hongsong%2520Wang%2520and%2520Chaolei%2520Han%2520and%2520Yang%2520Zhang%2520and%2520Jie%2520Gui%26entry.1292438233%3D%2520%2520Zero-shot%2520action%2520recognition%252C%2520which%2520addresses%2520the%2520issue%2520of%2520scalability%2520and%250Ageneralization%2520in%2520action%2520recognition%2520and%2520allows%2520the%2520models%2520to%2520adapt%2520to%2520new%2520and%250Aunseen%2520actions%2520dynamically%252C%2520is%2520an%2520important%2520research%2520topic%2520in%2520computer%2520vision%250Acommunities.%2520The%2520key%2520to%2520zero-shot%2520action%2520recognition%2520lies%2520in%2520aligning%2520visual%250Afeatures%2520with%2520semantic%2520vectors%2520representing%2520action%2520categories.%2520Most%2520existing%250Amethods%2520either%2520directly%2520project%2520visual%2520features%2520onto%2520the%2520semantic%2520space%2520of%2520text%250Acategory%2520or%2520learn%2520a%2520shared%2520embedding%2520space%2520between%2520the%2520two%2520modalities.%2520However%252C%250Aa%2520direct%2520projection%2520cannot%2520accurately%2520align%2520the%2520two%2520modalities%252C%2520and%2520learning%250Arobust%2520and%2520discriminative%2520embedding%2520space%2520between%2520visual%2520and%2520text%250Arepresentations%2520is%2520often%2520difficult.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520Dual%250AVisual-Text%2520Alignment%2520%2528DVTA%2529%2520for%2520skeleton-based%2520zero-shot%2520action%2520recognition.%250AThe%2520DVTA%2520consists%2520of%2520two%2520alignment%2520modules--Direct%2520Alignment%2520%2528DA%2529%2520and%2520Augmented%250AAlignment%2520%2528AA%2529--along%2520with%2520a%2520designed%2520Semantic%2520Description%2520Enhancement%2520%2528SDE%2529.%250AThe%2520DA%2520module%2520maps%2520the%2520skeleton%2520features%2520to%2520the%2520semantic%2520space%2520through%2520a%250Aspecially%2520designed%2520visual%2520projector%252C%2520followed%2520by%2520the%2520SDE%252C%2520which%2520is%2520based%2520on%250Across-attention%2520to%2520enhance%2520the%2520connection%2520between%2520skeleton%2520and%2520text%252C%2520thereby%250Areducing%2520the%2520gap%2520between%2520modalities.%2520The%2520AA%2520module%2520further%2520strengthens%2520the%250Alearning%2520of%2520the%2520embedding%2520space%2520by%2520utilizing%2520deep%2520metric%2520learning%2520to%2520learn%2520the%250Asimilarity%2520between%2520skeleton%2520and%2520text.%2520Our%2520approach%2520achieves%2520state-of-the-art%250Aperformances%2520on%2520several%2520popular%2520zero-shot%2520skeleton-based%2520action%2520recognition%250Abenchmarks.%2520The%2520code%2520is%2520available%2520at%253A%2520https%253A//github.com/jidongkuang/DVTA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.14336v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Skeleton-based%20Action%20Recognition%20with%20Dual%20Visual-Text%0A%20%20Alignment&entry.906535625=Jidong%20Kuang%20and%20Hongsong%20Wang%20and%20Chaolei%20Han%20and%20Yang%20Zhang%20and%20Jie%20Gui&entry.1292438233=%20%20Zero-shot%20action%20recognition%2C%20which%20addresses%20the%20issue%20of%20scalability%20and%0Ageneralization%20in%20action%20recognition%20and%20allows%20the%20models%20to%20adapt%20to%20new%20and%0Aunseen%20actions%20dynamically%2C%20is%20an%20important%20research%20topic%20in%20computer%20vision%0Acommunities.%20The%20key%20to%20zero-shot%20action%20recognition%20lies%20in%20aligning%20visual%0Afeatures%20with%20semantic%20vectors%20representing%20action%20categories.%20Most%20existing%0Amethods%20either%20directly%20project%20visual%20features%20onto%20the%20semantic%20space%20of%20text%0Acategory%20or%20learn%20a%20shared%20embedding%20space%20between%20the%20two%20modalities.%20However%2C%0Aa%20direct%20projection%20cannot%20accurately%20align%20the%20two%20modalities%2C%20and%20learning%0Arobust%20and%20discriminative%20embedding%20space%20between%20visual%20and%20text%0Arepresentations%20is%20often%20difficult.%20To%20address%20these%20issues%2C%20we%20introduce%20Dual%0AVisual-Text%20Alignment%20%28DVTA%29%20for%20skeleton-based%20zero-shot%20action%20recognition.%0AThe%20DVTA%20consists%20of%20two%20alignment%20modules--Direct%20Alignment%20%28DA%29%20and%20Augmented%0AAlignment%20%28AA%29--along%20with%20a%20designed%20Semantic%20Description%20Enhancement%20%28SDE%29.%0AThe%20DA%20module%20maps%20the%20skeleton%20features%20to%20the%20semantic%20space%20through%20a%0Aspecially%20designed%20visual%20projector%2C%20followed%20by%20the%20SDE%2C%20which%20is%20based%20on%0Across-attention%20to%20enhance%20the%20connection%20between%20skeleton%20and%20text%2C%20thereby%0Areducing%20the%20gap%20between%20modalities.%20The%20AA%20module%20further%20strengthens%20the%0Alearning%20of%20the%20embedding%20space%20by%20utilizing%20deep%20metric%20learning%20to%20learn%20the%0Asimilarity%20between%20skeleton%20and%20text.%20Our%20approach%20achieves%20state-of-the-art%0Aperformances%20on%20several%20popular%20zero-shot%20skeleton-based%20action%20recognition%0Abenchmarks.%20The%20code%20is%20available%20at%3A%20https%3A//github.com/jidongkuang/DVTA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.14336v2&entry.124074799=Read"},
{"title": "Structuring GUI Elements through Vision Language Models: Towards Action\n  Space Generation", "author": "Yi Xu and Yesheng Zhang and jiajia Liu and Jingdong Chen", "abstract": "  Multimodal large language models (MLLMs) have emerged as pivotal tools in\nenhancing human-computer interaction. In this paper we focus on the application\nof MLLMs in the field of graphical user interface (GUI) elements structuring,\nwhere they assist in processing user instructions based on screen contents.\nDespite the promise of MLLMs, their performance in precisely generating UI\nelement coordinates, a critical aspect of GUI understanding, is hindered by the\nnature of next-token prediction training. This challenge arises from the\nsemantic void surrounding numerical UI coordinates in language representation\nspaces, necessitating a substantial and diverse dataset to bolster visual\nmodule capabilities. To address these limitations, we introduce an\nIoU-Augmented Maximum Likelihood (IAML) training paradigm. Specifically, our\napproach involves a novel pipeline for IoU-based coordinate sampling to augment\nthe training data, which considers the proximity to ground truth coordinates.\nThis data augmentation strategy is then employed to fine-tune MLLMs under the\nIAML paradigm, which is designed to mitigate the exposure bias problem inherent\nin traditional maximum likelihood estimation. Through extensive experiments, we\ndemonstrate the superior performance of our IAML training approach over\ntraditional training paradigms.\n", "link": "http://arxiv.org/abs/2508.16271v1", "date": "2025-08-22", "relevancy": 2.8487, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5703}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5703}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5686}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structuring%20GUI%20Elements%20through%20Vision%20Language%20Models%3A%20Towards%20Action%0A%20%20Space%20Generation&body=Title%3A%20Structuring%20GUI%20Elements%20through%20Vision%20Language%20Models%3A%20Towards%20Action%0A%20%20Space%20Generation%0AAuthor%3A%20Yi%20Xu%20and%20Yesheng%20Zhang%20and%20jiajia%20Liu%20and%20Jingdong%20Chen%0AAbstract%3A%20%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20emerged%20as%20pivotal%20tools%20in%0Aenhancing%20human-computer%20interaction.%20In%20this%20paper%20we%20focus%20on%20the%20application%0Aof%20MLLMs%20in%20the%20field%20of%20graphical%20user%20interface%20%28GUI%29%20elements%20structuring%2C%0Awhere%20they%20assist%20in%20processing%20user%20instructions%20based%20on%20screen%20contents.%0ADespite%20the%20promise%20of%20MLLMs%2C%20their%20performance%20in%20precisely%20generating%20UI%0Aelement%20coordinates%2C%20a%20critical%20aspect%20of%20GUI%20understanding%2C%20is%20hindered%20by%20the%0Anature%20of%20next-token%20prediction%20training.%20This%20challenge%20arises%20from%20the%0Asemantic%20void%20surrounding%20numerical%20UI%20coordinates%20in%20language%20representation%0Aspaces%2C%20necessitating%20a%20substantial%20and%20diverse%20dataset%20to%20bolster%20visual%0Amodule%20capabilities.%20To%20address%20these%20limitations%2C%20we%20introduce%20an%0AIoU-Augmented%20Maximum%20Likelihood%20%28IAML%29%20training%20paradigm.%20Specifically%2C%20our%0Aapproach%20involves%20a%20novel%20pipeline%20for%20IoU-based%20coordinate%20sampling%20to%20augment%0Athe%20training%20data%2C%20which%20considers%20the%20proximity%20to%20ground%20truth%20coordinates.%0AThis%20data%20augmentation%20strategy%20is%20then%20employed%20to%20fine-tune%20MLLMs%20under%20the%0AIAML%20paradigm%2C%20which%20is%20designed%20to%20mitigate%20the%20exposure%20bias%20problem%20inherent%0Ain%20traditional%20maximum%20likelihood%20estimation.%20Through%20extensive%20experiments%2C%20we%0Ademonstrate%20the%20superior%20performance%20of%20our%20IAML%20training%20approach%20over%0Atraditional%20training%20paradigms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16271v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructuring%2520GUI%2520Elements%2520through%2520Vision%2520Language%2520Models%253A%2520Towards%2520Action%250A%2520%2520Space%2520Generation%26entry.906535625%3DYi%2520Xu%2520and%2520Yesheng%2520Zhang%2520and%2520jiajia%2520Liu%2520and%2520Jingdong%2520Chen%26entry.1292438233%3D%2520%2520Multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520emerged%2520as%2520pivotal%2520tools%2520in%250Aenhancing%2520human-computer%2520interaction.%2520In%2520this%2520paper%2520we%2520focus%2520on%2520the%2520application%250Aof%2520MLLMs%2520in%2520the%2520field%2520of%2520graphical%2520user%2520interface%2520%2528GUI%2529%2520elements%2520structuring%252C%250Awhere%2520they%2520assist%2520in%2520processing%2520user%2520instructions%2520based%2520on%2520screen%2520contents.%250ADespite%2520the%2520promise%2520of%2520MLLMs%252C%2520their%2520performance%2520in%2520precisely%2520generating%2520UI%250Aelement%2520coordinates%252C%2520a%2520critical%2520aspect%2520of%2520GUI%2520understanding%252C%2520is%2520hindered%2520by%2520the%250Anature%2520of%2520next-token%2520prediction%2520training.%2520This%2520challenge%2520arises%2520from%2520the%250Asemantic%2520void%2520surrounding%2520numerical%2520UI%2520coordinates%2520in%2520language%2520representation%250Aspaces%252C%2520necessitating%2520a%2520substantial%2520and%2520diverse%2520dataset%2520to%2520bolster%2520visual%250Amodule%2520capabilities.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520an%250AIoU-Augmented%2520Maximum%2520Likelihood%2520%2528IAML%2529%2520training%2520paradigm.%2520Specifically%252C%2520our%250Aapproach%2520involves%2520a%2520novel%2520pipeline%2520for%2520IoU-based%2520coordinate%2520sampling%2520to%2520augment%250Athe%2520training%2520data%252C%2520which%2520considers%2520the%2520proximity%2520to%2520ground%2520truth%2520coordinates.%250AThis%2520data%2520augmentation%2520strategy%2520is%2520then%2520employed%2520to%2520fine-tune%2520MLLMs%2520under%2520the%250AIAML%2520paradigm%252C%2520which%2520is%2520designed%2520to%2520mitigate%2520the%2520exposure%2520bias%2520problem%2520inherent%250Ain%2520traditional%2520maximum%2520likelihood%2520estimation.%2520Through%2520extensive%2520experiments%252C%2520we%250Ademonstrate%2520the%2520superior%2520performance%2520of%2520our%2520IAML%2520training%2520approach%2520over%250Atraditional%2520training%2520paradigms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16271v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structuring%20GUI%20Elements%20through%20Vision%20Language%20Models%3A%20Towards%20Action%0A%20%20Space%20Generation&entry.906535625=Yi%20Xu%20and%20Yesheng%20Zhang%20and%20jiajia%20Liu%20and%20Jingdong%20Chen&entry.1292438233=%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20emerged%20as%20pivotal%20tools%20in%0Aenhancing%20human-computer%20interaction.%20In%20this%20paper%20we%20focus%20on%20the%20application%0Aof%20MLLMs%20in%20the%20field%20of%20graphical%20user%20interface%20%28GUI%29%20elements%20structuring%2C%0Awhere%20they%20assist%20in%20processing%20user%20instructions%20based%20on%20screen%20contents.%0ADespite%20the%20promise%20of%20MLLMs%2C%20their%20performance%20in%20precisely%20generating%20UI%0Aelement%20coordinates%2C%20a%20critical%20aspect%20of%20GUI%20understanding%2C%20is%20hindered%20by%20the%0Anature%20of%20next-token%20prediction%20training.%20This%20challenge%20arises%20from%20the%0Asemantic%20void%20surrounding%20numerical%20UI%20coordinates%20in%20language%20representation%0Aspaces%2C%20necessitating%20a%20substantial%20and%20diverse%20dataset%20to%20bolster%20visual%0Amodule%20capabilities.%20To%20address%20these%20limitations%2C%20we%20introduce%20an%0AIoU-Augmented%20Maximum%20Likelihood%20%28IAML%29%20training%20paradigm.%20Specifically%2C%20our%0Aapproach%20involves%20a%20novel%20pipeline%20for%20IoU-based%20coordinate%20sampling%20to%20augment%0Athe%20training%20data%2C%20which%20considers%20the%20proximity%20to%20ground%20truth%20coordinates.%0AThis%20data%20augmentation%20strategy%20is%20then%20employed%20to%20fine-tune%20MLLMs%20under%20the%0AIAML%20paradigm%2C%20which%20is%20designed%20to%20mitigate%20the%20exposure%20bias%20problem%20inherent%0Ain%20traditional%20maximum%20likelihood%20estimation.%20Through%20extensive%20experiments%2C%20we%0Ademonstrate%20the%20superior%20performance%20of%20our%20IAML%20training%20approach%20over%0Atraditional%20training%20paradigms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16271v1&entry.124074799=Read"},
{"title": "A Lightweight Group Multiscale Bidirectional Interactive Network for\n  Real-Time Steel Surface Defect Detection", "author": "Yong Zhang and Cunjian Chen and Qiang Gao and Yi Wang and Bin Fang", "abstract": "  Real-time surface defect detection is critical for maintaining product\nquality and production efficiency in the steel manufacturing industry. Despite\npromising accuracy, existing deep learning methods often suffer from high\ncomputational complexity and slow inference speeds, which limit their\ndeployment in resource-constrained industrial environments. Recent lightweight\napproaches adopt multibranch architectures based on depthwise separable\nconvolution (DSConv) to capture multiscale contextual information. However,\nthese methods often suffer from increased computational overhead and lack\neffective cross-scale feature interaction, limiting their ability to fully\nleverage multiscale representations. To address these challenges, we propose\nGMBINet, a lightweight framework that enhances multiscale feature extraction\nand interaction through novel Group Multiscale Bidirectional Interactive (GMBI)\nmodules. The GMBI adopts a group-wise strategy for multiscale feature\nextraction, ensuring scale-agnostic computational complexity. It further\nintegrates a Bidirectional Progressive Feature Interactor (BPFI) and a\nparameter-free Element-Wise Multiplication-Summation (EWMS) operation to\nenhance cross-scale interaction without introducing additional computational\noverhead. Experiments on SD-Saliency-900 and NRSD-MN datasets demonstrate that\nGMBINet delivers competitive accuracy with real-time speeds of 1048 FPS on GPU\nand 16.53 FPS on CPU at 512 resolution, using only 0.19 M parameters.\nAdditional evaluations on the NEU-CLS defect classification dataset further\nconfirm the strong generalization ability of our method, demonstrating its\npotential for broader industrial vision applications beyond surface defect\ndetection. The dataset and code are publicly available at:\nhttps://github.com/zhangyongcode/GMBINet.\n", "link": "http://arxiv.org/abs/2508.16397v1", "date": "2025-08-22", "relevancy": 2.8357, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5735}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5659}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Lightweight%20Group%20Multiscale%20Bidirectional%20Interactive%20Network%20for%0A%20%20Real-Time%20Steel%20Surface%20Defect%20Detection&body=Title%3A%20A%20Lightweight%20Group%20Multiscale%20Bidirectional%20Interactive%20Network%20for%0A%20%20Real-Time%20Steel%20Surface%20Defect%20Detection%0AAuthor%3A%20Yong%20Zhang%20and%20Cunjian%20Chen%20and%20Qiang%20Gao%20and%20Yi%20Wang%20and%20Bin%20Fang%0AAbstract%3A%20%20%20Real-time%20surface%20defect%20detection%20is%20critical%20for%20maintaining%20product%0Aquality%20and%20production%20efficiency%20in%20the%20steel%20manufacturing%20industry.%20Despite%0Apromising%20accuracy%2C%20existing%20deep%20learning%20methods%20often%20suffer%20from%20high%0Acomputational%20complexity%20and%20slow%20inference%20speeds%2C%20which%20limit%20their%0Adeployment%20in%20resource-constrained%20industrial%20environments.%20Recent%20lightweight%0Aapproaches%20adopt%20multibranch%20architectures%20based%20on%20depthwise%20separable%0Aconvolution%20%28DSConv%29%20to%20capture%20multiscale%20contextual%20information.%20However%2C%0Athese%20methods%20often%20suffer%20from%20increased%20computational%20overhead%20and%20lack%0Aeffective%20cross-scale%20feature%20interaction%2C%20limiting%20their%20ability%20to%20fully%0Aleverage%20multiscale%20representations.%20To%20address%20these%20challenges%2C%20we%20propose%0AGMBINet%2C%20a%20lightweight%20framework%20that%20enhances%20multiscale%20feature%20extraction%0Aand%20interaction%20through%20novel%20Group%20Multiscale%20Bidirectional%20Interactive%20%28GMBI%29%0Amodules.%20The%20GMBI%20adopts%20a%20group-wise%20strategy%20for%20multiscale%20feature%0Aextraction%2C%20ensuring%20scale-agnostic%20computational%20complexity.%20It%20further%0Aintegrates%20a%20Bidirectional%20Progressive%20Feature%20Interactor%20%28BPFI%29%20and%20a%0Aparameter-free%20Element-Wise%20Multiplication-Summation%20%28EWMS%29%20operation%20to%0Aenhance%20cross-scale%20interaction%20without%20introducing%20additional%20computational%0Aoverhead.%20Experiments%20on%20SD-Saliency-900%20and%20NRSD-MN%20datasets%20demonstrate%20that%0AGMBINet%20delivers%20competitive%20accuracy%20with%20real-time%20speeds%20of%201048%20FPS%20on%20GPU%0Aand%2016.53%20FPS%20on%20CPU%20at%20512%20resolution%2C%20using%20only%200.19%20M%20parameters.%0AAdditional%20evaluations%20on%20the%20NEU-CLS%20defect%20classification%20dataset%20further%0Aconfirm%20the%20strong%20generalization%20ability%20of%20our%20method%2C%20demonstrating%20its%0Apotential%20for%20broader%20industrial%20vision%20applications%20beyond%20surface%20defect%0Adetection.%20The%20dataset%20and%20code%20are%20publicly%20available%20at%3A%0Ahttps%3A//github.com/zhangyongcode/GMBINet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16397v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Lightweight%2520Group%2520Multiscale%2520Bidirectional%2520Interactive%2520Network%2520for%250A%2520%2520Real-Time%2520Steel%2520Surface%2520Defect%2520Detection%26entry.906535625%3DYong%2520Zhang%2520and%2520Cunjian%2520Chen%2520and%2520Qiang%2520Gao%2520and%2520Yi%2520Wang%2520and%2520Bin%2520Fang%26entry.1292438233%3D%2520%2520Real-time%2520surface%2520defect%2520detection%2520is%2520critical%2520for%2520maintaining%2520product%250Aquality%2520and%2520production%2520efficiency%2520in%2520the%2520steel%2520manufacturing%2520industry.%2520Despite%250Apromising%2520accuracy%252C%2520existing%2520deep%2520learning%2520methods%2520often%2520suffer%2520from%2520high%250Acomputational%2520complexity%2520and%2520slow%2520inference%2520speeds%252C%2520which%2520limit%2520their%250Adeployment%2520in%2520resource-constrained%2520industrial%2520environments.%2520Recent%2520lightweight%250Aapproaches%2520adopt%2520multibranch%2520architectures%2520based%2520on%2520depthwise%2520separable%250Aconvolution%2520%2528DSConv%2529%2520to%2520capture%2520multiscale%2520contextual%2520information.%2520However%252C%250Athese%2520methods%2520often%2520suffer%2520from%2520increased%2520computational%2520overhead%2520and%2520lack%250Aeffective%2520cross-scale%2520feature%2520interaction%252C%2520limiting%2520their%2520ability%2520to%2520fully%250Aleverage%2520multiscale%2520representations.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%250AGMBINet%252C%2520a%2520lightweight%2520framework%2520that%2520enhances%2520multiscale%2520feature%2520extraction%250Aand%2520interaction%2520through%2520novel%2520Group%2520Multiscale%2520Bidirectional%2520Interactive%2520%2528GMBI%2529%250Amodules.%2520The%2520GMBI%2520adopts%2520a%2520group-wise%2520strategy%2520for%2520multiscale%2520feature%250Aextraction%252C%2520ensuring%2520scale-agnostic%2520computational%2520complexity.%2520It%2520further%250Aintegrates%2520a%2520Bidirectional%2520Progressive%2520Feature%2520Interactor%2520%2528BPFI%2529%2520and%2520a%250Aparameter-free%2520Element-Wise%2520Multiplication-Summation%2520%2528EWMS%2529%2520operation%2520to%250Aenhance%2520cross-scale%2520interaction%2520without%2520introducing%2520additional%2520computational%250Aoverhead.%2520Experiments%2520on%2520SD-Saliency-900%2520and%2520NRSD-MN%2520datasets%2520demonstrate%2520that%250AGMBINet%2520delivers%2520competitive%2520accuracy%2520with%2520real-time%2520speeds%2520of%25201048%2520FPS%2520on%2520GPU%250Aand%252016.53%2520FPS%2520on%2520CPU%2520at%2520512%2520resolution%252C%2520using%2520only%25200.19%2520M%2520parameters.%250AAdditional%2520evaluations%2520on%2520the%2520NEU-CLS%2520defect%2520classification%2520dataset%2520further%250Aconfirm%2520the%2520strong%2520generalization%2520ability%2520of%2520our%2520method%252C%2520demonstrating%2520its%250Apotential%2520for%2520broader%2520industrial%2520vision%2520applications%2520beyond%2520surface%2520defect%250Adetection.%2520The%2520dataset%2520and%2520code%2520are%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/zhangyongcode/GMBINet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16397v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Lightweight%20Group%20Multiscale%20Bidirectional%20Interactive%20Network%20for%0A%20%20Real-Time%20Steel%20Surface%20Defect%20Detection&entry.906535625=Yong%20Zhang%20and%20Cunjian%20Chen%20and%20Qiang%20Gao%20and%20Yi%20Wang%20and%20Bin%20Fang&entry.1292438233=%20%20Real-time%20surface%20defect%20detection%20is%20critical%20for%20maintaining%20product%0Aquality%20and%20production%20efficiency%20in%20the%20steel%20manufacturing%20industry.%20Despite%0Apromising%20accuracy%2C%20existing%20deep%20learning%20methods%20often%20suffer%20from%20high%0Acomputational%20complexity%20and%20slow%20inference%20speeds%2C%20which%20limit%20their%0Adeployment%20in%20resource-constrained%20industrial%20environments.%20Recent%20lightweight%0Aapproaches%20adopt%20multibranch%20architectures%20based%20on%20depthwise%20separable%0Aconvolution%20%28DSConv%29%20to%20capture%20multiscale%20contextual%20information.%20However%2C%0Athese%20methods%20often%20suffer%20from%20increased%20computational%20overhead%20and%20lack%0Aeffective%20cross-scale%20feature%20interaction%2C%20limiting%20their%20ability%20to%20fully%0Aleverage%20multiscale%20representations.%20To%20address%20these%20challenges%2C%20we%20propose%0AGMBINet%2C%20a%20lightweight%20framework%20that%20enhances%20multiscale%20feature%20extraction%0Aand%20interaction%20through%20novel%20Group%20Multiscale%20Bidirectional%20Interactive%20%28GMBI%29%0Amodules.%20The%20GMBI%20adopts%20a%20group-wise%20strategy%20for%20multiscale%20feature%0Aextraction%2C%20ensuring%20scale-agnostic%20computational%20complexity.%20It%20further%0Aintegrates%20a%20Bidirectional%20Progressive%20Feature%20Interactor%20%28BPFI%29%20and%20a%0Aparameter-free%20Element-Wise%20Multiplication-Summation%20%28EWMS%29%20operation%20to%0Aenhance%20cross-scale%20interaction%20without%20introducing%20additional%20computational%0Aoverhead.%20Experiments%20on%20SD-Saliency-900%20and%20NRSD-MN%20datasets%20demonstrate%20that%0AGMBINet%20delivers%20competitive%20accuracy%20with%20real-time%20speeds%20of%201048%20FPS%20on%20GPU%0Aand%2016.53%20FPS%20on%20CPU%20at%20512%20resolution%2C%20using%20only%200.19%20M%20parameters.%0AAdditional%20evaluations%20on%20the%20NEU-CLS%20defect%20classification%20dataset%20further%0Aconfirm%20the%20strong%20generalization%20ability%20of%20our%20method%2C%20demonstrating%20its%0Apotential%20for%20broader%20industrial%20vision%20applications%20beyond%20surface%20defect%0Adetection.%20The%20dataset%20and%20code%20are%20publicly%20available%20at%3A%0Ahttps%3A//github.com/zhangyongcode/GMBINet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16397v1&entry.124074799=Read"},
{"title": "Towards Open World Detection: A Survey", "author": "Andrei-Stefan Bulzan and Cosmin Cernazanu-Glavan", "abstract": "  For decades, Computer Vision has aimed at enabling machines to perceive the\nexternal world. Initial limitations led to the development of highly\nspecialized niches. As success in each task accrued and research progressed,\nincreasingly complex perception tasks emerged. This survey charts the\nconvergence of these tasks and, in doing so, introduces Open World Detection\n(OWD), an umbrella term we propose to unify class-agnostic and generally\napplicable detection models in the vision domain. We start from the history of\nfoundational vision subdomains and cover key concepts, methodologies and\ndatasets making up today's state-of-the-art landscape. This traverses topics\nstarting from early saliency detection, foreground/background separation, out\nof distribution detection and leading up to open world object detection,\nzero-shot detection and Vision Large Language Models (VLLMs). We explore the\noverlap between these subdomains, their increasing convergence, and their\npotential to unify into a singular domain in the future, perception.\n", "link": "http://arxiv.org/abs/2508.16527v1", "date": "2025-08-22", "relevancy": 2.8309, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5831}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5831}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5324}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Open%20World%20Detection%3A%20A%20Survey&body=Title%3A%20Towards%20Open%20World%20Detection%3A%20A%20Survey%0AAuthor%3A%20Andrei-Stefan%20Bulzan%20and%20Cosmin%20Cernazanu-Glavan%0AAbstract%3A%20%20%20For%20decades%2C%20Computer%20Vision%20has%20aimed%20at%20enabling%20machines%20to%20perceive%20the%0Aexternal%20world.%20Initial%20limitations%20led%20to%20the%20development%20of%20highly%0Aspecialized%20niches.%20As%20success%20in%20each%20task%20accrued%20and%20research%20progressed%2C%0Aincreasingly%20complex%20perception%20tasks%20emerged.%20This%20survey%20charts%20the%0Aconvergence%20of%20these%20tasks%20and%2C%20in%20doing%20so%2C%20introduces%20Open%20World%20Detection%0A%28OWD%29%2C%20an%20umbrella%20term%20we%20propose%20to%20unify%20class-agnostic%20and%20generally%0Aapplicable%20detection%20models%20in%20the%20vision%20domain.%20We%20start%20from%20the%20history%20of%0Afoundational%20vision%20subdomains%20and%20cover%20key%20concepts%2C%20methodologies%20and%0Adatasets%20making%20up%20today%27s%20state-of-the-art%20landscape.%20This%20traverses%20topics%0Astarting%20from%20early%20saliency%20detection%2C%20foreground/background%20separation%2C%20out%0Aof%20distribution%20detection%20and%20leading%20up%20to%20open%20world%20object%20detection%2C%0Azero-shot%20detection%20and%20Vision%20Large%20Language%20Models%20%28VLLMs%29.%20We%20explore%20the%0Aoverlap%20between%20these%20subdomains%2C%20their%20increasing%20convergence%2C%20and%20their%0Apotential%20to%20unify%20into%20a%20singular%20domain%20in%20the%20future%2C%20perception.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16527v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Open%2520World%2520Detection%253A%2520A%2520Survey%26entry.906535625%3DAndrei-Stefan%2520Bulzan%2520and%2520Cosmin%2520Cernazanu-Glavan%26entry.1292438233%3D%2520%2520For%2520decades%252C%2520Computer%2520Vision%2520has%2520aimed%2520at%2520enabling%2520machines%2520to%2520perceive%2520the%250Aexternal%2520world.%2520Initial%2520limitations%2520led%2520to%2520the%2520development%2520of%2520highly%250Aspecialized%2520niches.%2520As%2520success%2520in%2520each%2520task%2520accrued%2520and%2520research%2520progressed%252C%250Aincreasingly%2520complex%2520perception%2520tasks%2520emerged.%2520This%2520survey%2520charts%2520the%250Aconvergence%2520of%2520these%2520tasks%2520and%252C%2520in%2520doing%2520so%252C%2520introduces%2520Open%2520World%2520Detection%250A%2528OWD%2529%252C%2520an%2520umbrella%2520term%2520we%2520propose%2520to%2520unify%2520class-agnostic%2520and%2520generally%250Aapplicable%2520detection%2520models%2520in%2520the%2520vision%2520domain.%2520We%2520start%2520from%2520the%2520history%2520of%250Afoundational%2520vision%2520subdomains%2520and%2520cover%2520key%2520concepts%252C%2520methodologies%2520and%250Adatasets%2520making%2520up%2520today%2527s%2520state-of-the-art%2520landscape.%2520This%2520traverses%2520topics%250Astarting%2520from%2520early%2520saliency%2520detection%252C%2520foreground/background%2520separation%252C%2520out%250Aof%2520distribution%2520detection%2520and%2520leading%2520up%2520to%2520open%2520world%2520object%2520detection%252C%250Azero-shot%2520detection%2520and%2520Vision%2520Large%2520Language%2520Models%2520%2528VLLMs%2529.%2520We%2520explore%2520the%250Aoverlap%2520between%2520these%2520subdomains%252C%2520their%2520increasing%2520convergence%252C%2520and%2520their%250Apotential%2520to%2520unify%2520into%2520a%2520singular%2520domain%2520in%2520the%2520future%252C%2520perception.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16527v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Open%20World%20Detection%3A%20A%20Survey&entry.906535625=Andrei-Stefan%20Bulzan%20and%20Cosmin%20Cernazanu-Glavan&entry.1292438233=%20%20For%20decades%2C%20Computer%20Vision%20has%20aimed%20at%20enabling%20machines%20to%20perceive%20the%0Aexternal%20world.%20Initial%20limitations%20led%20to%20the%20development%20of%20highly%0Aspecialized%20niches.%20As%20success%20in%20each%20task%20accrued%20and%20research%20progressed%2C%0Aincreasingly%20complex%20perception%20tasks%20emerged.%20This%20survey%20charts%20the%0Aconvergence%20of%20these%20tasks%20and%2C%20in%20doing%20so%2C%20introduces%20Open%20World%20Detection%0A%28OWD%29%2C%20an%20umbrella%20term%20we%20propose%20to%20unify%20class-agnostic%20and%20generally%0Aapplicable%20detection%20models%20in%20the%20vision%20domain.%20We%20start%20from%20the%20history%20of%0Afoundational%20vision%20subdomains%20and%20cover%20key%20concepts%2C%20methodologies%20and%0Adatasets%20making%20up%20today%27s%20state-of-the-art%20landscape.%20This%20traverses%20topics%0Astarting%20from%20early%20saliency%20detection%2C%20foreground/background%20separation%2C%20out%0Aof%20distribution%20detection%20and%20leading%20up%20to%20open%20world%20object%20detection%2C%0Azero-shot%20detection%20and%20Vision%20Large%20Language%20Models%20%28VLLMs%29.%20We%20explore%20the%0Aoverlap%20between%20these%20subdomains%2C%20their%20increasing%20convergence%2C%20and%20their%0Apotential%20to%20unify%20into%20a%20singular%20domain%20in%20the%20future%2C%20perception.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16527v1&entry.124074799=Read"},
{"title": "FLAIR: Frequency and Locality-Aware Implicit Neural Representations", "author": "Sukhun Ko and Dahyeon Kye and Kyle Min and Chanho Eom and Jihyong Oh", "abstract": "  Implicit Neural Representations (INRs) leverage neural networks to map\ncoordinates to corresponding signals, enabling continuous and compact\nrepresentations. This paradigm has driven significant advances in various\nvision tasks. However, existing INRs lack frequency selectivity, spatial\nlocalization, and sparse representations, leading to an over-reliance on\nredundant signal components. Consequently, they exhibit spectral bias, tending\nto learn low-frequency components early while struggling to capture fine\nhigh-frequency details. To address these issues, we propose FLAIR (Frequency-\nand Locality-Aware Implicit Neural Representations), which incorporates two key\ninnovations. The first is RC-GAUSS, a novel activation designed for explicit\nfrequency selection and spatial localization under the constraints of the\ntime-frequency uncertainty principle (TFUP). The second is\nWavelet-Energy-Guided Encoding (WEGE), which leverages the discrete wavelet\ntransform (DWT) to compute energy scores and explicitly guide frequency\ninformation to the network. Our method consistently outperforms existing INRs\nin 2D image representation and restoration, as well as 3D reconstruction.\n", "link": "http://arxiv.org/abs/2508.13544v2", "date": "2025-08-22", "relevancy": 2.8056, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6041}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5483}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5309}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FLAIR%3A%20Frequency%20and%20Locality-Aware%20Implicit%20Neural%20Representations&body=Title%3A%20FLAIR%3A%20Frequency%20and%20Locality-Aware%20Implicit%20Neural%20Representations%0AAuthor%3A%20Sukhun%20Ko%20and%20Dahyeon%20Kye%20and%20Kyle%20Min%20and%20Chanho%20Eom%20and%20Jihyong%20Oh%0AAbstract%3A%20%20%20Implicit%20Neural%20Representations%20%28INRs%29%20leverage%20neural%20networks%20to%20map%0Acoordinates%20to%20corresponding%20signals%2C%20enabling%20continuous%20and%20compact%0Arepresentations.%20This%20paradigm%20has%20driven%20significant%20advances%20in%20various%0Avision%20tasks.%20However%2C%20existing%20INRs%20lack%20frequency%20selectivity%2C%20spatial%0Alocalization%2C%20and%20sparse%20representations%2C%20leading%20to%20an%20over-reliance%20on%0Aredundant%20signal%20components.%20Consequently%2C%20they%20exhibit%20spectral%20bias%2C%20tending%0Ato%20learn%20low-frequency%20components%20early%20while%20struggling%20to%20capture%20fine%0Ahigh-frequency%20details.%20To%20address%20these%20issues%2C%20we%20propose%20FLAIR%20%28Frequency-%0Aand%20Locality-Aware%20Implicit%20Neural%20Representations%29%2C%20which%20incorporates%20two%20key%0Ainnovations.%20The%20first%20is%20RC-GAUSS%2C%20a%20novel%20activation%20designed%20for%20explicit%0Afrequency%20selection%20and%20spatial%20localization%20under%20the%20constraints%20of%20the%0Atime-frequency%20uncertainty%20principle%20%28TFUP%29.%20The%20second%20is%0AWavelet-Energy-Guided%20Encoding%20%28WEGE%29%2C%20which%20leverages%20the%20discrete%20wavelet%0Atransform%20%28DWT%29%20to%20compute%20energy%20scores%20and%20explicitly%20guide%20frequency%0Ainformation%20to%20the%20network.%20Our%20method%20consistently%20outperforms%20existing%20INRs%0Ain%202D%20image%20representation%20and%20restoration%2C%20as%20well%20as%203D%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13544v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFLAIR%253A%2520Frequency%2520and%2520Locality-Aware%2520Implicit%2520Neural%2520Representations%26entry.906535625%3DSukhun%2520Ko%2520and%2520Dahyeon%2520Kye%2520and%2520Kyle%2520Min%2520and%2520Chanho%2520Eom%2520and%2520Jihyong%2520Oh%26entry.1292438233%3D%2520%2520Implicit%2520Neural%2520Representations%2520%2528INRs%2529%2520leverage%2520neural%2520networks%2520to%2520map%250Acoordinates%2520to%2520corresponding%2520signals%252C%2520enabling%2520continuous%2520and%2520compact%250Arepresentations.%2520This%2520paradigm%2520has%2520driven%2520significant%2520advances%2520in%2520various%250Avision%2520tasks.%2520However%252C%2520existing%2520INRs%2520lack%2520frequency%2520selectivity%252C%2520spatial%250Alocalization%252C%2520and%2520sparse%2520representations%252C%2520leading%2520to%2520an%2520over-reliance%2520on%250Aredundant%2520signal%2520components.%2520Consequently%252C%2520they%2520exhibit%2520spectral%2520bias%252C%2520tending%250Ato%2520learn%2520low-frequency%2520components%2520early%2520while%2520struggling%2520to%2520capture%2520fine%250Ahigh-frequency%2520details.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520FLAIR%2520%2528Frequency-%250Aand%2520Locality-Aware%2520Implicit%2520Neural%2520Representations%2529%252C%2520which%2520incorporates%2520two%2520key%250Ainnovations.%2520The%2520first%2520is%2520RC-GAUSS%252C%2520a%2520novel%2520activation%2520designed%2520for%2520explicit%250Afrequency%2520selection%2520and%2520spatial%2520localization%2520under%2520the%2520constraints%2520of%2520the%250Atime-frequency%2520uncertainty%2520principle%2520%2528TFUP%2529.%2520The%2520second%2520is%250AWavelet-Energy-Guided%2520Encoding%2520%2528WEGE%2529%252C%2520which%2520leverages%2520the%2520discrete%2520wavelet%250Atransform%2520%2528DWT%2529%2520to%2520compute%2520energy%2520scores%2520and%2520explicitly%2520guide%2520frequency%250Ainformation%2520to%2520the%2520network.%2520Our%2520method%2520consistently%2520outperforms%2520existing%2520INRs%250Ain%25202D%2520image%2520representation%2520and%2520restoration%252C%2520as%2520well%2520as%25203D%2520reconstruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13544v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FLAIR%3A%20Frequency%20and%20Locality-Aware%20Implicit%20Neural%20Representations&entry.906535625=Sukhun%20Ko%20and%20Dahyeon%20Kye%20and%20Kyle%20Min%20and%20Chanho%20Eom%20and%20Jihyong%20Oh&entry.1292438233=%20%20Implicit%20Neural%20Representations%20%28INRs%29%20leverage%20neural%20networks%20to%20map%0Acoordinates%20to%20corresponding%20signals%2C%20enabling%20continuous%20and%20compact%0Arepresentations.%20This%20paradigm%20has%20driven%20significant%20advances%20in%20various%0Avision%20tasks.%20However%2C%20existing%20INRs%20lack%20frequency%20selectivity%2C%20spatial%0Alocalization%2C%20and%20sparse%20representations%2C%20leading%20to%20an%20over-reliance%20on%0Aredundant%20signal%20components.%20Consequently%2C%20they%20exhibit%20spectral%20bias%2C%20tending%0Ato%20learn%20low-frequency%20components%20early%20while%20struggling%20to%20capture%20fine%0Ahigh-frequency%20details.%20To%20address%20these%20issues%2C%20we%20propose%20FLAIR%20%28Frequency-%0Aand%20Locality-Aware%20Implicit%20Neural%20Representations%29%2C%20which%20incorporates%20two%20key%0Ainnovations.%20The%20first%20is%20RC-GAUSS%2C%20a%20novel%20activation%20designed%20for%20explicit%0Afrequency%20selection%20and%20spatial%20localization%20under%20the%20constraints%20of%20the%0Atime-frequency%20uncertainty%20principle%20%28TFUP%29.%20The%20second%20is%0AWavelet-Energy-Guided%20Encoding%20%28WEGE%29%2C%20which%20leverages%20the%20discrete%20wavelet%0Atransform%20%28DWT%29%20to%20compute%20energy%20scores%20and%20explicitly%20guide%20frequency%0Ainformation%20to%20the%20network.%20Our%20method%20consistently%20outperforms%20existing%20INRs%0Ain%202D%20image%20representation%20and%20restoration%2C%20as%20well%20as%203D%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13544v2&entry.124074799=Read"},
{"title": "CAMA: Enhancing Multimodal In-Context Learning with Context-Aware\n  Modulated Attention", "author": "Yanshu Li and Jianjiang Yang and Ziteng Yang and Bozheng Li and Hongyang He and Zhengtao Yao and Ligong Han and Yingjie Victor Chen and Songlin Fei and Dongfang Liu and Ruixiang Tang", "abstract": "  Multimodal in-context learning (ICL) is emerging as a key capability that\nenables large vision-language models (LVLMs) to adapt to novel tasks without\nparameter updates, expanding their utility across various real-world\napplications. However, ICL remains unstable, even with well-matched in-context\ndemonstrations (ICDs), suggesting that LVLMs struggle to fully utilize the\nprovided context. While existing efforts focus on prompt engineering or\npost-hoc logit calibration, we instead investigate the underlying attention\ndynamics to overcome LVLMs' inherent limitations. We identify two critical\ndeficits in their self-attention that impair effective ICL. To bridge the gap,\nwe propose \\textbf{Context-Aware Modulated Attention} (CAMA), a plug-and-play\nand training-free method that dynamically modulates LVLM's attention logits\nbased on the input in-context sequence. CAMA employs a two-stage attention\nmodulation to address both identified deficits, enhancing the focus on\nsemantically significant tokens, particularly visual ones. Across four LVLMs\nand seven benchmarks, CAMA consistently outperforms vanilla models and\nbaselines, demonstrating great effectiveness and generalization. It can also\nactivate the desired effects of prompt engineering methods and remains robust\nunder diverse sequence configurations. Thus, CAMA paves the way for deeper\nexplorations of attention dynamics to advance multimodal reasoning.\n", "link": "http://arxiv.org/abs/2505.17097v2", "date": "2025-08-22", "relevancy": 2.7811, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5985}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5351}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5351}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAMA%3A%20Enhancing%20Multimodal%20In-Context%20Learning%20with%20Context-Aware%0A%20%20Modulated%20Attention&body=Title%3A%20CAMA%3A%20Enhancing%20Multimodal%20In-Context%20Learning%20with%20Context-Aware%0A%20%20Modulated%20Attention%0AAuthor%3A%20Yanshu%20Li%20and%20Jianjiang%20Yang%20and%20Ziteng%20Yang%20and%20Bozheng%20Li%20and%20Hongyang%20He%20and%20Zhengtao%20Yao%20and%20Ligong%20Han%20and%20Yingjie%20Victor%20Chen%20and%20Songlin%20Fei%20and%20Dongfang%20Liu%20and%20Ruixiang%20Tang%0AAbstract%3A%20%20%20Multimodal%20in-context%20learning%20%28ICL%29%20is%20emerging%20as%20a%20key%20capability%20that%0Aenables%20large%20vision-language%20models%20%28LVLMs%29%20to%20adapt%20to%20novel%20tasks%20without%0Aparameter%20updates%2C%20expanding%20their%20utility%20across%20various%20real-world%0Aapplications.%20However%2C%20ICL%20remains%20unstable%2C%20even%20with%20well-matched%20in-context%0Ademonstrations%20%28ICDs%29%2C%20suggesting%20that%20LVLMs%20struggle%20to%20fully%20utilize%20the%0Aprovided%20context.%20While%20existing%20efforts%20focus%20on%20prompt%20engineering%20or%0Apost-hoc%20logit%20calibration%2C%20we%20instead%20investigate%20the%20underlying%20attention%0Adynamics%20to%20overcome%20LVLMs%27%20inherent%20limitations.%20We%20identify%20two%20critical%0Adeficits%20in%20their%20self-attention%20that%20impair%20effective%20ICL.%20To%20bridge%20the%20gap%2C%0Awe%20propose%20%5Ctextbf%7BContext-Aware%20Modulated%20Attention%7D%20%28CAMA%29%2C%20a%20plug-and-play%0Aand%20training-free%20method%20that%20dynamically%20modulates%20LVLM%27s%20attention%20logits%0Abased%20on%20the%20input%20in-context%20sequence.%20CAMA%20employs%20a%20two-stage%20attention%0Amodulation%20to%20address%20both%20identified%20deficits%2C%20enhancing%20the%20focus%20on%0Asemantically%20significant%20tokens%2C%20particularly%20visual%20ones.%20Across%20four%20LVLMs%0Aand%20seven%20benchmarks%2C%20CAMA%20consistently%20outperforms%20vanilla%20models%20and%0Abaselines%2C%20demonstrating%20great%20effectiveness%20and%20generalization.%20It%20can%20also%0Aactivate%20the%20desired%20effects%20of%20prompt%20engineering%20methods%20and%20remains%20robust%0Aunder%20diverse%20sequence%20configurations.%20Thus%2C%20CAMA%20paves%20the%20way%20for%20deeper%0Aexplorations%20of%20attention%20dynamics%20to%20advance%20multimodal%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17097v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAMA%253A%2520Enhancing%2520Multimodal%2520In-Context%2520Learning%2520with%2520Context-Aware%250A%2520%2520Modulated%2520Attention%26entry.906535625%3DYanshu%2520Li%2520and%2520Jianjiang%2520Yang%2520and%2520Ziteng%2520Yang%2520and%2520Bozheng%2520Li%2520and%2520Hongyang%2520He%2520and%2520Zhengtao%2520Yao%2520and%2520Ligong%2520Han%2520and%2520Yingjie%2520Victor%2520Chen%2520and%2520Songlin%2520Fei%2520and%2520Dongfang%2520Liu%2520and%2520Ruixiang%2520Tang%26entry.1292438233%3D%2520%2520Multimodal%2520in-context%2520learning%2520%2528ICL%2529%2520is%2520emerging%2520as%2520a%2520key%2520capability%2520that%250Aenables%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%2520to%2520adapt%2520to%2520novel%2520tasks%2520without%250Aparameter%2520updates%252C%2520expanding%2520their%2520utility%2520across%2520various%2520real-world%250Aapplications.%2520However%252C%2520ICL%2520remains%2520unstable%252C%2520even%2520with%2520well-matched%2520in-context%250Ademonstrations%2520%2528ICDs%2529%252C%2520suggesting%2520that%2520LVLMs%2520struggle%2520to%2520fully%2520utilize%2520the%250Aprovided%2520context.%2520While%2520existing%2520efforts%2520focus%2520on%2520prompt%2520engineering%2520or%250Apost-hoc%2520logit%2520calibration%252C%2520we%2520instead%2520investigate%2520the%2520underlying%2520attention%250Adynamics%2520to%2520overcome%2520LVLMs%2527%2520inherent%2520limitations.%2520We%2520identify%2520two%2520critical%250Adeficits%2520in%2520their%2520self-attention%2520that%2520impair%2520effective%2520ICL.%2520To%2520bridge%2520the%2520gap%252C%250Awe%2520propose%2520%255Ctextbf%257BContext-Aware%2520Modulated%2520Attention%257D%2520%2528CAMA%2529%252C%2520a%2520plug-and-play%250Aand%2520training-free%2520method%2520that%2520dynamically%2520modulates%2520LVLM%2527s%2520attention%2520logits%250Abased%2520on%2520the%2520input%2520in-context%2520sequence.%2520CAMA%2520employs%2520a%2520two-stage%2520attention%250Amodulation%2520to%2520address%2520both%2520identified%2520deficits%252C%2520enhancing%2520the%2520focus%2520on%250Asemantically%2520significant%2520tokens%252C%2520particularly%2520visual%2520ones.%2520Across%2520four%2520LVLMs%250Aand%2520seven%2520benchmarks%252C%2520CAMA%2520consistently%2520outperforms%2520vanilla%2520models%2520and%250Abaselines%252C%2520demonstrating%2520great%2520effectiveness%2520and%2520generalization.%2520It%2520can%2520also%250Aactivate%2520the%2520desired%2520effects%2520of%2520prompt%2520engineering%2520methods%2520and%2520remains%2520robust%250Aunder%2520diverse%2520sequence%2520configurations.%2520Thus%252C%2520CAMA%2520paves%2520the%2520way%2520for%2520deeper%250Aexplorations%2520of%2520attention%2520dynamics%2520to%2520advance%2520multimodal%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17097v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAMA%3A%20Enhancing%20Multimodal%20In-Context%20Learning%20with%20Context-Aware%0A%20%20Modulated%20Attention&entry.906535625=Yanshu%20Li%20and%20Jianjiang%20Yang%20and%20Ziteng%20Yang%20and%20Bozheng%20Li%20and%20Hongyang%20He%20and%20Zhengtao%20Yao%20and%20Ligong%20Han%20and%20Yingjie%20Victor%20Chen%20and%20Songlin%20Fei%20and%20Dongfang%20Liu%20and%20Ruixiang%20Tang&entry.1292438233=%20%20Multimodal%20in-context%20learning%20%28ICL%29%20is%20emerging%20as%20a%20key%20capability%20that%0Aenables%20large%20vision-language%20models%20%28LVLMs%29%20to%20adapt%20to%20novel%20tasks%20without%0Aparameter%20updates%2C%20expanding%20their%20utility%20across%20various%20real-world%0Aapplications.%20However%2C%20ICL%20remains%20unstable%2C%20even%20with%20well-matched%20in-context%0Ademonstrations%20%28ICDs%29%2C%20suggesting%20that%20LVLMs%20struggle%20to%20fully%20utilize%20the%0Aprovided%20context.%20While%20existing%20efforts%20focus%20on%20prompt%20engineering%20or%0Apost-hoc%20logit%20calibration%2C%20we%20instead%20investigate%20the%20underlying%20attention%0Adynamics%20to%20overcome%20LVLMs%27%20inherent%20limitations.%20We%20identify%20two%20critical%0Adeficits%20in%20their%20self-attention%20that%20impair%20effective%20ICL.%20To%20bridge%20the%20gap%2C%0Awe%20propose%20%5Ctextbf%7BContext-Aware%20Modulated%20Attention%7D%20%28CAMA%29%2C%20a%20plug-and-play%0Aand%20training-free%20method%20that%20dynamically%20modulates%20LVLM%27s%20attention%20logits%0Abased%20on%20the%20input%20in-context%20sequence.%20CAMA%20employs%20a%20two-stage%20attention%0Amodulation%20to%20address%20both%20identified%20deficits%2C%20enhancing%20the%20focus%20on%0Asemantically%20significant%20tokens%2C%20particularly%20visual%20ones.%20Across%20four%20LVLMs%0Aand%20seven%20benchmarks%2C%20CAMA%20consistently%20outperforms%20vanilla%20models%20and%0Abaselines%2C%20demonstrating%20great%20effectiveness%20and%20generalization.%20It%20can%20also%0Aactivate%20the%20desired%20effects%20of%20prompt%20engineering%20methods%20and%20remains%20robust%0Aunder%20diverse%20sequence%20configurations.%20Thus%2C%20CAMA%20paves%20the%20way%20for%20deeper%0Aexplorations%20of%20attention%20dynamics%20to%20advance%20multimodal%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17097v2&entry.124074799=Read"},
{"title": "IRSAMap:Towards Large-Scale, High-Resolution Land Cover Map\n  Vectorization", "author": "Yu Meng and Ligao Deng and Zhihao Xi and Jiansheng Chen and Jingbo Chen and Anzhi Yue and Diyou Liu and Kai Li and Chenhao Wang and Kaiyu Li and Yupeng Deng and Xian Sun", "abstract": "  With the enhancement of remote sensing image resolution and the rapid\nadvancement of deep learning, land cover mapping is transitioning from\npixel-level segmentation to object-based vector modeling. This shift demands\nmore from deep learning models, requiring precise object boundaries and\ntopological consistency. However, existing datasets face three main challenges:\nlimited class annotations, small data scale, and lack of spatial structural\ninformation. To overcome these issues, we introduce IRSAMap, the first global\nremote sensing dataset for large-scale, high-resolution, multi-feature land\ncover vector mapping. IRSAMap offers four key advantages: 1) a comprehensive\nvector annotation system with over 1.8 million instances of 10 typical objects\n(e.g., buildings, roads, rivers), ensuring semantic and spatial accuracy; 2) an\nintelligent annotation workflow combining manual and AI-based methods to\nimprove efficiency and consistency; 3) global coverage across 79 regions in six\ncontinents, totaling over 1,000 km; and 4) multi-task adaptability for tasks\nlike pixel-level classification, building outline extraction, road centerline\nextraction, and panoramic segmentation. IRSAMap provides a standardized\nbenchmark for the shift from pixel-based to object-based approaches, advancing\ngeographic feature automation and collaborative modeling. It is valuable for\nglobal geographic information updates and digital twin construction. The\ndataset is publicly available at https://github.com/ucas-dlg/IRSAMap\n", "link": "http://arxiv.org/abs/2508.16272v1", "date": "2025-08-22", "relevancy": 2.6846, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5795}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5214}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5099}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IRSAMap%3ATowards%20Large-Scale%2C%20High-Resolution%20Land%20Cover%20Map%0A%20%20Vectorization&body=Title%3A%20IRSAMap%3ATowards%20Large-Scale%2C%20High-Resolution%20Land%20Cover%20Map%0A%20%20Vectorization%0AAuthor%3A%20Yu%20Meng%20and%20Ligao%20Deng%20and%20Zhihao%20Xi%20and%20Jiansheng%20Chen%20and%20Jingbo%20Chen%20and%20Anzhi%20Yue%20and%20Diyou%20Liu%20and%20Kai%20Li%20and%20Chenhao%20Wang%20and%20Kaiyu%20Li%20and%20Yupeng%20Deng%20and%20Xian%20Sun%0AAbstract%3A%20%20%20With%20the%20enhancement%20of%20remote%20sensing%20image%20resolution%20and%20the%20rapid%0Aadvancement%20of%20deep%20learning%2C%20land%20cover%20mapping%20is%20transitioning%20from%0Apixel-level%20segmentation%20to%20object-based%20vector%20modeling.%20This%20shift%20demands%0Amore%20from%20deep%20learning%20models%2C%20requiring%20precise%20object%20boundaries%20and%0Atopological%20consistency.%20However%2C%20existing%20datasets%20face%20three%20main%20challenges%3A%0Alimited%20class%20annotations%2C%20small%20data%20scale%2C%20and%20lack%20of%20spatial%20structural%0Ainformation.%20To%20overcome%20these%20issues%2C%20we%20introduce%20IRSAMap%2C%20the%20first%20global%0Aremote%20sensing%20dataset%20for%20large-scale%2C%20high-resolution%2C%20multi-feature%20land%0Acover%20vector%20mapping.%20IRSAMap%20offers%20four%20key%20advantages%3A%201%29%20a%20comprehensive%0Avector%20annotation%20system%20with%20over%201.8%20million%20instances%20of%2010%20typical%20objects%0A%28e.g.%2C%20buildings%2C%20roads%2C%20rivers%29%2C%20ensuring%20semantic%20and%20spatial%20accuracy%3B%202%29%20an%0Aintelligent%20annotation%20workflow%20combining%20manual%20and%20AI-based%20methods%20to%0Aimprove%20efficiency%20and%20consistency%3B%203%29%20global%20coverage%20across%2079%20regions%20in%20six%0Acontinents%2C%20totaling%20over%201%2C000%20km%3B%20and%204%29%20multi-task%20adaptability%20for%20tasks%0Alike%20pixel-level%20classification%2C%20building%20outline%20extraction%2C%20road%20centerline%0Aextraction%2C%20and%20panoramic%20segmentation.%20IRSAMap%20provides%20a%20standardized%0Abenchmark%20for%20the%20shift%20from%20pixel-based%20to%20object-based%20approaches%2C%20advancing%0Ageographic%20feature%20automation%20and%20collaborative%20modeling.%20It%20is%20valuable%20for%0Aglobal%20geographic%20information%20updates%20and%20digital%20twin%20construction.%20The%0Adataset%20is%20publicly%20available%20at%20https%3A//github.com/ucas-dlg/IRSAMap%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16272v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIRSAMap%253ATowards%2520Large-Scale%252C%2520High-Resolution%2520Land%2520Cover%2520Map%250A%2520%2520Vectorization%26entry.906535625%3DYu%2520Meng%2520and%2520Ligao%2520Deng%2520and%2520Zhihao%2520Xi%2520and%2520Jiansheng%2520Chen%2520and%2520Jingbo%2520Chen%2520and%2520Anzhi%2520Yue%2520and%2520Diyou%2520Liu%2520and%2520Kai%2520Li%2520and%2520Chenhao%2520Wang%2520and%2520Kaiyu%2520Li%2520and%2520Yupeng%2520Deng%2520and%2520Xian%2520Sun%26entry.1292438233%3D%2520%2520With%2520the%2520enhancement%2520of%2520remote%2520sensing%2520image%2520resolution%2520and%2520the%2520rapid%250Aadvancement%2520of%2520deep%2520learning%252C%2520land%2520cover%2520mapping%2520is%2520transitioning%2520from%250Apixel-level%2520segmentation%2520to%2520object-based%2520vector%2520modeling.%2520This%2520shift%2520demands%250Amore%2520from%2520deep%2520learning%2520models%252C%2520requiring%2520precise%2520object%2520boundaries%2520and%250Atopological%2520consistency.%2520However%252C%2520existing%2520datasets%2520face%2520three%2520main%2520challenges%253A%250Alimited%2520class%2520annotations%252C%2520small%2520data%2520scale%252C%2520and%2520lack%2520of%2520spatial%2520structural%250Ainformation.%2520To%2520overcome%2520these%2520issues%252C%2520we%2520introduce%2520IRSAMap%252C%2520the%2520first%2520global%250Aremote%2520sensing%2520dataset%2520for%2520large-scale%252C%2520high-resolution%252C%2520multi-feature%2520land%250Acover%2520vector%2520mapping.%2520IRSAMap%2520offers%2520four%2520key%2520advantages%253A%25201%2529%2520a%2520comprehensive%250Avector%2520annotation%2520system%2520with%2520over%25201.8%2520million%2520instances%2520of%252010%2520typical%2520objects%250A%2528e.g.%252C%2520buildings%252C%2520roads%252C%2520rivers%2529%252C%2520ensuring%2520semantic%2520and%2520spatial%2520accuracy%253B%25202%2529%2520an%250Aintelligent%2520annotation%2520workflow%2520combining%2520manual%2520and%2520AI-based%2520methods%2520to%250Aimprove%2520efficiency%2520and%2520consistency%253B%25203%2529%2520global%2520coverage%2520across%252079%2520regions%2520in%2520six%250Acontinents%252C%2520totaling%2520over%25201%252C000%2520km%253B%2520and%25204%2529%2520multi-task%2520adaptability%2520for%2520tasks%250Alike%2520pixel-level%2520classification%252C%2520building%2520outline%2520extraction%252C%2520road%2520centerline%250Aextraction%252C%2520and%2520panoramic%2520segmentation.%2520IRSAMap%2520provides%2520a%2520standardized%250Abenchmark%2520for%2520the%2520shift%2520from%2520pixel-based%2520to%2520object-based%2520approaches%252C%2520advancing%250Ageographic%2520feature%2520automation%2520and%2520collaborative%2520modeling.%2520It%2520is%2520valuable%2520for%250Aglobal%2520geographic%2520information%2520updates%2520and%2520digital%2520twin%2520construction.%2520The%250Adataset%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/ucas-dlg/IRSAMap%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16272v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IRSAMap%3ATowards%20Large-Scale%2C%20High-Resolution%20Land%20Cover%20Map%0A%20%20Vectorization&entry.906535625=Yu%20Meng%20and%20Ligao%20Deng%20and%20Zhihao%20Xi%20and%20Jiansheng%20Chen%20and%20Jingbo%20Chen%20and%20Anzhi%20Yue%20and%20Diyou%20Liu%20and%20Kai%20Li%20and%20Chenhao%20Wang%20and%20Kaiyu%20Li%20and%20Yupeng%20Deng%20and%20Xian%20Sun&entry.1292438233=%20%20With%20the%20enhancement%20of%20remote%20sensing%20image%20resolution%20and%20the%20rapid%0Aadvancement%20of%20deep%20learning%2C%20land%20cover%20mapping%20is%20transitioning%20from%0Apixel-level%20segmentation%20to%20object-based%20vector%20modeling.%20This%20shift%20demands%0Amore%20from%20deep%20learning%20models%2C%20requiring%20precise%20object%20boundaries%20and%0Atopological%20consistency.%20However%2C%20existing%20datasets%20face%20three%20main%20challenges%3A%0Alimited%20class%20annotations%2C%20small%20data%20scale%2C%20and%20lack%20of%20spatial%20structural%0Ainformation.%20To%20overcome%20these%20issues%2C%20we%20introduce%20IRSAMap%2C%20the%20first%20global%0Aremote%20sensing%20dataset%20for%20large-scale%2C%20high-resolution%2C%20multi-feature%20land%0Acover%20vector%20mapping.%20IRSAMap%20offers%20four%20key%20advantages%3A%201%29%20a%20comprehensive%0Avector%20annotation%20system%20with%20over%201.8%20million%20instances%20of%2010%20typical%20objects%0A%28e.g.%2C%20buildings%2C%20roads%2C%20rivers%29%2C%20ensuring%20semantic%20and%20spatial%20accuracy%3B%202%29%20an%0Aintelligent%20annotation%20workflow%20combining%20manual%20and%20AI-based%20methods%20to%0Aimprove%20efficiency%20and%20consistency%3B%203%29%20global%20coverage%20across%2079%20regions%20in%20six%0Acontinents%2C%20totaling%20over%201%2C000%20km%3B%20and%204%29%20multi-task%20adaptability%20for%20tasks%0Alike%20pixel-level%20classification%2C%20building%20outline%20extraction%2C%20road%20centerline%0Aextraction%2C%20and%20panoramic%20segmentation.%20IRSAMap%20provides%20a%20standardized%0Abenchmark%20for%20the%20shift%20from%20pixel-based%20to%20object-based%20approaches%2C%20advancing%0Ageographic%20feature%20automation%20and%20collaborative%20modeling.%20It%20is%20valuable%20for%0Aglobal%20geographic%20information%20updates%20and%20digital%20twin%20construction.%20The%0Adataset%20is%20publicly%20available%20at%20https%3A//github.com/ucas-dlg/IRSAMap%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16272v1&entry.124074799=Read"},
{"title": "MV-RAG: Retrieval Augmented Multiview Diffusion", "author": "Yosef Dayani and Omer Benishu and Sagie Benaim", "abstract": "  Text-to-3D generation approaches have advanced significantly by leveraging\npretrained 2D diffusion priors, producing high-quality and 3D-consistent\noutputs. However, they often fail to produce out-of-domain (OOD) or rare\nconcepts, yielding inconsistent or inaccurate results. To this end, we propose\nMV-RAG, a novel text-to-3D pipeline that first retrieves relevant 2D images\nfrom a large in-the-wild 2D database and then conditions a multiview diffusion\nmodel on these images to synthesize consistent and accurate multiview outputs.\nTraining such a retrieval-conditioned model is achieved via a novel hybrid\nstrategy bridging structured multiview data and diverse 2D image collections.\nThis involves training on multiview data using augmented conditioning views\nthat simulate retrieval variance for view-specific reconstruction, alongside\ntraining on sets of retrieved real-world 2D images using a distinctive held-out\nview prediction objective: the model predicts the held-out view from the other\nviews to infer 3D consistency from 2D data. To facilitate a rigorous OOD\nevaluation, we introduce a new collection of challenging OOD prompts.\nExperiments against state-of-the-art text-to-3D, image-to-3D, and\npersonalization baselines show that our approach significantly improves 3D\nconsistency, photorealism, and text adherence for OOD/rare concepts, while\nmaintaining competitive performance on standard benchmarks.\n", "link": "http://arxiv.org/abs/2508.16577v1", "date": "2025-08-22", "relevancy": 2.6808, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6845}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6845}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5985}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MV-RAG%3A%20Retrieval%20Augmented%20Multiview%20Diffusion&body=Title%3A%20MV-RAG%3A%20Retrieval%20Augmented%20Multiview%20Diffusion%0AAuthor%3A%20Yosef%20Dayani%20and%20Omer%20Benishu%20and%20Sagie%20Benaim%0AAbstract%3A%20%20%20Text-to-3D%20generation%20approaches%20have%20advanced%20significantly%20by%20leveraging%0Apretrained%202D%20diffusion%20priors%2C%20producing%20high-quality%20and%203D-consistent%0Aoutputs.%20However%2C%20they%20often%20fail%20to%20produce%20out-of-domain%20%28OOD%29%20or%20rare%0Aconcepts%2C%20yielding%20inconsistent%20or%20inaccurate%20results.%20To%20this%20end%2C%20we%20propose%0AMV-RAG%2C%20a%20novel%20text-to-3D%20pipeline%20that%20first%20retrieves%20relevant%202D%20images%0Afrom%20a%20large%20in-the-wild%202D%20database%20and%20then%20conditions%20a%20multiview%20diffusion%0Amodel%20on%20these%20images%20to%20synthesize%20consistent%20and%20accurate%20multiview%20outputs.%0ATraining%20such%20a%20retrieval-conditioned%20model%20is%20achieved%20via%20a%20novel%20hybrid%0Astrategy%20bridging%20structured%20multiview%20data%20and%20diverse%202D%20image%20collections.%0AThis%20involves%20training%20on%20multiview%20data%20using%20augmented%20conditioning%20views%0Athat%20simulate%20retrieval%20variance%20for%20view-specific%20reconstruction%2C%20alongside%0Atraining%20on%20sets%20of%20retrieved%20real-world%202D%20images%20using%20a%20distinctive%20held-out%0Aview%20prediction%20objective%3A%20the%20model%20predicts%20the%20held-out%20view%20from%20the%20other%0Aviews%20to%20infer%203D%20consistency%20from%202D%20data.%20To%20facilitate%20a%20rigorous%20OOD%0Aevaluation%2C%20we%20introduce%20a%20new%20collection%20of%20challenging%20OOD%20prompts.%0AExperiments%20against%20state-of-the-art%20text-to-3D%2C%20image-to-3D%2C%20and%0Apersonalization%20baselines%20show%20that%20our%20approach%20significantly%20improves%203D%0Aconsistency%2C%20photorealism%2C%20and%20text%20adherence%20for%20OOD/rare%20concepts%2C%20while%0Amaintaining%20competitive%20performance%20on%20standard%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16577v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMV-RAG%253A%2520Retrieval%2520Augmented%2520Multiview%2520Diffusion%26entry.906535625%3DYosef%2520Dayani%2520and%2520Omer%2520Benishu%2520and%2520Sagie%2520Benaim%26entry.1292438233%3D%2520%2520Text-to-3D%2520generation%2520approaches%2520have%2520advanced%2520significantly%2520by%2520leveraging%250Apretrained%25202D%2520diffusion%2520priors%252C%2520producing%2520high-quality%2520and%25203D-consistent%250Aoutputs.%2520However%252C%2520they%2520often%2520fail%2520to%2520produce%2520out-of-domain%2520%2528OOD%2529%2520or%2520rare%250Aconcepts%252C%2520yielding%2520inconsistent%2520or%2520inaccurate%2520results.%2520To%2520this%2520end%252C%2520we%2520propose%250AMV-RAG%252C%2520a%2520novel%2520text-to-3D%2520pipeline%2520that%2520first%2520retrieves%2520relevant%25202D%2520images%250Afrom%2520a%2520large%2520in-the-wild%25202D%2520database%2520and%2520then%2520conditions%2520a%2520multiview%2520diffusion%250Amodel%2520on%2520these%2520images%2520to%2520synthesize%2520consistent%2520and%2520accurate%2520multiview%2520outputs.%250ATraining%2520such%2520a%2520retrieval-conditioned%2520model%2520is%2520achieved%2520via%2520a%2520novel%2520hybrid%250Astrategy%2520bridging%2520structured%2520multiview%2520data%2520and%2520diverse%25202D%2520image%2520collections.%250AThis%2520involves%2520training%2520on%2520multiview%2520data%2520using%2520augmented%2520conditioning%2520views%250Athat%2520simulate%2520retrieval%2520variance%2520for%2520view-specific%2520reconstruction%252C%2520alongside%250Atraining%2520on%2520sets%2520of%2520retrieved%2520real-world%25202D%2520images%2520using%2520a%2520distinctive%2520held-out%250Aview%2520prediction%2520objective%253A%2520the%2520model%2520predicts%2520the%2520held-out%2520view%2520from%2520the%2520other%250Aviews%2520to%2520infer%25203D%2520consistency%2520from%25202D%2520data.%2520To%2520facilitate%2520a%2520rigorous%2520OOD%250Aevaluation%252C%2520we%2520introduce%2520a%2520new%2520collection%2520of%2520challenging%2520OOD%2520prompts.%250AExperiments%2520against%2520state-of-the-art%2520text-to-3D%252C%2520image-to-3D%252C%2520and%250Apersonalization%2520baselines%2520show%2520that%2520our%2520approach%2520significantly%2520improves%25203D%250Aconsistency%252C%2520photorealism%252C%2520and%2520text%2520adherence%2520for%2520OOD/rare%2520concepts%252C%2520while%250Amaintaining%2520competitive%2520performance%2520on%2520standard%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16577v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MV-RAG%3A%20Retrieval%20Augmented%20Multiview%20Diffusion&entry.906535625=Yosef%20Dayani%20and%20Omer%20Benishu%20and%20Sagie%20Benaim&entry.1292438233=%20%20Text-to-3D%20generation%20approaches%20have%20advanced%20significantly%20by%20leveraging%0Apretrained%202D%20diffusion%20priors%2C%20producing%20high-quality%20and%203D-consistent%0Aoutputs.%20However%2C%20they%20often%20fail%20to%20produce%20out-of-domain%20%28OOD%29%20or%20rare%0Aconcepts%2C%20yielding%20inconsistent%20or%20inaccurate%20results.%20To%20this%20end%2C%20we%20propose%0AMV-RAG%2C%20a%20novel%20text-to-3D%20pipeline%20that%20first%20retrieves%20relevant%202D%20images%0Afrom%20a%20large%20in-the-wild%202D%20database%20and%20then%20conditions%20a%20multiview%20diffusion%0Amodel%20on%20these%20images%20to%20synthesize%20consistent%20and%20accurate%20multiview%20outputs.%0ATraining%20such%20a%20retrieval-conditioned%20model%20is%20achieved%20via%20a%20novel%20hybrid%0Astrategy%20bridging%20structured%20multiview%20data%20and%20diverse%202D%20image%20collections.%0AThis%20involves%20training%20on%20multiview%20data%20using%20augmented%20conditioning%20views%0Athat%20simulate%20retrieval%20variance%20for%20view-specific%20reconstruction%2C%20alongside%0Atraining%20on%20sets%20of%20retrieved%20real-world%202D%20images%20using%20a%20distinctive%20held-out%0Aview%20prediction%20objective%3A%20the%20model%20predicts%20the%20held-out%20view%20from%20the%20other%0Aviews%20to%20infer%203D%20consistency%20from%202D%20data.%20To%20facilitate%20a%20rigorous%20OOD%0Aevaluation%2C%20we%20introduce%20a%20new%20collection%20of%20challenging%20OOD%20prompts.%0AExperiments%20against%20state-of-the-art%20text-to-3D%2C%20image-to-3D%2C%20and%0Apersonalization%20baselines%20show%20that%20our%20approach%20significantly%20improves%203D%0Aconsistency%2C%20photorealism%2C%20and%20text%20adherence%20for%20OOD/rare%20concepts%2C%20while%0Amaintaining%20competitive%20performance%20on%20standard%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16577v1&entry.124074799=Read"},
{"title": "Retrieval Enhanced Feedback via In-context Neural Error-book", "author": "Jongyeop Hyun and Bumsoo Kim", "abstract": "  Recent advancements in Large Language Models (LLMs) have significantly\nimproved reasoning capabilities, with in-context learning (ICL) emerging as a\nkey technique for adaptation without retraining. While previous works have\nfocused on leveraging correct examples, recent research highlights the\nimportance of learning from errors to enhance performance. However, existing\nmethods lack a structured framework for analyzing and mitigating errors,\nparticularly in Multimodal Large Language Models (MLLMs), where integrating\nvisual and textual inputs adds complexity. To address this issue, we propose\nREFINE: Retrieval-Enhanced Feedback via In-context Neural Error-book, a\nteacher-student framework that systematically structures errors and provides\ntargeted feedback. REFINE introduces three systematic queries to construct\nstructured feedback -- Feed-Target, Feed-Check, and Feed-Path -- to enhance\nmultimodal reasoning by prioritizing relevant visual information, diagnosing\ncritical failure points, and formulating corrective actions. Unlike prior\napproaches that rely on redundant retrievals, REFINE optimizes structured\nfeedback retrieval, improving inference efficiency, token usage, and\nscalability. Our results demonstrate substantial speedup, reduced computational\ncosts, and successful generalization, highlighting REFINE's potential for\nenhancing multimodal reasoning.\n", "link": "http://arxiv.org/abs/2508.16313v1", "date": "2025-08-22", "relevancy": 2.6432, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5374}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5374}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5112}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Retrieval%20Enhanced%20Feedback%20via%20In-context%20Neural%20Error-book&body=Title%3A%20Retrieval%20Enhanced%20Feedback%20via%20In-context%20Neural%20Error-book%0AAuthor%3A%20Jongyeop%20Hyun%20and%20Bumsoo%20Kim%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20significantly%0Aimproved%20reasoning%20capabilities%2C%20with%20in-context%20learning%20%28ICL%29%20emerging%20as%20a%0Akey%20technique%20for%20adaptation%20without%20retraining.%20While%20previous%20works%20have%0Afocused%20on%20leveraging%20correct%20examples%2C%20recent%20research%20highlights%20the%0Aimportance%20of%20learning%20from%20errors%20to%20enhance%20performance.%20However%2C%20existing%0Amethods%20lack%20a%20structured%20framework%20for%20analyzing%20and%20mitigating%20errors%2C%0Aparticularly%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20where%20integrating%0Avisual%20and%20textual%20inputs%20adds%20complexity.%20To%20address%20this%20issue%2C%20we%20propose%0AREFINE%3A%20Retrieval-Enhanced%20Feedback%20via%20In-context%20Neural%20Error-book%2C%20a%0Ateacher-student%20framework%20that%20systematically%20structures%20errors%20and%20provides%0Atargeted%20feedback.%20REFINE%20introduces%20three%20systematic%20queries%20to%20construct%0Astructured%20feedback%20--%20Feed-Target%2C%20Feed-Check%2C%20and%20Feed-Path%20--%20to%20enhance%0Amultimodal%20reasoning%20by%20prioritizing%20relevant%20visual%20information%2C%20diagnosing%0Acritical%20failure%20points%2C%20and%20formulating%20corrective%20actions.%20Unlike%20prior%0Aapproaches%20that%20rely%20on%20redundant%20retrievals%2C%20REFINE%20optimizes%20structured%0Afeedback%20retrieval%2C%20improving%20inference%20efficiency%2C%20token%20usage%2C%20and%0Ascalability.%20Our%20results%20demonstrate%20substantial%20speedup%2C%20reduced%20computational%0Acosts%2C%20and%20successful%20generalization%2C%20highlighting%20REFINE%27s%20potential%20for%0Aenhancing%20multimodal%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16313v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetrieval%2520Enhanced%2520Feedback%2520via%2520In-context%2520Neural%2520Error-book%26entry.906535625%3DJongyeop%2520Hyun%2520and%2520Bumsoo%2520Kim%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520significantly%250Aimproved%2520reasoning%2520capabilities%252C%2520with%2520in-context%2520learning%2520%2528ICL%2529%2520emerging%2520as%2520a%250Akey%2520technique%2520for%2520adaptation%2520without%2520retraining.%2520While%2520previous%2520works%2520have%250Afocused%2520on%2520leveraging%2520correct%2520examples%252C%2520recent%2520research%2520highlights%2520the%250Aimportance%2520of%2520learning%2520from%2520errors%2520to%2520enhance%2520performance.%2520However%252C%2520existing%250Amethods%2520lack%2520a%2520structured%2520framework%2520for%2520analyzing%2520and%2520mitigating%2520errors%252C%250Aparticularly%2520in%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%252C%2520where%2520integrating%250Avisual%2520and%2520textual%2520inputs%2520adds%2520complexity.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%250AREFINE%253A%2520Retrieval-Enhanced%2520Feedback%2520via%2520In-context%2520Neural%2520Error-book%252C%2520a%250Ateacher-student%2520framework%2520that%2520systematically%2520structures%2520errors%2520and%2520provides%250Atargeted%2520feedback.%2520REFINE%2520introduces%2520three%2520systematic%2520queries%2520to%2520construct%250Astructured%2520feedback%2520--%2520Feed-Target%252C%2520Feed-Check%252C%2520and%2520Feed-Path%2520--%2520to%2520enhance%250Amultimodal%2520reasoning%2520by%2520prioritizing%2520relevant%2520visual%2520information%252C%2520diagnosing%250Acritical%2520failure%2520points%252C%2520and%2520formulating%2520corrective%2520actions.%2520Unlike%2520prior%250Aapproaches%2520that%2520rely%2520on%2520redundant%2520retrievals%252C%2520REFINE%2520optimizes%2520structured%250Afeedback%2520retrieval%252C%2520improving%2520inference%2520efficiency%252C%2520token%2520usage%252C%2520and%250Ascalability.%2520Our%2520results%2520demonstrate%2520substantial%2520speedup%252C%2520reduced%2520computational%250Acosts%252C%2520and%2520successful%2520generalization%252C%2520highlighting%2520REFINE%2527s%2520potential%2520for%250Aenhancing%2520multimodal%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16313v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retrieval%20Enhanced%20Feedback%20via%20In-context%20Neural%20Error-book&entry.906535625=Jongyeop%20Hyun%20and%20Bumsoo%20Kim&entry.1292438233=%20%20Recent%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20significantly%0Aimproved%20reasoning%20capabilities%2C%20with%20in-context%20learning%20%28ICL%29%20emerging%20as%20a%0Akey%20technique%20for%20adaptation%20without%20retraining.%20While%20previous%20works%20have%0Afocused%20on%20leveraging%20correct%20examples%2C%20recent%20research%20highlights%20the%0Aimportance%20of%20learning%20from%20errors%20to%20enhance%20performance.%20However%2C%20existing%0Amethods%20lack%20a%20structured%20framework%20for%20analyzing%20and%20mitigating%20errors%2C%0Aparticularly%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20where%20integrating%0Avisual%20and%20textual%20inputs%20adds%20complexity.%20To%20address%20this%20issue%2C%20we%20propose%0AREFINE%3A%20Retrieval-Enhanced%20Feedback%20via%20In-context%20Neural%20Error-book%2C%20a%0Ateacher-student%20framework%20that%20systematically%20structures%20errors%20and%20provides%0Atargeted%20feedback.%20REFINE%20introduces%20three%20systematic%20queries%20to%20construct%0Astructured%20feedback%20--%20Feed-Target%2C%20Feed-Check%2C%20and%20Feed-Path%20--%20to%20enhance%0Amultimodal%20reasoning%20by%20prioritizing%20relevant%20visual%20information%2C%20diagnosing%0Acritical%20failure%20points%2C%20and%20formulating%20corrective%20actions.%20Unlike%20prior%0Aapproaches%20that%20rely%20on%20redundant%20retrievals%2C%20REFINE%20optimizes%20structured%0Afeedback%20retrieval%2C%20improving%20inference%20efficiency%2C%20token%20usage%2C%20and%0Ascalability.%20Our%20results%20demonstrate%20substantial%20speedup%2C%20reduced%20computational%0Acosts%2C%20and%20successful%20generalization%2C%20highlighting%20REFINE%27s%20potential%20for%0Aenhancing%20multimodal%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16313v1&entry.124074799=Read"},
{"title": "NovoMolGen: Rethinking Molecular Language Model Pretraining", "author": "Kamran Chitsaz and Roshan Balaji and Quentin Fournier and Nirav Pravinbhai Bhatt and Sarath Chandar", "abstract": "  Designing de-novo molecules with desired property profiles requires efficient\nexploration of the vast chemical space ranging from $10^{23}$ to $10^{60}$\npossible synthesizable candidates. While various deep generative models have\nbeen developed to design small molecules using diverse input representations,\nMolecular Large Language Models (Mol-LLMs) based on string representations have\nemerged as a scalable approach capable of exploring billions of molecules.\nHowever, there remains limited understanding regarding how standard language\nmodeling practices such as textual representations, tokenization strategies,\nmodel size, and dataset scale impact molecular generation performance. In this\nwork, we systematically investigate these critical aspects by introducing\nNovoMolGen, a family of transformer-based foundation models pretrained on 1.5\nbillion molecules for de-novo molecule generation. Through extensive empirical\nanalyses, we identify a weak correlation between performance metrics measured\nduring pretraining and actual downstream performance, revealing important\ndistinctions between molecular and general NLP training dynamics. NovoMolGen\nestablishes new state-of-the-art results, substantially outperforming prior\nMol-LLMs and specialized generative models in both unconstrained and\ngoal-directed molecular generation tasks, thus providing a robust foundation\nfor advancing efficient and effective molecular modeling strategies.\n", "link": "http://arxiv.org/abs/2508.13408v2", "date": "2025-08-22", "relevancy": 2.6219, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5316}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5208}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NovoMolGen%3A%20Rethinking%20Molecular%20Language%20Model%20Pretraining&body=Title%3A%20NovoMolGen%3A%20Rethinking%20Molecular%20Language%20Model%20Pretraining%0AAuthor%3A%20Kamran%20Chitsaz%20and%20Roshan%20Balaji%20and%20Quentin%20Fournier%20and%20Nirav%20Pravinbhai%20Bhatt%20and%20Sarath%20Chandar%0AAbstract%3A%20%20%20Designing%20de-novo%20molecules%20with%20desired%20property%20profiles%20requires%20efficient%0Aexploration%20of%20the%20vast%20chemical%20space%20ranging%20from%20%2410%5E%7B23%7D%24%20to%20%2410%5E%7B60%7D%24%0Apossible%20synthesizable%20candidates.%20While%20various%20deep%20generative%20models%20have%0Abeen%20developed%20to%20design%20small%20molecules%20using%20diverse%20input%20representations%2C%0AMolecular%20Large%20Language%20Models%20%28Mol-LLMs%29%20based%20on%20string%20representations%20have%0Aemerged%20as%20a%20scalable%20approach%20capable%20of%20exploring%20billions%20of%20molecules.%0AHowever%2C%20there%20remains%20limited%20understanding%20regarding%20how%20standard%20language%0Amodeling%20practices%20such%20as%20textual%20representations%2C%20tokenization%20strategies%2C%0Amodel%20size%2C%20and%20dataset%20scale%20impact%20molecular%20generation%20performance.%20In%20this%0Awork%2C%20we%20systematically%20investigate%20these%20critical%20aspects%20by%20introducing%0ANovoMolGen%2C%20a%20family%20of%20transformer-based%20foundation%20models%20pretrained%20on%201.5%0Abillion%20molecules%20for%20de-novo%20molecule%20generation.%20Through%20extensive%20empirical%0Aanalyses%2C%20we%20identify%20a%20weak%20correlation%20between%20performance%20metrics%20measured%0Aduring%20pretraining%20and%20actual%20downstream%20performance%2C%20revealing%20important%0Adistinctions%20between%20molecular%20and%20general%20NLP%20training%20dynamics.%20NovoMolGen%0Aestablishes%20new%20state-of-the-art%20results%2C%20substantially%20outperforming%20prior%0AMol-LLMs%20and%20specialized%20generative%20models%20in%20both%20unconstrained%20and%0Agoal-directed%20molecular%20generation%20tasks%2C%20thus%20providing%20a%20robust%20foundation%0Afor%20advancing%20efficient%20and%20effective%20molecular%20modeling%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13408v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNovoMolGen%253A%2520Rethinking%2520Molecular%2520Language%2520Model%2520Pretraining%26entry.906535625%3DKamran%2520Chitsaz%2520and%2520Roshan%2520Balaji%2520and%2520Quentin%2520Fournier%2520and%2520Nirav%2520Pravinbhai%2520Bhatt%2520and%2520Sarath%2520Chandar%26entry.1292438233%3D%2520%2520Designing%2520de-novo%2520molecules%2520with%2520desired%2520property%2520profiles%2520requires%2520efficient%250Aexploration%2520of%2520the%2520vast%2520chemical%2520space%2520ranging%2520from%2520%252410%255E%257B23%257D%2524%2520to%2520%252410%255E%257B60%257D%2524%250Apossible%2520synthesizable%2520candidates.%2520While%2520various%2520deep%2520generative%2520models%2520have%250Abeen%2520developed%2520to%2520design%2520small%2520molecules%2520using%2520diverse%2520input%2520representations%252C%250AMolecular%2520Large%2520Language%2520Models%2520%2528Mol-LLMs%2529%2520based%2520on%2520string%2520representations%2520have%250Aemerged%2520as%2520a%2520scalable%2520approach%2520capable%2520of%2520exploring%2520billions%2520of%2520molecules.%250AHowever%252C%2520there%2520remains%2520limited%2520understanding%2520regarding%2520how%2520standard%2520language%250Amodeling%2520practices%2520such%2520as%2520textual%2520representations%252C%2520tokenization%2520strategies%252C%250Amodel%2520size%252C%2520and%2520dataset%2520scale%2520impact%2520molecular%2520generation%2520performance.%2520In%2520this%250Awork%252C%2520we%2520systematically%2520investigate%2520these%2520critical%2520aspects%2520by%2520introducing%250ANovoMolGen%252C%2520a%2520family%2520of%2520transformer-based%2520foundation%2520models%2520pretrained%2520on%25201.5%250Abillion%2520molecules%2520for%2520de-novo%2520molecule%2520generation.%2520Through%2520extensive%2520empirical%250Aanalyses%252C%2520we%2520identify%2520a%2520weak%2520correlation%2520between%2520performance%2520metrics%2520measured%250Aduring%2520pretraining%2520and%2520actual%2520downstream%2520performance%252C%2520revealing%2520important%250Adistinctions%2520between%2520molecular%2520and%2520general%2520NLP%2520training%2520dynamics.%2520NovoMolGen%250Aestablishes%2520new%2520state-of-the-art%2520results%252C%2520substantially%2520outperforming%2520prior%250AMol-LLMs%2520and%2520specialized%2520generative%2520models%2520in%2520both%2520unconstrained%2520and%250Agoal-directed%2520molecular%2520generation%2520tasks%252C%2520thus%2520providing%2520a%2520robust%2520foundation%250Afor%2520advancing%2520efficient%2520and%2520effective%2520molecular%2520modeling%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13408v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NovoMolGen%3A%20Rethinking%20Molecular%20Language%20Model%20Pretraining&entry.906535625=Kamran%20Chitsaz%20and%20Roshan%20Balaji%20and%20Quentin%20Fournier%20and%20Nirav%20Pravinbhai%20Bhatt%20and%20Sarath%20Chandar&entry.1292438233=%20%20Designing%20de-novo%20molecules%20with%20desired%20property%20profiles%20requires%20efficient%0Aexploration%20of%20the%20vast%20chemical%20space%20ranging%20from%20%2410%5E%7B23%7D%24%20to%20%2410%5E%7B60%7D%24%0Apossible%20synthesizable%20candidates.%20While%20various%20deep%20generative%20models%20have%0Abeen%20developed%20to%20design%20small%20molecules%20using%20diverse%20input%20representations%2C%0AMolecular%20Large%20Language%20Models%20%28Mol-LLMs%29%20based%20on%20string%20representations%20have%0Aemerged%20as%20a%20scalable%20approach%20capable%20of%20exploring%20billions%20of%20molecules.%0AHowever%2C%20there%20remains%20limited%20understanding%20regarding%20how%20standard%20language%0Amodeling%20practices%20such%20as%20textual%20representations%2C%20tokenization%20strategies%2C%0Amodel%20size%2C%20and%20dataset%20scale%20impact%20molecular%20generation%20performance.%20In%20this%0Awork%2C%20we%20systematically%20investigate%20these%20critical%20aspects%20by%20introducing%0ANovoMolGen%2C%20a%20family%20of%20transformer-based%20foundation%20models%20pretrained%20on%201.5%0Abillion%20molecules%20for%20de-novo%20molecule%20generation.%20Through%20extensive%20empirical%0Aanalyses%2C%20we%20identify%20a%20weak%20correlation%20between%20performance%20metrics%20measured%0Aduring%20pretraining%20and%20actual%20downstream%20performance%2C%20revealing%20important%0Adistinctions%20between%20molecular%20and%20general%20NLP%20training%20dynamics.%20NovoMolGen%0Aestablishes%20new%20state-of-the-art%20results%2C%20substantially%20outperforming%20prior%0AMol-LLMs%20and%20specialized%20generative%20models%20in%20both%20unconstrained%20and%0Agoal-directed%20molecular%20generation%20tasks%2C%20thus%20providing%20a%20robust%20foundation%0Afor%20advancing%20efficient%20and%20effective%20molecular%20modeling%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13408v2&entry.124074799=Read"},
{"title": "A Disease-Centric Vision-Language Foundation Model for Precision\n  Oncology in Kidney Cancer", "author": "Yuhui Tao and Zhongwei Zhao and Zilong Wang and Xufang Luo and Feng Chen and Kang Wang and Chuanfu Wu and Xue Zhang and Shaoting Zhang and Jiaxi Yao and Xingwei Jin and Xinyang Jiang and Yifan Yang and Dongsheng Li and Lili Qiu and Zhiqiang Shao and Jianming Guo and Nengwang Yu and Shuo Wang and Ying Xiong", "abstract": "  The non-invasive assessment of increasingly incidentally discovered renal\nmasses is a critical challenge in urologic oncology, where diagnostic\nuncertainty frequently leads to the overtreatment of benign or indolent tumors.\nIn this study, we developed and validated RenalCLIP using a dataset of 27,866\nCT scans from 8,809 patients across nine Chinese medical centers and the public\nTCIA cohort, a visual-language foundation model for characterization, diagnosis\nand prognosis of renal mass. The model was developed via a two-stage\npre-training strategy that first enhances the image and text encoders with\ndomain-specific knowledge before aligning them through a contrastive learning\nobjective, to create robust representations for superior generalization and\ndiagnostic precision. RenalCLIP achieved better performance and superior\ngeneralizability across 10 core tasks spanning the full clinical workflow of\nkidney cancer, including anatomical assessment, diagnostic classification, and\nsurvival prediction, compared with other state-of-the-art general-purpose CT\nfoundation models. Especially, for complicated task like recurrence-free\nsurvival prediction in the TCIA cohort, RenalCLIP achieved a C-index of 0.726,\nrepresenting a substantial improvement of approximately 20% over the leading\nbaselines. Furthermore, RenalCLIP's pre-training imparted remarkable data\nefficiency; in the diagnostic classification task, it only needs 20% training\ndata to achieve the peak performance of all baseline models even after they\nwere fully fine-tuned on 100% of the data. Additionally, it achieved superior\nperformance in report generation, image-text retrieval and zero-shot diagnosis\ntasks. Our findings establish that RenalCLIP provides a robust tool with the\npotential to enhance diagnostic accuracy, refine prognostic stratification, and\npersonalize the management of patients with kidney cancer.\n", "link": "http://arxiv.org/abs/2508.16569v1", "date": "2025-08-22", "relevancy": 2.6203, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5361}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5361}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Disease-Centric%20Vision-Language%20Foundation%20Model%20for%20Precision%0A%20%20Oncology%20in%20Kidney%20Cancer&body=Title%3A%20A%20Disease-Centric%20Vision-Language%20Foundation%20Model%20for%20Precision%0A%20%20Oncology%20in%20Kidney%20Cancer%0AAuthor%3A%20Yuhui%20Tao%20and%20Zhongwei%20Zhao%20and%20Zilong%20Wang%20and%20Xufang%20Luo%20and%20Feng%20Chen%20and%20Kang%20Wang%20and%20Chuanfu%20Wu%20and%20Xue%20Zhang%20and%20Shaoting%20Zhang%20and%20Jiaxi%20Yao%20and%20Xingwei%20Jin%20and%20Xinyang%20Jiang%20and%20Yifan%20Yang%20and%20Dongsheng%20Li%20and%20Lili%20Qiu%20and%20Zhiqiang%20Shao%20and%20Jianming%20Guo%20and%20Nengwang%20Yu%20and%20Shuo%20Wang%20and%20Ying%20Xiong%0AAbstract%3A%20%20%20The%20non-invasive%20assessment%20of%20increasingly%20incidentally%20discovered%20renal%0Amasses%20is%20a%20critical%20challenge%20in%20urologic%20oncology%2C%20where%20diagnostic%0Auncertainty%20frequently%20leads%20to%20the%20overtreatment%20of%20benign%20or%20indolent%20tumors.%0AIn%20this%20study%2C%20we%20developed%20and%20validated%20RenalCLIP%20using%20a%20dataset%20of%2027%2C866%0ACT%20scans%20from%208%2C809%20patients%20across%20nine%20Chinese%20medical%20centers%20and%20the%20public%0ATCIA%20cohort%2C%20a%20visual-language%20foundation%20model%20for%20characterization%2C%20diagnosis%0Aand%20prognosis%20of%20renal%20mass.%20The%20model%20was%20developed%20via%20a%20two-stage%0Apre-training%20strategy%20that%20first%20enhances%20the%20image%20and%20text%20encoders%20with%0Adomain-specific%20knowledge%20before%20aligning%20them%20through%20a%20contrastive%20learning%0Aobjective%2C%20to%20create%20robust%20representations%20for%20superior%20generalization%20and%0Adiagnostic%20precision.%20RenalCLIP%20achieved%20better%20performance%20and%20superior%0Ageneralizability%20across%2010%20core%20tasks%20spanning%20the%20full%20clinical%20workflow%20of%0Akidney%20cancer%2C%20including%20anatomical%20assessment%2C%20diagnostic%20classification%2C%20and%0Asurvival%20prediction%2C%20compared%20with%20other%20state-of-the-art%20general-purpose%20CT%0Afoundation%20models.%20Especially%2C%20for%20complicated%20task%20like%20recurrence-free%0Asurvival%20prediction%20in%20the%20TCIA%20cohort%2C%20RenalCLIP%20achieved%20a%20C-index%20of%200.726%2C%0Arepresenting%20a%20substantial%20improvement%20of%20approximately%2020%25%20over%20the%20leading%0Abaselines.%20Furthermore%2C%20RenalCLIP%27s%20pre-training%20imparted%20remarkable%20data%0Aefficiency%3B%20in%20the%20diagnostic%20classification%20task%2C%20it%20only%20needs%2020%25%20training%0Adata%20to%20achieve%20the%20peak%20performance%20of%20all%20baseline%20models%20even%20after%20they%0Awere%20fully%20fine-tuned%20on%20100%25%20of%20the%20data.%20Additionally%2C%20it%20achieved%20superior%0Aperformance%20in%20report%20generation%2C%20image-text%20retrieval%20and%20zero-shot%20diagnosis%0Atasks.%20Our%20findings%20establish%20that%20RenalCLIP%20provides%20a%20robust%20tool%20with%20the%0Apotential%20to%20enhance%20diagnostic%20accuracy%2C%20refine%20prognostic%20stratification%2C%20and%0Apersonalize%20the%20management%20of%20patients%20with%20kidney%20cancer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16569v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Disease-Centric%2520Vision-Language%2520Foundation%2520Model%2520for%2520Precision%250A%2520%2520Oncology%2520in%2520Kidney%2520Cancer%26entry.906535625%3DYuhui%2520Tao%2520and%2520Zhongwei%2520Zhao%2520and%2520Zilong%2520Wang%2520and%2520Xufang%2520Luo%2520and%2520Feng%2520Chen%2520and%2520Kang%2520Wang%2520and%2520Chuanfu%2520Wu%2520and%2520Xue%2520Zhang%2520and%2520Shaoting%2520Zhang%2520and%2520Jiaxi%2520Yao%2520and%2520Xingwei%2520Jin%2520and%2520Xinyang%2520Jiang%2520and%2520Yifan%2520Yang%2520and%2520Dongsheng%2520Li%2520and%2520Lili%2520Qiu%2520and%2520Zhiqiang%2520Shao%2520and%2520Jianming%2520Guo%2520and%2520Nengwang%2520Yu%2520and%2520Shuo%2520Wang%2520and%2520Ying%2520Xiong%26entry.1292438233%3D%2520%2520The%2520non-invasive%2520assessment%2520of%2520increasingly%2520incidentally%2520discovered%2520renal%250Amasses%2520is%2520a%2520critical%2520challenge%2520in%2520urologic%2520oncology%252C%2520where%2520diagnostic%250Auncertainty%2520frequently%2520leads%2520to%2520the%2520overtreatment%2520of%2520benign%2520or%2520indolent%2520tumors.%250AIn%2520this%2520study%252C%2520we%2520developed%2520and%2520validated%2520RenalCLIP%2520using%2520a%2520dataset%2520of%252027%252C866%250ACT%2520scans%2520from%25208%252C809%2520patients%2520across%2520nine%2520Chinese%2520medical%2520centers%2520and%2520the%2520public%250ATCIA%2520cohort%252C%2520a%2520visual-language%2520foundation%2520model%2520for%2520characterization%252C%2520diagnosis%250Aand%2520prognosis%2520of%2520renal%2520mass.%2520The%2520model%2520was%2520developed%2520via%2520a%2520two-stage%250Apre-training%2520strategy%2520that%2520first%2520enhances%2520the%2520image%2520and%2520text%2520encoders%2520with%250Adomain-specific%2520knowledge%2520before%2520aligning%2520them%2520through%2520a%2520contrastive%2520learning%250Aobjective%252C%2520to%2520create%2520robust%2520representations%2520for%2520superior%2520generalization%2520and%250Adiagnostic%2520precision.%2520RenalCLIP%2520achieved%2520better%2520performance%2520and%2520superior%250Ageneralizability%2520across%252010%2520core%2520tasks%2520spanning%2520the%2520full%2520clinical%2520workflow%2520of%250Akidney%2520cancer%252C%2520including%2520anatomical%2520assessment%252C%2520diagnostic%2520classification%252C%2520and%250Asurvival%2520prediction%252C%2520compared%2520with%2520other%2520state-of-the-art%2520general-purpose%2520CT%250Afoundation%2520models.%2520Especially%252C%2520for%2520complicated%2520task%2520like%2520recurrence-free%250Asurvival%2520prediction%2520in%2520the%2520TCIA%2520cohort%252C%2520RenalCLIP%2520achieved%2520a%2520C-index%2520of%25200.726%252C%250Arepresenting%2520a%2520substantial%2520improvement%2520of%2520approximately%252020%2525%2520over%2520the%2520leading%250Abaselines.%2520Furthermore%252C%2520RenalCLIP%2527s%2520pre-training%2520imparted%2520remarkable%2520data%250Aefficiency%253B%2520in%2520the%2520diagnostic%2520classification%2520task%252C%2520it%2520only%2520needs%252020%2525%2520training%250Adata%2520to%2520achieve%2520the%2520peak%2520performance%2520of%2520all%2520baseline%2520models%2520even%2520after%2520they%250Awere%2520fully%2520fine-tuned%2520on%2520100%2525%2520of%2520the%2520data.%2520Additionally%252C%2520it%2520achieved%2520superior%250Aperformance%2520in%2520report%2520generation%252C%2520image-text%2520retrieval%2520and%2520zero-shot%2520diagnosis%250Atasks.%2520Our%2520findings%2520establish%2520that%2520RenalCLIP%2520provides%2520a%2520robust%2520tool%2520with%2520the%250Apotential%2520to%2520enhance%2520diagnostic%2520accuracy%252C%2520refine%2520prognostic%2520stratification%252C%2520and%250Apersonalize%2520the%2520management%2520of%2520patients%2520with%2520kidney%2520cancer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16569v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Disease-Centric%20Vision-Language%20Foundation%20Model%20for%20Precision%0A%20%20Oncology%20in%20Kidney%20Cancer&entry.906535625=Yuhui%20Tao%20and%20Zhongwei%20Zhao%20and%20Zilong%20Wang%20and%20Xufang%20Luo%20and%20Feng%20Chen%20and%20Kang%20Wang%20and%20Chuanfu%20Wu%20and%20Xue%20Zhang%20and%20Shaoting%20Zhang%20and%20Jiaxi%20Yao%20and%20Xingwei%20Jin%20and%20Xinyang%20Jiang%20and%20Yifan%20Yang%20and%20Dongsheng%20Li%20and%20Lili%20Qiu%20and%20Zhiqiang%20Shao%20and%20Jianming%20Guo%20and%20Nengwang%20Yu%20and%20Shuo%20Wang%20and%20Ying%20Xiong&entry.1292438233=%20%20The%20non-invasive%20assessment%20of%20increasingly%20incidentally%20discovered%20renal%0Amasses%20is%20a%20critical%20challenge%20in%20urologic%20oncology%2C%20where%20diagnostic%0Auncertainty%20frequently%20leads%20to%20the%20overtreatment%20of%20benign%20or%20indolent%20tumors.%0AIn%20this%20study%2C%20we%20developed%20and%20validated%20RenalCLIP%20using%20a%20dataset%20of%2027%2C866%0ACT%20scans%20from%208%2C809%20patients%20across%20nine%20Chinese%20medical%20centers%20and%20the%20public%0ATCIA%20cohort%2C%20a%20visual-language%20foundation%20model%20for%20characterization%2C%20diagnosis%0Aand%20prognosis%20of%20renal%20mass.%20The%20model%20was%20developed%20via%20a%20two-stage%0Apre-training%20strategy%20that%20first%20enhances%20the%20image%20and%20text%20encoders%20with%0Adomain-specific%20knowledge%20before%20aligning%20them%20through%20a%20contrastive%20learning%0Aobjective%2C%20to%20create%20robust%20representations%20for%20superior%20generalization%20and%0Adiagnostic%20precision.%20RenalCLIP%20achieved%20better%20performance%20and%20superior%0Ageneralizability%20across%2010%20core%20tasks%20spanning%20the%20full%20clinical%20workflow%20of%0Akidney%20cancer%2C%20including%20anatomical%20assessment%2C%20diagnostic%20classification%2C%20and%0Asurvival%20prediction%2C%20compared%20with%20other%20state-of-the-art%20general-purpose%20CT%0Afoundation%20models.%20Especially%2C%20for%20complicated%20task%20like%20recurrence-free%0Asurvival%20prediction%20in%20the%20TCIA%20cohort%2C%20RenalCLIP%20achieved%20a%20C-index%20of%200.726%2C%0Arepresenting%20a%20substantial%20improvement%20of%20approximately%2020%25%20over%20the%20leading%0Abaselines.%20Furthermore%2C%20RenalCLIP%27s%20pre-training%20imparted%20remarkable%20data%0Aefficiency%3B%20in%20the%20diagnostic%20classification%20task%2C%20it%20only%20needs%2020%25%20training%0Adata%20to%20achieve%20the%20peak%20performance%20of%20all%20baseline%20models%20even%20after%20they%0Awere%20fully%20fine-tuned%20on%20100%25%20of%20the%20data.%20Additionally%2C%20it%20achieved%20superior%0Aperformance%20in%20report%20generation%2C%20image-text%20retrieval%20and%20zero-shot%20diagnosis%0Atasks.%20Our%20findings%20establish%20that%20RenalCLIP%20provides%20a%20robust%20tool%20with%20the%0Apotential%20to%20enhance%20diagnostic%20accuracy%2C%20refine%20prognostic%20stratification%2C%20and%0Apersonalize%20the%20management%20of%20patients%20with%20kidney%20cancer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16569v1&entry.124074799=Read"},
{"title": "GRAFT: Gradient-Aware Fast MaxVol Technique for Dynamic Data Sampling", "author": "Ashish Jha and Anh huy Phan and Razan Dibo and Valentin Leplat", "abstract": "  Training modern neural networks on large datasets is computationally and\nenvironmentally costly. We introduce GRAFT, a scalable in-training subset\nselection method that (i) extracts a low-rank feature representation for each\nbatch, (ii) applies a Fast MaxVol sampler to select a small, diverse subset\nthat spans the batch's dominant subspace, and (iii) dynamically adjusts the\nsubset size using a gradient-approximation criterion. By operating in low-rank\nsubspaces and training on carefully chosen examples instead of full batches,\nGRAFT preserves the training trajectory while reducing wall-clock time, energy\nconsumption, and $\\mathrm{CO}_2$ emissions. Across multiple benchmarks, GRAFT\nmatches or exceeds recent selection baselines in both accuracy and efficiency,\nproviding a favorable trade-off between accuracy, efficiency, and emissions.\n", "link": "http://arxiv.org/abs/2508.13653v2", "date": "2025-08-22", "relevancy": 2.6015, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5362}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5135}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5112}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GRAFT%3A%20Gradient-Aware%20Fast%20MaxVol%20Technique%20for%20Dynamic%20Data%20Sampling&body=Title%3A%20GRAFT%3A%20Gradient-Aware%20Fast%20MaxVol%20Technique%20for%20Dynamic%20Data%20Sampling%0AAuthor%3A%20Ashish%20Jha%20and%20Anh%20huy%20Phan%20and%20Razan%20Dibo%20and%20Valentin%20Leplat%0AAbstract%3A%20%20%20Training%20modern%20neural%20networks%20on%20large%20datasets%20is%20computationally%20and%0Aenvironmentally%20costly.%20We%20introduce%20GRAFT%2C%20a%20scalable%20in-training%20subset%0Aselection%20method%20that%20%28i%29%20extracts%20a%20low-rank%20feature%20representation%20for%20each%0Abatch%2C%20%28ii%29%20applies%20a%20Fast%20MaxVol%20sampler%20to%20select%20a%20small%2C%20diverse%20subset%0Athat%20spans%20the%20batch%27s%20dominant%20subspace%2C%20and%20%28iii%29%20dynamically%20adjusts%20the%0Asubset%20size%20using%20a%20gradient-approximation%20criterion.%20By%20operating%20in%20low-rank%0Asubspaces%20and%20training%20on%20carefully%20chosen%20examples%20instead%20of%20full%20batches%2C%0AGRAFT%20preserves%20the%20training%20trajectory%20while%20reducing%20wall-clock%20time%2C%20energy%0Aconsumption%2C%20and%20%24%5Cmathrm%7BCO%7D_2%24%20emissions.%20Across%20multiple%20benchmarks%2C%20GRAFT%0Amatches%20or%20exceeds%20recent%20selection%20baselines%20in%20both%20accuracy%20and%20efficiency%2C%0Aproviding%20a%20favorable%20trade-off%20between%20accuracy%2C%20efficiency%2C%20and%20emissions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13653v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGRAFT%253A%2520Gradient-Aware%2520Fast%2520MaxVol%2520Technique%2520for%2520Dynamic%2520Data%2520Sampling%26entry.906535625%3DAshish%2520Jha%2520and%2520Anh%2520huy%2520Phan%2520and%2520Razan%2520Dibo%2520and%2520Valentin%2520Leplat%26entry.1292438233%3D%2520%2520Training%2520modern%2520neural%2520networks%2520on%2520large%2520datasets%2520is%2520computationally%2520and%250Aenvironmentally%2520costly.%2520We%2520introduce%2520GRAFT%252C%2520a%2520scalable%2520in-training%2520subset%250Aselection%2520method%2520that%2520%2528i%2529%2520extracts%2520a%2520low-rank%2520feature%2520representation%2520for%2520each%250Abatch%252C%2520%2528ii%2529%2520applies%2520a%2520Fast%2520MaxVol%2520sampler%2520to%2520select%2520a%2520small%252C%2520diverse%2520subset%250Athat%2520spans%2520the%2520batch%2527s%2520dominant%2520subspace%252C%2520and%2520%2528iii%2529%2520dynamically%2520adjusts%2520the%250Asubset%2520size%2520using%2520a%2520gradient-approximation%2520criterion.%2520By%2520operating%2520in%2520low-rank%250Asubspaces%2520and%2520training%2520on%2520carefully%2520chosen%2520examples%2520instead%2520of%2520full%2520batches%252C%250AGRAFT%2520preserves%2520the%2520training%2520trajectory%2520while%2520reducing%2520wall-clock%2520time%252C%2520energy%250Aconsumption%252C%2520and%2520%2524%255Cmathrm%257BCO%257D_2%2524%2520emissions.%2520Across%2520multiple%2520benchmarks%252C%2520GRAFT%250Amatches%2520or%2520exceeds%2520recent%2520selection%2520baselines%2520in%2520both%2520accuracy%2520and%2520efficiency%252C%250Aproviding%2520a%2520favorable%2520trade-off%2520between%2520accuracy%252C%2520efficiency%252C%2520and%2520emissions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13653v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GRAFT%3A%20Gradient-Aware%20Fast%20MaxVol%20Technique%20for%20Dynamic%20Data%20Sampling&entry.906535625=Ashish%20Jha%20and%20Anh%20huy%20Phan%20and%20Razan%20Dibo%20and%20Valentin%20Leplat&entry.1292438233=%20%20Training%20modern%20neural%20networks%20on%20large%20datasets%20is%20computationally%20and%0Aenvironmentally%20costly.%20We%20introduce%20GRAFT%2C%20a%20scalable%20in-training%20subset%0Aselection%20method%20that%20%28i%29%20extracts%20a%20low-rank%20feature%20representation%20for%20each%0Abatch%2C%20%28ii%29%20applies%20a%20Fast%20MaxVol%20sampler%20to%20select%20a%20small%2C%20diverse%20subset%0Athat%20spans%20the%20batch%27s%20dominant%20subspace%2C%20and%20%28iii%29%20dynamically%20adjusts%20the%0Asubset%20size%20using%20a%20gradient-approximation%20criterion.%20By%20operating%20in%20low-rank%0Asubspaces%20and%20training%20on%20carefully%20chosen%20examples%20instead%20of%20full%20batches%2C%0AGRAFT%20preserves%20the%20training%20trajectory%20while%20reducing%20wall-clock%20time%2C%20energy%0Aconsumption%2C%20and%20%24%5Cmathrm%7BCO%7D_2%24%20emissions.%20Across%20multiple%20benchmarks%2C%20GRAFT%0Amatches%20or%20exceeds%20recent%20selection%20baselines%20in%20both%20accuracy%20and%20efficiency%2C%0Aproviding%20a%20favorable%20trade-off%20between%20accuracy%2C%20efficiency%2C%20and%20emissions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13653v2&entry.124074799=Read"},
{"title": "Vevo2: Bridging Controllable Speech and Singing Voice Generation via\n  Unified Prosody Learning", "author": "Xueyao Zhang and Junan Zhang and Yuancheng Wang and Chaoren Wang and Yuanzhe Chen and Dongya Jia and Zhuo Chen and Zhizheng Wu", "abstract": "  Controllable human voice generation, particularly for expressive domains like\nsinging, remains a significant challenge. This paper introduces Vevo2, a\nunified framework for controllable speech and singing voice generation. To\ntackle issues like the scarcity of annotated singing data and to enable\nflexible controllability, Vevo2 introduces two audio tokenizers: (1) a\nmusic-notation-free prosody tokenizer that captures prosody and melody from\nspeech, singing, and even instrumental sounds, and (2) a low-frame-rate (12.5\nHz) content-style tokenizer that encodes linguistic content, prosody, and style\nfor both speech and singing, while enabling timbre disentanglement. Vevo2\nconsists of an auto-regressive (AR) content-style modeling stage, which aims to\nenable controllability over text, prosody, and style, as well as a\nflow-matching acoustic modeling stage that allows for timbre control.\nParticularly, during pre-training of the AR model, we propose both explicit and\nimplicit prosody learning strategies to bridge speech and singing voice.\nMoreover, to further enhance the AR model's ability to follow text and prosody,\nwe design a multi-objective post-training task that integrates both\nintelligibility and prosody similarity alignment. Experimental results show\nthat the unified modeling in Vevo2 brings mutual benefits to both speech and\nsinging voice generation. Additionally, Vevo2's effectiveness across a wide\nrange of synthesis, conversion, and editing tasks for both speech and singing\nfurther demonstrates its strong generalization ability and versatility. Audio\nsamples are are available at https://versasinger.github.io/.\n", "link": "http://arxiv.org/abs/2508.16332v1", "date": "2025-08-22", "relevancy": 2.5937, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5193}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5193}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5176}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vevo2%3A%20Bridging%20Controllable%20Speech%20and%20Singing%20Voice%20Generation%20via%0A%20%20Unified%20Prosody%20Learning&body=Title%3A%20Vevo2%3A%20Bridging%20Controllable%20Speech%20and%20Singing%20Voice%20Generation%20via%0A%20%20Unified%20Prosody%20Learning%0AAuthor%3A%20Xueyao%20Zhang%20and%20Junan%20Zhang%20and%20Yuancheng%20Wang%20and%20Chaoren%20Wang%20and%20Yuanzhe%20Chen%20and%20Dongya%20Jia%20and%20Zhuo%20Chen%20and%20Zhizheng%20Wu%0AAbstract%3A%20%20%20Controllable%20human%20voice%20generation%2C%20particularly%20for%20expressive%20domains%20like%0Asinging%2C%20remains%20a%20significant%20challenge.%20This%20paper%20introduces%20Vevo2%2C%20a%0Aunified%20framework%20for%20controllable%20speech%20and%20singing%20voice%20generation.%20To%0Atackle%20issues%20like%20the%20scarcity%20of%20annotated%20singing%20data%20and%20to%20enable%0Aflexible%20controllability%2C%20Vevo2%20introduces%20two%20audio%20tokenizers%3A%20%281%29%20a%0Amusic-notation-free%20prosody%20tokenizer%20that%20captures%20prosody%20and%20melody%20from%0Aspeech%2C%20singing%2C%20and%20even%20instrumental%20sounds%2C%20and%20%282%29%20a%20low-frame-rate%20%2812.5%0AHz%29%20content-style%20tokenizer%20that%20encodes%20linguistic%20content%2C%20prosody%2C%20and%20style%0Afor%20both%20speech%20and%20singing%2C%20while%20enabling%20timbre%20disentanglement.%20Vevo2%0Aconsists%20of%20an%20auto-regressive%20%28AR%29%20content-style%20modeling%20stage%2C%20which%20aims%20to%0Aenable%20controllability%20over%20text%2C%20prosody%2C%20and%20style%2C%20as%20well%20as%20a%0Aflow-matching%20acoustic%20modeling%20stage%20that%20allows%20for%20timbre%20control.%0AParticularly%2C%20during%20pre-training%20of%20the%20AR%20model%2C%20we%20propose%20both%20explicit%20and%0Aimplicit%20prosody%20learning%20strategies%20to%20bridge%20speech%20and%20singing%20voice.%0AMoreover%2C%20to%20further%20enhance%20the%20AR%20model%27s%20ability%20to%20follow%20text%20and%20prosody%2C%0Awe%20design%20a%20multi-objective%20post-training%20task%20that%20integrates%20both%0Aintelligibility%20and%20prosody%20similarity%20alignment.%20Experimental%20results%20show%0Athat%20the%20unified%20modeling%20in%20Vevo2%20brings%20mutual%20benefits%20to%20both%20speech%20and%0Asinging%20voice%20generation.%20Additionally%2C%20Vevo2%27s%20effectiveness%20across%20a%20wide%0Arange%20of%20synthesis%2C%20conversion%2C%20and%20editing%20tasks%20for%20both%20speech%20and%20singing%0Afurther%20demonstrates%20its%20strong%20generalization%20ability%20and%20versatility.%20Audio%0Asamples%20are%20are%20available%20at%20https%3A//versasinger.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16332v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVevo2%253A%2520Bridging%2520Controllable%2520Speech%2520and%2520Singing%2520Voice%2520Generation%2520via%250A%2520%2520Unified%2520Prosody%2520Learning%26entry.906535625%3DXueyao%2520Zhang%2520and%2520Junan%2520Zhang%2520and%2520Yuancheng%2520Wang%2520and%2520Chaoren%2520Wang%2520and%2520Yuanzhe%2520Chen%2520and%2520Dongya%2520Jia%2520and%2520Zhuo%2520Chen%2520and%2520Zhizheng%2520Wu%26entry.1292438233%3D%2520%2520Controllable%2520human%2520voice%2520generation%252C%2520particularly%2520for%2520expressive%2520domains%2520like%250Asinging%252C%2520remains%2520a%2520significant%2520challenge.%2520This%2520paper%2520introduces%2520Vevo2%252C%2520a%250Aunified%2520framework%2520for%2520controllable%2520speech%2520and%2520singing%2520voice%2520generation.%2520To%250Atackle%2520issues%2520like%2520the%2520scarcity%2520of%2520annotated%2520singing%2520data%2520and%2520to%2520enable%250Aflexible%2520controllability%252C%2520Vevo2%2520introduces%2520two%2520audio%2520tokenizers%253A%2520%25281%2529%2520a%250Amusic-notation-free%2520prosody%2520tokenizer%2520that%2520captures%2520prosody%2520and%2520melody%2520from%250Aspeech%252C%2520singing%252C%2520and%2520even%2520instrumental%2520sounds%252C%2520and%2520%25282%2529%2520a%2520low-frame-rate%2520%252812.5%250AHz%2529%2520content-style%2520tokenizer%2520that%2520encodes%2520linguistic%2520content%252C%2520prosody%252C%2520and%2520style%250Afor%2520both%2520speech%2520and%2520singing%252C%2520while%2520enabling%2520timbre%2520disentanglement.%2520Vevo2%250Aconsists%2520of%2520an%2520auto-regressive%2520%2528AR%2529%2520content-style%2520modeling%2520stage%252C%2520which%2520aims%2520to%250Aenable%2520controllability%2520over%2520text%252C%2520prosody%252C%2520and%2520style%252C%2520as%2520well%2520as%2520a%250Aflow-matching%2520acoustic%2520modeling%2520stage%2520that%2520allows%2520for%2520timbre%2520control.%250AParticularly%252C%2520during%2520pre-training%2520of%2520the%2520AR%2520model%252C%2520we%2520propose%2520both%2520explicit%2520and%250Aimplicit%2520prosody%2520learning%2520strategies%2520to%2520bridge%2520speech%2520and%2520singing%2520voice.%250AMoreover%252C%2520to%2520further%2520enhance%2520the%2520AR%2520model%2527s%2520ability%2520to%2520follow%2520text%2520and%2520prosody%252C%250Awe%2520design%2520a%2520multi-objective%2520post-training%2520task%2520that%2520integrates%2520both%250Aintelligibility%2520and%2520prosody%2520similarity%2520alignment.%2520Experimental%2520results%2520show%250Athat%2520the%2520unified%2520modeling%2520in%2520Vevo2%2520brings%2520mutual%2520benefits%2520to%2520both%2520speech%2520and%250Asinging%2520voice%2520generation.%2520Additionally%252C%2520Vevo2%2527s%2520effectiveness%2520across%2520a%2520wide%250Arange%2520of%2520synthesis%252C%2520conversion%252C%2520and%2520editing%2520tasks%2520for%2520both%2520speech%2520and%2520singing%250Afurther%2520demonstrates%2520its%2520strong%2520generalization%2520ability%2520and%2520versatility.%2520Audio%250Asamples%2520are%2520are%2520available%2520at%2520https%253A//versasinger.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16332v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vevo2%3A%20Bridging%20Controllable%20Speech%20and%20Singing%20Voice%20Generation%20via%0A%20%20Unified%20Prosody%20Learning&entry.906535625=Xueyao%20Zhang%20and%20Junan%20Zhang%20and%20Yuancheng%20Wang%20and%20Chaoren%20Wang%20and%20Yuanzhe%20Chen%20and%20Dongya%20Jia%20and%20Zhuo%20Chen%20and%20Zhizheng%20Wu&entry.1292438233=%20%20Controllable%20human%20voice%20generation%2C%20particularly%20for%20expressive%20domains%20like%0Asinging%2C%20remains%20a%20significant%20challenge.%20This%20paper%20introduces%20Vevo2%2C%20a%0Aunified%20framework%20for%20controllable%20speech%20and%20singing%20voice%20generation.%20To%0Atackle%20issues%20like%20the%20scarcity%20of%20annotated%20singing%20data%20and%20to%20enable%0Aflexible%20controllability%2C%20Vevo2%20introduces%20two%20audio%20tokenizers%3A%20%281%29%20a%0Amusic-notation-free%20prosody%20tokenizer%20that%20captures%20prosody%20and%20melody%20from%0Aspeech%2C%20singing%2C%20and%20even%20instrumental%20sounds%2C%20and%20%282%29%20a%20low-frame-rate%20%2812.5%0AHz%29%20content-style%20tokenizer%20that%20encodes%20linguistic%20content%2C%20prosody%2C%20and%20style%0Afor%20both%20speech%20and%20singing%2C%20while%20enabling%20timbre%20disentanglement.%20Vevo2%0Aconsists%20of%20an%20auto-regressive%20%28AR%29%20content-style%20modeling%20stage%2C%20which%20aims%20to%0Aenable%20controllability%20over%20text%2C%20prosody%2C%20and%20style%2C%20as%20well%20as%20a%0Aflow-matching%20acoustic%20modeling%20stage%20that%20allows%20for%20timbre%20control.%0AParticularly%2C%20during%20pre-training%20of%20the%20AR%20model%2C%20we%20propose%20both%20explicit%20and%0Aimplicit%20prosody%20learning%20strategies%20to%20bridge%20speech%20and%20singing%20voice.%0AMoreover%2C%20to%20further%20enhance%20the%20AR%20model%27s%20ability%20to%20follow%20text%20and%20prosody%2C%0Awe%20design%20a%20multi-objective%20post-training%20task%20that%20integrates%20both%0Aintelligibility%20and%20prosody%20similarity%20alignment.%20Experimental%20results%20show%0Athat%20the%20unified%20modeling%20in%20Vevo2%20brings%20mutual%20benefits%20to%20both%20speech%20and%0Asinging%20voice%20generation.%20Additionally%2C%20Vevo2%27s%20effectiveness%20across%20a%20wide%0Arange%20of%20synthesis%2C%20conversion%2C%20and%20editing%20tasks%20for%20both%20speech%20and%20singing%0Afurther%20demonstrates%20its%20strong%20generalization%20ability%20and%20versatility.%20Audio%0Asamples%20are%20are%20available%20at%20https%3A//versasinger.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16332v1&entry.124074799=Read"},
{"title": "Towards Bridging the Reward-Generation Gap in Direct Alignment\n  Algorithms", "author": "Zeguan Xiao and Yun Chen and Guanhua Chen and Ke Tang", "abstract": "  Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization\n(DPO) and Simple Preference Optimization (SimPO), have emerged as efficient\nalternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms\nfor aligning large language models (LLMs) with human preferences. However, DAAs\nsuffer from a fundamental limitation we identify as the \"reward-generation gap\"\n-- a misalignment between optimization objectives during training and actual\ngeneration performance during inference. In this paper, we find a contributor\nto the reward-generation gap is the mismatch between the inherent importance of\nprefix tokens during the LLM generation process and how this importance is\nreflected in the implicit reward functions of DAAs. To bridge the gap, we adopt\na token-level MDP perspective of DAAs to analyze its limitations and introduce\na simple yet effective approach called Prefix-Oriented Equal-length Training\n(POET), which truncates both preferred and dispreferred responses to match the\nshorter one's length. Training with \\mname, where both responses in each sample\nare truncated to equal length, resulting in diverse truncated lengths across\nsamples, the optimization of DAAs objective is implicitly constrained to\nconverge across all timesteps of token-level MDP, thus paying more attention to\nprefix tokens than the standard DAAs. We conduct experiments with DPO and\nSimPO, two representative DAAs, demonstrating that POET improves over their\nstandard implementations, achieving up to 15.6 points in AlpacaEval 2 and\noverall improvements across downstream tasks. Our results highlight the\nimportance of addressing the misalignment between reward optimization and\ngeneration performance in DAAs.\n", "link": "http://arxiv.org/abs/2506.09457v2", "date": "2025-08-22", "relevancy": 2.4788, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.504}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4917}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4917}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Bridging%20the%20Reward-Generation%20Gap%20in%20Direct%20Alignment%0A%20%20Algorithms&body=Title%3A%20Towards%20Bridging%20the%20Reward-Generation%20Gap%20in%20Direct%20Alignment%0A%20%20Algorithms%0AAuthor%3A%20Zeguan%20Xiao%20and%20Yun%20Chen%20and%20Guanhua%20Chen%20and%20Ke%20Tang%0AAbstract%3A%20%20%20Direct%20Alignment%20Algorithms%20%28DAAs%29%2C%20such%20as%20Direct%20Preference%20Optimization%0A%28DPO%29%20and%20Simple%20Preference%20Optimization%20%28SimPO%29%2C%20have%20emerged%20as%20efficient%0Aalternatives%20to%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20algorithms%0Afor%20aligning%20large%20language%20models%20%28LLMs%29%20with%20human%20preferences.%20However%2C%20DAAs%0Asuffer%20from%20a%20fundamental%20limitation%20we%20identify%20as%20the%20%22reward-generation%20gap%22%0A--%20a%20misalignment%20between%20optimization%20objectives%20during%20training%20and%20actual%0Ageneration%20performance%20during%20inference.%20In%20this%20paper%2C%20we%20find%20a%20contributor%0Ato%20the%20reward-generation%20gap%20is%20the%20mismatch%20between%20the%20inherent%20importance%20of%0Aprefix%20tokens%20during%20the%20LLM%20generation%20process%20and%20how%20this%20importance%20is%0Areflected%20in%20the%20implicit%20reward%20functions%20of%20DAAs.%20To%20bridge%20the%20gap%2C%20we%20adopt%0Aa%20token-level%20MDP%20perspective%20of%20DAAs%20to%20analyze%20its%20limitations%20and%20introduce%0Aa%20simple%20yet%20effective%20approach%20called%20Prefix-Oriented%20Equal-length%20Training%0A%28POET%29%2C%20which%20truncates%20both%20preferred%20and%20dispreferred%20responses%20to%20match%20the%0Ashorter%20one%27s%20length.%20Training%20with%20%5Cmname%2C%20where%20both%20responses%20in%20each%20sample%0Aare%20truncated%20to%20equal%20length%2C%20resulting%20in%20diverse%20truncated%20lengths%20across%0Asamples%2C%20the%20optimization%20of%20DAAs%20objective%20is%20implicitly%20constrained%20to%0Aconverge%20across%20all%20timesteps%20of%20token-level%20MDP%2C%20thus%20paying%20more%20attention%20to%0Aprefix%20tokens%20than%20the%20standard%20DAAs.%20We%20conduct%20experiments%20with%20DPO%20and%0ASimPO%2C%20two%20representative%20DAAs%2C%20demonstrating%20that%20POET%20improves%20over%20their%0Astandard%20implementations%2C%20achieving%20up%20to%2015.6%20points%20in%20AlpacaEval%202%20and%0Aoverall%20improvements%20across%20downstream%20tasks.%20Our%20results%20highlight%20the%0Aimportance%20of%20addressing%20the%20misalignment%20between%20reward%20optimization%20and%0Ageneration%20performance%20in%20DAAs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09457v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Bridging%2520the%2520Reward-Generation%2520Gap%2520in%2520Direct%2520Alignment%250A%2520%2520Algorithms%26entry.906535625%3DZeguan%2520Xiao%2520and%2520Yun%2520Chen%2520and%2520Guanhua%2520Chen%2520and%2520Ke%2520Tang%26entry.1292438233%3D%2520%2520Direct%2520Alignment%2520Algorithms%2520%2528DAAs%2529%252C%2520such%2520as%2520Direct%2520Preference%2520Optimization%250A%2528DPO%2529%2520and%2520Simple%2520Preference%2520Optimization%2520%2528SimPO%2529%252C%2520have%2520emerged%2520as%2520efficient%250Aalternatives%2520to%2520Reinforcement%2520Learning%2520from%2520Human%2520Feedback%2520%2528RLHF%2529%2520algorithms%250Afor%2520aligning%2520large%2520language%2520models%2520%2528LLMs%2529%2520with%2520human%2520preferences.%2520However%252C%2520DAAs%250Asuffer%2520from%2520a%2520fundamental%2520limitation%2520we%2520identify%2520as%2520the%2520%2522reward-generation%2520gap%2522%250A--%2520a%2520misalignment%2520between%2520optimization%2520objectives%2520during%2520training%2520and%2520actual%250Ageneration%2520performance%2520during%2520inference.%2520In%2520this%2520paper%252C%2520we%2520find%2520a%2520contributor%250Ato%2520the%2520reward-generation%2520gap%2520is%2520the%2520mismatch%2520between%2520the%2520inherent%2520importance%2520of%250Aprefix%2520tokens%2520during%2520the%2520LLM%2520generation%2520process%2520and%2520how%2520this%2520importance%2520is%250Areflected%2520in%2520the%2520implicit%2520reward%2520functions%2520of%2520DAAs.%2520To%2520bridge%2520the%2520gap%252C%2520we%2520adopt%250Aa%2520token-level%2520MDP%2520perspective%2520of%2520DAAs%2520to%2520analyze%2520its%2520limitations%2520and%2520introduce%250Aa%2520simple%2520yet%2520effective%2520approach%2520called%2520Prefix-Oriented%2520Equal-length%2520Training%250A%2528POET%2529%252C%2520which%2520truncates%2520both%2520preferred%2520and%2520dispreferred%2520responses%2520to%2520match%2520the%250Ashorter%2520one%2527s%2520length.%2520Training%2520with%2520%255Cmname%252C%2520where%2520both%2520responses%2520in%2520each%2520sample%250Aare%2520truncated%2520to%2520equal%2520length%252C%2520resulting%2520in%2520diverse%2520truncated%2520lengths%2520across%250Asamples%252C%2520the%2520optimization%2520of%2520DAAs%2520objective%2520is%2520implicitly%2520constrained%2520to%250Aconverge%2520across%2520all%2520timesteps%2520of%2520token-level%2520MDP%252C%2520thus%2520paying%2520more%2520attention%2520to%250Aprefix%2520tokens%2520than%2520the%2520standard%2520DAAs.%2520We%2520conduct%2520experiments%2520with%2520DPO%2520and%250ASimPO%252C%2520two%2520representative%2520DAAs%252C%2520demonstrating%2520that%2520POET%2520improves%2520over%2520their%250Astandard%2520implementations%252C%2520achieving%2520up%2520to%252015.6%2520points%2520in%2520AlpacaEval%25202%2520and%250Aoverall%2520improvements%2520across%2520downstream%2520tasks.%2520Our%2520results%2520highlight%2520the%250Aimportance%2520of%2520addressing%2520the%2520misalignment%2520between%2520reward%2520optimization%2520and%250Ageneration%2520performance%2520in%2520DAAs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09457v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Bridging%20the%20Reward-Generation%20Gap%20in%20Direct%20Alignment%0A%20%20Algorithms&entry.906535625=Zeguan%20Xiao%20and%20Yun%20Chen%20and%20Guanhua%20Chen%20and%20Ke%20Tang&entry.1292438233=%20%20Direct%20Alignment%20Algorithms%20%28DAAs%29%2C%20such%20as%20Direct%20Preference%20Optimization%0A%28DPO%29%20and%20Simple%20Preference%20Optimization%20%28SimPO%29%2C%20have%20emerged%20as%20efficient%0Aalternatives%20to%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20algorithms%0Afor%20aligning%20large%20language%20models%20%28LLMs%29%20with%20human%20preferences.%20However%2C%20DAAs%0Asuffer%20from%20a%20fundamental%20limitation%20we%20identify%20as%20the%20%22reward-generation%20gap%22%0A--%20a%20misalignment%20between%20optimization%20objectives%20during%20training%20and%20actual%0Ageneration%20performance%20during%20inference.%20In%20this%20paper%2C%20we%20find%20a%20contributor%0Ato%20the%20reward-generation%20gap%20is%20the%20mismatch%20between%20the%20inherent%20importance%20of%0Aprefix%20tokens%20during%20the%20LLM%20generation%20process%20and%20how%20this%20importance%20is%0Areflected%20in%20the%20implicit%20reward%20functions%20of%20DAAs.%20To%20bridge%20the%20gap%2C%20we%20adopt%0Aa%20token-level%20MDP%20perspective%20of%20DAAs%20to%20analyze%20its%20limitations%20and%20introduce%0Aa%20simple%20yet%20effective%20approach%20called%20Prefix-Oriented%20Equal-length%20Training%0A%28POET%29%2C%20which%20truncates%20both%20preferred%20and%20dispreferred%20responses%20to%20match%20the%0Ashorter%20one%27s%20length.%20Training%20with%20%5Cmname%2C%20where%20both%20responses%20in%20each%20sample%0Aare%20truncated%20to%20equal%20length%2C%20resulting%20in%20diverse%20truncated%20lengths%20across%0Asamples%2C%20the%20optimization%20of%20DAAs%20objective%20is%20implicitly%20constrained%20to%0Aconverge%20across%20all%20timesteps%20of%20token-level%20MDP%2C%20thus%20paying%20more%20attention%20to%0Aprefix%20tokens%20than%20the%20standard%20DAAs.%20We%20conduct%20experiments%20with%20DPO%20and%0ASimPO%2C%20two%20representative%20DAAs%2C%20demonstrating%20that%20POET%20improves%20over%20their%0Astandard%20implementations%2C%20achieving%20up%20to%2015.6%20points%20in%20AlpacaEval%202%20and%0Aoverall%20improvements%20across%20downstream%20tasks.%20Our%20results%20highlight%20the%0Aimportance%20of%20addressing%20the%20misalignment%20between%20reward%20optimization%20and%0Ageneration%20performance%20in%20DAAs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09457v2&entry.124074799=Read"},
{"title": "Contextualize-then-Aggregate: Circuits for In-Context Learning in\n  Gemma-2 2B", "author": "Aleksandra Bakalova and Yana Veitsman and Xinting Huang and Michael Hahn", "abstract": "  In-Context Learning (ICL) is an intriguing ability of large language models\n(LLMs). Despite a substantial amount of work on its behavioral aspects and how\nit emerges in miniature setups, it remains unclear which mechanism assembles\ntask information from the individual examples in a fewshot prompt. We use\ncausal interventions to identify information flow in Gemma-2 2B for five\nnaturalistic ICL tasks. We find that the model infers task information using a\ntwo-step strategy we call contextualize-then-aggregate: In the lower layers,\nthe model builds up representations of individual fewshot examples, which are\ncontextualized by preceding examples through connections between fewshot input\nand output tokens across the sequence. In the higher layers, these\nrepresentations are aggregated to identify the task and prepare prediction of\nthe next output. The importance of the contextualization step differs between\ntasks, and it may become more important in the presence of ambiguous examples.\nOverall, by providing rigorous causal analysis, our results shed light on the\nmechanisms through which ICL happens in language models.\n", "link": "http://arxiv.org/abs/2504.00132v2", "date": "2025-08-22", "relevancy": 2.4775, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5023}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5023}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4819}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contextualize-then-Aggregate%3A%20Circuits%20for%20In-Context%20Learning%20in%0A%20%20Gemma-2%202B&body=Title%3A%20Contextualize-then-Aggregate%3A%20Circuits%20for%20In-Context%20Learning%20in%0A%20%20Gemma-2%202B%0AAuthor%3A%20Aleksandra%20Bakalova%20and%20Yana%20Veitsman%20and%20Xinting%20Huang%20and%20Michael%20Hahn%0AAbstract%3A%20%20%20In-Context%20Learning%20%28ICL%29%20is%20an%20intriguing%20ability%20of%20large%20language%20models%0A%28LLMs%29.%20Despite%20a%20substantial%20amount%20of%20work%20on%20its%20behavioral%20aspects%20and%20how%0Ait%20emerges%20in%20miniature%20setups%2C%20it%20remains%20unclear%20which%20mechanism%20assembles%0Atask%20information%20from%20the%20individual%20examples%20in%20a%20fewshot%20prompt.%20We%20use%0Acausal%20interventions%20to%20identify%20information%20flow%20in%20Gemma-2%202B%20for%20five%0Anaturalistic%20ICL%20tasks.%20We%20find%20that%20the%20model%20infers%20task%20information%20using%20a%0Atwo-step%20strategy%20we%20call%20contextualize-then-aggregate%3A%20In%20the%20lower%20layers%2C%0Athe%20model%20builds%20up%20representations%20of%20individual%20fewshot%20examples%2C%20which%20are%0Acontextualized%20by%20preceding%20examples%20through%20connections%20between%20fewshot%20input%0Aand%20output%20tokens%20across%20the%20sequence.%20In%20the%20higher%20layers%2C%20these%0Arepresentations%20are%20aggregated%20to%20identify%20the%20task%20and%20prepare%20prediction%20of%0Athe%20next%20output.%20The%20importance%20of%20the%20contextualization%20step%20differs%20between%0Atasks%2C%20and%20it%20may%20become%20more%20important%20in%20the%20presence%20of%20ambiguous%20examples.%0AOverall%2C%20by%20providing%20rigorous%20causal%20analysis%2C%20our%20results%20shed%20light%20on%20the%0Amechanisms%20through%20which%20ICL%20happens%20in%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.00132v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContextualize-then-Aggregate%253A%2520Circuits%2520for%2520In-Context%2520Learning%2520in%250A%2520%2520Gemma-2%25202B%26entry.906535625%3DAleksandra%2520Bakalova%2520and%2520Yana%2520Veitsman%2520and%2520Xinting%2520Huang%2520and%2520Michael%2520Hahn%26entry.1292438233%3D%2520%2520In-Context%2520Learning%2520%2528ICL%2529%2520is%2520an%2520intriguing%2520ability%2520of%2520large%2520language%2520models%250A%2528LLMs%2529.%2520Despite%2520a%2520substantial%2520amount%2520of%2520work%2520on%2520its%2520behavioral%2520aspects%2520and%2520how%250Ait%2520emerges%2520in%2520miniature%2520setups%252C%2520it%2520remains%2520unclear%2520which%2520mechanism%2520assembles%250Atask%2520information%2520from%2520the%2520individual%2520examples%2520in%2520a%2520fewshot%2520prompt.%2520We%2520use%250Acausal%2520interventions%2520to%2520identify%2520information%2520flow%2520in%2520Gemma-2%25202B%2520for%2520five%250Anaturalistic%2520ICL%2520tasks.%2520We%2520find%2520that%2520the%2520model%2520infers%2520task%2520information%2520using%2520a%250Atwo-step%2520strategy%2520we%2520call%2520contextualize-then-aggregate%253A%2520In%2520the%2520lower%2520layers%252C%250Athe%2520model%2520builds%2520up%2520representations%2520of%2520individual%2520fewshot%2520examples%252C%2520which%2520are%250Acontextualized%2520by%2520preceding%2520examples%2520through%2520connections%2520between%2520fewshot%2520input%250Aand%2520output%2520tokens%2520across%2520the%2520sequence.%2520In%2520the%2520higher%2520layers%252C%2520these%250Arepresentations%2520are%2520aggregated%2520to%2520identify%2520the%2520task%2520and%2520prepare%2520prediction%2520of%250Athe%2520next%2520output.%2520The%2520importance%2520of%2520the%2520contextualization%2520step%2520differs%2520between%250Atasks%252C%2520and%2520it%2520may%2520become%2520more%2520important%2520in%2520the%2520presence%2520of%2520ambiguous%2520examples.%250AOverall%252C%2520by%2520providing%2520rigorous%2520causal%2520analysis%252C%2520our%2520results%2520shed%2520light%2520on%2520the%250Amechanisms%2520through%2520which%2520ICL%2520happens%2520in%2520language%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.00132v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contextualize-then-Aggregate%3A%20Circuits%20for%20In-Context%20Learning%20in%0A%20%20Gemma-2%202B&entry.906535625=Aleksandra%20Bakalova%20and%20Yana%20Veitsman%20and%20Xinting%20Huang%20and%20Michael%20Hahn&entry.1292438233=%20%20In-Context%20Learning%20%28ICL%29%20is%20an%20intriguing%20ability%20of%20large%20language%20models%0A%28LLMs%29.%20Despite%20a%20substantial%20amount%20of%20work%20on%20its%20behavioral%20aspects%20and%20how%0Ait%20emerges%20in%20miniature%20setups%2C%20it%20remains%20unclear%20which%20mechanism%20assembles%0Atask%20information%20from%20the%20individual%20examples%20in%20a%20fewshot%20prompt.%20We%20use%0Acausal%20interventions%20to%20identify%20information%20flow%20in%20Gemma-2%202B%20for%20five%0Anaturalistic%20ICL%20tasks.%20We%20find%20that%20the%20model%20infers%20task%20information%20using%20a%0Atwo-step%20strategy%20we%20call%20contextualize-then-aggregate%3A%20In%20the%20lower%20layers%2C%0Athe%20model%20builds%20up%20representations%20of%20individual%20fewshot%20examples%2C%20which%20are%0Acontextualized%20by%20preceding%20examples%20through%20connections%20between%20fewshot%20input%0Aand%20output%20tokens%20across%20the%20sequence.%20In%20the%20higher%20layers%2C%20these%0Arepresentations%20are%20aggregated%20to%20identify%20the%20task%20and%20prepare%20prediction%20of%0Athe%20next%20output.%20The%20importance%20of%20the%20contextualization%20step%20differs%20between%0Atasks%2C%20and%20it%20may%20become%20more%20important%20in%20the%20presence%20of%20ambiguous%20examples.%0AOverall%2C%20by%20providing%20rigorous%20causal%20analysis%2C%20our%20results%20shed%20light%20on%20the%0Amechanisms%20through%20which%20ICL%20happens%20in%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.00132v2&entry.124074799=Read"},
{"title": "Representing spherical tensors with scalar-based machine-learning models", "author": "Michelangelo Domina and Filippo Bigi and Paolo Pegolo and Michele Ceriotti", "abstract": "  Rotational symmetry plays a central role in physics, providing an elegant\nframework to describe how the properties of 3D objects -- from atoms to the\nmacroscopic scale -- transform under the action of rigid rotations. Equivariant\nmodels of 3D point clouds are able to approximate structure-property relations\nin a way that is fully consistent with the structure of the rotation group, by\ncombining intermediate representations that are themselves spherical tensors.\nThe symmetry constraints however make this approach computationally demanding\nand cumbersome to implement, which motivates increasingly popular unconstrained\narchitectures that learn approximate symmetries as part of the training\nprocess. In this work, we explore a third route to tackle this learning\nproblem, where equivariant functions are expressed as the product of a scalar\nfunction of the point cloud coordinates and a small basis of tensors with the\nappropriate symmetry. We also propose approximations of the general expressions\nthat, while lacking universal approximation properties, are fast, simple to\nimplement, and accurate in practical settings.\n", "link": "http://arxiv.org/abs/2505.05404v2", "date": "2025-08-22", "relevancy": 2.4567, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5139}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4809}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4792}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Representing%20spherical%20tensors%20with%20scalar-based%20machine-learning%20models&body=Title%3A%20Representing%20spherical%20tensors%20with%20scalar-based%20machine-learning%20models%0AAuthor%3A%20Michelangelo%20Domina%20and%20Filippo%20Bigi%20and%20Paolo%20Pegolo%20and%20Michele%20Ceriotti%0AAbstract%3A%20%20%20Rotational%20symmetry%20plays%20a%20central%20role%20in%20physics%2C%20providing%20an%20elegant%0Aframework%20to%20describe%20how%20the%20properties%20of%203D%20objects%20--%20from%20atoms%20to%20the%0Amacroscopic%20scale%20--%20transform%20under%20the%20action%20of%20rigid%20rotations.%20Equivariant%0Amodels%20of%203D%20point%20clouds%20are%20able%20to%20approximate%20structure-property%20relations%0Ain%20a%20way%20that%20is%20fully%20consistent%20with%20the%20structure%20of%20the%20rotation%20group%2C%20by%0Acombining%20intermediate%20representations%20that%20are%20themselves%20spherical%20tensors.%0AThe%20symmetry%20constraints%20however%20make%20this%20approach%20computationally%20demanding%0Aand%20cumbersome%20to%20implement%2C%20which%20motivates%20increasingly%20popular%20unconstrained%0Aarchitectures%20that%20learn%20approximate%20symmetries%20as%20part%20of%20the%20training%0Aprocess.%20In%20this%20work%2C%20we%20explore%20a%20third%20route%20to%20tackle%20this%20learning%0Aproblem%2C%20where%20equivariant%20functions%20are%20expressed%20as%20the%20product%20of%20a%20scalar%0Afunction%20of%20the%20point%20cloud%20coordinates%20and%20a%20small%20basis%20of%20tensors%20with%20the%0Aappropriate%20symmetry.%20We%20also%20propose%20approximations%20of%20the%20general%20expressions%0Athat%2C%20while%20lacking%20universal%20approximation%20properties%2C%20are%20fast%2C%20simple%20to%0Aimplement%2C%20and%20accurate%20in%20practical%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05404v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepresenting%2520spherical%2520tensors%2520with%2520scalar-based%2520machine-learning%2520models%26entry.906535625%3DMichelangelo%2520Domina%2520and%2520Filippo%2520Bigi%2520and%2520Paolo%2520Pegolo%2520and%2520Michele%2520Ceriotti%26entry.1292438233%3D%2520%2520Rotational%2520symmetry%2520plays%2520a%2520central%2520role%2520in%2520physics%252C%2520providing%2520an%2520elegant%250Aframework%2520to%2520describe%2520how%2520the%2520properties%2520of%25203D%2520objects%2520--%2520from%2520atoms%2520to%2520the%250Amacroscopic%2520scale%2520--%2520transform%2520under%2520the%2520action%2520of%2520rigid%2520rotations.%2520Equivariant%250Amodels%2520of%25203D%2520point%2520clouds%2520are%2520able%2520to%2520approximate%2520structure-property%2520relations%250Ain%2520a%2520way%2520that%2520is%2520fully%2520consistent%2520with%2520the%2520structure%2520of%2520the%2520rotation%2520group%252C%2520by%250Acombining%2520intermediate%2520representations%2520that%2520are%2520themselves%2520spherical%2520tensors.%250AThe%2520symmetry%2520constraints%2520however%2520make%2520this%2520approach%2520computationally%2520demanding%250Aand%2520cumbersome%2520to%2520implement%252C%2520which%2520motivates%2520increasingly%2520popular%2520unconstrained%250Aarchitectures%2520that%2520learn%2520approximate%2520symmetries%2520as%2520part%2520of%2520the%2520training%250Aprocess.%2520In%2520this%2520work%252C%2520we%2520explore%2520a%2520third%2520route%2520to%2520tackle%2520this%2520learning%250Aproblem%252C%2520where%2520equivariant%2520functions%2520are%2520expressed%2520as%2520the%2520product%2520of%2520a%2520scalar%250Afunction%2520of%2520the%2520point%2520cloud%2520coordinates%2520and%2520a%2520small%2520basis%2520of%2520tensors%2520with%2520the%250Aappropriate%2520symmetry.%2520We%2520also%2520propose%2520approximations%2520of%2520the%2520general%2520expressions%250Athat%252C%2520while%2520lacking%2520universal%2520approximation%2520properties%252C%2520are%2520fast%252C%2520simple%2520to%250Aimplement%252C%2520and%2520accurate%2520in%2520practical%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05404v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Representing%20spherical%20tensors%20with%20scalar-based%20machine-learning%20models&entry.906535625=Michelangelo%20Domina%20and%20Filippo%20Bigi%20and%20Paolo%20Pegolo%20and%20Michele%20Ceriotti&entry.1292438233=%20%20Rotational%20symmetry%20plays%20a%20central%20role%20in%20physics%2C%20providing%20an%20elegant%0Aframework%20to%20describe%20how%20the%20properties%20of%203D%20objects%20--%20from%20atoms%20to%20the%0Amacroscopic%20scale%20--%20transform%20under%20the%20action%20of%20rigid%20rotations.%20Equivariant%0Amodels%20of%203D%20point%20clouds%20are%20able%20to%20approximate%20structure-property%20relations%0Ain%20a%20way%20that%20is%20fully%20consistent%20with%20the%20structure%20of%20the%20rotation%20group%2C%20by%0Acombining%20intermediate%20representations%20that%20are%20themselves%20spherical%20tensors.%0AThe%20symmetry%20constraints%20however%20make%20this%20approach%20computationally%20demanding%0Aand%20cumbersome%20to%20implement%2C%20which%20motivates%20increasingly%20popular%20unconstrained%0Aarchitectures%20that%20learn%20approximate%20symmetries%20as%20part%20of%20the%20training%0Aprocess.%20In%20this%20work%2C%20we%20explore%20a%20third%20route%20to%20tackle%20this%20learning%0Aproblem%2C%20where%20equivariant%20functions%20are%20expressed%20as%20the%20product%20of%20a%20scalar%0Afunction%20of%20the%20point%20cloud%20coordinates%20and%20a%20small%20basis%20of%20tensors%20with%20the%0Aappropriate%20symmetry.%20We%20also%20propose%20approximations%20of%20the%20general%20expressions%0Athat%2C%20while%20lacking%20universal%20approximation%20properties%2C%20are%20fast%2C%20simple%20to%0Aimplement%2C%20and%20accurate%20in%20practical%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05404v2&entry.124074799=Read"},
{"title": "Bring Your Rear Cameras for Egocentric 3D Human Pose Estimation", "author": "Hiroyasu Akada and Jian Wang and Vladislav Golyanik and Christian Theobalt", "abstract": "  Egocentric 3D human pose estimation has been actively studied using cameras\ninstalled in front of a head-mounted device (HMD). While frontal placement is\nthe optimal and the only option for some tasks, such as hand tracking, it\nremains unclear if the same holds for full-body tracking due to self-occlusion\nand limited field-of-view coverage. Notably, even the state-of-the-art methods\noften fail to estimate accurate 3D poses in many scenarios, such as when HMD\nusers tilt their heads upward -- a common motion in human activities. A key\nlimitation of existing HMD designs is their neglect of the back of the body,\ndespite its potential to provide crucial 3D reconstruction cues. Hence, this\npaper investigates the usefulness of rear cameras for full-body tracking. We\nalso show that simply adding rear views to the frontal inputs is not optimal\nfor existing methods due to their dependence on individual 2D joint detectors\nwithout effective multi-view integration. To address this issue, we propose a\nnew transformer-based method that refines 2D joint heatmap estimation with\nmulti-view information and heatmap uncertainty, thereby improving 3D pose\ntracking. Also, we introduce two new large-scale datasets, Ego4View-Syn and\nEgo4View-RW, for a rear-view evaluation. Our experiments show that the new\ncamera configurations with back views provide superior support for 3D pose\ntracking compared to only frontal placements. The proposed method achieves\nsignificant improvement over the current state of the art (>10% on MPJPE). The\nsource code, trained models, and datasets are available on our project page at\nhttps://4dqv.mpi-inf.mpg.de/EgoRear/.\n", "link": "http://arxiv.org/abs/2503.11652v2", "date": "2025-08-22", "relevancy": 2.4349, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6191}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6057}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bring%20Your%20Rear%20Cameras%20for%20Egocentric%203D%20Human%20Pose%20Estimation&body=Title%3A%20Bring%20Your%20Rear%20Cameras%20for%20Egocentric%203D%20Human%20Pose%20Estimation%0AAuthor%3A%20Hiroyasu%20Akada%20and%20Jian%20Wang%20and%20Vladislav%20Golyanik%20and%20Christian%20Theobalt%0AAbstract%3A%20%20%20Egocentric%203D%20human%20pose%20estimation%20has%20been%20actively%20studied%20using%20cameras%0Ainstalled%20in%20front%20of%20a%20head-mounted%20device%20%28HMD%29.%20While%20frontal%20placement%20is%0Athe%20optimal%20and%20the%20only%20option%20for%20some%20tasks%2C%20such%20as%20hand%20tracking%2C%20it%0Aremains%20unclear%20if%20the%20same%20holds%20for%20full-body%20tracking%20due%20to%20self-occlusion%0Aand%20limited%20field-of-view%20coverage.%20Notably%2C%20even%20the%20state-of-the-art%20methods%0Aoften%20fail%20to%20estimate%20accurate%203D%20poses%20in%20many%20scenarios%2C%20such%20as%20when%20HMD%0Ausers%20tilt%20their%20heads%20upward%20--%20a%20common%20motion%20in%20human%20activities.%20A%20key%0Alimitation%20of%20existing%20HMD%20designs%20is%20their%20neglect%20of%20the%20back%20of%20the%20body%2C%0Adespite%20its%20potential%20to%20provide%20crucial%203D%20reconstruction%20cues.%20Hence%2C%20this%0Apaper%20investigates%20the%20usefulness%20of%20rear%20cameras%20for%20full-body%20tracking.%20We%0Aalso%20show%20that%20simply%20adding%20rear%20views%20to%20the%20frontal%20inputs%20is%20not%20optimal%0Afor%20existing%20methods%20due%20to%20their%20dependence%20on%20individual%202D%20joint%20detectors%0Awithout%20effective%20multi-view%20integration.%20To%20address%20this%20issue%2C%20we%20propose%20a%0Anew%20transformer-based%20method%20that%20refines%202D%20joint%20heatmap%20estimation%20with%0Amulti-view%20information%20and%20heatmap%20uncertainty%2C%20thereby%20improving%203D%20pose%0Atracking.%20Also%2C%20we%20introduce%20two%20new%20large-scale%20datasets%2C%20Ego4View-Syn%20and%0AEgo4View-RW%2C%20for%20a%20rear-view%20evaluation.%20Our%20experiments%20show%20that%20the%20new%0Acamera%20configurations%20with%20back%20views%20provide%20superior%20support%20for%203D%20pose%0Atracking%20compared%20to%20only%20frontal%20placements.%20The%20proposed%20method%20achieves%0Asignificant%20improvement%20over%20the%20current%20state%20of%20the%20art%20%28%3E10%25%20on%20MPJPE%29.%20The%0Asource%20code%2C%20trained%20models%2C%20and%20datasets%20are%20available%20on%20our%20project%20page%20at%0Ahttps%3A//4dqv.mpi-inf.mpg.de/EgoRear/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.11652v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBring%2520Your%2520Rear%2520Cameras%2520for%2520Egocentric%25203D%2520Human%2520Pose%2520Estimation%26entry.906535625%3DHiroyasu%2520Akada%2520and%2520Jian%2520Wang%2520and%2520Vladislav%2520Golyanik%2520and%2520Christian%2520Theobalt%26entry.1292438233%3D%2520%2520Egocentric%25203D%2520human%2520pose%2520estimation%2520has%2520been%2520actively%2520studied%2520using%2520cameras%250Ainstalled%2520in%2520front%2520of%2520a%2520head-mounted%2520device%2520%2528HMD%2529.%2520While%2520frontal%2520placement%2520is%250Athe%2520optimal%2520and%2520the%2520only%2520option%2520for%2520some%2520tasks%252C%2520such%2520as%2520hand%2520tracking%252C%2520it%250Aremains%2520unclear%2520if%2520the%2520same%2520holds%2520for%2520full-body%2520tracking%2520due%2520to%2520self-occlusion%250Aand%2520limited%2520field-of-view%2520coverage.%2520Notably%252C%2520even%2520the%2520state-of-the-art%2520methods%250Aoften%2520fail%2520to%2520estimate%2520accurate%25203D%2520poses%2520in%2520many%2520scenarios%252C%2520such%2520as%2520when%2520HMD%250Ausers%2520tilt%2520their%2520heads%2520upward%2520--%2520a%2520common%2520motion%2520in%2520human%2520activities.%2520A%2520key%250Alimitation%2520of%2520existing%2520HMD%2520designs%2520is%2520their%2520neglect%2520of%2520the%2520back%2520of%2520the%2520body%252C%250Adespite%2520its%2520potential%2520to%2520provide%2520crucial%25203D%2520reconstruction%2520cues.%2520Hence%252C%2520this%250Apaper%2520investigates%2520the%2520usefulness%2520of%2520rear%2520cameras%2520for%2520full-body%2520tracking.%2520We%250Aalso%2520show%2520that%2520simply%2520adding%2520rear%2520views%2520to%2520the%2520frontal%2520inputs%2520is%2520not%2520optimal%250Afor%2520existing%2520methods%2520due%2520to%2520their%2520dependence%2520on%2520individual%25202D%2520joint%2520detectors%250Awithout%2520effective%2520multi-view%2520integration.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%250Anew%2520transformer-based%2520method%2520that%2520refines%25202D%2520joint%2520heatmap%2520estimation%2520with%250Amulti-view%2520information%2520and%2520heatmap%2520uncertainty%252C%2520thereby%2520improving%25203D%2520pose%250Atracking.%2520Also%252C%2520we%2520introduce%2520two%2520new%2520large-scale%2520datasets%252C%2520Ego4View-Syn%2520and%250AEgo4View-RW%252C%2520for%2520a%2520rear-view%2520evaluation.%2520Our%2520experiments%2520show%2520that%2520the%2520new%250Acamera%2520configurations%2520with%2520back%2520views%2520provide%2520superior%2520support%2520for%25203D%2520pose%250Atracking%2520compared%2520to%2520only%2520frontal%2520placements.%2520The%2520proposed%2520method%2520achieves%250Asignificant%2520improvement%2520over%2520the%2520current%2520state%2520of%2520the%2520art%2520%2528%253E10%2525%2520on%2520MPJPE%2529.%2520The%250Asource%2520code%252C%2520trained%2520models%252C%2520and%2520datasets%2520are%2520available%2520on%2520our%2520project%2520page%2520at%250Ahttps%253A//4dqv.mpi-inf.mpg.de/EgoRear/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.11652v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bring%20Your%20Rear%20Cameras%20for%20Egocentric%203D%20Human%20Pose%20Estimation&entry.906535625=Hiroyasu%20Akada%20and%20Jian%20Wang%20and%20Vladislav%20Golyanik%20and%20Christian%20Theobalt&entry.1292438233=%20%20Egocentric%203D%20human%20pose%20estimation%20has%20been%20actively%20studied%20using%20cameras%0Ainstalled%20in%20front%20of%20a%20head-mounted%20device%20%28HMD%29.%20While%20frontal%20placement%20is%0Athe%20optimal%20and%20the%20only%20option%20for%20some%20tasks%2C%20such%20as%20hand%20tracking%2C%20it%0Aremains%20unclear%20if%20the%20same%20holds%20for%20full-body%20tracking%20due%20to%20self-occlusion%0Aand%20limited%20field-of-view%20coverage.%20Notably%2C%20even%20the%20state-of-the-art%20methods%0Aoften%20fail%20to%20estimate%20accurate%203D%20poses%20in%20many%20scenarios%2C%20such%20as%20when%20HMD%0Ausers%20tilt%20their%20heads%20upward%20--%20a%20common%20motion%20in%20human%20activities.%20A%20key%0Alimitation%20of%20existing%20HMD%20designs%20is%20their%20neglect%20of%20the%20back%20of%20the%20body%2C%0Adespite%20its%20potential%20to%20provide%20crucial%203D%20reconstruction%20cues.%20Hence%2C%20this%0Apaper%20investigates%20the%20usefulness%20of%20rear%20cameras%20for%20full-body%20tracking.%20We%0Aalso%20show%20that%20simply%20adding%20rear%20views%20to%20the%20frontal%20inputs%20is%20not%20optimal%0Afor%20existing%20methods%20due%20to%20their%20dependence%20on%20individual%202D%20joint%20detectors%0Awithout%20effective%20multi-view%20integration.%20To%20address%20this%20issue%2C%20we%20propose%20a%0Anew%20transformer-based%20method%20that%20refines%202D%20joint%20heatmap%20estimation%20with%0Amulti-view%20information%20and%20heatmap%20uncertainty%2C%20thereby%20improving%203D%20pose%0Atracking.%20Also%2C%20we%20introduce%20two%20new%20large-scale%20datasets%2C%20Ego4View-Syn%20and%0AEgo4View-RW%2C%20for%20a%20rear-view%20evaluation.%20Our%20experiments%20show%20that%20the%20new%0Acamera%20configurations%20with%20back%20views%20provide%20superior%20support%20for%203D%20pose%0Atracking%20compared%20to%20only%20frontal%20placements.%20The%20proposed%20method%20achieves%0Asignificant%20improvement%20over%20the%20current%20state%20of%20the%20art%20%28%3E10%25%20on%20MPJPE%29.%20The%0Asource%20code%2C%20trained%20models%2C%20and%20datasets%20are%20available%20on%20our%20project%20page%20at%0Ahttps%3A//4dqv.mpi-inf.mpg.de/EgoRear/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.11652v2&entry.124074799=Read"},
{"title": "LearnLM: Improving Gemini for Learning", "author": " LearnLM Team and Abhinit Modi and Aditya Srikanth Veerubhotla and Aliya Rysbek and Andrea Huber and Brett Wiltshire and Brian Veprek and Daniel Gillick and Daniel Kasenberg and Derek Ahmed and Irina Jurenka and James Cohan and Jennifer She and Julia Wilkowski and Kaiz Alarakyia and Kevin R. McKee and Lisa Wang and Markus Kunesch and Mike Schaekermann and Miruna P\u00eeslar and Nikhil Joshi and Parsa Mahmoudieh and Paul Jhun and Sara Wiltberger and Shakir Mohamed and Shashank Agarwal and Shubham Milind Phal and Sun Jae Lee and Theofilos Strinopoulos and Wei-Jen Ko and Amy Wang and Ankit Anand and Avishkar Bhoopchand and Dan Wild and Divya Pandya and Filip Bar and Garth Graham and Holger Winnemoeller and Mahvish Nagda and Prateek Kolhar and Renee Schneider and Shaojian Zhu and Stephanie Chan and Steve Yadlowsky and Viknesh Sounderajah and Yannis Assael", "abstract": "  Today's generative AI systems are tuned to present information by default,\nrather than engage users in service of learning as a human tutor would. To\naddress the wide range of potential education use cases for these systems, we\nreframe the challenge of injecting pedagogical behavior as one of\n\\textit{pedagogical instruction following}, where training and evaluation\nexamples include system-level instructions describing the specific pedagogy\nattributes present or desired in subsequent model turns. This framing avoids\ncommitting our models to any particular definition of pedagogy, and instead\nallows teachers or developers to specify desired model behavior. It also clears\na path to improving Gemini models for learning -- by enabling the addition of\nour pedagogical data to post-training mixtures -- alongside their rapidly\nexpanding set of capabilities. Both represent important changes from our\ninitial tech report. We show how training with pedagogical instruction\nfollowing produces a LearnLM model (available on Google AI Studio) that experts\nsubstantially prefer across a diverse set of learning scenarios, with average\npreference strengths of +31\\% over GPT-4o, +11\\% over Claude 3.5 Sonnet, and\n+13\\% over the Gemini 1.5 Pro model on which LearnLM was based.\n", "link": "http://arxiv.org/abs/2412.16429v3", "date": "2025-08-22", "relevancy": 2.4147, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5041}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4803}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LearnLM%3A%20Improving%20Gemini%20for%20Learning&body=Title%3A%20LearnLM%3A%20Improving%20Gemini%20for%20Learning%0AAuthor%3A%20%20LearnLM%20Team%20and%20Abhinit%20Modi%20and%20Aditya%20Srikanth%20Veerubhotla%20and%20Aliya%20Rysbek%20and%20Andrea%20Huber%20and%20Brett%20Wiltshire%20and%20Brian%20Veprek%20and%20Daniel%20Gillick%20and%20Daniel%20Kasenberg%20and%20Derek%20Ahmed%20and%20Irina%20Jurenka%20and%20James%20Cohan%20and%20Jennifer%20She%20and%20Julia%20Wilkowski%20and%20Kaiz%20Alarakyia%20and%20Kevin%20R.%20McKee%20and%20Lisa%20Wang%20and%20Markus%20Kunesch%20and%20Mike%20Schaekermann%20and%20Miruna%20P%C3%AEslar%20and%20Nikhil%20Joshi%20and%20Parsa%20Mahmoudieh%20and%20Paul%20Jhun%20and%20Sara%20Wiltberger%20and%20Shakir%20Mohamed%20and%20Shashank%20Agarwal%20and%20Shubham%20Milind%20Phal%20and%20Sun%20Jae%20Lee%20and%20Theofilos%20Strinopoulos%20and%20Wei-Jen%20Ko%20and%20Amy%20Wang%20and%20Ankit%20Anand%20and%20Avishkar%20Bhoopchand%20and%20Dan%20Wild%20and%20Divya%20Pandya%20and%20Filip%20Bar%20and%20Garth%20Graham%20and%20Holger%20Winnemoeller%20and%20Mahvish%20Nagda%20and%20Prateek%20Kolhar%20and%20Renee%20Schneider%20and%20Shaojian%20Zhu%20and%20Stephanie%20Chan%20and%20Steve%20Yadlowsky%20and%20Viknesh%20Sounderajah%20and%20Yannis%20Assael%0AAbstract%3A%20%20%20Today%27s%20generative%20AI%20systems%20are%20tuned%20to%20present%20information%20by%20default%2C%0Arather%20than%20engage%20users%20in%20service%20of%20learning%20as%20a%20human%20tutor%20would.%20To%0Aaddress%20the%20wide%20range%20of%20potential%20education%20use%20cases%20for%20these%20systems%2C%20we%0Areframe%20the%20challenge%20of%20injecting%20pedagogical%20behavior%20as%20one%20of%0A%5Ctextit%7Bpedagogical%20instruction%20following%7D%2C%20where%20training%20and%20evaluation%0Aexamples%20include%20system-level%20instructions%20describing%20the%20specific%20pedagogy%0Aattributes%20present%20or%20desired%20in%20subsequent%20model%20turns.%20This%20framing%20avoids%0Acommitting%20our%20models%20to%20any%20particular%20definition%20of%20pedagogy%2C%20and%20instead%0Aallows%20teachers%20or%20developers%20to%20specify%20desired%20model%20behavior.%20It%20also%20clears%0Aa%20path%20to%20improving%20Gemini%20models%20for%20learning%20--%20by%20enabling%20the%20addition%20of%0Aour%20pedagogical%20data%20to%20post-training%20mixtures%20--%20alongside%20their%20rapidly%0Aexpanding%20set%20of%20capabilities.%20Both%20represent%20important%20changes%20from%20our%0Ainitial%20tech%20report.%20We%20show%20how%20training%20with%20pedagogical%20instruction%0Afollowing%20produces%20a%20LearnLM%20model%20%28available%20on%20Google%20AI%20Studio%29%20that%20experts%0Asubstantially%20prefer%20across%20a%20diverse%20set%20of%20learning%20scenarios%2C%20with%20average%0Apreference%20strengths%20of%20%2B31%5C%25%20over%20GPT-4o%2C%20%2B11%5C%25%20over%20Claude%203.5%20Sonnet%2C%20and%0A%2B13%5C%25%20over%20the%20Gemini%201.5%20Pro%20model%20on%20which%20LearnLM%20was%20based.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16429v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearnLM%253A%2520Improving%2520Gemini%2520for%2520Learning%26entry.906535625%3D%2520LearnLM%2520Team%2520and%2520Abhinit%2520Modi%2520and%2520Aditya%2520Srikanth%2520Veerubhotla%2520and%2520Aliya%2520Rysbek%2520and%2520Andrea%2520Huber%2520and%2520Brett%2520Wiltshire%2520and%2520Brian%2520Veprek%2520and%2520Daniel%2520Gillick%2520and%2520Daniel%2520Kasenberg%2520and%2520Derek%2520Ahmed%2520and%2520Irina%2520Jurenka%2520and%2520James%2520Cohan%2520and%2520Jennifer%2520She%2520and%2520Julia%2520Wilkowski%2520and%2520Kaiz%2520Alarakyia%2520and%2520Kevin%2520R.%2520McKee%2520and%2520Lisa%2520Wang%2520and%2520Markus%2520Kunesch%2520and%2520Mike%2520Schaekermann%2520and%2520Miruna%2520P%25C3%25AEslar%2520and%2520Nikhil%2520Joshi%2520and%2520Parsa%2520Mahmoudieh%2520and%2520Paul%2520Jhun%2520and%2520Sara%2520Wiltberger%2520and%2520Shakir%2520Mohamed%2520and%2520Shashank%2520Agarwal%2520and%2520Shubham%2520Milind%2520Phal%2520and%2520Sun%2520Jae%2520Lee%2520and%2520Theofilos%2520Strinopoulos%2520and%2520Wei-Jen%2520Ko%2520and%2520Amy%2520Wang%2520and%2520Ankit%2520Anand%2520and%2520Avishkar%2520Bhoopchand%2520and%2520Dan%2520Wild%2520and%2520Divya%2520Pandya%2520and%2520Filip%2520Bar%2520and%2520Garth%2520Graham%2520and%2520Holger%2520Winnemoeller%2520and%2520Mahvish%2520Nagda%2520and%2520Prateek%2520Kolhar%2520and%2520Renee%2520Schneider%2520and%2520Shaojian%2520Zhu%2520and%2520Stephanie%2520Chan%2520and%2520Steve%2520Yadlowsky%2520and%2520Viknesh%2520Sounderajah%2520and%2520Yannis%2520Assael%26entry.1292438233%3D%2520%2520Today%2527s%2520generative%2520AI%2520systems%2520are%2520tuned%2520to%2520present%2520information%2520by%2520default%252C%250Arather%2520than%2520engage%2520users%2520in%2520service%2520of%2520learning%2520as%2520a%2520human%2520tutor%2520would.%2520To%250Aaddress%2520the%2520wide%2520range%2520of%2520potential%2520education%2520use%2520cases%2520for%2520these%2520systems%252C%2520we%250Areframe%2520the%2520challenge%2520of%2520injecting%2520pedagogical%2520behavior%2520as%2520one%2520of%250A%255Ctextit%257Bpedagogical%2520instruction%2520following%257D%252C%2520where%2520training%2520and%2520evaluation%250Aexamples%2520include%2520system-level%2520instructions%2520describing%2520the%2520specific%2520pedagogy%250Aattributes%2520present%2520or%2520desired%2520in%2520subsequent%2520model%2520turns.%2520This%2520framing%2520avoids%250Acommitting%2520our%2520models%2520to%2520any%2520particular%2520definition%2520of%2520pedagogy%252C%2520and%2520instead%250Aallows%2520teachers%2520or%2520developers%2520to%2520specify%2520desired%2520model%2520behavior.%2520It%2520also%2520clears%250Aa%2520path%2520to%2520improving%2520Gemini%2520models%2520for%2520learning%2520--%2520by%2520enabling%2520the%2520addition%2520of%250Aour%2520pedagogical%2520data%2520to%2520post-training%2520mixtures%2520--%2520alongside%2520their%2520rapidly%250Aexpanding%2520set%2520of%2520capabilities.%2520Both%2520represent%2520important%2520changes%2520from%2520our%250Ainitial%2520tech%2520report.%2520We%2520show%2520how%2520training%2520with%2520pedagogical%2520instruction%250Afollowing%2520produces%2520a%2520LearnLM%2520model%2520%2528available%2520on%2520Google%2520AI%2520Studio%2529%2520that%2520experts%250Asubstantially%2520prefer%2520across%2520a%2520diverse%2520set%2520of%2520learning%2520scenarios%252C%2520with%2520average%250Apreference%2520strengths%2520of%2520%252B31%255C%2525%2520over%2520GPT-4o%252C%2520%252B11%255C%2525%2520over%2520Claude%25203.5%2520Sonnet%252C%2520and%250A%252B13%255C%2525%2520over%2520the%2520Gemini%25201.5%2520Pro%2520model%2520on%2520which%2520LearnLM%2520was%2520based.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16429v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LearnLM%3A%20Improving%20Gemini%20for%20Learning&entry.906535625=%20LearnLM%20Team%20and%20Abhinit%20Modi%20and%20Aditya%20Srikanth%20Veerubhotla%20and%20Aliya%20Rysbek%20and%20Andrea%20Huber%20and%20Brett%20Wiltshire%20and%20Brian%20Veprek%20and%20Daniel%20Gillick%20and%20Daniel%20Kasenberg%20and%20Derek%20Ahmed%20and%20Irina%20Jurenka%20and%20James%20Cohan%20and%20Jennifer%20She%20and%20Julia%20Wilkowski%20and%20Kaiz%20Alarakyia%20and%20Kevin%20R.%20McKee%20and%20Lisa%20Wang%20and%20Markus%20Kunesch%20and%20Mike%20Schaekermann%20and%20Miruna%20P%C3%AEslar%20and%20Nikhil%20Joshi%20and%20Parsa%20Mahmoudieh%20and%20Paul%20Jhun%20and%20Sara%20Wiltberger%20and%20Shakir%20Mohamed%20and%20Shashank%20Agarwal%20and%20Shubham%20Milind%20Phal%20and%20Sun%20Jae%20Lee%20and%20Theofilos%20Strinopoulos%20and%20Wei-Jen%20Ko%20and%20Amy%20Wang%20and%20Ankit%20Anand%20and%20Avishkar%20Bhoopchand%20and%20Dan%20Wild%20and%20Divya%20Pandya%20and%20Filip%20Bar%20and%20Garth%20Graham%20and%20Holger%20Winnemoeller%20and%20Mahvish%20Nagda%20and%20Prateek%20Kolhar%20and%20Renee%20Schneider%20and%20Shaojian%20Zhu%20and%20Stephanie%20Chan%20and%20Steve%20Yadlowsky%20and%20Viknesh%20Sounderajah%20and%20Yannis%20Assael&entry.1292438233=%20%20Today%27s%20generative%20AI%20systems%20are%20tuned%20to%20present%20information%20by%20default%2C%0Arather%20than%20engage%20users%20in%20service%20of%20learning%20as%20a%20human%20tutor%20would.%20To%0Aaddress%20the%20wide%20range%20of%20potential%20education%20use%20cases%20for%20these%20systems%2C%20we%0Areframe%20the%20challenge%20of%20injecting%20pedagogical%20behavior%20as%20one%20of%0A%5Ctextit%7Bpedagogical%20instruction%20following%7D%2C%20where%20training%20and%20evaluation%0Aexamples%20include%20system-level%20instructions%20describing%20the%20specific%20pedagogy%0Aattributes%20present%20or%20desired%20in%20subsequent%20model%20turns.%20This%20framing%20avoids%0Acommitting%20our%20models%20to%20any%20particular%20definition%20of%20pedagogy%2C%20and%20instead%0Aallows%20teachers%20or%20developers%20to%20specify%20desired%20model%20behavior.%20It%20also%20clears%0Aa%20path%20to%20improving%20Gemini%20models%20for%20learning%20--%20by%20enabling%20the%20addition%20of%0Aour%20pedagogical%20data%20to%20post-training%20mixtures%20--%20alongside%20their%20rapidly%0Aexpanding%20set%20of%20capabilities.%20Both%20represent%20important%20changes%20from%20our%0Ainitial%20tech%20report.%20We%20show%20how%20training%20with%20pedagogical%20instruction%0Afollowing%20produces%20a%20LearnLM%20model%20%28available%20on%20Google%20AI%20Studio%29%20that%20experts%0Asubstantially%20prefer%20across%20a%20diverse%20set%20of%20learning%20scenarios%2C%20with%20average%0Apreference%20strengths%20of%20%2B31%5C%25%20over%20GPT-4o%2C%20%2B11%5C%25%20over%20Claude%203.5%20Sonnet%2C%20and%0A%2B13%5C%25%20over%20the%20Gemini%201.5%20Pro%20model%20on%20which%20LearnLM%20was%20based.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16429v3&entry.124074799=Read"},
{"title": "Enhanced Hybrid Technique for Efficient Digitization of Handwritten\n  Marksheets", "author": "Junaid Ahmed Sifat and Abir Chowdhury and Hasnat Md. Imtiaz and Md. Irtiza Hossain and Md. Imran Bin Azad", "abstract": "  The digitization of handwritten marksheets presents huge challenges due to\nthe different styles of handwriting and complex table structures in such\ndocuments like marksheets. This work introduces a hybrid method that integrates\nOpenCV for table detection and PaddleOCR for recognizing sequential handwritten\ntext. The image processing capabilities of OpenCV efficiently detects rows and\ncolumns which enable computationally lightweight and accurate table detection.\nAdditionally, YOLOv8 and Modified YOLOv8 are implemented for handwritten text\nrecognition within the detected table structures alongside PaddleOCR which\nfurther enhance the system's versatility. The proposed model achieves high\naccuracy on our custom dataset which is designed to represent different and\ndiverse handwriting styles and complex table layouts. Experimental results\ndemonstrate that YOLOv8 Modified achieves an accuracy of 92.72 percent,\noutperforming PaddleOCR 91.37 percent and the YOLOv8 model 88.91 percent. This\nefficiency reduces the necessity for manual work which makes this a practical\nand fast solution for digitizing academic as well as administrative documents.\nThis research serves the field of document automation, particularly handwritten\ndocument understanding, by providing operational and reliable methods to scale,\nenhance, and integrate the technologies involved.\n", "link": "http://arxiv.org/abs/2508.16295v1", "date": "2025-08-22", "relevancy": 2.3804, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4899}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4767}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4617}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Hybrid%20Technique%20for%20Efficient%20Digitization%20of%20Handwritten%0A%20%20Marksheets&body=Title%3A%20Enhanced%20Hybrid%20Technique%20for%20Efficient%20Digitization%20of%20Handwritten%0A%20%20Marksheets%0AAuthor%3A%20Junaid%20Ahmed%20Sifat%20and%20Abir%20Chowdhury%20and%20Hasnat%20Md.%20Imtiaz%20and%20Md.%20Irtiza%20Hossain%20and%20Md.%20Imran%20Bin%20Azad%0AAbstract%3A%20%20%20The%20digitization%20of%20handwritten%20marksheets%20presents%20huge%20challenges%20due%20to%0Athe%20different%20styles%20of%20handwriting%20and%20complex%20table%20structures%20in%20such%0Adocuments%20like%20marksheets.%20This%20work%20introduces%20a%20hybrid%20method%20that%20integrates%0AOpenCV%20for%20table%20detection%20and%20PaddleOCR%20for%20recognizing%20sequential%20handwritten%0Atext.%20The%20image%20processing%20capabilities%20of%20OpenCV%20efficiently%20detects%20rows%20and%0Acolumns%20which%20enable%20computationally%20lightweight%20and%20accurate%20table%20detection.%0AAdditionally%2C%20YOLOv8%20and%20Modified%20YOLOv8%20are%20implemented%20for%20handwritten%20text%0Arecognition%20within%20the%20detected%20table%20structures%20alongside%20PaddleOCR%20which%0Afurther%20enhance%20the%20system%27s%20versatility.%20The%20proposed%20model%20achieves%20high%0Aaccuracy%20on%20our%20custom%20dataset%20which%20is%20designed%20to%20represent%20different%20and%0Adiverse%20handwriting%20styles%20and%20complex%20table%20layouts.%20Experimental%20results%0Ademonstrate%20that%20YOLOv8%20Modified%20achieves%20an%20accuracy%20of%2092.72%20percent%2C%0Aoutperforming%20PaddleOCR%2091.37%20percent%20and%20the%20YOLOv8%20model%2088.91%20percent.%20This%0Aefficiency%20reduces%20the%20necessity%20for%20manual%20work%20which%20makes%20this%20a%20practical%0Aand%20fast%20solution%20for%20digitizing%20academic%20as%20well%20as%20administrative%20documents.%0AThis%20research%20serves%20the%20field%20of%20document%20automation%2C%20particularly%20handwritten%0Adocument%20understanding%2C%20by%20providing%20operational%20and%20reliable%20methods%20to%20scale%2C%0Aenhance%2C%20and%20integrate%20the%20technologies%20involved.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16295v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Hybrid%2520Technique%2520for%2520Efficient%2520Digitization%2520of%2520Handwritten%250A%2520%2520Marksheets%26entry.906535625%3DJunaid%2520Ahmed%2520Sifat%2520and%2520Abir%2520Chowdhury%2520and%2520Hasnat%2520Md.%2520Imtiaz%2520and%2520Md.%2520Irtiza%2520Hossain%2520and%2520Md.%2520Imran%2520Bin%2520Azad%26entry.1292438233%3D%2520%2520The%2520digitization%2520of%2520handwritten%2520marksheets%2520presents%2520huge%2520challenges%2520due%2520to%250Athe%2520different%2520styles%2520of%2520handwriting%2520and%2520complex%2520table%2520structures%2520in%2520such%250Adocuments%2520like%2520marksheets.%2520This%2520work%2520introduces%2520a%2520hybrid%2520method%2520that%2520integrates%250AOpenCV%2520for%2520table%2520detection%2520and%2520PaddleOCR%2520for%2520recognizing%2520sequential%2520handwritten%250Atext.%2520The%2520image%2520processing%2520capabilities%2520of%2520OpenCV%2520efficiently%2520detects%2520rows%2520and%250Acolumns%2520which%2520enable%2520computationally%2520lightweight%2520and%2520accurate%2520table%2520detection.%250AAdditionally%252C%2520YOLOv8%2520and%2520Modified%2520YOLOv8%2520are%2520implemented%2520for%2520handwritten%2520text%250Arecognition%2520within%2520the%2520detected%2520table%2520structures%2520alongside%2520PaddleOCR%2520which%250Afurther%2520enhance%2520the%2520system%2527s%2520versatility.%2520The%2520proposed%2520model%2520achieves%2520high%250Aaccuracy%2520on%2520our%2520custom%2520dataset%2520which%2520is%2520designed%2520to%2520represent%2520different%2520and%250Adiverse%2520handwriting%2520styles%2520and%2520complex%2520table%2520layouts.%2520Experimental%2520results%250Ademonstrate%2520that%2520YOLOv8%2520Modified%2520achieves%2520an%2520accuracy%2520of%252092.72%2520percent%252C%250Aoutperforming%2520PaddleOCR%252091.37%2520percent%2520and%2520the%2520YOLOv8%2520model%252088.91%2520percent.%2520This%250Aefficiency%2520reduces%2520the%2520necessity%2520for%2520manual%2520work%2520which%2520makes%2520this%2520a%2520practical%250Aand%2520fast%2520solution%2520for%2520digitizing%2520academic%2520as%2520well%2520as%2520administrative%2520documents.%250AThis%2520research%2520serves%2520the%2520field%2520of%2520document%2520automation%252C%2520particularly%2520handwritten%250Adocument%2520understanding%252C%2520by%2520providing%2520operational%2520and%2520reliable%2520methods%2520to%2520scale%252C%250Aenhance%252C%2520and%2520integrate%2520the%2520technologies%2520involved.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16295v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Hybrid%20Technique%20for%20Efficient%20Digitization%20of%20Handwritten%0A%20%20Marksheets&entry.906535625=Junaid%20Ahmed%20Sifat%20and%20Abir%20Chowdhury%20and%20Hasnat%20Md.%20Imtiaz%20and%20Md.%20Irtiza%20Hossain%20and%20Md.%20Imran%20Bin%20Azad&entry.1292438233=%20%20The%20digitization%20of%20handwritten%20marksheets%20presents%20huge%20challenges%20due%20to%0Athe%20different%20styles%20of%20handwriting%20and%20complex%20table%20structures%20in%20such%0Adocuments%20like%20marksheets.%20This%20work%20introduces%20a%20hybrid%20method%20that%20integrates%0AOpenCV%20for%20table%20detection%20and%20PaddleOCR%20for%20recognizing%20sequential%20handwritten%0Atext.%20The%20image%20processing%20capabilities%20of%20OpenCV%20efficiently%20detects%20rows%20and%0Acolumns%20which%20enable%20computationally%20lightweight%20and%20accurate%20table%20detection.%0AAdditionally%2C%20YOLOv8%20and%20Modified%20YOLOv8%20are%20implemented%20for%20handwritten%20text%0Arecognition%20within%20the%20detected%20table%20structures%20alongside%20PaddleOCR%20which%0Afurther%20enhance%20the%20system%27s%20versatility.%20The%20proposed%20model%20achieves%20high%0Aaccuracy%20on%20our%20custom%20dataset%20which%20is%20designed%20to%20represent%20different%20and%0Adiverse%20handwriting%20styles%20and%20complex%20table%20layouts.%20Experimental%20results%0Ademonstrate%20that%20YOLOv8%20Modified%20achieves%20an%20accuracy%20of%2092.72%20percent%2C%0Aoutperforming%20PaddleOCR%2091.37%20percent%20and%20the%20YOLOv8%20model%2088.91%20percent.%20This%0Aefficiency%20reduces%20the%20necessity%20for%20manual%20work%20which%20makes%20this%20a%20practical%0Aand%20fast%20solution%20for%20digitizing%20academic%20as%20well%20as%20administrative%20documents.%0AThis%20research%20serves%20the%20field%20of%20document%20automation%2C%20particularly%20handwritten%0Adocument%20understanding%2C%20by%20providing%20operational%20and%20reliable%20methods%20to%20scale%2C%0Aenhance%2C%20and%20integrate%20the%20technologies%20involved.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16295v1&entry.124074799=Read"},
{"title": "Highly Accurate and Diverse Traffic Data: The DeepScenario Open 3D\n  Dataset", "author": "Oussema Dhaouadi and Johannes Meier and Luca Wahl and Jacques Kaiser and Luca Scalerandi and Nick Wandelburg and Zhuolun Zhou and Nijanthan Berinpanathan and Holger Banzhaf and Daniel Cremers", "abstract": "  Accurate 3D trajectory data is crucial for advancing autonomous driving. Yet,\ntraditional datasets are usually captured by fixed sensors mounted on a car and\nare susceptible to occlusion. Additionally, such an approach can precisely\nreconstruct the dynamic environment in the close vicinity of the measurement\nvehicle only, while neglecting objects that are further away. In this paper, we\nintroduce the DeepScenario Open 3D Dataset (DSC3D), a high-quality,\nocclusion-free dataset of 6 degrees of freedom bounding box trajectories\nacquired through a novel monocular camera drone tracking pipeline. Our dataset\nincludes more than 175,000 trajectories of 14 types of traffic participants and\nsignificantly exceeds existing datasets in terms of diversity and scale,\ncontaining many unprecedented scenarios such as complex vehicle-pedestrian\ninteraction on highly populated urban streets and comprehensive parking\nmaneuvers from entry to exit. DSC3D dataset was captured in five various\nlocations in Europe and the United States and include: a parking lot, a crowded\ninner-city, a steep urban intersection, a federal highway, and a suburban\nintersection. Our 3D trajectory dataset aims to enhance autonomous driving\nsystems by providing detailed environmental 3D representations, which could\nlead to improved obstacle interactions and safety. We demonstrate its utility\nacross multiple applications including motion prediction, motion planning,\nscenario mining, and generative reactive traffic agents. Our interactive online\nvisualization platform and the complete dataset are publicly available at\nhttps://app.deepscenario.com, facilitating research in motion prediction,\nbehavior modeling, and safety validation.\n", "link": "http://arxiv.org/abs/2504.17371v3", "date": "2025-08-22", "relevancy": 2.3795, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6011}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5936}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5936}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Highly%20Accurate%20and%20Diverse%20Traffic%20Data%3A%20The%20DeepScenario%20Open%203D%0A%20%20Dataset&body=Title%3A%20Highly%20Accurate%20and%20Diverse%20Traffic%20Data%3A%20The%20DeepScenario%20Open%203D%0A%20%20Dataset%0AAuthor%3A%20Oussema%20Dhaouadi%20and%20Johannes%20Meier%20and%20Luca%20Wahl%20and%20Jacques%20Kaiser%20and%20Luca%20Scalerandi%20and%20Nick%20Wandelburg%20and%20Zhuolun%20Zhou%20and%20Nijanthan%20Berinpanathan%20and%20Holger%20Banzhaf%20and%20Daniel%20Cremers%0AAbstract%3A%20%20%20Accurate%203D%20trajectory%20data%20is%20crucial%20for%20advancing%20autonomous%20driving.%20Yet%2C%0Atraditional%20datasets%20are%20usually%20captured%20by%20fixed%20sensors%20mounted%20on%20a%20car%20and%0Aare%20susceptible%20to%20occlusion.%20Additionally%2C%20such%20an%20approach%20can%20precisely%0Areconstruct%20the%20dynamic%20environment%20in%20the%20close%20vicinity%20of%20the%20measurement%0Avehicle%20only%2C%20while%20neglecting%20objects%20that%20are%20further%20away.%20In%20this%20paper%2C%20we%0Aintroduce%20the%20DeepScenario%20Open%203D%20Dataset%20%28DSC3D%29%2C%20a%20high-quality%2C%0Aocclusion-free%20dataset%20of%206%20degrees%20of%20freedom%20bounding%20box%20trajectories%0Aacquired%20through%20a%20novel%20monocular%20camera%20drone%20tracking%20pipeline.%20Our%20dataset%0Aincludes%20more%20than%20175%2C000%20trajectories%20of%2014%20types%20of%20traffic%20participants%20and%0Asignificantly%20exceeds%20existing%20datasets%20in%20terms%20of%20diversity%20and%20scale%2C%0Acontaining%20many%20unprecedented%20scenarios%20such%20as%20complex%20vehicle-pedestrian%0Ainteraction%20on%20highly%20populated%20urban%20streets%20and%20comprehensive%20parking%0Amaneuvers%20from%20entry%20to%20exit.%20DSC3D%20dataset%20was%20captured%20in%20five%20various%0Alocations%20in%20Europe%20and%20the%20United%20States%20and%20include%3A%20a%20parking%20lot%2C%20a%20crowded%0Ainner-city%2C%20a%20steep%20urban%20intersection%2C%20a%20federal%20highway%2C%20and%20a%20suburban%0Aintersection.%20Our%203D%20trajectory%20dataset%20aims%20to%20enhance%20autonomous%20driving%0Asystems%20by%20providing%20detailed%20environmental%203D%20representations%2C%20which%20could%0Alead%20to%20improved%20obstacle%20interactions%20and%20safety.%20We%20demonstrate%20its%20utility%0Aacross%20multiple%20applications%20including%20motion%20prediction%2C%20motion%20planning%2C%0Ascenario%20mining%2C%20and%20generative%20reactive%20traffic%20agents.%20Our%20interactive%20online%0Avisualization%20platform%20and%20the%20complete%20dataset%20are%20publicly%20available%20at%0Ahttps%3A//app.deepscenario.com%2C%20facilitating%20research%20in%20motion%20prediction%2C%0Abehavior%20modeling%2C%20and%20safety%20validation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17371v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHighly%2520Accurate%2520and%2520Diverse%2520Traffic%2520Data%253A%2520The%2520DeepScenario%2520Open%25203D%250A%2520%2520Dataset%26entry.906535625%3DOussema%2520Dhaouadi%2520and%2520Johannes%2520Meier%2520and%2520Luca%2520Wahl%2520and%2520Jacques%2520Kaiser%2520and%2520Luca%2520Scalerandi%2520and%2520Nick%2520Wandelburg%2520and%2520Zhuolun%2520Zhou%2520and%2520Nijanthan%2520Berinpanathan%2520and%2520Holger%2520Banzhaf%2520and%2520Daniel%2520Cremers%26entry.1292438233%3D%2520%2520Accurate%25203D%2520trajectory%2520data%2520is%2520crucial%2520for%2520advancing%2520autonomous%2520driving.%2520Yet%252C%250Atraditional%2520datasets%2520are%2520usually%2520captured%2520by%2520fixed%2520sensors%2520mounted%2520on%2520a%2520car%2520and%250Aare%2520susceptible%2520to%2520occlusion.%2520Additionally%252C%2520such%2520an%2520approach%2520can%2520precisely%250Areconstruct%2520the%2520dynamic%2520environment%2520in%2520the%2520close%2520vicinity%2520of%2520the%2520measurement%250Avehicle%2520only%252C%2520while%2520neglecting%2520objects%2520that%2520are%2520further%2520away.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520the%2520DeepScenario%2520Open%25203D%2520Dataset%2520%2528DSC3D%2529%252C%2520a%2520high-quality%252C%250Aocclusion-free%2520dataset%2520of%25206%2520degrees%2520of%2520freedom%2520bounding%2520box%2520trajectories%250Aacquired%2520through%2520a%2520novel%2520monocular%2520camera%2520drone%2520tracking%2520pipeline.%2520Our%2520dataset%250Aincludes%2520more%2520than%2520175%252C000%2520trajectories%2520of%252014%2520types%2520of%2520traffic%2520participants%2520and%250Asignificantly%2520exceeds%2520existing%2520datasets%2520in%2520terms%2520of%2520diversity%2520and%2520scale%252C%250Acontaining%2520many%2520unprecedented%2520scenarios%2520such%2520as%2520complex%2520vehicle-pedestrian%250Ainteraction%2520on%2520highly%2520populated%2520urban%2520streets%2520and%2520comprehensive%2520parking%250Amaneuvers%2520from%2520entry%2520to%2520exit.%2520DSC3D%2520dataset%2520was%2520captured%2520in%2520five%2520various%250Alocations%2520in%2520Europe%2520and%2520the%2520United%2520States%2520and%2520include%253A%2520a%2520parking%2520lot%252C%2520a%2520crowded%250Ainner-city%252C%2520a%2520steep%2520urban%2520intersection%252C%2520a%2520federal%2520highway%252C%2520and%2520a%2520suburban%250Aintersection.%2520Our%25203D%2520trajectory%2520dataset%2520aims%2520to%2520enhance%2520autonomous%2520driving%250Asystems%2520by%2520providing%2520detailed%2520environmental%25203D%2520representations%252C%2520which%2520could%250Alead%2520to%2520improved%2520obstacle%2520interactions%2520and%2520safety.%2520We%2520demonstrate%2520its%2520utility%250Aacross%2520multiple%2520applications%2520including%2520motion%2520prediction%252C%2520motion%2520planning%252C%250Ascenario%2520mining%252C%2520and%2520generative%2520reactive%2520traffic%2520agents.%2520Our%2520interactive%2520online%250Avisualization%2520platform%2520and%2520the%2520complete%2520dataset%2520are%2520publicly%2520available%2520at%250Ahttps%253A//app.deepscenario.com%252C%2520facilitating%2520research%2520in%2520motion%2520prediction%252C%250Abehavior%2520modeling%252C%2520and%2520safety%2520validation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17371v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Highly%20Accurate%20and%20Diverse%20Traffic%20Data%3A%20The%20DeepScenario%20Open%203D%0A%20%20Dataset&entry.906535625=Oussema%20Dhaouadi%20and%20Johannes%20Meier%20and%20Luca%20Wahl%20and%20Jacques%20Kaiser%20and%20Luca%20Scalerandi%20and%20Nick%20Wandelburg%20and%20Zhuolun%20Zhou%20and%20Nijanthan%20Berinpanathan%20and%20Holger%20Banzhaf%20and%20Daniel%20Cremers&entry.1292438233=%20%20Accurate%203D%20trajectory%20data%20is%20crucial%20for%20advancing%20autonomous%20driving.%20Yet%2C%0Atraditional%20datasets%20are%20usually%20captured%20by%20fixed%20sensors%20mounted%20on%20a%20car%20and%0Aare%20susceptible%20to%20occlusion.%20Additionally%2C%20such%20an%20approach%20can%20precisely%0Areconstruct%20the%20dynamic%20environment%20in%20the%20close%20vicinity%20of%20the%20measurement%0Avehicle%20only%2C%20while%20neglecting%20objects%20that%20are%20further%20away.%20In%20this%20paper%2C%20we%0Aintroduce%20the%20DeepScenario%20Open%203D%20Dataset%20%28DSC3D%29%2C%20a%20high-quality%2C%0Aocclusion-free%20dataset%20of%206%20degrees%20of%20freedom%20bounding%20box%20trajectories%0Aacquired%20through%20a%20novel%20monocular%20camera%20drone%20tracking%20pipeline.%20Our%20dataset%0Aincludes%20more%20than%20175%2C000%20trajectories%20of%2014%20types%20of%20traffic%20participants%20and%0Asignificantly%20exceeds%20existing%20datasets%20in%20terms%20of%20diversity%20and%20scale%2C%0Acontaining%20many%20unprecedented%20scenarios%20such%20as%20complex%20vehicle-pedestrian%0Ainteraction%20on%20highly%20populated%20urban%20streets%20and%20comprehensive%20parking%0Amaneuvers%20from%20entry%20to%20exit.%20DSC3D%20dataset%20was%20captured%20in%20five%20various%0Alocations%20in%20Europe%20and%20the%20United%20States%20and%20include%3A%20a%20parking%20lot%2C%20a%20crowded%0Ainner-city%2C%20a%20steep%20urban%20intersection%2C%20a%20federal%20highway%2C%20and%20a%20suburban%0Aintersection.%20Our%203D%20trajectory%20dataset%20aims%20to%20enhance%20autonomous%20driving%0Asystems%20by%20providing%20detailed%20environmental%203D%20representations%2C%20which%20could%0Alead%20to%20improved%20obstacle%20interactions%20and%20safety.%20We%20demonstrate%20its%20utility%0Aacross%20multiple%20applications%20including%20motion%20prediction%2C%20motion%20planning%2C%0Ascenario%20mining%2C%20and%20generative%20reactive%20traffic%20agents.%20Our%20interactive%20online%0Avisualization%20platform%20and%20the%20complete%20dataset%20are%20publicly%20available%20at%0Ahttps%3A//app.deepscenario.com%2C%20facilitating%20research%20in%20motion%20prediction%2C%0Abehavior%20modeling%2C%20and%20safety%20validation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17371v3&entry.124074799=Read"},
{"title": "Enhanced Hybrid Technique for Efficient Digitization of Handwritten\n  Marksheets", "author": "Junaid Ahmed Sifat and Abir Chowdhury and Hasnat Md. Imtiaz and Md. Irtiza Hossain and Md. Imran Bin Azad", "abstract": "  The digitization of handwritten marksheets presents huge challenges due to\nthe different styles of handwriting and complex table structures in such\ndocuments like marksheets. This work introduces a hybrid method that integrates\nOpenCV for table detection and PaddleOCR for recognizing sequential handwritten\ntext. The image processing capabilities of OpenCV efficiently detects rows and\ncolumns which enable computationally lightweight and accurate table detection.\nAdditionally, YOLOv8 and Modified YOLOv8 are implemented for handwritten text\nrecognition within the detected table structures alongside PaddleOCR which\nfurther enhance the system's versatility. The proposed model achieves high\naccuracy on our custom dataset which is designed to represent different and\ndiverse handwriting styles and complex table layouts. Experimental results\ndemonstrate that YOLOv8 Modified achieves an accuracy of 92.72 percent,\noutperforming PaddleOCR 91.37 percent and the YOLOv8 model 88.91 percent. This\nefficiency reduces the necessity for manual work which makes this a practical\nand fast solution for digitizing academic as well as administrative documents.\nThis research serves the field of document automation, particularly handwritten\ndocument understanding, by providing operational and reliable methods to scale,\nenhance, and integrate the technologies involved.\n", "link": "http://arxiv.org/abs/2508.16295v1", "date": "2025-08-22", "relevancy": 2.3774, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4896}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4753}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4616}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Hybrid%20Technique%20for%20Efficient%20Digitization%20of%20Handwritten%0A%20%20Marksheets&body=Title%3A%20Enhanced%20Hybrid%20Technique%20for%20Efficient%20Digitization%20of%20Handwritten%0A%20%20Marksheets%0AAuthor%3A%20Junaid%20Ahmed%20Sifat%20and%20Abir%20Chowdhury%20and%20Hasnat%20Md.%20Imtiaz%20and%20Md.%20Irtiza%20Hossain%20and%20Md.%20Imran%20Bin%20Azad%0AAbstract%3A%20%20%20The%20digitization%20of%20handwritten%20marksheets%20presents%20huge%20challenges%20due%20to%0Athe%20different%20styles%20of%20handwriting%20and%20complex%20table%20structures%20in%20such%0Adocuments%20like%20marksheets.%20This%20work%20introduces%20a%20hybrid%20method%20that%20integrates%0AOpenCV%20for%20table%20detection%20and%20PaddleOCR%20for%20recognizing%20sequential%20handwritten%0Atext.%20The%20image%20processing%20capabilities%20of%20OpenCV%20efficiently%20detects%20rows%20and%0Acolumns%20which%20enable%20computationally%20lightweight%20and%20accurate%20table%20detection.%0AAdditionally%2C%20YOLOv8%20and%20Modified%20YOLOv8%20are%20implemented%20for%20handwritten%20text%0Arecognition%20within%20the%20detected%20table%20structures%20alongside%20PaddleOCR%20which%0Afurther%20enhance%20the%20system%27s%20versatility.%20The%20proposed%20model%20achieves%20high%0Aaccuracy%20on%20our%20custom%20dataset%20which%20is%20designed%20to%20represent%20different%20and%0Adiverse%20handwriting%20styles%20and%20complex%20table%20layouts.%20Experimental%20results%0Ademonstrate%20that%20YOLOv8%20Modified%20achieves%20an%20accuracy%20of%2092.72%20percent%2C%0Aoutperforming%20PaddleOCR%2091.37%20percent%20and%20the%20YOLOv8%20model%2088.91%20percent.%20This%0Aefficiency%20reduces%20the%20necessity%20for%20manual%20work%20which%20makes%20this%20a%20practical%0Aand%20fast%20solution%20for%20digitizing%20academic%20as%20well%20as%20administrative%20documents.%0AThis%20research%20serves%20the%20field%20of%20document%20automation%2C%20particularly%20handwritten%0Adocument%20understanding%2C%20by%20providing%20operational%20and%20reliable%20methods%20to%20scale%2C%0Aenhance%2C%20and%20integrate%20the%20technologies%20involved.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16295v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Hybrid%2520Technique%2520for%2520Efficient%2520Digitization%2520of%2520Handwritten%250A%2520%2520Marksheets%26entry.906535625%3DJunaid%2520Ahmed%2520Sifat%2520and%2520Abir%2520Chowdhury%2520and%2520Hasnat%2520Md.%2520Imtiaz%2520and%2520Md.%2520Irtiza%2520Hossain%2520and%2520Md.%2520Imran%2520Bin%2520Azad%26entry.1292438233%3D%2520%2520The%2520digitization%2520of%2520handwritten%2520marksheets%2520presents%2520huge%2520challenges%2520due%2520to%250Athe%2520different%2520styles%2520of%2520handwriting%2520and%2520complex%2520table%2520structures%2520in%2520such%250Adocuments%2520like%2520marksheets.%2520This%2520work%2520introduces%2520a%2520hybrid%2520method%2520that%2520integrates%250AOpenCV%2520for%2520table%2520detection%2520and%2520PaddleOCR%2520for%2520recognizing%2520sequential%2520handwritten%250Atext.%2520The%2520image%2520processing%2520capabilities%2520of%2520OpenCV%2520efficiently%2520detects%2520rows%2520and%250Acolumns%2520which%2520enable%2520computationally%2520lightweight%2520and%2520accurate%2520table%2520detection.%250AAdditionally%252C%2520YOLOv8%2520and%2520Modified%2520YOLOv8%2520are%2520implemented%2520for%2520handwritten%2520text%250Arecognition%2520within%2520the%2520detected%2520table%2520structures%2520alongside%2520PaddleOCR%2520which%250Afurther%2520enhance%2520the%2520system%2527s%2520versatility.%2520The%2520proposed%2520model%2520achieves%2520high%250Aaccuracy%2520on%2520our%2520custom%2520dataset%2520which%2520is%2520designed%2520to%2520represent%2520different%2520and%250Adiverse%2520handwriting%2520styles%2520and%2520complex%2520table%2520layouts.%2520Experimental%2520results%250Ademonstrate%2520that%2520YOLOv8%2520Modified%2520achieves%2520an%2520accuracy%2520of%252092.72%2520percent%252C%250Aoutperforming%2520PaddleOCR%252091.37%2520percent%2520and%2520the%2520YOLOv8%2520model%252088.91%2520percent.%2520This%250Aefficiency%2520reduces%2520the%2520necessity%2520for%2520manual%2520work%2520which%2520makes%2520this%2520a%2520practical%250Aand%2520fast%2520solution%2520for%2520digitizing%2520academic%2520as%2520well%2520as%2520administrative%2520documents.%250AThis%2520research%2520serves%2520the%2520field%2520of%2520document%2520automation%252C%2520particularly%2520handwritten%250Adocument%2520understanding%252C%2520by%2520providing%2520operational%2520and%2520reliable%2520methods%2520to%2520scale%252C%250Aenhance%252C%2520and%2520integrate%2520the%2520technologies%2520involved.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16295v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Hybrid%20Technique%20for%20Efficient%20Digitization%20of%20Handwritten%0A%20%20Marksheets&entry.906535625=Junaid%20Ahmed%20Sifat%20and%20Abir%20Chowdhury%20and%20Hasnat%20Md.%20Imtiaz%20and%20Md.%20Irtiza%20Hossain%20and%20Md.%20Imran%20Bin%20Azad&entry.1292438233=%20%20The%20digitization%20of%20handwritten%20marksheets%20presents%20huge%20challenges%20due%20to%0Athe%20different%20styles%20of%20handwriting%20and%20complex%20table%20structures%20in%20such%0Adocuments%20like%20marksheets.%20This%20work%20introduces%20a%20hybrid%20method%20that%20integrates%0AOpenCV%20for%20table%20detection%20and%20PaddleOCR%20for%20recognizing%20sequential%20handwritten%0Atext.%20The%20image%20processing%20capabilities%20of%20OpenCV%20efficiently%20detects%20rows%20and%0Acolumns%20which%20enable%20computationally%20lightweight%20and%20accurate%20table%20detection.%0AAdditionally%2C%20YOLOv8%20and%20Modified%20YOLOv8%20are%20implemented%20for%20handwritten%20text%0Arecognition%20within%20the%20detected%20table%20structures%20alongside%20PaddleOCR%20which%0Afurther%20enhance%20the%20system%27s%20versatility.%20The%20proposed%20model%20achieves%20high%0Aaccuracy%20on%20our%20custom%20dataset%20which%20is%20designed%20to%20represent%20different%20and%0Adiverse%20handwriting%20styles%20and%20complex%20table%20layouts.%20Experimental%20results%0Ademonstrate%20that%20YOLOv8%20Modified%20achieves%20an%20accuracy%20of%2092.72%20percent%2C%0Aoutperforming%20PaddleOCR%2091.37%20percent%20and%20the%20YOLOv8%20model%2088.91%20percent.%20This%0Aefficiency%20reduces%20the%20necessity%20for%20manual%20work%20which%20makes%20this%20a%20practical%0Aand%20fast%20solution%20for%20digitizing%20academic%20as%20well%20as%20administrative%20documents.%0AThis%20research%20serves%20the%20field%20of%20document%20automation%2C%20particularly%20handwritten%0Adocument%20understanding%2C%20by%20providing%20operational%20and%20reliable%20methods%20to%20scale%2C%0Aenhance%2C%20and%20integrate%20the%20technologies%20involved.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16295v1&entry.124074799=Read"},
{"title": "RL Is Neither a Panacea Nor a Mirage: Understanding Supervised vs.\n  Reinforcement Learning Fine-Tuning for LLMs", "author": "Hangzhan Jin and Sicheng Lv and Sifan Wu and Mohammad Hamdaqa", "abstract": "  Training large language models (LLMs) from scratch is increasingly\nimpractical, making post-training methods such as supervised fine-tuning (SFT)\nand reinforcement-learning fine-tuning (RL-FT, e.g., PPO) central to modern\npractice. Using an out-of-distribution (OOD) variant of the 24-point card game\nand new spectrum-based diagnostics, we revisit how these two stages reshape\nmodel representation and OOD performance. Our key findings are- (1) RL-FT can\nrestore much of the OOD performance loss from SFT (e.g., Llama-11B 8.97% to\n15.38%, Qwen-7B 17.09% to 19.66%). But when SFT induces severe overfitting and\na clear distribution shift, RL-FT cannot fully recover OOD performance. (2)\nDirection shifts of singular vectors matter more than singular value\nmagnitudes. These shifts concentrate on directions linked to the largest and\nsmallest singular values, leaving the bulk spectrum intact. (3) Low-rank and\nshallow recovery is effective: restoring singular vector directions for the top\n20% of values or first 25% of layers recovers 70-80% of OOD performance. (4)\nStronger SFT checkpoints enable better recovery by RL, while overfitted ones\nresist restoration. These results reconcile prior reports of RL superior OOD\nperformance: RL primarily counteracts SFT-induced directional drift rather than\nfinding new solutions. Our spectrum-aware analysis highlights inexpensive\nrecovery knobs low-rank UV merging and shallow-layer resets that practitioners\ncan use before costly RL fine-tuning.\n", "link": "http://arxiv.org/abs/2508.16546v1", "date": "2025-08-22", "relevancy": 2.37, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.476}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.476}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.47}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RL%20Is%20Neither%20a%20Panacea%20Nor%20a%20Mirage%3A%20Understanding%20Supervised%20vs.%0A%20%20Reinforcement%20Learning%20Fine-Tuning%20for%20LLMs&body=Title%3A%20RL%20Is%20Neither%20a%20Panacea%20Nor%20a%20Mirage%3A%20Understanding%20Supervised%20vs.%0A%20%20Reinforcement%20Learning%20Fine-Tuning%20for%20LLMs%0AAuthor%3A%20Hangzhan%20Jin%20and%20Sicheng%20Lv%20and%20Sifan%20Wu%20and%20Mohammad%20Hamdaqa%0AAbstract%3A%20%20%20Training%20large%20language%20models%20%28LLMs%29%20from%20scratch%20is%20increasingly%0Aimpractical%2C%20making%20post-training%20methods%20such%20as%20supervised%20fine-tuning%20%28SFT%29%0Aand%20reinforcement-learning%20fine-tuning%20%28RL-FT%2C%20e.g.%2C%20PPO%29%20central%20to%20modern%0Apractice.%20Using%20an%20out-of-distribution%20%28OOD%29%20variant%20of%20the%2024-point%20card%20game%0Aand%20new%20spectrum-based%20diagnostics%2C%20we%20revisit%20how%20these%20two%20stages%20reshape%0Amodel%20representation%20and%20OOD%20performance.%20Our%20key%20findings%20are-%20%281%29%20RL-FT%20can%0Arestore%20much%20of%20the%20OOD%20performance%20loss%20from%20SFT%20%28e.g.%2C%20Llama-11B%208.97%25%20to%0A15.38%25%2C%20Qwen-7B%2017.09%25%20to%2019.66%25%29.%20But%20when%20SFT%20induces%20severe%20overfitting%20and%0Aa%20clear%20distribution%20shift%2C%20RL-FT%20cannot%20fully%20recover%20OOD%20performance.%20%282%29%0ADirection%20shifts%20of%20singular%20vectors%20matter%20more%20than%20singular%20value%0Amagnitudes.%20These%20shifts%20concentrate%20on%20directions%20linked%20to%20the%20largest%20and%0Asmallest%20singular%20values%2C%20leaving%20the%20bulk%20spectrum%20intact.%20%283%29%20Low-rank%20and%0Ashallow%20recovery%20is%20effective%3A%20restoring%20singular%20vector%20directions%20for%20the%20top%0A20%25%20of%20values%20or%20first%2025%25%20of%20layers%20recovers%2070-80%25%20of%20OOD%20performance.%20%284%29%0AStronger%20SFT%20checkpoints%20enable%20better%20recovery%20by%20RL%2C%20while%20overfitted%20ones%0Aresist%20restoration.%20These%20results%20reconcile%20prior%20reports%20of%20RL%20superior%20OOD%0Aperformance%3A%20RL%20primarily%20counteracts%20SFT-induced%20directional%20drift%20rather%20than%0Afinding%20new%20solutions.%20Our%20spectrum-aware%20analysis%20highlights%20inexpensive%0Arecovery%20knobs%20low-rank%20UV%20merging%20and%20shallow-layer%20resets%20that%20practitioners%0Acan%20use%20before%20costly%20RL%20fine-tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16546v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRL%2520Is%2520Neither%2520a%2520Panacea%2520Nor%2520a%2520Mirage%253A%2520Understanding%2520Supervised%2520vs.%250A%2520%2520Reinforcement%2520Learning%2520Fine-Tuning%2520for%2520LLMs%26entry.906535625%3DHangzhan%2520Jin%2520and%2520Sicheng%2520Lv%2520and%2520Sifan%2520Wu%2520and%2520Mohammad%2520Hamdaqa%26entry.1292438233%3D%2520%2520Training%2520large%2520language%2520models%2520%2528LLMs%2529%2520from%2520scratch%2520is%2520increasingly%250Aimpractical%252C%2520making%2520post-training%2520methods%2520such%2520as%2520supervised%2520fine-tuning%2520%2528SFT%2529%250Aand%2520reinforcement-learning%2520fine-tuning%2520%2528RL-FT%252C%2520e.g.%252C%2520PPO%2529%2520central%2520to%2520modern%250Apractice.%2520Using%2520an%2520out-of-distribution%2520%2528OOD%2529%2520variant%2520of%2520the%252024-point%2520card%2520game%250Aand%2520new%2520spectrum-based%2520diagnostics%252C%2520we%2520revisit%2520how%2520these%2520two%2520stages%2520reshape%250Amodel%2520representation%2520and%2520OOD%2520performance.%2520Our%2520key%2520findings%2520are-%2520%25281%2529%2520RL-FT%2520can%250Arestore%2520much%2520of%2520the%2520OOD%2520performance%2520loss%2520from%2520SFT%2520%2528e.g.%252C%2520Llama-11B%25208.97%2525%2520to%250A15.38%2525%252C%2520Qwen-7B%252017.09%2525%2520to%252019.66%2525%2529.%2520But%2520when%2520SFT%2520induces%2520severe%2520overfitting%2520and%250Aa%2520clear%2520distribution%2520shift%252C%2520RL-FT%2520cannot%2520fully%2520recover%2520OOD%2520performance.%2520%25282%2529%250ADirection%2520shifts%2520of%2520singular%2520vectors%2520matter%2520more%2520than%2520singular%2520value%250Amagnitudes.%2520These%2520shifts%2520concentrate%2520on%2520directions%2520linked%2520to%2520the%2520largest%2520and%250Asmallest%2520singular%2520values%252C%2520leaving%2520the%2520bulk%2520spectrum%2520intact.%2520%25283%2529%2520Low-rank%2520and%250Ashallow%2520recovery%2520is%2520effective%253A%2520restoring%2520singular%2520vector%2520directions%2520for%2520the%2520top%250A20%2525%2520of%2520values%2520or%2520first%252025%2525%2520of%2520layers%2520recovers%252070-80%2525%2520of%2520OOD%2520performance.%2520%25284%2529%250AStronger%2520SFT%2520checkpoints%2520enable%2520better%2520recovery%2520by%2520RL%252C%2520while%2520overfitted%2520ones%250Aresist%2520restoration.%2520These%2520results%2520reconcile%2520prior%2520reports%2520of%2520RL%2520superior%2520OOD%250Aperformance%253A%2520RL%2520primarily%2520counteracts%2520SFT-induced%2520directional%2520drift%2520rather%2520than%250Afinding%2520new%2520solutions.%2520Our%2520spectrum-aware%2520analysis%2520highlights%2520inexpensive%250Arecovery%2520knobs%2520low-rank%2520UV%2520merging%2520and%2520shallow-layer%2520resets%2520that%2520practitioners%250Acan%2520use%2520before%2520costly%2520RL%2520fine-tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16546v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RL%20Is%20Neither%20a%20Panacea%20Nor%20a%20Mirage%3A%20Understanding%20Supervised%20vs.%0A%20%20Reinforcement%20Learning%20Fine-Tuning%20for%20LLMs&entry.906535625=Hangzhan%20Jin%20and%20Sicheng%20Lv%20and%20Sifan%20Wu%20and%20Mohammad%20Hamdaqa&entry.1292438233=%20%20Training%20large%20language%20models%20%28LLMs%29%20from%20scratch%20is%20increasingly%0Aimpractical%2C%20making%20post-training%20methods%20such%20as%20supervised%20fine-tuning%20%28SFT%29%0Aand%20reinforcement-learning%20fine-tuning%20%28RL-FT%2C%20e.g.%2C%20PPO%29%20central%20to%20modern%0Apractice.%20Using%20an%20out-of-distribution%20%28OOD%29%20variant%20of%20the%2024-point%20card%20game%0Aand%20new%20spectrum-based%20diagnostics%2C%20we%20revisit%20how%20these%20two%20stages%20reshape%0Amodel%20representation%20and%20OOD%20performance.%20Our%20key%20findings%20are-%20%281%29%20RL-FT%20can%0Arestore%20much%20of%20the%20OOD%20performance%20loss%20from%20SFT%20%28e.g.%2C%20Llama-11B%208.97%25%20to%0A15.38%25%2C%20Qwen-7B%2017.09%25%20to%2019.66%25%29.%20But%20when%20SFT%20induces%20severe%20overfitting%20and%0Aa%20clear%20distribution%20shift%2C%20RL-FT%20cannot%20fully%20recover%20OOD%20performance.%20%282%29%0ADirection%20shifts%20of%20singular%20vectors%20matter%20more%20than%20singular%20value%0Amagnitudes.%20These%20shifts%20concentrate%20on%20directions%20linked%20to%20the%20largest%20and%0Asmallest%20singular%20values%2C%20leaving%20the%20bulk%20spectrum%20intact.%20%283%29%20Low-rank%20and%0Ashallow%20recovery%20is%20effective%3A%20restoring%20singular%20vector%20directions%20for%20the%20top%0A20%25%20of%20values%20or%20first%2025%25%20of%20layers%20recovers%2070-80%25%20of%20OOD%20performance.%20%284%29%0AStronger%20SFT%20checkpoints%20enable%20better%20recovery%20by%20RL%2C%20while%20overfitted%20ones%0Aresist%20restoration.%20These%20results%20reconcile%20prior%20reports%20of%20RL%20superior%20OOD%0Aperformance%3A%20RL%20primarily%20counteracts%20SFT-induced%20directional%20drift%20rather%20than%0Afinding%20new%20solutions.%20Our%20spectrum-aware%20analysis%20highlights%20inexpensive%0Arecovery%20knobs%20low-rank%20UV%20merging%20and%20shallow-layer%20resets%20that%20practitioners%0Acan%20use%20before%20costly%20RL%20fine-tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16546v1&entry.124074799=Read"},
{"title": "Boardwalk: Towards a Framework for Creating Board Games with LLMs", "author": "\u00c1lvaro Guglielmin Becker and Gabriel Bauer de Oliveira and Lana Bertoldo Rossato and Anderson Rocha Tavares", "abstract": "  Implementing board games in code can be a time-consuming task. However, Large\nLanguage Models (LLMs) have been proven effective at generating code for\ndomain-specific tasks with simple contextual information. We aim to investigate\nwhether LLMs can implement digital versions of board games from rules described\nin natural language. This would be a step towards an LLM-assisted framework for\nquick board game code generation. We expect to determine the main challenges\nfor LLMs to implement the board games, and how different approaches and models\ncompare to one another. We task three state-of-the-art LLMs (Claude, DeepSeek\nand ChatGPT) with coding a selection of 12 popular and obscure games in\nfree-form and within Boardwalk, our proposed General Game Playing API. We\nanonymize the games and components to avoid evoking pre-trained LLM knowledge.\nThe implementations are tested for playability and rule compliance. We evaluate\nsuccess rate and common errors across LLMs and game popularity. Our approach\nproves viable, with the best performing model, Claude 3.7 Sonnet, yielding\n55.6\\% of games without any errors. While compliance with the API increases\nerror frequency, the severity of errors is more significantly dependent on the\nLLM. We outline future steps for creating a framework to integrate this\nprocess, making the elaboration of board games more accessible.\n", "link": "http://arxiv.org/abs/2508.16447v1", "date": "2025-08-22", "relevancy": 2.3525, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4706}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4705}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boardwalk%3A%20Towards%20a%20Framework%20for%20Creating%20Board%20Games%20with%20LLMs&body=Title%3A%20Boardwalk%3A%20Towards%20a%20Framework%20for%20Creating%20Board%20Games%20with%20LLMs%0AAuthor%3A%20%C3%81lvaro%20Guglielmin%20Becker%20and%20Gabriel%20Bauer%20de%20Oliveira%20and%20Lana%20Bertoldo%20Rossato%20and%20Anderson%20Rocha%20Tavares%0AAbstract%3A%20%20%20Implementing%20board%20games%20in%20code%20can%20be%20a%20time-consuming%20task.%20However%2C%20Large%0ALanguage%20Models%20%28LLMs%29%20have%20been%20proven%20effective%20at%20generating%20code%20for%0Adomain-specific%20tasks%20with%20simple%20contextual%20information.%20We%20aim%20to%20investigate%0Awhether%20LLMs%20can%20implement%20digital%20versions%20of%20board%20games%20from%20rules%20described%0Ain%20natural%20language.%20This%20would%20be%20a%20step%20towards%20an%20LLM-assisted%20framework%20for%0Aquick%20board%20game%20code%20generation.%20We%20expect%20to%20determine%20the%20main%20challenges%0Afor%20LLMs%20to%20implement%20the%20board%20games%2C%20and%20how%20different%20approaches%20and%20models%0Acompare%20to%20one%20another.%20We%20task%20three%20state-of-the-art%20LLMs%20%28Claude%2C%20DeepSeek%0Aand%20ChatGPT%29%20with%20coding%20a%20selection%20of%2012%20popular%20and%20obscure%20games%20in%0Afree-form%20and%20within%20Boardwalk%2C%20our%20proposed%20General%20Game%20Playing%20API.%20We%0Aanonymize%20the%20games%20and%20components%20to%20avoid%20evoking%20pre-trained%20LLM%20knowledge.%0AThe%20implementations%20are%20tested%20for%20playability%20and%20rule%20compliance.%20We%20evaluate%0Asuccess%20rate%20and%20common%20errors%20across%20LLMs%20and%20game%20popularity.%20Our%20approach%0Aproves%20viable%2C%20with%20the%20best%20performing%20model%2C%20Claude%203.7%20Sonnet%2C%20yielding%0A55.6%5C%25%20of%20games%20without%20any%20errors.%20While%20compliance%20with%20the%20API%20increases%0Aerror%20frequency%2C%20the%20severity%20of%20errors%20is%20more%20significantly%20dependent%20on%20the%0ALLM.%20We%20outline%20future%20steps%20for%20creating%20a%20framework%20to%20integrate%20this%0Aprocess%2C%20making%20the%20elaboration%20of%20board%20games%20more%20accessible.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16447v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoardwalk%253A%2520Towards%2520a%2520Framework%2520for%2520Creating%2520Board%2520Games%2520with%2520LLMs%26entry.906535625%3D%25C3%2581lvaro%2520Guglielmin%2520Becker%2520and%2520Gabriel%2520Bauer%2520de%2520Oliveira%2520and%2520Lana%2520Bertoldo%2520Rossato%2520and%2520Anderson%2520Rocha%2520Tavares%26entry.1292438233%3D%2520%2520Implementing%2520board%2520games%2520in%2520code%2520can%2520be%2520a%2520time-consuming%2520task.%2520However%252C%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520have%2520been%2520proven%2520effective%2520at%2520generating%2520code%2520for%250Adomain-specific%2520tasks%2520with%2520simple%2520contextual%2520information.%2520We%2520aim%2520to%2520investigate%250Awhether%2520LLMs%2520can%2520implement%2520digital%2520versions%2520of%2520board%2520games%2520from%2520rules%2520described%250Ain%2520natural%2520language.%2520This%2520would%2520be%2520a%2520step%2520towards%2520an%2520LLM-assisted%2520framework%2520for%250Aquick%2520board%2520game%2520code%2520generation.%2520We%2520expect%2520to%2520determine%2520the%2520main%2520challenges%250Afor%2520LLMs%2520to%2520implement%2520the%2520board%2520games%252C%2520and%2520how%2520different%2520approaches%2520and%2520models%250Acompare%2520to%2520one%2520another.%2520We%2520task%2520three%2520state-of-the-art%2520LLMs%2520%2528Claude%252C%2520DeepSeek%250Aand%2520ChatGPT%2529%2520with%2520coding%2520a%2520selection%2520of%252012%2520popular%2520and%2520obscure%2520games%2520in%250Afree-form%2520and%2520within%2520Boardwalk%252C%2520our%2520proposed%2520General%2520Game%2520Playing%2520API.%2520We%250Aanonymize%2520the%2520games%2520and%2520components%2520to%2520avoid%2520evoking%2520pre-trained%2520LLM%2520knowledge.%250AThe%2520implementations%2520are%2520tested%2520for%2520playability%2520and%2520rule%2520compliance.%2520We%2520evaluate%250Asuccess%2520rate%2520and%2520common%2520errors%2520across%2520LLMs%2520and%2520game%2520popularity.%2520Our%2520approach%250Aproves%2520viable%252C%2520with%2520the%2520best%2520performing%2520model%252C%2520Claude%25203.7%2520Sonnet%252C%2520yielding%250A55.6%255C%2525%2520of%2520games%2520without%2520any%2520errors.%2520While%2520compliance%2520with%2520the%2520API%2520increases%250Aerror%2520frequency%252C%2520the%2520severity%2520of%2520errors%2520is%2520more%2520significantly%2520dependent%2520on%2520the%250ALLM.%2520We%2520outline%2520future%2520steps%2520for%2520creating%2520a%2520framework%2520to%2520integrate%2520this%250Aprocess%252C%2520making%2520the%2520elaboration%2520of%2520board%2520games%2520more%2520accessible.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16447v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boardwalk%3A%20Towards%20a%20Framework%20for%20Creating%20Board%20Games%20with%20LLMs&entry.906535625=%C3%81lvaro%20Guglielmin%20Becker%20and%20Gabriel%20Bauer%20de%20Oliveira%20and%20Lana%20Bertoldo%20Rossato%20and%20Anderson%20Rocha%20Tavares&entry.1292438233=%20%20Implementing%20board%20games%20in%20code%20can%20be%20a%20time-consuming%20task.%20However%2C%20Large%0ALanguage%20Models%20%28LLMs%29%20have%20been%20proven%20effective%20at%20generating%20code%20for%0Adomain-specific%20tasks%20with%20simple%20contextual%20information.%20We%20aim%20to%20investigate%0Awhether%20LLMs%20can%20implement%20digital%20versions%20of%20board%20games%20from%20rules%20described%0Ain%20natural%20language.%20This%20would%20be%20a%20step%20towards%20an%20LLM-assisted%20framework%20for%0Aquick%20board%20game%20code%20generation.%20We%20expect%20to%20determine%20the%20main%20challenges%0Afor%20LLMs%20to%20implement%20the%20board%20games%2C%20and%20how%20different%20approaches%20and%20models%0Acompare%20to%20one%20another.%20We%20task%20three%20state-of-the-art%20LLMs%20%28Claude%2C%20DeepSeek%0Aand%20ChatGPT%29%20with%20coding%20a%20selection%20of%2012%20popular%20and%20obscure%20games%20in%0Afree-form%20and%20within%20Boardwalk%2C%20our%20proposed%20General%20Game%20Playing%20API.%20We%0Aanonymize%20the%20games%20and%20components%20to%20avoid%20evoking%20pre-trained%20LLM%20knowledge.%0AThe%20implementations%20are%20tested%20for%20playability%20and%20rule%20compliance.%20We%20evaluate%0Asuccess%20rate%20and%20common%20errors%20across%20LLMs%20and%20game%20popularity.%20Our%20approach%0Aproves%20viable%2C%20with%20the%20best%20performing%20model%2C%20Claude%203.7%20Sonnet%2C%20yielding%0A55.6%5C%25%20of%20games%20without%20any%20errors.%20While%20compliance%20with%20the%20API%20increases%0Aerror%20frequency%2C%20the%20severity%20of%20errors%20is%20more%20significantly%20dependent%20on%20the%0ALLM.%20We%20outline%20future%20steps%20for%20creating%20a%20framework%20to%20integrate%20this%0Aprocess%2C%20making%20the%20elaboration%20of%20board%20games%20more%20accessible.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16447v1&entry.124074799=Read"},
{"title": "from Benign import Toxic: Jailbreaking the Language Model via\n  Adversarial Metaphors", "author": "Yu Yan and Sheng Sun and Zenghao Duan and Teli Liu and Min Liu and Zhiyi Yin and Jiangyu Lei and Qi Li", "abstract": "  Current studies have exposed the risk of Large Language Models (LLMs)\ngenerating harmful content by jailbreak attacks. However, they overlook that\nthe direct generation of harmful content from scratch is more difficult than\ninducing LLM to calibrate benign content into harmful forms. In our study, we\nintroduce a novel attack framework that exploits AdVersArial meTAphoR (AVATAR)\nto induce the LLM to calibrate malicious metaphors for jailbreaking.\nSpecifically, to answer harmful queries, AVATAR adaptively identifies a set of\nbenign but logically related metaphors as the initial seed. Then, driven by\nthese metaphors, the target LLM is induced to reason and calibrate about the\nmetaphorical content, thus jailbroken by either directly outputting harmful\nresponses or calibrating residuals between metaphorical and professional\nharmful content. Experimental results demonstrate that AVATAR can effectively\nand transferable jailbreak LLMs and achieve a state-of-the-art attack success\nrate across multiple advanced LLMs.\n", "link": "http://arxiv.org/abs/2503.00038v4", "date": "2025-08-22", "relevancy": 2.294, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4596}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4596}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20from%20Benign%20import%20Toxic%3A%20Jailbreaking%20the%20Language%20Model%20via%0A%20%20Adversarial%20Metaphors&body=Title%3A%20from%20Benign%20import%20Toxic%3A%20Jailbreaking%20the%20Language%20Model%20via%0A%20%20Adversarial%20Metaphors%0AAuthor%3A%20Yu%20Yan%20and%20Sheng%20Sun%20and%20Zenghao%20Duan%20and%20Teli%20Liu%20and%20Min%20Liu%20and%20Zhiyi%20Yin%20and%20Jiangyu%20Lei%20and%20Qi%20Li%0AAbstract%3A%20%20%20Current%20studies%20have%20exposed%20the%20risk%20of%20Large%20Language%20Models%20%28LLMs%29%0Agenerating%20harmful%20content%20by%20jailbreak%20attacks.%20However%2C%20they%20overlook%20that%0Athe%20direct%20generation%20of%20harmful%20content%20from%20scratch%20is%20more%20difficult%20than%0Ainducing%20LLM%20to%20calibrate%20benign%20content%20into%20harmful%20forms.%20In%20our%20study%2C%20we%0Aintroduce%20a%20novel%20attack%20framework%20that%20exploits%20AdVersArial%20meTAphoR%20%28AVATAR%29%0Ato%20induce%20the%20LLM%20to%20calibrate%20malicious%20metaphors%20for%20jailbreaking.%0ASpecifically%2C%20to%20answer%20harmful%20queries%2C%20AVATAR%20adaptively%20identifies%20a%20set%20of%0Abenign%20but%20logically%20related%20metaphors%20as%20the%20initial%20seed.%20Then%2C%20driven%20by%0Athese%20metaphors%2C%20the%20target%20LLM%20is%20induced%20to%20reason%20and%20calibrate%20about%20the%0Ametaphorical%20content%2C%20thus%20jailbroken%20by%20either%20directly%20outputting%20harmful%0Aresponses%20or%20calibrating%20residuals%20between%20metaphorical%20and%20professional%0Aharmful%20content.%20Experimental%20results%20demonstrate%20that%20AVATAR%20can%20effectively%0Aand%20transferable%20jailbreak%20LLMs%20and%20achieve%20a%20state-of-the-art%20attack%20success%0Arate%20across%20multiple%20advanced%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.00038v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dfrom%2520Benign%2520import%2520Toxic%253A%2520Jailbreaking%2520the%2520Language%2520Model%2520via%250A%2520%2520Adversarial%2520Metaphors%26entry.906535625%3DYu%2520Yan%2520and%2520Sheng%2520Sun%2520and%2520Zenghao%2520Duan%2520and%2520Teli%2520Liu%2520and%2520Min%2520Liu%2520and%2520Zhiyi%2520Yin%2520and%2520Jiangyu%2520Lei%2520and%2520Qi%2520Li%26entry.1292438233%3D%2520%2520Current%2520studies%2520have%2520exposed%2520the%2520risk%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%250Agenerating%2520harmful%2520content%2520by%2520jailbreak%2520attacks.%2520However%252C%2520they%2520overlook%2520that%250Athe%2520direct%2520generation%2520of%2520harmful%2520content%2520from%2520scratch%2520is%2520more%2520difficult%2520than%250Ainducing%2520LLM%2520to%2520calibrate%2520benign%2520content%2520into%2520harmful%2520forms.%2520In%2520our%2520study%252C%2520we%250Aintroduce%2520a%2520novel%2520attack%2520framework%2520that%2520exploits%2520AdVersArial%2520meTAphoR%2520%2528AVATAR%2529%250Ato%2520induce%2520the%2520LLM%2520to%2520calibrate%2520malicious%2520metaphors%2520for%2520jailbreaking.%250ASpecifically%252C%2520to%2520answer%2520harmful%2520queries%252C%2520AVATAR%2520adaptively%2520identifies%2520a%2520set%2520of%250Abenign%2520but%2520logically%2520related%2520metaphors%2520as%2520the%2520initial%2520seed.%2520Then%252C%2520driven%2520by%250Athese%2520metaphors%252C%2520the%2520target%2520LLM%2520is%2520induced%2520to%2520reason%2520and%2520calibrate%2520about%2520the%250Ametaphorical%2520content%252C%2520thus%2520jailbroken%2520by%2520either%2520directly%2520outputting%2520harmful%250Aresponses%2520or%2520calibrating%2520residuals%2520between%2520metaphorical%2520and%2520professional%250Aharmful%2520content.%2520Experimental%2520results%2520demonstrate%2520that%2520AVATAR%2520can%2520effectively%250Aand%2520transferable%2520jailbreak%2520LLMs%2520and%2520achieve%2520a%2520state-of-the-art%2520attack%2520success%250Arate%2520across%2520multiple%2520advanced%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.00038v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=from%20Benign%20import%20Toxic%3A%20Jailbreaking%20the%20Language%20Model%20via%0A%20%20Adversarial%20Metaphors&entry.906535625=Yu%20Yan%20and%20Sheng%20Sun%20and%20Zenghao%20Duan%20and%20Teli%20Liu%20and%20Min%20Liu%20and%20Zhiyi%20Yin%20and%20Jiangyu%20Lei%20and%20Qi%20Li&entry.1292438233=%20%20Current%20studies%20have%20exposed%20the%20risk%20of%20Large%20Language%20Models%20%28LLMs%29%0Agenerating%20harmful%20content%20by%20jailbreak%20attacks.%20However%2C%20they%20overlook%20that%0Athe%20direct%20generation%20of%20harmful%20content%20from%20scratch%20is%20more%20difficult%20than%0Ainducing%20LLM%20to%20calibrate%20benign%20content%20into%20harmful%20forms.%20In%20our%20study%2C%20we%0Aintroduce%20a%20novel%20attack%20framework%20that%20exploits%20AdVersArial%20meTAphoR%20%28AVATAR%29%0Ato%20induce%20the%20LLM%20to%20calibrate%20malicious%20metaphors%20for%20jailbreaking.%0ASpecifically%2C%20to%20answer%20harmful%20queries%2C%20AVATAR%20adaptively%20identifies%20a%20set%20of%0Abenign%20but%20logically%20related%20metaphors%20as%20the%20initial%20seed.%20Then%2C%20driven%20by%0Athese%20metaphors%2C%20the%20target%20LLM%20is%20induced%20to%20reason%20and%20calibrate%20about%20the%0Ametaphorical%20content%2C%20thus%20jailbroken%20by%20either%20directly%20outputting%20harmful%0Aresponses%20or%20calibrating%20residuals%20between%20metaphorical%20and%20professional%0Aharmful%20content.%20Experimental%20results%20demonstrate%20that%20AVATAR%20can%20effectively%0Aand%20transferable%20jailbreak%20LLMs%20and%20achieve%20a%20state-of-the-art%20attack%20success%0Arate%20across%20multiple%20advanced%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.00038v4&entry.124074799=Read"},
{"title": "Beyond Interpretability: Exploring the Comprehensibility of Adaptive\n  Video Streaming through Large Language Models", "author": "Lianchen Jia and Chaoyang Li and Ziqi Yuan and Jiahui Chen and Tianchi Huang and Jiangchuan Liu and Lifeng Sun", "abstract": "  Over the past decade, adaptive video streaming technology has witnessed\nsignificant advancements, particularly driven by the rapid evolution of deep\nlearning techniques. However, the black-box nature of deep learning algorithms\npresents challenges for developers in understanding decision-making processes\nand optimizing for specific application scenarios. Although existing research\nhas enhanced algorithm interpretability through decision tree conversion,\ninterpretability does not directly equate to developers' subjective\ncomprehensibility. To address this challenge, we introduce \\texttt{ComTree},\nthe first bitrate adaptation algorithm generation framework that considers\ncomprehensibility. The framework initially generates the complete set of\ndecision trees that meet performance requirements, then leverages large\nlanguage models to evaluate these trees for developer comprehensibility,\nultimately selecting solutions that best facilitate human understanding and\nenhancement. Experimental results demonstrate that \\texttt{ComTree}\nsignificantly improves comprehensibility while maintaining competitive\nperformance, showing potential for further advancement. The source code is\navailable at https://github.com/thu-media/ComTree.\n", "link": "http://arxiv.org/abs/2508.16448v1", "date": "2025-08-22", "relevancy": 2.2628, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.57}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.57}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5443}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Interpretability%3A%20Exploring%20the%20Comprehensibility%20of%20Adaptive%0A%20%20Video%20Streaming%20through%20Large%20Language%20Models&body=Title%3A%20Beyond%20Interpretability%3A%20Exploring%20the%20Comprehensibility%20of%20Adaptive%0A%20%20Video%20Streaming%20through%20Large%20Language%20Models%0AAuthor%3A%20Lianchen%20Jia%20and%20Chaoyang%20Li%20and%20Ziqi%20Yuan%20and%20Jiahui%20Chen%20and%20Tianchi%20Huang%20and%20Jiangchuan%20Liu%20and%20Lifeng%20Sun%0AAbstract%3A%20%20%20Over%20the%20past%20decade%2C%20adaptive%20video%20streaming%20technology%20has%20witnessed%0Asignificant%20advancements%2C%20particularly%20driven%20by%20the%20rapid%20evolution%20of%20deep%0Alearning%20techniques.%20However%2C%20the%20black-box%20nature%20of%20deep%20learning%20algorithms%0Apresents%20challenges%20for%20developers%20in%20understanding%20decision-making%20processes%0Aand%20optimizing%20for%20specific%20application%20scenarios.%20Although%20existing%20research%0Ahas%20enhanced%20algorithm%20interpretability%20through%20decision%20tree%20conversion%2C%0Ainterpretability%20does%20not%20directly%20equate%20to%20developers%27%20subjective%0Acomprehensibility.%20To%20address%20this%20challenge%2C%20we%20introduce%20%5Ctexttt%7BComTree%7D%2C%0Athe%20first%20bitrate%20adaptation%20algorithm%20generation%20framework%20that%20considers%0Acomprehensibility.%20The%20framework%20initially%20generates%20the%20complete%20set%20of%0Adecision%20trees%20that%20meet%20performance%20requirements%2C%20then%20leverages%20large%0Alanguage%20models%20to%20evaluate%20these%20trees%20for%20developer%20comprehensibility%2C%0Aultimately%20selecting%20solutions%20that%20best%20facilitate%20human%20understanding%20and%0Aenhancement.%20Experimental%20results%20demonstrate%20that%20%5Ctexttt%7BComTree%7D%0Asignificantly%20improves%20comprehensibility%20while%20maintaining%20competitive%0Aperformance%2C%20showing%20potential%20for%20further%20advancement.%20The%20source%20code%20is%0Aavailable%20at%20https%3A//github.com/thu-media/ComTree.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16448v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Interpretability%253A%2520Exploring%2520the%2520Comprehensibility%2520of%2520Adaptive%250A%2520%2520Video%2520Streaming%2520through%2520Large%2520Language%2520Models%26entry.906535625%3DLianchen%2520Jia%2520and%2520Chaoyang%2520Li%2520and%2520Ziqi%2520Yuan%2520and%2520Jiahui%2520Chen%2520and%2520Tianchi%2520Huang%2520and%2520Jiangchuan%2520Liu%2520and%2520Lifeng%2520Sun%26entry.1292438233%3D%2520%2520Over%2520the%2520past%2520decade%252C%2520adaptive%2520video%2520streaming%2520technology%2520has%2520witnessed%250Asignificant%2520advancements%252C%2520particularly%2520driven%2520by%2520the%2520rapid%2520evolution%2520of%2520deep%250Alearning%2520techniques.%2520However%252C%2520the%2520black-box%2520nature%2520of%2520deep%2520learning%2520algorithms%250Apresents%2520challenges%2520for%2520developers%2520in%2520understanding%2520decision-making%2520processes%250Aand%2520optimizing%2520for%2520specific%2520application%2520scenarios.%2520Although%2520existing%2520research%250Ahas%2520enhanced%2520algorithm%2520interpretability%2520through%2520decision%2520tree%2520conversion%252C%250Ainterpretability%2520does%2520not%2520directly%2520equate%2520to%2520developers%2527%2520subjective%250Acomprehensibility.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520%255Ctexttt%257BComTree%257D%252C%250Athe%2520first%2520bitrate%2520adaptation%2520algorithm%2520generation%2520framework%2520that%2520considers%250Acomprehensibility.%2520The%2520framework%2520initially%2520generates%2520the%2520complete%2520set%2520of%250Adecision%2520trees%2520that%2520meet%2520performance%2520requirements%252C%2520then%2520leverages%2520large%250Alanguage%2520models%2520to%2520evaluate%2520these%2520trees%2520for%2520developer%2520comprehensibility%252C%250Aultimately%2520selecting%2520solutions%2520that%2520best%2520facilitate%2520human%2520understanding%2520and%250Aenhancement.%2520Experimental%2520results%2520demonstrate%2520that%2520%255Ctexttt%257BComTree%257D%250Asignificantly%2520improves%2520comprehensibility%2520while%2520maintaining%2520competitive%250Aperformance%252C%2520showing%2520potential%2520for%2520further%2520advancement.%2520The%2520source%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/thu-media/ComTree.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16448v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Interpretability%3A%20Exploring%20the%20Comprehensibility%20of%20Adaptive%0A%20%20Video%20Streaming%20through%20Large%20Language%20Models&entry.906535625=Lianchen%20Jia%20and%20Chaoyang%20Li%20and%20Ziqi%20Yuan%20and%20Jiahui%20Chen%20and%20Tianchi%20Huang%20and%20Jiangchuan%20Liu%20and%20Lifeng%20Sun&entry.1292438233=%20%20Over%20the%20past%20decade%2C%20adaptive%20video%20streaming%20technology%20has%20witnessed%0Asignificant%20advancements%2C%20particularly%20driven%20by%20the%20rapid%20evolution%20of%20deep%0Alearning%20techniques.%20However%2C%20the%20black-box%20nature%20of%20deep%20learning%20algorithms%0Apresents%20challenges%20for%20developers%20in%20understanding%20decision-making%20processes%0Aand%20optimizing%20for%20specific%20application%20scenarios.%20Although%20existing%20research%0Ahas%20enhanced%20algorithm%20interpretability%20through%20decision%20tree%20conversion%2C%0Ainterpretability%20does%20not%20directly%20equate%20to%20developers%27%20subjective%0Acomprehensibility.%20To%20address%20this%20challenge%2C%20we%20introduce%20%5Ctexttt%7BComTree%7D%2C%0Athe%20first%20bitrate%20adaptation%20algorithm%20generation%20framework%20that%20considers%0Acomprehensibility.%20The%20framework%20initially%20generates%20the%20complete%20set%20of%0Adecision%20trees%20that%20meet%20performance%20requirements%2C%20then%20leverages%20large%0Alanguage%20models%20to%20evaluate%20these%20trees%20for%20developer%20comprehensibility%2C%0Aultimately%20selecting%20solutions%20that%20best%20facilitate%20human%20understanding%20and%0Aenhancement.%20Experimental%20results%20demonstrate%20that%20%5Ctexttt%7BComTree%7D%0Asignificantly%20improves%20comprehensibility%20while%20maintaining%20competitive%0Aperformance%2C%20showing%20potential%20for%20further%20advancement.%20The%20source%20code%20is%0Aavailable%20at%20https%3A//github.com/thu-media/ComTree.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16448v1&entry.124074799=Read"},
{"title": "VERUS-LM: a Versatile Framework for Combining LLMs with Symbolic\n  Reasoning", "author": "Benjamin Callewaert and Simon Vandevelde and Joost Vennekens", "abstract": "  A recent approach to neurosymbolic reasoning is to explicitly combine the\nstrengths of large language models (LLMs) and symbolic solvers to tackle\ncomplex reasoning tasks. However, current approaches face significant\nlimitations, including poor generalizability due to task-specific prompts,\ninefficiencies caused by the lack of separation between knowledge and queries,\nand restricted inferential capabilities. These shortcomings hinder their\nscalability and applicability across diverse domains. In this paper, we\nintroduce VERUS-LM, a novel framework designed to address these challenges.\nVERUS-LM employs a generic prompting mechanism, clearly separates domain\nknowledge from queries, and supports a wide range of different logical\nreasoning tasks. This framework enhances adaptability, reduces computational\ncost, and allows for richer forms of reasoning, such as optimization and\nconstraint satisfaction. We show that our approach succeeds in diverse\nreasoning on a novel dataset, markedly outperforming LLMs. Additionally, our\nsystem achieves competitive results on common reasoning benchmarks when\ncompared to other state-of-the-art approaches, and significantly surpasses them\non the difficult AR-LSAT dataset. By pushing the boundaries of hybrid\nreasoning, VERUS-LM represents a significant step towards more versatile\nneurosymbolic AI systems\n", "link": "http://arxiv.org/abs/2501.14540v2", "date": "2025-08-22", "relevancy": 2.2615, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.576}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.576}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VERUS-LM%3A%20a%20Versatile%20Framework%20for%20Combining%20LLMs%20with%20Symbolic%0A%20%20Reasoning&body=Title%3A%20VERUS-LM%3A%20a%20Versatile%20Framework%20for%20Combining%20LLMs%20with%20Symbolic%0A%20%20Reasoning%0AAuthor%3A%20Benjamin%20Callewaert%20and%20Simon%20Vandevelde%20and%20Joost%20Vennekens%0AAbstract%3A%20%20%20A%20recent%20approach%20to%20neurosymbolic%20reasoning%20is%20to%20explicitly%20combine%20the%0Astrengths%20of%20large%20language%20models%20%28LLMs%29%20and%20symbolic%20solvers%20to%20tackle%0Acomplex%20reasoning%20tasks.%20However%2C%20current%20approaches%20face%20significant%0Alimitations%2C%20including%20poor%20generalizability%20due%20to%20task-specific%20prompts%2C%0Ainefficiencies%20caused%20by%20the%20lack%20of%20separation%20between%20knowledge%20and%20queries%2C%0Aand%20restricted%20inferential%20capabilities.%20These%20shortcomings%20hinder%20their%0Ascalability%20and%20applicability%20across%20diverse%20domains.%20In%20this%20paper%2C%20we%0Aintroduce%20VERUS-LM%2C%20a%20novel%20framework%20designed%20to%20address%20these%20challenges.%0AVERUS-LM%20employs%20a%20generic%20prompting%20mechanism%2C%20clearly%20separates%20domain%0Aknowledge%20from%20queries%2C%20and%20supports%20a%20wide%20range%20of%20different%20logical%0Areasoning%20tasks.%20This%20framework%20enhances%20adaptability%2C%20reduces%20computational%0Acost%2C%20and%20allows%20for%20richer%20forms%20of%20reasoning%2C%20such%20as%20optimization%20and%0Aconstraint%20satisfaction.%20We%20show%20that%20our%20approach%20succeeds%20in%20diverse%0Areasoning%20on%20a%20novel%20dataset%2C%20markedly%20outperforming%20LLMs.%20Additionally%2C%20our%0Asystem%20achieves%20competitive%20results%20on%20common%20reasoning%20benchmarks%20when%0Acompared%20to%20other%20state-of-the-art%20approaches%2C%20and%20significantly%20surpasses%20them%0Aon%20the%20difficult%20AR-LSAT%20dataset.%20By%20pushing%20the%20boundaries%20of%20hybrid%0Areasoning%2C%20VERUS-LM%20represents%20a%20significant%20step%20towards%20more%20versatile%0Aneurosymbolic%20AI%20systems%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.14540v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVERUS-LM%253A%2520a%2520Versatile%2520Framework%2520for%2520Combining%2520LLMs%2520with%2520Symbolic%250A%2520%2520Reasoning%26entry.906535625%3DBenjamin%2520Callewaert%2520and%2520Simon%2520Vandevelde%2520and%2520Joost%2520Vennekens%26entry.1292438233%3D%2520%2520A%2520recent%2520approach%2520to%2520neurosymbolic%2520reasoning%2520is%2520to%2520explicitly%2520combine%2520the%250Astrengths%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%2520symbolic%2520solvers%2520to%2520tackle%250Acomplex%2520reasoning%2520tasks.%2520However%252C%2520current%2520approaches%2520face%2520significant%250Alimitations%252C%2520including%2520poor%2520generalizability%2520due%2520to%2520task-specific%2520prompts%252C%250Ainefficiencies%2520caused%2520by%2520the%2520lack%2520of%2520separation%2520between%2520knowledge%2520and%2520queries%252C%250Aand%2520restricted%2520inferential%2520capabilities.%2520These%2520shortcomings%2520hinder%2520their%250Ascalability%2520and%2520applicability%2520across%2520diverse%2520domains.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520VERUS-LM%252C%2520a%2520novel%2520framework%2520designed%2520to%2520address%2520these%2520challenges.%250AVERUS-LM%2520employs%2520a%2520generic%2520prompting%2520mechanism%252C%2520clearly%2520separates%2520domain%250Aknowledge%2520from%2520queries%252C%2520and%2520supports%2520a%2520wide%2520range%2520of%2520different%2520logical%250Areasoning%2520tasks.%2520This%2520framework%2520enhances%2520adaptability%252C%2520reduces%2520computational%250Acost%252C%2520and%2520allows%2520for%2520richer%2520forms%2520of%2520reasoning%252C%2520such%2520as%2520optimization%2520and%250Aconstraint%2520satisfaction.%2520We%2520show%2520that%2520our%2520approach%2520succeeds%2520in%2520diverse%250Areasoning%2520on%2520a%2520novel%2520dataset%252C%2520markedly%2520outperforming%2520LLMs.%2520Additionally%252C%2520our%250Asystem%2520achieves%2520competitive%2520results%2520on%2520common%2520reasoning%2520benchmarks%2520when%250Acompared%2520to%2520other%2520state-of-the-art%2520approaches%252C%2520and%2520significantly%2520surpasses%2520them%250Aon%2520the%2520difficult%2520AR-LSAT%2520dataset.%2520By%2520pushing%2520the%2520boundaries%2520of%2520hybrid%250Areasoning%252C%2520VERUS-LM%2520represents%2520a%2520significant%2520step%2520towards%2520more%2520versatile%250Aneurosymbolic%2520AI%2520systems%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.14540v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VERUS-LM%3A%20a%20Versatile%20Framework%20for%20Combining%20LLMs%20with%20Symbolic%0A%20%20Reasoning&entry.906535625=Benjamin%20Callewaert%20and%20Simon%20Vandevelde%20and%20Joost%20Vennekens&entry.1292438233=%20%20A%20recent%20approach%20to%20neurosymbolic%20reasoning%20is%20to%20explicitly%20combine%20the%0Astrengths%20of%20large%20language%20models%20%28LLMs%29%20and%20symbolic%20solvers%20to%20tackle%0Acomplex%20reasoning%20tasks.%20However%2C%20current%20approaches%20face%20significant%0Alimitations%2C%20including%20poor%20generalizability%20due%20to%20task-specific%20prompts%2C%0Ainefficiencies%20caused%20by%20the%20lack%20of%20separation%20between%20knowledge%20and%20queries%2C%0Aand%20restricted%20inferential%20capabilities.%20These%20shortcomings%20hinder%20their%0Ascalability%20and%20applicability%20across%20diverse%20domains.%20In%20this%20paper%2C%20we%0Aintroduce%20VERUS-LM%2C%20a%20novel%20framework%20designed%20to%20address%20these%20challenges.%0AVERUS-LM%20employs%20a%20generic%20prompting%20mechanism%2C%20clearly%20separates%20domain%0Aknowledge%20from%20queries%2C%20and%20supports%20a%20wide%20range%20of%20different%20logical%0Areasoning%20tasks.%20This%20framework%20enhances%20adaptability%2C%20reduces%20computational%0Acost%2C%20and%20allows%20for%20richer%20forms%20of%20reasoning%2C%20such%20as%20optimization%20and%0Aconstraint%20satisfaction.%20We%20show%20that%20our%20approach%20succeeds%20in%20diverse%0Areasoning%20on%20a%20novel%20dataset%2C%20markedly%20outperforming%20LLMs.%20Additionally%2C%20our%0Asystem%20achieves%20competitive%20results%20on%20common%20reasoning%20benchmarks%20when%0Acompared%20to%20other%20state-of-the-art%20approaches%2C%20and%20significantly%20surpasses%20them%0Aon%20the%20difficult%20AR-LSAT%20dataset.%20By%20pushing%20the%20boundaries%20of%20hybrid%0Areasoning%2C%20VERUS-LM%20represents%20a%20significant%20step%20towards%20more%20versatile%0Aneurosymbolic%20AI%20systems%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.14540v2&entry.124074799=Read"},
{"title": "SAMFusion: Sensor-Adaptive Multimodal Fusion for 3D Object Detection in\n  Adverse Weather", "author": "Edoardo Palladin and Roland Dietze and Praveen Narayanan and Mario Bijelic and Felix Heide", "abstract": "  Multimodal sensor fusion is an essential capability for autonomous robots,\nenabling object detection and decision-making in the presence of failing or\nuncertain inputs. While recent fusion methods excel in normal environmental\nconditions, these approaches fail in adverse weather, e.g., heavy fog, snow, or\nobstructions due to soiling. We introduce a novel multi-sensor fusion approach\ntailored to adverse weather conditions. In addition to fusing RGB and LiDAR\nsensors, which are employed in recent autonomous driving literature, our sensor\nfusion stack is also capable of learning from NIR gated camera and radar\nmodalities to tackle low light and inclement weather. We fuse multimodal sensor\ndata through attentive, depth-based blending schemes, with learned refinement\non the Bird's Eye View (BEV) plane to combine image and range features\neffectively. Our detections are predicted by a transformer decoder that weighs\nmodalities based on distance and visibility. We demonstrate that our method\nimproves the reliability of multimodal sensor fusion in autonomous vehicles\nunder challenging weather conditions, bridging the gap between ideal conditions\nand real-world edge cases. Our approach improves average precision by 17.2 AP\ncompared to the next best method for vulnerable pedestrians in long distances\nand challenging foggy scenes. Our project page is available at\nhttps://light.princeton.edu/samfusion/\n", "link": "http://arxiv.org/abs/2508.16408v1", "date": "2025-08-22", "relevancy": 2.2495, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6049}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5711}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAMFusion%3A%20Sensor-Adaptive%20Multimodal%20Fusion%20for%203D%20Object%20Detection%20in%0A%20%20Adverse%20Weather&body=Title%3A%20SAMFusion%3A%20Sensor-Adaptive%20Multimodal%20Fusion%20for%203D%20Object%20Detection%20in%0A%20%20Adverse%20Weather%0AAuthor%3A%20Edoardo%20Palladin%20and%20Roland%20Dietze%20and%20Praveen%20Narayanan%20and%20Mario%20Bijelic%20and%20Felix%20Heide%0AAbstract%3A%20%20%20Multimodal%20sensor%20fusion%20is%20an%20essential%20capability%20for%20autonomous%20robots%2C%0Aenabling%20object%20detection%20and%20decision-making%20in%20the%20presence%20of%20failing%20or%0Auncertain%20inputs.%20While%20recent%20fusion%20methods%20excel%20in%20normal%20environmental%0Aconditions%2C%20these%20approaches%20fail%20in%20adverse%20weather%2C%20e.g.%2C%20heavy%20fog%2C%20snow%2C%20or%0Aobstructions%20due%20to%20soiling.%20We%20introduce%20a%20novel%20multi-sensor%20fusion%20approach%0Atailored%20to%20adverse%20weather%20conditions.%20In%20addition%20to%20fusing%20RGB%20and%20LiDAR%0Asensors%2C%20which%20are%20employed%20in%20recent%20autonomous%20driving%20literature%2C%20our%20sensor%0Afusion%20stack%20is%20also%20capable%20of%20learning%20from%20NIR%20gated%20camera%20and%20radar%0Amodalities%20to%20tackle%20low%20light%20and%20inclement%20weather.%20We%20fuse%20multimodal%20sensor%0Adata%20through%20attentive%2C%20depth-based%20blending%20schemes%2C%20with%20learned%20refinement%0Aon%20the%20Bird%27s%20Eye%20View%20%28BEV%29%20plane%20to%20combine%20image%20and%20range%20features%0Aeffectively.%20Our%20detections%20are%20predicted%20by%20a%20transformer%20decoder%20that%20weighs%0Amodalities%20based%20on%20distance%20and%20visibility.%20We%20demonstrate%20that%20our%20method%0Aimproves%20the%20reliability%20of%20multimodal%20sensor%20fusion%20in%20autonomous%20vehicles%0Aunder%20challenging%20weather%20conditions%2C%20bridging%20the%20gap%20between%20ideal%20conditions%0Aand%20real-world%20edge%20cases.%20Our%20approach%20improves%20average%20precision%20by%2017.2%20AP%0Acompared%20to%20the%20next%20best%20method%20for%20vulnerable%20pedestrians%20in%20long%20distances%0Aand%20challenging%20foggy%20scenes.%20Our%20project%20page%20is%20available%20at%0Ahttps%3A//light.princeton.edu/samfusion/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16408v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAMFusion%253A%2520Sensor-Adaptive%2520Multimodal%2520Fusion%2520for%25203D%2520Object%2520Detection%2520in%250A%2520%2520Adverse%2520Weather%26entry.906535625%3DEdoardo%2520Palladin%2520and%2520Roland%2520Dietze%2520and%2520Praveen%2520Narayanan%2520and%2520Mario%2520Bijelic%2520and%2520Felix%2520Heide%26entry.1292438233%3D%2520%2520Multimodal%2520sensor%2520fusion%2520is%2520an%2520essential%2520capability%2520for%2520autonomous%2520robots%252C%250Aenabling%2520object%2520detection%2520and%2520decision-making%2520in%2520the%2520presence%2520of%2520failing%2520or%250Auncertain%2520inputs.%2520While%2520recent%2520fusion%2520methods%2520excel%2520in%2520normal%2520environmental%250Aconditions%252C%2520these%2520approaches%2520fail%2520in%2520adverse%2520weather%252C%2520e.g.%252C%2520heavy%2520fog%252C%2520snow%252C%2520or%250Aobstructions%2520due%2520to%2520soiling.%2520We%2520introduce%2520a%2520novel%2520multi-sensor%2520fusion%2520approach%250Atailored%2520to%2520adverse%2520weather%2520conditions.%2520In%2520addition%2520to%2520fusing%2520RGB%2520and%2520LiDAR%250Asensors%252C%2520which%2520are%2520employed%2520in%2520recent%2520autonomous%2520driving%2520literature%252C%2520our%2520sensor%250Afusion%2520stack%2520is%2520also%2520capable%2520of%2520learning%2520from%2520NIR%2520gated%2520camera%2520and%2520radar%250Amodalities%2520to%2520tackle%2520low%2520light%2520and%2520inclement%2520weather.%2520We%2520fuse%2520multimodal%2520sensor%250Adata%2520through%2520attentive%252C%2520depth-based%2520blending%2520schemes%252C%2520with%2520learned%2520refinement%250Aon%2520the%2520Bird%2527s%2520Eye%2520View%2520%2528BEV%2529%2520plane%2520to%2520combine%2520image%2520and%2520range%2520features%250Aeffectively.%2520Our%2520detections%2520are%2520predicted%2520by%2520a%2520transformer%2520decoder%2520that%2520weighs%250Amodalities%2520based%2520on%2520distance%2520and%2520visibility.%2520We%2520demonstrate%2520that%2520our%2520method%250Aimproves%2520the%2520reliability%2520of%2520multimodal%2520sensor%2520fusion%2520in%2520autonomous%2520vehicles%250Aunder%2520challenging%2520weather%2520conditions%252C%2520bridging%2520the%2520gap%2520between%2520ideal%2520conditions%250Aand%2520real-world%2520edge%2520cases.%2520Our%2520approach%2520improves%2520average%2520precision%2520by%252017.2%2520AP%250Acompared%2520to%2520the%2520next%2520best%2520method%2520for%2520vulnerable%2520pedestrians%2520in%2520long%2520distances%250Aand%2520challenging%2520foggy%2520scenes.%2520Our%2520project%2520page%2520is%2520available%2520at%250Ahttps%253A//light.princeton.edu/samfusion/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16408v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAMFusion%3A%20Sensor-Adaptive%20Multimodal%20Fusion%20for%203D%20Object%20Detection%20in%0A%20%20Adverse%20Weather&entry.906535625=Edoardo%20Palladin%20and%20Roland%20Dietze%20and%20Praveen%20Narayanan%20and%20Mario%20Bijelic%20and%20Felix%20Heide&entry.1292438233=%20%20Multimodal%20sensor%20fusion%20is%20an%20essential%20capability%20for%20autonomous%20robots%2C%0Aenabling%20object%20detection%20and%20decision-making%20in%20the%20presence%20of%20failing%20or%0Auncertain%20inputs.%20While%20recent%20fusion%20methods%20excel%20in%20normal%20environmental%0Aconditions%2C%20these%20approaches%20fail%20in%20adverse%20weather%2C%20e.g.%2C%20heavy%20fog%2C%20snow%2C%20or%0Aobstructions%20due%20to%20soiling.%20We%20introduce%20a%20novel%20multi-sensor%20fusion%20approach%0Atailored%20to%20adverse%20weather%20conditions.%20In%20addition%20to%20fusing%20RGB%20and%20LiDAR%0Asensors%2C%20which%20are%20employed%20in%20recent%20autonomous%20driving%20literature%2C%20our%20sensor%0Afusion%20stack%20is%20also%20capable%20of%20learning%20from%20NIR%20gated%20camera%20and%20radar%0Amodalities%20to%20tackle%20low%20light%20and%20inclement%20weather.%20We%20fuse%20multimodal%20sensor%0Adata%20through%20attentive%2C%20depth-based%20blending%20schemes%2C%20with%20learned%20refinement%0Aon%20the%20Bird%27s%20Eye%20View%20%28BEV%29%20plane%20to%20combine%20image%20and%20range%20features%0Aeffectively.%20Our%20detections%20are%20predicted%20by%20a%20transformer%20decoder%20that%20weighs%0Amodalities%20based%20on%20distance%20and%20visibility.%20We%20demonstrate%20that%20our%20method%0Aimproves%20the%20reliability%20of%20multimodal%20sensor%20fusion%20in%20autonomous%20vehicles%0Aunder%20challenging%20weather%20conditions%2C%20bridging%20the%20gap%20between%20ideal%20conditions%0Aand%20real-world%20edge%20cases.%20Our%20approach%20improves%20average%20precision%20by%2017.2%20AP%0Acompared%20to%20the%20next%20best%20method%20for%20vulnerable%20pedestrians%20in%20long%20distances%0Aand%20challenging%20foggy%20scenes.%20Our%20project%20page%20is%20available%20at%0Ahttps%3A//light.princeton.edu/samfusion/%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16408v1&entry.124074799=Read"},
{"title": "Parity-Aware Byte-Pair Encoding: Improving Cross-lingual Fairness in\n  Tokenization", "author": "Negar Foroutan and Clara Meister and Debjit Paul and Joel Niklaus and Sina Ahmadi and Antoine Bosselut and Rico Sennrich", "abstract": "  Tokenization is the first -- and often least scrutinized -- step of most NLP\npipelines. Standard algorithms for learning tokenizers rely on frequency-based\nobjectives, which favor languages dominant in the training data and\nconsequently leave lower-resource languages with tokenizations that are\ndisproportionately longer, morphologically implausible, or even riddled with\n<UNK> placeholders. This phenomenon ultimately amplifies computational and\nfinancial inequalities between users from different language backgrounds. To\nremedy this, we introduce Parity-aware Byte Pair Encoding (BPE), a variant of\nthe widely-used BPE algorithm. At every merge step, Parity-aware BPE maximizes\nthe compression gain of the currently worst-compressed language, trading a\nsmall amount of global compression for cross-lingual parity. We find\nempirically that Parity-aware BPE leads to more equitable token counts across\nlanguages, with negligible impact on global compression rate and no substantial\neffect on language-model performance in downstream tasks.\n", "link": "http://arxiv.org/abs/2508.04796v2", "date": "2025-08-22", "relevancy": 2.2375, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4612}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4422}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parity-Aware%20Byte-Pair%20Encoding%3A%20Improving%20Cross-lingual%20Fairness%20in%0A%20%20Tokenization&body=Title%3A%20Parity-Aware%20Byte-Pair%20Encoding%3A%20Improving%20Cross-lingual%20Fairness%20in%0A%20%20Tokenization%0AAuthor%3A%20Negar%20Foroutan%20and%20Clara%20Meister%20and%20Debjit%20Paul%20and%20Joel%20Niklaus%20and%20Sina%20Ahmadi%20and%20Antoine%20Bosselut%20and%20Rico%20Sennrich%0AAbstract%3A%20%20%20Tokenization%20is%20the%20first%20--%20and%20often%20least%20scrutinized%20--%20step%20of%20most%20NLP%0Apipelines.%20Standard%20algorithms%20for%20learning%20tokenizers%20rely%20on%20frequency-based%0Aobjectives%2C%20which%20favor%20languages%20dominant%20in%20the%20training%20data%20and%0Aconsequently%20leave%20lower-resource%20languages%20with%20tokenizations%20that%20are%0Adisproportionately%20longer%2C%20morphologically%20implausible%2C%20or%20even%20riddled%20with%0A%3CUNK%3E%20placeholders.%20This%20phenomenon%20ultimately%20amplifies%20computational%20and%0Afinancial%20inequalities%20between%20users%20from%20different%20language%20backgrounds.%20To%0Aremedy%20this%2C%20we%20introduce%20Parity-aware%20Byte%20Pair%20Encoding%20%28BPE%29%2C%20a%20variant%20of%0Athe%20widely-used%20BPE%20algorithm.%20At%20every%20merge%20step%2C%20Parity-aware%20BPE%20maximizes%0Athe%20compression%20gain%20of%20the%20currently%20worst-compressed%20language%2C%20trading%20a%0Asmall%20amount%20of%20global%20compression%20for%20cross-lingual%20parity.%20We%20find%0Aempirically%20that%20Parity-aware%20BPE%20leads%20to%20more%20equitable%20token%20counts%20across%0Alanguages%2C%20with%20negligible%20impact%20on%20global%20compression%20rate%20and%20no%20substantial%0Aeffect%20on%20language-model%20performance%20in%20downstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04796v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParity-Aware%2520Byte-Pair%2520Encoding%253A%2520Improving%2520Cross-lingual%2520Fairness%2520in%250A%2520%2520Tokenization%26entry.906535625%3DNegar%2520Foroutan%2520and%2520Clara%2520Meister%2520and%2520Debjit%2520Paul%2520and%2520Joel%2520Niklaus%2520and%2520Sina%2520Ahmadi%2520and%2520Antoine%2520Bosselut%2520and%2520Rico%2520Sennrich%26entry.1292438233%3D%2520%2520Tokenization%2520is%2520the%2520first%2520--%2520and%2520often%2520least%2520scrutinized%2520--%2520step%2520of%2520most%2520NLP%250Apipelines.%2520Standard%2520algorithms%2520for%2520learning%2520tokenizers%2520rely%2520on%2520frequency-based%250Aobjectives%252C%2520which%2520favor%2520languages%2520dominant%2520in%2520the%2520training%2520data%2520and%250Aconsequently%2520leave%2520lower-resource%2520languages%2520with%2520tokenizations%2520that%2520are%250Adisproportionately%2520longer%252C%2520morphologically%2520implausible%252C%2520or%2520even%2520riddled%2520with%250A%253CUNK%253E%2520placeholders.%2520This%2520phenomenon%2520ultimately%2520amplifies%2520computational%2520and%250Afinancial%2520inequalities%2520between%2520users%2520from%2520different%2520language%2520backgrounds.%2520To%250Aremedy%2520this%252C%2520we%2520introduce%2520Parity-aware%2520Byte%2520Pair%2520Encoding%2520%2528BPE%2529%252C%2520a%2520variant%2520of%250Athe%2520widely-used%2520BPE%2520algorithm.%2520At%2520every%2520merge%2520step%252C%2520Parity-aware%2520BPE%2520maximizes%250Athe%2520compression%2520gain%2520of%2520the%2520currently%2520worst-compressed%2520language%252C%2520trading%2520a%250Asmall%2520amount%2520of%2520global%2520compression%2520for%2520cross-lingual%2520parity.%2520We%2520find%250Aempirically%2520that%2520Parity-aware%2520BPE%2520leads%2520to%2520more%2520equitable%2520token%2520counts%2520across%250Alanguages%252C%2520with%2520negligible%2520impact%2520on%2520global%2520compression%2520rate%2520and%2520no%2520substantial%250Aeffect%2520on%2520language-model%2520performance%2520in%2520downstream%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04796v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parity-Aware%20Byte-Pair%20Encoding%3A%20Improving%20Cross-lingual%20Fairness%20in%0A%20%20Tokenization&entry.906535625=Negar%20Foroutan%20and%20Clara%20Meister%20and%20Debjit%20Paul%20and%20Joel%20Niklaus%20and%20Sina%20Ahmadi%20and%20Antoine%20Bosselut%20and%20Rico%20Sennrich&entry.1292438233=%20%20Tokenization%20is%20the%20first%20--%20and%20often%20least%20scrutinized%20--%20step%20of%20most%20NLP%0Apipelines.%20Standard%20algorithms%20for%20learning%20tokenizers%20rely%20on%20frequency-based%0Aobjectives%2C%20which%20favor%20languages%20dominant%20in%20the%20training%20data%20and%0Aconsequently%20leave%20lower-resource%20languages%20with%20tokenizations%20that%20are%0Adisproportionately%20longer%2C%20morphologically%20implausible%2C%20or%20even%20riddled%20with%0A%3CUNK%3E%20placeholders.%20This%20phenomenon%20ultimately%20amplifies%20computational%20and%0Afinancial%20inequalities%20between%20users%20from%20different%20language%20backgrounds.%20To%0Aremedy%20this%2C%20we%20introduce%20Parity-aware%20Byte%20Pair%20Encoding%20%28BPE%29%2C%20a%20variant%20of%0Athe%20widely-used%20BPE%20algorithm.%20At%20every%20merge%20step%2C%20Parity-aware%20BPE%20maximizes%0Athe%20compression%20gain%20of%20the%20currently%20worst-compressed%20language%2C%20trading%20a%0Asmall%20amount%20of%20global%20compression%20for%20cross-lingual%20parity.%20We%20find%0Aempirically%20that%20Parity-aware%20BPE%20leads%20to%20more%20equitable%20token%20counts%20across%0Alanguages%2C%20with%20negligible%20impact%20on%20global%20compression%20rate%20and%20no%20substantial%0Aeffect%20on%20language-model%20performance%20in%20downstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04796v2&entry.124074799=Read"},
{"title": "Sparse but Wrong: Incorrect L0 Leads to Incorrect Features in Sparse\n  Autoencoders", "author": "David Chanin and Adri\u00e0 Garriga-Alonso", "abstract": "  Sparse Autoencoders (SAEs) extract features from LLM internal activations,\nmeant to correspond to single concepts. A core SAE training hyperparameter is\nL0: how many features should fire per token on average. Existing work compares\nSAE algorithms using sparsity--reconstruction tradeoff plots, implying L0 is a\nfree parameter with no single correct value. In this work we study the effect\nof L0 on BatchTopK SAEs, and show that if L0 is not set precisely, the SAE\nfails to learn the underlying features of the LLM. If L0 is too low, the SAE\nwill mix correlated features to improve reconstruction. If L0 is too high, the\nSAE finds degenerate solutions that also mix features. Further, we demonstrate\na method to determine the correct L0 value for an SAE on a given training\ndistribution, which finds the true L0 in toy models and coincides with peak\nsparse probing performance in LLMs. We find that most commonly used SAEs have\nan L0 that is too low. Our work shows that, to train SAEs with correct\nfeatures, practitioners must set L0 correctly.\n", "link": "http://arxiv.org/abs/2508.16560v1", "date": "2025-08-22", "relevancy": 2.2338, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4623}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4431}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20but%20Wrong%3A%20Incorrect%20L0%20Leads%20to%20Incorrect%20Features%20in%20Sparse%0A%20%20Autoencoders&body=Title%3A%20Sparse%20but%20Wrong%3A%20Incorrect%20L0%20Leads%20to%20Incorrect%20Features%20in%20Sparse%0A%20%20Autoencoders%0AAuthor%3A%20David%20Chanin%20and%20Adri%C3%A0%20Garriga-Alonso%0AAbstract%3A%20%20%20Sparse%20Autoencoders%20%28SAEs%29%20extract%20features%20from%20LLM%20internal%20activations%2C%0Ameant%20to%20correspond%20to%20single%20concepts.%20A%20core%20SAE%20training%20hyperparameter%20is%0AL0%3A%20how%20many%20features%20should%20fire%20per%20token%20on%20average.%20Existing%20work%20compares%0ASAE%20algorithms%20using%20sparsity--reconstruction%20tradeoff%20plots%2C%20implying%20L0%20is%20a%0Afree%20parameter%20with%20no%20single%20correct%20value.%20In%20this%20work%20we%20study%20the%20effect%0Aof%20L0%20on%20BatchTopK%20SAEs%2C%20and%20show%20that%20if%20L0%20is%20not%20set%20precisely%2C%20the%20SAE%0Afails%20to%20learn%20the%20underlying%20features%20of%20the%20LLM.%20If%20L0%20is%20too%20low%2C%20the%20SAE%0Awill%20mix%20correlated%20features%20to%20improve%20reconstruction.%20If%20L0%20is%20too%20high%2C%20the%0ASAE%20finds%20degenerate%20solutions%20that%20also%20mix%20features.%20Further%2C%20we%20demonstrate%0Aa%20method%20to%20determine%20the%20correct%20L0%20value%20for%20an%20SAE%20on%20a%20given%20training%0Adistribution%2C%20which%20finds%20the%20true%20L0%20in%20toy%20models%20and%20coincides%20with%20peak%0Asparse%20probing%20performance%20in%20LLMs.%20We%20find%20that%20most%20commonly%20used%20SAEs%20have%0Aan%20L0%20that%20is%20too%20low.%20Our%20work%20shows%20that%2C%20to%20train%20SAEs%20with%20correct%0Afeatures%2C%20practitioners%20must%20set%20L0%20correctly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16560v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520but%2520Wrong%253A%2520Incorrect%2520L0%2520Leads%2520to%2520Incorrect%2520Features%2520in%2520Sparse%250A%2520%2520Autoencoders%26entry.906535625%3DDavid%2520Chanin%2520and%2520Adri%25C3%25A0%2520Garriga-Alonso%26entry.1292438233%3D%2520%2520Sparse%2520Autoencoders%2520%2528SAEs%2529%2520extract%2520features%2520from%2520LLM%2520internal%2520activations%252C%250Ameant%2520to%2520correspond%2520to%2520single%2520concepts.%2520A%2520core%2520SAE%2520training%2520hyperparameter%2520is%250AL0%253A%2520how%2520many%2520features%2520should%2520fire%2520per%2520token%2520on%2520average.%2520Existing%2520work%2520compares%250ASAE%2520algorithms%2520using%2520sparsity--reconstruction%2520tradeoff%2520plots%252C%2520implying%2520L0%2520is%2520a%250Afree%2520parameter%2520with%2520no%2520single%2520correct%2520value.%2520In%2520this%2520work%2520we%2520study%2520the%2520effect%250Aof%2520L0%2520on%2520BatchTopK%2520SAEs%252C%2520and%2520show%2520that%2520if%2520L0%2520is%2520not%2520set%2520precisely%252C%2520the%2520SAE%250Afails%2520to%2520learn%2520the%2520underlying%2520features%2520of%2520the%2520LLM.%2520If%2520L0%2520is%2520too%2520low%252C%2520the%2520SAE%250Awill%2520mix%2520correlated%2520features%2520to%2520improve%2520reconstruction.%2520If%2520L0%2520is%2520too%2520high%252C%2520the%250ASAE%2520finds%2520degenerate%2520solutions%2520that%2520also%2520mix%2520features.%2520Further%252C%2520we%2520demonstrate%250Aa%2520method%2520to%2520determine%2520the%2520correct%2520L0%2520value%2520for%2520an%2520SAE%2520on%2520a%2520given%2520training%250Adistribution%252C%2520which%2520finds%2520the%2520true%2520L0%2520in%2520toy%2520models%2520and%2520coincides%2520with%2520peak%250Asparse%2520probing%2520performance%2520in%2520LLMs.%2520We%2520find%2520that%2520most%2520commonly%2520used%2520SAEs%2520have%250Aan%2520L0%2520that%2520is%2520too%2520low.%2520Our%2520work%2520shows%2520that%252C%2520to%2520train%2520SAEs%2520with%2520correct%250Afeatures%252C%2520practitioners%2520must%2520set%2520L0%2520correctly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16560v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20but%20Wrong%3A%20Incorrect%20L0%20Leads%20to%20Incorrect%20Features%20in%20Sparse%0A%20%20Autoencoders&entry.906535625=David%20Chanin%20and%20Adri%C3%A0%20Garriga-Alonso&entry.1292438233=%20%20Sparse%20Autoencoders%20%28SAEs%29%20extract%20features%20from%20LLM%20internal%20activations%2C%0Ameant%20to%20correspond%20to%20single%20concepts.%20A%20core%20SAE%20training%20hyperparameter%20is%0AL0%3A%20how%20many%20features%20should%20fire%20per%20token%20on%20average.%20Existing%20work%20compares%0ASAE%20algorithms%20using%20sparsity--reconstruction%20tradeoff%20plots%2C%20implying%20L0%20is%20a%0Afree%20parameter%20with%20no%20single%20correct%20value.%20In%20this%20work%20we%20study%20the%20effect%0Aof%20L0%20on%20BatchTopK%20SAEs%2C%20and%20show%20that%20if%20L0%20is%20not%20set%20precisely%2C%20the%20SAE%0Afails%20to%20learn%20the%20underlying%20features%20of%20the%20LLM.%20If%20L0%20is%20too%20low%2C%20the%20SAE%0Awill%20mix%20correlated%20features%20to%20improve%20reconstruction.%20If%20L0%20is%20too%20high%2C%20the%0ASAE%20finds%20degenerate%20solutions%20that%20also%20mix%20features.%20Further%2C%20we%20demonstrate%0Aa%20method%20to%20determine%20the%20correct%20L0%20value%20for%20an%20SAE%20on%20a%20given%20training%0Adistribution%2C%20which%20finds%20the%20true%20L0%20in%20toy%20models%20and%20coincides%20with%20peak%0Asparse%20probing%20performance%20in%20LLMs.%20We%20find%20that%20most%20commonly%20used%20SAEs%20have%0Aan%20L0%20that%20is%20too%20low.%20Our%20work%20shows%20that%2C%20to%20train%20SAEs%20with%20correct%0Afeatures%2C%20practitioners%20must%20set%20L0%20correctly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16560v1&entry.124074799=Read"},
{"title": "Guiding Diffusion Models with Reinforcement Learning for Stable Molecule\n  Generation", "author": "Zhijian Zhou and Junyi An and Zongkai Liu and Yunfei Shi and Xuan Zhang and Fenglei Cao and Chao Qu and Yuan Qi", "abstract": "  Generating physically realistic 3D molecular structures remains a core\nchallenge in molecular generative modeling. While diffusion models equipped\nwith equivariant neural networks have made progress in capturing molecular\ngeometries, they often struggle to produce equilibrium structures that adhere\nto physical principles such as force field consistency. To bridge this gap, we\npropose Reinforcement Learning with Physical Feedback (RLPF), a novel framework\nthat extends Denoising Diffusion Policy Optimization to 3D molecular\ngeneration. RLPF formulates the task as a Markov decision process and applies\nproximal policy optimization to fine-tune equivariant diffusion models.\nCrucially, RLPF introduces reward functions derived from force-field\nevaluations, providing direct physical feedback to guide the generation toward\nenergetically stable and physically meaningful structures. Experiments on the\nQM9 and GEOM-drug datasets demonstrate that RLPF significantly improves\nmolecular stability compared to existing methods. These results highlight the\nvalue of incorporating physics-based feedback into generative modeling. The\ncode is available at: https://github.com/ZhijianZhou/RLPF/tree/verl_diffusion.\n", "link": "http://arxiv.org/abs/2508.16521v1", "date": "2025-08-22", "relevancy": 2.2229, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5619}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5595}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5308}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Guiding%20Diffusion%20Models%20with%20Reinforcement%20Learning%20for%20Stable%20Molecule%0A%20%20Generation&body=Title%3A%20Guiding%20Diffusion%20Models%20with%20Reinforcement%20Learning%20for%20Stable%20Molecule%0A%20%20Generation%0AAuthor%3A%20Zhijian%20Zhou%20and%20Junyi%20An%20and%20Zongkai%20Liu%20and%20Yunfei%20Shi%20and%20Xuan%20Zhang%20and%20Fenglei%20Cao%20and%20Chao%20Qu%20and%20Yuan%20Qi%0AAbstract%3A%20%20%20Generating%20physically%20realistic%203D%20molecular%20structures%20remains%20a%20core%0Achallenge%20in%20molecular%20generative%20modeling.%20While%20diffusion%20models%20equipped%0Awith%20equivariant%20neural%20networks%20have%20made%20progress%20in%20capturing%20molecular%0Ageometries%2C%20they%20often%20struggle%20to%20produce%20equilibrium%20structures%20that%20adhere%0Ato%20physical%20principles%20such%20as%20force%20field%20consistency.%20To%20bridge%20this%20gap%2C%20we%0Apropose%20Reinforcement%20Learning%20with%20Physical%20Feedback%20%28RLPF%29%2C%20a%20novel%20framework%0Athat%20extends%20Denoising%20Diffusion%20Policy%20Optimization%20to%203D%20molecular%0Ageneration.%20RLPF%20formulates%20the%20task%20as%20a%20Markov%20decision%20process%20and%20applies%0Aproximal%20policy%20optimization%20to%20fine-tune%20equivariant%20diffusion%20models.%0ACrucially%2C%20RLPF%20introduces%20reward%20functions%20derived%20from%20force-field%0Aevaluations%2C%20providing%20direct%20physical%20feedback%20to%20guide%20the%20generation%20toward%0Aenergetically%20stable%20and%20physically%20meaningful%20structures.%20Experiments%20on%20the%0AQM9%20and%20GEOM-drug%20datasets%20demonstrate%20that%20RLPF%20significantly%20improves%0Amolecular%20stability%20compared%20to%20existing%20methods.%20These%20results%20highlight%20the%0Avalue%20of%20incorporating%20physics-based%20feedback%20into%20generative%20modeling.%20The%0Acode%20is%20available%20at%3A%20https%3A//github.com/ZhijianZhou/RLPF/tree/verl_diffusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16521v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuiding%2520Diffusion%2520Models%2520with%2520Reinforcement%2520Learning%2520for%2520Stable%2520Molecule%250A%2520%2520Generation%26entry.906535625%3DZhijian%2520Zhou%2520and%2520Junyi%2520An%2520and%2520Zongkai%2520Liu%2520and%2520Yunfei%2520Shi%2520and%2520Xuan%2520Zhang%2520and%2520Fenglei%2520Cao%2520and%2520Chao%2520Qu%2520and%2520Yuan%2520Qi%26entry.1292438233%3D%2520%2520Generating%2520physically%2520realistic%25203D%2520molecular%2520structures%2520remains%2520a%2520core%250Achallenge%2520in%2520molecular%2520generative%2520modeling.%2520While%2520diffusion%2520models%2520equipped%250Awith%2520equivariant%2520neural%2520networks%2520have%2520made%2520progress%2520in%2520capturing%2520molecular%250Ageometries%252C%2520they%2520often%2520struggle%2520to%2520produce%2520equilibrium%2520structures%2520that%2520adhere%250Ato%2520physical%2520principles%2520such%2520as%2520force%2520field%2520consistency.%2520To%2520bridge%2520this%2520gap%252C%2520we%250Apropose%2520Reinforcement%2520Learning%2520with%2520Physical%2520Feedback%2520%2528RLPF%2529%252C%2520a%2520novel%2520framework%250Athat%2520extends%2520Denoising%2520Diffusion%2520Policy%2520Optimization%2520to%25203D%2520molecular%250Ageneration.%2520RLPF%2520formulates%2520the%2520task%2520as%2520a%2520Markov%2520decision%2520process%2520and%2520applies%250Aproximal%2520policy%2520optimization%2520to%2520fine-tune%2520equivariant%2520diffusion%2520models.%250ACrucially%252C%2520RLPF%2520introduces%2520reward%2520functions%2520derived%2520from%2520force-field%250Aevaluations%252C%2520providing%2520direct%2520physical%2520feedback%2520to%2520guide%2520the%2520generation%2520toward%250Aenergetically%2520stable%2520and%2520physically%2520meaningful%2520structures.%2520Experiments%2520on%2520the%250AQM9%2520and%2520GEOM-drug%2520datasets%2520demonstrate%2520that%2520RLPF%2520significantly%2520improves%250Amolecular%2520stability%2520compared%2520to%2520existing%2520methods.%2520These%2520results%2520highlight%2520the%250Avalue%2520of%2520incorporating%2520physics-based%2520feedback%2520into%2520generative%2520modeling.%2520The%250Acode%2520is%2520available%2520at%253A%2520https%253A//github.com/ZhijianZhou/RLPF/tree/verl_diffusion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16521v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guiding%20Diffusion%20Models%20with%20Reinforcement%20Learning%20for%20Stable%20Molecule%0A%20%20Generation&entry.906535625=Zhijian%20Zhou%20and%20Junyi%20An%20and%20Zongkai%20Liu%20and%20Yunfei%20Shi%20and%20Xuan%20Zhang%20and%20Fenglei%20Cao%20and%20Chao%20Qu%20and%20Yuan%20Qi&entry.1292438233=%20%20Generating%20physically%20realistic%203D%20molecular%20structures%20remains%20a%20core%0Achallenge%20in%20molecular%20generative%20modeling.%20While%20diffusion%20models%20equipped%0Awith%20equivariant%20neural%20networks%20have%20made%20progress%20in%20capturing%20molecular%0Ageometries%2C%20they%20often%20struggle%20to%20produce%20equilibrium%20structures%20that%20adhere%0Ato%20physical%20principles%20such%20as%20force%20field%20consistency.%20To%20bridge%20this%20gap%2C%20we%0Apropose%20Reinforcement%20Learning%20with%20Physical%20Feedback%20%28RLPF%29%2C%20a%20novel%20framework%0Athat%20extends%20Denoising%20Diffusion%20Policy%20Optimization%20to%203D%20molecular%0Ageneration.%20RLPF%20formulates%20the%20task%20as%20a%20Markov%20decision%20process%20and%20applies%0Aproximal%20policy%20optimization%20to%20fine-tune%20equivariant%20diffusion%20models.%0ACrucially%2C%20RLPF%20introduces%20reward%20functions%20derived%20from%20force-field%0Aevaluations%2C%20providing%20direct%20physical%20feedback%20to%20guide%20the%20generation%20toward%0Aenergetically%20stable%20and%20physically%20meaningful%20structures.%20Experiments%20on%20the%0AQM9%20and%20GEOM-drug%20datasets%20demonstrate%20that%20RLPF%20significantly%20improves%0Amolecular%20stability%20compared%20to%20existing%20methods.%20These%20results%20highlight%20the%0Avalue%20of%20incorporating%20physics-based%20feedback%20into%20generative%20modeling.%20The%0Acode%20is%20available%20at%3A%20https%3A//github.com/ZhijianZhou/RLPF/tree/verl_diffusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16521v1&entry.124074799=Read"},
{"title": "Vision encoders should be image size agnostic and task driven", "author": "Nedyalko Prisadnikov and Danda Pani Paudel and Yuqian Fu and Luc Van Gool", "abstract": "  This position paper argues that the next generation of vision encoders should\nbe image size agnostic and task driven. The source of our inspiration is\nbiological. Not a structural aspect of biological vision, but a behavioral\ntrait -- efficiency. We focus on a couple of ways in which vision in nature is\nefficient, but modern vision encoders not. We -- humans and animals -- deal\nwith vast quantities of visual data, and need to be smart where we focus our\nlimited energy -- it depends on the task. It is our belief that vision encoders\nshould be dynamic and the computational complexity should depend on the task at\nhand rather than the size of the image. We, also, provide concrete first steps\ntowards our vision -- a proof-of-concept solution for image classification.\nDespite classification being not very representative for what we are trying to\nachieve, it shows that our approach is feasible and promising.\n", "link": "http://arxiv.org/abs/2508.16317v1", "date": "2025-08-22", "relevancy": 2.2226, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5624}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5624}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5219}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision%20encoders%20should%20be%20image%20size%20agnostic%20and%20task%20driven&body=Title%3A%20Vision%20encoders%20should%20be%20image%20size%20agnostic%20and%20task%20driven%0AAuthor%3A%20Nedyalko%20Prisadnikov%20and%20Danda%20Pani%20Paudel%20and%20Yuqian%20Fu%20and%20Luc%20Van%20Gool%0AAbstract%3A%20%20%20This%20position%20paper%20argues%20that%20the%20next%20generation%20of%20vision%20encoders%20should%0Abe%20image%20size%20agnostic%20and%20task%20driven.%20The%20source%20of%20our%20inspiration%20is%0Abiological.%20Not%20a%20structural%20aspect%20of%20biological%20vision%2C%20but%20a%20behavioral%0Atrait%20--%20efficiency.%20We%20focus%20on%20a%20couple%20of%20ways%20in%20which%20vision%20in%20nature%20is%0Aefficient%2C%20but%20modern%20vision%20encoders%20not.%20We%20--%20humans%20and%20animals%20--%20deal%0Awith%20vast%20quantities%20of%20visual%20data%2C%20and%20need%20to%20be%20smart%20where%20we%20focus%20our%0Alimited%20energy%20--%20it%20depends%20on%20the%20task.%20It%20is%20our%20belief%20that%20vision%20encoders%0Ashould%20be%20dynamic%20and%20the%20computational%20complexity%20should%20depend%20on%20the%20task%20at%0Ahand%20rather%20than%20the%20size%20of%20the%20image.%20We%2C%20also%2C%20provide%20concrete%20first%20steps%0Atowards%20our%20vision%20--%20a%20proof-of-concept%20solution%20for%20image%20classification.%0ADespite%20classification%20being%20not%20very%20representative%20for%20what%20we%20are%20trying%20to%0Aachieve%2C%20it%20shows%20that%20our%20approach%20is%20feasible%20and%20promising.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16317v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision%2520encoders%2520should%2520be%2520image%2520size%2520agnostic%2520and%2520task%2520driven%26entry.906535625%3DNedyalko%2520Prisadnikov%2520and%2520Danda%2520Pani%2520Paudel%2520and%2520Yuqian%2520Fu%2520and%2520Luc%2520Van%2520Gool%26entry.1292438233%3D%2520%2520This%2520position%2520paper%2520argues%2520that%2520the%2520next%2520generation%2520of%2520vision%2520encoders%2520should%250Abe%2520image%2520size%2520agnostic%2520and%2520task%2520driven.%2520The%2520source%2520of%2520our%2520inspiration%2520is%250Abiological.%2520Not%2520a%2520structural%2520aspect%2520of%2520biological%2520vision%252C%2520but%2520a%2520behavioral%250Atrait%2520--%2520efficiency.%2520We%2520focus%2520on%2520a%2520couple%2520of%2520ways%2520in%2520which%2520vision%2520in%2520nature%2520is%250Aefficient%252C%2520but%2520modern%2520vision%2520encoders%2520not.%2520We%2520--%2520humans%2520and%2520animals%2520--%2520deal%250Awith%2520vast%2520quantities%2520of%2520visual%2520data%252C%2520and%2520need%2520to%2520be%2520smart%2520where%2520we%2520focus%2520our%250Alimited%2520energy%2520--%2520it%2520depends%2520on%2520the%2520task.%2520It%2520is%2520our%2520belief%2520that%2520vision%2520encoders%250Ashould%2520be%2520dynamic%2520and%2520the%2520computational%2520complexity%2520should%2520depend%2520on%2520the%2520task%2520at%250Ahand%2520rather%2520than%2520the%2520size%2520of%2520the%2520image.%2520We%252C%2520also%252C%2520provide%2520concrete%2520first%2520steps%250Atowards%2520our%2520vision%2520--%2520a%2520proof-of-concept%2520solution%2520for%2520image%2520classification.%250ADespite%2520classification%2520being%2520not%2520very%2520representative%2520for%2520what%2520we%2520are%2520trying%2520to%250Aachieve%252C%2520it%2520shows%2520that%2520our%2520approach%2520is%2520feasible%2520and%2520promising.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16317v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision%20encoders%20should%20be%20image%20size%20agnostic%20and%20task%20driven&entry.906535625=Nedyalko%20Prisadnikov%20and%20Danda%20Pani%20Paudel%20and%20Yuqian%20Fu%20and%20Luc%20Van%20Gool&entry.1292438233=%20%20This%20position%20paper%20argues%20that%20the%20next%20generation%20of%20vision%20encoders%20should%0Abe%20image%20size%20agnostic%20and%20task%20driven.%20The%20source%20of%20our%20inspiration%20is%0Abiological.%20Not%20a%20structural%20aspect%20of%20biological%20vision%2C%20but%20a%20behavioral%0Atrait%20--%20efficiency.%20We%20focus%20on%20a%20couple%20of%20ways%20in%20which%20vision%20in%20nature%20is%0Aefficient%2C%20but%20modern%20vision%20encoders%20not.%20We%20--%20humans%20and%20animals%20--%20deal%0Awith%20vast%20quantities%20of%20visual%20data%2C%20and%20need%20to%20be%20smart%20where%20we%20focus%20our%0Alimited%20energy%20--%20it%20depends%20on%20the%20task.%20It%20is%20our%20belief%20that%20vision%20encoders%0Ashould%20be%20dynamic%20and%20the%20computational%20complexity%20should%20depend%20on%20the%20task%20at%0Ahand%20rather%20than%20the%20size%20of%20the%20image.%20We%2C%20also%2C%20provide%20concrete%20first%20steps%0Atowards%20our%20vision%20--%20a%20proof-of-concept%20solution%20for%20image%20classification.%0ADespite%20classification%20being%20not%20very%20representative%20for%20what%20we%20are%20trying%20to%0Aachieve%2C%20it%20shows%20that%20our%20approach%20is%20feasible%20and%20promising.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16317v1&entry.124074799=Read"},
{"title": "Deep Intrinsic Coregionalization Multi-Output Gaussian Process Surrogate\n  with Active Learning", "author": "Chun-Yi Chang and Chih-Li Sung", "abstract": "  Deep Gaussian Processes (DGPs) are powerful surrogate models known for their\nflexibility and ability to capture complex functions. However, extending them\nto multi-output settings remains challenging due to the need for efficient\ndependency modeling. We propose the Deep Intrinsic Coregionalization\nMulti-Output Gaussian Process (deepICMGP) surrogate for computer simulation\nexperiments involving multiple outputs, which extends the Intrinsic\nCoregionalization Model (ICM) by introducing hierarchical coregionalization\nstructures across layers. This enables deepICMGP to effectively model nonlinear\nand structured dependencies between multiple outputs, addressing key\nlimitations of traditional multi-output GPs. We benchmark deepICMGP against\nstate-of-the-art models, demonstrating its competitive performance.\nFurthermore, we incorporate active learning strategies into deepICMGP to\noptimize sequential design tasks, enhancing its ability to efficiently select\ninformative input locations for multi-output systems.\n", "link": "http://arxiv.org/abs/2508.16434v1", "date": "2025-08-22", "relevancy": 2.2045, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5603}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5549}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Intrinsic%20Coregionalization%20Multi-Output%20Gaussian%20Process%20Surrogate%0A%20%20with%20Active%20Learning&body=Title%3A%20Deep%20Intrinsic%20Coregionalization%20Multi-Output%20Gaussian%20Process%20Surrogate%0A%20%20with%20Active%20Learning%0AAuthor%3A%20Chun-Yi%20Chang%20and%20Chih-Li%20Sung%0AAbstract%3A%20%20%20Deep%20Gaussian%20Processes%20%28DGPs%29%20are%20powerful%20surrogate%20models%20known%20for%20their%0Aflexibility%20and%20ability%20to%20capture%20complex%20functions.%20However%2C%20extending%20them%0Ato%20multi-output%20settings%20remains%20challenging%20due%20to%20the%20need%20for%20efficient%0Adependency%20modeling.%20We%20propose%20the%20Deep%20Intrinsic%20Coregionalization%0AMulti-Output%20Gaussian%20Process%20%28deepICMGP%29%20surrogate%20for%20computer%20simulation%0Aexperiments%20involving%20multiple%20outputs%2C%20which%20extends%20the%20Intrinsic%0ACoregionalization%20Model%20%28ICM%29%20by%20introducing%20hierarchical%20coregionalization%0Astructures%20across%20layers.%20This%20enables%20deepICMGP%20to%20effectively%20model%20nonlinear%0Aand%20structured%20dependencies%20between%20multiple%20outputs%2C%20addressing%20key%0Alimitations%20of%20traditional%20multi-output%20GPs.%20We%20benchmark%20deepICMGP%20against%0Astate-of-the-art%20models%2C%20demonstrating%20its%20competitive%20performance.%0AFurthermore%2C%20we%20incorporate%20active%20learning%20strategies%20into%20deepICMGP%20to%0Aoptimize%20sequential%20design%20tasks%2C%20enhancing%20its%20ability%20to%20efficiently%20select%0Ainformative%20input%20locations%20for%20multi-output%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16434v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Intrinsic%2520Coregionalization%2520Multi-Output%2520Gaussian%2520Process%2520Surrogate%250A%2520%2520with%2520Active%2520Learning%26entry.906535625%3DChun-Yi%2520Chang%2520and%2520Chih-Li%2520Sung%26entry.1292438233%3D%2520%2520Deep%2520Gaussian%2520Processes%2520%2528DGPs%2529%2520are%2520powerful%2520surrogate%2520models%2520known%2520for%2520their%250Aflexibility%2520and%2520ability%2520to%2520capture%2520complex%2520functions.%2520However%252C%2520extending%2520them%250Ato%2520multi-output%2520settings%2520remains%2520challenging%2520due%2520to%2520the%2520need%2520for%2520efficient%250Adependency%2520modeling.%2520We%2520propose%2520the%2520Deep%2520Intrinsic%2520Coregionalization%250AMulti-Output%2520Gaussian%2520Process%2520%2528deepICMGP%2529%2520surrogate%2520for%2520computer%2520simulation%250Aexperiments%2520involving%2520multiple%2520outputs%252C%2520which%2520extends%2520the%2520Intrinsic%250ACoregionalization%2520Model%2520%2528ICM%2529%2520by%2520introducing%2520hierarchical%2520coregionalization%250Astructures%2520across%2520layers.%2520This%2520enables%2520deepICMGP%2520to%2520effectively%2520model%2520nonlinear%250Aand%2520structured%2520dependencies%2520between%2520multiple%2520outputs%252C%2520addressing%2520key%250Alimitations%2520of%2520traditional%2520multi-output%2520GPs.%2520We%2520benchmark%2520deepICMGP%2520against%250Astate-of-the-art%2520models%252C%2520demonstrating%2520its%2520competitive%2520performance.%250AFurthermore%252C%2520we%2520incorporate%2520active%2520learning%2520strategies%2520into%2520deepICMGP%2520to%250Aoptimize%2520sequential%2520design%2520tasks%252C%2520enhancing%2520its%2520ability%2520to%2520efficiently%2520select%250Ainformative%2520input%2520locations%2520for%2520multi-output%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16434v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Intrinsic%20Coregionalization%20Multi-Output%20Gaussian%20Process%20Surrogate%0A%20%20with%20Active%20Learning&entry.906535625=Chun-Yi%20Chang%20and%20Chih-Li%20Sung&entry.1292438233=%20%20Deep%20Gaussian%20Processes%20%28DGPs%29%20are%20powerful%20surrogate%20models%20known%20for%20their%0Aflexibility%20and%20ability%20to%20capture%20complex%20functions.%20However%2C%20extending%20them%0Ato%20multi-output%20settings%20remains%20challenging%20due%20to%20the%20need%20for%20efficient%0Adependency%20modeling.%20We%20propose%20the%20Deep%20Intrinsic%20Coregionalization%0AMulti-Output%20Gaussian%20Process%20%28deepICMGP%29%20surrogate%20for%20computer%20simulation%0Aexperiments%20involving%20multiple%20outputs%2C%20which%20extends%20the%20Intrinsic%0ACoregionalization%20Model%20%28ICM%29%20by%20introducing%20hierarchical%20coregionalization%0Astructures%20across%20layers.%20This%20enables%20deepICMGP%20to%20effectively%20model%20nonlinear%0Aand%20structured%20dependencies%20between%20multiple%20outputs%2C%20addressing%20key%0Alimitations%20of%20traditional%20multi-output%20GPs.%20We%20benchmark%20deepICMGP%20against%0Astate-of-the-art%20models%2C%20demonstrating%20its%20competitive%20performance.%0AFurthermore%2C%20we%20incorporate%20active%20learning%20strategies%20into%20deepICMGP%20to%0Aoptimize%20sequential%20design%20tasks%2C%20enhancing%20its%20ability%20to%20efficiently%20select%0Ainformative%20input%20locations%20for%20multi-output%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16434v1&entry.124074799=Read"},
{"title": "EdgeDoc: Hybrid CNN-Transformer Model for Accurate Forgery Detection and\n  Localization in ID Documents", "author": "Anjith George and Sebastien Marcel", "abstract": "  The widespread availability of tools for manipulating images and documents\nhas made it increasingly easy to forge digital documents, posing a serious\nthreat to Know Your Customer (KYC) processes and remote onboarding systems.\nDetecting such forgeries is essential to preserving the integrity and security\nof these services. In this work, we present EdgeDoc, a novel approach for the\ndetection and localization of document forgeries. Our architecture combines a\nlightweight convolutional transformer with auxiliary noiseprint features\nextracted from the images, enhancing its ability to detect subtle\nmanipulations. EdgeDoc achieved third place in the ICCV 2025 DeepID Challenge,\ndemonstrating its competitiveness. Experimental results on the FantasyID\ndataset show that our method outperforms baseline approaches, highlighting its\neffectiveness in realworld scenarios. Project page : https://www.idiap.\nch/paper/edgedoc/\n", "link": "http://arxiv.org/abs/2508.16284v1", "date": "2025-08-22", "relevancy": 2.1935, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5575}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5538}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EdgeDoc%3A%20Hybrid%20CNN-Transformer%20Model%20for%20Accurate%20Forgery%20Detection%20and%0A%20%20Localization%20in%20ID%20Documents&body=Title%3A%20EdgeDoc%3A%20Hybrid%20CNN-Transformer%20Model%20for%20Accurate%20Forgery%20Detection%20and%0A%20%20Localization%20in%20ID%20Documents%0AAuthor%3A%20Anjith%20George%20and%20Sebastien%20Marcel%0AAbstract%3A%20%20%20The%20widespread%20availability%20of%20tools%20for%20manipulating%20images%20and%20documents%0Ahas%20made%20it%20increasingly%20easy%20to%20forge%20digital%20documents%2C%20posing%20a%20serious%0Athreat%20to%20Know%20Your%20Customer%20%28KYC%29%20processes%20and%20remote%20onboarding%20systems.%0ADetecting%20such%20forgeries%20is%20essential%20to%20preserving%20the%20integrity%20and%20security%0Aof%20these%20services.%20In%20this%20work%2C%20we%20present%20EdgeDoc%2C%20a%20novel%20approach%20for%20the%0Adetection%20and%20localization%20of%20document%20forgeries.%20Our%20architecture%20combines%20a%0Alightweight%20convolutional%20transformer%20with%20auxiliary%20noiseprint%20features%0Aextracted%20from%20the%20images%2C%20enhancing%20its%20ability%20to%20detect%20subtle%0Amanipulations.%20EdgeDoc%20achieved%20third%20place%20in%20the%20ICCV%202025%20DeepID%20Challenge%2C%0Ademonstrating%20its%20competitiveness.%20Experimental%20results%20on%20the%20FantasyID%0Adataset%20show%20that%20our%20method%20outperforms%20baseline%20approaches%2C%20highlighting%20its%0Aeffectiveness%20in%20realworld%20scenarios.%20Project%20page%20%3A%20https%3A//www.idiap.%0Ach/paper/edgedoc/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16284v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEdgeDoc%253A%2520Hybrid%2520CNN-Transformer%2520Model%2520for%2520Accurate%2520Forgery%2520Detection%2520and%250A%2520%2520Localization%2520in%2520ID%2520Documents%26entry.906535625%3DAnjith%2520George%2520and%2520Sebastien%2520Marcel%26entry.1292438233%3D%2520%2520The%2520widespread%2520availability%2520of%2520tools%2520for%2520manipulating%2520images%2520and%2520documents%250Ahas%2520made%2520it%2520increasingly%2520easy%2520to%2520forge%2520digital%2520documents%252C%2520posing%2520a%2520serious%250Athreat%2520to%2520Know%2520Your%2520Customer%2520%2528KYC%2529%2520processes%2520and%2520remote%2520onboarding%2520systems.%250ADetecting%2520such%2520forgeries%2520is%2520essential%2520to%2520preserving%2520the%2520integrity%2520and%2520security%250Aof%2520these%2520services.%2520In%2520this%2520work%252C%2520we%2520present%2520EdgeDoc%252C%2520a%2520novel%2520approach%2520for%2520the%250Adetection%2520and%2520localization%2520of%2520document%2520forgeries.%2520Our%2520architecture%2520combines%2520a%250Alightweight%2520convolutional%2520transformer%2520with%2520auxiliary%2520noiseprint%2520features%250Aextracted%2520from%2520the%2520images%252C%2520enhancing%2520its%2520ability%2520to%2520detect%2520subtle%250Amanipulations.%2520EdgeDoc%2520achieved%2520third%2520place%2520in%2520the%2520ICCV%25202025%2520DeepID%2520Challenge%252C%250Ademonstrating%2520its%2520competitiveness.%2520Experimental%2520results%2520on%2520the%2520FantasyID%250Adataset%2520show%2520that%2520our%2520method%2520outperforms%2520baseline%2520approaches%252C%2520highlighting%2520its%250Aeffectiveness%2520in%2520realworld%2520scenarios.%2520Project%2520page%2520%253A%2520https%253A//www.idiap.%250Ach/paper/edgedoc/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16284v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EdgeDoc%3A%20Hybrid%20CNN-Transformer%20Model%20for%20Accurate%20Forgery%20Detection%20and%0A%20%20Localization%20in%20ID%20Documents&entry.906535625=Anjith%20George%20and%20Sebastien%20Marcel&entry.1292438233=%20%20The%20widespread%20availability%20of%20tools%20for%20manipulating%20images%20and%20documents%0Ahas%20made%20it%20increasingly%20easy%20to%20forge%20digital%20documents%2C%20posing%20a%20serious%0Athreat%20to%20Know%20Your%20Customer%20%28KYC%29%20processes%20and%20remote%20onboarding%20systems.%0ADetecting%20such%20forgeries%20is%20essential%20to%20preserving%20the%20integrity%20and%20security%0Aof%20these%20services.%20In%20this%20work%2C%20we%20present%20EdgeDoc%2C%20a%20novel%20approach%20for%20the%0Adetection%20and%20localization%20of%20document%20forgeries.%20Our%20architecture%20combines%20a%0Alightweight%20convolutional%20transformer%20with%20auxiliary%20noiseprint%20features%0Aextracted%20from%20the%20images%2C%20enhancing%20its%20ability%20to%20detect%20subtle%0Amanipulations.%20EdgeDoc%20achieved%20third%20place%20in%20the%20ICCV%202025%20DeepID%20Challenge%2C%0Ademonstrating%20its%20competitiveness.%20Experimental%20results%20on%20the%20FantasyID%0Adataset%20show%20that%20our%20method%20outperforms%20baseline%20approaches%2C%20highlighting%20its%0Aeffectiveness%20in%20realworld%20scenarios.%20Project%20page%20%3A%20https%3A//www.idiap.%0Ach/paper/edgedoc/%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16284v1&entry.124074799=Read"},
{"title": "Do What? Teaching Vision-Language-Action Models to Reject the Impossible", "author": "Wen-Han Hsieh and Elvis Hsieh and Dantong Niu and Trevor Darrell and Roei Herzig and David M. Chan", "abstract": "  Recently, Vision-Language-Action (VLA) models have demonstrated strong\nperformance on a range of robotic tasks. These models rely on multimodal\ninputs, with language instructions playing a crucial role -- not only in\npredicting actions, but also in robustly interpreting user intent, even when\nthe requests are impossible to fulfill. In this work, we investigate how VLAs\ncan recognize, interpret, and respond to false-premise instructions: natural\nlanguage commands that reference objects or conditions absent from the\nenvironment. We propose Instruct-Verify-and-Act (IVA), a unified framework that\n(i) detects when an instruction cannot be executed due to a false premise, (ii)\nengages in language-based clarification or correction, and (iii) grounds\nplausible alternatives in perception and action. Towards this end, we construct\na large-scale instruction tuning setup with structured language prompts and\ntrain a VLA model capable of handling both accurate and erroneous requests. Our\napproach leverages a contextually augmented, semi-synthetic dataset containing\npaired positive and false-premise instructions, enabling robust detection and\nnatural language correction. Our experiments show that IVA improves false\npremise detection accuracy by 97.56% over baselines, while increasing\nsuccessful responses in false-premise scenarios by 50.78%.\n", "link": "http://arxiv.org/abs/2508.16292v1", "date": "2025-08-22", "relevancy": 2.1903, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5497}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5497}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5372}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20What%3F%20Teaching%20Vision-Language-Action%20Models%20to%20Reject%20the%20Impossible&body=Title%3A%20Do%20What%3F%20Teaching%20Vision-Language-Action%20Models%20to%20Reject%20the%20Impossible%0AAuthor%3A%20Wen-Han%20Hsieh%20and%20Elvis%20Hsieh%20and%20Dantong%20Niu%20and%20Trevor%20Darrell%20and%20Roei%20Herzig%20and%20David%20M.%20Chan%0AAbstract%3A%20%20%20Recently%2C%20Vision-Language-Action%20%28VLA%29%20models%20have%20demonstrated%20strong%0Aperformance%20on%20a%20range%20of%20robotic%20tasks.%20These%20models%20rely%20on%20multimodal%0Ainputs%2C%20with%20language%20instructions%20playing%20a%20crucial%20role%20--%20not%20only%20in%0Apredicting%20actions%2C%20but%20also%20in%20robustly%20interpreting%20user%20intent%2C%20even%20when%0Athe%20requests%20are%20impossible%20to%20fulfill.%20In%20this%20work%2C%20we%20investigate%20how%20VLAs%0Acan%20recognize%2C%20interpret%2C%20and%20respond%20to%20false-premise%20instructions%3A%20natural%0Alanguage%20commands%20that%20reference%20objects%20or%20conditions%20absent%20from%20the%0Aenvironment.%20We%20propose%20Instruct-Verify-and-Act%20%28IVA%29%2C%20a%20unified%20framework%20that%0A%28i%29%20detects%20when%20an%20instruction%20cannot%20be%20executed%20due%20to%20a%20false%20premise%2C%20%28ii%29%0Aengages%20in%20language-based%20clarification%20or%20correction%2C%20and%20%28iii%29%20grounds%0Aplausible%20alternatives%20in%20perception%20and%20action.%20Towards%20this%20end%2C%20we%20construct%0Aa%20large-scale%20instruction%20tuning%20setup%20with%20structured%20language%20prompts%20and%0Atrain%20a%20VLA%20model%20capable%20of%20handling%20both%20accurate%20and%20erroneous%20requests.%20Our%0Aapproach%20leverages%20a%20contextually%20augmented%2C%20semi-synthetic%20dataset%20containing%0Apaired%20positive%20and%20false-premise%20instructions%2C%20enabling%20robust%20detection%20and%0Anatural%20language%20correction.%20Our%20experiments%20show%20that%20IVA%20improves%20false%0Apremise%20detection%20accuracy%20by%2097.56%25%20over%20baselines%2C%20while%20increasing%0Asuccessful%20responses%20in%20false-premise%20scenarios%20by%2050.78%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16292v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520What%253F%2520Teaching%2520Vision-Language-Action%2520Models%2520to%2520Reject%2520the%2520Impossible%26entry.906535625%3DWen-Han%2520Hsieh%2520and%2520Elvis%2520Hsieh%2520and%2520Dantong%2520Niu%2520and%2520Trevor%2520Darrell%2520and%2520Roei%2520Herzig%2520and%2520David%2520M.%2520Chan%26entry.1292438233%3D%2520%2520Recently%252C%2520Vision-Language-Action%2520%2528VLA%2529%2520models%2520have%2520demonstrated%2520strong%250Aperformance%2520on%2520a%2520range%2520of%2520robotic%2520tasks.%2520These%2520models%2520rely%2520on%2520multimodal%250Ainputs%252C%2520with%2520language%2520instructions%2520playing%2520a%2520crucial%2520role%2520--%2520not%2520only%2520in%250Apredicting%2520actions%252C%2520but%2520also%2520in%2520robustly%2520interpreting%2520user%2520intent%252C%2520even%2520when%250Athe%2520requests%2520are%2520impossible%2520to%2520fulfill.%2520In%2520this%2520work%252C%2520we%2520investigate%2520how%2520VLAs%250Acan%2520recognize%252C%2520interpret%252C%2520and%2520respond%2520to%2520false-premise%2520instructions%253A%2520natural%250Alanguage%2520commands%2520that%2520reference%2520objects%2520or%2520conditions%2520absent%2520from%2520the%250Aenvironment.%2520We%2520propose%2520Instruct-Verify-and-Act%2520%2528IVA%2529%252C%2520a%2520unified%2520framework%2520that%250A%2528i%2529%2520detects%2520when%2520an%2520instruction%2520cannot%2520be%2520executed%2520due%2520to%2520a%2520false%2520premise%252C%2520%2528ii%2529%250Aengages%2520in%2520language-based%2520clarification%2520or%2520correction%252C%2520and%2520%2528iii%2529%2520grounds%250Aplausible%2520alternatives%2520in%2520perception%2520and%2520action.%2520Towards%2520this%2520end%252C%2520we%2520construct%250Aa%2520large-scale%2520instruction%2520tuning%2520setup%2520with%2520structured%2520language%2520prompts%2520and%250Atrain%2520a%2520VLA%2520model%2520capable%2520of%2520handling%2520both%2520accurate%2520and%2520erroneous%2520requests.%2520Our%250Aapproach%2520leverages%2520a%2520contextually%2520augmented%252C%2520semi-synthetic%2520dataset%2520containing%250Apaired%2520positive%2520and%2520false-premise%2520instructions%252C%2520enabling%2520robust%2520detection%2520and%250Anatural%2520language%2520correction.%2520Our%2520experiments%2520show%2520that%2520IVA%2520improves%2520false%250Apremise%2520detection%2520accuracy%2520by%252097.56%2525%2520over%2520baselines%252C%2520while%2520increasing%250Asuccessful%2520responses%2520in%2520false-premise%2520scenarios%2520by%252050.78%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16292v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20What%3F%20Teaching%20Vision-Language-Action%20Models%20to%20Reject%20the%20Impossible&entry.906535625=Wen-Han%20Hsieh%20and%20Elvis%20Hsieh%20and%20Dantong%20Niu%20and%20Trevor%20Darrell%20and%20Roei%20Herzig%20and%20David%20M.%20Chan&entry.1292438233=%20%20Recently%2C%20Vision-Language-Action%20%28VLA%29%20models%20have%20demonstrated%20strong%0Aperformance%20on%20a%20range%20of%20robotic%20tasks.%20These%20models%20rely%20on%20multimodal%0Ainputs%2C%20with%20language%20instructions%20playing%20a%20crucial%20role%20--%20not%20only%20in%0Apredicting%20actions%2C%20but%20also%20in%20robustly%20interpreting%20user%20intent%2C%20even%20when%0Athe%20requests%20are%20impossible%20to%20fulfill.%20In%20this%20work%2C%20we%20investigate%20how%20VLAs%0Acan%20recognize%2C%20interpret%2C%20and%20respond%20to%20false-premise%20instructions%3A%20natural%0Alanguage%20commands%20that%20reference%20objects%20or%20conditions%20absent%20from%20the%0Aenvironment.%20We%20propose%20Instruct-Verify-and-Act%20%28IVA%29%2C%20a%20unified%20framework%20that%0A%28i%29%20detects%20when%20an%20instruction%20cannot%20be%20executed%20due%20to%20a%20false%20premise%2C%20%28ii%29%0Aengages%20in%20language-based%20clarification%20or%20correction%2C%20and%20%28iii%29%20grounds%0Aplausible%20alternatives%20in%20perception%20and%20action.%20Towards%20this%20end%2C%20we%20construct%0Aa%20large-scale%20instruction%20tuning%20setup%20with%20structured%20language%20prompts%20and%0Atrain%20a%20VLA%20model%20capable%20of%20handling%20both%20accurate%20and%20erroneous%20requests.%20Our%0Aapproach%20leverages%20a%20contextually%20augmented%2C%20semi-synthetic%20dataset%20containing%0Apaired%20positive%20and%20false-premise%20instructions%2C%20enabling%20robust%20detection%20and%0Anatural%20language%20correction.%20Our%20experiments%20show%20that%20IVA%20improves%20false%0Apremise%20detection%20accuracy%20by%2097.56%25%20over%20baselines%2C%20while%20increasing%0Asuccessful%20responses%20in%20false-premise%20scenarios%20by%2050.78%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16292v1&entry.124074799=Read"},
{"title": "Explicit Correspondence Matching for Generalizable Neural Radiance\n  Fields", "author": "Yuedong Chen and Haofei Xu and Qianyi Wu and Chuanxia Zheng and Tat-Jen Cham and Jianfei Cai", "abstract": "  We present a new generalizable NeRF method that is able to directly\ngeneralize to new unseen scenarios and perform novel view synthesis with as few\nas two source views. The key to our approach lies in the explicitly modeled\ncorrespondence matching information, so as to provide the geometry prior to the\nprediction of NeRF color and density for volume rendering. The explicit\ncorrespondence matching is quantified with the cosine similarity between image\nfeatures sampled at the 2D projections of a 3D point on different views, which\nis able to provide reliable cues about the surface geometry. Unlike previous\nmethods where image features are extracted independently for each view, we\nconsider modeling the cross-view interactions via Transformer cross-attention,\nwhich greatly improves the feature matching quality. Our method achieves\nstate-of-the-art results on different evaluation settings, with the experiments\nshowing a strong correlation between our learned cosine feature similarity and\nvolume density, demonstrating the effectiveness and superiority of our proposed\nmethod. The code and model are on our project page:\nhttps://donydchen.github.io/matchnerf\n", "link": "http://arxiv.org/abs/2304.12294v2", "date": "2025-08-22", "relevancy": 2.1655, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5516}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5365}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5331}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explicit%20Correspondence%20Matching%20for%20Generalizable%20Neural%20Radiance%0A%20%20Fields&body=Title%3A%20Explicit%20Correspondence%20Matching%20for%20Generalizable%20Neural%20Radiance%0A%20%20Fields%0AAuthor%3A%20Yuedong%20Chen%20and%20Haofei%20Xu%20and%20Qianyi%20Wu%20and%20Chuanxia%20Zheng%20and%20Tat-Jen%20Cham%20and%20Jianfei%20Cai%0AAbstract%3A%20%20%20We%20present%20a%20new%20generalizable%20NeRF%20method%20that%20is%20able%20to%20directly%0Ageneralize%20to%20new%20unseen%20scenarios%20and%20perform%20novel%20view%20synthesis%20with%20as%20few%0Aas%20two%20source%20views.%20The%20key%20to%20our%20approach%20lies%20in%20the%20explicitly%20modeled%0Acorrespondence%20matching%20information%2C%20so%20as%20to%20provide%20the%20geometry%20prior%20to%20the%0Aprediction%20of%20NeRF%20color%20and%20density%20for%20volume%20rendering.%20The%20explicit%0Acorrespondence%20matching%20is%20quantified%20with%20the%20cosine%20similarity%20between%20image%0Afeatures%20sampled%20at%20the%202D%20projections%20of%20a%203D%20point%20on%20different%20views%2C%20which%0Ais%20able%20to%20provide%20reliable%20cues%20about%20the%20surface%20geometry.%20Unlike%20previous%0Amethods%20where%20image%20features%20are%20extracted%20independently%20for%20each%20view%2C%20we%0Aconsider%20modeling%20the%20cross-view%20interactions%20via%20Transformer%20cross-attention%2C%0Awhich%20greatly%20improves%20the%20feature%20matching%20quality.%20Our%20method%20achieves%0Astate-of-the-art%20results%20on%20different%20evaluation%20settings%2C%20with%20the%20experiments%0Ashowing%20a%20strong%20correlation%20between%20our%20learned%20cosine%20feature%20similarity%20and%0Avolume%20density%2C%20demonstrating%20the%20effectiveness%20and%20superiority%20of%20our%20proposed%0Amethod.%20The%20code%20and%20model%20are%20on%20our%20project%20page%3A%0Ahttps%3A//donydchen.github.io/matchnerf%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.12294v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplicit%2520Correspondence%2520Matching%2520for%2520Generalizable%2520Neural%2520Radiance%250A%2520%2520Fields%26entry.906535625%3DYuedong%2520Chen%2520and%2520Haofei%2520Xu%2520and%2520Qianyi%2520Wu%2520and%2520Chuanxia%2520Zheng%2520and%2520Tat-Jen%2520Cham%2520and%2520Jianfei%2520Cai%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520new%2520generalizable%2520NeRF%2520method%2520that%2520is%2520able%2520to%2520directly%250Ageneralize%2520to%2520new%2520unseen%2520scenarios%2520and%2520perform%2520novel%2520view%2520synthesis%2520with%2520as%2520few%250Aas%2520two%2520source%2520views.%2520The%2520key%2520to%2520our%2520approach%2520lies%2520in%2520the%2520explicitly%2520modeled%250Acorrespondence%2520matching%2520information%252C%2520so%2520as%2520to%2520provide%2520the%2520geometry%2520prior%2520to%2520the%250Aprediction%2520of%2520NeRF%2520color%2520and%2520density%2520for%2520volume%2520rendering.%2520The%2520explicit%250Acorrespondence%2520matching%2520is%2520quantified%2520with%2520the%2520cosine%2520similarity%2520between%2520image%250Afeatures%2520sampled%2520at%2520the%25202D%2520projections%2520of%2520a%25203D%2520point%2520on%2520different%2520views%252C%2520which%250Ais%2520able%2520to%2520provide%2520reliable%2520cues%2520about%2520the%2520surface%2520geometry.%2520Unlike%2520previous%250Amethods%2520where%2520image%2520features%2520are%2520extracted%2520independently%2520for%2520each%2520view%252C%2520we%250Aconsider%2520modeling%2520the%2520cross-view%2520interactions%2520via%2520Transformer%2520cross-attention%252C%250Awhich%2520greatly%2520improves%2520the%2520feature%2520matching%2520quality.%2520Our%2520method%2520achieves%250Astate-of-the-art%2520results%2520on%2520different%2520evaluation%2520settings%252C%2520with%2520the%2520experiments%250Ashowing%2520a%2520strong%2520correlation%2520between%2520our%2520learned%2520cosine%2520feature%2520similarity%2520and%250Avolume%2520density%252C%2520demonstrating%2520the%2520effectiveness%2520and%2520superiority%2520of%2520our%2520proposed%250Amethod.%2520The%2520code%2520and%2520model%2520are%2520on%2520our%2520project%2520page%253A%250Ahttps%253A//donydchen.github.io/matchnerf%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.12294v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explicit%20Correspondence%20Matching%20for%20Generalizable%20Neural%20Radiance%0A%20%20Fields&entry.906535625=Yuedong%20Chen%20and%20Haofei%20Xu%20and%20Qianyi%20Wu%20and%20Chuanxia%20Zheng%20and%20Tat-Jen%20Cham%20and%20Jianfei%20Cai&entry.1292438233=%20%20We%20present%20a%20new%20generalizable%20NeRF%20method%20that%20is%20able%20to%20directly%0Ageneralize%20to%20new%20unseen%20scenarios%20and%20perform%20novel%20view%20synthesis%20with%20as%20few%0Aas%20two%20source%20views.%20The%20key%20to%20our%20approach%20lies%20in%20the%20explicitly%20modeled%0Acorrespondence%20matching%20information%2C%20so%20as%20to%20provide%20the%20geometry%20prior%20to%20the%0Aprediction%20of%20NeRF%20color%20and%20density%20for%20volume%20rendering.%20The%20explicit%0Acorrespondence%20matching%20is%20quantified%20with%20the%20cosine%20similarity%20between%20image%0Afeatures%20sampled%20at%20the%202D%20projections%20of%20a%203D%20point%20on%20different%20views%2C%20which%0Ais%20able%20to%20provide%20reliable%20cues%20about%20the%20surface%20geometry.%20Unlike%20previous%0Amethods%20where%20image%20features%20are%20extracted%20independently%20for%20each%20view%2C%20we%0Aconsider%20modeling%20the%20cross-view%20interactions%20via%20Transformer%20cross-attention%2C%0Awhich%20greatly%20improves%20the%20feature%20matching%20quality.%20Our%20method%20achieves%0Astate-of-the-art%20results%20on%20different%20evaluation%20settings%2C%20with%20the%20experiments%0Ashowing%20a%20strong%20correlation%20between%20our%20learned%20cosine%20feature%20similarity%20and%0Avolume%20density%2C%20demonstrating%20the%20effectiveness%20and%20superiority%20of%20our%20proposed%0Amethod.%20The%20code%20and%20model%20are%20on%20our%20project%20page%3A%0Ahttps%3A//donydchen.github.io/matchnerf%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.12294v2&entry.124074799=Read"},
{"title": "Perceptual Implications of Automatic Anonymization in Pathological\n  Speech", "author": "Soroosh Tayebi Arasteh and Saba Afza and Tri-Thien Nguyen and Lukas Buess and Maryam Parvin and Tomas Arias-Vergara and Paula Andrea Perez-Toro and Hiu Ching Hung and Mahshad Lotfinia and Thomas Gorges and Elmar Noeth and Maria Schuster and Seung Hee Yang and Andreas Maier", "abstract": "  Automatic anonymization techniques are essential for ethical sharing of\npathological speech data, yet their perceptual consequences remain\nunderstudied. We present a comprehensive human-centered analysis of anonymized\npathological speech, using a structured protocol involving ten native and\nnon-native German listeners with diverse linguistic, clinical, and technical\nbackgrounds. Listeners evaluated anonymized-original utterance pairs from 180\nspeakers spanning Cleft Lip and Palate, Dysarthria, Dysglossia, Dysphonia, and\nhealthy controls. Speech was anonymized using state-of-the-art automatic\nmethods (equal error rates in the range of 30-40%). Listeners completed\nTuring-style discrimination and quality rating tasks under zero-shot\n(single-exposure) and few-shot (repeated-exposure) conditions. Discrimination\naccuracy was high overall (91% zero-shot; 93% few-shot), but varied by disorder\n(repeated-measures ANOVA: p=0.007), ranging from 96% (Dysarthria) to 86%\n(Dysphonia). Anonymization consistently reduced perceived quality across groups\n(from 83% to 59%, p<0.001), with pathology-specific degradation patterns\n(one-way ANOVA: p=0.005). Native listeners showed a non-significant trend\ntoward higher original speech ratings (Delta=4%, p=0.199), but this difference\nwas minimal after anonymization (Delta=1%, p=0.724). No significant\ngender-based bias was observed. Perceptual outcomes did not correlate with\nautomatic metrics; intelligibility was linked to perceived quality in original\nspeech but not after anonymization. These findings underscore the need for\nlistener-informed, disorder-specific anonymization strategies that preserve\nboth privacy and perceptual integrity.\n", "link": "http://arxiv.org/abs/2505.00409v2", "date": "2025-08-22", "relevancy": 2.1622, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4346}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4346}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4281}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perceptual%20Implications%20of%20Automatic%20Anonymization%20in%20Pathological%0A%20%20Speech&body=Title%3A%20Perceptual%20Implications%20of%20Automatic%20Anonymization%20in%20Pathological%0A%20%20Speech%0AAuthor%3A%20Soroosh%20Tayebi%20Arasteh%20and%20Saba%20Afza%20and%20Tri-Thien%20Nguyen%20and%20Lukas%20Buess%20and%20Maryam%20Parvin%20and%20Tomas%20Arias-Vergara%20and%20Paula%20Andrea%20Perez-Toro%20and%20Hiu%20Ching%20Hung%20and%20Mahshad%20Lotfinia%20and%20Thomas%20Gorges%20and%20Elmar%20Noeth%20and%20Maria%20Schuster%20and%20Seung%20Hee%20Yang%20and%20Andreas%20Maier%0AAbstract%3A%20%20%20Automatic%20anonymization%20techniques%20are%20essential%20for%20ethical%20sharing%20of%0Apathological%20speech%20data%2C%20yet%20their%20perceptual%20consequences%20remain%0Aunderstudied.%20We%20present%20a%20comprehensive%20human-centered%20analysis%20of%20anonymized%0Apathological%20speech%2C%20using%20a%20structured%20protocol%20involving%20ten%20native%20and%0Anon-native%20German%20listeners%20with%20diverse%20linguistic%2C%20clinical%2C%20and%20technical%0Abackgrounds.%20Listeners%20evaluated%20anonymized-original%20utterance%20pairs%20from%20180%0Aspeakers%20spanning%20Cleft%20Lip%20and%20Palate%2C%20Dysarthria%2C%20Dysglossia%2C%20Dysphonia%2C%20and%0Ahealthy%20controls.%20Speech%20was%20anonymized%20using%20state-of-the-art%20automatic%0Amethods%20%28equal%20error%20rates%20in%20the%20range%20of%2030-40%25%29.%20Listeners%20completed%0ATuring-style%20discrimination%20and%20quality%20rating%20tasks%20under%20zero-shot%0A%28single-exposure%29%20and%20few-shot%20%28repeated-exposure%29%20conditions.%20Discrimination%0Aaccuracy%20was%20high%20overall%20%2891%25%20zero-shot%3B%2093%25%20few-shot%29%2C%20but%20varied%20by%20disorder%0A%28repeated-measures%20ANOVA%3A%20p%3D0.007%29%2C%20ranging%20from%2096%25%20%28Dysarthria%29%20to%2086%25%0A%28Dysphonia%29.%20Anonymization%20consistently%20reduced%20perceived%20quality%20across%20groups%0A%28from%2083%25%20to%2059%25%2C%20p%3C0.001%29%2C%20with%20pathology-specific%20degradation%20patterns%0A%28one-way%20ANOVA%3A%20p%3D0.005%29.%20Native%20listeners%20showed%20a%20non-significant%20trend%0Atoward%20higher%20original%20speech%20ratings%20%28Delta%3D4%25%2C%20p%3D0.199%29%2C%20but%20this%20difference%0Awas%20minimal%20after%20anonymization%20%28Delta%3D1%25%2C%20p%3D0.724%29.%20No%20significant%0Agender-based%20bias%20was%20observed.%20Perceptual%20outcomes%20did%20not%20correlate%20with%0Aautomatic%20metrics%3B%20intelligibility%20was%20linked%20to%20perceived%20quality%20in%20original%0Aspeech%20but%20not%20after%20anonymization.%20These%20findings%20underscore%20the%20need%20for%0Alistener-informed%2C%20disorder-specific%20anonymization%20strategies%20that%20preserve%0Aboth%20privacy%20and%20perceptual%20integrity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00409v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerceptual%2520Implications%2520of%2520Automatic%2520Anonymization%2520in%2520Pathological%250A%2520%2520Speech%26entry.906535625%3DSoroosh%2520Tayebi%2520Arasteh%2520and%2520Saba%2520Afza%2520and%2520Tri-Thien%2520Nguyen%2520and%2520Lukas%2520Buess%2520and%2520Maryam%2520Parvin%2520and%2520Tomas%2520Arias-Vergara%2520and%2520Paula%2520Andrea%2520Perez-Toro%2520and%2520Hiu%2520Ching%2520Hung%2520and%2520Mahshad%2520Lotfinia%2520and%2520Thomas%2520Gorges%2520and%2520Elmar%2520Noeth%2520and%2520Maria%2520Schuster%2520and%2520Seung%2520Hee%2520Yang%2520and%2520Andreas%2520Maier%26entry.1292438233%3D%2520%2520Automatic%2520anonymization%2520techniques%2520are%2520essential%2520for%2520ethical%2520sharing%2520of%250Apathological%2520speech%2520data%252C%2520yet%2520their%2520perceptual%2520consequences%2520remain%250Aunderstudied.%2520We%2520present%2520a%2520comprehensive%2520human-centered%2520analysis%2520of%2520anonymized%250Apathological%2520speech%252C%2520using%2520a%2520structured%2520protocol%2520involving%2520ten%2520native%2520and%250Anon-native%2520German%2520listeners%2520with%2520diverse%2520linguistic%252C%2520clinical%252C%2520and%2520technical%250Abackgrounds.%2520Listeners%2520evaluated%2520anonymized-original%2520utterance%2520pairs%2520from%2520180%250Aspeakers%2520spanning%2520Cleft%2520Lip%2520and%2520Palate%252C%2520Dysarthria%252C%2520Dysglossia%252C%2520Dysphonia%252C%2520and%250Ahealthy%2520controls.%2520Speech%2520was%2520anonymized%2520using%2520state-of-the-art%2520automatic%250Amethods%2520%2528equal%2520error%2520rates%2520in%2520the%2520range%2520of%252030-40%2525%2529.%2520Listeners%2520completed%250ATuring-style%2520discrimination%2520and%2520quality%2520rating%2520tasks%2520under%2520zero-shot%250A%2528single-exposure%2529%2520and%2520few-shot%2520%2528repeated-exposure%2529%2520conditions.%2520Discrimination%250Aaccuracy%2520was%2520high%2520overall%2520%252891%2525%2520zero-shot%253B%252093%2525%2520few-shot%2529%252C%2520but%2520varied%2520by%2520disorder%250A%2528repeated-measures%2520ANOVA%253A%2520p%253D0.007%2529%252C%2520ranging%2520from%252096%2525%2520%2528Dysarthria%2529%2520to%252086%2525%250A%2528Dysphonia%2529.%2520Anonymization%2520consistently%2520reduced%2520perceived%2520quality%2520across%2520groups%250A%2528from%252083%2525%2520to%252059%2525%252C%2520p%253C0.001%2529%252C%2520with%2520pathology-specific%2520degradation%2520patterns%250A%2528one-way%2520ANOVA%253A%2520p%253D0.005%2529.%2520Native%2520listeners%2520showed%2520a%2520non-significant%2520trend%250Atoward%2520higher%2520original%2520speech%2520ratings%2520%2528Delta%253D4%2525%252C%2520p%253D0.199%2529%252C%2520but%2520this%2520difference%250Awas%2520minimal%2520after%2520anonymization%2520%2528Delta%253D1%2525%252C%2520p%253D0.724%2529.%2520No%2520significant%250Agender-based%2520bias%2520was%2520observed.%2520Perceptual%2520outcomes%2520did%2520not%2520correlate%2520with%250Aautomatic%2520metrics%253B%2520intelligibility%2520was%2520linked%2520to%2520perceived%2520quality%2520in%2520original%250Aspeech%2520but%2520not%2520after%2520anonymization.%2520These%2520findings%2520underscore%2520the%2520need%2520for%250Alistener-informed%252C%2520disorder-specific%2520anonymization%2520strategies%2520that%2520preserve%250Aboth%2520privacy%2520and%2520perceptual%2520integrity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00409v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perceptual%20Implications%20of%20Automatic%20Anonymization%20in%20Pathological%0A%20%20Speech&entry.906535625=Soroosh%20Tayebi%20Arasteh%20and%20Saba%20Afza%20and%20Tri-Thien%20Nguyen%20and%20Lukas%20Buess%20and%20Maryam%20Parvin%20and%20Tomas%20Arias-Vergara%20and%20Paula%20Andrea%20Perez-Toro%20and%20Hiu%20Ching%20Hung%20and%20Mahshad%20Lotfinia%20and%20Thomas%20Gorges%20and%20Elmar%20Noeth%20and%20Maria%20Schuster%20and%20Seung%20Hee%20Yang%20and%20Andreas%20Maier&entry.1292438233=%20%20Automatic%20anonymization%20techniques%20are%20essential%20for%20ethical%20sharing%20of%0Apathological%20speech%20data%2C%20yet%20their%20perceptual%20consequences%20remain%0Aunderstudied.%20We%20present%20a%20comprehensive%20human-centered%20analysis%20of%20anonymized%0Apathological%20speech%2C%20using%20a%20structured%20protocol%20involving%20ten%20native%20and%0Anon-native%20German%20listeners%20with%20diverse%20linguistic%2C%20clinical%2C%20and%20technical%0Abackgrounds.%20Listeners%20evaluated%20anonymized-original%20utterance%20pairs%20from%20180%0Aspeakers%20spanning%20Cleft%20Lip%20and%20Palate%2C%20Dysarthria%2C%20Dysglossia%2C%20Dysphonia%2C%20and%0Ahealthy%20controls.%20Speech%20was%20anonymized%20using%20state-of-the-art%20automatic%0Amethods%20%28equal%20error%20rates%20in%20the%20range%20of%2030-40%25%29.%20Listeners%20completed%0ATuring-style%20discrimination%20and%20quality%20rating%20tasks%20under%20zero-shot%0A%28single-exposure%29%20and%20few-shot%20%28repeated-exposure%29%20conditions.%20Discrimination%0Aaccuracy%20was%20high%20overall%20%2891%25%20zero-shot%3B%2093%25%20few-shot%29%2C%20but%20varied%20by%20disorder%0A%28repeated-measures%20ANOVA%3A%20p%3D0.007%29%2C%20ranging%20from%2096%25%20%28Dysarthria%29%20to%2086%25%0A%28Dysphonia%29.%20Anonymization%20consistently%20reduced%20perceived%20quality%20across%20groups%0A%28from%2083%25%20to%2059%25%2C%20p%3C0.001%29%2C%20with%20pathology-specific%20degradation%20patterns%0A%28one-way%20ANOVA%3A%20p%3D0.005%29.%20Native%20listeners%20showed%20a%20non-significant%20trend%0Atoward%20higher%20original%20speech%20ratings%20%28Delta%3D4%25%2C%20p%3D0.199%29%2C%20but%20this%20difference%0Awas%20minimal%20after%20anonymization%20%28Delta%3D1%25%2C%20p%3D0.724%29.%20No%20significant%0Agender-based%20bias%20was%20observed.%20Perceptual%20outcomes%20did%20not%20correlate%20with%0Aautomatic%20metrics%3B%20intelligibility%20was%20linked%20to%20perceived%20quality%20in%20original%0Aspeech%20but%20not%20after%20anonymization.%20These%20findings%20underscore%20the%20need%20for%0Alistener-informed%2C%20disorder-specific%20anonymization%20strategies%20that%20preserve%0Aboth%20privacy%20and%20perceptual%20integrity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00409v2&entry.124074799=Read"},
{"title": "Enhanced NIRMAL Optimizer With Damped Nesterov Acceleration: A\n  Comparative Analysis", "author": "Nirmal Gaud and Prasad Krishna Murthy and Mostaque Md. Morshedur Hassan and Abhijit Ganguly and Vinay Mali and Ms Lalita Bhagwat Randive and Abhaypratap Singh", "abstract": "  This study introduces the Enhanced NIRMAL (Novel Integrated Robust\nMulti-Adaptation Learning with Damped Nesterov Acceleration) optimizer, an\nimproved version of the original NIRMAL optimizer. By incorporating an\n$(\\alpha, r)$-damped Nesterov acceleration mechanism, Enhanced NIRMAL improves\nconvergence stability while retaining chess-inspired strategies of gradient\ndescent, momentum, stochastic perturbations, adaptive learning rates, and\nnon-linear transformations.\n  We evaluate Enhanced NIRMAL against Adam, SGD with Momentum, Nesterov, and\nthe original NIRMAL on four benchmark image classification datasets: MNIST,\nFashionMNIST, CIFAR-10, and CIFAR-100, using tailored convolutional neural\nnetwork (CNN) architectures.\n  Enhanced NIRMAL achieves a test accuracy of 46.06\\% and the lowest test loss\n(1.960435) on CIFAR-100, surpassing the original NIRMAL (44.34\\% accuracy) and\nclosely rivaling SGD with Momentum (46.43\\% accuracy). These results underscore\nEnhanced NIRMAL's superior generalization and stability, particularly on\ncomplex datasets.\n", "link": "http://arxiv.org/abs/2508.16550v1", "date": "2025-08-22", "relevancy": 2.1368, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5804}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5125}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4967}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20NIRMAL%20Optimizer%20With%20Damped%20Nesterov%20Acceleration%3A%20A%0A%20%20Comparative%20Analysis&body=Title%3A%20Enhanced%20NIRMAL%20Optimizer%20With%20Damped%20Nesterov%20Acceleration%3A%20A%0A%20%20Comparative%20Analysis%0AAuthor%3A%20Nirmal%20Gaud%20and%20Prasad%20Krishna%20Murthy%20and%20Mostaque%20Md.%20Morshedur%20Hassan%20and%20Abhijit%20Ganguly%20and%20Vinay%20Mali%20and%20Ms%20Lalita%20Bhagwat%20Randive%20and%20Abhaypratap%20Singh%0AAbstract%3A%20%20%20This%20study%20introduces%20the%20Enhanced%20NIRMAL%20%28Novel%20Integrated%20Robust%0AMulti-Adaptation%20Learning%20with%20Damped%20Nesterov%20Acceleration%29%20optimizer%2C%20an%0Aimproved%20version%20of%20the%20original%20NIRMAL%20optimizer.%20By%20incorporating%20an%0A%24%28%5Calpha%2C%20r%29%24-damped%20Nesterov%20acceleration%20mechanism%2C%20Enhanced%20NIRMAL%20improves%0Aconvergence%20stability%20while%20retaining%20chess-inspired%20strategies%20of%20gradient%0Adescent%2C%20momentum%2C%20stochastic%20perturbations%2C%20adaptive%20learning%20rates%2C%20and%0Anon-linear%20transformations.%0A%20%20We%20evaluate%20Enhanced%20NIRMAL%20against%20Adam%2C%20SGD%20with%20Momentum%2C%20Nesterov%2C%20and%0Athe%20original%20NIRMAL%20on%20four%20benchmark%20image%20classification%20datasets%3A%20MNIST%2C%0AFashionMNIST%2C%20CIFAR-10%2C%20and%20CIFAR-100%2C%20using%20tailored%20convolutional%20neural%0Anetwork%20%28CNN%29%20architectures.%0A%20%20Enhanced%20NIRMAL%20achieves%20a%20test%20accuracy%20of%2046.06%5C%25%20and%20the%20lowest%20test%20loss%0A%281.960435%29%20on%20CIFAR-100%2C%20surpassing%20the%20original%20NIRMAL%20%2844.34%5C%25%20accuracy%29%20and%0Aclosely%20rivaling%20SGD%20with%20Momentum%20%2846.43%5C%25%20accuracy%29.%20These%20results%20underscore%0AEnhanced%20NIRMAL%27s%20superior%20generalization%20and%20stability%2C%20particularly%20on%0Acomplex%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16550v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520NIRMAL%2520Optimizer%2520With%2520Damped%2520Nesterov%2520Acceleration%253A%2520A%250A%2520%2520Comparative%2520Analysis%26entry.906535625%3DNirmal%2520Gaud%2520and%2520Prasad%2520Krishna%2520Murthy%2520and%2520Mostaque%2520Md.%2520Morshedur%2520Hassan%2520and%2520Abhijit%2520Ganguly%2520and%2520Vinay%2520Mali%2520and%2520Ms%2520Lalita%2520Bhagwat%2520Randive%2520and%2520Abhaypratap%2520Singh%26entry.1292438233%3D%2520%2520This%2520study%2520introduces%2520the%2520Enhanced%2520NIRMAL%2520%2528Novel%2520Integrated%2520Robust%250AMulti-Adaptation%2520Learning%2520with%2520Damped%2520Nesterov%2520Acceleration%2529%2520optimizer%252C%2520an%250Aimproved%2520version%2520of%2520the%2520original%2520NIRMAL%2520optimizer.%2520By%2520incorporating%2520an%250A%2524%2528%255Calpha%252C%2520r%2529%2524-damped%2520Nesterov%2520acceleration%2520mechanism%252C%2520Enhanced%2520NIRMAL%2520improves%250Aconvergence%2520stability%2520while%2520retaining%2520chess-inspired%2520strategies%2520of%2520gradient%250Adescent%252C%2520momentum%252C%2520stochastic%2520perturbations%252C%2520adaptive%2520learning%2520rates%252C%2520and%250Anon-linear%2520transformations.%250A%2520%2520We%2520evaluate%2520Enhanced%2520NIRMAL%2520against%2520Adam%252C%2520SGD%2520with%2520Momentum%252C%2520Nesterov%252C%2520and%250Athe%2520original%2520NIRMAL%2520on%2520four%2520benchmark%2520image%2520classification%2520datasets%253A%2520MNIST%252C%250AFashionMNIST%252C%2520CIFAR-10%252C%2520and%2520CIFAR-100%252C%2520using%2520tailored%2520convolutional%2520neural%250Anetwork%2520%2528CNN%2529%2520architectures.%250A%2520%2520Enhanced%2520NIRMAL%2520achieves%2520a%2520test%2520accuracy%2520of%252046.06%255C%2525%2520and%2520the%2520lowest%2520test%2520loss%250A%25281.960435%2529%2520on%2520CIFAR-100%252C%2520surpassing%2520the%2520original%2520NIRMAL%2520%252844.34%255C%2525%2520accuracy%2529%2520and%250Aclosely%2520rivaling%2520SGD%2520with%2520Momentum%2520%252846.43%255C%2525%2520accuracy%2529.%2520These%2520results%2520underscore%250AEnhanced%2520NIRMAL%2527s%2520superior%2520generalization%2520and%2520stability%252C%2520particularly%2520on%250Acomplex%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16550v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20NIRMAL%20Optimizer%20With%20Damped%20Nesterov%20Acceleration%3A%20A%0A%20%20Comparative%20Analysis&entry.906535625=Nirmal%20Gaud%20and%20Prasad%20Krishna%20Murthy%20and%20Mostaque%20Md.%20Morshedur%20Hassan%20and%20Abhijit%20Ganguly%20and%20Vinay%20Mali%20and%20Ms%20Lalita%20Bhagwat%20Randive%20and%20Abhaypratap%20Singh&entry.1292438233=%20%20This%20study%20introduces%20the%20Enhanced%20NIRMAL%20%28Novel%20Integrated%20Robust%0AMulti-Adaptation%20Learning%20with%20Damped%20Nesterov%20Acceleration%29%20optimizer%2C%20an%0Aimproved%20version%20of%20the%20original%20NIRMAL%20optimizer.%20By%20incorporating%20an%0A%24%28%5Calpha%2C%20r%29%24-damped%20Nesterov%20acceleration%20mechanism%2C%20Enhanced%20NIRMAL%20improves%0Aconvergence%20stability%20while%20retaining%20chess-inspired%20strategies%20of%20gradient%0Adescent%2C%20momentum%2C%20stochastic%20perturbations%2C%20adaptive%20learning%20rates%2C%20and%0Anon-linear%20transformations.%0A%20%20We%20evaluate%20Enhanced%20NIRMAL%20against%20Adam%2C%20SGD%20with%20Momentum%2C%20Nesterov%2C%20and%0Athe%20original%20NIRMAL%20on%20four%20benchmark%20image%20classification%20datasets%3A%20MNIST%2C%0AFashionMNIST%2C%20CIFAR-10%2C%20and%20CIFAR-100%2C%20using%20tailored%20convolutional%20neural%0Anetwork%20%28CNN%29%20architectures.%0A%20%20Enhanced%20NIRMAL%20achieves%20a%20test%20accuracy%20of%2046.06%5C%25%20and%20the%20lowest%20test%20loss%0A%281.960435%29%20on%20CIFAR-100%2C%20surpassing%20the%20original%20NIRMAL%20%2844.34%5C%25%20accuracy%29%20and%0Aclosely%20rivaling%20SGD%20with%20Momentum%20%2846.43%5C%25%20accuracy%29.%20These%20results%20underscore%0AEnhanced%20NIRMAL%27s%20superior%20generalization%20and%20stability%2C%20particularly%20on%0Acomplex%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16550v1&entry.124074799=Read"},
{"title": "Improving U-Net Confidence on TEM Image Data with L2-Regularization,\n  Transfer Learning, and Deep Fine-Tuning", "author": "Aiden Ochoa and Xinyuan Xu and Xing Wang", "abstract": "  With ever-increasing data volumes, it is essential to develop automated\napproaches for identifying nanoscale defects in transmission electron\nmicroscopy (TEM) images. However, compared to features in conventional\nphotographs, nanoscale defects in TEM images exhibit far greater variation due\nto the complex contrast mechanisms and intricate defect structures. These\nchallenges often result in much less labeled data and higher rates of\nannotation errors, posing significant obstacles to improving machine learning\nmodel performance for TEM image analysis. To address these limitations, we\nexamined transfer learning by leveraging large, pre-trained models used for\nnatural images.\n  We demonstrated that by using the pre-trained encoder and L2-regularization,\nsemantically complex features are ignored in favor of simpler, more reliable\ncues, substantially improving the model performance. However, this improvement\ncannot be captured by conventional evaluation metrics such as F1-score, which\ncan be skewed by human annotation errors treated as ground truth. Instead, we\nintroduced novel evaluation metrics that are independent of the annotation\naccuracy. Using grain boundary detection in UO2 TEM images as a case study, we\nfound that our approach led to a 57% increase in defect detection rate, which\nis a robust and holistic measure of model performance on the TEM dataset used\nin this work. Finally, we showed that model self-confidence is only achieved\nthrough transfer learning and fine-tuning of very deep layers.\n", "link": "http://arxiv.org/abs/2507.16779v2", "date": "2025-08-22", "relevancy": 2.128, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.577}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5355}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5104}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20U-Net%20Confidence%20on%20TEM%20Image%20Data%20with%20L2-Regularization%2C%0A%20%20Transfer%20Learning%2C%20and%20Deep%20Fine-Tuning&body=Title%3A%20Improving%20U-Net%20Confidence%20on%20TEM%20Image%20Data%20with%20L2-Regularization%2C%0A%20%20Transfer%20Learning%2C%20and%20Deep%20Fine-Tuning%0AAuthor%3A%20Aiden%20Ochoa%20and%20Xinyuan%20Xu%20and%20Xing%20Wang%0AAbstract%3A%20%20%20With%20ever-increasing%20data%20volumes%2C%20it%20is%20essential%20to%20develop%20automated%0Aapproaches%20for%20identifying%20nanoscale%20defects%20in%20transmission%20electron%0Amicroscopy%20%28TEM%29%20images.%20However%2C%20compared%20to%20features%20in%20conventional%0Aphotographs%2C%20nanoscale%20defects%20in%20TEM%20images%20exhibit%20far%20greater%20variation%20due%0Ato%20the%20complex%20contrast%20mechanisms%20and%20intricate%20defect%20structures.%20These%0Achallenges%20often%20result%20in%20much%20less%20labeled%20data%20and%20higher%20rates%20of%0Aannotation%20errors%2C%20posing%20significant%20obstacles%20to%20improving%20machine%20learning%0Amodel%20performance%20for%20TEM%20image%20analysis.%20To%20address%20these%20limitations%2C%20we%0Aexamined%20transfer%20learning%20by%20leveraging%20large%2C%20pre-trained%20models%20used%20for%0Anatural%20images.%0A%20%20We%20demonstrated%20that%20by%20using%20the%20pre-trained%20encoder%20and%20L2-regularization%2C%0Asemantically%20complex%20features%20are%20ignored%20in%20favor%20of%20simpler%2C%20more%20reliable%0Acues%2C%20substantially%20improving%20the%20model%20performance.%20However%2C%20this%20improvement%0Acannot%20be%20captured%20by%20conventional%20evaluation%20metrics%20such%20as%20F1-score%2C%20which%0Acan%20be%20skewed%20by%20human%20annotation%20errors%20treated%20as%20ground%20truth.%20Instead%2C%20we%0Aintroduced%20novel%20evaluation%20metrics%20that%20are%20independent%20of%20the%20annotation%0Aaccuracy.%20Using%20grain%20boundary%20detection%20in%20UO2%20TEM%20images%20as%20a%20case%20study%2C%20we%0Afound%20that%20our%20approach%20led%20to%20a%2057%25%20increase%20in%20defect%20detection%20rate%2C%20which%0Ais%20a%20robust%20and%20holistic%20measure%20of%20model%20performance%20on%20the%20TEM%20dataset%20used%0Ain%20this%20work.%20Finally%2C%20we%20showed%20that%20model%20self-confidence%20is%20only%20achieved%0Athrough%20transfer%20learning%20and%20fine-tuning%20of%20very%20deep%20layers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16779v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520U-Net%2520Confidence%2520on%2520TEM%2520Image%2520Data%2520with%2520L2-Regularization%252C%250A%2520%2520Transfer%2520Learning%252C%2520and%2520Deep%2520Fine-Tuning%26entry.906535625%3DAiden%2520Ochoa%2520and%2520Xinyuan%2520Xu%2520and%2520Xing%2520Wang%26entry.1292438233%3D%2520%2520With%2520ever-increasing%2520data%2520volumes%252C%2520it%2520is%2520essential%2520to%2520develop%2520automated%250Aapproaches%2520for%2520identifying%2520nanoscale%2520defects%2520in%2520transmission%2520electron%250Amicroscopy%2520%2528TEM%2529%2520images.%2520However%252C%2520compared%2520to%2520features%2520in%2520conventional%250Aphotographs%252C%2520nanoscale%2520defects%2520in%2520TEM%2520images%2520exhibit%2520far%2520greater%2520variation%2520due%250Ato%2520the%2520complex%2520contrast%2520mechanisms%2520and%2520intricate%2520defect%2520structures.%2520These%250Achallenges%2520often%2520result%2520in%2520much%2520less%2520labeled%2520data%2520and%2520higher%2520rates%2520of%250Aannotation%2520errors%252C%2520posing%2520significant%2520obstacles%2520to%2520improving%2520machine%2520learning%250Amodel%2520performance%2520for%2520TEM%2520image%2520analysis.%2520To%2520address%2520these%2520limitations%252C%2520we%250Aexamined%2520transfer%2520learning%2520by%2520leveraging%2520large%252C%2520pre-trained%2520models%2520used%2520for%250Anatural%2520images.%250A%2520%2520We%2520demonstrated%2520that%2520by%2520using%2520the%2520pre-trained%2520encoder%2520and%2520L2-regularization%252C%250Asemantically%2520complex%2520features%2520are%2520ignored%2520in%2520favor%2520of%2520simpler%252C%2520more%2520reliable%250Acues%252C%2520substantially%2520improving%2520the%2520model%2520performance.%2520However%252C%2520this%2520improvement%250Acannot%2520be%2520captured%2520by%2520conventional%2520evaluation%2520metrics%2520such%2520as%2520F1-score%252C%2520which%250Acan%2520be%2520skewed%2520by%2520human%2520annotation%2520errors%2520treated%2520as%2520ground%2520truth.%2520Instead%252C%2520we%250Aintroduced%2520novel%2520evaluation%2520metrics%2520that%2520are%2520independent%2520of%2520the%2520annotation%250Aaccuracy.%2520Using%2520grain%2520boundary%2520detection%2520in%2520UO2%2520TEM%2520images%2520as%2520a%2520case%2520study%252C%2520we%250Afound%2520that%2520our%2520approach%2520led%2520to%2520a%252057%2525%2520increase%2520in%2520defect%2520detection%2520rate%252C%2520which%250Ais%2520a%2520robust%2520and%2520holistic%2520measure%2520of%2520model%2520performance%2520on%2520the%2520TEM%2520dataset%2520used%250Ain%2520this%2520work.%2520Finally%252C%2520we%2520showed%2520that%2520model%2520self-confidence%2520is%2520only%2520achieved%250Athrough%2520transfer%2520learning%2520and%2520fine-tuning%2520of%2520very%2520deep%2520layers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16779v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20U-Net%20Confidence%20on%20TEM%20Image%20Data%20with%20L2-Regularization%2C%0A%20%20Transfer%20Learning%2C%20and%20Deep%20Fine-Tuning&entry.906535625=Aiden%20Ochoa%20and%20Xinyuan%20Xu%20and%20Xing%20Wang&entry.1292438233=%20%20With%20ever-increasing%20data%20volumes%2C%20it%20is%20essential%20to%20develop%20automated%0Aapproaches%20for%20identifying%20nanoscale%20defects%20in%20transmission%20electron%0Amicroscopy%20%28TEM%29%20images.%20However%2C%20compared%20to%20features%20in%20conventional%0Aphotographs%2C%20nanoscale%20defects%20in%20TEM%20images%20exhibit%20far%20greater%20variation%20due%0Ato%20the%20complex%20contrast%20mechanisms%20and%20intricate%20defect%20structures.%20These%0Achallenges%20often%20result%20in%20much%20less%20labeled%20data%20and%20higher%20rates%20of%0Aannotation%20errors%2C%20posing%20significant%20obstacles%20to%20improving%20machine%20learning%0Amodel%20performance%20for%20TEM%20image%20analysis.%20To%20address%20these%20limitations%2C%20we%0Aexamined%20transfer%20learning%20by%20leveraging%20large%2C%20pre-trained%20models%20used%20for%0Anatural%20images.%0A%20%20We%20demonstrated%20that%20by%20using%20the%20pre-trained%20encoder%20and%20L2-regularization%2C%0Asemantically%20complex%20features%20are%20ignored%20in%20favor%20of%20simpler%2C%20more%20reliable%0Acues%2C%20substantially%20improving%20the%20model%20performance.%20However%2C%20this%20improvement%0Acannot%20be%20captured%20by%20conventional%20evaluation%20metrics%20such%20as%20F1-score%2C%20which%0Acan%20be%20skewed%20by%20human%20annotation%20errors%20treated%20as%20ground%20truth.%20Instead%2C%20we%0Aintroduced%20novel%20evaluation%20metrics%20that%20are%20independent%20of%20the%20annotation%0Aaccuracy.%20Using%20grain%20boundary%20detection%20in%20UO2%20TEM%20images%20as%20a%20case%20study%2C%20we%0Afound%20that%20our%20approach%20led%20to%20a%2057%25%20increase%20in%20defect%20detection%20rate%2C%20which%0Ais%20a%20robust%20and%20holistic%20measure%20of%20model%20performance%20on%20the%20TEM%20dataset%20used%0Ain%20this%20work.%20Finally%2C%20we%20showed%20that%20model%20self-confidence%20is%20only%20achieved%0Athrough%20transfer%20learning%20and%20fine-tuning%20of%20very%20deep%20layers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16779v2&entry.124074799=Read"},
{"title": "Learning Long-Range Action Representation by Two-Stream Mamba Pyramid\n  Network for Figure Skating Assessment", "author": "Fengshun Wang and Qiurui Wang and Peilin Zhao", "abstract": "  Technical Element Score (TES) and Program Component Score (PCS) evaluations\nin figure skating demand precise assessment of athletic actions and artistic\ninterpretation, respectively. Existing methods face three major challenges.\nFirstly, video and audio cues are regarded as common features for both TES and\nPCS predictions in previous works without considering the prior evaluation\ncriterion of figure skating. Secondly, action elements in competitions are\nseparated in time, TES should be derived from each element's score, but\nexisting methods try to give an overall TES prediction without evaluating each\naction element. Thirdly, lengthy competition videos make it difficult and\ninefficient to handle long-range contexts. To address these challenges, we\npropose a two-stream Mamba pyramid network that aligns with actual judging\ncriteria to predict TES and PCS by separating visual-feature based TES\nevaluation stream from audio-visual-feature based PCS evaluation stream. In the\nPCS evaluation stream, we introduce a multi-level fusion mechanism to guarantee\nthat video-based features remain unaffected when assessing TES, and enhance PCS\nestimation by fusing visual and auditory cues across each contextual level of\nthe pyramid. In the TES evaluation stream, the multi-scale Mamba pyramid and\nTES head we proposed effectively address the challenges of localizing and\nevaluating action elements with various temporal scales and give score\npredictions. With Mamba's superior ability to capture long-range dependencies\nand its linear computational complexity, our method is ideal for handling\nlengthy figure skating videos. Comprehensive experimentation demonstrates that\nour framework attains state-of-the-art performance on the FineFS benchmark. Our\nsource code is available at\nhttps://github.com/ycwfs/Figure-Skating-Action-Quality-Assessment.\n", "link": "http://arxiv.org/abs/2508.16291v1", "date": "2025-08-22", "relevancy": 2.1083, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5415}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5242}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5242}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Long-Range%20Action%20Representation%20by%20Two-Stream%20Mamba%20Pyramid%0A%20%20Network%20for%20Figure%20Skating%20Assessment&body=Title%3A%20Learning%20Long-Range%20Action%20Representation%20by%20Two-Stream%20Mamba%20Pyramid%0A%20%20Network%20for%20Figure%20Skating%20Assessment%0AAuthor%3A%20Fengshun%20Wang%20and%20Qiurui%20Wang%20and%20Peilin%20Zhao%0AAbstract%3A%20%20%20Technical%20Element%20Score%20%28TES%29%20and%20Program%20Component%20Score%20%28PCS%29%20evaluations%0Ain%20figure%20skating%20demand%20precise%20assessment%20of%20athletic%20actions%20and%20artistic%0Ainterpretation%2C%20respectively.%20Existing%20methods%20face%20three%20major%20challenges.%0AFirstly%2C%20video%20and%20audio%20cues%20are%20regarded%20as%20common%20features%20for%20both%20TES%20and%0APCS%20predictions%20in%20previous%20works%20without%20considering%20the%20prior%20evaluation%0Acriterion%20of%20figure%20skating.%20Secondly%2C%20action%20elements%20in%20competitions%20are%0Aseparated%20in%20time%2C%20TES%20should%20be%20derived%20from%20each%20element%27s%20score%2C%20but%0Aexisting%20methods%20try%20to%20give%20an%20overall%20TES%20prediction%20without%20evaluating%20each%0Aaction%20element.%20Thirdly%2C%20lengthy%20competition%20videos%20make%20it%20difficult%20and%0Ainefficient%20to%20handle%20long-range%20contexts.%20To%20address%20these%20challenges%2C%20we%0Apropose%20a%20two-stream%20Mamba%20pyramid%20network%20that%20aligns%20with%20actual%20judging%0Acriteria%20to%20predict%20TES%20and%20PCS%20by%20separating%20visual-feature%20based%20TES%0Aevaluation%20stream%20from%20audio-visual-feature%20based%20PCS%20evaluation%20stream.%20In%20the%0APCS%20evaluation%20stream%2C%20we%20introduce%20a%20multi-level%20fusion%20mechanism%20to%20guarantee%0Athat%20video-based%20features%20remain%20unaffected%20when%20assessing%20TES%2C%20and%20enhance%20PCS%0Aestimation%20by%20fusing%20visual%20and%20auditory%20cues%20across%20each%20contextual%20level%20of%0Athe%20pyramid.%20In%20the%20TES%20evaluation%20stream%2C%20the%20multi-scale%20Mamba%20pyramid%20and%0ATES%20head%20we%20proposed%20effectively%20address%20the%20challenges%20of%20localizing%20and%0Aevaluating%20action%20elements%20with%20various%20temporal%20scales%20and%20give%20score%0Apredictions.%20With%20Mamba%27s%20superior%20ability%20to%20capture%20long-range%20dependencies%0Aand%20its%20linear%20computational%20complexity%2C%20our%20method%20is%20ideal%20for%20handling%0Alengthy%20figure%20skating%20videos.%20Comprehensive%20experimentation%20demonstrates%20that%0Aour%20framework%20attains%20state-of-the-art%20performance%20on%20the%20FineFS%20benchmark.%20Our%0Asource%20code%20is%20available%20at%0Ahttps%3A//github.com/ycwfs/Figure-Skating-Action-Quality-Assessment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16291v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Long-Range%2520Action%2520Representation%2520by%2520Two-Stream%2520Mamba%2520Pyramid%250A%2520%2520Network%2520for%2520Figure%2520Skating%2520Assessment%26entry.906535625%3DFengshun%2520Wang%2520and%2520Qiurui%2520Wang%2520and%2520Peilin%2520Zhao%26entry.1292438233%3D%2520%2520Technical%2520Element%2520Score%2520%2528TES%2529%2520and%2520Program%2520Component%2520Score%2520%2528PCS%2529%2520evaluations%250Ain%2520figure%2520skating%2520demand%2520precise%2520assessment%2520of%2520athletic%2520actions%2520and%2520artistic%250Ainterpretation%252C%2520respectively.%2520Existing%2520methods%2520face%2520three%2520major%2520challenges.%250AFirstly%252C%2520video%2520and%2520audio%2520cues%2520are%2520regarded%2520as%2520common%2520features%2520for%2520both%2520TES%2520and%250APCS%2520predictions%2520in%2520previous%2520works%2520without%2520considering%2520the%2520prior%2520evaluation%250Acriterion%2520of%2520figure%2520skating.%2520Secondly%252C%2520action%2520elements%2520in%2520competitions%2520are%250Aseparated%2520in%2520time%252C%2520TES%2520should%2520be%2520derived%2520from%2520each%2520element%2527s%2520score%252C%2520but%250Aexisting%2520methods%2520try%2520to%2520give%2520an%2520overall%2520TES%2520prediction%2520without%2520evaluating%2520each%250Aaction%2520element.%2520Thirdly%252C%2520lengthy%2520competition%2520videos%2520make%2520it%2520difficult%2520and%250Ainefficient%2520to%2520handle%2520long-range%2520contexts.%2520To%2520address%2520these%2520challenges%252C%2520we%250Apropose%2520a%2520two-stream%2520Mamba%2520pyramid%2520network%2520that%2520aligns%2520with%2520actual%2520judging%250Acriteria%2520to%2520predict%2520TES%2520and%2520PCS%2520by%2520separating%2520visual-feature%2520based%2520TES%250Aevaluation%2520stream%2520from%2520audio-visual-feature%2520based%2520PCS%2520evaluation%2520stream.%2520In%2520the%250APCS%2520evaluation%2520stream%252C%2520we%2520introduce%2520a%2520multi-level%2520fusion%2520mechanism%2520to%2520guarantee%250Athat%2520video-based%2520features%2520remain%2520unaffected%2520when%2520assessing%2520TES%252C%2520and%2520enhance%2520PCS%250Aestimation%2520by%2520fusing%2520visual%2520and%2520auditory%2520cues%2520across%2520each%2520contextual%2520level%2520of%250Athe%2520pyramid.%2520In%2520the%2520TES%2520evaluation%2520stream%252C%2520the%2520multi-scale%2520Mamba%2520pyramid%2520and%250ATES%2520head%2520we%2520proposed%2520effectively%2520address%2520the%2520challenges%2520of%2520localizing%2520and%250Aevaluating%2520action%2520elements%2520with%2520various%2520temporal%2520scales%2520and%2520give%2520score%250Apredictions.%2520With%2520Mamba%2527s%2520superior%2520ability%2520to%2520capture%2520long-range%2520dependencies%250Aand%2520its%2520linear%2520computational%2520complexity%252C%2520our%2520method%2520is%2520ideal%2520for%2520handling%250Alengthy%2520figure%2520skating%2520videos.%2520Comprehensive%2520experimentation%2520demonstrates%2520that%250Aour%2520framework%2520attains%2520state-of-the-art%2520performance%2520on%2520the%2520FineFS%2520benchmark.%2520Our%250Asource%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/ycwfs/Figure-Skating-Action-Quality-Assessment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16291v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Long-Range%20Action%20Representation%20by%20Two-Stream%20Mamba%20Pyramid%0A%20%20Network%20for%20Figure%20Skating%20Assessment&entry.906535625=Fengshun%20Wang%20and%20Qiurui%20Wang%20and%20Peilin%20Zhao&entry.1292438233=%20%20Technical%20Element%20Score%20%28TES%29%20and%20Program%20Component%20Score%20%28PCS%29%20evaluations%0Ain%20figure%20skating%20demand%20precise%20assessment%20of%20athletic%20actions%20and%20artistic%0Ainterpretation%2C%20respectively.%20Existing%20methods%20face%20three%20major%20challenges.%0AFirstly%2C%20video%20and%20audio%20cues%20are%20regarded%20as%20common%20features%20for%20both%20TES%20and%0APCS%20predictions%20in%20previous%20works%20without%20considering%20the%20prior%20evaluation%0Acriterion%20of%20figure%20skating.%20Secondly%2C%20action%20elements%20in%20competitions%20are%0Aseparated%20in%20time%2C%20TES%20should%20be%20derived%20from%20each%20element%27s%20score%2C%20but%0Aexisting%20methods%20try%20to%20give%20an%20overall%20TES%20prediction%20without%20evaluating%20each%0Aaction%20element.%20Thirdly%2C%20lengthy%20competition%20videos%20make%20it%20difficult%20and%0Ainefficient%20to%20handle%20long-range%20contexts.%20To%20address%20these%20challenges%2C%20we%0Apropose%20a%20two-stream%20Mamba%20pyramid%20network%20that%20aligns%20with%20actual%20judging%0Acriteria%20to%20predict%20TES%20and%20PCS%20by%20separating%20visual-feature%20based%20TES%0Aevaluation%20stream%20from%20audio-visual-feature%20based%20PCS%20evaluation%20stream.%20In%20the%0APCS%20evaluation%20stream%2C%20we%20introduce%20a%20multi-level%20fusion%20mechanism%20to%20guarantee%0Athat%20video-based%20features%20remain%20unaffected%20when%20assessing%20TES%2C%20and%20enhance%20PCS%0Aestimation%20by%20fusing%20visual%20and%20auditory%20cues%20across%20each%20contextual%20level%20of%0Athe%20pyramid.%20In%20the%20TES%20evaluation%20stream%2C%20the%20multi-scale%20Mamba%20pyramid%20and%0ATES%20head%20we%20proposed%20effectively%20address%20the%20challenges%20of%20localizing%20and%0Aevaluating%20action%20elements%20with%20various%20temporal%20scales%20and%20give%20score%0Apredictions.%20With%20Mamba%27s%20superior%20ability%20to%20capture%20long-range%20dependencies%0Aand%20its%20linear%20computational%20complexity%2C%20our%20method%20is%20ideal%20for%20handling%0Alengthy%20figure%20skating%20videos.%20Comprehensive%20experimentation%20demonstrates%20that%0Aour%20framework%20attains%20state-of-the-art%20performance%20on%20the%20FineFS%20benchmark.%20Our%0Asource%20code%20is%20available%20at%0Ahttps%3A//github.com/ycwfs/Figure-Skating-Action-Quality-Assessment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16291v1&entry.124074799=Read"},
{"title": "Transfer Learning via Lexical Relatedness: A Sarcasm and Hate Speech\n  Case Study", "author": "Angelly Cabrera and Linus Lei and Antonio Ortega", "abstract": "  Detecting hate speech in non-direct forms, such as irony, sarcasm, and\ninnuendos, remains a persistent challenge for social networks. Although sarcasm\nand hate speech are regarded as distinct expressions, our work explores whether\nintegrating sarcasm as a pre-training step improves implicit hate speech\ndetection and, by extension, explicit hate speech detection. Incorporating\nsamples from ETHOS, Sarcasm on Reddit, and Implicit Hate Corpus, we devised two\ntraining strategies to compare the effectiveness of sarcasm pre-training on a\nCNN+LSTM and BERT+BiLSTM model. The first strategy is a single-step training\napproach, where a model trained only on sarcasm is then tested on hate speech.\nThe second strategy uses sequential transfer learning to fine-tune models for\nsarcasm, implicit hate, and explicit hate. Our results show that sarcasm\npre-training improved the BERT+BiLSTM's recall by 9.7%, AUC by 7.8%, and\nF1-score by 6% on ETHOS. On the Implicit Hate Corpus, precision increased by\n7.8% when tested only on implicit samples. By incorporating sarcasm into the\ntraining process, we show that models can more effectively detect both implicit\nand explicit hate.\n", "link": "http://arxiv.org/abs/2508.16555v1", "date": "2025-08-22", "relevancy": 2.1041, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4332}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4186}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4107}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transfer%20Learning%20via%20Lexical%20Relatedness%3A%20A%20Sarcasm%20and%20Hate%20Speech%0A%20%20Case%20Study&body=Title%3A%20Transfer%20Learning%20via%20Lexical%20Relatedness%3A%20A%20Sarcasm%20and%20Hate%20Speech%0A%20%20Case%20Study%0AAuthor%3A%20Angelly%20Cabrera%20and%20Linus%20Lei%20and%20Antonio%20Ortega%0AAbstract%3A%20%20%20Detecting%20hate%20speech%20in%20non-direct%20forms%2C%20such%20as%20irony%2C%20sarcasm%2C%20and%0Ainnuendos%2C%20remains%20a%20persistent%20challenge%20for%20social%20networks.%20Although%20sarcasm%0Aand%20hate%20speech%20are%20regarded%20as%20distinct%20expressions%2C%20our%20work%20explores%20whether%0Aintegrating%20sarcasm%20as%20a%20pre-training%20step%20improves%20implicit%20hate%20speech%0Adetection%20and%2C%20by%20extension%2C%20explicit%20hate%20speech%20detection.%20Incorporating%0Asamples%20from%20ETHOS%2C%20Sarcasm%20on%20Reddit%2C%20and%20Implicit%20Hate%20Corpus%2C%20we%20devised%20two%0Atraining%20strategies%20to%20compare%20the%20effectiveness%20of%20sarcasm%20pre-training%20on%20a%0ACNN%2BLSTM%20and%20BERT%2BBiLSTM%20model.%20The%20first%20strategy%20is%20a%20single-step%20training%0Aapproach%2C%20where%20a%20model%20trained%20only%20on%20sarcasm%20is%20then%20tested%20on%20hate%20speech.%0AThe%20second%20strategy%20uses%20sequential%20transfer%20learning%20to%20fine-tune%20models%20for%0Asarcasm%2C%20implicit%20hate%2C%20and%20explicit%20hate.%20Our%20results%20show%20that%20sarcasm%0Apre-training%20improved%20the%20BERT%2BBiLSTM%27s%20recall%20by%209.7%25%2C%20AUC%20by%207.8%25%2C%20and%0AF1-score%20by%206%25%20on%20ETHOS.%20On%20the%20Implicit%20Hate%20Corpus%2C%20precision%20increased%20by%0A7.8%25%20when%20tested%20only%20on%20implicit%20samples.%20By%20incorporating%20sarcasm%20into%20the%0Atraining%20process%2C%20we%20show%20that%20models%20can%20more%20effectively%20detect%20both%20implicit%0Aand%20explicit%20hate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16555v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransfer%2520Learning%2520via%2520Lexical%2520Relatedness%253A%2520A%2520Sarcasm%2520and%2520Hate%2520Speech%250A%2520%2520Case%2520Study%26entry.906535625%3DAngelly%2520Cabrera%2520and%2520Linus%2520Lei%2520and%2520Antonio%2520Ortega%26entry.1292438233%3D%2520%2520Detecting%2520hate%2520speech%2520in%2520non-direct%2520forms%252C%2520such%2520as%2520irony%252C%2520sarcasm%252C%2520and%250Ainnuendos%252C%2520remains%2520a%2520persistent%2520challenge%2520for%2520social%2520networks.%2520Although%2520sarcasm%250Aand%2520hate%2520speech%2520are%2520regarded%2520as%2520distinct%2520expressions%252C%2520our%2520work%2520explores%2520whether%250Aintegrating%2520sarcasm%2520as%2520a%2520pre-training%2520step%2520improves%2520implicit%2520hate%2520speech%250Adetection%2520and%252C%2520by%2520extension%252C%2520explicit%2520hate%2520speech%2520detection.%2520Incorporating%250Asamples%2520from%2520ETHOS%252C%2520Sarcasm%2520on%2520Reddit%252C%2520and%2520Implicit%2520Hate%2520Corpus%252C%2520we%2520devised%2520two%250Atraining%2520strategies%2520to%2520compare%2520the%2520effectiveness%2520of%2520sarcasm%2520pre-training%2520on%2520a%250ACNN%252BLSTM%2520and%2520BERT%252BBiLSTM%2520model.%2520The%2520first%2520strategy%2520is%2520a%2520single-step%2520training%250Aapproach%252C%2520where%2520a%2520model%2520trained%2520only%2520on%2520sarcasm%2520is%2520then%2520tested%2520on%2520hate%2520speech.%250AThe%2520second%2520strategy%2520uses%2520sequential%2520transfer%2520learning%2520to%2520fine-tune%2520models%2520for%250Asarcasm%252C%2520implicit%2520hate%252C%2520and%2520explicit%2520hate.%2520Our%2520results%2520show%2520that%2520sarcasm%250Apre-training%2520improved%2520the%2520BERT%252BBiLSTM%2527s%2520recall%2520by%25209.7%2525%252C%2520AUC%2520by%25207.8%2525%252C%2520and%250AF1-score%2520by%25206%2525%2520on%2520ETHOS.%2520On%2520the%2520Implicit%2520Hate%2520Corpus%252C%2520precision%2520increased%2520by%250A7.8%2525%2520when%2520tested%2520only%2520on%2520implicit%2520samples.%2520By%2520incorporating%2520sarcasm%2520into%2520the%250Atraining%2520process%252C%2520we%2520show%2520that%2520models%2520can%2520more%2520effectively%2520detect%2520both%2520implicit%250Aand%2520explicit%2520hate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16555v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transfer%20Learning%20via%20Lexical%20Relatedness%3A%20A%20Sarcasm%20and%20Hate%20Speech%0A%20%20Case%20Study&entry.906535625=Angelly%20Cabrera%20and%20Linus%20Lei%20and%20Antonio%20Ortega&entry.1292438233=%20%20Detecting%20hate%20speech%20in%20non-direct%20forms%2C%20such%20as%20irony%2C%20sarcasm%2C%20and%0Ainnuendos%2C%20remains%20a%20persistent%20challenge%20for%20social%20networks.%20Although%20sarcasm%0Aand%20hate%20speech%20are%20regarded%20as%20distinct%20expressions%2C%20our%20work%20explores%20whether%0Aintegrating%20sarcasm%20as%20a%20pre-training%20step%20improves%20implicit%20hate%20speech%0Adetection%20and%2C%20by%20extension%2C%20explicit%20hate%20speech%20detection.%20Incorporating%0Asamples%20from%20ETHOS%2C%20Sarcasm%20on%20Reddit%2C%20and%20Implicit%20Hate%20Corpus%2C%20we%20devised%20two%0Atraining%20strategies%20to%20compare%20the%20effectiveness%20of%20sarcasm%20pre-training%20on%20a%0ACNN%2BLSTM%20and%20BERT%2BBiLSTM%20model.%20The%20first%20strategy%20is%20a%20single-step%20training%0Aapproach%2C%20where%20a%20model%20trained%20only%20on%20sarcasm%20is%20then%20tested%20on%20hate%20speech.%0AThe%20second%20strategy%20uses%20sequential%20transfer%20learning%20to%20fine-tune%20models%20for%0Asarcasm%2C%20implicit%20hate%2C%20and%20explicit%20hate.%20Our%20results%20show%20that%20sarcasm%0Apre-training%20improved%20the%20BERT%2BBiLSTM%27s%20recall%20by%209.7%25%2C%20AUC%20by%207.8%25%2C%20and%0AF1-score%20by%206%25%20on%20ETHOS.%20On%20the%20Implicit%20Hate%20Corpus%2C%20precision%20increased%20by%0A7.8%25%20when%20tested%20only%20on%20implicit%20samples.%20By%20incorporating%20sarcasm%20into%20the%0Atraining%20process%2C%20we%20show%20that%20models%20can%20more%20effectively%20detect%20both%20implicit%0Aand%20explicit%20hate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16555v1&entry.124074799=Read"},
{"title": "Multi-Level Knowledge Distillation and Dynamic Self-Supervised Learning\n  for Continual Learning", "author": "Taeheon Kim and San Kim and Minhyuk Seo and Dongjae Jeon and Wonje Jeung and Jonghyun Choi", "abstract": "  Class-incremental with repetition (CIR), where previously trained classes\nrepeatedly introduced in future tasks, is a more realistic scenario than the\ntraditional class incremental setup, which assumes that each task contains\nunseen classes. CIR assumes that we can easily access abundant unlabeled data\nfrom external sources, such as the Internet. Therefore, we propose two\ncomponents that efficiently use the unlabeled data to ensure the high stability\nand the plasticity of models trained in CIR setup. First, we introduce\nmulti-level knowledge distillation (MLKD) that distills knowledge from multiple\nprevious models across multiple perspectives, including features and logits, so\nthe model can maintain much various previous knowledge. Moreover, we implement\ndynamic self-supervised loss (SSL) to utilize the unlabeled data that\naccelerates the learning of new classes, while dynamic weighting of SSL keeps\nthe focus of training to the primary task. Both of our proposed components\nsignificantly improve the performance in CIR setup, achieving 2nd place in the\nCVPR 5th CLVISION Challenge.\n", "link": "http://arxiv.org/abs/2508.12692v2", "date": "2025-08-22", "relevancy": 2.0889, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5307}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5227}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5184}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Level%20Knowledge%20Distillation%20and%20Dynamic%20Self-Supervised%20Learning%0A%20%20for%20Continual%20Learning&body=Title%3A%20Multi-Level%20Knowledge%20Distillation%20and%20Dynamic%20Self-Supervised%20Learning%0A%20%20for%20Continual%20Learning%0AAuthor%3A%20Taeheon%20Kim%20and%20San%20Kim%20and%20Minhyuk%20Seo%20and%20Dongjae%20Jeon%20and%20Wonje%20Jeung%20and%20Jonghyun%20Choi%0AAbstract%3A%20%20%20Class-incremental%20with%20repetition%20%28CIR%29%2C%20where%20previously%20trained%20classes%0Arepeatedly%20introduced%20in%20future%20tasks%2C%20is%20a%20more%20realistic%20scenario%20than%20the%0Atraditional%20class%20incremental%20setup%2C%20which%20assumes%20that%20each%20task%20contains%0Aunseen%20classes.%20CIR%20assumes%20that%20we%20can%20easily%20access%20abundant%20unlabeled%20data%0Afrom%20external%20sources%2C%20such%20as%20the%20Internet.%20Therefore%2C%20we%20propose%20two%0Acomponents%20that%20efficiently%20use%20the%20unlabeled%20data%20to%20ensure%20the%20high%20stability%0Aand%20the%20plasticity%20of%20models%20trained%20in%20CIR%20setup.%20First%2C%20we%20introduce%0Amulti-level%20knowledge%20distillation%20%28MLKD%29%20that%20distills%20knowledge%20from%20multiple%0Aprevious%20models%20across%20multiple%20perspectives%2C%20including%20features%20and%20logits%2C%20so%0Athe%20model%20can%20maintain%20much%20various%20previous%20knowledge.%20Moreover%2C%20we%20implement%0Adynamic%20self-supervised%20loss%20%28SSL%29%20to%20utilize%20the%20unlabeled%20data%20that%0Aaccelerates%20the%20learning%20of%20new%20classes%2C%20while%20dynamic%20weighting%20of%20SSL%20keeps%0Athe%20focus%20of%20training%20to%20the%20primary%20task.%20Both%20of%20our%20proposed%20components%0Asignificantly%20improve%20the%20performance%20in%20CIR%20setup%2C%20achieving%202nd%20place%20in%20the%0ACVPR%205th%20CLVISION%20Challenge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12692v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Level%2520Knowledge%2520Distillation%2520and%2520Dynamic%2520Self-Supervised%2520Learning%250A%2520%2520for%2520Continual%2520Learning%26entry.906535625%3DTaeheon%2520Kim%2520and%2520San%2520Kim%2520and%2520Minhyuk%2520Seo%2520and%2520Dongjae%2520Jeon%2520and%2520Wonje%2520Jeung%2520and%2520Jonghyun%2520Choi%26entry.1292438233%3D%2520%2520Class-incremental%2520with%2520repetition%2520%2528CIR%2529%252C%2520where%2520previously%2520trained%2520classes%250Arepeatedly%2520introduced%2520in%2520future%2520tasks%252C%2520is%2520a%2520more%2520realistic%2520scenario%2520than%2520the%250Atraditional%2520class%2520incremental%2520setup%252C%2520which%2520assumes%2520that%2520each%2520task%2520contains%250Aunseen%2520classes.%2520CIR%2520assumes%2520that%2520we%2520can%2520easily%2520access%2520abundant%2520unlabeled%2520data%250Afrom%2520external%2520sources%252C%2520such%2520as%2520the%2520Internet.%2520Therefore%252C%2520we%2520propose%2520two%250Acomponents%2520that%2520efficiently%2520use%2520the%2520unlabeled%2520data%2520to%2520ensure%2520the%2520high%2520stability%250Aand%2520the%2520plasticity%2520of%2520models%2520trained%2520in%2520CIR%2520setup.%2520First%252C%2520we%2520introduce%250Amulti-level%2520knowledge%2520distillation%2520%2528MLKD%2529%2520that%2520distills%2520knowledge%2520from%2520multiple%250Aprevious%2520models%2520across%2520multiple%2520perspectives%252C%2520including%2520features%2520and%2520logits%252C%2520so%250Athe%2520model%2520can%2520maintain%2520much%2520various%2520previous%2520knowledge.%2520Moreover%252C%2520we%2520implement%250Adynamic%2520self-supervised%2520loss%2520%2528SSL%2529%2520to%2520utilize%2520the%2520unlabeled%2520data%2520that%250Aaccelerates%2520the%2520learning%2520of%2520new%2520classes%252C%2520while%2520dynamic%2520weighting%2520of%2520SSL%2520keeps%250Athe%2520focus%2520of%2520training%2520to%2520the%2520primary%2520task.%2520Both%2520of%2520our%2520proposed%2520components%250Asignificantly%2520improve%2520the%2520performance%2520in%2520CIR%2520setup%252C%2520achieving%25202nd%2520place%2520in%2520the%250ACVPR%25205th%2520CLVISION%2520Challenge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12692v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Level%20Knowledge%20Distillation%20and%20Dynamic%20Self-Supervised%20Learning%0A%20%20for%20Continual%20Learning&entry.906535625=Taeheon%20Kim%20and%20San%20Kim%20and%20Minhyuk%20Seo%20and%20Dongjae%20Jeon%20and%20Wonje%20Jeung%20and%20Jonghyun%20Choi&entry.1292438233=%20%20Class-incremental%20with%20repetition%20%28CIR%29%2C%20where%20previously%20trained%20classes%0Arepeatedly%20introduced%20in%20future%20tasks%2C%20is%20a%20more%20realistic%20scenario%20than%20the%0Atraditional%20class%20incremental%20setup%2C%20which%20assumes%20that%20each%20task%20contains%0Aunseen%20classes.%20CIR%20assumes%20that%20we%20can%20easily%20access%20abundant%20unlabeled%20data%0Afrom%20external%20sources%2C%20such%20as%20the%20Internet.%20Therefore%2C%20we%20propose%20two%0Acomponents%20that%20efficiently%20use%20the%20unlabeled%20data%20to%20ensure%20the%20high%20stability%0Aand%20the%20plasticity%20of%20models%20trained%20in%20CIR%20setup.%20First%2C%20we%20introduce%0Amulti-level%20knowledge%20distillation%20%28MLKD%29%20that%20distills%20knowledge%20from%20multiple%0Aprevious%20models%20across%20multiple%20perspectives%2C%20including%20features%20and%20logits%2C%20so%0Athe%20model%20can%20maintain%20much%20various%20previous%20knowledge.%20Moreover%2C%20we%20implement%0Adynamic%20self-supervised%20loss%20%28SSL%29%20to%20utilize%20the%20unlabeled%20data%20that%0Aaccelerates%20the%20learning%20of%20new%20classes%2C%20while%20dynamic%20weighting%20of%20SSL%20keeps%0Athe%20focus%20of%20training%20to%20the%20primary%20task.%20Both%20of%20our%20proposed%20components%0Asignificantly%20improve%20the%20performance%20in%20CIR%20setup%2C%20achieving%202nd%20place%20in%20the%0ACVPR%205th%20CLVISION%20Challenge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12692v2&entry.124074799=Read"},
{"title": "Integrated Noise and Safety Management in UAM via A Unified\n  Reinforcement Learning Framework", "author": "Surya Murthy and Zhenyu Gao and John-Paul Clarke and Ufuk Topcu", "abstract": "  Urban Air Mobility (UAM) envisions the widespread use of small aerial\nvehicles to transform transportation in dense urban environments. However, UAM\nfaces critical operational challenges, particularly the balance between\nminimizing noise exposure and maintaining safe separation in low-altitude urban\nairspace, two objectives that are often addressed separately. We propose a\nreinforcement learning (RL)-based air traffic management system that integrates\nboth noise and safety considerations within a unified, decentralized framework.\nUnder this scalable air traffic coordination solution, agents operate in a\nstructured, multi-layered airspace and learn altitude adjustment policies to\njointly manage noise impact and separation constraints. The system demonstrates\nstrong performance across both objectives and reveals tradeoffs among\nseparation, noise exposure, and energy efficiency under high traffic density.\nThe findings highlight the potential of RL and multi-objective coordination\nstrategies in enhancing the safety, quietness, and efficiency of UAM\noperations.\n", "link": "http://arxiv.org/abs/2508.16440v1", "date": "2025-08-22", "relevancy": 2.0723, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5529}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4949}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4925}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrated%20Noise%20and%20Safety%20Management%20in%20UAM%20via%20A%20Unified%0A%20%20Reinforcement%20Learning%20Framework&body=Title%3A%20Integrated%20Noise%20and%20Safety%20Management%20in%20UAM%20via%20A%20Unified%0A%20%20Reinforcement%20Learning%20Framework%0AAuthor%3A%20Surya%20Murthy%20and%20Zhenyu%20Gao%20and%20John-Paul%20Clarke%20and%20Ufuk%20Topcu%0AAbstract%3A%20%20%20Urban%20Air%20Mobility%20%28UAM%29%20envisions%20the%20widespread%20use%20of%20small%20aerial%0Avehicles%20to%20transform%20transportation%20in%20dense%20urban%20environments.%20However%2C%20UAM%0Afaces%20critical%20operational%20challenges%2C%20particularly%20the%20balance%20between%0Aminimizing%20noise%20exposure%20and%20maintaining%20safe%20separation%20in%20low-altitude%20urban%0Aairspace%2C%20two%20objectives%20that%20are%20often%20addressed%20separately.%20We%20propose%20a%0Areinforcement%20learning%20%28RL%29-based%20air%20traffic%20management%20system%20that%20integrates%0Aboth%20noise%20and%20safety%20considerations%20within%20a%20unified%2C%20decentralized%20framework.%0AUnder%20this%20scalable%20air%20traffic%20coordination%20solution%2C%20agents%20operate%20in%20a%0Astructured%2C%20multi-layered%20airspace%20and%20learn%20altitude%20adjustment%20policies%20to%0Ajointly%20manage%20noise%20impact%20and%20separation%20constraints.%20The%20system%20demonstrates%0Astrong%20performance%20across%20both%20objectives%20and%20reveals%20tradeoffs%20among%0Aseparation%2C%20noise%20exposure%2C%20and%20energy%20efficiency%20under%20high%20traffic%20density.%0AThe%20findings%20highlight%20the%20potential%20of%20RL%20and%20multi-objective%20coordination%0Astrategies%20in%20enhancing%20the%20safety%2C%20quietness%2C%20and%20efficiency%20of%20UAM%0Aoperations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16440v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrated%2520Noise%2520and%2520Safety%2520Management%2520in%2520UAM%2520via%2520A%2520Unified%250A%2520%2520Reinforcement%2520Learning%2520Framework%26entry.906535625%3DSurya%2520Murthy%2520and%2520Zhenyu%2520Gao%2520and%2520John-Paul%2520Clarke%2520and%2520Ufuk%2520Topcu%26entry.1292438233%3D%2520%2520Urban%2520Air%2520Mobility%2520%2528UAM%2529%2520envisions%2520the%2520widespread%2520use%2520of%2520small%2520aerial%250Avehicles%2520to%2520transform%2520transportation%2520in%2520dense%2520urban%2520environments.%2520However%252C%2520UAM%250Afaces%2520critical%2520operational%2520challenges%252C%2520particularly%2520the%2520balance%2520between%250Aminimizing%2520noise%2520exposure%2520and%2520maintaining%2520safe%2520separation%2520in%2520low-altitude%2520urban%250Aairspace%252C%2520two%2520objectives%2520that%2520are%2520often%2520addressed%2520separately.%2520We%2520propose%2520a%250Areinforcement%2520learning%2520%2528RL%2529-based%2520air%2520traffic%2520management%2520system%2520that%2520integrates%250Aboth%2520noise%2520and%2520safety%2520considerations%2520within%2520a%2520unified%252C%2520decentralized%2520framework.%250AUnder%2520this%2520scalable%2520air%2520traffic%2520coordination%2520solution%252C%2520agents%2520operate%2520in%2520a%250Astructured%252C%2520multi-layered%2520airspace%2520and%2520learn%2520altitude%2520adjustment%2520policies%2520to%250Ajointly%2520manage%2520noise%2520impact%2520and%2520separation%2520constraints.%2520The%2520system%2520demonstrates%250Astrong%2520performance%2520across%2520both%2520objectives%2520and%2520reveals%2520tradeoffs%2520among%250Aseparation%252C%2520noise%2520exposure%252C%2520and%2520energy%2520efficiency%2520under%2520high%2520traffic%2520density.%250AThe%2520findings%2520highlight%2520the%2520potential%2520of%2520RL%2520and%2520multi-objective%2520coordination%250Astrategies%2520in%2520enhancing%2520the%2520safety%252C%2520quietness%252C%2520and%2520efficiency%2520of%2520UAM%250Aoperations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16440v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrated%20Noise%20and%20Safety%20Management%20in%20UAM%20via%20A%20Unified%0A%20%20Reinforcement%20Learning%20Framework&entry.906535625=Surya%20Murthy%20and%20Zhenyu%20Gao%20and%20John-Paul%20Clarke%20and%20Ufuk%20Topcu&entry.1292438233=%20%20Urban%20Air%20Mobility%20%28UAM%29%20envisions%20the%20widespread%20use%20of%20small%20aerial%0Avehicles%20to%20transform%20transportation%20in%20dense%20urban%20environments.%20However%2C%20UAM%0Afaces%20critical%20operational%20challenges%2C%20particularly%20the%20balance%20between%0Aminimizing%20noise%20exposure%20and%20maintaining%20safe%20separation%20in%20low-altitude%20urban%0Aairspace%2C%20two%20objectives%20that%20are%20often%20addressed%20separately.%20We%20propose%20a%0Areinforcement%20learning%20%28RL%29-based%20air%20traffic%20management%20system%20that%20integrates%0Aboth%20noise%20and%20safety%20considerations%20within%20a%20unified%2C%20decentralized%20framework.%0AUnder%20this%20scalable%20air%20traffic%20coordination%20solution%2C%20agents%20operate%20in%20a%0Astructured%2C%20multi-layered%20airspace%20and%20learn%20altitude%20adjustment%20policies%20to%0Ajointly%20manage%20noise%20impact%20and%20separation%20constraints.%20The%20system%20demonstrates%0Astrong%20performance%20across%20both%20objectives%20and%20reveals%20tradeoffs%20among%0Aseparation%2C%20noise%20exposure%2C%20and%20energy%20efficiency%20under%20high%20traffic%20density.%0AThe%20findings%20highlight%20the%20potential%20of%20RL%20and%20multi-objective%20coordination%0Astrategies%20in%20enhancing%20the%20safety%2C%20quietness%2C%20and%20efficiency%20of%20UAM%0Aoperations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16440v1&entry.124074799=Read"},
{"title": "Closer to Reality: Practical Semi-Supervised Federated Learning for\n  Foundation Model Adaptation", "author": "Guangyu Sun and Jingtao Li and Weiming Zhuang and Chen Chen and Chen Chen and Lingjuan Lyu", "abstract": "  Foundation models (FMs) exhibit remarkable generalization but require\nadaptation to downstream tasks, particularly in privacy-sensitive applications.\nDue to data privacy regulations, cloud-based FMs cannot directly access private\nedge data, limiting their adaptation. Federated learning (FL) provides a\nprivacy-aware alternative, but existing FL approaches overlook the constraints\nimposed by edge devices -- namely, limited computational resources and the\nscarcity of labeled data. To address these challenges, we introduce Practical\nSemi-Supervised Federated Learning (PSSFL), where edge devices hold only\nunlabeled, low-resolution data, while the server has limited labeled,\nhigh-resolution data. In this setting, we propose the Federated Mixture of\nExperts (FedMox), a novel framework that enhances FM adaptation in FL. FedMox\ntackles computational and resolution mismatch challenges via a sparse\nMixture-of-Experts architecture, employing a spatial router to align features\nacross resolutions and a Soft-Mixture strategy to stabilize semi-supervised\nlearning. We take object detection as a case study, and experiments on\nreal-world autonomous driving datasets demonstrate that FedMox effectively\nadapts FMs under PSSFL, significantly improving performance with constrained\nmemory costs on edge devices. Our work paves the way for scalable and\nprivacy-preserving FM adaptation in federated scenarios.\n", "link": "http://arxiv.org/abs/2508.16568v1", "date": "2025-08-22", "relevancy": 2.0679, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5192}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5162}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Closer%20to%20Reality%3A%20Practical%20Semi-Supervised%20Federated%20Learning%20for%0A%20%20Foundation%20Model%20Adaptation&body=Title%3A%20Closer%20to%20Reality%3A%20Practical%20Semi-Supervised%20Federated%20Learning%20for%0A%20%20Foundation%20Model%20Adaptation%0AAuthor%3A%20Guangyu%20Sun%20and%20Jingtao%20Li%20and%20Weiming%20Zhuang%20and%20Chen%20Chen%20and%20Chen%20Chen%20and%20Lingjuan%20Lyu%0AAbstract%3A%20%20%20Foundation%20models%20%28FMs%29%20exhibit%20remarkable%20generalization%20but%20require%0Aadaptation%20to%20downstream%20tasks%2C%20particularly%20in%20privacy-sensitive%20applications.%0ADue%20to%20data%20privacy%20regulations%2C%20cloud-based%20FMs%20cannot%20directly%20access%20private%0Aedge%20data%2C%20limiting%20their%20adaptation.%20Federated%20learning%20%28FL%29%20provides%20a%0Aprivacy-aware%20alternative%2C%20but%20existing%20FL%20approaches%20overlook%20the%20constraints%0Aimposed%20by%20edge%20devices%20--%20namely%2C%20limited%20computational%20resources%20and%20the%0Ascarcity%20of%20labeled%20data.%20To%20address%20these%20challenges%2C%20we%20introduce%20Practical%0ASemi-Supervised%20Federated%20Learning%20%28PSSFL%29%2C%20where%20edge%20devices%20hold%20only%0Aunlabeled%2C%20low-resolution%20data%2C%20while%20the%20server%20has%20limited%20labeled%2C%0Ahigh-resolution%20data.%20In%20this%20setting%2C%20we%20propose%20the%20Federated%20Mixture%20of%0AExperts%20%28FedMox%29%2C%20a%20novel%20framework%20that%20enhances%20FM%20adaptation%20in%20FL.%20FedMox%0Atackles%20computational%20and%20resolution%20mismatch%20challenges%20via%20a%20sparse%0AMixture-of-Experts%20architecture%2C%20employing%20a%20spatial%20router%20to%20align%20features%0Aacross%20resolutions%20and%20a%20Soft-Mixture%20strategy%20to%20stabilize%20semi-supervised%0Alearning.%20We%20take%20object%20detection%20as%20a%20case%20study%2C%20and%20experiments%20on%0Areal-world%20autonomous%20driving%20datasets%20demonstrate%20that%20FedMox%20effectively%0Aadapts%20FMs%20under%20PSSFL%2C%20significantly%20improving%20performance%20with%20constrained%0Amemory%20costs%20on%20edge%20devices.%20Our%20work%20paves%20the%20way%20for%20scalable%20and%0Aprivacy-preserving%20FM%20adaptation%20in%20federated%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16568v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCloser%2520to%2520Reality%253A%2520Practical%2520Semi-Supervised%2520Federated%2520Learning%2520for%250A%2520%2520Foundation%2520Model%2520Adaptation%26entry.906535625%3DGuangyu%2520Sun%2520and%2520Jingtao%2520Li%2520and%2520Weiming%2520Zhuang%2520and%2520Chen%2520Chen%2520and%2520Chen%2520Chen%2520and%2520Lingjuan%2520Lyu%26entry.1292438233%3D%2520%2520Foundation%2520models%2520%2528FMs%2529%2520exhibit%2520remarkable%2520generalization%2520but%2520require%250Aadaptation%2520to%2520downstream%2520tasks%252C%2520particularly%2520in%2520privacy-sensitive%2520applications.%250ADue%2520to%2520data%2520privacy%2520regulations%252C%2520cloud-based%2520FMs%2520cannot%2520directly%2520access%2520private%250Aedge%2520data%252C%2520limiting%2520their%2520adaptation.%2520Federated%2520learning%2520%2528FL%2529%2520provides%2520a%250Aprivacy-aware%2520alternative%252C%2520but%2520existing%2520FL%2520approaches%2520overlook%2520the%2520constraints%250Aimposed%2520by%2520edge%2520devices%2520--%2520namely%252C%2520limited%2520computational%2520resources%2520and%2520the%250Ascarcity%2520of%2520labeled%2520data.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520Practical%250ASemi-Supervised%2520Federated%2520Learning%2520%2528PSSFL%2529%252C%2520where%2520edge%2520devices%2520hold%2520only%250Aunlabeled%252C%2520low-resolution%2520data%252C%2520while%2520the%2520server%2520has%2520limited%2520labeled%252C%250Ahigh-resolution%2520data.%2520In%2520this%2520setting%252C%2520we%2520propose%2520the%2520Federated%2520Mixture%2520of%250AExperts%2520%2528FedMox%2529%252C%2520a%2520novel%2520framework%2520that%2520enhances%2520FM%2520adaptation%2520in%2520FL.%2520FedMox%250Atackles%2520computational%2520and%2520resolution%2520mismatch%2520challenges%2520via%2520a%2520sparse%250AMixture-of-Experts%2520architecture%252C%2520employing%2520a%2520spatial%2520router%2520to%2520align%2520features%250Aacross%2520resolutions%2520and%2520a%2520Soft-Mixture%2520strategy%2520to%2520stabilize%2520semi-supervised%250Alearning.%2520We%2520take%2520object%2520detection%2520as%2520a%2520case%2520study%252C%2520and%2520experiments%2520on%250Areal-world%2520autonomous%2520driving%2520datasets%2520demonstrate%2520that%2520FedMox%2520effectively%250Aadapts%2520FMs%2520under%2520PSSFL%252C%2520significantly%2520improving%2520performance%2520with%2520constrained%250Amemory%2520costs%2520on%2520edge%2520devices.%2520Our%2520work%2520paves%2520the%2520way%2520for%2520scalable%2520and%250Aprivacy-preserving%2520FM%2520adaptation%2520in%2520federated%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16568v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Closer%20to%20Reality%3A%20Practical%20Semi-Supervised%20Federated%20Learning%20for%0A%20%20Foundation%20Model%20Adaptation&entry.906535625=Guangyu%20Sun%20and%20Jingtao%20Li%20and%20Weiming%20Zhuang%20and%20Chen%20Chen%20and%20Chen%20Chen%20and%20Lingjuan%20Lyu&entry.1292438233=%20%20Foundation%20models%20%28FMs%29%20exhibit%20remarkable%20generalization%20but%20require%0Aadaptation%20to%20downstream%20tasks%2C%20particularly%20in%20privacy-sensitive%20applications.%0ADue%20to%20data%20privacy%20regulations%2C%20cloud-based%20FMs%20cannot%20directly%20access%20private%0Aedge%20data%2C%20limiting%20their%20adaptation.%20Federated%20learning%20%28FL%29%20provides%20a%0Aprivacy-aware%20alternative%2C%20but%20existing%20FL%20approaches%20overlook%20the%20constraints%0Aimposed%20by%20edge%20devices%20--%20namely%2C%20limited%20computational%20resources%20and%20the%0Ascarcity%20of%20labeled%20data.%20To%20address%20these%20challenges%2C%20we%20introduce%20Practical%0ASemi-Supervised%20Federated%20Learning%20%28PSSFL%29%2C%20where%20edge%20devices%20hold%20only%0Aunlabeled%2C%20low-resolution%20data%2C%20while%20the%20server%20has%20limited%20labeled%2C%0Ahigh-resolution%20data.%20In%20this%20setting%2C%20we%20propose%20the%20Federated%20Mixture%20of%0AExperts%20%28FedMox%29%2C%20a%20novel%20framework%20that%20enhances%20FM%20adaptation%20in%20FL.%20FedMox%0Atackles%20computational%20and%20resolution%20mismatch%20challenges%20via%20a%20sparse%0AMixture-of-Experts%20architecture%2C%20employing%20a%20spatial%20router%20to%20align%20features%0Aacross%20resolutions%20and%20a%20Soft-Mixture%20strategy%20to%20stabilize%20semi-supervised%0Alearning.%20We%20take%20object%20detection%20as%20a%20case%20study%2C%20and%20experiments%20on%0Areal-world%20autonomous%20driving%20datasets%20demonstrate%20that%20FedMox%20effectively%0Aadapts%20FMs%20under%20PSSFL%2C%20significantly%20improving%20performance%20with%20constrained%0Amemory%20costs%20on%20edge%20devices.%20Our%20work%20paves%20the%20way%20for%20scalable%20and%0Aprivacy-preserving%20FM%20adaptation%20in%20federated%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16568v1&entry.124074799=Read"},
{"title": "PAR-AdvGAN: Improving Adversarial Attack Capability with Progressive\n  Auto-Regression AdvGAN", "author": "Jiayu Zhang and Zhiyu Zhu and Xinyi Wang and Silin Liao and Zhibo Jin and Flora D. Salim and Huaming Chen", "abstract": "  Deep neural networks have demonstrated remarkable performance across various\ndomains. However, they are vulnerable to adversarial examples, which can lead\nto erroneous predictions. Generative Adversarial Networks (GANs) can leverage\nthe generators and discriminators model to quickly produce high-quality\nadversarial examples. Since both modules train in a competitive and\nsimultaneous manner, GAN-based algorithms like AdvGAN can generate adversarial\nexamples with better transferability compared to traditional methods. However,\nthe generation of perturbations is usually limited to a single iteration,\npreventing these examples from fully exploiting the potential of the methods.\nTo tackle this issue, we introduce a novel approach named Progressive\nAuto-Regression AdvGAN (PAR-AdvGAN). It incorporates an auto-regressive\niteration mechanism within a progressive generation network to craft\nadversarial examples with enhanced attack capability. We thoroughly evaluate\nour PAR-AdvGAN method with a large-scale experiment, demonstrating its superior\nperformance over various state-of-the-art black-box adversarial attacks, as\nwell as the original AdvGAN.Moreover, PAR-AdvGAN significantly accelerates the\nadversarial example generation, i.e., achieving the speeds of up to 335.5\nframes per second on Inception-v3 model, outperforming the gradient-based\ntransferable attack algorithms. Our code is available at:\nhttps://github.com/LMBTough/PAR\n", "link": "http://arxiv.org/abs/2502.12207v4", "date": "2025-08-22", "relevancy": 2.0586, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5228}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5133}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5127}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PAR-AdvGAN%3A%20Improving%20Adversarial%20Attack%20Capability%20with%20Progressive%0A%20%20Auto-Regression%20AdvGAN&body=Title%3A%20PAR-AdvGAN%3A%20Improving%20Adversarial%20Attack%20Capability%20with%20Progressive%0A%20%20Auto-Regression%20AdvGAN%0AAuthor%3A%20Jiayu%20Zhang%20and%20Zhiyu%20Zhu%20and%20Xinyi%20Wang%20and%20Silin%20Liao%20and%20Zhibo%20Jin%20and%20Flora%20D.%20Salim%20and%20Huaming%20Chen%0AAbstract%3A%20%20%20Deep%20neural%20networks%20have%20demonstrated%20remarkable%20performance%20across%20various%0Adomains.%20However%2C%20they%20are%20vulnerable%20to%20adversarial%20examples%2C%20which%20can%20lead%0Ato%20erroneous%20predictions.%20Generative%20Adversarial%20Networks%20%28GANs%29%20can%20leverage%0Athe%20generators%20and%20discriminators%20model%20to%20quickly%20produce%20high-quality%0Aadversarial%20examples.%20Since%20both%20modules%20train%20in%20a%20competitive%20and%0Asimultaneous%20manner%2C%20GAN-based%20algorithms%20like%20AdvGAN%20can%20generate%20adversarial%0Aexamples%20with%20better%20transferability%20compared%20to%20traditional%20methods.%20However%2C%0Athe%20generation%20of%20perturbations%20is%20usually%20limited%20to%20a%20single%20iteration%2C%0Apreventing%20these%20examples%20from%20fully%20exploiting%20the%20potential%20of%20the%20methods.%0ATo%20tackle%20this%20issue%2C%20we%20introduce%20a%20novel%20approach%20named%20Progressive%0AAuto-Regression%20AdvGAN%20%28PAR-AdvGAN%29.%20It%20incorporates%20an%20auto-regressive%0Aiteration%20mechanism%20within%20a%20progressive%20generation%20network%20to%20craft%0Aadversarial%20examples%20with%20enhanced%20attack%20capability.%20We%20thoroughly%20evaluate%0Aour%20PAR-AdvGAN%20method%20with%20a%20large-scale%20experiment%2C%20demonstrating%20its%20superior%0Aperformance%20over%20various%20state-of-the-art%20black-box%20adversarial%20attacks%2C%20as%0Awell%20as%20the%20original%20AdvGAN.Moreover%2C%20PAR-AdvGAN%20significantly%20accelerates%20the%0Aadversarial%20example%20generation%2C%20i.e.%2C%20achieving%20the%20speeds%20of%20up%20to%20335.5%0Aframes%20per%20second%20on%20Inception-v3%20model%2C%20outperforming%20the%20gradient-based%0Atransferable%20attack%20algorithms.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/LMBTough/PAR%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12207v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPAR-AdvGAN%253A%2520Improving%2520Adversarial%2520Attack%2520Capability%2520with%2520Progressive%250A%2520%2520Auto-Regression%2520AdvGAN%26entry.906535625%3DJiayu%2520Zhang%2520and%2520Zhiyu%2520Zhu%2520and%2520Xinyi%2520Wang%2520and%2520Silin%2520Liao%2520and%2520Zhibo%2520Jin%2520and%2520Flora%2520D.%2520Salim%2520and%2520Huaming%2520Chen%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520have%2520demonstrated%2520remarkable%2520performance%2520across%2520various%250Adomains.%2520However%252C%2520they%2520are%2520vulnerable%2520to%2520adversarial%2520examples%252C%2520which%2520can%2520lead%250Ato%2520erroneous%2520predictions.%2520Generative%2520Adversarial%2520Networks%2520%2528GANs%2529%2520can%2520leverage%250Athe%2520generators%2520and%2520discriminators%2520model%2520to%2520quickly%2520produce%2520high-quality%250Aadversarial%2520examples.%2520Since%2520both%2520modules%2520train%2520in%2520a%2520competitive%2520and%250Asimultaneous%2520manner%252C%2520GAN-based%2520algorithms%2520like%2520AdvGAN%2520can%2520generate%2520adversarial%250Aexamples%2520with%2520better%2520transferability%2520compared%2520to%2520traditional%2520methods.%2520However%252C%250Athe%2520generation%2520of%2520perturbations%2520is%2520usually%2520limited%2520to%2520a%2520single%2520iteration%252C%250Apreventing%2520these%2520examples%2520from%2520fully%2520exploiting%2520the%2520potential%2520of%2520the%2520methods.%250ATo%2520tackle%2520this%2520issue%252C%2520we%2520introduce%2520a%2520novel%2520approach%2520named%2520Progressive%250AAuto-Regression%2520AdvGAN%2520%2528PAR-AdvGAN%2529.%2520It%2520incorporates%2520an%2520auto-regressive%250Aiteration%2520mechanism%2520within%2520a%2520progressive%2520generation%2520network%2520to%2520craft%250Aadversarial%2520examples%2520with%2520enhanced%2520attack%2520capability.%2520We%2520thoroughly%2520evaluate%250Aour%2520PAR-AdvGAN%2520method%2520with%2520a%2520large-scale%2520experiment%252C%2520demonstrating%2520its%2520superior%250Aperformance%2520over%2520various%2520state-of-the-art%2520black-box%2520adversarial%2520attacks%252C%2520as%250Awell%2520as%2520the%2520original%2520AdvGAN.Moreover%252C%2520PAR-AdvGAN%2520significantly%2520accelerates%2520the%250Aadversarial%2520example%2520generation%252C%2520i.e.%252C%2520achieving%2520the%2520speeds%2520of%2520up%2520to%2520335.5%250Aframes%2520per%2520second%2520on%2520Inception-v3%2520model%252C%2520outperforming%2520the%2520gradient-based%250Atransferable%2520attack%2520algorithms.%2520Our%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/LMBTough/PAR%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12207v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PAR-AdvGAN%3A%20Improving%20Adversarial%20Attack%20Capability%20with%20Progressive%0A%20%20Auto-Regression%20AdvGAN&entry.906535625=Jiayu%20Zhang%20and%20Zhiyu%20Zhu%20and%20Xinyi%20Wang%20and%20Silin%20Liao%20and%20Zhibo%20Jin%20and%20Flora%20D.%20Salim%20and%20Huaming%20Chen&entry.1292438233=%20%20Deep%20neural%20networks%20have%20demonstrated%20remarkable%20performance%20across%20various%0Adomains.%20However%2C%20they%20are%20vulnerable%20to%20adversarial%20examples%2C%20which%20can%20lead%0Ato%20erroneous%20predictions.%20Generative%20Adversarial%20Networks%20%28GANs%29%20can%20leverage%0Athe%20generators%20and%20discriminators%20model%20to%20quickly%20produce%20high-quality%0Aadversarial%20examples.%20Since%20both%20modules%20train%20in%20a%20competitive%20and%0Asimultaneous%20manner%2C%20GAN-based%20algorithms%20like%20AdvGAN%20can%20generate%20adversarial%0Aexamples%20with%20better%20transferability%20compared%20to%20traditional%20methods.%20However%2C%0Athe%20generation%20of%20perturbations%20is%20usually%20limited%20to%20a%20single%20iteration%2C%0Apreventing%20these%20examples%20from%20fully%20exploiting%20the%20potential%20of%20the%20methods.%0ATo%20tackle%20this%20issue%2C%20we%20introduce%20a%20novel%20approach%20named%20Progressive%0AAuto-Regression%20AdvGAN%20%28PAR-AdvGAN%29.%20It%20incorporates%20an%20auto-regressive%0Aiteration%20mechanism%20within%20a%20progressive%20generation%20network%20to%20craft%0Aadversarial%20examples%20with%20enhanced%20attack%20capability.%20We%20thoroughly%20evaluate%0Aour%20PAR-AdvGAN%20method%20with%20a%20large-scale%20experiment%2C%20demonstrating%20its%20superior%0Aperformance%20over%20various%20state-of-the-art%20black-box%20adversarial%20attacks%2C%20as%0Awell%20as%20the%20original%20AdvGAN.Moreover%2C%20PAR-AdvGAN%20significantly%20accelerates%20the%0Aadversarial%20example%20generation%2C%20i.e.%2C%20achieving%20the%20speeds%20of%20up%20to%20335.5%0Aframes%20per%20second%20on%20Inception-v3%20model%2C%20outperforming%20the%20gradient-based%0Atransferable%20attack%20algorithms.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/LMBTough/PAR%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12207v4&entry.124074799=Read"},
{"title": "Overcoming classic challenges for artificial neural networks by\n  providing incentives and practice", "author": "Kazuki Irie and Brenden M. Lake", "abstract": "  Since the earliest proposals for artificial neural network (ANN) models of\nthe mind and brain, critics have pointed out key weaknesses in these models\ncompared to human cognitive abilities. Here we review recent work that uses\nmetalearning to overcome several classic challenges, which we characterise as\naddressing the Problem of Incentive and Practice -- that is, providing machines\nwith both incentives to improve specific skills and opportunities to practice\nthose skills. This explicit optimization contrasts with more conventional\napproaches that hope the desired behaviour will emerge through optimising\nrelated but different objectives. We review applications of this principle to\naddressing four classic challenges for ANNs: systematic generalisation,\ncatastrophic forgetting, few-shot learning and multi-step reasoning. We also\ndiscuss how large language models incorporate key aspects of this metalearning\nframework (namely, sequence prediction with feedback trained on diverse data),\nwhich helps to explain some of their successes on these classic challenges.\nFinally, we discuss the prospects for understanding aspects of human\ndevelopment through this framework, and whether natural environments provide\nthe right incentives and practice for learning how to make challenging\ngeneralisations.\n", "link": "http://arxiv.org/abs/2410.10596v3", "date": "2025-08-22", "relevancy": 2.0585, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.532}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5064}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5006}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Overcoming%20classic%20challenges%20for%20artificial%20neural%20networks%20by%0A%20%20providing%20incentives%20and%20practice&body=Title%3A%20Overcoming%20classic%20challenges%20for%20artificial%20neural%20networks%20by%0A%20%20providing%20incentives%20and%20practice%0AAuthor%3A%20Kazuki%20Irie%20and%20Brenden%20M.%20Lake%0AAbstract%3A%20%20%20Since%20the%20earliest%20proposals%20for%20artificial%20neural%20network%20%28ANN%29%20models%20of%0Athe%20mind%20and%20brain%2C%20critics%20have%20pointed%20out%20key%20weaknesses%20in%20these%20models%0Acompared%20to%20human%20cognitive%20abilities.%20Here%20we%20review%20recent%20work%20that%20uses%0Ametalearning%20to%20overcome%20several%20classic%20challenges%2C%20which%20we%20characterise%20as%0Aaddressing%20the%20Problem%20of%20Incentive%20and%20Practice%20--%20that%20is%2C%20providing%20machines%0Awith%20both%20incentives%20to%20improve%20specific%20skills%20and%20opportunities%20to%20practice%0Athose%20skills.%20This%20explicit%20optimization%20contrasts%20with%20more%20conventional%0Aapproaches%20that%20hope%20the%20desired%20behaviour%20will%20emerge%20through%20optimising%0Arelated%20but%20different%20objectives.%20We%20review%20applications%20of%20this%20principle%20to%0Aaddressing%20four%20classic%20challenges%20for%20ANNs%3A%20systematic%20generalisation%2C%0Acatastrophic%20forgetting%2C%20few-shot%20learning%20and%20multi-step%20reasoning.%20We%20also%0Adiscuss%20how%20large%20language%20models%20incorporate%20key%20aspects%20of%20this%20metalearning%0Aframework%20%28namely%2C%20sequence%20prediction%20with%20feedback%20trained%20on%20diverse%20data%29%2C%0Awhich%20helps%20to%20explain%20some%20of%20their%20successes%20on%20these%20classic%20challenges.%0AFinally%2C%20we%20discuss%20the%20prospects%20for%20understanding%20aspects%20of%20human%0Adevelopment%20through%20this%20framework%2C%20and%20whether%20natural%20environments%20provide%0Athe%20right%20incentives%20and%20practice%20for%20learning%20how%20to%20make%20challenging%0Ageneralisations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10596v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOvercoming%2520classic%2520challenges%2520for%2520artificial%2520neural%2520networks%2520by%250A%2520%2520providing%2520incentives%2520and%2520practice%26entry.906535625%3DKazuki%2520Irie%2520and%2520Brenden%2520M.%2520Lake%26entry.1292438233%3D%2520%2520Since%2520the%2520earliest%2520proposals%2520for%2520artificial%2520neural%2520network%2520%2528ANN%2529%2520models%2520of%250Athe%2520mind%2520and%2520brain%252C%2520critics%2520have%2520pointed%2520out%2520key%2520weaknesses%2520in%2520these%2520models%250Acompared%2520to%2520human%2520cognitive%2520abilities.%2520Here%2520we%2520review%2520recent%2520work%2520that%2520uses%250Ametalearning%2520to%2520overcome%2520several%2520classic%2520challenges%252C%2520which%2520we%2520characterise%2520as%250Aaddressing%2520the%2520Problem%2520of%2520Incentive%2520and%2520Practice%2520--%2520that%2520is%252C%2520providing%2520machines%250Awith%2520both%2520incentives%2520to%2520improve%2520specific%2520skills%2520and%2520opportunities%2520to%2520practice%250Athose%2520skills.%2520This%2520explicit%2520optimization%2520contrasts%2520with%2520more%2520conventional%250Aapproaches%2520that%2520hope%2520the%2520desired%2520behaviour%2520will%2520emerge%2520through%2520optimising%250Arelated%2520but%2520different%2520objectives.%2520We%2520review%2520applications%2520of%2520this%2520principle%2520to%250Aaddressing%2520four%2520classic%2520challenges%2520for%2520ANNs%253A%2520systematic%2520generalisation%252C%250Acatastrophic%2520forgetting%252C%2520few-shot%2520learning%2520and%2520multi-step%2520reasoning.%2520We%2520also%250Adiscuss%2520how%2520large%2520language%2520models%2520incorporate%2520key%2520aspects%2520of%2520this%2520metalearning%250Aframework%2520%2528namely%252C%2520sequence%2520prediction%2520with%2520feedback%2520trained%2520on%2520diverse%2520data%2529%252C%250Awhich%2520helps%2520to%2520explain%2520some%2520of%2520their%2520successes%2520on%2520these%2520classic%2520challenges.%250AFinally%252C%2520we%2520discuss%2520the%2520prospects%2520for%2520understanding%2520aspects%2520of%2520human%250Adevelopment%2520through%2520this%2520framework%252C%2520and%2520whether%2520natural%2520environments%2520provide%250Athe%2520right%2520incentives%2520and%2520practice%2520for%2520learning%2520how%2520to%2520make%2520challenging%250Ageneralisations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10596v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Overcoming%20classic%20challenges%20for%20artificial%20neural%20networks%20by%0A%20%20providing%20incentives%20and%20practice&entry.906535625=Kazuki%20Irie%20and%20Brenden%20M.%20Lake&entry.1292438233=%20%20Since%20the%20earliest%20proposals%20for%20artificial%20neural%20network%20%28ANN%29%20models%20of%0Athe%20mind%20and%20brain%2C%20critics%20have%20pointed%20out%20key%20weaknesses%20in%20these%20models%0Acompared%20to%20human%20cognitive%20abilities.%20Here%20we%20review%20recent%20work%20that%20uses%0Ametalearning%20to%20overcome%20several%20classic%20challenges%2C%20which%20we%20characterise%20as%0Aaddressing%20the%20Problem%20of%20Incentive%20and%20Practice%20--%20that%20is%2C%20providing%20machines%0Awith%20both%20incentives%20to%20improve%20specific%20skills%20and%20opportunities%20to%20practice%0Athose%20skills.%20This%20explicit%20optimization%20contrasts%20with%20more%20conventional%0Aapproaches%20that%20hope%20the%20desired%20behaviour%20will%20emerge%20through%20optimising%0Arelated%20but%20different%20objectives.%20We%20review%20applications%20of%20this%20principle%20to%0Aaddressing%20four%20classic%20challenges%20for%20ANNs%3A%20systematic%20generalisation%2C%0Acatastrophic%20forgetting%2C%20few-shot%20learning%20and%20multi-step%20reasoning.%20We%20also%0Adiscuss%20how%20large%20language%20models%20incorporate%20key%20aspects%20of%20this%20metalearning%0Aframework%20%28namely%2C%20sequence%20prediction%20with%20feedback%20trained%20on%20diverse%20data%29%2C%0Awhich%20helps%20to%20explain%20some%20of%20their%20successes%20on%20these%20classic%20challenges.%0AFinally%2C%20we%20discuss%20the%20prospects%20for%20understanding%20aspects%20of%20human%0Adevelopment%20through%20this%20framework%2C%20and%20whether%20natural%20environments%20provide%0Athe%20right%20incentives%20and%20practice%20for%20learning%20how%20to%20make%20challenging%0Ageneralisations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10596v3&entry.124074799=Read"},
{"title": "Swarming Without an Anchor (SWA): Robot Swarms Adapt Better to\n  Localization Dropouts Then a Single Robot", "author": "Jiri Horyna and Roland Jung and Stephan Weiss and Eliseo Ferrante and Martin Saska", "abstract": "  In this paper, we present the Swarming Without an Anchor (SWA) approach to\nstate estimation in swarms of Unmanned Aerial Vehicles (UAVs) experiencing\nego-localization dropout, where individual agents are laterally stabilized\nusing relative information only. We propose to fuse decentralized state\nestimation with robust mutual perception and onboard sensor data to maintain\naccurate state awareness despite intermittent localization failures. Thus, the\nrelative information used to estimate the lateral state of UAVs enables the\nidentification of the unambiguous state of UAVs with respect to the local\nconstellation. The resulting behavior reaches velocity consensus, as this task\ncan be referred to as the double integrator synchronization problem. All\ndisturbances and performance degradations except a uniform translation drift of\nthe swarm as a whole is attenuated which is enabling new opportunities in using\ntight cooperation for increasing reliability and resilience of multi-UAV\nsystems. Simulations and real-world experiments validate the effectiveness of\nour approach, demonstrating its capability to sustain cohesive swarm behavior\nin challenging conditions of unreliable or unavailable primary localization.\n", "link": "http://arxiv.org/abs/2508.16460v1", "date": "2025-08-22", "relevancy": 2.0541, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5757}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5031}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Swarming%20Without%20an%20Anchor%20%28SWA%29%3A%20Robot%20Swarms%20Adapt%20Better%20to%0A%20%20Localization%20Dropouts%20Then%20a%20Single%20Robot&body=Title%3A%20Swarming%20Without%20an%20Anchor%20%28SWA%29%3A%20Robot%20Swarms%20Adapt%20Better%20to%0A%20%20Localization%20Dropouts%20Then%20a%20Single%20Robot%0AAuthor%3A%20Jiri%20Horyna%20and%20Roland%20Jung%20and%20Stephan%20Weiss%20and%20Eliseo%20Ferrante%20and%20Martin%20Saska%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20the%20Swarming%20Without%20an%20Anchor%20%28SWA%29%20approach%20to%0Astate%20estimation%20in%20swarms%20of%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20experiencing%0Aego-localization%20dropout%2C%20where%20individual%20agents%20are%20laterally%20stabilized%0Ausing%20relative%20information%20only.%20We%20propose%20to%20fuse%20decentralized%20state%0Aestimation%20with%20robust%20mutual%20perception%20and%20onboard%20sensor%20data%20to%20maintain%0Aaccurate%20state%20awareness%20despite%20intermittent%20localization%20failures.%20Thus%2C%20the%0Arelative%20information%20used%20to%20estimate%20the%20lateral%20state%20of%20UAVs%20enables%20the%0Aidentification%20of%20the%20unambiguous%20state%20of%20UAVs%20with%20respect%20to%20the%20local%0Aconstellation.%20The%20resulting%20behavior%20reaches%20velocity%20consensus%2C%20as%20this%20task%0Acan%20be%20referred%20to%20as%20the%20double%20integrator%20synchronization%20problem.%20All%0Adisturbances%20and%20performance%20degradations%20except%20a%20uniform%20translation%20drift%20of%0Athe%20swarm%20as%20a%20whole%20is%20attenuated%20which%20is%20enabling%20new%20opportunities%20in%20using%0Atight%20cooperation%20for%20increasing%20reliability%20and%20resilience%20of%20multi-UAV%0Asystems.%20Simulations%20and%20real-world%20experiments%20validate%20the%20effectiveness%20of%0Aour%20approach%2C%20demonstrating%20its%20capability%20to%20sustain%20cohesive%20swarm%20behavior%0Ain%20challenging%20conditions%20of%20unreliable%20or%20unavailable%20primary%20localization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16460v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSwarming%2520Without%2520an%2520Anchor%2520%2528SWA%2529%253A%2520Robot%2520Swarms%2520Adapt%2520Better%2520to%250A%2520%2520Localization%2520Dropouts%2520Then%2520a%2520Single%2520Robot%26entry.906535625%3DJiri%2520Horyna%2520and%2520Roland%2520Jung%2520and%2520Stephan%2520Weiss%2520and%2520Eliseo%2520Ferrante%2520and%2520Martin%2520Saska%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520Swarming%2520Without%2520an%2520Anchor%2520%2528SWA%2529%2520approach%2520to%250Astate%2520estimation%2520in%2520swarms%2520of%2520Unmanned%2520Aerial%2520Vehicles%2520%2528UAVs%2529%2520experiencing%250Aego-localization%2520dropout%252C%2520where%2520individual%2520agents%2520are%2520laterally%2520stabilized%250Ausing%2520relative%2520information%2520only.%2520We%2520propose%2520to%2520fuse%2520decentralized%2520state%250Aestimation%2520with%2520robust%2520mutual%2520perception%2520and%2520onboard%2520sensor%2520data%2520to%2520maintain%250Aaccurate%2520state%2520awareness%2520despite%2520intermittent%2520localization%2520failures.%2520Thus%252C%2520the%250Arelative%2520information%2520used%2520to%2520estimate%2520the%2520lateral%2520state%2520of%2520UAVs%2520enables%2520the%250Aidentification%2520of%2520the%2520unambiguous%2520state%2520of%2520UAVs%2520with%2520respect%2520to%2520the%2520local%250Aconstellation.%2520The%2520resulting%2520behavior%2520reaches%2520velocity%2520consensus%252C%2520as%2520this%2520task%250Acan%2520be%2520referred%2520to%2520as%2520the%2520double%2520integrator%2520synchronization%2520problem.%2520All%250Adisturbances%2520and%2520performance%2520degradations%2520except%2520a%2520uniform%2520translation%2520drift%2520of%250Athe%2520swarm%2520as%2520a%2520whole%2520is%2520attenuated%2520which%2520is%2520enabling%2520new%2520opportunities%2520in%2520using%250Atight%2520cooperation%2520for%2520increasing%2520reliability%2520and%2520resilience%2520of%2520multi-UAV%250Asystems.%2520Simulations%2520and%2520real-world%2520experiments%2520validate%2520the%2520effectiveness%2520of%250Aour%2520approach%252C%2520demonstrating%2520its%2520capability%2520to%2520sustain%2520cohesive%2520swarm%2520behavior%250Ain%2520challenging%2520conditions%2520of%2520unreliable%2520or%2520unavailable%2520primary%2520localization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16460v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Swarming%20Without%20an%20Anchor%20%28SWA%29%3A%20Robot%20Swarms%20Adapt%20Better%20to%0A%20%20Localization%20Dropouts%20Then%20a%20Single%20Robot&entry.906535625=Jiri%20Horyna%20and%20Roland%20Jung%20and%20Stephan%20Weiss%20and%20Eliseo%20Ferrante%20and%20Martin%20Saska&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20the%20Swarming%20Without%20an%20Anchor%20%28SWA%29%20approach%20to%0Astate%20estimation%20in%20swarms%20of%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20experiencing%0Aego-localization%20dropout%2C%20where%20individual%20agents%20are%20laterally%20stabilized%0Ausing%20relative%20information%20only.%20We%20propose%20to%20fuse%20decentralized%20state%0Aestimation%20with%20robust%20mutual%20perception%20and%20onboard%20sensor%20data%20to%20maintain%0Aaccurate%20state%20awareness%20despite%20intermittent%20localization%20failures.%20Thus%2C%20the%0Arelative%20information%20used%20to%20estimate%20the%20lateral%20state%20of%20UAVs%20enables%20the%0Aidentification%20of%20the%20unambiguous%20state%20of%20UAVs%20with%20respect%20to%20the%20local%0Aconstellation.%20The%20resulting%20behavior%20reaches%20velocity%20consensus%2C%20as%20this%20task%0Acan%20be%20referred%20to%20as%20the%20double%20integrator%20synchronization%20problem.%20All%0Adisturbances%20and%20performance%20degradations%20except%20a%20uniform%20translation%20drift%20of%0Athe%20swarm%20as%20a%20whole%20is%20attenuated%20which%20is%20enabling%20new%20opportunities%20in%20using%0Atight%20cooperation%20for%20increasing%20reliability%20and%20resilience%20of%20multi-UAV%0Asystems.%20Simulations%20and%20real-world%20experiments%20validate%20the%20effectiveness%20of%0Aour%20approach%2C%20demonstrating%20its%20capability%20to%20sustain%20cohesive%20swarm%20behavior%0Ain%20challenging%20conditions%20of%20unreliable%20or%20unavailable%20primary%20localization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16460v1&entry.124074799=Read"},
{"title": "OwkinZero: Accelerating Biological Discovery with AI", "author": "Nathan Bigaud and Vincent Cabeli and Meltem Gurel and Arthur Pignet and John Klein and Gilles Wainrib and Eric Durand", "abstract": "  While large language models (LLMs) are rapidly advancing scientific research,\nthey continue to struggle with core biological reasoning tasks essential for\ntranslational and biomedical discovery. To address this limitation, we created\nand curated eight comprehensive benchmark datasets comprising over 300,000\nverifiable question-and-answer pairs, each targeting critical challenges in\ndrug discovery including target druggability, modality suitability, and drug\nperturbation effects. Using this resource, we developed the OwkinZero models by\npost-training open-source LLMs through a Reinforcement Learning from Verifiable\nRewards strategy. Our results demonstrate that specialized 8-32B OwkinZero\nmodels substantially outperform larger, state-of-the-art commercial LLMs on\nthese biological benchmarks. Remarkably, we uncover evidence of a key aspect of\ngeneralization: specialist models trained on a single task consistently\noutperform their base models on previously unseen tasks. This generalization\neffect is further amplified in our comprehensive OwkinZero models, which were\ntrained on a mixture of datasets and achieve even broader cross-task\nimprovements. This study represents a significant step toward addressing the\nbiological reasoning blind spot in current LLMs, demonstrating that targeted\nreinforcement learning on carefully curated data can unlock generalizable\nperformance in specialized models, thereby accelerating AI-driven biological\ndiscovery.\n", "link": "http://arxiv.org/abs/2508.16315v1", "date": "2025-08-22", "relevancy": 2.0451, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5187}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5187}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OwkinZero%3A%20Accelerating%20Biological%20Discovery%20with%20AI&body=Title%3A%20OwkinZero%3A%20Accelerating%20Biological%20Discovery%20with%20AI%0AAuthor%3A%20Nathan%20Bigaud%20and%20Vincent%20Cabeli%20and%20Meltem%20Gurel%20and%20Arthur%20Pignet%20and%20John%20Klein%20and%20Gilles%20Wainrib%20and%20Eric%20Durand%0AAbstract%3A%20%20%20While%20large%20language%20models%20%28LLMs%29%20are%20rapidly%20advancing%20scientific%20research%2C%0Athey%20continue%20to%20struggle%20with%20core%20biological%20reasoning%20tasks%20essential%20for%0Atranslational%20and%20biomedical%20discovery.%20To%20address%20this%20limitation%2C%20we%20created%0Aand%20curated%20eight%20comprehensive%20benchmark%20datasets%20comprising%20over%20300%2C000%0Averifiable%20question-and-answer%20pairs%2C%20each%20targeting%20critical%20challenges%20in%0Adrug%20discovery%20including%20target%20druggability%2C%20modality%20suitability%2C%20and%20drug%0Aperturbation%20effects.%20Using%20this%20resource%2C%20we%20developed%20the%20OwkinZero%20models%20by%0Apost-training%20open-source%20LLMs%20through%20a%20Reinforcement%20Learning%20from%20Verifiable%0ARewards%20strategy.%20Our%20results%20demonstrate%20that%20specialized%208-32B%20OwkinZero%0Amodels%20substantially%20outperform%20larger%2C%20state-of-the-art%20commercial%20LLMs%20on%0Athese%20biological%20benchmarks.%20Remarkably%2C%20we%20uncover%20evidence%20of%20a%20key%20aspect%20of%0Ageneralization%3A%20specialist%20models%20trained%20on%20a%20single%20task%20consistently%0Aoutperform%20their%20base%20models%20on%20previously%20unseen%20tasks.%20This%20generalization%0Aeffect%20is%20further%20amplified%20in%20our%20comprehensive%20OwkinZero%20models%2C%20which%20were%0Atrained%20on%20a%20mixture%20of%20datasets%20and%20achieve%20even%20broader%20cross-task%0Aimprovements.%20This%20study%20represents%20a%20significant%20step%20toward%20addressing%20the%0Abiological%20reasoning%20blind%20spot%20in%20current%20LLMs%2C%20demonstrating%20that%20targeted%0Areinforcement%20learning%20on%20carefully%20curated%20data%20can%20unlock%20generalizable%0Aperformance%20in%20specialized%20models%2C%20thereby%20accelerating%20AI-driven%20biological%0Adiscovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16315v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOwkinZero%253A%2520Accelerating%2520Biological%2520Discovery%2520with%2520AI%26entry.906535625%3DNathan%2520Bigaud%2520and%2520Vincent%2520Cabeli%2520and%2520Meltem%2520Gurel%2520and%2520Arthur%2520Pignet%2520and%2520John%2520Klein%2520and%2520Gilles%2520Wainrib%2520and%2520Eric%2520Durand%26entry.1292438233%3D%2520%2520While%2520large%2520language%2520models%2520%2528LLMs%2529%2520are%2520rapidly%2520advancing%2520scientific%2520research%252C%250Athey%2520continue%2520to%2520struggle%2520with%2520core%2520biological%2520reasoning%2520tasks%2520essential%2520for%250Atranslational%2520and%2520biomedical%2520discovery.%2520To%2520address%2520this%2520limitation%252C%2520we%2520created%250Aand%2520curated%2520eight%2520comprehensive%2520benchmark%2520datasets%2520comprising%2520over%2520300%252C000%250Averifiable%2520question-and-answer%2520pairs%252C%2520each%2520targeting%2520critical%2520challenges%2520in%250Adrug%2520discovery%2520including%2520target%2520druggability%252C%2520modality%2520suitability%252C%2520and%2520drug%250Aperturbation%2520effects.%2520Using%2520this%2520resource%252C%2520we%2520developed%2520the%2520OwkinZero%2520models%2520by%250Apost-training%2520open-source%2520LLMs%2520through%2520a%2520Reinforcement%2520Learning%2520from%2520Verifiable%250ARewards%2520strategy.%2520Our%2520results%2520demonstrate%2520that%2520specialized%25208-32B%2520OwkinZero%250Amodels%2520substantially%2520outperform%2520larger%252C%2520state-of-the-art%2520commercial%2520LLMs%2520on%250Athese%2520biological%2520benchmarks.%2520Remarkably%252C%2520we%2520uncover%2520evidence%2520of%2520a%2520key%2520aspect%2520of%250Ageneralization%253A%2520specialist%2520models%2520trained%2520on%2520a%2520single%2520task%2520consistently%250Aoutperform%2520their%2520base%2520models%2520on%2520previously%2520unseen%2520tasks.%2520This%2520generalization%250Aeffect%2520is%2520further%2520amplified%2520in%2520our%2520comprehensive%2520OwkinZero%2520models%252C%2520which%2520were%250Atrained%2520on%2520a%2520mixture%2520of%2520datasets%2520and%2520achieve%2520even%2520broader%2520cross-task%250Aimprovements.%2520This%2520study%2520represents%2520a%2520significant%2520step%2520toward%2520addressing%2520the%250Abiological%2520reasoning%2520blind%2520spot%2520in%2520current%2520LLMs%252C%2520demonstrating%2520that%2520targeted%250Areinforcement%2520learning%2520on%2520carefully%2520curated%2520data%2520can%2520unlock%2520generalizable%250Aperformance%2520in%2520specialized%2520models%252C%2520thereby%2520accelerating%2520AI-driven%2520biological%250Adiscovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16315v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OwkinZero%3A%20Accelerating%20Biological%20Discovery%20with%20AI&entry.906535625=Nathan%20Bigaud%20and%20Vincent%20Cabeli%20and%20Meltem%20Gurel%20and%20Arthur%20Pignet%20and%20John%20Klein%20and%20Gilles%20Wainrib%20and%20Eric%20Durand&entry.1292438233=%20%20While%20large%20language%20models%20%28LLMs%29%20are%20rapidly%20advancing%20scientific%20research%2C%0Athey%20continue%20to%20struggle%20with%20core%20biological%20reasoning%20tasks%20essential%20for%0Atranslational%20and%20biomedical%20discovery.%20To%20address%20this%20limitation%2C%20we%20created%0Aand%20curated%20eight%20comprehensive%20benchmark%20datasets%20comprising%20over%20300%2C000%0Averifiable%20question-and-answer%20pairs%2C%20each%20targeting%20critical%20challenges%20in%0Adrug%20discovery%20including%20target%20druggability%2C%20modality%20suitability%2C%20and%20drug%0Aperturbation%20effects.%20Using%20this%20resource%2C%20we%20developed%20the%20OwkinZero%20models%20by%0Apost-training%20open-source%20LLMs%20through%20a%20Reinforcement%20Learning%20from%20Verifiable%0ARewards%20strategy.%20Our%20results%20demonstrate%20that%20specialized%208-32B%20OwkinZero%0Amodels%20substantially%20outperform%20larger%2C%20state-of-the-art%20commercial%20LLMs%20on%0Athese%20biological%20benchmarks.%20Remarkably%2C%20we%20uncover%20evidence%20of%20a%20key%20aspect%20of%0Ageneralization%3A%20specialist%20models%20trained%20on%20a%20single%20task%20consistently%0Aoutperform%20their%20base%20models%20on%20previously%20unseen%20tasks.%20This%20generalization%0Aeffect%20is%20further%20amplified%20in%20our%20comprehensive%20OwkinZero%20models%2C%20which%20were%0Atrained%20on%20a%20mixture%20of%20datasets%20and%20achieve%20even%20broader%20cross-task%0Aimprovements.%20This%20study%20represents%20a%20significant%20step%20toward%20addressing%20the%0Abiological%20reasoning%20blind%20spot%20in%20current%20LLMs%2C%20demonstrating%20that%20targeted%0Areinforcement%20learning%20on%20carefully%20curated%20data%20can%20unlock%20generalizable%0Aperformance%20in%20specialized%20models%2C%20thereby%20accelerating%20AI-driven%20biological%0Adiscovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16315v1&entry.124074799=Read"},
{"title": "DIDS: Domain Impact-aware Data Sampling for Large Language Model\n  Training", "author": "Weijie Shi and Jipeng Zhang and Yaguang Wu and Jingzhi Fang and Ruiyuan Zhang and Jiajie Xu and Jia Zhu and Hao Chen and Yao Zhao and Sirui Han and Xiaofang Zhou", "abstract": "  Large language models (LLMs) are commonly trained on multi-domain datasets,\nwhere domain sampling strategies significantly impact model performance due to\nvarying domain importance across downstream tasks. Existing approaches for\noptimizing domain-level sampling strategies struggle with maintaining\nintra-domain consistency and accurately measuring domain impact. In this paper,\nwe present Domain Impact-aware Data Sampling (DIDS). To ensure intra-domain\nconsistency, a gradient clustering algorithm is proposed to group training data\nbased on their learning effects, where a proxy language model and\ndimensionality reduction are employed to reduce computational overhead. To\naccurately measure domain impact, we develop a Fisher Information Matrix (FIM)\nguided metric that quantifies how domain-specific parameter updates affect the\nmodel's output distributions on downstream tasks, with theoretical guarantees.\nFurthermore, to determine optimal sampling ratios, DIDS combines both the\nFIM-guided domain impact assessment and loss learning trajectories that\nindicate domain-specific potential, while accounting for diminishing marginal\nreturns. Extensive experiments demonstrate that DIDS achieves 3.4% higher\naverage performance while maintaining comparable training efficiency. The code\nis available at https://github.com/shiweijiezero/DIDS.\n", "link": "http://arxiv.org/abs/2504.13227v2", "date": "2025-08-22", "relevancy": 2.0399, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5132}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5089}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DIDS%3A%20Domain%20Impact-aware%20Data%20Sampling%20for%20Large%20Language%20Model%0A%20%20Training&body=Title%3A%20DIDS%3A%20Domain%20Impact-aware%20Data%20Sampling%20for%20Large%20Language%20Model%0A%20%20Training%0AAuthor%3A%20Weijie%20Shi%20and%20Jipeng%20Zhang%20and%20Yaguang%20Wu%20and%20Jingzhi%20Fang%20and%20Ruiyuan%20Zhang%20and%20Jiajie%20Xu%20and%20Jia%20Zhu%20and%20Hao%20Chen%20and%20Yao%20Zhao%20and%20Sirui%20Han%20and%20Xiaofang%20Zhou%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20commonly%20trained%20on%20multi-domain%20datasets%2C%0Awhere%20domain%20sampling%20strategies%20significantly%20impact%20model%20performance%20due%20to%0Avarying%20domain%20importance%20across%20downstream%20tasks.%20Existing%20approaches%20for%0Aoptimizing%20domain-level%20sampling%20strategies%20struggle%20with%20maintaining%0Aintra-domain%20consistency%20and%20accurately%20measuring%20domain%20impact.%20In%20this%20paper%2C%0Awe%20present%20Domain%20Impact-aware%20Data%20Sampling%20%28DIDS%29.%20To%20ensure%20intra-domain%0Aconsistency%2C%20a%20gradient%20clustering%20algorithm%20is%20proposed%20to%20group%20training%20data%0Abased%20on%20their%20learning%20effects%2C%20where%20a%20proxy%20language%20model%20and%0Adimensionality%20reduction%20are%20employed%20to%20reduce%20computational%20overhead.%20To%0Aaccurately%20measure%20domain%20impact%2C%20we%20develop%20a%20Fisher%20Information%20Matrix%20%28FIM%29%0Aguided%20metric%20that%20quantifies%20how%20domain-specific%20parameter%20updates%20affect%20the%0Amodel%27s%20output%20distributions%20on%20downstream%20tasks%2C%20with%20theoretical%20guarantees.%0AFurthermore%2C%20to%20determine%20optimal%20sampling%20ratios%2C%20DIDS%20combines%20both%20the%0AFIM-guided%20domain%20impact%20assessment%20and%20loss%20learning%20trajectories%20that%0Aindicate%20domain-specific%20potential%2C%20while%20accounting%20for%20diminishing%20marginal%0Areturns.%20Extensive%20experiments%20demonstrate%20that%20DIDS%20achieves%203.4%25%20higher%0Aaverage%20performance%20while%20maintaining%20comparable%20training%20efficiency.%20The%20code%0Ais%20available%20at%20https%3A//github.com/shiweijiezero/DIDS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13227v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDIDS%253A%2520Domain%2520Impact-aware%2520Data%2520Sampling%2520for%2520Large%2520Language%2520Model%250A%2520%2520Training%26entry.906535625%3DWeijie%2520Shi%2520and%2520Jipeng%2520Zhang%2520and%2520Yaguang%2520Wu%2520and%2520Jingzhi%2520Fang%2520and%2520Ruiyuan%2520Zhang%2520and%2520Jiajie%2520Xu%2520and%2520Jia%2520Zhu%2520and%2520Hao%2520Chen%2520and%2520Yao%2520Zhao%2520and%2520Sirui%2520Han%2520and%2520Xiaofang%2520Zhou%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520commonly%2520trained%2520on%2520multi-domain%2520datasets%252C%250Awhere%2520domain%2520sampling%2520strategies%2520significantly%2520impact%2520model%2520performance%2520due%2520to%250Avarying%2520domain%2520importance%2520across%2520downstream%2520tasks.%2520Existing%2520approaches%2520for%250Aoptimizing%2520domain-level%2520sampling%2520strategies%2520struggle%2520with%2520maintaining%250Aintra-domain%2520consistency%2520and%2520accurately%2520measuring%2520domain%2520impact.%2520In%2520this%2520paper%252C%250Awe%2520present%2520Domain%2520Impact-aware%2520Data%2520Sampling%2520%2528DIDS%2529.%2520To%2520ensure%2520intra-domain%250Aconsistency%252C%2520a%2520gradient%2520clustering%2520algorithm%2520is%2520proposed%2520to%2520group%2520training%2520data%250Abased%2520on%2520their%2520learning%2520effects%252C%2520where%2520a%2520proxy%2520language%2520model%2520and%250Adimensionality%2520reduction%2520are%2520employed%2520to%2520reduce%2520computational%2520overhead.%2520To%250Aaccurately%2520measure%2520domain%2520impact%252C%2520we%2520develop%2520a%2520Fisher%2520Information%2520Matrix%2520%2528FIM%2529%250Aguided%2520metric%2520that%2520quantifies%2520how%2520domain-specific%2520parameter%2520updates%2520affect%2520the%250Amodel%2527s%2520output%2520distributions%2520on%2520downstream%2520tasks%252C%2520with%2520theoretical%2520guarantees.%250AFurthermore%252C%2520to%2520determine%2520optimal%2520sampling%2520ratios%252C%2520DIDS%2520combines%2520both%2520the%250AFIM-guided%2520domain%2520impact%2520assessment%2520and%2520loss%2520learning%2520trajectories%2520that%250Aindicate%2520domain-specific%2520potential%252C%2520while%2520accounting%2520for%2520diminishing%2520marginal%250Areturns.%2520Extensive%2520experiments%2520demonstrate%2520that%2520DIDS%2520achieves%25203.4%2525%2520higher%250Aaverage%2520performance%2520while%2520maintaining%2520comparable%2520training%2520efficiency.%2520The%2520code%250Ais%2520available%2520at%2520https%253A//github.com/shiweijiezero/DIDS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13227v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DIDS%3A%20Domain%20Impact-aware%20Data%20Sampling%20for%20Large%20Language%20Model%0A%20%20Training&entry.906535625=Weijie%20Shi%20and%20Jipeng%20Zhang%20and%20Yaguang%20Wu%20and%20Jingzhi%20Fang%20and%20Ruiyuan%20Zhang%20and%20Jiajie%20Xu%20and%20Jia%20Zhu%20and%20Hao%20Chen%20and%20Yao%20Zhao%20and%20Sirui%20Han%20and%20Xiaofang%20Zhou&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20commonly%20trained%20on%20multi-domain%20datasets%2C%0Awhere%20domain%20sampling%20strategies%20significantly%20impact%20model%20performance%20due%20to%0Avarying%20domain%20importance%20across%20downstream%20tasks.%20Existing%20approaches%20for%0Aoptimizing%20domain-level%20sampling%20strategies%20struggle%20with%20maintaining%0Aintra-domain%20consistency%20and%20accurately%20measuring%20domain%20impact.%20In%20this%20paper%2C%0Awe%20present%20Domain%20Impact-aware%20Data%20Sampling%20%28DIDS%29.%20To%20ensure%20intra-domain%0Aconsistency%2C%20a%20gradient%20clustering%20algorithm%20is%20proposed%20to%20group%20training%20data%0Abased%20on%20their%20learning%20effects%2C%20where%20a%20proxy%20language%20model%20and%0Adimensionality%20reduction%20are%20employed%20to%20reduce%20computational%20overhead.%20To%0Aaccurately%20measure%20domain%20impact%2C%20we%20develop%20a%20Fisher%20Information%20Matrix%20%28FIM%29%0Aguided%20metric%20that%20quantifies%20how%20domain-specific%20parameter%20updates%20affect%20the%0Amodel%27s%20output%20distributions%20on%20downstream%20tasks%2C%20with%20theoretical%20guarantees.%0AFurthermore%2C%20to%20determine%20optimal%20sampling%20ratios%2C%20DIDS%20combines%20both%20the%0AFIM-guided%20domain%20impact%20assessment%20and%20loss%20learning%20trajectories%20that%0Aindicate%20domain-specific%20potential%2C%20while%20accounting%20for%20diminishing%20marginal%0Areturns.%20Extensive%20experiments%20demonstrate%20that%20DIDS%20achieves%203.4%25%20higher%0Aaverage%20performance%20while%20maintaining%20comparable%20training%20efficiency.%20The%20code%0Ais%20available%20at%20https%3A//github.com/shiweijiezero/DIDS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13227v2&entry.124074799=Read"},
{"title": "Benchmarking Training Paradigms, Dataset Composition, and Model Scaling\n  for Child ASR in ESPnet", "author": "Anyu Ying and Natarajan Balaji Shankar and Chyi-Jiunn Lin and Mohan Shi and Pu Wang and Hye-jin Shim and Siddhant Arora and Hugo Van hamme and Abeer Alwan and Shinji Watanabe", "abstract": "  Despite advancements in ASR, child speech recognition remains challenging due\nto acoustic variability and limited annotated data. While fine-tuning adult ASR\nmodels on child speech is common, comparisons with flat-start training remain\nunderexplored. We compare flat-start training across multiple datasets, SSL\nrepresentations (WavLM, XEUS), and decoder architectures. Our results show that\nSSL representations are biased toward adult speech, with flat-start training on\nchild speech mitigating these biases. We also analyze model scaling, finding\nconsistent improvements up to 1B parameters, beyond which performance plateaus.\nAdditionally, age-related ASR and speaker verification analysis highlights the\nlimitations of proprietary models like Whisper, emphasizing the need for\nopen-data models for reliable child speech research. All investigations are\nconducted using ESPnet, and our publicly available benchmark provides insights\ninto training strategies for robust child speech processing.\n", "link": "http://arxiv.org/abs/2508.16576v1", "date": "2025-08-22", "relevancy": 2.028, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5102}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5102}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Training%20Paradigms%2C%20Dataset%20Composition%2C%20and%20Model%20Scaling%0A%20%20for%20Child%20ASR%20in%20ESPnet&body=Title%3A%20Benchmarking%20Training%20Paradigms%2C%20Dataset%20Composition%2C%20and%20Model%20Scaling%0A%20%20for%20Child%20ASR%20in%20ESPnet%0AAuthor%3A%20Anyu%20Ying%20and%20Natarajan%20Balaji%20Shankar%20and%20Chyi-Jiunn%20Lin%20and%20Mohan%20Shi%20and%20Pu%20Wang%20and%20Hye-jin%20Shim%20and%20Siddhant%20Arora%20and%20Hugo%20Van%20hamme%20and%20Abeer%20Alwan%20and%20Shinji%20Watanabe%0AAbstract%3A%20%20%20Despite%20advancements%20in%20ASR%2C%20child%20speech%20recognition%20remains%20challenging%20due%0Ato%20acoustic%20variability%20and%20limited%20annotated%20data.%20While%20fine-tuning%20adult%20ASR%0Amodels%20on%20child%20speech%20is%20common%2C%20comparisons%20with%20flat-start%20training%20remain%0Aunderexplored.%20We%20compare%20flat-start%20training%20across%20multiple%20datasets%2C%20SSL%0Arepresentations%20%28WavLM%2C%20XEUS%29%2C%20and%20decoder%20architectures.%20Our%20results%20show%20that%0ASSL%20representations%20are%20biased%20toward%20adult%20speech%2C%20with%20flat-start%20training%20on%0Achild%20speech%20mitigating%20these%20biases.%20We%20also%20analyze%20model%20scaling%2C%20finding%0Aconsistent%20improvements%20up%20to%201B%20parameters%2C%20beyond%20which%20performance%20plateaus.%0AAdditionally%2C%20age-related%20ASR%20and%20speaker%20verification%20analysis%20highlights%20the%0Alimitations%20of%20proprietary%20models%20like%20Whisper%2C%20emphasizing%20the%20need%20for%0Aopen-data%20models%20for%20reliable%20child%20speech%20research.%20All%20investigations%20are%0Aconducted%20using%20ESPnet%2C%20and%20our%20publicly%20available%20benchmark%20provides%20insights%0Ainto%20training%20strategies%20for%20robust%20child%20speech%20processing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16576v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Training%2520Paradigms%252C%2520Dataset%2520Composition%252C%2520and%2520Model%2520Scaling%250A%2520%2520for%2520Child%2520ASR%2520in%2520ESPnet%26entry.906535625%3DAnyu%2520Ying%2520and%2520Natarajan%2520Balaji%2520Shankar%2520and%2520Chyi-Jiunn%2520Lin%2520and%2520Mohan%2520Shi%2520and%2520Pu%2520Wang%2520and%2520Hye-jin%2520Shim%2520and%2520Siddhant%2520Arora%2520and%2520Hugo%2520Van%2520hamme%2520and%2520Abeer%2520Alwan%2520and%2520Shinji%2520Watanabe%26entry.1292438233%3D%2520%2520Despite%2520advancements%2520in%2520ASR%252C%2520child%2520speech%2520recognition%2520remains%2520challenging%2520due%250Ato%2520acoustic%2520variability%2520and%2520limited%2520annotated%2520data.%2520While%2520fine-tuning%2520adult%2520ASR%250Amodels%2520on%2520child%2520speech%2520is%2520common%252C%2520comparisons%2520with%2520flat-start%2520training%2520remain%250Aunderexplored.%2520We%2520compare%2520flat-start%2520training%2520across%2520multiple%2520datasets%252C%2520SSL%250Arepresentations%2520%2528WavLM%252C%2520XEUS%2529%252C%2520and%2520decoder%2520architectures.%2520Our%2520results%2520show%2520that%250ASSL%2520representations%2520are%2520biased%2520toward%2520adult%2520speech%252C%2520with%2520flat-start%2520training%2520on%250Achild%2520speech%2520mitigating%2520these%2520biases.%2520We%2520also%2520analyze%2520model%2520scaling%252C%2520finding%250Aconsistent%2520improvements%2520up%2520to%25201B%2520parameters%252C%2520beyond%2520which%2520performance%2520plateaus.%250AAdditionally%252C%2520age-related%2520ASR%2520and%2520speaker%2520verification%2520analysis%2520highlights%2520the%250Alimitations%2520of%2520proprietary%2520models%2520like%2520Whisper%252C%2520emphasizing%2520the%2520need%2520for%250Aopen-data%2520models%2520for%2520reliable%2520child%2520speech%2520research.%2520All%2520investigations%2520are%250Aconducted%2520using%2520ESPnet%252C%2520and%2520our%2520publicly%2520available%2520benchmark%2520provides%2520insights%250Ainto%2520training%2520strategies%2520for%2520robust%2520child%2520speech%2520processing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16576v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Training%20Paradigms%2C%20Dataset%20Composition%2C%20and%20Model%20Scaling%0A%20%20for%20Child%20ASR%20in%20ESPnet&entry.906535625=Anyu%20Ying%20and%20Natarajan%20Balaji%20Shankar%20and%20Chyi-Jiunn%20Lin%20and%20Mohan%20Shi%20and%20Pu%20Wang%20and%20Hye-jin%20Shim%20and%20Siddhant%20Arora%20and%20Hugo%20Van%20hamme%20and%20Abeer%20Alwan%20and%20Shinji%20Watanabe&entry.1292438233=%20%20Despite%20advancements%20in%20ASR%2C%20child%20speech%20recognition%20remains%20challenging%20due%0Ato%20acoustic%20variability%20and%20limited%20annotated%20data.%20While%20fine-tuning%20adult%20ASR%0Amodels%20on%20child%20speech%20is%20common%2C%20comparisons%20with%20flat-start%20training%20remain%0Aunderexplored.%20We%20compare%20flat-start%20training%20across%20multiple%20datasets%2C%20SSL%0Arepresentations%20%28WavLM%2C%20XEUS%29%2C%20and%20decoder%20architectures.%20Our%20results%20show%20that%0ASSL%20representations%20are%20biased%20toward%20adult%20speech%2C%20with%20flat-start%20training%20on%0Achild%20speech%20mitigating%20these%20biases.%20We%20also%20analyze%20model%20scaling%2C%20finding%0Aconsistent%20improvements%20up%20to%201B%20parameters%2C%20beyond%20which%20performance%20plateaus.%0AAdditionally%2C%20age-related%20ASR%20and%20speaker%20verification%20analysis%20highlights%20the%0Alimitations%20of%20proprietary%20models%20like%20Whisper%2C%20emphasizing%20the%20need%20for%0Aopen-data%20models%20for%20reliable%20child%20speech%20research.%20All%20investigations%20are%0Aconducted%20using%20ESPnet%2C%20and%20our%20publicly%20available%20benchmark%20provides%20insights%0Ainto%20training%20strategies%20for%20robust%20child%20speech%20processing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16576v1&entry.124074799=Read"},
{"title": "Source-Guided Flow Matching", "author": "Zifan Wang and Alice Harting and Matthieu Barreau and Michael M. Zavlanos and Karl H. Johansson", "abstract": "  Guidance of generative models is typically achieved by modifying the\nprobability flow vector field through the addition of a guidance field. In this\npaper, we instead propose the Source-Guided Flow Matching (SGFM) framework,\nwhich modifies the source distribution directly while keeping the pre-trained\nvector field intact. This reduces the guidance problem to a well-defined\nproblem of sampling from the source distribution. We theoretically show that\nSGFM recovers the desired target distribution exactly. Furthermore, we provide\nbounds on the Wasserstein error for the generated distribution when using an\napproximate sampler of the source distribution and an approximate vector field.\nThe key benefit of our approach is that it allows the user to flexibly choose\nthe sampling method depending on their specific problem. To illustrate this, we\nsystematically compare different sampling methods and discuss conditions for\nasymptotically exact guidance. Moreover, our framework integrates well with\noptimal flow matching models since the straight transport map generated by the\nvector field is preserved. Experimental results on synthetic 2D benchmarks,\nphysics-informed generative tasks, and imaging inverse problems demonstrate the\neffectiveness and flexibility of the proposed framework.\n", "link": "http://arxiv.org/abs/2508.14807v2", "date": "2025-08-22", "relevancy": 2.0162, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5602}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4977}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4879}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Source-Guided%20Flow%20Matching&body=Title%3A%20Source-Guided%20Flow%20Matching%0AAuthor%3A%20Zifan%20Wang%20and%20Alice%20Harting%20and%20Matthieu%20Barreau%20and%20Michael%20M.%20Zavlanos%20and%20Karl%20H.%20Johansson%0AAbstract%3A%20%20%20Guidance%20of%20generative%20models%20is%20typically%20achieved%20by%20modifying%20the%0Aprobability%20flow%20vector%20field%20through%20the%20addition%20of%20a%20guidance%20field.%20In%20this%0Apaper%2C%20we%20instead%20propose%20the%20Source-Guided%20Flow%20Matching%20%28SGFM%29%20framework%2C%0Awhich%20modifies%20the%20source%20distribution%20directly%20while%20keeping%20the%20pre-trained%0Avector%20field%20intact.%20This%20reduces%20the%20guidance%20problem%20to%20a%20well-defined%0Aproblem%20of%20sampling%20from%20the%20source%20distribution.%20We%20theoretically%20show%20that%0ASGFM%20recovers%20the%20desired%20target%20distribution%20exactly.%20Furthermore%2C%20we%20provide%0Abounds%20on%20the%20Wasserstein%20error%20for%20the%20generated%20distribution%20when%20using%20an%0Aapproximate%20sampler%20of%20the%20source%20distribution%20and%20an%20approximate%20vector%20field.%0AThe%20key%20benefit%20of%20our%20approach%20is%20that%20it%20allows%20the%20user%20to%20flexibly%20choose%0Athe%20sampling%20method%20depending%20on%20their%20specific%20problem.%20To%20illustrate%20this%2C%20we%0Asystematically%20compare%20different%20sampling%20methods%20and%20discuss%20conditions%20for%0Aasymptotically%20exact%20guidance.%20Moreover%2C%20our%20framework%20integrates%20well%20with%0Aoptimal%20flow%20matching%20models%20since%20the%20straight%20transport%20map%20generated%20by%20the%0Avector%20field%20is%20preserved.%20Experimental%20results%20on%20synthetic%202D%20benchmarks%2C%0Aphysics-informed%20generative%20tasks%2C%20and%20imaging%20inverse%20problems%20demonstrate%20the%0Aeffectiveness%20and%20flexibility%20of%20the%20proposed%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14807v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSource-Guided%2520Flow%2520Matching%26entry.906535625%3DZifan%2520Wang%2520and%2520Alice%2520Harting%2520and%2520Matthieu%2520Barreau%2520and%2520Michael%2520M.%2520Zavlanos%2520and%2520Karl%2520H.%2520Johansson%26entry.1292438233%3D%2520%2520Guidance%2520of%2520generative%2520models%2520is%2520typically%2520achieved%2520by%2520modifying%2520the%250Aprobability%2520flow%2520vector%2520field%2520through%2520the%2520addition%2520of%2520a%2520guidance%2520field.%2520In%2520this%250Apaper%252C%2520we%2520instead%2520propose%2520the%2520Source-Guided%2520Flow%2520Matching%2520%2528SGFM%2529%2520framework%252C%250Awhich%2520modifies%2520the%2520source%2520distribution%2520directly%2520while%2520keeping%2520the%2520pre-trained%250Avector%2520field%2520intact.%2520This%2520reduces%2520the%2520guidance%2520problem%2520to%2520a%2520well-defined%250Aproblem%2520of%2520sampling%2520from%2520the%2520source%2520distribution.%2520We%2520theoretically%2520show%2520that%250ASGFM%2520recovers%2520the%2520desired%2520target%2520distribution%2520exactly.%2520Furthermore%252C%2520we%2520provide%250Abounds%2520on%2520the%2520Wasserstein%2520error%2520for%2520the%2520generated%2520distribution%2520when%2520using%2520an%250Aapproximate%2520sampler%2520of%2520the%2520source%2520distribution%2520and%2520an%2520approximate%2520vector%2520field.%250AThe%2520key%2520benefit%2520of%2520our%2520approach%2520is%2520that%2520it%2520allows%2520the%2520user%2520to%2520flexibly%2520choose%250Athe%2520sampling%2520method%2520depending%2520on%2520their%2520specific%2520problem.%2520To%2520illustrate%2520this%252C%2520we%250Asystematically%2520compare%2520different%2520sampling%2520methods%2520and%2520discuss%2520conditions%2520for%250Aasymptotically%2520exact%2520guidance.%2520Moreover%252C%2520our%2520framework%2520integrates%2520well%2520with%250Aoptimal%2520flow%2520matching%2520models%2520since%2520the%2520straight%2520transport%2520map%2520generated%2520by%2520the%250Avector%2520field%2520is%2520preserved.%2520Experimental%2520results%2520on%2520synthetic%25202D%2520benchmarks%252C%250Aphysics-informed%2520generative%2520tasks%252C%2520and%2520imaging%2520inverse%2520problems%2520demonstrate%2520the%250Aeffectiveness%2520and%2520flexibility%2520of%2520the%2520proposed%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14807v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Source-Guided%20Flow%20Matching&entry.906535625=Zifan%20Wang%20and%20Alice%20Harting%20and%20Matthieu%20Barreau%20and%20Michael%20M.%20Zavlanos%20and%20Karl%20H.%20Johansson&entry.1292438233=%20%20Guidance%20of%20generative%20models%20is%20typically%20achieved%20by%20modifying%20the%0Aprobability%20flow%20vector%20field%20through%20the%20addition%20of%20a%20guidance%20field.%20In%20this%0Apaper%2C%20we%20instead%20propose%20the%20Source-Guided%20Flow%20Matching%20%28SGFM%29%20framework%2C%0Awhich%20modifies%20the%20source%20distribution%20directly%20while%20keeping%20the%20pre-trained%0Avector%20field%20intact.%20This%20reduces%20the%20guidance%20problem%20to%20a%20well-defined%0Aproblem%20of%20sampling%20from%20the%20source%20distribution.%20We%20theoretically%20show%20that%0ASGFM%20recovers%20the%20desired%20target%20distribution%20exactly.%20Furthermore%2C%20we%20provide%0Abounds%20on%20the%20Wasserstein%20error%20for%20the%20generated%20distribution%20when%20using%20an%0Aapproximate%20sampler%20of%20the%20source%20distribution%20and%20an%20approximate%20vector%20field.%0AThe%20key%20benefit%20of%20our%20approach%20is%20that%20it%20allows%20the%20user%20to%20flexibly%20choose%0Athe%20sampling%20method%20depending%20on%20their%20specific%20problem.%20To%20illustrate%20this%2C%20we%0Asystematically%20compare%20different%20sampling%20methods%20and%20discuss%20conditions%20for%0Aasymptotically%20exact%20guidance.%20Moreover%2C%20our%20framework%20integrates%20well%20with%0Aoptimal%20flow%20matching%20models%20since%20the%20straight%20transport%20map%20generated%20by%20the%0Avector%20field%20is%20preserved.%20Experimental%20results%20on%20synthetic%202D%20benchmarks%2C%0Aphysics-informed%20generative%20tasks%2C%20and%20imaging%20inverse%20problems%20demonstrate%20the%0Aeffectiveness%20and%20flexibility%20of%20the%20proposed%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14807v2&entry.124074799=Read"},
{"title": "FedEFC: Federated Learning Using Enhanced Forward Correction Against\n  Noisy Labels", "author": "Seunghun Yu and Jin-Hyun Ahn and Joonhyuk Kang", "abstract": "  Federated Learning (FL) is a powerful framework for privacy-preserving\ndistributed learning. It enables multiple clients to collaboratively train a\nglobal model without sharing raw data. However, handling noisy labels in FL\nremains a major challenge due to heterogeneous data distributions and\ncommunication constraints, which can severely degrade model performance. To\naddress this issue, we propose FedEFC, a novel method designed to tackle the\nimpact of noisy labels in FL. FedEFC mitigates this issue through two key\ntechniques: (1) prestopping, which prevents overfitting to mislabeled data by\ndynamically halting training at an optimal point, and (2) loss correction,\nwhich adjusts model updates to account for label noise. In particular, we\ndevelop an effective loss correction tailored to the unique challenges of FL,\nincluding data heterogeneity and decentralized training. Furthermore, we\nprovide a theoretical analysis, leveraging the composite proper loss property,\nto demonstrate that the FL objective function under noisy label distributions\ncan be aligned with the clean label distribution. Extensive experimental\nresults validate the effectiveness of our approach, showing that it\nconsistently outperforms existing FL techniques in mitigating the impact of\nnoisy labels, particularly under heterogeneous data settings (e.g., achieving\nup to 41.64% relative performance improvement over the existing loss correction\nmethod).\n", "link": "http://arxiv.org/abs/2504.05615v2", "date": "2025-08-22", "relevancy": 2.011, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5394}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4967}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedEFC%3A%20Federated%20Learning%20Using%20Enhanced%20Forward%20Correction%20Against%0A%20%20Noisy%20Labels&body=Title%3A%20FedEFC%3A%20Federated%20Learning%20Using%20Enhanced%20Forward%20Correction%20Against%0A%20%20Noisy%20Labels%0AAuthor%3A%20Seunghun%20Yu%20and%20Jin-Hyun%20Ahn%20and%20Joonhyuk%20Kang%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20is%20a%20powerful%20framework%20for%20privacy-preserving%0Adistributed%20learning.%20It%20enables%20multiple%20clients%20to%20collaboratively%20train%20a%0Aglobal%20model%20without%20sharing%20raw%20data.%20However%2C%20handling%20noisy%20labels%20in%20FL%0Aremains%20a%20major%20challenge%20due%20to%20heterogeneous%20data%20distributions%20and%0Acommunication%20constraints%2C%20which%20can%20severely%20degrade%20model%20performance.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20FedEFC%2C%20a%20novel%20method%20designed%20to%20tackle%20the%0Aimpact%20of%20noisy%20labels%20in%20FL.%20FedEFC%20mitigates%20this%20issue%20through%20two%20key%0Atechniques%3A%20%281%29%20prestopping%2C%20which%20prevents%20overfitting%20to%20mislabeled%20data%20by%0Adynamically%20halting%20training%20at%20an%20optimal%20point%2C%20and%20%282%29%20loss%20correction%2C%0Awhich%20adjusts%20model%20updates%20to%20account%20for%20label%20noise.%20In%20particular%2C%20we%0Adevelop%20an%20effective%20loss%20correction%20tailored%20to%20the%20unique%20challenges%20of%20FL%2C%0Aincluding%20data%20heterogeneity%20and%20decentralized%20training.%20Furthermore%2C%20we%0Aprovide%20a%20theoretical%20analysis%2C%20leveraging%20the%20composite%20proper%20loss%20property%2C%0Ato%20demonstrate%20that%20the%20FL%20objective%20function%20under%20noisy%20label%20distributions%0Acan%20be%20aligned%20with%20the%20clean%20label%20distribution.%20Extensive%20experimental%0Aresults%20validate%20the%20effectiveness%20of%20our%20approach%2C%20showing%20that%20it%0Aconsistently%20outperforms%20existing%20FL%20techniques%20in%20mitigating%20the%20impact%20of%0Anoisy%20labels%2C%20particularly%20under%20heterogeneous%20data%20settings%20%28e.g.%2C%20achieving%0Aup%20to%2041.64%25%20relative%20performance%20improvement%20over%20the%20existing%20loss%20correction%0Amethod%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.05615v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedEFC%253A%2520Federated%2520Learning%2520Using%2520Enhanced%2520Forward%2520Correction%2520Against%250A%2520%2520Noisy%2520Labels%26entry.906535625%3DSeunghun%2520Yu%2520and%2520Jin-Hyun%2520Ahn%2520and%2520Joonhyuk%2520Kang%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520is%2520a%2520powerful%2520framework%2520for%2520privacy-preserving%250Adistributed%2520learning.%2520It%2520enables%2520multiple%2520clients%2520to%2520collaboratively%2520train%2520a%250Aglobal%2520model%2520without%2520sharing%2520raw%2520data.%2520However%252C%2520handling%2520noisy%2520labels%2520in%2520FL%250Aremains%2520a%2520major%2520challenge%2520due%2520to%2520heterogeneous%2520data%2520distributions%2520and%250Acommunication%2520constraints%252C%2520which%2520can%2520severely%2520degrade%2520model%2520performance.%2520To%250Aaddress%2520this%2520issue%252C%2520we%2520propose%2520FedEFC%252C%2520a%2520novel%2520method%2520designed%2520to%2520tackle%2520the%250Aimpact%2520of%2520noisy%2520labels%2520in%2520FL.%2520FedEFC%2520mitigates%2520this%2520issue%2520through%2520two%2520key%250Atechniques%253A%2520%25281%2529%2520prestopping%252C%2520which%2520prevents%2520overfitting%2520to%2520mislabeled%2520data%2520by%250Adynamically%2520halting%2520training%2520at%2520an%2520optimal%2520point%252C%2520and%2520%25282%2529%2520loss%2520correction%252C%250Awhich%2520adjusts%2520model%2520updates%2520to%2520account%2520for%2520label%2520noise.%2520In%2520particular%252C%2520we%250Adevelop%2520an%2520effective%2520loss%2520correction%2520tailored%2520to%2520the%2520unique%2520challenges%2520of%2520FL%252C%250Aincluding%2520data%2520heterogeneity%2520and%2520decentralized%2520training.%2520Furthermore%252C%2520we%250Aprovide%2520a%2520theoretical%2520analysis%252C%2520leveraging%2520the%2520composite%2520proper%2520loss%2520property%252C%250Ato%2520demonstrate%2520that%2520the%2520FL%2520objective%2520function%2520under%2520noisy%2520label%2520distributions%250Acan%2520be%2520aligned%2520with%2520the%2520clean%2520label%2520distribution.%2520Extensive%2520experimental%250Aresults%2520validate%2520the%2520effectiveness%2520of%2520our%2520approach%252C%2520showing%2520that%2520it%250Aconsistently%2520outperforms%2520existing%2520FL%2520techniques%2520in%2520mitigating%2520the%2520impact%2520of%250Anoisy%2520labels%252C%2520particularly%2520under%2520heterogeneous%2520data%2520settings%2520%2528e.g.%252C%2520achieving%250Aup%2520to%252041.64%2525%2520relative%2520performance%2520improvement%2520over%2520the%2520existing%2520loss%2520correction%250Amethod%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.05615v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedEFC%3A%20Federated%20Learning%20Using%20Enhanced%20Forward%20Correction%20Against%0A%20%20Noisy%20Labels&entry.906535625=Seunghun%20Yu%20and%20Jin-Hyun%20Ahn%20and%20Joonhyuk%20Kang&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20is%20a%20powerful%20framework%20for%20privacy-preserving%0Adistributed%20learning.%20It%20enables%20multiple%20clients%20to%20collaboratively%20train%20a%0Aglobal%20model%20without%20sharing%20raw%20data.%20However%2C%20handling%20noisy%20labels%20in%20FL%0Aremains%20a%20major%20challenge%20due%20to%20heterogeneous%20data%20distributions%20and%0Acommunication%20constraints%2C%20which%20can%20severely%20degrade%20model%20performance.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20FedEFC%2C%20a%20novel%20method%20designed%20to%20tackle%20the%0Aimpact%20of%20noisy%20labels%20in%20FL.%20FedEFC%20mitigates%20this%20issue%20through%20two%20key%0Atechniques%3A%20%281%29%20prestopping%2C%20which%20prevents%20overfitting%20to%20mislabeled%20data%20by%0Adynamically%20halting%20training%20at%20an%20optimal%20point%2C%20and%20%282%29%20loss%20correction%2C%0Awhich%20adjusts%20model%20updates%20to%20account%20for%20label%20noise.%20In%20particular%2C%20we%0Adevelop%20an%20effective%20loss%20correction%20tailored%20to%20the%20unique%20challenges%20of%20FL%2C%0Aincluding%20data%20heterogeneity%20and%20decentralized%20training.%20Furthermore%2C%20we%0Aprovide%20a%20theoretical%20analysis%2C%20leveraging%20the%20composite%20proper%20loss%20property%2C%0Ato%20demonstrate%20that%20the%20FL%20objective%20function%20under%20noisy%20label%20distributions%0Acan%20be%20aligned%20with%20the%20clean%20label%20distribution.%20Extensive%20experimental%0Aresults%20validate%20the%20effectiveness%20of%20our%20approach%2C%20showing%20that%20it%0Aconsistently%20outperforms%20existing%20FL%20techniques%20in%20mitigating%20the%20impact%20of%0Anoisy%20labels%2C%20particularly%20under%20heterogeneous%20data%20settings%20%28e.g.%2C%20achieving%0Aup%20to%2041.64%25%20relative%20performance%20improvement%20over%20the%20existing%20loss%20correction%0Amethod%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.05615v2&entry.124074799=Read"},
{"title": "On the Evolution of Federated Post-Training Large Language Models: A\n  Model Accessibility View", "author": "Tao Guo and Junxiao Wang and Fushuo Huo and Laizhong Cui and Song Guo and Jie Gui and Dacheng Tao", "abstract": "  Federated Learning (FL) enables training models across decentralized data\nsilos while preserving client data privacy. Recent research has explored\nefficient methods for post-training large language models (LLMs) within FL to\naddress computational and communication challenges. While existing approaches\noften rely on access to LLMs' internal information, which is frequently\nrestricted in real-world scenarios, an inference-only paradigm (black-box\nFedLLM) has emerged to address these limitations. This paper presents a\ncomprehensive survey on federated tuning for LLMs. We propose a taxonomy\ncategorizing existing studies along two axes: model access-based and parameter\nefficiency-based optimization. We classify FedLLM approaches into white-box,\ngray-box, and black-box techniques, highlighting representative methods within\neach category. We review emerging research treating LLMs as black-box inference\nAPIs and discuss promising directions and open challenges for future research.\n", "link": "http://arxiv.org/abs/2508.16261v1", "date": "2025-08-22", "relevancy": 2.0032, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5034}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5034}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Evolution%20of%20Federated%20Post-Training%20Large%20Language%20Models%3A%20A%0A%20%20Model%20Accessibility%20View&body=Title%3A%20On%20the%20Evolution%20of%20Federated%20Post-Training%20Large%20Language%20Models%3A%20A%0A%20%20Model%20Accessibility%20View%0AAuthor%3A%20Tao%20Guo%20and%20Junxiao%20Wang%20and%20Fushuo%20Huo%20and%20Laizhong%20Cui%20and%20Song%20Guo%20and%20Jie%20Gui%20and%20Dacheng%20Tao%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20enables%20training%20models%20across%20decentralized%20data%0Asilos%20while%20preserving%20client%20data%20privacy.%20Recent%20research%20has%20explored%0Aefficient%20methods%20for%20post-training%20large%20language%20models%20%28LLMs%29%20within%20FL%20to%0Aaddress%20computational%20and%20communication%20challenges.%20While%20existing%20approaches%0Aoften%20rely%20on%20access%20to%20LLMs%27%20internal%20information%2C%20which%20is%20frequently%0Arestricted%20in%20real-world%20scenarios%2C%20an%20inference-only%20paradigm%20%28black-box%0AFedLLM%29%20has%20emerged%20to%20address%20these%20limitations.%20This%20paper%20presents%20a%0Acomprehensive%20survey%20on%20federated%20tuning%20for%20LLMs.%20We%20propose%20a%20taxonomy%0Acategorizing%20existing%20studies%20along%20two%20axes%3A%20model%20access-based%20and%20parameter%0Aefficiency-based%20optimization.%20We%20classify%20FedLLM%20approaches%20into%20white-box%2C%0Agray-box%2C%20and%20black-box%20techniques%2C%20highlighting%20representative%20methods%20within%0Aeach%20category.%20We%20review%20emerging%20research%20treating%20LLMs%20as%20black-box%20inference%0AAPIs%20and%20discuss%20promising%20directions%20and%20open%20challenges%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16261v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Evolution%2520of%2520Federated%2520Post-Training%2520Large%2520Language%2520Models%253A%2520A%250A%2520%2520Model%2520Accessibility%2520View%26entry.906535625%3DTao%2520Guo%2520and%2520Junxiao%2520Wang%2520and%2520Fushuo%2520Huo%2520and%2520Laizhong%2520Cui%2520and%2520Song%2520Guo%2520and%2520Jie%2520Gui%2520and%2520Dacheng%2520Tao%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520enables%2520training%2520models%2520across%2520decentralized%2520data%250Asilos%2520while%2520preserving%2520client%2520data%2520privacy.%2520Recent%2520research%2520has%2520explored%250Aefficient%2520methods%2520for%2520post-training%2520large%2520language%2520models%2520%2528LLMs%2529%2520within%2520FL%2520to%250Aaddress%2520computational%2520and%2520communication%2520challenges.%2520While%2520existing%2520approaches%250Aoften%2520rely%2520on%2520access%2520to%2520LLMs%2527%2520internal%2520information%252C%2520which%2520is%2520frequently%250Arestricted%2520in%2520real-world%2520scenarios%252C%2520an%2520inference-only%2520paradigm%2520%2528black-box%250AFedLLM%2529%2520has%2520emerged%2520to%2520address%2520these%2520limitations.%2520This%2520paper%2520presents%2520a%250Acomprehensive%2520survey%2520on%2520federated%2520tuning%2520for%2520LLMs.%2520We%2520propose%2520a%2520taxonomy%250Acategorizing%2520existing%2520studies%2520along%2520two%2520axes%253A%2520model%2520access-based%2520and%2520parameter%250Aefficiency-based%2520optimization.%2520We%2520classify%2520FedLLM%2520approaches%2520into%2520white-box%252C%250Agray-box%252C%2520and%2520black-box%2520techniques%252C%2520highlighting%2520representative%2520methods%2520within%250Aeach%2520category.%2520We%2520review%2520emerging%2520research%2520treating%2520LLMs%2520as%2520black-box%2520inference%250AAPIs%2520and%2520discuss%2520promising%2520directions%2520and%2520open%2520challenges%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16261v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Evolution%20of%20Federated%20Post-Training%20Large%20Language%20Models%3A%20A%0A%20%20Model%20Accessibility%20View&entry.906535625=Tao%20Guo%20and%20Junxiao%20Wang%20and%20Fushuo%20Huo%20and%20Laizhong%20Cui%20and%20Song%20Guo%20and%20Jie%20Gui%20and%20Dacheng%20Tao&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20enables%20training%20models%20across%20decentralized%20data%0Asilos%20while%20preserving%20client%20data%20privacy.%20Recent%20research%20has%20explored%0Aefficient%20methods%20for%20post-training%20large%20language%20models%20%28LLMs%29%20within%20FL%20to%0Aaddress%20computational%20and%20communication%20challenges.%20While%20existing%20approaches%0Aoften%20rely%20on%20access%20to%20LLMs%27%20internal%20information%2C%20which%20is%20frequently%0Arestricted%20in%20real-world%20scenarios%2C%20an%20inference-only%20paradigm%20%28black-box%0AFedLLM%29%20has%20emerged%20to%20address%20these%20limitations.%20This%20paper%20presents%20a%0Acomprehensive%20survey%20on%20federated%20tuning%20for%20LLMs.%20We%20propose%20a%20taxonomy%0Acategorizing%20existing%20studies%20along%20two%20axes%3A%20model%20access-based%20and%20parameter%0Aefficiency-based%20optimization.%20We%20classify%20FedLLM%20approaches%20into%20white-box%2C%0Agray-box%2C%20and%20black-box%20techniques%2C%20highlighting%20representative%20methods%20within%0Aeach%20category.%20We%20review%20emerging%20research%20treating%20LLMs%20as%20black-box%20inference%0AAPIs%20and%20discuss%20promising%20directions%20and%20open%20challenges%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16261v1&entry.124074799=Read"},
{"title": "Robustness of deep learning classification to adversarial input on GPUs:\n  asynchronous parallel accumulation is a source of vulnerability", "author": "Sanjif Shanmugavelu and Mathieu Taillefumier and Christopher Culver and Vijay Ganesh and Oscar Hernandez and Ada Sedova", "abstract": "  The ability of machine learning (ML) classification models to resist small,\ntargeted input perturbations -- known as adversarial attacks -- is a key\nmeasure of their safety and reliability. We show that floating-point\nnon-associativity (FPNA) coupled with asynchronous parallel programming on GPUs\nis sufficient to result in misclassification, without any perturbation to the\ninput. Additionally, we show that standard adversarial robustness results may\nbe overestimated up to 4.6 when not considering machine-level details. We\ndevelop a novel black-box attack using Bayesian optimization to discover\nexternal workloads that can change the instruction scheduling which bias the\noutput of reductions on GPUs and reliably lead to misclassification. Motivated\nby these results, we present a new learnable permutation (LP) gradient-based\napproach to learning floating-point operation orderings that lead to\nmisclassifications. The LP approach provides a worst-case estimate in a\ncomputationally efficient manner, avoiding the need to run identical\nexperiments tens of thousands of times over a potentially large set of possible\nGPU states or architectures. Finally, using instrumentation-based testing, we\ninvestigate parallel reduction ordering across different GPU architectures\nunder external background workloads, when utilizing multi-GPU virtualization,\nand when applying power capping. Our results demonstrate that parallel\nreduction ordering varies significantly across architectures under the first\ntwo conditions, substantially increasing the search space required to fully\ntest the effects of this parallel scheduler-based vulnerability. These results\nand the methods developed here can help to include machine-level considerations\ninto adversarial robustness assessments, which can make a difference in safety\nand mission critical applications.\n", "link": "http://arxiv.org/abs/2503.17173v2", "date": "2025-08-22", "relevancy": 1.9926, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4999}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4983}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robustness%20of%20deep%20learning%20classification%20to%20adversarial%20input%20on%20GPUs%3A%0A%20%20asynchronous%20parallel%20accumulation%20is%20a%20source%20of%20vulnerability&body=Title%3A%20Robustness%20of%20deep%20learning%20classification%20to%20adversarial%20input%20on%20GPUs%3A%0A%20%20asynchronous%20parallel%20accumulation%20is%20a%20source%20of%20vulnerability%0AAuthor%3A%20Sanjif%20Shanmugavelu%20and%20Mathieu%20Taillefumier%20and%20Christopher%20Culver%20and%20Vijay%20Ganesh%20and%20Oscar%20Hernandez%20and%20Ada%20Sedova%0AAbstract%3A%20%20%20The%20ability%20of%20machine%20learning%20%28ML%29%20classification%20models%20to%20resist%20small%2C%0Atargeted%20input%20perturbations%20--%20known%20as%20adversarial%20attacks%20--%20is%20a%20key%0Ameasure%20of%20their%20safety%20and%20reliability.%20We%20show%20that%20floating-point%0Anon-associativity%20%28FPNA%29%20coupled%20with%20asynchronous%20parallel%20programming%20on%20GPUs%0Ais%20sufficient%20to%20result%20in%20misclassification%2C%20without%20any%20perturbation%20to%20the%0Ainput.%20Additionally%2C%20we%20show%20that%20standard%20adversarial%20robustness%20results%20may%0Abe%20overestimated%20up%20to%204.6%20when%20not%20considering%20machine-level%20details.%20We%0Adevelop%20a%20novel%20black-box%20attack%20using%20Bayesian%20optimization%20to%20discover%0Aexternal%20workloads%20that%20can%20change%20the%20instruction%20scheduling%20which%20bias%20the%0Aoutput%20of%20reductions%20on%20GPUs%20and%20reliably%20lead%20to%20misclassification.%20Motivated%0Aby%20these%20results%2C%20we%20present%20a%20new%20learnable%20permutation%20%28LP%29%20gradient-based%0Aapproach%20to%20learning%20floating-point%20operation%20orderings%20that%20lead%20to%0Amisclassifications.%20The%20LP%20approach%20provides%20a%20worst-case%20estimate%20in%20a%0Acomputationally%20efficient%20manner%2C%20avoiding%20the%20need%20to%20run%20identical%0Aexperiments%20tens%20of%20thousands%20of%20times%20over%20a%20potentially%20large%20set%20of%20possible%0AGPU%20states%20or%20architectures.%20Finally%2C%20using%20instrumentation-based%20testing%2C%20we%0Ainvestigate%20parallel%20reduction%20ordering%20across%20different%20GPU%20architectures%0Aunder%20external%20background%20workloads%2C%20when%20utilizing%20multi-GPU%20virtualization%2C%0Aand%20when%20applying%20power%20capping.%20Our%20results%20demonstrate%20that%20parallel%0Areduction%20ordering%20varies%20significantly%20across%20architectures%20under%20the%20first%0Atwo%20conditions%2C%20substantially%20increasing%20the%20search%20space%20required%20to%20fully%0Atest%20the%20effects%20of%20this%20parallel%20scheduler-based%20vulnerability.%20These%20results%0Aand%20the%20methods%20developed%20here%20can%20help%20to%20include%20machine-level%20considerations%0Ainto%20adversarial%20robustness%20assessments%2C%20which%20can%20make%20a%20difference%20in%20safety%0Aand%20mission%20critical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.17173v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobustness%2520of%2520deep%2520learning%2520classification%2520to%2520adversarial%2520input%2520on%2520GPUs%253A%250A%2520%2520asynchronous%2520parallel%2520accumulation%2520is%2520a%2520source%2520of%2520vulnerability%26entry.906535625%3DSanjif%2520Shanmugavelu%2520and%2520Mathieu%2520Taillefumier%2520and%2520Christopher%2520Culver%2520and%2520Vijay%2520Ganesh%2520and%2520Oscar%2520Hernandez%2520and%2520Ada%2520Sedova%26entry.1292438233%3D%2520%2520The%2520ability%2520of%2520machine%2520learning%2520%2528ML%2529%2520classification%2520models%2520to%2520resist%2520small%252C%250Atargeted%2520input%2520perturbations%2520--%2520known%2520as%2520adversarial%2520attacks%2520--%2520is%2520a%2520key%250Ameasure%2520of%2520their%2520safety%2520and%2520reliability.%2520We%2520show%2520that%2520floating-point%250Anon-associativity%2520%2528FPNA%2529%2520coupled%2520with%2520asynchronous%2520parallel%2520programming%2520on%2520GPUs%250Ais%2520sufficient%2520to%2520result%2520in%2520misclassification%252C%2520without%2520any%2520perturbation%2520to%2520the%250Ainput.%2520Additionally%252C%2520we%2520show%2520that%2520standard%2520adversarial%2520robustness%2520results%2520may%250Abe%2520overestimated%2520up%2520to%25204.6%2520when%2520not%2520considering%2520machine-level%2520details.%2520We%250Adevelop%2520a%2520novel%2520black-box%2520attack%2520using%2520Bayesian%2520optimization%2520to%2520discover%250Aexternal%2520workloads%2520that%2520can%2520change%2520the%2520instruction%2520scheduling%2520which%2520bias%2520the%250Aoutput%2520of%2520reductions%2520on%2520GPUs%2520and%2520reliably%2520lead%2520to%2520misclassification.%2520Motivated%250Aby%2520these%2520results%252C%2520we%2520present%2520a%2520new%2520learnable%2520permutation%2520%2528LP%2529%2520gradient-based%250Aapproach%2520to%2520learning%2520floating-point%2520operation%2520orderings%2520that%2520lead%2520to%250Amisclassifications.%2520The%2520LP%2520approach%2520provides%2520a%2520worst-case%2520estimate%2520in%2520a%250Acomputationally%2520efficient%2520manner%252C%2520avoiding%2520the%2520need%2520to%2520run%2520identical%250Aexperiments%2520tens%2520of%2520thousands%2520of%2520times%2520over%2520a%2520potentially%2520large%2520set%2520of%2520possible%250AGPU%2520states%2520or%2520architectures.%2520Finally%252C%2520using%2520instrumentation-based%2520testing%252C%2520we%250Ainvestigate%2520parallel%2520reduction%2520ordering%2520across%2520different%2520GPU%2520architectures%250Aunder%2520external%2520background%2520workloads%252C%2520when%2520utilizing%2520multi-GPU%2520virtualization%252C%250Aand%2520when%2520applying%2520power%2520capping.%2520Our%2520results%2520demonstrate%2520that%2520parallel%250Areduction%2520ordering%2520varies%2520significantly%2520across%2520architectures%2520under%2520the%2520first%250Atwo%2520conditions%252C%2520substantially%2520increasing%2520the%2520search%2520space%2520required%2520to%2520fully%250Atest%2520the%2520effects%2520of%2520this%2520parallel%2520scheduler-based%2520vulnerability.%2520These%2520results%250Aand%2520the%2520methods%2520developed%2520here%2520can%2520help%2520to%2520include%2520machine-level%2520considerations%250Ainto%2520adversarial%2520robustness%2520assessments%252C%2520which%2520can%2520make%2520a%2520difference%2520in%2520safety%250Aand%2520mission%2520critical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.17173v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robustness%20of%20deep%20learning%20classification%20to%20adversarial%20input%20on%20GPUs%3A%0A%20%20asynchronous%20parallel%20accumulation%20is%20a%20source%20of%20vulnerability&entry.906535625=Sanjif%20Shanmugavelu%20and%20Mathieu%20Taillefumier%20and%20Christopher%20Culver%20and%20Vijay%20Ganesh%20and%20Oscar%20Hernandez%20and%20Ada%20Sedova&entry.1292438233=%20%20The%20ability%20of%20machine%20learning%20%28ML%29%20classification%20models%20to%20resist%20small%2C%0Atargeted%20input%20perturbations%20--%20known%20as%20adversarial%20attacks%20--%20is%20a%20key%0Ameasure%20of%20their%20safety%20and%20reliability.%20We%20show%20that%20floating-point%0Anon-associativity%20%28FPNA%29%20coupled%20with%20asynchronous%20parallel%20programming%20on%20GPUs%0Ais%20sufficient%20to%20result%20in%20misclassification%2C%20without%20any%20perturbation%20to%20the%0Ainput.%20Additionally%2C%20we%20show%20that%20standard%20adversarial%20robustness%20results%20may%0Abe%20overestimated%20up%20to%204.6%20when%20not%20considering%20machine-level%20details.%20We%0Adevelop%20a%20novel%20black-box%20attack%20using%20Bayesian%20optimization%20to%20discover%0Aexternal%20workloads%20that%20can%20change%20the%20instruction%20scheduling%20which%20bias%20the%0Aoutput%20of%20reductions%20on%20GPUs%20and%20reliably%20lead%20to%20misclassification.%20Motivated%0Aby%20these%20results%2C%20we%20present%20a%20new%20learnable%20permutation%20%28LP%29%20gradient-based%0Aapproach%20to%20learning%20floating-point%20operation%20orderings%20that%20lead%20to%0Amisclassifications.%20The%20LP%20approach%20provides%20a%20worst-case%20estimate%20in%20a%0Acomputationally%20efficient%20manner%2C%20avoiding%20the%20need%20to%20run%20identical%0Aexperiments%20tens%20of%20thousands%20of%20times%20over%20a%20potentially%20large%20set%20of%20possible%0AGPU%20states%20or%20architectures.%20Finally%2C%20using%20instrumentation-based%20testing%2C%20we%0Ainvestigate%20parallel%20reduction%20ordering%20across%20different%20GPU%20architectures%0Aunder%20external%20background%20workloads%2C%20when%20utilizing%20multi-GPU%20virtualization%2C%0Aand%20when%20applying%20power%20capping.%20Our%20results%20demonstrate%20that%20parallel%0Areduction%20ordering%20varies%20significantly%20across%20architectures%20under%20the%20first%0Atwo%20conditions%2C%20substantially%20increasing%20the%20search%20space%20required%20to%20fully%0Atest%20the%20effects%20of%20this%20parallel%20scheduler-based%20vulnerability.%20These%20results%0Aand%20the%20methods%20developed%20here%20can%20help%20to%20include%20machine-level%20considerations%0Ainto%20adversarial%20robustness%20assessments%2C%20which%20can%20make%20a%20difference%20in%20safety%0Aand%20mission%20critical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.17173v2&entry.124074799=Read"},
{"title": "Representation Learning of Auxiliary Concepts for Improved Student\n  Modeling and Exercise Recommendation", "author": "Yahya Badran and Christine Preisach", "abstract": "  Personalized recommendation is a key feature of intelligent tutoring systems,\ntypically relying on accurate models of student knowledge. Knowledge Tracing\n(KT) models enable this by estimating a student's mastery based on their\nhistorical interactions. Many KT models rely on human-annotated knowledge\nconcepts (KCs), which tag each exercise with one or more skills or concepts\nbelieved to be necessary for solving it. However, these KCs can be incomplete,\nerror-prone, or overly general.\n  In this paper, we propose a deep learning model that learns sparse binary\nrepresentations of exercises, where each bit indicates the presence or absence\nof a latent concept. We refer to these representations as auxiliary KCs. These\nrepresentations capture conceptual structure beyond human-defined annotations\nand are compatible with both classical models (e.g., BKT) and modern deep\nlearning KT architectures.\n  We demonstrate that incorporating auxiliary KCs improves both student\nmodeling and adaptive exercise recommendation. For student modeling, we show\nthat augmenting classical models like BKT with auxiliary KCs leads to improved\npredictive performance. For recommendation, we show that using auxiliary KCs\nenhances both reinforcement learning-based policies and a simple planning-based\nmethod (expectimax), resulting in measurable gains in student learning outcomes\nwithin a simulated student environment.\n", "link": "http://arxiv.org/abs/2508.16269v1", "date": "2025-08-22", "relevancy": 1.9891, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5192}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4965}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4893}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Representation%20Learning%20of%20Auxiliary%20Concepts%20for%20Improved%20Student%0A%20%20Modeling%20and%20Exercise%20Recommendation&body=Title%3A%20Representation%20Learning%20of%20Auxiliary%20Concepts%20for%20Improved%20Student%0A%20%20Modeling%20and%20Exercise%20Recommendation%0AAuthor%3A%20Yahya%20Badran%20and%20Christine%20Preisach%0AAbstract%3A%20%20%20Personalized%20recommendation%20is%20a%20key%20feature%20of%20intelligent%20tutoring%20systems%2C%0Atypically%20relying%20on%20accurate%20models%20of%20student%20knowledge.%20Knowledge%20Tracing%0A%28KT%29%20models%20enable%20this%20by%20estimating%20a%20student%27s%20mastery%20based%20on%20their%0Ahistorical%20interactions.%20Many%20KT%20models%20rely%20on%20human-annotated%20knowledge%0Aconcepts%20%28KCs%29%2C%20which%20tag%20each%20exercise%20with%20one%20or%20more%20skills%20or%20concepts%0Abelieved%20to%20be%20necessary%20for%20solving%20it.%20However%2C%20these%20KCs%20can%20be%20incomplete%2C%0Aerror-prone%2C%20or%20overly%20general.%0A%20%20In%20this%20paper%2C%20we%20propose%20a%20deep%20learning%20model%20that%20learns%20sparse%20binary%0Arepresentations%20of%20exercises%2C%20where%20each%20bit%20indicates%20the%20presence%20or%20absence%0Aof%20a%20latent%20concept.%20We%20refer%20to%20these%20representations%20as%20auxiliary%20KCs.%20These%0Arepresentations%20capture%20conceptual%20structure%20beyond%20human-defined%20annotations%0Aand%20are%20compatible%20with%20both%20classical%20models%20%28e.g.%2C%20BKT%29%20and%20modern%20deep%0Alearning%20KT%20architectures.%0A%20%20We%20demonstrate%20that%20incorporating%20auxiliary%20KCs%20improves%20both%20student%0Amodeling%20and%20adaptive%20exercise%20recommendation.%20For%20student%20modeling%2C%20we%20show%0Athat%20augmenting%20classical%20models%20like%20BKT%20with%20auxiliary%20KCs%20leads%20to%20improved%0Apredictive%20performance.%20For%20recommendation%2C%20we%20show%20that%20using%20auxiliary%20KCs%0Aenhances%20both%20reinforcement%20learning-based%20policies%20and%20a%20simple%20planning-based%0Amethod%20%28expectimax%29%2C%20resulting%20in%20measurable%20gains%20in%20student%20learning%20outcomes%0Awithin%20a%20simulated%20student%20environment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16269v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepresentation%2520Learning%2520of%2520Auxiliary%2520Concepts%2520for%2520Improved%2520Student%250A%2520%2520Modeling%2520and%2520Exercise%2520Recommendation%26entry.906535625%3DYahya%2520Badran%2520and%2520Christine%2520Preisach%26entry.1292438233%3D%2520%2520Personalized%2520recommendation%2520is%2520a%2520key%2520feature%2520of%2520intelligent%2520tutoring%2520systems%252C%250Atypically%2520relying%2520on%2520accurate%2520models%2520of%2520student%2520knowledge.%2520Knowledge%2520Tracing%250A%2528KT%2529%2520models%2520enable%2520this%2520by%2520estimating%2520a%2520student%2527s%2520mastery%2520based%2520on%2520their%250Ahistorical%2520interactions.%2520Many%2520KT%2520models%2520rely%2520on%2520human-annotated%2520knowledge%250Aconcepts%2520%2528KCs%2529%252C%2520which%2520tag%2520each%2520exercise%2520with%2520one%2520or%2520more%2520skills%2520or%2520concepts%250Abelieved%2520to%2520be%2520necessary%2520for%2520solving%2520it.%2520However%252C%2520these%2520KCs%2520can%2520be%2520incomplete%252C%250Aerror-prone%252C%2520or%2520overly%2520general.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520deep%2520learning%2520model%2520that%2520learns%2520sparse%2520binary%250Arepresentations%2520of%2520exercises%252C%2520where%2520each%2520bit%2520indicates%2520the%2520presence%2520or%2520absence%250Aof%2520a%2520latent%2520concept.%2520We%2520refer%2520to%2520these%2520representations%2520as%2520auxiliary%2520KCs.%2520These%250Arepresentations%2520capture%2520conceptual%2520structure%2520beyond%2520human-defined%2520annotations%250Aand%2520are%2520compatible%2520with%2520both%2520classical%2520models%2520%2528e.g.%252C%2520BKT%2529%2520and%2520modern%2520deep%250Alearning%2520KT%2520architectures.%250A%2520%2520We%2520demonstrate%2520that%2520incorporating%2520auxiliary%2520KCs%2520improves%2520both%2520student%250Amodeling%2520and%2520adaptive%2520exercise%2520recommendation.%2520For%2520student%2520modeling%252C%2520we%2520show%250Athat%2520augmenting%2520classical%2520models%2520like%2520BKT%2520with%2520auxiliary%2520KCs%2520leads%2520to%2520improved%250Apredictive%2520performance.%2520For%2520recommendation%252C%2520we%2520show%2520that%2520using%2520auxiliary%2520KCs%250Aenhances%2520both%2520reinforcement%2520learning-based%2520policies%2520and%2520a%2520simple%2520planning-based%250Amethod%2520%2528expectimax%2529%252C%2520resulting%2520in%2520measurable%2520gains%2520in%2520student%2520learning%2520outcomes%250Awithin%2520a%2520simulated%2520student%2520environment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16269v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Representation%20Learning%20of%20Auxiliary%20Concepts%20for%20Improved%20Student%0A%20%20Modeling%20and%20Exercise%20Recommendation&entry.906535625=Yahya%20Badran%20and%20Christine%20Preisach&entry.1292438233=%20%20Personalized%20recommendation%20is%20a%20key%20feature%20of%20intelligent%20tutoring%20systems%2C%0Atypically%20relying%20on%20accurate%20models%20of%20student%20knowledge.%20Knowledge%20Tracing%0A%28KT%29%20models%20enable%20this%20by%20estimating%20a%20student%27s%20mastery%20based%20on%20their%0Ahistorical%20interactions.%20Many%20KT%20models%20rely%20on%20human-annotated%20knowledge%0Aconcepts%20%28KCs%29%2C%20which%20tag%20each%20exercise%20with%20one%20or%20more%20skills%20or%20concepts%0Abelieved%20to%20be%20necessary%20for%20solving%20it.%20However%2C%20these%20KCs%20can%20be%20incomplete%2C%0Aerror-prone%2C%20or%20overly%20general.%0A%20%20In%20this%20paper%2C%20we%20propose%20a%20deep%20learning%20model%20that%20learns%20sparse%20binary%0Arepresentations%20of%20exercises%2C%20where%20each%20bit%20indicates%20the%20presence%20or%20absence%0Aof%20a%20latent%20concept.%20We%20refer%20to%20these%20representations%20as%20auxiliary%20KCs.%20These%0Arepresentations%20capture%20conceptual%20structure%20beyond%20human-defined%20annotations%0Aand%20are%20compatible%20with%20both%20classical%20models%20%28e.g.%2C%20BKT%29%20and%20modern%20deep%0Alearning%20KT%20architectures.%0A%20%20We%20demonstrate%20that%20incorporating%20auxiliary%20KCs%20improves%20both%20student%0Amodeling%20and%20adaptive%20exercise%20recommendation.%20For%20student%20modeling%2C%20we%20show%0Athat%20augmenting%20classical%20models%20like%20BKT%20with%20auxiliary%20KCs%20leads%20to%20improved%0Apredictive%20performance.%20For%20recommendation%2C%20we%20show%20that%20using%20auxiliary%20KCs%0Aenhances%20both%20reinforcement%20learning-based%20policies%20and%20a%20simple%20planning-based%0Amethod%20%28expectimax%29%2C%20resulting%20in%20measurable%20gains%20in%20student%20learning%20outcomes%0Awithin%20a%20simulated%20student%20environment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16269v1&entry.124074799=Read"},
{"title": "Quality control in sublinear time: a case study via random graphs", "author": "Cassandra Marcussen and Ronitt Rubinfeld and Madhu Sudan", "abstract": "  Many algorithms are designed to work well on average over inputs. When\nrunning such an algorithm on an arbitrary input, we must ask: Can we trust the\nalgorithm on this input? We identify a new class of algorithmic problems\naddressing this, which we call \"Quality Control Problems.\" These problems are\nspecified by a (positive, real-valued) \"quality function\" $\\rho$ and a\ndistribution $D$ such that, with high probability, a sample drawn from $D$ is\n\"high quality,\" meaning its $\\rho$-value is near $1$. The goal is to accept\ninputs $x \\sim D$ and reject potentially adversarially generated inputs $x$\nwith $\\rho(x)$ far from $1$. The objective of quality control is thus weaker\nthan either component problem: testing for \"$\\rho(x) \\approx 1$\" or testing if\n$x \\sim D$, and offers the possibility of more efficient algorithms.\n  In this work, we consider the sublinear version of the quality control\nproblem, where $D \\in \\Delta(\\{0,1\\}^N)$ and the goal is to solve the $(D\n,\\rho)$-quality problem with $o(N)$ queries and time. As a case study, we\nconsider random graphs, i.e., $D = G_{n,p}$ (and $N = \\binom{n}2$), and the\n$k$-clique count function $\\rho_k := C_k(G)/\\mathbb{E}_{G' \\sim\nG_{n,p}}[C_k(G')]$, where $C_k(G)$ is the number of $k$-cliques in $G$. Testing\nif $G \\sim G_{n,p}$ with one sample, let alone with sublinear query access to\nthe sample, is of course impossible. Testing if $\\rho_k(G)\\approx 1$ requires\n$p^{-\\Omega(k^2)}$ samples. In contrast, we show that the quality control\nproblem for $G_{n,p}$ (with $n \\geq p^{-ck}$ for some constant $c$) with\nrespect to $\\rho_k$ can be tested with $p^{-O(k)}$ queries and time, showing\nquality control is provably superpolynomially more efficient in this setting.\nMore generally, for a motif $H$ of maximum degree $\\Delta(H)$, the respective\nquality control problem can be solved with $p^{-O(\\Delta(H))}$ queries and\nrunning time.\n", "link": "http://arxiv.org/abs/2508.16531v1", "date": "2025-08-22", "relevancy": 1.9776, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4077}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.3952}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.3837}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quality%20control%20in%20sublinear%20time%3A%20a%20case%20study%20via%20random%20graphs&body=Title%3A%20Quality%20control%20in%20sublinear%20time%3A%20a%20case%20study%20via%20random%20graphs%0AAuthor%3A%20Cassandra%20Marcussen%20and%20Ronitt%20Rubinfeld%20and%20Madhu%20Sudan%0AAbstract%3A%20%20%20Many%20algorithms%20are%20designed%20to%20work%20well%20on%20average%20over%20inputs.%20When%0Arunning%20such%20an%20algorithm%20on%20an%20arbitrary%20input%2C%20we%20must%20ask%3A%20Can%20we%20trust%20the%0Aalgorithm%20on%20this%20input%3F%20We%20identify%20a%20new%20class%20of%20algorithmic%20problems%0Aaddressing%20this%2C%20which%20we%20call%20%22Quality%20Control%20Problems.%22%20These%20problems%20are%0Aspecified%20by%20a%20%28positive%2C%20real-valued%29%20%22quality%20function%22%20%24%5Crho%24%20and%20a%0Adistribution%20%24D%24%20such%20that%2C%20with%20high%20probability%2C%20a%20sample%20drawn%20from%20%24D%24%20is%0A%22high%20quality%2C%22%20meaning%20its%20%24%5Crho%24-value%20is%20near%20%241%24.%20The%20goal%20is%20to%20accept%0Ainputs%20%24x%20%5Csim%20D%24%20and%20reject%20potentially%20adversarially%20generated%20inputs%20%24x%24%0Awith%20%24%5Crho%28x%29%24%20far%20from%20%241%24.%20The%20objective%20of%20quality%20control%20is%20thus%20weaker%0Athan%20either%20component%20problem%3A%20testing%20for%20%22%24%5Crho%28x%29%20%5Capprox%201%24%22%20or%20testing%20if%0A%24x%20%5Csim%20D%24%2C%20and%20offers%20the%20possibility%20of%20more%20efficient%20algorithms.%0A%20%20In%20this%20work%2C%20we%20consider%20the%20sublinear%20version%20of%20the%20quality%20control%0Aproblem%2C%20where%20%24D%20%5Cin%20%5CDelta%28%5C%7B0%2C1%5C%7D%5EN%29%24%20and%20the%20goal%20is%20to%20solve%20the%20%24%28D%0A%2C%5Crho%29%24-quality%20problem%20with%20%24o%28N%29%24%20queries%20and%20time.%20As%20a%20case%20study%2C%20we%0Aconsider%20random%20graphs%2C%20i.e.%2C%20%24D%20%3D%20G_%7Bn%2Cp%7D%24%20%28and%20%24N%20%3D%20%5Cbinom%7Bn%7D2%24%29%2C%20and%20the%0A%24k%24-clique%20count%20function%20%24%5Crho_k%20%3A%3D%20C_k%28G%29/%5Cmathbb%7BE%7D_%7BG%27%20%5Csim%0AG_%7Bn%2Cp%7D%7D%5BC_k%28G%27%29%5D%24%2C%20where%20%24C_k%28G%29%24%20is%20the%20number%20of%20%24k%24-cliques%20in%20%24G%24.%20Testing%0Aif%20%24G%20%5Csim%20G_%7Bn%2Cp%7D%24%20with%20one%20sample%2C%20let%20alone%20with%20sublinear%20query%20access%20to%0Athe%20sample%2C%20is%20of%20course%20impossible.%20Testing%20if%20%24%5Crho_k%28G%29%5Capprox%201%24%20requires%0A%24p%5E%7B-%5COmega%28k%5E2%29%7D%24%20samples.%20In%20contrast%2C%20we%20show%20that%20the%20quality%20control%0Aproblem%20for%20%24G_%7Bn%2Cp%7D%24%20%28with%20%24n%20%5Cgeq%20p%5E%7B-ck%7D%24%20for%20some%20constant%20%24c%24%29%20with%0Arespect%20to%20%24%5Crho_k%24%20can%20be%20tested%20with%20%24p%5E%7B-O%28k%29%7D%24%20queries%20and%20time%2C%20showing%0Aquality%20control%20is%20provably%20superpolynomially%20more%20efficient%20in%20this%20setting.%0AMore%20generally%2C%20for%20a%20motif%20%24H%24%20of%20maximum%20degree%20%24%5CDelta%28H%29%24%2C%20the%20respective%0Aquality%20control%20problem%20can%20be%20solved%20with%20%24p%5E%7B-O%28%5CDelta%28H%29%29%7D%24%20queries%20and%0Arunning%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16531v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuality%2520control%2520in%2520sublinear%2520time%253A%2520a%2520case%2520study%2520via%2520random%2520graphs%26entry.906535625%3DCassandra%2520Marcussen%2520and%2520Ronitt%2520Rubinfeld%2520and%2520Madhu%2520Sudan%26entry.1292438233%3D%2520%2520Many%2520algorithms%2520are%2520designed%2520to%2520work%2520well%2520on%2520average%2520over%2520inputs.%2520When%250Arunning%2520such%2520an%2520algorithm%2520on%2520an%2520arbitrary%2520input%252C%2520we%2520must%2520ask%253A%2520Can%2520we%2520trust%2520the%250Aalgorithm%2520on%2520this%2520input%253F%2520We%2520identify%2520a%2520new%2520class%2520of%2520algorithmic%2520problems%250Aaddressing%2520this%252C%2520which%2520we%2520call%2520%2522Quality%2520Control%2520Problems.%2522%2520These%2520problems%2520are%250Aspecified%2520by%2520a%2520%2528positive%252C%2520real-valued%2529%2520%2522quality%2520function%2522%2520%2524%255Crho%2524%2520and%2520a%250Adistribution%2520%2524D%2524%2520such%2520that%252C%2520with%2520high%2520probability%252C%2520a%2520sample%2520drawn%2520from%2520%2524D%2524%2520is%250A%2522high%2520quality%252C%2522%2520meaning%2520its%2520%2524%255Crho%2524-value%2520is%2520near%2520%25241%2524.%2520The%2520goal%2520is%2520to%2520accept%250Ainputs%2520%2524x%2520%255Csim%2520D%2524%2520and%2520reject%2520potentially%2520adversarially%2520generated%2520inputs%2520%2524x%2524%250Awith%2520%2524%255Crho%2528x%2529%2524%2520far%2520from%2520%25241%2524.%2520The%2520objective%2520of%2520quality%2520control%2520is%2520thus%2520weaker%250Athan%2520either%2520component%2520problem%253A%2520testing%2520for%2520%2522%2524%255Crho%2528x%2529%2520%255Capprox%25201%2524%2522%2520or%2520testing%2520if%250A%2524x%2520%255Csim%2520D%2524%252C%2520and%2520offers%2520the%2520possibility%2520of%2520more%2520efficient%2520algorithms.%250A%2520%2520In%2520this%2520work%252C%2520we%2520consider%2520the%2520sublinear%2520version%2520of%2520the%2520quality%2520control%250Aproblem%252C%2520where%2520%2524D%2520%255Cin%2520%255CDelta%2528%255C%257B0%252C1%255C%257D%255EN%2529%2524%2520and%2520the%2520goal%2520is%2520to%2520solve%2520the%2520%2524%2528D%250A%252C%255Crho%2529%2524-quality%2520problem%2520with%2520%2524o%2528N%2529%2524%2520queries%2520and%2520time.%2520As%2520a%2520case%2520study%252C%2520we%250Aconsider%2520random%2520graphs%252C%2520i.e.%252C%2520%2524D%2520%253D%2520G_%257Bn%252Cp%257D%2524%2520%2528and%2520%2524N%2520%253D%2520%255Cbinom%257Bn%257D2%2524%2529%252C%2520and%2520the%250A%2524k%2524-clique%2520count%2520function%2520%2524%255Crho_k%2520%253A%253D%2520C_k%2528G%2529/%255Cmathbb%257BE%257D_%257BG%2527%2520%255Csim%250AG_%257Bn%252Cp%257D%257D%255BC_k%2528G%2527%2529%255D%2524%252C%2520where%2520%2524C_k%2528G%2529%2524%2520is%2520the%2520number%2520of%2520%2524k%2524-cliques%2520in%2520%2524G%2524.%2520Testing%250Aif%2520%2524G%2520%255Csim%2520G_%257Bn%252Cp%257D%2524%2520with%2520one%2520sample%252C%2520let%2520alone%2520with%2520sublinear%2520query%2520access%2520to%250Athe%2520sample%252C%2520is%2520of%2520course%2520impossible.%2520Testing%2520if%2520%2524%255Crho_k%2528G%2529%255Capprox%25201%2524%2520requires%250A%2524p%255E%257B-%255COmega%2528k%255E2%2529%257D%2524%2520samples.%2520In%2520contrast%252C%2520we%2520show%2520that%2520the%2520quality%2520control%250Aproblem%2520for%2520%2524G_%257Bn%252Cp%257D%2524%2520%2528with%2520%2524n%2520%255Cgeq%2520p%255E%257B-ck%257D%2524%2520for%2520some%2520constant%2520%2524c%2524%2529%2520with%250Arespect%2520to%2520%2524%255Crho_k%2524%2520can%2520be%2520tested%2520with%2520%2524p%255E%257B-O%2528k%2529%257D%2524%2520queries%2520and%2520time%252C%2520showing%250Aquality%2520control%2520is%2520provably%2520superpolynomially%2520more%2520efficient%2520in%2520this%2520setting.%250AMore%2520generally%252C%2520for%2520a%2520motif%2520%2524H%2524%2520of%2520maximum%2520degree%2520%2524%255CDelta%2528H%2529%2524%252C%2520the%2520respective%250Aquality%2520control%2520problem%2520can%2520be%2520solved%2520with%2520%2524p%255E%257B-O%2528%255CDelta%2528H%2529%2529%257D%2524%2520queries%2520and%250Arunning%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16531v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quality%20control%20in%20sublinear%20time%3A%20a%20case%20study%20via%20random%20graphs&entry.906535625=Cassandra%20Marcussen%20and%20Ronitt%20Rubinfeld%20and%20Madhu%20Sudan&entry.1292438233=%20%20Many%20algorithms%20are%20designed%20to%20work%20well%20on%20average%20over%20inputs.%20When%0Arunning%20such%20an%20algorithm%20on%20an%20arbitrary%20input%2C%20we%20must%20ask%3A%20Can%20we%20trust%20the%0Aalgorithm%20on%20this%20input%3F%20We%20identify%20a%20new%20class%20of%20algorithmic%20problems%0Aaddressing%20this%2C%20which%20we%20call%20%22Quality%20Control%20Problems.%22%20These%20problems%20are%0Aspecified%20by%20a%20%28positive%2C%20real-valued%29%20%22quality%20function%22%20%24%5Crho%24%20and%20a%0Adistribution%20%24D%24%20such%20that%2C%20with%20high%20probability%2C%20a%20sample%20drawn%20from%20%24D%24%20is%0A%22high%20quality%2C%22%20meaning%20its%20%24%5Crho%24-value%20is%20near%20%241%24.%20The%20goal%20is%20to%20accept%0Ainputs%20%24x%20%5Csim%20D%24%20and%20reject%20potentially%20adversarially%20generated%20inputs%20%24x%24%0Awith%20%24%5Crho%28x%29%24%20far%20from%20%241%24.%20The%20objective%20of%20quality%20control%20is%20thus%20weaker%0Athan%20either%20component%20problem%3A%20testing%20for%20%22%24%5Crho%28x%29%20%5Capprox%201%24%22%20or%20testing%20if%0A%24x%20%5Csim%20D%24%2C%20and%20offers%20the%20possibility%20of%20more%20efficient%20algorithms.%0A%20%20In%20this%20work%2C%20we%20consider%20the%20sublinear%20version%20of%20the%20quality%20control%0Aproblem%2C%20where%20%24D%20%5Cin%20%5CDelta%28%5C%7B0%2C1%5C%7D%5EN%29%24%20and%20the%20goal%20is%20to%20solve%20the%20%24%28D%0A%2C%5Crho%29%24-quality%20problem%20with%20%24o%28N%29%24%20queries%20and%20time.%20As%20a%20case%20study%2C%20we%0Aconsider%20random%20graphs%2C%20i.e.%2C%20%24D%20%3D%20G_%7Bn%2Cp%7D%24%20%28and%20%24N%20%3D%20%5Cbinom%7Bn%7D2%24%29%2C%20and%20the%0A%24k%24-clique%20count%20function%20%24%5Crho_k%20%3A%3D%20C_k%28G%29/%5Cmathbb%7BE%7D_%7BG%27%20%5Csim%0AG_%7Bn%2Cp%7D%7D%5BC_k%28G%27%29%5D%24%2C%20where%20%24C_k%28G%29%24%20is%20the%20number%20of%20%24k%24-cliques%20in%20%24G%24.%20Testing%0Aif%20%24G%20%5Csim%20G_%7Bn%2Cp%7D%24%20with%20one%20sample%2C%20let%20alone%20with%20sublinear%20query%20access%20to%0Athe%20sample%2C%20is%20of%20course%20impossible.%20Testing%20if%20%24%5Crho_k%28G%29%5Capprox%201%24%20requires%0A%24p%5E%7B-%5COmega%28k%5E2%29%7D%24%20samples.%20In%20contrast%2C%20we%20show%20that%20the%20quality%20control%0Aproblem%20for%20%24G_%7Bn%2Cp%7D%24%20%28with%20%24n%20%5Cgeq%20p%5E%7B-ck%7D%24%20for%20some%20constant%20%24c%24%29%20with%0Arespect%20to%20%24%5Crho_k%24%20can%20be%20tested%20with%20%24p%5E%7B-O%28k%29%7D%24%20queries%20and%20time%2C%20showing%0Aquality%20control%20is%20provably%20superpolynomially%20more%20efficient%20in%20this%20setting.%0AMore%20generally%2C%20for%20a%20motif%20%24H%24%20of%20maximum%20degree%20%24%5CDelta%28H%29%24%2C%20the%20respective%0Aquality%20control%20problem%20can%20be%20solved%20with%20%24p%5E%7B-O%28%5CDelta%28H%29%29%7D%24%20queries%20and%0Arunning%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16531v1&entry.124074799=Read"},
{"title": "GLARE: Agentic Reasoning for Legal Judgment Prediction", "author": "Xinyu Yang and Chenlong Deng and Zhicheng Dou", "abstract": "  Legal judgment prediction (LJP) has become increasingly important in the\nlegal field. In this paper, we identify that existing large language models\n(LLMs) have significant problems of insufficient reasoning due to a lack of\nlegal knowledge. Therefore, we introduce GLARE, an agentic legal reasoning\nframework that dynamically acquires key legal knowledge by invoking different\nmodules, thereby improving the breadth and depth of reasoning. Experiments\nconducted on the real-world dataset verify the effectiveness of our method.\nFurthermore, the reasoning chain generated during the analysis process can\nincrease interpretability and provide the possibility for practical\napplications.\n", "link": "http://arxiv.org/abs/2508.16383v1", "date": "2025-08-22", "relevancy": 1.973, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5089}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.487}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4801}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GLARE%3A%20Agentic%20Reasoning%20for%20Legal%20Judgment%20Prediction&body=Title%3A%20GLARE%3A%20Agentic%20Reasoning%20for%20Legal%20Judgment%20Prediction%0AAuthor%3A%20Xinyu%20Yang%20and%20Chenlong%20Deng%20and%20Zhicheng%20Dou%0AAbstract%3A%20%20%20Legal%20judgment%20prediction%20%28LJP%29%20has%20become%20increasingly%20important%20in%20the%0Alegal%20field.%20In%20this%20paper%2C%20we%20identify%20that%20existing%20large%20language%20models%0A%28LLMs%29%20have%20significant%20problems%20of%20insufficient%20reasoning%20due%20to%20a%20lack%20of%0Alegal%20knowledge.%20Therefore%2C%20we%20introduce%20GLARE%2C%20an%20agentic%20legal%20reasoning%0Aframework%20that%20dynamically%20acquires%20key%20legal%20knowledge%20by%20invoking%20different%0Amodules%2C%20thereby%20improving%20the%20breadth%20and%20depth%20of%20reasoning.%20Experiments%0Aconducted%20on%20the%20real-world%20dataset%20verify%20the%20effectiveness%20of%20our%20method.%0AFurthermore%2C%20the%20reasoning%20chain%20generated%20during%20the%20analysis%20process%20can%0Aincrease%20interpretability%20and%20provide%20the%20possibility%20for%20practical%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16383v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGLARE%253A%2520Agentic%2520Reasoning%2520for%2520Legal%2520Judgment%2520Prediction%26entry.906535625%3DXinyu%2520Yang%2520and%2520Chenlong%2520Deng%2520and%2520Zhicheng%2520Dou%26entry.1292438233%3D%2520%2520Legal%2520judgment%2520prediction%2520%2528LJP%2529%2520has%2520become%2520increasingly%2520important%2520in%2520the%250Alegal%2520field.%2520In%2520this%2520paper%252C%2520we%2520identify%2520that%2520existing%2520large%2520language%2520models%250A%2528LLMs%2529%2520have%2520significant%2520problems%2520of%2520insufficient%2520reasoning%2520due%2520to%2520a%2520lack%2520of%250Alegal%2520knowledge.%2520Therefore%252C%2520we%2520introduce%2520GLARE%252C%2520an%2520agentic%2520legal%2520reasoning%250Aframework%2520that%2520dynamically%2520acquires%2520key%2520legal%2520knowledge%2520by%2520invoking%2520different%250Amodules%252C%2520thereby%2520improving%2520the%2520breadth%2520and%2520depth%2520of%2520reasoning.%2520Experiments%250Aconducted%2520on%2520the%2520real-world%2520dataset%2520verify%2520the%2520effectiveness%2520of%2520our%2520method.%250AFurthermore%252C%2520the%2520reasoning%2520chain%2520generated%2520during%2520the%2520analysis%2520process%2520can%250Aincrease%2520interpretability%2520and%2520provide%2520the%2520possibility%2520for%2520practical%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16383v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLARE%3A%20Agentic%20Reasoning%20for%20Legal%20Judgment%20Prediction&entry.906535625=Xinyu%20Yang%20and%20Chenlong%20Deng%20and%20Zhicheng%20Dou&entry.1292438233=%20%20Legal%20judgment%20prediction%20%28LJP%29%20has%20become%20increasingly%20important%20in%20the%0Alegal%20field.%20In%20this%20paper%2C%20we%20identify%20that%20existing%20large%20language%20models%0A%28LLMs%29%20have%20significant%20problems%20of%20insufficient%20reasoning%20due%20to%20a%20lack%20of%0Alegal%20knowledge.%20Therefore%2C%20we%20introduce%20GLARE%2C%20an%20agentic%20legal%20reasoning%0Aframework%20that%20dynamically%20acquires%20key%20legal%20knowledge%20by%20invoking%20different%0Amodules%2C%20thereby%20improving%20the%20breadth%20and%20depth%20of%20reasoning.%20Experiments%0Aconducted%20on%20the%20real-world%20dataset%20verify%20the%20effectiveness%20of%20our%20method.%0AFurthermore%2C%20the%20reasoning%20chain%20generated%20during%20the%20analysis%20process%20can%0Aincrease%20interpretability%20and%20provide%20the%20possibility%20for%20practical%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16383v1&entry.124074799=Read"},
{"title": "PediatricsMQA: a Multi-modal Pediatrics Question Answering Benchmark", "author": "Adil Bahaj and Mounir Ghogho", "abstract": "  Large language models (LLMs) and vision-augmented LLMs (VLMs) have\nsignificantly advanced medical informatics, diagnostics, and decision support.\nHowever, these models exhibit systematic biases, particularly age bias,\ncompromising their reliability and equity. This is evident in their poorer\nperformance on pediatric-focused text and visual question-answering tasks. This\nbias reflects a broader imbalance in medical research, where pediatric studies\nreceive less funding and representation despite the significant disease burden\nin children. To address these issues, a new comprehensive multi-modal pediatric\nquestion-answering benchmark, PediatricsMQA, has been introduced. It consists\nof 3,417 text-based multiple-choice questions (MCQs) covering 131 pediatric\ntopics across seven developmental stages (prenatal to adolescent) and 2,067\nvision-based MCQs using 634 pediatric images from 67 imaging modalities and 256\nanatomical regions. The dataset was developed using a hybrid manual-automatic\npipeline, incorporating peer-reviewed pediatric literature, validated question\nbanks, existing benchmarks, and existing QA resources. Evaluating\nstate-of-the-art open models, we find dramatic performance drops in younger\ncohorts, highlighting the need for age-aware methods to ensure equitable AI\nsupport in pediatric care.\n", "link": "http://arxiv.org/abs/2508.16439v1", "date": "2025-08-22", "relevancy": 1.9679, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5021}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4899}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PediatricsMQA%3A%20a%20Multi-modal%20Pediatrics%20Question%20Answering%20Benchmark&body=Title%3A%20PediatricsMQA%3A%20a%20Multi-modal%20Pediatrics%20Question%20Answering%20Benchmark%0AAuthor%3A%20Adil%20Bahaj%20and%20Mounir%20Ghogho%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20and%20vision-augmented%20LLMs%20%28VLMs%29%20have%0Asignificantly%20advanced%20medical%20informatics%2C%20diagnostics%2C%20and%20decision%20support.%0AHowever%2C%20these%20models%20exhibit%20systematic%20biases%2C%20particularly%20age%20bias%2C%0Acompromising%20their%20reliability%20and%20equity.%20This%20is%20evident%20in%20their%20poorer%0Aperformance%20on%20pediatric-focused%20text%20and%20visual%20question-answering%20tasks.%20This%0Abias%20reflects%20a%20broader%20imbalance%20in%20medical%20research%2C%20where%20pediatric%20studies%0Areceive%20less%20funding%20and%20representation%20despite%20the%20significant%20disease%20burden%0Ain%20children.%20To%20address%20these%20issues%2C%20a%20new%20comprehensive%20multi-modal%20pediatric%0Aquestion-answering%20benchmark%2C%20PediatricsMQA%2C%20has%20been%20introduced.%20It%20consists%0Aof%203%2C417%20text-based%20multiple-choice%20questions%20%28MCQs%29%20covering%20131%20pediatric%0Atopics%20across%20seven%20developmental%20stages%20%28prenatal%20to%20adolescent%29%20and%202%2C067%0Avision-based%20MCQs%20using%20634%20pediatric%20images%20from%2067%20imaging%20modalities%20and%20256%0Aanatomical%20regions.%20The%20dataset%20was%20developed%20using%20a%20hybrid%20manual-automatic%0Apipeline%2C%20incorporating%20peer-reviewed%20pediatric%20literature%2C%20validated%20question%0Abanks%2C%20existing%20benchmarks%2C%20and%20existing%20QA%20resources.%20Evaluating%0Astate-of-the-art%20open%20models%2C%20we%20find%20dramatic%20performance%20drops%20in%20younger%0Acohorts%2C%20highlighting%20the%20need%20for%20age-aware%20methods%20to%20ensure%20equitable%20AI%0Asupport%20in%20pediatric%20care.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16439v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPediatricsMQA%253A%2520a%2520Multi-modal%2520Pediatrics%2520Question%2520Answering%2520Benchmark%26entry.906535625%3DAdil%2520Bahaj%2520and%2520Mounir%2520Ghogho%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520and%2520vision-augmented%2520LLMs%2520%2528VLMs%2529%2520have%250Asignificantly%2520advanced%2520medical%2520informatics%252C%2520diagnostics%252C%2520and%2520decision%2520support.%250AHowever%252C%2520these%2520models%2520exhibit%2520systematic%2520biases%252C%2520particularly%2520age%2520bias%252C%250Acompromising%2520their%2520reliability%2520and%2520equity.%2520This%2520is%2520evident%2520in%2520their%2520poorer%250Aperformance%2520on%2520pediatric-focused%2520text%2520and%2520visual%2520question-answering%2520tasks.%2520This%250Abias%2520reflects%2520a%2520broader%2520imbalance%2520in%2520medical%2520research%252C%2520where%2520pediatric%2520studies%250Areceive%2520less%2520funding%2520and%2520representation%2520despite%2520the%2520significant%2520disease%2520burden%250Ain%2520children.%2520To%2520address%2520these%2520issues%252C%2520a%2520new%2520comprehensive%2520multi-modal%2520pediatric%250Aquestion-answering%2520benchmark%252C%2520PediatricsMQA%252C%2520has%2520been%2520introduced.%2520It%2520consists%250Aof%25203%252C417%2520text-based%2520multiple-choice%2520questions%2520%2528MCQs%2529%2520covering%2520131%2520pediatric%250Atopics%2520across%2520seven%2520developmental%2520stages%2520%2528prenatal%2520to%2520adolescent%2529%2520and%25202%252C067%250Avision-based%2520MCQs%2520using%2520634%2520pediatric%2520images%2520from%252067%2520imaging%2520modalities%2520and%2520256%250Aanatomical%2520regions.%2520The%2520dataset%2520was%2520developed%2520using%2520a%2520hybrid%2520manual-automatic%250Apipeline%252C%2520incorporating%2520peer-reviewed%2520pediatric%2520literature%252C%2520validated%2520question%250Abanks%252C%2520existing%2520benchmarks%252C%2520and%2520existing%2520QA%2520resources.%2520Evaluating%250Astate-of-the-art%2520open%2520models%252C%2520we%2520find%2520dramatic%2520performance%2520drops%2520in%2520younger%250Acohorts%252C%2520highlighting%2520the%2520need%2520for%2520age-aware%2520methods%2520to%2520ensure%2520equitable%2520AI%250Asupport%2520in%2520pediatric%2520care.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16439v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PediatricsMQA%3A%20a%20Multi-modal%20Pediatrics%20Question%20Answering%20Benchmark&entry.906535625=Adil%20Bahaj%20and%20Mounir%20Ghogho&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20and%20vision-augmented%20LLMs%20%28VLMs%29%20have%0Asignificantly%20advanced%20medical%20informatics%2C%20diagnostics%2C%20and%20decision%20support.%0AHowever%2C%20these%20models%20exhibit%20systematic%20biases%2C%20particularly%20age%20bias%2C%0Acompromising%20their%20reliability%20and%20equity.%20This%20is%20evident%20in%20their%20poorer%0Aperformance%20on%20pediatric-focused%20text%20and%20visual%20question-answering%20tasks.%20This%0Abias%20reflects%20a%20broader%20imbalance%20in%20medical%20research%2C%20where%20pediatric%20studies%0Areceive%20less%20funding%20and%20representation%20despite%20the%20significant%20disease%20burden%0Ain%20children.%20To%20address%20these%20issues%2C%20a%20new%20comprehensive%20multi-modal%20pediatric%0Aquestion-answering%20benchmark%2C%20PediatricsMQA%2C%20has%20been%20introduced.%20It%20consists%0Aof%203%2C417%20text-based%20multiple-choice%20questions%20%28MCQs%29%20covering%20131%20pediatric%0Atopics%20across%20seven%20developmental%20stages%20%28prenatal%20to%20adolescent%29%20and%202%2C067%0Avision-based%20MCQs%20using%20634%20pediatric%20images%20from%2067%20imaging%20modalities%20and%20256%0Aanatomical%20regions.%20The%20dataset%20was%20developed%20using%20a%20hybrid%20manual-automatic%0Apipeline%2C%20incorporating%20peer-reviewed%20pediatric%20literature%2C%20validated%20question%0Abanks%2C%20existing%20benchmarks%2C%20and%20existing%20QA%20resources.%20Evaluating%0Astate-of-the-art%20open%20models%2C%20we%20find%20dramatic%20performance%20drops%20in%20younger%0Acohorts%2C%20highlighting%20the%20need%20for%20age-aware%20methods%20to%20ensure%20equitable%20AI%0Asupport%20in%20pediatric%20care.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16439v1&entry.124074799=Read"},
{"title": "Your Reward Function for RL is Your Best PRM for Search: Unifying RL and\n  Search-Based TTS", "author": "Can Jin and Yang Zhou and Qixin Zhang and Hongwu Peng and Di Zhang and Marco Pavone and Ligong Han and Zhang-Wei Hong and Tong Che and Dimitris N. Metaxas", "abstract": "  Test-time scaling (TTS) for large language models (LLMs) has thus far fallen\ninto two largely separate paradigms: (1) reinforcement learning (RL) methods\nthat optimize sparse outcome-based rewards, yet suffer from instability and low\nsample efficiency; and (2) search-based techniques guided by independently\ntrained, static process reward models (PRMs), which require expensive human- or\nLLM-generated labels and often degrade under distribution shifts. In this\npaper, we introduce AIRL-S, the first natural unification of RL-based and\nsearch-based TTS. Central to AIRL-S is the insight that the reward function\nlearned during RL training inherently represents the ideal PRM for guiding\ndownstream search. Specifically, we leverage adversarial inverse reinforcement\nlearning (AIRL) combined with group relative policy optimization (GRPO) to\nlearn a dense, dynamic PRM directly from correct reasoning traces, entirely\neliminating the need for labeled intermediate process data. At inference, the\nresulting PRM simultaneously serves as the critic for RL rollouts and as a\nheuristic to effectively guide search procedures, facilitating robust reasoning\nchain extension, mitigating reward hacking, and enhancing cross-task\ngeneralization. Experimental results across eight benchmarks, including\nmathematics, scientific reasoning, and code generation, demonstrate that our\nunified approach improves performance by 9 % on average over the base model,\nmatching GPT-4o. Furthermore, when integrated into multiple search algorithms,\nour PRM consistently outperforms all baseline PRMs trained with labeled data.\nThese results underscore that, indeed, your reward function for RL is your best\nPRM for search, providing a robust and cost-effective solution to complex\nreasoning tasks in LLMs.\n", "link": "http://arxiv.org/abs/2508.14313v2", "date": "2025-08-22", "relevancy": 1.9653, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.504}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4888}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Your%20Reward%20Function%20for%20RL%20is%20Your%20Best%20PRM%20for%20Search%3A%20Unifying%20RL%20and%0A%20%20Search-Based%20TTS&body=Title%3A%20Your%20Reward%20Function%20for%20RL%20is%20Your%20Best%20PRM%20for%20Search%3A%20Unifying%20RL%20and%0A%20%20Search-Based%20TTS%0AAuthor%3A%20Can%20Jin%20and%20Yang%20Zhou%20and%20Qixin%20Zhang%20and%20Hongwu%20Peng%20and%20Di%20Zhang%20and%20Marco%20Pavone%20and%20Ligong%20Han%20and%20Zhang-Wei%20Hong%20and%20Tong%20Che%20and%20Dimitris%20N.%20Metaxas%0AAbstract%3A%20%20%20Test-time%20scaling%20%28TTS%29%20for%20large%20language%20models%20%28LLMs%29%20has%20thus%20far%20fallen%0Ainto%20two%20largely%20separate%20paradigms%3A%20%281%29%20reinforcement%20learning%20%28RL%29%20methods%0Athat%20optimize%20sparse%20outcome-based%20rewards%2C%20yet%20suffer%20from%20instability%20and%20low%0Asample%20efficiency%3B%20and%20%282%29%20search-based%20techniques%20guided%20by%20independently%0Atrained%2C%20static%20process%20reward%20models%20%28PRMs%29%2C%20which%20require%20expensive%20human-%20or%0ALLM-generated%20labels%20and%20often%20degrade%20under%20distribution%20shifts.%20In%20this%0Apaper%2C%20we%20introduce%20AIRL-S%2C%20the%20first%20natural%20unification%20of%20RL-based%20and%0Asearch-based%20TTS.%20Central%20to%20AIRL-S%20is%20the%20insight%20that%20the%20reward%20function%0Alearned%20during%20RL%20training%20inherently%20represents%20the%20ideal%20PRM%20for%20guiding%0Adownstream%20search.%20Specifically%2C%20we%20leverage%20adversarial%20inverse%20reinforcement%0Alearning%20%28AIRL%29%20combined%20with%20group%20relative%20policy%20optimization%20%28GRPO%29%20to%0Alearn%20a%20dense%2C%20dynamic%20PRM%20directly%20from%20correct%20reasoning%20traces%2C%20entirely%0Aeliminating%20the%20need%20for%20labeled%20intermediate%20process%20data.%20At%20inference%2C%20the%0Aresulting%20PRM%20simultaneously%20serves%20as%20the%20critic%20for%20RL%20rollouts%20and%20as%20a%0Aheuristic%20to%20effectively%20guide%20search%20procedures%2C%20facilitating%20robust%20reasoning%0Achain%20extension%2C%20mitigating%20reward%20hacking%2C%20and%20enhancing%20cross-task%0Ageneralization.%20Experimental%20results%20across%20eight%20benchmarks%2C%20including%0Amathematics%2C%20scientific%20reasoning%2C%20and%20code%20generation%2C%20demonstrate%20that%20our%0Aunified%20approach%20improves%20performance%20by%209%20%25%20on%20average%20over%20the%20base%20model%2C%0Amatching%20GPT-4o.%20Furthermore%2C%20when%20integrated%20into%20multiple%20search%20algorithms%2C%0Aour%20PRM%20consistently%20outperforms%20all%20baseline%20PRMs%20trained%20with%20labeled%20data.%0AThese%20results%20underscore%20that%2C%20indeed%2C%20your%20reward%20function%20for%20RL%20is%20your%20best%0APRM%20for%20search%2C%20providing%20a%20robust%20and%20cost-effective%20solution%20to%20complex%0Areasoning%20tasks%20in%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14313v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYour%2520Reward%2520Function%2520for%2520RL%2520is%2520Your%2520Best%2520PRM%2520for%2520Search%253A%2520Unifying%2520RL%2520and%250A%2520%2520Search-Based%2520TTS%26entry.906535625%3DCan%2520Jin%2520and%2520Yang%2520Zhou%2520and%2520Qixin%2520Zhang%2520and%2520Hongwu%2520Peng%2520and%2520Di%2520Zhang%2520and%2520Marco%2520Pavone%2520and%2520Ligong%2520Han%2520and%2520Zhang-Wei%2520Hong%2520and%2520Tong%2520Che%2520and%2520Dimitris%2520N.%2520Metaxas%26entry.1292438233%3D%2520%2520Test-time%2520scaling%2520%2528TTS%2529%2520for%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520thus%2520far%2520fallen%250Ainto%2520two%2520largely%2520separate%2520paradigms%253A%2520%25281%2529%2520reinforcement%2520learning%2520%2528RL%2529%2520methods%250Athat%2520optimize%2520sparse%2520outcome-based%2520rewards%252C%2520yet%2520suffer%2520from%2520instability%2520and%2520low%250Asample%2520efficiency%253B%2520and%2520%25282%2529%2520search-based%2520techniques%2520guided%2520by%2520independently%250Atrained%252C%2520static%2520process%2520reward%2520models%2520%2528PRMs%2529%252C%2520which%2520require%2520expensive%2520human-%2520or%250ALLM-generated%2520labels%2520and%2520often%2520degrade%2520under%2520distribution%2520shifts.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520AIRL-S%252C%2520the%2520first%2520natural%2520unification%2520of%2520RL-based%2520and%250Asearch-based%2520TTS.%2520Central%2520to%2520AIRL-S%2520is%2520the%2520insight%2520that%2520the%2520reward%2520function%250Alearned%2520during%2520RL%2520training%2520inherently%2520represents%2520the%2520ideal%2520PRM%2520for%2520guiding%250Adownstream%2520search.%2520Specifically%252C%2520we%2520leverage%2520adversarial%2520inverse%2520reinforcement%250Alearning%2520%2528AIRL%2529%2520combined%2520with%2520group%2520relative%2520policy%2520optimization%2520%2528GRPO%2529%2520to%250Alearn%2520a%2520dense%252C%2520dynamic%2520PRM%2520directly%2520from%2520correct%2520reasoning%2520traces%252C%2520entirely%250Aeliminating%2520the%2520need%2520for%2520labeled%2520intermediate%2520process%2520data.%2520At%2520inference%252C%2520the%250Aresulting%2520PRM%2520simultaneously%2520serves%2520as%2520the%2520critic%2520for%2520RL%2520rollouts%2520and%2520as%2520a%250Aheuristic%2520to%2520effectively%2520guide%2520search%2520procedures%252C%2520facilitating%2520robust%2520reasoning%250Achain%2520extension%252C%2520mitigating%2520reward%2520hacking%252C%2520and%2520enhancing%2520cross-task%250Ageneralization.%2520Experimental%2520results%2520across%2520eight%2520benchmarks%252C%2520including%250Amathematics%252C%2520scientific%2520reasoning%252C%2520and%2520code%2520generation%252C%2520demonstrate%2520that%2520our%250Aunified%2520approach%2520improves%2520performance%2520by%25209%2520%2525%2520on%2520average%2520over%2520the%2520base%2520model%252C%250Amatching%2520GPT-4o.%2520Furthermore%252C%2520when%2520integrated%2520into%2520multiple%2520search%2520algorithms%252C%250Aour%2520PRM%2520consistently%2520outperforms%2520all%2520baseline%2520PRMs%2520trained%2520with%2520labeled%2520data.%250AThese%2520results%2520underscore%2520that%252C%2520indeed%252C%2520your%2520reward%2520function%2520for%2520RL%2520is%2520your%2520best%250APRM%2520for%2520search%252C%2520providing%2520a%2520robust%2520and%2520cost-effective%2520solution%2520to%2520complex%250Areasoning%2520tasks%2520in%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14313v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Your%20Reward%20Function%20for%20RL%20is%20Your%20Best%20PRM%20for%20Search%3A%20Unifying%20RL%20and%0A%20%20Search-Based%20TTS&entry.906535625=Can%20Jin%20and%20Yang%20Zhou%20and%20Qixin%20Zhang%20and%20Hongwu%20Peng%20and%20Di%20Zhang%20and%20Marco%20Pavone%20and%20Ligong%20Han%20and%20Zhang-Wei%20Hong%20and%20Tong%20Che%20and%20Dimitris%20N.%20Metaxas&entry.1292438233=%20%20Test-time%20scaling%20%28TTS%29%20for%20large%20language%20models%20%28LLMs%29%20has%20thus%20far%20fallen%0Ainto%20two%20largely%20separate%20paradigms%3A%20%281%29%20reinforcement%20learning%20%28RL%29%20methods%0Athat%20optimize%20sparse%20outcome-based%20rewards%2C%20yet%20suffer%20from%20instability%20and%20low%0Asample%20efficiency%3B%20and%20%282%29%20search-based%20techniques%20guided%20by%20independently%0Atrained%2C%20static%20process%20reward%20models%20%28PRMs%29%2C%20which%20require%20expensive%20human-%20or%0ALLM-generated%20labels%20and%20often%20degrade%20under%20distribution%20shifts.%20In%20this%0Apaper%2C%20we%20introduce%20AIRL-S%2C%20the%20first%20natural%20unification%20of%20RL-based%20and%0Asearch-based%20TTS.%20Central%20to%20AIRL-S%20is%20the%20insight%20that%20the%20reward%20function%0Alearned%20during%20RL%20training%20inherently%20represents%20the%20ideal%20PRM%20for%20guiding%0Adownstream%20search.%20Specifically%2C%20we%20leverage%20adversarial%20inverse%20reinforcement%0Alearning%20%28AIRL%29%20combined%20with%20group%20relative%20policy%20optimization%20%28GRPO%29%20to%0Alearn%20a%20dense%2C%20dynamic%20PRM%20directly%20from%20correct%20reasoning%20traces%2C%20entirely%0Aeliminating%20the%20need%20for%20labeled%20intermediate%20process%20data.%20At%20inference%2C%20the%0Aresulting%20PRM%20simultaneously%20serves%20as%20the%20critic%20for%20RL%20rollouts%20and%20as%20a%0Aheuristic%20to%20effectively%20guide%20search%20procedures%2C%20facilitating%20robust%20reasoning%0Achain%20extension%2C%20mitigating%20reward%20hacking%2C%20and%20enhancing%20cross-task%0Ageneralization.%20Experimental%20results%20across%20eight%20benchmarks%2C%20including%0Amathematics%2C%20scientific%20reasoning%2C%20and%20code%20generation%2C%20demonstrate%20that%20our%0Aunified%20approach%20improves%20performance%20by%209%20%25%20on%20average%20over%20the%20base%20model%2C%0Amatching%20GPT-4o.%20Furthermore%2C%20when%20integrated%20into%20multiple%20search%20algorithms%2C%0Aour%20PRM%20consistently%20outperforms%20all%20baseline%20PRMs%20trained%20with%20labeled%20data.%0AThese%20results%20underscore%20that%2C%20indeed%2C%20your%20reward%20function%20for%20RL%20is%20your%20best%0APRM%20for%20search%2C%20providing%20a%20robust%20and%20cost-effective%20solution%20to%20complex%0Areasoning%20tasks%20in%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14313v2&entry.124074799=Read"},
{"title": "Unsupervised Online Detection of Pipe Blockages and Leakages in Water\n  Distribution Networks", "author": "Jin Li and Kleanthis Malialis and Stelios G. Vrachimis and Marios M. Polycarpou", "abstract": "  Water Distribution Networks (WDNs), critical to public well-being and\neconomic stability, face challenges such as pipe blockages and background\nleakages, exacerbated by operational constraints such as data non-stationarity\nand limited labeled data. This paper proposes an unsupervised, online learning\nframework that aims to detect two types of faults in WDNs: pipe blockages,\nmodeled as collective anomalies, and background leakages, modeled as concept\ndrift. Our approach combines a Long Short-Term Memory Variational Autoencoder\n(LSTM-VAE) with a dual drift detection mechanism, enabling robust detection and\nadaptation under non-stationary conditions. Its lightweight, memory-efficient\ndesign enables real-time, edge-level monitoring. Experiments on two realistic\nWDNs show that the proposed approach consistently outperforms strong baselines\nin detecting anomalies and adapting to recurrent drift, demonstrating its\neffectiveness in unsupervised event detection for dynamic WDN environments.\n", "link": "http://arxiv.org/abs/2508.16336v1", "date": "2025-08-22", "relevancy": 1.9641, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5035}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4936}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4834}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Online%20Detection%20of%20Pipe%20Blockages%20and%20Leakages%20in%20Water%0A%20%20Distribution%20Networks&body=Title%3A%20Unsupervised%20Online%20Detection%20of%20Pipe%20Blockages%20and%20Leakages%20in%20Water%0A%20%20Distribution%20Networks%0AAuthor%3A%20Jin%20Li%20and%20Kleanthis%20Malialis%20and%20Stelios%20G.%20Vrachimis%20and%20Marios%20M.%20Polycarpou%0AAbstract%3A%20%20%20Water%20Distribution%20Networks%20%28WDNs%29%2C%20critical%20to%20public%20well-being%20and%0Aeconomic%20stability%2C%20face%20challenges%20such%20as%20pipe%20blockages%20and%20background%0Aleakages%2C%20exacerbated%20by%20operational%20constraints%20such%20as%20data%20non-stationarity%0Aand%20limited%20labeled%20data.%20This%20paper%20proposes%20an%20unsupervised%2C%20online%20learning%0Aframework%20that%20aims%20to%20detect%20two%20types%20of%20faults%20in%20WDNs%3A%20pipe%20blockages%2C%0Amodeled%20as%20collective%20anomalies%2C%20and%20background%20leakages%2C%20modeled%20as%20concept%0Adrift.%20Our%20approach%20combines%20a%20Long%20Short-Term%20Memory%20Variational%20Autoencoder%0A%28LSTM-VAE%29%20with%20a%20dual%20drift%20detection%20mechanism%2C%20enabling%20robust%20detection%20and%0Aadaptation%20under%20non-stationary%20conditions.%20Its%20lightweight%2C%20memory-efficient%0Adesign%20enables%20real-time%2C%20edge-level%20monitoring.%20Experiments%20on%20two%20realistic%0AWDNs%20show%20that%20the%20proposed%20approach%20consistently%20outperforms%20strong%20baselines%0Ain%20detecting%20anomalies%20and%20adapting%20to%20recurrent%20drift%2C%20demonstrating%20its%0Aeffectiveness%20in%20unsupervised%20event%20detection%20for%20dynamic%20WDN%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16336v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Online%2520Detection%2520of%2520Pipe%2520Blockages%2520and%2520Leakages%2520in%2520Water%250A%2520%2520Distribution%2520Networks%26entry.906535625%3DJin%2520Li%2520and%2520Kleanthis%2520Malialis%2520and%2520Stelios%2520G.%2520Vrachimis%2520and%2520Marios%2520M.%2520Polycarpou%26entry.1292438233%3D%2520%2520Water%2520Distribution%2520Networks%2520%2528WDNs%2529%252C%2520critical%2520to%2520public%2520well-being%2520and%250Aeconomic%2520stability%252C%2520face%2520challenges%2520such%2520as%2520pipe%2520blockages%2520and%2520background%250Aleakages%252C%2520exacerbated%2520by%2520operational%2520constraints%2520such%2520as%2520data%2520non-stationarity%250Aand%2520limited%2520labeled%2520data.%2520This%2520paper%2520proposes%2520an%2520unsupervised%252C%2520online%2520learning%250Aframework%2520that%2520aims%2520to%2520detect%2520two%2520types%2520of%2520faults%2520in%2520WDNs%253A%2520pipe%2520blockages%252C%250Amodeled%2520as%2520collective%2520anomalies%252C%2520and%2520background%2520leakages%252C%2520modeled%2520as%2520concept%250Adrift.%2520Our%2520approach%2520combines%2520a%2520Long%2520Short-Term%2520Memory%2520Variational%2520Autoencoder%250A%2528LSTM-VAE%2529%2520with%2520a%2520dual%2520drift%2520detection%2520mechanism%252C%2520enabling%2520robust%2520detection%2520and%250Aadaptation%2520under%2520non-stationary%2520conditions.%2520Its%2520lightweight%252C%2520memory-efficient%250Adesign%2520enables%2520real-time%252C%2520edge-level%2520monitoring.%2520Experiments%2520on%2520two%2520realistic%250AWDNs%2520show%2520that%2520the%2520proposed%2520approach%2520consistently%2520outperforms%2520strong%2520baselines%250Ain%2520detecting%2520anomalies%2520and%2520adapting%2520to%2520recurrent%2520drift%252C%2520demonstrating%2520its%250Aeffectiveness%2520in%2520unsupervised%2520event%2520detection%2520for%2520dynamic%2520WDN%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16336v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Online%20Detection%20of%20Pipe%20Blockages%20and%20Leakages%20in%20Water%0A%20%20Distribution%20Networks&entry.906535625=Jin%20Li%20and%20Kleanthis%20Malialis%20and%20Stelios%20G.%20Vrachimis%20and%20Marios%20M.%20Polycarpou&entry.1292438233=%20%20Water%20Distribution%20Networks%20%28WDNs%29%2C%20critical%20to%20public%20well-being%20and%0Aeconomic%20stability%2C%20face%20challenges%20such%20as%20pipe%20blockages%20and%20background%0Aleakages%2C%20exacerbated%20by%20operational%20constraints%20such%20as%20data%20non-stationarity%0Aand%20limited%20labeled%20data.%20This%20paper%20proposes%20an%20unsupervised%2C%20online%20learning%0Aframework%20that%20aims%20to%20detect%20two%20types%20of%20faults%20in%20WDNs%3A%20pipe%20blockages%2C%0Amodeled%20as%20collective%20anomalies%2C%20and%20background%20leakages%2C%20modeled%20as%20concept%0Adrift.%20Our%20approach%20combines%20a%20Long%20Short-Term%20Memory%20Variational%20Autoencoder%0A%28LSTM-VAE%29%20with%20a%20dual%20drift%20detection%20mechanism%2C%20enabling%20robust%20detection%20and%0Aadaptation%20under%20non-stationary%20conditions.%20Its%20lightweight%2C%20memory-efficient%0Adesign%20enables%20real-time%2C%20edge-level%20monitoring.%20Experiments%20on%20two%20realistic%0AWDNs%20show%20that%20the%20proposed%20approach%20consistently%20outperforms%20strong%20baselines%0Ain%20detecting%20anomalies%20and%20adapting%20to%20recurrent%20drift%2C%20demonstrating%20its%0Aeffectiveness%20in%20unsupervised%20event%20detection%20for%20dynamic%20WDN%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16336v1&entry.124074799=Read"},
{"title": "Leveraging LLMs for Utility-Focused Annotation: Reducing Manual Effort\n  for Retrieval and RAG", "author": "Hengran Zhang and Minghao Tang and Keping Bi and Jiafeng Guo and Shihao Liu and Daiting Shi and Dawei Yin and Xueqi Cheng", "abstract": "  Retrieval models typically rely on costly human-labeled query-document\nrelevance annotations for training and evaluation. To reduce this cost and\nleverage the potential of Large Language Models (LLMs) in relevance judgments,\nwe aim to explore whether LLM-generated annotations can effectively replace\nhuman annotations in training retrieval models. Retrieval usually emphasizes\nrelevance, which indicates \"topic-relatedness\" of a document to a query, while\nin RAG, the value of a document (or utility) depends on how it contributes to\nanswer generation. Recognizing this mismatch, some researchers use LLM\nperformance on downstream tasks with documents as labels, but this approach\nrequires manual answers for specific tasks, leading to high costs and limited\ngeneralization. In another line of work, prompting LLMs to select useful\ndocuments as RAG references eliminates the need for human annotation and is not\ntask-specific. If we leverage LLMs' utility judgments to annotate retrieval\ndata, we may retain cross-task generalization without human annotation in\nlarge-scale corpora. Therefore, we investigate utility-focused annotation via\nLLMs for large-scale retriever training data across both in-domain and\nout-of-domain settings on the retrieval and RAG tasks. To reduce the impact of\nlow-quality positives labeled by LLMs, we design a novel loss function, i.e.,\nDisj-InfoNCE. Our experiments reveal that: (1) Retrievers trained on\nutility-focused annotations significantly outperform those trained on human\nannotations in the out-of-domain setting on both tasks, demonstrating superior\ngeneralization capabilities. (2) LLM annotation does not replace human\nannotation in the in-domain setting. However, incorporating just 20%\nhuman-annotated data enables retrievers trained with utility-focused\nannotations to match the performance of models trained entirely with human\nannotations.\n", "link": "http://arxiv.org/abs/2504.05220v3", "date": "2025-08-22", "relevancy": 1.9605, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5757}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.473}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20LLMs%20for%20Utility-Focused%20Annotation%3A%20Reducing%20Manual%20Effort%0A%20%20for%20Retrieval%20and%20RAG&body=Title%3A%20Leveraging%20LLMs%20for%20Utility-Focused%20Annotation%3A%20Reducing%20Manual%20Effort%0A%20%20for%20Retrieval%20and%20RAG%0AAuthor%3A%20Hengran%20Zhang%20and%20Minghao%20Tang%20and%20Keping%20Bi%20and%20Jiafeng%20Guo%20and%20Shihao%20Liu%20and%20Daiting%20Shi%20and%20Dawei%20Yin%20and%20Xueqi%20Cheng%0AAbstract%3A%20%20%20Retrieval%20models%20typically%20rely%20on%20costly%20human-labeled%20query-document%0Arelevance%20annotations%20for%20training%20and%20evaluation.%20To%20reduce%20this%20cost%20and%0Aleverage%20the%20potential%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20relevance%20judgments%2C%0Awe%20aim%20to%20explore%20whether%20LLM-generated%20annotations%20can%20effectively%20replace%0Ahuman%20annotations%20in%20training%20retrieval%20models.%20Retrieval%20usually%20emphasizes%0Arelevance%2C%20which%20indicates%20%22topic-relatedness%22%20of%20a%20document%20to%20a%20query%2C%20while%0Ain%20RAG%2C%20the%20value%20of%20a%20document%20%28or%20utility%29%20depends%20on%20how%20it%20contributes%20to%0Aanswer%20generation.%20Recognizing%20this%20mismatch%2C%20some%20researchers%20use%20LLM%0Aperformance%20on%20downstream%20tasks%20with%20documents%20as%20labels%2C%20but%20this%20approach%0Arequires%20manual%20answers%20for%20specific%20tasks%2C%20leading%20to%20high%20costs%20and%20limited%0Ageneralization.%20In%20another%20line%20of%20work%2C%20prompting%20LLMs%20to%20select%20useful%0Adocuments%20as%20RAG%20references%20eliminates%20the%20need%20for%20human%20annotation%20and%20is%20not%0Atask-specific.%20If%20we%20leverage%20LLMs%27%20utility%20judgments%20to%20annotate%20retrieval%0Adata%2C%20we%20may%20retain%20cross-task%20generalization%20without%20human%20annotation%20in%0Alarge-scale%20corpora.%20Therefore%2C%20we%20investigate%20utility-focused%20annotation%20via%0ALLMs%20for%20large-scale%20retriever%20training%20data%20across%20both%20in-domain%20and%0Aout-of-domain%20settings%20on%20the%20retrieval%20and%20RAG%20tasks.%20To%20reduce%20the%20impact%20of%0Alow-quality%20positives%20labeled%20by%20LLMs%2C%20we%20design%20a%20novel%20loss%20function%2C%20i.e.%2C%0ADisj-InfoNCE.%20Our%20experiments%20reveal%20that%3A%20%281%29%20Retrievers%20trained%20on%0Autility-focused%20annotations%20significantly%20outperform%20those%20trained%20on%20human%0Aannotations%20in%20the%20out-of-domain%20setting%20on%20both%20tasks%2C%20demonstrating%20superior%0Ageneralization%20capabilities.%20%282%29%20LLM%20annotation%20does%20not%20replace%20human%0Aannotation%20in%20the%20in-domain%20setting.%20However%2C%20incorporating%20just%2020%25%0Ahuman-annotated%20data%20enables%20retrievers%20trained%20with%20utility-focused%0Aannotations%20to%20match%20the%20performance%20of%20models%20trained%20entirely%20with%20human%0Aannotations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.05220v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520LLMs%2520for%2520Utility-Focused%2520Annotation%253A%2520Reducing%2520Manual%2520Effort%250A%2520%2520for%2520Retrieval%2520and%2520RAG%26entry.906535625%3DHengran%2520Zhang%2520and%2520Minghao%2520Tang%2520and%2520Keping%2520Bi%2520and%2520Jiafeng%2520Guo%2520and%2520Shihao%2520Liu%2520and%2520Daiting%2520Shi%2520and%2520Dawei%2520Yin%2520and%2520Xueqi%2520Cheng%26entry.1292438233%3D%2520%2520Retrieval%2520models%2520typically%2520rely%2520on%2520costly%2520human-labeled%2520query-document%250Arelevance%2520annotations%2520for%2520training%2520and%2520evaluation.%2520To%2520reduce%2520this%2520cost%2520and%250Aleverage%2520the%2520potential%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520in%2520relevance%2520judgments%252C%250Awe%2520aim%2520to%2520explore%2520whether%2520LLM-generated%2520annotations%2520can%2520effectively%2520replace%250Ahuman%2520annotations%2520in%2520training%2520retrieval%2520models.%2520Retrieval%2520usually%2520emphasizes%250Arelevance%252C%2520which%2520indicates%2520%2522topic-relatedness%2522%2520of%2520a%2520document%2520to%2520a%2520query%252C%2520while%250Ain%2520RAG%252C%2520the%2520value%2520of%2520a%2520document%2520%2528or%2520utility%2529%2520depends%2520on%2520how%2520it%2520contributes%2520to%250Aanswer%2520generation.%2520Recognizing%2520this%2520mismatch%252C%2520some%2520researchers%2520use%2520LLM%250Aperformance%2520on%2520downstream%2520tasks%2520with%2520documents%2520as%2520labels%252C%2520but%2520this%2520approach%250Arequires%2520manual%2520answers%2520for%2520specific%2520tasks%252C%2520leading%2520to%2520high%2520costs%2520and%2520limited%250Ageneralization.%2520In%2520another%2520line%2520of%2520work%252C%2520prompting%2520LLMs%2520to%2520select%2520useful%250Adocuments%2520as%2520RAG%2520references%2520eliminates%2520the%2520need%2520for%2520human%2520annotation%2520and%2520is%2520not%250Atask-specific.%2520If%2520we%2520leverage%2520LLMs%2527%2520utility%2520judgments%2520to%2520annotate%2520retrieval%250Adata%252C%2520we%2520may%2520retain%2520cross-task%2520generalization%2520without%2520human%2520annotation%2520in%250Alarge-scale%2520corpora.%2520Therefore%252C%2520we%2520investigate%2520utility-focused%2520annotation%2520via%250ALLMs%2520for%2520large-scale%2520retriever%2520training%2520data%2520across%2520both%2520in-domain%2520and%250Aout-of-domain%2520settings%2520on%2520the%2520retrieval%2520and%2520RAG%2520tasks.%2520To%2520reduce%2520the%2520impact%2520of%250Alow-quality%2520positives%2520labeled%2520by%2520LLMs%252C%2520we%2520design%2520a%2520novel%2520loss%2520function%252C%2520i.e.%252C%250ADisj-InfoNCE.%2520Our%2520experiments%2520reveal%2520that%253A%2520%25281%2529%2520Retrievers%2520trained%2520on%250Autility-focused%2520annotations%2520significantly%2520outperform%2520those%2520trained%2520on%2520human%250Aannotations%2520in%2520the%2520out-of-domain%2520setting%2520on%2520both%2520tasks%252C%2520demonstrating%2520superior%250Ageneralization%2520capabilities.%2520%25282%2529%2520LLM%2520annotation%2520does%2520not%2520replace%2520human%250Aannotation%2520in%2520the%2520in-domain%2520setting.%2520However%252C%2520incorporating%2520just%252020%2525%250Ahuman-annotated%2520data%2520enables%2520retrievers%2520trained%2520with%2520utility-focused%250Aannotations%2520to%2520match%2520the%2520performance%2520of%2520models%2520trained%2520entirely%2520with%2520human%250Aannotations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.05220v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20LLMs%20for%20Utility-Focused%20Annotation%3A%20Reducing%20Manual%20Effort%0A%20%20for%20Retrieval%20and%20RAG&entry.906535625=Hengran%20Zhang%20and%20Minghao%20Tang%20and%20Keping%20Bi%20and%20Jiafeng%20Guo%20and%20Shihao%20Liu%20and%20Daiting%20Shi%20and%20Dawei%20Yin%20and%20Xueqi%20Cheng&entry.1292438233=%20%20Retrieval%20models%20typically%20rely%20on%20costly%20human-labeled%20query-document%0Arelevance%20annotations%20for%20training%20and%20evaluation.%20To%20reduce%20this%20cost%20and%0Aleverage%20the%20potential%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20relevance%20judgments%2C%0Awe%20aim%20to%20explore%20whether%20LLM-generated%20annotations%20can%20effectively%20replace%0Ahuman%20annotations%20in%20training%20retrieval%20models.%20Retrieval%20usually%20emphasizes%0Arelevance%2C%20which%20indicates%20%22topic-relatedness%22%20of%20a%20document%20to%20a%20query%2C%20while%0Ain%20RAG%2C%20the%20value%20of%20a%20document%20%28or%20utility%29%20depends%20on%20how%20it%20contributes%20to%0Aanswer%20generation.%20Recognizing%20this%20mismatch%2C%20some%20researchers%20use%20LLM%0Aperformance%20on%20downstream%20tasks%20with%20documents%20as%20labels%2C%20but%20this%20approach%0Arequires%20manual%20answers%20for%20specific%20tasks%2C%20leading%20to%20high%20costs%20and%20limited%0Ageneralization.%20In%20another%20line%20of%20work%2C%20prompting%20LLMs%20to%20select%20useful%0Adocuments%20as%20RAG%20references%20eliminates%20the%20need%20for%20human%20annotation%20and%20is%20not%0Atask-specific.%20If%20we%20leverage%20LLMs%27%20utility%20judgments%20to%20annotate%20retrieval%0Adata%2C%20we%20may%20retain%20cross-task%20generalization%20without%20human%20annotation%20in%0Alarge-scale%20corpora.%20Therefore%2C%20we%20investigate%20utility-focused%20annotation%20via%0ALLMs%20for%20large-scale%20retriever%20training%20data%20across%20both%20in-domain%20and%0Aout-of-domain%20settings%20on%20the%20retrieval%20and%20RAG%20tasks.%20To%20reduce%20the%20impact%20of%0Alow-quality%20positives%20labeled%20by%20LLMs%2C%20we%20design%20a%20novel%20loss%20function%2C%20i.e.%2C%0ADisj-InfoNCE.%20Our%20experiments%20reveal%20that%3A%20%281%29%20Retrievers%20trained%20on%0Autility-focused%20annotations%20significantly%20outperform%20those%20trained%20on%20human%0Aannotations%20in%20the%20out-of-domain%20setting%20on%20both%20tasks%2C%20demonstrating%20superior%0Ageneralization%20capabilities.%20%282%29%20LLM%20annotation%20does%20not%20replace%20human%0Aannotation%20in%20the%20in-domain%20setting.%20However%2C%20incorporating%20just%2020%25%0Ahuman-annotated%20data%20enables%20retrievers%20trained%20with%20utility-focused%0Aannotations%20to%20match%20the%20performance%20of%20models%20trained%20entirely%20with%20human%0Aannotations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.05220v3&entry.124074799=Read"},
{"title": "Coarse-to-Fine Process Reward Modeling for Mathematical Reasoning", "author": "Yulan Hu and Sheng Ouyang and Jinman Zhao and Yong Liu", "abstract": "  The Process Reward Model (PRM) plays a crucial role in mathematical reasoning\ntasks, requiring high-quality supervised process data. However, we observe that\nreasoning steps generated by Large Language Models (LLMs) often fail to exhibit\nstrictly incremental information, leading to redundancy that can hinder\neffective reasoning. To address this issue, we propose CFPRM, a simple yet\neffective coarse-to-fine strategy. Instead of focusing on the detection of\nredundant steps, our approach first establishes a coarse-grained window to\nmerge adjacent reasoning steps into unified, holistic steps. The window size is\nthen progressively reduced to extract fine-grained reasoning steps, enabling\ndata collection at multiple granularities for training. By leveraging this\nhierarchical refinement process, CFPRM mitigates redundancy while preserving\nessential fine-grained knowledge. Extensive experiments on two reasoning\ndatasets across three loss criteria validate the CFPRM's effectiveness and\nversatility.\n", "link": "http://arxiv.org/abs/2501.13622v4", "date": "2025-08-22", "relevancy": 1.9589, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4928}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4928}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4744}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Coarse-to-Fine%20Process%20Reward%20Modeling%20for%20Mathematical%20Reasoning&body=Title%3A%20Coarse-to-Fine%20Process%20Reward%20Modeling%20for%20Mathematical%20Reasoning%0AAuthor%3A%20Yulan%20Hu%20and%20Sheng%20Ouyang%20and%20Jinman%20Zhao%20and%20Yong%20Liu%0AAbstract%3A%20%20%20The%20Process%20Reward%20Model%20%28PRM%29%20plays%20a%20crucial%20role%20in%20mathematical%20reasoning%0Atasks%2C%20requiring%20high-quality%20supervised%20process%20data.%20However%2C%20we%20observe%20that%0Areasoning%20steps%20generated%20by%20Large%20Language%20Models%20%28LLMs%29%20often%20fail%20to%20exhibit%0Astrictly%20incremental%20information%2C%20leading%20to%20redundancy%20that%20can%20hinder%0Aeffective%20reasoning.%20To%20address%20this%20issue%2C%20we%20propose%20CFPRM%2C%20a%20simple%20yet%0Aeffective%20coarse-to-fine%20strategy.%20Instead%20of%20focusing%20on%20the%20detection%20of%0Aredundant%20steps%2C%20our%20approach%20first%20establishes%20a%20coarse-grained%20window%20to%0Amerge%20adjacent%20reasoning%20steps%20into%20unified%2C%20holistic%20steps.%20The%20window%20size%20is%0Athen%20progressively%20reduced%20to%20extract%20fine-grained%20reasoning%20steps%2C%20enabling%0Adata%20collection%20at%20multiple%20granularities%20for%20training.%20By%20leveraging%20this%0Ahierarchical%20refinement%20process%2C%20CFPRM%20mitigates%20redundancy%20while%20preserving%0Aessential%20fine-grained%20knowledge.%20Extensive%20experiments%20on%20two%20reasoning%0Adatasets%20across%20three%20loss%20criteria%20validate%20the%20CFPRM%27s%20effectiveness%20and%0Aversatility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13622v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoarse-to-Fine%2520Process%2520Reward%2520Modeling%2520for%2520Mathematical%2520Reasoning%26entry.906535625%3DYulan%2520Hu%2520and%2520Sheng%2520Ouyang%2520and%2520Jinman%2520Zhao%2520and%2520Yong%2520Liu%26entry.1292438233%3D%2520%2520The%2520Process%2520Reward%2520Model%2520%2528PRM%2529%2520plays%2520a%2520crucial%2520role%2520in%2520mathematical%2520reasoning%250Atasks%252C%2520requiring%2520high-quality%2520supervised%2520process%2520data.%2520However%252C%2520we%2520observe%2520that%250Areasoning%2520steps%2520generated%2520by%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520often%2520fail%2520to%2520exhibit%250Astrictly%2520incremental%2520information%252C%2520leading%2520to%2520redundancy%2520that%2520can%2520hinder%250Aeffective%2520reasoning.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520CFPRM%252C%2520a%2520simple%2520yet%250Aeffective%2520coarse-to-fine%2520strategy.%2520Instead%2520of%2520focusing%2520on%2520the%2520detection%2520of%250Aredundant%2520steps%252C%2520our%2520approach%2520first%2520establishes%2520a%2520coarse-grained%2520window%2520to%250Amerge%2520adjacent%2520reasoning%2520steps%2520into%2520unified%252C%2520holistic%2520steps.%2520The%2520window%2520size%2520is%250Athen%2520progressively%2520reduced%2520to%2520extract%2520fine-grained%2520reasoning%2520steps%252C%2520enabling%250Adata%2520collection%2520at%2520multiple%2520granularities%2520for%2520training.%2520By%2520leveraging%2520this%250Ahierarchical%2520refinement%2520process%252C%2520CFPRM%2520mitigates%2520redundancy%2520while%2520preserving%250Aessential%2520fine-grained%2520knowledge.%2520Extensive%2520experiments%2520on%2520two%2520reasoning%250Adatasets%2520across%2520three%2520loss%2520criteria%2520validate%2520the%2520CFPRM%2527s%2520effectiveness%2520and%250Aversatility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13622v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Coarse-to-Fine%20Process%20Reward%20Modeling%20for%20Mathematical%20Reasoning&entry.906535625=Yulan%20Hu%20and%20Sheng%20Ouyang%20and%20Jinman%20Zhao%20and%20Yong%20Liu&entry.1292438233=%20%20The%20Process%20Reward%20Model%20%28PRM%29%20plays%20a%20crucial%20role%20in%20mathematical%20reasoning%0Atasks%2C%20requiring%20high-quality%20supervised%20process%20data.%20However%2C%20we%20observe%20that%0Areasoning%20steps%20generated%20by%20Large%20Language%20Models%20%28LLMs%29%20often%20fail%20to%20exhibit%0Astrictly%20incremental%20information%2C%20leading%20to%20redundancy%20that%20can%20hinder%0Aeffective%20reasoning.%20To%20address%20this%20issue%2C%20we%20propose%20CFPRM%2C%20a%20simple%20yet%0Aeffective%20coarse-to-fine%20strategy.%20Instead%20of%20focusing%20on%20the%20detection%20of%0Aredundant%20steps%2C%20our%20approach%20first%20establishes%20a%20coarse-grained%20window%20to%0Amerge%20adjacent%20reasoning%20steps%20into%20unified%2C%20holistic%20steps.%20The%20window%20size%20is%0Athen%20progressively%20reduced%20to%20extract%20fine-grained%20reasoning%20steps%2C%20enabling%0Adata%20collection%20at%20multiple%20granularities%20for%20training.%20By%20leveraging%20this%0Ahierarchical%20refinement%20process%2C%20CFPRM%20mitigates%20redundancy%20while%20preserving%0Aessential%20fine-grained%20knowledge.%20Extensive%20experiments%20on%20two%20reasoning%0Adatasets%20across%20three%20loss%20criteria%20validate%20the%20CFPRM%27s%20effectiveness%20and%0Aversatility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13622v4&entry.124074799=Read"},
{"title": "Nested Graph Pseudo-Label Refinement for Noisy Label Domain Adaptation\n  Learning", "author": "Yingxu Wang and Mengzhu Wang and Zhichao Huang and Suyu Liu and Nan Yin", "abstract": "  Graph Domain Adaptation (GDA) facilitates knowledge transfer from labeled\nsource graphs to unlabeled target graphs by learning domain-invariant\nrepresentations, which is essential in applications such as molecular property\nprediction and social network analysis. However, most existing GDA methods rely\non the assumption of clean source labels, which rarely holds in real-world\nscenarios where annotation noise is pervasive. This label noise severely\nimpairs feature alignment and degrades adaptation performance under domain\nshifts. To address this challenge, we propose Nested Graph Pseudo-Label\nRefinement (NeGPR), a novel framework tailored for graph-level domain\nadaptation with noisy labels. NeGPR first pretrains dual branches, i.e.,\nsemantic and topology branches, by enforcing neighborhood consistency in the\nfeature space, thereby reducing the influence of noisy supervision. To bridge\ndomain gaps, NeGPR employs a nested refinement mechanism in which one branch\nselects high-confidence target samples to guide the adaptation of the other,\nenabling progressive cross-domain learning. Furthermore, since pseudo-labels\nmay still contain noise and the pre-trained branches are already overfitted to\nthe noisy labels in the source domain, NeGPR incorporates a noise-aware\nregularization strategy. This regularization is theoretically proven to\nmitigate the adverse effects of pseudo-label noise, even under the presence of\nsource overfitting, thus enhancing the robustness of the adaptation process.\nExtensive experiments on benchmark datasets demonstrate that NeGPR consistently\noutperforms state-of-the-art methods under severe label noise, achieving gains\nof up to 12.7% in accuracy.\n", "link": "http://arxiv.org/abs/2508.00716v2", "date": "2025-08-22", "relevancy": 1.9566, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5056}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4822}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nested%20Graph%20Pseudo-Label%20Refinement%20for%20Noisy%20Label%20Domain%20Adaptation%0A%20%20Learning&body=Title%3A%20Nested%20Graph%20Pseudo-Label%20Refinement%20for%20Noisy%20Label%20Domain%20Adaptation%0A%20%20Learning%0AAuthor%3A%20Yingxu%20Wang%20and%20Mengzhu%20Wang%20and%20Zhichao%20Huang%20and%20Suyu%20Liu%20and%20Nan%20Yin%0AAbstract%3A%20%20%20Graph%20Domain%20Adaptation%20%28GDA%29%20facilitates%20knowledge%20transfer%20from%20labeled%0Asource%20graphs%20to%20unlabeled%20target%20graphs%20by%20learning%20domain-invariant%0Arepresentations%2C%20which%20is%20essential%20in%20applications%20such%20as%20molecular%20property%0Aprediction%20and%20social%20network%20analysis.%20However%2C%20most%20existing%20GDA%20methods%20rely%0Aon%20the%20assumption%20of%20clean%20source%20labels%2C%20which%20rarely%20holds%20in%20real-world%0Ascenarios%20where%20annotation%20noise%20is%20pervasive.%20This%20label%20noise%20severely%0Aimpairs%20feature%20alignment%20and%20degrades%20adaptation%20performance%20under%20domain%0Ashifts.%20To%20address%20this%20challenge%2C%20we%20propose%20Nested%20Graph%20Pseudo-Label%0ARefinement%20%28NeGPR%29%2C%20a%20novel%20framework%20tailored%20for%20graph-level%20domain%0Aadaptation%20with%20noisy%20labels.%20NeGPR%20first%20pretrains%20dual%20branches%2C%20i.e.%2C%0Asemantic%20and%20topology%20branches%2C%20by%20enforcing%20neighborhood%20consistency%20in%20the%0Afeature%20space%2C%20thereby%20reducing%20the%20influence%20of%20noisy%20supervision.%20To%20bridge%0Adomain%20gaps%2C%20NeGPR%20employs%20a%20nested%20refinement%20mechanism%20in%20which%20one%20branch%0Aselects%20high-confidence%20target%20samples%20to%20guide%20the%20adaptation%20of%20the%20other%2C%0Aenabling%20progressive%20cross-domain%20learning.%20Furthermore%2C%20since%20pseudo-labels%0Amay%20still%20contain%20noise%20and%20the%20pre-trained%20branches%20are%20already%20overfitted%20to%0Athe%20noisy%20labels%20in%20the%20source%20domain%2C%20NeGPR%20incorporates%20a%20noise-aware%0Aregularization%20strategy.%20This%20regularization%20is%20theoretically%20proven%20to%0Amitigate%20the%20adverse%20effects%20of%20pseudo-label%20noise%2C%20even%20under%20the%20presence%20of%0Asource%20overfitting%2C%20thus%20enhancing%20the%20robustness%20of%20the%20adaptation%20process.%0AExtensive%20experiments%20on%20benchmark%20datasets%20demonstrate%20that%20NeGPR%20consistently%0Aoutperforms%20state-of-the-art%20methods%20under%20severe%20label%20noise%2C%20achieving%20gains%0Aof%20up%20to%2012.7%25%20in%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00716v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNested%2520Graph%2520Pseudo-Label%2520Refinement%2520for%2520Noisy%2520Label%2520Domain%2520Adaptation%250A%2520%2520Learning%26entry.906535625%3DYingxu%2520Wang%2520and%2520Mengzhu%2520Wang%2520and%2520Zhichao%2520Huang%2520and%2520Suyu%2520Liu%2520and%2520Nan%2520Yin%26entry.1292438233%3D%2520%2520Graph%2520Domain%2520Adaptation%2520%2528GDA%2529%2520facilitates%2520knowledge%2520transfer%2520from%2520labeled%250Asource%2520graphs%2520to%2520unlabeled%2520target%2520graphs%2520by%2520learning%2520domain-invariant%250Arepresentations%252C%2520which%2520is%2520essential%2520in%2520applications%2520such%2520as%2520molecular%2520property%250Aprediction%2520and%2520social%2520network%2520analysis.%2520However%252C%2520most%2520existing%2520GDA%2520methods%2520rely%250Aon%2520the%2520assumption%2520of%2520clean%2520source%2520labels%252C%2520which%2520rarely%2520holds%2520in%2520real-world%250Ascenarios%2520where%2520annotation%2520noise%2520is%2520pervasive.%2520This%2520label%2520noise%2520severely%250Aimpairs%2520feature%2520alignment%2520and%2520degrades%2520adaptation%2520performance%2520under%2520domain%250Ashifts.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520Nested%2520Graph%2520Pseudo-Label%250ARefinement%2520%2528NeGPR%2529%252C%2520a%2520novel%2520framework%2520tailored%2520for%2520graph-level%2520domain%250Aadaptation%2520with%2520noisy%2520labels.%2520NeGPR%2520first%2520pretrains%2520dual%2520branches%252C%2520i.e.%252C%250Asemantic%2520and%2520topology%2520branches%252C%2520by%2520enforcing%2520neighborhood%2520consistency%2520in%2520the%250Afeature%2520space%252C%2520thereby%2520reducing%2520the%2520influence%2520of%2520noisy%2520supervision.%2520To%2520bridge%250Adomain%2520gaps%252C%2520NeGPR%2520employs%2520a%2520nested%2520refinement%2520mechanism%2520in%2520which%2520one%2520branch%250Aselects%2520high-confidence%2520target%2520samples%2520to%2520guide%2520the%2520adaptation%2520of%2520the%2520other%252C%250Aenabling%2520progressive%2520cross-domain%2520learning.%2520Furthermore%252C%2520since%2520pseudo-labels%250Amay%2520still%2520contain%2520noise%2520and%2520the%2520pre-trained%2520branches%2520are%2520already%2520overfitted%2520to%250Athe%2520noisy%2520labels%2520in%2520the%2520source%2520domain%252C%2520NeGPR%2520incorporates%2520a%2520noise-aware%250Aregularization%2520strategy.%2520This%2520regularization%2520is%2520theoretically%2520proven%2520to%250Amitigate%2520the%2520adverse%2520effects%2520of%2520pseudo-label%2520noise%252C%2520even%2520under%2520the%2520presence%2520of%250Asource%2520overfitting%252C%2520thus%2520enhancing%2520the%2520robustness%2520of%2520the%2520adaptation%2520process.%250AExtensive%2520experiments%2520on%2520benchmark%2520datasets%2520demonstrate%2520that%2520NeGPR%2520consistently%250Aoutperforms%2520state-of-the-art%2520methods%2520under%2520severe%2520label%2520noise%252C%2520achieving%2520gains%250Aof%2520up%2520to%252012.7%2525%2520in%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00716v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nested%20Graph%20Pseudo-Label%20Refinement%20for%20Noisy%20Label%20Domain%20Adaptation%0A%20%20Learning&entry.906535625=Yingxu%20Wang%20and%20Mengzhu%20Wang%20and%20Zhichao%20Huang%20and%20Suyu%20Liu%20and%20Nan%20Yin&entry.1292438233=%20%20Graph%20Domain%20Adaptation%20%28GDA%29%20facilitates%20knowledge%20transfer%20from%20labeled%0Asource%20graphs%20to%20unlabeled%20target%20graphs%20by%20learning%20domain-invariant%0Arepresentations%2C%20which%20is%20essential%20in%20applications%20such%20as%20molecular%20property%0Aprediction%20and%20social%20network%20analysis.%20However%2C%20most%20existing%20GDA%20methods%20rely%0Aon%20the%20assumption%20of%20clean%20source%20labels%2C%20which%20rarely%20holds%20in%20real-world%0Ascenarios%20where%20annotation%20noise%20is%20pervasive.%20This%20label%20noise%20severely%0Aimpairs%20feature%20alignment%20and%20degrades%20adaptation%20performance%20under%20domain%0Ashifts.%20To%20address%20this%20challenge%2C%20we%20propose%20Nested%20Graph%20Pseudo-Label%0ARefinement%20%28NeGPR%29%2C%20a%20novel%20framework%20tailored%20for%20graph-level%20domain%0Aadaptation%20with%20noisy%20labels.%20NeGPR%20first%20pretrains%20dual%20branches%2C%20i.e.%2C%0Asemantic%20and%20topology%20branches%2C%20by%20enforcing%20neighborhood%20consistency%20in%20the%0Afeature%20space%2C%20thereby%20reducing%20the%20influence%20of%20noisy%20supervision.%20To%20bridge%0Adomain%20gaps%2C%20NeGPR%20employs%20a%20nested%20refinement%20mechanism%20in%20which%20one%20branch%0Aselects%20high-confidence%20target%20samples%20to%20guide%20the%20adaptation%20of%20the%20other%2C%0Aenabling%20progressive%20cross-domain%20learning.%20Furthermore%2C%20since%20pseudo-labels%0Amay%20still%20contain%20noise%20and%20the%20pre-trained%20branches%20are%20already%20overfitted%20to%0Athe%20noisy%20labels%20in%20the%20source%20domain%2C%20NeGPR%20incorporates%20a%20noise-aware%0Aregularization%20strategy.%20This%20regularization%20is%20theoretically%20proven%20to%0Amitigate%20the%20adverse%20effects%20of%20pseudo-label%20noise%2C%20even%20under%20the%20presence%20of%0Asource%20overfitting%2C%20thus%20enhancing%20the%20robustness%20of%20the%20adaptation%20process.%0AExtensive%20experiments%20on%20benchmark%20datasets%20demonstrate%20that%20NeGPR%20consistently%0Aoutperforms%20state-of-the-art%20methods%20under%20severe%20label%20noise%2C%20achieving%20gains%0Aof%20up%20to%2012.7%25%20in%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00716v2&entry.124074799=Read"},
{"title": "AgentScope 1.0: A Developer-Centric Framework for Building Agentic\n  Applications", "author": "Dawei Gao and Zitao Li and Yuexiang Xie and Weirui Kuang and Liuyi Yao and Bingchen Qian and Zhijian Ma and Yue Cui and Haohao Luo and Shen Li and Lu Yi and Yi Yu and Shiqi He and Zhiling Luo and Wenmeng Zhou and Zhicheng Zhang and Xuguang He and Ziqian Chen and Weikai Liao and Farruh Isakulovich Kushnazarov and Yaliang Li and Bolin Ding and Jingren Zhou", "abstract": "  Driven by rapid advancements of Large Language Models (LLMs), agents are\nempowered to combine intrinsic knowledge with dynamic tool use, greatly\nenhancing their capacity to address real-world tasks. In line with such an\nevolution, AgentScope introduces major improvements in a new version (1.0),\ntowards comprehensively supporting flexible and efficient tool-based\nagent-environment interactions for building agentic applications. Specifically,\nwe abstract foundational components essential for agentic applications and\nprovide unified interfaces and extensible modules, enabling developers to\neasily leverage the latest progress, such as new models and MCPs. Furthermore,\nwe ground agent behaviors in the ReAct paradigm and offer advanced agent-level\ninfrastructure based on a systematic asynchronous design, which enriches both\nhuman-agent and agent-agent interaction patterns while improving execution\nefficiency. Building on this foundation, we integrate several built-in agents\ntailored to specific practical scenarios. AgentScope also includes robust\nengineering support for developer-friendly experiences. We provide a scalable\nevaluation module with a visual studio interface, making the development of\nlong-trajectory agentic applications more manageable and easier to trace. In\naddition, AgentScope offers a runtime sandbox to ensure safe agent execution\nand facilitates rapid deployment in production environments. With these\nenhancements, AgentScope provides a practical foundation for building scalable,\nadaptive, and effective agentic applications.\n", "link": "http://arxiv.org/abs/2508.16279v1", "date": "2025-08-22", "relevancy": 1.9526, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5511}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4897}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AgentScope%201.0%3A%20A%20Developer-Centric%20Framework%20for%20Building%20Agentic%0A%20%20Applications&body=Title%3A%20AgentScope%201.0%3A%20A%20Developer-Centric%20Framework%20for%20Building%20Agentic%0A%20%20Applications%0AAuthor%3A%20Dawei%20Gao%20and%20Zitao%20Li%20and%20Yuexiang%20Xie%20and%20Weirui%20Kuang%20and%20Liuyi%20Yao%20and%20Bingchen%20Qian%20and%20Zhijian%20Ma%20and%20Yue%20Cui%20and%20Haohao%20Luo%20and%20Shen%20Li%20and%20Lu%20Yi%20and%20Yi%20Yu%20and%20Shiqi%20He%20and%20Zhiling%20Luo%20and%20Wenmeng%20Zhou%20and%20Zhicheng%20Zhang%20and%20Xuguang%20He%20and%20Ziqian%20Chen%20and%20Weikai%20Liao%20and%20Farruh%20Isakulovich%20Kushnazarov%20and%20Yaliang%20Li%20and%20Bolin%20Ding%20and%20Jingren%20Zhou%0AAbstract%3A%20%20%20Driven%20by%20rapid%20advancements%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20agents%20are%0Aempowered%20to%20combine%20intrinsic%20knowledge%20with%20dynamic%20tool%20use%2C%20greatly%0Aenhancing%20their%20capacity%20to%20address%20real-world%20tasks.%20In%20line%20with%20such%20an%0Aevolution%2C%20AgentScope%20introduces%20major%20improvements%20in%20a%20new%20version%20%281.0%29%2C%0Atowards%20comprehensively%20supporting%20flexible%20and%20efficient%20tool-based%0Aagent-environment%20interactions%20for%20building%20agentic%20applications.%20Specifically%2C%0Awe%20abstract%20foundational%20components%20essential%20for%20agentic%20applications%20and%0Aprovide%20unified%20interfaces%20and%20extensible%20modules%2C%20enabling%20developers%20to%0Aeasily%20leverage%20the%20latest%20progress%2C%20such%20as%20new%20models%20and%20MCPs.%20Furthermore%2C%0Awe%20ground%20agent%20behaviors%20in%20the%20ReAct%20paradigm%20and%20offer%20advanced%20agent-level%0Ainfrastructure%20based%20on%20a%20systematic%20asynchronous%20design%2C%20which%20enriches%20both%0Ahuman-agent%20and%20agent-agent%20interaction%20patterns%20while%20improving%20execution%0Aefficiency.%20Building%20on%20this%20foundation%2C%20we%20integrate%20several%20built-in%20agents%0Atailored%20to%20specific%20practical%20scenarios.%20AgentScope%20also%20includes%20robust%0Aengineering%20support%20for%20developer-friendly%20experiences.%20We%20provide%20a%20scalable%0Aevaluation%20module%20with%20a%20visual%20studio%20interface%2C%20making%20the%20development%20of%0Along-trajectory%20agentic%20applications%20more%20manageable%20and%20easier%20to%20trace.%20In%0Aaddition%2C%20AgentScope%20offers%20a%20runtime%20sandbox%20to%20ensure%20safe%20agent%20execution%0Aand%20facilitates%20rapid%20deployment%20in%20production%20environments.%20With%20these%0Aenhancements%2C%20AgentScope%20provides%20a%20practical%20foundation%20for%20building%20scalable%2C%0Aadaptive%2C%20and%20effective%20agentic%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16279v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentScope%25201.0%253A%2520A%2520Developer-Centric%2520Framework%2520for%2520Building%2520Agentic%250A%2520%2520Applications%26entry.906535625%3DDawei%2520Gao%2520and%2520Zitao%2520Li%2520and%2520Yuexiang%2520Xie%2520and%2520Weirui%2520Kuang%2520and%2520Liuyi%2520Yao%2520and%2520Bingchen%2520Qian%2520and%2520Zhijian%2520Ma%2520and%2520Yue%2520Cui%2520and%2520Haohao%2520Luo%2520and%2520Shen%2520Li%2520and%2520Lu%2520Yi%2520and%2520Yi%2520Yu%2520and%2520Shiqi%2520He%2520and%2520Zhiling%2520Luo%2520and%2520Wenmeng%2520Zhou%2520and%2520Zhicheng%2520Zhang%2520and%2520Xuguang%2520He%2520and%2520Ziqian%2520Chen%2520and%2520Weikai%2520Liao%2520and%2520Farruh%2520Isakulovich%2520Kushnazarov%2520and%2520Yaliang%2520Li%2520and%2520Bolin%2520Ding%2520and%2520Jingren%2520Zhou%26entry.1292438233%3D%2520%2520Driven%2520by%2520rapid%2520advancements%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520agents%2520are%250Aempowered%2520to%2520combine%2520intrinsic%2520knowledge%2520with%2520dynamic%2520tool%2520use%252C%2520greatly%250Aenhancing%2520their%2520capacity%2520to%2520address%2520real-world%2520tasks.%2520In%2520line%2520with%2520such%2520an%250Aevolution%252C%2520AgentScope%2520introduces%2520major%2520improvements%2520in%2520a%2520new%2520version%2520%25281.0%2529%252C%250Atowards%2520comprehensively%2520supporting%2520flexible%2520and%2520efficient%2520tool-based%250Aagent-environment%2520interactions%2520for%2520building%2520agentic%2520applications.%2520Specifically%252C%250Awe%2520abstract%2520foundational%2520components%2520essential%2520for%2520agentic%2520applications%2520and%250Aprovide%2520unified%2520interfaces%2520and%2520extensible%2520modules%252C%2520enabling%2520developers%2520to%250Aeasily%2520leverage%2520the%2520latest%2520progress%252C%2520such%2520as%2520new%2520models%2520and%2520MCPs.%2520Furthermore%252C%250Awe%2520ground%2520agent%2520behaviors%2520in%2520the%2520ReAct%2520paradigm%2520and%2520offer%2520advanced%2520agent-level%250Ainfrastructure%2520based%2520on%2520a%2520systematic%2520asynchronous%2520design%252C%2520which%2520enriches%2520both%250Ahuman-agent%2520and%2520agent-agent%2520interaction%2520patterns%2520while%2520improving%2520execution%250Aefficiency.%2520Building%2520on%2520this%2520foundation%252C%2520we%2520integrate%2520several%2520built-in%2520agents%250Atailored%2520to%2520specific%2520practical%2520scenarios.%2520AgentScope%2520also%2520includes%2520robust%250Aengineering%2520support%2520for%2520developer-friendly%2520experiences.%2520We%2520provide%2520a%2520scalable%250Aevaluation%2520module%2520with%2520a%2520visual%2520studio%2520interface%252C%2520making%2520the%2520development%2520of%250Along-trajectory%2520agentic%2520applications%2520more%2520manageable%2520and%2520easier%2520to%2520trace.%2520In%250Aaddition%252C%2520AgentScope%2520offers%2520a%2520runtime%2520sandbox%2520to%2520ensure%2520safe%2520agent%2520execution%250Aand%2520facilitates%2520rapid%2520deployment%2520in%2520production%2520environments.%2520With%2520these%250Aenhancements%252C%2520AgentScope%2520provides%2520a%2520practical%2520foundation%2520for%2520building%2520scalable%252C%250Aadaptive%252C%2520and%2520effective%2520agentic%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16279v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AgentScope%201.0%3A%20A%20Developer-Centric%20Framework%20for%20Building%20Agentic%0A%20%20Applications&entry.906535625=Dawei%20Gao%20and%20Zitao%20Li%20and%20Yuexiang%20Xie%20and%20Weirui%20Kuang%20and%20Liuyi%20Yao%20and%20Bingchen%20Qian%20and%20Zhijian%20Ma%20and%20Yue%20Cui%20and%20Haohao%20Luo%20and%20Shen%20Li%20and%20Lu%20Yi%20and%20Yi%20Yu%20and%20Shiqi%20He%20and%20Zhiling%20Luo%20and%20Wenmeng%20Zhou%20and%20Zhicheng%20Zhang%20and%20Xuguang%20He%20and%20Ziqian%20Chen%20and%20Weikai%20Liao%20and%20Farruh%20Isakulovich%20Kushnazarov%20and%20Yaliang%20Li%20and%20Bolin%20Ding%20and%20Jingren%20Zhou&entry.1292438233=%20%20Driven%20by%20rapid%20advancements%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20agents%20are%0Aempowered%20to%20combine%20intrinsic%20knowledge%20with%20dynamic%20tool%20use%2C%20greatly%0Aenhancing%20their%20capacity%20to%20address%20real-world%20tasks.%20In%20line%20with%20such%20an%0Aevolution%2C%20AgentScope%20introduces%20major%20improvements%20in%20a%20new%20version%20%281.0%29%2C%0Atowards%20comprehensively%20supporting%20flexible%20and%20efficient%20tool-based%0Aagent-environment%20interactions%20for%20building%20agentic%20applications.%20Specifically%2C%0Awe%20abstract%20foundational%20components%20essential%20for%20agentic%20applications%20and%0Aprovide%20unified%20interfaces%20and%20extensible%20modules%2C%20enabling%20developers%20to%0Aeasily%20leverage%20the%20latest%20progress%2C%20such%20as%20new%20models%20and%20MCPs.%20Furthermore%2C%0Awe%20ground%20agent%20behaviors%20in%20the%20ReAct%20paradigm%20and%20offer%20advanced%20agent-level%0Ainfrastructure%20based%20on%20a%20systematic%20asynchronous%20design%2C%20which%20enriches%20both%0Ahuman-agent%20and%20agent-agent%20interaction%20patterns%20while%20improving%20execution%0Aefficiency.%20Building%20on%20this%20foundation%2C%20we%20integrate%20several%20built-in%20agents%0Atailored%20to%20specific%20practical%20scenarios.%20AgentScope%20also%20includes%20robust%0Aengineering%20support%20for%20developer-friendly%20experiences.%20We%20provide%20a%20scalable%0Aevaluation%20module%20with%20a%20visual%20studio%20interface%2C%20making%20the%20development%20of%0Along-trajectory%20agentic%20applications%20more%20manageable%20and%20easier%20to%20trace.%20In%0Aaddition%2C%20AgentScope%20offers%20a%20runtime%20sandbox%20to%20ensure%20safe%20agent%20execution%0Aand%20facilitates%20rapid%20deployment%20in%20production%20environments.%20With%20these%0Aenhancements%2C%20AgentScope%20provides%20a%20practical%20foundation%20for%20building%20scalable%2C%0Aadaptive%2C%20and%20effective%20agentic%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16279v1&entry.124074799=Read"},
{"title": "NOSTRA: A noise-resilient and sparse data framework for trust region\n  based multi objective Bayesian optimization", "author": "Maryam Ghasemzadeh and Anton van Beek", "abstract": "  Multi-objective Bayesian optimization (MOBO) struggles with sparse\n(non-space-filling), scarce (limited observations) datasets affected by\nexperimental uncertainty, where identical inputs can yield varying outputs.\nThese challenges are common in physical and simulation experiments (e.g.,\nrandomized medical trials and, molecular dynamics simulations) and are\ntherefore incompatible with conventional MOBO methods. As a result,\nexperimental resources are inefficiently allocated, leading to suboptimal\ndesigns. To address this challenge, we introduce NOSTRA (Noisy and Sparse Data\nTrust Region-based Optimization Algorithm), a novel sampling framework that\nintegrates prior knowledge of experimental uncertainty to construct more\naccurate surrogate models while employing trust regions to focus sampling on\npromising areas of the design space. By strategically leveraging prior\ninformation and refining search regions, NOSTRA accelerates convergence to the\nPareto frontier, enhances data efficiency, and improves solution quality.\nThrough two test functions with varying levels of experimental uncertainty, we\ndemonstrate that NOSTRA outperforms existing methods in handling noisy, sparse,\nand scarce data. Specifically, we illustrate that, NOSTRA effectively\nprioritizes regions where samples enhance the accuracy of the identified Pareto\nfrontier, offering a resource-efficient algorithm that is practical in\nscenarios with limited experimental budgets while ensuring efficient\nperformance.\n", "link": "http://arxiv.org/abs/2508.16476v1", "date": "2025-08-22", "relevancy": 1.9503, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5208}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4888}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NOSTRA%3A%20A%20noise-resilient%20and%20sparse%20data%20framework%20for%20trust%20region%0A%20%20based%20multi%20objective%20Bayesian%20optimization&body=Title%3A%20NOSTRA%3A%20A%20noise-resilient%20and%20sparse%20data%20framework%20for%20trust%20region%0A%20%20based%20multi%20objective%20Bayesian%20optimization%0AAuthor%3A%20Maryam%20Ghasemzadeh%20and%20Anton%20van%20Beek%0AAbstract%3A%20%20%20Multi-objective%20Bayesian%20optimization%20%28MOBO%29%20struggles%20with%20sparse%0A%28non-space-filling%29%2C%20scarce%20%28limited%20observations%29%20datasets%20affected%20by%0Aexperimental%20uncertainty%2C%20where%20identical%20inputs%20can%20yield%20varying%20outputs.%0AThese%20challenges%20are%20common%20in%20physical%20and%20simulation%20experiments%20%28e.g.%2C%0Arandomized%20medical%20trials%20and%2C%20molecular%20dynamics%20simulations%29%20and%20are%0Atherefore%20incompatible%20with%20conventional%20MOBO%20methods.%20As%20a%20result%2C%0Aexperimental%20resources%20are%20inefficiently%20allocated%2C%20leading%20to%20suboptimal%0Adesigns.%20To%20address%20this%20challenge%2C%20we%20introduce%20NOSTRA%20%28Noisy%20and%20Sparse%20Data%0ATrust%20Region-based%20Optimization%20Algorithm%29%2C%20a%20novel%20sampling%20framework%20that%0Aintegrates%20prior%20knowledge%20of%20experimental%20uncertainty%20to%20construct%20more%0Aaccurate%20surrogate%20models%20while%20employing%20trust%20regions%20to%20focus%20sampling%20on%0Apromising%20areas%20of%20the%20design%20space.%20By%20strategically%20leveraging%20prior%0Ainformation%20and%20refining%20search%20regions%2C%20NOSTRA%20accelerates%20convergence%20to%20the%0APareto%20frontier%2C%20enhances%20data%20efficiency%2C%20and%20improves%20solution%20quality.%0AThrough%20two%20test%20functions%20with%20varying%20levels%20of%20experimental%20uncertainty%2C%20we%0Ademonstrate%20that%20NOSTRA%20outperforms%20existing%20methods%20in%20handling%20noisy%2C%20sparse%2C%0Aand%20scarce%20data.%20Specifically%2C%20we%20illustrate%20that%2C%20NOSTRA%20effectively%0Aprioritizes%20regions%20where%20samples%20enhance%20the%20accuracy%20of%20the%20identified%20Pareto%0Afrontier%2C%20offering%20a%20resource-efficient%20algorithm%20that%20is%20practical%20in%0Ascenarios%20with%20limited%20experimental%20budgets%20while%20ensuring%20efficient%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16476v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNOSTRA%253A%2520A%2520noise-resilient%2520and%2520sparse%2520data%2520framework%2520for%2520trust%2520region%250A%2520%2520based%2520multi%2520objective%2520Bayesian%2520optimization%26entry.906535625%3DMaryam%2520Ghasemzadeh%2520and%2520Anton%2520van%2520Beek%26entry.1292438233%3D%2520%2520Multi-objective%2520Bayesian%2520optimization%2520%2528MOBO%2529%2520struggles%2520with%2520sparse%250A%2528non-space-filling%2529%252C%2520scarce%2520%2528limited%2520observations%2529%2520datasets%2520affected%2520by%250Aexperimental%2520uncertainty%252C%2520where%2520identical%2520inputs%2520can%2520yield%2520varying%2520outputs.%250AThese%2520challenges%2520are%2520common%2520in%2520physical%2520and%2520simulation%2520experiments%2520%2528e.g.%252C%250Arandomized%2520medical%2520trials%2520and%252C%2520molecular%2520dynamics%2520simulations%2529%2520and%2520are%250Atherefore%2520incompatible%2520with%2520conventional%2520MOBO%2520methods.%2520As%2520a%2520result%252C%250Aexperimental%2520resources%2520are%2520inefficiently%2520allocated%252C%2520leading%2520to%2520suboptimal%250Adesigns.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520NOSTRA%2520%2528Noisy%2520and%2520Sparse%2520Data%250ATrust%2520Region-based%2520Optimization%2520Algorithm%2529%252C%2520a%2520novel%2520sampling%2520framework%2520that%250Aintegrates%2520prior%2520knowledge%2520of%2520experimental%2520uncertainty%2520to%2520construct%2520more%250Aaccurate%2520surrogate%2520models%2520while%2520employing%2520trust%2520regions%2520to%2520focus%2520sampling%2520on%250Apromising%2520areas%2520of%2520the%2520design%2520space.%2520By%2520strategically%2520leveraging%2520prior%250Ainformation%2520and%2520refining%2520search%2520regions%252C%2520NOSTRA%2520accelerates%2520convergence%2520to%2520the%250APareto%2520frontier%252C%2520enhances%2520data%2520efficiency%252C%2520and%2520improves%2520solution%2520quality.%250AThrough%2520two%2520test%2520functions%2520with%2520varying%2520levels%2520of%2520experimental%2520uncertainty%252C%2520we%250Ademonstrate%2520that%2520NOSTRA%2520outperforms%2520existing%2520methods%2520in%2520handling%2520noisy%252C%2520sparse%252C%250Aand%2520scarce%2520data.%2520Specifically%252C%2520we%2520illustrate%2520that%252C%2520NOSTRA%2520effectively%250Aprioritizes%2520regions%2520where%2520samples%2520enhance%2520the%2520accuracy%2520of%2520the%2520identified%2520Pareto%250Afrontier%252C%2520offering%2520a%2520resource-efficient%2520algorithm%2520that%2520is%2520practical%2520in%250Ascenarios%2520with%2520limited%2520experimental%2520budgets%2520while%2520ensuring%2520efficient%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16476v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NOSTRA%3A%20A%20noise-resilient%20and%20sparse%20data%20framework%20for%20trust%20region%0A%20%20based%20multi%20objective%20Bayesian%20optimization&entry.906535625=Maryam%20Ghasemzadeh%20and%20Anton%20van%20Beek&entry.1292438233=%20%20Multi-objective%20Bayesian%20optimization%20%28MOBO%29%20struggles%20with%20sparse%0A%28non-space-filling%29%2C%20scarce%20%28limited%20observations%29%20datasets%20affected%20by%0Aexperimental%20uncertainty%2C%20where%20identical%20inputs%20can%20yield%20varying%20outputs.%0AThese%20challenges%20are%20common%20in%20physical%20and%20simulation%20experiments%20%28e.g.%2C%0Arandomized%20medical%20trials%20and%2C%20molecular%20dynamics%20simulations%29%20and%20are%0Atherefore%20incompatible%20with%20conventional%20MOBO%20methods.%20As%20a%20result%2C%0Aexperimental%20resources%20are%20inefficiently%20allocated%2C%20leading%20to%20suboptimal%0Adesigns.%20To%20address%20this%20challenge%2C%20we%20introduce%20NOSTRA%20%28Noisy%20and%20Sparse%20Data%0ATrust%20Region-based%20Optimization%20Algorithm%29%2C%20a%20novel%20sampling%20framework%20that%0Aintegrates%20prior%20knowledge%20of%20experimental%20uncertainty%20to%20construct%20more%0Aaccurate%20surrogate%20models%20while%20employing%20trust%20regions%20to%20focus%20sampling%20on%0Apromising%20areas%20of%20the%20design%20space.%20By%20strategically%20leveraging%20prior%0Ainformation%20and%20refining%20search%20regions%2C%20NOSTRA%20accelerates%20convergence%20to%20the%0APareto%20frontier%2C%20enhances%20data%20efficiency%2C%20and%20improves%20solution%20quality.%0AThrough%20two%20test%20functions%20with%20varying%20levels%20of%20experimental%20uncertainty%2C%20we%0Ademonstrate%20that%20NOSTRA%20outperforms%20existing%20methods%20in%20handling%20noisy%2C%20sparse%2C%0Aand%20scarce%20data.%20Specifically%2C%20we%20illustrate%20that%2C%20NOSTRA%20effectively%0Aprioritizes%20regions%20where%20samples%20enhance%20the%20accuracy%20of%20the%20identified%20Pareto%0Afrontier%2C%20offering%20a%20resource-efficient%20algorithm%20that%20is%20practical%20in%0Ascenarios%20with%20limited%20experimental%20budgets%20while%20ensuring%20efficient%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16476v1&entry.124074799=Read"},
{"title": "Can Hallucinations Help? Boosting LLMs for Drug Discovery", "author": "Shuzhou Yuan and Zhan Qu and Ashish Yashwanth Kangen and Michael F\u00e4rber", "abstract": "  Hallucinations in large language models (LLMs), plausible but factually\ninaccurate text, are often viewed as undesirable. However, recent work suggests\nthat such outputs may hold creative potential. In this paper, we investigate\nwhether hallucinations can improve LLMs on molecule property prediction, a key\ntask in early-stage drug discovery. We prompt LLMs to generate natural language\ndescriptions from molecular SMILES strings and incorporate these often\nhallucinated descriptions into downstream classification tasks. Evaluating\nseven instruction-tuned LLMs across five datasets, we find that hallucinations\nsignificantly improve predictive accuracy for some models. Notably,\nFalcon3-Mamba-7B outperforms all baselines when hallucinated text is included,\nwhile hallucinations generated by GPT-4o consistently yield the greatest gains\nbetween models. We further identify and categorize over 18,000 beneficial\nhallucinations, with structural misdescriptions emerging as the most impactful\ntype, suggesting that hallucinated statements about molecular structure may\nincrease model confidence. Ablation studies show that larger models benefit\nmore from hallucinations, while temperature has a limited effect. Our findings\nchallenge conventional views of hallucination as purely problematic and suggest\nnew directions for leveraging hallucinations as a useful signal in scientific\nmodeling tasks like drug discovery.\n", "link": "http://arxiv.org/abs/2501.13824v2", "date": "2025-08-22", "relevancy": 1.9481, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4881}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4881}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4816}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Hallucinations%20Help%3F%20Boosting%20LLMs%20for%20Drug%20Discovery&body=Title%3A%20Can%20Hallucinations%20Help%3F%20Boosting%20LLMs%20for%20Drug%20Discovery%0AAuthor%3A%20Shuzhou%20Yuan%20and%20Zhan%20Qu%20and%20Ashish%20Yashwanth%20Kangen%20and%20Michael%20F%C3%A4rber%0AAbstract%3A%20%20%20Hallucinations%20in%20large%20language%20models%20%28LLMs%29%2C%20plausible%20but%20factually%0Ainaccurate%20text%2C%20are%20often%20viewed%20as%20undesirable.%20However%2C%20recent%20work%20suggests%0Athat%20such%20outputs%20may%20hold%20creative%20potential.%20In%20this%20paper%2C%20we%20investigate%0Awhether%20hallucinations%20can%20improve%20LLMs%20on%20molecule%20property%20prediction%2C%20a%20key%0Atask%20in%20early-stage%20drug%20discovery.%20We%20prompt%20LLMs%20to%20generate%20natural%20language%0Adescriptions%20from%20molecular%20SMILES%20strings%20and%20incorporate%20these%20often%0Ahallucinated%20descriptions%20into%20downstream%20classification%20tasks.%20Evaluating%0Aseven%20instruction-tuned%20LLMs%20across%20five%20datasets%2C%20we%20find%20that%20hallucinations%0Asignificantly%20improve%20predictive%20accuracy%20for%20some%20models.%20Notably%2C%0AFalcon3-Mamba-7B%20outperforms%20all%20baselines%20when%20hallucinated%20text%20is%20included%2C%0Awhile%20hallucinations%20generated%20by%20GPT-4o%20consistently%20yield%20the%20greatest%20gains%0Abetween%20models.%20We%20further%20identify%20and%20categorize%20over%2018%2C000%20beneficial%0Ahallucinations%2C%20with%20structural%20misdescriptions%20emerging%20as%20the%20most%20impactful%0Atype%2C%20suggesting%20that%20hallucinated%20statements%20about%20molecular%20structure%20may%0Aincrease%20model%20confidence.%20Ablation%20studies%20show%20that%20larger%20models%20benefit%0Amore%20from%20hallucinations%2C%20while%20temperature%20has%20a%20limited%20effect.%20Our%20findings%0Achallenge%20conventional%20views%20of%20hallucination%20as%20purely%20problematic%20and%20suggest%0Anew%20directions%20for%20leveraging%20hallucinations%20as%20a%20useful%20signal%20in%20scientific%0Amodeling%20tasks%20like%20drug%20discovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13824v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Hallucinations%2520Help%253F%2520Boosting%2520LLMs%2520for%2520Drug%2520Discovery%26entry.906535625%3DShuzhou%2520Yuan%2520and%2520Zhan%2520Qu%2520and%2520Ashish%2520Yashwanth%2520Kangen%2520and%2520Michael%2520F%25C3%25A4rber%26entry.1292438233%3D%2520%2520Hallucinations%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520plausible%2520but%2520factually%250Ainaccurate%2520text%252C%2520are%2520often%2520viewed%2520as%2520undesirable.%2520However%252C%2520recent%2520work%2520suggests%250Athat%2520such%2520outputs%2520may%2520hold%2520creative%2520potential.%2520In%2520this%2520paper%252C%2520we%2520investigate%250Awhether%2520hallucinations%2520can%2520improve%2520LLMs%2520on%2520molecule%2520property%2520prediction%252C%2520a%2520key%250Atask%2520in%2520early-stage%2520drug%2520discovery.%2520We%2520prompt%2520LLMs%2520to%2520generate%2520natural%2520language%250Adescriptions%2520from%2520molecular%2520SMILES%2520strings%2520and%2520incorporate%2520these%2520often%250Ahallucinated%2520descriptions%2520into%2520downstream%2520classification%2520tasks.%2520Evaluating%250Aseven%2520instruction-tuned%2520LLMs%2520across%2520five%2520datasets%252C%2520we%2520find%2520that%2520hallucinations%250Asignificantly%2520improve%2520predictive%2520accuracy%2520for%2520some%2520models.%2520Notably%252C%250AFalcon3-Mamba-7B%2520outperforms%2520all%2520baselines%2520when%2520hallucinated%2520text%2520is%2520included%252C%250Awhile%2520hallucinations%2520generated%2520by%2520GPT-4o%2520consistently%2520yield%2520the%2520greatest%2520gains%250Abetween%2520models.%2520We%2520further%2520identify%2520and%2520categorize%2520over%252018%252C000%2520beneficial%250Ahallucinations%252C%2520with%2520structural%2520misdescriptions%2520emerging%2520as%2520the%2520most%2520impactful%250Atype%252C%2520suggesting%2520that%2520hallucinated%2520statements%2520about%2520molecular%2520structure%2520may%250Aincrease%2520model%2520confidence.%2520Ablation%2520studies%2520show%2520that%2520larger%2520models%2520benefit%250Amore%2520from%2520hallucinations%252C%2520while%2520temperature%2520has%2520a%2520limited%2520effect.%2520Our%2520findings%250Achallenge%2520conventional%2520views%2520of%2520hallucination%2520as%2520purely%2520problematic%2520and%2520suggest%250Anew%2520directions%2520for%2520leveraging%2520hallucinations%2520as%2520a%2520useful%2520signal%2520in%2520scientific%250Amodeling%2520tasks%2520like%2520drug%2520discovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13824v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Hallucinations%20Help%3F%20Boosting%20LLMs%20for%20Drug%20Discovery&entry.906535625=Shuzhou%20Yuan%20and%20Zhan%20Qu%20and%20Ashish%20Yashwanth%20Kangen%20and%20Michael%20F%C3%A4rber&entry.1292438233=%20%20Hallucinations%20in%20large%20language%20models%20%28LLMs%29%2C%20plausible%20but%20factually%0Ainaccurate%20text%2C%20are%20often%20viewed%20as%20undesirable.%20However%2C%20recent%20work%20suggests%0Athat%20such%20outputs%20may%20hold%20creative%20potential.%20In%20this%20paper%2C%20we%20investigate%0Awhether%20hallucinations%20can%20improve%20LLMs%20on%20molecule%20property%20prediction%2C%20a%20key%0Atask%20in%20early-stage%20drug%20discovery.%20We%20prompt%20LLMs%20to%20generate%20natural%20language%0Adescriptions%20from%20molecular%20SMILES%20strings%20and%20incorporate%20these%20often%0Ahallucinated%20descriptions%20into%20downstream%20classification%20tasks.%20Evaluating%0Aseven%20instruction-tuned%20LLMs%20across%20five%20datasets%2C%20we%20find%20that%20hallucinations%0Asignificantly%20improve%20predictive%20accuracy%20for%20some%20models.%20Notably%2C%0AFalcon3-Mamba-7B%20outperforms%20all%20baselines%20when%20hallucinated%20text%20is%20included%2C%0Awhile%20hallucinations%20generated%20by%20GPT-4o%20consistently%20yield%20the%20greatest%20gains%0Abetween%20models.%20We%20further%20identify%20and%20categorize%20over%2018%2C000%20beneficial%0Ahallucinations%2C%20with%20structural%20misdescriptions%20emerging%20as%20the%20most%20impactful%0Atype%2C%20suggesting%20that%20hallucinated%20statements%20about%20molecular%20structure%20may%0Aincrease%20model%20confidence.%20Ablation%20studies%20show%20that%20larger%20models%20benefit%0Amore%20from%20hallucinations%2C%20while%20temperature%20has%20a%20limited%20effect.%20Our%20findings%0Achallenge%20conventional%20views%20of%20hallucination%20as%20purely%20problematic%20and%20suggest%0Anew%20directions%20for%20leveraging%20hallucinations%20as%20a%20useful%20signal%20in%20scientific%0Amodeling%20tasks%20like%20drug%20discovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13824v2&entry.124074799=Read"},
{"title": "Robust Small Methane Plume Segmentation in Satellite Imagery", "author": "Khai Duc Minh Tran and Hoa Van Nguyen and Aimuni Binti Muhammad Rawi and Hareeshrao Athinarayanarao and Ba-Ngu Vo", "abstract": "  This paper tackles the challenging problem of detecting methane plumes, a\npotent greenhouse gas, using Sentinel-2 imagery. This contributes to the\nmitigation of rapid climate change. We propose a novel deep learning solution\nbased on U-Net with a ResNet34 encoder, integrating dual spectral enhancement\ntechniques (Varon ratio and Sanchez regression) to optimise input features for\nheightened sensitivity. A key achievement is the ability to detect small plumes\ndown to 400 m2 (i.e., for a single pixel at 20 m resolution), surpassing\ntraditional methods limited to larger plumes. Experiments show our approach\nachieves a 78.39% F1-score on the validation set, demonstrating superior\nperformance in sensitivity and precision over existing remote sensing\ntechniques for automated methane monitoring, especially for small plumes.\n", "link": "http://arxiv.org/abs/2508.16282v1", "date": "2025-08-22", "relevancy": 1.9343, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5116}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4645}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Small%20Methane%20Plume%20Segmentation%20in%20Satellite%20Imagery&body=Title%3A%20Robust%20Small%20Methane%20Plume%20Segmentation%20in%20Satellite%20Imagery%0AAuthor%3A%20Khai%20Duc%20Minh%20Tran%20and%20Hoa%20Van%20Nguyen%20and%20Aimuni%20Binti%20Muhammad%20Rawi%20and%20Hareeshrao%20Athinarayanarao%20and%20Ba-Ngu%20Vo%0AAbstract%3A%20%20%20This%20paper%20tackles%20the%20challenging%20problem%20of%20detecting%20methane%20plumes%2C%20a%0Apotent%20greenhouse%20gas%2C%20using%20Sentinel-2%20imagery.%20This%20contributes%20to%20the%0Amitigation%20of%20rapid%20climate%20change.%20We%20propose%20a%20novel%20deep%20learning%20solution%0Abased%20on%20U-Net%20with%20a%20ResNet34%20encoder%2C%20integrating%20dual%20spectral%20enhancement%0Atechniques%20%28Varon%20ratio%20and%20Sanchez%20regression%29%20to%20optimise%20input%20features%20for%0Aheightened%20sensitivity.%20A%20key%20achievement%20is%20the%20ability%20to%20detect%20small%20plumes%0Adown%20to%20400%20m2%20%28i.e.%2C%20for%20a%20single%20pixel%20at%2020%20m%20resolution%29%2C%20surpassing%0Atraditional%20methods%20limited%20to%20larger%20plumes.%20Experiments%20show%20our%20approach%0Aachieves%20a%2078.39%25%20F1-score%20on%20the%20validation%20set%2C%20demonstrating%20superior%0Aperformance%20in%20sensitivity%20and%20precision%20over%20existing%20remote%20sensing%0Atechniques%20for%20automated%20methane%20monitoring%2C%20especially%20for%20small%20plumes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16282v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Small%2520Methane%2520Plume%2520Segmentation%2520in%2520Satellite%2520Imagery%26entry.906535625%3DKhai%2520Duc%2520Minh%2520Tran%2520and%2520Hoa%2520Van%2520Nguyen%2520and%2520Aimuni%2520Binti%2520Muhammad%2520Rawi%2520and%2520Hareeshrao%2520Athinarayanarao%2520and%2520Ba-Ngu%2520Vo%26entry.1292438233%3D%2520%2520This%2520paper%2520tackles%2520the%2520challenging%2520problem%2520of%2520detecting%2520methane%2520plumes%252C%2520a%250Apotent%2520greenhouse%2520gas%252C%2520using%2520Sentinel-2%2520imagery.%2520This%2520contributes%2520to%2520the%250Amitigation%2520of%2520rapid%2520climate%2520change.%2520We%2520propose%2520a%2520novel%2520deep%2520learning%2520solution%250Abased%2520on%2520U-Net%2520with%2520a%2520ResNet34%2520encoder%252C%2520integrating%2520dual%2520spectral%2520enhancement%250Atechniques%2520%2528Varon%2520ratio%2520and%2520Sanchez%2520regression%2529%2520to%2520optimise%2520input%2520features%2520for%250Aheightened%2520sensitivity.%2520A%2520key%2520achievement%2520is%2520the%2520ability%2520to%2520detect%2520small%2520plumes%250Adown%2520to%2520400%2520m2%2520%2528i.e.%252C%2520for%2520a%2520single%2520pixel%2520at%252020%2520m%2520resolution%2529%252C%2520surpassing%250Atraditional%2520methods%2520limited%2520to%2520larger%2520plumes.%2520Experiments%2520show%2520our%2520approach%250Aachieves%2520a%252078.39%2525%2520F1-score%2520on%2520the%2520validation%2520set%252C%2520demonstrating%2520superior%250Aperformance%2520in%2520sensitivity%2520and%2520precision%2520over%2520existing%2520remote%2520sensing%250Atechniques%2520for%2520automated%2520methane%2520monitoring%252C%2520especially%2520for%2520small%2520plumes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16282v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Small%20Methane%20Plume%20Segmentation%20in%20Satellite%20Imagery&entry.906535625=Khai%20Duc%20Minh%20Tran%20and%20Hoa%20Van%20Nguyen%20and%20Aimuni%20Binti%20Muhammad%20Rawi%20and%20Hareeshrao%20Athinarayanarao%20and%20Ba-Ngu%20Vo&entry.1292438233=%20%20This%20paper%20tackles%20the%20challenging%20problem%20of%20detecting%20methane%20plumes%2C%20a%0Apotent%20greenhouse%20gas%2C%20using%20Sentinel-2%20imagery.%20This%20contributes%20to%20the%0Amitigation%20of%20rapid%20climate%20change.%20We%20propose%20a%20novel%20deep%20learning%20solution%0Abased%20on%20U-Net%20with%20a%20ResNet34%20encoder%2C%20integrating%20dual%20spectral%20enhancement%0Atechniques%20%28Varon%20ratio%20and%20Sanchez%20regression%29%20to%20optimise%20input%20features%20for%0Aheightened%20sensitivity.%20A%20key%20achievement%20is%20the%20ability%20to%20detect%20small%20plumes%0Adown%20to%20400%20m2%20%28i.e.%2C%20for%20a%20single%20pixel%20at%2020%20m%20resolution%29%2C%20surpassing%0Atraditional%20methods%20limited%20to%20larger%20plumes.%20Experiments%20show%20our%20approach%0Aachieves%20a%2078.39%25%20F1-score%20on%20the%20validation%20set%2C%20demonstrating%20superior%0Aperformance%20in%20sensitivity%20and%20precision%20over%20existing%20remote%20sensing%0Atechniques%20for%20automated%20methane%20monitoring%2C%20especially%20for%20small%20plumes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16282v1&entry.124074799=Read"},
{"title": "Decoding MGMT Methylation: A Step Towards Precision Medicine in\n  Glioblastoma", "author": "Hafeez Ur Rehman and Sumaiya Fazal and Moutaz Alazab and Ali Baydoun", "abstract": "  Glioblastomas, constituting over 50% of malignant brain tumors, are highly\naggressive brain tumors that pose substantial treatment challenges due to their\nrapid progression and resistance to standard therapies. The methylation status\nof the O-6-Methylguanine-DNA Methyltransferase (MGMT) gene is a critical\nbiomarker for predicting patient response to treatment, particularly with the\nalkylating agent temozolomide. However, accurately predicting MGMT methylation\nstatus using non-invasive imaging techniques remains challenging due to the\ncomplex and heterogeneous nature of glioblastomas, that includes, uneven\ncontrast, variability within lesions, and irregular enhancement patterns. This\nstudy introduces the Convolutional Autoencoders for MGMT Methylation Status\nPrediction (CAMP) framework, which is based on adaptive sparse penalties to\nenhance predictive accuracy. The CAMP framework operates in two phases: first,\ngenerating synthetic MRI slices through a tailored autoencoder that effectively\ncaptures and preserves intricate tissue and tumor structures across different\nMRI modalities; second, predicting MGMT methylation status using a\nconvolutional neural network enhanced by adaptive sparse penalties. The\nadaptive sparse penalty dynamically adjusts to variations in the data, such as\ncontrast differences and tumor locations in MR images. Our method excels in MRI\nimage synthesis, preserving brain tissue, fat, and individual tumor structures\nacross all MRI modalities. Validated on benchmark datasets, CAMP achieved an\naccuracy of 0.97, specificity of 0.98, and sensitivity of 0.97, significantly\noutperforming existing methods. These results demonstrate the potential of the\nCAMP framework to improve the interpretation of MRI data and contribute to more\npersonalized treatment strategies for glioblastoma patients.\n", "link": "http://arxiv.org/abs/2508.16424v1", "date": "2025-08-22", "relevancy": 1.9321, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4939}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4772}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoding%20MGMT%20Methylation%3A%20A%20Step%20Towards%20Precision%20Medicine%20in%0A%20%20Glioblastoma&body=Title%3A%20Decoding%20MGMT%20Methylation%3A%20A%20Step%20Towards%20Precision%20Medicine%20in%0A%20%20Glioblastoma%0AAuthor%3A%20Hafeez%20Ur%20Rehman%20and%20Sumaiya%20Fazal%20and%20Moutaz%20Alazab%20and%20Ali%20Baydoun%0AAbstract%3A%20%20%20Glioblastomas%2C%20constituting%20over%2050%25%20of%20malignant%20brain%20tumors%2C%20are%20highly%0Aaggressive%20brain%20tumors%20that%20pose%20substantial%20treatment%20challenges%20due%20to%20their%0Arapid%20progression%20and%20resistance%20to%20standard%20therapies.%20The%20methylation%20status%0Aof%20the%20O-6-Methylguanine-DNA%20Methyltransferase%20%28MGMT%29%20gene%20is%20a%20critical%0Abiomarker%20for%20predicting%20patient%20response%20to%20treatment%2C%20particularly%20with%20the%0Aalkylating%20agent%20temozolomide.%20However%2C%20accurately%20predicting%20MGMT%20methylation%0Astatus%20using%20non-invasive%20imaging%20techniques%20remains%20challenging%20due%20to%20the%0Acomplex%20and%20heterogeneous%20nature%20of%20glioblastomas%2C%20that%20includes%2C%20uneven%0Acontrast%2C%20variability%20within%20lesions%2C%20and%20irregular%20enhancement%20patterns.%20This%0Astudy%20introduces%20the%20Convolutional%20Autoencoders%20for%20MGMT%20Methylation%20Status%0APrediction%20%28CAMP%29%20framework%2C%20which%20is%20based%20on%20adaptive%20sparse%20penalties%20to%0Aenhance%20predictive%20accuracy.%20The%20CAMP%20framework%20operates%20in%20two%20phases%3A%20first%2C%0Agenerating%20synthetic%20MRI%20slices%20through%20a%20tailored%20autoencoder%20that%20effectively%0Acaptures%20and%20preserves%20intricate%20tissue%20and%20tumor%20structures%20across%20different%0AMRI%20modalities%3B%20second%2C%20predicting%20MGMT%20methylation%20status%20using%20a%0Aconvolutional%20neural%20network%20enhanced%20by%20adaptive%20sparse%20penalties.%20The%0Aadaptive%20sparse%20penalty%20dynamically%20adjusts%20to%20variations%20in%20the%20data%2C%20such%20as%0Acontrast%20differences%20and%20tumor%20locations%20in%20MR%20images.%20Our%20method%20excels%20in%20MRI%0Aimage%20synthesis%2C%20preserving%20brain%20tissue%2C%20fat%2C%20and%20individual%20tumor%20structures%0Aacross%20all%20MRI%20modalities.%20Validated%20on%20benchmark%20datasets%2C%20CAMP%20achieved%20an%0Aaccuracy%20of%200.97%2C%20specificity%20of%200.98%2C%20and%20sensitivity%20of%200.97%2C%20significantly%0Aoutperforming%20existing%20methods.%20These%20results%20demonstrate%20the%20potential%20of%20the%0ACAMP%20framework%20to%20improve%20the%20interpretation%20of%20MRI%20data%20and%20contribute%20to%20more%0Apersonalized%20treatment%20strategies%20for%20glioblastoma%20patients.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16424v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoding%2520MGMT%2520Methylation%253A%2520A%2520Step%2520Towards%2520Precision%2520Medicine%2520in%250A%2520%2520Glioblastoma%26entry.906535625%3DHafeez%2520Ur%2520Rehman%2520and%2520Sumaiya%2520Fazal%2520and%2520Moutaz%2520Alazab%2520and%2520Ali%2520Baydoun%26entry.1292438233%3D%2520%2520Glioblastomas%252C%2520constituting%2520over%252050%2525%2520of%2520malignant%2520brain%2520tumors%252C%2520are%2520highly%250Aaggressive%2520brain%2520tumors%2520that%2520pose%2520substantial%2520treatment%2520challenges%2520due%2520to%2520their%250Arapid%2520progression%2520and%2520resistance%2520to%2520standard%2520therapies.%2520The%2520methylation%2520status%250Aof%2520the%2520O-6-Methylguanine-DNA%2520Methyltransferase%2520%2528MGMT%2529%2520gene%2520is%2520a%2520critical%250Abiomarker%2520for%2520predicting%2520patient%2520response%2520to%2520treatment%252C%2520particularly%2520with%2520the%250Aalkylating%2520agent%2520temozolomide.%2520However%252C%2520accurately%2520predicting%2520MGMT%2520methylation%250Astatus%2520using%2520non-invasive%2520imaging%2520techniques%2520remains%2520challenging%2520due%2520to%2520the%250Acomplex%2520and%2520heterogeneous%2520nature%2520of%2520glioblastomas%252C%2520that%2520includes%252C%2520uneven%250Acontrast%252C%2520variability%2520within%2520lesions%252C%2520and%2520irregular%2520enhancement%2520patterns.%2520This%250Astudy%2520introduces%2520the%2520Convolutional%2520Autoencoders%2520for%2520MGMT%2520Methylation%2520Status%250APrediction%2520%2528CAMP%2529%2520framework%252C%2520which%2520is%2520based%2520on%2520adaptive%2520sparse%2520penalties%2520to%250Aenhance%2520predictive%2520accuracy.%2520The%2520CAMP%2520framework%2520operates%2520in%2520two%2520phases%253A%2520first%252C%250Agenerating%2520synthetic%2520MRI%2520slices%2520through%2520a%2520tailored%2520autoencoder%2520that%2520effectively%250Acaptures%2520and%2520preserves%2520intricate%2520tissue%2520and%2520tumor%2520structures%2520across%2520different%250AMRI%2520modalities%253B%2520second%252C%2520predicting%2520MGMT%2520methylation%2520status%2520using%2520a%250Aconvolutional%2520neural%2520network%2520enhanced%2520by%2520adaptive%2520sparse%2520penalties.%2520The%250Aadaptive%2520sparse%2520penalty%2520dynamically%2520adjusts%2520to%2520variations%2520in%2520the%2520data%252C%2520such%2520as%250Acontrast%2520differences%2520and%2520tumor%2520locations%2520in%2520MR%2520images.%2520Our%2520method%2520excels%2520in%2520MRI%250Aimage%2520synthesis%252C%2520preserving%2520brain%2520tissue%252C%2520fat%252C%2520and%2520individual%2520tumor%2520structures%250Aacross%2520all%2520MRI%2520modalities.%2520Validated%2520on%2520benchmark%2520datasets%252C%2520CAMP%2520achieved%2520an%250Aaccuracy%2520of%25200.97%252C%2520specificity%2520of%25200.98%252C%2520and%2520sensitivity%2520of%25200.97%252C%2520significantly%250Aoutperforming%2520existing%2520methods.%2520These%2520results%2520demonstrate%2520the%2520potential%2520of%2520the%250ACAMP%2520framework%2520to%2520improve%2520the%2520interpretation%2520of%2520MRI%2520data%2520and%2520contribute%2520to%2520more%250Apersonalized%2520treatment%2520strategies%2520for%2520glioblastoma%2520patients.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16424v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoding%20MGMT%20Methylation%3A%20A%20Step%20Towards%20Precision%20Medicine%20in%0A%20%20Glioblastoma&entry.906535625=Hafeez%20Ur%20Rehman%20and%20Sumaiya%20Fazal%20and%20Moutaz%20Alazab%20and%20Ali%20Baydoun&entry.1292438233=%20%20Glioblastomas%2C%20constituting%20over%2050%25%20of%20malignant%20brain%20tumors%2C%20are%20highly%0Aaggressive%20brain%20tumors%20that%20pose%20substantial%20treatment%20challenges%20due%20to%20their%0Arapid%20progression%20and%20resistance%20to%20standard%20therapies.%20The%20methylation%20status%0Aof%20the%20O-6-Methylguanine-DNA%20Methyltransferase%20%28MGMT%29%20gene%20is%20a%20critical%0Abiomarker%20for%20predicting%20patient%20response%20to%20treatment%2C%20particularly%20with%20the%0Aalkylating%20agent%20temozolomide.%20However%2C%20accurately%20predicting%20MGMT%20methylation%0Astatus%20using%20non-invasive%20imaging%20techniques%20remains%20challenging%20due%20to%20the%0Acomplex%20and%20heterogeneous%20nature%20of%20glioblastomas%2C%20that%20includes%2C%20uneven%0Acontrast%2C%20variability%20within%20lesions%2C%20and%20irregular%20enhancement%20patterns.%20This%0Astudy%20introduces%20the%20Convolutional%20Autoencoders%20for%20MGMT%20Methylation%20Status%0APrediction%20%28CAMP%29%20framework%2C%20which%20is%20based%20on%20adaptive%20sparse%20penalties%20to%0Aenhance%20predictive%20accuracy.%20The%20CAMP%20framework%20operates%20in%20two%20phases%3A%20first%2C%0Agenerating%20synthetic%20MRI%20slices%20through%20a%20tailored%20autoencoder%20that%20effectively%0Acaptures%20and%20preserves%20intricate%20tissue%20and%20tumor%20structures%20across%20different%0AMRI%20modalities%3B%20second%2C%20predicting%20MGMT%20methylation%20status%20using%20a%0Aconvolutional%20neural%20network%20enhanced%20by%20adaptive%20sparse%20penalties.%20The%0Aadaptive%20sparse%20penalty%20dynamically%20adjusts%20to%20variations%20in%20the%20data%2C%20such%20as%0Acontrast%20differences%20and%20tumor%20locations%20in%20MR%20images.%20Our%20method%20excels%20in%20MRI%0Aimage%20synthesis%2C%20preserving%20brain%20tissue%2C%20fat%2C%20and%20individual%20tumor%20structures%0Aacross%20all%20MRI%20modalities.%20Validated%20on%20benchmark%20datasets%2C%20CAMP%20achieved%20an%0Aaccuracy%20of%200.97%2C%20specificity%20of%200.98%2C%20and%20sensitivity%20of%200.97%2C%20significantly%0Aoutperforming%20existing%20methods.%20These%20results%20demonstrate%20the%20potential%20of%20the%0ACAMP%20framework%20to%20improve%20the%20interpretation%20of%20MRI%20data%20and%20contribute%20to%20more%0Apersonalized%20treatment%20strategies%20for%20glioblastoma%20patients.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16424v1&entry.124074799=Read"},
{"title": "Collaborative Stance Detection via Small-Large Language Model\n  Consistency Verification", "author": "Yu Yan and Sheng Sun and Zixiang Tang and Teli Liu and Min Liu", "abstract": "  Stance detection on social media aims to identify attitudes expressed in\ntweets towards specific targets. Current studies prioritize Large Language\nModels (LLMs) over Small Language Models (SLMs) due to the overwhelming\nperformance improving provided by LLMs. However, heavily relying on LLMs for\nstance detection, regardless of the cost, is impractical for real-world social\nmedia monitoring systems that require vast data analysis. To this end, we\npropose \\textbf{\\underline{Co}}llaborative Stance Detection via Small-Large\nLanguage Model Consistency \\textbf{\\underline{Ver}}ification (\\textbf{CoVer})\nframework, which enhances LLM utilization via context-shared batch reasoning\nand logical verification between LLM and SLM. Specifically, instead of\nprocessing each text individually, CoVer processes texts batch-by-batch,\nobtaining stance predictions and corresponding explanations via LLM reasoning\nin a shared context. Then, to exclude the bias caused by context noises, CoVer\nintroduces the SLM for logical consistency verification. Finally, texts that\nrepeatedly exhibit low logical consistency are classified using\nconsistency-weighted aggregation of prior LLM stance predictions. Our\nexperiments show that CoVer outperforms state-of-the-art methods across\nmultiple benchmarks in the zero-shot setting, achieving 0.54 LLM queries per\ntweet while significantly enhancing performance. Our CoVer offers a more\npractical solution for LLM deploying for social media stance detection.\n", "link": "http://arxiv.org/abs/2502.19954v2", "date": "2025-08-22", "relevancy": 1.9194, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4807}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4807}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Collaborative%20Stance%20Detection%20via%20Small-Large%20Language%20Model%0A%20%20Consistency%20Verification&body=Title%3A%20Collaborative%20Stance%20Detection%20via%20Small-Large%20Language%20Model%0A%20%20Consistency%20Verification%0AAuthor%3A%20Yu%20Yan%20and%20Sheng%20Sun%20and%20Zixiang%20Tang%20and%20Teli%20Liu%20and%20Min%20Liu%0AAbstract%3A%20%20%20Stance%20detection%20on%20social%20media%20aims%20to%20identify%20attitudes%20expressed%20in%0Atweets%20towards%20specific%20targets.%20Current%20studies%20prioritize%20Large%20Language%0AModels%20%28LLMs%29%20over%20Small%20Language%20Models%20%28SLMs%29%20due%20to%20the%20overwhelming%0Aperformance%20improving%20provided%20by%20LLMs.%20However%2C%20heavily%20relying%20on%20LLMs%20for%0Astance%20detection%2C%20regardless%20of%20the%20cost%2C%20is%20impractical%20for%20real-world%20social%0Amedia%20monitoring%20systems%20that%20require%20vast%20data%20analysis.%20To%20this%20end%2C%20we%0Apropose%20%5Ctextbf%7B%5Cunderline%7BCo%7D%7Dllaborative%20Stance%20Detection%20via%20Small-Large%0ALanguage%20Model%20Consistency%20%5Ctextbf%7B%5Cunderline%7BVer%7D%7Dification%20%28%5Ctextbf%7BCoVer%7D%29%0Aframework%2C%20which%20enhances%20LLM%20utilization%20via%20context-shared%20batch%20reasoning%0Aand%20logical%20verification%20between%20LLM%20and%20SLM.%20Specifically%2C%20instead%20of%0Aprocessing%20each%20text%20individually%2C%20CoVer%20processes%20texts%20batch-by-batch%2C%0Aobtaining%20stance%20predictions%20and%20corresponding%20explanations%20via%20LLM%20reasoning%0Ain%20a%20shared%20context.%20Then%2C%20to%20exclude%20the%20bias%20caused%20by%20context%20noises%2C%20CoVer%0Aintroduces%20the%20SLM%20for%20logical%20consistency%20verification.%20Finally%2C%20texts%20that%0Arepeatedly%20exhibit%20low%20logical%20consistency%20are%20classified%20using%0Aconsistency-weighted%20aggregation%20of%20prior%20LLM%20stance%20predictions.%20Our%0Aexperiments%20show%20that%20CoVer%20outperforms%20state-of-the-art%20methods%20across%0Amultiple%20benchmarks%20in%20the%20zero-shot%20setting%2C%20achieving%200.54%20LLM%20queries%20per%0Atweet%20while%20significantly%20enhancing%20performance.%20Our%20CoVer%20offers%20a%20more%0Apractical%20solution%20for%20LLM%20deploying%20for%20social%20media%20stance%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19954v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollaborative%2520Stance%2520Detection%2520via%2520Small-Large%2520Language%2520Model%250A%2520%2520Consistency%2520Verification%26entry.906535625%3DYu%2520Yan%2520and%2520Sheng%2520Sun%2520and%2520Zixiang%2520Tang%2520and%2520Teli%2520Liu%2520and%2520Min%2520Liu%26entry.1292438233%3D%2520%2520Stance%2520detection%2520on%2520social%2520media%2520aims%2520to%2520identify%2520attitudes%2520expressed%2520in%250Atweets%2520towards%2520specific%2520targets.%2520Current%2520studies%2520prioritize%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520over%2520Small%2520Language%2520Models%2520%2528SLMs%2529%2520due%2520to%2520the%2520overwhelming%250Aperformance%2520improving%2520provided%2520by%2520LLMs.%2520However%252C%2520heavily%2520relying%2520on%2520LLMs%2520for%250Astance%2520detection%252C%2520regardless%2520of%2520the%2520cost%252C%2520is%2520impractical%2520for%2520real-world%2520social%250Amedia%2520monitoring%2520systems%2520that%2520require%2520vast%2520data%2520analysis.%2520To%2520this%2520end%252C%2520we%250Apropose%2520%255Ctextbf%257B%255Cunderline%257BCo%257D%257Dllaborative%2520Stance%2520Detection%2520via%2520Small-Large%250ALanguage%2520Model%2520Consistency%2520%255Ctextbf%257B%255Cunderline%257BVer%257D%257Dification%2520%2528%255Ctextbf%257BCoVer%257D%2529%250Aframework%252C%2520which%2520enhances%2520LLM%2520utilization%2520via%2520context-shared%2520batch%2520reasoning%250Aand%2520logical%2520verification%2520between%2520LLM%2520and%2520SLM.%2520Specifically%252C%2520instead%2520of%250Aprocessing%2520each%2520text%2520individually%252C%2520CoVer%2520processes%2520texts%2520batch-by-batch%252C%250Aobtaining%2520stance%2520predictions%2520and%2520corresponding%2520explanations%2520via%2520LLM%2520reasoning%250Ain%2520a%2520shared%2520context.%2520Then%252C%2520to%2520exclude%2520the%2520bias%2520caused%2520by%2520context%2520noises%252C%2520CoVer%250Aintroduces%2520the%2520SLM%2520for%2520logical%2520consistency%2520verification.%2520Finally%252C%2520texts%2520that%250Arepeatedly%2520exhibit%2520low%2520logical%2520consistency%2520are%2520classified%2520using%250Aconsistency-weighted%2520aggregation%2520of%2520prior%2520LLM%2520stance%2520predictions.%2520Our%250Aexperiments%2520show%2520that%2520CoVer%2520outperforms%2520state-of-the-art%2520methods%2520across%250Amultiple%2520benchmarks%2520in%2520the%2520zero-shot%2520setting%252C%2520achieving%25200.54%2520LLM%2520queries%2520per%250Atweet%2520while%2520significantly%2520enhancing%2520performance.%2520Our%2520CoVer%2520offers%2520a%2520more%250Apractical%2520solution%2520for%2520LLM%2520deploying%2520for%2520social%2520media%2520stance%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19954v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collaborative%20Stance%20Detection%20via%20Small-Large%20Language%20Model%0A%20%20Consistency%20Verification&entry.906535625=Yu%20Yan%20and%20Sheng%20Sun%20and%20Zixiang%20Tang%20and%20Teli%20Liu%20and%20Min%20Liu&entry.1292438233=%20%20Stance%20detection%20on%20social%20media%20aims%20to%20identify%20attitudes%20expressed%20in%0Atweets%20towards%20specific%20targets.%20Current%20studies%20prioritize%20Large%20Language%0AModels%20%28LLMs%29%20over%20Small%20Language%20Models%20%28SLMs%29%20due%20to%20the%20overwhelming%0Aperformance%20improving%20provided%20by%20LLMs.%20However%2C%20heavily%20relying%20on%20LLMs%20for%0Astance%20detection%2C%20regardless%20of%20the%20cost%2C%20is%20impractical%20for%20real-world%20social%0Amedia%20monitoring%20systems%20that%20require%20vast%20data%20analysis.%20To%20this%20end%2C%20we%0Apropose%20%5Ctextbf%7B%5Cunderline%7BCo%7D%7Dllaborative%20Stance%20Detection%20via%20Small-Large%0ALanguage%20Model%20Consistency%20%5Ctextbf%7B%5Cunderline%7BVer%7D%7Dification%20%28%5Ctextbf%7BCoVer%7D%29%0Aframework%2C%20which%20enhances%20LLM%20utilization%20via%20context-shared%20batch%20reasoning%0Aand%20logical%20verification%20between%20LLM%20and%20SLM.%20Specifically%2C%20instead%20of%0Aprocessing%20each%20text%20individually%2C%20CoVer%20processes%20texts%20batch-by-batch%2C%0Aobtaining%20stance%20predictions%20and%20corresponding%20explanations%20via%20LLM%20reasoning%0Ain%20a%20shared%20context.%20Then%2C%20to%20exclude%20the%20bias%20caused%20by%20context%20noises%2C%20CoVer%0Aintroduces%20the%20SLM%20for%20logical%20consistency%20verification.%20Finally%2C%20texts%20that%0Arepeatedly%20exhibit%20low%20logical%20consistency%20are%20classified%20using%0Aconsistency-weighted%20aggregation%20of%20prior%20LLM%20stance%20predictions.%20Our%0Aexperiments%20show%20that%20CoVer%20outperforms%20state-of-the-art%20methods%20across%0Amultiple%20benchmarks%20in%20the%20zero-shot%20setting%2C%20achieving%200.54%20LLM%20queries%20per%0Atweet%20while%20significantly%20enhancing%20performance.%20Our%20CoVer%20offers%20a%20more%0Apractical%20solution%20for%20LLM%20deploying%20for%20social%20media%20stance%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19954v2&entry.124074799=Read"},
{"title": "LLM-GUARD: Large Language Model-Based Detection and Repair of Bugs and\n  Security Vulnerabilities in C++ and Python", "author": "Akshay Mhatre and Noujoud Nader and Patrick Diehl and Deepti Gupta", "abstract": "  Large Language Models (LLMs) such as ChatGPT-4, Claude 3, and LLaMA 4 are\nincreasingly embedded in software/application development, supporting tasks\nfrom code generation to debugging. Yet, their real-world effectiveness in\ndetecting diverse software bugs, particularly complex, security-relevant\nvulnerabilities, remains underexplored. This study presents a systematic,\nempirical evaluation of these three leading LLMs using a benchmark of\nfoundational programming errors, classic security flaws, and advanced,\nproduction-grade bugs in C++ and Python. The dataset integrates real code from\nSEED Labs, OpenSSL (via the Suresoft GLaDOS database), and PyBugHive, validated\nthrough local compilation and testing pipelines. A novel multi-stage,\ncontext-aware prompting protocol simulates realistic debugging scenarios, while\na graded rubric measures detection accuracy, reasoning depth, and remediation\nquality. Our results show that all models excel at identifying syntactic and\nsemantic issues in well-scoped code, making them promising for educational use\nand as first-pass reviewers in automated code auditing. Performance diminishes\nin scenarios involving complex security vulnerabilities and large-scale\nproduction code, with ChatGPT-4 and Claude 3 generally providing more nuanced\ncontextual analyses than LLaMA 4. This highlights both the promise and the\npresent constraints of LLMs in serving as reliable code analysis tools.\n", "link": "http://arxiv.org/abs/2508.16419v1", "date": "2025-08-22", "relevancy": 1.912, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4898}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4756}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-GUARD%3A%20Large%20Language%20Model-Based%20Detection%20and%20Repair%20of%20Bugs%20and%0A%20%20Security%20Vulnerabilities%20in%20C%2B%2B%20and%20Python&body=Title%3A%20LLM-GUARD%3A%20Large%20Language%20Model-Based%20Detection%20and%20Repair%20of%20Bugs%20and%0A%20%20Security%20Vulnerabilities%20in%20C%2B%2B%20and%20Python%0AAuthor%3A%20Akshay%20Mhatre%20and%20Noujoud%20Nader%20and%20Patrick%20Diehl%20and%20Deepti%20Gupta%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20such%20as%20ChatGPT-4%2C%20Claude%203%2C%20and%20LLaMA%204%20are%0Aincreasingly%20embedded%20in%20software/application%20development%2C%20supporting%20tasks%0Afrom%20code%20generation%20to%20debugging.%20Yet%2C%20their%20real-world%20effectiveness%20in%0Adetecting%20diverse%20software%20bugs%2C%20particularly%20complex%2C%20security-relevant%0Avulnerabilities%2C%20remains%20underexplored.%20This%20study%20presents%20a%20systematic%2C%0Aempirical%20evaluation%20of%20these%20three%20leading%20LLMs%20using%20a%20benchmark%20of%0Afoundational%20programming%20errors%2C%20classic%20security%20flaws%2C%20and%20advanced%2C%0Aproduction-grade%20bugs%20in%20C%2B%2B%20and%20Python.%20The%20dataset%20integrates%20real%20code%20from%0ASEED%20Labs%2C%20OpenSSL%20%28via%20the%20Suresoft%20GLaDOS%20database%29%2C%20and%20PyBugHive%2C%20validated%0Athrough%20local%20compilation%20and%20testing%20pipelines.%20A%20novel%20multi-stage%2C%0Acontext-aware%20prompting%20protocol%20simulates%20realistic%20debugging%20scenarios%2C%20while%0Aa%20graded%20rubric%20measures%20detection%20accuracy%2C%20reasoning%20depth%2C%20and%20remediation%0Aquality.%20Our%20results%20show%20that%20all%20models%20excel%20at%20identifying%20syntactic%20and%0Asemantic%20issues%20in%20well-scoped%20code%2C%20making%20them%20promising%20for%20educational%20use%0Aand%20as%20first-pass%20reviewers%20in%20automated%20code%20auditing.%20Performance%20diminishes%0Ain%20scenarios%20involving%20complex%20security%20vulnerabilities%20and%20large-scale%0Aproduction%20code%2C%20with%20ChatGPT-4%20and%20Claude%203%20generally%20providing%20more%20nuanced%0Acontextual%20analyses%20than%20LLaMA%204.%20This%20highlights%20both%20the%20promise%20and%20the%0Apresent%20constraints%20of%20LLMs%20in%20serving%20as%20reliable%20code%20analysis%20tools.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16419v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-GUARD%253A%2520Large%2520Language%2520Model-Based%2520Detection%2520and%2520Repair%2520of%2520Bugs%2520and%250A%2520%2520Security%2520Vulnerabilities%2520in%2520C%252B%252B%2520and%2520Python%26entry.906535625%3DAkshay%2520Mhatre%2520and%2520Noujoud%2520Nader%2520and%2520Patrick%2520Diehl%2520and%2520Deepti%2520Gupta%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520such%2520as%2520ChatGPT-4%252C%2520Claude%25203%252C%2520and%2520LLaMA%25204%2520are%250Aincreasingly%2520embedded%2520in%2520software/application%2520development%252C%2520supporting%2520tasks%250Afrom%2520code%2520generation%2520to%2520debugging.%2520Yet%252C%2520their%2520real-world%2520effectiveness%2520in%250Adetecting%2520diverse%2520software%2520bugs%252C%2520particularly%2520complex%252C%2520security-relevant%250Avulnerabilities%252C%2520remains%2520underexplored.%2520This%2520study%2520presents%2520a%2520systematic%252C%250Aempirical%2520evaluation%2520of%2520these%2520three%2520leading%2520LLMs%2520using%2520a%2520benchmark%2520of%250Afoundational%2520programming%2520errors%252C%2520classic%2520security%2520flaws%252C%2520and%2520advanced%252C%250Aproduction-grade%2520bugs%2520in%2520C%252B%252B%2520and%2520Python.%2520The%2520dataset%2520integrates%2520real%2520code%2520from%250ASEED%2520Labs%252C%2520OpenSSL%2520%2528via%2520the%2520Suresoft%2520GLaDOS%2520database%2529%252C%2520and%2520PyBugHive%252C%2520validated%250Athrough%2520local%2520compilation%2520and%2520testing%2520pipelines.%2520A%2520novel%2520multi-stage%252C%250Acontext-aware%2520prompting%2520protocol%2520simulates%2520realistic%2520debugging%2520scenarios%252C%2520while%250Aa%2520graded%2520rubric%2520measures%2520detection%2520accuracy%252C%2520reasoning%2520depth%252C%2520and%2520remediation%250Aquality.%2520Our%2520results%2520show%2520that%2520all%2520models%2520excel%2520at%2520identifying%2520syntactic%2520and%250Asemantic%2520issues%2520in%2520well-scoped%2520code%252C%2520making%2520them%2520promising%2520for%2520educational%2520use%250Aand%2520as%2520first-pass%2520reviewers%2520in%2520automated%2520code%2520auditing.%2520Performance%2520diminishes%250Ain%2520scenarios%2520involving%2520complex%2520security%2520vulnerabilities%2520and%2520large-scale%250Aproduction%2520code%252C%2520with%2520ChatGPT-4%2520and%2520Claude%25203%2520generally%2520providing%2520more%2520nuanced%250Acontextual%2520analyses%2520than%2520LLaMA%25204.%2520This%2520highlights%2520both%2520the%2520promise%2520and%2520the%250Apresent%2520constraints%2520of%2520LLMs%2520in%2520serving%2520as%2520reliable%2520code%2520analysis%2520tools.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16419v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-GUARD%3A%20Large%20Language%20Model-Based%20Detection%20and%20Repair%20of%20Bugs%20and%0A%20%20Security%20Vulnerabilities%20in%20C%2B%2B%20and%20Python&entry.906535625=Akshay%20Mhatre%20and%20Noujoud%20Nader%20and%20Patrick%20Diehl%20and%20Deepti%20Gupta&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20such%20as%20ChatGPT-4%2C%20Claude%203%2C%20and%20LLaMA%204%20are%0Aincreasingly%20embedded%20in%20software/application%20development%2C%20supporting%20tasks%0Afrom%20code%20generation%20to%20debugging.%20Yet%2C%20their%20real-world%20effectiveness%20in%0Adetecting%20diverse%20software%20bugs%2C%20particularly%20complex%2C%20security-relevant%0Avulnerabilities%2C%20remains%20underexplored.%20This%20study%20presents%20a%20systematic%2C%0Aempirical%20evaluation%20of%20these%20three%20leading%20LLMs%20using%20a%20benchmark%20of%0Afoundational%20programming%20errors%2C%20classic%20security%20flaws%2C%20and%20advanced%2C%0Aproduction-grade%20bugs%20in%20C%2B%2B%20and%20Python.%20The%20dataset%20integrates%20real%20code%20from%0ASEED%20Labs%2C%20OpenSSL%20%28via%20the%20Suresoft%20GLaDOS%20database%29%2C%20and%20PyBugHive%2C%20validated%0Athrough%20local%20compilation%20and%20testing%20pipelines.%20A%20novel%20multi-stage%2C%0Acontext-aware%20prompting%20protocol%20simulates%20realistic%20debugging%20scenarios%2C%20while%0Aa%20graded%20rubric%20measures%20detection%20accuracy%2C%20reasoning%20depth%2C%20and%20remediation%0Aquality.%20Our%20results%20show%20that%20all%20models%20excel%20at%20identifying%20syntactic%20and%0Asemantic%20issues%20in%20well-scoped%20code%2C%20making%20them%20promising%20for%20educational%20use%0Aand%20as%20first-pass%20reviewers%20in%20automated%20code%20auditing.%20Performance%20diminishes%0Ain%20scenarios%20involving%20complex%20security%20vulnerabilities%20and%20large-scale%0Aproduction%20code%2C%20with%20ChatGPT-4%20and%20Claude%203%20generally%20providing%20more%20nuanced%0Acontextual%20analyses%20than%20LLaMA%204.%20This%20highlights%20both%20the%20promise%20and%20the%0Apresent%20constraints%20of%20LLMs%20in%20serving%20as%20reliable%20code%20analysis%20tools.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16419v1&entry.124074799=Read"},
{"title": "On Zero-Shot Reinforcement Learning", "author": "Scott Jeen", "abstract": "  Modern reinforcement learning (RL) systems capture deep truths about general,\nhuman problem-solving. In domains where new data can be simulated cheaply,\nthese systems uncover sequential decision-making policies that far exceed the\nability of any human. Society faces many problems whose solutions require this\nskill, but they are often in domains where new data cannot be cheaply\nsimulated. In such scenarios, we can learn simulators from existing data, but\nthese will only ever be approximately correct, and can be pathologically\nincorrect when queried outside of their training distribution. As a result, a\nmisalignment between the environments in which we train our agents and the\nreal-world in which we wish to deploy our agents is inevitable. Dealing with\nthis misalignment is the primary concern of zero-shot reinforcement learning, a\nproblem setting where the agent must generalise to a new task or domain with\nzero practice shots. Whilst impressive progress has been made on methods that\nperform zero-shot RL in idealised settings, new work is needed if these results\nare to be replicated in real-world settings. In this thesis, we argue that\ndoing so requires us to navigate (at least) three constraints. First, the data\nquality constraint: real-world datasets are small and homogeneous. Second, the\nobservability constraint: states, dynamics and rewards in the real-world are\noften only partially observed. And third, the data availability constraint: a\npriori access to data cannot always be assumed. This work proposes a suite of\nmethods that perform zero-shot RL subject to these constraints. In a series of\nempirical studies we expose the failings of existing methods, and justify our\ntechniques for remedying them. We believe these designs take us a step closer\nto RL methods that can be deployed to solve real-world problems.\n", "link": "http://arxiv.org/abs/2508.16496v1", "date": "2025-08-22", "relevancy": 1.9115, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4901}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4731}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4675}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Zero-Shot%20Reinforcement%20Learning&body=Title%3A%20On%20Zero-Shot%20Reinforcement%20Learning%0AAuthor%3A%20Scott%20Jeen%0AAbstract%3A%20%20%20Modern%20reinforcement%20learning%20%28RL%29%20systems%20capture%20deep%20truths%20about%20general%2C%0Ahuman%20problem-solving.%20In%20domains%20where%20new%20data%20can%20be%20simulated%20cheaply%2C%0Athese%20systems%20uncover%20sequential%20decision-making%20policies%20that%20far%20exceed%20the%0Aability%20of%20any%20human.%20Society%20faces%20many%20problems%20whose%20solutions%20require%20this%0Askill%2C%20but%20they%20are%20often%20in%20domains%20where%20new%20data%20cannot%20be%20cheaply%0Asimulated.%20In%20such%20scenarios%2C%20we%20can%20learn%20simulators%20from%20existing%20data%2C%20but%0Athese%20will%20only%20ever%20be%20approximately%20correct%2C%20and%20can%20be%20pathologically%0Aincorrect%20when%20queried%20outside%20of%20their%20training%20distribution.%20As%20a%20result%2C%20a%0Amisalignment%20between%20the%20environments%20in%20which%20we%20train%20our%20agents%20and%20the%0Areal-world%20in%20which%20we%20wish%20to%20deploy%20our%20agents%20is%20inevitable.%20Dealing%20with%0Athis%20misalignment%20is%20the%20primary%20concern%20of%20zero-shot%20reinforcement%20learning%2C%20a%0Aproblem%20setting%20where%20the%20agent%20must%20generalise%20to%20a%20new%20task%20or%20domain%20with%0Azero%20practice%20shots.%20Whilst%20impressive%20progress%20has%20been%20made%20on%20methods%20that%0Aperform%20zero-shot%20RL%20in%20idealised%20settings%2C%20new%20work%20is%20needed%20if%20these%20results%0Aare%20to%20be%20replicated%20in%20real-world%20settings.%20In%20this%20thesis%2C%20we%20argue%20that%0Adoing%20so%20requires%20us%20to%20navigate%20%28at%20least%29%20three%20constraints.%20First%2C%20the%20data%0Aquality%20constraint%3A%20real-world%20datasets%20are%20small%20and%20homogeneous.%20Second%2C%20the%0Aobservability%20constraint%3A%20states%2C%20dynamics%20and%20rewards%20in%20the%20real-world%20are%0Aoften%20only%20partially%20observed.%20And%20third%2C%20the%20data%20availability%20constraint%3A%20a%0Apriori%20access%20to%20data%20cannot%20always%20be%20assumed.%20This%20work%20proposes%20a%20suite%20of%0Amethods%20that%20perform%20zero-shot%20RL%20subject%20to%20these%20constraints.%20In%20a%20series%20of%0Aempirical%20studies%20we%20expose%20the%20failings%20of%20existing%20methods%2C%20and%20justify%20our%0Atechniques%20for%20remedying%20them.%20We%20believe%20these%20designs%20take%20us%20a%20step%20closer%0Ato%20RL%20methods%20that%20can%20be%20deployed%20to%20solve%20real-world%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16496v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Zero-Shot%2520Reinforcement%2520Learning%26entry.906535625%3DScott%2520Jeen%26entry.1292438233%3D%2520%2520Modern%2520reinforcement%2520learning%2520%2528RL%2529%2520systems%2520capture%2520deep%2520truths%2520about%2520general%252C%250Ahuman%2520problem-solving.%2520In%2520domains%2520where%2520new%2520data%2520can%2520be%2520simulated%2520cheaply%252C%250Athese%2520systems%2520uncover%2520sequential%2520decision-making%2520policies%2520that%2520far%2520exceed%2520the%250Aability%2520of%2520any%2520human.%2520Society%2520faces%2520many%2520problems%2520whose%2520solutions%2520require%2520this%250Askill%252C%2520but%2520they%2520are%2520often%2520in%2520domains%2520where%2520new%2520data%2520cannot%2520be%2520cheaply%250Asimulated.%2520In%2520such%2520scenarios%252C%2520we%2520can%2520learn%2520simulators%2520from%2520existing%2520data%252C%2520but%250Athese%2520will%2520only%2520ever%2520be%2520approximately%2520correct%252C%2520and%2520can%2520be%2520pathologically%250Aincorrect%2520when%2520queried%2520outside%2520of%2520their%2520training%2520distribution.%2520As%2520a%2520result%252C%2520a%250Amisalignment%2520between%2520the%2520environments%2520in%2520which%2520we%2520train%2520our%2520agents%2520and%2520the%250Areal-world%2520in%2520which%2520we%2520wish%2520to%2520deploy%2520our%2520agents%2520is%2520inevitable.%2520Dealing%2520with%250Athis%2520misalignment%2520is%2520the%2520primary%2520concern%2520of%2520zero-shot%2520reinforcement%2520learning%252C%2520a%250Aproblem%2520setting%2520where%2520the%2520agent%2520must%2520generalise%2520to%2520a%2520new%2520task%2520or%2520domain%2520with%250Azero%2520practice%2520shots.%2520Whilst%2520impressive%2520progress%2520has%2520been%2520made%2520on%2520methods%2520that%250Aperform%2520zero-shot%2520RL%2520in%2520idealised%2520settings%252C%2520new%2520work%2520is%2520needed%2520if%2520these%2520results%250Aare%2520to%2520be%2520replicated%2520in%2520real-world%2520settings.%2520In%2520this%2520thesis%252C%2520we%2520argue%2520that%250Adoing%2520so%2520requires%2520us%2520to%2520navigate%2520%2528at%2520least%2529%2520three%2520constraints.%2520First%252C%2520the%2520data%250Aquality%2520constraint%253A%2520real-world%2520datasets%2520are%2520small%2520and%2520homogeneous.%2520Second%252C%2520the%250Aobservability%2520constraint%253A%2520states%252C%2520dynamics%2520and%2520rewards%2520in%2520the%2520real-world%2520are%250Aoften%2520only%2520partially%2520observed.%2520And%2520third%252C%2520the%2520data%2520availability%2520constraint%253A%2520a%250Apriori%2520access%2520to%2520data%2520cannot%2520always%2520be%2520assumed.%2520This%2520work%2520proposes%2520a%2520suite%2520of%250Amethods%2520that%2520perform%2520zero-shot%2520RL%2520subject%2520to%2520these%2520constraints.%2520In%2520a%2520series%2520of%250Aempirical%2520studies%2520we%2520expose%2520the%2520failings%2520of%2520existing%2520methods%252C%2520and%2520justify%2520our%250Atechniques%2520for%2520remedying%2520them.%2520We%2520believe%2520these%2520designs%2520take%2520us%2520a%2520step%2520closer%250Ato%2520RL%2520methods%2520that%2520can%2520be%2520deployed%2520to%2520solve%2520real-world%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16496v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Zero-Shot%20Reinforcement%20Learning&entry.906535625=Scott%20Jeen&entry.1292438233=%20%20Modern%20reinforcement%20learning%20%28RL%29%20systems%20capture%20deep%20truths%20about%20general%2C%0Ahuman%20problem-solving.%20In%20domains%20where%20new%20data%20can%20be%20simulated%20cheaply%2C%0Athese%20systems%20uncover%20sequential%20decision-making%20policies%20that%20far%20exceed%20the%0Aability%20of%20any%20human.%20Society%20faces%20many%20problems%20whose%20solutions%20require%20this%0Askill%2C%20but%20they%20are%20often%20in%20domains%20where%20new%20data%20cannot%20be%20cheaply%0Asimulated.%20In%20such%20scenarios%2C%20we%20can%20learn%20simulators%20from%20existing%20data%2C%20but%0Athese%20will%20only%20ever%20be%20approximately%20correct%2C%20and%20can%20be%20pathologically%0Aincorrect%20when%20queried%20outside%20of%20their%20training%20distribution.%20As%20a%20result%2C%20a%0Amisalignment%20between%20the%20environments%20in%20which%20we%20train%20our%20agents%20and%20the%0Areal-world%20in%20which%20we%20wish%20to%20deploy%20our%20agents%20is%20inevitable.%20Dealing%20with%0Athis%20misalignment%20is%20the%20primary%20concern%20of%20zero-shot%20reinforcement%20learning%2C%20a%0Aproblem%20setting%20where%20the%20agent%20must%20generalise%20to%20a%20new%20task%20or%20domain%20with%0Azero%20practice%20shots.%20Whilst%20impressive%20progress%20has%20been%20made%20on%20methods%20that%0Aperform%20zero-shot%20RL%20in%20idealised%20settings%2C%20new%20work%20is%20needed%20if%20these%20results%0Aare%20to%20be%20replicated%20in%20real-world%20settings.%20In%20this%20thesis%2C%20we%20argue%20that%0Adoing%20so%20requires%20us%20to%20navigate%20%28at%20least%29%20three%20constraints.%20First%2C%20the%20data%0Aquality%20constraint%3A%20real-world%20datasets%20are%20small%20and%20homogeneous.%20Second%2C%20the%0Aobservability%20constraint%3A%20states%2C%20dynamics%20and%20rewards%20in%20the%20real-world%20are%0Aoften%20only%20partially%20observed.%20And%20third%2C%20the%20data%20availability%20constraint%3A%20a%0Apriori%20access%20to%20data%20cannot%20always%20be%20assumed.%20This%20work%20proposes%20a%20suite%20of%0Amethods%20that%20perform%20zero-shot%20RL%20subject%20to%20these%20constraints.%20In%20a%20series%20of%0Aempirical%20studies%20we%20expose%20the%20failings%20of%20existing%20methods%2C%20and%20justify%20our%0Atechniques%20for%20remedying%20them.%20We%20believe%20these%20designs%20take%20us%20a%20step%20closer%0Ato%20RL%20methods%20that%20can%20be%20deployed%20to%20solve%20real-world%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16496v1&entry.124074799=Read"},
{"title": "From Confidence to Collapse in LLM Factual Robustness", "author": "Alina Fastowski and Bardh Prenkaj and Gjergji Kasneci", "abstract": "  Ensuring the robustness of factual knowledge in LLMs is critical for reliable\napplications in tasks such as question answering and reasoning. However,\nexisting evaluation methods predominantly focus on performance-based metrics,\noften investigating from the perspective of prompt perturbations, which\ncaptures only the externally triggered side of knowledge robustness. To bridge\nthis gap, we introduce a principled approach to measure factual robustness from\nthe perspective of the generation process by analyzing token distribution\nentropy in combination with temperature scaling sensitivity. These two factors\nbuild the Factual Robustness Score (FRS), a novel metric which quantifies the\nstability of a fact against perturbations in decoding conditions, given its\ninitial uncertainty. To validate our approach, we conduct extensive experiments\non 5 LLMs across 3 closed-book QA datasets (SQuAD, TriviaQA, and HotpotQA). We\nshow that factual robustness varies significantly -- smaller models report an\nFRS of $0.76$, larger ones $0.93$ -- with accuracy degrading by ~$60\\%$ under\nincreased uncertainty. These insights demonstrate how entropy and temperature\nscaling impact factual accuracy, and lay a foundation for developing more\nrobust knowledge retention and retrieval in future models.\n", "link": "http://arxiv.org/abs/2508.16267v1", "date": "2025-08-22", "relevancy": 1.9091, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4975}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4909}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Confidence%20to%20Collapse%20in%20LLM%20Factual%20Robustness&body=Title%3A%20From%20Confidence%20to%20Collapse%20in%20LLM%20Factual%20Robustness%0AAuthor%3A%20Alina%20Fastowski%20and%20Bardh%20Prenkaj%20and%20Gjergji%20Kasneci%0AAbstract%3A%20%20%20Ensuring%20the%20robustness%20of%20factual%20knowledge%20in%20LLMs%20is%20critical%20for%20reliable%0Aapplications%20in%20tasks%20such%20as%20question%20answering%20and%20reasoning.%20However%2C%0Aexisting%20evaluation%20methods%20predominantly%20focus%20on%20performance-based%20metrics%2C%0Aoften%20investigating%20from%20the%20perspective%20of%20prompt%20perturbations%2C%20which%0Acaptures%20only%20the%20externally%20triggered%20side%20of%20knowledge%20robustness.%20To%20bridge%0Athis%20gap%2C%20we%20introduce%20a%20principled%20approach%20to%20measure%20factual%20robustness%20from%0Athe%20perspective%20of%20the%20generation%20process%20by%20analyzing%20token%20distribution%0Aentropy%20in%20combination%20with%20temperature%20scaling%20sensitivity.%20These%20two%20factors%0Abuild%20the%20Factual%20Robustness%20Score%20%28FRS%29%2C%20a%20novel%20metric%20which%20quantifies%20the%0Astability%20of%20a%20fact%20against%20perturbations%20in%20decoding%20conditions%2C%20given%20its%0Ainitial%20uncertainty.%20To%20validate%20our%20approach%2C%20we%20conduct%20extensive%20experiments%0Aon%205%20LLMs%20across%203%20closed-book%20QA%20datasets%20%28SQuAD%2C%20TriviaQA%2C%20and%20HotpotQA%29.%20We%0Ashow%20that%20factual%20robustness%20varies%20significantly%20--%20smaller%20models%20report%20an%0AFRS%20of%20%240.76%24%2C%20larger%20ones%20%240.93%24%20--%20with%20accuracy%20degrading%20by%20~%2460%5C%25%24%20under%0Aincreased%20uncertainty.%20These%20insights%20demonstrate%20how%20entropy%20and%20temperature%0Ascaling%20impact%20factual%20accuracy%2C%20and%20lay%20a%20foundation%20for%20developing%20more%0Arobust%20knowledge%20retention%20and%20retrieval%20in%20future%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16267v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Confidence%2520to%2520Collapse%2520in%2520LLM%2520Factual%2520Robustness%26entry.906535625%3DAlina%2520Fastowski%2520and%2520Bardh%2520Prenkaj%2520and%2520Gjergji%2520Kasneci%26entry.1292438233%3D%2520%2520Ensuring%2520the%2520robustness%2520of%2520factual%2520knowledge%2520in%2520LLMs%2520is%2520critical%2520for%2520reliable%250Aapplications%2520in%2520tasks%2520such%2520as%2520question%2520answering%2520and%2520reasoning.%2520However%252C%250Aexisting%2520evaluation%2520methods%2520predominantly%2520focus%2520on%2520performance-based%2520metrics%252C%250Aoften%2520investigating%2520from%2520the%2520perspective%2520of%2520prompt%2520perturbations%252C%2520which%250Acaptures%2520only%2520the%2520externally%2520triggered%2520side%2520of%2520knowledge%2520robustness.%2520To%2520bridge%250Athis%2520gap%252C%2520we%2520introduce%2520a%2520principled%2520approach%2520to%2520measure%2520factual%2520robustness%2520from%250Athe%2520perspective%2520of%2520the%2520generation%2520process%2520by%2520analyzing%2520token%2520distribution%250Aentropy%2520in%2520combination%2520with%2520temperature%2520scaling%2520sensitivity.%2520These%2520two%2520factors%250Abuild%2520the%2520Factual%2520Robustness%2520Score%2520%2528FRS%2529%252C%2520a%2520novel%2520metric%2520which%2520quantifies%2520the%250Astability%2520of%2520a%2520fact%2520against%2520perturbations%2520in%2520decoding%2520conditions%252C%2520given%2520its%250Ainitial%2520uncertainty.%2520To%2520validate%2520our%2520approach%252C%2520we%2520conduct%2520extensive%2520experiments%250Aon%25205%2520LLMs%2520across%25203%2520closed-book%2520QA%2520datasets%2520%2528SQuAD%252C%2520TriviaQA%252C%2520and%2520HotpotQA%2529.%2520We%250Ashow%2520that%2520factual%2520robustness%2520varies%2520significantly%2520--%2520smaller%2520models%2520report%2520an%250AFRS%2520of%2520%25240.76%2524%252C%2520larger%2520ones%2520%25240.93%2524%2520--%2520with%2520accuracy%2520degrading%2520by%2520~%252460%255C%2525%2524%2520under%250Aincreased%2520uncertainty.%2520These%2520insights%2520demonstrate%2520how%2520entropy%2520and%2520temperature%250Ascaling%2520impact%2520factual%2520accuracy%252C%2520and%2520lay%2520a%2520foundation%2520for%2520developing%2520more%250Arobust%2520knowledge%2520retention%2520and%2520retrieval%2520in%2520future%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16267v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Confidence%20to%20Collapse%20in%20LLM%20Factual%20Robustness&entry.906535625=Alina%20Fastowski%20and%20Bardh%20Prenkaj%20and%20Gjergji%20Kasneci&entry.1292438233=%20%20Ensuring%20the%20robustness%20of%20factual%20knowledge%20in%20LLMs%20is%20critical%20for%20reliable%0Aapplications%20in%20tasks%20such%20as%20question%20answering%20and%20reasoning.%20However%2C%0Aexisting%20evaluation%20methods%20predominantly%20focus%20on%20performance-based%20metrics%2C%0Aoften%20investigating%20from%20the%20perspective%20of%20prompt%20perturbations%2C%20which%0Acaptures%20only%20the%20externally%20triggered%20side%20of%20knowledge%20robustness.%20To%20bridge%0Athis%20gap%2C%20we%20introduce%20a%20principled%20approach%20to%20measure%20factual%20robustness%20from%0Athe%20perspective%20of%20the%20generation%20process%20by%20analyzing%20token%20distribution%0Aentropy%20in%20combination%20with%20temperature%20scaling%20sensitivity.%20These%20two%20factors%0Abuild%20the%20Factual%20Robustness%20Score%20%28FRS%29%2C%20a%20novel%20metric%20which%20quantifies%20the%0Astability%20of%20a%20fact%20against%20perturbations%20in%20decoding%20conditions%2C%20given%20its%0Ainitial%20uncertainty.%20To%20validate%20our%20approach%2C%20we%20conduct%20extensive%20experiments%0Aon%205%20LLMs%20across%203%20closed-book%20QA%20datasets%20%28SQuAD%2C%20TriviaQA%2C%20and%20HotpotQA%29.%20We%0Ashow%20that%20factual%20robustness%20varies%20significantly%20--%20smaller%20models%20report%20an%0AFRS%20of%20%240.76%24%2C%20larger%20ones%20%240.93%24%20--%20with%20accuracy%20degrading%20by%20~%2460%5C%25%24%20under%0Aincreased%20uncertainty.%20These%20insights%20demonstrate%20how%20entropy%20and%20temperature%0Ascaling%20impact%20factual%20accuracy%2C%20and%20lay%20a%20foundation%20for%20developing%20more%0Arobust%20knowledge%20retention%20and%20retrieval%20in%20future%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16267v1&entry.124074799=Read"},
{"title": "PENGUIN: Enhancing Transformer with Periodic-Nested Group Attention for\n  Long-term Time Series Forecasting", "author": "Tian Sun and Yuqi Chen and Weiwei Sun", "abstract": "  Long-term time series forecasting (LTSF) is a fundamental task with\nwide-ranging applications. Although Transformer-based models have made\nsignificant breakthroughs in forecasting, their effectiveness for time series\nforecasting remains debatable. In this paper, we revisit the significance of\nself-attention and propose a simple yet effective mechanism, Periodic-Nested\nGroup Attention, namely PENGUIN. Our approach highlights the importance of\nexplicitly modeling periodic patterns and incorporating relative attention bias\nfor effective time series modeling. To this end, we introduce a periodic-nested\nrelative attention bias that captures periodic structures directly. To handle\nmultiple coexisting periodicities (e.g., daily and weekly cycles), we design a\ngrouped attention mechanism, where each group targets a specific periodicity\nusing a multi-query attention mechanism. Extensive experiments across diverse\nbenchmarks demonstrate that PENGUIN consistently outperforms both MLP-based and\nTransformer-based models.\n", "link": "http://arxiv.org/abs/2508.13773v2", "date": "2025-08-22", "relevancy": 1.8985, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4939}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4755}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PENGUIN%3A%20Enhancing%20Transformer%20with%20Periodic-Nested%20Group%20Attention%20for%0A%20%20Long-term%20Time%20Series%20Forecasting&body=Title%3A%20PENGUIN%3A%20Enhancing%20Transformer%20with%20Periodic-Nested%20Group%20Attention%20for%0A%20%20Long-term%20Time%20Series%20Forecasting%0AAuthor%3A%20Tian%20Sun%20and%20Yuqi%20Chen%20and%20Weiwei%20Sun%0AAbstract%3A%20%20%20Long-term%20time%20series%20forecasting%20%28LTSF%29%20is%20a%20fundamental%20task%20with%0Awide-ranging%20applications.%20Although%20Transformer-based%20models%20have%20made%0Asignificant%20breakthroughs%20in%20forecasting%2C%20their%20effectiveness%20for%20time%20series%0Aforecasting%20remains%20debatable.%20In%20this%20paper%2C%20we%20revisit%20the%20significance%20of%0Aself-attention%20and%20propose%20a%20simple%20yet%20effective%20mechanism%2C%20Periodic-Nested%0AGroup%20Attention%2C%20namely%20PENGUIN.%20Our%20approach%20highlights%20the%20importance%20of%0Aexplicitly%20modeling%20periodic%20patterns%20and%20incorporating%20relative%20attention%20bias%0Afor%20effective%20time%20series%20modeling.%20To%20this%20end%2C%20we%20introduce%20a%20periodic-nested%0Arelative%20attention%20bias%20that%20captures%20periodic%20structures%20directly.%20To%20handle%0Amultiple%20coexisting%20periodicities%20%28e.g.%2C%20daily%20and%20weekly%20cycles%29%2C%20we%20design%20a%0Agrouped%20attention%20mechanism%2C%20where%20each%20group%20targets%20a%20specific%20periodicity%0Ausing%20a%20multi-query%20attention%20mechanism.%20Extensive%20experiments%20across%20diverse%0Abenchmarks%20demonstrate%20that%20PENGUIN%20consistently%20outperforms%20both%20MLP-based%20and%0ATransformer-based%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13773v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPENGUIN%253A%2520Enhancing%2520Transformer%2520with%2520Periodic-Nested%2520Group%2520Attention%2520for%250A%2520%2520Long-term%2520Time%2520Series%2520Forecasting%26entry.906535625%3DTian%2520Sun%2520and%2520Yuqi%2520Chen%2520and%2520Weiwei%2520Sun%26entry.1292438233%3D%2520%2520Long-term%2520time%2520series%2520forecasting%2520%2528LTSF%2529%2520is%2520a%2520fundamental%2520task%2520with%250Awide-ranging%2520applications.%2520Although%2520Transformer-based%2520models%2520have%2520made%250Asignificant%2520breakthroughs%2520in%2520forecasting%252C%2520their%2520effectiveness%2520for%2520time%2520series%250Aforecasting%2520remains%2520debatable.%2520In%2520this%2520paper%252C%2520we%2520revisit%2520the%2520significance%2520of%250Aself-attention%2520and%2520propose%2520a%2520simple%2520yet%2520effective%2520mechanism%252C%2520Periodic-Nested%250AGroup%2520Attention%252C%2520namely%2520PENGUIN.%2520Our%2520approach%2520highlights%2520the%2520importance%2520of%250Aexplicitly%2520modeling%2520periodic%2520patterns%2520and%2520incorporating%2520relative%2520attention%2520bias%250Afor%2520effective%2520time%2520series%2520modeling.%2520To%2520this%2520end%252C%2520we%2520introduce%2520a%2520periodic-nested%250Arelative%2520attention%2520bias%2520that%2520captures%2520periodic%2520structures%2520directly.%2520To%2520handle%250Amultiple%2520coexisting%2520periodicities%2520%2528e.g.%252C%2520daily%2520and%2520weekly%2520cycles%2529%252C%2520we%2520design%2520a%250Agrouped%2520attention%2520mechanism%252C%2520where%2520each%2520group%2520targets%2520a%2520specific%2520periodicity%250Ausing%2520a%2520multi-query%2520attention%2520mechanism.%2520Extensive%2520experiments%2520across%2520diverse%250Abenchmarks%2520demonstrate%2520that%2520PENGUIN%2520consistently%2520outperforms%2520both%2520MLP-based%2520and%250ATransformer-based%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13773v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PENGUIN%3A%20Enhancing%20Transformer%20with%20Periodic-Nested%20Group%20Attention%20for%0A%20%20Long-term%20Time%20Series%20Forecasting&entry.906535625=Tian%20Sun%20and%20Yuqi%20Chen%20and%20Weiwei%20Sun&entry.1292438233=%20%20Long-term%20time%20series%20forecasting%20%28LTSF%29%20is%20a%20fundamental%20task%20with%0Awide-ranging%20applications.%20Although%20Transformer-based%20models%20have%20made%0Asignificant%20breakthroughs%20in%20forecasting%2C%20their%20effectiveness%20for%20time%20series%0Aforecasting%20remains%20debatable.%20In%20this%20paper%2C%20we%20revisit%20the%20significance%20of%0Aself-attention%20and%20propose%20a%20simple%20yet%20effective%20mechanism%2C%20Periodic-Nested%0AGroup%20Attention%2C%20namely%20PENGUIN.%20Our%20approach%20highlights%20the%20importance%20of%0Aexplicitly%20modeling%20periodic%20patterns%20and%20incorporating%20relative%20attention%20bias%0Afor%20effective%20time%20series%20modeling.%20To%20this%20end%2C%20we%20introduce%20a%20periodic-nested%0Arelative%20attention%20bias%20that%20captures%20periodic%20structures%20directly.%20To%20handle%0Amultiple%20coexisting%20periodicities%20%28e.g.%2C%20daily%20and%20weekly%20cycles%29%2C%20we%20design%20a%0Agrouped%20attention%20mechanism%2C%20where%20each%20group%20targets%20a%20specific%20periodicity%0Ausing%20a%20multi-query%20attention%20mechanism.%20Extensive%20experiments%20across%20diverse%0Abenchmarks%20demonstrate%20that%20PENGUIN%20consistently%20outperforms%20both%20MLP-based%20and%0ATransformer-based%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13773v2&entry.124074799=Read"},
{"title": "Optimal Batch-Size Control for Low-Latency Federated Learning with\n  Device Heterogeneity", "author": "Huiling Yang and Zhanwei Wang and Kaibin Huang", "abstract": "  Federated learning (FL) has emerged as a popular approach for collaborative\nmachine learning in sixth-generation (6G) networks, primarily due to its\nprivacy-preserving capabilities. The deployment of FL algorithms is expected to\nempower a wide range of Internet-of-Things (IoT) applications, e.g., autonomous\ndriving, augmented reality, and healthcare. The mission-critical and\ntime-sensitive nature of these applications necessitates the design of\nlow-latency FL frameworks that guarantee high learning performance. In\npractice, achieving low-latency FL faces two challenges: the overhead of\ncomputing and transmitting high-dimensional model updates, and the\nheterogeneity in communication-and-computation (C$^2$) capabilities across\ndevices. To address these challenges, we propose a novel C$^2$-aware framework\nfor optimal batch-size control that minimizes end-to-end (E2E) learning latency\nwhile ensuring convergence. The framework is designed to balance a fundamental\nC$^2$ tradeoff as revealed through convergence analysis. Specifically,\nincreasing batch sizes improves the accuracy of gradient estimation in FL and\nthus reduces the number of communication rounds required for convergence, but\nresults in higher per-round latency, and vice versa. The associated problem of\nlatency minimization is intractable; however, we solve it by designing an\naccurate and tractable surrogate for convergence speed, with parameters fitted\nto real data. This approach yields two batch-size control strategies tailored\nto scenarios with slow and fast fading, while also accommodating device\nheterogeneity. Extensive experiments using real datasets demonstrate that the\nproposed strategies outperform conventional batch-size adaptation schemes that\ndo not consider the C$^2$ tradeoff or device heterogeneity.\n", "link": "http://arxiv.org/abs/2507.15601v2", "date": "2025-08-22", "relevancy": 1.8904, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4936}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4741}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4627}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Batch-Size%20Control%20for%20Low-Latency%20Federated%20Learning%20with%0A%20%20Device%20Heterogeneity&body=Title%3A%20Optimal%20Batch-Size%20Control%20for%20Low-Latency%20Federated%20Learning%20with%0A%20%20Device%20Heterogeneity%0AAuthor%3A%20Huiling%20Yang%20and%20Zhanwei%20Wang%20and%20Kaibin%20Huang%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20has%20emerged%20as%20a%20popular%20approach%20for%20collaborative%0Amachine%20learning%20in%20sixth-generation%20%286G%29%20networks%2C%20primarily%20due%20to%20its%0Aprivacy-preserving%20capabilities.%20The%20deployment%20of%20FL%20algorithms%20is%20expected%20to%0Aempower%20a%20wide%20range%20of%20Internet-of-Things%20%28IoT%29%20applications%2C%20e.g.%2C%20autonomous%0Adriving%2C%20augmented%20reality%2C%20and%20healthcare.%20The%20mission-critical%20and%0Atime-sensitive%20nature%20of%20these%20applications%20necessitates%20the%20design%20of%0Alow-latency%20FL%20frameworks%20that%20guarantee%20high%20learning%20performance.%20In%0Apractice%2C%20achieving%20low-latency%20FL%20faces%20two%20challenges%3A%20the%20overhead%20of%0Acomputing%20and%20transmitting%20high-dimensional%20model%20updates%2C%20and%20the%0Aheterogeneity%20in%20communication-and-computation%20%28C%24%5E2%24%29%20capabilities%20across%0Adevices.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20C%24%5E2%24-aware%20framework%0Afor%20optimal%20batch-size%20control%20that%20minimizes%20end-to-end%20%28E2E%29%20learning%20latency%0Awhile%20ensuring%20convergence.%20The%20framework%20is%20designed%20to%20balance%20a%20fundamental%0AC%24%5E2%24%20tradeoff%20as%20revealed%20through%20convergence%20analysis.%20Specifically%2C%0Aincreasing%20batch%20sizes%20improves%20the%20accuracy%20of%20gradient%20estimation%20in%20FL%20and%0Athus%20reduces%20the%20number%20of%20communication%20rounds%20required%20for%20convergence%2C%20but%0Aresults%20in%20higher%20per-round%20latency%2C%20and%20vice%20versa.%20The%20associated%20problem%20of%0Alatency%20minimization%20is%20intractable%3B%20however%2C%20we%20solve%20it%20by%20designing%20an%0Aaccurate%20and%20tractable%20surrogate%20for%20convergence%20speed%2C%20with%20parameters%20fitted%0Ato%20real%20data.%20This%20approach%20yields%20two%20batch-size%20control%20strategies%20tailored%0Ato%20scenarios%20with%20slow%20and%20fast%20fading%2C%20while%20also%20accommodating%20device%0Aheterogeneity.%20Extensive%20experiments%20using%20real%20datasets%20demonstrate%20that%20the%0Aproposed%20strategies%20outperform%20conventional%20batch-size%20adaptation%20schemes%20that%0Ado%20not%20consider%20the%20C%24%5E2%24%20tradeoff%20or%20device%20heterogeneity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15601v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Batch-Size%2520Control%2520for%2520Low-Latency%2520Federated%2520Learning%2520with%250A%2520%2520Device%2520Heterogeneity%26entry.906535625%3DHuiling%2520Yang%2520and%2520Zhanwei%2520Wang%2520and%2520Kaibin%2520Huang%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520has%2520emerged%2520as%2520a%2520popular%2520approach%2520for%2520collaborative%250Amachine%2520learning%2520in%2520sixth-generation%2520%25286G%2529%2520networks%252C%2520primarily%2520due%2520to%2520its%250Aprivacy-preserving%2520capabilities.%2520The%2520deployment%2520of%2520FL%2520algorithms%2520is%2520expected%2520to%250Aempower%2520a%2520wide%2520range%2520of%2520Internet-of-Things%2520%2528IoT%2529%2520applications%252C%2520e.g.%252C%2520autonomous%250Adriving%252C%2520augmented%2520reality%252C%2520and%2520healthcare.%2520The%2520mission-critical%2520and%250Atime-sensitive%2520nature%2520of%2520these%2520applications%2520necessitates%2520the%2520design%2520of%250Alow-latency%2520FL%2520frameworks%2520that%2520guarantee%2520high%2520learning%2520performance.%2520In%250Apractice%252C%2520achieving%2520low-latency%2520FL%2520faces%2520two%2520challenges%253A%2520the%2520overhead%2520of%250Acomputing%2520and%2520transmitting%2520high-dimensional%2520model%2520updates%252C%2520and%2520the%250Aheterogeneity%2520in%2520communication-and-computation%2520%2528C%2524%255E2%2524%2529%2520capabilities%2520across%250Adevices.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520C%2524%255E2%2524-aware%2520framework%250Afor%2520optimal%2520batch-size%2520control%2520that%2520minimizes%2520end-to-end%2520%2528E2E%2529%2520learning%2520latency%250Awhile%2520ensuring%2520convergence.%2520The%2520framework%2520is%2520designed%2520to%2520balance%2520a%2520fundamental%250AC%2524%255E2%2524%2520tradeoff%2520as%2520revealed%2520through%2520convergence%2520analysis.%2520Specifically%252C%250Aincreasing%2520batch%2520sizes%2520improves%2520the%2520accuracy%2520of%2520gradient%2520estimation%2520in%2520FL%2520and%250Athus%2520reduces%2520the%2520number%2520of%2520communication%2520rounds%2520required%2520for%2520convergence%252C%2520but%250Aresults%2520in%2520higher%2520per-round%2520latency%252C%2520and%2520vice%2520versa.%2520The%2520associated%2520problem%2520of%250Alatency%2520minimization%2520is%2520intractable%253B%2520however%252C%2520we%2520solve%2520it%2520by%2520designing%2520an%250Aaccurate%2520and%2520tractable%2520surrogate%2520for%2520convergence%2520speed%252C%2520with%2520parameters%2520fitted%250Ato%2520real%2520data.%2520This%2520approach%2520yields%2520two%2520batch-size%2520control%2520strategies%2520tailored%250Ato%2520scenarios%2520with%2520slow%2520and%2520fast%2520fading%252C%2520while%2520also%2520accommodating%2520device%250Aheterogeneity.%2520Extensive%2520experiments%2520using%2520real%2520datasets%2520demonstrate%2520that%2520the%250Aproposed%2520strategies%2520outperform%2520conventional%2520batch-size%2520adaptation%2520schemes%2520that%250Ado%2520not%2520consider%2520the%2520C%2524%255E2%2524%2520tradeoff%2520or%2520device%2520heterogeneity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15601v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Batch-Size%20Control%20for%20Low-Latency%20Federated%20Learning%20with%0A%20%20Device%20Heterogeneity&entry.906535625=Huiling%20Yang%20and%20Zhanwei%20Wang%20and%20Kaibin%20Huang&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20has%20emerged%20as%20a%20popular%20approach%20for%20collaborative%0Amachine%20learning%20in%20sixth-generation%20%286G%29%20networks%2C%20primarily%20due%20to%20its%0Aprivacy-preserving%20capabilities.%20The%20deployment%20of%20FL%20algorithms%20is%20expected%20to%0Aempower%20a%20wide%20range%20of%20Internet-of-Things%20%28IoT%29%20applications%2C%20e.g.%2C%20autonomous%0Adriving%2C%20augmented%20reality%2C%20and%20healthcare.%20The%20mission-critical%20and%0Atime-sensitive%20nature%20of%20these%20applications%20necessitates%20the%20design%20of%0Alow-latency%20FL%20frameworks%20that%20guarantee%20high%20learning%20performance.%20In%0Apractice%2C%20achieving%20low-latency%20FL%20faces%20two%20challenges%3A%20the%20overhead%20of%0Acomputing%20and%20transmitting%20high-dimensional%20model%20updates%2C%20and%20the%0Aheterogeneity%20in%20communication-and-computation%20%28C%24%5E2%24%29%20capabilities%20across%0Adevices.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20C%24%5E2%24-aware%20framework%0Afor%20optimal%20batch-size%20control%20that%20minimizes%20end-to-end%20%28E2E%29%20learning%20latency%0Awhile%20ensuring%20convergence.%20The%20framework%20is%20designed%20to%20balance%20a%20fundamental%0AC%24%5E2%24%20tradeoff%20as%20revealed%20through%20convergence%20analysis.%20Specifically%2C%0Aincreasing%20batch%20sizes%20improves%20the%20accuracy%20of%20gradient%20estimation%20in%20FL%20and%0Athus%20reduces%20the%20number%20of%20communication%20rounds%20required%20for%20convergence%2C%20but%0Aresults%20in%20higher%20per-round%20latency%2C%20and%20vice%20versa.%20The%20associated%20problem%20of%0Alatency%20minimization%20is%20intractable%3B%20however%2C%20we%20solve%20it%20by%20designing%20an%0Aaccurate%20and%20tractable%20surrogate%20for%20convergence%20speed%2C%20with%20parameters%20fitted%0Ato%20real%20data.%20This%20approach%20yields%20two%20batch-size%20control%20strategies%20tailored%0Ato%20scenarios%20with%20slow%20and%20fast%20fading%2C%20while%20also%20accommodating%20device%0Aheterogeneity.%20Extensive%20experiments%20using%20real%20datasets%20demonstrate%20that%20the%0Aproposed%20strategies%20outperform%20conventional%20batch-size%20adaptation%20schemes%20that%0Ado%20not%20consider%20the%20C%24%5E2%24%20tradeoff%20or%20device%20heterogeneity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15601v2&entry.124074799=Read"},
{"title": "FLAMES: Improving LLM Math Reasoning via a Fine-Grained Analysis of the\n  Data Synthesis Pipeline", "author": "Parker Seegmiller and Kartik Mehta and Soumya Saha and Chenyang Tao and Shereen Oraby and Arpit Gupta and Tagyoung Chung and Mohit Bansal and Nanyun Peng", "abstract": "  Recent works improving LLM math reasoning with synthetic data have used\nunique setups, making comparison of data synthesis strategies impractical. This\nleaves many unanswered questions about the roles of different factors in the\nsynthetic data pipeline, such as the impact of filtering low-quality problems.\nTo address this gap, we introduce FLAMES, a Framework for LLM Assessment of\nMath rEasoning Data Synthesis, and perform a systematic study of 10 existing\ndata synthesis strategies and multiple other factors impacting the performance\nof synthetic math reasoning data. Our FLAMES experiments provide several\nvaluable insights about the optimal balance of difficulty and diversity of\nsynthetic data. First, data agents designed to increase problem complexity lead\nto best improvements on most math metrics. Second, with a fixed data generation\nbudget, keeping higher problem coverage is more important than keeping only\nproblems with reliable solutions. Third, GSM8K- and MATH-based synthetic data\ncan lead to improvements on competition-level benchmarks, showcasing\neasy-to-hard generalization. Leveraging insights from our FLAMES experiments,\nwe design two novel data synthesis strategies for improving out-of-domain\ngeneralization and robustness. Further, we develop the FLAMES dataset, an\neffective blend of our novel and existing data synthesis strategies,\noutperforming public datasets on OlympiadBench (+15.7), CollegeMath (+4.5),\nGSMPlus (+6.5), and MATH (+3.1). Fine-tuning Qwen2.5-Math-7B on the FLAMES\ndataset achieves 81.4% on MATH, surpassing larger Llama3 405B, GPT-4o and\nClaude 3.5 Sonnet.\n", "link": "http://arxiv.org/abs/2508.16514v1", "date": "2025-08-22", "relevancy": 1.8751, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4824}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4668}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FLAMES%3A%20Improving%20LLM%20Math%20Reasoning%20via%20a%20Fine-Grained%20Analysis%20of%20the%0A%20%20Data%20Synthesis%20Pipeline&body=Title%3A%20FLAMES%3A%20Improving%20LLM%20Math%20Reasoning%20via%20a%20Fine-Grained%20Analysis%20of%20the%0A%20%20Data%20Synthesis%20Pipeline%0AAuthor%3A%20Parker%20Seegmiller%20and%20Kartik%20Mehta%20and%20Soumya%20Saha%20and%20Chenyang%20Tao%20and%20Shereen%20Oraby%20and%20Arpit%20Gupta%20and%20Tagyoung%20Chung%20and%20Mohit%20Bansal%20and%20Nanyun%20Peng%0AAbstract%3A%20%20%20Recent%20works%20improving%20LLM%20math%20reasoning%20with%20synthetic%20data%20have%20used%0Aunique%20setups%2C%20making%20comparison%20of%20data%20synthesis%20strategies%20impractical.%20This%0Aleaves%20many%20unanswered%20questions%20about%20the%20roles%20of%20different%20factors%20in%20the%0Asynthetic%20data%20pipeline%2C%20such%20as%20the%20impact%20of%20filtering%20low-quality%20problems.%0ATo%20address%20this%20gap%2C%20we%20introduce%20FLAMES%2C%20a%20Framework%20for%20LLM%20Assessment%20of%0AMath%20rEasoning%20Data%20Synthesis%2C%20and%20perform%20a%20systematic%20study%20of%2010%20existing%0Adata%20synthesis%20strategies%20and%20multiple%20other%20factors%20impacting%20the%20performance%0Aof%20synthetic%20math%20reasoning%20data.%20Our%20FLAMES%20experiments%20provide%20several%0Avaluable%20insights%20about%20the%20optimal%20balance%20of%20difficulty%20and%20diversity%20of%0Asynthetic%20data.%20First%2C%20data%20agents%20designed%20to%20increase%20problem%20complexity%20lead%0Ato%20best%20improvements%20on%20most%20math%20metrics.%20Second%2C%20with%20a%20fixed%20data%20generation%0Abudget%2C%20keeping%20higher%20problem%20coverage%20is%20more%20important%20than%20keeping%20only%0Aproblems%20with%20reliable%20solutions.%20Third%2C%20GSM8K-%20and%20MATH-based%20synthetic%20data%0Acan%20lead%20to%20improvements%20on%20competition-level%20benchmarks%2C%20showcasing%0Aeasy-to-hard%20generalization.%20Leveraging%20insights%20from%20our%20FLAMES%20experiments%2C%0Awe%20design%20two%20novel%20data%20synthesis%20strategies%20for%20improving%20out-of-domain%0Ageneralization%20and%20robustness.%20Further%2C%20we%20develop%20the%20FLAMES%20dataset%2C%20an%0Aeffective%20blend%20of%20our%20novel%20and%20existing%20data%20synthesis%20strategies%2C%0Aoutperforming%20public%20datasets%20on%20OlympiadBench%20%28%2B15.7%29%2C%20CollegeMath%20%28%2B4.5%29%2C%0AGSMPlus%20%28%2B6.5%29%2C%20and%20MATH%20%28%2B3.1%29.%20Fine-tuning%20Qwen2.5-Math-7B%20on%20the%20FLAMES%0Adataset%20achieves%2081.4%25%20on%20MATH%2C%20surpassing%20larger%20Llama3%20405B%2C%20GPT-4o%20and%0AClaude%203.5%20Sonnet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16514v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFLAMES%253A%2520Improving%2520LLM%2520Math%2520Reasoning%2520via%2520a%2520Fine-Grained%2520Analysis%2520of%2520the%250A%2520%2520Data%2520Synthesis%2520Pipeline%26entry.906535625%3DParker%2520Seegmiller%2520and%2520Kartik%2520Mehta%2520and%2520Soumya%2520Saha%2520and%2520Chenyang%2520Tao%2520and%2520Shereen%2520Oraby%2520and%2520Arpit%2520Gupta%2520and%2520Tagyoung%2520Chung%2520and%2520Mohit%2520Bansal%2520and%2520Nanyun%2520Peng%26entry.1292438233%3D%2520%2520Recent%2520works%2520improving%2520LLM%2520math%2520reasoning%2520with%2520synthetic%2520data%2520have%2520used%250Aunique%2520setups%252C%2520making%2520comparison%2520of%2520data%2520synthesis%2520strategies%2520impractical.%2520This%250Aleaves%2520many%2520unanswered%2520questions%2520about%2520the%2520roles%2520of%2520different%2520factors%2520in%2520the%250Asynthetic%2520data%2520pipeline%252C%2520such%2520as%2520the%2520impact%2520of%2520filtering%2520low-quality%2520problems.%250ATo%2520address%2520this%2520gap%252C%2520we%2520introduce%2520FLAMES%252C%2520a%2520Framework%2520for%2520LLM%2520Assessment%2520of%250AMath%2520rEasoning%2520Data%2520Synthesis%252C%2520and%2520perform%2520a%2520systematic%2520study%2520of%252010%2520existing%250Adata%2520synthesis%2520strategies%2520and%2520multiple%2520other%2520factors%2520impacting%2520the%2520performance%250Aof%2520synthetic%2520math%2520reasoning%2520data.%2520Our%2520FLAMES%2520experiments%2520provide%2520several%250Avaluable%2520insights%2520about%2520the%2520optimal%2520balance%2520of%2520difficulty%2520and%2520diversity%2520of%250Asynthetic%2520data.%2520First%252C%2520data%2520agents%2520designed%2520to%2520increase%2520problem%2520complexity%2520lead%250Ato%2520best%2520improvements%2520on%2520most%2520math%2520metrics.%2520Second%252C%2520with%2520a%2520fixed%2520data%2520generation%250Abudget%252C%2520keeping%2520higher%2520problem%2520coverage%2520is%2520more%2520important%2520than%2520keeping%2520only%250Aproblems%2520with%2520reliable%2520solutions.%2520Third%252C%2520GSM8K-%2520and%2520MATH-based%2520synthetic%2520data%250Acan%2520lead%2520to%2520improvements%2520on%2520competition-level%2520benchmarks%252C%2520showcasing%250Aeasy-to-hard%2520generalization.%2520Leveraging%2520insights%2520from%2520our%2520FLAMES%2520experiments%252C%250Awe%2520design%2520two%2520novel%2520data%2520synthesis%2520strategies%2520for%2520improving%2520out-of-domain%250Ageneralization%2520and%2520robustness.%2520Further%252C%2520we%2520develop%2520the%2520FLAMES%2520dataset%252C%2520an%250Aeffective%2520blend%2520of%2520our%2520novel%2520and%2520existing%2520data%2520synthesis%2520strategies%252C%250Aoutperforming%2520public%2520datasets%2520on%2520OlympiadBench%2520%2528%252B15.7%2529%252C%2520CollegeMath%2520%2528%252B4.5%2529%252C%250AGSMPlus%2520%2528%252B6.5%2529%252C%2520and%2520MATH%2520%2528%252B3.1%2529.%2520Fine-tuning%2520Qwen2.5-Math-7B%2520on%2520the%2520FLAMES%250Adataset%2520achieves%252081.4%2525%2520on%2520MATH%252C%2520surpassing%2520larger%2520Llama3%2520405B%252C%2520GPT-4o%2520and%250AClaude%25203.5%2520Sonnet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16514v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FLAMES%3A%20Improving%20LLM%20Math%20Reasoning%20via%20a%20Fine-Grained%20Analysis%20of%20the%0A%20%20Data%20Synthesis%20Pipeline&entry.906535625=Parker%20Seegmiller%20and%20Kartik%20Mehta%20and%20Soumya%20Saha%20and%20Chenyang%20Tao%20and%20Shereen%20Oraby%20and%20Arpit%20Gupta%20and%20Tagyoung%20Chung%20and%20Mohit%20Bansal%20and%20Nanyun%20Peng&entry.1292438233=%20%20Recent%20works%20improving%20LLM%20math%20reasoning%20with%20synthetic%20data%20have%20used%0Aunique%20setups%2C%20making%20comparison%20of%20data%20synthesis%20strategies%20impractical.%20This%0Aleaves%20many%20unanswered%20questions%20about%20the%20roles%20of%20different%20factors%20in%20the%0Asynthetic%20data%20pipeline%2C%20such%20as%20the%20impact%20of%20filtering%20low-quality%20problems.%0ATo%20address%20this%20gap%2C%20we%20introduce%20FLAMES%2C%20a%20Framework%20for%20LLM%20Assessment%20of%0AMath%20rEasoning%20Data%20Synthesis%2C%20and%20perform%20a%20systematic%20study%20of%2010%20existing%0Adata%20synthesis%20strategies%20and%20multiple%20other%20factors%20impacting%20the%20performance%0Aof%20synthetic%20math%20reasoning%20data.%20Our%20FLAMES%20experiments%20provide%20several%0Avaluable%20insights%20about%20the%20optimal%20balance%20of%20difficulty%20and%20diversity%20of%0Asynthetic%20data.%20First%2C%20data%20agents%20designed%20to%20increase%20problem%20complexity%20lead%0Ato%20best%20improvements%20on%20most%20math%20metrics.%20Second%2C%20with%20a%20fixed%20data%20generation%0Abudget%2C%20keeping%20higher%20problem%20coverage%20is%20more%20important%20than%20keeping%20only%0Aproblems%20with%20reliable%20solutions.%20Third%2C%20GSM8K-%20and%20MATH-based%20synthetic%20data%0Acan%20lead%20to%20improvements%20on%20competition-level%20benchmarks%2C%20showcasing%0Aeasy-to-hard%20generalization.%20Leveraging%20insights%20from%20our%20FLAMES%20experiments%2C%0Awe%20design%20two%20novel%20data%20synthesis%20strategies%20for%20improving%20out-of-domain%0Ageneralization%20and%20robustness.%20Further%2C%20we%20develop%20the%20FLAMES%20dataset%2C%20an%0Aeffective%20blend%20of%20our%20novel%20and%20existing%20data%20synthesis%20strategies%2C%0Aoutperforming%20public%20datasets%20on%20OlympiadBench%20%28%2B15.7%29%2C%20CollegeMath%20%28%2B4.5%29%2C%0AGSMPlus%20%28%2B6.5%29%2C%20and%20MATH%20%28%2B3.1%29.%20Fine-tuning%20Qwen2.5-Math-7B%20on%20the%20FLAMES%0Adataset%20achieves%2081.4%25%20on%20MATH%2C%20surpassing%20larger%20Llama3%20405B%2C%20GPT-4o%20and%0AClaude%203.5%20Sonnet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16514v1&entry.124074799=Read"},
{"title": "Time-Aware One Step Diffusion Network for Real-World Image\n  Super-Resolution", "author": "Tainyi Zhang and Zheng-Peng Duan and Peng-Tao Jiang and Bo Li and Ming-Ming Cheng and Chun-Le Guo and Chongyi Li", "abstract": "  Diffusion-based real-world image super-resolution (Real-ISR) methods have\ndemonstrated impressive performance. To achieve efficient Real-ISR, many works\nemploy Variational Score Distillation (VSD) to distill pre-trained\nstable-diffusion (SD) model for one-step SR with a fixed timestep. However, due\nto the different noise injection timesteps, the SD will perform different\ngenerative priors. Therefore, a fixed timestep is difficult for these methods\nto fully leverage the generative priors in SD, leading to suboptimal\nperformance. To address this, we propose a Time-Aware one-step Diffusion\nNetwork for Real-ISR (TADSR). We first introduce a Time-Aware VAE Encoder,\nwhich projects the same image into different latent features based on\ntimesteps. Through joint dynamic variation of timesteps and latent features,\nthe student model can better align with the input pattern distribution of the\npre-trained SD, thereby enabling more effective utilization of SD's generative\ncapabilities. To better activate the generative prior of SD at different\ntimesteps, we propose a Time-Aware VSD loss that bridges the timesteps of the\nstudent model and those of the teacher model, thereby producing more consistent\ngenerative prior guidance conditioned on timesteps. Additionally, though\nutilizing the generative prior in SD at different timesteps, our method can\nnaturally achieve controllable trade-offs between fidelity and realism by\nchanging the timestep condition. Experimental results demonstrate that our\nmethod achieves both state-of-the-art performance and controllable SR results\nwith only a single step.\n", "link": "http://arxiv.org/abs/2508.16557v1", "date": "2025-08-22", "relevancy": 1.8685, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.7027}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6105}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Time-Aware%20One%20Step%20Diffusion%20Network%20for%20Real-World%20Image%0A%20%20Super-Resolution&body=Title%3A%20Time-Aware%20One%20Step%20Diffusion%20Network%20for%20Real-World%20Image%0A%20%20Super-Resolution%0AAuthor%3A%20Tainyi%20Zhang%20and%20Zheng-Peng%20Duan%20and%20Peng-Tao%20Jiang%20and%20Bo%20Li%20and%20Ming-Ming%20Cheng%20and%20Chun-Le%20Guo%20and%20Chongyi%20Li%0AAbstract%3A%20%20%20Diffusion-based%20real-world%20image%20super-resolution%20%28Real-ISR%29%20methods%20have%0Ademonstrated%20impressive%20performance.%20To%20achieve%20efficient%20Real-ISR%2C%20many%20works%0Aemploy%20Variational%20Score%20Distillation%20%28VSD%29%20to%20distill%20pre-trained%0Astable-diffusion%20%28SD%29%20model%20for%20one-step%20SR%20with%20a%20fixed%20timestep.%20However%2C%20due%0Ato%20the%20different%20noise%20injection%20timesteps%2C%20the%20SD%20will%20perform%20different%0Agenerative%20priors.%20Therefore%2C%20a%20fixed%20timestep%20is%20difficult%20for%20these%20methods%0Ato%20fully%20leverage%20the%20generative%20priors%20in%20SD%2C%20leading%20to%20suboptimal%0Aperformance.%20To%20address%20this%2C%20we%20propose%20a%20Time-Aware%20one-step%20Diffusion%0ANetwork%20for%20Real-ISR%20%28TADSR%29.%20We%20first%20introduce%20a%20Time-Aware%20VAE%20Encoder%2C%0Awhich%20projects%20the%20same%20image%20into%20different%20latent%20features%20based%20on%0Atimesteps.%20Through%20joint%20dynamic%20variation%20of%20timesteps%20and%20latent%20features%2C%0Athe%20student%20model%20can%20better%20align%20with%20the%20input%20pattern%20distribution%20of%20the%0Apre-trained%20SD%2C%20thereby%20enabling%20more%20effective%20utilization%20of%20SD%27s%20generative%0Acapabilities.%20To%20better%20activate%20the%20generative%20prior%20of%20SD%20at%20different%0Atimesteps%2C%20we%20propose%20a%20Time-Aware%20VSD%20loss%20that%20bridges%20the%20timesteps%20of%20the%0Astudent%20model%20and%20those%20of%20the%20teacher%20model%2C%20thereby%20producing%20more%20consistent%0Agenerative%20prior%20guidance%20conditioned%20on%20timesteps.%20Additionally%2C%20though%0Autilizing%20the%20generative%20prior%20in%20SD%20at%20different%20timesteps%2C%20our%20method%20can%0Anaturally%20achieve%20controllable%20trade-offs%20between%20fidelity%20and%20realism%20by%0Achanging%20the%20timestep%20condition.%20Experimental%20results%20demonstrate%20that%20our%0Amethod%20achieves%20both%20state-of-the-art%20performance%20and%20controllable%20SR%20results%0Awith%20only%20a%20single%20step.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16557v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTime-Aware%2520One%2520Step%2520Diffusion%2520Network%2520for%2520Real-World%2520Image%250A%2520%2520Super-Resolution%26entry.906535625%3DTainyi%2520Zhang%2520and%2520Zheng-Peng%2520Duan%2520and%2520Peng-Tao%2520Jiang%2520and%2520Bo%2520Li%2520and%2520Ming-Ming%2520Cheng%2520and%2520Chun-Le%2520Guo%2520and%2520Chongyi%2520Li%26entry.1292438233%3D%2520%2520Diffusion-based%2520real-world%2520image%2520super-resolution%2520%2528Real-ISR%2529%2520methods%2520have%250Ademonstrated%2520impressive%2520performance.%2520To%2520achieve%2520efficient%2520Real-ISR%252C%2520many%2520works%250Aemploy%2520Variational%2520Score%2520Distillation%2520%2528VSD%2529%2520to%2520distill%2520pre-trained%250Astable-diffusion%2520%2528SD%2529%2520model%2520for%2520one-step%2520SR%2520with%2520a%2520fixed%2520timestep.%2520However%252C%2520due%250Ato%2520the%2520different%2520noise%2520injection%2520timesteps%252C%2520the%2520SD%2520will%2520perform%2520different%250Agenerative%2520priors.%2520Therefore%252C%2520a%2520fixed%2520timestep%2520is%2520difficult%2520for%2520these%2520methods%250Ato%2520fully%2520leverage%2520the%2520generative%2520priors%2520in%2520SD%252C%2520leading%2520to%2520suboptimal%250Aperformance.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520Time-Aware%2520one-step%2520Diffusion%250ANetwork%2520for%2520Real-ISR%2520%2528TADSR%2529.%2520We%2520first%2520introduce%2520a%2520Time-Aware%2520VAE%2520Encoder%252C%250Awhich%2520projects%2520the%2520same%2520image%2520into%2520different%2520latent%2520features%2520based%2520on%250Atimesteps.%2520Through%2520joint%2520dynamic%2520variation%2520of%2520timesteps%2520and%2520latent%2520features%252C%250Athe%2520student%2520model%2520can%2520better%2520align%2520with%2520the%2520input%2520pattern%2520distribution%2520of%2520the%250Apre-trained%2520SD%252C%2520thereby%2520enabling%2520more%2520effective%2520utilization%2520of%2520SD%2527s%2520generative%250Acapabilities.%2520To%2520better%2520activate%2520the%2520generative%2520prior%2520of%2520SD%2520at%2520different%250Atimesteps%252C%2520we%2520propose%2520a%2520Time-Aware%2520VSD%2520loss%2520that%2520bridges%2520the%2520timesteps%2520of%2520the%250Astudent%2520model%2520and%2520those%2520of%2520the%2520teacher%2520model%252C%2520thereby%2520producing%2520more%2520consistent%250Agenerative%2520prior%2520guidance%2520conditioned%2520on%2520timesteps.%2520Additionally%252C%2520though%250Autilizing%2520the%2520generative%2520prior%2520in%2520SD%2520at%2520different%2520timesteps%252C%2520our%2520method%2520can%250Anaturally%2520achieve%2520controllable%2520trade-offs%2520between%2520fidelity%2520and%2520realism%2520by%250Achanging%2520the%2520timestep%2520condition.%2520Experimental%2520results%2520demonstrate%2520that%2520our%250Amethod%2520achieves%2520both%2520state-of-the-art%2520performance%2520and%2520controllable%2520SR%2520results%250Awith%2520only%2520a%2520single%2520step.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16557v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Time-Aware%20One%20Step%20Diffusion%20Network%20for%20Real-World%20Image%0A%20%20Super-Resolution&entry.906535625=Tainyi%20Zhang%20and%20Zheng-Peng%20Duan%20and%20Peng-Tao%20Jiang%20and%20Bo%20Li%20and%20Ming-Ming%20Cheng%20and%20Chun-Le%20Guo%20and%20Chongyi%20Li&entry.1292438233=%20%20Diffusion-based%20real-world%20image%20super-resolution%20%28Real-ISR%29%20methods%20have%0Ademonstrated%20impressive%20performance.%20To%20achieve%20efficient%20Real-ISR%2C%20many%20works%0Aemploy%20Variational%20Score%20Distillation%20%28VSD%29%20to%20distill%20pre-trained%0Astable-diffusion%20%28SD%29%20model%20for%20one-step%20SR%20with%20a%20fixed%20timestep.%20However%2C%20due%0Ato%20the%20different%20noise%20injection%20timesteps%2C%20the%20SD%20will%20perform%20different%0Agenerative%20priors.%20Therefore%2C%20a%20fixed%20timestep%20is%20difficult%20for%20these%20methods%0Ato%20fully%20leverage%20the%20generative%20priors%20in%20SD%2C%20leading%20to%20suboptimal%0Aperformance.%20To%20address%20this%2C%20we%20propose%20a%20Time-Aware%20one-step%20Diffusion%0ANetwork%20for%20Real-ISR%20%28TADSR%29.%20We%20first%20introduce%20a%20Time-Aware%20VAE%20Encoder%2C%0Awhich%20projects%20the%20same%20image%20into%20different%20latent%20features%20based%20on%0Atimesteps.%20Through%20joint%20dynamic%20variation%20of%20timesteps%20and%20latent%20features%2C%0Athe%20student%20model%20can%20better%20align%20with%20the%20input%20pattern%20distribution%20of%20the%0Apre-trained%20SD%2C%20thereby%20enabling%20more%20effective%20utilization%20of%20SD%27s%20generative%0Acapabilities.%20To%20better%20activate%20the%20generative%20prior%20of%20SD%20at%20different%0Atimesteps%2C%20we%20propose%20a%20Time-Aware%20VSD%20loss%20that%20bridges%20the%20timesteps%20of%20the%0Astudent%20model%20and%20those%20of%20the%20teacher%20model%2C%20thereby%20producing%20more%20consistent%0Agenerative%20prior%20guidance%20conditioned%20on%20timesteps.%20Additionally%2C%20though%0Autilizing%20the%20generative%20prior%20in%20SD%20at%20different%20timesteps%2C%20our%20method%20can%0Anaturally%20achieve%20controllable%20trade-offs%20between%20fidelity%20and%20realism%20by%0Achanging%20the%20timestep%20condition.%20Experimental%20results%20demonstrate%20that%20our%0Amethod%20achieves%20both%20state-of-the-art%20performance%20and%20controllable%20SR%20results%0Awith%20only%20a%20single%20step.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16557v1&entry.124074799=Read"},
{"title": "PoisonSwarm: Universal Harmful Information Synthesis via Model\n  Crowdsourcing", "author": "Yu Yan and Sheng Sun and Zhifei Zheng and Ziji Hao and Teli Liu and Min Liu", "abstract": "  To construct responsible and secure AI applications, harmful information data\nis widely utilized for adversarial testing and the development of safeguards.\nExisting studies mainly leverage Large Language Models (LLMs) to synthesize\ndata to obtain high-quality task datasets at scale, thereby avoiding costly\nhuman annotation. However, limited by the safety alignment mechanisms of LLMs,\nthe synthesis of harmful data still faces challenges in generation reliability\nand content diversity. In this study, we propose a novel harmful information\nsynthesis framework, PoisonSwarm, which applies the model crowdsourcing\nstrategy to generate diverse harmful data while maintaining a high success\nrate. Specifically, we generate abundant benign data as the based templates in\na counterfactual manner. Subsequently, we decompose each based template into\nmultiple semantic units and perform unit-by-unit toxification and final\nrefinement through dynamic model switching, thus ensuring the success of\nsynthesis. Experimental results demonstrate that PoisonSwarm achieves\nstate-of-the-art performance in synthesizing different categories of harmful\ndata with high scalability and diversity.\n", "link": "http://arxiv.org/abs/2505.21184v2", "date": "2025-08-22", "relevancy": 1.865, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4744}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4682}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PoisonSwarm%3A%20Universal%20Harmful%20Information%20Synthesis%20via%20Model%0A%20%20Crowdsourcing&body=Title%3A%20PoisonSwarm%3A%20Universal%20Harmful%20Information%20Synthesis%20via%20Model%0A%20%20Crowdsourcing%0AAuthor%3A%20Yu%20Yan%20and%20Sheng%20Sun%20and%20Zhifei%20Zheng%20and%20Ziji%20Hao%20and%20Teli%20Liu%20and%20Min%20Liu%0AAbstract%3A%20%20%20To%20construct%20responsible%20and%20secure%20AI%20applications%2C%20harmful%20information%20data%0Ais%20widely%20utilized%20for%20adversarial%20testing%20and%20the%20development%20of%20safeguards.%0AExisting%20studies%20mainly%20leverage%20Large%20Language%20Models%20%28LLMs%29%20to%20synthesize%0Adata%20to%20obtain%20high-quality%20task%20datasets%20at%20scale%2C%20thereby%20avoiding%20costly%0Ahuman%20annotation.%20However%2C%20limited%20by%20the%20safety%20alignment%20mechanisms%20of%20LLMs%2C%0Athe%20synthesis%20of%20harmful%20data%20still%20faces%20challenges%20in%20generation%20reliability%0Aand%20content%20diversity.%20In%20this%20study%2C%20we%20propose%20a%20novel%20harmful%20information%0Asynthesis%20framework%2C%20PoisonSwarm%2C%20which%20applies%20the%20model%20crowdsourcing%0Astrategy%20to%20generate%20diverse%20harmful%20data%20while%20maintaining%20a%20high%20success%0Arate.%20Specifically%2C%20we%20generate%20abundant%20benign%20data%20as%20the%20based%20templates%20in%0Aa%20counterfactual%20manner.%20Subsequently%2C%20we%20decompose%20each%20based%20template%20into%0Amultiple%20semantic%20units%20and%20perform%20unit-by-unit%20toxification%20and%20final%0Arefinement%20through%20dynamic%20model%20switching%2C%20thus%20ensuring%20the%20success%20of%0Asynthesis.%20Experimental%20results%20demonstrate%20that%20PoisonSwarm%20achieves%0Astate-of-the-art%20performance%20in%20synthesizing%20different%20categories%20of%20harmful%0Adata%20with%20high%20scalability%20and%20diversity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21184v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoisonSwarm%253A%2520Universal%2520Harmful%2520Information%2520Synthesis%2520via%2520Model%250A%2520%2520Crowdsourcing%26entry.906535625%3DYu%2520Yan%2520and%2520Sheng%2520Sun%2520and%2520Zhifei%2520Zheng%2520and%2520Ziji%2520Hao%2520and%2520Teli%2520Liu%2520and%2520Min%2520Liu%26entry.1292438233%3D%2520%2520To%2520construct%2520responsible%2520and%2520secure%2520AI%2520applications%252C%2520harmful%2520information%2520data%250Ais%2520widely%2520utilized%2520for%2520adversarial%2520testing%2520and%2520the%2520development%2520of%2520safeguards.%250AExisting%2520studies%2520mainly%2520leverage%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520synthesize%250Adata%2520to%2520obtain%2520high-quality%2520task%2520datasets%2520at%2520scale%252C%2520thereby%2520avoiding%2520costly%250Ahuman%2520annotation.%2520However%252C%2520limited%2520by%2520the%2520safety%2520alignment%2520mechanisms%2520of%2520LLMs%252C%250Athe%2520synthesis%2520of%2520harmful%2520data%2520still%2520faces%2520challenges%2520in%2520generation%2520reliability%250Aand%2520content%2520diversity.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520novel%2520harmful%2520information%250Asynthesis%2520framework%252C%2520PoisonSwarm%252C%2520which%2520applies%2520the%2520model%2520crowdsourcing%250Astrategy%2520to%2520generate%2520diverse%2520harmful%2520data%2520while%2520maintaining%2520a%2520high%2520success%250Arate.%2520Specifically%252C%2520we%2520generate%2520abundant%2520benign%2520data%2520as%2520the%2520based%2520templates%2520in%250Aa%2520counterfactual%2520manner.%2520Subsequently%252C%2520we%2520decompose%2520each%2520based%2520template%2520into%250Amultiple%2520semantic%2520units%2520and%2520perform%2520unit-by-unit%2520toxification%2520and%2520final%250Arefinement%2520through%2520dynamic%2520model%2520switching%252C%2520thus%2520ensuring%2520the%2520success%2520of%250Asynthesis.%2520Experimental%2520results%2520demonstrate%2520that%2520PoisonSwarm%2520achieves%250Astate-of-the-art%2520performance%2520in%2520synthesizing%2520different%2520categories%2520of%2520harmful%250Adata%2520with%2520high%2520scalability%2520and%2520diversity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21184v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PoisonSwarm%3A%20Universal%20Harmful%20Information%20Synthesis%20via%20Model%0A%20%20Crowdsourcing&entry.906535625=Yu%20Yan%20and%20Sheng%20Sun%20and%20Zhifei%20Zheng%20and%20Ziji%20Hao%20and%20Teli%20Liu%20and%20Min%20Liu&entry.1292438233=%20%20To%20construct%20responsible%20and%20secure%20AI%20applications%2C%20harmful%20information%20data%0Ais%20widely%20utilized%20for%20adversarial%20testing%20and%20the%20development%20of%20safeguards.%0AExisting%20studies%20mainly%20leverage%20Large%20Language%20Models%20%28LLMs%29%20to%20synthesize%0Adata%20to%20obtain%20high-quality%20task%20datasets%20at%20scale%2C%20thereby%20avoiding%20costly%0Ahuman%20annotation.%20However%2C%20limited%20by%20the%20safety%20alignment%20mechanisms%20of%20LLMs%2C%0Athe%20synthesis%20of%20harmful%20data%20still%20faces%20challenges%20in%20generation%20reliability%0Aand%20content%20diversity.%20In%20this%20study%2C%20we%20propose%20a%20novel%20harmful%20information%0Asynthesis%20framework%2C%20PoisonSwarm%2C%20which%20applies%20the%20model%20crowdsourcing%0Astrategy%20to%20generate%20diverse%20harmful%20data%20while%20maintaining%20a%20high%20success%0Arate.%20Specifically%2C%20we%20generate%20abundant%20benign%20data%20as%20the%20based%20templates%20in%0Aa%20counterfactual%20manner.%20Subsequently%2C%20we%20decompose%20each%20based%20template%20into%0Amultiple%20semantic%20units%20and%20perform%20unit-by-unit%20toxification%20and%20final%0Arefinement%20through%20dynamic%20model%20switching%2C%20thus%20ensuring%20the%20success%20of%0Asynthesis.%20Experimental%20results%20demonstrate%20that%20PoisonSwarm%20achieves%0Astate-of-the-art%20performance%20in%20synthesizing%20different%20categories%20of%20harmful%0Adata%20with%20high%20scalability%20and%20diversity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21184v2&entry.124074799=Read"},
{"title": "Seeing Clearly, Forgetting Deeply: Revisiting Fine-Tuned Video\n  Generators for Driving Simulation", "author": "Chun-Peng Chang and Chen-Yu Wang and Julian Schmidt and Holger Caesar and Alain Pagani", "abstract": "  Recent advancements in video generation have substantially improved visual\nquality and temporal coherence, making these models increasingly appealing for\napplications such as autonomous driving, particularly in the context of driving\nsimulation and so-called \"world models\". In this work, we investigate the\neffects of existing fine-tuning video generation approaches on structured\ndriving datasets and uncover a potential trade-off: although visual fidelity\nimproves, spatial accuracy in modeling dynamic elements may degrade. We\nattribute this degradation to a shift in the alignment between visual quality\nand dynamic understanding objectives. In datasets with diverse scene structures\nwithin temporal space, where objects or perspective shift in varied ways, these\nobjectives tend to highly correlated. However, the very regular and repetitive\nnature of driving scenes allows visual quality to improve by modeling dominant\nscene motion patterns, without necessarily preserving fine-grained dynamic\nbehavior. As a result, fine-tuning encourages the model to prioritize\nsurface-level realism over dynamic accuracy. To further examine this\nphenomenon, we show that simple continual learning strategies, such as replay\nfrom diverse domains, can offer a balanced alternative by preserving spatial\naccuracy while maintaining strong visual quality.\n", "link": "http://arxiv.org/abs/2508.16512v1", "date": "2025-08-22", "relevancy": 1.861, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6377}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6063}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seeing%20Clearly%2C%20Forgetting%20Deeply%3A%20Revisiting%20Fine-Tuned%20Video%0A%20%20Generators%20for%20Driving%20Simulation&body=Title%3A%20Seeing%20Clearly%2C%20Forgetting%20Deeply%3A%20Revisiting%20Fine-Tuned%20Video%0A%20%20Generators%20for%20Driving%20Simulation%0AAuthor%3A%20Chun-Peng%20Chang%20and%20Chen-Yu%20Wang%20and%20Julian%20Schmidt%20and%20Holger%20Caesar%20and%20Alain%20Pagani%0AAbstract%3A%20%20%20Recent%20advancements%20in%20video%20generation%20have%20substantially%20improved%20visual%0Aquality%20and%20temporal%20coherence%2C%20making%20these%20models%20increasingly%20appealing%20for%0Aapplications%20such%20as%20autonomous%20driving%2C%20particularly%20in%20the%20context%20of%20driving%0Asimulation%20and%20so-called%20%22world%20models%22.%20In%20this%20work%2C%20we%20investigate%20the%0Aeffects%20of%20existing%20fine-tuning%20video%20generation%20approaches%20on%20structured%0Adriving%20datasets%20and%20uncover%20a%20potential%20trade-off%3A%20although%20visual%20fidelity%0Aimproves%2C%20spatial%20accuracy%20in%20modeling%20dynamic%20elements%20may%20degrade.%20We%0Aattribute%20this%20degradation%20to%20a%20shift%20in%20the%20alignment%20between%20visual%20quality%0Aand%20dynamic%20understanding%20objectives.%20In%20datasets%20with%20diverse%20scene%20structures%0Awithin%20temporal%20space%2C%20where%20objects%20or%20perspective%20shift%20in%20varied%20ways%2C%20these%0Aobjectives%20tend%20to%20highly%20correlated.%20However%2C%20the%20very%20regular%20and%20repetitive%0Anature%20of%20driving%20scenes%20allows%20visual%20quality%20to%20improve%20by%20modeling%20dominant%0Ascene%20motion%20patterns%2C%20without%20necessarily%20preserving%20fine-grained%20dynamic%0Abehavior.%20As%20a%20result%2C%20fine-tuning%20encourages%20the%20model%20to%20prioritize%0Asurface-level%20realism%20over%20dynamic%20accuracy.%20To%20further%20examine%20this%0Aphenomenon%2C%20we%20show%20that%20simple%20continual%20learning%20strategies%2C%20such%20as%20replay%0Afrom%20diverse%20domains%2C%20can%20offer%20a%20balanced%20alternative%20by%20preserving%20spatial%0Aaccuracy%20while%20maintaining%20strong%20visual%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16512v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeing%2520Clearly%252C%2520Forgetting%2520Deeply%253A%2520Revisiting%2520Fine-Tuned%2520Video%250A%2520%2520Generators%2520for%2520Driving%2520Simulation%26entry.906535625%3DChun-Peng%2520Chang%2520and%2520Chen-Yu%2520Wang%2520and%2520Julian%2520Schmidt%2520and%2520Holger%2520Caesar%2520and%2520Alain%2520Pagani%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520video%2520generation%2520have%2520substantially%2520improved%2520visual%250Aquality%2520and%2520temporal%2520coherence%252C%2520making%2520these%2520models%2520increasingly%2520appealing%2520for%250Aapplications%2520such%2520as%2520autonomous%2520driving%252C%2520particularly%2520in%2520the%2520context%2520of%2520driving%250Asimulation%2520and%2520so-called%2520%2522world%2520models%2522.%2520In%2520this%2520work%252C%2520we%2520investigate%2520the%250Aeffects%2520of%2520existing%2520fine-tuning%2520video%2520generation%2520approaches%2520on%2520structured%250Adriving%2520datasets%2520and%2520uncover%2520a%2520potential%2520trade-off%253A%2520although%2520visual%2520fidelity%250Aimproves%252C%2520spatial%2520accuracy%2520in%2520modeling%2520dynamic%2520elements%2520may%2520degrade.%2520We%250Aattribute%2520this%2520degradation%2520to%2520a%2520shift%2520in%2520the%2520alignment%2520between%2520visual%2520quality%250Aand%2520dynamic%2520understanding%2520objectives.%2520In%2520datasets%2520with%2520diverse%2520scene%2520structures%250Awithin%2520temporal%2520space%252C%2520where%2520objects%2520or%2520perspective%2520shift%2520in%2520varied%2520ways%252C%2520these%250Aobjectives%2520tend%2520to%2520highly%2520correlated.%2520However%252C%2520the%2520very%2520regular%2520and%2520repetitive%250Anature%2520of%2520driving%2520scenes%2520allows%2520visual%2520quality%2520to%2520improve%2520by%2520modeling%2520dominant%250Ascene%2520motion%2520patterns%252C%2520without%2520necessarily%2520preserving%2520fine-grained%2520dynamic%250Abehavior.%2520As%2520a%2520result%252C%2520fine-tuning%2520encourages%2520the%2520model%2520to%2520prioritize%250Asurface-level%2520realism%2520over%2520dynamic%2520accuracy.%2520To%2520further%2520examine%2520this%250Aphenomenon%252C%2520we%2520show%2520that%2520simple%2520continual%2520learning%2520strategies%252C%2520such%2520as%2520replay%250Afrom%2520diverse%2520domains%252C%2520can%2520offer%2520a%2520balanced%2520alternative%2520by%2520preserving%2520spatial%250Aaccuracy%2520while%2520maintaining%2520strong%2520visual%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16512v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%20Clearly%2C%20Forgetting%20Deeply%3A%20Revisiting%20Fine-Tuned%20Video%0A%20%20Generators%20for%20Driving%20Simulation&entry.906535625=Chun-Peng%20Chang%20and%20Chen-Yu%20Wang%20and%20Julian%20Schmidt%20and%20Holger%20Caesar%20and%20Alain%20Pagani&entry.1292438233=%20%20Recent%20advancements%20in%20video%20generation%20have%20substantially%20improved%20visual%0Aquality%20and%20temporal%20coherence%2C%20making%20these%20models%20increasingly%20appealing%20for%0Aapplications%20such%20as%20autonomous%20driving%2C%20particularly%20in%20the%20context%20of%20driving%0Asimulation%20and%20so-called%20%22world%20models%22.%20In%20this%20work%2C%20we%20investigate%20the%0Aeffects%20of%20existing%20fine-tuning%20video%20generation%20approaches%20on%20structured%0Adriving%20datasets%20and%20uncover%20a%20potential%20trade-off%3A%20although%20visual%20fidelity%0Aimproves%2C%20spatial%20accuracy%20in%20modeling%20dynamic%20elements%20may%20degrade.%20We%0Aattribute%20this%20degradation%20to%20a%20shift%20in%20the%20alignment%20between%20visual%20quality%0Aand%20dynamic%20understanding%20objectives.%20In%20datasets%20with%20diverse%20scene%20structures%0Awithin%20temporal%20space%2C%20where%20objects%20or%20perspective%20shift%20in%20varied%20ways%2C%20these%0Aobjectives%20tend%20to%20highly%20correlated.%20However%2C%20the%20very%20regular%20and%20repetitive%0Anature%20of%20driving%20scenes%20allows%20visual%20quality%20to%20improve%20by%20modeling%20dominant%0Ascene%20motion%20patterns%2C%20without%20necessarily%20preserving%20fine-grained%20dynamic%0Abehavior.%20As%20a%20result%2C%20fine-tuning%20encourages%20the%20model%20to%20prioritize%0Asurface-level%20realism%20over%20dynamic%20accuracy.%20To%20further%20examine%20this%0Aphenomenon%2C%20we%20show%20that%20simple%20continual%20learning%20strategies%2C%20such%20as%20replay%0Afrom%20diverse%20domains%2C%20can%20offer%20a%20balanced%20alternative%20by%20preserving%20spatial%0Aaccuracy%20while%20maintaining%20strong%20visual%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16512v1&entry.124074799=Read"},
{"title": "Probabilistic Pretraining for Neural Regression", "author": "Boris N. Oreshkin and Shiv Tavker and Dmitry Efimov", "abstract": "  Transfer learning for probabilistic regression remains underexplored. This\nwork closes this gap by introducing NIAQUE, Neural Interpretable Any-Quantile\nEstimation, a new model designed for transfer learning in probabilistic\nregression through permutation invariance. We demonstrate that pre-training\nNIAQUE directly on diverse downstream regression datasets and fine-tuning it on\na specific target dataset enhances performance on individual regression tasks,\nshowcasing the positive impact of probabilistic transfer learning. Furthermore,\nwe highlight the effectiveness of NIAQUE in Kaggle competitions against strong\nbaselines involving tree-based models and recent neural foundation models\nTabPFN and TabDPT. The findings highlight NIAQUE's efficacy as a robust and\nscalable framework for probabilistic regression, leveraging transfer learning\nto enhance predictive performance.\n", "link": "http://arxiv.org/abs/2508.16355v1", "date": "2025-08-22", "relevancy": 1.859, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4709}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4615}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probabilistic%20Pretraining%20for%20Neural%20Regression&body=Title%3A%20Probabilistic%20Pretraining%20for%20Neural%20Regression%0AAuthor%3A%20Boris%20N.%20Oreshkin%20and%20Shiv%20Tavker%20and%20Dmitry%20Efimov%0AAbstract%3A%20%20%20Transfer%20learning%20for%20probabilistic%20regression%20remains%20underexplored.%20This%0Awork%20closes%20this%20gap%20by%20introducing%20NIAQUE%2C%20Neural%20Interpretable%20Any-Quantile%0AEstimation%2C%20a%20new%20model%20designed%20for%20transfer%20learning%20in%20probabilistic%0Aregression%20through%20permutation%20invariance.%20We%20demonstrate%20that%20pre-training%0ANIAQUE%20directly%20on%20diverse%20downstream%20regression%20datasets%20and%20fine-tuning%20it%20on%0Aa%20specific%20target%20dataset%20enhances%20performance%20on%20individual%20regression%20tasks%2C%0Ashowcasing%20the%20positive%20impact%20of%20probabilistic%20transfer%20learning.%20Furthermore%2C%0Awe%20highlight%20the%20effectiveness%20of%20NIAQUE%20in%20Kaggle%20competitions%20against%20strong%0Abaselines%20involving%20tree-based%20models%20and%20recent%20neural%20foundation%20models%0ATabPFN%20and%20TabDPT.%20The%20findings%20highlight%20NIAQUE%27s%20efficacy%20as%20a%20robust%20and%0Ascalable%20framework%20for%20probabilistic%20regression%2C%20leveraging%20transfer%20learning%0Ato%20enhance%20predictive%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16355v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbabilistic%2520Pretraining%2520for%2520Neural%2520Regression%26entry.906535625%3DBoris%2520N.%2520Oreshkin%2520and%2520Shiv%2520Tavker%2520and%2520Dmitry%2520Efimov%26entry.1292438233%3D%2520%2520Transfer%2520learning%2520for%2520probabilistic%2520regression%2520remains%2520underexplored.%2520This%250Awork%2520closes%2520this%2520gap%2520by%2520introducing%2520NIAQUE%252C%2520Neural%2520Interpretable%2520Any-Quantile%250AEstimation%252C%2520a%2520new%2520model%2520designed%2520for%2520transfer%2520learning%2520in%2520probabilistic%250Aregression%2520through%2520permutation%2520invariance.%2520We%2520demonstrate%2520that%2520pre-training%250ANIAQUE%2520directly%2520on%2520diverse%2520downstream%2520regression%2520datasets%2520and%2520fine-tuning%2520it%2520on%250Aa%2520specific%2520target%2520dataset%2520enhances%2520performance%2520on%2520individual%2520regression%2520tasks%252C%250Ashowcasing%2520the%2520positive%2520impact%2520of%2520probabilistic%2520transfer%2520learning.%2520Furthermore%252C%250Awe%2520highlight%2520the%2520effectiveness%2520of%2520NIAQUE%2520in%2520Kaggle%2520competitions%2520against%2520strong%250Abaselines%2520involving%2520tree-based%2520models%2520and%2520recent%2520neural%2520foundation%2520models%250ATabPFN%2520and%2520TabDPT.%2520The%2520findings%2520highlight%2520NIAQUE%2527s%2520efficacy%2520as%2520a%2520robust%2520and%250Ascalable%2520framework%2520for%2520probabilistic%2520regression%252C%2520leveraging%2520transfer%2520learning%250Ato%2520enhance%2520predictive%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16355v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistic%20Pretraining%20for%20Neural%20Regression&entry.906535625=Boris%20N.%20Oreshkin%20and%20Shiv%20Tavker%20and%20Dmitry%20Efimov&entry.1292438233=%20%20Transfer%20learning%20for%20probabilistic%20regression%20remains%20underexplored.%20This%0Awork%20closes%20this%20gap%20by%20introducing%20NIAQUE%2C%20Neural%20Interpretable%20Any-Quantile%0AEstimation%2C%20a%20new%20model%20designed%20for%20transfer%20learning%20in%20probabilistic%0Aregression%20through%20permutation%20invariance.%20We%20demonstrate%20that%20pre-training%0ANIAQUE%20directly%20on%20diverse%20downstream%20regression%20datasets%20and%20fine-tuning%20it%20on%0Aa%20specific%20target%20dataset%20enhances%20performance%20on%20individual%20regression%20tasks%2C%0Ashowcasing%20the%20positive%20impact%20of%20probabilistic%20transfer%20learning.%20Furthermore%2C%0Awe%20highlight%20the%20effectiveness%20of%20NIAQUE%20in%20Kaggle%20competitions%20against%20strong%0Abaselines%20involving%20tree-based%20models%20and%20recent%20neural%20foundation%20models%0ATabPFN%20and%20TabDPT.%20The%20findings%20highlight%20NIAQUE%27s%20efficacy%20as%20a%20robust%20and%0Ascalable%20framework%20for%20probabilistic%20regression%2C%20leveraging%20transfer%20learning%0Ato%20enhance%20predictive%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16355v1&entry.124074799=Read"},
{"title": "Cetvel: A Unified Benchmark for Evaluating Language Understanding,\n  Generation and Cultural Capacity of LLMs for Turkish", "author": "Yakup Abrek Er and Ilker Kesen and G\u00f6zde G\u00fcl \u015eahin and Aykut Erdem", "abstract": "  We introduce Cetvel, a comprehensive benchmark designed to evaluate large\nlanguage models (LLMs) in Turkish. Existing Turkish benchmarks often lack\neither task diversity or culturally relevant content, or both. Cetvel addresses\nthese gaps by combining a broad range of both discriminative and generative\ntasks ensuring content that reflects the linguistic and cultural richness of\nTurkish language. Cetvel covers 23 tasks grouped into seven categories,\nincluding tasks such as grammatical error correction, machine translation, and\nquestion answering rooted in Turkish history and idiomatic language. We\nevaluate 33 open-weight LLMs (up to 70B parameters) covering different model\nfamilies and instruction paradigms. Our experiments reveal that Turkish-centric\ninstruction-tuned models generally underperform relative to multilingual or\ngeneral-purpose models (e.g. Llama 3 and Mistral), despite being tailored for\nthe language. Moreover, we show that tasks such as grammatical error correction\nand extractive question answering are particularly discriminative in\ndifferentiating model capabilities. Cetvel offers a comprehensive and\nculturally grounded evaluation suite for advancing the development and\nassessment of LLMs in Turkish.\n", "link": "http://arxiv.org/abs/2508.16431v1", "date": "2025-08-22", "relevancy": 1.8534, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4656}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4629}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cetvel%3A%20A%20Unified%20Benchmark%20for%20Evaluating%20Language%20Understanding%2C%0A%20%20Generation%20and%20Cultural%20Capacity%20of%20LLMs%20for%20Turkish&body=Title%3A%20Cetvel%3A%20A%20Unified%20Benchmark%20for%20Evaluating%20Language%20Understanding%2C%0A%20%20Generation%20and%20Cultural%20Capacity%20of%20LLMs%20for%20Turkish%0AAuthor%3A%20Yakup%20Abrek%20Er%20and%20Ilker%20Kesen%20and%20G%C3%B6zde%20G%C3%BCl%20%C5%9Eahin%20and%20Aykut%20Erdem%0AAbstract%3A%20%20%20We%20introduce%20Cetvel%2C%20a%20comprehensive%20benchmark%20designed%20to%20evaluate%20large%0Alanguage%20models%20%28LLMs%29%20in%20Turkish.%20Existing%20Turkish%20benchmarks%20often%20lack%0Aeither%20task%20diversity%20or%20culturally%20relevant%20content%2C%20or%20both.%20Cetvel%20addresses%0Athese%20gaps%20by%20combining%20a%20broad%20range%20of%20both%20discriminative%20and%20generative%0Atasks%20ensuring%20content%20that%20reflects%20the%20linguistic%20and%20cultural%20richness%20of%0ATurkish%20language.%20Cetvel%20covers%2023%20tasks%20grouped%20into%20seven%20categories%2C%0Aincluding%20tasks%20such%20as%20grammatical%20error%20correction%2C%20machine%20translation%2C%20and%0Aquestion%20answering%20rooted%20in%20Turkish%20history%20and%20idiomatic%20language.%20We%0Aevaluate%2033%20open-weight%20LLMs%20%28up%20to%2070B%20parameters%29%20covering%20different%20model%0Afamilies%20and%20instruction%20paradigms.%20Our%20experiments%20reveal%20that%20Turkish-centric%0Ainstruction-tuned%20models%20generally%20underperform%20relative%20to%20multilingual%20or%0Ageneral-purpose%20models%20%28e.g.%20Llama%203%20and%20Mistral%29%2C%20despite%20being%20tailored%20for%0Athe%20language.%20Moreover%2C%20we%20show%20that%20tasks%20such%20as%20grammatical%20error%20correction%0Aand%20extractive%20question%20answering%20are%20particularly%20discriminative%20in%0Adifferentiating%20model%20capabilities.%20Cetvel%20offers%20a%20comprehensive%20and%0Aculturally%20grounded%20evaluation%20suite%20for%20advancing%20the%20development%20and%0Aassessment%20of%20LLMs%20in%20Turkish.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16431v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCetvel%253A%2520A%2520Unified%2520Benchmark%2520for%2520Evaluating%2520Language%2520Understanding%252C%250A%2520%2520Generation%2520and%2520Cultural%2520Capacity%2520of%2520LLMs%2520for%2520Turkish%26entry.906535625%3DYakup%2520Abrek%2520Er%2520and%2520Ilker%2520Kesen%2520and%2520G%25C3%25B6zde%2520G%25C3%25BCl%2520%25C5%259Eahin%2520and%2520Aykut%2520Erdem%26entry.1292438233%3D%2520%2520We%2520introduce%2520Cetvel%252C%2520a%2520comprehensive%2520benchmark%2520designed%2520to%2520evaluate%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520in%2520Turkish.%2520Existing%2520Turkish%2520benchmarks%2520often%2520lack%250Aeither%2520task%2520diversity%2520or%2520culturally%2520relevant%2520content%252C%2520or%2520both.%2520Cetvel%2520addresses%250Athese%2520gaps%2520by%2520combining%2520a%2520broad%2520range%2520of%2520both%2520discriminative%2520and%2520generative%250Atasks%2520ensuring%2520content%2520that%2520reflects%2520the%2520linguistic%2520and%2520cultural%2520richness%2520of%250ATurkish%2520language.%2520Cetvel%2520covers%252023%2520tasks%2520grouped%2520into%2520seven%2520categories%252C%250Aincluding%2520tasks%2520such%2520as%2520grammatical%2520error%2520correction%252C%2520machine%2520translation%252C%2520and%250Aquestion%2520answering%2520rooted%2520in%2520Turkish%2520history%2520and%2520idiomatic%2520language.%2520We%250Aevaluate%252033%2520open-weight%2520LLMs%2520%2528up%2520to%252070B%2520parameters%2529%2520covering%2520different%2520model%250Afamilies%2520and%2520instruction%2520paradigms.%2520Our%2520experiments%2520reveal%2520that%2520Turkish-centric%250Ainstruction-tuned%2520models%2520generally%2520underperform%2520relative%2520to%2520multilingual%2520or%250Ageneral-purpose%2520models%2520%2528e.g.%2520Llama%25203%2520and%2520Mistral%2529%252C%2520despite%2520being%2520tailored%2520for%250Athe%2520language.%2520Moreover%252C%2520we%2520show%2520that%2520tasks%2520such%2520as%2520grammatical%2520error%2520correction%250Aand%2520extractive%2520question%2520answering%2520are%2520particularly%2520discriminative%2520in%250Adifferentiating%2520model%2520capabilities.%2520Cetvel%2520offers%2520a%2520comprehensive%2520and%250Aculturally%2520grounded%2520evaluation%2520suite%2520for%2520advancing%2520the%2520development%2520and%250Aassessment%2520of%2520LLMs%2520in%2520Turkish.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16431v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cetvel%3A%20A%20Unified%20Benchmark%20for%20Evaluating%20Language%20Understanding%2C%0A%20%20Generation%20and%20Cultural%20Capacity%20of%20LLMs%20for%20Turkish&entry.906535625=Yakup%20Abrek%20Er%20and%20Ilker%20Kesen%20and%20G%C3%B6zde%20G%C3%BCl%20%C5%9Eahin%20and%20Aykut%20Erdem&entry.1292438233=%20%20We%20introduce%20Cetvel%2C%20a%20comprehensive%20benchmark%20designed%20to%20evaluate%20large%0Alanguage%20models%20%28LLMs%29%20in%20Turkish.%20Existing%20Turkish%20benchmarks%20often%20lack%0Aeither%20task%20diversity%20or%20culturally%20relevant%20content%2C%20or%20both.%20Cetvel%20addresses%0Athese%20gaps%20by%20combining%20a%20broad%20range%20of%20both%20discriminative%20and%20generative%0Atasks%20ensuring%20content%20that%20reflects%20the%20linguistic%20and%20cultural%20richness%20of%0ATurkish%20language.%20Cetvel%20covers%2023%20tasks%20grouped%20into%20seven%20categories%2C%0Aincluding%20tasks%20such%20as%20grammatical%20error%20correction%2C%20machine%20translation%2C%20and%0Aquestion%20answering%20rooted%20in%20Turkish%20history%20and%20idiomatic%20language.%20We%0Aevaluate%2033%20open-weight%20LLMs%20%28up%20to%2070B%20parameters%29%20covering%20different%20model%0Afamilies%20and%20instruction%20paradigms.%20Our%20experiments%20reveal%20that%20Turkish-centric%0Ainstruction-tuned%20models%20generally%20underperform%20relative%20to%20multilingual%20or%0Ageneral-purpose%20models%20%28e.g.%20Llama%203%20and%20Mistral%29%2C%20despite%20being%20tailored%20for%0Athe%20language.%20Moreover%2C%20we%20show%20that%20tasks%20such%20as%20grammatical%20error%20correction%0Aand%20extractive%20question%20answering%20are%20particularly%20discriminative%20in%0Adifferentiating%20model%20capabilities.%20Cetvel%20offers%20a%20comprehensive%20and%0Aculturally%20grounded%20evaluation%20suite%20for%20advancing%20the%20development%20and%0Aassessment%20of%20LLMs%20in%20Turkish.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16431v1&entry.124074799=Read"},
{"title": "NeuroKoop: Neural Koopman Fusion of Structural-Functional Connectomes\n  for Identifying Prenatal Drug Exposure in Adolescents", "author": "Badhan Mazumder and Aline Kotoski and Vince D. Calhoun and Dong Hye Ye", "abstract": "  Understanding how prenatal exposure to psychoactive substances such as\ncannabis shapes adolescent brain organization remains a critical challenge,\ncomplicated by the complexity of multimodal neuroimaging data and the\nlimitations of conventional analytic methods. Existing approaches often fail to\nfully capture the complementary features embedded within structural and\nfunctional connectomes, constraining both biological insight and predictive\nperformance. To address this, we introduced NeuroKoop, a novel graph neural\nnetwork-based framework that integrates structural and functional brain\nnetworks utilizing neural Koopman operator-driven latent space fusion. By\nleveraging Koopman theory, NeuroKoop unifies node embeddings derived from\nsource-based morphometry (SBM) and functional network connectivity (FNC) based\nbrain graphs, resulting in enhanced representation learning and more robust\nclassification of prenatal drug exposure (PDE) status. Applied to a large\nadolescent cohort from the ABCD dataset, NeuroKoop outperformed relevant\nbaselines and revealed salient structural-functional connections, advancing our\nunderstanding of the neurodevelopmental impact of PDE.\n", "link": "http://arxiv.org/abs/2508.16414v1", "date": "2025-08-22", "relevancy": 1.8453, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.465}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4606}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeuroKoop%3A%20Neural%20Koopman%20Fusion%20of%20Structural-Functional%20Connectomes%0A%20%20for%20Identifying%20Prenatal%20Drug%20Exposure%20in%20Adolescents&body=Title%3A%20NeuroKoop%3A%20Neural%20Koopman%20Fusion%20of%20Structural-Functional%20Connectomes%0A%20%20for%20Identifying%20Prenatal%20Drug%20Exposure%20in%20Adolescents%0AAuthor%3A%20Badhan%20Mazumder%20and%20Aline%20Kotoski%20and%20Vince%20D.%20Calhoun%20and%20Dong%20Hye%20Ye%0AAbstract%3A%20%20%20Understanding%20how%20prenatal%20exposure%20to%20psychoactive%20substances%20such%20as%0Acannabis%20shapes%20adolescent%20brain%20organization%20remains%20a%20critical%20challenge%2C%0Acomplicated%20by%20the%20complexity%20of%20multimodal%20neuroimaging%20data%20and%20the%0Alimitations%20of%20conventional%20analytic%20methods.%20Existing%20approaches%20often%20fail%20to%0Afully%20capture%20the%20complementary%20features%20embedded%20within%20structural%20and%0Afunctional%20connectomes%2C%20constraining%20both%20biological%20insight%20and%20predictive%0Aperformance.%20To%20address%20this%2C%20we%20introduced%20NeuroKoop%2C%20a%20novel%20graph%20neural%0Anetwork-based%20framework%20that%20integrates%20structural%20and%20functional%20brain%0Anetworks%20utilizing%20neural%20Koopman%20operator-driven%20latent%20space%20fusion.%20By%0Aleveraging%20Koopman%20theory%2C%20NeuroKoop%20unifies%20node%20embeddings%20derived%20from%0Asource-based%20morphometry%20%28SBM%29%20and%20functional%20network%20connectivity%20%28FNC%29%20based%0Abrain%20graphs%2C%20resulting%20in%20enhanced%20representation%20learning%20and%20more%20robust%0Aclassification%20of%20prenatal%20drug%20exposure%20%28PDE%29%20status.%20Applied%20to%20a%20large%0Aadolescent%20cohort%20from%20the%20ABCD%20dataset%2C%20NeuroKoop%20outperformed%20relevant%0Abaselines%20and%20revealed%20salient%20structural-functional%20connections%2C%20advancing%20our%0Aunderstanding%20of%20the%20neurodevelopmental%20impact%20of%20PDE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16414v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuroKoop%253A%2520Neural%2520Koopman%2520Fusion%2520of%2520Structural-Functional%2520Connectomes%250A%2520%2520for%2520Identifying%2520Prenatal%2520Drug%2520Exposure%2520in%2520Adolescents%26entry.906535625%3DBadhan%2520Mazumder%2520and%2520Aline%2520Kotoski%2520and%2520Vince%2520D.%2520Calhoun%2520and%2520Dong%2520Hye%2520Ye%26entry.1292438233%3D%2520%2520Understanding%2520how%2520prenatal%2520exposure%2520to%2520psychoactive%2520substances%2520such%2520as%250Acannabis%2520shapes%2520adolescent%2520brain%2520organization%2520remains%2520a%2520critical%2520challenge%252C%250Acomplicated%2520by%2520the%2520complexity%2520of%2520multimodal%2520neuroimaging%2520data%2520and%2520the%250Alimitations%2520of%2520conventional%2520analytic%2520methods.%2520Existing%2520approaches%2520often%2520fail%2520to%250Afully%2520capture%2520the%2520complementary%2520features%2520embedded%2520within%2520structural%2520and%250Afunctional%2520connectomes%252C%2520constraining%2520both%2520biological%2520insight%2520and%2520predictive%250Aperformance.%2520To%2520address%2520this%252C%2520we%2520introduced%2520NeuroKoop%252C%2520a%2520novel%2520graph%2520neural%250Anetwork-based%2520framework%2520that%2520integrates%2520structural%2520and%2520functional%2520brain%250Anetworks%2520utilizing%2520neural%2520Koopman%2520operator-driven%2520latent%2520space%2520fusion.%2520By%250Aleveraging%2520Koopman%2520theory%252C%2520NeuroKoop%2520unifies%2520node%2520embeddings%2520derived%2520from%250Asource-based%2520morphometry%2520%2528SBM%2529%2520and%2520functional%2520network%2520connectivity%2520%2528FNC%2529%2520based%250Abrain%2520graphs%252C%2520resulting%2520in%2520enhanced%2520representation%2520learning%2520and%2520more%2520robust%250Aclassification%2520of%2520prenatal%2520drug%2520exposure%2520%2528PDE%2529%2520status.%2520Applied%2520to%2520a%2520large%250Aadolescent%2520cohort%2520from%2520the%2520ABCD%2520dataset%252C%2520NeuroKoop%2520outperformed%2520relevant%250Abaselines%2520and%2520revealed%2520salient%2520structural-functional%2520connections%252C%2520advancing%2520our%250Aunderstanding%2520of%2520the%2520neurodevelopmental%2520impact%2520of%2520PDE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16414v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuroKoop%3A%20Neural%20Koopman%20Fusion%20of%20Structural-Functional%20Connectomes%0A%20%20for%20Identifying%20Prenatal%20Drug%20Exposure%20in%20Adolescents&entry.906535625=Badhan%20Mazumder%20and%20Aline%20Kotoski%20and%20Vince%20D.%20Calhoun%20and%20Dong%20Hye%20Ye&entry.1292438233=%20%20Understanding%20how%20prenatal%20exposure%20to%20psychoactive%20substances%20such%20as%0Acannabis%20shapes%20adolescent%20brain%20organization%20remains%20a%20critical%20challenge%2C%0Acomplicated%20by%20the%20complexity%20of%20multimodal%20neuroimaging%20data%20and%20the%0Alimitations%20of%20conventional%20analytic%20methods.%20Existing%20approaches%20often%20fail%20to%0Afully%20capture%20the%20complementary%20features%20embedded%20within%20structural%20and%0Afunctional%20connectomes%2C%20constraining%20both%20biological%20insight%20and%20predictive%0Aperformance.%20To%20address%20this%2C%20we%20introduced%20NeuroKoop%2C%20a%20novel%20graph%20neural%0Anetwork-based%20framework%20that%20integrates%20structural%20and%20functional%20brain%0Anetworks%20utilizing%20neural%20Koopman%20operator-driven%20latent%20space%20fusion.%20By%0Aleveraging%20Koopman%20theory%2C%20NeuroKoop%20unifies%20node%20embeddings%20derived%20from%0Asource-based%20morphometry%20%28SBM%29%20and%20functional%20network%20connectivity%20%28FNC%29%20based%0Abrain%20graphs%2C%20resulting%20in%20enhanced%20representation%20learning%20and%20more%20robust%0Aclassification%20of%20prenatal%20drug%20exposure%20%28PDE%29%20status.%20Applied%20to%20a%20large%0Aadolescent%20cohort%20from%20the%20ABCD%20dataset%2C%20NeuroKoop%20outperformed%20relevant%0Abaselines%20and%20revealed%20salient%20structural-functional%20connections%2C%20advancing%20our%0Aunderstanding%20of%20the%20neurodevelopmental%20impact%20of%20PDE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16414v1&entry.124074799=Read"},
{"title": "Environmental Feature Engineering and Statistical Validation for\n  ML-Based Path Loss Prediction", "author": "Jonathan Ethier and Mathieu Chateauvert and Ryan G. Dempsey and Alexis Bose", "abstract": "  Wireless communications rely on path loss modeling, which is most effective\nwhen it includes the physical details of the propagation environment. Acquiring\nthis data has historically been challenging, but geographic information systems\ndata is becoming increasingly available with higher resolution and accuracy.\nAccess to such details enables propagation models to more accurately predict\ncoverage and account for interference in wireless deployments. Machine\nlearning-based modeling can significantly support this effort, with feature\nbased approaches allowing for accurate, efficient, and scalable propagation\nmodeling. Building on previous work, we introduce an extended set of features\nthat improves prediction accuracy while, most importantly, proving model\ngeneralization through rigorous statistical assessment and the use of test set\nholdouts.\n", "link": "http://arxiv.org/abs/2501.08306v3", "date": "2025-08-22", "relevancy": 1.8431, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4951}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4726}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4352}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Environmental%20Feature%20Engineering%20and%20Statistical%20Validation%20for%0A%20%20ML-Based%20Path%20Loss%20Prediction&body=Title%3A%20Environmental%20Feature%20Engineering%20and%20Statistical%20Validation%20for%0A%20%20ML-Based%20Path%20Loss%20Prediction%0AAuthor%3A%20Jonathan%20Ethier%20and%20Mathieu%20Chateauvert%20and%20Ryan%20G.%20Dempsey%20and%20Alexis%20Bose%0AAbstract%3A%20%20%20Wireless%20communications%20rely%20on%20path%20loss%20modeling%2C%20which%20is%20most%20effective%0Awhen%20it%20includes%20the%20physical%20details%20of%20the%20propagation%20environment.%20Acquiring%0Athis%20data%20has%20historically%20been%20challenging%2C%20but%20geographic%20information%20systems%0Adata%20is%20becoming%20increasingly%20available%20with%20higher%20resolution%20and%20accuracy.%0AAccess%20to%20such%20details%20enables%20propagation%20models%20to%20more%20accurately%20predict%0Acoverage%20and%20account%20for%20interference%20in%20wireless%20deployments.%20Machine%0Alearning-based%20modeling%20can%20significantly%20support%20this%20effort%2C%20with%20feature%0Abased%20approaches%20allowing%20for%20accurate%2C%20efficient%2C%20and%20scalable%20propagation%0Amodeling.%20Building%20on%20previous%20work%2C%20we%20introduce%20an%20extended%20set%20of%20features%0Athat%20improves%20prediction%20accuracy%20while%2C%20most%20importantly%2C%20proving%20model%0Ageneralization%20through%20rigorous%20statistical%20assessment%20and%20the%20use%20of%20test%20set%0Aholdouts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08306v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnvironmental%2520Feature%2520Engineering%2520and%2520Statistical%2520Validation%2520for%250A%2520%2520ML-Based%2520Path%2520Loss%2520Prediction%26entry.906535625%3DJonathan%2520Ethier%2520and%2520Mathieu%2520Chateauvert%2520and%2520Ryan%2520G.%2520Dempsey%2520and%2520Alexis%2520Bose%26entry.1292438233%3D%2520%2520Wireless%2520communications%2520rely%2520on%2520path%2520loss%2520modeling%252C%2520which%2520is%2520most%2520effective%250Awhen%2520it%2520includes%2520the%2520physical%2520details%2520of%2520the%2520propagation%2520environment.%2520Acquiring%250Athis%2520data%2520has%2520historically%2520been%2520challenging%252C%2520but%2520geographic%2520information%2520systems%250Adata%2520is%2520becoming%2520increasingly%2520available%2520with%2520higher%2520resolution%2520and%2520accuracy.%250AAccess%2520to%2520such%2520details%2520enables%2520propagation%2520models%2520to%2520more%2520accurately%2520predict%250Acoverage%2520and%2520account%2520for%2520interference%2520in%2520wireless%2520deployments.%2520Machine%250Alearning-based%2520modeling%2520can%2520significantly%2520support%2520this%2520effort%252C%2520with%2520feature%250Abased%2520approaches%2520allowing%2520for%2520accurate%252C%2520efficient%252C%2520and%2520scalable%2520propagation%250Amodeling.%2520Building%2520on%2520previous%2520work%252C%2520we%2520introduce%2520an%2520extended%2520set%2520of%2520features%250Athat%2520improves%2520prediction%2520accuracy%2520while%252C%2520most%2520importantly%252C%2520proving%2520model%250Ageneralization%2520through%2520rigorous%2520statistical%2520assessment%2520and%2520the%2520use%2520of%2520test%2520set%250Aholdouts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08306v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Environmental%20Feature%20Engineering%20and%20Statistical%20Validation%20for%0A%20%20ML-Based%20Path%20Loss%20Prediction&entry.906535625=Jonathan%20Ethier%20and%20Mathieu%20Chateauvert%20and%20Ryan%20G.%20Dempsey%20and%20Alexis%20Bose&entry.1292438233=%20%20Wireless%20communications%20rely%20on%20path%20loss%20modeling%2C%20which%20is%20most%20effective%0Awhen%20it%20includes%20the%20physical%20details%20of%20the%20propagation%20environment.%20Acquiring%0Athis%20data%20has%20historically%20been%20challenging%2C%20but%20geographic%20information%20systems%0Adata%20is%20becoming%20increasingly%20available%20with%20higher%20resolution%20and%20accuracy.%0AAccess%20to%20such%20details%20enables%20propagation%20models%20to%20more%20accurately%20predict%0Acoverage%20and%20account%20for%20interference%20in%20wireless%20deployments.%20Machine%0Alearning-based%20modeling%20can%20significantly%20support%20this%20effort%2C%20with%20feature%0Abased%20approaches%20allowing%20for%20accurate%2C%20efficient%2C%20and%20scalable%20propagation%0Amodeling.%20Building%20on%20previous%20work%2C%20we%20introduce%20an%20extended%20set%20of%20features%0Athat%20improves%20prediction%20accuracy%20while%2C%20most%20importantly%2C%20proving%20model%0Ageneralization%20through%20rigorous%20statistical%20assessment%20and%20the%20use%20of%20test%20set%0Aholdouts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08306v3&entry.124074799=Read"},
{"title": "A Curious Case of Remarkable Resilience to Gradient Attacks via Fully\n  Convolutional and Differentiable Front End with a Skip Connection", "author": "Leonid Boytsov and Ameya Joshi and Filipe Condessa", "abstract": "  We experimented with front-end enhanced neural models where a differentiable\nand fully convolutional model with a skip connection is added before a frozen\nbackbone classifier. By training such composite models using a small learning\nrate for about one epoch, we obtained models that retained the accuracy of the\nbackbone classifier while being unusually resistant to gradient\nattacks-including APGD and FAB-T attacks from the AutoAttack package-which we\nattribute to gradient masking. Although gradient masking is not new, the degree\nwe observe is striking for fully differentiable models without obvious\ngradient-shattering-e.g., JPEG compression-or gradient-diminishing components.\n  The training recipe to produce such models is also remarkably stable and\nreproducible: We applied it to three datasets (CIFAR10, CIFAR100, and ImageNet)\nand several modern architectures (including vision Transformers) without a\nsingle failure case. While black-box attacks such as the SQUARE attack and\nzero-order PGD can partially overcome gradient masking, these attacks are\neasily defeated by simple randomized ensembles. We estimate that these\nensembles achieve near-SOTA AutoAttack accuracy on CIFAR10, CIFAR100, and\nImageNet (while retaining almost all clean accuracy of the original\nclassifiers) despite having near-zero accuracy under adaptive attacks.\n  Adversarially training the backbone further amplifies this front-end\n\"robustness\". On CIFAR10, the respective randomized ensemble achieved 90.8$\\pm\n2.5\\%$ (99\\% CI) accuracy under the full AutoAttack while having only 18.2$\\pm\n3.6\\%$ accuracy under the adaptive attack ($\\varepsilon=8/255$, $L^\\infty$\nnorm). We conclude the paper with a discussion of whether randomized ensembling\ncan serve as a practical defense.\n  Code and instructions to reproduce key results are available.\nhttps://github.com/searchivarius/curious_case_of_gradient_masking\n", "link": "http://arxiv.org/abs/2402.17018v2", "date": "2025-08-22", "relevancy": 1.6682, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5671}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5458}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Curious%20Case%20of%20Remarkable%20Resilience%20to%20Gradient%20Attacks%20via%20Fully%0A%20%20Convolutional%20and%20Differentiable%20Front%20End%20with%20a%20Skip%20Connection&body=Title%3A%20A%20Curious%20Case%20of%20Remarkable%20Resilience%20to%20Gradient%20Attacks%20via%20Fully%0A%20%20Convolutional%20and%20Differentiable%20Front%20End%20with%20a%20Skip%20Connection%0AAuthor%3A%20Leonid%20Boytsov%20and%20Ameya%20Joshi%20and%20Filipe%20Condessa%0AAbstract%3A%20%20%20We%20experimented%20with%20front-end%20enhanced%20neural%20models%20where%20a%20differentiable%0Aand%20fully%20convolutional%20model%20with%20a%20skip%20connection%20is%20added%20before%20a%20frozen%0Abackbone%20classifier.%20By%20training%20such%20composite%20models%20using%20a%20small%20learning%0Arate%20for%20about%20one%20epoch%2C%20we%20obtained%20models%20that%20retained%20the%20accuracy%20of%20the%0Abackbone%20classifier%20while%20being%20unusually%20resistant%20to%20gradient%0Aattacks-including%20APGD%20and%20FAB-T%20attacks%20from%20the%20AutoAttack%20package-which%20we%0Aattribute%20to%20gradient%20masking.%20Although%20gradient%20masking%20is%20not%20new%2C%20the%20degree%0Awe%20observe%20is%20striking%20for%20fully%20differentiable%20models%20without%20obvious%0Agradient-shattering-e.g.%2C%20JPEG%20compression-or%20gradient-diminishing%20components.%0A%20%20The%20training%20recipe%20to%20produce%20such%20models%20is%20also%20remarkably%20stable%20and%0Areproducible%3A%20We%20applied%20it%20to%20three%20datasets%20%28CIFAR10%2C%20CIFAR100%2C%20and%20ImageNet%29%0Aand%20several%20modern%20architectures%20%28including%20vision%20Transformers%29%20without%20a%0Asingle%20failure%20case.%20While%20black-box%20attacks%20such%20as%20the%20SQUARE%20attack%20and%0Azero-order%20PGD%20can%20partially%20overcome%20gradient%20masking%2C%20these%20attacks%20are%0Aeasily%20defeated%20by%20simple%20randomized%20ensembles.%20We%20estimate%20that%20these%0Aensembles%20achieve%20near-SOTA%20AutoAttack%20accuracy%20on%20CIFAR10%2C%20CIFAR100%2C%20and%0AImageNet%20%28while%20retaining%20almost%20all%20clean%20accuracy%20of%20the%20original%0Aclassifiers%29%20despite%20having%20near-zero%20accuracy%20under%20adaptive%20attacks.%0A%20%20Adversarially%20training%20the%20backbone%20further%20amplifies%20this%20front-end%0A%22robustness%22.%20On%20CIFAR10%2C%20the%20respective%20randomized%20ensemble%20achieved%2090.8%24%5Cpm%0A2.5%5C%25%24%20%2899%5C%25%20CI%29%20accuracy%20under%20the%20full%20AutoAttack%20while%20having%20only%2018.2%24%5Cpm%0A3.6%5C%25%24%20accuracy%20under%20the%20adaptive%20attack%20%28%24%5Cvarepsilon%3D8/255%24%2C%20%24L%5E%5Cinfty%24%0Anorm%29.%20We%20conclude%20the%20paper%20with%20a%20discussion%20of%20whether%20randomized%20ensembling%0Acan%20serve%20as%20a%20practical%20defense.%0A%20%20Code%20and%20instructions%20to%20reproduce%20key%20results%20are%20available.%0Ahttps%3A//github.com/searchivarius/curious_case_of_gradient_masking%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.17018v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Curious%2520Case%2520of%2520Remarkable%2520Resilience%2520to%2520Gradient%2520Attacks%2520via%2520Fully%250A%2520%2520Convolutional%2520and%2520Differentiable%2520Front%2520End%2520with%2520a%2520Skip%2520Connection%26entry.906535625%3DLeonid%2520Boytsov%2520and%2520Ameya%2520Joshi%2520and%2520Filipe%2520Condessa%26entry.1292438233%3D%2520%2520We%2520experimented%2520with%2520front-end%2520enhanced%2520neural%2520models%2520where%2520a%2520differentiable%250Aand%2520fully%2520convolutional%2520model%2520with%2520a%2520skip%2520connection%2520is%2520added%2520before%2520a%2520frozen%250Abackbone%2520classifier.%2520By%2520training%2520such%2520composite%2520models%2520using%2520a%2520small%2520learning%250Arate%2520for%2520about%2520one%2520epoch%252C%2520we%2520obtained%2520models%2520that%2520retained%2520the%2520accuracy%2520of%2520the%250Abackbone%2520classifier%2520while%2520being%2520unusually%2520resistant%2520to%2520gradient%250Aattacks-including%2520APGD%2520and%2520FAB-T%2520attacks%2520from%2520the%2520AutoAttack%2520package-which%2520we%250Aattribute%2520to%2520gradient%2520masking.%2520Although%2520gradient%2520masking%2520is%2520not%2520new%252C%2520the%2520degree%250Awe%2520observe%2520is%2520striking%2520for%2520fully%2520differentiable%2520models%2520without%2520obvious%250Agradient-shattering-e.g.%252C%2520JPEG%2520compression-or%2520gradient-diminishing%2520components.%250A%2520%2520The%2520training%2520recipe%2520to%2520produce%2520such%2520models%2520is%2520also%2520remarkably%2520stable%2520and%250Areproducible%253A%2520We%2520applied%2520it%2520to%2520three%2520datasets%2520%2528CIFAR10%252C%2520CIFAR100%252C%2520and%2520ImageNet%2529%250Aand%2520several%2520modern%2520architectures%2520%2528including%2520vision%2520Transformers%2529%2520without%2520a%250Asingle%2520failure%2520case.%2520While%2520black-box%2520attacks%2520such%2520as%2520the%2520SQUARE%2520attack%2520and%250Azero-order%2520PGD%2520can%2520partially%2520overcome%2520gradient%2520masking%252C%2520these%2520attacks%2520are%250Aeasily%2520defeated%2520by%2520simple%2520randomized%2520ensembles.%2520We%2520estimate%2520that%2520these%250Aensembles%2520achieve%2520near-SOTA%2520AutoAttack%2520accuracy%2520on%2520CIFAR10%252C%2520CIFAR100%252C%2520and%250AImageNet%2520%2528while%2520retaining%2520almost%2520all%2520clean%2520accuracy%2520of%2520the%2520original%250Aclassifiers%2529%2520despite%2520having%2520near-zero%2520accuracy%2520under%2520adaptive%2520attacks.%250A%2520%2520Adversarially%2520training%2520the%2520backbone%2520further%2520amplifies%2520this%2520front-end%250A%2522robustness%2522.%2520On%2520CIFAR10%252C%2520the%2520respective%2520randomized%2520ensemble%2520achieved%252090.8%2524%255Cpm%250A2.5%255C%2525%2524%2520%252899%255C%2525%2520CI%2529%2520accuracy%2520under%2520the%2520full%2520AutoAttack%2520while%2520having%2520only%252018.2%2524%255Cpm%250A3.6%255C%2525%2524%2520accuracy%2520under%2520the%2520adaptive%2520attack%2520%2528%2524%255Cvarepsilon%253D8/255%2524%252C%2520%2524L%255E%255Cinfty%2524%250Anorm%2529.%2520We%2520conclude%2520the%2520paper%2520with%2520a%2520discussion%2520of%2520whether%2520randomized%2520ensembling%250Acan%2520serve%2520as%2520a%2520practical%2520defense.%250A%2520%2520Code%2520and%2520instructions%2520to%2520reproduce%2520key%2520results%2520are%2520available.%250Ahttps%253A//github.com/searchivarius/curious_case_of_gradient_masking%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.17018v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Curious%20Case%20of%20Remarkable%20Resilience%20to%20Gradient%20Attacks%20via%20Fully%0A%20%20Convolutional%20and%20Differentiable%20Front%20End%20with%20a%20Skip%20Connection&entry.906535625=Leonid%20Boytsov%20and%20Ameya%20Joshi%20and%20Filipe%20Condessa&entry.1292438233=%20%20We%20experimented%20with%20front-end%20enhanced%20neural%20models%20where%20a%20differentiable%0Aand%20fully%20convolutional%20model%20with%20a%20skip%20connection%20is%20added%20before%20a%20frozen%0Abackbone%20classifier.%20By%20training%20such%20composite%20models%20using%20a%20small%20learning%0Arate%20for%20about%20one%20epoch%2C%20we%20obtained%20models%20that%20retained%20the%20accuracy%20of%20the%0Abackbone%20classifier%20while%20being%20unusually%20resistant%20to%20gradient%0Aattacks-including%20APGD%20and%20FAB-T%20attacks%20from%20the%20AutoAttack%20package-which%20we%0Aattribute%20to%20gradient%20masking.%20Although%20gradient%20masking%20is%20not%20new%2C%20the%20degree%0Awe%20observe%20is%20striking%20for%20fully%20differentiable%20models%20without%20obvious%0Agradient-shattering-e.g.%2C%20JPEG%20compression-or%20gradient-diminishing%20components.%0A%20%20The%20training%20recipe%20to%20produce%20such%20models%20is%20also%20remarkably%20stable%20and%0Areproducible%3A%20We%20applied%20it%20to%20three%20datasets%20%28CIFAR10%2C%20CIFAR100%2C%20and%20ImageNet%29%0Aand%20several%20modern%20architectures%20%28including%20vision%20Transformers%29%20without%20a%0Asingle%20failure%20case.%20While%20black-box%20attacks%20such%20as%20the%20SQUARE%20attack%20and%0Azero-order%20PGD%20can%20partially%20overcome%20gradient%20masking%2C%20these%20attacks%20are%0Aeasily%20defeated%20by%20simple%20randomized%20ensembles.%20We%20estimate%20that%20these%0Aensembles%20achieve%20near-SOTA%20AutoAttack%20accuracy%20on%20CIFAR10%2C%20CIFAR100%2C%20and%0AImageNet%20%28while%20retaining%20almost%20all%20clean%20accuracy%20of%20the%20original%0Aclassifiers%29%20despite%20having%20near-zero%20accuracy%20under%20adaptive%20attacks.%0A%20%20Adversarially%20training%20the%20backbone%20further%20amplifies%20this%20front-end%0A%22robustness%22.%20On%20CIFAR10%2C%20the%20respective%20randomized%20ensemble%20achieved%2090.8%24%5Cpm%0A2.5%5C%25%24%20%2899%5C%25%20CI%29%20accuracy%20under%20the%20full%20AutoAttack%20while%20having%20only%2018.2%24%5Cpm%0A3.6%5C%25%24%20accuracy%20under%20the%20adaptive%20attack%20%28%24%5Cvarepsilon%3D8/255%24%2C%20%24L%5E%5Cinfty%24%0Anorm%29.%20We%20conclude%20the%20paper%20with%20a%20discussion%20of%20whether%20randomized%20ensembling%0Acan%20serve%20as%20a%20practical%20defense.%0A%20%20Code%20and%20instructions%20to%20reproduce%20key%20results%20are%20available.%0Ahttps%3A//github.com/searchivarius/curious_case_of_gradient_masking%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17018v2&entry.124074799=Read"},
{"title": "Disentangled Multi-modal Learning of Histology and Transcriptomics for\n  Cancer Characterization", "author": "Yupei Zhang and Xiaofei Wang and Anran Liu and Lequan Yu and Chao Li", "abstract": "  Histopathology remains the gold standard for cancer diagnosis and prognosis.\nWith the advent of transcriptome profiling, multi-modal learning combining\ntranscriptomics with histology offers more comprehensive information. However,\nexisting multi-modal approaches are challenged by intrinsic multi-modal\nheterogeneity, insufficient multi-scale integration, and reliance on paired\ndata, restricting clinical applicability. To address these challenges, we\npropose a disentangled multi-modal framework with four contributions: 1) To\nmitigate multi-modal heterogeneity, we decompose WSIs and transcriptomes into\ntumor and microenvironment subspaces using a disentangled multi-modal fusion\nmodule, and introduce a confidence-guided gradient coordination strategy to\nbalance subspace optimization. 2) To enhance multi-scale integration, we\npropose an inter-magnification gene-expression consistency strategy that aligns\ntranscriptomic signals across WSI magnifications. 3) To reduce dependency on\npaired data, we propose a subspace knowledge distillation strategy enabling\ntranscriptome-agnostic inference through a WSI-only student model. 4) To\nimprove inference efficiency, we propose an informative token aggregation\nmodule that suppresses WSI redundancy while preserving subspace semantics.\nExtensive experiments on cancer diagnosis, prognosis, and survival prediction\ndemonstrate our superiority over state-of-the-art methods across multiple\nsettings. Code is available at\nhttps://github.com/helenypzhang/Disentangled-Multimodal-Learning.\n", "link": "http://arxiv.org/abs/2508.16479v1", "date": "2025-08-22", "relevancy": 1.6389, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5722}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5242}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5036}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentangled%20Multi-modal%20Learning%20of%20Histology%20and%20Transcriptomics%20for%0A%20%20Cancer%20Characterization&body=Title%3A%20Disentangled%20Multi-modal%20Learning%20of%20Histology%20and%20Transcriptomics%20for%0A%20%20Cancer%20Characterization%0AAuthor%3A%20Yupei%20Zhang%20and%20Xiaofei%20Wang%20and%20Anran%20Liu%20and%20Lequan%20Yu%20and%20Chao%20Li%0AAbstract%3A%20%20%20Histopathology%20remains%20the%20gold%20standard%20for%20cancer%20diagnosis%20and%20prognosis.%0AWith%20the%20advent%20of%20transcriptome%20profiling%2C%20multi-modal%20learning%20combining%0Atranscriptomics%20with%20histology%20offers%20more%20comprehensive%20information.%20However%2C%0Aexisting%20multi-modal%20approaches%20are%20challenged%20by%20intrinsic%20multi-modal%0Aheterogeneity%2C%20insufficient%20multi-scale%20integration%2C%20and%20reliance%20on%20paired%0Adata%2C%20restricting%20clinical%20applicability.%20To%20address%20these%20challenges%2C%20we%0Apropose%20a%20disentangled%20multi-modal%20framework%20with%20four%20contributions%3A%201%29%20To%0Amitigate%20multi-modal%20heterogeneity%2C%20we%20decompose%20WSIs%20and%20transcriptomes%20into%0Atumor%20and%20microenvironment%20subspaces%20using%20a%20disentangled%20multi-modal%20fusion%0Amodule%2C%20and%20introduce%20a%20confidence-guided%20gradient%20coordination%20strategy%20to%0Abalance%20subspace%20optimization.%202%29%20To%20enhance%20multi-scale%20integration%2C%20we%0Apropose%20an%20inter-magnification%20gene-expression%20consistency%20strategy%20that%20aligns%0Atranscriptomic%20signals%20across%20WSI%20magnifications.%203%29%20To%20reduce%20dependency%20on%0Apaired%20data%2C%20we%20propose%20a%20subspace%20knowledge%20distillation%20strategy%20enabling%0Atranscriptome-agnostic%20inference%20through%20a%20WSI-only%20student%20model.%204%29%20To%0Aimprove%20inference%20efficiency%2C%20we%20propose%20an%20informative%20token%20aggregation%0Amodule%20that%20suppresses%20WSI%20redundancy%20while%20preserving%20subspace%20semantics.%0AExtensive%20experiments%20on%20cancer%20diagnosis%2C%20prognosis%2C%20and%20survival%20prediction%0Ademonstrate%20our%20superiority%20over%20state-of-the-art%20methods%20across%20multiple%0Asettings.%20Code%20is%20available%20at%0Ahttps%3A//github.com/helenypzhang/Disentangled-Multimodal-Learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16479v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentangled%2520Multi-modal%2520Learning%2520of%2520Histology%2520and%2520Transcriptomics%2520for%250A%2520%2520Cancer%2520Characterization%26entry.906535625%3DYupei%2520Zhang%2520and%2520Xiaofei%2520Wang%2520and%2520Anran%2520Liu%2520and%2520Lequan%2520Yu%2520and%2520Chao%2520Li%26entry.1292438233%3D%2520%2520Histopathology%2520remains%2520the%2520gold%2520standard%2520for%2520cancer%2520diagnosis%2520and%2520prognosis.%250AWith%2520the%2520advent%2520of%2520transcriptome%2520profiling%252C%2520multi-modal%2520learning%2520combining%250Atranscriptomics%2520with%2520histology%2520offers%2520more%2520comprehensive%2520information.%2520However%252C%250Aexisting%2520multi-modal%2520approaches%2520are%2520challenged%2520by%2520intrinsic%2520multi-modal%250Aheterogeneity%252C%2520insufficient%2520multi-scale%2520integration%252C%2520and%2520reliance%2520on%2520paired%250Adata%252C%2520restricting%2520clinical%2520applicability.%2520To%2520address%2520these%2520challenges%252C%2520we%250Apropose%2520a%2520disentangled%2520multi-modal%2520framework%2520with%2520four%2520contributions%253A%25201%2529%2520To%250Amitigate%2520multi-modal%2520heterogeneity%252C%2520we%2520decompose%2520WSIs%2520and%2520transcriptomes%2520into%250Atumor%2520and%2520microenvironment%2520subspaces%2520using%2520a%2520disentangled%2520multi-modal%2520fusion%250Amodule%252C%2520and%2520introduce%2520a%2520confidence-guided%2520gradient%2520coordination%2520strategy%2520to%250Abalance%2520subspace%2520optimization.%25202%2529%2520To%2520enhance%2520multi-scale%2520integration%252C%2520we%250Apropose%2520an%2520inter-magnification%2520gene-expression%2520consistency%2520strategy%2520that%2520aligns%250Atranscriptomic%2520signals%2520across%2520WSI%2520magnifications.%25203%2529%2520To%2520reduce%2520dependency%2520on%250Apaired%2520data%252C%2520we%2520propose%2520a%2520subspace%2520knowledge%2520distillation%2520strategy%2520enabling%250Atranscriptome-agnostic%2520inference%2520through%2520a%2520WSI-only%2520student%2520model.%25204%2529%2520To%250Aimprove%2520inference%2520efficiency%252C%2520we%2520propose%2520an%2520informative%2520token%2520aggregation%250Amodule%2520that%2520suppresses%2520WSI%2520redundancy%2520while%2520preserving%2520subspace%2520semantics.%250AExtensive%2520experiments%2520on%2520cancer%2520diagnosis%252C%2520prognosis%252C%2520and%2520survival%2520prediction%250Ademonstrate%2520our%2520superiority%2520over%2520state-of-the-art%2520methods%2520across%2520multiple%250Asettings.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/helenypzhang/Disentangled-Multimodal-Learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16479v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangled%20Multi-modal%20Learning%20of%20Histology%20and%20Transcriptomics%20for%0A%20%20Cancer%20Characterization&entry.906535625=Yupei%20Zhang%20and%20Xiaofei%20Wang%20and%20Anran%20Liu%20and%20Lequan%20Yu%20and%20Chao%20Li&entry.1292438233=%20%20Histopathology%20remains%20the%20gold%20standard%20for%20cancer%20diagnosis%20and%20prognosis.%0AWith%20the%20advent%20of%20transcriptome%20profiling%2C%20multi-modal%20learning%20combining%0Atranscriptomics%20with%20histology%20offers%20more%20comprehensive%20information.%20However%2C%0Aexisting%20multi-modal%20approaches%20are%20challenged%20by%20intrinsic%20multi-modal%0Aheterogeneity%2C%20insufficient%20multi-scale%20integration%2C%20and%20reliance%20on%20paired%0Adata%2C%20restricting%20clinical%20applicability.%20To%20address%20these%20challenges%2C%20we%0Apropose%20a%20disentangled%20multi-modal%20framework%20with%20four%20contributions%3A%201%29%20To%0Amitigate%20multi-modal%20heterogeneity%2C%20we%20decompose%20WSIs%20and%20transcriptomes%20into%0Atumor%20and%20microenvironment%20subspaces%20using%20a%20disentangled%20multi-modal%20fusion%0Amodule%2C%20and%20introduce%20a%20confidence-guided%20gradient%20coordination%20strategy%20to%0Abalance%20subspace%20optimization.%202%29%20To%20enhance%20multi-scale%20integration%2C%20we%0Apropose%20an%20inter-magnification%20gene-expression%20consistency%20strategy%20that%20aligns%0Atranscriptomic%20signals%20across%20WSI%20magnifications.%203%29%20To%20reduce%20dependency%20on%0Apaired%20data%2C%20we%20propose%20a%20subspace%20knowledge%20distillation%20strategy%20enabling%0Atranscriptome-agnostic%20inference%20through%20a%20WSI-only%20student%20model.%204%29%20To%0Aimprove%20inference%20efficiency%2C%20we%20propose%20an%20informative%20token%20aggregation%0Amodule%20that%20suppresses%20WSI%20redundancy%20while%20preserving%20subspace%20semantics.%0AExtensive%20experiments%20on%20cancer%20diagnosis%2C%20prognosis%2C%20and%20survival%20prediction%0Ademonstrate%20our%20superiority%20over%20state-of-the-art%20methods%20across%20multiple%0Asettings.%20Code%20is%20available%20at%0Ahttps%3A//github.com/helenypzhang/Disentangled-Multimodal-Learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16479v1&entry.124074799=Read"},
{"title": "Unsupervised Automata Learning via Discrete Optimization", "author": "Simon Lutz and Daniil Kaminskyi and Florian Wittbold and Simon Dierl and Falk Howar and Barbara K\u00f6nig and Emmanuel M\u00fcller and Daniel Neider", "abstract": "  Automata learning is a successful tool for many application domains such as\nrobotics and automatic verification. Typically, automata learning techniques\noperate in a supervised learning setting (active or passive) where they learn a\nfinite state machine in contexts where additional information, such as labeled\nsystem executions, is available. However, other settings, such as learning from\nunlabeled data - an important aspect in machine learning - remain unexplored.\nTo overcome this limitation, we propose a framework for learning a\ndeterministic finite automaton (DFA) from a given multi-set of unlabeled words.\nWe show that this problem is computationally hard and develop three learning\nalgorithms based on constraint optimization. Moreover, we introduce novel\nregularization schemes for our optimization problems that improve the overall\ninterpretability of our DFAs. Using a prototype implementation, we demonstrate\npractical feasibility in the context of unsupervised anomaly detection.\n", "link": "http://arxiv.org/abs/2303.14111v3", "date": "2025-08-22", "relevancy": 1.5245, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5349}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4885}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Automata%20Learning%20via%20Discrete%20Optimization&body=Title%3A%20Unsupervised%20Automata%20Learning%20via%20Discrete%20Optimization%0AAuthor%3A%20Simon%20Lutz%20and%20Daniil%20Kaminskyi%20and%20Florian%20Wittbold%20and%20Simon%20Dierl%20and%20Falk%20Howar%20and%20Barbara%20K%C3%B6nig%20and%20Emmanuel%20M%C3%BCller%20and%20Daniel%20Neider%0AAbstract%3A%20%20%20Automata%20learning%20is%20a%20successful%20tool%20for%20many%20application%20domains%20such%20as%0Arobotics%20and%20automatic%20verification.%20Typically%2C%20automata%20learning%20techniques%0Aoperate%20in%20a%20supervised%20learning%20setting%20%28active%20or%20passive%29%20where%20they%20learn%20a%0Afinite%20state%20machine%20in%20contexts%20where%20additional%20information%2C%20such%20as%20labeled%0Asystem%20executions%2C%20is%20available.%20However%2C%20other%20settings%2C%20such%20as%20learning%20from%0Aunlabeled%20data%20-%20an%20important%20aspect%20in%20machine%20learning%20-%20remain%20unexplored.%0ATo%20overcome%20this%20limitation%2C%20we%20propose%20a%20framework%20for%20learning%20a%0Adeterministic%20finite%20automaton%20%28DFA%29%20from%20a%20given%20multi-set%20of%20unlabeled%20words.%0AWe%20show%20that%20this%20problem%20is%20computationally%20hard%20and%20develop%20three%20learning%0Aalgorithms%20based%20on%20constraint%20optimization.%20Moreover%2C%20we%20introduce%20novel%0Aregularization%20schemes%20for%20our%20optimization%20problems%20that%20improve%20the%20overall%0Ainterpretability%20of%20our%20DFAs.%20Using%20a%20prototype%20implementation%2C%20we%20demonstrate%0Apractical%20feasibility%20in%20the%20context%20of%20unsupervised%20anomaly%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.14111v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Automata%2520Learning%2520via%2520Discrete%2520Optimization%26entry.906535625%3DSimon%2520Lutz%2520and%2520Daniil%2520Kaminskyi%2520and%2520Florian%2520Wittbold%2520and%2520Simon%2520Dierl%2520and%2520Falk%2520Howar%2520and%2520Barbara%2520K%25C3%25B6nig%2520and%2520Emmanuel%2520M%25C3%25BCller%2520and%2520Daniel%2520Neider%26entry.1292438233%3D%2520%2520Automata%2520learning%2520is%2520a%2520successful%2520tool%2520for%2520many%2520application%2520domains%2520such%2520as%250Arobotics%2520and%2520automatic%2520verification.%2520Typically%252C%2520automata%2520learning%2520techniques%250Aoperate%2520in%2520a%2520supervised%2520learning%2520setting%2520%2528active%2520or%2520passive%2529%2520where%2520they%2520learn%2520a%250Afinite%2520state%2520machine%2520in%2520contexts%2520where%2520additional%2520information%252C%2520such%2520as%2520labeled%250Asystem%2520executions%252C%2520is%2520available.%2520However%252C%2520other%2520settings%252C%2520such%2520as%2520learning%2520from%250Aunlabeled%2520data%2520-%2520an%2520important%2520aspect%2520in%2520machine%2520learning%2520-%2520remain%2520unexplored.%250ATo%2520overcome%2520this%2520limitation%252C%2520we%2520propose%2520a%2520framework%2520for%2520learning%2520a%250Adeterministic%2520finite%2520automaton%2520%2528DFA%2529%2520from%2520a%2520given%2520multi-set%2520of%2520unlabeled%2520words.%250AWe%2520show%2520that%2520this%2520problem%2520is%2520computationally%2520hard%2520and%2520develop%2520three%2520learning%250Aalgorithms%2520based%2520on%2520constraint%2520optimization.%2520Moreover%252C%2520we%2520introduce%2520novel%250Aregularization%2520schemes%2520for%2520our%2520optimization%2520problems%2520that%2520improve%2520the%2520overall%250Ainterpretability%2520of%2520our%2520DFAs.%2520Using%2520a%2520prototype%2520implementation%252C%2520we%2520demonstrate%250Apractical%2520feasibility%2520in%2520the%2520context%2520of%2520unsupervised%2520anomaly%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.14111v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Automata%20Learning%20via%20Discrete%20Optimization&entry.906535625=Simon%20Lutz%20and%20Daniil%20Kaminskyi%20and%20Florian%20Wittbold%20and%20Simon%20Dierl%20and%20Falk%20Howar%20and%20Barbara%20K%C3%B6nig%20and%20Emmanuel%20M%C3%BCller%20and%20Daniel%20Neider&entry.1292438233=%20%20Automata%20learning%20is%20a%20successful%20tool%20for%20many%20application%20domains%20such%20as%0Arobotics%20and%20automatic%20verification.%20Typically%2C%20automata%20learning%20techniques%0Aoperate%20in%20a%20supervised%20learning%20setting%20%28active%20or%20passive%29%20where%20they%20learn%20a%0Afinite%20state%20machine%20in%20contexts%20where%20additional%20information%2C%20such%20as%20labeled%0Asystem%20executions%2C%20is%20available.%20However%2C%20other%20settings%2C%20such%20as%20learning%20from%0Aunlabeled%20data%20-%20an%20important%20aspect%20in%20machine%20learning%20-%20remain%20unexplored.%0ATo%20overcome%20this%20limitation%2C%20we%20propose%20a%20framework%20for%20learning%20a%0Adeterministic%20finite%20automaton%20%28DFA%29%20from%20a%20given%20multi-set%20of%20unlabeled%20words.%0AWe%20show%20that%20this%20problem%20is%20computationally%20hard%20and%20develop%20three%20learning%0Aalgorithms%20based%20on%20constraint%20optimization.%20Moreover%2C%20we%20introduce%20novel%0Aregularization%20schemes%20for%20our%20optimization%20problems%20that%20improve%20the%20overall%0Ainterpretability%20of%20our%20DFAs.%20Using%20a%20prototype%20implementation%2C%20we%20demonstrate%0Apractical%20feasibility%20in%20the%20context%20of%20unsupervised%20anomaly%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.14111v3&entry.124074799=Read"},
{"title": "MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks", "author": "Mouath Abu Daoud and Chaimae Abouzahir and Leen Kharouf and Walid Al-Eisawi and Nizar Habash and Farah E. Shamout", "abstract": "  Large Language Models (LLMs) have demonstrated significant promise for\nvarious applications in healthcare. However, their efficacy in the Arabic\nmedical domain remains unexplored due to the lack of high-quality\ndomain-specific datasets and benchmarks. This study introduces MedArabiQ, a\nnovel benchmark dataset consisting of seven Arabic medical tasks, covering\nmultiple specialties and including multiple choice questions,\nfill-in-the-blank, and patient-doctor question answering. We first constructed\nthe dataset using past medical exams and publicly available datasets. We then\nintroduced different modifications to evaluate various LLM capabilities,\nincluding bias mitigation. We conducted an extensive evaluation with five\nstate-of-the-art open-source and proprietary LLMs, including GPT-4o, Claude\n3.5-Sonnet, and Gemini 1.5. Our findings highlight the need for the creation of\nnew high-quality benchmarks that span different languages to ensure fair\ndeployment and scalability of LLMs in healthcare. By establishing this\nbenchmark and releasing the dataset, we provide a foundation for future\nresearch aimed at evaluating and enhancing the multilingual capabilities of\nLLMs for the equitable use of generative AI in healthcare.\n", "link": "http://arxiv.org/abs/2505.03427v2", "date": "2025-08-22", "relevancy": 1.8325, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5089}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.448}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MedArabiQ%3A%20Benchmarking%20Large%20Language%20Models%20on%20Arabic%20Medical%20Tasks&body=Title%3A%20MedArabiQ%3A%20Benchmarking%20Large%20Language%20Models%20on%20Arabic%20Medical%20Tasks%0AAuthor%3A%20Mouath%20Abu%20Daoud%20and%20Chaimae%20Abouzahir%20and%20Leen%20Kharouf%20and%20Walid%20Al-Eisawi%20and%20Nizar%20Habash%20and%20Farah%20E.%20Shamout%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20significant%20promise%20for%0Avarious%20applications%20in%20healthcare.%20However%2C%20their%20efficacy%20in%20the%20Arabic%0Amedical%20domain%20remains%20unexplored%20due%20to%20the%20lack%20of%20high-quality%0Adomain-specific%20datasets%20and%20benchmarks.%20This%20study%20introduces%20MedArabiQ%2C%20a%0Anovel%20benchmark%20dataset%20consisting%20of%20seven%20Arabic%20medical%20tasks%2C%20covering%0Amultiple%20specialties%20and%20including%20multiple%20choice%20questions%2C%0Afill-in-the-blank%2C%20and%20patient-doctor%20question%20answering.%20We%20first%20constructed%0Athe%20dataset%20using%20past%20medical%20exams%20and%20publicly%20available%20datasets.%20We%20then%0Aintroduced%20different%20modifications%20to%20evaluate%20various%20LLM%20capabilities%2C%0Aincluding%20bias%20mitigation.%20We%20conducted%20an%20extensive%20evaluation%20with%20five%0Astate-of-the-art%20open-source%20and%20proprietary%20LLMs%2C%20including%20GPT-4o%2C%20Claude%0A3.5-Sonnet%2C%20and%20Gemini%201.5.%20Our%20findings%20highlight%20the%20need%20for%20the%20creation%20of%0Anew%20high-quality%20benchmarks%20that%20span%20different%20languages%20to%20ensure%20fair%0Adeployment%20and%20scalability%20of%20LLMs%20in%20healthcare.%20By%20establishing%20this%0Abenchmark%20and%20releasing%20the%20dataset%2C%20we%20provide%20a%20foundation%20for%20future%0Aresearch%20aimed%20at%20evaluating%20and%20enhancing%20the%20multilingual%20capabilities%20of%0ALLMs%20for%20the%20equitable%20use%20of%20generative%20AI%20in%20healthcare.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03427v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedArabiQ%253A%2520Benchmarking%2520Large%2520Language%2520Models%2520on%2520Arabic%2520Medical%2520Tasks%26entry.906535625%3DMouath%2520Abu%2520Daoud%2520and%2520Chaimae%2520Abouzahir%2520and%2520Leen%2520Kharouf%2520and%2520Walid%2520Al-Eisawi%2520and%2520Nizar%2520Habash%2520and%2520Farah%2520E.%2520Shamout%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520significant%2520promise%2520for%250Avarious%2520applications%2520in%2520healthcare.%2520However%252C%2520their%2520efficacy%2520in%2520the%2520Arabic%250Amedical%2520domain%2520remains%2520unexplored%2520due%2520to%2520the%2520lack%2520of%2520high-quality%250Adomain-specific%2520datasets%2520and%2520benchmarks.%2520This%2520study%2520introduces%2520MedArabiQ%252C%2520a%250Anovel%2520benchmark%2520dataset%2520consisting%2520of%2520seven%2520Arabic%2520medical%2520tasks%252C%2520covering%250Amultiple%2520specialties%2520and%2520including%2520multiple%2520choice%2520questions%252C%250Afill-in-the-blank%252C%2520and%2520patient-doctor%2520question%2520answering.%2520We%2520first%2520constructed%250Athe%2520dataset%2520using%2520past%2520medical%2520exams%2520and%2520publicly%2520available%2520datasets.%2520We%2520then%250Aintroduced%2520different%2520modifications%2520to%2520evaluate%2520various%2520LLM%2520capabilities%252C%250Aincluding%2520bias%2520mitigation.%2520We%2520conducted%2520an%2520extensive%2520evaluation%2520with%2520five%250Astate-of-the-art%2520open-source%2520and%2520proprietary%2520LLMs%252C%2520including%2520GPT-4o%252C%2520Claude%250A3.5-Sonnet%252C%2520and%2520Gemini%25201.5.%2520Our%2520findings%2520highlight%2520the%2520need%2520for%2520the%2520creation%2520of%250Anew%2520high-quality%2520benchmarks%2520that%2520span%2520different%2520languages%2520to%2520ensure%2520fair%250Adeployment%2520and%2520scalability%2520of%2520LLMs%2520in%2520healthcare.%2520By%2520establishing%2520this%250Abenchmark%2520and%2520releasing%2520the%2520dataset%252C%2520we%2520provide%2520a%2520foundation%2520for%2520future%250Aresearch%2520aimed%2520at%2520evaluating%2520and%2520enhancing%2520the%2520multilingual%2520capabilities%2520of%250ALLMs%2520for%2520the%2520equitable%2520use%2520of%2520generative%2520AI%2520in%2520healthcare.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03427v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedArabiQ%3A%20Benchmarking%20Large%20Language%20Models%20on%20Arabic%20Medical%20Tasks&entry.906535625=Mouath%20Abu%20Daoud%20and%20Chaimae%20Abouzahir%20and%20Leen%20Kharouf%20and%20Walid%20Al-Eisawi%20and%20Nizar%20Habash%20and%20Farah%20E.%20Shamout&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20significant%20promise%20for%0Avarious%20applications%20in%20healthcare.%20However%2C%20their%20efficacy%20in%20the%20Arabic%0Amedical%20domain%20remains%20unexplored%20due%20to%20the%20lack%20of%20high-quality%0Adomain-specific%20datasets%20and%20benchmarks.%20This%20study%20introduces%20MedArabiQ%2C%20a%0Anovel%20benchmark%20dataset%20consisting%20of%20seven%20Arabic%20medical%20tasks%2C%20covering%0Amultiple%20specialties%20and%20including%20multiple%20choice%20questions%2C%0Afill-in-the-blank%2C%20and%20patient-doctor%20question%20answering.%20We%20first%20constructed%0Athe%20dataset%20using%20past%20medical%20exams%20and%20publicly%20available%20datasets.%20We%20then%0Aintroduced%20different%20modifications%20to%20evaluate%20various%20LLM%20capabilities%2C%0Aincluding%20bias%20mitigation.%20We%20conducted%20an%20extensive%20evaluation%20with%20five%0Astate-of-the-art%20open-source%20and%20proprietary%20LLMs%2C%20including%20GPT-4o%2C%20Claude%0A3.5-Sonnet%2C%20and%20Gemini%201.5.%20Our%20findings%20highlight%20the%20need%20for%20the%20creation%20of%0Anew%20high-quality%20benchmarks%20that%20span%20different%20languages%20to%20ensure%20fair%0Adeployment%20and%20scalability%20of%20LLMs%20in%20healthcare.%20By%20establishing%20this%0Abenchmark%20and%20releasing%20the%20dataset%2C%20we%20provide%20a%20foundation%20for%20future%0Aresearch%20aimed%20at%20evaluating%20and%20enhancing%20the%20multilingual%20capabilities%20of%0ALLMs%20for%20the%20equitable%20use%20of%20generative%20AI%20in%20healthcare.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03427v2&entry.124074799=Read"},
{"title": "A Multimodal-Multitask Framework with Cross-modal Relation and\n  Hierarchical Interactive Attention for Semantic Comprehension", "author": "Mohammad Zia Ur Rehman and Devraj Raghuvanshi and Umang Jain and Shubhi Bansal and Nagendra Kumar", "abstract": "  A major challenge in multimodal learning is the presence of noise within\nindividual modalities. This noise inherently affects the resulting multimodal\nrepresentations, especially when these representations are obtained through\nexplicit interactions between different modalities. Moreover, the multimodal\nfusion techniques while aiming to achieve a strong joint representation, can\nneglect valuable discriminative information within the individual modalities.\nTo this end, we propose a Multimodal-Multitask framework with crOss-modal\nRelation and hIErarchical iNteractive aTtention (MM-ORIENT) that is effective\nfor multiple tasks. The proposed approach acquires multimodal representations\ncross-modally without explicit interaction between different modalities,\nreducing the noise effect at the latent stage. To achieve this, we propose\ncross-modal relation graphs that reconstruct monomodal features to acquire\nmultimodal representations. The features are reconstructed based on the node\nneighborhood, where the neighborhood is decided by the features of a different\nmodality. We also propose Hierarchical Interactive Monomadal Attention (HIMA)\nto focus on pertinent information within a modality. While cross-modal relation\ngraphs help comprehend high-order relationships between two modalities, HIMA\nhelps in multitasking by learning discriminative features of individual\nmodalities before late-fusing them. Finally, extensive experimental evaluation\non three datasets demonstrates that the proposed approach effectively\ncomprehends multimodal content for multiple tasks.\n", "link": "http://arxiv.org/abs/2508.16300v1", "date": "2025-08-22", "relevancy": 1.672, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5769}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5338}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multimodal-Multitask%20Framework%20with%20Cross-modal%20Relation%20and%0A%20%20Hierarchical%20Interactive%20Attention%20for%20Semantic%20Comprehension&body=Title%3A%20A%20Multimodal-Multitask%20Framework%20with%20Cross-modal%20Relation%20and%0A%20%20Hierarchical%20Interactive%20Attention%20for%20Semantic%20Comprehension%0AAuthor%3A%20Mohammad%20Zia%20Ur%20Rehman%20and%20Devraj%20Raghuvanshi%20and%20Umang%20Jain%20and%20Shubhi%20Bansal%20and%20Nagendra%20Kumar%0AAbstract%3A%20%20%20A%20major%20challenge%20in%20multimodal%20learning%20is%20the%20presence%20of%20noise%20within%0Aindividual%20modalities.%20This%20noise%20inherently%20affects%20the%20resulting%20multimodal%0Arepresentations%2C%20especially%20when%20these%20representations%20are%20obtained%20through%0Aexplicit%20interactions%20between%20different%20modalities.%20Moreover%2C%20the%20multimodal%0Afusion%20techniques%20while%20aiming%20to%20achieve%20a%20strong%20joint%20representation%2C%20can%0Aneglect%20valuable%20discriminative%20information%20within%20the%20individual%20modalities.%0ATo%20this%20end%2C%20we%20propose%20a%20Multimodal-Multitask%20framework%20with%20crOss-modal%0ARelation%20and%20hIErarchical%20iNteractive%20aTtention%20%28MM-ORIENT%29%20that%20is%20effective%0Afor%20multiple%20tasks.%20The%20proposed%20approach%20acquires%20multimodal%20representations%0Across-modally%20without%20explicit%20interaction%20between%20different%20modalities%2C%0Areducing%20the%20noise%20effect%20at%20the%20latent%20stage.%20To%20achieve%20this%2C%20we%20propose%0Across-modal%20relation%20graphs%20that%20reconstruct%20monomodal%20features%20to%20acquire%0Amultimodal%20representations.%20The%20features%20are%20reconstructed%20based%20on%20the%20node%0Aneighborhood%2C%20where%20the%20neighborhood%20is%20decided%20by%20the%20features%20of%20a%20different%0Amodality.%20We%20also%20propose%20Hierarchical%20Interactive%20Monomadal%20Attention%20%28HIMA%29%0Ato%20focus%20on%20pertinent%20information%20within%20a%20modality.%20While%20cross-modal%20relation%0Agraphs%20help%20comprehend%20high-order%20relationships%20between%20two%20modalities%2C%20HIMA%0Ahelps%20in%20multitasking%20by%20learning%20discriminative%20features%20of%20individual%0Amodalities%20before%20late-fusing%20them.%20Finally%2C%20extensive%20experimental%20evaluation%0Aon%20three%20datasets%20demonstrates%20that%20the%20proposed%20approach%20effectively%0Acomprehends%20multimodal%20content%20for%20multiple%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16300v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multimodal-Multitask%2520Framework%2520with%2520Cross-modal%2520Relation%2520and%250A%2520%2520Hierarchical%2520Interactive%2520Attention%2520for%2520Semantic%2520Comprehension%26entry.906535625%3DMohammad%2520Zia%2520Ur%2520Rehman%2520and%2520Devraj%2520Raghuvanshi%2520and%2520Umang%2520Jain%2520and%2520Shubhi%2520Bansal%2520and%2520Nagendra%2520Kumar%26entry.1292438233%3D%2520%2520A%2520major%2520challenge%2520in%2520multimodal%2520learning%2520is%2520the%2520presence%2520of%2520noise%2520within%250Aindividual%2520modalities.%2520This%2520noise%2520inherently%2520affects%2520the%2520resulting%2520multimodal%250Arepresentations%252C%2520especially%2520when%2520these%2520representations%2520are%2520obtained%2520through%250Aexplicit%2520interactions%2520between%2520different%2520modalities.%2520Moreover%252C%2520the%2520multimodal%250Afusion%2520techniques%2520while%2520aiming%2520to%2520achieve%2520a%2520strong%2520joint%2520representation%252C%2520can%250Aneglect%2520valuable%2520discriminative%2520information%2520within%2520the%2520individual%2520modalities.%250ATo%2520this%2520end%252C%2520we%2520propose%2520a%2520Multimodal-Multitask%2520framework%2520with%2520crOss-modal%250ARelation%2520and%2520hIErarchical%2520iNteractive%2520aTtention%2520%2528MM-ORIENT%2529%2520that%2520is%2520effective%250Afor%2520multiple%2520tasks.%2520The%2520proposed%2520approach%2520acquires%2520multimodal%2520representations%250Across-modally%2520without%2520explicit%2520interaction%2520between%2520different%2520modalities%252C%250Areducing%2520the%2520noise%2520effect%2520at%2520the%2520latent%2520stage.%2520To%2520achieve%2520this%252C%2520we%2520propose%250Across-modal%2520relation%2520graphs%2520that%2520reconstruct%2520monomodal%2520features%2520to%2520acquire%250Amultimodal%2520representations.%2520The%2520features%2520are%2520reconstructed%2520based%2520on%2520the%2520node%250Aneighborhood%252C%2520where%2520the%2520neighborhood%2520is%2520decided%2520by%2520the%2520features%2520of%2520a%2520different%250Amodality.%2520We%2520also%2520propose%2520Hierarchical%2520Interactive%2520Monomadal%2520Attention%2520%2528HIMA%2529%250Ato%2520focus%2520on%2520pertinent%2520information%2520within%2520a%2520modality.%2520While%2520cross-modal%2520relation%250Agraphs%2520help%2520comprehend%2520high-order%2520relationships%2520between%2520two%2520modalities%252C%2520HIMA%250Ahelps%2520in%2520multitasking%2520by%2520learning%2520discriminative%2520features%2520of%2520individual%250Amodalities%2520before%2520late-fusing%2520them.%2520Finally%252C%2520extensive%2520experimental%2520evaluation%250Aon%2520three%2520datasets%2520demonstrates%2520that%2520the%2520proposed%2520approach%2520effectively%250Acomprehends%2520multimodal%2520content%2520for%2520multiple%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16300v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multimodal-Multitask%20Framework%20with%20Cross-modal%20Relation%20and%0A%20%20Hierarchical%20Interactive%20Attention%20for%20Semantic%20Comprehension&entry.906535625=Mohammad%20Zia%20Ur%20Rehman%20and%20Devraj%20Raghuvanshi%20and%20Umang%20Jain%20and%20Shubhi%20Bansal%20and%20Nagendra%20Kumar&entry.1292438233=%20%20A%20major%20challenge%20in%20multimodal%20learning%20is%20the%20presence%20of%20noise%20within%0Aindividual%20modalities.%20This%20noise%20inherently%20affects%20the%20resulting%20multimodal%0Arepresentations%2C%20especially%20when%20these%20representations%20are%20obtained%20through%0Aexplicit%20interactions%20between%20different%20modalities.%20Moreover%2C%20the%20multimodal%0Afusion%20techniques%20while%20aiming%20to%20achieve%20a%20strong%20joint%20representation%2C%20can%0Aneglect%20valuable%20discriminative%20information%20within%20the%20individual%20modalities.%0ATo%20this%20end%2C%20we%20propose%20a%20Multimodal-Multitask%20framework%20with%20crOss-modal%0ARelation%20and%20hIErarchical%20iNteractive%20aTtention%20%28MM-ORIENT%29%20that%20is%20effective%0Afor%20multiple%20tasks.%20The%20proposed%20approach%20acquires%20multimodal%20representations%0Across-modally%20without%20explicit%20interaction%20between%20different%20modalities%2C%0Areducing%20the%20noise%20effect%20at%20the%20latent%20stage.%20To%20achieve%20this%2C%20we%20propose%0Across-modal%20relation%20graphs%20that%20reconstruct%20monomodal%20features%20to%20acquire%0Amultimodal%20representations.%20The%20features%20are%20reconstructed%20based%20on%20the%20node%0Aneighborhood%2C%20where%20the%20neighborhood%20is%20decided%20by%20the%20features%20of%20a%20different%0Amodality.%20We%20also%20propose%20Hierarchical%20Interactive%20Monomadal%20Attention%20%28HIMA%29%0Ato%20focus%20on%20pertinent%20information%20within%20a%20modality.%20While%20cross-modal%20relation%0Agraphs%20help%20comprehend%20high-order%20relationships%20between%20two%20modalities%2C%20HIMA%0Ahelps%20in%20multitasking%20by%20learning%20discriminative%20features%20of%20individual%0Amodalities%20before%20late-fusing%20them.%20Finally%2C%20extensive%20experimental%20evaluation%0Aon%20three%20datasets%20demonstrates%20that%20the%20proposed%20approach%20effectively%0Acomprehends%20multimodal%20content%20for%20multiple%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16300v1&entry.124074799=Read"},
{"title": "Ensembles of Neural Surrogates for Parametric Sensitivity in Ocean\n  Modeling", "author": "Yixuan Sun and Romain Egele and Sri Hari Krishna Narayana and Luke Van Roekel and Carmelo Gonzales and Steven Brus and Balu Nadiga and Sandeep Madireddy and Prasanna Balaprakash", "abstract": "  Accurate simulations of the oceans are crucial in understanding the Earth\nsystem. Despite their efficiency, simulations at lower resolutions must rely on\nvarious uncertain parameterizations to account for unresolved processes.\nHowever, model sensitivity to parameterizations is difficult to quantify,\nmaking it challenging to tune these parameterizations to reproduce\nobservations. Deep learning surrogates have shown promise for efficient\ncomputation of the parametric sensitivities in the form of partial derivatives,\nbut their reliability is difficult to evaluate without ground truth\nderivatives. In this work, we leverage large-scale hyperparameter search and\nensemble learning to improve both forward predictions, autoregressive rollout,\nand backward adjoint sensitivity estimation. Particularly, the ensemble method\nprovides epistemic uncertainty of function value predictions and their\nderivatives, providing improved reliability of the neural surrogates in\ndecision making.\n", "link": "http://arxiv.org/abs/2508.16489v1", "date": "2025-08-22", "relevancy": 1.5113, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5229}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5114}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ensembles%20of%20Neural%20Surrogates%20for%20Parametric%20Sensitivity%20in%20Ocean%0A%20%20Modeling&body=Title%3A%20Ensembles%20of%20Neural%20Surrogates%20for%20Parametric%20Sensitivity%20in%20Ocean%0A%20%20Modeling%0AAuthor%3A%20Yixuan%20Sun%20and%20Romain%20Egele%20and%20Sri%20Hari%20Krishna%20Narayana%20and%20Luke%20Van%20Roekel%20and%20Carmelo%20Gonzales%20and%20Steven%20Brus%20and%20Balu%20Nadiga%20and%20Sandeep%20Madireddy%20and%20Prasanna%20Balaprakash%0AAbstract%3A%20%20%20Accurate%20simulations%20of%20the%20oceans%20are%20crucial%20in%20understanding%20the%20Earth%0Asystem.%20Despite%20their%20efficiency%2C%20simulations%20at%20lower%20resolutions%20must%20rely%20on%0Avarious%20uncertain%20parameterizations%20to%20account%20for%20unresolved%20processes.%0AHowever%2C%20model%20sensitivity%20to%20parameterizations%20is%20difficult%20to%20quantify%2C%0Amaking%20it%20challenging%20to%20tune%20these%20parameterizations%20to%20reproduce%0Aobservations.%20Deep%20learning%20surrogates%20have%20shown%20promise%20for%20efficient%0Acomputation%20of%20the%20parametric%20sensitivities%20in%20the%20form%20of%20partial%20derivatives%2C%0Abut%20their%20reliability%20is%20difficult%20to%20evaluate%20without%20ground%20truth%0Aderivatives.%20In%20this%20work%2C%20we%20leverage%20large-scale%20hyperparameter%20search%20and%0Aensemble%20learning%20to%20improve%20both%20forward%20predictions%2C%20autoregressive%20rollout%2C%0Aand%20backward%20adjoint%20sensitivity%20estimation.%20Particularly%2C%20the%20ensemble%20method%0Aprovides%20epistemic%20uncertainty%20of%20function%20value%20predictions%20and%20their%0Aderivatives%2C%20providing%20improved%20reliability%20of%20the%20neural%20surrogates%20in%0Adecision%20making.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16489v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnsembles%2520of%2520Neural%2520Surrogates%2520for%2520Parametric%2520Sensitivity%2520in%2520Ocean%250A%2520%2520Modeling%26entry.906535625%3DYixuan%2520Sun%2520and%2520Romain%2520Egele%2520and%2520Sri%2520Hari%2520Krishna%2520Narayana%2520and%2520Luke%2520Van%2520Roekel%2520and%2520Carmelo%2520Gonzales%2520and%2520Steven%2520Brus%2520and%2520Balu%2520Nadiga%2520and%2520Sandeep%2520Madireddy%2520and%2520Prasanna%2520Balaprakash%26entry.1292438233%3D%2520%2520Accurate%2520simulations%2520of%2520the%2520oceans%2520are%2520crucial%2520in%2520understanding%2520the%2520Earth%250Asystem.%2520Despite%2520their%2520efficiency%252C%2520simulations%2520at%2520lower%2520resolutions%2520must%2520rely%2520on%250Avarious%2520uncertain%2520parameterizations%2520to%2520account%2520for%2520unresolved%2520processes.%250AHowever%252C%2520model%2520sensitivity%2520to%2520parameterizations%2520is%2520difficult%2520to%2520quantify%252C%250Amaking%2520it%2520challenging%2520to%2520tune%2520these%2520parameterizations%2520to%2520reproduce%250Aobservations.%2520Deep%2520learning%2520surrogates%2520have%2520shown%2520promise%2520for%2520efficient%250Acomputation%2520of%2520the%2520parametric%2520sensitivities%2520in%2520the%2520form%2520of%2520partial%2520derivatives%252C%250Abut%2520their%2520reliability%2520is%2520difficult%2520to%2520evaluate%2520without%2520ground%2520truth%250Aderivatives.%2520In%2520this%2520work%252C%2520we%2520leverage%2520large-scale%2520hyperparameter%2520search%2520and%250Aensemble%2520learning%2520to%2520improve%2520both%2520forward%2520predictions%252C%2520autoregressive%2520rollout%252C%250Aand%2520backward%2520adjoint%2520sensitivity%2520estimation.%2520Particularly%252C%2520the%2520ensemble%2520method%250Aprovides%2520epistemic%2520uncertainty%2520of%2520function%2520value%2520predictions%2520and%2520their%250Aderivatives%252C%2520providing%2520improved%2520reliability%2520of%2520the%2520neural%2520surrogates%2520in%250Adecision%2520making.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16489v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ensembles%20of%20Neural%20Surrogates%20for%20Parametric%20Sensitivity%20in%20Ocean%0A%20%20Modeling&entry.906535625=Yixuan%20Sun%20and%20Romain%20Egele%20and%20Sri%20Hari%20Krishna%20Narayana%20and%20Luke%20Van%20Roekel%20and%20Carmelo%20Gonzales%20and%20Steven%20Brus%20and%20Balu%20Nadiga%20and%20Sandeep%20Madireddy%20and%20Prasanna%20Balaprakash&entry.1292438233=%20%20Accurate%20simulations%20of%20the%20oceans%20are%20crucial%20in%20understanding%20the%20Earth%0Asystem.%20Despite%20their%20efficiency%2C%20simulations%20at%20lower%20resolutions%20must%20rely%20on%0Avarious%20uncertain%20parameterizations%20to%20account%20for%20unresolved%20processes.%0AHowever%2C%20model%20sensitivity%20to%20parameterizations%20is%20difficult%20to%20quantify%2C%0Amaking%20it%20challenging%20to%20tune%20these%20parameterizations%20to%20reproduce%0Aobservations.%20Deep%20learning%20surrogates%20have%20shown%20promise%20for%20efficient%0Acomputation%20of%20the%20parametric%20sensitivities%20in%20the%20form%20of%20partial%20derivatives%2C%0Abut%20their%20reliability%20is%20difficult%20to%20evaluate%20without%20ground%20truth%0Aderivatives.%20In%20this%20work%2C%20we%20leverage%20large-scale%20hyperparameter%20search%20and%0Aensemble%20learning%20to%20improve%20both%20forward%20predictions%2C%20autoregressive%20rollout%2C%0Aand%20backward%20adjoint%20sensitivity%20estimation.%20Particularly%2C%20the%20ensemble%20method%0Aprovides%20epistemic%20uncertainty%20of%20function%20value%20predictions%20and%20their%0Aderivatives%2C%20providing%20improved%20reliability%20of%20the%20neural%20surrogates%20in%0Adecision%20making.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16489v1&entry.124074799=Read"},
{"title": "Adaptive Task Space Non-Singular Terminal Super-Twisting Sliding Mode\n  Control of a 7-DOF Robotic Manipulator", "author": "L. Wan and S. Smith and Y. -J. Pan and E. Witrant", "abstract": "  This paper presents a new task-space Non-singular Terminal Super-Twisting\nSliding Mode (NT-STSM) controller with adaptive gains for robust trajectory\ntracking of a 7-DOF robotic manipulator. The proposed approach addresses the\nchallenges of chattering, unknown disturbances, and rotational motion tracking,\nmaking it suited for high-DOF manipulators in dexterous manipulation tasks. A\nrigorous boundedness proof is provided, offering gain selection guidelines for\npractical implementation. Simulations and hardware experiments with external\ndisturbances demonstrate the proposed controller's robust, accurate tracking\nwith reduced control effort under unknown disturbances compared to other\nNT-STSM and conventional controllers. The results demonstrated that the\nproposed NT-STSM controller mitigates chattering and instability in complex\nmotions, making it a viable solution for dexterous robotic manipulations and\nvarious industrial applications.\n", "link": "http://arxiv.org/abs/2504.13056v2", "date": "2025-08-22", "relevancy": 1.5148, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5284}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5104}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4933}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Task%20Space%20Non-Singular%20Terminal%20Super-Twisting%20Sliding%20Mode%0A%20%20Control%20of%20a%207-DOF%20Robotic%20Manipulator&body=Title%3A%20Adaptive%20Task%20Space%20Non-Singular%20Terminal%20Super-Twisting%20Sliding%20Mode%0A%20%20Control%20of%20a%207-DOF%20Robotic%20Manipulator%0AAuthor%3A%20L.%20Wan%20and%20S.%20Smith%20and%20Y.%20-J.%20Pan%20and%20E.%20Witrant%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20new%20task-space%20Non-singular%20Terminal%20Super-Twisting%0ASliding%20Mode%20%28NT-STSM%29%20controller%20with%20adaptive%20gains%20for%20robust%20trajectory%0Atracking%20of%20a%207-DOF%20robotic%20manipulator.%20The%20proposed%20approach%20addresses%20the%0Achallenges%20of%20chattering%2C%20unknown%20disturbances%2C%20and%20rotational%20motion%20tracking%2C%0Amaking%20it%20suited%20for%20high-DOF%20manipulators%20in%20dexterous%20manipulation%20tasks.%20A%0Arigorous%20boundedness%20proof%20is%20provided%2C%20offering%20gain%20selection%20guidelines%20for%0Apractical%20implementation.%20Simulations%20and%20hardware%20experiments%20with%20external%0Adisturbances%20demonstrate%20the%20proposed%20controller%27s%20robust%2C%20accurate%20tracking%0Awith%20reduced%20control%20effort%20under%20unknown%20disturbances%20compared%20to%20other%0ANT-STSM%20and%20conventional%20controllers.%20The%20results%20demonstrated%20that%20the%0Aproposed%20NT-STSM%20controller%20mitigates%20chattering%20and%20instability%20in%20complex%0Amotions%2C%20making%20it%20a%20viable%20solution%20for%20dexterous%20robotic%20manipulations%20and%0Avarious%20industrial%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13056v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Task%2520Space%2520Non-Singular%2520Terminal%2520Super-Twisting%2520Sliding%2520Mode%250A%2520%2520Control%2520of%2520a%25207-DOF%2520Robotic%2520Manipulator%26entry.906535625%3DL.%2520Wan%2520and%2520S.%2520Smith%2520and%2520Y.%2520-J.%2520Pan%2520and%2520E.%2520Witrant%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520new%2520task-space%2520Non-singular%2520Terminal%2520Super-Twisting%250ASliding%2520Mode%2520%2528NT-STSM%2529%2520controller%2520with%2520adaptive%2520gains%2520for%2520robust%2520trajectory%250Atracking%2520of%2520a%25207-DOF%2520robotic%2520manipulator.%2520The%2520proposed%2520approach%2520addresses%2520the%250Achallenges%2520of%2520chattering%252C%2520unknown%2520disturbances%252C%2520and%2520rotational%2520motion%2520tracking%252C%250Amaking%2520it%2520suited%2520for%2520high-DOF%2520manipulators%2520in%2520dexterous%2520manipulation%2520tasks.%2520A%250Arigorous%2520boundedness%2520proof%2520is%2520provided%252C%2520offering%2520gain%2520selection%2520guidelines%2520for%250Apractical%2520implementation.%2520Simulations%2520and%2520hardware%2520experiments%2520with%2520external%250Adisturbances%2520demonstrate%2520the%2520proposed%2520controller%2527s%2520robust%252C%2520accurate%2520tracking%250Awith%2520reduced%2520control%2520effort%2520under%2520unknown%2520disturbances%2520compared%2520to%2520other%250ANT-STSM%2520and%2520conventional%2520controllers.%2520The%2520results%2520demonstrated%2520that%2520the%250Aproposed%2520NT-STSM%2520controller%2520mitigates%2520chattering%2520and%2520instability%2520in%2520complex%250Amotions%252C%2520making%2520it%2520a%2520viable%2520solution%2520for%2520dexterous%2520robotic%2520manipulations%2520and%250Avarious%2520industrial%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13056v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Task%20Space%20Non-Singular%20Terminal%20Super-Twisting%20Sliding%20Mode%0A%20%20Control%20of%20a%207-DOF%20Robotic%20Manipulator&entry.906535625=L.%20Wan%20and%20S.%20Smith%20and%20Y.%20-J.%20Pan%20and%20E.%20Witrant&entry.1292438233=%20%20This%20paper%20presents%20a%20new%20task-space%20Non-singular%20Terminal%20Super-Twisting%0ASliding%20Mode%20%28NT-STSM%29%20controller%20with%20adaptive%20gains%20for%20robust%20trajectory%0Atracking%20of%20a%207-DOF%20robotic%20manipulator.%20The%20proposed%20approach%20addresses%20the%0Achallenges%20of%20chattering%2C%20unknown%20disturbances%2C%20and%20rotational%20motion%20tracking%2C%0Amaking%20it%20suited%20for%20high-DOF%20manipulators%20in%20dexterous%20manipulation%20tasks.%20A%0Arigorous%20boundedness%20proof%20is%20provided%2C%20offering%20gain%20selection%20guidelines%20for%0Apractical%20implementation.%20Simulations%20and%20hardware%20experiments%20with%20external%0Adisturbances%20demonstrate%20the%20proposed%20controller%27s%20robust%2C%20accurate%20tracking%0Awith%20reduced%20control%20effort%20under%20unknown%20disturbances%20compared%20to%20other%0ANT-STSM%20and%20conventional%20controllers.%20The%20results%20demonstrated%20that%20the%0Aproposed%20NT-STSM%20controller%20mitigates%20chattering%20and%20instability%20in%20complex%0Amotions%2C%20making%20it%20a%20viable%20solution%20for%20dexterous%20robotic%20manipulations%20and%0Avarious%20industrial%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13056v2&entry.124074799=Read"},
{"title": "Optimized Lattice-Structured Flexible EIT Sensor for Tactile\n  Reconstruction and Classification", "author": "Huazhi Dong and Sihao Teng and Xu Han and Xiaopeng Wu and Francesco Giorgio-Serchi and Yunjie Yang", "abstract": "  Flexible electrical impedance tomography (EIT) offers a promising alternative\nto traditional tactile sensing approaches, enabling low-cost, scalable, and\ndeformable sensor designs. Here, we propose an optimized lattice-structured\nflexible EIT tactile sensor incorporating a hydrogel-based conductive layer,\nsystematically designed through three-dimensional coupling field simulations to\noptimize structural parameters for enhanced sensitivity and robustness. By\ntuning the lattice channel width and conductive layer thickness, we achieve\nsignificant improvements in tactile reconstruction quality and classification\nperformance. Experimental results demonstrate high-quality tactile\nreconstruction with correlation coefficients up to 0.9275, peak signal-to-noise\nratios reaching 29.0303 dB, and structural similarity indexes up to 0.9660,\nwhile maintaining low relative errors down to 0.3798. Furthermore, the\noptimized sensor accurately classifies 12 distinct tactile stimuli with an\naccuracy reaching 99.6%. These results highlight the potential of\nsimulation-guided structural optimization for advancing flexible EIT-based\ntactile sensors toward practical applications in wearable systems, robotics,\nand human-machine interfaces.\n", "link": "http://arxiv.org/abs/2505.00161v2", "date": "2025-08-22", "relevancy": 1.8259, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4686}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4577}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.4504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimized%20Lattice-Structured%20Flexible%20EIT%20Sensor%20for%20Tactile%0A%20%20Reconstruction%20and%20Classification&body=Title%3A%20Optimized%20Lattice-Structured%20Flexible%20EIT%20Sensor%20for%20Tactile%0A%20%20Reconstruction%20and%20Classification%0AAuthor%3A%20Huazhi%20Dong%20and%20Sihao%20Teng%20and%20Xu%20Han%20and%20Xiaopeng%20Wu%20and%20Francesco%20Giorgio-Serchi%20and%20Yunjie%20Yang%0AAbstract%3A%20%20%20Flexible%20electrical%20impedance%20tomography%20%28EIT%29%20offers%20a%20promising%20alternative%0Ato%20traditional%20tactile%20sensing%20approaches%2C%20enabling%20low-cost%2C%20scalable%2C%20and%0Adeformable%20sensor%20designs.%20Here%2C%20we%20propose%20an%20optimized%20lattice-structured%0Aflexible%20EIT%20tactile%20sensor%20incorporating%20a%20hydrogel-based%20conductive%20layer%2C%0Asystematically%20designed%20through%20three-dimensional%20coupling%20field%20simulations%20to%0Aoptimize%20structural%20parameters%20for%20enhanced%20sensitivity%20and%20robustness.%20By%0Atuning%20the%20lattice%20channel%20width%20and%20conductive%20layer%20thickness%2C%20we%20achieve%0Asignificant%20improvements%20in%20tactile%20reconstruction%20quality%20and%20classification%0Aperformance.%20Experimental%20results%20demonstrate%20high-quality%20tactile%0Areconstruction%20with%20correlation%20coefficients%20up%20to%200.9275%2C%20peak%20signal-to-noise%0Aratios%20reaching%2029.0303%20dB%2C%20and%20structural%20similarity%20indexes%20up%20to%200.9660%2C%0Awhile%20maintaining%20low%20relative%20errors%20down%20to%200.3798.%20Furthermore%2C%20the%0Aoptimized%20sensor%20accurately%20classifies%2012%20distinct%20tactile%20stimuli%20with%20an%0Aaccuracy%20reaching%2099.6%25.%20These%20results%20highlight%20the%20potential%20of%0Asimulation-guided%20structural%20optimization%20for%20advancing%20flexible%20EIT-based%0Atactile%20sensors%20toward%20practical%20applications%20in%20wearable%20systems%2C%20robotics%2C%0Aand%20human-machine%20interfaces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00161v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimized%2520Lattice-Structured%2520Flexible%2520EIT%2520Sensor%2520for%2520Tactile%250A%2520%2520Reconstruction%2520and%2520Classification%26entry.906535625%3DHuazhi%2520Dong%2520and%2520Sihao%2520Teng%2520and%2520Xu%2520Han%2520and%2520Xiaopeng%2520Wu%2520and%2520Francesco%2520Giorgio-Serchi%2520and%2520Yunjie%2520Yang%26entry.1292438233%3D%2520%2520Flexible%2520electrical%2520impedance%2520tomography%2520%2528EIT%2529%2520offers%2520a%2520promising%2520alternative%250Ato%2520traditional%2520tactile%2520sensing%2520approaches%252C%2520enabling%2520low-cost%252C%2520scalable%252C%2520and%250Adeformable%2520sensor%2520designs.%2520Here%252C%2520we%2520propose%2520an%2520optimized%2520lattice-structured%250Aflexible%2520EIT%2520tactile%2520sensor%2520incorporating%2520a%2520hydrogel-based%2520conductive%2520layer%252C%250Asystematically%2520designed%2520through%2520three-dimensional%2520coupling%2520field%2520simulations%2520to%250Aoptimize%2520structural%2520parameters%2520for%2520enhanced%2520sensitivity%2520and%2520robustness.%2520By%250Atuning%2520the%2520lattice%2520channel%2520width%2520and%2520conductive%2520layer%2520thickness%252C%2520we%2520achieve%250Asignificant%2520improvements%2520in%2520tactile%2520reconstruction%2520quality%2520and%2520classification%250Aperformance.%2520Experimental%2520results%2520demonstrate%2520high-quality%2520tactile%250Areconstruction%2520with%2520correlation%2520coefficients%2520up%2520to%25200.9275%252C%2520peak%2520signal-to-noise%250Aratios%2520reaching%252029.0303%2520dB%252C%2520and%2520structural%2520similarity%2520indexes%2520up%2520to%25200.9660%252C%250Awhile%2520maintaining%2520low%2520relative%2520errors%2520down%2520to%25200.3798.%2520Furthermore%252C%2520the%250Aoptimized%2520sensor%2520accurately%2520classifies%252012%2520distinct%2520tactile%2520stimuli%2520with%2520an%250Aaccuracy%2520reaching%252099.6%2525.%2520These%2520results%2520highlight%2520the%2520potential%2520of%250Asimulation-guided%2520structural%2520optimization%2520for%2520advancing%2520flexible%2520EIT-based%250Atactile%2520sensors%2520toward%2520practical%2520applications%2520in%2520wearable%2520systems%252C%2520robotics%252C%250Aand%2520human-machine%2520interfaces.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00161v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimized%20Lattice-Structured%20Flexible%20EIT%20Sensor%20for%20Tactile%0A%20%20Reconstruction%20and%20Classification&entry.906535625=Huazhi%20Dong%20and%20Sihao%20Teng%20and%20Xu%20Han%20and%20Xiaopeng%20Wu%20and%20Francesco%20Giorgio-Serchi%20and%20Yunjie%20Yang&entry.1292438233=%20%20Flexible%20electrical%20impedance%20tomography%20%28EIT%29%20offers%20a%20promising%20alternative%0Ato%20traditional%20tactile%20sensing%20approaches%2C%20enabling%20low-cost%2C%20scalable%2C%20and%0Adeformable%20sensor%20designs.%20Here%2C%20we%20propose%20an%20optimized%20lattice-structured%0Aflexible%20EIT%20tactile%20sensor%20incorporating%20a%20hydrogel-based%20conductive%20layer%2C%0Asystematically%20designed%20through%20three-dimensional%20coupling%20field%20simulations%20to%0Aoptimize%20structural%20parameters%20for%20enhanced%20sensitivity%20and%20robustness.%20By%0Atuning%20the%20lattice%20channel%20width%20and%20conductive%20layer%20thickness%2C%20we%20achieve%0Asignificant%20improvements%20in%20tactile%20reconstruction%20quality%20and%20classification%0Aperformance.%20Experimental%20results%20demonstrate%20high-quality%20tactile%0Areconstruction%20with%20correlation%20coefficients%20up%20to%200.9275%2C%20peak%20signal-to-noise%0Aratios%20reaching%2029.0303%20dB%2C%20and%20structural%20similarity%20indexes%20up%20to%200.9660%2C%0Awhile%20maintaining%20low%20relative%20errors%20down%20to%200.3798.%20Furthermore%2C%20the%0Aoptimized%20sensor%20accurately%20classifies%2012%20distinct%20tactile%20stimuli%20with%20an%0Aaccuracy%20reaching%2099.6%25.%20These%20results%20highlight%20the%20potential%20of%0Asimulation-guided%20structural%20optimization%20for%20advancing%20flexible%20EIT-based%0Atactile%20sensors%20toward%20practical%20applications%20in%20wearable%20systems%2C%20robotics%2C%0Aand%20human-machine%20interfaces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00161v2&entry.124074799=Read"},
{"title": "On the Challenges and Opportunities in Generative AI", "author": "Laura Manduchi and Clara Meister and Kushagra Pandey and Robert Bamler and Ryan Cotterell and Sina D\u00e4ubener and Sophie Fellenz and Asja Fischer and Thomas G\u00e4rtner and Matthias Kirchler and Marius Kloft and Yingzhen Li and Christoph Lippert and Gerard de Melo and Eric Nalisnick and Bj\u00f6rn Ommer and Rajesh Ranganath and Maja Rudolph and Karen Ullrich and Guy Van den Broeck and Julia E Vogt and Yixin Wang and Florian Wenzel and Frank Wood and Stephan Mandt and Vincent Fortuin", "abstract": "  The field of deep generative modeling has grown rapidly in the last few\nyears. With the availability of massive amounts of training data coupled with\nadvances in scalable unsupervised learning paradigms, recent large-scale\ngenerative models show tremendous promise in synthesizing high-resolution\nimages and text, as well as structured data such as videos and molecules.\nHowever, we argue that current large-scale generative AI models exhibit several\nfundamental shortcomings that hinder their widespread adoption across domains.\nIn this work, our objective is to identify these issues and highlight key\nunresolved challenges in modern generative AI paradigms that should be\naddressed to further enhance their capabilities, versatility, and reliability.\nBy identifying these challenges, we aim to provide researchers with insights\nfor exploring fruitful research directions, thus fostering the development of\nmore robust and accessible generative AI solutions.\n", "link": "http://arxiv.org/abs/2403.00025v4", "date": "2025-08-22", "relevancy": 1.756, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6002}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5994}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5341}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Challenges%20and%20Opportunities%20in%20Generative%20AI&body=Title%3A%20On%20the%20Challenges%20and%20Opportunities%20in%20Generative%20AI%0AAuthor%3A%20Laura%20Manduchi%20and%20Clara%20Meister%20and%20Kushagra%20Pandey%20and%20Robert%20Bamler%20and%20Ryan%20Cotterell%20and%20Sina%20D%C3%A4ubener%20and%20Sophie%20Fellenz%20and%20Asja%20Fischer%20and%20Thomas%20G%C3%A4rtner%20and%20Matthias%20Kirchler%20and%20Marius%20Kloft%20and%20Yingzhen%20Li%20and%20Christoph%20Lippert%20and%20Gerard%20de%20Melo%20and%20Eric%20Nalisnick%20and%20Bj%C3%B6rn%20Ommer%20and%20Rajesh%20Ranganath%20and%20Maja%20Rudolph%20and%20Karen%20Ullrich%20and%20Guy%20Van%20den%20Broeck%20and%20Julia%20E%20Vogt%20and%20Yixin%20Wang%20and%20Florian%20Wenzel%20and%20Frank%20Wood%20and%20Stephan%20Mandt%20and%20Vincent%20Fortuin%0AAbstract%3A%20%20%20The%20field%20of%20deep%20generative%20modeling%20has%20grown%20rapidly%20in%20the%20last%20few%0Ayears.%20With%20the%20availability%20of%20massive%20amounts%20of%20training%20data%20coupled%20with%0Aadvances%20in%20scalable%20unsupervised%20learning%20paradigms%2C%20recent%20large-scale%0Agenerative%20models%20show%20tremendous%20promise%20in%20synthesizing%20high-resolution%0Aimages%20and%20text%2C%20as%20well%20as%20structured%20data%20such%20as%20videos%20and%20molecules.%0AHowever%2C%20we%20argue%20that%20current%20large-scale%20generative%20AI%20models%20exhibit%20several%0Afundamental%20shortcomings%20that%20hinder%20their%20widespread%20adoption%20across%20domains.%0AIn%20this%20work%2C%20our%20objective%20is%20to%20identify%20these%20issues%20and%20highlight%20key%0Aunresolved%20challenges%20in%20modern%20generative%20AI%20paradigms%20that%20should%20be%0Aaddressed%20to%20further%20enhance%20their%20capabilities%2C%20versatility%2C%20and%20reliability.%0ABy%20identifying%20these%20challenges%2C%20we%20aim%20to%20provide%20researchers%20with%20insights%0Afor%20exploring%20fruitful%20research%20directions%2C%20thus%20fostering%20the%20development%20of%0Amore%20robust%20and%20accessible%20generative%20AI%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.00025v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Challenges%2520and%2520Opportunities%2520in%2520Generative%2520AI%26entry.906535625%3DLaura%2520Manduchi%2520and%2520Clara%2520Meister%2520and%2520Kushagra%2520Pandey%2520and%2520Robert%2520Bamler%2520and%2520Ryan%2520Cotterell%2520and%2520Sina%2520D%25C3%25A4ubener%2520and%2520Sophie%2520Fellenz%2520and%2520Asja%2520Fischer%2520and%2520Thomas%2520G%25C3%25A4rtner%2520and%2520Matthias%2520Kirchler%2520and%2520Marius%2520Kloft%2520and%2520Yingzhen%2520Li%2520and%2520Christoph%2520Lippert%2520and%2520Gerard%2520de%2520Melo%2520and%2520Eric%2520Nalisnick%2520and%2520Bj%25C3%25B6rn%2520Ommer%2520and%2520Rajesh%2520Ranganath%2520and%2520Maja%2520Rudolph%2520and%2520Karen%2520Ullrich%2520and%2520Guy%2520Van%2520den%2520Broeck%2520and%2520Julia%2520E%2520Vogt%2520and%2520Yixin%2520Wang%2520and%2520Florian%2520Wenzel%2520and%2520Frank%2520Wood%2520and%2520Stephan%2520Mandt%2520and%2520Vincent%2520Fortuin%26entry.1292438233%3D%2520%2520The%2520field%2520of%2520deep%2520generative%2520modeling%2520has%2520grown%2520rapidly%2520in%2520the%2520last%2520few%250Ayears.%2520With%2520the%2520availability%2520of%2520massive%2520amounts%2520of%2520training%2520data%2520coupled%2520with%250Aadvances%2520in%2520scalable%2520unsupervised%2520learning%2520paradigms%252C%2520recent%2520large-scale%250Agenerative%2520models%2520show%2520tremendous%2520promise%2520in%2520synthesizing%2520high-resolution%250Aimages%2520and%2520text%252C%2520as%2520well%2520as%2520structured%2520data%2520such%2520as%2520videos%2520and%2520molecules.%250AHowever%252C%2520we%2520argue%2520that%2520current%2520large-scale%2520generative%2520AI%2520models%2520exhibit%2520several%250Afundamental%2520shortcomings%2520that%2520hinder%2520their%2520widespread%2520adoption%2520across%2520domains.%250AIn%2520this%2520work%252C%2520our%2520objective%2520is%2520to%2520identify%2520these%2520issues%2520and%2520highlight%2520key%250Aunresolved%2520challenges%2520in%2520modern%2520generative%2520AI%2520paradigms%2520that%2520should%2520be%250Aaddressed%2520to%2520further%2520enhance%2520their%2520capabilities%252C%2520versatility%252C%2520and%2520reliability.%250ABy%2520identifying%2520these%2520challenges%252C%2520we%2520aim%2520to%2520provide%2520researchers%2520with%2520insights%250Afor%2520exploring%2520fruitful%2520research%2520directions%252C%2520thus%2520fostering%2520the%2520development%2520of%250Amore%2520robust%2520and%2520accessible%2520generative%2520AI%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.00025v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Challenges%20and%20Opportunities%20in%20Generative%20AI&entry.906535625=Laura%20Manduchi%20and%20Clara%20Meister%20and%20Kushagra%20Pandey%20and%20Robert%20Bamler%20and%20Ryan%20Cotterell%20and%20Sina%20D%C3%A4ubener%20and%20Sophie%20Fellenz%20and%20Asja%20Fischer%20and%20Thomas%20G%C3%A4rtner%20and%20Matthias%20Kirchler%20and%20Marius%20Kloft%20and%20Yingzhen%20Li%20and%20Christoph%20Lippert%20and%20Gerard%20de%20Melo%20and%20Eric%20Nalisnick%20and%20Bj%C3%B6rn%20Ommer%20and%20Rajesh%20Ranganath%20and%20Maja%20Rudolph%20and%20Karen%20Ullrich%20and%20Guy%20Van%20den%20Broeck%20and%20Julia%20E%20Vogt%20and%20Yixin%20Wang%20and%20Florian%20Wenzel%20and%20Frank%20Wood%20and%20Stephan%20Mandt%20and%20Vincent%20Fortuin&entry.1292438233=%20%20The%20field%20of%20deep%20generative%20modeling%20has%20grown%20rapidly%20in%20the%20last%20few%0Ayears.%20With%20the%20availability%20of%20massive%20amounts%20of%20training%20data%20coupled%20with%0Aadvances%20in%20scalable%20unsupervised%20learning%20paradigms%2C%20recent%20large-scale%0Agenerative%20models%20show%20tremendous%20promise%20in%20synthesizing%20high-resolution%0Aimages%20and%20text%2C%20as%20well%20as%20structured%20data%20such%20as%20videos%20and%20molecules.%0AHowever%2C%20we%20argue%20that%20current%20large-scale%20generative%20AI%20models%20exhibit%20several%0Afundamental%20shortcomings%20that%20hinder%20their%20widespread%20adoption%20across%20domains.%0AIn%20this%20work%2C%20our%20objective%20is%20to%20identify%20these%20issues%20and%20highlight%20key%0Aunresolved%20challenges%20in%20modern%20generative%20AI%20paradigms%20that%20should%20be%0Aaddressed%20to%20further%20enhance%20their%20capabilities%2C%20versatility%2C%20and%20reliability.%0ABy%20identifying%20these%20challenges%2C%20we%20aim%20to%20provide%20researchers%20with%20insights%0Afor%20exploring%20fruitful%20research%20directions%2C%20thus%20fostering%20the%20development%20of%0Amore%20robust%20and%20accessible%20generative%20AI%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00025v4&entry.124074799=Read"},
{"title": "A Sharp KL-Convergence Analysis for Diffusion Models under Minimal\n  Assumptions", "author": "Nishant Jain and Tong Zhang", "abstract": "  Diffusion-based generative models have emerged as highly effective methods\nfor synthesizing high-quality samples. Recent works have focused on analyzing\nthe convergence of their generation process with minimal assumptions, either\nthrough reverse SDEs or Probability Flow ODEs. The best known guarantees,\nwithout any smoothness assumptions, for the KL divergence so far achieve a\nlinear dependence on the data dimension $d$ and an inverse quadratic dependence\non $\\varepsilon$. In this work, we present a refined analysis that improves the\ndependence on $\\varepsilon$. We model the generation process as a composition\nof two steps: a reverse ODE step, followed by a smaller noising step along the\nforward process. This design leverages the fact that the ODE step enables\ncontrol in Wasserstein-type error, which can then be converted into a KL\ndivergence bound via noise addition, leading to a better dependence on the\ndiscretization step size. We further provide a novel analysis to achieve the\nlinear $d$-dependence for the error due to discretizing this Probability Flow\nODE in absence of any smoothness assumptions. We show that\n$\\tilde{O}\\left(\\tfrac{d\\log^{3/2}(\\frac{1}{\\delta})}{\\varepsilon}\\right)$\nsteps suffice to approximate the target distribution corrupted with Gaussian\nnoise of variance $\\delta$ within $O(\\varepsilon^2)$ in KL divergence,\nimproving upon the previous best result, requiring\n$\\tilde{O}\\left(\\tfrac{d\\log^2(\\frac{1}{\\delta})}{\\varepsilon^2}\\right)$ steps.\n", "link": "http://arxiv.org/abs/2508.16306v1", "date": "2025-08-22", "relevancy": 1.5365, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5333}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5172}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5017}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Sharp%20KL-Convergence%20Analysis%20for%20Diffusion%20Models%20under%20Minimal%0A%20%20Assumptions&body=Title%3A%20A%20Sharp%20KL-Convergence%20Analysis%20for%20Diffusion%20Models%20under%20Minimal%0A%20%20Assumptions%0AAuthor%3A%20Nishant%20Jain%20and%20Tong%20Zhang%0AAbstract%3A%20%20%20Diffusion-based%20generative%20models%20have%20emerged%20as%20highly%20effective%20methods%0Afor%20synthesizing%20high-quality%20samples.%20Recent%20works%20have%20focused%20on%20analyzing%0Athe%20convergence%20of%20their%20generation%20process%20with%20minimal%20assumptions%2C%20either%0Athrough%20reverse%20SDEs%20or%20Probability%20Flow%20ODEs.%20The%20best%20known%20guarantees%2C%0Awithout%20any%20smoothness%20assumptions%2C%20for%20the%20KL%20divergence%20so%20far%20achieve%20a%0Alinear%20dependence%20on%20the%20data%20dimension%20%24d%24%20and%20an%20inverse%20quadratic%20dependence%0Aon%20%24%5Cvarepsilon%24.%20In%20this%20work%2C%20we%20present%20a%20refined%20analysis%20that%20improves%20the%0Adependence%20on%20%24%5Cvarepsilon%24.%20We%20model%20the%20generation%20process%20as%20a%20composition%0Aof%20two%20steps%3A%20a%20reverse%20ODE%20step%2C%20followed%20by%20a%20smaller%20noising%20step%20along%20the%0Aforward%20process.%20This%20design%20leverages%20the%20fact%20that%20the%20ODE%20step%20enables%0Acontrol%20in%20Wasserstein-type%20error%2C%20which%20can%20then%20be%20converted%20into%20a%20KL%0Adivergence%20bound%20via%20noise%20addition%2C%20leading%20to%20a%20better%20dependence%20on%20the%0Adiscretization%20step%20size.%20We%20further%20provide%20a%20novel%20analysis%20to%20achieve%20the%0Alinear%20%24d%24-dependence%20for%20the%20error%20due%20to%20discretizing%20this%20Probability%20Flow%0AODE%20in%20absence%20of%20any%20smoothness%20assumptions.%20We%20show%20that%0A%24%5Ctilde%7BO%7D%5Cleft%28%5Ctfrac%7Bd%5Clog%5E%7B3/2%7D%28%5Cfrac%7B1%7D%7B%5Cdelta%7D%29%7D%7B%5Cvarepsilon%7D%5Cright%29%24%0Asteps%20suffice%20to%20approximate%20the%20target%20distribution%20corrupted%20with%20Gaussian%0Anoise%20of%20variance%20%24%5Cdelta%24%20within%20%24O%28%5Cvarepsilon%5E2%29%24%20in%20KL%20divergence%2C%0Aimproving%20upon%20the%20previous%20best%20result%2C%20requiring%0A%24%5Ctilde%7BO%7D%5Cleft%28%5Ctfrac%7Bd%5Clog%5E2%28%5Cfrac%7B1%7D%7B%5Cdelta%7D%29%7D%7B%5Cvarepsilon%5E2%7D%5Cright%29%24%20steps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16306v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Sharp%2520KL-Convergence%2520Analysis%2520for%2520Diffusion%2520Models%2520under%2520Minimal%250A%2520%2520Assumptions%26entry.906535625%3DNishant%2520Jain%2520and%2520Tong%2520Zhang%26entry.1292438233%3D%2520%2520Diffusion-based%2520generative%2520models%2520have%2520emerged%2520as%2520highly%2520effective%2520methods%250Afor%2520synthesizing%2520high-quality%2520samples.%2520Recent%2520works%2520have%2520focused%2520on%2520analyzing%250Athe%2520convergence%2520of%2520their%2520generation%2520process%2520with%2520minimal%2520assumptions%252C%2520either%250Athrough%2520reverse%2520SDEs%2520or%2520Probability%2520Flow%2520ODEs.%2520The%2520best%2520known%2520guarantees%252C%250Awithout%2520any%2520smoothness%2520assumptions%252C%2520for%2520the%2520KL%2520divergence%2520so%2520far%2520achieve%2520a%250Alinear%2520dependence%2520on%2520the%2520data%2520dimension%2520%2524d%2524%2520and%2520an%2520inverse%2520quadratic%2520dependence%250Aon%2520%2524%255Cvarepsilon%2524.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520refined%2520analysis%2520that%2520improves%2520the%250Adependence%2520on%2520%2524%255Cvarepsilon%2524.%2520We%2520model%2520the%2520generation%2520process%2520as%2520a%2520composition%250Aof%2520two%2520steps%253A%2520a%2520reverse%2520ODE%2520step%252C%2520followed%2520by%2520a%2520smaller%2520noising%2520step%2520along%2520the%250Aforward%2520process.%2520This%2520design%2520leverages%2520the%2520fact%2520that%2520the%2520ODE%2520step%2520enables%250Acontrol%2520in%2520Wasserstein-type%2520error%252C%2520which%2520can%2520then%2520be%2520converted%2520into%2520a%2520KL%250Adivergence%2520bound%2520via%2520noise%2520addition%252C%2520leading%2520to%2520a%2520better%2520dependence%2520on%2520the%250Adiscretization%2520step%2520size.%2520We%2520further%2520provide%2520a%2520novel%2520analysis%2520to%2520achieve%2520the%250Alinear%2520%2524d%2524-dependence%2520for%2520the%2520error%2520due%2520to%2520discretizing%2520this%2520Probability%2520Flow%250AODE%2520in%2520absence%2520of%2520any%2520smoothness%2520assumptions.%2520We%2520show%2520that%250A%2524%255Ctilde%257BO%257D%255Cleft%2528%255Ctfrac%257Bd%255Clog%255E%257B3/2%257D%2528%255Cfrac%257B1%257D%257B%255Cdelta%257D%2529%257D%257B%255Cvarepsilon%257D%255Cright%2529%2524%250Asteps%2520suffice%2520to%2520approximate%2520the%2520target%2520distribution%2520corrupted%2520with%2520Gaussian%250Anoise%2520of%2520variance%2520%2524%255Cdelta%2524%2520within%2520%2524O%2528%255Cvarepsilon%255E2%2529%2524%2520in%2520KL%2520divergence%252C%250Aimproving%2520upon%2520the%2520previous%2520best%2520result%252C%2520requiring%250A%2524%255Ctilde%257BO%257D%255Cleft%2528%255Ctfrac%257Bd%255Clog%255E2%2528%255Cfrac%257B1%257D%257B%255Cdelta%257D%2529%257D%257B%255Cvarepsilon%255E2%257D%255Cright%2529%2524%2520steps.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16306v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Sharp%20KL-Convergence%20Analysis%20for%20Diffusion%20Models%20under%20Minimal%0A%20%20Assumptions&entry.906535625=Nishant%20Jain%20and%20Tong%20Zhang&entry.1292438233=%20%20Diffusion-based%20generative%20models%20have%20emerged%20as%20highly%20effective%20methods%0Afor%20synthesizing%20high-quality%20samples.%20Recent%20works%20have%20focused%20on%20analyzing%0Athe%20convergence%20of%20their%20generation%20process%20with%20minimal%20assumptions%2C%20either%0Athrough%20reverse%20SDEs%20or%20Probability%20Flow%20ODEs.%20The%20best%20known%20guarantees%2C%0Awithout%20any%20smoothness%20assumptions%2C%20for%20the%20KL%20divergence%20so%20far%20achieve%20a%0Alinear%20dependence%20on%20the%20data%20dimension%20%24d%24%20and%20an%20inverse%20quadratic%20dependence%0Aon%20%24%5Cvarepsilon%24.%20In%20this%20work%2C%20we%20present%20a%20refined%20analysis%20that%20improves%20the%0Adependence%20on%20%24%5Cvarepsilon%24.%20We%20model%20the%20generation%20process%20as%20a%20composition%0Aof%20two%20steps%3A%20a%20reverse%20ODE%20step%2C%20followed%20by%20a%20smaller%20noising%20step%20along%20the%0Aforward%20process.%20This%20design%20leverages%20the%20fact%20that%20the%20ODE%20step%20enables%0Acontrol%20in%20Wasserstein-type%20error%2C%20which%20can%20then%20be%20converted%20into%20a%20KL%0Adivergence%20bound%20via%20noise%20addition%2C%20leading%20to%20a%20better%20dependence%20on%20the%0Adiscretization%20step%20size.%20We%20further%20provide%20a%20novel%20analysis%20to%20achieve%20the%0Alinear%20%24d%24-dependence%20for%20the%20error%20due%20to%20discretizing%20this%20Probability%20Flow%0AODE%20in%20absence%20of%20any%20smoothness%20assumptions.%20We%20show%20that%0A%24%5Ctilde%7BO%7D%5Cleft%28%5Ctfrac%7Bd%5Clog%5E%7B3/2%7D%28%5Cfrac%7B1%7D%7B%5Cdelta%7D%29%7D%7B%5Cvarepsilon%7D%5Cright%29%24%0Asteps%20suffice%20to%20approximate%20the%20target%20distribution%20corrupted%20with%20Gaussian%0Anoise%20of%20variance%20%24%5Cdelta%24%20within%20%24O%28%5Cvarepsilon%5E2%29%24%20in%20KL%20divergence%2C%0Aimproving%20upon%20the%20previous%20best%20result%2C%20requiring%0A%24%5Ctilde%7BO%7D%5Cleft%28%5Ctfrac%7Bd%5Clog%5E2%28%5Cfrac%7B1%7D%7B%5Cdelta%7D%29%7D%7B%5Cvarepsilon%5E2%7D%5Cright%29%24%20steps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16306v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


