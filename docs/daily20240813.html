<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240812.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Novel View Synthesis from a Single Image with Pretrained Diffusion\n  Guidance", "author": "Taewon Kang and Divya Kothandaraman and Dinesh Manocha and Ming C. Lin", "abstract": "  Recent 3D novel view synthesis (NVS) methods are limited to\nsingle-object-centric scenes generated from new viewpoints and struggle with\ncomplex environments. They often require extensive 3D data for training,\nlacking generalization beyond training distribution. Conversely, 3D-free\nmethods can generate text-controlled views of complex, in-the-wild scenes using\na pretrained stable diffusion model without tedious fine-tuning, but lack\ncamera control. In this paper, we introduce HawkI++, a method capable of\ngenerating camera-controlled viewpoints from a single input image. HawkI++\nexcels in handling complex and diverse scenes without additional 3D data or\nextensive training. It leverages widely available pretrained NVS models for\nweak guidance, integrating this knowledge into a 3D-free view synthesis\napproach to achieve the desired results efficiently. Our experimental results\ndemonstrate that HawkI++ outperforms existing models in both qualitative and\nquantitative evaluations, providing high-fidelity and consistent novel view\nsynthesis at desired camera angles across a wide variety of scenes.\n", "link": "http://arxiv.org/abs/2408.06157v1", "date": "2024-08-12", "relevancy": 3.1399, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6659}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6659}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Novel%20View%20Synthesis%20from%20a%20Single%20Image%20with%20Pretrained%20Diffusion%0A%20%20Guidance&body=Title%3A%20Novel%20View%20Synthesis%20from%20a%20Single%20Image%20with%20Pretrained%20Diffusion%0A%20%20Guidance%0AAuthor%3A%20Taewon%20Kang%20and%20Divya%20Kothandaraman%20and%20Dinesh%20Manocha%20and%20Ming%20C.%20Lin%0AAbstract%3A%20%20%20Recent%203D%20novel%20view%20synthesis%20%28NVS%29%20methods%20are%20limited%20to%0Asingle-object-centric%20scenes%20generated%20from%20new%20viewpoints%20and%20struggle%20with%0Acomplex%20environments.%20They%20often%20require%20extensive%203D%20data%20for%20training%2C%0Alacking%20generalization%20beyond%20training%20distribution.%20Conversely%2C%203D-free%0Amethods%20can%20generate%20text-controlled%20views%20of%20complex%2C%20in-the-wild%20scenes%20using%0Aa%20pretrained%20stable%20diffusion%20model%20without%20tedious%20fine-tuning%2C%20but%20lack%0Acamera%20control.%20In%20this%20paper%2C%20we%20introduce%20HawkI%2B%2B%2C%20a%20method%20capable%20of%0Agenerating%20camera-controlled%20viewpoints%20from%20a%20single%20input%20image.%20HawkI%2B%2B%0Aexcels%20in%20handling%20complex%20and%20diverse%20scenes%20without%20additional%203D%20data%20or%0Aextensive%20training.%20It%20leverages%20widely%20available%20pretrained%20NVS%20models%20for%0Aweak%20guidance%2C%20integrating%20this%20knowledge%20into%20a%203D-free%20view%20synthesis%0Aapproach%20to%20achieve%20the%20desired%20results%20efficiently.%20Our%20experimental%20results%0Ademonstrate%20that%20HawkI%2B%2B%20outperforms%20existing%20models%20in%20both%20qualitative%20and%0Aquantitative%20evaluations%2C%20providing%20high-fidelity%20and%20consistent%20novel%20view%0Asynthesis%20at%20desired%20camera%20angles%20across%20a%20wide%20variety%20of%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06157v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNovel%2520View%2520Synthesis%2520from%2520a%2520Single%2520Image%2520with%2520Pretrained%2520Diffusion%250A%2520%2520Guidance%26entry.906535625%3DTaewon%2520Kang%2520and%2520Divya%2520Kothandaraman%2520and%2520Dinesh%2520Manocha%2520and%2520Ming%2520C.%2520Lin%26entry.1292438233%3D%2520%2520Recent%25203D%2520novel%2520view%2520synthesis%2520%2528NVS%2529%2520methods%2520are%2520limited%2520to%250Asingle-object-centric%2520scenes%2520generated%2520from%2520new%2520viewpoints%2520and%2520struggle%2520with%250Acomplex%2520environments.%2520They%2520often%2520require%2520extensive%25203D%2520data%2520for%2520training%252C%250Alacking%2520generalization%2520beyond%2520training%2520distribution.%2520Conversely%252C%25203D-free%250Amethods%2520can%2520generate%2520text-controlled%2520views%2520of%2520complex%252C%2520in-the-wild%2520scenes%2520using%250Aa%2520pretrained%2520stable%2520diffusion%2520model%2520without%2520tedious%2520fine-tuning%252C%2520but%2520lack%250Acamera%2520control.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520HawkI%252B%252B%252C%2520a%2520method%2520capable%2520of%250Agenerating%2520camera-controlled%2520viewpoints%2520from%2520a%2520single%2520input%2520image.%2520HawkI%252B%252B%250Aexcels%2520in%2520handling%2520complex%2520and%2520diverse%2520scenes%2520without%2520additional%25203D%2520data%2520or%250Aextensive%2520training.%2520It%2520leverages%2520widely%2520available%2520pretrained%2520NVS%2520models%2520for%250Aweak%2520guidance%252C%2520integrating%2520this%2520knowledge%2520into%2520a%25203D-free%2520view%2520synthesis%250Aapproach%2520to%2520achieve%2520the%2520desired%2520results%2520efficiently.%2520Our%2520experimental%2520results%250Ademonstrate%2520that%2520HawkI%252B%252B%2520outperforms%2520existing%2520models%2520in%2520both%2520qualitative%2520and%250Aquantitative%2520evaluations%252C%2520providing%2520high-fidelity%2520and%2520consistent%2520novel%2520view%250Asynthesis%2520at%2520desired%2520camera%2520angles%2520across%2520a%2520wide%2520variety%2520of%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06157v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Novel%20View%20Synthesis%20from%20a%20Single%20Image%20with%20Pretrained%20Diffusion%0A%20%20Guidance&entry.906535625=Taewon%20Kang%20and%20Divya%20Kothandaraman%20and%20Dinesh%20Manocha%20and%20Ming%20C.%20Lin&entry.1292438233=%20%20Recent%203D%20novel%20view%20synthesis%20%28NVS%29%20methods%20are%20limited%20to%0Asingle-object-centric%20scenes%20generated%20from%20new%20viewpoints%20and%20struggle%20with%0Acomplex%20environments.%20They%20often%20require%20extensive%203D%20data%20for%20training%2C%0Alacking%20generalization%20beyond%20training%20distribution.%20Conversely%2C%203D-free%0Amethods%20can%20generate%20text-controlled%20views%20of%20complex%2C%20in-the-wild%20scenes%20using%0Aa%20pretrained%20stable%20diffusion%20model%20without%20tedious%20fine-tuning%2C%20but%20lack%0Acamera%20control.%20In%20this%20paper%2C%20we%20introduce%20HawkI%2B%2B%2C%20a%20method%20capable%20of%0Agenerating%20camera-controlled%20viewpoints%20from%20a%20single%20input%20image.%20HawkI%2B%2B%0Aexcels%20in%20handling%20complex%20and%20diverse%20scenes%20without%20additional%203D%20data%20or%0Aextensive%20training.%20It%20leverages%20widely%20available%20pretrained%20NVS%20models%20for%0Aweak%20guidance%2C%20integrating%20this%20knowledge%20into%20a%203D-free%20view%20synthesis%0Aapproach%20to%20achieve%20the%20desired%20results%20efficiently.%20Our%20experimental%20results%0Ademonstrate%20that%20HawkI%2B%2B%20outperforms%20existing%20models%20in%20both%20qualitative%20and%0Aquantitative%20evaluations%2C%20providing%20high-fidelity%20and%20consistent%20novel%20view%0Asynthesis%20at%20desired%20camera%20angles%20across%20a%20wide%20variety%20of%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06157v1&entry.124074799=Read"},
{"title": "Mipmap-GS: Let Gaussians Deform with Scale-specific Mipmap for\n  Anti-aliasing Rendering", "author": "Jiameng Li and Yue Shi and Jiezhang Cao and Bingbing Ni and Wenjun Zhang and Kai Zhang and Luc Van Gool", "abstract": "  3D Gaussian Splatting (3DGS) has attracted great attention in novel view\nsynthesis because of its superior rendering efficiency and high fidelity.\nHowever, the trained Gaussians suffer from severe zooming degradation due to\nnon-adjustable representation derived from single-scale training. Though some\nmethods attempt to tackle this problem via post-processing techniques such as\nselective rendering or filtering techniques towards primitives, the\nscale-specific information is not involved in Gaussians. In this paper, we\npropose a unified optimization method to make Gaussians adaptive for arbitrary\nscales by self-adjusting the primitive properties (e.g., color, shape and size)\nand distribution (e.g., position). Inspired by the mipmap technique, we design\npseudo ground-truth for the target scale and propose a scale-consistency\nguidance loss to inject scale information into 3D Gaussians. Our method is a\nplug-in module, applicable for any 3DGS models to solve the zoom-in and\nzoom-out aliasing. Extensive experiments demonstrate the effectiveness of our\nmethod. Notably, our method outperforms 3DGS in PSNR by an average of 9.25 dB\nfor zoom-in and 10.40 dB for zoom-out on the NeRF Synthetic dataset.\n", "link": "http://arxiv.org/abs/2408.06286v1", "date": "2024-08-12", "relevancy": 3.0862, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6607}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6502}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5408}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mipmap-GS%3A%20Let%20Gaussians%20Deform%20with%20Scale-specific%20Mipmap%20for%0A%20%20Anti-aliasing%20Rendering&body=Title%3A%20Mipmap-GS%3A%20Let%20Gaussians%20Deform%20with%20Scale-specific%20Mipmap%20for%0A%20%20Anti-aliasing%20Rendering%0AAuthor%3A%20Jiameng%20Li%20and%20Yue%20Shi%20and%20Jiezhang%20Cao%20and%20Bingbing%20Ni%20and%20Wenjun%20Zhang%20and%20Kai%20Zhang%20and%20Luc%20Van%20Gool%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20attracted%20great%20attention%20in%20novel%20view%0Asynthesis%20because%20of%20its%20superior%20rendering%20efficiency%20and%20high%20fidelity.%0AHowever%2C%20the%20trained%20Gaussians%20suffer%20from%20severe%20zooming%20degradation%20due%20to%0Anon-adjustable%20representation%20derived%20from%20single-scale%20training.%20Though%20some%0Amethods%20attempt%20to%20tackle%20this%20problem%20via%20post-processing%20techniques%20such%20as%0Aselective%20rendering%20or%20filtering%20techniques%20towards%20primitives%2C%20the%0Ascale-specific%20information%20is%20not%20involved%20in%20Gaussians.%20In%20this%20paper%2C%20we%0Apropose%20a%20unified%20optimization%20method%20to%20make%20Gaussians%20adaptive%20for%20arbitrary%0Ascales%20by%20self-adjusting%20the%20primitive%20properties%20%28e.g.%2C%20color%2C%20shape%20and%20size%29%0Aand%20distribution%20%28e.g.%2C%20position%29.%20Inspired%20by%20the%20mipmap%20technique%2C%20we%20design%0Apseudo%20ground-truth%20for%20the%20target%20scale%20and%20propose%20a%20scale-consistency%0Aguidance%20loss%20to%20inject%20scale%20information%20into%203D%20Gaussians.%20Our%20method%20is%20a%0Aplug-in%20module%2C%20applicable%20for%20any%203DGS%20models%20to%20solve%20the%20zoom-in%20and%0Azoom-out%20aliasing.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20our%0Amethod.%20Notably%2C%20our%20method%20outperforms%203DGS%20in%20PSNR%20by%20an%20average%20of%209.25%20dB%0Afor%20zoom-in%20and%2010.40%20dB%20for%20zoom-out%20on%20the%20NeRF%20Synthetic%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06286v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMipmap-GS%253A%2520Let%2520Gaussians%2520Deform%2520with%2520Scale-specific%2520Mipmap%2520for%250A%2520%2520Anti-aliasing%2520Rendering%26entry.906535625%3DJiameng%2520Li%2520and%2520Yue%2520Shi%2520and%2520Jiezhang%2520Cao%2520and%2520Bingbing%2520Ni%2520and%2520Wenjun%2520Zhang%2520and%2520Kai%2520Zhang%2520and%2520Luc%2520Van%2520Gool%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520attracted%2520great%2520attention%2520in%2520novel%2520view%250Asynthesis%2520because%2520of%2520its%2520superior%2520rendering%2520efficiency%2520and%2520high%2520fidelity.%250AHowever%252C%2520the%2520trained%2520Gaussians%2520suffer%2520from%2520severe%2520zooming%2520degradation%2520due%2520to%250Anon-adjustable%2520representation%2520derived%2520from%2520single-scale%2520training.%2520Though%2520some%250Amethods%2520attempt%2520to%2520tackle%2520this%2520problem%2520via%2520post-processing%2520techniques%2520such%2520as%250Aselective%2520rendering%2520or%2520filtering%2520techniques%2520towards%2520primitives%252C%2520the%250Ascale-specific%2520information%2520is%2520not%2520involved%2520in%2520Gaussians.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520unified%2520optimization%2520method%2520to%2520make%2520Gaussians%2520adaptive%2520for%2520arbitrary%250Ascales%2520by%2520self-adjusting%2520the%2520primitive%2520properties%2520%2528e.g.%252C%2520color%252C%2520shape%2520and%2520size%2529%250Aand%2520distribution%2520%2528e.g.%252C%2520position%2529.%2520Inspired%2520by%2520the%2520mipmap%2520technique%252C%2520we%2520design%250Apseudo%2520ground-truth%2520for%2520the%2520target%2520scale%2520and%2520propose%2520a%2520scale-consistency%250Aguidance%2520loss%2520to%2520inject%2520scale%2520information%2520into%25203D%2520Gaussians.%2520Our%2520method%2520is%2520a%250Aplug-in%2520module%252C%2520applicable%2520for%2520any%25203DGS%2520models%2520to%2520solve%2520the%2520zoom-in%2520and%250Azoom-out%2520aliasing.%2520Extensive%2520experiments%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Amethod.%2520Notably%252C%2520our%2520method%2520outperforms%25203DGS%2520in%2520PSNR%2520by%2520an%2520average%2520of%25209.25%2520dB%250Afor%2520zoom-in%2520and%252010.40%2520dB%2520for%2520zoom-out%2520on%2520the%2520NeRF%2520Synthetic%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06286v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mipmap-GS%3A%20Let%20Gaussians%20Deform%20with%20Scale-specific%20Mipmap%20for%0A%20%20Anti-aliasing%20Rendering&entry.906535625=Jiameng%20Li%20and%20Yue%20Shi%20and%20Jiezhang%20Cao%20and%20Bingbing%20Ni%20and%20Wenjun%20Zhang%20and%20Kai%20Zhang%20and%20Luc%20Van%20Gool&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20attracted%20great%20attention%20in%20novel%20view%0Asynthesis%20because%20of%20its%20superior%20rendering%20efficiency%20and%20high%20fidelity.%0AHowever%2C%20the%20trained%20Gaussians%20suffer%20from%20severe%20zooming%20degradation%20due%20to%0Anon-adjustable%20representation%20derived%20from%20single-scale%20training.%20Though%20some%0Amethods%20attempt%20to%20tackle%20this%20problem%20via%20post-processing%20techniques%20such%20as%0Aselective%20rendering%20or%20filtering%20techniques%20towards%20primitives%2C%20the%0Ascale-specific%20information%20is%20not%20involved%20in%20Gaussians.%20In%20this%20paper%2C%20we%0Apropose%20a%20unified%20optimization%20method%20to%20make%20Gaussians%20adaptive%20for%20arbitrary%0Ascales%20by%20self-adjusting%20the%20primitive%20properties%20%28e.g.%2C%20color%2C%20shape%20and%20size%29%0Aand%20distribution%20%28e.g.%2C%20position%29.%20Inspired%20by%20the%20mipmap%20technique%2C%20we%20design%0Apseudo%20ground-truth%20for%20the%20target%20scale%20and%20propose%20a%20scale-consistency%0Aguidance%20loss%20to%20inject%20scale%20information%20into%203D%20Gaussians.%20Our%20method%20is%20a%0Aplug-in%20module%2C%20applicable%20for%20any%203DGS%20models%20to%20solve%20the%20zoom-in%20and%0Azoom-out%20aliasing.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20our%0Amethod.%20Notably%2C%20our%20method%20outperforms%203DGS%20in%20PSNR%20by%20an%20average%20of%209.25%20dB%0Afor%20zoom-in%20and%2010.40%20dB%20for%20zoom-out%20on%20the%20NeRF%20Synthetic%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06286v1&entry.124074799=Read"},
{"title": "OpenIns3D: Snap and Lookup for 3D Open-vocabulary Instance Segmentation", "author": "Zhening Huang and Xiaoyang Wu and Xi Chen and Hengshuang Zhao and Lei Zhu and Joan Lasenby", "abstract": "  In this work, we introduce OpenIns3D, a new 3D-input-only framework for 3D\nopen-vocabulary scene understanding. The OpenIns3D framework employs a\n\"Mask-Snap-Lookup\" scheme. The \"Mask\" module learns class-agnostic mask\nproposals in 3D point clouds, the \"Snap\" module generates synthetic scene-level\nimages at multiple scales and leverages 2D vision-language models to extract\ninteresting objects, and the \"Lookup\" module searches through the outcomes of\n\"Snap\" to assign category names to the proposed masks. This approach, yet\nsimple, achieves state-of-the-art performance across a wide range of 3D\nopen-vocabulary tasks, including recognition, object detection, and instance\nsegmentation, on both indoor and outdoor datasets. Moreover, OpenIns3D\nfacilitates effortless switching between different 2D detectors without\nrequiring retraining. When integrated with powerful 2D open-world models, it\nachieves excellent results in scene understanding tasks. Furthermore, when\ncombined with LLM-powered 2D models, OpenIns3D exhibits an impressive\ncapability to comprehend and process highly complex text queries that demand\nintricate reasoning and real-world knowledge. Project page:\nhttps://zheninghuang.github.io/OpenIns3D/\n", "link": "http://arxiv.org/abs/2309.00616v5", "date": "2024-08-12", "relevancy": 2.9582, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5976}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5886}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenIns3D%3A%20Snap%20and%20Lookup%20for%203D%20Open-vocabulary%20Instance%20Segmentation&body=Title%3A%20OpenIns3D%3A%20Snap%20and%20Lookup%20for%203D%20Open-vocabulary%20Instance%20Segmentation%0AAuthor%3A%20Zhening%20Huang%20and%20Xiaoyang%20Wu%20and%20Xi%20Chen%20and%20Hengshuang%20Zhao%20and%20Lei%20Zhu%20and%20Joan%20Lasenby%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20introduce%20OpenIns3D%2C%20a%20new%203D-input-only%20framework%20for%203D%0Aopen-vocabulary%20scene%20understanding.%20The%20OpenIns3D%20framework%20employs%20a%0A%22Mask-Snap-Lookup%22%20scheme.%20The%20%22Mask%22%20module%20learns%20class-agnostic%20mask%0Aproposals%20in%203D%20point%20clouds%2C%20the%20%22Snap%22%20module%20generates%20synthetic%20scene-level%0Aimages%20at%20multiple%20scales%20and%20leverages%202D%20vision-language%20models%20to%20extract%0Ainteresting%20objects%2C%20and%20the%20%22Lookup%22%20module%20searches%20through%20the%20outcomes%20of%0A%22Snap%22%20to%20assign%20category%20names%20to%20the%20proposed%20masks.%20This%20approach%2C%20yet%0Asimple%2C%20achieves%20state-of-the-art%20performance%20across%20a%20wide%20range%20of%203D%0Aopen-vocabulary%20tasks%2C%20including%20recognition%2C%20object%20detection%2C%20and%20instance%0Asegmentation%2C%20on%20both%20indoor%20and%20outdoor%20datasets.%20Moreover%2C%20OpenIns3D%0Afacilitates%20effortless%20switching%20between%20different%202D%20detectors%20without%0Arequiring%20retraining.%20When%20integrated%20with%20powerful%202D%20open-world%20models%2C%20it%0Aachieves%20excellent%20results%20in%20scene%20understanding%20tasks.%20Furthermore%2C%20when%0Acombined%20with%20LLM-powered%202D%20models%2C%20OpenIns3D%20exhibits%20an%20impressive%0Acapability%20to%20comprehend%20and%20process%20highly%20complex%20text%20queries%20that%20demand%0Aintricate%20reasoning%20and%20real-world%20knowledge.%20Project%20page%3A%0Ahttps%3A//zheninghuang.github.io/OpenIns3D/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.00616v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenIns3D%253A%2520Snap%2520and%2520Lookup%2520for%25203D%2520Open-vocabulary%2520Instance%2520Segmentation%26entry.906535625%3DZhening%2520Huang%2520and%2520Xiaoyang%2520Wu%2520and%2520Xi%2520Chen%2520and%2520Hengshuang%2520Zhao%2520and%2520Lei%2520Zhu%2520and%2520Joan%2520Lasenby%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520OpenIns3D%252C%2520a%2520new%25203D-input-only%2520framework%2520for%25203D%250Aopen-vocabulary%2520scene%2520understanding.%2520The%2520OpenIns3D%2520framework%2520employs%2520a%250A%2522Mask-Snap-Lookup%2522%2520scheme.%2520The%2520%2522Mask%2522%2520module%2520learns%2520class-agnostic%2520mask%250Aproposals%2520in%25203D%2520point%2520clouds%252C%2520the%2520%2522Snap%2522%2520module%2520generates%2520synthetic%2520scene-level%250Aimages%2520at%2520multiple%2520scales%2520and%2520leverages%25202D%2520vision-language%2520models%2520to%2520extract%250Ainteresting%2520objects%252C%2520and%2520the%2520%2522Lookup%2522%2520module%2520searches%2520through%2520the%2520outcomes%2520of%250A%2522Snap%2522%2520to%2520assign%2520category%2520names%2520to%2520the%2520proposed%2520masks.%2520This%2520approach%252C%2520yet%250Asimple%252C%2520achieves%2520state-of-the-art%2520performance%2520across%2520a%2520wide%2520range%2520of%25203D%250Aopen-vocabulary%2520tasks%252C%2520including%2520recognition%252C%2520object%2520detection%252C%2520and%2520instance%250Asegmentation%252C%2520on%2520both%2520indoor%2520and%2520outdoor%2520datasets.%2520Moreover%252C%2520OpenIns3D%250Afacilitates%2520effortless%2520switching%2520between%2520different%25202D%2520detectors%2520without%250Arequiring%2520retraining.%2520When%2520integrated%2520with%2520powerful%25202D%2520open-world%2520models%252C%2520it%250Aachieves%2520excellent%2520results%2520in%2520scene%2520understanding%2520tasks.%2520Furthermore%252C%2520when%250Acombined%2520with%2520LLM-powered%25202D%2520models%252C%2520OpenIns3D%2520exhibits%2520an%2520impressive%250Acapability%2520to%2520comprehend%2520and%2520process%2520highly%2520complex%2520text%2520queries%2520that%2520demand%250Aintricate%2520reasoning%2520and%2520real-world%2520knowledge.%2520Project%2520page%253A%250Ahttps%253A//zheninghuang.github.io/OpenIns3D/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.00616v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenIns3D%3A%20Snap%20and%20Lookup%20for%203D%20Open-vocabulary%20Instance%20Segmentation&entry.906535625=Zhening%20Huang%20and%20Xiaoyang%20Wu%20and%20Xi%20Chen%20and%20Hengshuang%20Zhao%20and%20Lei%20Zhu%20and%20Joan%20Lasenby&entry.1292438233=%20%20In%20this%20work%2C%20we%20introduce%20OpenIns3D%2C%20a%20new%203D-input-only%20framework%20for%203D%0Aopen-vocabulary%20scene%20understanding.%20The%20OpenIns3D%20framework%20employs%20a%0A%22Mask-Snap-Lookup%22%20scheme.%20The%20%22Mask%22%20module%20learns%20class-agnostic%20mask%0Aproposals%20in%203D%20point%20clouds%2C%20the%20%22Snap%22%20module%20generates%20synthetic%20scene-level%0Aimages%20at%20multiple%20scales%20and%20leverages%202D%20vision-language%20models%20to%20extract%0Ainteresting%20objects%2C%20and%20the%20%22Lookup%22%20module%20searches%20through%20the%20outcomes%20of%0A%22Snap%22%20to%20assign%20category%20names%20to%20the%20proposed%20masks.%20This%20approach%2C%20yet%0Asimple%2C%20achieves%20state-of-the-art%20performance%20across%20a%20wide%20range%20of%203D%0Aopen-vocabulary%20tasks%2C%20including%20recognition%2C%20object%20detection%2C%20and%20instance%0Asegmentation%2C%20on%20both%20indoor%20and%20outdoor%20datasets.%20Moreover%2C%20OpenIns3D%0Afacilitates%20effortless%20switching%20between%20different%202D%20detectors%20without%0Arequiring%20retraining.%20When%20integrated%20with%20powerful%202D%20open-world%20models%2C%20it%0Aachieves%20excellent%20results%20in%20scene%20understanding%20tasks.%20Furthermore%2C%20when%0Acombined%20with%20LLM-powered%202D%20models%2C%20OpenIns3D%20exhibits%20an%20impressive%0Acapability%20to%20comprehend%20and%20process%20highly%20complex%20text%20queries%20that%20demand%0Aintricate%20reasoning%20and%20real-world%20knowledge.%20Project%20page%3A%0Ahttps%3A//zheninghuang.github.io/OpenIns3D/%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.00616v5&entry.124074799=Read"},
{"title": "MR3D-Net: Dynamic Multi-Resolution 3D Sparse Voxel Grid Fusion for\n  LiDAR-Based Collective Perception", "author": "Sven Teufel and J\u00f6rg Gamerdinger and Georg Volk and Oliver Bringmann", "abstract": "  The safe operation of automated vehicles depends on their ability to perceive\nthe environment comprehensively. However, occlusion, sensor range, and\nenvironmental factors limit their perception capabilities. To overcome these\nlimitations, collective perception enables vehicles to exchange information.\nHowever, fusing this exchanged information is a challenging task. Early fusion\napproaches require large amounts of bandwidth, while intermediate fusion\napproaches face interchangeability issues. Late fusion of shared detections is\ncurrently the only feasible approach. However, it often results in inferior\nperformance due to information loss. To address this issue, we propose\nMR3D-Net, a dynamic multi-resolution 3D sparse voxel grid fusion backbone\narchitecture for LiDAR-based collective perception. We show that sparse voxel\ngrids at varying resolutions provide a meaningful and compact environment\nrepresentation that can adapt to the communication bandwidth. MR3D-Net achieves\nstate-of-the-art performance on the OPV2V 3D object detection benchmark while\nreducing the required bandwidth by up to 94% compared to early fusion. Code is\navailable at https://github.com/ekut-es/MR3D-Net\n", "link": "http://arxiv.org/abs/2408.06137v1", "date": "2024-08-12", "relevancy": 2.9368, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5969}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5943}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5709}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MR3D-Net%3A%20Dynamic%20Multi-Resolution%203D%20Sparse%20Voxel%20Grid%20Fusion%20for%0A%20%20LiDAR-Based%20Collective%20Perception&body=Title%3A%20MR3D-Net%3A%20Dynamic%20Multi-Resolution%203D%20Sparse%20Voxel%20Grid%20Fusion%20for%0A%20%20LiDAR-Based%20Collective%20Perception%0AAuthor%3A%20Sven%20Teufel%20and%20J%C3%B6rg%20Gamerdinger%20and%20Georg%20Volk%20and%20Oliver%20Bringmann%0AAbstract%3A%20%20%20The%20safe%20operation%20of%20automated%20vehicles%20depends%20on%20their%20ability%20to%20perceive%0Athe%20environment%20comprehensively.%20However%2C%20occlusion%2C%20sensor%20range%2C%20and%0Aenvironmental%20factors%20limit%20their%20perception%20capabilities.%20To%20overcome%20these%0Alimitations%2C%20collective%20perception%20enables%20vehicles%20to%20exchange%20information.%0AHowever%2C%20fusing%20this%20exchanged%20information%20is%20a%20challenging%20task.%20Early%20fusion%0Aapproaches%20require%20large%20amounts%20of%20bandwidth%2C%20while%20intermediate%20fusion%0Aapproaches%20face%20interchangeability%20issues.%20Late%20fusion%20of%20shared%20detections%20is%0Acurrently%20the%20only%20feasible%20approach.%20However%2C%20it%20often%20results%20in%20inferior%0Aperformance%20due%20to%20information%20loss.%20To%20address%20this%20issue%2C%20we%20propose%0AMR3D-Net%2C%20a%20dynamic%20multi-resolution%203D%20sparse%20voxel%20grid%20fusion%20backbone%0Aarchitecture%20for%20LiDAR-based%20collective%20perception.%20We%20show%20that%20sparse%20voxel%0Agrids%20at%20varying%20resolutions%20provide%20a%20meaningful%20and%20compact%20environment%0Arepresentation%20that%20can%20adapt%20to%20the%20communication%20bandwidth.%20MR3D-Net%20achieves%0Astate-of-the-art%20performance%20on%20the%20OPV2V%203D%20object%20detection%20benchmark%20while%0Areducing%20the%20required%20bandwidth%20by%20up%20to%2094%25%20compared%20to%20early%20fusion.%20Code%20is%0Aavailable%20at%20https%3A//github.com/ekut-es/MR3D-Net%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06137v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMR3D-Net%253A%2520Dynamic%2520Multi-Resolution%25203D%2520Sparse%2520Voxel%2520Grid%2520Fusion%2520for%250A%2520%2520LiDAR-Based%2520Collective%2520Perception%26entry.906535625%3DSven%2520Teufel%2520and%2520J%25C3%25B6rg%2520Gamerdinger%2520and%2520Georg%2520Volk%2520and%2520Oliver%2520Bringmann%26entry.1292438233%3D%2520%2520The%2520safe%2520operation%2520of%2520automated%2520vehicles%2520depends%2520on%2520their%2520ability%2520to%2520perceive%250Athe%2520environment%2520comprehensively.%2520However%252C%2520occlusion%252C%2520sensor%2520range%252C%2520and%250Aenvironmental%2520factors%2520limit%2520their%2520perception%2520capabilities.%2520To%2520overcome%2520these%250Alimitations%252C%2520collective%2520perception%2520enables%2520vehicles%2520to%2520exchange%2520information.%250AHowever%252C%2520fusing%2520this%2520exchanged%2520information%2520is%2520a%2520challenging%2520task.%2520Early%2520fusion%250Aapproaches%2520require%2520large%2520amounts%2520of%2520bandwidth%252C%2520while%2520intermediate%2520fusion%250Aapproaches%2520face%2520interchangeability%2520issues.%2520Late%2520fusion%2520of%2520shared%2520detections%2520is%250Acurrently%2520the%2520only%2520feasible%2520approach.%2520However%252C%2520it%2520often%2520results%2520in%2520inferior%250Aperformance%2520due%2520to%2520information%2520loss.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%250AMR3D-Net%252C%2520a%2520dynamic%2520multi-resolution%25203D%2520sparse%2520voxel%2520grid%2520fusion%2520backbone%250Aarchitecture%2520for%2520LiDAR-based%2520collective%2520perception.%2520We%2520show%2520that%2520sparse%2520voxel%250Agrids%2520at%2520varying%2520resolutions%2520provide%2520a%2520meaningful%2520and%2520compact%2520environment%250Arepresentation%2520that%2520can%2520adapt%2520to%2520the%2520communication%2520bandwidth.%2520MR3D-Net%2520achieves%250Astate-of-the-art%2520performance%2520on%2520the%2520OPV2V%25203D%2520object%2520detection%2520benchmark%2520while%250Areducing%2520the%2520required%2520bandwidth%2520by%2520up%2520to%252094%2525%2520compared%2520to%2520early%2520fusion.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/ekut-es/MR3D-Net%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06137v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MR3D-Net%3A%20Dynamic%20Multi-Resolution%203D%20Sparse%20Voxel%20Grid%20Fusion%20for%0A%20%20LiDAR-Based%20Collective%20Perception&entry.906535625=Sven%20Teufel%20and%20J%C3%B6rg%20Gamerdinger%20and%20Georg%20Volk%20and%20Oliver%20Bringmann&entry.1292438233=%20%20The%20safe%20operation%20of%20automated%20vehicles%20depends%20on%20their%20ability%20to%20perceive%0Athe%20environment%20comprehensively.%20However%2C%20occlusion%2C%20sensor%20range%2C%20and%0Aenvironmental%20factors%20limit%20their%20perception%20capabilities.%20To%20overcome%20these%0Alimitations%2C%20collective%20perception%20enables%20vehicles%20to%20exchange%20information.%0AHowever%2C%20fusing%20this%20exchanged%20information%20is%20a%20challenging%20task.%20Early%20fusion%0Aapproaches%20require%20large%20amounts%20of%20bandwidth%2C%20while%20intermediate%20fusion%0Aapproaches%20face%20interchangeability%20issues.%20Late%20fusion%20of%20shared%20detections%20is%0Acurrently%20the%20only%20feasible%20approach.%20However%2C%20it%20often%20results%20in%20inferior%0Aperformance%20due%20to%20information%20loss.%20To%20address%20this%20issue%2C%20we%20propose%0AMR3D-Net%2C%20a%20dynamic%20multi-resolution%203D%20sparse%20voxel%20grid%20fusion%20backbone%0Aarchitecture%20for%20LiDAR-based%20collective%20perception.%20We%20show%20that%20sparse%20voxel%0Agrids%20at%20varying%20resolutions%20provide%20a%20meaningful%20and%20compact%20environment%0Arepresentation%20that%20can%20adapt%20to%20the%20communication%20bandwidth.%20MR3D-Net%20achieves%0Astate-of-the-art%20performance%20on%20the%20OPV2V%203D%20object%20detection%20benchmark%20while%0Areducing%20the%20required%20bandwidth%20by%20up%20to%2094%25%20compared%20to%20early%20fusion.%20Code%20is%0Aavailable%20at%20https%3A//github.com/ekut-es/MR3D-Net%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06137v1&entry.124074799=Read"},
{"title": "MLAAN: Scaling Supervised Local Learning with Multilaminar Leap\n  Augmented Auxiliary Network", "author": "Yuming Zhang and Shouxin Zhang and Peizhe Wang and Feiyu Zhu and Dongzhi Guan and Junhao Su and Jiabin Liu and Changpeng Cai", "abstract": "  Deep neural networks (DNNs) typically employ an end-to-end (E2E) training\nparadigm which presents several challenges, including high GPU memory\nconsumption, inefficiency, and difficulties in model parallelization during\ntraining. Recent research has sought to address these issues, with one\npromising approach being local learning. This method involves partitioning the\nbackbone network into gradient-isolated modules and manually designing\nauxiliary networks to train these local modules. Existing methods often neglect\nthe interaction of information between local modules, leading to myopic issues\nand a performance gap compared to E2E training. To address these limitations,\nwe propose the Multilaminar Leap Augmented Auxiliary Network (MLAAN).\nSpecifically, MLAAN comprises Multilaminar Local Modules (MLM) and Leap\nAugmented Modules (LAM). MLM captures both local and global features through\nindependent and cascaded auxiliary networks, alleviating performance issues\ncaused by insufficient global features. However, overly simplistic auxiliary\nnetworks can impede MLM's ability to capture global information. To address\nthis, we further design LAM, an enhanced auxiliary network that uses the\nExponential Moving Average (EMA) method to facilitate information exchange\nbetween local modules, thereby mitigating the shortsightedness resulting from\ninadequate interaction. The synergy between MLM and LAM has demonstrated\nexcellent performance. Our experiments on the CIFAR-10, STL-10, SVHN, and\nImageNet datasets show that MLAAN can be seamlessly integrated into existing\nlocal learning frameworks, significantly enhancing their performance and even\nsurpassing end-to-end (E2E) training methods, while also reducing GPU memory\nconsumption.\n", "link": "http://arxiv.org/abs/2406.16633v2", "date": "2024-08-12", "relevancy": 2.8742, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5926}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5737}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MLAAN%3A%20Scaling%20Supervised%20Local%20Learning%20with%20Multilaminar%20Leap%0A%20%20Augmented%20Auxiliary%20Network&body=Title%3A%20MLAAN%3A%20Scaling%20Supervised%20Local%20Learning%20with%20Multilaminar%20Leap%0A%20%20Augmented%20Auxiliary%20Network%0AAuthor%3A%20Yuming%20Zhang%20and%20Shouxin%20Zhang%20and%20Peizhe%20Wang%20and%20Feiyu%20Zhu%20and%20Dongzhi%20Guan%20and%20Junhao%20Su%20and%20Jiabin%20Liu%20and%20Changpeng%20Cai%0AAbstract%3A%20%20%20Deep%20neural%20networks%20%28DNNs%29%20typically%20employ%20an%20end-to-end%20%28E2E%29%20training%0Aparadigm%20which%20presents%20several%20challenges%2C%20including%20high%20GPU%20memory%0Aconsumption%2C%20inefficiency%2C%20and%20difficulties%20in%20model%20parallelization%20during%0Atraining.%20Recent%20research%20has%20sought%20to%20address%20these%20issues%2C%20with%20one%0Apromising%20approach%20being%20local%20learning.%20This%20method%20involves%20partitioning%20the%0Abackbone%20network%20into%20gradient-isolated%20modules%20and%20manually%20designing%0Aauxiliary%20networks%20to%20train%20these%20local%20modules.%20Existing%20methods%20often%20neglect%0Athe%20interaction%20of%20information%20between%20local%20modules%2C%20leading%20to%20myopic%20issues%0Aand%20a%20performance%20gap%20compared%20to%20E2E%20training.%20To%20address%20these%20limitations%2C%0Awe%20propose%20the%20Multilaminar%20Leap%20Augmented%20Auxiliary%20Network%20%28MLAAN%29.%0ASpecifically%2C%20MLAAN%20comprises%20Multilaminar%20Local%20Modules%20%28MLM%29%20and%20Leap%0AAugmented%20Modules%20%28LAM%29.%20MLM%20captures%20both%20local%20and%20global%20features%20through%0Aindependent%20and%20cascaded%20auxiliary%20networks%2C%20alleviating%20performance%20issues%0Acaused%20by%20insufficient%20global%20features.%20However%2C%20overly%20simplistic%20auxiliary%0Anetworks%20can%20impede%20MLM%27s%20ability%20to%20capture%20global%20information.%20To%20address%0Athis%2C%20we%20further%20design%20LAM%2C%20an%20enhanced%20auxiliary%20network%20that%20uses%20the%0AExponential%20Moving%20Average%20%28EMA%29%20method%20to%20facilitate%20information%20exchange%0Abetween%20local%20modules%2C%20thereby%20mitigating%20the%20shortsightedness%20resulting%20from%0Ainadequate%20interaction.%20The%20synergy%20between%20MLM%20and%20LAM%20has%20demonstrated%0Aexcellent%20performance.%20Our%20experiments%20on%20the%20CIFAR-10%2C%20STL-10%2C%20SVHN%2C%20and%0AImageNet%20datasets%20show%20that%20MLAAN%20can%20be%20seamlessly%20integrated%20into%20existing%0Alocal%20learning%20frameworks%2C%20significantly%20enhancing%20their%20performance%20and%20even%0Asurpassing%20end-to-end%20%28E2E%29%20training%20methods%2C%20while%20also%20reducing%20GPU%20memory%0Aconsumption.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16633v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMLAAN%253A%2520Scaling%2520Supervised%2520Local%2520Learning%2520with%2520Multilaminar%2520Leap%250A%2520%2520Augmented%2520Auxiliary%2520Network%26entry.906535625%3DYuming%2520Zhang%2520and%2520Shouxin%2520Zhang%2520and%2520Peizhe%2520Wang%2520and%2520Feiyu%2520Zhu%2520and%2520Dongzhi%2520Guan%2520and%2520Junhao%2520Su%2520and%2520Jiabin%2520Liu%2520and%2520Changpeng%2520Cai%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520%2528DNNs%2529%2520typically%2520employ%2520an%2520end-to-end%2520%2528E2E%2529%2520training%250Aparadigm%2520which%2520presents%2520several%2520challenges%252C%2520including%2520high%2520GPU%2520memory%250Aconsumption%252C%2520inefficiency%252C%2520and%2520difficulties%2520in%2520model%2520parallelization%2520during%250Atraining.%2520Recent%2520research%2520has%2520sought%2520to%2520address%2520these%2520issues%252C%2520with%2520one%250Apromising%2520approach%2520being%2520local%2520learning.%2520This%2520method%2520involves%2520partitioning%2520the%250Abackbone%2520network%2520into%2520gradient-isolated%2520modules%2520and%2520manually%2520designing%250Aauxiliary%2520networks%2520to%2520train%2520these%2520local%2520modules.%2520Existing%2520methods%2520often%2520neglect%250Athe%2520interaction%2520of%2520information%2520between%2520local%2520modules%252C%2520leading%2520to%2520myopic%2520issues%250Aand%2520a%2520performance%2520gap%2520compared%2520to%2520E2E%2520training.%2520To%2520address%2520these%2520limitations%252C%250Awe%2520propose%2520the%2520Multilaminar%2520Leap%2520Augmented%2520Auxiliary%2520Network%2520%2528MLAAN%2529.%250ASpecifically%252C%2520MLAAN%2520comprises%2520Multilaminar%2520Local%2520Modules%2520%2528MLM%2529%2520and%2520Leap%250AAugmented%2520Modules%2520%2528LAM%2529.%2520MLM%2520captures%2520both%2520local%2520and%2520global%2520features%2520through%250Aindependent%2520and%2520cascaded%2520auxiliary%2520networks%252C%2520alleviating%2520performance%2520issues%250Acaused%2520by%2520insufficient%2520global%2520features.%2520However%252C%2520overly%2520simplistic%2520auxiliary%250Anetworks%2520can%2520impede%2520MLM%2527s%2520ability%2520to%2520capture%2520global%2520information.%2520To%2520address%250Athis%252C%2520we%2520further%2520design%2520LAM%252C%2520an%2520enhanced%2520auxiliary%2520network%2520that%2520uses%2520the%250AExponential%2520Moving%2520Average%2520%2528EMA%2529%2520method%2520to%2520facilitate%2520information%2520exchange%250Abetween%2520local%2520modules%252C%2520thereby%2520mitigating%2520the%2520shortsightedness%2520resulting%2520from%250Ainadequate%2520interaction.%2520The%2520synergy%2520between%2520MLM%2520and%2520LAM%2520has%2520demonstrated%250Aexcellent%2520performance.%2520Our%2520experiments%2520on%2520the%2520CIFAR-10%252C%2520STL-10%252C%2520SVHN%252C%2520and%250AImageNet%2520datasets%2520show%2520that%2520MLAAN%2520can%2520be%2520seamlessly%2520integrated%2520into%2520existing%250Alocal%2520learning%2520frameworks%252C%2520significantly%2520enhancing%2520their%2520performance%2520and%2520even%250Asurpassing%2520end-to-end%2520%2528E2E%2529%2520training%2520methods%252C%2520while%2520also%2520reducing%2520GPU%2520memory%250Aconsumption.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16633v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MLAAN%3A%20Scaling%20Supervised%20Local%20Learning%20with%20Multilaminar%20Leap%0A%20%20Augmented%20Auxiliary%20Network&entry.906535625=Yuming%20Zhang%20and%20Shouxin%20Zhang%20and%20Peizhe%20Wang%20and%20Feiyu%20Zhu%20and%20Dongzhi%20Guan%20and%20Junhao%20Su%20and%20Jiabin%20Liu%20and%20Changpeng%20Cai&entry.1292438233=%20%20Deep%20neural%20networks%20%28DNNs%29%20typically%20employ%20an%20end-to-end%20%28E2E%29%20training%0Aparadigm%20which%20presents%20several%20challenges%2C%20including%20high%20GPU%20memory%0Aconsumption%2C%20inefficiency%2C%20and%20difficulties%20in%20model%20parallelization%20during%0Atraining.%20Recent%20research%20has%20sought%20to%20address%20these%20issues%2C%20with%20one%0Apromising%20approach%20being%20local%20learning.%20This%20method%20involves%20partitioning%20the%0Abackbone%20network%20into%20gradient-isolated%20modules%20and%20manually%20designing%0Aauxiliary%20networks%20to%20train%20these%20local%20modules.%20Existing%20methods%20often%20neglect%0Athe%20interaction%20of%20information%20between%20local%20modules%2C%20leading%20to%20myopic%20issues%0Aand%20a%20performance%20gap%20compared%20to%20E2E%20training.%20To%20address%20these%20limitations%2C%0Awe%20propose%20the%20Multilaminar%20Leap%20Augmented%20Auxiliary%20Network%20%28MLAAN%29.%0ASpecifically%2C%20MLAAN%20comprises%20Multilaminar%20Local%20Modules%20%28MLM%29%20and%20Leap%0AAugmented%20Modules%20%28LAM%29.%20MLM%20captures%20both%20local%20and%20global%20features%20through%0Aindependent%20and%20cascaded%20auxiliary%20networks%2C%20alleviating%20performance%20issues%0Acaused%20by%20insufficient%20global%20features.%20However%2C%20overly%20simplistic%20auxiliary%0Anetworks%20can%20impede%20MLM%27s%20ability%20to%20capture%20global%20information.%20To%20address%0Athis%2C%20we%20further%20design%20LAM%2C%20an%20enhanced%20auxiliary%20network%20that%20uses%20the%0AExponential%20Moving%20Average%20%28EMA%29%20method%20to%20facilitate%20information%20exchange%0Abetween%20local%20modules%2C%20thereby%20mitigating%20the%20shortsightedness%20resulting%20from%0Ainadequate%20interaction.%20The%20synergy%20between%20MLM%20and%20LAM%20has%20demonstrated%0Aexcellent%20performance.%20Our%20experiments%20on%20the%20CIFAR-10%2C%20STL-10%2C%20SVHN%2C%20and%0AImageNet%20datasets%20show%20that%20MLAAN%20can%20be%20seamlessly%20integrated%20into%20existing%0Alocal%20learning%20frameworks%2C%20significantly%20enhancing%20their%20performance%20and%20even%0Asurpassing%20end-to-end%20%28E2E%29%20training%20methods%2C%20while%20also%20reducing%20GPU%20memory%0Aconsumption.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16633v2&entry.124074799=Read"},
{"title": "Momentum Auxiliary Network for Supervised Local Learning", "author": "Junhao Su and Changpeng Cai and Feiyu Zhu and Chenghao He and Xiaojie Xu and Dongzhi Guan and Chenyang Si", "abstract": "  Deep neural networks conventionally employ end-to-end backpropagation for\ntheir training process, which lacks biological credibility and triggers a\nlocking dilemma during network parameter updates, leading to significant GPU\nmemory use. Supervised local learning, which segments the network into multiple\nlocal blocks updated by independent auxiliary networks. However, these methods\ncannot replace end-to-end training due to lower accuracy, as gradients only\npropagate within their local block, creating a lack of information exchange\nbetween blocks. To address this issue and establish information transfer across\nblocks, we propose a Momentum Auxiliary Network (MAN) that establishes a\ndynamic interaction mechanism. The MAN leverages an exponential moving average\n(EMA) of the parameters from adjacent local blocks to enhance information flow.\nThis auxiliary network, updated through EMA, helps bridge the informational gap\nbetween blocks. Nevertheless, we observe that directly applying EMA parameters\nhas certain limitations due to feature discrepancies among local blocks. To\novercome this, we introduce learnable biases, further boosting performance. We\nhave validated our method on four image classification datasets (CIFAR-10,\nSTL-10, SVHN, ImageNet), attaining superior performance and substantial memory\nsavings. Notably, our method can reduce GPU memory usage by more than 45\\% on\nthe ImageNet dataset compared to end-to-end training, while achieving higher\nperformance. The Momentum Auxiliary Network thus offers a new perspective for\nsupervised local learning. Our code is available at:\nhttps://github.com/JunhaoSu0/MAN.\n", "link": "http://arxiv.org/abs/2407.05623v4", "date": "2024-08-12", "relevancy": 2.8337, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5877}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5739}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Momentum%20Auxiliary%20Network%20for%20Supervised%20Local%20Learning&body=Title%3A%20Momentum%20Auxiliary%20Network%20for%20Supervised%20Local%20Learning%0AAuthor%3A%20Junhao%20Su%20and%20Changpeng%20Cai%20and%20Feiyu%20Zhu%20and%20Chenghao%20He%20and%20Xiaojie%20Xu%20and%20Dongzhi%20Guan%20and%20Chenyang%20Si%0AAbstract%3A%20%20%20Deep%20neural%20networks%20conventionally%20employ%20end-to-end%20backpropagation%20for%0Atheir%20training%20process%2C%20which%20lacks%20biological%20credibility%20and%20triggers%20a%0Alocking%20dilemma%20during%20network%20parameter%20updates%2C%20leading%20to%20significant%20GPU%0Amemory%20use.%20Supervised%20local%20learning%2C%20which%20segments%20the%20network%20into%20multiple%0Alocal%20blocks%20updated%20by%20independent%20auxiliary%20networks.%20However%2C%20these%20methods%0Acannot%20replace%20end-to-end%20training%20due%20to%20lower%20accuracy%2C%20as%20gradients%20only%0Apropagate%20within%20their%20local%20block%2C%20creating%20a%20lack%20of%20information%20exchange%0Abetween%20blocks.%20To%20address%20this%20issue%20and%20establish%20information%20transfer%20across%0Ablocks%2C%20we%20propose%20a%20Momentum%20Auxiliary%20Network%20%28MAN%29%20that%20establishes%20a%0Adynamic%20interaction%20mechanism.%20The%20MAN%20leverages%20an%20exponential%20moving%20average%0A%28EMA%29%20of%20the%20parameters%20from%20adjacent%20local%20blocks%20to%20enhance%20information%20flow.%0AThis%20auxiliary%20network%2C%20updated%20through%20EMA%2C%20helps%20bridge%20the%20informational%20gap%0Abetween%20blocks.%20Nevertheless%2C%20we%20observe%20that%20directly%20applying%20EMA%20parameters%0Ahas%20certain%20limitations%20due%20to%20feature%20discrepancies%20among%20local%20blocks.%20To%0Aovercome%20this%2C%20we%20introduce%20learnable%20biases%2C%20further%20boosting%20performance.%20We%0Ahave%20validated%20our%20method%20on%20four%20image%20classification%20datasets%20%28CIFAR-10%2C%0ASTL-10%2C%20SVHN%2C%20ImageNet%29%2C%20attaining%20superior%20performance%20and%20substantial%20memory%0Asavings.%20Notably%2C%20our%20method%20can%20reduce%20GPU%20memory%20usage%20by%20more%20than%2045%5C%25%20on%0Athe%20ImageNet%20dataset%20compared%20to%20end-to-end%20training%2C%20while%20achieving%20higher%0Aperformance.%20The%20Momentum%20Auxiliary%20Network%20thus%20offers%20a%20new%20perspective%20for%0Asupervised%20local%20learning.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/JunhaoSu0/MAN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05623v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMomentum%2520Auxiliary%2520Network%2520for%2520Supervised%2520Local%2520Learning%26entry.906535625%3DJunhao%2520Su%2520and%2520Changpeng%2520Cai%2520and%2520Feiyu%2520Zhu%2520and%2520Chenghao%2520He%2520and%2520Xiaojie%2520Xu%2520and%2520Dongzhi%2520Guan%2520and%2520Chenyang%2520Si%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520conventionally%2520employ%2520end-to-end%2520backpropagation%2520for%250Atheir%2520training%2520process%252C%2520which%2520lacks%2520biological%2520credibility%2520and%2520triggers%2520a%250Alocking%2520dilemma%2520during%2520network%2520parameter%2520updates%252C%2520leading%2520to%2520significant%2520GPU%250Amemory%2520use.%2520Supervised%2520local%2520learning%252C%2520which%2520segments%2520the%2520network%2520into%2520multiple%250Alocal%2520blocks%2520updated%2520by%2520independent%2520auxiliary%2520networks.%2520However%252C%2520these%2520methods%250Acannot%2520replace%2520end-to-end%2520training%2520due%2520to%2520lower%2520accuracy%252C%2520as%2520gradients%2520only%250Apropagate%2520within%2520their%2520local%2520block%252C%2520creating%2520a%2520lack%2520of%2520information%2520exchange%250Abetween%2520blocks.%2520To%2520address%2520this%2520issue%2520and%2520establish%2520information%2520transfer%2520across%250Ablocks%252C%2520we%2520propose%2520a%2520Momentum%2520Auxiliary%2520Network%2520%2528MAN%2529%2520that%2520establishes%2520a%250Adynamic%2520interaction%2520mechanism.%2520The%2520MAN%2520leverages%2520an%2520exponential%2520moving%2520average%250A%2528EMA%2529%2520of%2520the%2520parameters%2520from%2520adjacent%2520local%2520blocks%2520to%2520enhance%2520information%2520flow.%250AThis%2520auxiliary%2520network%252C%2520updated%2520through%2520EMA%252C%2520helps%2520bridge%2520the%2520informational%2520gap%250Abetween%2520blocks.%2520Nevertheless%252C%2520we%2520observe%2520that%2520directly%2520applying%2520EMA%2520parameters%250Ahas%2520certain%2520limitations%2520due%2520to%2520feature%2520discrepancies%2520among%2520local%2520blocks.%2520To%250Aovercome%2520this%252C%2520we%2520introduce%2520learnable%2520biases%252C%2520further%2520boosting%2520performance.%2520We%250Ahave%2520validated%2520our%2520method%2520on%2520four%2520image%2520classification%2520datasets%2520%2528CIFAR-10%252C%250ASTL-10%252C%2520SVHN%252C%2520ImageNet%2529%252C%2520attaining%2520superior%2520performance%2520and%2520substantial%2520memory%250Asavings.%2520Notably%252C%2520our%2520method%2520can%2520reduce%2520GPU%2520memory%2520usage%2520by%2520more%2520than%252045%255C%2525%2520on%250Athe%2520ImageNet%2520dataset%2520compared%2520to%2520end-to-end%2520training%252C%2520while%2520achieving%2520higher%250Aperformance.%2520The%2520Momentum%2520Auxiliary%2520Network%2520thus%2520offers%2520a%2520new%2520perspective%2520for%250Asupervised%2520local%2520learning.%2520Our%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/JunhaoSu0/MAN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05623v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Momentum%20Auxiliary%20Network%20for%20Supervised%20Local%20Learning&entry.906535625=Junhao%20Su%20and%20Changpeng%20Cai%20and%20Feiyu%20Zhu%20and%20Chenghao%20He%20and%20Xiaojie%20Xu%20and%20Dongzhi%20Guan%20and%20Chenyang%20Si&entry.1292438233=%20%20Deep%20neural%20networks%20conventionally%20employ%20end-to-end%20backpropagation%20for%0Atheir%20training%20process%2C%20which%20lacks%20biological%20credibility%20and%20triggers%20a%0Alocking%20dilemma%20during%20network%20parameter%20updates%2C%20leading%20to%20significant%20GPU%0Amemory%20use.%20Supervised%20local%20learning%2C%20which%20segments%20the%20network%20into%20multiple%0Alocal%20blocks%20updated%20by%20independent%20auxiliary%20networks.%20However%2C%20these%20methods%0Acannot%20replace%20end-to-end%20training%20due%20to%20lower%20accuracy%2C%20as%20gradients%20only%0Apropagate%20within%20their%20local%20block%2C%20creating%20a%20lack%20of%20information%20exchange%0Abetween%20blocks.%20To%20address%20this%20issue%20and%20establish%20information%20transfer%20across%0Ablocks%2C%20we%20propose%20a%20Momentum%20Auxiliary%20Network%20%28MAN%29%20that%20establishes%20a%0Adynamic%20interaction%20mechanism.%20The%20MAN%20leverages%20an%20exponential%20moving%20average%0A%28EMA%29%20of%20the%20parameters%20from%20adjacent%20local%20blocks%20to%20enhance%20information%20flow.%0AThis%20auxiliary%20network%2C%20updated%20through%20EMA%2C%20helps%20bridge%20the%20informational%20gap%0Abetween%20blocks.%20Nevertheless%2C%20we%20observe%20that%20directly%20applying%20EMA%20parameters%0Ahas%20certain%20limitations%20due%20to%20feature%20discrepancies%20among%20local%20blocks.%20To%0Aovercome%20this%2C%20we%20introduce%20learnable%20biases%2C%20further%20boosting%20performance.%20We%0Ahave%20validated%20our%20method%20on%20four%20image%20classification%20datasets%20%28CIFAR-10%2C%0ASTL-10%2C%20SVHN%2C%20ImageNet%29%2C%20attaining%20superior%20performance%20and%20substantial%20memory%0Asavings.%20Notably%2C%20our%20method%20can%20reduce%20GPU%20memory%20usage%20by%20more%20than%2045%5C%25%20on%0Athe%20ImageNet%20dataset%20compared%20to%20end-to-end%20training%2C%20while%20achieving%20higher%0Aperformance.%20The%20Momentum%20Auxiliary%20Network%20thus%20offers%20a%20new%20perspective%20for%0Asupervised%20local%20learning.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/JunhaoSu0/MAN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05623v4&entry.124074799=Read"},
{"title": "3D Reconstruction of Protein Structures from Multi-view AFM Images using\n  Neural Radiance Fields (NeRFs)", "author": "Jaydeep Rade and Ethan Herron and Soumik Sarkar and Anwesha Sarkar and Adarsh Krishnamurthy", "abstract": "  Recent advancements in deep learning for predicting 3D protein structures\nhave shown promise, particularly when leveraging inputs like protein sequences\nand Cryo-Electron microscopy (Cryo-EM) images. However, these techniques often\nfall short when predicting the structures of protein complexes (PCs), which\ninvolve multiple proteins. In our study, we investigate using atomic force\nmicroscopy (AFM) combined with deep learning to predict the 3D structures of\nPCs. AFM generates height maps that depict the PCs in various random\norientations, providing a rich information for training a neural network to\npredict the 3D structures. We then employ the pre-trained UpFusion model (which\nutilizes a conditional diffusion model for synthesizing novel views) to train\nan instance-specific NeRF model for 3D reconstruction. The performance of\nUpFusion is evaluated through zero-shot predictions of 3D protein structures\nusing AFM images. The challenge, however, lies in the time-intensive and\nimpractical nature of collecting actual AFM images. To address this, we use a\nvirtual AFM imaging process that transforms a `PDB' protein file into\nmulti-view 2D virtual AFM images via volume rendering techniques. We\nextensively validate the UpFusion architecture using both virtual and actual\nmulti-view AFM images. Our results include a comparison of structures predicted\nwith varying numbers of views and different sets of views. This novel approach\nholds significant potential for enhancing the accuracy of protein complex\nstructure predictions with further fine-tuning of the UpFusion network.\n", "link": "http://arxiv.org/abs/2408.06244v1", "date": "2024-08-12", "relevancy": 2.8101, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5833}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5833}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5195}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Reconstruction%20of%20Protein%20Structures%20from%20Multi-view%20AFM%20Images%20using%0A%20%20Neural%20Radiance%20Fields%20%28NeRFs%29&body=Title%3A%203D%20Reconstruction%20of%20Protein%20Structures%20from%20Multi-view%20AFM%20Images%20using%0A%20%20Neural%20Radiance%20Fields%20%28NeRFs%29%0AAuthor%3A%20Jaydeep%20Rade%20and%20Ethan%20Herron%20and%20Soumik%20Sarkar%20and%20Anwesha%20Sarkar%20and%20Adarsh%20Krishnamurthy%0AAbstract%3A%20%20%20Recent%20advancements%20in%20deep%20learning%20for%20predicting%203D%20protein%20structures%0Ahave%20shown%20promise%2C%20particularly%20when%20leveraging%20inputs%20like%20protein%20sequences%0Aand%20Cryo-Electron%20microscopy%20%28Cryo-EM%29%20images.%20However%2C%20these%20techniques%20often%0Afall%20short%20when%20predicting%20the%20structures%20of%20protein%20complexes%20%28PCs%29%2C%20which%0Ainvolve%20multiple%20proteins.%20In%20our%20study%2C%20we%20investigate%20using%20atomic%20force%0Amicroscopy%20%28AFM%29%20combined%20with%20deep%20learning%20to%20predict%20the%203D%20structures%20of%0APCs.%20AFM%20generates%20height%20maps%20that%20depict%20the%20PCs%20in%20various%20random%0Aorientations%2C%20providing%20a%20rich%20information%20for%20training%20a%20neural%20network%20to%0Apredict%20the%203D%20structures.%20We%20then%20employ%20the%20pre-trained%20UpFusion%20model%20%28which%0Autilizes%20a%20conditional%20diffusion%20model%20for%20synthesizing%20novel%20views%29%20to%20train%0Aan%20instance-specific%20NeRF%20model%20for%203D%20reconstruction.%20The%20performance%20of%0AUpFusion%20is%20evaluated%20through%20zero-shot%20predictions%20of%203D%20protein%20structures%0Ausing%20AFM%20images.%20The%20challenge%2C%20however%2C%20lies%20in%20the%20time-intensive%20and%0Aimpractical%20nature%20of%20collecting%20actual%20AFM%20images.%20To%20address%20this%2C%20we%20use%20a%0Avirtual%20AFM%20imaging%20process%20that%20transforms%20a%20%60PDB%27%20protein%20file%20into%0Amulti-view%202D%20virtual%20AFM%20images%20via%20volume%20rendering%20techniques.%20We%0Aextensively%20validate%20the%20UpFusion%20architecture%20using%20both%20virtual%20and%20actual%0Amulti-view%20AFM%20images.%20Our%20results%20include%20a%20comparison%20of%20structures%20predicted%0Awith%20varying%20numbers%20of%20views%20and%20different%20sets%20of%20views.%20This%20novel%20approach%0Aholds%20significant%20potential%20for%20enhancing%20the%20accuracy%20of%20protein%20complex%0Astructure%20predictions%20with%20further%20fine-tuning%20of%20the%20UpFusion%20network.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06244v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Reconstruction%2520of%2520Protein%2520Structures%2520from%2520Multi-view%2520AFM%2520Images%2520using%250A%2520%2520Neural%2520Radiance%2520Fields%2520%2528NeRFs%2529%26entry.906535625%3DJaydeep%2520Rade%2520and%2520Ethan%2520Herron%2520and%2520Soumik%2520Sarkar%2520and%2520Anwesha%2520Sarkar%2520and%2520Adarsh%2520Krishnamurthy%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520deep%2520learning%2520for%2520predicting%25203D%2520protein%2520structures%250Ahave%2520shown%2520promise%252C%2520particularly%2520when%2520leveraging%2520inputs%2520like%2520protein%2520sequences%250Aand%2520Cryo-Electron%2520microscopy%2520%2528Cryo-EM%2529%2520images.%2520However%252C%2520these%2520techniques%2520often%250Afall%2520short%2520when%2520predicting%2520the%2520structures%2520of%2520protein%2520complexes%2520%2528PCs%2529%252C%2520which%250Ainvolve%2520multiple%2520proteins.%2520In%2520our%2520study%252C%2520we%2520investigate%2520using%2520atomic%2520force%250Amicroscopy%2520%2528AFM%2529%2520combined%2520with%2520deep%2520learning%2520to%2520predict%2520the%25203D%2520structures%2520of%250APCs.%2520AFM%2520generates%2520height%2520maps%2520that%2520depict%2520the%2520PCs%2520in%2520various%2520random%250Aorientations%252C%2520providing%2520a%2520rich%2520information%2520for%2520training%2520a%2520neural%2520network%2520to%250Apredict%2520the%25203D%2520structures.%2520We%2520then%2520employ%2520the%2520pre-trained%2520UpFusion%2520model%2520%2528which%250Autilizes%2520a%2520conditional%2520diffusion%2520model%2520for%2520synthesizing%2520novel%2520views%2529%2520to%2520train%250Aan%2520instance-specific%2520NeRF%2520model%2520for%25203D%2520reconstruction.%2520The%2520performance%2520of%250AUpFusion%2520is%2520evaluated%2520through%2520zero-shot%2520predictions%2520of%25203D%2520protein%2520structures%250Ausing%2520AFM%2520images.%2520The%2520challenge%252C%2520however%252C%2520lies%2520in%2520the%2520time-intensive%2520and%250Aimpractical%2520nature%2520of%2520collecting%2520actual%2520AFM%2520images.%2520To%2520address%2520this%252C%2520we%2520use%2520a%250Avirtual%2520AFM%2520imaging%2520process%2520that%2520transforms%2520a%2520%2560PDB%2527%2520protein%2520file%2520into%250Amulti-view%25202D%2520virtual%2520AFM%2520images%2520via%2520volume%2520rendering%2520techniques.%2520We%250Aextensively%2520validate%2520the%2520UpFusion%2520architecture%2520using%2520both%2520virtual%2520and%2520actual%250Amulti-view%2520AFM%2520images.%2520Our%2520results%2520include%2520a%2520comparison%2520of%2520structures%2520predicted%250Awith%2520varying%2520numbers%2520of%2520views%2520and%2520different%2520sets%2520of%2520views.%2520This%2520novel%2520approach%250Aholds%2520significant%2520potential%2520for%2520enhancing%2520the%2520accuracy%2520of%2520protein%2520complex%250Astructure%2520predictions%2520with%2520further%2520fine-tuning%2520of%2520the%2520UpFusion%2520network.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06244v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Reconstruction%20of%20Protein%20Structures%20from%20Multi-view%20AFM%20Images%20using%0A%20%20Neural%20Radiance%20Fields%20%28NeRFs%29&entry.906535625=Jaydeep%20Rade%20and%20Ethan%20Herron%20and%20Soumik%20Sarkar%20and%20Anwesha%20Sarkar%20and%20Adarsh%20Krishnamurthy&entry.1292438233=%20%20Recent%20advancements%20in%20deep%20learning%20for%20predicting%203D%20protein%20structures%0Ahave%20shown%20promise%2C%20particularly%20when%20leveraging%20inputs%20like%20protein%20sequences%0Aand%20Cryo-Electron%20microscopy%20%28Cryo-EM%29%20images.%20However%2C%20these%20techniques%20often%0Afall%20short%20when%20predicting%20the%20structures%20of%20protein%20complexes%20%28PCs%29%2C%20which%0Ainvolve%20multiple%20proteins.%20In%20our%20study%2C%20we%20investigate%20using%20atomic%20force%0Amicroscopy%20%28AFM%29%20combined%20with%20deep%20learning%20to%20predict%20the%203D%20structures%20of%0APCs.%20AFM%20generates%20height%20maps%20that%20depict%20the%20PCs%20in%20various%20random%0Aorientations%2C%20providing%20a%20rich%20information%20for%20training%20a%20neural%20network%20to%0Apredict%20the%203D%20structures.%20We%20then%20employ%20the%20pre-trained%20UpFusion%20model%20%28which%0Autilizes%20a%20conditional%20diffusion%20model%20for%20synthesizing%20novel%20views%29%20to%20train%0Aan%20instance-specific%20NeRF%20model%20for%203D%20reconstruction.%20The%20performance%20of%0AUpFusion%20is%20evaluated%20through%20zero-shot%20predictions%20of%203D%20protein%20structures%0Ausing%20AFM%20images.%20The%20challenge%2C%20however%2C%20lies%20in%20the%20time-intensive%20and%0Aimpractical%20nature%20of%20collecting%20actual%20AFM%20images.%20To%20address%20this%2C%20we%20use%20a%0Avirtual%20AFM%20imaging%20process%20that%20transforms%20a%20%60PDB%27%20protein%20file%20into%0Amulti-view%202D%20virtual%20AFM%20images%20via%20volume%20rendering%20techniques.%20We%0Aextensively%20validate%20the%20UpFusion%20architecture%20using%20both%20virtual%20and%20actual%0Amulti-view%20AFM%20images.%20Our%20results%20include%20a%20comparison%20of%20structures%20predicted%0Awith%20varying%20numbers%20of%20views%20and%20different%20sets%20of%20views.%20This%20novel%20approach%0Aholds%20significant%20potential%20for%20enhancing%20the%20accuracy%20of%20protein%20complex%0Astructure%20predictions%20with%20further%20fine-tuning%20of%20the%20UpFusion%20network.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06244v1&entry.124074799=Read"},
{"title": "Utilizing Navigation Paths to Generate Target Points for Enhanced\n  End-to-End Autonomous Driving Planning", "author": "Yuanhua Shen and Jun Li", "abstract": "  In recent years, end-to-end autonomous driving frameworks have been shown to\nnot only enhance perception performance but also improve planning capabilities.\nHowever, most previous end-to-end autonomous driving frameworks have focused\nprimarily on enhancing environmental perception while neglecting the learning\nof autonomous vehicle driving intent, which refers to the vehicle's intended\ndirection of travel. In planning, the autonomous vehicle's direction is clear\nand well-defined, yet this crucial aspect has often been overlooked. This paper\nintroduces NTT (Navigation to Target for Trajectory planning), a method within\nan end-to-end framework for autonomous driving. NTT generates the planned\ntrajectory in two steps. First, it generates the future target point for the\nautonomous vehicle on the basis of the navigation path. Then, it produces the\ncomplete planned trajectory on the basis of this target point. On the one hand,\ngenerating the target point for the autonomous vehicle from the navigation path\nenables the vehicle to learn a clear driving intent. On the other hand,\ngenerating the trajectory on the basis of the target point allows for a\nflexible planned trajectory that can adapt to complex environmental changes,\nthereby enhancing the safety of the planning process. Our method achieved\nexcellent planning performance on the widely used nuScenes dataset and its\neffectiveness was validated through ablation experiments.\n", "link": "http://arxiv.org/abs/2406.08349v2", "date": "2024-08-12", "relevancy": 2.7427, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5665}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.546}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5331}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Utilizing%20Navigation%20Paths%20to%20Generate%20Target%20Points%20for%20Enhanced%0A%20%20End-to-End%20Autonomous%20Driving%20Planning&body=Title%3A%20Utilizing%20Navigation%20Paths%20to%20Generate%20Target%20Points%20for%20Enhanced%0A%20%20End-to-End%20Autonomous%20Driving%20Planning%0AAuthor%3A%20Yuanhua%20Shen%20and%20Jun%20Li%0AAbstract%3A%20%20%20In%20recent%20years%2C%20end-to-end%20autonomous%20driving%20frameworks%20have%20been%20shown%20to%0Anot%20only%20enhance%20perception%20performance%20but%20also%20improve%20planning%20capabilities.%0AHowever%2C%20most%20previous%20end-to-end%20autonomous%20driving%20frameworks%20have%20focused%0Aprimarily%20on%20enhancing%20environmental%20perception%20while%20neglecting%20the%20learning%0Aof%20autonomous%20vehicle%20driving%20intent%2C%20which%20refers%20to%20the%20vehicle%27s%20intended%0Adirection%20of%20travel.%20In%20planning%2C%20the%20autonomous%20vehicle%27s%20direction%20is%20clear%0Aand%20well-defined%2C%20yet%20this%20crucial%20aspect%20has%20often%20been%20overlooked.%20This%20paper%0Aintroduces%20NTT%20%28Navigation%20to%20Target%20for%20Trajectory%20planning%29%2C%20a%20method%20within%0Aan%20end-to-end%20framework%20for%20autonomous%20driving.%20NTT%20generates%20the%20planned%0Atrajectory%20in%20two%20steps.%20First%2C%20it%20generates%20the%20future%20target%20point%20for%20the%0Aautonomous%20vehicle%20on%20the%20basis%20of%20the%20navigation%20path.%20Then%2C%20it%20produces%20the%0Acomplete%20planned%20trajectory%20on%20the%20basis%20of%20this%20target%20point.%20On%20the%20one%20hand%2C%0Agenerating%20the%20target%20point%20for%20the%20autonomous%20vehicle%20from%20the%20navigation%20path%0Aenables%20the%20vehicle%20to%20learn%20a%20clear%20driving%20intent.%20On%20the%20other%20hand%2C%0Agenerating%20the%20trajectory%20on%20the%20basis%20of%20the%20target%20point%20allows%20for%20a%0Aflexible%20planned%20trajectory%20that%20can%20adapt%20to%20complex%20environmental%20changes%2C%0Athereby%20enhancing%20the%20safety%20of%20the%20planning%20process.%20Our%20method%20achieved%0Aexcellent%20planning%20performance%20on%20the%20widely%20used%20nuScenes%20dataset%20and%20its%0Aeffectiveness%20was%20validated%20through%20ablation%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08349v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUtilizing%2520Navigation%2520Paths%2520to%2520Generate%2520Target%2520Points%2520for%2520Enhanced%250A%2520%2520End-to-End%2520Autonomous%2520Driving%2520Planning%26entry.906535625%3DYuanhua%2520Shen%2520and%2520Jun%2520Li%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520end-to-end%2520autonomous%2520driving%2520frameworks%2520have%2520been%2520shown%2520to%250Anot%2520only%2520enhance%2520perception%2520performance%2520but%2520also%2520improve%2520planning%2520capabilities.%250AHowever%252C%2520most%2520previous%2520end-to-end%2520autonomous%2520driving%2520frameworks%2520have%2520focused%250Aprimarily%2520on%2520enhancing%2520environmental%2520perception%2520while%2520neglecting%2520the%2520learning%250Aof%2520autonomous%2520vehicle%2520driving%2520intent%252C%2520which%2520refers%2520to%2520the%2520vehicle%2527s%2520intended%250Adirection%2520of%2520travel.%2520In%2520planning%252C%2520the%2520autonomous%2520vehicle%2527s%2520direction%2520is%2520clear%250Aand%2520well-defined%252C%2520yet%2520this%2520crucial%2520aspect%2520has%2520often%2520been%2520overlooked.%2520This%2520paper%250Aintroduces%2520NTT%2520%2528Navigation%2520to%2520Target%2520for%2520Trajectory%2520planning%2529%252C%2520a%2520method%2520within%250Aan%2520end-to-end%2520framework%2520for%2520autonomous%2520driving.%2520NTT%2520generates%2520the%2520planned%250Atrajectory%2520in%2520two%2520steps.%2520First%252C%2520it%2520generates%2520the%2520future%2520target%2520point%2520for%2520the%250Aautonomous%2520vehicle%2520on%2520the%2520basis%2520of%2520the%2520navigation%2520path.%2520Then%252C%2520it%2520produces%2520the%250Acomplete%2520planned%2520trajectory%2520on%2520the%2520basis%2520of%2520this%2520target%2520point.%2520On%2520the%2520one%2520hand%252C%250Agenerating%2520the%2520target%2520point%2520for%2520the%2520autonomous%2520vehicle%2520from%2520the%2520navigation%2520path%250Aenables%2520the%2520vehicle%2520to%2520learn%2520a%2520clear%2520driving%2520intent.%2520On%2520the%2520other%2520hand%252C%250Agenerating%2520the%2520trajectory%2520on%2520the%2520basis%2520of%2520the%2520target%2520point%2520allows%2520for%2520a%250Aflexible%2520planned%2520trajectory%2520that%2520can%2520adapt%2520to%2520complex%2520environmental%2520changes%252C%250Athereby%2520enhancing%2520the%2520safety%2520of%2520the%2520planning%2520process.%2520Our%2520method%2520achieved%250Aexcellent%2520planning%2520performance%2520on%2520the%2520widely%2520used%2520nuScenes%2520dataset%2520and%2520its%250Aeffectiveness%2520was%2520validated%2520through%2520ablation%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08349v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Utilizing%20Navigation%20Paths%20to%20Generate%20Target%20Points%20for%20Enhanced%0A%20%20End-to-End%20Autonomous%20Driving%20Planning&entry.906535625=Yuanhua%20Shen%20and%20Jun%20Li&entry.1292438233=%20%20In%20recent%20years%2C%20end-to-end%20autonomous%20driving%20frameworks%20have%20been%20shown%20to%0Anot%20only%20enhance%20perception%20performance%20but%20also%20improve%20planning%20capabilities.%0AHowever%2C%20most%20previous%20end-to-end%20autonomous%20driving%20frameworks%20have%20focused%0Aprimarily%20on%20enhancing%20environmental%20perception%20while%20neglecting%20the%20learning%0Aof%20autonomous%20vehicle%20driving%20intent%2C%20which%20refers%20to%20the%20vehicle%27s%20intended%0Adirection%20of%20travel.%20In%20planning%2C%20the%20autonomous%20vehicle%27s%20direction%20is%20clear%0Aand%20well-defined%2C%20yet%20this%20crucial%20aspect%20has%20often%20been%20overlooked.%20This%20paper%0Aintroduces%20NTT%20%28Navigation%20to%20Target%20for%20Trajectory%20planning%29%2C%20a%20method%20within%0Aan%20end-to-end%20framework%20for%20autonomous%20driving.%20NTT%20generates%20the%20planned%0Atrajectory%20in%20two%20steps.%20First%2C%20it%20generates%20the%20future%20target%20point%20for%20the%0Aautonomous%20vehicle%20on%20the%20basis%20of%20the%20navigation%20path.%20Then%2C%20it%20produces%20the%0Acomplete%20planned%20trajectory%20on%20the%20basis%20of%20this%20target%20point.%20On%20the%20one%20hand%2C%0Agenerating%20the%20target%20point%20for%20the%20autonomous%20vehicle%20from%20the%20navigation%20path%0Aenables%20the%20vehicle%20to%20learn%20a%20clear%20driving%20intent.%20On%20the%20other%20hand%2C%0Agenerating%20the%20trajectory%20on%20the%20basis%20of%20the%20target%20point%20allows%20for%20a%0Aflexible%20planned%20trajectory%20that%20can%20adapt%20to%20complex%20environmental%20changes%2C%0Athereby%20enhancing%20the%20safety%20of%20the%20planning%20process.%20Our%20method%20achieved%0Aexcellent%20planning%20performance%20on%20the%20widely%20used%20nuScenes%20dataset%20and%20its%0Aeffectiveness%20was%20validated%20through%20ablation%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08349v2&entry.124074799=Read"},
{"title": "Zero-shot 3D Segmentation of Abdominal Organs in CT Scans Using Segment\n  Anything Model 2: Adapting Video Tracking Capabilities for 3D Medical Imaging", "author": "Yosuke Yamagishi and Shouhei Hanaoka and Tomohiro Kikuchi and Takahiro Nakao and Yuta Nakamura and Yukihiro Nomura and Soichiro Miki and Takeharu Yoshikawa and Osamu Abe", "abstract": "  Purpose: This study aimed to evaluate the zero-shot performance of Segment\nAnything Model 2 (SAM 2) in 3D segmentation of abdominal organs in CT scans,\nleveraging its video tracking capabilities for volumetric medical imaging.\nMaterials and Methods: Using a subset of the TotalSegmentator CT dataset\n(n=123) from 8 different institutions, we assessed SAM 2's ability to segment 8\nabdominal organs. Segmentation was initiated from three different Z-coordinate\nlevels (caudal, mid, and cranial levels) of each organ. Performance was\nmeasured using the Dice similarity coefficient (DSC). We also analyzed organ\nvolumes to contextualize the results. Results: As a zero-shot approach, larger\norgans with clear boundaries demonstrated high segmentation performance, with\nmean(median) DSCs as follows: liver 0.821(0.898), left kidney 0.870(0.921),\nright kidney 0.862(0.935), and spleen 0.891(0.932). Smaller or less defined\nstructures showed lower performance: gallbladder 0.531(0.590), pancreas\n0.361(0.359), and adrenal glands 0.203-0.308(0.109-0.231). Significant\ndifferences in DSC were observed depending on the starting initial slice of\nsegmentation for different organs. A moderate positive correlation was observed\nbetween volume size and DSCs (Spearman's rs = 0.731, P <.001 at caudal-level).\nDSCs exhibited high variability within organs, ranging from near 0 to almost\n1.0, indicating substantial inconsistency in segmentation performance between\nscans. Conclusion: SAM 2 demonstrated promising zero-shot performance in\nsegmenting certain abdominal organs in CT scans, particularly larger organs\nwith clear boundaries. The model's ability to segment previously unseen targets\nwithout additional training highlights its potential for cross-domain\ngeneralization in medical imaging. However, improvements are needed for smaller\nand less defined structures.\n", "link": "http://arxiv.org/abs/2408.06170v1", "date": "2024-08-12", "relevancy": 2.7367, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5579}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.542}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-shot%203D%20Segmentation%20of%20Abdominal%20Organs%20in%20CT%20Scans%20Using%20Segment%0A%20%20Anything%20Model%202%3A%20Adapting%20Video%20Tracking%20Capabilities%20for%203D%20Medical%20Imaging&body=Title%3A%20Zero-shot%203D%20Segmentation%20of%20Abdominal%20Organs%20in%20CT%20Scans%20Using%20Segment%0A%20%20Anything%20Model%202%3A%20Adapting%20Video%20Tracking%20Capabilities%20for%203D%20Medical%20Imaging%0AAuthor%3A%20Yosuke%20Yamagishi%20and%20Shouhei%20Hanaoka%20and%20Tomohiro%20Kikuchi%20and%20Takahiro%20Nakao%20and%20Yuta%20Nakamura%20and%20Yukihiro%20Nomura%20and%20Soichiro%20Miki%20and%20Takeharu%20Yoshikawa%20and%20Osamu%20Abe%0AAbstract%3A%20%20%20Purpose%3A%20This%20study%20aimed%20to%20evaluate%20the%20zero-shot%20performance%20of%20Segment%0AAnything%20Model%202%20%28SAM%202%29%20in%203D%20segmentation%20of%20abdominal%20organs%20in%20CT%20scans%2C%0Aleveraging%20its%20video%20tracking%20capabilities%20for%20volumetric%20medical%20imaging.%0AMaterials%20and%20Methods%3A%20Using%20a%20subset%20of%20the%20TotalSegmentator%20CT%20dataset%0A%28n%3D123%29%20from%208%20different%20institutions%2C%20we%20assessed%20SAM%202%27s%20ability%20to%20segment%208%0Aabdominal%20organs.%20Segmentation%20was%20initiated%20from%20three%20different%20Z-coordinate%0Alevels%20%28caudal%2C%20mid%2C%20and%20cranial%20levels%29%20of%20each%20organ.%20Performance%20was%0Ameasured%20using%20the%20Dice%20similarity%20coefficient%20%28DSC%29.%20We%20also%20analyzed%20organ%0Avolumes%20to%20contextualize%20the%20results.%20Results%3A%20As%20a%20zero-shot%20approach%2C%20larger%0Aorgans%20with%20clear%20boundaries%20demonstrated%20high%20segmentation%20performance%2C%20with%0Amean%28median%29%20DSCs%20as%20follows%3A%20liver%200.821%280.898%29%2C%20left%20kidney%200.870%280.921%29%2C%0Aright%20kidney%200.862%280.935%29%2C%20and%20spleen%200.891%280.932%29.%20Smaller%20or%20less%20defined%0Astructures%20showed%20lower%20performance%3A%20gallbladder%200.531%280.590%29%2C%20pancreas%0A0.361%280.359%29%2C%20and%20adrenal%20glands%200.203-0.308%280.109-0.231%29.%20Significant%0Adifferences%20in%20DSC%20were%20observed%20depending%20on%20the%20starting%20initial%20slice%20of%0Asegmentation%20for%20different%20organs.%20A%20moderate%20positive%20correlation%20was%20observed%0Abetween%20volume%20size%20and%20DSCs%20%28Spearman%27s%20rs%20%3D%200.731%2C%20P%20%3C.001%20at%20caudal-level%29.%0ADSCs%20exhibited%20high%20variability%20within%20organs%2C%20ranging%20from%20near%200%20to%20almost%0A1.0%2C%20indicating%20substantial%20inconsistency%20in%20segmentation%20performance%20between%0Ascans.%20Conclusion%3A%20SAM%202%20demonstrated%20promising%20zero-shot%20performance%20in%0Asegmenting%20certain%20abdominal%20organs%20in%20CT%20scans%2C%20particularly%20larger%20organs%0Awith%20clear%20boundaries.%20The%20model%27s%20ability%20to%20segment%20previously%20unseen%20targets%0Awithout%20additional%20training%20highlights%20its%20potential%20for%20cross-domain%0Ageneralization%20in%20medical%20imaging.%20However%2C%20improvements%20are%20needed%20for%20smaller%0Aand%20less%20defined%20structures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06170v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-shot%25203D%2520Segmentation%2520of%2520Abdominal%2520Organs%2520in%2520CT%2520Scans%2520Using%2520Segment%250A%2520%2520Anything%2520Model%25202%253A%2520Adapting%2520Video%2520Tracking%2520Capabilities%2520for%25203D%2520Medical%2520Imaging%26entry.906535625%3DYosuke%2520Yamagishi%2520and%2520Shouhei%2520Hanaoka%2520and%2520Tomohiro%2520Kikuchi%2520and%2520Takahiro%2520Nakao%2520and%2520Yuta%2520Nakamura%2520and%2520Yukihiro%2520Nomura%2520and%2520Soichiro%2520Miki%2520and%2520Takeharu%2520Yoshikawa%2520and%2520Osamu%2520Abe%26entry.1292438233%3D%2520%2520Purpose%253A%2520This%2520study%2520aimed%2520to%2520evaluate%2520the%2520zero-shot%2520performance%2520of%2520Segment%250AAnything%2520Model%25202%2520%2528SAM%25202%2529%2520in%25203D%2520segmentation%2520of%2520abdominal%2520organs%2520in%2520CT%2520scans%252C%250Aleveraging%2520its%2520video%2520tracking%2520capabilities%2520for%2520volumetric%2520medical%2520imaging.%250AMaterials%2520and%2520Methods%253A%2520Using%2520a%2520subset%2520of%2520the%2520TotalSegmentator%2520CT%2520dataset%250A%2528n%253D123%2529%2520from%25208%2520different%2520institutions%252C%2520we%2520assessed%2520SAM%25202%2527s%2520ability%2520to%2520segment%25208%250Aabdominal%2520organs.%2520Segmentation%2520was%2520initiated%2520from%2520three%2520different%2520Z-coordinate%250Alevels%2520%2528caudal%252C%2520mid%252C%2520and%2520cranial%2520levels%2529%2520of%2520each%2520organ.%2520Performance%2520was%250Ameasured%2520using%2520the%2520Dice%2520similarity%2520coefficient%2520%2528DSC%2529.%2520We%2520also%2520analyzed%2520organ%250Avolumes%2520to%2520contextualize%2520the%2520results.%2520Results%253A%2520As%2520a%2520zero-shot%2520approach%252C%2520larger%250Aorgans%2520with%2520clear%2520boundaries%2520demonstrated%2520high%2520segmentation%2520performance%252C%2520with%250Amean%2528median%2529%2520DSCs%2520as%2520follows%253A%2520liver%25200.821%25280.898%2529%252C%2520left%2520kidney%25200.870%25280.921%2529%252C%250Aright%2520kidney%25200.862%25280.935%2529%252C%2520and%2520spleen%25200.891%25280.932%2529.%2520Smaller%2520or%2520less%2520defined%250Astructures%2520showed%2520lower%2520performance%253A%2520gallbladder%25200.531%25280.590%2529%252C%2520pancreas%250A0.361%25280.359%2529%252C%2520and%2520adrenal%2520glands%25200.203-0.308%25280.109-0.231%2529.%2520Significant%250Adifferences%2520in%2520DSC%2520were%2520observed%2520depending%2520on%2520the%2520starting%2520initial%2520slice%2520of%250Asegmentation%2520for%2520different%2520organs.%2520A%2520moderate%2520positive%2520correlation%2520was%2520observed%250Abetween%2520volume%2520size%2520and%2520DSCs%2520%2528Spearman%2527s%2520rs%2520%253D%25200.731%252C%2520P%2520%253C.001%2520at%2520caudal-level%2529.%250ADSCs%2520exhibited%2520high%2520variability%2520within%2520organs%252C%2520ranging%2520from%2520near%25200%2520to%2520almost%250A1.0%252C%2520indicating%2520substantial%2520inconsistency%2520in%2520segmentation%2520performance%2520between%250Ascans.%2520Conclusion%253A%2520SAM%25202%2520demonstrated%2520promising%2520zero-shot%2520performance%2520in%250Asegmenting%2520certain%2520abdominal%2520organs%2520in%2520CT%2520scans%252C%2520particularly%2520larger%2520organs%250Awith%2520clear%2520boundaries.%2520The%2520model%2527s%2520ability%2520to%2520segment%2520previously%2520unseen%2520targets%250Awithout%2520additional%2520training%2520highlights%2520its%2520potential%2520for%2520cross-domain%250Ageneralization%2520in%2520medical%2520imaging.%2520However%252C%2520improvements%2520are%2520needed%2520for%2520smaller%250Aand%2520less%2520defined%2520structures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06170v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-shot%203D%20Segmentation%20of%20Abdominal%20Organs%20in%20CT%20Scans%20Using%20Segment%0A%20%20Anything%20Model%202%3A%20Adapting%20Video%20Tracking%20Capabilities%20for%203D%20Medical%20Imaging&entry.906535625=Yosuke%20Yamagishi%20and%20Shouhei%20Hanaoka%20and%20Tomohiro%20Kikuchi%20and%20Takahiro%20Nakao%20and%20Yuta%20Nakamura%20and%20Yukihiro%20Nomura%20and%20Soichiro%20Miki%20and%20Takeharu%20Yoshikawa%20and%20Osamu%20Abe&entry.1292438233=%20%20Purpose%3A%20This%20study%20aimed%20to%20evaluate%20the%20zero-shot%20performance%20of%20Segment%0AAnything%20Model%202%20%28SAM%202%29%20in%203D%20segmentation%20of%20abdominal%20organs%20in%20CT%20scans%2C%0Aleveraging%20its%20video%20tracking%20capabilities%20for%20volumetric%20medical%20imaging.%0AMaterials%20and%20Methods%3A%20Using%20a%20subset%20of%20the%20TotalSegmentator%20CT%20dataset%0A%28n%3D123%29%20from%208%20different%20institutions%2C%20we%20assessed%20SAM%202%27s%20ability%20to%20segment%208%0Aabdominal%20organs.%20Segmentation%20was%20initiated%20from%20three%20different%20Z-coordinate%0Alevels%20%28caudal%2C%20mid%2C%20and%20cranial%20levels%29%20of%20each%20organ.%20Performance%20was%0Ameasured%20using%20the%20Dice%20similarity%20coefficient%20%28DSC%29.%20We%20also%20analyzed%20organ%0Avolumes%20to%20contextualize%20the%20results.%20Results%3A%20As%20a%20zero-shot%20approach%2C%20larger%0Aorgans%20with%20clear%20boundaries%20demonstrated%20high%20segmentation%20performance%2C%20with%0Amean%28median%29%20DSCs%20as%20follows%3A%20liver%200.821%280.898%29%2C%20left%20kidney%200.870%280.921%29%2C%0Aright%20kidney%200.862%280.935%29%2C%20and%20spleen%200.891%280.932%29.%20Smaller%20or%20less%20defined%0Astructures%20showed%20lower%20performance%3A%20gallbladder%200.531%280.590%29%2C%20pancreas%0A0.361%280.359%29%2C%20and%20adrenal%20glands%200.203-0.308%280.109-0.231%29.%20Significant%0Adifferences%20in%20DSC%20were%20observed%20depending%20on%20the%20starting%20initial%20slice%20of%0Asegmentation%20for%20different%20organs.%20A%20moderate%20positive%20correlation%20was%20observed%0Abetween%20volume%20size%20and%20DSCs%20%28Spearman%27s%20rs%20%3D%200.731%2C%20P%20%3C.001%20at%20caudal-level%29.%0ADSCs%20exhibited%20high%20variability%20within%20organs%2C%20ranging%20from%20near%200%20to%20almost%0A1.0%2C%20indicating%20substantial%20inconsistency%20in%20segmentation%20performance%20between%0Ascans.%20Conclusion%3A%20SAM%202%20demonstrated%20promising%20zero-shot%20performance%20in%0Asegmenting%20certain%20abdominal%20organs%20in%20CT%20scans%2C%20particularly%20larger%20organs%0Awith%20clear%20boundaries.%20The%20model%27s%20ability%20to%20segment%20previously%20unseen%20targets%0Awithout%20additional%20training%20highlights%20its%20potential%20for%20cross-domain%0Ageneralization%20in%20medical%20imaging.%20However%2C%20improvements%20are%20needed%20for%20smaller%0Aand%20less%20defined%20structures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06170v1&entry.124074799=Read"},
{"title": "Integrating Present and Past in Unsupervised Continual Learning", "author": "Yipeng Zhang and Laurent Charlin and Richard Zemel and Mengye Ren", "abstract": "  We formulate a unifying framework for unsupervised continual learning (UCL),\nwhich disentangles learning objectives that are specific to the present and the\npast data, encompassing stability, plasticity, and cross-task consolidation.\nThe framework reveals that many existing UCL approaches overlook cross-task\nconsolidation and try to balance plasticity and stability in a shared embedding\nspace. This results in worse performance due to a lack of within-task data\ndiversity and reduced effectiveness in learning the current task. Our method,\nOsiris, which explicitly optimizes all three objectives on separate embedding\nspaces, achieves state-of-the-art performance on all benchmarks, including two\nnovel benchmarks proposed in this paper featuring semantically structured task\nsequences. Compared to standard benchmarks, these two structured benchmarks\nmore closely resemble visual signals received by humans and animals when\nnavigating real-world environments. Finally, we show some preliminary evidence\nthat continual models can benefit from such realistic learning scenarios.\n", "link": "http://arxiv.org/abs/2404.19132v2", "date": "2024-08-12", "relevancy": 2.6968, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5428}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5394}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrating%20Present%20and%20Past%20in%20Unsupervised%20Continual%20Learning&body=Title%3A%20Integrating%20Present%20and%20Past%20in%20Unsupervised%20Continual%20Learning%0AAuthor%3A%20Yipeng%20Zhang%20and%20Laurent%20Charlin%20and%20Richard%20Zemel%20and%20Mengye%20Ren%0AAbstract%3A%20%20%20We%20formulate%20a%20unifying%20framework%20for%20unsupervised%20continual%20learning%20%28UCL%29%2C%0Awhich%20disentangles%20learning%20objectives%20that%20are%20specific%20to%20the%20present%20and%20the%0Apast%20data%2C%20encompassing%20stability%2C%20plasticity%2C%20and%20cross-task%20consolidation.%0AThe%20framework%20reveals%20that%20many%20existing%20UCL%20approaches%20overlook%20cross-task%0Aconsolidation%20and%20try%20to%20balance%20plasticity%20and%20stability%20in%20a%20shared%20embedding%0Aspace.%20This%20results%20in%20worse%20performance%20due%20to%20a%20lack%20of%20within-task%20data%0Adiversity%20and%20reduced%20effectiveness%20in%20learning%20the%20current%20task.%20Our%20method%2C%0AOsiris%2C%20which%20explicitly%20optimizes%20all%20three%20objectives%20on%20separate%20embedding%0Aspaces%2C%20achieves%20state-of-the-art%20performance%20on%20all%20benchmarks%2C%20including%20two%0Anovel%20benchmarks%20proposed%20in%20this%20paper%20featuring%20semantically%20structured%20task%0Asequences.%20Compared%20to%20standard%20benchmarks%2C%20these%20two%20structured%20benchmarks%0Amore%20closely%20resemble%20visual%20signals%20received%20by%20humans%20and%20animals%20when%0Anavigating%20real-world%20environments.%20Finally%2C%20we%20show%20some%20preliminary%20evidence%0Athat%20continual%20models%20can%20benefit%20from%20such%20realistic%20learning%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19132v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrating%2520Present%2520and%2520Past%2520in%2520Unsupervised%2520Continual%2520Learning%26entry.906535625%3DYipeng%2520Zhang%2520and%2520Laurent%2520Charlin%2520and%2520Richard%2520Zemel%2520and%2520Mengye%2520Ren%26entry.1292438233%3D%2520%2520We%2520formulate%2520a%2520unifying%2520framework%2520for%2520unsupervised%2520continual%2520learning%2520%2528UCL%2529%252C%250Awhich%2520disentangles%2520learning%2520objectives%2520that%2520are%2520specific%2520to%2520the%2520present%2520and%2520the%250Apast%2520data%252C%2520encompassing%2520stability%252C%2520plasticity%252C%2520and%2520cross-task%2520consolidation.%250AThe%2520framework%2520reveals%2520that%2520many%2520existing%2520UCL%2520approaches%2520overlook%2520cross-task%250Aconsolidation%2520and%2520try%2520to%2520balance%2520plasticity%2520and%2520stability%2520in%2520a%2520shared%2520embedding%250Aspace.%2520This%2520results%2520in%2520worse%2520performance%2520due%2520to%2520a%2520lack%2520of%2520within-task%2520data%250Adiversity%2520and%2520reduced%2520effectiveness%2520in%2520learning%2520the%2520current%2520task.%2520Our%2520method%252C%250AOsiris%252C%2520which%2520explicitly%2520optimizes%2520all%2520three%2520objectives%2520on%2520separate%2520embedding%250Aspaces%252C%2520achieves%2520state-of-the-art%2520performance%2520on%2520all%2520benchmarks%252C%2520including%2520two%250Anovel%2520benchmarks%2520proposed%2520in%2520this%2520paper%2520featuring%2520semantically%2520structured%2520task%250Asequences.%2520Compared%2520to%2520standard%2520benchmarks%252C%2520these%2520two%2520structured%2520benchmarks%250Amore%2520closely%2520resemble%2520visual%2520signals%2520received%2520by%2520humans%2520and%2520animals%2520when%250Anavigating%2520real-world%2520environments.%2520Finally%252C%2520we%2520show%2520some%2520preliminary%2520evidence%250Athat%2520continual%2520models%2520can%2520benefit%2520from%2520such%2520realistic%2520learning%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.19132v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20Present%20and%20Past%20in%20Unsupervised%20Continual%20Learning&entry.906535625=Yipeng%20Zhang%20and%20Laurent%20Charlin%20and%20Richard%20Zemel%20and%20Mengye%20Ren&entry.1292438233=%20%20We%20formulate%20a%20unifying%20framework%20for%20unsupervised%20continual%20learning%20%28UCL%29%2C%0Awhich%20disentangles%20learning%20objectives%20that%20are%20specific%20to%20the%20present%20and%20the%0Apast%20data%2C%20encompassing%20stability%2C%20plasticity%2C%20and%20cross-task%20consolidation.%0AThe%20framework%20reveals%20that%20many%20existing%20UCL%20approaches%20overlook%20cross-task%0Aconsolidation%20and%20try%20to%20balance%20plasticity%20and%20stability%20in%20a%20shared%20embedding%0Aspace.%20This%20results%20in%20worse%20performance%20due%20to%20a%20lack%20of%20within-task%20data%0Adiversity%20and%20reduced%20effectiveness%20in%20learning%20the%20current%20task.%20Our%20method%2C%0AOsiris%2C%20which%20explicitly%20optimizes%20all%20three%20objectives%20on%20separate%20embedding%0Aspaces%2C%20achieves%20state-of-the-art%20performance%20on%20all%20benchmarks%2C%20including%20two%0Anovel%20benchmarks%20proposed%20in%20this%20paper%20featuring%20semantically%20structured%20task%0Asequences.%20Compared%20to%20standard%20benchmarks%2C%20these%20two%20structured%20benchmarks%0Amore%20closely%20resemble%20visual%20signals%20received%20by%20humans%20and%20animals%20when%0Anavigating%20real-world%20environments.%20Finally%2C%20we%20show%20some%20preliminary%20evidence%0Athat%20continual%20models%20can%20benefit%20from%20such%20realistic%20learning%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19132v2&entry.124074799=Read"},
{"title": "A Text-guided Protein Design Framework", "author": "Shengchao Liu and Yanjing Li and Zhuoxinran Li and Anthony Gitter and Yutao Zhu and Jiarui Lu and Zhao Xu and Weili Nie and Arvind Ramanathan and Chaowei Xiao and Jian Tang and Hongyu Guo and Anima Anandkumar", "abstract": "  Current AI-assisted protein design mainly utilizes protein sequential and\nstructural information. Meanwhile, there exists tremendous knowledge curated by\nhumans in the text format describing proteins' high-level functionalities. Yet,\nwhether the incorporation of such text data can help protein design tasks has\nnot been explored. To bridge this gap, we propose ProteinDT, a multi-modal\nframework that leverages textual descriptions for protein design. ProteinDT\nconsists of three subsequent steps: ProteinCLAP which aligns the representation\nof two modalities, a facilitator that generates the protein representation from\nthe text modality, and a decoder that creates the protein sequences from the\nrepresentation. To train ProteinDT, we construct a large dataset,\nSwissProtCLAP, with 441K text and protein pairs. We quantitatively verify the\neffectiveness of ProteinDT on three challenging tasks: (1) over 90\\% accuracy\nfor text-guided protein generation; (2) best hit ratio on 12 zero-shot\ntext-guided protein editing tasks; (3) superior performance on four out of six\nprotein property prediction benchmarks.\n", "link": "http://arxiv.org/abs/2302.04611v3", "date": "2024-08-12", "relevancy": 2.6825, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6144}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5149}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4801}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Text-guided%20Protein%20Design%20Framework&body=Title%3A%20A%20Text-guided%20Protein%20Design%20Framework%0AAuthor%3A%20Shengchao%20Liu%20and%20Yanjing%20Li%20and%20Zhuoxinran%20Li%20and%20Anthony%20Gitter%20and%20Yutao%20Zhu%20and%20Jiarui%20Lu%20and%20Zhao%20Xu%20and%20Weili%20Nie%20and%20Arvind%20Ramanathan%20and%20Chaowei%20Xiao%20and%20Jian%20Tang%20and%20Hongyu%20Guo%20and%20Anima%20Anandkumar%0AAbstract%3A%20%20%20Current%20AI-assisted%20protein%20design%20mainly%20utilizes%20protein%20sequential%20and%0Astructural%20information.%20Meanwhile%2C%20there%20exists%20tremendous%20knowledge%20curated%20by%0Ahumans%20in%20the%20text%20format%20describing%20proteins%27%20high-level%20functionalities.%20Yet%2C%0Awhether%20the%20incorporation%20of%20such%20text%20data%20can%20help%20protein%20design%20tasks%20has%0Anot%20been%20explored.%20To%20bridge%20this%20gap%2C%20we%20propose%20ProteinDT%2C%20a%20multi-modal%0Aframework%20that%20leverages%20textual%20descriptions%20for%20protein%20design.%20ProteinDT%0Aconsists%20of%20three%20subsequent%20steps%3A%20ProteinCLAP%20which%20aligns%20the%20representation%0Aof%20two%20modalities%2C%20a%20facilitator%20that%20generates%20the%20protein%20representation%20from%0Athe%20text%20modality%2C%20and%20a%20decoder%20that%20creates%20the%20protein%20sequences%20from%20the%0Arepresentation.%20To%20train%20ProteinDT%2C%20we%20construct%20a%20large%20dataset%2C%0ASwissProtCLAP%2C%20with%20441K%20text%20and%20protein%20pairs.%20We%20quantitatively%20verify%20the%0Aeffectiveness%20of%20ProteinDT%20on%20three%20challenging%20tasks%3A%20%281%29%20over%2090%5C%25%20accuracy%0Afor%20text-guided%20protein%20generation%3B%20%282%29%20best%20hit%20ratio%20on%2012%20zero-shot%0Atext-guided%20protein%20editing%20tasks%3B%20%283%29%20superior%20performance%20on%20four%20out%20of%20six%0Aprotein%20property%20prediction%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.04611v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Text-guided%2520Protein%2520Design%2520Framework%26entry.906535625%3DShengchao%2520Liu%2520and%2520Yanjing%2520Li%2520and%2520Zhuoxinran%2520Li%2520and%2520Anthony%2520Gitter%2520and%2520Yutao%2520Zhu%2520and%2520Jiarui%2520Lu%2520and%2520Zhao%2520Xu%2520and%2520Weili%2520Nie%2520and%2520Arvind%2520Ramanathan%2520and%2520Chaowei%2520Xiao%2520and%2520Jian%2520Tang%2520and%2520Hongyu%2520Guo%2520and%2520Anima%2520Anandkumar%26entry.1292438233%3D%2520%2520Current%2520AI-assisted%2520protein%2520design%2520mainly%2520utilizes%2520protein%2520sequential%2520and%250Astructural%2520information.%2520Meanwhile%252C%2520there%2520exists%2520tremendous%2520knowledge%2520curated%2520by%250Ahumans%2520in%2520the%2520text%2520format%2520describing%2520proteins%2527%2520high-level%2520functionalities.%2520Yet%252C%250Awhether%2520the%2520incorporation%2520of%2520such%2520text%2520data%2520can%2520help%2520protein%2520design%2520tasks%2520has%250Anot%2520been%2520explored.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520ProteinDT%252C%2520a%2520multi-modal%250Aframework%2520that%2520leverages%2520textual%2520descriptions%2520for%2520protein%2520design.%2520ProteinDT%250Aconsists%2520of%2520three%2520subsequent%2520steps%253A%2520ProteinCLAP%2520which%2520aligns%2520the%2520representation%250Aof%2520two%2520modalities%252C%2520a%2520facilitator%2520that%2520generates%2520the%2520protein%2520representation%2520from%250Athe%2520text%2520modality%252C%2520and%2520a%2520decoder%2520that%2520creates%2520the%2520protein%2520sequences%2520from%2520the%250Arepresentation.%2520To%2520train%2520ProteinDT%252C%2520we%2520construct%2520a%2520large%2520dataset%252C%250ASwissProtCLAP%252C%2520with%2520441K%2520text%2520and%2520protein%2520pairs.%2520We%2520quantitatively%2520verify%2520the%250Aeffectiveness%2520of%2520ProteinDT%2520on%2520three%2520challenging%2520tasks%253A%2520%25281%2529%2520over%252090%255C%2525%2520accuracy%250Afor%2520text-guided%2520protein%2520generation%253B%2520%25282%2529%2520best%2520hit%2520ratio%2520on%252012%2520zero-shot%250Atext-guided%2520protein%2520editing%2520tasks%253B%2520%25283%2529%2520superior%2520performance%2520on%2520four%2520out%2520of%2520six%250Aprotein%2520property%2520prediction%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.04611v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Text-guided%20Protein%20Design%20Framework&entry.906535625=Shengchao%20Liu%20and%20Yanjing%20Li%20and%20Zhuoxinran%20Li%20and%20Anthony%20Gitter%20and%20Yutao%20Zhu%20and%20Jiarui%20Lu%20and%20Zhao%20Xu%20and%20Weili%20Nie%20and%20Arvind%20Ramanathan%20and%20Chaowei%20Xiao%20and%20Jian%20Tang%20and%20Hongyu%20Guo%20and%20Anima%20Anandkumar&entry.1292438233=%20%20Current%20AI-assisted%20protein%20design%20mainly%20utilizes%20protein%20sequential%20and%0Astructural%20information.%20Meanwhile%2C%20there%20exists%20tremendous%20knowledge%20curated%20by%0Ahumans%20in%20the%20text%20format%20describing%20proteins%27%20high-level%20functionalities.%20Yet%2C%0Awhether%20the%20incorporation%20of%20such%20text%20data%20can%20help%20protein%20design%20tasks%20has%0Anot%20been%20explored.%20To%20bridge%20this%20gap%2C%20we%20propose%20ProteinDT%2C%20a%20multi-modal%0Aframework%20that%20leverages%20textual%20descriptions%20for%20protein%20design.%20ProteinDT%0Aconsists%20of%20three%20subsequent%20steps%3A%20ProteinCLAP%20which%20aligns%20the%20representation%0Aof%20two%20modalities%2C%20a%20facilitator%20that%20generates%20the%20protein%20representation%20from%0Athe%20text%20modality%2C%20and%20a%20decoder%20that%20creates%20the%20protein%20sequences%20from%20the%0Arepresentation.%20To%20train%20ProteinDT%2C%20we%20construct%20a%20large%20dataset%2C%0ASwissProtCLAP%2C%20with%20441K%20text%20and%20protein%20pairs.%20We%20quantitatively%20verify%20the%0Aeffectiveness%20of%20ProteinDT%20on%20three%20challenging%20tasks%3A%20%281%29%20over%2090%5C%25%20accuracy%0Afor%20text-guided%20protein%20generation%3B%20%282%29%20best%20hit%20ratio%20on%2012%20zero-shot%0Atext-guided%20protein%20editing%20tasks%3B%20%283%29%20superior%20performance%20on%20four%20out%20of%20six%0Aprotein%20property%20prediction%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.04611v3&entry.124074799=Read"},
{"title": "RISurConv: Rotation Invariant Surface Attention-Augmented Convolutions\n  for 3D Point Cloud Classification and Segmentation", "author": "Zhiyuan Zhang and Licheng Yang and Zhiyu Xiang", "abstract": "  Despite the progress on 3D point cloud deep learning, most prior works focus\non learning features that are invariant to translation and point permutation,\nand very limited efforts have been devoted for rotation invariant property.\nSeveral recent studies achieve rotation invariance at the cost of lower\naccuracies. In this work, we close this gap by proposing a novel yet effective\nrotation invariant architecture for 3D point cloud classification and\nsegmentation. Instead of traditional pointwise operations, we construct local\ntriangle surfaces to capture more detailed surface structure, based on which we\ncan extract highly expressive rotation invariant surface properties which are\nthen integrated into an attention-augmented convolution operator named\nRISurConv to generate refined attention features via self-attention layers.\nBased on RISurConv we build an effective neural network for 3D point cloud\nanalysis that is invariant to arbitrary rotations while maintaining high\naccuracy. We verify the performance on various benchmarks with supreme results\nobtained surpassing the previous state-of-the-art by a large margin. We achieve\nan overall accuracy of 96.0% (+4.7%) on ModelNet40, 93.1% (+12.8%) on\nScanObjectNN, and class accuracies of 91.5% (+3.6%), 82.7% (+5.1%), and 78.5%\n(+9.2%) on the three categories of the FG3D dataset for the fine-grained\nclassification task. Additionally, we achieve 81.5% (+1.0%) mIoU on ShapeNet\nfor the segmentation task. Code is available here:\nhttps://github.com/cszyzhang/RISurConv\n", "link": "http://arxiv.org/abs/2408.06110v1", "date": "2024-08-12", "relevancy": 2.6417, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5384}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5271}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5196}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RISurConv%3A%20Rotation%20Invariant%20Surface%20Attention-Augmented%20Convolutions%0A%20%20for%203D%20Point%20Cloud%20Classification%20and%20Segmentation&body=Title%3A%20RISurConv%3A%20Rotation%20Invariant%20Surface%20Attention-Augmented%20Convolutions%0A%20%20for%203D%20Point%20Cloud%20Classification%20and%20Segmentation%0AAuthor%3A%20Zhiyuan%20Zhang%20and%20Licheng%20Yang%20and%20Zhiyu%20Xiang%0AAbstract%3A%20%20%20Despite%20the%20progress%20on%203D%20point%20cloud%20deep%20learning%2C%20most%20prior%20works%20focus%0Aon%20learning%20features%20that%20are%20invariant%20to%20translation%20and%20point%20permutation%2C%0Aand%20very%20limited%20efforts%20have%20been%20devoted%20for%20rotation%20invariant%20property.%0ASeveral%20recent%20studies%20achieve%20rotation%20invariance%20at%20the%20cost%20of%20lower%0Aaccuracies.%20In%20this%20work%2C%20we%20close%20this%20gap%20by%20proposing%20a%20novel%20yet%20effective%0Arotation%20invariant%20architecture%20for%203D%20point%20cloud%20classification%20and%0Asegmentation.%20Instead%20of%20traditional%20pointwise%20operations%2C%20we%20construct%20local%0Atriangle%20surfaces%20to%20capture%20more%20detailed%20surface%20structure%2C%20based%20on%20which%20we%0Acan%20extract%20highly%20expressive%20rotation%20invariant%20surface%20properties%20which%20are%0Athen%20integrated%20into%20an%20attention-augmented%20convolution%20operator%20named%0ARISurConv%20to%20generate%20refined%20attention%20features%20via%20self-attention%20layers.%0ABased%20on%20RISurConv%20we%20build%20an%20effective%20neural%20network%20for%203D%20point%20cloud%0Aanalysis%20that%20is%20invariant%20to%20arbitrary%20rotations%20while%20maintaining%20high%0Aaccuracy.%20We%20verify%20the%20performance%20on%20various%20benchmarks%20with%20supreme%20results%0Aobtained%20surpassing%20the%20previous%20state-of-the-art%20by%20a%20large%20margin.%20We%20achieve%0Aan%20overall%20accuracy%20of%2096.0%25%20%28%2B4.7%25%29%20on%20ModelNet40%2C%2093.1%25%20%28%2B12.8%25%29%20on%0AScanObjectNN%2C%20and%20class%20accuracies%20of%2091.5%25%20%28%2B3.6%25%29%2C%2082.7%25%20%28%2B5.1%25%29%2C%20and%2078.5%25%0A%28%2B9.2%25%29%20on%20the%20three%20categories%20of%20the%20FG3D%20dataset%20for%20the%20fine-grained%0Aclassification%20task.%20Additionally%2C%20we%20achieve%2081.5%25%20%28%2B1.0%25%29%20mIoU%20on%20ShapeNet%0Afor%20the%20segmentation%20task.%20Code%20is%20available%20here%3A%0Ahttps%3A//github.com/cszyzhang/RISurConv%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06110v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRISurConv%253A%2520Rotation%2520Invariant%2520Surface%2520Attention-Augmented%2520Convolutions%250A%2520%2520for%25203D%2520Point%2520Cloud%2520Classification%2520and%2520Segmentation%26entry.906535625%3DZhiyuan%2520Zhang%2520and%2520Licheng%2520Yang%2520and%2520Zhiyu%2520Xiang%26entry.1292438233%3D%2520%2520Despite%2520the%2520progress%2520on%25203D%2520point%2520cloud%2520deep%2520learning%252C%2520most%2520prior%2520works%2520focus%250Aon%2520learning%2520features%2520that%2520are%2520invariant%2520to%2520translation%2520and%2520point%2520permutation%252C%250Aand%2520very%2520limited%2520efforts%2520have%2520been%2520devoted%2520for%2520rotation%2520invariant%2520property.%250ASeveral%2520recent%2520studies%2520achieve%2520rotation%2520invariance%2520at%2520the%2520cost%2520of%2520lower%250Aaccuracies.%2520In%2520this%2520work%252C%2520we%2520close%2520this%2520gap%2520by%2520proposing%2520a%2520novel%2520yet%2520effective%250Arotation%2520invariant%2520architecture%2520for%25203D%2520point%2520cloud%2520classification%2520and%250Asegmentation.%2520Instead%2520of%2520traditional%2520pointwise%2520operations%252C%2520we%2520construct%2520local%250Atriangle%2520surfaces%2520to%2520capture%2520more%2520detailed%2520surface%2520structure%252C%2520based%2520on%2520which%2520we%250Acan%2520extract%2520highly%2520expressive%2520rotation%2520invariant%2520surface%2520properties%2520which%2520are%250Athen%2520integrated%2520into%2520an%2520attention-augmented%2520convolution%2520operator%2520named%250ARISurConv%2520to%2520generate%2520refined%2520attention%2520features%2520via%2520self-attention%2520layers.%250ABased%2520on%2520RISurConv%2520we%2520build%2520an%2520effective%2520neural%2520network%2520for%25203D%2520point%2520cloud%250Aanalysis%2520that%2520is%2520invariant%2520to%2520arbitrary%2520rotations%2520while%2520maintaining%2520high%250Aaccuracy.%2520We%2520verify%2520the%2520performance%2520on%2520various%2520benchmarks%2520with%2520supreme%2520results%250Aobtained%2520surpassing%2520the%2520previous%2520state-of-the-art%2520by%2520a%2520large%2520margin.%2520We%2520achieve%250Aan%2520overall%2520accuracy%2520of%252096.0%2525%2520%2528%252B4.7%2525%2529%2520on%2520ModelNet40%252C%252093.1%2525%2520%2528%252B12.8%2525%2529%2520on%250AScanObjectNN%252C%2520and%2520class%2520accuracies%2520of%252091.5%2525%2520%2528%252B3.6%2525%2529%252C%252082.7%2525%2520%2528%252B5.1%2525%2529%252C%2520and%252078.5%2525%250A%2528%252B9.2%2525%2529%2520on%2520the%2520three%2520categories%2520of%2520the%2520FG3D%2520dataset%2520for%2520the%2520fine-grained%250Aclassification%2520task.%2520Additionally%252C%2520we%2520achieve%252081.5%2525%2520%2528%252B1.0%2525%2529%2520mIoU%2520on%2520ShapeNet%250Afor%2520the%2520segmentation%2520task.%2520Code%2520is%2520available%2520here%253A%250Ahttps%253A//github.com/cszyzhang/RISurConv%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06110v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RISurConv%3A%20Rotation%20Invariant%20Surface%20Attention-Augmented%20Convolutions%0A%20%20for%203D%20Point%20Cloud%20Classification%20and%20Segmentation&entry.906535625=Zhiyuan%20Zhang%20and%20Licheng%20Yang%20and%20Zhiyu%20Xiang&entry.1292438233=%20%20Despite%20the%20progress%20on%203D%20point%20cloud%20deep%20learning%2C%20most%20prior%20works%20focus%0Aon%20learning%20features%20that%20are%20invariant%20to%20translation%20and%20point%20permutation%2C%0Aand%20very%20limited%20efforts%20have%20been%20devoted%20for%20rotation%20invariant%20property.%0ASeveral%20recent%20studies%20achieve%20rotation%20invariance%20at%20the%20cost%20of%20lower%0Aaccuracies.%20In%20this%20work%2C%20we%20close%20this%20gap%20by%20proposing%20a%20novel%20yet%20effective%0Arotation%20invariant%20architecture%20for%203D%20point%20cloud%20classification%20and%0Asegmentation.%20Instead%20of%20traditional%20pointwise%20operations%2C%20we%20construct%20local%0Atriangle%20surfaces%20to%20capture%20more%20detailed%20surface%20structure%2C%20based%20on%20which%20we%0Acan%20extract%20highly%20expressive%20rotation%20invariant%20surface%20properties%20which%20are%0Athen%20integrated%20into%20an%20attention-augmented%20convolution%20operator%20named%0ARISurConv%20to%20generate%20refined%20attention%20features%20via%20self-attention%20layers.%0ABased%20on%20RISurConv%20we%20build%20an%20effective%20neural%20network%20for%203D%20point%20cloud%0Aanalysis%20that%20is%20invariant%20to%20arbitrary%20rotations%20while%20maintaining%20high%0Aaccuracy.%20We%20verify%20the%20performance%20on%20various%20benchmarks%20with%20supreme%20results%0Aobtained%20surpassing%20the%20previous%20state-of-the-art%20by%20a%20large%20margin.%20We%20achieve%0Aan%20overall%20accuracy%20of%2096.0%25%20%28%2B4.7%25%29%20on%20ModelNet40%2C%2093.1%25%20%28%2B12.8%25%29%20on%0AScanObjectNN%2C%20and%20class%20accuracies%20of%2091.5%25%20%28%2B3.6%25%29%2C%2082.7%25%20%28%2B5.1%25%29%2C%20and%2078.5%25%0A%28%2B9.2%25%29%20on%20the%20three%20categories%20of%20the%20FG3D%20dataset%20for%20the%20fine-grained%0Aclassification%20task.%20Additionally%2C%20we%20achieve%2081.5%25%20%28%2B1.0%25%29%20mIoU%20on%20ShapeNet%0Afor%20the%20segmentation%20task.%20Code%20is%20available%20here%3A%0Ahttps%3A//github.com/cszyzhang/RISurConv%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06110v1&entry.124074799=Read"},
{"title": "Correlation Weighted Prototype-based Self-Supervised One-Shot\n  Segmentation of Medical Images", "author": "Siladittya Manna and Saumik Bhattacharya and Umapada Pal", "abstract": "  Medical image segmentation is one of the domains where sufficient annotated\ndata is not available. This necessitates the application of low-data frameworks\nlike few-shot learning. Contemporary prototype-based frameworks often do not\naccount for the variation in features within the support and query images,\ngiving rise to a large variance in prototype alignment. In this work, we adopt\na prototype-based self-supervised one-way one-shot learning framework using\npseudo-labels generated from superpixels to learn the semantic segmentation\ntask itself. We use a correlation-based probability score to generate a dynamic\nprototype for each query pixel from the bag of prototypes obtained from the\nsupport feature map. This weighting scheme helps to give a higher weightage to\ncontextually related prototypes. We also propose a quadrant masking strategy in\nthe downstream segmentation task by utilizing prior domain information to\ndiscard unwanted false positives. We present extensive experimentations and\nevaluations on abdominal CT and MR datasets to show that the proposed simple\nbut potent framework performs at par with the state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2408.06235v1", "date": "2024-08-12", "relevancy": 2.596, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5481}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5054}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5041}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Correlation%20Weighted%20Prototype-based%20Self-Supervised%20One-Shot%0A%20%20Segmentation%20of%20Medical%20Images&body=Title%3A%20Correlation%20Weighted%20Prototype-based%20Self-Supervised%20One-Shot%0A%20%20Segmentation%20of%20Medical%20Images%0AAuthor%3A%20Siladittya%20Manna%20and%20Saumik%20Bhattacharya%20and%20Umapada%20Pal%0AAbstract%3A%20%20%20Medical%20image%20segmentation%20is%20one%20of%20the%20domains%20where%20sufficient%20annotated%0Adata%20is%20not%20available.%20This%20necessitates%20the%20application%20of%20low-data%20frameworks%0Alike%20few-shot%20learning.%20Contemporary%20prototype-based%20frameworks%20often%20do%20not%0Aaccount%20for%20the%20variation%20in%20features%20within%20the%20support%20and%20query%20images%2C%0Agiving%20rise%20to%20a%20large%20variance%20in%20prototype%20alignment.%20In%20this%20work%2C%20we%20adopt%0Aa%20prototype-based%20self-supervised%20one-way%20one-shot%20learning%20framework%20using%0Apseudo-labels%20generated%20from%20superpixels%20to%20learn%20the%20semantic%20segmentation%0Atask%20itself.%20We%20use%20a%20correlation-based%20probability%20score%20to%20generate%20a%20dynamic%0Aprototype%20for%20each%20query%20pixel%20from%20the%20bag%20of%20prototypes%20obtained%20from%20the%0Asupport%20feature%20map.%20This%20weighting%20scheme%20helps%20to%20give%20a%20higher%20weightage%20to%0Acontextually%20related%20prototypes.%20We%20also%20propose%20a%20quadrant%20masking%20strategy%20in%0Athe%20downstream%20segmentation%20task%20by%20utilizing%20prior%20domain%20information%20to%0Adiscard%20unwanted%20false%20positives.%20We%20present%20extensive%20experimentations%20and%0Aevaluations%20on%20abdominal%20CT%20and%20MR%20datasets%20to%20show%20that%20the%20proposed%20simple%0Abut%20potent%20framework%20performs%20at%20par%20with%20the%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06235v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCorrelation%2520Weighted%2520Prototype-based%2520Self-Supervised%2520One-Shot%250A%2520%2520Segmentation%2520of%2520Medical%2520Images%26entry.906535625%3DSiladittya%2520Manna%2520and%2520Saumik%2520Bhattacharya%2520and%2520Umapada%2520Pal%26entry.1292438233%3D%2520%2520Medical%2520image%2520segmentation%2520is%2520one%2520of%2520the%2520domains%2520where%2520sufficient%2520annotated%250Adata%2520is%2520not%2520available.%2520This%2520necessitates%2520the%2520application%2520of%2520low-data%2520frameworks%250Alike%2520few-shot%2520learning.%2520Contemporary%2520prototype-based%2520frameworks%2520often%2520do%2520not%250Aaccount%2520for%2520the%2520variation%2520in%2520features%2520within%2520the%2520support%2520and%2520query%2520images%252C%250Agiving%2520rise%2520to%2520a%2520large%2520variance%2520in%2520prototype%2520alignment.%2520In%2520this%2520work%252C%2520we%2520adopt%250Aa%2520prototype-based%2520self-supervised%2520one-way%2520one-shot%2520learning%2520framework%2520using%250Apseudo-labels%2520generated%2520from%2520superpixels%2520to%2520learn%2520the%2520semantic%2520segmentation%250Atask%2520itself.%2520We%2520use%2520a%2520correlation-based%2520probability%2520score%2520to%2520generate%2520a%2520dynamic%250Aprototype%2520for%2520each%2520query%2520pixel%2520from%2520the%2520bag%2520of%2520prototypes%2520obtained%2520from%2520the%250Asupport%2520feature%2520map.%2520This%2520weighting%2520scheme%2520helps%2520to%2520give%2520a%2520higher%2520weightage%2520to%250Acontextually%2520related%2520prototypes.%2520We%2520also%2520propose%2520a%2520quadrant%2520masking%2520strategy%2520in%250Athe%2520downstream%2520segmentation%2520task%2520by%2520utilizing%2520prior%2520domain%2520information%2520to%250Adiscard%2520unwanted%2520false%2520positives.%2520We%2520present%2520extensive%2520experimentations%2520and%250Aevaluations%2520on%2520abdominal%2520CT%2520and%2520MR%2520datasets%2520to%2520show%2520that%2520the%2520proposed%2520simple%250Abut%2520potent%2520framework%2520performs%2520at%2520par%2520with%2520the%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06235v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Correlation%20Weighted%20Prototype-based%20Self-Supervised%20One-Shot%0A%20%20Segmentation%20of%20Medical%20Images&entry.906535625=Siladittya%20Manna%20and%20Saumik%20Bhattacharya%20and%20Umapada%20Pal&entry.1292438233=%20%20Medical%20image%20segmentation%20is%20one%20of%20the%20domains%20where%20sufficient%20annotated%0Adata%20is%20not%20available.%20This%20necessitates%20the%20application%20of%20low-data%20frameworks%0Alike%20few-shot%20learning.%20Contemporary%20prototype-based%20frameworks%20often%20do%20not%0Aaccount%20for%20the%20variation%20in%20features%20within%20the%20support%20and%20query%20images%2C%0Agiving%20rise%20to%20a%20large%20variance%20in%20prototype%20alignment.%20In%20this%20work%2C%20we%20adopt%0Aa%20prototype-based%20self-supervised%20one-way%20one-shot%20learning%20framework%20using%0Apseudo-labels%20generated%20from%20superpixels%20to%20learn%20the%20semantic%20segmentation%0Atask%20itself.%20We%20use%20a%20correlation-based%20probability%20score%20to%20generate%20a%20dynamic%0Aprototype%20for%20each%20query%20pixel%20from%20the%20bag%20of%20prototypes%20obtained%20from%20the%0Asupport%20feature%20map.%20This%20weighting%20scheme%20helps%20to%20give%20a%20higher%20weightage%20to%0Acontextually%20related%20prototypes.%20We%20also%20propose%20a%20quadrant%20masking%20strategy%20in%0Athe%20downstream%20segmentation%20task%20by%20utilizing%20prior%20domain%20information%20to%0Adiscard%20unwanted%20false%20positives.%20We%20present%20extensive%20experimentations%20and%0Aevaluations%20on%20abdominal%20CT%20and%20MR%20datasets%20to%20show%20that%20the%20proposed%20simple%0Abut%20potent%20framework%20performs%20at%20par%20with%20the%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06235v1&entry.124074799=Read"},
{"title": "Audio Enhancement for Computer Audition -- An Iterative Training\n  Paradigm Using Sample Importance", "author": "Manuel Milling and Shuo Liu and Andreas Triantafyllopoulos and Ilhan Aslan and Bj\u00f6rn W. Schuller", "abstract": "  Neural network models for audio tasks, such as automatic speech recognition\n(ASR) and acoustic scene classification (ASC), are susceptible to noise\ncontamination for real-life applications. To improve audio quality, an\nenhancement module, which can be developed independently, is explicitly used at\nthe front-end of the target audio applications. In this paper, we present an\nend-to-end learning solution to jointly optimise the models for audio\nenhancement (AE) and the subsequent applications. To guide the optimisation of\nthe AE module towards a target application, and especially to overcome\ndifficult samples, we make use of the sample-wise performance measure as an\nindication of sample importance. In experiments, we consider four\nrepresentative applications to evaluate our training paradigm, i.e., ASR,\nspeech command recognition (SCR), speech emotion recognition (SER), and ASC.\nThese applications are associated with speech and non-speech tasks concerning\nsemantic and non-semantic features, transient and global information, and the\nexperimental results indicate that our proposed approach can considerably boost\nthe noise robustness of the models, especially at low signal-to-noise ratios\n(SNRs), for a wide range of computer audition tasks in everyday-life noisy\nenvironments.\n", "link": "http://arxiv.org/abs/2408.06264v1", "date": "2024-08-12", "relevancy": 2.5627, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5342}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5138}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Audio%20Enhancement%20for%20Computer%20Audition%20--%20An%20Iterative%20Training%0A%20%20Paradigm%20Using%20Sample%20Importance&body=Title%3A%20Audio%20Enhancement%20for%20Computer%20Audition%20--%20An%20Iterative%20Training%0A%20%20Paradigm%20Using%20Sample%20Importance%0AAuthor%3A%20Manuel%20Milling%20and%20Shuo%20Liu%20and%20Andreas%20Triantafyllopoulos%20and%20Ilhan%20Aslan%20and%20Bj%C3%B6rn%20W.%20Schuller%0AAbstract%3A%20%20%20Neural%20network%20models%20for%20audio%20tasks%2C%20such%20as%20automatic%20speech%20recognition%0A%28ASR%29%20and%20acoustic%20scene%20classification%20%28ASC%29%2C%20are%20susceptible%20to%20noise%0Acontamination%20for%20real-life%20applications.%20To%20improve%20audio%20quality%2C%20an%0Aenhancement%20module%2C%20which%20can%20be%20developed%20independently%2C%20is%20explicitly%20used%20at%0Athe%20front-end%20of%20the%20target%20audio%20applications.%20In%20this%20paper%2C%20we%20present%20an%0Aend-to-end%20learning%20solution%20to%20jointly%20optimise%20the%20models%20for%20audio%0Aenhancement%20%28AE%29%20and%20the%20subsequent%20applications.%20To%20guide%20the%20optimisation%20of%0Athe%20AE%20module%20towards%20a%20target%20application%2C%20and%20especially%20to%20overcome%0Adifficult%20samples%2C%20we%20make%20use%20of%20the%20sample-wise%20performance%20measure%20as%20an%0Aindication%20of%20sample%20importance.%20In%20experiments%2C%20we%20consider%20four%0Arepresentative%20applications%20to%20evaluate%20our%20training%20paradigm%2C%20i.e.%2C%20ASR%2C%0Aspeech%20command%20recognition%20%28SCR%29%2C%20speech%20emotion%20recognition%20%28SER%29%2C%20and%20ASC.%0AThese%20applications%20are%20associated%20with%20speech%20and%20non-speech%20tasks%20concerning%0Asemantic%20and%20non-semantic%20features%2C%20transient%20and%20global%20information%2C%20and%20the%0Aexperimental%20results%20indicate%20that%20our%20proposed%20approach%20can%20considerably%20boost%0Athe%20noise%20robustness%20of%20the%20models%2C%20especially%20at%20low%20signal-to-noise%20ratios%0A%28SNRs%29%2C%20for%20a%20wide%20range%20of%20computer%20audition%20tasks%20in%20everyday-life%20noisy%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06264v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAudio%2520Enhancement%2520for%2520Computer%2520Audition%2520--%2520An%2520Iterative%2520Training%250A%2520%2520Paradigm%2520Using%2520Sample%2520Importance%26entry.906535625%3DManuel%2520Milling%2520and%2520Shuo%2520Liu%2520and%2520Andreas%2520Triantafyllopoulos%2520and%2520Ilhan%2520Aslan%2520and%2520Bj%25C3%25B6rn%2520W.%2520Schuller%26entry.1292438233%3D%2520%2520Neural%2520network%2520models%2520for%2520audio%2520tasks%252C%2520such%2520as%2520automatic%2520speech%2520recognition%250A%2528ASR%2529%2520and%2520acoustic%2520scene%2520classification%2520%2528ASC%2529%252C%2520are%2520susceptible%2520to%2520noise%250Acontamination%2520for%2520real-life%2520applications.%2520To%2520improve%2520audio%2520quality%252C%2520an%250Aenhancement%2520module%252C%2520which%2520can%2520be%2520developed%2520independently%252C%2520is%2520explicitly%2520used%2520at%250Athe%2520front-end%2520of%2520the%2520target%2520audio%2520applications.%2520In%2520this%2520paper%252C%2520we%2520present%2520an%250Aend-to-end%2520learning%2520solution%2520to%2520jointly%2520optimise%2520the%2520models%2520for%2520audio%250Aenhancement%2520%2528AE%2529%2520and%2520the%2520subsequent%2520applications.%2520To%2520guide%2520the%2520optimisation%2520of%250Athe%2520AE%2520module%2520towards%2520a%2520target%2520application%252C%2520and%2520especially%2520to%2520overcome%250Adifficult%2520samples%252C%2520we%2520make%2520use%2520of%2520the%2520sample-wise%2520performance%2520measure%2520as%2520an%250Aindication%2520of%2520sample%2520importance.%2520In%2520experiments%252C%2520we%2520consider%2520four%250Arepresentative%2520applications%2520to%2520evaluate%2520our%2520training%2520paradigm%252C%2520i.e.%252C%2520ASR%252C%250Aspeech%2520command%2520recognition%2520%2528SCR%2529%252C%2520speech%2520emotion%2520recognition%2520%2528SER%2529%252C%2520and%2520ASC.%250AThese%2520applications%2520are%2520associated%2520with%2520speech%2520and%2520non-speech%2520tasks%2520concerning%250Asemantic%2520and%2520non-semantic%2520features%252C%2520transient%2520and%2520global%2520information%252C%2520and%2520the%250Aexperimental%2520results%2520indicate%2520that%2520our%2520proposed%2520approach%2520can%2520considerably%2520boost%250Athe%2520noise%2520robustness%2520of%2520the%2520models%252C%2520especially%2520at%2520low%2520signal-to-noise%2520ratios%250A%2528SNRs%2529%252C%2520for%2520a%2520wide%2520range%2520of%2520computer%2520audition%2520tasks%2520in%2520everyday-life%2520noisy%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06264v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Audio%20Enhancement%20for%20Computer%20Audition%20--%20An%20Iterative%20Training%0A%20%20Paradigm%20Using%20Sample%20Importance&entry.906535625=Manuel%20Milling%20and%20Shuo%20Liu%20and%20Andreas%20Triantafyllopoulos%20and%20Ilhan%20Aslan%20and%20Bj%C3%B6rn%20W.%20Schuller&entry.1292438233=%20%20Neural%20network%20models%20for%20audio%20tasks%2C%20such%20as%20automatic%20speech%20recognition%0A%28ASR%29%20and%20acoustic%20scene%20classification%20%28ASC%29%2C%20are%20susceptible%20to%20noise%0Acontamination%20for%20real-life%20applications.%20To%20improve%20audio%20quality%2C%20an%0Aenhancement%20module%2C%20which%20can%20be%20developed%20independently%2C%20is%20explicitly%20used%20at%0Athe%20front-end%20of%20the%20target%20audio%20applications.%20In%20this%20paper%2C%20we%20present%20an%0Aend-to-end%20learning%20solution%20to%20jointly%20optimise%20the%20models%20for%20audio%0Aenhancement%20%28AE%29%20and%20the%20subsequent%20applications.%20To%20guide%20the%20optimisation%20of%0Athe%20AE%20module%20towards%20a%20target%20application%2C%20and%20especially%20to%20overcome%0Adifficult%20samples%2C%20we%20make%20use%20of%20the%20sample-wise%20performance%20measure%20as%20an%0Aindication%20of%20sample%20importance.%20In%20experiments%2C%20we%20consider%20four%0Arepresentative%20applications%20to%20evaluate%20our%20training%20paradigm%2C%20i.e.%2C%20ASR%2C%0Aspeech%20command%20recognition%20%28SCR%29%2C%20speech%20emotion%20recognition%20%28SER%29%2C%20and%20ASC.%0AThese%20applications%20are%20associated%20with%20speech%20and%20non-speech%20tasks%20concerning%0Asemantic%20and%20non-semantic%20features%2C%20transient%20and%20global%20information%2C%20and%20the%0Aexperimental%20results%20indicate%20that%20our%20proposed%20approach%20can%20considerably%20boost%0Athe%20noise%20robustness%20of%20the%20models%2C%20especially%20at%20low%20signal-to-noise%20ratios%0A%28SNRs%29%2C%20for%20a%20wide%20range%20of%20computer%20audition%20tasks%20in%20everyday-life%20noisy%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06264v1&entry.124074799=Read"},
{"title": "Anchored Preference Optimization and Contrastive Revisions: Addressing\n  Underspecification in Alignment", "author": "Karel D'Oosterlinck and Winnie Xu and Chris Develder and Thomas Demeester and Amanpreet Singh and Christopher Potts and Douwe Kiela and Shikib Mehri", "abstract": "  Large Language Models (LLMs) are often aligned using contrastive alignment\nobjectives and preference pair datasets. The interaction between model, paired\ndata, and objective makes alignment a complicated procedure, sometimes\nproducing subpar results. We study this and find that (i) preference data gives\na better learning signal when the underlying responses are contrastive, and\n(ii) alignment objectives lead to better performance when they specify more\ncontrol over the model during training. Based on these insights, we introduce\nContrastive Learning from AI Revisions (CLAIR), a data-creation method which\nleads to more contrastive preference pairs, and Anchored Preference\nOptimization (APO), a controllable and more stable alignment objective. We\nalign Llama-3-8B-Instruct using various comparable datasets and alignment\nobjectives and measure MixEval-Hard scores, which correlate highly with human\njudgments. The CLAIR preferences lead to the strongest performance out of all\ndatasets, and APO consistently outperforms less controllable objectives. Our\nbest model, trained on 32K CLAIR preferences with APO, improves\nLlama-3-8B-Instruct by 7.65%, closing the gap with GPT4-turbo by 45%. Our code\nis available at https://github.com/ContextualAI/CLAIR_and_APO.\n", "link": "http://arxiv.org/abs/2408.06266v1", "date": "2024-08-12", "relevancy": 2.5552, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5497}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5021}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4813}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Anchored%20Preference%20Optimization%20and%20Contrastive%20Revisions%3A%20Addressing%0A%20%20Underspecification%20in%20Alignment&body=Title%3A%20Anchored%20Preference%20Optimization%20and%20Contrastive%20Revisions%3A%20Addressing%0A%20%20Underspecification%20in%20Alignment%0AAuthor%3A%20Karel%20D%27Oosterlinck%20and%20Winnie%20Xu%20and%20Chris%20Develder%20and%20Thomas%20Demeester%20and%20Amanpreet%20Singh%20and%20Christopher%20Potts%20and%20Douwe%20Kiela%20and%20Shikib%20Mehri%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20often%20aligned%20using%20contrastive%20alignment%0Aobjectives%20and%20preference%20pair%20datasets.%20The%20interaction%20between%20model%2C%20paired%0Adata%2C%20and%20objective%20makes%20alignment%20a%20complicated%20procedure%2C%20sometimes%0Aproducing%20subpar%20results.%20We%20study%20this%20and%20find%20that%20%28i%29%20preference%20data%20gives%0Aa%20better%20learning%20signal%20when%20the%20underlying%20responses%20are%20contrastive%2C%20and%0A%28ii%29%20alignment%20objectives%20lead%20to%20better%20performance%20when%20they%20specify%20more%0Acontrol%20over%20the%20model%20during%20training.%20Based%20on%20these%20insights%2C%20we%20introduce%0AContrastive%20Learning%20from%20AI%20Revisions%20%28CLAIR%29%2C%20a%20data-creation%20method%20which%0Aleads%20to%20more%20contrastive%20preference%20pairs%2C%20and%20Anchored%20Preference%0AOptimization%20%28APO%29%2C%20a%20controllable%20and%20more%20stable%20alignment%20objective.%20We%0Aalign%20Llama-3-8B-Instruct%20using%20various%20comparable%20datasets%20and%20alignment%0Aobjectives%20and%20measure%20MixEval-Hard%20scores%2C%20which%20correlate%20highly%20with%20human%0Ajudgments.%20The%20CLAIR%20preferences%20lead%20to%20the%20strongest%20performance%20out%20of%20all%0Adatasets%2C%20and%20APO%20consistently%20outperforms%20less%20controllable%20objectives.%20Our%0Abest%20model%2C%20trained%20on%2032K%20CLAIR%20preferences%20with%20APO%2C%20improves%0ALlama-3-8B-Instruct%20by%207.65%25%2C%20closing%20the%20gap%20with%20GPT4-turbo%20by%2045%25.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/ContextualAI/CLAIR_and_APO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06266v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnchored%2520Preference%2520Optimization%2520and%2520Contrastive%2520Revisions%253A%2520Addressing%250A%2520%2520Underspecification%2520in%2520Alignment%26entry.906535625%3DKarel%2520D%2527Oosterlinck%2520and%2520Winnie%2520Xu%2520and%2520Chris%2520Develder%2520and%2520Thomas%2520Demeester%2520and%2520Amanpreet%2520Singh%2520and%2520Christopher%2520Potts%2520and%2520Douwe%2520Kiela%2520and%2520Shikib%2520Mehri%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520often%2520aligned%2520using%2520contrastive%2520alignment%250Aobjectives%2520and%2520preference%2520pair%2520datasets.%2520The%2520interaction%2520between%2520model%252C%2520paired%250Adata%252C%2520and%2520objective%2520makes%2520alignment%2520a%2520complicated%2520procedure%252C%2520sometimes%250Aproducing%2520subpar%2520results.%2520We%2520study%2520this%2520and%2520find%2520that%2520%2528i%2529%2520preference%2520data%2520gives%250Aa%2520better%2520learning%2520signal%2520when%2520the%2520underlying%2520responses%2520are%2520contrastive%252C%2520and%250A%2528ii%2529%2520alignment%2520objectives%2520lead%2520to%2520better%2520performance%2520when%2520they%2520specify%2520more%250Acontrol%2520over%2520the%2520model%2520during%2520training.%2520Based%2520on%2520these%2520insights%252C%2520we%2520introduce%250AContrastive%2520Learning%2520from%2520AI%2520Revisions%2520%2528CLAIR%2529%252C%2520a%2520data-creation%2520method%2520which%250Aleads%2520to%2520more%2520contrastive%2520preference%2520pairs%252C%2520and%2520Anchored%2520Preference%250AOptimization%2520%2528APO%2529%252C%2520a%2520controllable%2520and%2520more%2520stable%2520alignment%2520objective.%2520We%250Aalign%2520Llama-3-8B-Instruct%2520using%2520various%2520comparable%2520datasets%2520and%2520alignment%250Aobjectives%2520and%2520measure%2520MixEval-Hard%2520scores%252C%2520which%2520correlate%2520highly%2520with%2520human%250Ajudgments.%2520The%2520CLAIR%2520preferences%2520lead%2520to%2520the%2520strongest%2520performance%2520out%2520of%2520all%250Adatasets%252C%2520and%2520APO%2520consistently%2520outperforms%2520less%2520controllable%2520objectives.%2520Our%250Abest%2520model%252C%2520trained%2520on%252032K%2520CLAIR%2520preferences%2520with%2520APO%252C%2520improves%250ALlama-3-8B-Instruct%2520by%25207.65%2525%252C%2520closing%2520the%2520gap%2520with%2520GPT4-turbo%2520by%252045%2525.%2520Our%2520code%250Ais%2520available%2520at%2520https%253A//github.com/ContextualAI/CLAIR_and_APO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06266v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anchored%20Preference%20Optimization%20and%20Contrastive%20Revisions%3A%20Addressing%0A%20%20Underspecification%20in%20Alignment&entry.906535625=Karel%20D%27Oosterlinck%20and%20Winnie%20Xu%20and%20Chris%20Develder%20and%20Thomas%20Demeester%20and%20Amanpreet%20Singh%20and%20Christopher%20Potts%20and%20Douwe%20Kiela%20and%20Shikib%20Mehri&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20often%20aligned%20using%20contrastive%20alignment%0Aobjectives%20and%20preference%20pair%20datasets.%20The%20interaction%20between%20model%2C%20paired%0Adata%2C%20and%20objective%20makes%20alignment%20a%20complicated%20procedure%2C%20sometimes%0Aproducing%20subpar%20results.%20We%20study%20this%20and%20find%20that%20%28i%29%20preference%20data%20gives%0Aa%20better%20learning%20signal%20when%20the%20underlying%20responses%20are%20contrastive%2C%20and%0A%28ii%29%20alignment%20objectives%20lead%20to%20better%20performance%20when%20they%20specify%20more%0Acontrol%20over%20the%20model%20during%20training.%20Based%20on%20these%20insights%2C%20we%20introduce%0AContrastive%20Learning%20from%20AI%20Revisions%20%28CLAIR%29%2C%20a%20data-creation%20method%20which%0Aleads%20to%20more%20contrastive%20preference%20pairs%2C%20and%20Anchored%20Preference%0AOptimization%20%28APO%29%2C%20a%20controllable%20and%20more%20stable%20alignment%20objective.%20We%0Aalign%20Llama-3-8B-Instruct%20using%20various%20comparable%20datasets%20and%20alignment%0Aobjectives%20and%20measure%20MixEval-Hard%20scores%2C%20which%20correlate%20highly%20with%20human%0Ajudgments.%20The%20CLAIR%20preferences%20lead%20to%20the%20strongest%20performance%20out%20of%20all%0Adatasets%2C%20and%20APO%20consistently%20outperforms%20less%20controllable%20objectives.%20Our%0Abest%20model%2C%20trained%20on%2032K%20CLAIR%20preferences%20with%20APO%2C%20improves%0ALlama-3-8B-Instruct%20by%207.65%25%2C%20closing%20the%20gap%20with%20GPT4-turbo%20by%2045%25.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/ContextualAI/CLAIR_and_APO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06266v1&entry.124074799=Read"},
{"title": "From SAM to SAM 2: Exploring Improvements in Meta's Segment Anything\n  Model", "author": "Athulya Sundaresan Geetha and Muhammad Hussain", "abstract": "  The Segment Anything Model (SAM), introduced to the computer vision community\nby Meta in April 2023, is a groundbreaking tool that allows automated\nsegmentation of objects in images based on prompts such as text, clicks, or\nbounding boxes. SAM excels in zero-shot performance, segmenting unseen objects\nwithout additional training, stimulated by a large dataset of over one billion\nimage masks. SAM 2 expands this functionality to video, leveraging memory from\npreceding and subsequent frames to generate accurate segmentation across entire\nvideos, enabling near real-time performance. This comparison shows how SAM has\nevolved to meet the growing need for precise and efficient segmentation in\nvarious applications. The study suggests that future advancements in models\nlike SAM will be crucial for improving computer vision technology.\n", "link": "http://arxiv.org/abs/2408.06305v1", "date": "2024-08-12", "relevancy": 2.5379, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5454}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.49}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4873}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20SAM%20to%20SAM%202%3A%20Exploring%20Improvements%20in%20Meta%27s%20Segment%20Anything%0A%20%20Model&body=Title%3A%20From%20SAM%20to%20SAM%202%3A%20Exploring%20Improvements%20in%20Meta%27s%20Segment%20Anything%0A%20%20Model%0AAuthor%3A%20Athulya%20Sundaresan%20Geetha%20and%20Muhammad%20Hussain%0AAbstract%3A%20%20%20The%20Segment%20Anything%20Model%20%28SAM%29%2C%20introduced%20to%20the%20computer%20vision%20community%0Aby%20Meta%20in%20April%202023%2C%20is%20a%20groundbreaking%20tool%20that%20allows%20automated%0Asegmentation%20of%20objects%20in%20images%20based%20on%20prompts%20such%20as%20text%2C%20clicks%2C%20or%0Abounding%20boxes.%20SAM%20excels%20in%20zero-shot%20performance%2C%20segmenting%20unseen%20objects%0Awithout%20additional%20training%2C%20stimulated%20by%20a%20large%20dataset%20of%20over%20one%20billion%0Aimage%20masks.%20SAM%202%20expands%20this%20functionality%20to%20video%2C%20leveraging%20memory%20from%0Apreceding%20and%20subsequent%20frames%20to%20generate%20accurate%20segmentation%20across%20entire%0Avideos%2C%20enabling%20near%20real-time%20performance.%20This%20comparison%20shows%20how%20SAM%20has%0Aevolved%20to%20meet%20the%20growing%20need%20for%20precise%20and%20efficient%20segmentation%20in%0Avarious%20applications.%20The%20study%20suggests%20that%20future%20advancements%20in%20models%0Alike%20SAM%20will%20be%20crucial%20for%20improving%20computer%20vision%20technology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06305v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520SAM%2520to%2520SAM%25202%253A%2520Exploring%2520Improvements%2520in%2520Meta%2527s%2520Segment%2520Anything%250A%2520%2520Model%26entry.906535625%3DAthulya%2520Sundaresan%2520Geetha%2520and%2520Muhammad%2520Hussain%26entry.1292438233%3D%2520%2520The%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%252C%2520introduced%2520to%2520the%2520computer%2520vision%2520community%250Aby%2520Meta%2520in%2520April%25202023%252C%2520is%2520a%2520groundbreaking%2520tool%2520that%2520allows%2520automated%250Asegmentation%2520of%2520objects%2520in%2520images%2520based%2520on%2520prompts%2520such%2520as%2520text%252C%2520clicks%252C%2520or%250Abounding%2520boxes.%2520SAM%2520excels%2520in%2520zero-shot%2520performance%252C%2520segmenting%2520unseen%2520objects%250Awithout%2520additional%2520training%252C%2520stimulated%2520by%2520a%2520large%2520dataset%2520of%2520over%2520one%2520billion%250Aimage%2520masks.%2520SAM%25202%2520expands%2520this%2520functionality%2520to%2520video%252C%2520leveraging%2520memory%2520from%250Apreceding%2520and%2520subsequent%2520frames%2520to%2520generate%2520accurate%2520segmentation%2520across%2520entire%250Avideos%252C%2520enabling%2520near%2520real-time%2520performance.%2520This%2520comparison%2520shows%2520how%2520SAM%2520has%250Aevolved%2520to%2520meet%2520the%2520growing%2520need%2520for%2520precise%2520and%2520efficient%2520segmentation%2520in%250Avarious%2520applications.%2520The%2520study%2520suggests%2520that%2520future%2520advancements%2520in%2520models%250Alike%2520SAM%2520will%2520be%2520crucial%2520for%2520improving%2520computer%2520vision%2520technology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06305v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20SAM%20to%20SAM%202%3A%20Exploring%20Improvements%20in%20Meta%27s%20Segment%20Anything%0A%20%20Model&entry.906535625=Athulya%20Sundaresan%20Geetha%20and%20Muhammad%20Hussain&entry.1292438233=%20%20The%20Segment%20Anything%20Model%20%28SAM%29%2C%20introduced%20to%20the%20computer%20vision%20community%0Aby%20Meta%20in%20April%202023%2C%20is%20a%20groundbreaking%20tool%20that%20allows%20automated%0Asegmentation%20of%20objects%20in%20images%20based%20on%20prompts%20such%20as%20text%2C%20clicks%2C%20or%0Abounding%20boxes.%20SAM%20excels%20in%20zero-shot%20performance%2C%20segmenting%20unseen%20objects%0Awithout%20additional%20training%2C%20stimulated%20by%20a%20large%20dataset%20of%20over%20one%20billion%0Aimage%20masks.%20SAM%202%20expands%20this%20functionality%20to%20video%2C%20leveraging%20memory%20from%0Apreceding%20and%20subsequent%20frames%20to%20generate%20accurate%20segmentation%20across%20entire%0Avideos%2C%20enabling%20near%20real-time%20performance.%20This%20comparison%20shows%20how%20SAM%20has%0Aevolved%20to%20meet%20the%20growing%20need%20for%20precise%20and%20efficient%20segmentation%20in%0Avarious%20applications.%20The%20study%20suggests%20that%20future%20advancements%20in%20models%0Alike%20SAM%20will%20be%20crucial%20for%20improving%20computer%20vision%20technology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06305v1&entry.124074799=Read"},
{"title": "Manifold Learning by Mixture Models of VAEs for Inverse Problems", "author": "Giovanni S. Alberti and Johannes Hertrich and Matteo Santacesaria and Silvia Sciutto", "abstract": "  Representing a manifold of very high-dimensional data with generative models\nhas been shown to be computationally efficient in practice. However, this\nrequires that the data manifold admits a global parameterization. In order to\nrepresent manifolds of arbitrary topology, we propose to learn a mixture model\nof variational autoencoders. Here, every encoder-decoder pair represents one\nchart of a manifold. We propose a loss function for maximum likelihood\nestimation of the model weights and choose an architecture that provides us the\nanalytical expression of the charts and of their inverses. Once the manifold is\nlearned, we use it for solving inverse problems by minimizing a data fidelity\nterm restricted to the learned manifold. To solve the arising minimization\nproblem we propose a Riemannian gradient descent algorithm on the learned\nmanifold. We demonstrate the performance of our method for low-dimensional toy\nexamples as well as for deblurring and electrical impedance tomography on\ncertain image manifolds.\n", "link": "http://arxiv.org/abs/2303.15244v3", "date": "2024-08-12", "relevancy": 2.5294, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5306}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Manifold%20Learning%20by%20Mixture%20Models%20of%20VAEs%20for%20Inverse%20Problems&body=Title%3A%20Manifold%20Learning%20by%20Mixture%20Models%20of%20VAEs%20for%20Inverse%20Problems%0AAuthor%3A%20Giovanni%20S.%20Alberti%20and%20Johannes%20Hertrich%20and%20Matteo%20Santacesaria%20and%20Silvia%20Sciutto%0AAbstract%3A%20%20%20Representing%20a%20manifold%20of%20very%20high-dimensional%20data%20with%20generative%20models%0Ahas%20been%20shown%20to%20be%20computationally%20efficient%20in%20practice.%20However%2C%20this%0Arequires%20that%20the%20data%20manifold%20admits%20a%20global%20parameterization.%20In%20order%20to%0Arepresent%20manifolds%20of%20arbitrary%20topology%2C%20we%20propose%20to%20learn%20a%20mixture%20model%0Aof%20variational%20autoencoders.%20Here%2C%20every%20encoder-decoder%20pair%20represents%20one%0Achart%20of%20a%20manifold.%20We%20propose%20a%20loss%20function%20for%20maximum%20likelihood%0Aestimation%20of%20the%20model%20weights%20and%20choose%20an%20architecture%20that%20provides%20us%20the%0Aanalytical%20expression%20of%20the%20charts%20and%20of%20their%20inverses.%20Once%20the%20manifold%20is%0Alearned%2C%20we%20use%20it%20for%20solving%20inverse%20problems%20by%20minimizing%20a%20data%20fidelity%0Aterm%20restricted%20to%20the%20learned%20manifold.%20To%20solve%20the%20arising%20minimization%0Aproblem%20we%20propose%20a%20Riemannian%20gradient%20descent%20algorithm%20on%20the%20learned%0Amanifold.%20We%20demonstrate%20the%20performance%20of%20our%20method%20for%20low-dimensional%20toy%0Aexamples%20as%20well%20as%20for%20deblurring%20and%20electrical%20impedance%20tomography%20on%0Acertain%20image%20manifolds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.15244v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DManifold%2520Learning%2520by%2520Mixture%2520Models%2520of%2520VAEs%2520for%2520Inverse%2520Problems%26entry.906535625%3DGiovanni%2520S.%2520Alberti%2520and%2520Johannes%2520Hertrich%2520and%2520Matteo%2520Santacesaria%2520and%2520Silvia%2520Sciutto%26entry.1292438233%3D%2520%2520Representing%2520a%2520manifold%2520of%2520very%2520high-dimensional%2520data%2520with%2520generative%2520models%250Ahas%2520been%2520shown%2520to%2520be%2520computationally%2520efficient%2520in%2520practice.%2520However%252C%2520this%250Arequires%2520that%2520the%2520data%2520manifold%2520admits%2520a%2520global%2520parameterization.%2520In%2520order%2520to%250Arepresent%2520manifolds%2520of%2520arbitrary%2520topology%252C%2520we%2520propose%2520to%2520learn%2520a%2520mixture%2520model%250Aof%2520variational%2520autoencoders.%2520Here%252C%2520every%2520encoder-decoder%2520pair%2520represents%2520one%250Achart%2520of%2520a%2520manifold.%2520We%2520propose%2520a%2520loss%2520function%2520for%2520maximum%2520likelihood%250Aestimation%2520of%2520the%2520model%2520weights%2520and%2520choose%2520an%2520architecture%2520that%2520provides%2520us%2520the%250Aanalytical%2520expression%2520of%2520the%2520charts%2520and%2520of%2520their%2520inverses.%2520Once%2520the%2520manifold%2520is%250Alearned%252C%2520we%2520use%2520it%2520for%2520solving%2520inverse%2520problems%2520by%2520minimizing%2520a%2520data%2520fidelity%250Aterm%2520restricted%2520to%2520the%2520learned%2520manifold.%2520To%2520solve%2520the%2520arising%2520minimization%250Aproblem%2520we%2520propose%2520a%2520Riemannian%2520gradient%2520descent%2520algorithm%2520on%2520the%2520learned%250Amanifold.%2520We%2520demonstrate%2520the%2520performance%2520of%2520our%2520method%2520for%2520low-dimensional%2520toy%250Aexamples%2520as%2520well%2520as%2520for%2520deblurring%2520and%2520electrical%2520impedance%2520tomography%2520on%250Acertain%2520image%2520manifolds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.15244v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Manifold%20Learning%20by%20Mixture%20Models%20of%20VAEs%20for%20Inverse%20Problems&entry.906535625=Giovanni%20S.%20Alberti%20and%20Johannes%20Hertrich%20and%20Matteo%20Santacesaria%20and%20Silvia%20Sciutto&entry.1292438233=%20%20Representing%20a%20manifold%20of%20very%20high-dimensional%20data%20with%20generative%20models%0Ahas%20been%20shown%20to%20be%20computationally%20efficient%20in%20practice.%20However%2C%20this%0Arequires%20that%20the%20data%20manifold%20admits%20a%20global%20parameterization.%20In%20order%20to%0Arepresent%20manifolds%20of%20arbitrary%20topology%2C%20we%20propose%20to%20learn%20a%20mixture%20model%0Aof%20variational%20autoencoders.%20Here%2C%20every%20encoder-decoder%20pair%20represents%20one%0Achart%20of%20a%20manifold.%20We%20propose%20a%20loss%20function%20for%20maximum%20likelihood%0Aestimation%20of%20the%20model%20weights%20and%20choose%20an%20architecture%20that%20provides%20us%20the%0Aanalytical%20expression%20of%20the%20charts%20and%20of%20their%20inverses.%20Once%20the%20manifold%20is%0Alearned%2C%20we%20use%20it%20for%20solving%20inverse%20problems%20by%20minimizing%20a%20data%20fidelity%0Aterm%20restricted%20to%20the%20learned%20manifold.%20To%20solve%20the%20arising%20minimization%0Aproblem%20we%20propose%20a%20Riemannian%20gradient%20descent%20algorithm%20on%20the%20learned%0Amanifold.%20We%20demonstrate%20the%20performance%20of%20our%20method%20for%20low-dimensional%20toy%0Aexamples%20as%20well%20as%20for%20deblurring%20and%20electrical%20impedance%20tomography%20on%0Acertain%20image%20manifolds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.15244v3&entry.124074799=Read"},
{"title": "A secure and private ensemble matcher using multi-vault obfuscated\n  templates", "author": "Babak Poorebrahim Gilkalaye and Shubhabrata Mukherjee and Reza Derakhshani", "abstract": "  Generative AI has revolutionized modern machine learning by providing\nunprecedented realism, diversity, and efficiency in data generation. This\ntechnology holds immense potential for biometrics, including for securing\nsensitive and personally identifiable information. Given the irrevocability of\nbiometric samples and mounting privacy concerns, biometric template security\nand secure matching are among the most sought-after features of modern\nbiometric systems. This paper proposes a novel obfuscation method using\nGenerative AI to enhance biometric template security. Our approach utilizes\nsynthetic facial images generated by a Generative Adversarial Network (GAN) as\n\"random chaff points\" within a secure vault system. Our method creates n\nsub-templates from the original template, each obfuscated with m GAN chaff\npoints. During verification, s closest vectors to the biometric query are\nretrieved from each vault and combined to generate hash values, which are then\ncompared with the stored hash value. Thus, our method safeguards user\nidentities during the training and deployment phases by employing the\nGAN-generated synthetic images. Our protocol was tested using the AT&T, GT, and\nLFW face datasets, achieving ROC areas under the curve of 0.99, 0.99, and 0.90,\nrespectively. Our results demonstrate that the proposed method can maintain\nhigh accuracy and reasonable computational complexity comparable to those\nunprotected template methods while significantly enhancing security and\nprivacy, underscoring the potential of Generative AI in developing proactive\ndefensive strategies for biometric systems.\n", "link": "http://arxiv.org/abs/2404.05205v2", "date": "2024-08-12", "relevancy": 2.5274, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.549}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4936}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20secure%20and%20private%20ensemble%20matcher%20using%20multi-vault%20obfuscated%0A%20%20templates&body=Title%3A%20A%20secure%20and%20private%20ensemble%20matcher%20using%20multi-vault%20obfuscated%0A%20%20templates%0AAuthor%3A%20Babak%20Poorebrahim%20Gilkalaye%20and%20Shubhabrata%20Mukherjee%20and%20Reza%20Derakhshani%0AAbstract%3A%20%20%20Generative%20AI%20has%20revolutionized%20modern%20machine%20learning%20by%20providing%0Aunprecedented%20realism%2C%20diversity%2C%20and%20efficiency%20in%20data%20generation.%20This%0Atechnology%20holds%20immense%20potential%20for%20biometrics%2C%20including%20for%20securing%0Asensitive%20and%20personally%20identifiable%20information.%20Given%20the%20irrevocability%20of%0Abiometric%20samples%20and%20mounting%20privacy%20concerns%2C%20biometric%20template%20security%0Aand%20secure%20matching%20are%20among%20the%20most%20sought-after%20features%20of%20modern%0Abiometric%20systems.%20This%20paper%20proposes%20a%20novel%20obfuscation%20method%20using%0AGenerative%20AI%20to%20enhance%20biometric%20template%20security.%20Our%20approach%20utilizes%0Asynthetic%20facial%20images%20generated%20by%20a%20Generative%20Adversarial%20Network%20%28GAN%29%20as%0A%22random%20chaff%20points%22%20within%20a%20secure%20vault%20system.%20Our%20method%20creates%20n%0Asub-templates%20from%20the%20original%20template%2C%20each%20obfuscated%20with%20m%20GAN%20chaff%0Apoints.%20During%20verification%2C%20s%20closest%20vectors%20to%20the%20biometric%20query%20are%0Aretrieved%20from%20each%20vault%20and%20combined%20to%20generate%20hash%20values%2C%20which%20are%20then%0Acompared%20with%20the%20stored%20hash%20value.%20Thus%2C%20our%20method%20safeguards%20user%0Aidentities%20during%20the%20training%20and%20deployment%20phases%20by%20employing%20the%0AGAN-generated%20synthetic%20images.%20Our%20protocol%20was%20tested%20using%20the%20AT%26T%2C%20GT%2C%20and%0ALFW%20face%20datasets%2C%20achieving%20ROC%20areas%20under%20the%20curve%20of%200.99%2C%200.99%2C%20and%200.90%2C%0Arespectively.%20Our%20results%20demonstrate%20that%20the%20proposed%20method%20can%20maintain%0Ahigh%20accuracy%20and%20reasonable%20computational%20complexity%20comparable%20to%20those%0Aunprotected%20template%20methods%20while%20significantly%20enhancing%20security%20and%0Aprivacy%2C%20underscoring%20the%20potential%20of%20Generative%20AI%20in%20developing%20proactive%0Adefensive%20strategies%20for%20biometric%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05205v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520secure%2520and%2520private%2520ensemble%2520matcher%2520using%2520multi-vault%2520obfuscated%250A%2520%2520templates%26entry.906535625%3DBabak%2520Poorebrahim%2520Gilkalaye%2520and%2520Shubhabrata%2520Mukherjee%2520and%2520Reza%2520Derakhshani%26entry.1292438233%3D%2520%2520Generative%2520AI%2520has%2520revolutionized%2520modern%2520machine%2520learning%2520by%2520providing%250Aunprecedented%2520realism%252C%2520diversity%252C%2520and%2520efficiency%2520in%2520data%2520generation.%2520This%250Atechnology%2520holds%2520immense%2520potential%2520for%2520biometrics%252C%2520including%2520for%2520securing%250Asensitive%2520and%2520personally%2520identifiable%2520information.%2520Given%2520the%2520irrevocability%2520of%250Abiometric%2520samples%2520and%2520mounting%2520privacy%2520concerns%252C%2520biometric%2520template%2520security%250Aand%2520secure%2520matching%2520are%2520among%2520the%2520most%2520sought-after%2520features%2520of%2520modern%250Abiometric%2520systems.%2520This%2520paper%2520proposes%2520a%2520novel%2520obfuscation%2520method%2520using%250AGenerative%2520AI%2520to%2520enhance%2520biometric%2520template%2520security.%2520Our%2520approach%2520utilizes%250Asynthetic%2520facial%2520images%2520generated%2520by%2520a%2520Generative%2520Adversarial%2520Network%2520%2528GAN%2529%2520as%250A%2522random%2520chaff%2520points%2522%2520within%2520a%2520secure%2520vault%2520system.%2520Our%2520method%2520creates%2520n%250Asub-templates%2520from%2520the%2520original%2520template%252C%2520each%2520obfuscated%2520with%2520m%2520GAN%2520chaff%250Apoints.%2520During%2520verification%252C%2520s%2520closest%2520vectors%2520to%2520the%2520biometric%2520query%2520are%250Aretrieved%2520from%2520each%2520vault%2520and%2520combined%2520to%2520generate%2520hash%2520values%252C%2520which%2520are%2520then%250Acompared%2520with%2520the%2520stored%2520hash%2520value.%2520Thus%252C%2520our%2520method%2520safeguards%2520user%250Aidentities%2520during%2520the%2520training%2520and%2520deployment%2520phases%2520by%2520employing%2520the%250AGAN-generated%2520synthetic%2520images.%2520Our%2520protocol%2520was%2520tested%2520using%2520the%2520AT%2526T%252C%2520GT%252C%2520and%250ALFW%2520face%2520datasets%252C%2520achieving%2520ROC%2520areas%2520under%2520the%2520curve%2520of%25200.99%252C%25200.99%252C%2520and%25200.90%252C%250Arespectively.%2520Our%2520results%2520demonstrate%2520that%2520the%2520proposed%2520method%2520can%2520maintain%250Ahigh%2520accuracy%2520and%2520reasonable%2520computational%2520complexity%2520comparable%2520to%2520those%250Aunprotected%2520template%2520methods%2520while%2520significantly%2520enhancing%2520security%2520and%250Aprivacy%252C%2520underscoring%2520the%2520potential%2520of%2520Generative%2520AI%2520in%2520developing%2520proactive%250Adefensive%2520strategies%2520for%2520biometric%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.05205v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20secure%20and%20private%20ensemble%20matcher%20using%20multi-vault%20obfuscated%0A%20%20templates&entry.906535625=Babak%20Poorebrahim%20Gilkalaye%20and%20Shubhabrata%20Mukherjee%20and%20Reza%20Derakhshani&entry.1292438233=%20%20Generative%20AI%20has%20revolutionized%20modern%20machine%20learning%20by%20providing%0Aunprecedented%20realism%2C%20diversity%2C%20and%20efficiency%20in%20data%20generation.%20This%0Atechnology%20holds%20immense%20potential%20for%20biometrics%2C%20including%20for%20securing%0Asensitive%20and%20personally%20identifiable%20information.%20Given%20the%20irrevocability%20of%0Abiometric%20samples%20and%20mounting%20privacy%20concerns%2C%20biometric%20template%20security%0Aand%20secure%20matching%20are%20among%20the%20most%20sought-after%20features%20of%20modern%0Abiometric%20systems.%20This%20paper%20proposes%20a%20novel%20obfuscation%20method%20using%0AGenerative%20AI%20to%20enhance%20biometric%20template%20security.%20Our%20approach%20utilizes%0Asynthetic%20facial%20images%20generated%20by%20a%20Generative%20Adversarial%20Network%20%28GAN%29%20as%0A%22random%20chaff%20points%22%20within%20a%20secure%20vault%20system.%20Our%20method%20creates%20n%0Asub-templates%20from%20the%20original%20template%2C%20each%20obfuscated%20with%20m%20GAN%20chaff%0Apoints.%20During%20verification%2C%20s%20closest%20vectors%20to%20the%20biometric%20query%20are%0Aretrieved%20from%20each%20vault%20and%20combined%20to%20generate%20hash%20values%2C%20which%20are%20then%0Acompared%20with%20the%20stored%20hash%20value.%20Thus%2C%20our%20method%20safeguards%20user%0Aidentities%20during%20the%20training%20and%20deployment%20phases%20by%20employing%20the%0AGAN-generated%20synthetic%20images.%20Our%20protocol%20was%20tested%20using%20the%20AT%26T%2C%20GT%2C%20and%0ALFW%20face%20datasets%2C%20achieving%20ROC%20areas%20under%20the%20curve%20of%200.99%2C%200.99%2C%20and%200.90%2C%0Arespectively.%20Our%20results%20demonstrate%20that%20the%20proposed%20method%20can%20maintain%0Ahigh%20accuracy%20and%20reasonable%20computational%20complexity%20comparable%20to%20those%0Aunprotected%20template%20methods%20while%20significantly%20enhancing%20security%20and%0Aprivacy%2C%20underscoring%20the%20potential%20of%20Generative%20AI%20in%20developing%20proactive%0Adefensive%20strategies%20for%20biometric%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05205v2&entry.124074799=Read"},
{"title": "CT evaluation of 2D and 3D holistic deep learning methods for the\n  volumetric segmentation of airway lesions", "author": "Amel Imene Hadj Bouzid and Baudouin Denis de Senneville and Fabien Baldacci and Pascal Desbarats and Patrick Berger and Ilyes Benlala and Ga\u00ebl Dournes", "abstract": "  This research embarked on a comparative exploration of the holistic\nsegmentation capabilities of Convolutional Neural Networks (CNNs) in both 2D\nand 3D formats, focusing on cystic fibrosis (CF) lesions. The study utilized\ndata from two CF reference centers, covering five major CF structural changes.\nInitially, it compared the 2D and 3D models, highlighting the 3D model's\nsuperior capability in capturing complex features like mucus plugs and\nconsolidations. To improve the 2D model's performance, a loss adapted to fine\nstructures segmentation was implemented and evaluated, significantly enhancing\nits accuracy, though not surpassing the 3D model's performance. The models\nunderwent further validation through external evaluation against pulmonary\nfunction tests (PFTs), confirming the robustness of the findings. Moreover,\nthis study went beyond comparing metrics; it also included comprehensive\nassessments of the models' interpretability and reliability, providing valuable\ninsights for their clinical application.\n", "link": "http://arxiv.org/abs/2403.08042v2", "date": "2024-08-12", "relevancy": 2.5198, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5102}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5102}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CT%20evaluation%20of%202D%20and%203D%20holistic%20deep%20learning%20methods%20for%20the%0A%20%20volumetric%20segmentation%20of%20airway%20lesions&body=Title%3A%20CT%20evaluation%20of%202D%20and%203D%20holistic%20deep%20learning%20methods%20for%20the%0A%20%20volumetric%20segmentation%20of%20airway%20lesions%0AAuthor%3A%20Amel%20Imene%20Hadj%20Bouzid%20and%20Baudouin%20Denis%20de%20Senneville%20and%20Fabien%20Baldacci%20and%20Pascal%20Desbarats%20and%20Patrick%20Berger%20and%20Ilyes%20Benlala%20and%20Ga%C3%ABl%20Dournes%0AAbstract%3A%20%20%20This%20research%20embarked%20on%20a%20comparative%20exploration%20of%20the%20holistic%0Asegmentation%20capabilities%20of%20Convolutional%20Neural%20Networks%20%28CNNs%29%20in%20both%202D%0Aand%203D%20formats%2C%20focusing%20on%20cystic%20fibrosis%20%28CF%29%20lesions.%20The%20study%20utilized%0Adata%20from%20two%20CF%20reference%20centers%2C%20covering%20five%20major%20CF%20structural%20changes.%0AInitially%2C%20it%20compared%20the%202D%20and%203D%20models%2C%20highlighting%20the%203D%20model%27s%0Asuperior%20capability%20in%20capturing%20complex%20features%20like%20mucus%20plugs%20and%0Aconsolidations.%20To%20improve%20the%202D%20model%27s%20performance%2C%20a%20loss%20adapted%20to%20fine%0Astructures%20segmentation%20was%20implemented%20and%20evaluated%2C%20significantly%20enhancing%0Aits%20accuracy%2C%20though%20not%20surpassing%20the%203D%20model%27s%20performance.%20The%20models%0Aunderwent%20further%20validation%20through%20external%20evaluation%20against%20pulmonary%0Afunction%20tests%20%28PFTs%29%2C%20confirming%20the%20robustness%20of%20the%20findings.%20Moreover%2C%0Athis%20study%20went%20beyond%20comparing%20metrics%3B%20it%20also%20included%20comprehensive%0Aassessments%20of%20the%20models%27%20interpretability%20and%20reliability%2C%20providing%20valuable%0Ainsights%20for%20their%20clinical%20application.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08042v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCT%2520evaluation%2520of%25202D%2520and%25203D%2520holistic%2520deep%2520learning%2520methods%2520for%2520the%250A%2520%2520volumetric%2520segmentation%2520of%2520airway%2520lesions%26entry.906535625%3DAmel%2520Imene%2520Hadj%2520Bouzid%2520and%2520Baudouin%2520Denis%2520de%2520Senneville%2520and%2520Fabien%2520Baldacci%2520and%2520Pascal%2520Desbarats%2520and%2520Patrick%2520Berger%2520and%2520Ilyes%2520Benlala%2520and%2520Ga%25C3%25ABl%2520Dournes%26entry.1292438233%3D%2520%2520This%2520research%2520embarked%2520on%2520a%2520comparative%2520exploration%2520of%2520the%2520holistic%250Asegmentation%2520capabilities%2520of%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520in%2520both%25202D%250Aand%25203D%2520formats%252C%2520focusing%2520on%2520cystic%2520fibrosis%2520%2528CF%2529%2520lesions.%2520The%2520study%2520utilized%250Adata%2520from%2520two%2520CF%2520reference%2520centers%252C%2520covering%2520five%2520major%2520CF%2520structural%2520changes.%250AInitially%252C%2520it%2520compared%2520the%25202D%2520and%25203D%2520models%252C%2520highlighting%2520the%25203D%2520model%2527s%250Asuperior%2520capability%2520in%2520capturing%2520complex%2520features%2520like%2520mucus%2520plugs%2520and%250Aconsolidations.%2520To%2520improve%2520the%25202D%2520model%2527s%2520performance%252C%2520a%2520loss%2520adapted%2520to%2520fine%250Astructures%2520segmentation%2520was%2520implemented%2520and%2520evaluated%252C%2520significantly%2520enhancing%250Aits%2520accuracy%252C%2520though%2520not%2520surpassing%2520the%25203D%2520model%2527s%2520performance.%2520The%2520models%250Aunderwent%2520further%2520validation%2520through%2520external%2520evaluation%2520against%2520pulmonary%250Afunction%2520tests%2520%2528PFTs%2529%252C%2520confirming%2520the%2520robustness%2520of%2520the%2520findings.%2520Moreover%252C%250Athis%2520study%2520went%2520beyond%2520comparing%2520metrics%253B%2520it%2520also%2520included%2520comprehensive%250Aassessments%2520of%2520the%2520models%2527%2520interpretability%2520and%2520reliability%252C%2520providing%2520valuable%250Ainsights%2520for%2520their%2520clinical%2520application.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.08042v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CT%20evaluation%20of%202D%20and%203D%20holistic%20deep%20learning%20methods%20for%20the%0A%20%20volumetric%20segmentation%20of%20airway%20lesions&entry.906535625=Amel%20Imene%20Hadj%20Bouzid%20and%20Baudouin%20Denis%20de%20Senneville%20and%20Fabien%20Baldacci%20and%20Pascal%20Desbarats%20and%20Patrick%20Berger%20and%20Ilyes%20Benlala%20and%20Ga%C3%ABl%20Dournes&entry.1292438233=%20%20This%20research%20embarked%20on%20a%20comparative%20exploration%20of%20the%20holistic%0Asegmentation%20capabilities%20of%20Convolutional%20Neural%20Networks%20%28CNNs%29%20in%20both%202D%0Aand%203D%20formats%2C%20focusing%20on%20cystic%20fibrosis%20%28CF%29%20lesions.%20The%20study%20utilized%0Adata%20from%20two%20CF%20reference%20centers%2C%20covering%20five%20major%20CF%20structural%20changes.%0AInitially%2C%20it%20compared%20the%202D%20and%203D%20models%2C%20highlighting%20the%203D%20model%27s%0Asuperior%20capability%20in%20capturing%20complex%20features%20like%20mucus%20plugs%20and%0Aconsolidations.%20To%20improve%20the%202D%20model%27s%20performance%2C%20a%20loss%20adapted%20to%20fine%0Astructures%20segmentation%20was%20implemented%20and%20evaluated%2C%20significantly%20enhancing%0Aits%20accuracy%2C%20though%20not%20surpassing%20the%203D%20model%27s%20performance.%20The%20models%0Aunderwent%20further%20validation%20through%20external%20evaluation%20against%20pulmonary%0Afunction%20tests%20%28PFTs%29%2C%20confirming%20the%20robustness%20of%20the%20findings.%20Moreover%2C%0Athis%20study%20went%20beyond%20comparing%20metrics%3B%20it%20also%20included%20comprehensive%0Aassessments%20of%20the%20models%27%20interpretability%20and%20reliability%2C%20providing%20valuable%0Ainsights%20for%20their%20clinical%20application.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08042v2&entry.124074799=Read"},
{"title": "ControlNeXt: Powerful and Efficient Control for Image and Video\n  Generation", "author": "Bohao Peng and Jian Wang and Yuechen Zhang and Wenbo Li and Ming-Chang Yang and Jiaya Jia", "abstract": "  Diffusion models have demonstrated remarkable and robust abilities in both\nimage and video generation. To achieve greater control over generated results,\nresearchers introduce additional architectures, such as ControlNet, Adapters\nand ReferenceNet, to integrate conditioning controls. However, current\ncontrollable generation methods often require substantial additional\ncomputational resources, especially for video generation, and face challenges\nin training or exhibit weak control. In this paper, we propose ControlNeXt: a\npowerful and efficient method for controllable image and video generation. We\nfirst design a more straightforward and efficient architecture, replacing heavy\nadditional branches with minimal additional cost compared to the base model.\nSuch a concise structure also allows our method to seamlessly integrate with\nother LoRA weights, enabling style alteration without the need for additional\ntraining. As for training, we reduce up to 90% of learnable parameters compared\nto the alternatives. Furthermore, we propose another method called Cross\nNormalization (CN) as a replacement for Zero-Convolution' to achieve fast and\nstable training convergence. We have conducted various experiments with\ndifferent base models across images and videos, demonstrating the robustness of\nour method.\n", "link": "http://arxiv.org/abs/2408.06070v1", "date": "2024-08-12", "relevancy": 2.4954, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6422}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6262}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6141}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ControlNeXt%3A%20Powerful%20and%20Efficient%20Control%20for%20Image%20and%20Video%0A%20%20Generation&body=Title%3A%20ControlNeXt%3A%20Powerful%20and%20Efficient%20Control%20for%20Image%20and%20Video%0A%20%20Generation%0AAuthor%3A%20Bohao%20Peng%20and%20Jian%20Wang%20and%20Yuechen%20Zhang%20and%20Wenbo%20Li%20and%20Ming-Chang%20Yang%20and%20Jiaya%20Jia%0AAbstract%3A%20%20%20Diffusion%20models%20have%20demonstrated%20remarkable%20and%20robust%20abilities%20in%20both%0Aimage%20and%20video%20generation.%20To%20achieve%20greater%20control%20over%20generated%20results%2C%0Aresearchers%20introduce%20additional%20architectures%2C%20such%20as%20ControlNet%2C%20Adapters%0Aand%20ReferenceNet%2C%20to%20integrate%20conditioning%20controls.%20However%2C%20current%0Acontrollable%20generation%20methods%20often%20require%20substantial%20additional%0Acomputational%20resources%2C%20especially%20for%20video%20generation%2C%20and%20face%20challenges%0Ain%20training%20or%20exhibit%20weak%20control.%20In%20this%20paper%2C%20we%20propose%20ControlNeXt%3A%20a%0Apowerful%20and%20efficient%20method%20for%20controllable%20image%20and%20video%20generation.%20We%0Afirst%20design%20a%20more%20straightforward%20and%20efficient%20architecture%2C%20replacing%20heavy%0Aadditional%20branches%20with%20minimal%20additional%20cost%20compared%20to%20the%20base%20model.%0ASuch%20a%20concise%20structure%20also%20allows%20our%20method%20to%20seamlessly%20integrate%20with%0Aother%20LoRA%20weights%2C%20enabling%20style%20alteration%20without%20the%20need%20for%20additional%0Atraining.%20As%20for%20training%2C%20we%20reduce%20up%20to%2090%25%20of%20learnable%20parameters%20compared%0Ato%20the%20alternatives.%20Furthermore%2C%20we%20propose%20another%20method%20called%20Cross%0ANormalization%20%28CN%29%20as%20a%20replacement%20for%20Zero-Convolution%27%20to%20achieve%20fast%20and%0Astable%20training%20convergence.%20We%20have%20conducted%20various%20experiments%20with%0Adifferent%20base%20models%20across%20images%20and%20videos%2C%20demonstrating%20the%20robustness%20of%0Aour%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06070v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DControlNeXt%253A%2520Powerful%2520and%2520Efficient%2520Control%2520for%2520Image%2520and%2520Video%250A%2520%2520Generation%26entry.906535625%3DBohao%2520Peng%2520and%2520Jian%2520Wang%2520and%2520Yuechen%2520Zhang%2520and%2520Wenbo%2520Li%2520and%2520Ming-Chang%2520Yang%2520and%2520Jiaya%2520Jia%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520demonstrated%2520remarkable%2520and%2520robust%2520abilities%2520in%2520both%250Aimage%2520and%2520video%2520generation.%2520To%2520achieve%2520greater%2520control%2520over%2520generated%2520results%252C%250Aresearchers%2520introduce%2520additional%2520architectures%252C%2520such%2520as%2520ControlNet%252C%2520Adapters%250Aand%2520ReferenceNet%252C%2520to%2520integrate%2520conditioning%2520controls.%2520However%252C%2520current%250Acontrollable%2520generation%2520methods%2520often%2520require%2520substantial%2520additional%250Acomputational%2520resources%252C%2520especially%2520for%2520video%2520generation%252C%2520and%2520face%2520challenges%250Ain%2520training%2520or%2520exhibit%2520weak%2520control.%2520In%2520this%2520paper%252C%2520we%2520propose%2520ControlNeXt%253A%2520a%250Apowerful%2520and%2520efficient%2520method%2520for%2520controllable%2520image%2520and%2520video%2520generation.%2520We%250Afirst%2520design%2520a%2520more%2520straightforward%2520and%2520efficient%2520architecture%252C%2520replacing%2520heavy%250Aadditional%2520branches%2520with%2520minimal%2520additional%2520cost%2520compared%2520to%2520the%2520base%2520model.%250ASuch%2520a%2520concise%2520structure%2520also%2520allows%2520our%2520method%2520to%2520seamlessly%2520integrate%2520with%250Aother%2520LoRA%2520weights%252C%2520enabling%2520style%2520alteration%2520without%2520the%2520need%2520for%2520additional%250Atraining.%2520As%2520for%2520training%252C%2520we%2520reduce%2520up%2520to%252090%2525%2520of%2520learnable%2520parameters%2520compared%250Ato%2520the%2520alternatives.%2520Furthermore%252C%2520we%2520propose%2520another%2520method%2520called%2520Cross%250ANormalization%2520%2528CN%2529%2520as%2520a%2520replacement%2520for%2520Zero-Convolution%2527%2520to%2520achieve%2520fast%2520and%250Astable%2520training%2520convergence.%2520We%2520have%2520conducted%2520various%2520experiments%2520with%250Adifferent%2520base%2520models%2520across%2520images%2520and%2520videos%252C%2520demonstrating%2520the%2520robustness%2520of%250Aour%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06070v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ControlNeXt%3A%20Powerful%20and%20Efficient%20Control%20for%20Image%20and%20Video%0A%20%20Generation&entry.906535625=Bohao%20Peng%20and%20Jian%20Wang%20and%20Yuechen%20Zhang%20and%20Wenbo%20Li%20and%20Ming-Chang%20Yang%20and%20Jiaya%20Jia&entry.1292438233=%20%20Diffusion%20models%20have%20demonstrated%20remarkable%20and%20robust%20abilities%20in%20both%0Aimage%20and%20video%20generation.%20To%20achieve%20greater%20control%20over%20generated%20results%2C%0Aresearchers%20introduce%20additional%20architectures%2C%20such%20as%20ControlNet%2C%20Adapters%0Aand%20ReferenceNet%2C%20to%20integrate%20conditioning%20controls.%20However%2C%20current%0Acontrollable%20generation%20methods%20often%20require%20substantial%20additional%0Acomputational%20resources%2C%20especially%20for%20video%20generation%2C%20and%20face%20challenges%0Ain%20training%20or%20exhibit%20weak%20control.%20In%20this%20paper%2C%20we%20propose%20ControlNeXt%3A%20a%0Apowerful%20and%20efficient%20method%20for%20controllable%20image%20and%20video%20generation.%20We%0Afirst%20design%20a%20more%20straightforward%20and%20efficient%20architecture%2C%20replacing%20heavy%0Aadditional%20branches%20with%20minimal%20additional%20cost%20compared%20to%20the%20base%20model.%0ASuch%20a%20concise%20structure%20also%20allows%20our%20method%20to%20seamlessly%20integrate%20with%0Aother%20LoRA%20weights%2C%20enabling%20style%20alteration%20without%20the%20need%20for%20additional%0Atraining.%20As%20for%20training%2C%20we%20reduce%20up%20to%2090%25%20of%20learnable%20parameters%20compared%0Ato%20the%20alternatives.%20Furthermore%2C%20we%20propose%20another%20method%20called%20Cross%0ANormalization%20%28CN%29%20as%20a%20replacement%20for%20Zero-Convolution%27%20to%20achieve%20fast%20and%0Astable%20training%20convergence.%20We%20have%20conducted%20various%20experiments%20with%0Adifferent%20base%20models%20across%20images%20and%20videos%2C%20demonstrating%20the%20robustness%20of%0Aour%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06070v1&entry.124074799=Read"},
{"title": "What Ails Generative Structure-based Drug Design: Too Little or Too Much\n  Expressivity?", "author": "Rafa\u0142 Karczewski and Samuel Kaski and Markus Heinonen and Vikas Garg", "abstract": "  Several generative models with elaborate training and sampling procedures\nhave been proposed recently to accelerate structure-based drug design (SBDD);\nhowever, perplexingly, their empirical performance turns out to be suboptimal.\nWe seek to better understand this phenomenon from both theoretical and\nempirical perspectives. Since most of these models apply graph neural networks\n(GNNs), one may suspect that they inherit the representational limitations of\nGNNs. We analyze this aspect, establishing the first such results for\nprotein-ligand complexes. A plausible counterview may attribute the\nunderperformance of these models to their excessive parameterizations, inducing\nexpressivity at the expense of generalization. We also investigate this\npossibility with a simple metric-aware approach that learns an economical\nsurrogate for affinity to infer an unlabelled molecular graph and optimizes for\nlabels conditioned on this graph and molecular properties. The resulting model\nachieves state-of-the-art results using 100x fewer trainable parameters and\naffords up to 1000x speedup. Collectively, our findings underscore the need to\nreassess and redirect the existing paradigm and efforts for SBDD.\n", "link": "http://arxiv.org/abs/2408.06050v1", "date": "2024-08-12", "relevancy": 2.4561, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.508}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4993}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Ails%20Generative%20Structure-based%20Drug%20Design%3A%20Too%20Little%20or%20Too%20Much%0A%20%20Expressivity%3F&body=Title%3A%20What%20Ails%20Generative%20Structure-based%20Drug%20Design%3A%20Too%20Little%20or%20Too%20Much%0A%20%20Expressivity%3F%0AAuthor%3A%20Rafa%C5%82%20Karczewski%20and%20Samuel%20Kaski%20and%20Markus%20Heinonen%20and%20Vikas%20Garg%0AAbstract%3A%20%20%20Several%20generative%20models%20with%20elaborate%20training%20and%20sampling%20procedures%0Ahave%20been%20proposed%20recently%20to%20accelerate%20structure-based%20drug%20design%20%28SBDD%29%3B%0Ahowever%2C%20perplexingly%2C%20their%20empirical%20performance%20turns%20out%20to%20be%20suboptimal.%0AWe%20seek%20to%20better%20understand%20this%20phenomenon%20from%20both%20theoretical%20and%0Aempirical%20perspectives.%20Since%20most%20of%20these%20models%20apply%20graph%20neural%20networks%0A%28GNNs%29%2C%20one%20may%20suspect%20that%20they%20inherit%20the%20representational%20limitations%20of%0AGNNs.%20We%20analyze%20this%20aspect%2C%20establishing%20the%20first%20such%20results%20for%0Aprotein-ligand%20complexes.%20A%20plausible%20counterview%20may%20attribute%20the%0Aunderperformance%20of%20these%20models%20to%20their%20excessive%20parameterizations%2C%20inducing%0Aexpressivity%20at%20the%20expense%20of%20generalization.%20We%20also%20investigate%20this%0Apossibility%20with%20a%20simple%20metric-aware%20approach%20that%20learns%20an%20economical%0Asurrogate%20for%20affinity%20to%20infer%20an%20unlabelled%20molecular%20graph%20and%20optimizes%20for%0Alabels%20conditioned%20on%20this%20graph%20and%20molecular%20properties.%20The%20resulting%20model%0Aachieves%20state-of-the-art%20results%20using%20100x%20fewer%20trainable%20parameters%20and%0Aaffords%20up%20to%201000x%20speedup.%20Collectively%2C%20our%20findings%20underscore%20the%20need%20to%0Areassess%20and%20redirect%20the%20existing%20paradigm%20and%20efforts%20for%20SBDD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06050v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Ails%2520Generative%2520Structure-based%2520Drug%2520Design%253A%2520Too%2520Little%2520or%2520Too%2520Much%250A%2520%2520Expressivity%253F%26entry.906535625%3DRafa%25C5%2582%2520Karczewski%2520and%2520Samuel%2520Kaski%2520and%2520Markus%2520Heinonen%2520and%2520Vikas%2520Garg%26entry.1292438233%3D%2520%2520Several%2520generative%2520models%2520with%2520elaborate%2520training%2520and%2520sampling%2520procedures%250Ahave%2520been%2520proposed%2520recently%2520to%2520accelerate%2520structure-based%2520drug%2520design%2520%2528SBDD%2529%253B%250Ahowever%252C%2520perplexingly%252C%2520their%2520empirical%2520performance%2520turns%2520out%2520to%2520be%2520suboptimal.%250AWe%2520seek%2520to%2520better%2520understand%2520this%2520phenomenon%2520from%2520both%2520theoretical%2520and%250Aempirical%2520perspectives.%2520Since%2520most%2520of%2520these%2520models%2520apply%2520graph%2520neural%2520networks%250A%2528GNNs%2529%252C%2520one%2520may%2520suspect%2520that%2520they%2520inherit%2520the%2520representational%2520limitations%2520of%250AGNNs.%2520We%2520analyze%2520this%2520aspect%252C%2520establishing%2520the%2520first%2520such%2520results%2520for%250Aprotein-ligand%2520complexes.%2520A%2520plausible%2520counterview%2520may%2520attribute%2520the%250Aunderperformance%2520of%2520these%2520models%2520to%2520their%2520excessive%2520parameterizations%252C%2520inducing%250Aexpressivity%2520at%2520the%2520expense%2520of%2520generalization.%2520We%2520also%2520investigate%2520this%250Apossibility%2520with%2520a%2520simple%2520metric-aware%2520approach%2520that%2520learns%2520an%2520economical%250Asurrogate%2520for%2520affinity%2520to%2520infer%2520an%2520unlabelled%2520molecular%2520graph%2520and%2520optimizes%2520for%250Alabels%2520conditioned%2520on%2520this%2520graph%2520and%2520molecular%2520properties.%2520The%2520resulting%2520model%250Aachieves%2520state-of-the-art%2520results%2520using%2520100x%2520fewer%2520trainable%2520parameters%2520and%250Aaffords%2520up%2520to%25201000x%2520speedup.%2520Collectively%252C%2520our%2520findings%2520underscore%2520the%2520need%2520to%250Areassess%2520and%2520redirect%2520the%2520existing%2520paradigm%2520and%2520efforts%2520for%2520SBDD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06050v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Ails%20Generative%20Structure-based%20Drug%20Design%3A%20Too%20Little%20or%20Too%20Much%0A%20%20Expressivity%3F&entry.906535625=Rafa%C5%82%20Karczewski%20and%20Samuel%20Kaski%20and%20Markus%20Heinonen%20and%20Vikas%20Garg&entry.1292438233=%20%20Several%20generative%20models%20with%20elaborate%20training%20and%20sampling%20procedures%0Ahave%20been%20proposed%20recently%20to%20accelerate%20structure-based%20drug%20design%20%28SBDD%29%3B%0Ahowever%2C%20perplexingly%2C%20their%20empirical%20performance%20turns%20out%20to%20be%20suboptimal.%0AWe%20seek%20to%20better%20understand%20this%20phenomenon%20from%20both%20theoretical%20and%0Aempirical%20perspectives.%20Since%20most%20of%20these%20models%20apply%20graph%20neural%20networks%0A%28GNNs%29%2C%20one%20may%20suspect%20that%20they%20inherit%20the%20representational%20limitations%20of%0AGNNs.%20We%20analyze%20this%20aspect%2C%20establishing%20the%20first%20such%20results%20for%0Aprotein-ligand%20complexes.%20A%20plausible%20counterview%20may%20attribute%20the%0Aunderperformance%20of%20these%20models%20to%20their%20excessive%20parameterizations%2C%20inducing%0Aexpressivity%20at%20the%20expense%20of%20generalization.%20We%20also%20investigate%20this%0Apossibility%20with%20a%20simple%20metric-aware%20approach%20that%20learns%20an%20economical%0Asurrogate%20for%20affinity%20to%20infer%20an%20unlabelled%20molecular%20graph%20and%20optimizes%20for%0Alabels%20conditioned%20on%20this%20graph%20and%20molecular%20properties.%20The%20resulting%20model%0Aachieves%20state-of-the-art%20results%20using%20100x%20fewer%20trainable%20parameters%20and%0Aaffords%20up%20to%201000x%20speedup.%20Collectively%2C%20our%20findings%20underscore%20the%20need%20to%0Areassess%20and%20redirect%20the%20existing%20paradigm%20and%20efforts%20for%20SBDD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06050v1&entry.124074799=Read"},
{"title": "Efficient and Scalable Point Cloud Generation with Sparse Point-Voxel\n  Diffusion Models", "author": "Ioannis Romanelis and Vlassios Fotis and Athanasios Kalogeras and Christos Alexakos and Konstantinos Moustakas and Adrian Munteanu", "abstract": "  We propose a novel point cloud U-Net diffusion architecture for 3D generative\nmodeling capable of generating high-quality and diverse 3D shapes while\nmaintaining fast generation times. Our network employs a dual-branch\narchitecture, combining the high-resolution representations of points with the\ncomputational efficiency of sparse voxels. Our fastest variant outperforms all\nnon-diffusion generative approaches on unconditional shape generation, the most\npopular benchmark for evaluating point cloud generative models, while our\nlargest model achieves state-of-the-art results among diffusion methods, with a\nruntime approximately 70% of the previously state-of-the-art PVD. Beyond\nunconditional generation, we perform extensive evaluations, including\nconditional generation on all categories of ShapeNet, demonstrating the\nscalability of our model to larger datasets, and implicit generation which\nallows our network to produce high quality point clouds on fewer timesteps,\nfurther decreasing the generation time. Finally, we evaluate the architecture's\nperformance in point cloud completion and super-resolution. Our model excels in\nall tasks, establishing it as a state-of-the-art diffusion U-Net for point\ncloud generative modeling. The code is publicly available at\nhttps://github.com/JohnRomanelis/SPVD.git.\n", "link": "http://arxiv.org/abs/2408.06145v1", "date": "2024-08-12", "relevancy": 2.4426, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6261}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6076}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6076}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20and%20Scalable%20Point%20Cloud%20Generation%20with%20Sparse%20Point-Voxel%0A%20%20Diffusion%20Models&body=Title%3A%20Efficient%20and%20Scalable%20Point%20Cloud%20Generation%20with%20Sparse%20Point-Voxel%0A%20%20Diffusion%20Models%0AAuthor%3A%20Ioannis%20Romanelis%20and%20Vlassios%20Fotis%20and%20Athanasios%20Kalogeras%20and%20Christos%20Alexakos%20and%20Konstantinos%20Moustakas%20and%20Adrian%20Munteanu%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20point%20cloud%20U-Net%20diffusion%20architecture%20for%203D%20generative%0Amodeling%20capable%20of%20generating%20high-quality%20and%20diverse%203D%20shapes%20while%0Amaintaining%20fast%20generation%20times.%20Our%20network%20employs%20a%20dual-branch%0Aarchitecture%2C%20combining%20the%20high-resolution%20representations%20of%20points%20with%20the%0Acomputational%20efficiency%20of%20sparse%20voxels.%20Our%20fastest%20variant%20outperforms%20all%0Anon-diffusion%20generative%20approaches%20on%20unconditional%20shape%20generation%2C%20the%20most%0Apopular%20benchmark%20for%20evaluating%20point%20cloud%20generative%20models%2C%20while%20our%0Alargest%20model%20achieves%20state-of-the-art%20results%20among%20diffusion%20methods%2C%20with%20a%0Aruntime%20approximately%2070%25%20of%20the%20previously%20state-of-the-art%20PVD.%20Beyond%0Aunconditional%20generation%2C%20we%20perform%20extensive%20evaluations%2C%20including%0Aconditional%20generation%20on%20all%20categories%20of%20ShapeNet%2C%20demonstrating%20the%0Ascalability%20of%20our%20model%20to%20larger%20datasets%2C%20and%20implicit%20generation%20which%0Aallows%20our%20network%20to%20produce%20high%20quality%20point%20clouds%20on%20fewer%20timesteps%2C%0Afurther%20decreasing%20the%20generation%20time.%20Finally%2C%20we%20evaluate%20the%20architecture%27s%0Aperformance%20in%20point%20cloud%20completion%20and%20super-resolution.%20Our%20model%20excels%20in%0Aall%20tasks%2C%20establishing%20it%20as%20a%20state-of-the-art%20diffusion%20U-Net%20for%20point%0Acloud%20generative%20modeling.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/JohnRomanelis/SPVD.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06145v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520and%2520Scalable%2520Point%2520Cloud%2520Generation%2520with%2520Sparse%2520Point-Voxel%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DIoannis%2520Romanelis%2520and%2520Vlassios%2520Fotis%2520and%2520Athanasios%2520Kalogeras%2520and%2520Christos%2520Alexakos%2520and%2520Konstantinos%2520Moustakas%2520and%2520Adrian%2520Munteanu%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520point%2520cloud%2520U-Net%2520diffusion%2520architecture%2520for%25203D%2520generative%250Amodeling%2520capable%2520of%2520generating%2520high-quality%2520and%2520diverse%25203D%2520shapes%2520while%250Amaintaining%2520fast%2520generation%2520times.%2520Our%2520network%2520employs%2520a%2520dual-branch%250Aarchitecture%252C%2520combining%2520the%2520high-resolution%2520representations%2520of%2520points%2520with%2520the%250Acomputational%2520efficiency%2520of%2520sparse%2520voxels.%2520Our%2520fastest%2520variant%2520outperforms%2520all%250Anon-diffusion%2520generative%2520approaches%2520on%2520unconditional%2520shape%2520generation%252C%2520the%2520most%250Apopular%2520benchmark%2520for%2520evaluating%2520point%2520cloud%2520generative%2520models%252C%2520while%2520our%250Alargest%2520model%2520achieves%2520state-of-the-art%2520results%2520among%2520diffusion%2520methods%252C%2520with%2520a%250Aruntime%2520approximately%252070%2525%2520of%2520the%2520previously%2520state-of-the-art%2520PVD.%2520Beyond%250Aunconditional%2520generation%252C%2520we%2520perform%2520extensive%2520evaluations%252C%2520including%250Aconditional%2520generation%2520on%2520all%2520categories%2520of%2520ShapeNet%252C%2520demonstrating%2520the%250Ascalability%2520of%2520our%2520model%2520to%2520larger%2520datasets%252C%2520and%2520implicit%2520generation%2520which%250Aallows%2520our%2520network%2520to%2520produce%2520high%2520quality%2520point%2520clouds%2520on%2520fewer%2520timesteps%252C%250Afurther%2520decreasing%2520the%2520generation%2520time.%2520Finally%252C%2520we%2520evaluate%2520the%2520architecture%2527s%250Aperformance%2520in%2520point%2520cloud%2520completion%2520and%2520super-resolution.%2520Our%2520model%2520excels%2520in%250Aall%2520tasks%252C%2520establishing%2520it%2520as%2520a%2520state-of-the-art%2520diffusion%2520U-Net%2520for%2520point%250Acloud%2520generative%2520modeling.%2520The%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/JohnRomanelis/SPVD.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06145v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20and%20Scalable%20Point%20Cloud%20Generation%20with%20Sparse%20Point-Voxel%0A%20%20Diffusion%20Models&entry.906535625=Ioannis%20Romanelis%20and%20Vlassios%20Fotis%20and%20Athanasios%20Kalogeras%20and%20Christos%20Alexakos%20and%20Konstantinos%20Moustakas%20and%20Adrian%20Munteanu&entry.1292438233=%20%20We%20propose%20a%20novel%20point%20cloud%20U-Net%20diffusion%20architecture%20for%203D%20generative%0Amodeling%20capable%20of%20generating%20high-quality%20and%20diverse%203D%20shapes%20while%0Amaintaining%20fast%20generation%20times.%20Our%20network%20employs%20a%20dual-branch%0Aarchitecture%2C%20combining%20the%20high-resolution%20representations%20of%20points%20with%20the%0Acomputational%20efficiency%20of%20sparse%20voxels.%20Our%20fastest%20variant%20outperforms%20all%0Anon-diffusion%20generative%20approaches%20on%20unconditional%20shape%20generation%2C%20the%20most%0Apopular%20benchmark%20for%20evaluating%20point%20cloud%20generative%20models%2C%20while%20our%0Alargest%20model%20achieves%20state-of-the-art%20results%20among%20diffusion%20methods%2C%20with%20a%0Aruntime%20approximately%2070%25%20of%20the%20previously%20state-of-the-art%20PVD.%20Beyond%0Aunconditional%20generation%2C%20we%20perform%20extensive%20evaluations%2C%20including%0Aconditional%20generation%20on%20all%20categories%20of%20ShapeNet%2C%20demonstrating%20the%0Ascalability%20of%20our%20model%20to%20larger%20datasets%2C%20and%20implicit%20generation%20which%0Aallows%20our%20network%20to%20produce%20high%20quality%20point%20clouds%20on%20fewer%20timesteps%2C%0Afurther%20decreasing%20the%20generation%20time.%20Finally%2C%20we%20evaluate%20the%20architecture%27s%0Aperformance%20in%20point%20cloud%20completion%20and%20super-resolution.%20Our%20model%20excels%20in%0Aall%20tasks%2C%20establishing%20it%20as%20a%20state-of-the-art%20diffusion%20U-Net%20for%20point%0Acloud%20generative%20modeling.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/JohnRomanelis/SPVD.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06145v1&entry.124074799=Read"},
{"title": "BooW-VTON: Boosting In-the-Wild Virtual Try-On via Mask-Free Pseudo Data\n  Training", "author": "Xuanpu Zhang and Dan Song and Pengxin Zhan and Qingguo Chen and Zhao Xu and Weihua Luo and Kaifu Zhang and Anan Liu", "abstract": "  Image-based virtual try-on is an increasingly popular and important task to\ngenerate realistic try-on images of specific person. Existing methods always\nemploy an accurate mask to remove the original garment in the source image,\nthus achieving realistic synthesized images in simple and conventional try-on\nscenarios based on powerful diffusion model. Therefore, acquiring suitable mask\nis vital to the try-on performance of these methods. However, obtaining precise\ninpainting masks, especially for complex wild try-on data containing diverse\nforeground occlusions and person poses, is not easy as Figure 1-Top shows. This\ndifficulty often results in poor performance in more practical and challenging\nreal-life scenarios, such as the selfie scene shown in Figure 1-Bottom. To this\nend, we propose a novel training paradigm combined with an efficient data\naugmentation method to acquire large-scale unpaired training data from wild\nscenarios, thereby significantly facilitating the try-on performance of our\nmodel without the need for additional inpainting masks. Besides, a try-on\nlocalization loss is designed to localize a more accurate try-on area to obtain\nmore reasonable try-on results. It is noted that our method only needs the\nreference cloth image, source pose image and source person image as input,\nwhich is more cost-effective and user-friendly compared to existing methods.\nExtensive qualitative and quantitative experiments have demonstrated superior\nperformance in wild scenarios with such a low-demand input.\n", "link": "http://arxiv.org/abs/2408.06047v1", "date": "2024-08-12", "relevancy": 2.4389, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6409}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6066}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5799}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BooW-VTON%3A%20Boosting%20In-the-Wild%20Virtual%20Try-On%20via%20Mask-Free%20Pseudo%20Data%0A%20%20Training&body=Title%3A%20BooW-VTON%3A%20Boosting%20In-the-Wild%20Virtual%20Try-On%20via%20Mask-Free%20Pseudo%20Data%0A%20%20Training%0AAuthor%3A%20Xuanpu%20Zhang%20and%20Dan%20Song%20and%20Pengxin%20Zhan%20and%20Qingguo%20Chen%20and%20Zhao%20Xu%20and%20Weihua%20Luo%20and%20Kaifu%20Zhang%20and%20Anan%20Liu%0AAbstract%3A%20%20%20Image-based%20virtual%20try-on%20is%20an%20increasingly%20popular%20and%20important%20task%20to%0Agenerate%20realistic%20try-on%20images%20of%20specific%20person.%20Existing%20methods%20always%0Aemploy%20an%20accurate%20mask%20to%20remove%20the%20original%20garment%20in%20the%20source%20image%2C%0Athus%20achieving%20realistic%20synthesized%20images%20in%20simple%20and%20conventional%20try-on%0Ascenarios%20based%20on%20powerful%20diffusion%20model.%20Therefore%2C%20acquiring%20suitable%20mask%0Ais%20vital%20to%20the%20try-on%20performance%20of%20these%20methods.%20However%2C%20obtaining%20precise%0Ainpainting%20masks%2C%20especially%20for%20complex%20wild%20try-on%20data%20containing%20diverse%0Aforeground%20occlusions%20and%20person%20poses%2C%20is%20not%20easy%20as%20Figure%201-Top%20shows.%20This%0Adifficulty%20often%20results%20in%20poor%20performance%20in%20more%20practical%20and%20challenging%0Areal-life%20scenarios%2C%20such%20as%20the%20selfie%20scene%20shown%20in%20Figure%201-Bottom.%20To%20this%0Aend%2C%20we%20propose%20a%20novel%20training%20paradigm%20combined%20with%20an%20efficient%20data%0Aaugmentation%20method%20to%20acquire%20large-scale%20unpaired%20training%20data%20from%20wild%0Ascenarios%2C%20thereby%20significantly%20facilitating%20the%20try-on%20performance%20of%20our%0Amodel%20without%20the%20need%20for%20additional%20inpainting%20masks.%20Besides%2C%20a%20try-on%0Alocalization%20loss%20is%20designed%20to%20localize%20a%20more%20accurate%20try-on%20area%20to%20obtain%0Amore%20reasonable%20try-on%20results.%20It%20is%20noted%20that%20our%20method%20only%20needs%20the%0Areference%20cloth%20image%2C%20source%20pose%20image%20and%20source%20person%20image%20as%20input%2C%0Awhich%20is%20more%20cost-effective%20and%20user-friendly%20compared%20to%20existing%20methods.%0AExtensive%20qualitative%20and%20quantitative%20experiments%20have%20demonstrated%20superior%0Aperformance%20in%20wild%20scenarios%20with%20such%20a%20low-demand%20input.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06047v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBooW-VTON%253A%2520Boosting%2520In-the-Wild%2520Virtual%2520Try-On%2520via%2520Mask-Free%2520Pseudo%2520Data%250A%2520%2520Training%26entry.906535625%3DXuanpu%2520Zhang%2520and%2520Dan%2520Song%2520and%2520Pengxin%2520Zhan%2520and%2520Qingguo%2520Chen%2520and%2520Zhao%2520Xu%2520and%2520Weihua%2520Luo%2520and%2520Kaifu%2520Zhang%2520and%2520Anan%2520Liu%26entry.1292438233%3D%2520%2520Image-based%2520virtual%2520try-on%2520is%2520an%2520increasingly%2520popular%2520and%2520important%2520task%2520to%250Agenerate%2520realistic%2520try-on%2520images%2520of%2520specific%2520person.%2520Existing%2520methods%2520always%250Aemploy%2520an%2520accurate%2520mask%2520to%2520remove%2520the%2520original%2520garment%2520in%2520the%2520source%2520image%252C%250Athus%2520achieving%2520realistic%2520synthesized%2520images%2520in%2520simple%2520and%2520conventional%2520try-on%250Ascenarios%2520based%2520on%2520powerful%2520diffusion%2520model.%2520Therefore%252C%2520acquiring%2520suitable%2520mask%250Ais%2520vital%2520to%2520the%2520try-on%2520performance%2520of%2520these%2520methods.%2520However%252C%2520obtaining%2520precise%250Ainpainting%2520masks%252C%2520especially%2520for%2520complex%2520wild%2520try-on%2520data%2520containing%2520diverse%250Aforeground%2520occlusions%2520and%2520person%2520poses%252C%2520is%2520not%2520easy%2520as%2520Figure%25201-Top%2520shows.%2520This%250Adifficulty%2520often%2520results%2520in%2520poor%2520performance%2520in%2520more%2520practical%2520and%2520challenging%250Areal-life%2520scenarios%252C%2520such%2520as%2520the%2520selfie%2520scene%2520shown%2520in%2520Figure%25201-Bottom.%2520To%2520this%250Aend%252C%2520we%2520propose%2520a%2520novel%2520training%2520paradigm%2520combined%2520with%2520an%2520efficient%2520data%250Aaugmentation%2520method%2520to%2520acquire%2520large-scale%2520unpaired%2520training%2520data%2520from%2520wild%250Ascenarios%252C%2520thereby%2520significantly%2520facilitating%2520the%2520try-on%2520performance%2520of%2520our%250Amodel%2520without%2520the%2520need%2520for%2520additional%2520inpainting%2520masks.%2520Besides%252C%2520a%2520try-on%250Alocalization%2520loss%2520is%2520designed%2520to%2520localize%2520a%2520more%2520accurate%2520try-on%2520area%2520to%2520obtain%250Amore%2520reasonable%2520try-on%2520results.%2520It%2520is%2520noted%2520that%2520our%2520method%2520only%2520needs%2520the%250Areference%2520cloth%2520image%252C%2520source%2520pose%2520image%2520and%2520source%2520person%2520image%2520as%2520input%252C%250Awhich%2520is%2520more%2520cost-effective%2520and%2520user-friendly%2520compared%2520to%2520existing%2520methods.%250AExtensive%2520qualitative%2520and%2520quantitative%2520experiments%2520have%2520demonstrated%2520superior%250Aperformance%2520in%2520wild%2520scenarios%2520with%2520such%2520a%2520low-demand%2520input.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06047v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BooW-VTON%3A%20Boosting%20In-the-Wild%20Virtual%20Try-On%20via%20Mask-Free%20Pseudo%20Data%0A%20%20Training&entry.906535625=Xuanpu%20Zhang%20and%20Dan%20Song%20and%20Pengxin%20Zhan%20and%20Qingguo%20Chen%20and%20Zhao%20Xu%20and%20Weihua%20Luo%20and%20Kaifu%20Zhang%20and%20Anan%20Liu&entry.1292438233=%20%20Image-based%20virtual%20try-on%20is%20an%20increasingly%20popular%20and%20important%20task%20to%0Agenerate%20realistic%20try-on%20images%20of%20specific%20person.%20Existing%20methods%20always%0Aemploy%20an%20accurate%20mask%20to%20remove%20the%20original%20garment%20in%20the%20source%20image%2C%0Athus%20achieving%20realistic%20synthesized%20images%20in%20simple%20and%20conventional%20try-on%0Ascenarios%20based%20on%20powerful%20diffusion%20model.%20Therefore%2C%20acquiring%20suitable%20mask%0Ais%20vital%20to%20the%20try-on%20performance%20of%20these%20methods.%20However%2C%20obtaining%20precise%0Ainpainting%20masks%2C%20especially%20for%20complex%20wild%20try-on%20data%20containing%20diverse%0Aforeground%20occlusions%20and%20person%20poses%2C%20is%20not%20easy%20as%20Figure%201-Top%20shows.%20This%0Adifficulty%20often%20results%20in%20poor%20performance%20in%20more%20practical%20and%20challenging%0Areal-life%20scenarios%2C%20such%20as%20the%20selfie%20scene%20shown%20in%20Figure%201-Bottom.%20To%20this%0Aend%2C%20we%20propose%20a%20novel%20training%20paradigm%20combined%20with%20an%20efficient%20data%0Aaugmentation%20method%20to%20acquire%20large-scale%20unpaired%20training%20data%20from%20wild%0Ascenarios%2C%20thereby%20significantly%20facilitating%20the%20try-on%20performance%20of%20our%0Amodel%20without%20the%20need%20for%20additional%20inpainting%20masks.%20Besides%2C%20a%20try-on%0Alocalization%20loss%20is%20designed%20to%20localize%20a%20more%20accurate%20try-on%20area%20to%20obtain%0Amore%20reasonable%20try-on%20results.%20It%20is%20noted%20that%20our%20method%20only%20needs%20the%0Areference%20cloth%20image%2C%20source%20pose%20image%20and%20source%20person%20image%20as%20input%2C%0Awhich%20is%20more%20cost-effective%20and%20user-friendly%20compared%20to%20existing%20methods.%0AExtensive%20qualitative%20and%20quantitative%20experiments%20have%20demonstrated%20superior%0Aperformance%20in%20wild%20scenarios%20with%20such%20a%20low-demand%20input.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06047v1&entry.124074799=Read"},
{"title": "OmniCLIP: Adapting CLIP for Video Recognition with Spatial-Temporal\n  Omni-Scale Feature Learning", "author": "Mushui Liu and Bozheng Li and Yunlong Yu", "abstract": "  Recent Vision-Language Models (VLMs) \\textit{e.g.} CLIP have made great\nprogress in video recognition. Despite the improvement brought by the strong\nvisual backbone in extracting spatial features, CLIP still falls short in\ncapturing and integrating spatial-temporal features which is essential for\nvideo recognition. In this paper, we propose OmniCLIP, a framework that adapts\nCLIP for video recognition by focusing on learning comprehensive features\nencompassing spatial, temporal, and dynamic spatial-temporal scales, which we\nrefer to as omni-scale features. This is achieved through the design of\nspatial-temporal blocks that include parallel temporal adapters (PTA), enabling\nefficient temporal modeling. Additionally, we introduce a self-prompt generator\n(SPG) module to capture dynamic object spatial features. The synergy between\nPTA and SPG allows OmniCLIP to discern varying spatial information across\nframes and assess object scales over time. We have conducted extensive\nexperiments in supervised video recognition, few-shot video recognition, and\nzero-shot recognition tasks. The results demonstrate the effectiveness of our\nmethod, especially with OmniCLIP achieving a top-1 accuracy of 74.30\\% on\nHMDB51 in a 16-shot setting, surpassing the recent MotionPrompt approach even\nwith full training data. The code is available at\n\\url{https://github.com/XiaoBuL/OmniCLIP}.\n", "link": "http://arxiv.org/abs/2408.06158v1", "date": "2024-08-12", "relevancy": 2.4079, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6395}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5895}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5695}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniCLIP%3A%20Adapting%20CLIP%20for%20Video%20Recognition%20with%20Spatial-Temporal%0A%20%20Omni-Scale%20Feature%20Learning&body=Title%3A%20OmniCLIP%3A%20Adapting%20CLIP%20for%20Video%20Recognition%20with%20Spatial-Temporal%0A%20%20Omni-Scale%20Feature%20Learning%0AAuthor%3A%20Mushui%20Liu%20and%20Bozheng%20Li%20and%20Yunlong%20Yu%0AAbstract%3A%20%20%20Recent%20Vision-Language%20Models%20%28VLMs%29%20%5Ctextit%7Be.g.%7D%20CLIP%20have%20made%20great%0Aprogress%20in%20video%20recognition.%20Despite%20the%20improvement%20brought%20by%20the%20strong%0Avisual%20backbone%20in%20extracting%20spatial%20features%2C%20CLIP%20still%20falls%20short%20in%0Acapturing%20and%20integrating%20spatial-temporal%20features%20which%20is%20essential%20for%0Avideo%20recognition.%20In%20this%20paper%2C%20we%20propose%20OmniCLIP%2C%20a%20framework%20that%20adapts%0ACLIP%20for%20video%20recognition%20by%20focusing%20on%20learning%20comprehensive%20features%0Aencompassing%20spatial%2C%20temporal%2C%20and%20dynamic%20spatial-temporal%20scales%2C%20which%20we%0Arefer%20to%20as%20omni-scale%20features.%20This%20is%20achieved%20through%20the%20design%20of%0Aspatial-temporal%20blocks%20that%20include%20parallel%20temporal%20adapters%20%28PTA%29%2C%20enabling%0Aefficient%20temporal%20modeling.%20Additionally%2C%20we%20introduce%20a%20self-prompt%20generator%0A%28SPG%29%20module%20to%20capture%20dynamic%20object%20spatial%20features.%20The%20synergy%20between%0APTA%20and%20SPG%20allows%20OmniCLIP%20to%20discern%20varying%20spatial%20information%20across%0Aframes%20and%20assess%20object%20scales%20over%20time.%20We%20have%20conducted%20extensive%0Aexperiments%20in%20supervised%20video%20recognition%2C%20few-shot%20video%20recognition%2C%20and%0Azero-shot%20recognition%20tasks.%20The%20results%20demonstrate%20the%20effectiveness%20of%20our%0Amethod%2C%20especially%20with%20OmniCLIP%20achieving%20a%20top-1%20accuracy%20of%2074.30%5C%25%20on%0AHMDB51%20in%20a%2016-shot%20setting%2C%20surpassing%20the%20recent%20MotionPrompt%20approach%20even%0Awith%20full%20training%20data.%20The%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/XiaoBuL/OmniCLIP%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06158v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniCLIP%253A%2520Adapting%2520CLIP%2520for%2520Video%2520Recognition%2520with%2520Spatial-Temporal%250A%2520%2520Omni-Scale%2520Feature%2520Learning%26entry.906535625%3DMushui%2520Liu%2520and%2520Bozheng%2520Li%2520and%2520Yunlong%2520Yu%26entry.1292438233%3D%2520%2520Recent%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520%255Ctextit%257Be.g.%257D%2520CLIP%2520have%2520made%2520great%250Aprogress%2520in%2520video%2520recognition.%2520Despite%2520the%2520improvement%2520brought%2520by%2520the%2520strong%250Avisual%2520backbone%2520in%2520extracting%2520spatial%2520features%252C%2520CLIP%2520still%2520falls%2520short%2520in%250Acapturing%2520and%2520integrating%2520spatial-temporal%2520features%2520which%2520is%2520essential%2520for%250Avideo%2520recognition.%2520In%2520this%2520paper%252C%2520we%2520propose%2520OmniCLIP%252C%2520a%2520framework%2520that%2520adapts%250ACLIP%2520for%2520video%2520recognition%2520by%2520focusing%2520on%2520learning%2520comprehensive%2520features%250Aencompassing%2520spatial%252C%2520temporal%252C%2520and%2520dynamic%2520spatial-temporal%2520scales%252C%2520which%2520we%250Arefer%2520to%2520as%2520omni-scale%2520features.%2520This%2520is%2520achieved%2520through%2520the%2520design%2520of%250Aspatial-temporal%2520blocks%2520that%2520include%2520parallel%2520temporal%2520adapters%2520%2528PTA%2529%252C%2520enabling%250Aefficient%2520temporal%2520modeling.%2520Additionally%252C%2520we%2520introduce%2520a%2520self-prompt%2520generator%250A%2528SPG%2529%2520module%2520to%2520capture%2520dynamic%2520object%2520spatial%2520features.%2520The%2520synergy%2520between%250APTA%2520and%2520SPG%2520allows%2520OmniCLIP%2520to%2520discern%2520varying%2520spatial%2520information%2520across%250Aframes%2520and%2520assess%2520object%2520scales%2520over%2520time.%2520We%2520have%2520conducted%2520extensive%250Aexperiments%2520in%2520supervised%2520video%2520recognition%252C%2520few-shot%2520video%2520recognition%252C%2520and%250Azero-shot%2520recognition%2520tasks.%2520The%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Amethod%252C%2520especially%2520with%2520OmniCLIP%2520achieving%2520a%2520top-1%2520accuracy%2520of%252074.30%255C%2525%2520on%250AHMDB51%2520in%2520a%252016-shot%2520setting%252C%2520surpassing%2520the%2520recent%2520MotionPrompt%2520approach%2520even%250Awith%2520full%2520training%2520data.%2520The%2520code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/XiaoBuL/OmniCLIP%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06158v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniCLIP%3A%20Adapting%20CLIP%20for%20Video%20Recognition%20with%20Spatial-Temporal%0A%20%20Omni-Scale%20Feature%20Learning&entry.906535625=Mushui%20Liu%20and%20Bozheng%20Li%20and%20Yunlong%20Yu&entry.1292438233=%20%20Recent%20Vision-Language%20Models%20%28VLMs%29%20%5Ctextit%7Be.g.%7D%20CLIP%20have%20made%20great%0Aprogress%20in%20video%20recognition.%20Despite%20the%20improvement%20brought%20by%20the%20strong%0Avisual%20backbone%20in%20extracting%20spatial%20features%2C%20CLIP%20still%20falls%20short%20in%0Acapturing%20and%20integrating%20spatial-temporal%20features%20which%20is%20essential%20for%0Avideo%20recognition.%20In%20this%20paper%2C%20we%20propose%20OmniCLIP%2C%20a%20framework%20that%20adapts%0ACLIP%20for%20video%20recognition%20by%20focusing%20on%20learning%20comprehensive%20features%0Aencompassing%20spatial%2C%20temporal%2C%20and%20dynamic%20spatial-temporal%20scales%2C%20which%20we%0Arefer%20to%20as%20omni-scale%20features.%20This%20is%20achieved%20through%20the%20design%20of%0Aspatial-temporal%20blocks%20that%20include%20parallel%20temporal%20adapters%20%28PTA%29%2C%20enabling%0Aefficient%20temporal%20modeling.%20Additionally%2C%20we%20introduce%20a%20self-prompt%20generator%0A%28SPG%29%20module%20to%20capture%20dynamic%20object%20spatial%20features.%20The%20synergy%20between%0APTA%20and%20SPG%20allows%20OmniCLIP%20to%20discern%20varying%20spatial%20information%20across%0Aframes%20and%20assess%20object%20scales%20over%20time.%20We%20have%20conducted%20extensive%0Aexperiments%20in%20supervised%20video%20recognition%2C%20few-shot%20video%20recognition%2C%20and%0Azero-shot%20recognition%20tasks.%20The%20results%20demonstrate%20the%20effectiveness%20of%20our%0Amethod%2C%20especially%20with%20OmniCLIP%20achieving%20a%20top-1%20accuracy%20of%2074.30%5C%25%20on%0AHMDB51%20in%20a%2016-shot%20setting%2C%20surpassing%20the%20recent%20MotionPrompt%20approach%20even%0Awith%20full%20training%20data.%20The%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/XiaoBuL/OmniCLIP%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06158v1&entry.124074799=Read"},
{"title": "LEARN: An Invex Loss for Outlier Oblivious Robust Online Optimization", "author": "Adarsh Barik and Anand Krishna and Vincent Y. F. Tan", "abstract": "  We study a robust online convex optimization framework, where an adversary\ncan introduce outliers by corrupting loss functions in an arbitrary number of\nrounds k, unknown to the learner. Our focus is on a novel setting allowing\nunbounded domains and large gradients for the losses without relying on a\nLipschitz assumption. We introduce the Log Exponential Adjusted Robust and\niNvex (LEARN) loss, a non-convex (invex) robust loss function to mitigate the\neffects of outliers and develop a robust variant of the online gradient descent\nalgorithm by leveraging the LEARN loss. We establish tight regret guarantees\n(up to constants), in a dynamic setting, with respect to the uncorrupted rounds\nand conduct experiments to validate our theory. Furthermore, we present a\nunified analysis framework for developing online optimization algorithms for\nnon-convex (invex) losses, utilizing it to provide regret bounds with respect\nto the LEARN loss, which may be of independent interest.\n", "link": "http://arxiv.org/abs/2408.06297v1", "date": "2024-08-12", "relevancy": 2.3831, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4957}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4736}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LEARN%3A%20An%20Invex%20Loss%20for%20Outlier%20Oblivious%20Robust%20Online%20Optimization&body=Title%3A%20LEARN%3A%20An%20Invex%20Loss%20for%20Outlier%20Oblivious%20Robust%20Online%20Optimization%0AAuthor%3A%20Adarsh%20Barik%20and%20Anand%20Krishna%20and%20Vincent%20Y.%20F.%20Tan%0AAbstract%3A%20%20%20We%20study%20a%20robust%20online%20convex%20optimization%20framework%2C%20where%20an%20adversary%0Acan%20introduce%20outliers%20by%20corrupting%20loss%20functions%20in%20an%20arbitrary%20number%20of%0Arounds%20k%2C%20unknown%20to%20the%20learner.%20Our%20focus%20is%20on%20a%20novel%20setting%20allowing%0Aunbounded%20domains%20and%20large%20gradients%20for%20the%20losses%20without%20relying%20on%20a%0ALipschitz%20assumption.%20We%20introduce%20the%20Log%20Exponential%20Adjusted%20Robust%20and%0AiNvex%20%28LEARN%29%20loss%2C%20a%20non-convex%20%28invex%29%20robust%20loss%20function%20to%20mitigate%20the%0Aeffects%20of%20outliers%20and%20develop%20a%20robust%20variant%20of%20the%20online%20gradient%20descent%0Aalgorithm%20by%20leveraging%20the%20LEARN%20loss.%20We%20establish%20tight%20regret%20guarantees%0A%28up%20to%20constants%29%2C%20in%20a%20dynamic%20setting%2C%20with%20respect%20to%20the%20uncorrupted%20rounds%0Aand%20conduct%20experiments%20to%20validate%20our%20theory.%20Furthermore%2C%20we%20present%20a%0Aunified%20analysis%20framework%20for%20developing%20online%20optimization%20algorithms%20for%0Anon-convex%20%28invex%29%20losses%2C%20utilizing%20it%20to%20provide%20regret%20bounds%20with%20respect%0Ato%20the%20LEARN%20loss%2C%20which%20may%20be%20of%20independent%20interest.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06297v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLEARN%253A%2520An%2520Invex%2520Loss%2520for%2520Outlier%2520Oblivious%2520Robust%2520Online%2520Optimization%26entry.906535625%3DAdarsh%2520Barik%2520and%2520Anand%2520Krishna%2520and%2520Vincent%2520Y.%2520F.%2520Tan%26entry.1292438233%3D%2520%2520We%2520study%2520a%2520robust%2520online%2520convex%2520optimization%2520framework%252C%2520where%2520an%2520adversary%250Acan%2520introduce%2520outliers%2520by%2520corrupting%2520loss%2520functions%2520in%2520an%2520arbitrary%2520number%2520of%250Arounds%2520k%252C%2520unknown%2520to%2520the%2520learner.%2520Our%2520focus%2520is%2520on%2520a%2520novel%2520setting%2520allowing%250Aunbounded%2520domains%2520and%2520large%2520gradients%2520for%2520the%2520losses%2520without%2520relying%2520on%2520a%250ALipschitz%2520assumption.%2520We%2520introduce%2520the%2520Log%2520Exponential%2520Adjusted%2520Robust%2520and%250AiNvex%2520%2528LEARN%2529%2520loss%252C%2520a%2520non-convex%2520%2528invex%2529%2520robust%2520loss%2520function%2520to%2520mitigate%2520the%250Aeffects%2520of%2520outliers%2520and%2520develop%2520a%2520robust%2520variant%2520of%2520the%2520online%2520gradient%2520descent%250Aalgorithm%2520by%2520leveraging%2520the%2520LEARN%2520loss.%2520We%2520establish%2520tight%2520regret%2520guarantees%250A%2528up%2520to%2520constants%2529%252C%2520in%2520a%2520dynamic%2520setting%252C%2520with%2520respect%2520to%2520the%2520uncorrupted%2520rounds%250Aand%2520conduct%2520experiments%2520to%2520validate%2520our%2520theory.%2520Furthermore%252C%2520we%2520present%2520a%250Aunified%2520analysis%2520framework%2520for%2520developing%2520online%2520optimization%2520algorithms%2520for%250Anon-convex%2520%2528invex%2529%2520losses%252C%2520utilizing%2520it%2520to%2520provide%2520regret%2520bounds%2520with%2520respect%250Ato%2520the%2520LEARN%2520loss%252C%2520which%2520may%2520be%2520of%2520independent%2520interest.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06297v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LEARN%3A%20An%20Invex%20Loss%20for%20Outlier%20Oblivious%20Robust%20Online%20Optimization&entry.906535625=Adarsh%20Barik%20and%20Anand%20Krishna%20and%20Vincent%20Y.%20F.%20Tan&entry.1292438233=%20%20We%20study%20a%20robust%20online%20convex%20optimization%20framework%2C%20where%20an%20adversary%0Acan%20introduce%20outliers%20by%20corrupting%20loss%20functions%20in%20an%20arbitrary%20number%20of%0Arounds%20k%2C%20unknown%20to%20the%20learner.%20Our%20focus%20is%20on%20a%20novel%20setting%20allowing%0Aunbounded%20domains%20and%20large%20gradients%20for%20the%20losses%20without%20relying%20on%20a%0ALipschitz%20assumption.%20We%20introduce%20the%20Log%20Exponential%20Adjusted%20Robust%20and%0AiNvex%20%28LEARN%29%20loss%2C%20a%20non-convex%20%28invex%29%20robust%20loss%20function%20to%20mitigate%20the%0Aeffects%20of%20outliers%20and%20develop%20a%20robust%20variant%20of%20the%20online%20gradient%20descent%0Aalgorithm%20by%20leveraging%20the%20LEARN%20loss.%20We%20establish%20tight%20regret%20guarantees%0A%28up%20to%20constants%29%2C%20in%20a%20dynamic%20setting%2C%20with%20respect%20to%20the%20uncorrupted%20rounds%0Aand%20conduct%20experiments%20to%20validate%20our%20theory.%20Furthermore%2C%20we%20present%20a%0Aunified%20analysis%20framework%20for%20developing%20online%20optimization%20algorithms%20for%0Anon-convex%20%28invex%29%20losses%2C%20utilizing%20it%20to%20provide%20regret%20bounds%20with%20respect%0Ato%20the%20LEARN%20loss%2C%20which%20may%20be%20of%20independent%20interest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06297v1&entry.124074799=Read"},
{"title": "On the Generalization of Preference Learning with DPO", "author": "Shawn Im and Yixuan Li", "abstract": "  Large language models (LLMs) have demonstrated remarkable capabilities but\noften struggle to align with human preferences, leading to harmful or\nundesirable outputs. Preference learning, which trains models to distinguish\nbetween preferred and non-preferred responses based on human feedback, has\nbecome a crucial component for ensuring that LLMs align with human values.\nDespite the widespread adoption in real-world systems, a thorough theoretical\nunderstanding of the generalization guarantees for these models remain lacking.\nThis paper bridges that gap by introducing a new theoretical framework to\nanalyze the generalization guarantees of models trained with direct preference\noptimization (DPO). While existing generalization theory often focuses on\noverparameterized models achieving near-optimal loss or models independent of\nthe training process, our framework rigorously assesses how well models\ngeneralize after a finite number of gradient steps, reflecting real-world LLM\ntraining practices. By analyzing the reward margin associated with each sample\nand its trajectory throughout training, we can effectively bound the\ngeneralization error. We derive learning guarantees showing that, under\nspecific conditions, models trained with DPO can correctly discern preferred\nresponses on unseen data with high probability. These insights are empirically\nvalidated on contemporary LLMs, underscoring the practical relevance of our\ntheoretical findings.\n", "link": "http://arxiv.org/abs/2408.03459v2", "date": "2024-08-12", "relevancy": 2.352, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4829}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4654}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4628}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Generalization%20of%20Preference%20Learning%20with%20DPO&body=Title%3A%20On%20the%20Generalization%20of%20Preference%20Learning%20with%20DPO%0AAuthor%3A%20Shawn%20Im%20and%20Yixuan%20Li%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20but%0Aoften%20struggle%20to%20align%20with%20human%20preferences%2C%20leading%20to%20harmful%20or%0Aundesirable%20outputs.%20Preference%20learning%2C%20which%20trains%20models%20to%20distinguish%0Abetween%20preferred%20and%20non-preferred%20responses%20based%20on%20human%20feedback%2C%20has%0Abecome%20a%20crucial%20component%20for%20ensuring%20that%20LLMs%20align%20with%20human%20values.%0ADespite%20the%20widespread%20adoption%20in%20real-world%20systems%2C%20a%20thorough%20theoretical%0Aunderstanding%20of%20the%20generalization%20guarantees%20for%20these%20models%20remain%20lacking.%0AThis%20paper%20bridges%20that%20gap%20by%20introducing%20a%20new%20theoretical%20framework%20to%0Aanalyze%20the%20generalization%20guarantees%20of%20models%20trained%20with%20direct%20preference%0Aoptimization%20%28DPO%29.%20While%20existing%20generalization%20theory%20often%20focuses%20on%0Aoverparameterized%20models%20achieving%20near-optimal%20loss%20or%20models%20independent%20of%0Athe%20training%20process%2C%20our%20framework%20rigorously%20assesses%20how%20well%20models%0Ageneralize%20after%20a%20finite%20number%20of%20gradient%20steps%2C%20reflecting%20real-world%20LLM%0Atraining%20practices.%20By%20analyzing%20the%20reward%20margin%20associated%20with%20each%20sample%0Aand%20its%20trajectory%20throughout%20training%2C%20we%20can%20effectively%20bound%20the%0Ageneralization%20error.%20We%20derive%20learning%20guarantees%20showing%20that%2C%20under%0Aspecific%20conditions%2C%20models%20trained%20with%20DPO%20can%20correctly%20discern%20preferred%0Aresponses%20on%20unseen%20data%20with%20high%20probability.%20These%20insights%20are%20empirically%0Avalidated%20on%20contemporary%20LLMs%2C%20underscoring%20the%20practical%20relevance%20of%20our%0Atheoretical%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03459v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Generalization%2520of%2520Preference%2520Learning%2520with%2520DPO%26entry.906535625%3DShawn%2520Im%2520and%2520Yixuan%2520Li%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520capabilities%2520but%250Aoften%2520struggle%2520to%2520align%2520with%2520human%2520preferences%252C%2520leading%2520to%2520harmful%2520or%250Aundesirable%2520outputs.%2520Preference%2520learning%252C%2520which%2520trains%2520models%2520to%2520distinguish%250Abetween%2520preferred%2520and%2520non-preferred%2520responses%2520based%2520on%2520human%2520feedback%252C%2520has%250Abecome%2520a%2520crucial%2520component%2520for%2520ensuring%2520that%2520LLMs%2520align%2520with%2520human%2520values.%250ADespite%2520the%2520widespread%2520adoption%2520in%2520real-world%2520systems%252C%2520a%2520thorough%2520theoretical%250Aunderstanding%2520of%2520the%2520generalization%2520guarantees%2520for%2520these%2520models%2520remain%2520lacking.%250AThis%2520paper%2520bridges%2520that%2520gap%2520by%2520introducing%2520a%2520new%2520theoretical%2520framework%2520to%250Aanalyze%2520the%2520generalization%2520guarantees%2520of%2520models%2520trained%2520with%2520direct%2520preference%250Aoptimization%2520%2528DPO%2529.%2520While%2520existing%2520generalization%2520theory%2520often%2520focuses%2520on%250Aoverparameterized%2520models%2520achieving%2520near-optimal%2520loss%2520or%2520models%2520independent%2520of%250Athe%2520training%2520process%252C%2520our%2520framework%2520rigorously%2520assesses%2520how%2520well%2520models%250Ageneralize%2520after%2520a%2520finite%2520number%2520of%2520gradient%2520steps%252C%2520reflecting%2520real-world%2520LLM%250Atraining%2520practices.%2520By%2520analyzing%2520the%2520reward%2520margin%2520associated%2520with%2520each%2520sample%250Aand%2520its%2520trajectory%2520throughout%2520training%252C%2520we%2520can%2520effectively%2520bound%2520the%250Ageneralization%2520error.%2520We%2520derive%2520learning%2520guarantees%2520showing%2520that%252C%2520under%250Aspecific%2520conditions%252C%2520models%2520trained%2520with%2520DPO%2520can%2520correctly%2520discern%2520preferred%250Aresponses%2520on%2520unseen%2520data%2520with%2520high%2520probability.%2520These%2520insights%2520are%2520empirically%250Avalidated%2520on%2520contemporary%2520LLMs%252C%2520underscoring%2520the%2520practical%2520relevance%2520of%2520our%250Atheoretical%2520findings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03459v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Generalization%20of%20Preference%20Learning%20with%20DPO&entry.906535625=Shawn%20Im%20and%20Yixuan%20Li&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20but%0Aoften%20struggle%20to%20align%20with%20human%20preferences%2C%20leading%20to%20harmful%20or%0Aundesirable%20outputs.%20Preference%20learning%2C%20which%20trains%20models%20to%20distinguish%0Abetween%20preferred%20and%20non-preferred%20responses%20based%20on%20human%20feedback%2C%20has%0Abecome%20a%20crucial%20component%20for%20ensuring%20that%20LLMs%20align%20with%20human%20values.%0ADespite%20the%20widespread%20adoption%20in%20real-world%20systems%2C%20a%20thorough%20theoretical%0Aunderstanding%20of%20the%20generalization%20guarantees%20for%20these%20models%20remain%20lacking.%0AThis%20paper%20bridges%20that%20gap%20by%20introducing%20a%20new%20theoretical%20framework%20to%0Aanalyze%20the%20generalization%20guarantees%20of%20models%20trained%20with%20direct%20preference%0Aoptimization%20%28DPO%29.%20While%20existing%20generalization%20theory%20often%20focuses%20on%0Aoverparameterized%20models%20achieving%20near-optimal%20loss%20or%20models%20independent%20of%0Athe%20training%20process%2C%20our%20framework%20rigorously%20assesses%20how%20well%20models%0Ageneralize%20after%20a%20finite%20number%20of%20gradient%20steps%2C%20reflecting%20real-world%20LLM%0Atraining%20practices.%20By%20analyzing%20the%20reward%20margin%20associated%20with%20each%20sample%0Aand%20its%20trajectory%20throughout%20training%2C%20we%20can%20effectively%20bound%20the%0Ageneralization%20error.%20We%20derive%20learning%20guarantees%20showing%20that%2C%20under%0Aspecific%20conditions%2C%20models%20trained%20with%20DPO%20can%20correctly%20discern%20preferred%0Aresponses%20on%20unseen%20data%20with%20high%20probability.%20These%20insights%20are%20empirically%0Avalidated%20on%20contemporary%20LLMs%2C%20underscoring%20the%20practical%20relevance%20of%20our%0Atheoretical%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03459v2&entry.124074799=Read"},
{"title": "ViscoNet: Bridging and Harmonizing Visual and Textual Conditioning for\n  ControlNet", "author": "Soon Yau Cheong and Armin Mustafa and Andrew Gilbert", "abstract": "  This paper introduces ViscoNet, a novel one-branch-adapter architecture for\nconcurrent spatial and visual conditioning. Our lightweight model requires\ntrainable parameters and dataset size multiple orders of magnitude smaller than\nthe current state-of-the-art IP-Adapter. However, our method successfully\npreserves the generative power of the frozen text-to-image (T2I) backbone.\nNotably, it excels in addressing mode collapse, a pervasive issue previously\noverlooked. Our novel architecture demonstrates outstanding capabilities in\nachieving a harmonious visual-text balance, unlocking unparalleled versatility\nin various human image generation tasks, including pose re-targeting, virtual\ntry-on, stylization, person re-identification, and textile transfer.Demo and\ncode are available from project page https://soon-yau.github.io/visconet/ .\n", "link": "http://arxiv.org/abs/2312.03154v2", "date": "2024-08-12", "relevancy": 2.3517, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6031}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5853}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5845}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViscoNet%3A%20Bridging%20and%20Harmonizing%20Visual%20and%20Textual%20Conditioning%20for%0A%20%20ControlNet&body=Title%3A%20ViscoNet%3A%20Bridging%20and%20Harmonizing%20Visual%20and%20Textual%20Conditioning%20for%0A%20%20ControlNet%0AAuthor%3A%20Soon%20Yau%20Cheong%20and%20Armin%20Mustafa%20and%20Andrew%20Gilbert%0AAbstract%3A%20%20%20This%20paper%20introduces%20ViscoNet%2C%20a%20novel%20one-branch-adapter%20architecture%20for%0Aconcurrent%20spatial%20and%20visual%20conditioning.%20Our%20lightweight%20model%20requires%0Atrainable%20parameters%20and%20dataset%20size%20multiple%20orders%20of%20magnitude%20smaller%20than%0Athe%20current%20state-of-the-art%20IP-Adapter.%20However%2C%20our%20method%20successfully%0Apreserves%20the%20generative%20power%20of%20the%20frozen%20text-to-image%20%28T2I%29%20backbone.%0ANotably%2C%20it%20excels%20in%20addressing%20mode%20collapse%2C%20a%20pervasive%20issue%20previously%0Aoverlooked.%20Our%20novel%20architecture%20demonstrates%20outstanding%20capabilities%20in%0Aachieving%20a%20harmonious%20visual-text%20balance%2C%20unlocking%20unparalleled%20versatility%0Ain%20various%20human%20image%20generation%20tasks%2C%20including%20pose%20re-targeting%2C%20virtual%0Atry-on%2C%20stylization%2C%20person%20re-identification%2C%20and%20textile%20transfer.Demo%20and%0Acode%20are%20available%20from%20project%20page%20https%3A//soon-yau.github.io/visconet/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.03154v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViscoNet%253A%2520Bridging%2520and%2520Harmonizing%2520Visual%2520and%2520Textual%2520Conditioning%2520for%250A%2520%2520ControlNet%26entry.906535625%3DSoon%2520Yau%2520Cheong%2520and%2520Armin%2520Mustafa%2520and%2520Andrew%2520Gilbert%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520ViscoNet%252C%2520a%2520novel%2520one-branch-adapter%2520architecture%2520for%250Aconcurrent%2520spatial%2520and%2520visual%2520conditioning.%2520Our%2520lightweight%2520model%2520requires%250Atrainable%2520parameters%2520and%2520dataset%2520size%2520multiple%2520orders%2520of%2520magnitude%2520smaller%2520than%250Athe%2520current%2520state-of-the-art%2520IP-Adapter.%2520However%252C%2520our%2520method%2520successfully%250Apreserves%2520the%2520generative%2520power%2520of%2520the%2520frozen%2520text-to-image%2520%2528T2I%2529%2520backbone.%250ANotably%252C%2520it%2520excels%2520in%2520addressing%2520mode%2520collapse%252C%2520a%2520pervasive%2520issue%2520previously%250Aoverlooked.%2520Our%2520novel%2520architecture%2520demonstrates%2520outstanding%2520capabilities%2520in%250Aachieving%2520a%2520harmonious%2520visual-text%2520balance%252C%2520unlocking%2520unparalleled%2520versatility%250Ain%2520various%2520human%2520image%2520generation%2520tasks%252C%2520including%2520pose%2520re-targeting%252C%2520virtual%250Atry-on%252C%2520stylization%252C%2520person%2520re-identification%252C%2520and%2520textile%2520transfer.Demo%2520and%250Acode%2520are%2520available%2520from%2520project%2520page%2520https%253A//soon-yau.github.io/visconet/%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.03154v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViscoNet%3A%20Bridging%20and%20Harmonizing%20Visual%20and%20Textual%20Conditioning%20for%0A%20%20ControlNet&entry.906535625=Soon%20Yau%20Cheong%20and%20Armin%20Mustafa%20and%20Andrew%20Gilbert&entry.1292438233=%20%20This%20paper%20introduces%20ViscoNet%2C%20a%20novel%20one-branch-adapter%20architecture%20for%0Aconcurrent%20spatial%20and%20visual%20conditioning.%20Our%20lightweight%20model%20requires%0Atrainable%20parameters%20and%20dataset%20size%20multiple%20orders%20of%20magnitude%20smaller%20than%0Athe%20current%20state-of-the-art%20IP-Adapter.%20However%2C%20our%20method%20successfully%0Apreserves%20the%20generative%20power%20of%20the%20frozen%20text-to-image%20%28T2I%29%20backbone.%0ANotably%2C%20it%20excels%20in%20addressing%20mode%20collapse%2C%20a%20pervasive%20issue%20previously%0Aoverlooked.%20Our%20novel%20architecture%20demonstrates%20outstanding%20capabilities%20in%0Aachieving%20a%20harmonious%20visual-text%20balance%2C%20unlocking%20unparalleled%20versatility%0Ain%20various%20human%20image%20generation%20tasks%2C%20including%20pose%20re-targeting%2C%20virtual%0Atry-on%2C%20stylization%2C%20person%20re-identification%2C%20and%20textile%20transfer.Demo%20and%0Acode%20are%20available%20from%20project%20page%20https%3A//soon-yau.github.io/visconet/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.03154v2&entry.124074799=Read"},
{"title": "A Survey on LoRA of Large Language Models", "author": "Yuren Mao and Yuhang Ge and Yijiang Fan and Wenyi Xu and Yu Mi and Zhonghao Hu and Yunjun Gao", "abstract": "  Low-Rank Adaptation~(LoRA), which updates the dense neural network layers\nwith pluggable low-rank matrices, is one of the best performed parameter\nefficient fine-tuning paradigms. Furthermore, it has significant advantages in\ncross-task generalization and privacy-preserving. Hence, LoRA has gained much\nattention recently, and the number of related literature demonstrates\nexponential growth. It is necessary to conduct a comprehensive overview of the\ncurrent progress on LoRA. This survey categorizes and reviews the progress from\nthe perspectives of (1) downstream adaptation improving variants that improve\nLoRA's performance on downstream tasks; (2) cross-task generalization methods\nthat mix multiple LoRA plugins to achieve cross-task generalization; (3)\nefficiency-improving methods that boost the computation-efficiency of LoRA; (4)\ndata privacy-preserving methods that use LoRA in federated learning; (5)\napplication. Besides, this survey also discusses the future directions in this\nfield. At last, we provide a Github\npage~\\footnote{\\href{https://github.com/ZJU-LLMs/Awesome-LoRAs.git}{https://github.com/ZJU-LLMs/Awesome-LoRAs.git}}\nfor readers to check the updates and initiate discussions on this survey paper.\n", "link": "http://arxiv.org/abs/2407.11046v3", "date": "2024-08-12", "relevancy": 2.343, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4871}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4634}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20LoRA%20of%20Large%20Language%20Models&body=Title%3A%20A%20Survey%20on%20LoRA%20of%20Large%20Language%20Models%0AAuthor%3A%20Yuren%20Mao%20and%20Yuhang%20Ge%20and%20Yijiang%20Fan%20and%20Wenyi%20Xu%20and%20Yu%20Mi%20and%20Zhonghao%20Hu%20and%20Yunjun%20Gao%0AAbstract%3A%20%20%20Low-Rank%20Adaptation~%28LoRA%29%2C%20which%20updates%20the%20dense%20neural%20network%20layers%0Awith%20pluggable%20low-rank%20matrices%2C%20is%20one%20of%20the%20best%20performed%20parameter%0Aefficient%20fine-tuning%20paradigms.%20Furthermore%2C%20it%20has%20significant%20advantages%20in%0Across-task%20generalization%20and%20privacy-preserving.%20Hence%2C%20LoRA%20has%20gained%20much%0Aattention%20recently%2C%20and%20the%20number%20of%20related%20literature%20demonstrates%0Aexponential%20growth.%20It%20is%20necessary%20to%20conduct%20a%20comprehensive%20overview%20of%20the%0Acurrent%20progress%20on%20LoRA.%20This%20survey%20categorizes%20and%20reviews%20the%20progress%20from%0Athe%20perspectives%20of%20%281%29%20downstream%20adaptation%20improving%20variants%20that%20improve%0ALoRA%27s%20performance%20on%20downstream%20tasks%3B%20%282%29%20cross-task%20generalization%20methods%0Athat%20mix%20multiple%20LoRA%20plugins%20to%20achieve%20cross-task%20generalization%3B%20%283%29%0Aefficiency-improving%20methods%20that%20boost%20the%20computation-efficiency%20of%20LoRA%3B%20%284%29%0Adata%20privacy-preserving%20methods%20that%20use%20LoRA%20in%20federated%20learning%3B%20%285%29%0Aapplication.%20Besides%2C%20this%20survey%20also%20discusses%20the%20future%20directions%20in%20this%0Afield.%20At%20last%2C%20we%20provide%20a%20Github%0Apage~%5Cfootnote%7B%5Chref%7Bhttps%3A//github.com/ZJU-LLMs/Awesome-LoRAs.git%7D%7Bhttps%3A//github.com/ZJU-LLMs/Awesome-LoRAs.git%7D%7D%0Afor%20readers%20to%20check%20the%20updates%20and%20initiate%20discussions%20on%20this%20survey%20paper.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11046v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520LoRA%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DYuren%2520Mao%2520and%2520Yuhang%2520Ge%2520and%2520Yijiang%2520Fan%2520and%2520Wenyi%2520Xu%2520and%2520Yu%2520Mi%2520and%2520Zhonghao%2520Hu%2520and%2520Yunjun%2520Gao%26entry.1292438233%3D%2520%2520Low-Rank%2520Adaptation~%2528LoRA%2529%252C%2520which%2520updates%2520the%2520dense%2520neural%2520network%2520layers%250Awith%2520pluggable%2520low-rank%2520matrices%252C%2520is%2520one%2520of%2520the%2520best%2520performed%2520parameter%250Aefficient%2520fine-tuning%2520paradigms.%2520Furthermore%252C%2520it%2520has%2520significant%2520advantages%2520in%250Across-task%2520generalization%2520and%2520privacy-preserving.%2520Hence%252C%2520LoRA%2520has%2520gained%2520much%250Aattention%2520recently%252C%2520and%2520the%2520number%2520of%2520related%2520literature%2520demonstrates%250Aexponential%2520growth.%2520It%2520is%2520necessary%2520to%2520conduct%2520a%2520comprehensive%2520overview%2520of%2520the%250Acurrent%2520progress%2520on%2520LoRA.%2520This%2520survey%2520categorizes%2520and%2520reviews%2520the%2520progress%2520from%250Athe%2520perspectives%2520of%2520%25281%2529%2520downstream%2520adaptation%2520improving%2520variants%2520that%2520improve%250ALoRA%2527s%2520performance%2520on%2520downstream%2520tasks%253B%2520%25282%2529%2520cross-task%2520generalization%2520methods%250Athat%2520mix%2520multiple%2520LoRA%2520plugins%2520to%2520achieve%2520cross-task%2520generalization%253B%2520%25283%2529%250Aefficiency-improving%2520methods%2520that%2520boost%2520the%2520computation-efficiency%2520of%2520LoRA%253B%2520%25284%2529%250Adata%2520privacy-preserving%2520methods%2520that%2520use%2520LoRA%2520in%2520federated%2520learning%253B%2520%25285%2529%250Aapplication.%2520Besides%252C%2520this%2520survey%2520also%2520discusses%2520the%2520future%2520directions%2520in%2520this%250Afield.%2520At%2520last%252C%2520we%2520provide%2520a%2520Github%250Apage~%255Cfootnote%257B%255Chref%257Bhttps%253A//github.com/ZJU-LLMs/Awesome-LoRAs.git%257D%257Bhttps%253A//github.com/ZJU-LLMs/Awesome-LoRAs.git%257D%257D%250Afor%2520readers%2520to%2520check%2520the%2520updates%2520and%2520initiate%2520discussions%2520on%2520this%2520survey%2520paper.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11046v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20LoRA%20of%20Large%20Language%20Models&entry.906535625=Yuren%20Mao%20and%20Yuhang%20Ge%20and%20Yijiang%20Fan%20and%20Wenyi%20Xu%20and%20Yu%20Mi%20and%20Zhonghao%20Hu%20and%20Yunjun%20Gao&entry.1292438233=%20%20Low-Rank%20Adaptation~%28LoRA%29%2C%20which%20updates%20the%20dense%20neural%20network%20layers%0Awith%20pluggable%20low-rank%20matrices%2C%20is%20one%20of%20the%20best%20performed%20parameter%0Aefficient%20fine-tuning%20paradigms.%20Furthermore%2C%20it%20has%20significant%20advantages%20in%0Across-task%20generalization%20and%20privacy-preserving.%20Hence%2C%20LoRA%20has%20gained%20much%0Aattention%20recently%2C%20and%20the%20number%20of%20related%20literature%20demonstrates%0Aexponential%20growth.%20It%20is%20necessary%20to%20conduct%20a%20comprehensive%20overview%20of%20the%0Acurrent%20progress%20on%20LoRA.%20This%20survey%20categorizes%20and%20reviews%20the%20progress%20from%0Athe%20perspectives%20of%20%281%29%20downstream%20adaptation%20improving%20variants%20that%20improve%0ALoRA%27s%20performance%20on%20downstream%20tasks%3B%20%282%29%20cross-task%20generalization%20methods%0Athat%20mix%20multiple%20LoRA%20plugins%20to%20achieve%20cross-task%20generalization%3B%20%283%29%0Aefficiency-improving%20methods%20that%20boost%20the%20computation-efficiency%20of%20LoRA%3B%20%284%29%0Adata%20privacy-preserving%20methods%20that%20use%20LoRA%20in%20federated%20learning%3B%20%285%29%0Aapplication.%20Besides%2C%20this%20survey%20also%20discusses%20the%20future%20directions%20in%20this%0Afield.%20At%20last%2C%20we%20provide%20a%20Github%0Apage~%5Cfootnote%7B%5Chref%7Bhttps%3A//github.com/ZJU-LLMs/Awesome-LoRAs.git%7D%7Bhttps%3A//github.com/ZJU-LLMs/Awesome-LoRAs.git%7D%7D%0Afor%20readers%20to%20check%20the%20updates%20and%20initiate%20discussions%20on%20this%20survey%20paper.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11046v3&entry.124074799=Read"},
{"title": "EyeSight Hand: Design of a Fully-Actuated Dexterous Robot Hand with\n  Integrated Vision-Based Tactile Sensors and Compliant Actuation", "author": "Branden Romero and Hao-Shu Fang and Pulkit Agrawal and Edward Adelson", "abstract": "  In this work, we introduce the EyeSight Hand, a novel 7 degrees of freedom\n(DoF) humanoid hand featuring integrated vision-based tactile sensors tailored\nfor enhanced whole-hand manipulation. Additionally, we introduce an actuation\nscheme centered around quasi-direct drive actuation to achieve human-like\nstrength and speed while ensuring robustness for large-scale data collection.\nWe evaluate the EyeSight Hand on three challenging tasks: bottle opening,\nplasticine cutting, and plate pick and place, which require a blend of complex\nmanipulation, tool use, and precise force application. Imitation learning\nmodels trained on these tasks, with a novel vision dropout strategy, showcase\nthe benefits of tactile feedback in enhancing task success rates. Our results\nreveal that the integration of tactile sensing dramatically improves task\nperformance, underscoring the critical role of tactile information in dexterous\nmanipulation.\n", "link": "http://arxiv.org/abs/2408.06265v1", "date": "2024-08-12", "relevancy": 2.3301, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6134}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.568}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EyeSight%20Hand%3A%20Design%20of%20a%20Fully-Actuated%20Dexterous%20Robot%20Hand%20with%0A%20%20Integrated%20Vision-Based%20Tactile%20Sensors%20and%20Compliant%20Actuation&body=Title%3A%20EyeSight%20Hand%3A%20Design%20of%20a%20Fully-Actuated%20Dexterous%20Robot%20Hand%20with%0A%20%20Integrated%20Vision-Based%20Tactile%20Sensors%20and%20Compliant%20Actuation%0AAuthor%3A%20Branden%20Romero%20and%20Hao-Shu%20Fang%20and%20Pulkit%20Agrawal%20and%20Edward%20Adelson%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20introduce%20the%20EyeSight%20Hand%2C%20a%20novel%207%20degrees%20of%20freedom%0A%28DoF%29%20humanoid%20hand%20featuring%20integrated%20vision-based%20tactile%20sensors%20tailored%0Afor%20enhanced%20whole-hand%20manipulation.%20Additionally%2C%20we%20introduce%20an%20actuation%0Ascheme%20centered%20around%20quasi-direct%20drive%20actuation%20to%20achieve%20human-like%0Astrength%20and%20speed%20while%20ensuring%20robustness%20for%20large-scale%20data%20collection.%0AWe%20evaluate%20the%20EyeSight%20Hand%20on%20three%20challenging%20tasks%3A%20bottle%20opening%2C%0Aplasticine%20cutting%2C%20and%20plate%20pick%20and%20place%2C%20which%20require%20a%20blend%20of%20complex%0Amanipulation%2C%20tool%20use%2C%20and%20precise%20force%20application.%20Imitation%20learning%0Amodels%20trained%20on%20these%20tasks%2C%20with%20a%20novel%20vision%20dropout%20strategy%2C%20showcase%0Athe%20benefits%20of%20tactile%20feedback%20in%20enhancing%20task%20success%20rates.%20Our%20results%0Areveal%20that%20the%20integration%20of%20tactile%20sensing%20dramatically%20improves%20task%0Aperformance%2C%20underscoring%20the%20critical%20role%20of%20tactile%20information%20in%20dexterous%0Amanipulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06265v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEyeSight%2520Hand%253A%2520Design%2520of%2520a%2520Fully-Actuated%2520Dexterous%2520Robot%2520Hand%2520with%250A%2520%2520Integrated%2520Vision-Based%2520Tactile%2520Sensors%2520and%2520Compliant%2520Actuation%26entry.906535625%3DBranden%2520Romero%2520and%2520Hao-Shu%2520Fang%2520and%2520Pulkit%2520Agrawal%2520and%2520Edward%2520Adelson%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520the%2520EyeSight%2520Hand%252C%2520a%2520novel%25207%2520degrees%2520of%2520freedom%250A%2528DoF%2529%2520humanoid%2520hand%2520featuring%2520integrated%2520vision-based%2520tactile%2520sensors%2520tailored%250Afor%2520enhanced%2520whole-hand%2520manipulation.%2520Additionally%252C%2520we%2520introduce%2520an%2520actuation%250Ascheme%2520centered%2520around%2520quasi-direct%2520drive%2520actuation%2520to%2520achieve%2520human-like%250Astrength%2520and%2520speed%2520while%2520ensuring%2520robustness%2520for%2520large-scale%2520data%2520collection.%250AWe%2520evaluate%2520the%2520EyeSight%2520Hand%2520on%2520three%2520challenging%2520tasks%253A%2520bottle%2520opening%252C%250Aplasticine%2520cutting%252C%2520and%2520plate%2520pick%2520and%2520place%252C%2520which%2520require%2520a%2520blend%2520of%2520complex%250Amanipulation%252C%2520tool%2520use%252C%2520and%2520precise%2520force%2520application.%2520Imitation%2520learning%250Amodels%2520trained%2520on%2520these%2520tasks%252C%2520with%2520a%2520novel%2520vision%2520dropout%2520strategy%252C%2520showcase%250Athe%2520benefits%2520of%2520tactile%2520feedback%2520in%2520enhancing%2520task%2520success%2520rates.%2520Our%2520results%250Areveal%2520that%2520the%2520integration%2520of%2520tactile%2520sensing%2520dramatically%2520improves%2520task%250Aperformance%252C%2520underscoring%2520the%2520critical%2520role%2520of%2520tactile%2520information%2520in%2520dexterous%250Amanipulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06265v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EyeSight%20Hand%3A%20Design%20of%20a%20Fully-Actuated%20Dexterous%20Robot%20Hand%20with%0A%20%20Integrated%20Vision-Based%20Tactile%20Sensors%20and%20Compliant%20Actuation&entry.906535625=Branden%20Romero%20and%20Hao-Shu%20Fang%20and%20Pulkit%20Agrawal%20and%20Edward%20Adelson&entry.1292438233=%20%20In%20this%20work%2C%20we%20introduce%20the%20EyeSight%20Hand%2C%20a%20novel%207%20degrees%20of%20freedom%0A%28DoF%29%20humanoid%20hand%20featuring%20integrated%20vision-based%20tactile%20sensors%20tailored%0Afor%20enhanced%20whole-hand%20manipulation.%20Additionally%2C%20we%20introduce%20an%20actuation%0Ascheme%20centered%20around%20quasi-direct%20drive%20actuation%20to%20achieve%20human-like%0Astrength%20and%20speed%20while%20ensuring%20robustness%20for%20large-scale%20data%20collection.%0AWe%20evaluate%20the%20EyeSight%20Hand%20on%20three%20challenging%20tasks%3A%20bottle%20opening%2C%0Aplasticine%20cutting%2C%20and%20plate%20pick%20and%20place%2C%20which%20require%20a%20blend%20of%20complex%0Amanipulation%2C%20tool%20use%2C%20and%20precise%20force%20application.%20Imitation%20learning%0Amodels%20trained%20on%20these%20tasks%2C%20with%20a%20novel%20vision%20dropout%20strategy%2C%20showcase%0Athe%20benefits%20of%20tactile%20feedback%20in%20enhancing%20task%20success%20rates.%20Our%20results%0Areveal%20that%20the%20integration%20of%20tactile%20sensing%20dramatically%20improves%20task%0Aperformance%2C%20underscoring%20the%20critical%20role%20of%20tactile%20information%20in%20dexterous%0Amanipulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06265v1&entry.124074799=Read"},
{"title": "Towards Robust Monocular Depth Estimation in Non-Lambertian Surfaces", "author": "Junrui Zhang and Jiaqi Li and Yachuan Huang and Yiran Wang and Jinghong Zheng and Liao Shen and Zhiguo Cao", "abstract": "  In the field of monocular depth estimation (MDE), many models with excellent\nzero-shot performance in general scenes emerge recently. However, these methods\noften fail in predicting non-Lambertian surfaces, such as transparent or mirror\n(ToM) surfaces, due to the unique reflective properties of these regions.\nPrevious methods utilize externally provided ToM masks and aim to obtain\ncorrect depth maps through direct in-painting of RGB images. These methods\nhighly depend on the accuracy of additional input masks, and the use of random\ncolors during in-painting makes them insufficiently robust. We are committed to\nincrementally enabling the baseline model to directly learn the uniqueness of\nnon-Lambertian surface regions for depth estimation through a well-designed\ntraining framework. Therefore, we propose non-Lambertian surface regional\nguidance, which constrains the predictions of MDE model from the gradient\ndomain to enhance its robustness. Noting the significant impact of lighting on\nthis task, we employ the random tone-mapping augmentation during training to\nensure the network can predict correct results for varying lighting inputs.\nAdditionally, we propose an optional novel lighting fusion module, which uses\nVariational Autoencoders to fuse multiple images and obtain the most\nadvantageous input RGB image for depth estimation when multi-exposure images\nare available. Our method achieves accuracy improvements of 33.39% and 5.21% in\nzero-shot testing on the Booster and Mirror3D dataset for non-Lambertian\nsurfaces, respectively, compared to the Depth Anything V2. The state-of-the-art\nperformance of 90.75 in delta1.05 within the ToM regions on the TRICKY2024\ncompetition test set demonstrates the effectiveness of our approach.\n", "link": "http://arxiv.org/abs/2408.06083v1", "date": "2024-08-12", "relevancy": 2.3133, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6025}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5835}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5635}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Robust%20Monocular%20Depth%20Estimation%20in%20Non-Lambertian%20Surfaces&body=Title%3A%20Towards%20Robust%20Monocular%20Depth%20Estimation%20in%20Non-Lambertian%20Surfaces%0AAuthor%3A%20Junrui%20Zhang%20and%20Jiaqi%20Li%20and%20Yachuan%20Huang%20and%20Yiran%20Wang%20and%20Jinghong%20Zheng%20and%20Liao%20Shen%20and%20Zhiguo%20Cao%0AAbstract%3A%20%20%20In%20the%20field%20of%20monocular%20depth%20estimation%20%28MDE%29%2C%20many%20models%20with%20excellent%0Azero-shot%20performance%20in%20general%20scenes%20emerge%20recently.%20However%2C%20these%20methods%0Aoften%20fail%20in%20predicting%20non-Lambertian%20surfaces%2C%20such%20as%20transparent%20or%20mirror%0A%28ToM%29%20surfaces%2C%20due%20to%20the%20unique%20reflective%20properties%20of%20these%20regions.%0APrevious%20methods%20utilize%20externally%20provided%20ToM%20masks%20and%20aim%20to%20obtain%0Acorrect%20depth%20maps%20through%20direct%20in-painting%20of%20RGB%20images.%20These%20methods%0Ahighly%20depend%20on%20the%20accuracy%20of%20additional%20input%20masks%2C%20and%20the%20use%20of%20random%0Acolors%20during%20in-painting%20makes%20them%20insufficiently%20robust.%20We%20are%20committed%20to%0Aincrementally%20enabling%20the%20baseline%20model%20to%20directly%20learn%20the%20uniqueness%20of%0Anon-Lambertian%20surface%20regions%20for%20depth%20estimation%20through%20a%20well-designed%0Atraining%20framework.%20Therefore%2C%20we%20propose%20non-Lambertian%20surface%20regional%0Aguidance%2C%20which%20constrains%20the%20predictions%20of%20MDE%20model%20from%20the%20gradient%0Adomain%20to%20enhance%20its%20robustness.%20Noting%20the%20significant%20impact%20of%20lighting%20on%0Athis%20task%2C%20we%20employ%20the%20random%20tone-mapping%20augmentation%20during%20training%20to%0Aensure%20the%20network%20can%20predict%20correct%20results%20for%20varying%20lighting%20inputs.%0AAdditionally%2C%20we%20propose%20an%20optional%20novel%20lighting%20fusion%20module%2C%20which%20uses%0AVariational%20Autoencoders%20to%20fuse%20multiple%20images%20and%20obtain%20the%20most%0Aadvantageous%20input%20RGB%20image%20for%20depth%20estimation%20when%20multi-exposure%20images%0Aare%20available.%20Our%20method%20achieves%20accuracy%20improvements%20of%2033.39%25%20and%205.21%25%20in%0Azero-shot%20testing%20on%20the%20Booster%20and%20Mirror3D%20dataset%20for%20non-Lambertian%0Asurfaces%2C%20respectively%2C%20compared%20to%20the%20Depth%20Anything%20V2.%20The%20state-of-the-art%0Aperformance%20of%2090.75%20in%20delta1.05%20within%20the%20ToM%20regions%20on%20the%20TRICKY2024%0Acompetition%20test%20set%20demonstrates%20the%20effectiveness%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06083v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Robust%2520Monocular%2520Depth%2520Estimation%2520in%2520Non-Lambertian%2520Surfaces%26entry.906535625%3DJunrui%2520Zhang%2520and%2520Jiaqi%2520Li%2520and%2520Yachuan%2520Huang%2520and%2520Yiran%2520Wang%2520and%2520Jinghong%2520Zheng%2520and%2520Liao%2520Shen%2520and%2520Zhiguo%2520Cao%26entry.1292438233%3D%2520%2520In%2520the%2520field%2520of%2520monocular%2520depth%2520estimation%2520%2528MDE%2529%252C%2520many%2520models%2520with%2520excellent%250Azero-shot%2520performance%2520in%2520general%2520scenes%2520emerge%2520recently.%2520However%252C%2520these%2520methods%250Aoften%2520fail%2520in%2520predicting%2520non-Lambertian%2520surfaces%252C%2520such%2520as%2520transparent%2520or%2520mirror%250A%2528ToM%2529%2520surfaces%252C%2520due%2520to%2520the%2520unique%2520reflective%2520properties%2520of%2520these%2520regions.%250APrevious%2520methods%2520utilize%2520externally%2520provided%2520ToM%2520masks%2520and%2520aim%2520to%2520obtain%250Acorrect%2520depth%2520maps%2520through%2520direct%2520in-painting%2520of%2520RGB%2520images.%2520These%2520methods%250Ahighly%2520depend%2520on%2520the%2520accuracy%2520of%2520additional%2520input%2520masks%252C%2520and%2520the%2520use%2520of%2520random%250Acolors%2520during%2520in-painting%2520makes%2520them%2520insufficiently%2520robust.%2520We%2520are%2520committed%2520to%250Aincrementally%2520enabling%2520the%2520baseline%2520model%2520to%2520directly%2520learn%2520the%2520uniqueness%2520of%250Anon-Lambertian%2520surface%2520regions%2520for%2520depth%2520estimation%2520through%2520a%2520well-designed%250Atraining%2520framework.%2520Therefore%252C%2520we%2520propose%2520non-Lambertian%2520surface%2520regional%250Aguidance%252C%2520which%2520constrains%2520the%2520predictions%2520of%2520MDE%2520model%2520from%2520the%2520gradient%250Adomain%2520to%2520enhance%2520its%2520robustness.%2520Noting%2520the%2520significant%2520impact%2520of%2520lighting%2520on%250Athis%2520task%252C%2520we%2520employ%2520the%2520random%2520tone-mapping%2520augmentation%2520during%2520training%2520to%250Aensure%2520the%2520network%2520can%2520predict%2520correct%2520results%2520for%2520varying%2520lighting%2520inputs.%250AAdditionally%252C%2520we%2520propose%2520an%2520optional%2520novel%2520lighting%2520fusion%2520module%252C%2520which%2520uses%250AVariational%2520Autoencoders%2520to%2520fuse%2520multiple%2520images%2520and%2520obtain%2520the%2520most%250Aadvantageous%2520input%2520RGB%2520image%2520for%2520depth%2520estimation%2520when%2520multi-exposure%2520images%250Aare%2520available.%2520Our%2520method%2520achieves%2520accuracy%2520improvements%2520of%252033.39%2525%2520and%25205.21%2525%2520in%250Azero-shot%2520testing%2520on%2520the%2520Booster%2520and%2520Mirror3D%2520dataset%2520for%2520non-Lambertian%250Asurfaces%252C%2520respectively%252C%2520compared%2520to%2520the%2520Depth%2520Anything%2520V2.%2520The%2520state-of-the-art%250Aperformance%2520of%252090.75%2520in%2520delta1.05%2520within%2520the%2520ToM%2520regions%2520on%2520the%2520TRICKY2024%250Acompetition%2520test%2520set%2520demonstrates%2520the%2520effectiveness%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06083v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Robust%20Monocular%20Depth%20Estimation%20in%20Non-Lambertian%20Surfaces&entry.906535625=Junrui%20Zhang%20and%20Jiaqi%20Li%20and%20Yachuan%20Huang%20and%20Yiran%20Wang%20and%20Jinghong%20Zheng%20and%20Liao%20Shen%20and%20Zhiguo%20Cao&entry.1292438233=%20%20In%20the%20field%20of%20monocular%20depth%20estimation%20%28MDE%29%2C%20many%20models%20with%20excellent%0Azero-shot%20performance%20in%20general%20scenes%20emerge%20recently.%20However%2C%20these%20methods%0Aoften%20fail%20in%20predicting%20non-Lambertian%20surfaces%2C%20such%20as%20transparent%20or%20mirror%0A%28ToM%29%20surfaces%2C%20due%20to%20the%20unique%20reflective%20properties%20of%20these%20regions.%0APrevious%20methods%20utilize%20externally%20provided%20ToM%20masks%20and%20aim%20to%20obtain%0Acorrect%20depth%20maps%20through%20direct%20in-painting%20of%20RGB%20images.%20These%20methods%0Ahighly%20depend%20on%20the%20accuracy%20of%20additional%20input%20masks%2C%20and%20the%20use%20of%20random%0Acolors%20during%20in-painting%20makes%20them%20insufficiently%20robust.%20We%20are%20committed%20to%0Aincrementally%20enabling%20the%20baseline%20model%20to%20directly%20learn%20the%20uniqueness%20of%0Anon-Lambertian%20surface%20regions%20for%20depth%20estimation%20through%20a%20well-designed%0Atraining%20framework.%20Therefore%2C%20we%20propose%20non-Lambertian%20surface%20regional%0Aguidance%2C%20which%20constrains%20the%20predictions%20of%20MDE%20model%20from%20the%20gradient%0Adomain%20to%20enhance%20its%20robustness.%20Noting%20the%20significant%20impact%20of%20lighting%20on%0Athis%20task%2C%20we%20employ%20the%20random%20tone-mapping%20augmentation%20during%20training%20to%0Aensure%20the%20network%20can%20predict%20correct%20results%20for%20varying%20lighting%20inputs.%0AAdditionally%2C%20we%20propose%20an%20optional%20novel%20lighting%20fusion%20module%2C%20which%20uses%0AVariational%20Autoencoders%20to%20fuse%20multiple%20images%20and%20obtain%20the%20most%0Aadvantageous%20input%20RGB%20image%20for%20depth%20estimation%20when%20multi-exposure%20images%0Aare%20available.%20Our%20method%20achieves%20accuracy%20improvements%20of%2033.39%25%20and%205.21%25%20in%0Azero-shot%20testing%20on%20the%20Booster%20and%20Mirror3D%20dataset%20for%20non-Lambertian%0Asurfaces%2C%20respectively%2C%20compared%20to%20the%20Depth%20Anything%20V2.%20The%20state-of-the-art%0Aperformance%20of%2090.75%20in%20delta1.05%20within%20the%20ToM%20regions%20on%20the%20TRICKY2024%0Acompetition%20test%20set%20demonstrates%20the%20effectiveness%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06083v1&entry.124074799=Read"},
{"title": "An Experimental Comparison of Partitioning Strategies for Distributed\n  Graph Neural Network Training", "author": "Nikolai Merkel and Daniel Stoll and Ruben Mayer and Hans-Arno Jacobsen", "abstract": "  Recently, graph neural networks (GNNs) have gained much attention as a\ngrowing area of deep learning capable of learning on graph-structured data.\nHowever, the computational and memory requirements for training GNNs on\nlarge-scale graphs make it necessary to distribute the training. A prerequisite\nfor distributed GNN training is to partition the input graph into smaller parts\nthat are distributed among multiple machines of a compute cluster. Although\ngraph partitioning has been studied with regard to graph analytics and graph\ndatabases, its effect on GNN training performance is largely unexplored. As a\nconsequence, it is unclear whether investing computational efforts into\nhigh-quality graph partitioning would pay off in GNN training scenarios.\n  In this paper, we study the effectiveness of graph partitioning for\ndistributed GNN training. Our study aims to understand how different factors\nsuch as GNN parameters, mini-batch size, graph type, features size, and\nscale-out factor influence the effectiveness of graph partitioning. We conduct\nexperiments with two different GNN systems using vertex and edge partitioning.\nWe found that high-quality graph partitioning is a very effective optimization\nto speed up GNN training and to reduce memory consumption. Furthermore, our\nresults show that invested partitioning time can quickly be amortized by\nreduced GNN training time, making it a relevant optimization for most GNN\nscenarios. Compared to research on distributed graph processing, our study\nreveals that graph partitioning plays an even more significant role in\ndistributed GNN training, which motivates further research on the graph\npartitioning problem.\n", "link": "http://arxiv.org/abs/2308.15602v2", "date": "2024-08-12", "relevancy": 2.3077, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4852}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4559}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Experimental%20Comparison%20of%20Partitioning%20Strategies%20for%20Distributed%0A%20%20Graph%20Neural%20Network%20Training&body=Title%3A%20An%20Experimental%20Comparison%20of%20Partitioning%20Strategies%20for%20Distributed%0A%20%20Graph%20Neural%20Network%20Training%0AAuthor%3A%20Nikolai%20Merkel%20and%20Daniel%20Stoll%20and%20Ruben%20Mayer%20and%20Hans-Arno%20Jacobsen%0AAbstract%3A%20%20%20Recently%2C%20graph%20neural%20networks%20%28GNNs%29%20have%20gained%20much%20attention%20as%20a%0Agrowing%20area%20of%20deep%20learning%20capable%20of%20learning%20on%20graph-structured%20data.%0AHowever%2C%20the%20computational%20and%20memory%20requirements%20for%20training%20GNNs%20on%0Alarge-scale%20graphs%20make%20it%20necessary%20to%20distribute%20the%20training.%20A%20prerequisite%0Afor%20distributed%20GNN%20training%20is%20to%20partition%20the%20input%20graph%20into%20smaller%20parts%0Athat%20are%20distributed%20among%20multiple%20machines%20of%20a%20compute%20cluster.%20Although%0Agraph%20partitioning%20has%20been%20studied%20with%20regard%20to%20graph%20analytics%20and%20graph%0Adatabases%2C%20its%20effect%20on%20GNN%20training%20performance%20is%20largely%20unexplored.%20As%20a%0Aconsequence%2C%20it%20is%20unclear%20whether%20investing%20computational%20efforts%20into%0Ahigh-quality%20graph%20partitioning%20would%20pay%20off%20in%20GNN%20training%20scenarios.%0A%20%20In%20this%20paper%2C%20we%20study%20the%20effectiveness%20of%20graph%20partitioning%20for%0Adistributed%20GNN%20training.%20Our%20study%20aims%20to%20understand%20how%20different%20factors%0Asuch%20as%20GNN%20parameters%2C%20mini-batch%20size%2C%20graph%20type%2C%20features%20size%2C%20and%0Ascale-out%20factor%20influence%20the%20effectiveness%20of%20graph%20partitioning.%20We%20conduct%0Aexperiments%20with%20two%20different%20GNN%20systems%20using%20vertex%20and%20edge%20partitioning.%0AWe%20found%20that%20high-quality%20graph%20partitioning%20is%20a%20very%20effective%20optimization%0Ato%20speed%20up%20GNN%20training%20and%20to%20reduce%20memory%20consumption.%20Furthermore%2C%20our%0Aresults%20show%20that%20invested%20partitioning%20time%20can%20quickly%20be%20amortized%20by%0Areduced%20GNN%20training%20time%2C%20making%20it%20a%20relevant%20optimization%20for%20most%20GNN%0Ascenarios.%20Compared%20to%20research%20on%20distributed%20graph%20processing%2C%20our%20study%0Areveals%20that%20graph%20partitioning%20plays%20an%20even%20more%20significant%20role%20in%0Adistributed%20GNN%20training%2C%20which%20motivates%20further%20research%20on%20the%20graph%0Apartitioning%20problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.15602v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Experimental%2520Comparison%2520of%2520Partitioning%2520Strategies%2520for%2520Distributed%250A%2520%2520Graph%2520Neural%2520Network%2520Training%26entry.906535625%3DNikolai%2520Merkel%2520and%2520Daniel%2520Stoll%2520and%2520Ruben%2520Mayer%2520and%2520Hans-Arno%2520Jacobsen%26entry.1292438233%3D%2520%2520Recently%252C%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520have%2520gained%2520much%2520attention%2520as%2520a%250Agrowing%2520area%2520of%2520deep%2520learning%2520capable%2520of%2520learning%2520on%2520graph-structured%2520data.%250AHowever%252C%2520the%2520computational%2520and%2520memory%2520requirements%2520for%2520training%2520GNNs%2520on%250Alarge-scale%2520graphs%2520make%2520it%2520necessary%2520to%2520distribute%2520the%2520training.%2520A%2520prerequisite%250Afor%2520distributed%2520GNN%2520training%2520is%2520to%2520partition%2520the%2520input%2520graph%2520into%2520smaller%2520parts%250Athat%2520are%2520distributed%2520among%2520multiple%2520machines%2520of%2520a%2520compute%2520cluster.%2520Although%250Agraph%2520partitioning%2520has%2520been%2520studied%2520with%2520regard%2520to%2520graph%2520analytics%2520and%2520graph%250Adatabases%252C%2520its%2520effect%2520on%2520GNN%2520training%2520performance%2520is%2520largely%2520unexplored.%2520As%2520a%250Aconsequence%252C%2520it%2520is%2520unclear%2520whether%2520investing%2520computational%2520efforts%2520into%250Ahigh-quality%2520graph%2520partitioning%2520would%2520pay%2520off%2520in%2520GNN%2520training%2520scenarios.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520study%2520the%2520effectiveness%2520of%2520graph%2520partitioning%2520for%250Adistributed%2520GNN%2520training.%2520Our%2520study%2520aims%2520to%2520understand%2520how%2520different%2520factors%250Asuch%2520as%2520GNN%2520parameters%252C%2520mini-batch%2520size%252C%2520graph%2520type%252C%2520features%2520size%252C%2520and%250Ascale-out%2520factor%2520influence%2520the%2520effectiveness%2520of%2520graph%2520partitioning.%2520We%2520conduct%250Aexperiments%2520with%2520two%2520different%2520GNN%2520systems%2520using%2520vertex%2520and%2520edge%2520partitioning.%250AWe%2520found%2520that%2520high-quality%2520graph%2520partitioning%2520is%2520a%2520very%2520effective%2520optimization%250Ato%2520speed%2520up%2520GNN%2520training%2520and%2520to%2520reduce%2520memory%2520consumption.%2520Furthermore%252C%2520our%250Aresults%2520show%2520that%2520invested%2520partitioning%2520time%2520can%2520quickly%2520be%2520amortized%2520by%250Areduced%2520GNN%2520training%2520time%252C%2520making%2520it%2520a%2520relevant%2520optimization%2520for%2520most%2520GNN%250Ascenarios.%2520Compared%2520to%2520research%2520on%2520distributed%2520graph%2520processing%252C%2520our%2520study%250Areveals%2520that%2520graph%2520partitioning%2520plays%2520an%2520even%2520more%2520significant%2520role%2520in%250Adistributed%2520GNN%2520training%252C%2520which%2520motivates%2520further%2520research%2520on%2520the%2520graph%250Apartitioning%2520problem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.15602v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Experimental%20Comparison%20of%20Partitioning%20Strategies%20for%20Distributed%0A%20%20Graph%20Neural%20Network%20Training&entry.906535625=Nikolai%20Merkel%20and%20Daniel%20Stoll%20and%20Ruben%20Mayer%20and%20Hans-Arno%20Jacobsen&entry.1292438233=%20%20Recently%2C%20graph%20neural%20networks%20%28GNNs%29%20have%20gained%20much%20attention%20as%20a%0Agrowing%20area%20of%20deep%20learning%20capable%20of%20learning%20on%20graph-structured%20data.%0AHowever%2C%20the%20computational%20and%20memory%20requirements%20for%20training%20GNNs%20on%0Alarge-scale%20graphs%20make%20it%20necessary%20to%20distribute%20the%20training.%20A%20prerequisite%0Afor%20distributed%20GNN%20training%20is%20to%20partition%20the%20input%20graph%20into%20smaller%20parts%0Athat%20are%20distributed%20among%20multiple%20machines%20of%20a%20compute%20cluster.%20Although%0Agraph%20partitioning%20has%20been%20studied%20with%20regard%20to%20graph%20analytics%20and%20graph%0Adatabases%2C%20its%20effect%20on%20GNN%20training%20performance%20is%20largely%20unexplored.%20As%20a%0Aconsequence%2C%20it%20is%20unclear%20whether%20investing%20computational%20efforts%20into%0Ahigh-quality%20graph%20partitioning%20would%20pay%20off%20in%20GNN%20training%20scenarios.%0A%20%20In%20this%20paper%2C%20we%20study%20the%20effectiveness%20of%20graph%20partitioning%20for%0Adistributed%20GNN%20training.%20Our%20study%20aims%20to%20understand%20how%20different%20factors%0Asuch%20as%20GNN%20parameters%2C%20mini-batch%20size%2C%20graph%20type%2C%20features%20size%2C%20and%0Ascale-out%20factor%20influence%20the%20effectiveness%20of%20graph%20partitioning.%20We%20conduct%0Aexperiments%20with%20two%20different%20GNN%20systems%20using%20vertex%20and%20edge%20partitioning.%0AWe%20found%20that%20high-quality%20graph%20partitioning%20is%20a%20very%20effective%20optimization%0Ato%20speed%20up%20GNN%20training%20and%20to%20reduce%20memory%20consumption.%20Furthermore%2C%20our%0Aresults%20show%20that%20invested%20partitioning%20time%20can%20quickly%20be%20amortized%20by%0Areduced%20GNN%20training%20time%2C%20making%20it%20a%20relevant%20optimization%20for%20most%20GNN%0Ascenarios.%20Compared%20to%20research%20on%20distributed%20graph%20processing%2C%20our%20study%0Areveals%20that%20graph%20partitioning%20plays%20an%20even%20more%20significant%20role%20in%0Adistributed%20GNN%20training%2C%20which%20motivates%20further%20research%20on%20the%20graph%0Apartitioning%20problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.15602v2&entry.124074799=Read"},
{"title": "Blind-Match: Efficient Homomorphic Encryption-Based 1:N Matching for\n  Privacy-Preserving Biometric Identification", "author": "Hyunmin Choi and Jiwon Kim and Chiyoung Song and Simon S. Woo and Hyoungshick Kim", "abstract": "  We present Blind-Match, a novel biometric identification system that\nleverages homomorphic encryption (HE) for efficient and privacy-preserving 1:N\nmatching. Blind-Match introduces a HE-optimized cosine similarity computation\nmethod, where the key idea is to divide the feature vector into smaller parts\nfor processing rather than computing the entire vector at once. By optimizing\nthe number of these parts, Blind-Match minimizes execution time while ensuring\ndata privacy through HE. Blind-Match achieves superior performance compared to\nstate-of-the-art methods across various biometric datasets. On the LFW face\ndataset, Blind-Match attains a 99.63% Rank-1 accuracy with a 128-dimensional\nfeature vector, demonstrating its robustness in face recognition tasks. For\nfingerprint identification, Blind-Match achieves a remarkable 99.55% Rank-1\naccuracy on the PolyU dataset, even with a compact 16-dimensional feature\nvector, significantly outperforming the state-of-the-art method, Blind-Touch,\nwhich achieves only 59.17%. Furthermore, Blind-Match showcases practical\nefficiency in large-scale biometric identification scenarios, such as Naver\nCloud's FaceSign, by processing 6,144 biometric samples in 0.74 seconds using a\n128-dimensional feature vector.\n", "link": "http://arxiv.org/abs/2408.06167v1", "date": "2024-08-12", "relevancy": 2.254, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4657}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.456}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4308}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Blind-Match%3A%20Efficient%20Homomorphic%20Encryption-Based%201%3AN%20Matching%20for%0A%20%20Privacy-Preserving%20Biometric%20Identification&body=Title%3A%20Blind-Match%3A%20Efficient%20Homomorphic%20Encryption-Based%201%3AN%20Matching%20for%0A%20%20Privacy-Preserving%20Biometric%20Identification%0AAuthor%3A%20Hyunmin%20Choi%20and%20Jiwon%20Kim%20and%20Chiyoung%20Song%20and%20Simon%20S.%20Woo%20and%20Hyoungshick%20Kim%0AAbstract%3A%20%20%20We%20present%20Blind-Match%2C%20a%20novel%20biometric%20identification%20system%20that%0Aleverages%20homomorphic%20encryption%20%28HE%29%20for%20efficient%20and%20privacy-preserving%201%3AN%0Amatching.%20Blind-Match%20introduces%20a%20HE-optimized%20cosine%20similarity%20computation%0Amethod%2C%20where%20the%20key%20idea%20is%20to%20divide%20the%20feature%20vector%20into%20smaller%20parts%0Afor%20processing%20rather%20than%20computing%20the%20entire%20vector%20at%20once.%20By%20optimizing%0Athe%20number%20of%20these%20parts%2C%20Blind-Match%20minimizes%20execution%20time%20while%20ensuring%0Adata%20privacy%20through%20HE.%20Blind-Match%20achieves%20superior%20performance%20compared%20to%0Astate-of-the-art%20methods%20across%20various%20biometric%20datasets.%20On%20the%20LFW%20face%0Adataset%2C%20Blind-Match%20attains%20a%2099.63%25%20Rank-1%20accuracy%20with%20a%20128-dimensional%0Afeature%20vector%2C%20demonstrating%20its%20robustness%20in%20face%20recognition%20tasks.%20For%0Afingerprint%20identification%2C%20Blind-Match%20achieves%20a%20remarkable%2099.55%25%20Rank-1%0Aaccuracy%20on%20the%20PolyU%20dataset%2C%20even%20with%20a%20compact%2016-dimensional%20feature%0Avector%2C%20significantly%20outperforming%20the%20state-of-the-art%20method%2C%20Blind-Touch%2C%0Awhich%20achieves%20only%2059.17%25.%20Furthermore%2C%20Blind-Match%20showcases%20practical%0Aefficiency%20in%20large-scale%20biometric%20identification%20scenarios%2C%20such%20as%20Naver%0ACloud%27s%20FaceSign%2C%20by%20processing%206%2C144%20biometric%20samples%20in%200.74%20seconds%20using%20a%0A128-dimensional%20feature%20vector.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06167v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBlind-Match%253A%2520Efficient%2520Homomorphic%2520Encryption-Based%25201%253AN%2520Matching%2520for%250A%2520%2520Privacy-Preserving%2520Biometric%2520Identification%26entry.906535625%3DHyunmin%2520Choi%2520and%2520Jiwon%2520Kim%2520and%2520Chiyoung%2520Song%2520and%2520Simon%2520S.%2520Woo%2520and%2520Hyoungshick%2520Kim%26entry.1292438233%3D%2520%2520We%2520present%2520Blind-Match%252C%2520a%2520novel%2520biometric%2520identification%2520system%2520that%250Aleverages%2520homomorphic%2520encryption%2520%2528HE%2529%2520for%2520efficient%2520and%2520privacy-preserving%25201%253AN%250Amatching.%2520Blind-Match%2520introduces%2520a%2520HE-optimized%2520cosine%2520similarity%2520computation%250Amethod%252C%2520where%2520the%2520key%2520idea%2520is%2520to%2520divide%2520the%2520feature%2520vector%2520into%2520smaller%2520parts%250Afor%2520processing%2520rather%2520than%2520computing%2520the%2520entire%2520vector%2520at%2520once.%2520By%2520optimizing%250Athe%2520number%2520of%2520these%2520parts%252C%2520Blind-Match%2520minimizes%2520execution%2520time%2520while%2520ensuring%250Adata%2520privacy%2520through%2520HE.%2520Blind-Match%2520achieves%2520superior%2520performance%2520compared%2520to%250Astate-of-the-art%2520methods%2520across%2520various%2520biometric%2520datasets.%2520On%2520the%2520LFW%2520face%250Adataset%252C%2520Blind-Match%2520attains%2520a%252099.63%2525%2520Rank-1%2520accuracy%2520with%2520a%2520128-dimensional%250Afeature%2520vector%252C%2520demonstrating%2520its%2520robustness%2520in%2520face%2520recognition%2520tasks.%2520For%250Afingerprint%2520identification%252C%2520Blind-Match%2520achieves%2520a%2520remarkable%252099.55%2525%2520Rank-1%250Aaccuracy%2520on%2520the%2520PolyU%2520dataset%252C%2520even%2520with%2520a%2520compact%252016-dimensional%2520feature%250Avector%252C%2520significantly%2520outperforming%2520the%2520state-of-the-art%2520method%252C%2520Blind-Touch%252C%250Awhich%2520achieves%2520only%252059.17%2525.%2520Furthermore%252C%2520Blind-Match%2520showcases%2520practical%250Aefficiency%2520in%2520large-scale%2520biometric%2520identification%2520scenarios%252C%2520such%2520as%2520Naver%250ACloud%2527s%2520FaceSign%252C%2520by%2520processing%25206%252C144%2520biometric%2520samples%2520in%25200.74%2520seconds%2520using%2520a%250A128-dimensional%2520feature%2520vector.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06167v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Blind-Match%3A%20Efficient%20Homomorphic%20Encryption-Based%201%3AN%20Matching%20for%0A%20%20Privacy-Preserving%20Biometric%20Identification&entry.906535625=Hyunmin%20Choi%20and%20Jiwon%20Kim%20and%20Chiyoung%20Song%20and%20Simon%20S.%20Woo%20and%20Hyoungshick%20Kim&entry.1292438233=%20%20We%20present%20Blind-Match%2C%20a%20novel%20biometric%20identification%20system%20that%0Aleverages%20homomorphic%20encryption%20%28HE%29%20for%20efficient%20and%20privacy-preserving%201%3AN%0Amatching.%20Blind-Match%20introduces%20a%20HE-optimized%20cosine%20similarity%20computation%0Amethod%2C%20where%20the%20key%20idea%20is%20to%20divide%20the%20feature%20vector%20into%20smaller%20parts%0Afor%20processing%20rather%20than%20computing%20the%20entire%20vector%20at%20once.%20By%20optimizing%0Athe%20number%20of%20these%20parts%2C%20Blind-Match%20minimizes%20execution%20time%20while%20ensuring%0Adata%20privacy%20through%20HE.%20Blind-Match%20achieves%20superior%20performance%20compared%20to%0Astate-of-the-art%20methods%20across%20various%20biometric%20datasets.%20On%20the%20LFW%20face%0Adataset%2C%20Blind-Match%20attains%20a%2099.63%25%20Rank-1%20accuracy%20with%20a%20128-dimensional%0Afeature%20vector%2C%20demonstrating%20its%20robustness%20in%20face%20recognition%20tasks.%20For%0Afingerprint%20identification%2C%20Blind-Match%20achieves%20a%20remarkable%2099.55%25%20Rank-1%0Aaccuracy%20on%20the%20PolyU%20dataset%2C%20even%20with%20a%20compact%2016-dimensional%20feature%0Avector%2C%20significantly%20outperforming%20the%20state-of-the-art%20method%2C%20Blind-Touch%2C%0Awhich%20achieves%20only%2059.17%25.%20Furthermore%2C%20Blind-Match%20showcases%20practical%0Aefficiency%20in%20large-scale%20biometric%20identification%20scenarios%2C%20such%20as%20Naver%0ACloud%27s%20FaceSign%2C%20by%20processing%206%2C144%20biometric%20samples%20in%200.74%20seconds%20using%20a%0A128-dimensional%20feature%20vector.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06167v1&entry.124074799=Read"},
{"title": "Weakly Supervised LiDAR Semantic Segmentation via Scatter Image\n  Annotation", "author": "Yilong Chen and Zongyi Xu and xiaoshui Huang and Ruicheng Zhang and Xinqi Jiang and Xinbo Gao", "abstract": "  Weakly supervised LiDAR semantic segmentation has made significant strides\nwith limited labeled data. However, most existing methods focus on the network\ntraining under weak supervision, while efficient annotation strategies remain\nlargely unexplored. To tackle this gap, we implement LiDAR semantic\nsegmentation using scatter image annotation, effectively integrating an\nefficient annotation strategy with network training. Specifically, we propose\nemploying scatter images to annotate LiDAR point clouds, combining a\npre-trained optical flow estimation network with a foundation image\nsegmentation model to rapidly propagate manual annotations into dense labels\nfor both images and point clouds. Moreover, we propose ScatterNet, a network\nthat includes three pivotal strategies to reduce the performance gap caused by\nsuch annotations. Firstly, it utilizes dense semantic labels as supervision for\nthe image branch, alleviating the modality imbalance between point clouds and\nimages. Secondly, an intermediate fusion branch is proposed to obtain\nmultimodal texture and structural features. Lastly, a perception consistency\nloss is introduced to determine which information needs to be fused and which\nneeds to be discarded during the fusion process. Extensive experiments on the\nnuScenes and SemanticKITTI datasets have demonstrated that our method requires\nless than 0.02% of the labeled points to achieve over 95% of the performance of\nfully-supervised methods. Notably, our labeled points are only 5% of those used\nin the most advanced weakly supervised methods.\n", "link": "http://arxiv.org/abs/2404.12861v2", "date": "2024-08-12", "relevancy": 2.231, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.575}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5529}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weakly%20Supervised%20LiDAR%20Semantic%20Segmentation%20via%20Scatter%20Image%0A%20%20Annotation&body=Title%3A%20Weakly%20Supervised%20LiDAR%20Semantic%20Segmentation%20via%20Scatter%20Image%0A%20%20Annotation%0AAuthor%3A%20Yilong%20Chen%20and%20Zongyi%20Xu%20and%20xiaoshui%20Huang%20and%20Ruicheng%20Zhang%20and%20Xinqi%20Jiang%20and%20Xinbo%20Gao%0AAbstract%3A%20%20%20Weakly%20supervised%20LiDAR%20semantic%20segmentation%20has%20made%20significant%20strides%0Awith%20limited%20labeled%20data.%20However%2C%20most%20existing%20methods%20focus%20on%20the%20network%0Atraining%20under%20weak%20supervision%2C%20while%20efficient%20annotation%20strategies%20remain%0Alargely%20unexplored.%20To%20tackle%20this%20gap%2C%20we%20implement%20LiDAR%20semantic%0Asegmentation%20using%20scatter%20image%20annotation%2C%20effectively%20integrating%20an%0Aefficient%20annotation%20strategy%20with%20network%20training.%20Specifically%2C%20we%20propose%0Aemploying%20scatter%20images%20to%20annotate%20LiDAR%20point%20clouds%2C%20combining%20a%0Apre-trained%20optical%20flow%20estimation%20network%20with%20a%20foundation%20image%0Asegmentation%20model%20to%20rapidly%20propagate%20manual%20annotations%20into%20dense%20labels%0Afor%20both%20images%20and%20point%20clouds.%20Moreover%2C%20we%20propose%20ScatterNet%2C%20a%20network%0Athat%20includes%20three%20pivotal%20strategies%20to%20reduce%20the%20performance%20gap%20caused%20by%0Asuch%20annotations.%20Firstly%2C%20it%20utilizes%20dense%20semantic%20labels%20as%20supervision%20for%0Athe%20image%20branch%2C%20alleviating%20the%20modality%20imbalance%20between%20point%20clouds%20and%0Aimages.%20Secondly%2C%20an%20intermediate%20fusion%20branch%20is%20proposed%20to%20obtain%0Amultimodal%20texture%20and%20structural%20features.%20Lastly%2C%20a%20perception%20consistency%0Aloss%20is%20introduced%20to%20determine%20which%20information%20needs%20to%20be%20fused%20and%20which%0Aneeds%20to%20be%20discarded%20during%20the%20fusion%20process.%20Extensive%20experiments%20on%20the%0AnuScenes%20and%20SemanticKITTI%20datasets%20have%20demonstrated%20that%20our%20method%20requires%0Aless%20than%200.02%25%20of%20the%20labeled%20points%20to%20achieve%20over%2095%25%20of%20the%20performance%20of%0Afully-supervised%20methods.%20Notably%2C%20our%20labeled%20points%20are%20only%205%25%20of%20those%20used%0Ain%20the%20most%20advanced%20weakly%20supervised%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12861v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeakly%2520Supervised%2520LiDAR%2520Semantic%2520Segmentation%2520via%2520Scatter%2520Image%250A%2520%2520Annotation%26entry.906535625%3DYilong%2520Chen%2520and%2520Zongyi%2520Xu%2520and%2520xiaoshui%2520Huang%2520and%2520Ruicheng%2520Zhang%2520and%2520Xinqi%2520Jiang%2520and%2520Xinbo%2520Gao%26entry.1292438233%3D%2520%2520Weakly%2520supervised%2520LiDAR%2520semantic%2520segmentation%2520has%2520made%2520significant%2520strides%250Awith%2520limited%2520labeled%2520data.%2520However%252C%2520most%2520existing%2520methods%2520focus%2520on%2520the%2520network%250Atraining%2520under%2520weak%2520supervision%252C%2520while%2520efficient%2520annotation%2520strategies%2520remain%250Alargely%2520unexplored.%2520To%2520tackle%2520this%2520gap%252C%2520we%2520implement%2520LiDAR%2520semantic%250Asegmentation%2520using%2520scatter%2520image%2520annotation%252C%2520effectively%2520integrating%2520an%250Aefficient%2520annotation%2520strategy%2520with%2520network%2520training.%2520Specifically%252C%2520we%2520propose%250Aemploying%2520scatter%2520images%2520to%2520annotate%2520LiDAR%2520point%2520clouds%252C%2520combining%2520a%250Apre-trained%2520optical%2520flow%2520estimation%2520network%2520with%2520a%2520foundation%2520image%250Asegmentation%2520model%2520to%2520rapidly%2520propagate%2520manual%2520annotations%2520into%2520dense%2520labels%250Afor%2520both%2520images%2520and%2520point%2520clouds.%2520Moreover%252C%2520we%2520propose%2520ScatterNet%252C%2520a%2520network%250Athat%2520includes%2520three%2520pivotal%2520strategies%2520to%2520reduce%2520the%2520performance%2520gap%2520caused%2520by%250Asuch%2520annotations.%2520Firstly%252C%2520it%2520utilizes%2520dense%2520semantic%2520labels%2520as%2520supervision%2520for%250Athe%2520image%2520branch%252C%2520alleviating%2520the%2520modality%2520imbalance%2520between%2520point%2520clouds%2520and%250Aimages.%2520Secondly%252C%2520an%2520intermediate%2520fusion%2520branch%2520is%2520proposed%2520to%2520obtain%250Amultimodal%2520texture%2520and%2520structural%2520features.%2520Lastly%252C%2520a%2520perception%2520consistency%250Aloss%2520is%2520introduced%2520to%2520determine%2520which%2520information%2520needs%2520to%2520be%2520fused%2520and%2520which%250Aneeds%2520to%2520be%2520discarded%2520during%2520the%2520fusion%2520process.%2520Extensive%2520experiments%2520on%2520the%250AnuScenes%2520and%2520SemanticKITTI%2520datasets%2520have%2520demonstrated%2520that%2520our%2520method%2520requires%250Aless%2520than%25200.02%2525%2520of%2520the%2520labeled%2520points%2520to%2520achieve%2520over%252095%2525%2520of%2520the%2520performance%2520of%250Afully-supervised%2520methods.%2520Notably%252C%2520our%2520labeled%2520points%2520are%2520only%25205%2525%2520of%2520those%2520used%250Ain%2520the%2520most%2520advanced%2520weakly%2520supervised%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.12861v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weakly%20Supervised%20LiDAR%20Semantic%20Segmentation%20via%20Scatter%20Image%0A%20%20Annotation&entry.906535625=Yilong%20Chen%20and%20Zongyi%20Xu%20and%20xiaoshui%20Huang%20and%20Ruicheng%20Zhang%20and%20Xinqi%20Jiang%20and%20Xinbo%20Gao&entry.1292438233=%20%20Weakly%20supervised%20LiDAR%20semantic%20segmentation%20has%20made%20significant%20strides%0Awith%20limited%20labeled%20data.%20However%2C%20most%20existing%20methods%20focus%20on%20the%20network%0Atraining%20under%20weak%20supervision%2C%20while%20efficient%20annotation%20strategies%20remain%0Alargely%20unexplored.%20To%20tackle%20this%20gap%2C%20we%20implement%20LiDAR%20semantic%0Asegmentation%20using%20scatter%20image%20annotation%2C%20effectively%20integrating%20an%0Aefficient%20annotation%20strategy%20with%20network%20training.%20Specifically%2C%20we%20propose%0Aemploying%20scatter%20images%20to%20annotate%20LiDAR%20point%20clouds%2C%20combining%20a%0Apre-trained%20optical%20flow%20estimation%20network%20with%20a%20foundation%20image%0Asegmentation%20model%20to%20rapidly%20propagate%20manual%20annotations%20into%20dense%20labels%0Afor%20both%20images%20and%20point%20clouds.%20Moreover%2C%20we%20propose%20ScatterNet%2C%20a%20network%0Athat%20includes%20three%20pivotal%20strategies%20to%20reduce%20the%20performance%20gap%20caused%20by%0Asuch%20annotations.%20Firstly%2C%20it%20utilizes%20dense%20semantic%20labels%20as%20supervision%20for%0Athe%20image%20branch%2C%20alleviating%20the%20modality%20imbalance%20between%20point%20clouds%20and%0Aimages.%20Secondly%2C%20an%20intermediate%20fusion%20branch%20is%20proposed%20to%20obtain%0Amultimodal%20texture%20and%20structural%20features.%20Lastly%2C%20a%20perception%20consistency%0Aloss%20is%20introduced%20to%20determine%20which%20information%20needs%20to%20be%20fused%20and%20which%0Aneeds%20to%20be%20discarded%20during%20the%20fusion%20process.%20Extensive%20experiments%20on%20the%0AnuScenes%20and%20SemanticKITTI%20datasets%20have%20demonstrated%20that%20our%20method%20requires%0Aless%20than%200.02%25%20of%20the%20labeled%20points%20to%20achieve%20over%2095%25%20of%20the%20performance%20of%0Afully-supervised%20methods.%20Notably%2C%20our%20labeled%20points%20are%20only%205%25%20of%20those%20used%0Ain%20the%20most%20advanced%20weakly%20supervised%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12861v2&entry.124074799=Read"},
{"title": "Time is Not Enough: Time-Frequency based Explanation for Time-Series\n  Black-Box Models", "author": "Hyunseung Chung and Sumin Jo and Yeonsu Kwon and Edward Choi", "abstract": "  Despite the massive attention given to time-series explanations due to their\nextensive applications, a notable limitation in existing approaches is their\nprimary reliance on the time-domain. This overlooks the inherent characteristic\nof time-series data containing both time and frequency features. In this work,\nwe present Spectral eXplanation (SpectralX), an XAI framework that provides\ntime-frequency explanations for time-series black-box classifiers. This easily\nadaptable framework enables users to \"plug-in\" various perturbation-based XAI\nmethods for any pre-trained time-series classification models to assess their\nimpact on the explanation quality without having to modify the framework\narchitecture. Additionally, we introduce Feature Importance Approximations\n(FIA), a new perturbation-based XAI method. These methods consist of feature\ninsertion, deletion, and combination techniques to enhance computational\nefficiency and class-specific explanations in time-series classification tasks.\nWe conduct extensive experiments in the generated synthetic dataset and various\nUCR Time-Series datasets to first compare the explanation performance of FIA\nand other existing perturbation-based XAI methods in both time-domain and\ntime-frequency domain, and then show the superiority of our FIA in the\ntime-frequency domain with the SpectralX framework. Finally, we conduct a user\nstudy to confirm the practicality of our FIA in SpectralX framework for\nclass-specific time-frequency based time-series explanations. The source code\nis available in https://github.com/gustmd0121/Time_is_not_Enough\n", "link": "http://arxiv.org/abs/2408.03636v2", "date": "2024-08-12", "relevancy": 2.2138, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4605}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4345}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Time%20is%20Not%20Enough%3A%20Time-Frequency%20based%20Explanation%20for%20Time-Series%0A%20%20Black-Box%20Models&body=Title%3A%20Time%20is%20Not%20Enough%3A%20Time-Frequency%20based%20Explanation%20for%20Time-Series%0A%20%20Black-Box%20Models%0AAuthor%3A%20Hyunseung%20Chung%20and%20Sumin%20Jo%20and%20Yeonsu%20Kwon%20and%20Edward%20Choi%0AAbstract%3A%20%20%20Despite%20the%20massive%20attention%20given%20to%20time-series%20explanations%20due%20to%20their%0Aextensive%20applications%2C%20a%20notable%20limitation%20in%20existing%20approaches%20is%20their%0Aprimary%20reliance%20on%20the%20time-domain.%20This%20overlooks%20the%20inherent%20characteristic%0Aof%20time-series%20data%20containing%20both%20time%20and%20frequency%20features.%20In%20this%20work%2C%0Awe%20present%20Spectral%20eXplanation%20%28SpectralX%29%2C%20an%20XAI%20framework%20that%20provides%0Atime-frequency%20explanations%20for%20time-series%20black-box%20classifiers.%20This%20easily%0Aadaptable%20framework%20enables%20users%20to%20%22plug-in%22%20various%20perturbation-based%20XAI%0Amethods%20for%20any%20pre-trained%20time-series%20classification%20models%20to%20assess%20their%0Aimpact%20on%20the%20explanation%20quality%20without%20having%20to%20modify%20the%20framework%0Aarchitecture.%20Additionally%2C%20we%20introduce%20Feature%20Importance%20Approximations%0A%28FIA%29%2C%20a%20new%20perturbation-based%20XAI%20method.%20These%20methods%20consist%20of%20feature%0Ainsertion%2C%20deletion%2C%20and%20combination%20techniques%20to%20enhance%20computational%0Aefficiency%20and%20class-specific%20explanations%20in%20time-series%20classification%20tasks.%0AWe%20conduct%20extensive%20experiments%20in%20the%20generated%20synthetic%20dataset%20and%20various%0AUCR%20Time-Series%20datasets%20to%20first%20compare%20the%20explanation%20performance%20of%20FIA%0Aand%20other%20existing%20perturbation-based%20XAI%20methods%20in%20both%20time-domain%20and%0Atime-frequency%20domain%2C%20and%20then%20show%20the%20superiority%20of%20our%20FIA%20in%20the%0Atime-frequency%20domain%20with%20the%20SpectralX%20framework.%20Finally%2C%20we%20conduct%20a%20user%0Astudy%20to%20confirm%20the%20practicality%20of%20our%20FIA%20in%20SpectralX%20framework%20for%0Aclass-specific%20time-frequency%20based%20time-series%20explanations.%20The%20source%20code%0Ais%20available%20in%20https%3A//github.com/gustmd0121/Time_is_not_Enough%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03636v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTime%2520is%2520Not%2520Enough%253A%2520Time-Frequency%2520based%2520Explanation%2520for%2520Time-Series%250A%2520%2520Black-Box%2520Models%26entry.906535625%3DHyunseung%2520Chung%2520and%2520Sumin%2520Jo%2520and%2520Yeonsu%2520Kwon%2520and%2520Edward%2520Choi%26entry.1292438233%3D%2520%2520Despite%2520the%2520massive%2520attention%2520given%2520to%2520time-series%2520explanations%2520due%2520to%2520their%250Aextensive%2520applications%252C%2520a%2520notable%2520limitation%2520in%2520existing%2520approaches%2520is%2520their%250Aprimary%2520reliance%2520on%2520the%2520time-domain.%2520This%2520overlooks%2520the%2520inherent%2520characteristic%250Aof%2520time-series%2520data%2520containing%2520both%2520time%2520and%2520frequency%2520features.%2520In%2520this%2520work%252C%250Awe%2520present%2520Spectral%2520eXplanation%2520%2528SpectralX%2529%252C%2520an%2520XAI%2520framework%2520that%2520provides%250Atime-frequency%2520explanations%2520for%2520time-series%2520black-box%2520classifiers.%2520This%2520easily%250Aadaptable%2520framework%2520enables%2520users%2520to%2520%2522plug-in%2522%2520various%2520perturbation-based%2520XAI%250Amethods%2520for%2520any%2520pre-trained%2520time-series%2520classification%2520models%2520to%2520assess%2520their%250Aimpact%2520on%2520the%2520explanation%2520quality%2520without%2520having%2520to%2520modify%2520the%2520framework%250Aarchitecture.%2520Additionally%252C%2520we%2520introduce%2520Feature%2520Importance%2520Approximations%250A%2528FIA%2529%252C%2520a%2520new%2520perturbation-based%2520XAI%2520method.%2520These%2520methods%2520consist%2520of%2520feature%250Ainsertion%252C%2520deletion%252C%2520and%2520combination%2520techniques%2520to%2520enhance%2520computational%250Aefficiency%2520and%2520class-specific%2520explanations%2520in%2520time-series%2520classification%2520tasks.%250AWe%2520conduct%2520extensive%2520experiments%2520in%2520the%2520generated%2520synthetic%2520dataset%2520and%2520various%250AUCR%2520Time-Series%2520datasets%2520to%2520first%2520compare%2520the%2520explanation%2520performance%2520of%2520FIA%250Aand%2520other%2520existing%2520perturbation-based%2520XAI%2520methods%2520in%2520both%2520time-domain%2520and%250Atime-frequency%2520domain%252C%2520and%2520then%2520show%2520the%2520superiority%2520of%2520our%2520FIA%2520in%2520the%250Atime-frequency%2520domain%2520with%2520the%2520SpectralX%2520framework.%2520Finally%252C%2520we%2520conduct%2520a%2520user%250Astudy%2520to%2520confirm%2520the%2520practicality%2520of%2520our%2520FIA%2520in%2520SpectralX%2520framework%2520for%250Aclass-specific%2520time-frequency%2520based%2520time-series%2520explanations.%2520The%2520source%2520code%250Ais%2520available%2520in%2520https%253A//github.com/gustmd0121/Time_is_not_Enough%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03636v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Time%20is%20Not%20Enough%3A%20Time-Frequency%20based%20Explanation%20for%20Time-Series%0A%20%20Black-Box%20Models&entry.906535625=Hyunseung%20Chung%20and%20Sumin%20Jo%20and%20Yeonsu%20Kwon%20and%20Edward%20Choi&entry.1292438233=%20%20Despite%20the%20massive%20attention%20given%20to%20time-series%20explanations%20due%20to%20their%0Aextensive%20applications%2C%20a%20notable%20limitation%20in%20existing%20approaches%20is%20their%0Aprimary%20reliance%20on%20the%20time-domain.%20This%20overlooks%20the%20inherent%20characteristic%0Aof%20time-series%20data%20containing%20both%20time%20and%20frequency%20features.%20In%20this%20work%2C%0Awe%20present%20Spectral%20eXplanation%20%28SpectralX%29%2C%20an%20XAI%20framework%20that%20provides%0Atime-frequency%20explanations%20for%20time-series%20black-box%20classifiers.%20This%20easily%0Aadaptable%20framework%20enables%20users%20to%20%22plug-in%22%20various%20perturbation-based%20XAI%0Amethods%20for%20any%20pre-trained%20time-series%20classification%20models%20to%20assess%20their%0Aimpact%20on%20the%20explanation%20quality%20without%20having%20to%20modify%20the%20framework%0Aarchitecture.%20Additionally%2C%20we%20introduce%20Feature%20Importance%20Approximations%0A%28FIA%29%2C%20a%20new%20perturbation-based%20XAI%20method.%20These%20methods%20consist%20of%20feature%0Ainsertion%2C%20deletion%2C%20and%20combination%20techniques%20to%20enhance%20computational%0Aefficiency%20and%20class-specific%20explanations%20in%20time-series%20classification%20tasks.%0AWe%20conduct%20extensive%20experiments%20in%20the%20generated%20synthetic%20dataset%20and%20various%0AUCR%20Time-Series%20datasets%20to%20first%20compare%20the%20explanation%20performance%20of%20FIA%0Aand%20other%20existing%20perturbation-based%20XAI%20methods%20in%20both%20time-domain%20and%0Atime-frequency%20domain%2C%20and%20then%20show%20the%20superiority%20of%20our%20FIA%20in%20the%0Atime-frequency%20domain%20with%20the%20SpectralX%20framework.%20Finally%2C%20we%20conduct%20a%20user%0Astudy%20to%20confirm%20the%20practicality%20of%20our%20FIA%20in%20SpectralX%20framework%20for%0Aclass-specific%20time-frequency%20based%20time-series%20explanations.%20The%20source%20code%0Ais%20available%20in%20https%3A//github.com/gustmd0121/Time_is_not_Enough%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03636v2&entry.124074799=Read"},
{"title": "DPDETR: Decoupled Position Detection Transformer for Infrared-Visible\n  Object Detection", "author": "Junjie Guo and Chenqiang Gao and Fangcen Liu and Deyu Meng", "abstract": "  Infrared-visible object detection aims to achieve robust object detection by\nleveraging the complementary information of infrared and visible image pairs.\nHowever, the commonly existing modality misalignment problem presents two\nchallenges: fusing misalignment complementary features is difficult, and\ncurrent methods cannot accurately locate objects in both modalities under\nmisalignment conditions. In this paper, we propose a Decoupled Position\nDetection Transformer (DPDETR) to address these problems. Specifically, we\nexplicitly formulate the object category, visible modality position, and\ninfrared modality position to enable the network to learn the intrinsic\nrelationships and output accurate positions of objects in both modalities. To\nfuse misaligned object features accurately, we propose a Decoupled Position\nMultispectral Cross-attention module that adaptively samples and aggregates\nmultispectral complementary features with the constraint of infrared and\nvisible reference positions. Additionally, we design a query-decoupled\nMultispectral Decoder structure to address the optimization gap among the three\nkinds of object information in our task and propose a Decoupled Position\nContrastive DeNosing Training strategy to enhance the DPDETR's ability to learn\ndecoupled positions. Experiments on DroneVehicle and KAIST datasets demonstrate\nsignificant improvements compared to other state-of-the-art methods. The code\nwill be released at https://github.com/gjj45/DPDETR.\n", "link": "http://arxiv.org/abs/2408.06123v1", "date": "2024-08-12", "relevancy": 2.2074, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5692}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5395}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5394}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DPDETR%3A%20Decoupled%20Position%20Detection%20Transformer%20for%20Infrared-Visible%0A%20%20Object%20Detection&body=Title%3A%20DPDETR%3A%20Decoupled%20Position%20Detection%20Transformer%20for%20Infrared-Visible%0A%20%20Object%20Detection%0AAuthor%3A%20Junjie%20Guo%20and%20Chenqiang%20Gao%20and%20Fangcen%20Liu%20and%20Deyu%20Meng%0AAbstract%3A%20%20%20Infrared-visible%20object%20detection%20aims%20to%20achieve%20robust%20object%20detection%20by%0Aleveraging%20the%20complementary%20information%20of%20infrared%20and%20visible%20image%20pairs.%0AHowever%2C%20the%20commonly%20existing%20modality%20misalignment%20problem%20presents%20two%0Achallenges%3A%20fusing%20misalignment%20complementary%20features%20is%20difficult%2C%20and%0Acurrent%20methods%20cannot%20accurately%20locate%20objects%20in%20both%20modalities%20under%0Amisalignment%20conditions.%20In%20this%20paper%2C%20we%20propose%20a%20Decoupled%20Position%0ADetection%20Transformer%20%28DPDETR%29%20to%20address%20these%20problems.%20Specifically%2C%20we%0Aexplicitly%20formulate%20the%20object%20category%2C%20visible%20modality%20position%2C%20and%0Ainfrared%20modality%20position%20to%20enable%20the%20network%20to%20learn%20the%20intrinsic%0Arelationships%20and%20output%20accurate%20positions%20of%20objects%20in%20both%20modalities.%20To%0Afuse%20misaligned%20object%20features%20accurately%2C%20we%20propose%20a%20Decoupled%20Position%0AMultispectral%20Cross-attention%20module%20that%20adaptively%20samples%20and%20aggregates%0Amultispectral%20complementary%20features%20with%20the%20constraint%20of%20infrared%20and%0Avisible%20reference%20positions.%20Additionally%2C%20we%20design%20a%20query-decoupled%0AMultispectral%20Decoder%20structure%20to%20address%20the%20optimization%20gap%20among%20the%20three%0Akinds%20of%20object%20information%20in%20our%20task%20and%20propose%20a%20Decoupled%20Position%0AContrastive%20DeNosing%20Training%20strategy%20to%20enhance%20the%20DPDETR%27s%20ability%20to%20learn%0Adecoupled%20positions.%20Experiments%20on%20DroneVehicle%20and%20KAIST%20datasets%20demonstrate%0Asignificant%20improvements%20compared%20to%20other%20state-of-the-art%20methods.%20The%20code%0Awill%20be%20released%20at%20https%3A//github.com/gjj45/DPDETR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06123v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDPDETR%253A%2520Decoupled%2520Position%2520Detection%2520Transformer%2520for%2520Infrared-Visible%250A%2520%2520Object%2520Detection%26entry.906535625%3DJunjie%2520Guo%2520and%2520Chenqiang%2520Gao%2520and%2520Fangcen%2520Liu%2520and%2520Deyu%2520Meng%26entry.1292438233%3D%2520%2520Infrared-visible%2520object%2520detection%2520aims%2520to%2520achieve%2520robust%2520object%2520detection%2520by%250Aleveraging%2520the%2520complementary%2520information%2520of%2520infrared%2520and%2520visible%2520image%2520pairs.%250AHowever%252C%2520the%2520commonly%2520existing%2520modality%2520misalignment%2520problem%2520presents%2520two%250Achallenges%253A%2520fusing%2520misalignment%2520complementary%2520features%2520is%2520difficult%252C%2520and%250Acurrent%2520methods%2520cannot%2520accurately%2520locate%2520objects%2520in%2520both%2520modalities%2520under%250Amisalignment%2520conditions.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520Decoupled%2520Position%250ADetection%2520Transformer%2520%2528DPDETR%2529%2520to%2520address%2520these%2520problems.%2520Specifically%252C%2520we%250Aexplicitly%2520formulate%2520the%2520object%2520category%252C%2520visible%2520modality%2520position%252C%2520and%250Ainfrared%2520modality%2520position%2520to%2520enable%2520the%2520network%2520to%2520learn%2520the%2520intrinsic%250Arelationships%2520and%2520output%2520accurate%2520positions%2520of%2520objects%2520in%2520both%2520modalities.%2520To%250Afuse%2520misaligned%2520object%2520features%2520accurately%252C%2520we%2520propose%2520a%2520Decoupled%2520Position%250AMultispectral%2520Cross-attention%2520module%2520that%2520adaptively%2520samples%2520and%2520aggregates%250Amultispectral%2520complementary%2520features%2520with%2520the%2520constraint%2520of%2520infrared%2520and%250Avisible%2520reference%2520positions.%2520Additionally%252C%2520we%2520design%2520a%2520query-decoupled%250AMultispectral%2520Decoder%2520structure%2520to%2520address%2520the%2520optimization%2520gap%2520among%2520the%2520three%250Akinds%2520of%2520object%2520information%2520in%2520our%2520task%2520and%2520propose%2520a%2520Decoupled%2520Position%250AContrastive%2520DeNosing%2520Training%2520strategy%2520to%2520enhance%2520the%2520DPDETR%2527s%2520ability%2520to%2520learn%250Adecoupled%2520positions.%2520Experiments%2520on%2520DroneVehicle%2520and%2520KAIST%2520datasets%2520demonstrate%250Asignificant%2520improvements%2520compared%2520to%2520other%2520state-of-the-art%2520methods.%2520The%2520code%250Awill%2520be%2520released%2520at%2520https%253A//github.com/gjj45/DPDETR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06123v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DPDETR%3A%20Decoupled%20Position%20Detection%20Transformer%20for%20Infrared-Visible%0A%20%20Object%20Detection&entry.906535625=Junjie%20Guo%20and%20Chenqiang%20Gao%20and%20Fangcen%20Liu%20and%20Deyu%20Meng&entry.1292438233=%20%20Infrared-visible%20object%20detection%20aims%20to%20achieve%20robust%20object%20detection%20by%0Aleveraging%20the%20complementary%20information%20of%20infrared%20and%20visible%20image%20pairs.%0AHowever%2C%20the%20commonly%20existing%20modality%20misalignment%20problem%20presents%20two%0Achallenges%3A%20fusing%20misalignment%20complementary%20features%20is%20difficult%2C%20and%0Acurrent%20methods%20cannot%20accurately%20locate%20objects%20in%20both%20modalities%20under%0Amisalignment%20conditions.%20In%20this%20paper%2C%20we%20propose%20a%20Decoupled%20Position%0ADetection%20Transformer%20%28DPDETR%29%20to%20address%20these%20problems.%20Specifically%2C%20we%0Aexplicitly%20formulate%20the%20object%20category%2C%20visible%20modality%20position%2C%20and%0Ainfrared%20modality%20position%20to%20enable%20the%20network%20to%20learn%20the%20intrinsic%0Arelationships%20and%20output%20accurate%20positions%20of%20objects%20in%20both%20modalities.%20To%0Afuse%20misaligned%20object%20features%20accurately%2C%20we%20propose%20a%20Decoupled%20Position%0AMultispectral%20Cross-attention%20module%20that%20adaptively%20samples%20and%20aggregates%0Amultispectral%20complementary%20features%20with%20the%20constraint%20of%20infrared%20and%0Avisible%20reference%20positions.%20Additionally%2C%20we%20design%20a%20query-decoupled%0AMultispectral%20Decoder%20structure%20to%20address%20the%20optimization%20gap%20among%20the%20three%0Akinds%20of%20object%20information%20in%20our%20task%20and%20propose%20a%20Decoupled%20Position%0AContrastive%20DeNosing%20Training%20strategy%20to%20enhance%20the%20DPDETR%27s%20ability%20to%20learn%0Adecoupled%20positions.%20Experiments%20on%20DroneVehicle%20and%20KAIST%20datasets%20demonstrate%0Asignificant%20improvements%20compared%20to%20other%20state-of-the-art%20methods.%20The%20code%0Awill%20be%20released%20at%20https%3A//github.com/gjj45/DPDETR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06123v1&entry.124074799=Read"},
{"title": "Toward a Surgeon-in-the-Loop Ophthalmic Robotic Apprentice using\n  Reinforcement and Imitation Learning", "author": "Amr Gomaa and Bilal Mahdy and Niko Kleer and Antonio Kr\u00fcger", "abstract": "  Robot-assisted surgical systems have demonstrated significant potential in\nenhancing surgical precision and minimizing human errors. However, existing\nsystems cannot accommodate individual surgeons' unique preferences and\nrequirements. Additionally, they primarily focus on general surgeries (e.g.,\nlaparoscopy) and are unsuitable for highly precise microsurgeries, such as\nophthalmic procedures. Thus, we propose an image-guided approach for\nsurgeon-centered autonomous agents that can adapt to the individual surgeon's\nskill level and preferred surgical techniques during ophthalmic cataract\nsurgery. Our approach trains reinforcement and imitation learning agents\nsimultaneously using curriculum learning approaches guided by image data to\nperform all tasks of the incision phase of cataract surgery. By integrating the\nsurgeon's actions and preferences into the training process, our approach\nenables the robot to implicitly learn and adapt to the individual surgeon's\nunique techniques through surgeon-in-the-loop demonstrations. This results in a\nmore intuitive and personalized surgical experience for the surgeon while\nensuring consistent performance for the autonomous robotic apprentice. We\ndefine and evaluate the effectiveness of our approach in a simulated\nenvironment using our proposed metrics and highlight the trade-off between a\ngeneric agent and a surgeon-centered adapted agent. Finally, our approach has\nthe potential to extend to other ophthalmic and microsurgical procedures,\nopening the door to a new generation of surgeon-in-the-loop autonomous surgical\nrobots. We provide an open-source simulation framework for future development\nand reproducibility at\nhttps://github.com/amrgomaaelhady/CataractAdaptSurgRobot.\n", "link": "http://arxiv.org/abs/2311.17693v3", "date": "2024-08-12", "relevancy": 2.2029, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5755}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5585}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5228}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20a%20Surgeon-in-the-Loop%20Ophthalmic%20Robotic%20Apprentice%20using%0A%20%20Reinforcement%20and%20Imitation%20Learning&body=Title%3A%20Toward%20a%20Surgeon-in-the-Loop%20Ophthalmic%20Robotic%20Apprentice%20using%0A%20%20Reinforcement%20and%20Imitation%20Learning%0AAuthor%3A%20Amr%20Gomaa%20and%20Bilal%20Mahdy%20and%20Niko%20Kleer%20and%20Antonio%20Kr%C3%BCger%0AAbstract%3A%20%20%20Robot-assisted%20surgical%20systems%20have%20demonstrated%20significant%20potential%20in%0Aenhancing%20surgical%20precision%20and%20minimizing%20human%20errors.%20However%2C%20existing%0Asystems%20cannot%20accommodate%20individual%20surgeons%27%20unique%20preferences%20and%0Arequirements.%20Additionally%2C%20they%20primarily%20focus%20on%20general%20surgeries%20%28e.g.%2C%0Alaparoscopy%29%20and%20are%20unsuitable%20for%20highly%20precise%20microsurgeries%2C%20such%20as%0Aophthalmic%20procedures.%20Thus%2C%20we%20propose%20an%20image-guided%20approach%20for%0Asurgeon-centered%20autonomous%20agents%20that%20can%20adapt%20to%20the%20individual%20surgeon%27s%0Askill%20level%20and%20preferred%20surgical%20techniques%20during%20ophthalmic%20cataract%0Asurgery.%20Our%20approach%20trains%20reinforcement%20and%20imitation%20learning%20agents%0Asimultaneously%20using%20curriculum%20learning%20approaches%20guided%20by%20image%20data%20to%0Aperform%20all%20tasks%20of%20the%20incision%20phase%20of%20cataract%20surgery.%20By%20integrating%20the%0Asurgeon%27s%20actions%20and%20preferences%20into%20the%20training%20process%2C%20our%20approach%0Aenables%20the%20robot%20to%20implicitly%20learn%20and%20adapt%20to%20the%20individual%20surgeon%27s%0Aunique%20techniques%20through%20surgeon-in-the-loop%20demonstrations.%20This%20results%20in%20a%0Amore%20intuitive%20and%20personalized%20surgical%20experience%20for%20the%20surgeon%20while%0Aensuring%20consistent%20performance%20for%20the%20autonomous%20robotic%20apprentice.%20We%0Adefine%20and%20evaluate%20the%20effectiveness%20of%20our%20approach%20in%20a%20simulated%0Aenvironment%20using%20our%20proposed%20metrics%20and%20highlight%20the%20trade-off%20between%20a%0Ageneric%20agent%20and%20a%20surgeon-centered%20adapted%20agent.%20Finally%2C%20our%20approach%20has%0Athe%20potential%20to%20extend%20to%20other%20ophthalmic%20and%20microsurgical%20procedures%2C%0Aopening%20the%20door%20to%20a%20new%20generation%20of%20surgeon-in-the-loop%20autonomous%20surgical%0Arobots.%20We%20provide%20an%20open-source%20simulation%20framework%20for%20future%20development%0Aand%20reproducibility%20at%0Ahttps%3A//github.com/amrgomaaelhady/CataractAdaptSurgRobot.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.17693v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520a%2520Surgeon-in-the-Loop%2520Ophthalmic%2520Robotic%2520Apprentice%2520using%250A%2520%2520Reinforcement%2520and%2520Imitation%2520Learning%26entry.906535625%3DAmr%2520Gomaa%2520and%2520Bilal%2520Mahdy%2520and%2520Niko%2520Kleer%2520and%2520Antonio%2520Kr%25C3%25BCger%26entry.1292438233%3D%2520%2520Robot-assisted%2520surgical%2520systems%2520have%2520demonstrated%2520significant%2520potential%2520in%250Aenhancing%2520surgical%2520precision%2520and%2520minimizing%2520human%2520errors.%2520However%252C%2520existing%250Asystems%2520cannot%2520accommodate%2520individual%2520surgeons%2527%2520unique%2520preferences%2520and%250Arequirements.%2520Additionally%252C%2520they%2520primarily%2520focus%2520on%2520general%2520surgeries%2520%2528e.g.%252C%250Alaparoscopy%2529%2520and%2520are%2520unsuitable%2520for%2520highly%2520precise%2520microsurgeries%252C%2520such%2520as%250Aophthalmic%2520procedures.%2520Thus%252C%2520we%2520propose%2520an%2520image-guided%2520approach%2520for%250Asurgeon-centered%2520autonomous%2520agents%2520that%2520can%2520adapt%2520to%2520the%2520individual%2520surgeon%2527s%250Askill%2520level%2520and%2520preferred%2520surgical%2520techniques%2520during%2520ophthalmic%2520cataract%250Asurgery.%2520Our%2520approach%2520trains%2520reinforcement%2520and%2520imitation%2520learning%2520agents%250Asimultaneously%2520using%2520curriculum%2520learning%2520approaches%2520guided%2520by%2520image%2520data%2520to%250Aperform%2520all%2520tasks%2520of%2520the%2520incision%2520phase%2520of%2520cataract%2520surgery.%2520By%2520integrating%2520the%250Asurgeon%2527s%2520actions%2520and%2520preferences%2520into%2520the%2520training%2520process%252C%2520our%2520approach%250Aenables%2520the%2520robot%2520to%2520implicitly%2520learn%2520and%2520adapt%2520to%2520the%2520individual%2520surgeon%2527s%250Aunique%2520techniques%2520through%2520surgeon-in-the-loop%2520demonstrations.%2520This%2520results%2520in%2520a%250Amore%2520intuitive%2520and%2520personalized%2520surgical%2520experience%2520for%2520the%2520surgeon%2520while%250Aensuring%2520consistent%2520performance%2520for%2520the%2520autonomous%2520robotic%2520apprentice.%2520We%250Adefine%2520and%2520evaluate%2520the%2520effectiveness%2520of%2520our%2520approach%2520in%2520a%2520simulated%250Aenvironment%2520using%2520our%2520proposed%2520metrics%2520and%2520highlight%2520the%2520trade-off%2520between%2520a%250Ageneric%2520agent%2520and%2520a%2520surgeon-centered%2520adapted%2520agent.%2520Finally%252C%2520our%2520approach%2520has%250Athe%2520potential%2520to%2520extend%2520to%2520other%2520ophthalmic%2520and%2520microsurgical%2520procedures%252C%250Aopening%2520the%2520door%2520to%2520a%2520new%2520generation%2520of%2520surgeon-in-the-loop%2520autonomous%2520surgical%250Arobots.%2520We%2520provide%2520an%2520open-source%2520simulation%2520framework%2520for%2520future%2520development%250Aand%2520reproducibility%2520at%250Ahttps%253A//github.com/amrgomaaelhady/CataractAdaptSurgRobot.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.17693v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20a%20Surgeon-in-the-Loop%20Ophthalmic%20Robotic%20Apprentice%20using%0A%20%20Reinforcement%20and%20Imitation%20Learning&entry.906535625=Amr%20Gomaa%20and%20Bilal%20Mahdy%20and%20Niko%20Kleer%20and%20Antonio%20Kr%C3%BCger&entry.1292438233=%20%20Robot-assisted%20surgical%20systems%20have%20demonstrated%20significant%20potential%20in%0Aenhancing%20surgical%20precision%20and%20minimizing%20human%20errors.%20However%2C%20existing%0Asystems%20cannot%20accommodate%20individual%20surgeons%27%20unique%20preferences%20and%0Arequirements.%20Additionally%2C%20they%20primarily%20focus%20on%20general%20surgeries%20%28e.g.%2C%0Alaparoscopy%29%20and%20are%20unsuitable%20for%20highly%20precise%20microsurgeries%2C%20such%20as%0Aophthalmic%20procedures.%20Thus%2C%20we%20propose%20an%20image-guided%20approach%20for%0Asurgeon-centered%20autonomous%20agents%20that%20can%20adapt%20to%20the%20individual%20surgeon%27s%0Askill%20level%20and%20preferred%20surgical%20techniques%20during%20ophthalmic%20cataract%0Asurgery.%20Our%20approach%20trains%20reinforcement%20and%20imitation%20learning%20agents%0Asimultaneously%20using%20curriculum%20learning%20approaches%20guided%20by%20image%20data%20to%0Aperform%20all%20tasks%20of%20the%20incision%20phase%20of%20cataract%20surgery.%20By%20integrating%20the%0Asurgeon%27s%20actions%20and%20preferences%20into%20the%20training%20process%2C%20our%20approach%0Aenables%20the%20robot%20to%20implicitly%20learn%20and%20adapt%20to%20the%20individual%20surgeon%27s%0Aunique%20techniques%20through%20surgeon-in-the-loop%20demonstrations.%20This%20results%20in%20a%0Amore%20intuitive%20and%20personalized%20surgical%20experience%20for%20the%20surgeon%20while%0Aensuring%20consistent%20performance%20for%20the%20autonomous%20robotic%20apprentice.%20We%0Adefine%20and%20evaluate%20the%20effectiveness%20of%20our%20approach%20in%20a%20simulated%0Aenvironment%20using%20our%20proposed%20metrics%20and%20highlight%20the%20trade-off%20between%20a%0Ageneric%20agent%20and%20a%20surgeon-centered%20adapted%20agent.%20Finally%2C%20our%20approach%20has%0Athe%20potential%20to%20extend%20to%20other%20ophthalmic%20and%20microsurgical%20procedures%2C%0Aopening%20the%20door%20to%20a%20new%20generation%20of%20surgeon-in-the-loop%20autonomous%20surgical%0Arobots.%20We%20provide%20an%20open-source%20simulation%20framework%20for%20future%20development%0Aand%20reproducibility%20at%0Ahttps%3A//github.com/amrgomaaelhady/CataractAdaptSurgRobot.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.17693v3&entry.124074799=Read"},
{"title": "Rethinking Video with a Universal Event-Based Representation", "author": "Andrew Freeman", "abstract": "  Traditionally, video is structured as a sequence of discrete image frames.\nRecently, however, a novel video sensing paradigm has emerged which eschews\nvideo frames entirely. These \"event\" sensors aim to mimic the human vision\nsystem with asynchronous sensing, where each pixel has an independent, sparse\ndata stream. While these cameras enable high-speed and high-dynamic-range\nsensing, researchers often revert to a framed representation of the event data\nfor existing applications, or build bespoke applications for a particular\ncamera's event data type. At the same time, classical video systems have\nsignificant computational redundancy at the application layer, since pixel\nsamples are repeated across frames in the uncompressed domain.\n  To address the shortcomings of existing systems, I introduce Address,\nDecimation, {\\Delta}t Event Representation (AD{\\Delta}ER, pronounced \"adder\"),\na novel intermediate video representation and system framework. The framework\ntranscodes a variety of framed and event camera sources into a single\nevent-based representation, which supports source-modeled lossy compression and\nbackward compatibility with traditional frame-based applications. I demonstrate\nthat AD{\\Delta}ER achieves state-of-the-art application speed and compression\nperformance for scenes with high temporal redundancy. Crucially, I describe how\nAD{\\Delta}ER unlocks an entirely new control mechanism for computer vision:\napplication speed can correlate with both the scene content and the level of\nlossy compression. Finally, I discuss the implications for event-based video on\nlarge-scale video surveillance and resource-constrained sensing.\n", "link": "http://arxiv.org/abs/2408.06248v1", "date": "2024-08-12", "relevancy": 2.1927, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.552}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.552}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5427}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Video%20with%20a%20Universal%20Event-Based%20Representation&body=Title%3A%20Rethinking%20Video%20with%20a%20Universal%20Event-Based%20Representation%0AAuthor%3A%20Andrew%20Freeman%0AAbstract%3A%20%20%20Traditionally%2C%20video%20is%20structured%20as%20a%20sequence%20of%20discrete%20image%20frames.%0ARecently%2C%20however%2C%20a%20novel%20video%20sensing%20paradigm%20has%20emerged%20which%20eschews%0Avideo%20frames%20entirely.%20These%20%22event%22%20sensors%20aim%20to%20mimic%20the%20human%20vision%0Asystem%20with%20asynchronous%20sensing%2C%20where%20each%20pixel%20has%20an%20independent%2C%20sparse%0Adata%20stream.%20While%20these%20cameras%20enable%20high-speed%20and%20high-dynamic-range%0Asensing%2C%20researchers%20often%20revert%20to%20a%20framed%20representation%20of%20the%20event%20data%0Afor%20existing%20applications%2C%20or%20build%20bespoke%20applications%20for%20a%20particular%0Acamera%27s%20event%20data%20type.%20At%20the%20same%20time%2C%20classical%20video%20systems%20have%0Asignificant%20computational%20redundancy%20at%20the%20application%20layer%2C%20since%20pixel%0Asamples%20are%20repeated%20across%20frames%20in%20the%20uncompressed%20domain.%0A%20%20To%20address%20the%20shortcomings%20of%20existing%20systems%2C%20I%20introduce%20Address%2C%0ADecimation%2C%20%7B%5CDelta%7Dt%20Event%20Representation%20%28AD%7B%5CDelta%7DER%2C%20pronounced%20%22adder%22%29%2C%0Aa%20novel%20intermediate%20video%20representation%20and%20system%20framework.%20The%20framework%0Atranscodes%20a%20variety%20of%20framed%20and%20event%20camera%20sources%20into%20a%20single%0Aevent-based%20representation%2C%20which%20supports%20source-modeled%20lossy%20compression%20and%0Abackward%20compatibility%20with%20traditional%20frame-based%20applications.%20I%20demonstrate%0Athat%20AD%7B%5CDelta%7DER%20achieves%20state-of-the-art%20application%20speed%20and%20compression%0Aperformance%20for%20scenes%20with%20high%20temporal%20redundancy.%20Crucially%2C%20I%20describe%20how%0AAD%7B%5CDelta%7DER%20unlocks%20an%20entirely%20new%20control%20mechanism%20for%20computer%20vision%3A%0Aapplication%20speed%20can%20correlate%20with%20both%20the%20scene%20content%20and%20the%20level%20of%0Alossy%20compression.%20Finally%2C%20I%20discuss%20the%20implications%20for%20event-based%20video%20on%0Alarge-scale%20video%20surveillance%20and%20resource-constrained%20sensing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06248v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Video%2520with%2520a%2520Universal%2520Event-Based%2520Representation%26entry.906535625%3DAndrew%2520Freeman%26entry.1292438233%3D%2520%2520Traditionally%252C%2520video%2520is%2520structured%2520as%2520a%2520sequence%2520of%2520discrete%2520image%2520frames.%250ARecently%252C%2520however%252C%2520a%2520novel%2520video%2520sensing%2520paradigm%2520has%2520emerged%2520which%2520eschews%250Avideo%2520frames%2520entirely.%2520These%2520%2522event%2522%2520sensors%2520aim%2520to%2520mimic%2520the%2520human%2520vision%250Asystem%2520with%2520asynchronous%2520sensing%252C%2520where%2520each%2520pixel%2520has%2520an%2520independent%252C%2520sparse%250Adata%2520stream.%2520While%2520these%2520cameras%2520enable%2520high-speed%2520and%2520high-dynamic-range%250Asensing%252C%2520researchers%2520often%2520revert%2520to%2520a%2520framed%2520representation%2520of%2520the%2520event%2520data%250Afor%2520existing%2520applications%252C%2520or%2520build%2520bespoke%2520applications%2520for%2520a%2520particular%250Acamera%2527s%2520event%2520data%2520type.%2520At%2520the%2520same%2520time%252C%2520classical%2520video%2520systems%2520have%250Asignificant%2520computational%2520redundancy%2520at%2520the%2520application%2520layer%252C%2520since%2520pixel%250Asamples%2520are%2520repeated%2520across%2520frames%2520in%2520the%2520uncompressed%2520domain.%250A%2520%2520To%2520address%2520the%2520shortcomings%2520of%2520existing%2520systems%252C%2520I%2520introduce%2520Address%252C%250ADecimation%252C%2520%257B%255CDelta%257Dt%2520Event%2520Representation%2520%2528AD%257B%255CDelta%257DER%252C%2520pronounced%2520%2522adder%2522%2529%252C%250Aa%2520novel%2520intermediate%2520video%2520representation%2520and%2520system%2520framework.%2520The%2520framework%250Atranscodes%2520a%2520variety%2520of%2520framed%2520and%2520event%2520camera%2520sources%2520into%2520a%2520single%250Aevent-based%2520representation%252C%2520which%2520supports%2520source-modeled%2520lossy%2520compression%2520and%250Abackward%2520compatibility%2520with%2520traditional%2520frame-based%2520applications.%2520I%2520demonstrate%250Athat%2520AD%257B%255CDelta%257DER%2520achieves%2520state-of-the-art%2520application%2520speed%2520and%2520compression%250Aperformance%2520for%2520scenes%2520with%2520high%2520temporal%2520redundancy.%2520Crucially%252C%2520I%2520describe%2520how%250AAD%257B%255CDelta%257DER%2520unlocks%2520an%2520entirely%2520new%2520control%2520mechanism%2520for%2520computer%2520vision%253A%250Aapplication%2520speed%2520can%2520correlate%2520with%2520both%2520the%2520scene%2520content%2520and%2520the%2520level%2520of%250Alossy%2520compression.%2520Finally%252C%2520I%2520discuss%2520the%2520implications%2520for%2520event-based%2520video%2520on%250Alarge-scale%2520video%2520surveillance%2520and%2520resource-constrained%2520sensing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06248v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Video%20with%20a%20Universal%20Event-Based%20Representation&entry.906535625=Andrew%20Freeman&entry.1292438233=%20%20Traditionally%2C%20video%20is%20structured%20as%20a%20sequence%20of%20discrete%20image%20frames.%0ARecently%2C%20however%2C%20a%20novel%20video%20sensing%20paradigm%20has%20emerged%20which%20eschews%0Avideo%20frames%20entirely.%20These%20%22event%22%20sensors%20aim%20to%20mimic%20the%20human%20vision%0Asystem%20with%20asynchronous%20sensing%2C%20where%20each%20pixel%20has%20an%20independent%2C%20sparse%0Adata%20stream.%20While%20these%20cameras%20enable%20high-speed%20and%20high-dynamic-range%0Asensing%2C%20researchers%20often%20revert%20to%20a%20framed%20representation%20of%20the%20event%20data%0Afor%20existing%20applications%2C%20or%20build%20bespoke%20applications%20for%20a%20particular%0Acamera%27s%20event%20data%20type.%20At%20the%20same%20time%2C%20classical%20video%20systems%20have%0Asignificant%20computational%20redundancy%20at%20the%20application%20layer%2C%20since%20pixel%0Asamples%20are%20repeated%20across%20frames%20in%20the%20uncompressed%20domain.%0A%20%20To%20address%20the%20shortcomings%20of%20existing%20systems%2C%20I%20introduce%20Address%2C%0ADecimation%2C%20%7B%5CDelta%7Dt%20Event%20Representation%20%28AD%7B%5CDelta%7DER%2C%20pronounced%20%22adder%22%29%2C%0Aa%20novel%20intermediate%20video%20representation%20and%20system%20framework.%20The%20framework%0Atranscodes%20a%20variety%20of%20framed%20and%20event%20camera%20sources%20into%20a%20single%0Aevent-based%20representation%2C%20which%20supports%20source-modeled%20lossy%20compression%20and%0Abackward%20compatibility%20with%20traditional%20frame-based%20applications.%20I%20demonstrate%0Athat%20AD%7B%5CDelta%7DER%20achieves%20state-of-the-art%20application%20speed%20and%20compression%0Aperformance%20for%20scenes%20with%20high%20temporal%20redundancy.%20Crucially%2C%20I%20describe%20how%0AAD%7B%5CDelta%7DER%20unlocks%20an%20entirely%20new%20control%20mechanism%20for%20computer%20vision%3A%0Aapplication%20speed%20can%20correlate%20with%20both%20the%20scene%20content%20and%20the%20level%20of%0Alossy%20compression.%20Finally%2C%20I%20discuss%20the%20implications%20for%20event-based%20video%20on%0Alarge-scale%20video%20surveillance%20and%20resource-constrained%20sensing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06248v1&entry.124074799=Read"},
{"title": "Learning Invariant Causal Mechanism from Vision-Language Models", "author": "Zeen Song and Siyu Zhao and Xingyu Zhang and Jiangmeng Li and Changwen Zheng and Wenwen Qiang", "abstract": "  Large-scale pre-trained vision-language models such as CLIP have been widely\napplied to a variety of downstream scenarios. In real-world applications, the\nCLIP model is often utilized in more diverse scenarios than those encountered\nduring its training, a challenge known as the out-of-distribution (OOD)\nproblem. However, our experiments reveal that CLIP performs unsatisfactorily in\ncertain domains. Through a causal analysis, we find that CLIP's current\nprediction process cannot guarantee a low OOD risk. The lowest OOD risk can be\nachieved when the prediction process is based on invariant causal mechanisms,\ni.e., predicting solely based on invariant latent factors. However, theoretical\nanalysis indicates that CLIP does not identify these invariant latent factors.\nTherefore, we propose the Invariant Causal Mechanism for CLIP (CLIP-ICM), a\nframework that first identifies invariant latent factors using interventional\ndata and then performs invariant predictions across various domains. Our method\nis simple yet effective, without significant computational overhead.\nExperimental results demonstrate that CLIP-ICM significantly improves CLIP's\nperformance in OOD scenarios.\n", "link": "http://arxiv.org/abs/2405.15289v2", "date": "2024-08-12", "relevancy": 2.1836, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5536}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.547}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5377}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Invariant%20Causal%20Mechanism%20from%20Vision-Language%20Models&body=Title%3A%20Learning%20Invariant%20Causal%20Mechanism%20from%20Vision-Language%20Models%0AAuthor%3A%20Zeen%20Song%20and%20Siyu%20Zhao%20and%20Xingyu%20Zhang%20and%20Jiangmeng%20Li%20and%20Changwen%20Zheng%20and%20Wenwen%20Qiang%0AAbstract%3A%20%20%20Large-scale%20pre-trained%20vision-language%20models%20such%20as%20CLIP%20have%20been%20widely%0Aapplied%20to%20a%20variety%20of%20downstream%20scenarios.%20In%20real-world%20applications%2C%20the%0ACLIP%20model%20is%20often%20utilized%20in%20more%20diverse%20scenarios%20than%20those%20encountered%0Aduring%20its%20training%2C%20a%20challenge%20known%20as%20the%20out-of-distribution%20%28OOD%29%0Aproblem.%20However%2C%20our%20experiments%20reveal%20that%20CLIP%20performs%20unsatisfactorily%20in%0Acertain%20domains.%20Through%20a%20causal%20analysis%2C%20we%20find%20that%20CLIP%27s%20current%0Aprediction%20process%20cannot%20guarantee%20a%20low%20OOD%20risk.%20The%20lowest%20OOD%20risk%20can%20be%0Aachieved%20when%20the%20prediction%20process%20is%20based%20on%20invariant%20causal%20mechanisms%2C%0Ai.e.%2C%20predicting%20solely%20based%20on%20invariant%20latent%20factors.%20However%2C%20theoretical%0Aanalysis%20indicates%20that%20CLIP%20does%20not%20identify%20these%20invariant%20latent%20factors.%0ATherefore%2C%20we%20propose%20the%20Invariant%20Causal%20Mechanism%20for%20CLIP%20%28CLIP-ICM%29%2C%20a%0Aframework%20that%20first%20identifies%20invariant%20latent%20factors%20using%20interventional%0Adata%20and%20then%20performs%20invariant%20predictions%20across%20various%20domains.%20Our%20method%0Ais%20simple%20yet%20effective%2C%20without%20significant%20computational%20overhead.%0AExperimental%20results%20demonstrate%20that%20CLIP-ICM%20significantly%20improves%20CLIP%27s%0Aperformance%20in%20OOD%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15289v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Invariant%2520Causal%2520Mechanism%2520from%2520Vision-Language%2520Models%26entry.906535625%3DZeen%2520Song%2520and%2520Siyu%2520Zhao%2520and%2520Xingyu%2520Zhang%2520and%2520Jiangmeng%2520Li%2520and%2520Changwen%2520Zheng%2520and%2520Wenwen%2520Qiang%26entry.1292438233%3D%2520%2520Large-scale%2520pre-trained%2520vision-language%2520models%2520such%2520as%2520CLIP%2520have%2520been%2520widely%250Aapplied%2520to%2520a%2520variety%2520of%2520downstream%2520scenarios.%2520In%2520real-world%2520applications%252C%2520the%250ACLIP%2520model%2520is%2520often%2520utilized%2520in%2520more%2520diverse%2520scenarios%2520than%2520those%2520encountered%250Aduring%2520its%2520training%252C%2520a%2520challenge%2520known%2520as%2520the%2520out-of-distribution%2520%2528OOD%2529%250Aproblem.%2520However%252C%2520our%2520experiments%2520reveal%2520that%2520CLIP%2520performs%2520unsatisfactorily%2520in%250Acertain%2520domains.%2520Through%2520a%2520causal%2520analysis%252C%2520we%2520find%2520that%2520CLIP%2527s%2520current%250Aprediction%2520process%2520cannot%2520guarantee%2520a%2520low%2520OOD%2520risk.%2520The%2520lowest%2520OOD%2520risk%2520can%2520be%250Aachieved%2520when%2520the%2520prediction%2520process%2520is%2520based%2520on%2520invariant%2520causal%2520mechanisms%252C%250Ai.e.%252C%2520predicting%2520solely%2520based%2520on%2520invariant%2520latent%2520factors.%2520However%252C%2520theoretical%250Aanalysis%2520indicates%2520that%2520CLIP%2520does%2520not%2520identify%2520these%2520invariant%2520latent%2520factors.%250ATherefore%252C%2520we%2520propose%2520the%2520Invariant%2520Causal%2520Mechanism%2520for%2520CLIP%2520%2528CLIP-ICM%2529%252C%2520a%250Aframework%2520that%2520first%2520identifies%2520invariant%2520latent%2520factors%2520using%2520interventional%250Adata%2520and%2520then%2520performs%2520invariant%2520predictions%2520across%2520various%2520domains.%2520Our%2520method%250Ais%2520simple%2520yet%2520effective%252C%2520without%2520significant%2520computational%2520overhead.%250AExperimental%2520results%2520demonstrate%2520that%2520CLIP-ICM%2520significantly%2520improves%2520CLIP%2527s%250Aperformance%2520in%2520OOD%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15289v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Invariant%20Causal%20Mechanism%20from%20Vision-Language%20Models&entry.906535625=Zeen%20Song%20and%20Siyu%20Zhao%20and%20Xingyu%20Zhang%20and%20Jiangmeng%20Li%20and%20Changwen%20Zheng%20and%20Wenwen%20Qiang&entry.1292438233=%20%20Large-scale%20pre-trained%20vision-language%20models%20such%20as%20CLIP%20have%20been%20widely%0Aapplied%20to%20a%20variety%20of%20downstream%20scenarios.%20In%20real-world%20applications%2C%20the%0ACLIP%20model%20is%20often%20utilized%20in%20more%20diverse%20scenarios%20than%20those%20encountered%0Aduring%20its%20training%2C%20a%20challenge%20known%20as%20the%20out-of-distribution%20%28OOD%29%0Aproblem.%20However%2C%20our%20experiments%20reveal%20that%20CLIP%20performs%20unsatisfactorily%20in%0Acertain%20domains.%20Through%20a%20causal%20analysis%2C%20we%20find%20that%20CLIP%27s%20current%0Aprediction%20process%20cannot%20guarantee%20a%20low%20OOD%20risk.%20The%20lowest%20OOD%20risk%20can%20be%0Aachieved%20when%20the%20prediction%20process%20is%20based%20on%20invariant%20causal%20mechanisms%2C%0Ai.e.%2C%20predicting%20solely%20based%20on%20invariant%20latent%20factors.%20However%2C%20theoretical%0Aanalysis%20indicates%20that%20CLIP%20does%20not%20identify%20these%20invariant%20latent%20factors.%0ATherefore%2C%20we%20propose%20the%20Invariant%20Causal%20Mechanism%20for%20CLIP%20%28CLIP-ICM%29%2C%20a%0Aframework%20that%20first%20identifies%20invariant%20latent%20factors%20using%20interventional%0Adata%20and%20then%20performs%20invariant%20predictions%20across%20various%20domains.%20Our%20method%0Ais%20simple%20yet%20effective%2C%20without%20significant%20computational%20overhead.%0AExperimental%20results%20demonstrate%20that%20CLIP-ICM%20significantly%20improves%20CLIP%27s%0Aperformance%20in%20OOD%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15289v2&entry.124074799=Read"},
{"title": "Moderating Illicit Online Image Promotion for Unsafe User-Generated\n  Content Games Using Large Vision-Language Models", "author": "Keyan Guo and Ayush Utkarsh and Wenbo Ding and Isabelle Ondracek and Ziming Zhao and Guo Freeman and Nishant Vishwamitra and Hongxin Hu", "abstract": "  Online user generated content games (UGCGs) are increasingly popular among\nchildren and adolescents for social interaction and more creative online\nentertainment. However, they pose a heightened risk of exposure to explicit\ncontent, raising growing concerns for the online safety of children and\nadolescents. Despite these concerns, few studies have addressed the issue of\nillicit image-based promotions of unsafe UGCGs on social media, which can\ninadvertently attract young users. This challenge arises from the difficulty of\nobtaining comprehensive training data for UGCG images and the unique nature of\nthese images, which differ from traditional unsafe content. In this work, we\ntake the first step towards studying the threat of illicit promotions of unsafe\nUGCGs. We collect a real-world dataset comprising 2,924 images that display\ndiverse sexually explicit and violent content used to promote UGCGs by their\ngame creators. Our in-depth studies reveal a new understanding of this problem\nand the urgent need for automatically flagging illicit UGCG promotions. We\nadditionally create a cutting-edge system, UGCG-Guard, designed to aid social\nmedia platforms in effectively identifying images used for illicit UGCG\npromotions. This system leverages recently introduced large vision-language\nmodels (VLMs) and employs a novel conditional prompting strategy for zero-shot\ndomain adaptation, along with chain-of-thought (CoT) reasoning for contextual\nidentification. UGCG-Guard achieves outstanding results, with an accuracy rate\nof 94% in detecting these images used for the illicit promotion of such games\nin real-world scenarios.\n", "link": "http://arxiv.org/abs/2403.18957v2", "date": "2024-08-12", "relevancy": 2.1708, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5651}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5268}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Moderating%20Illicit%20Online%20Image%20Promotion%20for%20Unsafe%20User-Generated%0A%20%20Content%20Games%20Using%20Large%20Vision-Language%20Models&body=Title%3A%20Moderating%20Illicit%20Online%20Image%20Promotion%20for%20Unsafe%20User-Generated%0A%20%20Content%20Games%20Using%20Large%20Vision-Language%20Models%0AAuthor%3A%20Keyan%20Guo%20and%20Ayush%20Utkarsh%20and%20Wenbo%20Ding%20and%20Isabelle%20Ondracek%20and%20Ziming%20Zhao%20and%20Guo%20Freeman%20and%20Nishant%20Vishwamitra%20and%20Hongxin%20Hu%0AAbstract%3A%20%20%20Online%20user%20generated%20content%20games%20%28UGCGs%29%20are%20increasingly%20popular%20among%0Achildren%20and%20adolescents%20for%20social%20interaction%20and%20more%20creative%20online%0Aentertainment.%20However%2C%20they%20pose%20a%20heightened%20risk%20of%20exposure%20to%20explicit%0Acontent%2C%20raising%20growing%20concerns%20for%20the%20online%20safety%20of%20children%20and%0Aadolescents.%20Despite%20these%20concerns%2C%20few%20studies%20have%20addressed%20the%20issue%20of%0Aillicit%20image-based%20promotions%20of%20unsafe%20UGCGs%20on%20social%20media%2C%20which%20can%0Ainadvertently%20attract%20young%20users.%20This%20challenge%20arises%20from%20the%20difficulty%20of%0Aobtaining%20comprehensive%20training%20data%20for%20UGCG%20images%20and%20the%20unique%20nature%20of%0Athese%20images%2C%20which%20differ%20from%20traditional%20unsafe%20content.%20In%20this%20work%2C%20we%0Atake%20the%20first%20step%20towards%20studying%20the%20threat%20of%20illicit%20promotions%20of%20unsafe%0AUGCGs.%20We%20collect%20a%20real-world%20dataset%20comprising%202%2C924%20images%20that%20display%0Adiverse%20sexually%20explicit%20and%20violent%20content%20used%20to%20promote%20UGCGs%20by%20their%0Agame%20creators.%20Our%20in-depth%20studies%20reveal%20a%20new%20understanding%20of%20this%20problem%0Aand%20the%20urgent%20need%20for%20automatically%20flagging%20illicit%20UGCG%20promotions.%20We%0Aadditionally%20create%20a%20cutting-edge%20system%2C%20UGCG-Guard%2C%20designed%20to%20aid%20social%0Amedia%20platforms%20in%20effectively%20identifying%20images%20used%20for%20illicit%20UGCG%0Apromotions.%20This%20system%20leverages%20recently%20introduced%20large%20vision-language%0Amodels%20%28VLMs%29%20and%20employs%20a%20novel%20conditional%20prompting%20strategy%20for%20zero-shot%0Adomain%20adaptation%2C%20along%20with%20chain-of-thought%20%28CoT%29%20reasoning%20for%20contextual%0Aidentification.%20UGCG-Guard%20achieves%20outstanding%20results%2C%20with%20an%20accuracy%20rate%0Aof%2094%25%20in%20detecting%20these%20images%20used%20for%20the%20illicit%20promotion%20of%20such%20games%0Ain%20real-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18957v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModerating%2520Illicit%2520Online%2520Image%2520Promotion%2520for%2520Unsafe%2520User-Generated%250A%2520%2520Content%2520Games%2520Using%2520Large%2520Vision-Language%2520Models%26entry.906535625%3DKeyan%2520Guo%2520and%2520Ayush%2520Utkarsh%2520and%2520Wenbo%2520Ding%2520and%2520Isabelle%2520Ondracek%2520and%2520Ziming%2520Zhao%2520and%2520Guo%2520Freeman%2520and%2520Nishant%2520Vishwamitra%2520and%2520Hongxin%2520Hu%26entry.1292438233%3D%2520%2520Online%2520user%2520generated%2520content%2520games%2520%2528UGCGs%2529%2520are%2520increasingly%2520popular%2520among%250Achildren%2520and%2520adolescents%2520for%2520social%2520interaction%2520and%2520more%2520creative%2520online%250Aentertainment.%2520However%252C%2520they%2520pose%2520a%2520heightened%2520risk%2520of%2520exposure%2520to%2520explicit%250Acontent%252C%2520raising%2520growing%2520concerns%2520for%2520the%2520online%2520safety%2520of%2520children%2520and%250Aadolescents.%2520Despite%2520these%2520concerns%252C%2520few%2520studies%2520have%2520addressed%2520the%2520issue%2520of%250Aillicit%2520image-based%2520promotions%2520of%2520unsafe%2520UGCGs%2520on%2520social%2520media%252C%2520which%2520can%250Ainadvertently%2520attract%2520young%2520users.%2520This%2520challenge%2520arises%2520from%2520the%2520difficulty%2520of%250Aobtaining%2520comprehensive%2520training%2520data%2520for%2520UGCG%2520images%2520and%2520the%2520unique%2520nature%2520of%250Athese%2520images%252C%2520which%2520differ%2520from%2520traditional%2520unsafe%2520content.%2520In%2520this%2520work%252C%2520we%250Atake%2520the%2520first%2520step%2520towards%2520studying%2520the%2520threat%2520of%2520illicit%2520promotions%2520of%2520unsafe%250AUGCGs.%2520We%2520collect%2520a%2520real-world%2520dataset%2520comprising%25202%252C924%2520images%2520that%2520display%250Adiverse%2520sexually%2520explicit%2520and%2520violent%2520content%2520used%2520to%2520promote%2520UGCGs%2520by%2520their%250Agame%2520creators.%2520Our%2520in-depth%2520studies%2520reveal%2520a%2520new%2520understanding%2520of%2520this%2520problem%250Aand%2520the%2520urgent%2520need%2520for%2520automatically%2520flagging%2520illicit%2520UGCG%2520promotions.%2520We%250Aadditionally%2520create%2520a%2520cutting-edge%2520system%252C%2520UGCG-Guard%252C%2520designed%2520to%2520aid%2520social%250Amedia%2520platforms%2520in%2520effectively%2520identifying%2520images%2520used%2520for%2520illicit%2520UGCG%250Apromotions.%2520This%2520system%2520leverages%2520recently%2520introduced%2520large%2520vision-language%250Amodels%2520%2528VLMs%2529%2520and%2520employs%2520a%2520novel%2520conditional%2520prompting%2520strategy%2520for%2520zero-shot%250Adomain%2520adaptation%252C%2520along%2520with%2520chain-of-thought%2520%2528CoT%2529%2520reasoning%2520for%2520contextual%250Aidentification.%2520UGCG-Guard%2520achieves%2520outstanding%2520results%252C%2520with%2520an%2520accuracy%2520rate%250Aof%252094%2525%2520in%2520detecting%2520these%2520images%2520used%2520for%2520the%2520illicit%2520promotion%2520of%2520such%2520games%250Ain%2520real-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.18957v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Moderating%20Illicit%20Online%20Image%20Promotion%20for%20Unsafe%20User-Generated%0A%20%20Content%20Games%20Using%20Large%20Vision-Language%20Models&entry.906535625=Keyan%20Guo%20and%20Ayush%20Utkarsh%20and%20Wenbo%20Ding%20and%20Isabelle%20Ondracek%20and%20Ziming%20Zhao%20and%20Guo%20Freeman%20and%20Nishant%20Vishwamitra%20and%20Hongxin%20Hu&entry.1292438233=%20%20Online%20user%20generated%20content%20games%20%28UGCGs%29%20are%20increasingly%20popular%20among%0Achildren%20and%20adolescents%20for%20social%20interaction%20and%20more%20creative%20online%0Aentertainment.%20However%2C%20they%20pose%20a%20heightened%20risk%20of%20exposure%20to%20explicit%0Acontent%2C%20raising%20growing%20concerns%20for%20the%20online%20safety%20of%20children%20and%0Aadolescents.%20Despite%20these%20concerns%2C%20few%20studies%20have%20addressed%20the%20issue%20of%0Aillicit%20image-based%20promotions%20of%20unsafe%20UGCGs%20on%20social%20media%2C%20which%20can%0Ainadvertently%20attract%20young%20users.%20This%20challenge%20arises%20from%20the%20difficulty%20of%0Aobtaining%20comprehensive%20training%20data%20for%20UGCG%20images%20and%20the%20unique%20nature%20of%0Athese%20images%2C%20which%20differ%20from%20traditional%20unsafe%20content.%20In%20this%20work%2C%20we%0Atake%20the%20first%20step%20towards%20studying%20the%20threat%20of%20illicit%20promotions%20of%20unsafe%0AUGCGs.%20We%20collect%20a%20real-world%20dataset%20comprising%202%2C924%20images%20that%20display%0Adiverse%20sexually%20explicit%20and%20violent%20content%20used%20to%20promote%20UGCGs%20by%20their%0Agame%20creators.%20Our%20in-depth%20studies%20reveal%20a%20new%20understanding%20of%20this%20problem%0Aand%20the%20urgent%20need%20for%20automatically%20flagging%20illicit%20UGCG%20promotions.%20We%0Aadditionally%20create%20a%20cutting-edge%20system%2C%20UGCG-Guard%2C%20designed%20to%20aid%20social%0Amedia%20platforms%20in%20effectively%20identifying%20images%20used%20for%20illicit%20UGCG%0Apromotions.%20This%20system%20leverages%20recently%20introduced%20large%20vision-language%0Amodels%20%28VLMs%29%20and%20employs%20a%20novel%20conditional%20prompting%20strategy%20for%20zero-shot%0Adomain%20adaptation%2C%20along%20with%20chain-of-thought%20%28CoT%29%20reasoning%20for%20contextual%0Aidentification.%20UGCG-Guard%20achieves%20outstanding%20results%2C%20with%20an%20accuracy%20rate%0Aof%2094%25%20in%20detecting%20these%20images%20used%20for%20the%20illicit%20promotion%20of%20such%20games%0Ain%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18957v2&entry.124074799=Read"},
{"title": "Understanding Byzantine Robustness in Federated Learning with A\n  Black-box Server", "author": "Fangyuan Zhao and Yuexiang Xie and Xuebin Ren and Bolin Ding and Shusen Yang and Yaliang Li", "abstract": "  Federated learning (FL) becomes vulnerable to Byzantine attacks where some of\nparticipators tend to damage the utility or discourage the convergence of the\nlearned model via sending their malicious model updates. Previous works propose\nto apply robust rules to aggregate updates from participators against different\ntypes of Byzantine attacks, while at the same time, attackers can further\ndesign advanced Byzantine attack algorithms targeting specific aggregation rule\nwhen it is known. In practice, FL systems can involve a black-box server that\nmakes the adopted aggregation rule inaccessible to participants, which can\nnaturally defend or weaken some Byzantine attacks. In this paper, we provide an\nin-depth understanding on the Byzantine robustness of the FL system with a\nblack-box server. Our investigation demonstrates the improved Byzantine\nrobustness of a black-box server employing a dynamic defense strategy. We\nprovide both empirical evidence and theoretical analysis to reveal that the\nblack-box server can mitigate the worst-case attack impact from a maximum level\nto an expectation level, which is attributed to the inherent inaccessibility\nand randomness offered by a black-box server.The source code is available at\nhttps://github.com/alibaba/FederatedScope/tree/Byzantine_attack_defense to\npromote further research in the community.\n", "link": "http://arxiv.org/abs/2408.06042v1", "date": "2024-08-12", "relevancy": 2.1411, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4403}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4338}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4105}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Byzantine%20Robustness%20in%20Federated%20Learning%20with%20A%0A%20%20Black-box%20Server&body=Title%3A%20Understanding%20Byzantine%20Robustness%20in%20Federated%20Learning%20with%20A%0A%20%20Black-box%20Server%0AAuthor%3A%20Fangyuan%20Zhao%20and%20Yuexiang%20Xie%20and%20Xuebin%20Ren%20and%20Bolin%20Ding%20and%20Shusen%20Yang%20and%20Yaliang%20Li%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20becomes%20vulnerable%20to%20Byzantine%20attacks%20where%20some%20of%0Aparticipators%20tend%20to%20damage%20the%20utility%20or%20discourage%20the%20convergence%20of%20the%0Alearned%20model%20via%20sending%20their%20malicious%20model%20updates.%20Previous%20works%20propose%0Ato%20apply%20robust%20rules%20to%20aggregate%20updates%20from%20participators%20against%20different%0Atypes%20of%20Byzantine%20attacks%2C%20while%20at%20the%20same%20time%2C%20attackers%20can%20further%0Adesign%20advanced%20Byzantine%20attack%20algorithms%20targeting%20specific%20aggregation%20rule%0Awhen%20it%20is%20known.%20In%20practice%2C%20FL%20systems%20can%20involve%20a%20black-box%20server%20that%0Amakes%20the%20adopted%20aggregation%20rule%20inaccessible%20to%20participants%2C%20which%20can%0Anaturally%20defend%20or%20weaken%20some%20Byzantine%20attacks.%20In%20this%20paper%2C%20we%20provide%20an%0Ain-depth%20understanding%20on%20the%20Byzantine%20robustness%20of%20the%20FL%20system%20with%20a%0Ablack-box%20server.%20Our%20investigation%20demonstrates%20the%20improved%20Byzantine%0Arobustness%20of%20a%20black-box%20server%20employing%20a%20dynamic%20defense%20strategy.%20We%0Aprovide%20both%20empirical%20evidence%20and%20theoretical%20analysis%20to%20reveal%20that%20the%0Ablack-box%20server%20can%20mitigate%20the%20worst-case%20attack%20impact%20from%20a%20maximum%20level%0Ato%20an%20expectation%20level%2C%20which%20is%20attributed%20to%20the%20inherent%20inaccessibility%0Aand%20randomness%20offered%20by%20a%20black-box%20server.The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/alibaba/FederatedScope/tree/Byzantine_attack_defense%20to%0Apromote%20further%20research%20in%20the%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06042v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Byzantine%2520Robustness%2520in%2520Federated%2520Learning%2520with%2520A%250A%2520%2520Black-box%2520Server%26entry.906535625%3DFangyuan%2520Zhao%2520and%2520Yuexiang%2520Xie%2520and%2520Xuebin%2520Ren%2520and%2520Bolin%2520Ding%2520and%2520Shusen%2520Yang%2520and%2520Yaliang%2520Li%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520becomes%2520vulnerable%2520to%2520Byzantine%2520attacks%2520where%2520some%2520of%250Aparticipators%2520tend%2520to%2520damage%2520the%2520utility%2520or%2520discourage%2520the%2520convergence%2520of%2520the%250Alearned%2520model%2520via%2520sending%2520their%2520malicious%2520model%2520updates.%2520Previous%2520works%2520propose%250Ato%2520apply%2520robust%2520rules%2520to%2520aggregate%2520updates%2520from%2520participators%2520against%2520different%250Atypes%2520of%2520Byzantine%2520attacks%252C%2520while%2520at%2520the%2520same%2520time%252C%2520attackers%2520can%2520further%250Adesign%2520advanced%2520Byzantine%2520attack%2520algorithms%2520targeting%2520specific%2520aggregation%2520rule%250Awhen%2520it%2520is%2520known.%2520In%2520practice%252C%2520FL%2520systems%2520can%2520involve%2520a%2520black-box%2520server%2520that%250Amakes%2520the%2520adopted%2520aggregation%2520rule%2520inaccessible%2520to%2520participants%252C%2520which%2520can%250Anaturally%2520defend%2520or%2520weaken%2520some%2520Byzantine%2520attacks.%2520In%2520this%2520paper%252C%2520we%2520provide%2520an%250Ain-depth%2520understanding%2520on%2520the%2520Byzantine%2520robustness%2520of%2520the%2520FL%2520system%2520with%2520a%250Ablack-box%2520server.%2520Our%2520investigation%2520demonstrates%2520the%2520improved%2520Byzantine%250Arobustness%2520of%2520a%2520black-box%2520server%2520employing%2520a%2520dynamic%2520defense%2520strategy.%2520We%250Aprovide%2520both%2520empirical%2520evidence%2520and%2520theoretical%2520analysis%2520to%2520reveal%2520that%2520the%250Ablack-box%2520server%2520can%2520mitigate%2520the%2520worst-case%2520attack%2520impact%2520from%2520a%2520maximum%2520level%250Ato%2520an%2520expectation%2520level%252C%2520which%2520is%2520attributed%2520to%2520the%2520inherent%2520inaccessibility%250Aand%2520randomness%2520offered%2520by%2520a%2520black-box%2520server.The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/alibaba/FederatedScope/tree/Byzantine_attack_defense%2520to%250Apromote%2520further%2520research%2520in%2520the%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06042v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Byzantine%20Robustness%20in%20Federated%20Learning%20with%20A%0A%20%20Black-box%20Server&entry.906535625=Fangyuan%20Zhao%20and%20Yuexiang%20Xie%20and%20Xuebin%20Ren%20and%20Bolin%20Ding%20and%20Shusen%20Yang%20and%20Yaliang%20Li&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20becomes%20vulnerable%20to%20Byzantine%20attacks%20where%20some%20of%0Aparticipators%20tend%20to%20damage%20the%20utility%20or%20discourage%20the%20convergence%20of%20the%0Alearned%20model%20via%20sending%20their%20malicious%20model%20updates.%20Previous%20works%20propose%0Ato%20apply%20robust%20rules%20to%20aggregate%20updates%20from%20participators%20against%20different%0Atypes%20of%20Byzantine%20attacks%2C%20while%20at%20the%20same%20time%2C%20attackers%20can%20further%0Adesign%20advanced%20Byzantine%20attack%20algorithms%20targeting%20specific%20aggregation%20rule%0Awhen%20it%20is%20known.%20In%20practice%2C%20FL%20systems%20can%20involve%20a%20black-box%20server%20that%0Amakes%20the%20adopted%20aggregation%20rule%20inaccessible%20to%20participants%2C%20which%20can%0Anaturally%20defend%20or%20weaken%20some%20Byzantine%20attacks.%20In%20this%20paper%2C%20we%20provide%20an%0Ain-depth%20understanding%20on%20the%20Byzantine%20robustness%20of%20the%20FL%20system%20with%20a%0Ablack-box%20server.%20Our%20investigation%20demonstrates%20the%20improved%20Byzantine%0Arobustness%20of%20a%20black-box%20server%20employing%20a%20dynamic%20defense%20strategy.%20We%0Aprovide%20both%20empirical%20evidence%20and%20theoretical%20analysis%20to%20reveal%20that%20the%0Ablack-box%20server%20can%20mitigate%20the%20worst-case%20attack%20impact%20from%20a%20maximum%20level%0Ato%20an%20expectation%20level%2C%20which%20is%20attributed%20to%20the%20inherent%20inaccessibility%0Aand%20randomness%20offered%20by%20a%20black-box%20server.The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/alibaba/FederatedScope/tree/Byzantine_attack_defense%20to%0Apromote%20further%20research%20in%20the%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06042v1&entry.124074799=Read"},
{"title": "Towards Adversarial Robustness via Debiased High-Confidence Logit\n  Alignment", "author": "Kejia Zhang and Juanjuan Weng and Zhiming Luo and Shaozi Li", "abstract": "  Despite the significant advances that deep neural networks (DNNs) have\nachieved in various visual tasks, they still exhibit vulnerability to\nadversarial examples, leading to serious security concerns. Recent adversarial\ntraining techniques have utilized inverse adversarial attacks to generate\nhigh-confidence examples, aiming to align the distributions of adversarial\nexamples with the high-confidence regions of their corresponding classes.\nHowever, in this paper, our investigation reveals that high-confidence outputs\nunder inverse adversarial attacks are correlated with biased feature\nactivation. Specifically, training with inverse adversarial examples causes the\nmodel's attention to shift towards background features, introducing a spurious\ncorrelation bias. To address this bias, we propose Debiased High-Confidence\nAdversarial Training (DHAT), a novel approach that not only aligns the logits\nof adversarial examples with debiased high-confidence logits obtained from\ninverse adversarial examples, but also restores the model's attention to its\nnormal state by enhancing foreground logit orthogonality. Extensive experiments\ndemonstrate that DHAT achieves state-of-the-art performance and exhibits robust\ngeneralization capabilities across various vision datasets. Additionally, DHAT\ncan seamlessly integrate with existing advanced adversarial training techniques\nfor improving the performance.\n", "link": "http://arxiv.org/abs/2408.06079v1", "date": "2024-08-12", "relevancy": 2.1386, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5394}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5329}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5271}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Adversarial%20Robustness%20via%20Debiased%20High-Confidence%20Logit%0A%20%20Alignment&body=Title%3A%20Towards%20Adversarial%20Robustness%20via%20Debiased%20High-Confidence%20Logit%0A%20%20Alignment%0AAuthor%3A%20Kejia%20Zhang%20and%20Juanjuan%20Weng%20and%20Zhiming%20Luo%20and%20Shaozi%20Li%0AAbstract%3A%20%20%20Despite%20the%20significant%20advances%20that%20deep%20neural%20networks%20%28DNNs%29%20have%0Aachieved%20in%20various%20visual%20tasks%2C%20they%20still%20exhibit%20vulnerability%20to%0Aadversarial%20examples%2C%20leading%20to%20serious%20security%20concerns.%20Recent%20adversarial%0Atraining%20techniques%20have%20utilized%20inverse%20adversarial%20attacks%20to%20generate%0Ahigh-confidence%20examples%2C%20aiming%20to%20align%20the%20distributions%20of%20adversarial%0Aexamples%20with%20the%20high-confidence%20regions%20of%20their%20corresponding%20classes.%0AHowever%2C%20in%20this%20paper%2C%20our%20investigation%20reveals%20that%20high-confidence%20outputs%0Aunder%20inverse%20adversarial%20attacks%20are%20correlated%20with%20biased%20feature%0Aactivation.%20Specifically%2C%20training%20with%20inverse%20adversarial%20examples%20causes%20the%0Amodel%27s%20attention%20to%20shift%20towards%20background%20features%2C%20introducing%20a%20spurious%0Acorrelation%20bias.%20To%20address%20this%20bias%2C%20we%20propose%20Debiased%20High-Confidence%0AAdversarial%20Training%20%28DHAT%29%2C%20a%20novel%20approach%20that%20not%20only%20aligns%20the%20logits%0Aof%20adversarial%20examples%20with%20debiased%20high-confidence%20logits%20obtained%20from%0Ainverse%20adversarial%20examples%2C%20but%20also%20restores%20the%20model%27s%20attention%20to%20its%0Anormal%20state%20by%20enhancing%20foreground%20logit%20orthogonality.%20Extensive%20experiments%0Ademonstrate%20that%20DHAT%20achieves%20state-of-the-art%20performance%20and%20exhibits%20robust%0Ageneralization%20capabilities%20across%20various%20vision%20datasets.%20Additionally%2C%20DHAT%0Acan%20seamlessly%20integrate%20with%20existing%20advanced%20adversarial%20training%20techniques%0Afor%20improving%20the%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06079v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Adversarial%2520Robustness%2520via%2520Debiased%2520High-Confidence%2520Logit%250A%2520%2520Alignment%26entry.906535625%3DKejia%2520Zhang%2520and%2520Juanjuan%2520Weng%2520and%2520Zhiming%2520Luo%2520and%2520Shaozi%2520Li%26entry.1292438233%3D%2520%2520Despite%2520the%2520significant%2520advances%2520that%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520have%250Aachieved%2520in%2520various%2520visual%2520tasks%252C%2520they%2520still%2520exhibit%2520vulnerability%2520to%250Aadversarial%2520examples%252C%2520leading%2520to%2520serious%2520security%2520concerns.%2520Recent%2520adversarial%250Atraining%2520techniques%2520have%2520utilized%2520inverse%2520adversarial%2520attacks%2520to%2520generate%250Ahigh-confidence%2520examples%252C%2520aiming%2520to%2520align%2520the%2520distributions%2520of%2520adversarial%250Aexamples%2520with%2520the%2520high-confidence%2520regions%2520of%2520their%2520corresponding%2520classes.%250AHowever%252C%2520in%2520this%2520paper%252C%2520our%2520investigation%2520reveals%2520that%2520high-confidence%2520outputs%250Aunder%2520inverse%2520adversarial%2520attacks%2520are%2520correlated%2520with%2520biased%2520feature%250Aactivation.%2520Specifically%252C%2520training%2520with%2520inverse%2520adversarial%2520examples%2520causes%2520the%250Amodel%2527s%2520attention%2520to%2520shift%2520towards%2520background%2520features%252C%2520introducing%2520a%2520spurious%250Acorrelation%2520bias.%2520To%2520address%2520this%2520bias%252C%2520we%2520propose%2520Debiased%2520High-Confidence%250AAdversarial%2520Training%2520%2528DHAT%2529%252C%2520a%2520novel%2520approach%2520that%2520not%2520only%2520aligns%2520the%2520logits%250Aof%2520adversarial%2520examples%2520with%2520debiased%2520high-confidence%2520logits%2520obtained%2520from%250Ainverse%2520adversarial%2520examples%252C%2520but%2520also%2520restores%2520the%2520model%2527s%2520attention%2520to%2520its%250Anormal%2520state%2520by%2520enhancing%2520foreground%2520logit%2520orthogonality.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520DHAT%2520achieves%2520state-of-the-art%2520performance%2520and%2520exhibits%2520robust%250Ageneralization%2520capabilities%2520across%2520various%2520vision%2520datasets.%2520Additionally%252C%2520DHAT%250Acan%2520seamlessly%2520integrate%2520with%2520existing%2520advanced%2520adversarial%2520training%2520techniques%250Afor%2520improving%2520the%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06079v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Adversarial%20Robustness%20via%20Debiased%20High-Confidence%20Logit%0A%20%20Alignment&entry.906535625=Kejia%20Zhang%20and%20Juanjuan%20Weng%20and%20Zhiming%20Luo%20and%20Shaozi%20Li&entry.1292438233=%20%20Despite%20the%20significant%20advances%20that%20deep%20neural%20networks%20%28DNNs%29%20have%0Aachieved%20in%20various%20visual%20tasks%2C%20they%20still%20exhibit%20vulnerability%20to%0Aadversarial%20examples%2C%20leading%20to%20serious%20security%20concerns.%20Recent%20adversarial%0Atraining%20techniques%20have%20utilized%20inverse%20adversarial%20attacks%20to%20generate%0Ahigh-confidence%20examples%2C%20aiming%20to%20align%20the%20distributions%20of%20adversarial%0Aexamples%20with%20the%20high-confidence%20regions%20of%20their%20corresponding%20classes.%0AHowever%2C%20in%20this%20paper%2C%20our%20investigation%20reveals%20that%20high-confidence%20outputs%0Aunder%20inverse%20adversarial%20attacks%20are%20correlated%20with%20biased%20feature%0Aactivation.%20Specifically%2C%20training%20with%20inverse%20adversarial%20examples%20causes%20the%0Amodel%27s%20attention%20to%20shift%20towards%20background%20features%2C%20introducing%20a%20spurious%0Acorrelation%20bias.%20To%20address%20this%20bias%2C%20we%20propose%20Debiased%20High-Confidence%0AAdversarial%20Training%20%28DHAT%29%2C%20a%20novel%20approach%20that%20not%20only%20aligns%20the%20logits%0Aof%20adversarial%20examples%20with%20debiased%20high-confidence%20logits%20obtained%20from%0Ainverse%20adversarial%20examples%2C%20but%20also%20restores%20the%20model%27s%20attention%20to%20its%0Anormal%20state%20by%20enhancing%20foreground%20logit%20orthogonality.%20Extensive%20experiments%0Ademonstrate%20that%20DHAT%20achieves%20state-of-the-art%20performance%20and%20exhibits%20robust%0Ageneralization%20capabilities%20across%20various%20vision%20datasets.%20Additionally%2C%20DHAT%0Acan%20seamlessly%20integrate%20with%20existing%20advanced%20adversarial%20training%20techniques%0Afor%20improving%20the%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06079v1&entry.124074799=Read"},
{"title": "Generalization capabilities of MeshGraphNets to unseen geometries for\n  fluid dynamics", "author": "Robin Schm\u00f6cker and Alexander Henkes and Julian Roth and Thomas Wick", "abstract": "  This works investigates the generalization capabilities of MeshGraphNets\n(MGN) [Pfaff et al. Learning Mesh-Based Simulation with Graph Networks. ICML\n2021] to unseen geometries for fluid dynamics, e.g. predicting the flow around\na new obstacle that was not part of the training data. For this purpose, we\ncreate a new benchmark dataset for data-driven computational fluid dynamics\n(CFD) which extends DeepMind's flow around a cylinder dataset by including\ndifferent shapes and multiple objects. We then use this new dataset to extend\nthe generalization experiments conducted by DeepMind on MGNs by testing how\nwell an MGN can generalize to different shapes. In our numerical tests, we show\nthat MGNs can sometimes generalize well to various shapes by training on a\ndataset of one obstacle shape and testing on a dataset of another obstacle\nshape.\n", "link": "http://arxiv.org/abs/2408.06101v1", "date": "2024-08-12", "relevancy": 2.1352, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5973}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5179}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalization%20capabilities%20of%20MeshGraphNets%20to%20unseen%20geometries%20for%0A%20%20fluid%20dynamics&body=Title%3A%20Generalization%20capabilities%20of%20MeshGraphNets%20to%20unseen%20geometries%20for%0A%20%20fluid%20dynamics%0AAuthor%3A%20Robin%20Schm%C3%B6cker%20and%20Alexander%20Henkes%20and%20Julian%20Roth%20and%20Thomas%20Wick%0AAbstract%3A%20%20%20This%20works%20investigates%20the%20generalization%20capabilities%20of%20MeshGraphNets%0A%28MGN%29%20%5BPfaff%20et%20al.%20Learning%20Mesh-Based%20Simulation%20with%20Graph%20Networks.%20ICML%0A2021%5D%20to%20unseen%20geometries%20for%20fluid%20dynamics%2C%20e.g.%20predicting%20the%20flow%20around%0Aa%20new%20obstacle%20that%20was%20not%20part%20of%20the%20training%20data.%20For%20this%20purpose%2C%20we%0Acreate%20a%20new%20benchmark%20dataset%20for%20data-driven%20computational%20fluid%20dynamics%0A%28CFD%29%20which%20extends%20DeepMind%27s%20flow%20around%20a%20cylinder%20dataset%20by%20including%0Adifferent%20shapes%20and%20multiple%20objects.%20We%20then%20use%20this%20new%20dataset%20to%20extend%0Athe%20generalization%20experiments%20conducted%20by%20DeepMind%20on%20MGNs%20by%20testing%20how%0Awell%20an%20MGN%20can%20generalize%20to%20different%20shapes.%20In%20our%20numerical%20tests%2C%20we%20show%0Athat%20MGNs%20can%20sometimes%20generalize%20well%20to%20various%20shapes%20by%20training%20on%20a%0Adataset%20of%20one%20obstacle%20shape%20and%20testing%20on%20a%20dataset%20of%20another%20obstacle%0Ashape.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06101v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralization%2520capabilities%2520of%2520MeshGraphNets%2520to%2520unseen%2520geometries%2520for%250A%2520%2520fluid%2520dynamics%26entry.906535625%3DRobin%2520Schm%25C3%25B6cker%2520and%2520Alexander%2520Henkes%2520and%2520Julian%2520Roth%2520and%2520Thomas%2520Wick%26entry.1292438233%3D%2520%2520This%2520works%2520investigates%2520the%2520generalization%2520capabilities%2520of%2520MeshGraphNets%250A%2528MGN%2529%2520%255BPfaff%2520et%2520al.%2520Learning%2520Mesh-Based%2520Simulation%2520with%2520Graph%2520Networks.%2520ICML%250A2021%255D%2520to%2520unseen%2520geometries%2520for%2520fluid%2520dynamics%252C%2520e.g.%2520predicting%2520the%2520flow%2520around%250Aa%2520new%2520obstacle%2520that%2520was%2520not%2520part%2520of%2520the%2520training%2520data.%2520For%2520this%2520purpose%252C%2520we%250Acreate%2520a%2520new%2520benchmark%2520dataset%2520for%2520data-driven%2520computational%2520fluid%2520dynamics%250A%2528CFD%2529%2520which%2520extends%2520DeepMind%2527s%2520flow%2520around%2520a%2520cylinder%2520dataset%2520by%2520including%250Adifferent%2520shapes%2520and%2520multiple%2520objects.%2520We%2520then%2520use%2520this%2520new%2520dataset%2520to%2520extend%250Athe%2520generalization%2520experiments%2520conducted%2520by%2520DeepMind%2520on%2520MGNs%2520by%2520testing%2520how%250Awell%2520an%2520MGN%2520can%2520generalize%2520to%2520different%2520shapes.%2520In%2520our%2520numerical%2520tests%252C%2520we%2520show%250Athat%2520MGNs%2520can%2520sometimes%2520generalize%2520well%2520to%2520various%2520shapes%2520by%2520training%2520on%2520a%250Adataset%2520of%2520one%2520obstacle%2520shape%2520and%2520testing%2520on%2520a%2520dataset%2520of%2520another%2520obstacle%250Ashape.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06101v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalization%20capabilities%20of%20MeshGraphNets%20to%20unseen%20geometries%20for%0A%20%20fluid%20dynamics&entry.906535625=Robin%20Schm%C3%B6cker%20and%20Alexander%20Henkes%20and%20Julian%20Roth%20and%20Thomas%20Wick&entry.1292438233=%20%20This%20works%20investigates%20the%20generalization%20capabilities%20of%20MeshGraphNets%0A%28MGN%29%20%5BPfaff%20et%20al.%20Learning%20Mesh-Based%20Simulation%20with%20Graph%20Networks.%20ICML%0A2021%5D%20to%20unseen%20geometries%20for%20fluid%20dynamics%2C%20e.g.%20predicting%20the%20flow%20around%0Aa%20new%20obstacle%20that%20was%20not%20part%20of%20the%20training%20data.%20For%20this%20purpose%2C%20we%0Acreate%20a%20new%20benchmark%20dataset%20for%20data-driven%20computational%20fluid%20dynamics%0A%28CFD%29%20which%20extends%20DeepMind%27s%20flow%20around%20a%20cylinder%20dataset%20by%20including%0Adifferent%20shapes%20and%20multiple%20objects.%20We%20then%20use%20this%20new%20dataset%20to%20extend%0Athe%20generalization%20experiments%20conducted%20by%20DeepMind%20on%20MGNs%20by%20testing%20how%0Awell%20an%20MGN%20can%20generalize%20to%20different%20shapes.%20In%20our%20numerical%20tests%2C%20we%20show%0Athat%20MGNs%20can%20sometimes%20generalize%20well%20to%20various%20shapes%20by%20training%20on%20a%0Adataset%20of%20one%20obstacle%20shape%20and%20testing%20on%20a%20dataset%20of%20another%20obstacle%0Ashape.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06101v1&entry.124074799=Read"},
{"title": "ARPA: A Novel Hybrid Model for Advancing Visual Word Disambiguation\n  Using Large Language Models and Transformers", "author": "Aristi Papastavrou and Maria Lymperaiou and Giorgos Stamou", "abstract": "  In the rapidly evolving fields of natural language processing and computer\nvision, Visual Word Sense Disambiguation (VWSD) stands as a critical, yet\nchallenging task. The quest for models that can seamlessly integrate and\ninterpret multimodal data is more pressing than ever. Imagine a system that can\nunderstand language with the depth and nuance of human cognition, while\nsimultaneously interpreting the rich visual context of the world around it.\n  We present ARPA, an architecture that fuses the unparalleled contextual\nunderstanding of large language models with the advanced feature extraction\ncapabilities of transformers, which then pass through a custom Graph Neural\nNetwork (GNN) layer to learn intricate relationships and subtle nuances within\nthe data. This innovative architecture not only sets a new benchmark in visual\nword disambiguation but also introduces a versatile framework poised to\ntransform how linguistic and visual data interact by harnessing the synergistic\nstrengths of its components, ensuring robust performance even in the most\ncomplex disambiguation scenarios. Through a series of experiments and\ncomparative analysis, we reveal the substantial advantages of our model,\nunderscoring its potential to redefine standards in the field. Beyond its\narchitectural prowess, our architecture excels through experimental\nenrichments, including sophisticated data augmentation and multi-modal training\ntechniques.\n  ARPA's introduction marks a significant milestone in visual word\ndisambiguation, offering a compelling solution that bridges the gap between\nlinguistic and visual modalities. We invite researchers and practitioners to\nexplore the capabilities of our model, envisioning a future where such hybrid\nmodels drive unprecedented advancements in artificial intelligence.\n", "link": "http://arxiv.org/abs/2408.06040v1", "date": "2024-08-12", "relevancy": 2.1346, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5579}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5518}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ARPA%3A%20A%20Novel%20Hybrid%20Model%20for%20Advancing%20Visual%20Word%20Disambiguation%0A%20%20Using%20Large%20Language%20Models%20and%20Transformers&body=Title%3A%20ARPA%3A%20A%20Novel%20Hybrid%20Model%20for%20Advancing%20Visual%20Word%20Disambiguation%0A%20%20Using%20Large%20Language%20Models%20and%20Transformers%0AAuthor%3A%20Aristi%20Papastavrou%20and%20Maria%20Lymperaiou%20and%20Giorgos%20Stamou%0AAbstract%3A%20%20%20In%20the%20rapidly%20evolving%20fields%20of%20natural%20language%20processing%20and%20computer%0Avision%2C%20Visual%20Word%20Sense%20Disambiguation%20%28VWSD%29%20stands%20as%20a%20critical%2C%20yet%0Achallenging%20task.%20The%20quest%20for%20models%20that%20can%20seamlessly%20integrate%20and%0Ainterpret%20multimodal%20data%20is%20more%20pressing%20than%20ever.%20Imagine%20a%20system%20that%20can%0Aunderstand%20language%20with%20the%20depth%20and%20nuance%20of%20human%20cognition%2C%20while%0Asimultaneously%20interpreting%20the%20rich%20visual%20context%20of%20the%20world%20around%20it.%0A%20%20We%20present%20ARPA%2C%20an%20architecture%20that%20fuses%20the%20unparalleled%20contextual%0Aunderstanding%20of%20large%20language%20models%20with%20the%20advanced%20feature%20extraction%0Acapabilities%20of%20transformers%2C%20which%20then%20pass%20through%20a%20custom%20Graph%20Neural%0ANetwork%20%28GNN%29%20layer%20to%20learn%20intricate%20relationships%20and%20subtle%20nuances%20within%0Athe%20data.%20This%20innovative%20architecture%20not%20only%20sets%20a%20new%20benchmark%20in%20visual%0Aword%20disambiguation%20but%20also%20introduces%20a%20versatile%20framework%20poised%20to%0Atransform%20how%20linguistic%20and%20visual%20data%20interact%20by%20harnessing%20the%20synergistic%0Astrengths%20of%20its%20components%2C%20ensuring%20robust%20performance%20even%20in%20the%20most%0Acomplex%20disambiguation%20scenarios.%20Through%20a%20series%20of%20experiments%20and%0Acomparative%20analysis%2C%20we%20reveal%20the%20substantial%20advantages%20of%20our%20model%2C%0Aunderscoring%20its%20potential%20to%20redefine%20standards%20in%20the%20field.%20Beyond%20its%0Aarchitectural%20prowess%2C%20our%20architecture%20excels%20through%20experimental%0Aenrichments%2C%20including%20sophisticated%20data%20augmentation%20and%20multi-modal%20training%0Atechniques.%0A%20%20ARPA%27s%20introduction%20marks%20a%20significant%20milestone%20in%20visual%20word%0Adisambiguation%2C%20offering%20a%20compelling%20solution%20that%20bridges%20the%20gap%20between%0Alinguistic%20and%20visual%20modalities.%20We%20invite%20researchers%20and%20practitioners%20to%0Aexplore%20the%20capabilities%20of%20our%20model%2C%20envisioning%20a%20future%20where%20such%20hybrid%0Amodels%20drive%20unprecedented%20advancements%20in%20artificial%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06040v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DARPA%253A%2520A%2520Novel%2520Hybrid%2520Model%2520for%2520Advancing%2520Visual%2520Word%2520Disambiguation%250A%2520%2520Using%2520Large%2520Language%2520Models%2520and%2520Transformers%26entry.906535625%3DAristi%2520Papastavrou%2520and%2520Maria%2520Lymperaiou%2520and%2520Giorgos%2520Stamou%26entry.1292438233%3D%2520%2520In%2520the%2520rapidly%2520evolving%2520fields%2520of%2520natural%2520language%2520processing%2520and%2520computer%250Avision%252C%2520Visual%2520Word%2520Sense%2520Disambiguation%2520%2528VWSD%2529%2520stands%2520as%2520a%2520critical%252C%2520yet%250Achallenging%2520task.%2520The%2520quest%2520for%2520models%2520that%2520can%2520seamlessly%2520integrate%2520and%250Ainterpret%2520multimodal%2520data%2520is%2520more%2520pressing%2520than%2520ever.%2520Imagine%2520a%2520system%2520that%2520can%250Aunderstand%2520language%2520with%2520the%2520depth%2520and%2520nuance%2520of%2520human%2520cognition%252C%2520while%250Asimultaneously%2520interpreting%2520the%2520rich%2520visual%2520context%2520of%2520the%2520world%2520around%2520it.%250A%2520%2520We%2520present%2520ARPA%252C%2520an%2520architecture%2520that%2520fuses%2520the%2520unparalleled%2520contextual%250Aunderstanding%2520of%2520large%2520language%2520models%2520with%2520the%2520advanced%2520feature%2520extraction%250Acapabilities%2520of%2520transformers%252C%2520which%2520then%2520pass%2520through%2520a%2520custom%2520Graph%2520Neural%250ANetwork%2520%2528GNN%2529%2520layer%2520to%2520learn%2520intricate%2520relationships%2520and%2520subtle%2520nuances%2520within%250Athe%2520data.%2520This%2520innovative%2520architecture%2520not%2520only%2520sets%2520a%2520new%2520benchmark%2520in%2520visual%250Aword%2520disambiguation%2520but%2520also%2520introduces%2520a%2520versatile%2520framework%2520poised%2520to%250Atransform%2520how%2520linguistic%2520and%2520visual%2520data%2520interact%2520by%2520harnessing%2520the%2520synergistic%250Astrengths%2520of%2520its%2520components%252C%2520ensuring%2520robust%2520performance%2520even%2520in%2520the%2520most%250Acomplex%2520disambiguation%2520scenarios.%2520Through%2520a%2520series%2520of%2520experiments%2520and%250Acomparative%2520analysis%252C%2520we%2520reveal%2520the%2520substantial%2520advantages%2520of%2520our%2520model%252C%250Aunderscoring%2520its%2520potential%2520to%2520redefine%2520standards%2520in%2520the%2520field.%2520Beyond%2520its%250Aarchitectural%2520prowess%252C%2520our%2520architecture%2520excels%2520through%2520experimental%250Aenrichments%252C%2520including%2520sophisticated%2520data%2520augmentation%2520and%2520multi-modal%2520training%250Atechniques.%250A%2520%2520ARPA%2527s%2520introduction%2520marks%2520a%2520significant%2520milestone%2520in%2520visual%2520word%250Adisambiguation%252C%2520offering%2520a%2520compelling%2520solution%2520that%2520bridges%2520the%2520gap%2520between%250Alinguistic%2520and%2520visual%2520modalities.%2520We%2520invite%2520researchers%2520and%2520practitioners%2520to%250Aexplore%2520the%2520capabilities%2520of%2520our%2520model%252C%2520envisioning%2520a%2520future%2520where%2520such%2520hybrid%250Amodels%2520drive%2520unprecedented%2520advancements%2520in%2520artificial%2520intelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06040v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ARPA%3A%20A%20Novel%20Hybrid%20Model%20for%20Advancing%20Visual%20Word%20Disambiguation%0A%20%20Using%20Large%20Language%20Models%20and%20Transformers&entry.906535625=Aristi%20Papastavrou%20and%20Maria%20Lymperaiou%20and%20Giorgos%20Stamou&entry.1292438233=%20%20In%20the%20rapidly%20evolving%20fields%20of%20natural%20language%20processing%20and%20computer%0Avision%2C%20Visual%20Word%20Sense%20Disambiguation%20%28VWSD%29%20stands%20as%20a%20critical%2C%20yet%0Achallenging%20task.%20The%20quest%20for%20models%20that%20can%20seamlessly%20integrate%20and%0Ainterpret%20multimodal%20data%20is%20more%20pressing%20than%20ever.%20Imagine%20a%20system%20that%20can%0Aunderstand%20language%20with%20the%20depth%20and%20nuance%20of%20human%20cognition%2C%20while%0Asimultaneously%20interpreting%20the%20rich%20visual%20context%20of%20the%20world%20around%20it.%0A%20%20We%20present%20ARPA%2C%20an%20architecture%20that%20fuses%20the%20unparalleled%20contextual%0Aunderstanding%20of%20large%20language%20models%20with%20the%20advanced%20feature%20extraction%0Acapabilities%20of%20transformers%2C%20which%20then%20pass%20through%20a%20custom%20Graph%20Neural%0ANetwork%20%28GNN%29%20layer%20to%20learn%20intricate%20relationships%20and%20subtle%20nuances%20within%0Athe%20data.%20This%20innovative%20architecture%20not%20only%20sets%20a%20new%20benchmark%20in%20visual%0Aword%20disambiguation%20but%20also%20introduces%20a%20versatile%20framework%20poised%20to%0Atransform%20how%20linguistic%20and%20visual%20data%20interact%20by%20harnessing%20the%20synergistic%0Astrengths%20of%20its%20components%2C%20ensuring%20robust%20performance%20even%20in%20the%20most%0Acomplex%20disambiguation%20scenarios.%20Through%20a%20series%20of%20experiments%20and%0Acomparative%20analysis%2C%20we%20reveal%20the%20substantial%20advantages%20of%20our%20model%2C%0Aunderscoring%20its%20potential%20to%20redefine%20standards%20in%20the%20field.%20Beyond%20its%0Aarchitectural%20prowess%2C%20our%20architecture%20excels%20through%20experimental%0Aenrichments%2C%20including%20sophisticated%20data%20augmentation%20and%20multi-modal%20training%0Atechniques.%0A%20%20ARPA%27s%20introduction%20marks%20a%20significant%20milestone%20in%20visual%20word%0Adisambiguation%2C%20offering%20a%20compelling%20solution%20that%20bridges%20the%20gap%20between%0Alinguistic%20and%20visual%20modalities.%20We%20invite%20researchers%20and%20practitioners%20to%0Aexplore%20the%20capabilities%20of%20our%20model%2C%20envisioning%20a%20future%20where%20such%20hybrid%0Amodels%20drive%20unprecedented%20advancements%20in%20artificial%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06040v1&entry.124074799=Read"},
{"title": "cDVGAN: One Flexible Model for Multi-class Gravitational Wave Signal and\n  Glitch Generation", "author": "Tom Dooney and Lyana Curier and Daniel Tan and Melissa Lopez and Chris Van Den Broeck and Stefano Bromuri", "abstract": "  Simulating realistic time-domain observations of gravitational waves (GWs)\nand GW detector glitches can help in advancing GW data analysis. Simulated data\ncan be used in downstream tasks by augmenting datasets for signal searches,\nbalancing data sets for machine learning, and validating detection schemes. In\nthis work, we present Conditional Derivative GAN (cDVGAN), a novel conditional\nmodel in the Generative Adversarial Network framework for simulating multiple\nclasses of time-domain observations that represent gravitational waves (GWs)\nand detector glitches. cDVGAN can also generate generalized hybrid samples that\nspan the variation between classes through interpolation in the conditioned\nclass vector. cDVGAN introduces an additional player into the typical 2-player\nadversarial game of GANs, where an auxiliary discriminator analyzes the\nfirst-order derivative time-series. Our results show that this provides\nsynthetic data that better captures the features of the original data. cDVGAN\nconditions on three classes, two denoised from LIGO blip and tomte glitch\nevents from its 3rd observing run (O3), and the third representing binary black\nhole (BBH) mergers. Our proposed cDVGAN outperforms 4 different baseline GAN\nmodels in replicating the features of the three classes. Specifically, our\nexperiments show that training convolutional neural networks (CNNs) with our\ncDVGAN-generated data improves the detection of samples embedded in detector\nnoise beyond the synthetic data from other state-of-the-art GAN models. Our\nbest synthetic dataset yields as much as a 4.2% increase in\narea-under-the-curve (AUC) performance compared to synthetic datasets from\nbaseline GANs. Moreover, training the CNN with hybrid samples from our cDVGAN\noutperforms CNNs trained only on the standard classes, when identifying real\nsamples embedded in LIGO detector background (4% AUC improvement for cDVGAN).\n", "link": "http://arxiv.org/abs/2401.16356v5", "date": "2024-08-12", "relevancy": 2.1225, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5484}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5207}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20cDVGAN%3A%20One%20Flexible%20Model%20for%20Multi-class%20Gravitational%20Wave%20Signal%20and%0A%20%20Glitch%20Generation&body=Title%3A%20cDVGAN%3A%20One%20Flexible%20Model%20for%20Multi-class%20Gravitational%20Wave%20Signal%20and%0A%20%20Glitch%20Generation%0AAuthor%3A%20Tom%20Dooney%20and%20Lyana%20Curier%20and%20Daniel%20Tan%20and%20Melissa%20Lopez%20and%20Chris%20Van%20Den%20Broeck%20and%20Stefano%20Bromuri%0AAbstract%3A%20%20%20Simulating%20realistic%20time-domain%20observations%20of%20gravitational%20waves%20%28GWs%29%0Aand%20GW%20detector%20glitches%20can%20help%20in%20advancing%20GW%20data%20analysis.%20Simulated%20data%0Acan%20be%20used%20in%20downstream%20tasks%20by%20augmenting%20datasets%20for%20signal%20searches%2C%0Abalancing%20data%20sets%20for%20machine%20learning%2C%20and%20validating%20detection%20schemes.%20In%0Athis%20work%2C%20we%20present%20Conditional%20Derivative%20GAN%20%28cDVGAN%29%2C%20a%20novel%20conditional%0Amodel%20in%20the%20Generative%20Adversarial%20Network%20framework%20for%20simulating%20multiple%0Aclasses%20of%20time-domain%20observations%20that%20represent%20gravitational%20waves%20%28GWs%29%0Aand%20detector%20glitches.%20cDVGAN%20can%20also%20generate%20generalized%20hybrid%20samples%20that%0Aspan%20the%20variation%20between%20classes%20through%20interpolation%20in%20the%20conditioned%0Aclass%20vector.%20cDVGAN%20introduces%20an%20additional%20player%20into%20the%20typical%202-player%0Aadversarial%20game%20of%20GANs%2C%20where%20an%20auxiliary%20discriminator%20analyzes%20the%0Afirst-order%20derivative%20time-series.%20Our%20results%20show%20that%20this%20provides%0Asynthetic%20data%20that%20better%20captures%20the%20features%20of%20the%20original%20data.%20cDVGAN%0Aconditions%20on%20three%20classes%2C%20two%20denoised%20from%20LIGO%20blip%20and%20tomte%20glitch%0Aevents%20from%20its%203rd%20observing%20run%20%28O3%29%2C%20and%20the%20third%20representing%20binary%20black%0Ahole%20%28BBH%29%20mergers.%20Our%20proposed%20cDVGAN%20outperforms%204%20different%20baseline%20GAN%0Amodels%20in%20replicating%20the%20features%20of%20the%20three%20classes.%20Specifically%2C%20our%0Aexperiments%20show%20that%20training%20convolutional%20neural%20networks%20%28CNNs%29%20with%20our%0AcDVGAN-generated%20data%20improves%20the%20detection%20of%20samples%20embedded%20in%20detector%0Anoise%20beyond%20the%20synthetic%20data%20from%20other%20state-of-the-art%20GAN%20models.%20Our%0Abest%20synthetic%20dataset%20yields%20as%20much%20as%20a%204.2%25%20increase%20in%0Aarea-under-the-curve%20%28AUC%29%20performance%20compared%20to%20synthetic%20datasets%20from%0Abaseline%20GANs.%20Moreover%2C%20training%20the%20CNN%20with%20hybrid%20samples%20from%20our%20cDVGAN%0Aoutperforms%20CNNs%20trained%20only%20on%20the%20standard%20classes%2C%20when%20identifying%20real%0Asamples%20embedded%20in%20LIGO%20detector%20background%20%284%25%20AUC%20improvement%20for%20cDVGAN%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.16356v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DcDVGAN%253A%2520One%2520Flexible%2520Model%2520for%2520Multi-class%2520Gravitational%2520Wave%2520Signal%2520and%250A%2520%2520Glitch%2520Generation%26entry.906535625%3DTom%2520Dooney%2520and%2520Lyana%2520Curier%2520and%2520Daniel%2520Tan%2520and%2520Melissa%2520Lopez%2520and%2520Chris%2520Van%2520Den%2520Broeck%2520and%2520Stefano%2520Bromuri%26entry.1292438233%3D%2520%2520Simulating%2520realistic%2520time-domain%2520observations%2520of%2520gravitational%2520waves%2520%2528GWs%2529%250Aand%2520GW%2520detector%2520glitches%2520can%2520help%2520in%2520advancing%2520GW%2520data%2520analysis.%2520Simulated%2520data%250Acan%2520be%2520used%2520in%2520downstream%2520tasks%2520by%2520augmenting%2520datasets%2520for%2520signal%2520searches%252C%250Abalancing%2520data%2520sets%2520for%2520machine%2520learning%252C%2520and%2520validating%2520detection%2520schemes.%2520In%250Athis%2520work%252C%2520we%2520present%2520Conditional%2520Derivative%2520GAN%2520%2528cDVGAN%2529%252C%2520a%2520novel%2520conditional%250Amodel%2520in%2520the%2520Generative%2520Adversarial%2520Network%2520framework%2520for%2520simulating%2520multiple%250Aclasses%2520of%2520time-domain%2520observations%2520that%2520represent%2520gravitational%2520waves%2520%2528GWs%2529%250Aand%2520detector%2520glitches.%2520cDVGAN%2520can%2520also%2520generate%2520generalized%2520hybrid%2520samples%2520that%250Aspan%2520the%2520variation%2520between%2520classes%2520through%2520interpolation%2520in%2520the%2520conditioned%250Aclass%2520vector.%2520cDVGAN%2520introduces%2520an%2520additional%2520player%2520into%2520the%2520typical%25202-player%250Aadversarial%2520game%2520of%2520GANs%252C%2520where%2520an%2520auxiliary%2520discriminator%2520analyzes%2520the%250Afirst-order%2520derivative%2520time-series.%2520Our%2520results%2520show%2520that%2520this%2520provides%250Asynthetic%2520data%2520that%2520better%2520captures%2520the%2520features%2520of%2520the%2520original%2520data.%2520cDVGAN%250Aconditions%2520on%2520three%2520classes%252C%2520two%2520denoised%2520from%2520LIGO%2520blip%2520and%2520tomte%2520glitch%250Aevents%2520from%2520its%25203rd%2520observing%2520run%2520%2528O3%2529%252C%2520and%2520the%2520third%2520representing%2520binary%2520black%250Ahole%2520%2528BBH%2529%2520mergers.%2520Our%2520proposed%2520cDVGAN%2520outperforms%25204%2520different%2520baseline%2520GAN%250Amodels%2520in%2520replicating%2520the%2520features%2520of%2520the%2520three%2520classes.%2520Specifically%252C%2520our%250Aexperiments%2520show%2520that%2520training%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520with%2520our%250AcDVGAN-generated%2520data%2520improves%2520the%2520detection%2520of%2520samples%2520embedded%2520in%2520detector%250Anoise%2520beyond%2520the%2520synthetic%2520data%2520from%2520other%2520state-of-the-art%2520GAN%2520models.%2520Our%250Abest%2520synthetic%2520dataset%2520yields%2520as%2520much%2520as%2520a%25204.2%2525%2520increase%2520in%250Aarea-under-the-curve%2520%2528AUC%2529%2520performance%2520compared%2520to%2520synthetic%2520datasets%2520from%250Abaseline%2520GANs.%2520Moreover%252C%2520training%2520the%2520CNN%2520with%2520hybrid%2520samples%2520from%2520our%2520cDVGAN%250Aoutperforms%2520CNNs%2520trained%2520only%2520on%2520the%2520standard%2520classes%252C%2520when%2520identifying%2520real%250Asamples%2520embedded%2520in%2520LIGO%2520detector%2520background%2520%25284%2525%2520AUC%2520improvement%2520for%2520cDVGAN%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.16356v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=cDVGAN%3A%20One%20Flexible%20Model%20for%20Multi-class%20Gravitational%20Wave%20Signal%20and%0A%20%20Glitch%20Generation&entry.906535625=Tom%20Dooney%20and%20Lyana%20Curier%20and%20Daniel%20Tan%20and%20Melissa%20Lopez%20and%20Chris%20Van%20Den%20Broeck%20and%20Stefano%20Bromuri&entry.1292438233=%20%20Simulating%20realistic%20time-domain%20observations%20of%20gravitational%20waves%20%28GWs%29%0Aand%20GW%20detector%20glitches%20can%20help%20in%20advancing%20GW%20data%20analysis.%20Simulated%20data%0Acan%20be%20used%20in%20downstream%20tasks%20by%20augmenting%20datasets%20for%20signal%20searches%2C%0Abalancing%20data%20sets%20for%20machine%20learning%2C%20and%20validating%20detection%20schemes.%20In%0Athis%20work%2C%20we%20present%20Conditional%20Derivative%20GAN%20%28cDVGAN%29%2C%20a%20novel%20conditional%0Amodel%20in%20the%20Generative%20Adversarial%20Network%20framework%20for%20simulating%20multiple%0Aclasses%20of%20time-domain%20observations%20that%20represent%20gravitational%20waves%20%28GWs%29%0Aand%20detector%20glitches.%20cDVGAN%20can%20also%20generate%20generalized%20hybrid%20samples%20that%0Aspan%20the%20variation%20between%20classes%20through%20interpolation%20in%20the%20conditioned%0Aclass%20vector.%20cDVGAN%20introduces%20an%20additional%20player%20into%20the%20typical%202-player%0Aadversarial%20game%20of%20GANs%2C%20where%20an%20auxiliary%20discriminator%20analyzes%20the%0Afirst-order%20derivative%20time-series.%20Our%20results%20show%20that%20this%20provides%0Asynthetic%20data%20that%20better%20captures%20the%20features%20of%20the%20original%20data.%20cDVGAN%0Aconditions%20on%20three%20classes%2C%20two%20denoised%20from%20LIGO%20blip%20and%20tomte%20glitch%0Aevents%20from%20its%203rd%20observing%20run%20%28O3%29%2C%20and%20the%20third%20representing%20binary%20black%0Ahole%20%28BBH%29%20mergers.%20Our%20proposed%20cDVGAN%20outperforms%204%20different%20baseline%20GAN%0Amodels%20in%20replicating%20the%20features%20of%20the%20three%20classes.%20Specifically%2C%20our%0Aexperiments%20show%20that%20training%20convolutional%20neural%20networks%20%28CNNs%29%20with%20our%0AcDVGAN-generated%20data%20improves%20the%20detection%20of%20samples%20embedded%20in%20detector%0Anoise%20beyond%20the%20synthetic%20data%20from%20other%20state-of-the-art%20GAN%20models.%20Our%0Abest%20synthetic%20dataset%20yields%20as%20much%20as%20a%204.2%25%20increase%20in%0Aarea-under-the-curve%20%28AUC%29%20performance%20compared%20to%20synthetic%20datasets%20from%0Abaseline%20GANs.%20Moreover%2C%20training%20the%20CNN%20with%20hybrid%20samples%20from%20our%20cDVGAN%0Aoutperforms%20CNNs%20trained%20only%20on%20the%20standard%20classes%2C%20when%20identifying%20real%0Asamples%20embedded%20in%20LIGO%20detector%20background%20%284%25%20AUC%20improvement%20for%20cDVGAN%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.16356v5&entry.124074799=Read"},
{"title": "Palantir: Towards Efficient Super Resolution for Ultra-high-definition\n  Live Streaming", "author": "Xinqi Jin and Zhui Zhu and Xikai Sun and Fan Dang and Jiangchuan Liu and Jingao Xu and Kebin Liu and Xinlei Chen and Yunhao Liu", "abstract": "  Neural enhancement through super-resolution deep neural networks opens up new\npossibilities for ultra-high-definition live streaming over existing encoding\nand networking infrastructure. Yet, the heavy SR DNN inference overhead leads\nto severe deployment challenges. To reduce the overhead, existing systems\npropose to apply DNN-based SR only on selected anchor frames while upscaling\nnon-anchor frames via the lightweight reusing-based SR approach. However,\nframe-level scheduling is coarse-grained and fails to deliver optimal\nefficiency. In this work, we propose Palantir, the first neural-enhanced UHD\nlive streaming system with fine-grained patch-level scheduling. In the\npresented solutions, two novel techniques are incorporated to make good\nscheduling decisions for inference overhead optimization and reduce the\nscheduling latency. Firstly, under the guidance of our pioneering and\ntheoretical analysis, Palantir constructs a directed acyclic graph (DAG) for\nlightweight yet accurate quality estimation under any possible anchor patch\nset. Secondly, to further optimize the scheduling latency, Palantir improves\nparallelizability by refactoring the computation subprocedure of the estimation\nprocess into a sparse matrix-matrix multiplication operation. The evaluation\nresults suggest that Palantir incurs a negligible scheduling latency accounting\nfor less than 5.7% of the end-to-end latency requirement. When compared to the\nstate-of-the-art real-time frame-level scheduling strategy, Palantir reduces\nthe energy overhead of SR-integrated mobile clients by 38.1% at most (and 22.4%\non average) and the monetary costs of cloud-based SR by 80.1% at most (and\n38.4% on average).\n", "link": "http://arxiv.org/abs/2408.06152v1", "date": "2024-08-12", "relevancy": 2.1156, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5591}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5406}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5051}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Palantir%3A%20Towards%20Efficient%20Super%20Resolution%20for%20Ultra-high-definition%0A%20%20Live%20Streaming&body=Title%3A%20Palantir%3A%20Towards%20Efficient%20Super%20Resolution%20for%20Ultra-high-definition%0A%20%20Live%20Streaming%0AAuthor%3A%20Xinqi%20Jin%20and%20Zhui%20Zhu%20and%20Xikai%20Sun%20and%20Fan%20Dang%20and%20Jiangchuan%20Liu%20and%20Jingao%20Xu%20and%20Kebin%20Liu%20and%20Xinlei%20Chen%20and%20Yunhao%20Liu%0AAbstract%3A%20%20%20Neural%20enhancement%20through%20super-resolution%20deep%20neural%20networks%20opens%20up%20new%0Apossibilities%20for%20ultra-high-definition%20live%20streaming%20over%20existing%20encoding%0Aand%20networking%20infrastructure.%20Yet%2C%20the%20heavy%20SR%20DNN%20inference%20overhead%20leads%0Ato%20severe%20deployment%20challenges.%20To%20reduce%20the%20overhead%2C%20existing%20systems%0Apropose%20to%20apply%20DNN-based%20SR%20only%20on%20selected%20anchor%20frames%20while%20upscaling%0Anon-anchor%20frames%20via%20the%20lightweight%20reusing-based%20SR%20approach.%20However%2C%0Aframe-level%20scheduling%20is%20coarse-grained%20and%20fails%20to%20deliver%20optimal%0Aefficiency.%20In%20this%20work%2C%20we%20propose%20Palantir%2C%20the%20first%20neural-enhanced%20UHD%0Alive%20streaming%20system%20with%20fine-grained%20patch-level%20scheduling.%20In%20the%0Apresented%20solutions%2C%20two%20novel%20techniques%20are%20incorporated%20to%20make%20good%0Ascheduling%20decisions%20for%20inference%20overhead%20optimization%20and%20reduce%20the%0Ascheduling%20latency.%20Firstly%2C%20under%20the%20guidance%20of%20our%20pioneering%20and%0Atheoretical%20analysis%2C%20Palantir%20constructs%20a%20directed%20acyclic%20graph%20%28DAG%29%20for%0Alightweight%20yet%20accurate%20quality%20estimation%20under%20any%20possible%20anchor%20patch%0Aset.%20Secondly%2C%20to%20further%20optimize%20the%20scheduling%20latency%2C%20Palantir%20improves%0Aparallelizability%20by%20refactoring%20the%20computation%20subprocedure%20of%20the%20estimation%0Aprocess%20into%20a%20sparse%20matrix-matrix%20multiplication%20operation.%20The%20evaluation%0Aresults%20suggest%20that%20Palantir%20incurs%20a%20negligible%20scheduling%20latency%20accounting%0Afor%20less%20than%205.7%25%20of%20the%20end-to-end%20latency%20requirement.%20When%20compared%20to%20the%0Astate-of-the-art%20real-time%20frame-level%20scheduling%20strategy%2C%20Palantir%20reduces%0Athe%20energy%20overhead%20of%20SR-integrated%20mobile%20clients%20by%2038.1%25%20at%20most%20%28and%2022.4%25%0Aon%20average%29%20and%20the%20monetary%20costs%20of%20cloud-based%20SR%20by%2080.1%25%20at%20most%20%28and%0A38.4%25%20on%20average%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06152v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPalantir%253A%2520Towards%2520Efficient%2520Super%2520Resolution%2520for%2520Ultra-high-definition%250A%2520%2520Live%2520Streaming%26entry.906535625%3DXinqi%2520Jin%2520and%2520Zhui%2520Zhu%2520and%2520Xikai%2520Sun%2520and%2520Fan%2520Dang%2520and%2520Jiangchuan%2520Liu%2520and%2520Jingao%2520Xu%2520and%2520Kebin%2520Liu%2520and%2520Xinlei%2520Chen%2520and%2520Yunhao%2520Liu%26entry.1292438233%3D%2520%2520Neural%2520enhancement%2520through%2520super-resolution%2520deep%2520neural%2520networks%2520opens%2520up%2520new%250Apossibilities%2520for%2520ultra-high-definition%2520live%2520streaming%2520over%2520existing%2520encoding%250Aand%2520networking%2520infrastructure.%2520Yet%252C%2520the%2520heavy%2520SR%2520DNN%2520inference%2520overhead%2520leads%250Ato%2520severe%2520deployment%2520challenges.%2520To%2520reduce%2520the%2520overhead%252C%2520existing%2520systems%250Apropose%2520to%2520apply%2520DNN-based%2520SR%2520only%2520on%2520selected%2520anchor%2520frames%2520while%2520upscaling%250Anon-anchor%2520frames%2520via%2520the%2520lightweight%2520reusing-based%2520SR%2520approach.%2520However%252C%250Aframe-level%2520scheduling%2520is%2520coarse-grained%2520and%2520fails%2520to%2520deliver%2520optimal%250Aefficiency.%2520In%2520this%2520work%252C%2520we%2520propose%2520Palantir%252C%2520the%2520first%2520neural-enhanced%2520UHD%250Alive%2520streaming%2520system%2520with%2520fine-grained%2520patch-level%2520scheduling.%2520In%2520the%250Apresented%2520solutions%252C%2520two%2520novel%2520techniques%2520are%2520incorporated%2520to%2520make%2520good%250Ascheduling%2520decisions%2520for%2520inference%2520overhead%2520optimization%2520and%2520reduce%2520the%250Ascheduling%2520latency.%2520Firstly%252C%2520under%2520the%2520guidance%2520of%2520our%2520pioneering%2520and%250Atheoretical%2520analysis%252C%2520Palantir%2520constructs%2520a%2520directed%2520acyclic%2520graph%2520%2528DAG%2529%2520for%250Alightweight%2520yet%2520accurate%2520quality%2520estimation%2520under%2520any%2520possible%2520anchor%2520patch%250Aset.%2520Secondly%252C%2520to%2520further%2520optimize%2520the%2520scheduling%2520latency%252C%2520Palantir%2520improves%250Aparallelizability%2520by%2520refactoring%2520the%2520computation%2520subprocedure%2520of%2520the%2520estimation%250Aprocess%2520into%2520a%2520sparse%2520matrix-matrix%2520multiplication%2520operation.%2520The%2520evaluation%250Aresults%2520suggest%2520that%2520Palantir%2520incurs%2520a%2520negligible%2520scheduling%2520latency%2520accounting%250Afor%2520less%2520than%25205.7%2525%2520of%2520the%2520end-to-end%2520latency%2520requirement.%2520When%2520compared%2520to%2520the%250Astate-of-the-art%2520real-time%2520frame-level%2520scheduling%2520strategy%252C%2520Palantir%2520reduces%250Athe%2520energy%2520overhead%2520of%2520SR-integrated%2520mobile%2520clients%2520by%252038.1%2525%2520at%2520most%2520%2528and%252022.4%2525%250Aon%2520average%2529%2520and%2520the%2520monetary%2520costs%2520of%2520cloud-based%2520SR%2520by%252080.1%2525%2520at%2520most%2520%2528and%250A38.4%2525%2520on%2520average%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06152v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Palantir%3A%20Towards%20Efficient%20Super%20Resolution%20for%20Ultra-high-definition%0A%20%20Live%20Streaming&entry.906535625=Xinqi%20Jin%20and%20Zhui%20Zhu%20and%20Xikai%20Sun%20and%20Fan%20Dang%20and%20Jiangchuan%20Liu%20and%20Jingao%20Xu%20and%20Kebin%20Liu%20and%20Xinlei%20Chen%20and%20Yunhao%20Liu&entry.1292438233=%20%20Neural%20enhancement%20through%20super-resolution%20deep%20neural%20networks%20opens%20up%20new%0Apossibilities%20for%20ultra-high-definition%20live%20streaming%20over%20existing%20encoding%0Aand%20networking%20infrastructure.%20Yet%2C%20the%20heavy%20SR%20DNN%20inference%20overhead%20leads%0Ato%20severe%20deployment%20challenges.%20To%20reduce%20the%20overhead%2C%20existing%20systems%0Apropose%20to%20apply%20DNN-based%20SR%20only%20on%20selected%20anchor%20frames%20while%20upscaling%0Anon-anchor%20frames%20via%20the%20lightweight%20reusing-based%20SR%20approach.%20However%2C%0Aframe-level%20scheduling%20is%20coarse-grained%20and%20fails%20to%20deliver%20optimal%0Aefficiency.%20In%20this%20work%2C%20we%20propose%20Palantir%2C%20the%20first%20neural-enhanced%20UHD%0Alive%20streaming%20system%20with%20fine-grained%20patch-level%20scheduling.%20In%20the%0Apresented%20solutions%2C%20two%20novel%20techniques%20are%20incorporated%20to%20make%20good%0Ascheduling%20decisions%20for%20inference%20overhead%20optimization%20and%20reduce%20the%0Ascheduling%20latency.%20Firstly%2C%20under%20the%20guidance%20of%20our%20pioneering%20and%0Atheoretical%20analysis%2C%20Palantir%20constructs%20a%20directed%20acyclic%20graph%20%28DAG%29%20for%0Alightweight%20yet%20accurate%20quality%20estimation%20under%20any%20possible%20anchor%20patch%0Aset.%20Secondly%2C%20to%20further%20optimize%20the%20scheduling%20latency%2C%20Palantir%20improves%0Aparallelizability%20by%20refactoring%20the%20computation%20subprocedure%20of%20the%20estimation%0Aprocess%20into%20a%20sparse%20matrix-matrix%20multiplication%20operation.%20The%20evaluation%0Aresults%20suggest%20that%20Palantir%20incurs%20a%20negligible%20scheduling%20latency%20accounting%0Afor%20less%20than%205.7%25%20of%20the%20end-to-end%20latency%20requirement.%20When%20compared%20to%20the%0Astate-of-the-art%20real-time%20frame-level%20scheduling%20strategy%2C%20Palantir%20reduces%0Athe%20energy%20overhead%20of%20SR-integrated%20mobile%20clients%20by%2038.1%25%20at%20most%20%28and%2022.4%25%0Aon%20average%29%20and%20the%20monetary%20costs%20of%20cloud-based%20SR%20by%2080.1%25%20at%20most%20%28and%0A38.4%25%20on%20average%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06152v1&entry.124074799=Read"},
{"title": "ACCELERATION: Sequentially-scanning DECT Imaging Using High Temporal\n  Resolution Image Reconstruction And Temporal Extrapolation", "author": "Qiaoxin Li and Dong Liang and Yinsheng Li", "abstract": "  Dual-energy computed tomography (DECT) has been widely used to obtain\nquantitative elemental composition of imaged subjects for personalized and\nprecise medical diagnosis. Compared with existing high-end DECT leveraging\nadvanced X-ray source and/or detector technologies, the use of the\nsequentially-scanning data acquisition scheme to implement DECT may make\nbroader impact on clinical practice because this scheme requires no specialized\nhardware designs. However, since the concentration of iodinated contrast agent\nin the imaged subject varies over time, sequentially-scanned data sets acquired\nat two tube potentials are temporally inconsistent. As existing material\ndecomposition approaches for DECT assume that the data sets acquired at two\ntube potentials are temporally consistent, the violation of this assumption\nresults in inaccurate quantification accuracy of iodine concentration. In this\nwork, we developed a technique to achieve sequentially-scanning DECT imaging\nusing high temporal resolution image reconstruction and temporal extrapolation,\nACCELERATION in short, to address the technical challenge induced by temporal\ninconsistency of sequentially-scanned data sets and improve iodine\nquantification accuracy in sequentially-scanning DECT. ACCELERATION has been\nvalidated and evaluated using numerical simulation data sets generated from\nclinical human subject exams. Results demonstrated the improvement of iodine\nquantification accuracy using ACCELERATION.\n", "link": "http://arxiv.org/abs/2408.06163v1", "date": "2024-08-12", "relevancy": 2.0869, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5245}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5245}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5077}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ACCELERATION%3A%20Sequentially-scanning%20DECT%20Imaging%20Using%20High%20Temporal%0A%20%20Resolution%20Image%20Reconstruction%20And%20Temporal%20Extrapolation&body=Title%3A%20ACCELERATION%3A%20Sequentially-scanning%20DECT%20Imaging%20Using%20High%20Temporal%0A%20%20Resolution%20Image%20Reconstruction%20And%20Temporal%20Extrapolation%0AAuthor%3A%20Qiaoxin%20Li%20and%20Dong%20Liang%20and%20Yinsheng%20Li%0AAbstract%3A%20%20%20Dual-energy%20computed%20tomography%20%28DECT%29%20has%20been%20widely%20used%20to%20obtain%0Aquantitative%20elemental%20composition%20of%20imaged%20subjects%20for%20personalized%20and%0Aprecise%20medical%20diagnosis.%20Compared%20with%20existing%20high-end%20DECT%20leveraging%0Aadvanced%20X-ray%20source%20and/or%20detector%20technologies%2C%20the%20use%20of%20the%0Asequentially-scanning%20data%20acquisition%20scheme%20to%20implement%20DECT%20may%20make%0Abroader%20impact%20on%20clinical%20practice%20because%20this%20scheme%20requires%20no%20specialized%0Ahardware%20designs.%20However%2C%20since%20the%20concentration%20of%20iodinated%20contrast%20agent%0Ain%20the%20imaged%20subject%20varies%20over%20time%2C%20sequentially-scanned%20data%20sets%20acquired%0Aat%20two%20tube%20potentials%20are%20temporally%20inconsistent.%20As%20existing%20material%0Adecomposition%20approaches%20for%20DECT%20assume%20that%20the%20data%20sets%20acquired%20at%20two%0Atube%20potentials%20are%20temporally%20consistent%2C%20the%20violation%20of%20this%20assumption%0Aresults%20in%20inaccurate%20quantification%20accuracy%20of%20iodine%20concentration.%20In%20this%0Awork%2C%20we%20developed%20a%20technique%20to%20achieve%20sequentially-scanning%20DECT%20imaging%0Ausing%20high%20temporal%20resolution%20image%20reconstruction%20and%20temporal%20extrapolation%2C%0AACCELERATION%20in%20short%2C%20to%20address%20the%20technical%20challenge%20induced%20by%20temporal%0Ainconsistency%20of%20sequentially-scanned%20data%20sets%20and%20improve%20iodine%0Aquantification%20accuracy%20in%20sequentially-scanning%20DECT.%20ACCELERATION%20has%20been%0Avalidated%20and%20evaluated%20using%20numerical%20simulation%20data%20sets%20generated%20from%0Aclinical%20human%20subject%20exams.%20Results%20demonstrated%20the%20improvement%20of%20iodine%0Aquantification%20accuracy%20using%20ACCELERATION.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06163v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DACCELERATION%253A%2520Sequentially-scanning%2520DECT%2520Imaging%2520Using%2520High%2520Temporal%250A%2520%2520Resolution%2520Image%2520Reconstruction%2520And%2520Temporal%2520Extrapolation%26entry.906535625%3DQiaoxin%2520Li%2520and%2520Dong%2520Liang%2520and%2520Yinsheng%2520Li%26entry.1292438233%3D%2520%2520Dual-energy%2520computed%2520tomography%2520%2528DECT%2529%2520has%2520been%2520widely%2520used%2520to%2520obtain%250Aquantitative%2520elemental%2520composition%2520of%2520imaged%2520subjects%2520for%2520personalized%2520and%250Aprecise%2520medical%2520diagnosis.%2520Compared%2520with%2520existing%2520high-end%2520DECT%2520leveraging%250Aadvanced%2520X-ray%2520source%2520and/or%2520detector%2520technologies%252C%2520the%2520use%2520of%2520the%250Asequentially-scanning%2520data%2520acquisition%2520scheme%2520to%2520implement%2520DECT%2520may%2520make%250Abroader%2520impact%2520on%2520clinical%2520practice%2520because%2520this%2520scheme%2520requires%2520no%2520specialized%250Ahardware%2520designs.%2520However%252C%2520since%2520the%2520concentration%2520of%2520iodinated%2520contrast%2520agent%250Ain%2520the%2520imaged%2520subject%2520varies%2520over%2520time%252C%2520sequentially-scanned%2520data%2520sets%2520acquired%250Aat%2520two%2520tube%2520potentials%2520are%2520temporally%2520inconsistent.%2520As%2520existing%2520material%250Adecomposition%2520approaches%2520for%2520DECT%2520assume%2520that%2520the%2520data%2520sets%2520acquired%2520at%2520two%250Atube%2520potentials%2520are%2520temporally%2520consistent%252C%2520the%2520violation%2520of%2520this%2520assumption%250Aresults%2520in%2520inaccurate%2520quantification%2520accuracy%2520of%2520iodine%2520concentration.%2520In%2520this%250Awork%252C%2520we%2520developed%2520a%2520technique%2520to%2520achieve%2520sequentially-scanning%2520DECT%2520imaging%250Ausing%2520high%2520temporal%2520resolution%2520image%2520reconstruction%2520and%2520temporal%2520extrapolation%252C%250AACCELERATION%2520in%2520short%252C%2520to%2520address%2520the%2520technical%2520challenge%2520induced%2520by%2520temporal%250Ainconsistency%2520of%2520sequentially-scanned%2520data%2520sets%2520and%2520improve%2520iodine%250Aquantification%2520accuracy%2520in%2520sequentially-scanning%2520DECT.%2520ACCELERATION%2520has%2520been%250Avalidated%2520and%2520evaluated%2520using%2520numerical%2520simulation%2520data%2520sets%2520generated%2520from%250Aclinical%2520human%2520subject%2520exams.%2520Results%2520demonstrated%2520the%2520improvement%2520of%2520iodine%250Aquantification%2520accuracy%2520using%2520ACCELERATION.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06163v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ACCELERATION%3A%20Sequentially-scanning%20DECT%20Imaging%20Using%20High%20Temporal%0A%20%20Resolution%20Image%20Reconstruction%20And%20Temporal%20Extrapolation&entry.906535625=Qiaoxin%20Li%20and%20Dong%20Liang%20and%20Yinsheng%20Li&entry.1292438233=%20%20Dual-energy%20computed%20tomography%20%28DECT%29%20has%20been%20widely%20used%20to%20obtain%0Aquantitative%20elemental%20composition%20of%20imaged%20subjects%20for%20personalized%20and%0Aprecise%20medical%20diagnosis.%20Compared%20with%20existing%20high-end%20DECT%20leveraging%0Aadvanced%20X-ray%20source%20and/or%20detector%20technologies%2C%20the%20use%20of%20the%0Asequentially-scanning%20data%20acquisition%20scheme%20to%20implement%20DECT%20may%20make%0Abroader%20impact%20on%20clinical%20practice%20because%20this%20scheme%20requires%20no%20specialized%0Ahardware%20designs.%20However%2C%20since%20the%20concentration%20of%20iodinated%20contrast%20agent%0Ain%20the%20imaged%20subject%20varies%20over%20time%2C%20sequentially-scanned%20data%20sets%20acquired%0Aat%20two%20tube%20potentials%20are%20temporally%20inconsistent.%20As%20existing%20material%0Adecomposition%20approaches%20for%20DECT%20assume%20that%20the%20data%20sets%20acquired%20at%20two%0Atube%20potentials%20are%20temporally%20consistent%2C%20the%20violation%20of%20this%20assumption%0Aresults%20in%20inaccurate%20quantification%20accuracy%20of%20iodine%20concentration.%20In%20this%0Awork%2C%20we%20developed%20a%20technique%20to%20achieve%20sequentially-scanning%20DECT%20imaging%0Ausing%20high%20temporal%20resolution%20image%20reconstruction%20and%20temporal%20extrapolation%2C%0AACCELERATION%20in%20short%2C%20to%20address%20the%20technical%20challenge%20induced%20by%20temporal%0Ainconsistency%20of%20sequentially-scanned%20data%20sets%20and%20improve%20iodine%0Aquantification%20accuracy%20in%20sequentially-scanning%20DECT.%20ACCELERATION%20has%20been%0Avalidated%20and%20evaluated%20using%20numerical%20simulation%20data%20sets%20generated%20from%0Aclinical%20human%20subject%20exams.%20Results%20demonstrated%20the%20improvement%20of%20iodine%0Aquantification%20accuracy%20using%20ACCELERATION.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06163v1&entry.124074799=Read"},
{"title": "Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let's Take\n  TravelPlanner as an Example", "author": "Yanan Chen and Ali Pesaranghader and Tanmana Sadhu and Dong Hoon Yi", "abstract": "  Large language models (LLMs) have brought autonomous agents closer to\nartificial general intelligence (AGI) due to their promising generalization and\nemergent capabilities. There is, however, a lack of studies on how LLM-based\nagents behave, why they could potentially fail, and how to improve them,\nparticularly in demanding real-world planning tasks. In this paper, as an\neffort to fill the gap, we present our study using a realistic benchmark,\nTravelPlanner, where an agent must meet multiple constraints to generate\naccurate plans. We leverage this benchmark to address four key research\nquestions: (1) are LLM agents robust enough to lengthy and noisy contexts when\nit comes to reasoning and planning? (2) can few-shot prompting adversely impact\nthe performance of LLM agents in scenarios with long context? (3) can we rely\non refinement to improve plans, and (4) can fine-tuning LLMs with both positive\nand negative feedback lead to further improvement? Our comprehensive\nexperiments indicate that, firstly, LLMs often fail to attend to crucial parts\nof a long context, despite their ability to handle extensive reference\ninformation and few-shot examples; secondly, they still struggle with analyzing\nthe long plans and cannot provide accurate feedback for refinement; thirdly, we\npropose Feedback-Aware Fine-Tuning (FAFT), which leverages both positive and\nnegative feedback, resulting in substantial gains over Supervised Fine-Tuning\n(SFT). Our findings offer in-depth insights to the community on various aspects\nrelated to real-world planning applications.\n", "link": "http://arxiv.org/abs/2408.06318v1", "date": "2024-08-12", "relevancy": 2.0602, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5206}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5126}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5072}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20We%20Rely%20on%20LLM%20Agents%20to%20Draft%20Long-Horizon%20Plans%3F%20Let%27s%20Take%0A%20%20TravelPlanner%20as%20an%20Example&body=Title%3A%20Can%20We%20Rely%20on%20LLM%20Agents%20to%20Draft%20Long-Horizon%20Plans%3F%20Let%27s%20Take%0A%20%20TravelPlanner%20as%20an%20Example%0AAuthor%3A%20Yanan%20Chen%20and%20Ali%20Pesaranghader%20and%20Tanmana%20Sadhu%20and%20Dong%20Hoon%20Yi%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20brought%20autonomous%20agents%20closer%20to%0Aartificial%20general%20intelligence%20%28AGI%29%20due%20to%20their%20promising%20generalization%20and%0Aemergent%20capabilities.%20There%20is%2C%20however%2C%20a%20lack%20of%20studies%20on%20how%20LLM-based%0Aagents%20behave%2C%20why%20they%20could%20potentially%20fail%2C%20and%20how%20to%20improve%20them%2C%0Aparticularly%20in%20demanding%20real-world%20planning%20tasks.%20In%20this%20paper%2C%20as%20an%0Aeffort%20to%20fill%20the%20gap%2C%20we%20present%20our%20study%20using%20a%20realistic%20benchmark%2C%0ATravelPlanner%2C%20where%20an%20agent%20must%20meet%20multiple%20constraints%20to%20generate%0Aaccurate%20plans.%20We%20leverage%20this%20benchmark%20to%20address%20four%20key%20research%0Aquestions%3A%20%281%29%20are%20LLM%20agents%20robust%20enough%20to%20lengthy%20and%20noisy%20contexts%20when%0Ait%20comes%20to%20reasoning%20and%20planning%3F%20%282%29%20can%20few-shot%20prompting%20adversely%20impact%0Athe%20performance%20of%20LLM%20agents%20in%20scenarios%20with%20long%20context%3F%20%283%29%20can%20we%20rely%0Aon%20refinement%20to%20improve%20plans%2C%20and%20%284%29%20can%20fine-tuning%20LLMs%20with%20both%20positive%0Aand%20negative%20feedback%20lead%20to%20further%20improvement%3F%20Our%20comprehensive%0Aexperiments%20indicate%20that%2C%20firstly%2C%20LLMs%20often%20fail%20to%20attend%20to%20crucial%20parts%0Aof%20a%20long%20context%2C%20despite%20their%20ability%20to%20handle%20extensive%20reference%0Ainformation%20and%20few-shot%20examples%3B%20secondly%2C%20they%20still%20struggle%20with%20analyzing%0Athe%20long%20plans%20and%20cannot%20provide%20accurate%20feedback%20for%20refinement%3B%20thirdly%2C%20we%0Apropose%20Feedback-Aware%20Fine-Tuning%20%28FAFT%29%2C%20which%20leverages%20both%20positive%20and%0Anegative%20feedback%2C%20resulting%20in%20substantial%20gains%20over%20Supervised%20Fine-Tuning%0A%28SFT%29.%20Our%20findings%20offer%20in-depth%20insights%20to%20the%20community%20on%20various%20aspects%0Arelated%20to%20real-world%20planning%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06318v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520We%2520Rely%2520on%2520LLM%2520Agents%2520to%2520Draft%2520Long-Horizon%2520Plans%253F%2520Let%2527s%2520Take%250A%2520%2520TravelPlanner%2520as%2520an%2520Example%26entry.906535625%3DYanan%2520Chen%2520and%2520Ali%2520Pesaranghader%2520and%2520Tanmana%2520Sadhu%2520and%2520Dong%2520Hoon%2520Yi%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520brought%2520autonomous%2520agents%2520closer%2520to%250Aartificial%2520general%2520intelligence%2520%2528AGI%2529%2520due%2520to%2520their%2520promising%2520generalization%2520and%250Aemergent%2520capabilities.%2520There%2520is%252C%2520however%252C%2520a%2520lack%2520of%2520studies%2520on%2520how%2520LLM-based%250Aagents%2520behave%252C%2520why%2520they%2520could%2520potentially%2520fail%252C%2520and%2520how%2520to%2520improve%2520them%252C%250Aparticularly%2520in%2520demanding%2520real-world%2520planning%2520tasks.%2520In%2520this%2520paper%252C%2520as%2520an%250Aeffort%2520to%2520fill%2520the%2520gap%252C%2520we%2520present%2520our%2520study%2520using%2520a%2520realistic%2520benchmark%252C%250ATravelPlanner%252C%2520where%2520an%2520agent%2520must%2520meet%2520multiple%2520constraints%2520to%2520generate%250Aaccurate%2520plans.%2520We%2520leverage%2520this%2520benchmark%2520to%2520address%2520four%2520key%2520research%250Aquestions%253A%2520%25281%2529%2520are%2520LLM%2520agents%2520robust%2520enough%2520to%2520lengthy%2520and%2520noisy%2520contexts%2520when%250Ait%2520comes%2520to%2520reasoning%2520and%2520planning%253F%2520%25282%2529%2520can%2520few-shot%2520prompting%2520adversely%2520impact%250Athe%2520performance%2520of%2520LLM%2520agents%2520in%2520scenarios%2520with%2520long%2520context%253F%2520%25283%2529%2520can%2520we%2520rely%250Aon%2520refinement%2520to%2520improve%2520plans%252C%2520and%2520%25284%2529%2520can%2520fine-tuning%2520LLMs%2520with%2520both%2520positive%250Aand%2520negative%2520feedback%2520lead%2520to%2520further%2520improvement%253F%2520Our%2520comprehensive%250Aexperiments%2520indicate%2520that%252C%2520firstly%252C%2520LLMs%2520often%2520fail%2520to%2520attend%2520to%2520crucial%2520parts%250Aof%2520a%2520long%2520context%252C%2520despite%2520their%2520ability%2520to%2520handle%2520extensive%2520reference%250Ainformation%2520and%2520few-shot%2520examples%253B%2520secondly%252C%2520they%2520still%2520struggle%2520with%2520analyzing%250Athe%2520long%2520plans%2520and%2520cannot%2520provide%2520accurate%2520feedback%2520for%2520refinement%253B%2520thirdly%252C%2520we%250Apropose%2520Feedback-Aware%2520Fine-Tuning%2520%2528FAFT%2529%252C%2520which%2520leverages%2520both%2520positive%2520and%250Anegative%2520feedback%252C%2520resulting%2520in%2520substantial%2520gains%2520over%2520Supervised%2520Fine-Tuning%250A%2528SFT%2529.%2520Our%2520findings%2520offer%2520in-depth%2520insights%2520to%2520the%2520community%2520on%2520various%2520aspects%250Arelated%2520to%2520real-world%2520planning%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06318v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20We%20Rely%20on%20LLM%20Agents%20to%20Draft%20Long-Horizon%20Plans%3F%20Let%27s%20Take%0A%20%20TravelPlanner%20as%20an%20Example&entry.906535625=Yanan%20Chen%20and%20Ali%20Pesaranghader%20and%20Tanmana%20Sadhu%20and%20Dong%20Hoon%20Yi&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20brought%20autonomous%20agents%20closer%20to%0Aartificial%20general%20intelligence%20%28AGI%29%20due%20to%20their%20promising%20generalization%20and%0Aemergent%20capabilities.%20There%20is%2C%20however%2C%20a%20lack%20of%20studies%20on%20how%20LLM-based%0Aagents%20behave%2C%20why%20they%20could%20potentially%20fail%2C%20and%20how%20to%20improve%20them%2C%0Aparticularly%20in%20demanding%20real-world%20planning%20tasks.%20In%20this%20paper%2C%20as%20an%0Aeffort%20to%20fill%20the%20gap%2C%20we%20present%20our%20study%20using%20a%20realistic%20benchmark%2C%0ATravelPlanner%2C%20where%20an%20agent%20must%20meet%20multiple%20constraints%20to%20generate%0Aaccurate%20plans.%20We%20leverage%20this%20benchmark%20to%20address%20four%20key%20research%0Aquestions%3A%20%281%29%20are%20LLM%20agents%20robust%20enough%20to%20lengthy%20and%20noisy%20contexts%20when%0Ait%20comes%20to%20reasoning%20and%20planning%3F%20%282%29%20can%20few-shot%20prompting%20adversely%20impact%0Athe%20performance%20of%20LLM%20agents%20in%20scenarios%20with%20long%20context%3F%20%283%29%20can%20we%20rely%0Aon%20refinement%20to%20improve%20plans%2C%20and%20%284%29%20can%20fine-tuning%20LLMs%20with%20both%20positive%0Aand%20negative%20feedback%20lead%20to%20further%20improvement%3F%20Our%20comprehensive%0Aexperiments%20indicate%20that%2C%20firstly%2C%20LLMs%20often%20fail%20to%20attend%20to%20crucial%20parts%0Aof%20a%20long%20context%2C%20despite%20their%20ability%20to%20handle%20extensive%20reference%0Ainformation%20and%20few-shot%20examples%3B%20secondly%2C%20they%20still%20struggle%20with%20analyzing%0Athe%20long%20plans%20and%20cannot%20provide%20accurate%20feedback%20for%20refinement%3B%20thirdly%2C%20we%0Apropose%20Feedback-Aware%20Fine-Tuning%20%28FAFT%29%2C%20which%20leverages%20both%20positive%20and%0Anegative%20feedback%2C%20resulting%20in%20substantial%20gains%20over%20Supervised%20Fine-Tuning%0A%28SFT%29.%20Our%20findings%20offer%20in-depth%20insights%20to%20the%20community%20on%20various%20aspects%0Arelated%20to%20real-world%20planning%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06318v1&entry.124074799=Read"},
{"title": "Open-Source Molecular Processing Pipeline for Generating Molecules", "author": "Shreyas V and Jose Siguenza and Karan Bania and Bharath Ramsundar", "abstract": "  Generative models for molecules have shown considerable promise for use in\ncomputational chemistry, but remain difficult to use for non-experts. For this\nreason, we introduce open-source infrastructure for easily building generative\nmolecular models into the widely used DeepChem [Ramsundar et al., 2019] library\nwith the aim of creating a robust and reusable molecular generation pipeline.\nIn particular, we add high quality PyTorch [Paszke et al., 2019]\nimplementations of the Molecular Generative Adversarial Networks (MolGAN) [Cao\nand Kipf, 2022] and Normalizing Flows [Papamakarios et al., 2021]. Our\nimplementations show strong performance comparable with past work [Kuznetsov\nand Polykovskiy, 2021, Cao and Kipf, 2022].\n", "link": "http://arxiv.org/abs/2408.06261v1", "date": "2024-08-12", "relevancy": 2.0372, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5191}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5083}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4872}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open-Source%20Molecular%20Processing%20Pipeline%20for%20Generating%20Molecules&body=Title%3A%20Open-Source%20Molecular%20Processing%20Pipeline%20for%20Generating%20Molecules%0AAuthor%3A%20Shreyas%20V%20and%20Jose%20Siguenza%20and%20Karan%20Bania%20and%20Bharath%20Ramsundar%0AAbstract%3A%20%20%20Generative%20models%20for%20molecules%20have%20shown%20considerable%20promise%20for%20use%20in%0Acomputational%20chemistry%2C%20but%20remain%20difficult%20to%20use%20for%20non-experts.%20For%20this%0Areason%2C%20we%20introduce%20open-source%20infrastructure%20for%20easily%20building%20generative%0Amolecular%20models%20into%20the%20widely%20used%20DeepChem%20%5BRamsundar%20et%20al.%2C%202019%5D%20library%0Awith%20the%20aim%20of%20creating%20a%20robust%20and%20reusable%20molecular%20generation%20pipeline.%0AIn%20particular%2C%20we%20add%20high%20quality%20PyTorch%20%5BPaszke%20et%20al.%2C%202019%5D%0Aimplementations%20of%20the%20Molecular%20Generative%20Adversarial%20Networks%20%28MolGAN%29%20%5BCao%0Aand%20Kipf%2C%202022%5D%20and%20Normalizing%20Flows%20%5BPapamakarios%20et%20al.%2C%202021%5D.%20Our%0Aimplementations%20show%20strong%20performance%20comparable%20with%20past%20work%20%5BKuznetsov%0Aand%20Polykovskiy%2C%202021%2C%20Cao%20and%20Kipf%2C%202022%5D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06261v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen-Source%2520Molecular%2520Processing%2520Pipeline%2520for%2520Generating%2520Molecules%26entry.906535625%3DShreyas%2520V%2520and%2520Jose%2520Siguenza%2520and%2520Karan%2520Bania%2520and%2520Bharath%2520Ramsundar%26entry.1292438233%3D%2520%2520Generative%2520models%2520for%2520molecules%2520have%2520shown%2520considerable%2520promise%2520for%2520use%2520in%250Acomputational%2520chemistry%252C%2520but%2520remain%2520difficult%2520to%2520use%2520for%2520non-experts.%2520For%2520this%250Areason%252C%2520we%2520introduce%2520open-source%2520infrastructure%2520for%2520easily%2520building%2520generative%250Amolecular%2520models%2520into%2520the%2520widely%2520used%2520DeepChem%2520%255BRamsundar%2520et%2520al.%252C%25202019%255D%2520library%250Awith%2520the%2520aim%2520of%2520creating%2520a%2520robust%2520and%2520reusable%2520molecular%2520generation%2520pipeline.%250AIn%2520particular%252C%2520we%2520add%2520high%2520quality%2520PyTorch%2520%255BPaszke%2520et%2520al.%252C%25202019%255D%250Aimplementations%2520of%2520the%2520Molecular%2520Generative%2520Adversarial%2520Networks%2520%2528MolGAN%2529%2520%255BCao%250Aand%2520Kipf%252C%25202022%255D%2520and%2520Normalizing%2520Flows%2520%255BPapamakarios%2520et%2520al.%252C%25202021%255D.%2520Our%250Aimplementations%2520show%2520strong%2520performance%2520comparable%2520with%2520past%2520work%2520%255BKuznetsov%250Aand%2520Polykovskiy%252C%25202021%252C%2520Cao%2520and%2520Kipf%252C%25202022%255D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06261v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-Source%20Molecular%20Processing%20Pipeline%20for%20Generating%20Molecules&entry.906535625=Shreyas%20V%20and%20Jose%20Siguenza%20and%20Karan%20Bania%20and%20Bharath%20Ramsundar&entry.1292438233=%20%20Generative%20models%20for%20molecules%20have%20shown%20considerable%20promise%20for%20use%20in%0Acomputational%20chemistry%2C%20but%20remain%20difficult%20to%20use%20for%20non-experts.%20For%20this%0Areason%2C%20we%20introduce%20open-source%20infrastructure%20for%20easily%20building%20generative%0Amolecular%20models%20into%20the%20widely%20used%20DeepChem%20%5BRamsundar%20et%20al.%2C%202019%5D%20library%0Awith%20the%20aim%20of%20creating%20a%20robust%20and%20reusable%20molecular%20generation%20pipeline.%0AIn%20particular%2C%20we%20add%20high%20quality%20PyTorch%20%5BPaszke%20et%20al.%2C%202019%5D%0Aimplementations%20of%20the%20Molecular%20Generative%20Adversarial%20Networks%20%28MolGAN%29%20%5BCao%0Aand%20Kipf%2C%202022%5D%20and%20Normalizing%20Flows%20%5BPapamakarios%20et%20al.%2C%202021%5D.%20Our%0Aimplementations%20show%20strong%20performance%20comparable%20with%20past%20work%20%5BKuznetsov%0Aand%20Polykovskiy%2C%202021%2C%20Cao%20and%20Kipf%2C%202022%5D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06261v1&entry.124074799=Read"},
{"title": "HeLiMOS: A Dataset for Moving Object Segmentation in 3D Point Clouds\n  From Heterogeneous LiDAR Sensors", "author": "Hyungtae Lim and Seoyeon Jang and Benedikt Mersch and Jens Behley and Hyun Myung and Cyrill Stachniss", "abstract": "  Moving object segmentation (MOS) using a 3D light detection and ranging\n(LiDAR) sensor is crucial for scene understanding and identification of moving\nobjects. Despite the availability of various types of 3D LiDAR sensors in the\nmarket, MOS research still predominantly focuses on 3D point clouds from\nmechanically spinning omnidirectional LiDAR sensors. Thus, we are, for example,\nlacking a dataset with MOS labels for point clouds from solid-state LiDAR\nsensors which have irregular scanning patterns. In this paper, we present a\nlabeled dataset, called \\textit{HeLiMOS}, that enables to test MOS approaches\non four heterogeneous LiDAR sensors, including two solid-state LiDAR sensors.\nFurthermore, we introduce a novel automatic labeling method to substantially\nreduce the labeling effort required from human annotators. To this end, our\nframework exploits an instance-aware static map building approach and\ntracking-based false label filtering. Finally, we provide experimental results\nregarding the performance of commonly used state-of-the-art MOS approaches on\nHeLiMOS that suggest a new direction for a sensor-agnostic MOS, which generally\nworks regardless of the type of LiDAR sensors used to capture 3D point clouds.\nOur dataset is available at https://sites.google.com/view/helimos.\n", "link": "http://arxiv.org/abs/2408.06328v1", "date": "2024-08-12", "relevancy": 2.0333, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5158}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5117}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4995}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HeLiMOS%3A%20A%20Dataset%20for%20Moving%20Object%20Segmentation%20in%203D%20Point%20Clouds%0A%20%20From%20Heterogeneous%20LiDAR%20Sensors&body=Title%3A%20HeLiMOS%3A%20A%20Dataset%20for%20Moving%20Object%20Segmentation%20in%203D%20Point%20Clouds%0A%20%20From%20Heterogeneous%20LiDAR%20Sensors%0AAuthor%3A%20Hyungtae%20Lim%20and%20Seoyeon%20Jang%20and%20Benedikt%20Mersch%20and%20Jens%20Behley%20and%20Hyun%20Myung%20and%20Cyrill%20Stachniss%0AAbstract%3A%20%20%20Moving%20object%20segmentation%20%28MOS%29%20using%20a%203D%20light%20detection%20and%20ranging%0A%28LiDAR%29%20sensor%20is%20crucial%20for%20scene%20understanding%20and%20identification%20of%20moving%0Aobjects.%20Despite%20the%20availability%20of%20various%20types%20of%203D%20LiDAR%20sensors%20in%20the%0Amarket%2C%20MOS%20research%20still%20predominantly%20focuses%20on%203D%20point%20clouds%20from%0Amechanically%20spinning%20omnidirectional%20LiDAR%20sensors.%20Thus%2C%20we%20are%2C%20for%20example%2C%0Alacking%20a%20dataset%20with%20MOS%20labels%20for%20point%20clouds%20from%20solid-state%20LiDAR%0Asensors%20which%20have%20irregular%20scanning%20patterns.%20In%20this%20paper%2C%20we%20present%20a%0Alabeled%20dataset%2C%20called%20%5Ctextit%7BHeLiMOS%7D%2C%20that%20enables%20to%20test%20MOS%20approaches%0Aon%20four%20heterogeneous%20LiDAR%20sensors%2C%20including%20two%20solid-state%20LiDAR%20sensors.%0AFurthermore%2C%20we%20introduce%20a%20novel%20automatic%20labeling%20method%20to%20substantially%0Areduce%20the%20labeling%20effort%20required%20from%20human%20annotators.%20To%20this%20end%2C%20our%0Aframework%20exploits%20an%20instance-aware%20static%20map%20building%20approach%20and%0Atracking-based%20false%20label%20filtering.%20Finally%2C%20we%20provide%20experimental%20results%0Aregarding%20the%20performance%20of%20commonly%20used%20state-of-the-art%20MOS%20approaches%20on%0AHeLiMOS%20that%20suggest%20a%20new%20direction%20for%20a%20sensor-agnostic%20MOS%2C%20which%20generally%0Aworks%20regardless%20of%20the%20type%20of%20LiDAR%20sensors%20used%20to%20capture%203D%20point%20clouds.%0AOur%20dataset%20is%20available%20at%20https%3A//sites.google.com/view/helimos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06328v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeLiMOS%253A%2520A%2520Dataset%2520for%2520Moving%2520Object%2520Segmentation%2520in%25203D%2520Point%2520Clouds%250A%2520%2520From%2520Heterogeneous%2520LiDAR%2520Sensors%26entry.906535625%3DHyungtae%2520Lim%2520and%2520Seoyeon%2520Jang%2520and%2520Benedikt%2520Mersch%2520and%2520Jens%2520Behley%2520and%2520Hyun%2520Myung%2520and%2520Cyrill%2520Stachniss%26entry.1292438233%3D%2520%2520Moving%2520object%2520segmentation%2520%2528MOS%2529%2520using%2520a%25203D%2520light%2520detection%2520and%2520ranging%250A%2528LiDAR%2529%2520sensor%2520is%2520crucial%2520for%2520scene%2520understanding%2520and%2520identification%2520of%2520moving%250Aobjects.%2520Despite%2520the%2520availability%2520of%2520various%2520types%2520of%25203D%2520LiDAR%2520sensors%2520in%2520the%250Amarket%252C%2520MOS%2520research%2520still%2520predominantly%2520focuses%2520on%25203D%2520point%2520clouds%2520from%250Amechanically%2520spinning%2520omnidirectional%2520LiDAR%2520sensors.%2520Thus%252C%2520we%2520are%252C%2520for%2520example%252C%250Alacking%2520a%2520dataset%2520with%2520MOS%2520labels%2520for%2520point%2520clouds%2520from%2520solid-state%2520LiDAR%250Asensors%2520which%2520have%2520irregular%2520scanning%2520patterns.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%250Alabeled%2520dataset%252C%2520called%2520%255Ctextit%257BHeLiMOS%257D%252C%2520that%2520enables%2520to%2520test%2520MOS%2520approaches%250Aon%2520four%2520heterogeneous%2520LiDAR%2520sensors%252C%2520including%2520two%2520solid-state%2520LiDAR%2520sensors.%250AFurthermore%252C%2520we%2520introduce%2520a%2520novel%2520automatic%2520labeling%2520method%2520to%2520substantially%250Areduce%2520the%2520labeling%2520effort%2520required%2520from%2520human%2520annotators.%2520To%2520this%2520end%252C%2520our%250Aframework%2520exploits%2520an%2520instance-aware%2520static%2520map%2520building%2520approach%2520and%250Atracking-based%2520false%2520label%2520filtering.%2520Finally%252C%2520we%2520provide%2520experimental%2520results%250Aregarding%2520the%2520performance%2520of%2520commonly%2520used%2520state-of-the-art%2520MOS%2520approaches%2520on%250AHeLiMOS%2520that%2520suggest%2520a%2520new%2520direction%2520for%2520a%2520sensor-agnostic%2520MOS%252C%2520which%2520generally%250Aworks%2520regardless%2520of%2520the%2520type%2520of%2520LiDAR%2520sensors%2520used%2520to%2520capture%25203D%2520point%2520clouds.%250AOur%2520dataset%2520is%2520available%2520at%2520https%253A//sites.google.com/view/helimos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06328v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HeLiMOS%3A%20A%20Dataset%20for%20Moving%20Object%20Segmentation%20in%203D%20Point%20Clouds%0A%20%20From%20Heterogeneous%20LiDAR%20Sensors&entry.906535625=Hyungtae%20Lim%20and%20Seoyeon%20Jang%20and%20Benedikt%20Mersch%20and%20Jens%20Behley%20and%20Hyun%20Myung%20and%20Cyrill%20Stachniss&entry.1292438233=%20%20Moving%20object%20segmentation%20%28MOS%29%20using%20a%203D%20light%20detection%20and%20ranging%0A%28LiDAR%29%20sensor%20is%20crucial%20for%20scene%20understanding%20and%20identification%20of%20moving%0Aobjects.%20Despite%20the%20availability%20of%20various%20types%20of%203D%20LiDAR%20sensors%20in%20the%0Amarket%2C%20MOS%20research%20still%20predominantly%20focuses%20on%203D%20point%20clouds%20from%0Amechanically%20spinning%20omnidirectional%20LiDAR%20sensors.%20Thus%2C%20we%20are%2C%20for%20example%2C%0Alacking%20a%20dataset%20with%20MOS%20labels%20for%20point%20clouds%20from%20solid-state%20LiDAR%0Asensors%20which%20have%20irregular%20scanning%20patterns.%20In%20this%20paper%2C%20we%20present%20a%0Alabeled%20dataset%2C%20called%20%5Ctextit%7BHeLiMOS%7D%2C%20that%20enables%20to%20test%20MOS%20approaches%0Aon%20four%20heterogeneous%20LiDAR%20sensors%2C%20including%20two%20solid-state%20LiDAR%20sensors.%0AFurthermore%2C%20we%20introduce%20a%20novel%20automatic%20labeling%20method%20to%20substantially%0Areduce%20the%20labeling%20effort%20required%20from%20human%20annotators.%20To%20this%20end%2C%20our%0Aframework%20exploits%20an%20instance-aware%20static%20map%20building%20approach%20and%0Atracking-based%20false%20label%20filtering.%20Finally%2C%20we%20provide%20experimental%20results%0Aregarding%20the%20performance%20of%20commonly%20used%20state-of-the-art%20MOS%20approaches%20on%0AHeLiMOS%20that%20suggest%20a%20new%20direction%20for%20a%20sensor-agnostic%20MOS%2C%20which%20generally%0Aworks%20regardless%20of%20the%20type%20of%20LiDAR%20sensors%20used%20to%20capture%203D%20point%20clouds.%0AOur%20dataset%20is%20available%20at%20https%3A//sites.google.com/view/helimos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06328v1&entry.124074799=Read"},
{"title": "CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer", "author": "Zhuoyi Yang and Jiayan Teng and Wendi Zheng and Ming Ding and Shiyu Huang and Jiazheng Xu and Yuanming Yang and Wenyi Hong and Xiaohan Zhang and Guanyu Feng and Da Yin and Xiaotao Gu and Yuxuan Zhang and Weihan Wang and Yean Cheng and Ting Liu and Bin Xu and Yuxiao Dong and Jie Tang", "abstract": "  We introduce CogVideoX, a large-scale diffusion transformer model designed\nfor generating videos based on text prompts. To efficently model video data, we\npropose to levearge a 3D Variational Autoencoder (VAE) to compress videos along\nboth spatial and temporal dimensions. To improve the text-video alignment, we\npropose an expert transformer with the expert adaptive LayerNorm to facilitate\nthe deep fusion between the two modalities. By employing a progressive training\ntechnique, CogVideoX is adept at producing coherent, long-duration videos\ncharacterized by significant motions. In addition, we develop an effective\ntext-video data processing pipeline that includes various data preprocessing\nstrategies and a video captioning method. It significantly helps enhance the\nperformance of CogVideoX, improving both generation quality and semantic\nalignment. Results show that CogVideoX demonstrates state-of-the-art\nperformance across both multiple machine metrics and human evaluations. The\nmodel weights of both the 3D Causal VAE and CogVideoX are publicly available at\nhttps://github.com/THUDM/CogVideo.\n", "link": "http://arxiv.org/abs/2408.06072v1", "date": "2024-08-12", "relevancy": 2.0214, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.7004}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6708}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CogVideoX%3A%20Text-to-Video%20Diffusion%20Models%20with%20An%20Expert%20Transformer&body=Title%3A%20CogVideoX%3A%20Text-to-Video%20Diffusion%20Models%20with%20An%20Expert%20Transformer%0AAuthor%3A%20Zhuoyi%20Yang%20and%20Jiayan%20Teng%20and%20Wendi%20Zheng%20and%20Ming%20Ding%20and%20Shiyu%20Huang%20and%20Jiazheng%20Xu%20and%20Yuanming%20Yang%20and%20Wenyi%20Hong%20and%20Xiaohan%20Zhang%20and%20Guanyu%20Feng%20and%20Da%20Yin%20and%20Xiaotao%20Gu%20and%20Yuxuan%20Zhang%20and%20Weihan%20Wang%20and%20Yean%20Cheng%20and%20Ting%20Liu%20and%20Bin%20Xu%20and%20Yuxiao%20Dong%20and%20Jie%20Tang%0AAbstract%3A%20%20%20We%20introduce%20CogVideoX%2C%20a%20large-scale%20diffusion%20transformer%20model%20designed%0Afor%20generating%20videos%20based%20on%20text%20prompts.%20To%20efficently%20model%20video%20data%2C%20we%0Apropose%20to%20levearge%20a%203D%20Variational%20Autoencoder%20%28VAE%29%20to%20compress%20videos%20along%0Aboth%20spatial%20and%20temporal%20dimensions.%20To%20improve%20the%20text-video%20alignment%2C%20we%0Apropose%20an%20expert%20transformer%20with%20the%20expert%20adaptive%20LayerNorm%20to%20facilitate%0Athe%20deep%20fusion%20between%20the%20two%20modalities.%20By%20employing%20a%20progressive%20training%0Atechnique%2C%20CogVideoX%20is%20adept%20at%20producing%20coherent%2C%20long-duration%20videos%0Acharacterized%20by%20significant%20motions.%20In%20addition%2C%20we%20develop%20an%20effective%0Atext-video%20data%20processing%20pipeline%20that%20includes%20various%20data%20preprocessing%0Astrategies%20and%20a%20video%20captioning%20method.%20It%20significantly%20helps%20enhance%20the%0Aperformance%20of%20CogVideoX%2C%20improving%20both%20generation%20quality%20and%20semantic%0Aalignment.%20Results%20show%20that%20CogVideoX%20demonstrates%20state-of-the-art%0Aperformance%20across%20both%20multiple%20machine%20metrics%20and%20human%20evaluations.%20The%0Amodel%20weights%20of%20both%20the%203D%20Causal%20VAE%20and%20CogVideoX%20are%20publicly%20available%20at%0Ahttps%3A//github.com/THUDM/CogVideo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06072v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCogVideoX%253A%2520Text-to-Video%2520Diffusion%2520Models%2520with%2520An%2520Expert%2520Transformer%26entry.906535625%3DZhuoyi%2520Yang%2520and%2520Jiayan%2520Teng%2520and%2520Wendi%2520Zheng%2520and%2520Ming%2520Ding%2520and%2520Shiyu%2520Huang%2520and%2520Jiazheng%2520Xu%2520and%2520Yuanming%2520Yang%2520and%2520Wenyi%2520Hong%2520and%2520Xiaohan%2520Zhang%2520and%2520Guanyu%2520Feng%2520and%2520Da%2520Yin%2520and%2520Xiaotao%2520Gu%2520and%2520Yuxuan%2520Zhang%2520and%2520Weihan%2520Wang%2520and%2520Yean%2520Cheng%2520and%2520Ting%2520Liu%2520and%2520Bin%2520Xu%2520and%2520Yuxiao%2520Dong%2520and%2520Jie%2520Tang%26entry.1292438233%3D%2520%2520We%2520introduce%2520CogVideoX%252C%2520a%2520large-scale%2520diffusion%2520transformer%2520model%2520designed%250Afor%2520generating%2520videos%2520based%2520on%2520text%2520prompts.%2520To%2520efficently%2520model%2520video%2520data%252C%2520we%250Apropose%2520to%2520levearge%2520a%25203D%2520Variational%2520Autoencoder%2520%2528VAE%2529%2520to%2520compress%2520videos%2520along%250Aboth%2520spatial%2520and%2520temporal%2520dimensions.%2520To%2520improve%2520the%2520text-video%2520alignment%252C%2520we%250Apropose%2520an%2520expert%2520transformer%2520with%2520the%2520expert%2520adaptive%2520LayerNorm%2520to%2520facilitate%250Athe%2520deep%2520fusion%2520between%2520the%2520two%2520modalities.%2520By%2520employing%2520a%2520progressive%2520training%250Atechnique%252C%2520CogVideoX%2520is%2520adept%2520at%2520producing%2520coherent%252C%2520long-duration%2520videos%250Acharacterized%2520by%2520significant%2520motions.%2520In%2520addition%252C%2520we%2520develop%2520an%2520effective%250Atext-video%2520data%2520processing%2520pipeline%2520that%2520includes%2520various%2520data%2520preprocessing%250Astrategies%2520and%2520a%2520video%2520captioning%2520method.%2520It%2520significantly%2520helps%2520enhance%2520the%250Aperformance%2520of%2520CogVideoX%252C%2520improving%2520both%2520generation%2520quality%2520and%2520semantic%250Aalignment.%2520Results%2520show%2520that%2520CogVideoX%2520demonstrates%2520state-of-the-art%250Aperformance%2520across%2520both%2520multiple%2520machine%2520metrics%2520and%2520human%2520evaluations.%2520The%250Amodel%2520weights%2520of%2520both%2520the%25203D%2520Causal%2520VAE%2520and%2520CogVideoX%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/THUDM/CogVideo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06072v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CogVideoX%3A%20Text-to-Video%20Diffusion%20Models%20with%20An%20Expert%20Transformer&entry.906535625=Zhuoyi%20Yang%20and%20Jiayan%20Teng%20and%20Wendi%20Zheng%20and%20Ming%20Ding%20and%20Shiyu%20Huang%20and%20Jiazheng%20Xu%20and%20Yuanming%20Yang%20and%20Wenyi%20Hong%20and%20Xiaohan%20Zhang%20and%20Guanyu%20Feng%20and%20Da%20Yin%20and%20Xiaotao%20Gu%20and%20Yuxuan%20Zhang%20and%20Weihan%20Wang%20and%20Yean%20Cheng%20and%20Ting%20Liu%20and%20Bin%20Xu%20and%20Yuxiao%20Dong%20and%20Jie%20Tang&entry.1292438233=%20%20We%20introduce%20CogVideoX%2C%20a%20large-scale%20diffusion%20transformer%20model%20designed%0Afor%20generating%20videos%20based%20on%20text%20prompts.%20To%20efficently%20model%20video%20data%2C%20we%0Apropose%20to%20levearge%20a%203D%20Variational%20Autoencoder%20%28VAE%29%20to%20compress%20videos%20along%0Aboth%20spatial%20and%20temporal%20dimensions.%20To%20improve%20the%20text-video%20alignment%2C%20we%0Apropose%20an%20expert%20transformer%20with%20the%20expert%20adaptive%20LayerNorm%20to%20facilitate%0Athe%20deep%20fusion%20between%20the%20two%20modalities.%20By%20employing%20a%20progressive%20training%0Atechnique%2C%20CogVideoX%20is%20adept%20at%20producing%20coherent%2C%20long-duration%20videos%0Acharacterized%20by%20significant%20motions.%20In%20addition%2C%20we%20develop%20an%20effective%0Atext-video%20data%20processing%20pipeline%20that%20includes%20various%20data%20preprocessing%0Astrategies%20and%20a%20video%20captioning%20method.%20It%20significantly%20helps%20enhance%20the%0Aperformance%20of%20CogVideoX%2C%20improving%20both%20generation%20quality%20and%20semantic%0Aalignment.%20Results%20show%20that%20CogVideoX%20demonstrates%20state-of-the-art%0Aperformance%20across%20both%20multiple%20machine%20metrics%20and%20human%20evaluations.%20The%0Amodel%20weights%20of%20both%20the%203D%20Causal%20VAE%20and%20CogVideoX%20are%20publicly%20available%20at%0Ahttps%3A//github.com/THUDM/CogVideo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06072v1&entry.124074799=Read"},
{"title": "Mambular: A Sequential Model for Tabular Deep Learning", "author": "Anton Frederik Thielmann and Manish Kumar and Christoph Weisser and Arik Reuter and Benjamin S\u00e4fken and Soheila Samiee", "abstract": "  The analysis of tabular data has traditionally been dominated by\ngradient-boosted decision trees (GBDTs), known for their proficiency with mixed\ncategorical and numerical features. However, recent deep learning innovations\nare challenging this dominance. We introduce Mambular, an adaptation of the\nMamba architecture optimized for tabular data. We extensively benchmark\nMambular against state-of-the-art models, including neural networks and\ntree-based methods, and demonstrate its competitive performance across diverse\ndatasets. Additionally, we explore various adaptations of Mambular to\nunderstand its effectiveness for tabular data. We investigate different pooling\nstrategies, feature interaction mechanisms, and bi-directional processing. Our\nanalysis shows that interpreting features as a sequence and passing them\nthrough Mamba layers results in surprisingly performant models. The results\nhighlight Mambulars potential as a versatile and powerful architecture for\ntabular data analysis, expanding the scope of deep learning applications in\nthis domain.\n  The source code is available at https://github.com/basf/mamba-tabular.\n", "link": "http://arxiv.org/abs/2408.06291v1", "date": "2024-08-12", "relevancy": 2.0123, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5147}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.497}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4893}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mambular%3A%20A%20Sequential%20Model%20for%20Tabular%20Deep%20Learning&body=Title%3A%20Mambular%3A%20A%20Sequential%20Model%20for%20Tabular%20Deep%20Learning%0AAuthor%3A%20Anton%20Frederik%20Thielmann%20and%20Manish%20Kumar%20and%20Christoph%20Weisser%20and%20Arik%20Reuter%20and%20Benjamin%20S%C3%A4fken%20and%20Soheila%20Samiee%0AAbstract%3A%20%20%20The%20analysis%20of%20tabular%20data%20has%20traditionally%20been%20dominated%20by%0Agradient-boosted%20decision%20trees%20%28GBDTs%29%2C%20known%20for%20their%20proficiency%20with%20mixed%0Acategorical%20and%20numerical%20features.%20However%2C%20recent%20deep%20learning%20innovations%0Aare%20challenging%20this%20dominance.%20We%20introduce%20Mambular%2C%20an%20adaptation%20of%20the%0AMamba%20architecture%20optimized%20for%20tabular%20data.%20We%20extensively%20benchmark%0AMambular%20against%20state-of-the-art%20models%2C%20including%20neural%20networks%20and%0Atree-based%20methods%2C%20and%20demonstrate%20its%20competitive%20performance%20across%20diverse%0Adatasets.%20Additionally%2C%20we%20explore%20various%20adaptations%20of%20Mambular%20to%0Aunderstand%20its%20effectiveness%20for%20tabular%20data.%20We%20investigate%20different%20pooling%0Astrategies%2C%20feature%20interaction%20mechanisms%2C%20and%20bi-directional%20processing.%20Our%0Aanalysis%20shows%20that%20interpreting%20features%20as%20a%20sequence%20and%20passing%20them%0Athrough%20Mamba%20layers%20results%20in%20surprisingly%20performant%20models.%20The%20results%0Ahighlight%20Mambulars%20potential%20as%20a%20versatile%20and%20powerful%20architecture%20for%0Atabular%20data%20analysis%2C%20expanding%20the%20scope%20of%20deep%20learning%20applications%20in%0Athis%20domain.%0A%20%20The%20source%20code%20is%20available%20at%20https%3A//github.com/basf/mamba-tabular.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06291v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMambular%253A%2520A%2520Sequential%2520Model%2520for%2520Tabular%2520Deep%2520Learning%26entry.906535625%3DAnton%2520Frederik%2520Thielmann%2520and%2520Manish%2520Kumar%2520and%2520Christoph%2520Weisser%2520and%2520Arik%2520Reuter%2520and%2520Benjamin%2520S%25C3%25A4fken%2520and%2520Soheila%2520Samiee%26entry.1292438233%3D%2520%2520The%2520analysis%2520of%2520tabular%2520data%2520has%2520traditionally%2520been%2520dominated%2520by%250Agradient-boosted%2520decision%2520trees%2520%2528GBDTs%2529%252C%2520known%2520for%2520their%2520proficiency%2520with%2520mixed%250Acategorical%2520and%2520numerical%2520features.%2520However%252C%2520recent%2520deep%2520learning%2520innovations%250Aare%2520challenging%2520this%2520dominance.%2520We%2520introduce%2520Mambular%252C%2520an%2520adaptation%2520of%2520the%250AMamba%2520architecture%2520optimized%2520for%2520tabular%2520data.%2520We%2520extensively%2520benchmark%250AMambular%2520against%2520state-of-the-art%2520models%252C%2520including%2520neural%2520networks%2520and%250Atree-based%2520methods%252C%2520and%2520demonstrate%2520its%2520competitive%2520performance%2520across%2520diverse%250Adatasets.%2520Additionally%252C%2520we%2520explore%2520various%2520adaptations%2520of%2520Mambular%2520to%250Aunderstand%2520its%2520effectiveness%2520for%2520tabular%2520data.%2520We%2520investigate%2520different%2520pooling%250Astrategies%252C%2520feature%2520interaction%2520mechanisms%252C%2520and%2520bi-directional%2520processing.%2520Our%250Aanalysis%2520shows%2520that%2520interpreting%2520features%2520as%2520a%2520sequence%2520and%2520passing%2520them%250Athrough%2520Mamba%2520layers%2520results%2520in%2520surprisingly%2520performant%2520models.%2520The%2520results%250Ahighlight%2520Mambulars%2520potential%2520as%2520a%2520versatile%2520and%2520powerful%2520architecture%2520for%250Atabular%2520data%2520analysis%252C%2520expanding%2520the%2520scope%2520of%2520deep%2520learning%2520applications%2520in%250Athis%2520domain.%250A%2520%2520The%2520source%2520code%2520is%2520available%2520at%2520https%253A//github.com/basf/mamba-tabular.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06291v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mambular%3A%20A%20Sequential%20Model%20for%20Tabular%20Deep%20Learning&entry.906535625=Anton%20Frederik%20Thielmann%20and%20Manish%20Kumar%20and%20Christoph%20Weisser%20and%20Arik%20Reuter%20and%20Benjamin%20S%C3%A4fken%20and%20Soheila%20Samiee&entry.1292438233=%20%20The%20analysis%20of%20tabular%20data%20has%20traditionally%20been%20dominated%20by%0Agradient-boosted%20decision%20trees%20%28GBDTs%29%2C%20known%20for%20their%20proficiency%20with%20mixed%0Acategorical%20and%20numerical%20features.%20However%2C%20recent%20deep%20learning%20innovations%0Aare%20challenging%20this%20dominance.%20We%20introduce%20Mambular%2C%20an%20adaptation%20of%20the%0AMamba%20architecture%20optimized%20for%20tabular%20data.%20We%20extensively%20benchmark%0AMambular%20against%20state-of-the-art%20models%2C%20including%20neural%20networks%20and%0Atree-based%20methods%2C%20and%20demonstrate%20its%20competitive%20performance%20across%20diverse%0Adatasets.%20Additionally%2C%20we%20explore%20various%20adaptations%20of%20Mambular%20to%0Aunderstand%20its%20effectiveness%20for%20tabular%20data.%20We%20investigate%20different%20pooling%0Astrategies%2C%20feature%20interaction%20mechanisms%2C%20and%20bi-directional%20processing.%20Our%0Aanalysis%20shows%20that%20interpreting%20features%20as%20a%20sequence%20and%20passing%20them%0Athrough%20Mamba%20layers%20results%20in%20surprisingly%20performant%20models.%20The%20results%0Ahighlight%20Mambulars%20potential%20as%20a%20versatile%20and%20powerful%20architecture%20for%0Atabular%20data%20analysis%2C%20expanding%20the%20scope%20of%20deep%20learning%20applications%20in%0Athis%20domain.%0A%20%20The%20source%20code%20is%20available%20at%20https%3A//github.com/basf/mamba-tabular.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06291v1&entry.124074799=Read"},
{"title": "IIT Bombay Racing Driverless: Autonomous Driving Stack for Formula\n  Student AI", "author": "Yash Rampuria and Deep Boliya and Shreyash Gupta and Gopalan Iyengar and Ayush Rohilla and Mohak Vyas and Chaitanya Langde and Mehul Vijay Chanda and Ronak Gautam Matai and Kothapalli Namitha and Ajinkya Pawar and Bhaskar Biswas and Nakul Agarwal and Rajit Khandelwal and Rohan Kumar and Shubham Agarwal and Vishwam Patel and Abhimanyu Singh Rathore and Amna Rahman and Ayush Mishra and Yash Tangri", "abstract": "  This work presents the design and development of IIT Bombay Racing's Formula\nStudent style autonomous racecar algorithm capable of running at the racing\nevents of Formula Student-AI, held in the UK. The car employs a cutting-edge\nsensor suite of the compute unit NVIDIA Jetson Orin AGX, 2 ZED2i stereo\ncameras, 1 Velodyne Puck VLP16 LiDAR and SBG Systems Ellipse N GNSS/INS IMU. It\nfeatures deep learning algorithms and control systems to navigate complex\ntracks and execute maneuvers without any human intervention. The design process\ninvolved extensive simulations and testing to optimize the vehicle's\nperformance and ensure its safety. The algorithms have been tested on a small\nscale, in-house manufactured 4-wheeled robot and on simulation software. The\nresults obtained for testing various algorithms in perception, simultaneous\nlocalization and mapping, path planning and controls have been detailed.\n", "link": "http://arxiv.org/abs/2408.06113v1", "date": "2024-08-12", "relevancy": 2.0065, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5384}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5131}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4603}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IIT%20Bombay%20Racing%20Driverless%3A%20Autonomous%20Driving%20Stack%20for%20Formula%0A%20%20Student%20AI&body=Title%3A%20IIT%20Bombay%20Racing%20Driverless%3A%20Autonomous%20Driving%20Stack%20for%20Formula%0A%20%20Student%20AI%0AAuthor%3A%20Yash%20Rampuria%20and%20Deep%20Boliya%20and%20Shreyash%20Gupta%20and%20Gopalan%20Iyengar%20and%20Ayush%20Rohilla%20and%20Mohak%20Vyas%20and%20Chaitanya%20Langde%20and%20Mehul%20Vijay%20Chanda%20and%20Ronak%20Gautam%20Matai%20and%20Kothapalli%20Namitha%20and%20Ajinkya%20Pawar%20and%20Bhaskar%20Biswas%20and%20Nakul%20Agarwal%20and%20Rajit%20Khandelwal%20and%20Rohan%20Kumar%20and%20Shubham%20Agarwal%20and%20Vishwam%20Patel%20and%20Abhimanyu%20Singh%20Rathore%20and%20Amna%20Rahman%20and%20Ayush%20Mishra%20and%20Yash%20Tangri%0AAbstract%3A%20%20%20This%20work%20presents%20the%20design%20and%20development%20of%20IIT%20Bombay%20Racing%27s%20Formula%0AStudent%20style%20autonomous%20racecar%20algorithm%20capable%20of%20running%20at%20the%20racing%0Aevents%20of%20Formula%20Student-AI%2C%20held%20in%20the%20UK.%20The%20car%20employs%20a%20cutting-edge%0Asensor%20suite%20of%20the%20compute%20unit%20NVIDIA%20Jetson%20Orin%20AGX%2C%202%20ZED2i%20stereo%0Acameras%2C%201%20Velodyne%20Puck%20VLP16%20LiDAR%20and%20SBG%20Systems%20Ellipse%20N%20GNSS/INS%20IMU.%20It%0Afeatures%20deep%20learning%20algorithms%20and%20control%20systems%20to%20navigate%20complex%0Atracks%20and%20execute%20maneuvers%20without%20any%20human%20intervention.%20The%20design%20process%0Ainvolved%20extensive%20simulations%20and%20testing%20to%20optimize%20the%20vehicle%27s%0Aperformance%20and%20ensure%20its%20safety.%20The%20algorithms%20have%20been%20tested%20on%20a%20small%0Ascale%2C%20in-house%20manufactured%204-wheeled%20robot%20and%20on%20simulation%20software.%20The%0Aresults%20obtained%20for%20testing%20various%20algorithms%20in%20perception%2C%20simultaneous%0Alocalization%20and%20mapping%2C%20path%20planning%20and%20controls%20have%20been%20detailed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06113v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIIT%2520Bombay%2520Racing%2520Driverless%253A%2520Autonomous%2520Driving%2520Stack%2520for%2520Formula%250A%2520%2520Student%2520AI%26entry.906535625%3DYash%2520Rampuria%2520and%2520Deep%2520Boliya%2520and%2520Shreyash%2520Gupta%2520and%2520Gopalan%2520Iyengar%2520and%2520Ayush%2520Rohilla%2520and%2520Mohak%2520Vyas%2520and%2520Chaitanya%2520Langde%2520and%2520Mehul%2520Vijay%2520Chanda%2520and%2520Ronak%2520Gautam%2520Matai%2520and%2520Kothapalli%2520Namitha%2520and%2520Ajinkya%2520Pawar%2520and%2520Bhaskar%2520Biswas%2520and%2520Nakul%2520Agarwal%2520and%2520Rajit%2520Khandelwal%2520and%2520Rohan%2520Kumar%2520and%2520Shubham%2520Agarwal%2520and%2520Vishwam%2520Patel%2520and%2520Abhimanyu%2520Singh%2520Rathore%2520and%2520Amna%2520Rahman%2520and%2520Ayush%2520Mishra%2520and%2520Yash%2520Tangri%26entry.1292438233%3D%2520%2520This%2520work%2520presents%2520the%2520design%2520and%2520development%2520of%2520IIT%2520Bombay%2520Racing%2527s%2520Formula%250AStudent%2520style%2520autonomous%2520racecar%2520algorithm%2520capable%2520of%2520running%2520at%2520the%2520racing%250Aevents%2520of%2520Formula%2520Student-AI%252C%2520held%2520in%2520the%2520UK.%2520The%2520car%2520employs%2520a%2520cutting-edge%250Asensor%2520suite%2520of%2520the%2520compute%2520unit%2520NVIDIA%2520Jetson%2520Orin%2520AGX%252C%25202%2520ZED2i%2520stereo%250Acameras%252C%25201%2520Velodyne%2520Puck%2520VLP16%2520LiDAR%2520and%2520SBG%2520Systems%2520Ellipse%2520N%2520GNSS/INS%2520IMU.%2520It%250Afeatures%2520deep%2520learning%2520algorithms%2520and%2520control%2520systems%2520to%2520navigate%2520complex%250Atracks%2520and%2520execute%2520maneuvers%2520without%2520any%2520human%2520intervention.%2520The%2520design%2520process%250Ainvolved%2520extensive%2520simulations%2520and%2520testing%2520to%2520optimize%2520the%2520vehicle%2527s%250Aperformance%2520and%2520ensure%2520its%2520safety.%2520The%2520algorithms%2520have%2520been%2520tested%2520on%2520a%2520small%250Ascale%252C%2520in-house%2520manufactured%25204-wheeled%2520robot%2520and%2520on%2520simulation%2520software.%2520The%250Aresults%2520obtained%2520for%2520testing%2520various%2520algorithms%2520in%2520perception%252C%2520simultaneous%250Alocalization%2520and%2520mapping%252C%2520path%2520planning%2520and%2520controls%2520have%2520been%2520detailed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06113v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IIT%20Bombay%20Racing%20Driverless%3A%20Autonomous%20Driving%20Stack%20for%20Formula%0A%20%20Student%20AI&entry.906535625=Yash%20Rampuria%20and%20Deep%20Boliya%20and%20Shreyash%20Gupta%20and%20Gopalan%20Iyengar%20and%20Ayush%20Rohilla%20and%20Mohak%20Vyas%20and%20Chaitanya%20Langde%20and%20Mehul%20Vijay%20Chanda%20and%20Ronak%20Gautam%20Matai%20and%20Kothapalli%20Namitha%20and%20Ajinkya%20Pawar%20and%20Bhaskar%20Biswas%20and%20Nakul%20Agarwal%20and%20Rajit%20Khandelwal%20and%20Rohan%20Kumar%20and%20Shubham%20Agarwal%20and%20Vishwam%20Patel%20and%20Abhimanyu%20Singh%20Rathore%20and%20Amna%20Rahman%20and%20Ayush%20Mishra%20and%20Yash%20Tangri&entry.1292438233=%20%20This%20work%20presents%20the%20design%20and%20development%20of%20IIT%20Bombay%20Racing%27s%20Formula%0AStudent%20style%20autonomous%20racecar%20algorithm%20capable%20of%20running%20at%20the%20racing%0Aevents%20of%20Formula%20Student-AI%2C%20held%20in%20the%20UK.%20The%20car%20employs%20a%20cutting-edge%0Asensor%20suite%20of%20the%20compute%20unit%20NVIDIA%20Jetson%20Orin%20AGX%2C%202%20ZED2i%20stereo%0Acameras%2C%201%20Velodyne%20Puck%20VLP16%20LiDAR%20and%20SBG%20Systems%20Ellipse%20N%20GNSS/INS%20IMU.%20It%0Afeatures%20deep%20learning%20algorithms%20and%20control%20systems%20to%20navigate%20complex%0Atracks%20and%20execute%20maneuvers%20without%20any%20human%20intervention.%20The%20design%20process%0Ainvolved%20extensive%20simulations%20and%20testing%20to%20optimize%20the%20vehicle%27s%0Aperformance%20and%20ensure%20its%20safety.%20The%20algorithms%20have%20been%20tested%20on%20a%20small%0Ascale%2C%20in-house%20manufactured%204-wheeled%20robot%20and%20on%20simulation%20software.%20The%0Aresults%20obtained%20for%20testing%20various%20algorithms%20in%20perception%2C%20simultaneous%0Alocalization%20and%20mapping%2C%20path%20planning%20and%20controls%20have%20been%20detailed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06113v1&entry.124074799=Read"},
{"title": "FruitNeRF: A Unified Neural Radiance Field based Fruit Counting\n  Framework", "author": "Lukas Meyer and Andreas Gilson and Ute Schmidt and Marc Stamminger", "abstract": "  We introduce FruitNeRF, a unified novel fruit counting framework that\nleverages state-of-the-art view synthesis methods to count any fruit type\ndirectly in 3D. Our framework takes an unordered set of posed images captured\nby a monocular camera and segments fruit in each image. To make our system\nindependent of the fruit type, we employ a foundation model that generates\nbinary segmentation masks for any fruit. Utilizing both modalities, RGB and\nsemantic, we train a semantic neural radiance field. Through uniform volume\nsampling of the implicit Fruit Field, we obtain fruit-only point clouds. By\napplying cascaded clustering on the extracted point cloud, our approach\nachieves precise fruit count.The use of neural radiance fields provides\nsignificant advantages over conventional methods such as object tracking or\noptical flow, as the counting itself is lifted into 3D. Our method prevents\ndouble counting fruit and avoids counting irrelevant fruit.We evaluate our\nmethodology using both real-world and synthetic datasets. The real-world\ndataset consists of three apple trees with manually counted ground truths, a\nbenchmark apple dataset with one row and ground truth fruit location, while the\nsynthetic dataset comprises various fruit types including apple, plum, lemon,\npear, peach, and mango.Additionally, we assess the performance of fruit\ncounting using the foundation model compared to a U-Net.\n", "link": "http://arxiv.org/abs/2408.06190v1", "date": "2024-08-12", "relevancy": 2.0002, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5048}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4983}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4924}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FruitNeRF%3A%20A%20Unified%20Neural%20Radiance%20Field%20based%20Fruit%20Counting%0A%20%20Framework&body=Title%3A%20FruitNeRF%3A%20A%20Unified%20Neural%20Radiance%20Field%20based%20Fruit%20Counting%0A%20%20Framework%0AAuthor%3A%20Lukas%20Meyer%20and%20Andreas%20Gilson%20and%20Ute%20Schmidt%20and%20Marc%20Stamminger%0AAbstract%3A%20%20%20We%20introduce%20FruitNeRF%2C%20a%20unified%20novel%20fruit%20counting%20framework%20that%0Aleverages%20state-of-the-art%20view%20synthesis%20methods%20to%20count%20any%20fruit%20type%0Adirectly%20in%203D.%20Our%20framework%20takes%20an%20unordered%20set%20of%20posed%20images%20captured%0Aby%20a%20monocular%20camera%20and%20segments%20fruit%20in%20each%20image.%20To%20make%20our%20system%0Aindependent%20of%20the%20fruit%20type%2C%20we%20employ%20a%20foundation%20model%20that%20generates%0Abinary%20segmentation%20masks%20for%20any%20fruit.%20Utilizing%20both%20modalities%2C%20RGB%20and%0Asemantic%2C%20we%20train%20a%20semantic%20neural%20radiance%20field.%20Through%20uniform%20volume%0Asampling%20of%20the%20implicit%20Fruit%20Field%2C%20we%20obtain%20fruit-only%20point%20clouds.%20By%0Aapplying%20cascaded%20clustering%20on%20the%20extracted%20point%20cloud%2C%20our%20approach%0Aachieves%20precise%20fruit%20count.The%20use%20of%20neural%20radiance%20fields%20provides%0Asignificant%20advantages%20over%20conventional%20methods%20such%20as%20object%20tracking%20or%0Aoptical%20flow%2C%20as%20the%20counting%20itself%20is%20lifted%20into%203D.%20Our%20method%20prevents%0Adouble%20counting%20fruit%20and%20avoids%20counting%20irrelevant%20fruit.We%20evaluate%20our%0Amethodology%20using%20both%20real-world%20and%20synthetic%20datasets.%20The%20real-world%0Adataset%20consists%20of%20three%20apple%20trees%20with%20manually%20counted%20ground%20truths%2C%20a%0Abenchmark%20apple%20dataset%20with%20one%20row%20and%20ground%20truth%20fruit%20location%2C%20while%20the%0Asynthetic%20dataset%20comprises%20various%20fruit%20types%20including%20apple%2C%20plum%2C%20lemon%2C%0Apear%2C%20peach%2C%20and%20mango.Additionally%2C%20we%20assess%20the%20performance%20of%20fruit%0Acounting%20using%20the%20foundation%20model%20compared%20to%20a%20U-Net.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06190v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFruitNeRF%253A%2520A%2520Unified%2520Neural%2520Radiance%2520Field%2520based%2520Fruit%2520Counting%250A%2520%2520Framework%26entry.906535625%3DLukas%2520Meyer%2520and%2520Andreas%2520Gilson%2520and%2520Ute%2520Schmidt%2520and%2520Marc%2520Stamminger%26entry.1292438233%3D%2520%2520We%2520introduce%2520FruitNeRF%252C%2520a%2520unified%2520novel%2520fruit%2520counting%2520framework%2520that%250Aleverages%2520state-of-the-art%2520view%2520synthesis%2520methods%2520to%2520count%2520any%2520fruit%2520type%250Adirectly%2520in%25203D.%2520Our%2520framework%2520takes%2520an%2520unordered%2520set%2520of%2520posed%2520images%2520captured%250Aby%2520a%2520monocular%2520camera%2520and%2520segments%2520fruit%2520in%2520each%2520image.%2520To%2520make%2520our%2520system%250Aindependent%2520of%2520the%2520fruit%2520type%252C%2520we%2520employ%2520a%2520foundation%2520model%2520that%2520generates%250Abinary%2520segmentation%2520masks%2520for%2520any%2520fruit.%2520Utilizing%2520both%2520modalities%252C%2520RGB%2520and%250Asemantic%252C%2520we%2520train%2520a%2520semantic%2520neural%2520radiance%2520field.%2520Through%2520uniform%2520volume%250Asampling%2520of%2520the%2520implicit%2520Fruit%2520Field%252C%2520we%2520obtain%2520fruit-only%2520point%2520clouds.%2520By%250Aapplying%2520cascaded%2520clustering%2520on%2520the%2520extracted%2520point%2520cloud%252C%2520our%2520approach%250Aachieves%2520precise%2520fruit%2520count.The%2520use%2520of%2520neural%2520radiance%2520fields%2520provides%250Asignificant%2520advantages%2520over%2520conventional%2520methods%2520such%2520as%2520object%2520tracking%2520or%250Aoptical%2520flow%252C%2520as%2520the%2520counting%2520itself%2520is%2520lifted%2520into%25203D.%2520Our%2520method%2520prevents%250Adouble%2520counting%2520fruit%2520and%2520avoids%2520counting%2520irrelevant%2520fruit.We%2520evaluate%2520our%250Amethodology%2520using%2520both%2520real-world%2520and%2520synthetic%2520datasets.%2520The%2520real-world%250Adataset%2520consists%2520of%2520three%2520apple%2520trees%2520with%2520manually%2520counted%2520ground%2520truths%252C%2520a%250Abenchmark%2520apple%2520dataset%2520with%2520one%2520row%2520and%2520ground%2520truth%2520fruit%2520location%252C%2520while%2520the%250Asynthetic%2520dataset%2520comprises%2520various%2520fruit%2520types%2520including%2520apple%252C%2520plum%252C%2520lemon%252C%250Apear%252C%2520peach%252C%2520and%2520mango.Additionally%252C%2520we%2520assess%2520the%2520performance%2520of%2520fruit%250Acounting%2520using%2520the%2520foundation%2520model%2520compared%2520to%2520a%2520U-Net.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06190v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FruitNeRF%3A%20A%20Unified%20Neural%20Radiance%20Field%20based%20Fruit%20Counting%0A%20%20Framework&entry.906535625=Lukas%20Meyer%20and%20Andreas%20Gilson%20and%20Ute%20Schmidt%20and%20Marc%20Stamminger&entry.1292438233=%20%20We%20introduce%20FruitNeRF%2C%20a%20unified%20novel%20fruit%20counting%20framework%20that%0Aleverages%20state-of-the-art%20view%20synthesis%20methods%20to%20count%20any%20fruit%20type%0Adirectly%20in%203D.%20Our%20framework%20takes%20an%20unordered%20set%20of%20posed%20images%20captured%0Aby%20a%20monocular%20camera%20and%20segments%20fruit%20in%20each%20image.%20To%20make%20our%20system%0Aindependent%20of%20the%20fruit%20type%2C%20we%20employ%20a%20foundation%20model%20that%20generates%0Abinary%20segmentation%20masks%20for%20any%20fruit.%20Utilizing%20both%20modalities%2C%20RGB%20and%0Asemantic%2C%20we%20train%20a%20semantic%20neural%20radiance%20field.%20Through%20uniform%20volume%0Asampling%20of%20the%20implicit%20Fruit%20Field%2C%20we%20obtain%20fruit-only%20point%20clouds.%20By%0Aapplying%20cascaded%20clustering%20on%20the%20extracted%20point%20cloud%2C%20our%20approach%0Aachieves%20precise%20fruit%20count.The%20use%20of%20neural%20radiance%20fields%20provides%0Asignificant%20advantages%20over%20conventional%20methods%20such%20as%20object%20tracking%20or%0Aoptical%20flow%2C%20as%20the%20counting%20itself%20is%20lifted%20into%203D.%20Our%20method%20prevents%0Adouble%20counting%20fruit%20and%20avoids%20counting%20irrelevant%20fruit.We%20evaluate%20our%0Amethodology%20using%20both%20real-world%20and%20synthetic%20datasets.%20The%20real-world%0Adataset%20consists%20of%20three%20apple%20trees%20with%20manually%20counted%20ground%20truths%2C%20a%0Abenchmark%20apple%20dataset%20with%20one%20row%20and%20ground%20truth%20fruit%20location%2C%20while%20the%0Asynthetic%20dataset%20comprises%20various%20fruit%20types%20including%20apple%2C%20plum%2C%20lemon%2C%0Apear%2C%20peach%2C%20and%20mango.Additionally%2C%20we%20assess%20the%20performance%20of%20fruit%0Acounting%20using%20the%20foundation%20model%20compared%20to%20a%20U-Net.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06190v1&entry.124074799=Read"},
{"title": "Discover-then-Name: Task-Agnostic Concept Bottlenecks via Automated\n  Concept Discovery", "author": "Sukrut Rao and Sweta Mahajan and Moritz B\u00f6hle and Bernt Schiele", "abstract": "  Concept Bottleneck Models (CBMs) have recently been proposed to address the\n'black-box' problem of deep neural networks, by first mapping images to a\nhuman-understandable concept space and then linearly combining concepts for\nclassification. Such models typically require first coming up with a set of\nconcepts relevant to the task and then aligning the representations of a\nfeature extractor to map to these concepts. However, even with powerful\nfoundational feature extractors like CLIP, there are no guarantees that the\nspecified concepts are detectable. In this work, we leverage recent advances in\nmechanistic interpretability and propose a novel CBM approach -- called\nDiscover-then-Name-CBM (DN-CBM) -- that inverts the typical paradigm: instead\nof pre-selecting concepts based on the downstream classification task, we use\nsparse autoencoders to first discover concepts learnt by the model, and then\nname them and train linear probes for classification. Our concept extraction\nstrategy is efficient, since it is agnostic to the downstream task, and uses\nconcepts already known to the model. We perform a comprehensive evaluation\nacross multiple datasets and CLIP architectures and show that our method yields\nsemantically meaningful concepts, assigns appropriate names to them that make\nthem easy to interpret, and yields performant and interpretable CBMs. Code\navailable at https://github.com/neuroexplicit-saar/discover-then-name.\n", "link": "http://arxiv.org/abs/2407.14499v2", "date": "2024-08-12", "relevancy": 1.9957, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5082}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4941}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4916}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discover-then-Name%3A%20Task-Agnostic%20Concept%20Bottlenecks%20via%20Automated%0A%20%20Concept%20Discovery&body=Title%3A%20Discover-then-Name%3A%20Task-Agnostic%20Concept%20Bottlenecks%20via%20Automated%0A%20%20Concept%20Discovery%0AAuthor%3A%20Sukrut%20Rao%20and%20Sweta%20Mahajan%20and%20Moritz%20B%C3%B6hle%20and%20Bernt%20Schiele%0AAbstract%3A%20%20%20Concept%20Bottleneck%20Models%20%28CBMs%29%20have%20recently%20been%20proposed%20to%20address%20the%0A%27black-box%27%20problem%20of%20deep%20neural%20networks%2C%20by%20first%20mapping%20images%20to%20a%0Ahuman-understandable%20concept%20space%20and%20then%20linearly%20combining%20concepts%20for%0Aclassification.%20Such%20models%20typically%20require%20first%20coming%20up%20with%20a%20set%20of%0Aconcepts%20relevant%20to%20the%20task%20and%20then%20aligning%20the%20representations%20of%20a%0Afeature%20extractor%20to%20map%20to%20these%20concepts.%20However%2C%20even%20with%20powerful%0Afoundational%20feature%20extractors%20like%20CLIP%2C%20there%20are%20no%20guarantees%20that%20the%0Aspecified%20concepts%20are%20detectable.%20In%20this%20work%2C%20we%20leverage%20recent%20advances%20in%0Amechanistic%20interpretability%20and%20propose%20a%20novel%20CBM%20approach%20--%20called%0ADiscover-then-Name-CBM%20%28DN-CBM%29%20--%20that%20inverts%20the%20typical%20paradigm%3A%20instead%0Aof%20pre-selecting%20concepts%20based%20on%20the%20downstream%20classification%20task%2C%20we%20use%0Asparse%20autoencoders%20to%20first%20discover%20concepts%20learnt%20by%20the%20model%2C%20and%20then%0Aname%20them%20and%20train%20linear%20probes%20for%20classification.%20Our%20concept%20extraction%0Astrategy%20is%20efficient%2C%20since%20it%20is%20agnostic%20to%20the%20downstream%20task%2C%20and%20uses%0Aconcepts%20already%20known%20to%20the%20model.%20We%20perform%20a%20comprehensive%20evaluation%0Aacross%20multiple%20datasets%20and%20CLIP%20architectures%20and%20show%20that%20our%20method%20yields%0Asemantically%20meaningful%20concepts%2C%20assigns%20appropriate%20names%20to%20them%20that%20make%0Athem%20easy%20to%20interpret%2C%20and%20yields%20performant%20and%20interpretable%20CBMs.%20Code%0Aavailable%20at%20https%3A//github.com/neuroexplicit-saar/discover-then-name.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14499v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscover-then-Name%253A%2520Task-Agnostic%2520Concept%2520Bottlenecks%2520via%2520Automated%250A%2520%2520Concept%2520Discovery%26entry.906535625%3DSukrut%2520Rao%2520and%2520Sweta%2520Mahajan%2520and%2520Moritz%2520B%25C3%25B6hle%2520and%2520Bernt%2520Schiele%26entry.1292438233%3D%2520%2520Concept%2520Bottleneck%2520Models%2520%2528CBMs%2529%2520have%2520recently%2520been%2520proposed%2520to%2520address%2520the%250A%2527black-box%2527%2520problem%2520of%2520deep%2520neural%2520networks%252C%2520by%2520first%2520mapping%2520images%2520to%2520a%250Ahuman-understandable%2520concept%2520space%2520and%2520then%2520linearly%2520combining%2520concepts%2520for%250Aclassification.%2520Such%2520models%2520typically%2520require%2520first%2520coming%2520up%2520with%2520a%2520set%2520of%250Aconcepts%2520relevant%2520to%2520the%2520task%2520and%2520then%2520aligning%2520the%2520representations%2520of%2520a%250Afeature%2520extractor%2520to%2520map%2520to%2520these%2520concepts.%2520However%252C%2520even%2520with%2520powerful%250Afoundational%2520feature%2520extractors%2520like%2520CLIP%252C%2520there%2520are%2520no%2520guarantees%2520that%2520the%250Aspecified%2520concepts%2520are%2520detectable.%2520In%2520this%2520work%252C%2520we%2520leverage%2520recent%2520advances%2520in%250Amechanistic%2520interpretability%2520and%2520propose%2520a%2520novel%2520CBM%2520approach%2520--%2520called%250ADiscover-then-Name-CBM%2520%2528DN-CBM%2529%2520--%2520that%2520inverts%2520the%2520typical%2520paradigm%253A%2520instead%250Aof%2520pre-selecting%2520concepts%2520based%2520on%2520the%2520downstream%2520classification%2520task%252C%2520we%2520use%250Asparse%2520autoencoders%2520to%2520first%2520discover%2520concepts%2520learnt%2520by%2520the%2520model%252C%2520and%2520then%250Aname%2520them%2520and%2520train%2520linear%2520probes%2520for%2520classification.%2520Our%2520concept%2520extraction%250Astrategy%2520is%2520efficient%252C%2520since%2520it%2520is%2520agnostic%2520to%2520the%2520downstream%2520task%252C%2520and%2520uses%250Aconcepts%2520already%2520known%2520to%2520the%2520model.%2520We%2520perform%2520a%2520comprehensive%2520evaluation%250Aacross%2520multiple%2520datasets%2520and%2520CLIP%2520architectures%2520and%2520show%2520that%2520our%2520method%2520yields%250Asemantically%2520meaningful%2520concepts%252C%2520assigns%2520appropriate%2520names%2520to%2520them%2520that%2520make%250Athem%2520easy%2520to%2520interpret%252C%2520and%2520yields%2520performant%2520and%2520interpretable%2520CBMs.%2520Code%250Aavailable%2520at%2520https%253A//github.com/neuroexplicit-saar/discover-then-name.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14499v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discover-then-Name%3A%20Task-Agnostic%20Concept%20Bottlenecks%20via%20Automated%0A%20%20Concept%20Discovery&entry.906535625=Sukrut%20Rao%20and%20Sweta%20Mahajan%20and%20Moritz%20B%C3%B6hle%20and%20Bernt%20Schiele&entry.1292438233=%20%20Concept%20Bottleneck%20Models%20%28CBMs%29%20have%20recently%20been%20proposed%20to%20address%20the%0A%27black-box%27%20problem%20of%20deep%20neural%20networks%2C%20by%20first%20mapping%20images%20to%20a%0Ahuman-understandable%20concept%20space%20and%20then%20linearly%20combining%20concepts%20for%0Aclassification.%20Such%20models%20typically%20require%20first%20coming%20up%20with%20a%20set%20of%0Aconcepts%20relevant%20to%20the%20task%20and%20then%20aligning%20the%20representations%20of%20a%0Afeature%20extractor%20to%20map%20to%20these%20concepts.%20However%2C%20even%20with%20powerful%0Afoundational%20feature%20extractors%20like%20CLIP%2C%20there%20are%20no%20guarantees%20that%20the%0Aspecified%20concepts%20are%20detectable.%20In%20this%20work%2C%20we%20leverage%20recent%20advances%20in%0Amechanistic%20interpretability%20and%20propose%20a%20novel%20CBM%20approach%20--%20called%0ADiscover-then-Name-CBM%20%28DN-CBM%29%20--%20that%20inverts%20the%20typical%20paradigm%3A%20instead%0Aof%20pre-selecting%20concepts%20based%20on%20the%20downstream%20classification%20task%2C%20we%20use%0Asparse%20autoencoders%20to%20first%20discover%20concepts%20learnt%20by%20the%20model%2C%20and%20then%0Aname%20them%20and%20train%20linear%20probes%20for%20classification.%20Our%20concept%20extraction%0Astrategy%20is%20efficient%2C%20since%20it%20is%20agnostic%20to%20the%20downstream%20task%2C%20and%20uses%0Aconcepts%20already%20known%20to%20the%20model.%20We%20perform%20a%20comprehensive%20evaluation%0Aacross%20multiple%20datasets%20and%20CLIP%20architectures%20and%20show%20that%20our%20method%20yields%0Asemantically%20meaningful%20concepts%2C%20assigns%20appropriate%20names%20to%20them%20that%20make%0Athem%20easy%20to%20interpret%2C%20and%20yields%20performant%20and%20interpretable%20CBMs.%20Code%0Aavailable%20at%20https%3A//github.com/neuroexplicit-saar/discover-then-name.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14499v2&entry.124074799=Read"},
{"title": "Moo-ving Beyond Tradition: Revolutionizing Cattle Behavioural\n  Phenotyping with Pose Estimation Techniques", "author": "Navid Ghassemi and Ali Goldani and Ian Q. Whishaw and Majid H. Mohajerani", "abstract": "  The cattle industry has been a major contributor to the economy of many\ncountries, including the US and Canada. The integration of Artificial\nIntelligence (AI) has revolutionized this sector, mirroring its transformative\nimpact across all industries by enabling scalable and automated monitoring and\nintervention practices. AI has also introduced tools and methods that automate\nmany tasks previously performed by human labor with the help of computer\nvision, including health inspections. Among these methods, pose estimation has\na special place; pose estimation is the process of finding the position of\njoints in an image of animals. Analyzing the pose of animal subjects enables\nprecise identification and tracking of the animal's movement and the movements\nof its body parts. By summarizing the video and imagery data into movement and\njoint location using pose estimation and then analyzing this information, we\ncan address the scalability challenge in cattle management, focusing on health\nmonitoring, behavioural phenotyping and welfare concerns. Our study reviews\nrecent advancements in pose estimation methodologies, their applicability in\nimproving the cattle industry, existing challenges, and gaps in this field.\nFurthermore, we propose an initiative to enhance open science frameworks within\nthis field of study by launching a platform designed to connect industry and\nacademia.\n", "link": "http://arxiv.org/abs/2408.06336v1", "date": "2024-08-12", "relevancy": 1.9929, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5296}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4847}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4723}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Moo-ving%20Beyond%20Tradition%3A%20Revolutionizing%20Cattle%20Behavioural%0A%20%20Phenotyping%20with%20Pose%20Estimation%20Techniques&body=Title%3A%20Moo-ving%20Beyond%20Tradition%3A%20Revolutionizing%20Cattle%20Behavioural%0A%20%20Phenotyping%20with%20Pose%20Estimation%20Techniques%0AAuthor%3A%20Navid%20Ghassemi%20and%20Ali%20Goldani%20and%20Ian%20Q.%20Whishaw%20and%20Majid%20H.%20Mohajerani%0AAbstract%3A%20%20%20The%20cattle%20industry%20has%20been%20a%20major%20contributor%20to%20the%20economy%20of%20many%0Acountries%2C%20including%20the%20US%20and%20Canada.%20The%20integration%20of%20Artificial%0AIntelligence%20%28AI%29%20has%20revolutionized%20this%20sector%2C%20mirroring%20its%20transformative%0Aimpact%20across%20all%20industries%20by%20enabling%20scalable%20and%20automated%20monitoring%20and%0Aintervention%20practices.%20AI%20has%20also%20introduced%20tools%20and%20methods%20that%20automate%0Amany%20tasks%20previously%20performed%20by%20human%20labor%20with%20the%20help%20of%20computer%0Avision%2C%20including%20health%20inspections.%20Among%20these%20methods%2C%20pose%20estimation%20has%0Aa%20special%20place%3B%20pose%20estimation%20is%20the%20process%20of%20finding%20the%20position%20of%0Ajoints%20in%20an%20image%20of%20animals.%20Analyzing%20the%20pose%20of%20animal%20subjects%20enables%0Aprecise%20identification%20and%20tracking%20of%20the%20animal%27s%20movement%20and%20the%20movements%0Aof%20its%20body%20parts.%20By%20summarizing%20the%20video%20and%20imagery%20data%20into%20movement%20and%0Ajoint%20location%20using%20pose%20estimation%20and%20then%20analyzing%20this%20information%2C%20we%0Acan%20address%20the%20scalability%20challenge%20in%20cattle%20management%2C%20focusing%20on%20health%0Amonitoring%2C%20behavioural%20phenotyping%20and%20welfare%20concerns.%20Our%20study%20reviews%0Arecent%20advancements%20in%20pose%20estimation%20methodologies%2C%20their%20applicability%20in%0Aimproving%20the%20cattle%20industry%2C%20existing%20challenges%2C%20and%20gaps%20in%20this%20field.%0AFurthermore%2C%20we%20propose%20an%20initiative%20to%20enhance%20open%20science%20frameworks%20within%0Athis%20field%20of%20study%20by%20launching%20a%20platform%20designed%20to%20connect%20industry%20and%0Aacademia.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06336v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoo-ving%2520Beyond%2520Tradition%253A%2520Revolutionizing%2520Cattle%2520Behavioural%250A%2520%2520Phenotyping%2520with%2520Pose%2520Estimation%2520Techniques%26entry.906535625%3DNavid%2520Ghassemi%2520and%2520Ali%2520Goldani%2520and%2520Ian%2520Q.%2520Whishaw%2520and%2520Majid%2520H.%2520Mohajerani%26entry.1292438233%3D%2520%2520The%2520cattle%2520industry%2520has%2520been%2520a%2520major%2520contributor%2520to%2520the%2520economy%2520of%2520many%250Acountries%252C%2520including%2520the%2520US%2520and%2520Canada.%2520The%2520integration%2520of%2520Artificial%250AIntelligence%2520%2528AI%2529%2520has%2520revolutionized%2520this%2520sector%252C%2520mirroring%2520its%2520transformative%250Aimpact%2520across%2520all%2520industries%2520by%2520enabling%2520scalable%2520and%2520automated%2520monitoring%2520and%250Aintervention%2520practices.%2520AI%2520has%2520also%2520introduced%2520tools%2520and%2520methods%2520that%2520automate%250Amany%2520tasks%2520previously%2520performed%2520by%2520human%2520labor%2520with%2520the%2520help%2520of%2520computer%250Avision%252C%2520including%2520health%2520inspections.%2520Among%2520these%2520methods%252C%2520pose%2520estimation%2520has%250Aa%2520special%2520place%253B%2520pose%2520estimation%2520is%2520the%2520process%2520of%2520finding%2520the%2520position%2520of%250Ajoints%2520in%2520an%2520image%2520of%2520animals.%2520Analyzing%2520the%2520pose%2520of%2520animal%2520subjects%2520enables%250Aprecise%2520identification%2520and%2520tracking%2520of%2520the%2520animal%2527s%2520movement%2520and%2520the%2520movements%250Aof%2520its%2520body%2520parts.%2520By%2520summarizing%2520the%2520video%2520and%2520imagery%2520data%2520into%2520movement%2520and%250Ajoint%2520location%2520using%2520pose%2520estimation%2520and%2520then%2520analyzing%2520this%2520information%252C%2520we%250Acan%2520address%2520the%2520scalability%2520challenge%2520in%2520cattle%2520management%252C%2520focusing%2520on%2520health%250Amonitoring%252C%2520behavioural%2520phenotyping%2520and%2520welfare%2520concerns.%2520Our%2520study%2520reviews%250Arecent%2520advancements%2520in%2520pose%2520estimation%2520methodologies%252C%2520their%2520applicability%2520in%250Aimproving%2520the%2520cattle%2520industry%252C%2520existing%2520challenges%252C%2520and%2520gaps%2520in%2520this%2520field.%250AFurthermore%252C%2520we%2520propose%2520an%2520initiative%2520to%2520enhance%2520open%2520science%2520frameworks%2520within%250Athis%2520field%2520of%2520study%2520by%2520launching%2520a%2520platform%2520designed%2520to%2520connect%2520industry%2520and%250Aacademia.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06336v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Moo-ving%20Beyond%20Tradition%3A%20Revolutionizing%20Cattle%20Behavioural%0A%20%20Phenotyping%20with%20Pose%20Estimation%20Techniques&entry.906535625=Navid%20Ghassemi%20and%20Ali%20Goldani%20and%20Ian%20Q.%20Whishaw%20and%20Majid%20H.%20Mohajerani&entry.1292438233=%20%20The%20cattle%20industry%20has%20been%20a%20major%20contributor%20to%20the%20economy%20of%20many%0Acountries%2C%20including%20the%20US%20and%20Canada.%20The%20integration%20of%20Artificial%0AIntelligence%20%28AI%29%20has%20revolutionized%20this%20sector%2C%20mirroring%20its%20transformative%0Aimpact%20across%20all%20industries%20by%20enabling%20scalable%20and%20automated%20monitoring%20and%0Aintervention%20practices.%20AI%20has%20also%20introduced%20tools%20and%20methods%20that%20automate%0Amany%20tasks%20previously%20performed%20by%20human%20labor%20with%20the%20help%20of%20computer%0Avision%2C%20including%20health%20inspections.%20Among%20these%20methods%2C%20pose%20estimation%20has%0Aa%20special%20place%3B%20pose%20estimation%20is%20the%20process%20of%20finding%20the%20position%20of%0Ajoints%20in%20an%20image%20of%20animals.%20Analyzing%20the%20pose%20of%20animal%20subjects%20enables%0Aprecise%20identification%20and%20tracking%20of%20the%20animal%27s%20movement%20and%20the%20movements%0Aof%20its%20body%20parts.%20By%20summarizing%20the%20video%20and%20imagery%20data%20into%20movement%20and%0Ajoint%20location%20using%20pose%20estimation%20and%20then%20analyzing%20this%20information%2C%20we%0Acan%20address%20the%20scalability%20challenge%20in%20cattle%20management%2C%20focusing%20on%20health%0Amonitoring%2C%20behavioural%20phenotyping%20and%20welfare%20concerns.%20Our%20study%20reviews%0Arecent%20advancements%20in%20pose%20estimation%20methodologies%2C%20their%20applicability%20in%0Aimproving%20the%20cattle%20industry%2C%20existing%20challenges%2C%20and%20gaps%20in%20this%20field.%0AFurthermore%2C%20we%20propose%20an%20initiative%20to%20enhance%20open%20science%20frameworks%20within%0Athis%20field%20of%20study%20by%20launching%20a%20platform%20designed%20to%20connect%20industry%20and%0Aacademia.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06336v1&entry.124074799=Read"},
{"title": "KIX: A Knowledge and Interaction-Centric Metacognitive Framework for\n  Task Generalization", "author": "Arun Kumar and Paul Schrater", "abstract": "  People aptly exhibit general intelligence behaviors in solving a variety of\ntasks with flexibility and ability to adapt to novel situations by reusing and\napplying high-level knowledge acquired over time. But artificial agents are\nmore like specialists, lacking such generalist behaviors. Artificial agents\nwill require understanding and exploiting critical structured knowledge\nrepresentations. We present a metacognitive generalization framework,\nKnowledge-Interaction-eXecution (KIX), and argue that interactions with objects\nleveraging type space facilitate the learning of transferable interaction\nconcepts and generalization. It is a natural way of integrating knowledge into\nreinforcement learning and is promising to act as an enabler for autonomous and\ngeneralist behaviors in artificial intelligence systems.\n", "link": "http://arxiv.org/abs/2402.05346v2", "date": "2024-08-12", "relevancy": 1.9757, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5195}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5113}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KIX%3A%20A%20Knowledge%20and%20Interaction-Centric%20Metacognitive%20Framework%20for%0A%20%20Task%20Generalization&body=Title%3A%20KIX%3A%20A%20Knowledge%20and%20Interaction-Centric%20Metacognitive%20Framework%20for%0A%20%20Task%20Generalization%0AAuthor%3A%20Arun%20Kumar%20and%20Paul%20Schrater%0AAbstract%3A%20%20%20People%20aptly%20exhibit%20general%20intelligence%20behaviors%20in%20solving%20a%20variety%20of%0Atasks%20with%20flexibility%20and%20ability%20to%20adapt%20to%20novel%20situations%20by%20reusing%20and%0Aapplying%20high-level%20knowledge%20acquired%20over%20time.%20But%20artificial%20agents%20are%0Amore%20like%20specialists%2C%20lacking%20such%20generalist%20behaviors.%20Artificial%20agents%0Awill%20require%20understanding%20and%20exploiting%20critical%20structured%20knowledge%0Arepresentations.%20We%20present%20a%20metacognitive%20generalization%20framework%2C%0AKnowledge-Interaction-eXecution%20%28KIX%29%2C%20and%20argue%20that%20interactions%20with%20objects%0Aleveraging%20type%20space%20facilitate%20the%20learning%20of%20transferable%20interaction%0Aconcepts%20and%20generalization.%20It%20is%20a%20natural%20way%20of%20integrating%20knowledge%20into%0Areinforcement%20learning%20and%20is%20promising%20to%20act%20as%20an%20enabler%20for%20autonomous%20and%0Ageneralist%20behaviors%20in%20artificial%20intelligence%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.05346v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKIX%253A%2520A%2520Knowledge%2520and%2520Interaction-Centric%2520Metacognitive%2520Framework%2520for%250A%2520%2520Task%2520Generalization%26entry.906535625%3DArun%2520Kumar%2520and%2520Paul%2520Schrater%26entry.1292438233%3D%2520%2520People%2520aptly%2520exhibit%2520general%2520intelligence%2520behaviors%2520in%2520solving%2520a%2520variety%2520of%250Atasks%2520with%2520flexibility%2520and%2520ability%2520to%2520adapt%2520to%2520novel%2520situations%2520by%2520reusing%2520and%250Aapplying%2520high-level%2520knowledge%2520acquired%2520over%2520time.%2520But%2520artificial%2520agents%2520are%250Amore%2520like%2520specialists%252C%2520lacking%2520such%2520generalist%2520behaviors.%2520Artificial%2520agents%250Awill%2520require%2520understanding%2520and%2520exploiting%2520critical%2520structured%2520knowledge%250Arepresentations.%2520We%2520present%2520a%2520metacognitive%2520generalization%2520framework%252C%250AKnowledge-Interaction-eXecution%2520%2528KIX%2529%252C%2520and%2520argue%2520that%2520interactions%2520with%2520objects%250Aleveraging%2520type%2520space%2520facilitate%2520the%2520learning%2520of%2520transferable%2520interaction%250Aconcepts%2520and%2520generalization.%2520It%2520is%2520a%2520natural%2520way%2520of%2520integrating%2520knowledge%2520into%250Areinforcement%2520learning%2520and%2520is%2520promising%2520to%2520act%2520as%2520an%2520enabler%2520for%2520autonomous%2520and%250Ageneralist%2520behaviors%2520in%2520artificial%2520intelligence%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.05346v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KIX%3A%20A%20Knowledge%20and%20Interaction-Centric%20Metacognitive%20Framework%20for%0A%20%20Task%20Generalization&entry.906535625=Arun%20Kumar%20and%20Paul%20Schrater&entry.1292438233=%20%20People%20aptly%20exhibit%20general%20intelligence%20behaviors%20in%20solving%20a%20variety%20of%0Atasks%20with%20flexibility%20and%20ability%20to%20adapt%20to%20novel%20situations%20by%20reusing%20and%0Aapplying%20high-level%20knowledge%20acquired%20over%20time.%20But%20artificial%20agents%20are%0Amore%20like%20specialists%2C%20lacking%20such%20generalist%20behaviors.%20Artificial%20agents%0Awill%20require%20understanding%20and%20exploiting%20critical%20structured%20knowledge%0Arepresentations.%20We%20present%20a%20metacognitive%20generalization%20framework%2C%0AKnowledge-Interaction-eXecution%20%28KIX%29%2C%20and%20argue%20that%20interactions%20with%20objects%0Aleveraging%20type%20space%20facilitate%20the%20learning%20of%20transferable%20interaction%0Aconcepts%20and%20generalization.%20It%20is%20a%20natural%20way%20of%20integrating%20knowledge%20into%0Areinforcement%20learning%20and%20is%20promising%20to%20act%20as%20an%20enabler%20for%20autonomous%20and%0Ageneralist%20behaviors%20in%20artificial%20intelligence%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.05346v2&entry.124074799=Read"},
{"title": "TruVRF: Towards Triple-Granularity Verification on Machine Unlearning", "author": "Chunyi Zhou and Anmin Fu and Zhiyang Dai", "abstract": "  The concept of the right to be forgotten has led to growing interest in\nmachine unlearning, but reliable validation methods are lacking, creating\nopportunities for dishonest model providers to mislead data contributors.\nTraditional invasive methods like backdoor injection are not feasible for\nlegacy data. To address this, we introduce TruVRF, a non-invasive unlearning\nverification framework operating at class-, volume-, and sample-level\ngranularities. TruVRF includes three Unlearning-Metrics designed to detect\ndifferent types of dishonest servers: Neglecting, Lazy, and Deceiving.\nUnlearning-Metric-I checks class alignment, Unlearning-Metric-II verifies\nsample count, and Unlearning-Metric-III confirms specific sample deletion.\nEvaluations on three datasets show TruVRF's robust performance, with over 90%\naccuracy for Metrics I and III, and a 4.8% to 8.2% inference deviation for\nMetric II. TruVRF also demonstrates generalizability and practicality across\nvarious conditions and with state-of-the-art unlearning frameworks like SISA\nand Amnesiac Unlearning.\n", "link": "http://arxiv.org/abs/2408.06063v1", "date": "2024-08-12", "relevancy": 1.9755, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5452}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4997}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4676}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TruVRF%3A%20Towards%20Triple-Granularity%20Verification%20on%20Machine%20Unlearning&body=Title%3A%20TruVRF%3A%20Towards%20Triple-Granularity%20Verification%20on%20Machine%20Unlearning%0AAuthor%3A%20Chunyi%20Zhou%20and%20Anmin%20Fu%20and%20Zhiyang%20Dai%0AAbstract%3A%20%20%20The%20concept%20of%20the%20right%20to%20be%20forgotten%20has%20led%20to%20growing%20interest%20in%0Amachine%20unlearning%2C%20but%20reliable%20validation%20methods%20are%20lacking%2C%20creating%0Aopportunities%20for%20dishonest%20model%20providers%20to%20mislead%20data%20contributors.%0ATraditional%20invasive%20methods%20like%20backdoor%20injection%20are%20not%20feasible%20for%0Alegacy%20data.%20To%20address%20this%2C%20we%20introduce%20TruVRF%2C%20a%20non-invasive%20unlearning%0Averification%20framework%20operating%20at%20class-%2C%20volume-%2C%20and%20sample-level%0Agranularities.%20TruVRF%20includes%20three%20Unlearning-Metrics%20designed%20to%20detect%0Adifferent%20types%20of%20dishonest%20servers%3A%20Neglecting%2C%20Lazy%2C%20and%20Deceiving.%0AUnlearning-Metric-I%20checks%20class%20alignment%2C%20Unlearning-Metric-II%20verifies%0Asample%20count%2C%20and%20Unlearning-Metric-III%20confirms%20specific%20sample%20deletion.%0AEvaluations%20on%20three%20datasets%20show%20TruVRF%27s%20robust%20performance%2C%20with%20over%2090%25%0Aaccuracy%20for%20Metrics%20I%20and%20III%2C%20and%20a%204.8%25%20to%208.2%25%20inference%20deviation%20for%0AMetric%20II.%20TruVRF%20also%20demonstrates%20generalizability%20and%20practicality%20across%0Avarious%20conditions%20and%20with%20state-of-the-art%20unlearning%20frameworks%20like%20SISA%0Aand%20Amnesiac%20Unlearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06063v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTruVRF%253A%2520Towards%2520Triple-Granularity%2520Verification%2520on%2520Machine%2520Unlearning%26entry.906535625%3DChunyi%2520Zhou%2520and%2520Anmin%2520Fu%2520and%2520Zhiyang%2520Dai%26entry.1292438233%3D%2520%2520The%2520concept%2520of%2520the%2520right%2520to%2520be%2520forgotten%2520has%2520led%2520to%2520growing%2520interest%2520in%250Amachine%2520unlearning%252C%2520but%2520reliable%2520validation%2520methods%2520are%2520lacking%252C%2520creating%250Aopportunities%2520for%2520dishonest%2520model%2520providers%2520to%2520mislead%2520data%2520contributors.%250ATraditional%2520invasive%2520methods%2520like%2520backdoor%2520injection%2520are%2520not%2520feasible%2520for%250Alegacy%2520data.%2520To%2520address%2520this%252C%2520we%2520introduce%2520TruVRF%252C%2520a%2520non-invasive%2520unlearning%250Averification%2520framework%2520operating%2520at%2520class-%252C%2520volume-%252C%2520and%2520sample-level%250Agranularities.%2520TruVRF%2520includes%2520three%2520Unlearning-Metrics%2520designed%2520to%2520detect%250Adifferent%2520types%2520of%2520dishonest%2520servers%253A%2520Neglecting%252C%2520Lazy%252C%2520and%2520Deceiving.%250AUnlearning-Metric-I%2520checks%2520class%2520alignment%252C%2520Unlearning-Metric-II%2520verifies%250Asample%2520count%252C%2520and%2520Unlearning-Metric-III%2520confirms%2520specific%2520sample%2520deletion.%250AEvaluations%2520on%2520three%2520datasets%2520show%2520TruVRF%2527s%2520robust%2520performance%252C%2520with%2520over%252090%2525%250Aaccuracy%2520for%2520Metrics%2520I%2520and%2520III%252C%2520and%2520a%25204.8%2525%2520to%25208.2%2525%2520inference%2520deviation%2520for%250AMetric%2520II.%2520TruVRF%2520also%2520demonstrates%2520generalizability%2520and%2520practicality%2520across%250Avarious%2520conditions%2520and%2520with%2520state-of-the-art%2520unlearning%2520frameworks%2520like%2520SISA%250Aand%2520Amnesiac%2520Unlearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06063v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TruVRF%3A%20Towards%20Triple-Granularity%20Verification%20on%20Machine%20Unlearning&entry.906535625=Chunyi%20Zhou%20and%20Anmin%20Fu%20and%20Zhiyang%20Dai&entry.1292438233=%20%20The%20concept%20of%20the%20right%20to%20be%20forgotten%20has%20led%20to%20growing%20interest%20in%0Amachine%20unlearning%2C%20but%20reliable%20validation%20methods%20are%20lacking%2C%20creating%0Aopportunities%20for%20dishonest%20model%20providers%20to%20mislead%20data%20contributors.%0ATraditional%20invasive%20methods%20like%20backdoor%20injection%20are%20not%20feasible%20for%0Alegacy%20data.%20To%20address%20this%2C%20we%20introduce%20TruVRF%2C%20a%20non-invasive%20unlearning%0Averification%20framework%20operating%20at%20class-%2C%20volume-%2C%20and%20sample-level%0Agranularities.%20TruVRF%20includes%20three%20Unlearning-Metrics%20designed%20to%20detect%0Adifferent%20types%20of%20dishonest%20servers%3A%20Neglecting%2C%20Lazy%2C%20and%20Deceiving.%0AUnlearning-Metric-I%20checks%20class%20alignment%2C%20Unlearning-Metric-II%20verifies%0Asample%20count%2C%20and%20Unlearning-Metric-III%20confirms%20specific%20sample%20deletion.%0AEvaluations%20on%20three%20datasets%20show%20TruVRF%27s%20robust%20performance%2C%20with%20over%2090%25%0Aaccuracy%20for%20Metrics%20I%20and%20III%2C%20and%20a%204.8%25%20to%208.2%25%20inference%20deviation%20for%0AMetric%20II.%20TruVRF%20also%20demonstrates%20generalizability%20and%20practicality%20across%0Avarious%20conditions%20and%20with%20state-of-the-art%20unlearning%20frameworks%20like%20SISA%0Aand%20Amnesiac%20Unlearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06063v1&entry.124074799=Read"},
{"title": "On Effects of Steering Latent Representation for Large Language Model\n  Unlearning", "author": "Dang Huu-Tien and Trung-Tin Pham and Hoang Thanh-Tung and Naoya Inoue", "abstract": "  Representation Misdirection for Unlearning (RMU), which steers model\nrepresentation in the intermediate layer to a target random representation, is\nan effective method for large language model (LLM) unlearning. Despite its high\nperformance, the underlying cause and explanation remain underexplored. In this\npaper, we first theoretically demonstrate that steering forget representations\nin the intermediate layer reduces token confidence, causing LLMs to generate\nwrong or nonsense responses. Second, we investigate how the coefficient\ninfluences the alignment of forget-sample representations with the random\ndirection and hint at the optimal coefficient values for effective unlearning\nacross different network layers. Third, we show that RMU unlearned models are\nrobust against adversarial jailbreak attacks. Last, our empirical analysis\nshows that RMU is less effective when applied to the middle and later layers in\nLLMs. To resolve this drawback, we propose Adaptive RMU -- a simple yet\neffective alternative method that makes unlearning effective with most layers.\nExtensive experiments demonstrate that Adaptive RMU significantly improves the\nunlearning performance compared to prior art while incurring no additional\ncomputational cost.\n", "link": "http://arxiv.org/abs/2408.06223v1", "date": "2024-08-12", "relevancy": 1.9623, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4988}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4948}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4806}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Effects%20of%20Steering%20Latent%20Representation%20for%20Large%20Language%20Model%0A%20%20Unlearning&body=Title%3A%20On%20Effects%20of%20Steering%20Latent%20Representation%20for%20Large%20Language%20Model%0A%20%20Unlearning%0AAuthor%3A%20Dang%20Huu-Tien%20and%20Trung-Tin%20Pham%20and%20Hoang%20Thanh-Tung%20and%20Naoya%20Inoue%0AAbstract%3A%20%20%20Representation%20Misdirection%20for%20Unlearning%20%28RMU%29%2C%20which%20steers%20model%0Arepresentation%20in%20the%20intermediate%20layer%20to%20a%20target%20random%20representation%2C%20is%0Aan%20effective%20method%20for%20large%20language%20model%20%28LLM%29%20unlearning.%20Despite%20its%20high%0Aperformance%2C%20the%20underlying%20cause%20and%20explanation%20remain%20underexplored.%20In%20this%0Apaper%2C%20we%20first%20theoretically%20demonstrate%20that%20steering%20forget%20representations%0Ain%20the%20intermediate%20layer%20reduces%20token%20confidence%2C%20causing%20LLMs%20to%20generate%0Awrong%20or%20nonsense%20responses.%20Second%2C%20we%20investigate%20how%20the%20coefficient%0Ainfluences%20the%20alignment%20of%20forget-sample%20representations%20with%20the%20random%0Adirection%20and%20hint%20at%20the%20optimal%20coefficient%20values%20for%20effective%20unlearning%0Aacross%20different%20network%20layers.%20Third%2C%20we%20show%20that%20RMU%20unlearned%20models%20are%0Arobust%20against%20adversarial%20jailbreak%20attacks.%20Last%2C%20our%20empirical%20analysis%0Ashows%20that%20RMU%20is%20less%20effective%20when%20applied%20to%20the%20middle%20and%20later%20layers%20in%0ALLMs.%20To%20resolve%20this%20drawback%2C%20we%20propose%20Adaptive%20RMU%20--%20a%20simple%20yet%0Aeffective%20alternative%20method%20that%20makes%20unlearning%20effective%20with%20most%20layers.%0AExtensive%20experiments%20demonstrate%20that%20Adaptive%20RMU%20significantly%20improves%20the%0Aunlearning%20performance%20compared%20to%20prior%20art%20while%20incurring%20no%20additional%0Acomputational%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06223v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Effects%2520of%2520Steering%2520Latent%2520Representation%2520for%2520Large%2520Language%2520Model%250A%2520%2520Unlearning%26entry.906535625%3DDang%2520Huu-Tien%2520and%2520Trung-Tin%2520Pham%2520and%2520Hoang%2520Thanh-Tung%2520and%2520Naoya%2520Inoue%26entry.1292438233%3D%2520%2520Representation%2520Misdirection%2520for%2520Unlearning%2520%2528RMU%2529%252C%2520which%2520steers%2520model%250Arepresentation%2520in%2520the%2520intermediate%2520layer%2520to%2520a%2520target%2520random%2520representation%252C%2520is%250Aan%2520effective%2520method%2520for%2520large%2520language%2520model%2520%2528LLM%2529%2520unlearning.%2520Despite%2520its%2520high%250Aperformance%252C%2520the%2520underlying%2520cause%2520and%2520explanation%2520remain%2520underexplored.%2520In%2520this%250Apaper%252C%2520we%2520first%2520theoretically%2520demonstrate%2520that%2520steering%2520forget%2520representations%250Ain%2520the%2520intermediate%2520layer%2520reduces%2520token%2520confidence%252C%2520causing%2520LLMs%2520to%2520generate%250Awrong%2520or%2520nonsense%2520responses.%2520Second%252C%2520we%2520investigate%2520how%2520the%2520coefficient%250Ainfluences%2520the%2520alignment%2520of%2520forget-sample%2520representations%2520with%2520the%2520random%250Adirection%2520and%2520hint%2520at%2520the%2520optimal%2520coefficient%2520values%2520for%2520effective%2520unlearning%250Aacross%2520different%2520network%2520layers.%2520Third%252C%2520we%2520show%2520that%2520RMU%2520unlearned%2520models%2520are%250Arobust%2520against%2520adversarial%2520jailbreak%2520attacks.%2520Last%252C%2520our%2520empirical%2520analysis%250Ashows%2520that%2520RMU%2520is%2520less%2520effective%2520when%2520applied%2520to%2520the%2520middle%2520and%2520later%2520layers%2520in%250ALLMs.%2520To%2520resolve%2520this%2520drawback%252C%2520we%2520propose%2520Adaptive%2520RMU%2520--%2520a%2520simple%2520yet%250Aeffective%2520alternative%2520method%2520that%2520makes%2520unlearning%2520effective%2520with%2520most%2520layers.%250AExtensive%2520experiments%2520demonstrate%2520that%2520Adaptive%2520RMU%2520significantly%2520improves%2520the%250Aunlearning%2520performance%2520compared%2520to%2520prior%2520art%2520while%2520incurring%2520no%2520additional%250Acomputational%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06223v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Effects%20of%20Steering%20Latent%20Representation%20for%20Large%20Language%20Model%0A%20%20Unlearning&entry.906535625=Dang%20Huu-Tien%20and%20Trung-Tin%20Pham%20and%20Hoang%20Thanh-Tung%20and%20Naoya%20Inoue&entry.1292438233=%20%20Representation%20Misdirection%20for%20Unlearning%20%28RMU%29%2C%20which%20steers%20model%0Arepresentation%20in%20the%20intermediate%20layer%20to%20a%20target%20random%20representation%2C%20is%0Aan%20effective%20method%20for%20large%20language%20model%20%28LLM%29%20unlearning.%20Despite%20its%20high%0Aperformance%2C%20the%20underlying%20cause%20and%20explanation%20remain%20underexplored.%20In%20this%0Apaper%2C%20we%20first%20theoretically%20demonstrate%20that%20steering%20forget%20representations%0Ain%20the%20intermediate%20layer%20reduces%20token%20confidence%2C%20causing%20LLMs%20to%20generate%0Awrong%20or%20nonsense%20responses.%20Second%2C%20we%20investigate%20how%20the%20coefficient%0Ainfluences%20the%20alignment%20of%20forget-sample%20representations%20with%20the%20random%0Adirection%20and%20hint%20at%20the%20optimal%20coefficient%20values%20for%20effective%20unlearning%0Aacross%20different%20network%20layers.%20Third%2C%20we%20show%20that%20RMU%20unlearned%20models%20are%0Arobust%20against%20adversarial%20jailbreak%20attacks.%20Last%2C%20our%20empirical%20analysis%0Ashows%20that%20RMU%20is%20less%20effective%20when%20applied%20to%20the%20middle%20and%20later%20layers%20in%0ALLMs.%20To%20resolve%20this%20drawback%2C%20we%20propose%20Adaptive%20RMU%20--%20a%20simple%20yet%0Aeffective%20alternative%20method%20that%20makes%20unlearning%20effective%20with%20most%20layers.%0AExtensive%20experiments%20demonstrate%20that%20Adaptive%20RMU%20significantly%20improves%20the%0Aunlearning%20performance%20compared%20to%20prior%20art%20while%20incurring%20no%20additional%0Acomputational%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06223v1&entry.124074799=Read"},
{"title": "Don't You (Project Around Discs)? Neural Network Surrogate and Projected\n  Gradient Descent for Calibrating an Intervertebral Disc Finite Element Model", "author": "Matan Atad and Gabriel Gruber and Marx Ribeiro and Luis Fernando Nicolini and Robert Graf and Hendrik M\u00f6ller and Kati Nispel and Ivan Ezhov and Daniel Rueckert and Jan S. Kirschke", "abstract": "  Accurate calibration of finite element (FE) models of human intervertebral\ndiscs (IVDs) is essential for their reliability and application in diagnosing\nand planning treatments for spinal conditions. Traditional calibration methods\nare computationally intensive, requiring iterative, derivative-free\noptimization algorithms that often take hours or days to converge.\n  This study addresses these challenges by introducing a novel, efficient, and\neffective calibration method for an L4-L5 IVD FE model using a neural network\n(NN) surrogate. The NN surrogate predicts simulation outcomes with high\naccuracy, outperforming other machine learning models, and significantly\nreduces the computational cost associated with traditional FE simulations.\nNext, a Projected Gradient Descent (PGD) approach guided by gradients of the NN\nsurrogate is proposed to efficiently calibrate FE models. Our method explicitly\nenforces feasibility with a projection step, thus maintaining material bounds\nthroughout the optimization process.\n  The proposed method is evaluated against state-of-the-art Genetic Algorithm\n(GA) and inverse model baselines on synthetic and in vitro experimental\ndatasets. Our approach demonstrates superior performance on synthetic data,\nachieving a Mean Absolute Error (MAE) of 0.06 compared to the baselines' MAE of\n0.18 and 0.54, respectively. On experimental specimens, our method outperforms\nthe baseline in 5 out of 6 cases. Most importantly, our approach reduces\ncalibration time to under three seconds, compared to up to 8 days per sample\nrequired by traditional calibration. Such efficiency paves the way for applying\nmore complex FE models, enabling accurate patient-specific simulations and\nadvancing spinal treatment planning.\n", "link": "http://arxiv.org/abs/2408.06067v1", "date": "2024-08-12", "relevancy": 1.9368, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4896}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4846}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4816}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Don%27t%20You%20%28Project%20Around%20Discs%29%3F%20Neural%20Network%20Surrogate%20and%20Projected%0A%20%20Gradient%20Descent%20for%20Calibrating%20an%20Intervertebral%20Disc%20Finite%20Element%20Model&body=Title%3A%20Don%27t%20You%20%28Project%20Around%20Discs%29%3F%20Neural%20Network%20Surrogate%20and%20Projected%0A%20%20Gradient%20Descent%20for%20Calibrating%20an%20Intervertebral%20Disc%20Finite%20Element%20Model%0AAuthor%3A%20Matan%20Atad%20and%20Gabriel%20Gruber%20and%20Marx%20Ribeiro%20and%20Luis%20Fernando%20Nicolini%20and%20Robert%20Graf%20and%20Hendrik%20M%C3%B6ller%20and%20Kati%20Nispel%20and%20Ivan%20Ezhov%20and%20Daniel%20Rueckert%20and%20Jan%20S.%20Kirschke%0AAbstract%3A%20%20%20Accurate%20calibration%20of%20finite%20element%20%28FE%29%20models%20of%20human%20intervertebral%0Adiscs%20%28IVDs%29%20is%20essential%20for%20their%20reliability%20and%20application%20in%20diagnosing%0Aand%20planning%20treatments%20for%20spinal%20conditions.%20Traditional%20calibration%20methods%0Aare%20computationally%20intensive%2C%20requiring%20iterative%2C%20derivative-free%0Aoptimization%20algorithms%20that%20often%20take%20hours%20or%20days%20to%20converge.%0A%20%20This%20study%20addresses%20these%20challenges%20by%20introducing%20a%20novel%2C%20efficient%2C%20and%0Aeffective%20calibration%20method%20for%20an%20L4-L5%20IVD%20FE%20model%20using%20a%20neural%20network%0A%28NN%29%20surrogate.%20The%20NN%20surrogate%20predicts%20simulation%20outcomes%20with%20high%0Aaccuracy%2C%20outperforming%20other%20machine%20learning%20models%2C%20and%20significantly%0Areduces%20the%20computational%20cost%20associated%20with%20traditional%20FE%20simulations.%0ANext%2C%20a%20Projected%20Gradient%20Descent%20%28PGD%29%20approach%20guided%20by%20gradients%20of%20the%20NN%0Asurrogate%20is%20proposed%20to%20efficiently%20calibrate%20FE%20models.%20Our%20method%20explicitly%0Aenforces%20feasibility%20with%20a%20projection%20step%2C%20thus%20maintaining%20material%20bounds%0Athroughout%20the%20optimization%20process.%0A%20%20The%20proposed%20method%20is%20evaluated%20against%20state-of-the-art%20Genetic%20Algorithm%0A%28GA%29%20and%20inverse%20model%20baselines%20on%20synthetic%20and%20in%20vitro%20experimental%0Adatasets.%20Our%20approach%20demonstrates%20superior%20performance%20on%20synthetic%20data%2C%0Aachieving%20a%20Mean%20Absolute%20Error%20%28MAE%29%20of%200.06%20compared%20to%20the%20baselines%27%20MAE%20of%0A0.18%20and%200.54%2C%20respectively.%20On%20experimental%20specimens%2C%20our%20method%20outperforms%0Athe%20baseline%20in%205%20out%20of%206%20cases.%20Most%20importantly%2C%20our%20approach%20reduces%0Acalibration%20time%20to%20under%20three%20seconds%2C%20compared%20to%20up%20to%208%20days%20per%20sample%0Arequired%20by%20traditional%20calibration.%20Such%20efficiency%20paves%20the%20way%20for%20applying%0Amore%20complex%20FE%20models%2C%20enabling%20accurate%20patient-specific%20simulations%20and%0Aadvancing%20spinal%20treatment%20planning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06067v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDon%2527t%2520You%2520%2528Project%2520Around%2520Discs%2529%253F%2520Neural%2520Network%2520Surrogate%2520and%2520Projected%250A%2520%2520Gradient%2520Descent%2520for%2520Calibrating%2520an%2520Intervertebral%2520Disc%2520Finite%2520Element%2520Model%26entry.906535625%3DMatan%2520Atad%2520and%2520Gabriel%2520Gruber%2520and%2520Marx%2520Ribeiro%2520and%2520Luis%2520Fernando%2520Nicolini%2520and%2520Robert%2520Graf%2520and%2520Hendrik%2520M%25C3%25B6ller%2520and%2520Kati%2520Nispel%2520and%2520Ivan%2520Ezhov%2520and%2520Daniel%2520Rueckert%2520and%2520Jan%2520S.%2520Kirschke%26entry.1292438233%3D%2520%2520Accurate%2520calibration%2520of%2520finite%2520element%2520%2528FE%2529%2520models%2520of%2520human%2520intervertebral%250Adiscs%2520%2528IVDs%2529%2520is%2520essential%2520for%2520their%2520reliability%2520and%2520application%2520in%2520diagnosing%250Aand%2520planning%2520treatments%2520for%2520spinal%2520conditions.%2520Traditional%2520calibration%2520methods%250Aare%2520computationally%2520intensive%252C%2520requiring%2520iterative%252C%2520derivative-free%250Aoptimization%2520algorithms%2520that%2520often%2520take%2520hours%2520or%2520days%2520to%2520converge.%250A%2520%2520This%2520study%2520addresses%2520these%2520challenges%2520by%2520introducing%2520a%2520novel%252C%2520efficient%252C%2520and%250Aeffective%2520calibration%2520method%2520for%2520an%2520L4-L5%2520IVD%2520FE%2520model%2520using%2520a%2520neural%2520network%250A%2528NN%2529%2520surrogate.%2520The%2520NN%2520surrogate%2520predicts%2520simulation%2520outcomes%2520with%2520high%250Aaccuracy%252C%2520outperforming%2520other%2520machine%2520learning%2520models%252C%2520and%2520significantly%250Areduces%2520the%2520computational%2520cost%2520associated%2520with%2520traditional%2520FE%2520simulations.%250ANext%252C%2520a%2520Projected%2520Gradient%2520Descent%2520%2528PGD%2529%2520approach%2520guided%2520by%2520gradients%2520of%2520the%2520NN%250Asurrogate%2520is%2520proposed%2520to%2520efficiently%2520calibrate%2520FE%2520models.%2520Our%2520method%2520explicitly%250Aenforces%2520feasibility%2520with%2520a%2520projection%2520step%252C%2520thus%2520maintaining%2520material%2520bounds%250Athroughout%2520the%2520optimization%2520process.%250A%2520%2520The%2520proposed%2520method%2520is%2520evaluated%2520against%2520state-of-the-art%2520Genetic%2520Algorithm%250A%2528GA%2529%2520and%2520inverse%2520model%2520baselines%2520on%2520synthetic%2520and%2520in%2520vitro%2520experimental%250Adatasets.%2520Our%2520approach%2520demonstrates%2520superior%2520performance%2520on%2520synthetic%2520data%252C%250Aachieving%2520a%2520Mean%2520Absolute%2520Error%2520%2528MAE%2529%2520of%25200.06%2520compared%2520to%2520the%2520baselines%2527%2520MAE%2520of%250A0.18%2520and%25200.54%252C%2520respectively.%2520On%2520experimental%2520specimens%252C%2520our%2520method%2520outperforms%250Athe%2520baseline%2520in%25205%2520out%2520of%25206%2520cases.%2520Most%2520importantly%252C%2520our%2520approach%2520reduces%250Acalibration%2520time%2520to%2520under%2520three%2520seconds%252C%2520compared%2520to%2520up%2520to%25208%2520days%2520per%2520sample%250Arequired%2520by%2520traditional%2520calibration.%2520Such%2520efficiency%2520paves%2520the%2520way%2520for%2520applying%250Amore%2520complex%2520FE%2520models%252C%2520enabling%2520accurate%2520patient-specific%2520simulations%2520and%250Aadvancing%2520spinal%2520treatment%2520planning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06067v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Don%27t%20You%20%28Project%20Around%20Discs%29%3F%20Neural%20Network%20Surrogate%20and%20Projected%0A%20%20Gradient%20Descent%20for%20Calibrating%20an%20Intervertebral%20Disc%20Finite%20Element%20Model&entry.906535625=Matan%20Atad%20and%20Gabriel%20Gruber%20and%20Marx%20Ribeiro%20and%20Luis%20Fernando%20Nicolini%20and%20Robert%20Graf%20and%20Hendrik%20M%C3%B6ller%20and%20Kati%20Nispel%20and%20Ivan%20Ezhov%20and%20Daniel%20Rueckert%20and%20Jan%20S.%20Kirschke&entry.1292438233=%20%20Accurate%20calibration%20of%20finite%20element%20%28FE%29%20models%20of%20human%20intervertebral%0Adiscs%20%28IVDs%29%20is%20essential%20for%20their%20reliability%20and%20application%20in%20diagnosing%0Aand%20planning%20treatments%20for%20spinal%20conditions.%20Traditional%20calibration%20methods%0Aare%20computationally%20intensive%2C%20requiring%20iterative%2C%20derivative-free%0Aoptimization%20algorithms%20that%20often%20take%20hours%20or%20days%20to%20converge.%0A%20%20This%20study%20addresses%20these%20challenges%20by%20introducing%20a%20novel%2C%20efficient%2C%20and%0Aeffective%20calibration%20method%20for%20an%20L4-L5%20IVD%20FE%20model%20using%20a%20neural%20network%0A%28NN%29%20surrogate.%20The%20NN%20surrogate%20predicts%20simulation%20outcomes%20with%20high%0Aaccuracy%2C%20outperforming%20other%20machine%20learning%20models%2C%20and%20significantly%0Areduces%20the%20computational%20cost%20associated%20with%20traditional%20FE%20simulations.%0ANext%2C%20a%20Projected%20Gradient%20Descent%20%28PGD%29%20approach%20guided%20by%20gradients%20of%20the%20NN%0Asurrogate%20is%20proposed%20to%20efficiently%20calibrate%20FE%20models.%20Our%20method%20explicitly%0Aenforces%20feasibility%20with%20a%20projection%20step%2C%20thus%20maintaining%20material%20bounds%0Athroughout%20the%20optimization%20process.%0A%20%20The%20proposed%20method%20is%20evaluated%20against%20state-of-the-art%20Genetic%20Algorithm%0A%28GA%29%20and%20inverse%20model%20baselines%20on%20synthetic%20and%20in%20vitro%20experimental%0Adatasets.%20Our%20approach%20demonstrates%20superior%20performance%20on%20synthetic%20data%2C%0Aachieving%20a%20Mean%20Absolute%20Error%20%28MAE%29%20of%200.06%20compared%20to%20the%20baselines%27%20MAE%20of%0A0.18%20and%200.54%2C%20respectively.%20On%20experimental%20specimens%2C%20our%20method%20outperforms%0Athe%20baseline%20in%205%20out%20of%206%20cases.%20Most%20importantly%2C%20our%20approach%20reduces%0Acalibration%20time%20to%20under%20three%20seconds%2C%20compared%20to%20up%20to%208%20days%20per%20sample%0Arequired%20by%20traditional%20calibration.%20Such%20efficiency%20paves%20the%20way%20for%20applying%0Amore%20complex%20FE%20models%2C%20enabling%20accurate%20patient-specific%20simulations%20and%0Aadvancing%20spinal%20treatment%20planning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06067v1&entry.124074799=Read"},
{"title": "Localising the Seizure Onset Zone from Single-Pulse Electrical\n  Stimulation Responses with a CNN Transformer", "author": "Jamie Norris and Aswin Chari and Dorien van Blooijs and Gerald Cooray and Karl Friston and Martin Tisdall and Richard Rosch", "abstract": "  Epilepsy is one of the most common neurological disorders, often requiring\nsurgical intervention when medication fails to control seizures. For effective\nsurgical outcomes, precise localisation of the epileptogenic focus - often\napproximated through the Seizure Onset Zone (SOZ) - is critical yet remains a\nchallenge. Active probing through electrical stimulation is already standard\nclinical practice for identifying epileptogenic areas. Our study advances the\napplication of deep learning for SOZ localisation using Single-Pulse Electrical\nStimulation (SPES) responses, with two key contributions. Firstly, we implement\nan existing deep learning model to compare two SPES analysis paradigms:\ndivergent and convergent. These paradigms evaluate outward and inward effective\nconnections, respectively. We assess the generalisability of these models to\nunseen patients and electrode placements using held-out test sets. Our findings\nreveal a notable improvement in moving from a divergent (AUROC: 0.574) to a\nconvergent approach (AUROC: 0.666), marking the first application of the latter\nin this context. Secondly, we demonstrate the efficacy of CNN Transformers with\ncross-channel attention in handling heterogeneous electrode placements,\nincreasing the AUROC to 0.730. These findings represent a significant step in\nmodelling patient-specific intracranial EEG electrode placements in SPES.\nFuture work will explore integrating these models into clinical decision-making\nprocesses to bridge the gap between deep learning research and practical\nhealthcare applications.\n", "link": "http://arxiv.org/abs/2403.20324v2", "date": "2024-08-12", "relevancy": 1.9364, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.487}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4852}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Localising%20the%20Seizure%20Onset%20Zone%20from%20Single-Pulse%20Electrical%0A%20%20Stimulation%20Responses%20with%20a%20CNN%20Transformer&body=Title%3A%20Localising%20the%20Seizure%20Onset%20Zone%20from%20Single-Pulse%20Electrical%0A%20%20Stimulation%20Responses%20with%20a%20CNN%20Transformer%0AAuthor%3A%20Jamie%20Norris%20and%20Aswin%20Chari%20and%20Dorien%20van%20Blooijs%20and%20Gerald%20Cooray%20and%20Karl%20Friston%20and%20Martin%20Tisdall%20and%20Richard%20Rosch%0AAbstract%3A%20%20%20Epilepsy%20is%20one%20of%20the%20most%20common%20neurological%20disorders%2C%20often%20requiring%0Asurgical%20intervention%20when%20medication%20fails%20to%20control%20seizures.%20For%20effective%0Asurgical%20outcomes%2C%20precise%20localisation%20of%20the%20epileptogenic%20focus%20-%20often%0Aapproximated%20through%20the%20Seizure%20Onset%20Zone%20%28SOZ%29%20-%20is%20critical%20yet%20remains%20a%0Achallenge.%20Active%20probing%20through%20electrical%20stimulation%20is%20already%20standard%0Aclinical%20practice%20for%20identifying%20epileptogenic%20areas.%20Our%20study%20advances%20the%0Aapplication%20of%20deep%20learning%20for%20SOZ%20localisation%20using%20Single-Pulse%20Electrical%0AStimulation%20%28SPES%29%20responses%2C%20with%20two%20key%20contributions.%20Firstly%2C%20we%20implement%0Aan%20existing%20deep%20learning%20model%20to%20compare%20two%20SPES%20analysis%20paradigms%3A%0Adivergent%20and%20convergent.%20These%20paradigms%20evaluate%20outward%20and%20inward%20effective%0Aconnections%2C%20respectively.%20We%20assess%20the%20generalisability%20of%20these%20models%20to%0Aunseen%20patients%20and%20electrode%20placements%20using%20held-out%20test%20sets.%20Our%20findings%0Areveal%20a%20notable%20improvement%20in%20moving%20from%20a%20divergent%20%28AUROC%3A%200.574%29%20to%20a%0Aconvergent%20approach%20%28AUROC%3A%200.666%29%2C%20marking%20the%20first%20application%20of%20the%20latter%0Ain%20this%20context.%20Secondly%2C%20we%20demonstrate%20the%20efficacy%20of%20CNN%20Transformers%20with%0Across-channel%20attention%20in%20handling%20heterogeneous%20electrode%20placements%2C%0Aincreasing%20the%20AUROC%20to%200.730.%20These%20findings%20represent%20a%20significant%20step%20in%0Amodelling%20patient-specific%20intracranial%20EEG%20electrode%20placements%20in%20SPES.%0AFuture%20work%20will%20explore%20integrating%20these%20models%20into%20clinical%20decision-making%0Aprocesses%20to%20bridge%20the%20gap%20between%20deep%20learning%20research%20and%20practical%0Ahealthcare%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20324v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocalising%2520the%2520Seizure%2520Onset%2520Zone%2520from%2520Single-Pulse%2520Electrical%250A%2520%2520Stimulation%2520Responses%2520with%2520a%2520CNN%2520Transformer%26entry.906535625%3DJamie%2520Norris%2520and%2520Aswin%2520Chari%2520and%2520Dorien%2520van%2520Blooijs%2520and%2520Gerald%2520Cooray%2520and%2520Karl%2520Friston%2520and%2520Martin%2520Tisdall%2520and%2520Richard%2520Rosch%26entry.1292438233%3D%2520%2520Epilepsy%2520is%2520one%2520of%2520the%2520most%2520common%2520neurological%2520disorders%252C%2520often%2520requiring%250Asurgical%2520intervention%2520when%2520medication%2520fails%2520to%2520control%2520seizures.%2520For%2520effective%250Asurgical%2520outcomes%252C%2520precise%2520localisation%2520of%2520the%2520epileptogenic%2520focus%2520-%2520often%250Aapproximated%2520through%2520the%2520Seizure%2520Onset%2520Zone%2520%2528SOZ%2529%2520-%2520is%2520critical%2520yet%2520remains%2520a%250Achallenge.%2520Active%2520probing%2520through%2520electrical%2520stimulation%2520is%2520already%2520standard%250Aclinical%2520practice%2520for%2520identifying%2520epileptogenic%2520areas.%2520Our%2520study%2520advances%2520the%250Aapplication%2520of%2520deep%2520learning%2520for%2520SOZ%2520localisation%2520using%2520Single-Pulse%2520Electrical%250AStimulation%2520%2528SPES%2529%2520responses%252C%2520with%2520two%2520key%2520contributions.%2520Firstly%252C%2520we%2520implement%250Aan%2520existing%2520deep%2520learning%2520model%2520to%2520compare%2520two%2520SPES%2520analysis%2520paradigms%253A%250Adivergent%2520and%2520convergent.%2520These%2520paradigms%2520evaluate%2520outward%2520and%2520inward%2520effective%250Aconnections%252C%2520respectively.%2520We%2520assess%2520the%2520generalisability%2520of%2520these%2520models%2520to%250Aunseen%2520patients%2520and%2520electrode%2520placements%2520using%2520held-out%2520test%2520sets.%2520Our%2520findings%250Areveal%2520a%2520notable%2520improvement%2520in%2520moving%2520from%2520a%2520divergent%2520%2528AUROC%253A%25200.574%2529%2520to%2520a%250Aconvergent%2520approach%2520%2528AUROC%253A%25200.666%2529%252C%2520marking%2520the%2520first%2520application%2520of%2520the%2520latter%250Ain%2520this%2520context.%2520Secondly%252C%2520we%2520demonstrate%2520the%2520efficacy%2520of%2520CNN%2520Transformers%2520with%250Across-channel%2520attention%2520in%2520handling%2520heterogeneous%2520electrode%2520placements%252C%250Aincreasing%2520the%2520AUROC%2520to%25200.730.%2520These%2520findings%2520represent%2520a%2520significant%2520step%2520in%250Amodelling%2520patient-specific%2520intracranial%2520EEG%2520electrode%2520placements%2520in%2520SPES.%250AFuture%2520work%2520will%2520explore%2520integrating%2520these%2520models%2520into%2520clinical%2520decision-making%250Aprocesses%2520to%2520bridge%2520the%2520gap%2520between%2520deep%2520learning%2520research%2520and%2520practical%250Ahealthcare%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.20324v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Localising%20the%20Seizure%20Onset%20Zone%20from%20Single-Pulse%20Electrical%0A%20%20Stimulation%20Responses%20with%20a%20CNN%20Transformer&entry.906535625=Jamie%20Norris%20and%20Aswin%20Chari%20and%20Dorien%20van%20Blooijs%20and%20Gerald%20Cooray%20and%20Karl%20Friston%20and%20Martin%20Tisdall%20and%20Richard%20Rosch&entry.1292438233=%20%20Epilepsy%20is%20one%20of%20the%20most%20common%20neurological%20disorders%2C%20often%20requiring%0Asurgical%20intervention%20when%20medication%20fails%20to%20control%20seizures.%20For%20effective%0Asurgical%20outcomes%2C%20precise%20localisation%20of%20the%20epileptogenic%20focus%20-%20often%0Aapproximated%20through%20the%20Seizure%20Onset%20Zone%20%28SOZ%29%20-%20is%20critical%20yet%20remains%20a%0Achallenge.%20Active%20probing%20through%20electrical%20stimulation%20is%20already%20standard%0Aclinical%20practice%20for%20identifying%20epileptogenic%20areas.%20Our%20study%20advances%20the%0Aapplication%20of%20deep%20learning%20for%20SOZ%20localisation%20using%20Single-Pulse%20Electrical%0AStimulation%20%28SPES%29%20responses%2C%20with%20two%20key%20contributions.%20Firstly%2C%20we%20implement%0Aan%20existing%20deep%20learning%20model%20to%20compare%20two%20SPES%20analysis%20paradigms%3A%0Adivergent%20and%20convergent.%20These%20paradigms%20evaluate%20outward%20and%20inward%20effective%0Aconnections%2C%20respectively.%20We%20assess%20the%20generalisability%20of%20these%20models%20to%0Aunseen%20patients%20and%20electrode%20placements%20using%20held-out%20test%20sets.%20Our%20findings%0Areveal%20a%20notable%20improvement%20in%20moving%20from%20a%20divergent%20%28AUROC%3A%200.574%29%20to%20a%0Aconvergent%20approach%20%28AUROC%3A%200.666%29%2C%20marking%20the%20first%20application%20of%20the%20latter%0Ain%20this%20context.%20Secondly%2C%20we%20demonstrate%20the%20efficacy%20of%20CNN%20Transformers%20with%0Across-channel%20attention%20in%20handling%20heterogeneous%20electrode%20placements%2C%0Aincreasing%20the%20AUROC%20to%200.730.%20These%20findings%20represent%20a%20significant%20step%20in%0Amodelling%20patient-specific%20intracranial%20EEG%20electrode%20placements%20in%20SPES.%0AFuture%20work%20will%20explore%20integrating%20these%20models%20into%20clinical%20decision-making%0Aprocesses%20to%20bridge%20the%20gap%20between%20deep%20learning%20research%20and%20practical%0Ahealthcare%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20324v2&entry.124074799=Read"},
{"title": "ReLU-KAN: New Kolmogorov-Arnold Networks that Only Need Matrix Addition,\n  Dot Multiplication, and ReLU", "author": "Qi Qiu and Tao Zhu and Helin Gong and Liming Chen and Huansheng Ning", "abstract": "  Limited by the complexity of basis function (B-spline) calculations,\nKolmogorov-Arnold Networks (KAN) suffer from restricted parallel computing\ncapability on GPUs. This paper proposes a novel ReLU-KAN implementation that\ninherits the core idea of KAN. By adopting ReLU (Rectified Linear Unit) and\npoint-wise multiplication, we simplify the design of KAN's basis function and\noptimize the computation process for efficient CUDA computing. The proposed\nReLU-KAN architecture can be readily implemented on existing deep learning\nframeworks (e.g., PyTorch) for both inference and training. Experimental\nresults demonstrate that ReLU-KAN achieves a 20x speedup compared to\ntraditional KAN with 4-layer networks. Furthermore, ReLU-KAN exhibits a more\nstable training process with superior fitting ability while preserving the\n\"catastrophic forgetting avoidance\" property of KAN. You can get the code in\nhttps://github.com/quiqi/relu_kan\n", "link": "http://arxiv.org/abs/2406.02075v2", "date": "2024-08-12", "relevancy": 1.9232, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4909}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4752}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4697}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReLU-KAN%3A%20New%20Kolmogorov-Arnold%20Networks%20that%20Only%20Need%20Matrix%20Addition%2C%0A%20%20Dot%20Multiplication%2C%20and%20ReLU&body=Title%3A%20ReLU-KAN%3A%20New%20Kolmogorov-Arnold%20Networks%20that%20Only%20Need%20Matrix%20Addition%2C%0A%20%20Dot%20Multiplication%2C%20and%20ReLU%0AAuthor%3A%20Qi%20Qiu%20and%20Tao%20Zhu%20and%20Helin%20Gong%20and%20Liming%20Chen%20and%20Huansheng%20Ning%0AAbstract%3A%20%20%20Limited%20by%20the%20complexity%20of%20basis%20function%20%28B-spline%29%20calculations%2C%0AKolmogorov-Arnold%20Networks%20%28KAN%29%20suffer%20from%20restricted%20parallel%20computing%0Acapability%20on%20GPUs.%20This%20paper%20proposes%20a%20novel%20ReLU-KAN%20implementation%20that%0Ainherits%20the%20core%20idea%20of%20KAN.%20By%20adopting%20ReLU%20%28Rectified%20Linear%20Unit%29%20and%0Apoint-wise%20multiplication%2C%20we%20simplify%20the%20design%20of%20KAN%27s%20basis%20function%20and%0Aoptimize%20the%20computation%20process%20for%20efficient%20CUDA%20computing.%20The%20proposed%0AReLU-KAN%20architecture%20can%20be%20readily%20implemented%20on%20existing%20deep%20learning%0Aframeworks%20%28e.g.%2C%20PyTorch%29%20for%20both%20inference%20and%20training.%20Experimental%0Aresults%20demonstrate%20that%20ReLU-KAN%20achieves%20a%2020x%20speedup%20compared%20to%0Atraditional%20KAN%20with%204-layer%20networks.%20Furthermore%2C%20ReLU-KAN%20exhibits%20a%20more%0Astable%20training%20process%20with%20superior%20fitting%20ability%20while%20preserving%20the%0A%22catastrophic%20forgetting%20avoidance%22%20property%20of%20KAN.%20You%20can%20get%20the%20code%20in%0Ahttps%3A//github.com/quiqi/relu_kan%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02075v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReLU-KAN%253A%2520New%2520Kolmogorov-Arnold%2520Networks%2520that%2520Only%2520Need%2520Matrix%2520Addition%252C%250A%2520%2520Dot%2520Multiplication%252C%2520and%2520ReLU%26entry.906535625%3DQi%2520Qiu%2520and%2520Tao%2520Zhu%2520and%2520Helin%2520Gong%2520and%2520Liming%2520Chen%2520and%2520Huansheng%2520Ning%26entry.1292438233%3D%2520%2520Limited%2520by%2520the%2520complexity%2520of%2520basis%2520function%2520%2528B-spline%2529%2520calculations%252C%250AKolmogorov-Arnold%2520Networks%2520%2528KAN%2529%2520suffer%2520from%2520restricted%2520parallel%2520computing%250Acapability%2520on%2520GPUs.%2520This%2520paper%2520proposes%2520a%2520novel%2520ReLU-KAN%2520implementation%2520that%250Ainherits%2520the%2520core%2520idea%2520of%2520KAN.%2520By%2520adopting%2520ReLU%2520%2528Rectified%2520Linear%2520Unit%2529%2520and%250Apoint-wise%2520multiplication%252C%2520we%2520simplify%2520the%2520design%2520of%2520KAN%2527s%2520basis%2520function%2520and%250Aoptimize%2520the%2520computation%2520process%2520for%2520efficient%2520CUDA%2520computing.%2520The%2520proposed%250AReLU-KAN%2520architecture%2520can%2520be%2520readily%2520implemented%2520on%2520existing%2520deep%2520learning%250Aframeworks%2520%2528e.g.%252C%2520PyTorch%2529%2520for%2520both%2520inference%2520and%2520training.%2520Experimental%250Aresults%2520demonstrate%2520that%2520ReLU-KAN%2520achieves%2520a%252020x%2520speedup%2520compared%2520to%250Atraditional%2520KAN%2520with%25204-layer%2520networks.%2520Furthermore%252C%2520ReLU-KAN%2520exhibits%2520a%2520more%250Astable%2520training%2520process%2520with%2520superior%2520fitting%2520ability%2520while%2520preserving%2520the%250A%2522catastrophic%2520forgetting%2520avoidance%2522%2520property%2520of%2520KAN.%2520You%2520can%2520get%2520the%2520code%2520in%250Ahttps%253A//github.com/quiqi/relu_kan%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02075v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReLU-KAN%3A%20New%20Kolmogorov-Arnold%20Networks%20that%20Only%20Need%20Matrix%20Addition%2C%0A%20%20Dot%20Multiplication%2C%20and%20ReLU&entry.906535625=Qi%20Qiu%20and%20Tao%20Zhu%20and%20Helin%20Gong%20and%20Liming%20Chen%20and%20Huansheng%20Ning&entry.1292438233=%20%20Limited%20by%20the%20complexity%20of%20basis%20function%20%28B-spline%29%20calculations%2C%0AKolmogorov-Arnold%20Networks%20%28KAN%29%20suffer%20from%20restricted%20parallel%20computing%0Acapability%20on%20GPUs.%20This%20paper%20proposes%20a%20novel%20ReLU-KAN%20implementation%20that%0Ainherits%20the%20core%20idea%20of%20KAN.%20By%20adopting%20ReLU%20%28Rectified%20Linear%20Unit%29%20and%0Apoint-wise%20multiplication%2C%20we%20simplify%20the%20design%20of%20KAN%27s%20basis%20function%20and%0Aoptimize%20the%20computation%20process%20for%20efficient%20CUDA%20computing.%20The%20proposed%0AReLU-KAN%20architecture%20can%20be%20readily%20implemented%20on%20existing%20deep%20learning%0Aframeworks%20%28e.g.%2C%20PyTorch%29%20for%20both%20inference%20and%20training.%20Experimental%0Aresults%20demonstrate%20that%20ReLU-KAN%20achieves%20a%2020x%20speedup%20compared%20to%0Atraditional%20KAN%20with%204-layer%20networks.%20Furthermore%2C%20ReLU-KAN%20exhibits%20a%20more%0Astable%20training%20process%20with%20superior%20fitting%20ability%20while%20preserving%20the%0A%22catastrophic%20forgetting%20avoidance%22%20property%20of%20KAN.%20You%20can%20get%20the%20code%20in%0Ahttps%3A//github.com/quiqi/relu_kan%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02075v2&entry.124074799=Read"},
{"title": "ControlNet-XS: Rethinking the Control of Text-to-Image Diffusion Models\n  as Feedback-Control Systems", "author": "Denis Zavadski and Johann-Friedrich Feiden and Carsten Rother", "abstract": "  The field of image synthesis has made tremendous strides forward in the last\nyears. Besides defining the desired output image with text-prompts, an\nintuitive approach is to additionally use spatial guidance in form of an image,\nsuch as a depth map. In state-of-the-art approaches, this guidance is realized\nby a separate controlling model that controls a pre-trained image generation\nnetwork, such as a latent diffusion model. Understanding this process from a\ncontrol system perspective shows that it forms a feedback-control system, where\nthe control module receives a feedback signal from the generation process and\nsends a corrective signal back. When analysing existing systems, we observe\nthat the feedback signals are timely sparse and have a small number of bits. As\na consequence, there can be long delays between newly generated features and\nthe respective corrective signals for these features. It is known that this\ndelay is the most unwanted aspect of any control system. In this work, we take\nan existing controlling network (ControlNet) and change the communication\nbetween the controlling network and the generation process to be of\nhigh-frequency and with large-bandwidth. By doing so, we are able to\nconsiderably improve the quality of the generated images, as well as the\nfidelity of the control. Also, the controlling network needs noticeably fewer\nparameters and hence is about twice as fast during inference and training time.\nAnother benefit of small-sized models is that they help to democratise our\nfield and are likely easier to understand. We call our proposed network\nControlNet-XS. When comparing with the state-of-the-art approaches, we\noutperform them for pixel-level guidance, such as depth, canny-edges, and\nsemantic segmentation, and are on a par for loose keypoint-guidance of human\nposes. All code and pre-trained models will be made publicly available.\n", "link": "http://arxiv.org/abs/2312.06573v2", "date": "2024-08-12", "relevancy": 1.9226, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6777}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6655}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ControlNet-XS%3A%20Rethinking%20the%20Control%20of%20Text-to-Image%20Diffusion%20Models%0A%20%20as%20Feedback-Control%20Systems&body=Title%3A%20ControlNet-XS%3A%20Rethinking%20the%20Control%20of%20Text-to-Image%20Diffusion%20Models%0A%20%20as%20Feedback-Control%20Systems%0AAuthor%3A%20Denis%20Zavadski%20and%20Johann-Friedrich%20Feiden%20and%20Carsten%20Rother%0AAbstract%3A%20%20%20The%20field%20of%20image%20synthesis%20has%20made%20tremendous%20strides%20forward%20in%20the%20last%0Ayears.%20Besides%20defining%20the%20desired%20output%20image%20with%20text-prompts%2C%20an%0Aintuitive%20approach%20is%20to%20additionally%20use%20spatial%20guidance%20in%20form%20of%20an%20image%2C%0Asuch%20as%20a%20depth%20map.%20In%20state-of-the-art%20approaches%2C%20this%20guidance%20is%20realized%0Aby%20a%20separate%20controlling%20model%20that%20controls%20a%20pre-trained%20image%20generation%0Anetwork%2C%20such%20as%20a%20latent%20diffusion%20model.%20Understanding%20this%20process%20from%20a%0Acontrol%20system%20perspective%20shows%20that%20it%20forms%20a%20feedback-control%20system%2C%20where%0Athe%20control%20module%20receives%20a%20feedback%20signal%20from%20the%20generation%20process%20and%0Asends%20a%20corrective%20signal%20back.%20When%20analysing%20existing%20systems%2C%20we%20observe%0Athat%20the%20feedback%20signals%20are%20timely%20sparse%20and%20have%20a%20small%20number%20of%20bits.%20As%0Aa%20consequence%2C%20there%20can%20be%20long%20delays%20between%20newly%20generated%20features%20and%0Athe%20respective%20corrective%20signals%20for%20these%20features.%20It%20is%20known%20that%20this%0Adelay%20is%20the%20most%20unwanted%20aspect%20of%20any%20control%20system.%20In%20this%20work%2C%20we%20take%0Aan%20existing%20controlling%20network%20%28ControlNet%29%20and%20change%20the%20communication%0Abetween%20the%20controlling%20network%20and%20the%20generation%20process%20to%20be%20of%0Ahigh-frequency%20and%20with%20large-bandwidth.%20By%20doing%20so%2C%20we%20are%20able%20to%0Aconsiderably%20improve%20the%20quality%20of%20the%20generated%20images%2C%20as%20well%20as%20the%0Afidelity%20of%20the%20control.%20Also%2C%20the%20controlling%20network%20needs%20noticeably%20fewer%0Aparameters%20and%20hence%20is%20about%20twice%20as%20fast%20during%20inference%20and%20training%20time.%0AAnother%20benefit%20of%20small-sized%20models%20is%20that%20they%20help%20to%20democratise%20our%0Afield%20and%20are%20likely%20easier%20to%20understand.%20We%20call%20our%20proposed%20network%0AControlNet-XS.%20When%20comparing%20with%20the%20state-of-the-art%20approaches%2C%20we%0Aoutperform%20them%20for%20pixel-level%20guidance%2C%20such%20as%20depth%2C%20canny-edges%2C%20and%0Asemantic%20segmentation%2C%20and%20are%20on%20a%20par%20for%20loose%20keypoint-guidance%20of%20human%0Aposes.%20All%20code%20and%20pre-trained%20models%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.06573v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DControlNet-XS%253A%2520Rethinking%2520the%2520Control%2520of%2520Text-to-Image%2520Diffusion%2520Models%250A%2520%2520as%2520Feedback-Control%2520Systems%26entry.906535625%3DDenis%2520Zavadski%2520and%2520Johann-Friedrich%2520Feiden%2520and%2520Carsten%2520Rother%26entry.1292438233%3D%2520%2520The%2520field%2520of%2520image%2520synthesis%2520has%2520made%2520tremendous%2520strides%2520forward%2520in%2520the%2520last%250Ayears.%2520Besides%2520defining%2520the%2520desired%2520output%2520image%2520with%2520text-prompts%252C%2520an%250Aintuitive%2520approach%2520is%2520to%2520additionally%2520use%2520spatial%2520guidance%2520in%2520form%2520of%2520an%2520image%252C%250Asuch%2520as%2520a%2520depth%2520map.%2520In%2520state-of-the-art%2520approaches%252C%2520this%2520guidance%2520is%2520realized%250Aby%2520a%2520separate%2520controlling%2520model%2520that%2520controls%2520a%2520pre-trained%2520image%2520generation%250Anetwork%252C%2520such%2520as%2520a%2520latent%2520diffusion%2520model.%2520Understanding%2520this%2520process%2520from%2520a%250Acontrol%2520system%2520perspective%2520shows%2520that%2520it%2520forms%2520a%2520feedback-control%2520system%252C%2520where%250Athe%2520control%2520module%2520receives%2520a%2520feedback%2520signal%2520from%2520the%2520generation%2520process%2520and%250Asends%2520a%2520corrective%2520signal%2520back.%2520When%2520analysing%2520existing%2520systems%252C%2520we%2520observe%250Athat%2520the%2520feedback%2520signals%2520are%2520timely%2520sparse%2520and%2520have%2520a%2520small%2520number%2520of%2520bits.%2520As%250Aa%2520consequence%252C%2520there%2520can%2520be%2520long%2520delays%2520between%2520newly%2520generated%2520features%2520and%250Athe%2520respective%2520corrective%2520signals%2520for%2520these%2520features.%2520It%2520is%2520known%2520that%2520this%250Adelay%2520is%2520the%2520most%2520unwanted%2520aspect%2520of%2520any%2520control%2520system.%2520In%2520this%2520work%252C%2520we%2520take%250Aan%2520existing%2520controlling%2520network%2520%2528ControlNet%2529%2520and%2520change%2520the%2520communication%250Abetween%2520the%2520controlling%2520network%2520and%2520the%2520generation%2520process%2520to%2520be%2520of%250Ahigh-frequency%2520and%2520with%2520large-bandwidth.%2520By%2520doing%2520so%252C%2520we%2520are%2520able%2520to%250Aconsiderably%2520improve%2520the%2520quality%2520of%2520the%2520generated%2520images%252C%2520as%2520well%2520as%2520the%250Afidelity%2520of%2520the%2520control.%2520Also%252C%2520the%2520controlling%2520network%2520needs%2520noticeably%2520fewer%250Aparameters%2520and%2520hence%2520is%2520about%2520twice%2520as%2520fast%2520during%2520inference%2520and%2520training%2520time.%250AAnother%2520benefit%2520of%2520small-sized%2520models%2520is%2520that%2520they%2520help%2520to%2520democratise%2520our%250Afield%2520and%2520are%2520likely%2520easier%2520to%2520understand.%2520We%2520call%2520our%2520proposed%2520network%250AControlNet-XS.%2520When%2520comparing%2520with%2520the%2520state-of-the-art%2520approaches%252C%2520we%250Aoutperform%2520them%2520for%2520pixel-level%2520guidance%252C%2520such%2520as%2520depth%252C%2520canny-edges%252C%2520and%250Asemantic%2520segmentation%252C%2520and%2520are%2520on%2520a%2520par%2520for%2520loose%2520keypoint-guidance%2520of%2520human%250Aposes.%2520All%2520code%2520and%2520pre-trained%2520models%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.06573v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ControlNet-XS%3A%20Rethinking%20the%20Control%20of%20Text-to-Image%20Diffusion%20Models%0A%20%20as%20Feedback-Control%20Systems&entry.906535625=Denis%20Zavadski%20and%20Johann-Friedrich%20Feiden%20and%20Carsten%20Rother&entry.1292438233=%20%20The%20field%20of%20image%20synthesis%20has%20made%20tremendous%20strides%20forward%20in%20the%20last%0Ayears.%20Besides%20defining%20the%20desired%20output%20image%20with%20text-prompts%2C%20an%0Aintuitive%20approach%20is%20to%20additionally%20use%20spatial%20guidance%20in%20form%20of%20an%20image%2C%0Asuch%20as%20a%20depth%20map.%20In%20state-of-the-art%20approaches%2C%20this%20guidance%20is%20realized%0Aby%20a%20separate%20controlling%20model%20that%20controls%20a%20pre-trained%20image%20generation%0Anetwork%2C%20such%20as%20a%20latent%20diffusion%20model.%20Understanding%20this%20process%20from%20a%0Acontrol%20system%20perspective%20shows%20that%20it%20forms%20a%20feedback-control%20system%2C%20where%0Athe%20control%20module%20receives%20a%20feedback%20signal%20from%20the%20generation%20process%20and%0Asends%20a%20corrective%20signal%20back.%20When%20analysing%20existing%20systems%2C%20we%20observe%0Athat%20the%20feedback%20signals%20are%20timely%20sparse%20and%20have%20a%20small%20number%20of%20bits.%20As%0Aa%20consequence%2C%20there%20can%20be%20long%20delays%20between%20newly%20generated%20features%20and%0Athe%20respective%20corrective%20signals%20for%20these%20features.%20It%20is%20known%20that%20this%0Adelay%20is%20the%20most%20unwanted%20aspect%20of%20any%20control%20system.%20In%20this%20work%2C%20we%20take%0Aan%20existing%20controlling%20network%20%28ControlNet%29%20and%20change%20the%20communication%0Abetween%20the%20controlling%20network%20and%20the%20generation%20process%20to%20be%20of%0Ahigh-frequency%20and%20with%20large-bandwidth.%20By%20doing%20so%2C%20we%20are%20able%20to%0Aconsiderably%20improve%20the%20quality%20of%20the%20generated%20images%2C%20as%20well%20as%20the%0Afidelity%20of%20the%20control.%20Also%2C%20the%20controlling%20network%20needs%20noticeably%20fewer%0Aparameters%20and%20hence%20is%20about%20twice%20as%20fast%20during%20inference%20and%20training%20time.%0AAnother%20benefit%20of%20small-sized%20models%20is%20that%20they%20help%20to%20democratise%20our%0Afield%20and%20are%20likely%20easier%20to%20understand.%20We%20call%20our%20proposed%20network%0AControlNet-XS.%20When%20comparing%20with%20the%20state-of-the-art%20approaches%2C%20we%0Aoutperform%20them%20for%20pixel-level%20guidance%2C%20such%20as%20depth%2C%20canny-edges%2C%20and%0Asemantic%20segmentation%2C%20and%20are%20on%20a%20par%20for%20loose%20keypoint-guidance%20of%20human%0Aposes.%20All%20code%20and%20pre-trained%20models%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.06573v2&entry.124074799=Read"},
{"title": "Fair Column Subset Selection", "author": "Antonis Matakos and Bruno Ordozgoiti and Suhas Thejaswi", "abstract": "  The problem of column subset selection asks for a subset of columns from an\ninput matrix such that the matrix can be reconstructed as accurately as\npossible within the span of the selected columns. A natural extension is to\nconsider a setting where the matrix rows are partitioned into two groups, and\nthe goal is to choose a subset of columns that minimizes the maximum\nreconstruction error of both groups, relative to their respective best rank-k\napproximation. Extending the known results of column subset selection to this\nfair setting is not straightforward: in certain scenarios it is unavoidable to\nchoose columns separately for each group, resulting in double the expected\ncolumn count. We propose a deterministic leverage-score sampling strategy for\nthe fair setting and show that sampling a column subset of minimum size becomes\nNP-hard in the presence of two groups. Despite these negative results, we give\nan approximation algorithm that guarantees a solution within 1.5 times the\noptimal solution size. We also present practical heuristic algorithms based on\nrank-revealing QR factorization. Finally, we validate our methods through an\nextensive set of experiments using real-world data.\n", "link": "http://arxiv.org/abs/2306.04489v4", "date": "2024-08-12", "relevancy": 1.9128, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4046}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.375}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fair%20Column%20Subset%20Selection&body=Title%3A%20Fair%20Column%20Subset%20Selection%0AAuthor%3A%20Antonis%20Matakos%20and%20Bruno%20Ordozgoiti%20and%20Suhas%20Thejaswi%0AAbstract%3A%20%20%20The%20problem%20of%20column%20subset%20selection%20asks%20for%20a%20subset%20of%20columns%20from%20an%0Ainput%20matrix%20such%20that%20the%20matrix%20can%20be%20reconstructed%20as%20accurately%20as%0Apossible%20within%20the%20span%20of%20the%20selected%20columns.%20A%20natural%20extension%20is%20to%0Aconsider%20a%20setting%20where%20the%20matrix%20rows%20are%20partitioned%20into%20two%20groups%2C%20and%0Athe%20goal%20is%20to%20choose%20a%20subset%20of%20columns%20that%20minimizes%20the%20maximum%0Areconstruction%20error%20of%20both%20groups%2C%20relative%20to%20their%20respective%20best%20rank-k%0Aapproximation.%20Extending%20the%20known%20results%20of%20column%20subset%20selection%20to%20this%0Afair%20setting%20is%20not%20straightforward%3A%20in%20certain%20scenarios%20it%20is%20unavoidable%20to%0Achoose%20columns%20separately%20for%20each%20group%2C%20resulting%20in%20double%20the%20expected%0Acolumn%20count.%20We%20propose%20a%20deterministic%20leverage-score%20sampling%20strategy%20for%0Athe%20fair%20setting%20and%20show%20that%20sampling%20a%20column%20subset%20of%20minimum%20size%20becomes%0ANP-hard%20in%20the%20presence%20of%20two%20groups.%20Despite%20these%20negative%20results%2C%20we%20give%0Aan%20approximation%20algorithm%20that%20guarantees%20a%20solution%20within%201.5%20times%20the%0Aoptimal%20solution%20size.%20We%20also%20present%20practical%20heuristic%20algorithms%20based%20on%0Arank-revealing%20QR%20factorization.%20Finally%2C%20we%20validate%20our%20methods%20through%20an%0Aextensive%20set%20of%20experiments%20using%20real-world%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.04489v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFair%2520Column%2520Subset%2520Selection%26entry.906535625%3DAntonis%2520Matakos%2520and%2520Bruno%2520Ordozgoiti%2520and%2520Suhas%2520Thejaswi%26entry.1292438233%3D%2520%2520The%2520problem%2520of%2520column%2520subset%2520selection%2520asks%2520for%2520a%2520subset%2520of%2520columns%2520from%2520an%250Ainput%2520matrix%2520such%2520that%2520the%2520matrix%2520can%2520be%2520reconstructed%2520as%2520accurately%2520as%250Apossible%2520within%2520the%2520span%2520of%2520the%2520selected%2520columns.%2520A%2520natural%2520extension%2520is%2520to%250Aconsider%2520a%2520setting%2520where%2520the%2520matrix%2520rows%2520are%2520partitioned%2520into%2520two%2520groups%252C%2520and%250Athe%2520goal%2520is%2520to%2520choose%2520a%2520subset%2520of%2520columns%2520that%2520minimizes%2520the%2520maximum%250Areconstruction%2520error%2520of%2520both%2520groups%252C%2520relative%2520to%2520their%2520respective%2520best%2520rank-k%250Aapproximation.%2520Extending%2520the%2520known%2520results%2520of%2520column%2520subset%2520selection%2520to%2520this%250Afair%2520setting%2520is%2520not%2520straightforward%253A%2520in%2520certain%2520scenarios%2520it%2520is%2520unavoidable%2520to%250Achoose%2520columns%2520separately%2520for%2520each%2520group%252C%2520resulting%2520in%2520double%2520the%2520expected%250Acolumn%2520count.%2520We%2520propose%2520a%2520deterministic%2520leverage-score%2520sampling%2520strategy%2520for%250Athe%2520fair%2520setting%2520and%2520show%2520that%2520sampling%2520a%2520column%2520subset%2520of%2520minimum%2520size%2520becomes%250ANP-hard%2520in%2520the%2520presence%2520of%2520two%2520groups.%2520Despite%2520these%2520negative%2520results%252C%2520we%2520give%250Aan%2520approximation%2520algorithm%2520that%2520guarantees%2520a%2520solution%2520within%25201.5%2520times%2520the%250Aoptimal%2520solution%2520size.%2520We%2520also%2520present%2520practical%2520heuristic%2520algorithms%2520based%2520on%250Arank-revealing%2520QR%2520factorization.%2520Finally%252C%2520we%2520validate%2520our%2520methods%2520through%2520an%250Aextensive%2520set%2520of%2520experiments%2520using%2520real-world%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.04489v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fair%20Column%20Subset%20Selection&entry.906535625=Antonis%20Matakos%20and%20Bruno%20Ordozgoiti%20and%20Suhas%20Thejaswi&entry.1292438233=%20%20The%20problem%20of%20column%20subset%20selection%20asks%20for%20a%20subset%20of%20columns%20from%20an%0Ainput%20matrix%20such%20that%20the%20matrix%20can%20be%20reconstructed%20as%20accurately%20as%0Apossible%20within%20the%20span%20of%20the%20selected%20columns.%20A%20natural%20extension%20is%20to%0Aconsider%20a%20setting%20where%20the%20matrix%20rows%20are%20partitioned%20into%20two%20groups%2C%20and%0Athe%20goal%20is%20to%20choose%20a%20subset%20of%20columns%20that%20minimizes%20the%20maximum%0Areconstruction%20error%20of%20both%20groups%2C%20relative%20to%20their%20respective%20best%20rank-k%0Aapproximation.%20Extending%20the%20known%20results%20of%20column%20subset%20selection%20to%20this%0Afair%20setting%20is%20not%20straightforward%3A%20in%20certain%20scenarios%20it%20is%20unavoidable%20to%0Achoose%20columns%20separately%20for%20each%20group%2C%20resulting%20in%20double%20the%20expected%0Acolumn%20count.%20We%20propose%20a%20deterministic%20leverage-score%20sampling%20strategy%20for%0Athe%20fair%20setting%20and%20show%20that%20sampling%20a%20column%20subset%20of%20minimum%20size%20becomes%0ANP-hard%20in%20the%20presence%20of%20two%20groups.%20Despite%20these%20negative%20results%2C%20we%20give%0Aan%20approximation%20algorithm%20that%20guarantees%20a%20solution%20within%201.5%20times%20the%0Aoptimal%20solution%20size.%20We%20also%20present%20practical%20heuristic%20algorithms%20based%20on%0Arank-revealing%20QR%20factorization.%20Finally%2C%20we%20validate%20our%20methods%20through%20an%0Aextensive%20set%20of%20experiments%20using%20real-world%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.04489v4&entry.124074799=Read"},
{"title": "IN-Sight: Interactive Navigation through Sight", "author": "Philipp Schoch and Fan Yang and Yuntao Ma and Stefan Leutenegger and Marco Hutter and Quentin Leboutet", "abstract": "  Current visual navigation systems often treat the environment as static,\nlacking the ability to adaptively interact with obstacles. This limitation\nleads to navigation failure when encountering unavoidable obstructions. In\nresponse, we introduce IN-Sight, a novel approach to self-supervised path\nplanning, enabling more effective navigation strategies through interaction\nwith obstacles. Utilizing RGB-D observations, IN-Sight calculates\ntraversability scores and incorporates them into a semantic map, facilitating\nlong-range path planning in complex, maze-like environments. To precisely\nnavigate around obstacles, IN-Sight employs a local planner, trained\nimperatively on a differentiable costmap using representation learning\ntechniques. The entire framework undergoes end-to-end training within the\nstate-of-the-art photorealistic Intel SPEAR Simulator. We validate the\neffectiveness of IN-Sight through extensive benchmarking in a variety of\nsimulated scenarios and ablation studies. Moreover, we demonstrate the system's\nreal-world applicability with zero-shot sim-to-real transfer, deploying our\nplanner on the legged robot platform ANYmal, showcasing its practical potential\nfor interactive navigation in real environments.\n", "link": "http://arxiv.org/abs/2408.00343v2", "date": "2024-08-12", "relevancy": 1.8863, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6473}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.613}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5983}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IN-Sight%3A%20Interactive%20Navigation%20through%20Sight&body=Title%3A%20IN-Sight%3A%20Interactive%20Navigation%20through%20Sight%0AAuthor%3A%20Philipp%20Schoch%20and%20Fan%20Yang%20and%20Yuntao%20Ma%20and%20Stefan%20Leutenegger%20and%20Marco%20Hutter%20and%20Quentin%20Leboutet%0AAbstract%3A%20%20%20Current%20visual%20navigation%20systems%20often%20treat%20the%20environment%20as%20static%2C%0Alacking%20the%20ability%20to%20adaptively%20interact%20with%20obstacles.%20This%20limitation%0Aleads%20to%20navigation%20failure%20when%20encountering%20unavoidable%20obstructions.%20In%0Aresponse%2C%20we%20introduce%20IN-Sight%2C%20a%20novel%20approach%20to%20self-supervised%20path%0Aplanning%2C%20enabling%20more%20effective%20navigation%20strategies%20through%20interaction%0Awith%20obstacles.%20Utilizing%20RGB-D%20observations%2C%20IN-Sight%20calculates%0Atraversability%20scores%20and%20incorporates%20them%20into%20a%20semantic%20map%2C%20facilitating%0Along-range%20path%20planning%20in%20complex%2C%20maze-like%20environments.%20To%20precisely%0Anavigate%20around%20obstacles%2C%20IN-Sight%20employs%20a%20local%20planner%2C%20trained%0Aimperatively%20on%20a%20differentiable%20costmap%20using%20representation%20learning%0Atechniques.%20The%20entire%20framework%20undergoes%20end-to-end%20training%20within%20the%0Astate-of-the-art%20photorealistic%20Intel%20SPEAR%20Simulator.%20We%20validate%20the%0Aeffectiveness%20of%20IN-Sight%20through%20extensive%20benchmarking%20in%20a%20variety%20of%0Asimulated%20scenarios%20and%20ablation%20studies.%20Moreover%2C%20we%20demonstrate%20the%20system%27s%0Areal-world%20applicability%20with%20zero-shot%20sim-to-real%20transfer%2C%20deploying%20our%0Aplanner%20on%20the%20legged%20robot%20platform%20ANYmal%2C%20showcasing%20its%20practical%20potential%0Afor%20interactive%20navigation%20in%20real%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00343v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIN-Sight%253A%2520Interactive%2520Navigation%2520through%2520Sight%26entry.906535625%3DPhilipp%2520Schoch%2520and%2520Fan%2520Yang%2520and%2520Yuntao%2520Ma%2520and%2520Stefan%2520Leutenegger%2520and%2520Marco%2520Hutter%2520and%2520Quentin%2520Leboutet%26entry.1292438233%3D%2520%2520Current%2520visual%2520navigation%2520systems%2520often%2520treat%2520the%2520environment%2520as%2520static%252C%250Alacking%2520the%2520ability%2520to%2520adaptively%2520interact%2520with%2520obstacles.%2520This%2520limitation%250Aleads%2520to%2520navigation%2520failure%2520when%2520encountering%2520unavoidable%2520obstructions.%2520In%250Aresponse%252C%2520we%2520introduce%2520IN-Sight%252C%2520a%2520novel%2520approach%2520to%2520self-supervised%2520path%250Aplanning%252C%2520enabling%2520more%2520effective%2520navigation%2520strategies%2520through%2520interaction%250Awith%2520obstacles.%2520Utilizing%2520RGB-D%2520observations%252C%2520IN-Sight%2520calculates%250Atraversability%2520scores%2520and%2520incorporates%2520them%2520into%2520a%2520semantic%2520map%252C%2520facilitating%250Along-range%2520path%2520planning%2520in%2520complex%252C%2520maze-like%2520environments.%2520To%2520precisely%250Anavigate%2520around%2520obstacles%252C%2520IN-Sight%2520employs%2520a%2520local%2520planner%252C%2520trained%250Aimperatively%2520on%2520a%2520differentiable%2520costmap%2520using%2520representation%2520learning%250Atechniques.%2520The%2520entire%2520framework%2520undergoes%2520end-to-end%2520training%2520within%2520the%250Astate-of-the-art%2520photorealistic%2520Intel%2520SPEAR%2520Simulator.%2520We%2520validate%2520the%250Aeffectiveness%2520of%2520IN-Sight%2520through%2520extensive%2520benchmarking%2520in%2520a%2520variety%2520of%250Asimulated%2520scenarios%2520and%2520ablation%2520studies.%2520Moreover%252C%2520we%2520demonstrate%2520the%2520system%2527s%250Areal-world%2520applicability%2520with%2520zero-shot%2520sim-to-real%2520transfer%252C%2520deploying%2520our%250Aplanner%2520on%2520the%2520legged%2520robot%2520platform%2520ANYmal%252C%2520showcasing%2520its%2520practical%2520potential%250Afor%2520interactive%2520navigation%2520in%2520real%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00343v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IN-Sight%3A%20Interactive%20Navigation%20through%20Sight&entry.906535625=Philipp%20Schoch%20and%20Fan%20Yang%20and%20Yuntao%20Ma%20and%20Stefan%20Leutenegger%20and%20Marco%20Hutter%20and%20Quentin%20Leboutet&entry.1292438233=%20%20Current%20visual%20navigation%20systems%20often%20treat%20the%20environment%20as%20static%2C%0Alacking%20the%20ability%20to%20adaptively%20interact%20with%20obstacles.%20This%20limitation%0Aleads%20to%20navigation%20failure%20when%20encountering%20unavoidable%20obstructions.%20In%0Aresponse%2C%20we%20introduce%20IN-Sight%2C%20a%20novel%20approach%20to%20self-supervised%20path%0Aplanning%2C%20enabling%20more%20effective%20navigation%20strategies%20through%20interaction%0Awith%20obstacles.%20Utilizing%20RGB-D%20observations%2C%20IN-Sight%20calculates%0Atraversability%20scores%20and%20incorporates%20them%20into%20a%20semantic%20map%2C%20facilitating%0Along-range%20path%20planning%20in%20complex%2C%20maze-like%20environments.%20To%20precisely%0Anavigate%20around%20obstacles%2C%20IN-Sight%20employs%20a%20local%20planner%2C%20trained%0Aimperatively%20on%20a%20differentiable%20costmap%20using%20representation%20learning%0Atechniques.%20The%20entire%20framework%20undergoes%20end-to-end%20training%20within%20the%0Astate-of-the-art%20photorealistic%20Intel%20SPEAR%20Simulator.%20We%20validate%20the%0Aeffectiveness%20of%20IN-Sight%20through%20extensive%20benchmarking%20in%20a%20variety%20of%0Asimulated%20scenarios%20and%20ablation%20studies.%20Moreover%2C%20we%20demonstrate%20the%20system%27s%0Areal-world%20applicability%20with%20zero-shot%20sim-to-real%20transfer%2C%20deploying%20our%0Aplanner%20on%20the%20legged%20robot%20platform%20ANYmal%2C%20showcasing%20its%20practical%20potential%0Afor%20interactive%20navigation%20in%20real%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00343v2&entry.124074799=Read"},
{"title": "Long-Form Answers to Visual Questions from Blind and Low Vision People", "author": "Mina Huh and Fangyuan Xu and Yi-Hao Peng and Chongyan Chen and Hansika Murugu and Danna Gurari and Eunsol Choi and Amy Pavel", "abstract": "  Vision language models can now generate long-form answers to questions about\nimages - long-form visual question answers (LFVQA). We contribute VizWiz-LF, a\ndataset of long-form answers to visual questions posed by blind and low vision\n(BLV) users. VizWiz-LF contains 4.2k long-form answers to 600 visual questions,\ncollected from human expert describers and six VQA models. We develop and\nannotate functional roles of sentences of LFVQA and demonstrate that long-form\nanswers contain information beyond the question answer such as explanations and\nsuggestions. We further conduct automatic and human evaluations with BLV and\nsighted people to evaluate long-form answers. BLV people perceive both\nhuman-written and generated long-form answers to be plausible, but generated\nanswers often hallucinate incorrect visual details, especially for unanswerable\nvisual questions (e.g., blurry or irrelevant images). To reduce hallucinations,\nwe evaluate the ability of VQA models to abstain from answering unanswerable\nquestions across multiple prompting strategies.\n", "link": "http://arxiv.org/abs/2408.06303v1", "date": "2024-08-12", "relevancy": 1.8738, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4773}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4629}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Long-Form%20Answers%20to%20Visual%20Questions%20from%20Blind%20and%20Low%20Vision%20People&body=Title%3A%20Long-Form%20Answers%20to%20Visual%20Questions%20from%20Blind%20and%20Low%20Vision%20People%0AAuthor%3A%20Mina%20Huh%20and%20Fangyuan%20Xu%20and%20Yi-Hao%20Peng%20and%20Chongyan%20Chen%20and%20Hansika%20Murugu%20and%20Danna%20Gurari%20and%20Eunsol%20Choi%20and%20Amy%20Pavel%0AAbstract%3A%20%20%20Vision%20language%20models%20can%20now%20generate%20long-form%20answers%20to%20questions%20about%0Aimages%20-%20long-form%20visual%20question%20answers%20%28LFVQA%29.%20We%20contribute%20VizWiz-LF%2C%20a%0Adataset%20of%20long-form%20answers%20to%20visual%20questions%20posed%20by%20blind%20and%20low%20vision%0A%28BLV%29%20users.%20VizWiz-LF%20contains%204.2k%20long-form%20answers%20to%20600%20visual%20questions%2C%0Acollected%20from%20human%20expert%20describers%20and%20six%20VQA%20models.%20We%20develop%20and%0Aannotate%20functional%20roles%20of%20sentences%20of%20LFVQA%20and%20demonstrate%20that%20long-form%0Aanswers%20contain%20information%20beyond%20the%20question%20answer%20such%20as%20explanations%20and%0Asuggestions.%20We%20further%20conduct%20automatic%20and%20human%20evaluations%20with%20BLV%20and%0Asighted%20people%20to%20evaluate%20long-form%20answers.%20BLV%20people%20perceive%20both%0Ahuman-written%20and%20generated%20long-form%20answers%20to%20be%20plausible%2C%20but%20generated%0Aanswers%20often%20hallucinate%20incorrect%20visual%20details%2C%20especially%20for%20unanswerable%0Avisual%20questions%20%28e.g.%2C%20blurry%20or%20irrelevant%20images%29.%20To%20reduce%20hallucinations%2C%0Awe%20evaluate%20the%20ability%20of%20VQA%20models%20to%20abstain%20from%20answering%20unanswerable%0Aquestions%20across%20multiple%20prompting%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06303v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLong-Form%2520Answers%2520to%2520Visual%2520Questions%2520from%2520Blind%2520and%2520Low%2520Vision%2520People%26entry.906535625%3DMina%2520Huh%2520and%2520Fangyuan%2520Xu%2520and%2520Yi-Hao%2520Peng%2520and%2520Chongyan%2520Chen%2520and%2520Hansika%2520Murugu%2520and%2520Danna%2520Gurari%2520and%2520Eunsol%2520Choi%2520and%2520Amy%2520Pavel%26entry.1292438233%3D%2520%2520Vision%2520language%2520models%2520can%2520now%2520generate%2520long-form%2520answers%2520to%2520questions%2520about%250Aimages%2520-%2520long-form%2520visual%2520question%2520answers%2520%2528LFVQA%2529.%2520We%2520contribute%2520VizWiz-LF%252C%2520a%250Adataset%2520of%2520long-form%2520answers%2520to%2520visual%2520questions%2520posed%2520by%2520blind%2520and%2520low%2520vision%250A%2528BLV%2529%2520users.%2520VizWiz-LF%2520contains%25204.2k%2520long-form%2520answers%2520to%2520600%2520visual%2520questions%252C%250Acollected%2520from%2520human%2520expert%2520describers%2520and%2520six%2520VQA%2520models.%2520We%2520develop%2520and%250Aannotate%2520functional%2520roles%2520of%2520sentences%2520of%2520LFVQA%2520and%2520demonstrate%2520that%2520long-form%250Aanswers%2520contain%2520information%2520beyond%2520the%2520question%2520answer%2520such%2520as%2520explanations%2520and%250Asuggestions.%2520We%2520further%2520conduct%2520automatic%2520and%2520human%2520evaluations%2520with%2520BLV%2520and%250Asighted%2520people%2520to%2520evaluate%2520long-form%2520answers.%2520BLV%2520people%2520perceive%2520both%250Ahuman-written%2520and%2520generated%2520long-form%2520answers%2520to%2520be%2520plausible%252C%2520but%2520generated%250Aanswers%2520often%2520hallucinate%2520incorrect%2520visual%2520details%252C%2520especially%2520for%2520unanswerable%250Avisual%2520questions%2520%2528e.g.%252C%2520blurry%2520or%2520irrelevant%2520images%2529.%2520To%2520reduce%2520hallucinations%252C%250Awe%2520evaluate%2520the%2520ability%2520of%2520VQA%2520models%2520to%2520abstain%2520from%2520answering%2520unanswerable%250Aquestions%2520across%2520multiple%2520prompting%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06303v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long-Form%20Answers%20to%20Visual%20Questions%20from%20Blind%20and%20Low%20Vision%20People&entry.906535625=Mina%20Huh%20and%20Fangyuan%20Xu%20and%20Yi-Hao%20Peng%20and%20Chongyan%20Chen%20and%20Hansika%20Murugu%20and%20Danna%20Gurari%20and%20Eunsol%20Choi%20and%20Amy%20Pavel&entry.1292438233=%20%20Vision%20language%20models%20can%20now%20generate%20long-form%20answers%20to%20questions%20about%0Aimages%20-%20long-form%20visual%20question%20answers%20%28LFVQA%29.%20We%20contribute%20VizWiz-LF%2C%20a%0Adataset%20of%20long-form%20answers%20to%20visual%20questions%20posed%20by%20blind%20and%20low%20vision%0A%28BLV%29%20users.%20VizWiz-LF%20contains%204.2k%20long-form%20answers%20to%20600%20visual%20questions%2C%0Acollected%20from%20human%20expert%20describers%20and%20six%20VQA%20models.%20We%20develop%20and%0Aannotate%20functional%20roles%20of%20sentences%20of%20LFVQA%20and%20demonstrate%20that%20long-form%0Aanswers%20contain%20information%20beyond%20the%20question%20answer%20such%20as%20explanations%20and%0Asuggestions.%20We%20further%20conduct%20automatic%20and%20human%20evaluations%20with%20BLV%20and%0Asighted%20people%20to%20evaluate%20long-form%20answers.%20BLV%20people%20perceive%20both%0Ahuman-written%20and%20generated%20long-form%20answers%20to%20be%20plausible%2C%20but%20generated%0Aanswers%20often%20hallucinate%20incorrect%20visual%20details%2C%20especially%20for%20unanswerable%0Avisual%20questions%20%28e.g.%2C%20blurry%20or%20irrelevant%20images%29.%20To%20reduce%20hallucinations%2C%0Awe%20evaluate%20the%20ability%20of%20VQA%20models%20to%20abstain%20from%20answering%20unanswerable%0Aquestions%20across%20multiple%20prompting%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06303v1&entry.124074799=Read"},
{"title": "A Comprehensive Case Study on the Performance of Machine Learning\n  Methods on the Classification of Solar Panel Electroluminescence Images", "author": "Xinyi Song and Kennedy Odongo and Francis G. Pascual and Yili Hong", "abstract": "  Photovoltaics (PV) are widely used to harvest solar energy, an important form\nof renewable energy. Photovoltaic arrays consist of multiple solar panels\nconstructed from solar cells. Solar cells in the field are vulnerable to\nvarious defects, and electroluminescence (EL) imaging provides effective and\nnon-destructive diagnostics to detect those defects. We use multiple\ntraditional machine learning and modern deep learning models to classify EL\nsolar cell images into different functional/defective categories. Because of\nthe asymmetry in the number of functional vs. defective cells, an imbalanced\nlabel problem arises in the EL image data. The current literature lacks\ninsights on which methods and metrics to use for model training and prediction.\nIn this paper, we comprehensively compare different machine learning and deep\nlearning methods under different performance metrics on the classification of\nsolar cell EL images from monocrystalline and polycrystalline modules. We\nprovide a comprehensive discussion on different metrics. Our results provide\ninsights and guidelines for practitioners in selecting prediction methods and\nperformance metrics.\n", "link": "http://arxiv.org/abs/2408.06229v1", "date": "2024-08-12", "relevancy": 1.8731, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4914}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4551}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Case%20Study%20on%20the%20Performance%20of%20Machine%20Learning%0A%20%20Methods%20on%20the%20Classification%20of%20Solar%20Panel%20Electroluminescence%20Images&body=Title%3A%20A%20Comprehensive%20Case%20Study%20on%20the%20Performance%20of%20Machine%20Learning%0A%20%20Methods%20on%20the%20Classification%20of%20Solar%20Panel%20Electroluminescence%20Images%0AAuthor%3A%20Xinyi%20Song%20and%20Kennedy%20Odongo%20and%20Francis%20G.%20Pascual%20and%20Yili%20Hong%0AAbstract%3A%20%20%20Photovoltaics%20%28PV%29%20are%20widely%20used%20to%20harvest%20solar%20energy%2C%20an%20important%20form%0Aof%20renewable%20energy.%20Photovoltaic%20arrays%20consist%20of%20multiple%20solar%20panels%0Aconstructed%20from%20solar%20cells.%20Solar%20cells%20in%20the%20field%20are%20vulnerable%20to%0Avarious%20defects%2C%20and%20electroluminescence%20%28EL%29%20imaging%20provides%20effective%20and%0Anon-destructive%20diagnostics%20to%20detect%20those%20defects.%20We%20use%20multiple%0Atraditional%20machine%20learning%20and%20modern%20deep%20learning%20models%20to%20classify%20EL%0Asolar%20cell%20images%20into%20different%20functional/defective%20categories.%20Because%20of%0Athe%20asymmetry%20in%20the%20number%20of%20functional%20vs.%20defective%20cells%2C%20an%20imbalanced%0Alabel%20problem%20arises%20in%20the%20EL%20image%20data.%20The%20current%20literature%20lacks%0Ainsights%20on%20which%20methods%20and%20metrics%20to%20use%20for%20model%20training%20and%20prediction.%0AIn%20this%20paper%2C%20we%20comprehensively%20compare%20different%20machine%20learning%20and%20deep%0Alearning%20methods%20under%20different%20performance%20metrics%20on%20the%20classification%20of%0Asolar%20cell%20EL%20images%20from%20monocrystalline%20and%20polycrystalline%20modules.%20We%0Aprovide%20a%20comprehensive%20discussion%20on%20different%20metrics.%20Our%20results%20provide%0Ainsights%20and%20guidelines%20for%20practitioners%20in%20selecting%20prediction%20methods%20and%0Aperformance%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06229v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comprehensive%2520Case%2520Study%2520on%2520the%2520Performance%2520of%2520Machine%2520Learning%250A%2520%2520Methods%2520on%2520the%2520Classification%2520of%2520Solar%2520Panel%2520Electroluminescence%2520Images%26entry.906535625%3DXinyi%2520Song%2520and%2520Kennedy%2520Odongo%2520and%2520Francis%2520G.%2520Pascual%2520and%2520Yili%2520Hong%26entry.1292438233%3D%2520%2520Photovoltaics%2520%2528PV%2529%2520are%2520widely%2520used%2520to%2520harvest%2520solar%2520energy%252C%2520an%2520important%2520form%250Aof%2520renewable%2520energy.%2520Photovoltaic%2520arrays%2520consist%2520of%2520multiple%2520solar%2520panels%250Aconstructed%2520from%2520solar%2520cells.%2520Solar%2520cells%2520in%2520the%2520field%2520are%2520vulnerable%2520to%250Avarious%2520defects%252C%2520and%2520electroluminescence%2520%2528EL%2529%2520imaging%2520provides%2520effective%2520and%250Anon-destructive%2520diagnostics%2520to%2520detect%2520those%2520defects.%2520We%2520use%2520multiple%250Atraditional%2520machine%2520learning%2520and%2520modern%2520deep%2520learning%2520models%2520to%2520classify%2520EL%250Asolar%2520cell%2520images%2520into%2520different%2520functional/defective%2520categories.%2520Because%2520of%250Athe%2520asymmetry%2520in%2520the%2520number%2520of%2520functional%2520vs.%2520defective%2520cells%252C%2520an%2520imbalanced%250Alabel%2520problem%2520arises%2520in%2520the%2520EL%2520image%2520data.%2520The%2520current%2520literature%2520lacks%250Ainsights%2520on%2520which%2520methods%2520and%2520metrics%2520to%2520use%2520for%2520model%2520training%2520and%2520prediction.%250AIn%2520this%2520paper%252C%2520we%2520comprehensively%2520compare%2520different%2520machine%2520learning%2520and%2520deep%250Alearning%2520methods%2520under%2520different%2520performance%2520metrics%2520on%2520the%2520classification%2520of%250Asolar%2520cell%2520EL%2520images%2520from%2520monocrystalline%2520and%2520polycrystalline%2520modules.%2520We%250Aprovide%2520a%2520comprehensive%2520discussion%2520on%2520different%2520metrics.%2520Our%2520results%2520provide%250Ainsights%2520and%2520guidelines%2520for%2520practitioners%2520in%2520selecting%2520prediction%2520methods%2520and%250Aperformance%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06229v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Case%20Study%20on%20the%20Performance%20of%20Machine%20Learning%0A%20%20Methods%20on%20the%20Classification%20of%20Solar%20Panel%20Electroluminescence%20Images&entry.906535625=Xinyi%20Song%20and%20Kennedy%20Odongo%20and%20Francis%20G.%20Pascual%20and%20Yili%20Hong&entry.1292438233=%20%20Photovoltaics%20%28PV%29%20are%20widely%20used%20to%20harvest%20solar%20energy%2C%20an%20important%20form%0Aof%20renewable%20energy.%20Photovoltaic%20arrays%20consist%20of%20multiple%20solar%20panels%0Aconstructed%20from%20solar%20cells.%20Solar%20cells%20in%20the%20field%20are%20vulnerable%20to%0Avarious%20defects%2C%20and%20electroluminescence%20%28EL%29%20imaging%20provides%20effective%20and%0Anon-destructive%20diagnostics%20to%20detect%20those%20defects.%20We%20use%20multiple%0Atraditional%20machine%20learning%20and%20modern%20deep%20learning%20models%20to%20classify%20EL%0Asolar%20cell%20images%20into%20different%20functional/defective%20categories.%20Because%20of%0Athe%20asymmetry%20in%20the%20number%20of%20functional%20vs.%20defective%20cells%2C%20an%20imbalanced%0Alabel%20problem%20arises%20in%20the%20EL%20image%20data.%20The%20current%20literature%20lacks%0Ainsights%20on%20which%20methods%20and%20metrics%20to%20use%20for%20model%20training%20and%20prediction.%0AIn%20this%20paper%2C%20we%20comprehensively%20compare%20different%20machine%20learning%20and%20deep%0Alearning%20methods%20under%20different%20performance%20metrics%20on%20the%20classification%20of%0Asolar%20cell%20EL%20images%20from%20monocrystalline%20and%20polycrystalline%20modules.%20We%0Aprovide%20a%20comprehensive%20discussion%20on%20different%20metrics.%20Our%20results%20provide%0Ainsights%20and%20guidelines%20for%20practitioners%20in%20selecting%20prediction%20methods%20and%0Aperformance%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06229v1&entry.124074799=Read"},
{"title": "Helios: An extremely low power event-based gesture recognition for\n  always-on smart eyewear", "author": "Prarthana Bhattacharyya and Joshua Mitton and Ryan Page and Owen Morgan and Ben Menzies and Gabriel Homewood and Kemi Jacobs and Paolo Baesso and Dave Trickett and Chris Mair and Taru Muhonen and Rory Clark and Louis Berridge and Richard Vigars and Iain Wallace", "abstract": "  This paper introduces Helios, the first extremely low-power, real-time,\nevent-based hand gesture recognition system designed for all-day on smart\neyewear. As augmented reality (AR) evolves, current smart glasses like the Meta\nRay-Bans prioritize visual and wearable comfort at the expense of\nfunctionality. Existing human-machine interfaces (HMIs) in these devices, such\nas capacitive touch and voice controls, present limitations in ergonomics,\nprivacy and power consumption. Helios addresses these challenges by leveraging\nnatural hand interactions for a more intuitive and comfortable user experience.\nOur system utilizes a extremely low-power and compact 3mmx4mm/20mW event camera\nto perform natural hand-based gesture recognition for always-on smart eyewear.\nThe camera's output is processed by a convolutional neural network (CNN)\nrunning on a NXP Nano UltraLite compute platform, consuming less than 350mW.\nHelios can recognize seven classes of gestures, including subtle microgestures\nlike swipes and pinches, with 91% accuracy. We also demonstrate real-time\nperformance across 20 users at a remarkably low latency of 60ms. Our user\ntesting results align with the positive feedback we received during our recent\nsuccessful demo at AWE-USA-2024.\n", "link": "http://arxiv.org/abs/2407.05206v3", "date": "2024-08-12", "relevancy": 1.873, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4948}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4502}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Helios%3A%20An%20extremely%20low%20power%20event-based%20gesture%20recognition%20for%0A%20%20always-on%20smart%20eyewear&body=Title%3A%20Helios%3A%20An%20extremely%20low%20power%20event-based%20gesture%20recognition%20for%0A%20%20always-on%20smart%20eyewear%0AAuthor%3A%20Prarthana%20Bhattacharyya%20and%20Joshua%20Mitton%20and%20Ryan%20Page%20and%20Owen%20Morgan%20and%20Ben%20Menzies%20and%20Gabriel%20Homewood%20and%20Kemi%20Jacobs%20and%20Paolo%20Baesso%20and%20Dave%20Trickett%20and%20Chris%20Mair%20and%20Taru%20Muhonen%20and%20Rory%20Clark%20and%20Louis%20Berridge%20and%20Richard%20Vigars%20and%20Iain%20Wallace%0AAbstract%3A%20%20%20This%20paper%20introduces%20Helios%2C%20the%20first%20extremely%20low-power%2C%20real-time%2C%0Aevent-based%20hand%20gesture%20recognition%20system%20designed%20for%20all-day%20on%20smart%0Aeyewear.%20As%20augmented%20reality%20%28AR%29%20evolves%2C%20current%20smart%20glasses%20like%20the%20Meta%0ARay-Bans%20prioritize%20visual%20and%20wearable%20comfort%20at%20the%20expense%20of%0Afunctionality.%20Existing%20human-machine%20interfaces%20%28HMIs%29%20in%20these%20devices%2C%20such%0Aas%20capacitive%20touch%20and%20voice%20controls%2C%20present%20limitations%20in%20ergonomics%2C%0Aprivacy%20and%20power%20consumption.%20Helios%20addresses%20these%20challenges%20by%20leveraging%0Anatural%20hand%20interactions%20for%20a%20more%20intuitive%20and%20comfortable%20user%20experience.%0AOur%20system%20utilizes%20a%20extremely%20low-power%20and%20compact%203mmx4mm/20mW%20event%20camera%0Ato%20perform%20natural%20hand-based%20gesture%20recognition%20for%20always-on%20smart%20eyewear.%0AThe%20camera%27s%20output%20is%20processed%20by%20a%20convolutional%20neural%20network%20%28CNN%29%0Arunning%20on%20a%20NXP%20Nano%20UltraLite%20compute%20platform%2C%20consuming%20less%20than%20350mW.%0AHelios%20can%20recognize%20seven%20classes%20of%20gestures%2C%20including%20subtle%20microgestures%0Alike%20swipes%20and%20pinches%2C%20with%2091%25%20accuracy.%20We%20also%20demonstrate%20real-time%0Aperformance%20across%2020%20users%20at%20a%20remarkably%20low%20latency%20of%2060ms.%20Our%20user%0Atesting%20results%20align%20with%20the%20positive%20feedback%20we%20received%20during%20our%20recent%0Asuccessful%20demo%20at%20AWE-USA-2024.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05206v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHelios%253A%2520An%2520extremely%2520low%2520power%2520event-based%2520gesture%2520recognition%2520for%250A%2520%2520always-on%2520smart%2520eyewear%26entry.906535625%3DPrarthana%2520Bhattacharyya%2520and%2520Joshua%2520Mitton%2520and%2520Ryan%2520Page%2520and%2520Owen%2520Morgan%2520and%2520Ben%2520Menzies%2520and%2520Gabriel%2520Homewood%2520and%2520Kemi%2520Jacobs%2520and%2520Paolo%2520Baesso%2520and%2520Dave%2520Trickett%2520and%2520Chris%2520Mair%2520and%2520Taru%2520Muhonen%2520and%2520Rory%2520Clark%2520and%2520Louis%2520Berridge%2520and%2520Richard%2520Vigars%2520and%2520Iain%2520Wallace%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520Helios%252C%2520the%2520first%2520extremely%2520low-power%252C%2520real-time%252C%250Aevent-based%2520hand%2520gesture%2520recognition%2520system%2520designed%2520for%2520all-day%2520on%2520smart%250Aeyewear.%2520As%2520augmented%2520reality%2520%2528AR%2529%2520evolves%252C%2520current%2520smart%2520glasses%2520like%2520the%2520Meta%250ARay-Bans%2520prioritize%2520visual%2520and%2520wearable%2520comfort%2520at%2520the%2520expense%2520of%250Afunctionality.%2520Existing%2520human-machine%2520interfaces%2520%2528HMIs%2529%2520in%2520these%2520devices%252C%2520such%250Aas%2520capacitive%2520touch%2520and%2520voice%2520controls%252C%2520present%2520limitations%2520in%2520ergonomics%252C%250Aprivacy%2520and%2520power%2520consumption.%2520Helios%2520addresses%2520these%2520challenges%2520by%2520leveraging%250Anatural%2520hand%2520interactions%2520for%2520a%2520more%2520intuitive%2520and%2520comfortable%2520user%2520experience.%250AOur%2520system%2520utilizes%2520a%2520extremely%2520low-power%2520and%2520compact%25203mmx4mm/20mW%2520event%2520camera%250Ato%2520perform%2520natural%2520hand-based%2520gesture%2520recognition%2520for%2520always-on%2520smart%2520eyewear.%250AThe%2520camera%2527s%2520output%2520is%2520processed%2520by%2520a%2520convolutional%2520neural%2520network%2520%2528CNN%2529%250Arunning%2520on%2520a%2520NXP%2520Nano%2520UltraLite%2520compute%2520platform%252C%2520consuming%2520less%2520than%2520350mW.%250AHelios%2520can%2520recognize%2520seven%2520classes%2520of%2520gestures%252C%2520including%2520subtle%2520microgestures%250Alike%2520swipes%2520and%2520pinches%252C%2520with%252091%2525%2520accuracy.%2520We%2520also%2520demonstrate%2520real-time%250Aperformance%2520across%252020%2520users%2520at%2520a%2520remarkably%2520low%2520latency%2520of%252060ms.%2520Our%2520user%250Atesting%2520results%2520align%2520with%2520the%2520positive%2520feedback%2520we%2520received%2520during%2520our%2520recent%250Asuccessful%2520demo%2520at%2520AWE-USA-2024.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05206v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Helios%3A%20An%20extremely%20low%20power%20event-based%20gesture%20recognition%20for%0A%20%20always-on%20smart%20eyewear&entry.906535625=Prarthana%20Bhattacharyya%20and%20Joshua%20Mitton%20and%20Ryan%20Page%20and%20Owen%20Morgan%20and%20Ben%20Menzies%20and%20Gabriel%20Homewood%20and%20Kemi%20Jacobs%20and%20Paolo%20Baesso%20and%20Dave%20Trickett%20and%20Chris%20Mair%20and%20Taru%20Muhonen%20and%20Rory%20Clark%20and%20Louis%20Berridge%20and%20Richard%20Vigars%20and%20Iain%20Wallace&entry.1292438233=%20%20This%20paper%20introduces%20Helios%2C%20the%20first%20extremely%20low-power%2C%20real-time%2C%0Aevent-based%20hand%20gesture%20recognition%20system%20designed%20for%20all-day%20on%20smart%0Aeyewear.%20As%20augmented%20reality%20%28AR%29%20evolves%2C%20current%20smart%20glasses%20like%20the%20Meta%0ARay-Bans%20prioritize%20visual%20and%20wearable%20comfort%20at%20the%20expense%20of%0Afunctionality.%20Existing%20human-machine%20interfaces%20%28HMIs%29%20in%20these%20devices%2C%20such%0Aas%20capacitive%20touch%20and%20voice%20controls%2C%20present%20limitations%20in%20ergonomics%2C%0Aprivacy%20and%20power%20consumption.%20Helios%20addresses%20these%20challenges%20by%20leveraging%0Anatural%20hand%20interactions%20for%20a%20more%20intuitive%20and%20comfortable%20user%20experience.%0AOur%20system%20utilizes%20a%20extremely%20low-power%20and%20compact%203mmx4mm/20mW%20event%20camera%0Ato%20perform%20natural%20hand-based%20gesture%20recognition%20for%20always-on%20smart%20eyewear.%0AThe%20camera%27s%20output%20is%20processed%20by%20a%20convolutional%20neural%20network%20%28CNN%29%0Arunning%20on%20a%20NXP%20Nano%20UltraLite%20compute%20platform%2C%20consuming%20less%20than%20350mW.%0AHelios%20can%20recognize%20seven%20classes%20of%20gestures%2C%20including%20subtle%20microgestures%0Alike%20swipes%20and%20pinches%2C%20with%2091%25%20accuracy.%20We%20also%20demonstrate%20real-time%0Aperformance%20across%2020%20users%20at%20a%20remarkably%20low%20latency%20of%2060ms.%20Our%20user%0Atesting%20results%20align%20with%20the%20positive%20feedback%20we%20received%20during%20our%20recent%0Asuccessful%20demo%20at%20AWE-USA-2024.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05206v3&entry.124074799=Read"},
{"title": "Spacetime $E(n)$-Transformer: Equivariant Attention for Spatio-temporal\n  Graphs", "author": "Sergio G. Charles", "abstract": "  We introduce an $E(n)$-equivariant Transformer architecture for\nspatio-temporal graph data. By imposing rotation, translation, and permutation\nequivariance inductive biases in both space and time, we show that the\nSpacetime $E(n)$-Transformer (SET) outperforms purely spatial and temporal\nmodels without symmetry-preserving properties. We benchmark SET against said\nmodels on the charged $N$-body problem, a simple physical system with complex\ndynamics. While existing spatio-temporal graph neural networks focus on\nsequential modeling, we empirically demonstrate that leveraging underlying\ndomain symmetries yields considerable improvements for modeling dynamical\nsystems on graphs.\n", "link": "http://arxiv.org/abs/2408.06039v1", "date": "2024-08-12", "relevancy": 1.8571, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5334}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4566}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4443}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spacetime%20%24E%28n%29%24-Transformer%3A%20Equivariant%20Attention%20for%20Spatio-temporal%0A%20%20Graphs&body=Title%3A%20Spacetime%20%24E%28n%29%24-Transformer%3A%20Equivariant%20Attention%20for%20Spatio-temporal%0A%20%20Graphs%0AAuthor%3A%20Sergio%20G.%20Charles%0AAbstract%3A%20%20%20We%20introduce%20an%20%24E%28n%29%24-equivariant%20Transformer%20architecture%20for%0Aspatio-temporal%20graph%20data.%20By%20imposing%20rotation%2C%20translation%2C%20and%20permutation%0Aequivariance%20inductive%20biases%20in%20both%20space%20and%20time%2C%20we%20show%20that%20the%0ASpacetime%20%24E%28n%29%24-Transformer%20%28SET%29%20outperforms%20purely%20spatial%20and%20temporal%0Amodels%20without%20symmetry-preserving%20properties.%20We%20benchmark%20SET%20against%20said%0Amodels%20on%20the%20charged%20%24N%24-body%20problem%2C%20a%20simple%20physical%20system%20with%20complex%0Adynamics.%20While%20existing%20spatio-temporal%20graph%20neural%20networks%20focus%20on%0Asequential%20modeling%2C%20we%20empirically%20demonstrate%20that%20leveraging%20underlying%0Adomain%20symmetries%20yields%20considerable%20improvements%20for%20modeling%20dynamical%0Asystems%20on%20graphs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06039v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpacetime%2520%2524E%2528n%2529%2524-Transformer%253A%2520Equivariant%2520Attention%2520for%2520Spatio-temporal%250A%2520%2520Graphs%26entry.906535625%3DSergio%2520G.%2520Charles%26entry.1292438233%3D%2520%2520We%2520introduce%2520an%2520%2524E%2528n%2529%2524-equivariant%2520Transformer%2520architecture%2520for%250Aspatio-temporal%2520graph%2520data.%2520By%2520imposing%2520rotation%252C%2520translation%252C%2520and%2520permutation%250Aequivariance%2520inductive%2520biases%2520in%2520both%2520space%2520and%2520time%252C%2520we%2520show%2520that%2520the%250ASpacetime%2520%2524E%2528n%2529%2524-Transformer%2520%2528SET%2529%2520outperforms%2520purely%2520spatial%2520and%2520temporal%250Amodels%2520without%2520symmetry-preserving%2520properties.%2520We%2520benchmark%2520SET%2520against%2520said%250Amodels%2520on%2520the%2520charged%2520%2524N%2524-body%2520problem%252C%2520a%2520simple%2520physical%2520system%2520with%2520complex%250Adynamics.%2520While%2520existing%2520spatio-temporal%2520graph%2520neural%2520networks%2520focus%2520on%250Asequential%2520modeling%252C%2520we%2520empirically%2520demonstrate%2520that%2520leveraging%2520underlying%250Adomain%2520symmetries%2520yields%2520considerable%2520improvements%2520for%2520modeling%2520dynamical%250Asystems%2520on%2520graphs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06039v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spacetime%20%24E%28n%29%24-Transformer%3A%20Equivariant%20Attention%20for%20Spatio-temporal%0A%20%20Graphs&entry.906535625=Sergio%20G.%20Charles&entry.1292438233=%20%20We%20introduce%20an%20%24E%28n%29%24-equivariant%20Transformer%20architecture%20for%0Aspatio-temporal%20graph%20data.%20By%20imposing%20rotation%2C%20translation%2C%20and%20permutation%0Aequivariance%20inductive%20biases%20in%20both%20space%20and%20time%2C%20we%20show%20that%20the%0ASpacetime%20%24E%28n%29%24-Transformer%20%28SET%29%20outperforms%20purely%20spatial%20and%20temporal%0Amodels%20without%20symmetry-preserving%20properties.%20We%20benchmark%20SET%20against%20said%0Amodels%20on%20the%20charged%20%24N%24-body%20problem%2C%20a%20simple%20physical%20system%20with%20complex%0Adynamics.%20While%20existing%20spatio-temporal%20graph%20neural%20networks%20focus%20on%0Asequential%20modeling%2C%20we%20empirically%20demonstrate%20that%20leveraging%20underlying%0Adomain%20symmetries%20yields%20considerable%20improvements%20for%20modeling%20dynamical%0Asystems%20on%20graphs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06039v1&entry.124074799=Read"},
{"title": "Reciprocal Learning", "author": "Julian Rodemann and Christoph Jansen and Georg Schollmeyer", "abstract": "  We demonstrate that a wide array of machine learning algorithms are specific\ninstances of one single paradigm: reciprocal learning. These instances range\nfrom active learning over multi-armed bandits to self-training. We show that\nall these algorithms do not only learn parameters from data but also vice\nversa: They iteratively alter training data in a way that depends on the\ncurrent model fit. We introduce reciprocal learning as a generalization of\nthese algorithms using the language of decision theory. This allows us to study\nunder what conditions they converge. The key is to guarantee that reciprocal\nlearning contracts such that the Banach fixed-point theorem applies. In this\nway, we find that reciprocal learning algorithms converge at linear rates to an\napproximately optimal model under relatively mild assumptions on the loss\nfunction, if their predictions are probabilistic and the sample adaption is\nboth non-greedy and either randomized or regularized. We interpret these\nfindings and provide corollaries that relate them to specific active learning,\nself-training, and bandit algorithms.\n", "link": "http://arxiv.org/abs/2408.06257v1", "date": "2024-08-12", "relevancy": 1.8474, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4655}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4595}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4586}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reciprocal%20Learning&body=Title%3A%20Reciprocal%20Learning%0AAuthor%3A%20Julian%20Rodemann%20and%20Christoph%20Jansen%20and%20Georg%20Schollmeyer%0AAbstract%3A%20%20%20We%20demonstrate%20that%20a%20wide%20array%20of%20machine%20learning%20algorithms%20are%20specific%0Ainstances%20of%20one%20single%20paradigm%3A%20reciprocal%20learning.%20These%20instances%20range%0Afrom%20active%20learning%20over%20multi-armed%20bandits%20to%20self-training.%20We%20show%20that%0Aall%20these%20algorithms%20do%20not%20only%20learn%20parameters%20from%20data%20but%20also%20vice%0Aversa%3A%20They%20iteratively%20alter%20training%20data%20in%20a%20way%20that%20depends%20on%20the%0Acurrent%20model%20fit.%20We%20introduce%20reciprocal%20learning%20as%20a%20generalization%20of%0Athese%20algorithms%20using%20the%20language%20of%20decision%20theory.%20This%20allows%20us%20to%20study%0Aunder%20what%20conditions%20they%20converge.%20The%20key%20is%20to%20guarantee%20that%20reciprocal%0Alearning%20contracts%20such%20that%20the%20Banach%20fixed-point%20theorem%20applies.%20In%20this%0Away%2C%20we%20find%20that%20reciprocal%20learning%20algorithms%20converge%20at%20linear%20rates%20to%20an%0Aapproximately%20optimal%20model%20under%20relatively%20mild%20assumptions%20on%20the%20loss%0Afunction%2C%20if%20their%20predictions%20are%20probabilistic%20and%20the%20sample%20adaption%20is%0Aboth%20non-greedy%20and%20either%20randomized%20or%20regularized.%20We%20interpret%20these%0Afindings%20and%20provide%20corollaries%20that%20relate%20them%20to%20specific%20active%20learning%2C%0Aself-training%2C%20and%20bandit%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06257v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReciprocal%2520Learning%26entry.906535625%3DJulian%2520Rodemann%2520and%2520Christoph%2520Jansen%2520and%2520Georg%2520Schollmeyer%26entry.1292438233%3D%2520%2520We%2520demonstrate%2520that%2520a%2520wide%2520array%2520of%2520machine%2520learning%2520algorithms%2520are%2520specific%250Ainstances%2520of%2520one%2520single%2520paradigm%253A%2520reciprocal%2520learning.%2520These%2520instances%2520range%250Afrom%2520active%2520learning%2520over%2520multi-armed%2520bandits%2520to%2520self-training.%2520We%2520show%2520that%250Aall%2520these%2520algorithms%2520do%2520not%2520only%2520learn%2520parameters%2520from%2520data%2520but%2520also%2520vice%250Aversa%253A%2520They%2520iteratively%2520alter%2520training%2520data%2520in%2520a%2520way%2520that%2520depends%2520on%2520the%250Acurrent%2520model%2520fit.%2520We%2520introduce%2520reciprocal%2520learning%2520as%2520a%2520generalization%2520of%250Athese%2520algorithms%2520using%2520the%2520language%2520of%2520decision%2520theory.%2520This%2520allows%2520us%2520to%2520study%250Aunder%2520what%2520conditions%2520they%2520converge.%2520The%2520key%2520is%2520to%2520guarantee%2520that%2520reciprocal%250Alearning%2520contracts%2520such%2520that%2520the%2520Banach%2520fixed-point%2520theorem%2520applies.%2520In%2520this%250Away%252C%2520we%2520find%2520that%2520reciprocal%2520learning%2520algorithms%2520converge%2520at%2520linear%2520rates%2520to%2520an%250Aapproximately%2520optimal%2520model%2520under%2520relatively%2520mild%2520assumptions%2520on%2520the%2520loss%250Afunction%252C%2520if%2520their%2520predictions%2520are%2520probabilistic%2520and%2520the%2520sample%2520adaption%2520is%250Aboth%2520non-greedy%2520and%2520either%2520randomized%2520or%2520regularized.%2520We%2520interpret%2520these%250Afindings%2520and%2520provide%2520corollaries%2520that%2520relate%2520them%2520to%2520specific%2520active%2520learning%252C%250Aself-training%252C%2520and%2520bandit%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06257v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reciprocal%20Learning&entry.906535625=Julian%20Rodemann%20and%20Christoph%20Jansen%20and%20Georg%20Schollmeyer&entry.1292438233=%20%20We%20demonstrate%20that%20a%20wide%20array%20of%20machine%20learning%20algorithms%20are%20specific%0Ainstances%20of%20one%20single%20paradigm%3A%20reciprocal%20learning.%20These%20instances%20range%0Afrom%20active%20learning%20over%20multi-armed%20bandits%20to%20self-training.%20We%20show%20that%0Aall%20these%20algorithms%20do%20not%20only%20learn%20parameters%20from%20data%20but%20also%20vice%0Aversa%3A%20They%20iteratively%20alter%20training%20data%20in%20a%20way%20that%20depends%20on%20the%0Acurrent%20model%20fit.%20We%20introduce%20reciprocal%20learning%20as%20a%20generalization%20of%0Athese%20algorithms%20using%20the%20language%20of%20decision%20theory.%20This%20allows%20us%20to%20study%0Aunder%20what%20conditions%20they%20converge.%20The%20key%20is%20to%20guarantee%20that%20reciprocal%0Alearning%20contracts%20such%20that%20the%20Banach%20fixed-point%20theorem%20applies.%20In%20this%0Away%2C%20we%20find%20that%20reciprocal%20learning%20algorithms%20converge%20at%20linear%20rates%20to%20an%0Aapproximately%20optimal%20model%20under%20relatively%20mild%20assumptions%20on%20the%20loss%0Afunction%2C%20if%20their%20predictions%20are%20probabilistic%20and%20the%20sample%20adaption%20is%0Aboth%20non-greedy%20and%20either%20randomized%20or%20regularized.%20We%20interpret%20these%0Afindings%20and%20provide%20corollaries%20that%20relate%20them%20to%20specific%20active%20learning%2C%0Aself-training%2C%20and%20bandit%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06257v1&entry.124074799=Read"},
{"title": "Leveraging KANs For Enhanced Deep Koopman Operator Discovery", "author": "George Nehma and Madhur Tiwari", "abstract": "  Multi-layer perceptrons (MLP's) have been extensively utilized in discovering\nDeep Koopman operators for linearizing nonlinear dynamics. With the emergence\nof Kolmogorov-Arnold Networks (KANs) as a more efficient and accurate\nalternative to the MLP Neural Network, we propose a comparison of the\nperformance of each network type in the context of learning Koopman operators\nwith control. In this work, we propose a KANs-based deep Koopman framework with\napplications to an orbital Two-Body Problem (2BP) and the pendulum for\ndata-driven discovery of linear system dynamics. KANs were found to be superior\nin nearly all aspects of training; learning 31 times faster, being 15 times\nmore parameter efficiency, and predicting 1.25 times more accurately as\ncompared to the MLP Deep Neural Networks (DNNs) in the case of the 2BP. Thus,\nKANs shows potential for being an efficient tool in the development of Deep\nKoopman Theory.\n", "link": "http://arxiv.org/abs/2406.02875v3", "date": "2024-08-12", "relevancy": 1.838, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5006}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4625}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4401}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20KANs%20For%20Enhanced%20Deep%20Koopman%20Operator%20Discovery&body=Title%3A%20Leveraging%20KANs%20For%20Enhanced%20Deep%20Koopman%20Operator%20Discovery%0AAuthor%3A%20George%20Nehma%20and%20Madhur%20Tiwari%0AAbstract%3A%20%20%20Multi-layer%20perceptrons%20%28MLP%27s%29%20have%20been%20extensively%20utilized%20in%20discovering%0ADeep%20Koopman%20operators%20for%20linearizing%20nonlinear%20dynamics.%20With%20the%20emergence%0Aof%20Kolmogorov-Arnold%20Networks%20%28KANs%29%20as%20a%20more%20efficient%20and%20accurate%0Aalternative%20to%20the%20MLP%20Neural%20Network%2C%20we%20propose%20a%20comparison%20of%20the%0Aperformance%20of%20each%20network%20type%20in%20the%20context%20of%20learning%20Koopman%20operators%0Awith%20control.%20In%20this%20work%2C%20we%20propose%20a%20KANs-based%20deep%20Koopman%20framework%20with%0Aapplications%20to%20an%20orbital%20Two-Body%20Problem%20%282BP%29%20and%20the%20pendulum%20for%0Adata-driven%20discovery%20of%20linear%20system%20dynamics.%20KANs%20were%20found%20to%20be%20superior%0Ain%20nearly%20all%20aspects%20of%20training%3B%20learning%2031%20times%20faster%2C%20being%2015%20times%0Amore%20parameter%20efficiency%2C%20and%20predicting%201.25%20times%20more%20accurately%20as%0Acompared%20to%20the%20MLP%20Deep%20Neural%20Networks%20%28DNNs%29%20in%20the%20case%20of%20the%202BP.%20Thus%2C%0AKANs%20shows%20potential%20for%20being%20an%20efficient%20tool%20in%20the%20development%20of%20Deep%0AKoopman%20Theory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02875v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520KANs%2520For%2520Enhanced%2520Deep%2520Koopman%2520Operator%2520Discovery%26entry.906535625%3DGeorge%2520Nehma%2520and%2520Madhur%2520Tiwari%26entry.1292438233%3D%2520%2520Multi-layer%2520perceptrons%2520%2528MLP%2527s%2529%2520have%2520been%2520extensively%2520utilized%2520in%2520discovering%250ADeep%2520Koopman%2520operators%2520for%2520linearizing%2520nonlinear%2520dynamics.%2520With%2520the%2520emergence%250Aof%2520Kolmogorov-Arnold%2520Networks%2520%2528KANs%2529%2520as%2520a%2520more%2520efficient%2520and%2520accurate%250Aalternative%2520to%2520the%2520MLP%2520Neural%2520Network%252C%2520we%2520propose%2520a%2520comparison%2520of%2520the%250Aperformance%2520of%2520each%2520network%2520type%2520in%2520the%2520context%2520of%2520learning%2520Koopman%2520operators%250Awith%2520control.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520KANs-based%2520deep%2520Koopman%2520framework%2520with%250Aapplications%2520to%2520an%2520orbital%2520Two-Body%2520Problem%2520%25282BP%2529%2520and%2520the%2520pendulum%2520for%250Adata-driven%2520discovery%2520of%2520linear%2520system%2520dynamics.%2520KANs%2520were%2520found%2520to%2520be%2520superior%250Ain%2520nearly%2520all%2520aspects%2520of%2520training%253B%2520learning%252031%2520times%2520faster%252C%2520being%252015%2520times%250Amore%2520parameter%2520efficiency%252C%2520and%2520predicting%25201.25%2520times%2520more%2520accurately%2520as%250Acompared%2520to%2520the%2520MLP%2520Deep%2520Neural%2520Networks%2520%2528DNNs%2529%2520in%2520the%2520case%2520of%2520the%25202BP.%2520Thus%252C%250AKANs%2520shows%2520potential%2520for%2520being%2520an%2520efficient%2520tool%2520in%2520the%2520development%2520of%2520Deep%250AKoopman%2520Theory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02875v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20KANs%20For%20Enhanced%20Deep%20Koopman%20Operator%20Discovery&entry.906535625=George%20Nehma%20and%20Madhur%20Tiwari&entry.1292438233=%20%20Multi-layer%20perceptrons%20%28MLP%27s%29%20have%20been%20extensively%20utilized%20in%20discovering%0ADeep%20Koopman%20operators%20for%20linearizing%20nonlinear%20dynamics.%20With%20the%20emergence%0Aof%20Kolmogorov-Arnold%20Networks%20%28KANs%29%20as%20a%20more%20efficient%20and%20accurate%0Aalternative%20to%20the%20MLP%20Neural%20Network%2C%20we%20propose%20a%20comparison%20of%20the%0Aperformance%20of%20each%20network%20type%20in%20the%20context%20of%20learning%20Koopman%20operators%0Awith%20control.%20In%20this%20work%2C%20we%20propose%20a%20KANs-based%20deep%20Koopman%20framework%20with%0Aapplications%20to%20an%20orbital%20Two-Body%20Problem%20%282BP%29%20and%20the%20pendulum%20for%0Adata-driven%20discovery%20of%20linear%20system%20dynamics.%20KANs%20were%20found%20to%20be%20superior%0Ain%20nearly%20all%20aspects%20of%20training%3B%20learning%2031%20times%20faster%2C%20being%2015%20times%0Amore%20parameter%20efficiency%2C%20and%20predicting%201.25%20times%20more%20accurately%20as%0Acompared%20to%20the%20MLP%20Deep%20Neural%20Networks%20%28DNNs%29%20in%20the%20case%20of%20the%202BP.%20Thus%2C%0AKANs%20shows%20potential%20for%20being%20an%20efficient%20tool%20in%20the%20development%20of%20Deep%0AKoopman%20Theory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02875v3&entry.124074799=Read"},
{"title": "An Investigation Into Explainable Audio Hate Speech Detection", "author": "Jinmyeong An and Wonjun Lee and Yejin Jeon and Jungseul Ok and Yunsu Kim and Gary Geunbae Lee", "abstract": "  Research on hate speech has predominantly revolved around detection and\ninterpretation from textual inputs, leaving verbal content largely unexplored.\nWhile there has been limited exploration into hate speech detection within\nverbal acoustic speech inputs, the aspect of interpretability has been\noverlooked. Therefore, we introduce a new task of explainable audio hate speech\ndetection. Specifically, we aim to identify the precise time intervals,\nreferred to as audio frame-level rationales, which serve as evidence for hate\nspeech classification. Towards this end, we propose two different approaches:\ncascading and End-to-End (E2E). The cascading approach initially converts audio\nto transcripts, identifies hate speech within these transcripts, and\nsubsequently locates the corresponding audio time frames. Conversely, the E2E\napproach processes audio utterances directly, which allows it to pinpoint hate\nspeech within specific time frames. Additionally, due to the lack of\nexplainable audio hate speech datasets that include audio frame-level\nrationales, we curated a synthetic audio dataset to train our models. We\nfurther validated these models on actual human speech utterances and found that\nthe E2E approach outperforms the cascading method in terms of the audio frame\nIntersection over Union (IoU) metric. Furthermore, we observed that including\nframe-level rationales significantly enhances hate speech detection accuracy\nfor the E2E approach.\n  \\textbf{Disclaimer} The reader may encounter content of an offensive or\nhateful nature. However, given the nature of the work, this cannot be avoided.\n", "link": "http://arxiv.org/abs/2408.06065v1", "date": "2024-08-12", "relevancy": 1.8361, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4728}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4571}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Investigation%20Into%20Explainable%20Audio%20Hate%20Speech%20Detection&body=Title%3A%20An%20Investigation%20Into%20Explainable%20Audio%20Hate%20Speech%20Detection%0AAuthor%3A%20Jinmyeong%20An%20and%20Wonjun%20Lee%20and%20Yejin%20Jeon%20and%20Jungseul%20Ok%20and%20Yunsu%20Kim%20and%20Gary%20Geunbae%20Lee%0AAbstract%3A%20%20%20Research%20on%20hate%20speech%20has%20predominantly%20revolved%20around%20detection%20and%0Ainterpretation%20from%20textual%20inputs%2C%20leaving%20verbal%20content%20largely%20unexplored.%0AWhile%20there%20has%20been%20limited%20exploration%20into%20hate%20speech%20detection%20within%0Averbal%20acoustic%20speech%20inputs%2C%20the%20aspect%20of%20interpretability%20has%20been%0Aoverlooked.%20Therefore%2C%20we%20introduce%20a%20new%20task%20of%20explainable%20audio%20hate%20speech%0Adetection.%20Specifically%2C%20we%20aim%20to%20identify%20the%20precise%20time%20intervals%2C%0Areferred%20to%20as%20audio%20frame-level%20rationales%2C%20which%20serve%20as%20evidence%20for%20hate%0Aspeech%20classification.%20Towards%20this%20end%2C%20we%20propose%20two%20different%20approaches%3A%0Acascading%20and%20End-to-End%20%28E2E%29.%20The%20cascading%20approach%20initially%20converts%20audio%0Ato%20transcripts%2C%20identifies%20hate%20speech%20within%20these%20transcripts%2C%20and%0Asubsequently%20locates%20the%20corresponding%20audio%20time%20frames.%20Conversely%2C%20the%20E2E%0Aapproach%20processes%20audio%20utterances%20directly%2C%20which%20allows%20it%20to%20pinpoint%20hate%0Aspeech%20within%20specific%20time%20frames.%20Additionally%2C%20due%20to%20the%20lack%20of%0Aexplainable%20audio%20hate%20speech%20datasets%20that%20include%20audio%20frame-level%0Arationales%2C%20we%20curated%20a%20synthetic%20audio%20dataset%20to%20train%20our%20models.%20We%0Afurther%20validated%20these%20models%20on%20actual%20human%20speech%20utterances%20and%20found%20that%0Athe%20E2E%20approach%20outperforms%20the%20cascading%20method%20in%20terms%20of%20the%20audio%20frame%0AIntersection%20over%20Union%20%28IoU%29%20metric.%20Furthermore%2C%20we%20observed%20that%20including%0Aframe-level%20rationales%20significantly%20enhances%20hate%20speech%20detection%20accuracy%0Afor%20the%20E2E%20approach.%0A%20%20%5Ctextbf%7BDisclaimer%7D%20The%20reader%20may%20encounter%20content%20of%20an%20offensive%20or%0Ahateful%20nature.%20However%2C%20given%20the%20nature%20of%20the%20work%2C%20this%20cannot%20be%20avoided.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06065v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Investigation%2520Into%2520Explainable%2520Audio%2520Hate%2520Speech%2520Detection%26entry.906535625%3DJinmyeong%2520An%2520and%2520Wonjun%2520Lee%2520and%2520Yejin%2520Jeon%2520and%2520Jungseul%2520Ok%2520and%2520Yunsu%2520Kim%2520and%2520Gary%2520Geunbae%2520Lee%26entry.1292438233%3D%2520%2520Research%2520on%2520hate%2520speech%2520has%2520predominantly%2520revolved%2520around%2520detection%2520and%250Ainterpretation%2520from%2520textual%2520inputs%252C%2520leaving%2520verbal%2520content%2520largely%2520unexplored.%250AWhile%2520there%2520has%2520been%2520limited%2520exploration%2520into%2520hate%2520speech%2520detection%2520within%250Averbal%2520acoustic%2520speech%2520inputs%252C%2520the%2520aspect%2520of%2520interpretability%2520has%2520been%250Aoverlooked.%2520Therefore%252C%2520we%2520introduce%2520a%2520new%2520task%2520of%2520explainable%2520audio%2520hate%2520speech%250Adetection.%2520Specifically%252C%2520we%2520aim%2520to%2520identify%2520the%2520precise%2520time%2520intervals%252C%250Areferred%2520to%2520as%2520audio%2520frame-level%2520rationales%252C%2520which%2520serve%2520as%2520evidence%2520for%2520hate%250Aspeech%2520classification.%2520Towards%2520this%2520end%252C%2520we%2520propose%2520two%2520different%2520approaches%253A%250Acascading%2520and%2520End-to-End%2520%2528E2E%2529.%2520The%2520cascading%2520approach%2520initially%2520converts%2520audio%250Ato%2520transcripts%252C%2520identifies%2520hate%2520speech%2520within%2520these%2520transcripts%252C%2520and%250Asubsequently%2520locates%2520the%2520corresponding%2520audio%2520time%2520frames.%2520Conversely%252C%2520the%2520E2E%250Aapproach%2520processes%2520audio%2520utterances%2520directly%252C%2520which%2520allows%2520it%2520to%2520pinpoint%2520hate%250Aspeech%2520within%2520specific%2520time%2520frames.%2520Additionally%252C%2520due%2520to%2520the%2520lack%2520of%250Aexplainable%2520audio%2520hate%2520speech%2520datasets%2520that%2520include%2520audio%2520frame-level%250Arationales%252C%2520we%2520curated%2520a%2520synthetic%2520audio%2520dataset%2520to%2520train%2520our%2520models.%2520We%250Afurther%2520validated%2520these%2520models%2520on%2520actual%2520human%2520speech%2520utterances%2520and%2520found%2520that%250Athe%2520E2E%2520approach%2520outperforms%2520the%2520cascading%2520method%2520in%2520terms%2520of%2520the%2520audio%2520frame%250AIntersection%2520over%2520Union%2520%2528IoU%2529%2520metric.%2520Furthermore%252C%2520we%2520observed%2520that%2520including%250Aframe-level%2520rationales%2520significantly%2520enhances%2520hate%2520speech%2520detection%2520accuracy%250Afor%2520the%2520E2E%2520approach.%250A%2520%2520%255Ctextbf%257BDisclaimer%257D%2520The%2520reader%2520may%2520encounter%2520content%2520of%2520an%2520offensive%2520or%250Ahateful%2520nature.%2520However%252C%2520given%2520the%2520nature%2520of%2520the%2520work%252C%2520this%2520cannot%2520be%2520avoided.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06065v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Investigation%20Into%20Explainable%20Audio%20Hate%20Speech%20Detection&entry.906535625=Jinmyeong%20An%20and%20Wonjun%20Lee%20and%20Yejin%20Jeon%20and%20Jungseul%20Ok%20and%20Yunsu%20Kim%20and%20Gary%20Geunbae%20Lee&entry.1292438233=%20%20Research%20on%20hate%20speech%20has%20predominantly%20revolved%20around%20detection%20and%0Ainterpretation%20from%20textual%20inputs%2C%20leaving%20verbal%20content%20largely%20unexplored.%0AWhile%20there%20has%20been%20limited%20exploration%20into%20hate%20speech%20detection%20within%0Averbal%20acoustic%20speech%20inputs%2C%20the%20aspect%20of%20interpretability%20has%20been%0Aoverlooked.%20Therefore%2C%20we%20introduce%20a%20new%20task%20of%20explainable%20audio%20hate%20speech%0Adetection.%20Specifically%2C%20we%20aim%20to%20identify%20the%20precise%20time%20intervals%2C%0Areferred%20to%20as%20audio%20frame-level%20rationales%2C%20which%20serve%20as%20evidence%20for%20hate%0Aspeech%20classification.%20Towards%20this%20end%2C%20we%20propose%20two%20different%20approaches%3A%0Acascading%20and%20End-to-End%20%28E2E%29.%20The%20cascading%20approach%20initially%20converts%20audio%0Ato%20transcripts%2C%20identifies%20hate%20speech%20within%20these%20transcripts%2C%20and%0Asubsequently%20locates%20the%20corresponding%20audio%20time%20frames.%20Conversely%2C%20the%20E2E%0Aapproach%20processes%20audio%20utterances%20directly%2C%20which%20allows%20it%20to%20pinpoint%20hate%0Aspeech%20within%20specific%20time%20frames.%20Additionally%2C%20due%20to%20the%20lack%20of%0Aexplainable%20audio%20hate%20speech%20datasets%20that%20include%20audio%20frame-level%0Arationales%2C%20we%20curated%20a%20synthetic%20audio%20dataset%20to%20train%20our%20models.%20We%0Afurther%20validated%20these%20models%20on%20actual%20human%20speech%20utterances%20and%20found%20that%0Athe%20E2E%20approach%20outperforms%20the%20cascading%20method%20in%20terms%20of%20the%20audio%20frame%0AIntersection%20over%20Union%20%28IoU%29%20metric.%20Furthermore%2C%20we%20observed%20that%20including%0Aframe-level%20rationales%20significantly%20enhances%20hate%20speech%20detection%20accuracy%0Afor%20the%20E2E%20approach.%0A%20%20%5Ctextbf%7BDisclaimer%7D%20The%20reader%20may%20encounter%20content%20of%20an%20offensive%20or%0Ahateful%20nature.%20However%2C%20given%20the%20nature%20of%20the%20work%2C%20this%20cannot%20be%20avoided.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06065v1&entry.124074799=Read"},
{"title": "Centralized and Federated Heart Disease Classification Models Using UCI\n  Dataset and their Shapley-value Based Interpretability", "author": "Mario Padilla Rodriguez and Mohamed Nafea", "abstract": "  Cardiovascular diseases are a leading cause of mortality worldwide,\nhighlighting the need for accurate diagnostic methods. This study benchmarks\ncentralized and federated machine learning algorithms for heart disease\nclassification using the UCI dataset which includes 920 patient records from\nfour hospitals in the USA, Hungary and Switzerland. Our benchmark is supported\nby Shapley-value interpretability analysis to quantify features' importance for\nclassification. In the centralized setup, various binary classification\nalgorithms are trained on pooled data, with a support vector machine (SVM)\nachieving the highest testing accuracy of 83.3\\%, surpassing the established\nbenchmark of 78.7\\% with logistic regression. Additionally, federated learning\nalgorithms with four clients (hospitals) are explored, leveraging the dataset's\nnatural partition to enhance privacy without sacrificing accuracy. Federated\nSVM, an uncommon approach in the literature, achieves a top testing accuracy of\n73.8\\%. Our interpretability analysis aligns with existing medical knowledge of\nheart disease indicators. Overall, this study establishes a benchmark for\nefficient and interpretable pre-screening tools for heart disease while\nmaintaining patients' privacy.\n", "link": "http://arxiv.org/abs/2408.06183v1", "date": "2024-08-12", "relevancy": 1.8293, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4617}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4612}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Centralized%20and%20Federated%20Heart%20Disease%20Classification%20Models%20Using%20UCI%0A%20%20Dataset%20and%20their%20Shapley-value%20Based%20Interpretability&body=Title%3A%20Centralized%20and%20Federated%20Heart%20Disease%20Classification%20Models%20Using%20UCI%0A%20%20Dataset%20and%20their%20Shapley-value%20Based%20Interpretability%0AAuthor%3A%20Mario%20Padilla%20Rodriguez%20and%20Mohamed%20Nafea%0AAbstract%3A%20%20%20Cardiovascular%20diseases%20are%20a%20leading%20cause%20of%20mortality%20worldwide%2C%0Ahighlighting%20the%20need%20for%20accurate%20diagnostic%20methods.%20This%20study%20benchmarks%0Acentralized%20and%20federated%20machine%20learning%20algorithms%20for%20heart%20disease%0Aclassification%20using%20the%20UCI%20dataset%20which%20includes%20920%20patient%20records%20from%0Afour%20hospitals%20in%20the%20USA%2C%20Hungary%20and%20Switzerland.%20Our%20benchmark%20is%20supported%0Aby%20Shapley-value%20interpretability%20analysis%20to%20quantify%20features%27%20importance%20for%0Aclassification.%20In%20the%20centralized%20setup%2C%20various%20binary%20classification%0Aalgorithms%20are%20trained%20on%20pooled%20data%2C%20with%20a%20support%20vector%20machine%20%28SVM%29%0Aachieving%20the%20highest%20testing%20accuracy%20of%2083.3%5C%25%2C%20surpassing%20the%20established%0Abenchmark%20of%2078.7%5C%25%20with%20logistic%20regression.%20Additionally%2C%20federated%20learning%0Aalgorithms%20with%20four%20clients%20%28hospitals%29%20are%20explored%2C%20leveraging%20the%20dataset%27s%0Anatural%20partition%20to%20enhance%20privacy%20without%20sacrificing%20accuracy.%20Federated%0ASVM%2C%20an%20uncommon%20approach%20in%20the%20literature%2C%20achieves%20a%20top%20testing%20accuracy%20of%0A73.8%5C%25.%20Our%20interpretability%20analysis%20aligns%20with%20existing%20medical%20knowledge%20of%0Aheart%20disease%20indicators.%20Overall%2C%20this%20study%20establishes%20a%20benchmark%20for%0Aefficient%20and%20interpretable%20pre-screening%20tools%20for%20heart%20disease%20while%0Amaintaining%20patients%27%20privacy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06183v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCentralized%2520and%2520Federated%2520Heart%2520Disease%2520Classification%2520Models%2520Using%2520UCI%250A%2520%2520Dataset%2520and%2520their%2520Shapley-value%2520Based%2520Interpretability%26entry.906535625%3DMario%2520Padilla%2520Rodriguez%2520and%2520Mohamed%2520Nafea%26entry.1292438233%3D%2520%2520Cardiovascular%2520diseases%2520are%2520a%2520leading%2520cause%2520of%2520mortality%2520worldwide%252C%250Ahighlighting%2520the%2520need%2520for%2520accurate%2520diagnostic%2520methods.%2520This%2520study%2520benchmarks%250Acentralized%2520and%2520federated%2520machine%2520learning%2520algorithms%2520for%2520heart%2520disease%250Aclassification%2520using%2520the%2520UCI%2520dataset%2520which%2520includes%2520920%2520patient%2520records%2520from%250Afour%2520hospitals%2520in%2520the%2520USA%252C%2520Hungary%2520and%2520Switzerland.%2520Our%2520benchmark%2520is%2520supported%250Aby%2520Shapley-value%2520interpretability%2520analysis%2520to%2520quantify%2520features%2527%2520importance%2520for%250Aclassification.%2520In%2520the%2520centralized%2520setup%252C%2520various%2520binary%2520classification%250Aalgorithms%2520are%2520trained%2520on%2520pooled%2520data%252C%2520with%2520a%2520support%2520vector%2520machine%2520%2528SVM%2529%250Aachieving%2520the%2520highest%2520testing%2520accuracy%2520of%252083.3%255C%2525%252C%2520surpassing%2520the%2520established%250Abenchmark%2520of%252078.7%255C%2525%2520with%2520logistic%2520regression.%2520Additionally%252C%2520federated%2520learning%250Aalgorithms%2520with%2520four%2520clients%2520%2528hospitals%2529%2520are%2520explored%252C%2520leveraging%2520the%2520dataset%2527s%250Anatural%2520partition%2520to%2520enhance%2520privacy%2520without%2520sacrificing%2520accuracy.%2520Federated%250ASVM%252C%2520an%2520uncommon%2520approach%2520in%2520the%2520literature%252C%2520achieves%2520a%2520top%2520testing%2520accuracy%2520of%250A73.8%255C%2525.%2520Our%2520interpretability%2520analysis%2520aligns%2520with%2520existing%2520medical%2520knowledge%2520of%250Aheart%2520disease%2520indicators.%2520Overall%252C%2520this%2520study%2520establishes%2520a%2520benchmark%2520for%250Aefficient%2520and%2520interpretable%2520pre-screening%2520tools%2520for%2520heart%2520disease%2520while%250Amaintaining%2520patients%2527%2520privacy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06183v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Centralized%20and%20Federated%20Heart%20Disease%20Classification%20Models%20Using%20UCI%0A%20%20Dataset%20and%20their%20Shapley-value%20Based%20Interpretability&entry.906535625=Mario%20Padilla%20Rodriguez%20and%20Mohamed%20Nafea&entry.1292438233=%20%20Cardiovascular%20diseases%20are%20a%20leading%20cause%20of%20mortality%20worldwide%2C%0Ahighlighting%20the%20need%20for%20accurate%20diagnostic%20methods.%20This%20study%20benchmarks%0Acentralized%20and%20federated%20machine%20learning%20algorithms%20for%20heart%20disease%0Aclassification%20using%20the%20UCI%20dataset%20which%20includes%20920%20patient%20records%20from%0Afour%20hospitals%20in%20the%20USA%2C%20Hungary%20and%20Switzerland.%20Our%20benchmark%20is%20supported%0Aby%20Shapley-value%20interpretability%20analysis%20to%20quantify%20features%27%20importance%20for%0Aclassification.%20In%20the%20centralized%20setup%2C%20various%20binary%20classification%0Aalgorithms%20are%20trained%20on%20pooled%20data%2C%20with%20a%20support%20vector%20machine%20%28SVM%29%0Aachieving%20the%20highest%20testing%20accuracy%20of%2083.3%5C%25%2C%20surpassing%20the%20established%0Abenchmark%20of%2078.7%5C%25%20with%20logistic%20regression.%20Additionally%2C%20federated%20learning%0Aalgorithms%20with%20four%20clients%20%28hospitals%29%20are%20explored%2C%20leveraging%20the%20dataset%27s%0Anatural%20partition%20to%20enhance%20privacy%20without%20sacrificing%20accuracy.%20Federated%0ASVM%2C%20an%20uncommon%20approach%20in%20the%20literature%2C%20achieves%20a%20top%20testing%20accuracy%20of%0A73.8%5C%25.%20Our%20interpretability%20analysis%20aligns%20with%20existing%20medical%20knowledge%20of%0Aheart%20disease%20indicators.%20Overall%2C%20this%20study%20establishes%20a%20benchmark%20for%0Aefficient%20and%20interpretable%20pre-screening%20tools%20for%20heart%20disease%20while%0Amaintaining%20patients%27%20privacy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06183v1&entry.124074799=Read"},
{"title": "Identifying Three-Dimensional Radiative Patterns Associated with Early\n  Tropical Cyclone Intensification", "author": "Frederick Iat-Hin Tam and Tom Beucler and James H. Ruppert Jr", "abstract": "  Cloud radiative feedback impacts early tropical cyclone (TC) intensification,\nbut limitations in existing diagnostic frameworks make them unsuitable for\nstudying asymmetric or transient radiative heating. We propose a linear\nVariational Encoder-Decoder (VED) to learn the hidden relationship between\nradiation and the surface intensification of realistic simulated TCs. Limiting\nVED model inputs enables using its uncertainty to identify periods when\nradiation has more importance for intensification. A close examination of the\nextracted 3D radiative structures suggests that longwave radiative forcing from\ninner core deep convection and shallow clouds both contribute to\nintensification, with the deep convection having the most impact overall. We\nfind that deep convection downwind of the shallow clouds is critical to the\nintensification of Haiyan. Our work demonstrates that machine learning can\ndiscover thermodynamic-kinematic relationships without relying on axisymmetric\nor deterministic assumptions, paving the way towards the objective discovery of\nprocesses leading to TC intensification in realistic conditions.\n", "link": "http://arxiv.org/abs/2401.09493v5", "date": "2024-08-12", "relevancy": 1.8158, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4577}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4532}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identifying%20Three-Dimensional%20Radiative%20Patterns%20Associated%20with%20Early%0A%20%20Tropical%20Cyclone%20Intensification&body=Title%3A%20Identifying%20Three-Dimensional%20Radiative%20Patterns%20Associated%20with%20Early%0A%20%20Tropical%20Cyclone%20Intensification%0AAuthor%3A%20Frederick%20Iat-Hin%20Tam%20and%20Tom%20Beucler%20and%20James%20H.%20Ruppert%20Jr%0AAbstract%3A%20%20%20Cloud%20radiative%20feedback%20impacts%20early%20tropical%20cyclone%20%28TC%29%20intensification%2C%0Abut%20limitations%20in%20existing%20diagnostic%20frameworks%20make%20them%20unsuitable%20for%0Astudying%20asymmetric%20or%20transient%20radiative%20heating.%20We%20propose%20a%20linear%0AVariational%20Encoder-Decoder%20%28VED%29%20to%20learn%20the%20hidden%20relationship%20between%0Aradiation%20and%20the%20surface%20intensification%20of%20realistic%20simulated%20TCs.%20Limiting%0AVED%20model%20inputs%20enables%20using%20its%20uncertainty%20to%20identify%20periods%20when%0Aradiation%20has%20more%20importance%20for%20intensification.%20A%20close%20examination%20of%20the%0Aextracted%203D%20radiative%20structures%20suggests%20that%20longwave%20radiative%20forcing%20from%0Ainner%20core%20deep%20convection%20and%20shallow%20clouds%20both%20contribute%20to%0Aintensification%2C%20with%20the%20deep%20convection%20having%20the%20most%20impact%20overall.%20We%0Afind%20that%20deep%20convection%20downwind%20of%20the%20shallow%20clouds%20is%20critical%20to%20the%0Aintensification%20of%20Haiyan.%20Our%20work%20demonstrates%20that%20machine%20learning%20can%0Adiscover%20thermodynamic-kinematic%20relationships%20without%20relying%20on%20axisymmetric%0Aor%20deterministic%20assumptions%2C%20paving%20the%20way%20towards%20the%20objective%20discovery%20of%0Aprocesses%20leading%20to%20TC%20intensification%20in%20realistic%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.09493v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentifying%2520Three-Dimensional%2520Radiative%2520Patterns%2520Associated%2520with%2520Early%250A%2520%2520Tropical%2520Cyclone%2520Intensification%26entry.906535625%3DFrederick%2520Iat-Hin%2520Tam%2520and%2520Tom%2520Beucler%2520and%2520James%2520H.%2520Ruppert%2520Jr%26entry.1292438233%3D%2520%2520Cloud%2520radiative%2520feedback%2520impacts%2520early%2520tropical%2520cyclone%2520%2528TC%2529%2520intensification%252C%250Abut%2520limitations%2520in%2520existing%2520diagnostic%2520frameworks%2520make%2520them%2520unsuitable%2520for%250Astudying%2520asymmetric%2520or%2520transient%2520radiative%2520heating.%2520We%2520propose%2520a%2520linear%250AVariational%2520Encoder-Decoder%2520%2528VED%2529%2520to%2520learn%2520the%2520hidden%2520relationship%2520between%250Aradiation%2520and%2520the%2520surface%2520intensification%2520of%2520realistic%2520simulated%2520TCs.%2520Limiting%250AVED%2520model%2520inputs%2520enables%2520using%2520its%2520uncertainty%2520to%2520identify%2520periods%2520when%250Aradiation%2520has%2520more%2520importance%2520for%2520intensification.%2520A%2520close%2520examination%2520of%2520the%250Aextracted%25203D%2520radiative%2520structures%2520suggests%2520that%2520longwave%2520radiative%2520forcing%2520from%250Ainner%2520core%2520deep%2520convection%2520and%2520shallow%2520clouds%2520both%2520contribute%2520to%250Aintensification%252C%2520with%2520the%2520deep%2520convection%2520having%2520the%2520most%2520impact%2520overall.%2520We%250Afind%2520that%2520deep%2520convection%2520downwind%2520of%2520the%2520shallow%2520clouds%2520is%2520critical%2520to%2520the%250Aintensification%2520of%2520Haiyan.%2520Our%2520work%2520demonstrates%2520that%2520machine%2520learning%2520can%250Adiscover%2520thermodynamic-kinematic%2520relationships%2520without%2520relying%2520on%2520axisymmetric%250Aor%2520deterministic%2520assumptions%252C%2520paving%2520the%2520way%2520towards%2520the%2520objective%2520discovery%2520of%250Aprocesses%2520leading%2520to%2520TC%2520intensification%2520in%2520realistic%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.09493v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identifying%20Three-Dimensional%20Radiative%20Patterns%20Associated%20with%20Early%0A%20%20Tropical%20Cyclone%20Intensification&entry.906535625=Frederick%20Iat-Hin%20Tam%20and%20Tom%20Beucler%20and%20James%20H.%20Ruppert%20Jr&entry.1292438233=%20%20Cloud%20radiative%20feedback%20impacts%20early%20tropical%20cyclone%20%28TC%29%20intensification%2C%0Abut%20limitations%20in%20existing%20diagnostic%20frameworks%20make%20them%20unsuitable%20for%0Astudying%20asymmetric%20or%20transient%20radiative%20heating.%20We%20propose%20a%20linear%0AVariational%20Encoder-Decoder%20%28VED%29%20to%20learn%20the%20hidden%20relationship%20between%0Aradiation%20and%20the%20surface%20intensification%20of%20realistic%20simulated%20TCs.%20Limiting%0AVED%20model%20inputs%20enables%20using%20its%20uncertainty%20to%20identify%20periods%20when%0Aradiation%20has%20more%20importance%20for%20intensification.%20A%20close%20examination%20of%20the%0Aextracted%203D%20radiative%20structures%20suggests%20that%20longwave%20radiative%20forcing%20from%0Ainner%20core%20deep%20convection%20and%20shallow%20clouds%20both%20contribute%20to%0Aintensification%2C%20with%20the%20deep%20convection%20having%20the%20most%20impact%20overall.%20We%0Afind%20that%20deep%20convection%20downwind%20of%20the%20shallow%20clouds%20is%20critical%20to%20the%0Aintensification%20of%20Haiyan.%20Our%20work%20demonstrates%20that%20machine%20learning%20can%0Adiscover%20thermodynamic-kinematic%20relationships%20without%20relying%20on%20axisymmetric%0Aor%20deterministic%20assumptions%2C%20paving%20the%20way%20towards%20the%20objective%20discovery%20of%0Aprocesses%20leading%20to%20TC%20intensification%20in%20realistic%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.09493v5&entry.124074799=Read"},
{"title": "Improving Structural Diversity of Blackbox LLMs via\n  Chain-of-Specification Prompting", "author": "Halley Young and Yimeng Zeng and Jacob Gardner and Osbert Bastani", "abstract": "  The capability to generate diverse text is a key challenge facing large\nlanguage models (LLMs). Thus far, diversity has been studied via metrics such\nas $n$-gram diversity or diversity of BERT embeddings. However, for these kinds\nof diversity, the user has little control over the dimensions along which\ndiversity is considered. For example, in the poetry domain, one might desire\ndiversity in terms of rhyme and meter, whereas in the code domain, one might\ndesire diversity in terms of the kinds of expressions used to solve a problem.\nWe propose a diversity metric called structural diversity, where the user\nprovides a mapping from generated text to features capturing the kinds of\ndiversity that they care about. In addition, we propose a novel strategy called\nchain-of-specification (CoS) prompting for improving diversity by first having\nthe LLM generate a specification encoding one instance of structural features,\nand then prompting the LLM to generate text that satisfies these features;\nnotably, our strategy works with blackbox LLMs. In our experiments, we show\nthat for structural diversity in the poetry and code domains, CoS significantly\nimproves diversity compared to several baselines.\n", "link": "http://arxiv.org/abs/2408.06186v1", "date": "2024-08-12", "relevancy": 1.8138, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4636}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4514}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Structural%20Diversity%20of%20Blackbox%20LLMs%20via%0A%20%20Chain-of-Specification%20Prompting&body=Title%3A%20Improving%20Structural%20Diversity%20of%20Blackbox%20LLMs%20via%0A%20%20Chain-of-Specification%20Prompting%0AAuthor%3A%20Halley%20Young%20and%20Yimeng%20Zeng%20and%20Jacob%20Gardner%20and%20Osbert%20Bastani%0AAbstract%3A%20%20%20The%20capability%20to%20generate%20diverse%20text%20is%20a%20key%20challenge%20facing%20large%0Alanguage%20models%20%28LLMs%29.%20Thus%20far%2C%20diversity%20has%20been%20studied%20via%20metrics%20such%0Aas%20%24n%24-gram%20diversity%20or%20diversity%20of%20BERT%20embeddings.%20However%2C%20for%20these%20kinds%0Aof%20diversity%2C%20the%20user%20has%20little%20control%20over%20the%20dimensions%20along%20which%0Adiversity%20is%20considered.%20For%20example%2C%20in%20the%20poetry%20domain%2C%20one%20might%20desire%0Adiversity%20in%20terms%20of%20rhyme%20and%20meter%2C%20whereas%20in%20the%20code%20domain%2C%20one%20might%0Adesire%20diversity%20in%20terms%20of%20the%20kinds%20of%20expressions%20used%20to%20solve%20a%20problem.%0AWe%20propose%20a%20diversity%20metric%20called%20structural%20diversity%2C%20where%20the%20user%0Aprovides%20a%20mapping%20from%20generated%20text%20to%20features%20capturing%20the%20kinds%20of%0Adiversity%20that%20they%20care%20about.%20In%20addition%2C%20we%20propose%20a%20novel%20strategy%20called%0Achain-of-specification%20%28CoS%29%20prompting%20for%20improving%20diversity%20by%20first%20having%0Athe%20LLM%20generate%20a%20specification%20encoding%20one%20instance%20of%20structural%20features%2C%0Aand%20then%20prompting%20the%20LLM%20to%20generate%20text%20that%20satisfies%20these%20features%3B%0Anotably%2C%20our%20strategy%20works%20with%20blackbox%20LLMs.%20In%20our%20experiments%2C%20we%20show%0Athat%20for%20structural%20diversity%20in%20the%20poetry%20and%20code%20domains%2C%20CoS%20significantly%0Aimproves%20diversity%20compared%20to%20several%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06186v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Structural%2520Diversity%2520of%2520Blackbox%2520LLMs%2520via%250A%2520%2520Chain-of-Specification%2520Prompting%26entry.906535625%3DHalley%2520Young%2520and%2520Yimeng%2520Zeng%2520and%2520Jacob%2520Gardner%2520and%2520Osbert%2520Bastani%26entry.1292438233%3D%2520%2520The%2520capability%2520to%2520generate%2520diverse%2520text%2520is%2520a%2520key%2520challenge%2520facing%2520large%250Alanguage%2520models%2520%2528LLMs%2529.%2520Thus%2520far%252C%2520diversity%2520has%2520been%2520studied%2520via%2520metrics%2520such%250Aas%2520%2524n%2524-gram%2520diversity%2520or%2520diversity%2520of%2520BERT%2520embeddings.%2520However%252C%2520for%2520these%2520kinds%250Aof%2520diversity%252C%2520the%2520user%2520has%2520little%2520control%2520over%2520the%2520dimensions%2520along%2520which%250Adiversity%2520is%2520considered.%2520For%2520example%252C%2520in%2520the%2520poetry%2520domain%252C%2520one%2520might%2520desire%250Adiversity%2520in%2520terms%2520of%2520rhyme%2520and%2520meter%252C%2520whereas%2520in%2520the%2520code%2520domain%252C%2520one%2520might%250Adesire%2520diversity%2520in%2520terms%2520of%2520the%2520kinds%2520of%2520expressions%2520used%2520to%2520solve%2520a%2520problem.%250AWe%2520propose%2520a%2520diversity%2520metric%2520called%2520structural%2520diversity%252C%2520where%2520the%2520user%250Aprovides%2520a%2520mapping%2520from%2520generated%2520text%2520to%2520features%2520capturing%2520the%2520kinds%2520of%250Adiversity%2520that%2520they%2520care%2520about.%2520In%2520addition%252C%2520we%2520propose%2520a%2520novel%2520strategy%2520called%250Achain-of-specification%2520%2528CoS%2529%2520prompting%2520for%2520improving%2520diversity%2520by%2520first%2520having%250Athe%2520LLM%2520generate%2520a%2520specification%2520encoding%2520one%2520instance%2520of%2520structural%2520features%252C%250Aand%2520then%2520prompting%2520the%2520LLM%2520to%2520generate%2520text%2520that%2520satisfies%2520these%2520features%253B%250Anotably%252C%2520our%2520strategy%2520works%2520with%2520blackbox%2520LLMs.%2520In%2520our%2520experiments%252C%2520we%2520show%250Athat%2520for%2520structural%2520diversity%2520in%2520the%2520poetry%2520and%2520code%2520domains%252C%2520CoS%2520significantly%250Aimproves%2520diversity%2520compared%2520to%2520several%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06186v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Structural%20Diversity%20of%20Blackbox%20LLMs%20via%0A%20%20Chain-of-Specification%20Prompting&entry.906535625=Halley%20Young%20and%20Yimeng%20Zeng%20and%20Jacob%20Gardner%20and%20Osbert%20Bastani&entry.1292438233=%20%20The%20capability%20to%20generate%20diverse%20text%20is%20a%20key%20challenge%20facing%20large%0Alanguage%20models%20%28LLMs%29.%20Thus%20far%2C%20diversity%20has%20been%20studied%20via%20metrics%20such%0Aas%20%24n%24-gram%20diversity%20or%20diversity%20of%20BERT%20embeddings.%20However%2C%20for%20these%20kinds%0Aof%20diversity%2C%20the%20user%20has%20little%20control%20over%20the%20dimensions%20along%20which%0Adiversity%20is%20considered.%20For%20example%2C%20in%20the%20poetry%20domain%2C%20one%20might%20desire%0Adiversity%20in%20terms%20of%20rhyme%20and%20meter%2C%20whereas%20in%20the%20code%20domain%2C%20one%20might%0Adesire%20diversity%20in%20terms%20of%20the%20kinds%20of%20expressions%20used%20to%20solve%20a%20problem.%0AWe%20propose%20a%20diversity%20metric%20called%20structural%20diversity%2C%20where%20the%20user%0Aprovides%20a%20mapping%20from%20generated%20text%20to%20features%20capturing%20the%20kinds%20of%0Adiversity%20that%20they%20care%20about.%20In%20addition%2C%20we%20propose%20a%20novel%20strategy%20called%0Achain-of-specification%20%28CoS%29%20prompting%20for%20improving%20diversity%20by%20first%20having%0Athe%20LLM%20generate%20a%20specification%20encoding%20one%20instance%20of%20structural%20features%2C%0Aand%20then%20prompting%20the%20LLM%20to%20generate%20text%20that%20satisfies%20these%20features%3B%0Anotably%2C%20our%20strategy%20works%20with%20blackbox%20LLMs.%20In%20our%20experiments%2C%20we%20show%0Athat%20for%20structural%20diversity%20in%20the%20poetry%20and%20code%20domains%2C%20CoS%20significantly%0Aimproves%20diversity%20compared%20to%20several%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06186v1&entry.124074799=Read"},
{"title": "Investigating the ability of deep learning to predict Welding Depth and\n  Pore Volume in Hairpin Welding", "author": "Amena Darwish and Stefan Ericson and Rohollah Ghasemi and Tobias Andersson and Dan L\u00f6nn and Andreas Andersson Lassila and Kent Salomonsson", "abstract": "  To advance quality assurance in the welding process, this study presents a\ndeep learning DL model that enables the prediction of two critical welds' Key\nPerformance Characteristics (KPCs): welding depth and average pore volume. In\nthe proposed approach, a wide range of laser welding Key Input Characteristics\n(KICs) is utilized, including welding beam geometries, welding feed rates, path\nrepetitions for weld beam geometries, and bright light weld ratios for all\npaths, all of which were obtained from hairpin welding experiments. Two DL\nnetworks are employed with multiple hidden dense layers and linear activation\nfunctions to investigate the capabilities of deep neural networks in capturing\nthe complex nonlinear relationships between the welding input and output\nvariables (KPCs and KICs). Applying DL networks to the small numerical\nexperimental hairpin welding dataset has shown promising results, achieving\nMean Absolute Error (MAE) values 0.1079 for predicting welding depth and 0.0641\nfor average pore volume. This, in turn, promises significant advantages in\ncontrolling welding outcomes, moving beyond the current trend of relying only\non defect classification in weld monitoring, to capture the correlation between\nthe weld parameters and weld geometries.\n", "link": "http://arxiv.org/abs/2312.01606v4", "date": "2024-08-12", "relevancy": 1.8094, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4636}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4619}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4383}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20the%20ability%20of%20deep%20learning%20to%20predict%20Welding%20Depth%20and%0A%20%20Pore%20Volume%20in%20Hairpin%20Welding&body=Title%3A%20Investigating%20the%20ability%20of%20deep%20learning%20to%20predict%20Welding%20Depth%20and%0A%20%20Pore%20Volume%20in%20Hairpin%20Welding%0AAuthor%3A%20Amena%20Darwish%20and%20Stefan%20Ericson%20and%20Rohollah%20Ghasemi%20and%20Tobias%20Andersson%20and%20Dan%20L%C3%B6nn%20and%20Andreas%20Andersson%20Lassila%20and%20Kent%20Salomonsson%0AAbstract%3A%20%20%20To%20advance%20quality%20assurance%20in%20the%20welding%20process%2C%20this%20study%20presents%20a%0Adeep%20learning%20DL%20model%20that%20enables%20the%20prediction%20of%20two%20critical%20welds%27%20Key%0APerformance%20Characteristics%20%28KPCs%29%3A%20welding%20depth%20and%20average%20pore%20volume.%20In%0Athe%20proposed%20approach%2C%20a%20wide%20range%20of%20laser%20welding%20Key%20Input%20Characteristics%0A%28KICs%29%20is%20utilized%2C%20including%20welding%20beam%20geometries%2C%20welding%20feed%20rates%2C%20path%0Arepetitions%20for%20weld%20beam%20geometries%2C%20and%20bright%20light%20weld%20ratios%20for%20all%0Apaths%2C%20all%20of%20which%20were%20obtained%20from%20hairpin%20welding%20experiments.%20Two%20DL%0Anetworks%20are%20employed%20with%20multiple%20hidden%20dense%20layers%20and%20linear%20activation%0Afunctions%20to%20investigate%20the%20capabilities%20of%20deep%20neural%20networks%20in%20capturing%0Athe%20complex%20nonlinear%20relationships%20between%20the%20welding%20input%20and%20output%0Avariables%20%28KPCs%20and%20KICs%29.%20Applying%20DL%20networks%20to%20the%20small%20numerical%0Aexperimental%20hairpin%20welding%20dataset%20has%20shown%20promising%20results%2C%20achieving%0AMean%20Absolute%20Error%20%28MAE%29%20values%200.1079%20for%20predicting%20welding%20depth%20and%200.0641%0Afor%20average%20pore%20volume.%20This%2C%20in%20turn%2C%20promises%20significant%20advantages%20in%0Acontrolling%20welding%20outcomes%2C%20moving%20beyond%20the%20current%20trend%20of%20relying%20only%0Aon%20defect%20classification%20in%20weld%20monitoring%2C%20to%20capture%20the%20correlation%20between%0Athe%20weld%20parameters%20and%20weld%20geometries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.01606v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520the%2520ability%2520of%2520deep%2520learning%2520to%2520predict%2520Welding%2520Depth%2520and%250A%2520%2520Pore%2520Volume%2520in%2520Hairpin%2520Welding%26entry.906535625%3DAmena%2520Darwish%2520and%2520Stefan%2520Ericson%2520and%2520Rohollah%2520Ghasemi%2520and%2520Tobias%2520Andersson%2520and%2520Dan%2520L%25C3%25B6nn%2520and%2520Andreas%2520Andersson%2520Lassila%2520and%2520Kent%2520Salomonsson%26entry.1292438233%3D%2520%2520To%2520advance%2520quality%2520assurance%2520in%2520the%2520welding%2520process%252C%2520this%2520study%2520presents%2520a%250Adeep%2520learning%2520DL%2520model%2520that%2520enables%2520the%2520prediction%2520of%2520two%2520critical%2520welds%2527%2520Key%250APerformance%2520Characteristics%2520%2528KPCs%2529%253A%2520welding%2520depth%2520and%2520average%2520pore%2520volume.%2520In%250Athe%2520proposed%2520approach%252C%2520a%2520wide%2520range%2520of%2520laser%2520welding%2520Key%2520Input%2520Characteristics%250A%2528KICs%2529%2520is%2520utilized%252C%2520including%2520welding%2520beam%2520geometries%252C%2520welding%2520feed%2520rates%252C%2520path%250Arepetitions%2520for%2520weld%2520beam%2520geometries%252C%2520and%2520bright%2520light%2520weld%2520ratios%2520for%2520all%250Apaths%252C%2520all%2520of%2520which%2520were%2520obtained%2520from%2520hairpin%2520welding%2520experiments.%2520Two%2520DL%250Anetworks%2520are%2520employed%2520with%2520multiple%2520hidden%2520dense%2520layers%2520and%2520linear%2520activation%250Afunctions%2520to%2520investigate%2520the%2520capabilities%2520of%2520deep%2520neural%2520networks%2520in%2520capturing%250Athe%2520complex%2520nonlinear%2520relationships%2520between%2520the%2520welding%2520input%2520and%2520output%250Avariables%2520%2528KPCs%2520and%2520KICs%2529.%2520Applying%2520DL%2520networks%2520to%2520the%2520small%2520numerical%250Aexperimental%2520hairpin%2520welding%2520dataset%2520has%2520shown%2520promising%2520results%252C%2520achieving%250AMean%2520Absolute%2520Error%2520%2528MAE%2529%2520values%25200.1079%2520for%2520predicting%2520welding%2520depth%2520and%25200.0641%250Afor%2520average%2520pore%2520volume.%2520This%252C%2520in%2520turn%252C%2520promises%2520significant%2520advantages%2520in%250Acontrolling%2520welding%2520outcomes%252C%2520moving%2520beyond%2520the%2520current%2520trend%2520of%2520relying%2520only%250Aon%2520defect%2520classification%2520in%2520weld%2520monitoring%252C%2520to%2520capture%2520the%2520correlation%2520between%250Athe%2520weld%2520parameters%2520and%2520weld%2520geometries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.01606v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20the%20ability%20of%20deep%20learning%20to%20predict%20Welding%20Depth%20and%0A%20%20Pore%20Volume%20in%20Hairpin%20Welding&entry.906535625=Amena%20Darwish%20and%20Stefan%20Ericson%20and%20Rohollah%20Ghasemi%20and%20Tobias%20Andersson%20and%20Dan%20L%C3%B6nn%20and%20Andreas%20Andersson%20Lassila%20and%20Kent%20Salomonsson&entry.1292438233=%20%20To%20advance%20quality%20assurance%20in%20the%20welding%20process%2C%20this%20study%20presents%20a%0Adeep%20learning%20DL%20model%20that%20enables%20the%20prediction%20of%20two%20critical%20welds%27%20Key%0APerformance%20Characteristics%20%28KPCs%29%3A%20welding%20depth%20and%20average%20pore%20volume.%20In%0Athe%20proposed%20approach%2C%20a%20wide%20range%20of%20laser%20welding%20Key%20Input%20Characteristics%0A%28KICs%29%20is%20utilized%2C%20including%20welding%20beam%20geometries%2C%20welding%20feed%20rates%2C%20path%0Arepetitions%20for%20weld%20beam%20geometries%2C%20and%20bright%20light%20weld%20ratios%20for%20all%0Apaths%2C%20all%20of%20which%20were%20obtained%20from%20hairpin%20welding%20experiments.%20Two%20DL%0Anetworks%20are%20employed%20with%20multiple%20hidden%20dense%20layers%20and%20linear%20activation%0Afunctions%20to%20investigate%20the%20capabilities%20of%20deep%20neural%20networks%20in%20capturing%0Athe%20complex%20nonlinear%20relationships%20between%20the%20welding%20input%20and%20output%0Avariables%20%28KPCs%20and%20KICs%29.%20Applying%20DL%20networks%20to%20the%20small%20numerical%0Aexperimental%20hairpin%20welding%20dataset%20has%20shown%20promising%20results%2C%20achieving%0AMean%20Absolute%20Error%20%28MAE%29%20values%200.1079%20for%20predicting%20welding%20depth%20and%200.0641%0Afor%20average%20pore%20volume.%20This%2C%20in%20turn%2C%20promises%20significant%20advantages%20in%0Acontrolling%20welding%20outcomes%2C%20moving%20beyond%20the%20current%20trend%20of%20relying%20only%0Aon%20defect%20classification%20in%20weld%20monitoring%2C%20to%20capture%20the%20correlation%20between%0Athe%20weld%20parameters%20and%20weld%20geometries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.01606v4&entry.124074799=Read"},
{"title": "Fakes of Varying Shades: How Warning Affects Human Perception and\n  Engagement Regarding LLM Hallucinations", "author": "Mahjabin Nahar and Haeseung Seo and Eun-Ju Lee and Aiping Xiong and Dongwon Lee", "abstract": "  The widespread adoption and transformative effects of large language models\n(LLMs) have sparked concerns regarding their capacity to produce inaccurate and\nfictitious content, referred to as `hallucinations'. Given the potential risks\nassociated with hallucinations, humans should be able to identify them. This\nresearch aims to understand the human perception of LLM hallucinations by\nsystematically varying the degree of hallucination (genuine, minor\nhallucination, major hallucination) and examining its interaction with warning\n(i.e., a warning of potential inaccuracies: absent vs. present). Participants\n(N=419) from Prolific rated the perceived accuracy and engaged with content\n(e.g., like, dislike, share) in a Q/A format. Participants ranked content as\ntruthful in the order of genuine, minor hallucination, and major hallucination,\nand user engagement behaviors mirrored this pattern. More importantly, we\nobserved that warning improved the detection of hallucination without\nsignificantly affecting the perceived truthfulness of genuine content. We\nconclude by offering insights for future tools to aid human detection of\nhallucinations. All survey materials, demographic questions, and post-session\nquestions are available at:\nhttps://github.com/MahjabinNahar/fakes-of-varying-shades-survey-materials\n", "link": "http://arxiv.org/abs/2404.03745v3", "date": "2024-08-12", "relevancy": 1.808, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4624}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4533}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fakes%20of%20Varying%20Shades%3A%20How%20Warning%20Affects%20Human%20Perception%20and%0A%20%20Engagement%20Regarding%20LLM%20Hallucinations&body=Title%3A%20Fakes%20of%20Varying%20Shades%3A%20How%20Warning%20Affects%20Human%20Perception%20and%0A%20%20Engagement%20Regarding%20LLM%20Hallucinations%0AAuthor%3A%20Mahjabin%20Nahar%20and%20Haeseung%20Seo%20and%20Eun-Ju%20Lee%20and%20Aiping%20Xiong%20and%20Dongwon%20Lee%0AAbstract%3A%20%20%20The%20widespread%20adoption%20and%20transformative%20effects%20of%20large%20language%20models%0A%28LLMs%29%20have%20sparked%20concerns%20regarding%20their%20capacity%20to%20produce%20inaccurate%20and%0Afictitious%20content%2C%20referred%20to%20as%20%60hallucinations%27.%20Given%20the%20potential%20risks%0Aassociated%20with%20hallucinations%2C%20humans%20should%20be%20able%20to%20identify%20them.%20This%0Aresearch%20aims%20to%20understand%20the%20human%20perception%20of%20LLM%20hallucinations%20by%0Asystematically%20varying%20the%20degree%20of%20hallucination%20%28genuine%2C%20minor%0Ahallucination%2C%20major%20hallucination%29%20and%20examining%20its%20interaction%20with%20warning%0A%28i.e.%2C%20a%20warning%20of%20potential%20inaccuracies%3A%20absent%20vs.%20present%29.%20Participants%0A%28N%3D419%29%20from%20Prolific%20rated%20the%20perceived%20accuracy%20and%20engaged%20with%20content%0A%28e.g.%2C%20like%2C%20dislike%2C%20share%29%20in%20a%20Q/A%20format.%20Participants%20ranked%20content%20as%0Atruthful%20in%20the%20order%20of%20genuine%2C%20minor%20hallucination%2C%20and%20major%20hallucination%2C%0Aand%20user%20engagement%20behaviors%20mirrored%20this%20pattern.%20More%20importantly%2C%20we%0Aobserved%20that%20warning%20improved%20the%20detection%20of%20hallucination%20without%0Asignificantly%20affecting%20the%20perceived%20truthfulness%20of%20genuine%20content.%20We%0Aconclude%20by%20offering%20insights%20for%20future%20tools%20to%20aid%20human%20detection%20of%0Ahallucinations.%20All%20survey%20materials%2C%20demographic%20questions%2C%20and%20post-session%0Aquestions%20are%20available%20at%3A%0Ahttps%3A//github.com/MahjabinNahar/fakes-of-varying-shades-survey-materials%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03745v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFakes%2520of%2520Varying%2520Shades%253A%2520How%2520Warning%2520Affects%2520Human%2520Perception%2520and%250A%2520%2520Engagement%2520Regarding%2520LLM%2520Hallucinations%26entry.906535625%3DMahjabin%2520Nahar%2520and%2520Haeseung%2520Seo%2520and%2520Eun-Ju%2520Lee%2520and%2520Aiping%2520Xiong%2520and%2520Dongwon%2520Lee%26entry.1292438233%3D%2520%2520The%2520widespread%2520adoption%2520and%2520transformative%2520effects%2520of%2520large%2520language%2520models%250A%2528LLMs%2529%2520have%2520sparked%2520concerns%2520regarding%2520their%2520capacity%2520to%2520produce%2520inaccurate%2520and%250Afictitious%2520content%252C%2520referred%2520to%2520as%2520%2560hallucinations%2527.%2520Given%2520the%2520potential%2520risks%250Aassociated%2520with%2520hallucinations%252C%2520humans%2520should%2520be%2520able%2520to%2520identify%2520them.%2520This%250Aresearch%2520aims%2520to%2520understand%2520the%2520human%2520perception%2520of%2520LLM%2520hallucinations%2520by%250Asystematically%2520varying%2520the%2520degree%2520of%2520hallucination%2520%2528genuine%252C%2520minor%250Ahallucination%252C%2520major%2520hallucination%2529%2520and%2520examining%2520its%2520interaction%2520with%2520warning%250A%2528i.e.%252C%2520a%2520warning%2520of%2520potential%2520inaccuracies%253A%2520absent%2520vs.%2520present%2529.%2520Participants%250A%2528N%253D419%2529%2520from%2520Prolific%2520rated%2520the%2520perceived%2520accuracy%2520and%2520engaged%2520with%2520content%250A%2528e.g.%252C%2520like%252C%2520dislike%252C%2520share%2529%2520in%2520a%2520Q/A%2520format.%2520Participants%2520ranked%2520content%2520as%250Atruthful%2520in%2520the%2520order%2520of%2520genuine%252C%2520minor%2520hallucination%252C%2520and%2520major%2520hallucination%252C%250Aand%2520user%2520engagement%2520behaviors%2520mirrored%2520this%2520pattern.%2520More%2520importantly%252C%2520we%250Aobserved%2520that%2520warning%2520improved%2520the%2520detection%2520of%2520hallucination%2520without%250Asignificantly%2520affecting%2520the%2520perceived%2520truthfulness%2520of%2520genuine%2520content.%2520We%250Aconclude%2520by%2520offering%2520insights%2520for%2520future%2520tools%2520to%2520aid%2520human%2520detection%2520of%250Ahallucinations.%2520All%2520survey%2520materials%252C%2520demographic%2520questions%252C%2520and%2520post-session%250Aquestions%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/MahjabinNahar/fakes-of-varying-shades-survey-materials%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.03745v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fakes%20of%20Varying%20Shades%3A%20How%20Warning%20Affects%20Human%20Perception%20and%0A%20%20Engagement%20Regarding%20LLM%20Hallucinations&entry.906535625=Mahjabin%20Nahar%20and%20Haeseung%20Seo%20and%20Eun-Ju%20Lee%20and%20Aiping%20Xiong%20and%20Dongwon%20Lee&entry.1292438233=%20%20The%20widespread%20adoption%20and%20transformative%20effects%20of%20large%20language%20models%0A%28LLMs%29%20have%20sparked%20concerns%20regarding%20their%20capacity%20to%20produce%20inaccurate%20and%0Afictitious%20content%2C%20referred%20to%20as%20%60hallucinations%27.%20Given%20the%20potential%20risks%0Aassociated%20with%20hallucinations%2C%20humans%20should%20be%20able%20to%20identify%20them.%20This%0Aresearch%20aims%20to%20understand%20the%20human%20perception%20of%20LLM%20hallucinations%20by%0Asystematically%20varying%20the%20degree%20of%20hallucination%20%28genuine%2C%20minor%0Ahallucination%2C%20major%20hallucination%29%20and%20examining%20its%20interaction%20with%20warning%0A%28i.e.%2C%20a%20warning%20of%20potential%20inaccuracies%3A%20absent%20vs.%20present%29.%20Participants%0A%28N%3D419%29%20from%20Prolific%20rated%20the%20perceived%20accuracy%20and%20engaged%20with%20content%0A%28e.g.%2C%20like%2C%20dislike%2C%20share%29%20in%20a%20Q/A%20format.%20Participants%20ranked%20content%20as%0Atruthful%20in%20the%20order%20of%20genuine%2C%20minor%20hallucination%2C%20and%20major%20hallucination%2C%0Aand%20user%20engagement%20behaviors%20mirrored%20this%20pattern.%20More%20importantly%2C%20we%0Aobserved%20that%20warning%20improved%20the%20detection%20of%20hallucination%20without%0Asignificantly%20affecting%20the%20perceived%20truthfulness%20of%20genuine%20content.%20We%0Aconclude%20by%20offering%20insights%20for%20future%20tools%20to%20aid%20human%20detection%20of%0Ahallucinations.%20All%20survey%20materials%2C%20demographic%20questions%2C%20and%20post-session%0Aquestions%20are%20available%20at%3A%0Ahttps%3A//github.com/MahjabinNahar/fakes-of-varying-shades-survey-materials%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03745v3&entry.124074799=Read"},
{"title": "Strong and weak alignment of large language models with human values", "author": "Mehdi Khamassi and Marceau Nahon and Raja Chatila", "abstract": "  Minimizing negative impacts of Artificial Intelligent (AI) systems on human\nsocieties without human supervision requires them to be able to align with\nhuman values. However, most current work only addresses this issue from a\ntechnical point of view, e.g., improving current methods relying on\nreinforcement learning from human feedback, neglecting what it means and is\nrequired for alignment to occur. Here, we propose to distinguish strong and\nweak value alignment. Strong alignment requires cognitive abilities (either\nhuman-like or different from humans) such as understanding and reasoning about\nagents' intentions and their ability to causally produce desired effects. We\nargue that this is required for AI systems like large language models (LLMs) to\nbe able to recognize situations presenting a risk that human values may be\nflouted. To illustrate this distinction, we present a series of prompts showing\nChatGPT's, Gemini's and Copilot's failures to recognize some of these\nsituations. We moreover analyze word embeddings to show that the nearest\nneighbors of some human values in LLMs differ from humans' semantic\nrepresentations. We then propose a new thought experiment that we call \"the\nChinese room with a word transition dictionary\", in extension of John Searle's\nfamous proposal. We finally mention current promising research directions\ntowards a weak alignment, which could produce statistically satisfying answers\nin a number of common situations, however so far without ensuring any truth\nvalue.\n", "link": "http://arxiv.org/abs/2408.04655v2", "date": "2024-08-12", "relevancy": 1.7994, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.464}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4474}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Strong%20and%20weak%20alignment%20of%20large%20language%20models%20with%20human%20values&body=Title%3A%20Strong%20and%20weak%20alignment%20of%20large%20language%20models%20with%20human%20values%0AAuthor%3A%20Mehdi%20Khamassi%20and%20Marceau%20Nahon%20and%20Raja%20Chatila%0AAbstract%3A%20%20%20Minimizing%20negative%20impacts%20of%20Artificial%20Intelligent%20%28AI%29%20systems%20on%20human%0Asocieties%20without%20human%20supervision%20requires%20them%20to%20be%20able%20to%20align%20with%0Ahuman%20values.%20However%2C%20most%20current%20work%20only%20addresses%20this%20issue%20from%20a%0Atechnical%20point%20of%20view%2C%20e.g.%2C%20improving%20current%20methods%20relying%20on%0Areinforcement%20learning%20from%20human%20feedback%2C%20neglecting%20what%20it%20means%20and%20is%0Arequired%20for%20alignment%20to%20occur.%20Here%2C%20we%20propose%20to%20distinguish%20strong%20and%0Aweak%20value%20alignment.%20Strong%20alignment%20requires%20cognitive%20abilities%20%28either%0Ahuman-like%20or%20different%20from%20humans%29%20such%20as%20understanding%20and%20reasoning%20about%0Aagents%27%20intentions%20and%20their%20ability%20to%20causally%20produce%20desired%20effects.%20We%0Aargue%20that%20this%20is%20required%20for%20AI%20systems%20like%20large%20language%20models%20%28LLMs%29%20to%0Abe%20able%20to%20recognize%20situations%20presenting%20a%20risk%20that%20human%20values%20may%20be%0Aflouted.%20To%20illustrate%20this%20distinction%2C%20we%20present%20a%20series%20of%20prompts%20showing%0AChatGPT%27s%2C%20Gemini%27s%20and%20Copilot%27s%20failures%20to%20recognize%20some%20of%20these%0Asituations.%20We%20moreover%20analyze%20word%20embeddings%20to%20show%20that%20the%20nearest%0Aneighbors%20of%20some%20human%20values%20in%20LLMs%20differ%20from%20humans%27%20semantic%0Arepresentations.%20We%20then%20propose%20a%20new%20thought%20experiment%20that%20we%20call%20%22the%0AChinese%20room%20with%20a%20word%20transition%20dictionary%22%2C%20in%20extension%20of%20John%20Searle%27s%0Afamous%20proposal.%20We%20finally%20mention%20current%20promising%20research%20directions%0Atowards%20a%20weak%20alignment%2C%20which%20could%20produce%20statistically%20satisfying%20answers%0Ain%20a%20number%20of%20common%20situations%2C%20however%20so%20far%20without%20ensuring%20any%20truth%0Avalue.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04655v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStrong%2520and%2520weak%2520alignment%2520of%2520large%2520language%2520models%2520with%2520human%2520values%26entry.906535625%3DMehdi%2520Khamassi%2520and%2520Marceau%2520Nahon%2520and%2520Raja%2520Chatila%26entry.1292438233%3D%2520%2520Minimizing%2520negative%2520impacts%2520of%2520Artificial%2520Intelligent%2520%2528AI%2529%2520systems%2520on%2520human%250Asocieties%2520without%2520human%2520supervision%2520requires%2520them%2520to%2520be%2520able%2520to%2520align%2520with%250Ahuman%2520values.%2520However%252C%2520most%2520current%2520work%2520only%2520addresses%2520this%2520issue%2520from%2520a%250Atechnical%2520point%2520of%2520view%252C%2520e.g.%252C%2520improving%2520current%2520methods%2520relying%2520on%250Areinforcement%2520learning%2520from%2520human%2520feedback%252C%2520neglecting%2520what%2520it%2520means%2520and%2520is%250Arequired%2520for%2520alignment%2520to%2520occur.%2520Here%252C%2520we%2520propose%2520to%2520distinguish%2520strong%2520and%250Aweak%2520value%2520alignment.%2520Strong%2520alignment%2520requires%2520cognitive%2520abilities%2520%2528either%250Ahuman-like%2520or%2520different%2520from%2520humans%2529%2520such%2520as%2520understanding%2520and%2520reasoning%2520about%250Aagents%2527%2520intentions%2520and%2520their%2520ability%2520to%2520causally%2520produce%2520desired%2520effects.%2520We%250Aargue%2520that%2520this%2520is%2520required%2520for%2520AI%2520systems%2520like%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%250Abe%2520able%2520to%2520recognize%2520situations%2520presenting%2520a%2520risk%2520that%2520human%2520values%2520may%2520be%250Aflouted.%2520To%2520illustrate%2520this%2520distinction%252C%2520we%2520present%2520a%2520series%2520of%2520prompts%2520showing%250AChatGPT%2527s%252C%2520Gemini%2527s%2520and%2520Copilot%2527s%2520failures%2520to%2520recognize%2520some%2520of%2520these%250Asituations.%2520We%2520moreover%2520analyze%2520word%2520embeddings%2520to%2520show%2520that%2520the%2520nearest%250Aneighbors%2520of%2520some%2520human%2520values%2520in%2520LLMs%2520differ%2520from%2520humans%2527%2520semantic%250Arepresentations.%2520We%2520then%2520propose%2520a%2520new%2520thought%2520experiment%2520that%2520we%2520call%2520%2522the%250AChinese%2520room%2520with%2520a%2520word%2520transition%2520dictionary%2522%252C%2520in%2520extension%2520of%2520John%2520Searle%2527s%250Afamous%2520proposal.%2520We%2520finally%2520mention%2520current%2520promising%2520research%2520directions%250Atowards%2520a%2520weak%2520alignment%252C%2520which%2520could%2520produce%2520statistically%2520satisfying%2520answers%250Ain%2520a%2520number%2520of%2520common%2520situations%252C%2520however%2520so%2520far%2520without%2520ensuring%2520any%2520truth%250Avalue.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04655v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Strong%20and%20weak%20alignment%20of%20large%20language%20models%20with%20human%20values&entry.906535625=Mehdi%20Khamassi%20and%20Marceau%20Nahon%20and%20Raja%20Chatila&entry.1292438233=%20%20Minimizing%20negative%20impacts%20of%20Artificial%20Intelligent%20%28AI%29%20systems%20on%20human%0Asocieties%20without%20human%20supervision%20requires%20them%20to%20be%20able%20to%20align%20with%0Ahuman%20values.%20However%2C%20most%20current%20work%20only%20addresses%20this%20issue%20from%20a%0Atechnical%20point%20of%20view%2C%20e.g.%2C%20improving%20current%20methods%20relying%20on%0Areinforcement%20learning%20from%20human%20feedback%2C%20neglecting%20what%20it%20means%20and%20is%0Arequired%20for%20alignment%20to%20occur.%20Here%2C%20we%20propose%20to%20distinguish%20strong%20and%0Aweak%20value%20alignment.%20Strong%20alignment%20requires%20cognitive%20abilities%20%28either%0Ahuman-like%20or%20different%20from%20humans%29%20such%20as%20understanding%20and%20reasoning%20about%0Aagents%27%20intentions%20and%20their%20ability%20to%20causally%20produce%20desired%20effects.%20We%0Aargue%20that%20this%20is%20required%20for%20AI%20systems%20like%20large%20language%20models%20%28LLMs%29%20to%0Abe%20able%20to%20recognize%20situations%20presenting%20a%20risk%20that%20human%20values%20may%20be%0Aflouted.%20To%20illustrate%20this%20distinction%2C%20we%20present%20a%20series%20of%20prompts%20showing%0AChatGPT%27s%2C%20Gemini%27s%20and%20Copilot%27s%20failures%20to%20recognize%20some%20of%20these%0Asituations.%20We%20moreover%20analyze%20word%20embeddings%20to%20show%20that%20the%20nearest%0Aneighbors%20of%20some%20human%20values%20in%20LLMs%20differ%20from%20humans%27%20semantic%0Arepresentations.%20We%20then%20propose%20a%20new%20thought%20experiment%20that%20we%20call%20%22the%0AChinese%20room%20with%20a%20word%20transition%20dictionary%22%2C%20in%20extension%20of%20John%20Searle%27s%0Afamous%20proposal.%20We%20finally%20mention%20current%20promising%20research%20directions%0Atowards%20a%20weak%20alignment%2C%20which%20could%20produce%20statistically%20satisfying%20answers%0Ain%20a%20number%20of%20common%20situations%2C%20however%20so%20far%20without%20ensuring%20any%20truth%0Avalue.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04655v2&entry.124074799=Read"},
{"title": "LOLgorithm: Integrating Semantic,Syntactic and Contextual Elements for\n  Humor Classification", "author": "Tanisha Khurana and Kaushik Pillalamarri and Vikram Pande and Munindar Singh", "abstract": "  This paper explores humor detection through a linguistic lens, prioritizing\nsyntactic, semantic, and contextual features over computational methods in\nNatural Language Processing. We categorize features into syntactic, semantic,\nand contextual dimensions, including lexicons, structural statistics, Word2Vec,\nWordNet, and phonetic style. Our proposed model, Colbert, utilizes BERT\nembeddings and parallel hidden layers to capture sentence congruity. By\ncombining syntactic, semantic, and contextual features, we train Colbert for\nhumor detection. Feature engineering examines essential syntactic and semantic\nfeatures alongside BERT embeddings. SHAP interpretations and decision trees\nidentify influential features, revealing that a holistic approach improves\nhumor detection accuracy on unseen data. Integrating linguistic cues from\ndifferent dimensions enhances the model's ability to understand humor\ncomplexity beyond traditional computational methods.\n", "link": "http://arxiv.org/abs/2408.06335v1", "date": "2024-08-12", "relevancy": 1.7886, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4531}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4469}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LOLgorithm%3A%20Integrating%20Semantic%2CSyntactic%20and%20Contextual%20Elements%20for%0A%20%20Humor%20Classification&body=Title%3A%20LOLgorithm%3A%20Integrating%20Semantic%2CSyntactic%20and%20Contextual%20Elements%20for%0A%20%20Humor%20Classification%0AAuthor%3A%20Tanisha%20Khurana%20and%20Kaushik%20Pillalamarri%20and%20Vikram%20Pande%20and%20Munindar%20Singh%0AAbstract%3A%20%20%20This%20paper%20explores%20humor%20detection%20through%20a%20linguistic%20lens%2C%20prioritizing%0Asyntactic%2C%20semantic%2C%20and%20contextual%20features%20over%20computational%20methods%20in%0ANatural%20Language%20Processing.%20We%20categorize%20features%20into%20syntactic%2C%20semantic%2C%0Aand%20contextual%20dimensions%2C%20including%20lexicons%2C%20structural%20statistics%2C%20Word2Vec%2C%0AWordNet%2C%20and%20phonetic%20style.%20Our%20proposed%20model%2C%20Colbert%2C%20utilizes%20BERT%0Aembeddings%20and%20parallel%20hidden%20layers%20to%20capture%20sentence%20congruity.%20By%0Acombining%20syntactic%2C%20semantic%2C%20and%20contextual%20features%2C%20we%20train%20Colbert%20for%0Ahumor%20detection.%20Feature%20engineering%20examines%20essential%20syntactic%20and%20semantic%0Afeatures%20alongside%20BERT%20embeddings.%20SHAP%20interpretations%20and%20decision%20trees%0Aidentify%20influential%20features%2C%20revealing%20that%20a%20holistic%20approach%20improves%0Ahumor%20detection%20accuracy%20on%20unseen%20data.%20Integrating%20linguistic%20cues%20from%0Adifferent%20dimensions%20enhances%20the%20model%27s%20ability%20to%20understand%20humor%0Acomplexity%20beyond%20traditional%20computational%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06335v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLOLgorithm%253A%2520Integrating%2520Semantic%252CSyntactic%2520and%2520Contextual%2520Elements%2520for%250A%2520%2520Humor%2520Classification%26entry.906535625%3DTanisha%2520Khurana%2520and%2520Kaushik%2520Pillalamarri%2520and%2520Vikram%2520Pande%2520and%2520Munindar%2520Singh%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520humor%2520detection%2520through%2520a%2520linguistic%2520lens%252C%2520prioritizing%250Asyntactic%252C%2520semantic%252C%2520and%2520contextual%2520features%2520over%2520computational%2520methods%2520in%250ANatural%2520Language%2520Processing.%2520We%2520categorize%2520features%2520into%2520syntactic%252C%2520semantic%252C%250Aand%2520contextual%2520dimensions%252C%2520including%2520lexicons%252C%2520structural%2520statistics%252C%2520Word2Vec%252C%250AWordNet%252C%2520and%2520phonetic%2520style.%2520Our%2520proposed%2520model%252C%2520Colbert%252C%2520utilizes%2520BERT%250Aembeddings%2520and%2520parallel%2520hidden%2520layers%2520to%2520capture%2520sentence%2520congruity.%2520By%250Acombining%2520syntactic%252C%2520semantic%252C%2520and%2520contextual%2520features%252C%2520we%2520train%2520Colbert%2520for%250Ahumor%2520detection.%2520Feature%2520engineering%2520examines%2520essential%2520syntactic%2520and%2520semantic%250Afeatures%2520alongside%2520BERT%2520embeddings.%2520SHAP%2520interpretations%2520and%2520decision%2520trees%250Aidentify%2520influential%2520features%252C%2520revealing%2520that%2520a%2520holistic%2520approach%2520improves%250Ahumor%2520detection%2520accuracy%2520on%2520unseen%2520data.%2520Integrating%2520linguistic%2520cues%2520from%250Adifferent%2520dimensions%2520enhances%2520the%2520model%2527s%2520ability%2520to%2520understand%2520humor%250Acomplexity%2520beyond%2520traditional%2520computational%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06335v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LOLgorithm%3A%20Integrating%20Semantic%2CSyntactic%20and%20Contextual%20Elements%20for%0A%20%20Humor%20Classification&entry.906535625=Tanisha%20Khurana%20and%20Kaushik%20Pillalamarri%20and%20Vikram%20Pande%20and%20Munindar%20Singh&entry.1292438233=%20%20This%20paper%20explores%20humor%20detection%20through%20a%20linguistic%20lens%2C%20prioritizing%0Asyntactic%2C%20semantic%2C%20and%20contextual%20features%20over%20computational%20methods%20in%0ANatural%20Language%20Processing.%20We%20categorize%20features%20into%20syntactic%2C%20semantic%2C%0Aand%20contextual%20dimensions%2C%20including%20lexicons%2C%20structural%20statistics%2C%20Word2Vec%2C%0AWordNet%2C%20and%20phonetic%20style.%20Our%20proposed%20model%2C%20Colbert%2C%20utilizes%20BERT%0Aembeddings%20and%20parallel%20hidden%20layers%20to%20capture%20sentence%20congruity.%20By%0Acombining%20syntactic%2C%20semantic%2C%20and%20contextual%20features%2C%20we%20train%20Colbert%20for%0Ahumor%20detection.%20Feature%20engineering%20examines%20essential%20syntactic%20and%20semantic%0Afeatures%20alongside%20BERT%20embeddings.%20SHAP%20interpretations%20and%20decision%20trees%0Aidentify%20influential%20features%2C%20revealing%20that%20a%20holistic%20approach%20improves%0Ahumor%20detection%20accuracy%20on%20unseen%20data.%20Integrating%20linguistic%20cues%20from%0Adifferent%20dimensions%20enhances%20the%20model%27s%20ability%20to%20understand%20humor%0Acomplexity%20beyond%20traditional%20computational%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06335v1&entry.124074799=Read"},
{"title": "Synthetic Patient-Physician Dialogue Generation from Clinical Notes\n  Using LLM", "author": "Trisha Das and Dina Albassam and Jimeng Sun", "abstract": "  Medical dialogue systems (MDS) enhance patient-physician communication,\nimprove healthcare accessibility, and reduce costs. However, acquiring suitable\ndata to train these systems poses significant challenges. Privacy concerns\nprevent the use of real conversations, necessitating synthetic alternatives.\nSynthetic dialogue generation from publicly available clinical notes offers a\npromising solution to this issue, providing realistic data while safeguarding\nprivacy. Our approach, SynDial, uses a single LLM iteratively with zero-shot\nprompting and a feedback loop to generate and refine high-quality synthetic\ndialogues. The feedback consists of weighted evaluation scores for similarity\nand extractiveness. The iterative process ensures dialogues meet predefined\nthresholds, achieving superior extractiveness as a result of the feedback loop.\nAdditionally, evaluation shows that the generated dialogues excel in factuality\nmetric compared to the baselines and has comparable diversity scores with GPT4.\n", "link": "http://arxiv.org/abs/2408.06285v1", "date": "2024-08-12", "relevancy": 1.7746, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4507}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4471}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4374}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthetic%20Patient-Physician%20Dialogue%20Generation%20from%20Clinical%20Notes%0A%20%20Using%20LLM&body=Title%3A%20Synthetic%20Patient-Physician%20Dialogue%20Generation%20from%20Clinical%20Notes%0A%20%20Using%20LLM%0AAuthor%3A%20Trisha%20Das%20and%20Dina%20Albassam%20and%20Jimeng%20Sun%0AAbstract%3A%20%20%20Medical%20dialogue%20systems%20%28MDS%29%20enhance%20patient-physician%20communication%2C%0Aimprove%20healthcare%20accessibility%2C%20and%20reduce%20costs.%20However%2C%20acquiring%20suitable%0Adata%20to%20train%20these%20systems%20poses%20significant%20challenges.%20Privacy%20concerns%0Aprevent%20the%20use%20of%20real%20conversations%2C%20necessitating%20synthetic%20alternatives.%0ASynthetic%20dialogue%20generation%20from%20publicly%20available%20clinical%20notes%20offers%20a%0Apromising%20solution%20to%20this%20issue%2C%20providing%20realistic%20data%20while%20safeguarding%0Aprivacy.%20Our%20approach%2C%20SynDial%2C%20uses%20a%20single%20LLM%20iteratively%20with%20zero-shot%0Aprompting%20and%20a%20feedback%20loop%20to%20generate%20and%20refine%20high-quality%20synthetic%0Adialogues.%20The%20feedback%20consists%20of%20weighted%20evaluation%20scores%20for%20similarity%0Aand%20extractiveness.%20The%20iterative%20process%20ensures%20dialogues%20meet%20predefined%0Athresholds%2C%20achieving%20superior%20extractiveness%20as%20a%20result%20of%20the%20feedback%20loop.%0AAdditionally%2C%20evaluation%20shows%20that%20the%20generated%20dialogues%20excel%20in%20factuality%0Ametric%20compared%20to%20the%20baselines%20and%20has%20comparable%20diversity%20scores%20with%20GPT4.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06285v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthetic%2520Patient-Physician%2520Dialogue%2520Generation%2520from%2520Clinical%2520Notes%250A%2520%2520Using%2520LLM%26entry.906535625%3DTrisha%2520Das%2520and%2520Dina%2520Albassam%2520and%2520Jimeng%2520Sun%26entry.1292438233%3D%2520%2520Medical%2520dialogue%2520systems%2520%2528MDS%2529%2520enhance%2520patient-physician%2520communication%252C%250Aimprove%2520healthcare%2520accessibility%252C%2520and%2520reduce%2520costs.%2520However%252C%2520acquiring%2520suitable%250Adata%2520to%2520train%2520these%2520systems%2520poses%2520significant%2520challenges.%2520Privacy%2520concerns%250Aprevent%2520the%2520use%2520of%2520real%2520conversations%252C%2520necessitating%2520synthetic%2520alternatives.%250ASynthetic%2520dialogue%2520generation%2520from%2520publicly%2520available%2520clinical%2520notes%2520offers%2520a%250Apromising%2520solution%2520to%2520this%2520issue%252C%2520providing%2520realistic%2520data%2520while%2520safeguarding%250Aprivacy.%2520Our%2520approach%252C%2520SynDial%252C%2520uses%2520a%2520single%2520LLM%2520iteratively%2520with%2520zero-shot%250Aprompting%2520and%2520a%2520feedback%2520loop%2520to%2520generate%2520and%2520refine%2520high-quality%2520synthetic%250Adialogues.%2520The%2520feedback%2520consists%2520of%2520weighted%2520evaluation%2520scores%2520for%2520similarity%250Aand%2520extractiveness.%2520The%2520iterative%2520process%2520ensures%2520dialogues%2520meet%2520predefined%250Athresholds%252C%2520achieving%2520superior%2520extractiveness%2520as%2520a%2520result%2520of%2520the%2520feedback%2520loop.%250AAdditionally%252C%2520evaluation%2520shows%2520that%2520the%2520generated%2520dialogues%2520excel%2520in%2520factuality%250Ametric%2520compared%2520to%2520the%2520baselines%2520and%2520has%2520comparable%2520diversity%2520scores%2520with%2520GPT4.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06285v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthetic%20Patient-Physician%20Dialogue%20Generation%20from%20Clinical%20Notes%0A%20%20Using%20LLM&entry.906535625=Trisha%20Das%20and%20Dina%20Albassam%20and%20Jimeng%20Sun&entry.1292438233=%20%20Medical%20dialogue%20systems%20%28MDS%29%20enhance%20patient-physician%20communication%2C%0Aimprove%20healthcare%20accessibility%2C%20and%20reduce%20costs.%20However%2C%20acquiring%20suitable%0Adata%20to%20train%20these%20systems%20poses%20significant%20challenges.%20Privacy%20concerns%0Aprevent%20the%20use%20of%20real%20conversations%2C%20necessitating%20synthetic%20alternatives.%0ASynthetic%20dialogue%20generation%20from%20publicly%20available%20clinical%20notes%20offers%20a%0Apromising%20solution%20to%20this%20issue%2C%20providing%20realistic%20data%20while%20safeguarding%0Aprivacy.%20Our%20approach%2C%20SynDial%2C%20uses%20a%20single%20LLM%20iteratively%20with%20zero-shot%0Aprompting%20and%20a%20feedback%20loop%20to%20generate%20and%20refine%20high-quality%20synthetic%0Adialogues.%20The%20feedback%20consists%20of%20weighted%20evaluation%20scores%20for%20similarity%0Aand%20extractiveness.%20The%20iterative%20process%20ensures%20dialogues%20meet%20predefined%0Athresholds%2C%20achieving%20superior%20extractiveness%20as%20a%20result%20of%20the%20feedback%20loop.%0AAdditionally%2C%20evaluation%20shows%20that%20the%20generated%20dialogues%20excel%20in%20factuality%0Ametric%20compared%20to%20the%20baselines%20and%20has%20comparable%20diversity%20scores%20with%20GPT4.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06285v1&entry.124074799=Read"},
{"title": "Non-Stationary Latent Auto-Regressive Bandits", "author": "Anna L. Trella and Walter Dempsey and Finale Doshi-Velez and Susan A. Murphy", "abstract": "  We consider the stochastic multi-armed bandit problem with non-stationary\nrewards. We present a novel formulation of non-stationarity in the environment\nwhere changes in the mean reward of the arms over time are due to some unknown,\nlatent, auto-regressive (AR) state of order $k$. We call this new environment\nthe latent AR bandit. Different forms of the latent AR bandit appear in many\nreal-world settings, especially in emerging scientific fields such as\nbehavioral health or education where there are few mechanistic models of the\nenvironment. If the AR order $k$ is known, we propose an algorithm that\nachieves $\\tilde{O}(k\\sqrt{T})$ regret in this setting. Empirically, our\nalgorithm outperforms standard UCB across multiple non-stationary environments,\neven if $k$ is mis-specified.\n", "link": "http://arxiv.org/abs/2402.03110v2", "date": "2024-08-12", "relevancy": 1.7577, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4697}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4486}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Non-Stationary%20Latent%20Auto-Regressive%20Bandits&body=Title%3A%20Non-Stationary%20Latent%20Auto-Regressive%20Bandits%0AAuthor%3A%20Anna%20L.%20Trella%20and%20Walter%20Dempsey%20and%20Finale%20Doshi-Velez%20and%20Susan%20A.%20Murphy%0AAbstract%3A%20%20%20We%20consider%20the%20stochastic%20multi-armed%20bandit%20problem%20with%20non-stationary%0Arewards.%20We%20present%20a%20novel%20formulation%20of%20non-stationarity%20in%20the%20environment%0Awhere%20changes%20in%20the%20mean%20reward%20of%20the%20arms%20over%20time%20are%20due%20to%20some%20unknown%2C%0Alatent%2C%20auto-regressive%20%28AR%29%20state%20of%20order%20%24k%24.%20We%20call%20this%20new%20environment%0Athe%20latent%20AR%20bandit.%20Different%20forms%20of%20the%20latent%20AR%20bandit%20appear%20in%20many%0Areal-world%20settings%2C%20especially%20in%20emerging%20scientific%20fields%20such%20as%0Abehavioral%20health%20or%20education%20where%20there%20are%20few%20mechanistic%20models%20of%20the%0Aenvironment.%20If%20the%20AR%20order%20%24k%24%20is%20known%2C%20we%20propose%20an%20algorithm%20that%0Aachieves%20%24%5Ctilde%7BO%7D%28k%5Csqrt%7BT%7D%29%24%20regret%20in%20this%20setting.%20Empirically%2C%20our%0Aalgorithm%20outperforms%20standard%20UCB%20across%20multiple%20non-stationary%20environments%2C%0Aeven%20if%20%24k%24%20is%20mis-specified.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03110v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNon-Stationary%2520Latent%2520Auto-Regressive%2520Bandits%26entry.906535625%3DAnna%2520L.%2520Trella%2520and%2520Walter%2520Dempsey%2520and%2520Finale%2520Doshi-Velez%2520and%2520Susan%2520A.%2520Murphy%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520stochastic%2520multi-armed%2520bandit%2520problem%2520with%2520non-stationary%250Arewards.%2520We%2520present%2520a%2520novel%2520formulation%2520of%2520non-stationarity%2520in%2520the%2520environment%250Awhere%2520changes%2520in%2520the%2520mean%2520reward%2520of%2520the%2520arms%2520over%2520time%2520are%2520due%2520to%2520some%2520unknown%252C%250Alatent%252C%2520auto-regressive%2520%2528AR%2529%2520state%2520of%2520order%2520%2524k%2524.%2520We%2520call%2520this%2520new%2520environment%250Athe%2520latent%2520AR%2520bandit.%2520Different%2520forms%2520of%2520the%2520latent%2520AR%2520bandit%2520appear%2520in%2520many%250Areal-world%2520settings%252C%2520especially%2520in%2520emerging%2520scientific%2520fields%2520such%2520as%250Abehavioral%2520health%2520or%2520education%2520where%2520there%2520are%2520few%2520mechanistic%2520models%2520of%2520the%250Aenvironment.%2520If%2520the%2520AR%2520order%2520%2524k%2524%2520is%2520known%252C%2520we%2520propose%2520an%2520algorithm%2520that%250Aachieves%2520%2524%255Ctilde%257BO%257D%2528k%255Csqrt%257BT%257D%2529%2524%2520regret%2520in%2520this%2520setting.%2520Empirically%252C%2520our%250Aalgorithm%2520outperforms%2520standard%2520UCB%2520across%2520multiple%2520non-stationary%2520environments%252C%250Aeven%2520if%2520%2524k%2524%2520is%2520mis-specified.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03110v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-Stationary%20Latent%20Auto-Regressive%20Bandits&entry.906535625=Anna%20L.%20Trella%20and%20Walter%20Dempsey%20and%20Finale%20Doshi-Velez%20and%20Susan%20A.%20Murphy&entry.1292438233=%20%20We%20consider%20the%20stochastic%20multi-armed%20bandit%20problem%20with%20non-stationary%0Arewards.%20We%20present%20a%20novel%20formulation%20of%20non-stationarity%20in%20the%20environment%0Awhere%20changes%20in%20the%20mean%20reward%20of%20the%20arms%20over%20time%20are%20due%20to%20some%20unknown%2C%0Alatent%2C%20auto-regressive%20%28AR%29%20state%20of%20order%20%24k%24.%20We%20call%20this%20new%20environment%0Athe%20latent%20AR%20bandit.%20Different%20forms%20of%20the%20latent%20AR%20bandit%20appear%20in%20many%0Areal-world%20settings%2C%20especially%20in%20emerging%20scientific%20fields%20such%20as%0Abehavioral%20health%20or%20education%20where%20there%20are%20few%20mechanistic%20models%20of%20the%0Aenvironment.%20If%20the%20AR%20order%20%24k%24%20is%20known%2C%20we%20propose%20an%20algorithm%20that%0Aachieves%20%24%5Ctilde%7BO%7D%28k%5Csqrt%7BT%7D%29%24%20regret%20in%20this%20setting.%20Empirically%2C%20our%0Aalgorithm%20outperforms%20standard%20UCB%20across%20multiple%20non-stationary%20environments%2C%0Aeven%20if%20%24k%24%20is%20mis-specified.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03110v2&entry.124074799=Read"},
{"title": "LPGen: Enhancing High-Fidelity Landscape Painting Generation through\n  Diffusion Model", "author": "Wanggong Yang and Xiaona Wang and Yingrui Qiu and Yifei Zhao", "abstract": "  Generating landscape paintings expands the possibilities of artistic\ncreativity and imagination. Traditional landscape painting methods involve\nusing ink or colored ink on rice paper, which requires substantial time and\neffort. These methods are susceptible to errors and inconsistencies and lack\nprecise control over lines and colors. This paper presents LPGen, a\nhigh-fidelity, controllable model for landscape painting generation,\nintroducing a novel multi-modal framework that integrates image prompts into\nthe diffusion model. We extract its edges and contours by computing canny edges\nfrom the target landscape image. These, along with natural language text\nprompts and drawing style references, are fed into the latent diffusion model\nas conditions. We implement a decoupled cross-attention strategy to ensure\ncompatibility between image and text prompts, facilitating multi-modal image\ngeneration. A decoder generates the final image. Quantitative and qualitative\nanalyses demonstrate that our method outperforms existing approaches in\nlandscape painting generation and exceeds the current state-of-the-art. The\nLPGen network effectively controls the composition and color of landscape\npaintings, generates more accurate images, and supports further research in\ndeep learning-based landscape painting generation.\n", "link": "http://arxiv.org/abs/2407.17229v3", "date": "2024-08-12", "relevancy": 1.7354, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5903}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5805}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LPGen%3A%20Enhancing%20High-Fidelity%20Landscape%20Painting%20Generation%20through%0A%20%20Diffusion%20Model&body=Title%3A%20LPGen%3A%20Enhancing%20High-Fidelity%20Landscape%20Painting%20Generation%20through%0A%20%20Diffusion%20Model%0AAuthor%3A%20Wanggong%20Yang%20and%20Xiaona%20Wang%20and%20Yingrui%20Qiu%20and%20Yifei%20Zhao%0AAbstract%3A%20%20%20Generating%20landscape%20paintings%20expands%20the%20possibilities%20of%20artistic%0Acreativity%20and%20imagination.%20Traditional%20landscape%20painting%20methods%20involve%0Ausing%20ink%20or%20colored%20ink%20on%20rice%20paper%2C%20which%20requires%20substantial%20time%20and%0Aeffort.%20These%20methods%20are%20susceptible%20to%20errors%20and%20inconsistencies%20and%20lack%0Aprecise%20control%20over%20lines%20and%20colors.%20This%20paper%20presents%20LPGen%2C%20a%0Ahigh-fidelity%2C%20controllable%20model%20for%20landscape%20painting%20generation%2C%0Aintroducing%20a%20novel%20multi-modal%20framework%20that%20integrates%20image%20prompts%20into%0Athe%20diffusion%20model.%20We%20extract%20its%20edges%20and%20contours%20by%20computing%20canny%20edges%0Afrom%20the%20target%20landscape%20image.%20These%2C%20along%20with%20natural%20language%20text%0Aprompts%20and%20drawing%20style%20references%2C%20are%20fed%20into%20the%20latent%20diffusion%20model%0Aas%20conditions.%20We%20implement%20a%20decoupled%20cross-attention%20strategy%20to%20ensure%0Acompatibility%20between%20image%20and%20text%20prompts%2C%20facilitating%20multi-modal%20image%0Ageneration.%20A%20decoder%20generates%20the%20final%20image.%20Quantitative%20and%20qualitative%0Aanalyses%20demonstrate%20that%20our%20method%20outperforms%20existing%20approaches%20in%0Alandscape%20painting%20generation%20and%20exceeds%20the%20current%20state-of-the-art.%20The%0ALPGen%20network%20effectively%20controls%20the%20composition%20and%20color%20of%20landscape%0Apaintings%2C%20generates%20more%20accurate%20images%2C%20and%20supports%20further%20research%20in%0Adeep%20learning-based%20landscape%20painting%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17229v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLPGen%253A%2520Enhancing%2520High-Fidelity%2520Landscape%2520Painting%2520Generation%2520through%250A%2520%2520Diffusion%2520Model%26entry.906535625%3DWanggong%2520Yang%2520and%2520Xiaona%2520Wang%2520and%2520Yingrui%2520Qiu%2520and%2520Yifei%2520Zhao%26entry.1292438233%3D%2520%2520Generating%2520landscape%2520paintings%2520expands%2520the%2520possibilities%2520of%2520artistic%250Acreativity%2520and%2520imagination.%2520Traditional%2520landscape%2520painting%2520methods%2520involve%250Ausing%2520ink%2520or%2520colored%2520ink%2520on%2520rice%2520paper%252C%2520which%2520requires%2520substantial%2520time%2520and%250Aeffort.%2520These%2520methods%2520are%2520susceptible%2520to%2520errors%2520and%2520inconsistencies%2520and%2520lack%250Aprecise%2520control%2520over%2520lines%2520and%2520colors.%2520This%2520paper%2520presents%2520LPGen%252C%2520a%250Ahigh-fidelity%252C%2520controllable%2520model%2520for%2520landscape%2520painting%2520generation%252C%250Aintroducing%2520a%2520novel%2520multi-modal%2520framework%2520that%2520integrates%2520image%2520prompts%2520into%250Athe%2520diffusion%2520model.%2520We%2520extract%2520its%2520edges%2520and%2520contours%2520by%2520computing%2520canny%2520edges%250Afrom%2520the%2520target%2520landscape%2520image.%2520These%252C%2520along%2520with%2520natural%2520language%2520text%250Aprompts%2520and%2520drawing%2520style%2520references%252C%2520are%2520fed%2520into%2520the%2520latent%2520diffusion%2520model%250Aas%2520conditions.%2520We%2520implement%2520a%2520decoupled%2520cross-attention%2520strategy%2520to%2520ensure%250Acompatibility%2520between%2520image%2520and%2520text%2520prompts%252C%2520facilitating%2520multi-modal%2520image%250Ageneration.%2520A%2520decoder%2520generates%2520the%2520final%2520image.%2520Quantitative%2520and%2520qualitative%250Aanalyses%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520existing%2520approaches%2520in%250Alandscape%2520painting%2520generation%2520and%2520exceeds%2520the%2520current%2520state-of-the-art.%2520The%250ALPGen%2520network%2520effectively%2520controls%2520the%2520composition%2520and%2520color%2520of%2520landscape%250Apaintings%252C%2520generates%2520more%2520accurate%2520images%252C%2520and%2520supports%2520further%2520research%2520in%250Adeep%2520learning-based%2520landscape%2520painting%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17229v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LPGen%3A%20Enhancing%20High-Fidelity%20Landscape%20Painting%20Generation%20through%0A%20%20Diffusion%20Model&entry.906535625=Wanggong%20Yang%20and%20Xiaona%20Wang%20and%20Yingrui%20Qiu%20and%20Yifei%20Zhao&entry.1292438233=%20%20Generating%20landscape%20paintings%20expands%20the%20possibilities%20of%20artistic%0Acreativity%20and%20imagination.%20Traditional%20landscape%20painting%20methods%20involve%0Ausing%20ink%20or%20colored%20ink%20on%20rice%20paper%2C%20which%20requires%20substantial%20time%20and%0Aeffort.%20These%20methods%20are%20susceptible%20to%20errors%20and%20inconsistencies%20and%20lack%0Aprecise%20control%20over%20lines%20and%20colors.%20This%20paper%20presents%20LPGen%2C%20a%0Ahigh-fidelity%2C%20controllable%20model%20for%20landscape%20painting%20generation%2C%0Aintroducing%20a%20novel%20multi-modal%20framework%20that%20integrates%20image%20prompts%20into%0Athe%20diffusion%20model.%20We%20extract%20its%20edges%20and%20contours%20by%20computing%20canny%20edges%0Afrom%20the%20target%20landscape%20image.%20These%2C%20along%20with%20natural%20language%20text%0Aprompts%20and%20drawing%20style%20references%2C%20are%20fed%20into%20the%20latent%20diffusion%20model%0Aas%20conditions.%20We%20implement%20a%20decoupled%20cross-attention%20strategy%20to%20ensure%0Acompatibility%20between%20image%20and%20text%20prompts%2C%20facilitating%20multi-modal%20image%0Ageneration.%20A%20decoder%20generates%20the%20final%20image.%20Quantitative%20and%20qualitative%0Aanalyses%20demonstrate%20that%20our%20method%20outperforms%20existing%20approaches%20in%0Alandscape%20painting%20generation%20and%20exceeds%20the%20current%20state-of-the-art.%20The%0ALPGen%20network%20effectively%20controls%20the%20composition%20and%20color%20of%20landscape%0Apaintings%2C%20generates%20more%20accurate%20images%2C%20and%20supports%20further%20research%20in%0Adeep%20learning-based%20landscape%20painting%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17229v3&entry.124074799=Read"},
{"title": "Latent Disentanglement for Low Light Image Enhancement", "author": "Zhihao Zheng and Mooi Choo Chuah", "abstract": "  Many learning-based low-light image enhancement (LLIE) algorithms are based\non the Retinex theory. However, the Retinex-based decomposition techniques in\nsuch models introduce corruptions which limit their enhancement performance. In\nthis paper, we propose a Latent Disentangle-based Enhancement Network (LDE-Net)\nfor low light vision tasks. The latent disentanglement module disentangles the\ninput image in latent space such that no corruption remains in the disentangled\nContent and Illumination components. For LLIE task, we design a Content-Aware\nEmbedding (CAE) module that utilizes Content features to direct the enhancement\nof the Illumination component. For downstream tasks (e.g. nighttime UAV\ntracking and low-light object detection), we develop an effective light-weight\nenhancer based on the latent disentanglement framework. Comprehensive\nquantitative and qualitative experiments demonstrate that our LDE-Net\nsignificantly outperforms state-of-the-art methods on various LLIE benchmarks.\nIn addition, the great results obtained by applying our framework on the\ndownstream tasks also demonstrate the usefulness of our latent disentanglement\ndesign.\n", "link": "http://arxiv.org/abs/2408.06245v1", "date": "2024-08-12", "relevancy": 1.7185, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5988}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5666}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latent%20Disentanglement%20for%20Low%20Light%20Image%20Enhancement&body=Title%3A%20Latent%20Disentanglement%20for%20Low%20Light%20Image%20Enhancement%0AAuthor%3A%20Zhihao%20Zheng%20and%20Mooi%20Choo%20Chuah%0AAbstract%3A%20%20%20Many%20learning-based%20low-light%20image%20enhancement%20%28LLIE%29%20algorithms%20are%20based%0Aon%20the%20Retinex%20theory.%20However%2C%20the%20Retinex-based%20decomposition%20techniques%20in%0Asuch%20models%20introduce%20corruptions%20which%20limit%20their%20enhancement%20performance.%20In%0Athis%20paper%2C%20we%20propose%20a%20Latent%20Disentangle-based%20Enhancement%20Network%20%28LDE-Net%29%0Afor%20low%20light%20vision%20tasks.%20The%20latent%20disentanglement%20module%20disentangles%20the%0Ainput%20image%20in%20latent%20space%20such%20that%20no%20corruption%20remains%20in%20the%20disentangled%0AContent%20and%20Illumination%20components.%20For%20LLIE%20task%2C%20we%20design%20a%20Content-Aware%0AEmbedding%20%28CAE%29%20module%20that%20utilizes%20Content%20features%20to%20direct%20the%20enhancement%0Aof%20the%20Illumination%20component.%20For%20downstream%20tasks%20%28e.g.%20nighttime%20UAV%0Atracking%20and%20low-light%20object%20detection%29%2C%20we%20develop%20an%20effective%20light-weight%0Aenhancer%20based%20on%20the%20latent%20disentanglement%20framework.%20Comprehensive%0Aquantitative%20and%20qualitative%20experiments%20demonstrate%20that%20our%20LDE-Net%0Asignificantly%20outperforms%20state-of-the-art%20methods%20on%20various%20LLIE%20benchmarks.%0AIn%20addition%2C%20the%20great%20results%20obtained%20by%20applying%20our%20framework%20on%20the%0Adownstream%20tasks%20also%20demonstrate%20the%20usefulness%20of%20our%20latent%20disentanglement%0Adesign.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06245v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatent%2520Disentanglement%2520for%2520Low%2520Light%2520Image%2520Enhancement%26entry.906535625%3DZhihao%2520Zheng%2520and%2520Mooi%2520Choo%2520Chuah%26entry.1292438233%3D%2520%2520Many%2520learning-based%2520low-light%2520image%2520enhancement%2520%2528LLIE%2529%2520algorithms%2520are%2520based%250Aon%2520the%2520Retinex%2520theory.%2520However%252C%2520the%2520Retinex-based%2520decomposition%2520techniques%2520in%250Asuch%2520models%2520introduce%2520corruptions%2520which%2520limit%2520their%2520enhancement%2520performance.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520a%2520Latent%2520Disentangle-based%2520Enhancement%2520Network%2520%2528LDE-Net%2529%250Afor%2520low%2520light%2520vision%2520tasks.%2520The%2520latent%2520disentanglement%2520module%2520disentangles%2520the%250Ainput%2520image%2520in%2520latent%2520space%2520such%2520that%2520no%2520corruption%2520remains%2520in%2520the%2520disentangled%250AContent%2520and%2520Illumination%2520components.%2520For%2520LLIE%2520task%252C%2520we%2520design%2520a%2520Content-Aware%250AEmbedding%2520%2528CAE%2529%2520module%2520that%2520utilizes%2520Content%2520features%2520to%2520direct%2520the%2520enhancement%250Aof%2520the%2520Illumination%2520component.%2520For%2520downstream%2520tasks%2520%2528e.g.%2520nighttime%2520UAV%250Atracking%2520and%2520low-light%2520object%2520detection%2529%252C%2520we%2520develop%2520an%2520effective%2520light-weight%250Aenhancer%2520based%2520on%2520the%2520latent%2520disentanglement%2520framework.%2520Comprehensive%250Aquantitative%2520and%2520qualitative%2520experiments%2520demonstrate%2520that%2520our%2520LDE-Net%250Asignificantly%2520outperforms%2520state-of-the-art%2520methods%2520on%2520various%2520LLIE%2520benchmarks.%250AIn%2520addition%252C%2520the%2520great%2520results%2520obtained%2520by%2520applying%2520our%2520framework%2520on%2520the%250Adownstream%2520tasks%2520also%2520demonstrate%2520the%2520usefulness%2520of%2520our%2520latent%2520disentanglement%250Adesign.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06245v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent%20Disentanglement%20for%20Low%20Light%20Image%20Enhancement&entry.906535625=Zhihao%20Zheng%20and%20Mooi%20Choo%20Chuah&entry.1292438233=%20%20Many%20learning-based%20low-light%20image%20enhancement%20%28LLIE%29%20algorithms%20are%20based%0Aon%20the%20Retinex%20theory.%20However%2C%20the%20Retinex-based%20decomposition%20techniques%20in%0Asuch%20models%20introduce%20corruptions%20which%20limit%20their%20enhancement%20performance.%20In%0Athis%20paper%2C%20we%20propose%20a%20Latent%20Disentangle-based%20Enhancement%20Network%20%28LDE-Net%29%0Afor%20low%20light%20vision%20tasks.%20The%20latent%20disentanglement%20module%20disentangles%20the%0Ainput%20image%20in%20latent%20space%20such%20that%20no%20corruption%20remains%20in%20the%20disentangled%0AContent%20and%20Illumination%20components.%20For%20LLIE%20task%2C%20we%20design%20a%20Content-Aware%0AEmbedding%20%28CAE%29%20module%20that%20utilizes%20Content%20features%20to%20direct%20the%20enhancement%0Aof%20the%20Illumination%20component.%20For%20downstream%20tasks%20%28e.g.%20nighttime%20UAV%0Atracking%20and%20low-light%20object%20detection%29%2C%20we%20develop%20an%20effective%20light-weight%0Aenhancer%20based%20on%20the%20latent%20disentanglement%20framework.%20Comprehensive%0Aquantitative%20and%20qualitative%20experiments%20demonstrate%20that%20our%20LDE-Net%0Asignificantly%20outperforms%20state-of-the-art%20methods%20on%20various%20LLIE%20benchmarks.%0AIn%20addition%2C%20the%20great%20results%20obtained%20by%20applying%20our%20framework%20on%20the%0Adownstream%20tasks%20also%20demonstrate%20the%20usefulness%20of%20our%20latent%20disentanglement%0Adesign.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06245v1&entry.124074799=Read"},
{"title": "VisualAgentBench: Towards Large Multimodal Models as Visual Foundation\n  Agents", "author": "Xiao Liu and Tianjie Zhang and Yu Gu and Iat Long Iong and Yifan Xu and Xixuan Song and Shudan Zhang and Hanyu Lai and Xinyi Liu and Hanlin Zhao and Jiadai Sun and Xinyue Yang and Yu Yang and Zehan Qi and Shuntian Yao and Xueqiao Sun and Siyi Cheng and Qinkai Zheng and Hao Yu and Hanchen Zhang and Wenyi Hong and Ming Ding and Lihang Pan and Xiaotao Gu and Aohan Zeng and Zhengxiao Du and Chan Hee Song and Yu Su and Yuxiao Dong and Jie Tang", "abstract": "  Large Multimodal Models (LMMs) have ushered in a new era in artificial\nintelligence, merging capabilities in both language and vision to form highly\ncapable Visual Foundation Agents. These agents are postulated to excel across a\nmyriad of tasks, potentially approaching general artificial intelligence.\nHowever, existing benchmarks fail to sufficiently challenge or showcase the\nfull potential of LMMs in complex, real-world environments. To address this\ngap, we introduce VisualAgentBench (VAB), a comprehensive and pioneering\nbenchmark specifically designed to train and evaluate LMMs as visual foundation\nagents across diverse scenarios, including Embodied, Graphical User Interface,\nand Visual Design, with tasks formulated to probe the depth of LMMs'\nunderstanding and interaction capabilities. Through rigorous testing across\nnine proprietary LMM APIs and eight open models, we demonstrate the\nconsiderable yet still developing agent capabilities of these models.\nAdditionally, VAB constructs a trajectory training set constructed through\nhybrid methods including Program-based Solvers, LMM Agent Bootstrapping, and\nHuman Demonstrations, promoting substantial performance improvements in LMMs\nthrough behavior cloning. Our work not only aims to benchmark existing models\nbut also provides a solid foundation for future development into visual\nfoundation agents. Code, train \\& test data, and part of fine-tuned open LMMs\nare available at \\url{https://github.com/THUDM/VisualAgentBench}.\n", "link": "http://arxiv.org/abs/2408.06327v1", "date": "2024-08-12", "relevancy": 1.7084, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5785}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5727}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisualAgentBench%3A%20Towards%20Large%20Multimodal%20Models%20as%20Visual%20Foundation%0A%20%20Agents&body=Title%3A%20VisualAgentBench%3A%20Towards%20Large%20Multimodal%20Models%20as%20Visual%20Foundation%0A%20%20Agents%0AAuthor%3A%20Xiao%20Liu%20and%20Tianjie%20Zhang%20and%20Yu%20Gu%20and%20Iat%20Long%20Iong%20and%20Yifan%20Xu%20and%20Xixuan%20Song%20and%20Shudan%20Zhang%20and%20Hanyu%20Lai%20and%20Xinyi%20Liu%20and%20Hanlin%20Zhao%20and%20Jiadai%20Sun%20and%20Xinyue%20Yang%20and%20Yu%20Yang%20and%20Zehan%20Qi%20and%20Shuntian%20Yao%20and%20Xueqiao%20Sun%20and%20Siyi%20Cheng%20and%20Qinkai%20Zheng%20and%20Hao%20Yu%20and%20Hanchen%20Zhang%20and%20Wenyi%20Hong%20and%20Ming%20Ding%20and%20Lihang%20Pan%20and%20Xiaotao%20Gu%20and%20Aohan%20Zeng%20and%20Zhengxiao%20Du%20and%20Chan%20Hee%20Song%20and%20Yu%20Su%20and%20Yuxiao%20Dong%20and%20Jie%20Tang%0AAbstract%3A%20%20%20Large%20Multimodal%20Models%20%28LMMs%29%20have%20ushered%20in%20a%20new%20era%20in%20artificial%0Aintelligence%2C%20merging%20capabilities%20in%20both%20language%20and%20vision%20to%20form%20highly%0Acapable%20Visual%20Foundation%20Agents.%20These%20agents%20are%20postulated%20to%20excel%20across%20a%0Amyriad%20of%20tasks%2C%20potentially%20approaching%20general%20artificial%20intelligence.%0AHowever%2C%20existing%20benchmarks%20fail%20to%20sufficiently%20challenge%20or%20showcase%20the%0Afull%20potential%20of%20LMMs%20in%20complex%2C%20real-world%20environments.%20To%20address%20this%0Agap%2C%20we%20introduce%20VisualAgentBench%20%28VAB%29%2C%20a%20comprehensive%20and%20pioneering%0Abenchmark%20specifically%20designed%20to%20train%20and%20evaluate%20LMMs%20as%20visual%20foundation%0Aagents%20across%20diverse%20scenarios%2C%20including%20Embodied%2C%20Graphical%20User%20Interface%2C%0Aand%20Visual%20Design%2C%20with%20tasks%20formulated%20to%20probe%20the%20depth%20of%20LMMs%27%0Aunderstanding%20and%20interaction%20capabilities.%20Through%20rigorous%20testing%20across%0Anine%20proprietary%20LMM%20APIs%20and%20eight%20open%20models%2C%20we%20demonstrate%20the%0Aconsiderable%20yet%20still%20developing%20agent%20capabilities%20of%20these%20models.%0AAdditionally%2C%20VAB%20constructs%20a%20trajectory%20training%20set%20constructed%20through%0Ahybrid%20methods%20including%20Program-based%20Solvers%2C%20LMM%20Agent%20Bootstrapping%2C%20and%0AHuman%20Demonstrations%2C%20promoting%20substantial%20performance%20improvements%20in%20LMMs%0Athrough%20behavior%20cloning.%20Our%20work%20not%20only%20aims%20to%20benchmark%20existing%20models%0Abut%20also%20provides%20a%20solid%20foundation%20for%20future%20development%20into%20visual%0Afoundation%20agents.%20Code%2C%20train%20%5C%26%20test%20data%2C%20and%20part%20of%20fine-tuned%20open%20LMMs%0Aare%20available%20at%20%5Curl%7Bhttps%3A//github.com/THUDM/VisualAgentBench%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06327v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisualAgentBench%253A%2520Towards%2520Large%2520Multimodal%2520Models%2520as%2520Visual%2520Foundation%250A%2520%2520Agents%26entry.906535625%3DXiao%2520Liu%2520and%2520Tianjie%2520Zhang%2520and%2520Yu%2520Gu%2520and%2520Iat%2520Long%2520Iong%2520and%2520Yifan%2520Xu%2520and%2520Xixuan%2520Song%2520and%2520Shudan%2520Zhang%2520and%2520Hanyu%2520Lai%2520and%2520Xinyi%2520Liu%2520and%2520Hanlin%2520Zhao%2520and%2520Jiadai%2520Sun%2520and%2520Xinyue%2520Yang%2520and%2520Yu%2520Yang%2520and%2520Zehan%2520Qi%2520and%2520Shuntian%2520Yao%2520and%2520Xueqiao%2520Sun%2520and%2520Siyi%2520Cheng%2520and%2520Qinkai%2520Zheng%2520and%2520Hao%2520Yu%2520and%2520Hanchen%2520Zhang%2520and%2520Wenyi%2520Hong%2520and%2520Ming%2520Ding%2520and%2520Lihang%2520Pan%2520and%2520Xiaotao%2520Gu%2520and%2520Aohan%2520Zeng%2520and%2520Zhengxiao%2520Du%2520and%2520Chan%2520Hee%2520Song%2520and%2520Yu%2520Su%2520and%2520Yuxiao%2520Dong%2520and%2520Jie%2520Tang%26entry.1292438233%3D%2520%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%2520have%2520ushered%2520in%2520a%2520new%2520era%2520in%2520artificial%250Aintelligence%252C%2520merging%2520capabilities%2520in%2520both%2520language%2520and%2520vision%2520to%2520form%2520highly%250Acapable%2520Visual%2520Foundation%2520Agents.%2520These%2520agents%2520are%2520postulated%2520to%2520excel%2520across%2520a%250Amyriad%2520of%2520tasks%252C%2520potentially%2520approaching%2520general%2520artificial%2520intelligence.%250AHowever%252C%2520existing%2520benchmarks%2520fail%2520to%2520sufficiently%2520challenge%2520or%2520showcase%2520the%250Afull%2520potential%2520of%2520LMMs%2520in%2520complex%252C%2520real-world%2520environments.%2520To%2520address%2520this%250Agap%252C%2520we%2520introduce%2520VisualAgentBench%2520%2528VAB%2529%252C%2520a%2520comprehensive%2520and%2520pioneering%250Abenchmark%2520specifically%2520designed%2520to%2520train%2520and%2520evaluate%2520LMMs%2520as%2520visual%2520foundation%250Aagents%2520across%2520diverse%2520scenarios%252C%2520including%2520Embodied%252C%2520Graphical%2520User%2520Interface%252C%250Aand%2520Visual%2520Design%252C%2520with%2520tasks%2520formulated%2520to%2520probe%2520the%2520depth%2520of%2520LMMs%2527%250Aunderstanding%2520and%2520interaction%2520capabilities.%2520Through%2520rigorous%2520testing%2520across%250Anine%2520proprietary%2520LMM%2520APIs%2520and%2520eight%2520open%2520models%252C%2520we%2520demonstrate%2520the%250Aconsiderable%2520yet%2520still%2520developing%2520agent%2520capabilities%2520of%2520these%2520models.%250AAdditionally%252C%2520VAB%2520constructs%2520a%2520trajectory%2520training%2520set%2520constructed%2520through%250Ahybrid%2520methods%2520including%2520Program-based%2520Solvers%252C%2520LMM%2520Agent%2520Bootstrapping%252C%2520and%250AHuman%2520Demonstrations%252C%2520promoting%2520substantial%2520performance%2520improvements%2520in%2520LMMs%250Athrough%2520behavior%2520cloning.%2520Our%2520work%2520not%2520only%2520aims%2520to%2520benchmark%2520existing%2520models%250Abut%2520also%2520provides%2520a%2520solid%2520foundation%2520for%2520future%2520development%2520into%2520visual%250Afoundation%2520agents.%2520Code%252C%2520train%2520%255C%2526%2520test%2520data%252C%2520and%2520part%2520of%2520fine-tuned%2520open%2520LMMs%250Aare%2520available%2520at%2520%255Curl%257Bhttps%253A//github.com/THUDM/VisualAgentBench%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06327v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisualAgentBench%3A%20Towards%20Large%20Multimodal%20Models%20as%20Visual%20Foundation%0A%20%20Agents&entry.906535625=Xiao%20Liu%20and%20Tianjie%20Zhang%20and%20Yu%20Gu%20and%20Iat%20Long%20Iong%20and%20Yifan%20Xu%20and%20Xixuan%20Song%20and%20Shudan%20Zhang%20and%20Hanyu%20Lai%20and%20Xinyi%20Liu%20and%20Hanlin%20Zhao%20and%20Jiadai%20Sun%20and%20Xinyue%20Yang%20and%20Yu%20Yang%20and%20Zehan%20Qi%20and%20Shuntian%20Yao%20and%20Xueqiao%20Sun%20and%20Siyi%20Cheng%20and%20Qinkai%20Zheng%20and%20Hao%20Yu%20and%20Hanchen%20Zhang%20and%20Wenyi%20Hong%20and%20Ming%20Ding%20and%20Lihang%20Pan%20and%20Xiaotao%20Gu%20and%20Aohan%20Zeng%20and%20Zhengxiao%20Du%20and%20Chan%20Hee%20Song%20and%20Yu%20Su%20and%20Yuxiao%20Dong%20and%20Jie%20Tang&entry.1292438233=%20%20Large%20Multimodal%20Models%20%28LMMs%29%20have%20ushered%20in%20a%20new%20era%20in%20artificial%0Aintelligence%2C%20merging%20capabilities%20in%20both%20language%20and%20vision%20to%20form%20highly%0Acapable%20Visual%20Foundation%20Agents.%20These%20agents%20are%20postulated%20to%20excel%20across%20a%0Amyriad%20of%20tasks%2C%20potentially%20approaching%20general%20artificial%20intelligence.%0AHowever%2C%20existing%20benchmarks%20fail%20to%20sufficiently%20challenge%20or%20showcase%20the%0Afull%20potential%20of%20LMMs%20in%20complex%2C%20real-world%20environments.%20To%20address%20this%0Agap%2C%20we%20introduce%20VisualAgentBench%20%28VAB%29%2C%20a%20comprehensive%20and%20pioneering%0Abenchmark%20specifically%20designed%20to%20train%20and%20evaluate%20LMMs%20as%20visual%20foundation%0Aagents%20across%20diverse%20scenarios%2C%20including%20Embodied%2C%20Graphical%20User%20Interface%2C%0Aand%20Visual%20Design%2C%20with%20tasks%20formulated%20to%20probe%20the%20depth%20of%20LMMs%27%0Aunderstanding%20and%20interaction%20capabilities.%20Through%20rigorous%20testing%20across%0Anine%20proprietary%20LMM%20APIs%20and%20eight%20open%20models%2C%20we%20demonstrate%20the%0Aconsiderable%20yet%20still%20developing%20agent%20capabilities%20of%20these%20models.%0AAdditionally%2C%20VAB%20constructs%20a%20trajectory%20training%20set%20constructed%20through%0Ahybrid%20methods%20including%20Program-based%20Solvers%2C%20LMM%20Agent%20Bootstrapping%2C%20and%0AHuman%20Demonstrations%2C%20promoting%20substantial%20performance%20improvements%20in%20LMMs%0Athrough%20behavior%20cloning.%20Our%20work%20not%20only%20aims%20to%20benchmark%20existing%20models%0Abut%20also%20provides%20a%20solid%20foundation%20for%20future%20development%20into%20visual%0Afoundation%20agents.%20Code%2C%20train%20%5C%26%20test%20data%2C%20and%20part%20of%20fine-tuned%20open%20LMMs%0Aare%20available%20at%20%5Curl%7Bhttps%3A//github.com/THUDM/VisualAgentBench%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06327v1&entry.124074799=Read"},
{"title": "Context-aware Visual Storytelling with Visual Prefix Tuning and\n  Contrastive Learning", "author": "Yingjin Song and Denis Paperno and Albert Gatt", "abstract": "  Visual storytelling systems generate multi-sentence stories from image\nsequences. In this task, capturing contextual information and bridging visual\nvariation bring additional challenges. We propose a simple yet effective\nframework that leverages the generalization capabilities of pretrained\nfoundation models, only training a lightweight vision-language mapping network\nto connect modalities, while incorporating context to enhance coherence. We\nintroduce a multimodal contrastive objective that also improves visual\nrelevance and story informativeness. Extensive experimental results, across\nboth automatic metrics and human evaluations, demonstrate that the stories\ngenerated by our framework are diverse, coherent, informative, and interesting.\n", "link": "http://arxiv.org/abs/2408.06259v1", "date": "2024-08-12", "relevancy": 1.6826, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5738}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5468}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Context-aware%20Visual%20Storytelling%20with%20Visual%20Prefix%20Tuning%20and%0A%20%20Contrastive%20Learning&body=Title%3A%20Context-aware%20Visual%20Storytelling%20with%20Visual%20Prefix%20Tuning%20and%0A%20%20Contrastive%20Learning%0AAuthor%3A%20Yingjin%20Song%20and%20Denis%20Paperno%20and%20Albert%20Gatt%0AAbstract%3A%20%20%20Visual%20storytelling%20systems%20generate%20multi-sentence%20stories%20from%20image%0Asequences.%20In%20this%20task%2C%20capturing%20contextual%20information%20and%20bridging%20visual%0Avariation%20bring%20additional%20challenges.%20We%20propose%20a%20simple%20yet%20effective%0Aframework%20that%20leverages%20the%20generalization%20capabilities%20of%20pretrained%0Afoundation%20models%2C%20only%20training%20a%20lightweight%20vision-language%20mapping%20network%0Ato%20connect%20modalities%2C%20while%20incorporating%20context%20to%20enhance%20coherence.%20We%0Aintroduce%20a%20multimodal%20contrastive%20objective%20that%20also%20improves%20visual%0Arelevance%20and%20story%20informativeness.%20Extensive%20experimental%20results%2C%20across%0Aboth%20automatic%20metrics%20and%20human%20evaluations%2C%20demonstrate%20that%20the%20stories%0Agenerated%20by%20our%20framework%20are%20diverse%2C%20coherent%2C%20informative%2C%20and%20interesting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06259v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContext-aware%2520Visual%2520Storytelling%2520with%2520Visual%2520Prefix%2520Tuning%2520and%250A%2520%2520Contrastive%2520Learning%26entry.906535625%3DYingjin%2520Song%2520and%2520Denis%2520Paperno%2520and%2520Albert%2520Gatt%26entry.1292438233%3D%2520%2520Visual%2520storytelling%2520systems%2520generate%2520multi-sentence%2520stories%2520from%2520image%250Asequences.%2520In%2520this%2520task%252C%2520capturing%2520contextual%2520information%2520and%2520bridging%2520visual%250Avariation%2520bring%2520additional%2520challenges.%2520We%2520propose%2520a%2520simple%2520yet%2520effective%250Aframework%2520that%2520leverages%2520the%2520generalization%2520capabilities%2520of%2520pretrained%250Afoundation%2520models%252C%2520only%2520training%2520a%2520lightweight%2520vision-language%2520mapping%2520network%250Ato%2520connect%2520modalities%252C%2520while%2520incorporating%2520context%2520to%2520enhance%2520coherence.%2520We%250Aintroduce%2520a%2520multimodal%2520contrastive%2520objective%2520that%2520also%2520improves%2520visual%250Arelevance%2520and%2520story%2520informativeness.%2520Extensive%2520experimental%2520results%252C%2520across%250Aboth%2520automatic%2520metrics%2520and%2520human%2520evaluations%252C%2520demonstrate%2520that%2520the%2520stories%250Agenerated%2520by%2520our%2520framework%2520are%2520diverse%252C%2520coherent%252C%2520informative%252C%2520and%2520interesting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06259v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context-aware%20Visual%20Storytelling%20with%20Visual%20Prefix%20Tuning%20and%0A%20%20Contrastive%20Learning&entry.906535625=Yingjin%20Song%20and%20Denis%20Paperno%20and%20Albert%20Gatt&entry.1292438233=%20%20Visual%20storytelling%20systems%20generate%20multi-sentence%20stories%20from%20image%0Asequences.%20In%20this%20task%2C%20capturing%20contextual%20information%20and%20bridging%20visual%0Avariation%20bring%20additional%20challenges.%20We%20propose%20a%20simple%20yet%20effective%0Aframework%20that%20leverages%20the%20generalization%20capabilities%20of%20pretrained%0Afoundation%20models%2C%20only%20training%20a%20lightweight%20vision-language%20mapping%20network%0Ato%20connect%20modalities%2C%20while%20incorporating%20context%20to%20enhance%20coherence.%20We%0Aintroduce%20a%20multimodal%20contrastive%20objective%20that%20also%20improves%20visual%0Arelevance%20and%20story%20informativeness.%20Extensive%20experimental%20results%2C%20across%0Aboth%20automatic%20metrics%20and%20human%20evaluations%2C%20demonstrate%20that%20the%20stories%0Agenerated%20by%20our%20framework%20are%20diverse%2C%20coherent%2C%20informative%2C%20and%20interesting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06259v1&entry.124074799=Read"},
{"title": "Neural Randomized Planning for Whole Body Robot Motion", "author": "Yunfan Lu and Yuchen Ma and David Hsu and Panpan Cai", "abstract": "  Robot motion planning has made vast advances over the past decades, but the\nchallenge remains: robot mobile manipulators struggle to plan long-range\nwhole-body motion in common household environments in real time, because of\nhigh-dimensional robot configuration space and complex environment geometry. To\ntackle the challenge, this paper proposes Neural Randomized Planner (NRP),\nwhich combines a global sampling-based motion planning (SBMP) algorithm and a\nlocal neural sampler. Intuitively, NRP uses the search structure inside the\nglobal planner to stitch together learned local sampling distributions to form\na global sampling distribution adaptively. It benefits from both learning and\nplanning. Locally, it tackles high dimensionality by learning to sample in\npromising regions from data, with a rich neural network representation.\nGlobally, it composes the local sampling distributions through planning and\nexploits local geometric similarity to scale up to complex environments.\nExperiments both in simulation and on a real robot show \\NRP yields superior\nperformance compared to some of the best classical and learning-enhanced SBMP\nalgorithms. Further, despite being trained in simulation, NRP demonstrates\nzero-shot transfer to a real robot operating in novel household environments,\nwithout any fine-tuning or manual adaptation.\n", "link": "http://arxiv.org/abs/2405.11317v2", "date": "2024-08-12", "relevancy": 1.6788, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5893}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5701}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5435}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Randomized%20Planning%20for%20Whole%20Body%20Robot%20Motion&body=Title%3A%20Neural%20Randomized%20Planning%20for%20Whole%20Body%20Robot%20Motion%0AAuthor%3A%20Yunfan%20Lu%20and%20Yuchen%20Ma%20and%20David%20Hsu%20and%20Panpan%20Cai%0AAbstract%3A%20%20%20Robot%20motion%20planning%20has%20made%20vast%20advances%20over%20the%20past%20decades%2C%20but%20the%0Achallenge%20remains%3A%20robot%20mobile%20manipulators%20struggle%20to%20plan%20long-range%0Awhole-body%20motion%20in%20common%20household%20environments%20in%20real%20time%2C%20because%20of%0Ahigh-dimensional%20robot%20configuration%20space%20and%20complex%20environment%20geometry.%20To%0Atackle%20the%20challenge%2C%20this%20paper%20proposes%20Neural%20Randomized%20Planner%20%28NRP%29%2C%0Awhich%20combines%20a%20global%20sampling-based%20motion%20planning%20%28SBMP%29%20algorithm%20and%20a%0Alocal%20neural%20sampler.%20Intuitively%2C%20NRP%20uses%20the%20search%20structure%20inside%20the%0Aglobal%20planner%20to%20stitch%20together%20learned%20local%20sampling%20distributions%20to%20form%0Aa%20global%20sampling%20distribution%20adaptively.%20It%20benefits%20from%20both%20learning%20and%0Aplanning.%20Locally%2C%20it%20tackles%20high%20dimensionality%20by%20learning%20to%20sample%20in%0Apromising%20regions%20from%20data%2C%20with%20a%20rich%20neural%20network%20representation.%0AGlobally%2C%20it%20composes%20the%20local%20sampling%20distributions%20through%20planning%20and%0Aexploits%20local%20geometric%20similarity%20to%20scale%20up%20to%20complex%20environments.%0AExperiments%20both%20in%20simulation%20and%20on%20a%20real%20robot%20show%20%5CNRP%20yields%20superior%0Aperformance%20compared%20to%20some%20of%20the%20best%20classical%20and%20learning-enhanced%20SBMP%0Aalgorithms.%20Further%2C%20despite%20being%20trained%20in%20simulation%2C%20NRP%20demonstrates%0Azero-shot%20transfer%20to%20a%20real%20robot%20operating%20in%20novel%20household%20environments%2C%0Awithout%20any%20fine-tuning%20or%20manual%20adaptation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11317v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Randomized%2520Planning%2520for%2520Whole%2520Body%2520Robot%2520Motion%26entry.906535625%3DYunfan%2520Lu%2520and%2520Yuchen%2520Ma%2520and%2520David%2520Hsu%2520and%2520Panpan%2520Cai%26entry.1292438233%3D%2520%2520Robot%2520motion%2520planning%2520has%2520made%2520vast%2520advances%2520over%2520the%2520past%2520decades%252C%2520but%2520the%250Achallenge%2520remains%253A%2520robot%2520mobile%2520manipulators%2520struggle%2520to%2520plan%2520long-range%250Awhole-body%2520motion%2520in%2520common%2520household%2520environments%2520in%2520real%2520time%252C%2520because%2520of%250Ahigh-dimensional%2520robot%2520configuration%2520space%2520and%2520complex%2520environment%2520geometry.%2520To%250Atackle%2520the%2520challenge%252C%2520this%2520paper%2520proposes%2520Neural%2520Randomized%2520Planner%2520%2528NRP%2529%252C%250Awhich%2520combines%2520a%2520global%2520sampling-based%2520motion%2520planning%2520%2528SBMP%2529%2520algorithm%2520and%2520a%250Alocal%2520neural%2520sampler.%2520Intuitively%252C%2520NRP%2520uses%2520the%2520search%2520structure%2520inside%2520the%250Aglobal%2520planner%2520to%2520stitch%2520together%2520learned%2520local%2520sampling%2520distributions%2520to%2520form%250Aa%2520global%2520sampling%2520distribution%2520adaptively.%2520It%2520benefits%2520from%2520both%2520learning%2520and%250Aplanning.%2520Locally%252C%2520it%2520tackles%2520high%2520dimensionality%2520by%2520learning%2520to%2520sample%2520in%250Apromising%2520regions%2520from%2520data%252C%2520with%2520a%2520rich%2520neural%2520network%2520representation.%250AGlobally%252C%2520it%2520composes%2520the%2520local%2520sampling%2520distributions%2520through%2520planning%2520and%250Aexploits%2520local%2520geometric%2520similarity%2520to%2520scale%2520up%2520to%2520complex%2520environments.%250AExperiments%2520both%2520in%2520simulation%2520and%2520on%2520a%2520real%2520robot%2520show%2520%255CNRP%2520yields%2520superior%250Aperformance%2520compared%2520to%2520some%2520of%2520the%2520best%2520classical%2520and%2520learning-enhanced%2520SBMP%250Aalgorithms.%2520Further%252C%2520despite%2520being%2520trained%2520in%2520simulation%252C%2520NRP%2520demonstrates%250Azero-shot%2520transfer%2520to%2520a%2520real%2520robot%2520operating%2520in%2520novel%2520household%2520environments%252C%250Awithout%2520any%2520fine-tuning%2520or%2520manual%2520adaptation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11317v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Randomized%20Planning%20for%20Whole%20Body%20Robot%20Motion&entry.906535625=Yunfan%20Lu%20and%20Yuchen%20Ma%20and%20David%20Hsu%20and%20Panpan%20Cai&entry.1292438233=%20%20Robot%20motion%20planning%20has%20made%20vast%20advances%20over%20the%20past%20decades%2C%20but%20the%0Achallenge%20remains%3A%20robot%20mobile%20manipulators%20struggle%20to%20plan%20long-range%0Awhole-body%20motion%20in%20common%20household%20environments%20in%20real%20time%2C%20because%20of%0Ahigh-dimensional%20robot%20configuration%20space%20and%20complex%20environment%20geometry.%20To%0Atackle%20the%20challenge%2C%20this%20paper%20proposes%20Neural%20Randomized%20Planner%20%28NRP%29%2C%0Awhich%20combines%20a%20global%20sampling-based%20motion%20planning%20%28SBMP%29%20algorithm%20and%20a%0Alocal%20neural%20sampler.%20Intuitively%2C%20NRP%20uses%20the%20search%20structure%20inside%20the%0Aglobal%20planner%20to%20stitch%20together%20learned%20local%20sampling%20distributions%20to%20form%0Aa%20global%20sampling%20distribution%20adaptively.%20It%20benefits%20from%20both%20learning%20and%0Aplanning.%20Locally%2C%20it%20tackles%20high%20dimensionality%20by%20learning%20to%20sample%20in%0Apromising%20regions%20from%20data%2C%20with%20a%20rich%20neural%20network%20representation.%0AGlobally%2C%20it%20composes%20the%20local%20sampling%20distributions%20through%20planning%20and%0Aexploits%20local%20geometric%20similarity%20to%20scale%20up%20to%20complex%20environments.%0AExperiments%20both%20in%20simulation%20and%20on%20a%20real%20robot%20show%20%5CNRP%20yields%20superior%0Aperformance%20compared%20to%20some%20of%20the%20best%20classical%20and%20learning-enhanced%20SBMP%0Aalgorithms.%20Further%2C%20despite%20being%20trained%20in%20simulation%2C%20NRP%20demonstrates%0Azero-shot%20transfer%20to%20a%20real%20robot%20operating%20in%20novel%20household%20environments%2C%0Awithout%20any%20fine-tuning%20or%20manual%20adaptation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11317v2&entry.124074799=Read"},
{"title": "MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware\n  Experts", "author": "Xi Victoria Lin and Akshat Shrivastava and Liang Luo and Srinivasan Iyer and Mike Lewis and Gargi Ghosh and Luke Zettlemoyer and Armen Aghajanyan", "abstract": "  We introduce MoMa, a novel modality-aware mixture-of-experts (MoE)\narchitecture designed for pre-training mixed-modal, early-fusion language\nmodels. MoMa processes images and text in arbitrary sequences by dividing\nexpert modules into modality-specific groups. These groups exclusively process\ndesignated tokens while employing learned routing within each group to maintain\nsemantically informed adaptivity. Our empirical results reveal substantial\npre-training efficiency gains through this modality-specific parameter\nallocation. Under a 1-trillion-token training budget, the MoMa 1.4B model,\nfeaturing 4 text experts and 4 image experts, achieves impressive FLOPs\nsavings: 3.7x overall, with 2.6x for text and 5.2x for image processing\ncompared to a compute-equivalent dense baseline, measured by pre-training loss.\nThis outperforms the standard expert-choice MoE with 8 mixed-modal experts,\nwhich achieves 3x overall FLOPs savings (3x for text, 2.8x for image).\nCombining MoMa with mixture-of-depths (MoD) further improves pre-training FLOPs\nsavings to 4.2x overall (text: 3.4x, image: 5.3x), although this combination\nhurts performance in causal inference due to increased sensitivity to router\naccuracy. These results demonstrate MoMa's potential to significantly advance\nthe efficiency of mixed-modal, early-fusion language model pre-training, paving\nthe way for more resource-efficient and capable multimodal AI systems.\n", "link": "http://arxiv.org/abs/2407.21770v3", "date": "2024-08-12", "relevancy": 1.6775, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.583}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5409}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5178}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoMa%3A%20Efficient%20Early-Fusion%20Pre-training%20with%20Mixture%20of%20Modality-Aware%0A%20%20Experts&body=Title%3A%20MoMa%3A%20Efficient%20Early-Fusion%20Pre-training%20with%20Mixture%20of%20Modality-Aware%0A%20%20Experts%0AAuthor%3A%20Xi%20Victoria%20Lin%20and%20Akshat%20Shrivastava%20and%20Liang%20Luo%20and%20Srinivasan%20Iyer%20and%20Mike%20Lewis%20and%20Gargi%20Ghosh%20and%20Luke%20Zettlemoyer%20and%20Armen%20Aghajanyan%0AAbstract%3A%20%20%20We%20introduce%20MoMa%2C%20a%20novel%20modality-aware%20mixture-of-experts%20%28MoE%29%0Aarchitecture%20designed%20for%20pre-training%20mixed-modal%2C%20early-fusion%20language%0Amodels.%20MoMa%20processes%20images%20and%20text%20in%20arbitrary%20sequences%20by%20dividing%0Aexpert%20modules%20into%20modality-specific%20groups.%20These%20groups%20exclusively%20process%0Adesignated%20tokens%20while%20employing%20learned%20routing%20within%20each%20group%20to%20maintain%0Asemantically%20informed%20adaptivity.%20Our%20empirical%20results%20reveal%20substantial%0Apre-training%20efficiency%20gains%20through%20this%20modality-specific%20parameter%0Aallocation.%20Under%20a%201-trillion-token%20training%20budget%2C%20the%20MoMa%201.4B%20model%2C%0Afeaturing%204%20text%20experts%20and%204%20image%20experts%2C%20achieves%20impressive%20FLOPs%0Asavings%3A%203.7x%20overall%2C%20with%202.6x%20for%20text%20and%205.2x%20for%20image%20processing%0Acompared%20to%20a%20compute-equivalent%20dense%20baseline%2C%20measured%20by%20pre-training%20loss.%0AThis%20outperforms%20the%20standard%20expert-choice%20MoE%20with%208%20mixed-modal%20experts%2C%0Awhich%20achieves%203x%20overall%20FLOPs%20savings%20%283x%20for%20text%2C%202.8x%20for%20image%29.%0ACombining%20MoMa%20with%20mixture-of-depths%20%28MoD%29%20further%20improves%20pre-training%20FLOPs%0Asavings%20to%204.2x%20overall%20%28text%3A%203.4x%2C%20image%3A%205.3x%29%2C%20although%20this%20combination%0Ahurts%20performance%20in%20causal%20inference%20due%20to%20increased%20sensitivity%20to%20router%0Aaccuracy.%20These%20results%20demonstrate%20MoMa%27s%20potential%20to%20significantly%20advance%0Athe%20efficiency%20of%20mixed-modal%2C%20early-fusion%20language%20model%20pre-training%2C%20paving%0Athe%20way%20for%20more%20resource-efficient%20and%20capable%20multimodal%20AI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21770v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoMa%253A%2520Efficient%2520Early-Fusion%2520Pre-training%2520with%2520Mixture%2520of%2520Modality-Aware%250A%2520%2520Experts%26entry.906535625%3DXi%2520Victoria%2520Lin%2520and%2520Akshat%2520Shrivastava%2520and%2520Liang%2520Luo%2520and%2520Srinivasan%2520Iyer%2520and%2520Mike%2520Lewis%2520and%2520Gargi%2520Ghosh%2520and%2520Luke%2520Zettlemoyer%2520and%2520Armen%2520Aghajanyan%26entry.1292438233%3D%2520%2520We%2520introduce%2520MoMa%252C%2520a%2520novel%2520modality-aware%2520mixture-of-experts%2520%2528MoE%2529%250Aarchitecture%2520designed%2520for%2520pre-training%2520mixed-modal%252C%2520early-fusion%2520language%250Amodels.%2520MoMa%2520processes%2520images%2520and%2520text%2520in%2520arbitrary%2520sequences%2520by%2520dividing%250Aexpert%2520modules%2520into%2520modality-specific%2520groups.%2520These%2520groups%2520exclusively%2520process%250Adesignated%2520tokens%2520while%2520employing%2520learned%2520routing%2520within%2520each%2520group%2520to%2520maintain%250Asemantically%2520informed%2520adaptivity.%2520Our%2520empirical%2520results%2520reveal%2520substantial%250Apre-training%2520efficiency%2520gains%2520through%2520this%2520modality-specific%2520parameter%250Aallocation.%2520Under%2520a%25201-trillion-token%2520training%2520budget%252C%2520the%2520MoMa%25201.4B%2520model%252C%250Afeaturing%25204%2520text%2520experts%2520and%25204%2520image%2520experts%252C%2520achieves%2520impressive%2520FLOPs%250Asavings%253A%25203.7x%2520overall%252C%2520with%25202.6x%2520for%2520text%2520and%25205.2x%2520for%2520image%2520processing%250Acompared%2520to%2520a%2520compute-equivalent%2520dense%2520baseline%252C%2520measured%2520by%2520pre-training%2520loss.%250AThis%2520outperforms%2520the%2520standard%2520expert-choice%2520MoE%2520with%25208%2520mixed-modal%2520experts%252C%250Awhich%2520achieves%25203x%2520overall%2520FLOPs%2520savings%2520%25283x%2520for%2520text%252C%25202.8x%2520for%2520image%2529.%250ACombining%2520MoMa%2520with%2520mixture-of-depths%2520%2528MoD%2529%2520further%2520improves%2520pre-training%2520FLOPs%250Asavings%2520to%25204.2x%2520overall%2520%2528text%253A%25203.4x%252C%2520image%253A%25205.3x%2529%252C%2520although%2520this%2520combination%250Ahurts%2520performance%2520in%2520causal%2520inference%2520due%2520to%2520increased%2520sensitivity%2520to%2520router%250Aaccuracy.%2520These%2520results%2520demonstrate%2520MoMa%2527s%2520potential%2520to%2520significantly%2520advance%250Athe%2520efficiency%2520of%2520mixed-modal%252C%2520early-fusion%2520language%2520model%2520pre-training%252C%2520paving%250Athe%2520way%2520for%2520more%2520resource-efficient%2520and%2520capable%2520multimodal%2520AI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21770v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoMa%3A%20Efficient%20Early-Fusion%20Pre-training%20with%20Mixture%20of%20Modality-Aware%0A%20%20Experts&entry.906535625=Xi%20Victoria%20Lin%20and%20Akshat%20Shrivastava%20and%20Liang%20Luo%20and%20Srinivasan%20Iyer%20and%20Mike%20Lewis%20and%20Gargi%20Ghosh%20and%20Luke%20Zettlemoyer%20and%20Armen%20Aghajanyan&entry.1292438233=%20%20We%20introduce%20MoMa%2C%20a%20novel%20modality-aware%20mixture-of-experts%20%28MoE%29%0Aarchitecture%20designed%20for%20pre-training%20mixed-modal%2C%20early-fusion%20language%0Amodels.%20MoMa%20processes%20images%20and%20text%20in%20arbitrary%20sequences%20by%20dividing%0Aexpert%20modules%20into%20modality-specific%20groups.%20These%20groups%20exclusively%20process%0Adesignated%20tokens%20while%20employing%20learned%20routing%20within%20each%20group%20to%20maintain%0Asemantically%20informed%20adaptivity.%20Our%20empirical%20results%20reveal%20substantial%0Apre-training%20efficiency%20gains%20through%20this%20modality-specific%20parameter%0Aallocation.%20Under%20a%201-trillion-token%20training%20budget%2C%20the%20MoMa%201.4B%20model%2C%0Afeaturing%204%20text%20experts%20and%204%20image%20experts%2C%20achieves%20impressive%20FLOPs%0Asavings%3A%203.7x%20overall%2C%20with%202.6x%20for%20text%20and%205.2x%20for%20image%20processing%0Acompared%20to%20a%20compute-equivalent%20dense%20baseline%2C%20measured%20by%20pre-training%20loss.%0AThis%20outperforms%20the%20standard%20expert-choice%20MoE%20with%208%20mixed-modal%20experts%2C%0Awhich%20achieves%203x%20overall%20FLOPs%20savings%20%283x%20for%20text%2C%202.8x%20for%20image%29.%0ACombining%20MoMa%20with%20mixture-of-depths%20%28MoD%29%20further%20improves%20pre-training%20FLOPs%0Asavings%20to%204.2x%20overall%20%28text%3A%203.4x%2C%20image%3A%205.3x%29%2C%20although%20this%20combination%0Ahurts%20performance%20in%20causal%20inference%20due%20to%20increased%20sensitivity%20to%20router%0Aaccuracy.%20These%20results%20demonstrate%20MoMa%27s%20potential%20to%20significantly%20advance%0Athe%20efficiency%20of%20mixed-modal%2C%20early-fusion%20language%20model%20pre-training%2C%20paving%0Athe%20way%20for%20more%20resource-efficient%20and%20capable%20multimodal%20AI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21770v3&entry.124074799=Read"},
{"title": "A Soft Robotic System Automatically Learns Precise Agile Motions Without\n  Model Information", "author": "Simon Bachhuber and Alexander Pawluchin and Arka Pal and Ivo Boblan and Thomas Seel", "abstract": "  Many application domains, e.g., in medicine and manufacturing, can greatly\nbenefit from pneumatic Soft Robots (SRs). However, the accurate control of SRs\nhas remained a significant challenge to date, mainly due to their nonlinear\ndynamics and viscoelastic material properties. Conventional control design\nmethods often rely on either complex system modeling or time-intensive manual\ntuning, both of which require significant amounts of human expertise and thus\nlimit their practicality. In recent works, the data-driven method, Automatic\nNeural ODE Control (ANODEC) has been successfully used to -- fully\nautomatically and utilizing only input-output data -- design controllers for\nvarious nonlinear systems in silico, and without requiring prior model\nknowledge or extensive manual tuning. In this work, we successfully apply\nANODEC to automatically learn to perform agile, non-repetitive reference\ntracking motion tasks in a real-world SR and within a finite time horizon. To\nthe best of the authors' knowledge, ANODEC achieves, for the first time,\nperformant control of a SR with hysteresis effects from only 30 seconds of\ninput-output data and without any prior model knowledge. We show that for\nmultiple, qualitatively different and even out-of-training-distribution\nreference signals, a single feedback controller designed by ANODEC outperforms\na manually tuned PID baseline consistently. Overall, this contribution not only\nfurther strengthens the validity of ANODEC, but it marks an important step\ntowards more practical, easy-to-use SRs that can automatically learn to perform\nagile motions from minimal experimental interaction time.\n", "link": "http://arxiv.org/abs/2408.03754v2", "date": "2024-08-12", "relevancy": 1.6726, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5882}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5554}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Soft%20Robotic%20System%20Automatically%20Learns%20Precise%20Agile%20Motions%20Without%0A%20%20Model%20Information&body=Title%3A%20A%20Soft%20Robotic%20System%20Automatically%20Learns%20Precise%20Agile%20Motions%20Without%0A%20%20Model%20Information%0AAuthor%3A%20Simon%20Bachhuber%20and%20Alexander%20Pawluchin%20and%20Arka%20Pal%20and%20Ivo%20Boblan%20and%20Thomas%20Seel%0AAbstract%3A%20%20%20Many%20application%20domains%2C%20e.g.%2C%20in%20medicine%20and%20manufacturing%2C%20can%20greatly%0Abenefit%20from%20pneumatic%20Soft%20Robots%20%28SRs%29.%20However%2C%20the%20accurate%20control%20of%20SRs%0Ahas%20remained%20a%20significant%20challenge%20to%20date%2C%20mainly%20due%20to%20their%20nonlinear%0Adynamics%20and%20viscoelastic%20material%20properties.%20Conventional%20control%20design%0Amethods%20often%20rely%20on%20either%20complex%20system%20modeling%20or%20time-intensive%20manual%0Atuning%2C%20both%20of%20which%20require%20significant%20amounts%20of%20human%20expertise%20and%20thus%0Alimit%20their%20practicality.%20In%20recent%20works%2C%20the%20data-driven%20method%2C%20Automatic%0ANeural%20ODE%20Control%20%28ANODEC%29%20has%20been%20successfully%20used%20to%20--%20fully%0Aautomatically%20and%20utilizing%20only%20input-output%20data%20--%20design%20controllers%20for%0Avarious%20nonlinear%20systems%20in%20silico%2C%20and%20without%20requiring%20prior%20model%0Aknowledge%20or%20extensive%20manual%20tuning.%20In%20this%20work%2C%20we%20successfully%20apply%0AANODEC%20to%20automatically%20learn%20to%20perform%20agile%2C%20non-repetitive%20reference%0Atracking%20motion%20tasks%20in%20a%20real-world%20SR%20and%20within%20a%20finite%20time%20horizon.%20To%0Athe%20best%20of%20the%20authors%27%20knowledge%2C%20ANODEC%20achieves%2C%20for%20the%20first%20time%2C%0Aperformant%20control%20of%20a%20SR%20with%20hysteresis%20effects%20from%20only%2030%20seconds%20of%0Ainput-output%20data%20and%20without%20any%20prior%20model%20knowledge.%20We%20show%20that%20for%0Amultiple%2C%20qualitatively%20different%20and%20even%20out-of-training-distribution%0Areference%20signals%2C%20a%20single%20feedback%20controller%20designed%20by%20ANODEC%20outperforms%0Aa%20manually%20tuned%20PID%20baseline%20consistently.%20Overall%2C%20this%20contribution%20not%20only%0Afurther%20strengthens%20the%20validity%20of%20ANODEC%2C%20but%20it%20marks%20an%20important%20step%0Atowards%20more%20practical%2C%20easy-to-use%20SRs%20that%20can%20automatically%20learn%20to%20perform%0Aagile%20motions%20from%20minimal%20experimental%20interaction%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03754v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Soft%2520Robotic%2520System%2520Automatically%2520Learns%2520Precise%2520Agile%2520Motions%2520Without%250A%2520%2520Model%2520Information%26entry.906535625%3DSimon%2520Bachhuber%2520and%2520Alexander%2520Pawluchin%2520and%2520Arka%2520Pal%2520and%2520Ivo%2520Boblan%2520and%2520Thomas%2520Seel%26entry.1292438233%3D%2520%2520Many%2520application%2520domains%252C%2520e.g.%252C%2520in%2520medicine%2520and%2520manufacturing%252C%2520can%2520greatly%250Abenefit%2520from%2520pneumatic%2520Soft%2520Robots%2520%2528SRs%2529.%2520However%252C%2520the%2520accurate%2520control%2520of%2520SRs%250Ahas%2520remained%2520a%2520significant%2520challenge%2520to%2520date%252C%2520mainly%2520due%2520to%2520their%2520nonlinear%250Adynamics%2520and%2520viscoelastic%2520material%2520properties.%2520Conventional%2520control%2520design%250Amethods%2520often%2520rely%2520on%2520either%2520complex%2520system%2520modeling%2520or%2520time-intensive%2520manual%250Atuning%252C%2520both%2520of%2520which%2520require%2520significant%2520amounts%2520of%2520human%2520expertise%2520and%2520thus%250Alimit%2520their%2520practicality.%2520In%2520recent%2520works%252C%2520the%2520data-driven%2520method%252C%2520Automatic%250ANeural%2520ODE%2520Control%2520%2528ANODEC%2529%2520has%2520been%2520successfully%2520used%2520to%2520--%2520fully%250Aautomatically%2520and%2520utilizing%2520only%2520input-output%2520data%2520--%2520design%2520controllers%2520for%250Avarious%2520nonlinear%2520systems%2520in%2520silico%252C%2520and%2520without%2520requiring%2520prior%2520model%250Aknowledge%2520or%2520extensive%2520manual%2520tuning.%2520In%2520this%2520work%252C%2520we%2520successfully%2520apply%250AANODEC%2520to%2520automatically%2520learn%2520to%2520perform%2520agile%252C%2520non-repetitive%2520reference%250Atracking%2520motion%2520tasks%2520in%2520a%2520real-world%2520SR%2520and%2520within%2520a%2520finite%2520time%2520horizon.%2520To%250Athe%2520best%2520of%2520the%2520authors%2527%2520knowledge%252C%2520ANODEC%2520achieves%252C%2520for%2520the%2520first%2520time%252C%250Aperformant%2520control%2520of%2520a%2520SR%2520with%2520hysteresis%2520effects%2520from%2520only%252030%2520seconds%2520of%250Ainput-output%2520data%2520and%2520without%2520any%2520prior%2520model%2520knowledge.%2520We%2520show%2520that%2520for%250Amultiple%252C%2520qualitatively%2520different%2520and%2520even%2520out-of-training-distribution%250Areference%2520signals%252C%2520a%2520single%2520feedback%2520controller%2520designed%2520by%2520ANODEC%2520outperforms%250Aa%2520manually%2520tuned%2520PID%2520baseline%2520consistently.%2520Overall%252C%2520this%2520contribution%2520not%2520only%250Afurther%2520strengthens%2520the%2520validity%2520of%2520ANODEC%252C%2520but%2520it%2520marks%2520an%2520important%2520step%250Atowards%2520more%2520practical%252C%2520easy-to-use%2520SRs%2520that%2520can%2520automatically%2520learn%2520to%2520perform%250Aagile%2520motions%2520from%2520minimal%2520experimental%2520interaction%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03754v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Soft%20Robotic%20System%20Automatically%20Learns%20Precise%20Agile%20Motions%20Without%0A%20%20Model%20Information&entry.906535625=Simon%20Bachhuber%20and%20Alexander%20Pawluchin%20and%20Arka%20Pal%20and%20Ivo%20Boblan%20and%20Thomas%20Seel&entry.1292438233=%20%20Many%20application%20domains%2C%20e.g.%2C%20in%20medicine%20and%20manufacturing%2C%20can%20greatly%0Abenefit%20from%20pneumatic%20Soft%20Robots%20%28SRs%29.%20However%2C%20the%20accurate%20control%20of%20SRs%0Ahas%20remained%20a%20significant%20challenge%20to%20date%2C%20mainly%20due%20to%20their%20nonlinear%0Adynamics%20and%20viscoelastic%20material%20properties.%20Conventional%20control%20design%0Amethods%20often%20rely%20on%20either%20complex%20system%20modeling%20or%20time-intensive%20manual%0Atuning%2C%20both%20of%20which%20require%20significant%20amounts%20of%20human%20expertise%20and%20thus%0Alimit%20their%20practicality.%20In%20recent%20works%2C%20the%20data-driven%20method%2C%20Automatic%0ANeural%20ODE%20Control%20%28ANODEC%29%20has%20been%20successfully%20used%20to%20--%20fully%0Aautomatically%20and%20utilizing%20only%20input-output%20data%20--%20design%20controllers%20for%0Avarious%20nonlinear%20systems%20in%20silico%2C%20and%20without%20requiring%20prior%20model%0Aknowledge%20or%20extensive%20manual%20tuning.%20In%20this%20work%2C%20we%20successfully%20apply%0AANODEC%20to%20automatically%20learn%20to%20perform%20agile%2C%20non-repetitive%20reference%0Atracking%20motion%20tasks%20in%20a%20real-world%20SR%20and%20within%20a%20finite%20time%20horizon.%20To%0Athe%20best%20of%20the%20authors%27%20knowledge%2C%20ANODEC%20achieves%2C%20for%20the%20first%20time%2C%0Aperformant%20control%20of%20a%20SR%20with%20hysteresis%20effects%20from%20only%2030%20seconds%20of%0Ainput-output%20data%20and%20without%20any%20prior%20model%20knowledge.%20We%20show%20that%20for%0Amultiple%2C%20qualitatively%20different%20and%20even%20out-of-training-distribution%0Areference%20signals%2C%20a%20single%20feedback%20controller%20designed%20by%20ANODEC%20outperforms%0Aa%20manually%20tuned%20PID%20baseline%20consistently.%20Overall%2C%20this%20contribution%20not%20only%0Afurther%20strengthens%20the%20validity%20of%20ANODEC%2C%20but%20it%20marks%20an%20important%20step%0Atowards%20more%20practical%2C%20easy-to-use%20SRs%20that%20can%20automatically%20learn%20to%20perform%0Aagile%20motions%20from%20minimal%20experimental%20interaction%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03754v2&entry.124074799=Read"},
{"title": "Unified Discrete Diffusion for Categorical Data", "author": "Lingxiao Zhao and Xueying Ding and Lijun Yu and Leman Akoglu", "abstract": "  Discrete diffusion models have seen a surge of attention with applications on\nnaturally discrete data such as language and graphs. Although discrete-time\ndiscrete diffusion has been established for a while, only recently Campbell et\nal. (2022) introduced the first framework for continuous-time discrete\ndiffusion. However, their training and sampling processes differ significantly\nfrom the discrete-time version, necessitating nontrivial approximations for\ntractability. In this paper, we first present a series of mathematical\nsimplifications of the variational lower bound that enable more accurate and\neasy-to-optimize training for discrete diffusion. In addition, we derive a\nsimple formulation for backward denoising that enables exact and accelerated\nsampling, and importantly, an elegant unification of discrete-time and\ncontinuous-time discrete diffusion. Thanks to simpler analytical formulations,\nboth forward and now also backward probabilities can flexibly accommodate any\nnoise distribution, including different noise distributions for multi-element\nobjects. Experiments show that our proposed USD3 (for Unified Simplified\nDiscrete Denoising Diffusion) outperform all SOTA baselines on established\ndatasets. We open-source our unified code at\nhttps://github.com/LingxiaoShawn/USD3.\n", "link": "http://arxiv.org/abs/2402.03701v2", "date": "2024-08-12", "relevancy": 1.6709, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6111}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5516}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20Discrete%20Diffusion%20for%20Categorical%20Data&body=Title%3A%20Unified%20Discrete%20Diffusion%20for%20Categorical%20Data%0AAuthor%3A%20Lingxiao%20Zhao%20and%20Xueying%20Ding%20and%20Lijun%20Yu%20and%20Leman%20Akoglu%0AAbstract%3A%20%20%20Discrete%20diffusion%20models%20have%20seen%20a%20surge%20of%20attention%20with%20applications%20on%0Anaturally%20discrete%20data%20such%20as%20language%20and%20graphs.%20Although%20discrete-time%0Adiscrete%20diffusion%20has%20been%20established%20for%20a%20while%2C%20only%20recently%20Campbell%20et%0Aal.%20%282022%29%20introduced%20the%20first%20framework%20for%20continuous-time%20discrete%0Adiffusion.%20However%2C%20their%20training%20and%20sampling%20processes%20differ%20significantly%0Afrom%20the%20discrete-time%20version%2C%20necessitating%20nontrivial%20approximations%20for%0Atractability.%20In%20this%20paper%2C%20we%20first%20present%20a%20series%20of%20mathematical%0Asimplifications%20of%20the%20variational%20lower%20bound%20that%20enable%20more%20accurate%20and%0Aeasy-to-optimize%20training%20for%20discrete%20diffusion.%20In%20addition%2C%20we%20derive%20a%0Asimple%20formulation%20for%20backward%20denoising%20that%20enables%20exact%20and%20accelerated%0Asampling%2C%20and%20importantly%2C%20an%20elegant%20unification%20of%20discrete-time%20and%0Acontinuous-time%20discrete%20diffusion.%20Thanks%20to%20simpler%20analytical%20formulations%2C%0Aboth%20forward%20and%20now%20also%20backward%20probabilities%20can%20flexibly%20accommodate%20any%0Anoise%20distribution%2C%20including%20different%20noise%20distributions%20for%20multi-element%0Aobjects.%20Experiments%20show%20that%20our%20proposed%20USD3%20%28for%20Unified%20Simplified%0ADiscrete%20Denoising%20Diffusion%29%20outperform%20all%20SOTA%20baselines%20on%20established%0Adatasets.%20We%20open-source%20our%20unified%20code%20at%0Ahttps%3A//github.com/LingxiaoShawn/USD3.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03701v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520Discrete%2520Diffusion%2520for%2520Categorical%2520Data%26entry.906535625%3DLingxiao%2520Zhao%2520and%2520Xueying%2520Ding%2520and%2520Lijun%2520Yu%2520and%2520Leman%2520Akoglu%26entry.1292438233%3D%2520%2520Discrete%2520diffusion%2520models%2520have%2520seen%2520a%2520surge%2520of%2520attention%2520with%2520applications%2520on%250Anaturally%2520discrete%2520data%2520such%2520as%2520language%2520and%2520graphs.%2520Although%2520discrete-time%250Adiscrete%2520diffusion%2520has%2520been%2520established%2520for%2520a%2520while%252C%2520only%2520recently%2520Campbell%2520et%250Aal.%2520%25282022%2529%2520introduced%2520the%2520first%2520framework%2520for%2520continuous-time%2520discrete%250Adiffusion.%2520However%252C%2520their%2520training%2520and%2520sampling%2520processes%2520differ%2520significantly%250Afrom%2520the%2520discrete-time%2520version%252C%2520necessitating%2520nontrivial%2520approximations%2520for%250Atractability.%2520In%2520this%2520paper%252C%2520we%2520first%2520present%2520a%2520series%2520of%2520mathematical%250Asimplifications%2520of%2520the%2520variational%2520lower%2520bound%2520that%2520enable%2520more%2520accurate%2520and%250Aeasy-to-optimize%2520training%2520for%2520discrete%2520diffusion.%2520In%2520addition%252C%2520we%2520derive%2520a%250Asimple%2520formulation%2520for%2520backward%2520denoising%2520that%2520enables%2520exact%2520and%2520accelerated%250Asampling%252C%2520and%2520importantly%252C%2520an%2520elegant%2520unification%2520of%2520discrete-time%2520and%250Acontinuous-time%2520discrete%2520diffusion.%2520Thanks%2520to%2520simpler%2520analytical%2520formulations%252C%250Aboth%2520forward%2520and%2520now%2520also%2520backward%2520probabilities%2520can%2520flexibly%2520accommodate%2520any%250Anoise%2520distribution%252C%2520including%2520different%2520noise%2520distributions%2520for%2520multi-element%250Aobjects.%2520Experiments%2520show%2520that%2520our%2520proposed%2520USD3%2520%2528for%2520Unified%2520Simplified%250ADiscrete%2520Denoising%2520Diffusion%2529%2520outperform%2520all%2520SOTA%2520baselines%2520on%2520established%250Adatasets.%2520We%2520open-source%2520our%2520unified%2520code%2520at%250Ahttps%253A//github.com/LingxiaoShawn/USD3.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03701v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Discrete%20Diffusion%20for%20Categorical%20Data&entry.906535625=Lingxiao%20Zhao%20and%20Xueying%20Ding%20and%20Lijun%20Yu%20and%20Leman%20Akoglu&entry.1292438233=%20%20Discrete%20diffusion%20models%20have%20seen%20a%20surge%20of%20attention%20with%20applications%20on%0Anaturally%20discrete%20data%20such%20as%20language%20and%20graphs.%20Although%20discrete-time%0Adiscrete%20diffusion%20has%20been%20established%20for%20a%20while%2C%20only%20recently%20Campbell%20et%0Aal.%20%282022%29%20introduced%20the%20first%20framework%20for%20continuous-time%20discrete%0Adiffusion.%20However%2C%20their%20training%20and%20sampling%20processes%20differ%20significantly%0Afrom%20the%20discrete-time%20version%2C%20necessitating%20nontrivial%20approximations%20for%0Atractability.%20In%20this%20paper%2C%20we%20first%20present%20a%20series%20of%20mathematical%0Asimplifications%20of%20the%20variational%20lower%20bound%20that%20enable%20more%20accurate%20and%0Aeasy-to-optimize%20training%20for%20discrete%20diffusion.%20In%20addition%2C%20we%20derive%20a%0Asimple%20formulation%20for%20backward%20denoising%20that%20enables%20exact%20and%20accelerated%0Asampling%2C%20and%20importantly%2C%20an%20elegant%20unification%20of%20discrete-time%20and%0Acontinuous-time%20discrete%20diffusion.%20Thanks%20to%20simpler%20analytical%20formulations%2C%0Aboth%20forward%20and%20now%20also%20backward%20probabilities%20can%20flexibly%20accommodate%20any%0Anoise%20distribution%2C%20including%20different%20noise%20distributions%20for%20multi-element%0Aobjects.%20Experiments%20show%20that%20our%20proposed%20USD3%20%28for%20Unified%20Simplified%0ADiscrete%20Denoising%20Diffusion%29%20outperform%20all%20SOTA%20baselines%20on%20established%0Adatasets.%20We%20open-source%20our%20unified%20code%20at%0Ahttps%3A//github.com/LingxiaoShawn/USD3.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03701v2&entry.124074799=Read"},
{"title": "MC-GPT: Empowering Vision-and-Language Navigation with Memory Map and\n  Reasoning Chains", "author": "Zhaohuan Zhan and Lisha Yu and Sijie Yu and Guang Tan", "abstract": "  In the Vision-and-Language Navigation (VLN) task, the agent is required to\nnavigate to a destination following a natural language instruction. While\nlearning-based approaches have been a major solution to the task, they suffer\nfrom high training costs and lack of interpretability. Recently, Large Language\nModels (LLMs) have emerged as a promising tool for VLN due to their strong\ngeneralization capabilities. However, existing LLM-based methods face\nlimitations in memory construction and diversity of navigation strategies. To\naddress these challenges, we propose a suite of techniques. Firstly, we\nintroduce a method to maintain a topological map that stores navigation\nhistory, retaining information about viewpoints, objects, and their spatial\nrelationships. This map also serves as a global action space. Additionally, we\npresent a Navigation Chain of Thoughts module, leveraging human navigation\nexamples to enrich navigation strategy diversity. Finally, we establish a\npipeline that integrates navigational memory and strategies with perception and\naction prediction modules. Experimental results on the REVERIE and R2R datasets\nshow that our method effectively enhances the navigation ability of the LLM and\nimproves the interpretability of navigation reasoning.\n", "link": "http://arxiv.org/abs/2405.10620v2", "date": "2024-08-12", "relevancy": 1.657, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5564}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5523}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MC-GPT%3A%20Empowering%20Vision-and-Language%20Navigation%20with%20Memory%20Map%20and%0A%20%20Reasoning%20Chains&body=Title%3A%20MC-GPT%3A%20Empowering%20Vision-and-Language%20Navigation%20with%20Memory%20Map%20and%0A%20%20Reasoning%20Chains%0AAuthor%3A%20Zhaohuan%20Zhan%20and%20Lisha%20Yu%20and%20Sijie%20Yu%20and%20Guang%20Tan%0AAbstract%3A%20%20%20In%20the%20Vision-and-Language%20Navigation%20%28VLN%29%20task%2C%20the%20agent%20is%20required%20to%0Anavigate%20to%20a%20destination%20following%20a%20natural%20language%20instruction.%20While%0Alearning-based%20approaches%20have%20been%20a%20major%20solution%20to%20the%20task%2C%20they%20suffer%0Afrom%20high%20training%20costs%20and%20lack%20of%20interpretability.%20Recently%2C%20Large%20Language%0AModels%20%28LLMs%29%20have%20emerged%20as%20a%20promising%20tool%20for%20VLN%20due%20to%20their%20strong%0Ageneralization%20capabilities.%20However%2C%20existing%20LLM-based%20methods%20face%0Alimitations%20in%20memory%20construction%20and%20diversity%20of%20navigation%20strategies.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20a%20suite%20of%20techniques.%20Firstly%2C%20we%0Aintroduce%20a%20method%20to%20maintain%20a%20topological%20map%20that%20stores%20navigation%0Ahistory%2C%20retaining%20information%20about%20viewpoints%2C%20objects%2C%20and%20their%20spatial%0Arelationships.%20This%20map%20also%20serves%20as%20a%20global%20action%20space.%20Additionally%2C%20we%0Apresent%20a%20Navigation%20Chain%20of%20Thoughts%20module%2C%20leveraging%20human%20navigation%0Aexamples%20to%20enrich%20navigation%20strategy%20diversity.%20Finally%2C%20we%20establish%20a%0Apipeline%20that%20integrates%20navigational%20memory%20and%20strategies%20with%20perception%20and%0Aaction%20prediction%20modules.%20Experimental%20results%20on%20the%20REVERIE%20and%20R2R%20datasets%0Ashow%20that%20our%20method%20effectively%20enhances%20the%20navigation%20ability%20of%20the%20LLM%20and%0Aimproves%20the%20interpretability%20of%20navigation%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10620v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMC-GPT%253A%2520Empowering%2520Vision-and-Language%2520Navigation%2520with%2520Memory%2520Map%2520and%250A%2520%2520Reasoning%2520Chains%26entry.906535625%3DZhaohuan%2520Zhan%2520and%2520Lisha%2520Yu%2520and%2520Sijie%2520Yu%2520and%2520Guang%2520Tan%26entry.1292438233%3D%2520%2520In%2520the%2520Vision-and-Language%2520Navigation%2520%2528VLN%2529%2520task%252C%2520the%2520agent%2520is%2520required%2520to%250Anavigate%2520to%2520a%2520destination%2520following%2520a%2520natural%2520language%2520instruction.%2520While%250Alearning-based%2520approaches%2520have%2520been%2520a%2520major%2520solution%2520to%2520the%2520task%252C%2520they%2520suffer%250Afrom%2520high%2520training%2520costs%2520and%2520lack%2520of%2520interpretability.%2520Recently%252C%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520have%2520emerged%2520as%2520a%2520promising%2520tool%2520for%2520VLN%2520due%2520to%2520their%2520strong%250Ageneralization%2520capabilities.%2520However%252C%2520existing%2520LLM-based%2520methods%2520face%250Alimitations%2520in%2520memory%2520construction%2520and%2520diversity%2520of%2520navigation%2520strategies.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520propose%2520a%2520suite%2520of%2520techniques.%2520Firstly%252C%2520we%250Aintroduce%2520a%2520method%2520to%2520maintain%2520a%2520topological%2520map%2520that%2520stores%2520navigation%250Ahistory%252C%2520retaining%2520information%2520about%2520viewpoints%252C%2520objects%252C%2520and%2520their%2520spatial%250Arelationships.%2520This%2520map%2520also%2520serves%2520as%2520a%2520global%2520action%2520space.%2520Additionally%252C%2520we%250Apresent%2520a%2520Navigation%2520Chain%2520of%2520Thoughts%2520module%252C%2520leveraging%2520human%2520navigation%250Aexamples%2520to%2520enrich%2520navigation%2520strategy%2520diversity.%2520Finally%252C%2520we%2520establish%2520a%250Apipeline%2520that%2520integrates%2520navigational%2520memory%2520and%2520strategies%2520with%2520perception%2520and%250Aaction%2520prediction%2520modules.%2520Experimental%2520results%2520on%2520the%2520REVERIE%2520and%2520R2R%2520datasets%250Ashow%2520that%2520our%2520method%2520effectively%2520enhances%2520the%2520navigation%2520ability%2520of%2520the%2520LLM%2520and%250Aimproves%2520the%2520interpretability%2520of%2520navigation%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10620v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MC-GPT%3A%20Empowering%20Vision-and-Language%20Navigation%20with%20Memory%20Map%20and%0A%20%20Reasoning%20Chains&entry.906535625=Zhaohuan%20Zhan%20and%20Lisha%20Yu%20and%20Sijie%20Yu%20and%20Guang%20Tan&entry.1292438233=%20%20In%20the%20Vision-and-Language%20Navigation%20%28VLN%29%20task%2C%20the%20agent%20is%20required%20to%0Anavigate%20to%20a%20destination%20following%20a%20natural%20language%20instruction.%20While%0Alearning-based%20approaches%20have%20been%20a%20major%20solution%20to%20the%20task%2C%20they%20suffer%0Afrom%20high%20training%20costs%20and%20lack%20of%20interpretability.%20Recently%2C%20Large%20Language%0AModels%20%28LLMs%29%20have%20emerged%20as%20a%20promising%20tool%20for%20VLN%20due%20to%20their%20strong%0Ageneralization%20capabilities.%20However%2C%20existing%20LLM-based%20methods%20face%0Alimitations%20in%20memory%20construction%20and%20diversity%20of%20navigation%20strategies.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20a%20suite%20of%20techniques.%20Firstly%2C%20we%0Aintroduce%20a%20method%20to%20maintain%20a%20topological%20map%20that%20stores%20navigation%0Ahistory%2C%20retaining%20information%20about%20viewpoints%2C%20objects%2C%20and%20their%20spatial%0Arelationships.%20This%20map%20also%20serves%20as%20a%20global%20action%20space.%20Additionally%2C%20we%0Apresent%20a%20Navigation%20Chain%20of%20Thoughts%20module%2C%20leveraging%20human%20navigation%0Aexamples%20to%20enrich%20navigation%20strategy%20diversity.%20Finally%2C%20we%20establish%20a%0Apipeline%20that%20integrates%20navigational%20memory%20and%20strategies%20with%20perception%20and%0Aaction%20prediction%20modules.%20Experimental%20results%20on%20the%20REVERIE%20and%20R2R%20datasets%0Ashow%20that%20our%20method%20effectively%20enhances%20the%20navigation%20ability%20of%20the%20LLM%20and%0Aimproves%20the%20interpretability%20of%20navigation%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10620v2&entry.124074799=Read"},
{"title": "EqNIO: Subequivariant Neural Inertial Odometry", "author": "Royina Karegoudra Jayanth and Yinshuang Xu and Ziyun Wang and Evangelos Chatzipantazis and Daniel Gehrig and Kostas Daniilidis", "abstract": "  Presently, neural networks are widely employed to accurately estimate 2D\ndisplacements and associated uncertainties from Inertial Measurement Unit (IMU)\ndata that can be integrated into stochastic filter networks like the Extended\nKalman Filter (EKF) as measurements and uncertainties for the update step in\nthe filter. However, such neural approaches overlook symmetry which is a\ncrucial inductive bias for model generalization. This oversight is notable\nbecause (i) physical laws adhere to symmetry principles when considering the\ngravity axis, meaning there exists the same transformation for both the\nphysical entity and the resulting trajectory, and (ii) displacements should\nremain equivariant to frame transformations when the inertial frame changes. To\naddress this, we propose a subequivariant framework by: (i) deriving\nfundamental layers such as linear and nonlinear layers for a subequivariant\nnetwork, designed to handle sequences of vectors and scalars, (ii) employing\nthe subequivariant network to predict an equivariant frame for the sequence of\ninertial measurements. This predicted frame can then be utilized for extracting\ninvariant features through projection, which are integrated with arbitrary\nnetwork architectures, (iii) transforming the invariant output by frame\ntransformation to obtain equivariant displacements and covariances. We\ndemonstrate the effectiveness and generalization of our Equivariant Framework\non a filter-based approach with TLIO architecture for TLIO and Aria datasets,\nand an end-to-end deep learning approach with RONIN architecture for RONIN,\nRIDI and OxIOD datasets.\n", "link": "http://arxiv.org/abs/2408.06321v1", "date": "2024-08-12", "relevancy": 1.6486, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5553}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5524}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EqNIO%3A%20Subequivariant%20Neural%20Inertial%20Odometry&body=Title%3A%20EqNIO%3A%20Subequivariant%20Neural%20Inertial%20Odometry%0AAuthor%3A%20Royina%20Karegoudra%20Jayanth%20and%20Yinshuang%20Xu%20and%20Ziyun%20Wang%20and%20Evangelos%20Chatzipantazis%20and%20Daniel%20Gehrig%20and%20Kostas%20Daniilidis%0AAbstract%3A%20%20%20Presently%2C%20neural%20networks%20are%20widely%20employed%20to%20accurately%20estimate%202D%0Adisplacements%20and%20associated%20uncertainties%20from%20Inertial%20Measurement%20Unit%20%28IMU%29%0Adata%20that%20can%20be%20integrated%20into%20stochastic%20filter%20networks%20like%20the%20Extended%0AKalman%20Filter%20%28EKF%29%20as%20measurements%20and%20uncertainties%20for%20the%20update%20step%20in%0Athe%20filter.%20However%2C%20such%20neural%20approaches%20overlook%20symmetry%20which%20is%20a%0Acrucial%20inductive%20bias%20for%20model%20generalization.%20This%20oversight%20is%20notable%0Abecause%20%28i%29%20physical%20laws%20adhere%20to%20symmetry%20principles%20when%20considering%20the%0Agravity%20axis%2C%20meaning%20there%20exists%20the%20same%20transformation%20for%20both%20the%0Aphysical%20entity%20and%20the%20resulting%20trajectory%2C%20and%20%28ii%29%20displacements%20should%0Aremain%20equivariant%20to%20frame%20transformations%20when%20the%20inertial%20frame%20changes.%20To%0Aaddress%20this%2C%20we%20propose%20a%20subequivariant%20framework%20by%3A%20%28i%29%20deriving%0Afundamental%20layers%20such%20as%20linear%20and%20nonlinear%20layers%20for%20a%20subequivariant%0Anetwork%2C%20designed%20to%20handle%20sequences%20of%20vectors%20and%20scalars%2C%20%28ii%29%20employing%0Athe%20subequivariant%20network%20to%20predict%20an%20equivariant%20frame%20for%20the%20sequence%20of%0Ainertial%20measurements.%20This%20predicted%20frame%20can%20then%20be%20utilized%20for%20extracting%0Ainvariant%20features%20through%20projection%2C%20which%20are%20integrated%20with%20arbitrary%0Anetwork%20architectures%2C%20%28iii%29%20transforming%20the%20invariant%20output%20by%20frame%0Atransformation%20to%20obtain%20equivariant%20displacements%20and%20covariances.%20We%0Ademonstrate%20the%20effectiveness%20and%20generalization%20of%20our%20Equivariant%20Framework%0Aon%20a%20filter-based%20approach%20with%20TLIO%20architecture%20for%20TLIO%20and%20Aria%20datasets%2C%0Aand%20an%20end-to-end%20deep%20learning%20approach%20with%20RONIN%20architecture%20for%20RONIN%2C%0ARIDI%20and%20OxIOD%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06321v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEqNIO%253A%2520Subequivariant%2520Neural%2520Inertial%2520Odometry%26entry.906535625%3DRoyina%2520Karegoudra%2520Jayanth%2520and%2520Yinshuang%2520Xu%2520and%2520Ziyun%2520Wang%2520and%2520Evangelos%2520Chatzipantazis%2520and%2520Daniel%2520Gehrig%2520and%2520Kostas%2520Daniilidis%26entry.1292438233%3D%2520%2520Presently%252C%2520neural%2520networks%2520are%2520widely%2520employed%2520to%2520accurately%2520estimate%25202D%250Adisplacements%2520and%2520associated%2520uncertainties%2520from%2520Inertial%2520Measurement%2520Unit%2520%2528IMU%2529%250Adata%2520that%2520can%2520be%2520integrated%2520into%2520stochastic%2520filter%2520networks%2520like%2520the%2520Extended%250AKalman%2520Filter%2520%2528EKF%2529%2520as%2520measurements%2520and%2520uncertainties%2520for%2520the%2520update%2520step%2520in%250Athe%2520filter.%2520However%252C%2520such%2520neural%2520approaches%2520overlook%2520symmetry%2520which%2520is%2520a%250Acrucial%2520inductive%2520bias%2520for%2520model%2520generalization.%2520This%2520oversight%2520is%2520notable%250Abecause%2520%2528i%2529%2520physical%2520laws%2520adhere%2520to%2520symmetry%2520principles%2520when%2520considering%2520the%250Agravity%2520axis%252C%2520meaning%2520there%2520exists%2520the%2520same%2520transformation%2520for%2520both%2520the%250Aphysical%2520entity%2520and%2520the%2520resulting%2520trajectory%252C%2520and%2520%2528ii%2529%2520displacements%2520should%250Aremain%2520equivariant%2520to%2520frame%2520transformations%2520when%2520the%2520inertial%2520frame%2520changes.%2520To%250Aaddress%2520this%252C%2520we%2520propose%2520a%2520subequivariant%2520framework%2520by%253A%2520%2528i%2529%2520deriving%250Afundamental%2520layers%2520such%2520as%2520linear%2520and%2520nonlinear%2520layers%2520for%2520a%2520subequivariant%250Anetwork%252C%2520designed%2520to%2520handle%2520sequences%2520of%2520vectors%2520and%2520scalars%252C%2520%2528ii%2529%2520employing%250Athe%2520subequivariant%2520network%2520to%2520predict%2520an%2520equivariant%2520frame%2520for%2520the%2520sequence%2520of%250Ainertial%2520measurements.%2520This%2520predicted%2520frame%2520can%2520then%2520be%2520utilized%2520for%2520extracting%250Ainvariant%2520features%2520through%2520projection%252C%2520which%2520are%2520integrated%2520with%2520arbitrary%250Anetwork%2520architectures%252C%2520%2528iii%2529%2520transforming%2520the%2520invariant%2520output%2520by%2520frame%250Atransformation%2520to%2520obtain%2520equivariant%2520displacements%2520and%2520covariances.%2520We%250Ademonstrate%2520the%2520effectiveness%2520and%2520generalization%2520of%2520our%2520Equivariant%2520Framework%250Aon%2520a%2520filter-based%2520approach%2520with%2520TLIO%2520architecture%2520for%2520TLIO%2520and%2520Aria%2520datasets%252C%250Aand%2520an%2520end-to-end%2520deep%2520learning%2520approach%2520with%2520RONIN%2520architecture%2520for%2520RONIN%252C%250ARIDI%2520and%2520OxIOD%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06321v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EqNIO%3A%20Subequivariant%20Neural%20Inertial%20Odometry&entry.906535625=Royina%20Karegoudra%20Jayanth%20and%20Yinshuang%20Xu%20and%20Ziyun%20Wang%20and%20Evangelos%20Chatzipantazis%20and%20Daniel%20Gehrig%20and%20Kostas%20Daniilidis&entry.1292438233=%20%20Presently%2C%20neural%20networks%20are%20widely%20employed%20to%20accurately%20estimate%202D%0Adisplacements%20and%20associated%20uncertainties%20from%20Inertial%20Measurement%20Unit%20%28IMU%29%0Adata%20that%20can%20be%20integrated%20into%20stochastic%20filter%20networks%20like%20the%20Extended%0AKalman%20Filter%20%28EKF%29%20as%20measurements%20and%20uncertainties%20for%20the%20update%20step%20in%0Athe%20filter.%20However%2C%20such%20neural%20approaches%20overlook%20symmetry%20which%20is%20a%0Acrucial%20inductive%20bias%20for%20model%20generalization.%20This%20oversight%20is%20notable%0Abecause%20%28i%29%20physical%20laws%20adhere%20to%20symmetry%20principles%20when%20considering%20the%0Agravity%20axis%2C%20meaning%20there%20exists%20the%20same%20transformation%20for%20both%20the%0Aphysical%20entity%20and%20the%20resulting%20trajectory%2C%20and%20%28ii%29%20displacements%20should%0Aremain%20equivariant%20to%20frame%20transformations%20when%20the%20inertial%20frame%20changes.%20To%0Aaddress%20this%2C%20we%20propose%20a%20subequivariant%20framework%20by%3A%20%28i%29%20deriving%0Afundamental%20layers%20such%20as%20linear%20and%20nonlinear%20layers%20for%20a%20subequivariant%0Anetwork%2C%20designed%20to%20handle%20sequences%20of%20vectors%20and%20scalars%2C%20%28ii%29%20employing%0Athe%20subequivariant%20network%20to%20predict%20an%20equivariant%20frame%20for%20the%20sequence%20of%0Ainertial%20measurements.%20This%20predicted%20frame%20can%20then%20be%20utilized%20for%20extracting%0Ainvariant%20features%20through%20projection%2C%20which%20are%20integrated%20with%20arbitrary%0Anetwork%20architectures%2C%20%28iii%29%20transforming%20the%20invariant%20output%20by%20frame%0Atransformation%20to%20obtain%20equivariant%20displacements%20and%20covariances.%20We%0Ademonstrate%20the%20effectiveness%20and%20generalization%20of%20our%20Equivariant%20Framework%0Aon%20a%20filter-based%20approach%20with%20TLIO%20architecture%20for%20TLIO%20and%20Aria%20datasets%2C%0Aand%20an%20end-to-end%20deep%20learning%20approach%20with%20RONIN%20architecture%20for%20RONIN%2C%0ARIDI%20and%20OxIOD%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06321v1&entry.124074799=Read"},
{"title": "A Methodological Report on Anomaly Detection on Dynamic Knowledge Graphs", "author": "Xiaohua Lu and Leshanshui Yang", "abstract": "  In this paper, we explore different approaches to anomaly detection on\ndynamic knowledge graphs, specifically in a microservices environment for\nKubernetes applications. Our approach explores three dynamic knowledge graph\nrepresentations: sequential data, one-hop graph structure, and two-hop graph\nstructure, with each representation incorporating increasingly complex\nstructural information. Each phase includes different machine learning and deep\nlearning models. We empirically analyse their performance and propose an\napproach based on ensemble learning of these models. Our approach significantly\noutperforms the baseline on the ISWC 2024 Dynamic Knowledge Graph Anomaly\nDetection dataset, providing a robust solution for anomaly detection in dynamic\ncomplex data.\n", "link": "http://arxiv.org/abs/2408.06121v1", "date": "2024-08-12", "relevancy": 1.3038, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4376}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4313}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Methodological%20Report%20on%20Anomaly%20Detection%20on%20Dynamic%20Knowledge%20Graphs&body=Title%3A%20A%20Methodological%20Report%20on%20Anomaly%20Detection%20on%20Dynamic%20Knowledge%20Graphs%0AAuthor%3A%20Xiaohua%20Lu%20and%20Leshanshui%20Yang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20explore%20different%20approaches%20to%20anomaly%20detection%20on%0Adynamic%20knowledge%20graphs%2C%20specifically%20in%20a%20microservices%20environment%20for%0AKubernetes%20applications.%20Our%20approach%20explores%20three%20dynamic%20knowledge%20graph%0Arepresentations%3A%20sequential%20data%2C%20one-hop%20graph%20structure%2C%20and%20two-hop%20graph%0Astructure%2C%20with%20each%20representation%20incorporating%20increasingly%20complex%0Astructural%20information.%20Each%20phase%20includes%20different%20machine%20learning%20and%20deep%0Alearning%20models.%20We%20empirically%20analyse%20their%20performance%20and%20propose%20an%0Aapproach%20based%20on%20ensemble%20learning%20of%20these%20models.%20Our%20approach%20significantly%0Aoutperforms%20the%20baseline%20on%20the%20ISWC%202024%20Dynamic%20Knowledge%20Graph%20Anomaly%0ADetection%20dataset%2C%20providing%20a%20robust%20solution%20for%20anomaly%20detection%20in%20dynamic%0Acomplex%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06121v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Methodological%2520Report%2520on%2520Anomaly%2520Detection%2520on%2520Dynamic%2520Knowledge%2520Graphs%26entry.906535625%3DXiaohua%2520Lu%2520and%2520Leshanshui%2520Yang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520explore%2520different%2520approaches%2520to%2520anomaly%2520detection%2520on%250Adynamic%2520knowledge%2520graphs%252C%2520specifically%2520in%2520a%2520microservices%2520environment%2520for%250AKubernetes%2520applications.%2520Our%2520approach%2520explores%2520three%2520dynamic%2520knowledge%2520graph%250Arepresentations%253A%2520sequential%2520data%252C%2520one-hop%2520graph%2520structure%252C%2520and%2520two-hop%2520graph%250Astructure%252C%2520with%2520each%2520representation%2520incorporating%2520increasingly%2520complex%250Astructural%2520information.%2520Each%2520phase%2520includes%2520different%2520machine%2520learning%2520and%2520deep%250Alearning%2520models.%2520We%2520empirically%2520analyse%2520their%2520performance%2520and%2520propose%2520an%250Aapproach%2520based%2520on%2520ensemble%2520learning%2520of%2520these%2520models.%2520Our%2520approach%2520significantly%250Aoutperforms%2520the%2520baseline%2520on%2520the%2520ISWC%25202024%2520Dynamic%2520Knowledge%2520Graph%2520Anomaly%250ADetection%2520dataset%252C%2520providing%2520a%2520robust%2520solution%2520for%2520anomaly%2520detection%2520in%2520dynamic%250Acomplex%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06121v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Methodological%20Report%20on%20Anomaly%20Detection%20on%20Dynamic%20Knowledge%20Graphs&entry.906535625=Xiaohua%20Lu%20and%20Leshanshui%20Yang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20explore%20different%20approaches%20to%20anomaly%20detection%20on%0Adynamic%20knowledge%20graphs%2C%20specifically%20in%20a%20microservices%20environment%20for%0AKubernetes%20applications.%20Our%20approach%20explores%20three%20dynamic%20knowledge%20graph%0Arepresentations%3A%20sequential%20data%2C%20one-hop%20graph%20structure%2C%20and%20two-hop%20graph%0Astructure%2C%20with%20each%20representation%20incorporating%20increasingly%20complex%0Astructural%20information.%20Each%20phase%20includes%20different%20machine%20learning%20and%20deep%0Alearning%20models.%20We%20empirically%20analyse%20their%20performance%20and%20propose%20an%0Aapproach%20based%20on%20ensemble%20learning%20of%20these%20models.%20Our%20approach%20significantly%0Aoutperforms%20the%20baseline%20on%20the%20ISWC%202024%20Dynamic%20Knowledge%20Graph%20Anomaly%0ADetection%20dataset%2C%20providing%20a%20robust%20solution%20for%20anomaly%20detection%20in%20dynamic%0Acomplex%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06121v1&entry.124074799=Read"},
{"title": "Private Fine-tuning of Large Language Models with Zeroth-order\n  Optimization", "author": "Xinyu Tang and Ashwinee Panda and Milad Nasr and Saeed Mahloujifar and Prateek Mittal", "abstract": "  Differentially private stochastic gradient descent (DP-SGD) allows models to\nbe trained in a privacy-preserving manner, but has proven difficult to scale to\nthe era of foundation models. We introduce DP-ZO, a private fine-tuning\nframework for large language models by privatizing zeroth order optimization\nmethods. A key insight into the design of our method is that the direction of\nthe gradient in the zeroth-order optimization we use is random and the only\ninformation from training data is the step size, i.e., a scalar. Therefore, we\nonly need to privatize the scalar step size, which is memory-efficient. DP-ZO\nprovides a strong privacy-utility trade-off across different tasks, and model\nsizes that are comparable to DP-SGD in $(\\varepsilon,\\delta)$-DP. Notably,\nDP-ZO possesses significant advantages over DP-SGD in memory efficiency, and\nobtains higher utility in $\\varepsilon$-DP when using the Laplace mechanism.\n", "link": "http://arxiv.org/abs/2401.04343v2", "date": "2024-08-12", "relevancy": 1.4765, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.513}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4916}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4726}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Private%20Fine-tuning%20of%20Large%20Language%20Models%20with%20Zeroth-order%0A%20%20Optimization&body=Title%3A%20Private%20Fine-tuning%20of%20Large%20Language%20Models%20with%20Zeroth-order%0A%20%20Optimization%0AAuthor%3A%20Xinyu%20Tang%20and%20Ashwinee%20Panda%20and%20Milad%20Nasr%20and%20Saeed%20Mahloujifar%20and%20Prateek%20Mittal%0AAbstract%3A%20%20%20Differentially%20private%20stochastic%20gradient%20descent%20%28DP-SGD%29%20allows%20models%20to%0Abe%20trained%20in%20a%20privacy-preserving%20manner%2C%20but%20has%20proven%20difficult%20to%20scale%20to%0Athe%20era%20of%20foundation%20models.%20We%20introduce%20DP-ZO%2C%20a%20private%20fine-tuning%0Aframework%20for%20large%20language%20models%20by%20privatizing%20zeroth%20order%20optimization%0Amethods.%20A%20key%20insight%20into%20the%20design%20of%20our%20method%20is%20that%20the%20direction%20of%0Athe%20gradient%20in%20the%20zeroth-order%20optimization%20we%20use%20is%20random%20and%20the%20only%0Ainformation%20from%20training%20data%20is%20the%20step%20size%2C%20i.e.%2C%20a%20scalar.%20Therefore%2C%20we%0Aonly%20need%20to%20privatize%20the%20scalar%20step%20size%2C%20which%20is%20memory-efficient.%20DP-ZO%0Aprovides%20a%20strong%20privacy-utility%20trade-off%20across%20different%20tasks%2C%20and%20model%0Asizes%20that%20are%20comparable%20to%20DP-SGD%20in%20%24%28%5Cvarepsilon%2C%5Cdelta%29%24-DP.%20Notably%2C%0ADP-ZO%20possesses%20significant%20advantages%20over%20DP-SGD%20in%20memory%20efficiency%2C%20and%0Aobtains%20higher%20utility%20in%20%24%5Cvarepsilon%24-DP%20when%20using%20the%20Laplace%20mechanism.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.04343v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrivate%2520Fine-tuning%2520of%2520Large%2520Language%2520Models%2520with%2520Zeroth-order%250A%2520%2520Optimization%26entry.906535625%3DXinyu%2520Tang%2520and%2520Ashwinee%2520Panda%2520and%2520Milad%2520Nasr%2520and%2520Saeed%2520Mahloujifar%2520and%2520Prateek%2520Mittal%26entry.1292438233%3D%2520%2520Differentially%2520private%2520stochastic%2520gradient%2520descent%2520%2528DP-SGD%2529%2520allows%2520models%2520to%250Abe%2520trained%2520in%2520a%2520privacy-preserving%2520manner%252C%2520but%2520has%2520proven%2520difficult%2520to%2520scale%2520to%250Athe%2520era%2520of%2520foundation%2520models.%2520We%2520introduce%2520DP-ZO%252C%2520a%2520private%2520fine-tuning%250Aframework%2520for%2520large%2520language%2520models%2520by%2520privatizing%2520zeroth%2520order%2520optimization%250Amethods.%2520A%2520key%2520insight%2520into%2520the%2520design%2520of%2520our%2520method%2520is%2520that%2520the%2520direction%2520of%250Athe%2520gradient%2520in%2520the%2520zeroth-order%2520optimization%2520we%2520use%2520is%2520random%2520and%2520the%2520only%250Ainformation%2520from%2520training%2520data%2520is%2520the%2520step%2520size%252C%2520i.e.%252C%2520a%2520scalar.%2520Therefore%252C%2520we%250Aonly%2520need%2520to%2520privatize%2520the%2520scalar%2520step%2520size%252C%2520which%2520is%2520memory-efficient.%2520DP-ZO%250Aprovides%2520a%2520strong%2520privacy-utility%2520trade-off%2520across%2520different%2520tasks%252C%2520and%2520model%250Asizes%2520that%2520are%2520comparable%2520to%2520DP-SGD%2520in%2520%2524%2528%255Cvarepsilon%252C%255Cdelta%2529%2524-DP.%2520Notably%252C%250ADP-ZO%2520possesses%2520significant%2520advantages%2520over%2520DP-SGD%2520in%2520memory%2520efficiency%252C%2520and%250Aobtains%2520higher%2520utility%2520in%2520%2524%255Cvarepsilon%2524-DP%2520when%2520using%2520the%2520Laplace%2520mechanism.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.04343v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Private%20Fine-tuning%20of%20Large%20Language%20Models%20with%20Zeroth-order%0A%20%20Optimization&entry.906535625=Xinyu%20Tang%20and%20Ashwinee%20Panda%20and%20Milad%20Nasr%20and%20Saeed%20Mahloujifar%20and%20Prateek%20Mittal&entry.1292438233=%20%20Differentially%20private%20stochastic%20gradient%20descent%20%28DP-SGD%29%20allows%20models%20to%0Abe%20trained%20in%20a%20privacy-preserving%20manner%2C%20but%20has%20proven%20difficult%20to%20scale%20to%0Athe%20era%20of%20foundation%20models.%20We%20introduce%20DP-ZO%2C%20a%20private%20fine-tuning%0Aframework%20for%20large%20language%20models%20by%20privatizing%20zeroth%20order%20optimization%0Amethods.%20A%20key%20insight%20into%20the%20design%20of%20our%20method%20is%20that%20the%20direction%20of%0Athe%20gradient%20in%20the%20zeroth-order%20optimization%20we%20use%20is%20random%20and%20the%20only%0Ainformation%20from%20training%20data%20is%20the%20step%20size%2C%20i.e.%2C%20a%20scalar.%20Therefore%2C%20we%0Aonly%20need%20to%20privatize%20the%20scalar%20step%20size%2C%20which%20is%20memory-efficient.%20DP-ZO%0Aprovides%20a%20strong%20privacy-utility%20trade-off%20across%20different%20tasks%2C%20and%20model%0Asizes%20that%20are%20comparable%20to%20DP-SGD%20in%20%24%28%5Cvarepsilon%2C%5Cdelta%29%24-DP.%20Notably%2C%0ADP-ZO%20possesses%20significant%20advantages%20over%20DP-SGD%20in%20memory%20efficiency%2C%20and%0Aobtains%20higher%20utility%20in%20%24%5Cvarepsilon%24-DP%20when%20using%20the%20Laplace%20mechanism.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.04343v2&entry.124074799=Read"},
{"title": "Insights from the Usage of the Ansible Lightspeed Code Completion\n  Service", "author": "Priyam Sahoo and Saurabh Pujar and Ganesh Nalawade and Richard Gebhardt and Louis Mandel and Luca Buratti", "abstract": "  The availability of Large Language Models (LLMs) which can generate code, has\nmade it possible to create tools that improve developer productivity.\nIntegrated development environments or IDEs which developers use to write\nsoftware are often used as an interface to interact with LLMs. Although many\nsuch tools have been released, almost all of them focus on general-purpose\nprogramming languages. Domain-specific languages, such as those crucial for\nInformation Technology (IT) automation, have not received much attention.\nAnsible is one such YAML-based IT automation-specific language. Ansible\nLightspeed is an LLM-based service designed explicitly to generate Ansible YAML\ngiven natural language prompt.\n  This paper first presents the design and implementation of the Ansible\nLightspeed service. We then evaluate its utility to developers using diverse\nindicators, including extended utilization, analysis of user rejected\nsuggestions, as well as analysis of user sentiments. The analysis is based on\ndata collected for 10,696 real users including 3,910 returning users. The code\nfor Ansible Lightspeed service and the analysis framework is made available for\nothers to use.\n  To our knowledge, our study is the first to involve thousands of users in\nevaluating code assistants for domain-specific languages. We propose an\nimproved version of user acceptance rate and we are the first code completion\ntool to present N-Day user retention figures. With our findings we provide\ninsights into the effectiveness of small, dedicated models in a domain-specific\ncontext. We hope this work serves as a reference for software engineering and\nmachine learning researchers exploring code completion services for\ndomain-specific languages in particular and programming languages in general.\n", "link": "http://arxiv.org/abs/2402.17442v2", "date": "2024-08-12", "relevancy": 1.2995, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.452}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4286}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4274}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Insights%20from%20the%20Usage%20of%20the%20Ansible%20Lightspeed%20Code%20Completion%0A%20%20Service&body=Title%3A%20Insights%20from%20the%20Usage%20of%20the%20Ansible%20Lightspeed%20Code%20Completion%0A%20%20Service%0AAuthor%3A%20Priyam%20Sahoo%20and%20Saurabh%20Pujar%20and%20Ganesh%20Nalawade%20and%20Richard%20Gebhardt%20and%20Louis%20Mandel%20and%20Luca%20Buratti%0AAbstract%3A%20%20%20The%20availability%20of%20Large%20Language%20Models%20%28LLMs%29%20which%20can%20generate%20code%2C%20has%0Amade%20it%20possible%20to%20create%20tools%20that%20improve%20developer%20productivity.%0AIntegrated%20development%20environments%20or%20IDEs%20which%20developers%20use%20to%20write%0Asoftware%20are%20often%20used%20as%20an%20interface%20to%20interact%20with%20LLMs.%20Although%20many%0Asuch%20tools%20have%20been%20released%2C%20almost%20all%20of%20them%20focus%20on%20general-purpose%0Aprogramming%20languages.%20Domain-specific%20languages%2C%20such%20as%20those%20crucial%20for%0AInformation%20Technology%20%28IT%29%20automation%2C%20have%20not%20received%20much%20attention.%0AAnsible%20is%20one%20such%20YAML-based%20IT%20automation-specific%20language.%20Ansible%0ALightspeed%20is%20an%20LLM-based%20service%20designed%20explicitly%20to%20generate%20Ansible%20YAML%0Agiven%20natural%20language%20prompt.%0A%20%20This%20paper%20first%20presents%20the%20design%20and%20implementation%20of%20the%20Ansible%0ALightspeed%20service.%20We%20then%20evaluate%20its%20utility%20to%20developers%20using%20diverse%0Aindicators%2C%20including%20extended%20utilization%2C%20analysis%20of%20user%20rejected%0Asuggestions%2C%20as%20well%20as%20analysis%20of%20user%20sentiments.%20The%20analysis%20is%20based%20on%0Adata%20collected%20for%2010%2C696%20real%20users%20including%203%2C910%20returning%20users.%20The%20code%0Afor%20Ansible%20Lightspeed%20service%20and%20the%20analysis%20framework%20is%20made%20available%20for%0Aothers%20to%20use.%0A%20%20To%20our%20knowledge%2C%20our%20study%20is%20the%20first%20to%20involve%20thousands%20of%20users%20in%0Aevaluating%20code%20assistants%20for%20domain-specific%20languages.%20We%20propose%20an%0Aimproved%20version%20of%20user%20acceptance%20rate%20and%20we%20are%20the%20first%20code%20completion%0Atool%20to%20present%20N-Day%20user%20retention%20figures.%20With%20our%20findings%20we%20provide%0Ainsights%20into%20the%20effectiveness%20of%20small%2C%20dedicated%20models%20in%20a%20domain-specific%0Acontext.%20We%20hope%20this%20work%20serves%20as%20a%20reference%20for%20software%20engineering%20and%0Amachine%20learning%20researchers%20exploring%20code%20completion%20services%20for%0Adomain-specific%20languages%20in%20particular%20and%20programming%20languages%20in%20general.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.17442v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInsights%2520from%2520the%2520Usage%2520of%2520the%2520Ansible%2520Lightspeed%2520Code%2520Completion%250A%2520%2520Service%26entry.906535625%3DPriyam%2520Sahoo%2520and%2520Saurabh%2520Pujar%2520and%2520Ganesh%2520Nalawade%2520and%2520Richard%2520Gebhardt%2520and%2520Louis%2520Mandel%2520and%2520Luca%2520Buratti%26entry.1292438233%3D%2520%2520The%2520availability%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520which%2520can%2520generate%2520code%252C%2520has%250Amade%2520it%2520possible%2520to%2520create%2520tools%2520that%2520improve%2520developer%2520productivity.%250AIntegrated%2520development%2520environments%2520or%2520IDEs%2520which%2520developers%2520use%2520to%2520write%250Asoftware%2520are%2520often%2520used%2520as%2520an%2520interface%2520to%2520interact%2520with%2520LLMs.%2520Although%2520many%250Asuch%2520tools%2520have%2520been%2520released%252C%2520almost%2520all%2520of%2520them%2520focus%2520on%2520general-purpose%250Aprogramming%2520languages.%2520Domain-specific%2520languages%252C%2520such%2520as%2520those%2520crucial%2520for%250AInformation%2520Technology%2520%2528IT%2529%2520automation%252C%2520have%2520not%2520received%2520much%2520attention.%250AAnsible%2520is%2520one%2520such%2520YAML-based%2520IT%2520automation-specific%2520language.%2520Ansible%250ALightspeed%2520is%2520an%2520LLM-based%2520service%2520designed%2520explicitly%2520to%2520generate%2520Ansible%2520YAML%250Agiven%2520natural%2520language%2520prompt.%250A%2520%2520This%2520paper%2520first%2520presents%2520the%2520design%2520and%2520implementation%2520of%2520the%2520Ansible%250ALightspeed%2520service.%2520We%2520then%2520evaluate%2520its%2520utility%2520to%2520developers%2520using%2520diverse%250Aindicators%252C%2520including%2520extended%2520utilization%252C%2520analysis%2520of%2520user%2520rejected%250Asuggestions%252C%2520as%2520well%2520as%2520analysis%2520of%2520user%2520sentiments.%2520The%2520analysis%2520is%2520based%2520on%250Adata%2520collected%2520for%252010%252C696%2520real%2520users%2520including%25203%252C910%2520returning%2520users.%2520The%2520code%250Afor%2520Ansible%2520Lightspeed%2520service%2520and%2520the%2520analysis%2520framework%2520is%2520made%2520available%2520for%250Aothers%2520to%2520use.%250A%2520%2520To%2520our%2520knowledge%252C%2520our%2520study%2520is%2520the%2520first%2520to%2520involve%2520thousands%2520of%2520users%2520in%250Aevaluating%2520code%2520assistants%2520for%2520domain-specific%2520languages.%2520We%2520propose%2520an%250Aimproved%2520version%2520of%2520user%2520acceptance%2520rate%2520and%2520we%2520are%2520the%2520first%2520code%2520completion%250Atool%2520to%2520present%2520N-Day%2520user%2520retention%2520figures.%2520With%2520our%2520findings%2520we%2520provide%250Ainsights%2520into%2520the%2520effectiveness%2520of%2520small%252C%2520dedicated%2520models%2520in%2520a%2520domain-specific%250Acontext.%2520We%2520hope%2520this%2520work%2520serves%2520as%2520a%2520reference%2520for%2520software%2520engineering%2520and%250Amachine%2520learning%2520researchers%2520exploring%2520code%2520completion%2520services%2520for%250Adomain-specific%2520languages%2520in%2520particular%2520and%2520programming%2520languages%2520in%2520general.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.17442v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Insights%20from%20the%20Usage%20of%20the%20Ansible%20Lightspeed%20Code%20Completion%0A%20%20Service&entry.906535625=Priyam%20Sahoo%20and%20Saurabh%20Pujar%20and%20Ganesh%20Nalawade%20and%20Richard%20Gebhardt%20and%20Louis%20Mandel%20and%20Luca%20Buratti&entry.1292438233=%20%20The%20availability%20of%20Large%20Language%20Models%20%28LLMs%29%20which%20can%20generate%20code%2C%20has%0Amade%20it%20possible%20to%20create%20tools%20that%20improve%20developer%20productivity.%0AIntegrated%20development%20environments%20or%20IDEs%20which%20developers%20use%20to%20write%0Asoftware%20are%20often%20used%20as%20an%20interface%20to%20interact%20with%20LLMs.%20Although%20many%0Asuch%20tools%20have%20been%20released%2C%20almost%20all%20of%20them%20focus%20on%20general-purpose%0Aprogramming%20languages.%20Domain-specific%20languages%2C%20such%20as%20those%20crucial%20for%0AInformation%20Technology%20%28IT%29%20automation%2C%20have%20not%20received%20much%20attention.%0AAnsible%20is%20one%20such%20YAML-based%20IT%20automation-specific%20language.%20Ansible%0ALightspeed%20is%20an%20LLM-based%20service%20designed%20explicitly%20to%20generate%20Ansible%20YAML%0Agiven%20natural%20language%20prompt.%0A%20%20This%20paper%20first%20presents%20the%20design%20and%20implementation%20of%20the%20Ansible%0ALightspeed%20service.%20We%20then%20evaluate%20its%20utility%20to%20developers%20using%20diverse%0Aindicators%2C%20including%20extended%20utilization%2C%20analysis%20of%20user%20rejected%0Asuggestions%2C%20as%20well%20as%20analysis%20of%20user%20sentiments.%20The%20analysis%20is%20based%20on%0Adata%20collected%20for%2010%2C696%20real%20users%20including%203%2C910%20returning%20users.%20The%20code%0Afor%20Ansible%20Lightspeed%20service%20and%20the%20analysis%20framework%20is%20made%20available%20for%0Aothers%20to%20use.%0A%20%20To%20our%20knowledge%2C%20our%20study%20is%20the%20first%20to%20involve%20thousands%20of%20users%20in%0Aevaluating%20code%20assistants%20for%20domain-specific%20languages.%20We%20propose%20an%0Aimproved%20version%20of%20user%20acceptance%20rate%20and%20we%20are%20the%20first%20code%20completion%0Atool%20to%20present%20N-Day%20user%20retention%20figures.%20With%20our%20findings%20we%20provide%0Ainsights%20into%20the%20effectiveness%20of%20small%2C%20dedicated%20models%20in%20a%20domain-specific%0Acontext.%20We%20hope%20this%20work%20serves%20as%20a%20reference%20for%20software%20engineering%20and%0Amachine%20learning%20researchers%20exploring%20code%20completion%20services%20for%0Adomain-specific%20languages%20in%20particular%20and%20programming%20languages%20in%20general.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17442v2&entry.124074799=Read"},
{"title": "Inverse designing metamaterials with programmable nonlinear functional\n  responses in graph space", "author": "Marco Maurizi and Derek Xu and Yu-Tong Wang and Desheng Yao and David Hahn and Mourad Oudich and Anish Satpati and Mathieu Bauchy and Wei Wang and Yizhou Sun and Yun Jing and Xiaoyu Rayne Zheng", "abstract": "  Material responses to static and dynamic stimuli, represented as nonlinear\ncurves, are design targets for engineering functionalities like structural\nsupport, impact protection, and acoustic and photonic bandgaps.\nThree-dimensional metamaterials offer significant tunability due to their\ninternal structure, yet existing methods struggle to capture their complex\nbehavior-to-structure relationships. We present GraphMetaMat, a graph-based\nframework capable of designing three-dimensional metamaterials with\nprogrammable responses and arbitrary manufacturing constraints. Integrating\ngraph networks, physics biases, reinforcement learning, and tree search,\nGraphMetaMat can target stress-strain curves spanning four orders of magnitude\nand complex behaviors, as well as viscoelastic transmission responses with\nvarying attenuation gaps. GraphMetaMat can create cushioning materials for\nprotective equipment and vibration-damping panels for electric vehicles,\noutperforming commercial materials, and enabling the automatic design of\nmaterials with on-demand functionalities.\n", "link": "http://arxiv.org/abs/2408.06300v1", "date": "2024-08-12", "relevancy": 1.3195, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4609}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4368}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4326}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inverse%20designing%20metamaterials%20with%20programmable%20nonlinear%20functional%0A%20%20responses%20in%20graph%20space&body=Title%3A%20Inverse%20designing%20metamaterials%20with%20programmable%20nonlinear%20functional%0A%20%20responses%20in%20graph%20space%0AAuthor%3A%20Marco%20Maurizi%20and%20Derek%20Xu%20and%20Yu-Tong%20Wang%20and%20Desheng%20Yao%20and%20David%20Hahn%20and%20Mourad%20Oudich%20and%20Anish%20Satpati%20and%20Mathieu%20Bauchy%20and%20Wei%20Wang%20and%20Yizhou%20Sun%20and%20Yun%20Jing%20and%20Xiaoyu%20Rayne%20Zheng%0AAbstract%3A%20%20%20Material%20responses%20to%20static%20and%20dynamic%20stimuli%2C%20represented%20as%20nonlinear%0Acurves%2C%20are%20design%20targets%20for%20engineering%20functionalities%20like%20structural%0Asupport%2C%20impact%20protection%2C%20and%20acoustic%20and%20photonic%20bandgaps.%0AThree-dimensional%20metamaterials%20offer%20significant%20tunability%20due%20to%20their%0Ainternal%20structure%2C%20yet%20existing%20methods%20struggle%20to%20capture%20their%20complex%0Abehavior-to-structure%20relationships.%20We%20present%20GraphMetaMat%2C%20a%20graph-based%0Aframework%20capable%20of%20designing%20three-dimensional%20metamaterials%20with%0Aprogrammable%20responses%20and%20arbitrary%20manufacturing%20constraints.%20Integrating%0Agraph%20networks%2C%20physics%20biases%2C%20reinforcement%20learning%2C%20and%20tree%20search%2C%0AGraphMetaMat%20can%20target%20stress-strain%20curves%20spanning%20four%20orders%20of%20magnitude%0Aand%20complex%20behaviors%2C%20as%20well%20as%20viscoelastic%20transmission%20responses%20with%0Avarying%20attenuation%20gaps.%20GraphMetaMat%20can%20create%20cushioning%20materials%20for%0Aprotective%20equipment%20and%20vibration-damping%20panels%20for%20electric%20vehicles%2C%0Aoutperforming%20commercial%20materials%2C%20and%20enabling%20the%20automatic%20design%20of%0Amaterials%20with%20on-demand%20functionalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06300v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInverse%2520designing%2520metamaterials%2520with%2520programmable%2520nonlinear%2520functional%250A%2520%2520responses%2520in%2520graph%2520space%26entry.906535625%3DMarco%2520Maurizi%2520and%2520Derek%2520Xu%2520and%2520Yu-Tong%2520Wang%2520and%2520Desheng%2520Yao%2520and%2520David%2520Hahn%2520and%2520Mourad%2520Oudich%2520and%2520Anish%2520Satpati%2520and%2520Mathieu%2520Bauchy%2520and%2520Wei%2520Wang%2520and%2520Yizhou%2520Sun%2520and%2520Yun%2520Jing%2520and%2520Xiaoyu%2520Rayne%2520Zheng%26entry.1292438233%3D%2520%2520Material%2520responses%2520to%2520static%2520and%2520dynamic%2520stimuli%252C%2520represented%2520as%2520nonlinear%250Acurves%252C%2520are%2520design%2520targets%2520for%2520engineering%2520functionalities%2520like%2520structural%250Asupport%252C%2520impact%2520protection%252C%2520and%2520acoustic%2520and%2520photonic%2520bandgaps.%250AThree-dimensional%2520metamaterials%2520offer%2520significant%2520tunability%2520due%2520to%2520their%250Ainternal%2520structure%252C%2520yet%2520existing%2520methods%2520struggle%2520to%2520capture%2520their%2520complex%250Abehavior-to-structure%2520relationships.%2520We%2520present%2520GraphMetaMat%252C%2520a%2520graph-based%250Aframework%2520capable%2520of%2520designing%2520three-dimensional%2520metamaterials%2520with%250Aprogrammable%2520responses%2520and%2520arbitrary%2520manufacturing%2520constraints.%2520Integrating%250Agraph%2520networks%252C%2520physics%2520biases%252C%2520reinforcement%2520learning%252C%2520and%2520tree%2520search%252C%250AGraphMetaMat%2520can%2520target%2520stress-strain%2520curves%2520spanning%2520four%2520orders%2520of%2520magnitude%250Aand%2520complex%2520behaviors%252C%2520as%2520well%2520as%2520viscoelastic%2520transmission%2520responses%2520with%250Avarying%2520attenuation%2520gaps.%2520GraphMetaMat%2520can%2520create%2520cushioning%2520materials%2520for%250Aprotective%2520equipment%2520and%2520vibration-damping%2520panels%2520for%2520electric%2520vehicles%252C%250Aoutperforming%2520commercial%2520materials%252C%2520and%2520enabling%2520the%2520automatic%2520design%2520of%250Amaterials%2520with%2520on-demand%2520functionalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06300v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inverse%20designing%20metamaterials%20with%20programmable%20nonlinear%20functional%0A%20%20responses%20in%20graph%20space&entry.906535625=Marco%20Maurizi%20and%20Derek%20Xu%20and%20Yu-Tong%20Wang%20and%20Desheng%20Yao%20and%20David%20Hahn%20and%20Mourad%20Oudich%20and%20Anish%20Satpati%20and%20Mathieu%20Bauchy%20and%20Wei%20Wang%20and%20Yizhou%20Sun%20and%20Yun%20Jing%20and%20Xiaoyu%20Rayne%20Zheng&entry.1292438233=%20%20Material%20responses%20to%20static%20and%20dynamic%20stimuli%2C%20represented%20as%20nonlinear%0Acurves%2C%20are%20design%20targets%20for%20engineering%20functionalities%20like%20structural%0Asupport%2C%20impact%20protection%2C%20and%20acoustic%20and%20photonic%20bandgaps.%0AThree-dimensional%20metamaterials%20offer%20significant%20tunability%20due%20to%20their%0Ainternal%20structure%2C%20yet%20existing%20methods%20struggle%20to%20capture%20their%20complex%0Abehavior-to-structure%20relationships.%20We%20present%20GraphMetaMat%2C%20a%20graph-based%0Aframework%20capable%20of%20designing%20three-dimensional%20metamaterials%20with%0Aprogrammable%20responses%20and%20arbitrary%20manufacturing%20constraints.%20Integrating%0Agraph%20networks%2C%20physics%20biases%2C%20reinforcement%20learning%2C%20and%20tree%20search%2C%0AGraphMetaMat%20can%20target%20stress-strain%20curves%20spanning%20four%20orders%20of%20magnitude%0Aand%20complex%20behaviors%2C%20as%20well%20as%20viscoelastic%20transmission%20responses%20with%0Avarying%20attenuation%20gaps.%20GraphMetaMat%20can%20create%20cushioning%20materials%20for%0Aprotective%20equipment%20and%20vibration-damping%20panels%20for%20electric%20vehicles%2C%0Aoutperforming%20commercial%20materials%2C%20and%20enabling%20the%20automatic%20design%20of%0Amaterials%20with%20on-demand%20functionalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06300v1&entry.124074799=Read"},
{"title": "Five Pitfalls When Assessing Synthetic Medical Images with Reference\n  Metrics", "author": "Melanie Dohmen and Tuan Truong and Ivo M. Baltruschat and Matthias Lenga", "abstract": "  Reference metrics have been developed to objectively and quantitatively\ncompare two images. Especially for evaluating the quality of reconstructed or\ncompressed images, these metrics have shown very useful. Extensive tests of\nsuch metrics on benchmarks of artificially distorted natural images have\nrevealed which metric best correlate with human perception of quality. Direct\ntransfer of these metrics to the evaluation of generative models in medical\nimaging, however, can easily lead to pitfalls, because assumptions about image\ncontent, image data format and image interpretation are often very different.\nAlso, the correlation of reference metrics and human perception of quality can\nvary strongly for different kinds of distortions and commonly used metrics,\nsuch as SSIM, PSNR and MAE are not the best choice for all situations. We\nselected five pitfalls that showcase unexpected and probably undesired\nreference metric scores and discuss strategies to avoid them.\n", "link": "http://arxiv.org/abs/2408.06075v1", "date": "2024-08-12", "relevancy": 1.3903, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4866}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4583}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Five%20Pitfalls%20When%20Assessing%20Synthetic%20Medical%20Images%20with%20Reference%0A%20%20Metrics&body=Title%3A%20Five%20Pitfalls%20When%20Assessing%20Synthetic%20Medical%20Images%20with%20Reference%0A%20%20Metrics%0AAuthor%3A%20Melanie%20Dohmen%20and%20Tuan%20Truong%20and%20Ivo%20M.%20Baltruschat%20and%20Matthias%20Lenga%0AAbstract%3A%20%20%20Reference%20metrics%20have%20been%20developed%20to%20objectively%20and%20quantitatively%0Acompare%20two%20images.%20Especially%20for%20evaluating%20the%20quality%20of%20reconstructed%20or%0Acompressed%20images%2C%20these%20metrics%20have%20shown%20very%20useful.%20Extensive%20tests%20of%0Asuch%20metrics%20on%20benchmarks%20of%20artificially%20distorted%20natural%20images%20have%0Arevealed%20which%20metric%20best%20correlate%20with%20human%20perception%20of%20quality.%20Direct%0Atransfer%20of%20these%20metrics%20to%20the%20evaluation%20of%20generative%20models%20in%20medical%0Aimaging%2C%20however%2C%20can%20easily%20lead%20to%20pitfalls%2C%20because%20assumptions%20about%20image%0Acontent%2C%20image%20data%20format%20and%20image%20interpretation%20are%20often%20very%20different.%0AAlso%2C%20the%20correlation%20of%20reference%20metrics%20and%20human%20perception%20of%20quality%20can%0Avary%20strongly%20for%20different%20kinds%20of%20distortions%20and%20commonly%20used%20metrics%2C%0Asuch%20as%20SSIM%2C%20PSNR%20and%20MAE%20are%20not%20the%20best%20choice%20for%20all%20situations.%20We%0Aselected%20five%20pitfalls%20that%20showcase%20unexpected%20and%20probably%20undesired%0Areference%20metric%20scores%20and%20discuss%20strategies%20to%20avoid%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06075v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFive%2520Pitfalls%2520When%2520Assessing%2520Synthetic%2520Medical%2520Images%2520with%2520Reference%250A%2520%2520Metrics%26entry.906535625%3DMelanie%2520Dohmen%2520and%2520Tuan%2520Truong%2520and%2520Ivo%2520M.%2520Baltruschat%2520and%2520Matthias%2520Lenga%26entry.1292438233%3D%2520%2520Reference%2520metrics%2520have%2520been%2520developed%2520to%2520objectively%2520and%2520quantitatively%250Acompare%2520two%2520images.%2520Especially%2520for%2520evaluating%2520the%2520quality%2520of%2520reconstructed%2520or%250Acompressed%2520images%252C%2520these%2520metrics%2520have%2520shown%2520very%2520useful.%2520Extensive%2520tests%2520of%250Asuch%2520metrics%2520on%2520benchmarks%2520of%2520artificially%2520distorted%2520natural%2520images%2520have%250Arevealed%2520which%2520metric%2520best%2520correlate%2520with%2520human%2520perception%2520of%2520quality.%2520Direct%250Atransfer%2520of%2520these%2520metrics%2520to%2520the%2520evaluation%2520of%2520generative%2520models%2520in%2520medical%250Aimaging%252C%2520however%252C%2520can%2520easily%2520lead%2520to%2520pitfalls%252C%2520because%2520assumptions%2520about%2520image%250Acontent%252C%2520image%2520data%2520format%2520and%2520image%2520interpretation%2520are%2520often%2520very%2520different.%250AAlso%252C%2520the%2520correlation%2520of%2520reference%2520metrics%2520and%2520human%2520perception%2520of%2520quality%2520can%250Avary%2520strongly%2520for%2520different%2520kinds%2520of%2520distortions%2520and%2520commonly%2520used%2520metrics%252C%250Asuch%2520as%2520SSIM%252C%2520PSNR%2520and%2520MAE%2520are%2520not%2520the%2520best%2520choice%2520for%2520all%2520situations.%2520We%250Aselected%2520five%2520pitfalls%2520that%2520showcase%2520unexpected%2520and%2520probably%2520undesired%250Areference%2520metric%2520scores%2520and%2520discuss%2520strategies%2520to%2520avoid%2520them.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06075v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Five%20Pitfalls%20When%20Assessing%20Synthetic%20Medical%20Images%20with%20Reference%0A%20%20Metrics&entry.906535625=Melanie%20Dohmen%20and%20Tuan%20Truong%20and%20Ivo%20M.%20Baltruschat%20and%20Matthias%20Lenga&entry.1292438233=%20%20Reference%20metrics%20have%20been%20developed%20to%20objectively%20and%20quantitatively%0Acompare%20two%20images.%20Especially%20for%20evaluating%20the%20quality%20of%20reconstructed%20or%0Acompressed%20images%2C%20these%20metrics%20have%20shown%20very%20useful.%20Extensive%20tests%20of%0Asuch%20metrics%20on%20benchmarks%20of%20artificially%20distorted%20natural%20images%20have%0Arevealed%20which%20metric%20best%20correlate%20with%20human%20perception%20of%20quality.%20Direct%0Atransfer%20of%20these%20metrics%20to%20the%20evaluation%20of%20generative%20models%20in%20medical%0Aimaging%2C%20however%2C%20can%20easily%20lead%20to%20pitfalls%2C%20because%20assumptions%20about%20image%0Acontent%2C%20image%20data%20format%20and%20image%20interpretation%20are%20often%20very%20different.%0AAlso%2C%20the%20correlation%20of%20reference%20metrics%20and%20human%20perception%20of%20quality%20can%0Avary%20strongly%20for%20different%20kinds%20of%20distortions%20and%20commonly%20used%20metrics%2C%0Asuch%20as%20SSIM%2C%20PSNR%20and%20MAE%20are%20not%20the%20best%20choice%20for%20all%20situations.%20We%0Aselected%20five%20pitfalls%20that%20showcase%20unexpected%20and%20probably%20undesired%0Areference%20metric%20scores%20and%20discuss%20strategies%20to%20avoid%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06075v1&entry.124074799=Read"},
{"title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific\n  Discovery", "author": "Chris Lu and Cong Lu and Robert Tjarko Lange and Jakob Foerster and Jeff Clune and David Ha", "abstract": "  One of the grand challenges of artificial general intelligence is developing\nagents capable of conducting scientific research and discovering new knowledge.\nWhile frontier models have already been used as aids to human scientists, e.g.\nfor brainstorming ideas, writing code, or prediction tasks, they still conduct\nonly a small part of the scientific process. This paper presents the first\ncomprehensive framework for fully automatic scientific discovery, enabling\nfrontier large language models to perform research independently and\ncommunicate their findings. We introduce The AI Scientist, which generates\nnovel research ideas, writes code, executes experiments, visualizes results,\ndescribes its findings by writing a full scientific paper, and then runs a\nsimulated review process for evaluation. In principle, this process can be\nrepeated to iteratively develop ideas in an open-ended fashion, acting like the\nhuman scientific community. We demonstrate its versatility by applying it to\nthree distinct subfields of machine learning: diffusion modeling,\ntransformer-based language modeling, and learning dynamics. Each idea is\nimplemented and developed into a full paper at a cost of less than $15 per\npaper. To evaluate the generated papers, we design and validate an automated\nreviewer, which we show achieves near-human performance in evaluating paper\nscores. The AI Scientist can produce papers that exceed the acceptance\nthreshold at a top machine learning conference as judged by our automated\nreviewer. This approach signifies the beginning of a new era in scientific\ndiscovery in machine learning: bringing the transformative benefits of AI\nagents to the entire research process of AI itself, and taking us closer to a\nworld where endless affordable creativity and innovation can be unleashed on\nthe world's most challenging problems. Our code is open-sourced at\nhttps://github.com/SakanaAI/AI-Scientist\n", "link": "http://arxiv.org/abs/2408.06292v1", "date": "2024-08-12", "relevancy": 1.6293, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5586}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5554}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4969}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20AI%20Scientist%3A%20Towards%20Fully%20Automated%20Open-Ended%20Scientific%0A%20%20Discovery&body=Title%3A%20The%20AI%20Scientist%3A%20Towards%20Fully%20Automated%20Open-Ended%20Scientific%0A%20%20Discovery%0AAuthor%3A%20Chris%20Lu%20and%20Cong%20Lu%20and%20Robert%20Tjarko%20Lange%20and%20Jakob%20Foerster%20and%20Jeff%20Clune%20and%20David%20Ha%0AAbstract%3A%20%20%20One%20of%20the%20grand%20challenges%20of%20artificial%20general%20intelligence%20is%20developing%0Aagents%20capable%20of%20conducting%20scientific%20research%20and%20discovering%20new%20knowledge.%0AWhile%20frontier%20models%20have%20already%20been%20used%20as%20aids%20to%20human%20scientists%2C%20e.g.%0Afor%20brainstorming%20ideas%2C%20writing%20code%2C%20or%20prediction%20tasks%2C%20they%20still%20conduct%0Aonly%20a%20small%20part%20of%20the%20scientific%20process.%20This%20paper%20presents%20the%20first%0Acomprehensive%20framework%20for%20fully%20automatic%20scientific%20discovery%2C%20enabling%0Afrontier%20large%20language%20models%20to%20perform%20research%20independently%20and%0Acommunicate%20their%20findings.%20We%20introduce%20The%20AI%20Scientist%2C%20which%20generates%0Anovel%20research%20ideas%2C%20writes%20code%2C%20executes%20experiments%2C%20visualizes%20results%2C%0Adescribes%20its%20findings%20by%20writing%20a%20full%20scientific%20paper%2C%20and%20then%20runs%20a%0Asimulated%20review%20process%20for%20evaluation.%20In%20principle%2C%20this%20process%20can%20be%0Arepeated%20to%20iteratively%20develop%20ideas%20in%20an%20open-ended%20fashion%2C%20acting%20like%20the%0Ahuman%20scientific%20community.%20We%20demonstrate%20its%20versatility%20by%20applying%20it%20to%0Athree%20distinct%20subfields%20of%20machine%20learning%3A%20diffusion%20modeling%2C%0Atransformer-based%20language%20modeling%2C%20and%20learning%20dynamics.%20Each%20idea%20is%0Aimplemented%20and%20developed%20into%20a%20full%20paper%20at%20a%20cost%20of%20less%20than%20%2415%20per%0Apaper.%20To%20evaluate%20the%20generated%20papers%2C%20we%20design%20and%20validate%20an%20automated%0Areviewer%2C%20which%20we%20show%20achieves%20near-human%20performance%20in%20evaluating%20paper%0Ascores.%20The%20AI%20Scientist%20can%20produce%20papers%20that%20exceed%20the%20acceptance%0Athreshold%20at%20a%20top%20machine%20learning%20conference%20as%20judged%20by%20our%20automated%0Areviewer.%20This%20approach%20signifies%20the%20beginning%20of%20a%20new%20era%20in%20scientific%0Adiscovery%20in%20machine%20learning%3A%20bringing%20the%20transformative%20benefits%20of%20AI%0Aagents%20to%20the%20entire%20research%20process%20of%20AI%20itself%2C%20and%20taking%20us%20closer%20to%20a%0Aworld%20where%20endless%20affordable%20creativity%20and%20innovation%20can%20be%20unleashed%20on%0Athe%20world%27s%20most%20challenging%20problems.%20Our%20code%20is%20open-sourced%20at%0Ahttps%3A//github.com/SakanaAI/AI-Scientist%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06292v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520AI%2520Scientist%253A%2520Towards%2520Fully%2520Automated%2520Open-Ended%2520Scientific%250A%2520%2520Discovery%26entry.906535625%3DChris%2520Lu%2520and%2520Cong%2520Lu%2520and%2520Robert%2520Tjarko%2520Lange%2520and%2520Jakob%2520Foerster%2520and%2520Jeff%2520Clune%2520and%2520David%2520Ha%26entry.1292438233%3D%2520%2520One%2520of%2520the%2520grand%2520challenges%2520of%2520artificial%2520general%2520intelligence%2520is%2520developing%250Aagents%2520capable%2520of%2520conducting%2520scientific%2520research%2520and%2520discovering%2520new%2520knowledge.%250AWhile%2520frontier%2520models%2520have%2520already%2520been%2520used%2520as%2520aids%2520to%2520human%2520scientists%252C%2520e.g.%250Afor%2520brainstorming%2520ideas%252C%2520writing%2520code%252C%2520or%2520prediction%2520tasks%252C%2520they%2520still%2520conduct%250Aonly%2520a%2520small%2520part%2520of%2520the%2520scientific%2520process.%2520This%2520paper%2520presents%2520the%2520first%250Acomprehensive%2520framework%2520for%2520fully%2520automatic%2520scientific%2520discovery%252C%2520enabling%250Afrontier%2520large%2520language%2520models%2520to%2520perform%2520research%2520independently%2520and%250Acommunicate%2520their%2520findings.%2520We%2520introduce%2520The%2520AI%2520Scientist%252C%2520which%2520generates%250Anovel%2520research%2520ideas%252C%2520writes%2520code%252C%2520executes%2520experiments%252C%2520visualizes%2520results%252C%250Adescribes%2520its%2520findings%2520by%2520writing%2520a%2520full%2520scientific%2520paper%252C%2520and%2520then%2520runs%2520a%250Asimulated%2520review%2520process%2520for%2520evaluation.%2520In%2520principle%252C%2520this%2520process%2520can%2520be%250Arepeated%2520to%2520iteratively%2520develop%2520ideas%2520in%2520an%2520open-ended%2520fashion%252C%2520acting%2520like%2520the%250Ahuman%2520scientific%2520community.%2520We%2520demonstrate%2520its%2520versatility%2520by%2520applying%2520it%2520to%250Athree%2520distinct%2520subfields%2520of%2520machine%2520learning%253A%2520diffusion%2520modeling%252C%250Atransformer-based%2520language%2520modeling%252C%2520and%2520learning%2520dynamics.%2520Each%2520idea%2520is%250Aimplemented%2520and%2520developed%2520into%2520a%2520full%2520paper%2520at%2520a%2520cost%2520of%2520less%2520than%2520%252415%2520per%250Apaper.%2520To%2520evaluate%2520the%2520generated%2520papers%252C%2520we%2520design%2520and%2520validate%2520an%2520automated%250Areviewer%252C%2520which%2520we%2520show%2520achieves%2520near-human%2520performance%2520in%2520evaluating%2520paper%250Ascores.%2520The%2520AI%2520Scientist%2520can%2520produce%2520papers%2520that%2520exceed%2520the%2520acceptance%250Athreshold%2520at%2520a%2520top%2520machine%2520learning%2520conference%2520as%2520judged%2520by%2520our%2520automated%250Areviewer.%2520This%2520approach%2520signifies%2520the%2520beginning%2520of%2520a%2520new%2520era%2520in%2520scientific%250Adiscovery%2520in%2520machine%2520learning%253A%2520bringing%2520the%2520transformative%2520benefits%2520of%2520AI%250Aagents%2520to%2520the%2520entire%2520research%2520process%2520of%2520AI%2520itself%252C%2520and%2520taking%2520us%2520closer%2520to%2520a%250Aworld%2520where%2520endless%2520affordable%2520creativity%2520and%2520innovation%2520can%2520be%2520unleashed%2520on%250Athe%2520world%2527s%2520most%2520challenging%2520problems.%2520Our%2520code%2520is%2520open-sourced%2520at%250Ahttps%253A//github.com/SakanaAI/AI-Scientist%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06292v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20AI%20Scientist%3A%20Towards%20Fully%20Automated%20Open-Ended%20Scientific%0A%20%20Discovery&entry.906535625=Chris%20Lu%20and%20Cong%20Lu%20and%20Robert%20Tjarko%20Lange%20and%20Jakob%20Foerster%20and%20Jeff%20Clune%20and%20David%20Ha&entry.1292438233=%20%20One%20of%20the%20grand%20challenges%20of%20artificial%20general%20intelligence%20is%20developing%0Aagents%20capable%20of%20conducting%20scientific%20research%20and%20discovering%20new%20knowledge.%0AWhile%20frontier%20models%20have%20already%20been%20used%20as%20aids%20to%20human%20scientists%2C%20e.g.%0Afor%20brainstorming%20ideas%2C%20writing%20code%2C%20or%20prediction%20tasks%2C%20they%20still%20conduct%0Aonly%20a%20small%20part%20of%20the%20scientific%20process.%20This%20paper%20presents%20the%20first%0Acomprehensive%20framework%20for%20fully%20automatic%20scientific%20discovery%2C%20enabling%0Afrontier%20large%20language%20models%20to%20perform%20research%20independently%20and%0Acommunicate%20their%20findings.%20We%20introduce%20The%20AI%20Scientist%2C%20which%20generates%0Anovel%20research%20ideas%2C%20writes%20code%2C%20executes%20experiments%2C%20visualizes%20results%2C%0Adescribes%20its%20findings%20by%20writing%20a%20full%20scientific%20paper%2C%20and%20then%20runs%20a%0Asimulated%20review%20process%20for%20evaluation.%20In%20principle%2C%20this%20process%20can%20be%0Arepeated%20to%20iteratively%20develop%20ideas%20in%20an%20open-ended%20fashion%2C%20acting%20like%20the%0Ahuman%20scientific%20community.%20We%20demonstrate%20its%20versatility%20by%20applying%20it%20to%0Athree%20distinct%20subfields%20of%20machine%20learning%3A%20diffusion%20modeling%2C%0Atransformer-based%20language%20modeling%2C%20and%20learning%20dynamics.%20Each%20idea%20is%0Aimplemented%20and%20developed%20into%20a%20full%20paper%20at%20a%20cost%20of%20less%20than%20%2415%20per%0Apaper.%20To%20evaluate%20the%20generated%20papers%2C%20we%20design%20and%20validate%20an%20automated%0Areviewer%2C%20which%20we%20show%20achieves%20near-human%20performance%20in%20evaluating%20paper%0Ascores.%20The%20AI%20Scientist%20can%20produce%20papers%20that%20exceed%20the%20acceptance%0Athreshold%20at%20a%20top%20machine%20learning%20conference%20as%20judged%20by%20our%20automated%0Areviewer.%20This%20approach%20signifies%20the%20beginning%20of%20a%20new%20era%20in%20scientific%0Adiscovery%20in%20machine%20learning%3A%20bringing%20the%20transformative%20benefits%20of%20AI%0Aagents%20to%20the%20entire%20research%20process%20of%20AI%20itself%2C%20and%20taking%20us%20closer%20to%20a%0Aworld%20where%20endless%20affordable%20creativity%20and%20innovation%20can%20be%20unleashed%20on%0Athe%20world%27s%20most%20challenging%20problems.%20Our%20code%20is%20open-sourced%20at%0Ahttps%3A//github.com/SakanaAI/AI-Scientist%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06292v1&entry.124074799=Read"},
{"title": "A Large-Scale Study of Model Integration in ML-Enabled Software Systems", "author": "Yorick Sens and Henriette Knopp and Sven Peldszus and Thorsten Berger", "abstract": "  The rise of machine learning (ML) and its embedding in systems has\ndrastically changed the engineering of software-intensive systems.\nTraditionally, software engineering focuses on manually created artifacts such\nas source code and the process of creating them, as well as best practices for\nintegrating them, i.e., software architectures. In contrast, the development of\nML artifacts, i.e. ML models, comes from data science and focuses on the ML\nmodels and their training data. However, to deliver value to end users, these\nML models must be embedded in traditional software, often forming complex\ntopologies. In fact, ML-enabled software can easily incorporate many different\nML models. While the challenges and practices of building ML-enabled systems\nhave been studied to some extent, beyond isolated examples, little is known\nabout the characteristics of real-world ML-enabled systems. Properly embedding\nML models in systems so that they can be easily maintained or reused is far\nfrom trivial. We need to improve our empirical understanding of such systems,\nwhich we address by presenting the first large-scale study of real ML-enabled\nsoftware systems, covering over 2,928 open source systems on GitHub. We\nclassified and analyzed them to determine their characteristics, as well as\ntheir practices for reusing ML models and related code, and the architecture of\nthese systems. Our findings provide practitioners and researchers with insight\ninto practices for embedding and integrating ML models, bringing data science\nand software engineering closer together.\n", "link": "http://arxiv.org/abs/2408.06226v1", "date": "2024-08-12", "relevancy": 1.2958, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4501}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.427}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Large-Scale%20Study%20of%20Model%20Integration%20in%20ML-Enabled%20Software%20Systems&body=Title%3A%20A%20Large-Scale%20Study%20of%20Model%20Integration%20in%20ML-Enabled%20Software%20Systems%0AAuthor%3A%20Yorick%20Sens%20and%20Henriette%20Knopp%20and%20Sven%20Peldszus%20and%20Thorsten%20Berger%0AAbstract%3A%20%20%20The%20rise%20of%20machine%20learning%20%28ML%29%20and%20its%20embedding%20in%20systems%20has%0Adrastically%20changed%20the%20engineering%20of%20software-intensive%20systems.%0ATraditionally%2C%20software%20engineering%20focuses%20on%20manually%20created%20artifacts%20such%0Aas%20source%20code%20and%20the%20process%20of%20creating%20them%2C%20as%20well%20as%20best%20practices%20for%0Aintegrating%20them%2C%20i.e.%2C%20software%20architectures.%20In%20contrast%2C%20the%20development%20of%0AML%20artifacts%2C%20i.e.%20ML%20models%2C%20comes%20from%20data%20science%20and%20focuses%20on%20the%20ML%0Amodels%20and%20their%20training%20data.%20However%2C%20to%20deliver%20value%20to%20end%20users%2C%20these%0AML%20models%20must%20be%20embedded%20in%20traditional%20software%2C%20often%20forming%20complex%0Atopologies.%20In%20fact%2C%20ML-enabled%20software%20can%20easily%20incorporate%20many%20different%0AML%20models.%20While%20the%20challenges%20and%20practices%20of%20building%20ML-enabled%20systems%0Ahave%20been%20studied%20to%20some%20extent%2C%20beyond%20isolated%20examples%2C%20little%20is%20known%0Aabout%20the%20characteristics%20of%20real-world%20ML-enabled%20systems.%20Properly%20embedding%0AML%20models%20in%20systems%20so%20that%20they%20can%20be%20easily%20maintained%20or%20reused%20is%20far%0Afrom%20trivial.%20We%20need%20to%20improve%20our%20empirical%20understanding%20of%20such%20systems%2C%0Awhich%20we%20address%20by%20presenting%20the%20first%20large-scale%20study%20of%20real%20ML-enabled%0Asoftware%20systems%2C%20covering%20over%202%2C928%20open%20source%20systems%20on%20GitHub.%20We%0Aclassified%20and%20analyzed%20them%20to%20determine%20their%20characteristics%2C%20as%20well%20as%0Atheir%20practices%20for%20reusing%20ML%20models%20and%20related%20code%2C%20and%20the%20architecture%20of%0Athese%20systems.%20Our%20findings%20provide%20practitioners%20and%20researchers%20with%20insight%0Ainto%20practices%20for%20embedding%20and%20integrating%20ML%20models%2C%20bringing%20data%20science%0Aand%20software%20engineering%20closer%20together.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06226v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Large-Scale%2520Study%2520of%2520Model%2520Integration%2520in%2520ML-Enabled%2520Software%2520Systems%26entry.906535625%3DYorick%2520Sens%2520and%2520Henriette%2520Knopp%2520and%2520Sven%2520Peldszus%2520and%2520Thorsten%2520Berger%26entry.1292438233%3D%2520%2520The%2520rise%2520of%2520machine%2520learning%2520%2528ML%2529%2520and%2520its%2520embedding%2520in%2520systems%2520has%250Adrastically%2520changed%2520the%2520engineering%2520of%2520software-intensive%2520systems.%250ATraditionally%252C%2520software%2520engineering%2520focuses%2520on%2520manually%2520created%2520artifacts%2520such%250Aas%2520source%2520code%2520and%2520the%2520process%2520of%2520creating%2520them%252C%2520as%2520well%2520as%2520best%2520practices%2520for%250Aintegrating%2520them%252C%2520i.e.%252C%2520software%2520architectures.%2520In%2520contrast%252C%2520the%2520development%2520of%250AML%2520artifacts%252C%2520i.e.%2520ML%2520models%252C%2520comes%2520from%2520data%2520science%2520and%2520focuses%2520on%2520the%2520ML%250Amodels%2520and%2520their%2520training%2520data.%2520However%252C%2520to%2520deliver%2520value%2520to%2520end%2520users%252C%2520these%250AML%2520models%2520must%2520be%2520embedded%2520in%2520traditional%2520software%252C%2520often%2520forming%2520complex%250Atopologies.%2520In%2520fact%252C%2520ML-enabled%2520software%2520can%2520easily%2520incorporate%2520many%2520different%250AML%2520models.%2520While%2520the%2520challenges%2520and%2520practices%2520of%2520building%2520ML-enabled%2520systems%250Ahave%2520been%2520studied%2520to%2520some%2520extent%252C%2520beyond%2520isolated%2520examples%252C%2520little%2520is%2520known%250Aabout%2520the%2520characteristics%2520of%2520real-world%2520ML-enabled%2520systems.%2520Properly%2520embedding%250AML%2520models%2520in%2520systems%2520so%2520that%2520they%2520can%2520be%2520easily%2520maintained%2520or%2520reused%2520is%2520far%250Afrom%2520trivial.%2520We%2520need%2520to%2520improve%2520our%2520empirical%2520understanding%2520of%2520such%2520systems%252C%250Awhich%2520we%2520address%2520by%2520presenting%2520the%2520first%2520large-scale%2520study%2520of%2520real%2520ML-enabled%250Asoftware%2520systems%252C%2520covering%2520over%25202%252C928%2520open%2520source%2520systems%2520on%2520GitHub.%2520We%250Aclassified%2520and%2520analyzed%2520them%2520to%2520determine%2520their%2520characteristics%252C%2520as%2520well%2520as%250Atheir%2520practices%2520for%2520reusing%2520ML%2520models%2520and%2520related%2520code%252C%2520and%2520the%2520architecture%2520of%250Athese%2520systems.%2520Our%2520findings%2520provide%2520practitioners%2520and%2520researchers%2520with%2520insight%250Ainto%2520practices%2520for%2520embedding%2520and%2520integrating%2520ML%2520models%252C%2520bringing%2520data%2520science%250Aand%2520software%2520engineering%2520closer%2520together.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06226v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Large-Scale%20Study%20of%20Model%20Integration%20in%20ML-Enabled%20Software%20Systems&entry.906535625=Yorick%20Sens%20and%20Henriette%20Knopp%20and%20Sven%20Peldszus%20and%20Thorsten%20Berger&entry.1292438233=%20%20The%20rise%20of%20machine%20learning%20%28ML%29%20and%20its%20embedding%20in%20systems%20has%0Adrastically%20changed%20the%20engineering%20of%20software-intensive%20systems.%0ATraditionally%2C%20software%20engineering%20focuses%20on%20manually%20created%20artifacts%20such%0Aas%20source%20code%20and%20the%20process%20of%20creating%20them%2C%20as%20well%20as%20best%20practices%20for%0Aintegrating%20them%2C%20i.e.%2C%20software%20architectures.%20In%20contrast%2C%20the%20development%20of%0AML%20artifacts%2C%20i.e.%20ML%20models%2C%20comes%20from%20data%20science%20and%20focuses%20on%20the%20ML%0Amodels%20and%20their%20training%20data.%20However%2C%20to%20deliver%20value%20to%20end%20users%2C%20these%0AML%20models%20must%20be%20embedded%20in%20traditional%20software%2C%20often%20forming%20complex%0Atopologies.%20In%20fact%2C%20ML-enabled%20software%20can%20easily%20incorporate%20many%20different%0AML%20models.%20While%20the%20challenges%20and%20practices%20of%20building%20ML-enabled%20systems%0Ahave%20been%20studied%20to%20some%20extent%2C%20beyond%20isolated%20examples%2C%20little%20is%20known%0Aabout%20the%20characteristics%20of%20real-world%20ML-enabled%20systems.%20Properly%20embedding%0AML%20models%20in%20systems%20so%20that%20they%20can%20be%20easily%20maintained%20or%20reused%20is%20far%0Afrom%20trivial.%20We%20need%20to%20improve%20our%20empirical%20understanding%20of%20such%20systems%2C%0Awhich%20we%20address%20by%20presenting%20the%20first%20large-scale%20study%20of%20real%20ML-enabled%0Asoftware%20systems%2C%20covering%20over%202%2C928%20open%20source%20systems%20on%20GitHub.%20We%0Aclassified%20and%20analyzed%20them%20to%20determine%20their%20characteristics%2C%20as%20well%20as%0Atheir%20practices%20for%20reusing%20ML%20models%20and%20related%20code%2C%20and%20the%20architecture%20of%0Athese%20systems.%20Our%20findings%20provide%20practitioners%20and%20researchers%20with%20insight%0Ainto%20practices%20for%20embedding%20and%20integrating%20ML%20models%2C%20bringing%20data%20science%0Aand%20software%20engineering%20closer%20together.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06226v1&entry.124074799=Read"},
{"title": "A Digital Twin Framework Utilizing Machine Learning for Robust\n  Predictive Maintenance: Enhancing Tire Health Monitoring", "author": "Vispi Karkaria and Jie Chen and Christopher Luey and Chase Siuta and Damien Lim and Robert Radulescu and Wei Chen", "abstract": "  We introduce a novel digital twin framework for predictive maintenance of\nlong-term physical systems. Using monitoring tire health as an application, we\nshow how the digital twin framework can be used to enhance automotive safety\nand efficiency, and how the technical challenges can be overcome using a\nthree-step approach. Firstly, for managing the data complexity over a long\noperation span, we employ data reduction techniques to concisely represent\nphysical tires using historical performance and usage data. Relying on these\ndata, for fast real-time prediction, we train a transformer-based model offline\non our concise dataset to predict future tire health over time, represented as\nRemaining Casing Potential (RCP). Based on our architecture, our model\nquantifies both epistemic and aleatoric uncertainty, providing reliable\nconfidence intervals around predicted RCP. Secondly, to incorporate real-time\ndata, we update the predictive model in the digital twin framework, ensuring\nits accuracy throughout its life span with the aid of hybrid modeling and the\nuse of discrepancy function. Thirdly, to assist decision making in predictive\nmaintenance, we implement a Tire State Decision Algorithm, which strategically\ndetermines the optimal timing for tire replacement based on RCP forecasted by\nour transformer model. This approach ensures our digital twin accurately\npredicts system health, continually refines its digital representation, and\nsupports predictive maintenance decisions. Our framework effectively embodies a\nphysical system, leveraging big data and machine learning for predictive\nmaintenance, model updates, and decision-making.\n", "link": "http://arxiv.org/abs/2408.06220v1", "date": "2024-08-12", "relevancy": 0.9274, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5155}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4422}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4334}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Digital%20Twin%20Framework%20Utilizing%20Machine%20Learning%20for%20Robust%0A%20%20Predictive%20Maintenance%3A%20Enhancing%20Tire%20Health%20Monitoring&body=Title%3A%20A%20Digital%20Twin%20Framework%20Utilizing%20Machine%20Learning%20for%20Robust%0A%20%20Predictive%20Maintenance%3A%20Enhancing%20Tire%20Health%20Monitoring%0AAuthor%3A%20Vispi%20Karkaria%20and%20Jie%20Chen%20and%20Christopher%20Luey%20and%20Chase%20Siuta%20and%20Damien%20Lim%20and%20Robert%20Radulescu%20and%20Wei%20Chen%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20digital%20twin%20framework%20for%20predictive%20maintenance%20of%0Along-term%20physical%20systems.%20Using%20monitoring%20tire%20health%20as%20an%20application%2C%20we%0Ashow%20how%20the%20digital%20twin%20framework%20can%20be%20used%20to%20enhance%20automotive%20safety%0Aand%20efficiency%2C%20and%20how%20the%20technical%20challenges%20can%20be%20overcome%20using%20a%0Athree-step%20approach.%20Firstly%2C%20for%20managing%20the%20data%20complexity%20over%20a%20long%0Aoperation%20span%2C%20we%20employ%20data%20reduction%20techniques%20to%20concisely%20represent%0Aphysical%20tires%20using%20historical%20performance%20and%20usage%20data.%20Relying%20on%20these%0Adata%2C%20for%20fast%20real-time%20prediction%2C%20we%20train%20a%20transformer-based%20model%20offline%0Aon%20our%20concise%20dataset%20to%20predict%20future%20tire%20health%20over%20time%2C%20represented%20as%0ARemaining%20Casing%20Potential%20%28RCP%29.%20Based%20on%20our%20architecture%2C%20our%20model%0Aquantifies%20both%20epistemic%20and%20aleatoric%20uncertainty%2C%20providing%20reliable%0Aconfidence%20intervals%20around%20predicted%20RCP.%20Secondly%2C%20to%20incorporate%20real-time%0Adata%2C%20we%20update%20the%20predictive%20model%20in%20the%20digital%20twin%20framework%2C%20ensuring%0Aits%20accuracy%20throughout%20its%20life%20span%20with%20the%20aid%20of%20hybrid%20modeling%20and%20the%0Ause%20of%20discrepancy%20function.%20Thirdly%2C%20to%20assist%20decision%20making%20in%20predictive%0Amaintenance%2C%20we%20implement%20a%20Tire%20State%20Decision%20Algorithm%2C%20which%20strategically%0Adetermines%20the%20optimal%20timing%20for%20tire%20replacement%20based%20on%20RCP%20forecasted%20by%0Aour%20transformer%20model.%20This%20approach%20ensures%20our%20digital%20twin%20accurately%0Apredicts%20system%20health%2C%20continually%20refines%20its%20digital%20representation%2C%20and%0Asupports%20predictive%20maintenance%20decisions.%20Our%20framework%20effectively%20embodies%20a%0Aphysical%20system%2C%20leveraging%20big%20data%20and%20machine%20learning%20for%20predictive%0Amaintenance%2C%20model%20updates%2C%20and%20decision-making.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06220v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Digital%2520Twin%2520Framework%2520Utilizing%2520Machine%2520Learning%2520for%2520Robust%250A%2520%2520Predictive%2520Maintenance%253A%2520Enhancing%2520Tire%2520Health%2520Monitoring%26entry.906535625%3DVispi%2520Karkaria%2520and%2520Jie%2520Chen%2520and%2520Christopher%2520Luey%2520and%2520Chase%2520Siuta%2520and%2520Damien%2520Lim%2520and%2520Robert%2520Radulescu%2520and%2520Wei%2520Chen%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520digital%2520twin%2520framework%2520for%2520predictive%2520maintenance%2520of%250Along-term%2520physical%2520systems.%2520Using%2520monitoring%2520tire%2520health%2520as%2520an%2520application%252C%2520we%250Ashow%2520how%2520the%2520digital%2520twin%2520framework%2520can%2520be%2520used%2520to%2520enhance%2520automotive%2520safety%250Aand%2520efficiency%252C%2520and%2520how%2520the%2520technical%2520challenges%2520can%2520be%2520overcome%2520using%2520a%250Athree-step%2520approach.%2520Firstly%252C%2520for%2520managing%2520the%2520data%2520complexity%2520over%2520a%2520long%250Aoperation%2520span%252C%2520we%2520employ%2520data%2520reduction%2520techniques%2520to%2520concisely%2520represent%250Aphysical%2520tires%2520using%2520historical%2520performance%2520and%2520usage%2520data.%2520Relying%2520on%2520these%250Adata%252C%2520for%2520fast%2520real-time%2520prediction%252C%2520we%2520train%2520a%2520transformer-based%2520model%2520offline%250Aon%2520our%2520concise%2520dataset%2520to%2520predict%2520future%2520tire%2520health%2520over%2520time%252C%2520represented%2520as%250ARemaining%2520Casing%2520Potential%2520%2528RCP%2529.%2520Based%2520on%2520our%2520architecture%252C%2520our%2520model%250Aquantifies%2520both%2520epistemic%2520and%2520aleatoric%2520uncertainty%252C%2520providing%2520reliable%250Aconfidence%2520intervals%2520around%2520predicted%2520RCP.%2520Secondly%252C%2520to%2520incorporate%2520real-time%250Adata%252C%2520we%2520update%2520the%2520predictive%2520model%2520in%2520the%2520digital%2520twin%2520framework%252C%2520ensuring%250Aits%2520accuracy%2520throughout%2520its%2520life%2520span%2520with%2520the%2520aid%2520of%2520hybrid%2520modeling%2520and%2520the%250Ause%2520of%2520discrepancy%2520function.%2520Thirdly%252C%2520to%2520assist%2520decision%2520making%2520in%2520predictive%250Amaintenance%252C%2520we%2520implement%2520a%2520Tire%2520State%2520Decision%2520Algorithm%252C%2520which%2520strategically%250Adetermines%2520the%2520optimal%2520timing%2520for%2520tire%2520replacement%2520based%2520on%2520RCP%2520forecasted%2520by%250Aour%2520transformer%2520model.%2520This%2520approach%2520ensures%2520our%2520digital%2520twin%2520accurately%250Apredicts%2520system%2520health%252C%2520continually%2520refines%2520its%2520digital%2520representation%252C%2520and%250Asupports%2520predictive%2520maintenance%2520decisions.%2520Our%2520framework%2520effectively%2520embodies%2520a%250Aphysical%2520system%252C%2520leveraging%2520big%2520data%2520and%2520machine%2520learning%2520for%2520predictive%250Amaintenance%252C%2520model%2520updates%252C%2520and%2520decision-making.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06220v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Digital%20Twin%20Framework%20Utilizing%20Machine%20Learning%20for%20Robust%0A%20%20Predictive%20Maintenance%3A%20Enhancing%20Tire%20Health%20Monitoring&entry.906535625=Vispi%20Karkaria%20and%20Jie%20Chen%20and%20Christopher%20Luey%20and%20Chase%20Siuta%20and%20Damien%20Lim%20and%20Robert%20Radulescu%20and%20Wei%20Chen&entry.1292438233=%20%20We%20introduce%20a%20novel%20digital%20twin%20framework%20for%20predictive%20maintenance%20of%0Along-term%20physical%20systems.%20Using%20monitoring%20tire%20health%20as%20an%20application%2C%20we%0Ashow%20how%20the%20digital%20twin%20framework%20can%20be%20used%20to%20enhance%20automotive%20safety%0Aand%20efficiency%2C%20and%20how%20the%20technical%20challenges%20can%20be%20overcome%20using%20a%0Athree-step%20approach.%20Firstly%2C%20for%20managing%20the%20data%20complexity%20over%20a%20long%0Aoperation%20span%2C%20we%20employ%20data%20reduction%20techniques%20to%20concisely%20represent%0Aphysical%20tires%20using%20historical%20performance%20and%20usage%20data.%20Relying%20on%20these%0Adata%2C%20for%20fast%20real-time%20prediction%2C%20we%20train%20a%20transformer-based%20model%20offline%0Aon%20our%20concise%20dataset%20to%20predict%20future%20tire%20health%20over%20time%2C%20represented%20as%0ARemaining%20Casing%20Potential%20%28RCP%29.%20Based%20on%20our%20architecture%2C%20our%20model%0Aquantifies%20both%20epistemic%20and%20aleatoric%20uncertainty%2C%20providing%20reliable%0Aconfidence%20intervals%20around%20predicted%20RCP.%20Secondly%2C%20to%20incorporate%20real-time%0Adata%2C%20we%20update%20the%20predictive%20model%20in%20the%20digital%20twin%20framework%2C%20ensuring%0Aits%20accuracy%20throughout%20its%20life%20span%20with%20the%20aid%20of%20hybrid%20modeling%20and%20the%0Ause%20of%20discrepancy%20function.%20Thirdly%2C%20to%20assist%20decision%20making%20in%20predictive%0Amaintenance%2C%20we%20implement%20a%20Tire%20State%20Decision%20Algorithm%2C%20which%20strategically%0Adetermines%20the%20optimal%20timing%20for%20tire%20replacement%20based%20on%20RCP%20forecasted%20by%0Aour%20transformer%20model.%20This%20approach%20ensures%20our%20digital%20twin%20accurately%0Apredicts%20system%20health%2C%20continually%20refines%20its%20digital%20representation%2C%20and%0Asupports%20predictive%20maintenance%20decisions.%20Our%20framework%20effectively%20embodies%20a%0Aphysical%20system%2C%20leveraging%20big%20data%20and%20machine%20learning%20for%20predictive%0Amaintenance%2C%20model%20updates%2C%20and%20decision-making.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06220v1&entry.124074799=Read"},
{"title": "A Look at Value-Based Decision-Time vs. Background Planning Methods\n  Across Different Settings", "author": "Safa Alver and Doina Precup", "abstract": "  In model-based reinforcement learning (RL), an agent can leverage a learned\nmodel to improve its way of behaving in different ways. Two of the prevalent\nways to do this are through decision-time and background planning methods. In\nthis study, we are interested in understanding how the value-based versions of\nthese two planning methods will compare against each other across different\nsettings. Towards this goal, we first consider the simplest instantiations of\nvalue-based decision-time and background planning methods and provide\ntheoretical results on which one will perform better in the regular RL and\ntransfer learning settings. Then, we consider the modern instantiations of them\nand provide hypotheses on which one will perform better in the same settings.\nFinally, we perform illustrative experiments to validate these theoretical\nresults and hypotheses. Overall, our findings suggest that even though\nvalue-based versions of the two planning methods perform on par in their\nsimplest instantiations, the modern instantiations of value-based decision-time\nplanning methods can perform on par or better than the modern instantiations of\nvalue-based background planning methods in both the regular RL and transfer\nlearning settings.\n", "link": "http://arxiv.org/abs/2206.08442v3", "date": "2024-08-12", "relevancy": 1.3701, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4973}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4555}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Look%20at%20Value-Based%20Decision-Time%20vs.%20Background%20Planning%20Methods%0A%20%20Across%20Different%20Settings&body=Title%3A%20A%20Look%20at%20Value-Based%20Decision-Time%20vs.%20Background%20Planning%20Methods%0A%20%20Across%20Different%20Settings%0AAuthor%3A%20Safa%20Alver%20and%20Doina%20Precup%0AAbstract%3A%20%20%20In%20model-based%20reinforcement%20learning%20%28RL%29%2C%20an%20agent%20can%20leverage%20a%20learned%0Amodel%20to%20improve%20its%20way%20of%20behaving%20in%20different%20ways.%20Two%20of%20the%20prevalent%0Aways%20to%20do%20this%20are%20through%20decision-time%20and%20background%20planning%20methods.%20In%0Athis%20study%2C%20we%20are%20interested%20in%20understanding%20how%20the%20value-based%20versions%20of%0Athese%20two%20planning%20methods%20will%20compare%20against%20each%20other%20across%20different%0Asettings.%20Towards%20this%20goal%2C%20we%20first%20consider%20the%20simplest%20instantiations%20of%0Avalue-based%20decision-time%20and%20background%20planning%20methods%20and%20provide%0Atheoretical%20results%20on%20which%20one%20will%20perform%20better%20in%20the%20regular%20RL%20and%0Atransfer%20learning%20settings.%20Then%2C%20we%20consider%20the%20modern%20instantiations%20of%20them%0Aand%20provide%20hypotheses%20on%20which%20one%20will%20perform%20better%20in%20the%20same%20settings.%0AFinally%2C%20we%20perform%20illustrative%20experiments%20to%20validate%20these%20theoretical%0Aresults%20and%20hypotheses.%20Overall%2C%20our%20findings%20suggest%20that%20even%20though%0Avalue-based%20versions%20of%20the%20two%20planning%20methods%20perform%20on%20par%20in%20their%0Asimplest%20instantiations%2C%20the%20modern%20instantiations%20of%20value-based%20decision-time%0Aplanning%20methods%20can%20perform%20on%20par%20or%20better%20than%20the%20modern%20instantiations%20of%0Avalue-based%20background%20planning%20methods%20in%20both%20the%20regular%20RL%20and%20transfer%0Alearning%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2206.08442v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Look%2520at%2520Value-Based%2520Decision-Time%2520vs.%2520Background%2520Planning%2520Methods%250A%2520%2520Across%2520Different%2520Settings%26entry.906535625%3DSafa%2520Alver%2520and%2520Doina%2520Precup%26entry.1292438233%3D%2520%2520In%2520model-based%2520reinforcement%2520learning%2520%2528RL%2529%252C%2520an%2520agent%2520can%2520leverage%2520a%2520learned%250Amodel%2520to%2520improve%2520its%2520way%2520of%2520behaving%2520in%2520different%2520ways.%2520Two%2520of%2520the%2520prevalent%250Aways%2520to%2520do%2520this%2520are%2520through%2520decision-time%2520and%2520background%2520planning%2520methods.%2520In%250Athis%2520study%252C%2520we%2520are%2520interested%2520in%2520understanding%2520how%2520the%2520value-based%2520versions%2520of%250Athese%2520two%2520planning%2520methods%2520will%2520compare%2520against%2520each%2520other%2520across%2520different%250Asettings.%2520Towards%2520this%2520goal%252C%2520we%2520first%2520consider%2520the%2520simplest%2520instantiations%2520of%250Avalue-based%2520decision-time%2520and%2520background%2520planning%2520methods%2520and%2520provide%250Atheoretical%2520results%2520on%2520which%2520one%2520will%2520perform%2520better%2520in%2520the%2520regular%2520RL%2520and%250Atransfer%2520learning%2520settings.%2520Then%252C%2520we%2520consider%2520the%2520modern%2520instantiations%2520of%2520them%250Aand%2520provide%2520hypotheses%2520on%2520which%2520one%2520will%2520perform%2520better%2520in%2520the%2520same%2520settings.%250AFinally%252C%2520we%2520perform%2520illustrative%2520experiments%2520to%2520validate%2520these%2520theoretical%250Aresults%2520and%2520hypotheses.%2520Overall%252C%2520our%2520findings%2520suggest%2520that%2520even%2520though%250Avalue-based%2520versions%2520of%2520the%2520two%2520planning%2520methods%2520perform%2520on%2520par%2520in%2520their%250Asimplest%2520instantiations%252C%2520the%2520modern%2520instantiations%2520of%2520value-based%2520decision-time%250Aplanning%2520methods%2520can%2520perform%2520on%2520par%2520or%2520better%2520than%2520the%2520modern%2520instantiations%2520of%250Avalue-based%2520background%2520planning%2520methods%2520in%2520both%2520the%2520regular%2520RL%2520and%2520transfer%250Alearning%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2206.08442v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Look%20at%20Value-Based%20Decision-Time%20vs.%20Background%20Planning%20Methods%0A%20%20Across%20Different%20Settings&entry.906535625=Safa%20Alver%20and%20Doina%20Precup&entry.1292438233=%20%20In%20model-based%20reinforcement%20learning%20%28RL%29%2C%20an%20agent%20can%20leverage%20a%20learned%0Amodel%20to%20improve%20its%20way%20of%20behaving%20in%20different%20ways.%20Two%20of%20the%20prevalent%0Aways%20to%20do%20this%20are%20through%20decision-time%20and%20background%20planning%20methods.%20In%0Athis%20study%2C%20we%20are%20interested%20in%20understanding%20how%20the%20value-based%20versions%20of%0Athese%20two%20planning%20methods%20will%20compare%20against%20each%20other%20across%20different%0Asettings.%20Towards%20this%20goal%2C%20we%20first%20consider%20the%20simplest%20instantiations%20of%0Avalue-based%20decision-time%20and%20background%20planning%20methods%20and%20provide%0Atheoretical%20results%20on%20which%20one%20will%20perform%20better%20in%20the%20regular%20RL%20and%0Atransfer%20learning%20settings.%20Then%2C%20we%20consider%20the%20modern%20instantiations%20of%20them%0Aand%20provide%20hypotheses%20on%20which%20one%20will%20perform%20better%20in%20the%20same%20settings.%0AFinally%2C%20we%20perform%20illustrative%20experiments%20to%20validate%20these%20theoretical%0Aresults%20and%20hypotheses.%20Overall%2C%20our%20findings%20suggest%20that%20even%20though%0Avalue-based%20versions%20of%20the%20two%20planning%20methods%20perform%20on%20par%20in%20their%0Asimplest%20instantiations%2C%20the%20modern%20instantiations%20of%20value-based%20decision-time%0Aplanning%20methods%20can%20perform%20on%20par%20or%20better%20than%20the%20modern%20instantiations%20of%0Avalue-based%20background%20planning%20methods%20in%20both%20the%20regular%20RL%20and%20transfer%0Alearning%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2206.08442v3&entry.124074799=Read"},
{"title": "Body Transformer: Leveraging Robot Embodiment for Policy Learning", "author": "Carmelo Sferrazza and Dun-Ming Huang and Fangchen Liu and Jongmin Lee and Pieter Abbeel", "abstract": "  In recent years, the transformer architecture has become the de facto\nstandard for machine learning algorithms applied to natural language processing\nand computer vision. Despite notable evidence of successful deployment of this\narchitecture in the context of robot learning, we claim that vanilla\ntransformers do not fully exploit the structure of the robot learning problem.\nTherefore, we propose Body Transformer (BoT), an architecture that leverages\nthe robot embodiment by providing an inductive bias that guides the learning\nprocess. We represent the robot body as a graph of sensors and actuators, and\nrely on masked attention to pool information throughout the architecture. The\nresulting architecture outperforms the vanilla transformer, as well as the\nclassical multilayer perceptron, in terms of task completion, scaling\nproperties, and computational efficiency when representing either imitation or\nreinforcement learning policies. Additional material including the open-source\ncode is available at https://sferrazza.cc/bot_site.\n", "link": "http://arxiv.org/abs/2408.06316v1", "date": "2024-08-12", "relevancy": 1.6287, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5815}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5408}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5283}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Body%20Transformer%3A%20Leveraging%20Robot%20Embodiment%20for%20Policy%20Learning&body=Title%3A%20Body%20Transformer%3A%20Leveraging%20Robot%20Embodiment%20for%20Policy%20Learning%0AAuthor%3A%20Carmelo%20Sferrazza%20and%20Dun-Ming%20Huang%20and%20Fangchen%20Liu%20and%20Jongmin%20Lee%20and%20Pieter%20Abbeel%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20transformer%20architecture%20has%20become%20the%20de%20facto%0Astandard%20for%20machine%20learning%20algorithms%20applied%20to%20natural%20language%20processing%0Aand%20computer%20vision.%20Despite%20notable%20evidence%20of%20successful%20deployment%20of%20this%0Aarchitecture%20in%20the%20context%20of%20robot%20learning%2C%20we%20claim%20that%20vanilla%0Atransformers%20do%20not%20fully%20exploit%20the%20structure%20of%20the%20robot%20learning%20problem.%0ATherefore%2C%20we%20propose%20Body%20Transformer%20%28BoT%29%2C%20an%20architecture%20that%20leverages%0Athe%20robot%20embodiment%20by%20providing%20an%20inductive%20bias%20that%20guides%20the%20learning%0Aprocess.%20We%20represent%20the%20robot%20body%20as%20a%20graph%20of%20sensors%20and%20actuators%2C%20and%0Arely%20on%20masked%20attention%20to%20pool%20information%20throughout%20the%20architecture.%20The%0Aresulting%20architecture%20outperforms%20the%20vanilla%20transformer%2C%20as%20well%20as%20the%0Aclassical%20multilayer%20perceptron%2C%20in%20terms%20of%20task%20completion%2C%20scaling%0Aproperties%2C%20and%20computational%20efficiency%20when%20representing%20either%20imitation%20or%0Areinforcement%20learning%20policies.%20Additional%20material%20including%20the%20open-source%0Acode%20is%20available%20at%20https%3A//sferrazza.cc/bot_site.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06316v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBody%2520Transformer%253A%2520Leveraging%2520Robot%2520Embodiment%2520for%2520Policy%2520Learning%26entry.906535625%3DCarmelo%2520Sferrazza%2520and%2520Dun-Ming%2520Huang%2520and%2520Fangchen%2520Liu%2520and%2520Jongmin%2520Lee%2520and%2520Pieter%2520Abbeel%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520the%2520transformer%2520architecture%2520has%2520become%2520the%2520de%2520facto%250Astandard%2520for%2520machine%2520learning%2520algorithms%2520applied%2520to%2520natural%2520language%2520processing%250Aand%2520computer%2520vision.%2520Despite%2520notable%2520evidence%2520of%2520successful%2520deployment%2520of%2520this%250Aarchitecture%2520in%2520the%2520context%2520of%2520robot%2520learning%252C%2520we%2520claim%2520that%2520vanilla%250Atransformers%2520do%2520not%2520fully%2520exploit%2520the%2520structure%2520of%2520the%2520robot%2520learning%2520problem.%250ATherefore%252C%2520we%2520propose%2520Body%2520Transformer%2520%2528BoT%2529%252C%2520an%2520architecture%2520that%2520leverages%250Athe%2520robot%2520embodiment%2520by%2520providing%2520an%2520inductive%2520bias%2520that%2520guides%2520the%2520learning%250Aprocess.%2520We%2520represent%2520the%2520robot%2520body%2520as%2520a%2520graph%2520of%2520sensors%2520and%2520actuators%252C%2520and%250Arely%2520on%2520masked%2520attention%2520to%2520pool%2520information%2520throughout%2520the%2520architecture.%2520The%250Aresulting%2520architecture%2520outperforms%2520the%2520vanilla%2520transformer%252C%2520as%2520well%2520as%2520the%250Aclassical%2520multilayer%2520perceptron%252C%2520in%2520terms%2520of%2520task%2520completion%252C%2520scaling%250Aproperties%252C%2520and%2520computational%2520efficiency%2520when%2520representing%2520either%2520imitation%2520or%250Areinforcement%2520learning%2520policies.%2520Additional%2520material%2520including%2520the%2520open-source%250Acode%2520is%2520available%2520at%2520https%253A//sferrazza.cc/bot_site.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06316v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Body%20Transformer%3A%20Leveraging%20Robot%20Embodiment%20for%20Policy%20Learning&entry.906535625=Carmelo%20Sferrazza%20and%20Dun-Ming%20Huang%20and%20Fangchen%20Liu%20and%20Jongmin%20Lee%20and%20Pieter%20Abbeel&entry.1292438233=%20%20In%20recent%20years%2C%20the%20transformer%20architecture%20has%20become%20the%20de%20facto%0Astandard%20for%20machine%20learning%20algorithms%20applied%20to%20natural%20language%20processing%0Aand%20computer%20vision.%20Despite%20notable%20evidence%20of%20successful%20deployment%20of%20this%0Aarchitecture%20in%20the%20context%20of%20robot%20learning%2C%20we%20claim%20that%20vanilla%0Atransformers%20do%20not%20fully%20exploit%20the%20structure%20of%20the%20robot%20learning%20problem.%0ATherefore%2C%20we%20propose%20Body%20Transformer%20%28BoT%29%2C%20an%20architecture%20that%20leverages%0Athe%20robot%20embodiment%20by%20providing%20an%20inductive%20bias%20that%20guides%20the%20learning%0Aprocess.%20We%20represent%20the%20robot%20body%20as%20a%20graph%20of%20sensors%20and%20actuators%2C%20and%0Arely%20on%20masked%20attention%20to%20pool%20information%20throughout%20the%20architecture.%20The%0Aresulting%20architecture%20outperforms%20the%20vanilla%20transformer%2C%20as%20well%20as%20the%0Aclassical%20multilayer%20perceptron%2C%20in%20terms%20of%20task%20completion%2C%20scaling%0Aproperties%2C%20and%20computational%20efficiency%20when%20representing%20either%20imitation%20or%0Areinforcement%20learning%20policies.%20Additional%20material%20including%20the%20open-source%0Acode%20is%20available%20at%20https%3A//sferrazza.cc/bot_site.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06316v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


