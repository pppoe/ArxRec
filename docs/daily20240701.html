<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240630.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "EgoGaussian: Dynamic Scene Understanding from Egocentric Video with 3D\n  Gaussian Splatting", "author": "Daiwei Zhang and Gengyan Li and Jiajie Li and Micka\u00ebl Bressieux and Otmar Hilliges and Marc Pollefeys and Luc Van Gool and Xi Wang", "abstract": "  Human activities are inherently complex, and even simple household tasks\ninvolve numerous object interactions. To better understand these activities and\nbehaviors, it is crucial to model their dynamic interactions with the\nenvironment. The recent availability of affordable head-mounted cameras and\negocentric data offers a more accessible and efficient means to understand\ndynamic human-object interactions in 3D environments. However, most existing\nmethods for human activity modeling either focus on reconstructing 3D models of\nhand-object or human-scene interactions or on mapping 3D scenes, neglecting\ndynamic interactions with objects. The few existing solutions often require\ninputs from multiple sources, including multi-camera setups, depth-sensing\ncameras, or kinesthetic sensors. To this end, we introduce EgoGaussian, the\nfirst method capable of simultaneously reconstructing 3D scenes and dynamically\ntracking 3D object motion from RGB egocentric input alone. We leverage the\nuniquely discrete nature of Gaussian Splatting and segment dynamic interactions\nfrom the background. Our approach employs a clip-level online learning pipeline\nthat leverages the dynamic nature of human activities, allowing us to\nreconstruct the temporal evolution of the scene in chronological order and\ntrack rigid object motion. Additionally, our method automatically segments\nobject and background Gaussians, providing 3D representations for both static\nscenes and dynamic objects. EgoGaussian outperforms previous NeRF and Dynamic\nGaussian methods in challenging in-the-wild videos and we also qualitatively\ndemonstrate the high quality of the reconstructed models.\n", "link": "http://arxiv.org/abs/2406.19811v1", "date": "2024-06-28", "relevancy": 3.4018, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7104}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6954}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6353}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EgoGaussian%3A%20Dynamic%20Scene%20Understanding%20from%20Egocentric%20Video%20with%203D%0A%20%20Gaussian%20Splatting&body=Title%3A%20EgoGaussian%3A%20Dynamic%20Scene%20Understanding%20from%20Egocentric%20Video%20with%203D%0A%20%20Gaussian%20Splatting%0AAuthor%3A%20Daiwei%20Zhang%20and%20Gengyan%20Li%20and%20Jiajie%20Li%20and%20Micka%C3%ABl%20Bressieux%20and%20Otmar%20Hilliges%20and%20Marc%20Pollefeys%20and%20Luc%20Van%20Gool%20and%20Xi%20Wang%0AAbstract%3A%20%20%20Human%20activities%20are%20inherently%20complex%2C%20and%20even%20simple%20household%20tasks%0Ainvolve%20numerous%20object%20interactions.%20To%20better%20understand%20these%20activities%20and%0Abehaviors%2C%20it%20is%20crucial%20to%20model%20their%20dynamic%20interactions%20with%20the%0Aenvironment.%20The%20recent%20availability%20of%20affordable%20head-mounted%20cameras%20and%0Aegocentric%20data%20offers%20a%20more%20accessible%20and%20efficient%20means%20to%20understand%0Adynamic%20human-object%20interactions%20in%203D%20environments.%20However%2C%20most%20existing%0Amethods%20for%20human%20activity%20modeling%20either%20focus%20on%20reconstructing%203D%20models%20of%0Ahand-object%20or%20human-scene%20interactions%20or%20on%20mapping%203D%20scenes%2C%20neglecting%0Adynamic%20interactions%20with%20objects.%20The%20few%20existing%20solutions%20often%20require%0Ainputs%20from%20multiple%20sources%2C%20including%20multi-camera%20setups%2C%20depth-sensing%0Acameras%2C%20or%20kinesthetic%20sensors.%20To%20this%20end%2C%20we%20introduce%20EgoGaussian%2C%20the%0Afirst%20method%20capable%20of%20simultaneously%20reconstructing%203D%20scenes%20and%20dynamically%0Atracking%203D%20object%20motion%20from%20RGB%20egocentric%20input%20alone.%20We%20leverage%20the%0Auniquely%20discrete%20nature%20of%20Gaussian%20Splatting%20and%20segment%20dynamic%20interactions%0Afrom%20the%20background.%20Our%20approach%20employs%20a%20clip-level%20online%20learning%20pipeline%0Athat%20leverages%20the%20dynamic%20nature%20of%20human%20activities%2C%20allowing%20us%20to%0Areconstruct%20the%20temporal%20evolution%20of%20the%20scene%20in%20chronological%20order%20and%0Atrack%20rigid%20object%20motion.%20Additionally%2C%20our%20method%20automatically%20segments%0Aobject%20and%20background%20Gaussians%2C%20providing%203D%20representations%20for%20both%20static%0Ascenes%20and%20dynamic%20objects.%20EgoGaussian%20outperforms%20previous%20NeRF%20and%20Dynamic%0AGaussian%20methods%20in%20challenging%20in-the-wild%20videos%20and%20we%20also%20qualitatively%0Ademonstrate%20the%20high%20quality%20of%20the%20reconstructed%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19811v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEgoGaussian%253A%2520Dynamic%2520Scene%2520Understanding%2520from%2520Egocentric%2520Video%2520with%25203D%250A%2520%2520Gaussian%2520Splatting%26entry.906535625%3DDaiwei%2520Zhang%2520and%2520Gengyan%2520Li%2520and%2520Jiajie%2520Li%2520and%2520Micka%25C3%25ABl%2520Bressieux%2520and%2520Otmar%2520Hilliges%2520and%2520Marc%2520Pollefeys%2520and%2520Luc%2520Van%2520Gool%2520and%2520Xi%2520Wang%26entry.1292438233%3D%2520%2520Human%2520activities%2520are%2520inherently%2520complex%252C%2520and%2520even%2520simple%2520household%2520tasks%250Ainvolve%2520numerous%2520object%2520interactions.%2520To%2520better%2520understand%2520these%2520activities%2520and%250Abehaviors%252C%2520it%2520is%2520crucial%2520to%2520model%2520their%2520dynamic%2520interactions%2520with%2520the%250Aenvironment.%2520The%2520recent%2520availability%2520of%2520affordable%2520head-mounted%2520cameras%2520and%250Aegocentric%2520data%2520offers%2520a%2520more%2520accessible%2520and%2520efficient%2520means%2520to%2520understand%250Adynamic%2520human-object%2520interactions%2520in%25203D%2520environments.%2520However%252C%2520most%2520existing%250Amethods%2520for%2520human%2520activity%2520modeling%2520either%2520focus%2520on%2520reconstructing%25203D%2520models%2520of%250Ahand-object%2520or%2520human-scene%2520interactions%2520or%2520on%2520mapping%25203D%2520scenes%252C%2520neglecting%250Adynamic%2520interactions%2520with%2520objects.%2520The%2520few%2520existing%2520solutions%2520often%2520require%250Ainputs%2520from%2520multiple%2520sources%252C%2520including%2520multi-camera%2520setups%252C%2520depth-sensing%250Acameras%252C%2520or%2520kinesthetic%2520sensors.%2520To%2520this%2520end%252C%2520we%2520introduce%2520EgoGaussian%252C%2520the%250Afirst%2520method%2520capable%2520of%2520simultaneously%2520reconstructing%25203D%2520scenes%2520and%2520dynamically%250Atracking%25203D%2520object%2520motion%2520from%2520RGB%2520egocentric%2520input%2520alone.%2520We%2520leverage%2520the%250Auniquely%2520discrete%2520nature%2520of%2520Gaussian%2520Splatting%2520and%2520segment%2520dynamic%2520interactions%250Afrom%2520the%2520background.%2520Our%2520approach%2520employs%2520a%2520clip-level%2520online%2520learning%2520pipeline%250Athat%2520leverages%2520the%2520dynamic%2520nature%2520of%2520human%2520activities%252C%2520allowing%2520us%2520to%250Areconstruct%2520the%2520temporal%2520evolution%2520of%2520the%2520scene%2520in%2520chronological%2520order%2520and%250Atrack%2520rigid%2520object%2520motion.%2520Additionally%252C%2520our%2520method%2520automatically%2520segments%250Aobject%2520and%2520background%2520Gaussians%252C%2520providing%25203D%2520representations%2520for%2520both%2520static%250Ascenes%2520and%2520dynamic%2520objects.%2520EgoGaussian%2520outperforms%2520previous%2520NeRF%2520and%2520Dynamic%250AGaussian%2520methods%2520in%2520challenging%2520in-the-wild%2520videos%2520and%2520we%2520also%2520qualitatively%250Ademonstrate%2520the%2520high%2520quality%2520of%2520the%2520reconstructed%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19811v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EgoGaussian%3A%20Dynamic%20Scene%20Understanding%20from%20Egocentric%20Video%20with%203D%0A%20%20Gaussian%20Splatting&entry.906535625=Daiwei%20Zhang%20and%20Gengyan%20Li%20and%20Jiajie%20Li%20and%20Micka%C3%ABl%20Bressieux%20and%20Otmar%20Hilliges%20and%20Marc%20Pollefeys%20and%20Luc%20Van%20Gool%20and%20Xi%20Wang&entry.1292438233=%20%20Human%20activities%20are%20inherently%20complex%2C%20and%20even%20simple%20household%20tasks%0Ainvolve%20numerous%20object%20interactions.%20To%20better%20understand%20these%20activities%20and%0Abehaviors%2C%20it%20is%20crucial%20to%20model%20their%20dynamic%20interactions%20with%20the%0Aenvironment.%20The%20recent%20availability%20of%20affordable%20head-mounted%20cameras%20and%0Aegocentric%20data%20offers%20a%20more%20accessible%20and%20efficient%20means%20to%20understand%0Adynamic%20human-object%20interactions%20in%203D%20environments.%20However%2C%20most%20existing%0Amethods%20for%20human%20activity%20modeling%20either%20focus%20on%20reconstructing%203D%20models%20of%0Ahand-object%20or%20human-scene%20interactions%20or%20on%20mapping%203D%20scenes%2C%20neglecting%0Adynamic%20interactions%20with%20objects.%20The%20few%20existing%20solutions%20often%20require%0Ainputs%20from%20multiple%20sources%2C%20including%20multi-camera%20setups%2C%20depth-sensing%0Acameras%2C%20or%20kinesthetic%20sensors.%20To%20this%20end%2C%20we%20introduce%20EgoGaussian%2C%20the%0Afirst%20method%20capable%20of%20simultaneously%20reconstructing%203D%20scenes%20and%20dynamically%0Atracking%203D%20object%20motion%20from%20RGB%20egocentric%20input%20alone.%20We%20leverage%20the%0Auniquely%20discrete%20nature%20of%20Gaussian%20Splatting%20and%20segment%20dynamic%20interactions%0Afrom%20the%20background.%20Our%20approach%20employs%20a%20clip-level%20online%20learning%20pipeline%0Athat%20leverages%20the%20dynamic%20nature%20of%20human%20activities%2C%20allowing%20us%20to%0Areconstruct%20the%20temporal%20evolution%20of%20the%20scene%20in%20chronological%20order%20and%0Atrack%20rigid%20object%20motion.%20Additionally%2C%20our%20method%20automatically%20segments%0Aobject%20and%20background%20Gaussians%2C%20providing%203D%20representations%20for%20both%20static%0Ascenes%20and%20dynamic%20objects.%20EgoGaussian%20outperforms%20previous%20NeRF%20and%20Dynamic%0AGaussian%20methods%20in%20challenging%20in-the-wild%20videos%20and%20we%20also%20qualitatively%0Ademonstrate%20the%20high%20quality%20of%20the%20reconstructed%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19811v1&entry.124074799=Read"},
{"title": "SpotlessSplats: Ignoring Distractors in 3D Gaussian Splatting", "author": "Sara Sabour and Lily Goli and George Kopanas and Mark Matthews and Dmitry Lagun and Leonidas Guibas and Alec Jacobson and David J. Fleet and Andrea Tagliasacchi", "abstract": "  3D Gaussian Splatting (3DGS) is a promising technique for 3D reconstruction,\noffering efficient training and rendering speeds, making it suitable for\nreal-time applications.However, current methods require highly controlled\nenvironments (no moving people or wind-blown elements, and consistent lighting)\nto meet the inter-view consistency assumption of 3DGS. This makes\nreconstruction of real-world captures problematic. We present SpotlessSplats,\nan approach that leverages pre-trained and general-purpose features coupled\nwith robust optimization to effectively ignore transient distractors. Our\nmethod achieves state-of-the-art reconstruction quality both visually and\nquantitatively, on casual captures.\n", "link": "http://arxiv.org/abs/2406.20055v1", "date": "2024-06-28", "relevancy": 3.2346, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7274}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6517}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5617}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpotlessSplats%3A%20Ignoring%20Distractors%20in%203D%20Gaussian%20Splatting&body=Title%3A%20SpotlessSplats%3A%20Ignoring%20Distractors%20in%203D%20Gaussian%20Splatting%0AAuthor%3A%20Sara%20Sabour%20and%20Lily%20Goli%20and%20George%20Kopanas%20and%20Mark%20Matthews%20and%20Dmitry%20Lagun%20and%20Leonidas%20Guibas%20and%20Alec%20Jacobson%20and%20David%20J.%20Fleet%20and%20Andrea%20Tagliasacchi%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20is%20a%20promising%20technique%20for%203D%20reconstruction%2C%0Aoffering%20efficient%20training%20and%20rendering%20speeds%2C%20making%20it%20suitable%20for%0Areal-time%20applications.However%2C%20current%20methods%20require%20highly%20controlled%0Aenvironments%20%28no%20moving%20people%20or%20wind-blown%20elements%2C%20and%20consistent%20lighting%29%0Ato%20meet%20the%20inter-view%20consistency%20assumption%20of%203DGS.%20This%20makes%0Areconstruction%20of%20real-world%20captures%20problematic.%20We%20present%20SpotlessSplats%2C%0Aan%20approach%20that%20leverages%20pre-trained%20and%20general-purpose%20features%20coupled%0Awith%20robust%20optimization%20to%20effectively%20ignore%20transient%20distractors.%20Our%0Amethod%20achieves%20state-of-the-art%20reconstruction%20quality%20both%20visually%20and%0Aquantitatively%2C%20on%20casual%20captures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.20055v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpotlessSplats%253A%2520Ignoring%2520Distractors%2520in%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DSara%2520Sabour%2520and%2520Lily%2520Goli%2520and%2520George%2520Kopanas%2520and%2520Mark%2520Matthews%2520and%2520Dmitry%2520Lagun%2520and%2520Leonidas%2520Guibas%2520and%2520Alec%2520Jacobson%2520and%2520David%2520J.%2520Fleet%2520and%2520Andrea%2520Tagliasacchi%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520is%2520a%2520promising%2520technique%2520for%25203D%2520reconstruction%252C%250Aoffering%2520efficient%2520training%2520and%2520rendering%2520speeds%252C%2520making%2520it%2520suitable%2520for%250Areal-time%2520applications.However%252C%2520current%2520methods%2520require%2520highly%2520controlled%250Aenvironments%2520%2528no%2520moving%2520people%2520or%2520wind-blown%2520elements%252C%2520and%2520consistent%2520lighting%2529%250Ato%2520meet%2520the%2520inter-view%2520consistency%2520assumption%2520of%25203DGS.%2520This%2520makes%250Areconstruction%2520of%2520real-world%2520captures%2520problematic.%2520We%2520present%2520SpotlessSplats%252C%250Aan%2520approach%2520that%2520leverages%2520pre-trained%2520and%2520general-purpose%2520features%2520coupled%250Awith%2520robust%2520optimization%2520to%2520effectively%2520ignore%2520transient%2520distractors.%2520Our%250Amethod%2520achieves%2520state-of-the-art%2520reconstruction%2520quality%2520both%2520visually%2520and%250Aquantitatively%252C%2520on%2520casual%2520captures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.20055v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpotlessSplats%3A%20Ignoring%20Distractors%20in%203D%20Gaussian%20Splatting&entry.906535625=Sara%20Sabour%20and%20Lily%20Goli%20and%20George%20Kopanas%20and%20Mark%20Matthews%20and%20Dmitry%20Lagun%20and%20Leonidas%20Guibas%20and%20Alec%20Jacobson%20and%20David%20J.%20Fleet%20and%20Andrea%20Tagliasacchi&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20is%20a%20promising%20technique%20for%203D%20reconstruction%2C%0Aoffering%20efficient%20training%20and%20rendering%20speeds%2C%20making%20it%20suitable%20for%0Areal-time%20applications.However%2C%20current%20methods%20require%20highly%20controlled%0Aenvironments%20%28no%20moving%20people%20or%20wind-blown%20elements%2C%20and%20consistent%20lighting%29%0Ato%20meet%20the%20inter-view%20consistency%20assumption%20of%203DGS.%20This%20makes%0Areconstruction%20of%20real-world%20captures%20problematic.%20We%20present%20SpotlessSplats%2C%0Aan%20approach%20that%20leverages%20pre-trained%20and%20general-purpose%20features%20coupled%0Awith%20robust%20optimization%20to%20effectively%20ignore%20transient%20distractors.%20Our%0Amethod%20achieves%20state-of-the-art%20reconstruction%20quality%20both%20visually%20and%0Aquantitatively%2C%20on%20casual%20captures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.20055v1&entry.124074799=Read"},
{"title": "Comprehensive Generative Replay for Task-Incremental Segmentation with\n  Concurrent Appearance and Semantic Forgetting", "author": "Wei Li and Jingyang Zhang and Pheng-Ann Heng and Lixu Gu", "abstract": "  Generalist segmentation models are increasingly favored for diverse tasks\ninvolving various objects from different image sources. Task-Incremental\nLearning (TIL) offers a privacy-preserving training paradigm using tasks\narriving sequentially, instead of gathering them due to strict data sharing\npolicies. However, the task evolution can span a wide scope that involves\nshifts in both image appearance and segmentation semantics with intricate\ncorrelation, causing concurrent appearance and semantic forgetting. To solve\nthis issue, we propose a Comprehensive Generative Replay (CGR) framework that\nrestores appearance and semantic knowledge by synthesizing image-mask pairs to\nmimic past task data, which focuses on two aspects: modeling image-mask\ncorrespondence and promoting scalability for diverse tasks. Specifically, we\nintroduce a novel Bayesian Joint Diffusion (BJD) model for high-quality\nsynthesis of image-mask pairs with their correspondence explicitly preserved by\nconditional denoising. Furthermore, we develop a Task-Oriented Adapter (TOA)\nthat recalibrates prompt embeddings to modulate the diffusion model, making the\ndata synthesis compatible with different tasks. Experiments on incremental\ntasks (cardiac, fundus and prostate segmentation) show its clear advantage for\nalleviating concurrent appearance and semantic forgetting. Code is available at\nhttps://github.com/jingyzhang/CGR.\n", "link": "http://arxiv.org/abs/2406.19796v1", "date": "2024-06-28", "relevancy": 2.9243, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5915}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.582}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comprehensive%20Generative%20Replay%20for%20Task-Incremental%20Segmentation%20with%0A%20%20Concurrent%20Appearance%20and%20Semantic%20Forgetting&body=Title%3A%20Comprehensive%20Generative%20Replay%20for%20Task-Incremental%20Segmentation%20with%0A%20%20Concurrent%20Appearance%20and%20Semantic%20Forgetting%0AAuthor%3A%20Wei%20Li%20and%20Jingyang%20Zhang%20and%20Pheng-Ann%20Heng%20and%20Lixu%20Gu%0AAbstract%3A%20%20%20Generalist%20segmentation%20models%20are%20increasingly%20favored%20for%20diverse%20tasks%0Ainvolving%20various%20objects%20from%20different%20image%20sources.%20Task-Incremental%0ALearning%20%28TIL%29%20offers%20a%20privacy-preserving%20training%20paradigm%20using%20tasks%0Aarriving%20sequentially%2C%20instead%20of%20gathering%20them%20due%20to%20strict%20data%20sharing%0Apolicies.%20However%2C%20the%20task%20evolution%20can%20span%20a%20wide%20scope%20that%20involves%0Ashifts%20in%20both%20image%20appearance%20and%20segmentation%20semantics%20with%20intricate%0Acorrelation%2C%20causing%20concurrent%20appearance%20and%20semantic%20forgetting.%20To%20solve%0Athis%20issue%2C%20we%20propose%20a%20Comprehensive%20Generative%20Replay%20%28CGR%29%20framework%20that%0Arestores%20appearance%20and%20semantic%20knowledge%20by%20synthesizing%20image-mask%20pairs%20to%0Amimic%20past%20task%20data%2C%20which%20focuses%20on%20two%20aspects%3A%20modeling%20image-mask%0Acorrespondence%20and%20promoting%20scalability%20for%20diverse%20tasks.%20Specifically%2C%20we%0Aintroduce%20a%20novel%20Bayesian%20Joint%20Diffusion%20%28BJD%29%20model%20for%20high-quality%0Asynthesis%20of%20image-mask%20pairs%20with%20their%20correspondence%20explicitly%20preserved%20by%0Aconditional%20denoising.%20Furthermore%2C%20we%20develop%20a%20Task-Oriented%20Adapter%20%28TOA%29%0Athat%20recalibrates%20prompt%20embeddings%20to%20modulate%20the%20diffusion%20model%2C%20making%20the%0Adata%20synthesis%20compatible%20with%20different%20tasks.%20Experiments%20on%20incremental%0Atasks%20%28cardiac%2C%20fundus%20and%20prostate%20segmentation%29%20show%20its%20clear%20advantage%20for%0Aalleviating%20concurrent%20appearance%20and%20semantic%20forgetting.%20Code%20is%20available%20at%0Ahttps%3A//github.com/jingyzhang/CGR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19796v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComprehensive%2520Generative%2520Replay%2520for%2520Task-Incremental%2520Segmentation%2520with%250A%2520%2520Concurrent%2520Appearance%2520and%2520Semantic%2520Forgetting%26entry.906535625%3DWei%2520Li%2520and%2520Jingyang%2520Zhang%2520and%2520Pheng-Ann%2520Heng%2520and%2520Lixu%2520Gu%26entry.1292438233%3D%2520%2520Generalist%2520segmentation%2520models%2520are%2520increasingly%2520favored%2520for%2520diverse%2520tasks%250Ainvolving%2520various%2520objects%2520from%2520different%2520image%2520sources.%2520Task-Incremental%250ALearning%2520%2528TIL%2529%2520offers%2520a%2520privacy-preserving%2520training%2520paradigm%2520using%2520tasks%250Aarriving%2520sequentially%252C%2520instead%2520of%2520gathering%2520them%2520due%2520to%2520strict%2520data%2520sharing%250Apolicies.%2520However%252C%2520the%2520task%2520evolution%2520can%2520span%2520a%2520wide%2520scope%2520that%2520involves%250Ashifts%2520in%2520both%2520image%2520appearance%2520and%2520segmentation%2520semantics%2520with%2520intricate%250Acorrelation%252C%2520causing%2520concurrent%2520appearance%2520and%2520semantic%2520forgetting.%2520To%2520solve%250Athis%2520issue%252C%2520we%2520propose%2520a%2520Comprehensive%2520Generative%2520Replay%2520%2528CGR%2529%2520framework%2520that%250Arestores%2520appearance%2520and%2520semantic%2520knowledge%2520by%2520synthesizing%2520image-mask%2520pairs%2520to%250Amimic%2520past%2520task%2520data%252C%2520which%2520focuses%2520on%2520two%2520aspects%253A%2520modeling%2520image-mask%250Acorrespondence%2520and%2520promoting%2520scalability%2520for%2520diverse%2520tasks.%2520Specifically%252C%2520we%250Aintroduce%2520a%2520novel%2520Bayesian%2520Joint%2520Diffusion%2520%2528BJD%2529%2520model%2520for%2520high-quality%250Asynthesis%2520of%2520image-mask%2520pairs%2520with%2520their%2520correspondence%2520explicitly%2520preserved%2520by%250Aconditional%2520denoising.%2520Furthermore%252C%2520we%2520develop%2520a%2520Task-Oriented%2520Adapter%2520%2528TOA%2529%250Athat%2520recalibrates%2520prompt%2520embeddings%2520to%2520modulate%2520the%2520diffusion%2520model%252C%2520making%2520the%250Adata%2520synthesis%2520compatible%2520with%2520different%2520tasks.%2520Experiments%2520on%2520incremental%250Atasks%2520%2528cardiac%252C%2520fundus%2520and%2520prostate%2520segmentation%2529%2520show%2520its%2520clear%2520advantage%2520for%250Aalleviating%2520concurrent%2520appearance%2520and%2520semantic%2520forgetting.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/jingyzhang/CGR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19796v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comprehensive%20Generative%20Replay%20for%20Task-Incremental%20Segmentation%20with%0A%20%20Concurrent%20Appearance%20and%20Semantic%20Forgetting&entry.906535625=Wei%20Li%20and%20Jingyang%20Zhang%20and%20Pheng-Ann%20Heng%20and%20Lixu%20Gu&entry.1292438233=%20%20Generalist%20segmentation%20models%20are%20increasingly%20favored%20for%20diverse%20tasks%0Ainvolving%20various%20objects%20from%20different%20image%20sources.%20Task-Incremental%0ALearning%20%28TIL%29%20offers%20a%20privacy-preserving%20training%20paradigm%20using%20tasks%0Aarriving%20sequentially%2C%20instead%20of%20gathering%20them%20due%20to%20strict%20data%20sharing%0Apolicies.%20However%2C%20the%20task%20evolution%20can%20span%20a%20wide%20scope%20that%20involves%0Ashifts%20in%20both%20image%20appearance%20and%20segmentation%20semantics%20with%20intricate%0Acorrelation%2C%20causing%20concurrent%20appearance%20and%20semantic%20forgetting.%20To%20solve%0Athis%20issue%2C%20we%20propose%20a%20Comprehensive%20Generative%20Replay%20%28CGR%29%20framework%20that%0Arestores%20appearance%20and%20semantic%20knowledge%20by%20synthesizing%20image-mask%20pairs%20to%0Amimic%20past%20task%20data%2C%20which%20focuses%20on%20two%20aspects%3A%20modeling%20image-mask%0Acorrespondence%20and%20promoting%20scalability%20for%20diverse%20tasks.%20Specifically%2C%20we%0Aintroduce%20a%20novel%20Bayesian%20Joint%20Diffusion%20%28BJD%29%20model%20for%20high-quality%0Asynthesis%20of%20image-mask%20pairs%20with%20their%20correspondence%20explicitly%20preserved%20by%0Aconditional%20denoising.%20Furthermore%2C%20we%20develop%20a%20Task-Oriented%20Adapter%20%28TOA%29%0Athat%20recalibrates%20prompt%20embeddings%20to%20modulate%20the%20diffusion%20model%2C%20making%20the%0Adata%20synthesis%20compatible%20with%20different%20tasks.%20Experiments%20on%20incremental%0Atasks%20%28cardiac%2C%20fundus%20and%20prostate%20segmentation%29%20show%20its%20clear%20advantage%20for%0Aalleviating%20concurrent%20appearance%20and%20semantic%20forgetting.%20Code%20is%20available%20at%0Ahttps%3A//github.com/jingyzhang/CGR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19796v1&entry.124074799=Read"},
{"title": "GRACE: Graph-Regularized Attentive Convolutional Entanglement with\n  Laplacian Smoothing for Robust DeepFake Video Detection", "author": "Chih-Chung Hsu and Shao-Ning Chen and Mei-Hsuan Wu and Yi-Fang Wang and Chia-Ming Lee and Yi-Shiuan Chou", "abstract": "  As DeepFake video manipulation techniques escalate, posing profound threats,\nthe urgent need to develop efficient detection strategies is underscored.\nHowever, one particular issue lies with facial images being mis-detected, often\noriginating from degraded videos or adversarial attacks, leading to unexpected\ntemporal artifacts that can undermine the efficacy of DeepFake video detection\ntechniques. This paper introduces a novel method for robust DeepFake video\ndetection, harnessing the power of the proposed Graph-Regularized Attentive\nConvolutional Entanglement (GRACE) based on the graph convolutional network\nwith graph Laplacian to address the aforementioned challenges. First,\nconventional Convolution Neural Networks are deployed to perform spatiotemporal\nfeatures for the entire video. Then, the spatial and temporal features are\nmutually entangled by constructing a graph with sparse constraint, enforcing\nessential features of valid face images in the noisy face sequences remaining,\nthus augmenting stability and performance for DeepFake video detection.\nFurthermore, the Graph Laplacian prior is proposed in the graph convolutional\nnetwork to remove the noise pattern in the feature space to further improve the\nperformance. Comprehensive experiments are conducted to illustrate that our\nproposed method delivers state-of-the-art performance in DeepFake video\ndetection under noisy face sequences. The source code is available at\nhttps://github.com/ming053l/GRACE.\n", "link": "http://arxiv.org/abs/2406.19941v1", "date": "2024-06-28", "relevancy": 2.7378, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5558}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5542}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5326}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GRACE%3A%20Graph-Regularized%20Attentive%20Convolutional%20Entanglement%20with%0A%20%20Laplacian%20Smoothing%20for%20Robust%20DeepFake%20Video%20Detection&body=Title%3A%20GRACE%3A%20Graph-Regularized%20Attentive%20Convolutional%20Entanglement%20with%0A%20%20Laplacian%20Smoothing%20for%20Robust%20DeepFake%20Video%20Detection%0AAuthor%3A%20Chih-Chung%20Hsu%20and%20Shao-Ning%20Chen%20and%20Mei-Hsuan%20Wu%20and%20Yi-Fang%20Wang%20and%20Chia-Ming%20Lee%20and%20Yi-Shiuan%20Chou%0AAbstract%3A%20%20%20As%20DeepFake%20video%20manipulation%20techniques%20escalate%2C%20posing%20profound%20threats%2C%0Athe%20urgent%20need%20to%20develop%20efficient%20detection%20strategies%20is%20underscored.%0AHowever%2C%20one%20particular%20issue%20lies%20with%20facial%20images%20being%20mis-detected%2C%20often%0Aoriginating%20from%20degraded%20videos%20or%20adversarial%20attacks%2C%20leading%20to%20unexpected%0Atemporal%20artifacts%20that%20can%20undermine%20the%20efficacy%20of%20DeepFake%20video%20detection%0Atechniques.%20This%20paper%20introduces%20a%20novel%20method%20for%20robust%20DeepFake%20video%0Adetection%2C%20harnessing%20the%20power%20of%20the%20proposed%20Graph-Regularized%20Attentive%0AConvolutional%20Entanglement%20%28GRACE%29%20based%20on%20the%20graph%20convolutional%20network%0Awith%20graph%20Laplacian%20to%20address%20the%20aforementioned%20challenges.%20First%2C%0Aconventional%20Convolution%20Neural%20Networks%20are%20deployed%20to%20perform%20spatiotemporal%0Afeatures%20for%20the%20entire%20video.%20Then%2C%20the%20spatial%20and%20temporal%20features%20are%0Amutually%20entangled%20by%20constructing%20a%20graph%20with%20sparse%20constraint%2C%20enforcing%0Aessential%20features%20of%20valid%20face%20images%20in%20the%20noisy%20face%20sequences%20remaining%2C%0Athus%20augmenting%20stability%20and%20performance%20for%20DeepFake%20video%20detection.%0AFurthermore%2C%20the%20Graph%20Laplacian%20prior%20is%20proposed%20in%20the%20graph%20convolutional%0Anetwork%20to%20remove%20the%20noise%20pattern%20in%20the%20feature%20space%20to%20further%20improve%20the%0Aperformance.%20Comprehensive%20experiments%20are%20conducted%20to%20illustrate%20that%20our%0Aproposed%20method%20delivers%20state-of-the-art%20performance%20in%20DeepFake%20video%0Adetection%20under%20noisy%20face%20sequences.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/ming053l/GRACE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19941v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGRACE%253A%2520Graph-Regularized%2520Attentive%2520Convolutional%2520Entanglement%2520with%250A%2520%2520Laplacian%2520Smoothing%2520for%2520Robust%2520DeepFake%2520Video%2520Detection%26entry.906535625%3DChih-Chung%2520Hsu%2520and%2520Shao-Ning%2520Chen%2520and%2520Mei-Hsuan%2520Wu%2520and%2520Yi-Fang%2520Wang%2520and%2520Chia-Ming%2520Lee%2520and%2520Yi-Shiuan%2520Chou%26entry.1292438233%3D%2520%2520As%2520DeepFake%2520video%2520manipulation%2520techniques%2520escalate%252C%2520posing%2520profound%2520threats%252C%250Athe%2520urgent%2520need%2520to%2520develop%2520efficient%2520detection%2520strategies%2520is%2520underscored.%250AHowever%252C%2520one%2520particular%2520issue%2520lies%2520with%2520facial%2520images%2520being%2520mis-detected%252C%2520often%250Aoriginating%2520from%2520degraded%2520videos%2520or%2520adversarial%2520attacks%252C%2520leading%2520to%2520unexpected%250Atemporal%2520artifacts%2520that%2520can%2520undermine%2520the%2520efficacy%2520of%2520DeepFake%2520video%2520detection%250Atechniques.%2520This%2520paper%2520introduces%2520a%2520novel%2520method%2520for%2520robust%2520DeepFake%2520video%250Adetection%252C%2520harnessing%2520the%2520power%2520of%2520the%2520proposed%2520Graph-Regularized%2520Attentive%250AConvolutional%2520Entanglement%2520%2528GRACE%2529%2520based%2520on%2520the%2520graph%2520convolutional%2520network%250Awith%2520graph%2520Laplacian%2520to%2520address%2520the%2520aforementioned%2520challenges.%2520First%252C%250Aconventional%2520Convolution%2520Neural%2520Networks%2520are%2520deployed%2520to%2520perform%2520spatiotemporal%250Afeatures%2520for%2520the%2520entire%2520video.%2520Then%252C%2520the%2520spatial%2520and%2520temporal%2520features%2520are%250Amutually%2520entangled%2520by%2520constructing%2520a%2520graph%2520with%2520sparse%2520constraint%252C%2520enforcing%250Aessential%2520features%2520of%2520valid%2520face%2520images%2520in%2520the%2520noisy%2520face%2520sequences%2520remaining%252C%250Athus%2520augmenting%2520stability%2520and%2520performance%2520for%2520DeepFake%2520video%2520detection.%250AFurthermore%252C%2520the%2520Graph%2520Laplacian%2520prior%2520is%2520proposed%2520in%2520the%2520graph%2520convolutional%250Anetwork%2520to%2520remove%2520the%2520noise%2520pattern%2520in%2520the%2520feature%2520space%2520to%2520further%2520improve%2520the%250Aperformance.%2520Comprehensive%2520experiments%2520are%2520conducted%2520to%2520illustrate%2520that%2520our%250Aproposed%2520method%2520delivers%2520state-of-the-art%2520performance%2520in%2520DeepFake%2520video%250Adetection%2520under%2520noisy%2520face%2520sequences.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/ming053l/GRACE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19941v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GRACE%3A%20Graph-Regularized%20Attentive%20Convolutional%20Entanglement%20with%0A%20%20Laplacian%20Smoothing%20for%20Robust%20DeepFake%20Video%20Detection&entry.906535625=Chih-Chung%20Hsu%20and%20Shao-Ning%20Chen%20and%20Mei-Hsuan%20Wu%20and%20Yi-Fang%20Wang%20and%20Chia-Ming%20Lee%20and%20Yi-Shiuan%20Chou&entry.1292438233=%20%20As%20DeepFake%20video%20manipulation%20techniques%20escalate%2C%20posing%20profound%20threats%2C%0Athe%20urgent%20need%20to%20develop%20efficient%20detection%20strategies%20is%20underscored.%0AHowever%2C%20one%20particular%20issue%20lies%20with%20facial%20images%20being%20mis-detected%2C%20often%0Aoriginating%20from%20degraded%20videos%20or%20adversarial%20attacks%2C%20leading%20to%20unexpected%0Atemporal%20artifacts%20that%20can%20undermine%20the%20efficacy%20of%20DeepFake%20video%20detection%0Atechniques.%20This%20paper%20introduces%20a%20novel%20method%20for%20robust%20DeepFake%20video%0Adetection%2C%20harnessing%20the%20power%20of%20the%20proposed%20Graph-Regularized%20Attentive%0AConvolutional%20Entanglement%20%28GRACE%29%20based%20on%20the%20graph%20convolutional%20network%0Awith%20graph%20Laplacian%20to%20address%20the%20aforementioned%20challenges.%20First%2C%0Aconventional%20Convolution%20Neural%20Networks%20are%20deployed%20to%20perform%20spatiotemporal%0Afeatures%20for%20the%20entire%20video.%20Then%2C%20the%20spatial%20and%20temporal%20features%20are%0Amutually%20entangled%20by%20constructing%20a%20graph%20with%20sparse%20constraint%2C%20enforcing%0Aessential%20features%20of%20valid%20face%20images%20in%20the%20noisy%20face%20sequences%20remaining%2C%0Athus%20augmenting%20stability%20and%20performance%20for%20DeepFake%20video%20detection.%0AFurthermore%2C%20the%20Graph%20Laplacian%20prior%20is%20proposed%20in%20the%20graph%20convolutional%0Anetwork%20to%20remove%20the%20noise%20pattern%20in%20the%20feature%20space%20to%20further%20improve%20the%0Aperformance.%20Comprehensive%20experiments%20are%20conducted%20to%20illustrate%20that%20our%0Aproposed%20method%20delivers%20state-of-the-art%20performance%20in%20DeepFake%20video%0Adetection%20under%20noisy%20face%20sequences.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/ming053l/GRACE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19941v1&entry.124074799=Read"},
{"title": "GEO: Generative Engine Optimization", "author": "Pranjal Aggarwal and Vishvak Murahari and Tanmay Rajpurohit and Ashwin Kalyan and Karthik Narasimhan and Ameet Deshpande", "abstract": "  The advent of large language models (LLMs) has ushered in a new paradigm of\nsearch engines that use generative models to gather and summarize information\nto answer user queries. This emerging technology, which we formalize under the\nunified framework of generative engines (GEs), can generate accurate and\npersonalized responses, rapidly replacing traditional search engines like\nGoogle and Bing. Generative Engines typically satisfy queries by synthesizing\ninformation from multiple sources and summarizing them using LLMs. While this\nshift significantly improves $\\textit{user}$ utility and $\\textit{generative\nsearch engine}$ traffic, it poses a huge challenge for the third stakeholder --\nwebsite and content creators. Given the black-box and fast-moving nature of\ngenerative engines, content creators have little to no control over\n$\\textit{when}$ and $\\textit{how}$ their content is displayed. With generative\nengines here to stay, we must ensure the creator economy is not disadvantaged.\nTo address this, we introduce Generative Engine Optimization (GEO), the first\nnovel paradigm to aid content creators in improving their content visibility in\ngenerative engine responses through a flexible black-box optimization framework\nfor optimizing and defining visibility metrics. We facilitate systematic\nevaluation by introducing GEO-bench, a large-scale benchmark of diverse user\nqueries across multiple domains, along with relevant web sources to answer\nthese queries. Through rigorous evaluation, we demonstrate that GEO can boost\nvisibility by up to $40\\%$ in generative engine responses. Moreover, we show\nthe efficacy of these strategies varies across domains, underscoring the need\nfor domain-specific optimization methods. Our work opens a new frontier in\ninformation discovery systems, with profound implications for both developers\nof generative engines and content creators.\n", "link": "http://arxiv.org/abs/2311.09735v3", "date": "2024-06-28", "relevancy": 2.6597, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5842}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.511}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5005}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GEO%3A%20Generative%20Engine%20Optimization&body=Title%3A%20GEO%3A%20Generative%20Engine%20Optimization%0AAuthor%3A%20Pranjal%20Aggarwal%20and%20Vishvak%20Murahari%20and%20Tanmay%20Rajpurohit%20and%20Ashwin%20Kalyan%20and%20Karthik%20Narasimhan%20and%20Ameet%20Deshpande%0AAbstract%3A%20%20%20The%20advent%20of%20large%20language%20models%20%28LLMs%29%20has%20ushered%20in%20a%20new%20paradigm%20of%0Asearch%20engines%20that%20use%20generative%20models%20to%20gather%20and%20summarize%20information%0Ato%20answer%20user%20queries.%20This%20emerging%20technology%2C%20which%20we%20formalize%20under%20the%0Aunified%20framework%20of%20generative%20engines%20%28GEs%29%2C%20can%20generate%20accurate%20and%0Apersonalized%20responses%2C%20rapidly%20replacing%20traditional%20search%20engines%20like%0AGoogle%20and%20Bing.%20Generative%20Engines%20typically%20satisfy%20queries%20by%20synthesizing%0Ainformation%20from%20multiple%20sources%20and%20summarizing%20them%20using%20LLMs.%20While%20this%0Ashift%20significantly%20improves%20%24%5Ctextit%7Buser%7D%24%20utility%20and%20%24%5Ctextit%7Bgenerative%0Asearch%20engine%7D%24%20traffic%2C%20it%20poses%20a%20huge%20challenge%20for%20the%20third%20stakeholder%20--%0Awebsite%20and%20content%20creators.%20Given%20the%20black-box%20and%20fast-moving%20nature%20of%0Agenerative%20engines%2C%20content%20creators%20have%20little%20to%20no%20control%20over%0A%24%5Ctextit%7Bwhen%7D%24%20and%20%24%5Ctextit%7Bhow%7D%24%20their%20content%20is%20displayed.%20With%20generative%0Aengines%20here%20to%20stay%2C%20we%20must%20ensure%20the%20creator%20economy%20is%20not%20disadvantaged.%0ATo%20address%20this%2C%20we%20introduce%20Generative%20Engine%20Optimization%20%28GEO%29%2C%20the%20first%0Anovel%20paradigm%20to%20aid%20content%20creators%20in%20improving%20their%20content%20visibility%20in%0Agenerative%20engine%20responses%20through%20a%20flexible%20black-box%20optimization%20framework%0Afor%20optimizing%20and%20defining%20visibility%20metrics.%20We%20facilitate%20systematic%0Aevaluation%20by%20introducing%20GEO-bench%2C%20a%20large-scale%20benchmark%20of%20diverse%20user%0Aqueries%20across%20multiple%20domains%2C%20along%20with%20relevant%20web%20sources%20to%20answer%0Athese%20queries.%20Through%20rigorous%20evaluation%2C%20we%20demonstrate%20that%20GEO%20can%20boost%0Avisibility%20by%20up%20to%20%2440%5C%25%24%20in%20generative%20engine%20responses.%20Moreover%2C%20we%20show%0Athe%20efficacy%20of%20these%20strategies%20varies%20across%20domains%2C%20underscoring%20the%20need%0Afor%20domain-specific%20optimization%20methods.%20Our%20work%20opens%20a%20new%20frontier%20in%0Ainformation%20discovery%20systems%2C%20with%20profound%20implications%20for%20both%20developers%0Aof%20generative%20engines%20and%20content%20creators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.09735v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGEO%253A%2520Generative%2520Engine%2520Optimization%26entry.906535625%3DPranjal%2520Aggarwal%2520and%2520Vishvak%2520Murahari%2520and%2520Tanmay%2520Rajpurohit%2520and%2520Ashwin%2520Kalyan%2520and%2520Karthik%2520Narasimhan%2520and%2520Ameet%2520Deshpande%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520ushered%2520in%2520a%2520new%2520paradigm%2520of%250Asearch%2520engines%2520that%2520use%2520generative%2520models%2520to%2520gather%2520and%2520summarize%2520information%250Ato%2520answer%2520user%2520queries.%2520This%2520emerging%2520technology%252C%2520which%2520we%2520formalize%2520under%2520the%250Aunified%2520framework%2520of%2520generative%2520engines%2520%2528GEs%2529%252C%2520can%2520generate%2520accurate%2520and%250Apersonalized%2520responses%252C%2520rapidly%2520replacing%2520traditional%2520search%2520engines%2520like%250AGoogle%2520and%2520Bing.%2520Generative%2520Engines%2520typically%2520satisfy%2520queries%2520by%2520synthesizing%250Ainformation%2520from%2520multiple%2520sources%2520and%2520summarizing%2520them%2520using%2520LLMs.%2520While%2520this%250Ashift%2520significantly%2520improves%2520%2524%255Ctextit%257Buser%257D%2524%2520utility%2520and%2520%2524%255Ctextit%257Bgenerative%250Asearch%2520engine%257D%2524%2520traffic%252C%2520it%2520poses%2520a%2520huge%2520challenge%2520for%2520the%2520third%2520stakeholder%2520--%250Awebsite%2520and%2520content%2520creators.%2520Given%2520the%2520black-box%2520and%2520fast-moving%2520nature%2520of%250Agenerative%2520engines%252C%2520content%2520creators%2520have%2520little%2520to%2520no%2520control%2520over%250A%2524%255Ctextit%257Bwhen%257D%2524%2520and%2520%2524%255Ctextit%257Bhow%257D%2524%2520their%2520content%2520is%2520displayed.%2520With%2520generative%250Aengines%2520here%2520to%2520stay%252C%2520we%2520must%2520ensure%2520the%2520creator%2520economy%2520is%2520not%2520disadvantaged.%250ATo%2520address%2520this%252C%2520we%2520introduce%2520Generative%2520Engine%2520Optimization%2520%2528GEO%2529%252C%2520the%2520first%250Anovel%2520paradigm%2520to%2520aid%2520content%2520creators%2520in%2520improving%2520their%2520content%2520visibility%2520in%250Agenerative%2520engine%2520responses%2520through%2520a%2520flexible%2520black-box%2520optimization%2520framework%250Afor%2520optimizing%2520and%2520defining%2520visibility%2520metrics.%2520We%2520facilitate%2520systematic%250Aevaluation%2520by%2520introducing%2520GEO-bench%252C%2520a%2520large-scale%2520benchmark%2520of%2520diverse%2520user%250Aqueries%2520across%2520multiple%2520domains%252C%2520along%2520with%2520relevant%2520web%2520sources%2520to%2520answer%250Athese%2520queries.%2520Through%2520rigorous%2520evaluation%252C%2520we%2520demonstrate%2520that%2520GEO%2520can%2520boost%250Avisibility%2520by%2520up%2520to%2520%252440%255C%2525%2524%2520in%2520generative%2520engine%2520responses.%2520Moreover%252C%2520we%2520show%250Athe%2520efficacy%2520of%2520these%2520strategies%2520varies%2520across%2520domains%252C%2520underscoring%2520the%2520need%250Afor%2520domain-specific%2520optimization%2520methods.%2520Our%2520work%2520opens%2520a%2520new%2520frontier%2520in%250Ainformation%2520discovery%2520systems%252C%2520with%2520profound%2520implications%2520for%2520both%2520developers%250Aof%2520generative%2520engines%2520and%2520content%2520creators.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.09735v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GEO%3A%20Generative%20Engine%20Optimization&entry.906535625=Pranjal%20Aggarwal%20and%20Vishvak%20Murahari%20and%20Tanmay%20Rajpurohit%20and%20Ashwin%20Kalyan%20and%20Karthik%20Narasimhan%20and%20Ameet%20Deshpande&entry.1292438233=%20%20The%20advent%20of%20large%20language%20models%20%28LLMs%29%20has%20ushered%20in%20a%20new%20paradigm%20of%0Asearch%20engines%20that%20use%20generative%20models%20to%20gather%20and%20summarize%20information%0Ato%20answer%20user%20queries.%20This%20emerging%20technology%2C%20which%20we%20formalize%20under%20the%0Aunified%20framework%20of%20generative%20engines%20%28GEs%29%2C%20can%20generate%20accurate%20and%0Apersonalized%20responses%2C%20rapidly%20replacing%20traditional%20search%20engines%20like%0AGoogle%20and%20Bing.%20Generative%20Engines%20typically%20satisfy%20queries%20by%20synthesizing%0Ainformation%20from%20multiple%20sources%20and%20summarizing%20them%20using%20LLMs.%20While%20this%0Ashift%20significantly%20improves%20%24%5Ctextit%7Buser%7D%24%20utility%20and%20%24%5Ctextit%7Bgenerative%0Asearch%20engine%7D%24%20traffic%2C%20it%20poses%20a%20huge%20challenge%20for%20the%20third%20stakeholder%20--%0Awebsite%20and%20content%20creators.%20Given%20the%20black-box%20and%20fast-moving%20nature%20of%0Agenerative%20engines%2C%20content%20creators%20have%20little%20to%20no%20control%20over%0A%24%5Ctextit%7Bwhen%7D%24%20and%20%24%5Ctextit%7Bhow%7D%24%20their%20content%20is%20displayed.%20With%20generative%0Aengines%20here%20to%20stay%2C%20we%20must%20ensure%20the%20creator%20economy%20is%20not%20disadvantaged.%0ATo%20address%20this%2C%20we%20introduce%20Generative%20Engine%20Optimization%20%28GEO%29%2C%20the%20first%0Anovel%20paradigm%20to%20aid%20content%20creators%20in%20improving%20their%20content%20visibility%20in%0Agenerative%20engine%20responses%20through%20a%20flexible%20black-box%20optimization%20framework%0Afor%20optimizing%20and%20defining%20visibility%20metrics.%20We%20facilitate%20systematic%0Aevaluation%20by%20introducing%20GEO-bench%2C%20a%20large-scale%20benchmark%20of%20diverse%20user%0Aqueries%20across%20multiple%20domains%2C%20along%20with%20relevant%20web%20sources%20to%20answer%0Athese%20queries.%20Through%20rigorous%20evaluation%2C%20we%20demonstrate%20that%20GEO%20can%20boost%0Avisibility%20by%20up%20to%20%2440%5C%25%24%20in%20generative%20engine%20responses.%20Moreover%2C%20we%20show%0Athe%20efficacy%20of%20these%20strategies%20varies%20across%20domains%2C%20underscoring%20the%20need%0Afor%20domain-specific%20optimization%20methods.%20Our%20work%20opens%20a%20new%20frontier%20in%0Ainformation%20discovery%20systems%2C%20with%20profound%20implications%20for%20both%20developers%0Aof%20generative%20engines%20and%20content%20creators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.09735v3&entry.124074799=Read"},
{"title": "Modeling State Shifting via Local-Global Distillation for Event-Frame\n  Gaze Tracking", "author": "Jiading Li and Zhiyu Zhu and Jinhui Hou and Junhui Hou and Jinjian Wu", "abstract": "  This paper tackles the problem of passive gaze estimation using both event\nand frame data. Considering the inherently different physiological structures,\nit is intractable to accurately estimate gaze purely based on a given state.\nThus, we reformulate gaze estimation as the quantification of the state\nshifting from the current state to several prior registered anchor states.\nSpecifically, we propose a two-stage learning-based gaze estimation framework\nthat divides the whole gaze estimation process into a coarse-to-fine approach\ninvolving anchor state selection and final gaze location. Moreover, to improve\nthe generalization ability, instead of learning a large gaze estimation network\ndirectly, we align a group of local experts with a student network, where a\nnovel denoising distillation algorithm is introduced to utilize denoising\ndiffusion techniques to iteratively remove inherent noise in event data.\nExtensive experiments demonstrate the effectiveness of the proposed method,\nwhich surpasses state-of-the-art methods by a large margin of 15$\\%$. The code\nwill be publicly available at\nhttps://github.com/jdjdli/Denoise_distill_EF_gazetracker.\n", "link": "http://arxiv.org/abs/2404.00548v2", "date": "2024-06-28", "relevancy": 2.6512, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.559}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5195}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5123}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modeling%20State%20Shifting%20via%20Local-Global%20Distillation%20for%20Event-Frame%0A%20%20Gaze%20Tracking&body=Title%3A%20Modeling%20State%20Shifting%20via%20Local-Global%20Distillation%20for%20Event-Frame%0A%20%20Gaze%20Tracking%0AAuthor%3A%20Jiading%20Li%20and%20Zhiyu%20Zhu%20and%20Jinhui%20Hou%20and%20Junhui%20Hou%20and%20Jinjian%20Wu%0AAbstract%3A%20%20%20This%20paper%20tackles%20the%20problem%20of%20passive%20gaze%20estimation%20using%20both%20event%0Aand%20frame%20data.%20Considering%20the%20inherently%20different%20physiological%20structures%2C%0Ait%20is%20intractable%20to%20accurately%20estimate%20gaze%20purely%20based%20on%20a%20given%20state.%0AThus%2C%20we%20reformulate%20gaze%20estimation%20as%20the%20quantification%20of%20the%20state%0Ashifting%20from%20the%20current%20state%20to%20several%20prior%20registered%20anchor%20states.%0ASpecifically%2C%20we%20propose%20a%20two-stage%20learning-based%20gaze%20estimation%20framework%0Athat%20divides%20the%20whole%20gaze%20estimation%20process%20into%20a%20coarse-to-fine%20approach%0Ainvolving%20anchor%20state%20selection%20and%20final%20gaze%20location.%20Moreover%2C%20to%20improve%0Athe%20generalization%20ability%2C%20instead%20of%20learning%20a%20large%20gaze%20estimation%20network%0Adirectly%2C%20we%20align%20a%20group%20of%20local%20experts%20with%20a%20student%20network%2C%20where%20a%0Anovel%20denoising%20distillation%20algorithm%20is%20introduced%20to%20utilize%20denoising%0Adiffusion%20techniques%20to%20iteratively%20remove%20inherent%20noise%20in%20event%20data.%0AExtensive%20experiments%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method%2C%0Awhich%20surpasses%20state-of-the-art%20methods%20by%20a%20large%20margin%20of%2015%24%5C%25%24.%20The%20code%0Awill%20be%20publicly%20available%20at%0Ahttps%3A//github.com/jdjdli/Denoise_distill_EF_gazetracker.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.00548v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModeling%2520State%2520Shifting%2520via%2520Local-Global%2520Distillation%2520for%2520Event-Frame%250A%2520%2520Gaze%2520Tracking%26entry.906535625%3DJiading%2520Li%2520and%2520Zhiyu%2520Zhu%2520and%2520Jinhui%2520Hou%2520and%2520Junhui%2520Hou%2520and%2520Jinjian%2520Wu%26entry.1292438233%3D%2520%2520This%2520paper%2520tackles%2520the%2520problem%2520of%2520passive%2520gaze%2520estimation%2520using%2520both%2520event%250Aand%2520frame%2520data.%2520Considering%2520the%2520inherently%2520different%2520physiological%2520structures%252C%250Ait%2520is%2520intractable%2520to%2520accurately%2520estimate%2520gaze%2520purely%2520based%2520on%2520a%2520given%2520state.%250AThus%252C%2520we%2520reformulate%2520gaze%2520estimation%2520as%2520the%2520quantification%2520of%2520the%2520state%250Ashifting%2520from%2520the%2520current%2520state%2520to%2520several%2520prior%2520registered%2520anchor%2520states.%250ASpecifically%252C%2520we%2520propose%2520a%2520two-stage%2520learning-based%2520gaze%2520estimation%2520framework%250Athat%2520divides%2520the%2520whole%2520gaze%2520estimation%2520process%2520into%2520a%2520coarse-to-fine%2520approach%250Ainvolving%2520anchor%2520state%2520selection%2520and%2520final%2520gaze%2520location.%2520Moreover%252C%2520to%2520improve%250Athe%2520generalization%2520ability%252C%2520instead%2520of%2520learning%2520a%2520large%2520gaze%2520estimation%2520network%250Adirectly%252C%2520we%2520align%2520a%2520group%2520of%2520local%2520experts%2520with%2520a%2520student%2520network%252C%2520where%2520a%250Anovel%2520denoising%2520distillation%2520algorithm%2520is%2520introduced%2520to%2520utilize%2520denoising%250Adiffusion%2520techniques%2520to%2520iteratively%2520remove%2520inherent%2520noise%2520in%2520event%2520data.%250AExtensive%2520experiments%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method%252C%250Awhich%2520surpasses%2520state-of-the-art%2520methods%2520by%2520a%2520large%2520margin%2520of%252015%2524%255C%2525%2524.%2520The%2520code%250Awill%2520be%2520publicly%2520available%2520at%250Ahttps%253A//github.com/jdjdli/Denoise_distill_EF_gazetracker.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.00548v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modeling%20State%20Shifting%20via%20Local-Global%20Distillation%20for%20Event-Frame%0A%20%20Gaze%20Tracking&entry.906535625=Jiading%20Li%20and%20Zhiyu%20Zhu%20and%20Jinhui%20Hou%20and%20Junhui%20Hou%20and%20Jinjian%20Wu&entry.1292438233=%20%20This%20paper%20tackles%20the%20problem%20of%20passive%20gaze%20estimation%20using%20both%20event%0Aand%20frame%20data.%20Considering%20the%20inherently%20different%20physiological%20structures%2C%0Ait%20is%20intractable%20to%20accurately%20estimate%20gaze%20purely%20based%20on%20a%20given%20state.%0AThus%2C%20we%20reformulate%20gaze%20estimation%20as%20the%20quantification%20of%20the%20state%0Ashifting%20from%20the%20current%20state%20to%20several%20prior%20registered%20anchor%20states.%0ASpecifically%2C%20we%20propose%20a%20two-stage%20learning-based%20gaze%20estimation%20framework%0Athat%20divides%20the%20whole%20gaze%20estimation%20process%20into%20a%20coarse-to-fine%20approach%0Ainvolving%20anchor%20state%20selection%20and%20final%20gaze%20location.%20Moreover%2C%20to%20improve%0Athe%20generalization%20ability%2C%20instead%20of%20learning%20a%20large%20gaze%20estimation%20network%0Adirectly%2C%20we%20align%20a%20group%20of%20local%20experts%20with%20a%20student%20network%2C%20where%20a%0Anovel%20denoising%20distillation%20algorithm%20is%20introduced%20to%20utilize%20denoising%0Adiffusion%20techniques%20to%20iteratively%20remove%20inherent%20noise%20in%20event%20data.%0AExtensive%20experiments%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method%2C%0Awhich%20surpasses%20state-of-the-art%20methods%20by%20a%20large%20margin%20of%2015%24%5C%25%24.%20The%20code%0Awill%20be%20publicly%20available%20at%0Ahttps%3A//github.com/jdjdli/Denoise_distill_EF_gazetracker.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.00548v2&entry.124074799=Read"},
{"title": "UINav: A Practical Approach to Train On-Device Automation Agents", "author": "Wei Li and Fu-Lin Hsu and Will Bishop and Folawiyo Campbell-Ajala and Max Lin and Oriana Riva", "abstract": "  Automation systems that can autonomously drive application user interfaces to\ncomplete user tasks are of great benefit, especially when users are\nsituationally or permanently impaired. Prior automation systems do not produce\ngeneralizable models while AI-based automation agents work reliably only in\nsimple, hand-crafted applications or incur high computation costs. We propose\nUINav, a demonstration-based approach to train automation agents that fit\nmobile devices, yet achieving high success rates with modest numbers of\ndemonstrations. To reduce the demonstration overhead, UINav uses a referee\nmodel that provides users with immediate feedback on tasks where the agent\nfails, and automatically augments human demonstrations to increase diversity in\ntraining data. Our evaluation shows that with only 10 demonstrations UINav can\nachieve 70% accuracy, and that with enough demonstrations it can surpass 90%\naccuracy.\n", "link": "http://arxiv.org/abs/2312.10170v4", "date": "2024-06-28", "relevancy": 2.6504, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5553}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5274}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UINav%3A%20A%20Practical%20Approach%20to%20Train%20On-Device%20Automation%20Agents&body=Title%3A%20UINav%3A%20A%20Practical%20Approach%20to%20Train%20On-Device%20Automation%20Agents%0AAuthor%3A%20Wei%20Li%20and%20Fu-Lin%20Hsu%20and%20Will%20Bishop%20and%20Folawiyo%20Campbell-Ajala%20and%20Max%20Lin%20and%20Oriana%20Riva%0AAbstract%3A%20%20%20Automation%20systems%20that%20can%20autonomously%20drive%20application%20user%20interfaces%20to%0Acomplete%20user%20tasks%20are%20of%20great%20benefit%2C%20especially%20when%20users%20are%0Asituationally%20or%20permanently%20impaired.%20Prior%20automation%20systems%20do%20not%20produce%0Ageneralizable%20models%20while%20AI-based%20automation%20agents%20work%20reliably%20only%20in%0Asimple%2C%20hand-crafted%20applications%20or%20incur%20high%20computation%20costs.%20We%20propose%0AUINav%2C%20a%20demonstration-based%20approach%20to%20train%20automation%20agents%20that%20fit%0Amobile%20devices%2C%20yet%20achieving%20high%20success%20rates%20with%20modest%20numbers%20of%0Ademonstrations.%20To%20reduce%20the%20demonstration%20overhead%2C%20UINav%20uses%20a%20referee%0Amodel%20that%20provides%20users%20with%20immediate%20feedback%20on%20tasks%20where%20the%20agent%0Afails%2C%20and%20automatically%20augments%20human%20demonstrations%20to%20increase%20diversity%20in%0Atraining%20data.%20Our%20evaluation%20shows%20that%20with%20only%2010%20demonstrations%20UINav%20can%0Aachieve%2070%25%20accuracy%2C%20and%20that%20with%20enough%20demonstrations%20it%20can%20surpass%2090%25%0Aaccuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.10170v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUINav%253A%2520A%2520Practical%2520Approach%2520to%2520Train%2520On-Device%2520Automation%2520Agents%26entry.906535625%3DWei%2520Li%2520and%2520Fu-Lin%2520Hsu%2520and%2520Will%2520Bishop%2520and%2520Folawiyo%2520Campbell-Ajala%2520and%2520Max%2520Lin%2520and%2520Oriana%2520Riva%26entry.1292438233%3D%2520%2520Automation%2520systems%2520that%2520can%2520autonomously%2520drive%2520application%2520user%2520interfaces%2520to%250Acomplete%2520user%2520tasks%2520are%2520of%2520great%2520benefit%252C%2520especially%2520when%2520users%2520are%250Asituationally%2520or%2520permanently%2520impaired.%2520Prior%2520automation%2520systems%2520do%2520not%2520produce%250Ageneralizable%2520models%2520while%2520AI-based%2520automation%2520agents%2520work%2520reliably%2520only%2520in%250Asimple%252C%2520hand-crafted%2520applications%2520or%2520incur%2520high%2520computation%2520costs.%2520We%2520propose%250AUINav%252C%2520a%2520demonstration-based%2520approach%2520to%2520train%2520automation%2520agents%2520that%2520fit%250Amobile%2520devices%252C%2520yet%2520achieving%2520high%2520success%2520rates%2520with%2520modest%2520numbers%2520of%250Ademonstrations.%2520To%2520reduce%2520the%2520demonstration%2520overhead%252C%2520UINav%2520uses%2520a%2520referee%250Amodel%2520that%2520provides%2520users%2520with%2520immediate%2520feedback%2520on%2520tasks%2520where%2520the%2520agent%250Afails%252C%2520and%2520automatically%2520augments%2520human%2520demonstrations%2520to%2520increase%2520diversity%2520in%250Atraining%2520data.%2520Our%2520evaluation%2520shows%2520that%2520with%2520only%252010%2520demonstrations%2520UINav%2520can%250Aachieve%252070%2525%2520accuracy%252C%2520and%2520that%2520with%2520enough%2520demonstrations%2520it%2520can%2520surpass%252090%2525%250Aaccuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.10170v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UINav%3A%20A%20Practical%20Approach%20to%20Train%20On-Device%20Automation%20Agents&entry.906535625=Wei%20Li%20and%20Fu-Lin%20Hsu%20and%20Will%20Bishop%20and%20Folawiyo%20Campbell-Ajala%20and%20Max%20Lin%20and%20Oriana%20Riva&entry.1292438233=%20%20Automation%20systems%20that%20can%20autonomously%20drive%20application%20user%20interfaces%20to%0Acomplete%20user%20tasks%20are%20of%20great%20benefit%2C%20especially%20when%20users%20are%0Asituationally%20or%20permanently%20impaired.%20Prior%20automation%20systems%20do%20not%20produce%0Ageneralizable%20models%20while%20AI-based%20automation%20agents%20work%20reliably%20only%20in%0Asimple%2C%20hand-crafted%20applications%20or%20incur%20high%20computation%20costs.%20We%20propose%0AUINav%2C%20a%20demonstration-based%20approach%20to%20train%20automation%20agents%20that%20fit%0Amobile%20devices%2C%20yet%20achieving%20high%20success%20rates%20with%20modest%20numbers%20of%0Ademonstrations.%20To%20reduce%20the%20demonstration%20overhead%2C%20UINav%20uses%20a%20referee%0Amodel%20that%20provides%20users%20with%20immediate%20feedback%20on%20tasks%20where%20the%20agent%0Afails%2C%20and%20automatically%20augments%20human%20demonstrations%20to%20increase%20diversity%20in%0Atraining%20data.%20Our%20evaluation%20shows%20that%20with%20only%2010%20demonstrations%20UINav%20can%0Aachieve%2070%25%20accuracy%2C%20and%20that%20with%20enough%20demonstrations%20it%20can%20surpass%2090%25%0Aaccuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.10170v4&entry.124074799=Read"},
{"title": "HouseCrafter: Lifting Floorplans to 3D Scenes with 2D Diffusion Model", "author": "Hieu T. Nguyen and Yiwen Chen and Vikram Voleti and Varun Jampani and Huaizu Jiang", "abstract": "  We introduce HouseCrafter, a novel approach that can lift a floorplan into a\ncomplete large 3D indoor scene (e.g., a house). Our key insight is to adapt a\n2D diffusion model, which is trained on web-scale images, to generate\nconsistent multi-view color (RGB) and depth (D) images across different\nlocations of the scene. Specifically, the RGB-D images are generated\nautoregressively in a batch-wise manner along sampled locations based on the\nfloorplan, where previously generated images are used as condition to the\ndiffusion model to produce images at nearby locations. The global floorplan and\nattention design in the diffusion model ensures the consistency of the\ngenerated images, from which a 3D scene can be reconstructed. Through extensive\nevaluation on the 3D-Front dataset, we demonstrate that HouseCraft can generate\nhigh-quality house-scale 3D scenes. Ablation studies also validate the\neffectiveness of different design choices. We will release our code and model\nweights. Project page: https://neu-vi.github.io/houseCrafter/\n", "link": "http://arxiv.org/abs/2406.20077v1", "date": "2024-06-28", "relevancy": 2.6496, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6819}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6819}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5647}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HouseCrafter%3A%20Lifting%20Floorplans%20to%203D%20Scenes%20with%202D%20Diffusion%20Model&body=Title%3A%20HouseCrafter%3A%20Lifting%20Floorplans%20to%203D%20Scenes%20with%202D%20Diffusion%20Model%0AAuthor%3A%20Hieu%20T.%20Nguyen%20and%20Yiwen%20Chen%20and%20Vikram%20Voleti%20and%20Varun%20Jampani%20and%20Huaizu%20Jiang%0AAbstract%3A%20%20%20We%20introduce%20HouseCrafter%2C%20a%20novel%20approach%20that%20can%20lift%20a%20floorplan%20into%20a%0Acomplete%20large%203D%20indoor%20scene%20%28e.g.%2C%20a%20house%29.%20Our%20key%20insight%20is%20to%20adapt%20a%0A2D%20diffusion%20model%2C%20which%20is%20trained%20on%20web-scale%20images%2C%20to%20generate%0Aconsistent%20multi-view%20color%20%28RGB%29%20and%20depth%20%28D%29%20images%20across%20different%0Alocations%20of%20the%20scene.%20Specifically%2C%20the%20RGB-D%20images%20are%20generated%0Aautoregressively%20in%20a%20batch-wise%20manner%20along%20sampled%20locations%20based%20on%20the%0Afloorplan%2C%20where%20previously%20generated%20images%20are%20used%20as%20condition%20to%20the%0Adiffusion%20model%20to%20produce%20images%20at%20nearby%20locations.%20The%20global%20floorplan%20and%0Aattention%20design%20in%20the%20diffusion%20model%20ensures%20the%20consistency%20of%20the%0Agenerated%20images%2C%20from%20which%20a%203D%20scene%20can%20be%20reconstructed.%20Through%20extensive%0Aevaluation%20on%20the%203D-Front%20dataset%2C%20we%20demonstrate%20that%20HouseCraft%20can%20generate%0Ahigh-quality%20house-scale%203D%20scenes.%20Ablation%20studies%20also%20validate%20the%0Aeffectiveness%20of%20different%20design%20choices.%20We%20will%20release%20our%20code%20and%20model%0Aweights.%20Project%20page%3A%20https%3A//neu-vi.github.io/houseCrafter/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.20077v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHouseCrafter%253A%2520Lifting%2520Floorplans%2520to%25203D%2520Scenes%2520with%25202D%2520Diffusion%2520Model%26entry.906535625%3DHieu%2520T.%2520Nguyen%2520and%2520Yiwen%2520Chen%2520and%2520Vikram%2520Voleti%2520and%2520Varun%2520Jampani%2520and%2520Huaizu%2520Jiang%26entry.1292438233%3D%2520%2520We%2520introduce%2520HouseCrafter%252C%2520a%2520novel%2520approach%2520that%2520can%2520lift%2520a%2520floorplan%2520into%2520a%250Acomplete%2520large%25203D%2520indoor%2520scene%2520%2528e.g.%252C%2520a%2520house%2529.%2520Our%2520key%2520insight%2520is%2520to%2520adapt%2520a%250A2D%2520diffusion%2520model%252C%2520which%2520is%2520trained%2520on%2520web-scale%2520images%252C%2520to%2520generate%250Aconsistent%2520multi-view%2520color%2520%2528RGB%2529%2520and%2520depth%2520%2528D%2529%2520images%2520across%2520different%250Alocations%2520of%2520the%2520scene.%2520Specifically%252C%2520the%2520RGB-D%2520images%2520are%2520generated%250Aautoregressively%2520in%2520a%2520batch-wise%2520manner%2520along%2520sampled%2520locations%2520based%2520on%2520the%250Afloorplan%252C%2520where%2520previously%2520generated%2520images%2520are%2520used%2520as%2520condition%2520to%2520the%250Adiffusion%2520model%2520to%2520produce%2520images%2520at%2520nearby%2520locations.%2520The%2520global%2520floorplan%2520and%250Aattention%2520design%2520in%2520the%2520diffusion%2520model%2520ensures%2520the%2520consistency%2520of%2520the%250Agenerated%2520images%252C%2520from%2520which%2520a%25203D%2520scene%2520can%2520be%2520reconstructed.%2520Through%2520extensive%250Aevaluation%2520on%2520the%25203D-Front%2520dataset%252C%2520we%2520demonstrate%2520that%2520HouseCraft%2520can%2520generate%250Ahigh-quality%2520house-scale%25203D%2520scenes.%2520Ablation%2520studies%2520also%2520validate%2520the%250Aeffectiveness%2520of%2520different%2520design%2520choices.%2520We%2520will%2520release%2520our%2520code%2520and%2520model%250Aweights.%2520Project%2520page%253A%2520https%253A//neu-vi.github.io/houseCrafter/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.20077v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HouseCrafter%3A%20Lifting%20Floorplans%20to%203D%20Scenes%20with%202D%20Diffusion%20Model&entry.906535625=Hieu%20T.%20Nguyen%20and%20Yiwen%20Chen%20and%20Vikram%20Voleti%20and%20Varun%20Jampani%20and%20Huaizu%20Jiang&entry.1292438233=%20%20We%20introduce%20HouseCrafter%2C%20a%20novel%20approach%20that%20can%20lift%20a%20floorplan%20into%20a%0Acomplete%20large%203D%20indoor%20scene%20%28e.g.%2C%20a%20house%29.%20Our%20key%20insight%20is%20to%20adapt%20a%0A2D%20diffusion%20model%2C%20which%20is%20trained%20on%20web-scale%20images%2C%20to%20generate%0Aconsistent%20multi-view%20color%20%28RGB%29%20and%20depth%20%28D%29%20images%20across%20different%0Alocations%20of%20the%20scene.%20Specifically%2C%20the%20RGB-D%20images%20are%20generated%0Aautoregressively%20in%20a%20batch-wise%20manner%20along%20sampled%20locations%20based%20on%20the%0Afloorplan%2C%20where%20previously%20generated%20images%20are%20used%20as%20condition%20to%20the%0Adiffusion%20model%20to%20produce%20images%20at%20nearby%20locations.%20The%20global%20floorplan%20and%0Aattention%20design%20in%20the%20diffusion%20model%20ensures%20the%20consistency%20of%20the%0Agenerated%20images%2C%20from%20which%20a%203D%20scene%20can%20be%20reconstructed.%20Through%20extensive%0Aevaluation%20on%20the%203D-Front%20dataset%2C%20we%20demonstrate%20that%20HouseCraft%20can%20generate%0Ahigh-quality%20house-scale%203D%20scenes.%20Ablation%20studies%20also%20validate%20the%0Aeffectiveness%20of%20different%20design%20choices.%20We%20will%20release%20our%20code%20and%20model%0Aweights.%20Project%20page%3A%20https%3A//neu-vi.github.io/houseCrafter/%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.20077v1&entry.124074799=Read"},
{"title": "Generative AI-Driven Human Digital Twin in IoT-Healthcare: A\n  Comprehensive Survey", "author": "Jiayuan Chen and You Shi and Changyan Yi and Hongyang Du and Jiawen Kang and Dusit Niyato", "abstract": "  The Internet of things (IoT) can significantly enhance the quality of human\nlife, specifically in healthcare, attracting extensive attentions to\nIoT-healthcare services. Meanwhile, the human digital twin (HDT) is proposed as\nan innovative paradigm that can comprehensively characterize the replication of\nthe individual human body in the digital world and reflect its physical status\nin real time. Naturally, HDT is envisioned to empower IoT-healthcare beyond the\napplication of healthcare monitoring by acting as a versatile and vivid human\ndigital testbed, simulating the outcomes and guiding the practical treatments.\nHowever, successfully establishing HDT requires high-fidelity virtual modeling\nand strong information interactions but possibly with scarce, biased and noisy\ndata. Fortunately, a recent popular technology called generative artificial\nintelligence (GAI) may be a promising solution because it can leverage advanced\nAI algorithms to automatically create, manipulate, and modify valuable while\ndiverse data. This survey particularly focuses on the implementation of\nGAI-driven HDT in IoT-healthcare. We start by introducing the background of\nIoT-healthcare and the potential of GAI-driven HDT. Then, we delve into the\nfundamental techniques and present the overall framework of GAI-driven HDT.\nAfter that, we explore the realization of GAI-driven HDT in detail, including\nGAI-enabled data acquisition, communication, data management, digital modeling,\nand data analysis. Besides, we discuss typical IoT-healthcare applications that\ncan be revolutionized by GAI-driven HDT, namely personalized health monitoring\nand diagnosis, personalized prescription, and personalized rehabilitation.\nFinally, we conclude this survey by highlighting some future research\ndirections.\n", "link": "http://arxiv.org/abs/2401.13699v2", "date": "2024-06-28", "relevancy": 2.6219, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5575}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5119}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5038}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20AI-Driven%20Human%20Digital%20Twin%20in%20IoT-Healthcare%3A%20A%0A%20%20Comprehensive%20Survey&body=Title%3A%20Generative%20AI-Driven%20Human%20Digital%20Twin%20in%20IoT-Healthcare%3A%20A%0A%20%20Comprehensive%20Survey%0AAuthor%3A%20Jiayuan%20Chen%20and%20You%20Shi%20and%20Changyan%20Yi%20and%20Hongyang%20Du%20and%20Jiawen%20Kang%20and%20Dusit%20Niyato%0AAbstract%3A%20%20%20The%20Internet%20of%20things%20%28IoT%29%20can%20significantly%20enhance%20the%20quality%20of%20human%0Alife%2C%20specifically%20in%20healthcare%2C%20attracting%20extensive%20attentions%20to%0AIoT-healthcare%20services.%20Meanwhile%2C%20the%20human%20digital%20twin%20%28HDT%29%20is%20proposed%20as%0Aan%20innovative%20paradigm%20that%20can%20comprehensively%20characterize%20the%20replication%20of%0Athe%20individual%20human%20body%20in%20the%20digital%20world%20and%20reflect%20its%20physical%20status%0Ain%20real%20time.%20Naturally%2C%20HDT%20is%20envisioned%20to%20empower%20IoT-healthcare%20beyond%20the%0Aapplication%20of%20healthcare%20monitoring%20by%20acting%20as%20a%20versatile%20and%20vivid%20human%0Adigital%20testbed%2C%20simulating%20the%20outcomes%20and%20guiding%20the%20practical%20treatments.%0AHowever%2C%20successfully%20establishing%20HDT%20requires%20high-fidelity%20virtual%20modeling%0Aand%20strong%20information%20interactions%20but%20possibly%20with%20scarce%2C%20biased%20and%20noisy%0Adata.%20Fortunately%2C%20a%20recent%20popular%20technology%20called%20generative%20artificial%0Aintelligence%20%28GAI%29%20may%20be%20a%20promising%20solution%20because%20it%20can%20leverage%20advanced%0AAI%20algorithms%20to%20automatically%20create%2C%20manipulate%2C%20and%20modify%20valuable%20while%0Adiverse%20data.%20This%20survey%20particularly%20focuses%20on%20the%20implementation%20of%0AGAI-driven%20HDT%20in%20IoT-healthcare.%20We%20start%20by%20introducing%20the%20background%20of%0AIoT-healthcare%20and%20the%20potential%20of%20GAI-driven%20HDT.%20Then%2C%20we%20delve%20into%20the%0Afundamental%20techniques%20and%20present%20the%20overall%20framework%20of%20GAI-driven%20HDT.%0AAfter%20that%2C%20we%20explore%20the%20realization%20of%20GAI-driven%20HDT%20in%20detail%2C%20including%0AGAI-enabled%20data%20acquisition%2C%20communication%2C%20data%20management%2C%20digital%20modeling%2C%0Aand%20data%20analysis.%20Besides%2C%20we%20discuss%20typical%20IoT-healthcare%20applications%20that%0Acan%20be%20revolutionized%20by%20GAI-driven%20HDT%2C%20namely%20personalized%20health%20monitoring%0Aand%20diagnosis%2C%20personalized%20prescription%2C%20and%20personalized%20rehabilitation.%0AFinally%2C%20we%20conclude%20this%20survey%20by%20highlighting%20some%20future%20research%0Adirections.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.13699v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520AI-Driven%2520Human%2520Digital%2520Twin%2520in%2520IoT-Healthcare%253A%2520A%250A%2520%2520Comprehensive%2520Survey%26entry.906535625%3DJiayuan%2520Chen%2520and%2520You%2520Shi%2520and%2520Changyan%2520Yi%2520and%2520Hongyang%2520Du%2520and%2520Jiawen%2520Kang%2520and%2520Dusit%2520Niyato%26entry.1292438233%3D%2520%2520The%2520Internet%2520of%2520things%2520%2528IoT%2529%2520can%2520significantly%2520enhance%2520the%2520quality%2520of%2520human%250Alife%252C%2520specifically%2520in%2520healthcare%252C%2520attracting%2520extensive%2520attentions%2520to%250AIoT-healthcare%2520services.%2520Meanwhile%252C%2520the%2520human%2520digital%2520twin%2520%2528HDT%2529%2520is%2520proposed%2520as%250Aan%2520innovative%2520paradigm%2520that%2520can%2520comprehensively%2520characterize%2520the%2520replication%2520of%250Athe%2520individual%2520human%2520body%2520in%2520the%2520digital%2520world%2520and%2520reflect%2520its%2520physical%2520status%250Ain%2520real%2520time.%2520Naturally%252C%2520HDT%2520is%2520envisioned%2520to%2520empower%2520IoT-healthcare%2520beyond%2520the%250Aapplication%2520of%2520healthcare%2520monitoring%2520by%2520acting%2520as%2520a%2520versatile%2520and%2520vivid%2520human%250Adigital%2520testbed%252C%2520simulating%2520the%2520outcomes%2520and%2520guiding%2520the%2520practical%2520treatments.%250AHowever%252C%2520successfully%2520establishing%2520HDT%2520requires%2520high-fidelity%2520virtual%2520modeling%250Aand%2520strong%2520information%2520interactions%2520but%2520possibly%2520with%2520scarce%252C%2520biased%2520and%2520noisy%250Adata.%2520Fortunately%252C%2520a%2520recent%2520popular%2520technology%2520called%2520generative%2520artificial%250Aintelligence%2520%2528GAI%2529%2520may%2520be%2520a%2520promising%2520solution%2520because%2520it%2520can%2520leverage%2520advanced%250AAI%2520algorithms%2520to%2520automatically%2520create%252C%2520manipulate%252C%2520and%2520modify%2520valuable%2520while%250Adiverse%2520data.%2520This%2520survey%2520particularly%2520focuses%2520on%2520the%2520implementation%2520of%250AGAI-driven%2520HDT%2520in%2520IoT-healthcare.%2520We%2520start%2520by%2520introducing%2520the%2520background%2520of%250AIoT-healthcare%2520and%2520the%2520potential%2520of%2520GAI-driven%2520HDT.%2520Then%252C%2520we%2520delve%2520into%2520the%250Afundamental%2520techniques%2520and%2520present%2520the%2520overall%2520framework%2520of%2520GAI-driven%2520HDT.%250AAfter%2520that%252C%2520we%2520explore%2520the%2520realization%2520of%2520GAI-driven%2520HDT%2520in%2520detail%252C%2520including%250AGAI-enabled%2520data%2520acquisition%252C%2520communication%252C%2520data%2520management%252C%2520digital%2520modeling%252C%250Aand%2520data%2520analysis.%2520Besides%252C%2520we%2520discuss%2520typical%2520IoT-healthcare%2520applications%2520that%250Acan%2520be%2520revolutionized%2520by%2520GAI-driven%2520HDT%252C%2520namely%2520personalized%2520health%2520monitoring%250Aand%2520diagnosis%252C%2520personalized%2520prescription%252C%2520and%2520personalized%2520rehabilitation.%250AFinally%252C%2520we%2520conclude%2520this%2520survey%2520by%2520highlighting%2520some%2520future%2520research%250Adirections.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.13699v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20AI-Driven%20Human%20Digital%20Twin%20in%20IoT-Healthcare%3A%20A%0A%20%20Comprehensive%20Survey&entry.906535625=Jiayuan%20Chen%20and%20You%20Shi%20and%20Changyan%20Yi%20and%20Hongyang%20Du%20and%20Jiawen%20Kang%20and%20Dusit%20Niyato&entry.1292438233=%20%20The%20Internet%20of%20things%20%28IoT%29%20can%20significantly%20enhance%20the%20quality%20of%20human%0Alife%2C%20specifically%20in%20healthcare%2C%20attracting%20extensive%20attentions%20to%0AIoT-healthcare%20services.%20Meanwhile%2C%20the%20human%20digital%20twin%20%28HDT%29%20is%20proposed%20as%0Aan%20innovative%20paradigm%20that%20can%20comprehensively%20characterize%20the%20replication%20of%0Athe%20individual%20human%20body%20in%20the%20digital%20world%20and%20reflect%20its%20physical%20status%0Ain%20real%20time.%20Naturally%2C%20HDT%20is%20envisioned%20to%20empower%20IoT-healthcare%20beyond%20the%0Aapplication%20of%20healthcare%20monitoring%20by%20acting%20as%20a%20versatile%20and%20vivid%20human%0Adigital%20testbed%2C%20simulating%20the%20outcomes%20and%20guiding%20the%20practical%20treatments.%0AHowever%2C%20successfully%20establishing%20HDT%20requires%20high-fidelity%20virtual%20modeling%0Aand%20strong%20information%20interactions%20but%20possibly%20with%20scarce%2C%20biased%20and%20noisy%0Adata.%20Fortunately%2C%20a%20recent%20popular%20technology%20called%20generative%20artificial%0Aintelligence%20%28GAI%29%20may%20be%20a%20promising%20solution%20because%20it%20can%20leverage%20advanced%0AAI%20algorithms%20to%20automatically%20create%2C%20manipulate%2C%20and%20modify%20valuable%20while%0Adiverse%20data.%20This%20survey%20particularly%20focuses%20on%20the%20implementation%20of%0AGAI-driven%20HDT%20in%20IoT-healthcare.%20We%20start%20by%20introducing%20the%20background%20of%0AIoT-healthcare%20and%20the%20potential%20of%20GAI-driven%20HDT.%20Then%2C%20we%20delve%20into%20the%0Afundamental%20techniques%20and%20present%20the%20overall%20framework%20of%20GAI-driven%20HDT.%0AAfter%20that%2C%20we%20explore%20the%20realization%20of%20GAI-driven%20HDT%20in%20detail%2C%20including%0AGAI-enabled%20data%20acquisition%2C%20communication%2C%20data%20management%2C%20digital%20modeling%2C%0Aand%20data%20analysis.%20Besides%2C%20we%20discuss%20typical%20IoT-healthcare%20applications%20that%0Acan%20be%20revolutionized%20by%20GAI-driven%20HDT%2C%20namely%20personalized%20health%20monitoring%0Aand%20diagnosis%2C%20personalized%20prescription%2C%20and%20personalized%20rehabilitation.%0AFinally%2C%20we%20conclude%20this%20survey%20by%20highlighting%20some%20future%20research%0Adirections.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.13699v2&entry.124074799=Read"},
{"title": "Parallax-tolerant Image Stitching via Segmentation-guided\n  Multi-homography Warping", "author": "Tianli Liao and Ce Wang and Lei Li and Guangen Liu and Nan Li", "abstract": "  Large parallax between images is an intractable issue in image stitching.\nVarious warping-based methods are proposed to address it, yet the results are\nunsatisfactory. In this paper, we propose a novel image stitching method using\nmulti-homography warping guided by image segmentation. Specifically, we\nleverage the Segment Anything Model to segment the target image into numerous\ncontents and partition the feature points into multiple subsets via the\nenergy-based multi-homography fitting algorithm. The multiple subsets of\nfeature points are used to calculate the corresponding multiple homographies.\nFor each segmented content in the overlapping region, we select its\nbest-fitting homography with the lowest photometric error. For each segmented\ncontent in the non-overlapping region, we calculate a weighted combination of\nthe linearized homographies. Finally, the target image is warped via the\nbest-fitting homographies to align with the reference image, and the final\npanorama is generated via linear blending. Comprehensive experimental results\non the public datasets demonstrate that our method provides the best alignment\naccuracy by a large margin, compared with the state-of-the-art methods. The\nsource code is available at https://github.com/tlliao/multi-homo-warp.\n", "link": "http://arxiv.org/abs/2406.19922v1", "date": "2024-06-28", "relevancy": 2.6196, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5339}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5261}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parallax-tolerant%20Image%20Stitching%20via%20Segmentation-guided%0A%20%20Multi-homography%20Warping&body=Title%3A%20Parallax-tolerant%20Image%20Stitching%20via%20Segmentation-guided%0A%20%20Multi-homography%20Warping%0AAuthor%3A%20Tianli%20Liao%20and%20Ce%20Wang%20and%20Lei%20Li%20and%20Guangen%20Liu%20and%20Nan%20Li%0AAbstract%3A%20%20%20Large%20parallax%20between%20images%20is%20an%20intractable%20issue%20in%20image%20stitching.%0AVarious%20warping-based%20methods%20are%20proposed%20to%20address%20it%2C%20yet%20the%20results%20are%0Aunsatisfactory.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20image%20stitching%20method%20using%0Amulti-homography%20warping%20guided%20by%20image%20segmentation.%20Specifically%2C%20we%0Aleverage%20the%20Segment%20Anything%20Model%20to%20segment%20the%20target%20image%20into%20numerous%0Acontents%20and%20partition%20the%20feature%20points%20into%20multiple%20subsets%20via%20the%0Aenergy-based%20multi-homography%20fitting%20algorithm.%20The%20multiple%20subsets%20of%0Afeature%20points%20are%20used%20to%20calculate%20the%20corresponding%20multiple%20homographies.%0AFor%20each%20segmented%20content%20in%20the%20overlapping%20region%2C%20we%20select%20its%0Abest-fitting%20homography%20with%20the%20lowest%20photometric%20error.%20For%20each%20segmented%0Acontent%20in%20the%20non-overlapping%20region%2C%20we%20calculate%20a%20weighted%20combination%20of%0Athe%20linearized%20homographies.%20Finally%2C%20the%20target%20image%20is%20warped%20via%20the%0Abest-fitting%20homographies%20to%20align%20with%20the%20reference%20image%2C%20and%20the%20final%0Apanorama%20is%20generated%20via%20linear%20blending.%20Comprehensive%20experimental%20results%0Aon%20the%20public%20datasets%20demonstrate%20that%20our%20method%20provides%20the%20best%20alignment%0Aaccuracy%20by%20a%20large%20margin%2C%20compared%20with%20the%20state-of-the-art%20methods.%20The%0Asource%20code%20is%20available%20at%20https%3A//github.com/tlliao/multi-homo-warp.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19922v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParallax-tolerant%2520Image%2520Stitching%2520via%2520Segmentation-guided%250A%2520%2520Multi-homography%2520Warping%26entry.906535625%3DTianli%2520Liao%2520and%2520Ce%2520Wang%2520and%2520Lei%2520Li%2520and%2520Guangen%2520Liu%2520and%2520Nan%2520Li%26entry.1292438233%3D%2520%2520Large%2520parallax%2520between%2520images%2520is%2520an%2520intractable%2520issue%2520in%2520image%2520stitching.%250AVarious%2520warping-based%2520methods%2520are%2520proposed%2520to%2520address%2520it%252C%2520yet%2520the%2520results%2520are%250Aunsatisfactory.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520image%2520stitching%2520method%2520using%250Amulti-homography%2520warping%2520guided%2520by%2520image%2520segmentation.%2520Specifically%252C%2520we%250Aleverage%2520the%2520Segment%2520Anything%2520Model%2520to%2520segment%2520the%2520target%2520image%2520into%2520numerous%250Acontents%2520and%2520partition%2520the%2520feature%2520points%2520into%2520multiple%2520subsets%2520via%2520the%250Aenergy-based%2520multi-homography%2520fitting%2520algorithm.%2520The%2520multiple%2520subsets%2520of%250Afeature%2520points%2520are%2520used%2520to%2520calculate%2520the%2520corresponding%2520multiple%2520homographies.%250AFor%2520each%2520segmented%2520content%2520in%2520the%2520overlapping%2520region%252C%2520we%2520select%2520its%250Abest-fitting%2520homography%2520with%2520the%2520lowest%2520photometric%2520error.%2520For%2520each%2520segmented%250Acontent%2520in%2520the%2520non-overlapping%2520region%252C%2520we%2520calculate%2520a%2520weighted%2520combination%2520of%250Athe%2520linearized%2520homographies.%2520Finally%252C%2520the%2520target%2520image%2520is%2520warped%2520via%2520the%250Abest-fitting%2520homographies%2520to%2520align%2520with%2520the%2520reference%2520image%252C%2520and%2520the%2520final%250Apanorama%2520is%2520generated%2520via%2520linear%2520blending.%2520Comprehensive%2520experimental%2520results%250Aon%2520the%2520public%2520datasets%2520demonstrate%2520that%2520our%2520method%2520provides%2520the%2520best%2520alignment%250Aaccuracy%2520by%2520a%2520large%2520margin%252C%2520compared%2520with%2520the%2520state-of-the-art%2520methods.%2520The%250Asource%2520code%2520is%2520available%2520at%2520https%253A//github.com/tlliao/multi-homo-warp.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19922v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parallax-tolerant%20Image%20Stitching%20via%20Segmentation-guided%0A%20%20Multi-homography%20Warping&entry.906535625=Tianli%20Liao%20and%20Ce%20Wang%20and%20Lei%20Li%20and%20Guangen%20Liu%20and%20Nan%20Li&entry.1292438233=%20%20Large%20parallax%20between%20images%20is%20an%20intractable%20issue%20in%20image%20stitching.%0AVarious%20warping-based%20methods%20are%20proposed%20to%20address%20it%2C%20yet%20the%20results%20are%0Aunsatisfactory.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20image%20stitching%20method%20using%0Amulti-homography%20warping%20guided%20by%20image%20segmentation.%20Specifically%2C%20we%0Aleverage%20the%20Segment%20Anything%20Model%20to%20segment%20the%20target%20image%20into%20numerous%0Acontents%20and%20partition%20the%20feature%20points%20into%20multiple%20subsets%20via%20the%0Aenergy-based%20multi-homography%20fitting%20algorithm.%20The%20multiple%20subsets%20of%0Afeature%20points%20are%20used%20to%20calculate%20the%20corresponding%20multiple%20homographies.%0AFor%20each%20segmented%20content%20in%20the%20overlapping%20region%2C%20we%20select%20its%0Abest-fitting%20homography%20with%20the%20lowest%20photometric%20error.%20For%20each%20segmented%0Acontent%20in%20the%20non-overlapping%20region%2C%20we%20calculate%20a%20weighted%20combination%20of%0Athe%20linearized%20homographies.%20Finally%2C%20the%20target%20image%20is%20warped%20via%20the%0Abest-fitting%20homographies%20to%20align%20with%20the%20reference%20image%2C%20and%20the%20final%0Apanorama%20is%20generated%20via%20linear%20blending.%20Comprehensive%20experimental%20results%0Aon%20the%20public%20datasets%20demonstrate%20that%20our%20method%20provides%20the%20best%20alignment%0Aaccuracy%20by%20a%20large%20margin%2C%20compared%20with%20the%20state-of-the-art%20methods.%20The%0Asource%20code%20is%20available%20at%20https%3A//github.com/tlliao/multi-homo-warp.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19922v1&entry.124074799=Read"},
{"title": "HAITCH: A Framework for Distortion and Motion Correction in Fetal\n  Multi-Shell Diffusion-Weighted MRI", "author": "Haykel Snoussi and Davood Karimi and Onur Afacan and Mustafa Utkur and Ali Gholipour", "abstract": "  Diffusion magnetic resonance imaging (dMRI) is pivotal for probing the\nmicrostructure of the rapidly-developing fetal brain. However, fetal motion\nduring scans and its interaction with magnetic field inhomogeneities result in\nartifacts and data scattering across spatial and angular domains. The effects\nof those artifacts are more pronounced in high-angular resolution fetal dMRI,\nwhere signal-to-noise ratio is very low. Those effects lead to biased estimates\nand compromise the consistency and reliability of dMRI analysis. This work\npresents HAITCH, the first and the only publicly available tool to correct and\nreconstruct multi-shell high-angular resolution fetal dMRI data. HAITCH offers\nseveral technical advances that include a blip-reversed dual-echo acquisition\nfor dynamic distortion correction, advanced motion correction for model-free\nand robust reconstruction, optimized multi-shell design for enhanced\ninformation capture and increased tolerance to motion, and outlier detection\nfor improved reconstruction fidelity. The framework is open-source, flexible,\nand can be used to process any type of fetal dMRI data including single-echo or\nsingle-shell acquisitions, but is most effective when used with multi-shell\nmulti-echo fetal dMRI data that cannot be processed with any of the existing\ntools. Validation experiments on real fetal dMRI scans demonstrate significant\nimprovements and accurate correction across diverse fetal ages and motion\nlevels. HAITCH successfully removes artifacts and reconstructs high-fidelity\nfetal dMRI data suitable for advanced diffusion modeling, including fiber\norientation distribution function estimation. These advancements pave the way\nfor more reliable analysis of the fetal brain microstructure and tractography\nunder challenging imaging conditions.\n", "link": "http://arxiv.org/abs/2406.20042v1", "date": "2024-06-28", "relevancy": 2.6069, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5327}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5327}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4988}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HAITCH%3A%20A%20Framework%20for%20Distortion%20and%20Motion%20Correction%20in%20Fetal%0A%20%20Multi-Shell%20Diffusion-Weighted%20MRI&body=Title%3A%20HAITCH%3A%20A%20Framework%20for%20Distortion%20and%20Motion%20Correction%20in%20Fetal%0A%20%20Multi-Shell%20Diffusion-Weighted%20MRI%0AAuthor%3A%20Haykel%20Snoussi%20and%20Davood%20Karimi%20and%20Onur%20Afacan%20and%20Mustafa%20Utkur%20and%20Ali%20Gholipour%0AAbstract%3A%20%20%20Diffusion%20magnetic%20resonance%20imaging%20%28dMRI%29%20is%20pivotal%20for%20probing%20the%0Amicrostructure%20of%20the%20rapidly-developing%20fetal%20brain.%20However%2C%20fetal%20motion%0Aduring%20scans%20and%20its%20interaction%20with%20magnetic%20field%20inhomogeneities%20result%20in%0Aartifacts%20and%20data%20scattering%20across%20spatial%20and%20angular%20domains.%20The%20effects%0Aof%20those%20artifacts%20are%20more%20pronounced%20in%20high-angular%20resolution%20fetal%20dMRI%2C%0Awhere%20signal-to-noise%20ratio%20is%20very%20low.%20Those%20effects%20lead%20to%20biased%20estimates%0Aand%20compromise%20the%20consistency%20and%20reliability%20of%20dMRI%20analysis.%20This%20work%0Apresents%20HAITCH%2C%20the%20first%20and%20the%20only%20publicly%20available%20tool%20to%20correct%20and%0Areconstruct%20multi-shell%20high-angular%20resolution%20fetal%20dMRI%20data.%20HAITCH%20offers%0Aseveral%20technical%20advances%20that%20include%20a%20blip-reversed%20dual-echo%20acquisition%0Afor%20dynamic%20distortion%20correction%2C%20advanced%20motion%20correction%20for%20model-free%0Aand%20robust%20reconstruction%2C%20optimized%20multi-shell%20design%20for%20enhanced%0Ainformation%20capture%20and%20increased%20tolerance%20to%20motion%2C%20and%20outlier%20detection%0Afor%20improved%20reconstruction%20fidelity.%20The%20framework%20is%20open-source%2C%20flexible%2C%0Aand%20can%20be%20used%20to%20process%20any%20type%20of%20fetal%20dMRI%20data%20including%20single-echo%20or%0Asingle-shell%20acquisitions%2C%20but%20is%20most%20effective%20when%20used%20with%20multi-shell%0Amulti-echo%20fetal%20dMRI%20data%20that%20cannot%20be%20processed%20with%20any%20of%20the%20existing%0Atools.%20Validation%20experiments%20on%20real%20fetal%20dMRI%20scans%20demonstrate%20significant%0Aimprovements%20and%20accurate%20correction%20across%20diverse%20fetal%20ages%20and%20motion%0Alevels.%20HAITCH%20successfully%20removes%20artifacts%20and%20reconstructs%20high-fidelity%0Afetal%20dMRI%20data%20suitable%20for%20advanced%20diffusion%20modeling%2C%20including%20fiber%0Aorientation%20distribution%20function%20estimation.%20These%20advancements%20pave%20the%20way%0Afor%20more%20reliable%20analysis%20of%20the%20fetal%20brain%20microstructure%20and%20tractography%0Aunder%20challenging%20imaging%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.20042v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHAITCH%253A%2520A%2520Framework%2520for%2520Distortion%2520and%2520Motion%2520Correction%2520in%2520Fetal%250A%2520%2520Multi-Shell%2520Diffusion-Weighted%2520MRI%26entry.906535625%3DHaykel%2520Snoussi%2520and%2520Davood%2520Karimi%2520and%2520Onur%2520Afacan%2520and%2520Mustafa%2520Utkur%2520and%2520Ali%2520Gholipour%26entry.1292438233%3D%2520%2520Diffusion%2520magnetic%2520resonance%2520imaging%2520%2528dMRI%2529%2520is%2520pivotal%2520for%2520probing%2520the%250Amicrostructure%2520of%2520the%2520rapidly-developing%2520fetal%2520brain.%2520However%252C%2520fetal%2520motion%250Aduring%2520scans%2520and%2520its%2520interaction%2520with%2520magnetic%2520field%2520inhomogeneities%2520result%2520in%250Aartifacts%2520and%2520data%2520scattering%2520across%2520spatial%2520and%2520angular%2520domains.%2520The%2520effects%250Aof%2520those%2520artifacts%2520are%2520more%2520pronounced%2520in%2520high-angular%2520resolution%2520fetal%2520dMRI%252C%250Awhere%2520signal-to-noise%2520ratio%2520is%2520very%2520low.%2520Those%2520effects%2520lead%2520to%2520biased%2520estimates%250Aand%2520compromise%2520the%2520consistency%2520and%2520reliability%2520of%2520dMRI%2520analysis.%2520This%2520work%250Apresents%2520HAITCH%252C%2520the%2520first%2520and%2520the%2520only%2520publicly%2520available%2520tool%2520to%2520correct%2520and%250Areconstruct%2520multi-shell%2520high-angular%2520resolution%2520fetal%2520dMRI%2520data.%2520HAITCH%2520offers%250Aseveral%2520technical%2520advances%2520that%2520include%2520a%2520blip-reversed%2520dual-echo%2520acquisition%250Afor%2520dynamic%2520distortion%2520correction%252C%2520advanced%2520motion%2520correction%2520for%2520model-free%250Aand%2520robust%2520reconstruction%252C%2520optimized%2520multi-shell%2520design%2520for%2520enhanced%250Ainformation%2520capture%2520and%2520increased%2520tolerance%2520to%2520motion%252C%2520and%2520outlier%2520detection%250Afor%2520improved%2520reconstruction%2520fidelity.%2520The%2520framework%2520is%2520open-source%252C%2520flexible%252C%250Aand%2520can%2520be%2520used%2520to%2520process%2520any%2520type%2520of%2520fetal%2520dMRI%2520data%2520including%2520single-echo%2520or%250Asingle-shell%2520acquisitions%252C%2520but%2520is%2520most%2520effective%2520when%2520used%2520with%2520multi-shell%250Amulti-echo%2520fetal%2520dMRI%2520data%2520that%2520cannot%2520be%2520processed%2520with%2520any%2520of%2520the%2520existing%250Atools.%2520Validation%2520experiments%2520on%2520real%2520fetal%2520dMRI%2520scans%2520demonstrate%2520significant%250Aimprovements%2520and%2520accurate%2520correction%2520across%2520diverse%2520fetal%2520ages%2520and%2520motion%250Alevels.%2520HAITCH%2520successfully%2520removes%2520artifacts%2520and%2520reconstructs%2520high-fidelity%250Afetal%2520dMRI%2520data%2520suitable%2520for%2520advanced%2520diffusion%2520modeling%252C%2520including%2520fiber%250Aorientation%2520distribution%2520function%2520estimation.%2520These%2520advancements%2520pave%2520the%2520way%250Afor%2520more%2520reliable%2520analysis%2520of%2520the%2520fetal%2520brain%2520microstructure%2520and%2520tractography%250Aunder%2520challenging%2520imaging%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.20042v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HAITCH%3A%20A%20Framework%20for%20Distortion%20and%20Motion%20Correction%20in%20Fetal%0A%20%20Multi-Shell%20Diffusion-Weighted%20MRI&entry.906535625=Haykel%20Snoussi%20and%20Davood%20Karimi%20and%20Onur%20Afacan%20and%20Mustafa%20Utkur%20and%20Ali%20Gholipour&entry.1292438233=%20%20Diffusion%20magnetic%20resonance%20imaging%20%28dMRI%29%20is%20pivotal%20for%20probing%20the%0Amicrostructure%20of%20the%20rapidly-developing%20fetal%20brain.%20However%2C%20fetal%20motion%0Aduring%20scans%20and%20its%20interaction%20with%20magnetic%20field%20inhomogeneities%20result%20in%0Aartifacts%20and%20data%20scattering%20across%20spatial%20and%20angular%20domains.%20The%20effects%0Aof%20those%20artifacts%20are%20more%20pronounced%20in%20high-angular%20resolution%20fetal%20dMRI%2C%0Awhere%20signal-to-noise%20ratio%20is%20very%20low.%20Those%20effects%20lead%20to%20biased%20estimates%0Aand%20compromise%20the%20consistency%20and%20reliability%20of%20dMRI%20analysis.%20This%20work%0Apresents%20HAITCH%2C%20the%20first%20and%20the%20only%20publicly%20available%20tool%20to%20correct%20and%0Areconstruct%20multi-shell%20high-angular%20resolution%20fetal%20dMRI%20data.%20HAITCH%20offers%0Aseveral%20technical%20advances%20that%20include%20a%20blip-reversed%20dual-echo%20acquisition%0Afor%20dynamic%20distortion%20correction%2C%20advanced%20motion%20correction%20for%20model-free%0Aand%20robust%20reconstruction%2C%20optimized%20multi-shell%20design%20for%20enhanced%0Ainformation%20capture%20and%20increased%20tolerance%20to%20motion%2C%20and%20outlier%20detection%0Afor%20improved%20reconstruction%20fidelity.%20The%20framework%20is%20open-source%2C%20flexible%2C%0Aand%20can%20be%20used%20to%20process%20any%20type%20of%20fetal%20dMRI%20data%20including%20single-echo%20or%0Asingle-shell%20acquisitions%2C%20but%20is%20most%20effective%20when%20used%20with%20multi-shell%0Amulti-echo%20fetal%20dMRI%20data%20that%20cannot%20be%20processed%20with%20any%20of%20the%20existing%0Atools.%20Validation%20experiments%20on%20real%20fetal%20dMRI%20scans%20demonstrate%20significant%0Aimprovements%20and%20accurate%20correction%20across%20diverse%20fetal%20ages%20and%20motion%0Alevels.%20HAITCH%20successfully%20removes%20artifacts%20and%20reconstructs%20high-fidelity%0Afetal%20dMRI%20data%20suitable%20for%20advanced%20diffusion%20modeling%2C%20including%20fiber%0Aorientation%20distribution%20function%20estimation.%20These%20advancements%20pave%20the%20way%0Afor%20more%20reliable%20analysis%20of%20the%20fetal%20brain%20microstructure%20and%20tractography%0Aunder%20challenging%20imaging%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.20042v1&entry.124074799=Read"},
{"title": "Kolmogorov-Smirnov GAN", "author": "Maciej Falkiewicz and Naoya Takeishi and Alexandros Kalousis", "abstract": "  We propose a novel deep generative model, the Kolmogorov-Smirnov Generative\nAdversarial Network (KSGAN). Unlike existing approaches, KSGAN formulates the\nlearning process as a minimization of the Kolmogorov-Smirnov (KS) distance,\ngeneralized to handle multivariate distributions. This distance is calculated\nusing the quantile function, which acts as the critic in the adversarial\ntraining process. We formally demonstrate that minimizing the KS distance leads\nto the trained approximate distribution aligning with the target distribution.\nWe propose an efficient implementation and evaluate its effectiveness through\nexperiments. The results show that KSGAN performs on par with existing\nadversarial methods, exhibiting stability during training, resistance to mode\ndropping and collapse, and tolerance to variations in hyperparameter settings.\nAdditionally, we review the literature on the Generalized KS test and discuss\nthe connections between KSGAN and existing adversarial generative models.\n", "link": "http://arxiv.org/abs/2406.19948v1", "date": "2024-06-28", "relevancy": 2.5924, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5433}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5307}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4815}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kolmogorov-Smirnov%20GAN&body=Title%3A%20Kolmogorov-Smirnov%20GAN%0AAuthor%3A%20Maciej%20Falkiewicz%20and%20Naoya%20Takeishi%20and%20Alexandros%20Kalousis%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20deep%20generative%20model%2C%20the%20Kolmogorov-Smirnov%20Generative%0AAdversarial%20Network%20%28KSGAN%29.%20Unlike%20existing%20approaches%2C%20KSGAN%20formulates%20the%0Alearning%20process%20as%20a%20minimization%20of%20the%20Kolmogorov-Smirnov%20%28KS%29%20distance%2C%0Ageneralized%20to%20handle%20multivariate%20distributions.%20This%20distance%20is%20calculated%0Ausing%20the%20quantile%20function%2C%20which%20acts%20as%20the%20critic%20in%20the%20adversarial%0Atraining%20process.%20We%20formally%20demonstrate%20that%20minimizing%20the%20KS%20distance%20leads%0Ato%20the%20trained%20approximate%20distribution%20aligning%20with%20the%20target%20distribution.%0AWe%20propose%20an%20efficient%20implementation%20and%20evaluate%20its%20effectiveness%20through%0Aexperiments.%20The%20results%20show%20that%20KSGAN%20performs%20on%20par%20with%20existing%0Aadversarial%20methods%2C%20exhibiting%20stability%20during%20training%2C%20resistance%20to%20mode%0Adropping%20and%20collapse%2C%20and%20tolerance%20to%20variations%20in%20hyperparameter%20settings.%0AAdditionally%2C%20we%20review%20the%20literature%20on%20the%20Generalized%20KS%20test%20and%20discuss%0Athe%20connections%20between%20KSGAN%20and%20existing%20adversarial%20generative%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19948v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKolmogorov-Smirnov%2520GAN%26entry.906535625%3DMaciej%2520Falkiewicz%2520and%2520Naoya%2520Takeishi%2520and%2520Alexandros%2520Kalousis%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520deep%2520generative%2520model%252C%2520the%2520Kolmogorov-Smirnov%2520Generative%250AAdversarial%2520Network%2520%2528KSGAN%2529.%2520Unlike%2520existing%2520approaches%252C%2520KSGAN%2520formulates%2520the%250Alearning%2520process%2520as%2520a%2520minimization%2520of%2520the%2520Kolmogorov-Smirnov%2520%2528KS%2529%2520distance%252C%250Ageneralized%2520to%2520handle%2520multivariate%2520distributions.%2520This%2520distance%2520is%2520calculated%250Ausing%2520the%2520quantile%2520function%252C%2520which%2520acts%2520as%2520the%2520critic%2520in%2520the%2520adversarial%250Atraining%2520process.%2520We%2520formally%2520demonstrate%2520that%2520minimizing%2520the%2520KS%2520distance%2520leads%250Ato%2520the%2520trained%2520approximate%2520distribution%2520aligning%2520with%2520the%2520target%2520distribution.%250AWe%2520propose%2520an%2520efficient%2520implementation%2520and%2520evaluate%2520its%2520effectiveness%2520through%250Aexperiments.%2520The%2520results%2520show%2520that%2520KSGAN%2520performs%2520on%2520par%2520with%2520existing%250Aadversarial%2520methods%252C%2520exhibiting%2520stability%2520during%2520training%252C%2520resistance%2520to%2520mode%250Adropping%2520and%2520collapse%252C%2520and%2520tolerance%2520to%2520variations%2520in%2520hyperparameter%2520settings.%250AAdditionally%252C%2520we%2520review%2520the%2520literature%2520on%2520the%2520Generalized%2520KS%2520test%2520and%2520discuss%250Athe%2520connections%2520between%2520KSGAN%2520and%2520existing%2520adversarial%2520generative%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19948v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kolmogorov-Smirnov%20GAN&entry.906535625=Maciej%20Falkiewicz%20and%20Naoya%20Takeishi%20and%20Alexandros%20Kalousis&entry.1292438233=%20%20We%20propose%20a%20novel%20deep%20generative%20model%2C%20the%20Kolmogorov-Smirnov%20Generative%0AAdversarial%20Network%20%28KSGAN%29.%20Unlike%20existing%20approaches%2C%20KSGAN%20formulates%20the%0Alearning%20process%20as%20a%20minimization%20of%20the%20Kolmogorov-Smirnov%20%28KS%29%20distance%2C%0Ageneralized%20to%20handle%20multivariate%20distributions.%20This%20distance%20is%20calculated%0Ausing%20the%20quantile%20function%2C%20which%20acts%20as%20the%20critic%20in%20the%20adversarial%0Atraining%20process.%20We%20formally%20demonstrate%20that%20minimizing%20the%20KS%20distance%20leads%0Ato%20the%20trained%20approximate%20distribution%20aligning%20with%20the%20target%20distribution.%0AWe%20propose%20an%20efficient%20implementation%20and%20evaluate%20its%20effectiveness%20through%0Aexperiments.%20The%20results%20show%20that%20KSGAN%20performs%20on%20par%20with%20existing%0Aadversarial%20methods%2C%20exhibiting%20stability%20during%20training%2C%20resistance%20to%20mode%0Adropping%20and%20collapse%2C%20and%20tolerance%20to%20variations%20in%20hyperparameter%20settings.%0AAdditionally%2C%20we%20review%20the%20literature%20on%20the%20Generalized%20KS%20test%20and%20discuss%0Athe%20connections%20between%20KSGAN%20and%20existing%20adversarial%20generative%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19948v1&entry.124074799=Read"},
{"title": "Zero-Shot Reasoning: Personalized Content Generation Without the Cold\n  Start Problem", "author": "Davor Hafnar and Jure Dem\u0161ar", "abstract": "  Procedural content generation uses algorithmic techniques to create large\namounts of new content for games at much lower production costs. In newer\napproaches, procedural content generation utilizes machine learning. However,\nthese methods usually require expensive collection of large amounts of data, as\nwell as the development and training of fairly complex learning models, which\ncan be both extremely time-consuming and expensive. The core of our research is\nto explore whether we can lower the barrier to the use of personalized\nprocedural content generation through a more practical and generalizable\napproach with large language models. Matching game content with player\npreferences benefits both players, who enjoy the game more, and developers, who\nincreasingly depend on players enjoying the game before being able to monetize\nit. Therefore, this paper presents a novel approach to achieving\npersonalization by using large language models to propose levels based on the\ngameplay data continuously collected from individual players. We compared the\nlevels generated using our approach with levels generated with more traditional\nprocedural generation techniques. Our easily reproducible method has proven\nviable in a production setting and outperformed levels generated by traditional\nmethods in the probability that a player will not quit the game mid-level.\n", "link": "http://arxiv.org/abs/2402.10133v2", "date": "2024-06-28", "relevancy": 2.5855, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5331}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.51}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5081}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Reasoning%3A%20Personalized%20Content%20Generation%20Without%20the%20Cold%0A%20%20Start%20Problem&body=Title%3A%20Zero-Shot%20Reasoning%3A%20Personalized%20Content%20Generation%20Without%20the%20Cold%0A%20%20Start%20Problem%0AAuthor%3A%20Davor%20Hafnar%20and%20Jure%20Dem%C5%A1ar%0AAbstract%3A%20%20%20Procedural%20content%20generation%20uses%20algorithmic%20techniques%20to%20create%20large%0Aamounts%20of%20new%20content%20for%20games%20at%20much%20lower%20production%20costs.%20In%20newer%0Aapproaches%2C%20procedural%20content%20generation%20utilizes%20machine%20learning.%20However%2C%0Athese%20methods%20usually%20require%20expensive%20collection%20of%20large%20amounts%20of%20data%2C%20as%0Awell%20as%20the%20development%20and%20training%20of%20fairly%20complex%20learning%20models%2C%20which%0Acan%20be%20both%20extremely%20time-consuming%20and%20expensive.%20The%20core%20of%20our%20research%20is%0Ato%20explore%20whether%20we%20can%20lower%20the%20barrier%20to%20the%20use%20of%20personalized%0Aprocedural%20content%20generation%20through%20a%20more%20practical%20and%20generalizable%0Aapproach%20with%20large%20language%20models.%20Matching%20game%20content%20with%20player%0Apreferences%20benefits%20both%20players%2C%20who%20enjoy%20the%20game%20more%2C%20and%20developers%2C%20who%0Aincreasingly%20depend%20on%20players%20enjoying%20the%20game%20before%20being%20able%20to%20monetize%0Ait.%20Therefore%2C%20this%20paper%20presents%20a%20novel%20approach%20to%20achieving%0Apersonalization%20by%20using%20large%20language%20models%20to%20propose%20levels%20based%20on%20the%0Agameplay%20data%20continuously%20collected%20from%20individual%20players.%20We%20compared%20the%0Alevels%20generated%20using%20our%20approach%20with%20levels%20generated%20with%20more%20traditional%0Aprocedural%20generation%20techniques.%20Our%20easily%20reproducible%20method%20has%20proven%0Aviable%20in%20a%20production%20setting%20and%20outperformed%20levels%20generated%20by%20traditional%0Amethods%20in%20the%20probability%20that%20a%20player%20will%20not%20quit%20the%20game%20mid-level.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10133v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Reasoning%253A%2520Personalized%2520Content%2520Generation%2520Without%2520the%2520Cold%250A%2520%2520Start%2520Problem%26entry.906535625%3DDavor%2520Hafnar%2520and%2520Jure%2520Dem%25C5%25A1ar%26entry.1292438233%3D%2520%2520Procedural%2520content%2520generation%2520uses%2520algorithmic%2520techniques%2520to%2520create%2520large%250Aamounts%2520of%2520new%2520content%2520for%2520games%2520at%2520much%2520lower%2520production%2520costs.%2520In%2520newer%250Aapproaches%252C%2520procedural%2520content%2520generation%2520utilizes%2520machine%2520learning.%2520However%252C%250Athese%2520methods%2520usually%2520require%2520expensive%2520collection%2520of%2520large%2520amounts%2520of%2520data%252C%2520as%250Awell%2520as%2520the%2520development%2520and%2520training%2520of%2520fairly%2520complex%2520learning%2520models%252C%2520which%250Acan%2520be%2520both%2520extremely%2520time-consuming%2520and%2520expensive.%2520The%2520core%2520of%2520our%2520research%2520is%250Ato%2520explore%2520whether%2520we%2520can%2520lower%2520the%2520barrier%2520to%2520the%2520use%2520of%2520personalized%250Aprocedural%2520content%2520generation%2520through%2520a%2520more%2520practical%2520and%2520generalizable%250Aapproach%2520with%2520large%2520language%2520models.%2520Matching%2520game%2520content%2520with%2520player%250Apreferences%2520benefits%2520both%2520players%252C%2520who%2520enjoy%2520the%2520game%2520more%252C%2520and%2520developers%252C%2520who%250Aincreasingly%2520depend%2520on%2520players%2520enjoying%2520the%2520game%2520before%2520being%2520able%2520to%2520monetize%250Ait.%2520Therefore%252C%2520this%2520paper%2520presents%2520a%2520novel%2520approach%2520to%2520achieving%250Apersonalization%2520by%2520using%2520large%2520language%2520models%2520to%2520propose%2520levels%2520based%2520on%2520the%250Agameplay%2520data%2520continuously%2520collected%2520from%2520individual%2520players.%2520We%2520compared%2520the%250Alevels%2520generated%2520using%2520our%2520approach%2520with%2520levels%2520generated%2520with%2520more%2520traditional%250Aprocedural%2520generation%2520techniques.%2520Our%2520easily%2520reproducible%2520method%2520has%2520proven%250Aviable%2520in%2520a%2520production%2520setting%2520and%2520outperformed%2520levels%2520generated%2520by%2520traditional%250Amethods%2520in%2520the%2520probability%2520that%2520a%2520player%2520will%2520not%2520quit%2520the%2520game%2520mid-level.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.10133v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Reasoning%3A%20Personalized%20Content%20Generation%20Without%20the%20Cold%0A%20%20Start%20Problem&entry.906535625=Davor%20Hafnar%20and%20Jure%20Dem%C5%A1ar&entry.1292438233=%20%20Procedural%20content%20generation%20uses%20algorithmic%20techniques%20to%20create%20large%0Aamounts%20of%20new%20content%20for%20games%20at%20much%20lower%20production%20costs.%20In%20newer%0Aapproaches%2C%20procedural%20content%20generation%20utilizes%20machine%20learning.%20However%2C%0Athese%20methods%20usually%20require%20expensive%20collection%20of%20large%20amounts%20of%20data%2C%20as%0Awell%20as%20the%20development%20and%20training%20of%20fairly%20complex%20learning%20models%2C%20which%0Acan%20be%20both%20extremely%20time-consuming%20and%20expensive.%20The%20core%20of%20our%20research%20is%0Ato%20explore%20whether%20we%20can%20lower%20the%20barrier%20to%20the%20use%20of%20personalized%0Aprocedural%20content%20generation%20through%20a%20more%20practical%20and%20generalizable%0Aapproach%20with%20large%20language%20models.%20Matching%20game%20content%20with%20player%0Apreferences%20benefits%20both%20players%2C%20who%20enjoy%20the%20game%20more%2C%20and%20developers%2C%20who%0Aincreasingly%20depend%20on%20players%20enjoying%20the%20game%20before%20being%20able%20to%20monetize%0Ait.%20Therefore%2C%20this%20paper%20presents%20a%20novel%20approach%20to%20achieving%0Apersonalization%20by%20using%20large%20language%20models%20to%20propose%20levels%20based%20on%20the%0Agameplay%20data%20continuously%20collected%20from%20individual%20players.%20We%20compared%20the%0Alevels%20generated%20using%20our%20approach%20with%20levels%20generated%20with%20more%20traditional%0Aprocedural%20generation%20techniques.%20Our%20easily%20reproducible%20method%20has%20proven%0Aviable%20in%20a%20production%20setting%20and%20outperformed%20levels%20generated%20by%20traditional%0Amethods%20in%20the%20probability%20that%20a%20player%20will%20not%20quit%20the%20game%20mid-level.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10133v2&entry.124074799=Read"},
{"title": "Solving Token Gradient Conflict in Mixture-of-Experts for Large\n  Vision-Language Model", "author": "Longrong Yang and Dong Sheng and Chaoxiang Cai and Fan Yang and Size Li and Di Zhang and Xi Li", "abstract": "  The Mixture-of-Experts (MoE) has gained increasing attention in the study of\nLarge Vision-Language Models (LVLMs). It uses a sparse model to replace the\ndense model, achieving comparable performance while activating fewer parameters\nduring inference, thus significantly reducing the inference cost. Existing MoE\nmethods in LVLMs encourage different experts to handle different tokens, and\nthus they employ a router to predict the routing for each token. However, the\npredictions are based solely on sample features and do not truly reveal the\noptimization direction of tokens. This can lead to severe optimization\nconflicts between different tokens within an expert. To address this problem,\nthis paper proposes a novel method based on token-level gradient analysis.\nSpecifically, we first use token-level gradients to identify conflicting tokens\nin experts. Then, we add a specialized loss tailored to eliminate conflicts\namong tokens within each expert. Our method can serve as a plug-in for diverse\nLarge Vision-Language Models, and extensive experimental results demonstrate\nthe effectiveness of our method. The code will be publicly available at\nhttps://github.com/longrongyang/STGC.\n", "link": "http://arxiv.org/abs/2406.19905v1", "date": "2024-06-28", "relevancy": 2.5758, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5361}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5088}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5006}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Solving%20Token%20Gradient%20Conflict%20in%20Mixture-of-Experts%20for%20Large%0A%20%20Vision-Language%20Model&body=Title%3A%20Solving%20Token%20Gradient%20Conflict%20in%20Mixture-of-Experts%20for%20Large%0A%20%20Vision-Language%20Model%0AAuthor%3A%20Longrong%20Yang%20and%20Dong%20Sheng%20and%20Chaoxiang%20Cai%20and%20Fan%20Yang%20and%20Size%20Li%20and%20Di%20Zhang%20and%20Xi%20Li%0AAbstract%3A%20%20%20The%20Mixture-of-Experts%20%28MoE%29%20has%20gained%20increasing%20attention%20in%20the%20study%20of%0ALarge%20Vision-Language%20Models%20%28LVLMs%29.%20It%20uses%20a%20sparse%20model%20to%20replace%20the%0Adense%20model%2C%20achieving%20comparable%20performance%20while%20activating%20fewer%20parameters%0Aduring%20inference%2C%20thus%20significantly%20reducing%20the%20inference%20cost.%20Existing%20MoE%0Amethods%20in%20LVLMs%20encourage%20different%20experts%20to%20handle%20different%20tokens%2C%20and%0Athus%20they%20employ%20a%20router%20to%20predict%20the%20routing%20for%20each%20token.%20However%2C%20the%0Apredictions%20are%20based%20solely%20on%20sample%20features%20and%20do%20not%20truly%20reveal%20the%0Aoptimization%20direction%20of%20tokens.%20This%20can%20lead%20to%20severe%20optimization%0Aconflicts%20between%20different%20tokens%20within%20an%20expert.%20To%20address%20this%20problem%2C%0Athis%20paper%20proposes%20a%20novel%20method%20based%20on%20token-level%20gradient%20analysis.%0ASpecifically%2C%20we%20first%20use%20token-level%20gradients%20to%20identify%20conflicting%20tokens%0Ain%20experts.%20Then%2C%20we%20add%20a%20specialized%20loss%20tailored%20to%20eliminate%20conflicts%0Aamong%20tokens%20within%20each%20expert.%20Our%20method%20can%20serve%20as%20a%20plug-in%20for%20diverse%0ALarge%20Vision-Language%20Models%2C%20and%20extensive%20experimental%20results%20demonstrate%0Athe%20effectiveness%20of%20our%20method.%20The%20code%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/longrongyang/STGC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19905v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSolving%2520Token%2520Gradient%2520Conflict%2520in%2520Mixture-of-Experts%2520for%2520Large%250A%2520%2520Vision-Language%2520Model%26entry.906535625%3DLongrong%2520Yang%2520and%2520Dong%2520Sheng%2520and%2520Chaoxiang%2520Cai%2520and%2520Fan%2520Yang%2520and%2520Size%2520Li%2520and%2520Di%2520Zhang%2520and%2520Xi%2520Li%26entry.1292438233%3D%2520%2520The%2520Mixture-of-Experts%2520%2528MoE%2529%2520has%2520gained%2520increasing%2520attention%2520in%2520the%2520study%2520of%250ALarge%2520Vision-Language%2520Models%2520%2528LVLMs%2529.%2520It%2520uses%2520a%2520sparse%2520model%2520to%2520replace%2520the%250Adense%2520model%252C%2520achieving%2520comparable%2520performance%2520while%2520activating%2520fewer%2520parameters%250Aduring%2520inference%252C%2520thus%2520significantly%2520reducing%2520the%2520inference%2520cost.%2520Existing%2520MoE%250Amethods%2520in%2520LVLMs%2520encourage%2520different%2520experts%2520to%2520handle%2520different%2520tokens%252C%2520and%250Athus%2520they%2520employ%2520a%2520router%2520to%2520predict%2520the%2520routing%2520for%2520each%2520token.%2520However%252C%2520the%250Apredictions%2520are%2520based%2520solely%2520on%2520sample%2520features%2520and%2520do%2520not%2520truly%2520reveal%2520the%250Aoptimization%2520direction%2520of%2520tokens.%2520This%2520can%2520lead%2520to%2520severe%2520optimization%250Aconflicts%2520between%2520different%2520tokens%2520within%2520an%2520expert.%2520To%2520address%2520this%2520problem%252C%250Athis%2520paper%2520proposes%2520a%2520novel%2520method%2520based%2520on%2520token-level%2520gradient%2520analysis.%250ASpecifically%252C%2520we%2520first%2520use%2520token-level%2520gradients%2520to%2520identify%2520conflicting%2520tokens%250Ain%2520experts.%2520Then%252C%2520we%2520add%2520a%2520specialized%2520loss%2520tailored%2520to%2520eliminate%2520conflicts%250Aamong%2520tokens%2520within%2520each%2520expert.%2520Our%2520method%2520can%2520serve%2520as%2520a%2520plug-in%2520for%2520diverse%250ALarge%2520Vision-Language%2520Models%252C%2520and%2520extensive%2520experimental%2520results%2520demonstrate%250Athe%2520effectiveness%2520of%2520our%2520method.%2520The%2520code%2520will%2520be%2520publicly%2520available%2520at%250Ahttps%253A//github.com/longrongyang/STGC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19905v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solving%20Token%20Gradient%20Conflict%20in%20Mixture-of-Experts%20for%20Large%0A%20%20Vision-Language%20Model&entry.906535625=Longrong%20Yang%20and%20Dong%20Sheng%20and%20Chaoxiang%20Cai%20and%20Fan%20Yang%20and%20Size%20Li%20and%20Di%20Zhang%20and%20Xi%20Li&entry.1292438233=%20%20The%20Mixture-of-Experts%20%28MoE%29%20has%20gained%20increasing%20attention%20in%20the%20study%20of%0ALarge%20Vision-Language%20Models%20%28LVLMs%29.%20It%20uses%20a%20sparse%20model%20to%20replace%20the%0Adense%20model%2C%20achieving%20comparable%20performance%20while%20activating%20fewer%20parameters%0Aduring%20inference%2C%20thus%20significantly%20reducing%20the%20inference%20cost.%20Existing%20MoE%0Amethods%20in%20LVLMs%20encourage%20different%20experts%20to%20handle%20different%20tokens%2C%20and%0Athus%20they%20employ%20a%20router%20to%20predict%20the%20routing%20for%20each%20token.%20However%2C%20the%0Apredictions%20are%20based%20solely%20on%20sample%20features%20and%20do%20not%20truly%20reveal%20the%0Aoptimization%20direction%20of%20tokens.%20This%20can%20lead%20to%20severe%20optimization%0Aconflicts%20between%20different%20tokens%20within%20an%20expert.%20To%20address%20this%20problem%2C%0Athis%20paper%20proposes%20a%20novel%20method%20based%20on%20token-level%20gradient%20analysis.%0ASpecifically%2C%20we%20first%20use%20token-level%20gradients%20to%20identify%20conflicting%20tokens%0Ain%20experts.%20Then%2C%20we%20add%20a%20specialized%20loss%20tailored%20to%20eliminate%20conflicts%0Aamong%20tokens%20within%20each%20expert.%20Our%20method%20can%20serve%20as%20a%20plug-in%20for%20diverse%0ALarge%20Vision-Language%20Models%2C%20and%20extensive%20experimental%20results%20demonstrate%0Athe%20effectiveness%20of%20our%20method.%20The%20code%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/longrongyang/STGC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19905v1&entry.124074799=Read"},
{"title": "Segment Anything without Supervision", "author": "XuDong Wang and Jingfeng Yang and Trevor Darrell", "abstract": "  The Segmentation Anything Model (SAM) requires labor-intensive data labeling.\nWe present Unsupervised SAM (UnSAM) for promptable and automatic whole-image\nsegmentation that does not require human annotations. UnSAM utilizes a\ndivide-and-conquer strategy to \"discover\" the hierarchical structure of visual\nscenes. We first leverage top-down clustering methods to partition an unlabeled\nimage into instance/semantic level segments. For all pixels within a segment, a\nbottom-up clustering method is employed to iteratively merge them into larger\ngroups, thereby forming a hierarchical structure. These unsupervised\nmulti-granular masks are then utilized to supervise model training. Evaluated\nacross seven popular datasets, UnSAM achieves competitive results with the\nsupervised counterpart SAM, and surpasses the previous state-of-the-art in\nunsupervised segmentation by 11% in terms of AR. Moreover, we show that\nsupervised SAM can also benefit from our self-supervised labels. By integrating\nour unsupervised pseudo masks into SA-1B's ground-truth masks and training\nUnSAM with only 1% of SA-1B, a lightly semi-supervised UnSAM can often segment\nentities overlooked by supervised SAM, exceeding SAM's AR by over 6.7% and AP\nby 3.9% on SA-1B.\n", "link": "http://arxiv.org/abs/2406.20081v1", "date": "2024-06-28", "relevancy": 2.5686, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.517}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5146}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5096}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segment%20Anything%20without%20Supervision&body=Title%3A%20Segment%20Anything%20without%20Supervision%0AAuthor%3A%20XuDong%20Wang%20and%20Jingfeng%20Yang%20and%20Trevor%20Darrell%0AAbstract%3A%20%20%20The%20Segmentation%20Anything%20Model%20%28SAM%29%20requires%20labor-intensive%20data%20labeling.%0AWe%20present%20Unsupervised%20SAM%20%28UnSAM%29%20for%20promptable%20and%20automatic%20whole-image%0Asegmentation%20that%20does%20not%20require%20human%20annotations.%20UnSAM%20utilizes%20a%0Adivide-and-conquer%20strategy%20to%20%22discover%22%20the%20hierarchical%20structure%20of%20visual%0Ascenes.%20We%20first%20leverage%20top-down%20clustering%20methods%20to%20partition%20an%20unlabeled%0Aimage%20into%20instance/semantic%20level%20segments.%20For%20all%20pixels%20within%20a%20segment%2C%20a%0Abottom-up%20clustering%20method%20is%20employed%20to%20iteratively%20merge%20them%20into%20larger%0Agroups%2C%20thereby%20forming%20a%20hierarchical%20structure.%20These%20unsupervised%0Amulti-granular%20masks%20are%20then%20utilized%20to%20supervise%20model%20training.%20Evaluated%0Aacross%20seven%20popular%20datasets%2C%20UnSAM%20achieves%20competitive%20results%20with%20the%0Asupervised%20counterpart%20SAM%2C%20and%20surpasses%20the%20previous%20state-of-the-art%20in%0Aunsupervised%20segmentation%20by%2011%25%20in%20terms%20of%20AR.%20Moreover%2C%20we%20show%20that%0Asupervised%20SAM%20can%20also%20benefit%20from%20our%20self-supervised%20labels.%20By%20integrating%0Aour%20unsupervised%20pseudo%20masks%20into%20SA-1B%27s%20ground-truth%20masks%20and%20training%0AUnSAM%20with%20only%201%25%20of%20SA-1B%2C%20a%20lightly%20semi-supervised%20UnSAM%20can%20often%20segment%0Aentities%20overlooked%20by%20supervised%20SAM%2C%20exceeding%20SAM%27s%20AR%20by%20over%206.7%25%20and%20AP%0Aby%203.9%25%20on%20SA-1B.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.20081v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegment%2520Anything%2520without%2520Supervision%26entry.906535625%3DXuDong%2520Wang%2520and%2520Jingfeng%2520Yang%2520and%2520Trevor%2520Darrell%26entry.1292438233%3D%2520%2520The%2520Segmentation%2520Anything%2520Model%2520%2528SAM%2529%2520requires%2520labor-intensive%2520data%2520labeling.%250AWe%2520present%2520Unsupervised%2520SAM%2520%2528UnSAM%2529%2520for%2520promptable%2520and%2520automatic%2520whole-image%250Asegmentation%2520that%2520does%2520not%2520require%2520human%2520annotations.%2520UnSAM%2520utilizes%2520a%250Adivide-and-conquer%2520strategy%2520to%2520%2522discover%2522%2520the%2520hierarchical%2520structure%2520of%2520visual%250Ascenes.%2520We%2520first%2520leverage%2520top-down%2520clustering%2520methods%2520to%2520partition%2520an%2520unlabeled%250Aimage%2520into%2520instance/semantic%2520level%2520segments.%2520For%2520all%2520pixels%2520within%2520a%2520segment%252C%2520a%250Abottom-up%2520clustering%2520method%2520is%2520employed%2520to%2520iteratively%2520merge%2520them%2520into%2520larger%250Agroups%252C%2520thereby%2520forming%2520a%2520hierarchical%2520structure.%2520These%2520unsupervised%250Amulti-granular%2520masks%2520are%2520then%2520utilized%2520to%2520supervise%2520model%2520training.%2520Evaluated%250Aacross%2520seven%2520popular%2520datasets%252C%2520UnSAM%2520achieves%2520competitive%2520results%2520with%2520the%250Asupervised%2520counterpart%2520SAM%252C%2520and%2520surpasses%2520the%2520previous%2520state-of-the-art%2520in%250Aunsupervised%2520segmentation%2520by%252011%2525%2520in%2520terms%2520of%2520AR.%2520Moreover%252C%2520we%2520show%2520that%250Asupervised%2520SAM%2520can%2520also%2520benefit%2520from%2520our%2520self-supervised%2520labels.%2520By%2520integrating%250Aour%2520unsupervised%2520pseudo%2520masks%2520into%2520SA-1B%2527s%2520ground-truth%2520masks%2520and%2520training%250AUnSAM%2520with%2520only%25201%2525%2520of%2520SA-1B%252C%2520a%2520lightly%2520semi-supervised%2520UnSAM%2520can%2520often%2520segment%250Aentities%2520overlooked%2520by%2520supervised%2520SAM%252C%2520exceeding%2520SAM%2527s%2520AR%2520by%2520over%25206.7%2525%2520and%2520AP%250Aby%25203.9%2525%2520on%2520SA-1B.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.20081v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segment%20Anything%20without%20Supervision&entry.906535625=XuDong%20Wang%20and%20Jingfeng%20Yang%20and%20Trevor%20Darrell&entry.1292438233=%20%20The%20Segmentation%20Anything%20Model%20%28SAM%29%20requires%20labor-intensive%20data%20labeling.%0AWe%20present%20Unsupervised%20SAM%20%28UnSAM%29%20for%20promptable%20and%20automatic%20whole-image%0Asegmentation%20that%20does%20not%20require%20human%20annotations.%20UnSAM%20utilizes%20a%0Adivide-and-conquer%20strategy%20to%20%22discover%22%20the%20hierarchical%20structure%20of%20visual%0Ascenes.%20We%20first%20leverage%20top-down%20clustering%20methods%20to%20partition%20an%20unlabeled%0Aimage%20into%20instance/semantic%20level%20segments.%20For%20all%20pixels%20within%20a%20segment%2C%20a%0Abottom-up%20clustering%20method%20is%20employed%20to%20iteratively%20merge%20them%20into%20larger%0Agroups%2C%20thereby%20forming%20a%20hierarchical%20structure.%20These%20unsupervised%0Amulti-granular%20masks%20are%20then%20utilized%20to%20supervise%20model%20training.%20Evaluated%0Aacross%20seven%20popular%20datasets%2C%20UnSAM%20achieves%20competitive%20results%20with%20the%0Asupervised%20counterpart%20SAM%2C%20and%20surpasses%20the%20previous%20state-of-the-art%20in%0Aunsupervised%20segmentation%20by%2011%25%20in%20terms%20of%20AR.%20Moreover%2C%20we%20show%20that%0Asupervised%20SAM%20can%20also%20benefit%20from%20our%20self-supervised%20labels.%20By%20integrating%0Aour%20unsupervised%20pseudo%20masks%20into%20SA-1B%27s%20ground-truth%20masks%20and%20training%0AUnSAM%20with%20only%201%25%20of%20SA-1B%2C%20a%20lightly%20semi-supervised%20UnSAM%20can%20often%20segment%0Aentities%20overlooked%20by%20supervised%20SAM%2C%20exceeding%20SAM%27s%20AR%20by%20over%206.7%25%20and%20AP%0Aby%203.9%25%20on%20SA-1B.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.20081v1&entry.124074799=Read"},
{"title": "Spike Accumulation Forwarding for Effective Training of Spiking Neural\n  Networks", "author": "Ryuji Saiin and Tomoya Shirakawa and Sota Yoshihara and Yoshihide Sawada and Hiroyuki Kusumoto", "abstract": "  In this article, we propose a new paradigm for training spiking neural\nnetworks (SNNs), spike accumulation forwarding (SAF). It is known that SNNs are\nenergy-efficient but difficult to train. Consequently, many researchers have\nproposed various methods to solve this problem, among which online training\nthrough time (OTTT) is a method that allows inferring at each time step while\nsuppressing the memory cost. However, to compute efficiently on GPUs, OTTT\nrequires operations with spike trains and weighted summation of spike trains\nduring forwarding. In addition, OTTT has shown a relationship with the Spike\nRepresentation, an alternative training method, though theoretical agreement\nwith Spike Representation has yet to be proven. Our proposed method can solve\nthese problems; namely, SAF can halve the number of operations during the\nforward process, and it can be theoretically proven that SAF is consistent with\nthe Spike Representation and OTTT, respectively. Furthermore, we confirmed the\nabove contents through experiments and showed that it is possible to reduce\nmemory and training time while maintaining accuracy.\n", "link": "http://arxiv.org/abs/2310.02772v6", "date": "2024-06-28", "relevancy": 2.52, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5049}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5047}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5024}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spike%20Accumulation%20Forwarding%20for%20Effective%20Training%20of%20Spiking%20Neural%0A%20%20Networks&body=Title%3A%20Spike%20Accumulation%20Forwarding%20for%20Effective%20Training%20of%20Spiking%20Neural%0A%20%20Networks%0AAuthor%3A%20Ryuji%20Saiin%20and%20Tomoya%20Shirakawa%20and%20Sota%20Yoshihara%20and%20Yoshihide%20Sawada%20and%20Hiroyuki%20Kusumoto%0AAbstract%3A%20%20%20In%20this%20article%2C%20we%20propose%20a%20new%20paradigm%20for%20training%20spiking%20neural%0Anetworks%20%28SNNs%29%2C%20spike%20accumulation%20forwarding%20%28SAF%29.%20It%20is%20known%20that%20SNNs%20are%0Aenergy-efficient%20but%20difficult%20to%20train.%20Consequently%2C%20many%20researchers%20have%0Aproposed%20various%20methods%20to%20solve%20this%20problem%2C%20among%20which%20online%20training%0Athrough%20time%20%28OTTT%29%20is%20a%20method%20that%20allows%20inferring%20at%20each%20time%20step%20while%0Asuppressing%20the%20memory%20cost.%20However%2C%20to%20compute%20efficiently%20on%20GPUs%2C%20OTTT%0Arequires%20operations%20with%20spike%20trains%20and%20weighted%20summation%20of%20spike%20trains%0Aduring%20forwarding.%20In%20addition%2C%20OTTT%20has%20shown%20a%20relationship%20with%20the%20Spike%0ARepresentation%2C%20an%20alternative%20training%20method%2C%20though%20theoretical%20agreement%0Awith%20Spike%20Representation%20has%20yet%20to%20be%20proven.%20Our%20proposed%20method%20can%20solve%0Athese%20problems%3B%20namely%2C%20SAF%20can%20halve%20the%20number%20of%20operations%20during%20the%0Aforward%20process%2C%20and%20it%20can%20be%20theoretically%20proven%20that%20SAF%20is%20consistent%20with%0Athe%20Spike%20Representation%20and%20OTTT%2C%20respectively.%20Furthermore%2C%20we%20confirmed%20the%0Aabove%20contents%20through%20experiments%20and%20showed%20that%20it%20is%20possible%20to%20reduce%0Amemory%20and%20training%20time%20while%20maintaining%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.02772v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpike%2520Accumulation%2520Forwarding%2520for%2520Effective%2520Training%2520of%2520Spiking%2520Neural%250A%2520%2520Networks%26entry.906535625%3DRyuji%2520Saiin%2520and%2520Tomoya%2520Shirakawa%2520and%2520Sota%2520Yoshihara%2520and%2520Yoshihide%2520Sawada%2520and%2520Hiroyuki%2520Kusumoto%26entry.1292438233%3D%2520%2520In%2520this%2520article%252C%2520we%2520propose%2520a%2520new%2520paradigm%2520for%2520training%2520spiking%2520neural%250Anetworks%2520%2528SNNs%2529%252C%2520spike%2520accumulation%2520forwarding%2520%2528SAF%2529.%2520It%2520is%2520known%2520that%2520SNNs%2520are%250Aenergy-efficient%2520but%2520difficult%2520to%2520train.%2520Consequently%252C%2520many%2520researchers%2520have%250Aproposed%2520various%2520methods%2520to%2520solve%2520this%2520problem%252C%2520among%2520which%2520online%2520training%250Athrough%2520time%2520%2528OTTT%2529%2520is%2520a%2520method%2520that%2520allows%2520inferring%2520at%2520each%2520time%2520step%2520while%250Asuppressing%2520the%2520memory%2520cost.%2520However%252C%2520to%2520compute%2520efficiently%2520on%2520GPUs%252C%2520OTTT%250Arequires%2520operations%2520with%2520spike%2520trains%2520and%2520weighted%2520summation%2520of%2520spike%2520trains%250Aduring%2520forwarding.%2520In%2520addition%252C%2520OTTT%2520has%2520shown%2520a%2520relationship%2520with%2520the%2520Spike%250ARepresentation%252C%2520an%2520alternative%2520training%2520method%252C%2520though%2520theoretical%2520agreement%250Awith%2520Spike%2520Representation%2520has%2520yet%2520to%2520be%2520proven.%2520Our%2520proposed%2520method%2520can%2520solve%250Athese%2520problems%253B%2520namely%252C%2520SAF%2520can%2520halve%2520the%2520number%2520of%2520operations%2520during%2520the%250Aforward%2520process%252C%2520and%2520it%2520can%2520be%2520theoretically%2520proven%2520that%2520SAF%2520is%2520consistent%2520with%250Athe%2520Spike%2520Representation%2520and%2520OTTT%252C%2520respectively.%2520Furthermore%252C%2520we%2520confirmed%2520the%250Aabove%2520contents%2520through%2520experiments%2520and%2520showed%2520that%2520it%2520is%2520possible%2520to%2520reduce%250Amemory%2520and%2520training%2520time%2520while%2520maintaining%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.02772v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spike%20Accumulation%20Forwarding%20for%20Effective%20Training%20of%20Spiking%20Neural%0A%20%20Networks&entry.906535625=Ryuji%20Saiin%20and%20Tomoya%20Shirakawa%20and%20Sota%20Yoshihara%20and%20Yoshihide%20Sawada%20and%20Hiroyuki%20Kusumoto&entry.1292438233=%20%20In%20this%20article%2C%20we%20propose%20a%20new%20paradigm%20for%20training%20spiking%20neural%0Anetworks%20%28SNNs%29%2C%20spike%20accumulation%20forwarding%20%28SAF%29.%20It%20is%20known%20that%20SNNs%20are%0Aenergy-efficient%20but%20difficult%20to%20train.%20Consequently%2C%20many%20researchers%20have%0Aproposed%20various%20methods%20to%20solve%20this%20problem%2C%20among%20which%20online%20training%0Athrough%20time%20%28OTTT%29%20is%20a%20method%20that%20allows%20inferring%20at%20each%20time%20step%20while%0Asuppressing%20the%20memory%20cost.%20However%2C%20to%20compute%20efficiently%20on%20GPUs%2C%20OTTT%0Arequires%20operations%20with%20spike%20trains%20and%20weighted%20summation%20of%20spike%20trains%0Aduring%20forwarding.%20In%20addition%2C%20OTTT%20has%20shown%20a%20relationship%20with%20the%20Spike%0ARepresentation%2C%20an%20alternative%20training%20method%2C%20though%20theoretical%20agreement%0Awith%20Spike%20Representation%20has%20yet%20to%20be%20proven.%20Our%20proposed%20method%20can%20solve%0Athese%20problems%3B%20namely%2C%20SAF%20can%20halve%20the%20number%20of%20operations%20during%20the%0Aforward%20process%2C%20and%20it%20can%20be%20theoretically%20proven%20that%20SAF%20is%20consistent%20with%0Athe%20Spike%20Representation%20and%20OTTT%2C%20respectively.%20Furthermore%2C%20we%20confirmed%20the%0Aabove%20contents%20through%20experiments%20and%20showed%20that%20it%20is%20possible%20to%20reduce%0Amemory%20and%20training%20time%20while%20maintaining%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.02772v6&entry.124074799=Read"},
{"title": "MuGSI: Distilling GNNs with Multi-Granularity Structural Information for\n  Graph Classification", "author": "Tianjun Yao and Jiaqi Sun and Defu Cao and Kun Zhang and Guangyi Chen", "abstract": "  Recent works have introduced GNN-to-MLP knowledge distillation (KD)\nframeworks to combine both GNN's superior performance and MLP's fast inference\nspeed. However, existing KD frameworks are primarily designed for node\nclassification within single graphs, leaving their applicability to graph\nclassification largely unexplored. Two main challenges arise when extending KD\nfor node classification to graph classification: (1) The inherent sparsity of\nlearning signals due to soft labels being generated at the graph level; (2) The\nlimited expressiveness of student MLPs, especially in datasets with limited\ninput feature spaces. To overcome these challenges, we introduce MuGSI, a novel\nKD framework that employs Multi-granularity Structural Information for graph\nclassification. Specifically, we propose multi-granularity distillation loss in\nMuGSI to tackle the first challenge. This loss function is composed of three\ndistinct components: graph-level distillation, subgraph-level distillation, and\nnode-level distillation. Each component targets a specific granularity of the\ngraph structure, ensuring a comprehensive transfer of structural knowledge from\nthe teacher model to the student model. To tackle the second challenge, MuGSI\nproposes to incorporate a node feature augmentation component, thereby\nenhancing the expressiveness of the student MLPs and making them more capable\nlearners. We perform extensive experiments across a variety of datasets and\ndifferent teacher/student model architectures. The experiment results\ndemonstrate the effectiveness, efficiency, and robustness of MuGSI. Codes are\npublicly available at: \\textbf{\\url{https://github.com/tianyao-aka/MuGSI}.}\n", "link": "http://arxiv.org/abs/2406.19832v1", "date": "2024-06-28", "relevancy": 2.5114, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5333}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4892}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4843}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MuGSI%3A%20Distilling%20GNNs%20with%20Multi-Granularity%20Structural%20Information%20for%0A%20%20Graph%20Classification&body=Title%3A%20MuGSI%3A%20Distilling%20GNNs%20with%20Multi-Granularity%20Structural%20Information%20for%0A%20%20Graph%20Classification%0AAuthor%3A%20Tianjun%20Yao%20and%20Jiaqi%20Sun%20and%20Defu%20Cao%20and%20Kun%20Zhang%20and%20Guangyi%20Chen%0AAbstract%3A%20%20%20Recent%20works%20have%20introduced%20GNN-to-MLP%20knowledge%20distillation%20%28KD%29%0Aframeworks%20to%20combine%20both%20GNN%27s%20superior%20performance%20and%20MLP%27s%20fast%20inference%0Aspeed.%20However%2C%20existing%20KD%20frameworks%20are%20primarily%20designed%20for%20node%0Aclassification%20within%20single%20graphs%2C%20leaving%20their%20applicability%20to%20graph%0Aclassification%20largely%20unexplored.%20Two%20main%20challenges%20arise%20when%20extending%20KD%0Afor%20node%20classification%20to%20graph%20classification%3A%20%281%29%20The%20inherent%20sparsity%20of%0Alearning%20signals%20due%20to%20soft%20labels%20being%20generated%20at%20the%20graph%20level%3B%20%282%29%20The%0Alimited%20expressiveness%20of%20student%20MLPs%2C%20especially%20in%20datasets%20with%20limited%0Ainput%20feature%20spaces.%20To%20overcome%20these%20challenges%2C%20we%20introduce%20MuGSI%2C%20a%20novel%0AKD%20framework%20that%20employs%20Multi-granularity%20Structural%20Information%20for%20graph%0Aclassification.%20Specifically%2C%20we%20propose%20multi-granularity%20distillation%20loss%20in%0AMuGSI%20to%20tackle%20the%20first%20challenge.%20This%20loss%20function%20is%20composed%20of%20three%0Adistinct%20components%3A%20graph-level%20distillation%2C%20subgraph-level%20distillation%2C%20and%0Anode-level%20distillation.%20Each%20component%20targets%20a%20specific%20granularity%20of%20the%0Agraph%20structure%2C%20ensuring%20a%20comprehensive%20transfer%20of%20structural%20knowledge%20from%0Athe%20teacher%20model%20to%20the%20student%20model.%20To%20tackle%20the%20second%20challenge%2C%20MuGSI%0Aproposes%20to%20incorporate%20a%20node%20feature%20augmentation%20component%2C%20thereby%0Aenhancing%20the%20expressiveness%20of%20the%20student%20MLPs%20and%20making%20them%20more%20capable%0Alearners.%20We%20perform%20extensive%20experiments%20across%20a%20variety%20of%20datasets%20and%0Adifferent%20teacher/student%20model%20architectures.%20The%20experiment%20results%0Ademonstrate%20the%20effectiveness%2C%20efficiency%2C%20and%20robustness%20of%20MuGSI.%20Codes%20are%0Apublicly%20available%20at%3A%20%5Ctextbf%7B%5Curl%7Bhttps%3A//github.com/tianyao-aka/MuGSI%7D.%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19832v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMuGSI%253A%2520Distilling%2520GNNs%2520with%2520Multi-Granularity%2520Structural%2520Information%2520for%250A%2520%2520Graph%2520Classification%26entry.906535625%3DTianjun%2520Yao%2520and%2520Jiaqi%2520Sun%2520and%2520Defu%2520Cao%2520and%2520Kun%2520Zhang%2520and%2520Guangyi%2520Chen%26entry.1292438233%3D%2520%2520Recent%2520works%2520have%2520introduced%2520GNN-to-MLP%2520knowledge%2520distillation%2520%2528KD%2529%250Aframeworks%2520to%2520combine%2520both%2520GNN%2527s%2520superior%2520performance%2520and%2520MLP%2527s%2520fast%2520inference%250Aspeed.%2520However%252C%2520existing%2520KD%2520frameworks%2520are%2520primarily%2520designed%2520for%2520node%250Aclassification%2520within%2520single%2520graphs%252C%2520leaving%2520their%2520applicability%2520to%2520graph%250Aclassification%2520largely%2520unexplored.%2520Two%2520main%2520challenges%2520arise%2520when%2520extending%2520KD%250Afor%2520node%2520classification%2520to%2520graph%2520classification%253A%2520%25281%2529%2520The%2520inherent%2520sparsity%2520of%250Alearning%2520signals%2520due%2520to%2520soft%2520labels%2520being%2520generated%2520at%2520the%2520graph%2520level%253B%2520%25282%2529%2520The%250Alimited%2520expressiveness%2520of%2520student%2520MLPs%252C%2520especially%2520in%2520datasets%2520with%2520limited%250Ainput%2520feature%2520spaces.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520introduce%2520MuGSI%252C%2520a%2520novel%250AKD%2520framework%2520that%2520employs%2520Multi-granularity%2520Structural%2520Information%2520for%2520graph%250Aclassification.%2520Specifically%252C%2520we%2520propose%2520multi-granularity%2520distillation%2520loss%2520in%250AMuGSI%2520to%2520tackle%2520the%2520first%2520challenge.%2520This%2520loss%2520function%2520is%2520composed%2520of%2520three%250Adistinct%2520components%253A%2520graph-level%2520distillation%252C%2520subgraph-level%2520distillation%252C%2520and%250Anode-level%2520distillation.%2520Each%2520component%2520targets%2520a%2520specific%2520granularity%2520of%2520the%250Agraph%2520structure%252C%2520ensuring%2520a%2520comprehensive%2520transfer%2520of%2520structural%2520knowledge%2520from%250Athe%2520teacher%2520model%2520to%2520the%2520student%2520model.%2520To%2520tackle%2520the%2520second%2520challenge%252C%2520MuGSI%250Aproposes%2520to%2520incorporate%2520a%2520node%2520feature%2520augmentation%2520component%252C%2520thereby%250Aenhancing%2520the%2520expressiveness%2520of%2520the%2520student%2520MLPs%2520and%2520making%2520them%2520more%2520capable%250Alearners.%2520We%2520perform%2520extensive%2520experiments%2520across%2520a%2520variety%2520of%2520datasets%2520and%250Adifferent%2520teacher/student%2520model%2520architectures.%2520The%2520experiment%2520results%250Ademonstrate%2520the%2520effectiveness%252C%2520efficiency%252C%2520and%2520robustness%2520of%2520MuGSI.%2520Codes%2520are%250Apublicly%2520available%2520at%253A%2520%255Ctextbf%257B%255Curl%257Bhttps%253A//github.com/tianyao-aka/MuGSI%257D.%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19832v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MuGSI%3A%20Distilling%20GNNs%20with%20Multi-Granularity%20Structural%20Information%20for%0A%20%20Graph%20Classification&entry.906535625=Tianjun%20Yao%20and%20Jiaqi%20Sun%20and%20Defu%20Cao%20and%20Kun%20Zhang%20and%20Guangyi%20Chen&entry.1292438233=%20%20Recent%20works%20have%20introduced%20GNN-to-MLP%20knowledge%20distillation%20%28KD%29%0Aframeworks%20to%20combine%20both%20GNN%27s%20superior%20performance%20and%20MLP%27s%20fast%20inference%0Aspeed.%20However%2C%20existing%20KD%20frameworks%20are%20primarily%20designed%20for%20node%0Aclassification%20within%20single%20graphs%2C%20leaving%20their%20applicability%20to%20graph%0Aclassification%20largely%20unexplored.%20Two%20main%20challenges%20arise%20when%20extending%20KD%0Afor%20node%20classification%20to%20graph%20classification%3A%20%281%29%20The%20inherent%20sparsity%20of%0Alearning%20signals%20due%20to%20soft%20labels%20being%20generated%20at%20the%20graph%20level%3B%20%282%29%20The%0Alimited%20expressiveness%20of%20student%20MLPs%2C%20especially%20in%20datasets%20with%20limited%0Ainput%20feature%20spaces.%20To%20overcome%20these%20challenges%2C%20we%20introduce%20MuGSI%2C%20a%20novel%0AKD%20framework%20that%20employs%20Multi-granularity%20Structural%20Information%20for%20graph%0Aclassification.%20Specifically%2C%20we%20propose%20multi-granularity%20distillation%20loss%20in%0AMuGSI%20to%20tackle%20the%20first%20challenge.%20This%20loss%20function%20is%20composed%20of%20three%0Adistinct%20components%3A%20graph-level%20distillation%2C%20subgraph-level%20distillation%2C%20and%0Anode-level%20distillation.%20Each%20component%20targets%20a%20specific%20granularity%20of%20the%0Agraph%20structure%2C%20ensuring%20a%20comprehensive%20transfer%20of%20structural%20knowledge%20from%0Athe%20teacher%20model%20to%20the%20student%20model.%20To%20tackle%20the%20second%20challenge%2C%20MuGSI%0Aproposes%20to%20incorporate%20a%20node%20feature%20augmentation%20component%2C%20thereby%0Aenhancing%20the%20expressiveness%20of%20the%20student%20MLPs%20and%20making%20them%20more%20capable%0Alearners.%20We%20perform%20extensive%20experiments%20across%20a%20variety%20of%20datasets%20and%0Adifferent%20teacher/student%20model%20architectures.%20The%20experiment%20results%0Ademonstrate%20the%20effectiveness%2C%20efficiency%2C%20and%20robustness%20of%20MuGSI.%20Codes%20are%0Apublicly%20available%20at%3A%20%5Ctextbf%7B%5Curl%7Bhttps%3A//github.com/tianyao-aka/MuGSI%7D.%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19832v1&entry.124074799=Read"},
{"title": "Auto Cherry-Picker: Learning from High-quality Generative Data Driven by\n  Language", "author": "Yicheng Chen and Xiangtai Li and Yining Li and Yanhong Zeng and Jianzong Wu and Xiangyu Zhao and Kai Chen", "abstract": "  Diffusion-based models have shown great potential in generating high-quality\nimages with various layouts, which can benefit downstream perception tasks.\nHowever, a fully automatic layout generation driven only by language and a\nsuitable metric for measuring multiple generated instances has not been well\nexplored. In this work, we present Auto Cherry-Picker (ACP), a novel framework\nthat generates high-quality multi-modal training examples to augment perception\nand multi-modal training. Starting with a simple list of natural language\nconcepts, we prompt large language models (LLMs) to generate a detailed\ndescription and design reasonable layouts. Next, we use an off-the-shelf\ntext-to-image model to generate multiple images. Then, the generated data are\nrefined using a comprehensively designed metric to ensure quality. In\nparticular, we present a new metric, Composite Layout and Image Score (CLIS),\nto evaluate the generated images fairly. Our synthetic high-quality examples\nboost performance in various scenarios by customizing the initial concept list,\nespecially in addressing challenges associated with long-tailed distribution\nand imbalanced datasets. Experiment results on downstream tasks demonstrate\nthat Auto Cherry-Picker can significantly improve the performance of existing\nmodels. In addition, we have thoroughly investigated the correlation between\nCLIS and performance gains in downstream tasks, and we find that a better CLIS\nscore results in better performance. This finding shows the potential for\nevaluation metrics as the role for various visual perception and MLLM tasks.\nCode will be available.\n", "link": "http://arxiv.org/abs/2406.20085v1", "date": "2024-06-28", "relevancy": 2.4998, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6652}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6076}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Auto%20Cherry-Picker%3A%20Learning%20from%20High-quality%20Generative%20Data%20Driven%20by%0A%20%20Language&body=Title%3A%20Auto%20Cherry-Picker%3A%20Learning%20from%20High-quality%20Generative%20Data%20Driven%20by%0A%20%20Language%0AAuthor%3A%20Yicheng%20Chen%20and%20Xiangtai%20Li%20and%20Yining%20Li%20and%20Yanhong%20Zeng%20and%20Jianzong%20Wu%20and%20Xiangyu%20Zhao%20and%20Kai%20Chen%0AAbstract%3A%20%20%20Diffusion-based%20models%20have%20shown%20great%20potential%20in%20generating%20high-quality%0Aimages%20with%20various%20layouts%2C%20which%20can%20benefit%20downstream%20perception%20tasks.%0AHowever%2C%20a%20fully%20automatic%20layout%20generation%20driven%20only%20by%20language%20and%20a%0Asuitable%20metric%20for%20measuring%20multiple%20generated%20instances%20has%20not%20been%20well%0Aexplored.%20In%20this%20work%2C%20we%20present%20Auto%20Cherry-Picker%20%28ACP%29%2C%20a%20novel%20framework%0Athat%20generates%20high-quality%20multi-modal%20training%20examples%20to%20augment%20perception%0Aand%20multi-modal%20training.%20Starting%20with%20a%20simple%20list%20of%20natural%20language%0Aconcepts%2C%20we%20prompt%20large%20language%20models%20%28LLMs%29%20to%20generate%20a%20detailed%0Adescription%20and%20design%20reasonable%20layouts.%20Next%2C%20we%20use%20an%20off-the-shelf%0Atext-to-image%20model%20to%20generate%20multiple%20images.%20Then%2C%20the%20generated%20data%20are%0Arefined%20using%20a%20comprehensively%20designed%20metric%20to%20ensure%20quality.%20In%0Aparticular%2C%20we%20present%20a%20new%20metric%2C%20Composite%20Layout%20and%20Image%20Score%20%28CLIS%29%2C%0Ato%20evaluate%20the%20generated%20images%20fairly.%20Our%20synthetic%20high-quality%20examples%0Aboost%20performance%20in%20various%20scenarios%20by%20customizing%20the%20initial%20concept%20list%2C%0Aespecially%20in%20addressing%20challenges%20associated%20with%20long-tailed%20distribution%0Aand%20imbalanced%20datasets.%20Experiment%20results%20on%20downstream%20tasks%20demonstrate%0Athat%20Auto%20Cherry-Picker%20can%20significantly%20improve%20the%20performance%20of%20existing%0Amodels.%20In%20addition%2C%20we%20have%20thoroughly%20investigated%20the%20correlation%20between%0ACLIS%20and%20performance%20gains%20in%20downstream%20tasks%2C%20and%20we%20find%20that%20a%20better%20CLIS%0Ascore%20results%20in%20better%20performance.%20This%20finding%20shows%20the%20potential%20for%0Aevaluation%20metrics%20as%20the%20role%20for%20various%20visual%20perception%20and%20MLLM%20tasks.%0ACode%20will%20be%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.20085v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAuto%2520Cherry-Picker%253A%2520Learning%2520from%2520High-quality%2520Generative%2520Data%2520Driven%2520by%250A%2520%2520Language%26entry.906535625%3DYicheng%2520Chen%2520and%2520Xiangtai%2520Li%2520and%2520Yining%2520Li%2520and%2520Yanhong%2520Zeng%2520and%2520Jianzong%2520Wu%2520and%2520Xiangyu%2520Zhao%2520and%2520Kai%2520Chen%26entry.1292438233%3D%2520%2520Diffusion-based%2520models%2520have%2520shown%2520great%2520potential%2520in%2520generating%2520high-quality%250Aimages%2520with%2520various%2520layouts%252C%2520which%2520can%2520benefit%2520downstream%2520perception%2520tasks.%250AHowever%252C%2520a%2520fully%2520automatic%2520layout%2520generation%2520driven%2520only%2520by%2520language%2520and%2520a%250Asuitable%2520metric%2520for%2520measuring%2520multiple%2520generated%2520instances%2520has%2520not%2520been%2520well%250Aexplored.%2520In%2520this%2520work%252C%2520we%2520present%2520Auto%2520Cherry-Picker%2520%2528ACP%2529%252C%2520a%2520novel%2520framework%250Athat%2520generates%2520high-quality%2520multi-modal%2520training%2520examples%2520to%2520augment%2520perception%250Aand%2520multi-modal%2520training.%2520Starting%2520with%2520a%2520simple%2520list%2520of%2520natural%2520language%250Aconcepts%252C%2520we%2520prompt%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520generate%2520a%2520detailed%250Adescription%2520and%2520design%2520reasonable%2520layouts.%2520Next%252C%2520we%2520use%2520an%2520off-the-shelf%250Atext-to-image%2520model%2520to%2520generate%2520multiple%2520images.%2520Then%252C%2520the%2520generated%2520data%2520are%250Arefined%2520using%2520a%2520comprehensively%2520designed%2520metric%2520to%2520ensure%2520quality.%2520In%250Aparticular%252C%2520we%2520present%2520a%2520new%2520metric%252C%2520Composite%2520Layout%2520and%2520Image%2520Score%2520%2528CLIS%2529%252C%250Ato%2520evaluate%2520the%2520generated%2520images%2520fairly.%2520Our%2520synthetic%2520high-quality%2520examples%250Aboost%2520performance%2520in%2520various%2520scenarios%2520by%2520customizing%2520the%2520initial%2520concept%2520list%252C%250Aespecially%2520in%2520addressing%2520challenges%2520associated%2520with%2520long-tailed%2520distribution%250Aand%2520imbalanced%2520datasets.%2520Experiment%2520results%2520on%2520downstream%2520tasks%2520demonstrate%250Athat%2520Auto%2520Cherry-Picker%2520can%2520significantly%2520improve%2520the%2520performance%2520of%2520existing%250Amodels.%2520In%2520addition%252C%2520we%2520have%2520thoroughly%2520investigated%2520the%2520correlation%2520between%250ACLIS%2520and%2520performance%2520gains%2520in%2520downstream%2520tasks%252C%2520and%2520we%2520find%2520that%2520a%2520better%2520CLIS%250Ascore%2520results%2520in%2520better%2520performance.%2520This%2520finding%2520shows%2520the%2520potential%2520for%250Aevaluation%2520metrics%2520as%2520the%2520role%2520for%2520various%2520visual%2520perception%2520and%2520MLLM%2520tasks.%250ACode%2520will%2520be%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.20085v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Auto%20Cherry-Picker%3A%20Learning%20from%20High-quality%20Generative%20Data%20Driven%20by%0A%20%20Language&entry.906535625=Yicheng%20Chen%20and%20Xiangtai%20Li%20and%20Yining%20Li%20and%20Yanhong%20Zeng%20and%20Jianzong%20Wu%20and%20Xiangyu%20Zhao%20and%20Kai%20Chen&entry.1292438233=%20%20Diffusion-based%20models%20have%20shown%20great%20potential%20in%20generating%20high-quality%0Aimages%20with%20various%20layouts%2C%20which%20can%20benefit%20downstream%20perception%20tasks.%0AHowever%2C%20a%20fully%20automatic%20layout%20generation%20driven%20only%20by%20language%20and%20a%0Asuitable%20metric%20for%20measuring%20multiple%20generated%20instances%20has%20not%20been%20well%0Aexplored.%20In%20this%20work%2C%20we%20present%20Auto%20Cherry-Picker%20%28ACP%29%2C%20a%20novel%20framework%0Athat%20generates%20high-quality%20multi-modal%20training%20examples%20to%20augment%20perception%0Aand%20multi-modal%20training.%20Starting%20with%20a%20simple%20list%20of%20natural%20language%0Aconcepts%2C%20we%20prompt%20large%20language%20models%20%28LLMs%29%20to%20generate%20a%20detailed%0Adescription%20and%20design%20reasonable%20layouts.%20Next%2C%20we%20use%20an%20off-the-shelf%0Atext-to-image%20model%20to%20generate%20multiple%20images.%20Then%2C%20the%20generated%20data%20are%0Arefined%20using%20a%20comprehensively%20designed%20metric%20to%20ensure%20quality.%20In%0Aparticular%2C%20we%20present%20a%20new%20metric%2C%20Composite%20Layout%20and%20Image%20Score%20%28CLIS%29%2C%0Ato%20evaluate%20the%20generated%20images%20fairly.%20Our%20synthetic%20high-quality%20examples%0Aboost%20performance%20in%20various%20scenarios%20by%20customizing%20the%20initial%20concept%20list%2C%0Aespecially%20in%20addressing%20challenges%20associated%20with%20long-tailed%20distribution%0Aand%20imbalanced%20datasets.%20Experiment%20results%20on%20downstream%20tasks%20demonstrate%0Athat%20Auto%20Cherry-Picker%20can%20significantly%20improve%20the%20performance%20of%20existing%0Amodels.%20In%20addition%2C%20we%20have%20thoroughly%20investigated%20the%20correlation%20between%0ACLIS%20and%20performance%20gains%20in%20downstream%20tasks%2C%20and%20we%20find%20that%20a%20better%20CLIS%0Ascore%20results%20in%20better%20performance.%20This%20finding%20shows%20the%20potential%20for%0Aevaluation%20metrics%20as%20the%20role%20for%20various%20visual%20perception%20and%20MLLM%20tasks.%0ACode%20will%20be%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.20085v1&entry.124074799=Read"},
{"title": "Kandinsky 3.0 Technical Report", "author": "Vladimir Arkhipkin and Andrei Filatov and Viacheslav Vasilev and Anastasia Maltseva and Said Azizov and Igor Pavlov and Julia Agafonova and Andrey Kuznetsov and Denis Dimitrov", "abstract": "  We present Kandinsky 3.0, a large-scale text-to-image generation model based\non latent diffusion, continuing the series of text-to-image Kandinsky models\nand reflecting our progress to achieve higher quality and realism of image\ngeneration. In this report we describe the architecture of the model, the data\ncollection procedure, the training technique, and the production system for\nuser interaction. We focus on the key components that, as we have identified as\na result of a large number of experiments, had the most significant impact on\nimproving the quality of our model compared to the others. We also describe\nextensions and applications of our model, including super resolution,\ninpainting, image editing, image-to-video generation, and a distilled version\nof Kandinsky 3.0 - Kandinsky 3.1, which does inference in 4 steps of the\nreverse process and 20 times faster without visual quality decrease. By\nside-by-side human preferences comparison, Kandinsky becomes better in text\nunderstanding and works better on specific domains. The code is available at\nhttps://github.com/ai-forever/Kandinsky-3\n", "link": "http://arxiv.org/abs/2312.03511v3", "date": "2024-06-28", "relevancy": 2.398, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6143}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5965}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5965}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kandinsky%203.0%20Technical%20Report&body=Title%3A%20Kandinsky%203.0%20Technical%20Report%0AAuthor%3A%20Vladimir%20Arkhipkin%20and%20Andrei%20Filatov%20and%20Viacheslav%20Vasilev%20and%20Anastasia%20Maltseva%20and%20Said%20Azizov%20and%20Igor%20Pavlov%20and%20Julia%20Agafonova%20and%20Andrey%20Kuznetsov%20and%20Denis%20Dimitrov%0AAbstract%3A%20%20%20We%20present%20Kandinsky%203.0%2C%20a%20large-scale%20text-to-image%20generation%20model%20based%0Aon%20latent%20diffusion%2C%20continuing%20the%20series%20of%20text-to-image%20Kandinsky%20models%0Aand%20reflecting%20our%20progress%20to%20achieve%20higher%20quality%20and%20realism%20of%20image%0Ageneration.%20In%20this%20report%20we%20describe%20the%20architecture%20of%20the%20model%2C%20the%20data%0Acollection%20procedure%2C%20the%20training%20technique%2C%20and%20the%20production%20system%20for%0Auser%20interaction.%20We%20focus%20on%20the%20key%20components%20that%2C%20as%20we%20have%20identified%20as%0Aa%20result%20of%20a%20large%20number%20of%20experiments%2C%20had%20the%20most%20significant%20impact%20on%0Aimproving%20the%20quality%20of%20our%20model%20compared%20to%20the%20others.%20We%20also%20describe%0Aextensions%20and%20applications%20of%20our%20model%2C%20including%20super%20resolution%2C%0Ainpainting%2C%20image%20editing%2C%20image-to-video%20generation%2C%20and%20a%20distilled%20version%0Aof%20Kandinsky%203.0%20-%20Kandinsky%203.1%2C%20which%20does%20inference%20in%204%20steps%20of%20the%0Areverse%20process%20and%2020%20times%20faster%20without%20visual%20quality%20decrease.%20By%0Aside-by-side%20human%20preferences%20comparison%2C%20Kandinsky%20becomes%20better%20in%20text%0Aunderstanding%20and%20works%20better%20on%20specific%20domains.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/ai-forever/Kandinsky-3%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.03511v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKandinsky%25203.0%2520Technical%2520Report%26entry.906535625%3DVladimir%2520Arkhipkin%2520and%2520Andrei%2520Filatov%2520and%2520Viacheslav%2520Vasilev%2520and%2520Anastasia%2520Maltseva%2520and%2520Said%2520Azizov%2520and%2520Igor%2520Pavlov%2520and%2520Julia%2520Agafonova%2520and%2520Andrey%2520Kuznetsov%2520and%2520Denis%2520Dimitrov%26entry.1292438233%3D%2520%2520We%2520present%2520Kandinsky%25203.0%252C%2520a%2520large-scale%2520text-to-image%2520generation%2520model%2520based%250Aon%2520latent%2520diffusion%252C%2520continuing%2520the%2520series%2520of%2520text-to-image%2520Kandinsky%2520models%250Aand%2520reflecting%2520our%2520progress%2520to%2520achieve%2520higher%2520quality%2520and%2520realism%2520of%2520image%250Ageneration.%2520In%2520this%2520report%2520we%2520describe%2520the%2520architecture%2520of%2520the%2520model%252C%2520the%2520data%250Acollection%2520procedure%252C%2520the%2520training%2520technique%252C%2520and%2520the%2520production%2520system%2520for%250Auser%2520interaction.%2520We%2520focus%2520on%2520the%2520key%2520components%2520that%252C%2520as%2520we%2520have%2520identified%2520as%250Aa%2520result%2520of%2520a%2520large%2520number%2520of%2520experiments%252C%2520had%2520the%2520most%2520significant%2520impact%2520on%250Aimproving%2520the%2520quality%2520of%2520our%2520model%2520compared%2520to%2520the%2520others.%2520We%2520also%2520describe%250Aextensions%2520and%2520applications%2520of%2520our%2520model%252C%2520including%2520super%2520resolution%252C%250Ainpainting%252C%2520image%2520editing%252C%2520image-to-video%2520generation%252C%2520and%2520a%2520distilled%2520version%250Aof%2520Kandinsky%25203.0%2520-%2520Kandinsky%25203.1%252C%2520which%2520does%2520inference%2520in%25204%2520steps%2520of%2520the%250Areverse%2520process%2520and%252020%2520times%2520faster%2520without%2520visual%2520quality%2520decrease.%2520By%250Aside-by-side%2520human%2520preferences%2520comparison%252C%2520Kandinsky%2520becomes%2520better%2520in%2520text%250Aunderstanding%2520and%2520works%2520better%2520on%2520specific%2520domains.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/ai-forever/Kandinsky-3%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.03511v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kandinsky%203.0%20Technical%20Report&entry.906535625=Vladimir%20Arkhipkin%20and%20Andrei%20Filatov%20and%20Viacheslav%20Vasilev%20and%20Anastasia%20Maltseva%20and%20Said%20Azizov%20and%20Igor%20Pavlov%20and%20Julia%20Agafonova%20and%20Andrey%20Kuznetsov%20and%20Denis%20Dimitrov&entry.1292438233=%20%20We%20present%20Kandinsky%203.0%2C%20a%20large-scale%20text-to-image%20generation%20model%20based%0Aon%20latent%20diffusion%2C%20continuing%20the%20series%20of%20text-to-image%20Kandinsky%20models%0Aand%20reflecting%20our%20progress%20to%20achieve%20higher%20quality%20and%20realism%20of%20image%0Ageneration.%20In%20this%20report%20we%20describe%20the%20architecture%20of%20the%20model%2C%20the%20data%0Acollection%20procedure%2C%20the%20training%20technique%2C%20and%20the%20production%20system%20for%0Auser%20interaction.%20We%20focus%20on%20the%20key%20components%20that%2C%20as%20we%20have%20identified%20as%0Aa%20result%20of%20a%20large%20number%20of%20experiments%2C%20had%20the%20most%20significant%20impact%20on%0Aimproving%20the%20quality%20of%20our%20model%20compared%20to%20the%20others.%20We%20also%20describe%0Aextensions%20and%20applications%20of%20our%20model%2C%20including%20super%20resolution%2C%0Ainpainting%2C%20image%20editing%2C%20image-to-video%20generation%2C%20and%20a%20distilled%20version%0Aof%20Kandinsky%203.0%20-%20Kandinsky%203.1%2C%20which%20does%20inference%20in%204%20steps%20of%20the%0Areverse%20process%20and%2020%20times%20faster%20without%20visual%20quality%20decrease.%20By%0Aside-by-side%20human%20preferences%20comparison%2C%20Kandinsky%20becomes%20better%20in%20text%0Aunderstanding%20and%20works%20better%20on%20specific%20domains.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/ai-forever/Kandinsky-3%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.03511v3&entry.124074799=Read"},
{"title": "The Intelligible and Effective Graph Neural Additive Networks", "author": "Maya Bechler-Speicher and Amir Globerson and Ran Gilad-Bachrach", "abstract": "  Graph Neural Networks (GNNs) have emerged as the predominant approach for\nlearning over graph-structured data. However, most GNNs operate as black-box\nmodels and require post-hoc explanations, which may not suffice in high-stakes\nscenarios where transparency is crucial. In this paper, we present a GNN that\nis interpretable by design. Our model, Graph Neural Additive Network (GNAN), is\na novel extension of the interpretable class of Generalized Additive Models,\nand can be visualized and fully understood by humans. GNAN is designed to be\nfully interpretable, allowing both global and local explanations at the feature\nand graph levels through direct visualization of the model. These\nvisualizations describe the exact way the model uses the relationships between\nthe target variable, the features, and the graph. We demonstrate the\nintelligibility of GNANs in a series of examples on different tasks and\ndatasets. In addition, we show that the accuracy of GNAN is on par with\nblack-box GNNs, making it suitable for critical applications where transparency\nis essential, alongside high accuracy.\n", "link": "http://arxiv.org/abs/2406.01317v2", "date": "2024-06-28", "relevancy": 2.3805, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4989}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4737}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Intelligible%20and%20Effective%20Graph%20Neural%20Additive%20Networks&body=Title%3A%20The%20Intelligible%20and%20Effective%20Graph%20Neural%20Additive%20Networks%0AAuthor%3A%20Maya%20Bechler-Speicher%20and%20Amir%20Globerson%20and%20Ran%20Gilad-Bachrach%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20emerged%20as%20the%20predominant%20approach%20for%0Alearning%20over%20graph-structured%20data.%20However%2C%20most%20GNNs%20operate%20as%20black-box%0Amodels%20and%20require%20post-hoc%20explanations%2C%20which%20may%20not%20suffice%20in%20high-stakes%0Ascenarios%20where%20transparency%20is%20crucial.%20In%20this%20paper%2C%20we%20present%20a%20GNN%20that%0Ais%20interpretable%20by%20design.%20Our%20model%2C%20Graph%20Neural%20Additive%20Network%20%28GNAN%29%2C%20is%0Aa%20novel%20extension%20of%20the%20interpretable%20class%20of%20Generalized%20Additive%20Models%2C%0Aand%20can%20be%20visualized%20and%20fully%20understood%20by%20humans.%20GNAN%20is%20designed%20to%20be%0Afully%20interpretable%2C%20allowing%20both%20global%20and%20local%20explanations%20at%20the%20feature%0Aand%20graph%20levels%20through%20direct%20visualization%20of%20the%20model.%20These%0Avisualizations%20describe%20the%20exact%20way%20the%20model%20uses%20the%20relationships%20between%0Athe%20target%20variable%2C%20the%20features%2C%20and%20the%20graph.%20We%20demonstrate%20the%0Aintelligibility%20of%20GNANs%20in%20a%20series%20of%20examples%20on%20different%20tasks%20and%0Adatasets.%20In%20addition%2C%20we%20show%20that%20the%20accuracy%20of%20GNAN%20is%20on%20par%20with%0Ablack-box%20GNNs%2C%20making%20it%20suitable%20for%20critical%20applications%20where%20transparency%0Ais%20essential%2C%20alongside%20high%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.01317v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Intelligible%2520and%2520Effective%2520Graph%2520Neural%2520Additive%2520Networks%26entry.906535625%3DMaya%2520Bechler-Speicher%2520and%2520Amir%2520Globerson%2520and%2520Ran%2520Gilad-Bachrach%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520emerged%2520as%2520the%2520predominant%2520approach%2520for%250Alearning%2520over%2520graph-structured%2520data.%2520However%252C%2520most%2520GNNs%2520operate%2520as%2520black-box%250Amodels%2520and%2520require%2520post-hoc%2520explanations%252C%2520which%2520may%2520not%2520suffice%2520in%2520high-stakes%250Ascenarios%2520where%2520transparency%2520is%2520crucial.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520GNN%2520that%250Ais%2520interpretable%2520by%2520design.%2520Our%2520model%252C%2520Graph%2520Neural%2520Additive%2520Network%2520%2528GNAN%2529%252C%2520is%250Aa%2520novel%2520extension%2520of%2520the%2520interpretable%2520class%2520of%2520Generalized%2520Additive%2520Models%252C%250Aand%2520can%2520be%2520visualized%2520and%2520fully%2520understood%2520by%2520humans.%2520GNAN%2520is%2520designed%2520to%2520be%250Afully%2520interpretable%252C%2520allowing%2520both%2520global%2520and%2520local%2520explanations%2520at%2520the%2520feature%250Aand%2520graph%2520levels%2520through%2520direct%2520visualization%2520of%2520the%2520model.%2520These%250Avisualizations%2520describe%2520the%2520exact%2520way%2520the%2520model%2520uses%2520the%2520relationships%2520between%250Athe%2520target%2520variable%252C%2520the%2520features%252C%2520and%2520the%2520graph.%2520We%2520demonstrate%2520the%250Aintelligibility%2520of%2520GNANs%2520in%2520a%2520series%2520of%2520examples%2520on%2520different%2520tasks%2520and%250Adatasets.%2520In%2520addition%252C%2520we%2520show%2520that%2520the%2520accuracy%2520of%2520GNAN%2520is%2520on%2520par%2520with%250Ablack-box%2520GNNs%252C%2520making%2520it%2520suitable%2520for%2520critical%2520applications%2520where%2520transparency%250Ais%2520essential%252C%2520alongside%2520high%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01317v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Intelligible%20and%20Effective%20Graph%20Neural%20Additive%20Networks&entry.906535625=Maya%20Bechler-Speicher%20and%20Amir%20Globerson%20and%20Ran%20Gilad-Bachrach&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20emerged%20as%20the%20predominant%20approach%20for%0Alearning%20over%20graph-structured%20data.%20However%2C%20most%20GNNs%20operate%20as%20black-box%0Amodels%20and%20require%20post-hoc%20explanations%2C%20which%20may%20not%20suffice%20in%20high-stakes%0Ascenarios%20where%20transparency%20is%20crucial.%20In%20this%20paper%2C%20we%20present%20a%20GNN%20that%0Ais%20interpretable%20by%20design.%20Our%20model%2C%20Graph%20Neural%20Additive%20Network%20%28GNAN%29%2C%20is%0Aa%20novel%20extension%20of%20the%20interpretable%20class%20of%20Generalized%20Additive%20Models%2C%0Aand%20can%20be%20visualized%20and%20fully%20understood%20by%20humans.%20GNAN%20is%20designed%20to%20be%0Afully%20interpretable%2C%20allowing%20both%20global%20and%20local%20explanations%20at%20the%20feature%0Aand%20graph%20levels%20through%20direct%20visualization%20of%20the%20model.%20These%0Avisualizations%20describe%20the%20exact%20way%20the%20model%20uses%20the%20relationships%20between%0Athe%20target%20variable%2C%20the%20features%2C%20and%20the%20graph.%20We%20demonstrate%20the%0Aintelligibility%20of%20GNANs%20in%20a%20series%20of%20examples%20on%20different%20tasks%20and%0Adatasets.%20In%20addition%2C%20we%20show%20that%20the%20accuracy%20of%20GNAN%20is%20on%20par%20with%0Ablack-box%20GNNs%2C%20making%20it%20suitable%20for%20critical%20applications%20where%20transparency%0Ais%20essential%2C%20alongside%20high%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.01317v2&entry.124074799=Read"},
{"title": "Integrating occlusion awareness in urban motion prediction for enhanced\n  autonomous vehicle navigation", "author": "Vinicius Trentin and Juan Medina-Lee and Antonio Artu\u00f1edo and Jorge Villagra", "abstract": "  Motion prediction is a key factor towards the full deployment of autonomous\nvehicles. It is fundamental in order to ensure safety while navigating through\nhighly interactive and complex scenarios. Lack of visibility due to an\nobstructed view or sensor range poses a great safety issue for autonomous\nvehicles. The inclusion of occlusion in interaction-aware approaches is not\nvery well explored in the literature. In this work, the MultIAMP framework,\nwhich produces multimodal probabilistic outputs from the integration of a\nDynamic Bayesian Network and Markov chains, is extended to tackle occlusions.\nThe framework is evaluated with a state-of-the-art motion planner in two\nrealistic use cases.\n", "link": "http://arxiv.org/abs/2406.19798v1", "date": "2024-06-28", "relevancy": 2.3743, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6279}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6226}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrating%20occlusion%20awareness%20in%20urban%20motion%20prediction%20for%20enhanced%0A%20%20autonomous%20vehicle%20navigation&body=Title%3A%20Integrating%20occlusion%20awareness%20in%20urban%20motion%20prediction%20for%20enhanced%0A%20%20autonomous%20vehicle%20navigation%0AAuthor%3A%20Vinicius%20Trentin%20and%20Juan%20Medina-Lee%20and%20Antonio%20Artu%C3%B1edo%20and%20Jorge%20Villagra%0AAbstract%3A%20%20%20Motion%20prediction%20is%20a%20key%20factor%20towards%20the%20full%20deployment%20of%20autonomous%0Avehicles.%20It%20is%20fundamental%20in%20order%20to%20ensure%20safety%20while%20navigating%20through%0Ahighly%20interactive%20and%20complex%20scenarios.%20Lack%20of%20visibility%20due%20to%20an%0Aobstructed%20view%20or%20sensor%20range%20poses%20a%20great%20safety%20issue%20for%20autonomous%0Avehicles.%20The%20inclusion%20of%20occlusion%20in%20interaction-aware%20approaches%20is%20not%0Avery%20well%20explored%20in%20the%20literature.%20In%20this%20work%2C%20the%20MultIAMP%20framework%2C%0Awhich%20produces%20multimodal%20probabilistic%20outputs%20from%20the%20integration%20of%20a%0ADynamic%20Bayesian%20Network%20and%20Markov%20chains%2C%20is%20extended%20to%20tackle%20occlusions.%0AThe%20framework%20is%20evaluated%20with%20a%20state-of-the-art%20motion%20planner%20in%20two%0Arealistic%20use%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19798v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrating%2520occlusion%2520awareness%2520in%2520urban%2520motion%2520prediction%2520for%2520enhanced%250A%2520%2520autonomous%2520vehicle%2520navigation%26entry.906535625%3DVinicius%2520Trentin%2520and%2520Juan%2520Medina-Lee%2520and%2520Antonio%2520Artu%25C3%25B1edo%2520and%2520Jorge%2520Villagra%26entry.1292438233%3D%2520%2520Motion%2520prediction%2520is%2520a%2520key%2520factor%2520towards%2520the%2520full%2520deployment%2520of%2520autonomous%250Avehicles.%2520It%2520is%2520fundamental%2520in%2520order%2520to%2520ensure%2520safety%2520while%2520navigating%2520through%250Ahighly%2520interactive%2520and%2520complex%2520scenarios.%2520Lack%2520of%2520visibility%2520due%2520to%2520an%250Aobstructed%2520view%2520or%2520sensor%2520range%2520poses%2520a%2520great%2520safety%2520issue%2520for%2520autonomous%250Avehicles.%2520The%2520inclusion%2520of%2520occlusion%2520in%2520interaction-aware%2520approaches%2520is%2520not%250Avery%2520well%2520explored%2520in%2520the%2520literature.%2520In%2520this%2520work%252C%2520the%2520MultIAMP%2520framework%252C%250Awhich%2520produces%2520multimodal%2520probabilistic%2520outputs%2520from%2520the%2520integration%2520of%2520a%250ADynamic%2520Bayesian%2520Network%2520and%2520Markov%2520chains%252C%2520is%2520extended%2520to%2520tackle%2520occlusions.%250AThe%2520framework%2520is%2520evaluated%2520with%2520a%2520state-of-the-art%2520motion%2520planner%2520in%2520two%250Arealistic%2520use%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19798v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20occlusion%20awareness%20in%20urban%20motion%20prediction%20for%20enhanced%0A%20%20autonomous%20vehicle%20navigation&entry.906535625=Vinicius%20Trentin%20and%20Juan%20Medina-Lee%20and%20Antonio%20Artu%C3%B1edo%20and%20Jorge%20Villagra&entry.1292438233=%20%20Motion%20prediction%20is%20a%20key%20factor%20towards%20the%20full%20deployment%20of%20autonomous%0Avehicles.%20It%20is%20fundamental%20in%20order%20to%20ensure%20safety%20while%20navigating%20through%0Ahighly%20interactive%20and%20complex%20scenarios.%20Lack%20of%20visibility%20due%20to%20an%0Aobstructed%20view%20or%20sensor%20range%20poses%20a%20great%20safety%20issue%20for%20autonomous%0Avehicles.%20The%20inclusion%20of%20occlusion%20in%20interaction-aware%20approaches%20is%20not%0Avery%20well%20explored%20in%20the%20literature.%20In%20this%20work%2C%20the%20MultIAMP%20framework%2C%0Awhich%20produces%20multimodal%20probabilistic%20outputs%20from%20the%20integration%20of%20a%0ADynamic%20Bayesian%20Network%20and%20Markov%20chains%2C%20is%20extended%20to%20tackle%20occlusions.%0AThe%20framework%20is%20evaluated%20with%20a%20state-of-the-art%20motion%20planner%20in%20two%0Arealistic%20use%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19798v1&entry.124074799=Read"},
{"title": "Fishnets: Information-Optimal, Scalable Aggregation for Sets and Graphs", "author": "T. Lucas Makinen and Justin Alsing and Benjamin D. Wandelt", "abstract": "  Set-based learning is an essential component of modern deep learning and\nnetwork science. Graph Neural Networks (GNNs) and their edge-free counterparts\nDeepsets have proven remarkably useful on ragged and topologically challenging\ndatasets. The key to learning informative embeddings for set members is a\nspecified aggregation function, usually a sum, max, or mean. We propose\nFishnets, an aggregation strategy for learning information-optimal embeddings\nfor sets of data for both Bayesian inference and graph aggregation. We\ndemonstrate that i) Fishnets neural summaries can be scaled optimally to an\narbitrary number of data objects, ii) Fishnets aggregations are robust to\nchanges in data distribution, unlike standard deepsets, iii) Fishnets saturate\nBayesian information content and extend to regimes where MCMC techniques fail\nand iv) Fishnets can be used as a drop-in aggregation scheme within GNNs. We\nshow that by adopting a Fishnets aggregation scheme for message passing, GNNs\ncan achieve state-of-the-art performance versus architecture size on\nogbn-protein data over existing benchmarks with a fraction of learnable\nparameters and faster training time.\n", "link": "http://arxiv.org/abs/2310.03812v2", "date": "2024-06-28", "relevancy": 2.3597, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5001}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4777}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4381}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fishnets%3A%20Information-Optimal%2C%20Scalable%20Aggregation%20for%20Sets%20and%20Graphs&body=Title%3A%20Fishnets%3A%20Information-Optimal%2C%20Scalable%20Aggregation%20for%20Sets%20and%20Graphs%0AAuthor%3A%20T.%20Lucas%20Makinen%20and%20Justin%20Alsing%20and%20Benjamin%20D.%20Wandelt%0AAbstract%3A%20%20%20Set-based%20learning%20is%20an%20essential%20component%20of%20modern%20deep%20learning%20and%0Anetwork%20science.%20Graph%20Neural%20Networks%20%28GNNs%29%20and%20their%20edge-free%20counterparts%0ADeepsets%20have%20proven%20remarkably%20useful%20on%20ragged%20and%20topologically%20challenging%0Adatasets.%20The%20key%20to%20learning%20informative%20embeddings%20for%20set%20members%20is%20a%0Aspecified%20aggregation%20function%2C%20usually%20a%20sum%2C%20max%2C%20or%20mean.%20We%20propose%0AFishnets%2C%20an%20aggregation%20strategy%20for%20learning%20information-optimal%20embeddings%0Afor%20sets%20of%20data%20for%20both%20Bayesian%20inference%20and%20graph%20aggregation.%20We%0Ademonstrate%20that%20i%29%20Fishnets%20neural%20summaries%20can%20be%20scaled%20optimally%20to%20an%0Aarbitrary%20number%20of%20data%20objects%2C%20ii%29%20Fishnets%20aggregations%20are%20robust%20to%0Achanges%20in%20data%20distribution%2C%20unlike%20standard%20deepsets%2C%20iii%29%20Fishnets%20saturate%0ABayesian%20information%20content%20and%20extend%20to%20regimes%20where%20MCMC%20techniques%20fail%0Aand%20iv%29%20Fishnets%20can%20be%20used%20as%20a%20drop-in%20aggregation%20scheme%20within%20GNNs.%20We%0Ashow%20that%20by%20adopting%20a%20Fishnets%20aggregation%20scheme%20for%20message%20passing%2C%20GNNs%0Acan%20achieve%20state-of-the-art%20performance%20versus%20architecture%20size%20on%0Aogbn-protein%20data%20over%20existing%20benchmarks%20with%20a%20fraction%20of%20learnable%0Aparameters%20and%20faster%20training%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.03812v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFishnets%253A%2520Information-Optimal%252C%2520Scalable%2520Aggregation%2520for%2520Sets%2520and%2520Graphs%26entry.906535625%3DT.%2520Lucas%2520Makinen%2520and%2520Justin%2520Alsing%2520and%2520Benjamin%2520D.%2520Wandelt%26entry.1292438233%3D%2520%2520Set-based%2520learning%2520is%2520an%2520essential%2520component%2520of%2520modern%2520deep%2520learning%2520and%250Anetwork%2520science.%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520and%2520their%2520edge-free%2520counterparts%250ADeepsets%2520have%2520proven%2520remarkably%2520useful%2520on%2520ragged%2520and%2520topologically%2520challenging%250Adatasets.%2520The%2520key%2520to%2520learning%2520informative%2520embeddings%2520for%2520set%2520members%2520is%2520a%250Aspecified%2520aggregation%2520function%252C%2520usually%2520a%2520sum%252C%2520max%252C%2520or%2520mean.%2520We%2520propose%250AFishnets%252C%2520an%2520aggregation%2520strategy%2520for%2520learning%2520information-optimal%2520embeddings%250Afor%2520sets%2520of%2520data%2520for%2520both%2520Bayesian%2520inference%2520and%2520graph%2520aggregation.%2520We%250Ademonstrate%2520that%2520i%2529%2520Fishnets%2520neural%2520summaries%2520can%2520be%2520scaled%2520optimally%2520to%2520an%250Aarbitrary%2520number%2520of%2520data%2520objects%252C%2520ii%2529%2520Fishnets%2520aggregations%2520are%2520robust%2520to%250Achanges%2520in%2520data%2520distribution%252C%2520unlike%2520standard%2520deepsets%252C%2520iii%2529%2520Fishnets%2520saturate%250ABayesian%2520information%2520content%2520and%2520extend%2520to%2520regimes%2520where%2520MCMC%2520techniques%2520fail%250Aand%2520iv%2529%2520Fishnets%2520can%2520be%2520used%2520as%2520a%2520drop-in%2520aggregation%2520scheme%2520within%2520GNNs.%2520We%250Ashow%2520that%2520by%2520adopting%2520a%2520Fishnets%2520aggregation%2520scheme%2520for%2520message%2520passing%252C%2520GNNs%250Acan%2520achieve%2520state-of-the-art%2520performance%2520versus%2520architecture%2520size%2520on%250Aogbn-protein%2520data%2520over%2520existing%2520benchmarks%2520with%2520a%2520fraction%2520of%2520learnable%250Aparameters%2520and%2520faster%2520training%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.03812v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fishnets%3A%20Information-Optimal%2C%20Scalable%20Aggregation%20for%20Sets%20and%20Graphs&entry.906535625=T.%20Lucas%20Makinen%20and%20Justin%20Alsing%20and%20Benjamin%20D.%20Wandelt&entry.1292438233=%20%20Set-based%20learning%20is%20an%20essential%20component%20of%20modern%20deep%20learning%20and%0Anetwork%20science.%20Graph%20Neural%20Networks%20%28GNNs%29%20and%20their%20edge-free%20counterparts%0ADeepsets%20have%20proven%20remarkably%20useful%20on%20ragged%20and%20topologically%20challenging%0Adatasets.%20The%20key%20to%20learning%20informative%20embeddings%20for%20set%20members%20is%20a%0Aspecified%20aggregation%20function%2C%20usually%20a%20sum%2C%20max%2C%20or%20mean.%20We%20propose%0AFishnets%2C%20an%20aggregation%20strategy%20for%20learning%20information-optimal%20embeddings%0Afor%20sets%20of%20data%20for%20both%20Bayesian%20inference%20and%20graph%20aggregation.%20We%0Ademonstrate%20that%20i%29%20Fishnets%20neural%20summaries%20can%20be%20scaled%20optimally%20to%20an%0Aarbitrary%20number%20of%20data%20objects%2C%20ii%29%20Fishnets%20aggregations%20are%20robust%20to%0Achanges%20in%20data%20distribution%2C%20unlike%20standard%20deepsets%2C%20iii%29%20Fishnets%20saturate%0ABayesian%20information%20content%20and%20extend%20to%20regimes%20where%20MCMC%20techniques%20fail%0Aand%20iv%29%20Fishnets%20can%20be%20used%20as%20a%20drop-in%20aggregation%20scheme%20within%20GNNs.%20We%0Ashow%20that%20by%20adopting%20a%20Fishnets%20aggregation%20scheme%20for%20message%20passing%2C%20GNNs%0Acan%20achieve%20state-of-the-art%20performance%20versus%20architecture%20size%20on%0Aogbn-protein%20data%20over%20existing%20benchmarks%20with%20a%20fraction%20of%20learnable%0Aparameters%20and%20faster%20training%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.03812v2&entry.124074799=Read"},
{"title": "The Impact of Feature Representation on the Accuracy of Photonic Neural\n  Networks", "author": "Mauricio Gomes de Queiroz and Paul Jimenez and Raphael Cardoso and Mateus Vidaletti Costa and Mohab Abdalla and Ian O'Connor and Alberto Bosio and Fabio Pavanello", "abstract": "  Photonic Neural Networks (PNNs) are gaining significant interest in the\nresearch community due to their potential for high parallelization, low\nlatency, and energy efficiency. PNNs compute using light, which leads to\nseveral differences in implementation when compared to electronics, such as the\nneed to represent input features in the photonic domain before feeding them\ninto the network. In this encoding process, it is common to combine multiple\nfeatures into a single input to reduce the number of inputs and associated\ndevices, leading to smaller and more energy-efficient PNNs. Although this\nalters the network's handling of input data, its impact on PNNs remains\nunderstudied. This paper addresses this open question, investigating the effect\nof commonly used encoding strategies that combine features on the performance\nand learning capabilities of PNNs. Here, using the concept of feature\nimportance, we develop a mathematical methodology for analyzing feature\ncombination. Through this methodology, we demonstrate that encoding multiple\nfeatures together in a single input determines their relative importance, thus\nlimiting the network's ability to learn from the data. Given some prior\nknowledge of the data, however, this can also be leveraged for higher accuracy.\nBy selecting an optimal encoding method, we achieve up to a 12.3% improvement\nin accuracy of PNNs trained on the Iris dataset compared to other encoding\ntechniques, surpassing the performance of networks where features are not\ncombined. These findings highlight the importance of carefully choosing the\nencoding to the accuracy and decision-making strategies of PNNs, particularly\nin size or power constrained applications.\n", "link": "http://arxiv.org/abs/2406.18757v2", "date": "2024-06-28", "relevancy": 2.3441, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.474}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4667}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Impact%20of%20Feature%20Representation%20on%20the%20Accuracy%20of%20Photonic%20Neural%0A%20%20Networks&body=Title%3A%20The%20Impact%20of%20Feature%20Representation%20on%20the%20Accuracy%20of%20Photonic%20Neural%0A%20%20Networks%0AAuthor%3A%20Mauricio%20Gomes%20de%20Queiroz%20and%20Paul%20Jimenez%20and%20Raphael%20Cardoso%20and%20Mateus%20Vidaletti%20Costa%20and%20Mohab%20Abdalla%20and%20Ian%20O%27Connor%20and%20Alberto%20Bosio%20and%20Fabio%20Pavanello%0AAbstract%3A%20%20%20Photonic%20Neural%20Networks%20%28PNNs%29%20are%20gaining%20significant%20interest%20in%20the%0Aresearch%20community%20due%20to%20their%20potential%20for%20high%20parallelization%2C%20low%0Alatency%2C%20and%20energy%20efficiency.%20PNNs%20compute%20using%20light%2C%20which%20leads%20to%0Aseveral%20differences%20in%20implementation%20when%20compared%20to%20electronics%2C%20such%20as%20the%0Aneed%20to%20represent%20input%20features%20in%20the%20photonic%20domain%20before%20feeding%20them%0Ainto%20the%20network.%20In%20this%20encoding%20process%2C%20it%20is%20common%20to%20combine%20multiple%0Afeatures%20into%20a%20single%20input%20to%20reduce%20the%20number%20of%20inputs%20and%20associated%0Adevices%2C%20leading%20to%20smaller%20and%20more%20energy-efficient%20PNNs.%20Although%20this%0Aalters%20the%20network%27s%20handling%20of%20input%20data%2C%20its%20impact%20on%20PNNs%20remains%0Aunderstudied.%20This%20paper%20addresses%20this%20open%20question%2C%20investigating%20the%20effect%0Aof%20commonly%20used%20encoding%20strategies%20that%20combine%20features%20on%20the%20performance%0Aand%20learning%20capabilities%20of%20PNNs.%20Here%2C%20using%20the%20concept%20of%20feature%0Aimportance%2C%20we%20develop%20a%20mathematical%20methodology%20for%20analyzing%20feature%0Acombination.%20Through%20this%20methodology%2C%20we%20demonstrate%20that%20encoding%20multiple%0Afeatures%20together%20in%20a%20single%20input%20determines%20their%20relative%20importance%2C%20thus%0Alimiting%20the%20network%27s%20ability%20to%20learn%20from%20the%20data.%20Given%20some%20prior%0Aknowledge%20of%20the%20data%2C%20however%2C%20this%20can%20also%20be%20leveraged%20for%20higher%20accuracy.%0ABy%20selecting%20an%20optimal%20encoding%20method%2C%20we%20achieve%20up%20to%20a%2012.3%25%20improvement%0Ain%20accuracy%20of%20PNNs%20trained%20on%20the%20Iris%20dataset%20compared%20to%20other%20encoding%0Atechniques%2C%20surpassing%20the%20performance%20of%20networks%20where%20features%20are%20not%0Acombined.%20These%20findings%20highlight%20the%20importance%20of%20carefully%20choosing%20the%0Aencoding%20to%20the%20accuracy%20and%20decision-making%20strategies%20of%20PNNs%2C%20particularly%0Ain%20size%20or%20power%20constrained%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18757v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Impact%2520of%2520Feature%2520Representation%2520on%2520the%2520Accuracy%2520of%2520Photonic%2520Neural%250A%2520%2520Networks%26entry.906535625%3DMauricio%2520Gomes%2520de%2520Queiroz%2520and%2520Paul%2520Jimenez%2520and%2520Raphael%2520Cardoso%2520and%2520Mateus%2520Vidaletti%2520Costa%2520and%2520Mohab%2520Abdalla%2520and%2520Ian%2520O%2527Connor%2520and%2520Alberto%2520Bosio%2520and%2520Fabio%2520Pavanello%26entry.1292438233%3D%2520%2520Photonic%2520Neural%2520Networks%2520%2528PNNs%2529%2520are%2520gaining%2520significant%2520interest%2520in%2520the%250Aresearch%2520community%2520due%2520to%2520their%2520potential%2520for%2520high%2520parallelization%252C%2520low%250Alatency%252C%2520and%2520energy%2520efficiency.%2520PNNs%2520compute%2520using%2520light%252C%2520which%2520leads%2520to%250Aseveral%2520differences%2520in%2520implementation%2520when%2520compared%2520to%2520electronics%252C%2520such%2520as%2520the%250Aneed%2520to%2520represent%2520input%2520features%2520in%2520the%2520photonic%2520domain%2520before%2520feeding%2520them%250Ainto%2520the%2520network.%2520In%2520this%2520encoding%2520process%252C%2520it%2520is%2520common%2520to%2520combine%2520multiple%250Afeatures%2520into%2520a%2520single%2520input%2520to%2520reduce%2520the%2520number%2520of%2520inputs%2520and%2520associated%250Adevices%252C%2520leading%2520to%2520smaller%2520and%2520more%2520energy-efficient%2520PNNs.%2520Although%2520this%250Aalters%2520the%2520network%2527s%2520handling%2520of%2520input%2520data%252C%2520its%2520impact%2520on%2520PNNs%2520remains%250Aunderstudied.%2520This%2520paper%2520addresses%2520this%2520open%2520question%252C%2520investigating%2520the%2520effect%250Aof%2520commonly%2520used%2520encoding%2520strategies%2520that%2520combine%2520features%2520on%2520the%2520performance%250Aand%2520learning%2520capabilities%2520of%2520PNNs.%2520Here%252C%2520using%2520the%2520concept%2520of%2520feature%250Aimportance%252C%2520we%2520develop%2520a%2520mathematical%2520methodology%2520for%2520analyzing%2520feature%250Acombination.%2520Through%2520this%2520methodology%252C%2520we%2520demonstrate%2520that%2520encoding%2520multiple%250Afeatures%2520together%2520in%2520a%2520single%2520input%2520determines%2520their%2520relative%2520importance%252C%2520thus%250Alimiting%2520the%2520network%2527s%2520ability%2520to%2520learn%2520from%2520the%2520data.%2520Given%2520some%2520prior%250Aknowledge%2520of%2520the%2520data%252C%2520however%252C%2520this%2520can%2520also%2520be%2520leveraged%2520for%2520higher%2520accuracy.%250ABy%2520selecting%2520an%2520optimal%2520encoding%2520method%252C%2520we%2520achieve%2520up%2520to%2520a%252012.3%2525%2520improvement%250Ain%2520accuracy%2520of%2520PNNs%2520trained%2520on%2520the%2520Iris%2520dataset%2520compared%2520to%2520other%2520encoding%250Atechniques%252C%2520surpassing%2520the%2520performance%2520of%2520networks%2520where%2520features%2520are%2520not%250Acombined.%2520These%2520findings%2520highlight%2520the%2520importance%2520of%2520carefully%2520choosing%2520the%250Aencoding%2520to%2520the%2520accuracy%2520and%2520decision-making%2520strategies%2520of%2520PNNs%252C%2520particularly%250Ain%2520size%2520or%2520power%2520constrained%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18757v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Impact%20of%20Feature%20Representation%20on%20the%20Accuracy%20of%20Photonic%20Neural%0A%20%20Networks&entry.906535625=Mauricio%20Gomes%20de%20Queiroz%20and%20Paul%20Jimenez%20and%20Raphael%20Cardoso%20and%20Mateus%20Vidaletti%20Costa%20and%20Mohab%20Abdalla%20and%20Ian%20O%27Connor%20and%20Alberto%20Bosio%20and%20Fabio%20Pavanello&entry.1292438233=%20%20Photonic%20Neural%20Networks%20%28PNNs%29%20are%20gaining%20significant%20interest%20in%20the%0Aresearch%20community%20due%20to%20their%20potential%20for%20high%20parallelization%2C%20low%0Alatency%2C%20and%20energy%20efficiency.%20PNNs%20compute%20using%20light%2C%20which%20leads%20to%0Aseveral%20differences%20in%20implementation%20when%20compared%20to%20electronics%2C%20such%20as%20the%0Aneed%20to%20represent%20input%20features%20in%20the%20photonic%20domain%20before%20feeding%20them%0Ainto%20the%20network.%20In%20this%20encoding%20process%2C%20it%20is%20common%20to%20combine%20multiple%0Afeatures%20into%20a%20single%20input%20to%20reduce%20the%20number%20of%20inputs%20and%20associated%0Adevices%2C%20leading%20to%20smaller%20and%20more%20energy-efficient%20PNNs.%20Although%20this%0Aalters%20the%20network%27s%20handling%20of%20input%20data%2C%20its%20impact%20on%20PNNs%20remains%0Aunderstudied.%20This%20paper%20addresses%20this%20open%20question%2C%20investigating%20the%20effect%0Aof%20commonly%20used%20encoding%20strategies%20that%20combine%20features%20on%20the%20performance%0Aand%20learning%20capabilities%20of%20PNNs.%20Here%2C%20using%20the%20concept%20of%20feature%0Aimportance%2C%20we%20develop%20a%20mathematical%20methodology%20for%20analyzing%20feature%0Acombination.%20Through%20this%20methodology%2C%20we%20demonstrate%20that%20encoding%20multiple%0Afeatures%20together%20in%20a%20single%20input%20determines%20their%20relative%20importance%2C%20thus%0Alimiting%20the%20network%27s%20ability%20to%20learn%20from%20the%20data.%20Given%20some%20prior%0Aknowledge%20of%20the%20data%2C%20however%2C%20this%20can%20also%20be%20leveraged%20for%20higher%20accuracy.%0ABy%20selecting%20an%20optimal%20encoding%20method%2C%20we%20achieve%20up%20to%20a%2012.3%25%20improvement%0Ain%20accuracy%20of%20PNNs%20trained%20on%20the%20Iris%20dataset%20compared%20to%20other%20encoding%0Atechniques%2C%20surpassing%20the%20performance%20of%20networks%20where%20features%20are%20not%0Acombined.%20These%20findings%20highlight%20the%20importance%20of%20carefully%20choosing%20the%0Aencoding%20to%20the%20accuracy%20and%20decision-making%20strategies%20of%20PNNs%2C%20particularly%0Ain%20size%20or%20power%20constrained%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18757v2&entry.124074799=Read"},
{"title": "Extract More from Less: Efficient Fine-Grained Visual Recognition in\n  Low-Data Regimes", "author": "Dmitry Demidov and Abduragim Shtanchaev and Mihail Mihaylov and Mohammad Almansoori", "abstract": "  The emerging task of fine-grained image classification in low-data regimes\nassumes the presence of low inter-class variance and large intra-class\nvariation along with a highly limited amount of training samples per class.\nHowever, traditional ways of separately dealing with fine-grained\ncategorisation and extremely scarce data may be inefficient under both these\nharsh conditions presented together. In this paper, we present a novel\nframework, called AD-Net, aiming to enhance deep neural network performance on\nthis challenge by leveraging the power of Augmentation and Distillation\ntechniques. Specifically, our approach is designed to refine learned features\nthrough self-distillation on augmented samples, mitigating harmful overfitting.\nWe conduct comprehensive experiments on popular fine-grained image\nclassification benchmarks where our AD-Net demonstrates consistent improvement\nover traditional fine-tuning and state-of-the-art low-data techniques.\nRemarkably, with the smallest data available, our framework shows an\noutstanding relative accuracy increase of up to 45 % compared to standard\nResNet-50 and up to 27 % compared to the closest SOTA runner-up. We emphasise\nthat our approach is practically architecture-independent and adds zero extra\ncost at inference time. Additionally, we provide an extensive study on the\nimpact of every framework's component, highlighting the importance of each in\nachieving optimal performance. Source code and trained models are publicly\navailable at github.com/demidovd98/fgic_lowd.\n", "link": "http://arxiv.org/abs/2406.19814v1", "date": "2024-06-28", "relevancy": 2.3441, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6021}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5771}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5736}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Extract%20More%20from%20Less%3A%20Efficient%20Fine-Grained%20Visual%20Recognition%20in%0A%20%20Low-Data%20Regimes&body=Title%3A%20Extract%20More%20from%20Less%3A%20Efficient%20Fine-Grained%20Visual%20Recognition%20in%0A%20%20Low-Data%20Regimes%0AAuthor%3A%20Dmitry%20Demidov%20and%20Abduragim%20Shtanchaev%20and%20Mihail%20Mihaylov%20and%20Mohammad%20Almansoori%0AAbstract%3A%20%20%20The%20emerging%20task%20of%20fine-grained%20image%20classification%20in%20low-data%20regimes%0Aassumes%20the%20presence%20of%20low%20inter-class%20variance%20and%20large%20intra-class%0Avariation%20along%20with%20a%20highly%20limited%20amount%20of%20training%20samples%20per%20class.%0AHowever%2C%20traditional%20ways%20of%20separately%20dealing%20with%20fine-grained%0Acategorisation%20and%20extremely%20scarce%20data%20may%20be%20inefficient%20under%20both%20these%0Aharsh%20conditions%20presented%20together.%20In%20this%20paper%2C%20we%20present%20a%20novel%0Aframework%2C%20called%20AD-Net%2C%20aiming%20to%20enhance%20deep%20neural%20network%20performance%20on%0Athis%20challenge%20by%20leveraging%20the%20power%20of%20Augmentation%20and%20Distillation%0Atechniques.%20Specifically%2C%20our%20approach%20is%20designed%20to%20refine%20learned%20features%0Athrough%20self-distillation%20on%20augmented%20samples%2C%20mitigating%20harmful%20overfitting.%0AWe%20conduct%20comprehensive%20experiments%20on%20popular%20fine-grained%20image%0Aclassification%20benchmarks%20where%20our%20AD-Net%20demonstrates%20consistent%20improvement%0Aover%20traditional%20fine-tuning%20and%20state-of-the-art%20low-data%20techniques.%0ARemarkably%2C%20with%20the%20smallest%20data%20available%2C%20our%20framework%20shows%20an%0Aoutstanding%20relative%20accuracy%20increase%20of%20up%20to%2045%20%25%20compared%20to%20standard%0AResNet-50%20and%20up%20to%2027%20%25%20compared%20to%20the%20closest%20SOTA%20runner-up.%20We%20emphasise%0Athat%20our%20approach%20is%20practically%20architecture-independent%20and%20adds%20zero%20extra%0Acost%20at%20inference%20time.%20Additionally%2C%20we%20provide%20an%20extensive%20study%20on%20the%0Aimpact%20of%20every%20framework%27s%20component%2C%20highlighting%20the%20importance%20of%20each%20in%0Aachieving%20optimal%20performance.%20Source%20code%20and%20trained%20models%20are%20publicly%0Aavailable%20at%20github.com/demidovd98/fgic_lowd.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19814v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExtract%2520More%2520from%2520Less%253A%2520Efficient%2520Fine-Grained%2520Visual%2520Recognition%2520in%250A%2520%2520Low-Data%2520Regimes%26entry.906535625%3DDmitry%2520Demidov%2520and%2520Abduragim%2520Shtanchaev%2520and%2520Mihail%2520Mihaylov%2520and%2520Mohammad%2520Almansoori%26entry.1292438233%3D%2520%2520The%2520emerging%2520task%2520of%2520fine-grained%2520image%2520classification%2520in%2520low-data%2520regimes%250Aassumes%2520the%2520presence%2520of%2520low%2520inter-class%2520variance%2520and%2520large%2520intra-class%250Avariation%2520along%2520with%2520a%2520highly%2520limited%2520amount%2520of%2520training%2520samples%2520per%2520class.%250AHowever%252C%2520traditional%2520ways%2520of%2520separately%2520dealing%2520with%2520fine-grained%250Acategorisation%2520and%2520extremely%2520scarce%2520data%2520may%2520be%2520inefficient%2520under%2520both%2520these%250Aharsh%2520conditions%2520presented%2520together.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%250Aframework%252C%2520called%2520AD-Net%252C%2520aiming%2520to%2520enhance%2520deep%2520neural%2520network%2520performance%2520on%250Athis%2520challenge%2520by%2520leveraging%2520the%2520power%2520of%2520Augmentation%2520and%2520Distillation%250Atechniques.%2520Specifically%252C%2520our%2520approach%2520is%2520designed%2520to%2520refine%2520learned%2520features%250Athrough%2520self-distillation%2520on%2520augmented%2520samples%252C%2520mitigating%2520harmful%2520overfitting.%250AWe%2520conduct%2520comprehensive%2520experiments%2520on%2520popular%2520fine-grained%2520image%250Aclassification%2520benchmarks%2520where%2520our%2520AD-Net%2520demonstrates%2520consistent%2520improvement%250Aover%2520traditional%2520fine-tuning%2520and%2520state-of-the-art%2520low-data%2520techniques.%250ARemarkably%252C%2520with%2520the%2520smallest%2520data%2520available%252C%2520our%2520framework%2520shows%2520an%250Aoutstanding%2520relative%2520accuracy%2520increase%2520of%2520up%2520to%252045%2520%2525%2520compared%2520to%2520standard%250AResNet-50%2520and%2520up%2520to%252027%2520%2525%2520compared%2520to%2520the%2520closest%2520SOTA%2520runner-up.%2520We%2520emphasise%250Athat%2520our%2520approach%2520is%2520practically%2520architecture-independent%2520and%2520adds%2520zero%2520extra%250Acost%2520at%2520inference%2520time.%2520Additionally%252C%2520we%2520provide%2520an%2520extensive%2520study%2520on%2520the%250Aimpact%2520of%2520every%2520framework%2527s%2520component%252C%2520highlighting%2520the%2520importance%2520of%2520each%2520in%250Aachieving%2520optimal%2520performance.%2520Source%2520code%2520and%2520trained%2520models%2520are%2520publicly%250Aavailable%2520at%2520github.com/demidovd98/fgic_lowd.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19814v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Extract%20More%20from%20Less%3A%20Efficient%20Fine-Grained%20Visual%20Recognition%20in%0A%20%20Low-Data%20Regimes&entry.906535625=Dmitry%20Demidov%20and%20Abduragim%20Shtanchaev%20and%20Mihail%20Mihaylov%20and%20Mohammad%20Almansoori&entry.1292438233=%20%20The%20emerging%20task%20of%20fine-grained%20image%20classification%20in%20low-data%20regimes%0Aassumes%20the%20presence%20of%20low%20inter-class%20variance%20and%20large%20intra-class%0Avariation%20along%20with%20a%20highly%20limited%20amount%20of%20training%20samples%20per%20class.%0AHowever%2C%20traditional%20ways%20of%20separately%20dealing%20with%20fine-grained%0Acategorisation%20and%20extremely%20scarce%20data%20may%20be%20inefficient%20under%20both%20these%0Aharsh%20conditions%20presented%20together.%20In%20this%20paper%2C%20we%20present%20a%20novel%0Aframework%2C%20called%20AD-Net%2C%20aiming%20to%20enhance%20deep%20neural%20network%20performance%20on%0Athis%20challenge%20by%20leveraging%20the%20power%20of%20Augmentation%20and%20Distillation%0Atechniques.%20Specifically%2C%20our%20approach%20is%20designed%20to%20refine%20learned%20features%0Athrough%20self-distillation%20on%20augmented%20samples%2C%20mitigating%20harmful%20overfitting.%0AWe%20conduct%20comprehensive%20experiments%20on%20popular%20fine-grained%20image%0Aclassification%20benchmarks%20where%20our%20AD-Net%20demonstrates%20consistent%20improvement%0Aover%20traditional%20fine-tuning%20and%20state-of-the-art%20low-data%20techniques.%0ARemarkably%2C%20with%20the%20smallest%20data%20available%2C%20our%20framework%20shows%20an%0Aoutstanding%20relative%20accuracy%20increase%20of%20up%20to%2045%20%25%20compared%20to%20standard%0AResNet-50%20and%20up%20to%2027%20%25%20compared%20to%20the%20closest%20SOTA%20runner-up.%20We%20emphasise%0Athat%20our%20approach%20is%20practically%20architecture-independent%20and%20adds%20zero%20extra%0Acost%20at%20inference%20time.%20Additionally%2C%20we%20provide%20an%20extensive%20study%20on%20the%0Aimpact%20of%20every%20framework%27s%20component%2C%20highlighting%20the%20importance%20of%20each%20in%0Aachieving%20optimal%20performance.%20Source%20code%20and%20trained%20models%20are%20publicly%0Aavailable%20at%20github.com/demidovd98/fgic_lowd.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19814v1&entry.124074799=Read"},
{"title": "Modeling the Real World with High-Density Visual Particle Dynamics", "author": "William F. Whitney and Jacob Varley and Deepali Jain and Krzysztof Choromanski and Sumeet Singh and Vikas Sindhwani", "abstract": "  We present High-Density Visual Particle Dynamics (HD-VPD), a learned world\nmodel that can emulate the physical dynamics of real scenes by processing\nmassive latent point clouds containing 100K+ particles. To enable efficiency at\nthis scale, we introduce a novel family of Point Cloud Transformers (PCTs)\ncalled Interlacers leveraging intertwined linear-attention Performer layers and\ngraph-based neighbour attention layers. We demonstrate the capabilities of\nHD-VPD by modeling the dynamics of high degree-of-freedom bi-manual robots with\ntwo RGB-D cameras. Compared to the previous graph neural network approach, our\nInterlacer dynamics is twice as fast with the same prediction quality, and can\nachieve higher quality using 4x as many particles. We illustrate how HD-VPD can\nevaluate motion plan quality with robotic box pushing and can grasping tasks.\nSee videos and particle dynamics rendered by HD-VPD at\nhttps://sites.google.com/view/hd-vpd.\n", "link": "http://arxiv.org/abs/2406.19800v1", "date": "2024-06-28", "relevancy": 2.3205, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5876}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5825}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modeling%20the%20Real%20World%20with%20High-Density%20Visual%20Particle%20Dynamics&body=Title%3A%20Modeling%20the%20Real%20World%20with%20High-Density%20Visual%20Particle%20Dynamics%0AAuthor%3A%20William%20F.%20Whitney%20and%20Jacob%20Varley%20and%20Deepali%20Jain%20and%20Krzysztof%20Choromanski%20and%20Sumeet%20Singh%20and%20Vikas%20Sindhwani%0AAbstract%3A%20%20%20We%20present%20High-Density%20Visual%20Particle%20Dynamics%20%28HD-VPD%29%2C%20a%20learned%20world%0Amodel%20that%20can%20emulate%20the%20physical%20dynamics%20of%20real%20scenes%20by%20processing%0Amassive%20latent%20point%20clouds%20containing%20100K%2B%20particles.%20To%20enable%20efficiency%20at%0Athis%20scale%2C%20we%20introduce%20a%20novel%20family%20of%20Point%20Cloud%20Transformers%20%28PCTs%29%0Acalled%20Interlacers%20leveraging%20intertwined%20linear-attention%20Performer%20layers%20and%0Agraph-based%20neighbour%20attention%20layers.%20We%20demonstrate%20the%20capabilities%20of%0AHD-VPD%20by%20modeling%20the%20dynamics%20of%20high%20degree-of-freedom%20bi-manual%20robots%20with%0Atwo%20RGB-D%20cameras.%20Compared%20to%20the%20previous%20graph%20neural%20network%20approach%2C%20our%0AInterlacer%20dynamics%20is%20twice%20as%20fast%20with%20the%20same%20prediction%20quality%2C%20and%20can%0Aachieve%20higher%20quality%20using%204x%20as%20many%20particles.%20We%20illustrate%20how%20HD-VPD%20can%0Aevaluate%20motion%20plan%20quality%20with%20robotic%20box%20pushing%20and%20can%20grasping%20tasks.%0ASee%20videos%20and%20particle%20dynamics%20rendered%20by%20HD-VPD%20at%0Ahttps%3A//sites.google.com/view/hd-vpd.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19800v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModeling%2520the%2520Real%2520World%2520with%2520High-Density%2520Visual%2520Particle%2520Dynamics%26entry.906535625%3DWilliam%2520F.%2520Whitney%2520and%2520Jacob%2520Varley%2520and%2520Deepali%2520Jain%2520and%2520Krzysztof%2520Choromanski%2520and%2520Sumeet%2520Singh%2520and%2520Vikas%2520Sindhwani%26entry.1292438233%3D%2520%2520We%2520present%2520High-Density%2520Visual%2520Particle%2520Dynamics%2520%2528HD-VPD%2529%252C%2520a%2520learned%2520world%250Amodel%2520that%2520can%2520emulate%2520the%2520physical%2520dynamics%2520of%2520real%2520scenes%2520by%2520processing%250Amassive%2520latent%2520point%2520clouds%2520containing%2520100K%252B%2520particles.%2520To%2520enable%2520efficiency%2520at%250Athis%2520scale%252C%2520we%2520introduce%2520a%2520novel%2520family%2520of%2520Point%2520Cloud%2520Transformers%2520%2528PCTs%2529%250Acalled%2520Interlacers%2520leveraging%2520intertwined%2520linear-attention%2520Performer%2520layers%2520and%250Agraph-based%2520neighbour%2520attention%2520layers.%2520We%2520demonstrate%2520the%2520capabilities%2520of%250AHD-VPD%2520by%2520modeling%2520the%2520dynamics%2520of%2520high%2520degree-of-freedom%2520bi-manual%2520robots%2520with%250Atwo%2520RGB-D%2520cameras.%2520Compared%2520to%2520the%2520previous%2520graph%2520neural%2520network%2520approach%252C%2520our%250AInterlacer%2520dynamics%2520is%2520twice%2520as%2520fast%2520with%2520the%2520same%2520prediction%2520quality%252C%2520and%2520can%250Aachieve%2520higher%2520quality%2520using%25204x%2520as%2520many%2520particles.%2520We%2520illustrate%2520how%2520HD-VPD%2520can%250Aevaluate%2520motion%2520plan%2520quality%2520with%2520robotic%2520box%2520pushing%2520and%2520can%2520grasping%2520tasks.%250ASee%2520videos%2520and%2520particle%2520dynamics%2520rendered%2520by%2520HD-VPD%2520at%250Ahttps%253A//sites.google.com/view/hd-vpd.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19800v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modeling%20the%20Real%20World%20with%20High-Density%20Visual%20Particle%20Dynamics&entry.906535625=William%20F.%20Whitney%20and%20Jacob%20Varley%20and%20Deepali%20Jain%20and%20Krzysztof%20Choromanski%20and%20Sumeet%20Singh%20and%20Vikas%20Sindhwani&entry.1292438233=%20%20We%20present%20High-Density%20Visual%20Particle%20Dynamics%20%28HD-VPD%29%2C%20a%20learned%20world%0Amodel%20that%20can%20emulate%20the%20physical%20dynamics%20of%20real%20scenes%20by%20processing%0Amassive%20latent%20point%20clouds%20containing%20100K%2B%20particles.%20To%20enable%20efficiency%20at%0Athis%20scale%2C%20we%20introduce%20a%20novel%20family%20of%20Point%20Cloud%20Transformers%20%28PCTs%29%0Acalled%20Interlacers%20leveraging%20intertwined%20linear-attention%20Performer%20layers%20and%0Agraph-based%20neighbour%20attention%20layers.%20We%20demonstrate%20the%20capabilities%20of%0AHD-VPD%20by%20modeling%20the%20dynamics%20of%20high%20degree-of-freedom%20bi-manual%20robots%20with%0Atwo%20RGB-D%20cameras.%20Compared%20to%20the%20previous%20graph%20neural%20network%20approach%2C%20our%0AInterlacer%20dynamics%20is%20twice%20as%20fast%20with%20the%20same%20prediction%20quality%2C%20and%20can%0Aachieve%20higher%20quality%20using%204x%20as%20many%20particles.%20We%20illustrate%20how%20HD-VPD%20can%0Aevaluate%20motion%20plan%20quality%20with%20robotic%20box%20pushing%20and%20can%20grasping%20tasks.%0ASee%20videos%20and%20particle%20dynamics%20rendered%20by%20HD-VPD%20at%0Ahttps%3A//sites.google.com/view/hd-vpd.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19800v1&entry.124074799=Read"},
{"title": "The G-invariant graph Laplacian", "author": "Eitan Rosen and Paulina Hoyos and Xiuyuan Cheng and Joe Kileel and Yoel Shkolnisky", "abstract": "  Graph Laplacian based algorithms for data lying on a manifold have been\nproven effective for tasks such as dimensionality reduction, clustering, and\ndenoising. In this work, we consider data sets whose data points lie on a\nmanifold that is closed under the action of a known unitary matrix Lie group G.\nWe propose to construct the graph Laplacian by incorporating the distances\nbetween all the pairs of points generated by the action of G on the data set.\nWe deem the latter construction the ``G-invariant Graph Laplacian'' (G-GL). We\nshow that the G-GL converges to the Laplace-Beltrami operator on the data\nmanifold, while enjoying a significantly improved convergence rate compared to\nthe standard graph Laplacian which only utilizes the distances between the\npoints in the given data set. Furthermore, we show that the G-GL admits a set\nof eigenfunctions that have the form of certain products between the group\nelements and eigenvectors of certain matrices, which can be estimated from the\ndata efficiently using FFT-type algorithms. We demonstrate our construction and\nits advantages on the problem of filtering data on a noisy manifold closed\nunder the action of the special unitary group SU(2).\n", "link": "http://arxiv.org/abs/2303.17001v4", "date": "2024-06-28", "relevancy": 2.2844, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4906}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4518}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4283}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20G-invariant%20graph%20Laplacian&body=Title%3A%20The%20G-invariant%20graph%20Laplacian%0AAuthor%3A%20Eitan%20Rosen%20and%20Paulina%20Hoyos%20and%20Xiuyuan%20Cheng%20and%20Joe%20Kileel%20and%20Yoel%20Shkolnisky%0AAbstract%3A%20%20%20Graph%20Laplacian%20based%20algorithms%20for%20data%20lying%20on%20a%20manifold%20have%20been%0Aproven%20effective%20for%20tasks%20such%20as%20dimensionality%20reduction%2C%20clustering%2C%20and%0Adenoising.%20In%20this%20work%2C%20we%20consider%20data%20sets%20whose%20data%20points%20lie%20on%20a%0Amanifold%20that%20is%20closed%20under%20the%20action%20of%20a%20known%20unitary%20matrix%20Lie%20group%20G.%0AWe%20propose%20to%20construct%20the%20graph%20Laplacian%20by%20incorporating%20the%20distances%0Abetween%20all%20the%20pairs%20of%20points%20generated%20by%20the%20action%20of%20G%20on%20the%20data%20set.%0AWe%20deem%20the%20latter%20construction%20the%20%60%60G-invariant%20Graph%20Laplacian%27%27%20%28G-GL%29.%20We%0Ashow%20that%20the%20G-GL%20converges%20to%20the%20Laplace-Beltrami%20operator%20on%20the%20data%0Amanifold%2C%20while%20enjoying%20a%20significantly%20improved%20convergence%20rate%20compared%20to%0Athe%20standard%20graph%20Laplacian%20which%20only%20utilizes%20the%20distances%20between%20the%0Apoints%20in%20the%20given%20data%20set.%20Furthermore%2C%20we%20show%20that%20the%20G-GL%20admits%20a%20set%0Aof%20eigenfunctions%20that%20have%20the%20form%20of%20certain%20products%20between%20the%20group%0Aelements%20and%20eigenvectors%20of%20certain%20matrices%2C%20which%20can%20be%20estimated%20from%20the%0Adata%20efficiently%20using%20FFT-type%20algorithms.%20We%20demonstrate%20our%20construction%20and%0Aits%20advantages%20on%20the%20problem%20of%20filtering%20data%20on%20a%20noisy%20manifold%20closed%0Aunder%20the%20action%20of%20the%20special%20unitary%20group%20SU%282%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.17001v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520G-invariant%2520graph%2520Laplacian%26entry.906535625%3DEitan%2520Rosen%2520and%2520Paulina%2520Hoyos%2520and%2520Xiuyuan%2520Cheng%2520and%2520Joe%2520Kileel%2520and%2520Yoel%2520Shkolnisky%26entry.1292438233%3D%2520%2520Graph%2520Laplacian%2520based%2520algorithms%2520for%2520data%2520lying%2520on%2520a%2520manifold%2520have%2520been%250Aproven%2520effective%2520for%2520tasks%2520such%2520as%2520dimensionality%2520reduction%252C%2520clustering%252C%2520and%250Adenoising.%2520In%2520this%2520work%252C%2520we%2520consider%2520data%2520sets%2520whose%2520data%2520points%2520lie%2520on%2520a%250Amanifold%2520that%2520is%2520closed%2520under%2520the%2520action%2520of%2520a%2520known%2520unitary%2520matrix%2520Lie%2520group%2520G.%250AWe%2520propose%2520to%2520construct%2520the%2520graph%2520Laplacian%2520by%2520incorporating%2520the%2520distances%250Abetween%2520all%2520the%2520pairs%2520of%2520points%2520generated%2520by%2520the%2520action%2520of%2520G%2520on%2520the%2520data%2520set.%250AWe%2520deem%2520the%2520latter%2520construction%2520the%2520%2560%2560G-invariant%2520Graph%2520Laplacian%2527%2527%2520%2528G-GL%2529.%2520We%250Ashow%2520that%2520the%2520G-GL%2520converges%2520to%2520the%2520Laplace-Beltrami%2520operator%2520on%2520the%2520data%250Amanifold%252C%2520while%2520enjoying%2520a%2520significantly%2520improved%2520convergence%2520rate%2520compared%2520to%250Athe%2520standard%2520graph%2520Laplacian%2520which%2520only%2520utilizes%2520the%2520distances%2520between%2520the%250Apoints%2520in%2520the%2520given%2520data%2520set.%2520Furthermore%252C%2520we%2520show%2520that%2520the%2520G-GL%2520admits%2520a%2520set%250Aof%2520eigenfunctions%2520that%2520have%2520the%2520form%2520of%2520certain%2520products%2520between%2520the%2520group%250Aelements%2520and%2520eigenvectors%2520of%2520certain%2520matrices%252C%2520which%2520can%2520be%2520estimated%2520from%2520the%250Adata%2520efficiently%2520using%2520FFT-type%2520algorithms.%2520We%2520demonstrate%2520our%2520construction%2520and%250Aits%2520advantages%2520on%2520the%2520problem%2520of%2520filtering%2520data%2520on%2520a%2520noisy%2520manifold%2520closed%250Aunder%2520the%2520action%2520of%2520the%2520special%2520unitary%2520group%2520SU%25282%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.17001v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20G-invariant%20graph%20Laplacian&entry.906535625=Eitan%20Rosen%20and%20Paulina%20Hoyos%20and%20Xiuyuan%20Cheng%20and%20Joe%20Kileel%20and%20Yoel%20Shkolnisky&entry.1292438233=%20%20Graph%20Laplacian%20based%20algorithms%20for%20data%20lying%20on%20a%20manifold%20have%20been%0Aproven%20effective%20for%20tasks%20such%20as%20dimensionality%20reduction%2C%20clustering%2C%20and%0Adenoising.%20In%20this%20work%2C%20we%20consider%20data%20sets%20whose%20data%20points%20lie%20on%20a%0Amanifold%20that%20is%20closed%20under%20the%20action%20of%20a%20known%20unitary%20matrix%20Lie%20group%20G.%0AWe%20propose%20to%20construct%20the%20graph%20Laplacian%20by%20incorporating%20the%20distances%0Abetween%20all%20the%20pairs%20of%20points%20generated%20by%20the%20action%20of%20G%20on%20the%20data%20set.%0AWe%20deem%20the%20latter%20construction%20the%20%60%60G-invariant%20Graph%20Laplacian%27%27%20%28G-GL%29.%20We%0Ashow%20that%20the%20G-GL%20converges%20to%20the%20Laplace-Beltrami%20operator%20on%20the%20data%0Amanifold%2C%20while%20enjoying%20a%20significantly%20improved%20convergence%20rate%20compared%20to%0Athe%20standard%20graph%20Laplacian%20which%20only%20utilizes%20the%20distances%20between%20the%0Apoints%20in%20the%20given%20data%20set.%20Furthermore%2C%20we%20show%20that%20the%20G-GL%20admits%20a%20set%0Aof%20eigenfunctions%20that%20have%20the%20form%20of%20certain%20products%20between%20the%20group%0Aelements%20and%20eigenvectors%20of%20certain%20matrices%2C%20which%20can%20be%20estimated%20from%20the%0Adata%20efficiently%20using%20FFT-type%20algorithms.%20We%20demonstrate%20our%20construction%20and%0Aits%20advantages%20on%20the%20problem%20of%20filtering%20data%20on%20a%20noisy%20manifold%20closed%0Aunder%20the%20action%20of%20the%20special%20unitary%20group%20SU%282%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.17001v4&entry.124074799=Read"},
{"title": "ASSR-NeRF: Arbitrary-Scale Super-Resolution on Voxel Grid for\n  High-Quality Radiance Fields Reconstruction", "author": "Ding-Jiun Huang and Zi-Ting Chou and Yu-Chiang Frank Wang and Cheng Sun", "abstract": "  NeRF-based methods reconstruct 3D scenes by building a radiance field with\nimplicit or explicit representations. While NeRF-based methods can perform\nnovel view synthesis (NVS) at arbitrary scale, the performance in\nhigh-resolution novel view synthesis (HRNVS) with low-resolution (LR)\noptimization often results in oversmoothing. On the other hand, single-image\nsuper-resolution (SR) aims to enhance LR images to HR counterparts but lacks\nmulti-view consistency. To address these challenges, we propose Arbitrary-Scale\nSuper-Resolution NeRF (ASSR-NeRF), a novel framework for super-resolution novel\nview synthesis (SRNVS). We propose an attention-based VoxelGridSR model to\ndirectly perform 3D super-resolution (SR) on the optimized volume. Our model is\ntrained on diverse scenes to ensure generalizability. For unseen scenes trained\nwith LR views, we then can directly apply our VoxelGridSR to further refine the\nvolume and achieve multi-view consistent SR. We demonstrate quantitative and\nqualitatively that the proposed method achieves significant performance in\nSRNVS.\n", "link": "http://arxiv.org/abs/2406.20066v1", "date": "2024-06-28", "relevancy": 2.251, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6189}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5326}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ASSR-NeRF%3A%20Arbitrary-Scale%20Super-Resolution%20on%20Voxel%20Grid%20for%0A%20%20High-Quality%20Radiance%20Fields%20Reconstruction&body=Title%3A%20ASSR-NeRF%3A%20Arbitrary-Scale%20Super-Resolution%20on%20Voxel%20Grid%20for%0A%20%20High-Quality%20Radiance%20Fields%20Reconstruction%0AAuthor%3A%20Ding-Jiun%20Huang%20and%20Zi-Ting%20Chou%20and%20Yu-Chiang%20Frank%20Wang%20and%20Cheng%20Sun%0AAbstract%3A%20%20%20NeRF-based%20methods%20reconstruct%203D%20scenes%20by%20building%20a%20radiance%20field%20with%0Aimplicit%20or%20explicit%20representations.%20While%20NeRF-based%20methods%20can%20perform%0Anovel%20view%20synthesis%20%28NVS%29%20at%20arbitrary%20scale%2C%20the%20performance%20in%0Ahigh-resolution%20novel%20view%20synthesis%20%28HRNVS%29%20with%20low-resolution%20%28LR%29%0Aoptimization%20often%20results%20in%20oversmoothing.%20On%20the%20other%20hand%2C%20single-image%0Asuper-resolution%20%28SR%29%20aims%20to%20enhance%20LR%20images%20to%20HR%20counterparts%20but%20lacks%0Amulti-view%20consistency.%20To%20address%20these%20challenges%2C%20we%20propose%20Arbitrary-Scale%0ASuper-Resolution%20NeRF%20%28ASSR-NeRF%29%2C%20a%20novel%20framework%20for%20super-resolution%20novel%0Aview%20synthesis%20%28SRNVS%29.%20We%20propose%20an%20attention-based%20VoxelGridSR%20model%20to%0Adirectly%20perform%203D%20super-resolution%20%28SR%29%20on%20the%20optimized%20volume.%20Our%20model%20is%0Atrained%20on%20diverse%20scenes%20to%20ensure%20generalizability.%20For%20unseen%20scenes%20trained%0Awith%20LR%20views%2C%20we%20then%20can%20directly%20apply%20our%20VoxelGridSR%20to%20further%20refine%20the%0Avolume%20and%20achieve%20multi-view%20consistent%20SR.%20We%20demonstrate%20quantitative%20and%0Aqualitatively%20that%20the%20proposed%20method%20achieves%20significant%20performance%20in%0ASRNVS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.20066v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DASSR-NeRF%253A%2520Arbitrary-Scale%2520Super-Resolution%2520on%2520Voxel%2520Grid%2520for%250A%2520%2520High-Quality%2520Radiance%2520Fields%2520Reconstruction%26entry.906535625%3DDing-Jiun%2520Huang%2520and%2520Zi-Ting%2520Chou%2520and%2520Yu-Chiang%2520Frank%2520Wang%2520and%2520Cheng%2520Sun%26entry.1292438233%3D%2520%2520NeRF-based%2520methods%2520reconstruct%25203D%2520scenes%2520by%2520building%2520a%2520radiance%2520field%2520with%250Aimplicit%2520or%2520explicit%2520representations.%2520While%2520NeRF-based%2520methods%2520can%2520perform%250Anovel%2520view%2520synthesis%2520%2528NVS%2529%2520at%2520arbitrary%2520scale%252C%2520the%2520performance%2520in%250Ahigh-resolution%2520novel%2520view%2520synthesis%2520%2528HRNVS%2529%2520with%2520low-resolution%2520%2528LR%2529%250Aoptimization%2520often%2520results%2520in%2520oversmoothing.%2520On%2520the%2520other%2520hand%252C%2520single-image%250Asuper-resolution%2520%2528SR%2529%2520aims%2520to%2520enhance%2520LR%2520images%2520to%2520HR%2520counterparts%2520but%2520lacks%250Amulti-view%2520consistency.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520Arbitrary-Scale%250ASuper-Resolution%2520NeRF%2520%2528ASSR-NeRF%2529%252C%2520a%2520novel%2520framework%2520for%2520super-resolution%2520novel%250Aview%2520synthesis%2520%2528SRNVS%2529.%2520We%2520propose%2520an%2520attention-based%2520VoxelGridSR%2520model%2520to%250Adirectly%2520perform%25203D%2520super-resolution%2520%2528SR%2529%2520on%2520the%2520optimized%2520volume.%2520Our%2520model%2520is%250Atrained%2520on%2520diverse%2520scenes%2520to%2520ensure%2520generalizability.%2520For%2520unseen%2520scenes%2520trained%250Awith%2520LR%2520views%252C%2520we%2520then%2520can%2520directly%2520apply%2520our%2520VoxelGridSR%2520to%2520further%2520refine%2520the%250Avolume%2520and%2520achieve%2520multi-view%2520consistent%2520SR.%2520We%2520demonstrate%2520quantitative%2520and%250Aqualitatively%2520that%2520the%2520proposed%2520method%2520achieves%2520significant%2520performance%2520in%250ASRNVS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.20066v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ASSR-NeRF%3A%20Arbitrary-Scale%20Super-Resolution%20on%20Voxel%20Grid%20for%0A%20%20High-Quality%20Radiance%20Fields%20Reconstruction&entry.906535625=Ding-Jiun%20Huang%20and%20Zi-Ting%20Chou%20and%20Yu-Chiang%20Frank%20Wang%20and%20Cheng%20Sun&entry.1292438233=%20%20NeRF-based%20methods%20reconstruct%203D%20scenes%20by%20building%20a%20radiance%20field%20with%0Aimplicit%20or%20explicit%20representations.%20While%20NeRF-based%20methods%20can%20perform%0Anovel%20view%20synthesis%20%28NVS%29%20at%20arbitrary%20scale%2C%20the%20performance%20in%0Ahigh-resolution%20novel%20view%20synthesis%20%28HRNVS%29%20with%20low-resolution%20%28LR%29%0Aoptimization%20often%20results%20in%20oversmoothing.%20On%20the%20other%20hand%2C%20single-image%0Asuper-resolution%20%28SR%29%20aims%20to%20enhance%20LR%20images%20to%20HR%20counterparts%20but%20lacks%0Amulti-view%20consistency.%20To%20address%20these%20challenges%2C%20we%20propose%20Arbitrary-Scale%0ASuper-Resolution%20NeRF%20%28ASSR-NeRF%29%2C%20a%20novel%20framework%20for%20super-resolution%20novel%0Aview%20synthesis%20%28SRNVS%29.%20We%20propose%20an%20attention-based%20VoxelGridSR%20model%20to%0Adirectly%20perform%203D%20super-resolution%20%28SR%29%20on%20the%20optimized%20volume.%20Our%20model%20is%0Atrained%20on%20diverse%20scenes%20to%20ensure%20generalizability.%20For%20unseen%20scenes%20trained%0Awith%20LR%20views%2C%20we%20then%20can%20directly%20apply%20our%20VoxelGridSR%20to%20further%20refine%20the%0Avolume%20and%20achieve%20multi-view%20consistent%20SR.%20We%20demonstrate%20quantitative%20and%0Aqualitatively%20that%20the%20proposed%20method%20achieves%20significant%20performance%20in%0ASRNVS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.20066v1&entry.124074799=Read"},
{"title": "Into the Unknown: Generating Geospatial Descriptions for New\n  Environments", "author": "Tzuf Paz-Argaman and John Palowitch and Sayali Kulkarni and Reut Tsarfaty and Jason Baldridge", "abstract": "  Similar to vision-and-language navigation (VLN) tasks that focus on bridging\nthe gap between vision and language for embodied navigation, the new Rendezvous\n(RVS) task requires reasoning over allocentric spatial relationships\n(independent of the observer's viewpoint) using non-sequential navigation\ninstructions and maps. However, performance substantially drops in new\nenvironments with no training data. Using opensource descriptions paired with\ncoordinates (e.g., Wikipedia) provides training data but suffers from limited\nspatially-oriented text resulting in low geolocation resolution. We propose a\nlarge-scale augmentation method for generating high-quality synthetic data for\nnew environments using readily available geospatial data. Our method constructs\na grounded knowledge-graph, capturing entity relationships. Sampled entities\nand relations (`shop north of school') generate navigation instructions via (i)\ngenerating numerous templates using context-free grammar (CFG) to embed\nspecific entities and relations; (ii) feeding the entities and relation into a\nlarge language model (LLM) for instruction generation. A comprehensive\nevaluation on RVS, showed that our approach improves the 100-meter accuracy by\n45.83% on unseen environments. Furthermore, we demonstrate that models trained\nwith CFG-based augmentation achieve superior performance compared with those\ntrained with LLM-based augmentation, both in unseen and seen environments.\nThese findings suggest that the potential advantages of explicitly structuring\nspatial information for text-based geospatial reasoning in previously unknown,\ncan unlock data-scarce scenarios.\n", "link": "http://arxiv.org/abs/2406.19967v1", "date": "2024-06-28", "relevancy": 2.2277, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6072}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5553}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5384}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Into%20the%20Unknown%3A%20Generating%20Geospatial%20Descriptions%20for%20New%0A%20%20Environments&body=Title%3A%20Into%20the%20Unknown%3A%20Generating%20Geospatial%20Descriptions%20for%20New%0A%20%20Environments%0AAuthor%3A%20Tzuf%20Paz-Argaman%20and%20John%20Palowitch%20and%20Sayali%20Kulkarni%20and%20Reut%20Tsarfaty%20and%20Jason%20Baldridge%0AAbstract%3A%20%20%20Similar%20to%20vision-and-language%20navigation%20%28VLN%29%20tasks%20that%20focus%20on%20bridging%0Athe%20gap%20between%20vision%20and%20language%20for%20embodied%20navigation%2C%20the%20new%20Rendezvous%0A%28RVS%29%20task%20requires%20reasoning%20over%20allocentric%20spatial%20relationships%0A%28independent%20of%20the%20observer%27s%20viewpoint%29%20using%20non-sequential%20navigation%0Ainstructions%20and%20maps.%20However%2C%20performance%20substantially%20drops%20in%20new%0Aenvironments%20with%20no%20training%20data.%20Using%20opensource%20descriptions%20paired%20with%0Acoordinates%20%28e.g.%2C%20Wikipedia%29%20provides%20training%20data%20but%20suffers%20from%20limited%0Aspatially-oriented%20text%20resulting%20in%20low%20geolocation%20resolution.%20We%20propose%20a%0Alarge-scale%20augmentation%20method%20for%20generating%20high-quality%20synthetic%20data%20for%0Anew%20environments%20using%20readily%20available%20geospatial%20data.%20Our%20method%20constructs%0Aa%20grounded%20knowledge-graph%2C%20capturing%20entity%20relationships.%20Sampled%20entities%0Aand%20relations%20%28%60shop%20north%20of%20school%27%29%20generate%20navigation%20instructions%20via%20%28i%29%0Agenerating%20numerous%20templates%20using%20context-free%20grammar%20%28CFG%29%20to%20embed%0Aspecific%20entities%20and%20relations%3B%20%28ii%29%20feeding%20the%20entities%20and%20relation%20into%20a%0Alarge%20language%20model%20%28LLM%29%20for%20instruction%20generation.%20A%20comprehensive%0Aevaluation%20on%20RVS%2C%20showed%20that%20our%20approach%20improves%20the%20100-meter%20accuracy%20by%0A45.83%25%20on%20unseen%20environments.%20Furthermore%2C%20we%20demonstrate%20that%20models%20trained%0Awith%20CFG-based%20augmentation%20achieve%20superior%20performance%20compared%20with%20those%0Atrained%20with%20LLM-based%20augmentation%2C%20both%20in%20unseen%20and%20seen%20environments.%0AThese%20findings%20suggest%20that%20the%20potential%20advantages%20of%20explicitly%20structuring%0Aspatial%20information%20for%20text-based%20geospatial%20reasoning%20in%20previously%20unknown%2C%0Acan%20unlock%20data-scarce%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19967v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInto%2520the%2520Unknown%253A%2520Generating%2520Geospatial%2520Descriptions%2520for%2520New%250A%2520%2520Environments%26entry.906535625%3DTzuf%2520Paz-Argaman%2520and%2520John%2520Palowitch%2520and%2520Sayali%2520Kulkarni%2520and%2520Reut%2520Tsarfaty%2520and%2520Jason%2520Baldridge%26entry.1292438233%3D%2520%2520Similar%2520to%2520vision-and-language%2520navigation%2520%2528VLN%2529%2520tasks%2520that%2520focus%2520on%2520bridging%250Athe%2520gap%2520between%2520vision%2520and%2520language%2520for%2520embodied%2520navigation%252C%2520the%2520new%2520Rendezvous%250A%2528RVS%2529%2520task%2520requires%2520reasoning%2520over%2520allocentric%2520spatial%2520relationships%250A%2528independent%2520of%2520the%2520observer%2527s%2520viewpoint%2529%2520using%2520non-sequential%2520navigation%250Ainstructions%2520and%2520maps.%2520However%252C%2520performance%2520substantially%2520drops%2520in%2520new%250Aenvironments%2520with%2520no%2520training%2520data.%2520Using%2520opensource%2520descriptions%2520paired%2520with%250Acoordinates%2520%2528e.g.%252C%2520Wikipedia%2529%2520provides%2520training%2520data%2520but%2520suffers%2520from%2520limited%250Aspatially-oriented%2520text%2520resulting%2520in%2520low%2520geolocation%2520resolution.%2520We%2520propose%2520a%250Alarge-scale%2520augmentation%2520method%2520for%2520generating%2520high-quality%2520synthetic%2520data%2520for%250Anew%2520environments%2520using%2520readily%2520available%2520geospatial%2520data.%2520Our%2520method%2520constructs%250Aa%2520grounded%2520knowledge-graph%252C%2520capturing%2520entity%2520relationships.%2520Sampled%2520entities%250Aand%2520relations%2520%2528%2560shop%2520north%2520of%2520school%2527%2529%2520generate%2520navigation%2520instructions%2520via%2520%2528i%2529%250Agenerating%2520numerous%2520templates%2520using%2520context-free%2520grammar%2520%2528CFG%2529%2520to%2520embed%250Aspecific%2520entities%2520and%2520relations%253B%2520%2528ii%2529%2520feeding%2520the%2520entities%2520and%2520relation%2520into%2520a%250Alarge%2520language%2520model%2520%2528LLM%2529%2520for%2520instruction%2520generation.%2520A%2520comprehensive%250Aevaluation%2520on%2520RVS%252C%2520showed%2520that%2520our%2520approach%2520improves%2520the%2520100-meter%2520accuracy%2520by%250A45.83%2525%2520on%2520unseen%2520environments.%2520Furthermore%252C%2520we%2520demonstrate%2520that%2520models%2520trained%250Awith%2520CFG-based%2520augmentation%2520achieve%2520superior%2520performance%2520compared%2520with%2520those%250Atrained%2520with%2520LLM-based%2520augmentation%252C%2520both%2520in%2520unseen%2520and%2520seen%2520environments.%250AThese%2520findings%2520suggest%2520that%2520the%2520potential%2520advantages%2520of%2520explicitly%2520structuring%250Aspatial%2520information%2520for%2520text-based%2520geospatial%2520reasoning%2520in%2520previously%2520unknown%252C%250Acan%2520unlock%2520data-scarce%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19967v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Into%20the%20Unknown%3A%20Generating%20Geospatial%20Descriptions%20for%20New%0A%20%20Environments&entry.906535625=Tzuf%20Paz-Argaman%20and%20John%20Palowitch%20and%20Sayali%20Kulkarni%20and%20Reut%20Tsarfaty%20and%20Jason%20Baldridge&entry.1292438233=%20%20Similar%20to%20vision-and-language%20navigation%20%28VLN%29%20tasks%20that%20focus%20on%20bridging%0Athe%20gap%20between%20vision%20and%20language%20for%20embodied%20navigation%2C%20the%20new%20Rendezvous%0A%28RVS%29%20task%20requires%20reasoning%20over%20allocentric%20spatial%20relationships%0A%28independent%20of%20the%20observer%27s%20viewpoint%29%20using%20non-sequential%20navigation%0Ainstructions%20and%20maps.%20However%2C%20performance%20substantially%20drops%20in%20new%0Aenvironments%20with%20no%20training%20data.%20Using%20opensource%20descriptions%20paired%20with%0Acoordinates%20%28e.g.%2C%20Wikipedia%29%20provides%20training%20data%20but%20suffers%20from%20limited%0Aspatially-oriented%20text%20resulting%20in%20low%20geolocation%20resolution.%20We%20propose%20a%0Alarge-scale%20augmentation%20method%20for%20generating%20high-quality%20synthetic%20data%20for%0Anew%20environments%20using%20readily%20available%20geospatial%20data.%20Our%20method%20constructs%0Aa%20grounded%20knowledge-graph%2C%20capturing%20entity%20relationships.%20Sampled%20entities%0Aand%20relations%20%28%60shop%20north%20of%20school%27%29%20generate%20navigation%20instructions%20via%20%28i%29%0Agenerating%20numerous%20templates%20using%20context-free%20grammar%20%28CFG%29%20to%20embed%0Aspecific%20entities%20and%20relations%3B%20%28ii%29%20feeding%20the%20entities%20and%20relation%20into%20a%0Alarge%20language%20model%20%28LLM%29%20for%20instruction%20generation.%20A%20comprehensive%0Aevaluation%20on%20RVS%2C%20showed%20that%20our%20approach%20improves%20the%20100-meter%20accuracy%20by%0A45.83%25%20on%20unseen%20environments.%20Furthermore%2C%20we%20demonstrate%20that%20models%20trained%0Awith%20CFG-based%20augmentation%20achieve%20superior%20performance%20compared%20with%20those%0Atrained%20with%20LLM-based%20augmentation%2C%20both%20in%20unseen%20and%20seen%20environments.%0AThese%20findings%20suggest%20that%20the%20potential%20advantages%20of%20explicitly%20structuring%0Aspatial%20information%20for%20text-based%20geospatial%20reasoning%20in%20previously%20unknown%2C%0Acan%20unlock%20data-scarce%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19967v1&entry.124074799=Read"},
{"title": "Learning Human-Robot Handshaking Preferences for Quadruped Robots", "author": "Alessandra Chappuis and Guillaume Bellegarda and Auke Ijspeert", "abstract": "  Quadruped robots are showing impressive abilities to navigate the real world.\nIf they are to become more integrated into society, social trust in\ninteractions with humans will become increasingly important. Additionally,\nrobots will need to be adaptable to different humans based on individual\npreferences. In this work, we study the social interaction task of learning\noptimal handshakes for quadruped robots based on user preferences. While\nmaintaining balance on three legs, we parameterize handshakes with a Central\nPattern Generator consisting of an amplitude, frequency, stiffness, and\nduration. Through 10 binary choices between handshakes, we learn a belief model\nto fit individual preferences for 25 different subjects. Our results show that\nthis is an effective strategy, with 76% of users feeling happy with their\nidentified optimal handshake parameters, and 20% feeling neutral. Moreover,\ncompared with random and test handshakes, the optimized handshakes have\nsignificantly decreased errors in amplitude and frequency, lower Dynamic Time\nWarping scores, and improved energy efficiency, all of which indicate robot\nsynchronization to the user's preferences. Video results can be found at\nhttps://youtu.be/elvPv8mq1KM .\n", "link": "http://arxiv.org/abs/2406.19893v1", "date": "2024-06-28", "relevancy": 2.2104, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5652}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5622}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5361}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Human-Robot%20Handshaking%20Preferences%20for%20Quadruped%20Robots&body=Title%3A%20Learning%20Human-Robot%20Handshaking%20Preferences%20for%20Quadruped%20Robots%0AAuthor%3A%20Alessandra%20Chappuis%20and%20Guillaume%20Bellegarda%20and%20Auke%20Ijspeert%0AAbstract%3A%20%20%20Quadruped%20robots%20are%20showing%20impressive%20abilities%20to%20navigate%20the%20real%20world.%0AIf%20they%20are%20to%20become%20more%20integrated%20into%20society%2C%20social%20trust%20in%0Ainteractions%20with%20humans%20will%20become%20increasingly%20important.%20Additionally%2C%0Arobots%20will%20need%20to%20be%20adaptable%20to%20different%20humans%20based%20on%20individual%0Apreferences.%20In%20this%20work%2C%20we%20study%20the%20social%20interaction%20task%20of%20learning%0Aoptimal%20handshakes%20for%20quadruped%20robots%20based%20on%20user%20preferences.%20While%0Amaintaining%20balance%20on%20three%20legs%2C%20we%20parameterize%20handshakes%20with%20a%20Central%0APattern%20Generator%20consisting%20of%20an%20amplitude%2C%20frequency%2C%20stiffness%2C%20and%0Aduration.%20Through%2010%20binary%20choices%20between%20handshakes%2C%20we%20learn%20a%20belief%20model%0Ato%20fit%20individual%20preferences%20for%2025%20different%20subjects.%20Our%20results%20show%20that%0Athis%20is%20an%20effective%20strategy%2C%20with%2076%25%20of%20users%20feeling%20happy%20with%20their%0Aidentified%20optimal%20handshake%20parameters%2C%20and%2020%25%20feeling%20neutral.%20Moreover%2C%0Acompared%20with%20random%20and%20test%20handshakes%2C%20the%20optimized%20handshakes%20have%0Asignificantly%20decreased%20errors%20in%20amplitude%20and%20frequency%2C%20lower%20Dynamic%20Time%0AWarping%20scores%2C%20and%20improved%20energy%20efficiency%2C%20all%20of%20which%20indicate%20robot%0Asynchronization%20to%20the%20user%27s%20preferences.%20Video%20results%20can%20be%20found%20at%0Ahttps%3A//youtu.be/elvPv8mq1KM%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19893v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Human-Robot%2520Handshaking%2520Preferences%2520for%2520Quadruped%2520Robots%26entry.906535625%3DAlessandra%2520Chappuis%2520and%2520Guillaume%2520Bellegarda%2520and%2520Auke%2520Ijspeert%26entry.1292438233%3D%2520%2520Quadruped%2520robots%2520are%2520showing%2520impressive%2520abilities%2520to%2520navigate%2520the%2520real%2520world.%250AIf%2520they%2520are%2520to%2520become%2520more%2520integrated%2520into%2520society%252C%2520social%2520trust%2520in%250Ainteractions%2520with%2520humans%2520will%2520become%2520increasingly%2520important.%2520Additionally%252C%250Arobots%2520will%2520need%2520to%2520be%2520adaptable%2520to%2520different%2520humans%2520based%2520on%2520individual%250Apreferences.%2520In%2520this%2520work%252C%2520we%2520study%2520the%2520social%2520interaction%2520task%2520of%2520learning%250Aoptimal%2520handshakes%2520for%2520quadruped%2520robots%2520based%2520on%2520user%2520preferences.%2520While%250Amaintaining%2520balance%2520on%2520three%2520legs%252C%2520we%2520parameterize%2520handshakes%2520with%2520a%2520Central%250APattern%2520Generator%2520consisting%2520of%2520an%2520amplitude%252C%2520frequency%252C%2520stiffness%252C%2520and%250Aduration.%2520Through%252010%2520binary%2520choices%2520between%2520handshakes%252C%2520we%2520learn%2520a%2520belief%2520model%250Ato%2520fit%2520individual%2520preferences%2520for%252025%2520different%2520subjects.%2520Our%2520results%2520show%2520that%250Athis%2520is%2520an%2520effective%2520strategy%252C%2520with%252076%2525%2520of%2520users%2520feeling%2520happy%2520with%2520their%250Aidentified%2520optimal%2520handshake%2520parameters%252C%2520and%252020%2525%2520feeling%2520neutral.%2520Moreover%252C%250Acompared%2520with%2520random%2520and%2520test%2520handshakes%252C%2520the%2520optimized%2520handshakes%2520have%250Asignificantly%2520decreased%2520errors%2520in%2520amplitude%2520and%2520frequency%252C%2520lower%2520Dynamic%2520Time%250AWarping%2520scores%252C%2520and%2520improved%2520energy%2520efficiency%252C%2520all%2520of%2520which%2520indicate%2520robot%250Asynchronization%2520to%2520the%2520user%2527s%2520preferences.%2520Video%2520results%2520can%2520be%2520found%2520at%250Ahttps%253A//youtu.be/elvPv8mq1KM%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19893v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Human-Robot%20Handshaking%20Preferences%20for%20Quadruped%20Robots&entry.906535625=Alessandra%20Chappuis%20and%20Guillaume%20Bellegarda%20and%20Auke%20Ijspeert&entry.1292438233=%20%20Quadruped%20robots%20are%20showing%20impressive%20abilities%20to%20navigate%20the%20real%20world.%0AIf%20they%20are%20to%20become%20more%20integrated%20into%20society%2C%20social%20trust%20in%0Ainteractions%20with%20humans%20will%20become%20increasingly%20important.%20Additionally%2C%0Arobots%20will%20need%20to%20be%20adaptable%20to%20different%20humans%20based%20on%20individual%0Apreferences.%20In%20this%20work%2C%20we%20study%20the%20social%20interaction%20task%20of%20learning%0Aoptimal%20handshakes%20for%20quadruped%20robots%20based%20on%20user%20preferences.%20While%0Amaintaining%20balance%20on%20three%20legs%2C%20we%20parameterize%20handshakes%20with%20a%20Central%0APattern%20Generator%20consisting%20of%20an%20amplitude%2C%20frequency%2C%20stiffness%2C%20and%0Aduration.%20Through%2010%20binary%20choices%20between%20handshakes%2C%20we%20learn%20a%20belief%20model%0Ato%20fit%20individual%20preferences%20for%2025%20different%20subjects.%20Our%20results%20show%20that%0Athis%20is%20an%20effective%20strategy%2C%20with%2076%25%20of%20users%20feeling%20happy%20with%20their%0Aidentified%20optimal%20handshake%20parameters%2C%20and%2020%25%20feeling%20neutral.%20Moreover%2C%0Acompared%20with%20random%20and%20test%20handshakes%2C%20the%20optimized%20handshakes%20have%0Asignificantly%20decreased%20errors%20in%20amplitude%20and%20frequency%2C%20lower%20Dynamic%20Time%0AWarping%20scores%2C%20and%20improved%20energy%20efficiency%2C%20all%20of%20which%20indicate%20robot%0Asynchronization%20to%20the%20user%27s%20preferences.%20Video%20results%20can%20be%20found%20at%0Ahttps%3A//youtu.be/elvPv8mq1KM%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19893v1&entry.124074799=Read"},
{"title": "LLaVolta: Efficient Multi-modal Models via Stage-wise Visual Context\n  Compression", "author": "Jieneng Chen and Luoxin Ye and Ju He and Zhao-Yang Wang and Daniel Khashabi and Alan Yuille", "abstract": "  While significant advancements have been made in compressed representations\nfor text embeddings in large language models (LLMs), the compression of visual\ntokens in large multi-modal models (LMMs) has remained a largely overlooked\narea. In this work, we present the study on the analysis of redundancy\nconcerning visual tokens and efficient training within these models. Our\ninitial experiments show that eliminating up to 70% of visual tokens at the\ntesting stage by simply average pooling only leads to a minimal 3% reduction in\nvisual question answering accuracy on the GQA benchmark, indicating significant\nredundancy in visual context. Addressing this, we introduce Visual Context\nCompressor, which reduces the number of visual tokens during training to\nenhance training efficiency without sacrificing performance. To minimize\ninformation loss caused by the compression on visual tokens while maintaining\ntraining efficiency, we develop LLaVolta as a lite training scheme. LLaVolta\nincorporates stage-wise visual context compression to progressively compress\nthe visual tokens from heavily to lightly, and finally no compression at the\nend of training, yielding no loss of information when testing. Extensive\nexperiments demonstrate that our approach enhances the performance of MLLMs in\nboth image-language and video-language understanding, while also significantly\ncutting training costs. Code is available at\nhttps://github.com/Beckschen/LLaVolta\n", "link": "http://arxiv.org/abs/2406.20092v1", "date": "2024-06-28", "relevancy": 2.2023, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5648}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5412}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5401}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLaVolta%3A%20Efficient%20Multi-modal%20Models%20via%20Stage-wise%20Visual%20Context%0A%20%20Compression&body=Title%3A%20LLaVolta%3A%20Efficient%20Multi-modal%20Models%20via%20Stage-wise%20Visual%20Context%0A%20%20Compression%0AAuthor%3A%20Jieneng%20Chen%20and%20Luoxin%20Ye%20and%20Ju%20He%20and%20Zhao-Yang%20Wang%20and%20Daniel%20Khashabi%20and%20Alan%20Yuille%0AAbstract%3A%20%20%20While%20significant%20advancements%20have%20been%20made%20in%20compressed%20representations%0Afor%20text%20embeddings%20in%20large%20language%20models%20%28LLMs%29%2C%20the%20compression%20of%20visual%0Atokens%20in%20large%20multi-modal%20models%20%28LMMs%29%20has%20remained%20a%20largely%20overlooked%0Aarea.%20In%20this%20work%2C%20we%20present%20the%20study%20on%20the%20analysis%20of%20redundancy%0Aconcerning%20visual%20tokens%20and%20efficient%20training%20within%20these%20models.%20Our%0Ainitial%20experiments%20show%20that%20eliminating%20up%20to%2070%25%20of%20visual%20tokens%20at%20the%0Atesting%20stage%20by%20simply%20average%20pooling%20only%20leads%20to%20a%20minimal%203%25%20reduction%20in%0Avisual%20question%20answering%20accuracy%20on%20the%20GQA%20benchmark%2C%20indicating%20significant%0Aredundancy%20in%20visual%20context.%20Addressing%20this%2C%20we%20introduce%20Visual%20Context%0ACompressor%2C%20which%20reduces%20the%20number%20of%20visual%20tokens%20during%20training%20to%0Aenhance%20training%20efficiency%20without%20sacrificing%20performance.%20To%20minimize%0Ainformation%20loss%20caused%20by%20the%20compression%20on%20visual%20tokens%20while%20maintaining%0Atraining%20efficiency%2C%20we%20develop%20LLaVolta%20as%20a%20lite%20training%20scheme.%20LLaVolta%0Aincorporates%20stage-wise%20visual%20context%20compression%20to%20progressively%20compress%0Athe%20visual%20tokens%20from%20heavily%20to%20lightly%2C%20and%20finally%20no%20compression%20at%20the%0Aend%20of%20training%2C%20yielding%20no%20loss%20of%20information%20when%20testing.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20approach%20enhances%20the%20performance%20of%20MLLMs%20in%0Aboth%20image-language%20and%20video-language%20understanding%2C%20while%20also%20significantly%0Acutting%20training%20costs.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Beckschen/LLaVolta%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.20092v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLaVolta%253A%2520Efficient%2520Multi-modal%2520Models%2520via%2520Stage-wise%2520Visual%2520Context%250A%2520%2520Compression%26entry.906535625%3DJieneng%2520Chen%2520and%2520Luoxin%2520Ye%2520and%2520Ju%2520He%2520and%2520Zhao-Yang%2520Wang%2520and%2520Daniel%2520Khashabi%2520and%2520Alan%2520Yuille%26entry.1292438233%3D%2520%2520While%2520significant%2520advancements%2520have%2520been%2520made%2520in%2520compressed%2520representations%250Afor%2520text%2520embeddings%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520the%2520compression%2520of%2520visual%250Atokens%2520in%2520large%2520multi-modal%2520models%2520%2528LMMs%2529%2520has%2520remained%2520a%2520largely%2520overlooked%250Aarea.%2520In%2520this%2520work%252C%2520we%2520present%2520the%2520study%2520on%2520the%2520analysis%2520of%2520redundancy%250Aconcerning%2520visual%2520tokens%2520and%2520efficient%2520training%2520within%2520these%2520models.%2520Our%250Ainitial%2520experiments%2520show%2520that%2520eliminating%2520up%2520to%252070%2525%2520of%2520visual%2520tokens%2520at%2520the%250Atesting%2520stage%2520by%2520simply%2520average%2520pooling%2520only%2520leads%2520to%2520a%2520minimal%25203%2525%2520reduction%2520in%250Avisual%2520question%2520answering%2520accuracy%2520on%2520the%2520GQA%2520benchmark%252C%2520indicating%2520significant%250Aredundancy%2520in%2520visual%2520context.%2520Addressing%2520this%252C%2520we%2520introduce%2520Visual%2520Context%250ACompressor%252C%2520which%2520reduces%2520the%2520number%2520of%2520visual%2520tokens%2520during%2520training%2520to%250Aenhance%2520training%2520efficiency%2520without%2520sacrificing%2520performance.%2520To%2520minimize%250Ainformation%2520loss%2520caused%2520by%2520the%2520compression%2520on%2520visual%2520tokens%2520while%2520maintaining%250Atraining%2520efficiency%252C%2520we%2520develop%2520LLaVolta%2520as%2520a%2520lite%2520training%2520scheme.%2520LLaVolta%250Aincorporates%2520stage-wise%2520visual%2520context%2520compression%2520to%2520progressively%2520compress%250Athe%2520visual%2520tokens%2520from%2520heavily%2520to%2520lightly%252C%2520and%2520finally%2520no%2520compression%2520at%2520the%250Aend%2520of%2520training%252C%2520yielding%2520no%2520loss%2520of%2520information%2520when%2520testing.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520approach%2520enhances%2520the%2520performance%2520of%2520MLLMs%2520in%250Aboth%2520image-language%2520and%2520video-language%2520understanding%252C%2520while%2520also%2520significantly%250Acutting%2520training%2520costs.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/Beckschen/LLaVolta%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.20092v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLaVolta%3A%20Efficient%20Multi-modal%20Models%20via%20Stage-wise%20Visual%20Context%0A%20%20Compression&entry.906535625=Jieneng%20Chen%20and%20Luoxin%20Ye%20and%20Ju%20He%20and%20Zhao-Yang%20Wang%20and%20Daniel%20Khashabi%20and%20Alan%20Yuille&entry.1292438233=%20%20While%20significant%20advancements%20have%20been%20made%20in%20compressed%20representations%0Afor%20text%20embeddings%20in%20large%20language%20models%20%28LLMs%29%2C%20the%20compression%20of%20visual%0Atokens%20in%20large%20multi-modal%20models%20%28LMMs%29%20has%20remained%20a%20largely%20overlooked%0Aarea.%20In%20this%20work%2C%20we%20present%20the%20study%20on%20the%20analysis%20of%20redundancy%0Aconcerning%20visual%20tokens%20and%20efficient%20training%20within%20these%20models.%20Our%0Ainitial%20experiments%20show%20that%20eliminating%20up%20to%2070%25%20of%20visual%20tokens%20at%20the%0Atesting%20stage%20by%20simply%20average%20pooling%20only%20leads%20to%20a%20minimal%203%25%20reduction%20in%0Avisual%20question%20answering%20accuracy%20on%20the%20GQA%20benchmark%2C%20indicating%20significant%0Aredundancy%20in%20visual%20context.%20Addressing%20this%2C%20we%20introduce%20Visual%20Context%0ACompressor%2C%20which%20reduces%20the%20number%20of%20visual%20tokens%20during%20training%20to%0Aenhance%20training%20efficiency%20without%20sacrificing%20performance.%20To%20minimize%0Ainformation%20loss%20caused%20by%20the%20compression%20on%20visual%20tokens%20while%20maintaining%0Atraining%20efficiency%2C%20we%20develop%20LLaVolta%20as%20a%20lite%20training%20scheme.%20LLaVolta%0Aincorporates%20stage-wise%20visual%20context%20compression%20to%20progressively%20compress%0Athe%20visual%20tokens%20from%20heavily%20to%20lightly%2C%20and%20finally%20no%20compression%20at%20the%0Aend%20of%20training%2C%20yielding%20no%20loss%20of%20information%20when%20testing.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20approach%20enhances%20the%20performance%20of%20MLLMs%20in%0Aboth%20image-language%20and%20video-language%20understanding%2C%20while%20also%20significantly%0Acutting%20training%20costs.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Beckschen/LLaVolta%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.20092v1&entry.124074799=Read"},
{"title": "Mining Open Semantics from CLIP: A Relation Transition Perspective for\n  Few-Shot Learning", "author": "Cilin Yan and Haochen Wang and Xiaolong Jiang and Yao Hu and Xu Tang and Guoliang Kang and Efstratios Gavves", "abstract": "  Contrastive Vision-Language Pre-training(CLIP) demonstrates impressive\nzero-shot capability. The key to improve the adaptation of CLIP to downstream\ntask with few exemplars lies in how to effectively model and transfer the\nuseful knowledge embedded in CLIP. Previous work mines the knowledge typically\nbased on the limited visual samples and close-set semantics (i.e., within\ntarget category set of downstream task). However, the aligned CLIP image/text\nencoders contain abundant relationships between visual features and almost\ninfinite open semantics, which may benefit the few-shot learning but remains\nunexplored. In this paper, we propose to mine open semantics as anchors to\nperform a relation transition from image-anchor relationship to image-target\nrelationship to make predictions. Specifically, we adopt a transformer module\nwhich takes the visual feature as \"Query\", the text features of the anchors as\n\"Key\" and the similarity matrix between the text features of anchor and target\nclasses as \"Value\". In this way, the output of such a transformer module\nrepresents the relationship between the image and target categories, i.e., the\nclassification predictions. To avoid manually selecting the open semantics, we\nmake the [CLASS] token of input text embedding learnable. We conduct extensive\nexperiments on eleven representative classification datasets. The results show\nthat our method performs favorably against previous state-of-the-arts\nconsidering few-shot classification settings.\n", "link": "http://arxiv.org/abs/2406.11252v2", "date": "2024-06-28", "relevancy": 2.1966, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5994}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.517}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5038}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mining%20Open%20Semantics%20from%20CLIP%3A%20A%20Relation%20Transition%20Perspective%20for%0A%20%20Few-Shot%20Learning&body=Title%3A%20Mining%20Open%20Semantics%20from%20CLIP%3A%20A%20Relation%20Transition%20Perspective%20for%0A%20%20Few-Shot%20Learning%0AAuthor%3A%20Cilin%20Yan%20and%20Haochen%20Wang%20and%20Xiaolong%20Jiang%20and%20Yao%20Hu%20and%20Xu%20Tang%20and%20Guoliang%20Kang%20and%20Efstratios%20Gavves%0AAbstract%3A%20%20%20Contrastive%20Vision-Language%20Pre-training%28CLIP%29%20demonstrates%20impressive%0Azero-shot%20capability.%20The%20key%20to%20improve%20the%20adaptation%20of%20CLIP%20to%20downstream%0Atask%20with%20few%20exemplars%20lies%20in%20how%20to%20effectively%20model%20and%20transfer%20the%0Auseful%20knowledge%20embedded%20in%20CLIP.%20Previous%20work%20mines%20the%20knowledge%20typically%0Abased%20on%20the%20limited%20visual%20samples%20and%20close-set%20semantics%20%28i.e.%2C%20within%0Atarget%20category%20set%20of%20downstream%20task%29.%20However%2C%20the%20aligned%20CLIP%20image/text%0Aencoders%20contain%20abundant%20relationships%20between%20visual%20features%20and%20almost%0Ainfinite%20open%20semantics%2C%20which%20may%20benefit%20the%20few-shot%20learning%20but%20remains%0Aunexplored.%20In%20this%20paper%2C%20we%20propose%20to%20mine%20open%20semantics%20as%20anchors%20to%0Aperform%20a%20relation%20transition%20from%20image-anchor%20relationship%20to%20image-target%0Arelationship%20to%20make%20predictions.%20Specifically%2C%20we%20adopt%20a%20transformer%20module%0Awhich%20takes%20the%20visual%20feature%20as%20%22Query%22%2C%20the%20text%20features%20of%20the%20anchors%20as%0A%22Key%22%20and%20the%20similarity%20matrix%20between%20the%20text%20features%20of%20anchor%20and%20target%0Aclasses%20as%20%22Value%22.%20In%20this%20way%2C%20the%20output%20of%20such%20a%20transformer%20module%0Arepresents%20the%20relationship%20between%20the%20image%20and%20target%20categories%2C%20i.e.%2C%20the%0Aclassification%20predictions.%20To%20avoid%20manually%20selecting%20the%20open%20semantics%2C%20we%0Amake%20the%20%5BCLASS%5D%20token%20of%20input%20text%20embedding%20learnable.%20We%20conduct%20extensive%0Aexperiments%20on%20eleven%20representative%20classification%20datasets.%20The%20results%20show%0Athat%20our%20method%20performs%20favorably%20against%20previous%20state-of-the-arts%0Aconsidering%20few-shot%20classification%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11252v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMining%2520Open%2520Semantics%2520from%2520CLIP%253A%2520A%2520Relation%2520Transition%2520Perspective%2520for%250A%2520%2520Few-Shot%2520Learning%26entry.906535625%3DCilin%2520Yan%2520and%2520Haochen%2520Wang%2520and%2520Xiaolong%2520Jiang%2520and%2520Yao%2520Hu%2520and%2520Xu%2520Tang%2520and%2520Guoliang%2520Kang%2520and%2520Efstratios%2520Gavves%26entry.1292438233%3D%2520%2520Contrastive%2520Vision-Language%2520Pre-training%2528CLIP%2529%2520demonstrates%2520impressive%250Azero-shot%2520capability.%2520The%2520key%2520to%2520improve%2520the%2520adaptation%2520of%2520CLIP%2520to%2520downstream%250Atask%2520with%2520few%2520exemplars%2520lies%2520in%2520how%2520to%2520effectively%2520model%2520and%2520transfer%2520the%250Auseful%2520knowledge%2520embedded%2520in%2520CLIP.%2520Previous%2520work%2520mines%2520the%2520knowledge%2520typically%250Abased%2520on%2520the%2520limited%2520visual%2520samples%2520and%2520close-set%2520semantics%2520%2528i.e.%252C%2520within%250Atarget%2520category%2520set%2520of%2520downstream%2520task%2529.%2520However%252C%2520the%2520aligned%2520CLIP%2520image/text%250Aencoders%2520contain%2520abundant%2520relationships%2520between%2520visual%2520features%2520and%2520almost%250Ainfinite%2520open%2520semantics%252C%2520which%2520may%2520benefit%2520the%2520few-shot%2520learning%2520but%2520remains%250Aunexplored.%2520In%2520this%2520paper%252C%2520we%2520propose%2520to%2520mine%2520open%2520semantics%2520as%2520anchors%2520to%250Aperform%2520a%2520relation%2520transition%2520from%2520image-anchor%2520relationship%2520to%2520image-target%250Arelationship%2520to%2520make%2520predictions.%2520Specifically%252C%2520we%2520adopt%2520a%2520transformer%2520module%250Awhich%2520takes%2520the%2520visual%2520feature%2520as%2520%2522Query%2522%252C%2520the%2520text%2520features%2520of%2520the%2520anchors%2520as%250A%2522Key%2522%2520and%2520the%2520similarity%2520matrix%2520between%2520the%2520text%2520features%2520of%2520anchor%2520and%2520target%250Aclasses%2520as%2520%2522Value%2522.%2520In%2520this%2520way%252C%2520the%2520output%2520of%2520such%2520a%2520transformer%2520module%250Arepresents%2520the%2520relationship%2520between%2520the%2520image%2520and%2520target%2520categories%252C%2520i.e.%252C%2520the%250Aclassification%2520predictions.%2520To%2520avoid%2520manually%2520selecting%2520the%2520open%2520semantics%252C%2520we%250Amake%2520the%2520%255BCLASS%255D%2520token%2520of%2520input%2520text%2520embedding%2520learnable.%2520We%2520conduct%2520extensive%250Aexperiments%2520on%2520eleven%2520representative%2520classification%2520datasets.%2520The%2520results%2520show%250Athat%2520our%2520method%2520performs%2520favorably%2520against%2520previous%2520state-of-the-arts%250Aconsidering%2520few-shot%2520classification%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11252v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mining%20Open%20Semantics%20from%20CLIP%3A%20A%20Relation%20Transition%20Perspective%20for%0A%20%20Few-Shot%20Learning&entry.906535625=Cilin%20Yan%20and%20Haochen%20Wang%20and%20Xiaolong%20Jiang%20and%20Yao%20Hu%20and%20Xu%20Tang%20and%20Guoliang%20Kang%20and%20Efstratios%20Gavves&entry.1292438233=%20%20Contrastive%20Vision-Language%20Pre-training%28CLIP%29%20demonstrates%20impressive%0Azero-shot%20capability.%20The%20key%20to%20improve%20the%20adaptation%20of%20CLIP%20to%20downstream%0Atask%20with%20few%20exemplars%20lies%20in%20how%20to%20effectively%20model%20and%20transfer%20the%0Auseful%20knowledge%20embedded%20in%20CLIP.%20Previous%20work%20mines%20the%20knowledge%20typically%0Abased%20on%20the%20limited%20visual%20samples%20and%20close-set%20semantics%20%28i.e.%2C%20within%0Atarget%20category%20set%20of%20downstream%20task%29.%20However%2C%20the%20aligned%20CLIP%20image/text%0Aencoders%20contain%20abundant%20relationships%20between%20visual%20features%20and%20almost%0Ainfinite%20open%20semantics%2C%20which%20may%20benefit%20the%20few-shot%20learning%20but%20remains%0Aunexplored.%20In%20this%20paper%2C%20we%20propose%20to%20mine%20open%20semantics%20as%20anchors%20to%0Aperform%20a%20relation%20transition%20from%20image-anchor%20relationship%20to%20image-target%0Arelationship%20to%20make%20predictions.%20Specifically%2C%20we%20adopt%20a%20transformer%20module%0Awhich%20takes%20the%20visual%20feature%20as%20%22Query%22%2C%20the%20text%20features%20of%20the%20anchors%20as%0A%22Key%22%20and%20the%20similarity%20matrix%20between%20the%20text%20features%20of%20anchor%20and%20target%0Aclasses%20as%20%22Value%22.%20In%20this%20way%2C%20the%20output%20of%20such%20a%20transformer%20module%0Arepresents%20the%20relationship%20between%20the%20image%20and%20target%20categories%2C%20i.e.%2C%20the%0Aclassification%20predictions.%20To%20avoid%20manually%20selecting%20the%20open%20semantics%2C%20we%0Amake%20the%20%5BCLASS%5D%20token%20of%20input%20text%20embedding%20learnable.%20We%20conduct%20extensive%0Aexperiments%20on%20eleven%20representative%20classification%20datasets.%20The%20results%20show%0Athat%20our%20method%20performs%20favorably%20against%20previous%20state-of-the-arts%0Aconsidering%20few-shot%20classification%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11252v2&entry.124074799=Read"},
{"title": "Perception Stitching: Zero-Shot Perception Encoder Transfer for\n  Visuomotor Robot Policies", "author": "Pingcheng Jian and Easop Lee and Zachary Bell and Michael M. Zavlanos and Boyuan Chen", "abstract": "  Vision-based imitation learning has shown promising capabilities of endowing\nrobots with various motion skills given visual observation. However, current\nvisuomotor policies fail to adapt to drastic changes in their visual\nobservations. We present Perception Stitching that enables strong zero-shot\nadaptation to large visual changes by directly stitching novel combinations of\nvisual encoders. Our key idea is to enforce modularity of visual encoders by\naligning the latent visual features among different visuomotor policies. Our\nmethod disentangles the perceptual knowledge with the downstream motion skills\nand allows the reuse of the visual encoders by directly stitching them to a\npolicy network trained with partially different visual conditions. We evaluate\nour method in various simulated and real-world manipulation tasks. While\nbaseline methods failed at all attempts, our method could achieve zero-shot\nsuccess in real-world visuomotor tasks. Our quantitative and qualitative\nanalysis of the learned features of the policy network provides more insights\ninto the high performance of our proposed method.\n", "link": "http://arxiv.org/abs/2406.19971v1", "date": "2024-06-28", "relevancy": 2.1856, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5754}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5412}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.54}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perception%20Stitching%3A%20Zero-Shot%20Perception%20Encoder%20Transfer%20for%0A%20%20Visuomotor%20Robot%20Policies&body=Title%3A%20Perception%20Stitching%3A%20Zero-Shot%20Perception%20Encoder%20Transfer%20for%0A%20%20Visuomotor%20Robot%20Policies%0AAuthor%3A%20Pingcheng%20Jian%20and%20Easop%20Lee%20and%20Zachary%20Bell%20and%20Michael%20M.%20Zavlanos%20and%20Boyuan%20Chen%0AAbstract%3A%20%20%20Vision-based%20imitation%20learning%20has%20shown%20promising%20capabilities%20of%20endowing%0Arobots%20with%20various%20motion%20skills%20given%20visual%20observation.%20However%2C%20current%0Avisuomotor%20policies%20fail%20to%20adapt%20to%20drastic%20changes%20in%20their%20visual%0Aobservations.%20We%20present%20Perception%20Stitching%20that%20enables%20strong%20zero-shot%0Aadaptation%20to%20large%20visual%20changes%20by%20directly%20stitching%20novel%20combinations%20of%0Avisual%20encoders.%20Our%20key%20idea%20is%20to%20enforce%20modularity%20of%20visual%20encoders%20by%0Aaligning%20the%20latent%20visual%20features%20among%20different%20visuomotor%20policies.%20Our%0Amethod%20disentangles%20the%20perceptual%20knowledge%20with%20the%20downstream%20motion%20skills%0Aand%20allows%20the%20reuse%20of%20the%20visual%20encoders%20by%20directly%20stitching%20them%20to%20a%0Apolicy%20network%20trained%20with%20partially%20different%20visual%20conditions.%20We%20evaluate%0Aour%20method%20in%20various%20simulated%20and%20real-world%20manipulation%20tasks.%20While%0Abaseline%20methods%20failed%20at%20all%20attempts%2C%20our%20method%20could%20achieve%20zero-shot%0Asuccess%20in%20real-world%20visuomotor%20tasks.%20Our%20quantitative%20and%20qualitative%0Aanalysis%20of%20the%20learned%20features%20of%20the%20policy%20network%20provides%20more%20insights%0Ainto%20the%20high%20performance%20of%20our%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19971v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerception%2520Stitching%253A%2520Zero-Shot%2520Perception%2520Encoder%2520Transfer%2520for%250A%2520%2520Visuomotor%2520Robot%2520Policies%26entry.906535625%3DPingcheng%2520Jian%2520and%2520Easop%2520Lee%2520and%2520Zachary%2520Bell%2520and%2520Michael%2520M.%2520Zavlanos%2520and%2520Boyuan%2520Chen%26entry.1292438233%3D%2520%2520Vision-based%2520imitation%2520learning%2520has%2520shown%2520promising%2520capabilities%2520of%2520endowing%250Arobots%2520with%2520various%2520motion%2520skills%2520given%2520visual%2520observation.%2520However%252C%2520current%250Avisuomotor%2520policies%2520fail%2520to%2520adapt%2520to%2520drastic%2520changes%2520in%2520their%2520visual%250Aobservations.%2520We%2520present%2520Perception%2520Stitching%2520that%2520enables%2520strong%2520zero-shot%250Aadaptation%2520to%2520large%2520visual%2520changes%2520by%2520directly%2520stitching%2520novel%2520combinations%2520of%250Avisual%2520encoders.%2520Our%2520key%2520idea%2520is%2520to%2520enforce%2520modularity%2520of%2520visual%2520encoders%2520by%250Aaligning%2520the%2520latent%2520visual%2520features%2520among%2520different%2520visuomotor%2520policies.%2520Our%250Amethod%2520disentangles%2520the%2520perceptual%2520knowledge%2520with%2520the%2520downstream%2520motion%2520skills%250Aand%2520allows%2520the%2520reuse%2520of%2520the%2520visual%2520encoders%2520by%2520directly%2520stitching%2520them%2520to%2520a%250Apolicy%2520network%2520trained%2520with%2520partially%2520different%2520visual%2520conditions.%2520We%2520evaluate%250Aour%2520method%2520in%2520various%2520simulated%2520and%2520real-world%2520manipulation%2520tasks.%2520While%250Abaseline%2520methods%2520failed%2520at%2520all%2520attempts%252C%2520our%2520method%2520could%2520achieve%2520zero-shot%250Asuccess%2520in%2520real-world%2520visuomotor%2520tasks.%2520Our%2520quantitative%2520and%2520qualitative%250Aanalysis%2520of%2520the%2520learned%2520features%2520of%2520the%2520policy%2520network%2520provides%2520more%2520insights%250Ainto%2520the%2520high%2520performance%2520of%2520our%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19971v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perception%20Stitching%3A%20Zero-Shot%20Perception%20Encoder%20Transfer%20for%0A%20%20Visuomotor%20Robot%20Policies&entry.906535625=Pingcheng%20Jian%20and%20Easop%20Lee%20and%20Zachary%20Bell%20and%20Michael%20M.%20Zavlanos%20and%20Boyuan%20Chen&entry.1292438233=%20%20Vision-based%20imitation%20learning%20has%20shown%20promising%20capabilities%20of%20endowing%0Arobots%20with%20various%20motion%20skills%20given%20visual%20observation.%20However%2C%20current%0Avisuomotor%20policies%20fail%20to%20adapt%20to%20drastic%20changes%20in%20their%20visual%0Aobservations.%20We%20present%20Perception%20Stitching%20that%20enables%20strong%20zero-shot%0Aadaptation%20to%20large%20visual%20changes%20by%20directly%20stitching%20novel%20combinations%20of%0Avisual%20encoders.%20Our%20key%20idea%20is%20to%20enforce%20modularity%20of%20visual%20encoders%20by%0Aaligning%20the%20latent%20visual%20features%20among%20different%20visuomotor%20policies.%20Our%0Amethod%20disentangles%20the%20perceptual%20knowledge%20with%20the%20downstream%20motion%20skills%0Aand%20allows%20the%20reuse%20of%20the%20visual%20encoders%20by%20directly%20stitching%20them%20to%20a%0Apolicy%20network%20trained%20with%20partially%20different%20visual%20conditions.%20We%20evaluate%0Aour%20method%20in%20various%20simulated%20and%20real-world%20manipulation%20tasks.%20While%0Abaseline%20methods%20failed%20at%20all%20attempts%2C%20our%20method%20could%20achieve%20zero-shot%0Asuccess%20in%20real-world%20visuomotor%20tasks.%20Our%20quantitative%20and%20qualitative%0Aanalysis%20of%20the%20learned%20features%20of%20the%20policy%20network%20provides%20more%20insights%0Ainto%20the%20high%20performance%20of%20our%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19971v1&entry.124074799=Read"},
{"title": "StreamMOTP: Streaming and Unified Framework for Joint 3D Multi-Object\n  Tracking and Trajectory Prediction", "author": "Jiaheng Zhuang and Guoan Wang and Siyu Zhang and Xiyang Wang and Hangning Zhou and Ziyao Xu and Chi Zhang and Zhiheng Li", "abstract": "  3D multi-object tracking and trajectory prediction are two crucial modules in\nautonomous driving systems. Generally, the two tasks are handled separately in\ntraditional paradigms and a few methods have started to explore modeling these\ntwo tasks in a joint manner recently. However, these approaches suffer from the\nlimitations of single-frame training and inconsistent coordinate\nrepresentations between tracking and prediction tasks. In this paper, we\npropose a streaming and unified framework for joint 3D Multi-Object Tracking\nand trajectory Prediction (StreamMOTP) to address the above challenges.\nFirstly, we construct the model in a streaming manner and exploit a memory bank\nto preserve and leverage the long-term latent features for tracked objects more\neffectively. Secondly, a relative spatio-temporal positional encoding strategy\nis introduced to bridge the gap of coordinate representations between the two\ntasks and maintain the pose-invariance for trajectory prediction. Thirdly, we\nfurther improve the quality and consistency of predicted trajectories with a\ndual-stream predictor. We conduct extensive experiments on popular nuSences\ndataset and the experimental results demonstrate the effectiveness and\nsuperiority of StreamMOTP, which outperforms previous methods significantly on\nboth tasks. Furthermore, we also prove that the proposed framework has great\npotential and advantages in actual applications of autonomous driving.\n", "link": "http://arxiv.org/abs/2406.19844v1", "date": "2024-06-28", "relevancy": 2.179, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5806}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5477}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5275}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StreamMOTP%3A%20Streaming%20and%20Unified%20Framework%20for%20Joint%203D%20Multi-Object%0A%20%20Tracking%20and%20Trajectory%20Prediction&body=Title%3A%20StreamMOTP%3A%20Streaming%20and%20Unified%20Framework%20for%20Joint%203D%20Multi-Object%0A%20%20Tracking%20and%20Trajectory%20Prediction%0AAuthor%3A%20Jiaheng%20Zhuang%20and%20Guoan%20Wang%20and%20Siyu%20Zhang%20and%20Xiyang%20Wang%20and%20Hangning%20Zhou%20and%20Ziyao%20Xu%20and%20Chi%20Zhang%20and%20Zhiheng%20Li%0AAbstract%3A%20%20%203D%20multi-object%20tracking%20and%20trajectory%20prediction%20are%20two%20crucial%20modules%20in%0Aautonomous%20driving%20systems.%20Generally%2C%20the%20two%20tasks%20are%20handled%20separately%20in%0Atraditional%20paradigms%20and%20a%20few%20methods%20have%20started%20to%20explore%20modeling%20these%0Atwo%20tasks%20in%20a%20joint%20manner%20recently.%20However%2C%20these%20approaches%20suffer%20from%20the%0Alimitations%20of%20single-frame%20training%20and%20inconsistent%20coordinate%0Arepresentations%20between%20tracking%20and%20prediction%20tasks.%20In%20this%20paper%2C%20we%0Apropose%20a%20streaming%20and%20unified%20framework%20for%20joint%203D%20Multi-Object%20Tracking%0Aand%20trajectory%20Prediction%20%28StreamMOTP%29%20to%20address%20the%20above%20challenges.%0AFirstly%2C%20we%20construct%20the%20model%20in%20a%20streaming%20manner%20and%20exploit%20a%20memory%20bank%0Ato%20preserve%20and%20leverage%20the%20long-term%20latent%20features%20for%20tracked%20objects%20more%0Aeffectively.%20Secondly%2C%20a%20relative%20spatio-temporal%20positional%20encoding%20strategy%0Ais%20introduced%20to%20bridge%20the%20gap%20of%20coordinate%20representations%20between%20the%20two%0Atasks%20and%20maintain%20the%20pose-invariance%20for%20trajectory%20prediction.%20Thirdly%2C%20we%0Afurther%20improve%20the%20quality%20and%20consistency%20of%20predicted%20trajectories%20with%20a%0Adual-stream%20predictor.%20We%20conduct%20extensive%20experiments%20on%20popular%20nuSences%0Adataset%20and%20the%20experimental%20results%20demonstrate%20the%20effectiveness%20and%0Asuperiority%20of%20StreamMOTP%2C%20which%20outperforms%20previous%20methods%20significantly%20on%0Aboth%20tasks.%20Furthermore%2C%20we%20also%20prove%20that%20the%20proposed%20framework%20has%20great%0Apotential%20and%20advantages%20in%20actual%20applications%20of%20autonomous%20driving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19844v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStreamMOTP%253A%2520Streaming%2520and%2520Unified%2520Framework%2520for%2520Joint%25203D%2520Multi-Object%250A%2520%2520Tracking%2520and%2520Trajectory%2520Prediction%26entry.906535625%3DJiaheng%2520Zhuang%2520and%2520Guoan%2520Wang%2520and%2520Siyu%2520Zhang%2520and%2520Xiyang%2520Wang%2520and%2520Hangning%2520Zhou%2520and%2520Ziyao%2520Xu%2520and%2520Chi%2520Zhang%2520and%2520Zhiheng%2520Li%26entry.1292438233%3D%2520%25203D%2520multi-object%2520tracking%2520and%2520trajectory%2520prediction%2520are%2520two%2520crucial%2520modules%2520in%250Aautonomous%2520driving%2520systems.%2520Generally%252C%2520the%2520two%2520tasks%2520are%2520handled%2520separately%2520in%250Atraditional%2520paradigms%2520and%2520a%2520few%2520methods%2520have%2520started%2520to%2520explore%2520modeling%2520these%250Atwo%2520tasks%2520in%2520a%2520joint%2520manner%2520recently.%2520However%252C%2520these%2520approaches%2520suffer%2520from%2520the%250Alimitations%2520of%2520single-frame%2520training%2520and%2520inconsistent%2520coordinate%250Arepresentations%2520between%2520tracking%2520and%2520prediction%2520tasks.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520streaming%2520and%2520unified%2520framework%2520for%2520joint%25203D%2520Multi-Object%2520Tracking%250Aand%2520trajectory%2520Prediction%2520%2528StreamMOTP%2529%2520to%2520address%2520the%2520above%2520challenges.%250AFirstly%252C%2520we%2520construct%2520the%2520model%2520in%2520a%2520streaming%2520manner%2520and%2520exploit%2520a%2520memory%2520bank%250Ato%2520preserve%2520and%2520leverage%2520the%2520long-term%2520latent%2520features%2520for%2520tracked%2520objects%2520more%250Aeffectively.%2520Secondly%252C%2520a%2520relative%2520spatio-temporal%2520positional%2520encoding%2520strategy%250Ais%2520introduced%2520to%2520bridge%2520the%2520gap%2520of%2520coordinate%2520representations%2520between%2520the%2520two%250Atasks%2520and%2520maintain%2520the%2520pose-invariance%2520for%2520trajectory%2520prediction.%2520Thirdly%252C%2520we%250Afurther%2520improve%2520the%2520quality%2520and%2520consistency%2520of%2520predicted%2520trajectories%2520with%2520a%250Adual-stream%2520predictor.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520popular%2520nuSences%250Adataset%2520and%2520the%2520experimental%2520results%2520demonstrate%2520the%2520effectiveness%2520and%250Asuperiority%2520of%2520StreamMOTP%252C%2520which%2520outperforms%2520previous%2520methods%2520significantly%2520on%250Aboth%2520tasks.%2520Furthermore%252C%2520we%2520also%2520prove%2520that%2520the%2520proposed%2520framework%2520has%2520great%250Apotential%2520and%2520advantages%2520in%2520actual%2520applications%2520of%2520autonomous%2520driving.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19844v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StreamMOTP%3A%20Streaming%20and%20Unified%20Framework%20for%20Joint%203D%20Multi-Object%0A%20%20Tracking%20and%20Trajectory%20Prediction&entry.906535625=Jiaheng%20Zhuang%20and%20Guoan%20Wang%20and%20Siyu%20Zhang%20and%20Xiyang%20Wang%20and%20Hangning%20Zhou%20and%20Ziyao%20Xu%20and%20Chi%20Zhang%20and%20Zhiheng%20Li&entry.1292438233=%20%203D%20multi-object%20tracking%20and%20trajectory%20prediction%20are%20two%20crucial%20modules%20in%0Aautonomous%20driving%20systems.%20Generally%2C%20the%20two%20tasks%20are%20handled%20separately%20in%0Atraditional%20paradigms%20and%20a%20few%20methods%20have%20started%20to%20explore%20modeling%20these%0Atwo%20tasks%20in%20a%20joint%20manner%20recently.%20However%2C%20these%20approaches%20suffer%20from%20the%0Alimitations%20of%20single-frame%20training%20and%20inconsistent%20coordinate%0Arepresentations%20between%20tracking%20and%20prediction%20tasks.%20In%20this%20paper%2C%20we%0Apropose%20a%20streaming%20and%20unified%20framework%20for%20joint%203D%20Multi-Object%20Tracking%0Aand%20trajectory%20Prediction%20%28StreamMOTP%29%20to%20address%20the%20above%20challenges.%0AFirstly%2C%20we%20construct%20the%20model%20in%20a%20streaming%20manner%20and%20exploit%20a%20memory%20bank%0Ato%20preserve%20and%20leverage%20the%20long-term%20latent%20features%20for%20tracked%20objects%20more%0Aeffectively.%20Secondly%2C%20a%20relative%20spatio-temporal%20positional%20encoding%20strategy%0Ais%20introduced%20to%20bridge%20the%20gap%20of%20coordinate%20representations%20between%20the%20two%0Atasks%20and%20maintain%20the%20pose-invariance%20for%20trajectory%20prediction.%20Thirdly%2C%20we%0Afurther%20improve%20the%20quality%20and%20consistency%20of%20predicted%20trajectories%20with%20a%0Adual-stream%20predictor.%20We%20conduct%20extensive%20experiments%20on%20popular%20nuSences%0Adataset%20and%20the%20experimental%20results%20demonstrate%20the%20effectiveness%20and%0Asuperiority%20of%20StreamMOTP%2C%20which%20outperforms%20previous%20methods%20significantly%20on%0Aboth%20tasks.%20Furthermore%2C%20we%20also%20prove%20that%20the%20proposed%20framework%20has%20great%0Apotential%20and%20advantages%20in%20actual%20applications%20of%20autonomous%20driving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19844v1&entry.124074799=Read"},
{"title": "MulTi-Wise Sampling: Trading Uniform T-Wise Feature Interaction Coverage\n  for Smaller Samples", "author": "Tobias Pett and Sebastian Krieter and Thomas Th\u00fcm and Ina Schaefer", "abstract": "  Ensuring the functional safety of highly configurable systems often requires\ntesting representative subsets of all possible configurations to reduce testing\neffort and save resources. The ratio of covered t-wise feature interactions\n(i.e., T-Wise Feature Interaction Coverage) is a common criterion for\ndetermining whether a subset of configurations is representative and capable of\nfinding faults. Existing t-wise sampling algorithms uniformly cover t-wise\nfeature interactions for all features, resulting in lengthy execution times and\nlarge sample sizes, particularly when large t-wise feature interactions are\nconsidered (i.e., high values of t). In this paper, we introduce a novel\napproach to t-wise feature interaction sampling, questioning the necessity of\nuniform coverage across all t-wise feature interactions, called\n\\emph{\\mulTiWise{}}. Our approach prioritizes between subsets of critical and\nnon-critical features, considering higher t-values for subsets of critical\nfeatures when generating a t-wise feature interaction sample. We evaluate our\napproach using subject systems from real-world applications, including\n\\busybox{}, \\soletta{}, \\fiasco{}, and \\uclibc{}. Our results show that\nsacrificing uniform t-wise feature interaction coverage between all features\nreduces the time needed to generate a sample and the resulting sample size.\nHence, \\mulTiWise{} Sampling offers an alternative to existing approaches if\nknowledge about feature criticality is available.\n", "link": "http://arxiv.org/abs/2406.19801v1", "date": "2024-06-28", "relevancy": 2.1709, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4396}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4376}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4254}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MulTi-Wise%20Sampling%3A%20Trading%20Uniform%20T-Wise%20Feature%20Interaction%20Coverage%0A%20%20for%20Smaller%20Samples&body=Title%3A%20MulTi-Wise%20Sampling%3A%20Trading%20Uniform%20T-Wise%20Feature%20Interaction%20Coverage%0A%20%20for%20Smaller%20Samples%0AAuthor%3A%20Tobias%20Pett%20and%20Sebastian%20Krieter%20and%20Thomas%20Th%C3%BCm%20and%20Ina%20Schaefer%0AAbstract%3A%20%20%20Ensuring%20the%20functional%20safety%20of%20highly%20configurable%20systems%20often%20requires%0Atesting%20representative%20subsets%20of%20all%20possible%20configurations%20to%20reduce%20testing%0Aeffort%20and%20save%20resources.%20The%20ratio%20of%20covered%20t-wise%20feature%20interactions%0A%28i.e.%2C%20T-Wise%20Feature%20Interaction%20Coverage%29%20is%20a%20common%20criterion%20for%0Adetermining%20whether%20a%20subset%20of%20configurations%20is%20representative%20and%20capable%20of%0Afinding%20faults.%20Existing%20t-wise%20sampling%20algorithms%20uniformly%20cover%20t-wise%0Afeature%20interactions%20for%20all%20features%2C%20resulting%20in%20lengthy%20execution%20times%20and%0Alarge%20sample%20sizes%2C%20particularly%20when%20large%20t-wise%20feature%20interactions%20are%0Aconsidered%20%28i.e.%2C%20high%20values%20of%20t%29.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%0Aapproach%20to%20t-wise%20feature%20interaction%20sampling%2C%20questioning%20the%20necessity%20of%0Auniform%20coverage%20across%20all%20t-wise%20feature%20interactions%2C%20called%0A%5Cemph%7B%5CmulTiWise%7B%7D%7D.%20Our%20approach%20prioritizes%20between%20subsets%20of%20critical%20and%0Anon-critical%20features%2C%20considering%20higher%20t-values%20for%20subsets%20of%20critical%0Afeatures%20when%20generating%20a%20t-wise%20feature%20interaction%20sample.%20We%20evaluate%20our%0Aapproach%20using%20subject%20systems%20from%20real-world%20applications%2C%20including%0A%5Cbusybox%7B%7D%2C%20%5Csoletta%7B%7D%2C%20%5Cfiasco%7B%7D%2C%20and%20%5Cuclibc%7B%7D.%20Our%20results%20show%20that%0Asacrificing%20uniform%20t-wise%20feature%20interaction%20coverage%20between%20all%20features%0Areduces%20the%20time%20needed%20to%20generate%20a%20sample%20and%20the%20resulting%20sample%20size.%0AHence%2C%20%5CmulTiWise%7B%7D%20Sampling%20offers%20an%20alternative%20to%20existing%20approaches%20if%0Aknowledge%20about%20feature%20criticality%20is%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19801v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulTi-Wise%2520Sampling%253A%2520Trading%2520Uniform%2520T-Wise%2520Feature%2520Interaction%2520Coverage%250A%2520%2520for%2520Smaller%2520Samples%26entry.906535625%3DTobias%2520Pett%2520and%2520Sebastian%2520Krieter%2520and%2520Thomas%2520Th%25C3%25BCm%2520and%2520Ina%2520Schaefer%26entry.1292438233%3D%2520%2520Ensuring%2520the%2520functional%2520safety%2520of%2520highly%2520configurable%2520systems%2520often%2520requires%250Atesting%2520representative%2520subsets%2520of%2520all%2520possible%2520configurations%2520to%2520reduce%2520testing%250Aeffort%2520and%2520save%2520resources.%2520The%2520ratio%2520of%2520covered%2520t-wise%2520feature%2520interactions%250A%2528i.e.%252C%2520T-Wise%2520Feature%2520Interaction%2520Coverage%2529%2520is%2520a%2520common%2520criterion%2520for%250Adetermining%2520whether%2520a%2520subset%2520of%2520configurations%2520is%2520representative%2520and%2520capable%2520of%250Afinding%2520faults.%2520Existing%2520t-wise%2520sampling%2520algorithms%2520uniformly%2520cover%2520t-wise%250Afeature%2520interactions%2520for%2520all%2520features%252C%2520resulting%2520in%2520lengthy%2520execution%2520times%2520and%250Alarge%2520sample%2520sizes%252C%2520particularly%2520when%2520large%2520t-wise%2520feature%2520interactions%2520are%250Aconsidered%2520%2528i.e.%252C%2520high%2520values%2520of%2520t%2529.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%250Aapproach%2520to%2520t-wise%2520feature%2520interaction%2520sampling%252C%2520questioning%2520the%2520necessity%2520of%250Auniform%2520coverage%2520across%2520all%2520t-wise%2520feature%2520interactions%252C%2520called%250A%255Cemph%257B%255CmulTiWise%257B%257D%257D.%2520Our%2520approach%2520prioritizes%2520between%2520subsets%2520of%2520critical%2520and%250Anon-critical%2520features%252C%2520considering%2520higher%2520t-values%2520for%2520subsets%2520of%2520critical%250Afeatures%2520when%2520generating%2520a%2520t-wise%2520feature%2520interaction%2520sample.%2520We%2520evaluate%2520our%250Aapproach%2520using%2520subject%2520systems%2520from%2520real-world%2520applications%252C%2520including%250A%255Cbusybox%257B%257D%252C%2520%255Csoletta%257B%257D%252C%2520%255Cfiasco%257B%257D%252C%2520and%2520%255Cuclibc%257B%257D.%2520Our%2520results%2520show%2520that%250Asacrificing%2520uniform%2520t-wise%2520feature%2520interaction%2520coverage%2520between%2520all%2520features%250Areduces%2520the%2520time%2520needed%2520to%2520generate%2520a%2520sample%2520and%2520the%2520resulting%2520sample%2520size.%250AHence%252C%2520%255CmulTiWise%257B%257D%2520Sampling%2520offers%2520an%2520alternative%2520to%2520existing%2520approaches%2520if%250Aknowledge%2520about%2520feature%2520criticality%2520is%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19801v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MulTi-Wise%20Sampling%3A%20Trading%20Uniform%20T-Wise%20Feature%20Interaction%20Coverage%0A%20%20for%20Smaller%20Samples&entry.906535625=Tobias%20Pett%20and%20Sebastian%20Krieter%20and%20Thomas%20Th%C3%BCm%20and%20Ina%20Schaefer&entry.1292438233=%20%20Ensuring%20the%20functional%20safety%20of%20highly%20configurable%20systems%20often%20requires%0Atesting%20representative%20subsets%20of%20all%20possible%20configurations%20to%20reduce%20testing%0Aeffort%20and%20save%20resources.%20The%20ratio%20of%20covered%20t-wise%20feature%20interactions%0A%28i.e.%2C%20T-Wise%20Feature%20Interaction%20Coverage%29%20is%20a%20common%20criterion%20for%0Adetermining%20whether%20a%20subset%20of%20configurations%20is%20representative%20and%20capable%20of%0Afinding%20faults.%20Existing%20t-wise%20sampling%20algorithms%20uniformly%20cover%20t-wise%0Afeature%20interactions%20for%20all%20features%2C%20resulting%20in%20lengthy%20execution%20times%20and%0Alarge%20sample%20sizes%2C%20particularly%20when%20large%20t-wise%20feature%20interactions%20are%0Aconsidered%20%28i.e.%2C%20high%20values%20of%20t%29.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%0Aapproach%20to%20t-wise%20feature%20interaction%20sampling%2C%20questioning%20the%20necessity%20of%0Auniform%20coverage%20across%20all%20t-wise%20feature%20interactions%2C%20called%0A%5Cemph%7B%5CmulTiWise%7B%7D%7D.%20Our%20approach%20prioritizes%20between%20subsets%20of%20critical%20and%0Anon-critical%20features%2C%20considering%20higher%20t-values%20for%20subsets%20of%20critical%0Afeatures%20when%20generating%20a%20t-wise%20feature%20interaction%20sample.%20We%20evaluate%20our%0Aapproach%20using%20subject%20systems%20from%20real-world%20applications%2C%20including%0A%5Cbusybox%7B%7D%2C%20%5Csoletta%7B%7D%2C%20%5Cfiasco%7B%7D%2C%20and%20%5Cuclibc%7B%7D.%20Our%20results%20show%20that%0Asacrificing%20uniform%20t-wise%20feature%20interaction%20coverage%20between%20all%20features%0Areduces%20the%20time%20needed%20to%20generate%20a%20sample%20and%20the%20resulting%20sample%20size.%0AHence%2C%20%5CmulTiWise%7B%7D%20Sampling%20offers%20an%20alternative%20to%20existing%20approaches%20if%0Aknowledge%20about%20feature%20criticality%20is%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19801v1&entry.124074799=Read"},
{"title": "Emotion Loss Attacking: Adversarial Attack Perception for Skeleton based\n  on Multi-dimensional Features", "author": "Feng Liu and Qing Xu and Qijian Zheng", "abstract": "  Adversarial attack on skeletal motion is a hot topic. However, existing\nresearches only consider part of dynamic features when measuring distance\nbetween skeleton graph sequences, which results in poor imperceptibility. To\nthis end, we propose a novel adversarial attack method to attack action\nrecognizers for skeletal motions. Firstly, our method systematically proposes a\ndynamic distance function to measure the difference between skeletal motions.\nMeanwhile, we innovatively introduce emotional features for complementary\ninformation. In addition, we use Alternating Direction Method of\nMultipliers(ADMM) to solve the constrained optimization problem, which\ngenerates adversarial samples with better imperceptibility to deceive the\nclassifiers. Experiments show that our method is effective on multiple action\nclassifiers and datasets. When the perturbation magnitude measured by l norms\nis the same, the dynamic perturbations generated by our method are much lower\nthan that of other methods. What's more, we are the first to prove the\neffectiveness of emotional features, and provide a new idea for measuring the\ndistance between skeletal motions.\n", "link": "http://arxiv.org/abs/2406.19815v1", "date": "2024-06-28", "relevancy": 2.1499, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5575}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5399}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Emotion%20Loss%20Attacking%3A%20Adversarial%20Attack%20Perception%20for%20Skeleton%20based%0A%20%20on%20Multi-dimensional%20Features&body=Title%3A%20Emotion%20Loss%20Attacking%3A%20Adversarial%20Attack%20Perception%20for%20Skeleton%20based%0A%20%20on%20Multi-dimensional%20Features%0AAuthor%3A%20Feng%20Liu%20and%20Qing%20Xu%20and%20Qijian%20Zheng%0AAbstract%3A%20%20%20Adversarial%20attack%20on%20skeletal%20motion%20is%20a%20hot%20topic.%20However%2C%20existing%0Aresearches%20only%20consider%20part%20of%20dynamic%20features%20when%20measuring%20distance%0Abetween%20skeleton%20graph%20sequences%2C%20which%20results%20in%20poor%20imperceptibility.%20To%0Athis%20end%2C%20we%20propose%20a%20novel%20adversarial%20attack%20method%20to%20attack%20action%0Arecognizers%20for%20skeletal%20motions.%20Firstly%2C%20our%20method%20systematically%20proposes%20a%0Adynamic%20distance%20function%20to%20measure%20the%20difference%20between%20skeletal%20motions.%0AMeanwhile%2C%20we%20innovatively%20introduce%20emotional%20features%20for%20complementary%0Ainformation.%20In%20addition%2C%20we%20use%20Alternating%20Direction%20Method%20of%0AMultipliers%28ADMM%29%20to%20solve%20the%20constrained%20optimization%20problem%2C%20which%0Agenerates%20adversarial%20samples%20with%20better%20imperceptibility%20to%20deceive%20the%0Aclassifiers.%20Experiments%20show%20that%20our%20method%20is%20effective%20on%20multiple%20action%0Aclassifiers%20and%20datasets.%20When%20the%20perturbation%20magnitude%20measured%20by%20l%20norms%0Ais%20the%20same%2C%20the%20dynamic%20perturbations%20generated%20by%20our%20method%20are%20much%20lower%0Athan%20that%20of%20other%20methods.%20What%27s%20more%2C%20we%20are%20the%20first%20to%20prove%20the%0Aeffectiveness%20of%20emotional%20features%2C%20and%20provide%20a%20new%20idea%20for%20measuring%20the%0Adistance%20between%20skeletal%20motions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19815v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmotion%2520Loss%2520Attacking%253A%2520Adversarial%2520Attack%2520Perception%2520for%2520Skeleton%2520based%250A%2520%2520on%2520Multi-dimensional%2520Features%26entry.906535625%3DFeng%2520Liu%2520and%2520Qing%2520Xu%2520and%2520Qijian%2520Zheng%26entry.1292438233%3D%2520%2520Adversarial%2520attack%2520on%2520skeletal%2520motion%2520is%2520a%2520hot%2520topic.%2520However%252C%2520existing%250Aresearches%2520only%2520consider%2520part%2520of%2520dynamic%2520features%2520when%2520measuring%2520distance%250Abetween%2520skeleton%2520graph%2520sequences%252C%2520which%2520results%2520in%2520poor%2520imperceptibility.%2520To%250Athis%2520end%252C%2520we%2520propose%2520a%2520novel%2520adversarial%2520attack%2520method%2520to%2520attack%2520action%250Arecognizers%2520for%2520skeletal%2520motions.%2520Firstly%252C%2520our%2520method%2520systematically%2520proposes%2520a%250Adynamic%2520distance%2520function%2520to%2520measure%2520the%2520difference%2520between%2520skeletal%2520motions.%250AMeanwhile%252C%2520we%2520innovatively%2520introduce%2520emotional%2520features%2520for%2520complementary%250Ainformation.%2520In%2520addition%252C%2520we%2520use%2520Alternating%2520Direction%2520Method%2520of%250AMultipliers%2528ADMM%2529%2520to%2520solve%2520the%2520constrained%2520optimization%2520problem%252C%2520which%250Agenerates%2520adversarial%2520samples%2520with%2520better%2520imperceptibility%2520to%2520deceive%2520the%250Aclassifiers.%2520Experiments%2520show%2520that%2520our%2520method%2520is%2520effective%2520on%2520multiple%2520action%250Aclassifiers%2520and%2520datasets.%2520When%2520the%2520perturbation%2520magnitude%2520measured%2520by%2520l%2520norms%250Ais%2520the%2520same%252C%2520the%2520dynamic%2520perturbations%2520generated%2520by%2520our%2520method%2520are%2520much%2520lower%250Athan%2520that%2520of%2520other%2520methods.%2520What%2527s%2520more%252C%2520we%2520are%2520the%2520first%2520to%2520prove%2520the%250Aeffectiveness%2520of%2520emotional%2520features%252C%2520and%2520provide%2520a%2520new%2520idea%2520for%2520measuring%2520the%250Adistance%2520between%2520skeletal%2520motions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19815v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emotion%20Loss%20Attacking%3A%20Adversarial%20Attack%20Perception%20for%20Skeleton%20based%0A%20%20on%20Multi-dimensional%20Features&entry.906535625=Feng%20Liu%20and%20Qing%20Xu%20and%20Qijian%20Zheng&entry.1292438233=%20%20Adversarial%20attack%20on%20skeletal%20motion%20is%20a%20hot%20topic.%20However%2C%20existing%0Aresearches%20only%20consider%20part%20of%20dynamic%20features%20when%20measuring%20distance%0Abetween%20skeleton%20graph%20sequences%2C%20which%20results%20in%20poor%20imperceptibility.%20To%0Athis%20end%2C%20we%20propose%20a%20novel%20adversarial%20attack%20method%20to%20attack%20action%0Arecognizers%20for%20skeletal%20motions.%20Firstly%2C%20our%20method%20systematically%20proposes%20a%0Adynamic%20distance%20function%20to%20measure%20the%20difference%20between%20skeletal%20motions.%0AMeanwhile%2C%20we%20innovatively%20introduce%20emotional%20features%20for%20complementary%0Ainformation.%20In%20addition%2C%20we%20use%20Alternating%20Direction%20Method%20of%0AMultipliers%28ADMM%29%20to%20solve%20the%20constrained%20optimization%20problem%2C%20which%0Agenerates%20adversarial%20samples%20with%20better%20imperceptibility%20to%20deceive%20the%0Aclassifiers.%20Experiments%20show%20that%20our%20method%20is%20effective%20on%20multiple%20action%0Aclassifiers%20and%20datasets.%20When%20the%20perturbation%20magnitude%20measured%20by%20l%20norms%0Ais%20the%20same%2C%20the%20dynamic%20perturbations%20generated%20by%20our%20method%20are%20much%20lower%0Athan%20that%20of%20other%20methods.%20What%27s%20more%2C%20we%20are%20the%20first%20to%20prove%20the%0Aeffectiveness%20of%20emotional%20features%2C%20and%20provide%20a%20new%20idea%20for%20measuring%20the%0Adistance%20between%20skeletal%20motions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19815v1&entry.124074799=Read"},
{"title": "Wavelets Are All You Need for Autoregressive Image Generation", "author": "Wael Mattar and Idan Levy and Nir Sharon and Shai Dekel", "abstract": "  In this paper, we take a new approach to autoregressive image generation that\nis based on two main ingredients. The first is wavelet image coding, which\nallows to tokenize the visual details of an image from coarse to fine details\nby ordering the information starting with the most significant bits of the most\nsignificant wavelet coefficients. The second is a variant of a language\ntransformer whose architecture is re-designed and optimized for token sequences\nin this 'wavelet language'. The transformer learns the significant statistical\ncorrelations within a token sequence, which are the manifestations of\nwell-known correlations between the wavelet subbands at various resolutions. We\nshow experimental results with conditioning on the generation process.\n", "link": "http://arxiv.org/abs/2406.19997v1", "date": "2024-06-28", "relevancy": 2.1485, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5621}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5381}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5262}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Wavelets%20Are%20All%20You%20Need%20for%20Autoregressive%20Image%20Generation&body=Title%3A%20Wavelets%20Are%20All%20You%20Need%20for%20Autoregressive%20Image%20Generation%0AAuthor%3A%20Wael%20Mattar%20and%20Idan%20Levy%20and%20Nir%20Sharon%20and%20Shai%20Dekel%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20take%20a%20new%20approach%20to%20autoregressive%20image%20generation%20that%0Ais%20based%20on%20two%20main%20ingredients.%20The%20first%20is%20wavelet%20image%20coding%2C%20which%0Aallows%20to%20tokenize%20the%20visual%20details%20of%20an%20image%20from%20coarse%20to%20fine%20details%0Aby%20ordering%20the%20information%20starting%20with%20the%20most%20significant%20bits%20of%20the%20most%0Asignificant%20wavelet%20coefficients.%20The%20second%20is%20a%20variant%20of%20a%20language%0Atransformer%20whose%20architecture%20is%20re-designed%20and%20optimized%20for%20token%20sequences%0Ain%20this%20%27wavelet%20language%27.%20The%20transformer%20learns%20the%20significant%20statistical%0Acorrelations%20within%20a%20token%20sequence%2C%20which%20are%20the%20manifestations%20of%0Awell-known%20correlations%20between%20the%20wavelet%20subbands%20at%20various%20resolutions.%20We%0Ashow%20experimental%20results%20with%20conditioning%20on%20the%20generation%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19997v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWavelets%2520Are%2520All%2520You%2520Need%2520for%2520Autoregressive%2520Image%2520Generation%26entry.906535625%3DWael%2520Mattar%2520and%2520Idan%2520Levy%2520and%2520Nir%2520Sharon%2520and%2520Shai%2520Dekel%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520take%2520a%2520new%2520approach%2520to%2520autoregressive%2520image%2520generation%2520that%250Ais%2520based%2520on%2520two%2520main%2520ingredients.%2520The%2520first%2520is%2520wavelet%2520image%2520coding%252C%2520which%250Aallows%2520to%2520tokenize%2520the%2520visual%2520details%2520of%2520an%2520image%2520from%2520coarse%2520to%2520fine%2520details%250Aby%2520ordering%2520the%2520information%2520starting%2520with%2520the%2520most%2520significant%2520bits%2520of%2520the%2520most%250Asignificant%2520wavelet%2520coefficients.%2520The%2520second%2520is%2520a%2520variant%2520of%2520a%2520language%250Atransformer%2520whose%2520architecture%2520is%2520re-designed%2520and%2520optimized%2520for%2520token%2520sequences%250Ain%2520this%2520%2527wavelet%2520language%2527.%2520The%2520transformer%2520learns%2520the%2520significant%2520statistical%250Acorrelations%2520within%2520a%2520token%2520sequence%252C%2520which%2520are%2520the%2520manifestations%2520of%250Awell-known%2520correlations%2520between%2520the%2520wavelet%2520subbands%2520at%2520various%2520resolutions.%2520We%250Ashow%2520experimental%2520results%2520with%2520conditioning%2520on%2520the%2520generation%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19997v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wavelets%20Are%20All%20You%20Need%20for%20Autoregressive%20Image%20Generation&entry.906535625=Wael%20Mattar%20and%20Idan%20Levy%20and%20Nir%20Sharon%20and%20Shai%20Dekel&entry.1292438233=%20%20In%20this%20paper%2C%20we%20take%20a%20new%20approach%20to%20autoregressive%20image%20generation%20that%0Ais%20based%20on%20two%20main%20ingredients.%20The%20first%20is%20wavelet%20image%20coding%2C%20which%0Aallows%20to%20tokenize%20the%20visual%20details%20of%20an%20image%20from%20coarse%20to%20fine%20details%0Aby%20ordering%20the%20information%20starting%20with%20the%20most%20significant%20bits%20of%20the%20most%0Asignificant%20wavelet%20coefficients.%20The%20second%20is%20a%20variant%20of%20a%20language%0Atransformer%20whose%20architecture%20is%20re-designed%20and%20optimized%20for%20token%20sequences%0Ain%20this%20%27wavelet%20language%27.%20The%20transformer%20learns%20the%20significant%20statistical%0Acorrelations%20within%20a%20token%20sequence%2C%20which%20are%20the%20manifestations%20of%0Awell-known%20correlations%20between%20the%20wavelet%20subbands%20at%20various%20resolutions.%20We%0Ashow%20experimental%20results%20with%20conditioning%20on%20the%20generation%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19997v1&entry.124074799=Read"},
{"title": "STLLaVA-Med: Self-Training Large Language and Vision Assistant for\n  Medical", "author": "Guohao Sun and Can Qin and Huazhu Fu and Linwei Wang and Zhiqiang Tao", "abstract": "  Large Vision-Language Models (LVLMs) have shown significant potential in\nassisting medical diagnosis by leveraging extensive biomedical datasets.\nHowever, the advancement of medical image understanding and reasoning\ncritically depends on building high-quality visual instruction data, which is\ncostly and labor-intensive to obtain, particularly in the medical domain. To\nmitigate this data-starving issue, we introduce Self-Training Large Language\nand Vision Assistant for Medical (STLLaVA-Med). The proposed method is designed\nto train a policy model (an LVLM) capable of auto-generating medical visual\ninstruction data to improve data efficiency, guided through Direct Preference\nOptimization (DPO). Specifically, a more powerful and larger LVLM (e.g.,\nGPT-4o) is involved as a biomedical expert to oversee the DPO fine-tuning\nprocess on the auto-generated data, encouraging the policy model to align\nefficiently with human preferences. We validate the efficacy and data\nefficiency of STLLaVA-Med across three major medical Visual Question Answering\n(VQA) benchmarks, demonstrating competitive zero-shot performance with the\nutilization of only 9% of the medical data.\n", "link": "http://arxiv.org/abs/2406.19973v1", "date": "2024-06-28", "relevancy": 2.1451, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5716}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5304}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5281}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STLLaVA-Med%3A%20Self-Training%20Large%20Language%20and%20Vision%20Assistant%20for%0A%20%20Medical&body=Title%3A%20STLLaVA-Med%3A%20Self-Training%20Large%20Language%20and%20Vision%20Assistant%20for%0A%20%20Medical%0AAuthor%3A%20Guohao%20Sun%20and%20Can%20Qin%20and%20Huazhu%20Fu%20and%20Linwei%20Wang%20and%20Zhiqiang%20Tao%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20shown%20significant%20potential%20in%0Aassisting%20medical%20diagnosis%20by%20leveraging%20extensive%20biomedical%20datasets.%0AHowever%2C%20the%20advancement%20of%20medical%20image%20understanding%20and%20reasoning%0Acritically%20depends%20on%20building%20high-quality%20visual%20instruction%20data%2C%20which%20is%0Acostly%20and%20labor-intensive%20to%20obtain%2C%20particularly%20in%20the%20medical%20domain.%20To%0Amitigate%20this%20data-starving%20issue%2C%20we%20introduce%20Self-Training%20Large%20Language%0Aand%20Vision%20Assistant%20for%20Medical%20%28STLLaVA-Med%29.%20The%20proposed%20method%20is%20designed%0Ato%20train%20a%20policy%20model%20%28an%20LVLM%29%20capable%20of%20auto-generating%20medical%20visual%0Ainstruction%20data%20to%20improve%20data%20efficiency%2C%20guided%20through%20Direct%20Preference%0AOptimization%20%28DPO%29.%20Specifically%2C%20a%20more%20powerful%20and%20larger%20LVLM%20%28e.g.%2C%0AGPT-4o%29%20is%20involved%20as%20a%20biomedical%20expert%20to%20oversee%20the%20DPO%20fine-tuning%0Aprocess%20on%20the%20auto-generated%20data%2C%20encouraging%20the%20policy%20model%20to%20align%0Aefficiently%20with%20human%20preferences.%20We%20validate%20the%20efficacy%20and%20data%0Aefficiency%20of%20STLLaVA-Med%20across%20three%20major%20medical%20Visual%20Question%20Answering%0A%28VQA%29%20benchmarks%2C%20demonstrating%20competitive%20zero-shot%20performance%20with%20the%0Autilization%20of%20only%209%25%20of%20the%20medical%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19973v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTLLaVA-Med%253A%2520Self-Training%2520Large%2520Language%2520and%2520Vision%2520Assistant%2520for%250A%2520%2520Medical%26entry.906535625%3DGuohao%2520Sun%2520and%2520Can%2520Qin%2520and%2520Huazhu%2520Fu%2520and%2520Linwei%2520Wang%2520and%2520Zhiqiang%2520Tao%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520have%2520shown%2520significant%2520potential%2520in%250Aassisting%2520medical%2520diagnosis%2520by%2520leveraging%2520extensive%2520biomedical%2520datasets.%250AHowever%252C%2520the%2520advancement%2520of%2520medical%2520image%2520understanding%2520and%2520reasoning%250Acritically%2520depends%2520on%2520building%2520high-quality%2520visual%2520instruction%2520data%252C%2520which%2520is%250Acostly%2520and%2520labor-intensive%2520to%2520obtain%252C%2520particularly%2520in%2520the%2520medical%2520domain.%2520To%250Amitigate%2520this%2520data-starving%2520issue%252C%2520we%2520introduce%2520Self-Training%2520Large%2520Language%250Aand%2520Vision%2520Assistant%2520for%2520Medical%2520%2528STLLaVA-Med%2529.%2520The%2520proposed%2520method%2520is%2520designed%250Ato%2520train%2520a%2520policy%2520model%2520%2528an%2520LVLM%2529%2520capable%2520of%2520auto-generating%2520medical%2520visual%250Ainstruction%2520data%2520to%2520improve%2520data%2520efficiency%252C%2520guided%2520through%2520Direct%2520Preference%250AOptimization%2520%2528DPO%2529.%2520Specifically%252C%2520a%2520more%2520powerful%2520and%2520larger%2520LVLM%2520%2528e.g.%252C%250AGPT-4o%2529%2520is%2520involved%2520as%2520a%2520biomedical%2520expert%2520to%2520oversee%2520the%2520DPO%2520fine-tuning%250Aprocess%2520on%2520the%2520auto-generated%2520data%252C%2520encouraging%2520the%2520policy%2520model%2520to%2520align%250Aefficiently%2520with%2520human%2520preferences.%2520We%2520validate%2520the%2520efficacy%2520and%2520data%250Aefficiency%2520of%2520STLLaVA-Med%2520across%2520three%2520major%2520medical%2520Visual%2520Question%2520Answering%250A%2528VQA%2529%2520benchmarks%252C%2520demonstrating%2520competitive%2520zero-shot%2520performance%2520with%2520the%250Autilization%2520of%2520only%25209%2525%2520of%2520the%2520medical%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19973v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STLLaVA-Med%3A%20Self-Training%20Large%20Language%20and%20Vision%20Assistant%20for%0A%20%20Medical&entry.906535625=Guohao%20Sun%20and%20Can%20Qin%20and%20Huazhu%20Fu%20and%20Linwei%20Wang%20and%20Zhiqiang%20Tao&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20shown%20significant%20potential%20in%0Aassisting%20medical%20diagnosis%20by%20leveraging%20extensive%20biomedical%20datasets.%0AHowever%2C%20the%20advancement%20of%20medical%20image%20understanding%20and%20reasoning%0Acritically%20depends%20on%20building%20high-quality%20visual%20instruction%20data%2C%20which%20is%0Acostly%20and%20labor-intensive%20to%20obtain%2C%20particularly%20in%20the%20medical%20domain.%20To%0Amitigate%20this%20data-starving%20issue%2C%20we%20introduce%20Self-Training%20Large%20Language%0Aand%20Vision%20Assistant%20for%20Medical%20%28STLLaVA-Med%29.%20The%20proposed%20method%20is%20designed%0Ato%20train%20a%20policy%20model%20%28an%20LVLM%29%20capable%20of%20auto-generating%20medical%20visual%0Ainstruction%20data%20to%20improve%20data%20efficiency%2C%20guided%20through%20Direct%20Preference%0AOptimization%20%28DPO%29.%20Specifically%2C%20a%20more%20powerful%20and%20larger%20LVLM%20%28e.g.%2C%0AGPT-4o%29%20is%20involved%20as%20a%20biomedical%20expert%20to%20oversee%20the%20DPO%20fine-tuning%0Aprocess%20on%20the%20auto-generated%20data%2C%20encouraging%20the%20policy%20model%20to%20align%0Aefficiently%20with%20human%20preferences.%20We%20validate%20the%20efficacy%20and%20data%0Aefficiency%20of%20STLLaVA-Med%20across%20three%20major%20medical%20Visual%20Question%20Answering%0A%28VQA%29%20benchmarks%2C%20demonstrating%20competitive%20zero-shot%20performance%20with%20the%0Autilization%20of%20only%209%25%20of%20the%20medical%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19973v1&entry.124074799=Read"},
{"title": "Learning from Successful and Failed Demonstrations via Optimization", "author": "Brendan Hertel and S. Reza Ahmadzadeh", "abstract": "  Learning from Demonstration (LfD) is a popular approach that allows humans to\nteach robots new skills by showing the correct way(s) of performing the desired\nskill. Human-provided demonstrations, however, are not always optimal and the\nteacher usually addresses this issue by discarding or replacing sub-optimal\n(noisy or faulty) demonstrations. We propose a novel LfD representation that\nlearns from both successful and failed demonstrations of a skill. Our approach\nencodes the two subsets of captured demonstrations (labeled by the teacher)\ninto a statistical skill model, constructs a set of quadratic costs, and finds\nan optimal reproduction of the skill under novel problem conditions (i.e.\nconstraints). The optimal reproduction balances convergence towards successful\nexamples and divergence from failed examples. We evaluate our approach through\nseveral 2D and 3D experiments in real-world using a UR5e manipulator arm and\nalso show that it can reproduce a skill from only failed demonstrations. The\nbenefits of exploiting both failed and successful demonstrations are shown\nthrough comparison with two existing LfD approaches. We also compare our\napproach against an existing skill refinement method and show its capabilities\nin a multi-coordinate setting.\n", "link": "http://arxiv.org/abs/2107.11918v2", "date": "2024-06-28", "relevancy": 2.1314, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5432}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5312}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20Successful%20and%20Failed%20Demonstrations%20via%20Optimization&body=Title%3A%20Learning%20from%20Successful%20and%20Failed%20Demonstrations%20via%20Optimization%0AAuthor%3A%20Brendan%20Hertel%20and%20S.%20Reza%20Ahmadzadeh%0AAbstract%3A%20%20%20Learning%20from%20Demonstration%20%28LfD%29%20is%20a%20popular%20approach%20that%20allows%20humans%20to%0Ateach%20robots%20new%20skills%20by%20showing%20the%20correct%20way%28s%29%20of%20performing%20the%20desired%0Askill.%20Human-provided%20demonstrations%2C%20however%2C%20are%20not%20always%20optimal%20and%20the%0Ateacher%20usually%20addresses%20this%20issue%20by%20discarding%20or%20replacing%20sub-optimal%0A%28noisy%20or%20faulty%29%20demonstrations.%20We%20propose%20a%20novel%20LfD%20representation%20that%0Alearns%20from%20both%20successful%20and%20failed%20demonstrations%20of%20a%20skill.%20Our%20approach%0Aencodes%20the%20two%20subsets%20of%20captured%20demonstrations%20%28labeled%20by%20the%20teacher%29%0Ainto%20a%20statistical%20skill%20model%2C%20constructs%20a%20set%20of%20quadratic%20costs%2C%20and%20finds%0Aan%20optimal%20reproduction%20of%20the%20skill%20under%20novel%20problem%20conditions%20%28i.e.%0Aconstraints%29.%20The%20optimal%20reproduction%20balances%20convergence%20towards%20successful%0Aexamples%20and%20divergence%20from%20failed%20examples.%20We%20evaluate%20our%20approach%20through%0Aseveral%202D%20and%203D%20experiments%20in%20real-world%20using%20a%20UR5e%20manipulator%20arm%20and%0Aalso%20show%20that%20it%20can%20reproduce%20a%20skill%20from%20only%20failed%20demonstrations.%20The%0Abenefits%20of%20exploiting%20both%20failed%20and%20successful%20demonstrations%20are%20shown%0Athrough%20comparison%20with%20two%20existing%20LfD%20approaches.%20We%20also%20compare%20our%0Aapproach%20against%20an%20existing%20skill%20refinement%20method%20and%20show%20its%20capabilities%0Ain%20a%20multi-coordinate%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2107.11918v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520from%2520Successful%2520and%2520Failed%2520Demonstrations%2520via%2520Optimization%26entry.906535625%3DBrendan%2520Hertel%2520and%2520S.%2520Reza%2520Ahmadzadeh%26entry.1292438233%3D%2520%2520Learning%2520from%2520Demonstration%2520%2528LfD%2529%2520is%2520a%2520popular%2520approach%2520that%2520allows%2520humans%2520to%250Ateach%2520robots%2520new%2520skills%2520by%2520showing%2520the%2520correct%2520way%2528s%2529%2520of%2520performing%2520the%2520desired%250Askill.%2520Human-provided%2520demonstrations%252C%2520however%252C%2520are%2520not%2520always%2520optimal%2520and%2520the%250Ateacher%2520usually%2520addresses%2520this%2520issue%2520by%2520discarding%2520or%2520replacing%2520sub-optimal%250A%2528noisy%2520or%2520faulty%2529%2520demonstrations.%2520We%2520propose%2520a%2520novel%2520LfD%2520representation%2520that%250Alearns%2520from%2520both%2520successful%2520and%2520failed%2520demonstrations%2520of%2520a%2520skill.%2520Our%2520approach%250Aencodes%2520the%2520two%2520subsets%2520of%2520captured%2520demonstrations%2520%2528labeled%2520by%2520the%2520teacher%2529%250Ainto%2520a%2520statistical%2520skill%2520model%252C%2520constructs%2520a%2520set%2520of%2520quadratic%2520costs%252C%2520and%2520finds%250Aan%2520optimal%2520reproduction%2520of%2520the%2520skill%2520under%2520novel%2520problem%2520conditions%2520%2528i.e.%250Aconstraints%2529.%2520The%2520optimal%2520reproduction%2520balances%2520convergence%2520towards%2520successful%250Aexamples%2520and%2520divergence%2520from%2520failed%2520examples.%2520We%2520evaluate%2520our%2520approach%2520through%250Aseveral%25202D%2520and%25203D%2520experiments%2520in%2520real-world%2520using%2520a%2520UR5e%2520manipulator%2520arm%2520and%250Aalso%2520show%2520that%2520it%2520can%2520reproduce%2520a%2520skill%2520from%2520only%2520failed%2520demonstrations.%2520The%250Abenefits%2520of%2520exploiting%2520both%2520failed%2520and%2520successful%2520demonstrations%2520are%2520shown%250Athrough%2520comparison%2520with%2520two%2520existing%2520LfD%2520approaches.%2520We%2520also%2520compare%2520our%250Aapproach%2520against%2520an%2520existing%2520skill%2520refinement%2520method%2520and%2520show%2520its%2520capabilities%250Ain%2520a%2520multi-coordinate%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2107.11918v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20Successful%20and%20Failed%20Demonstrations%20via%20Optimization&entry.906535625=Brendan%20Hertel%20and%20S.%20Reza%20Ahmadzadeh&entry.1292438233=%20%20Learning%20from%20Demonstration%20%28LfD%29%20is%20a%20popular%20approach%20that%20allows%20humans%20to%0Ateach%20robots%20new%20skills%20by%20showing%20the%20correct%20way%28s%29%20of%20performing%20the%20desired%0Askill.%20Human-provided%20demonstrations%2C%20however%2C%20are%20not%20always%20optimal%20and%20the%0Ateacher%20usually%20addresses%20this%20issue%20by%20discarding%20or%20replacing%20sub-optimal%0A%28noisy%20or%20faulty%29%20demonstrations.%20We%20propose%20a%20novel%20LfD%20representation%20that%0Alearns%20from%20both%20successful%20and%20failed%20demonstrations%20of%20a%20skill.%20Our%20approach%0Aencodes%20the%20two%20subsets%20of%20captured%20demonstrations%20%28labeled%20by%20the%20teacher%29%0Ainto%20a%20statistical%20skill%20model%2C%20constructs%20a%20set%20of%20quadratic%20costs%2C%20and%20finds%0Aan%20optimal%20reproduction%20of%20the%20skill%20under%20novel%20problem%20conditions%20%28i.e.%0Aconstraints%29.%20The%20optimal%20reproduction%20balances%20convergence%20towards%20successful%0Aexamples%20and%20divergence%20from%20failed%20examples.%20We%20evaluate%20our%20approach%20through%0Aseveral%202D%20and%203D%20experiments%20in%20real-world%20using%20a%20UR5e%20manipulator%20arm%20and%0Aalso%20show%20that%20it%20can%20reproduce%20a%20skill%20from%20only%20failed%20demonstrations.%20The%0Abenefits%20of%20exploiting%20both%20failed%20and%20successful%20demonstrations%20are%20shown%0Athrough%20comparison%20with%20two%20existing%20LfD%20approaches.%20We%20also%20compare%20our%0Aapproach%20against%20an%20existing%20skill%20refinement%20method%20and%20show%20its%20capabilities%0Ain%20a%20multi-coordinate%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2107.11918v2&entry.124074799=Read"},
{"title": "Tracking Object Positions in Reinforcement Learning: A Metric for\n  Keypoint Detection (extended version)", "author": "Emma Cramer and Jonas Reiher and Sebastian Trimpe", "abstract": "  Reinforcement learning (RL) for robot control typically requires a detailed\nrepresentation of the environment state, including information about\ntask-relevant objects not directly measurable. Keypoint detectors, such as\nspatial autoencoders (SAEs), are a common approach to extracting a\nlow-dimensional representation from high-dimensional image data. SAEs aim at\nspatial features such as object positions, which are often useful\nrepresentations in robotic RL. However, whether an SAE is actually able to\ntrack objects in the scene and thus yields a spatial state representation well\nsuited for RL tasks has rarely been examined due to a lack of established\nmetrics. In this paper, we propose to assess the performance of an SAE instance\nby measuring how well keypoints track ground truth objects in images. We\npresent a computationally lightweight metric and use it to evaluate common\nbaseline SAE architectures on image data from a simulated robot task. We find\nthat common SAEs differ substantially in their spatial extraction capability.\nFurthermore, we validate that SAEs that perform well in our metric achieve\nsuperior performance when used in downstream RL. Thus, our metric is an\neffective and lightweight indicator of RL performance before executing\nexpensive RL training. Building on these insights, we identify three key\nmodifications of SAE architectures to improve tracking performance. We make our\ncode available at anonymous.4open.science/r/sae-rl.\n", "link": "http://arxiv.org/abs/2312.00592v2", "date": "2024-06-28", "relevancy": 2.1164, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5728}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5226}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5181}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tracking%20Object%20Positions%20in%20Reinforcement%20Learning%3A%20A%20Metric%20for%0A%20%20Keypoint%20Detection%20%28extended%20version%29&body=Title%3A%20Tracking%20Object%20Positions%20in%20Reinforcement%20Learning%3A%20A%20Metric%20for%0A%20%20Keypoint%20Detection%20%28extended%20version%29%0AAuthor%3A%20Emma%20Cramer%20and%20Jonas%20Reiher%20and%20Sebastian%20Trimpe%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29%20for%20robot%20control%20typically%20requires%20a%20detailed%0Arepresentation%20of%20the%20environment%20state%2C%20including%20information%20about%0Atask-relevant%20objects%20not%20directly%20measurable.%20Keypoint%20detectors%2C%20such%20as%0Aspatial%20autoencoders%20%28SAEs%29%2C%20are%20a%20common%20approach%20to%20extracting%20a%0Alow-dimensional%20representation%20from%20high-dimensional%20image%20data.%20SAEs%20aim%20at%0Aspatial%20features%20such%20as%20object%20positions%2C%20which%20are%20often%20useful%0Arepresentations%20in%20robotic%20RL.%20However%2C%20whether%20an%20SAE%20is%20actually%20able%20to%0Atrack%20objects%20in%20the%20scene%20and%20thus%20yields%20a%20spatial%20state%20representation%20well%0Asuited%20for%20RL%20tasks%20has%20rarely%20been%20examined%20due%20to%20a%20lack%20of%20established%0Ametrics.%20In%20this%20paper%2C%20we%20propose%20to%20assess%20the%20performance%20of%20an%20SAE%20instance%0Aby%20measuring%20how%20well%20keypoints%20track%20ground%20truth%20objects%20in%20images.%20We%0Apresent%20a%20computationally%20lightweight%20metric%20and%20use%20it%20to%20evaluate%20common%0Abaseline%20SAE%20architectures%20on%20image%20data%20from%20a%20simulated%20robot%20task.%20We%20find%0Athat%20common%20SAEs%20differ%20substantially%20in%20their%20spatial%20extraction%20capability.%0AFurthermore%2C%20we%20validate%20that%20SAEs%20that%20perform%20well%20in%20our%20metric%20achieve%0Asuperior%20performance%20when%20used%20in%20downstream%20RL.%20Thus%2C%20our%20metric%20is%20an%0Aeffective%20and%20lightweight%20indicator%20of%20RL%20performance%20before%20executing%0Aexpensive%20RL%20training.%20Building%20on%20these%20insights%2C%20we%20identify%20three%20key%0Amodifications%20of%20SAE%20architectures%20to%20improve%20tracking%20performance.%20We%20make%20our%0Acode%20available%20at%20anonymous.4open.science/r/sae-rl.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00592v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTracking%2520Object%2520Positions%2520in%2520Reinforcement%2520Learning%253A%2520A%2520Metric%2520for%250A%2520%2520Keypoint%2520Detection%2520%2528extended%2520version%2529%26entry.906535625%3DEmma%2520Cramer%2520and%2520Jonas%2520Reiher%2520and%2520Sebastian%2520Trimpe%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520%2528RL%2529%2520for%2520robot%2520control%2520typically%2520requires%2520a%2520detailed%250Arepresentation%2520of%2520the%2520environment%2520state%252C%2520including%2520information%2520about%250Atask-relevant%2520objects%2520not%2520directly%2520measurable.%2520Keypoint%2520detectors%252C%2520such%2520as%250Aspatial%2520autoencoders%2520%2528SAEs%2529%252C%2520are%2520a%2520common%2520approach%2520to%2520extracting%2520a%250Alow-dimensional%2520representation%2520from%2520high-dimensional%2520image%2520data.%2520SAEs%2520aim%2520at%250Aspatial%2520features%2520such%2520as%2520object%2520positions%252C%2520which%2520are%2520often%2520useful%250Arepresentations%2520in%2520robotic%2520RL.%2520However%252C%2520whether%2520an%2520SAE%2520is%2520actually%2520able%2520to%250Atrack%2520objects%2520in%2520the%2520scene%2520and%2520thus%2520yields%2520a%2520spatial%2520state%2520representation%2520well%250Asuited%2520for%2520RL%2520tasks%2520has%2520rarely%2520been%2520examined%2520due%2520to%2520a%2520lack%2520of%2520established%250Ametrics.%2520In%2520this%2520paper%252C%2520we%2520propose%2520to%2520assess%2520the%2520performance%2520of%2520an%2520SAE%2520instance%250Aby%2520measuring%2520how%2520well%2520keypoints%2520track%2520ground%2520truth%2520objects%2520in%2520images.%2520We%250Apresent%2520a%2520computationally%2520lightweight%2520metric%2520and%2520use%2520it%2520to%2520evaluate%2520common%250Abaseline%2520SAE%2520architectures%2520on%2520image%2520data%2520from%2520a%2520simulated%2520robot%2520task.%2520We%2520find%250Athat%2520common%2520SAEs%2520differ%2520substantially%2520in%2520their%2520spatial%2520extraction%2520capability.%250AFurthermore%252C%2520we%2520validate%2520that%2520SAEs%2520that%2520perform%2520well%2520in%2520our%2520metric%2520achieve%250Asuperior%2520performance%2520when%2520used%2520in%2520downstream%2520RL.%2520Thus%252C%2520our%2520metric%2520is%2520an%250Aeffective%2520and%2520lightweight%2520indicator%2520of%2520RL%2520performance%2520before%2520executing%250Aexpensive%2520RL%2520training.%2520Building%2520on%2520these%2520insights%252C%2520we%2520identify%2520three%2520key%250Amodifications%2520of%2520SAE%2520architectures%2520to%2520improve%2520tracking%2520performance.%2520We%2520make%2520our%250Acode%2520available%2520at%2520anonymous.4open.science/r/sae-rl.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.00592v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tracking%20Object%20Positions%20in%20Reinforcement%20Learning%3A%20A%20Metric%20for%0A%20%20Keypoint%20Detection%20%28extended%20version%29&entry.906535625=Emma%20Cramer%20and%20Jonas%20Reiher%20and%20Sebastian%20Trimpe&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29%20for%20robot%20control%20typically%20requires%20a%20detailed%0Arepresentation%20of%20the%20environment%20state%2C%20including%20information%20about%0Atask-relevant%20objects%20not%20directly%20measurable.%20Keypoint%20detectors%2C%20such%20as%0Aspatial%20autoencoders%20%28SAEs%29%2C%20are%20a%20common%20approach%20to%20extracting%20a%0Alow-dimensional%20representation%20from%20high-dimensional%20image%20data.%20SAEs%20aim%20at%0Aspatial%20features%20such%20as%20object%20positions%2C%20which%20are%20often%20useful%0Arepresentations%20in%20robotic%20RL.%20However%2C%20whether%20an%20SAE%20is%20actually%20able%20to%0Atrack%20objects%20in%20the%20scene%20and%20thus%20yields%20a%20spatial%20state%20representation%20well%0Asuited%20for%20RL%20tasks%20has%20rarely%20been%20examined%20due%20to%20a%20lack%20of%20established%0Ametrics.%20In%20this%20paper%2C%20we%20propose%20to%20assess%20the%20performance%20of%20an%20SAE%20instance%0Aby%20measuring%20how%20well%20keypoints%20track%20ground%20truth%20objects%20in%20images.%20We%0Apresent%20a%20computationally%20lightweight%20metric%20and%20use%20it%20to%20evaluate%20common%0Abaseline%20SAE%20architectures%20on%20image%20data%20from%20a%20simulated%20robot%20task.%20We%20find%0Athat%20common%20SAEs%20differ%20substantially%20in%20their%20spatial%20extraction%20capability.%0AFurthermore%2C%20we%20validate%20that%20SAEs%20that%20perform%20well%20in%20our%20metric%20achieve%0Asuperior%20performance%20when%20used%20in%20downstream%20RL.%20Thus%2C%20our%20metric%20is%20an%0Aeffective%20and%20lightweight%20indicator%20of%20RL%20performance%20before%20executing%0Aexpensive%20RL%20training.%20Building%20on%20these%20insights%2C%20we%20identify%20three%20key%0Amodifications%20of%20SAE%20architectures%20to%20improve%20tracking%20performance.%20We%20make%20our%0Acode%20available%20at%20anonymous.4open.science/r/sae-rl.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00592v2&entry.124074799=Read"},
{"title": "LatentExplainer: Explaining Latent Representations in Deep Generative\n  Models with Multi-modal Foundation Models", "author": "Mengdan Zhu and Raasikh Kanjiani and Jiahui Lu and Andrew Choi and Qirui Ye and Liang Zhao", "abstract": "  Deep generative models like VAEs and diffusion models have advanced various\ngeneration tasks by leveraging latent variables to learn data distributions and\ngenerate high-quality samples. Despite the field of explainable AI making\nstrides in interpreting machine learning models, understanding latent variables\nin generative models remains challenging. This paper introduces\nLatentExplainer, a framework for automatically generating semantically\nmeaningful explanations of latent variables in deep generative models.\nLatentExplainer tackles three main challenges: inferring the meaning of latent\nvariables, aligning explanations with inductive biases, and handling varying\ndegrees of explainability. By perturbing latent variables and interpreting\nchanges in generated data, the framework provides a systematic approach to\nunderstanding and controlling the data generation process, enhancing the\ntransparency and interpretability of deep generative models. We evaluate our\nproposed method on several real-world and synthetic datasets, and the results\ndemonstrate superior performance in generating high-quality explanations of\nlatent variables.\n", "link": "http://arxiv.org/abs/2406.14862v3", "date": "2024-06-28", "relevancy": 2.1039, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5326}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5214}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LatentExplainer%3A%20Explaining%20Latent%20Representations%20in%20Deep%20Generative%0A%20%20Models%20with%20Multi-modal%20Foundation%20Models&body=Title%3A%20LatentExplainer%3A%20Explaining%20Latent%20Representations%20in%20Deep%20Generative%0A%20%20Models%20with%20Multi-modal%20Foundation%20Models%0AAuthor%3A%20Mengdan%20Zhu%20and%20Raasikh%20Kanjiani%20and%20Jiahui%20Lu%20and%20Andrew%20Choi%20and%20Qirui%20Ye%20and%20Liang%20Zhao%0AAbstract%3A%20%20%20Deep%20generative%20models%20like%20VAEs%20and%20diffusion%20models%20have%20advanced%20various%0Ageneration%20tasks%20by%20leveraging%20latent%20variables%20to%20learn%20data%20distributions%20and%0Agenerate%20high-quality%20samples.%20Despite%20the%20field%20of%20explainable%20AI%20making%0Astrides%20in%20interpreting%20machine%20learning%20models%2C%20understanding%20latent%20variables%0Ain%20generative%20models%20remains%20challenging.%20This%20paper%20introduces%0ALatentExplainer%2C%20a%20framework%20for%20automatically%20generating%20semantically%0Ameaningful%20explanations%20of%20latent%20variables%20in%20deep%20generative%20models.%0ALatentExplainer%20tackles%20three%20main%20challenges%3A%20inferring%20the%20meaning%20of%20latent%0Avariables%2C%20aligning%20explanations%20with%20inductive%20biases%2C%20and%20handling%20varying%0Adegrees%20of%20explainability.%20By%20perturbing%20latent%20variables%20and%20interpreting%0Achanges%20in%20generated%20data%2C%20the%20framework%20provides%20a%20systematic%20approach%20to%0Aunderstanding%20and%20controlling%20the%20data%20generation%20process%2C%20enhancing%20the%0Atransparency%20and%20interpretability%20of%20deep%20generative%20models.%20We%20evaluate%20our%0Aproposed%20method%20on%20several%20real-world%20and%20synthetic%20datasets%2C%20and%20the%20results%0Ademonstrate%20superior%20performance%20in%20generating%20high-quality%20explanations%20of%0Alatent%20variables.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14862v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatentExplainer%253A%2520Explaining%2520Latent%2520Representations%2520in%2520Deep%2520Generative%250A%2520%2520Models%2520with%2520Multi-modal%2520Foundation%2520Models%26entry.906535625%3DMengdan%2520Zhu%2520and%2520Raasikh%2520Kanjiani%2520and%2520Jiahui%2520Lu%2520and%2520Andrew%2520Choi%2520and%2520Qirui%2520Ye%2520and%2520Liang%2520Zhao%26entry.1292438233%3D%2520%2520Deep%2520generative%2520models%2520like%2520VAEs%2520and%2520diffusion%2520models%2520have%2520advanced%2520various%250Ageneration%2520tasks%2520by%2520leveraging%2520latent%2520variables%2520to%2520learn%2520data%2520distributions%2520and%250Agenerate%2520high-quality%2520samples.%2520Despite%2520the%2520field%2520of%2520explainable%2520AI%2520making%250Astrides%2520in%2520interpreting%2520machine%2520learning%2520models%252C%2520understanding%2520latent%2520variables%250Ain%2520generative%2520models%2520remains%2520challenging.%2520This%2520paper%2520introduces%250ALatentExplainer%252C%2520a%2520framework%2520for%2520automatically%2520generating%2520semantically%250Ameaningful%2520explanations%2520of%2520latent%2520variables%2520in%2520deep%2520generative%2520models.%250ALatentExplainer%2520tackles%2520three%2520main%2520challenges%253A%2520inferring%2520the%2520meaning%2520of%2520latent%250Avariables%252C%2520aligning%2520explanations%2520with%2520inductive%2520biases%252C%2520and%2520handling%2520varying%250Adegrees%2520of%2520explainability.%2520By%2520perturbing%2520latent%2520variables%2520and%2520interpreting%250Achanges%2520in%2520generated%2520data%252C%2520the%2520framework%2520provides%2520a%2520systematic%2520approach%2520to%250Aunderstanding%2520and%2520controlling%2520the%2520data%2520generation%2520process%252C%2520enhancing%2520the%250Atransparency%2520and%2520interpretability%2520of%2520deep%2520generative%2520models.%2520We%2520evaluate%2520our%250Aproposed%2520method%2520on%2520several%2520real-world%2520and%2520synthetic%2520datasets%252C%2520and%2520the%2520results%250Ademonstrate%2520superior%2520performance%2520in%2520generating%2520high-quality%2520explanations%2520of%250Alatent%2520variables.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14862v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LatentExplainer%3A%20Explaining%20Latent%20Representations%20in%20Deep%20Generative%0A%20%20Models%20with%20Multi-modal%20Foundation%20Models&entry.906535625=Mengdan%20Zhu%20and%20Raasikh%20Kanjiani%20and%20Jiahui%20Lu%20and%20Andrew%20Choi%20and%20Qirui%20Ye%20and%20Liang%20Zhao&entry.1292438233=%20%20Deep%20generative%20models%20like%20VAEs%20and%20diffusion%20models%20have%20advanced%20various%0Ageneration%20tasks%20by%20leveraging%20latent%20variables%20to%20learn%20data%20distributions%20and%0Agenerate%20high-quality%20samples.%20Despite%20the%20field%20of%20explainable%20AI%20making%0Astrides%20in%20interpreting%20machine%20learning%20models%2C%20understanding%20latent%20variables%0Ain%20generative%20models%20remains%20challenging.%20This%20paper%20introduces%0ALatentExplainer%2C%20a%20framework%20for%20automatically%20generating%20semantically%0Ameaningful%20explanations%20of%20latent%20variables%20in%20deep%20generative%20models.%0ALatentExplainer%20tackles%20three%20main%20challenges%3A%20inferring%20the%20meaning%20of%20latent%0Avariables%2C%20aligning%20explanations%20with%20inductive%20biases%2C%20and%20handling%20varying%0Adegrees%20of%20explainability.%20By%20perturbing%20latent%20variables%20and%20interpreting%0Achanges%20in%20generated%20data%2C%20the%20framework%20provides%20a%20systematic%20approach%20to%0Aunderstanding%20and%20controlling%20the%20data%20generation%20process%2C%20enhancing%20the%0Atransparency%20and%20interpretability%20of%20deep%20generative%20models.%20We%20evaluate%20our%0Aproposed%20method%20on%20several%20real-world%20and%20synthetic%20datasets%2C%20and%20the%20results%0Ademonstrate%20superior%20performance%20in%20generating%20high-quality%20explanations%20of%0Alatent%20variables.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14862v3&entry.124074799=Read"},
{"title": "EnSolver: Uncertainty-Aware Ensemble CAPTCHA Solvers with Theoretical\n  Guarantees", "author": "Duc C. Hoang and Behzad Ousat and Amin Kharraz and Cuong V. Nguyen", "abstract": "  The popularity of text-based CAPTCHA as a security mechanism to protect\nwebsites from automated bots has prompted researches in CAPTCHA solvers, with\nthe aim of understanding its failure cases and subsequently making CAPTCHAs\nmore secure. Recently proposed solvers, built on advances in deep learning, are\nable to crack even the very challenging CAPTCHAs with high accuracy. However,\nthese solvers often perform poorly on out-of-distribution samples that contain\nvisual features different from those in the training set. Furthermore, they\nlack the ability to detect and avoid such samples, making them susceptible to\nbeing locked out by defense systems after a certain number of failed attempts.\nIn this paper, we propose EnSolver, a family of CAPTCHA solvers that use deep\nensemble uncertainty to detect and skip out-of-distribution CAPTCHAs, making it\nharder to be detected. We prove novel theoretical bounds on the effectiveness\nof our solvers and demonstrate their use with state-of-the-art CAPTCHA solvers.\nOur experiments show that the proposed approaches perform well when cracking\nCAPTCHA datasets that contain both in-distribution and out-of-distribution\nsamples.\n", "link": "http://arxiv.org/abs/2307.15180v2", "date": "2024-06-28", "relevancy": 2.0641, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5497}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5251}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EnSolver%3A%20Uncertainty-Aware%20Ensemble%20CAPTCHA%20Solvers%20with%20Theoretical%0A%20%20Guarantees&body=Title%3A%20EnSolver%3A%20Uncertainty-Aware%20Ensemble%20CAPTCHA%20Solvers%20with%20Theoretical%0A%20%20Guarantees%0AAuthor%3A%20Duc%20C.%20Hoang%20and%20Behzad%20Ousat%20and%20Amin%20Kharraz%20and%20Cuong%20V.%20Nguyen%0AAbstract%3A%20%20%20The%20popularity%20of%20text-based%20CAPTCHA%20as%20a%20security%20mechanism%20to%20protect%0Awebsites%20from%20automated%20bots%20has%20prompted%20researches%20in%20CAPTCHA%20solvers%2C%20with%0Athe%20aim%20of%20understanding%20its%20failure%20cases%20and%20subsequently%20making%20CAPTCHAs%0Amore%20secure.%20Recently%20proposed%20solvers%2C%20built%20on%20advances%20in%20deep%20learning%2C%20are%0Aable%20to%20crack%20even%20the%20very%20challenging%20CAPTCHAs%20with%20high%20accuracy.%20However%2C%0Athese%20solvers%20often%20perform%20poorly%20on%20out-of-distribution%20samples%20that%20contain%0Avisual%20features%20different%20from%20those%20in%20the%20training%20set.%20Furthermore%2C%20they%0Alack%20the%20ability%20to%20detect%20and%20avoid%20such%20samples%2C%20making%20them%20susceptible%20to%0Abeing%20locked%20out%20by%20defense%20systems%20after%20a%20certain%20number%20of%20failed%20attempts.%0AIn%20this%20paper%2C%20we%20propose%20EnSolver%2C%20a%20family%20of%20CAPTCHA%20solvers%20that%20use%20deep%0Aensemble%20uncertainty%20to%20detect%20and%20skip%20out-of-distribution%20CAPTCHAs%2C%20making%20it%0Aharder%20to%20be%20detected.%20We%20prove%20novel%20theoretical%20bounds%20on%20the%20effectiveness%0Aof%20our%20solvers%20and%20demonstrate%20their%20use%20with%20state-of-the-art%20CAPTCHA%20solvers.%0AOur%20experiments%20show%20that%20the%20proposed%20approaches%20perform%20well%20when%20cracking%0ACAPTCHA%20datasets%20that%20contain%20both%20in-distribution%20and%20out-of-distribution%0Asamples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.15180v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnSolver%253A%2520Uncertainty-Aware%2520Ensemble%2520CAPTCHA%2520Solvers%2520with%2520Theoretical%250A%2520%2520Guarantees%26entry.906535625%3DDuc%2520C.%2520Hoang%2520and%2520Behzad%2520Ousat%2520and%2520Amin%2520Kharraz%2520and%2520Cuong%2520V.%2520Nguyen%26entry.1292438233%3D%2520%2520The%2520popularity%2520of%2520text-based%2520CAPTCHA%2520as%2520a%2520security%2520mechanism%2520to%2520protect%250Awebsites%2520from%2520automated%2520bots%2520has%2520prompted%2520researches%2520in%2520CAPTCHA%2520solvers%252C%2520with%250Athe%2520aim%2520of%2520understanding%2520its%2520failure%2520cases%2520and%2520subsequently%2520making%2520CAPTCHAs%250Amore%2520secure.%2520Recently%2520proposed%2520solvers%252C%2520built%2520on%2520advances%2520in%2520deep%2520learning%252C%2520are%250Aable%2520to%2520crack%2520even%2520the%2520very%2520challenging%2520CAPTCHAs%2520with%2520high%2520accuracy.%2520However%252C%250Athese%2520solvers%2520often%2520perform%2520poorly%2520on%2520out-of-distribution%2520samples%2520that%2520contain%250Avisual%2520features%2520different%2520from%2520those%2520in%2520the%2520training%2520set.%2520Furthermore%252C%2520they%250Alack%2520the%2520ability%2520to%2520detect%2520and%2520avoid%2520such%2520samples%252C%2520making%2520them%2520susceptible%2520to%250Abeing%2520locked%2520out%2520by%2520defense%2520systems%2520after%2520a%2520certain%2520number%2520of%2520failed%2520attempts.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520EnSolver%252C%2520a%2520family%2520of%2520CAPTCHA%2520solvers%2520that%2520use%2520deep%250Aensemble%2520uncertainty%2520to%2520detect%2520and%2520skip%2520out-of-distribution%2520CAPTCHAs%252C%2520making%2520it%250Aharder%2520to%2520be%2520detected.%2520We%2520prove%2520novel%2520theoretical%2520bounds%2520on%2520the%2520effectiveness%250Aof%2520our%2520solvers%2520and%2520demonstrate%2520their%2520use%2520with%2520state-of-the-art%2520CAPTCHA%2520solvers.%250AOur%2520experiments%2520show%2520that%2520the%2520proposed%2520approaches%2520perform%2520well%2520when%2520cracking%250ACAPTCHA%2520datasets%2520that%2520contain%2520both%2520in-distribution%2520and%2520out-of-distribution%250Asamples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.15180v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EnSolver%3A%20Uncertainty-Aware%20Ensemble%20CAPTCHA%20Solvers%20with%20Theoretical%0A%20%20Guarantees&entry.906535625=Duc%20C.%20Hoang%20and%20Behzad%20Ousat%20and%20Amin%20Kharraz%20and%20Cuong%20V.%20Nguyen&entry.1292438233=%20%20The%20popularity%20of%20text-based%20CAPTCHA%20as%20a%20security%20mechanism%20to%20protect%0Awebsites%20from%20automated%20bots%20has%20prompted%20researches%20in%20CAPTCHA%20solvers%2C%20with%0Athe%20aim%20of%20understanding%20its%20failure%20cases%20and%20subsequently%20making%20CAPTCHAs%0Amore%20secure.%20Recently%20proposed%20solvers%2C%20built%20on%20advances%20in%20deep%20learning%2C%20are%0Aable%20to%20crack%20even%20the%20very%20challenging%20CAPTCHAs%20with%20high%20accuracy.%20However%2C%0Athese%20solvers%20often%20perform%20poorly%20on%20out-of-distribution%20samples%20that%20contain%0Avisual%20features%20different%20from%20those%20in%20the%20training%20set.%20Furthermore%2C%20they%0Alack%20the%20ability%20to%20detect%20and%20avoid%20such%20samples%2C%20making%20them%20susceptible%20to%0Abeing%20locked%20out%20by%20defense%20systems%20after%20a%20certain%20number%20of%20failed%20attempts.%0AIn%20this%20paper%2C%20we%20propose%20EnSolver%2C%20a%20family%20of%20CAPTCHA%20solvers%20that%20use%20deep%0Aensemble%20uncertainty%20to%20detect%20and%20skip%20out-of-distribution%20CAPTCHAs%2C%20making%20it%0Aharder%20to%20be%20detected.%20We%20prove%20novel%20theoretical%20bounds%20on%20the%20effectiveness%0Aof%20our%20solvers%20and%20demonstrate%20their%20use%20with%20state-of-the-art%20CAPTCHA%20solvers.%0AOur%20experiments%20show%20that%20the%20proposed%20approaches%20perform%20well%20when%20cracking%0ACAPTCHA%20datasets%20that%20contain%20both%20in-distribution%20and%20out-of-distribution%0Asamples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.15180v2&entry.124074799=Read"},
{"title": "PruningBench: A Comprehensive Benchmark of Structural Pruning", "author": "Haoling Li and Changhao Li and Mengqi Xue and Gongfan Fang and Sheng Zhou and Zunlei Feng and Huiqiong Wang and Yong Wang and Lechao Cheng and Mingli Song and Jie Song", "abstract": "  Structural pruning has emerged as a promising approach for producing more\nefficient models. Nevertheless, the community suffers from a lack of\nstandardized benchmarks and metrics, leaving the progress in this area not\nfully comprehended. To fill this gap, we present the first comprehensive\nbenchmark, termed \\textit{PruningBench}, for structural pruning. PruningBench\nshowcases the following three characteristics: 1) PruningBench employs a\nunified and consistent framework for evaluating the effectiveness of diverse\nstructural pruning techniques; 2) PruningBench systematically evaluates 16\nexisting pruning methods, encompassing a wide array of models (e.g., CNNs and\nViTs) and tasks (e.g., classification and detection); 3) PruningBench provides\neasily implementable interfaces to facilitate the implementation of future\npruning methods, and enables the subsequent researchers to incorporate their\nwork into our leaderboards. We provide an online pruning platform\nhttp://pruning.vipazoo.cn for customizing pruning tasks and reproducing all\nresults in this paper. Codes will be made publicly on\nhttps://github.com/HollyLee2000/PruningBench.\n", "link": "http://arxiv.org/abs/2406.12315v2", "date": "2024-06-28", "relevancy": 2.0603, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4386}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3989}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PruningBench%3A%20A%20Comprehensive%20Benchmark%20of%20Structural%20Pruning&body=Title%3A%20PruningBench%3A%20A%20Comprehensive%20Benchmark%20of%20Structural%20Pruning%0AAuthor%3A%20Haoling%20Li%20and%20Changhao%20Li%20and%20Mengqi%20Xue%20and%20Gongfan%20Fang%20and%20Sheng%20Zhou%20and%20Zunlei%20Feng%20and%20Huiqiong%20Wang%20and%20Yong%20Wang%20and%20Lechao%20Cheng%20and%20Mingli%20Song%20and%20Jie%20Song%0AAbstract%3A%20%20%20Structural%20pruning%20has%20emerged%20as%20a%20promising%20approach%20for%20producing%20more%0Aefficient%20models.%20Nevertheless%2C%20the%20community%20suffers%20from%20a%20lack%20of%0Astandardized%20benchmarks%20and%20metrics%2C%20leaving%20the%20progress%20in%20this%20area%20not%0Afully%20comprehended.%20To%20fill%20this%20gap%2C%20we%20present%20the%20first%20comprehensive%0Abenchmark%2C%20termed%20%5Ctextit%7BPruningBench%7D%2C%20for%20structural%20pruning.%20PruningBench%0Ashowcases%20the%20following%20three%20characteristics%3A%201%29%20PruningBench%20employs%20a%0Aunified%20and%20consistent%20framework%20for%20evaluating%20the%20effectiveness%20of%20diverse%0Astructural%20pruning%20techniques%3B%202%29%20PruningBench%20systematically%20evaluates%2016%0Aexisting%20pruning%20methods%2C%20encompassing%20a%20wide%20array%20of%20models%20%28e.g.%2C%20CNNs%20and%0AViTs%29%20and%20tasks%20%28e.g.%2C%20classification%20and%20detection%29%3B%203%29%20PruningBench%20provides%0Aeasily%20implementable%20interfaces%20to%20facilitate%20the%20implementation%20of%20future%0Apruning%20methods%2C%20and%20enables%20the%20subsequent%20researchers%20to%20incorporate%20their%0Awork%20into%20our%20leaderboards.%20We%20provide%20an%20online%20pruning%20platform%0Ahttp%3A//pruning.vipazoo.cn%20for%20customizing%20pruning%20tasks%20and%20reproducing%20all%0Aresults%20in%20this%20paper.%20Codes%20will%20be%20made%20publicly%20on%0Ahttps%3A//github.com/HollyLee2000/PruningBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12315v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPruningBench%253A%2520A%2520Comprehensive%2520Benchmark%2520of%2520Structural%2520Pruning%26entry.906535625%3DHaoling%2520Li%2520and%2520Changhao%2520Li%2520and%2520Mengqi%2520Xue%2520and%2520Gongfan%2520Fang%2520and%2520Sheng%2520Zhou%2520and%2520Zunlei%2520Feng%2520and%2520Huiqiong%2520Wang%2520and%2520Yong%2520Wang%2520and%2520Lechao%2520Cheng%2520and%2520Mingli%2520Song%2520and%2520Jie%2520Song%26entry.1292438233%3D%2520%2520Structural%2520pruning%2520has%2520emerged%2520as%2520a%2520promising%2520approach%2520for%2520producing%2520more%250Aefficient%2520models.%2520Nevertheless%252C%2520the%2520community%2520suffers%2520from%2520a%2520lack%2520of%250Astandardized%2520benchmarks%2520and%2520metrics%252C%2520leaving%2520the%2520progress%2520in%2520this%2520area%2520not%250Afully%2520comprehended.%2520To%2520fill%2520this%2520gap%252C%2520we%2520present%2520the%2520first%2520comprehensive%250Abenchmark%252C%2520termed%2520%255Ctextit%257BPruningBench%257D%252C%2520for%2520structural%2520pruning.%2520PruningBench%250Ashowcases%2520the%2520following%2520three%2520characteristics%253A%25201%2529%2520PruningBench%2520employs%2520a%250Aunified%2520and%2520consistent%2520framework%2520for%2520evaluating%2520the%2520effectiveness%2520of%2520diverse%250Astructural%2520pruning%2520techniques%253B%25202%2529%2520PruningBench%2520systematically%2520evaluates%252016%250Aexisting%2520pruning%2520methods%252C%2520encompassing%2520a%2520wide%2520array%2520of%2520models%2520%2528e.g.%252C%2520CNNs%2520and%250AViTs%2529%2520and%2520tasks%2520%2528e.g.%252C%2520classification%2520and%2520detection%2529%253B%25203%2529%2520PruningBench%2520provides%250Aeasily%2520implementable%2520interfaces%2520to%2520facilitate%2520the%2520implementation%2520of%2520future%250Apruning%2520methods%252C%2520and%2520enables%2520the%2520subsequent%2520researchers%2520to%2520incorporate%2520their%250Awork%2520into%2520our%2520leaderboards.%2520We%2520provide%2520an%2520online%2520pruning%2520platform%250Ahttp%253A//pruning.vipazoo.cn%2520for%2520customizing%2520pruning%2520tasks%2520and%2520reproducing%2520all%250Aresults%2520in%2520this%2520paper.%2520Codes%2520will%2520be%2520made%2520publicly%2520on%250Ahttps%253A//github.com/HollyLee2000/PruningBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12315v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PruningBench%3A%20A%20Comprehensive%20Benchmark%20of%20Structural%20Pruning&entry.906535625=Haoling%20Li%20and%20Changhao%20Li%20and%20Mengqi%20Xue%20and%20Gongfan%20Fang%20and%20Sheng%20Zhou%20and%20Zunlei%20Feng%20and%20Huiqiong%20Wang%20and%20Yong%20Wang%20and%20Lechao%20Cheng%20and%20Mingli%20Song%20and%20Jie%20Song&entry.1292438233=%20%20Structural%20pruning%20has%20emerged%20as%20a%20promising%20approach%20for%20producing%20more%0Aefficient%20models.%20Nevertheless%2C%20the%20community%20suffers%20from%20a%20lack%20of%0Astandardized%20benchmarks%20and%20metrics%2C%20leaving%20the%20progress%20in%20this%20area%20not%0Afully%20comprehended.%20To%20fill%20this%20gap%2C%20we%20present%20the%20first%20comprehensive%0Abenchmark%2C%20termed%20%5Ctextit%7BPruningBench%7D%2C%20for%20structural%20pruning.%20PruningBench%0Ashowcases%20the%20following%20three%20characteristics%3A%201%29%20PruningBench%20employs%20a%0Aunified%20and%20consistent%20framework%20for%20evaluating%20the%20effectiveness%20of%20diverse%0Astructural%20pruning%20techniques%3B%202%29%20PruningBench%20systematically%20evaluates%2016%0Aexisting%20pruning%20methods%2C%20encompassing%20a%20wide%20array%20of%20models%20%28e.g.%2C%20CNNs%20and%0AViTs%29%20and%20tasks%20%28e.g.%2C%20classification%20and%20detection%29%3B%203%29%20PruningBench%20provides%0Aeasily%20implementable%20interfaces%20to%20facilitate%20the%20implementation%20of%20future%0Apruning%20methods%2C%20and%20enables%20the%20subsequent%20researchers%20to%20incorporate%20their%0Awork%20into%20our%20leaderboards.%20We%20provide%20an%20online%20pruning%20platform%0Ahttp%3A//pruning.vipazoo.cn%20for%20customizing%20pruning%20tasks%20and%20reproducing%20all%0Aresults%20in%20this%20paper.%20Codes%20will%20be%20made%20publicly%20on%0Ahttps%3A//github.com/HollyLee2000/PruningBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12315v2&entry.124074799=Read"},
{"title": "A Multimodal Foundation Agent for Financial Trading: Tool-Augmented,\n  Diversified, and Generalist", "author": "Wentao Zhang and Lingxuan Zhao and Haochong Xia and Shuo Sun and Jiaze Sun and Molei Qin and Xinyi Li and Yuqing Zhao and Yilei Zhao and Xinyu Cai and Longtao Zheng and Xinrun Wang and Bo An", "abstract": "  Financial trading is a crucial component of the markets, informed by a\nmultimodal information landscape encompassing news, prices, and Kline charts,\nand encompasses diverse tasks such as quantitative trading and high-frequency\ntrading with various assets. While advanced AI techniques like deep learning\nand reinforcement learning are extensively utilized in finance, their\napplication in financial trading tasks often faces challenges due to inadequate\nhandling of multimodal data and limited generalizability across various tasks.\nTo address these challenges, we present FinAgent, a multimodal foundational\nagent with tool augmentation for financial trading. FinAgent's market\nintelligence module processes a diverse range of data-numerical, textual, and\nvisual-to accurately analyze the financial market. Its unique dual-level\nreflection module not only enables rapid adaptation to market dynamics but also\nincorporates a diversified memory retrieval system, enhancing the agent's\nability to learn from historical data and improve decision-making processes.\nThe agent's emphasis on reasoning for actions fosters trust in its financial\ndecisions. Moreover, FinAgent integrates established trading strategies and\nexpert insights, ensuring that its trading approaches are both data-driven and\nrooted in sound financial principles. With comprehensive experiments on 6\nfinancial datasets, including stocks and Crypto, FinAgent significantly\noutperforms 9 state-of-the-art baselines in terms of 6 financial metrics with\nover 36% average improvement on profit. Specifically, a 92.27% return (a 84.39%\nrelative improvement) is achieved on one dataset. Notably, FinAgent is the\nfirst advanced multimodal foundation agent designed for financial trading\ntasks.\n", "link": "http://arxiv.org/abs/2402.18485v3", "date": "2024-06-28", "relevancy": 2.0592, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.525}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5235}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5011}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multimodal%20Foundation%20Agent%20for%20Financial%20Trading%3A%20Tool-Augmented%2C%0A%20%20Diversified%2C%20and%20Generalist&body=Title%3A%20A%20Multimodal%20Foundation%20Agent%20for%20Financial%20Trading%3A%20Tool-Augmented%2C%0A%20%20Diversified%2C%20and%20Generalist%0AAuthor%3A%20Wentao%20Zhang%20and%20Lingxuan%20Zhao%20and%20Haochong%20Xia%20and%20Shuo%20Sun%20and%20Jiaze%20Sun%20and%20Molei%20Qin%20and%20Xinyi%20Li%20and%20Yuqing%20Zhao%20and%20Yilei%20Zhao%20and%20Xinyu%20Cai%20and%20Longtao%20Zheng%20and%20Xinrun%20Wang%20and%20Bo%20An%0AAbstract%3A%20%20%20Financial%20trading%20is%20a%20crucial%20component%20of%20the%20markets%2C%20informed%20by%20a%0Amultimodal%20information%20landscape%20encompassing%20news%2C%20prices%2C%20and%20Kline%20charts%2C%0Aand%20encompasses%20diverse%20tasks%20such%20as%20quantitative%20trading%20and%20high-frequency%0Atrading%20with%20various%20assets.%20While%20advanced%20AI%20techniques%20like%20deep%20learning%0Aand%20reinforcement%20learning%20are%20extensively%20utilized%20in%20finance%2C%20their%0Aapplication%20in%20financial%20trading%20tasks%20often%20faces%20challenges%20due%20to%20inadequate%0Ahandling%20of%20multimodal%20data%20and%20limited%20generalizability%20across%20various%20tasks.%0ATo%20address%20these%20challenges%2C%20we%20present%20FinAgent%2C%20a%20multimodal%20foundational%0Aagent%20with%20tool%20augmentation%20for%20financial%20trading.%20FinAgent%27s%20market%0Aintelligence%20module%20processes%20a%20diverse%20range%20of%20data-numerical%2C%20textual%2C%20and%0Avisual-to%20accurately%20analyze%20the%20financial%20market.%20Its%20unique%20dual-level%0Areflection%20module%20not%20only%20enables%20rapid%20adaptation%20to%20market%20dynamics%20but%20also%0Aincorporates%20a%20diversified%20memory%20retrieval%20system%2C%20enhancing%20the%20agent%27s%0Aability%20to%20learn%20from%20historical%20data%20and%20improve%20decision-making%20processes.%0AThe%20agent%27s%20emphasis%20on%20reasoning%20for%20actions%20fosters%20trust%20in%20its%20financial%0Adecisions.%20Moreover%2C%20FinAgent%20integrates%20established%20trading%20strategies%20and%0Aexpert%20insights%2C%20ensuring%20that%20its%20trading%20approaches%20are%20both%20data-driven%20and%0Arooted%20in%20sound%20financial%20principles.%20With%20comprehensive%20experiments%20on%206%0Afinancial%20datasets%2C%20including%20stocks%20and%20Crypto%2C%20FinAgent%20significantly%0Aoutperforms%209%20state-of-the-art%20baselines%20in%20terms%20of%206%20financial%20metrics%20with%0Aover%2036%25%20average%20improvement%20on%20profit.%20Specifically%2C%20a%2092.27%25%20return%20%28a%2084.39%25%0Arelative%20improvement%29%20is%20achieved%20on%20one%20dataset.%20Notably%2C%20FinAgent%20is%20the%0Afirst%20advanced%20multimodal%20foundation%20agent%20designed%20for%20financial%20trading%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.18485v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multimodal%2520Foundation%2520Agent%2520for%2520Financial%2520Trading%253A%2520Tool-Augmented%252C%250A%2520%2520Diversified%252C%2520and%2520Generalist%26entry.906535625%3DWentao%2520Zhang%2520and%2520Lingxuan%2520Zhao%2520and%2520Haochong%2520Xia%2520and%2520Shuo%2520Sun%2520and%2520Jiaze%2520Sun%2520and%2520Molei%2520Qin%2520and%2520Xinyi%2520Li%2520and%2520Yuqing%2520Zhao%2520and%2520Yilei%2520Zhao%2520and%2520Xinyu%2520Cai%2520and%2520Longtao%2520Zheng%2520and%2520Xinrun%2520Wang%2520and%2520Bo%2520An%26entry.1292438233%3D%2520%2520Financial%2520trading%2520is%2520a%2520crucial%2520component%2520of%2520the%2520markets%252C%2520informed%2520by%2520a%250Amultimodal%2520information%2520landscape%2520encompassing%2520news%252C%2520prices%252C%2520and%2520Kline%2520charts%252C%250Aand%2520encompasses%2520diverse%2520tasks%2520such%2520as%2520quantitative%2520trading%2520and%2520high-frequency%250Atrading%2520with%2520various%2520assets.%2520While%2520advanced%2520AI%2520techniques%2520like%2520deep%2520learning%250Aand%2520reinforcement%2520learning%2520are%2520extensively%2520utilized%2520in%2520finance%252C%2520their%250Aapplication%2520in%2520financial%2520trading%2520tasks%2520often%2520faces%2520challenges%2520due%2520to%2520inadequate%250Ahandling%2520of%2520multimodal%2520data%2520and%2520limited%2520generalizability%2520across%2520various%2520tasks.%250ATo%2520address%2520these%2520challenges%252C%2520we%2520present%2520FinAgent%252C%2520a%2520multimodal%2520foundational%250Aagent%2520with%2520tool%2520augmentation%2520for%2520financial%2520trading.%2520FinAgent%2527s%2520market%250Aintelligence%2520module%2520processes%2520a%2520diverse%2520range%2520of%2520data-numerical%252C%2520textual%252C%2520and%250Avisual-to%2520accurately%2520analyze%2520the%2520financial%2520market.%2520Its%2520unique%2520dual-level%250Areflection%2520module%2520not%2520only%2520enables%2520rapid%2520adaptation%2520to%2520market%2520dynamics%2520but%2520also%250Aincorporates%2520a%2520diversified%2520memory%2520retrieval%2520system%252C%2520enhancing%2520the%2520agent%2527s%250Aability%2520to%2520learn%2520from%2520historical%2520data%2520and%2520improve%2520decision-making%2520processes.%250AThe%2520agent%2527s%2520emphasis%2520on%2520reasoning%2520for%2520actions%2520fosters%2520trust%2520in%2520its%2520financial%250Adecisions.%2520Moreover%252C%2520FinAgent%2520integrates%2520established%2520trading%2520strategies%2520and%250Aexpert%2520insights%252C%2520ensuring%2520that%2520its%2520trading%2520approaches%2520are%2520both%2520data-driven%2520and%250Arooted%2520in%2520sound%2520financial%2520principles.%2520With%2520comprehensive%2520experiments%2520on%25206%250Afinancial%2520datasets%252C%2520including%2520stocks%2520and%2520Crypto%252C%2520FinAgent%2520significantly%250Aoutperforms%25209%2520state-of-the-art%2520baselines%2520in%2520terms%2520of%25206%2520financial%2520metrics%2520with%250Aover%252036%2525%2520average%2520improvement%2520on%2520profit.%2520Specifically%252C%2520a%252092.27%2525%2520return%2520%2528a%252084.39%2525%250Arelative%2520improvement%2529%2520is%2520achieved%2520on%2520one%2520dataset.%2520Notably%252C%2520FinAgent%2520is%2520the%250Afirst%2520advanced%2520multimodal%2520foundation%2520agent%2520designed%2520for%2520financial%2520trading%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.18485v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multimodal%20Foundation%20Agent%20for%20Financial%20Trading%3A%20Tool-Augmented%2C%0A%20%20Diversified%2C%20and%20Generalist&entry.906535625=Wentao%20Zhang%20and%20Lingxuan%20Zhao%20and%20Haochong%20Xia%20and%20Shuo%20Sun%20and%20Jiaze%20Sun%20and%20Molei%20Qin%20and%20Xinyi%20Li%20and%20Yuqing%20Zhao%20and%20Yilei%20Zhao%20and%20Xinyu%20Cai%20and%20Longtao%20Zheng%20and%20Xinrun%20Wang%20and%20Bo%20An&entry.1292438233=%20%20Financial%20trading%20is%20a%20crucial%20component%20of%20the%20markets%2C%20informed%20by%20a%0Amultimodal%20information%20landscape%20encompassing%20news%2C%20prices%2C%20and%20Kline%20charts%2C%0Aand%20encompasses%20diverse%20tasks%20such%20as%20quantitative%20trading%20and%20high-frequency%0Atrading%20with%20various%20assets.%20While%20advanced%20AI%20techniques%20like%20deep%20learning%0Aand%20reinforcement%20learning%20are%20extensively%20utilized%20in%20finance%2C%20their%0Aapplication%20in%20financial%20trading%20tasks%20often%20faces%20challenges%20due%20to%20inadequate%0Ahandling%20of%20multimodal%20data%20and%20limited%20generalizability%20across%20various%20tasks.%0ATo%20address%20these%20challenges%2C%20we%20present%20FinAgent%2C%20a%20multimodal%20foundational%0Aagent%20with%20tool%20augmentation%20for%20financial%20trading.%20FinAgent%27s%20market%0Aintelligence%20module%20processes%20a%20diverse%20range%20of%20data-numerical%2C%20textual%2C%20and%0Avisual-to%20accurately%20analyze%20the%20financial%20market.%20Its%20unique%20dual-level%0Areflection%20module%20not%20only%20enables%20rapid%20adaptation%20to%20market%20dynamics%20but%20also%0Aincorporates%20a%20diversified%20memory%20retrieval%20system%2C%20enhancing%20the%20agent%27s%0Aability%20to%20learn%20from%20historical%20data%20and%20improve%20decision-making%20processes.%0AThe%20agent%27s%20emphasis%20on%20reasoning%20for%20actions%20fosters%20trust%20in%20its%20financial%0Adecisions.%20Moreover%2C%20FinAgent%20integrates%20established%20trading%20strategies%20and%0Aexpert%20insights%2C%20ensuring%20that%20its%20trading%20approaches%20are%20both%20data-driven%20and%0Arooted%20in%20sound%20financial%20principles.%20With%20comprehensive%20experiments%20on%206%0Afinancial%20datasets%2C%20including%20stocks%20and%20Crypto%2C%20FinAgent%20significantly%0Aoutperforms%209%20state-of-the-art%20baselines%20in%20terms%20of%206%20financial%20metrics%20with%0Aover%2036%25%20average%20improvement%20on%20profit.%20Specifically%2C%20a%2092.27%25%20return%20%28a%2084.39%25%0Arelative%20improvement%29%20is%20achieved%20on%20one%20dataset.%20Notably%2C%20FinAgent%20is%20the%0Afirst%20advanced%20multimodal%20foundation%20agent%20designed%20for%20financial%20trading%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18485v3&entry.124074799=Read"},
{"title": "EVF-SAM: Early Vision-Language Fusion for Text-Prompted Segment Anything\n  Model", "author": "Yuxuan Zhang and Tianheng Cheng and Rui Hu and ei Liu and Heng Liu and Longjin Ran and Xiaoxin Chen and Wenyu Liu and Xinggang Wang", "abstract": "  Segment Anything Model (SAM) has attracted widespread attention for its\nsuperior interactive segmentation capabilities with visual prompts while\nlacking further exploration of text prompts. In this paper, we empirically\ninvestigate what text prompt encoders (e.g., CLIP or LLM) are good for adapting\nSAM for referring expression segmentation and introduce the Early\nVision-language Fusion-based SAM (EVF-SAM). EVF-SAM is a simple yet effective\nreferring segmentation method which exploits multimodal prompts (i.e., image\nand text) and comprises a pre-trained vision-language model to generate\nreferring prompts and a SAM model for segmentation. Surprisingly, we observe\nthat: (1) multimodal prompts and (2) vision-language models with early fusion\n(e.g., BEIT-3) are beneficial for prompting SAM for accurate referring\nsegmentation. Our experiments show that the proposed EVF-SAM based on BEIT-3\ncan obtain state-of-the-art performance on RefCOCO/+/g for referring expression\nsegmentation and demonstrate the superiority of prompting SAM with early\nvision-language fusion. In addition, the proposed EVF-SAM with 1.32B parameters\nachieves remarkably higher performance while reducing nearly 82% of parameters\ncompared to previous SAM methods based on large multimodal models.\n", "link": "http://arxiv.org/abs/2406.20076v1", "date": "2024-06-28", "relevancy": 2.0489, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5459}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4909}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4814}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EVF-SAM%3A%20Early%20Vision-Language%20Fusion%20for%20Text-Prompted%20Segment%20Anything%0A%20%20Model&body=Title%3A%20EVF-SAM%3A%20Early%20Vision-Language%20Fusion%20for%20Text-Prompted%20Segment%20Anything%0A%20%20Model%0AAuthor%3A%20Yuxuan%20Zhang%20and%20Tianheng%20Cheng%20and%20Rui%20Hu%20and%20ei%20Liu%20and%20Heng%20Liu%20and%20Longjin%20Ran%20and%20Xiaoxin%20Chen%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang%0AAbstract%3A%20%20%20Segment%20Anything%20Model%20%28SAM%29%20has%20attracted%20widespread%20attention%20for%20its%0Asuperior%20interactive%20segmentation%20capabilities%20with%20visual%20prompts%20while%0Alacking%20further%20exploration%20of%20text%20prompts.%20In%20this%20paper%2C%20we%20empirically%0Ainvestigate%20what%20text%20prompt%20encoders%20%28e.g.%2C%20CLIP%20or%20LLM%29%20are%20good%20for%20adapting%0ASAM%20for%20referring%20expression%20segmentation%20and%20introduce%20the%20Early%0AVision-language%20Fusion-based%20SAM%20%28EVF-SAM%29.%20EVF-SAM%20is%20a%20simple%20yet%20effective%0Areferring%20segmentation%20method%20which%20exploits%20multimodal%20prompts%20%28i.e.%2C%20image%0Aand%20text%29%20and%20comprises%20a%20pre-trained%20vision-language%20model%20to%20generate%0Areferring%20prompts%20and%20a%20SAM%20model%20for%20segmentation.%20Surprisingly%2C%20we%20observe%0Athat%3A%20%281%29%20multimodal%20prompts%20and%20%282%29%20vision-language%20models%20with%20early%20fusion%0A%28e.g.%2C%20BEIT-3%29%20are%20beneficial%20for%20prompting%20SAM%20for%20accurate%20referring%0Asegmentation.%20Our%20experiments%20show%20that%20the%20proposed%20EVF-SAM%20based%20on%20BEIT-3%0Acan%20obtain%20state-of-the-art%20performance%20on%20RefCOCO/%2B/g%20for%20referring%20expression%0Asegmentation%20and%20demonstrate%20the%20superiority%20of%20prompting%20SAM%20with%20early%0Avision-language%20fusion.%20In%20addition%2C%20the%20proposed%20EVF-SAM%20with%201.32B%20parameters%0Aachieves%20remarkably%20higher%20performance%20while%20reducing%20nearly%2082%25%20of%20parameters%0Acompared%20to%20previous%20SAM%20methods%20based%20on%20large%20multimodal%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.20076v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEVF-SAM%253A%2520Early%2520Vision-Language%2520Fusion%2520for%2520Text-Prompted%2520Segment%2520Anything%250A%2520%2520Model%26entry.906535625%3DYuxuan%2520Zhang%2520and%2520Tianheng%2520Cheng%2520and%2520Rui%2520Hu%2520and%2520ei%2520Liu%2520and%2520Heng%2520Liu%2520and%2520Longjin%2520Ran%2520and%2520Xiaoxin%2520Chen%2520and%2520Wenyu%2520Liu%2520and%2520Xinggang%2520Wang%26entry.1292438233%3D%2520%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520has%2520attracted%2520widespread%2520attention%2520for%2520its%250Asuperior%2520interactive%2520segmentation%2520capabilities%2520with%2520visual%2520prompts%2520while%250Alacking%2520further%2520exploration%2520of%2520text%2520prompts.%2520In%2520this%2520paper%252C%2520we%2520empirically%250Ainvestigate%2520what%2520text%2520prompt%2520encoders%2520%2528e.g.%252C%2520CLIP%2520or%2520LLM%2529%2520are%2520good%2520for%2520adapting%250ASAM%2520for%2520referring%2520expression%2520segmentation%2520and%2520introduce%2520the%2520Early%250AVision-language%2520Fusion-based%2520SAM%2520%2528EVF-SAM%2529.%2520EVF-SAM%2520is%2520a%2520simple%2520yet%2520effective%250Areferring%2520segmentation%2520method%2520which%2520exploits%2520multimodal%2520prompts%2520%2528i.e.%252C%2520image%250Aand%2520text%2529%2520and%2520comprises%2520a%2520pre-trained%2520vision-language%2520model%2520to%2520generate%250Areferring%2520prompts%2520and%2520a%2520SAM%2520model%2520for%2520segmentation.%2520Surprisingly%252C%2520we%2520observe%250Athat%253A%2520%25281%2529%2520multimodal%2520prompts%2520and%2520%25282%2529%2520vision-language%2520models%2520with%2520early%2520fusion%250A%2528e.g.%252C%2520BEIT-3%2529%2520are%2520beneficial%2520for%2520prompting%2520SAM%2520for%2520accurate%2520referring%250Asegmentation.%2520Our%2520experiments%2520show%2520that%2520the%2520proposed%2520EVF-SAM%2520based%2520on%2520BEIT-3%250Acan%2520obtain%2520state-of-the-art%2520performance%2520on%2520RefCOCO/%252B/g%2520for%2520referring%2520expression%250Asegmentation%2520and%2520demonstrate%2520the%2520superiority%2520of%2520prompting%2520SAM%2520with%2520early%250Avision-language%2520fusion.%2520In%2520addition%252C%2520the%2520proposed%2520EVF-SAM%2520with%25201.32B%2520parameters%250Aachieves%2520remarkably%2520higher%2520performance%2520while%2520reducing%2520nearly%252082%2525%2520of%2520parameters%250Acompared%2520to%2520previous%2520SAM%2520methods%2520based%2520on%2520large%2520multimodal%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.20076v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EVF-SAM%3A%20Early%20Vision-Language%20Fusion%20for%20Text-Prompted%20Segment%20Anything%0A%20%20Model&entry.906535625=Yuxuan%20Zhang%20and%20Tianheng%20Cheng%20and%20Rui%20Hu%20and%20ei%20Liu%20and%20Heng%20Liu%20and%20Longjin%20Ran%20and%20Xiaoxin%20Chen%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang&entry.1292438233=%20%20Segment%20Anything%20Model%20%28SAM%29%20has%20attracted%20widespread%20attention%20for%20its%0Asuperior%20interactive%20segmentation%20capabilities%20with%20visual%20prompts%20while%0Alacking%20further%20exploration%20of%20text%20prompts.%20In%20this%20paper%2C%20we%20empirically%0Ainvestigate%20what%20text%20prompt%20encoders%20%28e.g.%2C%20CLIP%20or%20LLM%29%20are%20good%20for%20adapting%0ASAM%20for%20referring%20expression%20segmentation%20and%20introduce%20the%20Early%0AVision-language%20Fusion-based%20SAM%20%28EVF-SAM%29.%20EVF-SAM%20is%20a%20simple%20yet%20effective%0Areferring%20segmentation%20method%20which%20exploits%20multimodal%20prompts%20%28i.e.%2C%20image%0Aand%20text%29%20and%20comprises%20a%20pre-trained%20vision-language%20model%20to%20generate%0Areferring%20prompts%20and%20a%20SAM%20model%20for%20segmentation.%20Surprisingly%2C%20we%20observe%0Athat%3A%20%281%29%20multimodal%20prompts%20and%20%282%29%20vision-language%20models%20with%20early%20fusion%0A%28e.g.%2C%20BEIT-3%29%20are%20beneficial%20for%20prompting%20SAM%20for%20accurate%20referring%0Asegmentation.%20Our%20experiments%20show%20that%20the%20proposed%20EVF-SAM%20based%20on%20BEIT-3%0Acan%20obtain%20state-of-the-art%20performance%20on%20RefCOCO/%2B/g%20for%20referring%20expression%0Asegmentation%20and%20demonstrate%20the%20superiority%20of%20prompting%20SAM%20with%20early%0Avision-language%20fusion.%20In%20addition%2C%20the%20proposed%20EVF-SAM%20with%201.32B%20parameters%0Aachieves%20remarkably%20higher%20performance%20while%20reducing%20nearly%2082%25%20of%20parameters%0Acompared%20to%20previous%20SAM%20methods%20based%20on%20large%20multimodal%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.20076v1&entry.124074799=Read"},
{"title": "Scalable Training of Graph Foundation Models for Atomistic Materials\n  Modeling: A Case Study with HydraGNN", "author": "Massimiliano Lupo Pasini and Jong Youl Choi and Kshitij Mehta and Pei Zhang and David Rogers and Jonghyun Bae and Khaled Z. Ibrahim and Ashwin M. Aji and Karl W. Schulz and Jorda Polo and Prasanna Balaprakash", "abstract": "  We present our work on developing and training scalable graph foundation\nmodels (GFM) using HydraGNN, a multi-headed graph convolutional neural network\narchitecture. HydraGNN expands the boundaries of graph neural network (GNN) in\nboth training scale and data diversity. It abstracts over message passing\nalgorithms, allowing both reproduction of and comparison across algorithmic\ninnovations that define convolution in GNNs. This work discusses a series of\noptimizations that have allowed scaling up the GFM training to tens of\nthousands of GPUs on datasets that consist of hundreds of millions of graphs.\nOur GFMs use multi-task learning (MTL) to simultaneously learn graph-level and\nnode-level properties of atomistic structures, such as the total energy and\natomic forces. Using over 150 million atomistic structures for training, we\nillustrate the performance of our approach along with the lessons learned on\ntwo United States Department of Energy (US-DOE) supercomputers, namely the\nPerlmutter petascale system at the National Energy Research Scientific\nComputing Center and the Frontier exascale system at Oak Ridge National\nLaboratory. The HydraGNN architecture enables the GFM to achieve near-linear\nstrong scaling performance using more than 2,000 GPUs on Perlmutter and 16,000\nGPUs on Frontier. Hyperparameter optimization (HPO) was performed on over\n64,000 GPUs on Frontier to select GFM architectures with high accuracy. Early\nstopping was applied on each GFM architecture for energy awareness in\nperforming such an extreme-scale task. The training of an ensemble of\nhighest-ranked GFM architectures continued until convergence to establish\nuncertainty quantification (UQ) capabilities with ensemble learning. Our\ncontribution opens the door for rapidly developing, training, and deploying\nGFMs using large-scale computational resources to enable AI-accelerated\nmaterials discovery and design.\n", "link": "http://arxiv.org/abs/2406.12909v2", "date": "2024-06-28", "relevancy": 2.0309, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.531}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5024}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Training%20of%20Graph%20Foundation%20Models%20for%20Atomistic%20Materials%0A%20%20Modeling%3A%20A%20Case%20Study%20with%20HydraGNN&body=Title%3A%20Scalable%20Training%20of%20Graph%20Foundation%20Models%20for%20Atomistic%20Materials%0A%20%20Modeling%3A%20A%20Case%20Study%20with%20HydraGNN%0AAuthor%3A%20Massimiliano%20Lupo%20Pasini%20and%20Jong%20Youl%20Choi%20and%20Kshitij%20Mehta%20and%20Pei%20Zhang%20and%20David%20Rogers%20and%20Jonghyun%20Bae%20and%20Khaled%20Z.%20Ibrahim%20and%20Ashwin%20M.%20Aji%20and%20Karl%20W.%20Schulz%20and%20Jorda%20Polo%20and%20Prasanna%20Balaprakash%0AAbstract%3A%20%20%20We%20present%20our%20work%20on%20developing%20and%20training%20scalable%20graph%20foundation%0Amodels%20%28GFM%29%20using%20HydraGNN%2C%20a%20multi-headed%20graph%20convolutional%20neural%20network%0Aarchitecture.%20HydraGNN%20expands%20the%20boundaries%20of%20graph%20neural%20network%20%28GNN%29%20in%0Aboth%20training%20scale%20and%20data%20diversity.%20It%20abstracts%20over%20message%20passing%0Aalgorithms%2C%20allowing%20both%20reproduction%20of%20and%20comparison%20across%20algorithmic%0Ainnovations%20that%20define%20convolution%20in%20GNNs.%20This%20work%20discusses%20a%20series%20of%0Aoptimizations%20that%20have%20allowed%20scaling%20up%20the%20GFM%20training%20to%20tens%20of%0Athousands%20of%20GPUs%20on%20datasets%20that%20consist%20of%20hundreds%20of%20millions%20of%20graphs.%0AOur%20GFMs%20use%20multi-task%20learning%20%28MTL%29%20to%20simultaneously%20learn%20graph-level%20and%0Anode-level%20properties%20of%20atomistic%20structures%2C%20such%20as%20the%20total%20energy%20and%0Aatomic%20forces.%20Using%20over%20150%20million%20atomistic%20structures%20for%20training%2C%20we%0Aillustrate%20the%20performance%20of%20our%20approach%20along%20with%20the%20lessons%20learned%20on%0Atwo%20United%20States%20Department%20of%20Energy%20%28US-DOE%29%20supercomputers%2C%20namely%20the%0APerlmutter%20petascale%20system%20at%20the%20National%20Energy%20Research%20Scientific%0AComputing%20Center%20and%20the%20Frontier%20exascale%20system%20at%20Oak%20Ridge%20National%0ALaboratory.%20The%20HydraGNN%20architecture%20enables%20the%20GFM%20to%20achieve%20near-linear%0Astrong%20scaling%20performance%20using%20more%20than%202%2C000%20GPUs%20on%20Perlmutter%20and%2016%2C000%0AGPUs%20on%20Frontier.%20Hyperparameter%20optimization%20%28HPO%29%20was%20performed%20on%20over%0A64%2C000%20GPUs%20on%20Frontier%20to%20select%20GFM%20architectures%20with%20high%20accuracy.%20Early%0Astopping%20was%20applied%20on%20each%20GFM%20architecture%20for%20energy%20awareness%20in%0Aperforming%20such%20an%20extreme-scale%20task.%20The%20training%20of%20an%20ensemble%20of%0Ahighest-ranked%20GFM%20architectures%20continued%20until%20convergence%20to%20establish%0Auncertainty%20quantification%20%28UQ%29%20capabilities%20with%20ensemble%20learning.%20Our%0Acontribution%20opens%20the%20door%20for%20rapidly%20developing%2C%20training%2C%20and%20deploying%0AGFMs%20using%20large-scale%20computational%20resources%20to%20enable%20AI-accelerated%0Amaterials%20discovery%20and%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12909v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Training%2520of%2520Graph%2520Foundation%2520Models%2520for%2520Atomistic%2520Materials%250A%2520%2520Modeling%253A%2520A%2520Case%2520Study%2520with%2520HydraGNN%26entry.906535625%3DMassimiliano%2520Lupo%2520Pasini%2520and%2520Jong%2520Youl%2520Choi%2520and%2520Kshitij%2520Mehta%2520and%2520Pei%2520Zhang%2520and%2520David%2520Rogers%2520and%2520Jonghyun%2520Bae%2520and%2520Khaled%2520Z.%2520Ibrahim%2520and%2520Ashwin%2520M.%2520Aji%2520and%2520Karl%2520W.%2520Schulz%2520and%2520Jorda%2520Polo%2520and%2520Prasanna%2520Balaprakash%26entry.1292438233%3D%2520%2520We%2520present%2520our%2520work%2520on%2520developing%2520and%2520training%2520scalable%2520graph%2520foundation%250Amodels%2520%2528GFM%2529%2520using%2520HydraGNN%252C%2520a%2520multi-headed%2520graph%2520convolutional%2520neural%2520network%250Aarchitecture.%2520HydraGNN%2520expands%2520the%2520boundaries%2520of%2520graph%2520neural%2520network%2520%2528GNN%2529%2520in%250Aboth%2520training%2520scale%2520and%2520data%2520diversity.%2520It%2520abstracts%2520over%2520message%2520passing%250Aalgorithms%252C%2520allowing%2520both%2520reproduction%2520of%2520and%2520comparison%2520across%2520algorithmic%250Ainnovations%2520that%2520define%2520convolution%2520in%2520GNNs.%2520This%2520work%2520discusses%2520a%2520series%2520of%250Aoptimizations%2520that%2520have%2520allowed%2520scaling%2520up%2520the%2520GFM%2520training%2520to%2520tens%2520of%250Athousands%2520of%2520GPUs%2520on%2520datasets%2520that%2520consist%2520of%2520hundreds%2520of%2520millions%2520of%2520graphs.%250AOur%2520GFMs%2520use%2520multi-task%2520learning%2520%2528MTL%2529%2520to%2520simultaneously%2520learn%2520graph-level%2520and%250Anode-level%2520properties%2520of%2520atomistic%2520structures%252C%2520such%2520as%2520the%2520total%2520energy%2520and%250Aatomic%2520forces.%2520Using%2520over%2520150%2520million%2520atomistic%2520structures%2520for%2520training%252C%2520we%250Aillustrate%2520the%2520performance%2520of%2520our%2520approach%2520along%2520with%2520the%2520lessons%2520learned%2520on%250Atwo%2520United%2520States%2520Department%2520of%2520Energy%2520%2528US-DOE%2529%2520supercomputers%252C%2520namely%2520the%250APerlmutter%2520petascale%2520system%2520at%2520the%2520National%2520Energy%2520Research%2520Scientific%250AComputing%2520Center%2520and%2520the%2520Frontier%2520exascale%2520system%2520at%2520Oak%2520Ridge%2520National%250ALaboratory.%2520The%2520HydraGNN%2520architecture%2520enables%2520the%2520GFM%2520to%2520achieve%2520near-linear%250Astrong%2520scaling%2520performance%2520using%2520more%2520than%25202%252C000%2520GPUs%2520on%2520Perlmutter%2520and%252016%252C000%250AGPUs%2520on%2520Frontier.%2520Hyperparameter%2520optimization%2520%2528HPO%2529%2520was%2520performed%2520on%2520over%250A64%252C000%2520GPUs%2520on%2520Frontier%2520to%2520select%2520GFM%2520architectures%2520with%2520high%2520accuracy.%2520Early%250Astopping%2520was%2520applied%2520on%2520each%2520GFM%2520architecture%2520for%2520energy%2520awareness%2520in%250Aperforming%2520such%2520an%2520extreme-scale%2520task.%2520The%2520training%2520of%2520an%2520ensemble%2520of%250Ahighest-ranked%2520GFM%2520architectures%2520continued%2520until%2520convergence%2520to%2520establish%250Auncertainty%2520quantification%2520%2528UQ%2529%2520capabilities%2520with%2520ensemble%2520learning.%2520Our%250Acontribution%2520opens%2520the%2520door%2520for%2520rapidly%2520developing%252C%2520training%252C%2520and%2520deploying%250AGFMs%2520using%2520large-scale%2520computational%2520resources%2520to%2520enable%2520AI-accelerated%250Amaterials%2520discovery%2520and%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12909v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Training%20of%20Graph%20Foundation%20Models%20for%20Atomistic%20Materials%0A%20%20Modeling%3A%20A%20Case%20Study%20with%20HydraGNN&entry.906535625=Massimiliano%20Lupo%20Pasini%20and%20Jong%20Youl%20Choi%20and%20Kshitij%20Mehta%20and%20Pei%20Zhang%20and%20David%20Rogers%20and%20Jonghyun%20Bae%20and%20Khaled%20Z.%20Ibrahim%20and%20Ashwin%20M.%20Aji%20and%20Karl%20W.%20Schulz%20and%20Jorda%20Polo%20and%20Prasanna%20Balaprakash&entry.1292438233=%20%20We%20present%20our%20work%20on%20developing%20and%20training%20scalable%20graph%20foundation%0Amodels%20%28GFM%29%20using%20HydraGNN%2C%20a%20multi-headed%20graph%20convolutional%20neural%20network%0Aarchitecture.%20HydraGNN%20expands%20the%20boundaries%20of%20graph%20neural%20network%20%28GNN%29%20in%0Aboth%20training%20scale%20and%20data%20diversity.%20It%20abstracts%20over%20message%20passing%0Aalgorithms%2C%20allowing%20both%20reproduction%20of%20and%20comparison%20across%20algorithmic%0Ainnovations%20that%20define%20convolution%20in%20GNNs.%20This%20work%20discusses%20a%20series%20of%0Aoptimizations%20that%20have%20allowed%20scaling%20up%20the%20GFM%20training%20to%20tens%20of%0Athousands%20of%20GPUs%20on%20datasets%20that%20consist%20of%20hundreds%20of%20millions%20of%20graphs.%0AOur%20GFMs%20use%20multi-task%20learning%20%28MTL%29%20to%20simultaneously%20learn%20graph-level%20and%0Anode-level%20properties%20of%20atomistic%20structures%2C%20such%20as%20the%20total%20energy%20and%0Aatomic%20forces.%20Using%20over%20150%20million%20atomistic%20structures%20for%20training%2C%20we%0Aillustrate%20the%20performance%20of%20our%20approach%20along%20with%20the%20lessons%20learned%20on%0Atwo%20United%20States%20Department%20of%20Energy%20%28US-DOE%29%20supercomputers%2C%20namely%20the%0APerlmutter%20petascale%20system%20at%20the%20National%20Energy%20Research%20Scientific%0AComputing%20Center%20and%20the%20Frontier%20exascale%20system%20at%20Oak%20Ridge%20National%0ALaboratory.%20The%20HydraGNN%20architecture%20enables%20the%20GFM%20to%20achieve%20near-linear%0Astrong%20scaling%20performance%20using%20more%20than%202%2C000%20GPUs%20on%20Perlmutter%20and%2016%2C000%0AGPUs%20on%20Frontier.%20Hyperparameter%20optimization%20%28HPO%29%20was%20performed%20on%20over%0A64%2C000%20GPUs%20on%20Frontier%20to%20select%20GFM%20architectures%20with%20high%20accuracy.%20Early%0Astopping%20was%20applied%20on%20each%20GFM%20architecture%20for%20energy%20awareness%20in%0Aperforming%20such%20an%20extreme-scale%20task.%20The%20training%20of%20an%20ensemble%20of%0Ahighest-ranked%20GFM%20architectures%20continued%20until%20convergence%20to%20establish%0Auncertainty%20quantification%20%28UQ%29%20capabilities%20with%20ensemble%20learning.%20Our%0Acontribution%20opens%20the%20door%20for%20rapidly%20developing%2C%20training%2C%20and%20deploying%0AGFMs%20using%20large-scale%20computational%20resources%20to%20enable%20AI-accelerated%0Amaterials%20discovery%20and%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12909v2&entry.124074799=Read"},
{"title": "FI-CBL: A Probabilistic Method for Concept-Based Learning with Expert\n  Rules", "author": "Lev V. Utkin and Andrei V. Konstantinov and Stanislav R. Kirpichenko", "abstract": "  A method for solving concept-based learning (CBL) problem is proposed. The\nmain idea behind the method is to divide each concept-annotated image into\npatches, to transform the patches into embeddings by using an autoencoder, and\nto cluster the embeddings assuming that each cluster will mainly contain\nembeddings of patches with certain concepts. To find concepts of a new image,\nthe method implements the frequentist inference by computing prior and\nposterior probabilities of concepts based on rates of patches from images with\ncertain values of the concepts. Therefore, the proposed method is called the\nFrequentist Inference CBL (FI-CBL). FI-CBL allows us to incorporate the expert\nrules in the form of logic functions into the inference procedure. An idea\nbehind the incorporation is to update prior and conditional probabilities of\nconcepts to satisfy the rules. The method is transparent because it has an\nexplicit sequence of probabilistic calculations and a clear frequency\ninterpretation. Numerical experiments show that FI-CBL outperforms the concept\nbottleneck model in cases when the number of training data is small. The code\nof proposed algorithms is publicly available.\n", "link": "http://arxiv.org/abs/2406.19897v1", "date": "2024-06-28", "relevancy": 2.024, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5301}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5199}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4763}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FI-CBL%3A%20A%20Probabilistic%20Method%20for%20Concept-Based%20Learning%20with%20Expert%0A%20%20Rules&body=Title%3A%20FI-CBL%3A%20A%20Probabilistic%20Method%20for%20Concept-Based%20Learning%20with%20Expert%0A%20%20Rules%0AAuthor%3A%20Lev%20V.%20Utkin%20and%20Andrei%20V.%20Konstantinov%20and%20Stanislav%20R.%20Kirpichenko%0AAbstract%3A%20%20%20A%20method%20for%20solving%20concept-based%20learning%20%28CBL%29%20problem%20is%20proposed.%20The%0Amain%20idea%20behind%20the%20method%20is%20to%20divide%20each%20concept-annotated%20image%20into%0Apatches%2C%20to%20transform%20the%20patches%20into%20embeddings%20by%20using%20an%20autoencoder%2C%20and%0Ato%20cluster%20the%20embeddings%20assuming%20that%20each%20cluster%20will%20mainly%20contain%0Aembeddings%20of%20patches%20with%20certain%20concepts.%20To%20find%20concepts%20of%20a%20new%20image%2C%0Athe%20method%20implements%20the%20frequentist%20inference%20by%20computing%20prior%20and%0Aposterior%20probabilities%20of%20concepts%20based%20on%20rates%20of%20patches%20from%20images%20with%0Acertain%20values%20of%20the%20concepts.%20Therefore%2C%20the%20proposed%20method%20is%20called%20the%0AFrequentist%20Inference%20CBL%20%28FI-CBL%29.%20FI-CBL%20allows%20us%20to%20incorporate%20the%20expert%0Arules%20in%20the%20form%20of%20logic%20functions%20into%20the%20inference%20procedure.%20An%20idea%0Abehind%20the%20incorporation%20is%20to%20update%20prior%20and%20conditional%20probabilities%20of%0Aconcepts%20to%20satisfy%20the%20rules.%20The%20method%20is%20transparent%20because%20it%20has%20an%0Aexplicit%20sequence%20of%20probabilistic%20calculations%20and%20a%20clear%20frequency%0Ainterpretation.%20Numerical%20experiments%20show%20that%20FI-CBL%20outperforms%20the%20concept%0Abottleneck%20model%20in%20cases%20when%20the%20number%20of%20training%20data%20is%20small.%20The%20code%0Aof%20proposed%20algorithms%20is%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19897v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFI-CBL%253A%2520A%2520Probabilistic%2520Method%2520for%2520Concept-Based%2520Learning%2520with%2520Expert%250A%2520%2520Rules%26entry.906535625%3DLev%2520V.%2520Utkin%2520and%2520Andrei%2520V.%2520Konstantinov%2520and%2520Stanislav%2520R.%2520Kirpichenko%26entry.1292438233%3D%2520%2520A%2520method%2520for%2520solving%2520concept-based%2520learning%2520%2528CBL%2529%2520problem%2520is%2520proposed.%2520The%250Amain%2520idea%2520behind%2520the%2520method%2520is%2520to%2520divide%2520each%2520concept-annotated%2520image%2520into%250Apatches%252C%2520to%2520transform%2520the%2520patches%2520into%2520embeddings%2520by%2520using%2520an%2520autoencoder%252C%2520and%250Ato%2520cluster%2520the%2520embeddings%2520assuming%2520that%2520each%2520cluster%2520will%2520mainly%2520contain%250Aembeddings%2520of%2520patches%2520with%2520certain%2520concepts.%2520To%2520find%2520concepts%2520of%2520a%2520new%2520image%252C%250Athe%2520method%2520implements%2520the%2520frequentist%2520inference%2520by%2520computing%2520prior%2520and%250Aposterior%2520probabilities%2520of%2520concepts%2520based%2520on%2520rates%2520of%2520patches%2520from%2520images%2520with%250Acertain%2520values%2520of%2520the%2520concepts.%2520Therefore%252C%2520the%2520proposed%2520method%2520is%2520called%2520the%250AFrequentist%2520Inference%2520CBL%2520%2528FI-CBL%2529.%2520FI-CBL%2520allows%2520us%2520to%2520incorporate%2520the%2520expert%250Arules%2520in%2520the%2520form%2520of%2520logic%2520functions%2520into%2520the%2520inference%2520procedure.%2520An%2520idea%250Abehind%2520the%2520incorporation%2520is%2520to%2520update%2520prior%2520and%2520conditional%2520probabilities%2520of%250Aconcepts%2520to%2520satisfy%2520the%2520rules.%2520The%2520method%2520is%2520transparent%2520because%2520it%2520has%2520an%250Aexplicit%2520sequence%2520of%2520probabilistic%2520calculations%2520and%2520a%2520clear%2520frequency%250Ainterpretation.%2520Numerical%2520experiments%2520show%2520that%2520FI-CBL%2520outperforms%2520the%2520concept%250Abottleneck%2520model%2520in%2520cases%2520when%2520the%2520number%2520of%2520training%2520data%2520is%2520small.%2520The%2520code%250Aof%2520proposed%2520algorithms%2520is%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19897v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FI-CBL%3A%20A%20Probabilistic%20Method%20for%20Concept-Based%20Learning%20with%20Expert%0A%20%20Rules&entry.906535625=Lev%20V.%20Utkin%20and%20Andrei%20V.%20Konstantinov%20and%20Stanislav%20R.%20Kirpichenko&entry.1292438233=%20%20A%20method%20for%20solving%20concept-based%20learning%20%28CBL%29%20problem%20is%20proposed.%20The%0Amain%20idea%20behind%20the%20method%20is%20to%20divide%20each%20concept-annotated%20image%20into%0Apatches%2C%20to%20transform%20the%20patches%20into%20embeddings%20by%20using%20an%20autoencoder%2C%20and%0Ato%20cluster%20the%20embeddings%20assuming%20that%20each%20cluster%20will%20mainly%20contain%0Aembeddings%20of%20patches%20with%20certain%20concepts.%20To%20find%20concepts%20of%20a%20new%20image%2C%0Athe%20method%20implements%20the%20frequentist%20inference%20by%20computing%20prior%20and%0Aposterior%20probabilities%20of%20concepts%20based%20on%20rates%20of%20patches%20from%20images%20with%0Acertain%20values%20of%20the%20concepts.%20Therefore%2C%20the%20proposed%20method%20is%20called%20the%0AFrequentist%20Inference%20CBL%20%28FI-CBL%29.%20FI-CBL%20allows%20us%20to%20incorporate%20the%20expert%0Arules%20in%20the%20form%20of%20logic%20functions%20into%20the%20inference%20procedure.%20An%20idea%0Abehind%20the%20incorporation%20is%20to%20update%20prior%20and%20conditional%20probabilities%20of%0Aconcepts%20to%20satisfy%20the%20rules.%20The%20method%20is%20transparent%20because%20it%20has%20an%0Aexplicit%20sequence%20of%20probabilistic%20calculations%20and%20a%20clear%20frequency%0Ainterpretation.%20Numerical%20experiments%20show%20that%20FI-CBL%20outperforms%20the%20concept%0Abottleneck%20model%20in%20cases%20when%20the%20number%20of%20training%20data%20is%20small.%20The%20code%0Aof%20proposed%20algorithms%20is%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19897v1&entry.124074799=Read"},
{"title": "Robustness Assessment of a Runway Object Classifier for Safe Aircraft\n  Taxiing", "author": "Yizhak Elboher and Raya Elsaleh and Omri Isac and M\u00e9lanie Ducoffe and Audrey Galametz and Guillaume Pov\u00e9da and Ryma Boumazouza and No\u00e9mie Cohen and Guy Katz", "abstract": "  As deep neural networks (DNNs) are becoming the prominent solution for many\ncomputational problems, the aviation industry seeks to explore their potential\nin alleviating pilot workload and in improving operational safety. However, the\nuse of DNNs in this type of safety-critical applications requires a thorough\ncertification process. This need can be addressed through formal verification,\nwhich provides rigorous assurances -- e.g.,~by proving the absence of certain\nmispredictions. In this case-study paper, we demonstrate this process using an\nimage-classifier DNN currently under development at Airbus and intended for use\nduring the aircraft taxiing phase. We use formal methods to assess this DNN's\nrobustness to three common image perturbation types: noise, brightness and\ncontrast, and some of their combinations. This process entails multiple\ninvocations of the underlying verifier, which might be computationally\nexpensive; and we therefore propose a method that leverages the monotonicity of\nthese robustness properties, as well as the results of past verification\nqueries, in order to reduce the overall number of verification queries required\nby nearly 60%. Our results provide an indication of the level of robustness\nachieved by the DNN classifier under study, and indicate that it is\nconsiderably more vulnerable to noise than to brightness or contrast\nperturbations.\n", "link": "http://arxiv.org/abs/2402.00035v3", "date": "2024-06-28", "relevancy": 2.0175, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5182}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5053}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robustness%20Assessment%20of%20a%20Runway%20Object%20Classifier%20for%20Safe%20Aircraft%0A%20%20Taxiing&body=Title%3A%20Robustness%20Assessment%20of%20a%20Runway%20Object%20Classifier%20for%20Safe%20Aircraft%0A%20%20Taxiing%0AAuthor%3A%20Yizhak%20Elboher%20and%20Raya%20Elsaleh%20and%20Omri%20Isac%20and%20M%C3%A9lanie%20Ducoffe%20and%20Audrey%20Galametz%20and%20Guillaume%20Pov%C3%A9da%20and%20Ryma%20Boumazouza%20and%20No%C3%A9mie%20Cohen%20and%20Guy%20Katz%0AAbstract%3A%20%20%20As%20deep%20neural%20networks%20%28DNNs%29%20are%20becoming%20the%20prominent%20solution%20for%20many%0Acomputational%20problems%2C%20the%20aviation%20industry%20seeks%20to%20explore%20their%20potential%0Ain%20alleviating%20pilot%20workload%20and%20in%20improving%20operational%20safety.%20However%2C%20the%0Ause%20of%20DNNs%20in%20this%20type%20of%20safety-critical%20applications%20requires%20a%20thorough%0Acertification%20process.%20This%20need%20can%20be%20addressed%20through%20formal%20verification%2C%0Awhich%20provides%20rigorous%20assurances%20--%20e.g.%2C~by%20proving%20the%20absence%20of%20certain%0Amispredictions.%20In%20this%20case-study%20paper%2C%20we%20demonstrate%20this%20process%20using%20an%0Aimage-classifier%20DNN%20currently%20under%20development%20at%20Airbus%20and%20intended%20for%20use%0Aduring%20the%20aircraft%20taxiing%20phase.%20We%20use%20formal%20methods%20to%20assess%20this%20DNN%27s%0Arobustness%20to%20three%20common%20image%20perturbation%20types%3A%20noise%2C%20brightness%20and%0Acontrast%2C%20and%20some%20of%20their%20combinations.%20This%20process%20entails%20multiple%0Ainvocations%20of%20the%20underlying%20verifier%2C%20which%20might%20be%20computationally%0Aexpensive%3B%20and%20we%20therefore%20propose%20a%20method%20that%20leverages%20the%20monotonicity%20of%0Athese%20robustness%20properties%2C%20as%20well%20as%20the%20results%20of%20past%20verification%0Aqueries%2C%20in%20order%20to%20reduce%20the%20overall%20number%20of%20verification%20queries%20required%0Aby%20nearly%2060%25.%20Our%20results%20provide%20an%20indication%20of%20the%20level%20of%20robustness%0Aachieved%20by%20the%20DNN%20classifier%20under%20study%2C%20and%20indicate%20that%20it%20is%0Aconsiderably%20more%20vulnerable%20to%20noise%20than%20to%20brightness%20or%20contrast%0Aperturbations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.00035v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobustness%2520Assessment%2520of%2520a%2520Runway%2520Object%2520Classifier%2520for%2520Safe%2520Aircraft%250A%2520%2520Taxiing%26entry.906535625%3DYizhak%2520Elboher%2520and%2520Raya%2520Elsaleh%2520and%2520Omri%2520Isac%2520and%2520M%25C3%25A9lanie%2520Ducoffe%2520and%2520Audrey%2520Galametz%2520and%2520Guillaume%2520Pov%25C3%25A9da%2520and%2520Ryma%2520Boumazouza%2520and%2520No%25C3%25A9mie%2520Cohen%2520and%2520Guy%2520Katz%26entry.1292438233%3D%2520%2520As%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520are%2520becoming%2520the%2520prominent%2520solution%2520for%2520many%250Acomputational%2520problems%252C%2520the%2520aviation%2520industry%2520seeks%2520to%2520explore%2520their%2520potential%250Ain%2520alleviating%2520pilot%2520workload%2520and%2520in%2520improving%2520operational%2520safety.%2520However%252C%2520the%250Ause%2520of%2520DNNs%2520in%2520this%2520type%2520of%2520safety-critical%2520applications%2520requires%2520a%2520thorough%250Acertification%2520process.%2520This%2520need%2520can%2520be%2520addressed%2520through%2520formal%2520verification%252C%250Awhich%2520provides%2520rigorous%2520assurances%2520--%2520e.g.%252C~by%2520proving%2520the%2520absence%2520of%2520certain%250Amispredictions.%2520In%2520this%2520case-study%2520paper%252C%2520we%2520demonstrate%2520this%2520process%2520using%2520an%250Aimage-classifier%2520DNN%2520currently%2520under%2520development%2520at%2520Airbus%2520and%2520intended%2520for%2520use%250Aduring%2520the%2520aircraft%2520taxiing%2520phase.%2520We%2520use%2520formal%2520methods%2520to%2520assess%2520this%2520DNN%2527s%250Arobustness%2520to%2520three%2520common%2520image%2520perturbation%2520types%253A%2520noise%252C%2520brightness%2520and%250Acontrast%252C%2520and%2520some%2520of%2520their%2520combinations.%2520This%2520process%2520entails%2520multiple%250Ainvocations%2520of%2520the%2520underlying%2520verifier%252C%2520which%2520might%2520be%2520computationally%250Aexpensive%253B%2520and%2520we%2520therefore%2520propose%2520a%2520method%2520that%2520leverages%2520the%2520monotonicity%2520of%250Athese%2520robustness%2520properties%252C%2520as%2520well%2520as%2520the%2520results%2520of%2520past%2520verification%250Aqueries%252C%2520in%2520order%2520to%2520reduce%2520the%2520overall%2520number%2520of%2520verification%2520queries%2520required%250Aby%2520nearly%252060%2525.%2520Our%2520results%2520provide%2520an%2520indication%2520of%2520the%2520level%2520of%2520robustness%250Aachieved%2520by%2520the%2520DNN%2520classifier%2520under%2520study%252C%2520and%2520indicate%2520that%2520it%2520is%250Aconsiderably%2520more%2520vulnerable%2520to%2520noise%2520than%2520to%2520brightness%2520or%2520contrast%250Aperturbations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.00035v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robustness%20Assessment%20of%20a%20Runway%20Object%20Classifier%20for%20Safe%20Aircraft%0A%20%20Taxiing&entry.906535625=Yizhak%20Elboher%20and%20Raya%20Elsaleh%20and%20Omri%20Isac%20and%20M%C3%A9lanie%20Ducoffe%20and%20Audrey%20Galametz%20and%20Guillaume%20Pov%C3%A9da%20and%20Ryma%20Boumazouza%20and%20No%C3%A9mie%20Cohen%20and%20Guy%20Katz&entry.1292438233=%20%20As%20deep%20neural%20networks%20%28DNNs%29%20are%20becoming%20the%20prominent%20solution%20for%20many%0Acomputational%20problems%2C%20the%20aviation%20industry%20seeks%20to%20explore%20their%20potential%0Ain%20alleviating%20pilot%20workload%20and%20in%20improving%20operational%20safety.%20However%2C%20the%0Ause%20of%20DNNs%20in%20this%20type%20of%20safety-critical%20applications%20requires%20a%20thorough%0Acertification%20process.%20This%20need%20can%20be%20addressed%20through%20formal%20verification%2C%0Awhich%20provides%20rigorous%20assurances%20--%20e.g.%2C~by%20proving%20the%20absence%20of%20certain%0Amispredictions.%20In%20this%20case-study%20paper%2C%20we%20demonstrate%20this%20process%20using%20an%0Aimage-classifier%20DNN%20currently%20under%20development%20at%20Airbus%20and%20intended%20for%20use%0Aduring%20the%20aircraft%20taxiing%20phase.%20We%20use%20formal%20methods%20to%20assess%20this%20DNN%27s%0Arobustness%20to%20three%20common%20image%20perturbation%20types%3A%20noise%2C%20brightness%20and%0Acontrast%2C%20and%20some%20of%20their%20combinations.%20This%20process%20entails%20multiple%0Ainvocations%20of%20the%20underlying%20verifier%2C%20which%20might%20be%20computationally%0Aexpensive%3B%20and%20we%20therefore%20propose%20a%20method%20that%20leverages%20the%20monotonicity%20of%0Athese%20robustness%20properties%2C%20as%20well%20as%20the%20results%20of%20past%20verification%0Aqueries%2C%20in%20order%20to%20reduce%20the%20overall%20number%20of%20verification%20queries%20required%0Aby%20nearly%2060%25.%20Our%20results%20provide%20an%20indication%20of%20the%20level%20of%20robustness%0Aachieved%20by%20the%20DNN%20classifier%20under%20study%2C%20and%20indicate%20that%20it%20is%0Aconsiderably%20more%20vulnerable%20to%20noise%20than%20to%20brightness%20or%20contrast%0Aperturbations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.00035v3&entry.124074799=Read"},
{"title": "LightStereo: Channel Boost Is All Your Need for Efficient 2D Cost\n  Aggregation", "author": "Xianda Guo and Chenming Zhang and Dujun Nie and Wenzhao Zheng and Youmin Zhang and Long Chen", "abstract": "  We present LightStereo, a cutting-edge stereo-matching network crafted to\naccelerate the matching process. Departing from conventional methodologies that\nrely on aggregating computationally intensive 4D costs, LightStereo adopts the\n3D cost volume as a lightweight alternative. While similar approaches have been\nexplored previously, our breakthrough lies in enhancing performance through a\ndedicated focus on the channel dimension of the 3D cost volume, where the\ndistribution of matching costs is encapsulated. Our exhaustive exploration has\nyielded plenty of strategies to amplify the capacity of the pivotal dimension,\nensuring both precision and efficiency. We compare the proposed LightStereo\nwith existing state-of-the-art methods across various benchmarks, which\ndemonstrate its superior performance in speed, accuracy, and resource\nutilization. LightStereo achieves a competitive EPE metric in the SceneFlow\ndatasets while demanding a minimum of only 22 GFLOPs, with an inference time of\njust 17 ms. Our comprehensive analysis reveals the effect of 2D cost\naggregation for stereo matching, paving the way for real-world applications of\nefficient stereo systems. Code will be available at\n\\url{https://github.com/XiandaGuo/OpenStereo}.\n", "link": "http://arxiv.org/abs/2406.19833v1", "date": "2024-06-28", "relevancy": 2.0095, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.523}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4965}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4655}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LightStereo%3A%20Channel%20Boost%20Is%20All%20Your%20Need%20for%20Efficient%202D%20Cost%0A%20%20Aggregation&body=Title%3A%20LightStereo%3A%20Channel%20Boost%20Is%20All%20Your%20Need%20for%20Efficient%202D%20Cost%0A%20%20Aggregation%0AAuthor%3A%20Xianda%20Guo%20and%20Chenming%20Zhang%20and%20Dujun%20Nie%20and%20Wenzhao%20Zheng%20and%20Youmin%20Zhang%20and%20Long%20Chen%0AAbstract%3A%20%20%20We%20present%20LightStereo%2C%20a%20cutting-edge%20stereo-matching%20network%20crafted%20to%0Aaccelerate%20the%20matching%20process.%20Departing%20from%20conventional%20methodologies%20that%0Arely%20on%20aggregating%20computationally%20intensive%204D%20costs%2C%20LightStereo%20adopts%20the%0A3D%20cost%20volume%20as%20a%20lightweight%20alternative.%20While%20similar%20approaches%20have%20been%0Aexplored%20previously%2C%20our%20breakthrough%20lies%20in%20enhancing%20performance%20through%20a%0Adedicated%20focus%20on%20the%20channel%20dimension%20of%20the%203D%20cost%20volume%2C%20where%20the%0Adistribution%20of%20matching%20costs%20is%20encapsulated.%20Our%20exhaustive%20exploration%20has%0Ayielded%20plenty%20of%20strategies%20to%20amplify%20the%20capacity%20of%20the%20pivotal%20dimension%2C%0Aensuring%20both%20precision%20and%20efficiency.%20We%20compare%20the%20proposed%20LightStereo%0Awith%20existing%20state-of-the-art%20methods%20across%20various%20benchmarks%2C%20which%0Ademonstrate%20its%20superior%20performance%20in%20speed%2C%20accuracy%2C%20and%20resource%0Autilization.%20LightStereo%20achieves%20a%20competitive%20EPE%20metric%20in%20the%20SceneFlow%0Adatasets%20while%20demanding%20a%20minimum%20of%20only%2022%20GFLOPs%2C%20with%20an%20inference%20time%20of%0Ajust%2017%20ms.%20Our%20comprehensive%20analysis%20reveals%20the%20effect%20of%202D%20cost%0Aaggregation%20for%20stereo%20matching%2C%20paving%20the%20way%20for%20real-world%20applications%20of%0Aefficient%20stereo%20systems.%20Code%20will%20be%20available%20at%0A%5Curl%7Bhttps%3A//github.com/XiandaGuo/OpenStereo%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19833v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightStereo%253A%2520Channel%2520Boost%2520Is%2520All%2520Your%2520Need%2520for%2520Efficient%25202D%2520Cost%250A%2520%2520Aggregation%26entry.906535625%3DXianda%2520Guo%2520and%2520Chenming%2520Zhang%2520and%2520Dujun%2520Nie%2520and%2520Wenzhao%2520Zheng%2520and%2520Youmin%2520Zhang%2520and%2520Long%2520Chen%26entry.1292438233%3D%2520%2520We%2520present%2520LightStereo%252C%2520a%2520cutting-edge%2520stereo-matching%2520network%2520crafted%2520to%250Aaccelerate%2520the%2520matching%2520process.%2520Departing%2520from%2520conventional%2520methodologies%2520that%250Arely%2520on%2520aggregating%2520computationally%2520intensive%25204D%2520costs%252C%2520LightStereo%2520adopts%2520the%250A3D%2520cost%2520volume%2520as%2520a%2520lightweight%2520alternative.%2520While%2520similar%2520approaches%2520have%2520been%250Aexplored%2520previously%252C%2520our%2520breakthrough%2520lies%2520in%2520enhancing%2520performance%2520through%2520a%250Adedicated%2520focus%2520on%2520the%2520channel%2520dimension%2520of%2520the%25203D%2520cost%2520volume%252C%2520where%2520the%250Adistribution%2520of%2520matching%2520costs%2520is%2520encapsulated.%2520Our%2520exhaustive%2520exploration%2520has%250Ayielded%2520plenty%2520of%2520strategies%2520to%2520amplify%2520the%2520capacity%2520of%2520the%2520pivotal%2520dimension%252C%250Aensuring%2520both%2520precision%2520and%2520efficiency.%2520We%2520compare%2520the%2520proposed%2520LightStereo%250Awith%2520existing%2520state-of-the-art%2520methods%2520across%2520various%2520benchmarks%252C%2520which%250Ademonstrate%2520its%2520superior%2520performance%2520in%2520speed%252C%2520accuracy%252C%2520and%2520resource%250Autilization.%2520LightStereo%2520achieves%2520a%2520competitive%2520EPE%2520metric%2520in%2520the%2520SceneFlow%250Adatasets%2520while%2520demanding%2520a%2520minimum%2520of%2520only%252022%2520GFLOPs%252C%2520with%2520an%2520inference%2520time%2520of%250Ajust%252017%2520ms.%2520Our%2520comprehensive%2520analysis%2520reveals%2520the%2520effect%2520of%25202D%2520cost%250Aaggregation%2520for%2520stereo%2520matching%252C%2520paving%2520the%2520way%2520for%2520real-world%2520applications%2520of%250Aefficient%2520stereo%2520systems.%2520Code%2520will%2520be%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/XiandaGuo/OpenStereo%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19833v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LightStereo%3A%20Channel%20Boost%20Is%20All%20Your%20Need%20for%20Efficient%202D%20Cost%0A%20%20Aggregation&entry.906535625=Xianda%20Guo%20and%20Chenming%20Zhang%20and%20Dujun%20Nie%20and%20Wenzhao%20Zheng%20and%20Youmin%20Zhang%20and%20Long%20Chen&entry.1292438233=%20%20We%20present%20LightStereo%2C%20a%20cutting-edge%20stereo-matching%20network%20crafted%20to%0Aaccelerate%20the%20matching%20process.%20Departing%20from%20conventional%20methodologies%20that%0Arely%20on%20aggregating%20computationally%20intensive%204D%20costs%2C%20LightStereo%20adopts%20the%0A3D%20cost%20volume%20as%20a%20lightweight%20alternative.%20While%20similar%20approaches%20have%20been%0Aexplored%20previously%2C%20our%20breakthrough%20lies%20in%20enhancing%20performance%20through%20a%0Adedicated%20focus%20on%20the%20channel%20dimension%20of%20the%203D%20cost%20volume%2C%20where%20the%0Adistribution%20of%20matching%20costs%20is%20encapsulated.%20Our%20exhaustive%20exploration%20has%0Ayielded%20plenty%20of%20strategies%20to%20amplify%20the%20capacity%20of%20the%20pivotal%20dimension%2C%0Aensuring%20both%20precision%20and%20efficiency.%20We%20compare%20the%20proposed%20LightStereo%0Awith%20existing%20state-of-the-art%20methods%20across%20various%20benchmarks%2C%20which%0Ademonstrate%20its%20superior%20performance%20in%20speed%2C%20accuracy%2C%20and%20resource%0Autilization.%20LightStereo%20achieves%20a%20competitive%20EPE%20metric%20in%20the%20SceneFlow%0Adatasets%20while%20demanding%20a%20minimum%20of%20only%2022%20GFLOPs%2C%20with%20an%20inference%20time%20of%0Ajust%2017%20ms.%20Our%20comprehensive%20analysis%20reveals%20the%20effect%20of%202D%20cost%0Aaggregation%20for%20stereo%20matching%2C%20paving%20the%20way%20for%20real-world%20applications%20of%0Aefficient%20stereo%20systems.%20Code%20will%20be%20available%20at%0A%5Curl%7Bhttps%3A//github.com/XiandaGuo/OpenStereo%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19833v1&entry.124074799=Read"},
{"title": "Methods for Combining and Representing Non-Contextual Autonomy Scores\n  for Unmanned Aerial Systems", "author": "Brendan Hertel and Ryan Donald and Christian Dumas and S. Reza Ahmadzadeh", "abstract": "  Measuring an overall autonomy score for a robotic system requires the\ncombination of a set of relevant aspects and features of the system that might\nbe measured in different units, qualitative, and/or discordant. In this paper,\nwe build upon an existing non-contextual autonomy framework that measures and\ncombines the Autonomy Level and the Component Performance of a system as\noverall autonomy score. We examine several methods of combining features,\nshowing how some methods find different rankings of the same data, and we\nemploy the weighted product method to resolve this issue. Furthermore, we\nintroduce the non-contextual autonomy coordinate and represent the overall\nautonomy of a system with an autonomy distance. We apply our method to a set of\nseven Unmanned Aerial Systems (UAS) and obtain their absolute autonomy score as\nwell as their relative score with respect to the best system.\n", "link": "http://arxiv.org/abs/2111.07438v2", "date": "2024-06-28", "relevancy": 2.003, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5091}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4956}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4945}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Methods%20for%20Combining%20and%20Representing%20Non-Contextual%20Autonomy%20Scores%0A%20%20for%20Unmanned%20Aerial%20Systems&body=Title%3A%20Methods%20for%20Combining%20and%20Representing%20Non-Contextual%20Autonomy%20Scores%0A%20%20for%20Unmanned%20Aerial%20Systems%0AAuthor%3A%20Brendan%20Hertel%20and%20Ryan%20Donald%20and%20Christian%20Dumas%20and%20S.%20Reza%20Ahmadzadeh%0AAbstract%3A%20%20%20Measuring%20an%20overall%20autonomy%20score%20for%20a%20robotic%20system%20requires%20the%0Acombination%20of%20a%20set%20of%20relevant%20aspects%20and%20features%20of%20the%20system%20that%20might%0Abe%20measured%20in%20different%20units%2C%20qualitative%2C%20and/or%20discordant.%20In%20this%20paper%2C%0Awe%20build%20upon%20an%20existing%20non-contextual%20autonomy%20framework%20that%20measures%20and%0Acombines%20the%20Autonomy%20Level%20and%20the%20Component%20Performance%20of%20a%20system%20as%0Aoverall%20autonomy%20score.%20We%20examine%20several%20methods%20of%20combining%20features%2C%0Ashowing%20how%20some%20methods%20find%20different%20rankings%20of%20the%20same%20data%2C%20and%20we%0Aemploy%20the%20weighted%20product%20method%20to%20resolve%20this%20issue.%20Furthermore%2C%20we%0Aintroduce%20the%20non-contextual%20autonomy%20coordinate%20and%20represent%20the%20overall%0Aautonomy%20of%20a%20system%20with%20an%20autonomy%20distance.%20We%20apply%20our%20method%20to%20a%20set%20of%0Aseven%20Unmanned%20Aerial%20Systems%20%28UAS%29%20and%20obtain%20their%20absolute%20autonomy%20score%20as%0Awell%20as%20their%20relative%20score%20with%20respect%20to%20the%20best%20system.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2111.07438v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMethods%2520for%2520Combining%2520and%2520Representing%2520Non-Contextual%2520Autonomy%2520Scores%250A%2520%2520for%2520Unmanned%2520Aerial%2520Systems%26entry.906535625%3DBrendan%2520Hertel%2520and%2520Ryan%2520Donald%2520and%2520Christian%2520Dumas%2520and%2520S.%2520Reza%2520Ahmadzadeh%26entry.1292438233%3D%2520%2520Measuring%2520an%2520overall%2520autonomy%2520score%2520for%2520a%2520robotic%2520system%2520requires%2520the%250Acombination%2520of%2520a%2520set%2520of%2520relevant%2520aspects%2520and%2520features%2520of%2520the%2520system%2520that%2520might%250Abe%2520measured%2520in%2520different%2520units%252C%2520qualitative%252C%2520and/or%2520discordant.%2520In%2520this%2520paper%252C%250Awe%2520build%2520upon%2520an%2520existing%2520non-contextual%2520autonomy%2520framework%2520that%2520measures%2520and%250Acombines%2520the%2520Autonomy%2520Level%2520and%2520the%2520Component%2520Performance%2520of%2520a%2520system%2520as%250Aoverall%2520autonomy%2520score.%2520We%2520examine%2520several%2520methods%2520of%2520combining%2520features%252C%250Ashowing%2520how%2520some%2520methods%2520find%2520different%2520rankings%2520of%2520the%2520same%2520data%252C%2520and%2520we%250Aemploy%2520the%2520weighted%2520product%2520method%2520to%2520resolve%2520this%2520issue.%2520Furthermore%252C%2520we%250Aintroduce%2520the%2520non-contextual%2520autonomy%2520coordinate%2520and%2520represent%2520the%2520overall%250Aautonomy%2520of%2520a%2520system%2520with%2520an%2520autonomy%2520distance.%2520We%2520apply%2520our%2520method%2520to%2520a%2520set%2520of%250Aseven%2520Unmanned%2520Aerial%2520Systems%2520%2528UAS%2529%2520and%2520obtain%2520their%2520absolute%2520autonomy%2520score%2520as%250Awell%2520as%2520their%2520relative%2520score%2520with%2520respect%2520to%2520the%2520best%2520system.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2111.07438v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Methods%20for%20Combining%20and%20Representing%20Non-Contextual%20Autonomy%20Scores%0A%20%20for%20Unmanned%20Aerial%20Systems&entry.906535625=Brendan%20Hertel%20and%20Ryan%20Donald%20and%20Christian%20Dumas%20and%20S.%20Reza%20Ahmadzadeh&entry.1292438233=%20%20Measuring%20an%20overall%20autonomy%20score%20for%20a%20robotic%20system%20requires%20the%0Acombination%20of%20a%20set%20of%20relevant%20aspects%20and%20features%20of%20the%20system%20that%20might%0Abe%20measured%20in%20different%20units%2C%20qualitative%2C%20and/or%20discordant.%20In%20this%20paper%2C%0Awe%20build%20upon%20an%20existing%20non-contextual%20autonomy%20framework%20that%20measures%20and%0Acombines%20the%20Autonomy%20Level%20and%20the%20Component%20Performance%20of%20a%20system%20as%0Aoverall%20autonomy%20score.%20We%20examine%20several%20methods%20of%20combining%20features%2C%0Ashowing%20how%20some%20methods%20find%20different%20rankings%20of%20the%20same%20data%2C%20and%20we%0Aemploy%20the%20weighted%20product%20method%20to%20resolve%20this%20issue.%20Furthermore%2C%20we%0Aintroduce%20the%20non-contextual%20autonomy%20coordinate%20and%20represent%20the%20overall%0Aautonomy%20of%20a%20system%20with%20an%20autonomy%20distance.%20We%20apply%20our%20method%20to%20a%20set%20of%0Aseven%20Unmanned%20Aerial%20Systems%20%28UAS%29%20and%20obtain%20their%20absolute%20autonomy%20score%20as%0Awell%20as%20their%20relative%20score%20with%20respect%20to%20the%20best%20system.%0A&entry.1838667208=http%3A//arxiv.org/abs/2111.07438v2&entry.124074799=Read"},
{"title": "On the Trade-off between Flatness and Optimization in Distributed\n  Learning", "author": "Ying Cao and Zhaoxian Wu and Kun Yuan and Ali H. Sayed", "abstract": "  This paper proposes a theoretical framework to evaluate and compare the\nperformance of gradient-descent algorithms for distributed learning in relation\nto their behavior around local minima in nonconvex environments. Previous works\nhave noticed that convergence toward flat local minima tend to enhance the\ngeneralization ability of learning algorithms. This work discovers two\ninteresting results. First, it shows that decentralized learning strategies are\nable to escape faster away from local minimizers and favor convergence toward\nflatter minima relative to the centralized solution in the large-batch training\nregime. Second, and importantly, the ultimate classification accuracy is not\nsolely dependent on the flatness of the local minimizer but also on how well a\nlearning algorithm can approach that minimum. In other words, the\nclassification accuracy is a function of both flatness and optimization\nperformance. The paper examines the interplay between the two measures of\nflatness and optimization error closely. One important conclusion is that\ndecentralized strategies of the diffusion type deliver enhanced classification\naccuracy because it strikes a more favorable balance between flatness and\noptimization performance.\n", "link": "http://arxiv.org/abs/2406.20006v1", "date": "2024-06-28", "relevancy": 2.0023, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5075}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4993}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Trade-off%20between%20Flatness%20and%20Optimization%20in%20Distributed%0A%20%20Learning&body=Title%3A%20On%20the%20Trade-off%20between%20Flatness%20and%20Optimization%20in%20Distributed%0A%20%20Learning%0AAuthor%3A%20Ying%20Cao%20and%20Zhaoxian%20Wu%20and%20Kun%20Yuan%20and%20Ali%20H.%20Sayed%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20theoretical%20framework%20to%20evaluate%20and%20compare%20the%0Aperformance%20of%20gradient-descent%20algorithms%20for%20distributed%20learning%20in%20relation%0Ato%20their%20behavior%20around%20local%20minima%20in%20nonconvex%20environments.%20Previous%20works%0Ahave%20noticed%20that%20convergence%20toward%20flat%20local%20minima%20tend%20to%20enhance%20the%0Ageneralization%20ability%20of%20learning%20algorithms.%20This%20work%20discovers%20two%0Ainteresting%20results.%20First%2C%20it%20shows%20that%20decentralized%20learning%20strategies%20are%0Aable%20to%20escape%20faster%20away%20from%20local%20minimizers%20and%20favor%20convergence%20toward%0Aflatter%20minima%20relative%20to%20the%20centralized%20solution%20in%20the%20large-batch%20training%0Aregime.%20Second%2C%20and%20importantly%2C%20the%20ultimate%20classification%20accuracy%20is%20not%0Asolely%20dependent%20on%20the%20flatness%20of%20the%20local%20minimizer%20but%20also%20on%20how%20well%20a%0Alearning%20algorithm%20can%20approach%20that%20minimum.%20In%20other%20words%2C%20the%0Aclassification%20accuracy%20is%20a%20function%20of%20both%20flatness%20and%20optimization%0Aperformance.%20The%20paper%20examines%20the%20interplay%20between%20the%20two%20measures%20of%0Aflatness%20and%20optimization%20error%20closely.%20One%20important%20conclusion%20is%20that%0Adecentralized%20strategies%20of%20the%20diffusion%20type%20deliver%20enhanced%20classification%0Aaccuracy%20because%20it%20strikes%20a%20more%20favorable%20balance%20between%20flatness%20and%0Aoptimization%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.20006v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Trade-off%2520between%2520Flatness%2520and%2520Optimization%2520in%2520Distributed%250A%2520%2520Learning%26entry.906535625%3DYing%2520Cao%2520and%2520Zhaoxian%2520Wu%2520and%2520Kun%2520Yuan%2520and%2520Ali%2520H.%2520Sayed%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520theoretical%2520framework%2520to%2520evaluate%2520and%2520compare%2520the%250Aperformance%2520of%2520gradient-descent%2520algorithms%2520for%2520distributed%2520learning%2520in%2520relation%250Ato%2520their%2520behavior%2520around%2520local%2520minima%2520in%2520nonconvex%2520environments.%2520Previous%2520works%250Ahave%2520noticed%2520that%2520convergence%2520toward%2520flat%2520local%2520minima%2520tend%2520to%2520enhance%2520the%250Ageneralization%2520ability%2520of%2520learning%2520algorithms.%2520This%2520work%2520discovers%2520two%250Ainteresting%2520results.%2520First%252C%2520it%2520shows%2520that%2520decentralized%2520learning%2520strategies%2520are%250Aable%2520to%2520escape%2520faster%2520away%2520from%2520local%2520minimizers%2520and%2520favor%2520convergence%2520toward%250Aflatter%2520minima%2520relative%2520to%2520the%2520centralized%2520solution%2520in%2520the%2520large-batch%2520training%250Aregime.%2520Second%252C%2520and%2520importantly%252C%2520the%2520ultimate%2520classification%2520accuracy%2520is%2520not%250Asolely%2520dependent%2520on%2520the%2520flatness%2520of%2520the%2520local%2520minimizer%2520but%2520also%2520on%2520how%2520well%2520a%250Alearning%2520algorithm%2520can%2520approach%2520that%2520minimum.%2520In%2520other%2520words%252C%2520the%250Aclassification%2520accuracy%2520is%2520a%2520function%2520of%2520both%2520flatness%2520and%2520optimization%250Aperformance.%2520The%2520paper%2520examines%2520the%2520interplay%2520between%2520the%2520two%2520measures%2520of%250Aflatness%2520and%2520optimization%2520error%2520closely.%2520One%2520important%2520conclusion%2520is%2520that%250Adecentralized%2520strategies%2520of%2520the%2520diffusion%2520type%2520deliver%2520enhanced%2520classification%250Aaccuracy%2520because%2520it%2520strikes%2520a%2520more%2520favorable%2520balance%2520between%2520flatness%2520and%250Aoptimization%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.20006v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Trade-off%20between%20Flatness%20and%20Optimization%20in%20Distributed%0A%20%20Learning&entry.906535625=Ying%20Cao%20and%20Zhaoxian%20Wu%20and%20Kun%20Yuan%20and%20Ali%20H.%20Sayed&entry.1292438233=%20%20This%20paper%20proposes%20a%20theoretical%20framework%20to%20evaluate%20and%20compare%20the%0Aperformance%20of%20gradient-descent%20algorithms%20for%20distributed%20learning%20in%20relation%0Ato%20their%20behavior%20around%20local%20minima%20in%20nonconvex%20environments.%20Previous%20works%0Ahave%20noticed%20that%20convergence%20toward%20flat%20local%20minima%20tend%20to%20enhance%20the%0Ageneralization%20ability%20of%20learning%20algorithms.%20This%20work%20discovers%20two%0Ainteresting%20results.%20First%2C%20it%20shows%20that%20decentralized%20learning%20strategies%20are%0Aable%20to%20escape%20faster%20away%20from%20local%20minimizers%20and%20favor%20convergence%20toward%0Aflatter%20minima%20relative%20to%20the%20centralized%20solution%20in%20the%20large-batch%20training%0Aregime.%20Second%2C%20and%20importantly%2C%20the%20ultimate%20classification%20accuracy%20is%20not%0Asolely%20dependent%20on%20the%20flatness%20of%20the%20local%20minimizer%20but%20also%20on%20how%20well%20a%0Alearning%20algorithm%20can%20approach%20that%20minimum.%20In%20other%20words%2C%20the%0Aclassification%20accuracy%20is%20a%20function%20of%20both%20flatness%20and%20optimization%0Aperformance.%20The%20paper%20examines%20the%20interplay%20between%20the%20two%20measures%20of%0Aflatness%20and%20optimization%20error%20closely.%20One%20important%20conclusion%20is%20that%0Adecentralized%20strategies%20of%20the%20diffusion%20type%20deliver%20enhanced%20classification%0Aaccuracy%20because%20it%20strikes%20a%20more%20favorable%20balance%20between%20flatness%20and%0Aoptimization%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.20006v1&entry.124074799=Read"},
{"title": "Evaluation of autonomous systems under data distribution shifts", "author": "Daniel Sikar and Artur Garcez", "abstract": "  We posit that data can only be safe to use up to a certain threshold of the\ndata distribution shift, after which control must be relinquished by the\nautonomous system and operation halted or handed to a human operator. With the\nuse of a computer vision toy example we demonstrate that network predictive\naccuracy is impacted by data distribution shifts and propose distance metrics\nbetween training and testing data to define safe operation limits within said\nshifts. We conclude that beyond an empirically obtained threshold of the data\ndistribution shift, it is unreasonable to expect network predictive accuracy\nnot to degrade\n", "link": "http://arxiv.org/abs/2406.20046v1", "date": "2024-06-28", "relevancy": 1.9951, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.51}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5003}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4928}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluation%20of%20autonomous%20systems%20under%20data%20distribution%20shifts&body=Title%3A%20Evaluation%20of%20autonomous%20systems%20under%20data%20distribution%20shifts%0AAuthor%3A%20Daniel%20Sikar%20and%20Artur%20Garcez%0AAbstract%3A%20%20%20We%20posit%20that%20data%20can%20only%20be%20safe%20to%20use%20up%20to%20a%20certain%20threshold%20of%20the%0Adata%20distribution%20shift%2C%20after%20which%20control%20must%20be%20relinquished%20by%20the%0Aautonomous%20system%20and%20operation%20halted%20or%20handed%20to%20a%20human%20operator.%20With%20the%0Ause%20of%20a%20computer%20vision%20toy%20example%20we%20demonstrate%20that%20network%20predictive%0Aaccuracy%20is%20impacted%20by%20data%20distribution%20shifts%20and%20propose%20distance%20metrics%0Abetween%20training%20and%20testing%20data%20to%20define%20safe%20operation%20limits%20within%20said%0Ashifts.%20We%20conclude%20that%20beyond%20an%20empirically%20obtained%20threshold%20of%20the%20data%0Adistribution%20shift%2C%20it%20is%20unreasonable%20to%20expect%20network%20predictive%20accuracy%0Anot%20to%20degrade%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.20046v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluation%2520of%2520autonomous%2520systems%2520under%2520data%2520distribution%2520shifts%26entry.906535625%3DDaniel%2520Sikar%2520and%2520Artur%2520Garcez%26entry.1292438233%3D%2520%2520We%2520posit%2520that%2520data%2520can%2520only%2520be%2520safe%2520to%2520use%2520up%2520to%2520a%2520certain%2520threshold%2520of%2520the%250Adata%2520distribution%2520shift%252C%2520after%2520which%2520control%2520must%2520be%2520relinquished%2520by%2520the%250Aautonomous%2520system%2520and%2520operation%2520halted%2520or%2520handed%2520to%2520a%2520human%2520operator.%2520With%2520the%250Ause%2520of%2520a%2520computer%2520vision%2520toy%2520example%2520we%2520demonstrate%2520that%2520network%2520predictive%250Aaccuracy%2520is%2520impacted%2520by%2520data%2520distribution%2520shifts%2520and%2520propose%2520distance%2520metrics%250Abetween%2520training%2520and%2520testing%2520data%2520to%2520define%2520safe%2520operation%2520limits%2520within%2520said%250Ashifts.%2520We%2520conclude%2520that%2520beyond%2520an%2520empirically%2520obtained%2520threshold%2520of%2520the%2520data%250Adistribution%2520shift%252C%2520it%2520is%2520unreasonable%2520to%2520expect%2520network%2520predictive%2520accuracy%250Anot%2520to%2520degrade%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.20046v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluation%20of%20autonomous%20systems%20under%20data%20distribution%20shifts&entry.906535625=Daniel%20Sikar%20and%20Artur%20Garcez&entry.1292438233=%20%20We%20posit%20that%20data%20can%20only%20be%20safe%20to%20use%20up%20to%20a%20certain%20threshold%20of%20the%0Adata%20distribution%20shift%2C%20after%20which%20control%20must%20be%20relinquished%20by%20the%0Aautonomous%20system%20and%20operation%20halted%20or%20handed%20to%20a%20human%20operator.%20With%20the%0Ause%20of%20a%20computer%20vision%20toy%20example%20we%20demonstrate%20that%20network%20predictive%0Aaccuracy%20is%20impacted%20by%20data%20distribution%20shifts%20and%20propose%20distance%20metrics%0Abetween%20training%20and%20testing%20data%20to%20define%20safe%20operation%20limits%20within%20said%0Ashifts.%20We%20conclude%20that%20beyond%20an%20empirically%20obtained%20threshold%20of%20the%20data%0Adistribution%20shift%2C%20it%20is%20unreasonable%20to%20expect%20network%20predictive%20accuracy%0Anot%20to%20degrade%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.20046v1&entry.124074799=Read"},
{"title": "BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity\n  Text Embeddings Through Self-Knowledge Distillation", "author": "Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu", "abstract": "  In this paper, we present a new embedding model, called M3-Embedding, which\nis distinguished for its versatility in Multi-Linguality, Multi-Functionality,\nand Multi-Granularity. It can support more than 100 working languages, leading\nto new state-of-the-art performances on multi-lingual and cross-lingual\nretrieval tasks. It can simultaneously perform the three common retrieval\nfunctionalities of embedding model: dense retrieval, multi-vector retrieval,\nand sparse retrieval, which provides a unified model foundation for real-world\nIR applications. It is able to process inputs of different granularities,\nspanning from short sentences to long documents of up to 8192 tokens. The\neffective training of M3-Embedding involves the following technical\ncontributions. We propose a novel self-knowledge distillation approach, where\nthe relevance scores from different retrieval functionalities can be integrated\nas the teacher signal to enhance the training quality. We also optimize the\nbatching strategy, enabling a large batch size and high training throughput to\nensure the discriminativeness of embeddings. To the best of our knowledge,\nM3-Embedding is the first embedding model which realizes such a strong\nversatility. The model and code will be publicly available at\nhttps://github.com/FlagOpen/FlagEmbedding.\n", "link": "http://arxiv.org/abs/2402.03216v4", "date": "2024-06-28", "relevancy": 1.9825, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5118}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5015}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4771}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BGE%20M3-Embedding%3A%20Multi-Lingual%2C%20Multi-Functionality%2C%20Multi-Granularity%0A%20%20Text%20Embeddings%20Through%20Self-Knowledge%20Distillation&body=Title%3A%20BGE%20M3-Embedding%3A%20Multi-Lingual%2C%20Multi-Functionality%2C%20Multi-Granularity%0A%20%20Text%20Embeddings%20Through%20Self-Knowledge%20Distillation%0AAuthor%3A%20Jianlv%20Chen%20and%20Shitao%20Xiao%20and%20Peitian%20Zhang%20and%20Kun%20Luo%20and%20Defu%20Lian%20and%20Zheng%20Liu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20new%20embedding%20model%2C%20called%20M3-Embedding%2C%20which%0Ais%20distinguished%20for%20its%20versatility%20in%20Multi-Linguality%2C%20Multi-Functionality%2C%0Aand%20Multi-Granularity.%20It%20can%20support%20more%20than%20100%20working%20languages%2C%20leading%0Ato%20new%20state-of-the-art%20performances%20on%20multi-lingual%20and%20cross-lingual%0Aretrieval%20tasks.%20It%20can%20simultaneously%20perform%20the%20three%20common%20retrieval%0Afunctionalities%20of%20embedding%20model%3A%20dense%20retrieval%2C%20multi-vector%20retrieval%2C%0Aand%20sparse%20retrieval%2C%20which%20provides%20a%20unified%20model%20foundation%20for%20real-world%0AIR%20applications.%20It%20is%20able%20to%20process%20inputs%20of%20different%20granularities%2C%0Aspanning%20from%20short%20sentences%20to%20long%20documents%20of%20up%20to%208192%20tokens.%20The%0Aeffective%20training%20of%20M3-Embedding%20involves%20the%20following%20technical%0Acontributions.%20We%20propose%20a%20novel%20self-knowledge%20distillation%20approach%2C%20where%0Athe%20relevance%20scores%20from%20different%20retrieval%20functionalities%20can%20be%20integrated%0Aas%20the%20teacher%20signal%20to%20enhance%20the%20training%20quality.%20We%20also%20optimize%20the%0Abatching%20strategy%2C%20enabling%20a%20large%20batch%20size%20and%20high%20training%20throughput%20to%0Aensure%20the%20discriminativeness%20of%20embeddings.%20To%20the%20best%20of%20our%20knowledge%2C%0AM3-Embedding%20is%20the%20first%20embedding%20model%20which%20realizes%20such%20a%20strong%0Aversatility.%20The%20model%20and%20code%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/FlagOpen/FlagEmbedding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03216v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBGE%2520M3-Embedding%253A%2520Multi-Lingual%252C%2520Multi-Functionality%252C%2520Multi-Granularity%250A%2520%2520Text%2520Embeddings%2520Through%2520Self-Knowledge%2520Distillation%26entry.906535625%3DJianlv%2520Chen%2520and%2520Shitao%2520Xiao%2520and%2520Peitian%2520Zhang%2520and%2520Kun%2520Luo%2520and%2520Defu%2520Lian%2520and%2520Zheng%2520Liu%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520new%2520embedding%2520model%252C%2520called%2520M3-Embedding%252C%2520which%250Ais%2520distinguished%2520for%2520its%2520versatility%2520in%2520Multi-Linguality%252C%2520Multi-Functionality%252C%250Aand%2520Multi-Granularity.%2520It%2520can%2520support%2520more%2520than%2520100%2520working%2520languages%252C%2520leading%250Ato%2520new%2520state-of-the-art%2520performances%2520on%2520multi-lingual%2520and%2520cross-lingual%250Aretrieval%2520tasks.%2520It%2520can%2520simultaneously%2520perform%2520the%2520three%2520common%2520retrieval%250Afunctionalities%2520of%2520embedding%2520model%253A%2520dense%2520retrieval%252C%2520multi-vector%2520retrieval%252C%250Aand%2520sparse%2520retrieval%252C%2520which%2520provides%2520a%2520unified%2520model%2520foundation%2520for%2520real-world%250AIR%2520applications.%2520It%2520is%2520able%2520to%2520process%2520inputs%2520of%2520different%2520granularities%252C%250Aspanning%2520from%2520short%2520sentences%2520to%2520long%2520documents%2520of%2520up%2520to%25208192%2520tokens.%2520The%250Aeffective%2520training%2520of%2520M3-Embedding%2520involves%2520the%2520following%2520technical%250Acontributions.%2520We%2520propose%2520a%2520novel%2520self-knowledge%2520distillation%2520approach%252C%2520where%250Athe%2520relevance%2520scores%2520from%2520different%2520retrieval%2520functionalities%2520can%2520be%2520integrated%250Aas%2520the%2520teacher%2520signal%2520to%2520enhance%2520the%2520training%2520quality.%2520We%2520also%2520optimize%2520the%250Abatching%2520strategy%252C%2520enabling%2520a%2520large%2520batch%2520size%2520and%2520high%2520training%2520throughput%2520to%250Aensure%2520the%2520discriminativeness%2520of%2520embeddings.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%250AM3-Embedding%2520is%2520the%2520first%2520embedding%2520model%2520which%2520realizes%2520such%2520a%2520strong%250Aversatility.%2520The%2520model%2520and%2520code%2520will%2520be%2520publicly%2520available%2520at%250Ahttps%253A//github.com/FlagOpen/FlagEmbedding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03216v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BGE%20M3-Embedding%3A%20Multi-Lingual%2C%20Multi-Functionality%2C%20Multi-Granularity%0A%20%20Text%20Embeddings%20Through%20Self-Knowledge%20Distillation&entry.906535625=Jianlv%20Chen%20and%20Shitao%20Xiao%20and%20Peitian%20Zhang%20and%20Kun%20Luo%20and%20Defu%20Lian%20and%20Zheng%20Liu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20new%20embedding%20model%2C%20called%20M3-Embedding%2C%20which%0Ais%20distinguished%20for%20its%20versatility%20in%20Multi-Linguality%2C%20Multi-Functionality%2C%0Aand%20Multi-Granularity.%20It%20can%20support%20more%20than%20100%20working%20languages%2C%20leading%0Ato%20new%20state-of-the-art%20performances%20on%20multi-lingual%20and%20cross-lingual%0Aretrieval%20tasks.%20It%20can%20simultaneously%20perform%20the%20three%20common%20retrieval%0Afunctionalities%20of%20embedding%20model%3A%20dense%20retrieval%2C%20multi-vector%20retrieval%2C%0Aand%20sparse%20retrieval%2C%20which%20provides%20a%20unified%20model%20foundation%20for%20real-world%0AIR%20applications.%20It%20is%20able%20to%20process%20inputs%20of%20different%20granularities%2C%0Aspanning%20from%20short%20sentences%20to%20long%20documents%20of%20up%20to%208192%20tokens.%20The%0Aeffective%20training%20of%20M3-Embedding%20involves%20the%20following%20technical%0Acontributions.%20We%20propose%20a%20novel%20self-knowledge%20distillation%20approach%2C%20where%0Athe%20relevance%20scores%20from%20different%20retrieval%20functionalities%20can%20be%20integrated%0Aas%20the%20teacher%20signal%20to%20enhance%20the%20training%20quality.%20We%20also%20optimize%20the%0Abatching%20strategy%2C%20enabling%20a%20large%20batch%20size%20and%20high%20training%20throughput%20to%0Aensure%20the%20discriminativeness%20of%20embeddings.%20To%20the%20best%20of%20our%20knowledge%2C%0AM3-Embedding%20is%20the%20first%20embedding%20model%20which%20realizes%20such%20a%20strong%0Aversatility.%20The%20model%20and%20code%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/FlagOpen/FlagEmbedding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03216v4&entry.124074799=Read"},
{"title": "InfiniBench: A Comprehensive Benchmark for Large Multimodal Models in\n  Very Long Video Understanding", "author": "Kirolos Ataallah and Chenhui Gou and Eslam Abdelrahman and Khushbu Pahwa and Jian Ding and Mohamed Elhoseiny", "abstract": "  Understanding long videos, ranging from tens of minutes to several hours,\npresents unique challenges in video comprehension. Despite the increasing\nimportance of long-form video content, existing benchmarks primarily focus on\nshorter clips. To address this gap, we introduce InfiniBench a comprehensive\nbenchmark for very long video understanding which presents 1)The longest video\nduration, averaging 76.34 minutes; 2) The largest number of question-answer\npairs, 108.2K; 3) Diversity in questions that examine nine different skills and\ninclude both multiple-choice questions and open-ended questions; 4)\nHumancentric, as the video sources come from movies and daily TV shows, with\nspecific human-level question designs such as Movie Spoiler Questions that\nrequire critical thinking and comprehensive understanding. Using InfiniBench,\nwe comprehensively evaluate existing Large MultiModality Models (LMMs) on each\nskill, including the commercial model Gemini 1.5 Flash and the open-source\nmodels. The evaluation shows significant challenges in our benchmark.Our\nresults show that the best AI models such Gemini struggles to perform well with\n42.72% average accuracy and 2.71 out of 5 average score. We hope this benchmark\nwill stimulate the LMMs community towards long video and human-level\nunderstanding. Our benchmark can be accessed at\nhttps://vision-cair.github.io/InfiniBench/\n", "link": "http://arxiv.org/abs/2406.19875v1", "date": "2024-06-28", "relevancy": 1.9822, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.502}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4947}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4938}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InfiniBench%3A%20A%20Comprehensive%20Benchmark%20for%20Large%20Multimodal%20Models%20in%0A%20%20Very%20Long%20Video%20Understanding&body=Title%3A%20InfiniBench%3A%20A%20Comprehensive%20Benchmark%20for%20Large%20Multimodal%20Models%20in%0A%20%20Very%20Long%20Video%20Understanding%0AAuthor%3A%20Kirolos%20Ataallah%20and%20Chenhui%20Gou%20and%20Eslam%20Abdelrahman%20and%20Khushbu%20Pahwa%20and%20Jian%20Ding%20and%20Mohamed%20Elhoseiny%0AAbstract%3A%20%20%20Understanding%20long%20videos%2C%20ranging%20from%20tens%20of%20minutes%20to%20several%20hours%2C%0Apresents%20unique%20challenges%20in%20video%20comprehension.%20Despite%20the%20increasing%0Aimportance%20of%20long-form%20video%20content%2C%20existing%20benchmarks%20primarily%20focus%20on%0Ashorter%20clips.%20To%20address%20this%20gap%2C%20we%20introduce%20InfiniBench%20a%20comprehensive%0Abenchmark%20for%20very%20long%20video%20understanding%20which%20presents%201%29The%20longest%20video%0Aduration%2C%20averaging%2076.34%20minutes%3B%202%29%20The%20largest%20number%20of%20question-answer%0Apairs%2C%20108.2K%3B%203%29%20Diversity%20in%20questions%20that%20examine%20nine%20different%20skills%20and%0Ainclude%20both%20multiple-choice%20questions%20and%20open-ended%20questions%3B%204%29%0AHumancentric%2C%20as%20the%20video%20sources%20come%20from%20movies%20and%20daily%20TV%20shows%2C%20with%0Aspecific%20human-level%20question%20designs%20such%20as%20Movie%20Spoiler%20Questions%20that%0Arequire%20critical%20thinking%20and%20comprehensive%20understanding.%20Using%20InfiniBench%2C%0Awe%20comprehensively%20evaluate%20existing%20Large%20MultiModality%20Models%20%28LMMs%29%20on%20each%0Askill%2C%20including%20the%20commercial%20model%20Gemini%201.5%20Flash%20and%20the%20open-source%0Amodels.%20The%20evaluation%20shows%20significant%20challenges%20in%20our%20benchmark.Our%0Aresults%20show%20that%20the%20best%20AI%20models%20such%20Gemini%20struggles%20to%20perform%20well%20with%0A42.72%25%20average%20accuracy%20and%202.71%20out%20of%205%20average%20score.%20We%20hope%20this%20benchmark%0Awill%20stimulate%20the%20LMMs%20community%20towards%20long%20video%20and%20human-level%0Aunderstanding.%20Our%20benchmark%20can%20be%20accessed%20at%0Ahttps%3A//vision-cair.github.io/InfiniBench/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19875v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfiniBench%253A%2520A%2520Comprehensive%2520Benchmark%2520for%2520Large%2520Multimodal%2520Models%2520in%250A%2520%2520Very%2520Long%2520Video%2520Understanding%26entry.906535625%3DKirolos%2520Ataallah%2520and%2520Chenhui%2520Gou%2520and%2520Eslam%2520Abdelrahman%2520and%2520Khushbu%2520Pahwa%2520and%2520Jian%2520Ding%2520and%2520Mohamed%2520Elhoseiny%26entry.1292438233%3D%2520%2520Understanding%2520long%2520videos%252C%2520ranging%2520from%2520tens%2520of%2520minutes%2520to%2520several%2520hours%252C%250Apresents%2520unique%2520challenges%2520in%2520video%2520comprehension.%2520Despite%2520the%2520increasing%250Aimportance%2520of%2520long-form%2520video%2520content%252C%2520existing%2520benchmarks%2520primarily%2520focus%2520on%250Ashorter%2520clips.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520InfiniBench%2520a%2520comprehensive%250Abenchmark%2520for%2520very%2520long%2520video%2520understanding%2520which%2520presents%25201%2529The%2520longest%2520video%250Aduration%252C%2520averaging%252076.34%2520minutes%253B%25202%2529%2520The%2520largest%2520number%2520of%2520question-answer%250Apairs%252C%2520108.2K%253B%25203%2529%2520Diversity%2520in%2520questions%2520that%2520examine%2520nine%2520different%2520skills%2520and%250Ainclude%2520both%2520multiple-choice%2520questions%2520and%2520open-ended%2520questions%253B%25204%2529%250AHumancentric%252C%2520as%2520the%2520video%2520sources%2520come%2520from%2520movies%2520and%2520daily%2520TV%2520shows%252C%2520with%250Aspecific%2520human-level%2520question%2520designs%2520such%2520as%2520Movie%2520Spoiler%2520Questions%2520that%250Arequire%2520critical%2520thinking%2520and%2520comprehensive%2520understanding.%2520Using%2520InfiniBench%252C%250Awe%2520comprehensively%2520evaluate%2520existing%2520Large%2520MultiModality%2520Models%2520%2528LMMs%2529%2520on%2520each%250Askill%252C%2520including%2520the%2520commercial%2520model%2520Gemini%25201.5%2520Flash%2520and%2520the%2520open-source%250Amodels.%2520The%2520evaluation%2520shows%2520significant%2520challenges%2520in%2520our%2520benchmark.Our%250Aresults%2520show%2520that%2520the%2520best%2520AI%2520models%2520such%2520Gemini%2520struggles%2520to%2520perform%2520well%2520with%250A42.72%2525%2520average%2520accuracy%2520and%25202.71%2520out%2520of%25205%2520average%2520score.%2520We%2520hope%2520this%2520benchmark%250Awill%2520stimulate%2520the%2520LMMs%2520community%2520towards%2520long%2520video%2520and%2520human-level%250Aunderstanding.%2520Our%2520benchmark%2520can%2520be%2520accessed%2520at%250Ahttps%253A//vision-cair.github.io/InfiniBench/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19875v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InfiniBench%3A%20A%20Comprehensive%20Benchmark%20for%20Large%20Multimodal%20Models%20in%0A%20%20Very%20Long%20Video%20Understanding&entry.906535625=Kirolos%20Ataallah%20and%20Chenhui%20Gou%20and%20Eslam%20Abdelrahman%20and%20Khushbu%20Pahwa%20and%20Jian%20Ding%20and%20Mohamed%20Elhoseiny&entry.1292438233=%20%20Understanding%20long%20videos%2C%20ranging%20from%20tens%20of%20minutes%20to%20several%20hours%2C%0Apresents%20unique%20challenges%20in%20video%20comprehension.%20Despite%20the%20increasing%0Aimportance%20of%20long-form%20video%20content%2C%20existing%20benchmarks%20primarily%20focus%20on%0Ashorter%20clips.%20To%20address%20this%20gap%2C%20we%20introduce%20InfiniBench%20a%20comprehensive%0Abenchmark%20for%20very%20long%20video%20understanding%20which%20presents%201%29The%20longest%20video%0Aduration%2C%20averaging%2076.34%20minutes%3B%202%29%20The%20largest%20number%20of%20question-answer%0Apairs%2C%20108.2K%3B%203%29%20Diversity%20in%20questions%20that%20examine%20nine%20different%20skills%20and%0Ainclude%20both%20multiple-choice%20questions%20and%20open-ended%20questions%3B%204%29%0AHumancentric%2C%20as%20the%20video%20sources%20come%20from%20movies%20and%20daily%20TV%20shows%2C%20with%0Aspecific%20human-level%20question%20designs%20such%20as%20Movie%20Spoiler%20Questions%20that%0Arequire%20critical%20thinking%20and%20comprehensive%20understanding.%20Using%20InfiniBench%2C%0Awe%20comprehensively%20evaluate%20existing%20Large%20MultiModality%20Models%20%28LMMs%29%20on%20each%0Askill%2C%20including%20the%20commercial%20model%20Gemini%201.5%20Flash%20and%20the%20open-source%0Amodels.%20The%20evaluation%20shows%20significant%20challenges%20in%20our%20benchmark.Our%0Aresults%20show%20that%20the%20best%20AI%20models%20such%20Gemini%20struggles%20to%20perform%20well%20with%0A42.72%25%20average%20accuracy%20and%202.71%20out%20of%205%20average%20score.%20We%20hope%20this%20benchmark%0Awill%20stimulate%20the%20LMMs%20community%20towards%20long%20video%20and%20human-level%0Aunderstanding.%20Our%20benchmark%20can%20be%20accessed%20at%0Ahttps%3A//vision-cair.github.io/InfiniBench/%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19875v1&entry.124074799=Read"},
{"title": "Odd-One-Out: Anomaly Detection by Comparing with Neighbors", "author": "Ankan Bhunia and Changjian Li and Hakan Bilen", "abstract": "  This paper introduces a novel anomaly detection (AD) problem that focuses on\nidentifying `odd-looking' objects relative to the other instances within a\nscene. Unlike the traditional AD benchmarks, in our setting, anomalies in this\ncontext are scene-specific, defined by the regular instances that make up the\nmajority. Since object instances are often partly visible from a single\nviewpoint, our setting provides multiple views of each scene as input. To\nprovide a testbed for future research in this task, we introduce two\nbenchmarks, ToysAD-8K and PartsAD-15K. We propose a novel method that generates\n3D object-centric representations for each instance and detects the anomalous\nones through a cross-examination between the instances. We rigorously analyze\nour method quantitatively and qualitatively in the presented benchmarks.\n", "link": "http://arxiv.org/abs/2406.20099v1", "date": "2024-06-28", "relevancy": 1.9512, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5019}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4793}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Odd-One-Out%3A%20Anomaly%20Detection%20by%20Comparing%20with%20Neighbors&body=Title%3A%20Odd-One-Out%3A%20Anomaly%20Detection%20by%20Comparing%20with%20Neighbors%0AAuthor%3A%20Ankan%20Bhunia%20and%20Changjian%20Li%20and%20Hakan%20Bilen%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20anomaly%20detection%20%28AD%29%20problem%20that%20focuses%20on%0Aidentifying%20%60odd-looking%27%20objects%20relative%20to%20the%20other%20instances%20within%20a%0Ascene.%20Unlike%20the%20traditional%20AD%20benchmarks%2C%20in%20our%20setting%2C%20anomalies%20in%20this%0Acontext%20are%20scene-specific%2C%20defined%20by%20the%20regular%20instances%20that%20make%20up%20the%0Amajority.%20Since%20object%20instances%20are%20often%20partly%20visible%20from%20a%20single%0Aviewpoint%2C%20our%20setting%20provides%20multiple%20views%20of%20each%20scene%20as%20input.%20To%0Aprovide%20a%20testbed%20for%20future%20research%20in%20this%20task%2C%20we%20introduce%20two%0Abenchmarks%2C%20ToysAD-8K%20and%20PartsAD-15K.%20We%20propose%20a%20novel%20method%20that%20generates%0A3D%20object-centric%20representations%20for%20each%20instance%20and%20detects%20the%20anomalous%0Aones%20through%20a%20cross-examination%20between%20the%20instances.%20We%20rigorously%20analyze%0Aour%20method%20quantitatively%20and%20qualitatively%20in%20the%20presented%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.20099v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOdd-One-Out%253A%2520Anomaly%2520Detection%2520by%2520Comparing%2520with%2520Neighbors%26entry.906535625%3DAnkan%2520Bhunia%2520and%2520Changjian%2520Li%2520and%2520Hakan%2520Bilen%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520anomaly%2520detection%2520%2528AD%2529%2520problem%2520that%2520focuses%2520on%250Aidentifying%2520%2560odd-looking%2527%2520objects%2520relative%2520to%2520the%2520other%2520instances%2520within%2520a%250Ascene.%2520Unlike%2520the%2520traditional%2520AD%2520benchmarks%252C%2520in%2520our%2520setting%252C%2520anomalies%2520in%2520this%250Acontext%2520are%2520scene-specific%252C%2520defined%2520by%2520the%2520regular%2520instances%2520that%2520make%2520up%2520the%250Amajority.%2520Since%2520object%2520instances%2520are%2520often%2520partly%2520visible%2520from%2520a%2520single%250Aviewpoint%252C%2520our%2520setting%2520provides%2520multiple%2520views%2520of%2520each%2520scene%2520as%2520input.%2520To%250Aprovide%2520a%2520testbed%2520for%2520future%2520research%2520in%2520this%2520task%252C%2520we%2520introduce%2520two%250Abenchmarks%252C%2520ToysAD-8K%2520and%2520PartsAD-15K.%2520We%2520propose%2520a%2520novel%2520method%2520that%2520generates%250A3D%2520object-centric%2520representations%2520for%2520each%2520instance%2520and%2520detects%2520the%2520anomalous%250Aones%2520through%2520a%2520cross-examination%2520between%2520the%2520instances.%2520We%2520rigorously%2520analyze%250Aour%2520method%2520quantitatively%2520and%2520qualitatively%2520in%2520the%2520presented%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.20099v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Odd-One-Out%3A%20Anomaly%20Detection%20by%20Comparing%20with%20Neighbors&entry.906535625=Ankan%20Bhunia%20and%20Changjian%20Li%20and%20Hakan%20Bilen&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20anomaly%20detection%20%28AD%29%20problem%20that%20focuses%20on%0Aidentifying%20%60odd-looking%27%20objects%20relative%20to%20the%20other%20instances%20within%20a%0Ascene.%20Unlike%20the%20traditional%20AD%20benchmarks%2C%20in%20our%20setting%2C%20anomalies%20in%20this%0Acontext%20are%20scene-specific%2C%20defined%20by%20the%20regular%20instances%20that%20make%20up%20the%0Amajority.%20Since%20object%20instances%20are%20often%20partly%20visible%20from%20a%20single%0Aviewpoint%2C%20our%20setting%20provides%20multiple%20views%20of%20each%20scene%20as%20input.%20To%0Aprovide%20a%20testbed%20for%20future%20research%20in%20this%20task%2C%20we%20introduce%20two%0Abenchmarks%2C%20ToysAD-8K%20and%20PartsAD-15K.%20We%20propose%20a%20novel%20method%20that%20generates%0A3D%20object-centric%20representations%20for%20each%20instance%20and%20detects%20the%20anomalous%0Aones%20through%20a%20cross-examination%20between%20the%20instances.%20We%20rigorously%20analyze%0Aour%20method%20quantitatively%20and%20qualitatively%20in%20the%20presented%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.20099v1&entry.124074799=Read"},
{"title": "Detecting Subtle Differences between Human and Model Languages Using\n  Spectrum of Relative Likelihood", "author": "Yang Xu and Yu Wang and Hao An and Zhichen Liu and Yongyuan Li", "abstract": "  Human and model-generated texts can be distinguished by examining the\nmagnitude of likelihood in language. However, it is becoming increasingly\ndifficult as language model's capabilities of generating human-like texts keep\nevolving. This study provides a new perspective by using the relative\nlikelihood values instead of absolute ones, and extracting useful features from\nthe spectrum-view of likelihood for the human-model text detection task. We\npropose a detection procedure with two classification methods, supervised and\nheuristic-based, respectively, which results in competitive performances with\nprevious zero-shot detection methods and a new state-of-the-art on short-text\ndetection. Our method can also reveal subtle differences between human and\nmodel languages, which find theoretical roots in psycholinguistics studies. Our\ncode is available at https://github.com/CLCS-SUSTech/FourierGPT\n", "link": "http://arxiv.org/abs/2406.19874v1", "date": "2024-06-28", "relevancy": 1.9397, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4914}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4883}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20Subtle%20Differences%20between%20Human%20and%20Model%20Languages%20Using%0A%20%20Spectrum%20of%20Relative%20Likelihood&body=Title%3A%20Detecting%20Subtle%20Differences%20between%20Human%20and%20Model%20Languages%20Using%0A%20%20Spectrum%20of%20Relative%20Likelihood%0AAuthor%3A%20Yang%20Xu%20and%20Yu%20Wang%20and%20Hao%20An%20and%20Zhichen%20Liu%20and%20Yongyuan%20Li%0AAbstract%3A%20%20%20Human%20and%20model-generated%20texts%20can%20be%20distinguished%20by%20examining%20the%0Amagnitude%20of%20likelihood%20in%20language.%20However%2C%20it%20is%20becoming%20increasingly%0Adifficult%20as%20language%20model%27s%20capabilities%20of%20generating%20human-like%20texts%20keep%0Aevolving.%20This%20study%20provides%20a%20new%20perspective%20by%20using%20the%20relative%0Alikelihood%20values%20instead%20of%20absolute%20ones%2C%20and%20extracting%20useful%20features%20from%0Athe%20spectrum-view%20of%20likelihood%20for%20the%20human-model%20text%20detection%20task.%20We%0Apropose%20a%20detection%20procedure%20with%20two%20classification%20methods%2C%20supervised%20and%0Aheuristic-based%2C%20respectively%2C%20which%20results%20in%20competitive%20performances%20with%0Aprevious%20zero-shot%20detection%20methods%20and%20a%20new%20state-of-the-art%20on%20short-text%0Adetection.%20Our%20method%20can%20also%20reveal%20subtle%20differences%20between%20human%20and%0Amodel%20languages%2C%20which%20find%20theoretical%20roots%20in%20psycholinguistics%20studies.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/CLCS-SUSTech/FourierGPT%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19874v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520Subtle%2520Differences%2520between%2520Human%2520and%2520Model%2520Languages%2520Using%250A%2520%2520Spectrum%2520of%2520Relative%2520Likelihood%26entry.906535625%3DYang%2520Xu%2520and%2520Yu%2520Wang%2520and%2520Hao%2520An%2520and%2520Zhichen%2520Liu%2520and%2520Yongyuan%2520Li%26entry.1292438233%3D%2520%2520Human%2520and%2520model-generated%2520texts%2520can%2520be%2520distinguished%2520by%2520examining%2520the%250Amagnitude%2520of%2520likelihood%2520in%2520language.%2520However%252C%2520it%2520is%2520becoming%2520increasingly%250Adifficult%2520as%2520language%2520model%2527s%2520capabilities%2520of%2520generating%2520human-like%2520texts%2520keep%250Aevolving.%2520This%2520study%2520provides%2520a%2520new%2520perspective%2520by%2520using%2520the%2520relative%250Alikelihood%2520values%2520instead%2520of%2520absolute%2520ones%252C%2520and%2520extracting%2520useful%2520features%2520from%250Athe%2520spectrum-view%2520of%2520likelihood%2520for%2520the%2520human-model%2520text%2520detection%2520task.%2520We%250Apropose%2520a%2520detection%2520procedure%2520with%2520two%2520classification%2520methods%252C%2520supervised%2520and%250Aheuristic-based%252C%2520respectively%252C%2520which%2520results%2520in%2520competitive%2520performances%2520with%250Aprevious%2520zero-shot%2520detection%2520methods%2520and%2520a%2520new%2520state-of-the-art%2520on%2520short-text%250Adetection.%2520Our%2520method%2520can%2520also%2520reveal%2520subtle%2520differences%2520between%2520human%2520and%250Amodel%2520languages%252C%2520which%2520find%2520theoretical%2520roots%2520in%2520psycholinguistics%2520studies.%2520Our%250Acode%2520is%2520available%2520at%2520https%253A//github.com/CLCS-SUSTech/FourierGPT%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19874v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20Subtle%20Differences%20between%20Human%20and%20Model%20Languages%20Using%0A%20%20Spectrum%20of%20Relative%20Likelihood&entry.906535625=Yang%20Xu%20and%20Yu%20Wang%20and%20Hao%20An%20and%20Zhichen%20Liu%20and%20Yongyuan%20Li&entry.1292438233=%20%20Human%20and%20model-generated%20texts%20can%20be%20distinguished%20by%20examining%20the%0Amagnitude%20of%20likelihood%20in%20language.%20However%2C%20it%20is%20becoming%20increasingly%0Adifficult%20as%20language%20model%27s%20capabilities%20of%20generating%20human-like%20texts%20keep%0Aevolving.%20This%20study%20provides%20a%20new%20perspective%20by%20using%20the%20relative%0Alikelihood%20values%20instead%20of%20absolute%20ones%2C%20and%20extracting%20useful%20features%20from%0Athe%20spectrum-view%20of%20likelihood%20for%20the%20human-model%20text%20detection%20task.%20We%0Apropose%20a%20detection%20procedure%20with%20two%20classification%20methods%2C%20supervised%20and%0Aheuristic-based%2C%20respectively%2C%20which%20results%20in%20competitive%20performances%20with%0Aprevious%20zero-shot%20detection%20methods%20and%20a%20new%20state-of-the-art%20on%20short-text%0Adetection.%20Our%20method%20can%20also%20reveal%20subtle%20differences%20between%20human%20and%0Amodel%20languages%2C%20which%20find%20theoretical%20roots%20in%20psycholinguistics%20studies.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/CLCS-SUSTech/FourierGPT%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19874v1&entry.124074799=Read"},
{"title": "Attention Meets UAVs: A Comprehensive Evaluation of DDoS Detection in\n  Low-Cost UAVs", "author": "Ashish Sharma and SVSLN Surya Suhas Vaddhiparthy and Sai Usha Goparaju and Deepak Gangadharan and Harikumar Kandath", "abstract": "  This paper explores the critical issue of enhancing cybersecurity measures\nfor low-cost, Wi-Fi-based Unmanned Aerial Vehicles (UAVs) against Distributed\nDenial of Service (DDoS) attacks. In the current work, we have explored three\nvariants of DDoS attacks, namely Transmission Control Protocol (TCP), Internet\nControl Message Protocol (ICMP), and TCP + ICMP flooding attacks, and developed\na detection mechanism that runs on the companion computer of the UAV system. As\na part of the detection mechanism, we have evaluated various machine learning,\nand deep learning algorithms, such as XGBoost, Isolation Forest, Long\nShort-Term Memory (LSTM), Bidirectional-LSTM (Bi-LSTM), LSTM with attention,\nBi-LSTM with attention, and Time Series Transformer (TST) in terms of various\nclassification metrics. Our evaluation reveals that algorithms with attention\nmechanisms outperform their counterparts in general, and TST stands out as the\nmost efficient model with a run time of 0.1 seconds. TST has demonstrated an F1\nscore of 0.999, 0.997, and 0.943 for TCP, ICMP, and TCP + ICMP flooding attacks\nrespectively. In this work, we present the necessary steps required to build an\non-board DDoS detection mechanism. Further, we also present the ablation study\nto identify the best TST hyperparameters for DDoS detection, and we have also\nunderscored the advantage of adapting learnable positional embeddings in TST\nfor DDoS detection with an improvement in F1 score from 0.94 to 0.99.\n", "link": "http://arxiv.org/abs/2406.19881v1", "date": "2024-06-28", "relevancy": 1.9317, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5081}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.479}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attention%20Meets%20UAVs%3A%20A%20Comprehensive%20Evaluation%20of%20DDoS%20Detection%20in%0A%20%20Low-Cost%20UAVs&body=Title%3A%20Attention%20Meets%20UAVs%3A%20A%20Comprehensive%20Evaluation%20of%20DDoS%20Detection%20in%0A%20%20Low-Cost%20UAVs%0AAuthor%3A%20Ashish%20Sharma%20and%20SVSLN%20Surya%20Suhas%20Vaddhiparthy%20and%20Sai%20Usha%20Goparaju%20and%20Deepak%20Gangadharan%20and%20Harikumar%20Kandath%0AAbstract%3A%20%20%20This%20paper%20explores%20the%20critical%20issue%20of%20enhancing%20cybersecurity%20measures%0Afor%20low-cost%2C%20Wi-Fi-based%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20against%20Distributed%0ADenial%20of%20Service%20%28DDoS%29%20attacks.%20In%20the%20current%20work%2C%20we%20have%20explored%20three%0Avariants%20of%20DDoS%20attacks%2C%20namely%20Transmission%20Control%20Protocol%20%28TCP%29%2C%20Internet%0AControl%20Message%20Protocol%20%28ICMP%29%2C%20and%20TCP%20%2B%20ICMP%20flooding%20attacks%2C%20and%20developed%0Aa%20detection%20mechanism%20that%20runs%20on%20the%20companion%20computer%20of%20the%20UAV%20system.%20As%0Aa%20part%20of%20the%20detection%20mechanism%2C%20we%20have%20evaluated%20various%20machine%20learning%2C%0Aand%20deep%20learning%20algorithms%2C%20such%20as%20XGBoost%2C%20Isolation%20Forest%2C%20Long%0AShort-Term%20Memory%20%28LSTM%29%2C%20Bidirectional-LSTM%20%28Bi-LSTM%29%2C%20LSTM%20with%20attention%2C%0ABi-LSTM%20with%20attention%2C%20and%20Time%20Series%20Transformer%20%28TST%29%20in%20terms%20of%20various%0Aclassification%20metrics.%20Our%20evaluation%20reveals%20that%20algorithms%20with%20attention%0Amechanisms%20outperform%20their%20counterparts%20in%20general%2C%20and%20TST%20stands%20out%20as%20the%0Amost%20efficient%20model%20with%20a%20run%20time%20of%200.1%20seconds.%20TST%20has%20demonstrated%20an%20F1%0Ascore%20of%200.999%2C%200.997%2C%20and%200.943%20for%20TCP%2C%20ICMP%2C%20and%20TCP%20%2B%20ICMP%20flooding%20attacks%0Arespectively.%20In%20this%20work%2C%20we%20present%20the%20necessary%20steps%20required%20to%20build%20an%0Aon-board%20DDoS%20detection%20mechanism.%20Further%2C%20we%20also%20present%20the%20ablation%20study%0Ato%20identify%20the%20best%20TST%20hyperparameters%20for%20DDoS%20detection%2C%20and%20we%20have%20also%0Aunderscored%20the%20advantage%20of%20adapting%20learnable%20positional%20embeddings%20in%20TST%0Afor%20DDoS%20detection%20with%20an%20improvement%20in%20F1%20score%20from%200.94%20to%200.99.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19881v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttention%2520Meets%2520UAVs%253A%2520A%2520Comprehensive%2520Evaluation%2520of%2520DDoS%2520Detection%2520in%250A%2520%2520Low-Cost%2520UAVs%26entry.906535625%3DAshish%2520Sharma%2520and%2520SVSLN%2520Surya%2520Suhas%2520Vaddhiparthy%2520and%2520Sai%2520Usha%2520Goparaju%2520and%2520Deepak%2520Gangadharan%2520and%2520Harikumar%2520Kandath%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520the%2520critical%2520issue%2520of%2520enhancing%2520cybersecurity%2520measures%250Afor%2520low-cost%252C%2520Wi-Fi-based%2520Unmanned%2520Aerial%2520Vehicles%2520%2528UAVs%2529%2520against%2520Distributed%250ADenial%2520of%2520Service%2520%2528DDoS%2529%2520attacks.%2520In%2520the%2520current%2520work%252C%2520we%2520have%2520explored%2520three%250Avariants%2520of%2520DDoS%2520attacks%252C%2520namely%2520Transmission%2520Control%2520Protocol%2520%2528TCP%2529%252C%2520Internet%250AControl%2520Message%2520Protocol%2520%2528ICMP%2529%252C%2520and%2520TCP%2520%252B%2520ICMP%2520flooding%2520attacks%252C%2520and%2520developed%250Aa%2520detection%2520mechanism%2520that%2520runs%2520on%2520the%2520companion%2520computer%2520of%2520the%2520UAV%2520system.%2520As%250Aa%2520part%2520of%2520the%2520detection%2520mechanism%252C%2520we%2520have%2520evaluated%2520various%2520machine%2520learning%252C%250Aand%2520deep%2520learning%2520algorithms%252C%2520such%2520as%2520XGBoost%252C%2520Isolation%2520Forest%252C%2520Long%250AShort-Term%2520Memory%2520%2528LSTM%2529%252C%2520Bidirectional-LSTM%2520%2528Bi-LSTM%2529%252C%2520LSTM%2520with%2520attention%252C%250ABi-LSTM%2520with%2520attention%252C%2520and%2520Time%2520Series%2520Transformer%2520%2528TST%2529%2520in%2520terms%2520of%2520various%250Aclassification%2520metrics.%2520Our%2520evaluation%2520reveals%2520that%2520algorithms%2520with%2520attention%250Amechanisms%2520outperform%2520their%2520counterparts%2520in%2520general%252C%2520and%2520TST%2520stands%2520out%2520as%2520the%250Amost%2520efficient%2520model%2520with%2520a%2520run%2520time%2520of%25200.1%2520seconds.%2520TST%2520has%2520demonstrated%2520an%2520F1%250Ascore%2520of%25200.999%252C%25200.997%252C%2520and%25200.943%2520for%2520TCP%252C%2520ICMP%252C%2520and%2520TCP%2520%252B%2520ICMP%2520flooding%2520attacks%250Arespectively.%2520In%2520this%2520work%252C%2520we%2520present%2520the%2520necessary%2520steps%2520required%2520to%2520build%2520an%250Aon-board%2520DDoS%2520detection%2520mechanism.%2520Further%252C%2520we%2520also%2520present%2520the%2520ablation%2520study%250Ato%2520identify%2520the%2520best%2520TST%2520hyperparameters%2520for%2520DDoS%2520detection%252C%2520and%2520we%2520have%2520also%250Aunderscored%2520the%2520advantage%2520of%2520adapting%2520learnable%2520positional%2520embeddings%2520in%2520TST%250Afor%2520DDoS%2520detection%2520with%2520an%2520improvement%2520in%2520F1%2520score%2520from%25200.94%2520to%25200.99.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19881v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention%20Meets%20UAVs%3A%20A%20Comprehensive%20Evaluation%20of%20DDoS%20Detection%20in%0A%20%20Low-Cost%20UAVs&entry.906535625=Ashish%20Sharma%20and%20SVSLN%20Surya%20Suhas%20Vaddhiparthy%20and%20Sai%20Usha%20Goparaju%20and%20Deepak%20Gangadharan%20and%20Harikumar%20Kandath&entry.1292438233=%20%20This%20paper%20explores%20the%20critical%20issue%20of%20enhancing%20cybersecurity%20measures%0Afor%20low-cost%2C%20Wi-Fi-based%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20against%20Distributed%0ADenial%20of%20Service%20%28DDoS%29%20attacks.%20In%20the%20current%20work%2C%20we%20have%20explored%20three%0Avariants%20of%20DDoS%20attacks%2C%20namely%20Transmission%20Control%20Protocol%20%28TCP%29%2C%20Internet%0AControl%20Message%20Protocol%20%28ICMP%29%2C%20and%20TCP%20%2B%20ICMP%20flooding%20attacks%2C%20and%20developed%0Aa%20detection%20mechanism%20that%20runs%20on%20the%20companion%20computer%20of%20the%20UAV%20system.%20As%0Aa%20part%20of%20the%20detection%20mechanism%2C%20we%20have%20evaluated%20various%20machine%20learning%2C%0Aand%20deep%20learning%20algorithms%2C%20such%20as%20XGBoost%2C%20Isolation%20Forest%2C%20Long%0AShort-Term%20Memory%20%28LSTM%29%2C%20Bidirectional-LSTM%20%28Bi-LSTM%29%2C%20LSTM%20with%20attention%2C%0ABi-LSTM%20with%20attention%2C%20and%20Time%20Series%20Transformer%20%28TST%29%20in%20terms%20of%20various%0Aclassification%20metrics.%20Our%20evaluation%20reveals%20that%20algorithms%20with%20attention%0Amechanisms%20outperform%20their%20counterparts%20in%20general%2C%20and%20TST%20stands%20out%20as%20the%0Amost%20efficient%20model%20with%20a%20run%20time%20of%200.1%20seconds.%20TST%20has%20demonstrated%20an%20F1%0Ascore%20of%200.999%2C%200.997%2C%20and%200.943%20for%20TCP%2C%20ICMP%2C%20and%20TCP%20%2B%20ICMP%20flooding%20attacks%0Arespectively.%20In%20this%20work%2C%20we%20present%20the%20necessary%20steps%20required%20to%20build%20an%0Aon-board%20DDoS%20detection%20mechanism.%20Further%2C%20we%20also%20present%20the%20ablation%20study%0Ato%20identify%20the%20best%20TST%20hyperparameters%20for%20DDoS%20detection%2C%20and%20we%20have%20also%0Aunderscored%20the%20advantage%20of%20adapting%20learnable%20positional%20embeddings%20in%20TST%0Afor%20DDoS%20detection%20with%20an%20improvement%20in%20F1%20score%20from%200.94%20to%200.99.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19881v1&entry.124074799=Read"},
{"title": "On the Value of PHH3 for Mitotic Figure Detection on H&E-stained Images", "author": "Jonathan Ganz and Christian Marzahl and Jonas Ammeling and Barbara Richter and Chlo\u00e9 Puget and Daniela Denk and Elena A. Demeter and Flaviu A. Tabaran and Gabriel Wasinger and Karoline Lipnik and Marco Tecilla and Matthew J. Valentine and Michael J. Dark and Niklas Abele and Pompei Bolfa and Ramona Erber and Robert Klopfleisch and Sophie Merz and Taryn A. Donovan and Samir Jabari and Christof A. Bertram and Katharina Breininger and Marc Aubreville", "abstract": "  The count of mitotic figures (MFs) observed in hematoxylin and eosin\n(H&E)-stained slides is an important prognostic marker as it is a measure for\ntumor cell proliferation. However, the identification of MFs has a known low\ninter-rater agreement. Deep learning algorithms can standardize this task, but\nthey require large amounts of annotated data for training and validation.\nFurthermore, label noise introduced during the annotation process may impede\nthe algorithm's performance. Unlike H&E, the mitosis-specific antibody\nphospho-histone H3 (PHH3) specifically highlights MFs. Counting MFs on slides\nstained against PHH3 leads to higher agreement among raters and has therefore\nrecently been used as a ground truth for the annotation of MFs in H&E. However,\nas PHH3 facilitates the recognition of cells indistinguishable from H&E stain\nalone, the use of this ground truth could potentially introduce noise into the\nH&E-related dataset, impacting model performance. This study analyzes the\nimpact of PHH3-assisted MF annotation on inter-rater reliability and object\nlevel agreement through an extensive multi-rater experiment. We found that the\nannotators' object-level agreement increased when using PHH3-assisted labeling.\nSubsequently, MF detectors were evaluated on the resulting datasets to\ninvestigate the influence of PHH3-assisted labeling on the models' performance.\nAdditionally, a novel dual-stain MF detector was developed to investigate the\ninterpretation-shift of PHH3-assisted labels used in H&E, which clearly\noutperformed single-stain detectors. However, the PHH3-assisted labels did not\nhave a positive effect on solely H&E-based models. The high performance of our\ndual-input detector reveals an information mismatch between the H&E and\nPHH3-stained images as the cause of this effect.\n", "link": "http://arxiv.org/abs/2406.19899v1", "date": "2024-06-28", "relevancy": 1.9198, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5242}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4812}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Value%20of%20PHH3%20for%20Mitotic%20Figure%20Detection%20on%20H%26E-stained%20Images&body=Title%3A%20On%20the%20Value%20of%20PHH3%20for%20Mitotic%20Figure%20Detection%20on%20H%26E-stained%20Images%0AAuthor%3A%20Jonathan%20Ganz%20and%20Christian%20Marzahl%20and%20Jonas%20Ammeling%20and%20Barbara%20Richter%20and%20Chlo%C3%A9%20Puget%20and%20Daniela%20Denk%20and%20Elena%20A.%20Demeter%20and%20Flaviu%20A.%20Tabaran%20and%20Gabriel%20Wasinger%20and%20Karoline%20Lipnik%20and%20Marco%20Tecilla%20and%20Matthew%20J.%20Valentine%20and%20Michael%20J.%20Dark%20and%20Niklas%20Abele%20and%20Pompei%20Bolfa%20and%20Ramona%20Erber%20and%20Robert%20Klopfleisch%20and%20Sophie%20Merz%20and%20Taryn%20A.%20Donovan%20and%20Samir%20Jabari%20and%20Christof%20A.%20Bertram%20and%20Katharina%20Breininger%20and%20Marc%20Aubreville%0AAbstract%3A%20%20%20The%20count%20of%20mitotic%20figures%20%28MFs%29%20observed%20in%20hematoxylin%20and%20eosin%0A%28H%26E%29-stained%20slides%20is%20an%20important%20prognostic%20marker%20as%20it%20is%20a%20measure%20for%0Atumor%20cell%20proliferation.%20However%2C%20the%20identification%20of%20MFs%20has%20a%20known%20low%0Ainter-rater%20agreement.%20Deep%20learning%20algorithms%20can%20standardize%20this%20task%2C%20but%0Athey%20require%20large%20amounts%20of%20annotated%20data%20for%20training%20and%20validation.%0AFurthermore%2C%20label%20noise%20introduced%20during%20the%20annotation%20process%20may%20impede%0Athe%20algorithm%27s%20performance.%20Unlike%20H%26E%2C%20the%20mitosis-specific%20antibody%0Aphospho-histone%20H3%20%28PHH3%29%20specifically%20highlights%20MFs.%20Counting%20MFs%20on%20slides%0Astained%20against%20PHH3%20leads%20to%20higher%20agreement%20among%20raters%20and%20has%20therefore%0Arecently%20been%20used%20as%20a%20ground%20truth%20for%20the%20annotation%20of%20MFs%20in%20H%26E.%20However%2C%0Aas%20PHH3%20facilitates%20the%20recognition%20of%20cells%20indistinguishable%20from%20H%26E%20stain%0Aalone%2C%20the%20use%20of%20this%20ground%20truth%20could%20potentially%20introduce%20noise%20into%20the%0AH%26E-related%20dataset%2C%20impacting%20model%20performance.%20This%20study%20analyzes%20the%0Aimpact%20of%20PHH3-assisted%20MF%20annotation%20on%20inter-rater%20reliability%20and%20object%0Alevel%20agreement%20through%20an%20extensive%20multi-rater%20experiment.%20We%20found%20that%20the%0Aannotators%27%20object-level%20agreement%20increased%20when%20using%20PHH3-assisted%20labeling.%0ASubsequently%2C%20MF%20detectors%20were%20evaluated%20on%20the%20resulting%20datasets%20to%0Ainvestigate%20the%20influence%20of%20PHH3-assisted%20labeling%20on%20the%20models%27%20performance.%0AAdditionally%2C%20a%20novel%20dual-stain%20MF%20detector%20was%20developed%20to%20investigate%20the%0Ainterpretation-shift%20of%20PHH3-assisted%20labels%20used%20in%20H%26E%2C%20which%20clearly%0Aoutperformed%20single-stain%20detectors.%20However%2C%20the%20PHH3-assisted%20labels%20did%20not%0Ahave%20a%20positive%20effect%20on%20solely%20H%26E-based%20models.%20The%20high%20performance%20of%20our%0Adual-input%20detector%20reveals%20an%20information%20mismatch%20between%20the%20H%26E%20and%0APHH3-stained%20images%20as%20the%20cause%20of%20this%20effect.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19899v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Value%2520of%2520PHH3%2520for%2520Mitotic%2520Figure%2520Detection%2520on%2520H%2526E-stained%2520Images%26entry.906535625%3DJonathan%2520Ganz%2520and%2520Christian%2520Marzahl%2520and%2520Jonas%2520Ammeling%2520and%2520Barbara%2520Richter%2520and%2520Chlo%25C3%25A9%2520Puget%2520and%2520Daniela%2520Denk%2520and%2520Elena%2520A.%2520Demeter%2520and%2520Flaviu%2520A.%2520Tabaran%2520and%2520Gabriel%2520Wasinger%2520and%2520Karoline%2520Lipnik%2520and%2520Marco%2520Tecilla%2520and%2520Matthew%2520J.%2520Valentine%2520and%2520Michael%2520J.%2520Dark%2520and%2520Niklas%2520Abele%2520and%2520Pompei%2520Bolfa%2520and%2520Ramona%2520Erber%2520and%2520Robert%2520Klopfleisch%2520and%2520Sophie%2520Merz%2520and%2520Taryn%2520A.%2520Donovan%2520and%2520Samir%2520Jabari%2520and%2520Christof%2520A.%2520Bertram%2520and%2520Katharina%2520Breininger%2520and%2520Marc%2520Aubreville%26entry.1292438233%3D%2520%2520The%2520count%2520of%2520mitotic%2520figures%2520%2528MFs%2529%2520observed%2520in%2520hematoxylin%2520and%2520eosin%250A%2528H%2526E%2529-stained%2520slides%2520is%2520an%2520important%2520prognostic%2520marker%2520as%2520it%2520is%2520a%2520measure%2520for%250Atumor%2520cell%2520proliferation.%2520However%252C%2520the%2520identification%2520of%2520MFs%2520has%2520a%2520known%2520low%250Ainter-rater%2520agreement.%2520Deep%2520learning%2520algorithms%2520can%2520standardize%2520this%2520task%252C%2520but%250Athey%2520require%2520large%2520amounts%2520of%2520annotated%2520data%2520for%2520training%2520and%2520validation.%250AFurthermore%252C%2520label%2520noise%2520introduced%2520during%2520the%2520annotation%2520process%2520may%2520impede%250Athe%2520algorithm%2527s%2520performance.%2520Unlike%2520H%2526E%252C%2520the%2520mitosis-specific%2520antibody%250Aphospho-histone%2520H3%2520%2528PHH3%2529%2520specifically%2520highlights%2520MFs.%2520Counting%2520MFs%2520on%2520slides%250Astained%2520against%2520PHH3%2520leads%2520to%2520higher%2520agreement%2520among%2520raters%2520and%2520has%2520therefore%250Arecently%2520been%2520used%2520as%2520a%2520ground%2520truth%2520for%2520the%2520annotation%2520of%2520MFs%2520in%2520H%2526E.%2520However%252C%250Aas%2520PHH3%2520facilitates%2520the%2520recognition%2520of%2520cells%2520indistinguishable%2520from%2520H%2526E%2520stain%250Aalone%252C%2520the%2520use%2520of%2520this%2520ground%2520truth%2520could%2520potentially%2520introduce%2520noise%2520into%2520the%250AH%2526E-related%2520dataset%252C%2520impacting%2520model%2520performance.%2520This%2520study%2520analyzes%2520the%250Aimpact%2520of%2520PHH3-assisted%2520MF%2520annotation%2520on%2520inter-rater%2520reliability%2520and%2520object%250Alevel%2520agreement%2520through%2520an%2520extensive%2520multi-rater%2520experiment.%2520We%2520found%2520that%2520the%250Aannotators%2527%2520object-level%2520agreement%2520increased%2520when%2520using%2520PHH3-assisted%2520labeling.%250ASubsequently%252C%2520MF%2520detectors%2520were%2520evaluated%2520on%2520the%2520resulting%2520datasets%2520to%250Ainvestigate%2520the%2520influence%2520of%2520PHH3-assisted%2520labeling%2520on%2520the%2520models%2527%2520performance.%250AAdditionally%252C%2520a%2520novel%2520dual-stain%2520MF%2520detector%2520was%2520developed%2520to%2520investigate%2520the%250Ainterpretation-shift%2520of%2520PHH3-assisted%2520labels%2520used%2520in%2520H%2526E%252C%2520which%2520clearly%250Aoutperformed%2520single-stain%2520detectors.%2520However%252C%2520the%2520PHH3-assisted%2520labels%2520did%2520not%250Ahave%2520a%2520positive%2520effect%2520on%2520solely%2520H%2526E-based%2520models.%2520The%2520high%2520performance%2520of%2520our%250Adual-input%2520detector%2520reveals%2520an%2520information%2520mismatch%2520between%2520the%2520H%2526E%2520and%250APHH3-stained%2520images%2520as%2520the%2520cause%2520of%2520this%2520effect.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19899v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Value%20of%20PHH3%20for%20Mitotic%20Figure%20Detection%20on%20H%26E-stained%20Images&entry.906535625=Jonathan%20Ganz%20and%20Christian%20Marzahl%20and%20Jonas%20Ammeling%20and%20Barbara%20Richter%20and%20Chlo%C3%A9%20Puget%20and%20Daniela%20Denk%20and%20Elena%20A.%20Demeter%20and%20Flaviu%20A.%20Tabaran%20and%20Gabriel%20Wasinger%20and%20Karoline%20Lipnik%20and%20Marco%20Tecilla%20and%20Matthew%20J.%20Valentine%20and%20Michael%20J.%20Dark%20and%20Niklas%20Abele%20and%20Pompei%20Bolfa%20and%20Ramona%20Erber%20and%20Robert%20Klopfleisch%20and%20Sophie%20Merz%20and%20Taryn%20A.%20Donovan%20and%20Samir%20Jabari%20and%20Christof%20A.%20Bertram%20and%20Katharina%20Breininger%20and%20Marc%20Aubreville&entry.1292438233=%20%20The%20count%20of%20mitotic%20figures%20%28MFs%29%20observed%20in%20hematoxylin%20and%20eosin%0A%28H%26E%29-stained%20slides%20is%20an%20important%20prognostic%20marker%20as%20it%20is%20a%20measure%20for%0Atumor%20cell%20proliferation.%20However%2C%20the%20identification%20of%20MFs%20has%20a%20known%20low%0Ainter-rater%20agreement.%20Deep%20learning%20algorithms%20can%20standardize%20this%20task%2C%20but%0Athey%20require%20large%20amounts%20of%20annotated%20data%20for%20training%20and%20validation.%0AFurthermore%2C%20label%20noise%20introduced%20during%20the%20annotation%20process%20may%20impede%0Athe%20algorithm%27s%20performance.%20Unlike%20H%26E%2C%20the%20mitosis-specific%20antibody%0Aphospho-histone%20H3%20%28PHH3%29%20specifically%20highlights%20MFs.%20Counting%20MFs%20on%20slides%0Astained%20against%20PHH3%20leads%20to%20higher%20agreement%20among%20raters%20and%20has%20therefore%0Arecently%20been%20used%20as%20a%20ground%20truth%20for%20the%20annotation%20of%20MFs%20in%20H%26E.%20However%2C%0Aas%20PHH3%20facilitates%20the%20recognition%20of%20cells%20indistinguishable%20from%20H%26E%20stain%0Aalone%2C%20the%20use%20of%20this%20ground%20truth%20could%20potentially%20introduce%20noise%20into%20the%0AH%26E-related%20dataset%2C%20impacting%20model%20performance.%20This%20study%20analyzes%20the%0Aimpact%20of%20PHH3-assisted%20MF%20annotation%20on%20inter-rater%20reliability%20and%20object%0Alevel%20agreement%20through%20an%20extensive%20multi-rater%20experiment.%20We%20found%20that%20the%0Aannotators%27%20object-level%20agreement%20increased%20when%20using%20PHH3-assisted%20labeling.%0ASubsequently%2C%20MF%20detectors%20were%20evaluated%20on%20the%20resulting%20datasets%20to%0Ainvestigate%20the%20influence%20of%20PHH3-assisted%20labeling%20on%20the%20models%27%20performance.%0AAdditionally%2C%20a%20novel%20dual-stain%20MF%20detector%20was%20developed%20to%20investigate%20the%0Ainterpretation-shift%20of%20PHH3-assisted%20labels%20used%20in%20H%26E%2C%20which%20clearly%0Aoutperformed%20single-stain%20detectors.%20However%2C%20the%20PHH3-assisted%20labels%20did%20not%0Ahave%20a%20positive%20effect%20on%20solely%20H%26E-based%20models.%20The%20high%20performance%20of%20our%0Adual-input%20detector%20reveals%20an%20information%20mismatch%20between%20the%20H%26E%20and%0APHH3-stained%20images%20as%20the%20cause%20of%20this%20effect.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19899v1&entry.124074799=Read"},
{"title": "Scaling Synthetic Data Creation with 1,000,000,000 Personas", "author": "Xin Chan and Xiaoyang Wang and Dian Yu and Haitao Mi and Dong Yu", "abstract": "  We propose a novel persona-driven data synthesis methodology that leverages\nvarious perspectives within a large language model (LLM) to create diverse\nsynthetic data. To fully exploit this methodology at scale, we introduce\nPersona Hub -- a collection of 1 billion diverse personas automatically curated\nfrom web data. These 1 billion personas (~13% of the world's total population),\nacting as distributed carriers of world knowledge, can tap into almost every\nperspective encapsulated within the LLM, thereby facilitating the creation of\ndiverse synthetic data at scale for various scenarios. By showcasing Persona\nHub's use cases in synthesizing high-quality mathematical and logical reasoning\nproblems, instructions (i.e., user prompts), knowledge-rich texts, game NPCs\nand tools (functions) at scale, we demonstrate persona-driven data synthesis is\nversatile, scalable, flexible, and easy to use, potentially driving a paradigm\nshift in synthetic data creation and applications in practice, which may have a\nprofound impact on LLM research and development.\n", "link": "http://arxiv.org/abs/2406.20094v1", "date": "2024-06-28", "relevancy": 1.9126, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4849}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4756}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4675}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Synthetic%20Data%20Creation%20with%201%2C000%2C000%2C000%20Personas&body=Title%3A%20Scaling%20Synthetic%20Data%20Creation%20with%201%2C000%2C000%2C000%20Personas%0AAuthor%3A%20Xin%20Chan%20and%20Xiaoyang%20Wang%20and%20Dian%20Yu%20and%20Haitao%20Mi%20and%20Dong%20Yu%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20persona-driven%20data%20synthesis%20methodology%20that%20leverages%0Avarious%20perspectives%20within%20a%20large%20language%20model%20%28LLM%29%20to%20create%20diverse%0Asynthetic%20data.%20To%20fully%20exploit%20this%20methodology%20at%20scale%2C%20we%20introduce%0APersona%20Hub%20--%20a%20collection%20of%201%20billion%20diverse%20personas%20automatically%20curated%0Afrom%20web%20data.%20These%201%20billion%20personas%20%28~13%25%20of%20the%20world%27s%20total%20population%29%2C%0Aacting%20as%20distributed%20carriers%20of%20world%20knowledge%2C%20can%20tap%20into%20almost%20every%0Aperspective%20encapsulated%20within%20the%20LLM%2C%20thereby%20facilitating%20the%20creation%20of%0Adiverse%20synthetic%20data%20at%20scale%20for%20various%20scenarios.%20By%20showcasing%20Persona%0AHub%27s%20use%20cases%20in%20synthesizing%20high-quality%20mathematical%20and%20logical%20reasoning%0Aproblems%2C%20instructions%20%28i.e.%2C%20user%20prompts%29%2C%20knowledge-rich%20texts%2C%20game%20NPCs%0Aand%20tools%20%28functions%29%20at%20scale%2C%20we%20demonstrate%20persona-driven%20data%20synthesis%20is%0Aversatile%2C%20scalable%2C%20flexible%2C%20and%20easy%20to%20use%2C%20potentially%20driving%20a%20paradigm%0Ashift%20in%20synthetic%20data%20creation%20and%20applications%20in%20practice%2C%20which%20may%20have%20a%0Aprofound%20impact%20on%20LLM%20research%20and%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.20094v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Synthetic%2520Data%2520Creation%2520with%25201%252C000%252C000%252C000%2520Personas%26entry.906535625%3DXin%2520Chan%2520and%2520Xiaoyang%2520Wang%2520and%2520Dian%2520Yu%2520and%2520Haitao%2520Mi%2520and%2520Dong%2520Yu%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520persona-driven%2520data%2520synthesis%2520methodology%2520that%2520leverages%250Avarious%2520perspectives%2520within%2520a%2520large%2520language%2520model%2520%2528LLM%2529%2520to%2520create%2520diverse%250Asynthetic%2520data.%2520To%2520fully%2520exploit%2520this%2520methodology%2520at%2520scale%252C%2520we%2520introduce%250APersona%2520Hub%2520--%2520a%2520collection%2520of%25201%2520billion%2520diverse%2520personas%2520automatically%2520curated%250Afrom%2520web%2520data.%2520These%25201%2520billion%2520personas%2520%2528~13%2525%2520of%2520the%2520world%2527s%2520total%2520population%2529%252C%250Aacting%2520as%2520distributed%2520carriers%2520of%2520world%2520knowledge%252C%2520can%2520tap%2520into%2520almost%2520every%250Aperspective%2520encapsulated%2520within%2520the%2520LLM%252C%2520thereby%2520facilitating%2520the%2520creation%2520of%250Adiverse%2520synthetic%2520data%2520at%2520scale%2520for%2520various%2520scenarios.%2520By%2520showcasing%2520Persona%250AHub%2527s%2520use%2520cases%2520in%2520synthesizing%2520high-quality%2520mathematical%2520and%2520logical%2520reasoning%250Aproblems%252C%2520instructions%2520%2528i.e.%252C%2520user%2520prompts%2529%252C%2520knowledge-rich%2520texts%252C%2520game%2520NPCs%250Aand%2520tools%2520%2528functions%2529%2520at%2520scale%252C%2520we%2520demonstrate%2520persona-driven%2520data%2520synthesis%2520is%250Aversatile%252C%2520scalable%252C%2520flexible%252C%2520and%2520easy%2520to%2520use%252C%2520potentially%2520driving%2520a%2520paradigm%250Ashift%2520in%2520synthetic%2520data%2520creation%2520and%2520applications%2520in%2520practice%252C%2520which%2520may%2520have%2520a%250Aprofound%2520impact%2520on%2520LLM%2520research%2520and%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.20094v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Synthetic%20Data%20Creation%20with%201%2C000%2C000%2C000%20Personas&entry.906535625=Xin%20Chan%20and%20Xiaoyang%20Wang%20and%20Dian%20Yu%20and%20Haitao%20Mi%20and%20Dong%20Yu&entry.1292438233=%20%20We%20propose%20a%20novel%20persona-driven%20data%20synthesis%20methodology%20that%20leverages%0Avarious%20perspectives%20within%20a%20large%20language%20model%20%28LLM%29%20to%20create%20diverse%0Asynthetic%20data.%20To%20fully%20exploit%20this%20methodology%20at%20scale%2C%20we%20introduce%0APersona%20Hub%20--%20a%20collection%20of%201%20billion%20diverse%20personas%20automatically%20curated%0Afrom%20web%20data.%20These%201%20billion%20personas%20%28~13%25%20of%20the%20world%27s%20total%20population%29%2C%0Aacting%20as%20distributed%20carriers%20of%20world%20knowledge%2C%20can%20tap%20into%20almost%20every%0Aperspective%20encapsulated%20within%20the%20LLM%2C%20thereby%20facilitating%20the%20creation%20of%0Adiverse%20synthetic%20data%20at%20scale%20for%20various%20scenarios.%20By%20showcasing%20Persona%0AHub%27s%20use%20cases%20in%20synthesizing%20high-quality%20mathematical%20and%20logical%20reasoning%0Aproblems%2C%20instructions%20%28i.e.%2C%20user%20prompts%29%2C%20knowledge-rich%20texts%2C%20game%20NPCs%0Aand%20tools%20%28functions%29%20at%20scale%2C%20we%20demonstrate%20persona-driven%20data%20synthesis%20is%0Aversatile%2C%20scalable%2C%20flexible%2C%20and%20easy%20to%20use%2C%20potentially%20driving%20a%20paradigm%0Ashift%20in%20synthetic%20data%20creation%20and%20applications%20in%20practice%2C%20which%20may%20have%20a%0Aprofound%20impact%20on%20LLM%20research%20and%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.20094v1&entry.124074799=Read"},
{"title": "ScaleBiO: Scalable Bilevel Optimization for LLM Data Reweighting", "author": "Rui Pan and Jipeng Zhang and Xingyuan Pan and Renjie Pi and Xiaoyu Wang and Tong Zhang", "abstract": "  Bilevel optimization has shown its utility across various machine learning\nsettings, yet most algorithms in practice require second-order information,\nmaking it challenging to scale them up. Only recently, a paradigm of\nfirst-order algorithms emerged, capable of effectively addressing bilevel\noptimization problems. Nevertheless, the practical efficiency of this paradigm\nremains unverified, particularly in the context of large language models\n(LLMs). This paper introduces the first scalable instantiation of this paradigm\ncalled ScaleBiO, focusing on bilevel optimization for large-scale LLM data\nreweighting. By combining with a recently proposed memory-efficient training\ntechnique called LISA, our novel algorithm allows the paradigm to scale to\n34-billion-parameter LLMs on eight A40 GPUs, marking the first successful\napplication of bilevel optimization under practical scenarios for large-sized\nLLMs. Empirically, extensive experiments on data reweighting verify the\neffectiveness of ScaleBiO for different-scaled models, including GPT-2,\nLLaMA-3-8B, GPT-NeoX-20B, and Yi-34B, where bilevel optimization succeeds in\nfiltering irrelevant data samples and selecting informative samples.\nTheoretically, ScaleBiO ensures the optimality of the learned data weights,\nalong with a convergence guarantee matching the conventional first-order\nbilevel optimization paradigm on smooth and strongly convex objectives.\n", "link": "http://arxiv.org/abs/2406.19976v1", "date": "2024-06-28", "relevancy": 1.8981, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5013}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.472}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4664}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ScaleBiO%3A%20Scalable%20Bilevel%20Optimization%20for%20LLM%20Data%20Reweighting&body=Title%3A%20ScaleBiO%3A%20Scalable%20Bilevel%20Optimization%20for%20LLM%20Data%20Reweighting%0AAuthor%3A%20Rui%20Pan%20and%20Jipeng%20Zhang%20and%20Xingyuan%20Pan%20and%20Renjie%20Pi%20and%20Xiaoyu%20Wang%20and%20Tong%20Zhang%0AAbstract%3A%20%20%20Bilevel%20optimization%20has%20shown%20its%20utility%20across%20various%20machine%20learning%0Asettings%2C%20yet%20most%20algorithms%20in%20practice%20require%20second-order%20information%2C%0Amaking%20it%20challenging%20to%20scale%20them%20up.%20Only%20recently%2C%20a%20paradigm%20of%0Afirst-order%20algorithms%20emerged%2C%20capable%20of%20effectively%20addressing%20bilevel%0Aoptimization%20problems.%20Nevertheless%2C%20the%20practical%20efficiency%20of%20this%20paradigm%0Aremains%20unverified%2C%20particularly%20in%20the%20context%20of%20large%20language%20models%0A%28LLMs%29.%20This%20paper%20introduces%20the%20first%20scalable%20instantiation%20of%20this%20paradigm%0Acalled%20ScaleBiO%2C%20focusing%20on%20bilevel%20optimization%20for%20large-scale%20LLM%20data%0Areweighting.%20By%20combining%20with%20a%20recently%20proposed%20memory-efficient%20training%0Atechnique%20called%20LISA%2C%20our%20novel%20algorithm%20allows%20the%20paradigm%20to%20scale%20to%0A34-billion-parameter%20LLMs%20on%20eight%20A40%20GPUs%2C%20marking%20the%20first%20successful%0Aapplication%20of%20bilevel%20optimization%20under%20practical%20scenarios%20for%20large-sized%0ALLMs.%20Empirically%2C%20extensive%20experiments%20on%20data%20reweighting%20verify%20the%0Aeffectiveness%20of%20ScaleBiO%20for%20different-scaled%20models%2C%20including%20GPT-2%2C%0ALLaMA-3-8B%2C%20GPT-NeoX-20B%2C%20and%20Yi-34B%2C%20where%20bilevel%20optimization%20succeeds%20in%0Afiltering%20irrelevant%20data%20samples%20and%20selecting%20informative%20samples.%0ATheoretically%2C%20ScaleBiO%20ensures%20the%20optimality%20of%20the%20learned%20data%20weights%2C%0Aalong%20with%20a%20convergence%20guarantee%20matching%20the%20conventional%20first-order%0Abilevel%20optimization%20paradigm%20on%20smooth%20and%20strongly%20convex%20objectives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19976v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaleBiO%253A%2520Scalable%2520Bilevel%2520Optimization%2520for%2520LLM%2520Data%2520Reweighting%26entry.906535625%3DRui%2520Pan%2520and%2520Jipeng%2520Zhang%2520and%2520Xingyuan%2520Pan%2520and%2520Renjie%2520Pi%2520and%2520Xiaoyu%2520Wang%2520and%2520Tong%2520Zhang%26entry.1292438233%3D%2520%2520Bilevel%2520optimization%2520has%2520shown%2520its%2520utility%2520across%2520various%2520machine%2520learning%250Asettings%252C%2520yet%2520most%2520algorithms%2520in%2520practice%2520require%2520second-order%2520information%252C%250Amaking%2520it%2520challenging%2520to%2520scale%2520them%2520up.%2520Only%2520recently%252C%2520a%2520paradigm%2520of%250Afirst-order%2520algorithms%2520emerged%252C%2520capable%2520of%2520effectively%2520addressing%2520bilevel%250Aoptimization%2520problems.%2520Nevertheless%252C%2520the%2520practical%2520efficiency%2520of%2520this%2520paradigm%250Aremains%2520unverified%252C%2520particularly%2520in%2520the%2520context%2520of%2520large%2520language%2520models%250A%2528LLMs%2529.%2520This%2520paper%2520introduces%2520the%2520first%2520scalable%2520instantiation%2520of%2520this%2520paradigm%250Acalled%2520ScaleBiO%252C%2520focusing%2520on%2520bilevel%2520optimization%2520for%2520large-scale%2520LLM%2520data%250Areweighting.%2520By%2520combining%2520with%2520a%2520recently%2520proposed%2520memory-efficient%2520training%250Atechnique%2520called%2520LISA%252C%2520our%2520novel%2520algorithm%2520allows%2520the%2520paradigm%2520to%2520scale%2520to%250A34-billion-parameter%2520LLMs%2520on%2520eight%2520A40%2520GPUs%252C%2520marking%2520the%2520first%2520successful%250Aapplication%2520of%2520bilevel%2520optimization%2520under%2520practical%2520scenarios%2520for%2520large-sized%250ALLMs.%2520Empirically%252C%2520extensive%2520experiments%2520on%2520data%2520reweighting%2520verify%2520the%250Aeffectiveness%2520of%2520ScaleBiO%2520for%2520different-scaled%2520models%252C%2520including%2520GPT-2%252C%250ALLaMA-3-8B%252C%2520GPT-NeoX-20B%252C%2520and%2520Yi-34B%252C%2520where%2520bilevel%2520optimization%2520succeeds%2520in%250Afiltering%2520irrelevant%2520data%2520samples%2520and%2520selecting%2520informative%2520samples.%250ATheoretically%252C%2520ScaleBiO%2520ensures%2520the%2520optimality%2520of%2520the%2520learned%2520data%2520weights%252C%250Aalong%2520with%2520a%2520convergence%2520guarantee%2520matching%2520the%2520conventional%2520first-order%250Abilevel%2520optimization%2520paradigm%2520on%2520smooth%2520and%2520strongly%2520convex%2520objectives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19976v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ScaleBiO%3A%20Scalable%20Bilevel%20Optimization%20for%20LLM%20Data%20Reweighting&entry.906535625=Rui%20Pan%20and%20Jipeng%20Zhang%20and%20Xingyuan%20Pan%20and%20Renjie%20Pi%20and%20Xiaoyu%20Wang%20and%20Tong%20Zhang&entry.1292438233=%20%20Bilevel%20optimization%20has%20shown%20its%20utility%20across%20various%20machine%20learning%0Asettings%2C%20yet%20most%20algorithms%20in%20practice%20require%20second-order%20information%2C%0Amaking%20it%20challenging%20to%20scale%20them%20up.%20Only%20recently%2C%20a%20paradigm%20of%0Afirst-order%20algorithms%20emerged%2C%20capable%20of%20effectively%20addressing%20bilevel%0Aoptimization%20problems.%20Nevertheless%2C%20the%20practical%20efficiency%20of%20this%20paradigm%0Aremains%20unverified%2C%20particularly%20in%20the%20context%20of%20large%20language%20models%0A%28LLMs%29.%20This%20paper%20introduces%20the%20first%20scalable%20instantiation%20of%20this%20paradigm%0Acalled%20ScaleBiO%2C%20focusing%20on%20bilevel%20optimization%20for%20large-scale%20LLM%20data%0Areweighting.%20By%20combining%20with%20a%20recently%20proposed%20memory-efficient%20training%0Atechnique%20called%20LISA%2C%20our%20novel%20algorithm%20allows%20the%20paradigm%20to%20scale%20to%0A34-billion-parameter%20LLMs%20on%20eight%20A40%20GPUs%2C%20marking%20the%20first%20successful%0Aapplication%20of%20bilevel%20optimization%20under%20practical%20scenarios%20for%20large-sized%0ALLMs.%20Empirically%2C%20extensive%20experiments%20on%20data%20reweighting%20verify%20the%0Aeffectiveness%20of%20ScaleBiO%20for%20different-scaled%20models%2C%20including%20GPT-2%2C%0ALLaMA-3-8B%2C%20GPT-NeoX-20B%2C%20and%20Yi-34B%2C%20where%20bilevel%20optimization%20succeeds%20in%0Afiltering%20irrelevant%20data%20samples%20and%20selecting%20informative%20samples.%0ATheoretically%2C%20ScaleBiO%20ensures%20the%20optimality%20of%20the%20learned%20data%20weights%2C%0Aalong%20with%20a%20convergence%20guarantee%20matching%20the%20conventional%20first-order%0Abilevel%20optimization%20paradigm%20on%20smooth%20and%20strongly%20convex%20objectives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19976v1&entry.124074799=Read"},
{"title": "AnomaLLMy -- Detecting anomalous tokens in black-box LLMs through\n  low-confidence single-token predictions", "author": "Walig\u00f3ra Witold", "abstract": "  This paper introduces AnomaLLMy, a novel technique for the automatic\ndetection of anomalous tokens in black-box Large Language Models (LLMs) with\nAPI-only access. Utilizing low-confidence single-token predictions as a\ncost-effective indicator, AnomaLLMy identifies irregularities in model\nbehavior, addressing the issue of anomalous tokens degrading the quality and\nreliability of models. Validated on the cl100k_base dataset, the token set of\nGPT-4, AnomaLLMy detected 413 major and 65 minor anomalies, demonstrating the\nmethod's efficiency with just \\$24.39 spent in API credits. The insights from\nthis research are expected to be beneficial for enhancing the robustness of and\naccuracy of LLMs, particularly in the development and assessment of tokenizers.\n", "link": "http://arxiv.org/abs/2406.19840v1", "date": "2024-06-28", "relevancy": 1.8963, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4868}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4839}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnomaLLMy%20--%20Detecting%20anomalous%20tokens%20in%20black-box%20LLMs%20through%0A%20%20low-confidence%20single-token%20predictions&body=Title%3A%20AnomaLLMy%20--%20Detecting%20anomalous%20tokens%20in%20black-box%20LLMs%20through%0A%20%20low-confidence%20single-token%20predictions%0AAuthor%3A%20Walig%C3%B3ra%20Witold%0AAbstract%3A%20%20%20This%20paper%20introduces%20AnomaLLMy%2C%20a%20novel%20technique%20for%20the%20automatic%0Adetection%20of%20anomalous%20tokens%20in%20black-box%20Large%20Language%20Models%20%28LLMs%29%20with%0AAPI-only%20access.%20Utilizing%20low-confidence%20single-token%20predictions%20as%20a%0Acost-effective%20indicator%2C%20AnomaLLMy%20identifies%20irregularities%20in%20model%0Abehavior%2C%20addressing%20the%20issue%20of%20anomalous%20tokens%20degrading%20the%20quality%20and%0Areliability%20of%20models.%20Validated%20on%20the%20cl100k_base%20dataset%2C%20the%20token%20set%20of%0AGPT-4%2C%20AnomaLLMy%20detected%20413%20major%20and%2065%20minor%20anomalies%2C%20demonstrating%20the%0Amethod%27s%20efficiency%20with%20just%20%5C%2424.39%20spent%20in%20API%20credits.%20The%20insights%20from%0Athis%20research%20are%20expected%20to%20be%20beneficial%20for%20enhancing%20the%20robustness%20of%20and%0Aaccuracy%20of%20LLMs%2C%20particularly%20in%20the%20development%20and%20assessment%20of%20tokenizers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19840v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnomaLLMy%2520--%2520Detecting%2520anomalous%2520tokens%2520in%2520black-box%2520LLMs%2520through%250A%2520%2520low-confidence%2520single-token%2520predictions%26entry.906535625%3DWalig%25C3%25B3ra%2520Witold%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520AnomaLLMy%252C%2520a%2520novel%2520technique%2520for%2520the%2520automatic%250Adetection%2520of%2520anomalous%2520tokens%2520in%2520black-box%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520with%250AAPI-only%2520access.%2520Utilizing%2520low-confidence%2520single-token%2520predictions%2520as%2520a%250Acost-effective%2520indicator%252C%2520AnomaLLMy%2520identifies%2520irregularities%2520in%2520model%250Abehavior%252C%2520addressing%2520the%2520issue%2520of%2520anomalous%2520tokens%2520degrading%2520the%2520quality%2520and%250Areliability%2520of%2520models.%2520Validated%2520on%2520the%2520cl100k_base%2520dataset%252C%2520the%2520token%2520set%2520of%250AGPT-4%252C%2520AnomaLLMy%2520detected%2520413%2520major%2520and%252065%2520minor%2520anomalies%252C%2520demonstrating%2520the%250Amethod%2527s%2520efficiency%2520with%2520just%2520%255C%252424.39%2520spent%2520in%2520API%2520credits.%2520The%2520insights%2520from%250Athis%2520research%2520are%2520expected%2520to%2520be%2520beneficial%2520for%2520enhancing%2520the%2520robustness%2520of%2520and%250Aaccuracy%2520of%2520LLMs%252C%2520particularly%2520in%2520the%2520development%2520and%2520assessment%2520of%2520tokenizers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19840v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnomaLLMy%20--%20Detecting%20anomalous%20tokens%20in%20black-box%20LLMs%20through%0A%20%20low-confidence%20single-token%20predictions&entry.906535625=Walig%C3%B3ra%20Witold&entry.1292438233=%20%20This%20paper%20introduces%20AnomaLLMy%2C%20a%20novel%20technique%20for%20the%20automatic%0Adetection%20of%20anomalous%20tokens%20in%20black-box%20Large%20Language%20Models%20%28LLMs%29%20with%0AAPI-only%20access.%20Utilizing%20low-confidence%20single-token%20predictions%20as%20a%0Acost-effective%20indicator%2C%20AnomaLLMy%20identifies%20irregularities%20in%20model%0Abehavior%2C%20addressing%20the%20issue%20of%20anomalous%20tokens%20degrading%20the%20quality%20and%0Areliability%20of%20models.%20Validated%20on%20the%20cl100k_base%20dataset%2C%20the%20token%20set%20of%0AGPT-4%2C%20AnomaLLMy%20detected%20413%20major%20and%2065%20minor%20anomalies%2C%20demonstrating%20the%0Amethod%27s%20efficiency%20with%20just%20%5C%2424.39%20spent%20in%20API%20credits.%20The%20insights%20from%0Athis%20research%20are%20expected%20to%20be%20beneficial%20for%20enhancing%20the%20robustness%20of%20and%0Aaccuracy%20of%20LLMs%2C%20particularly%20in%20the%20development%20and%20assessment%20of%20tokenizers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19840v1&entry.124074799=Read"},
{"title": "AuthAttLyzer-V2: Unveiling Code Authorship Attribution using Enhanced\n  Ensemble Learning Models & Generating Benchmark Dataset", "author": "Bhaskar Joshi and Sepideh HajiHossein Khani and Arash HabibiLashkari", "abstract": "  Source Code Authorship Attribution (SCAA) is crucial for software\nclassification because it provides insights into the origin and behavior of\nsoftware. By accurately identifying the author or group behind a piece of code,\nexperts can better understand the motivations and techniques of developers. In\nthe cybersecurity era, this attribution helps trace the source of malicious\nsoftware, identify patterns in the code that may indicate specific threat\nactors or groups, and ultimately enhance threat intelligence and mitigation\nstrategies. This paper presents AuthAttLyzer-V2, a new source code feature\nextractor for SCAA, focusing on lexical, semantic, syntactic, and N-gram\nfeatures. Our research explores author identification in C++ by examining\n24,000 source code samples from 3,000 authors. Our methodology integrates\nRandom Forest, Gradient Boosting, and XGBoost models, enhanced with SHAP for\ninterpretability. The study demonstrates how ensemble models can effectively\ndiscern individual coding styles, offering insights into the unique attributes\nof code authorship. This approach is pivotal in understanding and interpreting\ncomplex patterns in authorship attribution, especially for malware\nclassification.\n", "link": "http://arxiv.org/abs/2406.19896v1", "date": "2024-06-28", "relevancy": 1.8929, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5183}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4412}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4406}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AuthAttLyzer-V2%3A%20Unveiling%20Code%20Authorship%20Attribution%20using%20Enhanced%0A%20%20Ensemble%20Learning%20Models%20%26%20Generating%20Benchmark%20Dataset&body=Title%3A%20AuthAttLyzer-V2%3A%20Unveiling%20Code%20Authorship%20Attribution%20using%20Enhanced%0A%20%20Ensemble%20Learning%20Models%20%26%20Generating%20Benchmark%20Dataset%0AAuthor%3A%20Bhaskar%20Joshi%20and%20Sepideh%20HajiHossein%20Khani%20and%20Arash%20HabibiLashkari%0AAbstract%3A%20%20%20Source%20Code%20Authorship%20Attribution%20%28SCAA%29%20is%20crucial%20for%20software%0Aclassification%20because%20it%20provides%20insights%20into%20the%20origin%20and%20behavior%20of%0Asoftware.%20By%20accurately%20identifying%20the%20author%20or%20group%20behind%20a%20piece%20of%20code%2C%0Aexperts%20can%20better%20understand%20the%20motivations%20and%20techniques%20of%20developers.%20In%0Athe%20cybersecurity%20era%2C%20this%20attribution%20helps%20trace%20the%20source%20of%20malicious%0Asoftware%2C%20identify%20patterns%20in%20the%20code%20that%20may%20indicate%20specific%20threat%0Aactors%20or%20groups%2C%20and%20ultimately%20enhance%20threat%20intelligence%20and%20mitigation%0Astrategies.%20This%20paper%20presents%20AuthAttLyzer-V2%2C%20a%20new%20source%20code%20feature%0Aextractor%20for%20SCAA%2C%20focusing%20on%20lexical%2C%20semantic%2C%20syntactic%2C%20and%20N-gram%0Afeatures.%20Our%20research%20explores%20author%20identification%20in%20C%2B%2B%20by%20examining%0A24%2C000%20source%20code%20samples%20from%203%2C000%20authors.%20Our%20methodology%20integrates%0ARandom%20Forest%2C%20Gradient%20Boosting%2C%20and%20XGBoost%20models%2C%20enhanced%20with%20SHAP%20for%0Ainterpretability.%20The%20study%20demonstrates%20how%20ensemble%20models%20can%20effectively%0Adiscern%20individual%20coding%20styles%2C%20offering%20insights%20into%20the%20unique%20attributes%0Aof%20code%20authorship.%20This%20approach%20is%20pivotal%20in%20understanding%20and%20interpreting%0Acomplex%20patterns%20in%20authorship%20attribution%2C%20especially%20for%20malware%0Aclassification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19896v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAuthAttLyzer-V2%253A%2520Unveiling%2520Code%2520Authorship%2520Attribution%2520using%2520Enhanced%250A%2520%2520Ensemble%2520Learning%2520Models%2520%2526%2520Generating%2520Benchmark%2520Dataset%26entry.906535625%3DBhaskar%2520Joshi%2520and%2520Sepideh%2520HajiHossein%2520Khani%2520and%2520Arash%2520HabibiLashkari%26entry.1292438233%3D%2520%2520Source%2520Code%2520Authorship%2520Attribution%2520%2528SCAA%2529%2520is%2520crucial%2520for%2520software%250Aclassification%2520because%2520it%2520provides%2520insights%2520into%2520the%2520origin%2520and%2520behavior%2520of%250Asoftware.%2520By%2520accurately%2520identifying%2520the%2520author%2520or%2520group%2520behind%2520a%2520piece%2520of%2520code%252C%250Aexperts%2520can%2520better%2520understand%2520the%2520motivations%2520and%2520techniques%2520of%2520developers.%2520In%250Athe%2520cybersecurity%2520era%252C%2520this%2520attribution%2520helps%2520trace%2520the%2520source%2520of%2520malicious%250Asoftware%252C%2520identify%2520patterns%2520in%2520the%2520code%2520that%2520may%2520indicate%2520specific%2520threat%250Aactors%2520or%2520groups%252C%2520and%2520ultimately%2520enhance%2520threat%2520intelligence%2520and%2520mitigation%250Astrategies.%2520This%2520paper%2520presents%2520AuthAttLyzer-V2%252C%2520a%2520new%2520source%2520code%2520feature%250Aextractor%2520for%2520SCAA%252C%2520focusing%2520on%2520lexical%252C%2520semantic%252C%2520syntactic%252C%2520and%2520N-gram%250Afeatures.%2520Our%2520research%2520explores%2520author%2520identification%2520in%2520C%252B%252B%2520by%2520examining%250A24%252C000%2520source%2520code%2520samples%2520from%25203%252C000%2520authors.%2520Our%2520methodology%2520integrates%250ARandom%2520Forest%252C%2520Gradient%2520Boosting%252C%2520and%2520XGBoost%2520models%252C%2520enhanced%2520with%2520SHAP%2520for%250Ainterpretability.%2520The%2520study%2520demonstrates%2520how%2520ensemble%2520models%2520can%2520effectively%250Adiscern%2520individual%2520coding%2520styles%252C%2520offering%2520insights%2520into%2520the%2520unique%2520attributes%250Aof%2520code%2520authorship.%2520This%2520approach%2520is%2520pivotal%2520in%2520understanding%2520and%2520interpreting%250Acomplex%2520patterns%2520in%2520authorship%2520attribution%252C%2520especially%2520for%2520malware%250Aclassification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19896v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AuthAttLyzer-V2%3A%20Unveiling%20Code%20Authorship%20Attribution%20using%20Enhanced%0A%20%20Ensemble%20Learning%20Models%20%26%20Generating%20Benchmark%20Dataset&entry.906535625=Bhaskar%20Joshi%20and%20Sepideh%20HajiHossein%20Khani%20and%20Arash%20HabibiLashkari&entry.1292438233=%20%20Source%20Code%20Authorship%20Attribution%20%28SCAA%29%20is%20crucial%20for%20software%0Aclassification%20because%20it%20provides%20insights%20into%20the%20origin%20and%20behavior%20of%0Asoftware.%20By%20accurately%20identifying%20the%20author%20or%20group%20behind%20a%20piece%20of%20code%2C%0Aexperts%20can%20better%20understand%20the%20motivations%20and%20techniques%20of%20developers.%20In%0Athe%20cybersecurity%20era%2C%20this%20attribution%20helps%20trace%20the%20source%20of%20malicious%0Asoftware%2C%20identify%20patterns%20in%20the%20code%20that%20may%20indicate%20specific%20threat%0Aactors%20or%20groups%2C%20and%20ultimately%20enhance%20threat%20intelligence%20and%20mitigation%0Astrategies.%20This%20paper%20presents%20AuthAttLyzer-V2%2C%20a%20new%20source%20code%20feature%0Aextractor%20for%20SCAA%2C%20focusing%20on%20lexical%2C%20semantic%2C%20syntactic%2C%20and%20N-gram%0Afeatures.%20Our%20research%20explores%20author%20identification%20in%20C%2B%2B%20by%20examining%0A24%2C000%20source%20code%20samples%20from%203%2C000%20authors.%20Our%20methodology%20integrates%0ARandom%20Forest%2C%20Gradient%20Boosting%2C%20and%20XGBoost%20models%2C%20enhanced%20with%20SHAP%20for%0Ainterpretability.%20The%20study%20demonstrates%20how%20ensemble%20models%20can%20effectively%0Adiscern%20individual%20coding%20styles%2C%20offering%20insights%20into%20the%20unique%20attributes%0Aof%20code%20authorship.%20This%20approach%20is%20pivotal%20in%20understanding%20and%20interpreting%0Acomplex%20patterns%20in%20authorship%20attribution%2C%20especially%20for%20malware%0Aclassification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19896v1&entry.124074799=Read"},
{"title": "`Just One More Sensor is Enough' -- Iterative Water Leak Localization\n  with Physical Simulation and a Small Number of Pressure Sensors", "author": "Micha\u0142 Cholewa and Micha\u0142 Romaszewski and Przemys\u0142aw G\u0142omb and Katarzyna Ko\u0142odziej and Micha\u0142 Gorawski and Jakub Koral and Wojciech Koral and Andrzej Madej and Kryspin Musio\u0142", "abstract": "  In this article, we propose an approach to leak localisation in a complex\nwater delivery grid with the use of data from physical simulation (e.g. EPANET\nsoftware). This task is usually achieved by a network of multiple water\npressure sensors and analysis of the so-called sensitivity matrix of pressure\ndifferences between the network's simulated data and actual data of the network\naffected by the leak. However, most algorithms using this approach require a\nsignificant number of pressure sensors -- a condition that is not easy to\nfulfil in the case of many less equipped networks. Therefore, we answer the\nquestion of whether leak localisation is possible by utilising very few sensors\nbut having the ability to relocate one of them. Our algorithm is based on\nphysical simulations (EPANET software) and an iterative scheme for mobile\nsensor relocation. The experiments show that the proposed system can equalise\nthe low number of sensors with adjustments made for their positioning, giving a\nvery good approximation of leak's position both in simulated cases and\nreal-life example taken from BattLeDIM competition L-Town data.\n", "link": "http://arxiv.org/abs/2406.19900v1", "date": "2024-06-28", "relevancy": 1.8882, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5046}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4703}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%60Just%20One%20More%20Sensor%20is%20Enough%27%20--%20Iterative%20Water%20Leak%20Localization%0A%20%20with%20Physical%20Simulation%20and%20a%20Small%20Number%20of%20Pressure%20Sensors&body=Title%3A%20%60Just%20One%20More%20Sensor%20is%20Enough%27%20--%20Iterative%20Water%20Leak%20Localization%0A%20%20with%20Physical%20Simulation%20and%20a%20Small%20Number%20of%20Pressure%20Sensors%0AAuthor%3A%20Micha%C5%82%20Cholewa%20and%20Micha%C5%82%20Romaszewski%20and%20Przemys%C5%82aw%20G%C5%82omb%20and%20Katarzyna%20Ko%C5%82odziej%20and%20Micha%C5%82%20Gorawski%20and%20Jakub%20Koral%20and%20Wojciech%20Koral%20and%20Andrzej%20Madej%20and%20Kryspin%20Musio%C5%82%0AAbstract%3A%20%20%20In%20this%20article%2C%20we%20propose%20an%20approach%20to%20leak%20localisation%20in%20a%20complex%0Awater%20delivery%20grid%20with%20the%20use%20of%20data%20from%20physical%20simulation%20%28e.g.%20EPANET%0Asoftware%29.%20This%20task%20is%20usually%20achieved%20by%20a%20network%20of%20multiple%20water%0Apressure%20sensors%20and%20analysis%20of%20the%20so-called%20sensitivity%20matrix%20of%20pressure%0Adifferences%20between%20the%20network%27s%20simulated%20data%20and%20actual%20data%20of%20the%20network%0Aaffected%20by%20the%20leak.%20However%2C%20most%20algorithms%20using%20this%20approach%20require%20a%0Asignificant%20number%20of%20pressure%20sensors%20--%20a%20condition%20that%20is%20not%20easy%20to%0Afulfil%20in%20the%20case%20of%20many%20less%20equipped%20networks.%20Therefore%2C%20we%20answer%20the%0Aquestion%20of%20whether%20leak%20localisation%20is%20possible%20by%20utilising%20very%20few%20sensors%0Abut%20having%20the%20ability%20to%20relocate%20one%20of%20them.%20Our%20algorithm%20is%20based%20on%0Aphysical%20simulations%20%28EPANET%20software%29%20and%20an%20iterative%20scheme%20for%20mobile%0Asensor%20relocation.%20The%20experiments%20show%20that%20the%20proposed%20system%20can%20equalise%0Athe%20low%20number%20of%20sensors%20with%20adjustments%20made%20for%20their%20positioning%2C%20giving%20a%0Avery%20good%20approximation%20of%20leak%27s%20position%20both%20in%20simulated%20cases%20and%0Areal-life%20example%20taken%20from%20BattLeDIM%20competition%20L-Town%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19900v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2560Just%2520One%2520More%2520Sensor%2520is%2520Enough%2527%2520--%2520Iterative%2520Water%2520Leak%2520Localization%250A%2520%2520with%2520Physical%2520Simulation%2520and%2520a%2520Small%2520Number%2520of%2520Pressure%2520Sensors%26entry.906535625%3DMicha%25C5%2582%2520Cholewa%2520and%2520Micha%25C5%2582%2520Romaszewski%2520and%2520Przemys%25C5%2582aw%2520G%25C5%2582omb%2520and%2520Katarzyna%2520Ko%25C5%2582odziej%2520and%2520Micha%25C5%2582%2520Gorawski%2520and%2520Jakub%2520Koral%2520and%2520Wojciech%2520Koral%2520and%2520Andrzej%2520Madej%2520and%2520Kryspin%2520Musio%25C5%2582%26entry.1292438233%3D%2520%2520In%2520this%2520article%252C%2520we%2520propose%2520an%2520approach%2520to%2520leak%2520localisation%2520in%2520a%2520complex%250Awater%2520delivery%2520grid%2520with%2520the%2520use%2520of%2520data%2520from%2520physical%2520simulation%2520%2528e.g.%2520EPANET%250Asoftware%2529.%2520This%2520task%2520is%2520usually%2520achieved%2520by%2520a%2520network%2520of%2520multiple%2520water%250Apressure%2520sensors%2520and%2520analysis%2520of%2520the%2520so-called%2520sensitivity%2520matrix%2520of%2520pressure%250Adifferences%2520between%2520the%2520network%2527s%2520simulated%2520data%2520and%2520actual%2520data%2520of%2520the%2520network%250Aaffected%2520by%2520the%2520leak.%2520However%252C%2520most%2520algorithms%2520using%2520this%2520approach%2520require%2520a%250Asignificant%2520number%2520of%2520pressure%2520sensors%2520--%2520a%2520condition%2520that%2520is%2520not%2520easy%2520to%250Afulfil%2520in%2520the%2520case%2520of%2520many%2520less%2520equipped%2520networks.%2520Therefore%252C%2520we%2520answer%2520the%250Aquestion%2520of%2520whether%2520leak%2520localisation%2520is%2520possible%2520by%2520utilising%2520very%2520few%2520sensors%250Abut%2520having%2520the%2520ability%2520to%2520relocate%2520one%2520of%2520them.%2520Our%2520algorithm%2520is%2520based%2520on%250Aphysical%2520simulations%2520%2528EPANET%2520software%2529%2520and%2520an%2520iterative%2520scheme%2520for%2520mobile%250Asensor%2520relocation.%2520The%2520experiments%2520show%2520that%2520the%2520proposed%2520system%2520can%2520equalise%250Athe%2520low%2520number%2520of%2520sensors%2520with%2520adjustments%2520made%2520for%2520their%2520positioning%252C%2520giving%2520a%250Avery%2520good%2520approximation%2520of%2520leak%2527s%2520position%2520both%2520in%2520simulated%2520cases%2520and%250Areal-life%2520example%2520taken%2520from%2520BattLeDIM%2520competition%2520L-Town%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19900v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%60Just%20One%20More%20Sensor%20is%20Enough%27%20--%20Iterative%20Water%20Leak%20Localization%0A%20%20with%20Physical%20Simulation%20and%20a%20Small%20Number%20of%20Pressure%20Sensors&entry.906535625=Micha%C5%82%20Cholewa%20and%20Micha%C5%82%20Romaszewski%20and%20Przemys%C5%82aw%20G%C5%82omb%20and%20Katarzyna%20Ko%C5%82odziej%20and%20Micha%C5%82%20Gorawski%20and%20Jakub%20Koral%20and%20Wojciech%20Koral%20and%20Andrzej%20Madej%20and%20Kryspin%20Musio%C5%82&entry.1292438233=%20%20In%20this%20article%2C%20we%20propose%20an%20approach%20to%20leak%20localisation%20in%20a%20complex%0Awater%20delivery%20grid%20with%20the%20use%20of%20data%20from%20physical%20simulation%20%28e.g.%20EPANET%0Asoftware%29.%20This%20task%20is%20usually%20achieved%20by%20a%20network%20of%20multiple%20water%0Apressure%20sensors%20and%20analysis%20of%20the%20so-called%20sensitivity%20matrix%20of%20pressure%0Adifferences%20between%20the%20network%27s%20simulated%20data%20and%20actual%20data%20of%20the%20network%0Aaffected%20by%20the%20leak.%20However%2C%20most%20algorithms%20using%20this%20approach%20require%20a%0Asignificant%20number%20of%20pressure%20sensors%20--%20a%20condition%20that%20is%20not%20easy%20to%0Afulfil%20in%20the%20case%20of%20many%20less%20equipped%20networks.%20Therefore%2C%20we%20answer%20the%0Aquestion%20of%20whether%20leak%20localisation%20is%20possible%20by%20utilising%20very%20few%20sensors%0Abut%20having%20the%20ability%20to%20relocate%20one%20of%20them.%20Our%20algorithm%20is%20based%20on%0Aphysical%20simulations%20%28EPANET%20software%29%20and%20an%20iterative%20scheme%20for%20mobile%0Asensor%20relocation.%20The%20experiments%20show%20that%20the%20proposed%20system%20can%20equalise%0Athe%20low%20number%20of%20sensors%20with%20adjustments%20made%20for%20their%20positioning%2C%20giving%20a%0Avery%20good%20approximation%20of%20leak%27s%20position%20both%20in%20simulated%20cases%20and%0Areal-life%20example%20taken%20from%20BattLeDIM%20competition%20L-Town%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19900v1&entry.124074799=Read"},
{"title": "Modeling and LQR Control of Insect Sized Flapping Wing Robot", "author": "Daksh Dhingra and Kadierdan Kaheman and Sawyer B. Fuller", "abstract": "  Flying insects can perform rapid, sophisticated maneuvers like backflips,\nsharp banked turns, and in-flight collision recovery. To emulate these in\naerial robots weighing less than a gram, known as flying insect robots (FIRs),\na fast and responsive control system is essential. To date, these have largely\nbeen, at their core, elaborations of proportional-integral-derivative\n(PID)-type feedback control. Without exception, their gains have been\npainstakingly tuned by hand. Aggressive maneuvers have further required\ntask-specific tuning. Optimal control has the potential to mitigate these\nissues, but has to date only been demonstrated using approxiate models and\nreceding horizon controllers (RHC) that are too computationally demanding to be\ncarried out onboard the robot. Here we used a more accurate stroke-averaged\nmodel of forces and torques to implement the first demonstration of optimal\ncontrol on an FIR that is computationally efficient enough to be performed by a\nmicroprocessor carried onboard. We took force and torque measurements from a\n150 mg FIR, the UW Robofly, using a custom-built sensitive force-torque sensor,\nand validated them using motion capture data in free flight. We demonstrated\nstable hovering (RMS error of about 4 cm) and trajectory tracking maneuvers at\ntranslational velocities up to 25 cm/s using an optimal linear quadratic\nregulator (LQR). These results were enabled by a more accurate model and lay\nthe foundation for future work that uses our improved model and optimal\ncontroller in conjunction with recent advances in low-power receding horizon\ncontrol to perform accurate aggressive maneuvers without iterative,\ntask-specific tuning.\n", "link": "http://arxiv.org/abs/2406.20061v1", "date": "2024-06-28", "relevancy": 1.8716, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4815}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4741}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modeling%20and%20LQR%20Control%20of%20Insect%20Sized%20Flapping%20Wing%20Robot&body=Title%3A%20Modeling%20and%20LQR%20Control%20of%20Insect%20Sized%20Flapping%20Wing%20Robot%0AAuthor%3A%20Daksh%20Dhingra%20and%20Kadierdan%20Kaheman%20and%20Sawyer%20B.%20Fuller%0AAbstract%3A%20%20%20Flying%20insects%20can%20perform%20rapid%2C%20sophisticated%20maneuvers%20like%20backflips%2C%0Asharp%20banked%20turns%2C%20and%20in-flight%20collision%20recovery.%20To%20emulate%20these%20in%0Aaerial%20robots%20weighing%20less%20than%20a%20gram%2C%20known%20as%20flying%20insect%20robots%20%28FIRs%29%2C%0Aa%20fast%20and%20responsive%20control%20system%20is%20essential.%20To%20date%2C%20these%20have%20largely%0Abeen%2C%20at%20their%20core%2C%20elaborations%20of%20proportional-integral-derivative%0A%28PID%29-type%20feedback%20control.%20Without%20exception%2C%20their%20gains%20have%20been%0Apainstakingly%20tuned%20by%20hand.%20Aggressive%20maneuvers%20have%20further%20required%0Atask-specific%20tuning.%20Optimal%20control%20has%20the%20potential%20to%20mitigate%20these%0Aissues%2C%20but%20has%20to%20date%20only%20been%20demonstrated%20using%20approxiate%20models%20and%0Areceding%20horizon%20controllers%20%28RHC%29%20that%20are%20too%20computationally%20demanding%20to%20be%0Acarried%20out%20onboard%20the%20robot.%20Here%20we%20used%20a%20more%20accurate%20stroke-averaged%0Amodel%20of%20forces%20and%20torques%20to%20implement%20the%20first%20demonstration%20of%20optimal%0Acontrol%20on%20an%20FIR%20that%20is%20computationally%20efficient%20enough%20to%20be%20performed%20by%20a%0Amicroprocessor%20carried%20onboard.%20We%20took%20force%20and%20torque%20measurements%20from%20a%0A150%20mg%20FIR%2C%20the%20UW%20Robofly%2C%20using%20a%20custom-built%20sensitive%20force-torque%20sensor%2C%0Aand%20validated%20them%20using%20motion%20capture%20data%20in%20free%20flight.%20We%20demonstrated%0Astable%20hovering%20%28RMS%20error%20of%20about%204%20cm%29%20and%20trajectory%20tracking%20maneuvers%20at%0Atranslational%20velocities%20up%20to%2025%20cm/s%20using%20an%20optimal%20linear%20quadratic%0Aregulator%20%28LQR%29.%20These%20results%20were%20enabled%20by%20a%20more%20accurate%20model%20and%20lay%0Athe%20foundation%20for%20future%20work%20that%20uses%20our%20improved%20model%20and%20optimal%0Acontroller%20in%20conjunction%20with%20recent%20advances%20in%20low-power%20receding%20horizon%0Acontrol%20to%20perform%20accurate%20aggressive%20maneuvers%20without%20iterative%2C%0Atask-specific%20tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.20061v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModeling%2520and%2520LQR%2520Control%2520of%2520Insect%2520Sized%2520Flapping%2520Wing%2520Robot%26entry.906535625%3DDaksh%2520Dhingra%2520and%2520Kadierdan%2520Kaheman%2520and%2520Sawyer%2520B.%2520Fuller%26entry.1292438233%3D%2520%2520Flying%2520insects%2520can%2520perform%2520rapid%252C%2520sophisticated%2520maneuvers%2520like%2520backflips%252C%250Asharp%2520banked%2520turns%252C%2520and%2520in-flight%2520collision%2520recovery.%2520To%2520emulate%2520these%2520in%250Aaerial%2520robots%2520weighing%2520less%2520than%2520a%2520gram%252C%2520known%2520as%2520flying%2520insect%2520robots%2520%2528FIRs%2529%252C%250Aa%2520fast%2520and%2520responsive%2520control%2520system%2520is%2520essential.%2520To%2520date%252C%2520these%2520have%2520largely%250Abeen%252C%2520at%2520their%2520core%252C%2520elaborations%2520of%2520proportional-integral-derivative%250A%2528PID%2529-type%2520feedback%2520control.%2520Without%2520exception%252C%2520their%2520gains%2520have%2520been%250Apainstakingly%2520tuned%2520by%2520hand.%2520Aggressive%2520maneuvers%2520have%2520further%2520required%250Atask-specific%2520tuning.%2520Optimal%2520control%2520has%2520the%2520potential%2520to%2520mitigate%2520these%250Aissues%252C%2520but%2520has%2520to%2520date%2520only%2520been%2520demonstrated%2520using%2520approxiate%2520models%2520and%250Areceding%2520horizon%2520controllers%2520%2528RHC%2529%2520that%2520are%2520too%2520computationally%2520demanding%2520to%2520be%250Acarried%2520out%2520onboard%2520the%2520robot.%2520Here%2520we%2520used%2520a%2520more%2520accurate%2520stroke-averaged%250Amodel%2520of%2520forces%2520and%2520torques%2520to%2520implement%2520the%2520first%2520demonstration%2520of%2520optimal%250Acontrol%2520on%2520an%2520FIR%2520that%2520is%2520computationally%2520efficient%2520enough%2520to%2520be%2520performed%2520by%2520a%250Amicroprocessor%2520carried%2520onboard.%2520We%2520took%2520force%2520and%2520torque%2520measurements%2520from%2520a%250A150%2520mg%2520FIR%252C%2520the%2520UW%2520Robofly%252C%2520using%2520a%2520custom-built%2520sensitive%2520force-torque%2520sensor%252C%250Aand%2520validated%2520them%2520using%2520motion%2520capture%2520data%2520in%2520free%2520flight.%2520We%2520demonstrated%250Astable%2520hovering%2520%2528RMS%2520error%2520of%2520about%25204%2520cm%2529%2520and%2520trajectory%2520tracking%2520maneuvers%2520at%250Atranslational%2520velocities%2520up%2520to%252025%2520cm/s%2520using%2520an%2520optimal%2520linear%2520quadratic%250Aregulator%2520%2528LQR%2529.%2520These%2520results%2520were%2520enabled%2520by%2520a%2520more%2520accurate%2520model%2520and%2520lay%250Athe%2520foundation%2520for%2520future%2520work%2520that%2520uses%2520our%2520improved%2520model%2520and%2520optimal%250Acontroller%2520in%2520conjunction%2520with%2520recent%2520advances%2520in%2520low-power%2520receding%2520horizon%250Acontrol%2520to%2520perform%2520accurate%2520aggressive%2520maneuvers%2520without%2520iterative%252C%250Atask-specific%2520tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.20061v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modeling%20and%20LQR%20Control%20of%20Insect%20Sized%20Flapping%20Wing%20Robot&entry.906535625=Daksh%20Dhingra%20and%20Kadierdan%20Kaheman%20and%20Sawyer%20B.%20Fuller&entry.1292438233=%20%20Flying%20insects%20can%20perform%20rapid%2C%20sophisticated%20maneuvers%20like%20backflips%2C%0Asharp%20banked%20turns%2C%20and%20in-flight%20collision%20recovery.%20To%20emulate%20these%20in%0Aaerial%20robots%20weighing%20less%20than%20a%20gram%2C%20known%20as%20flying%20insect%20robots%20%28FIRs%29%2C%0Aa%20fast%20and%20responsive%20control%20system%20is%20essential.%20To%20date%2C%20these%20have%20largely%0Abeen%2C%20at%20their%20core%2C%20elaborations%20of%20proportional-integral-derivative%0A%28PID%29-type%20feedback%20control.%20Without%20exception%2C%20their%20gains%20have%20been%0Apainstakingly%20tuned%20by%20hand.%20Aggressive%20maneuvers%20have%20further%20required%0Atask-specific%20tuning.%20Optimal%20control%20has%20the%20potential%20to%20mitigate%20these%0Aissues%2C%20but%20has%20to%20date%20only%20been%20demonstrated%20using%20approxiate%20models%20and%0Areceding%20horizon%20controllers%20%28RHC%29%20that%20are%20too%20computationally%20demanding%20to%20be%0Acarried%20out%20onboard%20the%20robot.%20Here%20we%20used%20a%20more%20accurate%20stroke-averaged%0Amodel%20of%20forces%20and%20torques%20to%20implement%20the%20first%20demonstration%20of%20optimal%0Acontrol%20on%20an%20FIR%20that%20is%20computationally%20efficient%20enough%20to%20be%20performed%20by%20a%0Amicroprocessor%20carried%20onboard.%20We%20took%20force%20and%20torque%20measurements%20from%20a%0A150%20mg%20FIR%2C%20the%20UW%20Robofly%2C%20using%20a%20custom-built%20sensitive%20force-torque%20sensor%2C%0Aand%20validated%20them%20using%20motion%20capture%20data%20in%20free%20flight.%20We%20demonstrated%0Astable%20hovering%20%28RMS%20error%20of%20about%204%20cm%29%20and%20trajectory%20tracking%20maneuvers%20at%0Atranslational%20velocities%20up%20to%2025%20cm/s%20using%20an%20optimal%20linear%20quadratic%0Aregulator%20%28LQR%29.%20These%20results%20were%20enabled%20by%20a%20more%20accurate%20model%20and%20lay%0Athe%20foundation%20for%20future%20work%20that%20uses%20our%20improved%20model%20and%20optimal%0Acontroller%20in%20conjunction%20with%20recent%20advances%20in%20low-power%20receding%20horizon%0Acontrol%20to%20perform%20accurate%20aggressive%20maneuvers%20without%20iterative%2C%0Atask-specific%20tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.20061v1&entry.124074799=Read"},
{"title": "Towards Learning Stochastic Population Models by Gradient Descent", "author": "Justin N. Kreikemeyer and Philipp Andelfinger and Adelinde M. Uhrmacher", "abstract": "  Increasing effort is put into the development of methods for learning\nmechanistic models from data. This task entails not only the accurate\nestimation of parameters but also a suitable model structure. Recent work on\nthe discovery of dynamical systems formulates this problem as a linear equation\nsystem. Here, we explore several simulation-based optimization approaches,\nwhich allow much greater freedom in the objective formulation and weaker\nconditions on the available data. We show that even for relatively small\nstochastic population models, simultaneous estimation of parameters and\nstructure poses major challenges for optimization procedures. Particularly, we\ninvestigate the application of the local stochastic gradient descent method,\ncommonly used for training machine learning models. We demonstrate accurate\nestimation of models but find that enforcing the inference of parsimonious,\ninterpretable models drastically increases the difficulty. We give an outlook\non how this challenge can be overcome.\n", "link": "http://arxiv.org/abs/2404.07049v2", "date": "2024-06-28", "relevancy": 1.8694, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4723}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4671}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Learning%20Stochastic%20Population%20Models%20by%20Gradient%20Descent&body=Title%3A%20Towards%20Learning%20Stochastic%20Population%20Models%20by%20Gradient%20Descent%0AAuthor%3A%20Justin%20N.%20Kreikemeyer%20and%20Philipp%20Andelfinger%20and%20Adelinde%20M.%20Uhrmacher%0AAbstract%3A%20%20%20Increasing%20effort%20is%20put%20into%20the%20development%20of%20methods%20for%20learning%0Amechanistic%20models%20from%20data.%20This%20task%20entails%20not%20only%20the%20accurate%0Aestimation%20of%20parameters%20but%20also%20a%20suitable%20model%20structure.%20Recent%20work%20on%0Athe%20discovery%20of%20dynamical%20systems%20formulates%20this%20problem%20as%20a%20linear%20equation%0Asystem.%20Here%2C%20we%20explore%20several%20simulation-based%20optimization%20approaches%2C%0Awhich%20allow%20much%20greater%20freedom%20in%20the%20objective%20formulation%20and%20weaker%0Aconditions%20on%20the%20available%20data.%20We%20show%20that%20even%20for%20relatively%20small%0Astochastic%20population%20models%2C%20simultaneous%20estimation%20of%20parameters%20and%0Astructure%20poses%20major%20challenges%20for%20optimization%20procedures.%20Particularly%2C%20we%0Ainvestigate%20the%20application%20of%20the%20local%20stochastic%20gradient%20descent%20method%2C%0Acommonly%20used%20for%20training%20machine%20learning%20models.%20We%20demonstrate%20accurate%0Aestimation%20of%20models%20but%20find%20that%20enforcing%20the%20inference%20of%20parsimonious%2C%0Ainterpretable%20models%20drastically%20increases%20the%20difficulty.%20We%20give%20an%20outlook%0Aon%20how%20this%20challenge%20can%20be%20overcome.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07049v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Learning%2520Stochastic%2520Population%2520Models%2520by%2520Gradient%2520Descent%26entry.906535625%3DJustin%2520N.%2520Kreikemeyer%2520and%2520Philipp%2520Andelfinger%2520and%2520Adelinde%2520M.%2520Uhrmacher%26entry.1292438233%3D%2520%2520Increasing%2520effort%2520is%2520put%2520into%2520the%2520development%2520of%2520methods%2520for%2520learning%250Amechanistic%2520models%2520from%2520data.%2520This%2520task%2520entails%2520not%2520only%2520the%2520accurate%250Aestimation%2520of%2520parameters%2520but%2520also%2520a%2520suitable%2520model%2520structure.%2520Recent%2520work%2520on%250Athe%2520discovery%2520of%2520dynamical%2520systems%2520formulates%2520this%2520problem%2520as%2520a%2520linear%2520equation%250Asystem.%2520Here%252C%2520we%2520explore%2520several%2520simulation-based%2520optimization%2520approaches%252C%250Awhich%2520allow%2520much%2520greater%2520freedom%2520in%2520the%2520objective%2520formulation%2520and%2520weaker%250Aconditions%2520on%2520the%2520available%2520data.%2520We%2520show%2520that%2520even%2520for%2520relatively%2520small%250Astochastic%2520population%2520models%252C%2520simultaneous%2520estimation%2520of%2520parameters%2520and%250Astructure%2520poses%2520major%2520challenges%2520for%2520optimization%2520procedures.%2520Particularly%252C%2520we%250Ainvestigate%2520the%2520application%2520of%2520the%2520local%2520stochastic%2520gradient%2520descent%2520method%252C%250Acommonly%2520used%2520for%2520training%2520machine%2520learning%2520models.%2520We%2520demonstrate%2520accurate%250Aestimation%2520of%2520models%2520but%2520find%2520that%2520enforcing%2520the%2520inference%2520of%2520parsimonious%252C%250Ainterpretable%2520models%2520drastically%2520increases%2520the%2520difficulty.%2520We%2520give%2520an%2520outlook%250Aon%2520how%2520this%2520challenge%2520can%2520be%2520overcome.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.07049v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Learning%20Stochastic%20Population%20Models%20by%20Gradient%20Descent&entry.906535625=Justin%20N.%20Kreikemeyer%20and%20Philipp%20Andelfinger%20and%20Adelinde%20M.%20Uhrmacher&entry.1292438233=%20%20Increasing%20effort%20is%20put%20into%20the%20development%20of%20methods%20for%20learning%0Amechanistic%20models%20from%20data.%20This%20task%20entails%20not%20only%20the%20accurate%0Aestimation%20of%20parameters%20but%20also%20a%20suitable%20model%20structure.%20Recent%20work%20on%0Athe%20discovery%20of%20dynamical%20systems%20formulates%20this%20problem%20as%20a%20linear%20equation%0Asystem.%20Here%2C%20we%20explore%20several%20simulation-based%20optimization%20approaches%2C%0Awhich%20allow%20much%20greater%20freedom%20in%20the%20objective%20formulation%20and%20weaker%0Aconditions%20on%20the%20available%20data.%20We%20show%20that%20even%20for%20relatively%20small%0Astochastic%20population%20models%2C%20simultaneous%20estimation%20of%20parameters%20and%0Astructure%20poses%20major%20challenges%20for%20optimization%20procedures.%20Particularly%2C%20we%0Ainvestigate%20the%20application%20of%20the%20local%20stochastic%20gradient%20descent%20method%2C%0Acommonly%20used%20for%20training%20machine%20learning%20models.%20We%20demonstrate%20accurate%0Aestimation%20of%20models%20but%20find%20that%20enforcing%20the%20inference%20of%20parsimonious%2C%0Ainterpretable%20models%20drastically%20increases%20the%20difficulty.%20We%20give%20an%20outlook%0Aon%20how%20this%20challenge%20can%20be%20overcome.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07049v2&entry.124074799=Read"},
{"title": "TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in\n  Large Language Models", "author": "Zheng Chu and Jingchang Chen and Qianglong Chen and Weijiang Yu and Haotian Wang and Ming Liu and Bing Qin", "abstract": "  Grasping the concept of time is a fundamental facet of human cognition,\nindispensable for truly comprehending the intricacies of the world. Previous\nstudies typically focus on specific aspects of time, lacking a comprehensive\ntemporal reasoning benchmark. To address this, we propose TimeBench, a\ncomprehensive hierarchical temporal reasoning benchmark that covers a broad\nspectrum of temporal reasoning phenomena. TimeBench provides a thorough\nevaluation for investigating the temporal reasoning capabilities of large\nlanguage models. We conduct extensive experiments on GPT-4, LLaMA2, and other\npopular LLMs under various settings. Our experimental results indicate a\nsignificant performance gap between the state-of-the-art LLMs and humans,\nhighlighting that there is still a considerable distance to cover in temporal\nreasoning. Besides, LLMs exhibit capability discrepancies across different\nreasoning categories. Furthermore, we thoroughly analyze the impact of multiple\naspects on temporal reasoning and emphasize the associated challenges. We\naspire for TimeBench to serve as a comprehensive benchmark, fostering research\nin temporal reasoning. Resources are available at:\nhttps://github.com/zchuz/TimeBench\n", "link": "http://arxiv.org/abs/2311.17667v2", "date": "2024-06-28", "relevancy": 1.8663, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4798}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4644}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4389}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TimeBench%3A%20A%20Comprehensive%20Evaluation%20of%20Temporal%20Reasoning%20Abilities%20in%0A%20%20Large%20Language%20Models&body=Title%3A%20TimeBench%3A%20A%20Comprehensive%20Evaluation%20of%20Temporal%20Reasoning%20Abilities%20in%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Zheng%20Chu%20and%20Jingchang%20Chen%20and%20Qianglong%20Chen%20and%20Weijiang%20Yu%20and%20Haotian%20Wang%20and%20Ming%20Liu%20and%20Bing%20Qin%0AAbstract%3A%20%20%20Grasping%20the%20concept%20of%20time%20is%20a%20fundamental%20facet%20of%20human%20cognition%2C%0Aindispensable%20for%20truly%20comprehending%20the%20intricacies%20of%20the%20world.%20Previous%0Astudies%20typically%20focus%20on%20specific%20aspects%20of%20time%2C%20lacking%20a%20comprehensive%0Atemporal%20reasoning%20benchmark.%20To%20address%20this%2C%20we%20propose%20TimeBench%2C%20a%0Acomprehensive%20hierarchical%20temporal%20reasoning%20benchmark%20that%20covers%20a%20broad%0Aspectrum%20of%20temporal%20reasoning%20phenomena.%20TimeBench%20provides%20a%20thorough%0Aevaluation%20for%20investigating%20the%20temporal%20reasoning%20capabilities%20of%20large%0Alanguage%20models.%20We%20conduct%20extensive%20experiments%20on%20GPT-4%2C%20LLaMA2%2C%20and%20other%0Apopular%20LLMs%20under%20various%20settings.%20Our%20experimental%20results%20indicate%20a%0Asignificant%20performance%20gap%20between%20the%20state-of-the-art%20LLMs%20and%20humans%2C%0Ahighlighting%20that%20there%20is%20still%20a%20considerable%20distance%20to%20cover%20in%20temporal%0Areasoning.%20Besides%2C%20LLMs%20exhibit%20capability%20discrepancies%20across%20different%0Areasoning%20categories.%20Furthermore%2C%20we%20thoroughly%20analyze%20the%20impact%20of%20multiple%0Aaspects%20on%20temporal%20reasoning%20and%20emphasize%20the%20associated%20challenges.%20We%0Aaspire%20for%20TimeBench%20to%20serve%20as%20a%20comprehensive%20benchmark%2C%20fostering%20research%0Ain%20temporal%20reasoning.%20Resources%20are%20available%20at%3A%0Ahttps%3A//github.com/zchuz/TimeBench%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.17667v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTimeBench%253A%2520A%2520Comprehensive%2520Evaluation%2520of%2520Temporal%2520Reasoning%2520Abilities%2520in%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DZheng%2520Chu%2520and%2520Jingchang%2520Chen%2520and%2520Qianglong%2520Chen%2520and%2520Weijiang%2520Yu%2520and%2520Haotian%2520Wang%2520and%2520Ming%2520Liu%2520and%2520Bing%2520Qin%26entry.1292438233%3D%2520%2520Grasping%2520the%2520concept%2520of%2520time%2520is%2520a%2520fundamental%2520facet%2520of%2520human%2520cognition%252C%250Aindispensable%2520for%2520truly%2520comprehending%2520the%2520intricacies%2520of%2520the%2520world.%2520Previous%250Astudies%2520typically%2520focus%2520on%2520specific%2520aspects%2520of%2520time%252C%2520lacking%2520a%2520comprehensive%250Atemporal%2520reasoning%2520benchmark.%2520To%2520address%2520this%252C%2520we%2520propose%2520TimeBench%252C%2520a%250Acomprehensive%2520hierarchical%2520temporal%2520reasoning%2520benchmark%2520that%2520covers%2520a%2520broad%250Aspectrum%2520of%2520temporal%2520reasoning%2520phenomena.%2520TimeBench%2520provides%2520a%2520thorough%250Aevaluation%2520for%2520investigating%2520the%2520temporal%2520reasoning%2520capabilities%2520of%2520large%250Alanguage%2520models.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520GPT-4%252C%2520LLaMA2%252C%2520and%2520other%250Apopular%2520LLMs%2520under%2520various%2520settings.%2520Our%2520experimental%2520results%2520indicate%2520a%250Asignificant%2520performance%2520gap%2520between%2520the%2520state-of-the-art%2520LLMs%2520and%2520humans%252C%250Ahighlighting%2520that%2520there%2520is%2520still%2520a%2520considerable%2520distance%2520to%2520cover%2520in%2520temporal%250Areasoning.%2520Besides%252C%2520LLMs%2520exhibit%2520capability%2520discrepancies%2520across%2520different%250Areasoning%2520categories.%2520Furthermore%252C%2520we%2520thoroughly%2520analyze%2520the%2520impact%2520of%2520multiple%250Aaspects%2520on%2520temporal%2520reasoning%2520and%2520emphasize%2520the%2520associated%2520challenges.%2520We%250Aaspire%2520for%2520TimeBench%2520to%2520serve%2520as%2520a%2520comprehensive%2520benchmark%252C%2520fostering%2520research%250Ain%2520temporal%2520reasoning.%2520Resources%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/zchuz/TimeBench%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.17667v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TimeBench%3A%20A%20Comprehensive%20Evaluation%20of%20Temporal%20Reasoning%20Abilities%20in%0A%20%20Large%20Language%20Models&entry.906535625=Zheng%20Chu%20and%20Jingchang%20Chen%20and%20Qianglong%20Chen%20and%20Weijiang%20Yu%20and%20Haotian%20Wang%20and%20Ming%20Liu%20and%20Bing%20Qin&entry.1292438233=%20%20Grasping%20the%20concept%20of%20time%20is%20a%20fundamental%20facet%20of%20human%20cognition%2C%0Aindispensable%20for%20truly%20comprehending%20the%20intricacies%20of%20the%20world.%20Previous%0Astudies%20typically%20focus%20on%20specific%20aspects%20of%20time%2C%20lacking%20a%20comprehensive%0Atemporal%20reasoning%20benchmark.%20To%20address%20this%2C%20we%20propose%20TimeBench%2C%20a%0Acomprehensive%20hierarchical%20temporal%20reasoning%20benchmark%20that%20covers%20a%20broad%0Aspectrum%20of%20temporal%20reasoning%20phenomena.%20TimeBench%20provides%20a%20thorough%0Aevaluation%20for%20investigating%20the%20temporal%20reasoning%20capabilities%20of%20large%0Alanguage%20models.%20We%20conduct%20extensive%20experiments%20on%20GPT-4%2C%20LLaMA2%2C%20and%20other%0Apopular%20LLMs%20under%20various%20settings.%20Our%20experimental%20results%20indicate%20a%0Asignificant%20performance%20gap%20between%20the%20state-of-the-art%20LLMs%20and%20humans%2C%0Ahighlighting%20that%20there%20is%20still%20a%20considerable%20distance%20to%20cover%20in%20temporal%0Areasoning.%20Besides%2C%20LLMs%20exhibit%20capability%20discrepancies%20across%20different%0Areasoning%20categories.%20Furthermore%2C%20we%20thoroughly%20analyze%20the%20impact%20of%20multiple%0Aaspects%20on%20temporal%20reasoning%20and%20emphasize%20the%20associated%20challenges.%20We%0Aaspire%20for%20TimeBench%20to%20serve%20as%20a%20comprehensive%20benchmark%2C%20fostering%20research%0Ain%20temporal%20reasoning.%20Resources%20are%20available%20at%3A%0Ahttps%3A//github.com/zchuz/TimeBench%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.17667v2&entry.124074799=Read"},
{"title": "AutoMix: Automatically Mixing Language Models", "author": "Pranjal Aggarwal and Aman Madaan and Ankit Anand and Srividya Pranavi Potharaju and Swaroop Mishra and Pei Zhou and Aditya Gupta and Dheeraj Rajagopal and Karthik Kappaganthu and Yiming Yang and Shyam Upadhyay and Manaal Faruqui and  Mausam", "abstract": "  Large language models (LLMs) are now available from cloud API providers in\nvarious sizes and configurations. While this diversity offers a broad spectrum\nof choices, effectively leveraging the options to optimize computational cost\nand performance remains challenging. In this work, we present Automix, an\napproach that strategically routes queries to larger LMs, based on the\napproximate correctness of outputs from a smaller LM. Central to Automix are\ntwo key technical contributions. First, it has a few-shot self-verification\nmechanism, which estimates the reliability of its own outputs without requiring\nextensive training. Second, given that self-verification can be noisy, it\nemploys a POMDP based router that can effectively select an appropriately sized\nmodel, based on answer confidence. Experiments across five language models and\nfive challenging datasets show that Automix consistently surpasses strong\nbaselines, reducing computational cost by over 50% for comparable performance.\n", "link": "http://arxiv.org/abs/2310.12963v4", "date": "2024-06-28", "relevancy": 1.8657, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5086}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4616}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoMix%3A%20Automatically%20Mixing%20Language%20Models&body=Title%3A%20AutoMix%3A%20Automatically%20Mixing%20Language%20Models%0AAuthor%3A%20Pranjal%20Aggarwal%20and%20Aman%20Madaan%20and%20Ankit%20Anand%20and%20Srividya%20Pranavi%20Potharaju%20and%20Swaroop%20Mishra%20and%20Pei%20Zhou%20and%20Aditya%20Gupta%20and%20Dheeraj%20Rajagopal%20and%20Karthik%20Kappaganthu%20and%20Yiming%20Yang%20and%20Shyam%20Upadhyay%20and%20Manaal%20Faruqui%20and%20%20Mausam%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20now%20available%20from%20cloud%20API%20providers%20in%0Avarious%20sizes%20and%20configurations.%20While%20this%20diversity%20offers%20a%20broad%20spectrum%0Aof%20choices%2C%20effectively%20leveraging%20the%20options%20to%20optimize%20computational%20cost%0Aand%20performance%20remains%20challenging.%20In%20this%20work%2C%20we%20present%20Automix%2C%20an%0Aapproach%20that%20strategically%20routes%20queries%20to%20larger%20LMs%2C%20based%20on%20the%0Aapproximate%20correctness%20of%20outputs%20from%20a%20smaller%20LM.%20Central%20to%20Automix%20are%0Atwo%20key%20technical%20contributions.%20First%2C%20it%20has%20a%20few-shot%20self-verification%0Amechanism%2C%20which%20estimates%20the%20reliability%20of%20its%20own%20outputs%20without%20requiring%0Aextensive%20training.%20Second%2C%20given%20that%20self-verification%20can%20be%20noisy%2C%20it%0Aemploys%20a%20POMDP%20based%20router%20that%20can%20effectively%20select%20an%20appropriately%20sized%0Amodel%2C%20based%20on%20answer%20confidence.%20Experiments%20across%20five%20language%20models%20and%0Afive%20challenging%20datasets%20show%20that%20Automix%20consistently%20surpasses%20strong%0Abaselines%2C%20reducing%20computational%20cost%20by%20over%2050%25%20for%20comparable%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.12963v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoMix%253A%2520Automatically%2520Mixing%2520Language%2520Models%26entry.906535625%3DPranjal%2520Aggarwal%2520and%2520Aman%2520Madaan%2520and%2520Ankit%2520Anand%2520and%2520Srividya%2520Pranavi%2520Potharaju%2520and%2520Swaroop%2520Mishra%2520and%2520Pei%2520Zhou%2520and%2520Aditya%2520Gupta%2520and%2520Dheeraj%2520Rajagopal%2520and%2520Karthik%2520Kappaganthu%2520and%2520Yiming%2520Yang%2520and%2520Shyam%2520Upadhyay%2520and%2520Manaal%2520Faruqui%2520and%2520%2520Mausam%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520now%2520available%2520from%2520cloud%2520API%2520providers%2520in%250Avarious%2520sizes%2520and%2520configurations.%2520While%2520this%2520diversity%2520offers%2520a%2520broad%2520spectrum%250Aof%2520choices%252C%2520effectively%2520leveraging%2520the%2520options%2520to%2520optimize%2520computational%2520cost%250Aand%2520performance%2520remains%2520challenging.%2520In%2520this%2520work%252C%2520we%2520present%2520Automix%252C%2520an%250Aapproach%2520that%2520strategically%2520routes%2520queries%2520to%2520larger%2520LMs%252C%2520based%2520on%2520the%250Aapproximate%2520correctness%2520of%2520outputs%2520from%2520a%2520smaller%2520LM.%2520Central%2520to%2520Automix%2520are%250Atwo%2520key%2520technical%2520contributions.%2520First%252C%2520it%2520has%2520a%2520few-shot%2520self-verification%250Amechanism%252C%2520which%2520estimates%2520the%2520reliability%2520of%2520its%2520own%2520outputs%2520without%2520requiring%250Aextensive%2520training.%2520Second%252C%2520given%2520that%2520self-verification%2520can%2520be%2520noisy%252C%2520it%250Aemploys%2520a%2520POMDP%2520based%2520router%2520that%2520can%2520effectively%2520select%2520an%2520appropriately%2520sized%250Amodel%252C%2520based%2520on%2520answer%2520confidence.%2520Experiments%2520across%2520five%2520language%2520models%2520and%250Afive%2520challenging%2520datasets%2520show%2520that%2520Automix%2520consistently%2520surpasses%2520strong%250Abaselines%252C%2520reducing%2520computational%2520cost%2520by%2520over%252050%2525%2520for%2520comparable%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.12963v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoMix%3A%20Automatically%20Mixing%20Language%20Models&entry.906535625=Pranjal%20Aggarwal%20and%20Aman%20Madaan%20and%20Ankit%20Anand%20and%20Srividya%20Pranavi%20Potharaju%20and%20Swaroop%20Mishra%20and%20Pei%20Zhou%20and%20Aditya%20Gupta%20and%20Dheeraj%20Rajagopal%20and%20Karthik%20Kappaganthu%20and%20Yiming%20Yang%20and%20Shyam%20Upadhyay%20and%20Manaal%20Faruqui%20and%20%20Mausam&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20now%20available%20from%20cloud%20API%20providers%20in%0Avarious%20sizes%20and%20configurations.%20While%20this%20diversity%20offers%20a%20broad%20spectrum%0Aof%20choices%2C%20effectively%20leveraging%20the%20options%20to%20optimize%20computational%20cost%0Aand%20performance%20remains%20challenging.%20In%20this%20work%2C%20we%20present%20Automix%2C%20an%0Aapproach%20that%20strategically%20routes%20queries%20to%20larger%20LMs%2C%20based%20on%20the%0Aapproximate%20correctness%20of%20outputs%20from%20a%20smaller%20LM.%20Central%20to%20Automix%20are%0Atwo%20key%20technical%20contributions.%20First%2C%20it%20has%20a%20few-shot%20self-verification%0Amechanism%2C%20which%20estimates%20the%20reliability%20of%20its%20own%20outputs%20without%20requiring%0Aextensive%20training.%20Second%2C%20given%20that%20self-verification%20can%20be%20noisy%2C%20it%0Aemploys%20a%20POMDP%20based%20router%20that%20can%20effectively%20select%20an%20appropriately%20sized%0Amodel%2C%20based%20on%20answer%20confidence.%20Experiments%20across%20five%20language%20models%20and%0Afive%20challenging%20datasets%20show%20that%20Automix%20consistently%20surpasses%20strong%0Abaselines%2C%20reducing%20computational%20cost%20by%20over%2050%25%20for%20comparable%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.12963v4&entry.124074799=Read"},
{"title": "YuLan: An Open-source Large Language Model", "author": "Yutao Zhu and Kun Zhou and Kelong Mao and Wentong Chen and Yiding Sun and Zhipeng Chen and Qian Cao and Yihan Wu and Yushuo Chen and Feng Wang and Lei Zhang and Junyi Li and Xiaolei Wang and Lei Wang and Beichen Zhang and Zican Dong and Xiaoxue Cheng and Yuhan Chen and Xinyu Tang and Yupeng Hou and Qiangqiang Ren and Xincheng Pang and Shufang Xie and Wayne Xin Zhao and Zhicheng Dou and Jiaxin Mao and Yankai Lin and Ruihua Song and Jun Xu and Xu Chen and Rui Yan and Zhewei Wei and Di Hu and Wenbing Huang and Ze-Feng Gao and Yueguo Chen and Weizheng Lu and Ji-Rong Wen", "abstract": "  Large language models (LLMs) have become the foundation of many applications,\nleveraging their extensive capabilities in processing and understanding natural\nlanguage. While many open-source LLMs have been released with technical\nreports, the lack of training details hinders further research and development.\nThis paper presents the development of YuLan, a series of open-source LLMs with\n$12$ billion parameters. The base model of YuLan is pre-trained on\napproximately $1.7$T tokens derived from a diverse corpus, including massive\nEnglish, Chinese, and multilingual texts. We design a three-stage pre-training\nmethod to enhance YuLan's overall capabilities. Subsequent phases of training\nincorporate instruction-tuning and human alignment, employing a substantial\nvolume of high-quality synthesized data. To facilitate the learning of complex\nand long-tail knowledge, we devise a curriculum-learning framework throughout\nacross these stages, which helps LLMs learn knowledge in an easy-to-hard\nmanner. YuLan's training is finished on Jan, 2024 and has achieved performance\non par with state-of-the-art LLMs across various English and Chinese\nbenchmarks. This paper outlines a comprehensive technical roadmap for\ndeveloping LLMs from scratch. Our model and codes are available at\nhttps://github.com/RUC-GSAI/YuLan-Chat.\n", "link": "http://arxiv.org/abs/2406.19853v1", "date": "2024-06-28", "relevancy": 1.864, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4886}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4509}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20YuLan%3A%20An%20Open-source%20Large%20Language%20Model&body=Title%3A%20YuLan%3A%20An%20Open-source%20Large%20Language%20Model%0AAuthor%3A%20Yutao%20Zhu%20and%20Kun%20Zhou%20and%20Kelong%20Mao%20and%20Wentong%20Chen%20and%20Yiding%20Sun%20and%20Zhipeng%20Chen%20and%20Qian%20Cao%20and%20Yihan%20Wu%20and%20Yushuo%20Chen%20and%20Feng%20Wang%20and%20Lei%20Zhang%20and%20Junyi%20Li%20and%20Xiaolei%20Wang%20and%20Lei%20Wang%20and%20Beichen%20Zhang%20and%20Zican%20Dong%20and%20Xiaoxue%20Cheng%20and%20Yuhan%20Chen%20and%20Xinyu%20Tang%20and%20Yupeng%20Hou%20and%20Qiangqiang%20Ren%20and%20Xincheng%20Pang%20and%20Shufang%20Xie%20and%20Wayne%20Xin%20Zhao%20and%20Zhicheng%20Dou%20and%20Jiaxin%20Mao%20and%20Yankai%20Lin%20and%20Ruihua%20Song%20and%20Jun%20Xu%20and%20Xu%20Chen%20and%20Rui%20Yan%20and%20Zhewei%20Wei%20and%20Di%20Hu%20and%20Wenbing%20Huang%20and%20Ze-Feng%20Gao%20and%20Yueguo%20Chen%20and%20Weizheng%20Lu%20and%20Ji-Rong%20Wen%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20become%20the%20foundation%20of%20many%20applications%2C%0Aleveraging%20their%20extensive%20capabilities%20in%20processing%20and%20understanding%20natural%0Alanguage.%20While%20many%20open-source%20LLMs%20have%20been%20released%20with%20technical%0Areports%2C%20the%20lack%20of%20training%20details%20hinders%20further%20research%20and%20development.%0AThis%20paper%20presents%20the%20development%20of%20YuLan%2C%20a%20series%20of%20open-source%20LLMs%20with%0A%2412%24%20billion%20parameters.%20The%20base%20model%20of%20YuLan%20is%20pre-trained%20on%0Aapproximately%20%241.7%24T%20tokens%20derived%20from%20a%20diverse%20corpus%2C%20including%20massive%0AEnglish%2C%20Chinese%2C%20and%20multilingual%20texts.%20We%20design%20a%20three-stage%20pre-training%0Amethod%20to%20enhance%20YuLan%27s%20overall%20capabilities.%20Subsequent%20phases%20of%20training%0Aincorporate%20instruction-tuning%20and%20human%20alignment%2C%20employing%20a%20substantial%0Avolume%20of%20high-quality%20synthesized%20data.%20To%20facilitate%20the%20learning%20of%20complex%0Aand%20long-tail%20knowledge%2C%20we%20devise%20a%20curriculum-learning%20framework%20throughout%0Aacross%20these%20stages%2C%20which%20helps%20LLMs%20learn%20knowledge%20in%20an%20easy-to-hard%0Amanner.%20YuLan%27s%20training%20is%20finished%20on%20Jan%2C%202024%20and%20has%20achieved%20performance%0Aon%20par%20with%20state-of-the-art%20LLMs%20across%20various%20English%20and%20Chinese%0Abenchmarks.%20This%20paper%20outlines%20a%20comprehensive%20technical%20roadmap%20for%0Adeveloping%20LLMs%20from%20scratch.%20Our%20model%20and%20codes%20are%20available%20at%0Ahttps%3A//github.com/RUC-GSAI/YuLan-Chat.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19853v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYuLan%253A%2520An%2520Open-source%2520Large%2520Language%2520Model%26entry.906535625%3DYutao%2520Zhu%2520and%2520Kun%2520Zhou%2520and%2520Kelong%2520Mao%2520and%2520Wentong%2520Chen%2520and%2520Yiding%2520Sun%2520and%2520Zhipeng%2520Chen%2520and%2520Qian%2520Cao%2520and%2520Yihan%2520Wu%2520and%2520Yushuo%2520Chen%2520and%2520Feng%2520Wang%2520and%2520Lei%2520Zhang%2520and%2520Junyi%2520Li%2520and%2520Xiaolei%2520Wang%2520and%2520Lei%2520Wang%2520and%2520Beichen%2520Zhang%2520and%2520Zican%2520Dong%2520and%2520Xiaoxue%2520Cheng%2520and%2520Yuhan%2520Chen%2520and%2520Xinyu%2520Tang%2520and%2520Yupeng%2520Hou%2520and%2520Qiangqiang%2520Ren%2520and%2520Xincheng%2520Pang%2520and%2520Shufang%2520Xie%2520and%2520Wayne%2520Xin%2520Zhao%2520and%2520Zhicheng%2520Dou%2520and%2520Jiaxin%2520Mao%2520and%2520Yankai%2520Lin%2520and%2520Ruihua%2520Song%2520and%2520Jun%2520Xu%2520and%2520Xu%2520Chen%2520and%2520Rui%2520Yan%2520and%2520Zhewei%2520Wei%2520and%2520Di%2520Hu%2520and%2520Wenbing%2520Huang%2520and%2520Ze-Feng%2520Gao%2520and%2520Yueguo%2520Chen%2520and%2520Weizheng%2520Lu%2520and%2520Ji-Rong%2520Wen%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520become%2520the%2520foundation%2520of%2520many%2520applications%252C%250Aleveraging%2520their%2520extensive%2520capabilities%2520in%2520processing%2520and%2520understanding%2520natural%250Alanguage.%2520While%2520many%2520open-source%2520LLMs%2520have%2520been%2520released%2520with%2520technical%250Areports%252C%2520the%2520lack%2520of%2520training%2520details%2520hinders%2520further%2520research%2520and%2520development.%250AThis%2520paper%2520presents%2520the%2520development%2520of%2520YuLan%252C%2520a%2520series%2520of%2520open-source%2520LLMs%2520with%250A%252412%2524%2520billion%2520parameters.%2520The%2520base%2520model%2520of%2520YuLan%2520is%2520pre-trained%2520on%250Aapproximately%2520%25241.7%2524T%2520tokens%2520derived%2520from%2520a%2520diverse%2520corpus%252C%2520including%2520massive%250AEnglish%252C%2520Chinese%252C%2520and%2520multilingual%2520texts.%2520We%2520design%2520a%2520three-stage%2520pre-training%250Amethod%2520to%2520enhance%2520YuLan%2527s%2520overall%2520capabilities.%2520Subsequent%2520phases%2520of%2520training%250Aincorporate%2520instruction-tuning%2520and%2520human%2520alignment%252C%2520employing%2520a%2520substantial%250Avolume%2520of%2520high-quality%2520synthesized%2520data.%2520To%2520facilitate%2520the%2520learning%2520of%2520complex%250Aand%2520long-tail%2520knowledge%252C%2520we%2520devise%2520a%2520curriculum-learning%2520framework%2520throughout%250Aacross%2520these%2520stages%252C%2520which%2520helps%2520LLMs%2520learn%2520knowledge%2520in%2520an%2520easy-to-hard%250Amanner.%2520YuLan%2527s%2520training%2520is%2520finished%2520on%2520Jan%252C%25202024%2520and%2520has%2520achieved%2520performance%250Aon%2520par%2520with%2520state-of-the-art%2520LLMs%2520across%2520various%2520English%2520and%2520Chinese%250Abenchmarks.%2520This%2520paper%2520outlines%2520a%2520comprehensive%2520technical%2520roadmap%2520for%250Adeveloping%2520LLMs%2520from%2520scratch.%2520Our%2520model%2520and%2520codes%2520are%2520available%2520at%250Ahttps%253A//github.com/RUC-GSAI/YuLan-Chat.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19853v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=YuLan%3A%20An%20Open-source%20Large%20Language%20Model&entry.906535625=Yutao%20Zhu%20and%20Kun%20Zhou%20and%20Kelong%20Mao%20and%20Wentong%20Chen%20and%20Yiding%20Sun%20and%20Zhipeng%20Chen%20and%20Qian%20Cao%20and%20Yihan%20Wu%20and%20Yushuo%20Chen%20and%20Feng%20Wang%20and%20Lei%20Zhang%20and%20Junyi%20Li%20and%20Xiaolei%20Wang%20and%20Lei%20Wang%20and%20Beichen%20Zhang%20and%20Zican%20Dong%20and%20Xiaoxue%20Cheng%20and%20Yuhan%20Chen%20and%20Xinyu%20Tang%20and%20Yupeng%20Hou%20and%20Qiangqiang%20Ren%20and%20Xincheng%20Pang%20and%20Shufang%20Xie%20and%20Wayne%20Xin%20Zhao%20and%20Zhicheng%20Dou%20and%20Jiaxin%20Mao%20and%20Yankai%20Lin%20and%20Ruihua%20Song%20and%20Jun%20Xu%20and%20Xu%20Chen%20and%20Rui%20Yan%20and%20Zhewei%20Wei%20and%20Di%20Hu%20and%20Wenbing%20Huang%20and%20Ze-Feng%20Gao%20and%20Yueguo%20Chen%20and%20Weizheng%20Lu%20and%20Ji-Rong%20Wen&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20become%20the%20foundation%20of%20many%20applications%2C%0Aleveraging%20their%20extensive%20capabilities%20in%20processing%20and%20understanding%20natural%0Alanguage.%20While%20many%20open-source%20LLMs%20have%20been%20released%20with%20technical%0Areports%2C%20the%20lack%20of%20training%20details%20hinders%20further%20research%20and%20development.%0AThis%20paper%20presents%20the%20development%20of%20YuLan%2C%20a%20series%20of%20open-source%20LLMs%20with%0A%2412%24%20billion%20parameters.%20The%20base%20model%20of%20YuLan%20is%20pre-trained%20on%0Aapproximately%20%241.7%24T%20tokens%20derived%20from%20a%20diverse%20corpus%2C%20including%20massive%0AEnglish%2C%20Chinese%2C%20and%20multilingual%20texts.%20We%20design%20a%20three-stage%20pre-training%0Amethod%20to%20enhance%20YuLan%27s%20overall%20capabilities.%20Subsequent%20phases%20of%20training%0Aincorporate%20instruction-tuning%20and%20human%20alignment%2C%20employing%20a%20substantial%0Avolume%20of%20high-quality%20synthesized%20data.%20To%20facilitate%20the%20learning%20of%20complex%0Aand%20long-tail%20knowledge%2C%20we%20devise%20a%20curriculum-learning%20framework%20throughout%0Aacross%20these%20stages%2C%20which%20helps%20LLMs%20learn%20knowledge%20in%20an%20easy-to-hard%0Amanner.%20YuLan%27s%20training%20is%20finished%20on%20Jan%2C%202024%20and%20has%20achieved%20performance%0Aon%20par%20with%20state-of-the-art%20LLMs%20across%20various%20English%20and%20Chinese%0Abenchmarks.%20This%20paper%20outlines%20a%20comprehensive%20technical%20roadmap%20for%0Adeveloping%20LLMs%20from%20scratch.%20Our%20model%20and%20codes%20are%20available%20at%0Ahttps%3A//github.com/RUC-GSAI/YuLan-Chat.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19853v1&entry.124074799=Read"},
{"title": "Deep Maxout Network-based Feature Fusion and Political Tangent Search\n  Optimizer enabled Transfer Learning for Thalassemia Detection", "author": "Hemn Barzan Abdalla and Awder Ahmed and Guoquan Li and Nasser Mustafa and Abdur Rashid Sangi", "abstract": "  Thalassemia is a heritable blood disorder which is the outcome of a genetic\ndefect causing lack of production of hemoglobin polypeptide chains. However,\nthere is less understanding of the precise frequency as well as sharing in\nthese areas. Knowing about the frequency of thalassemia occurrence and\ndependable mutations is thus a significant step in preventing, controlling, and\ntreatment planning. Here, Political Tangent Search Optimizer based Transfer\nLearning (PTSO_TL) is introduced for thalassemia detection. Initially, input\ndata obtained from a particular dataset is normalized in the data normalization\nstage. Quantile normalization is utilized in the data normalization stage, and\nthe data are then passed to the feature fusion phase, in which Weighted\nEuclidean Distance with Deep Maxout Network (DMN) is utilized. Thereafter, data\naugmentation is performed using the oversampling method to increase data\ndimensionality. Lastly, thalassemia detection is carried out by TL, wherein a\nconvolutional neural network (CNN) is utilized with hyperparameters from a\ntrained model such as Xception. TL is tuned by PTSO, and the training algorithm\nPTSO is presented by merging of Political Optimizer (PO) and Tangent Search\nAlgorithm (TSA). Furthermore, PTSO_TL obtained maximal precision, recall, and\nf-measure values of about 94.3%, 96.1%, and 95.2%, respectively.\n", "link": "http://arxiv.org/abs/2308.02029v3", "date": "2024-06-28", "relevancy": 1.86, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4751}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4609}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Maxout%20Network-based%20Feature%20Fusion%20and%20Political%20Tangent%20Search%0A%20%20Optimizer%20enabled%20Transfer%20Learning%20for%20Thalassemia%20Detection&body=Title%3A%20Deep%20Maxout%20Network-based%20Feature%20Fusion%20and%20Political%20Tangent%20Search%0A%20%20Optimizer%20enabled%20Transfer%20Learning%20for%20Thalassemia%20Detection%0AAuthor%3A%20Hemn%20Barzan%20Abdalla%20and%20Awder%20Ahmed%20and%20Guoquan%20Li%20and%20Nasser%20Mustafa%20and%20Abdur%20Rashid%20Sangi%0AAbstract%3A%20%20%20Thalassemia%20is%20a%20heritable%20blood%20disorder%20which%20is%20the%20outcome%20of%20a%20genetic%0Adefect%20causing%20lack%20of%20production%20of%20hemoglobin%20polypeptide%20chains.%20However%2C%0Athere%20is%20less%20understanding%20of%20the%20precise%20frequency%20as%20well%20as%20sharing%20in%0Athese%20areas.%20Knowing%20about%20the%20frequency%20of%20thalassemia%20occurrence%20and%0Adependable%20mutations%20is%20thus%20a%20significant%20step%20in%20preventing%2C%20controlling%2C%20and%0Atreatment%20planning.%20Here%2C%20Political%20Tangent%20Search%20Optimizer%20based%20Transfer%0ALearning%20%28PTSO_TL%29%20is%20introduced%20for%20thalassemia%20detection.%20Initially%2C%20input%0Adata%20obtained%20from%20a%20particular%20dataset%20is%20normalized%20in%20the%20data%20normalization%0Astage.%20Quantile%20normalization%20is%20utilized%20in%20the%20data%20normalization%20stage%2C%20and%0Athe%20data%20are%20then%20passed%20to%20the%20feature%20fusion%20phase%2C%20in%20which%20Weighted%0AEuclidean%20Distance%20with%20Deep%20Maxout%20Network%20%28DMN%29%20is%20utilized.%20Thereafter%2C%20data%0Aaugmentation%20is%20performed%20using%20the%20oversampling%20method%20to%20increase%20data%0Adimensionality.%20Lastly%2C%20thalassemia%20detection%20is%20carried%20out%20by%20TL%2C%20wherein%20a%0Aconvolutional%20neural%20network%20%28CNN%29%20is%20utilized%20with%20hyperparameters%20from%20a%0Atrained%20model%20such%20as%20Xception.%20TL%20is%20tuned%20by%20PTSO%2C%20and%20the%20training%20algorithm%0APTSO%20is%20presented%20by%20merging%20of%20Political%20Optimizer%20%28PO%29%20and%20Tangent%20Search%0AAlgorithm%20%28TSA%29.%20Furthermore%2C%20PTSO_TL%20obtained%20maximal%20precision%2C%20recall%2C%20and%0Af-measure%20values%20of%20about%2094.3%25%2C%2096.1%25%2C%20and%2095.2%25%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.02029v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Maxout%2520Network-based%2520Feature%2520Fusion%2520and%2520Political%2520Tangent%2520Search%250A%2520%2520Optimizer%2520enabled%2520Transfer%2520Learning%2520for%2520Thalassemia%2520Detection%26entry.906535625%3DHemn%2520Barzan%2520Abdalla%2520and%2520Awder%2520Ahmed%2520and%2520Guoquan%2520Li%2520and%2520Nasser%2520Mustafa%2520and%2520Abdur%2520Rashid%2520Sangi%26entry.1292438233%3D%2520%2520Thalassemia%2520is%2520a%2520heritable%2520blood%2520disorder%2520which%2520is%2520the%2520outcome%2520of%2520a%2520genetic%250Adefect%2520causing%2520lack%2520of%2520production%2520of%2520hemoglobin%2520polypeptide%2520chains.%2520However%252C%250Athere%2520is%2520less%2520understanding%2520of%2520the%2520precise%2520frequency%2520as%2520well%2520as%2520sharing%2520in%250Athese%2520areas.%2520Knowing%2520about%2520the%2520frequency%2520of%2520thalassemia%2520occurrence%2520and%250Adependable%2520mutations%2520is%2520thus%2520a%2520significant%2520step%2520in%2520preventing%252C%2520controlling%252C%2520and%250Atreatment%2520planning.%2520Here%252C%2520Political%2520Tangent%2520Search%2520Optimizer%2520based%2520Transfer%250ALearning%2520%2528PTSO_TL%2529%2520is%2520introduced%2520for%2520thalassemia%2520detection.%2520Initially%252C%2520input%250Adata%2520obtained%2520from%2520a%2520particular%2520dataset%2520is%2520normalized%2520in%2520the%2520data%2520normalization%250Astage.%2520Quantile%2520normalization%2520is%2520utilized%2520in%2520the%2520data%2520normalization%2520stage%252C%2520and%250Athe%2520data%2520are%2520then%2520passed%2520to%2520the%2520feature%2520fusion%2520phase%252C%2520in%2520which%2520Weighted%250AEuclidean%2520Distance%2520with%2520Deep%2520Maxout%2520Network%2520%2528DMN%2529%2520is%2520utilized.%2520Thereafter%252C%2520data%250Aaugmentation%2520is%2520performed%2520using%2520the%2520oversampling%2520method%2520to%2520increase%2520data%250Adimensionality.%2520Lastly%252C%2520thalassemia%2520detection%2520is%2520carried%2520out%2520by%2520TL%252C%2520wherein%2520a%250Aconvolutional%2520neural%2520network%2520%2528CNN%2529%2520is%2520utilized%2520with%2520hyperparameters%2520from%2520a%250Atrained%2520model%2520such%2520as%2520Xception.%2520TL%2520is%2520tuned%2520by%2520PTSO%252C%2520and%2520the%2520training%2520algorithm%250APTSO%2520is%2520presented%2520by%2520merging%2520of%2520Political%2520Optimizer%2520%2528PO%2529%2520and%2520Tangent%2520Search%250AAlgorithm%2520%2528TSA%2529.%2520Furthermore%252C%2520PTSO_TL%2520obtained%2520maximal%2520precision%252C%2520recall%252C%2520and%250Af-measure%2520values%2520of%2520about%252094.3%2525%252C%252096.1%2525%252C%2520and%252095.2%2525%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.02029v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Maxout%20Network-based%20Feature%20Fusion%20and%20Political%20Tangent%20Search%0A%20%20Optimizer%20enabled%20Transfer%20Learning%20for%20Thalassemia%20Detection&entry.906535625=Hemn%20Barzan%20Abdalla%20and%20Awder%20Ahmed%20and%20Guoquan%20Li%20and%20Nasser%20Mustafa%20and%20Abdur%20Rashid%20Sangi&entry.1292438233=%20%20Thalassemia%20is%20a%20heritable%20blood%20disorder%20which%20is%20the%20outcome%20of%20a%20genetic%0Adefect%20causing%20lack%20of%20production%20of%20hemoglobin%20polypeptide%20chains.%20However%2C%0Athere%20is%20less%20understanding%20of%20the%20precise%20frequency%20as%20well%20as%20sharing%20in%0Athese%20areas.%20Knowing%20about%20the%20frequency%20of%20thalassemia%20occurrence%20and%0Adependable%20mutations%20is%20thus%20a%20significant%20step%20in%20preventing%2C%20controlling%2C%20and%0Atreatment%20planning.%20Here%2C%20Political%20Tangent%20Search%20Optimizer%20based%20Transfer%0ALearning%20%28PTSO_TL%29%20is%20introduced%20for%20thalassemia%20detection.%20Initially%2C%20input%0Adata%20obtained%20from%20a%20particular%20dataset%20is%20normalized%20in%20the%20data%20normalization%0Astage.%20Quantile%20normalization%20is%20utilized%20in%20the%20data%20normalization%20stage%2C%20and%0Athe%20data%20are%20then%20passed%20to%20the%20feature%20fusion%20phase%2C%20in%20which%20Weighted%0AEuclidean%20Distance%20with%20Deep%20Maxout%20Network%20%28DMN%29%20is%20utilized.%20Thereafter%2C%20data%0Aaugmentation%20is%20performed%20using%20the%20oversampling%20method%20to%20increase%20data%0Adimensionality.%20Lastly%2C%20thalassemia%20detection%20is%20carried%20out%20by%20TL%2C%20wherein%20a%0Aconvolutional%20neural%20network%20%28CNN%29%20is%20utilized%20with%20hyperparameters%20from%20a%0Atrained%20model%20such%20as%20Xception.%20TL%20is%20tuned%20by%20PTSO%2C%20and%20the%20training%20algorithm%0APTSO%20is%20presented%20by%20merging%20of%20Political%20Optimizer%20%28PO%29%20and%20Tangent%20Search%0AAlgorithm%20%28TSA%29.%20Furthermore%2C%20PTSO_TL%20obtained%20maximal%20precision%2C%20recall%2C%20and%0Af-measure%20values%20of%20about%2094.3%25%2C%2096.1%25%2C%20and%2095.2%25%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.02029v3&entry.124074799=Read"},
{"title": "Uncovering the hidden core-periphery structure in hyperbolic networks", "author": "Imran Ansari and Pawanesh Yadav and Niteesh Sahni", "abstract": "  The hyperbolic network models exhibit very fundamental and essential\nfeatures, like small-worldness, scale-freeness, high-clustering coefficient,\nand community structure. In this paper, we comprehensively explore the presence\nof an important feature, the core-periphery structure, in the hyperbolic\nnetwork models, which is often exhibited by real-world networks. We focused on\nwell-known hyperbolic models such as popularity-similarity optimization model\n(PSO) and S1/H2 models and studied core-periphery structures using a\nwell-established method that is based on standard random walk Markov chain\nmodel. The observed core-periphery centralization values indicate that the\ncore-periphery structure can be very pronounced under certain conditions. We\nalso validate our findings by statistically testing for the significance of the\nobserved core-periphery structure in the network geometry. This study extends\nnetwork science and reveals core-periphery insights applicable to various\ndomains, enhancing network performance and resiliency in transportation and\ninformation systems.\n", "link": "http://arxiv.org/abs/2406.19953v1", "date": "2024-06-28", "relevancy": 1.8483, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.3743}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3721}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.3625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncovering%20the%20hidden%20core-periphery%20structure%20in%20hyperbolic%20networks&body=Title%3A%20Uncovering%20the%20hidden%20core-periphery%20structure%20in%20hyperbolic%20networks%0AAuthor%3A%20Imran%20Ansari%20and%20Pawanesh%20Yadav%20and%20Niteesh%20Sahni%0AAbstract%3A%20%20%20The%20hyperbolic%20network%20models%20exhibit%20very%20fundamental%20and%20essential%0Afeatures%2C%20like%20small-worldness%2C%20scale-freeness%2C%20high-clustering%20coefficient%2C%0Aand%20community%20structure.%20In%20this%20paper%2C%20we%20comprehensively%20explore%20the%20presence%0Aof%20an%20important%20feature%2C%20the%20core-periphery%20structure%2C%20in%20the%20hyperbolic%0Anetwork%20models%2C%20which%20is%20often%20exhibited%20by%20real-world%20networks.%20We%20focused%20on%0Awell-known%20hyperbolic%20models%20such%20as%20popularity-similarity%20optimization%20model%0A%28PSO%29%20and%20S1/H2%20models%20and%20studied%20core-periphery%20structures%20using%20a%0Awell-established%20method%20that%20is%20based%20on%20standard%20random%20walk%20Markov%20chain%0Amodel.%20The%20observed%20core-periphery%20centralization%20values%20indicate%20that%20the%0Acore-periphery%20structure%20can%20be%20very%20pronounced%20under%20certain%20conditions.%20We%0Aalso%20validate%20our%20findings%20by%20statistically%20testing%20for%20the%20significance%20of%20the%0Aobserved%20core-periphery%20structure%20in%20the%20network%20geometry.%20This%20study%20extends%0Anetwork%20science%20and%20reveals%20core-periphery%20insights%20applicable%20to%20various%0Adomains%2C%20enhancing%20network%20performance%20and%20resiliency%20in%20transportation%20and%0Ainformation%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19953v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncovering%2520the%2520hidden%2520core-periphery%2520structure%2520in%2520hyperbolic%2520networks%26entry.906535625%3DImran%2520Ansari%2520and%2520Pawanesh%2520Yadav%2520and%2520Niteesh%2520Sahni%26entry.1292438233%3D%2520%2520The%2520hyperbolic%2520network%2520models%2520exhibit%2520very%2520fundamental%2520and%2520essential%250Afeatures%252C%2520like%2520small-worldness%252C%2520scale-freeness%252C%2520high-clustering%2520coefficient%252C%250Aand%2520community%2520structure.%2520In%2520this%2520paper%252C%2520we%2520comprehensively%2520explore%2520the%2520presence%250Aof%2520an%2520important%2520feature%252C%2520the%2520core-periphery%2520structure%252C%2520in%2520the%2520hyperbolic%250Anetwork%2520models%252C%2520which%2520is%2520often%2520exhibited%2520by%2520real-world%2520networks.%2520We%2520focused%2520on%250Awell-known%2520hyperbolic%2520models%2520such%2520as%2520popularity-similarity%2520optimization%2520model%250A%2528PSO%2529%2520and%2520S1/H2%2520models%2520and%2520studied%2520core-periphery%2520structures%2520using%2520a%250Awell-established%2520method%2520that%2520is%2520based%2520on%2520standard%2520random%2520walk%2520Markov%2520chain%250Amodel.%2520The%2520observed%2520core-periphery%2520centralization%2520values%2520indicate%2520that%2520the%250Acore-periphery%2520structure%2520can%2520be%2520very%2520pronounced%2520under%2520certain%2520conditions.%2520We%250Aalso%2520validate%2520our%2520findings%2520by%2520statistically%2520testing%2520for%2520the%2520significance%2520of%2520the%250Aobserved%2520core-periphery%2520structure%2520in%2520the%2520network%2520geometry.%2520This%2520study%2520extends%250Anetwork%2520science%2520and%2520reveals%2520core-periphery%2520insights%2520applicable%2520to%2520various%250Adomains%252C%2520enhancing%2520network%2520performance%2520and%2520resiliency%2520in%2520transportation%2520and%250Ainformation%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19953v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncovering%20the%20hidden%20core-periphery%20structure%20in%20hyperbolic%20networks&entry.906535625=Imran%20Ansari%20and%20Pawanesh%20Yadav%20and%20Niteesh%20Sahni&entry.1292438233=%20%20The%20hyperbolic%20network%20models%20exhibit%20very%20fundamental%20and%20essential%0Afeatures%2C%20like%20small-worldness%2C%20scale-freeness%2C%20high-clustering%20coefficient%2C%0Aand%20community%20structure.%20In%20this%20paper%2C%20we%20comprehensively%20explore%20the%20presence%0Aof%20an%20important%20feature%2C%20the%20core-periphery%20structure%2C%20in%20the%20hyperbolic%0Anetwork%20models%2C%20which%20is%20often%20exhibited%20by%20real-world%20networks.%20We%20focused%20on%0Awell-known%20hyperbolic%20models%20such%20as%20popularity-similarity%20optimization%20model%0A%28PSO%29%20and%20S1/H2%20models%20and%20studied%20core-periphery%20structures%20using%20a%0Awell-established%20method%20that%20is%20based%20on%20standard%20random%20walk%20Markov%20chain%0Amodel.%20The%20observed%20core-periphery%20centralization%20values%20indicate%20that%20the%0Acore-periphery%20structure%20can%20be%20very%20pronounced%20under%20certain%20conditions.%20We%0Aalso%20validate%20our%20findings%20by%20statistically%20testing%20for%20the%20significance%20of%20the%0Aobserved%20core-periphery%20structure%20in%20the%20network%20geometry.%20This%20study%20extends%0Anetwork%20science%20and%20reveals%20core-periphery%20insights%20applicable%20to%20various%0Adomains%2C%20enhancing%20network%20performance%20and%20resiliency%20in%20transportation%20and%0Ainformation%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19953v1&entry.124074799=Read"},
{"title": "Exploiting Diffusion Prior for Real-World Image Super-Resolution", "author": "Jianyi Wang and Zongsheng Yue and Shangchen Zhou and Kelvin C. K. Chan and Chen Change Loy", "abstract": "  We present a novel approach to leverage prior knowledge encapsulated in\npre-trained text-to-image diffusion models for blind super-resolution (SR).\nSpecifically, by employing our time-aware encoder, we can achieve promising\nrestoration results without altering the pre-trained synthesis model, thereby\npreserving the generative prior and minimizing training cost. To remedy the\nloss of fidelity caused by the inherent stochasticity of diffusion models, we\nemploy a controllable feature wrapping module that allows users to balance\nquality and fidelity by simply adjusting a scalar value during the inference\nprocess. Moreover, we develop a progressive aggregation sampling strategy to\novercome the fixed-size constraints of pre-trained diffusion models, enabling\nadaptation to resolutions of any size. A comprehensive evaluation of our method\nusing both synthetic and real-world benchmarks demonstrates its superiority\nover current state-of-the-art approaches. Code and models are available at\nhttps://github.com/IceClear/StableSR.\n", "link": "http://arxiv.org/abs/2305.07015v4", "date": "2024-06-28", "relevancy": 1.8473, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6602}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6194}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20Diffusion%20Prior%20for%20Real-World%20Image%20Super-Resolution&body=Title%3A%20Exploiting%20Diffusion%20Prior%20for%20Real-World%20Image%20Super-Resolution%0AAuthor%3A%20Jianyi%20Wang%20and%20Zongsheng%20Yue%20and%20Shangchen%20Zhou%20and%20Kelvin%20C.%20K.%20Chan%20and%20Chen%20Change%20Loy%0AAbstract%3A%20%20%20We%20present%20a%20novel%20approach%20to%20leverage%20prior%20knowledge%20encapsulated%20in%0Apre-trained%20text-to-image%20diffusion%20models%20for%20blind%20super-resolution%20%28SR%29.%0ASpecifically%2C%20by%20employing%20our%20time-aware%20encoder%2C%20we%20can%20achieve%20promising%0Arestoration%20results%20without%20altering%20the%20pre-trained%20synthesis%20model%2C%20thereby%0Apreserving%20the%20generative%20prior%20and%20minimizing%20training%20cost.%20To%20remedy%20the%0Aloss%20of%20fidelity%20caused%20by%20the%20inherent%20stochasticity%20of%20diffusion%20models%2C%20we%0Aemploy%20a%20controllable%20feature%20wrapping%20module%20that%20allows%20users%20to%20balance%0Aquality%20and%20fidelity%20by%20simply%20adjusting%20a%20scalar%20value%20during%20the%20inference%0Aprocess.%20Moreover%2C%20we%20develop%20a%20progressive%20aggregation%20sampling%20strategy%20to%0Aovercome%20the%20fixed-size%20constraints%20of%20pre-trained%20diffusion%20models%2C%20enabling%0Aadaptation%20to%20resolutions%20of%20any%20size.%20A%20comprehensive%20evaluation%20of%20our%20method%0Ausing%20both%20synthetic%20and%20real-world%20benchmarks%20demonstrates%20its%20superiority%0Aover%20current%20state-of-the-art%20approaches.%20Code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/IceClear/StableSR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.07015v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520Diffusion%2520Prior%2520for%2520Real-World%2520Image%2520Super-Resolution%26entry.906535625%3DJianyi%2520Wang%2520and%2520Zongsheng%2520Yue%2520and%2520Shangchen%2520Zhou%2520and%2520Kelvin%2520C.%2520K.%2520Chan%2520and%2520Chen%2520Change%2520Loy%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520approach%2520to%2520leverage%2520prior%2520knowledge%2520encapsulated%2520in%250Apre-trained%2520text-to-image%2520diffusion%2520models%2520for%2520blind%2520super-resolution%2520%2528SR%2529.%250ASpecifically%252C%2520by%2520employing%2520our%2520time-aware%2520encoder%252C%2520we%2520can%2520achieve%2520promising%250Arestoration%2520results%2520without%2520altering%2520the%2520pre-trained%2520synthesis%2520model%252C%2520thereby%250Apreserving%2520the%2520generative%2520prior%2520and%2520minimizing%2520training%2520cost.%2520To%2520remedy%2520the%250Aloss%2520of%2520fidelity%2520caused%2520by%2520the%2520inherent%2520stochasticity%2520of%2520diffusion%2520models%252C%2520we%250Aemploy%2520a%2520controllable%2520feature%2520wrapping%2520module%2520that%2520allows%2520users%2520to%2520balance%250Aquality%2520and%2520fidelity%2520by%2520simply%2520adjusting%2520a%2520scalar%2520value%2520during%2520the%2520inference%250Aprocess.%2520Moreover%252C%2520we%2520develop%2520a%2520progressive%2520aggregation%2520sampling%2520strategy%2520to%250Aovercome%2520the%2520fixed-size%2520constraints%2520of%2520pre-trained%2520diffusion%2520models%252C%2520enabling%250Aadaptation%2520to%2520resolutions%2520of%2520any%2520size.%2520A%2520comprehensive%2520evaluation%2520of%2520our%2520method%250Ausing%2520both%2520synthetic%2520and%2520real-world%2520benchmarks%2520demonstrates%2520its%2520superiority%250Aover%2520current%2520state-of-the-art%2520approaches.%2520Code%2520and%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/IceClear/StableSR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.07015v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20Diffusion%20Prior%20for%20Real-World%20Image%20Super-Resolution&entry.906535625=Jianyi%20Wang%20and%20Zongsheng%20Yue%20and%20Shangchen%20Zhou%20and%20Kelvin%20C.%20K.%20Chan%20and%20Chen%20Change%20Loy&entry.1292438233=%20%20We%20present%20a%20novel%20approach%20to%20leverage%20prior%20knowledge%20encapsulated%20in%0Apre-trained%20text-to-image%20diffusion%20models%20for%20blind%20super-resolution%20%28SR%29.%0ASpecifically%2C%20by%20employing%20our%20time-aware%20encoder%2C%20we%20can%20achieve%20promising%0Arestoration%20results%20without%20altering%20the%20pre-trained%20synthesis%20model%2C%20thereby%0Apreserving%20the%20generative%20prior%20and%20minimizing%20training%20cost.%20To%20remedy%20the%0Aloss%20of%20fidelity%20caused%20by%20the%20inherent%20stochasticity%20of%20diffusion%20models%2C%20we%0Aemploy%20a%20controllable%20feature%20wrapping%20module%20that%20allows%20users%20to%20balance%0Aquality%20and%20fidelity%20by%20simply%20adjusting%20a%20scalar%20value%20during%20the%20inference%0Aprocess.%20Moreover%2C%20we%20develop%20a%20progressive%20aggregation%20sampling%20strategy%20to%0Aovercome%20the%20fixed-size%20constraints%20of%20pre-trained%20diffusion%20models%2C%20enabling%0Aadaptation%20to%20resolutions%20of%20any%20size.%20A%20comprehensive%20evaluation%20of%20our%20method%0Ausing%20both%20synthetic%20and%20real-world%20benchmarks%20demonstrates%20its%20superiority%0Aover%20current%20state-of-the-art%20approaches.%20Code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/IceClear/StableSR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.07015v4&entry.124074799=Read"},
{"title": "A Simple Mixture Policy Parameterization for Improving Sample Efficiency\n  of CVaR Optimization", "author": "Yudong Luo and Yangchen Pan and Han Wang and Philip Torr and Pascal Poupart", "abstract": "  Reinforcement learning algorithms utilizing policy gradients (PG) to optimize\nConditional Value at Risk (CVaR) face significant challenges with sample\ninefficiency, hindering their practical applications. This inefficiency stems\nfrom two main facts: a focus on tail-end performance that overlooks many\nsampled trajectories, and the potential of gradient vanishing when the lower\ntail of the return distribution is overly flat. To address these challenges, we\npropose a simple mixture policy parameterization. This method integrates a\nrisk-neutral policy with an adjustable policy to form a risk-averse policy. By\nemploying this strategy, all collected trajectories can be utilized for policy\nupdating, and the issue of vanishing gradients is counteracted by stimulating\nhigher returns through the risk-neutral component, thus lifting the tail and\npreventing flatness. Our empirical study reveals that this mixture\nparameterization is uniquely effective across a variety of benchmark domains.\nSpecifically, it excels in identifying risk-averse CVaR policies in some Mujoco\nenvironments where the traditional CVaR-PG fails to learn a reasonable policy.\n", "link": "http://arxiv.org/abs/2403.11062v3", "date": "2024-06-28", "relevancy": 1.8437, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4876}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4582}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Simple%20Mixture%20Policy%20Parameterization%20for%20Improving%20Sample%20Efficiency%0A%20%20of%20CVaR%20Optimization&body=Title%3A%20A%20Simple%20Mixture%20Policy%20Parameterization%20for%20Improving%20Sample%20Efficiency%0A%20%20of%20CVaR%20Optimization%0AAuthor%3A%20Yudong%20Luo%20and%20Yangchen%20Pan%20and%20Han%20Wang%20and%20Philip%20Torr%20and%20Pascal%20Poupart%0AAbstract%3A%20%20%20Reinforcement%20learning%20algorithms%20utilizing%20policy%20gradients%20%28PG%29%20to%20optimize%0AConditional%20Value%20at%20Risk%20%28CVaR%29%20face%20significant%20challenges%20with%20sample%0Ainefficiency%2C%20hindering%20their%20practical%20applications.%20This%20inefficiency%20stems%0Afrom%20two%20main%20facts%3A%20a%20focus%20on%20tail-end%20performance%20that%20overlooks%20many%0Asampled%20trajectories%2C%20and%20the%20potential%20of%20gradient%20vanishing%20when%20the%20lower%0Atail%20of%20the%20return%20distribution%20is%20overly%20flat.%20To%20address%20these%20challenges%2C%20we%0Apropose%20a%20simple%20mixture%20policy%20parameterization.%20This%20method%20integrates%20a%0Arisk-neutral%20policy%20with%20an%20adjustable%20policy%20to%20form%20a%20risk-averse%20policy.%20By%0Aemploying%20this%20strategy%2C%20all%20collected%20trajectories%20can%20be%20utilized%20for%20policy%0Aupdating%2C%20and%20the%20issue%20of%20vanishing%20gradients%20is%20counteracted%20by%20stimulating%0Ahigher%20returns%20through%20the%20risk-neutral%20component%2C%20thus%20lifting%20the%20tail%20and%0Apreventing%20flatness.%20Our%20empirical%20study%20reveals%20that%20this%20mixture%0Aparameterization%20is%20uniquely%20effective%20across%20a%20variety%20of%20benchmark%20domains.%0ASpecifically%2C%20it%20excels%20in%20identifying%20risk-averse%20CVaR%20policies%20in%20some%20Mujoco%0Aenvironments%20where%20the%20traditional%20CVaR-PG%20fails%20to%20learn%20a%20reasonable%20policy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11062v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Simple%2520Mixture%2520Policy%2520Parameterization%2520for%2520Improving%2520Sample%2520Efficiency%250A%2520%2520of%2520CVaR%2520Optimization%26entry.906535625%3DYudong%2520Luo%2520and%2520Yangchen%2520Pan%2520and%2520Han%2520Wang%2520and%2520Philip%2520Torr%2520and%2520Pascal%2520Poupart%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520algorithms%2520utilizing%2520policy%2520gradients%2520%2528PG%2529%2520to%2520optimize%250AConditional%2520Value%2520at%2520Risk%2520%2528CVaR%2529%2520face%2520significant%2520challenges%2520with%2520sample%250Ainefficiency%252C%2520hindering%2520their%2520practical%2520applications.%2520This%2520inefficiency%2520stems%250Afrom%2520two%2520main%2520facts%253A%2520a%2520focus%2520on%2520tail-end%2520performance%2520that%2520overlooks%2520many%250Asampled%2520trajectories%252C%2520and%2520the%2520potential%2520of%2520gradient%2520vanishing%2520when%2520the%2520lower%250Atail%2520of%2520the%2520return%2520distribution%2520is%2520overly%2520flat.%2520To%2520address%2520these%2520challenges%252C%2520we%250Apropose%2520a%2520simple%2520mixture%2520policy%2520parameterization.%2520This%2520method%2520integrates%2520a%250Arisk-neutral%2520policy%2520with%2520an%2520adjustable%2520policy%2520to%2520form%2520a%2520risk-averse%2520policy.%2520By%250Aemploying%2520this%2520strategy%252C%2520all%2520collected%2520trajectories%2520can%2520be%2520utilized%2520for%2520policy%250Aupdating%252C%2520and%2520the%2520issue%2520of%2520vanishing%2520gradients%2520is%2520counteracted%2520by%2520stimulating%250Ahigher%2520returns%2520through%2520the%2520risk-neutral%2520component%252C%2520thus%2520lifting%2520the%2520tail%2520and%250Apreventing%2520flatness.%2520Our%2520empirical%2520study%2520reveals%2520that%2520this%2520mixture%250Aparameterization%2520is%2520uniquely%2520effective%2520across%2520a%2520variety%2520of%2520benchmark%2520domains.%250ASpecifically%252C%2520it%2520excels%2520in%2520identifying%2520risk-averse%2520CVaR%2520policies%2520in%2520some%2520Mujoco%250Aenvironments%2520where%2520the%2520traditional%2520CVaR-PG%2520fails%2520to%2520learn%2520a%2520reasonable%2520policy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.11062v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Simple%20Mixture%20Policy%20Parameterization%20for%20Improving%20Sample%20Efficiency%0A%20%20of%20CVaR%20Optimization&entry.906535625=Yudong%20Luo%20and%20Yangchen%20Pan%20and%20Han%20Wang%20and%20Philip%20Torr%20and%20Pascal%20Poupart&entry.1292438233=%20%20Reinforcement%20learning%20algorithms%20utilizing%20policy%20gradients%20%28PG%29%20to%20optimize%0AConditional%20Value%20at%20Risk%20%28CVaR%29%20face%20significant%20challenges%20with%20sample%0Ainefficiency%2C%20hindering%20their%20practical%20applications.%20This%20inefficiency%20stems%0Afrom%20two%20main%20facts%3A%20a%20focus%20on%20tail-end%20performance%20that%20overlooks%20many%0Asampled%20trajectories%2C%20and%20the%20potential%20of%20gradient%20vanishing%20when%20the%20lower%0Atail%20of%20the%20return%20distribution%20is%20overly%20flat.%20To%20address%20these%20challenges%2C%20we%0Apropose%20a%20simple%20mixture%20policy%20parameterization.%20This%20method%20integrates%20a%0Arisk-neutral%20policy%20with%20an%20adjustable%20policy%20to%20form%20a%20risk-averse%20policy.%20By%0Aemploying%20this%20strategy%2C%20all%20collected%20trajectories%20can%20be%20utilized%20for%20policy%0Aupdating%2C%20and%20the%20issue%20of%20vanishing%20gradients%20is%20counteracted%20by%20stimulating%0Ahigher%20returns%20through%20the%20risk-neutral%20component%2C%20thus%20lifting%20the%20tail%20and%0Apreventing%20flatness.%20Our%20empirical%20study%20reveals%20that%20this%20mixture%0Aparameterization%20is%20uniquely%20effective%20across%20a%20variety%20of%20benchmark%20domains.%0ASpecifically%2C%20it%20excels%20in%20identifying%20risk-averse%20CVaR%20policies%20in%20some%20Mujoco%0Aenvironments%20where%20the%20traditional%20CVaR-PG%20fails%20to%20learn%20a%20reasonable%20policy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11062v3&entry.124074799=Read"},
{"title": "Koopman based trajectory model and computation offloading for high\n  mobility paradigm in ISAC enabled IoT system", "author": "Minh-Tuan Tran", "abstract": "  User experience on mobile devices is constrained by limited battery capacity\nand processing power, but 6G technology advancements are diving rapidly into\nmobile technical evolution. Mobile edge computing (MEC) offers a solution,\noffloading computationally intensive tasks to edge cloud servers, reducing\nbattery drain compared to local processing. The upcoming integrated sensing and\ncommunication in mobile communication may improve the trajectory prediction and\nprocessing delays. This study proposes a greedy resource allocation\noptimization strategy for multi-user networks to minimize aggregate energy\nusage. Numerical results show potential improvement at 33\\% for every 1000\niteration. Addressing prediction model division and velocity accuracy issues is\ncrucial for better results. A plan for further improvement and achieving\nobjectives is outlined for the upcoming work phase.\n", "link": "http://arxiv.org/abs/2406.19871v1", "date": "2024-06-28", "relevancy": 1.8327, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4947}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.457}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Koopman%20based%20trajectory%20model%20and%20computation%20offloading%20for%20high%0A%20%20mobility%20paradigm%20in%20ISAC%20enabled%20IoT%20system&body=Title%3A%20Koopman%20based%20trajectory%20model%20and%20computation%20offloading%20for%20high%0A%20%20mobility%20paradigm%20in%20ISAC%20enabled%20IoT%20system%0AAuthor%3A%20Minh-Tuan%20Tran%0AAbstract%3A%20%20%20User%20experience%20on%20mobile%20devices%20is%20constrained%20by%20limited%20battery%20capacity%0Aand%20processing%20power%2C%20but%206G%20technology%20advancements%20are%20diving%20rapidly%20into%0Amobile%20technical%20evolution.%20Mobile%20edge%20computing%20%28MEC%29%20offers%20a%20solution%2C%0Aoffloading%20computationally%20intensive%20tasks%20to%20edge%20cloud%20servers%2C%20reducing%0Abattery%20drain%20compared%20to%20local%20processing.%20The%20upcoming%20integrated%20sensing%20and%0Acommunication%20in%20mobile%20communication%20may%20improve%20the%20trajectory%20prediction%20and%0Aprocessing%20delays.%20This%20study%20proposes%20a%20greedy%20resource%20allocation%0Aoptimization%20strategy%20for%20multi-user%20networks%20to%20minimize%20aggregate%20energy%0Ausage.%20Numerical%20results%20show%20potential%20improvement%20at%2033%5C%25%20for%20every%201000%0Aiteration.%20Addressing%20prediction%20model%20division%20and%20velocity%20accuracy%20issues%20is%0Acrucial%20for%20better%20results.%20A%20plan%20for%20further%20improvement%20and%20achieving%0Aobjectives%20is%20outlined%20for%20the%20upcoming%20work%20phase.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19871v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKoopman%2520based%2520trajectory%2520model%2520and%2520computation%2520offloading%2520for%2520high%250A%2520%2520mobility%2520paradigm%2520in%2520ISAC%2520enabled%2520IoT%2520system%26entry.906535625%3DMinh-Tuan%2520Tran%26entry.1292438233%3D%2520%2520User%2520experience%2520on%2520mobile%2520devices%2520is%2520constrained%2520by%2520limited%2520battery%2520capacity%250Aand%2520processing%2520power%252C%2520but%25206G%2520technology%2520advancements%2520are%2520diving%2520rapidly%2520into%250Amobile%2520technical%2520evolution.%2520Mobile%2520edge%2520computing%2520%2528MEC%2529%2520offers%2520a%2520solution%252C%250Aoffloading%2520computationally%2520intensive%2520tasks%2520to%2520edge%2520cloud%2520servers%252C%2520reducing%250Abattery%2520drain%2520compared%2520to%2520local%2520processing.%2520The%2520upcoming%2520integrated%2520sensing%2520and%250Acommunication%2520in%2520mobile%2520communication%2520may%2520improve%2520the%2520trajectory%2520prediction%2520and%250Aprocessing%2520delays.%2520This%2520study%2520proposes%2520a%2520greedy%2520resource%2520allocation%250Aoptimization%2520strategy%2520for%2520multi-user%2520networks%2520to%2520minimize%2520aggregate%2520energy%250Ausage.%2520Numerical%2520results%2520show%2520potential%2520improvement%2520at%252033%255C%2525%2520for%2520every%25201000%250Aiteration.%2520Addressing%2520prediction%2520model%2520division%2520and%2520velocity%2520accuracy%2520issues%2520is%250Acrucial%2520for%2520better%2520results.%2520A%2520plan%2520for%2520further%2520improvement%2520and%2520achieving%250Aobjectives%2520is%2520outlined%2520for%2520the%2520upcoming%2520work%2520phase.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19871v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Koopman%20based%20trajectory%20model%20and%20computation%20offloading%20for%20high%0A%20%20mobility%20paradigm%20in%20ISAC%20enabled%20IoT%20system&entry.906535625=Minh-Tuan%20Tran&entry.1292438233=%20%20User%20experience%20on%20mobile%20devices%20is%20constrained%20by%20limited%20battery%20capacity%0Aand%20processing%20power%2C%20but%206G%20technology%20advancements%20are%20diving%20rapidly%20into%0Amobile%20technical%20evolution.%20Mobile%20edge%20computing%20%28MEC%29%20offers%20a%20solution%2C%0Aoffloading%20computationally%20intensive%20tasks%20to%20edge%20cloud%20servers%2C%20reducing%0Abattery%20drain%20compared%20to%20local%20processing.%20The%20upcoming%20integrated%20sensing%20and%0Acommunication%20in%20mobile%20communication%20may%20improve%20the%20trajectory%20prediction%20and%0Aprocessing%20delays.%20This%20study%20proposes%20a%20greedy%20resource%20allocation%0Aoptimization%20strategy%20for%20multi-user%20networks%20to%20minimize%20aggregate%20energy%0Ausage.%20Numerical%20results%20show%20potential%20improvement%20at%2033%5C%25%20for%20every%201000%0Aiteration.%20Addressing%20prediction%20model%20division%20and%20velocity%20accuracy%20issues%20is%0Acrucial%20for%20better%20results.%20A%20plan%20for%20further%20improvement%20and%20achieving%0Aobjectives%20is%20outlined%20for%20the%20upcoming%20work%20phase.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19871v1&entry.124074799=Read"},
{"title": "Decoupling General and Personalized Knowledge in Federated Learning via\n  Additive and Low-Rank Decomposition", "author": "Xinghao Wu and Xuefeng Liu and Jianwei Niu and Haolin Wang and Shaojie Tang and Guogang Zhu and Hao Su", "abstract": "  To address data heterogeneity, the key strategy of Personalized Federated\nLearning (PFL) is to decouple general knowledge (shared among clients) and\nclient-specific knowledge, as the latter can have a negative impact on\ncollaboration if not removed. Existing PFL methods primarily adopt a parameter\npartitioning approach, where the parameters of a model are designated as one of\ntwo types: parameters shared with other clients to extract general knowledge\nand parameters retained locally to learn client-specific knowledge. However, as\nthese two types of parameters are put together like a jigsaw puzzle into a\nsingle model during the training process, each parameter may simultaneously\nabsorb both general and client-specific knowledge, thus struggling to separate\nthe two types of knowledge effectively. In this paper, we introduce FedDecomp,\na simple but effective PFL paradigm that employs parameter additive\ndecomposition to address this issue. Instead of assigning each parameter of a\nmodel as either a shared or personalized one, FedDecomp decomposes each\nparameter into the sum of two parameters: a shared one and a personalized one,\nthus achieving a more thorough decoupling of shared and personalized knowledge\ncompared to the parameter partitioning method. In addition, as we find that\nretaining local knowledge of specific clients requires much lower model\ncapacity compared with general knowledge across all clients, we let the matrix\ncontaining personalized parameters be low rank during the training process.\nMoreover, a new alternating training strategy is proposed to further improve\nthe performance. Experimental results across multiple datasets and varying\ndegrees of data heterogeneity demonstrate that FedDecomp outperforms\nstate-of-the-art methods up to 4.9\\%.\n", "link": "http://arxiv.org/abs/2406.19931v1", "date": "2024-06-28", "relevancy": 1.8285, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4641}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.459}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoupling%20General%20and%20Personalized%20Knowledge%20in%20Federated%20Learning%20via%0A%20%20Additive%20and%20Low-Rank%20Decomposition&body=Title%3A%20Decoupling%20General%20and%20Personalized%20Knowledge%20in%20Federated%20Learning%20via%0A%20%20Additive%20and%20Low-Rank%20Decomposition%0AAuthor%3A%20Xinghao%20Wu%20and%20Xuefeng%20Liu%20and%20Jianwei%20Niu%20and%20Haolin%20Wang%20and%20Shaojie%20Tang%20and%20Guogang%20Zhu%20and%20Hao%20Su%0AAbstract%3A%20%20%20To%20address%20data%20heterogeneity%2C%20the%20key%20strategy%20of%20Personalized%20Federated%0ALearning%20%28PFL%29%20is%20to%20decouple%20general%20knowledge%20%28shared%20among%20clients%29%20and%0Aclient-specific%20knowledge%2C%20as%20the%20latter%20can%20have%20a%20negative%20impact%20on%0Acollaboration%20if%20not%20removed.%20Existing%20PFL%20methods%20primarily%20adopt%20a%20parameter%0Apartitioning%20approach%2C%20where%20the%20parameters%20of%20a%20model%20are%20designated%20as%20one%20of%0Atwo%20types%3A%20parameters%20shared%20with%20other%20clients%20to%20extract%20general%20knowledge%0Aand%20parameters%20retained%20locally%20to%20learn%20client-specific%20knowledge.%20However%2C%20as%0Athese%20two%20types%20of%20parameters%20are%20put%20together%20like%20a%20jigsaw%20puzzle%20into%20a%0Asingle%20model%20during%20the%20training%20process%2C%20each%20parameter%20may%20simultaneously%0Aabsorb%20both%20general%20and%20client-specific%20knowledge%2C%20thus%20struggling%20to%20separate%0Athe%20two%20types%20of%20knowledge%20effectively.%20In%20this%20paper%2C%20we%20introduce%20FedDecomp%2C%0Aa%20simple%20but%20effective%20PFL%20paradigm%20that%20employs%20parameter%20additive%0Adecomposition%20to%20address%20this%20issue.%20Instead%20of%20assigning%20each%20parameter%20of%20a%0Amodel%20as%20either%20a%20shared%20or%20personalized%20one%2C%20FedDecomp%20decomposes%20each%0Aparameter%20into%20the%20sum%20of%20two%20parameters%3A%20a%20shared%20one%20and%20a%20personalized%20one%2C%0Athus%20achieving%20a%20more%20thorough%20decoupling%20of%20shared%20and%20personalized%20knowledge%0Acompared%20to%20the%20parameter%20partitioning%20method.%20In%20addition%2C%20as%20we%20find%20that%0Aretaining%20local%20knowledge%20of%20specific%20clients%20requires%20much%20lower%20model%0Acapacity%20compared%20with%20general%20knowledge%20across%20all%20clients%2C%20we%20let%20the%20matrix%0Acontaining%20personalized%20parameters%20be%20low%20rank%20during%20the%20training%20process.%0AMoreover%2C%20a%20new%20alternating%20training%20strategy%20is%20proposed%20to%20further%20improve%0Athe%20performance.%20Experimental%20results%20across%20multiple%20datasets%20and%20varying%0Adegrees%20of%20data%20heterogeneity%20demonstrate%20that%20FedDecomp%20outperforms%0Astate-of-the-art%20methods%20up%20to%204.9%5C%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19931v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoupling%2520General%2520and%2520Personalized%2520Knowledge%2520in%2520Federated%2520Learning%2520via%250A%2520%2520Additive%2520and%2520Low-Rank%2520Decomposition%26entry.906535625%3DXinghao%2520Wu%2520and%2520Xuefeng%2520Liu%2520and%2520Jianwei%2520Niu%2520and%2520Haolin%2520Wang%2520and%2520Shaojie%2520Tang%2520and%2520Guogang%2520Zhu%2520and%2520Hao%2520Su%26entry.1292438233%3D%2520%2520To%2520address%2520data%2520heterogeneity%252C%2520the%2520key%2520strategy%2520of%2520Personalized%2520Federated%250ALearning%2520%2528PFL%2529%2520is%2520to%2520decouple%2520general%2520knowledge%2520%2528shared%2520among%2520clients%2529%2520and%250Aclient-specific%2520knowledge%252C%2520as%2520the%2520latter%2520can%2520have%2520a%2520negative%2520impact%2520on%250Acollaboration%2520if%2520not%2520removed.%2520Existing%2520PFL%2520methods%2520primarily%2520adopt%2520a%2520parameter%250Apartitioning%2520approach%252C%2520where%2520the%2520parameters%2520of%2520a%2520model%2520are%2520designated%2520as%2520one%2520of%250Atwo%2520types%253A%2520parameters%2520shared%2520with%2520other%2520clients%2520to%2520extract%2520general%2520knowledge%250Aand%2520parameters%2520retained%2520locally%2520to%2520learn%2520client-specific%2520knowledge.%2520However%252C%2520as%250Athese%2520two%2520types%2520of%2520parameters%2520are%2520put%2520together%2520like%2520a%2520jigsaw%2520puzzle%2520into%2520a%250Asingle%2520model%2520during%2520the%2520training%2520process%252C%2520each%2520parameter%2520may%2520simultaneously%250Aabsorb%2520both%2520general%2520and%2520client-specific%2520knowledge%252C%2520thus%2520struggling%2520to%2520separate%250Athe%2520two%2520types%2520of%2520knowledge%2520effectively.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520FedDecomp%252C%250Aa%2520simple%2520but%2520effective%2520PFL%2520paradigm%2520that%2520employs%2520parameter%2520additive%250Adecomposition%2520to%2520address%2520this%2520issue.%2520Instead%2520of%2520assigning%2520each%2520parameter%2520of%2520a%250Amodel%2520as%2520either%2520a%2520shared%2520or%2520personalized%2520one%252C%2520FedDecomp%2520decomposes%2520each%250Aparameter%2520into%2520the%2520sum%2520of%2520two%2520parameters%253A%2520a%2520shared%2520one%2520and%2520a%2520personalized%2520one%252C%250Athus%2520achieving%2520a%2520more%2520thorough%2520decoupling%2520of%2520shared%2520and%2520personalized%2520knowledge%250Acompared%2520to%2520the%2520parameter%2520partitioning%2520method.%2520In%2520addition%252C%2520as%2520we%2520find%2520that%250Aretaining%2520local%2520knowledge%2520of%2520specific%2520clients%2520requires%2520much%2520lower%2520model%250Acapacity%2520compared%2520with%2520general%2520knowledge%2520across%2520all%2520clients%252C%2520we%2520let%2520the%2520matrix%250Acontaining%2520personalized%2520parameters%2520be%2520low%2520rank%2520during%2520the%2520training%2520process.%250AMoreover%252C%2520a%2520new%2520alternating%2520training%2520strategy%2520is%2520proposed%2520to%2520further%2520improve%250Athe%2520performance.%2520Experimental%2520results%2520across%2520multiple%2520datasets%2520and%2520varying%250Adegrees%2520of%2520data%2520heterogeneity%2520demonstrate%2520that%2520FedDecomp%2520outperforms%250Astate-of-the-art%2520methods%2520up%2520to%25204.9%255C%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19931v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoupling%20General%20and%20Personalized%20Knowledge%20in%20Federated%20Learning%20via%0A%20%20Additive%20and%20Low-Rank%20Decomposition&entry.906535625=Xinghao%20Wu%20and%20Xuefeng%20Liu%20and%20Jianwei%20Niu%20and%20Haolin%20Wang%20and%20Shaojie%20Tang%20and%20Guogang%20Zhu%20and%20Hao%20Su&entry.1292438233=%20%20To%20address%20data%20heterogeneity%2C%20the%20key%20strategy%20of%20Personalized%20Federated%0ALearning%20%28PFL%29%20is%20to%20decouple%20general%20knowledge%20%28shared%20among%20clients%29%20and%0Aclient-specific%20knowledge%2C%20as%20the%20latter%20can%20have%20a%20negative%20impact%20on%0Acollaboration%20if%20not%20removed.%20Existing%20PFL%20methods%20primarily%20adopt%20a%20parameter%0Apartitioning%20approach%2C%20where%20the%20parameters%20of%20a%20model%20are%20designated%20as%20one%20of%0Atwo%20types%3A%20parameters%20shared%20with%20other%20clients%20to%20extract%20general%20knowledge%0Aand%20parameters%20retained%20locally%20to%20learn%20client-specific%20knowledge.%20However%2C%20as%0Athese%20two%20types%20of%20parameters%20are%20put%20together%20like%20a%20jigsaw%20puzzle%20into%20a%0Asingle%20model%20during%20the%20training%20process%2C%20each%20parameter%20may%20simultaneously%0Aabsorb%20both%20general%20and%20client-specific%20knowledge%2C%20thus%20struggling%20to%20separate%0Athe%20two%20types%20of%20knowledge%20effectively.%20In%20this%20paper%2C%20we%20introduce%20FedDecomp%2C%0Aa%20simple%20but%20effective%20PFL%20paradigm%20that%20employs%20parameter%20additive%0Adecomposition%20to%20address%20this%20issue.%20Instead%20of%20assigning%20each%20parameter%20of%20a%0Amodel%20as%20either%20a%20shared%20or%20personalized%20one%2C%20FedDecomp%20decomposes%20each%0Aparameter%20into%20the%20sum%20of%20two%20parameters%3A%20a%20shared%20one%20and%20a%20personalized%20one%2C%0Athus%20achieving%20a%20more%20thorough%20decoupling%20of%20shared%20and%20personalized%20knowledge%0Acompared%20to%20the%20parameter%20partitioning%20method.%20In%20addition%2C%20as%20we%20find%20that%0Aretaining%20local%20knowledge%20of%20specific%20clients%20requires%20much%20lower%20model%0Acapacity%20compared%20with%20general%20knowledge%20across%20all%20clients%2C%20we%20let%20the%20matrix%0Acontaining%20personalized%20parameters%20be%20low%20rank%20during%20the%20training%20process.%0AMoreover%2C%20a%20new%20alternating%20training%20strategy%20is%20proposed%20to%20further%20improve%0Athe%20performance.%20Experimental%20results%20across%20multiple%20datasets%20and%20varying%0Adegrees%20of%20data%20heterogeneity%20demonstrate%20that%20FedDecomp%20outperforms%0Astate-of-the-art%20methods%20up%20to%204.9%5C%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19931v1&entry.124074799=Read"},
{"title": "Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs", "author": "Sheridan Feucht and David Atkinson and Byron Wallace and David Bau", "abstract": "  LLMs process text as sequences of tokens that roughly correspond to words,\nwhere less common words are represented by multiple tokens. However, individual\ntokens are often semantically unrelated to the meanings of the words/concepts\nthey comprise. For example, Llama-2-7b's tokenizer splits the word\n\"northeastern\" into the tokens ['_n', 'ort', 'he', 'astern'], none of which\ncorrespond to semantically meaningful units like \"north\" or \"east.\" Similarly,\nthe overall meanings of named entities like \"Neil Young\" and multi-word\nexpressions like \"break a leg\" cannot be directly inferred from their\nconstituent tokens. Mechanistically, how do LLMs convert such arbitrary groups\nof tokens into useful higher-level representations? In this work, we find that\nlast token representations of named entities and multi-token words exhibit a\npronounced \"erasure\" effect, where information about previous and current\ntokens is rapidly forgotten in early layers. Using this observation, we propose\na method to \"read out\" the implicit vocabulary of an autoregressive LLM by\nexamining differences in token representations across layers, and present\nresults of this method for Llama-2-7b and Llama-3-8B. To our knowledge, this is\nthe first attempt to probe the implicit vocabulary of an LLM.\n", "link": "http://arxiv.org/abs/2406.20086v1", "date": "2024-06-28", "relevancy": 1.8088, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4721}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4399}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Token%20Erasure%20as%20a%20Footprint%20of%20Implicit%20Vocabulary%20Items%20in%20LLMs&body=Title%3A%20Token%20Erasure%20as%20a%20Footprint%20of%20Implicit%20Vocabulary%20Items%20in%20LLMs%0AAuthor%3A%20Sheridan%20Feucht%20and%20David%20Atkinson%20and%20Byron%20Wallace%20and%20David%20Bau%0AAbstract%3A%20%20%20LLMs%20process%20text%20as%20sequences%20of%20tokens%20that%20roughly%20correspond%20to%20words%2C%0Awhere%20less%20common%20words%20are%20represented%20by%20multiple%20tokens.%20However%2C%20individual%0Atokens%20are%20often%20semantically%20unrelated%20to%20the%20meanings%20of%20the%20words/concepts%0Athey%20comprise.%20For%20example%2C%20Llama-2-7b%27s%20tokenizer%20splits%20the%20word%0A%22northeastern%22%20into%20the%20tokens%20%5B%27_n%27%2C%20%27ort%27%2C%20%27he%27%2C%20%27astern%27%5D%2C%20none%20of%20which%0Acorrespond%20to%20semantically%20meaningful%20units%20like%20%22north%22%20or%20%22east.%22%20Similarly%2C%0Athe%20overall%20meanings%20of%20named%20entities%20like%20%22Neil%20Young%22%20and%20multi-word%0Aexpressions%20like%20%22break%20a%20leg%22%20cannot%20be%20directly%20inferred%20from%20their%0Aconstituent%20tokens.%20Mechanistically%2C%20how%20do%20LLMs%20convert%20such%20arbitrary%20groups%0Aof%20tokens%20into%20useful%20higher-level%20representations%3F%20In%20this%20work%2C%20we%20find%20that%0Alast%20token%20representations%20of%20named%20entities%20and%20multi-token%20words%20exhibit%20a%0Apronounced%20%22erasure%22%20effect%2C%20where%20information%20about%20previous%20and%20current%0Atokens%20is%20rapidly%20forgotten%20in%20early%20layers.%20Using%20this%20observation%2C%20we%20propose%0Aa%20method%20to%20%22read%20out%22%20the%20implicit%20vocabulary%20of%20an%20autoregressive%20LLM%20by%0Aexamining%20differences%20in%20token%20representations%20across%20layers%2C%20and%20present%0Aresults%20of%20this%20method%20for%20Llama-2-7b%20and%20Llama-3-8B.%20To%20our%20knowledge%2C%20this%20is%0Athe%20first%20attempt%20to%20probe%20the%20implicit%20vocabulary%20of%20an%20LLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.20086v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToken%2520Erasure%2520as%2520a%2520Footprint%2520of%2520Implicit%2520Vocabulary%2520Items%2520in%2520LLMs%26entry.906535625%3DSheridan%2520Feucht%2520and%2520David%2520Atkinson%2520and%2520Byron%2520Wallace%2520and%2520David%2520Bau%26entry.1292438233%3D%2520%2520LLMs%2520process%2520text%2520as%2520sequences%2520of%2520tokens%2520that%2520roughly%2520correspond%2520to%2520words%252C%250Awhere%2520less%2520common%2520words%2520are%2520represented%2520by%2520multiple%2520tokens.%2520However%252C%2520individual%250Atokens%2520are%2520often%2520semantically%2520unrelated%2520to%2520the%2520meanings%2520of%2520the%2520words/concepts%250Athey%2520comprise.%2520For%2520example%252C%2520Llama-2-7b%2527s%2520tokenizer%2520splits%2520the%2520word%250A%2522northeastern%2522%2520into%2520the%2520tokens%2520%255B%2527_n%2527%252C%2520%2527ort%2527%252C%2520%2527he%2527%252C%2520%2527astern%2527%255D%252C%2520none%2520of%2520which%250Acorrespond%2520to%2520semantically%2520meaningful%2520units%2520like%2520%2522north%2522%2520or%2520%2522east.%2522%2520Similarly%252C%250Athe%2520overall%2520meanings%2520of%2520named%2520entities%2520like%2520%2522Neil%2520Young%2522%2520and%2520multi-word%250Aexpressions%2520like%2520%2522break%2520a%2520leg%2522%2520cannot%2520be%2520directly%2520inferred%2520from%2520their%250Aconstituent%2520tokens.%2520Mechanistically%252C%2520how%2520do%2520LLMs%2520convert%2520such%2520arbitrary%2520groups%250Aof%2520tokens%2520into%2520useful%2520higher-level%2520representations%253F%2520In%2520this%2520work%252C%2520we%2520find%2520that%250Alast%2520token%2520representations%2520of%2520named%2520entities%2520and%2520multi-token%2520words%2520exhibit%2520a%250Apronounced%2520%2522erasure%2522%2520effect%252C%2520where%2520information%2520about%2520previous%2520and%2520current%250Atokens%2520is%2520rapidly%2520forgotten%2520in%2520early%2520layers.%2520Using%2520this%2520observation%252C%2520we%2520propose%250Aa%2520method%2520to%2520%2522read%2520out%2522%2520the%2520implicit%2520vocabulary%2520of%2520an%2520autoregressive%2520LLM%2520by%250Aexamining%2520differences%2520in%2520token%2520representations%2520across%2520layers%252C%2520and%2520present%250Aresults%2520of%2520this%2520method%2520for%2520Llama-2-7b%2520and%2520Llama-3-8B.%2520To%2520our%2520knowledge%252C%2520this%2520is%250Athe%2520first%2520attempt%2520to%2520probe%2520the%2520implicit%2520vocabulary%2520of%2520an%2520LLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.20086v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Token%20Erasure%20as%20a%20Footprint%20of%20Implicit%20Vocabulary%20Items%20in%20LLMs&entry.906535625=Sheridan%20Feucht%20and%20David%20Atkinson%20and%20Byron%20Wallace%20and%20David%20Bau&entry.1292438233=%20%20LLMs%20process%20text%20as%20sequences%20of%20tokens%20that%20roughly%20correspond%20to%20words%2C%0Awhere%20less%20common%20words%20are%20represented%20by%20multiple%20tokens.%20However%2C%20individual%0Atokens%20are%20often%20semantically%20unrelated%20to%20the%20meanings%20of%20the%20words/concepts%0Athey%20comprise.%20For%20example%2C%20Llama-2-7b%27s%20tokenizer%20splits%20the%20word%0A%22northeastern%22%20into%20the%20tokens%20%5B%27_n%27%2C%20%27ort%27%2C%20%27he%27%2C%20%27astern%27%5D%2C%20none%20of%20which%0Acorrespond%20to%20semantically%20meaningful%20units%20like%20%22north%22%20or%20%22east.%22%20Similarly%2C%0Athe%20overall%20meanings%20of%20named%20entities%20like%20%22Neil%20Young%22%20and%20multi-word%0Aexpressions%20like%20%22break%20a%20leg%22%20cannot%20be%20directly%20inferred%20from%20their%0Aconstituent%20tokens.%20Mechanistically%2C%20how%20do%20LLMs%20convert%20such%20arbitrary%20groups%0Aof%20tokens%20into%20useful%20higher-level%20representations%3F%20In%20this%20work%2C%20we%20find%20that%0Alast%20token%20representations%20of%20named%20entities%20and%20multi-token%20words%20exhibit%20a%0Apronounced%20%22erasure%22%20effect%2C%20where%20information%20about%20previous%20and%20current%0Atokens%20is%20rapidly%20forgotten%20in%20early%20layers.%20Using%20this%20observation%2C%20we%20propose%0Aa%20method%20to%20%22read%20out%22%20the%20implicit%20vocabulary%20of%20an%20autoregressive%20LLM%20by%0Aexamining%20differences%20in%20token%20representations%20across%20layers%2C%20and%20present%0Aresults%20of%20this%20method%20for%20Llama-2-7b%20and%20Llama-3-8B.%20To%20our%20knowledge%2C%20this%20is%0Athe%20first%20attempt%20to%20probe%20the%20implicit%20vocabulary%20of%20an%20LLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.20086v1&entry.124074799=Read"},
{"title": "Covert Malicious Finetuning: Challenges in Safeguarding LLM Adaptation", "author": "Danny Halawi and Alexander Wei and Eric Wallace and Tony T. Wang and Nika Haghtalab and Jacob Steinhardt", "abstract": "  Black-box finetuning is an emerging interface for adapting state-of-the-art\nlanguage models to user needs. However, such access may also let malicious\nactors undermine model safety. To demonstrate the challenge of defending\nfinetuning interfaces, we introduce covert malicious finetuning, a method to\ncompromise model safety via finetuning while evading detection. Our method\nconstructs a malicious dataset where every individual datapoint appears\ninnocuous, but finetuning on the dataset teaches the model to respond to\nencoded harmful requests with encoded harmful responses. Applied to GPT-4, our\nmethod produces a finetuned model that acts on harmful instructions 99% of the\ntime and avoids detection by defense mechanisms such as dataset inspection,\nsafety evaluations, and input/output classifiers. Our findings question whether\nblack-box finetuning access can be secured against sophisticated adversaries.\n", "link": "http://arxiv.org/abs/2406.20053v1", "date": "2024-06-28", "relevancy": 1.7923, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4531}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4477}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Covert%20Malicious%20Finetuning%3A%20Challenges%20in%20Safeguarding%20LLM%20Adaptation&body=Title%3A%20Covert%20Malicious%20Finetuning%3A%20Challenges%20in%20Safeguarding%20LLM%20Adaptation%0AAuthor%3A%20Danny%20Halawi%20and%20Alexander%20Wei%20and%20Eric%20Wallace%20and%20Tony%20T.%20Wang%20and%20Nika%20Haghtalab%20and%20Jacob%20Steinhardt%0AAbstract%3A%20%20%20Black-box%20finetuning%20is%20an%20emerging%20interface%20for%20adapting%20state-of-the-art%0Alanguage%20models%20to%20user%20needs.%20However%2C%20such%20access%20may%20also%20let%20malicious%0Aactors%20undermine%20model%20safety.%20To%20demonstrate%20the%20challenge%20of%20defending%0Afinetuning%20interfaces%2C%20we%20introduce%20covert%20malicious%20finetuning%2C%20a%20method%20to%0Acompromise%20model%20safety%20via%20finetuning%20while%20evading%20detection.%20Our%20method%0Aconstructs%20a%20malicious%20dataset%20where%20every%20individual%20datapoint%20appears%0Ainnocuous%2C%20but%20finetuning%20on%20the%20dataset%20teaches%20the%20model%20to%20respond%20to%0Aencoded%20harmful%20requests%20with%20encoded%20harmful%20responses.%20Applied%20to%20GPT-4%2C%20our%0Amethod%20produces%20a%20finetuned%20model%20that%20acts%20on%20harmful%20instructions%2099%25%20of%20the%0Atime%20and%20avoids%20detection%20by%20defense%20mechanisms%20such%20as%20dataset%20inspection%2C%0Asafety%20evaluations%2C%20and%20input/output%20classifiers.%20Our%20findings%20question%20whether%0Ablack-box%20finetuning%20access%20can%20be%20secured%20against%20sophisticated%20adversaries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.20053v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCovert%2520Malicious%2520Finetuning%253A%2520Challenges%2520in%2520Safeguarding%2520LLM%2520Adaptation%26entry.906535625%3DDanny%2520Halawi%2520and%2520Alexander%2520Wei%2520and%2520Eric%2520Wallace%2520and%2520Tony%2520T.%2520Wang%2520and%2520Nika%2520Haghtalab%2520and%2520Jacob%2520Steinhardt%26entry.1292438233%3D%2520%2520Black-box%2520finetuning%2520is%2520an%2520emerging%2520interface%2520for%2520adapting%2520state-of-the-art%250Alanguage%2520models%2520to%2520user%2520needs.%2520However%252C%2520such%2520access%2520may%2520also%2520let%2520malicious%250Aactors%2520undermine%2520model%2520safety.%2520To%2520demonstrate%2520the%2520challenge%2520of%2520defending%250Afinetuning%2520interfaces%252C%2520we%2520introduce%2520covert%2520malicious%2520finetuning%252C%2520a%2520method%2520to%250Acompromise%2520model%2520safety%2520via%2520finetuning%2520while%2520evading%2520detection.%2520Our%2520method%250Aconstructs%2520a%2520malicious%2520dataset%2520where%2520every%2520individual%2520datapoint%2520appears%250Ainnocuous%252C%2520but%2520finetuning%2520on%2520the%2520dataset%2520teaches%2520the%2520model%2520to%2520respond%2520to%250Aencoded%2520harmful%2520requests%2520with%2520encoded%2520harmful%2520responses.%2520Applied%2520to%2520GPT-4%252C%2520our%250Amethod%2520produces%2520a%2520finetuned%2520model%2520that%2520acts%2520on%2520harmful%2520instructions%252099%2525%2520of%2520the%250Atime%2520and%2520avoids%2520detection%2520by%2520defense%2520mechanisms%2520such%2520as%2520dataset%2520inspection%252C%250Asafety%2520evaluations%252C%2520and%2520input/output%2520classifiers.%2520Our%2520findings%2520question%2520whether%250Ablack-box%2520finetuning%2520access%2520can%2520be%2520secured%2520against%2520sophisticated%2520adversaries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.20053v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Covert%20Malicious%20Finetuning%3A%20Challenges%20in%20Safeguarding%20LLM%20Adaptation&entry.906535625=Danny%20Halawi%20and%20Alexander%20Wei%20and%20Eric%20Wallace%20and%20Tony%20T.%20Wang%20and%20Nika%20Haghtalab%20and%20Jacob%20Steinhardt&entry.1292438233=%20%20Black-box%20finetuning%20is%20an%20emerging%20interface%20for%20adapting%20state-of-the-art%0Alanguage%20models%20to%20user%20needs.%20However%2C%20such%20access%20may%20also%20let%20malicious%0Aactors%20undermine%20model%20safety.%20To%20demonstrate%20the%20challenge%20of%20defending%0Afinetuning%20interfaces%2C%20we%20introduce%20covert%20malicious%20finetuning%2C%20a%20method%20to%0Acompromise%20model%20safety%20via%20finetuning%20while%20evading%20detection.%20Our%20method%0Aconstructs%20a%20malicious%20dataset%20where%20every%20individual%20datapoint%20appears%0Ainnocuous%2C%20but%20finetuning%20on%20the%20dataset%20teaches%20the%20model%20to%20respond%20to%0Aencoded%20harmful%20requests%20with%20encoded%20harmful%20responses.%20Applied%20to%20GPT-4%2C%20our%0Amethod%20produces%20a%20finetuned%20model%20that%20acts%20on%20harmful%20instructions%2099%25%20of%20the%0Atime%20and%20avoids%20detection%20by%20defense%20mechanisms%20such%20as%20dataset%20inspection%2C%0Asafety%20evaluations%2C%20and%20input/output%20classifiers.%20Our%20findings%20question%20whether%0Ablack-box%20finetuning%20access%20can%20be%20secured%20against%20sophisticated%20adversaries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.20053v1&entry.124074799=Read"},
{"title": "The Computational Curse of Big Data for Bayesian Additive Regression\n  Trees: A Hitting Time Analysis", "author": "Yan Shuo Tan and Omer Ronen and Theo Saarinen and Bin Yu", "abstract": "  Bayesian Additive Regression Trees (BART) is a popular Bayesian\nnon-parametric regression model that is commonly used in causal inference and\nbeyond. Its strong predictive performance is supported by theoretical\nguarantees that its posterior distribution concentrates around the true\nregression function at optimal rates under various data generative settings and\nfor appropriate prior choices. In this paper, we show that the BART sampler\noften converges slowly, confirming empirical observations by other researchers.\nAssuming discrete covariates, we show that, while the BART posterior\nconcentrates on a set comprising all optimal tree structures (smallest bias and\ncomplexity), the Markov chain's hitting time for this set increases with $n$\n(training sample size), under several common data generative settings. As $n$\nincreases, the approximate BART posterior thus becomes increasingly different\nfrom the exact posterior (for the same number of MCMC samples), contrasting\nwith earlier concentration results on the exact posterior. This contrast is\nhighlighted by our simulations showing worsening frequentist undercoverage for\napproximate posterior intervals and a growing ratio between the MSE of the\napproximate posterior and that obtainable by artificially improving convergence\nvia averaging multiple sampler chains. Finally, based on our theoretical\ninsights, possibilities are discussed to improve the BART sampler convergence\nperformance.\n", "link": "http://arxiv.org/abs/2406.19958v1", "date": "2024-06-28", "relevancy": 1.7821, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4936}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4499}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Computational%20Curse%20of%20Big%20Data%20for%20Bayesian%20Additive%20Regression%0A%20%20Trees%3A%20A%20Hitting%20Time%20Analysis&body=Title%3A%20The%20Computational%20Curse%20of%20Big%20Data%20for%20Bayesian%20Additive%20Regression%0A%20%20Trees%3A%20A%20Hitting%20Time%20Analysis%0AAuthor%3A%20Yan%20Shuo%20Tan%20and%20Omer%20Ronen%20and%20Theo%20Saarinen%20and%20Bin%20Yu%0AAbstract%3A%20%20%20Bayesian%20Additive%20Regression%20Trees%20%28BART%29%20is%20a%20popular%20Bayesian%0Anon-parametric%20regression%20model%20that%20is%20commonly%20used%20in%20causal%20inference%20and%0Abeyond.%20Its%20strong%20predictive%20performance%20is%20supported%20by%20theoretical%0Aguarantees%20that%20its%20posterior%20distribution%20concentrates%20around%20the%20true%0Aregression%20function%20at%20optimal%20rates%20under%20various%20data%20generative%20settings%20and%0Afor%20appropriate%20prior%20choices.%20In%20this%20paper%2C%20we%20show%20that%20the%20BART%20sampler%0Aoften%20converges%20slowly%2C%20confirming%20empirical%20observations%20by%20other%20researchers.%0AAssuming%20discrete%20covariates%2C%20we%20show%20that%2C%20while%20the%20BART%20posterior%0Aconcentrates%20on%20a%20set%20comprising%20all%20optimal%20tree%20structures%20%28smallest%20bias%20and%0Acomplexity%29%2C%20the%20Markov%20chain%27s%20hitting%20time%20for%20this%20set%20increases%20with%20%24n%24%0A%28training%20sample%20size%29%2C%20under%20several%20common%20data%20generative%20settings.%20As%20%24n%24%0Aincreases%2C%20the%20approximate%20BART%20posterior%20thus%20becomes%20increasingly%20different%0Afrom%20the%20exact%20posterior%20%28for%20the%20same%20number%20of%20MCMC%20samples%29%2C%20contrasting%0Awith%20earlier%20concentration%20results%20on%20the%20exact%20posterior.%20This%20contrast%20is%0Ahighlighted%20by%20our%20simulations%20showing%20worsening%20frequentist%20undercoverage%20for%0Aapproximate%20posterior%20intervals%20and%20a%20growing%20ratio%20between%20the%20MSE%20of%20the%0Aapproximate%20posterior%20and%20that%20obtainable%20by%20artificially%20improving%20convergence%0Avia%20averaging%20multiple%20sampler%20chains.%20Finally%2C%20based%20on%20our%20theoretical%0Ainsights%2C%20possibilities%20are%20discussed%20to%20improve%20the%20BART%20sampler%20convergence%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19958v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Computational%2520Curse%2520of%2520Big%2520Data%2520for%2520Bayesian%2520Additive%2520Regression%250A%2520%2520Trees%253A%2520A%2520Hitting%2520Time%2520Analysis%26entry.906535625%3DYan%2520Shuo%2520Tan%2520and%2520Omer%2520Ronen%2520and%2520Theo%2520Saarinen%2520and%2520Bin%2520Yu%26entry.1292438233%3D%2520%2520Bayesian%2520Additive%2520Regression%2520Trees%2520%2528BART%2529%2520is%2520a%2520popular%2520Bayesian%250Anon-parametric%2520regression%2520model%2520that%2520is%2520commonly%2520used%2520in%2520causal%2520inference%2520and%250Abeyond.%2520Its%2520strong%2520predictive%2520performance%2520is%2520supported%2520by%2520theoretical%250Aguarantees%2520that%2520its%2520posterior%2520distribution%2520concentrates%2520around%2520the%2520true%250Aregression%2520function%2520at%2520optimal%2520rates%2520under%2520various%2520data%2520generative%2520settings%2520and%250Afor%2520appropriate%2520prior%2520choices.%2520In%2520this%2520paper%252C%2520we%2520show%2520that%2520the%2520BART%2520sampler%250Aoften%2520converges%2520slowly%252C%2520confirming%2520empirical%2520observations%2520by%2520other%2520researchers.%250AAssuming%2520discrete%2520covariates%252C%2520we%2520show%2520that%252C%2520while%2520the%2520BART%2520posterior%250Aconcentrates%2520on%2520a%2520set%2520comprising%2520all%2520optimal%2520tree%2520structures%2520%2528smallest%2520bias%2520and%250Acomplexity%2529%252C%2520the%2520Markov%2520chain%2527s%2520hitting%2520time%2520for%2520this%2520set%2520increases%2520with%2520%2524n%2524%250A%2528training%2520sample%2520size%2529%252C%2520under%2520several%2520common%2520data%2520generative%2520settings.%2520As%2520%2524n%2524%250Aincreases%252C%2520the%2520approximate%2520BART%2520posterior%2520thus%2520becomes%2520increasingly%2520different%250Afrom%2520the%2520exact%2520posterior%2520%2528for%2520the%2520same%2520number%2520of%2520MCMC%2520samples%2529%252C%2520contrasting%250Awith%2520earlier%2520concentration%2520results%2520on%2520the%2520exact%2520posterior.%2520This%2520contrast%2520is%250Ahighlighted%2520by%2520our%2520simulations%2520showing%2520worsening%2520frequentist%2520undercoverage%2520for%250Aapproximate%2520posterior%2520intervals%2520and%2520a%2520growing%2520ratio%2520between%2520the%2520MSE%2520of%2520the%250Aapproximate%2520posterior%2520and%2520that%2520obtainable%2520by%2520artificially%2520improving%2520convergence%250Avia%2520averaging%2520multiple%2520sampler%2520chains.%2520Finally%252C%2520based%2520on%2520our%2520theoretical%250Ainsights%252C%2520possibilities%2520are%2520discussed%2520to%2520improve%2520the%2520BART%2520sampler%2520convergence%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19958v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Computational%20Curse%20of%20Big%20Data%20for%20Bayesian%20Additive%20Regression%0A%20%20Trees%3A%20A%20Hitting%20Time%20Analysis&entry.906535625=Yan%20Shuo%20Tan%20and%20Omer%20Ronen%20and%20Theo%20Saarinen%20and%20Bin%20Yu&entry.1292438233=%20%20Bayesian%20Additive%20Regression%20Trees%20%28BART%29%20is%20a%20popular%20Bayesian%0Anon-parametric%20regression%20model%20that%20is%20commonly%20used%20in%20causal%20inference%20and%0Abeyond.%20Its%20strong%20predictive%20performance%20is%20supported%20by%20theoretical%0Aguarantees%20that%20its%20posterior%20distribution%20concentrates%20around%20the%20true%0Aregression%20function%20at%20optimal%20rates%20under%20various%20data%20generative%20settings%20and%0Afor%20appropriate%20prior%20choices.%20In%20this%20paper%2C%20we%20show%20that%20the%20BART%20sampler%0Aoften%20converges%20slowly%2C%20confirming%20empirical%20observations%20by%20other%20researchers.%0AAssuming%20discrete%20covariates%2C%20we%20show%20that%2C%20while%20the%20BART%20posterior%0Aconcentrates%20on%20a%20set%20comprising%20all%20optimal%20tree%20structures%20%28smallest%20bias%20and%0Acomplexity%29%2C%20the%20Markov%20chain%27s%20hitting%20time%20for%20this%20set%20increases%20with%20%24n%24%0A%28training%20sample%20size%29%2C%20under%20several%20common%20data%20generative%20settings.%20As%20%24n%24%0Aincreases%2C%20the%20approximate%20BART%20posterior%20thus%20becomes%20increasingly%20different%0Afrom%20the%20exact%20posterior%20%28for%20the%20same%20number%20of%20MCMC%20samples%29%2C%20contrasting%0Awith%20earlier%20concentration%20results%20on%20the%20exact%20posterior.%20This%20contrast%20is%0Ahighlighted%20by%20our%20simulations%20showing%20worsening%20frequentist%20undercoverage%20for%0Aapproximate%20posterior%20intervals%20and%20a%20growing%20ratio%20between%20the%20MSE%20of%20the%0Aapproximate%20posterior%20and%20that%20obtainable%20by%20artificially%20improving%20convergence%0Avia%20averaging%20multiple%20sampler%20chains.%20Finally%2C%20based%20on%20our%20theoretical%0Ainsights%2C%20possibilities%20are%20discussed%20to%20improve%20the%20BART%20sampler%20convergence%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19958v1&entry.124074799=Read"},
{"title": "Pairwise Difference Learning for Classification", "author": "Mohamed Karim Belaid and Maximilian Rabus and Eyke H\u00fcllermeier", "abstract": "  Pairwise difference learning (PDL) has recently been introduced as a new\nmeta-learning technique for regression. Instead of learning a mapping from\ninstances to outcomes in the standard way, the key idea is to learn a function\nthat takes two instances as input and predicts the difference between the\nrespective outcomes. Given a function of this kind, predictions for a query\ninstance are derived from every training example and then averaged. This paper\nextends PDL toward the task of classification and proposes a meta-learning\ntechnique for inducing a PDL classifier by solving a suitably defined (binary)\nclassification problem on a paired version of the original training data. We\nanalyze the performance of the PDL classifier in a large-scale empirical study\nand find that it outperforms state-of-the-art methods in terms of prediction\nperformance. Last but not least, we provide an easy-to-use and publicly\navailable implementation of PDL in a Python package.\n", "link": "http://arxiv.org/abs/2406.20031v1", "date": "2024-06-28", "relevancy": 1.7778, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.451}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4461}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4401}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pairwise%20Difference%20Learning%20for%20Classification&body=Title%3A%20Pairwise%20Difference%20Learning%20for%20Classification%0AAuthor%3A%20Mohamed%20Karim%20Belaid%20and%20Maximilian%20Rabus%20and%20Eyke%20H%C3%BCllermeier%0AAbstract%3A%20%20%20Pairwise%20difference%20learning%20%28PDL%29%20has%20recently%20been%20introduced%20as%20a%20new%0Ameta-learning%20technique%20for%20regression.%20Instead%20of%20learning%20a%20mapping%20from%0Ainstances%20to%20outcomes%20in%20the%20standard%20way%2C%20the%20key%20idea%20is%20to%20learn%20a%20function%0Athat%20takes%20two%20instances%20as%20input%20and%20predicts%20the%20difference%20between%20the%0Arespective%20outcomes.%20Given%20a%20function%20of%20this%20kind%2C%20predictions%20for%20a%20query%0Ainstance%20are%20derived%20from%20every%20training%20example%20and%20then%20averaged.%20This%20paper%0Aextends%20PDL%20toward%20the%20task%20of%20classification%20and%20proposes%20a%20meta-learning%0Atechnique%20for%20inducing%20a%20PDL%20classifier%20by%20solving%20a%20suitably%20defined%20%28binary%29%0Aclassification%20problem%20on%20a%20paired%20version%20of%20the%20original%20training%20data.%20We%0Aanalyze%20the%20performance%20of%20the%20PDL%20classifier%20in%20a%20large-scale%20empirical%20study%0Aand%20find%20that%20it%20outperforms%20state-of-the-art%20methods%20in%20terms%20of%20prediction%0Aperformance.%20Last%20but%20not%20least%2C%20we%20provide%20an%20easy-to-use%20and%20publicly%0Aavailable%20implementation%20of%20PDL%20in%20a%20Python%20package.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.20031v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPairwise%2520Difference%2520Learning%2520for%2520Classification%26entry.906535625%3DMohamed%2520Karim%2520Belaid%2520and%2520Maximilian%2520Rabus%2520and%2520Eyke%2520H%25C3%25BCllermeier%26entry.1292438233%3D%2520%2520Pairwise%2520difference%2520learning%2520%2528PDL%2529%2520has%2520recently%2520been%2520introduced%2520as%2520a%2520new%250Ameta-learning%2520technique%2520for%2520regression.%2520Instead%2520of%2520learning%2520a%2520mapping%2520from%250Ainstances%2520to%2520outcomes%2520in%2520the%2520standard%2520way%252C%2520the%2520key%2520idea%2520is%2520to%2520learn%2520a%2520function%250Athat%2520takes%2520two%2520instances%2520as%2520input%2520and%2520predicts%2520the%2520difference%2520between%2520the%250Arespective%2520outcomes.%2520Given%2520a%2520function%2520of%2520this%2520kind%252C%2520predictions%2520for%2520a%2520query%250Ainstance%2520are%2520derived%2520from%2520every%2520training%2520example%2520and%2520then%2520averaged.%2520This%2520paper%250Aextends%2520PDL%2520toward%2520the%2520task%2520of%2520classification%2520and%2520proposes%2520a%2520meta-learning%250Atechnique%2520for%2520inducing%2520a%2520PDL%2520classifier%2520by%2520solving%2520a%2520suitably%2520defined%2520%2528binary%2529%250Aclassification%2520problem%2520on%2520a%2520paired%2520version%2520of%2520the%2520original%2520training%2520data.%2520We%250Aanalyze%2520the%2520performance%2520of%2520the%2520PDL%2520classifier%2520in%2520a%2520large-scale%2520empirical%2520study%250Aand%2520find%2520that%2520it%2520outperforms%2520state-of-the-art%2520methods%2520in%2520terms%2520of%2520prediction%250Aperformance.%2520Last%2520but%2520not%2520least%252C%2520we%2520provide%2520an%2520easy-to-use%2520and%2520publicly%250Aavailable%2520implementation%2520of%2520PDL%2520in%2520a%2520Python%2520package.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.20031v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pairwise%20Difference%20Learning%20for%20Classification&entry.906535625=Mohamed%20Karim%20Belaid%20and%20Maximilian%20Rabus%20and%20Eyke%20H%C3%BCllermeier&entry.1292438233=%20%20Pairwise%20difference%20learning%20%28PDL%29%20has%20recently%20been%20introduced%20as%20a%20new%0Ameta-learning%20technique%20for%20regression.%20Instead%20of%20learning%20a%20mapping%20from%0Ainstances%20to%20outcomes%20in%20the%20standard%20way%2C%20the%20key%20idea%20is%20to%20learn%20a%20function%0Athat%20takes%20two%20instances%20as%20input%20and%20predicts%20the%20difference%20between%20the%0Arespective%20outcomes.%20Given%20a%20function%20of%20this%20kind%2C%20predictions%20for%20a%20query%0Ainstance%20are%20derived%20from%20every%20training%20example%20and%20then%20averaged.%20This%20paper%0Aextends%20PDL%20toward%20the%20task%20of%20classification%20and%20proposes%20a%20meta-learning%0Atechnique%20for%20inducing%20a%20PDL%20classifier%20by%20solving%20a%20suitably%20defined%20%28binary%29%0Aclassification%20problem%20on%20a%20paired%20version%20of%20the%20original%20training%20data.%20We%0Aanalyze%20the%20performance%20of%20the%20PDL%20classifier%20in%20a%20large-scale%20empirical%20study%0Aand%20find%20that%20it%20outperforms%20state-of-the-art%20methods%20in%20terms%20of%20prediction%0Aperformance.%20Last%20but%20not%20least%2C%20we%20provide%20an%20easy-to-use%20and%20publicly%0Aavailable%20implementation%20of%20PDL%20in%20a%20Python%20package.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.20031v1&entry.124074799=Read"},
{"title": "Straggler-Resilient Differentially-Private Decentralized Learning", "author": "Yauhen Yakimenka and Chung-Wei Weng and Hsuan-Yin Lin and Eirik Rosnes and J\u00f6rg Kliewer", "abstract": "  We consider the straggler problem in decentralized learning over a logical\nring while preserving user data privacy. Especially, we extend the recently\nproposed framework of differential privacy (DP) amplification by\ndecentralization by Cyffers and Bellet to include overall training\nlatency--comprising both computation and communication latency. Analytical\nresults on both the convergence speed and the DP level are derived for both a\nskipping scheme (which ignores the stragglers after a timeout) and a baseline\nscheme that waits for each node to finish before the training continues. A\ntrade-off between overall training latency, accuracy, and privacy,\nparameterized by the timeout of the skipping scheme, is identified and\nempirically validated for logistic regression on a real-world dataset and for\nimage classification using the MNIST and CIFAR-10 datasets.\n", "link": "http://arxiv.org/abs/2212.03080v3", "date": "2024-06-28", "relevancy": 1.7761, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4599}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4437}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.438}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Straggler-Resilient%20Differentially-Private%20Decentralized%20Learning&body=Title%3A%20Straggler-Resilient%20Differentially-Private%20Decentralized%20Learning%0AAuthor%3A%20Yauhen%20Yakimenka%20and%20Chung-Wei%20Weng%20and%20Hsuan-Yin%20Lin%20and%20Eirik%20Rosnes%20and%20J%C3%B6rg%20Kliewer%0AAbstract%3A%20%20%20We%20consider%20the%20straggler%20problem%20in%20decentralized%20learning%20over%20a%20logical%0Aring%20while%20preserving%20user%20data%20privacy.%20Especially%2C%20we%20extend%20the%20recently%0Aproposed%20framework%20of%20differential%20privacy%20%28DP%29%20amplification%20by%0Adecentralization%20by%20Cyffers%20and%20Bellet%20to%20include%20overall%20training%0Alatency--comprising%20both%20computation%20and%20communication%20latency.%20Analytical%0Aresults%20on%20both%20the%20convergence%20speed%20and%20the%20DP%20level%20are%20derived%20for%20both%20a%0Askipping%20scheme%20%28which%20ignores%20the%20stragglers%20after%20a%20timeout%29%20and%20a%20baseline%0Ascheme%20that%20waits%20for%20each%20node%20to%20finish%20before%20the%20training%20continues.%20A%0Atrade-off%20between%20overall%20training%20latency%2C%20accuracy%2C%20and%20privacy%2C%0Aparameterized%20by%20the%20timeout%20of%20the%20skipping%20scheme%2C%20is%20identified%20and%0Aempirically%20validated%20for%20logistic%20regression%20on%20a%20real-world%20dataset%20and%20for%0Aimage%20classification%20using%20the%20MNIST%20and%20CIFAR-10%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.03080v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStraggler-Resilient%2520Differentially-Private%2520Decentralized%2520Learning%26entry.906535625%3DYauhen%2520Yakimenka%2520and%2520Chung-Wei%2520Weng%2520and%2520Hsuan-Yin%2520Lin%2520and%2520Eirik%2520Rosnes%2520and%2520J%25C3%25B6rg%2520Kliewer%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520straggler%2520problem%2520in%2520decentralized%2520learning%2520over%2520a%2520logical%250Aring%2520while%2520preserving%2520user%2520data%2520privacy.%2520Especially%252C%2520we%2520extend%2520the%2520recently%250Aproposed%2520framework%2520of%2520differential%2520privacy%2520%2528DP%2529%2520amplification%2520by%250Adecentralization%2520by%2520Cyffers%2520and%2520Bellet%2520to%2520include%2520overall%2520training%250Alatency--comprising%2520both%2520computation%2520and%2520communication%2520latency.%2520Analytical%250Aresults%2520on%2520both%2520the%2520convergence%2520speed%2520and%2520the%2520DP%2520level%2520are%2520derived%2520for%2520both%2520a%250Askipping%2520scheme%2520%2528which%2520ignores%2520the%2520stragglers%2520after%2520a%2520timeout%2529%2520and%2520a%2520baseline%250Ascheme%2520that%2520waits%2520for%2520each%2520node%2520to%2520finish%2520before%2520the%2520training%2520continues.%2520A%250Atrade-off%2520between%2520overall%2520training%2520latency%252C%2520accuracy%252C%2520and%2520privacy%252C%250Aparameterized%2520by%2520the%2520timeout%2520of%2520the%2520skipping%2520scheme%252C%2520is%2520identified%2520and%250Aempirically%2520validated%2520for%2520logistic%2520regression%2520on%2520a%2520real-world%2520dataset%2520and%2520for%250Aimage%2520classification%2520using%2520the%2520MNIST%2520and%2520CIFAR-10%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2212.03080v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Straggler-Resilient%20Differentially-Private%20Decentralized%20Learning&entry.906535625=Yauhen%20Yakimenka%20and%20Chung-Wei%20Weng%20and%20Hsuan-Yin%20Lin%20and%20Eirik%20Rosnes%20and%20J%C3%B6rg%20Kliewer&entry.1292438233=%20%20We%20consider%20the%20straggler%20problem%20in%20decentralized%20learning%20over%20a%20logical%0Aring%20while%20preserving%20user%20data%20privacy.%20Especially%2C%20we%20extend%20the%20recently%0Aproposed%20framework%20of%20differential%20privacy%20%28DP%29%20amplification%20by%0Adecentralization%20by%20Cyffers%20and%20Bellet%20to%20include%20overall%20training%0Alatency--comprising%20both%20computation%20and%20communication%20latency.%20Analytical%0Aresults%20on%20both%20the%20convergence%20speed%20and%20the%20DP%20level%20are%20derived%20for%20both%20a%0Askipping%20scheme%20%28which%20ignores%20the%20stragglers%20after%20a%20timeout%29%20and%20a%20baseline%0Ascheme%20that%20waits%20for%20each%20node%20to%20finish%20before%20the%20training%20continues.%20A%0Atrade-off%20between%20overall%20training%20latency%2C%20accuracy%2C%20and%20privacy%2C%0Aparameterized%20by%20the%20timeout%20of%20the%20skipping%20scheme%2C%20is%20identified%20and%0Aempirically%20validated%20for%20logistic%20regression%20on%20a%20real-world%20dataset%20and%20for%0Aimage%20classification%20using%20the%20MNIST%20and%20CIFAR-10%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.03080v3&entry.124074799=Read"},
{"title": "Networked Communication for Decentralised Agents in Mean-Field Games", "author": "Patrick Benjamin and Alessandro Abate", "abstract": "  We introduce networked communication to the mean-field game framework, in\nparticular to oracle-free settings where $N$ decentralised agents learn along a\nsingle, non-episodic run of the empirical system. We prove that our\narchitecture, with only a few reasonable assumptions about network structure,\nhas sample guarantees bounded between those of the centralised- and\nindependent-learning cases. We discuss how the sample guarantees of the three\ntheoretical algorithms do not actually result in practical convergence. We\ntherefore show that in practical settings where the theoretical parameters are\nnot observed (leading to poor estimation of the Q-function), our communication\nscheme significantly accelerates convergence over the independent case (and\noften even the centralised case), without relying on the assumption of a\ncentralised learner. We contribute further practical enhancements to all three\ntheoretical algorithms, allowing us to present their first empirical\ndemonstrations. Our experiments confirm that we can remove several of the\ntheoretical assumptions of the algorithms, and display the empirical\nconvergence benefits brought by our new networked communication. We\nadditionally show that the networked approach has significant advantages, over\nboth the centralised and independent alternatives, in terms of robustness to\nunexpected learning failures and to changes in population size.\n", "link": "http://arxiv.org/abs/2306.02766v3", "date": "2024-06-28", "relevancy": 1.7736, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4793}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.422}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4161}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Networked%20Communication%20for%20Decentralised%20Agents%20in%20Mean-Field%20Games&body=Title%3A%20Networked%20Communication%20for%20Decentralised%20Agents%20in%20Mean-Field%20Games%0AAuthor%3A%20Patrick%20Benjamin%20and%20Alessandro%20Abate%0AAbstract%3A%20%20%20We%20introduce%20networked%20communication%20to%20the%20mean-field%20game%20framework%2C%20in%0Aparticular%20to%20oracle-free%20settings%20where%20%24N%24%20decentralised%20agents%20learn%20along%20a%0Asingle%2C%20non-episodic%20run%20of%20the%20empirical%20system.%20We%20prove%20that%20our%0Aarchitecture%2C%20with%20only%20a%20few%20reasonable%20assumptions%20about%20network%20structure%2C%0Ahas%20sample%20guarantees%20bounded%20between%20those%20of%20the%20centralised-%20and%0Aindependent-learning%20cases.%20We%20discuss%20how%20the%20sample%20guarantees%20of%20the%20three%0Atheoretical%20algorithms%20do%20not%20actually%20result%20in%20practical%20convergence.%20We%0Atherefore%20show%20that%20in%20practical%20settings%20where%20the%20theoretical%20parameters%20are%0Anot%20observed%20%28leading%20to%20poor%20estimation%20of%20the%20Q-function%29%2C%20our%20communication%0Ascheme%20significantly%20accelerates%20convergence%20over%20the%20independent%20case%20%28and%0Aoften%20even%20the%20centralised%20case%29%2C%20without%20relying%20on%20the%20assumption%20of%20a%0Acentralised%20learner.%20We%20contribute%20further%20practical%20enhancements%20to%20all%20three%0Atheoretical%20algorithms%2C%20allowing%20us%20to%20present%20their%20first%20empirical%0Ademonstrations.%20Our%20experiments%20confirm%20that%20we%20can%20remove%20several%20of%20the%0Atheoretical%20assumptions%20of%20the%20algorithms%2C%20and%20display%20the%20empirical%0Aconvergence%20benefits%20brought%20by%20our%20new%20networked%20communication.%20We%0Aadditionally%20show%20that%20the%20networked%20approach%20has%20significant%20advantages%2C%20over%0Aboth%20the%20centralised%20and%20independent%20alternatives%2C%20in%20terms%20of%20robustness%20to%0Aunexpected%20learning%20failures%20and%20to%20changes%20in%20population%20size.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.02766v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNetworked%2520Communication%2520for%2520Decentralised%2520Agents%2520in%2520Mean-Field%2520Games%26entry.906535625%3DPatrick%2520Benjamin%2520and%2520Alessandro%2520Abate%26entry.1292438233%3D%2520%2520We%2520introduce%2520networked%2520communication%2520to%2520the%2520mean-field%2520game%2520framework%252C%2520in%250Aparticular%2520to%2520oracle-free%2520settings%2520where%2520%2524N%2524%2520decentralised%2520agents%2520learn%2520along%2520a%250Asingle%252C%2520non-episodic%2520run%2520of%2520the%2520empirical%2520system.%2520We%2520prove%2520that%2520our%250Aarchitecture%252C%2520with%2520only%2520a%2520few%2520reasonable%2520assumptions%2520about%2520network%2520structure%252C%250Ahas%2520sample%2520guarantees%2520bounded%2520between%2520those%2520of%2520the%2520centralised-%2520and%250Aindependent-learning%2520cases.%2520We%2520discuss%2520how%2520the%2520sample%2520guarantees%2520of%2520the%2520three%250Atheoretical%2520algorithms%2520do%2520not%2520actually%2520result%2520in%2520practical%2520convergence.%2520We%250Atherefore%2520show%2520that%2520in%2520practical%2520settings%2520where%2520the%2520theoretical%2520parameters%2520are%250Anot%2520observed%2520%2528leading%2520to%2520poor%2520estimation%2520of%2520the%2520Q-function%2529%252C%2520our%2520communication%250Ascheme%2520significantly%2520accelerates%2520convergence%2520over%2520the%2520independent%2520case%2520%2528and%250Aoften%2520even%2520the%2520centralised%2520case%2529%252C%2520without%2520relying%2520on%2520the%2520assumption%2520of%2520a%250Acentralised%2520learner.%2520We%2520contribute%2520further%2520practical%2520enhancements%2520to%2520all%2520three%250Atheoretical%2520algorithms%252C%2520allowing%2520us%2520to%2520present%2520their%2520first%2520empirical%250Ademonstrations.%2520Our%2520experiments%2520confirm%2520that%2520we%2520can%2520remove%2520several%2520of%2520the%250Atheoretical%2520assumptions%2520of%2520the%2520algorithms%252C%2520and%2520display%2520the%2520empirical%250Aconvergence%2520benefits%2520brought%2520by%2520our%2520new%2520networked%2520communication.%2520We%250Aadditionally%2520show%2520that%2520the%2520networked%2520approach%2520has%2520significant%2520advantages%252C%2520over%250Aboth%2520the%2520centralised%2520and%2520independent%2520alternatives%252C%2520in%2520terms%2520of%2520robustness%2520to%250Aunexpected%2520learning%2520failures%2520and%2520to%2520changes%2520in%2520population%2520size.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.02766v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Networked%20Communication%20for%20Decentralised%20Agents%20in%20Mean-Field%20Games&entry.906535625=Patrick%20Benjamin%20and%20Alessandro%20Abate&entry.1292438233=%20%20We%20introduce%20networked%20communication%20to%20the%20mean-field%20game%20framework%2C%20in%0Aparticular%20to%20oracle-free%20settings%20where%20%24N%24%20decentralised%20agents%20learn%20along%20a%0Asingle%2C%20non-episodic%20run%20of%20the%20empirical%20system.%20We%20prove%20that%20our%0Aarchitecture%2C%20with%20only%20a%20few%20reasonable%20assumptions%20about%20network%20structure%2C%0Ahas%20sample%20guarantees%20bounded%20between%20those%20of%20the%20centralised-%20and%0Aindependent-learning%20cases.%20We%20discuss%20how%20the%20sample%20guarantees%20of%20the%20three%0Atheoretical%20algorithms%20do%20not%20actually%20result%20in%20practical%20convergence.%20We%0Atherefore%20show%20that%20in%20practical%20settings%20where%20the%20theoretical%20parameters%20are%0Anot%20observed%20%28leading%20to%20poor%20estimation%20of%20the%20Q-function%29%2C%20our%20communication%0Ascheme%20significantly%20accelerates%20convergence%20over%20the%20independent%20case%20%28and%0Aoften%20even%20the%20centralised%20case%29%2C%20without%20relying%20on%20the%20assumption%20of%20a%0Acentralised%20learner.%20We%20contribute%20further%20practical%20enhancements%20to%20all%20three%0Atheoretical%20algorithms%2C%20allowing%20us%20to%20present%20their%20first%20empirical%0Ademonstrations.%20Our%20experiments%20confirm%20that%20we%20can%20remove%20several%20of%20the%0Atheoretical%20assumptions%20of%20the%20algorithms%2C%20and%20display%20the%20empirical%0Aconvergence%20benefits%20brought%20by%20our%20new%20networked%20communication.%20We%0Aadditionally%20show%20that%20the%20networked%20approach%20has%20significant%20advantages%2C%20over%0Aboth%20the%20centralised%20and%20independent%20alternatives%2C%20in%20terms%20of%20robustness%20to%0Aunexpected%20learning%20failures%20and%20to%20changes%20in%20population%20size.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.02766v3&entry.124074799=Read"},
{"title": "Malaria Cell Detection Using Deep Neural Networks", "author": "Saurabh Sawant and Anurag Singh", "abstract": "  Malaria remains one of the most pressing public health concerns globally,\ncausing significant morbidity and mortality, especially in sub-Saharan Africa.\nRapid and accurate diagnosis is crucial for effective treatment and disease\nmanagement. Traditional diagnostic methods, such as microscopic examination of\nblood smears, are labor-intensive and require significant expertise, which may\nnot be readily available in resource-limited settings. This project aims to\nautomate the detection of malaria-infected cells using a deep learning\napproach. We employed a convolutional neural network (CNN) based on the\nResNet50 architecture, leveraging transfer learning to enhance performance. The\nMalaria Cell Images Dataset from Kaggle, containing 27,558 images categorized\ninto infected and uninfected cells, was used for training and evaluation. Our\nmodel demonstrated high accuracy, precision, and recall, indicating its\npotential as a reliable tool for assisting in malaria diagnosis. Additionally,\na web application was developed using Streamlit to allow users to upload cell\nimages and receive predictions about malaria infection, making the technology\naccessible and user-friendly. This paper provides a comprehensive overview of\nthe methodology, experiments, and results, highlighting the effectiveness of\ndeep learning in medical image analysis.\n", "link": "http://arxiv.org/abs/2406.20005v1", "date": "2024-06-28", "relevancy": 1.7651, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4426}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4409}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4401}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Malaria%20Cell%20Detection%20Using%20Deep%20Neural%20Networks&body=Title%3A%20Malaria%20Cell%20Detection%20Using%20Deep%20Neural%20Networks%0AAuthor%3A%20Saurabh%20Sawant%20and%20Anurag%20Singh%0AAbstract%3A%20%20%20Malaria%20remains%20one%20of%20the%20most%20pressing%20public%20health%20concerns%20globally%2C%0Acausing%20significant%20morbidity%20and%20mortality%2C%20especially%20in%20sub-Saharan%20Africa.%0ARapid%20and%20accurate%20diagnosis%20is%20crucial%20for%20effective%20treatment%20and%20disease%0Amanagement.%20Traditional%20diagnostic%20methods%2C%20such%20as%20microscopic%20examination%20of%0Ablood%20smears%2C%20are%20labor-intensive%20and%20require%20significant%20expertise%2C%20which%20may%0Anot%20be%20readily%20available%20in%20resource-limited%20settings.%20This%20project%20aims%20to%0Aautomate%20the%20detection%20of%20malaria-infected%20cells%20using%20a%20deep%20learning%0Aapproach.%20We%20employed%20a%20convolutional%20neural%20network%20%28CNN%29%20based%20on%20the%0AResNet50%20architecture%2C%20leveraging%20transfer%20learning%20to%20enhance%20performance.%20The%0AMalaria%20Cell%20Images%20Dataset%20from%20Kaggle%2C%20containing%2027%2C558%20images%20categorized%0Ainto%20infected%20and%20uninfected%20cells%2C%20was%20used%20for%20training%20and%20evaluation.%20Our%0Amodel%20demonstrated%20high%20accuracy%2C%20precision%2C%20and%20recall%2C%20indicating%20its%0Apotential%20as%20a%20reliable%20tool%20for%20assisting%20in%20malaria%20diagnosis.%20Additionally%2C%0Aa%20web%20application%20was%20developed%20using%20Streamlit%20to%20allow%20users%20to%20upload%20cell%0Aimages%20and%20receive%20predictions%20about%20malaria%20infection%2C%20making%20the%20technology%0Aaccessible%20and%20user-friendly.%20This%20paper%20provides%20a%20comprehensive%20overview%20of%0Athe%20methodology%2C%20experiments%2C%20and%20results%2C%20highlighting%20the%20effectiveness%20of%0Adeep%20learning%20in%20medical%20image%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.20005v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMalaria%2520Cell%2520Detection%2520Using%2520Deep%2520Neural%2520Networks%26entry.906535625%3DSaurabh%2520Sawant%2520and%2520Anurag%2520Singh%26entry.1292438233%3D%2520%2520Malaria%2520remains%2520one%2520of%2520the%2520most%2520pressing%2520public%2520health%2520concerns%2520globally%252C%250Acausing%2520significant%2520morbidity%2520and%2520mortality%252C%2520especially%2520in%2520sub-Saharan%2520Africa.%250ARapid%2520and%2520accurate%2520diagnosis%2520is%2520crucial%2520for%2520effective%2520treatment%2520and%2520disease%250Amanagement.%2520Traditional%2520diagnostic%2520methods%252C%2520such%2520as%2520microscopic%2520examination%2520of%250Ablood%2520smears%252C%2520are%2520labor-intensive%2520and%2520require%2520significant%2520expertise%252C%2520which%2520may%250Anot%2520be%2520readily%2520available%2520in%2520resource-limited%2520settings.%2520This%2520project%2520aims%2520to%250Aautomate%2520the%2520detection%2520of%2520malaria-infected%2520cells%2520using%2520a%2520deep%2520learning%250Aapproach.%2520We%2520employed%2520a%2520convolutional%2520neural%2520network%2520%2528CNN%2529%2520based%2520on%2520the%250AResNet50%2520architecture%252C%2520leveraging%2520transfer%2520learning%2520to%2520enhance%2520performance.%2520The%250AMalaria%2520Cell%2520Images%2520Dataset%2520from%2520Kaggle%252C%2520containing%252027%252C558%2520images%2520categorized%250Ainto%2520infected%2520and%2520uninfected%2520cells%252C%2520was%2520used%2520for%2520training%2520and%2520evaluation.%2520Our%250Amodel%2520demonstrated%2520high%2520accuracy%252C%2520precision%252C%2520and%2520recall%252C%2520indicating%2520its%250Apotential%2520as%2520a%2520reliable%2520tool%2520for%2520assisting%2520in%2520malaria%2520diagnosis.%2520Additionally%252C%250Aa%2520web%2520application%2520was%2520developed%2520using%2520Streamlit%2520to%2520allow%2520users%2520to%2520upload%2520cell%250Aimages%2520and%2520receive%2520predictions%2520about%2520malaria%2520infection%252C%2520making%2520the%2520technology%250Aaccessible%2520and%2520user-friendly.%2520This%2520paper%2520provides%2520a%2520comprehensive%2520overview%2520of%250Athe%2520methodology%252C%2520experiments%252C%2520and%2520results%252C%2520highlighting%2520the%2520effectiveness%2520of%250Adeep%2520learning%2520in%2520medical%2520image%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.20005v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Malaria%20Cell%20Detection%20Using%20Deep%20Neural%20Networks&entry.906535625=Saurabh%20Sawant%20and%20Anurag%20Singh&entry.1292438233=%20%20Malaria%20remains%20one%20of%20the%20most%20pressing%20public%20health%20concerns%20globally%2C%0Acausing%20significant%20morbidity%20and%20mortality%2C%20especially%20in%20sub-Saharan%20Africa.%0ARapid%20and%20accurate%20diagnosis%20is%20crucial%20for%20effective%20treatment%20and%20disease%0Amanagement.%20Traditional%20diagnostic%20methods%2C%20such%20as%20microscopic%20examination%20of%0Ablood%20smears%2C%20are%20labor-intensive%20and%20require%20significant%20expertise%2C%20which%20may%0Anot%20be%20readily%20available%20in%20resource-limited%20settings.%20This%20project%20aims%20to%0Aautomate%20the%20detection%20of%20malaria-infected%20cells%20using%20a%20deep%20learning%0Aapproach.%20We%20employed%20a%20convolutional%20neural%20network%20%28CNN%29%20based%20on%20the%0AResNet50%20architecture%2C%20leveraging%20transfer%20learning%20to%20enhance%20performance.%20The%0AMalaria%20Cell%20Images%20Dataset%20from%20Kaggle%2C%20containing%2027%2C558%20images%20categorized%0Ainto%20infected%20and%20uninfected%20cells%2C%20was%20used%20for%20training%20and%20evaluation.%20Our%0Amodel%20demonstrated%20high%20accuracy%2C%20precision%2C%20and%20recall%2C%20indicating%20its%0Apotential%20as%20a%20reliable%20tool%20for%20assisting%20in%20malaria%20diagnosis.%20Additionally%2C%0Aa%20web%20application%20was%20developed%20using%20Streamlit%20to%20allow%20users%20to%20upload%20cell%0Aimages%20and%20receive%20predictions%20about%20malaria%20infection%2C%20making%20the%20technology%0Aaccessible%20and%20user-friendly.%20This%20paper%20provides%20a%20comprehensive%20overview%20of%0Athe%20methodology%2C%20experiments%2C%20and%20results%2C%20highlighting%20the%20effectiveness%20of%0Adeep%20learning%20in%20medical%20image%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.20005v1&entry.124074799=Read"},
{"title": "Catastrophic-risk-aware reinforcement learning with\n  extreme-value-theory-based policy gradients", "author": "Parisa Davar and Fr\u00e9d\u00e9ric Godin and Jose Garrido", "abstract": "  This paper tackles the problem of mitigating catastrophic risk (which is risk\nwith very low frequency but very high severity) in the context of a sequential\ndecision making process. This problem is particularly challenging due to the\nscarcity of observations in the far tail of the distribution of cumulative\ncosts (negative rewards). A policy gradient algorithm is developed, that we\ncall POTPG. It is based on approximations of the tail risk derived from extreme\nvalue theory. Numerical experiments highlight the out-performance of our method\nover common benchmarks, relying on the empirical distribution. An application\nto financial risk management, more precisely to the dynamic hedging of a\nfinancial option, is presented.\n", "link": "http://arxiv.org/abs/2406.15612v2", "date": "2024-06-28", "relevancy": 1.7571, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4746}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4397}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Catastrophic-risk-aware%20reinforcement%20learning%20with%0A%20%20extreme-value-theory-based%20policy%20gradients&body=Title%3A%20Catastrophic-risk-aware%20reinforcement%20learning%20with%0A%20%20extreme-value-theory-based%20policy%20gradients%0AAuthor%3A%20Parisa%20Davar%20and%20Fr%C3%A9d%C3%A9ric%20Godin%20and%20Jose%20Garrido%0AAbstract%3A%20%20%20This%20paper%20tackles%20the%20problem%20of%20mitigating%20catastrophic%20risk%20%28which%20is%20risk%0Awith%20very%20low%20frequency%20but%20very%20high%20severity%29%20in%20the%20context%20of%20a%20sequential%0Adecision%20making%20process.%20This%20problem%20is%20particularly%20challenging%20due%20to%20the%0Ascarcity%20of%20observations%20in%20the%20far%20tail%20of%20the%20distribution%20of%20cumulative%0Acosts%20%28negative%20rewards%29.%20A%20policy%20gradient%20algorithm%20is%20developed%2C%20that%20we%0Acall%20POTPG.%20It%20is%20based%20on%20approximations%20of%20the%20tail%20risk%20derived%20from%20extreme%0Avalue%20theory.%20Numerical%20experiments%20highlight%20the%20out-performance%20of%20our%20method%0Aover%20common%20benchmarks%2C%20relying%20on%20the%20empirical%20distribution.%20An%20application%0Ato%20financial%20risk%20management%2C%20more%20precisely%20to%20the%20dynamic%20hedging%20of%20a%0Afinancial%20option%2C%20is%20presented.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15612v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCatastrophic-risk-aware%2520reinforcement%2520learning%2520with%250A%2520%2520extreme-value-theory-based%2520policy%2520gradients%26entry.906535625%3DParisa%2520Davar%2520and%2520Fr%25C3%25A9d%25C3%25A9ric%2520Godin%2520and%2520Jose%2520Garrido%26entry.1292438233%3D%2520%2520This%2520paper%2520tackles%2520the%2520problem%2520of%2520mitigating%2520catastrophic%2520risk%2520%2528which%2520is%2520risk%250Awith%2520very%2520low%2520frequency%2520but%2520very%2520high%2520severity%2529%2520in%2520the%2520context%2520of%2520a%2520sequential%250Adecision%2520making%2520process.%2520This%2520problem%2520is%2520particularly%2520challenging%2520due%2520to%2520the%250Ascarcity%2520of%2520observations%2520in%2520the%2520far%2520tail%2520of%2520the%2520distribution%2520of%2520cumulative%250Acosts%2520%2528negative%2520rewards%2529.%2520A%2520policy%2520gradient%2520algorithm%2520is%2520developed%252C%2520that%2520we%250Acall%2520POTPG.%2520It%2520is%2520based%2520on%2520approximations%2520of%2520the%2520tail%2520risk%2520derived%2520from%2520extreme%250Avalue%2520theory.%2520Numerical%2520experiments%2520highlight%2520the%2520out-performance%2520of%2520our%2520method%250Aover%2520common%2520benchmarks%252C%2520relying%2520on%2520the%2520empirical%2520distribution.%2520An%2520application%250Ato%2520financial%2520risk%2520management%252C%2520more%2520precisely%2520to%2520the%2520dynamic%2520hedging%2520of%2520a%250Afinancial%2520option%252C%2520is%2520presented.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15612v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Catastrophic-risk-aware%20reinforcement%20learning%20with%0A%20%20extreme-value-theory-based%20policy%20gradients&entry.906535625=Parisa%20Davar%20and%20Fr%C3%A9d%C3%A9ric%20Godin%20and%20Jose%20Garrido&entry.1292438233=%20%20This%20paper%20tackles%20the%20problem%20of%20mitigating%20catastrophic%20risk%20%28which%20is%20risk%0Awith%20very%20low%20frequency%20but%20very%20high%20severity%29%20in%20the%20context%20of%20a%20sequential%0Adecision%20making%20process.%20This%20problem%20is%20particularly%20challenging%20due%20to%20the%0Ascarcity%20of%20observations%20in%20the%20far%20tail%20of%20the%20distribution%20of%20cumulative%0Acosts%20%28negative%20rewards%29.%20A%20policy%20gradient%20algorithm%20is%20developed%2C%20that%20we%0Acall%20POTPG.%20It%20is%20based%20on%20approximations%20of%20the%20tail%20risk%20derived%20from%20extreme%0Avalue%20theory.%20Numerical%20experiments%20highlight%20the%20out-performance%20of%20our%20method%0Aover%20common%20benchmarks%2C%20relying%20on%20the%20empirical%20distribution.%20An%20application%0Ato%20financial%20risk%20management%2C%20more%20precisely%20to%20the%20dynamic%20hedging%20of%20a%0Afinancial%20option%2C%20is%20presented.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15612v2&entry.124074799=Read"},
{"title": "LLMs and Memorization: On Quality and Specificity of Copyright\n  Compliance", "author": "Felix B Mueller and Rebekka G\u00f6rge and Anna K Bernzen and Janna C Pirk and Maximilian Poretschkin", "abstract": "  Memorization in large language models (LLMs) is a growing concern. LLMs have\nbeen shown to easily reproduce parts of their training data, including\ncopyrighted work. This is an important problem to solve, as it may violate\nexisting copyright laws as well as the European AI Act. In this work, we\npropose a systematic analysis to quantify the extent of potential copyright\ninfringements in LLMs using European law as an example. Unlike previous work,\nwe evaluate instruction-finetuned models in a realistic end-user scenario. Our\nanalysis builds on a proposed threshold of 160 characters, which we borrow from\nthe German Copyright Service Provider Act and a fuzzy text matching algorithm\nto identify potentially copyright-infringing textual reproductions. The\nspecificity of countermeasures against copyright infringement is analyzed by\ncomparing model behavior on copyrighted and public domain data. We investigate\nwhat behaviors models show instead of producing protected text (such as refusal\nor hallucination) and provide a first legal assessment of these behaviors. We\nfind that there are huge differences in copyright compliance, specificity, and\nappropriate refusal among popular LLMs. Alpaca, GPT 4, GPT 3.5, and Luminous\nperform best in our comparison, with OpenGPT-X, Alpaca, and Luminous producing\na particularly low absolute number of potential copyright violations. Code will\nbe published soon.\n", "link": "http://arxiv.org/abs/2405.18492v2", "date": "2024-06-28", "relevancy": 1.7564, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4627}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4419}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4269}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMs%20and%20Memorization%3A%20On%20Quality%20and%20Specificity%20of%20Copyright%0A%20%20Compliance&body=Title%3A%20LLMs%20and%20Memorization%3A%20On%20Quality%20and%20Specificity%20of%20Copyright%0A%20%20Compliance%0AAuthor%3A%20Felix%20B%20Mueller%20and%20Rebekka%20G%C3%B6rge%20and%20Anna%20K%20Bernzen%20and%20Janna%20C%20Pirk%20and%20Maximilian%20Poretschkin%0AAbstract%3A%20%20%20Memorization%20in%20large%20language%20models%20%28LLMs%29%20is%20a%20growing%20concern.%20LLMs%20have%0Abeen%20shown%20to%20easily%20reproduce%20parts%20of%20their%20training%20data%2C%20including%0Acopyrighted%20work.%20This%20is%20an%20important%20problem%20to%20solve%2C%20as%20it%20may%20violate%0Aexisting%20copyright%20laws%20as%20well%20as%20the%20European%20AI%20Act.%20In%20this%20work%2C%20we%0Apropose%20a%20systematic%20analysis%20to%20quantify%20the%20extent%20of%20potential%20copyright%0Ainfringements%20in%20LLMs%20using%20European%20law%20as%20an%20example.%20Unlike%20previous%20work%2C%0Awe%20evaluate%20instruction-finetuned%20models%20in%20a%20realistic%20end-user%20scenario.%20Our%0Aanalysis%20builds%20on%20a%20proposed%20threshold%20of%20160%20characters%2C%20which%20we%20borrow%20from%0Athe%20German%20Copyright%20Service%20Provider%20Act%20and%20a%20fuzzy%20text%20matching%20algorithm%0Ato%20identify%20potentially%20copyright-infringing%20textual%20reproductions.%20The%0Aspecificity%20of%20countermeasures%20against%20copyright%20infringement%20is%20analyzed%20by%0Acomparing%20model%20behavior%20on%20copyrighted%20and%20public%20domain%20data.%20We%20investigate%0Awhat%20behaviors%20models%20show%20instead%20of%20producing%20protected%20text%20%28such%20as%20refusal%0Aor%20hallucination%29%20and%20provide%20a%20first%20legal%20assessment%20of%20these%20behaviors.%20We%0Afind%20that%20there%20are%20huge%20differences%20in%20copyright%20compliance%2C%20specificity%2C%20and%0Aappropriate%20refusal%20among%20popular%20LLMs.%20Alpaca%2C%20GPT%204%2C%20GPT%203.5%2C%20and%20Luminous%0Aperform%20best%20in%20our%20comparison%2C%20with%20OpenGPT-X%2C%20Alpaca%2C%20and%20Luminous%20producing%0Aa%20particularly%20low%20absolute%20number%20of%20potential%20copyright%20violations.%20Code%20will%0Abe%20published%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18492v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMs%2520and%2520Memorization%253A%2520On%2520Quality%2520and%2520Specificity%2520of%2520Copyright%250A%2520%2520Compliance%26entry.906535625%3DFelix%2520B%2520Mueller%2520and%2520Rebekka%2520G%25C3%25B6rge%2520and%2520Anna%2520K%2520Bernzen%2520and%2520Janna%2520C%2520Pirk%2520and%2520Maximilian%2520Poretschkin%26entry.1292438233%3D%2520%2520Memorization%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520a%2520growing%2520concern.%2520LLMs%2520have%250Abeen%2520shown%2520to%2520easily%2520reproduce%2520parts%2520of%2520their%2520training%2520data%252C%2520including%250Acopyrighted%2520work.%2520This%2520is%2520an%2520important%2520problem%2520to%2520solve%252C%2520as%2520it%2520may%2520violate%250Aexisting%2520copyright%2520laws%2520as%2520well%2520as%2520the%2520European%2520AI%2520Act.%2520In%2520this%2520work%252C%2520we%250Apropose%2520a%2520systematic%2520analysis%2520to%2520quantify%2520the%2520extent%2520of%2520potential%2520copyright%250Ainfringements%2520in%2520LLMs%2520using%2520European%2520law%2520as%2520an%2520example.%2520Unlike%2520previous%2520work%252C%250Awe%2520evaluate%2520instruction-finetuned%2520models%2520in%2520a%2520realistic%2520end-user%2520scenario.%2520Our%250Aanalysis%2520builds%2520on%2520a%2520proposed%2520threshold%2520of%2520160%2520characters%252C%2520which%2520we%2520borrow%2520from%250Athe%2520German%2520Copyright%2520Service%2520Provider%2520Act%2520and%2520a%2520fuzzy%2520text%2520matching%2520algorithm%250Ato%2520identify%2520potentially%2520copyright-infringing%2520textual%2520reproductions.%2520The%250Aspecificity%2520of%2520countermeasures%2520against%2520copyright%2520infringement%2520is%2520analyzed%2520by%250Acomparing%2520model%2520behavior%2520on%2520copyrighted%2520and%2520public%2520domain%2520data.%2520We%2520investigate%250Awhat%2520behaviors%2520models%2520show%2520instead%2520of%2520producing%2520protected%2520text%2520%2528such%2520as%2520refusal%250Aor%2520hallucination%2529%2520and%2520provide%2520a%2520first%2520legal%2520assessment%2520of%2520these%2520behaviors.%2520We%250Afind%2520that%2520there%2520are%2520huge%2520differences%2520in%2520copyright%2520compliance%252C%2520specificity%252C%2520and%250Aappropriate%2520refusal%2520among%2520popular%2520LLMs.%2520Alpaca%252C%2520GPT%25204%252C%2520GPT%25203.5%252C%2520and%2520Luminous%250Aperform%2520best%2520in%2520our%2520comparison%252C%2520with%2520OpenGPT-X%252C%2520Alpaca%252C%2520and%2520Luminous%2520producing%250Aa%2520particularly%2520low%2520absolute%2520number%2520of%2520potential%2520copyright%2520violations.%2520Code%2520will%250Abe%2520published%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18492v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs%20and%20Memorization%3A%20On%20Quality%20and%20Specificity%20of%20Copyright%0A%20%20Compliance&entry.906535625=Felix%20B%20Mueller%20and%20Rebekka%20G%C3%B6rge%20and%20Anna%20K%20Bernzen%20and%20Janna%20C%20Pirk%20and%20Maximilian%20Poretschkin&entry.1292438233=%20%20Memorization%20in%20large%20language%20models%20%28LLMs%29%20is%20a%20growing%20concern.%20LLMs%20have%0Abeen%20shown%20to%20easily%20reproduce%20parts%20of%20their%20training%20data%2C%20including%0Acopyrighted%20work.%20This%20is%20an%20important%20problem%20to%20solve%2C%20as%20it%20may%20violate%0Aexisting%20copyright%20laws%20as%20well%20as%20the%20European%20AI%20Act.%20In%20this%20work%2C%20we%0Apropose%20a%20systematic%20analysis%20to%20quantify%20the%20extent%20of%20potential%20copyright%0Ainfringements%20in%20LLMs%20using%20European%20law%20as%20an%20example.%20Unlike%20previous%20work%2C%0Awe%20evaluate%20instruction-finetuned%20models%20in%20a%20realistic%20end-user%20scenario.%20Our%0Aanalysis%20builds%20on%20a%20proposed%20threshold%20of%20160%20characters%2C%20which%20we%20borrow%20from%0Athe%20German%20Copyright%20Service%20Provider%20Act%20and%20a%20fuzzy%20text%20matching%20algorithm%0Ato%20identify%20potentially%20copyright-infringing%20textual%20reproductions.%20The%0Aspecificity%20of%20countermeasures%20against%20copyright%20infringement%20is%20analyzed%20by%0Acomparing%20model%20behavior%20on%20copyrighted%20and%20public%20domain%20data.%20We%20investigate%0Awhat%20behaviors%20models%20show%20instead%20of%20producing%20protected%20text%20%28such%20as%20refusal%0Aor%20hallucination%29%20and%20provide%20a%20first%20legal%20assessment%20of%20these%20behaviors.%20We%0Afind%20that%20there%20are%20huge%20differences%20in%20copyright%20compliance%2C%20specificity%2C%20and%0Aappropriate%20refusal%20among%20popular%20LLMs.%20Alpaca%2C%20GPT%204%2C%20GPT%203.5%2C%20and%20Luminous%0Aperform%20best%20in%20our%20comparison%2C%20with%20OpenGPT-X%2C%20Alpaca%2C%20and%20Luminous%20producing%0Aa%20particularly%20low%20absolute%20number%20of%20potential%20copyright%20violations.%20Code%20will%0Abe%20published%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18492v2&entry.124074799=Read"},
{"title": "Text2Robot: Evolutionary Robot Design from Text Descriptions", "author": "Ryan P. Ringel and Zachary S. Charlick and Jiaxun Liu and Boxi Xia and Boyuan Chen", "abstract": "  Robot design has traditionally been costly and labor-intensive. Despite\nadvancements in automated processes, it remains challenging to navigate a vast\ndesign space while producing physically manufacturable robots. We introduce\nText2Robot, a framework that converts user text specifications and performance\npreferences into physical quadrupedal robots. Within minutes, Text2Robot can\nuse text-to-3D models to provide strong initializations of diverse\nmorphologies. Within a day, our geometric processing algorithms and\nbody-control co-optimization produce a walking robot by explicitly considering\nreal-world electronics and manufacturability. Text2Robot enables rapid\nprototyping and opens new opportunities for robot design with generative\nmodels.\n", "link": "http://arxiv.org/abs/2406.19963v1", "date": "2024-06-28", "relevancy": 1.7544, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.618}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5668}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5199}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text2Robot%3A%20Evolutionary%20Robot%20Design%20from%20Text%20Descriptions&body=Title%3A%20Text2Robot%3A%20Evolutionary%20Robot%20Design%20from%20Text%20Descriptions%0AAuthor%3A%20Ryan%20P.%20Ringel%20and%20Zachary%20S.%20Charlick%20and%20Jiaxun%20Liu%20and%20Boxi%20Xia%20and%20Boyuan%20Chen%0AAbstract%3A%20%20%20Robot%20design%20has%20traditionally%20been%20costly%20and%20labor-intensive.%20Despite%0Aadvancements%20in%20automated%20processes%2C%20it%20remains%20challenging%20to%20navigate%20a%20vast%0Adesign%20space%20while%20producing%20physically%20manufacturable%20robots.%20We%20introduce%0AText2Robot%2C%20a%20framework%20that%20converts%20user%20text%20specifications%20and%20performance%0Apreferences%20into%20physical%20quadrupedal%20robots.%20Within%20minutes%2C%20Text2Robot%20can%0Ause%20text-to-3D%20models%20to%20provide%20strong%20initializations%20of%20diverse%0Amorphologies.%20Within%20a%20day%2C%20our%20geometric%20processing%20algorithms%20and%0Abody-control%20co-optimization%20produce%20a%20walking%20robot%20by%20explicitly%20considering%0Areal-world%20electronics%20and%20manufacturability.%20Text2Robot%20enables%20rapid%0Aprototyping%20and%20opens%20new%20opportunities%20for%20robot%20design%20with%20generative%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19963v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText2Robot%253A%2520Evolutionary%2520Robot%2520Design%2520from%2520Text%2520Descriptions%26entry.906535625%3DRyan%2520P.%2520Ringel%2520and%2520Zachary%2520S.%2520Charlick%2520and%2520Jiaxun%2520Liu%2520and%2520Boxi%2520Xia%2520and%2520Boyuan%2520Chen%26entry.1292438233%3D%2520%2520Robot%2520design%2520has%2520traditionally%2520been%2520costly%2520and%2520labor-intensive.%2520Despite%250Aadvancements%2520in%2520automated%2520processes%252C%2520it%2520remains%2520challenging%2520to%2520navigate%2520a%2520vast%250Adesign%2520space%2520while%2520producing%2520physically%2520manufacturable%2520robots.%2520We%2520introduce%250AText2Robot%252C%2520a%2520framework%2520that%2520converts%2520user%2520text%2520specifications%2520and%2520performance%250Apreferences%2520into%2520physical%2520quadrupedal%2520robots.%2520Within%2520minutes%252C%2520Text2Robot%2520can%250Ause%2520text-to-3D%2520models%2520to%2520provide%2520strong%2520initializations%2520of%2520diverse%250Amorphologies.%2520Within%2520a%2520day%252C%2520our%2520geometric%2520processing%2520algorithms%2520and%250Abody-control%2520co-optimization%2520produce%2520a%2520walking%2520robot%2520by%2520explicitly%2520considering%250Areal-world%2520electronics%2520and%2520manufacturability.%2520Text2Robot%2520enables%2520rapid%250Aprototyping%2520and%2520opens%2520new%2520opportunities%2520for%2520robot%2520design%2520with%2520generative%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19963v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text2Robot%3A%20Evolutionary%20Robot%20Design%20from%20Text%20Descriptions&entry.906535625=Ryan%20P.%20Ringel%20and%20Zachary%20S.%20Charlick%20and%20Jiaxun%20Liu%20and%20Boxi%20Xia%20and%20Boyuan%20Chen&entry.1292438233=%20%20Robot%20design%20has%20traditionally%20been%20costly%20and%20labor-intensive.%20Despite%0Aadvancements%20in%20automated%20processes%2C%20it%20remains%20challenging%20to%20navigate%20a%20vast%0Adesign%20space%20while%20producing%20physically%20manufacturable%20robots.%20We%20introduce%0AText2Robot%2C%20a%20framework%20that%20converts%20user%20text%20specifications%20and%20performance%0Apreferences%20into%20physical%20quadrupedal%20robots.%20Within%20minutes%2C%20Text2Robot%20can%0Ause%20text-to-3D%20models%20to%20provide%20strong%20initializations%20of%20diverse%0Amorphologies.%20Within%20a%20day%2C%20our%20geometric%20processing%20algorithms%20and%0Abody-control%20co-optimization%20produce%20a%20walking%20robot%20by%20explicitly%20considering%0Areal-world%20electronics%20and%20manufacturability.%20Text2Robot%20enables%20rapid%0Aprototyping%20and%20opens%20new%20opportunities%20for%20robot%20design%20with%20generative%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19963v1&entry.124074799=Read"},
{"title": "ChIRAAG: ChatGPT Informed Rapid and Automated Assertion Generation", "author": "Bhabesh Mali and Karthik Maddala and Vatsal Gupta and Sweeya Reddy and Chandan Karfa and Ramesh Karri", "abstract": "  System Verilog Assertion (SVA) formulation -- a critical yet complex task is\na prerequisite in the Assertion Based Verification (ABV) process.\nTraditionally, SVA formulation involves expert-driven interpretation of\nspecifications, which is time-consuming and prone to human error. Recently,\nLLM-informed automatic assertion generation is gaining interest. We designed a\nnovel framework called ChIRAAG, based on OpenAI GPT4, to generate SVA from\nnatural language specifications of a design. ChIRAAG constitutes the systematic\nbreakdown of design specifications into a standardized format, further\ngenerating assertions from formatted specifications using LLM. Furthermore, we\nused few test cases to validate the LLM-generated assertions. Automatic\nfeedback of log messages from the simulation tool to the LLM ensures that the\nframework can generate correct SVAs. In our experiments, only 27% of\nLLM-generated raw assertions had errors, which was rectified in few iterations\nbased on the simulation log. Our results on OpenTitan designs show that LLMs\ncan streamline and assist engineers in the assertion generation process,\nreshaping verification workflows.\n", "link": "http://arxiv.org/abs/2402.00093v3", "date": "2024-06-28", "relevancy": 1.7099, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4367}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4305}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChIRAAG%3A%20ChatGPT%20Informed%20Rapid%20and%20Automated%20Assertion%20Generation&body=Title%3A%20ChIRAAG%3A%20ChatGPT%20Informed%20Rapid%20and%20Automated%20Assertion%20Generation%0AAuthor%3A%20Bhabesh%20Mali%20and%20Karthik%20Maddala%20and%20Vatsal%20Gupta%20and%20Sweeya%20Reddy%20and%20Chandan%20Karfa%20and%20Ramesh%20Karri%0AAbstract%3A%20%20%20System%20Verilog%20Assertion%20%28SVA%29%20formulation%20--%20a%20critical%20yet%20complex%20task%20is%0Aa%20prerequisite%20in%20the%20Assertion%20Based%20Verification%20%28ABV%29%20process.%0ATraditionally%2C%20SVA%20formulation%20involves%20expert-driven%20interpretation%20of%0Aspecifications%2C%20which%20is%20time-consuming%20and%20prone%20to%20human%20error.%20Recently%2C%0ALLM-informed%20automatic%20assertion%20generation%20is%20gaining%20interest.%20We%20designed%20a%0Anovel%20framework%20called%20ChIRAAG%2C%20based%20on%20OpenAI%20GPT4%2C%20to%20generate%20SVA%20from%0Anatural%20language%20specifications%20of%20a%20design.%20ChIRAAG%20constitutes%20the%20systematic%0Abreakdown%20of%20design%20specifications%20into%20a%20standardized%20format%2C%20further%0Agenerating%20assertions%20from%20formatted%20specifications%20using%20LLM.%20Furthermore%2C%20we%0Aused%20few%20test%20cases%20to%20validate%20the%20LLM-generated%20assertions.%20Automatic%0Afeedback%20of%20log%20messages%20from%20the%20simulation%20tool%20to%20the%20LLM%20ensures%20that%20the%0Aframework%20can%20generate%20correct%20SVAs.%20In%20our%20experiments%2C%20only%2027%25%20of%0ALLM-generated%20raw%20assertions%20had%20errors%2C%20which%20was%20rectified%20in%20few%20iterations%0Abased%20on%20the%20simulation%20log.%20Our%20results%20on%20OpenTitan%20designs%20show%20that%20LLMs%0Acan%20streamline%20and%20assist%20engineers%20in%20the%20assertion%20generation%20process%2C%0Areshaping%20verification%20workflows.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.00093v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChIRAAG%253A%2520ChatGPT%2520Informed%2520Rapid%2520and%2520Automated%2520Assertion%2520Generation%26entry.906535625%3DBhabesh%2520Mali%2520and%2520Karthik%2520Maddala%2520and%2520Vatsal%2520Gupta%2520and%2520Sweeya%2520Reddy%2520and%2520Chandan%2520Karfa%2520and%2520Ramesh%2520Karri%26entry.1292438233%3D%2520%2520System%2520Verilog%2520Assertion%2520%2528SVA%2529%2520formulation%2520--%2520a%2520critical%2520yet%2520complex%2520task%2520is%250Aa%2520prerequisite%2520in%2520the%2520Assertion%2520Based%2520Verification%2520%2528ABV%2529%2520process.%250ATraditionally%252C%2520SVA%2520formulation%2520involves%2520expert-driven%2520interpretation%2520of%250Aspecifications%252C%2520which%2520is%2520time-consuming%2520and%2520prone%2520to%2520human%2520error.%2520Recently%252C%250ALLM-informed%2520automatic%2520assertion%2520generation%2520is%2520gaining%2520interest.%2520We%2520designed%2520a%250Anovel%2520framework%2520called%2520ChIRAAG%252C%2520based%2520on%2520OpenAI%2520GPT4%252C%2520to%2520generate%2520SVA%2520from%250Anatural%2520language%2520specifications%2520of%2520a%2520design.%2520ChIRAAG%2520constitutes%2520the%2520systematic%250Abreakdown%2520of%2520design%2520specifications%2520into%2520a%2520standardized%2520format%252C%2520further%250Agenerating%2520assertions%2520from%2520formatted%2520specifications%2520using%2520LLM.%2520Furthermore%252C%2520we%250Aused%2520few%2520test%2520cases%2520to%2520validate%2520the%2520LLM-generated%2520assertions.%2520Automatic%250Afeedback%2520of%2520log%2520messages%2520from%2520the%2520simulation%2520tool%2520to%2520the%2520LLM%2520ensures%2520that%2520the%250Aframework%2520can%2520generate%2520correct%2520SVAs.%2520In%2520our%2520experiments%252C%2520only%252027%2525%2520of%250ALLM-generated%2520raw%2520assertions%2520had%2520errors%252C%2520which%2520was%2520rectified%2520in%2520few%2520iterations%250Abased%2520on%2520the%2520simulation%2520log.%2520Our%2520results%2520on%2520OpenTitan%2520designs%2520show%2520that%2520LLMs%250Acan%2520streamline%2520and%2520assist%2520engineers%2520in%2520the%2520assertion%2520generation%2520process%252C%250Areshaping%2520verification%2520workflows.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.00093v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChIRAAG%3A%20ChatGPT%20Informed%20Rapid%20and%20Automated%20Assertion%20Generation&entry.906535625=Bhabesh%20Mali%20and%20Karthik%20Maddala%20and%20Vatsal%20Gupta%20and%20Sweeya%20Reddy%20and%20Chandan%20Karfa%20and%20Ramesh%20Karri&entry.1292438233=%20%20System%20Verilog%20Assertion%20%28SVA%29%20formulation%20--%20a%20critical%20yet%20complex%20task%20is%0Aa%20prerequisite%20in%20the%20Assertion%20Based%20Verification%20%28ABV%29%20process.%0ATraditionally%2C%20SVA%20formulation%20involves%20expert-driven%20interpretation%20of%0Aspecifications%2C%20which%20is%20time-consuming%20and%20prone%20to%20human%20error.%20Recently%2C%0ALLM-informed%20automatic%20assertion%20generation%20is%20gaining%20interest.%20We%20designed%20a%0Anovel%20framework%20called%20ChIRAAG%2C%20based%20on%20OpenAI%20GPT4%2C%20to%20generate%20SVA%20from%0Anatural%20language%20specifications%20of%20a%20design.%20ChIRAAG%20constitutes%20the%20systematic%0Abreakdown%20of%20design%20specifications%20into%20a%20standardized%20format%2C%20further%0Agenerating%20assertions%20from%20formatted%20specifications%20using%20LLM.%20Furthermore%2C%20we%0Aused%20few%20test%20cases%20to%20validate%20the%20LLM-generated%20assertions.%20Automatic%0Afeedback%20of%20log%20messages%20from%20the%20simulation%20tool%20to%20the%20LLM%20ensures%20that%20the%0Aframework%20can%20generate%20correct%20SVAs.%20In%20our%20experiments%2C%20only%2027%25%20of%0ALLM-generated%20raw%20assertions%20had%20errors%2C%20which%20was%20rectified%20in%20few%20iterations%0Abased%20on%20the%20simulation%20log.%20Our%20results%20on%20OpenTitan%20designs%20show%20that%20LLMs%0Acan%20streamline%20and%20assist%20engineers%20in%20the%20assertion%20generation%20process%2C%0Areshaping%20verification%20workflows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.00093v3&entry.124074799=Read"},
{"title": "Optimal Rate of Kernel Regression in Large Dimensions", "author": "Weihao Lu and Haobo Zhang and Yicheng Li and Manyun Xu and Qian Lin", "abstract": "  We perform a study on kernel regression for large-dimensional data (where the\nsample size $n$ is polynomially depending on the dimension $d$ of the samples,\ni.e., $n\\asymp d^{\\gamma}$ for some $\\gamma >0$ ). We first build a general\ntool to characterize the upper bound and the minimax lower bound of kernel\nregression for large dimensional data through the Mendelson complexity\n$\\varepsilon_{n}^{2}$ and the metric entropy $\\bar{\\varepsilon}_{n}^{2}$\nrespectively. When the target function falls into the RKHS associated with a\n(general) inner product model defined on $\\mathbb{S}^{d}$, we utilize the new\ntool to show that the minimax rate of the excess risk of kernel regression is\n$n^{-1/2}$ when $n\\asymp d^{\\gamma}$ for $\\gamma =2, 4, 6, 8, \\cdots$. We then\nfurther determine the optimal rate of the excess risk of kernel regression for\nall the $\\gamma>0$ and find that the curve of optimal rate varying along\n$\\gamma$ exhibits several new phenomena including the multiple descent behavior\nand the periodic plateau behavior. As an application, For the neural tangent\nkernel (NTK), we also provide a similar explicit description of the curve of\noptimal rate. As a direct corollary, we know these claims hold for wide neural\nnetworks as well.\n", "link": "http://arxiv.org/abs/2309.04268v2", "date": "2024-06-28", "relevancy": 1.7072, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4368}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.42}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4188}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Rate%20of%20Kernel%20Regression%20in%20Large%20Dimensions&body=Title%3A%20Optimal%20Rate%20of%20Kernel%20Regression%20in%20Large%20Dimensions%0AAuthor%3A%20Weihao%20Lu%20and%20Haobo%20Zhang%20and%20Yicheng%20Li%20and%20Manyun%20Xu%20and%20Qian%20Lin%0AAbstract%3A%20%20%20We%20perform%20a%20study%20on%20kernel%20regression%20for%20large-dimensional%20data%20%28where%20the%0Asample%20size%20%24n%24%20is%20polynomially%20depending%20on%20the%20dimension%20%24d%24%20of%20the%20samples%2C%0Ai.e.%2C%20%24n%5Casymp%20d%5E%7B%5Cgamma%7D%24%20for%20some%20%24%5Cgamma%20%3E0%24%20%29.%20We%20first%20build%20a%20general%0Atool%20to%20characterize%20the%20upper%20bound%20and%20the%20minimax%20lower%20bound%20of%20kernel%0Aregression%20for%20large%20dimensional%20data%20through%20the%20Mendelson%20complexity%0A%24%5Cvarepsilon_%7Bn%7D%5E%7B2%7D%24%20and%20the%20metric%20entropy%20%24%5Cbar%7B%5Cvarepsilon%7D_%7Bn%7D%5E%7B2%7D%24%0Arespectively.%20When%20the%20target%20function%20falls%20into%20the%20RKHS%20associated%20with%20a%0A%28general%29%20inner%20product%20model%20defined%20on%20%24%5Cmathbb%7BS%7D%5E%7Bd%7D%24%2C%20we%20utilize%20the%20new%0Atool%20to%20show%20that%20the%20minimax%20rate%20of%20the%20excess%20risk%20of%20kernel%20regression%20is%0A%24n%5E%7B-1/2%7D%24%20when%20%24n%5Casymp%20d%5E%7B%5Cgamma%7D%24%20for%20%24%5Cgamma%20%3D2%2C%204%2C%206%2C%208%2C%20%5Ccdots%24.%20We%20then%0Afurther%20determine%20the%20optimal%20rate%20of%20the%20excess%20risk%20of%20kernel%20regression%20for%0Aall%20the%20%24%5Cgamma%3E0%24%20and%20find%20that%20the%20curve%20of%20optimal%20rate%20varying%20along%0A%24%5Cgamma%24%20exhibits%20several%20new%20phenomena%20including%20the%20multiple%20descent%20behavior%0Aand%20the%20periodic%20plateau%20behavior.%20As%20an%20application%2C%20For%20the%20neural%20tangent%0Akernel%20%28NTK%29%2C%20we%20also%20provide%20a%20similar%20explicit%20description%20of%20the%20curve%20of%0Aoptimal%20rate.%20As%20a%20direct%20corollary%2C%20we%20know%20these%20claims%20hold%20for%20wide%20neural%0Anetworks%20as%20well.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.04268v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Rate%2520of%2520Kernel%2520Regression%2520in%2520Large%2520Dimensions%26entry.906535625%3DWeihao%2520Lu%2520and%2520Haobo%2520Zhang%2520and%2520Yicheng%2520Li%2520and%2520Manyun%2520Xu%2520and%2520Qian%2520Lin%26entry.1292438233%3D%2520%2520We%2520perform%2520a%2520study%2520on%2520kernel%2520regression%2520for%2520large-dimensional%2520data%2520%2528where%2520the%250Asample%2520size%2520%2524n%2524%2520is%2520polynomially%2520depending%2520on%2520the%2520dimension%2520%2524d%2524%2520of%2520the%2520samples%252C%250Ai.e.%252C%2520%2524n%255Casymp%2520d%255E%257B%255Cgamma%257D%2524%2520for%2520some%2520%2524%255Cgamma%2520%253E0%2524%2520%2529.%2520We%2520first%2520build%2520a%2520general%250Atool%2520to%2520characterize%2520the%2520upper%2520bound%2520and%2520the%2520minimax%2520lower%2520bound%2520of%2520kernel%250Aregression%2520for%2520large%2520dimensional%2520data%2520through%2520the%2520Mendelson%2520complexity%250A%2524%255Cvarepsilon_%257Bn%257D%255E%257B2%257D%2524%2520and%2520the%2520metric%2520entropy%2520%2524%255Cbar%257B%255Cvarepsilon%257D_%257Bn%257D%255E%257B2%257D%2524%250Arespectively.%2520When%2520the%2520target%2520function%2520falls%2520into%2520the%2520RKHS%2520associated%2520with%2520a%250A%2528general%2529%2520inner%2520product%2520model%2520defined%2520on%2520%2524%255Cmathbb%257BS%257D%255E%257Bd%257D%2524%252C%2520we%2520utilize%2520the%2520new%250Atool%2520to%2520show%2520that%2520the%2520minimax%2520rate%2520of%2520the%2520excess%2520risk%2520of%2520kernel%2520regression%2520is%250A%2524n%255E%257B-1/2%257D%2524%2520when%2520%2524n%255Casymp%2520d%255E%257B%255Cgamma%257D%2524%2520for%2520%2524%255Cgamma%2520%253D2%252C%25204%252C%25206%252C%25208%252C%2520%255Ccdots%2524.%2520We%2520then%250Afurther%2520determine%2520the%2520optimal%2520rate%2520of%2520the%2520excess%2520risk%2520of%2520kernel%2520regression%2520for%250Aall%2520the%2520%2524%255Cgamma%253E0%2524%2520and%2520find%2520that%2520the%2520curve%2520of%2520optimal%2520rate%2520varying%2520along%250A%2524%255Cgamma%2524%2520exhibits%2520several%2520new%2520phenomena%2520including%2520the%2520multiple%2520descent%2520behavior%250Aand%2520the%2520periodic%2520plateau%2520behavior.%2520As%2520an%2520application%252C%2520For%2520the%2520neural%2520tangent%250Akernel%2520%2528NTK%2529%252C%2520we%2520also%2520provide%2520a%2520similar%2520explicit%2520description%2520of%2520the%2520curve%2520of%250Aoptimal%2520rate.%2520As%2520a%2520direct%2520corollary%252C%2520we%2520know%2520these%2520claims%2520hold%2520for%2520wide%2520neural%250Anetworks%2520as%2520well.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.04268v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Rate%20of%20Kernel%20Regression%20in%20Large%20Dimensions&entry.906535625=Weihao%20Lu%20and%20Haobo%20Zhang%20and%20Yicheng%20Li%20and%20Manyun%20Xu%20and%20Qian%20Lin&entry.1292438233=%20%20We%20perform%20a%20study%20on%20kernel%20regression%20for%20large-dimensional%20data%20%28where%20the%0Asample%20size%20%24n%24%20is%20polynomially%20depending%20on%20the%20dimension%20%24d%24%20of%20the%20samples%2C%0Ai.e.%2C%20%24n%5Casymp%20d%5E%7B%5Cgamma%7D%24%20for%20some%20%24%5Cgamma%20%3E0%24%20%29.%20We%20first%20build%20a%20general%0Atool%20to%20characterize%20the%20upper%20bound%20and%20the%20minimax%20lower%20bound%20of%20kernel%0Aregression%20for%20large%20dimensional%20data%20through%20the%20Mendelson%20complexity%0A%24%5Cvarepsilon_%7Bn%7D%5E%7B2%7D%24%20and%20the%20metric%20entropy%20%24%5Cbar%7B%5Cvarepsilon%7D_%7Bn%7D%5E%7B2%7D%24%0Arespectively.%20When%20the%20target%20function%20falls%20into%20the%20RKHS%20associated%20with%20a%0A%28general%29%20inner%20product%20model%20defined%20on%20%24%5Cmathbb%7BS%7D%5E%7Bd%7D%24%2C%20we%20utilize%20the%20new%0Atool%20to%20show%20that%20the%20minimax%20rate%20of%20the%20excess%20risk%20of%20kernel%20regression%20is%0A%24n%5E%7B-1/2%7D%24%20when%20%24n%5Casymp%20d%5E%7B%5Cgamma%7D%24%20for%20%24%5Cgamma%20%3D2%2C%204%2C%206%2C%208%2C%20%5Ccdots%24.%20We%20then%0Afurther%20determine%20the%20optimal%20rate%20of%20the%20excess%20risk%20of%20kernel%20regression%20for%0Aall%20the%20%24%5Cgamma%3E0%24%20and%20find%20that%20the%20curve%20of%20optimal%20rate%20varying%20along%0A%24%5Cgamma%24%20exhibits%20several%20new%20phenomena%20including%20the%20multiple%20descent%20behavior%0Aand%20the%20periodic%20plateau%20behavior.%20As%20an%20application%2C%20For%20the%20neural%20tangent%0Akernel%20%28NTK%29%2C%20we%20also%20provide%20a%20similar%20explicit%20description%20of%20the%20curve%20of%0Aoptimal%20rate.%20As%20a%20direct%20corollary%2C%20we%20know%20these%20claims%20hold%20for%20wide%20neural%0Anetworks%20as%20well.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.04268v2&entry.124074799=Read"},
{"title": "Machine Learning Predictors for Min-Entropy Estimation", "author": "Javier Blanco-Romero and Vicente Lorenzo and Florina Almenares Mendoza and Daniel D\u00edaz-S\u00e1nchez", "abstract": "  This study investigates the application of machine learning predictors for\nmin-entropy estimation in Random Number Generators (RNGs), a key component in\ncryptographic applications where accurate entropy assessment is essential for\ncybersecurity. Our research indicates that these predictors, and indeed any\npredictor that leverages sequence correlations, primarily estimate average\nmin-entropy, a metric not extensively studied in this context. We explore the\nrelationship between average min-entropy and the traditional min-entropy,\nfocusing on their dependence on the number of target bits being predicted.\nUtilizing data from Generalized Binary Autoregressive Models, a subset of\nMarkov processes, we demonstrate that machine learning models (including a\nhybrid of convolutional and recurrent Long Short-Term Memory layers and the\ntransformer-based GPT-2 model) outperform traditional NIST SP 800-90B\npredictors in certain scenarios. Our findings underscore the importance of\nconsidering the number of target bits in min-entropy assessment for RNGs and\nhighlight the potential of machine learning approaches in enhancing entropy\nestimation techniques for improved cryptographic security.\n", "link": "http://arxiv.org/abs/2406.19983v1", "date": "2024-06-28", "relevancy": 1.6997, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.428}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4247}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Machine%20Learning%20Predictors%20for%20Min-Entropy%20Estimation&body=Title%3A%20Machine%20Learning%20Predictors%20for%20Min-Entropy%20Estimation%0AAuthor%3A%20Javier%20Blanco-Romero%20and%20Vicente%20Lorenzo%20and%20Florina%20Almenares%20Mendoza%20and%20Daniel%20D%C3%ADaz-S%C3%A1nchez%0AAbstract%3A%20%20%20This%20study%20investigates%20the%20application%20of%20machine%20learning%20predictors%20for%0Amin-entropy%20estimation%20in%20Random%20Number%20Generators%20%28RNGs%29%2C%20a%20key%20component%20in%0Acryptographic%20applications%20where%20accurate%20entropy%20assessment%20is%20essential%20for%0Acybersecurity.%20Our%20research%20indicates%20that%20these%20predictors%2C%20and%20indeed%20any%0Apredictor%20that%20leverages%20sequence%20correlations%2C%20primarily%20estimate%20average%0Amin-entropy%2C%20a%20metric%20not%20extensively%20studied%20in%20this%20context.%20We%20explore%20the%0Arelationship%20between%20average%20min-entropy%20and%20the%20traditional%20min-entropy%2C%0Afocusing%20on%20their%20dependence%20on%20the%20number%20of%20target%20bits%20being%20predicted.%0AUtilizing%20data%20from%20Generalized%20Binary%20Autoregressive%20Models%2C%20a%20subset%20of%0AMarkov%20processes%2C%20we%20demonstrate%20that%20machine%20learning%20models%20%28including%20a%0Ahybrid%20of%20convolutional%20and%20recurrent%20Long%20Short-Term%20Memory%20layers%20and%20the%0Atransformer-based%20GPT-2%20model%29%20outperform%20traditional%20NIST%20SP%20800-90B%0Apredictors%20in%20certain%20scenarios.%20Our%20findings%20underscore%20the%20importance%20of%0Aconsidering%20the%20number%20of%20target%20bits%20in%20min-entropy%20assessment%20for%20RNGs%20and%0Ahighlight%20the%20potential%20of%20machine%20learning%20approaches%20in%20enhancing%20entropy%0Aestimation%20techniques%20for%20improved%20cryptographic%20security.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19983v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMachine%2520Learning%2520Predictors%2520for%2520Min-Entropy%2520Estimation%26entry.906535625%3DJavier%2520Blanco-Romero%2520and%2520Vicente%2520Lorenzo%2520and%2520Florina%2520Almenares%2520Mendoza%2520and%2520Daniel%2520D%25C3%25ADaz-S%25C3%25A1nchez%26entry.1292438233%3D%2520%2520This%2520study%2520investigates%2520the%2520application%2520of%2520machine%2520learning%2520predictors%2520for%250Amin-entropy%2520estimation%2520in%2520Random%2520Number%2520Generators%2520%2528RNGs%2529%252C%2520a%2520key%2520component%2520in%250Acryptographic%2520applications%2520where%2520accurate%2520entropy%2520assessment%2520is%2520essential%2520for%250Acybersecurity.%2520Our%2520research%2520indicates%2520that%2520these%2520predictors%252C%2520and%2520indeed%2520any%250Apredictor%2520that%2520leverages%2520sequence%2520correlations%252C%2520primarily%2520estimate%2520average%250Amin-entropy%252C%2520a%2520metric%2520not%2520extensively%2520studied%2520in%2520this%2520context.%2520We%2520explore%2520the%250Arelationship%2520between%2520average%2520min-entropy%2520and%2520the%2520traditional%2520min-entropy%252C%250Afocusing%2520on%2520their%2520dependence%2520on%2520the%2520number%2520of%2520target%2520bits%2520being%2520predicted.%250AUtilizing%2520data%2520from%2520Generalized%2520Binary%2520Autoregressive%2520Models%252C%2520a%2520subset%2520of%250AMarkov%2520processes%252C%2520we%2520demonstrate%2520that%2520machine%2520learning%2520models%2520%2528including%2520a%250Ahybrid%2520of%2520convolutional%2520and%2520recurrent%2520Long%2520Short-Term%2520Memory%2520layers%2520and%2520the%250Atransformer-based%2520GPT-2%2520model%2529%2520outperform%2520traditional%2520NIST%2520SP%2520800-90B%250Apredictors%2520in%2520certain%2520scenarios.%2520Our%2520findings%2520underscore%2520the%2520importance%2520of%250Aconsidering%2520the%2520number%2520of%2520target%2520bits%2520in%2520min-entropy%2520assessment%2520for%2520RNGs%2520and%250Ahighlight%2520the%2520potential%2520of%2520machine%2520learning%2520approaches%2520in%2520enhancing%2520entropy%250Aestimation%2520techniques%2520for%2520improved%2520cryptographic%2520security.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19983v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Machine%20Learning%20Predictors%20for%20Min-Entropy%20Estimation&entry.906535625=Javier%20Blanco-Romero%20and%20Vicente%20Lorenzo%20and%20Florina%20Almenares%20Mendoza%20and%20Daniel%20D%C3%ADaz-S%C3%A1nchez&entry.1292438233=%20%20This%20study%20investigates%20the%20application%20of%20machine%20learning%20predictors%20for%0Amin-entropy%20estimation%20in%20Random%20Number%20Generators%20%28RNGs%29%2C%20a%20key%20component%20in%0Acryptographic%20applications%20where%20accurate%20entropy%20assessment%20is%20essential%20for%0Acybersecurity.%20Our%20research%20indicates%20that%20these%20predictors%2C%20and%20indeed%20any%0Apredictor%20that%20leverages%20sequence%20correlations%2C%20primarily%20estimate%20average%0Amin-entropy%2C%20a%20metric%20not%20extensively%20studied%20in%20this%20context.%20We%20explore%20the%0Arelationship%20between%20average%20min-entropy%20and%20the%20traditional%20min-entropy%2C%0Afocusing%20on%20their%20dependence%20on%20the%20number%20of%20target%20bits%20being%20predicted.%0AUtilizing%20data%20from%20Generalized%20Binary%20Autoregressive%20Models%2C%20a%20subset%20of%0AMarkov%20processes%2C%20we%20demonstrate%20that%20machine%20learning%20models%20%28including%20a%0Ahybrid%20of%20convolutional%20and%20recurrent%20Long%20Short-Term%20Memory%20layers%20and%20the%0Atransformer-based%20GPT-2%20model%29%20outperform%20traditional%20NIST%20SP%20800-90B%0Apredictors%20in%20certain%20scenarios.%20Our%20findings%20underscore%20the%20importance%20of%0Aconsidering%20the%20number%20of%20target%20bits%20in%20min-entropy%20assessment%20for%20RNGs%20and%0Ahighlight%20the%20potential%20of%20machine%20learning%20approaches%20in%20enhancing%20entropy%0Aestimation%20techniques%20for%20improved%20cryptographic%20security.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19983v1&entry.124074799=Read"},
{"title": "Improved Monte Carlo tree search (MCTS) formulation with multiple root\n  nodes for discrete sizing optimization of truss structures", "author": "Fu-Yao Ko and Katsuyuki Suzuki and Kazuo Yonekura", "abstract": "  This paper proposes a new method for discrete optimum design of truss\nstructures utilizing Monte Carlo tree search (MCTS) with update process, the\nbest reward, accelerating technique, and terminal condition. An improved MCTS\nformulation with multiple root nodes is developed in this study. Update process\nmeans that once a final solution is found, it is used as the initial solution\nfor next search tree. The best reward is used in the backpropagation step.\nAccelerating technique is introduced by decreasing the width of search tree and\nreducing maximum number of iterations. The agent is trained to minimize the\ntotal structural weight under various constraints until the terminal condition\nis satisfied. Then, optimal solution is the minimum value of all solutions\nfound by search trees. These numerical examples show that the agent can find\noptimal solution with low computational cost, stably produces an optimal\ndesign, and is suitable for practical engineering problems.\n", "link": "http://arxiv.org/abs/2309.06045v2", "date": "2024-06-28", "relevancy": 1.6815, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4365}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4216}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4127}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improved%20Monte%20Carlo%20tree%20search%20%28MCTS%29%20formulation%20with%20multiple%20root%0A%20%20nodes%20for%20discrete%20sizing%20optimization%20of%20truss%20structures&body=Title%3A%20Improved%20Monte%20Carlo%20tree%20search%20%28MCTS%29%20formulation%20with%20multiple%20root%0A%20%20nodes%20for%20discrete%20sizing%20optimization%20of%20truss%20structures%0AAuthor%3A%20Fu-Yao%20Ko%20and%20Katsuyuki%20Suzuki%20and%20Kazuo%20Yonekura%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20new%20method%20for%20discrete%20optimum%20design%20of%20truss%0Astructures%20utilizing%20Monte%20Carlo%20tree%20search%20%28MCTS%29%20with%20update%20process%2C%20the%0Abest%20reward%2C%20accelerating%20technique%2C%20and%20terminal%20condition.%20An%20improved%20MCTS%0Aformulation%20with%20multiple%20root%20nodes%20is%20developed%20in%20this%20study.%20Update%20process%0Ameans%20that%20once%20a%20final%20solution%20is%20found%2C%20it%20is%20used%20as%20the%20initial%20solution%0Afor%20next%20search%20tree.%20The%20best%20reward%20is%20used%20in%20the%20backpropagation%20step.%0AAccelerating%20technique%20is%20introduced%20by%20decreasing%20the%20width%20of%20search%20tree%20and%0Areducing%20maximum%20number%20of%20iterations.%20The%20agent%20is%20trained%20to%20minimize%20the%0Atotal%20structural%20weight%20under%20various%20constraints%20until%20the%20terminal%20condition%0Ais%20satisfied.%20Then%2C%20optimal%20solution%20is%20the%20minimum%20value%20of%20all%20solutions%0Afound%20by%20search%20trees.%20These%20numerical%20examples%20show%20that%20the%20agent%20can%20find%0Aoptimal%20solution%20with%20low%20computational%20cost%2C%20stably%20produces%20an%20optimal%0Adesign%2C%20and%20is%20suitable%20for%20practical%20engineering%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.06045v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproved%2520Monte%2520Carlo%2520tree%2520search%2520%2528MCTS%2529%2520formulation%2520with%2520multiple%2520root%250A%2520%2520nodes%2520for%2520discrete%2520sizing%2520optimization%2520of%2520truss%2520structures%26entry.906535625%3DFu-Yao%2520Ko%2520and%2520Katsuyuki%2520Suzuki%2520and%2520Kazuo%2520Yonekura%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520new%2520method%2520for%2520discrete%2520optimum%2520design%2520of%2520truss%250Astructures%2520utilizing%2520Monte%2520Carlo%2520tree%2520search%2520%2528MCTS%2529%2520with%2520update%2520process%252C%2520the%250Abest%2520reward%252C%2520accelerating%2520technique%252C%2520and%2520terminal%2520condition.%2520An%2520improved%2520MCTS%250Aformulation%2520with%2520multiple%2520root%2520nodes%2520is%2520developed%2520in%2520this%2520study.%2520Update%2520process%250Ameans%2520that%2520once%2520a%2520final%2520solution%2520is%2520found%252C%2520it%2520is%2520used%2520as%2520the%2520initial%2520solution%250Afor%2520next%2520search%2520tree.%2520The%2520best%2520reward%2520is%2520used%2520in%2520the%2520backpropagation%2520step.%250AAccelerating%2520technique%2520is%2520introduced%2520by%2520decreasing%2520the%2520width%2520of%2520search%2520tree%2520and%250Areducing%2520maximum%2520number%2520of%2520iterations.%2520The%2520agent%2520is%2520trained%2520to%2520minimize%2520the%250Atotal%2520structural%2520weight%2520under%2520various%2520constraints%2520until%2520the%2520terminal%2520condition%250Ais%2520satisfied.%2520Then%252C%2520optimal%2520solution%2520is%2520the%2520minimum%2520value%2520of%2520all%2520solutions%250Afound%2520by%2520search%2520trees.%2520These%2520numerical%2520examples%2520show%2520that%2520the%2520agent%2520can%2520find%250Aoptimal%2520solution%2520with%2520low%2520computational%2520cost%252C%2520stably%2520produces%2520an%2520optimal%250Adesign%252C%2520and%2520is%2520suitable%2520for%2520practical%2520engineering%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.06045v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20Monte%20Carlo%20tree%20search%20%28MCTS%29%20formulation%20with%20multiple%20root%0A%20%20nodes%20for%20discrete%20sizing%20optimization%20of%20truss%20structures&entry.906535625=Fu-Yao%20Ko%20and%20Katsuyuki%20Suzuki%20and%20Kazuo%20Yonekura&entry.1292438233=%20%20This%20paper%20proposes%20a%20new%20method%20for%20discrete%20optimum%20design%20of%20truss%0Astructures%20utilizing%20Monte%20Carlo%20tree%20search%20%28MCTS%29%20with%20update%20process%2C%20the%0Abest%20reward%2C%20accelerating%20technique%2C%20and%20terminal%20condition.%20An%20improved%20MCTS%0Aformulation%20with%20multiple%20root%20nodes%20is%20developed%20in%20this%20study.%20Update%20process%0Ameans%20that%20once%20a%20final%20solution%20is%20found%2C%20it%20is%20used%20as%20the%20initial%20solution%0Afor%20next%20search%20tree.%20The%20best%20reward%20is%20used%20in%20the%20backpropagation%20step.%0AAccelerating%20technique%20is%20introduced%20by%20decreasing%20the%20width%20of%20search%20tree%20and%0Areducing%20maximum%20number%20of%20iterations.%20The%20agent%20is%20trained%20to%20minimize%20the%0Atotal%20structural%20weight%20under%20various%20constraints%20until%20the%20terminal%20condition%0Ais%20satisfied.%20Then%2C%20optimal%20solution%20is%20the%20minimum%20value%20of%20all%20solutions%0Afound%20by%20search%20trees.%20These%20numerical%20examples%20show%20that%20the%20agent%20can%20find%0Aoptimal%20solution%20with%20low%20computational%20cost%2C%20stably%20produces%20an%20optimal%0Adesign%2C%20and%20is%20suitable%20for%20practical%20engineering%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.06045v2&entry.124074799=Read"},
{"title": "Mobile Robot Oriented Large-Scale Indoor Dataset for Dynamic Scene\n  Understanding", "author": "Yifan Tang and Cong Tai and Fangxing Chen and Wanting Zhang and Tao Zhang and Xueping Liu and Yongjin Liu and Long Zeng", "abstract": "  Most existing robotic datasets capture static scene data and thus are limited\nin evaluating robots' dynamic performance. To address this, we present a mobile\nrobot oriented large-scale indoor dataset, denoted as THUD (Tsinghua University\nDynamic) robotic dataset, for training and evaluating their dynamic scene\nunderstanding algorithms. Specifically, the THUD dataset construction is first\ndetailed, including organization, acquisition, and annotation methods. It\ncomprises both real-world and synthetic data, collected with a real robot\nplatform and a physical simulation platform, respectively. Our current dataset\nincludes 13 larges-scale dynamic scenarios, 90K image frames, 20M 2D/3D\nbounding boxes of static and dynamic objects, camera poses, and IMU. The\ndataset is still continuously expanding. Then, the performance of mainstream\nindoor scene understanding tasks, e.g. 3D object detection, semantic\nsegmentation, and robot relocalization, is evaluated on our THUD dataset. These\nexperiments reveal serious challenges for some robot scene understanding tasks\nin dynamic scenes. By sharing this dataset, we aim to foster and iterate new\nmobile robot algorithms quickly for robot actual working dynamic environment,\ni.e. complex crowded dynamic scenes.\n", "link": "http://arxiv.org/abs/2406.19791v1", "date": "2024-06-28", "relevancy": 1.6759, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6011}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5586}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mobile%20Robot%20Oriented%20Large-Scale%20Indoor%20Dataset%20for%20Dynamic%20Scene%0A%20%20Understanding&body=Title%3A%20Mobile%20Robot%20Oriented%20Large-Scale%20Indoor%20Dataset%20for%20Dynamic%20Scene%0A%20%20Understanding%0AAuthor%3A%20Yifan%20Tang%20and%20Cong%20Tai%20and%20Fangxing%20Chen%20and%20Wanting%20Zhang%20and%20Tao%20Zhang%20and%20Xueping%20Liu%20and%20Yongjin%20Liu%20and%20Long%20Zeng%0AAbstract%3A%20%20%20Most%20existing%20robotic%20datasets%20capture%20static%20scene%20data%20and%20thus%20are%20limited%0Ain%20evaluating%20robots%27%20dynamic%20performance.%20To%20address%20this%2C%20we%20present%20a%20mobile%0Arobot%20oriented%20large-scale%20indoor%20dataset%2C%20denoted%20as%20THUD%20%28Tsinghua%20University%0ADynamic%29%20robotic%20dataset%2C%20for%20training%20and%20evaluating%20their%20dynamic%20scene%0Aunderstanding%20algorithms.%20Specifically%2C%20the%20THUD%20dataset%20construction%20is%20first%0Adetailed%2C%20including%20organization%2C%20acquisition%2C%20and%20annotation%20methods.%20It%0Acomprises%20both%20real-world%20and%20synthetic%20data%2C%20collected%20with%20a%20real%20robot%0Aplatform%20and%20a%20physical%20simulation%20platform%2C%20respectively.%20Our%20current%20dataset%0Aincludes%2013%20larges-scale%20dynamic%20scenarios%2C%2090K%20image%20frames%2C%2020M%202D/3D%0Abounding%20boxes%20of%20static%20and%20dynamic%20objects%2C%20camera%20poses%2C%20and%20IMU.%20The%0Adataset%20is%20still%20continuously%20expanding.%20Then%2C%20the%20performance%20of%20mainstream%0Aindoor%20scene%20understanding%20tasks%2C%20e.g.%203D%20object%20detection%2C%20semantic%0Asegmentation%2C%20and%20robot%20relocalization%2C%20is%20evaluated%20on%20our%20THUD%20dataset.%20These%0Aexperiments%20reveal%20serious%20challenges%20for%20some%20robot%20scene%20understanding%20tasks%0Ain%20dynamic%20scenes.%20By%20sharing%20this%20dataset%2C%20we%20aim%20to%20foster%20and%20iterate%20new%0Amobile%20robot%20algorithms%20quickly%20for%20robot%20actual%20working%20dynamic%20environment%2C%0Ai.e.%20complex%20crowded%20dynamic%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19791v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMobile%2520Robot%2520Oriented%2520Large-Scale%2520Indoor%2520Dataset%2520for%2520Dynamic%2520Scene%250A%2520%2520Understanding%26entry.906535625%3DYifan%2520Tang%2520and%2520Cong%2520Tai%2520and%2520Fangxing%2520Chen%2520and%2520Wanting%2520Zhang%2520and%2520Tao%2520Zhang%2520and%2520Xueping%2520Liu%2520and%2520Yongjin%2520Liu%2520and%2520Long%2520Zeng%26entry.1292438233%3D%2520%2520Most%2520existing%2520robotic%2520datasets%2520capture%2520static%2520scene%2520data%2520and%2520thus%2520are%2520limited%250Ain%2520evaluating%2520robots%2527%2520dynamic%2520performance.%2520To%2520address%2520this%252C%2520we%2520present%2520a%2520mobile%250Arobot%2520oriented%2520large-scale%2520indoor%2520dataset%252C%2520denoted%2520as%2520THUD%2520%2528Tsinghua%2520University%250ADynamic%2529%2520robotic%2520dataset%252C%2520for%2520training%2520and%2520evaluating%2520their%2520dynamic%2520scene%250Aunderstanding%2520algorithms.%2520Specifically%252C%2520the%2520THUD%2520dataset%2520construction%2520is%2520first%250Adetailed%252C%2520including%2520organization%252C%2520acquisition%252C%2520and%2520annotation%2520methods.%2520It%250Acomprises%2520both%2520real-world%2520and%2520synthetic%2520data%252C%2520collected%2520with%2520a%2520real%2520robot%250Aplatform%2520and%2520a%2520physical%2520simulation%2520platform%252C%2520respectively.%2520Our%2520current%2520dataset%250Aincludes%252013%2520larges-scale%2520dynamic%2520scenarios%252C%252090K%2520image%2520frames%252C%252020M%25202D/3D%250Abounding%2520boxes%2520of%2520static%2520and%2520dynamic%2520objects%252C%2520camera%2520poses%252C%2520and%2520IMU.%2520The%250Adataset%2520is%2520still%2520continuously%2520expanding.%2520Then%252C%2520the%2520performance%2520of%2520mainstream%250Aindoor%2520scene%2520understanding%2520tasks%252C%2520e.g.%25203D%2520object%2520detection%252C%2520semantic%250Asegmentation%252C%2520and%2520robot%2520relocalization%252C%2520is%2520evaluated%2520on%2520our%2520THUD%2520dataset.%2520These%250Aexperiments%2520reveal%2520serious%2520challenges%2520for%2520some%2520robot%2520scene%2520understanding%2520tasks%250Ain%2520dynamic%2520scenes.%2520By%2520sharing%2520this%2520dataset%252C%2520we%2520aim%2520to%2520foster%2520and%2520iterate%2520new%250Amobile%2520robot%2520algorithms%2520quickly%2520for%2520robot%2520actual%2520working%2520dynamic%2520environment%252C%250Ai.e.%2520complex%2520crowded%2520dynamic%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19791v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mobile%20Robot%20Oriented%20Large-Scale%20Indoor%20Dataset%20for%20Dynamic%20Scene%0A%20%20Understanding&entry.906535625=Yifan%20Tang%20and%20Cong%20Tai%20and%20Fangxing%20Chen%20and%20Wanting%20Zhang%20and%20Tao%20Zhang%20and%20Xueping%20Liu%20and%20Yongjin%20Liu%20and%20Long%20Zeng&entry.1292438233=%20%20Most%20existing%20robotic%20datasets%20capture%20static%20scene%20data%20and%20thus%20are%20limited%0Ain%20evaluating%20robots%27%20dynamic%20performance.%20To%20address%20this%2C%20we%20present%20a%20mobile%0Arobot%20oriented%20large-scale%20indoor%20dataset%2C%20denoted%20as%20THUD%20%28Tsinghua%20University%0ADynamic%29%20robotic%20dataset%2C%20for%20training%20and%20evaluating%20their%20dynamic%20scene%0Aunderstanding%20algorithms.%20Specifically%2C%20the%20THUD%20dataset%20construction%20is%20first%0Adetailed%2C%20including%20organization%2C%20acquisition%2C%20and%20annotation%20methods.%20It%0Acomprises%20both%20real-world%20and%20synthetic%20data%2C%20collected%20with%20a%20real%20robot%0Aplatform%20and%20a%20physical%20simulation%20platform%2C%20respectively.%20Our%20current%20dataset%0Aincludes%2013%20larges-scale%20dynamic%20scenarios%2C%2090K%20image%20frames%2C%2020M%202D/3D%0Abounding%20boxes%20of%20static%20and%20dynamic%20objects%2C%20camera%20poses%2C%20and%20IMU.%20The%0Adataset%20is%20still%20continuously%20expanding.%20Then%2C%20the%20performance%20of%20mainstream%0Aindoor%20scene%20understanding%20tasks%2C%20e.g.%203D%20object%20detection%2C%20semantic%0Asegmentation%2C%20and%20robot%20relocalization%2C%20is%20evaluated%20on%20our%20THUD%20dataset.%20These%0Aexperiments%20reveal%20serious%20challenges%20for%20some%20robot%20scene%20understanding%20tasks%0Ain%20dynamic%20scenes.%20By%20sharing%20this%20dataset%2C%20we%20aim%20to%20foster%20and%20iterate%20new%0Amobile%20robot%20algorithms%20quickly%20for%20robot%20actual%20working%20dynamic%20environment%2C%0Ai.e.%20complex%20crowded%20dynamic%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19791v1&entry.124074799=Read"},
{"title": "Learning to utilize image second-order derivative information for crisp\n  edge detection", "author": "Changsong Liu and Wei Zhang and Yanyan Liu and Yimeng Fan and Mingyang Li and Wenlin Li", "abstract": "  Edge detection is a fundamental task in computer vision. It has made great\nprogress under the development of deep convolutional neural networks (DCNNs),\nsome of which have achieved a beyond human-level performance. However, recent\ntop-performing edge detection methods tend to generate thick and noisy edge\nlines. In this work, we solve this problem from two aspects: (1) the lack of\nprior knowledge regarding image edges, and (2) the issue of imbalanced pixel\ndistribution. We propose a second-order derivative-based multi-scale contextual\nenhancement module (SDMCM) to help the model locate true edge pixels accurately\nby introducing the edge prior knowledge. We also construct a hybrid focal loss\nfunction (HFL) to alleviate the imbalanced distribution issue. In addition, we\nemploy the conditionally parameterized convolution (CondConv) to develop a\nnovel boundary refinement module (BRM), which can further refine the final\noutput edge maps. In the end, we propose a U-shape network named LUS-Net which\nis based on the SDMCM and BRM for crisp edge detection. We perform extensive\nexperiments on three standard benchmarks, and the experiment results illustrate\nthat our method can predict crisp and clean edge maps and achieves\nstate-of-the-art performance on the BSDS500 dataset (ODS=0.829), NYUD-V2\ndataset (ODS=0.768), and BIPED dataset (ODS=0.903).\n", "link": "http://arxiv.org/abs/2406.05779v3", "date": "2024-06-28", "relevancy": 1.6712, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6052}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5441}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20utilize%20image%20second-order%20derivative%20information%20for%20crisp%0A%20%20edge%20detection&body=Title%3A%20Learning%20to%20utilize%20image%20second-order%20derivative%20information%20for%20crisp%0A%20%20edge%20detection%0AAuthor%3A%20Changsong%20Liu%20and%20Wei%20Zhang%20and%20Yanyan%20Liu%20and%20Yimeng%20Fan%20and%20Mingyang%20Li%20and%20Wenlin%20Li%0AAbstract%3A%20%20%20Edge%20detection%20is%20a%20fundamental%20task%20in%20computer%20vision.%20It%20has%20made%20great%0Aprogress%20under%20the%20development%20of%20deep%20convolutional%20neural%20networks%20%28DCNNs%29%2C%0Asome%20of%20which%20have%20achieved%20a%20beyond%20human-level%20performance.%20However%2C%20recent%0Atop-performing%20edge%20detection%20methods%20tend%20to%20generate%20thick%20and%20noisy%20edge%0Alines.%20In%20this%20work%2C%20we%20solve%20this%20problem%20from%20two%20aspects%3A%20%281%29%20the%20lack%20of%0Aprior%20knowledge%20regarding%20image%20edges%2C%20and%20%282%29%20the%20issue%20of%20imbalanced%20pixel%0Adistribution.%20We%20propose%20a%20second-order%20derivative-based%20multi-scale%20contextual%0Aenhancement%20module%20%28SDMCM%29%20to%20help%20the%20model%20locate%20true%20edge%20pixels%20accurately%0Aby%20introducing%20the%20edge%20prior%20knowledge.%20We%20also%20construct%20a%20hybrid%20focal%20loss%0Afunction%20%28HFL%29%20to%20alleviate%20the%20imbalanced%20distribution%20issue.%20In%20addition%2C%20we%0Aemploy%20the%20conditionally%20parameterized%20convolution%20%28CondConv%29%20to%20develop%20a%0Anovel%20boundary%20refinement%20module%20%28BRM%29%2C%20which%20can%20further%20refine%20the%20final%0Aoutput%20edge%20maps.%20In%20the%20end%2C%20we%20propose%20a%20U-shape%20network%20named%20LUS-Net%20which%0Ais%20based%20on%20the%20SDMCM%20and%20BRM%20for%20crisp%20edge%20detection.%20We%20perform%20extensive%0Aexperiments%20on%20three%20standard%20benchmarks%2C%20and%20the%20experiment%20results%20illustrate%0Athat%20our%20method%20can%20predict%20crisp%20and%20clean%20edge%20maps%20and%20achieves%0Astate-of-the-art%20performance%20on%20the%20BSDS500%20dataset%20%28ODS%3D0.829%29%2C%20NYUD-V2%0Adataset%20%28ODS%3D0.768%29%2C%20and%20BIPED%20dataset%20%28ODS%3D0.903%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05779v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520utilize%2520image%2520second-order%2520derivative%2520information%2520for%2520crisp%250A%2520%2520edge%2520detection%26entry.906535625%3DChangsong%2520Liu%2520and%2520Wei%2520Zhang%2520and%2520Yanyan%2520Liu%2520and%2520Yimeng%2520Fan%2520and%2520Mingyang%2520Li%2520and%2520Wenlin%2520Li%26entry.1292438233%3D%2520%2520Edge%2520detection%2520is%2520a%2520fundamental%2520task%2520in%2520computer%2520vision.%2520It%2520has%2520made%2520great%250Aprogress%2520under%2520the%2520development%2520of%2520deep%2520convolutional%2520neural%2520networks%2520%2528DCNNs%2529%252C%250Asome%2520of%2520which%2520have%2520achieved%2520a%2520beyond%2520human-level%2520performance.%2520However%252C%2520recent%250Atop-performing%2520edge%2520detection%2520methods%2520tend%2520to%2520generate%2520thick%2520and%2520noisy%2520edge%250Alines.%2520In%2520this%2520work%252C%2520we%2520solve%2520this%2520problem%2520from%2520two%2520aspects%253A%2520%25281%2529%2520the%2520lack%2520of%250Aprior%2520knowledge%2520regarding%2520image%2520edges%252C%2520and%2520%25282%2529%2520the%2520issue%2520of%2520imbalanced%2520pixel%250Adistribution.%2520We%2520propose%2520a%2520second-order%2520derivative-based%2520multi-scale%2520contextual%250Aenhancement%2520module%2520%2528SDMCM%2529%2520to%2520help%2520the%2520model%2520locate%2520true%2520edge%2520pixels%2520accurately%250Aby%2520introducing%2520the%2520edge%2520prior%2520knowledge.%2520We%2520also%2520construct%2520a%2520hybrid%2520focal%2520loss%250Afunction%2520%2528HFL%2529%2520to%2520alleviate%2520the%2520imbalanced%2520distribution%2520issue.%2520In%2520addition%252C%2520we%250Aemploy%2520the%2520conditionally%2520parameterized%2520convolution%2520%2528CondConv%2529%2520to%2520develop%2520a%250Anovel%2520boundary%2520refinement%2520module%2520%2528BRM%2529%252C%2520which%2520can%2520further%2520refine%2520the%2520final%250Aoutput%2520edge%2520maps.%2520In%2520the%2520end%252C%2520we%2520propose%2520a%2520U-shape%2520network%2520named%2520LUS-Net%2520which%250Ais%2520based%2520on%2520the%2520SDMCM%2520and%2520BRM%2520for%2520crisp%2520edge%2520detection.%2520We%2520perform%2520extensive%250Aexperiments%2520on%2520three%2520standard%2520benchmarks%252C%2520and%2520the%2520experiment%2520results%2520illustrate%250Athat%2520our%2520method%2520can%2520predict%2520crisp%2520and%2520clean%2520edge%2520maps%2520and%2520achieves%250Astate-of-the-art%2520performance%2520on%2520the%2520BSDS500%2520dataset%2520%2528ODS%253D0.829%2529%252C%2520NYUD-V2%250Adataset%2520%2528ODS%253D0.768%2529%252C%2520and%2520BIPED%2520dataset%2520%2528ODS%253D0.903%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05779v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20utilize%20image%20second-order%20derivative%20information%20for%20crisp%0A%20%20edge%20detection&entry.906535625=Changsong%20Liu%20and%20Wei%20Zhang%20and%20Yanyan%20Liu%20and%20Yimeng%20Fan%20and%20Mingyang%20Li%20and%20Wenlin%20Li&entry.1292438233=%20%20Edge%20detection%20is%20a%20fundamental%20task%20in%20computer%20vision.%20It%20has%20made%20great%0Aprogress%20under%20the%20development%20of%20deep%20convolutional%20neural%20networks%20%28DCNNs%29%2C%0Asome%20of%20which%20have%20achieved%20a%20beyond%20human-level%20performance.%20However%2C%20recent%0Atop-performing%20edge%20detection%20methods%20tend%20to%20generate%20thick%20and%20noisy%20edge%0Alines.%20In%20this%20work%2C%20we%20solve%20this%20problem%20from%20two%20aspects%3A%20%281%29%20the%20lack%20of%0Aprior%20knowledge%20regarding%20image%20edges%2C%20and%20%282%29%20the%20issue%20of%20imbalanced%20pixel%0Adistribution.%20We%20propose%20a%20second-order%20derivative-based%20multi-scale%20contextual%0Aenhancement%20module%20%28SDMCM%29%20to%20help%20the%20model%20locate%20true%20edge%20pixels%20accurately%0Aby%20introducing%20the%20edge%20prior%20knowledge.%20We%20also%20construct%20a%20hybrid%20focal%20loss%0Afunction%20%28HFL%29%20to%20alleviate%20the%20imbalanced%20distribution%20issue.%20In%20addition%2C%20we%0Aemploy%20the%20conditionally%20parameterized%20convolution%20%28CondConv%29%20to%20develop%20a%0Anovel%20boundary%20refinement%20module%20%28BRM%29%2C%20which%20can%20further%20refine%20the%20final%0Aoutput%20edge%20maps.%20In%20the%20end%2C%20we%20propose%20a%20U-shape%20network%20named%20LUS-Net%20which%0Ais%20based%20on%20the%20SDMCM%20and%20BRM%20for%20crisp%20edge%20detection.%20We%20perform%20extensive%0Aexperiments%20on%20three%20standard%20benchmarks%2C%20and%20the%20experiment%20results%20illustrate%0Athat%20our%20method%20can%20predict%20crisp%20and%20clean%20edge%20maps%20and%20achieves%0Astate-of-the-art%20performance%20on%20the%20BSDS500%20dataset%20%28ODS%3D0.829%29%2C%20NYUD-V2%0Adataset%20%28ODS%3D0.768%29%2C%20and%20BIPED%20dataset%20%28ODS%3D0.903%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05779v3&entry.124074799=Read"},
{"title": "A Framework for Learning and Reusing Robotic Skills", "author": "Brendan Hertel and Nhu Tran and Meriem Elkoudi and Reza Azadeh", "abstract": "  In this paper, we present our work in progress towards creating a library of\nmotion primitives. This library facilitates easier and more intuitive learning\nand reusing of robotic skills. Users can teach robots complex skills through\nLearning from Demonstration, which is automatically segmented into primitives\nand stored in clusters of similar skills. We propose a novel multimodal\nsegmentation method as well as a novel trajectory clustering method. Then, when\nneeded for reuse, we transform primitives into new environments using\ntrajectory editing. We present simulated results for our framework with\ndemonstrations taken on real-world robots.\n", "link": "http://arxiv.org/abs/2404.18383v2", "date": "2024-06-28", "relevancy": 1.6656, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6102}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5748}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5254}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Framework%20for%20Learning%20and%20Reusing%20Robotic%20Skills&body=Title%3A%20A%20Framework%20for%20Learning%20and%20Reusing%20Robotic%20Skills%0AAuthor%3A%20Brendan%20Hertel%20and%20Nhu%20Tran%20and%20Meriem%20Elkoudi%20and%20Reza%20Azadeh%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20our%20work%20in%20progress%20towards%20creating%20a%20library%20of%0Amotion%20primitives.%20This%20library%20facilitates%20easier%20and%20more%20intuitive%20learning%0Aand%20reusing%20of%20robotic%20skills.%20Users%20can%20teach%20robots%20complex%20skills%20through%0ALearning%20from%20Demonstration%2C%20which%20is%20automatically%20segmented%20into%20primitives%0Aand%20stored%20in%20clusters%20of%20similar%20skills.%20We%20propose%20a%20novel%20multimodal%0Asegmentation%20method%20as%20well%20as%20a%20novel%20trajectory%20clustering%20method.%20Then%2C%20when%0Aneeded%20for%20reuse%2C%20we%20transform%20primitives%20into%20new%20environments%20using%0Atrajectory%20editing.%20We%20present%20simulated%20results%20for%20our%20framework%20with%0Ademonstrations%20taken%20on%20real-world%20robots.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18383v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Framework%2520for%2520Learning%2520and%2520Reusing%2520Robotic%2520Skills%26entry.906535625%3DBrendan%2520Hertel%2520and%2520Nhu%2520Tran%2520and%2520Meriem%2520Elkoudi%2520and%2520Reza%2520Azadeh%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520our%2520work%2520in%2520progress%2520towards%2520creating%2520a%2520library%2520of%250Amotion%2520primitives.%2520This%2520library%2520facilitates%2520easier%2520and%2520more%2520intuitive%2520learning%250Aand%2520reusing%2520of%2520robotic%2520skills.%2520Users%2520can%2520teach%2520robots%2520complex%2520skills%2520through%250ALearning%2520from%2520Demonstration%252C%2520which%2520is%2520automatically%2520segmented%2520into%2520primitives%250Aand%2520stored%2520in%2520clusters%2520of%2520similar%2520skills.%2520We%2520propose%2520a%2520novel%2520multimodal%250Asegmentation%2520method%2520as%2520well%2520as%2520a%2520novel%2520trajectory%2520clustering%2520method.%2520Then%252C%2520when%250Aneeded%2520for%2520reuse%252C%2520we%2520transform%2520primitives%2520into%2520new%2520environments%2520using%250Atrajectory%2520editing.%2520We%2520present%2520simulated%2520results%2520for%2520our%2520framework%2520with%250Ademonstrations%2520taken%2520on%2520real-world%2520robots.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.18383v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Framework%20for%20Learning%20and%20Reusing%20Robotic%20Skills&entry.906535625=Brendan%20Hertel%20and%20Nhu%20Tran%20and%20Meriem%20Elkoudi%20and%20Reza%20Azadeh&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20our%20work%20in%20progress%20towards%20creating%20a%20library%20of%0Amotion%20primitives.%20This%20library%20facilitates%20easier%20and%20more%20intuitive%20learning%0Aand%20reusing%20of%20robotic%20skills.%20Users%20can%20teach%20robots%20complex%20skills%20through%0ALearning%20from%20Demonstration%2C%20which%20is%20automatically%20segmented%20into%20primitives%0Aand%20stored%20in%20clusters%20of%20similar%20skills.%20We%20propose%20a%20novel%20multimodal%0Asegmentation%20method%20as%20well%20as%20a%20novel%20trajectory%20clustering%20method.%20Then%2C%20when%0Aneeded%20for%20reuse%2C%20we%20transform%20primitives%20into%20new%20environments%20using%0Atrajectory%20editing.%20We%20present%20simulated%20results%20for%20our%20framework%20with%0Ademonstrations%20taken%20on%20real-world%20robots.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18383v2&entry.124074799=Read"},
{"title": "Molecular Facts: Desiderata for Decontextualization in LLM Fact\n  Verification", "author": "Anisha Gunjal and Greg Durrett", "abstract": "  Automatic factuality verification of large language model (LLM) generations\nis becoming more and more widely used to combat hallucinations. A major point\nof tension in the literature is the granularity of this fact-checking: larger\nchunks of text are hard to fact-check, but more atomic facts like propositions\nmay lack context to interpret correctly. In this work, we assess the role of\ncontext in these atomic facts. We argue that fully atomic facts are not the\nright representation, and define two criteria for molecular facts:\ndecontextuality, or how well they can stand alone, and minimality, or how\nlittle extra information is added to achieve decontexuality. We quantify the\nimpact of decontextualization on minimality, then present a baseline\nmethodology for generating molecular facts automatically, aiming to add the\nright amount of information. We compare against various methods of\ndecontextualization and find that molecular facts balance minimality with fact\nverification accuracy in ambiguous settings.\n", "link": "http://arxiv.org/abs/2406.20079v1", "date": "2024-06-28", "relevancy": 1.2447, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4361}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4137}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.3968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Molecular%20Facts%3A%20Desiderata%20for%20Decontextualization%20in%20LLM%20Fact%0A%20%20Verification&body=Title%3A%20Molecular%20Facts%3A%20Desiderata%20for%20Decontextualization%20in%20LLM%20Fact%0A%20%20Verification%0AAuthor%3A%20Anisha%20Gunjal%20and%20Greg%20Durrett%0AAbstract%3A%20%20%20Automatic%20factuality%20verification%20of%20large%20language%20model%20%28LLM%29%20generations%0Ais%20becoming%20more%20and%20more%20widely%20used%20to%20combat%20hallucinations.%20A%20major%20point%0Aof%20tension%20in%20the%20literature%20is%20the%20granularity%20of%20this%20fact-checking%3A%20larger%0Achunks%20of%20text%20are%20hard%20to%20fact-check%2C%20but%20more%20atomic%20facts%20like%20propositions%0Amay%20lack%20context%20to%20interpret%20correctly.%20In%20this%20work%2C%20we%20assess%20the%20role%20of%0Acontext%20in%20these%20atomic%20facts.%20We%20argue%20that%20fully%20atomic%20facts%20are%20not%20the%0Aright%20representation%2C%20and%20define%20two%20criteria%20for%20molecular%20facts%3A%0Adecontextuality%2C%20or%20how%20well%20they%20can%20stand%20alone%2C%20and%20minimality%2C%20or%20how%0Alittle%20extra%20information%20is%20added%20to%20achieve%20decontexuality.%20We%20quantify%20the%0Aimpact%20of%20decontextualization%20on%20minimality%2C%20then%20present%20a%20baseline%0Amethodology%20for%20generating%20molecular%20facts%20automatically%2C%20aiming%20to%20add%20the%0Aright%20amount%20of%20information.%20We%20compare%20against%20various%20methods%20of%0Adecontextualization%20and%20find%20that%20molecular%20facts%20balance%20minimality%20with%20fact%0Averification%20accuracy%20in%20ambiguous%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.20079v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMolecular%2520Facts%253A%2520Desiderata%2520for%2520Decontextualization%2520in%2520LLM%2520Fact%250A%2520%2520Verification%26entry.906535625%3DAnisha%2520Gunjal%2520and%2520Greg%2520Durrett%26entry.1292438233%3D%2520%2520Automatic%2520factuality%2520verification%2520of%2520large%2520language%2520model%2520%2528LLM%2529%2520generations%250Ais%2520becoming%2520more%2520and%2520more%2520widely%2520used%2520to%2520combat%2520hallucinations.%2520A%2520major%2520point%250Aof%2520tension%2520in%2520the%2520literature%2520is%2520the%2520granularity%2520of%2520this%2520fact-checking%253A%2520larger%250Achunks%2520of%2520text%2520are%2520hard%2520to%2520fact-check%252C%2520but%2520more%2520atomic%2520facts%2520like%2520propositions%250Amay%2520lack%2520context%2520to%2520interpret%2520correctly.%2520In%2520this%2520work%252C%2520we%2520assess%2520the%2520role%2520of%250Acontext%2520in%2520these%2520atomic%2520facts.%2520We%2520argue%2520that%2520fully%2520atomic%2520facts%2520are%2520not%2520the%250Aright%2520representation%252C%2520and%2520define%2520two%2520criteria%2520for%2520molecular%2520facts%253A%250Adecontextuality%252C%2520or%2520how%2520well%2520they%2520can%2520stand%2520alone%252C%2520and%2520minimality%252C%2520or%2520how%250Alittle%2520extra%2520information%2520is%2520added%2520to%2520achieve%2520decontexuality.%2520We%2520quantify%2520the%250Aimpact%2520of%2520decontextualization%2520on%2520minimality%252C%2520then%2520present%2520a%2520baseline%250Amethodology%2520for%2520generating%2520molecular%2520facts%2520automatically%252C%2520aiming%2520to%2520add%2520the%250Aright%2520amount%2520of%2520information.%2520We%2520compare%2520against%2520various%2520methods%2520of%250Adecontextualization%2520and%2520find%2520that%2520molecular%2520facts%2520balance%2520minimality%2520with%2520fact%250Averification%2520accuracy%2520in%2520ambiguous%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.20079v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Molecular%20Facts%3A%20Desiderata%20for%20Decontextualization%20in%20LLM%20Fact%0A%20%20Verification&entry.906535625=Anisha%20Gunjal%20and%20Greg%20Durrett&entry.1292438233=%20%20Automatic%20factuality%20verification%20of%20large%20language%20model%20%28LLM%29%20generations%0Ais%20becoming%20more%20and%20more%20widely%20used%20to%20combat%20hallucinations.%20A%20major%20point%0Aof%20tension%20in%20the%20literature%20is%20the%20granularity%20of%20this%20fact-checking%3A%20larger%0Achunks%20of%20text%20are%20hard%20to%20fact-check%2C%20but%20more%20atomic%20facts%20like%20propositions%0Amay%20lack%20context%20to%20interpret%20correctly.%20In%20this%20work%2C%20we%20assess%20the%20role%20of%0Acontext%20in%20these%20atomic%20facts.%20We%20argue%20that%20fully%20atomic%20facts%20are%20not%20the%0Aright%20representation%2C%20and%20define%20two%20criteria%20for%20molecular%20facts%3A%0Adecontextuality%2C%20or%20how%20well%20they%20can%20stand%20alone%2C%20and%20minimality%2C%20or%20how%0Alittle%20extra%20information%20is%20added%20to%20achieve%20decontexuality.%20We%20quantify%20the%0Aimpact%20of%20decontextualization%20on%20minimality%2C%20then%20present%20a%20baseline%0Amethodology%20for%20generating%20molecular%20facts%20automatically%2C%20aiming%20to%20add%20the%0Aright%20amount%20of%20information.%20We%20compare%20against%20various%20methods%20of%0Adecontextualization%20and%20find%20that%20molecular%20facts%20balance%20minimality%20with%20fact%0Averification%20accuracy%20in%20ambiguous%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.20079v1&entry.124074799=Read"},
{"title": "Electrostatics-based particle sampling and approximate inference", "author": "Yongchao Huang", "abstract": "  A new particle-based sampling and approximate inference method, based on\nelectrostatics and Newton mechanics principles, is introduced with theoretical\nground, algorithm design and experimental validation. This method simulates an\ninteracting particle system (IPS) where particles, i.e. the freely-moving\nnegative charges and spatially-fixed positive charges with magnitudes\nproportional to the target distribution, interact with each other via\nattraction and repulsion induced by the resulting electric fields described by\nPoisson's equation. The IPS evolves towards a steady-state where the\ndistribution of negative charges conforms to the target distribution. This\nphysics-inspired method offers deterministic, gradient-free sampling and\ninference, achieving comparable performance as other particle-based and MCMC\nmethods in benchmark tasks of inferring complex densities, Bayesian logistic\nregression and dynamical system identification. A discrete-time, discrete-space\nalgorithmic design, readily extendable to continuous time and space, is\nprovided for usage in more general inference problems occurring in\nprobabilistic machine learning scenarios such as Bayesian inference, generative\nmodelling, and beyond.\n", "link": "http://arxiv.org/abs/2406.20044v1", "date": "2024-06-28", "relevancy": 1.365, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5106}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4459}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Electrostatics-based%20particle%20sampling%20and%20approximate%20inference&body=Title%3A%20Electrostatics-based%20particle%20sampling%20and%20approximate%20inference%0AAuthor%3A%20Yongchao%20Huang%0AAbstract%3A%20%20%20A%20new%20particle-based%20sampling%20and%20approximate%20inference%20method%2C%20based%20on%0Aelectrostatics%20and%20Newton%20mechanics%20principles%2C%20is%20introduced%20with%20theoretical%0Aground%2C%20algorithm%20design%20and%20experimental%20validation.%20This%20method%20simulates%20an%0Ainteracting%20particle%20system%20%28IPS%29%20where%20particles%2C%20i.e.%20the%20freely-moving%0Anegative%20charges%20and%20spatially-fixed%20positive%20charges%20with%20magnitudes%0Aproportional%20to%20the%20target%20distribution%2C%20interact%20with%20each%20other%20via%0Aattraction%20and%20repulsion%20induced%20by%20the%20resulting%20electric%20fields%20described%20by%0APoisson%27s%20equation.%20The%20IPS%20evolves%20towards%20a%20steady-state%20where%20the%0Adistribution%20of%20negative%20charges%20conforms%20to%20the%20target%20distribution.%20This%0Aphysics-inspired%20method%20offers%20deterministic%2C%20gradient-free%20sampling%20and%0Ainference%2C%20achieving%20comparable%20performance%20as%20other%20particle-based%20and%20MCMC%0Amethods%20in%20benchmark%20tasks%20of%20inferring%20complex%20densities%2C%20Bayesian%20logistic%0Aregression%20and%20dynamical%20system%20identification.%20A%20discrete-time%2C%20discrete-space%0Aalgorithmic%20design%2C%20readily%20extendable%20to%20continuous%20time%20and%20space%2C%20is%0Aprovided%20for%20usage%20in%20more%20general%20inference%20problems%20occurring%20in%0Aprobabilistic%20machine%20learning%20scenarios%20such%20as%20Bayesian%20inference%2C%20generative%0Amodelling%2C%20and%20beyond.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.20044v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DElectrostatics-based%2520particle%2520sampling%2520and%2520approximate%2520inference%26entry.906535625%3DYongchao%2520Huang%26entry.1292438233%3D%2520%2520A%2520new%2520particle-based%2520sampling%2520and%2520approximate%2520inference%2520method%252C%2520based%2520on%250Aelectrostatics%2520and%2520Newton%2520mechanics%2520principles%252C%2520is%2520introduced%2520with%2520theoretical%250Aground%252C%2520algorithm%2520design%2520and%2520experimental%2520validation.%2520This%2520method%2520simulates%2520an%250Ainteracting%2520particle%2520system%2520%2528IPS%2529%2520where%2520particles%252C%2520i.e.%2520the%2520freely-moving%250Anegative%2520charges%2520and%2520spatially-fixed%2520positive%2520charges%2520with%2520magnitudes%250Aproportional%2520to%2520the%2520target%2520distribution%252C%2520interact%2520with%2520each%2520other%2520via%250Aattraction%2520and%2520repulsion%2520induced%2520by%2520the%2520resulting%2520electric%2520fields%2520described%2520by%250APoisson%2527s%2520equation.%2520The%2520IPS%2520evolves%2520towards%2520a%2520steady-state%2520where%2520the%250Adistribution%2520of%2520negative%2520charges%2520conforms%2520to%2520the%2520target%2520distribution.%2520This%250Aphysics-inspired%2520method%2520offers%2520deterministic%252C%2520gradient-free%2520sampling%2520and%250Ainference%252C%2520achieving%2520comparable%2520performance%2520as%2520other%2520particle-based%2520and%2520MCMC%250Amethods%2520in%2520benchmark%2520tasks%2520of%2520inferring%2520complex%2520densities%252C%2520Bayesian%2520logistic%250Aregression%2520and%2520dynamical%2520system%2520identification.%2520A%2520discrete-time%252C%2520discrete-space%250Aalgorithmic%2520design%252C%2520readily%2520extendable%2520to%2520continuous%2520time%2520and%2520space%252C%2520is%250Aprovided%2520for%2520usage%2520in%2520more%2520general%2520inference%2520problems%2520occurring%2520in%250Aprobabilistic%2520machine%2520learning%2520scenarios%2520such%2520as%2520Bayesian%2520inference%252C%2520generative%250Amodelling%252C%2520and%2520beyond.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.20044v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Electrostatics-based%20particle%20sampling%20and%20approximate%20inference&entry.906535625=Yongchao%20Huang&entry.1292438233=%20%20A%20new%20particle-based%20sampling%20and%20approximate%20inference%20method%2C%20based%20on%0Aelectrostatics%20and%20Newton%20mechanics%20principles%2C%20is%20introduced%20with%20theoretical%0Aground%2C%20algorithm%20design%20and%20experimental%20validation.%20This%20method%20simulates%20an%0Ainteracting%20particle%20system%20%28IPS%29%20where%20particles%2C%20i.e.%20the%20freely-moving%0Anegative%20charges%20and%20spatially-fixed%20positive%20charges%20with%20magnitudes%0Aproportional%20to%20the%20target%20distribution%2C%20interact%20with%20each%20other%20via%0Aattraction%20and%20repulsion%20induced%20by%20the%20resulting%20electric%20fields%20described%20by%0APoisson%27s%20equation.%20The%20IPS%20evolves%20towards%20a%20steady-state%20where%20the%0Adistribution%20of%20negative%20charges%20conforms%20to%20the%20target%20distribution.%20This%0Aphysics-inspired%20method%20offers%20deterministic%2C%20gradient-free%20sampling%20and%0Ainference%2C%20achieving%20comparable%20performance%20as%20other%20particle-based%20and%20MCMC%0Amethods%20in%20benchmark%20tasks%20of%20inferring%20complex%20densities%2C%20Bayesian%20logistic%0Aregression%20and%20dynamical%20system%20identification.%20A%20discrete-time%2C%20discrete-space%0Aalgorithmic%20design%2C%20readily%20extendable%20to%20continuous%20time%20and%20space%2C%20is%0Aprovided%20for%20usage%20in%20more%20general%20inference%20problems%20occurring%20in%0Aprobabilistic%20machine%20learning%20scenarios%20such%20as%20Bayesian%20inference%2C%20generative%0Amodelling%2C%20and%20beyond.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.20044v1&entry.124074799=Read"},
{"title": "Comparative Analysis of LSTM Neural Networks and Traditional Machine\n  Learning Models for Predicting Diabetes Patient Readmission", "author": "Abolfazl Zarghani", "abstract": "  Diabetes mellitus is a chronic metabolic disorder that has emerged as one of\nthe major health problems worldwide due to its high prevalence and serious\ncomplications, which are pricey to manage. Effective management requires good\nglycemic control and regular follow-up in the clinic; however, non-adherence to\nscheduled follow-ups is very common. This study uses the Diabetes 130-US\nHospitals dataset for analysis and prediction of readmission patients by\nvarious traditional machine learning models, such as XGBoost, LightGBM,\nCatBoost, Decision Tree, and Random Forest, and also uses an in-house LSTM\nneural network for comparison. The quality of the data was assured by\npreprocessing it, and the performance evaluation for all these models was based\non accuracy, precision, recall, and F1-score. LightGBM turned out to be the\nbest traditional model, while XGBoost was the runner-up. The LSTM model\nsuffered from overfitting despite high training accuracy. A major strength of\nLSTM is capturing temporal dependencies among the patient data. Further, SHAP\nvalues were used, which improved model interpretability, whereby key factors\namong them number of lab procedures and discharge disposition were identified\nas critical in the prediction of readmissions. This study demonstrates that\nmodel selection, validation, and interpretability are key steps in predictive\nhealthcare modeling. This will help health providers design interventions for\nimproved follow-up adherence and better management of diabetes.\n", "link": "http://arxiv.org/abs/2406.19980v1", "date": "2024-06-28", "relevancy": 1.3106, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4495}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4212}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.421}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparative%20Analysis%20of%20LSTM%20Neural%20Networks%20and%20Traditional%20Machine%0A%20%20Learning%20Models%20for%20Predicting%20Diabetes%20Patient%20Readmission&body=Title%3A%20Comparative%20Analysis%20of%20LSTM%20Neural%20Networks%20and%20Traditional%20Machine%0A%20%20Learning%20Models%20for%20Predicting%20Diabetes%20Patient%20Readmission%0AAuthor%3A%20Abolfazl%20Zarghani%0AAbstract%3A%20%20%20Diabetes%20mellitus%20is%20a%20chronic%20metabolic%20disorder%20that%20has%20emerged%20as%20one%20of%0Athe%20major%20health%20problems%20worldwide%20due%20to%20its%20high%20prevalence%20and%20serious%0Acomplications%2C%20which%20are%20pricey%20to%20manage.%20Effective%20management%20requires%20good%0Aglycemic%20control%20and%20regular%20follow-up%20in%20the%20clinic%3B%20however%2C%20non-adherence%20to%0Ascheduled%20follow-ups%20is%20very%20common.%20This%20study%20uses%20the%20Diabetes%20130-US%0AHospitals%20dataset%20for%20analysis%20and%20prediction%20of%20readmission%20patients%20by%0Avarious%20traditional%20machine%20learning%20models%2C%20such%20as%20XGBoost%2C%20LightGBM%2C%0ACatBoost%2C%20Decision%20Tree%2C%20and%20Random%20Forest%2C%20and%20also%20uses%20an%20in-house%20LSTM%0Aneural%20network%20for%20comparison.%20The%20quality%20of%20the%20data%20was%20assured%20by%0Apreprocessing%20it%2C%20and%20the%20performance%20evaluation%20for%20all%20these%20models%20was%20based%0Aon%20accuracy%2C%20precision%2C%20recall%2C%20and%20F1-score.%20LightGBM%20turned%20out%20to%20be%20the%0Abest%20traditional%20model%2C%20while%20XGBoost%20was%20the%20runner-up.%20The%20LSTM%20model%0Asuffered%20from%20overfitting%20despite%20high%20training%20accuracy.%20A%20major%20strength%20of%0ALSTM%20is%20capturing%20temporal%20dependencies%20among%20the%20patient%20data.%20Further%2C%20SHAP%0Avalues%20were%20used%2C%20which%20improved%20model%20interpretability%2C%20whereby%20key%20factors%0Aamong%20them%20number%20of%20lab%20procedures%20and%20discharge%20disposition%20were%20identified%0Aas%20critical%20in%20the%20prediction%20of%20readmissions.%20This%20study%20demonstrates%20that%0Amodel%20selection%2C%20validation%2C%20and%20interpretability%20are%20key%20steps%20in%20predictive%0Ahealthcare%20modeling.%20This%20will%20help%20health%20providers%20design%20interventions%20for%0Aimproved%20follow-up%20adherence%20and%20better%20management%20of%20diabetes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19980v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparative%2520Analysis%2520of%2520LSTM%2520Neural%2520Networks%2520and%2520Traditional%2520Machine%250A%2520%2520Learning%2520Models%2520for%2520Predicting%2520Diabetes%2520Patient%2520Readmission%26entry.906535625%3DAbolfazl%2520Zarghani%26entry.1292438233%3D%2520%2520Diabetes%2520mellitus%2520is%2520a%2520chronic%2520metabolic%2520disorder%2520that%2520has%2520emerged%2520as%2520one%2520of%250Athe%2520major%2520health%2520problems%2520worldwide%2520due%2520to%2520its%2520high%2520prevalence%2520and%2520serious%250Acomplications%252C%2520which%2520are%2520pricey%2520to%2520manage.%2520Effective%2520management%2520requires%2520good%250Aglycemic%2520control%2520and%2520regular%2520follow-up%2520in%2520the%2520clinic%253B%2520however%252C%2520non-adherence%2520to%250Ascheduled%2520follow-ups%2520is%2520very%2520common.%2520This%2520study%2520uses%2520the%2520Diabetes%2520130-US%250AHospitals%2520dataset%2520for%2520analysis%2520and%2520prediction%2520of%2520readmission%2520patients%2520by%250Avarious%2520traditional%2520machine%2520learning%2520models%252C%2520such%2520as%2520XGBoost%252C%2520LightGBM%252C%250ACatBoost%252C%2520Decision%2520Tree%252C%2520and%2520Random%2520Forest%252C%2520and%2520also%2520uses%2520an%2520in-house%2520LSTM%250Aneural%2520network%2520for%2520comparison.%2520The%2520quality%2520of%2520the%2520data%2520was%2520assured%2520by%250Apreprocessing%2520it%252C%2520and%2520the%2520performance%2520evaluation%2520for%2520all%2520these%2520models%2520was%2520based%250Aon%2520accuracy%252C%2520precision%252C%2520recall%252C%2520and%2520F1-score.%2520LightGBM%2520turned%2520out%2520to%2520be%2520the%250Abest%2520traditional%2520model%252C%2520while%2520XGBoost%2520was%2520the%2520runner-up.%2520The%2520LSTM%2520model%250Asuffered%2520from%2520overfitting%2520despite%2520high%2520training%2520accuracy.%2520A%2520major%2520strength%2520of%250ALSTM%2520is%2520capturing%2520temporal%2520dependencies%2520among%2520the%2520patient%2520data.%2520Further%252C%2520SHAP%250Avalues%2520were%2520used%252C%2520which%2520improved%2520model%2520interpretability%252C%2520whereby%2520key%2520factors%250Aamong%2520them%2520number%2520of%2520lab%2520procedures%2520and%2520discharge%2520disposition%2520were%2520identified%250Aas%2520critical%2520in%2520the%2520prediction%2520of%2520readmissions.%2520This%2520study%2520demonstrates%2520that%250Amodel%2520selection%252C%2520validation%252C%2520and%2520interpretability%2520are%2520key%2520steps%2520in%2520predictive%250Ahealthcare%2520modeling.%2520This%2520will%2520help%2520health%2520providers%2520design%2520interventions%2520for%250Aimproved%2520follow-up%2520adherence%2520and%2520better%2520management%2520of%2520diabetes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19980v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparative%20Analysis%20of%20LSTM%20Neural%20Networks%20and%20Traditional%20Machine%0A%20%20Learning%20Models%20for%20Predicting%20Diabetes%20Patient%20Readmission&entry.906535625=Abolfazl%20Zarghani&entry.1292438233=%20%20Diabetes%20mellitus%20is%20a%20chronic%20metabolic%20disorder%20that%20has%20emerged%20as%20one%20of%0Athe%20major%20health%20problems%20worldwide%20due%20to%20its%20high%20prevalence%20and%20serious%0Acomplications%2C%20which%20are%20pricey%20to%20manage.%20Effective%20management%20requires%20good%0Aglycemic%20control%20and%20regular%20follow-up%20in%20the%20clinic%3B%20however%2C%20non-adherence%20to%0Ascheduled%20follow-ups%20is%20very%20common.%20This%20study%20uses%20the%20Diabetes%20130-US%0AHospitals%20dataset%20for%20analysis%20and%20prediction%20of%20readmission%20patients%20by%0Avarious%20traditional%20machine%20learning%20models%2C%20such%20as%20XGBoost%2C%20LightGBM%2C%0ACatBoost%2C%20Decision%20Tree%2C%20and%20Random%20Forest%2C%20and%20also%20uses%20an%20in-house%20LSTM%0Aneural%20network%20for%20comparison.%20The%20quality%20of%20the%20data%20was%20assured%20by%0Apreprocessing%20it%2C%20and%20the%20performance%20evaluation%20for%20all%20these%20models%20was%20based%0Aon%20accuracy%2C%20precision%2C%20recall%2C%20and%20F1-score.%20LightGBM%20turned%20out%20to%20be%20the%0Abest%20traditional%20model%2C%20while%20XGBoost%20was%20the%20runner-up.%20The%20LSTM%20model%0Asuffered%20from%20overfitting%20despite%20high%20training%20accuracy.%20A%20major%20strength%20of%0ALSTM%20is%20capturing%20temporal%20dependencies%20among%20the%20patient%20data.%20Further%2C%20SHAP%0Avalues%20were%20used%2C%20which%20improved%20model%20interpretability%2C%20whereby%20key%20factors%0Aamong%20them%20number%20of%20lab%20procedures%20and%20discharge%20disposition%20were%20identified%0Aas%20critical%20in%20the%20prediction%20of%20readmissions.%20This%20study%20demonstrates%20that%0Amodel%20selection%2C%20validation%2C%20and%20interpretability%20are%20key%20steps%20in%20predictive%0Ahealthcare%20modeling.%20This%20will%20help%20health%20providers%20design%20interventions%20for%0Aimproved%20follow-up%20adherence%20and%20better%20management%20of%20diabetes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19980v1&entry.124074799=Read"},
{"title": "BMW Agents -- A Framework For Task Automation Through Multi-agent\n  Collaboration", "author": "Noel Crawford and Edward B. Duffy and Iman Evazzade and Torsten Foehr and Gregory Robbins and Debbrata Kumar Saha and Jiya Varma and Marcin Ziolkowski", "abstract": "  Autonomous agents driven by Large Language Models (LLMs) offer enormous\npotential for automation. Early proof of this technology can be found in\nvarious demonstrations of agents solving complex tasks, interacting with\nexternal systems to augment their knowledge, and triggering actions. In\nparticular, workflows involving multiple agents solving complex tasks in a\ncollaborative fashion exemplify their capacity to operate in less strict and\nless well-defined environments. Thus, a multi-agent approach has great\npotential for serving as a backbone in many industrial applications, ranging\nfrom complex knowledge retrieval systems to next generation robotic process\nautomation. Given the reasoning abilities within the current generation of\nLLMs, complex processes require a multi-step approach that includes a plan of\nwell-defined and modular tasks. Depending on the level of complexity, these\ntasks can be executed either by a single agent or a group of agents. In this\nwork, we focus on designing a flexible agent engineering framework with careful\nattention to planning and execution, capable of handling complex use case\napplications across various domains. The proposed framework provides\nreliability in industrial applications and presents techniques to ensure a\nscalable, flexible, and collaborative workflow for multiple autonomous agents\nworking together towards solving tasks.\n", "link": "http://arxiv.org/abs/2406.20041v1", "date": "2024-06-28", "relevancy": 1.5452, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5413}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5116}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4974}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BMW%20Agents%20--%20A%20Framework%20For%20Task%20Automation%20Through%20Multi-agent%0A%20%20Collaboration&body=Title%3A%20BMW%20Agents%20--%20A%20Framework%20For%20Task%20Automation%20Through%20Multi-agent%0A%20%20Collaboration%0AAuthor%3A%20Noel%20Crawford%20and%20Edward%20B.%20Duffy%20and%20Iman%20Evazzade%20and%20Torsten%20Foehr%20and%20Gregory%20Robbins%20and%20Debbrata%20Kumar%20Saha%20and%20Jiya%20Varma%20and%20Marcin%20Ziolkowski%0AAbstract%3A%20%20%20Autonomous%20agents%20driven%20by%20Large%20Language%20Models%20%28LLMs%29%20offer%20enormous%0Apotential%20for%20automation.%20Early%20proof%20of%20this%20technology%20can%20be%20found%20in%0Avarious%20demonstrations%20of%20agents%20solving%20complex%20tasks%2C%20interacting%20with%0Aexternal%20systems%20to%20augment%20their%20knowledge%2C%20and%20triggering%20actions.%20In%0Aparticular%2C%20workflows%20involving%20multiple%20agents%20solving%20complex%20tasks%20in%20a%0Acollaborative%20fashion%20exemplify%20their%20capacity%20to%20operate%20in%20less%20strict%20and%0Aless%20well-defined%20environments.%20Thus%2C%20a%20multi-agent%20approach%20has%20great%0Apotential%20for%20serving%20as%20a%20backbone%20in%20many%20industrial%20applications%2C%20ranging%0Afrom%20complex%20knowledge%20retrieval%20systems%20to%20next%20generation%20robotic%20process%0Aautomation.%20Given%20the%20reasoning%20abilities%20within%20the%20current%20generation%20of%0ALLMs%2C%20complex%20processes%20require%20a%20multi-step%20approach%20that%20includes%20a%20plan%20of%0Awell-defined%20and%20modular%20tasks.%20Depending%20on%20the%20level%20of%20complexity%2C%20these%0Atasks%20can%20be%20executed%20either%20by%20a%20single%20agent%20or%20a%20group%20of%20agents.%20In%20this%0Awork%2C%20we%20focus%20on%20designing%20a%20flexible%20agent%20engineering%20framework%20with%20careful%0Aattention%20to%20planning%20and%20execution%2C%20capable%20of%20handling%20complex%20use%20case%0Aapplications%20across%20various%20domains.%20The%20proposed%20framework%20provides%0Areliability%20in%20industrial%20applications%20and%20presents%20techniques%20to%20ensure%20a%0Ascalable%2C%20flexible%2C%20and%20collaborative%20workflow%20for%20multiple%20autonomous%20agents%0Aworking%20together%20towards%20solving%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.20041v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBMW%2520Agents%2520--%2520A%2520Framework%2520For%2520Task%2520Automation%2520Through%2520Multi-agent%250A%2520%2520Collaboration%26entry.906535625%3DNoel%2520Crawford%2520and%2520Edward%2520B.%2520Duffy%2520and%2520Iman%2520Evazzade%2520and%2520Torsten%2520Foehr%2520and%2520Gregory%2520Robbins%2520and%2520Debbrata%2520Kumar%2520Saha%2520and%2520Jiya%2520Varma%2520and%2520Marcin%2520Ziolkowski%26entry.1292438233%3D%2520%2520Autonomous%2520agents%2520driven%2520by%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520offer%2520enormous%250Apotential%2520for%2520automation.%2520Early%2520proof%2520of%2520this%2520technology%2520can%2520be%2520found%2520in%250Avarious%2520demonstrations%2520of%2520agents%2520solving%2520complex%2520tasks%252C%2520interacting%2520with%250Aexternal%2520systems%2520to%2520augment%2520their%2520knowledge%252C%2520and%2520triggering%2520actions.%2520In%250Aparticular%252C%2520workflows%2520involving%2520multiple%2520agents%2520solving%2520complex%2520tasks%2520in%2520a%250Acollaborative%2520fashion%2520exemplify%2520their%2520capacity%2520to%2520operate%2520in%2520less%2520strict%2520and%250Aless%2520well-defined%2520environments.%2520Thus%252C%2520a%2520multi-agent%2520approach%2520has%2520great%250Apotential%2520for%2520serving%2520as%2520a%2520backbone%2520in%2520many%2520industrial%2520applications%252C%2520ranging%250Afrom%2520complex%2520knowledge%2520retrieval%2520systems%2520to%2520next%2520generation%2520robotic%2520process%250Aautomation.%2520Given%2520the%2520reasoning%2520abilities%2520within%2520the%2520current%2520generation%2520of%250ALLMs%252C%2520complex%2520processes%2520require%2520a%2520multi-step%2520approach%2520that%2520includes%2520a%2520plan%2520of%250Awell-defined%2520and%2520modular%2520tasks.%2520Depending%2520on%2520the%2520level%2520of%2520complexity%252C%2520these%250Atasks%2520can%2520be%2520executed%2520either%2520by%2520a%2520single%2520agent%2520or%2520a%2520group%2520of%2520agents.%2520In%2520this%250Awork%252C%2520we%2520focus%2520on%2520designing%2520a%2520flexible%2520agent%2520engineering%2520framework%2520with%2520careful%250Aattention%2520to%2520planning%2520and%2520execution%252C%2520capable%2520of%2520handling%2520complex%2520use%2520case%250Aapplications%2520across%2520various%2520domains.%2520The%2520proposed%2520framework%2520provides%250Areliability%2520in%2520industrial%2520applications%2520and%2520presents%2520techniques%2520to%2520ensure%2520a%250Ascalable%252C%2520flexible%252C%2520and%2520collaborative%2520workflow%2520for%2520multiple%2520autonomous%2520agents%250Aworking%2520together%2520towards%2520solving%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.20041v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BMW%20Agents%20--%20A%20Framework%20For%20Task%20Automation%20Through%20Multi-agent%0A%20%20Collaboration&entry.906535625=Noel%20Crawford%20and%20Edward%20B.%20Duffy%20and%20Iman%20Evazzade%20and%20Torsten%20Foehr%20and%20Gregory%20Robbins%20and%20Debbrata%20Kumar%20Saha%20and%20Jiya%20Varma%20and%20Marcin%20Ziolkowski&entry.1292438233=%20%20Autonomous%20agents%20driven%20by%20Large%20Language%20Models%20%28LLMs%29%20offer%20enormous%0Apotential%20for%20automation.%20Early%20proof%20of%20this%20technology%20can%20be%20found%20in%0Avarious%20demonstrations%20of%20agents%20solving%20complex%20tasks%2C%20interacting%20with%0Aexternal%20systems%20to%20augment%20their%20knowledge%2C%20and%20triggering%20actions.%20In%0Aparticular%2C%20workflows%20involving%20multiple%20agents%20solving%20complex%20tasks%20in%20a%0Acollaborative%20fashion%20exemplify%20their%20capacity%20to%20operate%20in%20less%20strict%20and%0Aless%20well-defined%20environments.%20Thus%2C%20a%20multi-agent%20approach%20has%20great%0Apotential%20for%20serving%20as%20a%20backbone%20in%20many%20industrial%20applications%2C%20ranging%0Afrom%20complex%20knowledge%20retrieval%20systems%20to%20next%20generation%20robotic%20process%0Aautomation.%20Given%20the%20reasoning%20abilities%20within%20the%20current%20generation%20of%0ALLMs%2C%20complex%20processes%20require%20a%20multi-step%20approach%20that%20includes%20a%20plan%20of%0Awell-defined%20and%20modular%20tasks.%20Depending%20on%20the%20level%20of%20complexity%2C%20these%0Atasks%20can%20be%20executed%20either%20by%20a%20single%20agent%20or%20a%20group%20of%20agents.%20In%20this%0Awork%2C%20we%20focus%20on%20designing%20a%20flexible%20agent%20engineering%20framework%20with%20careful%0Aattention%20to%20planning%20and%20execution%2C%20capable%20of%20handling%20complex%20use%20case%0Aapplications%20across%20various%20domains.%20The%20proposed%20framework%20provides%0Areliability%20in%20industrial%20applications%20and%20presents%20techniques%20to%20ensure%20a%0Ascalable%2C%20flexible%2C%20and%20collaborative%20workflow%20for%20multiple%20autonomous%20agents%0Aworking%20together%20towards%20solving%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.20041v1&entry.124074799=Read"},
{"title": "GM-DF: Generalized Multi-Scenario Deepfake Detection", "author": "Yingxin Lai and Zitong Yu and Jing Yang and Bin Li and Xiangui Kang and Linlin Shen", "abstract": "  Existing face forgery detection usually follows the paradigm of training\nmodels in a single domain, which leads to limited generalization capacity when\nunseen scenarios and unknown attacks occur. In this paper, we elaborately\ninvestigate the generalization capacity of deepfake detection models when\njointly trained on multiple face forgery detection datasets. We first find a\nrapid degradation of detection accuracy when models are directly trained on\ncombined datasets due to the discrepancy across collection scenarios and\ngeneration methods. To address the above issue, a Generalized Multi-Scenario\nDeepfake Detection framework (GM-DF) is proposed to serve multiple real-world\nscenarios by a unified model. First, we propose a hybrid expert modeling\napproach for domain-specific real/forgery feature extraction. Besides, as for\nthe commonality representation, we use CLIP to extract the common features for\nbetter aligning visual and textual features across domains. Meanwhile, we\nintroduce a masked image reconstruction mechanism to force models to capture\nrich forged details. Finally, we supervise the models via a domain-aware\nmeta-learning strategy to further enhance their generalization capacities.\nSpecifically, we design a novel domain alignment loss to strongly align the\ndistributions of the meta-test domains and meta-train domains. Thus, the\nupdated models are able to represent both specific and common real/forgery\nfeatures across multiple datasets. In consideration of the lack of study of\nmulti-dataset training, we establish a new benchmark leveraging multi-source\ndata to fairly evaluate the models' generalization capacity on unseen\nscenarios. Both qualitative and quantitative experiments on five datasets\nconducted on traditional protocols as well as the proposed benchmark\ndemonstrate the effectiveness of our approach.\n", "link": "http://arxiv.org/abs/2406.20078v1", "date": "2024-06-28", "relevancy": 1.508, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.51}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5009}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GM-DF%3A%20Generalized%20Multi-Scenario%20Deepfake%20Detection&body=Title%3A%20GM-DF%3A%20Generalized%20Multi-Scenario%20Deepfake%20Detection%0AAuthor%3A%20Yingxin%20Lai%20and%20Zitong%20Yu%20and%20Jing%20Yang%20and%20Bin%20Li%20and%20Xiangui%20Kang%20and%20Linlin%20Shen%0AAbstract%3A%20%20%20Existing%20face%20forgery%20detection%20usually%20follows%20the%20paradigm%20of%20training%0Amodels%20in%20a%20single%20domain%2C%20which%20leads%20to%20limited%20generalization%20capacity%20when%0Aunseen%20scenarios%20and%20unknown%20attacks%20occur.%20In%20this%20paper%2C%20we%20elaborately%0Ainvestigate%20the%20generalization%20capacity%20of%20deepfake%20detection%20models%20when%0Ajointly%20trained%20on%20multiple%20face%20forgery%20detection%20datasets.%20We%20first%20find%20a%0Arapid%20degradation%20of%20detection%20accuracy%20when%20models%20are%20directly%20trained%20on%0Acombined%20datasets%20due%20to%20the%20discrepancy%20across%20collection%20scenarios%20and%0Ageneration%20methods.%20To%20address%20the%20above%20issue%2C%20a%20Generalized%20Multi-Scenario%0ADeepfake%20Detection%20framework%20%28GM-DF%29%20is%20proposed%20to%20serve%20multiple%20real-world%0Ascenarios%20by%20a%20unified%20model.%20First%2C%20we%20propose%20a%20hybrid%20expert%20modeling%0Aapproach%20for%20domain-specific%20real/forgery%20feature%20extraction.%20Besides%2C%20as%20for%0Athe%20commonality%20representation%2C%20we%20use%20CLIP%20to%20extract%20the%20common%20features%20for%0Abetter%20aligning%20visual%20and%20textual%20features%20across%20domains.%20Meanwhile%2C%20we%0Aintroduce%20a%20masked%20image%20reconstruction%20mechanism%20to%20force%20models%20to%20capture%0Arich%20forged%20details.%20Finally%2C%20we%20supervise%20the%20models%20via%20a%20domain-aware%0Ameta-learning%20strategy%20to%20further%20enhance%20their%20generalization%20capacities.%0ASpecifically%2C%20we%20design%20a%20novel%20domain%20alignment%20loss%20to%20strongly%20align%20the%0Adistributions%20of%20the%20meta-test%20domains%20and%20meta-train%20domains.%20Thus%2C%20the%0Aupdated%20models%20are%20able%20to%20represent%20both%20specific%20and%20common%20real/forgery%0Afeatures%20across%20multiple%20datasets.%20In%20consideration%20of%20the%20lack%20of%20study%20of%0Amulti-dataset%20training%2C%20we%20establish%20a%20new%20benchmark%20leveraging%20multi-source%0Adata%20to%20fairly%20evaluate%20the%20models%27%20generalization%20capacity%20on%20unseen%0Ascenarios.%20Both%20qualitative%20and%20quantitative%20experiments%20on%20five%20datasets%0Aconducted%20on%20traditional%20protocols%20as%20well%20as%20the%20proposed%20benchmark%0Ademonstrate%20the%20effectiveness%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.20078v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGM-DF%253A%2520Generalized%2520Multi-Scenario%2520Deepfake%2520Detection%26entry.906535625%3DYingxin%2520Lai%2520and%2520Zitong%2520Yu%2520and%2520Jing%2520Yang%2520and%2520Bin%2520Li%2520and%2520Xiangui%2520Kang%2520and%2520Linlin%2520Shen%26entry.1292438233%3D%2520%2520Existing%2520face%2520forgery%2520detection%2520usually%2520follows%2520the%2520paradigm%2520of%2520training%250Amodels%2520in%2520a%2520single%2520domain%252C%2520which%2520leads%2520to%2520limited%2520generalization%2520capacity%2520when%250Aunseen%2520scenarios%2520and%2520unknown%2520attacks%2520occur.%2520In%2520this%2520paper%252C%2520we%2520elaborately%250Ainvestigate%2520the%2520generalization%2520capacity%2520of%2520deepfake%2520detection%2520models%2520when%250Ajointly%2520trained%2520on%2520multiple%2520face%2520forgery%2520detection%2520datasets.%2520We%2520first%2520find%2520a%250Arapid%2520degradation%2520of%2520detection%2520accuracy%2520when%2520models%2520are%2520directly%2520trained%2520on%250Acombined%2520datasets%2520due%2520to%2520the%2520discrepancy%2520across%2520collection%2520scenarios%2520and%250Ageneration%2520methods.%2520To%2520address%2520the%2520above%2520issue%252C%2520a%2520Generalized%2520Multi-Scenario%250ADeepfake%2520Detection%2520framework%2520%2528GM-DF%2529%2520is%2520proposed%2520to%2520serve%2520multiple%2520real-world%250Ascenarios%2520by%2520a%2520unified%2520model.%2520First%252C%2520we%2520propose%2520a%2520hybrid%2520expert%2520modeling%250Aapproach%2520for%2520domain-specific%2520real/forgery%2520feature%2520extraction.%2520Besides%252C%2520as%2520for%250Athe%2520commonality%2520representation%252C%2520we%2520use%2520CLIP%2520to%2520extract%2520the%2520common%2520features%2520for%250Abetter%2520aligning%2520visual%2520and%2520textual%2520features%2520across%2520domains.%2520Meanwhile%252C%2520we%250Aintroduce%2520a%2520masked%2520image%2520reconstruction%2520mechanism%2520to%2520force%2520models%2520to%2520capture%250Arich%2520forged%2520details.%2520Finally%252C%2520we%2520supervise%2520the%2520models%2520via%2520a%2520domain-aware%250Ameta-learning%2520strategy%2520to%2520further%2520enhance%2520their%2520generalization%2520capacities.%250ASpecifically%252C%2520we%2520design%2520a%2520novel%2520domain%2520alignment%2520loss%2520to%2520strongly%2520align%2520the%250Adistributions%2520of%2520the%2520meta-test%2520domains%2520and%2520meta-train%2520domains.%2520Thus%252C%2520the%250Aupdated%2520models%2520are%2520able%2520to%2520represent%2520both%2520specific%2520and%2520common%2520real/forgery%250Afeatures%2520across%2520multiple%2520datasets.%2520In%2520consideration%2520of%2520the%2520lack%2520of%2520study%2520of%250Amulti-dataset%2520training%252C%2520we%2520establish%2520a%2520new%2520benchmark%2520leveraging%2520multi-source%250Adata%2520to%2520fairly%2520evaluate%2520the%2520models%2527%2520generalization%2520capacity%2520on%2520unseen%250Ascenarios.%2520Both%2520qualitative%2520and%2520quantitative%2520experiments%2520on%2520five%2520datasets%250Aconducted%2520on%2520traditional%2520protocols%2520as%2520well%2520as%2520the%2520proposed%2520benchmark%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.20078v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GM-DF%3A%20Generalized%20Multi-Scenario%20Deepfake%20Detection&entry.906535625=Yingxin%20Lai%20and%20Zitong%20Yu%20and%20Jing%20Yang%20and%20Bin%20Li%20and%20Xiangui%20Kang%20and%20Linlin%20Shen&entry.1292438233=%20%20Existing%20face%20forgery%20detection%20usually%20follows%20the%20paradigm%20of%20training%0Amodels%20in%20a%20single%20domain%2C%20which%20leads%20to%20limited%20generalization%20capacity%20when%0Aunseen%20scenarios%20and%20unknown%20attacks%20occur.%20In%20this%20paper%2C%20we%20elaborately%0Ainvestigate%20the%20generalization%20capacity%20of%20deepfake%20detection%20models%20when%0Ajointly%20trained%20on%20multiple%20face%20forgery%20detection%20datasets.%20We%20first%20find%20a%0Arapid%20degradation%20of%20detection%20accuracy%20when%20models%20are%20directly%20trained%20on%0Acombined%20datasets%20due%20to%20the%20discrepancy%20across%20collection%20scenarios%20and%0Ageneration%20methods.%20To%20address%20the%20above%20issue%2C%20a%20Generalized%20Multi-Scenario%0ADeepfake%20Detection%20framework%20%28GM-DF%29%20is%20proposed%20to%20serve%20multiple%20real-world%0Ascenarios%20by%20a%20unified%20model.%20First%2C%20we%20propose%20a%20hybrid%20expert%20modeling%0Aapproach%20for%20domain-specific%20real/forgery%20feature%20extraction.%20Besides%2C%20as%20for%0Athe%20commonality%20representation%2C%20we%20use%20CLIP%20to%20extract%20the%20common%20features%20for%0Abetter%20aligning%20visual%20and%20textual%20features%20across%20domains.%20Meanwhile%2C%20we%0Aintroduce%20a%20masked%20image%20reconstruction%20mechanism%20to%20force%20models%20to%20capture%0Arich%20forged%20details.%20Finally%2C%20we%20supervise%20the%20models%20via%20a%20domain-aware%0Ameta-learning%20strategy%20to%20further%20enhance%20their%20generalization%20capacities.%0ASpecifically%2C%20we%20design%20a%20novel%20domain%20alignment%20loss%20to%20strongly%20align%20the%0Adistributions%20of%20the%20meta-test%20domains%20and%20meta-train%20domains.%20Thus%2C%20the%0Aupdated%20models%20are%20able%20to%20represent%20both%20specific%20and%20common%20real/forgery%0Afeatures%20across%20multiple%20datasets.%20In%20consideration%20of%20the%20lack%20of%20study%20of%0Amulti-dataset%20training%2C%20we%20establish%20a%20new%20benchmark%20leveraging%20multi-source%0Adata%20to%20fairly%20evaluate%20the%20models%27%20generalization%20capacity%20on%20unseen%0Ascenarios.%20Both%20qualitative%20and%20quantitative%20experiments%20on%20five%20datasets%0Aconducted%20on%20traditional%20protocols%20as%20well%20as%20the%20proposed%20benchmark%0Ademonstrate%20the%20effectiveness%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.20078v1&entry.124074799=Read"},
{"title": "Reinforcement Learning for Efficient Design and Control Co-optimisation\n  of Energy Systems", "author": "Marine Cauz and Adrien Bolland and Nicolas Wyrsch and Christophe Ballif", "abstract": "  The ongoing energy transition drives the development of decentralised\nrenewable energy sources, which are heterogeneous and weather-dependent,\ncomplicating their integration into energy systems. This study tackles this\nissue by introducing a novel reinforcement learning (RL) framework tailored for\nthe co-optimisation of design and control in energy systems. Traditionally, the\nintegration of renewable sources in the energy sector has relied on complex\nmathematical modelling and sequential processes. By leveraging RL's model-free\ncapabilities, the framework eliminates the need for explicit system modelling.\nBy optimising both control and design policies jointly, the framework enhances\nthe integration of renewable sources and improves system efficiency. This\ncontribution paves the way for advanced RL applications in energy management,\nleading to more efficient and effective use of renewable energy sources.\n", "link": "http://arxiv.org/abs/2406.19825v1", "date": "2024-06-28", "relevancy": 1.4083, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4852}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4499}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcement%20Learning%20for%20Efficient%20Design%20and%20Control%20Co-optimisation%0A%20%20of%20Energy%20Systems&body=Title%3A%20Reinforcement%20Learning%20for%20Efficient%20Design%20and%20Control%20Co-optimisation%0A%20%20of%20Energy%20Systems%0AAuthor%3A%20Marine%20Cauz%20and%20Adrien%20Bolland%20and%20Nicolas%20Wyrsch%20and%20Christophe%20Ballif%0AAbstract%3A%20%20%20The%20ongoing%20energy%20transition%20drives%20the%20development%20of%20decentralised%0Arenewable%20energy%20sources%2C%20which%20are%20heterogeneous%20and%20weather-dependent%2C%0Acomplicating%20their%20integration%20into%20energy%20systems.%20This%20study%20tackles%20this%0Aissue%20by%20introducing%20a%20novel%20reinforcement%20learning%20%28RL%29%20framework%20tailored%20for%0Athe%20co-optimisation%20of%20design%20and%20control%20in%20energy%20systems.%20Traditionally%2C%20the%0Aintegration%20of%20renewable%20sources%20in%20the%20energy%20sector%20has%20relied%20on%20complex%0Amathematical%20modelling%20and%20sequential%20processes.%20By%20leveraging%20RL%27s%20model-free%0Acapabilities%2C%20the%20framework%20eliminates%20the%20need%20for%20explicit%20system%20modelling.%0ABy%20optimising%20both%20control%20and%20design%20policies%20jointly%2C%20the%20framework%20enhances%0Athe%20integration%20of%20renewable%20sources%20and%20improves%20system%20efficiency.%20This%0Acontribution%20paves%20the%20way%20for%20advanced%20RL%20applications%20in%20energy%20management%2C%0Aleading%20to%20more%20efficient%20and%20effective%20use%20of%20renewable%20energy%20sources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19825v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcement%2520Learning%2520for%2520Efficient%2520Design%2520and%2520Control%2520Co-optimisation%250A%2520%2520of%2520Energy%2520Systems%26entry.906535625%3DMarine%2520Cauz%2520and%2520Adrien%2520Bolland%2520and%2520Nicolas%2520Wyrsch%2520and%2520Christophe%2520Ballif%26entry.1292438233%3D%2520%2520The%2520ongoing%2520energy%2520transition%2520drives%2520the%2520development%2520of%2520decentralised%250Arenewable%2520energy%2520sources%252C%2520which%2520are%2520heterogeneous%2520and%2520weather-dependent%252C%250Acomplicating%2520their%2520integration%2520into%2520energy%2520systems.%2520This%2520study%2520tackles%2520this%250Aissue%2520by%2520introducing%2520a%2520novel%2520reinforcement%2520learning%2520%2528RL%2529%2520framework%2520tailored%2520for%250Athe%2520co-optimisation%2520of%2520design%2520and%2520control%2520in%2520energy%2520systems.%2520Traditionally%252C%2520the%250Aintegration%2520of%2520renewable%2520sources%2520in%2520the%2520energy%2520sector%2520has%2520relied%2520on%2520complex%250Amathematical%2520modelling%2520and%2520sequential%2520processes.%2520By%2520leveraging%2520RL%2527s%2520model-free%250Acapabilities%252C%2520the%2520framework%2520eliminates%2520the%2520need%2520for%2520explicit%2520system%2520modelling.%250ABy%2520optimising%2520both%2520control%2520and%2520design%2520policies%2520jointly%252C%2520the%2520framework%2520enhances%250Athe%2520integration%2520of%2520renewable%2520sources%2520and%2520improves%2520system%2520efficiency.%2520This%250Acontribution%2520paves%2520the%2520way%2520for%2520advanced%2520RL%2520applications%2520in%2520energy%2520management%252C%250Aleading%2520to%2520more%2520efficient%2520and%2520effective%2520use%2520of%2520renewable%2520energy%2520sources.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19825v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcement%20Learning%20for%20Efficient%20Design%20and%20Control%20Co-optimisation%0A%20%20of%20Energy%20Systems&entry.906535625=Marine%20Cauz%20and%20Adrien%20Bolland%20and%20Nicolas%20Wyrsch%20and%20Christophe%20Ballif&entry.1292438233=%20%20The%20ongoing%20energy%20transition%20drives%20the%20development%20of%20decentralised%0Arenewable%20energy%20sources%2C%20which%20are%20heterogeneous%20and%20weather-dependent%2C%0Acomplicating%20their%20integration%20into%20energy%20systems.%20This%20study%20tackles%20this%0Aissue%20by%20introducing%20a%20novel%20reinforcement%20learning%20%28RL%29%20framework%20tailored%20for%0Athe%20co-optimisation%20of%20design%20and%20control%20in%20energy%20systems.%20Traditionally%2C%20the%0Aintegration%20of%20renewable%20sources%20in%20the%20energy%20sector%20has%20relied%20on%20complex%0Amathematical%20modelling%20and%20sequential%20processes.%20By%20leveraging%20RL%27s%20model-free%0Acapabilities%2C%20the%20framework%20eliminates%20the%20need%20for%20explicit%20system%20modelling.%0ABy%20optimising%20both%20control%20and%20design%20policies%20jointly%2C%20the%20framework%20enhances%0Athe%20integration%20of%20renewable%20sources%20and%20improves%20system%20efficiency.%20This%0Acontribution%20paves%20the%20way%20for%20advanced%20RL%20applications%20in%20energy%20management%2C%0Aleading%20to%20more%20efficient%20and%20effective%20use%20of%20renewable%20energy%20sources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19825v1&entry.124074799=Read"},
{"title": "From the Least to the Most: Building a Plug-and-Play Visual Reasoner via\n  Data Synthesis", "author": "Chuanqi Cheng and Jian Guan and Wei Wu and Rui Yan", "abstract": "  We explore multi-step reasoning in vision-language models (VLMs). The problem\nis challenging, as reasoning data consisting of multiple steps of visual and\nlanguage processing are barely available. To overcome the challenge, we first\nintroduce a least-to-most visual reasoning paradigm, which interleaves steps of\ndecomposing a question into sub-questions and invoking external tools for\nresolving sub-questions. Based on the paradigm, we further propose a novel data\nsynthesis approach that can automatically create questions and multi-step\nreasoning paths for an image in a bottom-up manner. Our approach divides the\ncomplex synthesis task into a few simple sub-tasks, and (almost entirely)\nrelies on open-sourced models to accomplish the sub-tasks. Therefore, the\nentire synthesis process is reproducible and cost-efficient, and the\nsynthesized data is quality guaranteed. With the approach, we construct $50$k\nvisual reasoning examples. Then, we develop a visual reasoner through\nsupervised fine-tuning, which is capable of generally enhancing the reasoning\nabilities of a wide range of existing VLMs in a plug-and-play fashion.\nExtensive experiments indicate that the visual reasoner can consistently and\nsignificantly improve four VLMs on four VQA benchmarks. Our code and dataset\nare available at https://github.com/steven-ccq/VisualReasoner.\n", "link": "http://arxiv.org/abs/2406.19934v1", "date": "2024-06-28", "relevancy": 1.5596, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.538}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5342}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5069}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20the%20Least%20to%20the%20Most%3A%20Building%20a%20Plug-and-Play%20Visual%20Reasoner%20via%0A%20%20Data%20Synthesis&body=Title%3A%20From%20the%20Least%20to%20the%20Most%3A%20Building%20a%20Plug-and-Play%20Visual%20Reasoner%20via%0A%20%20Data%20Synthesis%0AAuthor%3A%20Chuanqi%20Cheng%20and%20Jian%20Guan%20and%20Wei%20Wu%20and%20Rui%20Yan%0AAbstract%3A%20%20%20We%20explore%20multi-step%20reasoning%20in%20vision-language%20models%20%28VLMs%29.%20The%20problem%0Ais%20challenging%2C%20as%20reasoning%20data%20consisting%20of%20multiple%20steps%20of%20visual%20and%0Alanguage%20processing%20are%20barely%20available.%20To%20overcome%20the%20challenge%2C%20we%20first%0Aintroduce%20a%20least-to-most%20visual%20reasoning%20paradigm%2C%20which%20interleaves%20steps%20of%0Adecomposing%20a%20question%20into%20sub-questions%20and%20invoking%20external%20tools%20for%0Aresolving%20sub-questions.%20Based%20on%20the%20paradigm%2C%20we%20further%20propose%20a%20novel%20data%0Asynthesis%20approach%20that%20can%20automatically%20create%20questions%20and%20multi-step%0Areasoning%20paths%20for%20an%20image%20in%20a%20bottom-up%20manner.%20Our%20approach%20divides%20the%0Acomplex%20synthesis%20task%20into%20a%20few%20simple%20sub-tasks%2C%20and%20%28almost%20entirely%29%0Arelies%20on%20open-sourced%20models%20to%20accomplish%20the%20sub-tasks.%20Therefore%2C%20the%0Aentire%20synthesis%20process%20is%20reproducible%20and%20cost-efficient%2C%20and%20the%0Asynthesized%20data%20is%20quality%20guaranteed.%20With%20the%20approach%2C%20we%20construct%20%2450%24k%0Avisual%20reasoning%20examples.%20Then%2C%20we%20develop%20a%20visual%20reasoner%20through%0Asupervised%20fine-tuning%2C%20which%20is%20capable%20of%20generally%20enhancing%20the%20reasoning%0Aabilities%20of%20a%20wide%20range%20of%20existing%20VLMs%20in%20a%20plug-and-play%20fashion.%0AExtensive%20experiments%20indicate%20that%20the%20visual%20reasoner%20can%20consistently%20and%0Asignificantly%20improve%20four%20VLMs%20on%20four%20VQA%20benchmarks.%20Our%20code%20and%20dataset%0Aare%20available%20at%20https%3A//github.com/steven-ccq/VisualReasoner.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19934v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520the%2520Least%2520to%2520the%2520Most%253A%2520Building%2520a%2520Plug-and-Play%2520Visual%2520Reasoner%2520via%250A%2520%2520Data%2520Synthesis%26entry.906535625%3DChuanqi%2520Cheng%2520and%2520Jian%2520Guan%2520and%2520Wei%2520Wu%2520and%2520Rui%2520Yan%26entry.1292438233%3D%2520%2520We%2520explore%2520multi-step%2520reasoning%2520in%2520vision-language%2520models%2520%2528VLMs%2529.%2520The%2520problem%250Ais%2520challenging%252C%2520as%2520reasoning%2520data%2520consisting%2520of%2520multiple%2520steps%2520of%2520visual%2520and%250Alanguage%2520processing%2520are%2520barely%2520available.%2520To%2520overcome%2520the%2520challenge%252C%2520we%2520first%250Aintroduce%2520a%2520least-to-most%2520visual%2520reasoning%2520paradigm%252C%2520which%2520interleaves%2520steps%2520of%250Adecomposing%2520a%2520question%2520into%2520sub-questions%2520and%2520invoking%2520external%2520tools%2520for%250Aresolving%2520sub-questions.%2520Based%2520on%2520the%2520paradigm%252C%2520we%2520further%2520propose%2520a%2520novel%2520data%250Asynthesis%2520approach%2520that%2520can%2520automatically%2520create%2520questions%2520and%2520multi-step%250Areasoning%2520paths%2520for%2520an%2520image%2520in%2520a%2520bottom-up%2520manner.%2520Our%2520approach%2520divides%2520the%250Acomplex%2520synthesis%2520task%2520into%2520a%2520few%2520simple%2520sub-tasks%252C%2520and%2520%2528almost%2520entirely%2529%250Arelies%2520on%2520open-sourced%2520models%2520to%2520accomplish%2520the%2520sub-tasks.%2520Therefore%252C%2520the%250Aentire%2520synthesis%2520process%2520is%2520reproducible%2520and%2520cost-efficient%252C%2520and%2520the%250Asynthesized%2520data%2520is%2520quality%2520guaranteed.%2520With%2520the%2520approach%252C%2520we%2520construct%2520%252450%2524k%250Avisual%2520reasoning%2520examples.%2520Then%252C%2520we%2520develop%2520a%2520visual%2520reasoner%2520through%250Asupervised%2520fine-tuning%252C%2520which%2520is%2520capable%2520of%2520generally%2520enhancing%2520the%2520reasoning%250Aabilities%2520of%2520a%2520wide%2520range%2520of%2520existing%2520VLMs%2520in%2520a%2520plug-and-play%2520fashion.%250AExtensive%2520experiments%2520indicate%2520that%2520the%2520visual%2520reasoner%2520can%2520consistently%2520and%250Asignificantly%2520improve%2520four%2520VLMs%2520on%2520four%2520VQA%2520benchmarks.%2520Our%2520code%2520and%2520dataset%250Aare%2520available%2520at%2520https%253A//github.com/steven-ccq/VisualReasoner.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19934v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20the%20Least%20to%20the%20Most%3A%20Building%20a%20Plug-and-Play%20Visual%20Reasoner%20via%0A%20%20Data%20Synthesis&entry.906535625=Chuanqi%20Cheng%20and%20Jian%20Guan%20and%20Wei%20Wu%20and%20Rui%20Yan&entry.1292438233=%20%20We%20explore%20multi-step%20reasoning%20in%20vision-language%20models%20%28VLMs%29.%20The%20problem%0Ais%20challenging%2C%20as%20reasoning%20data%20consisting%20of%20multiple%20steps%20of%20visual%20and%0Alanguage%20processing%20are%20barely%20available.%20To%20overcome%20the%20challenge%2C%20we%20first%0Aintroduce%20a%20least-to-most%20visual%20reasoning%20paradigm%2C%20which%20interleaves%20steps%20of%0Adecomposing%20a%20question%20into%20sub-questions%20and%20invoking%20external%20tools%20for%0Aresolving%20sub-questions.%20Based%20on%20the%20paradigm%2C%20we%20further%20propose%20a%20novel%20data%0Asynthesis%20approach%20that%20can%20automatically%20create%20questions%20and%20multi-step%0Areasoning%20paths%20for%20an%20image%20in%20a%20bottom-up%20manner.%20Our%20approach%20divides%20the%0Acomplex%20synthesis%20task%20into%20a%20few%20simple%20sub-tasks%2C%20and%20%28almost%20entirely%29%0Arelies%20on%20open-sourced%20models%20to%20accomplish%20the%20sub-tasks.%20Therefore%2C%20the%0Aentire%20synthesis%20process%20is%20reproducible%20and%20cost-efficient%2C%20and%20the%0Asynthesized%20data%20is%20quality%20guaranteed.%20With%20the%20approach%2C%20we%20construct%20%2450%24k%0Avisual%20reasoning%20examples.%20Then%2C%20we%20develop%20a%20visual%20reasoner%20through%0Asupervised%20fine-tuning%2C%20which%20is%20capable%20of%20generally%20enhancing%20the%20reasoning%0Aabilities%20of%20a%20wide%20range%20of%20existing%20VLMs%20in%20a%20plug-and-play%20fashion.%0AExtensive%20experiments%20indicate%20that%20the%20visual%20reasoner%20can%20consistently%20and%0Asignificantly%20improve%20four%20VLMs%20on%20four%20VQA%20benchmarks.%20Our%20code%20and%20dataset%0Aare%20available%20at%20https%3A//github.com/steven-ccq/VisualReasoner.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19934v1&entry.124074799=Read"},
{"title": "Impact of Initialization on Intra-subject Pediatric Brain MR Image\n  Registration: A Comparative Analysis between SyN ANTs and Deep Learning-Based\n  Approaches", "author": "Andjela Dimitrijevic and Vincent Noblet and Benjamin De Leener", "abstract": "  This study evaluates the performance of conventional SyN ANTs and\nlearning-based registration methods in the context of pediatric neuroimaging,\nspecifically focusing on intrasubject deformable registration. The comparison\ninvolves three approaches: without (NR), with rigid (RR), and with rigid and\naffine (RAR) initializations. In addition to initialization, performances are\nevaluated in terms of accuracy, speed, and the impact of age intervals and sex\nper pair. Data consists of the publicly available MRI scans from the Calgary\nPreschool dataset, which includes 63 children aged 2-7 years, allowing for 431\nregistration pairs. We implemented the unsupervised DL framework with a U-Net\narchitecture using DeepReg and it was 5-fold cross-validated. Evaluation\nincludes Dice scores for tissue segmentation from 18 smaller regions obtained\nby SynthSeg, analysis of log Jacobian determinants, and registration pro-rated\ntraining and inference times. Learning-based approaches, with or without linear\ninitializations, exhibit slight superiority over SyN ANTs in terms of Dice\nscores. Indeed, DL-based implementations with RR and RAR initializations\nsignificantly outperform SyN ANTs. Both SyN ANTs and DL-based registration\ninvolve parameter optimization, but the choice between these methods depends on\nthe scale of registration: network-based for broader coverage or SyN ANTs for\nspecific structures. Both methods face challenges with larger age intervals due\nto greater growth changes. The main takeaway is that while DL-based methods\nshow promise with faster and more accurate registrations, SyN ANTs remains\nrobust and generalizable without the need for extensive training, highlighting\nthe importance of method selection based on specific registration needs in the\npediatric context. Our code is available at\nhttps://github.com/neuropoly/pediatric-DL-registration\n", "link": "http://arxiv.org/abs/2406.19943v1", "date": "2024-06-28", "relevancy": 1.4908, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4984}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4974}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4961}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Impact%20of%20Initialization%20on%20Intra-subject%20Pediatric%20Brain%20MR%20Image%0A%20%20Registration%3A%20A%20Comparative%20Analysis%20between%20SyN%20ANTs%20and%20Deep%20Learning-Based%0A%20%20Approaches&body=Title%3A%20Impact%20of%20Initialization%20on%20Intra-subject%20Pediatric%20Brain%20MR%20Image%0A%20%20Registration%3A%20A%20Comparative%20Analysis%20between%20SyN%20ANTs%20and%20Deep%20Learning-Based%0A%20%20Approaches%0AAuthor%3A%20Andjela%20Dimitrijevic%20and%20Vincent%20Noblet%20and%20Benjamin%20De%20Leener%0AAbstract%3A%20%20%20This%20study%20evaluates%20the%20performance%20of%20conventional%20SyN%20ANTs%20and%0Alearning-based%20registration%20methods%20in%20the%20context%20of%20pediatric%20neuroimaging%2C%0Aspecifically%20focusing%20on%20intrasubject%20deformable%20registration.%20The%20comparison%0Ainvolves%20three%20approaches%3A%20without%20%28NR%29%2C%20with%20rigid%20%28RR%29%2C%20and%20with%20rigid%20and%0Aaffine%20%28RAR%29%20initializations.%20In%20addition%20to%20initialization%2C%20performances%20are%0Aevaluated%20in%20terms%20of%20accuracy%2C%20speed%2C%20and%20the%20impact%20of%20age%20intervals%20and%20sex%0Aper%20pair.%20Data%20consists%20of%20the%20publicly%20available%20MRI%20scans%20from%20the%20Calgary%0APreschool%20dataset%2C%20which%20includes%2063%20children%20aged%202-7%20years%2C%20allowing%20for%20431%0Aregistration%20pairs.%20We%20implemented%20the%20unsupervised%20DL%20framework%20with%20a%20U-Net%0Aarchitecture%20using%20DeepReg%20and%20it%20was%205-fold%20cross-validated.%20Evaluation%0Aincludes%20Dice%20scores%20for%20tissue%20segmentation%20from%2018%20smaller%20regions%20obtained%0Aby%20SynthSeg%2C%20analysis%20of%20log%20Jacobian%20determinants%2C%20and%20registration%20pro-rated%0Atraining%20and%20inference%20times.%20Learning-based%20approaches%2C%20with%20or%20without%20linear%0Ainitializations%2C%20exhibit%20slight%20superiority%20over%20SyN%20ANTs%20in%20terms%20of%20Dice%0Ascores.%20Indeed%2C%20DL-based%20implementations%20with%20RR%20and%20RAR%20initializations%0Asignificantly%20outperform%20SyN%20ANTs.%20Both%20SyN%20ANTs%20and%20DL-based%20registration%0Ainvolve%20parameter%20optimization%2C%20but%20the%20choice%20between%20these%20methods%20depends%20on%0Athe%20scale%20of%20registration%3A%20network-based%20for%20broader%20coverage%20or%20SyN%20ANTs%20for%0Aspecific%20structures.%20Both%20methods%20face%20challenges%20with%20larger%20age%20intervals%20due%0Ato%20greater%20growth%20changes.%20The%20main%20takeaway%20is%20that%20while%20DL-based%20methods%0Ashow%20promise%20with%20faster%20and%20more%20accurate%20registrations%2C%20SyN%20ANTs%20remains%0Arobust%20and%20generalizable%20without%20the%20need%20for%20extensive%20training%2C%20highlighting%0Athe%20importance%20of%20method%20selection%20based%20on%20specific%20registration%20needs%20in%20the%0Apediatric%20context.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/neuropoly/pediatric-DL-registration%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19943v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImpact%2520of%2520Initialization%2520on%2520Intra-subject%2520Pediatric%2520Brain%2520MR%2520Image%250A%2520%2520Registration%253A%2520A%2520Comparative%2520Analysis%2520between%2520SyN%2520ANTs%2520and%2520Deep%2520Learning-Based%250A%2520%2520Approaches%26entry.906535625%3DAndjela%2520Dimitrijevic%2520and%2520Vincent%2520Noblet%2520and%2520Benjamin%2520De%2520Leener%26entry.1292438233%3D%2520%2520This%2520study%2520evaluates%2520the%2520performance%2520of%2520conventional%2520SyN%2520ANTs%2520and%250Alearning-based%2520registration%2520methods%2520in%2520the%2520context%2520of%2520pediatric%2520neuroimaging%252C%250Aspecifically%2520focusing%2520on%2520intrasubject%2520deformable%2520registration.%2520The%2520comparison%250Ainvolves%2520three%2520approaches%253A%2520without%2520%2528NR%2529%252C%2520with%2520rigid%2520%2528RR%2529%252C%2520and%2520with%2520rigid%2520and%250Aaffine%2520%2528RAR%2529%2520initializations.%2520In%2520addition%2520to%2520initialization%252C%2520performances%2520are%250Aevaluated%2520in%2520terms%2520of%2520accuracy%252C%2520speed%252C%2520and%2520the%2520impact%2520of%2520age%2520intervals%2520and%2520sex%250Aper%2520pair.%2520Data%2520consists%2520of%2520the%2520publicly%2520available%2520MRI%2520scans%2520from%2520the%2520Calgary%250APreschool%2520dataset%252C%2520which%2520includes%252063%2520children%2520aged%25202-7%2520years%252C%2520allowing%2520for%2520431%250Aregistration%2520pairs.%2520We%2520implemented%2520the%2520unsupervised%2520DL%2520framework%2520with%2520a%2520U-Net%250Aarchitecture%2520using%2520DeepReg%2520and%2520it%2520was%25205-fold%2520cross-validated.%2520Evaluation%250Aincludes%2520Dice%2520scores%2520for%2520tissue%2520segmentation%2520from%252018%2520smaller%2520regions%2520obtained%250Aby%2520SynthSeg%252C%2520analysis%2520of%2520log%2520Jacobian%2520determinants%252C%2520and%2520registration%2520pro-rated%250Atraining%2520and%2520inference%2520times.%2520Learning-based%2520approaches%252C%2520with%2520or%2520without%2520linear%250Ainitializations%252C%2520exhibit%2520slight%2520superiority%2520over%2520SyN%2520ANTs%2520in%2520terms%2520of%2520Dice%250Ascores.%2520Indeed%252C%2520DL-based%2520implementations%2520with%2520RR%2520and%2520RAR%2520initializations%250Asignificantly%2520outperform%2520SyN%2520ANTs.%2520Both%2520SyN%2520ANTs%2520and%2520DL-based%2520registration%250Ainvolve%2520parameter%2520optimization%252C%2520but%2520the%2520choice%2520between%2520these%2520methods%2520depends%2520on%250Athe%2520scale%2520of%2520registration%253A%2520network-based%2520for%2520broader%2520coverage%2520or%2520SyN%2520ANTs%2520for%250Aspecific%2520structures.%2520Both%2520methods%2520face%2520challenges%2520with%2520larger%2520age%2520intervals%2520due%250Ato%2520greater%2520growth%2520changes.%2520The%2520main%2520takeaway%2520is%2520that%2520while%2520DL-based%2520methods%250Ashow%2520promise%2520with%2520faster%2520and%2520more%2520accurate%2520registrations%252C%2520SyN%2520ANTs%2520remains%250Arobust%2520and%2520generalizable%2520without%2520the%2520need%2520for%2520extensive%2520training%252C%2520highlighting%250Athe%2520importance%2520of%2520method%2520selection%2520based%2520on%2520specific%2520registration%2520needs%2520in%2520the%250Apediatric%2520context.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/neuropoly/pediatric-DL-registration%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19943v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Impact%20of%20Initialization%20on%20Intra-subject%20Pediatric%20Brain%20MR%20Image%0A%20%20Registration%3A%20A%20Comparative%20Analysis%20between%20SyN%20ANTs%20and%20Deep%20Learning-Based%0A%20%20Approaches&entry.906535625=Andjela%20Dimitrijevic%20and%20Vincent%20Noblet%20and%20Benjamin%20De%20Leener&entry.1292438233=%20%20This%20study%20evaluates%20the%20performance%20of%20conventional%20SyN%20ANTs%20and%0Alearning-based%20registration%20methods%20in%20the%20context%20of%20pediatric%20neuroimaging%2C%0Aspecifically%20focusing%20on%20intrasubject%20deformable%20registration.%20The%20comparison%0Ainvolves%20three%20approaches%3A%20without%20%28NR%29%2C%20with%20rigid%20%28RR%29%2C%20and%20with%20rigid%20and%0Aaffine%20%28RAR%29%20initializations.%20In%20addition%20to%20initialization%2C%20performances%20are%0Aevaluated%20in%20terms%20of%20accuracy%2C%20speed%2C%20and%20the%20impact%20of%20age%20intervals%20and%20sex%0Aper%20pair.%20Data%20consists%20of%20the%20publicly%20available%20MRI%20scans%20from%20the%20Calgary%0APreschool%20dataset%2C%20which%20includes%2063%20children%20aged%202-7%20years%2C%20allowing%20for%20431%0Aregistration%20pairs.%20We%20implemented%20the%20unsupervised%20DL%20framework%20with%20a%20U-Net%0Aarchitecture%20using%20DeepReg%20and%20it%20was%205-fold%20cross-validated.%20Evaluation%0Aincludes%20Dice%20scores%20for%20tissue%20segmentation%20from%2018%20smaller%20regions%20obtained%0Aby%20SynthSeg%2C%20analysis%20of%20log%20Jacobian%20determinants%2C%20and%20registration%20pro-rated%0Atraining%20and%20inference%20times.%20Learning-based%20approaches%2C%20with%20or%20without%20linear%0Ainitializations%2C%20exhibit%20slight%20superiority%20over%20SyN%20ANTs%20in%20terms%20of%20Dice%0Ascores.%20Indeed%2C%20DL-based%20implementations%20with%20RR%20and%20RAR%20initializations%0Asignificantly%20outperform%20SyN%20ANTs.%20Both%20SyN%20ANTs%20and%20DL-based%20registration%0Ainvolve%20parameter%20optimization%2C%20but%20the%20choice%20between%20these%20methods%20depends%20on%0Athe%20scale%20of%20registration%3A%20network-based%20for%20broader%20coverage%20or%20SyN%20ANTs%20for%0Aspecific%20structures.%20Both%20methods%20face%20challenges%20with%20larger%20age%20intervals%20due%0Ato%20greater%20growth%20changes.%20The%20main%20takeaway%20is%20that%20while%20DL-based%20methods%0Ashow%20promise%20with%20faster%20and%20more%20accurate%20registrations%2C%20SyN%20ANTs%20remains%0Arobust%20and%20generalizable%20without%20the%20need%20for%20extensive%20training%2C%20highlighting%0Athe%20importance%20of%20method%20selection%20based%20on%20specific%20registration%20needs%20in%20the%0Apediatric%20context.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/neuropoly/pediatric-DL-registration%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19943v1&entry.124074799=Read"},
{"title": "Learning Decision Policies with Instrumental Variables through Double\n  Machine Learning", "author": "Daqian Shao and Ashkan Soleymani and Francesco Quinzan and Marta Kwiatkowska", "abstract": "  A common issue in learning decision-making policies in data-rich settings is\nspurious correlations in the offline dataset, which can be caused by hidden\nconfounders. Instrumental variable (IV) regression, which utilises a key\nunconfounded variable known as the instrument, is a standard technique for\nlearning causal relationships between confounded action, outcome, and context\nvariables. Most recent IV regression algorithms use a two-stage approach, where\na deep neural network (DNN) estimator learnt in the first stage is directly\nplugged into the second stage, in which another DNN is used to estimate the\ncausal effect. Naively plugging the estimator can cause heavy bias in the\nsecond stage, especially when regularisation bias is present in the first stage\nestimator. We propose DML-IV, a non-linear IV regression method that reduces\nthe bias in two-stage IV regressions and effectively learns high-performing\npolicies. We derive a novel learning objective to reduce bias and design the\nDML-IV algorithm following the double/debiased machine learning (DML)\nframework. The learnt DML-IV estimator has strong convergence rate and\n$O(N^{-1/2})$ suboptimality guarantees that match those when the dataset is\nunconfounded. DML-IV outperforms state-of-the-art IV regression methods on IV\nregression benchmarks and learns high-performing policies in the presence of\ninstruments.\n", "link": "http://arxiv.org/abs/2405.08498v3", "date": "2024-06-28", "relevancy": 1.4245, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4808}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4737}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Decision%20Policies%20with%20Instrumental%20Variables%20through%20Double%0A%20%20Machine%20Learning&body=Title%3A%20Learning%20Decision%20Policies%20with%20Instrumental%20Variables%20through%20Double%0A%20%20Machine%20Learning%0AAuthor%3A%20Daqian%20Shao%20and%20Ashkan%20Soleymani%20and%20Francesco%20Quinzan%20and%20Marta%20Kwiatkowska%0AAbstract%3A%20%20%20A%20common%20issue%20in%20learning%20decision-making%20policies%20in%20data-rich%20settings%20is%0Aspurious%20correlations%20in%20the%20offline%20dataset%2C%20which%20can%20be%20caused%20by%20hidden%0Aconfounders.%20Instrumental%20variable%20%28IV%29%20regression%2C%20which%20utilises%20a%20key%0Aunconfounded%20variable%20known%20as%20the%20instrument%2C%20is%20a%20standard%20technique%20for%0Alearning%20causal%20relationships%20between%20confounded%20action%2C%20outcome%2C%20and%20context%0Avariables.%20Most%20recent%20IV%20regression%20algorithms%20use%20a%20two-stage%20approach%2C%20where%0Aa%20deep%20neural%20network%20%28DNN%29%20estimator%20learnt%20in%20the%20first%20stage%20is%20directly%0Aplugged%20into%20the%20second%20stage%2C%20in%20which%20another%20DNN%20is%20used%20to%20estimate%20the%0Acausal%20effect.%20Naively%20plugging%20the%20estimator%20can%20cause%20heavy%20bias%20in%20the%0Asecond%20stage%2C%20especially%20when%20regularisation%20bias%20is%20present%20in%20the%20first%20stage%0Aestimator.%20We%20propose%20DML-IV%2C%20a%20non-linear%20IV%20regression%20method%20that%20reduces%0Athe%20bias%20in%20two-stage%20IV%20regressions%20and%20effectively%20learns%20high-performing%0Apolicies.%20We%20derive%20a%20novel%20learning%20objective%20to%20reduce%20bias%20and%20design%20the%0ADML-IV%20algorithm%20following%20the%20double/debiased%20machine%20learning%20%28DML%29%0Aframework.%20The%20learnt%20DML-IV%20estimator%20has%20strong%20convergence%20rate%20and%0A%24O%28N%5E%7B-1/2%7D%29%24%20suboptimality%20guarantees%20that%20match%20those%20when%20the%20dataset%20is%0Aunconfounded.%20DML-IV%20outperforms%20state-of-the-art%20IV%20regression%20methods%20on%20IV%0Aregression%20benchmarks%20and%20learns%20high-performing%20policies%20in%20the%20presence%20of%0Ainstruments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.08498v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Decision%2520Policies%2520with%2520Instrumental%2520Variables%2520through%2520Double%250A%2520%2520Machine%2520Learning%26entry.906535625%3DDaqian%2520Shao%2520and%2520Ashkan%2520Soleymani%2520and%2520Francesco%2520Quinzan%2520and%2520Marta%2520Kwiatkowska%26entry.1292438233%3D%2520%2520A%2520common%2520issue%2520in%2520learning%2520decision-making%2520policies%2520in%2520data-rich%2520settings%2520is%250Aspurious%2520correlations%2520in%2520the%2520offline%2520dataset%252C%2520which%2520can%2520be%2520caused%2520by%2520hidden%250Aconfounders.%2520Instrumental%2520variable%2520%2528IV%2529%2520regression%252C%2520which%2520utilises%2520a%2520key%250Aunconfounded%2520variable%2520known%2520as%2520the%2520instrument%252C%2520is%2520a%2520standard%2520technique%2520for%250Alearning%2520causal%2520relationships%2520between%2520confounded%2520action%252C%2520outcome%252C%2520and%2520context%250Avariables.%2520Most%2520recent%2520IV%2520regression%2520algorithms%2520use%2520a%2520two-stage%2520approach%252C%2520where%250Aa%2520deep%2520neural%2520network%2520%2528DNN%2529%2520estimator%2520learnt%2520in%2520the%2520first%2520stage%2520is%2520directly%250Aplugged%2520into%2520the%2520second%2520stage%252C%2520in%2520which%2520another%2520DNN%2520is%2520used%2520to%2520estimate%2520the%250Acausal%2520effect.%2520Naively%2520plugging%2520the%2520estimator%2520can%2520cause%2520heavy%2520bias%2520in%2520the%250Asecond%2520stage%252C%2520especially%2520when%2520regularisation%2520bias%2520is%2520present%2520in%2520the%2520first%2520stage%250Aestimator.%2520We%2520propose%2520DML-IV%252C%2520a%2520non-linear%2520IV%2520regression%2520method%2520that%2520reduces%250Athe%2520bias%2520in%2520two-stage%2520IV%2520regressions%2520and%2520effectively%2520learns%2520high-performing%250Apolicies.%2520We%2520derive%2520a%2520novel%2520learning%2520objective%2520to%2520reduce%2520bias%2520and%2520design%2520the%250ADML-IV%2520algorithm%2520following%2520the%2520double/debiased%2520machine%2520learning%2520%2528DML%2529%250Aframework.%2520The%2520learnt%2520DML-IV%2520estimator%2520has%2520strong%2520convergence%2520rate%2520and%250A%2524O%2528N%255E%257B-1/2%257D%2529%2524%2520suboptimality%2520guarantees%2520that%2520match%2520those%2520when%2520the%2520dataset%2520is%250Aunconfounded.%2520DML-IV%2520outperforms%2520state-of-the-art%2520IV%2520regression%2520methods%2520on%2520IV%250Aregression%2520benchmarks%2520and%2520learns%2520high-performing%2520policies%2520in%2520the%2520presence%2520of%250Ainstruments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.08498v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Decision%20Policies%20with%20Instrumental%20Variables%20through%20Double%0A%20%20Machine%20Learning&entry.906535625=Daqian%20Shao%20and%20Ashkan%20Soleymani%20and%20Francesco%20Quinzan%20and%20Marta%20Kwiatkowska&entry.1292438233=%20%20A%20common%20issue%20in%20learning%20decision-making%20policies%20in%20data-rich%20settings%20is%0Aspurious%20correlations%20in%20the%20offline%20dataset%2C%20which%20can%20be%20caused%20by%20hidden%0Aconfounders.%20Instrumental%20variable%20%28IV%29%20regression%2C%20which%20utilises%20a%20key%0Aunconfounded%20variable%20known%20as%20the%20instrument%2C%20is%20a%20standard%20technique%20for%0Alearning%20causal%20relationships%20between%20confounded%20action%2C%20outcome%2C%20and%20context%0Avariables.%20Most%20recent%20IV%20regression%20algorithms%20use%20a%20two-stage%20approach%2C%20where%0Aa%20deep%20neural%20network%20%28DNN%29%20estimator%20learnt%20in%20the%20first%20stage%20is%20directly%0Aplugged%20into%20the%20second%20stage%2C%20in%20which%20another%20DNN%20is%20used%20to%20estimate%20the%0Acausal%20effect.%20Naively%20plugging%20the%20estimator%20can%20cause%20heavy%20bias%20in%20the%0Asecond%20stage%2C%20especially%20when%20regularisation%20bias%20is%20present%20in%20the%20first%20stage%0Aestimator.%20We%20propose%20DML-IV%2C%20a%20non-linear%20IV%20regression%20method%20that%20reduces%0Athe%20bias%20in%20two-stage%20IV%20regressions%20and%20effectively%20learns%20high-performing%0Apolicies.%20We%20derive%20a%20novel%20learning%20objective%20to%20reduce%20bias%20and%20design%20the%0ADML-IV%20algorithm%20following%20the%20double/debiased%20machine%20learning%20%28DML%29%0Aframework.%20The%20learnt%20DML-IV%20estimator%20has%20strong%20convergence%20rate%20and%0A%24O%28N%5E%7B-1/2%7D%29%24%20suboptimality%20guarantees%20that%20match%20those%20when%20the%20dataset%20is%0Aunconfounded.%20DML-IV%20outperforms%20state-of-the-art%20IV%20regression%20methods%20on%20IV%0Aregression%20benchmarks%20and%20learns%20high-performing%20policies%20in%20the%20presence%20of%0Ainstruments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.08498v3&entry.124074799=Read"},
{"title": "Deceptive Diffusion: Generating Synthetic Adversarial Examples", "author": "Lucas Beerens and Catherine F. Higham and Desmond J. Higham", "abstract": "  We introduce the concept of deceptive diffusion -- training a generative AI\nmodel to produce adversarial images. Whereas a traditional adversarial attack\nalgorithm aims to perturb an existing image to induce a misclassificaton, the\ndeceptive diffusion model can create an arbitrary number of new, misclassified\nimages that are not directly associated with training or test images. Deceptive\ndiffusion offers the possibility of strengthening defence algorithms by\nproviding adversarial training data at scale, including types of\nmisclassification that are otherwise difficult to find. In our experiments, we\nalso investigate the effect of training on a partially attacked data set. This\nhighlights a new type of vulnerability for generative diffusion models: if an\nattacker is able to stealthily poison a portion of the training data, then the\nresulting diffusion model will generate a similar proportion of misleading\noutputs.\n", "link": "http://arxiv.org/abs/2406.19807v1", "date": "2024-06-28", "relevancy": 1.6315, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5591}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5557}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deceptive%20Diffusion%3A%20Generating%20Synthetic%20Adversarial%20Examples&body=Title%3A%20Deceptive%20Diffusion%3A%20Generating%20Synthetic%20Adversarial%20Examples%0AAuthor%3A%20Lucas%20Beerens%20and%20Catherine%20F.%20Higham%20and%20Desmond%20J.%20Higham%0AAbstract%3A%20%20%20We%20introduce%20the%20concept%20of%20deceptive%20diffusion%20--%20training%20a%20generative%20AI%0Amodel%20to%20produce%20adversarial%20images.%20Whereas%20a%20traditional%20adversarial%20attack%0Aalgorithm%20aims%20to%20perturb%20an%20existing%20image%20to%20induce%20a%20misclassificaton%2C%20the%0Adeceptive%20diffusion%20model%20can%20create%20an%20arbitrary%20number%20of%20new%2C%20misclassified%0Aimages%20that%20are%20not%20directly%20associated%20with%20training%20or%20test%20images.%20Deceptive%0Adiffusion%20offers%20the%20possibility%20of%20strengthening%20defence%20algorithms%20by%0Aproviding%20adversarial%20training%20data%20at%20scale%2C%20including%20types%20of%0Amisclassification%20that%20are%20otherwise%20difficult%20to%20find.%20In%20our%20experiments%2C%20we%0Aalso%20investigate%20the%20effect%20of%20training%20on%20a%20partially%20attacked%20data%20set.%20This%0Ahighlights%20a%20new%20type%20of%20vulnerability%20for%20generative%20diffusion%20models%3A%20if%20an%0Aattacker%20is%20able%20to%20stealthily%20poison%20a%20portion%20of%20the%20training%20data%2C%20then%20the%0Aresulting%20diffusion%20model%20will%20generate%20a%20similar%20proportion%20of%20misleading%0Aoutputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19807v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeceptive%2520Diffusion%253A%2520Generating%2520Synthetic%2520Adversarial%2520Examples%26entry.906535625%3DLucas%2520Beerens%2520and%2520Catherine%2520F.%2520Higham%2520and%2520Desmond%2520J.%2520Higham%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520concept%2520of%2520deceptive%2520diffusion%2520--%2520training%2520a%2520generative%2520AI%250Amodel%2520to%2520produce%2520adversarial%2520images.%2520Whereas%2520a%2520traditional%2520adversarial%2520attack%250Aalgorithm%2520aims%2520to%2520perturb%2520an%2520existing%2520image%2520to%2520induce%2520a%2520misclassificaton%252C%2520the%250Adeceptive%2520diffusion%2520model%2520can%2520create%2520an%2520arbitrary%2520number%2520of%2520new%252C%2520misclassified%250Aimages%2520that%2520are%2520not%2520directly%2520associated%2520with%2520training%2520or%2520test%2520images.%2520Deceptive%250Adiffusion%2520offers%2520the%2520possibility%2520of%2520strengthening%2520defence%2520algorithms%2520by%250Aproviding%2520adversarial%2520training%2520data%2520at%2520scale%252C%2520including%2520types%2520of%250Amisclassification%2520that%2520are%2520otherwise%2520difficult%2520to%2520find.%2520In%2520our%2520experiments%252C%2520we%250Aalso%2520investigate%2520the%2520effect%2520of%2520training%2520on%2520a%2520partially%2520attacked%2520data%2520set.%2520This%250Ahighlights%2520a%2520new%2520type%2520of%2520vulnerability%2520for%2520generative%2520diffusion%2520models%253A%2520if%2520an%250Aattacker%2520is%2520able%2520to%2520stealthily%2520poison%2520a%2520portion%2520of%2520the%2520training%2520data%252C%2520then%2520the%250Aresulting%2520diffusion%2520model%2520will%2520generate%2520a%2520similar%2520proportion%2520of%2520misleading%250Aoutputs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19807v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deceptive%20Diffusion%3A%20Generating%20Synthetic%20Adversarial%20Examples&entry.906535625=Lucas%20Beerens%20and%20Catherine%20F.%20Higham%20and%20Desmond%20J.%20Higham&entry.1292438233=%20%20We%20introduce%20the%20concept%20of%20deceptive%20diffusion%20--%20training%20a%20generative%20AI%0Amodel%20to%20produce%20adversarial%20images.%20Whereas%20a%20traditional%20adversarial%20attack%0Aalgorithm%20aims%20to%20perturb%20an%20existing%20image%20to%20induce%20a%20misclassificaton%2C%20the%0Adeceptive%20diffusion%20model%20can%20create%20an%20arbitrary%20number%20of%20new%2C%20misclassified%0Aimages%20that%20are%20not%20directly%20associated%20with%20training%20or%20test%20images.%20Deceptive%0Adiffusion%20offers%20the%20possibility%20of%20strengthening%20defence%20algorithms%20by%0Aproviding%20adversarial%20training%20data%20at%20scale%2C%20including%20types%20of%0Amisclassification%20that%20are%20otherwise%20difficult%20to%20find.%20In%20our%20experiments%2C%20we%0Aalso%20investigate%20the%20effect%20of%20training%20on%20a%20partially%20attacked%20data%20set.%20This%0Ahighlights%20a%20new%20type%20of%20vulnerability%20for%20generative%20diffusion%20models%3A%20if%20an%0Aattacker%20is%20able%20to%20stealthily%20poison%20a%20portion%20of%20the%20training%20data%2C%20then%20the%0Aresulting%20diffusion%20model%20will%20generate%20a%20similar%20proportion%20of%20misleading%0Aoutputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19807v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


