<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250119.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "GSTAR: Gaussian Surface Tracking and Reconstruction", "author": "Chengwei Zheng and Lixin Xue and Juan Zarate and Jie Song", "abstract": "  3D Gaussian Splatting techniques have enabled efficient photo-realistic\nrendering of static scenes. Recent works have extended these approaches to\nsupport surface reconstruction and tracking. However, tracking dynamic surfaces\nwith 3D Gaussians remains challenging due to complex topology changes, such as\nsurfaces appearing, disappearing, or splitting. To address these challenges, we\npropose GSTAR, a novel method that achieves photo-realistic rendering, accurate\nsurface reconstruction, and reliable 3D tracking for general dynamic scenes\nwith changing topology. Given multi-view captures as input, GSTAR binds\nGaussians to mesh faces to represent dynamic objects. For surfaces with\nconsistent topology, GSTAR maintains the mesh topology and tracks the meshes\nusing Gaussians. In regions where topology changes, GSTAR adaptively unbinds\nGaussians from the mesh, enabling accurate registration and the generation of\nnew surfaces based on these optimized Gaussians. Additionally, we introduce a\nsurface-based scene flow method that provides robust initialization for\ntracking between frames. Experiments demonstrate that our method effectively\ntracks and reconstructs dynamic surfaces, enabling a range of applications. Our\nproject page with the code release is available at\nhttps://chengwei-zheng.github.io/GSTAR/.\n", "link": "http://arxiv.org/abs/2501.10283v1", "date": "2025-01-17", "relevancy": 3.5804, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7303}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7215}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6965}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GSTAR%3A%20Gaussian%20Surface%20Tracking%20and%20Reconstruction&body=Title%3A%20GSTAR%3A%20Gaussian%20Surface%20Tracking%20and%20Reconstruction%0AAuthor%3A%20Chengwei%20Zheng%20and%20Lixin%20Xue%20and%20Juan%20Zarate%20and%20Jie%20Song%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20techniques%20have%20enabled%20efficient%20photo-realistic%0Arendering%20of%20static%20scenes.%20Recent%20works%20have%20extended%20these%20approaches%20to%0Asupport%20surface%20reconstruction%20and%20tracking.%20However%2C%20tracking%20dynamic%20surfaces%0Awith%203D%20Gaussians%20remains%20challenging%20due%20to%20complex%20topology%20changes%2C%20such%20as%0Asurfaces%20appearing%2C%20disappearing%2C%20or%20splitting.%20To%20address%20these%20challenges%2C%20we%0Apropose%20GSTAR%2C%20a%20novel%20method%20that%20achieves%20photo-realistic%20rendering%2C%20accurate%0Asurface%20reconstruction%2C%20and%20reliable%203D%20tracking%20for%20general%20dynamic%20scenes%0Awith%20changing%20topology.%20Given%20multi-view%20captures%20as%20input%2C%20GSTAR%20binds%0AGaussians%20to%20mesh%20faces%20to%20represent%20dynamic%20objects.%20For%20surfaces%20with%0Aconsistent%20topology%2C%20GSTAR%20maintains%20the%20mesh%20topology%20and%20tracks%20the%20meshes%0Ausing%20Gaussians.%20In%20regions%20where%20topology%20changes%2C%20GSTAR%20adaptively%20unbinds%0AGaussians%20from%20the%20mesh%2C%20enabling%20accurate%20registration%20and%20the%20generation%20of%0Anew%20surfaces%20based%20on%20these%20optimized%20Gaussians.%20Additionally%2C%20we%20introduce%20a%0Asurface-based%20scene%20flow%20method%20that%20provides%20robust%20initialization%20for%0Atracking%20between%20frames.%20Experiments%20demonstrate%20that%20our%20method%20effectively%0Atracks%20and%20reconstructs%20dynamic%20surfaces%2C%20enabling%20a%20range%20of%20applications.%20Our%0Aproject%20page%20with%20the%20code%20release%20is%20available%20at%0Ahttps%3A//chengwei-zheng.github.io/GSTAR/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10283v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGSTAR%253A%2520Gaussian%2520Surface%2520Tracking%2520and%2520Reconstruction%26entry.906535625%3DChengwei%2520Zheng%2520and%2520Lixin%2520Xue%2520and%2520Juan%2520Zarate%2520and%2520Jie%2520Song%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520techniques%2520have%2520enabled%2520efficient%2520photo-realistic%250Arendering%2520of%2520static%2520scenes.%2520Recent%2520works%2520have%2520extended%2520these%2520approaches%2520to%250Asupport%2520surface%2520reconstruction%2520and%2520tracking.%2520However%252C%2520tracking%2520dynamic%2520surfaces%250Awith%25203D%2520Gaussians%2520remains%2520challenging%2520due%2520to%2520complex%2520topology%2520changes%252C%2520such%2520as%250Asurfaces%2520appearing%252C%2520disappearing%252C%2520or%2520splitting.%2520To%2520address%2520these%2520challenges%252C%2520we%250Apropose%2520GSTAR%252C%2520a%2520novel%2520method%2520that%2520achieves%2520photo-realistic%2520rendering%252C%2520accurate%250Asurface%2520reconstruction%252C%2520and%2520reliable%25203D%2520tracking%2520for%2520general%2520dynamic%2520scenes%250Awith%2520changing%2520topology.%2520Given%2520multi-view%2520captures%2520as%2520input%252C%2520GSTAR%2520binds%250AGaussians%2520to%2520mesh%2520faces%2520to%2520represent%2520dynamic%2520objects.%2520For%2520surfaces%2520with%250Aconsistent%2520topology%252C%2520GSTAR%2520maintains%2520the%2520mesh%2520topology%2520and%2520tracks%2520the%2520meshes%250Ausing%2520Gaussians.%2520In%2520regions%2520where%2520topology%2520changes%252C%2520GSTAR%2520adaptively%2520unbinds%250AGaussians%2520from%2520the%2520mesh%252C%2520enabling%2520accurate%2520registration%2520and%2520the%2520generation%2520of%250Anew%2520surfaces%2520based%2520on%2520these%2520optimized%2520Gaussians.%2520Additionally%252C%2520we%2520introduce%2520a%250Asurface-based%2520scene%2520flow%2520method%2520that%2520provides%2520robust%2520initialization%2520for%250Atracking%2520between%2520frames.%2520Experiments%2520demonstrate%2520that%2520our%2520method%2520effectively%250Atracks%2520and%2520reconstructs%2520dynamic%2520surfaces%252C%2520enabling%2520a%2520range%2520of%2520applications.%2520Our%250Aproject%2520page%2520with%2520the%2520code%2520release%2520is%2520available%2520at%250Ahttps%253A//chengwei-zheng.github.io/GSTAR/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10283v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GSTAR%3A%20Gaussian%20Surface%20Tracking%20and%20Reconstruction&entry.906535625=Chengwei%20Zheng%20and%20Lixin%20Xue%20and%20Juan%20Zarate%20and%20Jie%20Song&entry.1292438233=%20%203D%20Gaussian%20Splatting%20techniques%20have%20enabled%20efficient%20photo-realistic%0Arendering%20of%20static%20scenes.%20Recent%20works%20have%20extended%20these%20approaches%20to%0Asupport%20surface%20reconstruction%20and%20tracking.%20However%2C%20tracking%20dynamic%20surfaces%0Awith%203D%20Gaussians%20remains%20challenging%20due%20to%20complex%20topology%20changes%2C%20such%20as%0Asurfaces%20appearing%2C%20disappearing%2C%20or%20splitting.%20To%20address%20these%20challenges%2C%20we%0Apropose%20GSTAR%2C%20a%20novel%20method%20that%20achieves%20photo-realistic%20rendering%2C%20accurate%0Asurface%20reconstruction%2C%20and%20reliable%203D%20tracking%20for%20general%20dynamic%20scenes%0Awith%20changing%20topology.%20Given%20multi-view%20captures%20as%20input%2C%20GSTAR%20binds%0AGaussians%20to%20mesh%20faces%20to%20represent%20dynamic%20objects.%20For%20surfaces%20with%0Aconsistent%20topology%2C%20GSTAR%20maintains%20the%20mesh%20topology%20and%20tracks%20the%20meshes%0Ausing%20Gaussians.%20In%20regions%20where%20topology%20changes%2C%20GSTAR%20adaptively%20unbinds%0AGaussians%20from%20the%20mesh%2C%20enabling%20accurate%20registration%20and%20the%20generation%20of%0Anew%20surfaces%20based%20on%20these%20optimized%20Gaussians.%20Additionally%2C%20we%20introduce%20a%0Asurface-based%20scene%20flow%20method%20that%20provides%20robust%20initialization%20for%0Atracking%20between%20frames.%20Experiments%20demonstrate%20that%20our%20method%20effectively%0Atracks%20and%20reconstructs%20dynamic%20surfaces%2C%20enabling%20a%20range%20of%20applications.%20Our%0Aproject%20page%20with%20the%20code%20release%20is%20available%20at%0Ahttps%3A//chengwei-zheng.github.io/GSTAR/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10283v1&entry.124074799=Read"},
{"title": "Tarsier2: Advancing Large Vision-Language Models from Detailed Video\n  Description to Comprehensive Video Understanding", "author": "Liping Yuan and Jiawei Wang and Haomiao Sun and Yuchen Zhang and Yuan Lin", "abstract": "  We introduce Tarsier2, a state-of-the-art large vision-language model (LVLM)\ndesigned for generating detailed and accurate video descriptions, while also\nexhibiting superior general video understanding capabilities. Tarsier2 achieves\nsignificant advancements through three key upgrades: (1) Scaling pre-training\ndata from 11M to 40M video-text pairs, enriching both volume and diversity; (2)\nPerforming fine-grained temporal alignment during supervised fine-tuning; (3)\nUsing model-based sampling to automatically construct preference data and\napplying DPO training for optimization. Extensive experiments show that\nTarsier2-7B consistently outperforms leading proprietary models, including\nGPT-4o and Gemini 1.5 Pro, in detailed video description tasks. On the DREAM-1K\nbenchmark, Tarsier2-7B improves F1 by 2.8\\% over GPT-4o and 5.8\\% over\nGemini-1.5-Pro. In human side-by-side evaluations, Tarsier2-7B shows a +8.6\\%\nperformance advantage over GPT-4o and +24.9\\% over Gemini-1.5-Pro. Tarsier2-7B\nalso sets new state-of-the-art results across 15 public benchmarks, spanning\ntasks such as video question-answering, video grounding, hallucination test,\nand embodied question-answering, demonstrating its versatility as a robust\ngeneralist vision-language model.\n", "link": "http://arxiv.org/abs/2501.07888v2", "date": "2025-01-17", "relevancy": 3.0794, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.63}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.63}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tarsier2%3A%20Advancing%20Large%20Vision-Language%20Models%20from%20Detailed%20Video%0A%20%20Description%20to%20Comprehensive%20Video%20Understanding&body=Title%3A%20Tarsier2%3A%20Advancing%20Large%20Vision-Language%20Models%20from%20Detailed%20Video%0A%20%20Description%20to%20Comprehensive%20Video%20Understanding%0AAuthor%3A%20Liping%20Yuan%20and%20Jiawei%20Wang%20and%20Haomiao%20Sun%20and%20Yuchen%20Zhang%20and%20Yuan%20Lin%0AAbstract%3A%20%20%20We%20introduce%20Tarsier2%2C%20a%20state-of-the-art%20large%20vision-language%20model%20%28LVLM%29%0Adesigned%20for%20generating%20detailed%20and%20accurate%20video%20descriptions%2C%20while%20also%0Aexhibiting%20superior%20general%20video%20understanding%20capabilities.%20Tarsier2%20achieves%0Asignificant%20advancements%20through%20three%20key%20upgrades%3A%20%281%29%20Scaling%20pre-training%0Adata%20from%2011M%20to%2040M%20video-text%20pairs%2C%20enriching%20both%20volume%20and%20diversity%3B%20%282%29%0APerforming%20fine-grained%20temporal%20alignment%20during%20supervised%20fine-tuning%3B%20%283%29%0AUsing%20model-based%20sampling%20to%20automatically%20construct%20preference%20data%20and%0Aapplying%20DPO%20training%20for%20optimization.%20Extensive%20experiments%20show%20that%0ATarsier2-7B%20consistently%20outperforms%20leading%20proprietary%20models%2C%20including%0AGPT-4o%20and%20Gemini%201.5%20Pro%2C%20in%20detailed%20video%20description%20tasks.%20On%20the%20DREAM-1K%0Abenchmark%2C%20Tarsier2-7B%20improves%20F1%20by%202.8%5C%25%20over%20GPT-4o%20and%205.8%5C%25%20over%0AGemini-1.5-Pro.%20In%20human%20side-by-side%20evaluations%2C%20Tarsier2-7B%20shows%20a%20%2B8.6%5C%25%0Aperformance%20advantage%20over%20GPT-4o%20and%20%2B24.9%5C%25%20over%20Gemini-1.5-Pro.%20Tarsier2-7B%0Aalso%20sets%20new%20state-of-the-art%20results%20across%2015%20public%20benchmarks%2C%20spanning%0Atasks%20such%20as%20video%20question-answering%2C%20video%20grounding%2C%20hallucination%20test%2C%0Aand%20embodied%20question-answering%2C%20demonstrating%20its%20versatility%20as%20a%20robust%0Ageneralist%20vision-language%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07888v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTarsier2%253A%2520Advancing%2520Large%2520Vision-Language%2520Models%2520from%2520Detailed%2520Video%250A%2520%2520Description%2520to%2520Comprehensive%2520Video%2520Understanding%26entry.906535625%3DLiping%2520Yuan%2520and%2520Jiawei%2520Wang%2520and%2520Haomiao%2520Sun%2520and%2520Yuchen%2520Zhang%2520and%2520Yuan%2520Lin%26entry.1292438233%3D%2520%2520We%2520introduce%2520Tarsier2%252C%2520a%2520state-of-the-art%2520large%2520vision-language%2520model%2520%2528LVLM%2529%250Adesigned%2520for%2520generating%2520detailed%2520and%2520accurate%2520video%2520descriptions%252C%2520while%2520also%250Aexhibiting%2520superior%2520general%2520video%2520understanding%2520capabilities.%2520Tarsier2%2520achieves%250Asignificant%2520advancements%2520through%2520three%2520key%2520upgrades%253A%2520%25281%2529%2520Scaling%2520pre-training%250Adata%2520from%252011M%2520to%252040M%2520video-text%2520pairs%252C%2520enriching%2520both%2520volume%2520and%2520diversity%253B%2520%25282%2529%250APerforming%2520fine-grained%2520temporal%2520alignment%2520during%2520supervised%2520fine-tuning%253B%2520%25283%2529%250AUsing%2520model-based%2520sampling%2520to%2520automatically%2520construct%2520preference%2520data%2520and%250Aapplying%2520DPO%2520training%2520for%2520optimization.%2520Extensive%2520experiments%2520show%2520that%250ATarsier2-7B%2520consistently%2520outperforms%2520leading%2520proprietary%2520models%252C%2520including%250AGPT-4o%2520and%2520Gemini%25201.5%2520Pro%252C%2520in%2520detailed%2520video%2520description%2520tasks.%2520On%2520the%2520DREAM-1K%250Abenchmark%252C%2520Tarsier2-7B%2520improves%2520F1%2520by%25202.8%255C%2525%2520over%2520GPT-4o%2520and%25205.8%255C%2525%2520over%250AGemini-1.5-Pro.%2520In%2520human%2520side-by-side%2520evaluations%252C%2520Tarsier2-7B%2520shows%2520a%2520%252B8.6%255C%2525%250Aperformance%2520advantage%2520over%2520GPT-4o%2520and%2520%252B24.9%255C%2525%2520over%2520Gemini-1.5-Pro.%2520Tarsier2-7B%250Aalso%2520sets%2520new%2520state-of-the-art%2520results%2520across%252015%2520public%2520benchmarks%252C%2520spanning%250Atasks%2520such%2520as%2520video%2520question-answering%252C%2520video%2520grounding%252C%2520hallucination%2520test%252C%250Aand%2520embodied%2520question-answering%252C%2520demonstrating%2520its%2520versatility%2520as%2520a%2520robust%250Ageneralist%2520vision-language%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07888v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tarsier2%3A%20Advancing%20Large%20Vision-Language%20Models%20from%20Detailed%20Video%0A%20%20Description%20to%20Comprehensive%20Video%20Understanding&entry.906535625=Liping%20Yuan%20and%20Jiawei%20Wang%20and%20Haomiao%20Sun%20and%20Yuchen%20Zhang%20and%20Yuan%20Lin&entry.1292438233=%20%20We%20introduce%20Tarsier2%2C%20a%20state-of-the-art%20large%20vision-language%20model%20%28LVLM%29%0Adesigned%20for%20generating%20detailed%20and%20accurate%20video%20descriptions%2C%20while%20also%0Aexhibiting%20superior%20general%20video%20understanding%20capabilities.%20Tarsier2%20achieves%0Asignificant%20advancements%20through%20three%20key%20upgrades%3A%20%281%29%20Scaling%20pre-training%0Adata%20from%2011M%20to%2040M%20video-text%20pairs%2C%20enriching%20both%20volume%20and%20diversity%3B%20%282%29%0APerforming%20fine-grained%20temporal%20alignment%20during%20supervised%20fine-tuning%3B%20%283%29%0AUsing%20model-based%20sampling%20to%20automatically%20construct%20preference%20data%20and%0Aapplying%20DPO%20training%20for%20optimization.%20Extensive%20experiments%20show%20that%0ATarsier2-7B%20consistently%20outperforms%20leading%20proprietary%20models%2C%20including%0AGPT-4o%20and%20Gemini%201.5%20Pro%2C%20in%20detailed%20video%20description%20tasks.%20On%20the%20DREAM-1K%0Abenchmark%2C%20Tarsier2-7B%20improves%20F1%20by%202.8%5C%25%20over%20GPT-4o%20and%205.8%5C%25%20over%0AGemini-1.5-Pro.%20In%20human%20side-by-side%20evaluations%2C%20Tarsier2-7B%20shows%20a%20%2B8.6%5C%25%0Aperformance%20advantage%20over%20GPT-4o%20and%20%2B24.9%5C%25%20over%20Gemini-1.5-Pro.%20Tarsier2-7B%0Aalso%20sets%20new%20state-of-the-art%20results%20across%2015%20public%20benchmarks%2C%20spanning%0Atasks%20such%20as%20video%20question-answering%2C%20video%20grounding%2C%20hallucination%20test%2C%0Aand%20embodied%20question-answering%2C%20demonstrating%20its%20versatility%20as%20a%20robust%0Ageneralist%20vision-language%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07888v2&entry.124074799=Read"},
{"title": "A Vision-Language Framework for Multispectral Scene Representation Using\n  Language-Grounded Features", "author": "Enes Karanfil and Nevrez Imamoglu and Erkut Erdem and Aykut Erdem", "abstract": "  Scene understanding in remote sensing often faces challenges in generating\naccurate representations for complex environments such as various land use\nareas or coastal regions, which may also include snow, clouds, or haze. To\naddress this, we present a vision-language framework named Spectral LLaVA,\nwhich integrates multispectral data with vision-language alignment techniques\nto enhance scene representation and description. Using the BigEarthNet v2\ndataset from Sentinel-2, we establish a baseline with RGB-based scene\ndescriptions and further demonstrate substantial improvements through the\nincorporation of multispectral information. Our framework optimizes a\nlightweight linear projection layer for alignment while keeping the vision\nbackbone of SpectralGPT frozen. Our experiments encompass scene classification\nusing linear probing and language modeling for jointly performing scene\nclassification and description generation. Our results highlight Spectral\nLLaVA's ability to produce detailed and accurate descriptions, particularly for\nscenarios where RGB data alone proves inadequate, while also enhancing\nclassification performance by refining SpectralGPT features into semantically\nmeaningful representations.\n", "link": "http://arxiv.org/abs/2501.10144v1", "date": "2025-01-17", "relevancy": 3.056, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6364}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6364}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Vision-Language%20Framework%20for%20Multispectral%20Scene%20Representation%20Using%0A%20%20Language-Grounded%20Features&body=Title%3A%20A%20Vision-Language%20Framework%20for%20Multispectral%20Scene%20Representation%20Using%0A%20%20Language-Grounded%20Features%0AAuthor%3A%20Enes%20Karanfil%20and%20Nevrez%20Imamoglu%20and%20Erkut%20Erdem%20and%20Aykut%20Erdem%0AAbstract%3A%20%20%20Scene%20understanding%20in%20remote%20sensing%20often%20faces%20challenges%20in%20generating%0Aaccurate%20representations%20for%20complex%20environments%20such%20as%20various%20land%20use%0Aareas%20or%20coastal%20regions%2C%20which%20may%20also%20include%20snow%2C%20clouds%2C%20or%20haze.%20To%0Aaddress%20this%2C%20we%20present%20a%20vision-language%20framework%20named%20Spectral%20LLaVA%2C%0Awhich%20integrates%20multispectral%20data%20with%20vision-language%20alignment%20techniques%0Ato%20enhance%20scene%20representation%20and%20description.%20Using%20the%20BigEarthNet%20v2%0Adataset%20from%20Sentinel-2%2C%20we%20establish%20a%20baseline%20with%20RGB-based%20scene%0Adescriptions%20and%20further%20demonstrate%20substantial%20improvements%20through%20the%0Aincorporation%20of%20multispectral%20information.%20Our%20framework%20optimizes%20a%0Alightweight%20linear%20projection%20layer%20for%20alignment%20while%20keeping%20the%20vision%0Abackbone%20of%20SpectralGPT%20frozen.%20Our%20experiments%20encompass%20scene%20classification%0Ausing%20linear%20probing%20and%20language%20modeling%20for%20jointly%20performing%20scene%0Aclassification%20and%20description%20generation.%20Our%20results%20highlight%20Spectral%0ALLaVA%27s%20ability%20to%20produce%20detailed%20and%20accurate%20descriptions%2C%20particularly%20for%0Ascenarios%20where%20RGB%20data%20alone%20proves%20inadequate%2C%20while%20also%20enhancing%0Aclassification%20performance%20by%20refining%20SpectralGPT%20features%20into%20semantically%0Ameaningful%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10144v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Vision-Language%2520Framework%2520for%2520Multispectral%2520Scene%2520Representation%2520Using%250A%2520%2520Language-Grounded%2520Features%26entry.906535625%3DEnes%2520Karanfil%2520and%2520Nevrez%2520Imamoglu%2520and%2520Erkut%2520Erdem%2520and%2520Aykut%2520Erdem%26entry.1292438233%3D%2520%2520Scene%2520understanding%2520in%2520remote%2520sensing%2520often%2520faces%2520challenges%2520in%2520generating%250Aaccurate%2520representations%2520for%2520complex%2520environments%2520such%2520as%2520various%2520land%2520use%250Aareas%2520or%2520coastal%2520regions%252C%2520which%2520may%2520also%2520include%2520snow%252C%2520clouds%252C%2520or%2520haze.%2520To%250Aaddress%2520this%252C%2520we%2520present%2520a%2520vision-language%2520framework%2520named%2520Spectral%2520LLaVA%252C%250Awhich%2520integrates%2520multispectral%2520data%2520with%2520vision-language%2520alignment%2520techniques%250Ato%2520enhance%2520scene%2520representation%2520and%2520description.%2520Using%2520the%2520BigEarthNet%2520v2%250Adataset%2520from%2520Sentinel-2%252C%2520we%2520establish%2520a%2520baseline%2520with%2520RGB-based%2520scene%250Adescriptions%2520and%2520further%2520demonstrate%2520substantial%2520improvements%2520through%2520the%250Aincorporation%2520of%2520multispectral%2520information.%2520Our%2520framework%2520optimizes%2520a%250Alightweight%2520linear%2520projection%2520layer%2520for%2520alignment%2520while%2520keeping%2520the%2520vision%250Abackbone%2520of%2520SpectralGPT%2520frozen.%2520Our%2520experiments%2520encompass%2520scene%2520classification%250Ausing%2520linear%2520probing%2520and%2520language%2520modeling%2520for%2520jointly%2520performing%2520scene%250Aclassification%2520and%2520description%2520generation.%2520Our%2520results%2520highlight%2520Spectral%250ALLaVA%2527s%2520ability%2520to%2520produce%2520detailed%2520and%2520accurate%2520descriptions%252C%2520particularly%2520for%250Ascenarios%2520where%2520RGB%2520data%2520alone%2520proves%2520inadequate%252C%2520while%2520also%2520enhancing%250Aclassification%2520performance%2520by%2520refining%2520SpectralGPT%2520features%2520into%2520semantically%250Ameaningful%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10144v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Vision-Language%20Framework%20for%20Multispectral%20Scene%20Representation%20Using%0A%20%20Language-Grounded%20Features&entry.906535625=Enes%20Karanfil%20and%20Nevrez%20Imamoglu%20and%20Erkut%20Erdem%20and%20Aykut%20Erdem&entry.1292438233=%20%20Scene%20understanding%20in%20remote%20sensing%20often%20faces%20challenges%20in%20generating%0Aaccurate%20representations%20for%20complex%20environments%20such%20as%20various%20land%20use%0Aareas%20or%20coastal%20regions%2C%20which%20may%20also%20include%20snow%2C%20clouds%2C%20or%20haze.%20To%0Aaddress%20this%2C%20we%20present%20a%20vision-language%20framework%20named%20Spectral%20LLaVA%2C%0Awhich%20integrates%20multispectral%20data%20with%20vision-language%20alignment%20techniques%0Ato%20enhance%20scene%20representation%20and%20description.%20Using%20the%20BigEarthNet%20v2%0Adataset%20from%20Sentinel-2%2C%20we%20establish%20a%20baseline%20with%20RGB-based%20scene%0Adescriptions%20and%20further%20demonstrate%20substantial%20improvements%20through%20the%0Aincorporation%20of%20multispectral%20information.%20Our%20framework%20optimizes%20a%0Alightweight%20linear%20projection%20layer%20for%20alignment%20while%20keeping%20the%20vision%0Abackbone%20of%20SpectralGPT%20frozen.%20Our%20experiments%20encompass%20scene%20classification%0Ausing%20linear%20probing%20and%20language%20modeling%20for%20jointly%20performing%20scene%0Aclassification%20and%20description%20generation.%20Our%20results%20highlight%20Spectral%0ALLaVA%27s%20ability%20to%20produce%20detailed%20and%20accurate%20descriptions%2C%20particularly%20for%0Ascenarios%20where%20RGB%20data%20alone%20proves%20inadequate%2C%20while%20also%20enhancing%0Aclassification%20performance%20by%20refining%20SpectralGPT%20features%20into%20semantically%0Ameaningful%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10144v1&entry.124074799=Read"},
{"title": "MutualForce: Mutual-Aware Enhancement for 4D Radar-LiDAR 3D Object\n  Detection", "author": "Xiangyuan Peng and Huawei Sun and Kay Bierzynski and Anton Fischbacher and Lorenzo Servadei and Robert Wille", "abstract": "  Radar and LiDAR have been widely used in autonomous driving as LiDAR provides\nrich structure information, and radar demonstrates high robustness under\nadverse weather. Recent studies highlight the effectiveness of fusing radar and\nLiDAR point clouds. However, challenges remain due to the modality misalignment\nand information loss during feature extractions. To address these issues, we\npropose a 4D radar-LiDAR framework to mutually enhance their representations.\nInitially, the indicative features from radar are utilized to guide both radar\nand LiDAR geometric feature learning. Subsequently, to mitigate their sparsity\ngap, the shape information from LiDAR is used to enrich radar BEV features.\nExtensive experiments on the View-of-Delft (VoD) dataset demonstrate our\napproach's superiority over existing methods, achieving the highest mAP of\n71.76% across the entire area and 86.36\\% within the driving corridor.\nEspecially for cars, we improve the AP by 4.17% and 4.20% due to the strong\nindicative features and symmetric shapes.\n", "link": "http://arxiv.org/abs/2501.10266v1", "date": "2025-01-17", "relevancy": 2.8887, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5862}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.581}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MutualForce%3A%20Mutual-Aware%20Enhancement%20for%204D%20Radar-LiDAR%203D%20Object%0A%20%20Detection&body=Title%3A%20MutualForce%3A%20Mutual-Aware%20Enhancement%20for%204D%20Radar-LiDAR%203D%20Object%0A%20%20Detection%0AAuthor%3A%20Xiangyuan%20Peng%20and%20Huawei%20Sun%20and%20Kay%20Bierzynski%20and%20Anton%20Fischbacher%20and%20Lorenzo%20Servadei%20and%20Robert%20Wille%0AAbstract%3A%20%20%20Radar%20and%20LiDAR%20have%20been%20widely%20used%20in%20autonomous%20driving%20as%20LiDAR%20provides%0Arich%20structure%20information%2C%20and%20radar%20demonstrates%20high%20robustness%20under%0Aadverse%20weather.%20Recent%20studies%20highlight%20the%20effectiveness%20of%20fusing%20radar%20and%0ALiDAR%20point%20clouds.%20However%2C%20challenges%20remain%20due%20to%20the%20modality%20misalignment%0Aand%20information%20loss%20during%20feature%20extractions.%20To%20address%20these%20issues%2C%20we%0Apropose%20a%204D%20radar-LiDAR%20framework%20to%20mutually%20enhance%20their%20representations.%0AInitially%2C%20the%20indicative%20features%20from%20radar%20are%20utilized%20to%20guide%20both%20radar%0Aand%20LiDAR%20geometric%20feature%20learning.%20Subsequently%2C%20to%20mitigate%20their%20sparsity%0Agap%2C%20the%20shape%20information%20from%20LiDAR%20is%20used%20to%20enrich%20radar%20BEV%20features.%0AExtensive%20experiments%20on%20the%20View-of-Delft%20%28VoD%29%20dataset%20demonstrate%20our%0Aapproach%27s%20superiority%20over%20existing%20methods%2C%20achieving%20the%20highest%20mAP%20of%0A71.76%25%20across%20the%20entire%20area%20and%2086.36%5C%25%20within%20the%20driving%20corridor.%0AEspecially%20for%20cars%2C%20we%20improve%20the%20AP%20by%204.17%25%20and%204.20%25%20due%20to%20the%20strong%0Aindicative%20features%20and%20symmetric%20shapes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10266v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMutualForce%253A%2520Mutual-Aware%2520Enhancement%2520for%25204D%2520Radar-LiDAR%25203D%2520Object%250A%2520%2520Detection%26entry.906535625%3DXiangyuan%2520Peng%2520and%2520Huawei%2520Sun%2520and%2520Kay%2520Bierzynski%2520and%2520Anton%2520Fischbacher%2520and%2520Lorenzo%2520Servadei%2520and%2520Robert%2520Wille%26entry.1292438233%3D%2520%2520Radar%2520and%2520LiDAR%2520have%2520been%2520widely%2520used%2520in%2520autonomous%2520driving%2520as%2520LiDAR%2520provides%250Arich%2520structure%2520information%252C%2520and%2520radar%2520demonstrates%2520high%2520robustness%2520under%250Aadverse%2520weather.%2520Recent%2520studies%2520highlight%2520the%2520effectiveness%2520of%2520fusing%2520radar%2520and%250ALiDAR%2520point%2520clouds.%2520However%252C%2520challenges%2520remain%2520due%2520to%2520the%2520modality%2520misalignment%250Aand%2520information%2520loss%2520during%2520feature%2520extractions.%2520To%2520address%2520these%2520issues%252C%2520we%250Apropose%2520a%25204D%2520radar-LiDAR%2520framework%2520to%2520mutually%2520enhance%2520their%2520representations.%250AInitially%252C%2520the%2520indicative%2520features%2520from%2520radar%2520are%2520utilized%2520to%2520guide%2520both%2520radar%250Aand%2520LiDAR%2520geometric%2520feature%2520learning.%2520Subsequently%252C%2520to%2520mitigate%2520their%2520sparsity%250Agap%252C%2520the%2520shape%2520information%2520from%2520LiDAR%2520is%2520used%2520to%2520enrich%2520radar%2520BEV%2520features.%250AExtensive%2520experiments%2520on%2520the%2520View-of-Delft%2520%2528VoD%2529%2520dataset%2520demonstrate%2520our%250Aapproach%2527s%2520superiority%2520over%2520existing%2520methods%252C%2520achieving%2520the%2520highest%2520mAP%2520of%250A71.76%2525%2520across%2520the%2520entire%2520area%2520and%252086.36%255C%2525%2520within%2520the%2520driving%2520corridor.%250AEspecially%2520for%2520cars%252C%2520we%2520improve%2520the%2520AP%2520by%25204.17%2525%2520and%25204.20%2525%2520due%2520to%2520the%2520strong%250Aindicative%2520features%2520and%2520symmetric%2520shapes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10266v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MutualForce%3A%20Mutual-Aware%20Enhancement%20for%204D%20Radar-LiDAR%203D%20Object%0A%20%20Detection&entry.906535625=Xiangyuan%20Peng%20and%20Huawei%20Sun%20and%20Kay%20Bierzynski%20and%20Anton%20Fischbacher%20and%20Lorenzo%20Servadei%20and%20Robert%20Wille&entry.1292438233=%20%20Radar%20and%20LiDAR%20have%20been%20widely%20used%20in%20autonomous%20driving%20as%20LiDAR%20provides%0Arich%20structure%20information%2C%20and%20radar%20demonstrates%20high%20robustness%20under%0Aadverse%20weather.%20Recent%20studies%20highlight%20the%20effectiveness%20of%20fusing%20radar%20and%0ALiDAR%20point%20clouds.%20However%2C%20challenges%20remain%20due%20to%20the%20modality%20misalignment%0Aand%20information%20loss%20during%20feature%20extractions.%20To%20address%20these%20issues%2C%20we%0Apropose%20a%204D%20radar-LiDAR%20framework%20to%20mutually%20enhance%20their%20representations.%0AInitially%2C%20the%20indicative%20features%20from%20radar%20are%20utilized%20to%20guide%20both%20radar%0Aand%20LiDAR%20geometric%20feature%20learning.%20Subsequently%2C%20to%20mitigate%20their%20sparsity%0Agap%2C%20the%20shape%20information%20from%20LiDAR%20is%20used%20to%20enrich%20radar%20BEV%20features.%0AExtensive%20experiments%20on%20the%20View-of-Delft%20%28VoD%29%20dataset%20demonstrate%20our%0Aapproach%27s%20superiority%20over%20existing%20methods%2C%20achieving%20the%20highest%20mAP%20of%0A71.76%25%20across%20the%20entire%20area%20and%2086.36%5C%25%20within%20the%20driving%20corridor.%0AEspecially%20for%20cars%2C%20we%20improve%20the%20AP%20by%204.17%25%20and%204.20%25%20due%20to%20the%20strong%0Aindicative%20features%20and%20symmetric%20shapes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10266v1&entry.124074799=Read"},
{"title": "FaceXBench: Evaluating Multimodal LLMs on Face Understanding", "author": "Kartik Narayan and Vibashan VS and Vishal M. Patel", "abstract": "  Multimodal Large Language Models (MLLMs) demonstrate impressive\nproblem-solving abilities across a wide range of tasks and domains. However,\ntheir capacity for face understanding has not been systematically studied. To\naddress this gap, we introduce FaceXBench, a comprehensive benchmark designed\nto evaluate MLLMs on complex face understanding tasks. FaceXBench includes\n5,000 multimodal multiple-choice questions derived from 25 public datasets and\na newly created dataset, FaceXAPI. These questions cover 14 tasks across 6\nbroad categories, assessing MLLMs' face understanding abilities in bias and\nfairness, face authentication, recognition, analysis, localization and tool\nretrieval. Using FaceXBench, we conduct an extensive evaluation of 26\nopen-source MLLMs alongside 2 proprietary models, revealing the unique\nchallenges in complex face understanding tasks. We analyze the models across\nthree evaluation settings: zero-shot, in-context task description, and\nchain-of-thought prompting. Our detailed analysis reveals that current MLLMs,\nincluding advanced models like GPT-4o, and GeminiPro 1.5, show significant room\nfor improvement. We believe FaceXBench will be a crucial resource for\ndeveloping MLLMs equipped to perform sophisticated face understanding. Code:\nhttps://github.com/Kartik-3004/facexbench\n", "link": "http://arxiv.org/abs/2501.10360v1", "date": "2025-01-17", "relevancy": 2.8297, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5722}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5722}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FaceXBench%3A%20Evaluating%20Multimodal%20LLMs%20on%20Face%20Understanding&body=Title%3A%20FaceXBench%3A%20Evaluating%20Multimodal%20LLMs%20on%20Face%20Understanding%0AAuthor%3A%20Kartik%20Narayan%20and%20Vibashan%20VS%20and%20Vishal%20M.%20Patel%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20demonstrate%20impressive%0Aproblem-solving%20abilities%20across%20a%20wide%20range%20of%20tasks%20and%20domains.%20However%2C%0Atheir%20capacity%20for%20face%20understanding%20has%20not%20been%20systematically%20studied.%20To%0Aaddress%20this%20gap%2C%20we%20introduce%20FaceXBench%2C%20a%20comprehensive%20benchmark%20designed%0Ato%20evaluate%20MLLMs%20on%20complex%20face%20understanding%20tasks.%20FaceXBench%20includes%0A5%2C000%20multimodal%20multiple-choice%20questions%20derived%20from%2025%20public%20datasets%20and%0Aa%20newly%20created%20dataset%2C%20FaceXAPI.%20These%20questions%20cover%2014%20tasks%20across%206%0Abroad%20categories%2C%20assessing%20MLLMs%27%20face%20understanding%20abilities%20in%20bias%20and%0Afairness%2C%20face%20authentication%2C%20recognition%2C%20analysis%2C%20localization%20and%20tool%0Aretrieval.%20Using%20FaceXBench%2C%20we%20conduct%20an%20extensive%20evaluation%20of%2026%0Aopen-source%20MLLMs%20alongside%202%20proprietary%20models%2C%20revealing%20the%20unique%0Achallenges%20in%20complex%20face%20understanding%20tasks.%20We%20analyze%20the%20models%20across%0Athree%20evaluation%20settings%3A%20zero-shot%2C%20in-context%20task%20description%2C%20and%0Achain-of-thought%20prompting.%20Our%20detailed%20analysis%20reveals%20that%20current%20MLLMs%2C%0Aincluding%20advanced%20models%20like%20GPT-4o%2C%20and%20GeminiPro%201.5%2C%20show%20significant%20room%0Afor%20improvement.%20We%20believe%20FaceXBench%20will%20be%20a%20crucial%20resource%20for%0Adeveloping%20MLLMs%20equipped%20to%20perform%20sophisticated%20face%20understanding.%20Code%3A%0Ahttps%3A//github.com/Kartik-3004/facexbench%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10360v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFaceXBench%253A%2520Evaluating%2520Multimodal%2520LLMs%2520on%2520Face%2520Understanding%26entry.906535625%3DKartik%2520Narayan%2520and%2520Vibashan%2520VS%2520and%2520Vishal%2520M.%2520Patel%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520demonstrate%2520impressive%250Aproblem-solving%2520abilities%2520across%2520a%2520wide%2520range%2520of%2520tasks%2520and%2520domains.%2520However%252C%250Atheir%2520capacity%2520for%2520face%2520understanding%2520has%2520not%2520been%2520systematically%2520studied.%2520To%250Aaddress%2520this%2520gap%252C%2520we%2520introduce%2520FaceXBench%252C%2520a%2520comprehensive%2520benchmark%2520designed%250Ato%2520evaluate%2520MLLMs%2520on%2520complex%2520face%2520understanding%2520tasks.%2520FaceXBench%2520includes%250A5%252C000%2520multimodal%2520multiple-choice%2520questions%2520derived%2520from%252025%2520public%2520datasets%2520and%250Aa%2520newly%2520created%2520dataset%252C%2520FaceXAPI.%2520These%2520questions%2520cover%252014%2520tasks%2520across%25206%250Abroad%2520categories%252C%2520assessing%2520MLLMs%2527%2520face%2520understanding%2520abilities%2520in%2520bias%2520and%250Afairness%252C%2520face%2520authentication%252C%2520recognition%252C%2520analysis%252C%2520localization%2520and%2520tool%250Aretrieval.%2520Using%2520FaceXBench%252C%2520we%2520conduct%2520an%2520extensive%2520evaluation%2520of%252026%250Aopen-source%2520MLLMs%2520alongside%25202%2520proprietary%2520models%252C%2520revealing%2520the%2520unique%250Achallenges%2520in%2520complex%2520face%2520understanding%2520tasks.%2520We%2520analyze%2520the%2520models%2520across%250Athree%2520evaluation%2520settings%253A%2520zero-shot%252C%2520in-context%2520task%2520description%252C%2520and%250Achain-of-thought%2520prompting.%2520Our%2520detailed%2520analysis%2520reveals%2520that%2520current%2520MLLMs%252C%250Aincluding%2520advanced%2520models%2520like%2520GPT-4o%252C%2520and%2520GeminiPro%25201.5%252C%2520show%2520significant%2520room%250Afor%2520improvement.%2520We%2520believe%2520FaceXBench%2520will%2520be%2520a%2520crucial%2520resource%2520for%250Adeveloping%2520MLLMs%2520equipped%2520to%2520perform%2520sophisticated%2520face%2520understanding.%2520Code%253A%250Ahttps%253A//github.com/Kartik-3004/facexbench%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10360v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FaceXBench%3A%20Evaluating%20Multimodal%20LLMs%20on%20Face%20Understanding&entry.906535625=Kartik%20Narayan%20and%20Vibashan%20VS%20and%20Vishal%20M.%20Patel&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20demonstrate%20impressive%0Aproblem-solving%20abilities%20across%20a%20wide%20range%20of%20tasks%20and%20domains.%20However%2C%0Atheir%20capacity%20for%20face%20understanding%20has%20not%20been%20systematically%20studied.%20To%0Aaddress%20this%20gap%2C%20we%20introduce%20FaceXBench%2C%20a%20comprehensive%20benchmark%20designed%0Ato%20evaluate%20MLLMs%20on%20complex%20face%20understanding%20tasks.%20FaceXBench%20includes%0A5%2C000%20multimodal%20multiple-choice%20questions%20derived%20from%2025%20public%20datasets%20and%0Aa%20newly%20created%20dataset%2C%20FaceXAPI.%20These%20questions%20cover%2014%20tasks%20across%206%0Abroad%20categories%2C%20assessing%20MLLMs%27%20face%20understanding%20abilities%20in%20bias%20and%0Afairness%2C%20face%20authentication%2C%20recognition%2C%20analysis%2C%20localization%20and%20tool%0Aretrieval.%20Using%20FaceXBench%2C%20we%20conduct%20an%20extensive%20evaluation%20of%2026%0Aopen-source%20MLLMs%20alongside%202%20proprietary%20models%2C%20revealing%20the%20unique%0Achallenges%20in%20complex%20face%20understanding%20tasks.%20We%20analyze%20the%20models%20across%0Athree%20evaluation%20settings%3A%20zero-shot%2C%20in-context%20task%20description%2C%20and%0Achain-of-thought%20prompting.%20Our%20detailed%20analysis%20reveals%20that%20current%20MLLMs%2C%0Aincluding%20advanced%20models%20like%20GPT-4o%2C%20and%20GeminiPro%201.5%2C%20show%20significant%20room%0Afor%20improvement.%20We%20believe%20FaceXBench%20will%20be%20a%20crucial%20resource%20for%0Adeveloping%20MLLMs%20equipped%20to%20perform%20sophisticated%20face%20understanding.%20Code%3A%0Ahttps%3A//github.com/Kartik-3004/facexbench%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10360v1&entry.124074799=Read"},
{"title": "Mesh2SLAM in VR: A Fast Geometry-Based SLAM Framework for Rapid\n  Prototyping in Virtual Reality Applications", "author": "Carlos Augusto Pinheiro de Sousa and Heiko Hamann and Oliver Deussen", "abstract": "  SLAM is a foundational technique with broad applications in robotics and\nAR/VR. SLAM simulations evaluate new concepts, but testing on\nresource-constrained devices, such as VR HMDs, faces challenges: high\ncomputational cost and restricted sensor data access. This work proposes a\nsparse framework using mesh geometry projections as features, which improves\nefficiency and circumvents direct sensor data access, advancing SLAM research\nas we demonstrate in VR and through numerical evaluation.\n", "link": "http://arxiv.org/abs/2501.09600v2", "date": "2025-01-17", "relevancy": 2.8114, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6086}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5714}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mesh2SLAM%20in%20VR%3A%20A%20Fast%20Geometry-Based%20SLAM%20Framework%20for%20Rapid%0A%20%20Prototyping%20in%20Virtual%20Reality%20Applications&body=Title%3A%20Mesh2SLAM%20in%20VR%3A%20A%20Fast%20Geometry-Based%20SLAM%20Framework%20for%20Rapid%0A%20%20Prototyping%20in%20Virtual%20Reality%20Applications%0AAuthor%3A%20Carlos%20Augusto%20Pinheiro%20de%20Sousa%20and%20Heiko%20Hamann%20and%20Oliver%20Deussen%0AAbstract%3A%20%20%20SLAM%20is%20a%20foundational%20technique%20with%20broad%20applications%20in%20robotics%20and%0AAR/VR.%20SLAM%20simulations%20evaluate%20new%20concepts%2C%20but%20testing%20on%0Aresource-constrained%20devices%2C%20such%20as%20VR%20HMDs%2C%20faces%20challenges%3A%20high%0Acomputational%20cost%20and%20restricted%20sensor%20data%20access.%20This%20work%20proposes%20a%0Asparse%20framework%20using%20mesh%20geometry%20projections%20as%20features%2C%20which%20improves%0Aefficiency%20and%20circumvents%20direct%20sensor%20data%20access%2C%20advancing%20SLAM%20research%0Aas%20we%20demonstrate%20in%20VR%20and%20through%20numerical%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09600v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMesh2SLAM%2520in%2520VR%253A%2520A%2520Fast%2520Geometry-Based%2520SLAM%2520Framework%2520for%2520Rapid%250A%2520%2520Prototyping%2520in%2520Virtual%2520Reality%2520Applications%26entry.906535625%3DCarlos%2520Augusto%2520Pinheiro%2520de%2520Sousa%2520and%2520Heiko%2520Hamann%2520and%2520Oliver%2520Deussen%26entry.1292438233%3D%2520%2520SLAM%2520is%2520a%2520foundational%2520technique%2520with%2520broad%2520applications%2520in%2520robotics%2520and%250AAR/VR.%2520SLAM%2520simulations%2520evaluate%2520new%2520concepts%252C%2520but%2520testing%2520on%250Aresource-constrained%2520devices%252C%2520such%2520as%2520VR%2520HMDs%252C%2520faces%2520challenges%253A%2520high%250Acomputational%2520cost%2520and%2520restricted%2520sensor%2520data%2520access.%2520This%2520work%2520proposes%2520a%250Asparse%2520framework%2520using%2520mesh%2520geometry%2520projections%2520as%2520features%252C%2520which%2520improves%250Aefficiency%2520and%2520circumvents%2520direct%2520sensor%2520data%2520access%252C%2520advancing%2520SLAM%2520research%250Aas%2520we%2520demonstrate%2520in%2520VR%2520and%2520through%2520numerical%2520evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09600v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mesh2SLAM%20in%20VR%3A%20A%20Fast%20Geometry-Based%20SLAM%20Framework%20for%20Rapid%0A%20%20Prototyping%20in%20Virtual%20Reality%20Applications&entry.906535625=Carlos%20Augusto%20Pinheiro%20de%20Sousa%20and%20Heiko%20Hamann%20and%20Oliver%20Deussen&entry.1292438233=%20%20SLAM%20is%20a%20foundational%20technique%20with%20broad%20applications%20in%20robotics%20and%0AAR/VR.%20SLAM%20simulations%20evaluate%20new%20concepts%2C%20but%20testing%20on%0Aresource-constrained%20devices%2C%20such%20as%20VR%20HMDs%2C%20faces%20challenges%3A%20high%0Acomputational%20cost%20and%20restricted%20sensor%20data%20access.%20This%20work%20proposes%20a%0Asparse%20framework%20using%20mesh%20geometry%20projections%20as%20features%2C%20which%20improves%0Aefficiency%20and%20circumvents%20direct%20sensor%20data%20access%2C%20advancing%20SLAM%20research%0Aas%20we%20demonstrate%20in%20VR%20and%20through%20numerical%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09600v2&entry.124074799=Read"},
{"title": "Multi-stage Deep Learning Artifact Reduction for Pallel-beam Computed\n  Tomography", "author": "Jiayang Shi and Daniel M. Pelt and K. Joost Batenburg", "abstract": "  Computed Tomography (CT) using synchrotron radiation is a powerful technique\nthat, compared to lab-CT techniques, boosts high spatial and temporal\nresolution while also providing access to a range of contrast-formation\nmechanisms. The acquired projection data is typically processed by a\ncomputational pipeline composed of multiple stages. Artifacts introduced during\ndata acquisition can propagate through the pipeline, and degrade image quality\nin the reconstructed images. Recently, deep learning has shown significant\npromise in enhancing image quality for images representing scientific data.\nThis success has driven increasing adoption of deep learning techniques in CT\nimaging. Various approaches have been proposed to incorporate deep learning\ninto computational pipelines, but each has limitations in addressing artifacts\neffectively and efficiently in synchrotron CT, either in properly addressing\nthe specific artifacts, or in computational efficiency.\n  Recognizing these challenges, we introduce a novel method that incorporates\nseparate deep learning models at each stage of the tomography\npipeline-projection, sinogram, and reconstruction-to address specific artifacts\nlocally in a data-driven way. Our approach includes bypass connections that\nfeed both the outputs from previous stages and raw data to subsequent stages,\nminimizing the risk of error propagation. Extensive evaluations on both\nsimulated and real-world datasets illustrate that our approach effectively\nreduces artifacts and outperforms comparison methods.\n", "link": "http://arxiv.org/abs/2309.00494v2", "date": "2025-01-17", "relevancy": 2.8114, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5701}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5701}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-stage%20Deep%20Learning%20Artifact%20Reduction%20for%20Pallel-beam%20Computed%0A%20%20Tomography&body=Title%3A%20Multi-stage%20Deep%20Learning%20Artifact%20Reduction%20for%20Pallel-beam%20Computed%0A%20%20Tomography%0AAuthor%3A%20Jiayang%20Shi%20and%20Daniel%20M.%20Pelt%20and%20K.%20Joost%20Batenburg%0AAbstract%3A%20%20%20Computed%20Tomography%20%28CT%29%20using%20synchrotron%20radiation%20is%20a%20powerful%20technique%0Athat%2C%20compared%20to%20lab-CT%20techniques%2C%20boosts%20high%20spatial%20and%20temporal%0Aresolution%20while%20also%20providing%20access%20to%20a%20range%20of%20contrast-formation%0Amechanisms.%20The%20acquired%20projection%20data%20is%20typically%20processed%20by%20a%0Acomputational%20pipeline%20composed%20of%20multiple%20stages.%20Artifacts%20introduced%20during%0Adata%20acquisition%20can%20propagate%20through%20the%20pipeline%2C%20and%20degrade%20image%20quality%0Ain%20the%20reconstructed%20images.%20Recently%2C%20deep%20learning%20has%20shown%20significant%0Apromise%20in%20enhancing%20image%20quality%20for%20images%20representing%20scientific%20data.%0AThis%20success%20has%20driven%20increasing%20adoption%20of%20deep%20learning%20techniques%20in%20CT%0Aimaging.%20Various%20approaches%20have%20been%20proposed%20to%20incorporate%20deep%20learning%0Ainto%20computational%20pipelines%2C%20but%20each%20has%20limitations%20in%20addressing%20artifacts%0Aeffectively%20and%20efficiently%20in%20synchrotron%20CT%2C%20either%20in%20properly%20addressing%0Athe%20specific%20artifacts%2C%20or%20in%20computational%20efficiency.%0A%20%20Recognizing%20these%20challenges%2C%20we%20introduce%20a%20novel%20method%20that%20incorporates%0Aseparate%20deep%20learning%20models%20at%20each%20stage%20of%20the%20tomography%0Apipeline-projection%2C%20sinogram%2C%20and%20reconstruction-to%20address%20specific%20artifacts%0Alocally%20in%20a%20data-driven%20way.%20Our%20approach%20includes%20bypass%20connections%20that%0Afeed%20both%20the%20outputs%20from%20previous%20stages%20and%20raw%20data%20to%20subsequent%20stages%2C%0Aminimizing%20the%20risk%20of%20error%20propagation.%20Extensive%20evaluations%20on%20both%0Asimulated%20and%20real-world%20datasets%20illustrate%20that%20our%20approach%20effectively%0Areduces%20artifacts%20and%20outperforms%20comparison%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.00494v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-stage%2520Deep%2520Learning%2520Artifact%2520Reduction%2520for%2520Pallel-beam%2520Computed%250A%2520%2520Tomography%26entry.906535625%3DJiayang%2520Shi%2520and%2520Daniel%2520M.%2520Pelt%2520and%2520K.%2520Joost%2520Batenburg%26entry.1292438233%3D%2520%2520Computed%2520Tomography%2520%2528CT%2529%2520using%2520synchrotron%2520radiation%2520is%2520a%2520powerful%2520technique%250Athat%252C%2520compared%2520to%2520lab-CT%2520techniques%252C%2520boosts%2520high%2520spatial%2520and%2520temporal%250Aresolution%2520while%2520also%2520providing%2520access%2520to%2520a%2520range%2520of%2520contrast-formation%250Amechanisms.%2520The%2520acquired%2520projection%2520data%2520is%2520typically%2520processed%2520by%2520a%250Acomputational%2520pipeline%2520composed%2520of%2520multiple%2520stages.%2520Artifacts%2520introduced%2520during%250Adata%2520acquisition%2520can%2520propagate%2520through%2520the%2520pipeline%252C%2520and%2520degrade%2520image%2520quality%250Ain%2520the%2520reconstructed%2520images.%2520Recently%252C%2520deep%2520learning%2520has%2520shown%2520significant%250Apromise%2520in%2520enhancing%2520image%2520quality%2520for%2520images%2520representing%2520scientific%2520data.%250AThis%2520success%2520has%2520driven%2520increasing%2520adoption%2520of%2520deep%2520learning%2520techniques%2520in%2520CT%250Aimaging.%2520Various%2520approaches%2520have%2520been%2520proposed%2520to%2520incorporate%2520deep%2520learning%250Ainto%2520computational%2520pipelines%252C%2520but%2520each%2520has%2520limitations%2520in%2520addressing%2520artifacts%250Aeffectively%2520and%2520efficiently%2520in%2520synchrotron%2520CT%252C%2520either%2520in%2520properly%2520addressing%250Athe%2520specific%2520artifacts%252C%2520or%2520in%2520computational%2520efficiency.%250A%2520%2520Recognizing%2520these%2520challenges%252C%2520we%2520introduce%2520a%2520novel%2520method%2520that%2520incorporates%250Aseparate%2520deep%2520learning%2520models%2520at%2520each%2520stage%2520of%2520the%2520tomography%250Apipeline-projection%252C%2520sinogram%252C%2520and%2520reconstruction-to%2520address%2520specific%2520artifacts%250Alocally%2520in%2520a%2520data-driven%2520way.%2520Our%2520approach%2520includes%2520bypass%2520connections%2520that%250Afeed%2520both%2520the%2520outputs%2520from%2520previous%2520stages%2520and%2520raw%2520data%2520to%2520subsequent%2520stages%252C%250Aminimizing%2520the%2520risk%2520of%2520error%2520propagation.%2520Extensive%2520evaluations%2520on%2520both%250Asimulated%2520and%2520real-world%2520datasets%2520illustrate%2520that%2520our%2520approach%2520effectively%250Areduces%2520artifacts%2520and%2520outperforms%2520comparison%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.00494v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-stage%20Deep%20Learning%20Artifact%20Reduction%20for%20Pallel-beam%20Computed%0A%20%20Tomography&entry.906535625=Jiayang%20Shi%20and%20Daniel%20M.%20Pelt%20and%20K.%20Joost%20Batenburg&entry.1292438233=%20%20Computed%20Tomography%20%28CT%29%20using%20synchrotron%20radiation%20is%20a%20powerful%20technique%0Athat%2C%20compared%20to%20lab-CT%20techniques%2C%20boosts%20high%20spatial%20and%20temporal%0Aresolution%20while%20also%20providing%20access%20to%20a%20range%20of%20contrast-formation%0Amechanisms.%20The%20acquired%20projection%20data%20is%20typically%20processed%20by%20a%0Acomputational%20pipeline%20composed%20of%20multiple%20stages.%20Artifacts%20introduced%20during%0Adata%20acquisition%20can%20propagate%20through%20the%20pipeline%2C%20and%20degrade%20image%20quality%0Ain%20the%20reconstructed%20images.%20Recently%2C%20deep%20learning%20has%20shown%20significant%0Apromise%20in%20enhancing%20image%20quality%20for%20images%20representing%20scientific%20data.%0AThis%20success%20has%20driven%20increasing%20adoption%20of%20deep%20learning%20techniques%20in%20CT%0Aimaging.%20Various%20approaches%20have%20been%20proposed%20to%20incorporate%20deep%20learning%0Ainto%20computational%20pipelines%2C%20but%20each%20has%20limitations%20in%20addressing%20artifacts%0Aeffectively%20and%20efficiently%20in%20synchrotron%20CT%2C%20either%20in%20properly%20addressing%0Athe%20specific%20artifacts%2C%20or%20in%20computational%20efficiency.%0A%20%20Recognizing%20these%20challenges%2C%20we%20introduce%20a%20novel%20method%20that%20incorporates%0Aseparate%20deep%20learning%20models%20at%20each%20stage%20of%20the%20tomography%0Apipeline-projection%2C%20sinogram%2C%20and%20reconstruction-to%20address%20specific%20artifacts%0Alocally%20in%20a%20data-driven%20way.%20Our%20approach%20includes%20bypass%20connections%20that%0Afeed%20both%20the%20outputs%20from%20previous%20stages%20and%20raw%20data%20to%20subsequent%20stages%2C%0Aminimizing%20the%20risk%20of%20error%20propagation.%20Extensive%20evaluations%20on%20both%0Asimulated%20and%20real-world%20datasets%20illustrate%20that%20our%20approach%20effectively%0Areduces%20artifacts%20and%20outperforms%20comparison%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.00494v2&entry.124074799=Read"},
{"title": "ACE: Anatomically Consistent Embeddings in Composition and Decomposition", "author": "Ziyu Zhou and Haozhe Luo and Mohammad Reza Hosseinzadeh Taher and Jiaxuan Pang and Xiaowei Ding and Michael Gotway and Jianming Liang", "abstract": "  Medical images acquired from standardized protocols show consistent\nmacroscopic or microscopic anatomical structures, and these structures consist\nof composable/decomposable organs and tissues, but existing self-supervised\nlearning (SSL) methods do not appreciate such composable/decomposable structure\nattributes inherent to medical images. To overcome this limitation, this paper\nintroduces a novel SSL approach called ACE to learn anatomically consistent\nembedding via composition and decomposition with two key branches: (1) global\nconsistency, capturing discriminative macro-structures via extracting global\nfeatures; (2) local consistency, learning fine-grained anatomical details from\ncomposable/decomposable patch features via corresponding matrix matching.\nExperimental results across 6 datasets 2 backbones, evaluated in few-shot\nlearning, fine-tuning, and property analysis, show ACE's superior robustness,\ntransferability, and clinical potential. The innovations of our ACE lie in\ngrid-wise image cropping, leveraging the intrinsic properties of\ncompositionality and decompositionality of medical images, bridging the\nsemantic gap from high-level pathologies to low-level tissue anomalies, and\nproviding a new SSL method for medical imaging.\n", "link": "http://arxiv.org/abs/2501.10131v1", "date": "2025-01-17", "relevancy": 2.7371, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5662}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5628}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ACE%3A%20Anatomically%20Consistent%20Embeddings%20in%20Composition%20and%20Decomposition&body=Title%3A%20ACE%3A%20Anatomically%20Consistent%20Embeddings%20in%20Composition%20and%20Decomposition%0AAuthor%3A%20Ziyu%20Zhou%20and%20Haozhe%20Luo%20and%20Mohammad%20Reza%20Hosseinzadeh%20Taher%20and%20Jiaxuan%20Pang%20and%20Xiaowei%20Ding%20and%20Michael%20Gotway%20and%20Jianming%20Liang%0AAbstract%3A%20%20%20Medical%20images%20acquired%20from%20standardized%20protocols%20show%20consistent%0Amacroscopic%20or%20microscopic%20anatomical%20structures%2C%20and%20these%20structures%20consist%0Aof%20composable/decomposable%20organs%20and%20tissues%2C%20but%20existing%20self-supervised%0Alearning%20%28SSL%29%20methods%20do%20not%20appreciate%20such%20composable/decomposable%20structure%0Aattributes%20inherent%20to%20medical%20images.%20To%20overcome%20this%20limitation%2C%20this%20paper%0Aintroduces%20a%20novel%20SSL%20approach%20called%20ACE%20to%20learn%20anatomically%20consistent%0Aembedding%20via%20composition%20and%20decomposition%20with%20two%20key%20branches%3A%20%281%29%20global%0Aconsistency%2C%20capturing%20discriminative%20macro-structures%20via%20extracting%20global%0Afeatures%3B%20%282%29%20local%20consistency%2C%20learning%20fine-grained%20anatomical%20details%20from%0Acomposable/decomposable%20patch%20features%20via%20corresponding%20matrix%20matching.%0AExperimental%20results%20across%206%20datasets%202%20backbones%2C%20evaluated%20in%20few-shot%0Alearning%2C%20fine-tuning%2C%20and%20property%20analysis%2C%20show%20ACE%27s%20superior%20robustness%2C%0Atransferability%2C%20and%20clinical%20potential.%20The%20innovations%20of%20our%20ACE%20lie%20in%0Agrid-wise%20image%20cropping%2C%20leveraging%20the%20intrinsic%20properties%20of%0Acompositionality%20and%20decompositionality%20of%20medical%20images%2C%20bridging%20the%0Asemantic%20gap%20from%20high-level%20pathologies%20to%20low-level%20tissue%20anomalies%2C%20and%0Aproviding%20a%20new%20SSL%20method%20for%20medical%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10131v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DACE%253A%2520Anatomically%2520Consistent%2520Embeddings%2520in%2520Composition%2520and%2520Decomposition%26entry.906535625%3DZiyu%2520Zhou%2520and%2520Haozhe%2520Luo%2520and%2520Mohammad%2520Reza%2520Hosseinzadeh%2520Taher%2520and%2520Jiaxuan%2520Pang%2520and%2520Xiaowei%2520Ding%2520and%2520Michael%2520Gotway%2520and%2520Jianming%2520Liang%26entry.1292438233%3D%2520%2520Medical%2520images%2520acquired%2520from%2520standardized%2520protocols%2520show%2520consistent%250Amacroscopic%2520or%2520microscopic%2520anatomical%2520structures%252C%2520and%2520these%2520structures%2520consist%250Aof%2520composable/decomposable%2520organs%2520and%2520tissues%252C%2520but%2520existing%2520self-supervised%250Alearning%2520%2528SSL%2529%2520methods%2520do%2520not%2520appreciate%2520such%2520composable/decomposable%2520structure%250Aattributes%2520inherent%2520to%2520medical%2520images.%2520To%2520overcome%2520this%2520limitation%252C%2520this%2520paper%250Aintroduces%2520a%2520novel%2520SSL%2520approach%2520called%2520ACE%2520to%2520learn%2520anatomically%2520consistent%250Aembedding%2520via%2520composition%2520and%2520decomposition%2520with%2520two%2520key%2520branches%253A%2520%25281%2529%2520global%250Aconsistency%252C%2520capturing%2520discriminative%2520macro-structures%2520via%2520extracting%2520global%250Afeatures%253B%2520%25282%2529%2520local%2520consistency%252C%2520learning%2520fine-grained%2520anatomical%2520details%2520from%250Acomposable/decomposable%2520patch%2520features%2520via%2520corresponding%2520matrix%2520matching.%250AExperimental%2520results%2520across%25206%2520datasets%25202%2520backbones%252C%2520evaluated%2520in%2520few-shot%250Alearning%252C%2520fine-tuning%252C%2520and%2520property%2520analysis%252C%2520show%2520ACE%2527s%2520superior%2520robustness%252C%250Atransferability%252C%2520and%2520clinical%2520potential.%2520The%2520innovations%2520of%2520our%2520ACE%2520lie%2520in%250Agrid-wise%2520image%2520cropping%252C%2520leveraging%2520the%2520intrinsic%2520properties%2520of%250Acompositionality%2520and%2520decompositionality%2520of%2520medical%2520images%252C%2520bridging%2520the%250Asemantic%2520gap%2520from%2520high-level%2520pathologies%2520to%2520low-level%2520tissue%2520anomalies%252C%2520and%250Aproviding%2520a%2520new%2520SSL%2520method%2520for%2520medical%2520imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10131v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ACE%3A%20Anatomically%20Consistent%20Embeddings%20in%20Composition%20and%20Decomposition&entry.906535625=Ziyu%20Zhou%20and%20Haozhe%20Luo%20and%20Mohammad%20Reza%20Hosseinzadeh%20Taher%20and%20Jiaxuan%20Pang%20and%20Xiaowei%20Ding%20and%20Michael%20Gotway%20and%20Jianming%20Liang&entry.1292438233=%20%20Medical%20images%20acquired%20from%20standardized%20protocols%20show%20consistent%0Amacroscopic%20or%20microscopic%20anatomical%20structures%2C%20and%20these%20structures%20consist%0Aof%20composable/decomposable%20organs%20and%20tissues%2C%20but%20existing%20self-supervised%0Alearning%20%28SSL%29%20methods%20do%20not%20appreciate%20such%20composable/decomposable%20structure%0Aattributes%20inherent%20to%20medical%20images.%20To%20overcome%20this%20limitation%2C%20this%20paper%0Aintroduces%20a%20novel%20SSL%20approach%20called%20ACE%20to%20learn%20anatomically%20consistent%0Aembedding%20via%20composition%20and%20decomposition%20with%20two%20key%20branches%3A%20%281%29%20global%0Aconsistency%2C%20capturing%20discriminative%20macro-structures%20via%20extracting%20global%0Afeatures%3B%20%282%29%20local%20consistency%2C%20learning%20fine-grained%20anatomical%20details%20from%0Acomposable/decomposable%20patch%20features%20via%20corresponding%20matrix%20matching.%0AExperimental%20results%20across%206%20datasets%202%20backbones%2C%20evaluated%20in%20few-shot%0Alearning%2C%20fine-tuning%2C%20and%20property%20analysis%2C%20show%20ACE%27s%20superior%20robustness%2C%0Atransferability%2C%20and%20clinical%20potential.%20The%20innovations%20of%20our%20ACE%20lie%20in%0Agrid-wise%20image%20cropping%2C%20leveraging%20the%20intrinsic%20properties%20of%0Acompositionality%20and%20decompositionality%20of%20medical%20images%2C%20bridging%20the%0Asemantic%20gap%20from%20high-level%20pathologies%20to%20low-level%20tissue%20anomalies%2C%20and%0Aproviding%20a%20new%20SSL%20method%20for%20medical%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10131v1&entry.124074799=Read"},
{"title": "Few-shot Structure-Informed Machinery Part Segmentation with Foundation\n  Models and Graph Neural Networks", "author": "Michael Schwingshackl and Fabio Francisco Oberweger and Markus Murschitz", "abstract": "  This paper proposes a novel approach to few-shot semantic segmentation for\nmachinery with multiple parts that exhibit spatial and hierarchical\nrelationships. Our method integrates the foundation models CLIPSeg and Segment\nAnything Model (SAM) with the interest point detector SuperPoint and a graph\nconvolutional network (GCN) to accurately segment machinery parts. By providing\n1 to 25 annotated samples, our model, evaluated on a purely synthetic dataset\ndepicting a truck-mounted loading crane, achieves effective segmentation across\nvarious levels of detail. Training times are kept under five minutes on\nconsumer GPUs. The model demonstrates robust generalization to real data,\nachieving a qualitative synthetic-to-real generalization with a $J\\&F$ score of\n92.2 on real data using 10 synthetic support samples. When benchmarked on the\nDAVIS 2017 dataset, it achieves a $J\\&F$ score of 71.5 in semi-supervised video\nsegmentation with three support samples. This method's fast training times and\neffective generalization to real data make it a valuable tool for autonomous\nsystems interacting with machinery and infrastructure, and illustrate the\npotential of combined and orchestrated foundation models for few-shot\nsegmentation tasks.\n", "link": "http://arxiv.org/abs/2501.10080v1", "date": "2025-01-17", "relevancy": 2.7307, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5484}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.545}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Few-shot%20Structure-Informed%20Machinery%20Part%20Segmentation%20with%20Foundation%0A%20%20Models%20and%20Graph%20Neural%20Networks&body=Title%3A%20Few-shot%20Structure-Informed%20Machinery%20Part%20Segmentation%20with%20Foundation%0A%20%20Models%20and%20Graph%20Neural%20Networks%0AAuthor%3A%20Michael%20Schwingshackl%20and%20Fabio%20Francisco%20Oberweger%20and%20Markus%20Murschitz%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20novel%20approach%20to%20few-shot%20semantic%20segmentation%20for%0Amachinery%20with%20multiple%20parts%20that%20exhibit%20spatial%20and%20hierarchical%0Arelationships.%20Our%20method%20integrates%20the%20foundation%20models%20CLIPSeg%20and%20Segment%0AAnything%20Model%20%28SAM%29%20with%20the%20interest%20point%20detector%20SuperPoint%20and%20a%20graph%0Aconvolutional%20network%20%28GCN%29%20to%20accurately%20segment%20machinery%20parts.%20By%20providing%0A1%20to%2025%20annotated%20samples%2C%20our%20model%2C%20evaluated%20on%20a%20purely%20synthetic%20dataset%0Adepicting%20a%20truck-mounted%20loading%20crane%2C%20achieves%20effective%20segmentation%20across%0Avarious%20levels%20of%20detail.%20Training%20times%20are%20kept%20under%20five%20minutes%20on%0Aconsumer%20GPUs.%20The%20model%20demonstrates%20robust%20generalization%20to%20real%20data%2C%0Aachieving%20a%20qualitative%20synthetic-to-real%20generalization%20with%20a%20%24J%5C%26F%24%20score%20of%0A92.2%20on%20real%20data%20using%2010%20synthetic%20support%20samples.%20When%20benchmarked%20on%20the%0ADAVIS%202017%20dataset%2C%20it%20achieves%20a%20%24J%5C%26F%24%20score%20of%2071.5%20in%20semi-supervised%20video%0Asegmentation%20with%20three%20support%20samples.%20This%20method%27s%20fast%20training%20times%20and%0Aeffective%20generalization%20to%20real%20data%20make%20it%20a%20valuable%20tool%20for%20autonomous%0Asystems%20interacting%20with%20machinery%20and%20infrastructure%2C%20and%20illustrate%20the%0Apotential%20of%20combined%20and%20orchestrated%20foundation%20models%20for%20few-shot%0Asegmentation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10080v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFew-shot%2520Structure-Informed%2520Machinery%2520Part%2520Segmentation%2520with%2520Foundation%250A%2520%2520Models%2520and%2520Graph%2520Neural%2520Networks%26entry.906535625%3DMichael%2520Schwingshackl%2520and%2520Fabio%2520Francisco%2520Oberweger%2520and%2520Markus%2520Murschitz%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520novel%2520approach%2520to%2520few-shot%2520semantic%2520segmentation%2520for%250Amachinery%2520with%2520multiple%2520parts%2520that%2520exhibit%2520spatial%2520and%2520hierarchical%250Arelationships.%2520Our%2520method%2520integrates%2520the%2520foundation%2520models%2520CLIPSeg%2520and%2520Segment%250AAnything%2520Model%2520%2528SAM%2529%2520with%2520the%2520interest%2520point%2520detector%2520SuperPoint%2520and%2520a%2520graph%250Aconvolutional%2520network%2520%2528GCN%2529%2520to%2520accurately%2520segment%2520machinery%2520parts.%2520By%2520providing%250A1%2520to%252025%2520annotated%2520samples%252C%2520our%2520model%252C%2520evaluated%2520on%2520a%2520purely%2520synthetic%2520dataset%250Adepicting%2520a%2520truck-mounted%2520loading%2520crane%252C%2520achieves%2520effective%2520segmentation%2520across%250Avarious%2520levels%2520of%2520detail.%2520Training%2520times%2520are%2520kept%2520under%2520five%2520minutes%2520on%250Aconsumer%2520GPUs.%2520The%2520model%2520demonstrates%2520robust%2520generalization%2520to%2520real%2520data%252C%250Aachieving%2520a%2520qualitative%2520synthetic-to-real%2520generalization%2520with%2520a%2520%2524J%255C%2526F%2524%2520score%2520of%250A92.2%2520on%2520real%2520data%2520using%252010%2520synthetic%2520support%2520samples.%2520When%2520benchmarked%2520on%2520the%250ADAVIS%25202017%2520dataset%252C%2520it%2520achieves%2520a%2520%2524J%255C%2526F%2524%2520score%2520of%252071.5%2520in%2520semi-supervised%2520video%250Asegmentation%2520with%2520three%2520support%2520samples.%2520This%2520method%2527s%2520fast%2520training%2520times%2520and%250Aeffective%2520generalization%2520to%2520real%2520data%2520make%2520it%2520a%2520valuable%2520tool%2520for%2520autonomous%250Asystems%2520interacting%2520with%2520machinery%2520and%2520infrastructure%252C%2520and%2520illustrate%2520the%250Apotential%2520of%2520combined%2520and%2520orchestrated%2520foundation%2520models%2520for%2520few-shot%250Asegmentation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10080v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Few-shot%20Structure-Informed%20Machinery%20Part%20Segmentation%20with%20Foundation%0A%20%20Models%20and%20Graph%20Neural%20Networks&entry.906535625=Michael%20Schwingshackl%20and%20Fabio%20Francisco%20Oberweger%20and%20Markus%20Murschitz&entry.1292438233=%20%20This%20paper%20proposes%20a%20novel%20approach%20to%20few-shot%20semantic%20segmentation%20for%0Amachinery%20with%20multiple%20parts%20that%20exhibit%20spatial%20and%20hierarchical%0Arelationships.%20Our%20method%20integrates%20the%20foundation%20models%20CLIPSeg%20and%20Segment%0AAnything%20Model%20%28SAM%29%20with%20the%20interest%20point%20detector%20SuperPoint%20and%20a%20graph%0Aconvolutional%20network%20%28GCN%29%20to%20accurately%20segment%20machinery%20parts.%20By%20providing%0A1%20to%2025%20annotated%20samples%2C%20our%20model%2C%20evaluated%20on%20a%20purely%20synthetic%20dataset%0Adepicting%20a%20truck-mounted%20loading%20crane%2C%20achieves%20effective%20segmentation%20across%0Avarious%20levels%20of%20detail.%20Training%20times%20are%20kept%20under%20five%20minutes%20on%0Aconsumer%20GPUs.%20The%20model%20demonstrates%20robust%20generalization%20to%20real%20data%2C%0Aachieving%20a%20qualitative%20synthetic-to-real%20generalization%20with%20a%20%24J%5C%26F%24%20score%20of%0A92.2%20on%20real%20data%20using%2010%20synthetic%20support%20samples.%20When%20benchmarked%20on%20the%0ADAVIS%202017%20dataset%2C%20it%20achieves%20a%20%24J%5C%26F%24%20score%20of%2071.5%20in%20semi-supervised%20video%0Asegmentation%20with%20three%20support%20samples.%20This%20method%27s%20fast%20training%20times%20and%0Aeffective%20generalization%20to%20real%20data%20make%20it%20a%20valuable%20tool%20for%20autonomous%0Asystems%20interacting%20with%20machinery%20and%20infrastructure%2C%20and%20illustrate%20the%0Apotential%20of%20combined%20and%20orchestrated%20foundation%20models%20for%20few-shot%0Asegmentation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10080v1&entry.124074799=Read"},
{"title": "Enhancing UAV Path Planning Efficiency Through Accelerated Learning", "author": "Joseanne Viana and Boris Galkin and Lester Ho and Holger Claussen", "abstract": "  Unmanned Aerial Vehicles (UAVs) are increasingly essential in various fields\nsuch as surveillance, reconnaissance, and telecommunications. This study aims\nto develop a learning algorithm for the path planning of UAV wireless\ncommunication relays, which can reduce storage requirements and accelerate Deep\nReinforcement Learning (DRL) convergence. Assuming the system possesses terrain\nmaps of the area and can estimate user locations using localization algorithms\nor direct GPS reporting, it can input these parameters into the learning\nalgorithms to achieve optimized path planning performance. However, higher\nresolution terrain maps are necessary to extract topological information such\nas terrain height, object distances, and signal blockages. This requirement\nincreases memory and storage demands on UAVs while also lengthening convergence\ntimes in DRL algorithms. Similarly, defining the telecommunication coverage map\nin UAV wireless communication relays using these terrain maps and user position\nestimations demands higher memory and storage utilization for the learning path\nplanning algorithms. Our approach reduces path planning training time by\napplying a dimensionality reduction technique based on Principal Component\nAnalysis (PCA), sample combination, Prioritized Experience Replay (PER), and\nthe combination of Mean Squared Error (MSE) and Mean Absolute Error (MAE) loss\ncalculations in the coverage map estimates, thereby enhancing a Twin Delayed\nDeep Deterministic Policy Gradient (TD3) algorithm. The proposed solution\nreduces the convergence episodes needed for basic training by approximately\nfour times compared to the traditional TD3.\n", "link": "http://arxiv.org/abs/2501.10141v1", "date": "2025-01-17", "relevancy": 2.6525, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5822}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5089}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5005}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20UAV%20Path%20Planning%20Efficiency%20Through%20Accelerated%20Learning&body=Title%3A%20Enhancing%20UAV%20Path%20Planning%20Efficiency%20Through%20Accelerated%20Learning%0AAuthor%3A%20Joseanne%20Viana%20and%20Boris%20Galkin%20and%20Lester%20Ho%20and%20Holger%20Claussen%0AAbstract%3A%20%20%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20are%20increasingly%20essential%20in%20various%20fields%0Asuch%20as%20surveillance%2C%20reconnaissance%2C%20and%20telecommunications.%20This%20study%20aims%0Ato%20develop%20a%20learning%20algorithm%20for%20the%20path%20planning%20of%20UAV%20wireless%0Acommunication%20relays%2C%20which%20can%20reduce%20storage%20requirements%20and%20accelerate%20Deep%0AReinforcement%20Learning%20%28DRL%29%20convergence.%20Assuming%20the%20system%20possesses%20terrain%0Amaps%20of%20the%20area%20and%20can%20estimate%20user%20locations%20using%20localization%20algorithms%0Aor%20direct%20GPS%20reporting%2C%20it%20can%20input%20these%20parameters%20into%20the%20learning%0Aalgorithms%20to%20achieve%20optimized%20path%20planning%20performance.%20However%2C%20higher%0Aresolution%20terrain%20maps%20are%20necessary%20to%20extract%20topological%20information%20such%0Aas%20terrain%20height%2C%20object%20distances%2C%20and%20signal%20blockages.%20This%20requirement%0Aincreases%20memory%20and%20storage%20demands%20on%20UAVs%20while%20also%20lengthening%20convergence%0Atimes%20in%20DRL%20algorithms.%20Similarly%2C%20defining%20the%20telecommunication%20coverage%20map%0Ain%20UAV%20wireless%20communication%20relays%20using%20these%20terrain%20maps%20and%20user%20position%0Aestimations%20demands%20higher%20memory%20and%20storage%20utilization%20for%20the%20learning%20path%0Aplanning%20algorithms.%20Our%20approach%20reduces%20path%20planning%20training%20time%20by%0Aapplying%20a%20dimensionality%20reduction%20technique%20based%20on%20Principal%20Component%0AAnalysis%20%28PCA%29%2C%20sample%20combination%2C%20Prioritized%20Experience%20Replay%20%28PER%29%2C%20and%0Athe%20combination%20of%20Mean%20Squared%20Error%20%28MSE%29%20and%20Mean%20Absolute%20Error%20%28MAE%29%20loss%0Acalculations%20in%20the%20coverage%20map%20estimates%2C%20thereby%20enhancing%20a%20Twin%20Delayed%0ADeep%20Deterministic%20Policy%20Gradient%20%28TD3%29%20algorithm.%20The%20proposed%20solution%0Areduces%20the%20convergence%20episodes%20needed%20for%20basic%20training%20by%20approximately%0Afour%20times%20compared%20to%20the%20traditional%20TD3.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10141v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520UAV%2520Path%2520Planning%2520Efficiency%2520Through%2520Accelerated%2520Learning%26entry.906535625%3DJoseanne%2520Viana%2520and%2520Boris%2520Galkin%2520and%2520Lester%2520Ho%2520and%2520Holger%2520Claussen%26entry.1292438233%3D%2520%2520Unmanned%2520Aerial%2520Vehicles%2520%2528UAVs%2529%2520are%2520increasingly%2520essential%2520in%2520various%2520fields%250Asuch%2520as%2520surveillance%252C%2520reconnaissance%252C%2520and%2520telecommunications.%2520This%2520study%2520aims%250Ato%2520develop%2520a%2520learning%2520algorithm%2520for%2520the%2520path%2520planning%2520of%2520UAV%2520wireless%250Acommunication%2520relays%252C%2520which%2520can%2520reduce%2520storage%2520requirements%2520and%2520accelerate%2520Deep%250AReinforcement%2520Learning%2520%2528DRL%2529%2520convergence.%2520Assuming%2520the%2520system%2520possesses%2520terrain%250Amaps%2520of%2520the%2520area%2520and%2520can%2520estimate%2520user%2520locations%2520using%2520localization%2520algorithms%250Aor%2520direct%2520GPS%2520reporting%252C%2520it%2520can%2520input%2520these%2520parameters%2520into%2520the%2520learning%250Aalgorithms%2520to%2520achieve%2520optimized%2520path%2520planning%2520performance.%2520However%252C%2520higher%250Aresolution%2520terrain%2520maps%2520are%2520necessary%2520to%2520extract%2520topological%2520information%2520such%250Aas%2520terrain%2520height%252C%2520object%2520distances%252C%2520and%2520signal%2520blockages.%2520This%2520requirement%250Aincreases%2520memory%2520and%2520storage%2520demands%2520on%2520UAVs%2520while%2520also%2520lengthening%2520convergence%250Atimes%2520in%2520DRL%2520algorithms.%2520Similarly%252C%2520defining%2520the%2520telecommunication%2520coverage%2520map%250Ain%2520UAV%2520wireless%2520communication%2520relays%2520using%2520these%2520terrain%2520maps%2520and%2520user%2520position%250Aestimations%2520demands%2520higher%2520memory%2520and%2520storage%2520utilization%2520for%2520the%2520learning%2520path%250Aplanning%2520algorithms.%2520Our%2520approach%2520reduces%2520path%2520planning%2520training%2520time%2520by%250Aapplying%2520a%2520dimensionality%2520reduction%2520technique%2520based%2520on%2520Principal%2520Component%250AAnalysis%2520%2528PCA%2529%252C%2520sample%2520combination%252C%2520Prioritized%2520Experience%2520Replay%2520%2528PER%2529%252C%2520and%250Athe%2520combination%2520of%2520Mean%2520Squared%2520Error%2520%2528MSE%2529%2520and%2520Mean%2520Absolute%2520Error%2520%2528MAE%2529%2520loss%250Acalculations%2520in%2520the%2520coverage%2520map%2520estimates%252C%2520thereby%2520enhancing%2520a%2520Twin%2520Delayed%250ADeep%2520Deterministic%2520Policy%2520Gradient%2520%2528TD3%2529%2520algorithm.%2520The%2520proposed%2520solution%250Areduces%2520the%2520convergence%2520episodes%2520needed%2520for%2520basic%2520training%2520by%2520approximately%250Afour%2520times%2520compared%2520to%2520the%2520traditional%2520TD3.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10141v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20UAV%20Path%20Planning%20Efficiency%20Through%20Accelerated%20Learning&entry.906535625=Joseanne%20Viana%20and%20Boris%20Galkin%20and%20Lester%20Ho%20and%20Holger%20Claussen&entry.1292438233=%20%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20are%20increasingly%20essential%20in%20various%20fields%0Asuch%20as%20surveillance%2C%20reconnaissance%2C%20and%20telecommunications.%20This%20study%20aims%0Ato%20develop%20a%20learning%20algorithm%20for%20the%20path%20planning%20of%20UAV%20wireless%0Acommunication%20relays%2C%20which%20can%20reduce%20storage%20requirements%20and%20accelerate%20Deep%0AReinforcement%20Learning%20%28DRL%29%20convergence.%20Assuming%20the%20system%20possesses%20terrain%0Amaps%20of%20the%20area%20and%20can%20estimate%20user%20locations%20using%20localization%20algorithms%0Aor%20direct%20GPS%20reporting%2C%20it%20can%20input%20these%20parameters%20into%20the%20learning%0Aalgorithms%20to%20achieve%20optimized%20path%20planning%20performance.%20However%2C%20higher%0Aresolution%20terrain%20maps%20are%20necessary%20to%20extract%20topological%20information%20such%0Aas%20terrain%20height%2C%20object%20distances%2C%20and%20signal%20blockages.%20This%20requirement%0Aincreases%20memory%20and%20storage%20demands%20on%20UAVs%20while%20also%20lengthening%20convergence%0Atimes%20in%20DRL%20algorithms.%20Similarly%2C%20defining%20the%20telecommunication%20coverage%20map%0Ain%20UAV%20wireless%20communication%20relays%20using%20these%20terrain%20maps%20and%20user%20position%0Aestimations%20demands%20higher%20memory%20and%20storage%20utilization%20for%20the%20learning%20path%0Aplanning%20algorithms.%20Our%20approach%20reduces%20path%20planning%20training%20time%20by%0Aapplying%20a%20dimensionality%20reduction%20technique%20based%20on%20Principal%20Component%0AAnalysis%20%28PCA%29%2C%20sample%20combination%2C%20Prioritized%20Experience%20Replay%20%28PER%29%2C%20and%0Athe%20combination%20of%20Mean%20Squared%20Error%20%28MSE%29%20and%20Mean%20Absolute%20Error%20%28MAE%29%20loss%0Acalculations%20in%20the%20coverage%20map%20estimates%2C%20thereby%20enhancing%20a%20Twin%20Delayed%0ADeep%20Deterministic%20Policy%20Gradient%20%28TD3%29%20algorithm.%20The%20proposed%20solution%0Areduces%20the%20convergence%20episodes%20needed%20for%20basic%20training%20by%20approximately%0Afour%20times%20compared%20to%20the%20traditional%20TD3.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10141v1&entry.124074799=Read"},
{"title": "landmarker: a Toolkit for Anatomical Landmark Localization in 2D/3D\n  Images", "author": "Jef Jonkers and Luc Duchateau and Glenn Van Wallendael and Sofie Van Hoecke", "abstract": "  Anatomical landmark localization in 2D/3D images is a critical task in\nmedical imaging. Although many general-purpose tools exist for landmark\nlocalization in classical computer vision tasks, such as pose estimation, they\nlack the specialized features and modularity necessary for anatomical landmark\nlocalization applications in the medical domain. Therefore, we introduce\nlandmarker, a Python package built on PyTorch. The package provides a\ncomprehensive, flexible toolkit for developing and evaluating landmark\nlocalization algorithms, supporting a range of methodologies, including static\nand adaptive heatmap regression. landmarker enhances the accuracy of landmark\nidentification, streamlines research and development processes, and supports\nvarious image formats and preprocessing pipelines. Its modular design allows\nusers to customize and extend the toolkit for specific datasets and\napplications, accelerating innovation in medical imaging. landmarker addresses\na critical need for precision and customization in landmark localization tasks\nnot adequately met by existing general-purpose pose estimation tools.\n", "link": "http://arxiv.org/abs/2501.10098v1", "date": "2025-01-17", "relevancy": 2.6509, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.551}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5204}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5192}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20landmarker%3A%20a%20Toolkit%20for%20Anatomical%20Landmark%20Localization%20in%202D/3D%0A%20%20Images&body=Title%3A%20landmarker%3A%20a%20Toolkit%20for%20Anatomical%20Landmark%20Localization%20in%202D/3D%0A%20%20Images%0AAuthor%3A%20Jef%20Jonkers%20and%20Luc%20Duchateau%20and%20Glenn%20Van%20Wallendael%20and%20Sofie%20Van%20Hoecke%0AAbstract%3A%20%20%20Anatomical%20landmark%20localization%20in%202D/3D%20images%20is%20a%20critical%20task%20in%0Amedical%20imaging.%20Although%20many%20general-purpose%20tools%20exist%20for%20landmark%0Alocalization%20in%20classical%20computer%20vision%20tasks%2C%20such%20as%20pose%20estimation%2C%20they%0Alack%20the%20specialized%20features%20and%20modularity%20necessary%20for%20anatomical%20landmark%0Alocalization%20applications%20in%20the%20medical%20domain.%20Therefore%2C%20we%20introduce%0Alandmarker%2C%20a%20Python%20package%20built%20on%20PyTorch.%20The%20package%20provides%20a%0Acomprehensive%2C%20flexible%20toolkit%20for%20developing%20and%20evaluating%20landmark%0Alocalization%20algorithms%2C%20supporting%20a%20range%20of%20methodologies%2C%20including%20static%0Aand%20adaptive%20heatmap%20regression.%20landmarker%20enhances%20the%20accuracy%20of%20landmark%0Aidentification%2C%20streamlines%20research%20and%20development%20processes%2C%20and%20supports%0Avarious%20image%20formats%20and%20preprocessing%20pipelines.%20Its%20modular%20design%20allows%0Ausers%20to%20customize%20and%20extend%20the%20toolkit%20for%20specific%20datasets%20and%0Aapplications%2C%20accelerating%20innovation%20in%20medical%20imaging.%20landmarker%20addresses%0Aa%20critical%20need%20for%20precision%20and%20customization%20in%20landmark%20localization%20tasks%0Anot%20adequately%20met%20by%20existing%20general-purpose%20pose%20estimation%20tools.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10098v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dlandmarker%253A%2520a%2520Toolkit%2520for%2520Anatomical%2520Landmark%2520Localization%2520in%25202D/3D%250A%2520%2520Images%26entry.906535625%3DJef%2520Jonkers%2520and%2520Luc%2520Duchateau%2520and%2520Glenn%2520Van%2520Wallendael%2520and%2520Sofie%2520Van%2520Hoecke%26entry.1292438233%3D%2520%2520Anatomical%2520landmark%2520localization%2520in%25202D/3D%2520images%2520is%2520a%2520critical%2520task%2520in%250Amedical%2520imaging.%2520Although%2520many%2520general-purpose%2520tools%2520exist%2520for%2520landmark%250Alocalization%2520in%2520classical%2520computer%2520vision%2520tasks%252C%2520such%2520as%2520pose%2520estimation%252C%2520they%250Alack%2520the%2520specialized%2520features%2520and%2520modularity%2520necessary%2520for%2520anatomical%2520landmark%250Alocalization%2520applications%2520in%2520the%2520medical%2520domain.%2520Therefore%252C%2520we%2520introduce%250Alandmarker%252C%2520a%2520Python%2520package%2520built%2520on%2520PyTorch.%2520The%2520package%2520provides%2520a%250Acomprehensive%252C%2520flexible%2520toolkit%2520for%2520developing%2520and%2520evaluating%2520landmark%250Alocalization%2520algorithms%252C%2520supporting%2520a%2520range%2520of%2520methodologies%252C%2520including%2520static%250Aand%2520adaptive%2520heatmap%2520regression.%2520landmarker%2520enhances%2520the%2520accuracy%2520of%2520landmark%250Aidentification%252C%2520streamlines%2520research%2520and%2520development%2520processes%252C%2520and%2520supports%250Avarious%2520image%2520formats%2520and%2520preprocessing%2520pipelines.%2520Its%2520modular%2520design%2520allows%250Ausers%2520to%2520customize%2520and%2520extend%2520the%2520toolkit%2520for%2520specific%2520datasets%2520and%250Aapplications%252C%2520accelerating%2520innovation%2520in%2520medical%2520imaging.%2520landmarker%2520addresses%250Aa%2520critical%2520need%2520for%2520precision%2520and%2520customization%2520in%2520landmark%2520localization%2520tasks%250Anot%2520adequately%2520met%2520by%2520existing%2520general-purpose%2520pose%2520estimation%2520tools.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10098v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=landmarker%3A%20a%20Toolkit%20for%20Anatomical%20Landmark%20Localization%20in%202D/3D%0A%20%20Images&entry.906535625=Jef%20Jonkers%20and%20Luc%20Duchateau%20and%20Glenn%20Van%20Wallendael%20and%20Sofie%20Van%20Hoecke&entry.1292438233=%20%20Anatomical%20landmark%20localization%20in%202D/3D%20images%20is%20a%20critical%20task%20in%0Amedical%20imaging.%20Although%20many%20general-purpose%20tools%20exist%20for%20landmark%0Alocalization%20in%20classical%20computer%20vision%20tasks%2C%20such%20as%20pose%20estimation%2C%20they%0Alack%20the%20specialized%20features%20and%20modularity%20necessary%20for%20anatomical%20landmark%0Alocalization%20applications%20in%20the%20medical%20domain.%20Therefore%2C%20we%20introduce%0Alandmarker%2C%20a%20Python%20package%20built%20on%20PyTorch.%20The%20package%20provides%20a%0Acomprehensive%2C%20flexible%20toolkit%20for%20developing%20and%20evaluating%20landmark%0Alocalization%20algorithms%2C%20supporting%20a%20range%20of%20methodologies%2C%20including%20static%0Aand%20adaptive%20heatmap%20regression.%20landmarker%20enhances%20the%20accuracy%20of%20landmark%0Aidentification%2C%20streamlines%20research%20and%20development%20processes%2C%20and%20supports%0Avarious%20image%20formats%20and%20preprocessing%20pipelines.%20Its%20modular%20design%20allows%0Ausers%20to%20customize%20and%20extend%20the%20toolkit%20for%20specific%20datasets%20and%0Aapplications%2C%20accelerating%20innovation%20in%20medical%20imaging.%20landmarker%20addresses%0Aa%20critical%20need%20for%20precision%20and%20customization%20in%20landmark%20localization%20tasks%0Anot%20adequately%20met%20by%20existing%20general-purpose%20pose%20estimation%20tools.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10098v1&entry.124074799=Read"},
{"title": "Structure-guided Deep Multi-View Clustering", "author": "Jinrong Cui and Xiaohuang Wu and Haitao Zhang and Chongjie Dong and Jie Wen", "abstract": "  Deep multi-view clustering seeks to utilize the abundant information from\nmultiple views to improve clustering performance. However, most of the existing\nclustering methods often neglect to fully mine multi-view structural\ninformation and fail to explore the distribution of multi-view data, limiting\nclustering performance. To address these limitations, we propose a\nstructure-guided deep multi-view clustering model. Specifically, we introduce a\npositive sample selection strategy based on neighborhood relationships, coupled\nwith a corresponding loss function. This strategy constructs multi-view nearest\nneighbor graphs to dynamically redefine positive sample pairs, enabling the\nmining of local structural information within multi-view data and enhancing the\nreliability of positive sample selection. Additionally, we introduce a Gaussian\ndistribution model to uncover latent structural information and introduce a\nloss function to reduce discrepancies between view embeddings. These two\nstrategies explore multi-view structural information and data distribution from\ndifferent perspectives, enhancing consistency across views and increasing\nintra-cluster compactness. Experimental evaluations demonstrate the efficacy of\nour method, showing significant improvements in clustering performance on\nmultiple benchmark datasets compared to state-of-the-art multi-view clustering\napproaches.\n", "link": "http://arxiv.org/abs/2501.10157v1", "date": "2025-01-17", "relevancy": 2.5991, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5324}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5135}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5135}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structure-guided%20Deep%20Multi-View%20Clustering&body=Title%3A%20Structure-guided%20Deep%20Multi-View%20Clustering%0AAuthor%3A%20Jinrong%20Cui%20and%20Xiaohuang%20Wu%20and%20Haitao%20Zhang%20and%20Chongjie%20Dong%20and%20Jie%20Wen%0AAbstract%3A%20%20%20Deep%20multi-view%20clustering%20seeks%20to%20utilize%20the%20abundant%20information%20from%0Amultiple%20views%20to%20improve%20clustering%20performance.%20However%2C%20most%20of%20the%20existing%0Aclustering%20methods%20often%20neglect%20to%20fully%20mine%20multi-view%20structural%0Ainformation%20and%20fail%20to%20explore%20the%20distribution%20of%20multi-view%20data%2C%20limiting%0Aclustering%20performance.%20To%20address%20these%20limitations%2C%20we%20propose%20a%0Astructure-guided%20deep%20multi-view%20clustering%20model.%20Specifically%2C%20we%20introduce%20a%0Apositive%20sample%20selection%20strategy%20based%20on%20neighborhood%20relationships%2C%20coupled%0Awith%20a%20corresponding%20loss%20function.%20This%20strategy%20constructs%20multi-view%20nearest%0Aneighbor%20graphs%20to%20dynamically%20redefine%20positive%20sample%20pairs%2C%20enabling%20the%0Amining%20of%20local%20structural%20information%20within%20multi-view%20data%20and%20enhancing%20the%0Areliability%20of%20positive%20sample%20selection.%20Additionally%2C%20we%20introduce%20a%20Gaussian%0Adistribution%20model%20to%20uncover%20latent%20structural%20information%20and%20introduce%20a%0Aloss%20function%20to%20reduce%20discrepancies%20between%20view%20embeddings.%20These%20two%0Astrategies%20explore%20multi-view%20structural%20information%20and%20data%20distribution%20from%0Adifferent%20perspectives%2C%20enhancing%20consistency%20across%20views%20and%20increasing%0Aintra-cluster%20compactness.%20Experimental%20evaluations%20demonstrate%20the%20efficacy%20of%0Aour%20method%2C%20showing%20significant%20improvements%20in%20clustering%20performance%20on%0Amultiple%20benchmark%20datasets%20compared%20to%20state-of-the-art%20multi-view%20clustering%0Aapproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10157v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructure-guided%2520Deep%2520Multi-View%2520Clustering%26entry.906535625%3DJinrong%2520Cui%2520and%2520Xiaohuang%2520Wu%2520and%2520Haitao%2520Zhang%2520and%2520Chongjie%2520Dong%2520and%2520Jie%2520Wen%26entry.1292438233%3D%2520%2520Deep%2520multi-view%2520clustering%2520seeks%2520to%2520utilize%2520the%2520abundant%2520information%2520from%250Amultiple%2520views%2520to%2520improve%2520clustering%2520performance.%2520However%252C%2520most%2520of%2520the%2520existing%250Aclustering%2520methods%2520often%2520neglect%2520to%2520fully%2520mine%2520multi-view%2520structural%250Ainformation%2520and%2520fail%2520to%2520explore%2520the%2520distribution%2520of%2520multi-view%2520data%252C%2520limiting%250Aclustering%2520performance.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%250Astructure-guided%2520deep%2520multi-view%2520clustering%2520model.%2520Specifically%252C%2520we%2520introduce%2520a%250Apositive%2520sample%2520selection%2520strategy%2520based%2520on%2520neighborhood%2520relationships%252C%2520coupled%250Awith%2520a%2520corresponding%2520loss%2520function.%2520This%2520strategy%2520constructs%2520multi-view%2520nearest%250Aneighbor%2520graphs%2520to%2520dynamically%2520redefine%2520positive%2520sample%2520pairs%252C%2520enabling%2520the%250Amining%2520of%2520local%2520structural%2520information%2520within%2520multi-view%2520data%2520and%2520enhancing%2520the%250Areliability%2520of%2520positive%2520sample%2520selection.%2520Additionally%252C%2520we%2520introduce%2520a%2520Gaussian%250Adistribution%2520model%2520to%2520uncover%2520latent%2520structural%2520information%2520and%2520introduce%2520a%250Aloss%2520function%2520to%2520reduce%2520discrepancies%2520between%2520view%2520embeddings.%2520These%2520two%250Astrategies%2520explore%2520multi-view%2520structural%2520information%2520and%2520data%2520distribution%2520from%250Adifferent%2520perspectives%252C%2520enhancing%2520consistency%2520across%2520views%2520and%2520increasing%250Aintra-cluster%2520compactness.%2520Experimental%2520evaluations%2520demonstrate%2520the%2520efficacy%2520of%250Aour%2520method%252C%2520showing%2520significant%2520improvements%2520in%2520clustering%2520performance%2520on%250Amultiple%2520benchmark%2520datasets%2520compared%2520to%2520state-of-the-art%2520multi-view%2520clustering%250Aapproaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10157v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structure-guided%20Deep%20Multi-View%20Clustering&entry.906535625=Jinrong%20Cui%20and%20Xiaohuang%20Wu%20and%20Haitao%20Zhang%20and%20Chongjie%20Dong%20and%20Jie%20Wen&entry.1292438233=%20%20Deep%20multi-view%20clustering%20seeks%20to%20utilize%20the%20abundant%20information%20from%0Amultiple%20views%20to%20improve%20clustering%20performance.%20However%2C%20most%20of%20the%20existing%0Aclustering%20methods%20often%20neglect%20to%20fully%20mine%20multi-view%20structural%0Ainformation%20and%20fail%20to%20explore%20the%20distribution%20of%20multi-view%20data%2C%20limiting%0Aclustering%20performance.%20To%20address%20these%20limitations%2C%20we%20propose%20a%0Astructure-guided%20deep%20multi-view%20clustering%20model.%20Specifically%2C%20we%20introduce%20a%0Apositive%20sample%20selection%20strategy%20based%20on%20neighborhood%20relationships%2C%20coupled%0Awith%20a%20corresponding%20loss%20function.%20This%20strategy%20constructs%20multi-view%20nearest%0Aneighbor%20graphs%20to%20dynamically%20redefine%20positive%20sample%20pairs%2C%20enabling%20the%0Amining%20of%20local%20structural%20information%20within%20multi-view%20data%20and%20enhancing%20the%0Areliability%20of%20positive%20sample%20selection.%20Additionally%2C%20we%20introduce%20a%20Gaussian%0Adistribution%20model%20to%20uncover%20latent%20structural%20information%20and%20introduce%20a%0Aloss%20function%20to%20reduce%20discrepancies%20between%20view%20embeddings.%20These%20two%0Astrategies%20explore%20multi-view%20structural%20information%20and%20data%20distribution%20from%0Adifferent%20perspectives%2C%20enhancing%20consistency%20across%20views%20and%20increasing%0Aintra-cluster%20compactness.%20Experimental%20evaluations%20demonstrate%20the%20efficacy%20of%0Aour%20method%2C%20showing%20significant%20improvements%20in%20clustering%20performance%20on%0Amultiple%20benchmark%20datasets%20compared%20to%20state-of-the-art%20multi-view%20clustering%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10157v1&entry.124074799=Read"},
{"title": "IncSAR: A Dual Fusion Incremental Learning Framework for SAR Target\n  Recognition", "author": "George Karantaidis and Athanasios Pantsios and Ioannis Kompatsiaris and Symeon Papadopoulos", "abstract": "  Deep learning techniques have achieved significant success in Synthetic\nAperture Radar (SAR) target recognition using predefined datasets in static\nscenarios. However, real-world applications demand that models incrementally\nlearn new information without forgetting previously acquired knowledge. The\nchallenge of catastrophic forgetting, where models lose past knowledge when\nadapting to new tasks, remains a critical issue. In this paper, we introduce\nIncSAR, an incremental learning framework designed to tackle catastrophic\nforgetting in SAR target recognition. IncSAR combines the power of a Vision\nTransformer (ViT) and a custom-designed Convolutional Neural Network (CNN) in a\ndual-branch architecture, integrated via a late-fusion strategy. Additionally,\nwe explore the use of TinyViT to reduce computational complexity and propose an\nattention mechanism to dynamically enhance feature representation. To mitigate\nthe speckle noise inherent in SAR images, we employ a denoising module based on\na neural network approximation of Robust Principal Component Analysis (RPCA),\nleveraging a simple neural network for efficient noise reduction in SAR\nimagery. Moreover, a random projection layer improves the linear separability\nof features, and a variant of Linear Discriminant Analysis (LDA) decorrelates\nextracted class prototypes for better generalization. Extensive experiments on\nthe MSTAR, SAR-AIRcraft-1.0, and OpenSARShip benchmark datasets demonstrate\nthat IncSAR significantly outperforms state-of-the-art approaches, achieving a\n99.63\\% average accuracy and a 0.33\\% performance drop, representing an 89\\%\nimprovement in retention compared to existing techniques. The source code is\navailable at https://github.com/geokarant/IncSAR.\n", "link": "http://arxiv.org/abs/2410.05820v2", "date": "2025-01-17", "relevancy": 2.5919, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5353}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5106}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IncSAR%3A%20A%20Dual%20Fusion%20Incremental%20Learning%20Framework%20for%20SAR%20Target%0A%20%20Recognition&body=Title%3A%20IncSAR%3A%20A%20Dual%20Fusion%20Incremental%20Learning%20Framework%20for%20SAR%20Target%0A%20%20Recognition%0AAuthor%3A%20George%20Karantaidis%20and%20Athanasios%20Pantsios%20and%20Ioannis%20Kompatsiaris%20and%20Symeon%20Papadopoulos%0AAbstract%3A%20%20%20Deep%20learning%20techniques%20have%20achieved%20significant%20success%20in%20Synthetic%0AAperture%20Radar%20%28SAR%29%20target%20recognition%20using%20predefined%20datasets%20in%20static%0Ascenarios.%20However%2C%20real-world%20applications%20demand%20that%20models%20incrementally%0Alearn%20new%20information%20without%20forgetting%20previously%20acquired%20knowledge.%20The%0Achallenge%20of%20catastrophic%20forgetting%2C%20where%20models%20lose%20past%20knowledge%20when%0Aadapting%20to%20new%20tasks%2C%20remains%20a%20critical%20issue.%20In%20this%20paper%2C%20we%20introduce%0AIncSAR%2C%20an%20incremental%20learning%20framework%20designed%20to%20tackle%20catastrophic%0Aforgetting%20in%20SAR%20target%20recognition.%20IncSAR%20combines%20the%20power%20of%20a%20Vision%0ATransformer%20%28ViT%29%20and%20a%20custom-designed%20Convolutional%20Neural%20Network%20%28CNN%29%20in%20a%0Adual-branch%20architecture%2C%20integrated%20via%20a%20late-fusion%20strategy.%20Additionally%2C%0Awe%20explore%20the%20use%20of%20TinyViT%20to%20reduce%20computational%20complexity%20and%20propose%20an%0Aattention%20mechanism%20to%20dynamically%20enhance%20feature%20representation.%20To%20mitigate%0Athe%20speckle%20noise%20inherent%20in%20SAR%20images%2C%20we%20employ%20a%20denoising%20module%20based%20on%0Aa%20neural%20network%20approximation%20of%20Robust%20Principal%20Component%20Analysis%20%28RPCA%29%2C%0Aleveraging%20a%20simple%20neural%20network%20for%20efficient%20noise%20reduction%20in%20SAR%0Aimagery.%20Moreover%2C%20a%20random%20projection%20layer%20improves%20the%20linear%20separability%0Aof%20features%2C%20and%20a%20variant%20of%20Linear%20Discriminant%20Analysis%20%28LDA%29%20decorrelates%0Aextracted%20class%20prototypes%20for%20better%20generalization.%20Extensive%20experiments%20on%0Athe%20MSTAR%2C%20SAR-AIRcraft-1.0%2C%20and%20OpenSARShip%20benchmark%20datasets%20demonstrate%0Athat%20IncSAR%20significantly%20outperforms%20state-of-the-art%20approaches%2C%20achieving%20a%0A99.63%5C%25%20average%20accuracy%20and%20a%200.33%5C%25%20performance%20drop%2C%20representing%20an%2089%5C%25%0Aimprovement%20in%20retention%20compared%20to%20existing%20techniques.%20The%20source%20code%20is%0Aavailable%20at%20https%3A//github.com/geokarant/IncSAR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05820v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncSAR%253A%2520A%2520Dual%2520Fusion%2520Incremental%2520Learning%2520Framework%2520for%2520SAR%2520Target%250A%2520%2520Recognition%26entry.906535625%3DGeorge%2520Karantaidis%2520and%2520Athanasios%2520Pantsios%2520and%2520Ioannis%2520Kompatsiaris%2520and%2520Symeon%2520Papadopoulos%26entry.1292438233%3D%2520%2520Deep%2520learning%2520techniques%2520have%2520achieved%2520significant%2520success%2520in%2520Synthetic%250AAperture%2520Radar%2520%2528SAR%2529%2520target%2520recognition%2520using%2520predefined%2520datasets%2520in%2520static%250Ascenarios.%2520However%252C%2520real-world%2520applications%2520demand%2520that%2520models%2520incrementally%250Alearn%2520new%2520information%2520without%2520forgetting%2520previously%2520acquired%2520knowledge.%2520The%250Achallenge%2520of%2520catastrophic%2520forgetting%252C%2520where%2520models%2520lose%2520past%2520knowledge%2520when%250Aadapting%2520to%2520new%2520tasks%252C%2520remains%2520a%2520critical%2520issue.%2520In%2520this%2520paper%252C%2520we%2520introduce%250AIncSAR%252C%2520an%2520incremental%2520learning%2520framework%2520designed%2520to%2520tackle%2520catastrophic%250Aforgetting%2520in%2520SAR%2520target%2520recognition.%2520IncSAR%2520combines%2520the%2520power%2520of%2520a%2520Vision%250ATransformer%2520%2528ViT%2529%2520and%2520a%2520custom-designed%2520Convolutional%2520Neural%2520Network%2520%2528CNN%2529%2520in%2520a%250Adual-branch%2520architecture%252C%2520integrated%2520via%2520a%2520late-fusion%2520strategy.%2520Additionally%252C%250Awe%2520explore%2520the%2520use%2520of%2520TinyViT%2520to%2520reduce%2520computational%2520complexity%2520and%2520propose%2520an%250Aattention%2520mechanism%2520to%2520dynamically%2520enhance%2520feature%2520representation.%2520To%2520mitigate%250Athe%2520speckle%2520noise%2520inherent%2520in%2520SAR%2520images%252C%2520we%2520employ%2520a%2520denoising%2520module%2520based%2520on%250Aa%2520neural%2520network%2520approximation%2520of%2520Robust%2520Principal%2520Component%2520Analysis%2520%2528RPCA%2529%252C%250Aleveraging%2520a%2520simple%2520neural%2520network%2520for%2520efficient%2520noise%2520reduction%2520in%2520SAR%250Aimagery.%2520Moreover%252C%2520a%2520random%2520projection%2520layer%2520improves%2520the%2520linear%2520separability%250Aof%2520features%252C%2520and%2520a%2520variant%2520of%2520Linear%2520Discriminant%2520Analysis%2520%2528LDA%2529%2520decorrelates%250Aextracted%2520class%2520prototypes%2520for%2520better%2520generalization.%2520Extensive%2520experiments%2520on%250Athe%2520MSTAR%252C%2520SAR-AIRcraft-1.0%252C%2520and%2520OpenSARShip%2520benchmark%2520datasets%2520demonstrate%250Athat%2520IncSAR%2520significantly%2520outperforms%2520state-of-the-art%2520approaches%252C%2520achieving%2520a%250A99.63%255C%2525%2520average%2520accuracy%2520and%2520a%25200.33%255C%2525%2520performance%2520drop%252C%2520representing%2520an%252089%255C%2525%250Aimprovement%2520in%2520retention%2520compared%2520to%2520existing%2520techniques.%2520The%2520source%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/geokarant/IncSAR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05820v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IncSAR%3A%20A%20Dual%20Fusion%20Incremental%20Learning%20Framework%20for%20SAR%20Target%0A%20%20Recognition&entry.906535625=George%20Karantaidis%20and%20Athanasios%20Pantsios%20and%20Ioannis%20Kompatsiaris%20and%20Symeon%20Papadopoulos&entry.1292438233=%20%20Deep%20learning%20techniques%20have%20achieved%20significant%20success%20in%20Synthetic%0AAperture%20Radar%20%28SAR%29%20target%20recognition%20using%20predefined%20datasets%20in%20static%0Ascenarios.%20However%2C%20real-world%20applications%20demand%20that%20models%20incrementally%0Alearn%20new%20information%20without%20forgetting%20previously%20acquired%20knowledge.%20The%0Achallenge%20of%20catastrophic%20forgetting%2C%20where%20models%20lose%20past%20knowledge%20when%0Aadapting%20to%20new%20tasks%2C%20remains%20a%20critical%20issue.%20In%20this%20paper%2C%20we%20introduce%0AIncSAR%2C%20an%20incremental%20learning%20framework%20designed%20to%20tackle%20catastrophic%0Aforgetting%20in%20SAR%20target%20recognition.%20IncSAR%20combines%20the%20power%20of%20a%20Vision%0ATransformer%20%28ViT%29%20and%20a%20custom-designed%20Convolutional%20Neural%20Network%20%28CNN%29%20in%20a%0Adual-branch%20architecture%2C%20integrated%20via%20a%20late-fusion%20strategy.%20Additionally%2C%0Awe%20explore%20the%20use%20of%20TinyViT%20to%20reduce%20computational%20complexity%20and%20propose%20an%0Aattention%20mechanism%20to%20dynamically%20enhance%20feature%20representation.%20To%20mitigate%0Athe%20speckle%20noise%20inherent%20in%20SAR%20images%2C%20we%20employ%20a%20denoising%20module%20based%20on%0Aa%20neural%20network%20approximation%20of%20Robust%20Principal%20Component%20Analysis%20%28RPCA%29%2C%0Aleveraging%20a%20simple%20neural%20network%20for%20efficient%20noise%20reduction%20in%20SAR%0Aimagery.%20Moreover%2C%20a%20random%20projection%20layer%20improves%20the%20linear%20separability%0Aof%20features%2C%20and%20a%20variant%20of%20Linear%20Discriminant%20Analysis%20%28LDA%29%20decorrelates%0Aextracted%20class%20prototypes%20for%20better%20generalization.%20Extensive%20experiments%20on%0Athe%20MSTAR%2C%20SAR-AIRcraft-1.0%2C%20and%20OpenSARShip%20benchmark%20datasets%20demonstrate%0Athat%20IncSAR%20significantly%20outperforms%20state-of-the-art%20approaches%2C%20achieving%20a%0A99.63%5C%25%20average%20accuracy%20and%20a%200.33%5C%25%20performance%20drop%2C%20representing%20an%2089%5C%25%0Aimprovement%20in%20retention%20compared%20to%20existing%20techniques.%20The%20source%20code%20is%0Aavailable%20at%20https%3A//github.com/geokarant/IncSAR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05820v2&entry.124074799=Read"},
{"title": "How Do Programming Students Use Generative AI?", "author": "Christian Rahe and Walid Maalej", "abstract": "  Programming students have a widespread access to powerful Generative AI tools\nlike ChatGPT. While this can help understand the learning material and assist\nwith exercises, educators are voicing more and more concerns about an\nover-reliance on generated outputs and lack of critical thinking skills. It is\nthus important to understand how students actually use generative AI and what\nimpact this could have on their learning behavior. To this end, we conducted a\nstudy including an exploratory experiment with 37 programming students, giving\nthem monitored access to ChatGPT while solving a code understanding and\nimproving exercise. While only 23 of the students actually opted to use the\nchatbot, the majority of those eventually prompted it to simply generate a full\nsolution. We observed two prevalent usage strategies: to seek knowledge about\ngeneral concepts and to directly generate solutions. Instead of using the bot\nto comprehend the code and their own mistakes, students often got trapped in a\nvicious cycle of submitting wrong generated code and then asking the bot for a\nfix. Those who self-reported using generative AI regularly were more likely to\nprompt the bot to generate a solution. Our findings indicate that concerns\nabout potential decrease in programmers' agency and productivity with\nGenerative AI are justified. We discuss how researchers and educators can\nrespond to the potential risk of students uncritically over-relying on\ngenerative AI. We also discuss potential modifications to our study design for\nlarge-scale replications.\n", "link": "http://arxiv.org/abs/2501.10091v1", "date": "2025-01-17", "relevancy": 2.526, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.552}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5045}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Do%20Programming%20Students%20Use%20Generative%20AI%3F&body=Title%3A%20How%20Do%20Programming%20Students%20Use%20Generative%20AI%3F%0AAuthor%3A%20Christian%20Rahe%20and%20Walid%20Maalej%0AAbstract%3A%20%20%20Programming%20students%20have%20a%20widespread%20access%20to%20powerful%20Generative%20AI%20tools%0Alike%20ChatGPT.%20While%20this%20can%20help%20understand%20the%20learning%20material%20and%20assist%0Awith%20exercises%2C%20educators%20are%20voicing%20more%20and%20more%20concerns%20about%20an%0Aover-reliance%20on%20generated%20outputs%20and%20lack%20of%20critical%20thinking%20skills.%20It%20is%0Athus%20important%20to%20understand%20how%20students%20actually%20use%20generative%20AI%20and%20what%0Aimpact%20this%20could%20have%20on%20their%20learning%20behavior.%20To%20this%20end%2C%20we%20conducted%20a%0Astudy%20including%20an%20exploratory%20experiment%20with%2037%20programming%20students%2C%20giving%0Athem%20monitored%20access%20to%20ChatGPT%20while%20solving%20a%20code%20understanding%20and%0Aimproving%20exercise.%20While%20only%2023%20of%20the%20students%20actually%20opted%20to%20use%20the%0Achatbot%2C%20the%20majority%20of%20those%20eventually%20prompted%20it%20to%20simply%20generate%20a%20full%0Asolution.%20We%20observed%20two%20prevalent%20usage%20strategies%3A%20to%20seek%20knowledge%20about%0Ageneral%20concepts%20and%20to%20directly%20generate%20solutions.%20Instead%20of%20using%20the%20bot%0Ato%20comprehend%20the%20code%20and%20their%20own%20mistakes%2C%20students%20often%20got%20trapped%20in%20a%0Avicious%20cycle%20of%20submitting%20wrong%20generated%20code%20and%20then%20asking%20the%20bot%20for%20a%0Afix.%20Those%20who%20self-reported%20using%20generative%20AI%20regularly%20were%20more%20likely%20to%0Aprompt%20the%20bot%20to%20generate%20a%20solution.%20Our%20findings%20indicate%20that%20concerns%0Aabout%20potential%20decrease%20in%20programmers%27%20agency%20and%20productivity%20with%0AGenerative%20AI%20are%20justified.%20We%20discuss%20how%20researchers%20and%20educators%20can%0Arespond%20to%20the%20potential%20risk%20of%20students%20uncritically%20over-relying%20on%0Agenerative%20AI.%20We%20also%20discuss%20potential%20modifications%20to%20our%20study%20design%20for%0Alarge-scale%20replications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10091v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Do%2520Programming%2520Students%2520Use%2520Generative%2520AI%253F%26entry.906535625%3DChristian%2520Rahe%2520and%2520Walid%2520Maalej%26entry.1292438233%3D%2520%2520Programming%2520students%2520have%2520a%2520widespread%2520access%2520to%2520powerful%2520Generative%2520AI%2520tools%250Alike%2520ChatGPT.%2520While%2520this%2520can%2520help%2520understand%2520the%2520learning%2520material%2520and%2520assist%250Awith%2520exercises%252C%2520educators%2520are%2520voicing%2520more%2520and%2520more%2520concerns%2520about%2520an%250Aover-reliance%2520on%2520generated%2520outputs%2520and%2520lack%2520of%2520critical%2520thinking%2520skills.%2520It%2520is%250Athus%2520important%2520to%2520understand%2520how%2520students%2520actually%2520use%2520generative%2520AI%2520and%2520what%250Aimpact%2520this%2520could%2520have%2520on%2520their%2520learning%2520behavior.%2520To%2520this%2520end%252C%2520we%2520conducted%2520a%250Astudy%2520including%2520an%2520exploratory%2520experiment%2520with%252037%2520programming%2520students%252C%2520giving%250Athem%2520monitored%2520access%2520to%2520ChatGPT%2520while%2520solving%2520a%2520code%2520understanding%2520and%250Aimproving%2520exercise.%2520While%2520only%252023%2520of%2520the%2520students%2520actually%2520opted%2520to%2520use%2520the%250Achatbot%252C%2520the%2520majority%2520of%2520those%2520eventually%2520prompted%2520it%2520to%2520simply%2520generate%2520a%2520full%250Asolution.%2520We%2520observed%2520two%2520prevalent%2520usage%2520strategies%253A%2520to%2520seek%2520knowledge%2520about%250Ageneral%2520concepts%2520and%2520to%2520directly%2520generate%2520solutions.%2520Instead%2520of%2520using%2520the%2520bot%250Ato%2520comprehend%2520the%2520code%2520and%2520their%2520own%2520mistakes%252C%2520students%2520often%2520got%2520trapped%2520in%2520a%250Avicious%2520cycle%2520of%2520submitting%2520wrong%2520generated%2520code%2520and%2520then%2520asking%2520the%2520bot%2520for%2520a%250Afix.%2520Those%2520who%2520self-reported%2520using%2520generative%2520AI%2520regularly%2520were%2520more%2520likely%2520to%250Aprompt%2520the%2520bot%2520to%2520generate%2520a%2520solution.%2520Our%2520findings%2520indicate%2520that%2520concerns%250Aabout%2520potential%2520decrease%2520in%2520programmers%2527%2520agency%2520and%2520productivity%2520with%250AGenerative%2520AI%2520are%2520justified.%2520We%2520discuss%2520how%2520researchers%2520and%2520educators%2520can%250Arespond%2520to%2520the%2520potential%2520risk%2520of%2520students%2520uncritically%2520over-relying%2520on%250Agenerative%2520AI.%2520We%2520also%2520discuss%2520potential%2520modifications%2520to%2520our%2520study%2520design%2520for%250Alarge-scale%2520replications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10091v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Do%20Programming%20Students%20Use%20Generative%20AI%3F&entry.906535625=Christian%20Rahe%20and%20Walid%20Maalej&entry.1292438233=%20%20Programming%20students%20have%20a%20widespread%20access%20to%20powerful%20Generative%20AI%20tools%0Alike%20ChatGPT.%20While%20this%20can%20help%20understand%20the%20learning%20material%20and%20assist%0Awith%20exercises%2C%20educators%20are%20voicing%20more%20and%20more%20concerns%20about%20an%0Aover-reliance%20on%20generated%20outputs%20and%20lack%20of%20critical%20thinking%20skills.%20It%20is%0Athus%20important%20to%20understand%20how%20students%20actually%20use%20generative%20AI%20and%20what%0Aimpact%20this%20could%20have%20on%20their%20learning%20behavior.%20To%20this%20end%2C%20we%20conducted%20a%0Astudy%20including%20an%20exploratory%20experiment%20with%2037%20programming%20students%2C%20giving%0Athem%20monitored%20access%20to%20ChatGPT%20while%20solving%20a%20code%20understanding%20and%0Aimproving%20exercise.%20While%20only%2023%20of%20the%20students%20actually%20opted%20to%20use%20the%0Achatbot%2C%20the%20majority%20of%20those%20eventually%20prompted%20it%20to%20simply%20generate%20a%20full%0Asolution.%20We%20observed%20two%20prevalent%20usage%20strategies%3A%20to%20seek%20knowledge%20about%0Ageneral%20concepts%20and%20to%20directly%20generate%20solutions.%20Instead%20of%20using%20the%20bot%0Ato%20comprehend%20the%20code%20and%20their%20own%20mistakes%2C%20students%20often%20got%20trapped%20in%20a%0Avicious%20cycle%20of%20submitting%20wrong%20generated%20code%20and%20then%20asking%20the%20bot%20for%20a%0Afix.%20Those%20who%20self-reported%20using%20generative%20AI%20regularly%20were%20more%20likely%20to%0Aprompt%20the%20bot%20to%20generate%20a%20solution.%20Our%20findings%20indicate%20that%20concerns%0Aabout%20potential%20decrease%20in%20programmers%27%20agency%20and%20productivity%20with%0AGenerative%20AI%20are%20justified.%20We%20discuss%20how%20researchers%20and%20educators%20can%0Arespond%20to%20the%20potential%20risk%20of%20students%20uncritically%20over-relying%20on%0Agenerative%20AI.%20We%20also%20discuss%20potential%20modifications%20to%20our%20study%20design%20for%0Alarge-scale%20replications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10091v1&entry.124074799=Read"},
{"title": "News Without Borders: Domain Adaptation of Multilingual Sentence\n  Embeddings for Cross-lingual News Recommendation", "author": "Andreea Iana and Fabian David Schmidt and Goran Glava\u0161 and Heiko Paulheim", "abstract": "  Rapidly growing numbers of multilingual news consumers pose an increasing\nchallenge to news recommender systems in terms of providing customized\nrecommendations. First, existing neural news recommenders, even when powered by\nmultilingual language models (LMs), suffer substantial performance losses in\nzero-shot cross-lingual transfer (ZS-XLT). Second, the current paradigm of\nfine-tuning the backbone LM of a neural recommender on task-specific data is\ncomputationally expensive and infeasible in few-shot recommendation and\ncold-start setups, where data is scarce or completely unavailable. In this\nwork, we propose a news-adapted sentence encoder (NaSE), domain-specialized\nfrom a pretrained massively multilingual sentence encoder (SE). To this end, we\nconstruct and leverage PolyNews and PolyNewsParallel, two multilingual\nnews-specific corpora. With the news-adapted multilingual SE in place, we test\nthe effectiveness of (i.e., question the need for) supervised fine-tuning for\nnews recommendation, and propose a simple and strong baseline based on (i)\nfrozen NaSE embeddings and (ii) late click-behavior fusion. We show that NaSE\nachieves state-of-the-art performance in ZS-XLT in true cold-start and few-shot\nnews recommendation.\n", "link": "http://arxiv.org/abs/2406.12634v2", "date": "2025-01-17", "relevancy": 2.4841, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5059}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5059}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20News%20Without%20Borders%3A%20Domain%20Adaptation%20of%20Multilingual%20Sentence%0A%20%20Embeddings%20for%20Cross-lingual%20News%20Recommendation&body=Title%3A%20News%20Without%20Borders%3A%20Domain%20Adaptation%20of%20Multilingual%20Sentence%0A%20%20Embeddings%20for%20Cross-lingual%20News%20Recommendation%0AAuthor%3A%20Andreea%20Iana%20and%20Fabian%20David%20Schmidt%20and%20Goran%20Glava%C5%A1%20and%20Heiko%20Paulheim%0AAbstract%3A%20%20%20Rapidly%20growing%20numbers%20of%20multilingual%20news%20consumers%20pose%20an%20increasing%0Achallenge%20to%20news%20recommender%20systems%20in%20terms%20of%20providing%20customized%0Arecommendations.%20First%2C%20existing%20neural%20news%20recommenders%2C%20even%20when%20powered%20by%0Amultilingual%20language%20models%20%28LMs%29%2C%20suffer%20substantial%20performance%20losses%20in%0Azero-shot%20cross-lingual%20transfer%20%28ZS-XLT%29.%20Second%2C%20the%20current%20paradigm%20of%0Afine-tuning%20the%20backbone%20LM%20of%20a%20neural%20recommender%20on%20task-specific%20data%20is%0Acomputationally%20expensive%20and%20infeasible%20in%20few-shot%20recommendation%20and%0Acold-start%20setups%2C%20where%20data%20is%20scarce%20or%20completely%20unavailable.%20In%20this%0Awork%2C%20we%20propose%20a%20news-adapted%20sentence%20encoder%20%28NaSE%29%2C%20domain-specialized%0Afrom%20a%20pretrained%20massively%20multilingual%20sentence%20encoder%20%28SE%29.%20To%20this%20end%2C%20we%0Aconstruct%20and%20leverage%20PolyNews%20and%20PolyNewsParallel%2C%20two%20multilingual%0Anews-specific%20corpora.%20With%20the%20news-adapted%20multilingual%20SE%20in%20place%2C%20we%20test%0Athe%20effectiveness%20of%20%28i.e.%2C%20question%20the%20need%20for%29%20supervised%20fine-tuning%20for%0Anews%20recommendation%2C%20and%20propose%20a%20simple%20and%20strong%20baseline%20based%20on%20%28i%29%0Afrozen%20NaSE%20embeddings%20and%20%28ii%29%20late%20click-behavior%20fusion.%20We%20show%20that%20NaSE%0Aachieves%20state-of-the-art%20performance%20in%20ZS-XLT%20in%20true%20cold-start%20and%20few-shot%0Anews%20recommendation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12634v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNews%2520Without%2520Borders%253A%2520Domain%2520Adaptation%2520of%2520Multilingual%2520Sentence%250A%2520%2520Embeddings%2520for%2520Cross-lingual%2520News%2520Recommendation%26entry.906535625%3DAndreea%2520Iana%2520and%2520Fabian%2520David%2520Schmidt%2520and%2520Goran%2520Glava%25C5%25A1%2520and%2520Heiko%2520Paulheim%26entry.1292438233%3D%2520%2520Rapidly%2520growing%2520numbers%2520of%2520multilingual%2520news%2520consumers%2520pose%2520an%2520increasing%250Achallenge%2520to%2520news%2520recommender%2520systems%2520in%2520terms%2520of%2520providing%2520customized%250Arecommendations.%2520First%252C%2520existing%2520neural%2520news%2520recommenders%252C%2520even%2520when%2520powered%2520by%250Amultilingual%2520language%2520models%2520%2528LMs%2529%252C%2520suffer%2520substantial%2520performance%2520losses%2520in%250Azero-shot%2520cross-lingual%2520transfer%2520%2528ZS-XLT%2529.%2520Second%252C%2520the%2520current%2520paradigm%2520of%250Afine-tuning%2520the%2520backbone%2520LM%2520of%2520a%2520neural%2520recommender%2520on%2520task-specific%2520data%2520is%250Acomputationally%2520expensive%2520and%2520infeasible%2520in%2520few-shot%2520recommendation%2520and%250Acold-start%2520setups%252C%2520where%2520data%2520is%2520scarce%2520or%2520completely%2520unavailable.%2520In%2520this%250Awork%252C%2520we%2520propose%2520a%2520news-adapted%2520sentence%2520encoder%2520%2528NaSE%2529%252C%2520domain-specialized%250Afrom%2520a%2520pretrained%2520massively%2520multilingual%2520sentence%2520encoder%2520%2528SE%2529.%2520To%2520this%2520end%252C%2520we%250Aconstruct%2520and%2520leverage%2520PolyNews%2520and%2520PolyNewsParallel%252C%2520two%2520multilingual%250Anews-specific%2520corpora.%2520With%2520the%2520news-adapted%2520multilingual%2520SE%2520in%2520place%252C%2520we%2520test%250Athe%2520effectiveness%2520of%2520%2528i.e.%252C%2520question%2520the%2520need%2520for%2529%2520supervised%2520fine-tuning%2520for%250Anews%2520recommendation%252C%2520and%2520propose%2520a%2520simple%2520and%2520strong%2520baseline%2520based%2520on%2520%2528i%2529%250Afrozen%2520NaSE%2520embeddings%2520and%2520%2528ii%2529%2520late%2520click-behavior%2520fusion.%2520We%2520show%2520that%2520NaSE%250Aachieves%2520state-of-the-art%2520performance%2520in%2520ZS-XLT%2520in%2520true%2520cold-start%2520and%2520few-shot%250Anews%2520recommendation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12634v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=News%20Without%20Borders%3A%20Domain%20Adaptation%20of%20Multilingual%20Sentence%0A%20%20Embeddings%20for%20Cross-lingual%20News%20Recommendation&entry.906535625=Andreea%20Iana%20and%20Fabian%20David%20Schmidt%20and%20Goran%20Glava%C5%A1%20and%20Heiko%20Paulheim&entry.1292438233=%20%20Rapidly%20growing%20numbers%20of%20multilingual%20news%20consumers%20pose%20an%20increasing%0Achallenge%20to%20news%20recommender%20systems%20in%20terms%20of%20providing%20customized%0Arecommendations.%20First%2C%20existing%20neural%20news%20recommenders%2C%20even%20when%20powered%20by%0Amultilingual%20language%20models%20%28LMs%29%2C%20suffer%20substantial%20performance%20losses%20in%0Azero-shot%20cross-lingual%20transfer%20%28ZS-XLT%29.%20Second%2C%20the%20current%20paradigm%20of%0Afine-tuning%20the%20backbone%20LM%20of%20a%20neural%20recommender%20on%20task-specific%20data%20is%0Acomputationally%20expensive%20and%20infeasible%20in%20few-shot%20recommendation%20and%0Acold-start%20setups%2C%20where%20data%20is%20scarce%20or%20completely%20unavailable.%20In%20this%0Awork%2C%20we%20propose%20a%20news-adapted%20sentence%20encoder%20%28NaSE%29%2C%20domain-specialized%0Afrom%20a%20pretrained%20massively%20multilingual%20sentence%20encoder%20%28SE%29.%20To%20this%20end%2C%20we%0Aconstruct%20and%20leverage%20PolyNews%20and%20PolyNewsParallel%2C%20two%20multilingual%0Anews-specific%20corpora.%20With%20the%20news-adapted%20multilingual%20SE%20in%20place%2C%20we%20test%0Athe%20effectiveness%20of%20%28i.e.%2C%20question%20the%20need%20for%29%20supervised%20fine-tuning%20for%0Anews%20recommendation%2C%20and%20propose%20a%20simple%20and%20strong%20baseline%20based%20on%20%28i%29%0Afrozen%20NaSE%20embeddings%20and%20%28ii%29%20late%20click-behavior%20fusion.%20We%20show%20that%20NaSE%0Aachieves%20state-of-the-art%20performance%20in%20ZS-XLT%20in%20true%20cold-start%20and%20few-shot%0Anews%20recommendation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12634v2&entry.124074799=Read"},
{"title": "Visual Exploration of Stopword Probabilities in Topic Models", "author": "Shuangjiang Xue and Pierre Le Bras and David A. Robb and Mike J. Chantler and Stefano Padilla", "abstract": "  Stopword removal is a critical stage in many Machine Learning methods but\noften receives little consideration, it interferes with the model\nvisualizations and disrupts user confidence. Inappropriately chosen or hastily\nomitted stopwords not only lead to suboptimal performance but also\nsignificantly affect the quality of models, thus reducing the willingness of\npractitioners and stakeholders to rely on the output visualizations. This paper\nproposes a novel extraction method that provides a corpus-specific\nprobabilistic estimation of stopword likelihood and an interactive\nvisualization system to support their analysis. We evaluated our approach and\ninterface using real-world data, a commonly used Machine Learning method (Topic\nModelling), and a comprehensive qualitative experiment probing user confidence.\nThe results of our work show that our system increases user confidence in the\ncredibility of topic models by (1) returning reasonable probabilities, (2)\ngenerating an appropriate and representative extension of common stopword\nlists, and (3) providing an adjustable threshold for estimating and analyzing\nstopwords visually. Finally, we discuss insights, recommendations, and best\npractices to support practitioners while improving the output of Machine\nLearning methods and topic model visualizations with robust stopword analysis\nand removal.\n", "link": "http://arxiv.org/abs/2501.10137v1", "date": "2025-01-17", "relevancy": 2.4658, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5045}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5045}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Exploration%20of%20Stopword%20Probabilities%20in%20Topic%20Models&body=Title%3A%20Visual%20Exploration%20of%20Stopword%20Probabilities%20in%20Topic%20Models%0AAuthor%3A%20Shuangjiang%20Xue%20and%20Pierre%20Le%20Bras%20and%20David%20A.%20Robb%20and%20Mike%20J.%20Chantler%20and%20Stefano%20Padilla%0AAbstract%3A%20%20%20Stopword%20removal%20is%20a%20critical%20stage%20in%20many%20Machine%20Learning%20methods%20but%0Aoften%20receives%20little%20consideration%2C%20it%20interferes%20with%20the%20model%0Avisualizations%20and%20disrupts%20user%20confidence.%20Inappropriately%20chosen%20or%20hastily%0Aomitted%20stopwords%20not%20only%20lead%20to%20suboptimal%20performance%20but%20also%0Asignificantly%20affect%20the%20quality%20of%20models%2C%20thus%20reducing%20the%20willingness%20of%0Apractitioners%20and%20stakeholders%20to%20rely%20on%20the%20output%20visualizations.%20This%20paper%0Aproposes%20a%20novel%20extraction%20method%20that%20provides%20a%20corpus-specific%0Aprobabilistic%20estimation%20of%20stopword%20likelihood%20and%20an%20interactive%0Avisualization%20system%20to%20support%20their%20analysis.%20We%20evaluated%20our%20approach%20and%0Ainterface%20using%20real-world%20data%2C%20a%20commonly%20used%20Machine%20Learning%20method%20%28Topic%0AModelling%29%2C%20and%20a%20comprehensive%20qualitative%20experiment%20probing%20user%20confidence.%0AThe%20results%20of%20our%20work%20show%20that%20our%20system%20increases%20user%20confidence%20in%20the%0Acredibility%20of%20topic%20models%20by%20%281%29%20returning%20reasonable%20probabilities%2C%20%282%29%0Agenerating%20an%20appropriate%20and%20representative%20extension%20of%20common%20stopword%0Alists%2C%20and%20%283%29%20providing%20an%20adjustable%20threshold%20for%20estimating%20and%20analyzing%0Astopwords%20visually.%20Finally%2C%20we%20discuss%20insights%2C%20recommendations%2C%20and%20best%0Apractices%20to%20support%20practitioners%20while%20improving%20the%20output%20of%20Machine%0ALearning%20methods%20and%20topic%20model%20visualizations%20with%20robust%20stopword%20analysis%0Aand%20removal.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10137v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Exploration%2520of%2520Stopword%2520Probabilities%2520in%2520Topic%2520Models%26entry.906535625%3DShuangjiang%2520Xue%2520and%2520Pierre%2520Le%2520Bras%2520and%2520David%2520A.%2520Robb%2520and%2520Mike%2520J.%2520Chantler%2520and%2520Stefano%2520Padilla%26entry.1292438233%3D%2520%2520Stopword%2520removal%2520is%2520a%2520critical%2520stage%2520in%2520many%2520Machine%2520Learning%2520methods%2520but%250Aoften%2520receives%2520little%2520consideration%252C%2520it%2520interferes%2520with%2520the%2520model%250Avisualizations%2520and%2520disrupts%2520user%2520confidence.%2520Inappropriately%2520chosen%2520or%2520hastily%250Aomitted%2520stopwords%2520not%2520only%2520lead%2520to%2520suboptimal%2520performance%2520but%2520also%250Asignificantly%2520affect%2520the%2520quality%2520of%2520models%252C%2520thus%2520reducing%2520the%2520willingness%2520of%250Apractitioners%2520and%2520stakeholders%2520to%2520rely%2520on%2520the%2520output%2520visualizations.%2520This%2520paper%250Aproposes%2520a%2520novel%2520extraction%2520method%2520that%2520provides%2520a%2520corpus-specific%250Aprobabilistic%2520estimation%2520of%2520stopword%2520likelihood%2520and%2520an%2520interactive%250Avisualization%2520system%2520to%2520support%2520their%2520analysis.%2520We%2520evaluated%2520our%2520approach%2520and%250Ainterface%2520using%2520real-world%2520data%252C%2520a%2520commonly%2520used%2520Machine%2520Learning%2520method%2520%2528Topic%250AModelling%2529%252C%2520and%2520a%2520comprehensive%2520qualitative%2520experiment%2520probing%2520user%2520confidence.%250AThe%2520results%2520of%2520our%2520work%2520show%2520that%2520our%2520system%2520increases%2520user%2520confidence%2520in%2520the%250Acredibility%2520of%2520topic%2520models%2520by%2520%25281%2529%2520returning%2520reasonable%2520probabilities%252C%2520%25282%2529%250Agenerating%2520an%2520appropriate%2520and%2520representative%2520extension%2520of%2520common%2520stopword%250Alists%252C%2520and%2520%25283%2529%2520providing%2520an%2520adjustable%2520threshold%2520for%2520estimating%2520and%2520analyzing%250Astopwords%2520visually.%2520Finally%252C%2520we%2520discuss%2520insights%252C%2520recommendations%252C%2520and%2520best%250Apractices%2520to%2520support%2520practitioners%2520while%2520improving%2520the%2520output%2520of%2520Machine%250ALearning%2520methods%2520and%2520topic%2520model%2520visualizations%2520with%2520robust%2520stopword%2520analysis%250Aand%2520removal.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10137v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Exploration%20of%20Stopword%20Probabilities%20in%20Topic%20Models&entry.906535625=Shuangjiang%20Xue%20and%20Pierre%20Le%20Bras%20and%20David%20A.%20Robb%20and%20Mike%20J.%20Chantler%20and%20Stefano%20Padilla&entry.1292438233=%20%20Stopword%20removal%20is%20a%20critical%20stage%20in%20many%20Machine%20Learning%20methods%20but%0Aoften%20receives%20little%20consideration%2C%20it%20interferes%20with%20the%20model%0Avisualizations%20and%20disrupts%20user%20confidence.%20Inappropriately%20chosen%20or%20hastily%0Aomitted%20stopwords%20not%20only%20lead%20to%20suboptimal%20performance%20but%20also%0Asignificantly%20affect%20the%20quality%20of%20models%2C%20thus%20reducing%20the%20willingness%20of%0Apractitioners%20and%20stakeholders%20to%20rely%20on%20the%20output%20visualizations.%20This%20paper%0Aproposes%20a%20novel%20extraction%20method%20that%20provides%20a%20corpus-specific%0Aprobabilistic%20estimation%20of%20stopword%20likelihood%20and%20an%20interactive%0Avisualization%20system%20to%20support%20their%20analysis.%20We%20evaluated%20our%20approach%20and%0Ainterface%20using%20real-world%20data%2C%20a%20commonly%20used%20Machine%20Learning%20method%20%28Topic%0AModelling%29%2C%20and%20a%20comprehensive%20qualitative%20experiment%20probing%20user%20confidence.%0AThe%20results%20of%20our%20work%20show%20that%20our%20system%20increases%20user%20confidence%20in%20the%0Acredibility%20of%20topic%20models%20by%20%281%29%20returning%20reasonable%20probabilities%2C%20%282%29%0Agenerating%20an%20appropriate%20and%20representative%20extension%20of%20common%20stopword%0Alists%2C%20and%20%283%29%20providing%20an%20adjustable%20threshold%20for%20estimating%20and%20analyzing%0Astopwords%20visually.%20Finally%2C%20we%20discuss%20insights%2C%20recommendations%2C%20and%20best%0Apractices%20to%20support%20practitioners%20while%20improving%20the%20output%20of%20Machine%0ALearning%20methods%20and%20topic%20model%20visualizations%20with%20robust%20stopword%20analysis%0Aand%20removal.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10137v1&entry.124074799=Read"},
{"title": "Topology-Driven Attribute Recovery for Attribute Missing Graph Learning\n  in Social Internet of Things", "author": "Mengran Li and Junzhou Chen and Chenyun Yu and Guanying Jiang and Ronghui Zhang and Yanming Shen and Houbing Herbert Song", "abstract": "  With the advancement of information technology, the Social Internet of Things\n(SIoT) has fostered the integration of physical devices and social networks,\ndeepening the study of complex interaction patterns. Text Attribute Graphs\n(TAGs) capture both topological structures and semantic attributes, enhancing\nthe analysis of complex interactions within the SIoT. However, existing graph\nlearning methods are typically designed for complete attributed graphs, and the\ncommon issue of missing attributes in Attribute Missing Graphs (AMGs) increases\nthe difficulty of analysis tasks. To address this, we propose the\nTopology-Driven Attribute Recovery (TDAR) framework, which leverages\ntopological data for AMG learning. TDAR introduces an improved pre-filling\nmethod for initial attribute recovery using native graph topology.\nAdditionally, it dynamically adjusts propagation weights and incorporates\nhomogeneity strategies within the embedding space to suit AMGs' unique\ntopological structures, effectively reducing noise during information\npropagation. Extensive experiments on public datasets demonstrate that TDAR\nsignificantly outperforms state-of-the-art methods in attribute reconstruction\nand downstream tasks, offering a robust solution to the challenges posed by\nAMGs. The code is available at https://github.com/limengran98/TDAR.\n", "link": "http://arxiv.org/abs/2501.10151v1", "date": "2025-01-17", "relevancy": 2.4054, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4902}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.477}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topology-Driven%20Attribute%20Recovery%20for%20Attribute%20Missing%20Graph%20Learning%0A%20%20in%20Social%20Internet%20of%20Things&body=Title%3A%20Topology-Driven%20Attribute%20Recovery%20for%20Attribute%20Missing%20Graph%20Learning%0A%20%20in%20Social%20Internet%20of%20Things%0AAuthor%3A%20Mengran%20Li%20and%20Junzhou%20Chen%20and%20Chenyun%20Yu%20and%20Guanying%20Jiang%20and%20Ronghui%20Zhang%20and%20Yanming%20Shen%20and%20Houbing%20Herbert%20Song%0AAbstract%3A%20%20%20With%20the%20advancement%20of%20information%20technology%2C%20the%20Social%20Internet%20of%20Things%0A%28SIoT%29%20has%20fostered%20the%20integration%20of%20physical%20devices%20and%20social%20networks%2C%0Adeepening%20the%20study%20of%20complex%20interaction%20patterns.%20Text%20Attribute%20Graphs%0A%28TAGs%29%20capture%20both%20topological%20structures%20and%20semantic%20attributes%2C%20enhancing%0Athe%20analysis%20of%20complex%20interactions%20within%20the%20SIoT.%20However%2C%20existing%20graph%0Alearning%20methods%20are%20typically%20designed%20for%20complete%20attributed%20graphs%2C%20and%20the%0Acommon%20issue%20of%20missing%20attributes%20in%20Attribute%20Missing%20Graphs%20%28AMGs%29%20increases%0Athe%20difficulty%20of%20analysis%20tasks.%20To%20address%20this%2C%20we%20propose%20the%0ATopology-Driven%20Attribute%20Recovery%20%28TDAR%29%20framework%2C%20which%20leverages%0Atopological%20data%20for%20AMG%20learning.%20TDAR%20introduces%20an%20improved%20pre-filling%0Amethod%20for%20initial%20attribute%20recovery%20using%20native%20graph%20topology.%0AAdditionally%2C%20it%20dynamically%20adjusts%20propagation%20weights%20and%20incorporates%0Ahomogeneity%20strategies%20within%20the%20embedding%20space%20to%20suit%20AMGs%27%20unique%0Atopological%20structures%2C%20effectively%20reducing%20noise%20during%20information%0Apropagation.%20Extensive%20experiments%20on%20public%20datasets%20demonstrate%20that%20TDAR%0Asignificantly%20outperforms%20state-of-the-art%20methods%20in%20attribute%20reconstruction%0Aand%20downstream%20tasks%2C%20offering%20a%20robust%20solution%20to%20the%20challenges%20posed%20by%0AAMGs.%20The%20code%20is%20available%20at%20https%3A//github.com/limengran98/TDAR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10151v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopology-Driven%2520Attribute%2520Recovery%2520for%2520Attribute%2520Missing%2520Graph%2520Learning%250A%2520%2520in%2520Social%2520Internet%2520of%2520Things%26entry.906535625%3DMengran%2520Li%2520and%2520Junzhou%2520Chen%2520and%2520Chenyun%2520Yu%2520and%2520Guanying%2520Jiang%2520and%2520Ronghui%2520Zhang%2520and%2520Yanming%2520Shen%2520and%2520Houbing%2520Herbert%2520Song%26entry.1292438233%3D%2520%2520With%2520the%2520advancement%2520of%2520information%2520technology%252C%2520the%2520Social%2520Internet%2520of%2520Things%250A%2528SIoT%2529%2520has%2520fostered%2520the%2520integration%2520of%2520physical%2520devices%2520and%2520social%2520networks%252C%250Adeepening%2520the%2520study%2520of%2520complex%2520interaction%2520patterns.%2520Text%2520Attribute%2520Graphs%250A%2528TAGs%2529%2520capture%2520both%2520topological%2520structures%2520and%2520semantic%2520attributes%252C%2520enhancing%250Athe%2520analysis%2520of%2520complex%2520interactions%2520within%2520the%2520SIoT.%2520However%252C%2520existing%2520graph%250Alearning%2520methods%2520are%2520typically%2520designed%2520for%2520complete%2520attributed%2520graphs%252C%2520and%2520the%250Acommon%2520issue%2520of%2520missing%2520attributes%2520in%2520Attribute%2520Missing%2520Graphs%2520%2528AMGs%2529%2520increases%250Athe%2520difficulty%2520of%2520analysis%2520tasks.%2520To%2520address%2520this%252C%2520we%2520propose%2520the%250ATopology-Driven%2520Attribute%2520Recovery%2520%2528TDAR%2529%2520framework%252C%2520which%2520leverages%250Atopological%2520data%2520for%2520AMG%2520learning.%2520TDAR%2520introduces%2520an%2520improved%2520pre-filling%250Amethod%2520for%2520initial%2520attribute%2520recovery%2520using%2520native%2520graph%2520topology.%250AAdditionally%252C%2520it%2520dynamically%2520adjusts%2520propagation%2520weights%2520and%2520incorporates%250Ahomogeneity%2520strategies%2520within%2520the%2520embedding%2520space%2520to%2520suit%2520AMGs%2527%2520unique%250Atopological%2520structures%252C%2520effectively%2520reducing%2520noise%2520during%2520information%250Apropagation.%2520Extensive%2520experiments%2520on%2520public%2520datasets%2520demonstrate%2520that%2520TDAR%250Asignificantly%2520outperforms%2520state-of-the-art%2520methods%2520in%2520attribute%2520reconstruction%250Aand%2520downstream%2520tasks%252C%2520offering%2520a%2520robust%2520solution%2520to%2520the%2520challenges%2520posed%2520by%250AAMGs.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/limengran98/TDAR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10151v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topology-Driven%20Attribute%20Recovery%20for%20Attribute%20Missing%20Graph%20Learning%0A%20%20in%20Social%20Internet%20of%20Things&entry.906535625=Mengran%20Li%20and%20Junzhou%20Chen%20and%20Chenyun%20Yu%20and%20Guanying%20Jiang%20and%20Ronghui%20Zhang%20and%20Yanming%20Shen%20and%20Houbing%20Herbert%20Song&entry.1292438233=%20%20With%20the%20advancement%20of%20information%20technology%2C%20the%20Social%20Internet%20of%20Things%0A%28SIoT%29%20has%20fostered%20the%20integration%20of%20physical%20devices%20and%20social%20networks%2C%0Adeepening%20the%20study%20of%20complex%20interaction%20patterns.%20Text%20Attribute%20Graphs%0A%28TAGs%29%20capture%20both%20topological%20structures%20and%20semantic%20attributes%2C%20enhancing%0Athe%20analysis%20of%20complex%20interactions%20within%20the%20SIoT.%20However%2C%20existing%20graph%0Alearning%20methods%20are%20typically%20designed%20for%20complete%20attributed%20graphs%2C%20and%20the%0Acommon%20issue%20of%20missing%20attributes%20in%20Attribute%20Missing%20Graphs%20%28AMGs%29%20increases%0Athe%20difficulty%20of%20analysis%20tasks.%20To%20address%20this%2C%20we%20propose%20the%0ATopology-Driven%20Attribute%20Recovery%20%28TDAR%29%20framework%2C%20which%20leverages%0Atopological%20data%20for%20AMG%20learning.%20TDAR%20introduces%20an%20improved%20pre-filling%0Amethod%20for%20initial%20attribute%20recovery%20using%20native%20graph%20topology.%0AAdditionally%2C%20it%20dynamically%20adjusts%20propagation%20weights%20and%20incorporates%0Ahomogeneity%20strategies%20within%20the%20embedding%20space%20to%20suit%20AMGs%27%20unique%0Atopological%20structures%2C%20effectively%20reducing%20noise%20during%20information%0Apropagation.%20Extensive%20experiments%20on%20public%20datasets%20demonstrate%20that%20TDAR%0Asignificantly%20outperforms%20state-of-the-art%20methods%20in%20attribute%20reconstruction%0Aand%20downstream%20tasks%2C%20offering%20a%20robust%20solution%20to%20the%20challenges%20posed%20by%0AAMGs.%20The%20code%20is%20available%20at%20https%3A//github.com/limengran98/TDAR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10151v1&entry.124074799=Read"},
{"title": "Expression Prompt Collaboration Transformer for Universal Referring\n  Video Object Segmentation", "author": "Jiajun Chen and Jiacheng Lin and Guojin Zhong and Haolong Fu and Ke Nai and Kailun Yang and Zhiyong Li", "abstract": "  Audio-guided Video Object Segmentation (A-VOS) and Referring Video Object\nSegmentation (R-VOS) are two highly related tasks that both aim to segment\nspecific objects from video sequences according to expression prompts. However,\ndue to the challenges of modeling representations for different modalities,\nexisting methods struggle to strike a balance between interaction flexibility\nand localization precision. In this paper, we address this problem from two\nperspectives: the alignment of audio and text and the deep interaction among\naudio, text, and visual modalities. First, we propose a universal architecture,\nthe Expression Prompt Collaboration Transformer, herein EPCFormer. Next, we\npropose an Expression Alignment (EA) mechanism for audio and text. The proposed\nEPCFormer exploits the fact that audio and text prompts referring to the same\nobjects are semantically equivalent by using contrastive learning for both\ntypes of expressions. Then, to facilitate deep interactions among audio, text,\nand visual modalities, we introduce an Expression-Visual Attention (EVA)\nmodule. The knowledge of video object segmentation in terms of the expression\nprompts can seamlessly transfer between the two tasks by deeply exploring\ncomplementary cues between text and audio. Experiments on well-recognized\nbenchmarks demonstrate that our EPCFormer attains state-of-the-art results on\nboth tasks. The source code will be made publicly available at\nhttps://github.com/lab206/EPCFormer.\n", "link": "http://arxiv.org/abs/2308.04162v2", "date": "2025-01-17", "relevancy": 2.4031, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6053}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6053}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5784}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Expression%20Prompt%20Collaboration%20Transformer%20for%20Universal%20Referring%0A%20%20Video%20Object%20Segmentation&body=Title%3A%20Expression%20Prompt%20Collaboration%20Transformer%20for%20Universal%20Referring%0A%20%20Video%20Object%20Segmentation%0AAuthor%3A%20Jiajun%20Chen%20and%20Jiacheng%20Lin%20and%20Guojin%20Zhong%20and%20Haolong%20Fu%20and%20Ke%20Nai%20and%20Kailun%20Yang%20and%20Zhiyong%20Li%0AAbstract%3A%20%20%20Audio-guided%20Video%20Object%20Segmentation%20%28A-VOS%29%20and%20Referring%20Video%20Object%0ASegmentation%20%28R-VOS%29%20are%20two%20highly%20related%20tasks%20that%20both%20aim%20to%20segment%0Aspecific%20objects%20from%20video%20sequences%20according%20to%20expression%20prompts.%20However%2C%0Adue%20to%20the%20challenges%20of%20modeling%20representations%20for%20different%20modalities%2C%0Aexisting%20methods%20struggle%20to%20strike%20a%20balance%20between%20interaction%20flexibility%0Aand%20localization%20precision.%20In%20this%20paper%2C%20we%20address%20this%20problem%20from%20two%0Aperspectives%3A%20the%20alignment%20of%20audio%20and%20text%20and%20the%20deep%20interaction%20among%0Aaudio%2C%20text%2C%20and%20visual%20modalities.%20First%2C%20we%20propose%20a%20universal%20architecture%2C%0Athe%20Expression%20Prompt%20Collaboration%20Transformer%2C%20herein%20EPCFormer.%20Next%2C%20we%0Apropose%20an%20Expression%20Alignment%20%28EA%29%20mechanism%20for%20audio%20and%20text.%20The%20proposed%0AEPCFormer%20exploits%20the%20fact%20that%20audio%20and%20text%20prompts%20referring%20to%20the%20same%0Aobjects%20are%20semantically%20equivalent%20by%20using%20contrastive%20learning%20for%20both%0Atypes%20of%20expressions.%20Then%2C%20to%20facilitate%20deep%20interactions%20among%20audio%2C%20text%2C%0Aand%20visual%20modalities%2C%20we%20introduce%20an%20Expression-Visual%20Attention%20%28EVA%29%0Amodule.%20The%20knowledge%20of%20video%20object%20segmentation%20in%20terms%20of%20the%20expression%0Aprompts%20can%20seamlessly%20transfer%20between%20the%20two%20tasks%20by%20deeply%20exploring%0Acomplementary%20cues%20between%20text%20and%20audio.%20Experiments%20on%20well-recognized%0Abenchmarks%20demonstrate%20that%20our%20EPCFormer%20attains%20state-of-the-art%20results%20on%0Aboth%20tasks.%20The%20source%20code%20will%20be%20made%20publicly%20available%20at%0Ahttps%3A//github.com/lab206/EPCFormer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.04162v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExpression%2520Prompt%2520Collaboration%2520Transformer%2520for%2520Universal%2520Referring%250A%2520%2520Video%2520Object%2520Segmentation%26entry.906535625%3DJiajun%2520Chen%2520and%2520Jiacheng%2520Lin%2520and%2520Guojin%2520Zhong%2520and%2520Haolong%2520Fu%2520and%2520Ke%2520Nai%2520and%2520Kailun%2520Yang%2520and%2520Zhiyong%2520Li%26entry.1292438233%3D%2520%2520Audio-guided%2520Video%2520Object%2520Segmentation%2520%2528A-VOS%2529%2520and%2520Referring%2520Video%2520Object%250ASegmentation%2520%2528R-VOS%2529%2520are%2520two%2520highly%2520related%2520tasks%2520that%2520both%2520aim%2520to%2520segment%250Aspecific%2520objects%2520from%2520video%2520sequences%2520according%2520to%2520expression%2520prompts.%2520However%252C%250Adue%2520to%2520the%2520challenges%2520of%2520modeling%2520representations%2520for%2520different%2520modalities%252C%250Aexisting%2520methods%2520struggle%2520to%2520strike%2520a%2520balance%2520between%2520interaction%2520flexibility%250Aand%2520localization%2520precision.%2520In%2520this%2520paper%252C%2520we%2520address%2520this%2520problem%2520from%2520two%250Aperspectives%253A%2520the%2520alignment%2520of%2520audio%2520and%2520text%2520and%2520the%2520deep%2520interaction%2520among%250Aaudio%252C%2520text%252C%2520and%2520visual%2520modalities.%2520First%252C%2520we%2520propose%2520a%2520universal%2520architecture%252C%250Athe%2520Expression%2520Prompt%2520Collaboration%2520Transformer%252C%2520herein%2520EPCFormer.%2520Next%252C%2520we%250Apropose%2520an%2520Expression%2520Alignment%2520%2528EA%2529%2520mechanism%2520for%2520audio%2520and%2520text.%2520The%2520proposed%250AEPCFormer%2520exploits%2520the%2520fact%2520that%2520audio%2520and%2520text%2520prompts%2520referring%2520to%2520the%2520same%250Aobjects%2520are%2520semantically%2520equivalent%2520by%2520using%2520contrastive%2520learning%2520for%2520both%250Atypes%2520of%2520expressions.%2520Then%252C%2520to%2520facilitate%2520deep%2520interactions%2520among%2520audio%252C%2520text%252C%250Aand%2520visual%2520modalities%252C%2520we%2520introduce%2520an%2520Expression-Visual%2520Attention%2520%2528EVA%2529%250Amodule.%2520The%2520knowledge%2520of%2520video%2520object%2520segmentation%2520in%2520terms%2520of%2520the%2520expression%250Aprompts%2520can%2520seamlessly%2520transfer%2520between%2520the%2520two%2520tasks%2520by%2520deeply%2520exploring%250Acomplementary%2520cues%2520between%2520text%2520and%2520audio.%2520Experiments%2520on%2520well-recognized%250Abenchmarks%2520demonstrate%2520that%2520our%2520EPCFormer%2520attains%2520state-of-the-art%2520results%2520on%250Aboth%2520tasks.%2520The%2520source%2520code%2520will%2520be%2520made%2520publicly%2520available%2520at%250Ahttps%253A//github.com/lab206/EPCFormer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.04162v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Expression%20Prompt%20Collaboration%20Transformer%20for%20Universal%20Referring%0A%20%20Video%20Object%20Segmentation&entry.906535625=Jiajun%20Chen%20and%20Jiacheng%20Lin%20and%20Guojin%20Zhong%20and%20Haolong%20Fu%20and%20Ke%20Nai%20and%20Kailun%20Yang%20and%20Zhiyong%20Li&entry.1292438233=%20%20Audio-guided%20Video%20Object%20Segmentation%20%28A-VOS%29%20and%20Referring%20Video%20Object%0ASegmentation%20%28R-VOS%29%20are%20two%20highly%20related%20tasks%20that%20both%20aim%20to%20segment%0Aspecific%20objects%20from%20video%20sequences%20according%20to%20expression%20prompts.%20However%2C%0Adue%20to%20the%20challenges%20of%20modeling%20representations%20for%20different%20modalities%2C%0Aexisting%20methods%20struggle%20to%20strike%20a%20balance%20between%20interaction%20flexibility%0Aand%20localization%20precision.%20In%20this%20paper%2C%20we%20address%20this%20problem%20from%20two%0Aperspectives%3A%20the%20alignment%20of%20audio%20and%20text%20and%20the%20deep%20interaction%20among%0Aaudio%2C%20text%2C%20and%20visual%20modalities.%20First%2C%20we%20propose%20a%20universal%20architecture%2C%0Athe%20Expression%20Prompt%20Collaboration%20Transformer%2C%20herein%20EPCFormer.%20Next%2C%20we%0Apropose%20an%20Expression%20Alignment%20%28EA%29%20mechanism%20for%20audio%20and%20text.%20The%20proposed%0AEPCFormer%20exploits%20the%20fact%20that%20audio%20and%20text%20prompts%20referring%20to%20the%20same%0Aobjects%20are%20semantically%20equivalent%20by%20using%20contrastive%20learning%20for%20both%0Atypes%20of%20expressions.%20Then%2C%20to%20facilitate%20deep%20interactions%20among%20audio%2C%20text%2C%0Aand%20visual%20modalities%2C%20we%20introduce%20an%20Expression-Visual%20Attention%20%28EVA%29%0Amodule.%20The%20knowledge%20of%20video%20object%20segmentation%20in%20terms%20of%20the%20expression%0Aprompts%20can%20seamlessly%20transfer%20between%20the%20two%20tasks%20by%20deeply%20exploring%0Acomplementary%20cues%20between%20text%20and%20audio.%20Experiments%20on%20well-recognized%0Abenchmarks%20demonstrate%20that%20our%20EPCFormer%20attains%20state-of-the-art%20results%20on%0Aboth%20tasks.%20The%20source%20code%20will%20be%20made%20publicly%20available%20at%0Ahttps%3A//github.com/lab206/EPCFormer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.04162v2&entry.124074799=Read"},
{"title": "ColNet: Collaborative Optimization in Decentralized Federated Multi-task\n  Learning Systems", "author": "Chao Feng and Nicolas Fazli Kohler and Alberto Huertas Celdran and Gerome Bovet and Burkhard Stiller", "abstract": "  The integration of Federated Learning (FL) and Multi-Task Learning (MTL) has\nbeen explored to address client heterogeneity, with Federated Multi-Task\nLearning (FMTL) treating each client as a distinct task. However, most existing\nresearch focuses on data heterogeneity (e.g., addressing non-IID data) rather\nthan task heterogeneity, where clients solve fundamentally different tasks.\nAdditionally, much of the work relies on centralized settings with a server\nmanaging the federation, leaving the more challenging domain of decentralized\nFMTL largely unexplored. Thus, this work bridges this gap by proposing ColNet,\na framework designed for heterogeneous tasks in decentralized federated\nenvironments. ColNet divides models into the backbone and task-specific layers,\nforming groups of similar clients, with group leaders performing\nconflict-averse cross-group aggregation. A pool of experiments with different\nfederations demonstrated ColNet outperforms the compared aggregation schemes in\ndecentralized settings with label and task heterogeneity scenarios.\n", "link": "http://arxiv.org/abs/2501.10347v1", "date": "2025-01-17", "relevancy": 2.3955, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4866}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4851}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ColNet%3A%20Collaborative%20Optimization%20in%20Decentralized%20Federated%20Multi-task%0A%20%20Learning%20Systems&body=Title%3A%20ColNet%3A%20Collaborative%20Optimization%20in%20Decentralized%20Federated%20Multi-task%0A%20%20Learning%20Systems%0AAuthor%3A%20Chao%20Feng%20and%20Nicolas%20Fazli%20Kohler%20and%20Alberto%20Huertas%20Celdran%20and%20Gerome%20Bovet%20and%20Burkhard%20Stiller%0AAbstract%3A%20%20%20The%20integration%20of%20Federated%20Learning%20%28FL%29%20and%20Multi-Task%20Learning%20%28MTL%29%20has%0Abeen%20explored%20to%20address%20client%20heterogeneity%2C%20with%20Federated%20Multi-Task%0ALearning%20%28FMTL%29%20treating%20each%20client%20as%20a%20distinct%20task.%20However%2C%20most%20existing%0Aresearch%20focuses%20on%20data%20heterogeneity%20%28e.g.%2C%20addressing%20non-IID%20data%29%20rather%0Athan%20task%20heterogeneity%2C%20where%20clients%20solve%20fundamentally%20different%20tasks.%0AAdditionally%2C%20much%20of%20the%20work%20relies%20on%20centralized%20settings%20with%20a%20server%0Amanaging%20the%20federation%2C%20leaving%20the%20more%20challenging%20domain%20of%20decentralized%0AFMTL%20largely%20unexplored.%20Thus%2C%20this%20work%20bridges%20this%20gap%20by%20proposing%20ColNet%2C%0Aa%20framework%20designed%20for%20heterogeneous%20tasks%20in%20decentralized%20federated%0Aenvironments.%20ColNet%20divides%20models%20into%20the%20backbone%20and%20task-specific%20layers%2C%0Aforming%20groups%20of%20similar%20clients%2C%20with%20group%20leaders%20performing%0Aconflict-averse%20cross-group%20aggregation.%20A%20pool%20of%20experiments%20with%20different%0Afederations%20demonstrated%20ColNet%20outperforms%20the%20compared%20aggregation%20schemes%20in%0Adecentralized%20settings%20with%20label%20and%20task%20heterogeneity%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10347v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DColNet%253A%2520Collaborative%2520Optimization%2520in%2520Decentralized%2520Federated%2520Multi-task%250A%2520%2520Learning%2520Systems%26entry.906535625%3DChao%2520Feng%2520and%2520Nicolas%2520Fazli%2520Kohler%2520and%2520Alberto%2520Huertas%2520Celdran%2520and%2520Gerome%2520Bovet%2520and%2520Burkhard%2520Stiller%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520Federated%2520Learning%2520%2528FL%2529%2520and%2520Multi-Task%2520Learning%2520%2528MTL%2529%2520has%250Abeen%2520explored%2520to%2520address%2520client%2520heterogeneity%252C%2520with%2520Federated%2520Multi-Task%250ALearning%2520%2528FMTL%2529%2520treating%2520each%2520client%2520as%2520a%2520distinct%2520task.%2520However%252C%2520most%2520existing%250Aresearch%2520focuses%2520on%2520data%2520heterogeneity%2520%2528e.g.%252C%2520addressing%2520non-IID%2520data%2529%2520rather%250Athan%2520task%2520heterogeneity%252C%2520where%2520clients%2520solve%2520fundamentally%2520different%2520tasks.%250AAdditionally%252C%2520much%2520of%2520the%2520work%2520relies%2520on%2520centralized%2520settings%2520with%2520a%2520server%250Amanaging%2520the%2520federation%252C%2520leaving%2520the%2520more%2520challenging%2520domain%2520of%2520decentralized%250AFMTL%2520largely%2520unexplored.%2520Thus%252C%2520this%2520work%2520bridges%2520this%2520gap%2520by%2520proposing%2520ColNet%252C%250Aa%2520framework%2520designed%2520for%2520heterogeneous%2520tasks%2520in%2520decentralized%2520federated%250Aenvironments.%2520ColNet%2520divides%2520models%2520into%2520the%2520backbone%2520and%2520task-specific%2520layers%252C%250Aforming%2520groups%2520of%2520similar%2520clients%252C%2520with%2520group%2520leaders%2520performing%250Aconflict-averse%2520cross-group%2520aggregation.%2520A%2520pool%2520of%2520experiments%2520with%2520different%250Afederations%2520demonstrated%2520ColNet%2520outperforms%2520the%2520compared%2520aggregation%2520schemes%2520in%250Adecentralized%2520settings%2520with%2520label%2520and%2520task%2520heterogeneity%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10347v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ColNet%3A%20Collaborative%20Optimization%20in%20Decentralized%20Federated%20Multi-task%0A%20%20Learning%20Systems&entry.906535625=Chao%20Feng%20and%20Nicolas%20Fazli%20Kohler%20and%20Alberto%20Huertas%20Celdran%20and%20Gerome%20Bovet%20and%20Burkhard%20Stiller&entry.1292438233=%20%20The%20integration%20of%20Federated%20Learning%20%28FL%29%20and%20Multi-Task%20Learning%20%28MTL%29%20has%0Abeen%20explored%20to%20address%20client%20heterogeneity%2C%20with%20Federated%20Multi-Task%0ALearning%20%28FMTL%29%20treating%20each%20client%20as%20a%20distinct%20task.%20However%2C%20most%20existing%0Aresearch%20focuses%20on%20data%20heterogeneity%20%28e.g.%2C%20addressing%20non-IID%20data%29%20rather%0Athan%20task%20heterogeneity%2C%20where%20clients%20solve%20fundamentally%20different%20tasks.%0AAdditionally%2C%20much%20of%20the%20work%20relies%20on%20centralized%20settings%20with%20a%20server%0Amanaging%20the%20federation%2C%20leaving%20the%20more%20challenging%20domain%20of%20decentralized%0AFMTL%20largely%20unexplored.%20Thus%2C%20this%20work%20bridges%20this%20gap%20by%20proposing%20ColNet%2C%0Aa%20framework%20designed%20for%20heterogeneous%20tasks%20in%20decentralized%20federated%0Aenvironments.%20ColNet%20divides%20models%20into%20the%20backbone%20and%20task-specific%20layers%2C%0Aforming%20groups%20of%20similar%20clients%2C%20with%20group%20leaders%20performing%0Aconflict-averse%20cross-group%20aggregation.%20A%20pool%20of%20experiments%20with%20different%0Afederations%20demonstrated%20ColNet%20outperforms%20the%20compared%20aggregation%20schemes%20in%0Adecentralized%20settings%20with%20label%20and%20task%20heterogeneity%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10347v1&entry.124074799=Read"},
{"title": "Large Process Models: A Vision for Business Process Management in the\n  Age of Generative AI", "author": "Timotheus Kampik and Christian Warmuth and Adrian Rebmann and Ron Agam and Lukas N. P. Egger and Andreas Gerber and Johannes Hoffart and Jonas Kolk and Philipp Herzig and Gero Decker and Han van der Aa and Artem Polyvyanyy and Stefanie Rinderle-Ma and Ingo Weber and Matthias Weidlich", "abstract": "  The continued success of Large Language Models (LLMs) and other generative\nartificial intelligence approaches highlights the advantages that large\ninformation corpora can have over rigidly defined symbolic models, but also\nserves as a proof-point of the challenges that purely statistics-based\napproaches have in terms of safety and trustworthiness. As a framework for\ncontextualizing the potential, as well as the limitations of LLMs and other\nfoundation model-based technologies, we propose the concept of a Large Process\nModel (LPM) that combines the correlation power of LLMs with the analytical\nprecision and reliability of knowledge-based systems and automated reasoning\napproaches. LPMs are envisioned to directly utilize the wealth of process\nmanagement experience that experts have accumulated, as well as process\nperformance data of organizations with diverse characteristics, e.g.,\\\nregarding size, region, or industry. In this vision, the proposed LPM would\nallow organizations to receive context-specific (tailored) process and other\nbusiness models, analytical deep-dives, and improvement recommendations. As\nsuch, they would allow to substantially decrease the time and effort required\nfor business transformation, while also allowing for deeper, more impactful,\nand more actionable insights than previously possible. We argue that\nimplementing an LPM is feasible, but also highlight limitations and research\nchallenges that need to be solved to implement particular aspects of the LPM\nvision.\n", "link": "http://arxiv.org/abs/2309.00900v3", "date": "2025-01-17", "relevancy": 2.3922, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4832}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4832}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4689}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Process%20Models%3A%20A%20Vision%20for%20Business%20Process%20Management%20in%20the%0A%20%20Age%20of%20Generative%20AI&body=Title%3A%20Large%20Process%20Models%3A%20A%20Vision%20for%20Business%20Process%20Management%20in%20the%0A%20%20Age%20of%20Generative%20AI%0AAuthor%3A%20Timotheus%20Kampik%20and%20Christian%20Warmuth%20and%20Adrian%20Rebmann%20and%20Ron%20Agam%20and%20Lukas%20N.%20P.%20Egger%20and%20Andreas%20Gerber%20and%20Johannes%20Hoffart%20and%20Jonas%20Kolk%20and%20Philipp%20Herzig%20and%20Gero%20Decker%20and%20Han%20van%20der%20Aa%20and%20Artem%20Polyvyanyy%20and%20Stefanie%20Rinderle-Ma%20and%20Ingo%20Weber%20and%20Matthias%20Weidlich%0AAbstract%3A%20%20%20The%20continued%20success%20of%20Large%20Language%20Models%20%28LLMs%29%20and%20other%20generative%0Aartificial%20intelligence%20approaches%20highlights%20the%20advantages%20that%20large%0Ainformation%20corpora%20can%20have%20over%20rigidly%20defined%20symbolic%20models%2C%20but%20also%0Aserves%20as%20a%20proof-point%20of%20the%20challenges%20that%20purely%20statistics-based%0Aapproaches%20have%20in%20terms%20of%20safety%20and%20trustworthiness.%20As%20a%20framework%20for%0Acontextualizing%20the%20potential%2C%20as%20well%20as%20the%20limitations%20of%20LLMs%20and%20other%0Afoundation%20model-based%20technologies%2C%20we%20propose%20the%20concept%20of%20a%20Large%20Process%0AModel%20%28LPM%29%20that%20combines%20the%20correlation%20power%20of%20LLMs%20with%20the%20analytical%0Aprecision%20and%20reliability%20of%20knowledge-based%20systems%20and%20automated%20reasoning%0Aapproaches.%20LPMs%20are%20envisioned%20to%20directly%20utilize%20the%20wealth%20of%20process%0Amanagement%20experience%20that%20experts%20have%20accumulated%2C%20as%20well%20as%20process%0Aperformance%20data%20of%20organizations%20with%20diverse%20characteristics%2C%20e.g.%2C%5C%0Aregarding%20size%2C%20region%2C%20or%20industry.%20In%20this%20vision%2C%20the%20proposed%20LPM%20would%0Aallow%20organizations%20to%20receive%20context-specific%20%28tailored%29%20process%20and%20other%0Abusiness%20models%2C%20analytical%20deep-dives%2C%20and%20improvement%20recommendations.%20As%0Asuch%2C%20they%20would%20allow%20to%20substantially%20decrease%20the%20time%20and%20effort%20required%0Afor%20business%20transformation%2C%20while%20also%20allowing%20for%20deeper%2C%20more%20impactful%2C%0Aand%20more%20actionable%20insights%20than%20previously%20possible.%20We%20argue%20that%0Aimplementing%20an%20LPM%20is%20feasible%2C%20but%20also%20highlight%20limitations%20and%20research%0Achallenges%20that%20need%20to%20be%20solved%20to%20implement%20particular%20aspects%20of%20the%20LPM%0Avision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.00900v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Process%2520Models%253A%2520A%2520Vision%2520for%2520Business%2520Process%2520Management%2520in%2520the%250A%2520%2520Age%2520of%2520Generative%2520AI%26entry.906535625%3DTimotheus%2520Kampik%2520and%2520Christian%2520Warmuth%2520and%2520Adrian%2520Rebmann%2520and%2520Ron%2520Agam%2520and%2520Lukas%2520N.%2520P.%2520Egger%2520and%2520Andreas%2520Gerber%2520and%2520Johannes%2520Hoffart%2520and%2520Jonas%2520Kolk%2520and%2520Philipp%2520Herzig%2520and%2520Gero%2520Decker%2520and%2520Han%2520van%2520der%2520Aa%2520and%2520Artem%2520Polyvyanyy%2520and%2520Stefanie%2520Rinderle-Ma%2520and%2520Ingo%2520Weber%2520and%2520Matthias%2520Weidlich%26entry.1292438233%3D%2520%2520The%2520continued%2520success%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520and%2520other%2520generative%250Aartificial%2520intelligence%2520approaches%2520highlights%2520the%2520advantages%2520that%2520large%250Ainformation%2520corpora%2520can%2520have%2520over%2520rigidly%2520defined%2520symbolic%2520models%252C%2520but%2520also%250Aserves%2520as%2520a%2520proof-point%2520of%2520the%2520challenges%2520that%2520purely%2520statistics-based%250Aapproaches%2520have%2520in%2520terms%2520of%2520safety%2520and%2520trustworthiness.%2520As%2520a%2520framework%2520for%250Acontextualizing%2520the%2520potential%252C%2520as%2520well%2520as%2520the%2520limitations%2520of%2520LLMs%2520and%2520other%250Afoundation%2520model-based%2520technologies%252C%2520we%2520propose%2520the%2520concept%2520of%2520a%2520Large%2520Process%250AModel%2520%2528LPM%2529%2520that%2520combines%2520the%2520correlation%2520power%2520of%2520LLMs%2520with%2520the%2520analytical%250Aprecision%2520and%2520reliability%2520of%2520knowledge-based%2520systems%2520and%2520automated%2520reasoning%250Aapproaches.%2520LPMs%2520are%2520envisioned%2520to%2520directly%2520utilize%2520the%2520wealth%2520of%2520process%250Amanagement%2520experience%2520that%2520experts%2520have%2520accumulated%252C%2520as%2520well%2520as%2520process%250Aperformance%2520data%2520of%2520organizations%2520with%2520diverse%2520characteristics%252C%2520e.g.%252C%255C%250Aregarding%2520size%252C%2520region%252C%2520or%2520industry.%2520In%2520this%2520vision%252C%2520the%2520proposed%2520LPM%2520would%250Aallow%2520organizations%2520to%2520receive%2520context-specific%2520%2528tailored%2529%2520process%2520and%2520other%250Abusiness%2520models%252C%2520analytical%2520deep-dives%252C%2520and%2520improvement%2520recommendations.%2520As%250Asuch%252C%2520they%2520would%2520allow%2520to%2520substantially%2520decrease%2520the%2520time%2520and%2520effort%2520required%250Afor%2520business%2520transformation%252C%2520while%2520also%2520allowing%2520for%2520deeper%252C%2520more%2520impactful%252C%250Aand%2520more%2520actionable%2520insights%2520than%2520previously%2520possible.%2520We%2520argue%2520that%250Aimplementing%2520an%2520LPM%2520is%2520feasible%252C%2520but%2520also%2520highlight%2520limitations%2520and%2520research%250Achallenges%2520that%2520need%2520to%2520be%2520solved%2520to%2520implement%2520particular%2520aspects%2520of%2520the%2520LPM%250Avision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.00900v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Process%20Models%3A%20A%20Vision%20for%20Business%20Process%20Management%20in%20the%0A%20%20Age%20of%20Generative%20AI&entry.906535625=Timotheus%20Kampik%20and%20Christian%20Warmuth%20and%20Adrian%20Rebmann%20and%20Ron%20Agam%20and%20Lukas%20N.%20P.%20Egger%20and%20Andreas%20Gerber%20and%20Johannes%20Hoffart%20and%20Jonas%20Kolk%20and%20Philipp%20Herzig%20and%20Gero%20Decker%20and%20Han%20van%20der%20Aa%20and%20Artem%20Polyvyanyy%20and%20Stefanie%20Rinderle-Ma%20and%20Ingo%20Weber%20and%20Matthias%20Weidlich&entry.1292438233=%20%20The%20continued%20success%20of%20Large%20Language%20Models%20%28LLMs%29%20and%20other%20generative%0Aartificial%20intelligence%20approaches%20highlights%20the%20advantages%20that%20large%0Ainformation%20corpora%20can%20have%20over%20rigidly%20defined%20symbolic%20models%2C%20but%20also%0Aserves%20as%20a%20proof-point%20of%20the%20challenges%20that%20purely%20statistics-based%0Aapproaches%20have%20in%20terms%20of%20safety%20and%20trustworthiness.%20As%20a%20framework%20for%0Acontextualizing%20the%20potential%2C%20as%20well%20as%20the%20limitations%20of%20LLMs%20and%20other%0Afoundation%20model-based%20technologies%2C%20we%20propose%20the%20concept%20of%20a%20Large%20Process%0AModel%20%28LPM%29%20that%20combines%20the%20correlation%20power%20of%20LLMs%20with%20the%20analytical%0Aprecision%20and%20reliability%20of%20knowledge-based%20systems%20and%20automated%20reasoning%0Aapproaches.%20LPMs%20are%20envisioned%20to%20directly%20utilize%20the%20wealth%20of%20process%0Amanagement%20experience%20that%20experts%20have%20accumulated%2C%20as%20well%20as%20process%0Aperformance%20data%20of%20organizations%20with%20diverse%20characteristics%2C%20e.g.%2C%5C%0Aregarding%20size%2C%20region%2C%20or%20industry.%20In%20this%20vision%2C%20the%20proposed%20LPM%20would%0Aallow%20organizations%20to%20receive%20context-specific%20%28tailored%29%20process%20and%20other%0Abusiness%20models%2C%20analytical%20deep-dives%2C%20and%20improvement%20recommendations.%20As%0Asuch%2C%20they%20would%20allow%20to%20substantially%20decrease%20the%20time%20and%20effort%20required%0Afor%20business%20transformation%2C%20while%20also%20allowing%20for%20deeper%2C%20more%20impactful%2C%0Aand%20more%20actionable%20insights%20than%20previously%20possible.%20We%20argue%20that%0Aimplementing%20an%20LPM%20is%20feasible%2C%20but%20also%20highlight%20limitations%20and%20research%0Achallenges%20that%20need%20to%20be%20solved%20to%20implement%20particular%20aspects%20of%20the%20LPM%0Avision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.00900v3&entry.124074799=Read"},
{"title": "Piece of Table: A Divide-and-Conquer Approach for Selecting Sub-Tables\n  in Table Question Answering", "author": "Wonjin Lee and Kyumin Kim and Sungjae Lee and Jihun Lee and Kwang In Kim", "abstract": "  Applying language models (LMs) to tables is challenging due to the inherent\nstructural differences between two-dimensional tables and one-dimensional text\nfor which the LMs were originally designed. Furthermore, when applying\nlinearized tables to LMs, the maximum token lengths often imposed in\nself-attention calculations make it difficult to comprehensively understand the\ncontext spread across large tables. To address these challenges, we present\nPieTa (Piece of Table), a new framework for sub-table-based question answering\n(QA). PieTa operates through an iterative process of dividing tables into\nsmaller windows, using LMs to select relevant cells within each window, and\nmerging these cells into a sub-table. This multi-resolution approach captures\ndependencies across multiple rows and columns while avoiding the limitations\ncaused by long context inputs. Instantiated as a simple iterative sub-table\nunion algorithm, PieTa demonstrates improved performance over previous\nsub-table-based QA approaches.\n", "link": "http://arxiv.org/abs/2412.07629v3", "date": "2025-01-17", "relevancy": 2.3548, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4724}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4724}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Piece%20of%20Table%3A%20A%20Divide-and-Conquer%20Approach%20for%20Selecting%20Sub-Tables%0A%20%20in%20Table%20Question%20Answering&body=Title%3A%20Piece%20of%20Table%3A%20A%20Divide-and-Conquer%20Approach%20for%20Selecting%20Sub-Tables%0A%20%20in%20Table%20Question%20Answering%0AAuthor%3A%20Wonjin%20Lee%20and%20Kyumin%20Kim%20and%20Sungjae%20Lee%20and%20Jihun%20Lee%20and%20Kwang%20In%20Kim%0AAbstract%3A%20%20%20Applying%20language%20models%20%28LMs%29%20to%20tables%20is%20challenging%20due%20to%20the%20inherent%0Astructural%20differences%20between%20two-dimensional%20tables%20and%20one-dimensional%20text%0Afor%20which%20the%20LMs%20were%20originally%20designed.%20Furthermore%2C%20when%20applying%0Alinearized%20tables%20to%20LMs%2C%20the%20maximum%20token%20lengths%20often%20imposed%20in%0Aself-attention%20calculations%20make%20it%20difficult%20to%20comprehensively%20understand%20the%0Acontext%20spread%20across%20large%20tables.%20To%20address%20these%20challenges%2C%20we%20present%0APieTa%20%28Piece%20of%20Table%29%2C%20a%20new%20framework%20for%20sub-table-based%20question%20answering%0A%28QA%29.%20PieTa%20operates%20through%20an%20iterative%20process%20of%20dividing%20tables%20into%0Asmaller%20windows%2C%20using%20LMs%20to%20select%20relevant%20cells%20within%20each%20window%2C%20and%0Amerging%20these%20cells%20into%20a%20sub-table.%20This%20multi-resolution%20approach%20captures%0Adependencies%20across%20multiple%20rows%20and%20columns%20while%20avoiding%20the%20limitations%0Acaused%20by%20long%20context%20inputs.%20Instantiated%20as%20a%20simple%20iterative%20sub-table%0Aunion%20algorithm%2C%20PieTa%20demonstrates%20improved%20performance%20over%20previous%0Asub-table-based%20QA%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.07629v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPiece%2520of%2520Table%253A%2520A%2520Divide-and-Conquer%2520Approach%2520for%2520Selecting%2520Sub-Tables%250A%2520%2520in%2520Table%2520Question%2520Answering%26entry.906535625%3DWonjin%2520Lee%2520and%2520Kyumin%2520Kim%2520and%2520Sungjae%2520Lee%2520and%2520Jihun%2520Lee%2520and%2520Kwang%2520In%2520Kim%26entry.1292438233%3D%2520%2520Applying%2520language%2520models%2520%2528LMs%2529%2520to%2520tables%2520is%2520challenging%2520due%2520to%2520the%2520inherent%250Astructural%2520differences%2520between%2520two-dimensional%2520tables%2520and%2520one-dimensional%2520text%250Afor%2520which%2520the%2520LMs%2520were%2520originally%2520designed.%2520Furthermore%252C%2520when%2520applying%250Alinearized%2520tables%2520to%2520LMs%252C%2520the%2520maximum%2520token%2520lengths%2520often%2520imposed%2520in%250Aself-attention%2520calculations%2520make%2520it%2520difficult%2520to%2520comprehensively%2520understand%2520the%250Acontext%2520spread%2520across%2520large%2520tables.%2520To%2520address%2520these%2520challenges%252C%2520we%2520present%250APieTa%2520%2528Piece%2520of%2520Table%2529%252C%2520a%2520new%2520framework%2520for%2520sub-table-based%2520question%2520answering%250A%2528QA%2529.%2520PieTa%2520operates%2520through%2520an%2520iterative%2520process%2520of%2520dividing%2520tables%2520into%250Asmaller%2520windows%252C%2520using%2520LMs%2520to%2520select%2520relevant%2520cells%2520within%2520each%2520window%252C%2520and%250Amerging%2520these%2520cells%2520into%2520a%2520sub-table.%2520This%2520multi-resolution%2520approach%2520captures%250Adependencies%2520across%2520multiple%2520rows%2520and%2520columns%2520while%2520avoiding%2520the%2520limitations%250Acaused%2520by%2520long%2520context%2520inputs.%2520Instantiated%2520as%2520a%2520simple%2520iterative%2520sub-table%250Aunion%2520algorithm%252C%2520PieTa%2520demonstrates%2520improved%2520performance%2520over%2520previous%250Asub-table-based%2520QA%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.07629v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Piece%20of%20Table%3A%20A%20Divide-and-Conquer%20Approach%20for%20Selecting%20Sub-Tables%0A%20%20in%20Table%20Question%20Answering&entry.906535625=Wonjin%20Lee%20and%20Kyumin%20Kim%20and%20Sungjae%20Lee%20and%20Jihun%20Lee%20and%20Kwang%20In%20Kim&entry.1292438233=%20%20Applying%20language%20models%20%28LMs%29%20to%20tables%20is%20challenging%20due%20to%20the%20inherent%0Astructural%20differences%20between%20two-dimensional%20tables%20and%20one-dimensional%20text%0Afor%20which%20the%20LMs%20were%20originally%20designed.%20Furthermore%2C%20when%20applying%0Alinearized%20tables%20to%20LMs%2C%20the%20maximum%20token%20lengths%20often%20imposed%20in%0Aself-attention%20calculations%20make%20it%20difficult%20to%20comprehensively%20understand%20the%0Acontext%20spread%20across%20large%20tables.%20To%20address%20these%20challenges%2C%20we%20present%0APieTa%20%28Piece%20of%20Table%29%2C%20a%20new%20framework%20for%20sub-table-based%20question%20answering%0A%28QA%29.%20PieTa%20operates%20through%20an%20iterative%20process%20of%20dividing%20tables%20into%0Asmaller%20windows%2C%20using%20LMs%20to%20select%20relevant%20cells%20within%20each%20window%2C%20and%0Amerging%20these%20cells%20into%20a%20sub-table.%20This%20multi-resolution%20approach%20captures%0Adependencies%20across%20multiple%20rows%20and%20columns%20while%20avoiding%20the%20limitations%0Acaused%20by%20long%20context%20inputs.%20Instantiated%20as%20a%20simple%20iterative%20sub-table%0Aunion%20algorithm%2C%20PieTa%20demonstrates%20improved%20performance%20over%20previous%0Asub-table-based%20QA%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.07629v3&entry.124074799=Read"},
{"title": "A Simple but Effective Closed-form Solution for Extreme Multi-label\n  Learning", "author": "Kazuma Onishi and Katsuhiko Hayashi", "abstract": "  Extreme multi-label learning (XML) is a task of assigning multiple labels\nfrom an extremely large set of labels to each data instance. Many current\nhigh-performance XML models are composed of a lot of hyperparameters, which\ncomplicates the tuning process. Additionally, the models themselves are adapted\nspecifically to XML, which complicates their reimplementation. To remedy this\nproblem, we propose a simple method based on ridge regression for XML. The\nproposed method not only has a closed-form solution but also is composed of a\nsingle hyperparameter. Since there are no precedents on applying ridge\nregression to XML, this paper verified the performance of the method by using\nvarious XML benchmark datasets. Furthermore, we enhanced the prediction of\nlow-frequency labels in XML, which hold informative content. This prediction is\nessential yet challenging because of the limited amount of data. Here, we\nemployed a simple frequency-based weighting. This approach greatly simplifies\nthe process compared with existing techniques. Experimental results revealed\nthat it can achieve levels of performance comparable to, or even exceeding,\nthose of models with numerous hyperparameters. Additionally, we found that the\nfrequency-based weighting significantly improved the predictive performance for\nlow-frequency labels, while requiring almost no changes in implementation. The\nsource code for the proposed method is available on github at\nhttps://github.com/cars1015/XML-ridge.\n", "link": "http://arxiv.org/abs/2501.10179v1", "date": "2025-01-17", "relevancy": 2.3543, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4817}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4698}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Simple%20but%20Effective%20Closed-form%20Solution%20for%20Extreme%20Multi-label%0A%20%20Learning&body=Title%3A%20A%20Simple%20but%20Effective%20Closed-form%20Solution%20for%20Extreme%20Multi-label%0A%20%20Learning%0AAuthor%3A%20Kazuma%20Onishi%20and%20Katsuhiko%20Hayashi%0AAbstract%3A%20%20%20Extreme%20multi-label%20learning%20%28XML%29%20is%20a%20task%20of%20assigning%20multiple%20labels%0Afrom%20an%20extremely%20large%20set%20of%20labels%20to%20each%20data%20instance.%20Many%20current%0Ahigh-performance%20XML%20models%20are%20composed%20of%20a%20lot%20of%20hyperparameters%2C%20which%0Acomplicates%20the%20tuning%20process.%20Additionally%2C%20the%20models%20themselves%20are%20adapted%0Aspecifically%20to%20XML%2C%20which%20complicates%20their%20reimplementation.%20To%20remedy%20this%0Aproblem%2C%20we%20propose%20a%20simple%20method%20based%20on%20ridge%20regression%20for%20XML.%20The%0Aproposed%20method%20not%20only%20has%20a%20closed-form%20solution%20but%20also%20is%20composed%20of%20a%0Asingle%20hyperparameter.%20Since%20there%20are%20no%20precedents%20on%20applying%20ridge%0Aregression%20to%20XML%2C%20this%20paper%20verified%20the%20performance%20of%20the%20method%20by%20using%0Avarious%20XML%20benchmark%20datasets.%20Furthermore%2C%20we%20enhanced%20the%20prediction%20of%0Alow-frequency%20labels%20in%20XML%2C%20which%20hold%20informative%20content.%20This%20prediction%20is%0Aessential%20yet%20challenging%20because%20of%20the%20limited%20amount%20of%20data.%20Here%2C%20we%0Aemployed%20a%20simple%20frequency-based%20weighting.%20This%20approach%20greatly%20simplifies%0Athe%20process%20compared%20with%20existing%20techniques.%20Experimental%20results%20revealed%0Athat%20it%20can%20achieve%20levels%20of%20performance%20comparable%20to%2C%20or%20even%20exceeding%2C%0Athose%20of%20models%20with%20numerous%20hyperparameters.%20Additionally%2C%20we%20found%20that%20the%0Afrequency-based%20weighting%20significantly%20improved%20the%20predictive%20performance%20for%0Alow-frequency%20labels%2C%20while%20requiring%20almost%20no%20changes%20in%20implementation.%20The%0Asource%20code%20for%20the%20proposed%20method%20is%20available%20on%20github%20at%0Ahttps%3A//github.com/cars1015/XML-ridge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10179v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Simple%2520but%2520Effective%2520Closed-form%2520Solution%2520for%2520Extreme%2520Multi-label%250A%2520%2520Learning%26entry.906535625%3DKazuma%2520Onishi%2520and%2520Katsuhiko%2520Hayashi%26entry.1292438233%3D%2520%2520Extreme%2520multi-label%2520learning%2520%2528XML%2529%2520is%2520a%2520task%2520of%2520assigning%2520multiple%2520labels%250Afrom%2520an%2520extremely%2520large%2520set%2520of%2520labels%2520to%2520each%2520data%2520instance.%2520Many%2520current%250Ahigh-performance%2520XML%2520models%2520are%2520composed%2520of%2520a%2520lot%2520of%2520hyperparameters%252C%2520which%250Acomplicates%2520the%2520tuning%2520process.%2520Additionally%252C%2520the%2520models%2520themselves%2520are%2520adapted%250Aspecifically%2520to%2520XML%252C%2520which%2520complicates%2520their%2520reimplementation.%2520To%2520remedy%2520this%250Aproblem%252C%2520we%2520propose%2520a%2520simple%2520method%2520based%2520on%2520ridge%2520regression%2520for%2520XML.%2520The%250Aproposed%2520method%2520not%2520only%2520has%2520a%2520closed-form%2520solution%2520but%2520also%2520is%2520composed%2520of%2520a%250Asingle%2520hyperparameter.%2520Since%2520there%2520are%2520no%2520precedents%2520on%2520applying%2520ridge%250Aregression%2520to%2520XML%252C%2520this%2520paper%2520verified%2520the%2520performance%2520of%2520the%2520method%2520by%2520using%250Avarious%2520XML%2520benchmark%2520datasets.%2520Furthermore%252C%2520we%2520enhanced%2520the%2520prediction%2520of%250Alow-frequency%2520labels%2520in%2520XML%252C%2520which%2520hold%2520informative%2520content.%2520This%2520prediction%2520is%250Aessential%2520yet%2520challenging%2520because%2520of%2520the%2520limited%2520amount%2520of%2520data.%2520Here%252C%2520we%250Aemployed%2520a%2520simple%2520frequency-based%2520weighting.%2520This%2520approach%2520greatly%2520simplifies%250Athe%2520process%2520compared%2520with%2520existing%2520techniques.%2520Experimental%2520results%2520revealed%250Athat%2520it%2520can%2520achieve%2520levels%2520of%2520performance%2520comparable%2520to%252C%2520or%2520even%2520exceeding%252C%250Athose%2520of%2520models%2520with%2520numerous%2520hyperparameters.%2520Additionally%252C%2520we%2520found%2520that%2520the%250Afrequency-based%2520weighting%2520significantly%2520improved%2520the%2520predictive%2520performance%2520for%250Alow-frequency%2520labels%252C%2520while%2520requiring%2520almost%2520no%2520changes%2520in%2520implementation.%2520The%250Asource%2520code%2520for%2520the%2520proposed%2520method%2520is%2520available%2520on%2520github%2520at%250Ahttps%253A//github.com/cars1015/XML-ridge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10179v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Simple%20but%20Effective%20Closed-form%20Solution%20for%20Extreme%20Multi-label%0A%20%20Learning&entry.906535625=Kazuma%20Onishi%20and%20Katsuhiko%20Hayashi&entry.1292438233=%20%20Extreme%20multi-label%20learning%20%28XML%29%20is%20a%20task%20of%20assigning%20multiple%20labels%0Afrom%20an%20extremely%20large%20set%20of%20labels%20to%20each%20data%20instance.%20Many%20current%0Ahigh-performance%20XML%20models%20are%20composed%20of%20a%20lot%20of%20hyperparameters%2C%20which%0Acomplicates%20the%20tuning%20process.%20Additionally%2C%20the%20models%20themselves%20are%20adapted%0Aspecifically%20to%20XML%2C%20which%20complicates%20their%20reimplementation.%20To%20remedy%20this%0Aproblem%2C%20we%20propose%20a%20simple%20method%20based%20on%20ridge%20regression%20for%20XML.%20The%0Aproposed%20method%20not%20only%20has%20a%20closed-form%20solution%20but%20also%20is%20composed%20of%20a%0Asingle%20hyperparameter.%20Since%20there%20are%20no%20precedents%20on%20applying%20ridge%0Aregression%20to%20XML%2C%20this%20paper%20verified%20the%20performance%20of%20the%20method%20by%20using%0Avarious%20XML%20benchmark%20datasets.%20Furthermore%2C%20we%20enhanced%20the%20prediction%20of%0Alow-frequency%20labels%20in%20XML%2C%20which%20hold%20informative%20content.%20This%20prediction%20is%0Aessential%20yet%20challenging%20because%20of%20the%20limited%20amount%20of%20data.%20Here%2C%20we%0Aemployed%20a%20simple%20frequency-based%20weighting.%20This%20approach%20greatly%20simplifies%0Athe%20process%20compared%20with%20existing%20techniques.%20Experimental%20results%20revealed%0Athat%20it%20can%20achieve%20levels%20of%20performance%20comparable%20to%2C%20or%20even%20exceeding%2C%0Athose%20of%20models%20with%20numerous%20hyperparameters.%20Additionally%2C%20we%20found%20that%20the%0Afrequency-based%20weighting%20significantly%20improved%20the%20predictive%20performance%20for%0Alow-frequency%20labels%2C%20while%20requiring%20almost%20no%20changes%20in%20implementation.%20The%0Asource%20code%20for%20the%20proposed%20method%20is%20available%20on%20github%20at%0Ahttps%3A//github.com/cars1015/XML-ridge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10179v1&entry.124074799=Read"},
{"title": "LayerAnimate: Layer-specific Control for Animation", "author": "Yuxue Yang and Lue Fan and Zuzeng Lin and Feng Wang and Zhaoxiang Zhang", "abstract": "  Animated video separates foreground and background elements into layers, with\ndistinct processes for sketching, refining, coloring, and in-betweening.\nExisting video generation methods typically treat animation as a monolithic\ndata domain, lacking fine-grained control over individual layers. In this\npaper, we introduce LayerAnimate, a novel architectural approach that enhances\nfine-grained control over individual animation layers within a video diffusion\nmodel, allowing users to independently manipulate foreground and background\nelements in distinct layers. To address the challenge of limited layer-specific\ndata, we propose a data curation pipeline that features automated element\nsegmentation, motion-state hierarchical merging, and motion coherence\nrefinement. Through quantitative and qualitative comparisons, and user study,\nwe demonstrate that LayerAnimate outperforms current methods in terms of\nanimation quality, control precision, and usability, making it an ideal tool\nfor both professional animators and amateur enthusiasts. This framework opens\nup new possibilities for layer-specific animation applications and creative\nflexibility. Our code is available at https://layeranimate.github.io.\n", "link": "http://arxiv.org/abs/2501.08295v2", "date": "2025-01-17", "relevancy": 2.3205, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5884}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5814}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5713}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LayerAnimate%3A%20Layer-specific%20Control%20for%20Animation&body=Title%3A%20LayerAnimate%3A%20Layer-specific%20Control%20for%20Animation%0AAuthor%3A%20Yuxue%20Yang%20and%20Lue%20Fan%20and%20Zuzeng%20Lin%20and%20Feng%20Wang%20and%20Zhaoxiang%20Zhang%0AAbstract%3A%20%20%20Animated%20video%20separates%20foreground%20and%20background%20elements%20into%20layers%2C%20with%0Adistinct%20processes%20for%20sketching%2C%20refining%2C%20coloring%2C%20and%20in-betweening.%0AExisting%20video%20generation%20methods%20typically%20treat%20animation%20as%20a%20monolithic%0Adata%20domain%2C%20lacking%20fine-grained%20control%20over%20individual%20layers.%20In%20this%0Apaper%2C%20we%20introduce%20LayerAnimate%2C%20a%20novel%20architectural%20approach%20that%20enhances%0Afine-grained%20control%20over%20individual%20animation%20layers%20within%20a%20video%20diffusion%0Amodel%2C%20allowing%20users%20to%20independently%20manipulate%20foreground%20and%20background%0Aelements%20in%20distinct%20layers.%20To%20address%20the%20challenge%20of%20limited%20layer-specific%0Adata%2C%20we%20propose%20a%20data%20curation%20pipeline%20that%20features%20automated%20element%0Asegmentation%2C%20motion-state%20hierarchical%20merging%2C%20and%20motion%20coherence%0Arefinement.%20Through%20quantitative%20and%20qualitative%20comparisons%2C%20and%20user%20study%2C%0Awe%20demonstrate%20that%20LayerAnimate%20outperforms%20current%20methods%20in%20terms%20of%0Aanimation%20quality%2C%20control%20precision%2C%20and%20usability%2C%20making%20it%20an%20ideal%20tool%0Afor%20both%20professional%20animators%20and%20amateur%20enthusiasts.%20This%20framework%20opens%0Aup%20new%20possibilities%20for%20layer-specific%20animation%20applications%20and%20creative%0Aflexibility.%20Our%20code%20is%20available%20at%20https%3A//layeranimate.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08295v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLayerAnimate%253A%2520Layer-specific%2520Control%2520for%2520Animation%26entry.906535625%3DYuxue%2520Yang%2520and%2520Lue%2520Fan%2520and%2520Zuzeng%2520Lin%2520and%2520Feng%2520Wang%2520and%2520Zhaoxiang%2520Zhang%26entry.1292438233%3D%2520%2520Animated%2520video%2520separates%2520foreground%2520and%2520background%2520elements%2520into%2520layers%252C%2520with%250Adistinct%2520processes%2520for%2520sketching%252C%2520refining%252C%2520coloring%252C%2520and%2520in-betweening.%250AExisting%2520video%2520generation%2520methods%2520typically%2520treat%2520animation%2520as%2520a%2520monolithic%250Adata%2520domain%252C%2520lacking%2520fine-grained%2520control%2520over%2520individual%2520layers.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520LayerAnimate%252C%2520a%2520novel%2520architectural%2520approach%2520that%2520enhances%250Afine-grained%2520control%2520over%2520individual%2520animation%2520layers%2520within%2520a%2520video%2520diffusion%250Amodel%252C%2520allowing%2520users%2520to%2520independently%2520manipulate%2520foreground%2520and%2520background%250Aelements%2520in%2520distinct%2520layers.%2520To%2520address%2520the%2520challenge%2520of%2520limited%2520layer-specific%250Adata%252C%2520we%2520propose%2520a%2520data%2520curation%2520pipeline%2520that%2520features%2520automated%2520element%250Asegmentation%252C%2520motion-state%2520hierarchical%2520merging%252C%2520and%2520motion%2520coherence%250Arefinement.%2520Through%2520quantitative%2520and%2520qualitative%2520comparisons%252C%2520and%2520user%2520study%252C%250Awe%2520demonstrate%2520that%2520LayerAnimate%2520outperforms%2520current%2520methods%2520in%2520terms%2520of%250Aanimation%2520quality%252C%2520control%2520precision%252C%2520and%2520usability%252C%2520making%2520it%2520an%2520ideal%2520tool%250Afor%2520both%2520professional%2520animators%2520and%2520amateur%2520enthusiasts.%2520This%2520framework%2520opens%250Aup%2520new%2520possibilities%2520for%2520layer-specific%2520animation%2520applications%2520and%2520creative%250Aflexibility.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//layeranimate.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08295v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LayerAnimate%3A%20Layer-specific%20Control%20for%20Animation&entry.906535625=Yuxue%20Yang%20and%20Lue%20Fan%20and%20Zuzeng%20Lin%20and%20Feng%20Wang%20and%20Zhaoxiang%20Zhang&entry.1292438233=%20%20Animated%20video%20separates%20foreground%20and%20background%20elements%20into%20layers%2C%20with%0Adistinct%20processes%20for%20sketching%2C%20refining%2C%20coloring%2C%20and%20in-betweening.%0AExisting%20video%20generation%20methods%20typically%20treat%20animation%20as%20a%20monolithic%0Adata%20domain%2C%20lacking%20fine-grained%20control%20over%20individual%20layers.%20In%20this%0Apaper%2C%20we%20introduce%20LayerAnimate%2C%20a%20novel%20architectural%20approach%20that%20enhances%0Afine-grained%20control%20over%20individual%20animation%20layers%20within%20a%20video%20diffusion%0Amodel%2C%20allowing%20users%20to%20independently%20manipulate%20foreground%20and%20background%0Aelements%20in%20distinct%20layers.%20To%20address%20the%20challenge%20of%20limited%20layer-specific%0Adata%2C%20we%20propose%20a%20data%20curation%20pipeline%20that%20features%20automated%20element%0Asegmentation%2C%20motion-state%20hierarchical%20merging%2C%20and%20motion%20coherence%0Arefinement.%20Through%20quantitative%20and%20qualitative%20comparisons%2C%20and%20user%20study%2C%0Awe%20demonstrate%20that%20LayerAnimate%20outperforms%20current%20methods%20in%20terms%20of%0Aanimation%20quality%2C%20control%20precision%2C%20and%20usability%2C%20making%20it%20an%20ideal%20tool%0Afor%20both%20professional%20animators%20and%20amateur%20enthusiasts.%20This%20framework%20opens%0Aup%20new%20possibilities%20for%20layer-specific%20animation%20applications%20and%20creative%0Aflexibility.%20Our%20code%20is%20available%20at%20https%3A//layeranimate.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08295v2&entry.124074799=Read"},
{"title": "HiMix: Reducing Computational Complexity in Large Vision-Language Models", "author": "Xuange Zhang and Dengjie Li and Bo Liu and Zenghao Bao and Yao Zhou and Baisong Yang and Zhongying Liu and Yujie Zhong and Zheng Zhao and Tongtong Yuan", "abstract": "  Benefiting from recent advancements in large language models and modality\nalignment techniques, existing Large Vision-Language Models(LVLMs) have\nachieved prominent performance across a wide range of scenarios. However, the\nexcessive computational complexity limits the widespread use of these models in\npractical applications. We argue that one main bottleneck in computational\ncomplexity is caused by the involvement of redundant vision sequences in model\ncomputation. This is inspired by a reassessment of the efficiency of vision and\nlanguage information transmission in the language decoder of LVLMs. Then, we\npropose a novel hierarchical vision-language interaction mechanism called\nHierarchical Vision injection for Mixture Attention (HiMix). In HiMix, only the\nlanguage sequence undergoes full forward propagation, while the vision sequence\ninteracts with the language at specific stages within each language decoder\nlayer. It is striking that our approach significantly reduces computational\ncomplexity with minimal performance loss. Specifically, HiMix achieves a 10x\nreduction in the computational cost of the language decoder across multiple\nLVLM models while maintaining comparable performance. This highlights the\nadvantages of our method, and we hope our research brings new perspectives to\nthe field of vision-language understanding. Project Page:\nhttps://xuange923.github.io/HiMix\n", "link": "http://arxiv.org/abs/2501.10318v1", "date": "2025-01-17", "relevancy": 2.3119, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5854}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5854}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HiMix%3A%20Reducing%20Computational%20Complexity%20in%20Large%20Vision-Language%20Models&body=Title%3A%20HiMix%3A%20Reducing%20Computational%20Complexity%20in%20Large%20Vision-Language%20Models%0AAuthor%3A%20Xuange%20Zhang%20and%20Dengjie%20Li%20and%20Bo%20Liu%20and%20Zenghao%20Bao%20and%20Yao%20Zhou%20and%20Baisong%20Yang%20and%20Zhongying%20Liu%20and%20Yujie%20Zhong%20and%20Zheng%20Zhao%20and%20Tongtong%20Yuan%0AAbstract%3A%20%20%20Benefiting%20from%20recent%20advancements%20in%20large%20language%20models%20and%20modality%0Aalignment%20techniques%2C%20existing%20Large%20Vision-Language%20Models%28LVLMs%29%20have%0Aachieved%20prominent%20performance%20across%20a%20wide%20range%20of%20scenarios.%20However%2C%20the%0Aexcessive%20computational%20complexity%20limits%20the%20widespread%20use%20of%20these%20models%20in%0Apractical%20applications.%20We%20argue%20that%20one%20main%20bottleneck%20in%20computational%0Acomplexity%20is%20caused%20by%20the%20involvement%20of%20redundant%20vision%20sequences%20in%20model%0Acomputation.%20This%20is%20inspired%20by%20a%20reassessment%20of%20the%20efficiency%20of%20vision%20and%0Alanguage%20information%20transmission%20in%20the%20language%20decoder%20of%20LVLMs.%20Then%2C%20we%0Apropose%20a%20novel%20hierarchical%20vision-language%20interaction%20mechanism%20called%0AHierarchical%20Vision%20injection%20for%20Mixture%20Attention%20%28HiMix%29.%20In%20HiMix%2C%20only%20the%0Alanguage%20sequence%20undergoes%20full%20forward%20propagation%2C%20while%20the%20vision%20sequence%0Ainteracts%20with%20the%20language%20at%20specific%20stages%20within%20each%20language%20decoder%0Alayer.%20It%20is%20striking%20that%20our%20approach%20significantly%20reduces%20computational%0Acomplexity%20with%20minimal%20performance%20loss.%20Specifically%2C%20HiMix%20achieves%20a%2010x%0Areduction%20in%20the%20computational%20cost%20of%20the%20language%20decoder%20across%20multiple%0ALVLM%20models%20while%20maintaining%20comparable%20performance.%20This%20highlights%20the%0Aadvantages%20of%20our%20method%2C%20and%20we%20hope%20our%20research%20brings%20new%20perspectives%20to%0Athe%20field%20of%20vision-language%20understanding.%20Project%20Page%3A%0Ahttps%3A//xuange923.github.io/HiMix%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10318v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHiMix%253A%2520Reducing%2520Computational%2520Complexity%2520in%2520Large%2520Vision-Language%2520Models%26entry.906535625%3DXuange%2520Zhang%2520and%2520Dengjie%2520Li%2520and%2520Bo%2520Liu%2520and%2520Zenghao%2520Bao%2520and%2520Yao%2520Zhou%2520and%2520Baisong%2520Yang%2520and%2520Zhongying%2520Liu%2520and%2520Yujie%2520Zhong%2520and%2520Zheng%2520Zhao%2520and%2520Tongtong%2520Yuan%26entry.1292438233%3D%2520%2520Benefiting%2520from%2520recent%2520advancements%2520in%2520large%2520language%2520models%2520and%2520modality%250Aalignment%2520techniques%252C%2520existing%2520Large%2520Vision-Language%2520Models%2528LVLMs%2529%2520have%250Aachieved%2520prominent%2520performance%2520across%2520a%2520wide%2520range%2520of%2520scenarios.%2520However%252C%2520the%250Aexcessive%2520computational%2520complexity%2520limits%2520the%2520widespread%2520use%2520of%2520these%2520models%2520in%250Apractical%2520applications.%2520We%2520argue%2520that%2520one%2520main%2520bottleneck%2520in%2520computational%250Acomplexity%2520is%2520caused%2520by%2520the%2520involvement%2520of%2520redundant%2520vision%2520sequences%2520in%2520model%250Acomputation.%2520This%2520is%2520inspired%2520by%2520a%2520reassessment%2520of%2520the%2520efficiency%2520of%2520vision%2520and%250Alanguage%2520information%2520transmission%2520in%2520the%2520language%2520decoder%2520of%2520LVLMs.%2520Then%252C%2520we%250Apropose%2520a%2520novel%2520hierarchical%2520vision-language%2520interaction%2520mechanism%2520called%250AHierarchical%2520Vision%2520injection%2520for%2520Mixture%2520Attention%2520%2528HiMix%2529.%2520In%2520HiMix%252C%2520only%2520the%250Alanguage%2520sequence%2520undergoes%2520full%2520forward%2520propagation%252C%2520while%2520the%2520vision%2520sequence%250Ainteracts%2520with%2520the%2520language%2520at%2520specific%2520stages%2520within%2520each%2520language%2520decoder%250Alayer.%2520It%2520is%2520striking%2520that%2520our%2520approach%2520significantly%2520reduces%2520computational%250Acomplexity%2520with%2520minimal%2520performance%2520loss.%2520Specifically%252C%2520HiMix%2520achieves%2520a%252010x%250Areduction%2520in%2520the%2520computational%2520cost%2520of%2520the%2520language%2520decoder%2520across%2520multiple%250ALVLM%2520models%2520while%2520maintaining%2520comparable%2520performance.%2520This%2520highlights%2520the%250Aadvantages%2520of%2520our%2520method%252C%2520and%2520we%2520hope%2520our%2520research%2520brings%2520new%2520perspectives%2520to%250Athe%2520field%2520of%2520vision-language%2520understanding.%2520Project%2520Page%253A%250Ahttps%253A//xuange923.github.io/HiMix%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10318v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HiMix%3A%20Reducing%20Computational%20Complexity%20in%20Large%20Vision-Language%20Models&entry.906535625=Xuange%20Zhang%20and%20Dengjie%20Li%20and%20Bo%20Liu%20and%20Zenghao%20Bao%20and%20Yao%20Zhou%20and%20Baisong%20Yang%20and%20Zhongying%20Liu%20and%20Yujie%20Zhong%20and%20Zheng%20Zhao%20and%20Tongtong%20Yuan&entry.1292438233=%20%20Benefiting%20from%20recent%20advancements%20in%20large%20language%20models%20and%20modality%0Aalignment%20techniques%2C%20existing%20Large%20Vision-Language%20Models%28LVLMs%29%20have%0Aachieved%20prominent%20performance%20across%20a%20wide%20range%20of%20scenarios.%20However%2C%20the%0Aexcessive%20computational%20complexity%20limits%20the%20widespread%20use%20of%20these%20models%20in%0Apractical%20applications.%20We%20argue%20that%20one%20main%20bottleneck%20in%20computational%0Acomplexity%20is%20caused%20by%20the%20involvement%20of%20redundant%20vision%20sequences%20in%20model%0Acomputation.%20This%20is%20inspired%20by%20a%20reassessment%20of%20the%20efficiency%20of%20vision%20and%0Alanguage%20information%20transmission%20in%20the%20language%20decoder%20of%20LVLMs.%20Then%2C%20we%0Apropose%20a%20novel%20hierarchical%20vision-language%20interaction%20mechanism%20called%0AHierarchical%20Vision%20injection%20for%20Mixture%20Attention%20%28HiMix%29.%20In%20HiMix%2C%20only%20the%0Alanguage%20sequence%20undergoes%20full%20forward%20propagation%2C%20while%20the%20vision%20sequence%0Ainteracts%20with%20the%20language%20at%20specific%20stages%20within%20each%20language%20decoder%0Alayer.%20It%20is%20striking%20that%20our%20approach%20significantly%20reduces%20computational%0Acomplexity%20with%20minimal%20performance%20loss.%20Specifically%2C%20HiMix%20achieves%20a%2010x%0Areduction%20in%20the%20computational%20cost%20of%20the%20language%20decoder%20across%20multiple%0ALVLM%20models%20while%20maintaining%20comparable%20performance.%20This%20highlights%20the%0Aadvantages%20of%20our%20method%2C%20and%20we%20hope%20our%20research%20brings%20new%20perspectives%20to%0Athe%20field%20of%20vision-language%20understanding.%20Project%20Page%3A%0Ahttps%3A//xuange923.github.io/HiMix%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10318v1&entry.124074799=Read"},
{"title": "STPOTR: Simultaneous Human Trajectory and Pose Prediction Using a\n  Non-Autoregressive Transformer for Robot Following Ahead", "author": "Mohammad Mahdavian and Payam Nikdel and Mahdi TaherAhmadi and Mo Chen", "abstract": "  In this paper, we develop a neural network model to predict future human\nmotion from an observed human motion history. We propose a non-autoregressive\ntransformer architecture to leverage its parallel nature for easier training\nand fast, accurate predictions at test time. The proposed architecture divides\nhuman motion prediction into two parts: 1) the human trajectory, which is the\nhip joint 3D position over time and 2) the human pose which is the all other\njoints 3D positions over time with respect to a fixed hip joint. We propose to\nmake the two predictions simultaneously, as the shared representation can\nimprove the model performance. Therefore, the model consists of two sets of\nencoders and decoders. First, a multi-head attention module applied to encoder\noutputs improves human trajectory. Second, another multi-head self-attention\nmodule applied to encoder outputs concatenated with decoder outputs facilitates\nlearning of temporal dependencies. Our model is well-suited for robotic\napplications in terms of test accuracy and speed, and compares favorably with\nrespect to state-of-the-art methods. We demonstrate the real-world\napplicability of our work via the Robot Follow-Ahead task, a challenging yet\npractical case study for our proposed model.\n", "link": "http://arxiv.org/abs/2209.07600v4", "date": "2025-01-17", "relevancy": 2.2894, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5991}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5799}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STPOTR%3A%20Simultaneous%20Human%20Trajectory%20and%20Pose%20Prediction%20Using%20a%0A%20%20Non-Autoregressive%20Transformer%20for%20Robot%20Following%20Ahead&body=Title%3A%20STPOTR%3A%20Simultaneous%20Human%20Trajectory%20and%20Pose%20Prediction%20Using%20a%0A%20%20Non-Autoregressive%20Transformer%20for%20Robot%20Following%20Ahead%0AAuthor%3A%20Mohammad%20Mahdavian%20and%20Payam%20Nikdel%20and%20Mahdi%20TaherAhmadi%20and%20Mo%20Chen%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20develop%20a%20neural%20network%20model%20to%20predict%20future%20human%0Amotion%20from%20an%20observed%20human%20motion%20history.%20We%20propose%20a%20non-autoregressive%0Atransformer%20architecture%20to%20leverage%20its%20parallel%20nature%20for%20easier%20training%0Aand%20fast%2C%20accurate%20predictions%20at%20test%20time.%20The%20proposed%20architecture%20divides%0Ahuman%20motion%20prediction%20into%20two%20parts%3A%201%29%20the%20human%20trajectory%2C%20which%20is%20the%0Ahip%20joint%203D%20position%20over%20time%20and%202%29%20the%20human%20pose%20which%20is%20the%20all%20other%0Ajoints%203D%20positions%20over%20time%20with%20respect%20to%20a%20fixed%20hip%20joint.%20We%20propose%20to%0Amake%20the%20two%20predictions%20simultaneously%2C%20as%20the%20shared%20representation%20can%0Aimprove%20the%20model%20performance.%20Therefore%2C%20the%20model%20consists%20of%20two%20sets%20of%0Aencoders%20and%20decoders.%20First%2C%20a%20multi-head%20attention%20module%20applied%20to%20encoder%0Aoutputs%20improves%20human%20trajectory.%20Second%2C%20another%20multi-head%20self-attention%0Amodule%20applied%20to%20encoder%20outputs%20concatenated%20with%20decoder%20outputs%20facilitates%0Alearning%20of%20temporal%20dependencies.%20Our%20model%20is%20well-suited%20for%20robotic%0Aapplications%20in%20terms%20of%20test%20accuracy%20and%20speed%2C%20and%20compares%20favorably%20with%0Arespect%20to%20state-of-the-art%20methods.%20We%20demonstrate%20the%20real-world%0Aapplicability%20of%20our%20work%20via%20the%20Robot%20Follow-Ahead%20task%2C%20a%20challenging%20yet%0Apractical%20case%20study%20for%20our%20proposed%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.07600v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTPOTR%253A%2520Simultaneous%2520Human%2520Trajectory%2520and%2520Pose%2520Prediction%2520Using%2520a%250A%2520%2520Non-Autoregressive%2520Transformer%2520for%2520Robot%2520Following%2520Ahead%26entry.906535625%3DMohammad%2520Mahdavian%2520and%2520Payam%2520Nikdel%2520and%2520Mahdi%2520TaherAhmadi%2520and%2520Mo%2520Chen%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520develop%2520a%2520neural%2520network%2520model%2520to%2520predict%2520future%2520human%250Amotion%2520from%2520an%2520observed%2520human%2520motion%2520history.%2520We%2520propose%2520a%2520non-autoregressive%250Atransformer%2520architecture%2520to%2520leverage%2520its%2520parallel%2520nature%2520for%2520easier%2520training%250Aand%2520fast%252C%2520accurate%2520predictions%2520at%2520test%2520time.%2520The%2520proposed%2520architecture%2520divides%250Ahuman%2520motion%2520prediction%2520into%2520two%2520parts%253A%25201%2529%2520the%2520human%2520trajectory%252C%2520which%2520is%2520the%250Ahip%2520joint%25203D%2520position%2520over%2520time%2520and%25202%2529%2520the%2520human%2520pose%2520which%2520is%2520the%2520all%2520other%250Ajoints%25203D%2520positions%2520over%2520time%2520with%2520respect%2520to%2520a%2520fixed%2520hip%2520joint.%2520We%2520propose%2520to%250Amake%2520the%2520two%2520predictions%2520simultaneously%252C%2520as%2520the%2520shared%2520representation%2520can%250Aimprove%2520the%2520model%2520performance.%2520Therefore%252C%2520the%2520model%2520consists%2520of%2520two%2520sets%2520of%250Aencoders%2520and%2520decoders.%2520First%252C%2520a%2520multi-head%2520attention%2520module%2520applied%2520to%2520encoder%250Aoutputs%2520improves%2520human%2520trajectory.%2520Second%252C%2520another%2520multi-head%2520self-attention%250Amodule%2520applied%2520to%2520encoder%2520outputs%2520concatenated%2520with%2520decoder%2520outputs%2520facilitates%250Alearning%2520of%2520temporal%2520dependencies.%2520Our%2520model%2520is%2520well-suited%2520for%2520robotic%250Aapplications%2520in%2520terms%2520of%2520test%2520accuracy%2520and%2520speed%252C%2520and%2520compares%2520favorably%2520with%250Arespect%2520to%2520state-of-the-art%2520methods.%2520We%2520demonstrate%2520the%2520real-world%250Aapplicability%2520of%2520our%2520work%2520via%2520the%2520Robot%2520Follow-Ahead%2520task%252C%2520a%2520challenging%2520yet%250Apractical%2520case%2520study%2520for%2520our%2520proposed%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2209.07600v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STPOTR%3A%20Simultaneous%20Human%20Trajectory%20and%20Pose%20Prediction%20Using%20a%0A%20%20Non-Autoregressive%20Transformer%20for%20Robot%20Following%20Ahead&entry.906535625=Mohammad%20Mahdavian%20and%20Payam%20Nikdel%20and%20Mahdi%20TaherAhmadi%20and%20Mo%20Chen&entry.1292438233=%20%20In%20this%20paper%2C%20we%20develop%20a%20neural%20network%20model%20to%20predict%20future%20human%0Amotion%20from%20an%20observed%20human%20motion%20history.%20We%20propose%20a%20non-autoregressive%0Atransformer%20architecture%20to%20leverage%20its%20parallel%20nature%20for%20easier%20training%0Aand%20fast%2C%20accurate%20predictions%20at%20test%20time.%20The%20proposed%20architecture%20divides%0Ahuman%20motion%20prediction%20into%20two%20parts%3A%201%29%20the%20human%20trajectory%2C%20which%20is%20the%0Ahip%20joint%203D%20position%20over%20time%20and%202%29%20the%20human%20pose%20which%20is%20the%20all%20other%0Ajoints%203D%20positions%20over%20time%20with%20respect%20to%20a%20fixed%20hip%20joint.%20We%20propose%20to%0Amake%20the%20two%20predictions%20simultaneously%2C%20as%20the%20shared%20representation%20can%0Aimprove%20the%20model%20performance.%20Therefore%2C%20the%20model%20consists%20of%20two%20sets%20of%0Aencoders%20and%20decoders.%20First%2C%20a%20multi-head%20attention%20module%20applied%20to%20encoder%0Aoutputs%20improves%20human%20trajectory.%20Second%2C%20another%20multi-head%20self-attention%0Amodule%20applied%20to%20encoder%20outputs%20concatenated%20with%20decoder%20outputs%20facilitates%0Alearning%20of%20temporal%20dependencies.%20Our%20model%20is%20well-suited%20for%20robotic%0Aapplications%20in%20terms%20of%20test%20accuracy%20and%20speed%2C%20and%20compares%20favorably%20with%0Arespect%20to%20state-of-the-art%20methods.%20We%20demonstrate%20the%20real-world%0Aapplicability%20of%20our%20work%20via%20the%20Robot%20Follow-Ahead%20task%2C%20a%20challenging%20yet%0Apractical%20case%20study%20for%20our%20proposed%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.07600v4&entry.124074799=Read"},
{"title": "Spatio-temporal Graph Learning on Adaptive Mined Key Frames for\n  High-performance Multi-Object Tracking", "author": "Futian Wang and Fengxiang Liu and Xiao Wang", "abstract": "  In the realm of multi-object tracking, the challenge of accurately capturing\nthe spatial and temporal relationships between objects in video sequences\nremains a significant hurdle. This is further complicated by frequent\noccurrences of mutual occlusions among objects, which can lead to tracking\nerrors and reduced performance in existing methods. Motivated by these\nchallenges, we propose a novel adaptive key frame mining strategy that\naddresses the limitations of current tracking approaches. Specifically, we\nintroduce a Key Frame Extraction (KFE) module that leverages reinforcement\nlearning to adaptively segment videos, thereby guiding the tracker to exploit\nthe intrinsic logic of the video content. This approach allows us to capture\nstructured spatial relationships between different objects as well as the\ntemporal relationships of objects across frames. To tackle the issue of object\nocclusions, we have developed an Intra-Frame Feature Fusion (IFF) module.\nUnlike traditional graph-based methods that primarily focus on inter-frame\nfeature fusion, our IFF module uses a Graph Convolutional Network (GCN) to\nfacilitate information exchange between the target and surrounding objects\nwithin a frame. This innovation significantly enhances target\ndistinguishability and mitigates tracking loss and appearance similarity due to\nocclusions. By combining the strengths of both long and short trajectories and\nconsidering the spatial relationships between objects, our proposed tracker\nachieves impressive results on the MOT17 dataset, i.e., 68.6 HOTA, 81.0 IDF1,\n66.6 AssA, and 893 IDS, proving its effectiveness and accuracy.\n", "link": "http://arxiv.org/abs/2501.10129v1", "date": "2025-01-17", "relevancy": 2.2758, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5788}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5672}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatio-temporal%20Graph%20Learning%20on%20Adaptive%20Mined%20Key%20Frames%20for%0A%20%20High-performance%20Multi-Object%20Tracking&body=Title%3A%20Spatio-temporal%20Graph%20Learning%20on%20Adaptive%20Mined%20Key%20Frames%20for%0A%20%20High-performance%20Multi-Object%20Tracking%0AAuthor%3A%20Futian%20Wang%20and%20Fengxiang%20Liu%20and%20Xiao%20Wang%0AAbstract%3A%20%20%20In%20the%20realm%20of%20multi-object%20tracking%2C%20the%20challenge%20of%20accurately%20capturing%0Athe%20spatial%20and%20temporal%20relationships%20between%20objects%20in%20video%20sequences%0Aremains%20a%20significant%20hurdle.%20This%20is%20further%20complicated%20by%20frequent%0Aoccurrences%20of%20mutual%20occlusions%20among%20objects%2C%20which%20can%20lead%20to%20tracking%0Aerrors%20and%20reduced%20performance%20in%20existing%20methods.%20Motivated%20by%20these%0Achallenges%2C%20we%20propose%20a%20novel%20adaptive%20key%20frame%20mining%20strategy%20that%0Aaddresses%20the%20limitations%20of%20current%20tracking%20approaches.%20Specifically%2C%20we%0Aintroduce%20a%20Key%20Frame%20Extraction%20%28KFE%29%20module%20that%20leverages%20reinforcement%0Alearning%20to%20adaptively%20segment%20videos%2C%20thereby%20guiding%20the%20tracker%20to%20exploit%0Athe%20intrinsic%20logic%20of%20the%20video%20content.%20This%20approach%20allows%20us%20to%20capture%0Astructured%20spatial%20relationships%20between%20different%20objects%20as%20well%20as%20the%0Atemporal%20relationships%20of%20objects%20across%20frames.%20To%20tackle%20the%20issue%20of%20object%0Aocclusions%2C%20we%20have%20developed%20an%20Intra-Frame%20Feature%20Fusion%20%28IFF%29%20module.%0AUnlike%20traditional%20graph-based%20methods%20that%20primarily%20focus%20on%20inter-frame%0Afeature%20fusion%2C%20our%20IFF%20module%20uses%20a%20Graph%20Convolutional%20Network%20%28GCN%29%20to%0Afacilitate%20information%20exchange%20between%20the%20target%20and%20surrounding%20objects%0Awithin%20a%20frame.%20This%20innovation%20significantly%20enhances%20target%0Adistinguishability%20and%20mitigates%20tracking%20loss%20and%20appearance%20similarity%20due%20to%0Aocclusions.%20By%20combining%20the%20strengths%20of%20both%20long%20and%20short%20trajectories%20and%0Aconsidering%20the%20spatial%20relationships%20between%20objects%2C%20our%20proposed%20tracker%0Aachieves%20impressive%20results%20on%20the%20MOT17%20dataset%2C%20i.e.%2C%2068.6%20HOTA%2C%2081.0%20IDF1%2C%0A66.6%20AssA%2C%20and%20893%20IDS%2C%20proving%20its%20effectiveness%20and%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10129v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatio-temporal%2520Graph%2520Learning%2520on%2520Adaptive%2520Mined%2520Key%2520Frames%2520for%250A%2520%2520High-performance%2520Multi-Object%2520Tracking%26entry.906535625%3DFutian%2520Wang%2520and%2520Fengxiang%2520Liu%2520and%2520Xiao%2520Wang%26entry.1292438233%3D%2520%2520In%2520the%2520realm%2520of%2520multi-object%2520tracking%252C%2520the%2520challenge%2520of%2520accurately%2520capturing%250Athe%2520spatial%2520and%2520temporal%2520relationships%2520between%2520objects%2520in%2520video%2520sequences%250Aremains%2520a%2520significant%2520hurdle.%2520This%2520is%2520further%2520complicated%2520by%2520frequent%250Aoccurrences%2520of%2520mutual%2520occlusions%2520among%2520objects%252C%2520which%2520can%2520lead%2520to%2520tracking%250Aerrors%2520and%2520reduced%2520performance%2520in%2520existing%2520methods.%2520Motivated%2520by%2520these%250Achallenges%252C%2520we%2520propose%2520a%2520novel%2520adaptive%2520key%2520frame%2520mining%2520strategy%2520that%250Aaddresses%2520the%2520limitations%2520of%2520current%2520tracking%2520approaches.%2520Specifically%252C%2520we%250Aintroduce%2520a%2520Key%2520Frame%2520Extraction%2520%2528KFE%2529%2520module%2520that%2520leverages%2520reinforcement%250Alearning%2520to%2520adaptively%2520segment%2520videos%252C%2520thereby%2520guiding%2520the%2520tracker%2520to%2520exploit%250Athe%2520intrinsic%2520logic%2520of%2520the%2520video%2520content.%2520This%2520approach%2520allows%2520us%2520to%2520capture%250Astructured%2520spatial%2520relationships%2520between%2520different%2520objects%2520as%2520well%2520as%2520the%250Atemporal%2520relationships%2520of%2520objects%2520across%2520frames.%2520To%2520tackle%2520the%2520issue%2520of%2520object%250Aocclusions%252C%2520we%2520have%2520developed%2520an%2520Intra-Frame%2520Feature%2520Fusion%2520%2528IFF%2529%2520module.%250AUnlike%2520traditional%2520graph-based%2520methods%2520that%2520primarily%2520focus%2520on%2520inter-frame%250Afeature%2520fusion%252C%2520our%2520IFF%2520module%2520uses%2520a%2520Graph%2520Convolutional%2520Network%2520%2528GCN%2529%2520to%250Afacilitate%2520information%2520exchange%2520between%2520the%2520target%2520and%2520surrounding%2520objects%250Awithin%2520a%2520frame.%2520This%2520innovation%2520significantly%2520enhances%2520target%250Adistinguishability%2520and%2520mitigates%2520tracking%2520loss%2520and%2520appearance%2520similarity%2520due%2520to%250Aocclusions.%2520By%2520combining%2520the%2520strengths%2520of%2520both%2520long%2520and%2520short%2520trajectories%2520and%250Aconsidering%2520the%2520spatial%2520relationships%2520between%2520objects%252C%2520our%2520proposed%2520tracker%250Aachieves%2520impressive%2520results%2520on%2520the%2520MOT17%2520dataset%252C%2520i.e.%252C%252068.6%2520HOTA%252C%252081.0%2520IDF1%252C%250A66.6%2520AssA%252C%2520and%2520893%2520IDS%252C%2520proving%2520its%2520effectiveness%2520and%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10129v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatio-temporal%20Graph%20Learning%20on%20Adaptive%20Mined%20Key%20Frames%20for%0A%20%20High-performance%20Multi-Object%20Tracking&entry.906535625=Futian%20Wang%20and%20Fengxiang%20Liu%20and%20Xiao%20Wang&entry.1292438233=%20%20In%20the%20realm%20of%20multi-object%20tracking%2C%20the%20challenge%20of%20accurately%20capturing%0Athe%20spatial%20and%20temporal%20relationships%20between%20objects%20in%20video%20sequences%0Aremains%20a%20significant%20hurdle.%20This%20is%20further%20complicated%20by%20frequent%0Aoccurrences%20of%20mutual%20occlusions%20among%20objects%2C%20which%20can%20lead%20to%20tracking%0Aerrors%20and%20reduced%20performance%20in%20existing%20methods.%20Motivated%20by%20these%0Achallenges%2C%20we%20propose%20a%20novel%20adaptive%20key%20frame%20mining%20strategy%20that%0Aaddresses%20the%20limitations%20of%20current%20tracking%20approaches.%20Specifically%2C%20we%0Aintroduce%20a%20Key%20Frame%20Extraction%20%28KFE%29%20module%20that%20leverages%20reinforcement%0Alearning%20to%20adaptively%20segment%20videos%2C%20thereby%20guiding%20the%20tracker%20to%20exploit%0Athe%20intrinsic%20logic%20of%20the%20video%20content.%20This%20approach%20allows%20us%20to%20capture%0Astructured%20spatial%20relationships%20between%20different%20objects%20as%20well%20as%20the%0Atemporal%20relationships%20of%20objects%20across%20frames.%20To%20tackle%20the%20issue%20of%20object%0Aocclusions%2C%20we%20have%20developed%20an%20Intra-Frame%20Feature%20Fusion%20%28IFF%29%20module.%0AUnlike%20traditional%20graph-based%20methods%20that%20primarily%20focus%20on%20inter-frame%0Afeature%20fusion%2C%20our%20IFF%20module%20uses%20a%20Graph%20Convolutional%20Network%20%28GCN%29%20to%0Afacilitate%20information%20exchange%20between%20the%20target%20and%20surrounding%20objects%0Awithin%20a%20frame.%20This%20innovation%20significantly%20enhances%20target%0Adistinguishability%20and%20mitigates%20tracking%20loss%20and%20appearance%20similarity%20due%20to%0Aocclusions.%20By%20combining%20the%20strengths%20of%20both%20long%20and%20short%20trajectories%20and%0Aconsidering%20the%20spatial%20relationships%20between%20objects%2C%20our%20proposed%20tracker%0Aachieves%20impressive%20results%20on%20the%20MOT17%20dataset%2C%20i.e.%2C%2068.6%20HOTA%2C%2081.0%20IDF1%2C%0A66.6%20AssA%2C%20and%20893%20IDS%2C%20proving%20its%20effectiveness%20and%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10129v1&entry.124074799=Read"},
{"title": "Large Language Model is Secretly a Protein Sequence Optimizer", "author": "Yinkai Wang and Jiaxing He and Yuanqi Du and Xiaohui Chen and Jianan Canal Li and Li-Ping Liu and Xiaolin Xu and Soha Hassoun", "abstract": "  We consider the protein sequence engineering problem, which aims to find\nprotein sequences with high fitness levels, starting from a given wild-type\nsequence. Directed evolution has been a dominating paradigm in this field which\nhas an iterative process to generate variants and select via experimental\nfeedback. We demonstrate large language models (LLMs), despite being trained on\nmassive texts, are secretly protein sequence optimizers. With a directed\nevolutionary method, LLM can perform protein engineering through Pareto and\nexperiment-budget constrained optimization, demonstrating success on both\nsynthetic and experimental fitness landscapes.\n", "link": "http://arxiv.org/abs/2501.09274v2", "date": "2025-01-17", "relevancy": 2.2248, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4616}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4616}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Model%20is%20Secretly%20a%20Protein%20Sequence%20Optimizer&body=Title%3A%20Large%20Language%20Model%20is%20Secretly%20a%20Protein%20Sequence%20Optimizer%0AAuthor%3A%20Yinkai%20Wang%20and%20Jiaxing%20He%20and%20Yuanqi%20Du%20and%20Xiaohui%20Chen%20and%20Jianan%20Canal%20Li%20and%20Li-Ping%20Liu%20and%20Xiaolin%20Xu%20and%20Soha%20Hassoun%0AAbstract%3A%20%20%20We%20consider%20the%20protein%20sequence%20engineering%20problem%2C%20which%20aims%20to%20find%0Aprotein%20sequences%20with%20high%20fitness%20levels%2C%20starting%20from%20a%20given%20wild-type%0Asequence.%20Directed%20evolution%20has%20been%20a%20dominating%20paradigm%20in%20this%20field%20which%0Ahas%20an%20iterative%20process%20to%20generate%20variants%20and%20select%20via%20experimental%0Afeedback.%20We%20demonstrate%20large%20language%20models%20%28LLMs%29%2C%20despite%20being%20trained%20on%0Amassive%20texts%2C%20are%20secretly%20protein%20sequence%20optimizers.%20With%20a%20directed%0Aevolutionary%20method%2C%20LLM%20can%20perform%20protein%20engineering%20through%20Pareto%20and%0Aexperiment-budget%20constrained%20optimization%2C%20demonstrating%20success%20on%20both%0Asynthetic%20and%20experimental%20fitness%20landscapes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09274v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Model%2520is%2520Secretly%2520a%2520Protein%2520Sequence%2520Optimizer%26entry.906535625%3DYinkai%2520Wang%2520and%2520Jiaxing%2520He%2520and%2520Yuanqi%2520Du%2520and%2520Xiaohui%2520Chen%2520and%2520Jianan%2520Canal%2520Li%2520and%2520Li-Ping%2520Liu%2520and%2520Xiaolin%2520Xu%2520and%2520Soha%2520Hassoun%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520protein%2520sequence%2520engineering%2520problem%252C%2520which%2520aims%2520to%2520find%250Aprotein%2520sequences%2520with%2520high%2520fitness%2520levels%252C%2520starting%2520from%2520a%2520given%2520wild-type%250Asequence.%2520Directed%2520evolution%2520has%2520been%2520a%2520dominating%2520paradigm%2520in%2520this%2520field%2520which%250Ahas%2520an%2520iterative%2520process%2520to%2520generate%2520variants%2520and%2520select%2520via%2520experimental%250Afeedback.%2520We%2520demonstrate%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520despite%2520being%2520trained%2520on%250Amassive%2520texts%252C%2520are%2520secretly%2520protein%2520sequence%2520optimizers.%2520With%2520a%2520directed%250Aevolutionary%2520method%252C%2520LLM%2520can%2520perform%2520protein%2520engineering%2520through%2520Pareto%2520and%250Aexperiment-budget%2520constrained%2520optimization%252C%2520demonstrating%2520success%2520on%2520both%250Asynthetic%2520and%2520experimental%2520fitness%2520landscapes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09274v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Model%20is%20Secretly%20a%20Protein%20Sequence%20Optimizer&entry.906535625=Yinkai%20Wang%20and%20Jiaxing%20He%20and%20Yuanqi%20Du%20and%20Xiaohui%20Chen%20and%20Jianan%20Canal%20Li%20and%20Li-Ping%20Liu%20and%20Xiaolin%20Xu%20and%20Soha%20Hassoun&entry.1292438233=%20%20We%20consider%20the%20protein%20sequence%20engineering%20problem%2C%20which%20aims%20to%20find%0Aprotein%20sequences%20with%20high%20fitness%20levels%2C%20starting%20from%20a%20given%20wild-type%0Asequence.%20Directed%20evolution%20has%20been%20a%20dominating%20paradigm%20in%20this%20field%20which%0Ahas%20an%20iterative%20process%20to%20generate%20variants%20and%20select%20via%20experimental%0Afeedback.%20We%20demonstrate%20large%20language%20models%20%28LLMs%29%2C%20despite%20being%20trained%20on%0Amassive%20texts%2C%20are%20secretly%20protein%20sequence%20optimizers.%20With%20a%20directed%0Aevolutionary%20method%2C%20LLM%20can%20perform%20protein%20engineering%20through%20Pareto%20and%0Aexperiment-budget%20constrained%20optimization%2C%20demonstrating%20success%20on%20both%0Asynthetic%20and%20experimental%20fitness%20landscapes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09274v2&entry.124074799=Read"},
{"title": "Large language models for automated scholarly paper review: A survey", "author": "Zhenzhen Zhuang and Jiandong Chen and Hongfeng Xu and Yuwen Jiang and Jialiang Lin", "abstract": "  Large language models (LLMs) have significantly impacted human society,\ninfluencing various domains. Among them, academia is not simply a domain\naffected by LLMs, but it is also the pivotal force in the development of LLMs.\nIn academic publications, this phenomenon is represented during the\nincorporation of LLMs into the peer review mechanism for reviewing manuscripts.\nWe proposed the concept of automated scholarly paper review (ASPR) in our\nprevious paper. As the incorporation grows, it now enters the coexistence phase\nof ASPR and peer review, which is described in that paper. LLMs hold\ntransformative potential for the full-scale implementation of ASPR, but they\nalso pose new issues and challenges that need to be addressed. In this survey\npaper, we aim to provide a holistic view of ASPR in the era of LLMs. We begin\nwith a survey to find out which LLMs are used to conduct ASPR. Then, we review\nwhat ASPR-related technological bottlenecks have been solved with the\nincorporation of LLM technology. After that, we move on to explore new methods,\nnew datasets, new source code, and new online systems that come with LLMs for\nASPR. Furthermore, we summarize the performance and issues of LLMs in ASPR, and\ninvestigate the attitudes and reactions of publishers and academia to ASPR.\nLastly, we discuss the challenges associated with the development of LLMs for\nASPR. We hope this survey can serve as an inspirational reference for the\nresearchers and promote the progress of ASPR for its actual implementation.\n", "link": "http://arxiv.org/abs/2501.10326v1", "date": "2025-01-17", "relevancy": 2.2149, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4542}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4542}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4206}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20language%20models%20for%20automated%20scholarly%20paper%20review%3A%20A%20survey&body=Title%3A%20Large%20language%20models%20for%20automated%20scholarly%20paper%20review%3A%20A%20survey%0AAuthor%3A%20Zhenzhen%20Zhuang%20and%20Jiandong%20Chen%20and%20Hongfeng%20Xu%20and%20Yuwen%20Jiang%20and%20Jialiang%20Lin%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20significantly%20impacted%20human%20society%2C%0Ainfluencing%20various%20domains.%20Among%20them%2C%20academia%20is%20not%20simply%20a%20domain%0Aaffected%20by%20LLMs%2C%20but%20it%20is%20also%20the%20pivotal%20force%20in%20the%20development%20of%20LLMs.%0AIn%20academic%20publications%2C%20this%20phenomenon%20is%20represented%20during%20the%0Aincorporation%20of%20LLMs%20into%20the%20peer%20review%20mechanism%20for%20reviewing%20manuscripts.%0AWe%20proposed%20the%20concept%20of%20automated%20scholarly%20paper%20review%20%28ASPR%29%20in%20our%0Aprevious%20paper.%20As%20the%20incorporation%20grows%2C%20it%20now%20enters%20the%20coexistence%20phase%0Aof%20ASPR%20and%20peer%20review%2C%20which%20is%20described%20in%20that%20paper.%20LLMs%20hold%0Atransformative%20potential%20for%20the%20full-scale%20implementation%20of%20ASPR%2C%20but%20they%0Aalso%20pose%20new%20issues%20and%20challenges%20that%20need%20to%20be%20addressed.%20In%20this%20survey%0Apaper%2C%20we%20aim%20to%20provide%20a%20holistic%20view%20of%20ASPR%20in%20the%20era%20of%20LLMs.%20We%20begin%0Awith%20a%20survey%20to%20find%20out%20which%20LLMs%20are%20used%20to%20conduct%20ASPR.%20Then%2C%20we%20review%0Awhat%20ASPR-related%20technological%20bottlenecks%20have%20been%20solved%20with%20the%0Aincorporation%20of%20LLM%20technology.%20After%20that%2C%20we%20move%20on%20to%20explore%20new%20methods%2C%0Anew%20datasets%2C%20new%20source%20code%2C%20and%20new%20online%20systems%20that%20come%20with%20LLMs%20for%0AASPR.%20Furthermore%2C%20we%20summarize%20the%20performance%20and%20issues%20of%20LLMs%20in%20ASPR%2C%20and%0Ainvestigate%20the%20attitudes%20and%20reactions%20of%20publishers%20and%20academia%20to%20ASPR.%0ALastly%2C%20we%20discuss%20the%20challenges%20associated%20with%20the%20development%20of%20LLMs%20for%0AASPR.%20We%20hope%20this%20survey%20can%20serve%20as%20an%20inspirational%20reference%20for%20the%0Aresearchers%20and%20promote%20the%20progress%20of%20ASPR%20for%20its%20actual%20implementation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10326v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520language%2520models%2520for%2520automated%2520scholarly%2520paper%2520review%253A%2520A%2520survey%26entry.906535625%3DZhenzhen%2520Zhuang%2520and%2520Jiandong%2520Chen%2520and%2520Hongfeng%2520Xu%2520and%2520Yuwen%2520Jiang%2520and%2520Jialiang%2520Lin%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520significantly%2520impacted%2520human%2520society%252C%250Ainfluencing%2520various%2520domains.%2520Among%2520them%252C%2520academia%2520is%2520not%2520simply%2520a%2520domain%250Aaffected%2520by%2520LLMs%252C%2520but%2520it%2520is%2520also%2520the%2520pivotal%2520force%2520in%2520the%2520development%2520of%2520LLMs.%250AIn%2520academic%2520publications%252C%2520this%2520phenomenon%2520is%2520represented%2520during%2520the%250Aincorporation%2520of%2520LLMs%2520into%2520the%2520peer%2520review%2520mechanism%2520for%2520reviewing%2520manuscripts.%250AWe%2520proposed%2520the%2520concept%2520of%2520automated%2520scholarly%2520paper%2520review%2520%2528ASPR%2529%2520in%2520our%250Aprevious%2520paper.%2520As%2520the%2520incorporation%2520grows%252C%2520it%2520now%2520enters%2520the%2520coexistence%2520phase%250Aof%2520ASPR%2520and%2520peer%2520review%252C%2520which%2520is%2520described%2520in%2520that%2520paper.%2520LLMs%2520hold%250Atransformative%2520potential%2520for%2520the%2520full-scale%2520implementation%2520of%2520ASPR%252C%2520but%2520they%250Aalso%2520pose%2520new%2520issues%2520and%2520challenges%2520that%2520need%2520to%2520be%2520addressed.%2520In%2520this%2520survey%250Apaper%252C%2520we%2520aim%2520to%2520provide%2520a%2520holistic%2520view%2520of%2520ASPR%2520in%2520the%2520era%2520of%2520LLMs.%2520We%2520begin%250Awith%2520a%2520survey%2520to%2520find%2520out%2520which%2520LLMs%2520are%2520used%2520to%2520conduct%2520ASPR.%2520Then%252C%2520we%2520review%250Awhat%2520ASPR-related%2520technological%2520bottlenecks%2520have%2520been%2520solved%2520with%2520the%250Aincorporation%2520of%2520LLM%2520technology.%2520After%2520that%252C%2520we%2520move%2520on%2520to%2520explore%2520new%2520methods%252C%250Anew%2520datasets%252C%2520new%2520source%2520code%252C%2520and%2520new%2520online%2520systems%2520that%2520come%2520with%2520LLMs%2520for%250AASPR.%2520Furthermore%252C%2520we%2520summarize%2520the%2520performance%2520and%2520issues%2520of%2520LLMs%2520in%2520ASPR%252C%2520and%250Ainvestigate%2520the%2520attitudes%2520and%2520reactions%2520of%2520publishers%2520and%2520academia%2520to%2520ASPR.%250ALastly%252C%2520we%2520discuss%2520the%2520challenges%2520associated%2520with%2520the%2520development%2520of%2520LLMs%2520for%250AASPR.%2520We%2520hope%2520this%2520survey%2520can%2520serve%2520as%2520an%2520inspirational%2520reference%2520for%2520the%250Aresearchers%2520and%2520promote%2520the%2520progress%2520of%2520ASPR%2520for%2520its%2520actual%2520implementation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10326v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20language%20models%20for%20automated%20scholarly%20paper%20review%3A%20A%20survey&entry.906535625=Zhenzhen%20Zhuang%20and%20Jiandong%20Chen%20and%20Hongfeng%20Xu%20and%20Yuwen%20Jiang%20and%20Jialiang%20Lin&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20significantly%20impacted%20human%20society%2C%0Ainfluencing%20various%20domains.%20Among%20them%2C%20academia%20is%20not%20simply%20a%20domain%0Aaffected%20by%20LLMs%2C%20but%20it%20is%20also%20the%20pivotal%20force%20in%20the%20development%20of%20LLMs.%0AIn%20academic%20publications%2C%20this%20phenomenon%20is%20represented%20during%20the%0Aincorporation%20of%20LLMs%20into%20the%20peer%20review%20mechanism%20for%20reviewing%20manuscripts.%0AWe%20proposed%20the%20concept%20of%20automated%20scholarly%20paper%20review%20%28ASPR%29%20in%20our%0Aprevious%20paper.%20As%20the%20incorporation%20grows%2C%20it%20now%20enters%20the%20coexistence%20phase%0Aof%20ASPR%20and%20peer%20review%2C%20which%20is%20described%20in%20that%20paper.%20LLMs%20hold%0Atransformative%20potential%20for%20the%20full-scale%20implementation%20of%20ASPR%2C%20but%20they%0Aalso%20pose%20new%20issues%20and%20challenges%20that%20need%20to%20be%20addressed.%20In%20this%20survey%0Apaper%2C%20we%20aim%20to%20provide%20a%20holistic%20view%20of%20ASPR%20in%20the%20era%20of%20LLMs.%20We%20begin%0Awith%20a%20survey%20to%20find%20out%20which%20LLMs%20are%20used%20to%20conduct%20ASPR.%20Then%2C%20we%20review%0Awhat%20ASPR-related%20technological%20bottlenecks%20have%20been%20solved%20with%20the%0Aincorporation%20of%20LLM%20technology.%20After%20that%2C%20we%20move%20on%20to%20explore%20new%20methods%2C%0Anew%20datasets%2C%20new%20source%20code%2C%20and%20new%20online%20systems%20that%20come%20with%20LLMs%20for%0AASPR.%20Furthermore%2C%20we%20summarize%20the%20performance%20and%20issues%20of%20LLMs%20in%20ASPR%2C%20and%0Ainvestigate%20the%20attitudes%20and%20reactions%20of%20publishers%20and%20academia%20to%20ASPR.%0ALastly%2C%20we%20discuss%20the%20challenges%20associated%20with%20the%20development%20of%20LLMs%20for%0AASPR.%20We%20hope%20this%20survey%20can%20serve%20as%20an%20inspirational%20reference%20for%20the%0Aresearchers%20and%20promote%20the%20progress%20of%20ASPR%20for%20its%20actual%20implementation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10326v1&entry.124074799=Read"},
{"title": "Moonshine: Distilling Game Content Generators into Steerable Generative\n  Models", "author": "Yuhe Nie and Michael Middleton and Tim Merino and Nidhushan Kanagaraja and Ashutosh Kumar and Zhan Zhuang and Julian Togelius", "abstract": "  Procedural Content Generation via Machine Learning (PCGML) has enhanced game\ncontent creation, yet challenges in controllability and limited training data\npersist. This study addresses these issues by distilling a constructive PCG\nalgorithm into a controllable PCGML model. We first generate a large amount of\ncontent with a constructive algorithm and label it using a Large Language Model\n(LLM). We use these synthetic labels to condition two PCGML models for\ncontent-specific generation, a diffusion model and the five-dollar model. This\nneural network distillation process ensures that the generation aligns with the\noriginal algorithm while introducing controllability through plain text. We\ndefine this text-conditioned PCGML as a Text-to-game-Map (T2M) task, offering\nan alternative to prevalent text-to-image multi-modal tasks. We compare our\ndistilled models with the baseline constructive algorithm. Our analysis of the\nvariety, accuracy, and quality of our generation demonstrates the efficacy of\ndistilling constructive methods into controllable text-conditioned PCGML\nmodels.\n", "link": "http://arxiv.org/abs/2408.09594v2", "date": "2025-01-17", "relevancy": 2.2143, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5753}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5552}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Moonshine%3A%20Distilling%20Game%20Content%20Generators%20into%20Steerable%20Generative%0A%20%20Models&body=Title%3A%20Moonshine%3A%20Distilling%20Game%20Content%20Generators%20into%20Steerable%20Generative%0A%20%20Models%0AAuthor%3A%20Yuhe%20Nie%20and%20Michael%20Middleton%20and%20Tim%20Merino%20and%20Nidhushan%20Kanagaraja%20and%20Ashutosh%20Kumar%20and%20Zhan%20Zhuang%20and%20Julian%20Togelius%0AAbstract%3A%20%20%20Procedural%20Content%20Generation%20via%20Machine%20Learning%20%28PCGML%29%20has%20enhanced%20game%0Acontent%20creation%2C%20yet%20challenges%20in%20controllability%20and%20limited%20training%20data%0Apersist.%20This%20study%20addresses%20these%20issues%20by%20distilling%20a%20constructive%20PCG%0Aalgorithm%20into%20a%20controllable%20PCGML%20model.%20We%20first%20generate%20a%20large%20amount%20of%0Acontent%20with%20a%20constructive%20algorithm%20and%20label%20it%20using%20a%20Large%20Language%20Model%0A%28LLM%29.%20We%20use%20these%20synthetic%20labels%20to%20condition%20two%20PCGML%20models%20for%0Acontent-specific%20generation%2C%20a%20diffusion%20model%20and%20the%20five-dollar%20model.%20This%0Aneural%20network%20distillation%20process%20ensures%20that%20the%20generation%20aligns%20with%20the%0Aoriginal%20algorithm%20while%20introducing%20controllability%20through%20plain%20text.%20We%0Adefine%20this%20text-conditioned%20PCGML%20as%20a%20Text-to-game-Map%20%28T2M%29%20task%2C%20offering%0Aan%20alternative%20to%20prevalent%20text-to-image%20multi-modal%20tasks.%20We%20compare%20our%0Adistilled%20models%20with%20the%20baseline%20constructive%20algorithm.%20Our%20analysis%20of%20the%0Avariety%2C%20accuracy%2C%20and%20quality%20of%20our%20generation%20demonstrates%20the%20efficacy%20of%0Adistilling%20constructive%20methods%20into%20controllable%20text-conditioned%20PCGML%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09594v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoonshine%253A%2520Distilling%2520Game%2520Content%2520Generators%2520into%2520Steerable%2520Generative%250A%2520%2520Models%26entry.906535625%3DYuhe%2520Nie%2520and%2520Michael%2520Middleton%2520and%2520Tim%2520Merino%2520and%2520Nidhushan%2520Kanagaraja%2520and%2520Ashutosh%2520Kumar%2520and%2520Zhan%2520Zhuang%2520and%2520Julian%2520Togelius%26entry.1292438233%3D%2520%2520Procedural%2520Content%2520Generation%2520via%2520Machine%2520Learning%2520%2528PCGML%2529%2520has%2520enhanced%2520game%250Acontent%2520creation%252C%2520yet%2520challenges%2520in%2520controllability%2520and%2520limited%2520training%2520data%250Apersist.%2520This%2520study%2520addresses%2520these%2520issues%2520by%2520distilling%2520a%2520constructive%2520PCG%250Aalgorithm%2520into%2520a%2520controllable%2520PCGML%2520model.%2520We%2520first%2520generate%2520a%2520large%2520amount%2520of%250Acontent%2520with%2520a%2520constructive%2520algorithm%2520and%2520label%2520it%2520using%2520a%2520Large%2520Language%2520Model%250A%2528LLM%2529.%2520We%2520use%2520these%2520synthetic%2520labels%2520to%2520condition%2520two%2520PCGML%2520models%2520for%250Acontent-specific%2520generation%252C%2520a%2520diffusion%2520model%2520and%2520the%2520five-dollar%2520model.%2520This%250Aneural%2520network%2520distillation%2520process%2520ensures%2520that%2520the%2520generation%2520aligns%2520with%2520the%250Aoriginal%2520algorithm%2520while%2520introducing%2520controllability%2520through%2520plain%2520text.%2520We%250Adefine%2520this%2520text-conditioned%2520PCGML%2520as%2520a%2520Text-to-game-Map%2520%2528T2M%2529%2520task%252C%2520offering%250Aan%2520alternative%2520to%2520prevalent%2520text-to-image%2520multi-modal%2520tasks.%2520We%2520compare%2520our%250Adistilled%2520models%2520with%2520the%2520baseline%2520constructive%2520algorithm.%2520Our%2520analysis%2520of%2520the%250Avariety%252C%2520accuracy%252C%2520and%2520quality%2520of%2520our%2520generation%2520demonstrates%2520the%2520efficacy%2520of%250Adistilling%2520constructive%2520methods%2520into%2520controllable%2520text-conditioned%2520PCGML%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09594v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Moonshine%3A%20Distilling%20Game%20Content%20Generators%20into%20Steerable%20Generative%0A%20%20Models&entry.906535625=Yuhe%20Nie%20and%20Michael%20Middleton%20and%20Tim%20Merino%20and%20Nidhushan%20Kanagaraja%20and%20Ashutosh%20Kumar%20and%20Zhan%20Zhuang%20and%20Julian%20Togelius&entry.1292438233=%20%20Procedural%20Content%20Generation%20via%20Machine%20Learning%20%28PCGML%29%20has%20enhanced%20game%0Acontent%20creation%2C%20yet%20challenges%20in%20controllability%20and%20limited%20training%20data%0Apersist.%20This%20study%20addresses%20these%20issues%20by%20distilling%20a%20constructive%20PCG%0Aalgorithm%20into%20a%20controllable%20PCGML%20model.%20We%20first%20generate%20a%20large%20amount%20of%0Acontent%20with%20a%20constructive%20algorithm%20and%20label%20it%20using%20a%20Large%20Language%20Model%0A%28LLM%29.%20We%20use%20these%20synthetic%20labels%20to%20condition%20two%20PCGML%20models%20for%0Acontent-specific%20generation%2C%20a%20diffusion%20model%20and%20the%20five-dollar%20model.%20This%0Aneural%20network%20distillation%20process%20ensures%20that%20the%20generation%20aligns%20with%20the%0Aoriginal%20algorithm%20while%20introducing%20controllability%20through%20plain%20text.%20We%0Adefine%20this%20text-conditioned%20PCGML%20as%20a%20Text-to-game-Map%20%28T2M%29%20task%2C%20offering%0Aan%20alternative%20to%20prevalent%20text-to-image%20multi-modal%20tasks.%20We%20compare%20our%0Adistilled%20models%20with%20the%20baseline%20constructive%20algorithm.%20Our%20analysis%20of%20the%0Avariety%2C%20accuracy%2C%20and%20quality%20of%20our%20generation%20demonstrates%20the%20efficacy%20of%0Adistilling%20constructive%20methods%20into%20controllable%20text-conditioned%20PCGML%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09594v2&entry.124074799=Read"},
{"title": "On Learning Informative Trajectory Embeddings for Imitation,\n  Classification and Regression", "author": "Zichang Ge and Changyu Chen and Arunesh Sinha and Pradeep Varakantham", "abstract": "  In real-world sequential decision making tasks like autonomous driving,\nrobotics, and healthcare, learning from observed state-action trajectories is\ncritical for tasks like imitation, classification, and clustering. For example,\nself-driving cars must replicate human driving behaviors, while robots and\nhealthcare systems benefit from modeling decision sequences, whether or not\nthey come from expert data. Existing trajectory encoding methods often focus on\nspecific tasks or rely on reward signals, limiting their ability to generalize\nacross domains and tasks. Inspired by the success of embedding models like CLIP\nand BERT in static domains, we propose a novel method for embedding\nstate-action trajectories into a latent space that captures the skills and\ncompetencies in the dynamic underlying decision-making processes. This method\noperates without the need for reward labels, enabling better generalization\nacross diverse domains and tasks. Our contributions are threefold: (1) We\nintroduce a trajectory embedding approach that captures multiple abilities from\nstate-action data. (2) The learned embeddings exhibit strong representational\npower across downstream tasks, including imitation, classification, clustering,\nand regression. (3) The embeddings demonstrate unique properties, such as\ncontrolling agent behaviors in IQ-Learn and an additive structure in the latent\nspace. Experimental results confirm that our method outperforms traditional\napproaches, offering more flexible and powerful trajectory representations for\nvarious applications. Our code is available at\nhttps://github.com/Erasmo1015/vte.\n", "link": "http://arxiv.org/abs/2501.09327v2", "date": "2025-01-17", "relevancy": 2.2023, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.599}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5439}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5379}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Learning%20Informative%20Trajectory%20Embeddings%20for%20Imitation%2C%0A%20%20Classification%20and%20Regression&body=Title%3A%20On%20Learning%20Informative%20Trajectory%20Embeddings%20for%20Imitation%2C%0A%20%20Classification%20and%20Regression%0AAuthor%3A%20Zichang%20Ge%20and%20Changyu%20Chen%20and%20Arunesh%20Sinha%20and%20Pradeep%20Varakantham%0AAbstract%3A%20%20%20In%20real-world%20sequential%20decision%20making%20tasks%20like%20autonomous%20driving%2C%0Arobotics%2C%20and%20healthcare%2C%20learning%20from%20observed%20state-action%20trajectories%20is%0Acritical%20for%20tasks%20like%20imitation%2C%20classification%2C%20and%20clustering.%20For%20example%2C%0Aself-driving%20cars%20must%20replicate%20human%20driving%20behaviors%2C%20while%20robots%20and%0Ahealthcare%20systems%20benefit%20from%20modeling%20decision%20sequences%2C%20whether%20or%20not%0Athey%20come%20from%20expert%20data.%20Existing%20trajectory%20encoding%20methods%20often%20focus%20on%0Aspecific%20tasks%20or%20rely%20on%20reward%20signals%2C%20limiting%20their%20ability%20to%20generalize%0Aacross%20domains%20and%20tasks.%20Inspired%20by%20the%20success%20of%20embedding%20models%20like%20CLIP%0Aand%20BERT%20in%20static%20domains%2C%20we%20propose%20a%20novel%20method%20for%20embedding%0Astate-action%20trajectories%20into%20a%20latent%20space%20that%20captures%20the%20skills%20and%0Acompetencies%20in%20the%20dynamic%20underlying%20decision-making%20processes.%20This%20method%0Aoperates%20without%20the%20need%20for%20reward%20labels%2C%20enabling%20better%20generalization%0Aacross%20diverse%20domains%20and%20tasks.%20Our%20contributions%20are%20threefold%3A%20%281%29%20We%0Aintroduce%20a%20trajectory%20embedding%20approach%20that%20captures%20multiple%20abilities%20from%0Astate-action%20data.%20%282%29%20The%20learned%20embeddings%20exhibit%20strong%20representational%0Apower%20across%20downstream%20tasks%2C%20including%20imitation%2C%20classification%2C%20clustering%2C%0Aand%20regression.%20%283%29%20The%20embeddings%20demonstrate%20unique%20properties%2C%20such%20as%0Acontrolling%20agent%20behaviors%20in%20IQ-Learn%20and%20an%20additive%20structure%20in%20the%20latent%0Aspace.%20Experimental%20results%20confirm%20that%20our%20method%20outperforms%20traditional%0Aapproaches%2C%20offering%20more%20flexible%20and%20powerful%20trajectory%20representations%20for%0Avarious%20applications.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Erasmo1015/vte.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09327v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Learning%2520Informative%2520Trajectory%2520Embeddings%2520for%2520Imitation%252C%250A%2520%2520Classification%2520and%2520Regression%26entry.906535625%3DZichang%2520Ge%2520and%2520Changyu%2520Chen%2520and%2520Arunesh%2520Sinha%2520and%2520Pradeep%2520Varakantham%26entry.1292438233%3D%2520%2520In%2520real-world%2520sequential%2520decision%2520making%2520tasks%2520like%2520autonomous%2520driving%252C%250Arobotics%252C%2520and%2520healthcare%252C%2520learning%2520from%2520observed%2520state-action%2520trajectories%2520is%250Acritical%2520for%2520tasks%2520like%2520imitation%252C%2520classification%252C%2520and%2520clustering.%2520For%2520example%252C%250Aself-driving%2520cars%2520must%2520replicate%2520human%2520driving%2520behaviors%252C%2520while%2520robots%2520and%250Ahealthcare%2520systems%2520benefit%2520from%2520modeling%2520decision%2520sequences%252C%2520whether%2520or%2520not%250Athey%2520come%2520from%2520expert%2520data.%2520Existing%2520trajectory%2520encoding%2520methods%2520often%2520focus%2520on%250Aspecific%2520tasks%2520or%2520rely%2520on%2520reward%2520signals%252C%2520limiting%2520their%2520ability%2520to%2520generalize%250Aacross%2520domains%2520and%2520tasks.%2520Inspired%2520by%2520the%2520success%2520of%2520embedding%2520models%2520like%2520CLIP%250Aand%2520BERT%2520in%2520static%2520domains%252C%2520we%2520propose%2520a%2520novel%2520method%2520for%2520embedding%250Astate-action%2520trajectories%2520into%2520a%2520latent%2520space%2520that%2520captures%2520the%2520skills%2520and%250Acompetencies%2520in%2520the%2520dynamic%2520underlying%2520decision-making%2520processes.%2520This%2520method%250Aoperates%2520without%2520the%2520need%2520for%2520reward%2520labels%252C%2520enabling%2520better%2520generalization%250Aacross%2520diverse%2520domains%2520and%2520tasks.%2520Our%2520contributions%2520are%2520threefold%253A%2520%25281%2529%2520We%250Aintroduce%2520a%2520trajectory%2520embedding%2520approach%2520that%2520captures%2520multiple%2520abilities%2520from%250Astate-action%2520data.%2520%25282%2529%2520The%2520learned%2520embeddings%2520exhibit%2520strong%2520representational%250Apower%2520across%2520downstream%2520tasks%252C%2520including%2520imitation%252C%2520classification%252C%2520clustering%252C%250Aand%2520regression.%2520%25283%2529%2520The%2520embeddings%2520demonstrate%2520unique%2520properties%252C%2520such%2520as%250Acontrolling%2520agent%2520behaviors%2520in%2520IQ-Learn%2520and%2520an%2520additive%2520structure%2520in%2520the%2520latent%250Aspace.%2520Experimental%2520results%2520confirm%2520that%2520our%2520method%2520outperforms%2520traditional%250Aapproaches%252C%2520offering%2520more%2520flexible%2520and%2520powerful%2520trajectory%2520representations%2520for%250Avarious%2520applications.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Erasmo1015/vte.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09327v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Learning%20Informative%20Trajectory%20Embeddings%20for%20Imitation%2C%0A%20%20Classification%20and%20Regression&entry.906535625=Zichang%20Ge%20and%20Changyu%20Chen%20and%20Arunesh%20Sinha%20and%20Pradeep%20Varakantham&entry.1292438233=%20%20In%20real-world%20sequential%20decision%20making%20tasks%20like%20autonomous%20driving%2C%0Arobotics%2C%20and%20healthcare%2C%20learning%20from%20observed%20state-action%20trajectories%20is%0Acritical%20for%20tasks%20like%20imitation%2C%20classification%2C%20and%20clustering.%20For%20example%2C%0Aself-driving%20cars%20must%20replicate%20human%20driving%20behaviors%2C%20while%20robots%20and%0Ahealthcare%20systems%20benefit%20from%20modeling%20decision%20sequences%2C%20whether%20or%20not%0Athey%20come%20from%20expert%20data.%20Existing%20trajectory%20encoding%20methods%20often%20focus%20on%0Aspecific%20tasks%20or%20rely%20on%20reward%20signals%2C%20limiting%20their%20ability%20to%20generalize%0Aacross%20domains%20and%20tasks.%20Inspired%20by%20the%20success%20of%20embedding%20models%20like%20CLIP%0Aand%20BERT%20in%20static%20domains%2C%20we%20propose%20a%20novel%20method%20for%20embedding%0Astate-action%20trajectories%20into%20a%20latent%20space%20that%20captures%20the%20skills%20and%0Acompetencies%20in%20the%20dynamic%20underlying%20decision-making%20processes.%20This%20method%0Aoperates%20without%20the%20need%20for%20reward%20labels%2C%20enabling%20better%20generalization%0Aacross%20diverse%20domains%20and%20tasks.%20Our%20contributions%20are%20threefold%3A%20%281%29%20We%0Aintroduce%20a%20trajectory%20embedding%20approach%20that%20captures%20multiple%20abilities%20from%0Astate-action%20data.%20%282%29%20The%20learned%20embeddings%20exhibit%20strong%20representational%0Apower%20across%20downstream%20tasks%2C%20including%20imitation%2C%20classification%2C%20clustering%2C%0Aand%20regression.%20%283%29%20The%20embeddings%20demonstrate%20unique%20properties%2C%20such%20as%0Acontrolling%20agent%20behaviors%20in%20IQ-Learn%20and%20an%20additive%20structure%20in%20the%20latent%0Aspace.%20Experimental%20results%20confirm%20that%20our%20method%20outperforms%20traditional%0Aapproaches%2C%20offering%20more%20flexible%20and%20powerful%20trajectory%20representations%20for%0Avarious%20applications.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Erasmo1015/vte.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09327v2&entry.124074799=Read"},
{"title": "Robust Egoistic Rigid Body Localization", "author": "Niclas F\u00fchrling and Giuseppe Thadeu Freitas de Abreu and David Gonz\u00e1lez G. and Osvaldo Gonsa", "abstract": "  We consider a robust and self-reliant (or \"egoistic\") variation of the rigid\nbody localization (RBL) problem, in which a primary rigid body seeks to\nestimate the pose (i.e., location and orientation) of another rigid body (or\n\"target\"), relative to its own, without the assistance of external\ninfrastructure, without prior knowledge of the shape of the target, and taking\ninto account the possibility that the available observations are incomplete.\nThree complementary contributions are then offered for such a scenario. The\nfirst is a method to estimate the translation vector between the center point\nof both rigid bodies, which unlike existing techniques does not require that\nboth objects have the same shape or even the same number of landmark points.\nThis technique is shown to significantly outperform the state-of-the-art (SotA)\nunder complete information, but to be sensitive to data erasures, even when\nenhanced by matrix completion methods. The second contribution, designed to\noffer improved performance in the presence of incomplete information, offers a\nrobust alternative to the latter, at the expense of a slight relative loss\nunder complete information. Finally, the third contribution is a scheme for the\nestimation of the rotation matrix describing the relative orientation of the\ntarget rigid body with respect to the primary. Comparisons of the proposed\nschemes and SotA techniques demonstrate the advantage of the contributed\nmethods in terms of root mean square error (RMSE) performance under fully\ncomplete information and incomplete conditions.\n", "link": "http://arxiv.org/abs/2501.10219v1", "date": "2025-01-17", "relevancy": 2.1985, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5635}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5473}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Egoistic%20Rigid%20Body%20Localization&body=Title%3A%20Robust%20Egoistic%20Rigid%20Body%20Localization%0AAuthor%3A%20Niclas%20F%C3%BChrling%20and%20Giuseppe%20Thadeu%20Freitas%20de%20Abreu%20and%20David%20Gonz%C3%A1lez%20G.%20and%20Osvaldo%20Gonsa%0AAbstract%3A%20%20%20We%20consider%20a%20robust%20and%20self-reliant%20%28or%20%22egoistic%22%29%20variation%20of%20the%20rigid%0Abody%20localization%20%28RBL%29%20problem%2C%20in%20which%20a%20primary%20rigid%20body%20seeks%20to%0Aestimate%20the%20pose%20%28i.e.%2C%20location%20and%20orientation%29%20of%20another%20rigid%20body%20%28or%0A%22target%22%29%2C%20relative%20to%20its%20own%2C%20without%20the%20assistance%20of%20external%0Ainfrastructure%2C%20without%20prior%20knowledge%20of%20the%20shape%20of%20the%20target%2C%20and%20taking%0Ainto%20account%20the%20possibility%20that%20the%20available%20observations%20are%20incomplete.%0AThree%20complementary%20contributions%20are%20then%20offered%20for%20such%20a%20scenario.%20The%0Afirst%20is%20a%20method%20to%20estimate%20the%20translation%20vector%20between%20the%20center%20point%0Aof%20both%20rigid%20bodies%2C%20which%20unlike%20existing%20techniques%20does%20not%20require%20that%0Aboth%20objects%20have%20the%20same%20shape%20or%20even%20the%20same%20number%20of%20landmark%20points.%0AThis%20technique%20is%20shown%20to%20significantly%20outperform%20the%20state-of-the-art%20%28SotA%29%0Aunder%20complete%20information%2C%20but%20to%20be%20sensitive%20to%20data%20erasures%2C%20even%20when%0Aenhanced%20by%20matrix%20completion%20methods.%20The%20second%20contribution%2C%20designed%20to%0Aoffer%20improved%20performance%20in%20the%20presence%20of%20incomplete%20information%2C%20offers%20a%0Arobust%20alternative%20to%20the%20latter%2C%20at%20the%20expense%20of%20a%20slight%20relative%20loss%0Aunder%20complete%20information.%20Finally%2C%20the%20third%20contribution%20is%20a%20scheme%20for%20the%0Aestimation%20of%20the%20rotation%20matrix%20describing%20the%20relative%20orientation%20of%20the%0Atarget%20rigid%20body%20with%20respect%20to%20the%20primary.%20Comparisons%20of%20the%20proposed%0Aschemes%20and%20SotA%20techniques%20demonstrate%20the%20advantage%20of%20the%20contributed%0Amethods%20in%20terms%20of%20root%20mean%20square%20error%20%28RMSE%29%20performance%20under%20fully%0Acomplete%20information%20and%20incomplete%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10219v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Egoistic%2520Rigid%2520Body%2520Localization%26entry.906535625%3DNiclas%2520F%25C3%25BChrling%2520and%2520Giuseppe%2520Thadeu%2520Freitas%2520de%2520Abreu%2520and%2520David%2520Gonz%25C3%25A1lez%2520G.%2520and%2520Osvaldo%2520Gonsa%26entry.1292438233%3D%2520%2520We%2520consider%2520a%2520robust%2520and%2520self-reliant%2520%2528or%2520%2522egoistic%2522%2529%2520variation%2520of%2520the%2520rigid%250Abody%2520localization%2520%2528RBL%2529%2520problem%252C%2520in%2520which%2520a%2520primary%2520rigid%2520body%2520seeks%2520to%250Aestimate%2520the%2520pose%2520%2528i.e.%252C%2520location%2520and%2520orientation%2529%2520of%2520another%2520rigid%2520body%2520%2528or%250A%2522target%2522%2529%252C%2520relative%2520to%2520its%2520own%252C%2520without%2520the%2520assistance%2520of%2520external%250Ainfrastructure%252C%2520without%2520prior%2520knowledge%2520of%2520the%2520shape%2520of%2520the%2520target%252C%2520and%2520taking%250Ainto%2520account%2520the%2520possibility%2520that%2520the%2520available%2520observations%2520are%2520incomplete.%250AThree%2520complementary%2520contributions%2520are%2520then%2520offered%2520for%2520such%2520a%2520scenario.%2520The%250Afirst%2520is%2520a%2520method%2520to%2520estimate%2520the%2520translation%2520vector%2520between%2520the%2520center%2520point%250Aof%2520both%2520rigid%2520bodies%252C%2520which%2520unlike%2520existing%2520techniques%2520does%2520not%2520require%2520that%250Aboth%2520objects%2520have%2520the%2520same%2520shape%2520or%2520even%2520the%2520same%2520number%2520of%2520landmark%2520points.%250AThis%2520technique%2520is%2520shown%2520to%2520significantly%2520outperform%2520the%2520state-of-the-art%2520%2528SotA%2529%250Aunder%2520complete%2520information%252C%2520but%2520to%2520be%2520sensitive%2520to%2520data%2520erasures%252C%2520even%2520when%250Aenhanced%2520by%2520matrix%2520completion%2520methods.%2520The%2520second%2520contribution%252C%2520designed%2520to%250Aoffer%2520improved%2520performance%2520in%2520the%2520presence%2520of%2520incomplete%2520information%252C%2520offers%2520a%250Arobust%2520alternative%2520to%2520the%2520latter%252C%2520at%2520the%2520expense%2520of%2520a%2520slight%2520relative%2520loss%250Aunder%2520complete%2520information.%2520Finally%252C%2520the%2520third%2520contribution%2520is%2520a%2520scheme%2520for%2520the%250Aestimation%2520of%2520the%2520rotation%2520matrix%2520describing%2520the%2520relative%2520orientation%2520of%2520the%250Atarget%2520rigid%2520body%2520with%2520respect%2520to%2520the%2520primary.%2520Comparisons%2520of%2520the%2520proposed%250Aschemes%2520and%2520SotA%2520techniques%2520demonstrate%2520the%2520advantage%2520of%2520the%2520contributed%250Amethods%2520in%2520terms%2520of%2520root%2520mean%2520square%2520error%2520%2528RMSE%2529%2520performance%2520under%2520fully%250Acomplete%2520information%2520and%2520incomplete%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10219v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Egoistic%20Rigid%20Body%20Localization&entry.906535625=Niclas%20F%C3%BChrling%20and%20Giuseppe%20Thadeu%20Freitas%20de%20Abreu%20and%20David%20Gonz%C3%A1lez%20G.%20and%20Osvaldo%20Gonsa&entry.1292438233=%20%20We%20consider%20a%20robust%20and%20self-reliant%20%28or%20%22egoistic%22%29%20variation%20of%20the%20rigid%0Abody%20localization%20%28RBL%29%20problem%2C%20in%20which%20a%20primary%20rigid%20body%20seeks%20to%0Aestimate%20the%20pose%20%28i.e.%2C%20location%20and%20orientation%29%20of%20another%20rigid%20body%20%28or%0A%22target%22%29%2C%20relative%20to%20its%20own%2C%20without%20the%20assistance%20of%20external%0Ainfrastructure%2C%20without%20prior%20knowledge%20of%20the%20shape%20of%20the%20target%2C%20and%20taking%0Ainto%20account%20the%20possibility%20that%20the%20available%20observations%20are%20incomplete.%0AThree%20complementary%20contributions%20are%20then%20offered%20for%20such%20a%20scenario.%20The%0Afirst%20is%20a%20method%20to%20estimate%20the%20translation%20vector%20between%20the%20center%20point%0Aof%20both%20rigid%20bodies%2C%20which%20unlike%20existing%20techniques%20does%20not%20require%20that%0Aboth%20objects%20have%20the%20same%20shape%20or%20even%20the%20same%20number%20of%20landmark%20points.%0AThis%20technique%20is%20shown%20to%20significantly%20outperform%20the%20state-of-the-art%20%28SotA%29%0Aunder%20complete%20information%2C%20but%20to%20be%20sensitive%20to%20data%20erasures%2C%20even%20when%0Aenhanced%20by%20matrix%20completion%20methods.%20The%20second%20contribution%2C%20designed%20to%0Aoffer%20improved%20performance%20in%20the%20presence%20of%20incomplete%20information%2C%20offers%20a%0Arobust%20alternative%20to%20the%20latter%2C%20at%20the%20expense%20of%20a%20slight%20relative%20loss%0Aunder%20complete%20information.%20Finally%2C%20the%20third%20contribution%20is%20a%20scheme%20for%20the%0Aestimation%20of%20the%20rotation%20matrix%20describing%20the%20relative%20orientation%20of%20the%0Atarget%20rigid%20body%20with%20respect%20to%20the%20primary.%20Comparisons%20of%20the%20proposed%0Aschemes%20and%20SotA%20techniques%20demonstrate%20the%20advantage%20of%20the%20contributed%0Amethods%20in%20terms%20of%20root%20mean%20square%20error%20%28RMSE%29%20performance%20under%20fully%0Acomplete%20information%20and%20incomplete%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10219v1&entry.124074799=Read"},
{"title": "Region-wise stacking ensembles for estimating brain-age using MRI", "author": "Georgios Antonopoulos and Shammi More and Simon B. Eickhoff and Federico Raimondo and Kaustubh R. Patil", "abstract": "  Predictive modeling using structural magnetic resonance imaging (MRI) data is\na prominent approach to study brain-aging. Machine learning algorithms and\nfeature extraction methods have been employed to improve predictions and\nexplore healthy and accelerated aging e.g. neurodegenerative and psychiatric\ndisorders. The high-dimensional MRI data pose challenges to building\ngeneralizable and interpretable models as well as for data privacy. Common\npractices are resampling or averaging voxels within predefined parcels, which\nreduces anatomical specificity and biological interpretability as voxels within\na region may differently relate to aging. Effectively, naive fusion by\naveraging can result in information loss and reduced accuracy. We present a\nconceptually novel two-level stacking ensemble (SE) approach. The first level\ncomprises regional models for predicting individuals' age based on voxel-wise\ninformation, fused by a second-level model yielding final predictions. Eight\ndata fusion scenarios were explored using as input Gray matter volume (GMV)\nestimates from four datasets covering the adult lifespan. Performance, measured\nusing mean absolute error (MAE), R2, correlation and prediction bias, showed\nthat SE outperformed the region-wise averages. The best performance was\nobtained when first-level regional predictions were obtained as out-of-sample\npredictions on the application site with second-level models trained on\nindependent and site-specific data (MAE=4.75 vs baseline regional mean GMV\nMAE=5.68). Performance improved as more datasets were used for training.\nFirst-level predictions showed improved and more robust aging signal providing\nnew biological insights and enhanced data privacy. Overall, the SE improves\naccuracy compared to the baseline while preserving or enhancing data privacy.\n", "link": "http://arxiv.org/abs/2501.10153v1", "date": "2025-01-17", "relevancy": 2.195, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4435}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4368}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Region-wise%20stacking%20ensembles%20for%20estimating%20brain-age%20using%20MRI&body=Title%3A%20Region-wise%20stacking%20ensembles%20for%20estimating%20brain-age%20using%20MRI%0AAuthor%3A%20Georgios%20Antonopoulos%20and%20Shammi%20More%20and%20Simon%20B.%20Eickhoff%20and%20Federico%20Raimondo%20and%20Kaustubh%20R.%20Patil%0AAbstract%3A%20%20%20Predictive%20modeling%20using%20structural%20magnetic%20resonance%20imaging%20%28MRI%29%20data%20is%0Aa%20prominent%20approach%20to%20study%20brain-aging.%20Machine%20learning%20algorithms%20and%0Afeature%20extraction%20methods%20have%20been%20employed%20to%20improve%20predictions%20and%0Aexplore%20healthy%20and%20accelerated%20aging%20e.g.%20neurodegenerative%20and%20psychiatric%0Adisorders.%20The%20high-dimensional%20MRI%20data%20pose%20challenges%20to%20building%0Ageneralizable%20and%20interpretable%20models%20as%20well%20as%20for%20data%20privacy.%20Common%0Apractices%20are%20resampling%20or%20averaging%20voxels%20within%20predefined%20parcels%2C%20which%0Areduces%20anatomical%20specificity%20and%20biological%20interpretability%20as%20voxels%20within%0Aa%20region%20may%20differently%20relate%20to%20aging.%20Effectively%2C%20naive%20fusion%20by%0Aaveraging%20can%20result%20in%20information%20loss%20and%20reduced%20accuracy.%20We%20present%20a%0Aconceptually%20novel%20two-level%20stacking%20ensemble%20%28SE%29%20approach.%20The%20first%20level%0Acomprises%20regional%20models%20for%20predicting%20individuals%27%20age%20based%20on%20voxel-wise%0Ainformation%2C%20fused%20by%20a%20second-level%20model%20yielding%20final%20predictions.%20Eight%0Adata%20fusion%20scenarios%20were%20explored%20using%20as%20input%20Gray%20matter%20volume%20%28GMV%29%0Aestimates%20from%20four%20datasets%20covering%20the%20adult%20lifespan.%20Performance%2C%20measured%0Ausing%20mean%20absolute%20error%20%28MAE%29%2C%20R2%2C%20correlation%20and%20prediction%20bias%2C%20showed%0Athat%20SE%20outperformed%20the%20region-wise%20averages.%20The%20best%20performance%20was%0Aobtained%20when%20first-level%20regional%20predictions%20were%20obtained%20as%20out-of-sample%0Apredictions%20on%20the%20application%20site%20with%20second-level%20models%20trained%20on%0Aindependent%20and%20site-specific%20data%20%28MAE%3D4.75%20vs%20baseline%20regional%20mean%20GMV%0AMAE%3D5.68%29.%20Performance%20improved%20as%20more%20datasets%20were%20used%20for%20training.%0AFirst-level%20predictions%20showed%20improved%20and%20more%20robust%20aging%20signal%20providing%0Anew%20biological%20insights%20and%20enhanced%20data%20privacy.%20Overall%2C%20the%20SE%20improves%0Aaccuracy%20compared%20to%20the%20baseline%20while%20preserving%20or%20enhancing%20data%20privacy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10153v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRegion-wise%2520stacking%2520ensembles%2520for%2520estimating%2520brain-age%2520using%2520MRI%26entry.906535625%3DGeorgios%2520Antonopoulos%2520and%2520Shammi%2520More%2520and%2520Simon%2520B.%2520Eickhoff%2520and%2520Federico%2520Raimondo%2520and%2520Kaustubh%2520R.%2520Patil%26entry.1292438233%3D%2520%2520Predictive%2520modeling%2520using%2520structural%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520data%2520is%250Aa%2520prominent%2520approach%2520to%2520study%2520brain-aging.%2520Machine%2520learning%2520algorithms%2520and%250Afeature%2520extraction%2520methods%2520have%2520been%2520employed%2520to%2520improve%2520predictions%2520and%250Aexplore%2520healthy%2520and%2520accelerated%2520aging%2520e.g.%2520neurodegenerative%2520and%2520psychiatric%250Adisorders.%2520The%2520high-dimensional%2520MRI%2520data%2520pose%2520challenges%2520to%2520building%250Ageneralizable%2520and%2520interpretable%2520models%2520as%2520well%2520as%2520for%2520data%2520privacy.%2520Common%250Apractices%2520are%2520resampling%2520or%2520averaging%2520voxels%2520within%2520predefined%2520parcels%252C%2520which%250Areduces%2520anatomical%2520specificity%2520and%2520biological%2520interpretability%2520as%2520voxels%2520within%250Aa%2520region%2520may%2520differently%2520relate%2520to%2520aging.%2520Effectively%252C%2520naive%2520fusion%2520by%250Aaveraging%2520can%2520result%2520in%2520information%2520loss%2520and%2520reduced%2520accuracy.%2520We%2520present%2520a%250Aconceptually%2520novel%2520two-level%2520stacking%2520ensemble%2520%2528SE%2529%2520approach.%2520The%2520first%2520level%250Acomprises%2520regional%2520models%2520for%2520predicting%2520individuals%2527%2520age%2520based%2520on%2520voxel-wise%250Ainformation%252C%2520fused%2520by%2520a%2520second-level%2520model%2520yielding%2520final%2520predictions.%2520Eight%250Adata%2520fusion%2520scenarios%2520were%2520explored%2520using%2520as%2520input%2520Gray%2520matter%2520volume%2520%2528GMV%2529%250Aestimates%2520from%2520four%2520datasets%2520covering%2520the%2520adult%2520lifespan.%2520Performance%252C%2520measured%250Ausing%2520mean%2520absolute%2520error%2520%2528MAE%2529%252C%2520R2%252C%2520correlation%2520and%2520prediction%2520bias%252C%2520showed%250Athat%2520SE%2520outperformed%2520the%2520region-wise%2520averages.%2520The%2520best%2520performance%2520was%250Aobtained%2520when%2520first-level%2520regional%2520predictions%2520were%2520obtained%2520as%2520out-of-sample%250Apredictions%2520on%2520the%2520application%2520site%2520with%2520second-level%2520models%2520trained%2520on%250Aindependent%2520and%2520site-specific%2520data%2520%2528MAE%253D4.75%2520vs%2520baseline%2520regional%2520mean%2520GMV%250AMAE%253D5.68%2529.%2520Performance%2520improved%2520as%2520more%2520datasets%2520were%2520used%2520for%2520training.%250AFirst-level%2520predictions%2520showed%2520improved%2520and%2520more%2520robust%2520aging%2520signal%2520providing%250Anew%2520biological%2520insights%2520and%2520enhanced%2520data%2520privacy.%2520Overall%252C%2520the%2520SE%2520improves%250Aaccuracy%2520compared%2520to%2520the%2520baseline%2520while%2520preserving%2520or%2520enhancing%2520data%2520privacy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10153v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Region-wise%20stacking%20ensembles%20for%20estimating%20brain-age%20using%20MRI&entry.906535625=Georgios%20Antonopoulos%20and%20Shammi%20More%20and%20Simon%20B.%20Eickhoff%20and%20Federico%20Raimondo%20and%20Kaustubh%20R.%20Patil&entry.1292438233=%20%20Predictive%20modeling%20using%20structural%20magnetic%20resonance%20imaging%20%28MRI%29%20data%20is%0Aa%20prominent%20approach%20to%20study%20brain-aging.%20Machine%20learning%20algorithms%20and%0Afeature%20extraction%20methods%20have%20been%20employed%20to%20improve%20predictions%20and%0Aexplore%20healthy%20and%20accelerated%20aging%20e.g.%20neurodegenerative%20and%20psychiatric%0Adisorders.%20The%20high-dimensional%20MRI%20data%20pose%20challenges%20to%20building%0Ageneralizable%20and%20interpretable%20models%20as%20well%20as%20for%20data%20privacy.%20Common%0Apractices%20are%20resampling%20or%20averaging%20voxels%20within%20predefined%20parcels%2C%20which%0Areduces%20anatomical%20specificity%20and%20biological%20interpretability%20as%20voxels%20within%0Aa%20region%20may%20differently%20relate%20to%20aging.%20Effectively%2C%20naive%20fusion%20by%0Aaveraging%20can%20result%20in%20information%20loss%20and%20reduced%20accuracy.%20We%20present%20a%0Aconceptually%20novel%20two-level%20stacking%20ensemble%20%28SE%29%20approach.%20The%20first%20level%0Acomprises%20regional%20models%20for%20predicting%20individuals%27%20age%20based%20on%20voxel-wise%0Ainformation%2C%20fused%20by%20a%20second-level%20model%20yielding%20final%20predictions.%20Eight%0Adata%20fusion%20scenarios%20were%20explored%20using%20as%20input%20Gray%20matter%20volume%20%28GMV%29%0Aestimates%20from%20four%20datasets%20covering%20the%20adult%20lifespan.%20Performance%2C%20measured%0Ausing%20mean%20absolute%20error%20%28MAE%29%2C%20R2%2C%20correlation%20and%20prediction%20bias%2C%20showed%0Athat%20SE%20outperformed%20the%20region-wise%20averages.%20The%20best%20performance%20was%0Aobtained%20when%20first-level%20regional%20predictions%20were%20obtained%20as%20out-of-sample%0Apredictions%20on%20the%20application%20site%20with%20second-level%20models%20trained%20on%0Aindependent%20and%20site-specific%20data%20%28MAE%3D4.75%20vs%20baseline%20regional%20mean%20GMV%0AMAE%3D5.68%29.%20Performance%20improved%20as%20more%20datasets%20were%20used%20for%20training.%0AFirst-level%20predictions%20showed%20improved%20and%20more%20robust%20aging%20signal%20providing%0Anew%20biological%20insights%20and%20enhanced%20data%20privacy.%20Overall%2C%20the%20SE%20improves%0Aaccuracy%20compared%20to%20the%20baseline%20while%20preserving%20or%20enhancing%20data%20privacy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10153v1&entry.124074799=Read"},
{"title": "Classifier Ensemble for Efficient Uncertainty Calibration of Deep Neural\n  Networks for Image Classification", "author": "Michael Schulze and Nikolas Ebert and Laurenz Reichardt and Oliver Wasenm\u00fcller", "abstract": "  This paper investigates novel classifier ensemble techniques for uncertainty\ncalibration applied to various deep neural networks for image classification.\nWe evaluate both accuracy and calibration metrics, focusing on Expected\nCalibration Error (ECE) and Maximum Calibration Error (MCE). Our work compares\ndifferent methods for building simple yet efficient classifier ensembles,\nincluding majority voting and several metamodel-based approaches. Our\nevaluation reveals that while state-of-the-art deep neural networks for image\nclassification achieve high accuracy on standard datasets, they frequently\nsuffer from significant calibration errors. Basic ensemble techniques like\nmajority voting provide modest improvements, while metamodel-based ensembles\nconsistently reduce ECE and MCE across all architectures. Notably, the largest\nof our compared metamodels demonstrate the most substantial calibration\nimprovements, with minimal impact on accuracy. Moreover, classifier ensembles\nwith metamodels outperform traditional model ensembles in calibration\nperformance, while requiring significantly fewer parameters. In comparison to\ntraditional post-hoc calibration methods, our approach removes the need for a\nseparate calibration dataset. These findings underscore the potential of our\nproposed metamodel-based classifier ensembles as an efficient and effective\napproach to improving model calibration, thereby contributing to more reliable\ndeep learning systems.\n", "link": "http://arxiv.org/abs/2501.10089v1", "date": "2025-01-17", "relevancy": 2.1864, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6079}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5468}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5219}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Classifier%20Ensemble%20for%20Efficient%20Uncertainty%20Calibration%20of%20Deep%20Neural%0A%20%20Networks%20for%20Image%20Classification&body=Title%3A%20Classifier%20Ensemble%20for%20Efficient%20Uncertainty%20Calibration%20of%20Deep%20Neural%0A%20%20Networks%20for%20Image%20Classification%0AAuthor%3A%20Michael%20Schulze%20and%20Nikolas%20Ebert%20and%20Laurenz%20Reichardt%20and%20Oliver%20Wasenm%C3%BCller%0AAbstract%3A%20%20%20This%20paper%20investigates%20novel%20classifier%20ensemble%20techniques%20for%20uncertainty%0Acalibration%20applied%20to%20various%20deep%20neural%20networks%20for%20image%20classification.%0AWe%20evaluate%20both%20accuracy%20and%20calibration%20metrics%2C%20focusing%20on%20Expected%0ACalibration%20Error%20%28ECE%29%20and%20Maximum%20Calibration%20Error%20%28MCE%29.%20Our%20work%20compares%0Adifferent%20methods%20for%20building%20simple%20yet%20efficient%20classifier%20ensembles%2C%0Aincluding%20majority%20voting%20and%20several%20metamodel-based%20approaches.%20Our%0Aevaluation%20reveals%20that%20while%20state-of-the-art%20deep%20neural%20networks%20for%20image%0Aclassification%20achieve%20high%20accuracy%20on%20standard%20datasets%2C%20they%20frequently%0Asuffer%20from%20significant%20calibration%20errors.%20Basic%20ensemble%20techniques%20like%0Amajority%20voting%20provide%20modest%20improvements%2C%20while%20metamodel-based%20ensembles%0Aconsistently%20reduce%20ECE%20and%20MCE%20across%20all%20architectures.%20Notably%2C%20the%20largest%0Aof%20our%20compared%20metamodels%20demonstrate%20the%20most%20substantial%20calibration%0Aimprovements%2C%20with%20minimal%20impact%20on%20accuracy.%20Moreover%2C%20classifier%20ensembles%0Awith%20metamodels%20outperform%20traditional%20model%20ensembles%20in%20calibration%0Aperformance%2C%20while%20requiring%20significantly%20fewer%20parameters.%20In%20comparison%20to%0Atraditional%20post-hoc%20calibration%20methods%2C%20our%20approach%20removes%20the%20need%20for%20a%0Aseparate%20calibration%20dataset.%20These%20findings%20underscore%20the%20potential%20of%20our%0Aproposed%20metamodel-based%20classifier%20ensembles%20as%20an%20efficient%20and%20effective%0Aapproach%20to%20improving%20model%20calibration%2C%20thereby%20contributing%20to%20more%20reliable%0Adeep%20learning%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10089v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClassifier%2520Ensemble%2520for%2520Efficient%2520Uncertainty%2520Calibration%2520of%2520Deep%2520Neural%250A%2520%2520Networks%2520for%2520Image%2520Classification%26entry.906535625%3DMichael%2520Schulze%2520and%2520Nikolas%2520Ebert%2520and%2520Laurenz%2520Reichardt%2520and%2520Oliver%2520Wasenm%25C3%25BCller%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520novel%2520classifier%2520ensemble%2520techniques%2520for%2520uncertainty%250Acalibration%2520applied%2520to%2520various%2520deep%2520neural%2520networks%2520for%2520image%2520classification.%250AWe%2520evaluate%2520both%2520accuracy%2520and%2520calibration%2520metrics%252C%2520focusing%2520on%2520Expected%250ACalibration%2520Error%2520%2528ECE%2529%2520and%2520Maximum%2520Calibration%2520Error%2520%2528MCE%2529.%2520Our%2520work%2520compares%250Adifferent%2520methods%2520for%2520building%2520simple%2520yet%2520efficient%2520classifier%2520ensembles%252C%250Aincluding%2520majority%2520voting%2520and%2520several%2520metamodel-based%2520approaches.%2520Our%250Aevaluation%2520reveals%2520that%2520while%2520state-of-the-art%2520deep%2520neural%2520networks%2520for%2520image%250Aclassification%2520achieve%2520high%2520accuracy%2520on%2520standard%2520datasets%252C%2520they%2520frequently%250Asuffer%2520from%2520significant%2520calibration%2520errors.%2520Basic%2520ensemble%2520techniques%2520like%250Amajority%2520voting%2520provide%2520modest%2520improvements%252C%2520while%2520metamodel-based%2520ensembles%250Aconsistently%2520reduce%2520ECE%2520and%2520MCE%2520across%2520all%2520architectures.%2520Notably%252C%2520the%2520largest%250Aof%2520our%2520compared%2520metamodels%2520demonstrate%2520the%2520most%2520substantial%2520calibration%250Aimprovements%252C%2520with%2520minimal%2520impact%2520on%2520accuracy.%2520Moreover%252C%2520classifier%2520ensembles%250Awith%2520metamodels%2520outperform%2520traditional%2520model%2520ensembles%2520in%2520calibration%250Aperformance%252C%2520while%2520requiring%2520significantly%2520fewer%2520parameters.%2520In%2520comparison%2520to%250Atraditional%2520post-hoc%2520calibration%2520methods%252C%2520our%2520approach%2520removes%2520the%2520need%2520for%2520a%250Aseparate%2520calibration%2520dataset.%2520These%2520findings%2520underscore%2520the%2520potential%2520of%2520our%250Aproposed%2520metamodel-based%2520classifier%2520ensembles%2520as%2520an%2520efficient%2520and%2520effective%250Aapproach%2520to%2520improving%2520model%2520calibration%252C%2520thereby%2520contributing%2520to%2520more%2520reliable%250Adeep%2520learning%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10089v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Classifier%20Ensemble%20for%20Efficient%20Uncertainty%20Calibration%20of%20Deep%20Neural%0A%20%20Networks%20for%20Image%20Classification&entry.906535625=Michael%20Schulze%20and%20Nikolas%20Ebert%20and%20Laurenz%20Reichardt%20and%20Oliver%20Wasenm%C3%BCller&entry.1292438233=%20%20This%20paper%20investigates%20novel%20classifier%20ensemble%20techniques%20for%20uncertainty%0Acalibration%20applied%20to%20various%20deep%20neural%20networks%20for%20image%20classification.%0AWe%20evaluate%20both%20accuracy%20and%20calibration%20metrics%2C%20focusing%20on%20Expected%0ACalibration%20Error%20%28ECE%29%20and%20Maximum%20Calibration%20Error%20%28MCE%29.%20Our%20work%20compares%0Adifferent%20methods%20for%20building%20simple%20yet%20efficient%20classifier%20ensembles%2C%0Aincluding%20majority%20voting%20and%20several%20metamodel-based%20approaches.%20Our%0Aevaluation%20reveals%20that%20while%20state-of-the-art%20deep%20neural%20networks%20for%20image%0Aclassification%20achieve%20high%20accuracy%20on%20standard%20datasets%2C%20they%20frequently%0Asuffer%20from%20significant%20calibration%20errors.%20Basic%20ensemble%20techniques%20like%0Amajority%20voting%20provide%20modest%20improvements%2C%20while%20metamodel-based%20ensembles%0Aconsistently%20reduce%20ECE%20and%20MCE%20across%20all%20architectures.%20Notably%2C%20the%20largest%0Aof%20our%20compared%20metamodels%20demonstrate%20the%20most%20substantial%20calibration%0Aimprovements%2C%20with%20minimal%20impact%20on%20accuracy.%20Moreover%2C%20classifier%20ensembles%0Awith%20metamodels%20outperform%20traditional%20model%20ensembles%20in%20calibration%0Aperformance%2C%20while%20requiring%20significantly%20fewer%20parameters.%20In%20comparison%20to%0Atraditional%20post-hoc%20calibration%20methods%2C%20our%20approach%20removes%20the%20need%20for%20a%0Aseparate%20calibration%20dataset.%20These%20findings%20underscore%20the%20potential%20of%20our%0Aproposed%20metamodel-based%20classifier%20ensembles%20as%20an%20efficient%20and%20effective%0Aapproach%20to%20improving%20model%20calibration%2C%20thereby%20contributing%20to%20more%20reliable%0Adeep%20learning%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10089v1&entry.124074799=Read"},
{"title": "MVTamperBench: Evaluating Robustness of Vision-Language Models", "author": "Amit Agarwal and Srikant Panda and Angeline Charles and Bhargava Kumar and Hitesh Patel and Priyaranjan Pattnayak and Taki Hasan Rafi and Tejaswini Kumar and Dong-Kyu Chae", "abstract": "  Multimodal Large Language Models (MLLMs) have driven major advances in video\nunderstanding, yet their vulnerability to adversarial tampering and\nmanipulations remains underexplored. To address this gap, we introduce\nMVTamperBench, a benchmark that systematically evaluates MLLM robustness\nagainst five prevalent tampering techniques: rotation, masking, substitution,\nrepetition, and dropping. Built from 3.4K original videos-expanded to over 17K\ntampered clips spanning 19 video tasks.\n  MVTamperBench challenges models to detect manipulations in spatial and\ntemporal coherence. We evaluate 45 recent MLLMs from 15+ model families,\nrevealing substantial variability in resilience across tampering types and\nshowing that larger parameter counts do not necessarily guarantee robustness.\nMVTamperBench sets a new benchmark for developing tamper-resilient MLLM in\nsafety-critical applications, including detecting clickbait, preventing harmful\ncontent distribution, and enforcing policies on media platforms. We release all\ncode and data to foster open research in trustworthy video understanding.\n  Code: https://amitbcp.github.io/MVTamperBench/ Data:\nhttps://huggingface.co/datasets/Srikant86/MVTamperBench\n", "link": "http://arxiv.org/abs/2412.19794v4", "date": "2025-01-17", "relevancy": 2.1731, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5684}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5389}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5199}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MVTamperBench%3A%20Evaluating%20Robustness%20of%20Vision-Language%20Models&body=Title%3A%20MVTamperBench%3A%20Evaluating%20Robustness%20of%20Vision-Language%20Models%0AAuthor%3A%20Amit%20Agarwal%20and%20Srikant%20Panda%20and%20Angeline%20Charles%20and%20Bhargava%20Kumar%20and%20Hitesh%20Patel%20and%20Priyaranjan%20Pattnayak%20and%20Taki%20Hasan%20Rafi%20and%20Tejaswini%20Kumar%20and%20Dong-Kyu%20Chae%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20driven%20major%20advances%20in%20video%0Aunderstanding%2C%20yet%20their%20vulnerability%20to%20adversarial%20tampering%20and%0Amanipulations%20remains%20underexplored.%20To%20address%20this%20gap%2C%20we%20introduce%0AMVTamperBench%2C%20a%20benchmark%20that%20systematically%20evaluates%20MLLM%20robustness%0Aagainst%20five%20prevalent%20tampering%20techniques%3A%20rotation%2C%20masking%2C%20substitution%2C%0Arepetition%2C%20and%20dropping.%20Built%20from%203.4K%20original%20videos-expanded%20to%20over%2017K%0Atampered%20clips%20spanning%2019%20video%20tasks.%0A%20%20MVTamperBench%20challenges%20models%20to%20detect%20manipulations%20in%20spatial%20and%0Atemporal%20coherence.%20We%20evaluate%2045%20recent%20MLLMs%20from%2015%2B%20model%20families%2C%0Arevealing%20substantial%20variability%20in%20resilience%20across%20tampering%20types%20and%0Ashowing%20that%20larger%20parameter%20counts%20do%20not%20necessarily%20guarantee%20robustness.%0AMVTamperBench%20sets%20a%20new%20benchmark%20for%20developing%20tamper-resilient%20MLLM%20in%0Asafety-critical%20applications%2C%20including%20detecting%20clickbait%2C%20preventing%20harmful%0Acontent%20distribution%2C%20and%20enforcing%20policies%20on%20media%20platforms.%20We%20release%20all%0Acode%20and%20data%20to%20foster%20open%20research%20in%20trustworthy%20video%20understanding.%0A%20%20Code%3A%20https%3A//amitbcp.github.io/MVTamperBench/%20Data%3A%0Ahttps%3A//huggingface.co/datasets/Srikant86/MVTamperBench%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19794v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMVTamperBench%253A%2520Evaluating%2520Robustness%2520of%2520Vision-Language%2520Models%26entry.906535625%3DAmit%2520Agarwal%2520and%2520Srikant%2520Panda%2520and%2520Angeline%2520Charles%2520and%2520Bhargava%2520Kumar%2520and%2520Hitesh%2520Patel%2520and%2520Priyaranjan%2520Pattnayak%2520and%2520Taki%2520Hasan%2520Rafi%2520and%2520Tejaswini%2520Kumar%2520and%2520Dong-Kyu%2520Chae%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520driven%2520major%2520advances%2520in%2520video%250Aunderstanding%252C%2520yet%2520their%2520vulnerability%2520to%2520adversarial%2520tampering%2520and%250Amanipulations%2520remains%2520underexplored.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%250AMVTamperBench%252C%2520a%2520benchmark%2520that%2520systematically%2520evaluates%2520MLLM%2520robustness%250Aagainst%2520five%2520prevalent%2520tampering%2520techniques%253A%2520rotation%252C%2520masking%252C%2520substitution%252C%250Arepetition%252C%2520and%2520dropping.%2520Built%2520from%25203.4K%2520original%2520videos-expanded%2520to%2520over%252017K%250Atampered%2520clips%2520spanning%252019%2520video%2520tasks.%250A%2520%2520MVTamperBench%2520challenges%2520models%2520to%2520detect%2520manipulations%2520in%2520spatial%2520and%250Atemporal%2520coherence.%2520We%2520evaluate%252045%2520recent%2520MLLMs%2520from%252015%252B%2520model%2520families%252C%250Arevealing%2520substantial%2520variability%2520in%2520resilience%2520across%2520tampering%2520types%2520and%250Ashowing%2520that%2520larger%2520parameter%2520counts%2520do%2520not%2520necessarily%2520guarantee%2520robustness.%250AMVTamperBench%2520sets%2520a%2520new%2520benchmark%2520for%2520developing%2520tamper-resilient%2520MLLM%2520in%250Asafety-critical%2520applications%252C%2520including%2520detecting%2520clickbait%252C%2520preventing%2520harmful%250Acontent%2520distribution%252C%2520and%2520enforcing%2520policies%2520on%2520media%2520platforms.%2520We%2520release%2520all%250Acode%2520and%2520data%2520to%2520foster%2520open%2520research%2520in%2520trustworthy%2520video%2520understanding.%250A%2520%2520Code%253A%2520https%253A//amitbcp.github.io/MVTamperBench/%2520Data%253A%250Ahttps%253A//huggingface.co/datasets/Srikant86/MVTamperBench%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19794v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVTamperBench%3A%20Evaluating%20Robustness%20of%20Vision-Language%20Models&entry.906535625=Amit%20Agarwal%20and%20Srikant%20Panda%20and%20Angeline%20Charles%20and%20Bhargava%20Kumar%20and%20Hitesh%20Patel%20and%20Priyaranjan%20Pattnayak%20and%20Taki%20Hasan%20Rafi%20and%20Tejaswini%20Kumar%20and%20Dong-Kyu%20Chae&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20driven%20major%20advances%20in%20video%0Aunderstanding%2C%20yet%20their%20vulnerability%20to%20adversarial%20tampering%20and%0Amanipulations%20remains%20underexplored.%20To%20address%20this%20gap%2C%20we%20introduce%0AMVTamperBench%2C%20a%20benchmark%20that%20systematically%20evaluates%20MLLM%20robustness%0Aagainst%20five%20prevalent%20tampering%20techniques%3A%20rotation%2C%20masking%2C%20substitution%2C%0Arepetition%2C%20and%20dropping.%20Built%20from%203.4K%20original%20videos-expanded%20to%20over%2017K%0Atampered%20clips%20spanning%2019%20video%20tasks.%0A%20%20MVTamperBench%20challenges%20models%20to%20detect%20manipulations%20in%20spatial%20and%0Atemporal%20coherence.%20We%20evaluate%2045%20recent%20MLLMs%20from%2015%2B%20model%20families%2C%0Arevealing%20substantial%20variability%20in%20resilience%20across%20tampering%20types%20and%0Ashowing%20that%20larger%20parameter%20counts%20do%20not%20necessarily%20guarantee%20robustness.%0AMVTamperBench%20sets%20a%20new%20benchmark%20for%20developing%20tamper-resilient%20MLLM%20in%0Asafety-critical%20applications%2C%20including%20detecting%20clickbait%2C%20preventing%20harmful%0Acontent%20distribution%2C%20and%20enforcing%20policies%20on%20media%20platforms.%20We%20release%20all%0Acode%20and%20data%20to%20foster%20open%20research%20in%20trustworthy%20video%20understanding.%0A%20%20Code%3A%20https%3A//amitbcp.github.io/MVTamperBench/%20Data%3A%0Ahttps%3A//huggingface.co/datasets/Srikant86/MVTamperBench%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19794v4&entry.124074799=Read"},
{"title": "Leveraging Confident Image Regions for Source-Free Domain-Adaptive\n  Object Detection", "author": "Mohamed Lamine Mekhalfi and Davide Boscaini and Fabio Poiesi", "abstract": "  Source-free domain-adaptive object detection is an interesting but scarcely\naddressed topic. It aims at adapting a source-pretrained detector to a distinct\ntarget domain without resorting to source data during adaptation. So far, there\nis no data augmentation scheme tailored to source-free domain-adaptive object\ndetection. To this end, this paper presents a novel data augmentation approach\nthat cuts out target image regions where the detector is confident, augments\nthem along with their respective pseudo-labels, and joins them into a\nchallenging target image to adapt the detector. As the source data is out of\nreach during adaptation, we implement our approach within a teacher-student\nlearning paradigm to ensure that the model does not collapse during the\nadaptation procedure. We evaluated our approach on three adaptation benchmarks\nof traffic scenes, scoring new state-of-the-art on two of them.\n", "link": "http://arxiv.org/abs/2501.10081v1", "date": "2025-01-17", "relevancy": 2.1681, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.546}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5443}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Confident%20Image%20Regions%20for%20Source-Free%20Domain-Adaptive%0A%20%20Object%20Detection&body=Title%3A%20Leveraging%20Confident%20Image%20Regions%20for%20Source-Free%20Domain-Adaptive%0A%20%20Object%20Detection%0AAuthor%3A%20Mohamed%20Lamine%20Mekhalfi%20and%20Davide%20Boscaini%20and%20Fabio%20Poiesi%0AAbstract%3A%20%20%20Source-free%20domain-adaptive%20object%20detection%20is%20an%20interesting%20but%20scarcely%0Aaddressed%20topic.%20It%20aims%20at%20adapting%20a%20source-pretrained%20detector%20to%20a%20distinct%0Atarget%20domain%20without%20resorting%20to%20source%20data%20during%20adaptation.%20So%20far%2C%20there%0Ais%20no%20data%20augmentation%20scheme%20tailored%20to%20source-free%20domain-adaptive%20object%0Adetection.%20To%20this%20end%2C%20this%20paper%20presents%20a%20novel%20data%20augmentation%20approach%0Athat%20cuts%20out%20target%20image%20regions%20where%20the%20detector%20is%20confident%2C%20augments%0Athem%20along%20with%20their%20respective%20pseudo-labels%2C%20and%20joins%20them%20into%20a%0Achallenging%20target%20image%20to%20adapt%20the%20detector.%20As%20the%20source%20data%20is%20out%20of%0Areach%20during%20adaptation%2C%20we%20implement%20our%20approach%20within%20a%20teacher-student%0Alearning%20paradigm%20to%20ensure%20that%20the%20model%20does%20not%20collapse%20during%20the%0Aadaptation%20procedure.%20We%20evaluated%20our%20approach%20on%20three%20adaptation%20benchmarks%0Aof%20traffic%20scenes%2C%20scoring%20new%20state-of-the-art%20on%20two%20of%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10081v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Confident%2520Image%2520Regions%2520for%2520Source-Free%2520Domain-Adaptive%250A%2520%2520Object%2520Detection%26entry.906535625%3DMohamed%2520Lamine%2520Mekhalfi%2520and%2520Davide%2520Boscaini%2520and%2520Fabio%2520Poiesi%26entry.1292438233%3D%2520%2520Source-free%2520domain-adaptive%2520object%2520detection%2520is%2520an%2520interesting%2520but%2520scarcely%250Aaddressed%2520topic.%2520It%2520aims%2520at%2520adapting%2520a%2520source-pretrained%2520detector%2520to%2520a%2520distinct%250Atarget%2520domain%2520without%2520resorting%2520to%2520source%2520data%2520during%2520adaptation.%2520So%2520far%252C%2520there%250Ais%2520no%2520data%2520augmentation%2520scheme%2520tailored%2520to%2520source-free%2520domain-adaptive%2520object%250Adetection.%2520To%2520this%2520end%252C%2520this%2520paper%2520presents%2520a%2520novel%2520data%2520augmentation%2520approach%250Athat%2520cuts%2520out%2520target%2520image%2520regions%2520where%2520the%2520detector%2520is%2520confident%252C%2520augments%250Athem%2520along%2520with%2520their%2520respective%2520pseudo-labels%252C%2520and%2520joins%2520them%2520into%2520a%250Achallenging%2520target%2520image%2520to%2520adapt%2520the%2520detector.%2520As%2520the%2520source%2520data%2520is%2520out%2520of%250Areach%2520during%2520adaptation%252C%2520we%2520implement%2520our%2520approach%2520within%2520a%2520teacher-student%250Alearning%2520paradigm%2520to%2520ensure%2520that%2520the%2520model%2520does%2520not%2520collapse%2520during%2520the%250Aadaptation%2520procedure.%2520We%2520evaluated%2520our%2520approach%2520on%2520three%2520adaptation%2520benchmarks%250Aof%2520traffic%2520scenes%252C%2520scoring%2520new%2520state-of-the-art%2520on%2520two%2520of%2520them.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10081v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Confident%20Image%20Regions%20for%20Source-Free%20Domain-Adaptive%0A%20%20Object%20Detection&entry.906535625=Mohamed%20Lamine%20Mekhalfi%20and%20Davide%20Boscaini%20and%20Fabio%20Poiesi&entry.1292438233=%20%20Source-free%20domain-adaptive%20object%20detection%20is%20an%20interesting%20but%20scarcely%0Aaddressed%20topic.%20It%20aims%20at%20adapting%20a%20source-pretrained%20detector%20to%20a%20distinct%0Atarget%20domain%20without%20resorting%20to%20source%20data%20during%20adaptation.%20So%20far%2C%20there%0Ais%20no%20data%20augmentation%20scheme%20tailored%20to%20source-free%20domain-adaptive%20object%0Adetection.%20To%20this%20end%2C%20this%20paper%20presents%20a%20novel%20data%20augmentation%20approach%0Athat%20cuts%20out%20target%20image%20regions%20where%20the%20detector%20is%20confident%2C%20augments%0Athem%20along%20with%20their%20respective%20pseudo-labels%2C%20and%20joins%20them%20into%20a%0Achallenging%20target%20image%20to%20adapt%20the%20detector.%20As%20the%20source%20data%20is%20out%20of%0Areach%20during%20adaptation%2C%20we%20implement%20our%20approach%20within%20a%20teacher-student%0Alearning%20paradigm%20to%20ensure%20that%20the%20model%20does%20not%20collapse%20during%20the%0Aadaptation%20procedure.%20We%20evaluated%20our%20approach%20on%20three%20adaptation%20benchmarks%0Aof%20traffic%20scenes%2C%20scoring%20new%20state-of-the-art%20on%20two%20of%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10081v1&entry.124074799=Read"},
{"title": "Bridging Diversity and Uncertainty in Active learning with\n  Self-Supervised Pre-Training", "author": "Paul Doucet and Benjamin Estermann and Till Aczel and Roger Wattenhofer", "abstract": "  This study addresses the integration of diversity-based and uncertainty-based\nsampling strategies in active learning, particularly within the context of\nself-supervised pre-trained models. We introduce a straightforward heuristic\ncalled TCM that mitigates the cold start problem while maintaining strong\nperformance across various data levels. By initially applying TypiClust for\ndiversity sampling and subsequently transitioning to uncertainty sampling with\nMargin, our approach effectively combines the strengths of both strategies. Our\nexperiments demonstrate that TCM consistently outperforms existing methods\nacross various datasets in both low and high data regimes.\n", "link": "http://arxiv.org/abs/2403.03728v2", "date": "2025-01-17", "relevancy": 2.1601, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5509}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5387}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5161}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Diversity%20and%20Uncertainty%20in%20Active%20learning%20with%0A%20%20Self-Supervised%20Pre-Training&body=Title%3A%20Bridging%20Diversity%20and%20Uncertainty%20in%20Active%20learning%20with%0A%20%20Self-Supervised%20Pre-Training%0AAuthor%3A%20Paul%20Doucet%20and%20Benjamin%20Estermann%20and%20Till%20Aczel%20and%20Roger%20Wattenhofer%0AAbstract%3A%20%20%20This%20study%20addresses%20the%20integration%20of%20diversity-based%20and%20uncertainty-based%0Asampling%20strategies%20in%20active%20learning%2C%20particularly%20within%20the%20context%20of%0Aself-supervised%20pre-trained%20models.%20We%20introduce%20a%20straightforward%20heuristic%0Acalled%20TCM%20that%20mitigates%20the%20cold%20start%20problem%20while%20maintaining%20strong%0Aperformance%20across%20various%20data%20levels.%20By%20initially%20applying%20TypiClust%20for%0Adiversity%20sampling%20and%20subsequently%20transitioning%20to%20uncertainty%20sampling%20with%0AMargin%2C%20our%20approach%20effectively%20combines%20the%20strengths%20of%20both%20strategies.%20Our%0Aexperiments%20demonstrate%20that%20TCM%20consistently%20outperforms%20existing%20methods%0Aacross%20various%20datasets%20in%20both%20low%20and%20high%20data%20regimes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.03728v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Diversity%2520and%2520Uncertainty%2520in%2520Active%2520learning%2520with%250A%2520%2520Self-Supervised%2520Pre-Training%26entry.906535625%3DPaul%2520Doucet%2520and%2520Benjamin%2520Estermann%2520and%2520Till%2520Aczel%2520and%2520Roger%2520Wattenhofer%26entry.1292438233%3D%2520%2520This%2520study%2520addresses%2520the%2520integration%2520of%2520diversity-based%2520and%2520uncertainty-based%250Asampling%2520strategies%2520in%2520active%2520learning%252C%2520particularly%2520within%2520the%2520context%2520of%250Aself-supervised%2520pre-trained%2520models.%2520We%2520introduce%2520a%2520straightforward%2520heuristic%250Acalled%2520TCM%2520that%2520mitigates%2520the%2520cold%2520start%2520problem%2520while%2520maintaining%2520strong%250Aperformance%2520across%2520various%2520data%2520levels.%2520By%2520initially%2520applying%2520TypiClust%2520for%250Adiversity%2520sampling%2520and%2520subsequently%2520transitioning%2520to%2520uncertainty%2520sampling%2520with%250AMargin%252C%2520our%2520approach%2520effectively%2520combines%2520the%2520strengths%2520of%2520both%2520strategies.%2520Our%250Aexperiments%2520demonstrate%2520that%2520TCM%2520consistently%2520outperforms%2520existing%2520methods%250Aacross%2520various%2520datasets%2520in%2520both%2520low%2520and%2520high%2520data%2520regimes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.03728v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Diversity%20and%20Uncertainty%20in%20Active%20learning%20with%0A%20%20Self-Supervised%20Pre-Training&entry.906535625=Paul%20Doucet%20and%20Benjamin%20Estermann%20and%20Till%20Aczel%20and%20Roger%20Wattenhofer&entry.1292438233=%20%20This%20study%20addresses%20the%20integration%20of%20diversity-based%20and%20uncertainty-based%0Asampling%20strategies%20in%20active%20learning%2C%20particularly%20within%20the%20context%20of%0Aself-supervised%20pre-trained%20models.%20We%20introduce%20a%20straightforward%20heuristic%0Acalled%20TCM%20that%20mitigates%20the%20cold%20start%20problem%20while%20maintaining%20strong%0Aperformance%20across%20various%20data%20levels.%20By%20initially%20applying%20TypiClust%20for%0Adiversity%20sampling%20and%20subsequently%20transitioning%20to%20uncertainty%20sampling%20with%0AMargin%2C%20our%20approach%20effectively%20combines%20the%20strengths%20of%20both%20strategies.%20Our%0Aexperiments%20demonstrate%20that%20TCM%20consistently%20outperforms%20existing%20methods%0Aacross%20various%20datasets%20in%20both%20low%20and%20high%20data%20regimes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03728v2&entry.124074799=Read"},
{"title": "Model Synthesis for Zero-Shot Model Attribution", "author": "Tianyun Yang and Juan Cao and Danding Wang and Chang Xu", "abstract": "  Nowadays, generative models are shaping various fields such as art, design,\nand human-computer interaction, yet accompanied by challenges related to\ncopyright infringement and content management. In response, existing research\nseeks to identify the unique fingerprints on the images they generate, which\ncan be leveraged to attribute the generated images to their source models.\nExisting methods, however, are constrained to identifying models within a\nstatic set included in the classifier training, failing to adapt to newly\nemerged unseen models dynamically. To bridge this gap, we aim to develop a\ngeneralized model fingerprint extractor capable of zero-shot attribution,\neffectively attributes unseen models without exposure during training. Central\nto our method is a model synthesis technique, which generates numerous\nsynthetic models mimicking the fingerprint patterns of real-world generative\nmodels. The design of the synthesis technique is motivated by observations on\nhow the basic generative model's architecture building blocks and parameters\ninfluence fingerprint patterns, and it is validated through two designed\nmetrics that examine synthetic models' fidelity and diversity. Our experiments\ndemonstrate that this fingerprint extractor, trained solely on synthetic\nmodels, achieves impressive zero-shot generalization on a wide range of\nreal-world generative models, improving model identification and verification\naccuracy on unseen models by over 40% and 15%, respectively, compared to\nexisting approaches.\n", "link": "http://arxiv.org/abs/2307.15977v3", "date": "2025-01-17", "relevancy": 2.1528, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5668}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5387}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5262}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model%20Synthesis%20for%20Zero-Shot%20Model%20Attribution&body=Title%3A%20Model%20Synthesis%20for%20Zero-Shot%20Model%20Attribution%0AAuthor%3A%20Tianyun%20Yang%20and%20Juan%20Cao%20and%20Danding%20Wang%20and%20Chang%20Xu%0AAbstract%3A%20%20%20Nowadays%2C%20generative%20models%20are%20shaping%20various%20fields%20such%20as%20art%2C%20design%2C%0Aand%20human-computer%20interaction%2C%20yet%20accompanied%20by%20challenges%20related%20to%0Acopyright%20infringement%20and%20content%20management.%20In%20response%2C%20existing%20research%0Aseeks%20to%20identify%20the%20unique%20fingerprints%20on%20the%20images%20they%20generate%2C%20which%0Acan%20be%20leveraged%20to%20attribute%20the%20generated%20images%20to%20their%20source%20models.%0AExisting%20methods%2C%20however%2C%20are%20constrained%20to%20identifying%20models%20within%20a%0Astatic%20set%20included%20in%20the%20classifier%20training%2C%20failing%20to%20adapt%20to%20newly%0Aemerged%20unseen%20models%20dynamically.%20To%20bridge%20this%20gap%2C%20we%20aim%20to%20develop%20a%0Ageneralized%20model%20fingerprint%20extractor%20capable%20of%20zero-shot%20attribution%2C%0Aeffectively%20attributes%20unseen%20models%20without%20exposure%20during%20training.%20Central%0Ato%20our%20method%20is%20a%20model%20synthesis%20technique%2C%20which%20generates%20numerous%0Asynthetic%20models%20mimicking%20the%20fingerprint%20patterns%20of%20real-world%20generative%0Amodels.%20The%20design%20of%20the%20synthesis%20technique%20is%20motivated%20by%20observations%20on%0Ahow%20the%20basic%20generative%20model%27s%20architecture%20building%20blocks%20and%20parameters%0Ainfluence%20fingerprint%20patterns%2C%20and%20it%20is%20validated%20through%20two%20designed%0Ametrics%20that%20examine%20synthetic%20models%27%20fidelity%20and%20diversity.%20Our%20experiments%0Ademonstrate%20that%20this%20fingerprint%20extractor%2C%20trained%20solely%20on%20synthetic%0Amodels%2C%20achieves%20impressive%20zero-shot%20generalization%20on%20a%20wide%20range%20of%0Areal-world%20generative%20models%2C%20improving%20model%20identification%20and%20verification%0Aaccuracy%20on%20unseen%20models%20by%20over%2040%25%20and%2015%25%2C%20respectively%2C%20compared%20to%0Aexisting%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.15977v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel%2520Synthesis%2520for%2520Zero-Shot%2520Model%2520Attribution%26entry.906535625%3DTianyun%2520Yang%2520and%2520Juan%2520Cao%2520and%2520Danding%2520Wang%2520and%2520Chang%2520Xu%26entry.1292438233%3D%2520%2520Nowadays%252C%2520generative%2520models%2520are%2520shaping%2520various%2520fields%2520such%2520as%2520art%252C%2520design%252C%250Aand%2520human-computer%2520interaction%252C%2520yet%2520accompanied%2520by%2520challenges%2520related%2520to%250Acopyright%2520infringement%2520and%2520content%2520management.%2520In%2520response%252C%2520existing%2520research%250Aseeks%2520to%2520identify%2520the%2520unique%2520fingerprints%2520on%2520the%2520images%2520they%2520generate%252C%2520which%250Acan%2520be%2520leveraged%2520to%2520attribute%2520the%2520generated%2520images%2520to%2520their%2520source%2520models.%250AExisting%2520methods%252C%2520however%252C%2520are%2520constrained%2520to%2520identifying%2520models%2520within%2520a%250Astatic%2520set%2520included%2520in%2520the%2520classifier%2520training%252C%2520failing%2520to%2520adapt%2520to%2520newly%250Aemerged%2520unseen%2520models%2520dynamically.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520aim%2520to%2520develop%2520a%250Ageneralized%2520model%2520fingerprint%2520extractor%2520capable%2520of%2520zero-shot%2520attribution%252C%250Aeffectively%2520attributes%2520unseen%2520models%2520without%2520exposure%2520during%2520training.%2520Central%250Ato%2520our%2520method%2520is%2520a%2520model%2520synthesis%2520technique%252C%2520which%2520generates%2520numerous%250Asynthetic%2520models%2520mimicking%2520the%2520fingerprint%2520patterns%2520of%2520real-world%2520generative%250Amodels.%2520The%2520design%2520of%2520the%2520synthesis%2520technique%2520is%2520motivated%2520by%2520observations%2520on%250Ahow%2520the%2520basic%2520generative%2520model%2527s%2520architecture%2520building%2520blocks%2520and%2520parameters%250Ainfluence%2520fingerprint%2520patterns%252C%2520and%2520it%2520is%2520validated%2520through%2520two%2520designed%250Ametrics%2520that%2520examine%2520synthetic%2520models%2527%2520fidelity%2520and%2520diversity.%2520Our%2520experiments%250Ademonstrate%2520that%2520this%2520fingerprint%2520extractor%252C%2520trained%2520solely%2520on%2520synthetic%250Amodels%252C%2520achieves%2520impressive%2520zero-shot%2520generalization%2520on%2520a%2520wide%2520range%2520of%250Areal-world%2520generative%2520models%252C%2520improving%2520model%2520identification%2520and%2520verification%250Aaccuracy%2520on%2520unseen%2520models%2520by%2520over%252040%2525%2520and%252015%2525%252C%2520respectively%252C%2520compared%2520to%250Aexisting%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.15977v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model%20Synthesis%20for%20Zero-Shot%20Model%20Attribution&entry.906535625=Tianyun%20Yang%20and%20Juan%20Cao%20and%20Danding%20Wang%20and%20Chang%20Xu&entry.1292438233=%20%20Nowadays%2C%20generative%20models%20are%20shaping%20various%20fields%20such%20as%20art%2C%20design%2C%0Aand%20human-computer%20interaction%2C%20yet%20accompanied%20by%20challenges%20related%20to%0Acopyright%20infringement%20and%20content%20management.%20In%20response%2C%20existing%20research%0Aseeks%20to%20identify%20the%20unique%20fingerprints%20on%20the%20images%20they%20generate%2C%20which%0Acan%20be%20leveraged%20to%20attribute%20the%20generated%20images%20to%20their%20source%20models.%0AExisting%20methods%2C%20however%2C%20are%20constrained%20to%20identifying%20models%20within%20a%0Astatic%20set%20included%20in%20the%20classifier%20training%2C%20failing%20to%20adapt%20to%20newly%0Aemerged%20unseen%20models%20dynamically.%20To%20bridge%20this%20gap%2C%20we%20aim%20to%20develop%20a%0Ageneralized%20model%20fingerprint%20extractor%20capable%20of%20zero-shot%20attribution%2C%0Aeffectively%20attributes%20unseen%20models%20without%20exposure%20during%20training.%20Central%0Ato%20our%20method%20is%20a%20model%20synthesis%20technique%2C%20which%20generates%20numerous%0Asynthetic%20models%20mimicking%20the%20fingerprint%20patterns%20of%20real-world%20generative%0Amodels.%20The%20design%20of%20the%20synthesis%20technique%20is%20motivated%20by%20observations%20on%0Ahow%20the%20basic%20generative%20model%27s%20architecture%20building%20blocks%20and%20parameters%0Ainfluence%20fingerprint%20patterns%2C%20and%20it%20is%20validated%20through%20two%20designed%0Ametrics%20that%20examine%20synthetic%20models%27%20fidelity%20and%20diversity.%20Our%20experiments%0Ademonstrate%20that%20this%20fingerprint%20extractor%2C%20trained%20solely%20on%20synthetic%0Amodels%2C%20achieves%20impressive%20zero-shot%20generalization%20on%20a%20wide%20range%20of%0Areal-world%20generative%20models%2C%20improving%20model%20identification%20and%20verification%0Aaccuracy%20on%20unseen%20models%20by%20over%2040%25%20and%2015%25%2C%20respectively%2C%20compared%20to%0Aexisting%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.15977v3&entry.124074799=Read"},
{"title": "Modelling Activity Scheduling Behaviour with Deep Generative Machine\n  Learning", "author": "Fred Shone and Tim Hillel", "abstract": "  We model human activity scheduling behaviour using a deep generative machine\nlearning approach. Activity schedules, which represent the activities and\nassociated travel behaviours of individuals, are a core component of many\napplied models in the transport, energy and epidemiology domains. Our data\ndriven approach learns human preferences and scheduling logic without the need\nfor complex interacting combinations of sub-models and custom-rules, this makes\nour approach significantly faster and simpler to operate that existing\napproaches. We find activity schedule data combines aspects of both continuous\nimage data and also discrete text data, requiring novel approaches. We\nadditionally contribute a novel schedule representation and comprehensive\nevaluation framework for generated schedules. Evaluation shows our approach is\nable to rapidly generate large, diverse and realistic synthetic samples of\nactivity schedules.\n", "link": "http://arxiv.org/abs/2501.10221v1", "date": "2025-01-17", "relevancy": 2.1459, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5541}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5268}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modelling%20Activity%20Scheduling%20Behaviour%20with%20Deep%20Generative%20Machine%0A%20%20Learning&body=Title%3A%20Modelling%20Activity%20Scheduling%20Behaviour%20with%20Deep%20Generative%20Machine%0A%20%20Learning%0AAuthor%3A%20Fred%20Shone%20and%20Tim%20Hillel%0AAbstract%3A%20%20%20We%20model%20human%20activity%20scheduling%20behaviour%20using%20a%20deep%20generative%20machine%0Alearning%20approach.%20Activity%20schedules%2C%20which%20represent%20the%20activities%20and%0Aassociated%20travel%20behaviours%20of%20individuals%2C%20are%20a%20core%20component%20of%20many%0Aapplied%20models%20in%20the%20transport%2C%20energy%20and%20epidemiology%20domains.%20Our%20data%0Adriven%20approach%20learns%20human%20preferences%20and%20scheduling%20logic%20without%20the%20need%0Afor%20complex%20interacting%20combinations%20of%20sub-models%20and%20custom-rules%2C%20this%20makes%0Aour%20approach%20significantly%20faster%20and%20simpler%20to%20operate%20that%20existing%0Aapproaches.%20We%20find%20activity%20schedule%20data%20combines%20aspects%20of%20both%20continuous%0Aimage%20data%20and%20also%20discrete%20text%20data%2C%20requiring%20novel%20approaches.%20We%0Aadditionally%20contribute%20a%20novel%20schedule%20representation%20and%20comprehensive%0Aevaluation%20framework%20for%20generated%20schedules.%20Evaluation%20shows%20our%20approach%20is%0Aable%20to%20rapidly%20generate%20large%2C%20diverse%20and%20realistic%20synthetic%20samples%20of%0Aactivity%20schedules.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10221v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModelling%2520Activity%2520Scheduling%2520Behaviour%2520with%2520Deep%2520Generative%2520Machine%250A%2520%2520Learning%26entry.906535625%3DFred%2520Shone%2520and%2520Tim%2520Hillel%26entry.1292438233%3D%2520%2520We%2520model%2520human%2520activity%2520scheduling%2520behaviour%2520using%2520a%2520deep%2520generative%2520machine%250Alearning%2520approach.%2520Activity%2520schedules%252C%2520which%2520represent%2520the%2520activities%2520and%250Aassociated%2520travel%2520behaviours%2520of%2520individuals%252C%2520are%2520a%2520core%2520component%2520of%2520many%250Aapplied%2520models%2520in%2520the%2520transport%252C%2520energy%2520and%2520epidemiology%2520domains.%2520Our%2520data%250Adriven%2520approach%2520learns%2520human%2520preferences%2520and%2520scheduling%2520logic%2520without%2520the%2520need%250Afor%2520complex%2520interacting%2520combinations%2520of%2520sub-models%2520and%2520custom-rules%252C%2520this%2520makes%250Aour%2520approach%2520significantly%2520faster%2520and%2520simpler%2520to%2520operate%2520that%2520existing%250Aapproaches.%2520We%2520find%2520activity%2520schedule%2520data%2520combines%2520aspects%2520of%2520both%2520continuous%250Aimage%2520data%2520and%2520also%2520discrete%2520text%2520data%252C%2520requiring%2520novel%2520approaches.%2520We%250Aadditionally%2520contribute%2520a%2520novel%2520schedule%2520representation%2520and%2520comprehensive%250Aevaluation%2520framework%2520for%2520generated%2520schedules.%2520Evaluation%2520shows%2520our%2520approach%2520is%250Aable%2520to%2520rapidly%2520generate%2520large%252C%2520diverse%2520and%2520realistic%2520synthetic%2520samples%2520of%250Aactivity%2520schedules.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10221v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modelling%20Activity%20Scheduling%20Behaviour%20with%20Deep%20Generative%20Machine%0A%20%20Learning&entry.906535625=Fred%20Shone%20and%20Tim%20Hillel&entry.1292438233=%20%20We%20model%20human%20activity%20scheduling%20behaviour%20using%20a%20deep%20generative%20machine%0Alearning%20approach.%20Activity%20schedules%2C%20which%20represent%20the%20activities%20and%0Aassociated%20travel%20behaviours%20of%20individuals%2C%20are%20a%20core%20component%20of%20many%0Aapplied%20models%20in%20the%20transport%2C%20energy%20and%20epidemiology%20domains.%20Our%20data%0Adriven%20approach%20learns%20human%20preferences%20and%20scheduling%20logic%20without%20the%20need%0Afor%20complex%20interacting%20combinations%20of%20sub-models%20and%20custom-rules%2C%20this%20makes%0Aour%20approach%20significantly%20faster%20and%20simpler%20to%20operate%20that%20existing%0Aapproaches.%20We%20find%20activity%20schedule%20data%20combines%20aspects%20of%20both%20continuous%0Aimage%20data%20and%20also%20discrete%20text%20data%2C%20requiring%20novel%20approaches.%20We%0Aadditionally%20contribute%20a%20novel%20schedule%20representation%20and%20comprehensive%0Aevaluation%20framework%20for%20generated%20schedules.%20Evaluation%20shows%20our%20approach%20is%0Aable%20to%20rapidly%20generate%20large%2C%20diverse%20and%20realistic%20synthetic%20samples%20of%0Aactivity%20schedules.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10221v1&entry.124074799=Read"},
{"title": "Continuous Urban Change Detection from Satellite Image Time Series with\n  Temporal Feature Refinement and Multi-Task Integration", "author": "Sebastian Hafner and Heng Fang and Hossein Azizpour and Yifang Ban", "abstract": "  Urbanization advances at unprecedented rates, resulting in negative effects\non the environment and human well-being. Remote sensing has the potential to\nmitigate these effects by supporting sustainable development strategies with\naccurate information on urban growth. Deep learning-based methods have achieved\npromising urban change detection results from optical satellite image pairs\nusing convolutional neural networks (ConvNets), transformers, and a multi-task\nlearning setup. However, transformers have not been leveraged for urban change\ndetection with multi-temporal data, i.e., >2 images, and multi-task learning\nmethods lack integration approaches that combine change and segmentation\noutputs. To fill this research gap, we propose a continuous urban change\ndetection method that identifies changes in each consecutive image pair of a\nsatellite image time series (SITS). Specifically, we propose a temporal feature\nrefinement (TFR) module that utilizes self-attention to improve ConvNet-based\nmulti-temporal building representations. Furthermore, we propose a multi-task\nintegration (MTI) module that utilizes Markov networks to find an optimal\nbuilding map time series based on segmentation and dense change outputs. The\nproposed method effectively identifies urban changes based on high-resolution\nSITS acquired by the PlanetScope constellation (F1 score 0.551) and Gaofen-2\n(F1 score 0.440). Moreover, our experiments on two challenging datasets\ndemonstrate the effectiveness of the proposed method compared to bi-temporal\nand multi-temporal urban change detection and segmentation methods.\n", "link": "http://arxiv.org/abs/2406.17458v2", "date": "2025-01-17", "relevancy": 2.1362, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5574}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5344}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5243}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continuous%20Urban%20Change%20Detection%20from%20Satellite%20Image%20Time%20Series%20with%0A%20%20Temporal%20Feature%20Refinement%20and%20Multi-Task%20Integration&body=Title%3A%20Continuous%20Urban%20Change%20Detection%20from%20Satellite%20Image%20Time%20Series%20with%0A%20%20Temporal%20Feature%20Refinement%20and%20Multi-Task%20Integration%0AAuthor%3A%20Sebastian%20Hafner%20and%20Heng%20Fang%20and%20Hossein%20Azizpour%20and%20Yifang%20Ban%0AAbstract%3A%20%20%20Urbanization%20advances%20at%20unprecedented%20rates%2C%20resulting%20in%20negative%20effects%0Aon%20the%20environment%20and%20human%20well-being.%20Remote%20sensing%20has%20the%20potential%20to%0Amitigate%20these%20effects%20by%20supporting%20sustainable%20development%20strategies%20with%0Aaccurate%20information%20on%20urban%20growth.%20Deep%20learning-based%20methods%20have%20achieved%0Apromising%20urban%20change%20detection%20results%20from%20optical%20satellite%20image%20pairs%0Ausing%20convolutional%20neural%20networks%20%28ConvNets%29%2C%20transformers%2C%20and%20a%20multi-task%0Alearning%20setup.%20However%2C%20transformers%20have%20not%20been%20leveraged%20for%20urban%20change%0Adetection%20with%20multi-temporal%20data%2C%20i.e.%2C%20%3E2%20images%2C%20and%20multi-task%20learning%0Amethods%20lack%20integration%20approaches%20that%20combine%20change%20and%20segmentation%0Aoutputs.%20To%20fill%20this%20research%20gap%2C%20we%20propose%20a%20continuous%20urban%20change%0Adetection%20method%20that%20identifies%20changes%20in%20each%20consecutive%20image%20pair%20of%20a%0Asatellite%20image%20time%20series%20%28SITS%29.%20Specifically%2C%20we%20propose%20a%20temporal%20feature%0Arefinement%20%28TFR%29%20module%20that%20utilizes%20self-attention%20to%20improve%20ConvNet-based%0Amulti-temporal%20building%20representations.%20Furthermore%2C%20we%20propose%20a%20multi-task%0Aintegration%20%28MTI%29%20module%20that%20utilizes%20Markov%20networks%20to%20find%20an%20optimal%0Abuilding%20map%20time%20series%20based%20on%20segmentation%20and%20dense%20change%20outputs.%20The%0Aproposed%20method%20effectively%20identifies%20urban%20changes%20based%20on%20high-resolution%0ASITS%20acquired%20by%20the%20PlanetScope%20constellation%20%28F1%20score%200.551%29%20and%20Gaofen-2%0A%28F1%20score%200.440%29.%20Moreover%2C%20our%20experiments%20on%20two%20challenging%20datasets%0Ademonstrate%20the%20effectiveness%20of%20the%20proposed%20method%20compared%20to%20bi-temporal%0Aand%20multi-temporal%20urban%20change%20detection%20and%20segmentation%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17458v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinuous%2520Urban%2520Change%2520Detection%2520from%2520Satellite%2520Image%2520Time%2520Series%2520with%250A%2520%2520Temporal%2520Feature%2520Refinement%2520and%2520Multi-Task%2520Integration%26entry.906535625%3DSebastian%2520Hafner%2520and%2520Heng%2520Fang%2520and%2520Hossein%2520Azizpour%2520and%2520Yifang%2520Ban%26entry.1292438233%3D%2520%2520Urbanization%2520advances%2520at%2520unprecedented%2520rates%252C%2520resulting%2520in%2520negative%2520effects%250Aon%2520the%2520environment%2520and%2520human%2520well-being.%2520Remote%2520sensing%2520has%2520the%2520potential%2520to%250Amitigate%2520these%2520effects%2520by%2520supporting%2520sustainable%2520development%2520strategies%2520with%250Aaccurate%2520information%2520on%2520urban%2520growth.%2520Deep%2520learning-based%2520methods%2520have%2520achieved%250Apromising%2520urban%2520change%2520detection%2520results%2520from%2520optical%2520satellite%2520image%2520pairs%250Ausing%2520convolutional%2520neural%2520networks%2520%2528ConvNets%2529%252C%2520transformers%252C%2520and%2520a%2520multi-task%250Alearning%2520setup.%2520However%252C%2520transformers%2520have%2520not%2520been%2520leveraged%2520for%2520urban%2520change%250Adetection%2520with%2520multi-temporal%2520data%252C%2520i.e.%252C%2520%253E2%2520images%252C%2520and%2520multi-task%2520learning%250Amethods%2520lack%2520integration%2520approaches%2520that%2520combine%2520change%2520and%2520segmentation%250Aoutputs.%2520To%2520fill%2520this%2520research%2520gap%252C%2520we%2520propose%2520a%2520continuous%2520urban%2520change%250Adetection%2520method%2520that%2520identifies%2520changes%2520in%2520each%2520consecutive%2520image%2520pair%2520of%2520a%250Asatellite%2520image%2520time%2520series%2520%2528SITS%2529.%2520Specifically%252C%2520we%2520propose%2520a%2520temporal%2520feature%250Arefinement%2520%2528TFR%2529%2520module%2520that%2520utilizes%2520self-attention%2520to%2520improve%2520ConvNet-based%250Amulti-temporal%2520building%2520representations.%2520Furthermore%252C%2520we%2520propose%2520a%2520multi-task%250Aintegration%2520%2528MTI%2529%2520module%2520that%2520utilizes%2520Markov%2520networks%2520to%2520find%2520an%2520optimal%250Abuilding%2520map%2520time%2520series%2520based%2520on%2520segmentation%2520and%2520dense%2520change%2520outputs.%2520The%250Aproposed%2520method%2520effectively%2520identifies%2520urban%2520changes%2520based%2520on%2520high-resolution%250ASITS%2520acquired%2520by%2520the%2520PlanetScope%2520constellation%2520%2528F1%2520score%25200.551%2529%2520and%2520Gaofen-2%250A%2528F1%2520score%25200.440%2529.%2520Moreover%252C%2520our%2520experiments%2520on%2520two%2520challenging%2520datasets%250Ademonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method%2520compared%2520to%2520bi-temporal%250Aand%2520multi-temporal%2520urban%2520change%2520detection%2520and%2520segmentation%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17458v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continuous%20Urban%20Change%20Detection%20from%20Satellite%20Image%20Time%20Series%20with%0A%20%20Temporal%20Feature%20Refinement%20and%20Multi-Task%20Integration&entry.906535625=Sebastian%20Hafner%20and%20Heng%20Fang%20and%20Hossein%20Azizpour%20and%20Yifang%20Ban&entry.1292438233=%20%20Urbanization%20advances%20at%20unprecedented%20rates%2C%20resulting%20in%20negative%20effects%0Aon%20the%20environment%20and%20human%20well-being.%20Remote%20sensing%20has%20the%20potential%20to%0Amitigate%20these%20effects%20by%20supporting%20sustainable%20development%20strategies%20with%0Aaccurate%20information%20on%20urban%20growth.%20Deep%20learning-based%20methods%20have%20achieved%0Apromising%20urban%20change%20detection%20results%20from%20optical%20satellite%20image%20pairs%0Ausing%20convolutional%20neural%20networks%20%28ConvNets%29%2C%20transformers%2C%20and%20a%20multi-task%0Alearning%20setup.%20However%2C%20transformers%20have%20not%20been%20leveraged%20for%20urban%20change%0Adetection%20with%20multi-temporal%20data%2C%20i.e.%2C%20%3E2%20images%2C%20and%20multi-task%20learning%0Amethods%20lack%20integration%20approaches%20that%20combine%20change%20and%20segmentation%0Aoutputs.%20To%20fill%20this%20research%20gap%2C%20we%20propose%20a%20continuous%20urban%20change%0Adetection%20method%20that%20identifies%20changes%20in%20each%20consecutive%20image%20pair%20of%20a%0Asatellite%20image%20time%20series%20%28SITS%29.%20Specifically%2C%20we%20propose%20a%20temporal%20feature%0Arefinement%20%28TFR%29%20module%20that%20utilizes%20self-attention%20to%20improve%20ConvNet-based%0Amulti-temporal%20building%20representations.%20Furthermore%2C%20we%20propose%20a%20multi-task%0Aintegration%20%28MTI%29%20module%20that%20utilizes%20Markov%20networks%20to%20find%20an%20optimal%0Abuilding%20map%20time%20series%20based%20on%20segmentation%20and%20dense%20change%20outputs.%20The%0Aproposed%20method%20effectively%20identifies%20urban%20changes%20based%20on%20high-resolution%0ASITS%20acquired%20by%20the%20PlanetScope%20constellation%20%28F1%20score%200.551%29%20and%20Gaofen-2%0A%28F1%20score%200.440%29.%20Moreover%2C%20our%20experiments%20on%20two%20challenging%20datasets%0Ademonstrate%20the%20effectiveness%20of%20the%20proposed%20method%20compared%20to%20bi-temporal%0Aand%20multi-temporal%20urban%20change%20detection%20and%20segmentation%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17458v2&entry.124074799=Read"},
{"title": "Agent4Edu: Generating Learner Response Data by Generative Agents for\n  Intelligent Education Systems", "author": "Weibo Gao and Qi Liu and Linan Yue and Fangzhou Yao and Rui Lv and Zheng Zhang and Hao Wang and Zhenya Huang", "abstract": "  Personalized learning represents a promising educational strategy within\nintelligent educational systems, aiming to enhance learners' practice\nefficiency. However, the discrepancy between offline metrics and online\nperformance significantly impedes their progress. To address this challenge, we\nintroduce Agent4Edu, a novel personalized learning simulator leveraging recent\nadvancements in human intelligence through large language models (LLMs).\nAgent4Edu features LLM-powered generative agents equipped with learner profile,\nmemory, and action modules tailored to personalized learning algorithms. The\nlearner profiles are initialized using real-world response data, capturing\npractice styles and cognitive factors. Inspired by human psychology theory, the\nmemory module records practice facts and high-level summaries, integrating\nreflection mechanisms. The action module supports various behaviors, including\nexercise understanding, analysis, and response generation. Each agent can\ninteract with personalized learning algorithms, such as computerized adaptive\ntesting, enabling a multifaceted evaluation and enhancement of customized\nservices. Through a comprehensive assessment, we explore the strengths and\nweaknesses of Agent4Edu, emphasizing the consistency and discrepancies in\nresponses between agents and human learners. The code, data, and appendix are\npublicly available at https://github.com/bigdata-ustc/Agent4Edu.\n", "link": "http://arxiv.org/abs/2501.10332v1", "date": "2025-01-17", "relevancy": 2.1329, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5494}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.529}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agent4Edu%3A%20Generating%20Learner%20Response%20Data%20by%20Generative%20Agents%20for%0A%20%20Intelligent%20Education%20Systems&body=Title%3A%20Agent4Edu%3A%20Generating%20Learner%20Response%20Data%20by%20Generative%20Agents%20for%0A%20%20Intelligent%20Education%20Systems%0AAuthor%3A%20Weibo%20Gao%20and%20Qi%20Liu%20and%20Linan%20Yue%20and%20Fangzhou%20Yao%20and%20Rui%20Lv%20and%20Zheng%20Zhang%20and%20Hao%20Wang%20and%20Zhenya%20Huang%0AAbstract%3A%20%20%20Personalized%20learning%20represents%20a%20promising%20educational%20strategy%20within%0Aintelligent%20educational%20systems%2C%20aiming%20to%20enhance%20learners%27%20practice%0Aefficiency.%20However%2C%20the%20discrepancy%20between%20offline%20metrics%20and%20online%0Aperformance%20significantly%20impedes%20their%20progress.%20To%20address%20this%20challenge%2C%20we%0Aintroduce%20Agent4Edu%2C%20a%20novel%20personalized%20learning%20simulator%20leveraging%20recent%0Aadvancements%20in%20human%20intelligence%20through%20large%20language%20models%20%28LLMs%29.%0AAgent4Edu%20features%20LLM-powered%20generative%20agents%20equipped%20with%20learner%20profile%2C%0Amemory%2C%20and%20action%20modules%20tailored%20to%20personalized%20learning%20algorithms.%20The%0Alearner%20profiles%20are%20initialized%20using%20real-world%20response%20data%2C%20capturing%0Apractice%20styles%20and%20cognitive%20factors.%20Inspired%20by%20human%20psychology%20theory%2C%20the%0Amemory%20module%20records%20practice%20facts%20and%20high-level%20summaries%2C%20integrating%0Areflection%20mechanisms.%20The%20action%20module%20supports%20various%20behaviors%2C%20including%0Aexercise%20understanding%2C%20analysis%2C%20and%20response%20generation.%20Each%20agent%20can%0Ainteract%20with%20personalized%20learning%20algorithms%2C%20such%20as%20computerized%20adaptive%0Atesting%2C%20enabling%20a%20multifaceted%20evaluation%20and%20enhancement%20of%20customized%0Aservices.%20Through%20a%20comprehensive%20assessment%2C%20we%20explore%20the%20strengths%20and%0Aweaknesses%20of%20Agent4Edu%2C%20emphasizing%20the%20consistency%20and%20discrepancies%20in%0Aresponses%20between%20agents%20and%20human%20learners.%20The%20code%2C%20data%2C%20and%20appendix%20are%0Apublicly%20available%20at%20https%3A//github.com/bigdata-ustc/Agent4Edu.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10332v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgent4Edu%253A%2520Generating%2520Learner%2520Response%2520Data%2520by%2520Generative%2520Agents%2520for%250A%2520%2520Intelligent%2520Education%2520Systems%26entry.906535625%3DWeibo%2520Gao%2520and%2520Qi%2520Liu%2520and%2520Linan%2520Yue%2520and%2520Fangzhou%2520Yao%2520and%2520Rui%2520Lv%2520and%2520Zheng%2520Zhang%2520and%2520Hao%2520Wang%2520and%2520Zhenya%2520Huang%26entry.1292438233%3D%2520%2520Personalized%2520learning%2520represents%2520a%2520promising%2520educational%2520strategy%2520within%250Aintelligent%2520educational%2520systems%252C%2520aiming%2520to%2520enhance%2520learners%2527%2520practice%250Aefficiency.%2520However%252C%2520the%2520discrepancy%2520between%2520offline%2520metrics%2520and%2520online%250Aperformance%2520significantly%2520impedes%2520their%2520progress.%2520To%2520address%2520this%2520challenge%252C%2520we%250Aintroduce%2520Agent4Edu%252C%2520a%2520novel%2520personalized%2520learning%2520simulator%2520leveraging%2520recent%250Aadvancements%2520in%2520human%2520intelligence%2520through%2520large%2520language%2520models%2520%2528LLMs%2529.%250AAgent4Edu%2520features%2520LLM-powered%2520generative%2520agents%2520equipped%2520with%2520learner%2520profile%252C%250Amemory%252C%2520and%2520action%2520modules%2520tailored%2520to%2520personalized%2520learning%2520algorithms.%2520The%250Alearner%2520profiles%2520are%2520initialized%2520using%2520real-world%2520response%2520data%252C%2520capturing%250Apractice%2520styles%2520and%2520cognitive%2520factors.%2520Inspired%2520by%2520human%2520psychology%2520theory%252C%2520the%250Amemory%2520module%2520records%2520practice%2520facts%2520and%2520high-level%2520summaries%252C%2520integrating%250Areflection%2520mechanisms.%2520The%2520action%2520module%2520supports%2520various%2520behaviors%252C%2520including%250Aexercise%2520understanding%252C%2520analysis%252C%2520and%2520response%2520generation.%2520Each%2520agent%2520can%250Ainteract%2520with%2520personalized%2520learning%2520algorithms%252C%2520such%2520as%2520computerized%2520adaptive%250Atesting%252C%2520enabling%2520a%2520multifaceted%2520evaluation%2520and%2520enhancement%2520of%2520customized%250Aservices.%2520Through%2520a%2520comprehensive%2520assessment%252C%2520we%2520explore%2520the%2520strengths%2520and%250Aweaknesses%2520of%2520Agent4Edu%252C%2520emphasizing%2520the%2520consistency%2520and%2520discrepancies%2520in%250Aresponses%2520between%2520agents%2520and%2520human%2520learners.%2520The%2520code%252C%2520data%252C%2520and%2520appendix%2520are%250Apublicly%2520available%2520at%2520https%253A//github.com/bigdata-ustc/Agent4Edu.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10332v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agent4Edu%3A%20Generating%20Learner%20Response%20Data%20by%20Generative%20Agents%20for%0A%20%20Intelligent%20Education%20Systems&entry.906535625=Weibo%20Gao%20and%20Qi%20Liu%20and%20Linan%20Yue%20and%20Fangzhou%20Yao%20and%20Rui%20Lv%20and%20Zheng%20Zhang%20and%20Hao%20Wang%20and%20Zhenya%20Huang&entry.1292438233=%20%20Personalized%20learning%20represents%20a%20promising%20educational%20strategy%20within%0Aintelligent%20educational%20systems%2C%20aiming%20to%20enhance%20learners%27%20practice%0Aefficiency.%20However%2C%20the%20discrepancy%20between%20offline%20metrics%20and%20online%0Aperformance%20significantly%20impedes%20their%20progress.%20To%20address%20this%20challenge%2C%20we%0Aintroduce%20Agent4Edu%2C%20a%20novel%20personalized%20learning%20simulator%20leveraging%20recent%0Aadvancements%20in%20human%20intelligence%20through%20large%20language%20models%20%28LLMs%29.%0AAgent4Edu%20features%20LLM-powered%20generative%20agents%20equipped%20with%20learner%20profile%2C%0Amemory%2C%20and%20action%20modules%20tailored%20to%20personalized%20learning%20algorithms.%20The%0Alearner%20profiles%20are%20initialized%20using%20real-world%20response%20data%2C%20capturing%0Apractice%20styles%20and%20cognitive%20factors.%20Inspired%20by%20human%20psychology%20theory%2C%20the%0Amemory%20module%20records%20practice%20facts%20and%20high-level%20summaries%2C%20integrating%0Areflection%20mechanisms.%20The%20action%20module%20supports%20various%20behaviors%2C%20including%0Aexercise%20understanding%2C%20analysis%2C%20and%20response%20generation.%20Each%20agent%20can%0Ainteract%20with%20personalized%20learning%20algorithms%2C%20such%20as%20computerized%20adaptive%0Atesting%2C%20enabling%20a%20multifaceted%20evaluation%20and%20enhancement%20of%20customized%0Aservices.%20Through%20a%20comprehensive%20assessment%2C%20we%20explore%20the%20strengths%20and%0Aweaknesses%20of%20Agent4Edu%2C%20emphasizing%20the%20consistency%20and%20discrepancies%20in%0Aresponses%20between%20agents%20and%20human%20learners.%20The%20code%2C%20data%2C%20and%20appendix%20are%0Apublicly%20available%20at%20https%3A//github.com/bigdata-ustc/Agent4Edu.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10332v1&entry.124074799=Read"},
{"title": "Towards Large Reasoning Models: A Survey on Scaling LLM Reasoning\n  Capabilities", "author": "Fengli Xu and Qianyue Hao and Zefang Zong and Jingwei Wang and Yunke Zhang and Jingyi Wang and Xiaochong Lan and Jiahui Gong and Tianjian Ouyang and Fanjin Meng and Chenyang Shao and Yuwei Yan and Qinglong Yang and Yiwen Song and Sijian Ren and Xinyuan Hu and Yu Li and Jie Feng and Chen Gao and Yong Li", "abstract": "  Language has long been conceived as an essential tool for human reasoning.\nThe breakthrough of Large Language Models (LLMs) has sparked significant\nresearch interest in leveraging these models to tackle complex reasoning tasks.\nResearchers have moved beyond simple autoregressive token generation by\nintroducing the concept of \"thought\" -- a sequence of tokens representing\nintermediate steps in the reasoning process. This innovative paradigm enables\nLLMs' to mimic complex human reasoning processes, such as tree search and\nreflective thinking. Recently, an emerging trend of learning to reason has\napplied reinforcement learning (RL) to train LLMs to master reasoning\nprocesses. This approach enables the automatic generation of high-quality\nreasoning trajectories through trial-and-error search algorithms, significantly\nexpanding LLMs' reasoning capacity by providing substantially more training\ndata. Furthermore, recent studies demonstrate that encouraging LLMs to \"think\"\nwith more tokens during test-time inference can further significantly boost\nreasoning accuracy. Therefore, the train-time and test-time scaling combined to\nshow a new research frontier -- a path toward Large Reasoning Model. The\nintroduction of OpenAI's o1 series marks a significant milestone in this\nresearch direction. In this survey, we present a comprehensive review of recent\nprogress in LLM reasoning. We begin by introducing the foundational background\nof LLMs and then explore the key technical components driving the development\nof large reasoning models, with a focus on automated data construction,\nlearning-to-reason techniques, and test-time scaling. We also analyze popular\nopen-source projects at building large reasoning models, and conclude with open\nchallenges and future research directions.\n", "link": "http://arxiv.org/abs/2501.09686v2", "date": "2025-01-17", "relevancy": 2.1257, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5426}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5426}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Large%20Reasoning%20Models%3A%20A%20Survey%20on%20Scaling%20LLM%20Reasoning%0A%20%20Capabilities&body=Title%3A%20Towards%20Large%20Reasoning%20Models%3A%20A%20Survey%20on%20Scaling%20LLM%20Reasoning%0A%20%20Capabilities%0AAuthor%3A%20Fengli%20Xu%20and%20Qianyue%20Hao%20and%20Zefang%20Zong%20and%20Jingwei%20Wang%20and%20Yunke%20Zhang%20and%20Jingyi%20Wang%20and%20Xiaochong%20Lan%20and%20Jiahui%20Gong%20and%20Tianjian%20Ouyang%20and%20Fanjin%20Meng%20and%20Chenyang%20Shao%20and%20Yuwei%20Yan%20and%20Qinglong%20Yang%20and%20Yiwen%20Song%20and%20Sijian%20Ren%20and%20Xinyuan%20Hu%20and%20Yu%20Li%20and%20Jie%20Feng%20and%20Chen%20Gao%20and%20Yong%20Li%0AAbstract%3A%20%20%20Language%20has%20long%20been%20conceived%20as%20an%20essential%20tool%20for%20human%20reasoning.%0AThe%20breakthrough%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20sparked%20significant%0Aresearch%20interest%20in%20leveraging%20these%20models%20to%20tackle%20complex%20reasoning%20tasks.%0AResearchers%20have%20moved%20beyond%20simple%20autoregressive%20token%20generation%20by%0Aintroducing%20the%20concept%20of%20%22thought%22%20--%20a%20sequence%20of%20tokens%20representing%0Aintermediate%20steps%20in%20the%20reasoning%20process.%20This%20innovative%20paradigm%20enables%0ALLMs%27%20to%20mimic%20complex%20human%20reasoning%20processes%2C%20such%20as%20tree%20search%20and%0Areflective%20thinking.%20Recently%2C%20an%20emerging%20trend%20of%20learning%20to%20reason%20has%0Aapplied%20reinforcement%20learning%20%28RL%29%20to%20train%20LLMs%20to%20master%20reasoning%0Aprocesses.%20This%20approach%20enables%20the%20automatic%20generation%20of%20high-quality%0Areasoning%20trajectories%20through%20trial-and-error%20search%20algorithms%2C%20significantly%0Aexpanding%20LLMs%27%20reasoning%20capacity%20by%20providing%20substantially%20more%20training%0Adata.%20Furthermore%2C%20recent%20studies%20demonstrate%20that%20encouraging%20LLMs%20to%20%22think%22%0Awith%20more%20tokens%20during%20test-time%20inference%20can%20further%20significantly%20boost%0Areasoning%20accuracy.%20Therefore%2C%20the%20train-time%20and%20test-time%20scaling%20combined%20to%0Ashow%20a%20new%20research%20frontier%20--%20a%20path%20toward%20Large%20Reasoning%20Model.%20The%0Aintroduction%20of%20OpenAI%27s%20o1%20series%20marks%20a%20significant%20milestone%20in%20this%0Aresearch%20direction.%20In%20this%20survey%2C%20we%20present%20a%20comprehensive%20review%20of%20recent%0Aprogress%20in%20LLM%20reasoning.%20We%20begin%20by%20introducing%20the%20foundational%20background%0Aof%20LLMs%20and%20then%20explore%20the%20key%20technical%20components%20driving%20the%20development%0Aof%20large%20reasoning%20models%2C%20with%20a%20focus%20on%20automated%20data%20construction%2C%0Alearning-to-reason%20techniques%2C%20and%20test-time%20scaling.%20We%20also%20analyze%20popular%0Aopen-source%20projects%20at%20building%20large%20reasoning%20models%2C%20and%20conclude%20with%20open%0Achallenges%20and%20future%20research%20directions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09686v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Large%2520Reasoning%2520Models%253A%2520A%2520Survey%2520on%2520Scaling%2520LLM%2520Reasoning%250A%2520%2520Capabilities%26entry.906535625%3DFengli%2520Xu%2520and%2520Qianyue%2520Hao%2520and%2520Zefang%2520Zong%2520and%2520Jingwei%2520Wang%2520and%2520Yunke%2520Zhang%2520and%2520Jingyi%2520Wang%2520and%2520Xiaochong%2520Lan%2520and%2520Jiahui%2520Gong%2520and%2520Tianjian%2520Ouyang%2520and%2520Fanjin%2520Meng%2520and%2520Chenyang%2520Shao%2520and%2520Yuwei%2520Yan%2520and%2520Qinglong%2520Yang%2520and%2520Yiwen%2520Song%2520and%2520Sijian%2520Ren%2520and%2520Xinyuan%2520Hu%2520and%2520Yu%2520Li%2520and%2520Jie%2520Feng%2520and%2520Chen%2520Gao%2520and%2520Yong%2520Li%26entry.1292438233%3D%2520%2520Language%2520has%2520long%2520been%2520conceived%2520as%2520an%2520essential%2520tool%2520for%2520human%2520reasoning.%250AThe%2520breakthrough%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520sparked%2520significant%250Aresearch%2520interest%2520in%2520leveraging%2520these%2520models%2520to%2520tackle%2520complex%2520reasoning%2520tasks.%250AResearchers%2520have%2520moved%2520beyond%2520simple%2520autoregressive%2520token%2520generation%2520by%250Aintroducing%2520the%2520concept%2520of%2520%2522thought%2522%2520--%2520a%2520sequence%2520of%2520tokens%2520representing%250Aintermediate%2520steps%2520in%2520the%2520reasoning%2520process.%2520This%2520innovative%2520paradigm%2520enables%250ALLMs%2527%2520to%2520mimic%2520complex%2520human%2520reasoning%2520processes%252C%2520such%2520as%2520tree%2520search%2520and%250Areflective%2520thinking.%2520Recently%252C%2520an%2520emerging%2520trend%2520of%2520learning%2520to%2520reason%2520has%250Aapplied%2520reinforcement%2520learning%2520%2528RL%2529%2520to%2520train%2520LLMs%2520to%2520master%2520reasoning%250Aprocesses.%2520This%2520approach%2520enables%2520the%2520automatic%2520generation%2520of%2520high-quality%250Areasoning%2520trajectories%2520through%2520trial-and-error%2520search%2520algorithms%252C%2520significantly%250Aexpanding%2520LLMs%2527%2520reasoning%2520capacity%2520by%2520providing%2520substantially%2520more%2520training%250Adata.%2520Furthermore%252C%2520recent%2520studies%2520demonstrate%2520that%2520encouraging%2520LLMs%2520to%2520%2522think%2522%250Awith%2520more%2520tokens%2520during%2520test-time%2520inference%2520can%2520further%2520significantly%2520boost%250Areasoning%2520accuracy.%2520Therefore%252C%2520the%2520train-time%2520and%2520test-time%2520scaling%2520combined%2520to%250Ashow%2520a%2520new%2520research%2520frontier%2520--%2520a%2520path%2520toward%2520Large%2520Reasoning%2520Model.%2520The%250Aintroduction%2520of%2520OpenAI%2527s%2520o1%2520series%2520marks%2520a%2520significant%2520milestone%2520in%2520this%250Aresearch%2520direction.%2520In%2520this%2520survey%252C%2520we%2520present%2520a%2520comprehensive%2520review%2520of%2520recent%250Aprogress%2520in%2520LLM%2520reasoning.%2520We%2520begin%2520by%2520introducing%2520the%2520foundational%2520background%250Aof%2520LLMs%2520and%2520then%2520explore%2520the%2520key%2520technical%2520components%2520driving%2520the%2520development%250Aof%2520large%2520reasoning%2520models%252C%2520with%2520a%2520focus%2520on%2520automated%2520data%2520construction%252C%250Alearning-to-reason%2520techniques%252C%2520and%2520test-time%2520scaling.%2520We%2520also%2520analyze%2520popular%250Aopen-source%2520projects%2520at%2520building%2520large%2520reasoning%2520models%252C%2520and%2520conclude%2520with%2520open%250Achallenges%2520and%2520future%2520research%2520directions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09686v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Large%20Reasoning%20Models%3A%20A%20Survey%20on%20Scaling%20LLM%20Reasoning%0A%20%20Capabilities&entry.906535625=Fengli%20Xu%20and%20Qianyue%20Hao%20and%20Zefang%20Zong%20and%20Jingwei%20Wang%20and%20Yunke%20Zhang%20and%20Jingyi%20Wang%20and%20Xiaochong%20Lan%20and%20Jiahui%20Gong%20and%20Tianjian%20Ouyang%20and%20Fanjin%20Meng%20and%20Chenyang%20Shao%20and%20Yuwei%20Yan%20and%20Qinglong%20Yang%20and%20Yiwen%20Song%20and%20Sijian%20Ren%20and%20Xinyuan%20Hu%20and%20Yu%20Li%20and%20Jie%20Feng%20and%20Chen%20Gao%20and%20Yong%20Li&entry.1292438233=%20%20Language%20has%20long%20been%20conceived%20as%20an%20essential%20tool%20for%20human%20reasoning.%0AThe%20breakthrough%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20sparked%20significant%0Aresearch%20interest%20in%20leveraging%20these%20models%20to%20tackle%20complex%20reasoning%20tasks.%0AResearchers%20have%20moved%20beyond%20simple%20autoregressive%20token%20generation%20by%0Aintroducing%20the%20concept%20of%20%22thought%22%20--%20a%20sequence%20of%20tokens%20representing%0Aintermediate%20steps%20in%20the%20reasoning%20process.%20This%20innovative%20paradigm%20enables%0ALLMs%27%20to%20mimic%20complex%20human%20reasoning%20processes%2C%20such%20as%20tree%20search%20and%0Areflective%20thinking.%20Recently%2C%20an%20emerging%20trend%20of%20learning%20to%20reason%20has%0Aapplied%20reinforcement%20learning%20%28RL%29%20to%20train%20LLMs%20to%20master%20reasoning%0Aprocesses.%20This%20approach%20enables%20the%20automatic%20generation%20of%20high-quality%0Areasoning%20trajectories%20through%20trial-and-error%20search%20algorithms%2C%20significantly%0Aexpanding%20LLMs%27%20reasoning%20capacity%20by%20providing%20substantially%20more%20training%0Adata.%20Furthermore%2C%20recent%20studies%20demonstrate%20that%20encouraging%20LLMs%20to%20%22think%22%0Awith%20more%20tokens%20during%20test-time%20inference%20can%20further%20significantly%20boost%0Areasoning%20accuracy.%20Therefore%2C%20the%20train-time%20and%20test-time%20scaling%20combined%20to%0Ashow%20a%20new%20research%20frontier%20--%20a%20path%20toward%20Large%20Reasoning%20Model.%20The%0Aintroduction%20of%20OpenAI%27s%20o1%20series%20marks%20a%20significant%20milestone%20in%20this%0Aresearch%20direction.%20In%20this%20survey%2C%20we%20present%20a%20comprehensive%20review%20of%20recent%0Aprogress%20in%20LLM%20reasoning.%20We%20begin%20by%20introducing%20the%20foundational%20background%0Aof%20LLMs%20and%20then%20explore%20the%20key%20technical%20components%20driving%20the%20development%0Aof%20large%20reasoning%20models%2C%20with%20a%20focus%20on%20automated%20data%20construction%2C%0Alearning-to-reason%20techniques%2C%20and%20test-time%20scaling.%20We%20also%20analyze%20popular%0Aopen-source%20projects%20at%20building%20large%20reasoning%20models%2C%20and%20conclude%20with%20open%0Achallenges%20and%20future%20research%20directions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09686v2&entry.124074799=Read"},
{"title": "Hierarchical Autoregressive Transformers: Combining Byte-~and Word-Level\n  Processing for Robust, Adaptable Language Models", "author": "Pit Neitemeier and Bj\u00f6rn Deiseroth and Constantin Eichenberg and Lukas Balles", "abstract": "  Tokenization is a fundamental step in natural language processing, breaking\ntext into units that computational models can process. While learned subword\ntokenizers have become the de-facto standard, they present challenges such as\nlarge vocabularies, limited adaptability to new domains or languages, and\nsensitivity to spelling errors and variations. To overcome these limitations,\nwe investigate a hierarchical architecture for autoregressive language\nmodelling that combines character-level and word-level processing. It employs a\nlightweight character-level encoder to convert character sequences into word\nembeddings, which are then processed by a word-level backbone model and decoded\nback into characters via a compact character-level decoder. This method retains\nthe sequence compression benefits of word-level tokenization without relying on\na rigid, predefined vocabulary. We demonstrate, at scales up to 7 billion\nparameters, that hierarchical transformers match the downstream task\nperformance of subword-tokenizer-based models while exhibiting significantly\ngreater robustness to input perturbations. Additionally, during continued\npretraining on an out-of-domain language, our model trains almost twice as\nfast, achieves superior performance on the target language, and retains more of\nits previously learned knowledge. Hierarchical transformers pave the way for\nNLP systems that are more robust, flexible, and generalizable across languages\nand domains.\n", "link": "http://arxiv.org/abs/2501.10322v1", "date": "2025-01-17", "relevancy": 2.1232, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5864}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5283}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Autoregressive%20Transformers%3A%20Combining%20Byte-~and%20Word-Level%0A%20%20Processing%20for%20Robust%2C%20Adaptable%20Language%20Models&body=Title%3A%20Hierarchical%20Autoregressive%20Transformers%3A%20Combining%20Byte-~and%20Word-Level%0A%20%20Processing%20for%20Robust%2C%20Adaptable%20Language%20Models%0AAuthor%3A%20Pit%20Neitemeier%20and%20Bj%C3%B6rn%20Deiseroth%20and%20Constantin%20Eichenberg%20and%20Lukas%20Balles%0AAbstract%3A%20%20%20Tokenization%20is%20a%20fundamental%20step%20in%20natural%20language%20processing%2C%20breaking%0Atext%20into%20units%20that%20computational%20models%20can%20process.%20While%20learned%20subword%0Atokenizers%20have%20become%20the%20de-facto%20standard%2C%20they%20present%20challenges%20such%20as%0Alarge%20vocabularies%2C%20limited%20adaptability%20to%20new%20domains%20or%20languages%2C%20and%0Asensitivity%20to%20spelling%20errors%20and%20variations.%20To%20overcome%20these%20limitations%2C%0Awe%20investigate%20a%20hierarchical%20architecture%20for%20autoregressive%20language%0Amodelling%20that%20combines%20character-level%20and%20word-level%20processing.%20It%20employs%20a%0Alightweight%20character-level%20encoder%20to%20convert%20character%20sequences%20into%20word%0Aembeddings%2C%20which%20are%20then%20processed%20by%20a%20word-level%20backbone%20model%20and%20decoded%0Aback%20into%20characters%20via%20a%20compact%20character-level%20decoder.%20This%20method%20retains%0Athe%20sequence%20compression%20benefits%20of%20word-level%20tokenization%20without%20relying%20on%0Aa%20rigid%2C%20predefined%20vocabulary.%20We%20demonstrate%2C%20at%20scales%20up%20to%207%20billion%0Aparameters%2C%20that%20hierarchical%20transformers%20match%20the%20downstream%20task%0Aperformance%20of%20subword-tokenizer-based%20models%20while%20exhibiting%20significantly%0Agreater%20robustness%20to%20input%20perturbations.%20Additionally%2C%20during%20continued%0Apretraining%20on%20an%20out-of-domain%20language%2C%20our%20model%20trains%20almost%20twice%20as%0Afast%2C%20achieves%20superior%20performance%20on%20the%20target%20language%2C%20and%20retains%20more%20of%0Aits%20previously%20learned%20knowledge.%20Hierarchical%20transformers%20pave%20the%20way%20for%0ANLP%20systems%20that%20are%20more%20robust%2C%20flexible%2C%20and%20generalizable%20across%20languages%0Aand%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10322v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Autoregressive%2520Transformers%253A%2520Combining%2520Byte-~and%2520Word-Level%250A%2520%2520Processing%2520for%2520Robust%252C%2520Adaptable%2520Language%2520Models%26entry.906535625%3DPit%2520Neitemeier%2520and%2520Bj%25C3%25B6rn%2520Deiseroth%2520and%2520Constantin%2520Eichenberg%2520and%2520Lukas%2520Balles%26entry.1292438233%3D%2520%2520Tokenization%2520is%2520a%2520fundamental%2520step%2520in%2520natural%2520language%2520processing%252C%2520breaking%250Atext%2520into%2520units%2520that%2520computational%2520models%2520can%2520process.%2520While%2520learned%2520subword%250Atokenizers%2520have%2520become%2520the%2520de-facto%2520standard%252C%2520they%2520present%2520challenges%2520such%2520as%250Alarge%2520vocabularies%252C%2520limited%2520adaptability%2520to%2520new%2520domains%2520or%2520languages%252C%2520and%250Asensitivity%2520to%2520spelling%2520errors%2520and%2520variations.%2520To%2520overcome%2520these%2520limitations%252C%250Awe%2520investigate%2520a%2520hierarchical%2520architecture%2520for%2520autoregressive%2520language%250Amodelling%2520that%2520combines%2520character-level%2520and%2520word-level%2520processing.%2520It%2520employs%2520a%250Alightweight%2520character-level%2520encoder%2520to%2520convert%2520character%2520sequences%2520into%2520word%250Aembeddings%252C%2520which%2520are%2520then%2520processed%2520by%2520a%2520word-level%2520backbone%2520model%2520and%2520decoded%250Aback%2520into%2520characters%2520via%2520a%2520compact%2520character-level%2520decoder.%2520This%2520method%2520retains%250Athe%2520sequence%2520compression%2520benefits%2520of%2520word-level%2520tokenization%2520without%2520relying%2520on%250Aa%2520rigid%252C%2520predefined%2520vocabulary.%2520We%2520demonstrate%252C%2520at%2520scales%2520up%2520to%25207%2520billion%250Aparameters%252C%2520that%2520hierarchical%2520transformers%2520match%2520the%2520downstream%2520task%250Aperformance%2520of%2520subword-tokenizer-based%2520models%2520while%2520exhibiting%2520significantly%250Agreater%2520robustness%2520to%2520input%2520perturbations.%2520Additionally%252C%2520during%2520continued%250Apretraining%2520on%2520an%2520out-of-domain%2520language%252C%2520our%2520model%2520trains%2520almost%2520twice%2520as%250Afast%252C%2520achieves%2520superior%2520performance%2520on%2520the%2520target%2520language%252C%2520and%2520retains%2520more%2520of%250Aits%2520previously%2520learned%2520knowledge.%2520Hierarchical%2520transformers%2520pave%2520the%2520way%2520for%250ANLP%2520systems%2520that%2520are%2520more%2520robust%252C%2520flexible%252C%2520and%2520generalizable%2520across%2520languages%250Aand%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10322v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Autoregressive%20Transformers%3A%20Combining%20Byte-~and%20Word-Level%0A%20%20Processing%20for%20Robust%2C%20Adaptable%20Language%20Models&entry.906535625=Pit%20Neitemeier%20and%20Bj%C3%B6rn%20Deiseroth%20and%20Constantin%20Eichenberg%20and%20Lukas%20Balles&entry.1292438233=%20%20Tokenization%20is%20a%20fundamental%20step%20in%20natural%20language%20processing%2C%20breaking%0Atext%20into%20units%20that%20computational%20models%20can%20process.%20While%20learned%20subword%0Atokenizers%20have%20become%20the%20de-facto%20standard%2C%20they%20present%20challenges%20such%20as%0Alarge%20vocabularies%2C%20limited%20adaptability%20to%20new%20domains%20or%20languages%2C%20and%0Asensitivity%20to%20spelling%20errors%20and%20variations.%20To%20overcome%20these%20limitations%2C%0Awe%20investigate%20a%20hierarchical%20architecture%20for%20autoregressive%20language%0Amodelling%20that%20combines%20character-level%20and%20word-level%20processing.%20It%20employs%20a%0Alightweight%20character-level%20encoder%20to%20convert%20character%20sequences%20into%20word%0Aembeddings%2C%20which%20are%20then%20processed%20by%20a%20word-level%20backbone%20model%20and%20decoded%0Aback%20into%20characters%20via%20a%20compact%20character-level%20decoder.%20This%20method%20retains%0Athe%20sequence%20compression%20benefits%20of%20word-level%20tokenization%20without%20relying%20on%0Aa%20rigid%2C%20predefined%20vocabulary.%20We%20demonstrate%2C%20at%20scales%20up%20to%207%20billion%0Aparameters%2C%20that%20hierarchical%20transformers%20match%20the%20downstream%20task%0Aperformance%20of%20subword-tokenizer-based%20models%20while%20exhibiting%20significantly%0Agreater%20robustness%20to%20input%20perturbations.%20Additionally%2C%20during%20continued%0Apretraining%20on%20an%20out-of-domain%20language%2C%20our%20model%20trains%20almost%20twice%20as%0Afast%2C%20achieves%20superior%20performance%20on%20the%20target%20language%2C%20and%20retains%20more%20of%0Aits%20previously%20learned%20knowledge.%20Hierarchical%20transformers%20pave%20the%20way%20for%0ANLP%20systems%20that%20are%20more%20robust%2C%20flexible%2C%20and%20generalizable%20across%20languages%0Aand%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10322v1&entry.124074799=Read"},
{"title": "VLSBench: Unveiling Visual Leakage in Multimodal Safety", "author": "Xuhao Hu and Dongrui Liu and Hao Li and Xuanjing Huang and Jing Shao", "abstract": "  Safety concerns of Multimodal large language models (MLLMs) have gradually\nbecome an important problem in various applications. Surprisingly, previous\nworks indicate a counter-intuitive phenomenon that using textual unlearning to\nalign MLLMs achieves comparable safety performances with MLLMs trained with\nimage-text pairs. To explain such a counter-intuitive phenomenon, we discover a\nvisual safety information leakage (VSIL) problem in existing multimodal safety\nbenchmarks, i.e., the potentially risky and sensitive content in the image has\nbeen revealed in the textual query. In this way, MLLMs can easily refuse these\nsensitive text-image queries according to textual queries. However, image-text\npairs without VSIL are common in real-world scenarios and are overlooked by\nexisting multimodal safety benchmarks. To this end, we construct multimodal\nvisual leakless safety benchmark (VLSBench) preventing visual safety leakage\nfrom image to textual query with 2.4k image-text pairs. Experimental results\nindicate that VLSBench poses a significant challenge to both open-source and\nclose-source MLLMs, including LLaVA, Qwen2-VL, Llama3.2-Vision, and GPT-4o.\nThis study demonstrates that textual alignment is enough for multimodal safety\nscenarios with VSIL, while multimodal alignment is a more promising solution\nfor multimodal safety scenarios without VSIL. Please see our code and data at:\nhttps://hxhcreate.github.io/vlsbench.github.io/\n", "link": "http://arxiv.org/abs/2411.19939v2", "date": "2025-01-17", "relevancy": 2.1193, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5385}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5315}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLSBench%3A%20Unveiling%20Visual%20Leakage%20in%20Multimodal%20Safety&body=Title%3A%20VLSBench%3A%20Unveiling%20Visual%20Leakage%20in%20Multimodal%20Safety%0AAuthor%3A%20Xuhao%20Hu%20and%20Dongrui%20Liu%20and%20Hao%20Li%20and%20Xuanjing%20Huang%20and%20Jing%20Shao%0AAbstract%3A%20%20%20Safety%20concerns%20of%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20gradually%0Abecome%20an%20important%20problem%20in%20various%20applications.%20Surprisingly%2C%20previous%0Aworks%20indicate%20a%20counter-intuitive%20phenomenon%20that%20using%20textual%20unlearning%20to%0Aalign%20MLLMs%20achieves%20comparable%20safety%20performances%20with%20MLLMs%20trained%20with%0Aimage-text%20pairs.%20To%20explain%20such%20a%20counter-intuitive%20phenomenon%2C%20we%20discover%20a%0Avisual%20safety%20information%20leakage%20%28VSIL%29%20problem%20in%20existing%20multimodal%20safety%0Abenchmarks%2C%20i.e.%2C%20the%20potentially%20risky%20and%20sensitive%20content%20in%20the%20image%20has%0Abeen%20revealed%20in%20the%20textual%20query.%20In%20this%20way%2C%20MLLMs%20can%20easily%20refuse%20these%0Asensitive%20text-image%20queries%20according%20to%20textual%20queries.%20However%2C%20image-text%0Apairs%20without%20VSIL%20are%20common%20in%20real-world%20scenarios%20and%20are%20overlooked%20by%0Aexisting%20multimodal%20safety%20benchmarks.%20To%20this%20end%2C%20we%20construct%20multimodal%0Avisual%20leakless%20safety%20benchmark%20%28VLSBench%29%20preventing%20visual%20safety%20leakage%0Afrom%20image%20to%20textual%20query%20with%202.4k%20image-text%20pairs.%20Experimental%20results%0Aindicate%20that%20VLSBench%20poses%20a%20significant%20challenge%20to%20both%20open-source%20and%0Aclose-source%20MLLMs%2C%20including%20LLaVA%2C%20Qwen2-VL%2C%20Llama3.2-Vision%2C%20and%20GPT-4o.%0AThis%20study%20demonstrates%20that%20textual%20alignment%20is%20enough%20for%20multimodal%20safety%0Ascenarios%20with%20VSIL%2C%20while%20multimodal%20alignment%20is%20a%20more%20promising%20solution%0Afor%20multimodal%20safety%20scenarios%20without%20VSIL.%20Please%20see%20our%20code%20and%20data%20at%3A%0Ahttps%3A//hxhcreate.github.io/vlsbench.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19939v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLSBench%253A%2520Unveiling%2520Visual%2520Leakage%2520in%2520Multimodal%2520Safety%26entry.906535625%3DXuhao%2520Hu%2520and%2520Dongrui%2520Liu%2520and%2520Hao%2520Li%2520and%2520Xuanjing%2520Huang%2520and%2520Jing%2520Shao%26entry.1292438233%3D%2520%2520Safety%2520concerns%2520of%2520Multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520gradually%250Abecome%2520an%2520important%2520problem%2520in%2520various%2520applications.%2520Surprisingly%252C%2520previous%250Aworks%2520indicate%2520a%2520counter-intuitive%2520phenomenon%2520that%2520using%2520textual%2520unlearning%2520to%250Aalign%2520MLLMs%2520achieves%2520comparable%2520safety%2520performances%2520with%2520MLLMs%2520trained%2520with%250Aimage-text%2520pairs.%2520To%2520explain%2520such%2520a%2520counter-intuitive%2520phenomenon%252C%2520we%2520discover%2520a%250Avisual%2520safety%2520information%2520leakage%2520%2528VSIL%2529%2520problem%2520in%2520existing%2520multimodal%2520safety%250Abenchmarks%252C%2520i.e.%252C%2520the%2520potentially%2520risky%2520and%2520sensitive%2520content%2520in%2520the%2520image%2520has%250Abeen%2520revealed%2520in%2520the%2520textual%2520query.%2520In%2520this%2520way%252C%2520MLLMs%2520can%2520easily%2520refuse%2520these%250Asensitive%2520text-image%2520queries%2520according%2520to%2520textual%2520queries.%2520However%252C%2520image-text%250Apairs%2520without%2520VSIL%2520are%2520common%2520in%2520real-world%2520scenarios%2520and%2520are%2520overlooked%2520by%250Aexisting%2520multimodal%2520safety%2520benchmarks.%2520To%2520this%2520end%252C%2520we%2520construct%2520multimodal%250Avisual%2520leakless%2520safety%2520benchmark%2520%2528VLSBench%2529%2520preventing%2520visual%2520safety%2520leakage%250Afrom%2520image%2520to%2520textual%2520query%2520with%25202.4k%2520image-text%2520pairs.%2520Experimental%2520results%250Aindicate%2520that%2520VLSBench%2520poses%2520a%2520significant%2520challenge%2520to%2520both%2520open-source%2520and%250Aclose-source%2520MLLMs%252C%2520including%2520LLaVA%252C%2520Qwen2-VL%252C%2520Llama3.2-Vision%252C%2520and%2520GPT-4o.%250AThis%2520study%2520demonstrates%2520that%2520textual%2520alignment%2520is%2520enough%2520for%2520multimodal%2520safety%250Ascenarios%2520with%2520VSIL%252C%2520while%2520multimodal%2520alignment%2520is%2520a%2520more%2520promising%2520solution%250Afor%2520multimodal%2520safety%2520scenarios%2520without%2520VSIL.%2520Please%2520see%2520our%2520code%2520and%2520data%2520at%253A%250Ahttps%253A//hxhcreate.github.io/vlsbench.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19939v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLSBench%3A%20Unveiling%20Visual%20Leakage%20in%20Multimodal%20Safety&entry.906535625=Xuhao%20Hu%20and%20Dongrui%20Liu%20and%20Hao%20Li%20and%20Xuanjing%20Huang%20and%20Jing%20Shao&entry.1292438233=%20%20Safety%20concerns%20of%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20gradually%0Abecome%20an%20important%20problem%20in%20various%20applications.%20Surprisingly%2C%20previous%0Aworks%20indicate%20a%20counter-intuitive%20phenomenon%20that%20using%20textual%20unlearning%20to%0Aalign%20MLLMs%20achieves%20comparable%20safety%20performances%20with%20MLLMs%20trained%20with%0Aimage-text%20pairs.%20To%20explain%20such%20a%20counter-intuitive%20phenomenon%2C%20we%20discover%20a%0Avisual%20safety%20information%20leakage%20%28VSIL%29%20problem%20in%20existing%20multimodal%20safety%0Abenchmarks%2C%20i.e.%2C%20the%20potentially%20risky%20and%20sensitive%20content%20in%20the%20image%20has%0Abeen%20revealed%20in%20the%20textual%20query.%20In%20this%20way%2C%20MLLMs%20can%20easily%20refuse%20these%0Asensitive%20text-image%20queries%20according%20to%20textual%20queries.%20However%2C%20image-text%0Apairs%20without%20VSIL%20are%20common%20in%20real-world%20scenarios%20and%20are%20overlooked%20by%0Aexisting%20multimodal%20safety%20benchmarks.%20To%20this%20end%2C%20we%20construct%20multimodal%0Avisual%20leakless%20safety%20benchmark%20%28VLSBench%29%20preventing%20visual%20safety%20leakage%0Afrom%20image%20to%20textual%20query%20with%202.4k%20image-text%20pairs.%20Experimental%20results%0Aindicate%20that%20VLSBench%20poses%20a%20significant%20challenge%20to%20both%20open-source%20and%0Aclose-source%20MLLMs%2C%20including%20LLaVA%2C%20Qwen2-VL%2C%20Llama3.2-Vision%2C%20and%20GPT-4o.%0AThis%20study%20demonstrates%20that%20textual%20alignment%20is%20enough%20for%20multimodal%20safety%0Ascenarios%20with%20VSIL%2C%20while%20multimodal%20alignment%20is%20a%20more%20promising%20solution%0Afor%20multimodal%20safety%20scenarios%20without%20VSIL.%20Please%20see%20our%20code%20and%20data%20at%3A%0Ahttps%3A//hxhcreate.github.io/vlsbench.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19939v2&entry.124074799=Read"},
{"title": "Audio-Driven Reinforcement Learning for Head-Orientation in Naturalistic\n  Environments", "author": "Wessel Ledder and Yuzhen Qin and Kiki van der Heijden", "abstract": "  Although deep reinforcement learning (DRL) approaches in audio signal\nprocessing have seen substantial progress in recent years, audio-driven DRL for\ntasks such as navigation, gaze control and head-orientation control in the\ncontext of human-robot interaction have received little attention. Here, we\npropose an audio-driven DRL framework in which we utilise deep Q-learning to\ndevelop an autonomous agent that orients towards a talker in the acoustic\nenvironment based on stereo speech recordings. Our results show that the agent\nlearned to perform the task at a near perfect level when trained on speech\nsegments in anechoic environments (that is, without reverberation). The\npresence of reverberation in naturalistic acoustic environments affected the\nagent's performance, although the agent still substantially outperformed a\nbaseline, randomly acting agent. Finally, we quantified the degree of\ngeneralization of the proposed DRL approach across naturalistic acoustic\nenvironments. Our experiments revealed that policies learned by agents trained\non medium or high reverb environments generalized to low reverb environments,\nbut policies learned by agents trained on anechoic or low reverb environments\ndid not generalize to medium or high reverb environments. Taken together, this\nstudy demonstrates the potential of audio-driven DRL for tasks such as\nhead-orientation control and highlights the need for training strategies that\nenable robust generalization across environments for real-world audio-driven\nDRL applications.\n", "link": "http://arxiv.org/abs/2409.10048v2", "date": "2025-01-17", "relevancy": 2.1188, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5385}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5279}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5279}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Audio-Driven%20Reinforcement%20Learning%20for%20Head-Orientation%20in%20Naturalistic%0A%20%20Environments&body=Title%3A%20Audio-Driven%20Reinforcement%20Learning%20for%20Head-Orientation%20in%20Naturalistic%0A%20%20Environments%0AAuthor%3A%20Wessel%20Ledder%20and%20Yuzhen%20Qin%20and%20Kiki%20van%20der%20Heijden%0AAbstract%3A%20%20%20Although%20deep%20reinforcement%20learning%20%28DRL%29%20approaches%20in%20audio%20signal%0Aprocessing%20have%20seen%20substantial%20progress%20in%20recent%20years%2C%20audio-driven%20DRL%20for%0Atasks%20such%20as%20navigation%2C%20gaze%20control%20and%20head-orientation%20control%20in%20the%0Acontext%20of%20human-robot%20interaction%20have%20received%20little%20attention.%20Here%2C%20we%0Apropose%20an%20audio-driven%20DRL%20framework%20in%20which%20we%20utilise%20deep%20Q-learning%20to%0Adevelop%20an%20autonomous%20agent%20that%20orients%20towards%20a%20talker%20in%20the%20acoustic%0Aenvironment%20based%20on%20stereo%20speech%20recordings.%20Our%20results%20show%20that%20the%20agent%0Alearned%20to%20perform%20the%20task%20at%20a%20near%20perfect%20level%20when%20trained%20on%20speech%0Asegments%20in%20anechoic%20environments%20%28that%20is%2C%20without%20reverberation%29.%20The%0Apresence%20of%20reverberation%20in%20naturalistic%20acoustic%20environments%20affected%20the%0Aagent%27s%20performance%2C%20although%20the%20agent%20still%20substantially%20outperformed%20a%0Abaseline%2C%20randomly%20acting%20agent.%20Finally%2C%20we%20quantified%20the%20degree%20of%0Ageneralization%20of%20the%20proposed%20DRL%20approach%20across%20naturalistic%20acoustic%0Aenvironments.%20Our%20experiments%20revealed%20that%20policies%20learned%20by%20agents%20trained%0Aon%20medium%20or%20high%20reverb%20environments%20generalized%20to%20low%20reverb%20environments%2C%0Abut%20policies%20learned%20by%20agents%20trained%20on%20anechoic%20or%20low%20reverb%20environments%0Adid%20not%20generalize%20to%20medium%20or%20high%20reverb%20environments.%20Taken%20together%2C%20this%0Astudy%20demonstrates%20the%20potential%20of%20audio-driven%20DRL%20for%20tasks%20such%20as%0Ahead-orientation%20control%20and%20highlights%20the%20need%20for%20training%20strategies%20that%0Aenable%20robust%20generalization%20across%20environments%20for%20real-world%20audio-driven%0ADRL%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10048v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAudio-Driven%2520Reinforcement%2520Learning%2520for%2520Head-Orientation%2520in%2520Naturalistic%250A%2520%2520Environments%26entry.906535625%3DWessel%2520Ledder%2520and%2520Yuzhen%2520Qin%2520and%2520Kiki%2520van%2520der%2520Heijden%26entry.1292438233%3D%2520%2520Although%2520deep%2520reinforcement%2520learning%2520%2528DRL%2529%2520approaches%2520in%2520audio%2520signal%250Aprocessing%2520have%2520seen%2520substantial%2520progress%2520in%2520recent%2520years%252C%2520audio-driven%2520DRL%2520for%250Atasks%2520such%2520as%2520navigation%252C%2520gaze%2520control%2520and%2520head-orientation%2520control%2520in%2520the%250Acontext%2520of%2520human-robot%2520interaction%2520have%2520received%2520little%2520attention.%2520Here%252C%2520we%250Apropose%2520an%2520audio-driven%2520DRL%2520framework%2520in%2520which%2520we%2520utilise%2520deep%2520Q-learning%2520to%250Adevelop%2520an%2520autonomous%2520agent%2520that%2520orients%2520towards%2520a%2520talker%2520in%2520the%2520acoustic%250Aenvironment%2520based%2520on%2520stereo%2520speech%2520recordings.%2520Our%2520results%2520show%2520that%2520the%2520agent%250Alearned%2520to%2520perform%2520the%2520task%2520at%2520a%2520near%2520perfect%2520level%2520when%2520trained%2520on%2520speech%250Asegments%2520in%2520anechoic%2520environments%2520%2528that%2520is%252C%2520without%2520reverberation%2529.%2520The%250Apresence%2520of%2520reverberation%2520in%2520naturalistic%2520acoustic%2520environments%2520affected%2520the%250Aagent%2527s%2520performance%252C%2520although%2520the%2520agent%2520still%2520substantially%2520outperformed%2520a%250Abaseline%252C%2520randomly%2520acting%2520agent.%2520Finally%252C%2520we%2520quantified%2520the%2520degree%2520of%250Ageneralization%2520of%2520the%2520proposed%2520DRL%2520approach%2520across%2520naturalistic%2520acoustic%250Aenvironments.%2520Our%2520experiments%2520revealed%2520that%2520policies%2520learned%2520by%2520agents%2520trained%250Aon%2520medium%2520or%2520high%2520reverb%2520environments%2520generalized%2520to%2520low%2520reverb%2520environments%252C%250Abut%2520policies%2520learned%2520by%2520agents%2520trained%2520on%2520anechoic%2520or%2520low%2520reverb%2520environments%250Adid%2520not%2520generalize%2520to%2520medium%2520or%2520high%2520reverb%2520environments.%2520Taken%2520together%252C%2520this%250Astudy%2520demonstrates%2520the%2520potential%2520of%2520audio-driven%2520DRL%2520for%2520tasks%2520such%2520as%250Ahead-orientation%2520control%2520and%2520highlights%2520the%2520need%2520for%2520training%2520strategies%2520that%250Aenable%2520robust%2520generalization%2520across%2520environments%2520for%2520real-world%2520audio-driven%250ADRL%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10048v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Audio-Driven%20Reinforcement%20Learning%20for%20Head-Orientation%20in%20Naturalistic%0A%20%20Environments&entry.906535625=Wessel%20Ledder%20and%20Yuzhen%20Qin%20and%20Kiki%20van%20der%20Heijden&entry.1292438233=%20%20Although%20deep%20reinforcement%20learning%20%28DRL%29%20approaches%20in%20audio%20signal%0Aprocessing%20have%20seen%20substantial%20progress%20in%20recent%20years%2C%20audio-driven%20DRL%20for%0Atasks%20such%20as%20navigation%2C%20gaze%20control%20and%20head-orientation%20control%20in%20the%0Acontext%20of%20human-robot%20interaction%20have%20received%20little%20attention.%20Here%2C%20we%0Apropose%20an%20audio-driven%20DRL%20framework%20in%20which%20we%20utilise%20deep%20Q-learning%20to%0Adevelop%20an%20autonomous%20agent%20that%20orients%20towards%20a%20talker%20in%20the%20acoustic%0Aenvironment%20based%20on%20stereo%20speech%20recordings.%20Our%20results%20show%20that%20the%20agent%0Alearned%20to%20perform%20the%20task%20at%20a%20near%20perfect%20level%20when%20trained%20on%20speech%0Asegments%20in%20anechoic%20environments%20%28that%20is%2C%20without%20reverberation%29.%20The%0Apresence%20of%20reverberation%20in%20naturalistic%20acoustic%20environments%20affected%20the%0Aagent%27s%20performance%2C%20although%20the%20agent%20still%20substantially%20outperformed%20a%0Abaseline%2C%20randomly%20acting%20agent.%20Finally%2C%20we%20quantified%20the%20degree%20of%0Ageneralization%20of%20the%20proposed%20DRL%20approach%20across%20naturalistic%20acoustic%0Aenvironments.%20Our%20experiments%20revealed%20that%20policies%20learned%20by%20agents%20trained%0Aon%20medium%20or%20high%20reverb%20environments%20generalized%20to%20low%20reverb%20environments%2C%0Abut%20policies%20learned%20by%20agents%20trained%20on%20anechoic%20or%20low%20reverb%20environments%0Adid%20not%20generalize%20to%20medium%20or%20high%20reverb%20environments.%20Taken%20together%2C%20this%0Astudy%20demonstrates%20the%20potential%20of%20audio-driven%20DRL%20for%20tasks%20such%20as%0Ahead-orientation%20control%20and%20highlights%20the%20need%20for%20training%20strategies%20that%0Aenable%20robust%20generalization%20across%20environments%20for%20real-world%20audio-driven%0ADRL%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10048v2&entry.124074799=Read"},
{"title": "Exploring the Impact of Generative Artificial Intelligence in Education:\n  A Thematic Analysis", "author": "Abhishek Kaushik and Sargam Yadav and Andrew Browne and David Lillis and David Williams and Jack Mc Donnell and Peadar Grant and Siobhan Connolly Kernan and Shubham Sharma and Mansi Arora", "abstract": "  The recent advancements in Generative Artificial intelligence (GenAI)\ntechnology have been transformative for the field of education. Large Language\nModels (LLMs) such as ChatGPT and Bard can be leveraged to automate boilerplate\ntasks, create content for personalised teaching, and handle repetitive tasks to\nallow more time for creative thinking. However, it is important to develop\nguidelines, policies, and assessment methods in the education sector to ensure\nthe responsible integration of these tools. In this article, thematic analysis\nhas been performed on seven essays obtained from professionals in the education\nsector to understand the advantages and pitfalls of using GenAI models such as\nChatGPT and Bard in education. Exploratory Data Analysis (EDA) has been\nperformed on the essays to extract further insights from the text. The study\nfound several themes which highlight benefits and drawbacks of GenAI tools, as\nwell as suggestions to overcome these limitations and ensure that students are\nusing these tools in a responsible and ethical manner.\n", "link": "http://arxiv.org/abs/2501.10134v1", "date": "2025-01-17", "relevancy": 2.1085, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5492}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5289}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4676}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Impact%20of%20Generative%20Artificial%20Intelligence%20in%20Education%3A%0A%20%20A%20Thematic%20Analysis&body=Title%3A%20Exploring%20the%20Impact%20of%20Generative%20Artificial%20Intelligence%20in%20Education%3A%0A%20%20A%20Thematic%20Analysis%0AAuthor%3A%20Abhishek%20Kaushik%20and%20Sargam%20Yadav%20and%20Andrew%20Browne%20and%20David%20Lillis%20and%20David%20Williams%20and%20Jack%20Mc%20Donnell%20and%20Peadar%20Grant%20and%20Siobhan%20Connolly%20Kernan%20and%20Shubham%20Sharma%20and%20Mansi%20Arora%0AAbstract%3A%20%20%20The%20recent%20advancements%20in%20Generative%20Artificial%20intelligence%20%28GenAI%29%0Atechnology%20have%20been%20transformative%20for%20the%20field%20of%20education.%20Large%20Language%0AModels%20%28LLMs%29%20such%20as%20ChatGPT%20and%20Bard%20can%20be%20leveraged%20to%20automate%20boilerplate%0Atasks%2C%20create%20content%20for%20personalised%20teaching%2C%20and%20handle%20repetitive%20tasks%20to%0Aallow%20more%20time%20for%20creative%20thinking.%20However%2C%20it%20is%20important%20to%20develop%0Aguidelines%2C%20policies%2C%20and%20assessment%20methods%20in%20the%20education%20sector%20to%20ensure%0Athe%20responsible%20integration%20of%20these%20tools.%20In%20this%20article%2C%20thematic%20analysis%0Ahas%20been%20performed%20on%20seven%20essays%20obtained%20from%20professionals%20in%20the%20education%0Asector%20to%20understand%20the%20advantages%20and%20pitfalls%20of%20using%20GenAI%20models%20such%20as%0AChatGPT%20and%20Bard%20in%20education.%20Exploratory%20Data%20Analysis%20%28EDA%29%20has%20been%0Aperformed%20on%20the%20essays%20to%20extract%20further%20insights%20from%20the%20text.%20The%20study%0Afound%20several%20themes%20which%20highlight%20benefits%20and%20drawbacks%20of%20GenAI%20tools%2C%20as%0Awell%20as%20suggestions%20to%20overcome%20these%20limitations%20and%20ensure%20that%20students%20are%0Ausing%20these%20tools%20in%20a%20responsible%20and%20ethical%20manner.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10134v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Impact%2520of%2520Generative%2520Artificial%2520Intelligence%2520in%2520Education%253A%250A%2520%2520A%2520Thematic%2520Analysis%26entry.906535625%3DAbhishek%2520Kaushik%2520and%2520Sargam%2520Yadav%2520and%2520Andrew%2520Browne%2520and%2520David%2520Lillis%2520and%2520David%2520Williams%2520and%2520Jack%2520Mc%2520Donnell%2520and%2520Peadar%2520Grant%2520and%2520Siobhan%2520Connolly%2520Kernan%2520and%2520Shubham%2520Sharma%2520and%2520Mansi%2520Arora%26entry.1292438233%3D%2520%2520The%2520recent%2520advancements%2520in%2520Generative%2520Artificial%2520intelligence%2520%2528GenAI%2529%250Atechnology%2520have%2520been%2520transformative%2520for%2520the%2520field%2520of%2520education.%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520such%2520as%2520ChatGPT%2520and%2520Bard%2520can%2520be%2520leveraged%2520to%2520automate%2520boilerplate%250Atasks%252C%2520create%2520content%2520for%2520personalised%2520teaching%252C%2520and%2520handle%2520repetitive%2520tasks%2520to%250Aallow%2520more%2520time%2520for%2520creative%2520thinking.%2520However%252C%2520it%2520is%2520important%2520to%2520develop%250Aguidelines%252C%2520policies%252C%2520and%2520assessment%2520methods%2520in%2520the%2520education%2520sector%2520to%2520ensure%250Athe%2520responsible%2520integration%2520of%2520these%2520tools.%2520In%2520this%2520article%252C%2520thematic%2520analysis%250Ahas%2520been%2520performed%2520on%2520seven%2520essays%2520obtained%2520from%2520professionals%2520in%2520the%2520education%250Asector%2520to%2520understand%2520the%2520advantages%2520and%2520pitfalls%2520of%2520using%2520GenAI%2520models%2520such%2520as%250AChatGPT%2520and%2520Bard%2520in%2520education.%2520Exploratory%2520Data%2520Analysis%2520%2528EDA%2529%2520has%2520been%250Aperformed%2520on%2520the%2520essays%2520to%2520extract%2520further%2520insights%2520from%2520the%2520text.%2520The%2520study%250Afound%2520several%2520themes%2520which%2520highlight%2520benefits%2520and%2520drawbacks%2520of%2520GenAI%2520tools%252C%2520as%250Awell%2520as%2520suggestions%2520to%2520overcome%2520these%2520limitations%2520and%2520ensure%2520that%2520students%2520are%250Ausing%2520these%2520tools%2520in%2520a%2520responsible%2520and%2520ethical%2520manner.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10134v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Impact%20of%20Generative%20Artificial%20Intelligence%20in%20Education%3A%0A%20%20A%20Thematic%20Analysis&entry.906535625=Abhishek%20Kaushik%20and%20Sargam%20Yadav%20and%20Andrew%20Browne%20and%20David%20Lillis%20and%20David%20Williams%20and%20Jack%20Mc%20Donnell%20and%20Peadar%20Grant%20and%20Siobhan%20Connolly%20Kernan%20and%20Shubham%20Sharma%20and%20Mansi%20Arora&entry.1292438233=%20%20The%20recent%20advancements%20in%20Generative%20Artificial%20intelligence%20%28GenAI%29%0Atechnology%20have%20been%20transformative%20for%20the%20field%20of%20education.%20Large%20Language%0AModels%20%28LLMs%29%20such%20as%20ChatGPT%20and%20Bard%20can%20be%20leveraged%20to%20automate%20boilerplate%0Atasks%2C%20create%20content%20for%20personalised%20teaching%2C%20and%20handle%20repetitive%20tasks%20to%0Aallow%20more%20time%20for%20creative%20thinking.%20However%2C%20it%20is%20important%20to%20develop%0Aguidelines%2C%20policies%2C%20and%20assessment%20methods%20in%20the%20education%20sector%20to%20ensure%0Athe%20responsible%20integration%20of%20these%20tools.%20In%20this%20article%2C%20thematic%20analysis%0Ahas%20been%20performed%20on%20seven%20essays%20obtained%20from%20professionals%20in%20the%20education%0Asector%20to%20understand%20the%20advantages%20and%20pitfalls%20of%20using%20GenAI%20models%20such%20as%0AChatGPT%20and%20Bard%20in%20education.%20Exploratory%20Data%20Analysis%20%28EDA%29%20has%20been%0Aperformed%20on%20the%20essays%20to%20extract%20further%20insights%20from%20the%20text.%20The%20study%0Afound%20several%20themes%20which%20highlight%20benefits%20and%20drawbacks%20of%20GenAI%20tools%2C%20as%0Awell%20as%20suggestions%20to%20overcome%20these%20limitations%20and%20ensure%20that%20students%20are%0Ausing%20these%20tools%20in%20a%20responsible%20and%20ethical%20manner.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10134v1&entry.124074799=Read"},
{"title": "DexForce: Extracting Force-informed Actions from Kinesthetic\n  Demonstrations for Dexterous Manipulation", "author": "Claire Chen and Zhongchun Yu and Hojung Choi and Mark Cutkosky and Jeannette Bohg", "abstract": "  Imitation learning requires high-quality demonstrations consisting of\nsequences of state-action pairs. For contact-rich dexterous manipulation tasks\nthat require fine-grained dexterity, the actions in these state-action pairs\nmust produce the right forces. Current widely-used methods for collecting\ndexterous manipulation demonstrations are difficult to use for demonstrating\ncontact-rich tasks due to unintuitive human-to-robot motion retargeting and the\nlack of direct haptic feedback. Motivated by this, we propose DexForce, a\nmethod for collecting demonstrations of contact-rich dexterous manipulation.\nDexForce leverages contact forces, measured during kinesthetic demonstrations,\nto compute force-informed actions for policy learning. We use DexForce to\ncollect demonstrations for six tasks and show that policies trained on our\nforce-informed actions achieve an average success rate of 76% across all tasks.\nIn contrast, policies trained directly on actions that do not account for\ncontact forces have near-zero success rates. We also conduct a study ablating\nthe inclusion of force data in policy observations. We find that while using\nforce data never hurts policy performance, it helps the most for tasks that\nrequire an advanced level of precision and coordination, like opening an\nAirPods case and unscrewing a nut.\n", "link": "http://arxiv.org/abs/2501.10356v1", "date": "2025-01-17", "relevancy": 2.1025, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5365}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5242}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5153}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DexForce%3A%20Extracting%20Force-informed%20Actions%20from%20Kinesthetic%0A%20%20Demonstrations%20for%20Dexterous%20Manipulation&body=Title%3A%20DexForce%3A%20Extracting%20Force-informed%20Actions%20from%20Kinesthetic%0A%20%20Demonstrations%20for%20Dexterous%20Manipulation%0AAuthor%3A%20Claire%20Chen%20and%20Zhongchun%20Yu%20and%20Hojung%20Choi%20and%20Mark%20Cutkosky%20and%20Jeannette%20Bohg%0AAbstract%3A%20%20%20Imitation%20learning%20requires%20high-quality%20demonstrations%20consisting%20of%0Asequences%20of%20state-action%20pairs.%20For%20contact-rich%20dexterous%20manipulation%20tasks%0Athat%20require%20fine-grained%20dexterity%2C%20the%20actions%20in%20these%20state-action%20pairs%0Amust%20produce%20the%20right%20forces.%20Current%20widely-used%20methods%20for%20collecting%0Adexterous%20manipulation%20demonstrations%20are%20difficult%20to%20use%20for%20demonstrating%0Acontact-rich%20tasks%20due%20to%20unintuitive%20human-to-robot%20motion%20retargeting%20and%20the%0Alack%20of%20direct%20haptic%20feedback.%20Motivated%20by%20this%2C%20we%20propose%20DexForce%2C%20a%0Amethod%20for%20collecting%20demonstrations%20of%20contact-rich%20dexterous%20manipulation.%0ADexForce%20leverages%20contact%20forces%2C%20measured%20during%20kinesthetic%20demonstrations%2C%0Ato%20compute%20force-informed%20actions%20for%20policy%20learning.%20We%20use%20DexForce%20to%0Acollect%20demonstrations%20for%20six%20tasks%20and%20show%20that%20policies%20trained%20on%20our%0Aforce-informed%20actions%20achieve%20an%20average%20success%20rate%20of%2076%25%20across%20all%20tasks.%0AIn%20contrast%2C%20policies%20trained%20directly%20on%20actions%20that%20do%20not%20account%20for%0Acontact%20forces%20have%20near-zero%20success%20rates.%20We%20also%20conduct%20a%20study%20ablating%0Athe%20inclusion%20of%20force%20data%20in%20policy%20observations.%20We%20find%20that%20while%20using%0Aforce%20data%20never%20hurts%20policy%20performance%2C%20it%20helps%20the%20most%20for%20tasks%20that%0Arequire%20an%20advanced%20level%20of%20precision%20and%20coordination%2C%20like%20opening%20an%0AAirPods%20case%20and%20unscrewing%20a%20nut.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10356v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDexForce%253A%2520Extracting%2520Force-informed%2520Actions%2520from%2520Kinesthetic%250A%2520%2520Demonstrations%2520for%2520Dexterous%2520Manipulation%26entry.906535625%3DClaire%2520Chen%2520and%2520Zhongchun%2520Yu%2520and%2520Hojung%2520Choi%2520and%2520Mark%2520Cutkosky%2520and%2520Jeannette%2520Bohg%26entry.1292438233%3D%2520%2520Imitation%2520learning%2520requires%2520high-quality%2520demonstrations%2520consisting%2520of%250Asequences%2520of%2520state-action%2520pairs.%2520For%2520contact-rich%2520dexterous%2520manipulation%2520tasks%250Athat%2520require%2520fine-grained%2520dexterity%252C%2520the%2520actions%2520in%2520these%2520state-action%2520pairs%250Amust%2520produce%2520the%2520right%2520forces.%2520Current%2520widely-used%2520methods%2520for%2520collecting%250Adexterous%2520manipulation%2520demonstrations%2520are%2520difficult%2520to%2520use%2520for%2520demonstrating%250Acontact-rich%2520tasks%2520due%2520to%2520unintuitive%2520human-to-robot%2520motion%2520retargeting%2520and%2520the%250Alack%2520of%2520direct%2520haptic%2520feedback.%2520Motivated%2520by%2520this%252C%2520we%2520propose%2520DexForce%252C%2520a%250Amethod%2520for%2520collecting%2520demonstrations%2520of%2520contact-rich%2520dexterous%2520manipulation.%250ADexForce%2520leverages%2520contact%2520forces%252C%2520measured%2520during%2520kinesthetic%2520demonstrations%252C%250Ato%2520compute%2520force-informed%2520actions%2520for%2520policy%2520learning.%2520We%2520use%2520DexForce%2520to%250Acollect%2520demonstrations%2520for%2520six%2520tasks%2520and%2520show%2520that%2520policies%2520trained%2520on%2520our%250Aforce-informed%2520actions%2520achieve%2520an%2520average%2520success%2520rate%2520of%252076%2525%2520across%2520all%2520tasks.%250AIn%2520contrast%252C%2520policies%2520trained%2520directly%2520on%2520actions%2520that%2520do%2520not%2520account%2520for%250Acontact%2520forces%2520have%2520near-zero%2520success%2520rates.%2520We%2520also%2520conduct%2520a%2520study%2520ablating%250Athe%2520inclusion%2520of%2520force%2520data%2520in%2520policy%2520observations.%2520We%2520find%2520that%2520while%2520using%250Aforce%2520data%2520never%2520hurts%2520policy%2520performance%252C%2520it%2520helps%2520the%2520most%2520for%2520tasks%2520that%250Arequire%2520an%2520advanced%2520level%2520of%2520precision%2520and%2520coordination%252C%2520like%2520opening%2520an%250AAirPods%2520case%2520and%2520unscrewing%2520a%2520nut.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10356v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DexForce%3A%20Extracting%20Force-informed%20Actions%20from%20Kinesthetic%0A%20%20Demonstrations%20for%20Dexterous%20Manipulation&entry.906535625=Claire%20Chen%20and%20Zhongchun%20Yu%20and%20Hojung%20Choi%20and%20Mark%20Cutkosky%20and%20Jeannette%20Bohg&entry.1292438233=%20%20Imitation%20learning%20requires%20high-quality%20demonstrations%20consisting%20of%0Asequences%20of%20state-action%20pairs.%20For%20contact-rich%20dexterous%20manipulation%20tasks%0Athat%20require%20fine-grained%20dexterity%2C%20the%20actions%20in%20these%20state-action%20pairs%0Amust%20produce%20the%20right%20forces.%20Current%20widely-used%20methods%20for%20collecting%0Adexterous%20manipulation%20demonstrations%20are%20difficult%20to%20use%20for%20demonstrating%0Acontact-rich%20tasks%20due%20to%20unintuitive%20human-to-robot%20motion%20retargeting%20and%20the%0Alack%20of%20direct%20haptic%20feedback.%20Motivated%20by%20this%2C%20we%20propose%20DexForce%2C%20a%0Amethod%20for%20collecting%20demonstrations%20of%20contact-rich%20dexterous%20manipulation.%0ADexForce%20leverages%20contact%20forces%2C%20measured%20during%20kinesthetic%20demonstrations%2C%0Ato%20compute%20force-informed%20actions%20for%20policy%20learning.%20We%20use%20DexForce%20to%0Acollect%20demonstrations%20for%20six%20tasks%20and%20show%20that%20policies%20trained%20on%20our%0Aforce-informed%20actions%20achieve%20an%20average%20success%20rate%20of%2076%25%20across%20all%20tasks.%0AIn%20contrast%2C%20policies%20trained%20directly%20on%20actions%20that%20do%20not%20account%20for%0Acontact%20forces%20have%20near-zero%20success%20rates.%20We%20also%20conduct%20a%20study%20ablating%0Athe%20inclusion%20of%20force%20data%20in%20policy%20observations.%20We%20find%20that%20while%20using%0Aforce%20data%20never%20hurts%20policy%20performance%2C%20it%20helps%20the%20most%20for%20tasks%20that%0Arequire%20an%20advanced%20level%20of%20precision%20and%20coordination%2C%20like%20opening%20an%0AAirPods%20case%20and%20unscrewing%20a%20nut.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10356v1&entry.124074799=Read"},
{"title": "New Fashion Products Performance Forecasting: A Survey on Evolutions,\n  Models and Emerging Trends", "author": "Andrea Avogaro and Luigi Capogrosso and Andrea Toaiari and Franco Fummi and Marco Cristani", "abstract": "  The fast fashion industry's insatiable demand for new styles and rapid\nproduction cycles has led to a significant environmental burden.\nOverproduction, excessive waste, and harmful chemicals have contributed to the\nnegative environmental impact of the industry. To mitigate these issues, a\nparadigm shift that prioritizes sustainability and efficiency is urgently\nneeded. Integrating learning-based predictive analytics into the fashion\nindustry represents a significant opportunity to address environmental\nchallenges and drive sustainable practices. By forecasting fashion trends and\noptimizing production, brands can reduce their ecological footprint while\nremaining competitive in a rapidly changing market. However, one of the key\nchallenges in forecasting fashion sales is the dynamic nature of consumer\npreferences. Fashion is acyclical, with trends constantly evolving and\nresurfacing. In addition, cultural changes and unexpected events can disrupt\nestablished patterns. This problem is also known as New Fashion Products\nPerformance Forecasting (NFPPF), and it has recently gained more and more\ninterest in the global research landscape. Given its multidisciplinary nature,\nthe field of NFPPF has been approached from many different angles. This\ncomprehensive survey wishes to provide an up-to-date overview that focuses on\nlearning-based NFPPF strategies. The survey is based on the Preferred Reporting\nItems for Systematic Reviews and Meta-Analyses (PRISMA) methodological flow,\nallowing for a systematic and complete literature review. In particular, we\npropose the first taxonomy that covers the learning panorama for NFPPF,\nexamining in detail the different methodologies used to increase the amount of\nmultimodal information, as well as the state-of-the-art available datasets.\nFinally, we discuss the challenges and future directions.\n", "link": "http://arxiv.org/abs/2501.10324v1", "date": "2025-01-17", "relevancy": 2.0916, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5757}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5145}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5101}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20New%20Fashion%20Products%20Performance%20Forecasting%3A%20A%20Survey%20on%20Evolutions%2C%0A%20%20Models%20and%20Emerging%20Trends&body=Title%3A%20New%20Fashion%20Products%20Performance%20Forecasting%3A%20A%20Survey%20on%20Evolutions%2C%0A%20%20Models%20and%20Emerging%20Trends%0AAuthor%3A%20Andrea%20Avogaro%20and%20Luigi%20Capogrosso%20and%20Andrea%20Toaiari%20and%20Franco%20Fummi%20and%20Marco%20Cristani%0AAbstract%3A%20%20%20The%20fast%20fashion%20industry%27s%20insatiable%20demand%20for%20new%20styles%20and%20rapid%0Aproduction%20cycles%20has%20led%20to%20a%20significant%20environmental%20burden.%0AOverproduction%2C%20excessive%20waste%2C%20and%20harmful%20chemicals%20have%20contributed%20to%20the%0Anegative%20environmental%20impact%20of%20the%20industry.%20To%20mitigate%20these%20issues%2C%20a%0Aparadigm%20shift%20that%20prioritizes%20sustainability%20and%20efficiency%20is%20urgently%0Aneeded.%20Integrating%20learning-based%20predictive%20analytics%20into%20the%20fashion%0Aindustry%20represents%20a%20significant%20opportunity%20to%20address%20environmental%0Achallenges%20and%20drive%20sustainable%20practices.%20By%20forecasting%20fashion%20trends%20and%0Aoptimizing%20production%2C%20brands%20can%20reduce%20their%20ecological%20footprint%20while%0Aremaining%20competitive%20in%20a%20rapidly%20changing%20market.%20However%2C%20one%20of%20the%20key%0Achallenges%20in%20forecasting%20fashion%20sales%20is%20the%20dynamic%20nature%20of%20consumer%0Apreferences.%20Fashion%20is%20acyclical%2C%20with%20trends%20constantly%20evolving%20and%0Aresurfacing.%20In%20addition%2C%20cultural%20changes%20and%20unexpected%20events%20can%20disrupt%0Aestablished%20patterns.%20This%20problem%20is%20also%20known%20as%20New%20Fashion%20Products%0APerformance%20Forecasting%20%28NFPPF%29%2C%20and%20it%20has%20recently%20gained%20more%20and%20more%0Ainterest%20in%20the%20global%20research%20landscape.%20Given%20its%20multidisciplinary%20nature%2C%0Athe%20field%20of%20NFPPF%20has%20been%20approached%20from%20many%20different%20angles.%20This%0Acomprehensive%20survey%20wishes%20to%20provide%20an%20up-to-date%20overview%20that%20focuses%20on%0Alearning-based%20NFPPF%20strategies.%20The%20survey%20is%20based%20on%20the%20Preferred%20Reporting%0AItems%20for%20Systematic%20Reviews%20and%20Meta-Analyses%20%28PRISMA%29%20methodological%20flow%2C%0Aallowing%20for%20a%20systematic%20and%20complete%20literature%20review.%20In%20particular%2C%20we%0Apropose%20the%20first%20taxonomy%20that%20covers%20the%20learning%20panorama%20for%20NFPPF%2C%0Aexamining%20in%20detail%20the%20different%20methodologies%20used%20to%20increase%20the%20amount%20of%0Amultimodal%20information%2C%20as%20well%20as%20the%20state-of-the-art%20available%20datasets.%0AFinally%2C%20we%20discuss%20the%20challenges%20and%20future%20directions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10324v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNew%2520Fashion%2520Products%2520Performance%2520Forecasting%253A%2520A%2520Survey%2520on%2520Evolutions%252C%250A%2520%2520Models%2520and%2520Emerging%2520Trends%26entry.906535625%3DAndrea%2520Avogaro%2520and%2520Luigi%2520Capogrosso%2520and%2520Andrea%2520Toaiari%2520and%2520Franco%2520Fummi%2520and%2520Marco%2520Cristani%26entry.1292438233%3D%2520%2520The%2520fast%2520fashion%2520industry%2527s%2520insatiable%2520demand%2520for%2520new%2520styles%2520and%2520rapid%250Aproduction%2520cycles%2520has%2520led%2520to%2520a%2520significant%2520environmental%2520burden.%250AOverproduction%252C%2520excessive%2520waste%252C%2520and%2520harmful%2520chemicals%2520have%2520contributed%2520to%2520the%250Anegative%2520environmental%2520impact%2520of%2520the%2520industry.%2520To%2520mitigate%2520these%2520issues%252C%2520a%250Aparadigm%2520shift%2520that%2520prioritizes%2520sustainability%2520and%2520efficiency%2520is%2520urgently%250Aneeded.%2520Integrating%2520learning-based%2520predictive%2520analytics%2520into%2520the%2520fashion%250Aindustry%2520represents%2520a%2520significant%2520opportunity%2520to%2520address%2520environmental%250Achallenges%2520and%2520drive%2520sustainable%2520practices.%2520By%2520forecasting%2520fashion%2520trends%2520and%250Aoptimizing%2520production%252C%2520brands%2520can%2520reduce%2520their%2520ecological%2520footprint%2520while%250Aremaining%2520competitive%2520in%2520a%2520rapidly%2520changing%2520market.%2520However%252C%2520one%2520of%2520the%2520key%250Achallenges%2520in%2520forecasting%2520fashion%2520sales%2520is%2520the%2520dynamic%2520nature%2520of%2520consumer%250Apreferences.%2520Fashion%2520is%2520acyclical%252C%2520with%2520trends%2520constantly%2520evolving%2520and%250Aresurfacing.%2520In%2520addition%252C%2520cultural%2520changes%2520and%2520unexpected%2520events%2520can%2520disrupt%250Aestablished%2520patterns.%2520This%2520problem%2520is%2520also%2520known%2520as%2520New%2520Fashion%2520Products%250APerformance%2520Forecasting%2520%2528NFPPF%2529%252C%2520and%2520it%2520has%2520recently%2520gained%2520more%2520and%2520more%250Ainterest%2520in%2520the%2520global%2520research%2520landscape.%2520Given%2520its%2520multidisciplinary%2520nature%252C%250Athe%2520field%2520of%2520NFPPF%2520has%2520been%2520approached%2520from%2520many%2520different%2520angles.%2520This%250Acomprehensive%2520survey%2520wishes%2520to%2520provide%2520an%2520up-to-date%2520overview%2520that%2520focuses%2520on%250Alearning-based%2520NFPPF%2520strategies.%2520The%2520survey%2520is%2520based%2520on%2520the%2520Preferred%2520Reporting%250AItems%2520for%2520Systematic%2520Reviews%2520and%2520Meta-Analyses%2520%2528PRISMA%2529%2520methodological%2520flow%252C%250Aallowing%2520for%2520a%2520systematic%2520and%2520complete%2520literature%2520review.%2520In%2520particular%252C%2520we%250Apropose%2520the%2520first%2520taxonomy%2520that%2520covers%2520the%2520learning%2520panorama%2520for%2520NFPPF%252C%250Aexamining%2520in%2520detail%2520the%2520different%2520methodologies%2520used%2520to%2520increase%2520the%2520amount%2520of%250Amultimodal%2520information%252C%2520as%2520well%2520as%2520the%2520state-of-the-art%2520available%2520datasets.%250AFinally%252C%2520we%2520discuss%2520the%2520challenges%2520and%2520future%2520directions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10324v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=New%20Fashion%20Products%20Performance%20Forecasting%3A%20A%20Survey%20on%20Evolutions%2C%0A%20%20Models%20and%20Emerging%20Trends&entry.906535625=Andrea%20Avogaro%20and%20Luigi%20Capogrosso%20and%20Andrea%20Toaiari%20and%20Franco%20Fummi%20and%20Marco%20Cristani&entry.1292438233=%20%20The%20fast%20fashion%20industry%27s%20insatiable%20demand%20for%20new%20styles%20and%20rapid%0Aproduction%20cycles%20has%20led%20to%20a%20significant%20environmental%20burden.%0AOverproduction%2C%20excessive%20waste%2C%20and%20harmful%20chemicals%20have%20contributed%20to%20the%0Anegative%20environmental%20impact%20of%20the%20industry.%20To%20mitigate%20these%20issues%2C%20a%0Aparadigm%20shift%20that%20prioritizes%20sustainability%20and%20efficiency%20is%20urgently%0Aneeded.%20Integrating%20learning-based%20predictive%20analytics%20into%20the%20fashion%0Aindustry%20represents%20a%20significant%20opportunity%20to%20address%20environmental%0Achallenges%20and%20drive%20sustainable%20practices.%20By%20forecasting%20fashion%20trends%20and%0Aoptimizing%20production%2C%20brands%20can%20reduce%20their%20ecological%20footprint%20while%0Aremaining%20competitive%20in%20a%20rapidly%20changing%20market.%20However%2C%20one%20of%20the%20key%0Achallenges%20in%20forecasting%20fashion%20sales%20is%20the%20dynamic%20nature%20of%20consumer%0Apreferences.%20Fashion%20is%20acyclical%2C%20with%20trends%20constantly%20evolving%20and%0Aresurfacing.%20In%20addition%2C%20cultural%20changes%20and%20unexpected%20events%20can%20disrupt%0Aestablished%20patterns.%20This%20problem%20is%20also%20known%20as%20New%20Fashion%20Products%0APerformance%20Forecasting%20%28NFPPF%29%2C%20and%20it%20has%20recently%20gained%20more%20and%20more%0Ainterest%20in%20the%20global%20research%20landscape.%20Given%20its%20multidisciplinary%20nature%2C%0Athe%20field%20of%20NFPPF%20has%20been%20approached%20from%20many%20different%20angles.%20This%0Acomprehensive%20survey%20wishes%20to%20provide%20an%20up-to-date%20overview%20that%20focuses%20on%0Alearning-based%20NFPPF%20strategies.%20The%20survey%20is%20based%20on%20the%20Preferred%20Reporting%0AItems%20for%20Systematic%20Reviews%20and%20Meta-Analyses%20%28PRISMA%29%20methodological%20flow%2C%0Aallowing%20for%20a%20systematic%20and%20complete%20literature%20review.%20In%20particular%2C%20we%0Apropose%20the%20first%20taxonomy%20that%20covers%20the%20learning%20panorama%20for%20NFPPF%2C%0Aexamining%20in%20detail%20the%20different%20methodologies%20used%20to%20increase%20the%20amount%20of%0Amultimodal%20information%2C%20as%20well%20as%20the%20state-of-the-art%20available%20datasets.%0AFinally%2C%20we%20discuss%20the%20challenges%20and%20future%20directions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10324v1&entry.124074799=Read"},
{"title": "High-Rank Irreducible Cartesian Tensor Decomposition and Bases of\n  Equivariant Spaces", "author": "Shihao Shao and Yikang Li and Zhouchen Lin and Qinghua Cui", "abstract": "  Irreducible Cartesian tensors (ICTs) play a crucial role in the design of\nequivariant graph neural networks, as well as in theoretical chemistry and\nchemical physics. Meanwhile, the design space of available linear operations on\ntensors that preserve symmetry presents a significant challenge. The ICT\ndecomposition and a basis of this equivariant space are difficult to obtain for\nhigh-rank tensors. After decades of research, Bonvicini (2024) recently\nachieves an explicit ICT decomposition for $n=5$ with factorial time/space\ncomplexity. In this work we, for the first time, obtains decomposition matrices\nfor ICTs up to rank $n=9$ with reduced and affordable complexity, by\nconstructing what we call path matrices. The path matrices are obtained via\nperforming chain-like contractions with Clebsch-Gordan matrices following the\nparentage scheme. We prove and leverage that the concatenation of path matrices\nis an orthonormal change-of-basis matrix between the Cartesian tensor product\nspace and the spherical direct sum spaces. Furthermore, we identify a complete\northogonal basis for the equivariant space, rather than a spanning set\n(Pearce-Crump, 2023), through this path matrices technique. To the best of our\nknowledge, this is also the first analytic, rather than numerical, method for\ntheoretically obtaining arbitrary rank orthogonal ICT decomposition matrices\nand orthogonal equivariant bases. We further extend our result to the arbitrary\ntensor product and direct sum spaces, enabling free design between different\nspaces while keeping symmetry. The Python code is available at\nhttps://github.com/ShihaoShao-GH/ICT-decomposition-and-equivariant-bases, where\nthe $n=6,\\dots,9$ ICT decomposition matrices are obtained in 1s, 3s, 11s, and\n4m32s on 28-cores Intel(R) Xeon(R) Gold 6330 CPU @ 2.00GHz, respectively.\n", "link": "http://arxiv.org/abs/2412.18263v4", "date": "2025-01-17", "relevancy": 2.0497, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4144}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4087}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4067}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Rank%20Irreducible%20Cartesian%20Tensor%20Decomposition%20and%20Bases%20of%0A%20%20Equivariant%20Spaces&body=Title%3A%20High-Rank%20Irreducible%20Cartesian%20Tensor%20Decomposition%20and%20Bases%20of%0A%20%20Equivariant%20Spaces%0AAuthor%3A%20Shihao%20Shao%20and%20Yikang%20Li%20and%20Zhouchen%20Lin%20and%20Qinghua%20Cui%0AAbstract%3A%20%20%20Irreducible%20Cartesian%20tensors%20%28ICTs%29%20play%20a%20crucial%20role%20in%20the%20design%20of%0Aequivariant%20graph%20neural%20networks%2C%20as%20well%20as%20in%20theoretical%20chemistry%20and%0Achemical%20physics.%20Meanwhile%2C%20the%20design%20space%20of%20available%20linear%20operations%20on%0Atensors%20that%20preserve%20symmetry%20presents%20a%20significant%20challenge.%20The%20ICT%0Adecomposition%20and%20a%20basis%20of%20this%20equivariant%20space%20are%20difficult%20to%20obtain%20for%0Ahigh-rank%20tensors.%20After%20decades%20of%20research%2C%20Bonvicini%20%282024%29%20recently%0Aachieves%20an%20explicit%20ICT%20decomposition%20for%20%24n%3D5%24%20with%20factorial%20time/space%0Acomplexity.%20In%20this%20work%20we%2C%20for%20the%20first%20time%2C%20obtains%20decomposition%20matrices%0Afor%20ICTs%20up%20to%20rank%20%24n%3D9%24%20with%20reduced%20and%20affordable%20complexity%2C%20by%0Aconstructing%20what%20we%20call%20path%20matrices.%20The%20path%20matrices%20are%20obtained%20via%0Aperforming%20chain-like%20contractions%20with%20Clebsch-Gordan%20matrices%20following%20the%0Aparentage%20scheme.%20We%20prove%20and%20leverage%20that%20the%20concatenation%20of%20path%20matrices%0Ais%20an%20orthonormal%20change-of-basis%20matrix%20between%20the%20Cartesian%20tensor%20product%0Aspace%20and%20the%20spherical%20direct%20sum%20spaces.%20Furthermore%2C%20we%20identify%20a%20complete%0Aorthogonal%20basis%20for%20the%20equivariant%20space%2C%20rather%20than%20a%20spanning%20set%0A%28Pearce-Crump%2C%202023%29%2C%20through%20this%20path%20matrices%20technique.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20also%20the%20first%20analytic%2C%20rather%20than%20numerical%2C%20method%20for%0Atheoretically%20obtaining%20arbitrary%20rank%20orthogonal%20ICT%20decomposition%20matrices%0Aand%20orthogonal%20equivariant%20bases.%20We%20further%20extend%20our%20result%20to%20the%20arbitrary%0Atensor%20product%20and%20direct%20sum%20spaces%2C%20enabling%20free%20design%20between%20different%0Aspaces%20while%20keeping%20symmetry.%20The%20Python%20code%20is%20available%20at%0Ahttps%3A//github.com/ShihaoShao-GH/ICT-decomposition-and-equivariant-bases%2C%20where%0Athe%20%24n%3D6%2C%5Cdots%2C9%24%20ICT%20decomposition%20matrices%20are%20obtained%20in%201s%2C%203s%2C%2011s%2C%20and%0A4m32s%20on%2028-cores%20Intel%28R%29%20Xeon%28R%29%20Gold%206330%20CPU%20%40%202.00GHz%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18263v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Rank%2520Irreducible%2520Cartesian%2520Tensor%2520Decomposition%2520and%2520Bases%2520of%250A%2520%2520Equivariant%2520Spaces%26entry.906535625%3DShihao%2520Shao%2520and%2520Yikang%2520Li%2520and%2520Zhouchen%2520Lin%2520and%2520Qinghua%2520Cui%26entry.1292438233%3D%2520%2520Irreducible%2520Cartesian%2520tensors%2520%2528ICTs%2529%2520play%2520a%2520crucial%2520role%2520in%2520the%2520design%2520of%250Aequivariant%2520graph%2520neural%2520networks%252C%2520as%2520well%2520as%2520in%2520theoretical%2520chemistry%2520and%250Achemical%2520physics.%2520Meanwhile%252C%2520the%2520design%2520space%2520of%2520available%2520linear%2520operations%2520on%250Atensors%2520that%2520preserve%2520symmetry%2520presents%2520a%2520significant%2520challenge.%2520The%2520ICT%250Adecomposition%2520and%2520a%2520basis%2520of%2520this%2520equivariant%2520space%2520are%2520difficult%2520to%2520obtain%2520for%250Ahigh-rank%2520tensors.%2520After%2520decades%2520of%2520research%252C%2520Bonvicini%2520%25282024%2529%2520recently%250Aachieves%2520an%2520explicit%2520ICT%2520decomposition%2520for%2520%2524n%253D5%2524%2520with%2520factorial%2520time/space%250Acomplexity.%2520In%2520this%2520work%2520we%252C%2520for%2520the%2520first%2520time%252C%2520obtains%2520decomposition%2520matrices%250Afor%2520ICTs%2520up%2520to%2520rank%2520%2524n%253D9%2524%2520with%2520reduced%2520and%2520affordable%2520complexity%252C%2520by%250Aconstructing%2520what%2520we%2520call%2520path%2520matrices.%2520The%2520path%2520matrices%2520are%2520obtained%2520via%250Aperforming%2520chain-like%2520contractions%2520with%2520Clebsch-Gordan%2520matrices%2520following%2520the%250Aparentage%2520scheme.%2520We%2520prove%2520and%2520leverage%2520that%2520the%2520concatenation%2520of%2520path%2520matrices%250Ais%2520an%2520orthonormal%2520change-of-basis%2520matrix%2520between%2520the%2520Cartesian%2520tensor%2520product%250Aspace%2520and%2520the%2520spherical%2520direct%2520sum%2520spaces.%2520Furthermore%252C%2520we%2520identify%2520a%2520complete%250Aorthogonal%2520basis%2520for%2520the%2520equivariant%2520space%252C%2520rather%2520than%2520a%2520spanning%2520set%250A%2528Pearce-Crump%252C%25202023%2529%252C%2520through%2520this%2520path%2520matrices%2520technique.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520this%2520is%2520also%2520the%2520first%2520analytic%252C%2520rather%2520than%2520numerical%252C%2520method%2520for%250Atheoretically%2520obtaining%2520arbitrary%2520rank%2520orthogonal%2520ICT%2520decomposition%2520matrices%250Aand%2520orthogonal%2520equivariant%2520bases.%2520We%2520further%2520extend%2520our%2520result%2520to%2520the%2520arbitrary%250Atensor%2520product%2520and%2520direct%2520sum%2520spaces%252C%2520enabling%2520free%2520design%2520between%2520different%250Aspaces%2520while%2520keeping%2520symmetry.%2520The%2520Python%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/ShihaoShao-GH/ICT-decomposition-and-equivariant-bases%252C%2520where%250Athe%2520%2524n%253D6%252C%255Cdots%252C9%2524%2520ICT%2520decomposition%2520matrices%2520are%2520obtained%2520in%25201s%252C%25203s%252C%252011s%252C%2520and%250A4m32s%2520on%252028-cores%2520Intel%2528R%2529%2520Xeon%2528R%2529%2520Gold%25206330%2520CPU%2520%2540%25202.00GHz%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18263v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Rank%20Irreducible%20Cartesian%20Tensor%20Decomposition%20and%20Bases%20of%0A%20%20Equivariant%20Spaces&entry.906535625=Shihao%20Shao%20and%20Yikang%20Li%20and%20Zhouchen%20Lin%20and%20Qinghua%20Cui&entry.1292438233=%20%20Irreducible%20Cartesian%20tensors%20%28ICTs%29%20play%20a%20crucial%20role%20in%20the%20design%20of%0Aequivariant%20graph%20neural%20networks%2C%20as%20well%20as%20in%20theoretical%20chemistry%20and%0Achemical%20physics.%20Meanwhile%2C%20the%20design%20space%20of%20available%20linear%20operations%20on%0Atensors%20that%20preserve%20symmetry%20presents%20a%20significant%20challenge.%20The%20ICT%0Adecomposition%20and%20a%20basis%20of%20this%20equivariant%20space%20are%20difficult%20to%20obtain%20for%0Ahigh-rank%20tensors.%20After%20decades%20of%20research%2C%20Bonvicini%20%282024%29%20recently%0Aachieves%20an%20explicit%20ICT%20decomposition%20for%20%24n%3D5%24%20with%20factorial%20time/space%0Acomplexity.%20In%20this%20work%20we%2C%20for%20the%20first%20time%2C%20obtains%20decomposition%20matrices%0Afor%20ICTs%20up%20to%20rank%20%24n%3D9%24%20with%20reduced%20and%20affordable%20complexity%2C%20by%0Aconstructing%20what%20we%20call%20path%20matrices.%20The%20path%20matrices%20are%20obtained%20via%0Aperforming%20chain-like%20contractions%20with%20Clebsch-Gordan%20matrices%20following%20the%0Aparentage%20scheme.%20We%20prove%20and%20leverage%20that%20the%20concatenation%20of%20path%20matrices%0Ais%20an%20orthonormal%20change-of-basis%20matrix%20between%20the%20Cartesian%20tensor%20product%0Aspace%20and%20the%20spherical%20direct%20sum%20spaces.%20Furthermore%2C%20we%20identify%20a%20complete%0Aorthogonal%20basis%20for%20the%20equivariant%20space%2C%20rather%20than%20a%20spanning%20set%0A%28Pearce-Crump%2C%202023%29%2C%20through%20this%20path%20matrices%20technique.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20also%20the%20first%20analytic%2C%20rather%20than%20numerical%2C%20method%20for%0Atheoretically%20obtaining%20arbitrary%20rank%20orthogonal%20ICT%20decomposition%20matrices%0Aand%20orthogonal%20equivariant%20bases.%20We%20further%20extend%20our%20result%20to%20the%20arbitrary%0Atensor%20product%20and%20direct%20sum%20spaces%2C%20enabling%20free%20design%20between%20different%0Aspaces%20while%20keeping%20symmetry.%20The%20Python%20code%20is%20available%20at%0Ahttps%3A//github.com/ShihaoShao-GH/ICT-decomposition-and-equivariant-bases%2C%20where%0Athe%20%24n%3D6%2C%5Cdots%2C9%24%20ICT%20decomposition%20matrices%20are%20obtained%20in%201s%2C%203s%2C%2011s%2C%20and%0A4m32s%20on%2028-cores%20Intel%28R%29%20Xeon%28R%29%20Gold%206330%20CPU%20%40%202.00GHz%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18263v4&entry.124074799=Read"},
{"title": "BILTS: A Bi-Invariant Similarity Measure for Robust Object Trajectory\n  Recognition under Reference Frame Variations", "author": "Arno Verduyn and Erwin Aertbeli\u00ebn and Glenn Maes and Joris De Schutter and Maxim Vochten", "abstract": "  When similar object motions are performed in diverse contexts but are meant\nto be recognized under a single classification, these contextual variations act\nas disturbances that negatively affect accurate motion recognition. In this\npaper, we focus on contextual variations caused by reference frame variations.\nTo robustly deal with these variations, similarity measures have been\nintroduced that compare object motion trajectories in a context-invariant\nmanner. However, most are highly sensitive to noise near singularities, where\nthe measure is not uniquely defined, and lack bi-invariance (invariance to both\nworld and body frame variations). To address these issues, we propose the novel\n\\textit{Bi-Invariant Local Trajectory-Shape Similarity} (BILTS) measure.\nCompared to other measures, the BILTS measure uniquely offers bi-invariance,\nboundedness, and third-order shape identity. Aimed at practical\nimplementations, we devised a discretized and regularized version of the BILTS\nmeasure which shows exceptional robustness to singularities. This is\ndemonstrated through rigorous recognition experiments using multiple datasets.\nOn average, BILTS attained the highest recognition ratio and least sensitivity\nto contextual variations compared to other invariant object motion similarity\nmeasures. We believe that the BILTS measure is a valuable tool for recognizing\nmotions performed in diverse contexts and has potential in other applications,\nincluding the recognition, segmentation, and adaptation of both motion and\nforce trajectories.\n", "link": "http://arxiv.org/abs/2405.04392v2", "date": "2025-01-17", "relevancy": 2.0334, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5462}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5008}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BILTS%3A%20A%20Bi-Invariant%20Similarity%20Measure%20for%20Robust%20Object%20Trajectory%0A%20%20Recognition%20under%20Reference%20Frame%20Variations&body=Title%3A%20BILTS%3A%20A%20Bi-Invariant%20Similarity%20Measure%20for%20Robust%20Object%20Trajectory%0A%20%20Recognition%20under%20Reference%20Frame%20Variations%0AAuthor%3A%20Arno%20Verduyn%20and%20Erwin%20Aertbeli%C3%ABn%20and%20Glenn%20Maes%20and%20Joris%20De%20Schutter%20and%20Maxim%20Vochten%0AAbstract%3A%20%20%20When%20similar%20object%20motions%20are%20performed%20in%20diverse%20contexts%20but%20are%20meant%0Ato%20be%20recognized%20under%20a%20single%20classification%2C%20these%20contextual%20variations%20act%0Aas%20disturbances%20that%20negatively%20affect%20accurate%20motion%20recognition.%20In%20this%0Apaper%2C%20we%20focus%20on%20contextual%20variations%20caused%20by%20reference%20frame%20variations.%0ATo%20robustly%20deal%20with%20these%20variations%2C%20similarity%20measures%20have%20been%0Aintroduced%20that%20compare%20object%20motion%20trajectories%20in%20a%20context-invariant%0Amanner.%20However%2C%20most%20are%20highly%20sensitive%20to%20noise%20near%20singularities%2C%20where%0Athe%20measure%20is%20not%20uniquely%20defined%2C%20and%20lack%20bi-invariance%20%28invariance%20to%20both%0Aworld%20and%20body%20frame%20variations%29.%20To%20address%20these%20issues%2C%20we%20propose%20the%20novel%0A%5Ctextit%7BBi-Invariant%20Local%20Trajectory-Shape%20Similarity%7D%20%28BILTS%29%20measure.%0ACompared%20to%20other%20measures%2C%20the%20BILTS%20measure%20uniquely%20offers%20bi-invariance%2C%0Aboundedness%2C%20and%20third-order%20shape%20identity.%20Aimed%20at%20practical%0Aimplementations%2C%20we%20devised%20a%20discretized%20and%20regularized%20version%20of%20the%20BILTS%0Ameasure%20which%20shows%20exceptional%20robustness%20to%20singularities.%20This%20is%0Ademonstrated%20through%20rigorous%20recognition%20experiments%20using%20multiple%20datasets.%0AOn%20average%2C%20BILTS%20attained%20the%20highest%20recognition%20ratio%20and%20least%20sensitivity%0Ato%20contextual%20variations%20compared%20to%20other%20invariant%20object%20motion%20similarity%0Ameasures.%20We%20believe%20that%20the%20BILTS%20measure%20is%20a%20valuable%20tool%20for%20recognizing%0Amotions%20performed%20in%20diverse%20contexts%20and%20has%20potential%20in%20other%20applications%2C%0Aincluding%20the%20recognition%2C%20segmentation%2C%20and%20adaptation%20of%20both%20motion%20and%0Aforce%20trajectories.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04392v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBILTS%253A%2520A%2520Bi-Invariant%2520Similarity%2520Measure%2520for%2520Robust%2520Object%2520Trajectory%250A%2520%2520Recognition%2520under%2520Reference%2520Frame%2520Variations%26entry.906535625%3DArno%2520Verduyn%2520and%2520Erwin%2520Aertbeli%25C3%25ABn%2520and%2520Glenn%2520Maes%2520and%2520Joris%2520De%2520Schutter%2520and%2520Maxim%2520Vochten%26entry.1292438233%3D%2520%2520When%2520similar%2520object%2520motions%2520are%2520performed%2520in%2520diverse%2520contexts%2520but%2520are%2520meant%250Ato%2520be%2520recognized%2520under%2520a%2520single%2520classification%252C%2520these%2520contextual%2520variations%2520act%250Aas%2520disturbances%2520that%2520negatively%2520affect%2520accurate%2520motion%2520recognition.%2520In%2520this%250Apaper%252C%2520we%2520focus%2520on%2520contextual%2520variations%2520caused%2520by%2520reference%2520frame%2520variations.%250ATo%2520robustly%2520deal%2520with%2520these%2520variations%252C%2520similarity%2520measures%2520have%2520been%250Aintroduced%2520that%2520compare%2520object%2520motion%2520trajectories%2520in%2520a%2520context-invariant%250Amanner.%2520However%252C%2520most%2520are%2520highly%2520sensitive%2520to%2520noise%2520near%2520singularities%252C%2520where%250Athe%2520measure%2520is%2520not%2520uniquely%2520defined%252C%2520and%2520lack%2520bi-invariance%2520%2528invariance%2520to%2520both%250Aworld%2520and%2520body%2520frame%2520variations%2529.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520the%2520novel%250A%255Ctextit%257BBi-Invariant%2520Local%2520Trajectory-Shape%2520Similarity%257D%2520%2528BILTS%2529%2520measure.%250ACompared%2520to%2520other%2520measures%252C%2520the%2520BILTS%2520measure%2520uniquely%2520offers%2520bi-invariance%252C%250Aboundedness%252C%2520and%2520third-order%2520shape%2520identity.%2520Aimed%2520at%2520practical%250Aimplementations%252C%2520we%2520devised%2520a%2520discretized%2520and%2520regularized%2520version%2520of%2520the%2520BILTS%250Ameasure%2520which%2520shows%2520exceptional%2520robustness%2520to%2520singularities.%2520This%2520is%250Ademonstrated%2520through%2520rigorous%2520recognition%2520experiments%2520using%2520multiple%2520datasets.%250AOn%2520average%252C%2520BILTS%2520attained%2520the%2520highest%2520recognition%2520ratio%2520and%2520least%2520sensitivity%250Ato%2520contextual%2520variations%2520compared%2520to%2520other%2520invariant%2520object%2520motion%2520similarity%250Ameasures.%2520We%2520believe%2520that%2520the%2520BILTS%2520measure%2520is%2520a%2520valuable%2520tool%2520for%2520recognizing%250Amotions%2520performed%2520in%2520diverse%2520contexts%2520and%2520has%2520potential%2520in%2520other%2520applications%252C%250Aincluding%2520the%2520recognition%252C%2520segmentation%252C%2520and%2520adaptation%2520of%2520both%2520motion%2520and%250Aforce%2520trajectories.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04392v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BILTS%3A%20A%20Bi-Invariant%20Similarity%20Measure%20for%20Robust%20Object%20Trajectory%0A%20%20Recognition%20under%20Reference%20Frame%20Variations&entry.906535625=Arno%20Verduyn%20and%20Erwin%20Aertbeli%C3%ABn%20and%20Glenn%20Maes%20and%20Joris%20De%20Schutter%20and%20Maxim%20Vochten&entry.1292438233=%20%20When%20similar%20object%20motions%20are%20performed%20in%20diverse%20contexts%20but%20are%20meant%0Ato%20be%20recognized%20under%20a%20single%20classification%2C%20these%20contextual%20variations%20act%0Aas%20disturbances%20that%20negatively%20affect%20accurate%20motion%20recognition.%20In%20this%0Apaper%2C%20we%20focus%20on%20contextual%20variations%20caused%20by%20reference%20frame%20variations.%0ATo%20robustly%20deal%20with%20these%20variations%2C%20similarity%20measures%20have%20been%0Aintroduced%20that%20compare%20object%20motion%20trajectories%20in%20a%20context-invariant%0Amanner.%20However%2C%20most%20are%20highly%20sensitive%20to%20noise%20near%20singularities%2C%20where%0Athe%20measure%20is%20not%20uniquely%20defined%2C%20and%20lack%20bi-invariance%20%28invariance%20to%20both%0Aworld%20and%20body%20frame%20variations%29.%20To%20address%20these%20issues%2C%20we%20propose%20the%20novel%0A%5Ctextit%7BBi-Invariant%20Local%20Trajectory-Shape%20Similarity%7D%20%28BILTS%29%20measure.%0ACompared%20to%20other%20measures%2C%20the%20BILTS%20measure%20uniquely%20offers%20bi-invariance%2C%0Aboundedness%2C%20and%20third-order%20shape%20identity.%20Aimed%20at%20practical%0Aimplementations%2C%20we%20devised%20a%20discretized%20and%20regularized%20version%20of%20the%20BILTS%0Ameasure%20which%20shows%20exceptional%20robustness%20to%20singularities.%20This%20is%0Ademonstrated%20through%20rigorous%20recognition%20experiments%20using%20multiple%20datasets.%0AOn%20average%2C%20BILTS%20attained%20the%20highest%20recognition%20ratio%20and%20least%20sensitivity%0Ato%20contextual%20variations%20compared%20to%20other%20invariant%20object%20motion%20similarity%0Ameasures.%20We%20believe%20that%20the%20BILTS%20measure%20is%20a%20valuable%20tool%20for%20recognizing%0Amotions%20performed%20in%20diverse%20contexts%20and%20has%20potential%20in%20other%20applications%2C%0Aincluding%20the%20recognition%2C%20segmentation%2C%20and%20adaptation%20of%20both%20motion%20and%0Aforce%20trajectories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04392v2&entry.124074799=Read"},
{"title": "Neuradicon: operational representation learning of neuroimaging reports", "author": "Henry Watkins and Robert Gray and Adam Julius and Yee-Haur Mah and Walter H. L. Pinaya and Paul Wright and Ashwani Jha and Holger Engleitner and Jorge Cardoso and Sebastien Ourselin and Geraint Rees and Rolf Jaeger and Parashkev Nachev", "abstract": "  Radiological reports typically summarize the content and interpretation of\nimaging studies in unstructured form that precludes quantitative analysis. This\nlimits the monitoring of radiological services to throughput undifferentiated\nby content, impeding specific, targeted operational optimization. Here we\npresent Neuradicon, a natural language processing (NLP) framework for\nquantitative analysis of neuroradiological reports. Our framework is a hybrid\nof rule-based and artificial intelligence models to represent neurological\nreports in succinct, quantitative form optimally suited to operational\nguidance. We demonstrate the application of Neuradicon to operational\nphenotyping of a corpus of 336,569 reports, and report excellent\ngeneralizability across time and two independent healthcare institutions.\n", "link": "http://arxiv.org/abs/2107.10021v3", "date": "2025-01-17", "relevancy": 2.0268, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.515}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.515}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neuradicon%3A%20operational%20representation%20learning%20of%20neuroimaging%20reports&body=Title%3A%20Neuradicon%3A%20operational%20representation%20learning%20of%20neuroimaging%20reports%0AAuthor%3A%20Henry%20Watkins%20and%20Robert%20Gray%20and%20Adam%20Julius%20and%20Yee-Haur%20Mah%20and%20Walter%20H.%20L.%20Pinaya%20and%20Paul%20Wright%20and%20Ashwani%20Jha%20and%20Holger%20Engleitner%20and%20Jorge%20Cardoso%20and%20Sebastien%20Ourselin%20and%20Geraint%20Rees%20and%20Rolf%20Jaeger%20and%20Parashkev%20Nachev%0AAbstract%3A%20%20%20Radiological%20reports%20typically%20summarize%20the%20content%20and%20interpretation%20of%0Aimaging%20studies%20in%20unstructured%20form%20that%20precludes%20quantitative%20analysis.%20This%0Alimits%20the%20monitoring%20of%20radiological%20services%20to%20throughput%20undifferentiated%0Aby%20content%2C%20impeding%20specific%2C%20targeted%20operational%20optimization.%20Here%20we%0Apresent%20Neuradicon%2C%20a%20natural%20language%20processing%20%28NLP%29%20framework%20for%0Aquantitative%20analysis%20of%20neuroradiological%20reports.%20Our%20framework%20is%20a%20hybrid%0Aof%20rule-based%20and%20artificial%20intelligence%20models%20to%20represent%20neurological%0Areports%20in%20succinct%2C%20quantitative%20form%20optimally%20suited%20to%20operational%0Aguidance.%20We%20demonstrate%20the%20application%20of%20Neuradicon%20to%20operational%0Aphenotyping%20of%20a%20corpus%20of%20336%2C569%20reports%2C%20and%20report%20excellent%0Ageneralizability%20across%20time%20and%20two%20independent%20healthcare%20institutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2107.10021v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuradicon%253A%2520operational%2520representation%2520learning%2520of%2520neuroimaging%2520reports%26entry.906535625%3DHenry%2520Watkins%2520and%2520Robert%2520Gray%2520and%2520Adam%2520Julius%2520and%2520Yee-Haur%2520Mah%2520and%2520Walter%2520H.%2520L.%2520Pinaya%2520and%2520Paul%2520Wright%2520and%2520Ashwani%2520Jha%2520and%2520Holger%2520Engleitner%2520and%2520Jorge%2520Cardoso%2520and%2520Sebastien%2520Ourselin%2520and%2520Geraint%2520Rees%2520and%2520Rolf%2520Jaeger%2520and%2520Parashkev%2520Nachev%26entry.1292438233%3D%2520%2520Radiological%2520reports%2520typically%2520summarize%2520the%2520content%2520and%2520interpretation%2520of%250Aimaging%2520studies%2520in%2520unstructured%2520form%2520that%2520precludes%2520quantitative%2520analysis.%2520This%250Alimits%2520the%2520monitoring%2520of%2520radiological%2520services%2520to%2520throughput%2520undifferentiated%250Aby%2520content%252C%2520impeding%2520specific%252C%2520targeted%2520operational%2520optimization.%2520Here%2520we%250Apresent%2520Neuradicon%252C%2520a%2520natural%2520language%2520processing%2520%2528NLP%2529%2520framework%2520for%250Aquantitative%2520analysis%2520of%2520neuroradiological%2520reports.%2520Our%2520framework%2520is%2520a%2520hybrid%250Aof%2520rule-based%2520and%2520artificial%2520intelligence%2520models%2520to%2520represent%2520neurological%250Areports%2520in%2520succinct%252C%2520quantitative%2520form%2520optimally%2520suited%2520to%2520operational%250Aguidance.%2520We%2520demonstrate%2520the%2520application%2520of%2520Neuradicon%2520to%2520operational%250Aphenotyping%2520of%2520a%2520corpus%2520of%2520336%252C569%2520reports%252C%2520and%2520report%2520excellent%250Ageneralizability%2520across%2520time%2520and%2520two%2520independent%2520healthcare%2520institutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2107.10021v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neuradicon%3A%20operational%20representation%20learning%20of%20neuroimaging%20reports&entry.906535625=Henry%20Watkins%20and%20Robert%20Gray%20and%20Adam%20Julius%20and%20Yee-Haur%20Mah%20and%20Walter%20H.%20L.%20Pinaya%20and%20Paul%20Wright%20and%20Ashwani%20Jha%20and%20Holger%20Engleitner%20and%20Jorge%20Cardoso%20and%20Sebastien%20Ourselin%20and%20Geraint%20Rees%20and%20Rolf%20Jaeger%20and%20Parashkev%20Nachev&entry.1292438233=%20%20Radiological%20reports%20typically%20summarize%20the%20content%20and%20interpretation%20of%0Aimaging%20studies%20in%20unstructured%20form%20that%20precludes%20quantitative%20analysis.%20This%0Alimits%20the%20monitoring%20of%20radiological%20services%20to%20throughput%20undifferentiated%0Aby%20content%2C%20impeding%20specific%2C%20targeted%20operational%20optimization.%20Here%20we%0Apresent%20Neuradicon%2C%20a%20natural%20language%20processing%20%28NLP%29%20framework%20for%0Aquantitative%20analysis%20of%20neuroradiological%20reports.%20Our%20framework%20is%20a%20hybrid%0Aof%20rule-based%20and%20artificial%20intelligence%20models%20to%20represent%20neurological%0Areports%20in%20succinct%2C%20quantitative%20form%20optimally%20suited%20to%20operational%0Aguidance.%20We%20demonstrate%20the%20application%20of%20Neuradicon%20to%20operational%0Aphenotyping%20of%20a%20corpus%20of%20336%2C569%20reports%2C%20and%20report%20excellent%0Ageneralizability%20across%20time%20and%20two%20independent%20healthcare%20institutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2107.10021v3&entry.124074799=Read"},
{"title": "Provably Safeguarding a Classifier from OOD and Adversarial Samples: an\n  Extreme Value Theory Approach", "author": "Nicolas Atienza and Christophe Labreuche and Johanne Cohen and Michele Sebag", "abstract": "  This paper introduces a novel method, Sample-efficient Probabilistic\nDetection using Extreme Value Theory (SPADE), which transforms a classifier\ninto an abstaining classifier, offering provable protection against\nout-of-distribution and adversarial samples. The approach is based on a\nGeneralized Extreme Value (GEV) model of the training distribution in the\nclassifier's latent space, enabling the formal characterization of OOD samples.\nInterestingly, under mild assumptions, the GEV model also allows for formally\ncharacterizing adversarial samples. The abstaining classifier, which rejects\nsamples based on their assessment by the GEV model, provably avoids OOD and\nadversarial samples. The empirical validation of the approach, conducted on\nvarious neural architectures (ResNet, VGG, and Vision Transformer) and medium\nand large-sized datasets (CIFAR-10, CIFAR-100, and ImageNet), demonstrates its\nfrugality, stability, and efficiency compared to the state of the art.\n", "link": "http://arxiv.org/abs/2501.10202v1", "date": "2025-01-17", "relevancy": 2.0114, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.555}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4699}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4639}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Provably%20Safeguarding%20a%20Classifier%20from%20OOD%20and%20Adversarial%20Samples%3A%20an%0A%20%20Extreme%20Value%20Theory%20Approach&body=Title%3A%20Provably%20Safeguarding%20a%20Classifier%20from%20OOD%20and%20Adversarial%20Samples%3A%20an%0A%20%20Extreme%20Value%20Theory%20Approach%0AAuthor%3A%20Nicolas%20Atienza%20and%20Christophe%20Labreuche%20and%20Johanne%20Cohen%20and%20Michele%20Sebag%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20method%2C%20Sample-efficient%20Probabilistic%0ADetection%20using%20Extreme%20Value%20Theory%20%28SPADE%29%2C%20which%20transforms%20a%20classifier%0Ainto%20an%20abstaining%20classifier%2C%20offering%20provable%20protection%20against%0Aout-of-distribution%20and%20adversarial%20samples.%20The%20approach%20is%20based%20on%20a%0AGeneralized%20Extreme%20Value%20%28GEV%29%20model%20of%20the%20training%20distribution%20in%20the%0Aclassifier%27s%20latent%20space%2C%20enabling%20the%20formal%20characterization%20of%20OOD%20samples.%0AInterestingly%2C%20under%20mild%20assumptions%2C%20the%20GEV%20model%20also%20allows%20for%20formally%0Acharacterizing%20adversarial%20samples.%20The%20abstaining%20classifier%2C%20which%20rejects%0Asamples%20based%20on%20their%20assessment%20by%20the%20GEV%20model%2C%20provably%20avoids%20OOD%20and%0Aadversarial%20samples.%20The%20empirical%20validation%20of%20the%20approach%2C%20conducted%20on%0Avarious%20neural%20architectures%20%28ResNet%2C%20VGG%2C%20and%20Vision%20Transformer%29%20and%20medium%0Aand%20large-sized%20datasets%20%28CIFAR-10%2C%20CIFAR-100%2C%20and%20ImageNet%29%2C%20demonstrates%20its%0Afrugality%2C%20stability%2C%20and%20efficiency%20compared%20to%20the%20state%20of%20the%20art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10202v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProvably%2520Safeguarding%2520a%2520Classifier%2520from%2520OOD%2520and%2520Adversarial%2520Samples%253A%2520an%250A%2520%2520Extreme%2520Value%2520Theory%2520Approach%26entry.906535625%3DNicolas%2520Atienza%2520and%2520Christophe%2520Labreuche%2520and%2520Johanne%2520Cohen%2520and%2520Michele%2520Sebag%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520method%252C%2520Sample-efficient%2520Probabilistic%250ADetection%2520using%2520Extreme%2520Value%2520Theory%2520%2528SPADE%2529%252C%2520which%2520transforms%2520a%2520classifier%250Ainto%2520an%2520abstaining%2520classifier%252C%2520offering%2520provable%2520protection%2520against%250Aout-of-distribution%2520and%2520adversarial%2520samples.%2520The%2520approach%2520is%2520based%2520on%2520a%250AGeneralized%2520Extreme%2520Value%2520%2528GEV%2529%2520model%2520of%2520the%2520training%2520distribution%2520in%2520the%250Aclassifier%2527s%2520latent%2520space%252C%2520enabling%2520the%2520formal%2520characterization%2520of%2520OOD%2520samples.%250AInterestingly%252C%2520under%2520mild%2520assumptions%252C%2520the%2520GEV%2520model%2520also%2520allows%2520for%2520formally%250Acharacterizing%2520adversarial%2520samples.%2520The%2520abstaining%2520classifier%252C%2520which%2520rejects%250Asamples%2520based%2520on%2520their%2520assessment%2520by%2520the%2520GEV%2520model%252C%2520provably%2520avoids%2520OOD%2520and%250Aadversarial%2520samples.%2520The%2520empirical%2520validation%2520of%2520the%2520approach%252C%2520conducted%2520on%250Avarious%2520neural%2520architectures%2520%2528ResNet%252C%2520VGG%252C%2520and%2520Vision%2520Transformer%2529%2520and%2520medium%250Aand%2520large-sized%2520datasets%2520%2528CIFAR-10%252C%2520CIFAR-100%252C%2520and%2520ImageNet%2529%252C%2520demonstrates%2520its%250Afrugality%252C%2520stability%252C%2520and%2520efficiency%2520compared%2520to%2520the%2520state%2520of%2520the%2520art.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10202v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Provably%20Safeguarding%20a%20Classifier%20from%20OOD%20and%20Adversarial%20Samples%3A%20an%0A%20%20Extreme%20Value%20Theory%20Approach&entry.906535625=Nicolas%20Atienza%20and%20Christophe%20Labreuche%20and%20Johanne%20Cohen%20and%20Michele%20Sebag&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20method%2C%20Sample-efficient%20Probabilistic%0ADetection%20using%20Extreme%20Value%20Theory%20%28SPADE%29%2C%20which%20transforms%20a%20classifier%0Ainto%20an%20abstaining%20classifier%2C%20offering%20provable%20protection%20against%0Aout-of-distribution%20and%20adversarial%20samples.%20The%20approach%20is%20based%20on%20a%0AGeneralized%20Extreme%20Value%20%28GEV%29%20model%20of%20the%20training%20distribution%20in%20the%0Aclassifier%27s%20latent%20space%2C%20enabling%20the%20formal%20characterization%20of%20OOD%20samples.%0AInterestingly%2C%20under%20mild%20assumptions%2C%20the%20GEV%20model%20also%20allows%20for%20formally%0Acharacterizing%20adversarial%20samples.%20The%20abstaining%20classifier%2C%20which%20rejects%0Asamples%20based%20on%20their%20assessment%20by%20the%20GEV%20model%2C%20provably%20avoids%20OOD%20and%0Aadversarial%20samples.%20The%20empirical%20validation%20of%20the%20approach%2C%20conducted%20on%0Avarious%20neural%20architectures%20%28ResNet%2C%20VGG%2C%20and%20Vision%20Transformer%29%20and%20medium%0Aand%20large-sized%20datasets%20%28CIFAR-10%2C%20CIFAR-100%2C%20and%20ImageNet%29%2C%20demonstrates%20its%0Afrugality%2C%20stability%2C%20and%20efficiency%20compared%20to%20the%20state%20of%20the%20art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10202v1&entry.124074799=Read"},
{"title": "Optimal Virtual Model Control for Robotics: Design and Tuning of\n  Passivity-Based Controllers", "author": "Daniel Larby and Fulvio Forni", "abstract": "  Passivity-based control is a cornerstone of control theory and an established\ndesign approach in robotics. Its strength is based on the passivity theorem,\nwhich provides a powerful interconnection framework for robotics. However, the\ndesign of passivity-based controllers and their optimal tuning remain\nchallenging. We propose here an intuitive design approach for fully actuated\nrobots, where the control action is determined by a `virtual-mechanism' as in\nclassical virtual model control. The result is a robot whose controlled\nbehavior can be understood in terms of physics. We achieve optimal tuning by\napplying algorithmic differentiation to ODE simulations of the rigid body\ndynamics. Overall, this leads to a flexible design and optimization approach:\nstability is proven by passivity of the virtual mechanism, while performance is\nobtained by optimization using algorithmic differentiation.\n", "link": "http://arxiv.org/abs/2411.06627v2", "date": "2025-01-17", "relevancy": 2.0031, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5211}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5207}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Virtual%20Model%20Control%20for%20Robotics%3A%20Design%20and%20Tuning%20of%0A%20%20Passivity-Based%20Controllers&body=Title%3A%20Optimal%20Virtual%20Model%20Control%20for%20Robotics%3A%20Design%20and%20Tuning%20of%0A%20%20Passivity-Based%20Controllers%0AAuthor%3A%20Daniel%20Larby%20and%20Fulvio%20Forni%0AAbstract%3A%20%20%20Passivity-based%20control%20is%20a%20cornerstone%20of%20control%20theory%20and%20an%20established%0Adesign%20approach%20in%20robotics.%20Its%20strength%20is%20based%20on%20the%20passivity%20theorem%2C%0Awhich%20provides%20a%20powerful%20interconnection%20framework%20for%20robotics.%20However%2C%20the%0Adesign%20of%20passivity-based%20controllers%20and%20their%20optimal%20tuning%20remain%0Achallenging.%20We%20propose%20here%20an%20intuitive%20design%20approach%20for%20fully%20actuated%0Arobots%2C%20where%20the%20control%20action%20is%20determined%20by%20a%20%60virtual-mechanism%27%20as%20in%0Aclassical%20virtual%20model%20control.%20The%20result%20is%20a%20robot%20whose%20controlled%0Abehavior%20can%20be%20understood%20in%20terms%20of%20physics.%20We%20achieve%20optimal%20tuning%20by%0Aapplying%20algorithmic%20differentiation%20to%20ODE%20simulations%20of%20the%20rigid%20body%0Adynamics.%20Overall%2C%20this%20leads%20to%20a%20flexible%20design%20and%20optimization%20approach%3A%0Astability%20is%20proven%20by%20passivity%20of%20the%20virtual%20mechanism%2C%20while%20performance%20is%0Aobtained%20by%20optimization%20using%20algorithmic%20differentiation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06627v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Virtual%2520Model%2520Control%2520for%2520Robotics%253A%2520Design%2520and%2520Tuning%2520of%250A%2520%2520Passivity-Based%2520Controllers%26entry.906535625%3DDaniel%2520Larby%2520and%2520Fulvio%2520Forni%26entry.1292438233%3D%2520%2520Passivity-based%2520control%2520is%2520a%2520cornerstone%2520of%2520control%2520theory%2520and%2520an%2520established%250Adesign%2520approach%2520in%2520robotics.%2520Its%2520strength%2520is%2520based%2520on%2520the%2520passivity%2520theorem%252C%250Awhich%2520provides%2520a%2520powerful%2520interconnection%2520framework%2520for%2520robotics.%2520However%252C%2520the%250Adesign%2520of%2520passivity-based%2520controllers%2520and%2520their%2520optimal%2520tuning%2520remain%250Achallenging.%2520We%2520propose%2520here%2520an%2520intuitive%2520design%2520approach%2520for%2520fully%2520actuated%250Arobots%252C%2520where%2520the%2520control%2520action%2520is%2520determined%2520by%2520a%2520%2560virtual-mechanism%2527%2520as%2520in%250Aclassical%2520virtual%2520model%2520control.%2520The%2520result%2520is%2520a%2520robot%2520whose%2520controlled%250Abehavior%2520can%2520be%2520understood%2520in%2520terms%2520of%2520physics.%2520We%2520achieve%2520optimal%2520tuning%2520by%250Aapplying%2520algorithmic%2520differentiation%2520to%2520ODE%2520simulations%2520of%2520the%2520rigid%2520body%250Adynamics.%2520Overall%252C%2520this%2520leads%2520to%2520a%2520flexible%2520design%2520and%2520optimization%2520approach%253A%250Astability%2520is%2520proven%2520by%2520passivity%2520of%2520the%2520virtual%2520mechanism%252C%2520while%2520performance%2520is%250Aobtained%2520by%2520optimization%2520using%2520algorithmic%2520differentiation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06627v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Virtual%20Model%20Control%20for%20Robotics%3A%20Design%20and%20Tuning%20of%0A%20%20Passivity-Based%20Controllers&entry.906535625=Daniel%20Larby%20and%20Fulvio%20Forni&entry.1292438233=%20%20Passivity-based%20control%20is%20a%20cornerstone%20of%20control%20theory%20and%20an%20established%0Adesign%20approach%20in%20robotics.%20Its%20strength%20is%20based%20on%20the%20passivity%20theorem%2C%0Awhich%20provides%20a%20powerful%20interconnection%20framework%20for%20robotics.%20However%2C%20the%0Adesign%20of%20passivity-based%20controllers%20and%20their%20optimal%20tuning%20remain%0Achallenging.%20We%20propose%20here%20an%20intuitive%20design%20approach%20for%20fully%20actuated%0Arobots%2C%20where%20the%20control%20action%20is%20determined%20by%20a%20%60virtual-mechanism%27%20as%20in%0Aclassical%20virtual%20model%20control.%20The%20result%20is%20a%20robot%20whose%20controlled%0Abehavior%20can%20be%20understood%20in%20terms%20of%20physics.%20We%20achieve%20optimal%20tuning%20by%0Aapplying%20algorithmic%20differentiation%20to%20ODE%20simulations%20of%20the%20rigid%20body%0Adynamics.%20Overall%2C%20this%20leads%20to%20a%20flexible%20design%20and%20optimization%20approach%3A%0Astability%20is%20proven%20by%20passivity%20of%20the%20virtual%20mechanism%2C%20while%20performance%20is%0Aobtained%20by%20optimization%20using%20algorithmic%20differentiation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06627v2&entry.124074799=Read"},
{"title": "DADA: Dual Averaging with Distance Adaptation", "author": "Mohammad Moshtaghifar and Anton Rodomanov and Daniil Vankov and Sebastian Stich", "abstract": "  We present a novel universal gradient method for solving convex optimization\nproblems. Our algorithm -- Dual Averaging with Distance Adaptation (DADA) -- is\nbased on the classical scheme of dual averaging and dynamically adjusts its\ncoefficients based on observed gradients and the distance between iterates and\nthe starting point, eliminating the need for problem-specific parameters. DADA\nis a universal algorithm that simultaneously works for a broad spectrum of\nproblem classes, provided the local growth of the objective function around its\nminimizer can be bounded. Particular examples of such problem classes are\nnonsmooth Lipschitz functions, Lipschitz-smooth functions, H\\\"older-smooth\nfunctions, functions with high-order Lipschitz derivative,\nquasi-self-concordant functions, and $(L_0,L_1)$-smooth functions. Crucially,\nDADA is applicable to both unconstrained and constrained problems, even when\nthe domain is unbounded, without requiring prior knowledge of the number of\niterations or desired accuracy.\n", "link": "http://arxiv.org/abs/2501.10258v1", "date": "2025-01-17", "relevancy": 1.9953, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5056}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5008}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DADA%3A%20Dual%20Averaging%20with%20Distance%20Adaptation&body=Title%3A%20DADA%3A%20Dual%20Averaging%20with%20Distance%20Adaptation%0AAuthor%3A%20Mohammad%20Moshtaghifar%20and%20Anton%20Rodomanov%20and%20Daniil%20Vankov%20and%20Sebastian%20Stich%0AAbstract%3A%20%20%20We%20present%20a%20novel%20universal%20gradient%20method%20for%20solving%20convex%20optimization%0Aproblems.%20Our%20algorithm%20--%20Dual%20Averaging%20with%20Distance%20Adaptation%20%28DADA%29%20--%20is%0Abased%20on%20the%20classical%20scheme%20of%20dual%20averaging%20and%20dynamically%20adjusts%20its%0Acoefficients%20based%20on%20observed%20gradients%20and%20the%20distance%20between%20iterates%20and%0Athe%20starting%20point%2C%20eliminating%20the%20need%20for%20problem-specific%20parameters.%20DADA%0Ais%20a%20universal%20algorithm%20that%20simultaneously%20works%20for%20a%20broad%20spectrum%20of%0Aproblem%20classes%2C%20provided%20the%20local%20growth%20of%20the%20objective%20function%20around%20its%0Aminimizer%20can%20be%20bounded.%20Particular%20examples%20of%20such%20problem%20classes%20are%0Anonsmooth%20Lipschitz%20functions%2C%20Lipschitz-smooth%20functions%2C%20H%5C%22older-smooth%0Afunctions%2C%20functions%20with%20high-order%20Lipschitz%20derivative%2C%0Aquasi-self-concordant%20functions%2C%20and%20%24%28L_0%2CL_1%29%24-smooth%20functions.%20Crucially%2C%0ADADA%20is%20applicable%20to%20both%20unconstrained%20and%20constrained%20problems%2C%20even%20when%0Athe%20domain%20is%20unbounded%2C%20without%20requiring%20prior%20knowledge%20of%20the%20number%20of%0Aiterations%20or%20desired%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10258v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDADA%253A%2520Dual%2520Averaging%2520with%2520Distance%2520Adaptation%26entry.906535625%3DMohammad%2520Moshtaghifar%2520and%2520Anton%2520Rodomanov%2520and%2520Daniil%2520Vankov%2520and%2520Sebastian%2520Stich%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520universal%2520gradient%2520method%2520for%2520solving%2520convex%2520optimization%250Aproblems.%2520Our%2520algorithm%2520--%2520Dual%2520Averaging%2520with%2520Distance%2520Adaptation%2520%2528DADA%2529%2520--%2520is%250Abased%2520on%2520the%2520classical%2520scheme%2520of%2520dual%2520averaging%2520and%2520dynamically%2520adjusts%2520its%250Acoefficients%2520based%2520on%2520observed%2520gradients%2520and%2520the%2520distance%2520between%2520iterates%2520and%250Athe%2520starting%2520point%252C%2520eliminating%2520the%2520need%2520for%2520problem-specific%2520parameters.%2520DADA%250Ais%2520a%2520universal%2520algorithm%2520that%2520simultaneously%2520works%2520for%2520a%2520broad%2520spectrum%2520of%250Aproblem%2520classes%252C%2520provided%2520the%2520local%2520growth%2520of%2520the%2520objective%2520function%2520around%2520its%250Aminimizer%2520can%2520be%2520bounded.%2520Particular%2520examples%2520of%2520such%2520problem%2520classes%2520are%250Anonsmooth%2520Lipschitz%2520functions%252C%2520Lipschitz-smooth%2520functions%252C%2520H%255C%2522older-smooth%250Afunctions%252C%2520functions%2520with%2520high-order%2520Lipschitz%2520derivative%252C%250Aquasi-self-concordant%2520functions%252C%2520and%2520%2524%2528L_0%252CL_1%2529%2524-smooth%2520functions.%2520Crucially%252C%250ADADA%2520is%2520applicable%2520to%2520both%2520unconstrained%2520and%2520constrained%2520problems%252C%2520even%2520when%250Athe%2520domain%2520is%2520unbounded%252C%2520without%2520requiring%2520prior%2520knowledge%2520of%2520the%2520number%2520of%250Aiterations%2520or%2520desired%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10258v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DADA%3A%20Dual%20Averaging%20with%20Distance%20Adaptation&entry.906535625=Mohammad%20Moshtaghifar%20and%20Anton%20Rodomanov%20and%20Daniil%20Vankov%20and%20Sebastian%20Stich&entry.1292438233=%20%20We%20present%20a%20novel%20universal%20gradient%20method%20for%20solving%20convex%20optimization%0Aproblems.%20Our%20algorithm%20--%20Dual%20Averaging%20with%20Distance%20Adaptation%20%28DADA%29%20--%20is%0Abased%20on%20the%20classical%20scheme%20of%20dual%20averaging%20and%20dynamically%20adjusts%20its%0Acoefficients%20based%20on%20observed%20gradients%20and%20the%20distance%20between%20iterates%20and%0Athe%20starting%20point%2C%20eliminating%20the%20need%20for%20problem-specific%20parameters.%20DADA%0Ais%20a%20universal%20algorithm%20that%20simultaneously%20works%20for%20a%20broad%20spectrum%20of%0Aproblem%20classes%2C%20provided%20the%20local%20growth%20of%20the%20objective%20function%20around%20its%0Aminimizer%20can%20be%20bounded.%20Particular%20examples%20of%20such%20problem%20classes%20are%0Anonsmooth%20Lipschitz%20functions%2C%20Lipschitz-smooth%20functions%2C%20H%5C%22older-smooth%0Afunctions%2C%20functions%20with%20high-order%20Lipschitz%20derivative%2C%0Aquasi-self-concordant%20functions%2C%20and%20%24%28L_0%2CL_1%29%24-smooth%20functions.%20Crucially%2C%0ADADA%20is%20applicable%20to%20both%20unconstrained%20and%20constrained%20problems%2C%20even%20when%0Athe%20domain%20is%20unbounded%2C%20without%20requiring%20prior%20knowledge%20of%20the%20number%20of%0Aiterations%20or%20desired%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10258v1&entry.124074799=Read"},
{"title": "Agent Hospital: A Simulacrum of Hospital with Evolvable Medical Agents", "author": "Junkai Li and Yunghwei Lai and Weitao Li and Jingyi Ren and Meng Zhang and Xinhui Kang and Siyu Wang and Peng Li and Ya-Qin Zhang and Weizhi Ma and Yang Liu", "abstract": "  The recent rapid development of large language models (LLMs) has sparked a\nnew wave of technological revolution in medical artificial intelligence (AI).\nWhile LLMs are designed to understand and generate text like a human,\nautonomous agents that utilize LLMs as their \"brain\" have exhibited\ncapabilities beyond text processing such as planning, reflection, and using\ntools by enabling their \"bodies\" to interact with the environment. We introduce\na simulacrum of hospital called Agent Hospital that simulates the entire\nprocess of treating illness, in which all patients, nurses, and doctors are\nLLM-powered autonomous agents. Within the simulacrum, doctor agents are able to\nevolve by treating a large number of patient agents without the need to label\ntraining data manually. After treating tens of thousands of patient agents in\nthe simulacrum (human doctors may take several years in the real world), the\nevolved doctor agents outperform state-of-the-art medical agent methods on the\nMedQA benchmark comprising US Medical Licensing Examination (USMLE) test\nquestions. Our methods of simulacrum construction and agent evolution have the\npotential in benefiting a broad range of applications beyond medical AI.\n", "link": "http://arxiv.org/abs/2405.02957v3", "date": "2025-01-17", "relevancy": 1.9887, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5135}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4876}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4801}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agent%20Hospital%3A%20A%20Simulacrum%20of%20Hospital%20with%20Evolvable%20Medical%20Agents&body=Title%3A%20Agent%20Hospital%3A%20A%20Simulacrum%20of%20Hospital%20with%20Evolvable%20Medical%20Agents%0AAuthor%3A%20Junkai%20Li%20and%20Yunghwei%20Lai%20and%20Weitao%20Li%20and%20Jingyi%20Ren%20and%20Meng%20Zhang%20and%20Xinhui%20Kang%20and%20Siyu%20Wang%20and%20Peng%20Li%20and%20Ya-Qin%20Zhang%20and%20Weizhi%20Ma%20and%20Yang%20Liu%0AAbstract%3A%20%20%20The%20recent%20rapid%20development%20of%20large%20language%20models%20%28LLMs%29%20has%20sparked%20a%0Anew%20wave%20of%20technological%20revolution%20in%20medical%20artificial%20intelligence%20%28AI%29.%0AWhile%20LLMs%20are%20designed%20to%20understand%20and%20generate%20text%20like%20a%20human%2C%0Aautonomous%20agents%20that%20utilize%20LLMs%20as%20their%20%22brain%22%20have%20exhibited%0Acapabilities%20beyond%20text%20processing%20such%20as%20planning%2C%20reflection%2C%20and%20using%0Atools%20by%20enabling%20their%20%22bodies%22%20to%20interact%20with%20the%20environment.%20We%20introduce%0Aa%20simulacrum%20of%20hospital%20called%20Agent%20Hospital%20that%20simulates%20the%20entire%0Aprocess%20of%20treating%20illness%2C%20in%20which%20all%20patients%2C%20nurses%2C%20and%20doctors%20are%0ALLM-powered%20autonomous%20agents.%20Within%20the%20simulacrum%2C%20doctor%20agents%20are%20able%20to%0Aevolve%20by%20treating%20a%20large%20number%20of%20patient%20agents%20without%20the%20need%20to%20label%0Atraining%20data%20manually.%20After%20treating%20tens%20of%20thousands%20of%20patient%20agents%20in%0Athe%20simulacrum%20%28human%20doctors%20may%20take%20several%20years%20in%20the%20real%20world%29%2C%20the%0Aevolved%20doctor%20agents%20outperform%20state-of-the-art%20medical%20agent%20methods%20on%20the%0AMedQA%20benchmark%20comprising%20US%20Medical%20Licensing%20Examination%20%28USMLE%29%20test%0Aquestions.%20Our%20methods%20of%20simulacrum%20construction%20and%20agent%20evolution%20have%20the%0Apotential%20in%20benefiting%20a%20broad%20range%20of%20applications%20beyond%20medical%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02957v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgent%2520Hospital%253A%2520A%2520Simulacrum%2520of%2520Hospital%2520with%2520Evolvable%2520Medical%2520Agents%26entry.906535625%3DJunkai%2520Li%2520and%2520Yunghwei%2520Lai%2520and%2520Weitao%2520Li%2520and%2520Jingyi%2520Ren%2520and%2520Meng%2520Zhang%2520and%2520Xinhui%2520Kang%2520and%2520Siyu%2520Wang%2520and%2520Peng%2520Li%2520and%2520Ya-Qin%2520Zhang%2520and%2520Weizhi%2520Ma%2520and%2520Yang%2520Liu%26entry.1292438233%3D%2520%2520The%2520recent%2520rapid%2520development%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520sparked%2520a%250Anew%2520wave%2520of%2520technological%2520revolution%2520in%2520medical%2520artificial%2520intelligence%2520%2528AI%2529.%250AWhile%2520LLMs%2520are%2520designed%2520to%2520understand%2520and%2520generate%2520text%2520like%2520a%2520human%252C%250Aautonomous%2520agents%2520that%2520utilize%2520LLMs%2520as%2520their%2520%2522brain%2522%2520have%2520exhibited%250Acapabilities%2520beyond%2520text%2520processing%2520such%2520as%2520planning%252C%2520reflection%252C%2520and%2520using%250Atools%2520by%2520enabling%2520their%2520%2522bodies%2522%2520to%2520interact%2520with%2520the%2520environment.%2520We%2520introduce%250Aa%2520simulacrum%2520of%2520hospital%2520called%2520Agent%2520Hospital%2520that%2520simulates%2520the%2520entire%250Aprocess%2520of%2520treating%2520illness%252C%2520in%2520which%2520all%2520patients%252C%2520nurses%252C%2520and%2520doctors%2520are%250ALLM-powered%2520autonomous%2520agents.%2520Within%2520the%2520simulacrum%252C%2520doctor%2520agents%2520are%2520able%2520to%250Aevolve%2520by%2520treating%2520a%2520large%2520number%2520of%2520patient%2520agents%2520without%2520the%2520need%2520to%2520label%250Atraining%2520data%2520manually.%2520After%2520treating%2520tens%2520of%2520thousands%2520of%2520patient%2520agents%2520in%250Athe%2520simulacrum%2520%2528human%2520doctors%2520may%2520take%2520several%2520years%2520in%2520the%2520real%2520world%2529%252C%2520the%250Aevolved%2520doctor%2520agents%2520outperform%2520state-of-the-art%2520medical%2520agent%2520methods%2520on%2520the%250AMedQA%2520benchmark%2520comprising%2520US%2520Medical%2520Licensing%2520Examination%2520%2528USMLE%2529%2520test%250Aquestions.%2520Our%2520methods%2520of%2520simulacrum%2520construction%2520and%2520agent%2520evolution%2520have%2520the%250Apotential%2520in%2520benefiting%2520a%2520broad%2520range%2520of%2520applications%2520beyond%2520medical%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02957v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agent%20Hospital%3A%20A%20Simulacrum%20of%20Hospital%20with%20Evolvable%20Medical%20Agents&entry.906535625=Junkai%20Li%20and%20Yunghwei%20Lai%20and%20Weitao%20Li%20and%20Jingyi%20Ren%20and%20Meng%20Zhang%20and%20Xinhui%20Kang%20and%20Siyu%20Wang%20and%20Peng%20Li%20and%20Ya-Qin%20Zhang%20and%20Weizhi%20Ma%20and%20Yang%20Liu&entry.1292438233=%20%20The%20recent%20rapid%20development%20of%20large%20language%20models%20%28LLMs%29%20has%20sparked%20a%0Anew%20wave%20of%20technological%20revolution%20in%20medical%20artificial%20intelligence%20%28AI%29.%0AWhile%20LLMs%20are%20designed%20to%20understand%20and%20generate%20text%20like%20a%20human%2C%0Aautonomous%20agents%20that%20utilize%20LLMs%20as%20their%20%22brain%22%20have%20exhibited%0Acapabilities%20beyond%20text%20processing%20such%20as%20planning%2C%20reflection%2C%20and%20using%0Atools%20by%20enabling%20their%20%22bodies%22%20to%20interact%20with%20the%20environment.%20We%20introduce%0Aa%20simulacrum%20of%20hospital%20called%20Agent%20Hospital%20that%20simulates%20the%20entire%0Aprocess%20of%20treating%20illness%2C%20in%20which%20all%20patients%2C%20nurses%2C%20and%20doctors%20are%0ALLM-powered%20autonomous%20agents.%20Within%20the%20simulacrum%2C%20doctor%20agents%20are%20able%20to%0Aevolve%20by%20treating%20a%20large%20number%20of%20patient%20agents%20without%20the%20need%20to%20label%0Atraining%20data%20manually.%20After%20treating%20tens%20of%20thousands%20of%20patient%20agents%20in%0Athe%20simulacrum%20%28human%20doctors%20may%20take%20several%20years%20in%20the%20real%20world%29%2C%20the%0Aevolved%20doctor%20agents%20outperform%20state-of-the-art%20medical%20agent%20methods%20on%20the%0AMedQA%20benchmark%20comprising%20US%20Medical%20Licensing%20Examination%20%28USMLE%29%20test%0Aquestions.%20Our%20methods%20of%20simulacrum%20construction%20and%20agent%20evolution%20have%20the%0Apotential%20in%20benefiting%20a%20broad%20range%20of%20applications%20beyond%20medical%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02957v3&entry.124074799=Read"},
{"title": "Conformal Prediction Sets with Improved Conditional Coverage using Trust\n  Scores", "author": "Jivat Neet Kaur and Michael I. Jordan and Ahmed Alaa", "abstract": "  Standard conformal prediction offers a marginal guarantee on coverage, but\nfor prediction sets to be truly useful, they should ideally ensure coverage\nconditional on each test point. Unfortunately, it is impossible to achieve\nexact, distribution-free conditional coverage in finite samples. In this work,\nwe propose an alternative conformal prediction algorithm that targets coverage\nwhere it matters most--in instances where a classifier is overconfident in its\nincorrect predictions. We start by dissecting miscoverage events in\nmarginally-valid conformal prediction, and show that miscoverage rates vary\nbased on the classifier's confidence and its deviation from the Bayes optimal\nclassifier. Motivated by this insight, we develop a variant of conformal\nprediction that targets coverage conditional on a reduced set of two variables:\nthe classifier's confidence in a prediction and a nonparametric trust score\nthat measures its deviation from the Bayes classifier. Empirical evaluation on\nmultiple image datasets shows that our method generally improves conditional\ncoverage properties compared to standard conformal prediction, including\nclass-conditional coverage, coverage over arbitrary subgroups, and coverage\nover demographic groups.\n", "link": "http://arxiv.org/abs/2501.10139v1", "date": "2025-01-17", "relevancy": 1.9509, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5065}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4875}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4804}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conformal%20Prediction%20Sets%20with%20Improved%20Conditional%20Coverage%20using%20Trust%0A%20%20Scores&body=Title%3A%20Conformal%20Prediction%20Sets%20with%20Improved%20Conditional%20Coverage%20using%20Trust%0A%20%20Scores%0AAuthor%3A%20Jivat%20Neet%20Kaur%20and%20Michael%20I.%20Jordan%20and%20Ahmed%20Alaa%0AAbstract%3A%20%20%20Standard%20conformal%20prediction%20offers%20a%20marginal%20guarantee%20on%20coverage%2C%20but%0Afor%20prediction%20sets%20to%20be%20truly%20useful%2C%20they%20should%20ideally%20ensure%20coverage%0Aconditional%20on%20each%20test%20point.%20Unfortunately%2C%20it%20is%20impossible%20to%20achieve%0Aexact%2C%20distribution-free%20conditional%20coverage%20in%20finite%20samples.%20In%20this%20work%2C%0Awe%20propose%20an%20alternative%20conformal%20prediction%20algorithm%20that%20targets%20coverage%0Awhere%20it%20matters%20most--in%20instances%20where%20a%20classifier%20is%20overconfident%20in%20its%0Aincorrect%20predictions.%20We%20start%20by%20dissecting%20miscoverage%20events%20in%0Amarginally-valid%20conformal%20prediction%2C%20and%20show%20that%20miscoverage%20rates%20vary%0Abased%20on%20the%20classifier%27s%20confidence%20and%20its%20deviation%20from%20the%20Bayes%20optimal%0Aclassifier.%20Motivated%20by%20this%20insight%2C%20we%20develop%20a%20variant%20of%20conformal%0Aprediction%20that%20targets%20coverage%20conditional%20on%20a%20reduced%20set%20of%20two%20variables%3A%0Athe%20classifier%27s%20confidence%20in%20a%20prediction%20and%20a%20nonparametric%20trust%20score%0Athat%20measures%20its%20deviation%20from%20the%20Bayes%20classifier.%20Empirical%20evaluation%20on%0Amultiple%20image%20datasets%20shows%20that%20our%20method%20generally%20improves%20conditional%0Acoverage%20properties%20compared%20to%20standard%20conformal%20prediction%2C%20including%0Aclass-conditional%20coverage%2C%20coverage%20over%20arbitrary%20subgroups%2C%20and%20coverage%0Aover%20demographic%20groups.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10139v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConformal%2520Prediction%2520Sets%2520with%2520Improved%2520Conditional%2520Coverage%2520using%2520Trust%250A%2520%2520Scores%26entry.906535625%3DJivat%2520Neet%2520Kaur%2520and%2520Michael%2520I.%2520Jordan%2520and%2520Ahmed%2520Alaa%26entry.1292438233%3D%2520%2520Standard%2520conformal%2520prediction%2520offers%2520a%2520marginal%2520guarantee%2520on%2520coverage%252C%2520but%250Afor%2520prediction%2520sets%2520to%2520be%2520truly%2520useful%252C%2520they%2520should%2520ideally%2520ensure%2520coverage%250Aconditional%2520on%2520each%2520test%2520point.%2520Unfortunately%252C%2520it%2520is%2520impossible%2520to%2520achieve%250Aexact%252C%2520distribution-free%2520conditional%2520coverage%2520in%2520finite%2520samples.%2520In%2520this%2520work%252C%250Awe%2520propose%2520an%2520alternative%2520conformal%2520prediction%2520algorithm%2520that%2520targets%2520coverage%250Awhere%2520it%2520matters%2520most--in%2520instances%2520where%2520a%2520classifier%2520is%2520overconfident%2520in%2520its%250Aincorrect%2520predictions.%2520We%2520start%2520by%2520dissecting%2520miscoverage%2520events%2520in%250Amarginally-valid%2520conformal%2520prediction%252C%2520and%2520show%2520that%2520miscoverage%2520rates%2520vary%250Abased%2520on%2520the%2520classifier%2527s%2520confidence%2520and%2520its%2520deviation%2520from%2520the%2520Bayes%2520optimal%250Aclassifier.%2520Motivated%2520by%2520this%2520insight%252C%2520we%2520develop%2520a%2520variant%2520of%2520conformal%250Aprediction%2520that%2520targets%2520coverage%2520conditional%2520on%2520a%2520reduced%2520set%2520of%2520two%2520variables%253A%250Athe%2520classifier%2527s%2520confidence%2520in%2520a%2520prediction%2520and%2520a%2520nonparametric%2520trust%2520score%250Athat%2520measures%2520its%2520deviation%2520from%2520the%2520Bayes%2520classifier.%2520Empirical%2520evaluation%2520on%250Amultiple%2520image%2520datasets%2520shows%2520that%2520our%2520method%2520generally%2520improves%2520conditional%250Acoverage%2520properties%2520compared%2520to%2520standard%2520conformal%2520prediction%252C%2520including%250Aclass-conditional%2520coverage%252C%2520coverage%2520over%2520arbitrary%2520subgroups%252C%2520and%2520coverage%250Aover%2520demographic%2520groups.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10139v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conformal%20Prediction%20Sets%20with%20Improved%20Conditional%20Coverage%20using%20Trust%0A%20%20Scores&entry.906535625=Jivat%20Neet%20Kaur%20and%20Michael%20I.%20Jordan%20and%20Ahmed%20Alaa&entry.1292438233=%20%20Standard%20conformal%20prediction%20offers%20a%20marginal%20guarantee%20on%20coverage%2C%20but%0Afor%20prediction%20sets%20to%20be%20truly%20useful%2C%20they%20should%20ideally%20ensure%20coverage%0Aconditional%20on%20each%20test%20point.%20Unfortunately%2C%20it%20is%20impossible%20to%20achieve%0Aexact%2C%20distribution-free%20conditional%20coverage%20in%20finite%20samples.%20In%20this%20work%2C%0Awe%20propose%20an%20alternative%20conformal%20prediction%20algorithm%20that%20targets%20coverage%0Awhere%20it%20matters%20most--in%20instances%20where%20a%20classifier%20is%20overconfident%20in%20its%0Aincorrect%20predictions.%20We%20start%20by%20dissecting%20miscoverage%20events%20in%0Amarginally-valid%20conformal%20prediction%2C%20and%20show%20that%20miscoverage%20rates%20vary%0Abased%20on%20the%20classifier%27s%20confidence%20and%20its%20deviation%20from%20the%20Bayes%20optimal%0Aclassifier.%20Motivated%20by%20this%20insight%2C%20we%20develop%20a%20variant%20of%20conformal%0Aprediction%20that%20targets%20coverage%20conditional%20on%20a%20reduced%20set%20of%20two%20variables%3A%0Athe%20classifier%27s%20confidence%20in%20a%20prediction%20and%20a%20nonparametric%20trust%20score%0Athat%20measures%20its%20deviation%20from%20the%20Bayes%20classifier.%20Empirical%20evaluation%20on%0Amultiple%20image%20datasets%20shows%20that%20our%20method%20generally%20improves%20conditional%0Acoverage%20properties%20compared%20to%20standard%20conformal%20prediction%2C%20including%0Aclass-conditional%20coverage%2C%20coverage%20over%20arbitrary%20subgroups%2C%20and%20coverage%0Aover%20demographic%20groups.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10139v1&entry.124074799=Read"},
{"title": "Stochastic gradient descent for streaming linear and rectified linear\n  systems with adversarial corruptions", "author": "Halyun Jeong and Deanna Needell and Elizaveta Rebrova", "abstract": "  We propose SGD-exp, a stochastic gradient descent approach for linear and\nReLU regressions under Massart noise (adversarial semi-random corruption model)\nfor the fully streaming setting. We show novel nearly linear convergence\nguarantees of SGD-exp to the true parameter with up to $50\\%$ Massart\ncorruption rate, and with any corruption rate in the case of symmetric\noblivious corruptions. This is the first convergence guarantee result for\nrobust ReLU regression in the streaming setting, and it shows the improved\nconvergence rate over previous robust methods for $L_1$ linear regression due\nto a choice of an exponentially decaying step size, known for its efficiency in\npractice. Our analysis is based on the drift analysis of a discrete stochastic\nprocess, which could also be interesting on its own.\n", "link": "http://arxiv.org/abs/2403.01204v2", "date": "2025-01-17", "relevancy": 1.937, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5002}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4844}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4777}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stochastic%20gradient%20descent%20for%20streaming%20linear%20and%20rectified%20linear%0A%20%20systems%20with%20adversarial%20corruptions&body=Title%3A%20Stochastic%20gradient%20descent%20for%20streaming%20linear%20and%20rectified%20linear%0A%20%20systems%20with%20adversarial%20corruptions%0AAuthor%3A%20Halyun%20Jeong%20and%20Deanna%20Needell%20and%20Elizaveta%20Rebrova%0AAbstract%3A%20%20%20We%20propose%20SGD-exp%2C%20a%20stochastic%20gradient%20descent%20approach%20for%20linear%20and%0AReLU%20regressions%20under%20Massart%20noise%20%28adversarial%20semi-random%20corruption%20model%29%0Afor%20the%20fully%20streaming%20setting.%20We%20show%20novel%20nearly%20linear%20convergence%0Aguarantees%20of%20SGD-exp%20to%20the%20true%20parameter%20with%20up%20to%20%2450%5C%25%24%20Massart%0Acorruption%20rate%2C%20and%20with%20any%20corruption%20rate%20in%20the%20case%20of%20symmetric%0Aoblivious%20corruptions.%20This%20is%20the%20first%20convergence%20guarantee%20result%20for%0Arobust%20ReLU%20regression%20in%20the%20streaming%20setting%2C%20and%20it%20shows%20the%20improved%0Aconvergence%20rate%20over%20previous%20robust%20methods%20for%20%24L_1%24%20linear%20regression%20due%0Ato%20a%20choice%20of%20an%20exponentially%20decaying%20step%20size%2C%20known%20for%20its%20efficiency%20in%0Apractice.%20Our%20analysis%20is%20based%20on%20the%20drift%20analysis%20of%20a%20discrete%20stochastic%0Aprocess%2C%20which%20could%20also%20be%20interesting%20on%20its%20own.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01204v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStochastic%2520gradient%2520descent%2520for%2520streaming%2520linear%2520and%2520rectified%2520linear%250A%2520%2520systems%2520with%2520adversarial%2520corruptions%26entry.906535625%3DHalyun%2520Jeong%2520and%2520Deanna%2520Needell%2520and%2520Elizaveta%2520Rebrova%26entry.1292438233%3D%2520%2520We%2520propose%2520SGD-exp%252C%2520a%2520stochastic%2520gradient%2520descent%2520approach%2520for%2520linear%2520and%250AReLU%2520regressions%2520under%2520Massart%2520noise%2520%2528adversarial%2520semi-random%2520corruption%2520model%2529%250Afor%2520the%2520fully%2520streaming%2520setting.%2520We%2520show%2520novel%2520nearly%2520linear%2520convergence%250Aguarantees%2520of%2520SGD-exp%2520to%2520the%2520true%2520parameter%2520with%2520up%2520to%2520%252450%255C%2525%2524%2520Massart%250Acorruption%2520rate%252C%2520and%2520with%2520any%2520corruption%2520rate%2520in%2520the%2520case%2520of%2520symmetric%250Aoblivious%2520corruptions.%2520This%2520is%2520the%2520first%2520convergence%2520guarantee%2520result%2520for%250Arobust%2520ReLU%2520regression%2520in%2520the%2520streaming%2520setting%252C%2520and%2520it%2520shows%2520the%2520improved%250Aconvergence%2520rate%2520over%2520previous%2520robust%2520methods%2520for%2520%2524L_1%2524%2520linear%2520regression%2520due%250Ato%2520a%2520choice%2520of%2520an%2520exponentially%2520decaying%2520step%2520size%252C%2520known%2520for%2520its%2520efficiency%2520in%250Apractice.%2520Our%2520analysis%2520is%2520based%2520on%2520the%2520drift%2520analysis%2520of%2520a%2520discrete%2520stochastic%250Aprocess%252C%2520which%2520could%2520also%2520be%2520interesting%2520on%2520its%2520own.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.01204v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stochastic%20gradient%20descent%20for%20streaming%20linear%20and%20rectified%20linear%0A%20%20systems%20with%20adversarial%20corruptions&entry.906535625=Halyun%20Jeong%20and%20Deanna%20Needell%20and%20Elizaveta%20Rebrova&entry.1292438233=%20%20We%20propose%20SGD-exp%2C%20a%20stochastic%20gradient%20descent%20approach%20for%20linear%20and%0AReLU%20regressions%20under%20Massart%20noise%20%28adversarial%20semi-random%20corruption%20model%29%0Afor%20the%20fully%20streaming%20setting.%20We%20show%20novel%20nearly%20linear%20convergence%0Aguarantees%20of%20SGD-exp%20to%20the%20true%20parameter%20with%20up%20to%20%2450%5C%25%24%20Massart%0Acorruption%20rate%2C%20and%20with%20any%20corruption%20rate%20in%20the%20case%20of%20symmetric%0Aoblivious%20corruptions.%20This%20is%20the%20first%20convergence%20guarantee%20result%20for%0Arobust%20ReLU%20regression%20in%20the%20streaming%20setting%2C%20and%20it%20shows%20the%20improved%0Aconvergence%20rate%20over%20previous%20robust%20methods%20for%20%24L_1%24%20linear%20regression%20due%0Ato%20a%20choice%20of%20an%20exponentially%20decaying%20step%20size%2C%20known%20for%20its%20efficiency%20in%0Apractice.%20Our%20analysis%20is%20based%20on%20the%20drift%20analysis%20of%20a%20discrete%20stochastic%0Aprocess%2C%20which%20could%20also%20be%20interesting%20on%20its%20own.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01204v2&entry.124074799=Read"},
{"title": "A recursive Bayesian neural network for constitutive modeling of sands\n  under monotonic loading", "author": "Toiba Noor and Soban Nasir Lone and G. V. Ramana and Rajdip Nayek", "abstract": "  In geotechnical engineering, constitutive models play a crucial role in\ndescribing soil behavior under varying loading conditions. Data-driven deep\nlearning (DL) models offer a promising alternative for developing predictive\nconstitutive models. When prediction is the primary focus, quantifying the\npredictive uncertainty of a trained DL model and communicating this uncertainty\nto end users is crucial for informed decision-making.\n  This study proposes a recursive Bayesian neural network (rBNN) framework,\nwhich builds upon recursive feedforward neural networks (rFFNNs) by introducing\ngeneralized Bayesian inference for uncertainty quantification. A significant\ncontribution of this work is the incorporation of a sliding window approach in\nrFFNNs, allowing the models to effectively capture temporal dependencies across\nload steps. The rBNN extends this framework by treating model parameters as\nrandom variables, with their posterior distributions inferred using generalized\nvariational inference.\n  The proposed framework is validated on two datasets: (i) a numerically\nsimulated consolidated drained (CD) triaxial dataset employing a hardening soil\nmodel and (ii) an experimental dataset comprising 28 CD triaxial tests on\nBaskarp sand. Comparative analyses with LSTM, Bi-LSTM, and GRU models\ndemonstrate that the deterministic rFFNN achieves superior predictive accuracy,\nattributed to its transparent structure and sliding window design. While the\nrBNN marginally trails in accuracy for the experimental case, it provides\nrobust confidence intervals, addressing data sparsity and measurement noise in\nexperimental conditions. The study underscores the trade-offs between\ndeterministic and probabilistic approaches and the potential of rBNNs for\nuncertainty-aware constitutive modeling.\n", "link": "http://arxiv.org/abs/2501.10088v1", "date": "2025-01-17", "relevancy": 1.9324, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5557}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4712}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20recursive%20Bayesian%20neural%20network%20for%20constitutive%20modeling%20of%20sands%0A%20%20under%20monotonic%20loading&body=Title%3A%20A%20recursive%20Bayesian%20neural%20network%20for%20constitutive%20modeling%20of%20sands%0A%20%20under%20monotonic%20loading%0AAuthor%3A%20Toiba%20Noor%20and%20Soban%20Nasir%20Lone%20and%20G.%20V.%20Ramana%20and%20Rajdip%20Nayek%0AAbstract%3A%20%20%20In%20geotechnical%20engineering%2C%20constitutive%20models%20play%20a%20crucial%20role%20in%0Adescribing%20soil%20behavior%20under%20varying%20loading%20conditions.%20Data-driven%20deep%0Alearning%20%28DL%29%20models%20offer%20a%20promising%20alternative%20for%20developing%20predictive%0Aconstitutive%20models.%20When%20prediction%20is%20the%20primary%20focus%2C%20quantifying%20the%0Apredictive%20uncertainty%20of%20a%20trained%20DL%20model%20and%20communicating%20this%20uncertainty%0Ato%20end%20users%20is%20crucial%20for%20informed%20decision-making.%0A%20%20This%20study%20proposes%20a%20recursive%20Bayesian%20neural%20network%20%28rBNN%29%20framework%2C%0Awhich%20builds%20upon%20recursive%20feedforward%20neural%20networks%20%28rFFNNs%29%20by%20introducing%0Ageneralized%20Bayesian%20inference%20for%20uncertainty%20quantification.%20A%20significant%0Acontribution%20of%20this%20work%20is%20the%20incorporation%20of%20a%20sliding%20window%20approach%20in%0ArFFNNs%2C%20allowing%20the%20models%20to%20effectively%20capture%20temporal%20dependencies%20across%0Aload%20steps.%20The%20rBNN%20extends%20this%20framework%20by%20treating%20model%20parameters%20as%0Arandom%20variables%2C%20with%20their%20posterior%20distributions%20inferred%20using%20generalized%0Avariational%20inference.%0A%20%20The%20proposed%20framework%20is%20validated%20on%20two%20datasets%3A%20%28i%29%20a%20numerically%0Asimulated%20consolidated%20drained%20%28CD%29%20triaxial%20dataset%20employing%20a%20hardening%20soil%0Amodel%20and%20%28ii%29%20an%20experimental%20dataset%20comprising%2028%20CD%20triaxial%20tests%20on%0ABaskarp%20sand.%20Comparative%20analyses%20with%20LSTM%2C%20Bi-LSTM%2C%20and%20GRU%20models%0Ademonstrate%20that%20the%20deterministic%20rFFNN%20achieves%20superior%20predictive%20accuracy%2C%0Aattributed%20to%20its%20transparent%20structure%20and%20sliding%20window%20design.%20While%20the%0ArBNN%20marginally%20trails%20in%20accuracy%20for%20the%20experimental%20case%2C%20it%20provides%0Arobust%20confidence%20intervals%2C%20addressing%20data%20sparsity%20and%20measurement%20noise%20in%0Aexperimental%20conditions.%20The%20study%20underscores%20the%20trade-offs%20between%0Adeterministic%20and%20probabilistic%20approaches%20and%20the%20potential%20of%20rBNNs%20for%0Auncertainty-aware%20constitutive%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10088v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520recursive%2520Bayesian%2520neural%2520network%2520for%2520constitutive%2520modeling%2520of%2520sands%250A%2520%2520under%2520monotonic%2520loading%26entry.906535625%3DToiba%2520Noor%2520and%2520Soban%2520Nasir%2520Lone%2520and%2520G.%2520V.%2520Ramana%2520and%2520Rajdip%2520Nayek%26entry.1292438233%3D%2520%2520In%2520geotechnical%2520engineering%252C%2520constitutive%2520models%2520play%2520a%2520crucial%2520role%2520in%250Adescribing%2520soil%2520behavior%2520under%2520varying%2520loading%2520conditions.%2520Data-driven%2520deep%250Alearning%2520%2528DL%2529%2520models%2520offer%2520a%2520promising%2520alternative%2520for%2520developing%2520predictive%250Aconstitutive%2520models.%2520When%2520prediction%2520is%2520the%2520primary%2520focus%252C%2520quantifying%2520the%250Apredictive%2520uncertainty%2520of%2520a%2520trained%2520DL%2520model%2520and%2520communicating%2520this%2520uncertainty%250Ato%2520end%2520users%2520is%2520crucial%2520for%2520informed%2520decision-making.%250A%2520%2520This%2520study%2520proposes%2520a%2520recursive%2520Bayesian%2520neural%2520network%2520%2528rBNN%2529%2520framework%252C%250Awhich%2520builds%2520upon%2520recursive%2520feedforward%2520neural%2520networks%2520%2528rFFNNs%2529%2520by%2520introducing%250Ageneralized%2520Bayesian%2520inference%2520for%2520uncertainty%2520quantification.%2520A%2520significant%250Acontribution%2520of%2520this%2520work%2520is%2520the%2520incorporation%2520of%2520a%2520sliding%2520window%2520approach%2520in%250ArFFNNs%252C%2520allowing%2520the%2520models%2520to%2520effectively%2520capture%2520temporal%2520dependencies%2520across%250Aload%2520steps.%2520The%2520rBNN%2520extends%2520this%2520framework%2520by%2520treating%2520model%2520parameters%2520as%250Arandom%2520variables%252C%2520with%2520their%2520posterior%2520distributions%2520inferred%2520using%2520generalized%250Avariational%2520inference.%250A%2520%2520The%2520proposed%2520framework%2520is%2520validated%2520on%2520two%2520datasets%253A%2520%2528i%2529%2520a%2520numerically%250Asimulated%2520consolidated%2520drained%2520%2528CD%2529%2520triaxial%2520dataset%2520employing%2520a%2520hardening%2520soil%250Amodel%2520and%2520%2528ii%2529%2520an%2520experimental%2520dataset%2520comprising%252028%2520CD%2520triaxial%2520tests%2520on%250ABaskarp%2520sand.%2520Comparative%2520analyses%2520with%2520LSTM%252C%2520Bi-LSTM%252C%2520and%2520GRU%2520models%250Ademonstrate%2520that%2520the%2520deterministic%2520rFFNN%2520achieves%2520superior%2520predictive%2520accuracy%252C%250Aattributed%2520to%2520its%2520transparent%2520structure%2520and%2520sliding%2520window%2520design.%2520While%2520the%250ArBNN%2520marginally%2520trails%2520in%2520accuracy%2520for%2520the%2520experimental%2520case%252C%2520it%2520provides%250Arobust%2520confidence%2520intervals%252C%2520addressing%2520data%2520sparsity%2520and%2520measurement%2520noise%2520in%250Aexperimental%2520conditions.%2520The%2520study%2520underscores%2520the%2520trade-offs%2520between%250Adeterministic%2520and%2520probabilistic%2520approaches%2520and%2520the%2520potential%2520of%2520rBNNs%2520for%250Auncertainty-aware%2520constitutive%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10088v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20recursive%20Bayesian%20neural%20network%20for%20constitutive%20modeling%20of%20sands%0A%20%20under%20monotonic%20loading&entry.906535625=Toiba%20Noor%20and%20Soban%20Nasir%20Lone%20and%20G.%20V.%20Ramana%20and%20Rajdip%20Nayek&entry.1292438233=%20%20In%20geotechnical%20engineering%2C%20constitutive%20models%20play%20a%20crucial%20role%20in%0Adescribing%20soil%20behavior%20under%20varying%20loading%20conditions.%20Data-driven%20deep%0Alearning%20%28DL%29%20models%20offer%20a%20promising%20alternative%20for%20developing%20predictive%0Aconstitutive%20models.%20When%20prediction%20is%20the%20primary%20focus%2C%20quantifying%20the%0Apredictive%20uncertainty%20of%20a%20trained%20DL%20model%20and%20communicating%20this%20uncertainty%0Ato%20end%20users%20is%20crucial%20for%20informed%20decision-making.%0A%20%20This%20study%20proposes%20a%20recursive%20Bayesian%20neural%20network%20%28rBNN%29%20framework%2C%0Awhich%20builds%20upon%20recursive%20feedforward%20neural%20networks%20%28rFFNNs%29%20by%20introducing%0Ageneralized%20Bayesian%20inference%20for%20uncertainty%20quantification.%20A%20significant%0Acontribution%20of%20this%20work%20is%20the%20incorporation%20of%20a%20sliding%20window%20approach%20in%0ArFFNNs%2C%20allowing%20the%20models%20to%20effectively%20capture%20temporal%20dependencies%20across%0Aload%20steps.%20The%20rBNN%20extends%20this%20framework%20by%20treating%20model%20parameters%20as%0Arandom%20variables%2C%20with%20their%20posterior%20distributions%20inferred%20using%20generalized%0Avariational%20inference.%0A%20%20The%20proposed%20framework%20is%20validated%20on%20two%20datasets%3A%20%28i%29%20a%20numerically%0Asimulated%20consolidated%20drained%20%28CD%29%20triaxial%20dataset%20employing%20a%20hardening%20soil%0Amodel%20and%20%28ii%29%20an%20experimental%20dataset%20comprising%2028%20CD%20triaxial%20tests%20on%0ABaskarp%20sand.%20Comparative%20analyses%20with%20LSTM%2C%20Bi-LSTM%2C%20and%20GRU%20models%0Ademonstrate%20that%20the%20deterministic%20rFFNN%20achieves%20superior%20predictive%20accuracy%2C%0Aattributed%20to%20its%20transparent%20structure%20and%20sliding%20window%20design.%20While%20the%0ArBNN%20marginally%20trails%20in%20accuracy%20for%20the%20experimental%20case%2C%20it%20provides%0Arobust%20confidence%20intervals%2C%20addressing%20data%20sparsity%20and%20measurement%20noise%20in%0Aexperimental%20conditions.%20The%20study%20underscores%20the%20trade-offs%20between%0Adeterministic%20and%20probabilistic%20approaches%20and%20the%20potential%20of%20rBNNs%20for%0Auncertainty-aware%20constitutive%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10088v1&entry.124074799=Read"},
{"title": "3rd Workshop on Maritime Computer Vision (MaCVi) 2025: Challenge Results", "author": "Benjamin Kiefer and Lojze \u017dust and Jon Muhovi\u010d and Matej Kristan and Janez Per\u0161 and Matija Ter\u0161ek and Uma Mudenagudi Chaitra Desai and Arnold Wiliem and Marten Kreis and Nikhil Akalwadi and Yitong Quan and Zhiqiang Zhong and Zhe Zhang and Sujie Liu and Xuran Chen and Yang Yang and Matej Fabijani\u0107 and Fausto Ferreira and Seongju Lee and Junseok Lee and Kyoobin Lee and Shanliang Yao and Runwei Guan and Xiaoyu Huang and Yi Ni and Himanshu Kumar and Yuan Feng and Yi-Ching Cheng and Tzu-Yu Lin and Chia-Ming Lee and Chih-Chung Hsu and Jannik Sheikh and Andreas Michel and Wolfgang Gross and Martin Weinmann and Josip \u0160ari\u0107 and Yipeng Lin and Xiang Yang and Nan Jiang and Yutang Lu and Fei Feng and Ali Awad and Evan Lucas and Ashraf Saleem and Ching-Heng Cheng and Yu-Fan Lin and Tzu-Yu Lin and Chih-Chung Hsu", "abstract": "  The 3rd Workshop on Maritime Computer Vision (MaCVi) 2025 addresses maritime\ncomputer vision for Unmanned Surface Vehicles (USV) and underwater. This report\noffers a comprehensive overview of the findings from the challenges. We provide\nboth statistical and qualitative analyses, evaluating trends from over 700\nsubmissions. All datasets, evaluation code, and the leaderboard are available\nto the public at https://macvi.org/workshop/macvi25.\n", "link": "http://arxiv.org/abs/2501.10343v1", "date": "2025-01-17", "relevancy": 1.9253, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5014}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4813}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203rd%20Workshop%20on%20Maritime%20Computer%20Vision%20%28MaCVi%29%202025%3A%20Challenge%20Results&body=Title%3A%203rd%20Workshop%20on%20Maritime%20Computer%20Vision%20%28MaCVi%29%202025%3A%20Challenge%20Results%0AAuthor%3A%20Benjamin%20Kiefer%20and%20Lojze%20%C5%BDust%20and%20Jon%20Muhovi%C4%8D%20and%20Matej%20Kristan%20and%20Janez%20Per%C5%A1%20and%20Matija%20Ter%C5%A1ek%20and%20Uma%20Mudenagudi%20Chaitra%20Desai%20and%20Arnold%20Wiliem%20and%20Marten%20Kreis%20and%20Nikhil%20Akalwadi%20and%20Yitong%20Quan%20and%20Zhiqiang%20Zhong%20and%20Zhe%20Zhang%20and%20Sujie%20Liu%20and%20Xuran%20Chen%20and%20Yang%20Yang%20and%20Matej%20Fabijani%C4%87%20and%20Fausto%20Ferreira%20and%20Seongju%20Lee%20and%20Junseok%20Lee%20and%20Kyoobin%20Lee%20and%20Shanliang%20Yao%20and%20Runwei%20Guan%20and%20Xiaoyu%20Huang%20and%20Yi%20Ni%20and%20Himanshu%20Kumar%20and%20Yuan%20Feng%20and%20Yi-Ching%20Cheng%20and%20Tzu-Yu%20Lin%20and%20Chia-Ming%20Lee%20and%20Chih-Chung%20Hsu%20and%20Jannik%20Sheikh%20and%20Andreas%20Michel%20and%20Wolfgang%20Gross%20and%20Martin%20Weinmann%20and%20Josip%20%C5%A0ari%C4%87%20and%20Yipeng%20Lin%20and%20Xiang%20Yang%20and%20Nan%20Jiang%20and%20Yutang%20Lu%20and%20Fei%20Feng%20and%20Ali%20Awad%20and%20Evan%20Lucas%20and%20Ashraf%20Saleem%20and%20Ching-Heng%20Cheng%20and%20Yu-Fan%20Lin%20and%20Tzu-Yu%20Lin%20and%20Chih-Chung%20Hsu%0AAbstract%3A%20%20%20The%203rd%20Workshop%20on%20Maritime%20Computer%20Vision%20%28MaCVi%29%202025%20addresses%20maritime%0Acomputer%20vision%20for%20Unmanned%20Surface%20Vehicles%20%28USV%29%20and%20underwater.%20This%20report%0Aoffers%20a%20comprehensive%20overview%20of%20the%20findings%20from%20the%20challenges.%20We%20provide%0Aboth%20statistical%20and%20qualitative%20analyses%2C%20evaluating%20trends%20from%20over%20700%0Asubmissions.%20All%20datasets%2C%20evaluation%20code%2C%20and%20the%20leaderboard%20are%20available%0Ato%20the%20public%20at%20https%3A//macvi.org/workshop/macvi25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10343v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3rd%2520Workshop%2520on%2520Maritime%2520Computer%2520Vision%2520%2528MaCVi%2529%25202025%253A%2520Challenge%2520Results%26entry.906535625%3DBenjamin%2520Kiefer%2520and%2520Lojze%2520%25C5%25BDust%2520and%2520Jon%2520Muhovi%25C4%258D%2520and%2520Matej%2520Kristan%2520and%2520Janez%2520Per%25C5%25A1%2520and%2520Matija%2520Ter%25C5%25A1ek%2520and%2520Uma%2520Mudenagudi%2520Chaitra%2520Desai%2520and%2520Arnold%2520Wiliem%2520and%2520Marten%2520Kreis%2520and%2520Nikhil%2520Akalwadi%2520and%2520Yitong%2520Quan%2520and%2520Zhiqiang%2520Zhong%2520and%2520Zhe%2520Zhang%2520and%2520Sujie%2520Liu%2520and%2520Xuran%2520Chen%2520and%2520Yang%2520Yang%2520and%2520Matej%2520Fabijani%25C4%2587%2520and%2520Fausto%2520Ferreira%2520and%2520Seongju%2520Lee%2520and%2520Junseok%2520Lee%2520and%2520Kyoobin%2520Lee%2520and%2520Shanliang%2520Yao%2520and%2520Runwei%2520Guan%2520and%2520Xiaoyu%2520Huang%2520and%2520Yi%2520Ni%2520and%2520Himanshu%2520Kumar%2520and%2520Yuan%2520Feng%2520and%2520Yi-Ching%2520Cheng%2520and%2520Tzu-Yu%2520Lin%2520and%2520Chia-Ming%2520Lee%2520and%2520Chih-Chung%2520Hsu%2520and%2520Jannik%2520Sheikh%2520and%2520Andreas%2520Michel%2520and%2520Wolfgang%2520Gross%2520and%2520Martin%2520Weinmann%2520and%2520Josip%2520%25C5%25A0ari%25C4%2587%2520and%2520Yipeng%2520Lin%2520and%2520Xiang%2520Yang%2520and%2520Nan%2520Jiang%2520and%2520Yutang%2520Lu%2520and%2520Fei%2520Feng%2520and%2520Ali%2520Awad%2520and%2520Evan%2520Lucas%2520and%2520Ashraf%2520Saleem%2520and%2520Ching-Heng%2520Cheng%2520and%2520Yu-Fan%2520Lin%2520and%2520Tzu-Yu%2520Lin%2520and%2520Chih-Chung%2520Hsu%26entry.1292438233%3D%2520%2520The%25203rd%2520Workshop%2520on%2520Maritime%2520Computer%2520Vision%2520%2528MaCVi%2529%25202025%2520addresses%2520maritime%250Acomputer%2520vision%2520for%2520Unmanned%2520Surface%2520Vehicles%2520%2528USV%2529%2520and%2520underwater.%2520This%2520report%250Aoffers%2520a%2520comprehensive%2520overview%2520of%2520the%2520findings%2520from%2520the%2520challenges.%2520We%2520provide%250Aboth%2520statistical%2520and%2520qualitative%2520analyses%252C%2520evaluating%2520trends%2520from%2520over%2520700%250Asubmissions.%2520All%2520datasets%252C%2520evaluation%2520code%252C%2520and%2520the%2520leaderboard%2520are%2520available%250Ato%2520the%2520public%2520at%2520https%253A//macvi.org/workshop/macvi25.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10343v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3rd%20Workshop%20on%20Maritime%20Computer%20Vision%20%28MaCVi%29%202025%3A%20Challenge%20Results&entry.906535625=Benjamin%20Kiefer%20and%20Lojze%20%C5%BDust%20and%20Jon%20Muhovi%C4%8D%20and%20Matej%20Kristan%20and%20Janez%20Per%C5%A1%20and%20Matija%20Ter%C5%A1ek%20and%20Uma%20Mudenagudi%20Chaitra%20Desai%20and%20Arnold%20Wiliem%20and%20Marten%20Kreis%20and%20Nikhil%20Akalwadi%20and%20Yitong%20Quan%20and%20Zhiqiang%20Zhong%20and%20Zhe%20Zhang%20and%20Sujie%20Liu%20and%20Xuran%20Chen%20and%20Yang%20Yang%20and%20Matej%20Fabijani%C4%87%20and%20Fausto%20Ferreira%20and%20Seongju%20Lee%20and%20Junseok%20Lee%20and%20Kyoobin%20Lee%20and%20Shanliang%20Yao%20and%20Runwei%20Guan%20and%20Xiaoyu%20Huang%20and%20Yi%20Ni%20and%20Himanshu%20Kumar%20and%20Yuan%20Feng%20and%20Yi-Ching%20Cheng%20and%20Tzu-Yu%20Lin%20and%20Chia-Ming%20Lee%20and%20Chih-Chung%20Hsu%20and%20Jannik%20Sheikh%20and%20Andreas%20Michel%20and%20Wolfgang%20Gross%20and%20Martin%20Weinmann%20and%20Josip%20%C5%A0ari%C4%87%20and%20Yipeng%20Lin%20and%20Xiang%20Yang%20and%20Nan%20Jiang%20and%20Yutang%20Lu%20and%20Fei%20Feng%20and%20Ali%20Awad%20and%20Evan%20Lucas%20and%20Ashraf%20Saleem%20and%20Ching-Heng%20Cheng%20and%20Yu-Fan%20Lin%20and%20Tzu-Yu%20Lin%20and%20Chih-Chung%20Hsu&entry.1292438233=%20%20The%203rd%20Workshop%20on%20Maritime%20Computer%20Vision%20%28MaCVi%29%202025%20addresses%20maritime%0Acomputer%20vision%20for%20Unmanned%20Surface%20Vehicles%20%28USV%29%20and%20underwater.%20This%20report%0Aoffers%20a%20comprehensive%20overview%20of%20the%20findings%20from%20the%20challenges.%20We%20provide%0Aboth%20statistical%20and%20qualitative%20analyses%2C%20evaluating%20trends%20from%20over%20700%0Asubmissions.%20All%20datasets%2C%20evaluation%20code%2C%20and%20the%20leaderboard%20are%20available%0Ato%20the%20public%20at%20https%3A//macvi.org/workshop/macvi25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10343v1&entry.124074799=Read"},
{"title": "Generative AI in Cybersecurity: A Comprehensive Review of LLM\n  Applications and Vulnerabilities", "author": "Mohamed Amine Ferrag and Fatima Alwahedi and Ammar Battah and Bilel Cherif and Abdechakour Mechri and Norbert Tihanyi and Tamas Bisztray and Merouane Debbah", "abstract": "  This paper provides a comprehensive review of the future of cybersecurity\nthrough Generative AI and Large Language Models (LLMs). We explore LLM\napplications across various domains, including hardware design security,\nintrusion detection, software engineering, design verification, cyber threat\nintelligence, malware detection, and phishing detection. We present an overview\nof LLM evolution and its current state, focusing on advancements in models such\nas GPT-4, GPT-3.5, Mixtral-8x7B, BERT, Falcon2, and LLaMA. Our analysis extends\nto LLM vulnerabilities, such as prompt injection, insecure output handling,\ndata poisoning, DDoS attacks, and adversarial instructions. We delve into\nmitigation strategies to protect these models, providing a comprehensive look\nat potential attack scenarios and prevention techniques. Furthermore, we\nevaluate the performance of 42 LLM models in cybersecurity knowledge and\nhardware security, highlighting their strengths and weaknesses. We thoroughly\nevaluate cybersecurity datasets for LLM training and testing, covering the\nlifecycle from data creation to usage and identifying gaps for future research.\nIn addition, we review new strategies for leveraging LLMs, including techniques\nlike Half-Quadratic Quantization (HQQ), Reinforcement Learning with Human\nFeedback (RLHF), Direct Preference Optimization (DPO), Quantized Low-Rank\nAdapters (QLoRA), and Retrieval-Augmented Generation (RAG). These insights aim\nto enhance real-time cybersecurity defenses and improve the sophistication of\nLLM applications in threat detection and response. Our paper provides a\nfoundational understanding and strategic direction for integrating LLMs into\nfuture cybersecurity frameworks, emphasizing innovation and robust model\ndeployment to safeguard against evolving cyber threats.\n", "link": "http://arxiv.org/abs/2405.12750v2", "date": "2025-01-17", "relevancy": 1.9249, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4882}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4805}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20AI%20in%20Cybersecurity%3A%20A%20Comprehensive%20Review%20of%20LLM%0A%20%20Applications%20and%20Vulnerabilities&body=Title%3A%20Generative%20AI%20in%20Cybersecurity%3A%20A%20Comprehensive%20Review%20of%20LLM%0A%20%20Applications%20and%20Vulnerabilities%0AAuthor%3A%20Mohamed%20Amine%20Ferrag%20and%20Fatima%20Alwahedi%20and%20Ammar%20Battah%20and%20Bilel%20Cherif%20and%20Abdechakour%20Mechri%20and%20Norbert%20Tihanyi%20and%20Tamas%20Bisztray%20and%20Merouane%20Debbah%0AAbstract%3A%20%20%20This%20paper%20provides%20a%20comprehensive%20review%20of%20the%20future%20of%20cybersecurity%0Athrough%20Generative%20AI%20and%20Large%20Language%20Models%20%28LLMs%29.%20We%20explore%20LLM%0Aapplications%20across%20various%20domains%2C%20including%20hardware%20design%20security%2C%0Aintrusion%20detection%2C%20software%20engineering%2C%20design%20verification%2C%20cyber%20threat%0Aintelligence%2C%20malware%20detection%2C%20and%20phishing%20detection.%20We%20present%20an%20overview%0Aof%20LLM%20evolution%20and%20its%20current%20state%2C%20focusing%20on%20advancements%20in%20models%20such%0Aas%20GPT-4%2C%20GPT-3.5%2C%20Mixtral-8x7B%2C%20BERT%2C%20Falcon2%2C%20and%20LLaMA.%20Our%20analysis%20extends%0Ato%20LLM%20vulnerabilities%2C%20such%20as%20prompt%20injection%2C%20insecure%20output%20handling%2C%0Adata%20poisoning%2C%20DDoS%20attacks%2C%20and%20adversarial%20instructions.%20We%20delve%20into%0Amitigation%20strategies%20to%20protect%20these%20models%2C%20providing%20a%20comprehensive%20look%0Aat%20potential%20attack%20scenarios%20and%20prevention%20techniques.%20Furthermore%2C%20we%0Aevaluate%20the%20performance%20of%2042%20LLM%20models%20in%20cybersecurity%20knowledge%20and%0Ahardware%20security%2C%20highlighting%20their%20strengths%20and%20weaknesses.%20We%20thoroughly%0Aevaluate%20cybersecurity%20datasets%20for%20LLM%20training%20and%20testing%2C%20covering%20the%0Alifecycle%20from%20data%20creation%20to%20usage%20and%20identifying%20gaps%20for%20future%20research.%0AIn%20addition%2C%20we%20review%20new%20strategies%20for%20leveraging%20LLMs%2C%20including%20techniques%0Alike%20Half-Quadratic%20Quantization%20%28HQQ%29%2C%20Reinforcement%20Learning%20with%20Human%0AFeedback%20%28RLHF%29%2C%20Direct%20Preference%20Optimization%20%28DPO%29%2C%20Quantized%20Low-Rank%0AAdapters%20%28QLoRA%29%2C%20and%20Retrieval-Augmented%20Generation%20%28RAG%29.%20These%20insights%20aim%0Ato%20enhance%20real-time%20cybersecurity%20defenses%20and%20improve%20the%20sophistication%20of%0ALLM%20applications%20in%20threat%20detection%20and%20response.%20Our%20paper%20provides%20a%0Afoundational%20understanding%20and%20strategic%20direction%20for%20integrating%20LLMs%20into%0Afuture%20cybersecurity%20frameworks%2C%20emphasizing%20innovation%20and%20robust%20model%0Adeployment%20to%20safeguard%20against%20evolving%20cyber%20threats.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12750v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520AI%2520in%2520Cybersecurity%253A%2520A%2520Comprehensive%2520Review%2520of%2520LLM%250A%2520%2520Applications%2520and%2520Vulnerabilities%26entry.906535625%3DMohamed%2520Amine%2520Ferrag%2520and%2520Fatima%2520Alwahedi%2520and%2520Ammar%2520Battah%2520and%2520Bilel%2520Cherif%2520and%2520Abdechakour%2520Mechri%2520and%2520Norbert%2520Tihanyi%2520and%2520Tamas%2520Bisztray%2520and%2520Merouane%2520Debbah%26entry.1292438233%3D%2520%2520This%2520paper%2520provides%2520a%2520comprehensive%2520review%2520of%2520the%2520future%2520of%2520cybersecurity%250Athrough%2520Generative%2520AI%2520and%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520We%2520explore%2520LLM%250Aapplications%2520across%2520various%2520domains%252C%2520including%2520hardware%2520design%2520security%252C%250Aintrusion%2520detection%252C%2520software%2520engineering%252C%2520design%2520verification%252C%2520cyber%2520threat%250Aintelligence%252C%2520malware%2520detection%252C%2520and%2520phishing%2520detection.%2520We%2520present%2520an%2520overview%250Aof%2520LLM%2520evolution%2520and%2520its%2520current%2520state%252C%2520focusing%2520on%2520advancements%2520in%2520models%2520such%250Aas%2520GPT-4%252C%2520GPT-3.5%252C%2520Mixtral-8x7B%252C%2520BERT%252C%2520Falcon2%252C%2520and%2520LLaMA.%2520Our%2520analysis%2520extends%250Ato%2520LLM%2520vulnerabilities%252C%2520such%2520as%2520prompt%2520injection%252C%2520insecure%2520output%2520handling%252C%250Adata%2520poisoning%252C%2520DDoS%2520attacks%252C%2520and%2520adversarial%2520instructions.%2520We%2520delve%2520into%250Amitigation%2520strategies%2520to%2520protect%2520these%2520models%252C%2520providing%2520a%2520comprehensive%2520look%250Aat%2520potential%2520attack%2520scenarios%2520and%2520prevention%2520techniques.%2520Furthermore%252C%2520we%250Aevaluate%2520the%2520performance%2520of%252042%2520LLM%2520models%2520in%2520cybersecurity%2520knowledge%2520and%250Ahardware%2520security%252C%2520highlighting%2520their%2520strengths%2520and%2520weaknesses.%2520We%2520thoroughly%250Aevaluate%2520cybersecurity%2520datasets%2520for%2520LLM%2520training%2520and%2520testing%252C%2520covering%2520the%250Alifecycle%2520from%2520data%2520creation%2520to%2520usage%2520and%2520identifying%2520gaps%2520for%2520future%2520research.%250AIn%2520addition%252C%2520we%2520review%2520new%2520strategies%2520for%2520leveraging%2520LLMs%252C%2520including%2520techniques%250Alike%2520Half-Quadratic%2520Quantization%2520%2528HQQ%2529%252C%2520Reinforcement%2520Learning%2520with%2520Human%250AFeedback%2520%2528RLHF%2529%252C%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529%252C%2520Quantized%2520Low-Rank%250AAdapters%2520%2528QLoRA%2529%252C%2520and%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529.%2520These%2520insights%2520aim%250Ato%2520enhance%2520real-time%2520cybersecurity%2520defenses%2520and%2520improve%2520the%2520sophistication%2520of%250ALLM%2520applications%2520in%2520threat%2520detection%2520and%2520response.%2520Our%2520paper%2520provides%2520a%250Afoundational%2520understanding%2520and%2520strategic%2520direction%2520for%2520integrating%2520LLMs%2520into%250Afuture%2520cybersecurity%2520frameworks%252C%2520emphasizing%2520innovation%2520and%2520robust%2520model%250Adeployment%2520to%2520safeguard%2520against%2520evolving%2520cyber%2520threats.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12750v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20AI%20in%20Cybersecurity%3A%20A%20Comprehensive%20Review%20of%20LLM%0A%20%20Applications%20and%20Vulnerabilities&entry.906535625=Mohamed%20Amine%20Ferrag%20and%20Fatima%20Alwahedi%20and%20Ammar%20Battah%20and%20Bilel%20Cherif%20and%20Abdechakour%20Mechri%20and%20Norbert%20Tihanyi%20and%20Tamas%20Bisztray%20and%20Merouane%20Debbah&entry.1292438233=%20%20This%20paper%20provides%20a%20comprehensive%20review%20of%20the%20future%20of%20cybersecurity%0Athrough%20Generative%20AI%20and%20Large%20Language%20Models%20%28LLMs%29.%20We%20explore%20LLM%0Aapplications%20across%20various%20domains%2C%20including%20hardware%20design%20security%2C%0Aintrusion%20detection%2C%20software%20engineering%2C%20design%20verification%2C%20cyber%20threat%0Aintelligence%2C%20malware%20detection%2C%20and%20phishing%20detection.%20We%20present%20an%20overview%0Aof%20LLM%20evolution%20and%20its%20current%20state%2C%20focusing%20on%20advancements%20in%20models%20such%0Aas%20GPT-4%2C%20GPT-3.5%2C%20Mixtral-8x7B%2C%20BERT%2C%20Falcon2%2C%20and%20LLaMA.%20Our%20analysis%20extends%0Ato%20LLM%20vulnerabilities%2C%20such%20as%20prompt%20injection%2C%20insecure%20output%20handling%2C%0Adata%20poisoning%2C%20DDoS%20attacks%2C%20and%20adversarial%20instructions.%20We%20delve%20into%0Amitigation%20strategies%20to%20protect%20these%20models%2C%20providing%20a%20comprehensive%20look%0Aat%20potential%20attack%20scenarios%20and%20prevention%20techniques.%20Furthermore%2C%20we%0Aevaluate%20the%20performance%20of%2042%20LLM%20models%20in%20cybersecurity%20knowledge%20and%0Ahardware%20security%2C%20highlighting%20their%20strengths%20and%20weaknesses.%20We%20thoroughly%0Aevaluate%20cybersecurity%20datasets%20for%20LLM%20training%20and%20testing%2C%20covering%20the%0Alifecycle%20from%20data%20creation%20to%20usage%20and%20identifying%20gaps%20for%20future%20research.%0AIn%20addition%2C%20we%20review%20new%20strategies%20for%20leveraging%20LLMs%2C%20including%20techniques%0Alike%20Half-Quadratic%20Quantization%20%28HQQ%29%2C%20Reinforcement%20Learning%20with%20Human%0AFeedback%20%28RLHF%29%2C%20Direct%20Preference%20Optimization%20%28DPO%29%2C%20Quantized%20Low-Rank%0AAdapters%20%28QLoRA%29%2C%20and%20Retrieval-Augmented%20Generation%20%28RAG%29.%20These%20insights%20aim%0Ato%20enhance%20real-time%20cybersecurity%20defenses%20and%20improve%20the%20sophistication%20of%0ALLM%20applications%20in%20threat%20detection%20and%20response.%20Our%20paper%20provides%20a%0Afoundational%20understanding%20and%20strategic%20direction%20for%20integrating%20LLMs%20into%0Afuture%20cybersecurity%20frameworks%2C%20emphasizing%20innovation%20and%20robust%20model%0Adeployment%20to%20safeguard%20against%20evolving%20cyber%20threats.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12750v2&entry.124074799=Read"},
{"title": "Hypercone Assisted Contour Generation for Out-of-Distribution Detection", "author": "Annita Vapsi and Andr\u00e9s Mu\u00f1oz and Nancy Thomas and Keshav Ramani and Daniel Borrajo", "abstract": "  Recent advances in the field of out-of-distribution (OOD) detection have\nplaced great emphasis on learning better representations suited to this task.\nWhile there are distance-based approaches, distributional awareness has seldom\nbeen exploited for better performance. We present HAC$_k$-OOD, a novel OOD\ndetection method that makes no distributional assumption about the data, but\nautomatically adapts to its distribution. Specifically, HAC$_k$-OOD constructs\na set of hypercones by maximizing the angular distance to neighbors in a given\ndata-point's vicinity to approximate the contour within which in-distribution\n(ID) data-points lie. Experimental results show state-of-the-art FPR@95 and\nAUROC performance on Near-OOD detection and on Far-OOD detection on the\nchallenging CIFAR-100 benchmark without explicitly training for OOD\nperformance.\n", "link": "http://arxiv.org/abs/2501.10209v1", "date": "2025-01-17", "relevancy": 1.921, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5009}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4821}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4702}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hypercone%20Assisted%20Contour%20Generation%20for%20Out-of-Distribution%20Detection&body=Title%3A%20Hypercone%20Assisted%20Contour%20Generation%20for%20Out-of-Distribution%20Detection%0AAuthor%3A%20Annita%20Vapsi%20and%20Andr%C3%A9s%20Mu%C3%B1oz%20and%20Nancy%20Thomas%20and%20Keshav%20Ramani%20and%20Daniel%20Borrajo%0AAbstract%3A%20%20%20Recent%20advances%20in%20the%20field%20of%20out-of-distribution%20%28OOD%29%20detection%20have%0Aplaced%20great%20emphasis%20on%20learning%20better%20representations%20suited%20to%20this%20task.%0AWhile%20there%20are%20distance-based%20approaches%2C%20distributional%20awareness%20has%20seldom%0Abeen%20exploited%20for%20better%20performance.%20We%20present%20HAC%24_k%24-OOD%2C%20a%20novel%20OOD%0Adetection%20method%20that%20makes%20no%20distributional%20assumption%20about%20the%20data%2C%20but%0Aautomatically%20adapts%20to%20its%20distribution.%20Specifically%2C%20HAC%24_k%24-OOD%20constructs%0Aa%20set%20of%20hypercones%20by%20maximizing%20the%20angular%20distance%20to%20neighbors%20in%20a%20given%0Adata-point%27s%20vicinity%20to%20approximate%20the%20contour%20within%20which%20in-distribution%0A%28ID%29%20data-points%20lie.%20Experimental%20results%20show%20state-of-the-art%20FPR%4095%20and%0AAUROC%20performance%20on%20Near-OOD%20detection%20and%20on%20Far-OOD%20detection%20on%20the%0Achallenging%20CIFAR-100%20benchmark%20without%20explicitly%20training%20for%20OOD%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10209v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHypercone%2520Assisted%2520Contour%2520Generation%2520for%2520Out-of-Distribution%2520Detection%26entry.906535625%3DAnnita%2520Vapsi%2520and%2520Andr%25C3%25A9s%2520Mu%25C3%25B1oz%2520and%2520Nancy%2520Thomas%2520and%2520Keshav%2520Ramani%2520and%2520Daniel%2520Borrajo%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520the%2520field%2520of%2520out-of-distribution%2520%2528OOD%2529%2520detection%2520have%250Aplaced%2520great%2520emphasis%2520on%2520learning%2520better%2520representations%2520suited%2520to%2520this%2520task.%250AWhile%2520there%2520are%2520distance-based%2520approaches%252C%2520distributional%2520awareness%2520has%2520seldom%250Abeen%2520exploited%2520for%2520better%2520performance.%2520We%2520present%2520HAC%2524_k%2524-OOD%252C%2520a%2520novel%2520OOD%250Adetection%2520method%2520that%2520makes%2520no%2520distributional%2520assumption%2520about%2520the%2520data%252C%2520but%250Aautomatically%2520adapts%2520to%2520its%2520distribution.%2520Specifically%252C%2520HAC%2524_k%2524-OOD%2520constructs%250Aa%2520set%2520of%2520hypercones%2520by%2520maximizing%2520the%2520angular%2520distance%2520to%2520neighbors%2520in%2520a%2520given%250Adata-point%2527s%2520vicinity%2520to%2520approximate%2520the%2520contour%2520within%2520which%2520in-distribution%250A%2528ID%2529%2520data-points%2520lie.%2520Experimental%2520results%2520show%2520state-of-the-art%2520FPR%254095%2520and%250AAUROC%2520performance%2520on%2520Near-OOD%2520detection%2520and%2520on%2520Far-OOD%2520detection%2520on%2520the%250Achallenging%2520CIFAR-100%2520benchmark%2520without%2520explicitly%2520training%2520for%2520OOD%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10209v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hypercone%20Assisted%20Contour%20Generation%20for%20Out-of-Distribution%20Detection&entry.906535625=Annita%20Vapsi%20and%20Andr%C3%A9s%20Mu%C3%B1oz%20and%20Nancy%20Thomas%20and%20Keshav%20Ramani%20and%20Daniel%20Borrajo&entry.1292438233=%20%20Recent%20advances%20in%20the%20field%20of%20out-of-distribution%20%28OOD%29%20detection%20have%0Aplaced%20great%20emphasis%20on%20learning%20better%20representations%20suited%20to%20this%20task.%0AWhile%20there%20are%20distance-based%20approaches%2C%20distributional%20awareness%20has%20seldom%0Abeen%20exploited%20for%20better%20performance.%20We%20present%20HAC%24_k%24-OOD%2C%20a%20novel%20OOD%0Adetection%20method%20that%20makes%20no%20distributional%20assumption%20about%20the%20data%2C%20but%0Aautomatically%20adapts%20to%20its%20distribution.%20Specifically%2C%20HAC%24_k%24-OOD%20constructs%0Aa%20set%20of%20hypercones%20by%20maximizing%20the%20angular%20distance%20to%20neighbors%20in%20a%20given%0Adata-point%27s%20vicinity%20to%20approximate%20the%20contour%20within%20which%20in-distribution%0A%28ID%29%20data-points%20lie.%20Experimental%20results%20show%20state-of-the-art%20FPR%4095%20and%0AAUROC%20performance%20on%20Near-OOD%20detection%20and%20on%20Far-OOD%20detection%20on%20the%0Achallenging%20CIFAR-100%20benchmark%20without%20explicitly%20training%20for%20OOD%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10209v1&entry.124074799=Read"},
{"title": "DiffVSR: Enhancing Real-World Video Super-Resolution with Diffusion\n  Models for Advanced Visual Quality and Temporal Consistency", "author": "Xiaohui Li and Yihao Liu and Shuo Cao and Ziyan Chen and Shaobin Zhuang and Xiangyu Chen and Yinan He and Yi Wang and Yu Qiao", "abstract": "  Diffusion models have demonstrated exceptional capabilities in image\ngeneration and restoration, yet their application to video super-resolution\nfaces significant challenges in maintaining both high fidelity and temporal\nconsistency. We present DiffVSR, a diffusion-based framework for real-world\nvideo super-resolution that effectively addresses these challenges through key\ninnovations. For intra-sequence coherence, we develop a multi-scale temporal\nattention module and temporal-enhanced VAE decoder that capture fine-grained\nmotion details. To ensure inter-sequence stability, we introduce a noise\nrescheduling mechanism with an interweaved latent transition approach, which\nenhances temporal consistency without additional training overhead. We propose\na progressive learning strategy that transitions from simple to complex\ndegradations, enabling robust optimization despite limited high-quality video\ndata. Extensive experiments demonstrate that DiffVSR delivers superior results\nin both visual quality and temporal consistency, setting a new performance\nstandard in real-world video super-resolution.\n", "link": "http://arxiv.org/abs/2501.10110v1", "date": "2025-01-17", "relevancy": 1.9131, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6513}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.634}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffVSR%3A%20Enhancing%20Real-World%20Video%20Super-Resolution%20with%20Diffusion%0A%20%20Models%20for%20Advanced%20Visual%20Quality%20and%20Temporal%20Consistency&body=Title%3A%20DiffVSR%3A%20Enhancing%20Real-World%20Video%20Super-Resolution%20with%20Diffusion%0A%20%20Models%20for%20Advanced%20Visual%20Quality%20and%20Temporal%20Consistency%0AAuthor%3A%20Xiaohui%20Li%20and%20Yihao%20Liu%20and%20Shuo%20Cao%20and%20Ziyan%20Chen%20and%20Shaobin%20Zhuang%20and%20Xiangyu%20Chen%20and%20Yinan%20He%20and%20Yi%20Wang%20and%20Yu%20Qiao%0AAbstract%3A%20%20%20Diffusion%20models%20have%20demonstrated%20exceptional%20capabilities%20in%20image%0Ageneration%20and%20restoration%2C%20yet%20their%20application%20to%20video%20super-resolution%0Afaces%20significant%20challenges%20in%20maintaining%20both%20high%20fidelity%20and%20temporal%0Aconsistency.%20We%20present%20DiffVSR%2C%20a%20diffusion-based%20framework%20for%20real-world%0Avideo%20super-resolution%20that%20effectively%20addresses%20these%20challenges%20through%20key%0Ainnovations.%20For%20intra-sequence%20coherence%2C%20we%20develop%20a%20multi-scale%20temporal%0Aattention%20module%20and%20temporal-enhanced%20VAE%20decoder%20that%20capture%20fine-grained%0Amotion%20details.%20To%20ensure%20inter-sequence%20stability%2C%20we%20introduce%20a%20noise%0Arescheduling%20mechanism%20with%20an%20interweaved%20latent%20transition%20approach%2C%20which%0Aenhances%20temporal%20consistency%20without%20additional%20training%20overhead.%20We%20propose%0Aa%20progressive%20learning%20strategy%20that%20transitions%20from%20simple%20to%20complex%0Adegradations%2C%20enabling%20robust%20optimization%20despite%20limited%20high-quality%20video%0Adata.%20Extensive%20experiments%20demonstrate%20that%20DiffVSR%20delivers%20superior%20results%0Ain%20both%20visual%20quality%20and%20temporal%20consistency%2C%20setting%20a%20new%20performance%0Astandard%20in%20real-world%20video%20super-resolution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10110v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffVSR%253A%2520Enhancing%2520Real-World%2520Video%2520Super-Resolution%2520with%2520Diffusion%250A%2520%2520Models%2520for%2520Advanced%2520Visual%2520Quality%2520and%2520Temporal%2520Consistency%26entry.906535625%3DXiaohui%2520Li%2520and%2520Yihao%2520Liu%2520and%2520Shuo%2520Cao%2520and%2520Ziyan%2520Chen%2520and%2520Shaobin%2520Zhuang%2520and%2520Xiangyu%2520Chen%2520and%2520Yinan%2520He%2520and%2520Yi%2520Wang%2520and%2520Yu%2520Qiao%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520demonstrated%2520exceptional%2520capabilities%2520in%2520image%250Ageneration%2520and%2520restoration%252C%2520yet%2520their%2520application%2520to%2520video%2520super-resolution%250Afaces%2520significant%2520challenges%2520in%2520maintaining%2520both%2520high%2520fidelity%2520and%2520temporal%250Aconsistency.%2520We%2520present%2520DiffVSR%252C%2520a%2520diffusion-based%2520framework%2520for%2520real-world%250Avideo%2520super-resolution%2520that%2520effectively%2520addresses%2520these%2520challenges%2520through%2520key%250Ainnovations.%2520For%2520intra-sequence%2520coherence%252C%2520we%2520develop%2520a%2520multi-scale%2520temporal%250Aattention%2520module%2520and%2520temporal-enhanced%2520VAE%2520decoder%2520that%2520capture%2520fine-grained%250Amotion%2520details.%2520To%2520ensure%2520inter-sequence%2520stability%252C%2520we%2520introduce%2520a%2520noise%250Arescheduling%2520mechanism%2520with%2520an%2520interweaved%2520latent%2520transition%2520approach%252C%2520which%250Aenhances%2520temporal%2520consistency%2520without%2520additional%2520training%2520overhead.%2520We%2520propose%250Aa%2520progressive%2520learning%2520strategy%2520that%2520transitions%2520from%2520simple%2520to%2520complex%250Adegradations%252C%2520enabling%2520robust%2520optimization%2520despite%2520limited%2520high-quality%2520video%250Adata.%2520Extensive%2520experiments%2520demonstrate%2520that%2520DiffVSR%2520delivers%2520superior%2520results%250Ain%2520both%2520visual%2520quality%2520and%2520temporal%2520consistency%252C%2520setting%2520a%2520new%2520performance%250Astandard%2520in%2520real-world%2520video%2520super-resolution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10110v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffVSR%3A%20Enhancing%20Real-World%20Video%20Super-Resolution%20with%20Diffusion%0A%20%20Models%20for%20Advanced%20Visual%20Quality%20and%20Temporal%20Consistency&entry.906535625=Xiaohui%20Li%20and%20Yihao%20Liu%20and%20Shuo%20Cao%20and%20Ziyan%20Chen%20and%20Shaobin%20Zhuang%20and%20Xiangyu%20Chen%20and%20Yinan%20He%20and%20Yi%20Wang%20and%20Yu%20Qiao&entry.1292438233=%20%20Diffusion%20models%20have%20demonstrated%20exceptional%20capabilities%20in%20image%0Ageneration%20and%20restoration%2C%20yet%20their%20application%20to%20video%20super-resolution%0Afaces%20significant%20challenges%20in%20maintaining%20both%20high%20fidelity%20and%20temporal%0Aconsistency.%20We%20present%20DiffVSR%2C%20a%20diffusion-based%20framework%20for%20real-world%0Avideo%20super-resolution%20that%20effectively%20addresses%20these%20challenges%20through%20key%0Ainnovations.%20For%20intra-sequence%20coherence%2C%20we%20develop%20a%20multi-scale%20temporal%0Aattention%20module%20and%20temporal-enhanced%20VAE%20decoder%20that%20capture%20fine-grained%0Amotion%20details.%20To%20ensure%20inter-sequence%20stability%2C%20we%20introduce%20a%20noise%0Arescheduling%20mechanism%20with%20an%20interweaved%20latent%20transition%20approach%2C%20which%0Aenhances%20temporal%20consistency%20without%20additional%20training%20overhead.%20We%20propose%0Aa%20progressive%20learning%20strategy%20that%20transitions%20from%20simple%20to%20complex%0Adegradations%2C%20enabling%20robust%20optimization%20despite%20limited%20high-quality%20video%0Adata.%20Extensive%20experiments%20demonstrate%20that%20DiffVSR%20delivers%20superior%20results%0Ain%20both%20visual%20quality%20and%20temporal%20consistency%2C%20setting%20a%20new%20performance%0Astandard%20in%20real-world%20video%20super-resolution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10110v1&entry.124074799=Read"},
{"title": "How Redundant Is the Transformer Stack in Speech Representation Models?", "author": "Teresa Dorszewski and Albert Kj\u00f8ller Jacobsen and Lenka T\u011btkov\u00e1 and Lars Kai Hansen", "abstract": "  Self-supervised speech representation models, particularly those leveraging\ntransformer architectures, have demonstrated remarkable performance across\nvarious tasks such as speech recognition, speaker identification, and emotion\ndetection. Recent studies on transformer models revealed a high redundancy\nbetween layers and the potential for significant pruning, which we will\ninvestigate here for transformer-based speech representation models. We perform\na detailed analysis of layer similarity in speech representation models using\nthree similarity metrics: cosine similarity, centered kernel alignment, and\nmutual nearest-neighbor alignment. Our findings reveal a block-like structure\nof high similarity, suggesting two main processing steps and significant\nredundancy of layers. We demonstrate the effectiveness of pruning\ntransformer-based speech representation models without the need for\npost-training, achieving up to 40% reduction in transformer layers while\nmaintaining over 95% of the model's predictive capacity. Furthermore, we employ\na knowledge distillation method to substitute the entire transformer stack with\nmimicking layers, reducing the network size 95-98% and the inference time by up\nto 94%. This substantial decrease in computational load occurs without\nconsiderable performance loss, suggesting that the transformer stack is almost\ncompletely redundant for downstream applications of speech representation\nmodels.\n", "link": "http://arxiv.org/abs/2409.16302v2", "date": "2025-01-17", "relevancy": 1.9068, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5683}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4658}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Redundant%20Is%20the%20Transformer%20Stack%20in%20Speech%20Representation%20Models%3F&body=Title%3A%20How%20Redundant%20Is%20the%20Transformer%20Stack%20in%20Speech%20Representation%20Models%3F%0AAuthor%3A%20Teresa%20Dorszewski%20and%20Albert%20Kj%C3%B8ller%20Jacobsen%20and%20Lenka%20T%C4%9Btkov%C3%A1%20and%20Lars%20Kai%20Hansen%0AAbstract%3A%20%20%20Self-supervised%20speech%20representation%20models%2C%20particularly%20those%20leveraging%0Atransformer%20architectures%2C%20have%20demonstrated%20remarkable%20performance%20across%0Avarious%20tasks%20such%20as%20speech%20recognition%2C%20speaker%20identification%2C%20and%20emotion%0Adetection.%20Recent%20studies%20on%20transformer%20models%20revealed%20a%20high%20redundancy%0Abetween%20layers%20and%20the%20potential%20for%20significant%20pruning%2C%20which%20we%20will%0Ainvestigate%20here%20for%20transformer-based%20speech%20representation%20models.%20We%20perform%0Aa%20detailed%20analysis%20of%20layer%20similarity%20in%20speech%20representation%20models%20using%0Athree%20similarity%20metrics%3A%20cosine%20similarity%2C%20centered%20kernel%20alignment%2C%20and%0Amutual%20nearest-neighbor%20alignment.%20Our%20findings%20reveal%20a%20block-like%20structure%0Aof%20high%20similarity%2C%20suggesting%20two%20main%20processing%20steps%20and%20significant%0Aredundancy%20of%20layers.%20We%20demonstrate%20the%20effectiveness%20of%20pruning%0Atransformer-based%20speech%20representation%20models%20without%20the%20need%20for%0Apost-training%2C%20achieving%20up%20to%2040%25%20reduction%20in%20transformer%20layers%20while%0Amaintaining%20over%2095%25%20of%20the%20model%27s%20predictive%20capacity.%20Furthermore%2C%20we%20employ%0Aa%20knowledge%20distillation%20method%20to%20substitute%20the%20entire%20transformer%20stack%20with%0Amimicking%20layers%2C%20reducing%20the%20network%20size%2095-98%25%20and%20the%20inference%20time%20by%20up%0Ato%2094%25.%20This%20substantial%20decrease%20in%20computational%20load%20occurs%20without%0Aconsiderable%20performance%20loss%2C%20suggesting%20that%20the%20transformer%20stack%20is%20almost%0Acompletely%20redundant%20for%20downstream%20applications%20of%20speech%20representation%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16302v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Redundant%2520Is%2520the%2520Transformer%2520Stack%2520in%2520Speech%2520Representation%2520Models%253F%26entry.906535625%3DTeresa%2520Dorszewski%2520and%2520Albert%2520Kj%25C3%25B8ller%2520Jacobsen%2520and%2520Lenka%2520T%25C4%259Btkov%25C3%25A1%2520and%2520Lars%2520Kai%2520Hansen%26entry.1292438233%3D%2520%2520Self-supervised%2520speech%2520representation%2520models%252C%2520particularly%2520those%2520leveraging%250Atransformer%2520architectures%252C%2520have%2520demonstrated%2520remarkable%2520performance%2520across%250Avarious%2520tasks%2520such%2520as%2520speech%2520recognition%252C%2520speaker%2520identification%252C%2520and%2520emotion%250Adetection.%2520Recent%2520studies%2520on%2520transformer%2520models%2520revealed%2520a%2520high%2520redundancy%250Abetween%2520layers%2520and%2520the%2520potential%2520for%2520significant%2520pruning%252C%2520which%2520we%2520will%250Ainvestigate%2520here%2520for%2520transformer-based%2520speech%2520representation%2520models.%2520We%2520perform%250Aa%2520detailed%2520analysis%2520of%2520layer%2520similarity%2520in%2520speech%2520representation%2520models%2520using%250Athree%2520similarity%2520metrics%253A%2520cosine%2520similarity%252C%2520centered%2520kernel%2520alignment%252C%2520and%250Amutual%2520nearest-neighbor%2520alignment.%2520Our%2520findings%2520reveal%2520a%2520block-like%2520structure%250Aof%2520high%2520similarity%252C%2520suggesting%2520two%2520main%2520processing%2520steps%2520and%2520significant%250Aredundancy%2520of%2520layers.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520pruning%250Atransformer-based%2520speech%2520representation%2520models%2520without%2520the%2520need%2520for%250Apost-training%252C%2520achieving%2520up%2520to%252040%2525%2520reduction%2520in%2520transformer%2520layers%2520while%250Amaintaining%2520over%252095%2525%2520of%2520the%2520model%2527s%2520predictive%2520capacity.%2520Furthermore%252C%2520we%2520employ%250Aa%2520knowledge%2520distillation%2520method%2520to%2520substitute%2520the%2520entire%2520transformer%2520stack%2520with%250Amimicking%2520layers%252C%2520reducing%2520the%2520network%2520size%252095-98%2525%2520and%2520the%2520inference%2520time%2520by%2520up%250Ato%252094%2525.%2520This%2520substantial%2520decrease%2520in%2520computational%2520load%2520occurs%2520without%250Aconsiderable%2520performance%2520loss%252C%2520suggesting%2520that%2520the%2520transformer%2520stack%2520is%2520almost%250Acompletely%2520redundant%2520for%2520downstream%2520applications%2520of%2520speech%2520representation%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16302v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Redundant%20Is%20the%20Transformer%20Stack%20in%20Speech%20Representation%20Models%3F&entry.906535625=Teresa%20Dorszewski%20and%20Albert%20Kj%C3%B8ller%20Jacobsen%20and%20Lenka%20T%C4%9Btkov%C3%A1%20and%20Lars%20Kai%20Hansen&entry.1292438233=%20%20Self-supervised%20speech%20representation%20models%2C%20particularly%20those%20leveraging%0Atransformer%20architectures%2C%20have%20demonstrated%20remarkable%20performance%20across%0Avarious%20tasks%20such%20as%20speech%20recognition%2C%20speaker%20identification%2C%20and%20emotion%0Adetection.%20Recent%20studies%20on%20transformer%20models%20revealed%20a%20high%20redundancy%0Abetween%20layers%20and%20the%20potential%20for%20significant%20pruning%2C%20which%20we%20will%0Ainvestigate%20here%20for%20transformer-based%20speech%20representation%20models.%20We%20perform%0Aa%20detailed%20analysis%20of%20layer%20similarity%20in%20speech%20representation%20models%20using%0Athree%20similarity%20metrics%3A%20cosine%20similarity%2C%20centered%20kernel%20alignment%2C%20and%0Amutual%20nearest-neighbor%20alignment.%20Our%20findings%20reveal%20a%20block-like%20structure%0Aof%20high%20similarity%2C%20suggesting%20two%20main%20processing%20steps%20and%20significant%0Aredundancy%20of%20layers.%20We%20demonstrate%20the%20effectiveness%20of%20pruning%0Atransformer-based%20speech%20representation%20models%20without%20the%20need%20for%0Apost-training%2C%20achieving%20up%20to%2040%25%20reduction%20in%20transformer%20layers%20while%0Amaintaining%20over%2095%25%20of%20the%20model%27s%20predictive%20capacity.%20Furthermore%2C%20we%20employ%0Aa%20knowledge%20distillation%20method%20to%20substitute%20the%20entire%20transformer%20stack%20with%0Amimicking%20layers%2C%20reducing%20the%20network%20size%2095-98%25%20and%20the%20inference%20time%20by%20up%0Ato%2094%25.%20This%20substantial%20decrease%20in%20computational%20load%20occurs%20without%0Aconsiderable%20performance%20loss%2C%20suggesting%20that%20the%20transformer%20stack%20is%20almost%0Acompletely%20redundant%20for%20downstream%20applications%20of%20speech%20representation%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16302v2&entry.124074799=Read"},
{"title": "Surrogate-based multiscale analysis of experiments on thermoplastic\n  composites under off-axis loading", "author": "M. A. Maia and I. B. C. M. Rocha and D. Kova\u010devi\u0107 and F. P. van der Meer", "abstract": "  In this paper, we present a surrogate-based multiscale approach to model\nconstant strain-rate and creep experiments on unidirectional thermoplastic\ncomposites under off-axis loading. In previous contributions, these experiments\nwere modeled through a single-scale micromechanical simulation under the\nassumption of macroscopic homogeneity. Although efficient and accurate in many\nscenarios, simulations with low-off axis angles showed significant\ndiscrepancies with the experiments. It was hypothesized that the mismatch was\ncaused by macroscopic inhomogeneity, which would require a multiscale approach\nto capture it. However, full-field multiscale simulations remain\ncomputationally prohibitive. To address this issue, we replace the micromodel\nwith a Physically Recurrent Neural Network (PRNN), a surrogate model that\ncombines data-driven components with embedded constitutive models to capture\nhistory-dependent behavior naturally. The explainability of the latent space of\nthis network is also explored in a transfer learning strategy that requires no\nre-training. With the surrogate-based simulations, we confirm the hypothesis\nraised on the inhomogeneity of the macroscopic strain field and gain insights\ninto the influence of adjustment of the experimental setup with oblique\nend-tabs. Results from the surrogate-based multiscale approach show better\nagreement with experiments than the single-scale micromechanical approach over\na wide range of settings, although with limited accuracy on the creep\nexperiments, where macroscopic test effects were implicitly taken into account\nin the material properties calibration.\n", "link": "http://arxiv.org/abs/2501.10193v1", "date": "2025-01-17", "relevancy": 1.906, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5266}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4691}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4638}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Surrogate-based%20multiscale%20analysis%20of%20experiments%20on%20thermoplastic%0A%20%20composites%20under%20off-axis%20loading&body=Title%3A%20Surrogate-based%20multiscale%20analysis%20of%20experiments%20on%20thermoplastic%0A%20%20composites%20under%20off-axis%20loading%0AAuthor%3A%20M.%20A.%20Maia%20and%20I.%20B.%20C.%20M.%20Rocha%20and%20D.%20Kova%C4%8Devi%C4%87%20and%20F.%20P.%20van%20der%20Meer%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20surrogate-based%20multiscale%20approach%20to%20model%0Aconstant%20strain-rate%20and%20creep%20experiments%20on%20unidirectional%20thermoplastic%0Acomposites%20under%20off-axis%20loading.%20In%20previous%20contributions%2C%20these%20experiments%0Awere%20modeled%20through%20a%20single-scale%20micromechanical%20simulation%20under%20the%0Aassumption%20of%20macroscopic%20homogeneity.%20Although%20efficient%20and%20accurate%20in%20many%0Ascenarios%2C%20simulations%20with%20low-off%20axis%20angles%20showed%20significant%0Adiscrepancies%20with%20the%20experiments.%20It%20was%20hypothesized%20that%20the%20mismatch%20was%0Acaused%20by%20macroscopic%20inhomogeneity%2C%20which%20would%20require%20a%20multiscale%20approach%0Ato%20capture%20it.%20However%2C%20full-field%20multiscale%20simulations%20remain%0Acomputationally%20prohibitive.%20To%20address%20this%20issue%2C%20we%20replace%20the%20micromodel%0Awith%20a%20Physically%20Recurrent%20Neural%20Network%20%28PRNN%29%2C%20a%20surrogate%20model%20that%0Acombines%20data-driven%20components%20with%20embedded%20constitutive%20models%20to%20capture%0Ahistory-dependent%20behavior%20naturally.%20The%20explainability%20of%20the%20latent%20space%20of%0Athis%20network%20is%20also%20explored%20in%20a%20transfer%20learning%20strategy%20that%20requires%20no%0Are-training.%20With%20the%20surrogate-based%20simulations%2C%20we%20confirm%20the%20hypothesis%0Araised%20on%20the%20inhomogeneity%20of%20the%20macroscopic%20strain%20field%20and%20gain%20insights%0Ainto%20the%20influence%20of%20adjustment%20of%20the%20experimental%20setup%20with%20oblique%0Aend-tabs.%20Results%20from%20the%20surrogate-based%20multiscale%20approach%20show%20better%0Aagreement%20with%20experiments%20than%20the%20single-scale%20micromechanical%20approach%20over%0Aa%20wide%20range%20of%20settings%2C%20although%20with%20limited%20accuracy%20on%20the%20creep%0Aexperiments%2C%20where%20macroscopic%20test%20effects%20were%20implicitly%20taken%20into%20account%0Ain%20the%20material%20properties%20calibration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10193v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurrogate-based%2520multiscale%2520analysis%2520of%2520experiments%2520on%2520thermoplastic%250A%2520%2520composites%2520under%2520off-axis%2520loading%26entry.906535625%3DM.%2520A.%2520Maia%2520and%2520I.%2520B.%2520C.%2520M.%2520Rocha%2520and%2520D.%2520Kova%25C4%258Devi%25C4%2587%2520and%2520F.%2520P.%2520van%2520der%2520Meer%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520surrogate-based%2520multiscale%2520approach%2520to%2520model%250Aconstant%2520strain-rate%2520and%2520creep%2520experiments%2520on%2520unidirectional%2520thermoplastic%250Acomposites%2520under%2520off-axis%2520loading.%2520In%2520previous%2520contributions%252C%2520these%2520experiments%250Awere%2520modeled%2520through%2520a%2520single-scale%2520micromechanical%2520simulation%2520under%2520the%250Aassumption%2520of%2520macroscopic%2520homogeneity.%2520Although%2520efficient%2520and%2520accurate%2520in%2520many%250Ascenarios%252C%2520simulations%2520with%2520low-off%2520axis%2520angles%2520showed%2520significant%250Adiscrepancies%2520with%2520the%2520experiments.%2520It%2520was%2520hypothesized%2520that%2520the%2520mismatch%2520was%250Acaused%2520by%2520macroscopic%2520inhomogeneity%252C%2520which%2520would%2520require%2520a%2520multiscale%2520approach%250Ato%2520capture%2520it.%2520However%252C%2520full-field%2520multiscale%2520simulations%2520remain%250Acomputationally%2520prohibitive.%2520To%2520address%2520this%2520issue%252C%2520we%2520replace%2520the%2520micromodel%250Awith%2520a%2520Physically%2520Recurrent%2520Neural%2520Network%2520%2528PRNN%2529%252C%2520a%2520surrogate%2520model%2520that%250Acombines%2520data-driven%2520components%2520with%2520embedded%2520constitutive%2520models%2520to%2520capture%250Ahistory-dependent%2520behavior%2520naturally.%2520The%2520explainability%2520of%2520the%2520latent%2520space%2520of%250Athis%2520network%2520is%2520also%2520explored%2520in%2520a%2520transfer%2520learning%2520strategy%2520that%2520requires%2520no%250Are-training.%2520With%2520the%2520surrogate-based%2520simulations%252C%2520we%2520confirm%2520the%2520hypothesis%250Araised%2520on%2520the%2520inhomogeneity%2520of%2520the%2520macroscopic%2520strain%2520field%2520and%2520gain%2520insights%250Ainto%2520the%2520influence%2520of%2520adjustment%2520of%2520the%2520experimental%2520setup%2520with%2520oblique%250Aend-tabs.%2520Results%2520from%2520the%2520surrogate-based%2520multiscale%2520approach%2520show%2520better%250Aagreement%2520with%2520experiments%2520than%2520the%2520single-scale%2520micromechanical%2520approach%2520over%250Aa%2520wide%2520range%2520of%2520settings%252C%2520although%2520with%2520limited%2520accuracy%2520on%2520the%2520creep%250Aexperiments%252C%2520where%2520macroscopic%2520test%2520effects%2520were%2520implicitly%2520taken%2520into%2520account%250Ain%2520the%2520material%2520properties%2520calibration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10193v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Surrogate-based%20multiscale%20analysis%20of%20experiments%20on%20thermoplastic%0A%20%20composites%20under%20off-axis%20loading&entry.906535625=M.%20A.%20Maia%20and%20I.%20B.%20C.%20M.%20Rocha%20and%20D.%20Kova%C4%8Devi%C4%87%20and%20F.%20P.%20van%20der%20Meer&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20surrogate-based%20multiscale%20approach%20to%20model%0Aconstant%20strain-rate%20and%20creep%20experiments%20on%20unidirectional%20thermoplastic%0Acomposites%20under%20off-axis%20loading.%20In%20previous%20contributions%2C%20these%20experiments%0Awere%20modeled%20through%20a%20single-scale%20micromechanical%20simulation%20under%20the%0Aassumption%20of%20macroscopic%20homogeneity.%20Although%20efficient%20and%20accurate%20in%20many%0Ascenarios%2C%20simulations%20with%20low-off%20axis%20angles%20showed%20significant%0Adiscrepancies%20with%20the%20experiments.%20It%20was%20hypothesized%20that%20the%20mismatch%20was%0Acaused%20by%20macroscopic%20inhomogeneity%2C%20which%20would%20require%20a%20multiscale%20approach%0Ato%20capture%20it.%20However%2C%20full-field%20multiscale%20simulations%20remain%0Acomputationally%20prohibitive.%20To%20address%20this%20issue%2C%20we%20replace%20the%20micromodel%0Awith%20a%20Physically%20Recurrent%20Neural%20Network%20%28PRNN%29%2C%20a%20surrogate%20model%20that%0Acombines%20data-driven%20components%20with%20embedded%20constitutive%20models%20to%20capture%0Ahistory-dependent%20behavior%20naturally.%20The%20explainability%20of%20the%20latent%20space%20of%0Athis%20network%20is%20also%20explored%20in%20a%20transfer%20learning%20strategy%20that%20requires%20no%0Are-training.%20With%20the%20surrogate-based%20simulations%2C%20we%20confirm%20the%20hypothesis%0Araised%20on%20the%20inhomogeneity%20of%20the%20macroscopic%20strain%20field%20and%20gain%20insights%0Ainto%20the%20influence%20of%20adjustment%20of%20the%20experimental%20setup%20with%20oblique%0Aend-tabs.%20Results%20from%20the%20surrogate-based%20multiscale%20approach%20show%20better%0Aagreement%20with%20experiments%20than%20the%20single-scale%20micromechanical%20approach%20over%0Aa%20wide%20range%20of%20settings%2C%20although%20with%20limited%20accuracy%20on%20the%20creep%0Aexperiments%2C%20where%20macroscopic%20test%20effects%20were%20implicitly%20taken%20into%20account%0Ain%20the%20material%20properties%20calibration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10193v1&entry.124074799=Read"},
{"title": "A Fairness-Oriented Reinforcement Learning Approach for the Operation\n  and Control of Shared Micromobility Services", "author": "Matteo Cederle and Luca Vittorio Piron and Marina Ceccon and Federico Chiariotti and Alessandro Fabris and Marco Fabris and Gian Antonio Susto", "abstract": "  As Machine Learning grows in popularity across various fields, equity has\nbecome a key focus for the AI community. However, fairness-oriented approaches\nare still underexplored in smart mobility. Addressing this gap, our study\ninvestigates the balance between performance optimization and algorithmic\nfairness in shared micromobility services providing a novel framework based on\nReinforcement Learning. Exploiting Q-learning, the proposed methodology\nachieves equitable outcomes in terms of the Gini index across different areas\ncharacterized by their distance from central hubs. Through vehicle rebalancing,\nthe provided scheme maximizes operator performance while ensuring fairness\nprinciples for users, reducing iniquity by up to 85% while only increasing\ncosts by 30% (w.r.t. applying no equity adjustment). A case study with\nsynthetic data validates our insights and highlights the importance of fairness\nin urban micromobility (source code:\nhttps://github.com/mcederle99/FairMSS.git).\n", "link": "http://arxiv.org/abs/2403.15780v3", "date": "2025-01-17", "relevancy": 1.9048, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5156}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4699}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4667}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Fairness-Oriented%20Reinforcement%20Learning%20Approach%20for%20the%20Operation%0A%20%20and%20Control%20of%20Shared%20Micromobility%20Services&body=Title%3A%20A%20Fairness-Oriented%20Reinforcement%20Learning%20Approach%20for%20the%20Operation%0A%20%20and%20Control%20of%20Shared%20Micromobility%20Services%0AAuthor%3A%20Matteo%20Cederle%20and%20Luca%20Vittorio%20Piron%20and%20Marina%20Ceccon%20and%20Federico%20Chiariotti%20and%20Alessandro%20Fabris%20and%20Marco%20Fabris%20and%20Gian%20Antonio%20Susto%0AAbstract%3A%20%20%20As%20Machine%20Learning%20grows%20in%20popularity%20across%20various%20fields%2C%20equity%20has%0Abecome%20a%20key%20focus%20for%20the%20AI%20community.%20However%2C%20fairness-oriented%20approaches%0Aare%20still%20underexplored%20in%20smart%20mobility.%20Addressing%20this%20gap%2C%20our%20study%0Ainvestigates%20the%20balance%20between%20performance%20optimization%20and%20algorithmic%0Afairness%20in%20shared%20micromobility%20services%20providing%20a%20novel%20framework%20based%20on%0AReinforcement%20Learning.%20Exploiting%20Q-learning%2C%20the%20proposed%20methodology%0Aachieves%20equitable%20outcomes%20in%20terms%20of%20the%20Gini%20index%20across%20different%20areas%0Acharacterized%20by%20their%20distance%20from%20central%20hubs.%20Through%20vehicle%20rebalancing%2C%0Athe%20provided%20scheme%20maximizes%20operator%20performance%20while%20ensuring%20fairness%0Aprinciples%20for%20users%2C%20reducing%20iniquity%20by%20up%20to%2085%25%20while%20only%20increasing%0Acosts%20by%2030%25%20%28w.r.t.%20applying%20no%20equity%20adjustment%29.%20A%20case%20study%20with%0Asynthetic%20data%20validates%20our%20insights%20and%20highlights%20the%20importance%20of%20fairness%0Ain%20urban%20micromobility%20%28source%20code%3A%0Ahttps%3A//github.com/mcederle99/FairMSS.git%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15780v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Fairness-Oriented%2520Reinforcement%2520Learning%2520Approach%2520for%2520the%2520Operation%250A%2520%2520and%2520Control%2520of%2520Shared%2520Micromobility%2520Services%26entry.906535625%3DMatteo%2520Cederle%2520and%2520Luca%2520Vittorio%2520Piron%2520and%2520Marina%2520Ceccon%2520and%2520Federico%2520Chiariotti%2520and%2520Alessandro%2520Fabris%2520and%2520Marco%2520Fabris%2520and%2520Gian%2520Antonio%2520Susto%26entry.1292438233%3D%2520%2520As%2520Machine%2520Learning%2520grows%2520in%2520popularity%2520across%2520various%2520fields%252C%2520equity%2520has%250Abecome%2520a%2520key%2520focus%2520for%2520the%2520AI%2520community.%2520However%252C%2520fairness-oriented%2520approaches%250Aare%2520still%2520underexplored%2520in%2520smart%2520mobility.%2520Addressing%2520this%2520gap%252C%2520our%2520study%250Ainvestigates%2520the%2520balance%2520between%2520performance%2520optimization%2520and%2520algorithmic%250Afairness%2520in%2520shared%2520micromobility%2520services%2520providing%2520a%2520novel%2520framework%2520based%2520on%250AReinforcement%2520Learning.%2520Exploiting%2520Q-learning%252C%2520the%2520proposed%2520methodology%250Aachieves%2520equitable%2520outcomes%2520in%2520terms%2520of%2520the%2520Gini%2520index%2520across%2520different%2520areas%250Acharacterized%2520by%2520their%2520distance%2520from%2520central%2520hubs.%2520Through%2520vehicle%2520rebalancing%252C%250Athe%2520provided%2520scheme%2520maximizes%2520operator%2520performance%2520while%2520ensuring%2520fairness%250Aprinciples%2520for%2520users%252C%2520reducing%2520iniquity%2520by%2520up%2520to%252085%2525%2520while%2520only%2520increasing%250Acosts%2520by%252030%2525%2520%2528w.r.t.%2520applying%2520no%2520equity%2520adjustment%2529.%2520A%2520case%2520study%2520with%250Asynthetic%2520data%2520validates%2520our%2520insights%2520and%2520highlights%2520the%2520importance%2520of%2520fairness%250Ain%2520urban%2520micromobility%2520%2528source%2520code%253A%250Ahttps%253A//github.com/mcederle99/FairMSS.git%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.15780v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Fairness-Oriented%20Reinforcement%20Learning%20Approach%20for%20the%20Operation%0A%20%20and%20Control%20of%20Shared%20Micromobility%20Services&entry.906535625=Matteo%20Cederle%20and%20Luca%20Vittorio%20Piron%20and%20Marina%20Ceccon%20and%20Federico%20Chiariotti%20and%20Alessandro%20Fabris%20and%20Marco%20Fabris%20and%20Gian%20Antonio%20Susto&entry.1292438233=%20%20As%20Machine%20Learning%20grows%20in%20popularity%20across%20various%20fields%2C%20equity%20has%0Abecome%20a%20key%20focus%20for%20the%20AI%20community.%20However%2C%20fairness-oriented%20approaches%0Aare%20still%20underexplored%20in%20smart%20mobility.%20Addressing%20this%20gap%2C%20our%20study%0Ainvestigates%20the%20balance%20between%20performance%20optimization%20and%20algorithmic%0Afairness%20in%20shared%20micromobility%20services%20providing%20a%20novel%20framework%20based%20on%0AReinforcement%20Learning.%20Exploiting%20Q-learning%2C%20the%20proposed%20methodology%0Aachieves%20equitable%20outcomes%20in%20terms%20of%20the%20Gini%20index%20across%20different%20areas%0Acharacterized%20by%20their%20distance%20from%20central%20hubs.%20Through%20vehicle%20rebalancing%2C%0Athe%20provided%20scheme%20maximizes%20operator%20performance%20while%20ensuring%20fairness%0Aprinciples%20for%20users%2C%20reducing%20iniquity%20by%20up%20to%2085%25%20while%20only%20increasing%0Acosts%20by%2030%25%20%28w.r.t.%20applying%20no%20equity%20adjustment%29.%20A%20case%20study%20with%0Asynthetic%20data%20validates%20our%20insights%20and%20highlights%20the%20importance%20of%20fairness%0Ain%20urban%20micromobility%20%28source%20code%3A%0Ahttps%3A//github.com/mcederle99/FairMSS.git%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15780v3&entry.124074799=Read"},
{"title": "Credit Risk Identification in Supply Chains Using Generative Adversarial\n  Networks", "author": "Zizhou Zhang and Xinshi Li and Yu Cheng and Zhenrui Chen and Qianying Liu", "abstract": "  Credit risk management within supply chains has emerged as a critical\nresearch area due to its significant implications for operational stability and\nfinancial sustainability. The intricate interdependencies among supply chain\nparticipants mean that credit risks can propagate across networks, with impacts\nvarying by industry. This study explores the application of Generative\nAdversarial Networks (GANs) to enhance credit risk identification in supply\nchains. GANs enable the generation of synthetic credit risk scenarios,\naddressing challenges related to data scarcity and imbalanced datasets. By\nleveraging GAN-generated data, the model improves predictive accuracy while\neffectively capturing dynamic and temporal dependencies in supply chain data.\nThe research focuses on three representative industries-manufacturing (steel),\ndistribution (pharmaceuticals), and services (e-commerce) to assess\nindustry-specific credit risk contagion. Experimental results demonstrate that\nthe GAN-based model outperforms traditional methods, including logistic\nregression, decision trees, and neural networks, achieving superior accuracy,\nrecall, and F1 scores. The findings underscore the potential of GANs in\nproactive risk management, offering robust tools for mitigating financial\ndisruptions in supply chains. Future research could expand the model by\nincorporating external market factors and supplier relationships to further\nenhance predictive capabilities. Keywords- Generative Adversarial Networks\n(GANs); Supply Chain Risk; Credit Risk Identification; Machine Learning; Data\nAugmentation\n", "link": "http://arxiv.org/abs/2501.10348v1", "date": "2025-01-17", "relevancy": 1.9026, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4929}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4734}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Credit%20Risk%20Identification%20in%20Supply%20Chains%20Using%20Generative%20Adversarial%0A%20%20Networks&body=Title%3A%20Credit%20Risk%20Identification%20in%20Supply%20Chains%20Using%20Generative%20Adversarial%0A%20%20Networks%0AAuthor%3A%20Zizhou%20Zhang%20and%20Xinshi%20Li%20and%20Yu%20Cheng%20and%20Zhenrui%20Chen%20and%20Qianying%20Liu%0AAbstract%3A%20%20%20Credit%20risk%20management%20within%20supply%20chains%20has%20emerged%20as%20a%20critical%0Aresearch%20area%20due%20to%20its%20significant%20implications%20for%20operational%20stability%20and%0Afinancial%20sustainability.%20The%20intricate%20interdependencies%20among%20supply%20chain%0Aparticipants%20mean%20that%20credit%20risks%20can%20propagate%20across%20networks%2C%20with%20impacts%0Avarying%20by%20industry.%20This%20study%20explores%20the%20application%20of%20Generative%0AAdversarial%20Networks%20%28GANs%29%20to%20enhance%20credit%20risk%20identification%20in%20supply%0Achains.%20GANs%20enable%20the%20generation%20of%20synthetic%20credit%20risk%20scenarios%2C%0Aaddressing%20challenges%20related%20to%20data%20scarcity%20and%20imbalanced%20datasets.%20By%0Aleveraging%20GAN-generated%20data%2C%20the%20model%20improves%20predictive%20accuracy%20while%0Aeffectively%20capturing%20dynamic%20and%20temporal%20dependencies%20in%20supply%20chain%20data.%0AThe%20research%20focuses%20on%20three%20representative%20industries-manufacturing%20%28steel%29%2C%0Adistribution%20%28pharmaceuticals%29%2C%20and%20services%20%28e-commerce%29%20to%20assess%0Aindustry-specific%20credit%20risk%20contagion.%20Experimental%20results%20demonstrate%20that%0Athe%20GAN-based%20model%20outperforms%20traditional%20methods%2C%20including%20logistic%0Aregression%2C%20decision%20trees%2C%20and%20neural%20networks%2C%20achieving%20superior%20accuracy%2C%0Arecall%2C%20and%20F1%20scores.%20The%20findings%20underscore%20the%20potential%20of%20GANs%20in%0Aproactive%20risk%20management%2C%20offering%20robust%20tools%20for%20mitigating%20financial%0Adisruptions%20in%20supply%20chains.%20Future%20research%20could%20expand%20the%20model%20by%0Aincorporating%20external%20market%20factors%20and%20supplier%20relationships%20to%20further%0Aenhance%20predictive%20capabilities.%20Keywords-%20Generative%20Adversarial%20Networks%0A%28GANs%29%3B%20Supply%20Chain%20Risk%3B%20Credit%20Risk%20Identification%3B%20Machine%20Learning%3B%20Data%0AAugmentation%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10348v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCredit%2520Risk%2520Identification%2520in%2520Supply%2520Chains%2520Using%2520Generative%2520Adversarial%250A%2520%2520Networks%26entry.906535625%3DZizhou%2520Zhang%2520and%2520Xinshi%2520Li%2520and%2520Yu%2520Cheng%2520and%2520Zhenrui%2520Chen%2520and%2520Qianying%2520Liu%26entry.1292438233%3D%2520%2520Credit%2520risk%2520management%2520within%2520supply%2520chains%2520has%2520emerged%2520as%2520a%2520critical%250Aresearch%2520area%2520due%2520to%2520its%2520significant%2520implications%2520for%2520operational%2520stability%2520and%250Afinancial%2520sustainability.%2520The%2520intricate%2520interdependencies%2520among%2520supply%2520chain%250Aparticipants%2520mean%2520that%2520credit%2520risks%2520can%2520propagate%2520across%2520networks%252C%2520with%2520impacts%250Avarying%2520by%2520industry.%2520This%2520study%2520explores%2520the%2520application%2520of%2520Generative%250AAdversarial%2520Networks%2520%2528GANs%2529%2520to%2520enhance%2520credit%2520risk%2520identification%2520in%2520supply%250Achains.%2520GANs%2520enable%2520the%2520generation%2520of%2520synthetic%2520credit%2520risk%2520scenarios%252C%250Aaddressing%2520challenges%2520related%2520to%2520data%2520scarcity%2520and%2520imbalanced%2520datasets.%2520By%250Aleveraging%2520GAN-generated%2520data%252C%2520the%2520model%2520improves%2520predictive%2520accuracy%2520while%250Aeffectively%2520capturing%2520dynamic%2520and%2520temporal%2520dependencies%2520in%2520supply%2520chain%2520data.%250AThe%2520research%2520focuses%2520on%2520three%2520representative%2520industries-manufacturing%2520%2528steel%2529%252C%250Adistribution%2520%2528pharmaceuticals%2529%252C%2520and%2520services%2520%2528e-commerce%2529%2520to%2520assess%250Aindustry-specific%2520credit%2520risk%2520contagion.%2520Experimental%2520results%2520demonstrate%2520that%250Athe%2520GAN-based%2520model%2520outperforms%2520traditional%2520methods%252C%2520including%2520logistic%250Aregression%252C%2520decision%2520trees%252C%2520and%2520neural%2520networks%252C%2520achieving%2520superior%2520accuracy%252C%250Arecall%252C%2520and%2520F1%2520scores.%2520The%2520findings%2520underscore%2520the%2520potential%2520of%2520GANs%2520in%250Aproactive%2520risk%2520management%252C%2520offering%2520robust%2520tools%2520for%2520mitigating%2520financial%250Adisruptions%2520in%2520supply%2520chains.%2520Future%2520research%2520could%2520expand%2520the%2520model%2520by%250Aincorporating%2520external%2520market%2520factors%2520and%2520supplier%2520relationships%2520to%2520further%250Aenhance%2520predictive%2520capabilities.%2520Keywords-%2520Generative%2520Adversarial%2520Networks%250A%2528GANs%2529%253B%2520Supply%2520Chain%2520Risk%253B%2520Credit%2520Risk%2520Identification%253B%2520Machine%2520Learning%253B%2520Data%250AAugmentation%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10348v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Credit%20Risk%20Identification%20in%20Supply%20Chains%20Using%20Generative%20Adversarial%0A%20%20Networks&entry.906535625=Zizhou%20Zhang%20and%20Xinshi%20Li%20and%20Yu%20Cheng%20and%20Zhenrui%20Chen%20and%20Qianying%20Liu&entry.1292438233=%20%20Credit%20risk%20management%20within%20supply%20chains%20has%20emerged%20as%20a%20critical%0Aresearch%20area%20due%20to%20its%20significant%20implications%20for%20operational%20stability%20and%0Afinancial%20sustainability.%20The%20intricate%20interdependencies%20among%20supply%20chain%0Aparticipants%20mean%20that%20credit%20risks%20can%20propagate%20across%20networks%2C%20with%20impacts%0Avarying%20by%20industry.%20This%20study%20explores%20the%20application%20of%20Generative%0AAdversarial%20Networks%20%28GANs%29%20to%20enhance%20credit%20risk%20identification%20in%20supply%0Achains.%20GANs%20enable%20the%20generation%20of%20synthetic%20credit%20risk%20scenarios%2C%0Aaddressing%20challenges%20related%20to%20data%20scarcity%20and%20imbalanced%20datasets.%20By%0Aleveraging%20GAN-generated%20data%2C%20the%20model%20improves%20predictive%20accuracy%20while%0Aeffectively%20capturing%20dynamic%20and%20temporal%20dependencies%20in%20supply%20chain%20data.%0AThe%20research%20focuses%20on%20three%20representative%20industries-manufacturing%20%28steel%29%2C%0Adistribution%20%28pharmaceuticals%29%2C%20and%20services%20%28e-commerce%29%20to%20assess%0Aindustry-specific%20credit%20risk%20contagion.%20Experimental%20results%20demonstrate%20that%0Athe%20GAN-based%20model%20outperforms%20traditional%20methods%2C%20including%20logistic%0Aregression%2C%20decision%20trees%2C%20and%20neural%20networks%2C%20achieving%20superior%20accuracy%2C%0Arecall%2C%20and%20F1%20scores.%20The%20findings%20underscore%20the%20potential%20of%20GANs%20in%0Aproactive%20risk%20management%2C%20offering%20robust%20tools%20for%20mitigating%20financial%0Adisruptions%20in%20supply%20chains.%20Future%20research%20could%20expand%20the%20model%20by%0Aincorporating%20external%20market%20factors%20and%20supplier%20relationships%20to%20further%0Aenhance%20predictive%20capabilities.%20Keywords-%20Generative%20Adversarial%20Networks%0A%28GANs%29%3B%20Supply%20Chain%20Risk%3B%20Credit%20Risk%20Identification%3B%20Machine%20Learning%3B%20Data%0AAugmentation%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10348v1&entry.124074799=Read"},
{"title": "A Survey on Deep Learning for Polyp Segmentation: Techniques, Challenges\n  and Future Trends", "author": "Jiaxin Mei and Tao Zhou and Kaiwen Huang and Yizhe Zhang and Yi Zhou and Ye Wu and Huazhu Fu", "abstract": "  Early detection and assessment of polyps play a crucial role in the\nprevention and treatment of colorectal cancer (CRC). Polyp segmentation\nprovides an effective solution to assist clinicians in accurately locating and\nsegmenting polyp regions. In the past, people often relied on manually\nextracted lower-level features such as color, texture, and shape, which often\nhad issues capturing global context and lacked robustness to complex scenarios.\nWith the advent of deep learning, more and more outstanding medical image\nsegmentation algorithms based on deep learning networks have emerged, making\nsignificant progress in this field. This paper provides a comprehensive review\nof polyp segmentation algorithms. We first review some traditional algorithms\nbased on manually extracted features and deep segmentation algorithms, then\ndetail benchmark datasets related to the topic. Specifically, we carry out a\ncomprehensive evaluation of recent deep learning models and results based on\npolyp sizes, considering the pain points of research topics and differences in\nnetwork structures. Finally, we discuss the challenges of polyp segmentation\nand future trends in this field. The models, benchmark datasets, and source\ncode links we collected are all published at\nhttps://github.com/taozh2017/Awesome-Polyp-Segmentation.\n", "link": "http://arxiv.org/abs/2311.18373v3", "date": "2025-01-17", "relevancy": 1.9023, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5005}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4706}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Deep%20Learning%20for%20Polyp%20Segmentation%3A%20Techniques%2C%20Challenges%0A%20%20and%20Future%20Trends&body=Title%3A%20A%20Survey%20on%20Deep%20Learning%20for%20Polyp%20Segmentation%3A%20Techniques%2C%20Challenges%0A%20%20and%20Future%20Trends%0AAuthor%3A%20Jiaxin%20Mei%20and%20Tao%20Zhou%20and%20Kaiwen%20Huang%20and%20Yizhe%20Zhang%20and%20Yi%20Zhou%20and%20Ye%20Wu%20and%20Huazhu%20Fu%0AAbstract%3A%20%20%20Early%20detection%20and%20assessment%20of%20polyps%20play%20a%20crucial%20role%20in%20the%0Aprevention%20and%20treatment%20of%20colorectal%20cancer%20%28CRC%29.%20Polyp%20segmentation%0Aprovides%20an%20effective%20solution%20to%20assist%20clinicians%20in%20accurately%20locating%20and%0Asegmenting%20polyp%20regions.%20In%20the%20past%2C%20people%20often%20relied%20on%20manually%0Aextracted%20lower-level%20features%20such%20as%20color%2C%20texture%2C%20and%20shape%2C%20which%20often%0Ahad%20issues%20capturing%20global%20context%20and%20lacked%20robustness%20to%20complex%20scenarios.%0AWith%20the%20advent%20of%20deep%20learning%2C%20more%20and%20more%20outstanding%20medical%20image%0Asegmentation%20algorithms%20based%20on%20deep%20learning%20networks%20have%20emerged%2C%20making%0Asignificant%20progress%20in%20this%20field.%20This%20paper%20provides%20a%20comprehensive%20review%0Aof%20polyp%20segmentation%20algorithms.%20We%20first%20review%20some%20traditional%20algorithms%0Abased%20on%20manually%20extracted%20features%20and%20deep%20segmentation%20algorithms%2C%20then%0Adetail%20benchmark%20datasets%20related%20to%20the%20topic.%20Specifically%2C%20we%20carry%20out%20a%0Acomprehensive%20evaluation%20of%20recent%20deep%20learning%20models%20and%20results%20based%20on%0Apolyp%20sizes%2C%20considering%20the%20pain%20points%20of%20research%20topics%20and%20differences%20in%0Anetwork%20structures.%20Finally%2C%20we%20discuss%20the%20challenges%20of%20polyp%20segmentation%0Aand%20future%20trends%20in%20this%20field.%20The%20models%2C%20benchmark%20datasets%2C%20and%20source%0Acode%20links%20we%20collected%20are%20all%20published%20at%0Ahttps%3A//github.com/taozh2017/Awesome-Polyp-Segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.18373v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Deep%2520Learning%2520for%2520Polyp%2520Segmentation%253A%2520Techniques%252C%2520Challenges%250A%2520%2520and%2520Future%2520Trends%26entry.906535625%3DJiaxin%2520Mei%2520and%2520Tao%2520Zhou%2520and%2520Kaiwen%2520Huang%2520and%2520Yizhe%2520Zhang%2520and%2520Yi%2520Zhou%2520and%2520Ye%2520Wu%2520and%2520Huazhu%2520Fu%26entry.1292438233%3D%2520%2520Early%2520detection%2520and%2520assessment%2520of%2520polyps%2520play%2520a%2520crucial%2520role%2520in%2520the%250Aprevention%2520and%2520treatment%2520of%2520colorectal%2520cancer%2520%2528CRC%2529.%2520Polyp%2520segmentation%250Aprovides%2520an%2520effective%2520solution%2520to%2520assist%2520clinicians%2520in%2520accurately%2520locating%2520and%250Asegmenting%2520polyp%2520regions.%2520In%2520the%2520past%252C%2520people%2520often%2520relied%2520on%2520manually%250Aextracted%2520lower-level%2520features%2520such%2520as%2520color%252C%2520texture%252C%2520and%2520shape%252C%2520which%2520often%250Ahad%2520issues%2520capturing%2520global%2520context%2520and%2520lacked%2520robustness%2520to%2520complex%2520scenarios.%250AWith%2520the%2520advent%2520of%2520deep%2520learning%252C%2520more%2520and%2520more%2520outstanding%2520medical%2520image%250Asegmentation%2520algorithms%2520based%2520on%2520deep%2520learning%2520networks%2520have%2520emerged%252C%2520making%250Asignificant%2520progress%2520in%2520this%2520field.%2520This%2520paper%2520provides%2520a%2520comprehensive%2520review%250Aof%2520polyp%2520segmentation%2520algorithms.%2520We%2520first%2520review%2520some%2520traditional%2520algorithms%250Abased%2520on%2520manually%2520extracted%2520features%2520and%2520deep%2520segmentation%2520algorithms%252C%2520then%250Adetail%2520benchmark%2520datasets%2520related%2520to%2520the%2520topic.%2520Specifically%252C%2520we%2520carry%2520out%2520a%250Acomprehensive%2520evaluation%2520of%2520recent%2520deep%2520learning%2520models%2520and%2520results%2520based%2520on%250Apolyp%2520sizes%252C%2520considering%2520the%2520pain%2520points%2520of%2520research%2520topics%2520and%2520differences%2520in%250Anetwork%2520structures.%2520Finally%252C%2520we%2520discuss%2520the%2520challenges%2520of%2520polyp%2520segmentation%250Aand%2520future%2520trends%2520in%2520this%2520field.%2520The%2520models%252C%2520benchmark%2520datasets%252C%2520and%2520source%250Acode%2520links%2520we%2520collected%2520are%2520all%2520published%2520at%250Ahttps%253A//github.com/taozh2017/Awesome-Polyp-Segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.18373v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Deep%20Learning%20for%20Polyp%20Segmentation%3A%20Techniques%2C%20Challenges%0A%20%20and%20Future%20Trends&entry.906535625=Jiaxin%20Mei%20and%20Tao%20Zhou%20and%20Kaiwen%20Huang%20and%20Yizhe%20Zhang%20and%20Yi%20Zhou%20and%20Ye%20Wu%20and%20Huazhu%20Fu&entry.1292438233=%20%20Early%20detection%20and%20assessment%20of%20polyps%20play%20a%20crucial%20role%20in%20the%0Aprevention%20and%20treatment%20of%20colorectal%20cancer%20%28CRC%29.%20Polyp%20segmentation%0Aprovides%20an%20effective%20solution%20to%20assist%20clinicians%20in%20accurately%20locating%20and%0Asegmenting%20polyp%20regions.%20In%20the%20past%2C%20people%20often%20relied%20on%20manually%0Aextracted%20lower-level%20features%20such%20as%20color%2C%20texture%2C%20and%20shape%2C%20which%20often%0Ahad%20issues%20capturing%20global%20context%20and%20lacked%20robustness%20to%20complex%20scenarios.%0AWith%20the%20advent%20of%20deep%20learning%2C%20more%20and%20more%20outstanding%20medical%20image%0Asegmentation%20algorithms%20based%20on%20deep%20learning%20networks%20have%20emerged%2C%20making%0Asignificant%20progress%20in%20this%20field.%20This%20paper%20provides%20a%20comprehensive%20review%0Aof%20polyp%20segmentation%20algorithms.%20We%20first%20review%20some%20traditional%20algorithms%0Abased%20on%20manually%20extracted%20features%20and%20deep%20segmentation%20algorithms%2C%20then%0Adetail%20benchmark%20datasets%20related%20to%20the%20topic.%20Specifically%2C%20we%20carry%20out%20a%0Acomprehensive%20evaluation%20of%20recent%20deep%20learning%20models%20and%20results%20based%20on%0Apolyp%20sizes%2C%20considering%20the%20pain%20points%20of%20research%20topics%20and%20differences%20in%0Anetwork%20structures.%20Finally%2C%20we%20discuss%20the%20challenges%20of%20polyp%20segmentation%0Aand%20future%20trends%20in%20this%20field.%20The%20models%2C%20benchmark%20datasets%2C%20and%20source%0Acode%20links%20we%20collected%20are%20all%20published%20at%0Ahttps%3A//github.com/taozh2017/Awesome-Polyp-Segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.18373v3&entry.124074799=Read"},
{"title": "Temporal Graph MLP Mixer for Spatio-Temporal Forecasting", "author": "Muhammad Bilal and Luis Carretero Lopez", "abstract": "  Spatiotemporal forecasting is critical in applications such as traffic\nprediction, climate modeling, and environmental monitoring. However, the\nprevalence of missing data in real-world sensor networks significantly\ncomplicates this task. In this paper, we introduce the Temporal Graph MLP-Mixer\n(T-GMM), a novel architecture designed to address these challenges. The model\ncombines node-level processing with patch-level subgraph encoding to capture\nlocalized spatial dependencies while leveraging a three-dimensional MLP-Mixer\nto handle temporal, spatial, and feature-based dependencies. Experiments on the\nAQI, ENGRAD, PV-US and METR-LA datasets demonstrate the model's ability to\neffectively forecast even in the presence of significant missing data. While\nnot surpassing state-of-the-art models in all scenarios, the T-GMM exhibits\nstrong learning capabilities, particularly in capturing long-range\ndependencies. These results highlight its potential for robust, scalable\nspatiotemporal forecasting.\n", "link": "http://arxiv.org/abs/2501.10214v1", "date": "2025-01-17", "relevancy": 1.8944, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.481}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4752}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4655}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Temporal%20Graph%20MLP%20Mixer%20for%20Spatio-Temporal%20Forecasting&body=Title%3A%20Temporal%20Graph%20MLP%20Mixer%20for%20Spatio-Temporal%20Forecasting%0AAuthor%3A%20Muhammad%20Bilal%20and%20Luis%20Carretero%20Lopez%0AAbstract%3A%20%20%20Spatiotemporal%20forecasting%20is%20critical%20in%20applications%20such%20as%20traffic%0Aprediction%2C%20climate%20modeling%2C%20and%20environmental%20monitoring.%20However%2C%20the%0Aprevalence%20of%20missing%20data%20in%20real-world%20sensor%20networks%20significantly%0Acomplicates%20this%20task.%20In%20this%20paper%2C%20we%20introduce%20the%20Temporal%20Graph%20MLP-Mixer%0A%28T-GMM%29%2C%20a%20novel%20architecture%20designed%20to%20address%20these%20challenges.%20The%20model%0Acombines%20node-level%20processing%20with%20patch-level%20subgraph%20encoding%20to%20capture%0Alocalized%20spatial%20dependencies%20while%20leveraging%20a%20three-dimensional%20MLP-Mixer%0Ato%20handle%20temporal%2C%20spatial%2C%20and%20feature-based%20dependencies.%20Experiments%20on%20the%0AAQI%2C%20ENGRAD%2C%20PV-US%20and%20METR-LA%20datasets%20demonstrate%20the%20model%27s%20ability%20to%0Aeffectively%20forecast%20even%20in%20the%20presence%20of%20significant%20missing%20data.%20While%0Anot%20surpassing%20state-of-the-art%20models%20in%20all%20scenarios%2C%20the%20T-GMM%20exhibits%0Astrong%20learning%20capabilities%2C%20particularly%20in%20capturing%20long-range%0Adependencies.%20These%20results%20highlight%20its%20potential%20for%20robust%2C%20scalable%0Aspatiotemporal%20forecasting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10214v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemporal%2520Graph%2520MLP%2520Mixer%2520for%2520Spatio-Temporal%2520Forecasting%26entry.906535625%3DMuhammad%2520Bilal%2520and%2520Luis%2520Carretero%2520Lopez%26entry.1292438233%3D%2520%2520Spatiotemporal%2520forecasting%2520is%2520critical%2520in%2520applications%2520such%2520as%2520traffic%250Aprediction%252C%2520climate%2520modeling%252C%2520and%2520environmental%2520monitoring.%2520However%252C%2520the%250Aprevalence%2520of%2520missing%2520data%2520in%2520real-world%2520sensor%2520networks%2520significantly%250Acomplicates%2520this%2520task.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520Temporal%2520Graph%2520MLP-Mixer%250A%2528T-GMM%2529%252C%2520a%2520novel%2520architecture%2520designed%2520to%2520address%2520these%2520challenges.%2520The%2520model%250Acombines%2520node-level%2520processing%2520with%2520patch-level%2520subgraph%2520encoding%2520to%2520capture%250Alocalized%2520spatial%2520dependencies%2520while%2520leveraging%2520a%2520three-dimensional%2520MLP-Mixer%250Ato%2520handle%2520temporal%252C%2520spatial%252C%2520and%2520feature-based%2520dependencies.%2520Experiments%2520on%2520the%250AAQI%252C%2520ENGRAD%252C%2520PV-US%2520and%2520METR-LA%2520datasets%2520demonstrate%2520the%2520model%2527s%2520ability%2520to%250Aeffectively%2520forecast%2520even%2520in%2520the%2520presence%2520of%2520significant%2520missing%2520data.%2520While%250Anot%2520surpassing%2520state-of-the-art%2520models%2520in%2520all%2520scenarios%252C%2520the%2520T-GMM%2520exhibits%250Astrong%2520learning%2520capabilities%252C%2520particularly%2520in%2520capturing%2520long-range%250Adependencies.%2520These%2520results%2520highlight%2520its%2520potential%2520for%2520robust%252C%2520scalable%250Aspatiotemporal%2520forecasting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10214v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Temporal%20Graph%20MLP%20Mixer%20for%20Spatio-Temporal%20Forecasting&entry.906535625=Muhammad%20Bilal%20and%20Luis%20Carretero%20Lopez&entry.1292438233=%20%20Spatiotemporal%20forecasting%20is%20critical%20in%20applications%20such%20as%20traffic%0Aprediction%2C%20climate%20modeling%2C%20and%20environmental%20monitoring.%20However%2C%20the%0Aprevalence%20of%20missing%20data%20in%20real-world%20sensor%20networks%20significantly%0Acomplicates%20this%20task.%20In%20this%20paper%2C%20we%20introduce%20the%20Temporal%20Graph%20MLP-Mixer%0A%28T-GMM%29%2C%20a%20novel%20architecture%20designed%20to%20address%20these%20challenges.%20The%20model%0Acombines%20node-level%20processing%20with%20patch-level%20subgraph%20encoding%20to%20capture%0Alocalized%20spatial%20dependencies%20while%20leveraging%20a%20three-dimensional%20MLP-Mixer%0Ato%20handle%20temporal%2C%20spatial%2C%20and%20feature-based%20dependencies.%20Experiments%20on%20the%0AAQI%2C%20ENGRAD%2C%20PV-US%20and%20METR-LA%20datasets%20demonstrate%20the%20model%27s%20ability%20to%0Aeffectively%20forecast%20even%20in%20the%20presence%20of%20significant%20missing%20data.%20While%0Anot%20surpassing%20state-of-the-art%20models%20in%20all%20scenarios%2C%20the%20T-GMM%20exhibits%0Astrong%20learning%20capabilities%2C%20particularly%20in%20capturing%20long-range%0Adependencies.%20These%20results%20highlight%20its%20potential%20for%20robust%2C%20scalable%0Aspatiotemporal%20forecasting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10214v1&entry.124074799=Read"},
{"title": "Generalized Multi-hop Traffic Pressure for Heterogeneous Traffic\n  Perimeter Control", "author": "Xiaocan Li and Xiaoyu Wang and Ilia Smirnov and Scott Sanner and Baher Abdulhai", "abstract": "  Perimeter control (PC) prevents loss of traffic network capacity due to\ncongestion in urban areas. Homogeneous PC allows all access points to a\nprotected region to have identical permitted inflow. However, homogeneous PC\nperforms poorly when the congestion in the protected region is heterogeneous\n(e.g., imbalanced demand) since the homogeneous PC does not consider specific\ntraffic conditions around each perimeter intersection. When the protected\nregion has spatially heterogeneous congestion, one needs to modulate the\nperimeter inflow rate to be higher near low-density regions and vice versa for\nhigh-density regions. A na\\\"ive approach is to leverage 1-hop traffic pressure\nto measure traffic condition around perimeter intersections, but such metric is\ntoo spatially myopic for PC. To address this issue, we formulate multi-hop\ndownstream pressure grounded on Markov chain theory, which ``looks deeper''\ninto the protected region beyond perimeter intersections. In addition, we\nformulate a two-stage hierarchical control scheme that can leverage this novel\nmulti-hop pressure to redistribute the total permitted inflow provided by a\npre-trained deep reinforcement learning homogeneous control policy.\nExperimental results show that our heterogeneous PC approaches leveraging\nmulti-hop pressure significantly outperform homogeneous PC in scenarios where\nthe origin-destination flows are highly imbalanced with high spatial\nheterogeneity. Moveover, our approach is shown to be robust against turning\nratio uncertainties by a sensitivity analysis.\n", "link": "http://arxiv.org/abs/2409.00753v2", "date": "2025-01-17", "relevancy": 1.8814, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4929}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4902}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalized%20Multi-hop%20Traffic%20Pressure%20for%20Heterogeneous%20Traffic%0A%20%20Perimeter%20Control&body=Title%3A%20Generalized%20Multi-hop%20Traffic%20Pressure%20for%20Heterogeneous%20Traffic%0A%20%20Perimeter%20Control%0AAuthor%3A%20Xiaocan%20Li%20and%20Xiaoyu%20Wang%20and%20Ilia%20Smirnov%20and%20Scott%20Sanner%20and%20Baher%20Abdulhai%0AAbstract%3A%20%20%20Perimeter%20control%20%28PC%29%20prevents%20loss%20of%20traffic%20network%20capacity%20due%20to%0Acongestion%20in%20urban%20areas.%20Homogeneous%20PC%20allows%20all%20access%20points%20to%20a%0Aprotected%20region%20to%20have%20identical%20permitted%20inflow.%20However%2C%20homogeneous%20PC%0Aperforms%20poorly%20when%20the%20congestion%20in%20the%20protected%20region%20is%20heterogeneous%0A%28e.g.%2C%20imbalanced%20demand%29%20since%20the%20homogeneous%20PC%20does%20not%20consider%20specific%0Atraffic%20conditions%20around%20each%20perimeter%20intersection.%20When%20the%20protected%0Aregion%20has%20spatially%20heterogeneous%20congestion%2C%20one%20needs%20to%20modulate%20the%0Aperimeter%20inflow%20rate%20to%20be%20higher%20near%20low-density%20regions%20and%20vice%20versa%20for%0Ahigh-density%20regions.%20A%20na%5C%22ive%20approach%20is%20to%20leverage%201-hop%20traffic%20pressure%0Ato%20measure%20traffic%20condition%20around%20perimeter%20intersections%2C%20but%20such%20metric%20is%0Atoo%20spatially%20myopic%20for%20PC.%20To%20address%20this%20issue%2C%20we%20formulate%20multi-hop%0Adownstream%20pressure%20grounded%20on%20Markov%20chain%20theory%2C%20which%20%60%60looks%20deeper%27%27%0Ainto%20the%20protected%20region%20beyond%20perimeter%20intersections.%20In%20addition%2C%20we%0Aformulate%20a%20two-stage%20hierarchical%20control%20scheme%20that%20can%20leverage%20this%20novel%0Amulti-hop%20pressure%20to%20redistribute%20the%20total%20permitted%20inflow%20provided%20by%20a%0Apre-trained%20deep%20reinforcement%20learning%20homogeneous%20control%20policy.%0AExperimental%20results%20show%20that%20our%20heterogeneous%20PC%20approaches%20leveraging%0Amulti-hop%20pressure%20significantly%20outperform%20homogeneous%20PC%20in%20scenarios%20where%0Athe%20origin-destination%20flows%20are%20highly%20imbalanced%20with%20high%20spatial%0Aheterogeneity.%20Moveover%2C%20our%20approach%20is%20shown%20to%20be%20robust%20against%20turning%0Aratio%20uncertainties%20by%20a%20sensitivity%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.00753v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralized%2520Multi-hop%2520Traffic%2520Pressure%2520for%2520Heterogeneous%2520Traffic%250A%2520%2520Perimeter%2520Control%26entry.906535625%3DXiaocan%2520Li%2520and%2520Xiaoyu%2520Wang%2520and%2520Ilia%2520Smirnov%2520and%2520Scott%2520Sanner%2520and%2520Baher%2520Abdulhai%26entry.1292438233%3D%2520%2520Perimeter%2520control%2520%2528PC%2529%2520prevents%2520loss%2520of%2520traffic%2520network%2520capacity%2520due%2520to%250Acongestion%2520in%2520urban%2520areas.%2520Homogeneous%2520PC%2520allows%2520all%2520access%2520points%2520to%2520a%250Aprotected%2520region%2520to%2520have%2520identical%2520permitted%2520inflow.%2520However%252C%2520homogeneous%2520PC%250Aperforms%2520poorly%2520when%2520the%2520congestion%2520in%2520the%2520protected%2520region%2520is%2520heterogeneous%250A%2528e.g.%252C%2520imbalanced%2520demand%2529%2520since%2520the%2520homogeneous%2520PC%2520does%2520not%2520consider%2520specific%250Atraffic%2520conditions%2520around%2520each%2520perimeter%2520intersection.%2520When%2520the%2520protected%250Aregion%2520has%2520spatially%2520heterogeneous%2520congestion%252C%2520one%2520needs%2520to%2520modulate%2520the%250Aperimeter%2520inflow%2520rate%2520to%2520be%2520higher%2520near%2520low-density%2520regions%2520and%2520vice%2520versa%2520for%250Ahigh-density%2520regions.%2520A%2520na%255C%2522ive%2520approach%2520is%2520to%2520leverage%25201-hop%2520traffic%2520pressure%250Ato%2520measure%2520traffic%2520condition%2520around%2520perimeter%2520intersections%252C%2520but%2520such%2520metric%2520is%250Atoo%2520spatially%2520myopic%2520for%2520PC.%2520To%2520address%2520this%2520issue%252C%2520we%2520formulate%2520multi-hop%250Adownstream%2520pressure%2520grounded%2520on%2520Markov%2520chain%2520theory%252C%2520which%2520%2560%2560looks%2520deeper%2527%2527%250Ainto%2520the%2520protected%2520region%2520beyond%2520perimeter%2520intersections.%2520In%2520addition%252C%2520we%250Aformulate%2520a%2520two-stage%2520hierarchical%2520control%2520scheme%2520that%2520can%2520leverage%2520this%2520novel%250Amulti-hop%2520pressure%2520to%2520redistribute%2520the%2520total%2520permitted%2520inflow%2520provided%2520by%2520a%250Apre-trained%2520deep%2520reinforcement%2520learning%2520homogeneous%2520control%2520policy.%250AExperimental%2520results%2520show%2520that%2520our%2520heterogeneous%2520PC%2520approaches%2520leveraging%250Amulti-hop%2520pressure%2520significantly%2520outperform%2520homogeneous%2520PC%2520in%2520scenarios%2520where%250Athe%2520origin-destination%2520flows%2520are%2520highly%2520imbalanced%2520with%2520high%2520spatial%250Aheterogeneity.%2520Moveover%252C%2520our%2520approach%2520is%2520shown%2520to%2520be%2520robust%2520against%2520turning%250Aratio%2520uncertainties%2520by%2520a%2520sensitivity%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.00753v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalized%20Multi-hop%20Traffic%20Pressure%20for%20Heterogeneous%20Traffic%0A%20%20Perimeter%20Control&entry.906535625=Xiaocan%20Li%20and%20Xiaoyu%20Wang%20and%20Ilia%20Smirnov%20and%20Scott%20Sanner%20and%20Baher%20Abdulhai&entry.1292438233=%20%20Perimeter%20control%20%28PC%29%20prevents%20loss%20of%20traffic%20network%20capacity%20due%20to%0Acongestion%20in%20urban%20areas.%20Homogeneous%20PC%20allows%20all%20access%20points%20to%20a%0Aprotected%20region%20to%20have%20identical%20permitted%20inflow.%20However%2C%20homogeneous%20PC%0Aperforms%20poorly%20when%20the%20congestion%20in%20the%20protected%20region%20is%20heterogeneous%0A%28e.g.%2C%20imbalanced%20demand%29%20since%20the%20homogeneous%20PC%20does%20not%20consider%20specific%0Atraffic%20conditions%20around%20each%20perimeter%20intersection.%20When%20the%20protected%0Aregion%20has%20spatially%20heterogeneous%20congestion%2C%20one%20needs%20to%20modulate%20the%0Aperimeter%20inflow%20rate%20to%20be%20higher%20near%20low-density%20regions%20and%20vice%20versa%20for%0Ahigh-density%20regions.%20A%20na%5C%22ive%20approach%20is%20to%20leverage%201-hop%20traffic%20pressure%0Ato%20measure%20traffic%20condition%20around%20perimeter%20intersections%2C%20but%20such%20metric%20is%0Atoo%20spatially%20myopic%20for%20PC.%20To%20address%20this%20issue%2C%20we%20formulate%20multi-hop%0Adownstream%20pressure%20grounded%20on%20Markov%20chain%20theory%2C%20which%20%60%60looks%20deeper%27%27%0Ainto%20the%20protected%20region%20beyond%20perimeter%20intersections.%20In%20addition%2C%20we%0Aformulate%20a%20two-stage%20hierarchical%20control%20scheme%20that%20can%20leverage%20this%20novel%0Amulti-hop%20pressure%20to%20redistribute%20the%20total%20permitted%20inflow%20provided%20by%20a%0Apre-trained%20deep%20reinforcement%20learning%20homogeneous%20control%20policy.%0AExperimental%20results%20show%20that%20our%20heterogeneous%20PC%20approaches%20leveraging%0Amulti-hop%20pressure%20significantly%20outperform%20homogeneous%20PC%20in%20scenarios%20where%0Athe%20origin-destination%20flows%20are%20highly%20imbalanced%20with%20high%20spatial%0Aheterogeneity.%20Moveover%2C%20our%20approach%20is%20shown%20to%20be%20robust%20against%20turning%0Aratio%20uncertainties%20by%20a%20sensitivity%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.00753v2&entry.124074799=Read"},
{"title": "Infrastructure for AI Agents", "author": "Alan Chan and Kevin Wei and Sihao Huang and Nitarshan Rajkumar and Elija Perrier and Seth Lazar and Gillian K. Hadfield and Markus Anderljung", "abstract": "  Increasingly many AI systems can plan and execute interactions in open-ended\nenvironments, such as making phone calls or buying online goods. As developers\ngrow the space of tasks that such AI agents can accomplish, we will need tools\nboth to unlock their benefits and manage their risks. Current tools are largely\ninsufficient because they are not designed to shape how agents interact with\nexisting institutions (e.g., legal and economic systems) or actors (e.g.,\ndigital service providers, humans, other AI agents). For example, alignment\ntechniques by nature do not assure counterparties that some human will be held\naccountable when a user instructs an agent to perform an illegal action. To\nfill this gap, we propose the concept of agent infrastructure: technical\nsystems and shared protocols external to agents that are designed to mediate\nand influence their interactions with and impacts on their environments. Agent\ninfrastructure comprises both new tools and reconfigurations or extensions of\nexisting tools. For example, to facilitate accountability, protocols that tie\nusers to agents could build upon existing systems for user authentication, such\nas OpenID. Just as the Internet relies on infrastructure like HTTPS, we argue\nthat agent infrastructure will be similarly indispensable to ecosystems of\nagents. We identify three functions for agent infrastructure: 1) attributing\nactions, properties, and other information to specific agents, their users, or\nother actors; 2) shaping agents' interactions; and 3) detecting and remedying\nharmful actions from agents. We propose infrastructure that could help achieve\neach function, explaining use cases, adoption, limitations, and open questions.\nMaking progress on agent infrastructure can prepare society for the adoption of\nmore advanced agents.\n", "link": "http://arxiv.org/abs/2501.10114v1", "date": "2025-01-17", "relevancy": 1.8705, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4781}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4638}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Infrastructure%20for%20AI%20Agents&body=Title%3A%20Infrastructure%20for%20AI%20Agents%0AAuthor%3A%20Alan%20Chan%20and%20Kevin%20Wei%20and%20Sihao%20Huang%20and%20Nitarshan%20Rajkumar%20and%20Elija%20Perrier%20and%20Seth%20Lazar%20and%20Gillian%20K.%20Hadfield%20and%20Markus%20Anderljung%0AAbstract%3A%20%20%20Increasingly%20many%20AI%20systems%20can%20plan%20and%20execute%20interactions%20in%20open-ended%0Aenvironments%2C%20such%20as%20making%20phone%20calls%20or%20buying%20online%20goods.%20As%20developers%0Agrow%20the%20space%20of%20tasks%20that%20such%20AI%20agents%20can%20accomplish%2C%20we%20will%20need%20tools%0Aboth%20to%20unlock%20their%20benefits%20and%20manage%20their%20risks.%20Current%20tools%20are%20largely%0Ainsufficient%20because%20they%20are%20not%20designed%20to%20shape%20how%20agents%20interact%20with%0Aexisting%20institutions%20%28e.g.%2C%20legal%20and%20economic%20systems%29%20or%20actors%20%28e.g.%2C%0Adigital%20service%20providers%2C%20humans%2C%20other%20AI%20agents%29.%20For%20example%2C%20alignment%0Atechniques%20by%20nature%20do%20not%20assure%20counterparties%20that%20some%20human%20will%20be%20held%0Aaccountable%20when%20a%20user%20instructs%20an%20agent%20to%20perform%20an%20illegal%20action.%20To%0Afill%20this%20gap%2C%20we%20propose%20the%20concept%20of%20agent%20infrastructure%3A%20technical%0Asystems%20and%20shared%20protocols%20external%20to%20agents%20that%20are%20designed%20to%20mediate%0Aand%20influence%20their%20interactions%20with%20and%20impacts%20on%20their%20environments.%20Agent%0Ainfrastructure%20comprises%20both%20new%20tools%20and%20reconfigurations%20or%20extensions%20of%0Aexisting%20tools.%20For%20example%2C%20to%20facilitate%20accountability%2C%20protocols%20that%20tie%0Ausers%20to%20agents%20could%20build%20upon%20existing%20systems%20for%20user%20authentication%2C%20such%0Aas%20OpenID.%20Just%20as%20the%20Internet%20relies%20on%20infrastructure%20like%20HTTPS%2C%20we%20argue%0Athat%20agent%20infrastructure%20will%20be%20similarly%20indispensable%20to%20ecosystems%20of%0Aagents.%20We%20identify%20three%20functions%20for%20agent%20infrastructure%3A%201%29%20attributing%0Aactions%2C%20properties%2C%20and%20other%20information%20to%20specific%20agents%2C%20their%20users%2C%20or%0Aother%20actors%3B%202%29%20shaping%20agents%27%20interactions%3B%20and%203%29%20detecting%20and%20remedying%0Aharmful%20actions%20from%20agents.%20We%20propose%20infrastructure%20that%20could%20help%20achieve%0Aeach%20function%2C%20explaining%20use%20cases%2C%20adoption%2C%20limitations%2C%20and%20open%20questions.%0AMaking%20progress%20on%20agent%20infrastructure%20can%20prepare%20society%20for%20the%20adoption%20of%0Amore%20advanced%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10114v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfrastructure%2520for%2520AI%2520Agents%26entry.906535625%3DAlan%2520Chan%2520and%2520Kevin%2520Wei%2520and%2520Sihao%2520Huang%2520and%2520Nitarshan%2520Rajkumar%2520and%2520Elija%2520Perrier%2520and%2520Seth%2520Lazar%2520and%2520Gillian%2520K.%2520Hadfield%2520and%2520Markus%2520Anderljung%26entry.1292438233%3D%2520%2520Increasingly%2520many%2520AI%2520systems%2520can%2520plan%2520and%2520execute%2520interactions%2520in%2520open-ended%250Aenvironments%252C%2520such%2520as%2520making%2520phone%2520calls%2520or%2520buying%2520online%2520goods.%2520As%2520developers%250Agrow%2520the%2520space%2520of%2520tasks%2520that%2520such%2520AI%2520agents%2520can%2520accomplish%252C%2520we%2520will%2520need%2520tools%250Aboth%2520to%2520unlock%2520their%2520benefits%2520and%2520manage%2520their%2520risks.%2520Current%2520tools%2520are%2520largely%250Ainsufficient%2520because%2520they%2520are%2520not%2520designed%2520to%2520shape%2520how%2520agents%2520interact%2520with%250Aexisting%2520institutions%2520%2528e.g.%252C%2520legal%2520and%2520economic%2520systems%2529%2520or%2520actors%2520%2528e.g.%252C%250Adigital%2520service%2520providers%252C%2520humans%252C%2520other%2520AI%2520agents%2529.%2520For%2520example%252C%2520alignment%250Atechniques%2520by%2520nature%2520do%2520not%2520assure%2520counterparties%2520that%2520some%2520human%2520will%2520be%2520held%250Aaccountable%2520when%2520a%2520user%2520instructs%2520an%2520agent%2520to%2520perform%2520an%2520illegal%2520action.%2520To%250Afill%2520this%2520gap%252C%2520we%2520propose%2520the%2520concept%2520of%2520agent%2520infrastructure%253A%2520technical%250Asystems%2520and%2520shared%2520protocols%2520external%2520to%2520agents%2520that%2520are%2520designed%2520to%2520mediate%250Aand%2520influence%2520their%2520interactions%2520with%2520and%2520impacts%2520on%2520their%2520environments.%2520Agent%250Ainfrastructure%2520comprises%2520both%2520new%2520tools%2520and%2520reconfigurations%2520or%2520extensions%2520of%250Aexisting%2520tools.%2520For%2520example%252C%2520to%2520facilitate%2520accountability%252C%2520protocols%2520that%2520tie%250Ausers%2520to%2520agents%2520could%2520build%2520upon%2520existing%2520systems%2520for%2520user%2520authentication%252C%2520such%250Aas%2520OpenID.%2520Just%2520as%2520the%2520Internet%2520relies%2520on%2520infrastructure%2520like%2520HTTPS%252C%2520we%2520argue%250Athat%2520agent%2520infrastructure%2520will%2520be%2520similarly%2520indispensable%2520to%2520ecosystems%2520of%250Aagents.%2520We%2520identify%2520three%2520functions%2520for%2520agent%2520infrastructure%253A%25201%2529%2520attributing%250Aactions%252C%2520properties%252C%2520and%2520other%2520information%2520to%2520specific%2520agents%252C%2520their%2520users%252C%2520or%250Aother%2520actors%253B%25202%2529%2520shaping%2520agents%2527%2520interactions%253B%2520and%25203%2529%2520detecting%2520and%2520remedying%250Aharmful%2520actions%2520from%2520agents.%2520We%2520propose%2520infrastructure%2520that%2520could%2520help%2520achieve%250Aeach%2520function%252C%2520explaining%2520use%2520cases%252C%2520adoption%252C%2520limitations%252C%2520and%2520open%2520questions.%250AMaking%2520progress%2520on%2520agent%2520infrastructure%2520can%2520prepare%2520society%2520for%2520the%2520adoption%2520of%250Amore%2520advanced%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10114v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Infrastructure%20for%20AI%20Agents&entry.906535625=Alan%20Chan%20and%20Kevin%20Wei%20and%20Sihao%20Huang%20and%20Nitarshan%20Rajkumar%20and%20Elija%20Perrier%20and%20Seth%20Lazar%20and%20Gillian%20K.%20Hadfield%20and%20Markus%20Anderljung&entry.1292438233=%20%20Increasingly%20many%20AI%20systems%20can%20plan%20and%20execute%20interactions%20in%20open-ended%0Aenvironments%2C%20such%20as%20making%20phone%20calls%20or%20buying%20online%20goods.%20As%20developers%0Agrow%20the%20space%20of%20tasks%20that%20such%20AI%20agents%20can%20accomplish%2C%20we%20will%20need%20tools%0Aboth%20to%20unlock%20their%20benefits%20and%20manage%20their%20risks.%20Current%20tools%20are%20largely%0Ainsufficient%20because%20they%20are%20not%20designed%20to%20shape%20how%20agents%20interact%20with%0Aexisting%20institutions%20%28e.g.%2C%20legal%20and%20economic%20systems%29%20or%20actors%20%28e.g.%2C%0Adigital%20service%20providers%2C%20humans%2C%20other%20AI%20agents%29.%20For%20example%2C%20alignment%0Atechniques%20by%20nature%20do%20not%20assure%20counterparties%20that%20some%20human%20will%20be%20held%0Aaccountable%20when%20a%20user%20instructs%20an%20agent%20to%20perform%20an%20illegal%20action.%20To%0Afill%20this%20gap%2C%20we%20propose%20the%20concept%20of%20agent%20infrastructure%3A%20technical%0Asystems%20and%20shared%20protocols%20external%20to%20agents%20that%20are%20designed%20to%20mediate%0Aand%20influence%20their%20interactions%20with%20and%20impacts%20on%20their%20environments.%20Agent%0Ainfrastructure%20comprises%20both%20new%20tools%20and%20reconfigurations%20or%20extensions%20of%0Aexisting%20tools.%20For%20example%2C%20to%20facilitate%20accountability%2C%20protocols%20that%20tie%0Ausers%20to%20agents%20could%20build%20upon%20existing%20systems%20for%20user%20authentication%2C%20such%0Aas%20OpenID.%20Just%20as%20the%20Internet%20relies%20on%20infrastructure%20like%20HTTPS%2C%20we%20argue%0Athat%20agent%20infrastructure%20will%20be%20similarly%20indispensable%20to%20ecosystems%20of%0Aagents.%20We%20identify%20three%20functions%20for%20agent%20infrastructure%3A%201%29%20attributing%0Aactions%2C%20properties%2C%20and%20other%20information%20to%20specific%20agents%2C%20their%20users%2C%20or%0Aother%20actors%3B%202%29%20shaping%20agents%27%20interactions%3B%20and%203%29%20detecting%20and%20remedying%0Aharmful%20actions%20from%20agents.%20We%20propose%20infrastructure%20that%20could%20help%20achieve%0Aeach%20function%2C%20explaining%20use%20cases%2C%20adoption%2C%20limitations%2C%20and%20open%20questions.%0AMaking%20progress%20on%20agent%20infrastructure%20can%20prepare%20society%20for%20the%20adoption%20of%0Amore%20advanced%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10114v1&entry.124074799=Read"},
{"title": "FECT: Classification of Breast Cancer Pathological Images Based on\n  Fusion Features", "author": "Jiacheng Hao and Yiqing Liu and Siqi Zeng and Yonghong He", "abstract": "  Breast cancer is one of the most common cancers among women globally, with\nearly diagnosis and precise classification being crucial. With the advancement\nof deep learning and computer vision, the automatic classification of breast\ntissue pathological images has emerged as a research focus. Existing methods\ntypically rely on singular cell or tissue features and lack design\nconsiderations for morphological characteristics of challenging-to-classify\ncategories, resulting in suboptimal classification performance. To address\nthese problems, we proposes a novel breast cancer tissue classification model\nthat Fused features of Edges, Cells, and Tissues (FECT), employing the\nResMTUNet and an attention-based aggregator to extract and aggregate these\nfeatures. Extensive testing on the BRACS dataset demonstrates that our model\nsurpasses current advanced methods in terms of classification accuracy and F1\nscores. Moreover, due to its feature fusion that aligns with the diagnostic\napproach of pathologists, our model exhibits interpretability and holds promise\nfor significant roles in future clinical applications.\n", "link": "http://arxiv.org/abs/2501.10128v1", "date": "2025-01-17", "relevancy": 1.8537, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4694}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4603}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FECT%3A%20Classification%20of%20Breast%20Cancer%20Pathological%20Images%20Based%20on%0A%20%20Fusion%20Features&body=Title%3A%20FECT%3A%20Classification%20of%20Breast%20Cancer%20Pathological%20Images%20Based%20on%0A%20%20Fusion%20Features%0AAuthor%3A%20Jiacheng%20Hao%20and%20Yiqing%20Liu%20and%20Siqi%20Zeng%20and%20Yonghong%20He%0AAbstract%3A%20%20%20Breast%20cancer%20is%20one%20of%20the%20most%20common%20cancers%20among%20women%20globally%2C%20with%0Aearly%20diagnosis%20and%20precise%20classification%20being%20crucial.%20With%20the%20advancement%0Aof%20deep%20learning%20and%20computer%20vision%2C%20the%20automatic%20classification%20of%20breast%0Atissue%20pathological%20images%20has%20emerged%20as%20a%20research%20focus.%20Existing%20methods%0Atypically%20rely%20on%20singular%20cell%20or%20tissue%20features%20and%20lack%20design%0Aconsiderations%20for%20morphological%20characteristics%20of%20challenging-to-classify%0Acategories%2C%20resulting%20in%20suboptimal%20classification%20performance.%20To%20address%0Athese%20problems%2C%20we%20proposes%20a%20novel%20breast%20cancer%20tissue%20classification%20model%0Athat%20Fused%20features%20of%20Edges%2C%20Cells%2C%20and%20Tissues%20%28FECT%29%2C%20employing%20the%0AResMTUNet%20and%20an%20attention-based%20aggregator%20to%20extract%20and%20aggregate%20these%0Afeatures.%20Extensive%20testing%20on%20the%20BRACS%20dataset%20demonstrates%20that%20our%20model%0Asurpasses%20current%20advanced%20methods%20in%20terms%20of%20classification%20accuracy%20and%20F1%0Ascores.%20Moreover%2C%20due%20to%20its%20feature%20fusion%20that%20aligns%20with%20the%20diagnostic%0Aapproach%20of%20pathologists%2C%20our%20model%20exhibits%20interpretability%20and%20holds%20promise%0Afor%20significant%20roles%20in%20future%20clinical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10128v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFECT%253A%2520Classification%2520of%2520Breast%2520Cancer%2520Pathological%2520Images%2520Based%2520on%250A%2520%2520Fusion%2520Features%26entry.906535625%3DJiacheng%2520Hao%2520and%2520Yiqing%2520Liu%2520and%2520Siqi%2520Zeng%2520and%2520Yonghong%2520He%26entry.1292438233%3D%2520%2520Breast%2520cancer%2520is%2520one%2520of%2520the%2520most%2520common%2520cancers%2520among%2520women%2520globally%252C%2520with%250Aearly%2520diagnosis%2520and%2520precise%2520classification%2520being%2520crucial.%2520With%2520the%2520advancement%250Aof%2520deep%2520learning%2520and%2520computer%2520vision%252C%2520the%2520automatic%2520classification%2520of%2520breast%250Atissue%2520pathological%2520images%2520has%2520emerged%2520as%2520a%2520research%2520focus.%2520Existing%2520methods%250Atypically%2520rely%2520on%2520singular%2520cell%2520or%2520tissue%2520features%2520and%2520lack%2520design%250Aconsiderations%2520for%2520morphological%2520characteristics%2520of%2520challenging-to-classify%250Acategories%252C%2520resulting%2520in%2520suboptimal%2520classification%2520performance.%2520To%2520address%250Athese%2520problems%252C%2520we%2520proposes%2520a%2520novel%2520breast%2520cancer%2520tissue%2520classification%2520model%250Athat%2520Fused%2520features%2520of%2520Edges%252C%2520Cells%252C%2520and%2520Tissues%2520%2528FECT%2529%252C%2520employing%2520the%250AResMTUNet%2520and%2520an%2520attention-based%2520aggregator%2520to%2520extract%2520and%2520aggregate%2520these%250Afeatures.%2520Extensive%2520testing%2520on%2520the%2520BRACS%2520dataset%2520demonstrates%2520that%2520our%2520model%250Asurpasses%2520current%2520advanced%2520methods%2520in%2520terms%2520of%2520classification%2520accuracy%2520and%2520F1%250Ascores.%2520Moreover%252C%2520due%2520to%2520its%2520feature%2520fusion%2520that%2520aligns%2520with%2520the%2520diagnostic%250Aapproach%2520of%2520pathologists%252C%2520our%2520model%2520exhibits%2520interpretability%2520and%2520holds%2520promise%250Afor%2520significant%2520roles%2520in%2520future%2520clinical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10128v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FECT%3A%20Classification%20of%20Breast%20Cancer%20Pathological%20Images%20Based%20on%0A%20%20Fusion%20Features&entry.906535625=Jiacheng%20Hao%20and%20Yiqing%20Liu%20and%20Siqi%20Zeng%20and%20Yonghong%20He&entry.1292438233=%20%20Breast%20cancer%20is%20one%20of%20the%20most%20common%20cancers%20among%20women%20globally%2C%20with%0Aearly%20diagnosis%20and%20precise%20classification%20being%20crucial.%20With%20the%20advancement%0Aof%20deep%20learning%20and%20computer%20vision%2C%20the%20automatic%20classification%20of%20breast%0Atissue%20pathological%20images%20has%20emerged%20as%20a%20research%20focus.%20Existing%20methods%0Atypically%20rely%20on%20singular%20cell%20or%20tissue%20features%20and%20lack%20design%0Aconsiderations%20for%20morphological%20characteristics%20of%20challenging-to-classify%0Acategories%2C%20resulting%20in%20suboptimal%20classification%20performance.%20To%20address%0Athese%20problems%2C%20we%20proposes%20a%20novel%20breast%20cancer%20tissue%20classification%20model%0Athat%20Fused%20features%20of%20Edges%2C%20Cells%2C%20and%20Tissues%20%28FECT%29%2C%20employing%20the%0AResMTUNet%20and%20an%20attention-based%20aggregator%20to%20extract%20and%20aggregate%20these%0Afeatures.%20Extensive%20testing%20on%20the%20BRACS%20dataset%20demonstrates%20that%20our%20model%0Asurpasses%20current%20advanced%20methods%20in%20terms%20of%20classification%20accuracy%20and%20F1%0Ascores.%20Moreover%2C%20due%20to%20its%20feature%20fusion%20that%20aligns%20with%20the%20diagnostic%0Aapproach%20of%20pathologists%2C%20our%20model%20exhibits%20interpretability%20and%20holds%20promise%0Afor%20significant%20roles%20in%20future%20clinical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10128v1&entry.124074799=Read"},
{"title": "Convex Physics Informed Neural Networks for the Monge-Amp\u00e8re Optimal\n  Transport Problem", "author": "Alexandre Caboussat and Anna Peruso", "abstract": "  Optimal transportation of raw material from suppliers to customers is an\nissue arising in logistics that is addressed here with a continuous model\nrelying on optimal transport theory. A physics informed neuralnetwork method is\nadvocated here for the solution of the corresponding generalized Monge-Amp`ere\nequation. Convex neural networks are advocated to enforce the convexity of the\nsolution to the Monge-Amp\\`ere equation and obtain a suitable approximation of\nthe optimal transport map. A particular focus is set on the enforcement of\ntransport boundary conditions in the loss function. Numerical experiments\nillustrate the solution to the optimal transport problem in several\nconfigurations, and sensitivity analyses are performed.\n", "link": "http://arxiv.org/abs/2501.10162v1", "date": "2025-01-17", "relevancy": 1.8455, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.482}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4622}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Convex%20Physics%20Informed%20Neural%20Networks%20for%20the%20Monge-Amp%C3%A8re%20Optimal%0A%20%20Transport%20Problem&body=Title%3A%20Convex%20Physics%20Informed%20Neural%20Networks%20for%20the%20Monge-Amp%C3%A8re%20Optimal%0A%20%20Transport%20Problem%0AAuthor%3A%20Alexandre%20Caboussat%20and%20Anna%20Peruso%0AAbstract%3A%20%20%20Optimal%20transportation%20of%20raw%20material%20from%20suppliers%20to%20customers%20is%20an%0Aissue%20arising%20in%20logistics%20that%20is%20addressed%20here%20with%20a%20continuous%20model%0Arelying%20on%20optimal%20transport%20theory.%20A%20physics%20informed%20neuralnetwork%20method%20is%0Aadvocated%20here%20for%20the%20solution%20of%20the%20corresponding%20generalized%20Monge-Amp%60ere%0Aequation.%20Convex%20neural%20networks%20are%20advocated%20to%20enforce%20the%20convexity%20of%20the%0Asolution%20to%20the%20Monge-Amp%5C%60ere%20equation%20and%20obtain%20a%20suitable%20approximation%20of%0Athe%20optimal%20transport%20map.%20A%20particular%20focus%20is%20set%20on%20the%20enforcement%20of%0Atransport%20boundary%20conditions%20in%20the%20loss%20function.%20Numerical%20experiments%0Aillustrate%20the%20solution%20to%20the%20optimal%20transport%20problem%20in%20several%0Aconfigurations%2C%20and%20sensitivity%20analyses%20are%20performed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10162v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvex%2520Physics%2520Informed%2520Neural%2520Networks%2520for%2520the%2520Monge-Amp%25C3%25A8re%2520Optimal%250A%2520%2520Transport%2520Problem%26entry.906535625%3DAlexandre%2520Caboussat%2520and%2520Anna%2520Peruso%26entry.1292438233%3D%2520%2520Optimal%2520transportation%2520of%2520raw%2520material%2520from%2520suppliers%2520to%2520customers%2520is%2520an%250Aissue%2520arising%2520in%2520logistics%2520that%2520is%2520addressed%2520here%2520with%2520a%2520continuous%2520model%250Arelying%2520on%2520optimal%2520transport%2520theory.%2520A%2520physics%2520informed%2520neuralnetwork%2520method%2520is%250Aadvocated%2520here%2520for%2520the%2520solution%2520of%2520the%2520corresponding%2520generalized%2520Monge-Amp%2560ere%250Aequation.%2520Convex%2520neural%2520networks%2520are%2520advocated%2520to%2520enforce%2520the%2520convexity%2520of%2520the%250Asolution%2520to%2520the%2520Monge-Amp%255C%2560ere%2520equation%2520and%2520obtain%2520a%2520suitable%2520approximation%2520of%250Athe%2520optimal%2520transport%2520map.%2520A%2520particular%2520focus%2520is%2520set%2520on%2520the%2520enforcement%2520of%250Atransport%2520boundary%2520conditions%2520in%2520the%2520loss%2520function.%2520Numerical%2520experiments%250Aillustrate%2520the%2520solution%2520to%2520the%2520optimal%2520transport%2520problem%2520in%2520several%250Aconfigurations%252C%2520and%2520sensitivity%2520analyses%2520are%2520performed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10162v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convex%20Physics%20Informed%20Neural%20Networks%20for%20the%20Monge-Amp%C3%A8re%20Optimal%0A%20%20Transport%20Problem&entry.906535625=Alexandre%20Caboussat%20and%20Anna%20Peruso&entry.1292438233=%20%20Optimal%20transportation%20of%20raw%20material%20from%20suppliers%20to%20customers%20is%20an%0Aissue%20arising%20in%20logistics%20that%20is%20addressed%20here%20with%20a%20continuous%20model%0Arelying%20on%20optimal%20transport%20theory.%20A%20physics%20informed%20neuralnetwork%20method%20is%0Aadvocated%20here%20for%20the%20solution%20of%20the%20corresponding%20generalized%20Monge-Amp%60ere%0Aequation.%20Convex%20neural%20networks%20are%20advocated%20to%20enforce%20the%20convexity%20of%20the%0Asolution%20to%20the%20Monge-Amp%5C%60ere%20equation%20and%20obtain%20a%20suitable%20approximation%20of%0Athe%20optimal%20transport%20map.%20A%20particular%20focus%20is%20set%20on%20the%20enforcement%20of%0Atransport%20boundary%20conditions%20in%20the%20loss%20function.%20Numerical%20experiments%0Aillustrate%20the%20solution%20to%20the%20optimal%20transport%20problem%20in%20several%0Aconfigurations%2C%20and%20sensitivity%20analyses%20are%20performed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10162v1&entry.124074799=Read"},
{"title": "Gene Regulatory Network Inference in the Presence of Selection Bias and\n  Latent Confounders", "author": "Gongxu Luo and Haoyue Dai and Boyang Sun and Loka Li and Biwei Huang and Petar Stojanov and Kun Zhang", "abstract": "  Gene Regulatory Network Inference (GRNI) aims to identify causal\nrelationships among genes using gene expression data, providing insights into\nregulatory mechanisms. A significant yet often overlooked challenge is\nselection bias, a process where only cells meeting specific criteria, such as\ngene expression thresholds, survive or are observed, distorting the true joint\ndistribution of genes and thus biasing GRNI results. Furthermore, gene\nexpression is influenced by latent confounders, such as non-coding RNAs, which\nadd complexity to GRNI. To address these challenges, we propose GISL (Gene\nRegulatory Network Inference in the presence of Selection bias and Latent\nconfounders), a novel algorithm to infer true regulatory relationships in the\npresence of selection and confounding issues. Leveraging data obtained via\nmultiple gene perturbation experiments, we show that the true regulatory\nrelationships, as well as selection processes and latent confounders can be\npartially identified without strong parametric models and under mild graphical\nassumptions. Experimental results on both synthetic and real-world single-cell\ngene expression datasets demonstrate the superiority of GISL over existing\nmethods.\n", "link": "http://arxiv.org/abs/2501.10124v1", "date": "2025-01-17", "relevancy": 1.8396, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4706}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4557}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gene%20Regulatory%20Network%20Inference%20in%20the%20Presence%20of%20Selection%20Bias%20and%0A%20%20Latent%20Confounders&body=Title%3A%20Gene%20Regulatory%20Network%20Inference%20in%20the%20Presence%20of%20Selection%20Bias%20and%0A%20%20Latent%20Confounders%0AAuthor%3A%20Gongxu%20Luo%20and%20Haoyue%20Dai%20and%20Boyang%20Sun%20and%20Loka%20Li%20and%20Biwei%20Huang%20and%20Petar%20Stojanov%20and%20Kun%20Zhang%0AAbstract%3A%20%20%20Gene%20Regulatory%20Network%20Inference%20%28GRNI%29%20aims%20to%20identify%20causal%0Arelationships%20among%20genes%20using%20gene%20expression%20data%2C%20providing%20insights%20into%0Aregulatory%20mechanisms.%20A%20significant%20yet%20often%20overlooked%20challenge%20is%0Aselection%20bias%2C%20a%20process%20where%20only%20cells%20meeting%20specific%20criteria%2C%20such%20as%0Agene%20expression%20thresholds%2C%20survive%20or%20are%20observed%2C%20distorting%20the%20true%20joint%0Adistribution%20of%20genes%20and%20thus%20biasing%20GRNI%20results.%20Furthermore%2C%20gene%0Aexpression%20is%20influenced%20by%20latent%20confounders%2C%20such%20as%20non-coding%20RNAs%2C%20which%0Aadd%20complexity%20to%20GRNI.%20To%20address%20these%20challenges%2C%20we%20propose%20GISL%20%28Gene%0ARegulatory%20Network%20Inference%20in%20the%20presence%20of%20Selection%20bias%20and%20Latent%0Aconfounders%29%2C%20a%20novel%20algorithm%20to%20infer%20true%20regulatory%20relationships%20in%20the%0Apresence%20of%20selection%20and%20confounding%20issues.%20Leveraging%20data%20obtained%20via%0Amultiple%20gene%20perturbation%20experiments%2C%20we%20show%20that%20the%20true%20regulatory%0Arelationships%2C%20as%20well%20as%20selection%20processes%20and%20latent%20confounders%20can%20be%0Apartially%20identified%20without%20strong%20parametric%20models%20and%20under%20mild%20graphical%0Aassumptions.%20Experimental%20results%20on%20both%20synthetic%20and%20real-world%20single-cell%0Agene%20expression%20datasets%20demonstrate%20the%20superiority%20of%20GISL%20over%20existing%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10124v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGene%2520Regulatory%2520Network%2520Inference%2520in%2520the%2520Presence%2520of%2520Selection%2520Bias%2520and%250A%2520%2520Latent%2520Confounders%26entry.906535625%3DGongxu%2520Luo%2520and%2520Haoyue%2520Dai%2520and%2520Boyang%2520Sun%2520and%2520Loka%2520Li%2520and%2520Biwei%2520Huang%2520and%2520Petar%2520Stojanov%2520and%2520Kun%2520Zhang%26entry.1292438233%3D%2520%2520Gene%2520Regulatory%2520Network%2520Inference%2520%2528GRNI%2529%2520aims%2520to%2520identify%2520causal%250Arelationships%2520among%2520genes%2520using%2520gene%2520expression%2520data%252C%2520providing%2520insights%2520into%250Aregulatory%2520mechanisms.%2520A%2520significant%2520yet%2520often%2520overlooked%2520challenge%2520is%250Aselection%2520bias%252C%2520a%2520process%2520where%2520only%2520cells%2520meeting%2520specific%2520criteria%252C%2520such%2520as%250Agene%2520expression%2520thresholds%252C%2520survive%2520or%2520are%2520observed%252C%2520distorting%2520the%2520true%2520joint%250Adistribution%2520of%2520genes%2520and%2520thus%2520biasing%2520GRNI%2520results.%2520Furthermore%252C%2520gene%250Aexpression%2520is%2520influenced%2520by%2520latent%2520confounders%252C%2520such%2520as%2520non-coding%2520RNAs%252C%2520which%250Aadd%2520complexity%2520to%2520GRNI.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520GISL%2520%2528Gene%250ARegulatory%2520Network%2520Inference%2520in%2520the%2520presence%2520of%2520Selection%2520bias%2520and%2520Latent%250Aconfounders%2529%252C%2520a%2520novel%2520algorithm%2520to%2520infer%2520true%2520regulatory%2520relationships%2520in%2520the%250Apresence%2520of%2520selection%2520and%2520confounding%2520issues.%2520Leveraging%2520data%2520obtained%2520via%250Amultiple%2520gene%2520perturbation%2520experiments%252C%2520we%2520show%2520that%2520the%2520true%2520regulatory%250Arelationships%252C%2520as%2520well%2520as%2520selection%2520processes%2520and%2520latent%2520confounders%2520can%2520be%250Apartially%2520identified%2520without%2520strong%2520parametric%2520models%2520and%2520under%2520mild%2520graphical%250Aassumptions.%2520Experimental%2520results%2520on%2520both%2520synthetic%2520and%2520real-world%2520single-cell%250Agene%2520expression%2520datasets%2520demonstrate%2520the%2520superiority%2520of%2520GISL%2520over%2520existing%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10124v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gene%20Regulatory%20Network%20Inference%20in%20the%20Presence%20of%20Selection%20Bias%20and%0A%20%20Latent%20Confounders&entry.906535625=Gongxu%20Luo%20and%20Haoyue%20Dai%20and%20Boyang%20Sun%20and%20Loka%20Li%20and%20Biwei%20Huang%20and%20Petar%20Stojanov%20and%20Kun%20Zhang&entry.1292438233=%20%20Gene%20Regulatory%20Network%20Inference%20%28GRNI%29%20aims%20to%20identify%20causal%0Arelationships%20among%20genes%20using%20gene%20expression%20data%2C%20providing%20insights%20into%0Aregulatory%20mechanisms.%20A%20significant%20yet%20often%20overlooked%20challenge%20is%0Aselection%20bias%2C%20a%20process%20where%20only%20cells%20meeting%20specific%20criteria%2C%20such%20as%0Agene%20expression%20thresholds%2C%20survive%20or%20are%20observed%2C%20distorting%20the%20true%20joint%0Adistribution%20of%20genes%20and%20thus%20biasing%20GRNI%20results.%20Furthermore%2C%20gene%0Aexpression%20is%20influenced%20by%20latent%20confounders%2C%20such%20as%20non-coding%20RNAs%2C%20which%0Aadd%20complexity%20to%20GRNI.%20To%20address%20these%20challenges%2C%20we%20propose%20GISL%20%28Gene%0ARegulatory%20Network%20Inference%20in%20the%20presence%20of%20Selection%20bias%20and%20Latent%0Aconfounders%29%2C%20a%20novel%20algorithm%20to%20infer%20true%20regulatory%20relationships%20in%20the%0Apresence%20of%20selection%20and%20confounding%20issues.%20Leveraging%20data%20obtained%20via%0Amultiple%20gene%20perturbation%20experiments%2C%20we%20show%20that%20the%20true%20regulatory%0Arelationships%2C%20as%20well%20as%20selection%20processes%20and%20latent%20confounders%20can%20be%0Apartially%20identified%20without%20strong%20parametric%20models%20and%20under%20mild%20graphical%0Aassumptions.%20Experimental%20results%20on%20both%20synthetic%20and%20real-world%20single-cell%0Agene%20expression%20datasets%20demonstrate%20the%20superiority%20of%20GISL%20over%20existing%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10124v1&entry.124074799=Read"},
{"title": "Generate E-commerce Product Background by Integrating Category\n  Commonality and Personalized Style", "author": "Haohan Wang and Wei Feng and Yaoyu Li and Zheng Zhang and Jingjing Lv and Junjie Shen and Zhangang Lin and Jingping Shao", "abstract": "  The state-of-the-art methods for e-commerce product background generation\nsuffer from the inefficiency of designing product-wise prompts when scaling up\nthe production, as well as the ineffectiveness of describing fine-grained\nstyles when customizing personalized backgrounds for some specific brands. To\naddress these obstacles, we integrate the category commonality and personalized\nstyle into diffusion models. Concretely, we propose a Category-Wise Generator\nto enable large-scale background generation with only one model for the first\ntime. A unique identifier in the prompt is assigned to each category, whose\nattention is located on the background by a mask-guided cross attention layer\nto learn the category-wise style. Furthermore, for products with specific and\nfine-grained requirements in layout, elements, etc, a Personality-Wise\nGenerator is devised to learn such personalized style directly from a reference\nimage to resolve textual ambiguities, and is trained in a self-supervised\nmanner for more efficient training data usage. To advance research in this\nfield, the first large-scale e-commerce product background generation dataset\nBG60k is constructed, which covers more than 60k product images from over 2k\ncategories. Experiments demonstrate that our method could generate high-quality\nbackgrounds for different categories, and maintain the personalized background\nstyle of reference images. BG60k will be available at\n\\url{https://github.com/Whileherham/BG60k}.\n", "link": "http://arxiv.org/abs/2312.13309v2", "date": "2025-01-17", "relevancy": 1.8313, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6389}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5762}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5734}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generate%20E-commerce%20Product%20Background%20by%20Integrating%20Category%0A%20%20Commonality%20and%20Personalized%20Style&body=Title%3A%20Generate%20E-commerce%20Product%20Background%20by%20Integrating%20Category%0A%20%20Commonality%20and%20Personalized%20Style%0AAuthor%3A%20Haohan%20Wang%20and%20Wei%20Feng%20and%20Yaoyu%20Li%20and%20Zheng%20Zhang%20and%20Jingjing%20Lv%20and%20Junjie%20Shen%20and%20Zhangang%20Lin%20and%20Jingping%20Shao%0AAbstract%3A%20%20%20The%20state-of-the-art%20methods%20for%20e-commerce%20product%20background%20generation%0Asuffer%20from%20the%20inefficiency%20of%20designing%20product-wise%20prompts%20when%20scaling%20up%0Athe%20production%2C%20as%20well%20as%20the%20ineffectiveness%20of%20describing%20fine-grained%0Astyles%20when%20customizing%20personalized%20backgrounds%20for%20some%20specific%20brands.%20To%0Aaddress%20these%20obstacles%2C%20we%20integrate%20the%20category%20commonality%20and%20personalized%0Astyle%20into%20diffusion%20models.%20Concretely%2C%20we%20propose%20a%20Category-Wise%20Generator%0Ato%20enable%20large-scale%20background%20generation%20with%20only%20one%20model%20for%20the%20first%0Atime.%20A%20unique%20identifier%20in%20the%20prompt%20is%20assigned%20to%20each%20category%2C%20whose%0Aattention%20is%20located%20on%20the%20background%20by%20a%20mask-guided%20cross%20attention%20layer%0Ato%20learn%20the%20category-wise%20style.%20Furthermore%2C%20for%20products%20with%20specific%20and%0Afine-grained%20requirements%20in%20layout%2C%20elements%2C%20etc%2C%20a%20Personality-Wise%0AGenerator%20is%20devised%20to%20learn%20such%20personalized%20style%20directly%20from%20a%20reference%0Aimage%20to%20resolve%20textual%20ambiguities%2C%20and%20is%20trained%20in%20a%20self-supervised%0Amanner%20for%20more%20efficient%20training%20data%20usage.%20To%20advance%20research%20in%20this%0Afield%2C%20the%20first%20large-scale%20e-commerce%20product%20background%20generation%20dataset%0ABG60k%20is%20constructed%2C%20which%20covers%20more%20than%2060k%20product%20images%20from%20over%202k%0Acategories.%20Experiments%20demonstrate%20that%20our%20method%20could%20generate%20high-quality%0Abackgrounds%20for%20different%20categories%2C%20and%20maintain%20the%20personalized%20background%0Astyle%20of%20reference%20images.%20BG60k%20will%20be%20available%20at%0A%5Curl%7Bhttps%3A//github.com/Whileherham/BG60k%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.13309v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerate%2520E-commerce%2520Product%2520Background%2520by%2520Integrating%2520Category%250A%2520%2520Commonality%2520and%2520Personalized%2520Style%26entry.906535625%3DHaohan%2520Wang%2520and%2520Wei%2520Feng%2520and%2520Yaoyu%2520Li%2520and%2520Zheng%2520Zhang%2520and%2520Jingjing%2520Lv%2520and%2520Junjie%2520Shen%2520and%2520Zhangang%2520Lin%2520and%2520Jingping%2520Shao%26entry.1292438233%3D%2520%2520The%2520state-of-the-art%2520methods%2520for%2520e-commerce%2520product%2520background%2520generation%250Asuffer%2520from%2520the%2520inefficiency%2520of%2520designing%2520product-wise%2520prompts%2520when%2520scaling%2520up%250Athe%2520production%252C%2520as%2520well%2520as%2520the%2520ineffectiveness%2520of%2520describing%2520fine-grained%250Astyles%2520when%2520customizing%2520personalized%2520backgrounds%2520for%2520some%2520specific%2520brands.%2520To%250Aaddress%2520these%2520obstacles%252C%2520we%2520integrate%2520the%2520category%2520commonality%2520and%2520personalized%250Astyle%2520into%2520diffusion%2520models.%2520Concretely%252C%2520we%2520propose%2520a%2520Category-Wise%2520Generator%250Ato%2520enable%2520large-scale%2520background%2520generation%2520with%2520only%2520one%2520model%2520for%2520the%2520first%250Atime.%2520A%2520unique%2520identifier%2520in%2520the%2520prompt%2520is%2520assigned%2520to%2520each%2520category%252C%2520whose%250Aattention%2520is%2520located%2520on%2520the%2520background%2520by%2520a%2520mask-guided%2520cross%2520attention%2520layer%250Ato%2520learn%2520the%2520category-wise%2520style.%2520Furthermore%252C%2520for%2520products%2520with%2520specific%2520and%250Afine-grained%2520requirements%2520in%2520layout%252C%2520elements%252C%2520etc%252C%2520a%2520Personality-Wise%250AGenerator%2520is%2520devised%2520to%2520learn%2520such%2520personalized%2520style%2520directly%2520from%2520a%2520reference%250Aimage%2520to%2520resolve%2520textual%2520ambiguities%252C%2520and%2520is%2520trained%2520in%2520a%2520self-supervised%250Amanner%2520for%2520more%2520efficient%2520training%2520data%2520usage.%2520To%2520advance%2520research%2520in%2520this%250Afield%252C%2520the%2520first%2520large-scale%2520e-commerce%2520product%2520background%2520generation%2520dataset%250ABG60k%2520is%2520constructed%252C%2520which%2520covers%2520more%2520than%252060k%2520product%2520images%2520from%2520over%25202k%250Acategories.%2520Experiments%2520demonstrate%2520that%2520our%2520method%2520could%2520generate%2520high-quality%250Abackgrounds%2520for%2520different%2520categories%252C%2520and%2520maintain%2520the%2520personalized%2520background%250Astyle%2520of%2520reference%2520images.%2520BG60k%2520will%2520be%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/Whileherham/BG60k%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.13309v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generate%20E-commerce%20Product%20Background%20by%20Integrating%20Category%0A%20%20Commonality%20and%20Personalized%20Style&entry.906535625=Haohan%20Wang%20and%20Wei%20Feng%20and%20Yaoyu%20Li%20and%20Zheng%20Zhang%20and%20Jingjing%20Lv%20and%20Junjie%20Shen%20and%20Zhangang%20Lin%20and%20Jingping%20Shao&entry.1292438233=%20%20The%20state-of-the-art%20methods%20for%20e-commerce%20product%20background%20generation%0Asuffer%20from%20the%20inefficiency%20of%20designing%20product-wise%20prompts%20when%20scaling%20up%0Athe%20production%2C%20as%20well%20as%20the%20ineffectiveness%20of%20describing%20fine-grained%0Astyles%20when%20customizing%20personalized%20backgrounds%20for%20some%20specific%20brands.%20To%0Aaddress%20these%20obstacles%2C%20we%20integrate%20the%20category%20commonality%20and%20personalized%0Astyle%20into%20diffusion%20models.%20Concretely%2C%20we%20propose%20a%20Category-Wise%20Generator%0Ato%20enable%20large-scale%20background%20generation%20with%20only%20one%20model%20for%20the%20first%0Atime.%20A%20unique%20identifier%20in%20the%20prompt%20is%20assigned%20to%20each%20category%2C%20whose%0Aattention%20is%20located%20on%20the%20background%20by%20a%20mask-guided%20cross%20attention%20layer%0Ato%20learn%20the%20category-wise%20style.%20Furthermore%2C%20for%20products%20with%20specific%20and%0Afine-grained%20requirements%20in%20layout%2C%20elements%2C%20etc%2C%20a%20Personality-Wise%0AGenerator%20is%20devised%20to%20learn%20such%20personalized%20style%20directly%20from%20a%20reference%0Aimage%20to%20resolve%20textual%20ambiguities%2C%20and%20is%20trained%20in%20a%20self-supervised%0Amanner%20for%20more%20efficient%20training%20data%20usage.%20To%20advance%20research%20in%20this%0Afield%2C%20the%20first%20large-scale%20e-commerce%20product%20background%20generation%20dataset%0ABG60k%20is%20constructed%2C%20which%20covers%20more%20than%2060k%20product%20images%20from%20over%202k%0Acategories.%20Experiments%20demonstrate%20that%20our%20method%20could%20generate%20high-quality%0Abackgrounds%20for%20different%20categories%2C%20and%20maintain%20the%20personalized%20background%0Astyle%20of%20reference%20images.%20BG60k%20will%20be%20available%20at%0A%5Curl%7Bhttps%3A//github.com/Whileherham/BG60k%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.13309v2&entry.124074799=Read"},
{"title": "Deep Compression Autoencoder for Efficient High-Resolution Diffusion\n  Models", "author": "Junyu Chen and Han Cai and Junsong Chen and Enze Xie and Shang Yang and Haotian Tang and Muyang Li and Yao Lu and Song Han", "abstract": "  We present Deep Compression Autoencoder (DC-AE), a new family of autoencoder\nmodels for accelerating high-resolution diffusion models. Existing autoencoder\nmodels have demonstrated impressive results at a moderate spatial compression\nratio (e.g., 8x), but fail to maintain satisfactory reconstruction accuracy for\nhigh spatial compression ratios (e.g., 64x). We address this challenge by\nintroducing two key techniques: (1) Residual Autoencoding, where we design our\nmodels to learn residuals based on the space-to-channel transformed features to\nalleviate the optimization difficulty of high spatial-compression autoencoders;\n(2) Decoupled High-Resolution Adaptation, an efficient decoupled three-phases\ntraining strategy for mitigating the generalization penalty of high\nspatial-compression autoencoders. With these designs, we improve the\nautoencoder's spatial compression ratio up to 128 while maintaining the\nreconstruction quality. Applying our DC-AE to latent diffusion models, we\nachieve significant speedup without accuracy drop. For example, on ImageNet\n512x512, our DC-AE provides 19.1x inference speedup and 17.9x training speedup\non H100 GPU for UViT-H while achieving a better FID, compared with the widely\nused SD-VAE-f8 autoencoder. Our code is available at\nhttps://github.com/mit-han-lab/efficientvit.\n", "link": "http://arxiv.org/abs/2410.10733v5", "date": "2025-01-17", "relevancy": 1.8166, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6656}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5886}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Compression%20Autoencoder%20for%20Efficient%20High-Resolution%20Diffusion%0A%20%20Models&body=Title%3A%20Deep%20Compression%20Autoencoder%20for%20Efficient%20High-Resolution%20Diffusion%0A%20%20Models%0AAuthor%3A%20Junyu%20Chen%20and%20Han%20Cai%20and%20Junsong%20Chen%20and%20Enze%20Xie%20and%20Shang%20Yang%20and%20Haotian%20Tang%20and%20Muyang%20Li%20and%20Yao%20Lu%20and%20Song%20Han%0AAbstract%3A%20%20%20We%20present%20Deep%20Compression%20Autoencoder%20%28DC-AE%29%2C%20a%20new%20family%20of%20autoencoder%0Amodels%20for%20accelerating%20high-resolution%20diffusion%20models.%20Existing%20autoencoder%0Amodels%20have%20demonstrated%20impressive%20results%20at%20a%20moderate%20spatial%20compression%0Aratio%20%28e.g.%2C%208x%29%2C%20but%20fail%20to%20maintain%20satisfactory%20reconstruction%20accuracy%20for%0Ahigh%20spatial%20compression%20ratios%20%28e.g.%2C%2064x%29.%20We%20address%20this%20challenge%20by%0Aintroducing%20two%20key%20techniques%3A%20%281%29%20Residual%20Autoencoding%2C%20where%20we%20design%20our%0Amodels%20to%20learn%20residuals%20based%20on%20the%20space-to-channel%20transformed%20features%20to%0Aalleviate%20the%20optimization%20difficulty%20of%20high%20spatial-compression%20autoencoders%3B%0A%282%29%20Decoupled%20High-Resolution%20Adaptation%2C%20an%20efficient%20decoupled%20three-phases%0Atraining%20strategy%20for%20mitigating%20the%20generalization%20penalty%20of%20high%0Aspatial-compression%20autoencoders.%20With%20these%20designs%2C%20we%20improve%20the%0Aautoencoder%27s%20spatial%20compression%20ratio%20up%20to%20128%20while%20maintaining%20the%0Areconstruction%20quality.%20Applying%20our%20DC-AE%20to%20latent%20diffusion%20models%2C%20we%0Aachieve%20significant%20speedup%20without%20accuracy%20drop.%20For%20example%2C%20on%20ImageNet%0A512x512%2C%20our%20DC-AE%20provides%2019.1x%20inference%20speedup%20and%2017.9x%20training%20speedup%0Aon%20H100%20GPU%20for%20UViT-H%20while%20achieving%20a%20better%20FID%2C%20compared%20with%20the%20widely%0Aused%20SD-VAE-f8%20autoencoder.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/mit-han-lab/efficientvit.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10733v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Compression%2520Autoencoder%2520for%2520Efficient%2520High-Resolution%2520Diffusion%250A%2520%2520Models%26entry.906535625%3DJunyu%2520Chen%2520and%2520Han%2520Cai%2520and%2520Junsong%2520Chen%2520and%2520Enze%2520Xie%2520and%2520Shang%2520Yang%2520and%2520Haotian%2520Tang%2520and%2520Muyang%2520Li%2520and%2520Yao%2520Lu%2520and%2520Song%2520Han%26entry.1292438233%3D%2520%2520We%2520present%2520Deep%2520Compression%2520Autoencoder%2520%2528DC-AE%2529%252C%2520a%2520new%2520family%2520of%2520autoencoder%250Amodels%2520for%2520accelerating%2520high-resolution%2520diffusion%2520models.%2520Existing%2520autoencoder%250Amodels%2520have%2520demonstrated%2520impressive%2520results%2520at%2520a%2520moderate%2520spatial%2520compression%250Aratio%2520%2528e.g.%252C%25208x%2529%252C%2520but%2520fail%2520to%2520maintain%2520satisfactory%2520reconstruction%2520accuracy%2520for%250Ahigh%2520spatial%2520compression%2520ratios%2520%2528e.g.%252C%252064x%2529.%2520We%2520address%2520this%2520challenge%2520by%250Aintroducing%2520two%2520key%2520techniques%253A%2520%25281%2529%2520Residual%2520Autoencoding%252C%2520where%2520we%2520design%2520our%250Amodels%2520to%2520learn%2520residuals%2520based%2520on%2520the%2520space-to-channel%2520transformed%2520features%2520to%250Aalleviate%2520the%2520optimization%2520difficulty%2520of%2520high%2520spatial-compression%2520autoencoders%253B%250A%25282%2529%2520Decoupled%2520High-Resolution%2520Adaptation%252C%2520an%2520efficient%2520decoupled%2520three-phases%250Atraining%2520strategy%2520for%2520mitigating%2520the%2520generalization%2520penalty%2520of%2520high%250Aspatial-compression%2520autoencoders.%2520With%2520these%2520designs%252C%2520we%2520improve%2520the%250Aautoencoder%2527s%2520spatial%2520compression%2520ratio%2520up%2520to%2520128%2520while%2520maintaining%2520the%250Areconstruction%2520quality.%2520Applying%2520our%2520DC-AE%2520to%2520latent%2520diffusion%2520models%252C%2520we%250Aachieve%2520significant%2520speedup%2520without%2520accuracy%2520drop.%2520For%2520example%252C%2520on%2520ImageNet%250A512x512%252C%2520our%2520DC-AE%2520provides%252019.1x%2520inference%2520speedup%2520and%252017.9x%2520training%2520speedup%250Aon%2520H100%2520GPU%2520for%2520UViT-H%2520while%2520achieving%2520a%2520better%2520FID%252C%2520compared%2520with%2520the%2520widely%250Aused%2520SD-VAE-f8%2520autoencoder.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/mit-han-lab/efficientvit.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10733v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Compression%20Autoencoder%20for%20Efficient%20High-Resolution%20Diffusion%0A%20%20Models&entry.906535625=Junyu%20Chen%20and%20Han%20Cai%20and%20Junsong%20Chen%20and%20Enze%20Xie%20and%20Shang%20Yang%20and%20Haotian%20Tang%20and%20Muyang%20Li%20and%20Yao%20Lu%20and%20Song%20Han&entry.1292438233=%20%20We%20present%20Deep%20Compression%20Autoencoder%20%28DC-AE%29%2C%20a%20new%20family%20of%20autoencoder%0Amodels%20for%20accelerating%20high-resolution%20diffusion%20models.%20Existing%20autoencoder%0Amodels%20have%20demonstrated%20impressive%20results%20at%20a%20moderate%20spatial%20compression%0Aratio%20%28e.g.%2C%208x%29%2C%20but%20fail%20to%20maintain%20satisfactory%20reconstruction%20accuracy%20for%0Ahigh%20spatial%20compression%20ratios%20%28e.g.%2C%2064x%29.%20We%20address%20this%20challenge%20by%0Aintroducing%20two%20key%20techniques%3A%20%281%29%20Residual%20Autoencoding%2C%20where%20we%20design%20our%0Amodels%20to%20learn%20residuals%20based%20on%20the%20space-to-channel%20transformed%20features%20to%0Aalleviate%20the%20optimization%20difficulty%20of%20high%20spatial-compression%20autoencoders%3B%0A%282%29%20Decoupled%20High-Resolution%20Adaptation%2C%20an%20efficient%20decoupled%20three-phases%0Atraining%20strategy%20for%20mitigating%20the%20generalization%20penalty%20of%20high%0Aspatial-compression%20autoencoders.%20With%20these%20designs%2C%20we%20improve%20the%0Aautoencoder%27s%20spatial%20compression%20ratio%20up%20to%20128%20while%20maintaining%20the%0Areconstruction%20quality.%20Applying%20our%20DC-AE%20to%20latent%20diffusion%20models%2C%20we%0Aachieve%20significant%20speedup%20without%20accuracy%20drop.%20For%20example%2C%20on%20ImageNet%0A512x512%2C%20our%20DC-AE%20provides%2019.1x%20inference%20speedup%20and%2017.9x%20training%20speedup%0Aon%20H100%20GPU%20for%20UViT-H%20while%20achieving%20a%20better%20FID%2C%20compared%20with%20the%20widely%0Aused%20SD-VAE-f8%20autoencoder.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/mit-han-lab/efficientvit.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10733v5&entry.124074799=Read"},
{"title": "Jailbreaking as a Reward Misspecification Problem", "author": "Zhihui Xie and Jiahui Gao and Lei Li and Zhenguo Li and Qi Liu and Lingpeng Kong", "abstract": "  The widespread adoption of large language models (LLMs) has raised concerns\nabout their safety and reliability, particularly regarding their vulnerability\nto adversarial attacks. In this paper, we propose a novel perspective that\nattributes this vulnerability to reward misspecification during the alignment\nprocess. This misspecification occurs when the reward function fails to\naccurately capture the intended behavior, leading to misaligned model outputs.\nWe introduce a metric ReGap to quantify the extent of reward misspecification\nand demonstrate its effectiveness and robustness in detecting harmful backdoor\nprompts. Building upon these insights, we present ReMiss, a system for\nautomated red teaming that generates adversarial prompts in a\nreward-misspecified space. ReMiss achieves state-of-the-art attack success\nrates on the AdvBench benchmark against various target aligned LLMs while\npreserving the human readability of the generated prompts. Furthermore, these\nattacks on open-source models demonstrate high transferability to closed-source\nmodels like GPT-4o and out-of-distribution tasks from HarmBench. Detailed\nanalysis highlights the unique advantages of the proposed reward\nmisspecification objective compared to previous methods, offering new insights\nfor improving LLM safety and robustness.\n", "link": "http://arxiv.org/abs/2406.14393v4", "date": "2025-01-17", "relevancy": 1.7726, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4535}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4451}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Jailbreaking%20as%20a%20Reward%20Misspecification%20Problem&body=Title%3A%20Jailbreaking%20as%20a%20Reward%20Misspecification%20Problem%0AAuthor%3A%20Zhihui%20Xie%20and%20Jiahui%20Gao%20and%20Lei%20Li%20and%20Zhenguo%20Li%20and%20Qi%20Liu%20and%20Lingpeng%20Kong%0AAbstract%3A%20%20%20The%20widespread%20adoption%20of%20large%20language%20models%20%28LLMs%29%20has%20raised%20concerns%0Aabout%20their%20safety%20and%20reliability%2C%20particularly%20regarding%20their%20vulnerability%0Ato%20adversarial%20attacks.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20perspective%20that%0Aattributes%20this%20vulnerability%20to%20reward%20misspecification%20during%20the%20alignment%0Aprocess.%20This%20misspecification%20occurs%20when%20the%20reward%20function%20fails%20to%0Aaccurately%20capture%20the%20intended%20behavior%2C%20leading%20to%20misaligned%20model%20outputs.%0AWe%20introduce%20a%20metric%20ReGap%20to%20quantify%20the%20extent%20of%20reward%20misspecification%0Aand%20demonstrate%20its%20effectiveness%20and%20robustness%20in%20detecting%20harmful%20backdoor%0Aprompts.%20Building%20upon%20these%20insights%2C%20we%20present%20ReMiss%2C%20a%20system%20for%0Aautomated%20red%20teaming%20that%20generates%20adversarial%20prompts%20in%20a%0Areward-misspecified%20space.%20ReMiss%20achieves%20state-of-the-art%20attack%20success%0Arates%20on%20the%20AdvBench%20benchmark%20against%20various%20target%20aligned%20LLMs%20while%0Apreserving%20the%20human%20readability%20of%20the%20generated%20prompts.%20Furthermore%2C%20these%0Aattacks%20on%20open-source%20models%20demonstrate%20high%20transferability%20to%20closed-source%0Amodels%20like%20GPT-4o%20and%20out-of-distribution%20tasks%20from%20HarmBench.%20Detailed%0Aanalysis%20highlights%20the%20unique%20advantages%20of%20the%20proposed%20reward%0Amisspecification%20objective%20compared%20to%20previous%20methods%2C%20offering%20new%20insights%0Afor%20improving%20LLM%20safety%20and%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14393v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJailbreaking%2520as%2520a%2520Reward%2520Misspecification%2520Problem%26entry.906535625%3DZhihui%2520Xie%2520and%2520Jiahui%2520Gao%2520and%2520Lei%2520Li%2520and%2520Zhenguo%2520Li%2520and%2520Qi%2520Liu%2520and%2520Lingpeng%2520Kong%26entry.1292438233%3D%2520%2520The%2520widespread%2520adoption%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520raised%2520concerns%250Aabout%2520their%2520safety%2520and%2520reliability%252C%2520particularly%2520regarding%2520their%2520vulnerability%250Ato%2520adversarial%2520attacks.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520perspective%2520that%250Aattributes%2520this%2520vulnerability%2520to%2520reward%2520misspecification%2520during%2520the%2520alignment%250Aprocess.%2520This%2520misspecification%2520occurs%2520when%2520the%2520reward%2520function%2520fails%2520to%250Aaccurately%2520capture%2520the%2520intended%2520behavior%252C%2520leading%2520to%2520misaligned%2520model%2520outputs.%250AWe%2520introduce%2520a%2520metric%2520ReGap%2520to%2520quantify%2520the%2520extent%2520of%2520reward%2520misspecification%250Aand%2520demonstrate%2520its%2520effectiveness%2520and%2520robustness%2520in%2520detecting%2520harmful%2520backdoor%250Aprompts.%2520Building%2520upon%2520these%2520insights%252C%2520we%2520present%2520ReMiss%252C%2520a%2520system%2520for%250Aautomated%2520red%2520teaming%2520that%2520generates%2520adversarial%2520prompts%2520in%2520a%250Areward-misspecified%2520space.%2520ReMiss%2520achieves%2520state-of-the-art%2520attack%2520success%250Arates%2520on%2520the%2520AdvBench%2520benchmark%2520against%2520various%2520target%2520aligned%2520LLMs%2520while%250Apreserving%2520the%2520human%2520readability%2520of%2520the%2520generated%2520prompts.%2520Furthermore%252C%2520these%250Aattacks%2520on%2520open-source%2520models%2520demonstrate%2520high%2520transferability%2520to%2520closed-source%250Amodels%2520like%2520GPT-4o%2520and%2520out-of-distribution%2520tasks%2520from%2520HarmBench.%2520Detailed%250Aanalysis%2520highlights%2520the%2520unique%2520advantages%2520of%2520the%2520proposed%2520reward%250Amisspecification%2520objective%2520compared%2520to%2520previous%2520methods%252C%2520offering%2520new%2520insights%250Afor%2520improving%2520LLM%2520safety%2520and%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14393v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Jailbreaking%20as%20a%20Reward%20Misspecification%20Problem&entry.906535625=Zhihui%20Xie%20and%20Jiahui%20Gao%20and%20Lei%20Li%20and%20Zhenguo%20Li%20and%20Qi%20Liu%20and%20Lingpeng%20Kong&entry.1292438233=%20%20The%20widespread%20adoption%20of%20large%20language%20models%20%28LLMs%29%20has%20raised%20concerns%0Aabout%20their%20safety%20and%20reliability%2C%20particularly%20regarding%20their%20vulnerability%0Ato%20adversarial%20attacks.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20perspective%20that%0Aattributes%20this%20vulnerability%20to%20reward%20misspecification%20during%20the%20alignment%0Aprocess.%20This%20misspecification%20occurs%20when%20the%20reward%20function%20fails%20to%0Aaccurately%20capture%20the%20intended%20behavior%2C%20leading%20to%20misaligned%20model%20outputs.%0AWe%20introduce%20a%20metric%20ReGap%20to%20quantify%20the%20extent%20of%20reward%20misspecification%0Aand%20demonstrate%20its%20effectiveness%20and%20robustness%20in%20detecting%20harmful%20backdoor%0Aprompts.%20Building%20upon%20these%20insights%2C%20we%20present%20ReMiss%2C%20a%20system%20for%0Aautomated%20red%20teaming%20that%20generates%20adversarial%20prompts%20in%20a%0Areward-misspecified%20space.%20ReMiss%20achieves%20state-of-the-art%20attack%20success%0Arates%20on%20the%20AdvBench%20benchmark%20against%20various%20target%20aligned%20LLMs%20while%0Apreserving%20the%20human%20readability%20of%20the%20generated%20prompts.%20Furthermore%2C%20these%0Aattacks%20on%20open-source%20models%20demonstrate%20high%20transferability%20to%20closed-source%0Amodels%20like%20GPT-4o%20and%20out-of-distribution%20tasks%20from%20HarmBench.%20Detailed%0Aanalysis%20highlights%20the%20unique%20advantages%20of%20the%20proposed%20reward%0Amisspecification%20objective%20compared%20to%20previous%20methods%2C%20offering%20new%20insights%0Afor%20improving%20LLM%20safety%20and%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14393v4&entry.124074799=Read"},
{"title": "Robotic World Model: A Neural Network Simulator for Robust Policy\n  Optimization in Robotics", "author": "Chenhao Li and Andreas Krause and Marco Hutter", "abstract": "  Learning robust and generalizable world models is crucial for enabling\nefficient and scalable robotic control in real-world environments. In this\nwork, we introduce a novel framework for learning world models that accurately\ncapture complex, partially observable, and stochastic dynamics. The proposed\nmethod employs a dual-autoregressive mechanism and self-supervised training to\nachieve reliable long-horizon predictions without relying on domain-specific\ninductive biases, ensuring adaptability across diverse robotic tasks. We\nfurther propose a policy optimization framework that leverages world models for\nefficient training in imagined environments and seamless deployment in\nreal-world systems. Through extensive experiments, our approach consistently\noutperforms state-of-the-art methods, demonstrating superior autoregressive\nprediction accuracy, robustness to noise, and generalization across\nmanipulation and locomotion tasks. Notably, policies trained with our method\nare successfully deployed on ANYmal D hardware in a zero-shot transfer,\nachieving robust performance with minimal sim-to-real performance loss. This\nwork advances model-based reinforcement learning by addressing the challenges\nof long-horizon prediction, error accumulation, and sim-to-real transfer. By\nproviding a scalable and robust framework, the introduced methods pave the way\nfor adaptive and efficient robotic systems in real-world applications.\n", "link": "http://arxiv.org/abs/2501.10100v1", "date": "2025-01-17", "relevancy": 1.7578, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6249}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6024}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5637}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robotic%20World%20Model%3A%20A%20Neural%20Network%20Simulator%20for%20Robust%20Policy%0A%20%20Optimization%20in%20Robotics&body=Title%3A%20Robotic%20World%20Model%3A%20A%20Neural%20Network%20Simulator%20for%20Robust%20Policy%0A%20%20Optimization%20in%20Robotics%0AAuthor%3A%20Chenhao%20Li%20and%20Andreas%20Krause%20and%20Marco%20Hutter%0AAbstract%3A%20%20%20Learning%20robust%20and%20generalizable%20world%20models%20is%20crucial%20for%20enabling%0Aefficient%20and%20scalable%20robotic%20control%20in%20real-world%20environments.%20In%20this%0Awork%2C%20we%20introduce%20a%20novel%20framework%20for%20learning%20world%20models%20that%20accurately%0Acapture%20complex%2C%20partially%20observable%2C%20and%20stochastic%20dynamics.%20The%20proposed%0Amethod%20employs%20a%20dual-autoregressive%20mechanism%20and%20self-supervised%20training%20to%0Aachieve%20reliable%20long-horizon%20predictions%20without%20relying%20on%20domain-specific%0Ainductive%20biases%2C%20ensuring%20adaptability%20across%20diverse%20robotic%20tasks.%20We%0Afurther%20propose%20a%20policy%20optimization%20framework%20that%20leverages%20world%20models%20for%0Aefficient%20training%20in%20imagined%20environments%20and%20seamless%20deployment%20in%0Areal-world%20systems.%20Through%20extensive%20experiments%2C%20our%20approach%20consistently%0Aoutperforms%20state-of-the-art%20methods%2C%20demonstrating%20superior%20autoregressive%0Aprediction%20accuracy%2C%20robustness%20to%20noise%2C%20and%20generalization%20across%0Amanipulation%20and%20locomotion%20tasks.%20Notably%2C%20policies%20trained%20with%20our%20method%0Aare%20successfully%20deployed%20on%20ANYmal%20D%20hardware%20in%20a%20zero-shot%20transfer%2C%0Aachieving%20robust%20performance%20with%20minimal%20sim-to-real%20performance%20loss.%20This%0Awork%20advances%20model-based%20reinforcement%20learning%20by%20addressing%20the%20challenges%0Aof%20long-horizon%20prediction%2C%20error%20accumulation%2C%20and%20sim-to-real%20transfer.%20By%0Aproviding%20a%20scalable%20and%20robust%20framework%2C%20the%20introduced%20methods%20pave%20the%20way%0Afor%20adaptive%20and%20efficient%20robotic%20systems%20in%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10100v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobotic%2520World%2520Model%253A%2520A%2520Neural%2520Network%2520Simulator%2520for%2520Robust%2520Policy%250A%2520%2520Optimization%2520in%2520Robotics%26entry.906535625%3DChenhao%2520Li%2520and%2520Andreas%2520Krause%2520and%2520Marco%2520Hutter%26entry.1292438233%3D%2520%2520Learning%2520robust%2520and%2520generalizable%2520world%2520models%2520is%2520crucial%2520for%2520enabling%250Aefficient%2520and%2520scalable%2520robotic%2520control%2520in%2520real-world%2520environments.%2520In%2520this%250Awork%252C%2520we%2520introduce%2520a%2520novel%2520framework%2520for%2520learning%2520world%2520models%2520that%2520accurately%250Acapture%2520complex%252C%2520partially%2520observable%252C%2520and%2520stochastic%2520dynamics.%2520The%2520proposed%250Amethod%2520employs%2520a%2520dual-autoregressive%2520mechanism%2520and%2520self-supervised%2520training%2520to%250Aachieve%2520reliable%2520long-horizon%2520predictions%2520without%2520relying%2520on%2520domain-specific%250Ainductive%2520biases%252C%2520ensuring%2520adaptability%2520across%2520diverse%2520robotic%2520tasks.%2520We%250Afurther%2520propose%2520a%2520policy%2520optimization%2520framework%2520that%2520leverages%2520world%2520models%2520for%250Aefficient%2520training%2520in%2520imagined%2520environments%2520and%2520seamless%2520deployment%2520in%250Areal-world%2520systems.%2520Through%2520extensive%2520experiments%252C%2520our%2520approach%2520consistently%250Aoutperforms%2520state-of-the-art%2520methods%252C%2520demonstrating%2520superior%2520autoregressive%250Aprediction%2520accuracy%252C%2520robustness%2520to%2520noise%252C%2520and%2520generalization%2520across%250Amanipulation%2520and%2520locomotion%2520tasks.%2520Notably%252C%2520policies%2520trained%2520with%2520our%2520method%250Aare%2520successfully%2520deployed%2520on%2520ANYmal%2520D%2520hardware%2520in%2520a%2520zero-shot%2520transfer%252C%250Aachieving%2520robust%2520performance%2520with%2520minimal%2520sim-to-real%2520performance%2520loss.%2520This%250Awork%2520advances%2520model-based%2520reinforcement%2520learning%2520by%2520addressing%2520the%2520challenges%250Aof%2520long-horizon%2520prediction%252C%2520error%2520accumulation%252C%2520and%2520sim-to-real%2520transfer.%2520By%250Aproviding%2520a%2520scalable%2520and%2520robust%2520framework%252C%2520the%2520introduced%2520methods%2520pave%2520the%2520way%250Afor%2520adaptive%2520and%2520efficient%2520robotic%2520systems%2520in%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10100v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robotic%20World%20Model%3A%20A%20Neural%20Network%20Simulator%20for%20Robust%20Policy%0A%20%20Optimization%20in%20Robotics&entry.906535625=Chenhao%20Li%20and%20Andreas%20Krause%20and%20Marco%20Hutter&entry.1292438233=%20%20Learning%20robust%20and%20generalizable%20world%20models%20is%20crucial%20for%20enabling%0Aefficient%20and%20scalable%20robotic%20control%20in%20real-world%20environments.%20In%20this%0Awork%2C%20we%20introduce%20a%20novel%20framework%20for%20learning%20world%20models%20that%20accurately%0Acapture%20complex%2C%20partially%20observable%2C%20and%20stochastic%20dynamics.%20The%20proposed%0Amethod%20employs%20a%20dual-autoregressive%20mechanism%20and%20self-supervised%20training%20to%0Aachieve%20reliable%20long-horizon%20predictions%20without%20relying%20on%20domain-specific%0Ainductive%20biases%2C%20ensuring%20adaptability%20across%20diverse%20robotic%20tasks.%20We%0Afurther%20propose%20a%20policy%20optimization%20framework%20that%20leverages%20world%20models%20for%0Aefficient%20training%20in%20imagined%20environments%20and%20seamless%20deployment%20in%0Areal-world%20systems.%20Through%20extensive%20experiments%2C%20our%20approach%20consistently%0Aoutperforms%20state-of-the-art%20methods%2C%20demonstrating%20superior%20autoregressive%0Aprediction%20accuracy%2C%20robustness%20to%20noise%2C%20and%20generalization%20across%0Amanipulation%20and%20locomotion%20tasks.%20Notably%2C%20policies%20trained%20with%20our%20method%0Aare%20successfully%20deployed%20on%20ANYmal%20D%20hardware%20in%20a%20zero-shot%20transfer%2C%0Aachieving%20robust%20performance%20with%20minimal%20sim-to-real%20performance%20loss.%20This%0Awork%20advances%20model-based%20reinforcement%20learning%20by%20addressing%20the%20challenges%0Aof%20long-horizon%20prediction%2C%20error%20accumulation%2C%20and%20sim-to-real%20transfer.%20By%0Aproviding%20a%20scalable%20and%20robust%20framework%2C%20the%20introduced%20methods%20pave%20the%20way%0Afor%20adaptive%20and%20efficient%20robotic%20systems%20in%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10100v1&entry.124074799=Read"},
{"title": "Zero-Shot Monocular Scene Flow Estimation in the Wild", "author": "Yiqing Liang and Abhishek Badki and Hang Su and James Tompkin and Orazio Gallo", "abstract": "  Large models have shown generalization across datasets for many low-level\nvision tasks, like depth estimation, but no such general models exist for scene\nflow. Even though scene flow has wide potential use, it is not used in practice\nbecause current predictive models do not generalize well. We identify three key\nchallenges and propose solutions for each.First, we create a method that\njointly estimates geometry and motion for accurate prediction. Second, we\nalleviate scene flow data scarcity with a data recipe that affords us 1M\nannotated training samples across diverse synthetic scenes. Third, we evaluate\ndifferent parameterizations for scene flow prediction and adopt a natural and\neffective parameterization. Our resulting model outperforms existing methods as\nwell as baselines built on large-scale models in terms of 3D end-point error,\nand shows zero-shot generalization to the casually captured videos from DAVIS\nand the robotic manipulation scenes from RoboTAP. Overall, our approach makes\nscene flow prediction more practical in-the-wild.\n", "link": "http://arxiv.org/abs/2501.10357v1", "date": "2025-01-17", "relevancy": 1.7443, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6054}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Monocular%20Scene%20Flow%20Estimation%20in%20the%20Wild&body=Title%3A%20Zero-Shot%20Monocular%20Scene%20Flow%20Estimation%20in%20the%20Wild%0AAuthor%3A%20Yiqing%20Liang%20and%20Abhishek%20Badki%20and%20Hang%20Su%20and%20James%20Tompkin%20and%20Orazio%20Gallo%0AAbstract%3A%20%20%20Large%20models%20have%20shown%20generalization%20across%20datasets%20for%20many%20low-level%0Avision%20tasks%2C%20like%20depth%20estimation%2C%20but%20no%20such%20general%20models%20exist%20for%20scene%0Aflow.%20Even%20though%20scene%20flow%20has%20wide%20potential%20use%2C%20it%20is%20not%20used%20in%20practice%0Abecause%20current%20predictive%20models%20do%20not%20generalize%20well.%20We%20identify%20three%20key%0Achallenges%20and%20propose%20solutions%20for%20each.First%2C%20we%20create%20a%20method%20that%0Ajointly%20estimates%20geometry%20and%20motion%20for%20accurate%20prediction.%20Second%2C%20we%0Aalleviate%20scene%20flow%20data%20scarcity%20with%20a%20data%20recipe%20that%20affords%20us%201M%0Aannotated%20training%20samples%20across%20diverse%20synthetic%20scenes.%20Third%2C%20we%20evaluate%0Adifferent%20parameterizations%20for%20scene%20flow%20prediction%20and%20adopt%20a%20natural%20and%0Aeffective%20parameterization.%20Our%20resulting%20model%20outperforms%20existing%20methods%20as%0Awell%20as%20baselines%20built%20on%20large-scale%20models%20in%20terms%20of%203D%20end-point%20error%2C%0Aand%20shows%20zero-shot%20generalization%20to%20the%20casually%20captured%20videos%20from%20DAVIS%0Aand%20the%20robotic%20manipulation%20scenes%20from%20RoboTAP.%20Overall%2C%20our%20approach%20makes%0Ascene%20flow%20prediction%20more%20practical%20in-the-wild.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10357v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Monocular%2520Scene%2520Flow%2520Estimation%2520in%2520the%2520Wild%26entry.906535625%3DYiqing%2520Liang%2520and%2520Abhishek%2520Badki%2520and%2520Hang%2520Su%2520and%2520James%2520Tompkin%2520and%2520Orazio%2520Gallo%26entry.1292438233%3D%2520%2520Large%2520models%2520have%2520shown%2520generalization%2520across%2520datasets%2520for%2520many%2520low-level%250Avision%2520tasks%252C%2520like%2520depth%2520estimation%252C%2520but%2520no%2520such%2520general%2520models%2520exist%2520for%2520scene%250Aflow.%2520Even%2520though%2520scene%2520flow%2520has%2520wide%2520potential%2520use%252C%2520it%2520is%2520not%2520used%2520in%2520practice%250Abecause%2520current%2520predictive%2520models%2520do%2520not%2520generalize%2520well.%2520We%2520identify%2520three%2520key%250Achallenges%2520and%2520propose%2520solutions%2520for%2520each.First%252C%2520we%2520create%2520a%2520method%2520that%250Ajointly%2520estimates%2520geometry%2520and%2520motion%2520for%2520accurate%2520prediction.%2520Second%252C%2520we%250Aalleviate%2520scene%2520flow%2520data%2520scarcity%2520with%2520a%2520data%2520recipe%2520that%2520affords%2520us%25201M%250Aannotated%2520training%2520samples%2520across%2520diverse%2520synthetic%2520scenes.%2520Third%252C%2520we%2520evaluate%250Adifferent%2520parameterizations%2520for%2520scene%2520flow%2520prediction%2520and%2520adopt%2520a%2520natural%2520and%250Aeffective%2520parameterization.%2520Our%2520resulting%2520model%2520outperforms%2520existing%2520methods%2520as%250Awell%2520as%2520baselines%2520built%2520on%2520large-scale%2520models%2520in%2520terms%2520of%25203D%2520end-point%2520error%252C%250Aand%2520shows%2520zero-shot%2520generalization%2520to%2520the%2520casually%2520captured%2520videos%2520from%2520DAVIS%250Aand%2520the%2520robotic%2520manipulation%2520scenes%2520from%2520RoboTAP.%2520Overall%252C%2520our%2520approach%2520makes%250Ascene%2520flow%2520prediction%2520more%2520practical%2520in-the-wild.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10357v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Monocular%20Scene%20Flow%20Estimation%20in%20the%20Wild&entry.906535625=Yiqing%20Liang%20and%20Abhishek%20Badki%20and%20Hang%20Su%20and%20James%20Tompkin%20and%20Orazio%20Gallo&entry.1292438233=%20%20Large%20models%20have%20shown%20generalization%20across%20datasets%20for%20many%20low-level%0Avision%20tasks%2C%20like%20depth%20estimation%2C%20but%20no%20such%20general%20models%20exist%20for%20scene%0Aflow.%20Even%20though%20scene%20flow%20has%20wide%20potential%20use%2C%20it%20is%20not%20used%20in%20practice%0Abecause%20current%20predictive%20models%20do%20not%20generalize%20well.%20We%20identify%20three%20key%0Achallenges%20and%20propose%20solutions%20for%20each.First%2C%20we%20create%20a%20method%20that%0Ajointly%20estimates%20geometry%20and%20motion%20for%20accurate%20prediction.%20Second%2C%20we%0Aalleviate%20scene%20flow%20data%20scarcity%20with%20a%20data%20recipe%20that%20affords%20us%201M%0Aannotated%20training%20samples%20across%20diverse%20synthetic%20scenes.%20Third%2C%20we%20evaluate%0Adifferent%20parameterizations%20for%20scene%20flow%20prediction%20and%20adopt%20a%20natural%20and%0Aeffective%20parameterization.%20Our%20resulting%20model%20outperforms%20existing%20methods%20as%0Awell%20as%20baselines%20built%20on%20large-scale%20models%20in%20terms%20of%203D%20end-point%20error%2C%0Aand%20shows%20zero-shot%20generalization%20to%20the%20casually%20captured%20videos%20from%20DAVIS%0Aand%20the%20robotic%20manipulation%20scenes%20from%20RoboTAP.%20Overall%2C%20our%20approach%20makes%0Ascene%20flow%20prediction%20more%20practical%20in-the-wild.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10357v1&entry.124074799=Read"},
{"title": "Contributions to the Decision Theoretic Foundations of Machine Learning\n  and Robust Statistics under Weakly Structured Information", "author": "Christoph Jansen", "abstract": "  This habilitation thesis is cumulative and, therefore, is collecting and\nconnecting research that I (together with several co-authors) have conducted\nover the last few years. Thus, the absolute core of the work is formed by the\nten publications listed on page 5 under the name Contributions 1 to 10. The\nreferences to the complete versions of these articles are also found in this\nlist, making them as easily accessible as possible for readers wishing to dive\ndeep into the different research projects. The chapters following this thesis,\nnamely Parts A to C and the concluding remarks, serve to place the articles in\na larger scientific context, to (briefly) explain their respective content on a\nless formal level, and to highlight some interesting perspectives for future\nresearch in their respective contexts. Naturally, therefore, the following\npresentation has neither the level of detail nor the formal rigor that can\n(hopefully) be found in the papers. The purpose of the following text is to\nprovide the reader an easy and high-level access to this interesting and\nimportant research field as a whole, thereby, advertising it to a broader\naudience.\n", "link": "http://arxiv.org/abs/2501.10195v1", "date": "2025-01-17", "relevancy": 1.7414, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4696}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4418}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contributions%20to%20the%20Decision%20Theoretic%20Foundations%20of%20Machine%20Learning%0A%20%20and%20Robust%20Statistics%20under%20Weakly%20Structured%20Information&body=Title%3A%20Contributions%20to%20the%20Decision%20Theoretic%20Foundations%20of%20Machine%20Learning%0A%20%20and%20Robust%20Statistics%20under%20Weakly%20Structured%20Information%0AAuthor%3A%20Christoph%20Jansen%0AAbstract%3A%20%20%20This%20habilitation%20thesis%20is%20cumulative%20and%2C%20therefore%2C%20is%20collecting%20and%0Aconnecting%20research%20that%20I%20%28together%20with%20several%20co-authors%29%20have%20conducted%0Aover%20the%20last%20few%20years.%20Thus%2C%20the%20absolute%20core%20of%20the%20work%20is%20formed%20by%20the%0Aten%20publications%20listed%20on%20page%205%20under%20the%20name%20Contributions%201%20to%2010.%20The%0Areferences%20to%20the%20complete%20versions%20of%20these%20articles%20are%20also%20found%20in%20this%0Alist%2C%20making%20them%20as%20easily%20accessible%20as%20possible%20for%20readers%20wishing%20to%20dive%0Adeep%20into%20the%20different%20research%20projects.%20The%20chapters%20following%20this%20thesis%2C%0Anamely%20Parts%20A%20to%20C%20and%20the%20concluding%20remarks%2C%20serve%20to%20place%20the%20articles%20in%0Aa%20larger%20scientific%20context%2C%20to%20%28briefly%29%20explain%20their%20respective%20content%20on%20a%0Aless%20formal%20level%2C%20and%20to%20highlight%20some%20interesting%20perspectives%20for%20future%0Aresearch%20in%20their%20respective%20contexts.%20Naturally%2C%20therefore%2C%20the%20following%0Apresentation%20has%20neither%20the%20level%20of%20detail%20nor%20the%20formal%20rigor%20that%20can%0A%28hopefully%29%20be%20found%20in%20the%20papers.%20The%20purpose%20of%20the%20following%20text%20is%20to%0Aprovide%20the%20reader%20an%20easy%20and%20high-level%20access%20to%20this%20interesting%20and%0Aimportant%20research%20field%20as%20a%20whole%2C%20thereby%2C%20advertising%20it%20to%20a%20broader%0Aaudience.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10195v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContributions%2520to%2520the%2520Decision%2520Theoretic%2520Foundations%2520of%2520Machine%2520Learning%250A%2520%2520and%2520Robust%2520Statistics%2520under%2520Weakly%2520Structured%2520Information%26entry.906535625%3DChristoph%2520Jansen%26entry.1292438233%3D%2520%2520This%2520habilitation%2520thesis%2520is%2520cumulative%2520and%252C%2520therefore%252C%2520is%2520collecting%2520and%250Aconnecting%2520research%2520that%2520I%2520%2528together%2520with%2520several%2520co-authors%2529%2520have%2520conducted%250Aover%2520the%2520last%2520few%2520years.%2520Thus%252C%2520the%2520absolute%2520core%2520of%2520the%2520work%2520is%2520formed%2520by%2520the%250Aten%2520publications%2520listed%2520on%2520page%25205%2520under%2520the%2520name%2520Contributions%25201%2520to%252010.%2520The%250Areferences%2520to%2520the%2520complete%2520versions%2520of%2520these%2520articles%2520are%2520also%2520found%2520in%2520this%250Alist%252C%2520making%2520them%2520as%2520easily%2520accessible%2520as%2520possible%2520for%2520readers%2520wishing%2520to%2520dive%250Adeep%2520into%2520the%2520different%2520research%2520projects.%2520The%2520chapters%2520following%2520this%2520thesis%252C%250Anamely%2520Parts%2520A%2520to%2520C%2520and%2520the%2520concluding%2520remarks%252C%2520serve%2520to%2520place%2520the%2520articles%2520in%250Aa%2520larger%2520scientific%2520context%252C%2520to%2520%2528briefly%2529%2520explain%2520their%2520respective%2520content%2520on%2520a%250Aless%2520formal%2520level%252C%2520and%2520to%2520highlight%2520some%2520interesting%2520perspectives%2520for%2520future%250Aresearch%2520in%2520their%2520respective%2520contexts.%2520Naturally%252C%2520therefore%252C%2520the%2520following%250Apresentation%2520has%2520neither%2520the%2520level%2520of%2520detail%2520nor%2520the%2520formal%2520rigor%2520that%2520can%250A%2528hopefully%2529%2520be%2520found%2520in%2520the%2520papers.%2520The%2520purpose%2520of%2520the%2520following%2520text%2520is%2520to%250Aprovide%2520the%2520reader%2520an%2520easy%2520and%2520high-level%2520access%2520to%2520this%2520interesting%2520and%250Aimportant%2520research%2520field%2520as%2520a%2520whole%252C%2520thereby%252C%2520advertising%2520it%2520to%2520a%2520broader%250Aaudience.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10195v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contributions%20to%20the%20Decision%20Theoretic%20Foundations%20of%20Machine%20Learning%0A%20%20and%20Robust%20Statistics%20under%20Weakly%20Structured%20Information&entry.906535625=Christoph%20Jansen&entry.1292438233=%20%20This%20habilitation%20thesis%20is%20cumulative%20and%2C%20therefore%2C%20is%20collecting%20and%0Aconnecting%20research%20that%20I%20%28together%20with%20several%20co-authors%29%20have%20conducted%0Aover%20the%20last%20few%20years.%20Thus%2C%20the%20absolute%20core%20of%20the%20work%20is%20formed%20by%20the%0Aten%20publications%20listed%20on%20page%205%20under%20the%20name%20Contributions%201%20to%2010.%20The%0Areferences%20to%20the%20complete%20versions%20of%20these%20articles%20are%20also%20found%20in%20this%0Alist%2C%20making%20them%20as%20easily%20accessible%20as%20possible%20for%20readers%20wishing%20to%20dive%0Adeep%20into%20the%20different%20research%20projects.%20The%20chapters%20following%20this%20thesis%2C%0Anamely%20Parts%20A%20to%20C%20and%20the%20concluding%20remarks%2C%20serve%20to%20place%20the%20articles%20in%0Aa%20larger%20scientific%20context%2C%20to%20%28briefly%29%20explain%20their%20respective%20content%20on%20a%0Aless%20formal%20level%2C%20and%20to%20highlight%20some%20interesting%20perspectives%20for%20future%0Aresearch%20in%20their%20respective%20contexts.%20Naturally%2C%20therefore%2C%20the%20following%0Apresentation%20has%20neither%20the%20level%20of%20detail%20nor%20the%20formal%20rigor%20that%20can%0A%28hopefully%29%20be%20found%20in%20the%20papers.%20The%20purpose%20of%20the%20following%20text%20is%20to%0Aprovide%20the%20reader%20an%20easy%20and%20high-level%20access%20to%20this%20interesting%20and%0Aimportant%20research%20field%20as%20a%20whole%2C%20thereby%2C%20advertising%20it%20to%20a%20broader%0Aaudience.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10195v1&entry.124074799=Read"},
{"title": "ESVO2: Direct Visual-Inertial Odometry with Stereo Event Cameras", "author": "Junkai Niu and Sheng Zhong and Xiuyuan Lu and Shaojie Shen and Guillermo Gallego and Yi Zhou", "abstract": "  Event-based visual odometry is a specific branch of visual Simultaneous\nLocalization and Mapping (SLAM) techniques, which aims at solving tracking and\nmapping subproblems (typically in parallel), by exploiting the special working\nprinciples of neuromorphic (i.e., event-based) cameras. Due to the\nmotion-dependent nature of event data, explicit data association (i.e., feature\nmatching) under large-baseline view-point changes is difficult to establish,\nmaking direct methods a more rational choice. However, state-of-the-art direct\nmethods are limited by the high computational complexity of the mapping\nsub-problem and the degeneracy of camera pose tracking in certain degrees of\nfreedom (DoF) in rotation. In this paper, we tackle these issues by building an\nevent-based stereo visual-inertial odometry system on top of a direct pipeline.\nSpecifically, to speed up the mapping operation, we propose an efficient\nstrategy for sampling contour points according to the local dynamics of events.\nThe mapping performance is also improved in terms of structure completeness and\nlocal smoothness by merging the temporal stereo and static stereo results. To\ncircumvent the degeneracy of camera pose tracking in recovering the pitch and\nyaw components of general 6-DoF motion, we introduce IMU measurements as motion\npriors via pre-integration. To this end, a compact back-end is proposed for\ncontinuously updating the IMU bias and predicting the linear velocity, enabling\nan accurate motion prediction for camera pose tracking. The resulting system\nscales well with modern high-resolution event cameras and leads to better\nglobal positioning accuracy in large-scale outdoor environments. Extensive\nevaluations on five publicly available datasets featuring different resolutions\nand scenarios justify the superior performance of the proposed system against\nfive state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2410.09374v2", "date": "2025-01-17", "relevancy": 1.7408, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5869}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5806}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5775}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ESVO2%3A%20Direct%20Visual-Inertial%20Odometry%20with%20Stereo%20Event%20Cameras&body=Title%3A%20ESVO2%3A%20Direct%20Visual-Inertial%20Odometry%20with%20Stereo%20Event%20Cameras%0AAuthor%3A%20Junkai%20Niu%20and%20Sheng%20Zhong%20and%20Xiuyuan%20Lu%20and%20Shaojie%20Shen%20and%20Guillermo%20Gallego%20and%20Yi%20Zhou%0AAbstract%3A%20%20%20Event-based%20visual%20odometry%20is%20a%20specific%20branch%20of%20visual%20Simultaneous%0ALocalization%20and%20Mapping%20%28SLAM%29%20techniques%2C%20which%20aims%20at%20solving%20tracking%20and%0Amapping%20subproblems%20%28typically%20in%20parallel%29%2C%20by%20exploiting%20the%20special%20working%0Aprinciples%20of%20neuromorphic%20%28i.e.%2C%20event-based%29%20cameras.%20Due%20to%20the%0Amotion-dependent%20nature%20of%20event%20data%2C%20explicit%20data%20association%20%28i.e.%2C%20feature%0Amatching%29%20under%20large-baseline%20view-point%20changes%20is%20difficult%20to%20establish%2C%0Amaking%20direct%20methods%20a%20more%20rational%20choice.%20However%2C%20state-of-the-art%20direct%0Amethods%20are%20limited%20by%20the%20high%20computational%20complexity%20of%20the%20mapping%0Asub-problem%20and%20the%20degeneracy%20of%20camera%20pose%20tracking%20in%20certain%20degrees%20of%0Afreedom%20%28DoF%29%20in%20rotation.%20In%20this%20paper%2C%20we%20tackle%20these%20issues%20by%20building%20an%0Aevent-based%20stereo%20visual-inertial%20odometry%20system%20on%20top%20of%20a%20direct%20pipeline.%0ASpecifically%2C%20to%20speed%20up%20the%20mapping%20operation%2C%20we%20propose%20an%20efficient%0Astrategy%20for%20sampling%20contour%20points%20according%20to%20the%20local%20dynamics%20of%20events.%0AThe%20mapping%20performance%20is%20also%20improved%20in%20terms%20of%20structure%20completeness%20and%0Alocal%20smoothness%20by%20merging%20the%20temporal%20stereo%20and%20static%20stereo%20results.%20To%0Acircumvent%20the%20degeneracy%20of%20camera%20pose%20tracking%20in%20recovering%20the%20pitch%20and%0Ayaw%20components%20of%20general%206-DoF%20motion%2C%20we%20introduce%20IMU%20measurements%20as%20motion%0Apriors%20via%20pre-integration.%20To%20this%20end%2C%20a%20compact%20back-end%20is%20proposed%20for%0Acontinuously%20updating%20the%20IMU%20bias%20and%20predicting%20the%20linear%20velocity%2C%20enabling%0Aan%20accurate%20motion%20prediction%20for%20camera%20pose%20tracking.%20The%20resulting%20system%0Ascales%20well%20with%20modern%20high-resolution%20event%20cameras%20and%20leads%20to%20better%0Aglobal%20positioning%20accuracy%20in%20large-scale%20outdoor%20environments.%20Extensive%0Aevaluations%20on%20five%20publicly%20available%20datasets%20featuring%20different%20resolutions%0Aand%20scenarios%20justify%20the%20superior%20performance%20of%20the%20proposed%20system%20against%0Afive%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.09374v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DESVO2%253A%2520Direct%2520Visual-Inertial%2520Odometry%2520with%2520Stereo%2520Event%2520Cameras%26entry.906535625%3DJunkai%2520Niu%2520and%2520Sheng%2520Zhong%2520and%2520Xiuyuan%2520Lu%2520and%2520Shaojie%2520Shen%2520and%2520Guillermo%2520Gallego%2520and%2520Yi%2520Zhou%26entry.1292438233%3D%2520%2520Event-based%2520visual%2520odometry%2520is%2520a%2520specific%2520branch%2520of%2520visual%2520Simultaneous%250ALocalization%2520and%2520Mapping%2520%2528SLAM%2529%2520techniques%252C%2520which%2520aims%2520at%2520solving%2520tracking%2520and%250Amapping%2520subproblems%2520%2528typically%2520in%2520parallel%2529%252C%2520by%2520exploiting%2520the%2520special%2520working%250Aprinciples%2520of%2520neuromorphic%2520%2528i.e.%252C%2520event-based%2529%2520cameras.%2520Due%2520to%2520the%250Amotion-dependent%2520nature%2520of%2520event%2520data%252C%2520explicit%2520data%2520association%2520%2528i.e.%252C%2520feature%250Amatching%2529%2520under%2520large-baseline%2520view-point%2520changes%2520is%2520difficult%2520to%2520establish%252C%250Amaking%2520direct%2520methods%2520a%2520more%2520rational%2520choice.%2520However%252C%2520state-of-the-art%2520direct%250Amethods%2520are%2520limited%2520by%2520the%2520high%2520computational%2520complexity%2520of%2520the%2520mapping%250Asub-problem%2520and%2520the%2520degeneracy%2520of%2520camera%2520pose%2520tracking%2520in%2520certain%2520degrees%2520of%250Afreedom%2520%2528DoF%2529%2520in%2520rotation.%2520In%2520this%2520paper%252C%2520we%2520tackle%2520these%2520issues%2520by%2520building%2520an%250Aevent-based%2520stereo%2520visual-inertial%2520odometry%2520system%2520on%2520top%2520of%2520a%2520direct%2520pipeline.%250ASpecifically%252C%2520to%2520speed%2520up%2520the%2520mapping%2520operation%252C%2520we%2520propose%2520an%2520efficient%250Astrategy%2520for%2520sampling%2520contour%2520points%2520according%2520to%2520the%2520local%2520dynamics%2520of%2520events.%250AThe%2520mapping%2520performance%2520is%2520also%2520improved%2520in%2520terms%2520of%2520structure%2520completeness%2520and%250Alocal%2520smoothness%2520by%2520merging%2520the%2520temporal%2520stereo%2520and%2520static%2520stereo%2520results.%2520To%250Acircumvent%2520the%2520degeneracy%2520of%2520camera%2520pose%2520tracking%2520in%2520recovering%2520the%2520pitch%2520and%250Ayaw%2520components%2520of%2520general%25206-DoF%2520motion%252C%2520we%2520introduce%2520IMU%2520measurements%2520as%2520motion%250Apriors%2520via%2520pre-integration.%2520To%2520this%2520end%252C%2520a%2520compact%2520back-end%2520is%2520proposed%2520for%250Acontinuously%2520updating%2520the%2520IMU%2520bias%2520and%2520predicting%2520the%2520linear%2520velocity%252C%2520enabling%250Aan%2520accurate%2520motion%2520prediction%2520for%2520camera%2520pose%2520tracking.%2520The%2520resulting%2520system%250Ascales%2520well%2520with%2520modern%2520high-resolution%2520event%2520cameras%2520and%2520leads%2520to%2520better%250Aglobal%2520positioning%2520accuracy%2520in%2520large-scale%2520outdoor%2520environments.%2520Extensive%250Aevaluations%2520on%2520five%2520publicly%2520available%2520datasets%2520featuring%2520different%2520resolutions%250Aand%2520scenarios%2520justify%2520the%2520superior%2520performance%2520of%2520the%2520proposed%2520system%2520against%250Afive%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.09374v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ESVO2%3A%20Direct%20Visual-Inertial%20Odometry%20with%20Stereo%20Event%20Cameras&entry.906535625=Junkai%20Niu%20and%20Sheng%20Zhong%20and%20Xiuyuan%20Lu%20and%20Shaojie%20Shen%20and%20Guillermo%20Gallego%20and%20Yi%20Zhou&entry.1292438233=%20%20Event-based%20visual%20odometry%20is%20a%20specific%20branch%20of%20visual%20Simultaneous%0ALocalization%20and%20Mapping%20%28SLAM%29%20techniques%2C%20which%20aims%20at%20solving%20tracking%20and%0Amapping%20subproblems%20%28typically%20in%20parallel%29%2C%20by%20exploiting%20the%20special%20working%0Aprinciples%20of%20neuromorphic%20%28i.e.%2C%20event-based%29%20cameras.%20Due%20to%20the%0Amotion-dependent%20nature%20of%20event%20data%2C%20explicit%20data%20association%20%28i.e.%2C%20feature%0Amatching%29%20under%20large-baseline%20view-point%20changes%20is%20difficult%20to%20establish%2C%0Amaking%20direct%20methods%20a%20more%20rational%20choice.%20However%2C%20state-of-the-art%20direct%0Amethods%20are%20limited%20by%20the%20high%20computational%20complexity%20of%20the%20mapping%0Asub-problem%20and%20the%20degeneracy%20of%20camera%20pose%20tracking%20in%20certain%20degrees%20of%0Afreedom%20%28DoF%29%20in%20rotation.%20In%20this%20paper%2C%20we%20tackle%20these%20issues%20by%20building%20an%0Aevent-based%20stereo%20visual-inertial%20odometry%20system%20on%20top%20of%20a%20direct%20pipeline.%0ASpecifically%2C%20to%20speed%20up%20the%20mapping%20operation%2C%20we%20propose%20an%20efficient%0Astrategy%20for%20sampling%20contour%20points%20according%20to%20the%20local%20dynamics%20of%20events.%0AThe%20mapping%20performance%20is%20also%20improved%20in%20terms%20of%20structure%20completeness%20and%0Alocal%20smoothness%20by%20merging%20the%20temporal%20stereo%20and%20static%20stereo%20results.%20To%0Acircumvent%20the%20degeneracy%20of%20camera%20pose%20tracking%20in%20recovering%20the%20pitch%20and%0Ayaw%20components%20of%20general%206-DoF%20motion%2C%20we%20introduce%20IMU%20measurements%20as%20motion%0Apriors%20via%20pre-integration.%20To%20this%20end%2C%20a%20compact%20back-end%20is%20proposed%20for%0Acontinuously%20updating%20the%20IMU%20bias%20and%20predicting%20the%20linear%20velocity%2C%20enabling%0Aan%20accurate%20motion%20prediction%20for%20camera%20pose%20tracking.%20The%20resulting%20system%0Ascales%20well%20with%20modern%20high-resolution%20event%20cameras%20and%20leads%20to%20better%0Aglobal%20positioning%20accuracy%20in%20large-scale%20outdoor%20environments.%20Extensive%0Aevaluations%20on%20five%20publicly%20available%20datasets%20featuring%20different%20resolutions%0Aand%20scenarios%20justify%20the%20superior%20performance%20of%20the%20proposed%20system%20against%0Afive%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.09374v2&entry.124074799=Read"},
{"title": "DiffStereo: High-Frequency Aware Diffusion Model for Stereo Image\n  Restoration", "author": "Huiyun Cao and Yuan Shi and Bin Xia and Xiaoyu Jin and Wenming Yang", "abstract": "  Diffusion models (DMs) have achieved promising performance in image\nrestoration but haven't been explored for stereo images. The application of DM\nin stereo image restoration is confronted with a series of challenges. The need\nto reconstruct two images exacerbates DM's computational cost. Additionally,\nexisting latent DMs usually focus on semantic information and remove\nhigh-frequency details as redundancy during latent compression, which is\nprecisely what matters for image restoration. To address the above problems, we\npropose a high-frequency aware diffusion model, DiffStereo for stereo image\nrestoration as the first attempt at DM in this domain. Specifically, DiffStereo\nfirst learns latent high-frequency representations (LHFR) of HQ images. DM is\nthen trained in the learned space to estimate LHFR for stereo images, which are\nfused into a transformer-based stereo image restoration network providing\nbeneficial high-frequency information of corresponding HQ images. The\nresolution of LHFR is kept the same as input images, which preserves the\ninherent texture from distortion. And the compression in channels alleviates\nthe computational burden of DM. Furthermore, we devise a position encoding\nscheme when integrating the LHFR into the restoration network, enabling\ndistinctive guidance in different depths of the restoration network.\nComprehensive experiments verify that by combining generative DM and\ntransformer, DiffStereo achieves both higher reconstruction accuracy and better\nperceptual quality on stereo super-resolution, deblurring, and low-light\nenhancement compared with state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2501.10325v1", "date": "2025-01-17", "relevancy": 1.7387, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6121}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5716}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5669}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffStereo%3A%20High-Frequency%20Aware%20Diffusion%20Model%20for%20Stereo%20Image%0A%20%20Restoration&body=Title%3A%20DiffStereo%3A%20High-Frequency%20Aware%20Diffusion%20Model%20for%20Stereo%20Image%0A%20%20Restoration%0AAuthor%3A%20Huiyun%20Cao%20and%20Yuan%20Shi%20and%20Bin%20Xia%20and%20Xiaoyu%20Jin%20and%20Wenming%20Yang%0AAbstract%3A%20%20%20Diffusion%20models%20%28DMs%29%20have%20achieved%20promising%20performance%20in%20image%0Arestoration%20but%20haven%27t%20been%20explored%20for%20stereo%20images.%20The%20application%20of%20DM%0Ain%20stereo%20image%20restoration%20is%20confronted%20with%20a%20series%20of%20challenges.%20The%20need%0Ato%20reconstruct%20two%20images%20exacerbates%20DM%27s%20computational%20cost.%20Additionally%2C%0Aexisting%20latent%20DMs%20usually%20focus%20on%20semantic%20information%20and%20remove%0Ahigh-frequency%20details%20as%20redundancy%20during%20latent%20compression%2C%20which%20is%0Aprecisely%20what%20matters%20for%20image%20restoration.%20To%20address%20the%20above%20problems%2C%20we%0Apropose%20a%20high-frequency%20aware%20diffusion%20model%2C%20DiffStereo%20for%20stereo%20image%0Arestoration%20as%20the%20first%20attempt%20at%20DM%20in%20this%20domain.%20Specifically%2C%20DiffStereo%0Afirst%20learns%20latent%20high-frequency%20representations%20%28LHFR%29%20of%20HQ%20images.%20DM%20is%0Athen%20trained%20in%20the%20learned%20space%20to%20estimate%20LHFR%20for%20stereo%20images%2C%20which%20are%0Afused%20into%20a%20transformer-based%20stereo%20image%20restoration%20network%20providing%0Abeneficial%20high-frequency%20information%20of%20corresponding%20HQ%20images.%20The%0Aresolution%20of%20LHFR%20is%20kept%20the%20same%20as%20input%20images%2C%20which%20preserves%20the%0Ainherent%20texture%20from%20distortion.%20And%20the%20compression%20in%20channels%20alleviates%0Athe%20computational%20burden%20of%20DM.%20Furthermore%2C%20we%20devise%20a%20position%20encoding%0Ascheme%20when%20integrating%20the%20LHFR%20into%20the%20restoration%20network%2C%20enabling%0Adistinctive%20guidance%20in%20different%20depths%20of%20the%20restoration%20network.%0AComprehensive%20experiments%20verify%20that%20by%20combining%20generative%20DM%20and%0Atransformer%2C%20DiffStereo%20achieves%20both%20higher%20reconstruction%20accuracy%20and%20better%0Aperceptual%20quality%20on%20stereo%20super-resolution%2C%20deblurring%2C%20and%20low-light%0Aenhancement%20compared%20with%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10325v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffStereo%253A%2520High-Frequency%2520Aware%2520Diffusion%2520Model%2520for%2520Stereo%2520Image%250A%2520%2520Restoration%26entry.906535625%3DHuiyun%2520Cao%2520and%2520Yuan%2520Shi%2520and%2520Bin%2520Xia%2520and%2520Xiaoyu%2520Jin%2520and%2520Wenming%2520Yang%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520%2528DMs%2529%2520have%2520achieved%2520promising%2520performance%2520in%2520image%250Arestoration%2520but%2520haven%2527t%2520been%2520explored%2520for%2520stereo%2520images.%2520The%2520application%2520of%2520DM%250Ain%2520stereo%2520image%2520restoration%2520is%2520confronted%2520with%2520a%2520series%2520of%2520challenges.%2520The%2520need%250Ato%2520reconstruct%2520two%2520images%2520exacerbates%2520DM%2527s%2520computational%2520cost.%2520Additionally%252C%250Aexisting%2520latent%2520DMs%2520usually%2520focus%2520on%2520semantic%2520information%2520and%2520remove%250Ahigh-frequency%2520details%2520as%2520redundancy%2520during%2520latent%2520compression%252C%2520which%2520is%250Aprecisely%2520what%2520matters%2520for%2520image%2520restoration.%2520To%2520address%2520the%2520above%2520problems%252C%2520we%250Apropose%2520a%2520high-frequency%2520aware%2520diffusion%2520model%252C%2520DiffStereo%2520for%2520stereo%2520image%250Arestoration%2520as%2520the%2520first%2520attempt%2520at%2520DM%2520in%2520this%2520domain.%2520Specifically%252C%2520DiffStereo%250Afirst%2520learns%2520latent%2520high-frequency%2520representations%2520%2528LHFR%2529%2520of%2520HQ%2520images.%2520DM%2520is%250Athen%2520trained%2520in%2520the%2520learned%2520space%2520to%2520estimate%2520LHFR%2520for%2520stereo%2520images%252C%2520which%2520are%250Afused%2520into%2520a%2520transformer-based%2520stereo%2520image%2520restoration%2520network%2520providing%250Abeneficial%2520high-frequency%2520information%2520of%2520corresponding%2520HQ%2520images.%2520The%250Aresolution%2520of%2520LHFR%2520is%2520kept%2520the%2520same%2520as%2520input%2520images%252C%2520which%2520preserves%2520the%250Ainherent%2520texture%2520from%2520distortion.%2520And%2520the%2520compression%2520in%2520channels%2520alleviates%250Athe%2520computational%2520burden%2520of%2520DM.%2520Furthermore%252C%2520we%2520devise%2520a%2520position%2520encoding%250Ascheme%2520when%2520integrating%2520the%2520LHFR%2520into%2520the%2520restoration%2520network%252C%2520enabling%250Adistinctive%2520guidance%2520in%2520different%2520depths%2520of%2520the%2520restoration%2520network.%250AComprehensive%2520experiments%2520verify%2520that%2520by%2520combining%2520generative%2520DM%2520and%250Atransformer%252C%2520DiffStereo%2520achieves%2520both%2520higher%2520reconstruction%2520accuracy%2520and%2520better%250Aperceptual%2520quality%2520on%2520stereo%2520super-resolution%252C%2520deblurring%252C%2520and%2520low-light%250Aenhancement%2520compared%2520with%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10325v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffStereo%3A%20High-Frequency%20Aware%20Diffusion%20Model%20for%20Stereo%20Image%0A%20%20Restoration&entry.906535625=Huiyun%20Cao%20and%20Yuan%20Shi%20and%20Bin%20Xia%20and%20Xiaoyu%20Jin%20and%20Wenming%20Yang&entry.1292438233=%20%20Diffusion%20models%20%28DMs%29%20have%20achieved%20promising%20performance%20in%20image%0Arestoration%20but%20haven%27t%20been%20explored%20for%20stereo%20images.%20The%20application%20of%20DM%0Ain%20stereo%20image%20restoration%20is%20confronted%20with%20a%20series%20of%20challenges.%20The%20need%0Ato%20reconstruct%20two%20images%20exacerbates%20DM%27s%20computational%20cost.%20Additionally%2C%0Aexisting%20latent%20DMs%20usually%20focus%20on%20semantic%20information%20and%20remove%0Ahigh-frequency%20details%20as%20redundancy%20during%20latent%20compression%2C%20which%20is%0Aprecisely%20what%20matters%20for%20image%20restoration.%20To%20address%20the%20above%20problems%2C%20we%0Apropose%20a%20high-frequency%20aware%20diffusion%20model%2C%20DiffStereo%20for%20stereo%20image%0Arestoration%20as%20the%20first%20attempt%20at%20DM%20in%20this%20domain.%20Specifically%2C%20DiffStereo%0Afirst%20learns%20latent%20high-frequency%20representations%20%28LHFR%29%20of%20HQ%20images.%20DM%20is%0Athen%20trained%20in%20the%20learned%20space%20to%20estimate%20LHFR%20for%20stereo%20images%2C%20which%20are%0Afused%20into%20a%20transformer-based%20stereo%20image%20restoration%20network%20providing%0Abeneficial%20high-frequency%20information%20of%20corresponding%20HQ%20images.%20The%0Aresolution%20of%20LHFR%20is%20kept%20the%20same%20as%20input%20images%2C%20which%20preserves%20the%0Ainherent%20texture%20from%20distortion.%20And%20the%20compression%20in%20channels%20alleviates%0Athe%20computational%20burden%20of%20DM.%20Furthermore%2C%20we%20devise%20a%20position%20encoding%0Ascheme%20when%20integrating%20the%20LHFR%20into%20the%20restoration%20network%2C%20enabling%0Adistinctive%20guidance%20in%20different%20depths%20of%20the%20restoration%20network.%0AComprehensive%20experiments%20verify%20that%20by%20combining%20generative%20DM%20and%0Atransformer%2C%20DiffStereo%20achieves%20both%20higher%20reconstruction%20accuracy%20and%20better%0Aperceptual%20quality%20on%20stereo%20super-resolution%2C%20deblurring%2C%20and%20low-light%0Aenhancement%20compared%20with%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10325v1&entry.124074799=Read"},
{"title": "Hybrid Deep Learning Model for epileptic seizure classification by using\n  1D-CNN with multi-head attention mechanism", "author": "Mohammed Guhdar and Ramadhan J. Mstafa and Abdulhakeem O. Mohammed", "abstract": "  Epilepsy is a prevalent neurological disorder globally, impacting around 50\nmillion people \\cite{WHO_epilepsy_50million}. Epileptic seizures result from\nsudden abnormal electrical activity in the brain, which can be read as sudden\nand significant changes in the EEG signal of the brain. The signal can vary in\nseverity and frequency, which results in loss of consciousness and muscle\ncontractions for a short period of time \\cite{epilepsyfoundation_myoclonic}.\nIndividuals with epilepsy often face significant employment challenges due to\nsafety concerns in certain work environments. Many jobs that involve working at\nheights, operating heavy machinery, or in other potentially hazardous settings\nmay be restricted for people with seizure disorders. This certainly limits job\noptions and economic opportunities for those living with epilepsy.\n", "link": "http://arxiv.org/abs/2501.10342v1", "date": "2025-01-17", "relevancy": 1.731, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4672}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4142}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Deep%20Learning%20Model%20for%20epileptic%20seizure%20classification%20by%20using%0A%20%201D-CNN%20with%20multi-head%20attention%20mechanism&body=Title%3A%20Hybrid%20Deep%20Learning%20Model%20for%20epileptic%20seizure%20classification%20by%20using%0A%20%201D-CNN%20with%20multi-head%20attention%20mechanism%0AAuthor%3A%20Mohammed%20Guhdar%20and%20Ramadhan%20J.%20Mstafa%20and%20Abdulhakeem%20O.%20Mohammed%0AAbstract%3A%20%20%20Epilepsy%20is%20a%20prevalent%20neurological%20disorder%20globally%2C%20impacting%20around%2050%0Amillion%20people%20%5Ccite%7BWHO_epilepsy_50million%7D.%20Epileptic%20seizures%20result%20from%0Asudden%20abnormal%20electrical%20activity%20in%20the%20brain%2C%20which%20can%20be%20read%20as%20sudden%0Aand%20significant%20changes%20in%20the%20EEG%20signal%20of%20the%20brain.%20The%20signal%20can%20vary%20in%0Aseverity%20and%20frequency%2C%20which%20results%20in%20loss%20of%20consciousness%20and%20muscle%0Acontractions%20for%20a%20short%20period%20of%20time%20%5Ccite%7Bepilepsyfoundation_myoclonic%7D.%0AIndividuals%20with%20epilepsy%20often%20face%20significant%20employment%20challenges%20due%20to%0Asafety%20concerns%20in%20certain%20work%20environments.%20Many%20jobs%20that%20involve%20working%20at%0Aheights%2C%20operating%20heavy%20machinery%2C%20or%20in%20other%20potentially%20hazardous%20settings%0Amay%20be%20restricted%20for%20people%20with%20seizure%20disorders.%20This%20certainly%20limits%20job%0Aoptions%20and%20economic%20opportunities%20for%20those%20living%20with%20epilepsy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10342v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Deep%2520Learning%2520Model%2520for%2520epileptic%2520seizure%2520classification%2520by%2520using%250A%2520%25201D-CNN%2520with%2520multi-head%2520attention%2520mechanism%26entry.906535625%3DMohammed%2520Guhdar%2520and%2520Ramadhan%2520J.%2520Mstafa%2520and%2520Abdulhakeem%2520O.%2520Mohammed%26entry.1292438233%3D%2520%2520Epilepsy%2520is%2520a%2520prevalent%2520neurological%2520disorder%2520globally%252C%2520impacting%2520around%252050%250Amillion%2520people%2520%255Ccite%257BWHO_epilepsy_50million%257D.%2520Epileptic%2520seizures%2520result%2520from%250Asudden%2520abnormal%2520electrical%2520activity%2520in%2520the%2520brain%252C%2520which%2520can%2520be%2520read%2520as%2520sudden%250Aand%2520significant%2520changes%2520in%2520the%2520EEG%2520signal%2520of%2520the%2520brain.%2520The%2520signal%2520can%2520vary%2520in%250Aseverity%2520and%2520frequency%252C%2520which%2520results%2520in%2520loss%2520of%2520consciousness%2520and%2520muscle%250Acontractions%2520for%2520a%2520short%2520period%2520of%2520time%2520%255Ccite%257Bepilepsyfoundation_myoclonic%257D.%250AIndividuals%2520with%2520epilepsy%2520often%2520face%2520significant%2520employment%2520challenges%2520due%2520to%250Asafety%2520concerns%2520in%2520certain%2520work%2520environments.%2520Many%2520jobs%2520that%2520involve%2520working%2520at%250Aheights%252C%2520operating%2520heavy%2520machinery%252C%2520or%2520in%2520other%2520potentially%2520hazardous%2520settings%250Amay%2520be%2520restricted%2520for%2520people%2520with%2520seizure%2520disorders.%2520This%2520certainly%2520limits%2520job%250Aoptions%2520and%2520economic%2520opportunities%2520for%2520those%2520living%2520with%2520epilepsy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10342v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Deep%20Learning%20Model%20for%20epileptic%20seizure%20classification%20by%20using%0A%20%201D-CNN%20with%20multi-head%20attention%20mechanism&entry.906535625=Mohammed%20Guhdar%20and%20Ramadhan%20J.%20Mstafa%20and%20Abdulhakeem%20O.%20Mohammed&entry.1292438233=%20%20Epilepsy%20is%20a%20prevalent%20neurological%20disorder%20globally%2C%20impacting%20around%2050%0Amillion%20people%20%5Ccite%7BWHO_epilepsy_50million%7D.%20Epileptic%20seizures%20result%20from%0Asudden%20abnormal%20electrical%20activity%20in%20the%20brain%2C%20which%20can%20be%20read%20as%20sudden%0Aand%20significant%20changes%20in%20the%20EEG%20signal%20of%20the%20brain.%20The%20signal%20can%20vary%20in%0Aseverity%20and%20frequency%2C%20which%20results%20in%20loss%20of%20consciousness%20and%20muscle%0Acontractions%20for%20a%20short%20period%20of%20time%20%5Ccite%7Bepilepsyfoundation_myoclonic%7D.%0AIndividuals%20with%20epilepsy%20often%20face%20significant%20employment%20challenges%20due%20to%0Asafety%20concerns%20in%20certain%20work%20environments.%20Many%20jobs%20that%20involve%20working%20at%0Aheights%2C%20operating%20heavy%20machinery%2C%20or%20in%20other%20potentially%20hazardous%20settings%0Amay%20be%20restricted%20for%20people%20with%20seizure%20disorders.%20This%20certainly%20limits%20job%0Aoptions%20and%20economic%20opportunities%20for%20those%20living%20with%20epilepsy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10342v1&entry.124074799=Read"},
{"title": "Challenges and recommendations for Electronic Health Records data\n  extraction and preparation for dynamic prediction modelling in hospitalized\n  patients -- a practical guide", "author": "Elena Albu and Shan Gao and Pieter Stijnen and Frank E. Rademakers and Bas C T van Bussel and Taya Collyer and Tina Hernandez-Boussard and Laure Wynants and Ben Van Calster", "abstract": "  Dynamic predictive modeling using electronic health record (EHR) data has\ngained significant attention in recent years. The reliability and\ntrustworthiness of such models depend heavily on the quality of the underlying\ndata, which is largely determined by the stages preceding the model\ndevelopment: data extraction from EHR systems and data preparation. We list\nover forty challenges encountered during these stages and provide actionable\nrecommendations for addressing them. These challenges are organized into four\ncategories: cohort definition, outcome definition, feature engineering, and\ndata cleaning. This list is designed to serve as a practical guide for data\nextraction engineers and researchers, supporting better practices and improving\nthe quality and real-world applicability of dynamic prediction models in\nclinical settings.\n", "link": "http://arxiv.org/abs/2501.10240v1", "date": "2025-01-17", "relevancy": 1.6988, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4566}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4145}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.3969}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Challenges%20and%20recommendations%20for%20Electronic%20Health%20Records%20data%0A%20%20extraction%20and%20preparation%20for%20dynamic%20prediction%20modelling%20in%20hospitalized%0A%20%20patients%20--%20a%20practical%20guide&body=Title%3A%20Challenges%20and%20recommendations%20for%20Electronic%20Health%20Records%20data%0A%20%20extraction%20and%20preparation%20for%20dynamic%20prediction%20modelling%20in%20hospitalized%0A%20%20patients%20--%20a%20practical%20guide%0AAuthor%3A%20Elena%20Albu%20and%20Shan%20Gao%20and%20Pieter%20Stijnen%20and%20Frank%20E.%20Rademakers%20and%20Bas%20C%20T%20van%20Bussel%20and%20Taya%20Collyer%20and%20Tina%20Hernandez-Boussard%20and%20Laure%20Wynants%20and%20Ben%20Van%20Calster%0AAbstract%3A%20%20%20Dynamic%20predictive%20modeling%20using%20electronic%20health%20record%20%28EHR%29%20data%20has%0Agained%20significant%20attention%20in%20recent%20years.%20The%20reliability%20and%0Atrustworthiness%20of%20such%20models%20depend%20heavily%20on%20the%20quality%20of%20the%20underlying%0Adata%2C%20which%20is%20largely%20determined%20by%20the%20stages%20preceding%20the%20model%0Adevelopment%3A%20data%20extraction%20from%20EHR%20systems%20and%20data%20preparation.%20We%20list%0Aover%20forty%20challenges%20encountered%20during%20these%20stages%20and%20provide%20actionable%0Arecommendations%20for%20addressing%20them.%20These%20challenges%20are%20organized%20into%20four%0Acategories%3A%20cohort%20definition%2C%20outcome%20definition%2C%20feature%20engineering%2C%20and%0Adata%20cleaning.%20This%20list%20is%20designed%20to%20serve%20as%20a%20practical%20guide%20for%20data%0Aextraction%20engineers%20and%20researchers%2C%20supporting%20better%20practices%20and%20improving%0Athe%20quality%20and%20real-world%20applicability%20of%20dynamic%20prediction%20models%20in%0Aclinical%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10240v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChallenges%2520and%2520recommendations%2520for%2520Electronic%2520Health%2520Records%2520data%250A%2520%2520extraction%2520and%2520preparation%2520for%2520dynamic%2520prediction%2520modelling%2520in%2520hospitalized%250A%2520%2520patients%2520--%2520a%2520practical%2520guide%26entry.906535625%3DElena%2520Albu%2520and%2520Shan%2520Gao%2520and%2520Pieter%2520Stijnen%2520and%2520Frank%2520E.%2520Rademakers%2520and%2520Bas%2520C%2520T%2520van%2520Bussel%2520and%2520Taya%2520Collyer%2520and%2520Tina%2520Hernandez-Boussard%2520and%2520Laure%2520Wynants%2520and%2520Ben%2520Van%2520Calster%26entry.1292438233%3D%2520%2520Dynamic%2520predictive%2520modeling%2520using%2520electronic%2520health%2520record%2520%2528EHR%2529%2520data%2520has%250Agained%2520significant%2520attention%2520in%2520recent%2520years.%2520The%2520reliability%2520and%250Atrustworthiness%2520of%2520such%2520models%2520depend%2520heavily%2520on%2520the%2520quality%2520of%2520the%2520underlying%250Adata%252C%2520which%2520is%2520largely%2520determined%2520by%2520the%2520stages%2520preceding%2520the%2520model%250Adevelopment%253A%2520data%2520extraction%2520from%2520EHR%2520systems%2520and%2520data%2520preparation.%2520We%2520list%250Aover%2520forty%2520challenges%2520encountered%2520during%2520these%2520stages%2520and%2520provide%2520actionable%250Arecommendations%2520for%2520addressing%2520them.%2520These%2520challenges%2520are%2520organized%2520into%2520four%250Acategories%253A%2520cohort%2520definition%252C%2520outcome%2520definition%252C%2520feature%2520engineering%252C%2520and%250Adata%2520cleaning.%2520This%2520list%2520is%2520designed%2520to%2520serve%2520as%2520a%2520practical%2520guide%2520for%2520data%250Aextraction%2520engineers%2520and%2520researchers%252C%2520supporting%2520better%2520practices%2520and%2520improving%250Athe%2520quality%2520and%2520real-world%2520applicability%2520of%2520dynamic%2520prediction%2520models%2520in%250Aclinical%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10240v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Challenges%20and%20recommendations%20for%20Electronic%20Health%20Records%20data%0A%20%20extraction%20and%20preparation%20for%20dynamic%20prediction%20modelling%20in%20hospitalized%0A%20%20patients%20--%20a%20practical%20guide&entry.906535625=Elena%20Albu%20and%20Shan%20Gao%20and%20Pieter%20Stijnen%20and%20Frank%20E.%20Rademakers%20and%20Bas%20C%20T%20van%20Bussel%20and%20Taya%20Collyer%20and%20Tina%20Hernandez-Boussard%20and%20Laure%20Wynants%20and%20Ben%20Van%20Calster&entry.1292438233=%20%20Dynamic%20predictive%20modeling%20using%20electronic%20health%20record%20%28EHR%29%20data%20has%0Agained%20significant%20attention%20in%20recent%20years.%20The%20reliability%20and%0Atrustworthiness%20of%20such%20models%20depend%20heavily%20on%20the%20quality%20of%20the%20underlying%0Adata%2C%20which%20is%20largely%20determined%20by%20the%20stages%20preceding%20the%20model%0Adevelopment%3A%20data%20extraction%20from%20EHR%20systems%20and%20data%20preparation.%20We%20list%0Aover%20forty%20challenges%20encountered%20during%20these%20stages%20and%20provide%20actionable%0Arecommendations%20for%20addressing%20them.%20These%20challenges%20are%20organized%20into%20four%0Acategories%3A%20cohort%20definition%2C%20outcome%20definition%2C%20feature%20engineering%2C%20and%0Adata%20cleaning.%20This%20list%20is%20designed%20to%20serve%20as%20a%20practical%20guide%20for%20data%0Aextraction%20engineers%20and%20researchers%2C%20supporting%20better%20practices%20and%20improving%0Athe%20quality%20and%20real-world%20applicability%20of%20dynamic%20prediction%20models%20in%0Aclinical%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10240v1&entry.124074799=Read"},
{"title": "Random-Key Algorithms for Optimizing Integrated Operating Room\n  Scheduling", "author": "Bruno Salezze Vieira and Eduardo Machado Silva and Antonio Augusto Chaves", "abstract": "  Efficient surgery room scheduling is essential for hospital efficiency,\npatient satisfaction, and resource utilization. This study addresses this\nchallenge by introducing a novel concept of Random-Key Optimizer (RKO),\nrigorously tested on literature and new, real-world inspired instances. Our\ncombinatorial optimization problem incorporates multi-room scheduling,\nequipment scheduling, and complex availability constraints for rooms, patients,\nand surgeons, facilitating rescheduling and enhancing operational flexibility.\nThe RKO approach represents solutions as points in a continuous space, which\nare then mapped in the problem solution space via a deterministic function\nknown as a decoder. The core idea is to operate metaheuristics and heuristics\nin the random-key space, unaware of the original solution space. We design the\nBiased Random-Key Genetic Algorithm with $Q$-Learning, Simulated Annealing, and\nIterated Local Search for use within an RKO framework, employing a single\ndecoder function. The proposed metaheuristics are complemented by lower-bound\nformulations, providing optimal gaps for evaluating the effectiveness of the\nheuristic results. Our results demonstrate significant lower and upper bounds\nimprovements for the literature instances, notably proving one optimal result.\nFurthermore, the best-proposed metaheuristic efficiently generates schedules\nfor the newly introduced instances, even in highly constrained scenarios. This\nresearch offers valuable insights and practical solutions for improving surgery\nscheduling processes, offering tangible benefits to hospitals by optimising\nresource allocation, reducing patient wait times, and enhancing overall\noperational efficiency.\n", "link": "http://arxiv.org/abs/2501.10243v1", "date": "2025-01-17", "relevancy": 1.6914, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4673}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4179}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.41}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Random-Key%20Algorithms%20for%20Optimizing%20Integrated%20Operating%20Room%0A%20%20Scheduling&body=Title%3A%20Random-Key%20Algorithms%20for%20Optimizing%20Integrated%20Operating%20Room%0A%20%20Scheduling%0AAuthor%3A%20Bruno%20Salezze%20Vieira%20and%20Eduardo%20Machado%20Silva%20and%20Antonio%20Augusto%20Chaves%0AAbstract%3A%20%20%20Efficient%20surgery%20room%20scheduling%20is%20essential%20for%20hospital%20efficiency%2C%0Apatient%20satisfaction%2C%20and%20resource%20utilization.%20This%20study%20addresses%20this%0Achallenge%20by%20introducing%20a%20novel%20concept%20of%20Random-Key%20Optimizer%20%28RKO%29%2C%0Arigorously%20tested%20on%20literature%20and%20new%2C%20real-world%20inspired%20instances.%20Our%0Acombinatorial%20optimization%20problem%20incorporates%20multi-room%20scheduling%2C%0Aequipment%20scheduling%2C%20and%20complex%20availability%20constraints%20for%20rooms%2C%20patients%2C%0Aand%20surgeons%2C%20facilitating%20rescheduling%20and%20enhancing%20operational%20flexibility.%0AThe%20RKO%20approach%20represents%20solutions%20as%20points%20in%20a%20continuous%20space%2C%20which%0Aare%20then%20mapped%20in%20the%20problem%20solution%20space%20via%20a%20deterministic%20function%0Aknown%20as%20a%20decoder.%20The%20core%20idea%20is%20to%20operate%20metaheuristics%20and%20heuristics%0Ain%20the%20random-key%20space%2C%20unaware%20of%20the%20original%20solution%20space.%20We%20design%20the%0ABiased%20Random-Key%20Genetic%20Algorithm%20with%20%24Q%24-Learning%2C%20Simulated%20Annealing%2C%20and%0AIterated%20Local%20Search%20for%20use%20within%20an%20RKO%20framework%2C%20employing%20a%20single%0Adecoder%20function.%20The%20proposed%20metaheuristics%20are%20complemented%20by%20lower-bound%0Aformulations%2C%20providing%20optimal%20gaps%20for%20evaluating%20the%20effectiveness%20of%20the%0Aheuristic%20results.%20Our%20results%20demonstrate%20significant%20lower%20and%20upper%20bounds%0Aimprovements%20for%20the%20literature%20instances%2C%20notably%20proving%20one%20optimal%20result.%0AFurthermore%2C%20the%20best-proposed%20metaheuristic%20efficiently%20generates%20schedules%0Afor%20the%20newly%20introduced%20instances%2C%20even%20in%20highly%20constrained%20scenarios.%20This%0Aresearch%20offers%20valuable%20insights%20and%20practical%20solutions%20for%20improving%20surgery%0Ascheduling%20processes%2C%20offering%20tangible%20benefits%20to%20hospitals%20by%20optimising%0Aresource%20allocation%2C%20reducing%20patient%20wait%20times%2C%20and%20enhancing%20overall%0Aoperational%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10243v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRandom-Key%2520Algorithms%2520for%2520Optimizing%2520Integrated%2520Operating%2520Room%250A%2520%2520Scheduling%26entry.906535625%3DBruno%2520Salezze%2520Vieira%2520and%2520Eduardo%2520Machado%2520Silva%2520and%2520Antonio%2520Augusto%2520Chaves%26entry.1292438233%3D%2520%2520Efficient%2520surgery%2520room%2520scheduling%2520is%2520essential%2520for%2520hospital%2520efficiency%252C%250Apatient%2520satisfaction%252C%2520and%2520resource%2520utilization.%2520This%2520study%2520addresses%2520this%250Achallenge%2520by%2520introducing%2520a%2520novel%2520concept%2520of%2520Random-Key%2520Optimizer%2520%2528RKO%2529%252C%250Arigorously%2520tested%2520on%2520literature%2520and%2520new%252C%2520real-world%2520inspired%2520instances.%2520Our%250Acombinatorial%2520optimization%2520problem%2520incorporates%2520multi-room%2520scheduling%252C%250Aequipment%2520scheduling%252C%2520and%2520complex%2520availability%2520constraints%2520for%2520rooms%252C%2520patients%252C%250Aand%2520surgeons%252C%2520facilitating%2520rescheduling%2520and%2520enhancing%2520operational%2520flexibility.%250AThe%2520RKO%2520approach%2520represents%2520solutions%2520as%2520points%2520in%2520a%2520continuous%2520space%252C%2520which%250Aare%2520then%2520mapped%2520in%2520the%2520problem%2520solution%2520space%2520via%2520a%2520deterministic%2520function%250Aknown%2520as%2520a%2520decoder.%2520The%2520core%2520idea%2520is%2520to%2520operate%2520metaheuristics%2520and%2520heuristics%250Ain%2520the%2520random-key%2520space%252C%2520unaware%2520of%2520the%2520original%2520solution%2520space.%2520We%2520design%2520the%250ABiased%2520Random-Key%2520Genetic%2520Algorithm%2520with%2520%2524Q%2524-Learning%252C%2520Simulated%2520Annealing%252C%2520and%250AIterated%2520Local%2520Search%2520for%2520use%2520within%2520an%2520RKO%2520framework%252C%2520employing%2520a%2520single%250Adecoder%2520function.%2520The%2520proposed%2520metaheuristics%2520are%2520complemented%2520by%2520lower-bound%250Aformulations%252C%2520providing%2520optimal%2520gaps%2520for%2520evaluating%2520the%2520effectiveness%2520of%2520the%250Aheuristic%2520results.%2520Our%2520results%2520demonstrate%2520significant%2520lower%2520and%2520upper%2520bounds%250Aimprovements%2520for%2520the%2520literature%2520instances%252C%2520notably%2520proving%2520one%2520optimal%2520result.%250AFurthermore%252C%2520the%2520best-proposed%2520metaheuristic%2520efficiently%2520generates%2520schedules%250Afor%2520the%2520newly%2520introduced%2520instances%252C%2520even%2520in%2520highly%2520constrained%2520scenarios.%2520This%250Aresearch%2520offers%2520valuable%2520insights%2520and%2520practical%2520solutions%2520for%2520improving%2520surgery%250Ascheduling%2520processes%252C%2520offering%2520tangible%2520benefits%2520to%2520hospitals%2520by%2520optimising%250Aresource%2520allocation%252C%2520reducing%2520patient%2520wait%2520times%252C%2520and%2520enhancing%2520overall%250Aoperational%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10243v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Random-Key%20Algorithms%20for%20Optimizing%20Integrated%20Operating%20Room%0A%20%20Scheduling&entry.906535625=Bruno%20Salezze%20Vieira%20and%20Eduardo%20Machado%20Silva%20and%20Antonio%20Augusto%20Chaves&entry.1292438233=%20%20Efficient%20surgery%20room%20scheduling%20is%20essential%20for%20hospital%20efficiency%2C%0Apatient%20satisfaction%2C%20and%20resource%20utilization.%20This%20study%20addresses%20this%0Achallenge%20by%20introducing%20a%20novel%20concept%20of%20Random-Key%20Optimizer%20%28RKO%29%2C%0Arigorously%20tested%20on%20literature%20and%20new%2C%20real-world%20inspired%20instances.%20Our%0Acombinatorial%20optimization%20problem%20incorporates%20multi-room%20scheduling%2C%0Aequipment%20scheduling%2C%20and%20complex%20availability%20constraints%20for%20rooms%2C%20patients%2C%0Aand%20surgeons%2C%20facilitating%20rescheduling%20and%20enhancing%20operational%20flexibility.%0AThe%20RKO%20approach%20represents%20solutions%20as%20points%20in%20a%20continuous%20space%2C%20which%0Aare%20then%20mapped%20in%20the%20problem%20solution%20space%20via%20a%20deterministic%20function%0Aknown%20as%20a%20decoder.%20The%20core%20idea%20is%20to%20operate%20metaheuristics%20and%20heuristics%0Ain%20the%20random-key%20space%2C%20unaware%20of%20the%20original%20solution%20space.%20We%20design%20the%0ABiased%20Random-Key%20Genetic%20Algorithm%20with%20%24Q%24-Learning%2C%20Simulated%20Annealing%2C%20and%0AIterated%20Local%20Search%20for%20use%20within%20an%20RKO%20framework%2C%20employing%20a%20single%0Adecoder%20function.%20The%20proposed%20metaheuristics%20are%20complemented%20by%20lower-bound%0Aformulations%2C%20providing%20optimal%20gaps%20for%20evaluating%20the%20effectiveness%20of%20the%0Aheuristic%20results.%20Our%20results%20demonstrate%20significant%20lower%20and%20upper%20bounds%0Aimprovements%20for%20the%20literature%20instances%2C%20notably%20proving%20one%20optimal%20result.%0AFurthermore%2C%20the%20best-proposed%20metaheuristic%20efficiently%20generates%20schedules%0Afor%20the%20newly%20introduced%20instances%2C%20even%20in%20highly%20constrained%20scenarios.%20This%0Aresearch%20offers%20valuable%20insights%20and%20practical%20solutions%20for%20improving%20surgery%0Ascheduling%20processes%2C%20offering%20tangible%20benefits%20to%20hospitals%20by%20optimising%0Aresource%20allocation%2C%20reducing%20patient%20wait%20times%2C%20and%20enhancing%20overall%0Aoperational%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10243v1&entry.124074799=Read"},
{"title": "Improved learning rates in multi-unit uniform price auctions", "author": "Marius Potfer and Dorian Baudry and Hugo Richard and Vianney Perchet and Cheng Wan", "abstract": "  Motivated by the strategic participation of electricity producers in\nelectricity day-ahead market, we study the problem of online learning in\nrepeated multi-unit uniform price auctions focusing on the adversarial opposing\nbid setting. The main contribution of this paper is the introduction of a new\nmodeling of the bid space. Indeed, we prove that a learning algorithm\nleveraging the structure of this problem achieves a regret of\n$\\tilde{O}(K^{4/3}T^{2/3})$ under bandit feedback, improving over the bound of\n$\\tilde{O}(K^{7/4}T^{3/4})$ previously obtained in the literature. This\nimproved regret rate is tight up to logarithmic terms. Inspired by electricity\nreserve markets, we further introduce a different feedback model under which\nall winning bids are revealed. This feedback interpolates between the\nfull-information and bandit scenarios depending on the auctions' results. We\nprove that, under this feedback, the algorithm that we propose achieves regret\n$\\tilde{O}(K^{5/2}\\sqrt{T})$.\n", "link": "http://arxiv.org/abs/2501.10181v1", "date": "2025-01-17", "relevancy": 1.6852, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4412}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4269}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4078}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improved%20learning%20rates%20in%20multi-unit%20uniform%20price%20auctions&body=Title%3A%20Improved%20learning%20rates%20in%20multi-unit%20uniform%20price%20auctions%0AAuthor%3A%20Marius%20Potfer%20and%20Dorian%20Baudry%20and%20Hugo%20Richard%20and%20Vianney%20Perchet%20and%20Cheng%20Wan%0AAbstract%3A%20%20%20Motivated%20by%20the%20strategic%20participation%20of%20electricity%20producers%20in%0Aelectricity%20day-ahead%20market%2C%20we%20study%20the%20problem%20of%20online%20learning%20in%0Arepeated%20multi-unit%20uniform%20price%20auctions%20focusing%20on%20the%20adversarial%20opposing%0Abid%20setting.%20The%20main%20contribution%20of%20this%20paper%20is%20the%20introduction%20of%20a%20new%0Amodeling%20of%20the%20bid%20space.%20Indeed%2C%20we%20prove%20that%20a%20learning%20algorithm%0Aleveraging%20the%20structure%20of%20this%20problem%20achieves%20a%20regret%20of%0A%24%5Ctilde%7BO%7D%28K%5E%7B4/3%7DT%5E%7B2/3%7D%29%24%20under%20bandit%20feedback%2C%20improving%20over%20the%20bound%20of%0A%24%5Ctilde%7BO%7D%28K%5E%7B7/4%7DT%5E%7B3/4%7D%29%24%20previously%20obtained%20in%20the%20literature.%20This%0Aimproved%20regret%20rate%20is%20tight%20up%20to%20logarithmic%20terms.%20Inspired%20by%20electricity%0Areserve%20markets%2C%20we%20further%20introduce%20a%20different%20feedback%20model%20under%20which%0Aall%20winning%20bids%20are%20revealed.%20This%20feedback%20interpolates%20between%20the%0Afull-information%20and%20bandit%20scenarios%20depending%20on%20the%20auctions%27%20results.%20We%0Aprove%20that%2C%20under%20this%20feedback%2C%20the%20algorithm%20that%20we%20propose%20achieves%20regret%0A%24%5Ctilde%7BO%7D%28K%5E%7B5/2%7D%5Csqrt%7BT%7D%29%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10181v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproved%2520learning%2520rates%2520in%2520multi-unit%2520uniform%2520price%2520auctions%26entry.906535625%3DMarius%2520Potfer%2520and%2520Dorian%2520Baudry%2520and%2520Hugo%2520Richard%2520and%2520Vianney%2520Perchet%2520and%2520Cheng%2520Wan%26entry.1292438233%3D%2520%2520Motivated%2520by%2520the%2520strategic%2520participation%2520of%2520electricity%2520producers%2520in%250Aelectricity%2520day-ahead%2520market%252C%2520we%2520study%2520the%2520problem%2520of%2520online%2520learning%2520in%250Arepeated%2520multi-unit%2520uniform%2520price%2520auctions%2520focusing%2520on%2520the%2520adversarial%2520opposing%250Abid%2520setting.%2520The%2520main%2520contribution%2520of%2520this%2520paper%2520is%2520the%2520introduction%2520of%2520a%2520new%250Amodeling%2520of%2520the%2520bid%2520space.%2520Indeed%252C%2520we%2520prove%2520that%2520a%2520learning%2520algorithm%250Aleveraging%2520the%2520structure%2520of%2520this%2520problem%2520achieves%2520a%2520regret%2520of%250A%2524%255Ctilde%257BO%257D%2528K%255E%257B4/3%257DT%255E%257B2/3%257D%2529%2524%2520under%2520bandit%2520feedback%252C%2520improving%2520over%2520the%2520bound%2520of%250A%2524%255Ctilde%257BO%257D%2528K%255E%257B7/4%257DT%255E%257B3/4%257D%2529%2524%2520previously%2520obtained%2520in%2520the%2520literature.%2520This%250Aimproved%2520regret%2520rate%2520is%2520tight%2520up%2520to%2520logarithmic%2520terms.%2520Inspired%2520by%2520electricity%250Areserve%2520markets%252C%2520we%2520further%2520introduce%2520a%2520different%2520feedback%2520model%2520under%2520which%250Aall%2520winning%2520bids%2520are%2520revealed.%2520This%2520feedback%2520interpolates%2520between%2520the%250Afull-information%2520and%2520bandit%2520scenarios%2520depending%2520on%2520the%2520auctions%2527%2520results.%2520We%250Aprove%2520that%252C%2520under%2520this%2520feedback%252C%2520the%2520algorithm%2520that%2520we%2520propose%2520achieves%2520regret%250A%2524%255Ctilde%257BO%257D%2528K%255E%257B5/2%257D%255Csqrt%257BT%257D%2529%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10181v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20learning%20rates%20in%20multi-unit%20uniform%20price%20auctions&entry.906535625=Marius%20Potfer%20and%20Dorian%20Baudry%20and%20Hugo%20Richard%20and%20Vianney%20Perchet%20and%20Cheng%20Wan&entry.1292438233=%20%20Motivated%20by%20the%20strategic%20participation%20of%20electricity%20producers%20in%0Aelectricity%20day-ahead%20market%2C%20we%20study%20the%20problem%20of%20online%20learning%20in%0Arepeated%20multi-unit%20uniform%20price%20auctions%20focusing%20on%20the%20adversarial%20opposing%0Abid%20setting.%20The%20main%20contribution%20of%20this%20paper%20is%20the%20introduction%20of%20a%20new%0Amodeling%20of%20the%20bid%20space.%20Indeed%2C%20we%20prove%20that%20a%20learning%20algorithm%0Aleveraging%20the%20structure%20of%20this%20problem%20achieves%20a%20regret%20of%0A%24%5Ctilde%7BO%7D%28K%5E%7B4/3%7DT%5E%7B2/3%7D%29%24%20under%20bandit%20feedback%2C%20improving%20over%20the%20bound%20of%0A%24%5Ctilde%7BO%7D%28K%5E%7B7/4%7DT%5E%7B3/4%7D%29%24%20previously%20obtained%20in%20the%20literature.%20This%0Aimproved%20regret%20rate%20is%20tight%20up%20to%20logarithmic%20terms.%20Inspired%20by%20electricity%0Areserve%20markets%2C%20we%20further%20introduce%20a%20different%20feedback%20model%20under%20which%0Aall%20winning%20bids%20are%20revealed.%20This%20feedback%20interpolates%20between%20the%0Afull-information%20and%20bandit%20scenarios%20depending%20on%20the%20auctions%27%20results.%20We%0Aprove%20that%2C%20under%20this%20feedback%2C%20the%20algorithm%20that%20we%20propose%20achieves%20regret%0A%24%5Ctilde%7BO%7D%28K%5E%7B5/2%7D%5Csqrt%7BT%7D%29%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10181v1&entry.124074799=Read"},
{"title": "Mamba2D: A Natively Multi-Dimensional State-Space Model for Vision Tasks", "author": "Enis Baty and Alejandro Hern\u00e1ndez D\u00edaz and Chris Bridges and Rebecca Davidson and Steve Eckersley and Simon Hadfield", "abstract": "  State-Space Models (SSMs) have recently emerged as a powerful and efficient\nalternative to the long-standing transformer architecture. However, existing\nSSM conceptualizations retain deeply rooted biases from their roots in natural\nlanguage processing. This constrains their ability to appropriately model the\nspatially-dependent characteristics of visual inputs. In this paper, we address\nthese limitations by re-deriving modern selective state-space techniques,\nstarting from a natively multidimensional formulation. Currently, prior works\nattempt to apply natively 1D SSMs to 2D data (i.e. images) by relying on\narbitrary combinations of 1D scan directions to capture spatial dependencies.\nIn contrast, Mamba2D improves upon this with a single 2D scan direction that\nfactors in both dimensions of the input natively, effectively modelling spatial\ndependencies when constructing hidden states. Mamba2D shows comparable\nperformance to prior adaptations of SSMs for vision tasks, on standard image\nclassification evaluations with the ImageNet-1K dataset. Source code is\navailable at https://github.com/cocoalex00/Mamba2D.\n", "link": "http://arxiv.org/abs/2412.16146v2", "date": "2025-01-17", "relevancy": 1.6746, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5645}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5568}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mamba2D%3A%20A%20Natively%20Multi-Dimensional%20State-Space%20Model%20for%20Vision%20Tasks&body=Title%3A%20Mamba2D%3A%20A%20Natively%20Multi-Dimensional%20State-Space%20Model%20for%20Vision%20Tasks%0AAuthor%3A%20Enis%20Baty%20and%20Alejandro%20Hern%C3%A1ndez%20D%C3%ADaz%20and%20Chris%20Bridges%20and%20Rebecca%20Davidson%20and%20Steve%20Eckersley%20and%20Simon%20Hadfield%0AAbstract%3A%20%20%20State-Space%20Models%20%28SSMs%29%20have%20recently%20emerged%20as%20a%20powerful%20and%20efficient%0Aalternative%20to%20the%20long-standing%20transformer%20architecture.%20However%2C%20existing%0ASSM%20conceptualizations%20retain%20deeply%20rooted%20biases%20from%20their%20roots%20in%20natural%0Alanguage%20processing.%20This%20constrains%20their%20ability%20to%20appropriately%20model%20the%0Aspatially-dependent%20characteristics%20of%20visual%20inputs.%20In%20this%20paper%2C%20we%20address%0Athese%20limitations%20by%20re-deriving%20modern%20selective%20state-space%20techniques%2C%0Astarting%20from%20a%20natively%20multidimensional%20formulation.%20Currently%2C%20prior%20works%0Aattempt%20to%20apply%20natively%201D%20SSMs%20to%202D%20data%20%28i.e.%20images%29%20by%20relying%20on%0Aarbitrary%20combinations%20of%201D%20scan%20directions%20to%20capture%20spatial%20dependencies.%0AIn%20contrast%2C%20Mamba2D%20improves%20upon%20this%20with%20a%20single%202D%20scan%20direction%20that%0Afactors%20in%20both%20dimensions%20of%20the%20input%20natively%2C%20effectively%20modelling%20spatial%0Adependencies%20when%20constructing%20hidden%20states.%20Mamba2D%20shows%20comparable%0Aperformance%20to%20prior%20adaptations%20of%20SSMs%20for%20vision%20tasks%2C%20on%20standard%20image%0Aclassification%20evaluations%20with%20the%20ImageNet-1K%20dataset.%20Source%20code%20is%0Aavailable%20at%20https%3A//github.com/cocoalex00/Mamba2D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16146v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMamba2D%253A%2520A%2520Natively%2520Multi-Dimensional%2520State-Space%2520Model%2520for%2520Vision%2520Tasks%26entry.906535625%3DEnis%2520Baty%2520and%2520Alejandro%2520Hern%25C3%25A1ndez%2520D%25C3%25ADaz%2520and%2520Chris%2520Bridges%2520and%2520Rebecca%2520Davidson%2520and%2520Steve%2520Eckersley%2520and%2520Simon%2520Hadfield%26entry.1292438233%3D%2520%2520State-Space%2520Models%2520%2528SSMs%2529%2520have%2520recently%2520emerged%2520as%2520a%2520powerful%2520and%2520efficient%250Aalternative%2520to%2520the%2520long-standing%2520transformer%2520architecture.%2520However%252C%2520existing%250ASSM%2520conceptualizations%2520retain%2520deeply%2520rooted%2520biases%2520from%2520their%2520roots%2520in%2520natural%250Alanguage%2520processing.%2520This%2520constrains%2520their%2520ability%2520to%2520appropriately%2520model%2520the%250Aspatially-dependent%2520characteristics%2520of%2520visual%2520inputs.%2520In%2520this%2520paper%252C%2520we%2520address%250Athese%2520limitations%2520by%2520re-deriving%2520modern%2520selective%2520state-space%2520techniques%252C%250Astarting%2520from%2520a%2520natively%2520multidimensional%2520formulation.%2520Currently%252C%2520prior%2520works%250Aattempt%2520to%2520apply%2520natively%25201D%2520SSMs%2520to%25202D%2520data%2520%2528i.e.%2520images%2529%2520by%2520relying%2520on%250Aarbitrary%2520combinations%2520of%25201D%2520scan%2520directions%2520to%2520capture%2520spatial%2520dependencies.%250AIn%2520contrast%252C%2520Mamba2D%2520improves%2520upon%2520this%2520with%2520a%2520single%25202D%2520scan%2520direction%2520that%250Afactors%2520in%2520both%2520dimensions%2520of%2520the%2520input%2520natively%252C%2520effectively%2520modelling%2520spatial%250Adependencies%2520when%2520constructing%2520hidden%2520states.%2520Mamba2D%2520shows%2520comparable%250Aperformance%2520to%2520prior%2520adaptations%2520of%2520SSMs%2520for%2520vision%2520tasks%252C%2520on%2520standard%2520image%250Aclassification%2520evaluations%2520with%2520the%2520ImageNet-1K%2520dataset.%2520Source%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/cocoalex00/Mamba2D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16146v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mamba2D%3A%20A%20Natively%20Multi-Dimensional%20State-Space%20Model%20for%20Vision%20Tasks&entry.906535625=Enis%20Baty%20and%20Alejandro%20Hern%C3%A1ndez%20D%C3%ADaz%20and%20Chris%20Bridges%20and%20Rebecca%20Davidson%20and%20Steve%20Eckersley%20and%20Simon%20Hadfield&entry.1292438233=%20%20State-Space%20Models%20%28SSMs%29%20have%20recently%20emerged%20as%20a%20powerful%20and%20efficient%0Aalternative%20to%20the%20long-standing%20transformer%20architecture.%20However%2C%20existing%0ASSM%20conceptualizations%20retain%20deeply%20rooted%20biases%20from%20their%20roots%20in%20natural%0Alanguage%20processing.%20This%20constrains%20their%20ability%20to%20appropriately%20model%20the%0Aspatially-dependent%20characteristics%20of%20visual%20inputs.%20In%20this%20paper%2C%20we%20address%0Athese%20limitations%20by%20re-deriving%20modern%20selective%20state-space%20techniques%2C%0Astarting%20from%20a%20natively%20multidimensional%20formulation.%20Currently%2C%20prior%20works%0Aattempt%20to%20apply%20natively%201D%20SSMs%20to%202D%20data%20%28i.e.%20images%29%20by%20relying%20on%0Aarbitrary%20combinations%20of%201D%20scan%20directions%20to%20capture%20spatial%20dependencies.%0AIn%20contrast%2C%20Mamba2D%20improves%20upon%20this%20with%20a%20single%202D%20scan%20direction%20that%0Afactors%20in%20both%20dimensions%20of%20the%20input%20natively%2C%20effectively%20modelling%20spatial%0Adependencies%20when%20constructing%20hidden%20states.%20Mamba2D%20shows%20comparable%0Aperformance%20to%20prior%20adaptations%20of%20SSMs%20for%20vision%20tasks%2C%20on%20standard%20image%0Aclassification%20evaluations%20with%20the%20ImageNet-1K%20dataset.%20Source%20code%20is%0Aavailable%20at%20https%3A//github.com/cocoalex00/Mamba2D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16146v2&entry.124074799=Read"},
{"title": "CSSDM Ontology to Enable Continuity of Care Data Interoperability", "author": "Subhashis Das and Debashis Naskar and Sara Rodriguez Gonzalez and Pamela Hussey", "abstract": "  The rapid advancement of digital technologies and recent global pandemic\nscenarios have led to a growing focus on how these technologies can enhance\nhealthcare service delivery and workflow to address crises. Action plans that\nconsolidate existing digital transformation programs are being reviewed to\nestablish core infrastructure and foundations for sustainable healthcare\nsolutions. Reforming health and social care to personalize home care, for\nexample, can help avoid treatment in overcrowded acute hospital settings and\nimprove the experiences and outcomes for both healthcare professionals and\nservice users. In this information-intensive domain, addressing the\ninteroperability challenge through standards-based roadmaps is crucial for\nenabling effective connections between health and social care services. This\napproach facilitates safe and trustworthy data workflows between different\nhealthcare system providers. In this paper, we present a methodology for\nextracting, transforming, and loading data through a semi-automated process\nusing a Common Semantic Standardized Data Model (CSSDM) to create personalized\nhealthcare knowledge graph (KG). The CSSDM is grounded in the formal ontology\nof ISO 13940 ContSys and incorporates FHIR-based specifications to support\nstructural attributes for generating KGs. We propose that the CSSDM facilitates\ndata harmonization and linking, offering an alternative approach to\ninteroperability. This approach promotes a novel form of collaboration between\ncompanies developing health information systems and cloud-enabled health\nservices. Consequently, it provides multiple stakeholders with access to\nhigh-quality data and information sharing.\n", "link": "http://arxiv.org/abs/2501.10160v1", "date": "2025-01-17", "relevancy": 1.6659, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4238}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4126}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4107}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CSSDM%20Ontology%20to%20Enable%20Continuity%20of%20Care%20Data%20Interoperability&body=Title%3A%20CSSDM%20Ontology%20to%20Enable%20Continuity%20of%20Care%20Data%20Interoperability%0AAuthor%3A%20Subhashis%20Das%20and%20Debashis%20Naskar%20and%20Sara%20Rodriguez%20Gonzalez%20and%20Pamela%20Hussey%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20digital%20technologies%20and%20recent%20global%20pandemic%0Ascenarios%20have%20led%20to%20a%20growing%20focus%20on%20how%20these%20technologies%20can%20enhance%0Ahealthcare%20service%20delivery%20and%20workflow%20to%20address%20crises.%20Action%20plans%20that%0Aconsolidate%20existing%20digital%20transformation%20programs%20are%20being%20reviewed%20to%0Aestablish%20core%20infrastructure%20and%20foundations%20for%20sustainable%20healthcare%0Asolutions.%20Reforming%20health%20and%20social%20care%20to%20personalize%20home%20care%2C%20for%0Aexample%2C%20can%20help%20avoid%20treatment%20in%20overcrowded%20acute%20hospital%20settings%20and%0Aimprove%20the%20experiences%20and%20outcomes%20for%20both%20healthcare%20professionals%20and%0Aservice%20users.%20In%20this%20information-intensive%20domain%2C%20addressing%20the%0Ainteroperability%20challenge%20through%20standards-based%20roadmaps%20is%20crucial%20for%0Aenabling%20effective%20connections%20between%20health%20and%20social%20care%20services.%20This%0Aapproach%20facilitates%20safe%20and%20trustworthy%20data%20workflows%20between%20different%0Ahealthcare%20system%20providers.%20In%20this%20paper%2C%20we%20present%20a%20methodology%20for%0Aextracting%2C%20transforming%2C%20and%20loading%20data%20through%20a%20semi-automated%20process%0Ausing%20a%20Common%20Semantic%20Standardized%20Data%20Model%20%28CSSDM%29%20to%20create%20personalized%0Ahealthcare%20knowledge%20graph%20%28KG%29.%20The%20CSSDM%20is%20grounded%20in%20the%20formal%20ontology%0Aof%20ISO%2013940%20ContSys%20and%20incorporates%20FHIR-based%20specifications%20to%20support%0Astructural%20attributes%20for%20generating%20KGs.%20We%20propose%20that%20the%20CSSDM%20facilitates%0Adata%20harmonization%20and%20linking%2C%20offering%20an%20alternative%20approach%20to%0Ainteroperability.%20This%20approach%20promotes%20a%20novel%20form%20of%20collaboration%20between%0Acompanies%20developing%20health%20information%20systems%20and%20cloud-enabled%20health%0Aservices.%20Consequently%2C%20it%20provides%20multiple%20stakeholders%20with%20access%20to%0Ahigh-quality%20data%20and%20information%20sharing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10160v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCSSDM%2520Ontology%2520to%2520Enable%2520Continuity%2520of%2520Care%2520Data%2520Interoperability%26entry.906535625%3DSubhashis%2520Das%2520and%2520Debashis%2520Naskar%2520and%2520Sara%2520Rodriguez%2520Gonzalez%2520and%2520Pamela%2520Hussey%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520digital%2520technologies%2520and%2520recent%2520global%2520pandemic%250Ascenarios%2520have%2520led%2520to%2520a%2520growing%2520focus%2520on%2520how%2520these%2520technologies%2520can%2520enhance%250Ahealthcare%2520service%2520delivery%2520and%2520workflow%2520to%2520address%2520crises.%2520Action%2520plans%2520that%250Aconsolidate%2520existing%2520digital%2520transformation%2520programs%2520are%2520being%2520reviewed%2520to%250Aestablish%2520core%2520infrastructure%2520and%2520foundations%2520for%2520sustainable%2520healthcare%250Asolutions.%2520Reforming%2520health%2520and%2520social%2520care%2520to%2520personalize%2520home%2520care%252C%2520for%250Aexample%252C%2520can%2520help%2520avoid%2520treatment%2520in%2520overcrowded%2520acute%2520hospital%2520settings%2520and%250Aimprove%2520the%2520experiences%2520and%2520outcomes%2520for%2520both%2520healthcare%2520professionals%2520and%250Aservice%2520users.%2520In%2520this%2520information-intensive%2520domain%252C%2520addressing%2520the%250Ainteroperability%2520challenge%2520through%2520standards-based%2520roadmaps%2520is%2520crucial%2520for%250Aenabling%2520effective%2520connections%2520between%2520health%2520and%2520social%2520care%2520services.%2520This%250Aapproach%2520facilitates%2520safe%2520and%2520trustworthy%2520data%2520workflows%2520between%2520different%250Ahealthcare%2520system%2520providers.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520methodology%2520for%250Aextracting%252C%2520transforming%252C%2520and%2520loading%2520data%2520through%2520a%2520semi-automated%2520process%250Ausing%2520a%2520Common%2520Semantic%2520Standardized%2520Data%2520Model%2520%2528CSSDM%2529%2520to%2520create%2520personalized%250Ahealthcare%2520knowledge%2520graph%2520%2528KG%2529.%2520The%2520CSSDM%2520is%2520grounded%2520in%2520the%2520formal%2520ontology%250Aof%2520ISO%252013940%2520ContSys%2520and%2520incorporates%2520FHIR-based%2520specifications%2520to%2520support%250Astructural%2520attributes%2520for%2520generating%2520KGs.%2520We%2520propose%2520that%2520the%2520CSSDM%2520facilitates%250Adata%2520harmonization%2520and%2520linking%252C%2520offering%2520an%2520alternative%2520approach%2520to%250Ainteroperability.%2520This%2520approach%2520promotes%2520a%2520novel%2520form%2520of%2520collaboration%2520between%250Acompanies%2520developing%2520health%2520information%2520systems%2520and%2520cloud-enabled%2520health%250Aservices.%2520Consequently%252C%2520it%2520provides%2520multiple%2520stakeholders%2520with%2520access%2520to%250Ahigh-quality%2520data%2520and%2520information%2520sharing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10160v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CSSDM%20Ontology%20to%20Enable%20Continuity%20of%20Care%20Data%20Interoperability&entry.906535625=Subhashis%20Das%20and%20Debashis%20Naskar%20and%20Sara%20Rodriguez%20Gonzalez%20and%20Pamela%20Hussey&entry.1292438233=%20%20The%20rapid%20advancement%20of%20digital%20technologies%20and%20recent%20global%20pandemic%0Ascenarios%20have%20led%20to%20a%20growing%20focus%20on%20how%20these%20technologies%20can%20enhance%0Ahealthcare%20service%20delivery%20and%20workflow%20to%20address%20crises.%20Action%20plans%20that%0Aconsolidate%20existing%20digital%20transformation%20programs%20are%20being%20reviewed%20to%0Aestablish%20core%20infrastructure%20and%20foundations%20for%20sustainable%20healthcare%0Asolutions.%20Reforming%20health%20and%20social%20care%20to%20personalize%20home%20care%2C%20for%0Aexample%2C%20can%20help%20avoid%20treatment%20in%20overcrowded%20acute%20hospital%20settings%20and%0Aimprove%20the%20experiences%20and%20outcomes%20for%20both%20healthcare%20professionals%20and%0Aservice%20users.%20In%20this%20information-intensive%20domain%2C%20addressing%20the%0Ainteroperability%20challenge%20through%20standards-based%20roadmaps%20is%20crucial%20for%0Aenabling%20effective%20connections%20between%20health%20and%20social%20care%20services.%20This%0Aapproach%20facilitates%20safe%20and%20trustworthy%20data%20workflows%20between%20different%0Ahealthcare%20system%20providers.%20In%20this%20paper%2C%20we%20present%20a%20methodology%20for%0Aextracting%2C%20transforming%2C%20and%20loading%20data%20through%20a%20semi-automated%20process%0Ausing%20a%20Common%20Semantic%20Standardized%20Data%20Model%20%28CSSDM%29%20to%20create%20personalized%0Ahealthcare%20knowledge%20graph%20%28KG%29.%20The%20CSSDM%20is%20grounded%20in%20the%20formal%20ontology%0Aof%20ISO%2013940%20ContSys%20and%20incorporates%20FHIR-based%20specifications%20to%20support%0Astructural%20attributes%20for%20generating%20KGs.%20We%20propose%20that%20the%20CSSDM%20facilitates%0Adata%20harmonization%20and%20linking%2C%20offering%20an%20alternative%20approach%20to%0Ainteroperability.%20This%20approach%20promotes%20a%20novel%20form%20of%20collaboration%20between%0Acompanies%20developing%20health%20information%20systems%20and%20cloud-enabled%20health%0Aservices.%20Consequently%2C%20it%20provides%20multiple%20stakeholders%20with%20access%20to%0Ahigh-quality%20data%20and%20information%20sharing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10160v1&entry.124074799=Read"},
{"title": "Disharmony: Forensics using Reverse Lighting Harmonization", "author": "Philip Wootaek Shin and Jack Sampson and Vijaykrishnan Narayanan and Andres Marquez and Mahantesh Halappanavar", "abstract": "  Content generation and manipulation approaches based on deep learning methods\nhave seen significant advancements, leading to an increased need for techniques\nto detect whether an image has been generated or edited. Another area of\nresearch focuses on the insertion and harmonization of objects within images.\nIn this study, we explore the potential of using harmonization data in\nconjunction with a segmentation model to enhance the detection of edited image\nregions. These edits can be either manually crafted or generated using deep\nlearning methods. Our findings demonstrate that this approach can effectively\nidentify such edits. Existing forensic models often overlook the detection of\nharmonized objects in relation to the background, but our proposed Disharmony\nNetwork addresses this gap. By utilizing an aggregated dataset of harmonization\ntechniques, our model outperforms existing forensic networks in identifying\nharmonized objects integrated into their backgrounds, and shows potential for\ndetecting various forms of edits, including virtual try-on tasks.\n", "link": "http://arxiv.org/abs/2501.10212v1", "date": "2025-01-17", "relevancy": 1.6622, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5723}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5643}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disharmony%3A%20Forensics%20using%20Reverse%20Lighting%20Harmonization&body=Title%3A%20Disharmony%3A%20Forensics%20using%20Reverse%20Lighting%20Harmonization%0AAuthor%3A%20Philip%20Wootaek%20Shin%20and%20Jack%20Sampson%20and%20Vijaykrishnan%20Narayanan%20and%20Andres%20Marquez%20and%20Mahantesh%20Halappanavar%0AAbstract%3A%20%20%20Content%20generation%20and%20manipulation%20approaches%20based%20on%20deep%20learning%20methods%0Ahave%20seen%20significant%20advancements%2C%20leading%20to%20an%20increased%20need%20for%20techniques%0Ato%20detect%20whether%20an%20image%20has%20been%20generated%20or%20edited.%20Another%20area%20of%0Aresearch%20focuses%20on%20the%20insertion%20and%20harmonization%20of%20objects%20within%20images.%0AIn%20this%20study%2C%20we%20explore%20the%20potential%20of%20using%20harmonization%20data%20in%0Aconjunction%20with%20a%20segmentation%20model%20to%20enhance%20the%20detection%20of%20edited%20image%0Aregions.%20These%20edits%20can%20be%20either%20manually%20crafted%20or%20generated%20using%20deep%0Alearning%20methods.%20Our%20findings%20demonstrate%20that%20this%20approach%20can%20effectively%0Aidentify%20such%20edits.%20Existing%20forensic%20models%20often%20overlook%20the%20detection%20of%0Aharmonized%20objects%20in%20relation%20to%20the%20background%2C%20but%20our%20proposed%20Disharmony%0ANetwork%20addresses%20this%20gap.%20By%20utilizing%20an%20aggregated%20dataset%20of%20harmonization%0Atechniques%2C%20our%20model%20outperforms%20existing%20forensic%20networks%20in%20identifying%0Aharmonized%20objects%20integrated%20into%20their%20backgrounds%2C%20and%20shows%20potential%20for%0Adetecting%20various%20forms%20of%20edits%2C%20including%20virtual%20try-on%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10212v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisharmony%253A%2520Forensics%2520using%2520Reverse%2520Lighting%2520Harmonization%26entry.906535625%3DPhilip%2520Wootaek%2520Shin%2520and%2520Jack%2520Sampson%2520and%2520Vijaykrishnan%2520Narayanan%2520and%2520Andres%2520Marquez%2520and%2520Mahantesh%2520Halappanavar%26entry.1292438233%3D%2520%2520Content%2520generation%2520and%2520manipulation%2520approaches%2520based%2520on%2520deep%2520learning%2520methods%250Ahave%2520seen%2520significant%2520advancements%252C%2520leading%2520to%2520an%2520increased%2520need%2520for%2520techniques%250Ato%2520detect%2520whether%2520an%2520image%2520has%2520been%2520generated%2520or%2520edited.%2520Another%2520area%2520of%250Aresearch%2520focuses%2520on%2520the%2520insertion%2520and%2520harmonization%2520of%2520objects%2520within%2520images.%250AIn%2520this%2520study%252C%2520we%2520explore%2520the%2520potential%2520of%2520using%2520harmonization%2520data%2520in%250Aconjunction%2520with%2520a%2520segmentation%2520model%2520to%2520enhance%2520the%2520detection%2520of%2520edited%2520image%250Aregions.%2520These%2520edits%2520can%2520be%2520either%2520manually%2520crafted%2520or%2520generated%2520using%2520deep%250Alearning%2520methods.%2520Our%2520findings%2520demonstrate%2520that%2520this%2520approach%2520can%2520effectively%250Aidentify%2520such%2520edits.%2520Existing%2520forensic%2520models%2520often%2520overlook%2520the%2520detection%2520of%250Aharmonized%2520objects%2520in%2520relation%2520to%2520the%2520background%252C%2520but%2520our%2520proposed%2520Disharmony%250ANetwork%2520addresses%2520this%2520gap.%2520By%2520utilizing%2520an%2520aggregated%2520dataset%2520of%2520harmonization%250Atechniques%252C%2520our%2520model%2520outperforms%2520existing%2520forensic%2520networks%2520in%2520identifying%250Aharmonized%2520objects%2520integrated%2520into%2520their%2520backgrounds%252C%2520and%2520shows%2520potential%2520for%250Adetecting%2520various%2520forms%2520of%2520edits%252C%2520including%2520virtual%2520try-on%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10212v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disharmony%3A%20Forensics%20using%20Reverse%20Lighting%20Harmonization&entry.906535625=Philip%20Wootaek%20Shin%20and%20Jack%20Sampson%20and%20Vijaykrishnan%20Narayanan%20and%20Andres%20Marquez%20and%20Mahantesh%20Halappanavar&entry.1292438233=%20%20Content%20generation%20and%20manipulation%20approaches%20based%20on%20deep%20learning%20methods%0Ahave%20seen%20significant%20advancements%2C%20leading%20to%20an%20increased%20need%20for%20techniques%0Ato%20detect%20whether%20an%20image%20has%20been%20generated%20or%20edited.%20Another%20area%20of%0Aresearch%20focuses%20on%20the%20insertion%20and%20harmonization%20of%20objects%20within%20images.%0AIn%20this%20study%2C%20we%20explore%20the%20potential%20of%20using%20harmonization%20data%20in%0Aconjunction%20with%20a%20segmentation%20model%20to%20enhance%20the%20detection%20of%20edited%20image%0Aregions.%20These%20edits%20can%20be%20either%20manually%20crafted%20or%20generated%20using%20deep%0Alearning%20methods.%20Our%20findings%20demonstrate%20that%20this%20approach%20can%20effectively%0Aidentify%20such%20edits.%20Existing%20forensic%20models%20often%20overlook%20the%20detection%20of%0Aharmonized%20objects%20in%20relation%20to%20the%20background%2C%20but%20our%20proposed%20Disharmony%0ANetwork%20addresses%20this%20gap.%20By%20utilizing%20an%20aggregated%20dataset%20of%20harmonization%0Atechniques%2C%20our%20model%20outperforms%20existing%20forensic%20networks%20in%20identifying%0Aharmonized%20objects%20integrated%20into%20their%20backgrounds%2C%20and%20shows%20potential%20for%0Adetecting%20various%20forms%20of%20edits%2C%20including%20virtual%20try-on%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10212v1&entry.124074799=Read"},
{"title": "Enabling Low-Resource Language Retrieval: Establishing Baselines for\n  Urdu MS MARCO", "author": "Umer Butt and Stalin Veranasi and G\u00fcnter Neumann", "abstract": "  As the Information Retrieval (IR) field increasingly recognizes the\nimportance of inclusivity, addressing the needs of low-resource languages\nremains a significant challenge. This paper introduces the first large-scale\nUrdu IR dataset, created by translating the MS MARCO dataset through machine\ntranslation. We establish baseline results through zero-shot learning for IR in\nUrdu and subsequently apply the mMARCO multilingual IR methodology to this\nnewly translated dataset. Our findings demonstrate that the fine-tuned model\n(Urdu-mT5-mMARCO) achieves a Mean Reciprocal Rank (MRR@10) of 0.247 and a\nRecall@10 of 0.439, representing significant improvements over zero-shot\nresults and showing the potential for expanding IR access for Urdu speakers. By\nbridging access gaps for speakers of low-resource languages, this work not only\nadvances multilingual IR research but also emphasizes the ethical and societal\nimportance of inclusive IR technologies. This work provides valuable insights\ninto the challenges and solutions for improving language representation and\nlays the groundwork for future research, especially in South Asian languages,\nwhich can benefit from the adaptable methods used in this study.\n", "link": "http://arxiv.org/abs/2412.12997v2", "date": "2025-01-17", "relevancy": 1.6596, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4153}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4148}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4148}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enabling%20Low-Resource%20Language%20Retrieval%3A%20Establishing%20Baselines%20for%0A%20%20Urdu%20MS%20MARCO&body=Title%3A%20Enabling%20Low-Resource%20Language%20Retrieval%3A%20Establishing%20Baselines%20for%0A%20%20Urdu%20MS%20MARCO%0AAuthor%3A%20Umer%20Butt%20and%20Stalin%20Veranasi%20and%20G%C3%BCnter%20Neumann%0AAbstract%3A%20%20%20As%20the%20Information%20Retrieval%20%28IR%29%20field%20increasingly%20recognizes%20the%0Aimportance%20of%20inclusivity%2C%20addressing%20the%20needs%20of%20low-resource%20languages%0Aremains%20a%20significant%20challenge.%20This%20paper%20introduces%20the%20first%20large-scale%0AUrdu%20IR%20dataset%2C%20created%20by%20translating%20the%20MS%20MARCO%20dataset%20through%20machine%0Atranslation.%20We%20establish%20baseline%20results%20through%20zero-shot%20learning%20for%20IR%20in%0AUrdu%20and%20subsequently%20apply%20the%20mMARCO%20multilingual%20IR%20methodology%20to%20this%0Anewly%20translated%20dataset.%20Our%20findings%20demonstrate%20that%20the%20fine-tuned%20model%0A%28Urdu-mT5-mMARCO%29%20achieves%20a%20Mean%20Reciprocal%20Rank%20%28MRR%4010%29%20of%200.247%20and%20a%0ARecall%4010%20of%200.439%2C%20representing%20significant%20improvements%20over%20zero-shot%0Aresults%20and%20showing%20the%20potential%20for%20expanding%20IR%20access%20for%20Urdu%20speakers.%20By%0Abridging%20access%20gaps%20for%20speakers%20of%20low-resource%20languages%2C%20this%20work%20not%20only%0Aadvances%20multilingual%20IR%20research%20but%20also%20emphasizes%20the%20ethical%20and%20societal%0Aimportance%20of%20inclusive%20IR%20technologies.%20This%20work%20provides%20valuable%20insights%0Ainto%20the%20challenges%20and%20solutions%20for%20improving%20language%20representation%20and%0Alays%20the%20groundwork%20for%20future%20research%2C%20especially%20in%20South%20Asian%20languages%2C%0Awhich%20can%20benefit%20from%20the%20adaptable%20methods%20used%20in%20this%20study.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12997v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnabling%2520Low-Resource%2520Language%2520Retrieval%253A%2520Establishing%2520Baselines%2520for%250A%2520%2520Urdu%2520MS%2520MARCO%26entry.906535625%3DUmer%2520Butt%2520and%2520Stalin%2520Veranasi%2520and%2520G%25C3%25BCnter%2520Neumann%26entry.1292438233%3D%2520%2520As%2520the%2520Information%2520Retrieval%2520%2528IR%2529%2520field%2520increasingly%2520recognizes%2520the%250Aimportance%2520of%2520inclusivity%252C%2520addressing%2520the%2520needs%2520of%2520low-resource%2520languages%250Aremains%2520a%2520significant%2520challenge.%2520This%2520paper%2520introduces%2520the%2520first%2520large-scale%250AUrdu%2520IR%2520dataset%252C%2520created%2520by%2520translating%2520the%2520MS%2520MARCO%2520dataset%2520through%2520machine%250Atranslation.%2520We%2520establish%2520baseline%2520results%2520through%2520zero-shot%2520learning%2520for%2520IR%2520in%250AUrdu%2520and%2520subsequently%2520apply%2520the%2520mMARCO%2520multilingual%2520IR%2520methodology%2520to%2520this%250Anewly%2520translated%2520dataset.%2520Our%2520findings%2520demonstrate%2520that%2520the%2520fine-tuned%2520model%250A%2528Urdu-mT5-mMARCO%2529%2520achieves%2520a%2520Mean%2520Reciprocal%2520Rank%2520%2528MRR%254010%2529%2520of%25200.247%2520and%2520a%250ARecall%254010%2520of%25200.439%252C%2520representing%2520significant%2520improvements%2520over%2520zero-shot%250Aresults%2520and%2520showing%2520the%2520potential%2520for%2520expanding%2520IR%2520access%2520for%2520Urdu%2520speakers.%2520By%250Abridging%2520access%2520gaps%2520for%2520speakers%2520of%2520low-resource%2520languages%252C%2520this%2520work%2520not%2520only%250Aadvances%2520multilingual%2520IR%2520research%2520but%2520also%2520emphasizes%2520the%2520ethical%2520and%2520societal%250Aimportance%2520of%2520inclusive%2520IR%2520technologies.%2520This%2520work%2520provides%2520valuable%2520insights%250Ainto%2520the%2520challenges%2520and%2520solutions%2520for%2520improving%2520language%2520representation%2520and%250Alays%2520the%2520groundwork%2520for%2520future%2520research%252C%2520especially%2520in%2520South%2520Asian%2520languages%252C%250Awhich%2520can%2520benefit%2520from%2520the%2520adaptable%2520methods%2520used%2520in%2520this%2520study.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12997v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enabling%20Low-Resource%20Language%20Retrieval%3A%20Establishing%20Baselines%20for%0A%20%20Urdu%20MS%20MARCO&entry.906535625=Umer%20Butt%20and%20Stalin%20Veranasi%20and%20G%C3%BCnter%20Neumann&entry.1292438233=%20%20As%20the%20Information%20Retrieval%20%28IR%29%20field%20increasingly%20recognizes%20the%0Aimportance%20of%20inclusivity%2C%20addressing%20the%20needs%20of%20low-resource%20languages%0Aremains%20a%20significant%20challenge.%20This%20paper%20introduces%20the%20first%20large-scale%0AUrdu%20IR%20dataset%2C%20created%20by%20translating%20the%20MS%20MARCO%20dataset%20through%20machine%0Atranslation.%20We%20establish%20baseline%20results%20through%20zero-shot%20learning%20for%20IR%20in%0AUrdu%20and%20subsequently%20apply%20the%20mMARCO%20multilingual%20IR%20methodology%20to%20this%0Anewly%20translated%20dataset.%20Our%20findings%20demonstrate%20that%20the%20fine-tuned%20model%0A%28Urdu-mT5-mMARCO%29%20achieves%20a%20Mean%20Reciprocal%20Rank%20%28MRR%4010%29%20of%200.247%20and%20a%0ARecall%4010%20of%200.439%2C%20representing%20significant%20improvements%20over%20zero-shot%0Aresults%20and%20showing%20the%20potential%20for%20expanding%20IR%20access%20for%20Urdu%20speakers.%20By%0Abridging%20access%20gaps%20for%20speakers%20of%20low-resource%20languages%2C%20this%20work%20not%20only%0Aadvances%20multilingual%20IR%20research%20but%20also%20emphasizes%20the%20ethical%20and%20societal%0Aimportance%20of%20inclusive%20IR%20technologies.%20This%20work%20provides%20valuable%20insights%0Ainto%20the%20challenges%20and%20solutions%20for%20improving%20language%20representation%20and%0Alays%20the%20groundwork%20for%20future%20research%2C%20especially%20in%20South%20Asian%20languages%2C%0Awhich%20can%20benefit%20from%20the%20adaptable%20methods%20used%20in%20this%20study.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12997v2&entry.124074799=Read"},
{"title": "DPCL-Diff: The Temporal Knowledge Graph Reasoning Based on Graph Node\n  Diffusion Model with Dual-Domain Periodic Contrastive Learning", "author": "Yukun Cao and Lisheng Wang and Luobin Huang", "abstract": "  Temporal knowledge graph (TKG) reasoning that infers future missing facts is\nan essential and challenging task. Predicting future events typically relies on\nclosely related historical facts, yielding more accurate results for repetitive\nor periodic events. However, for future events with sparse historical\ninteractions, the effectiveness of this method, which focuses on leveraging\nhigh-frequency historical information, diminishes. Recently, the capabilities\nof diffusion models in image generation have opened new opportunities for TKG\nreasoning. Therefore, we propose a graph node diffusion model with dual-domain\nperiodic contrastive learning (DPCL-Diff). Graph node diffusion model (GNDiff)\nintroduces noise into sparsely related events to simulate new events,\ngenerating high-quality data that better conforms to the actual distribution.\nThis generative mechanism significantly enhances the model's ability to reason\nabout new events. Additionally, the dual-domain periodic contrastive learning\n(DPCL) maps periodic and non-periodic event entities to Poincar\\'e and\nEuclidean spaces, leveraging their characteristics to distinguish similar\nperiodic events effectively. Experimental results on four public datasets\ndemonstrate that DPCL-Diff significantly outperforms state-of-the-art TKG\nmodels in event prediction, demonstrating our approach's effectiveness. This\nstudy also investigates the combined effectiveness of GNDiff and DPCL in TKG\ntasks.\n", "link": "http://arxiv.org/abs/2411.01477v2", "date": "2025-01-17", "relevancy": 1.6586, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5792}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5514}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DPCL-Diff%3A%20The%20Temporal%20Knowledge%20Graph%20Reasoning%20Based%20on%20Graph%20Node%0A%20%20Diffusion%20Model%20with%20Dual-Domain%20Periodic%20Contrastive%20Learning&body=Title%3A%20DPCL-Diff%3A%20The%20Temporal%20Knowledge%20Graph%20Reasoning%20Based%20on%20Graph%20Node%0A%20%20Diffusion%20Model%20with%20Dual-Domain%20Periodic%20Contrastive%20Learning%0AAuthor%3A%20Yukun%20Cao%20and%20Lisheng%20Wang%20and%20Luobin%20Huang%0AAbstract%3A%20%20%20Temporal%20knowledge%20graph%20%28TKG%29%20reasoning%20that%20infers%20future%20missing%20facts%20is%0Aan%20essential%20and%20challenging%20task.%20Predicting%20future%20events%20typically%20relies%20on%0Aclosely%20related%20historical%20facts%2C%20yielding%20more%20accurate%20results%20for%20repetitive%0Aor%20periodic%20events.%20However%2C%20for%20future%20events%20with%20sparse%20historical%0Ainteractions%2C%20the%20effectiveness%20of%20this%20method%2C%20which%20focuses%20on%20leveraging%0Ahigh-frequency%20historical%20information%2C%20diminishes.%20Recently%2C%20the%20capabilities%0Aof%20diffusion%20models%20in%20image%20generation%20have%20opened%20new%20opportunities%20for%20TKG%0Areasoning.%20Therefore%2C%20we%20propose%20a%20graph%20node%20diffusion%20model%20with%20dual-domain%0Aperiodic%20contrastive%20learning%20%28DPCL-Diff%29.%20Graph%20node%20diffusion%20model%20%28GNDiff%29%0Aintroduces%20noise%20into%20sparsely%20related%20events%20to%20simulate%20new%20events%2C%0Agenerating%20high-quality%20data%20that%20better%20conforms%20to%20the%20actual%20distribution.%0AThis%20generative%20mechanism%20significantly%20enhances%20the%20model%27s%20ability%20to%20reason%0Aabout%20new%20events.%20Additionally%2C%20the%20dual-domain%20periodic%20contrastive%20learning%0A%28DPCL%29%20maps%20periodic%20and%20non-periodic%20event%20entities%20to%20Poincar%5C%27e%20and%0AEuclidean%20spaces%2C%20leveraging%20their%20characteristics%20to%20distinguish%20similar%0Aperiodic%20events%20effectively.%20Experimental%20results%20on%20four%20public%20datasets%0Ademonstrate%20that%20DPCL-Diff%20significantly%20outperforms%20state-of-the-art%20TKG%0Amodels%20in%20event%20prediction%2C%20demonstrating%20our%20approach%27s%20effectiveness.%20This%0Astudy%20also%20investigates%20the%20combined%20effectiveness%20of%20GNDiff%20and%20DPCL%20in%20TKG%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.01477v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDPCL-Diff%253A%2520The%2520Temporal%2520Knowledge%2520Graph%2520Reasoning%2520Based%2520on%2520Graph%2520Node%250A%2520%2520Diffusion%2520Model%2520with%2520Dual-Domain%2520Periodic%2520Contrastive%2520Learning%26entry.906535625%3DYukun%2520Cao%2520and%2520Lisheng%2520Wang%2520and%2520Luobin%2520Huang%26entry.1292438233%3D%2520%2520Temporal%2520knowledge%2520graph%2520%2528TKG%2529%2520reasoning%2520that%2520infers%2520future%2520missing%2520facts%2520is%250Aan%2520essential%2520and%2520challenging%2520task.%2520Predicting%2520future%2520events%2520typically%2520relies%2520on%250Aclosely%2520related%2520historical%2520facts%252C%2520yielding%2520more%2520accurate%2520results%2520for%2520repetitive%250Aor%2520periodic%2520events.%2520However%252C%2520for%2520future%2520events%2520with%2520sparse%2520historical%250Ainteractions%252C%2520the%2520effectiveness%2520of%2520this%2520method%252C%2520which%2520focuses%2520on%2520leveraging%250Ahigh-frequency%2520historical%2520information%252C%2520diminishes.%2520Recently%252C%2520the%2520capabilities%250Aof%2520diffusion%2520models%2520in%2520image%2520generation%2520have%2520opened%2520new%2520opportunities%2520for%2520TKG%250Areasoning.%2520Therefore%252C%2520we%2520propose%2520a%2520graph%2520node%2520diffusion%2520model%2520with%2520dual-domain%250Aperiodic%2520contrastive%2520learning%2520%2528DPCL-Diff%2529.%2520Graph%2520node%2520diffusion%2520model%2520%2528GNDiff%2529%250Aintroduces%2520noise%2520into%2520sparsely%2520related%2520events%2520to%2520simulate%2520new%2520events%252C%250Agenerating%2520high-quality%2520data%2520that%2520better%2520conforms%2520to%2520the%2520actual%2520distribution.%250AThis%2520generative%2520mechanism%2520significantly%2520enhances%2520the%2520model%2527s%2520ability%2520to%2520reason%250Aabout%2520new%2520events.%2520Additionally%252C%2520the%2520dual-domain%2520periodic%2520contrastive%2520learning%250A%2528DPCL%2529%2520maps%2520periodic%2520and%2520non-periodic%2520event%2520entities%2520to%2520Poincar%255C%2527e%2520and%250AEuclidean%2520spaces%252C%2520leveraging%2520their%2520characteristics%2520to%2520distinguish%2520similar%250Aperiodic%2520events%2520effectively.%2520Experimental%2520results%2520on%2520four%2520public%2520datasets%250Ademonstrate%2520that%2520DPCL-Diff%2520significantly%2520outperforms%2520state-of-the-art%2520TKG%250Amodels%2520in%2520event%2520prediction%252C%2520demonstrating%2520our%2520approach%2527s%2520effectiveness.%2520This%250Astudy%2520also%2520investigates%2520the%2520combined%2520effectiveness%2520of%2520GNDiff%2520and%2520DPCL%2520in%2520TKG%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.01477v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DPCL-Diff%3A%20The%20Temporal%20Knowledge%20Graph%20Reasoning%20Based%20on%20Graph%20Node%0A%20%20Diffusion%20Model%20with%20Dual-Domain%20Periodic%20Contrastive%20Learning&entry.906535625=Yukun%20Cao%20and%20Lisheng%20Wang%20and%20Luobin%20Huang&entry.1292438233=%20%20Temporal%20knowledge%20graph%20%28TKG%29%20reasoning%20that%20infers%20future%20missing%20facts%20is%0Aan%20essential%20and%20challenging%20task.%20Predicting%20future%20events%20typically%20relies%20on%0Aclosely%20related%20historical%20facts%2C%20yielding%20more%20accurate%20results%20for%20repetitive%0Aor%20periodic%20events.%20However%2C%20for%20future%20events%20with%20sparse%20historical%0Ainteractions%2C%20the%20effectiveness%20of%20this%20method%2C%20which%20focuses%20on%20leveraging%0Ahigh-frequency%20historical%20information%2C%20diminishes.%20Recently%2C%20the%20capabilities%0Aof%20diffusion%20models%20in%20image%20generation%20have%20opened%20new%20opportunities%20for%20TKG%0Areasoning.%20Therefore%2C%20we%20propose%20a%20graph%20node%20diffusion%20model%20with%20dual-domain%0Aperiodic%20contrastive%20learning%20%28DPCL-Diff%29.%20Graph%20node%20diffusion%20model%20%28GNDiff%29%0Aintroduces%20noise%20into%20sparsely%20related%20events%20to%20simulate%20new%20events%2C%0Agenerating%20high-quality%20data%20that%20better%20conforms%20to%20the%20actual%20distribution.%0AThis%20generative%20mechanism%20significantly%20enhances%20the%20model%27s%20ability%20to%20reason%0Aabout%20new%20events.%20Additionally%2C%20the%20dual-domain%20periodic%20contrastive%20learning%0A%28DPCL%29%20maps%20periodic%20and%20non-periodic%20event%20entities%20to%20Poincar%5C%27e%20and%0AEuclidean%20spaces%2C%20leveraging%20their%20characteristics%20to%20distinguish%20similar%0Aperiodic%20events%20effectively.%20Experimental%20results%20on%20four%20public%20datasets%0Ademonstrate%20that%20DPCL-Diff%20significantly%20outperforms%20state-of-the-art%20TKG%0Amodels%20in%20event%20prediction%2C%20demonstrating%20our%20approach%27s%20effectiveness.%20This%0Astudy%20also%20investigates%20the%20combined%20effectiveness%20of%20GNDiff%20and%20DPCL%20in%20TKG%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.01477v2&entry.124074799=Read"},
{"title": "CSHNet: A Novel Information Asymmetric Image Translation Method", "author": "Xi Yang and Haoyuan Shi and Zihan Wang and Nannan Wang and Xinbo Gao", "abstract": "  Despite advancements in cross-domain image translation, challenges persist in\nasymmetric tasks such as SAR-to-Optical and Sketch-to-Instance conversions,\nwhich involve transforming data from a less detailed domain into one with\nricher content. Traditional CNN-based methods are effective at capturing fine\ndetails but struggle with global structure, leading to unwanted merging of\nimage regions. To address this, we propose the CNN-Swin Hybrid Network\n(CSHNet), which combines two key modules: Swin Embedded CNN (SEC) and CNN\nEmbedded Swin (CES), forming the SEC-CES-Bottleneck (SCB). SEC leverages CNN's\ndetailed feature extraction while integrating the Swin Transformer's structural\nbias. CES, in turn, preserves the Swin Transformer's global integrity,\ncompensating for CNN's lack of focus on structure. Additionally, CSHNet\nincludes two components designed to enhance cross-domain information retention:\nthe Interactive Guided Connection (IGC), which enables dynamic information\nexchange between SEC and CES, and Adaptive Edge Perception Loss (AEPL), which\nmaintains structural boundaries during translation. Experimental results show\nthat CSHNet outperforms existing methods in both visual quality and performance\nmetrics across scene-level and instance-level datasets. Our code is available\nat: https://github.com/XduShi/CSHNet.\n", "link": "http://arxiv.org/abs/2501.10197v1", "date": "2025-01-17", "relevancy": 1.6504, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5542}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5466}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CSHNet%3A%20A%20Novel%20Information%20Asymmetric%20Image%20Translation%20Method&body=Title%3A%20CSHNet%3A%20A%20Novel%20Information%20Asymmetric%20Image%20Translation%20Method%0AAuthor%3A%20Xi%20Yang%20and%20Haoyuan%20Shi%20and%20Zihan%20Wang%20and%20Nannan%20Wang%20and%20Xinbo%20Gao%0AAbstract%3A%20%20%20Despite%20advancements%20in%20cross-domain%20image%20translation%2C%20challenges%20persist%20in%0Aasymmetric%20tasks%20such%20as%20SAR-to-Optical%20and%20Sketch-to-Instance%20conversions%2C%0Awhich%20involve%20transforming%20data%20from%20a%20less%20detailed%20domain%20into%20one%20with%0Aricher%20content.%20Traditional%20CNN-based%20methods%20are%20effective%20at%20capturing%20fine%0Adetails%20but%20struggle%20with%20global%20structure%2C%20leading%20to%20unwanted%20merging%20of%0Aimage%20regions.%20To%20address%20this%2C%20we%20propose%20the%20CNN-Swin%20Hybrid%20Network%0A%28CSHNet%29%2C%20which%20combines%20two%20key%20modules%3A%20Swin%20Embedded%20CNN%20%28SEC%29%20and%20CNN%0AEmbedded%20Swin%20%28CES%29%2C%20forming%20the%20SEC-CES-Bottleneck%20%28SCB%29.%20SEC%20leverages%20CNN%27s%0Adetailed%20feature%20extraction%20while%20integrating%20the%20Swin%20Transformer%27s%20structural%0Abias.%20CES%2C%20in%20turn%2C%20preserves%20the%20Swin%20Transformer%27s%20global%20integrity%2C%0Acompensating%20for%20CNN%27s%20lack%20of%20focus%20on%20structure.%20Additionally%2C%20CSHNet%0Aincludes%20two%20components%20designed%20to%20enhance%20cross-domain%20information%20retention%3A%0Athe%20Interactive%20Guided%20Connection%20%28IGC%29%2C%20which%20enables%20dynamic%20information%0Aexchange%20between%20SEC%20and%20CES%2C%20and%20Adaptive%20Edge%20Perception%20Loss%20%28AEPL%29%2C%20which%0Amaintains%20structural%20boundaries%20during%20translation.%20Experimental%20results%20show%0Athat%20CSHNet%20outperforms%20existing%20methods%20in%20both%20visual%20quality%20and%20performance%0Ametrics%20across%20scene-level%20and%20instance-level%20datasets.%20Our%20code%20is%20available%0Aat%3A%20https%3A//github.com/XduShi/CSHNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10197v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCSHNet%253A%2520A%2520Novel%2520Information%2520Asymmetric%2520Image%2520Translation%2520Method%26entry.906535625%3DXi%2520Yang%2520and%2520Haoyuan%2520Shi%2520and%2520Zihan%2520Wang%2520and%2520Nannan%2520Wang%2520and%2520Xinbo%2520Gao%26entry.1292438233%3D%2520%2520Despite%2520advancements%2520in%2520cross-domain%2520image%2520translation%252C%2520challenges%2520persist%2520in%250Aasymmetric%2520tasks%2520such%2520as%2520SAR-to-Optical%2520and%2520Sketch-to-Instance%2520conversions%252C%250Awhich%2520involve%2520transforming%2520data%2520from%2520a%2520less%2520detailed%2520domain%2520into%2520one%2520with%250Aricher%2520content.%2520Traditional%2520CNN-based%2520methods%2520are%2520effective%2520at%2520capturing%2520fine%250Adetails%2520but%2520struggle%2520with%2520global%2520structure%252C%2520leading%2520to%2520unwanted%2520merging%2520of%250Aimage%2520regions.%2520To%2520address%2520this%252C%2520we%2520propose%2520the%2520CNN-Swin%2520Hybrid%2520Network%250A%2528CSHNet%2529%252C%2520which%2520combines%2520two%2520key%2520modules%253A%2520Swin%2520Embedded%2520CNN%2520%2528SEC%2529%2520and%2520CNN%250AEmbedded%2520Swin%2520%2528CES%2529%252C%2520forming%2520the%2520SEC-CES-Bottleneck%2520%2528SCB%2529.%2520SEC%2520leverages%2520CNN%2527s%250Adetailed%2520feature%2520extraction%2520while%2520integrating%2520the%2520Swin%2520Transformer%2527s%2520structural%250Abias.%2520CES%252C%2520in%2520turn%252C%2520preserves%2520the%2520Swin%2520Transformer%2527s%2520global%2520integrity%252C%250Acompensating%2520for%2520CNN%2527s%2520lack%2520of%2520focus%2520on%2520structure.%2520Additionally%252C%2520CSHNet%250Aincludes%2520two%2520components%2520designed%2520to%2520enhance%2520cross-domain%2520information%2520retention%253A%250Athe%2520Interactive%2520Guided%2520Connection%2520%2528IGC%2529%252C%2520which%2520enables%2520dynamic%2520information%250Aexchange%2520between%2520SEC%2520and%2520CES%252C%2520and%2520Adaptive%2520Edge%2520Perception%2520Loss%2520%2528AEPL%2529%252C%2520which%250Amaintains%2520structural%2520boundaries%2520during%2520translation.%2520Experimental%2520results%2520show%250Athat%2520CSHNet%2520outperforms%2520existing%2520methods%2520in%2520both%2520visual%2520quality%2520and%2520performance%250Ametrics%2520across%2520scene-level%2520and%2520instance-level%2520datasets.%2520Our%2520code%2520is%2520available%250Aat%253A%2520https%253A//github.com/XduShi/CSHNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10197v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CSHNet%3A%20A%20Novel%20Information%20Asymmetric%20Image%20Translation%20Method&entry.906535625=Xi%20Yang%20and%20Haoyuan%20Shi%20and%20Zihan%20Wang%20and%20Nannan%20Wang%20and%20Xinbo%20Gao&entry.1292438233=%20%20Despite%20advancements%20in%20cross-domain%20image%20translation%2C%20challenges%20persist%20in%0Aasymmetric%20tasks%20such%20as%20SAR-to-Optical%20and%20Sketch-to-Instance%20conversions%2C%0Awhich%20involve%20transforming%20data%20from%20a%20less%20detailed%20domain%20into%20one%20with%0Aricher%20content.%20Traditional%20CNN-based%20methods%20are%20effective%20at%20capturing%20fine%0Adetails%20but%20struggle%20with%20global%20structure%2C%20leading%20to%20unwanted%20merging%20of%0Aimage%20regions.%20To%20address%20this%2C%20we%20propose%20the%20CNN-Swin%20Hybrid%20Network%0A%28CSHNet%29%2C%20which%20combines%20two%20key%20modules%3A%20Swin%20Embedded%20CNN%20%28SEC%29%20and%20CNN%0AEmbedded%20Swin%20%28CES%29%2C%20forming%20the%20SEC-CES-Bottleneck%20%28SCB%29.%20SEC%20leverages%20CNN%27s%0Adetailed%20feature%20extraction%20while%20integrating%20the%20Swin%20Transformer%27s%20structural%0Abias.%20CES%2C%20in%20turn%2C%20preserves%20the%20Swin%20Transformer%27s%20global%20integrity%2C%0Acompensating%20for%20CNN%27s%20lack%20of%20focus%20on%20structure.%20Additionally%2C%20CSHNet%0Aincludes%20two%20components%20designed%20to%20enhance%20cross-domain%20information%20retention%3A%0Athe%20Interactive%20Guided%20Connection%20%28IGC%29%2C%20which%20enables%20dynamic%20information%0Aexchange%20between%20SEC%20and%20CES%2C%20and%20Adaptive%20Edge%20Perception%20Loss%20%28AEPL%29%2C%20which%0Amaintains%20structural%20boundaries%20during%20translation.%20Experimental%20results%20show%0Athat%20CSHNet%20outperforms%20existing%20methods%20in%20both%20visual%20quality%20and%20performance%0Ametrics%20across%20scene-level%20and%20instance-level%20datasets.%20Our%20code%20is%20available%0Aat%3A%20https%3A//github.com/XduShi/CSHNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10197v1&entry.124074799=Read"},
{"title": "Tethered Variable Inertial Attitude Control Mechanisms through a Modular\n  Jumping Limbed Robot", "author": "Yusuke Tanaka and Alvin Zhu and Dennis Hong", "abstract": "  This paper presents the concept of a tethered variable inertial attitude\ncontrol mechanism for a modular jumping-limbed robot designed for planetary\nexploration in low-gravity environments. The system, named SPLITTER, comprises\ntwo sub-10 kg quadrupedal robots connected by a tether, capable of executing\nsuccessive jumping gaits and stabilizing in-flight using inertial morphing\ntechnology. Through model predictive control (MPC), attitude control was\ndemonstrated by adjusting the limbs and tether length to modulate the system's\nprincipal moments of inertia. Our results indicate that this control strategy\nallows the robot to stabilize during flight phases without needing traditional\nflywheel-based systems or relying on aerodynamics, making the approach\nmass-efficient and ideal for small-scale planetary robots' successive jumps.\nThe paper outlines the dynamics, MPC formulation for inertial morphing,\nactuator requirements, and simulation results, illustrating the potential of\nagile exploration for small-scale rovers in low-gravity environments like the\nMoon or asteroids.\n", "link": "http://arxiv.org/abs/2501.10156v1", "date": "2025-01-17", "relevancy": 1.5024, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5409}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4899}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4879}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tethered%20Variable%20Inertial%20Attitude%20Control%20Mechanisms%20through%20a%20Modular%0A%20%20Jumping%20Limbed%20Robot&body=Title%3A%20Tethered%20Variable%20Inertial%20Attitude%20Control%20Mechanisms%20through%20a%20Modular%0A%20%20Jumping%20Limbed%20Robot%0AAuthor%3A%20Yusuke%20Tanaka%20and%20Alvin%20Zhu%20and%20Dennis%20Hong%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20concept%20of%20a%20tethered%20variable%20inertial%20attitude%0Acontrol%20mechanism%20for%20a%20modular%20jumping-limbed%20robot%20designed%20for%20planetary%0Aexploration%20in%20low-gravity%20environments.%20The%20system%2C%20named%20SPLITTER%2C%20comprises%0Atwo%20sub-10%20kg%20quadrupedal%20robots%20connected%20by%20a%20tether%2C%20capable%20of%20executing%0Asuccessive%20jumping%20gaits%20and%20stabilizing%20in-flight%20using%20inertial%20morphing%0Atechnology.%20Through%20model%20predictive%20control%20%28MPC%29%2C%20attitude%20control%20was%0Ademonstrated%20by%20adjusting%20the%20limbs%20and%20tether%20length%20to%20modulate%20the%20system%27s%0Aprincipal%20moments%20of%20inertia.%20Our%20results%20indicate%20that%20this%20control%20strategy%0Aallows%20the%20robot%20to%20stabilize%20during%20flight%20phases%20without%20needing%20traditional%0Aflywheel-based%20systems%20or%20relying%20on%20aerodynamics%2C%20making%20the%20approach%0Amass-efficient%20and%20ideal%20for%20small-scale%20planetary%20robots%27%20successive%20jumps.%0AThe%20paper%20outlines%20the%20dynamics%2C%20MPC%20formulation%20for%20inertial%20morphing%2C%0Aactuator%20requirements%2C%20and%20simulation%20results%2C%20illustrating%20the%20potential%20of%0Aagile%20exploration%20for%20small-scale%20rovers%20in%20low-gravity%20environments%20like%20the%0AMoon%20or%20asteroids.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10156v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTethered%2520Variable%2520Inertial%2520Attitude%2520Control%2520Mechanisms%2520through%2520a%2520Modular%250A%2520%2520Jumping%2520Limbed%2520Robot%26entry.906535625%3DYusuke%2520Tanaka%2520and%2520Alvin%2520Zhu%2520and%2520Dennis%2520Hong%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520the%2520concept%2520of%2520a%2520tethered%2520variable%2520inertial%2520attitude%250Acontrol%2520mechanism%2520for%2520a%2520modular%2520jumping-limbed%2520robot%2520designed%2520for%2520planetary%250Aexploration%2520in%2520low-gravity%2520environments.%2520The%2520system%252C%2520named%2520SPLITTER%252C%2520comprises%250Atwo%2520sub-10%2520kg%2520quadrupedal%2520robots%2520connected%2520by%2520a%2520tether%252C%2520capable%2520of%2520executing%250Asuccessive%2520jumping%2520gaits%2520and%2520stabilizing%2520in-flight%2520using%2520inertial%2520morphing%250Atechnology.%2520Through%2520model%2520predictive%2520control%2520%2528MPC%2529%252C%2520attitude%2520control%2520was%250Ademonstrated%2520by%2520adjusting%2520the%2520limbs%2520and%2520tether%2520length%2520to%2520modulate%2520the%2520system%2527s%250Aprincipal%2520moments%2520of%2520inertia.%2520Our%2520results%2520indicate%2520that%2520this%2520control%2520strategy%250Aallows%2520the%2520robot%2520to%2520stabilize%2520during%2520flight%2520phases%2520without%2520needing%2520traditional%250Aflywheel-based%2520systems%2520or%2520relying%2520on%2520aerodynamics%252C%2520making%2520the%2520approach%250Amass-efficient%2520and%2520ideal%2520for%2520small-scale%2520planetary%2520robots%2527%2520successive%2520jumps.%250AThe%2520paper%2520outlines%2520the%2520dynamics%252C%2520MPC%2520formulation%2520for%2520inertial%2520morphing%252C%250Aactuator%2520requirements%252C%2520and%2520simulation%2520results%252C%2520illustrating%2520the%2520potential%2520of%250Aagile%2520exploration%2520for%2520small-scale%2520rovers%2520in%2520low-gravity%2520environments%2520like%2520the%250AMoon%2520or%2520asteroids.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10156v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tethered%20Variable%20Inertial%20Attitude%20Control%20Mechanisms%20through%20a%20Modular%0A%20%20Jumping%20Limbed%20Robot&entry.906535625=Yusuke%20Tanaka%20and%20Alvin%20Zhu%20and%20Dennis%20Hong&entry.1292438233=%20%20This%20paper%20presents%20the%20concept%20of%20a%20tethered%20variable%20inertial%20attitude%0Acontrol%20mechanism%20for%20a%20modular%20jumping-limbed%20robot%20designed%20for%20planetary%0Aexploration%20in%20low-gravity%20environments.%20The%20system%2C%20named%20SPLITTER%2C%20comprises%0Atwo%20sub-10%20kg%20quadrupedal%20robots%20connected%20by%20a%20tether%2C%20capable%20of%20executing%0Asuccessive%20jumping%20gaits%20and%20stabilizing%20in-flight%20using%20inertial%20morphing%0Atechnology.%20Through%20model%20predictive%20control%20%28MPC%29%2C%20attitude%20control%20was%0Ademonstrated%20by%20adjusting%20the%20limbs%20and%20tether%20length%20to%20modulate%20the%20system%27s%0Aprincipal%20moments%20of%20inertia.%20Our%20results%20indicate%20that%20this%20control%20strategy%0Aallows%20the%20robot%20to%20stabilize%20during%20flight%20phases%20without%20needing%20traditional%0Aflywheel-based%20systems%20or%20relying%20on%20aerodynamics%2C%20making%20the%20approach%0Amass-efficient%20and%20ideal%20for%20small-scale%20planetary%20robots%27%20successive%20jumps.%0AThe%20paper%20outlines%20the%20dynamics%2C%20MPC%20formulation%20for%20inertial%20morphing%2C%0Aactuator%20requirements%2C%20and%20simulation%20results%2C%20illustrating%20the%20potential%20of%0Aagile%20exploration%20for%20small-scale%20rovers%20in%20low-gravity%20environments%20like%20the%0AMoon%20or%20asteroids.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10156v1&entry.124074799=Read"},
{"title": "The Animal-AI Environment: A Virtual Laboratory For Comparative\n  Cognition and Artificial Intelligence Research", "author": "Konstantinos Voudouris and Ibrahim Alhas and Wout Schellaert and Matteo G. Mecattaf and Ben Slater and Matthew Crosby and Joel Holmes and John Burden and Niharika Chaubey and Niall Donnelly and Matishalin Patel and Marta Halina and Jos\u00e9 Hern\u00e1ndez-Orallo and Lucy G. Cheke", "abstract": "  The Animal-AI Environment is a unique game-based research platform designed\nto facilitate collaboration between the artificial intelligence and comparative\ncognition research communities. In this paper, we present the latest version of\nthe Animal-AI Environment, outlining several major features that make the game\nmore engaging for humans and more complex for AI systems. These features\ninclude interactive buttons, reward dispensers, and player notifications, as\nwell as an overhaul of the environment's graphics and processing for\nsignificant improvements in agent training time and quality of the human player\nexperience. We provide detailed guidance on how to build computational and\nbehavioural experiments with the Animal-AI Environment. We present results from\na series of agents, including the state-of-the-art deep reinforcement learning\nagent Dreamer-v3, on newly designed tests and the Animal-AI Testbed of 900\ntasks inspired by research in the field of comparative cognition. The Animal-AI\nEnvironment offers a new approach for modelling cognition in humans and\nnon-human animals, and for building biologically inspired artificial\nintelligence.\n", "link": "http://arxiv.org/abs/2312.11414v3", "date": "2025-01-17", "relevancy": 1.5905, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5489}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5356}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Animal-AI%20Environment%3A%20A%20Virtual%20Laboratory%20For%20Comparative%0A%20%20Cognition%20and%20Artificial%20Intelligence%20Research&body=Title%3A%20The%20Animal-AI%20Environment%3A%20A%20Virtual%20Laboratory%20For%20Comparative%0A%20%20Cognition%20and%20Artificial%20Intelligence%20Research%0AAuthor%3A%20Konstantinos%20Voudouris%20and%20Ibrahim%20Alhas%20and%20Wout%20Schellaert%20and%20Matteo%20G.%20Mecattaf%20and%20Ben%20Slater%20and%20Matthew%20Crosby%20and%20Joel%20Holmes%20and%20John%20Burden%20and%20Niharika%20Chaubey%20and%20Niall%20Donnelly%20and%20Matishalin%20Patel%20and%20Marta%20Halina%20and%20Jos%C3%A9%20Hern%C3%A1ndez-Orallo%20and%20Lucy%20G.%20Cheke%0AAbstract%3A%20%20%20The%20Animal-AI%20Environment%20is%20a%20unique%20game-based%20research%20platform%20designed%0Ato%20facilitate%20collaboration%20between%20the%20artificial%20intelligence%20and%20comparative%0Acognition%20research%20communities.%20In%20this%20paper%2C%20we%20present%20the%20latest%20version%20of%0Athe%20Animal-AI%20Environment%2C%20outlining%20several%20major%20features%20that%20make%20the%20game%0Amore%20engaging%20for%20humans%20and%20more%20complex%20for%20AI%20systems.%20These%20features%0Ainclude%20interactive%20buttons%2C%20reward%20dispensers%2C%20and%20player%20notifications%2C%20as%0Awell%20as%20an%20overhaul%20of%20the%20environment%27s%20graphics%20and%20processing%20for%0Asignificant%20improvements%20in%20agent%20training%20time%20and%20quality%20of%20the%20human%20player%0Aexperience.%20We%20provide%20detailed%20guidance%20on%20how%20to%20build%20computational%20and%0Abehavioural%20experiments%20with%20the%20Animal-AI%20Environment.%20We%20present%20results%20from%0Aa%20series%20of%20agents%2C%20including%20the%20state-of-the-art%20deep%20reinforcement%20learning%0Aagent%20Dreamer-v3%2C%20on%20newly%20designed%20tests%20and%20the%20Animal-AI%20Testbed%20of%20900%0Atasks%20inspired%20by%20research%20in%20the%20field%20of%20comparative%20cognition.%20The%20Animal-AI%0AEnvironment%20offers%20a%20new%20approach%20for%20modelling%20cognition%20in%20humans%20and%0Anon-human%20animals%2C%20and%20for%20building%20biologically%20inspired%20artificial%0Aintelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.11414v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Animal-AI%2520Environment%253A%2520A%2520Virtual%2520Laboratory%2520For%2520Comparative%250A%2520%2520Cognition%2520and%2520Artificial%2520Intelligence%2520Research%26entry.906535625%3DKonstantinos%2520Voudouris%2520and%2520Ibrahim%2520Alhas%2520and%2520Wout%2520Schellaert%2520and%2520Matteo%2520G.%2520Mecattaf%2520and%2520Ben%2520Slater%2520and%2520Matthew%2520Crosby%2520and%2520Joel%2520Holmes%2520and%2520John%2520Burden%2520and%2520Niharika%2520Chaubey%2520and%2520Niall%2520Donnelly%2520and%2520Matishalin%2520Patel%2520and%2520Marta%2520Halina%2520and%2520Jos%25C3%25A9%2520Hern%25C3%25A1ndez-Orallo%2520and%2520Lucy%2520G.%2520Cheke%26entry.1292438233%3D%2520%2520The%2520Animal-AI%2520Environment%2520is%2520a%2520unique%2520game-based%2520research%2520platform%2520designed%250Ato%2520facilitate%2520collaboration%2520between%2520the%2520artificial%2520intelligence%2520and%2520comparative%250Acognition%2520research%2520communities.%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520latest%2520version%2520of%250Athe%2520Animal-AI%2520Environment%252C%2520outlining%2520several%2520major%2520features%2520that%2520make%2520the%2520game%250Amore%2520engaging%2520for%2520humans%2520and%2520more%2520complex%2520for%2520AI%2520systems.%2520These%2520features%250Ainclude%2520interactive%2520buttons%252C%2520reward%2520dispensers%252C%2520and%2520player%2520notifications%252C%2520as%250Awell%2520as%2520an%2520overhaul%2520of%2520the%2520environment%2527s%2520graphics%2520and%2520processing%2520for%250Asignificant%2520improvements%2520in%2520agent%2520training%2520time%2520and%2520quality%2520of%2520the%2520human%2520player%250Aexperience.%2520We%2520provide%2520detailed%2520guidance%2520on%2520how%2520to%2520build%2520computational%2520and%250Abehavioural%2520experiments%2520with%2520the%2520Animal-AI%2520Environment.%2520We%2520present%2520results%2520from%250Aa%2520series%2520of%2520agents%252C%2520including%2520the%2520state-of-the-art%2520deep%2520reinforcement%2520learning%250Aagent%2520Dreamer-v3%252C%2520on%2520newly%2520designed%2520tests%2520and%2520the%2520Animal-AI%2520Testbed%2520of%2520900%250Atasks%2520inspired%2520by%2520research%2520in%2520the%2520field%2520of%2520comparative%2520cognition.%2520The%2520Animal-AI%250AEnvironment%2520offers%2520a%2520new%2520approach%2520for%2520modelling%2520cognition%2520in%2520humans%2520and%250Anon-human%2520animals%252C%2520and%2520for%2520building%2520biologically%2520inspired%2520artificial%250Aintelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.11414v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Animal-AI%20Environment%3A%20A%20Virtual%20Laboratory%20For%20Comparative%0A%20%20Cognition%20and%20Artificial%20Intelligence%20Research&entry.906535625=Konstantinos%20Voudouris%20and%20Ibrahim%20Alhas%20and%20Wout%20Schellaert%20and%20Matteo%20G.%20Mecattaf%20and%20Ben%20Slater%20and%20Matthew%20Crosby%20and%20Joel%20Holmes%20and%20John%20Burden%20and%20Niharika%20Chaubey%20and%20Niall%20Donnelly%20and%20Matishalin%20Patel%20and%20Marta%20Halina%20and%20Jos%C3%A9%20Hern%C3%A1ndez-Orallo%20and%20Lucy%20G.%20Cheke&entry.1292438233=%20%20The%20Animal-AI%20Environment%20is%20a%20unique%20game-based%20research%20platform%20designed%0Ato%20facilitate%20collaboration%20between%20the%20artificial%20intelligence%20and%20comparative%0Acognition%20research%20communities.%20In%20this%20paper%2C%20we%20present%20the%20latest%20version%20of%0Athe%20Animal-AI%20Environment%2C%20outlining%20several%20major%20features%20that%20make%20the%20game%0Amore%20engaging%20for%20humans%20and%20more%20complex%20for%20AI%20systems.%20These%20features%0Ainclude%20interactive%20buttons%2C%20reward%20dispensers%2C%20and%20player%20notifications%2C%20as%0Awell%20as%20an%20overhaul%20of%20the%20environment%27s%20graphics%20and%20processing%20for%0Asignificant%20improvements%20in%20agent%20training%20time%20and%20quality%20of%20the%20human%20player%0Aexperience.%20We%20provide%20detailed%20guidance%20on%20how%20to%20build%20computational%20and%0Abehavioural%20experiments%20with%20the%20Animal-AI%20Environment.%20We%20present%20results%20from%0Aa%20series%20of%20agents%2C%20including%20the%20state-of-the-art%20deep%20reinforcement%20learning%0Aagent%20Dreamer-v3%2C%20on%20newly%20designed%20tests%20and%20the%20Animal-AI%20Testbed%20of%20900%0Atasks%20inspired%20by%20research%20in%20the%20field%20of%20comparative%20cognition.%20The%20Animal-AI%0AEnvironment%20offers%20a%20new%20approach%20for%20modelling%20cognition%20in%20humans%20and%0Anon-human%20animals%2C%20and%20for%20building%20biologically%20inspired%20artificial%0Aintelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.11414v3&entry.124074799=Read"},
{"title": "Enhancing reliability in prediction intervals using point forecasters:\n  Heteroscedastic Quantile Regression and Width-Adaptive Conformal Inference", "author": "Carlos Sebasti\u00e1n and Carlos E. Gonz\u00e1lez-Guill\u00e9n and Jes\u00fas Juan", "abstract": "  Constructing prediction intervals for time series forecasting is challenging,\nparticularly when practitioners rely solely on point forecasts. While previous\nresearch has focused on creating increasingly efficient intervals, we argue\nthat standard measures alone are inadequate. Beyond efficiency, prediction\nintervals must adapt their width based on the difficulty of the prediction\nwhile preserving coverage regardless of complexity. To address these issues, we\npropose combining Heteroscedastic Quantile Regression (HQR) with Width-Adaptive\nConformal Inference (WACI). This integrated procedure guarantees theoretical\ncoverage and enables interval widths to vary with predictive uncertainty. We\nassess its performance using both a synthetic example and a real world\nElectricity Price Forecasting scenario. Our results show that this combined\napproach meets or surpasses typical benchmarks for validity and efficiency,\nwhile also fulfilling important yet often overlooked practical requirements.\n", "link": "http://arxiv.org/abs/2406.14904v2", "date": "2025-01-17", "relevancy": 1.3014, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4778}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4254}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20reliability%20in%20prediction%20intervals%20using%20point%20forecasters%3A%0A%20%20Heteroscedastic%20Quantile%20Regression%20and%20Width-Adaptive%20Conformal%20Inference&body=Title%3A%20Enhancing%20reliability%20in%20prediction%20intervals%20using%20point%20forecasters%3A%0A%20%20Heteroscedastic%20Quantile%20Regression%20and%20Width-Adaptive%20Conformal%20Inference%0AAuthor%3A%20Carlos%20Sebasti%C3%A1n%20and%20Carlos%20E.%20Gonz%C3%A1lez-Guill%C3%A9n%20and%20Jes%C3%BAs%20Juan%0AAbstract%3A%20%20%20Constructing%20prediction%20intervals%20for%20time%20series%20forecasting%20is%20challenging%2C%0Aparticularly%20when%20practitioners%20rely%20solely%20on%20point%20forecasts.%20While%20previous%0Aresearch%20has%20focused%20on%20creating%20increasingly%20efficient%20intervals%2C%20we%20argue%0Athat%20standard%20measures%20alone%20are%20inadequate.%20Beyond%20efficiency%2C%20prediction%0Aintervals%20must%20adapt%20their%20width%20based%20on%20the%20difficulty%20of%20the%20prediction%0Awhile%20preserving%20coverage%20regardless%20of%20complexity.%20To%20address%20these%20issues%2C%20we%0Apropose%20combining%20Heteroscedastic%20Quantile%20Regression%20%28HQR%29%20with%20Width-Adaptive%0AConformal%20Inference%20%28WACI%29.%20This%20integrated%20procedure%20guarantees%20theoretical%0Acoverage%20and%20enables%20interval%20widths%20to%20vary%20with%20predictive%20uncertainty.%20We%0Aassess%20its%20performance%20using%20both%20a%20synthetic%20example%20and%20a%20real%20world%0AElectricity%20Price%20Forecasting%20scenario.%20Our%20results%20show%20that%20this%20combined%0Aapproach%20meets%20or%20surpasses%20typical%20benchmarks%20for%20validity%20and%20efficiency%2C%0Awhile%20also%20fulfilling%20important%20yet%20often%20overlooked%20practical%20requirements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14904v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520reliability%2520in%2520prediction%2520intervals%2520using%2520point%2520forecasters%253A%250A%2520%2520Heteroscedastic%2520Quantile%2520Regression%2520and%2520Width-Adaptive%2520Conformal%2520Inference%26entry.906535625%3DCarlos%2520Sebasti%25C3%25A1n%2520and%2520Carlos%2520E.%2520Gonz%25C3%25A1lez-Guill%25C3%25A9n%2520and%2520Jes%25C3%25BAs%2520Juan%26entry.1292438233%3D%2520%2520Constructing%2520prediction%2520intervals%2520for%2520time%2520series%2520forecasting%2520is%2520challenging%252C%250Aparticularly%2520when%2520practitioners%2520rely%2520solely%2520on%2520point%2520forecasts.%2520While%2520previous%250Aresearch%2520has%2520focused%2520on%2520creating%2520increasingly%2520efficient%2520intervals%252C%2520we%2520argue%250Athat%2520standard%2520measures%2520alone%2520are%2520inadequate.%2520Beyond%2520efficiency%252C%2520prediction%250Aintervals%2520must%2520adapt%2520their%2520width%2520based%2520on%2520the%2520difficulty%2520of%2520the%2520prediction%250Awhile%2520preserving%2520coverage%2520regardless%2520of%2520complexity.%2520To%2520address%2520these%2520issues%252C%2520we%250Apropose%2520combining%2520Heteroscedastic%2520Quantile%2520Regression%2520%2528HQR%2529%2520with%2520Width-Adaptive%250AConformal%2520Inference%2520%2528WACI%2529.%2520This%2520integrated%2520procedure%2520guarantees%2520theoretical%250Acoverage%2520and%2520enables%2520interval%2520widths%2520to%2520vary%2520with%2520predictive%2520uncertainty.%2520We%250Aassess%2520its%2520performance%2520using%2520both%2520a%2520synthetic%2520example%2520and%2520a%2520real%2520world%250AElectricity%2520Price%2520Forecasting%2520scenario.%2520Our%2520results%2520show%2520that%2520this%2520combined%250Aapproach%2520meets%2520or%2520surpasses%2520typical%2520benchmarks%2520for%2520validity%2520and%2520efficiency%252C%250Awhile%2520also%2520fulfilling%2520important%2520yet%2520often%2520overlooked%2520practical%2520requirements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14904v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20reliability%20in%20prediction%20intervals%20using%20point%20forecasters%3A%0A%20%20Heteroscedastic%20Quantile%20Regression%20and%20Width-Adaptive%20Conformal%20Inference&entry.906535625=Carlos%20Sebasti%C3%A1n%20and%20Carlos%20E.%20Gonz%C3%A1lez-Guill%C3%A9n%20and%20Jes%C3%BAs%20Juan&entry.1292438233=%20%20Constructing%20prediction%20intervals%20for%20time%20series%20forecasting%20is%20challenging%2C%0Aparticularly%20when%20practitioners%20rely%20solely%20on%20point%20forecasts.%20While%20previous%0Aresearch%20has%20focused%20on%20creating%20increasingly%20efficient%20intervals%2C%20we%20argue%0Athat%20standard%20measures%20alone%20are%20inadequate.%20Beyond%20efficiency%2C%20prediction%0Aintervals%20must%20adapt%20their%20width%20based%20on%20the%20difficulty%20of%20the%20prediction%0Awhile%20preserving%20coverage%20regardless%20of%20complexity.%20To%20address%20these%20issues%2C%20we%0Apropose%20combining%20Heteroscedastic%20Quantile%20Regression%20%28HQR%29%20with%20Width-Adaptive%0AConformal%20Inference%20%28WACI%29.%20This%20integrated%20procedure%20guarantees%20theoretical%0Acoverage%20and%20enables%20interval%20widths%20to%20vary%20with%20predictive%20uncertainty.%20We%0Aassess%20its%20performance%20using%20both%20a%20synthetic%20example%20and%20a%20real%20world%0AElectricity%20Price%20Forecasting%20scenario.%20Our%20results%20show%20that%20this%20combined%0Aapproach%20meets%20or%20surpasses%20typical%20benchmarks%20for%20validity%20and%20efficiency%2C%0Awhile%20also%20fulfilling%20important%20yet%20often%20overlooked%20practical%20requirements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14904v2&entry.124074799=Read"},
{"title": "Counterfactual Explanations for k-means and Gaussian Clustering", "author": "Georgios Vardakas and Antonia Karra and Evaggelia Pitoura and Aristidis Likas", "abstract": "  Counterfactuals have been recognized as an effective approach to explain\nclassifier decisions. Nevertheless, they have not yet been considered in the\ncontext of clustering. In this work, we propose the use of counterfactuals to\nexplain clustering solutions. First, we present a general definition for\ncounterfactuals for model-based clustering that includes plausibility and\nfeasibility constraints. Then we consider the counterfactual generation problem\nfor k-means and Gaussian clustering assuming Euclidean distance. Our approach\ntakes as input the factual, the target cluster, a binary mask indicating\nactionable or immutable features and a plausibility factor specifying how far\nfrom the cluster boundary the counterfactual should be placed. In the k-means\nclustering case, analytical mathematical formulas are presented for computing\nthe optimal solution, while in the Gaussian clustering case (assuming full,\ndiagonal, or spherical covariances) our method requires the numerical solution\nof a nonlinear equation with a single parameter only. We demonstrate the\nadvantages of our approach through illustrative examples and quantitative\nexperimental comparisons.\n", "link": "http://arxiv.org/abs/2501.10234v1", "date": "2025-01-17", "relevancy": 1.2245, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4349}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4024}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3998}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Counterfactual%20Explanations%20for%20k-means%20and%20Gaussian%20Clustering&body=Title%3A%20Counterfactual%20Explanations%20for%20k-means%20and%20Gaussian%20Clustering%0AAuthor%3A%20Georgios%20Vardakas%20and%20Antonia%20Karra%20and%20Evaggelia%20Pitoura%20and%20Aristidis%20Likas%0AAbstract%3A%20%20%20Counterfactuals%20have%20been%20recognized%20as%20an%20effective%20approach%20to%20explain%0Aclassifier%20decisions.%20Nevertheless%2C%20they%20have%20not%20yet%20been%20considered%20in%20the%0Acontext%20of%20clustering.%20In%20this%20work%2C%20we%20propose%20the%20use%20of%20counterfactuals%20to%0Aexplain%20clustering%20solutions.%20First%2C%20we%20present%20a%20general%20definition%20for%0Acounterfactuals%20for%20model-based%20clustering%20that%20includes%20plausibility%20and%0Afeasibility%20constraints.%20Then%20we%20consider%20the%20counterfactual%20generation%20problem%0Afor%20k-means%20and%20Gaussian%20clustering%20assuming%20Euclidean%20distance.%20Our%20approach%0Atakes%20as%20input%20the%20factual%2C%20the%20target%20cluster%2C%20a%20binary%20mask%20indicating%0Aactionable%20or%20immutable%20features%20and%20a%20plausibility%20factor%20specifying%20how%20far%0Afrom%20the%20cluster%20boundary%20the%20counterfactual%20should%20be%20placed.%20In%20the%20k-means%0Aclustering%20case%2C%20analytical%20mathematical%20formulas%20are%20presented%20for%20computing%0Athe%20optimal%20solution%2C%20while%20in%20the%20Gaussian%20clustering%20case%20%28assuming%20full%2C%0Adiagonal%2C%20or%20spherical%20covariances%29%20our%20method%20requires%20the%20numerical%20solution%0Aof%20a%20nonlinear%20equation%20with%20a%20single%20parameter%20only.%20We%20demonstrate%20the%0Aadvantages%20of%20our%20approach%20through%20illustrative%20examples%20and%20quantitative%0Aexperimental%20comparisons.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10234v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCounterfactual%2520Explanations%2520for%2520k-means%2520and%2520Gaussian%2520Clustering%26entry.906535625%3DGeorgios%2520Vardakas%2520and%2520Antonia%2520Karra%2520and%2520Evaggelia%2520Pitoura%2520and%2520Aristidis%2520Likas%26entry.1292438233%3D%2520%2520Counterfactuals%2520have%2520been%2520recognized%2520as%2520an%2520effective%2520approach%2520to%2520explain%250Aclassifier%2520decisions.%2520Nevertheless%252C%2520they%2520have%2520not%2520yet%2520been%2520considered%2520in%2520the%250Acontext%2520of%2520clustering.%2520In%2520this%2520work%252C%2520we%2520propose%2520the%2520use%2520of%2520counterfactuals%2520to%250Aexplain%2520clustering%2520solutions.%2520First%252C%2520we%2520present%2520a%2520general%2520definition%2520for%250Acounterfactuals%2520for%2520model-based%2520clustering%2520that%2520includes%2520plausibility%2520and%250Afeasibility%2520constraints.%2520Then%2520we%2520consider%2520the%2520counterfactual%2520generation%2520problem%250Afor%2520k-means%2520and%2520Gaussian%2520clustering%2520assuming%2520Euclidean%2520distance.%2520Our%2520approach%250Atakes%2520as%2520input%2520the%2520factual%252C%2520the%2520target%2520cluster%252C%2520a%2520binary%2520mask%2520indicating%250Aactionable%2520or%2520immutable%2520features%2520and%2520a%2520plausibility%2520factor%2520specifying%2520how%2520far%250Afrom%2520the%2520cluster%2520boundary%2520the%2520counterfactual%2520should%2520be%2520placed.%2520In%2520the%2520k-means%250Aclustering%2520case%252C%2520analytical%2520mathematical%2520formulas%2520are%2520presented%2520for%2520computing%250Athe%2520optimal%2520solution%252C%2520while%2520in%2520the%2520Gaussian%2520clustering%2520case%2520%2528assuming%2520full%252C%250Adiagonal%252C%2520or%2520spherical%2520covariances%2529%2520our%2520method%2520requires%2520the%2520numerical%2520solution%250Aof%2520a%2520nonlinear%2520equation%2520with%2520a%2520single%2520parameter%2520only.%2520We%2520demonstrate%2520the%250Aadvantages%2520of%2520our%2520approach%2520through%2520illustrative%2520examples%2520and%2520quantitative%250Aexperimental%2520comparisons.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10234v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Counterfactual%20Explanations%20for%20k-means%20and%20Gaussian%20Clustering&entry.906535625=Georgios%20Vardakas%20and%20Antonia%20Karra%20and%20Evaggelia%20Pitoura%20and%20Aristidis%20Likas&entry.1292438233=%20%20Counterfactuals%20have%20been%20recognized%20as%20an%20effective%20approach%20to%20explain%0Aclassifier%20decisions.%20Nevertheless%2C%20they%20have%20not%20yet%20been%20considered%20in%20the%0Acontext%20of%20clustering.%20In%20this%20work%2C%20we%20propose%20the%20use%20of%20counterfactuals%20to%0Aexplain%20clustering%20solutions.%20First%2C%20we%20present%20a%20general%20definition%20for%0Acounterfactuals%20for%20model-based%20clustering%20that%20includes%20plausibility%20and%0Afeasibility%20constraints.%20Then%20we%20consider%20the%20counterfactual%20generation%20problem%0Afor%20k-means%20and%20Gaussian%20clustering%20assuming%20Euclidean%20distance.%20Our%20approach%0Atakes%20as%20input%20the%20factual%2C%20the%20target%20cluster%2C%20a%20binary%20mask%20indicating%0Aactionable%20or%20immutable%20features%20and%20a%20plausibility%20factor%20specifying%20how%20far%0Afrom%20the%20cluster%20boundary%20the%20counterfactual%20should%20be%20placed.%20In%20the%20k-means%0Aclustering%20case%2C%20analytical%20mathematical%20formulas%20are%20presented%20for%20computing%0Athe%20optimal%20solution%2C%20while%20in%20the%20Gaussian%20clustering%20case%20%28assuming%20full%2C%0Adiagonal%2C%20or%20spherical%20covariances%29%20our%20method%20requires%20the%20numerical%20solution%0Aof%20a%20nonlinear%20equation%20with%20a%20single%20parameter%20only.%20We%20demonstrate%20the%0Aadvantages%20of%20our%20approach%20through%20illustrative%20examples%20and%20quantitative%0Aexperimental%20comparisons.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10234v1&entry.124074799=Read"},
{"title": "MRI2Speech: Speech Synthesis from Articulatory Movements Recorded by\n  Real-time MRI", "author": "Neil Shah and Ayan Kashyap and Shirish Karande and Vineet Gandhi", "abstract": "  Previous real-time MRI (rtMRI)-based speech synthesis models depend heavily\non noisy ground-truth speech. Applying loss directly over ground truth\nmel-spectrograms entangles speech content with MRI noise, resulting in poor\nintelligibility. We introduce a novel approach that adapts the multi-modal\nself-supervised AV-HuBERT model for text prediction from rtMRI and incorporates\na new flow-based duration predictor for speaker-specific alignment. The\npredicted text and durations are then used by a speech decoder to synthesize\naligned speech in any novel voice. We conduct thorough experiments on two\ndatasets and demonstrate our method's generalization ability to unseen\nspeakers. We assess our framework's performance by masking parts of the rtMRI\nvideo to evaluate the impact of different articulators on text prediction. Our\nmethod achieves a $15.18\\%$ Word Error Rate (WER) on the USC-TIMIT MRI corpus,\nmarking a huge improvement over the current state-of-the-art. Speech samples\nare available at https://mri2speech.github.io/MRI2Speech/\n", "link": "http://arxiv.org/abs/2412.18836v2", "date": "2025-01-17", "relevancy": 1.5149, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5443}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4956}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4892}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MRI2Speech%3A%20Speech%20Synthesis%20from%20Articulatory%20Movements%20Recorded%20by%0A%20%20Real-time%20MRI&body=Title%3A%20MRI2Speech%3A%20Speech%20Synthesis%20from%20Articulatory%20Movements%20Recorded%20by%0A%20%20Real-time%20MRI%0AAuthor%3A%20Neil%20Shah%20and%20Ayan%20Kashyap%20and%20Shirish%20Karande%20and%20Vineet%20Gandhi%0AAbstract%3A%20%20%20Previous%20real-time%20MRI%20%28rtMRI%29-based%20speech%20synthesis%20models%20depend%20heavily%0Aon%20noisy%20ground-truth%20speech.%20Applying%20loss%20directly%20over%20ground%20truth%0Amel-spectrograms%20entangles%20speech%20content%20with%20MRI%20noise%2C%20resulting%20in%20poor%0Aintelligibility.%20We%20introduce%20a%20novel%20approach%20that%20adapts%20the%20multi-modal%0Aself-supervised%20AV-HuBERT%20model%20for%20text%20prediction%20from%20rtMRI%20and%20incorporates%0Aa%20new%20flow-based%20duration%20predictor%20for%20speaker-specific%20alignment.%20The%0Apredicted%20text%20and%20durations%20are%20then%20used%20by%20a%20speech%20decoder%20to%20synthesize%0Aaligned%20speech%20in%20any%20novel%20voice.%20We%20conduct%20thorough%20experiments%20on%20two%0Adatasets%20and%20demonstrate%20our%20method%27s%20generalization%20ability%20to%20unseen%0Aspeakers.%20We%20assess%20our%20framework%27s%20performance%20by%20masking%20parts%20of%20the%20rtMRI%0Avideo%20to%20evaluate%20the%20impact%20of%20different%20articulators%20on%20text%20prediction.%20Our%0Amethod%20achieves%20a%20%2415.18%5C%25%24%20Word%20Error%20Rate%20%28WER%29%20on%20the%20USC-TIMIT%20MRI%20corpus%2C%0Amarking%20a%20huge%20improvement%20over%20the%20current%20state-of-the-art.%20Speech%20samples%0Aare%20available%20at%20https%3A//mri2speech.github.io/MRI2Speech/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18836v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMRI2Speech%253A%2520Speech%2520Synthesis%2520from%2520Articulatory%2520Movements%2520Recorded%2520by%250A%2520%2520Real-time%2520MRI%26entry.906535625%3DNeil%2520Shah%2520and%2520Ayan%2520Kashyap%2520and%2520Shirish%2520Karande%2520and%2520Vineet%2520Gandhi%26entry.1292438233%3D%2520%2520Previous%2520real-time%2520MRI%2520%2528rtMRI%2529-based%2520speech%2520synthesis%2520models%2520depend%2520heavily%250Aon%2520noisy%2520ground-truth%2520speech.%2520Applying%2520loss%2520directly%2520over%2520ground%2520truth%250Amel-spectrograms%2520entangles%2520speech%2520content%2520with%2520MRI%2520noise%252C%2520resulting%2520in%2520poor%250Aintelligibility.%2520We%2520introduce%2520a%2520novel%2520approach%2520that%2520adapts%2520the%2520multi-modal%250Aself-supervised%2520AV-HuBERT%2520model%2520for%2520text%2520prediction%2520from%2520rtMRI%2520and%2520incorporates%250Aa%2520new%2520flow-based%2520duration%2520predictor%2520for%2520speaker-specific%2520alignment.%2520The%250Apredicted%2520text%2520and%2520durations%2520are%2520then%2520used%2520by%2520a%2520speech%2520decoder%2520to%2520synthesize%250Aaligned%2520speech%2520in%2520any%2520novel%2520voice.%2520We%2520conduct%2520thorough%2520experiments%2520on%2520two%250Adatasets%2520and%2520demonstrate%2520our%2520method%2527s%2520generalization%2520ability%2520to%2520unseen%250Aspeakers.%2520We%2520assess%2520our%2520framework%2527s%2520performance%2520by%2520masking%2520parts%2520of%2520the%2520rtMRI%250Avideo%2520to%2520evaluate%2520the%2520impact%2520of%2520different%2520articulators%2520on%2520text%2520prediction.%2520Our%250Amethod%2520achieves%2520a%2520%252415.18%255C%2525%2524%2520Word%2520Error%2520Rate%2520%2528WER%2529%2520on%2520the%2520USC-TIMIT%2520MRI%2520corpus%252C%250Amarking%2520a%2520huge%2520improvement%2520over%2520the%2520current%2520state-of-the-art.%2520Speech%2520samples%250Aare%2520available%2520at%2520https%253A//mri2speech.github.io/MRI2Speech/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18836v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MRI2Speech%3A%20Speech%20Synthesis%20from%20Articulatory%20Movements%20Recorded%20by%0A%20%20Real-time%20MRI&entry.906535625=Neil%20Shah%20and%20Ayan%20Kashyap%20and%20Shirish%20Karande%20and%20Vineet%20Gandhi&entry.1292438233=%20%20Previous%20real-time%20MRI%20%28rtMRI%29-based%20speech%20synthesis%20models%20depend%20heavily%0Aon%20noisy%20ground-truth%20speech.%20Applying%20loss%20directly%20over%20ground%20truth%0Amel-spectrograms%20entangles%20speech%20content%20with%20MRI%20noise%2C%20resulting%20in%20poor%0Aintelligibility.%20We%20introduce%20a%20novel%20approach%20that%20adapts%20the%20multi-modal%0Aself-supervised%20AV-HuBERT%20model%20for%20text%20prediction%20from%20rtMRI%20and%20incorporates%0Aa%20new%20flow-based%20duration%20predictor%20for%20speaker-specific%20alignment.%20The%0Apredicted%20text%20and%20durations%20are%20then%20used%20by%20a%20speech%20decoder%20to%20synthesize%0Aaligned%20speech%20in%20any%20novel%20voice.%20We%20conduct%20thorough%20experiments%20on%20two%0Adatasets%20and%20demonstrate%20our%20method%27s%20generalization%20ability%20to%20unseen%0Aspeakers.%20We%20assess%20our%20framework%27s%20performance%20by%20masking%20parts%20of%20the%20rtMRI%0Avideo%20to%20evaluate%20the%20impact%20of%20different%20articulators%20on%20text%20prediction.%20Our%0Amethod%20achieves%20a%20%2415.18%5C%25%24%20Word%20Error%20Rate%20%28WER%29%20on%20the%20USC-TIMIT%20MRI%20corpus%2C%0Amarking%20a%20huge%20improvement%20over%20the%20current%20state-of-the-art.%20Speech%20samples%0Aare%20available%20at%20https%3A//mri2speech.github.io/MRI2Speech/%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18836v2&entry.124074799=Read"},
{"title": "Bandit on the Hunt: Dynamic Crawling for Cyber Threat Intelligence", "author": "Philipp Kuehn and Dilara Nadermahmoodi and Markus Bayer and Christian Reuter", "abstract": "  Public information contains valuable Cyber Threat Intelligence (CTI) that is\nused to prevent attacks in the future. Ideally, the learnings from previous\nattacks help to mitigate all those that follow. While there are standards for\nsharing this information, much of it is shared in non-standardized news\narticles or blog posts. It is a time-consuming task to monitor online sources\nfor threats and even then, one can never be sure, to use the right sources.\nCurrent research propose extractors of Indicators of Compromise from known\nsources, while the identification of new sources is rarely considered. This\npaper proposes a focused crawler focused on the CTI domain based on multi-armed\nbandit ( MAB) and different crawling strategies. It uses SBERT to identify\nrelevant documents, while dynamically adapt its crawling path. We propose a\nsystem called ThreatCrawl, which achieve a harvest rate of over 25% and is able\nto expand its used seed by over 300%, while retaining focus on the topic at\nhand. In addition, this crawler identified previously unknown but highly\nrelevant overview pages, datasets, and domains.\n", "link": "http://arxiv.org/abs/2304.11960v3", "date": "2025-01-17", "relevancy": 1.2773, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4462}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4208}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4177}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bandit%20on%20the%20Hunt%3A%20Dynamic%20Crawling%20for%20Cyber%20Threat%20Intelligence&body=Title%3A%20Bandit%20on%20the%20Hunt%3A%20Dynamic%20Crawling%20for%20Cyber%20Threat%20Intelligence%0AAuthor%3A%20Philipp%20Kuehn%20and%20Dilara%20Nadermahmoodi%20and%20Markus%20Bayer%20and%20Christian%20Reuter%0AAbstract%3A%20%20%20Public%20information%20contains%20valuable%20Cyber%20Threat%20Intelligence%20%28CTI%29%20that%20is%0Aused%20to%20prevent%20attacks%20in%20the%20future.%20Ideally%2C%20the%20learnings%20from%20previous%0Aattacks%20help%20to%20mitigate%20all%20those%20that%20follow.%20While%20there%20are%20standards%20for%0Asharing%20this%20information%2C%20much%20of%20it%20is%20shared%20in%20non-standardized%20news%0Aarticles%20or%20blog%20posts.%20It%20is%20a%20time-consuming%20task%20to%20monitor%20online%20sources%0Afor%20threats%20and%20even%20then%2C%20one%20can%20never%20be%20sure%2C%20to%20use%20the%20right%20sources.%0ACurrent%20research%20propose%20extractors%20of%20Indicators%20of%20Compromise%20from%20known%0Asources%2C%20while%20the%20identification%20of%20new%20sources%20is%20rarely%20considered.%20This%0Apaper%20proposes%20a%20focused%20crawler%20focused%20on%20the%20CTI%20domain%20based%20on%20multi-armed%0Abandit%20%28%20MAB%29%20and%20different%20crawling%20strategies.%20It%20uses%20SBERT%20to%20identify%0Arelevant%20documents%2C%20while%20dynamically%20adapt%20its%20crawling%20path.%20We%20propose%20a%0Asystem%20called%20ThreatCrawl%2C%20which%20achieve%20a%20harvest%20rate%20of%20over%2025%25%20and%20is%20able%0Ato%20expand%20its%20used%20seed%20by%20over%20300%25%2C%20while%20retaining%20focus%20on%20the%20topic%20at%0Ahand.%20In%20addition%2C%20this%20crawler%20identified%20previously%20unknown%20but%20highly%0Arelevant%20overview%20pages%2C%20datasets%2C%20and%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.11960v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBandit%2520on%2520the%2520Hunt%253A%2520Dynamic%2520Crawling%2520for%2520Cyber%2520Threat%2520Intelligence%26entry.906535625%3DPhilipp%2520Kuehn%2520and%2520Dilara%2520Nadermahmoodi%2520and%2520Markus%2520Bayer%2520and%2520Christian%2520Reuter%26entry.1292438233%3D%2520%2520Public%2520information%2520contains%2520valuable%2520Cyber%2520Threat%2520Intelligence%2520%2528CTI%2529%2520that%2520is%250Aused%2520to%2520prevent%2520attacks%2520in%2520the%2520future.%2520Ideally%252C%2520the%2520learnings%2520from%2520previous%250Aattacks%2520help%2520to%2520mitigate%2520all%2520those%2520that%2520follow.%2520While%2520there%2520are%2520standards%2520for%250Asharing%2520this%2520information%252C%2520much%2520of%2520it%2520is%2520shared%2520in%2520non-standardized%2520news%250Aarticles%2520or%2520blog%2520posts.%2520It%2520is%2520a%2520time-consuming%2520task%2520to%2520monitor%2520online%2520sources%250Afor%2520threats%2520and%2520even%2520then%252C%2520one%2520can%2520never%2520be%2520sure%252C%2520to%2520use%2520the%2520right%2520sources.%250ACurrent%2520research%2520propose%2520extractors%2520of%2520Indicators%2520of%2520Compromise%2520from%2520known%250Asources%252C%2520while%2520the%2520identification%2520of%2520new%2520sources%2520is%2520rarely%2520considered.%2520This%250Apaper%2520proposes%2520a%2520focused%2520crawler%2520focused%2520on%2520the%2520CTI%2520domain%2520based%2520on%2520multi-armed%250Abandit%2520%2528%2520MAB%2529%2520and%2520different%2520crawling%2520strategies.%2520It%2520uses%2520SBERT%2520to%2520identify%250Arelevant%2520documents%252C%2520while%2520dynamically%2520adapt%2520its%2520crawling%2520path.%2520We%2520propose%2520a%250Asystem%2520called%2520ThreatCrawl%252C%2520which%2520achieve%2520a%2520harvest%2520rate%2520of%2520over%252025%2525%2520and%2520is%2520able%250Ato%2520expand%2520its%2520used%2520seed%2520by%2520over%2520300%2525%252C%2520while%2520retaining%2520focus%2520on%2520the%2520topic%2520at%250Ahand.%2520In%2520addition%252C%2520this%2520crawler%2520identified%2520previously%2520unknown%2520but%2520highly%250Arelevant%2520overview%2520pages%252C%2520datasets%252C%2520and%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.11960v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bandit%20on%20the%20Hunt%3A%20Dynamic%20Crawling%20for%20Cyber%20Threat%20Intelligence&entry.906535625=Philipp%20Kuehn%20and%20Dilara%20Nadermahmoodi%20and%20Markus%20Bayer%20and%20Christian%20Reuter&entry.1292438233=%20%20Public%20information%20contains%20valuable%20Cyber%20Threat%20Intelligence%20%28CTI%29%20that%20is%0Aused%20to%20prevent%20attacks%20in%20the%20future.%20Ideally%2C%20the%20learnings%20from%20previous%0Aattacks%20help%20to%20mitigate%20all%20those%20that%20follow.%20While%20there%20are%20standards%20for%0Asharing%20this%20information%2C%20much%20of%20it%20is%20shared%20in%20non-standardized%20news%0Aarticles%20or%20blog%20posts.%20It%20is%20a%20time-consuming%20task%20to%20monitor%20online%20sources%0Afor%20threats%20and%20even%20then%2C%20one%20can%20never%20be%20sure%2C%20to%20use%20the%20right%20sources.%0ACurrent%20research%20propose%20extractors%20of%20Indicators%20of%20Compromise%20from%20known%0Asources%2C%20while%20the%20identification%20of%20new%20sources%20is%20rarely%20considered.%20This%0Apaper%20proposes%20a%20focused%20crawler%20focused%20on%20the%20CTI%20domain%20based%20on%20multi-armed%0Abandit%20%28%20MAB%29%20and%20different%20crawling%20strategies.%20It%20uses%20SBERT%20to%20identify%0Arelevant%20documents%2C%20while%20dynamically%20adapt%20its%20crawling%20path.%20We%20propose%20a%0Asystem%20called%20ThreatCrawl%2C%20which%20achieve%20a%20harvest%20rate%20of%20over%2025%25%20and%20is%20able%0Ato%20expand%20its%20used%20seed%20by%20over%20300%25%2C%20while%20retaining%20focus%20on%20the%20topic%20at%0Ahand.%20In%20addition%2C%20this%20crawler%20identified%20previously%20unknown%20but%20highly%0Arelevant%20overview%20pages%2C%20datasets%2C%20and%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.11960v3&entry.124074799=Read"},
{"title": "Logarithmic Regret for Nonlinear Control", "author": "James Wang and Bruce D. Lee and Ingvar Ziemann and Nikolai Matni", "abstract": "  We address the problem of learning to control an unknown nonlinear dynamical\nsystem through sequential interactions. Motivated by high-stakes applications\nin which mistakes can be catastrophic, such as robotics and healthcare, we\nstudy situations where it is possible for fast sequential learning to occur.\nFast sequential learning is characterized by the ability of the learning agent\nto incur logarithmic regret relative to a fully-informed baseline. We\ndemonstrate that fast sequential learning is achievable in a diverse class of\ncontinuous control problems where the system dynamics depend smoothly on\nunknown parameters, provided the optimal control policy is persistently\nexciting. Additionally, we derive a regret bound which grows with the square\nroot of the number of interactions for cases where the optimal policy is not\npersistently exciting. Our results provide the first regret bounds for\ncontrolling nonlinear dynamical systems depending nonlinearly on unknown\nparameters. We validate the trends our theory predicts in simulation on a\nsimple dynamical system.\n", "link": "http://arxiv.org/abs/2501.10261v1", "date": "2025-01-17", "relevancy": 1.4544, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5166}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4583}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4317}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Logarithmic%20Regret%20for%20Nonlinear%20Control&body=Title%3A%20Logarithmic%20Regret%20for%20Nonlinear%20Control%0AAuthor%3A%20James%20Wang%20and%20Bruce%20D.%20Lee%20and%20Ingvar%20Ziemann%20and%20Nikolai%20Matni%0AAbstract%3A%20%20%20We%20address%20the%20problem%20of%20learning%20to%20control%20an%20unknown%20nonlinear%20dynamical%0Asystem%20through%20sequential%20interactions.%20Motivated%20by%20high-stakes%20applications%0Ain%20which%20mistakes%20can%20be%20catastrophic%2C%20such%20as%20robotics%20and%20healthcare%2C%20we%0Astudy%20situations%20where%20it%20is%20possible%20for%20fast%20sequential%20learning%20to%20occur.%0AFast%20sequential%20learning%20is%20characterized%20by%20the%20ability%20of%20the%20learning%20agent%0Ato%20incur%20logarithmic%20regret%20relative%20to%20a%20fully-informed%20baseline.%20We%0Ademonstrate%20that%20fast%20sequential%20learning%20is%20achievable%20in%20a%20diverse%20class%20of%0Acontinuous%20control%20problems%20where%20the%20system%20dynamics%20depend%20smoothly%20on%0Aunknown%20parameters%2C%20provided%20the%20optimal%20control%20policy%20is%20persistently%0Aexciting.%20Additionally%2C%20we%20derive%20a%20regret%20bound%20which%20grows%20with%20the%20square%0Aroot%20of%20the%20number%20of%20interactions%20for%20cases%20where%20the%20optimal%20policy%20is%20not%0Apersistently%20exciting.%20Our%20results%20provide%20the%20first%20regret%20bounds%20for%0Acontrolling%20nonlinear%20dynamical%20systems%20depending%20nonlinearly%20on%20unknown%0Aparameters.%20We%20validate%20the%20trends%20our%20theory%20predicts%20in%20simulation%20on%20a%0Asimple%20dynamical%20system.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10261v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLogarithmic%2520Regret%2520for%2520Nonlinear%2520Control%26entry.906535625%3DJames%2520Wang%2520and%2520Bruce%2520D.%2520Lee%2520and%2520Ingvar%2520Ziemann%2520and%2520Nikolai%2520Matni%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520problem%2520of%2520learning%2520to%2520control%2520an%2520unknown%2520nonlinear%2520dynamical%250Asystem%2520through%2520sequential%2520interactions.%2520Motivated%2520by%2520high-stakes%2520applications%250Ain%2520which%2520mistakes%2520can%2520be%2520catastrophic%252C%2520such%2520as%2520robotics%2520and%2520healthcare%252C%2520we%250Astudy%2520situations%2520where%2520it%2520is%2520possible%2520for%2520fast%2520sequential%2520learning%2520to%2520occur.%250AFast%2520sequential%2520learning%2520is%2520characterized%2520by%2520the%2520ability%2520of%2520the%2520learning%2520agent%250Ato%2520incur%2520logarithmic%2520regret%2520relative%2520to%2520a%2520fully-informed%2520baseline.%2520We%250Ademonstrate%2520that%2520fast%2520sequential%2520learning%2520is%2520achievable%2520in%2520a%2520diverse%2520class%2520of%250Acontinuous%2520control%2520problems%2520where%2520the%2520system%2520dynamics%2520depend%2520smoothly%2520on%250Aunknown%2520parameters%252C%2520provided%2520the%2520optimal%2520control%2520policy%2520is%2520persistently%250Aexciting.%2520Additionally%252C%2520we%2520derive%2520a%2520regret%2520bound%2520which%2520grows%2520with%2520the%2520square%250Aroot%2520of%2520the%2520number%2520of%2520interactions%2520for%2520cases%2520where%2520the%2520optimal%2520policy%2520is%2520not%250Apersistently%2520exciting.%2520Our%2520results%2520provide%2520the%2520first%2520regret%2520bounds%2520for%250Acontrolling%2520nonlinear%2520dynamical%2520systems%2520depending%2520nonlinearly%2520on%2520unknown%250Aparameters.%2520We%2520validate%2520the%2520trends%2520our%2520theory%2520predicts%2520in%2520simulation%2520on%2520a%250Asimple%2520dynamical%2520system.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10261v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Logarithmic%20Regret%20for%20Nonlinear%20Control&entry.906535625=James%20Wang%20and%20Bruce%20D.%20Lee%20and%20Ingvar%20Ziemann%20and%20Nikolai%20Matni&entry.1292438233=%20%20We%20address%20the%20problem%20of%20learning%20to%20control%20an%20unknown%20nonlinear%20dynamical%0Asystem%20through%20sequential%20interactions.%20Motivated%20by%20high-stakes%20applications%0Ain%20which%20mistakes%20can%20be%20catastrophic%2C%20such%20as%20robotics%20and%20healthcare%2C%20we%0Astudy%20situations%20where%20it%20is%20possible%20for%20fast%20sequential%20learning%20to%20occur.%0AFast%20sequential%20learning%20is%20characterized%20by%20the%20ability%20of%20the%20learning%20agent%0Ato%20incur%20logarithmic%20regret%20relative%20to%20a%20fully-informed%20baseline.%20We%0Ademonstrate%20that%20fast%20sequential%20learning%20is%20achievable%20in%20a%20diverse%20class%20of%0Acontinuous%20control%20problems%20where%20the%20system%20dynamics%20depend%20smoothly%20on%0Aunknown%20parameters%2C%20provided%20the%20optimal%20control%20policy%20is%20persistently%0Aexciting.%20Additionally%2C%20we%20derive%20a%20regret%20bound%20which%20grows%20with%20the%20square%0Aroot%20of%20the%20number%20of%20interactions%20for%20cases%20where%20the%20optimal%20policy%20is%20not%0Apersistently%20exciting.%20Our%20results%20provide%20the%20first%20regret%20bounds%20for%0Acontrolling%20nonlinear%20dynamical%20systems%20depending%20nonlinearly%20on%20unknown%0Aparameters.%20We%20validate%20the%20trends%20our%20theory%20predicts%20in%20simulation%20on%20a%0Asimple%20dynamical%20system.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10261v1&entry.124074799=Read"},
{"title": "BBPOS: BERT-based Part-of-Speech Tagging for Uzbek", "author": "Latofat Bobojonova and Arofat Akhundjanova and Phil Ostheimer and Sophie Fellenz", "abstract": "  This paper advances NLP research for the low-resource Uzbek language by\nevaluating two previously untested monolingual Uzbek BERT models on the\npart-of-speech (POS) tagging task and introducing the first publicly available\nUPOS-tagged benchmark dataset for Uzbek. Our fine-tuned models achieve 91%\naverage accuracy, outperforming the baseline multi-lingual BERT as well as the\nrule-based tagger. Notably, these models capture intermediate POS changes\nthrough affixes and demonstrate context sensitivity, unlike existing rule-based\ntaggers.\n", "link": "http://arxiv.org/abs/2501.10107v1", "date": "2025-01-17", "relevancy": 1.5263, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4083}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.378}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3744}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BBPOS%3A%20BERT-based%20Part-of-Speech%20Tagging%20for%20Uzbek&body=Title%3A%20BBPOS%3A%20BERT-based%20Part-of-Speech%20Tagging%20for%20Uzbek%0AAuthor%3A%20Latofat%20Bobojonova%20and%20Arofat%20Akhundjanova%20and%20Phil%20Ostheimer%20and%20Sophie%20Fellenz%0AAbstract%3A%20%20%20This%20paper%20advances%20NLP%20research%20for%20the%20low-resource%20Uzbek%20language%20by%0Aevaluating%20two%20previously%20untested%20monolingual%20Uzbek%20BERT%20models%20on%20the%0Apart-of-speech%20%28POS%29%20tagging%20task%20and%20introducing%20the%20first%20publicly%20available%0AUPOS-tagged%20benchmark%20dataset%20for%20Uzbek.%20Our%20fine-tuned%20models%20achieve%2091%25%0Aaverage%20accuracy%2C%20outperforming%20the%20baseline%20multi-lingual%20BERT%20as%20well%20as%20the%0Arule-based%20tagger.%20Notably%2C%20these%20models%20capture%20intermediate%20POS%20changes%0Athrough%20affixes%20and%20demonstrate%20context%20sensitivity%2C%20unlike%20existing%20rule-based%0Ataggers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10107v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBBPOS%253A%2520BERT-based%2520Part-of-Speech%2520Tagging%2520for%2520Uzbek%26entry.906535625%3DLatofat%2520Bobojonova%2520and%2520Arofat%2520Akhundjanova%2520and%2520Phil%2520Ostheimer%2520and%2520Sophie%2520Fellenz%26entry.1292438233%3D%2520%2520This%2520paper%2520advances%2520NLP%2520research%2520for%2520the%2520low-resource%2520Uzbek%2520language%2520by%250Aevaluating%2520two%2520previously%2520untested%2520monolingual%2520Uzbek%2520BERT%2520models%2520on%2520the%250Apart-of-speech%2520%2528POS%2529%2520tagging%2520task%2520and%2520introducing%2520the%2520first%2520publicly%2520available%250AUPOS-tagged%2520benchmark%2520dataset%2520for%2520Uzbek.%2520Our%2520fine-tuned%2520models%2520achieve%252091%2525%250Aaverage%2520accuracy%252C%2520outperforming%2520the%2520baseline%2520multi-lingual%2520BERT%2520as%2520well%2520as%2520the%250Arule-based%2520tagger.%2520Notably%252C%2520these%2520models%2520capture%2520intermediate%2520POS%2520changes%250Athrough%2520affixes%2520and%2520demonstrate%2520context%2520sensitivity%252C%2520unlike%2520existing%2520rule-based%250Ataggers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10107v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BBPOS%3A%20BERT-based%20Part-of-Speech%20Tagging%20for%20Uzbek&entry.906535625=Latofat%20Bobojonova%20and%20Arofat%20Akhundjanova%20and%20Phil%20Ostheimer%20and%20Sophie%20Fellenz&entry.1292438233=%20%20This%20paper%20advances%20NLP%20research%20for%20the%20low-resource%20Uzbek%20language%20by%0Aevaluating%20two%20previously%20untested%20monolingual%20Uzbek%20BERT%20models%20on%20the%0Apart-of-speech%20%28POS%29%20tagging%20task%20and%20introducing%20the%20first%20publicly%20available%0AUPOS-tagged%20benchmark%20dataset%20for%20Uzbek.%20Our%20fine-tuned%20models%20achieve%2091%25%0Aaverage%20accuracy%2C%20outperforming%20the%20baseline%20multi-lingual%20BERT%20as%20well%20as%20the%0Arule-based%20tagger.%20Notably%2C%20these%20models%20capture%20intermediate%20POS%20changes%0Athrough%20affixes%20and%20demonstrate%20context%20sensitivity%2C%20unlike%20existing%20rule-based%0Ataggers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10107v1&entry.124074799=Read"},
{"title": "Annealed Multiple Choice Learning: Overcoming limitations of\n  Winner-takes-all with annealing", "author": "David Perera and Victor Letzelter and Th\u00e9o Mariotte and Adrien Cort\u00e9s and Mickael Chen and Slim Essid and Ga\u00ebl Richard", "abstract": "  We introduce Annealed Multiple Choice Learning (aMCL) which combines\nsimulated annealing with MCL. MCL is a learning framework handling ambiguous\ntasks by predicting a small set of plausible hypotheses. These hypotheses are\ntrained using the Winner-takes-all (WTA) scheme, which promotes the diversity\nof the predictions. However, this scheme may converge toward an arbitrarily\nsuboptimal local minimum, due to the greedy nature of WTA. We overcome this\nlimitation using annealing, which enhances the exploration of the hypothesis\nspace during training. We leverage insights from statistical physics and\ninformation theory to provide a detailed description of the model training\ntrajectory. Additionally, we validate our algorithm by extensive experiments on\nsynthetic datasets, on the standard UCI benchmark, and on speech separation.\n", "link": "http://arxiv.org/abs/2407.15580v3", "date": "2025-01-17", "relevancy": 1.4397, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4892}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4735}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Annealed%20Multiple%20Choice%20Learning%3A%20Overcoming%20limitations%20of%0A%20%20Winner-takes-all%20with%20annealing&body=Title%3A%20Annealed%20Multiple%20Choice%20Learning%3A%20Overcoming%20limitations%20of%0A%20%20Winner-takes-all%20with%20annealing%0AAuthor%3A%20David%20Perera%20and%20Victor%20Letzelter%20and%20Th%C3%A9o%20Mariotte%20and%20Adrien%20Cort%C3%A9s%20and%20Mickael%20Chen%20and%20Slim%20Essid%20and%20Ga%C3%ABl%20Richard%0AAbstract%3A%20%20%20We%20introduce%20Annealed%20Multiple%20Choice%20Learning%20%28aMCL%29%20which%20combines%0Asimulated%20annealing%20with%20MCL.%20MCL%20is%20a%20learning%20framework%20handling%20ambiguous%0Atasks%20by%20predicting%20a%20small%20set%20of%20plausible%20hypotheses.%20These%20hypotheses%20are%0Atrained%20using%20the%20Winner-takes-all%20%28WTA%29%20scheme%2C%20which%20promotes%20the%20diversity%0Aof%20the%20predictions.%20However%2C%20this%20scheme%20may%20converge%20toward%20an%20arbitrarily%0Asuboptimal%20local%20minimum%2C%20due%20to%20the%20greedy%20nature%20of%20WTA.%20We%20overcome%20this%0Alimitation%20using%20annealing%2C%20which%20enhances%20the%20exploration%20of%20the%20hypothesis%0Aspace%20during%20training.%20We%20leverage%20insights%20from%20statistical%20physics%20and%0Ainformation%20theory%20to%20provide%20a%20detailed%20description%20of%20the%20model%20training%0Atrajectory.%20Additionally%2C%20we%20validate%20our%20algorithm%20by%20extensive%20experiments%20on%0Asynthetic%20datasets%2C%20on%20the%20standard%20UCI%20benchmark%2C%20and%20on%20speech%20separation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15580v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnnealed%2520Multiple%2520Choice%2520Learning%253A%2520Overcoming%2520limitations%2520of%250A%2520%2520Winner-takes-all%2520with%2520annealing%26entry.906535625%3DDavid%2520Perera%2520and%2520Victor%2520Letzelter%2520and%2520Th%25C3%25A9o%2520Mariotte%2520and%2520Adrien%2520Cort%25C3%25A9s%2520and%2520Mickael%2520Chen%2520and%2520Slim%2520Essid%2520and%2520Ga%25C3%25ABl%2520Richard%26entry.1292438233%3D%2520%2520We%2520introduce%2520Annealed%2520Multiple%2520Choice%2520Learning%2520%2528aMCL%2529%2520which%2520combines%250Asimulated%2520annealing%2520with%2520MCL.%2520MCL%2520is%2520a%2520learning%2520framework%2520handling%2520ambiguous%250Atasks%2520by%2520predicting%2520a%2520small%2520set%2520of%2520plausible%2520hypotheses.%2520These%2520hypotheses%2520are%250Atrained%2520using%2520the%2520Winner-takes-all%2520%2528WTA%2529%2520scheme%252C%2520which%2520promotes%2520the%2520diversity%250Aof%2520the%2520predictions.%2520However%252C%2520this%2520scheme%2520may%2520converge%2520toward%2520an%2520arbitrarily%250Asuboptimal%2520local%2520minimum%252C%2520due%2520to%2520the%2520greedy%2520nature%2520of%2520WTA.%2520We%2520overcome%2520this%250Alimitation%2520using%2520annealing%252C%2520which%2520enhances%2520the%2520exploration%2520of%2520the%2520hypothesis%250Aspace%2520during%2520training.%2520We%2520leverage%2520insights%2520from%2520statistical%2520physics%2520and%250Ainformation%2520theory%2520to%2520provide%2520a%2520detailed%2520description%2520of%2520the%2520model%2520training%250Atrajectory.%2520Additionally%252C%2520we%2520validate%2520our%2520algorithm%2520by%2520extensive%2520experiments%2520on%250Asynthetic%2520datasets%252C%2520on%2520the%2520standard%2520UCI%2520benchmark%252C%2520and%2520on%2520speech%2520separation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15580v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Annealed%20Multiple%20Choice%20Learning%3A%20Overcoming%20limitations%20of%0A%20%20Winner-takes-all%20with%20annealing&entry.906535625=David%20Perera%20and%20Victor%20Letzelter%20and%20Th%C3%A9o%20Mariotte%20and%20Adrien%20Cort%C3%A9s%20and%20Mickael%20Chen%20and%20Slim%20Essid%20and%20Ga%C3%ABl%20Richard&entry.1292438233=%20%20We%20introduce%20Annealed%20Multiple%20Choice%20Learning%20%28aMCL%29%20which%20combines%0Asimulated%20annealing%20with%20MCL.%20MCL%20is%20a%20learning%20framework%20handling%20ambiguous%0Atasks%20by%20predicting%20a%20small%20set%20of%20plausible%20hypotheses.%20These%20hypotheses%20are%0Atrained%20using%20the%20Winner-takes-all%20%28WTA%29%20scheme%2C%20which%20promotes%20the%20diversity%0Aof%20the%20predictions.%20However%2C%20this%20scheme%20may%20converge%20toward%20an%20arbitrarily%0Asuboptimal%20local%20minimum%2C%20due%20to%20the%20greedy%20nature%20of%20WTA.%20We%20overcome%20this%0Alimitation%20using%20annealing%2C%20which%20enhances%20the%20exploration%20of%20the%20hypothesis%0Aspace%20during%20training.%20We%20leverage%20insights%20from%20statistical%20physics%20and%0Ainformation%20theory%20to%20provide%20a%20detailed%20description%20of%20the%20model%20training%0Atrajectory.%20Additionally%2C%20we%20validate%20our%20algorithm%20by%20extensive%20experiments%20on%0Asynthetic%20datasets%2C%20on%20the%20standard%20UCI%20benchmark%2C%20and%20on%20speech%20separation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15580v3&entry.124074799=Read"},
{"title": "The Relevance of AWS Chronos: An Evaluation of Standard Methods for Time\n  Series Forecasting with Limited Tuning", "author": "Matthew Baron and Alex Karpinski", "abstract": "  A systematic comparison of Chronos, a transformer-based time series\nforecasting framework, against traditional approaches including ARIMA and\nProphet. We evaluate these models across multiple time horizons and user\ncategories, with a focus on the impact of historical context length. Our\nanalysis reveals that while Chronos demonstrates superior performance for\nlonger-term predictions and maintains accuracy with increased context,\ntraditional models show significant degradation as context length increases. We\nfind that prediction quality varies systematically between user classes,\nsuggesting that underlying behavior patterns always influence model\nperformance. This study provides a case for deploying Chronos in real-world\napplications where limited model tuning is feasible, especially in scenarios\nrequiring longer prediction.\n", "link": "http://arxiv.org/abs/2501.10216v1", "date": "2025-01-17", "relevancy": 1.5925, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4046}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3968}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Relevance%20of%20AWS%20Chronos%3A%20An%20Evaluation%20of%20Standard%20Methods%20for%20Time%0A%20%20Series%20Forecasting%20with%20Limited%20Tuning&body=Title%3A%20The%20Relevance%20of%20AWS%20Chronos%3A%20An%20Evaluation%20of%20Standard%20Methods%20for%20Time%0A%20%20Series%20Forecasting%20with%20Limited%20Tuning%0AAuthor%3A%20Matthew%20Baron%20and%20Alex%20Karpinski%0AAbstract%3A%20%20%20A%20systematic%20comparison%20of%20Chronos%2C%20a%20transformer-based%20time%20series%0Aforecasting%20framework%2C%20against%20traditional%20approaches%20including%20ARIMA%20and%0AProphet.%20We%20evaluate%20these%20models%20across%20multiple%20time%20horizons%20and%20user%0Acategories%2C%20with%20a%20focus%20on%20the%20impact%20of%20historical%20context%20length.%20Our%0Aanalysis%20reveals%20that%20while%20Chronos%20demonstrates%20superior%20performance%20for%0Alonger-term%20predictions%20and%20maintains%20accuracy%20with%20increased%20context%2C%0Atraditional%20models%20show%20significant%20degradation%20as%20context%20length%20increases.%20We%0Afind%20that%20prediction%20quality%20varies%20systematically%20between%20user%20classes%2C%0Asuggesting%20that%20underlying%20behavior%20patterns%20always%20influence%20model%0Aperformance.%20This%20study%20provides%20a%20case%20for%20deploying%20Chronos%20in%20real-world%0Aapplications%20where%20limited%20model%20tuning%20is%20feasible%2C%20especially%20in%20scenarios%0Arequiring%20longer%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10216v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Relevance%2520of%2520AWS%2520Chronos%253A%2520An%2520Evaluation%2520of%2520Standard%2520Methods%2520for%2520Time%250A%2520%2520Series%2520Forecasting%2520with%2520Limited%2520Tuning%26entry.906535625%3DMatthew%2520Baron%2520and%2520Alex%2520Karpinski%26entry.1292438233%3D%2520%2520A%2520systematic%2520comparison%2520of%2520Chronos%252C%2520a%2520transformer-based%2520time%2520series%250Aforecasting%2520framework%252C%2520against%2520traditional%2520approaches%2520including%2520ARIMA%2520and%250AProphet.%2520We%2520evaluate%2520these%2520models%2520across%2520multiple%2520time%2520horizons%2520and%2520user%250Acategories%252C%2520with%2520a%2520focus%2520on%2520the%2520impact%2520of%2520historical%2520context%2520length.%2520Our%250Aanalysis%2520reveals%2520that%2520while%2520Chronos%2520demonstrates%2520superior%2520performance%2520for%250Alonger-term%2520predictions%2520and%2520maintains%2520accuracy%2520with%2520increased%2520context%252C%250Atraditional%2520models%2520show%2520significant%2520degradation%2520as%2520context%2520length%2520increases.%2520We%250Afind%2520that%2520prediction%2520quality%2520varies%2520systematically%2520between%2520user%2520classes%252C%250Asuggesting%2520that%2520underlying%2520behavior%2520patterns%2520always%2520influence%2520model%250Aperformance.%2520This%2520study%2520provides%2520a%2520case%2520for%2520deploying%2520Chronos%2520in%2520real-world%250Aapplications%2520where%2520limited%2520model%2520tuning%2520is%2520feasible%252C%2520especially%2520in%2520scenarios%250Arequiring%2520longer%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10216v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Relevance%20of%20AWS%20Chronos%3A%20An%20Evaluation%20of%20Standard%20Methods%20for%20Time%0A%20%20Series%20Forecasting%20with%20Limited%20Tuning&entry.906535625=Matthew%20Baron%20and%20Alex%20Karpinski&entry.1292438233=%20%20A%20systematic%20comparison%20of%20Chronos%2C%20a%20transformer-based%20time%20series%0Aforecasting%20framework%2C%20against%20traditional%20approaches%20including%20ARIMA%20and%0AProphet.%20We%20evaluate%20these%20models%20across%20multiple%20time%20horizons%20and%20user%0Acategories%2C%20with%20a%20focus%20on%20the%20impact%20of%20historical%20context%20length.%20Our%0Aanalysis%20reveals%20that%20while%20Chronos%20demonstrates%20superior%20performance%20for%0Alonger-term%20predictions%20and%20maintains%20accuracy%20with%20increased%20context%2C%0Atraditional%20models%20show%20significant%20degradation%20as%20context%20length%20increases.%20We%0Afind%20that%20prediction%20quality%20varies%20systematically%20between%20user%20classes%2C%0Asuggesting%20that%20underlying%20behavior%20patterns%20always%20influence%20model%0Aperformance.%20This%20study%20provides%20a%20case%20for%20deploying%20Chronos%20in%20real-world%0Aapplications%20where%20limited%20model%20tuning%20is%20feasible%2C%20especially%20in%20scenarios%0Arequiring%20longer%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10216v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


