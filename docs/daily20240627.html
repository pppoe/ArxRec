<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240626.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "VDG: Vision-Only Dynamic Gaussian for Driving Simulation", "author": "Hao Li and Jingfeng Li and Dingwen Zhang and Chenming Wu and Jieqi Shi and Chen Zhao and Haocheng Feng and Errui Ding and Jingdong Wang and Junwei Han", "abstract": "  Dynamic Gaussian splatting has led to impressive scene reconstruction and\nimage synthesis advances in novel views. Existing methods, however, heavily\nrely on pre-computed poses and Gaussian initialization by Structure from Motion\n(SfM) algorithms or expensive sensors. For the first time, this paper addresses\nthis issue by integrating self-supervised VO into our pose-free dynamic\nGaussian method (VDG) to boost pose and depth initialization and static-dynamic\ndecomposition. Moreover, VDG can work with only RGB image input and construct\ndynamic scenes at a faster speed and larger scenes compared with the pose-free\ndynamic view-synthesis method. We demonstrate the robustness of our approach\nvia extensive quantitative and qualitative experiments. Our results show\nfavorable performance over the state-of-the-art dynamic view synthesis methods.\nAdditional video and source code will be posted on our project page at\nhttps://3d-aigc.github.io/VDG.\n", "link": "http://arxiv.org/abs/2406.18198v1", "date": "2024-06-26", "relevancy": 3.3443, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7225}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6768}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VDG%3A%20Vision-Only%20Dynamic%20Gaussian%20for%20Driving%20Simulation&body=Title%3A%20VDG%3A%20Vision-Only%20Dynamic%20Gaussian%20for%20Driving%20Simulation%0AAuthor%3A%20Hao%20Li%20and%20Jingfeng%20Li%20and%20Dingwen%20Zhang%20and%20Chenming%20Wu%20and%20Jieqi%20Shi%20and%20Chen%20Zhao%20and%20Haocheng%20Feng%20and%20Errui%20Ding%20and%20Jingdong%20Wang%20and%20Junwei%20Han%0AAbstract%3A%20%20%20Dynamic%20Gaussian%20splatting%20has%20led%20to%20impressive%20scene%20reconstruction%20and%0Aimage%20synthesis%20advances%20in%20novel%20views.%20Existing%20methods%2C%20however%2C%20heavily%0Arely%20on%20pre-computed%20poses%20and%20Gaussian%20initialization%20by%20Structure%20from%20Motion%0A%28SfM%29%20algorithms%20or%20expensive%20sensors.%20For%20the%20first%20time%2C%20this%20paper%20addresses%0Athis%20issue%20by%20integrating%20self-supervised%20VO%20into%20our%20pose-free%20dynamic%0AGaussian%20method%20%28VDG%29%20to%20boost%20pose%20and%20depth%20initialization%20and%20static-dynamic%0Adecomposition.%20Moreover%2C%20VDG%20can%20work%20with%20only%20RGB%20image%20input%20and%20construct%0Adynamic%20scenes%20at%20a%20faster%20speed%20and%20larger%20scenes%20compared%20with%20the%20pose-free%0Adynamic%20view-synthesis%20method.%20We%20demonstrate%20the%20robustness%20of%20our%20approach%0Avia%20extensive%20quantitative%20and%20qualitative%20experiments.%20Our%20results%20show%0Afavorable%20performance%20over%20the%20state-of-the-art%20dynamic%20view%20synthesis%20methods.%0AAdditional%20video%20and%20source%20code%20will%20be%20posted%20on%20our%20project%20page%20at%0Ahttps%3A//3d-aigc.github.io/VDG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18198v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVDG%253A%2520Vision-Only%2520Dynamic%2520Gaussian%2520for%2520Driving%2520Simulation%26entry.906535625%3DHao%2520Li%2520and%2520Jingfeng%2520Li%2520and%2520Dingwen%2520Zhang%2520and%2520Chenming%2520Wu%2520and%2520Jieqi%2520Shi%2520and%2520Chen%2520Zhao%2520and%2520Haocheng%2520Feng%2520and%2520Errui%2520Ding%2520and%2520Jingdong%2520Wang%2520and%2520Junwei%2520Han%26entry.1292438233%3D%2520%2520Dynamic%2520Gaussian%2520splatting%2520has%2520led%2520to%2520impressive%2520scene%2520reconstruction%2520and%250Aimage%2520synthesis%2520advances%2520in%2520novel%2520views.%2520Existing%2520methods%252C%2520however%252C%2520heavily%250Arely%2520on%2520pre-computed%2520poses%2520and%2520Gaussian%2520initialization%2520by%2520Structure%2520from%2520Motion%250A%2528SfM%2529%2520algorithms%2520or%2520expensive%2520sensors.%2520For%2520the%2520first%2520time%252C%2520this%2520paper%2520addresses%250Athis%2520issue%2520by%2520integrating%2520self-supervised%2520VO%2520into%2520our%2520pose-free%2520dynamic%250AGaussian%2520method%2520%2528VDG%2529%2520to%2520boost%2520pose%2520and%2520depth%2520initialization%2520and%2520static-dynamic%250Adecomposition.%2520Moreover%252C%2520VDG%2520can%2520work%2520with%2520only%2520RGB%2520image%2520input%2520and%2520construct%250Adynamic%2520scenes%2520at%2520a%2520faster%2520speed%2520and%2520larger%2520scenes%2520compared%2520with%2520the%2520pose-free%250Adynamic%2520view-synthesis%2520method.%2520We%2520demonstrate%2520the%2520robustness%2520of%2520our%2520approach%250Avia%2520extensive%2520quantitative%2520and%2520qualitative%2520experiments.%2520Our%2520results%2520show%250Afavorable%2520performance%2520over%2520the%2520state-of-the-art%2520dynamic%2520view%2520synthesis%2520methods.%250AAdditional%2520video%2520and%2520source%2520code%2520will%2520be%2520posted%2520on%2520our%2520project%2520page%2520at%250Ahttps%253A//3d-aigc.github.io/VDG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18198v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VDG%3A%20Vision-Only%20Dynamic%20Gaussian%20for%20Driving%20Simulation&entry.906535625=Hao%20Li%20and%20Jingfeng%20Li%20and%20Dingwen%20Zhang%20and%20Chenming%20Wu%20and%20Jieqi%20Shi%20and%20Chen%20Zhao%20and%20Haocheng%20Feng%20and%20Errui%20Ding%20and%20Jingdong%20Wang%20and%20Junwei%20Han&entry.1292438233=%20%20Dynamic%20Gaussian%20splatting%20has%20led%20to%20impressive%20scene%20reconstruction%20and%0Aimage%20synthesis%20advances%20in%20novel%20views.%20Existing%20methods%2C%20however%2C%20heavily%0Arely%20on%20pre-computed%20poses%20and%20Gaussian%20initialization%20by%20Structure%20from%20Motion%0A%28SfM%29%20algorithms%20or%20expensive%20sensors.%20For%20the%20first%20time%2C%20this%20paper%20addresses%0Athis%20issue%20by%20integrating%20self-supervised%20VO%20into%20our%20pose-free%20dynamic%0AGaussian%20method%20%28VDG%29%20to%20boost%20pose%20and%20depth%20initialization%20and%20static-dynamic%0Adecomposition.%20Moreover%2C%20VDG%20can%20work%20with%20only%20RGB%20image%20input%20and%20construct%0Adynamic%20scenes%20at%20a%20faster%20speed%20and%20larger%20scenes%20compared%20with%20the%20pose-free%0Adynamic%20view-synthesis%20method.%20We%20demonstrate%20the%20robustness%20of%20our%20approach%0Avia%20extensive%20quantitative%20and%20qualitative%20experiments.%20Our%20results%20show%0Afavorable%20performance%20over%20the%20state-of-the-art%20dynamic%20view%20synthesis%20methods.%0AAdditional%20video%20and%20source%20code%20will%20be%20posted%20on%20our%20project%20page%20at%0Ahttps%3A//3d-aigc.github.io/VDG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18198v1&entry.124074799=Read"},
{"title": "GaussianDreamerPro: Text to Manipulable 3D Gaussians with Highly\n  Enhanced Quality", "author": "Taoran Yi and Jiemin Fang and Zanwei Zhou and Junjie Wang and Guanjun Wu and Lingxi Xie and Xiaopeng Zhang and Wenyu Liu and Xinggang Wang and Qi Tian", "abstract": "  Recently, 3D Gaussian splatting (3D-GS) has achieved great success in\nreconstructing and rendering real-world scenes. To transfer the high rendering\nquality to generation tasks, a series of research works attempt to generate\n3D-Gaussian assets from text. However, the generated assets have not achieved\nthe same quality as those in reconstruction tasks. We observe that Gaussians\ntend to grow without control as the generation process may cause indeterminacy.\nAiming at highly enhancing the generation quality, we propose a novel framework\nnamed GaussianDreamerPro. The main idea is to bind Gaussians to reasonable\ngeometry, which evolves over the whole generation process. Along different\nstages of our framework, both the geometry and appearance can be enriched\nprogressively. The final output asset is constructed with 3D Gaussians bound to\nmesh, which shows significantly enhanced details and quality compared with\nprevious methods. Notably, the generated asset can also be seamlessly\nintegrated into downstream manipulation pipelines, e.g. animation, composition,\nand simulation etc., greatly promoting its potential in wide applications.\nDemos are available at https://taoranyi.com/gaussiandreamerpro/.\n", "link": "http://arxiv.org/abs/2406.18462v1", "date": "2024-06-26", "relevancy": 3.2991, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.702}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6649}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6125}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaussianDreamerPro%3A%20Text%20to%20Manipulable%203D%20Gaussians%20with%20Highly%0A%20%20Enhanced%20Quality&body=Title%3A%20GaussianDreamerPro%3A%20Text%20to%20Manipulable%203D%20Gaussians%20with%20Highly%0A%20%20Enhanced%20Quality%0AAuthor%3A%20Taoran%20Yi%20and%20Jiemin%20Fang%20and%20Zanwei%20Zhou%20and%20Junjie%20Wang%20and%20Guanjun%20Wu%20and%20Lingxi%20Xie%20and%20Xiaopeng%20Zhang%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang%20and%20Qi%20Tian%0AAbstract%3A%20%20%20Recently%2C%203D%20Gaussian%20splatting%20%283D-GS%29%20has%20achieved%20great%20success%20in%0Areconstructing%20and%20rendering%20real-world%20scenes.%20To%20transfer%20the%20high%20rendering%0Aquality%20to%20generation%20tasks%2C%20a%20series%20of%20research%20works%20attempt%20to%20generate%0A3D-Gaussian%20assets%20from%20text.%20However%2C%20the%20generated%20assets%20have%20not%20achieved%0Athe%20same%20quality%20as%20those%20in%20reconstruction%20tasks.%20We%20observe%20that%20Gaussians%0Atend%20to%20grow%20without%20control%20as%20the%20generation%20process%20may%20cause%20indeterminacy.%0AAiming%20at%20highly%20enhancing%20the%20generation%20quality%2C%20we%20propose%20a%20novel%20framework%0Anamed%20GaussianDreamerPro.%20The%20main%20idea%20is%20to%20bind%20Gaussians%20to%20reasonable%0Ageometry%2C%20which%20evolves%20over%20the%20whole%20generation%20process.%20Along%20different%0Astages%20of%20our%20framework%2C%20both%20the%20geometry%20and%20appearance%20can%20be%20enriched%0Aprogressively.%20The%20final%20output%20asset%20is%20constructed%20with%203D%20Gaussians%20bound%20to%0Amesh%2C%20which%20shows%20significantly%20enhanced%20details%20and%20quality%20compared%20with%0Aprevious%20methods.%20Notably%2C%20the%20generated%20asset%20can%20also%20be%20seamlessly%0Aintegrated%20into%20downstream%20manipulation%20pipelines%2C%20e.g.%20animation%2C%20composition%2C%0Aand%20simulation%20etc.%2C%20greatly%20promoting%20its%20potential%20in%20wide%20applications.%0ADemos%20are%20available%20at%20https%3A//taoranyi.com/gaussiandreamerpro/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18462v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussianDreamerPro%253A%2520Text%2520to%2520Manipulable%25203D%2520Gaussians%2520with%2520Highly%250A%2520%2520Enhanced%2520Quality%26entry.906535625%3DTaoran%2520Yi%2520and%2520Jiemin%2520Fang%2520and%2520Zanwei%2520Zhou%2520and%2520Junjie%2520Wang%2520and%2520Guanjun%2520Wu%2520and%2520Lingxi%2520Xie%2520and%2520Xiaopeng%2520Zhang%2520and%2520Wenyu%2520Liu%2520and%2520Xinggang%2520Wang%2520and%2520Qi%2520Tian%26entry.1292438233%3D%2520%2520Recently%252C%25203D%2520Gaussian%2520splatting%2520%25283D-GS%2529%2520has%2520achieved%2520great%2520success%2520in%250Areconstructing%2520and%2520rendering%2520real-world%2520scenes.%2520To%2520transfer%2520the%2520high%2520rendering%250Aquality%2520to%2520generation%2520tasks%252C%2520a%2520series%2520of%2520research%2520works%2520attempt%2520to%2520generate%250A3D-Gaussian%2520assets%2520from%2520text.%2520However%252C%2520the%2520generated%2520assets%2520have%2520not%2520achieved%250Athe%2520same%2520quality%2520as%2520those%2520in%2520reconstruction%2520tasks.%2520We%2520observe%2520that%2520Gaussians%250Atend%2520to%2520grow%2520without%2520control%2520as%2520the%2520generation%2520process%2520may%2520cause%2520indeterminacy.%250AAiming%2520at%2520highly%2520enhancing%2520the%2520generation%2520quality%252C%2520we%2520propose%2520a%2520novel%2520framework%250Anamed%2520GaussianDreamerPro.%2520The%2520main%2520idea%2520is%2520to%2520bind%2520Gaussians%2520to%2520reasonable%250Ageometry%252C%2520which%2520evolves%2520over%2520the%2520whole%2520generation%2520process.%2520Along%2520different%250Astages%2520of%2520our%2520framework%252C%2520both%2520the%2520geometry%2520and%2520appearance%2520can%2520be%2520enriched%250Aprogressively.%2520The%2520final%2520output%2520asset%2520is%2520constructed%2520with%25203D%2520Gaussians%2520bound%2520to%250Amesh%252C%2520which%2520shows%2520significantly%2520enhanced%2520details%2520and%2520quality%2520compared%2520with%250Aprevious%2520methods.%2520Notably%252C%2520the%2520generated%2520asset%2520can%2520also%2520be%2520seamlessly%250Aintegrated%2520into%2520downstream%2520manipulation%2520pipelines%252C%2520e.g.%2520animation%252C%2520composition%252C%250Aand%2520simulation%2520etc.%252C%2520greatly%2520promoting%2520its%2520potential%2520in%2520wide%2520applications.%250ADemos%2520are%2520available%2520at%2520https%253A//taoranyi.com/gaussiandreamerpro/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18462v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussianDreamerPro%3A%20Text%20to%20Manipulable%203D%20Gaussians%20with%20Highly%0A%20%20Enhanced%20Quality&entry.906535625=Taoran%20Yi%20and%20Jiemin%20Fang%20and%20Zanwei%20Zhou%20and%20Junjie%20Wang%20and%20Guanjun%20Wu%20and%20Lingxi%20Xie%20and%20Xiaopeng%20Zhang%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang%20and%20Qi%20Tian&entry.1292438233=%20%20Recently%2C%203D%20Gaussian%20splatting%20%283D-GS%29%20has%20achieved%20great%20success%20in%0Areconstructing%20and%20rendering%20real-world%20scenes.%20To%20transfer%20the%20high%20rendering%0Aquality%20to%20generation%20tasks%2C%20a%20series%20of%20research%20works%20attempt%20to%20generate%0A3D-Gaussian%20assets%20from%20text.%20However%2C%20the%20generated%20assets%20have%20not%20achieved%0Athe%20same%20quality%20as%20those%20in%20reconstruction%20tasks.%20We%20observe%20that%20Gaussians%0Atend%20to%20grow%20without%20control%20as%20the%20generation%20process%20may%20cause%20indeterminacy.%0AAiming%20at%20highly%20enhancing%20the%20generation%20quality%2C%20we%20propose%20a%20novel%20framework%0Anamed%20GaussianDreamerPro.%20The%20main%20idea%20is%20to%20bind%20Gaussians%20to%20reasonable%0Ageometry%2C%20which%20evolves%20over%20the%20whole%20generation%20process.%20Along%20different%0Astages%20of%20our%20framework%2C%20both%20the%20geometry%20and%20appearance%20can%20be%20enriched%0Aprogressively.%20The%20final%20output%20asset%20is%20constructed%20with%203D%20Gaussians%20bound%20to%0Amesh%2C%20which%20shows%20significantly%20enhanced%20details%20and%20quality%20compared%20with%0Aprevious%20methods.%20Notably%2C%20the%20generated%20asset%20can%20also%20be%20seamlessly%0Aintegrated%20into%20downstream%20manipulation%20pipelines%2C%20e.g.%20animation%2C%20composition%2C%0Aand%20simulation%20etc.%2C%20greatly%20promoting%20its%20potential%20in%20wide%20applications.%0ADemos%20are%20available%20at%20https%3A//taoranyi.com/gaussiandreamerpro/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18462v1&entry.124074799=Read"},
{"title": "GS-Octree: Octree-based 3D Gaussian Splatting for Robust Object-level 3D\n  Reconstruction Under Strong Lighting", "author": "Jiaze Li and Zhengyu Wen and Luo Zhang and Jiangbei Hu and Fei Hou and Zhebin Zhang and Ying He", "abstract": "  The 3D Gaussian Splatting technique has significantly advanced the\nconstruction of radiance fields from multi-view images, enabling real-time\nrendering. While point-based rasterization effectively reduces computational\ndemands for rendering, it often struggles to accurately reconstruct the\ngeometry of the target object, especially under strong lighting. To address\nthis challenge, we introduce a novel approach that combines octree-based\nimplicit surface representations with Gaussian splatting. Our method consists\nof four stages. Initially, it reconstructs a signed distance field (SDF) and a\nradiance field through volume rendering, encoding them in a low-resolution\noctree. The initial SDF represents the coarse geometry of the target object.\nSubsequently, it introduces 3D Gaussians as additional degrees of freedom,\nwhich are guided by the SDF. In the third stage, the optimized Gaussians\nfurther improve the accuracy of the SDF, allowing it to recover finer geometric\ndetails compared to the initial SDF obtained in the first stage. Finally, it\nadopts the refined SDF to further optimize the 3D Gaussians via splatting,\neliminating those that contribute little to visual appearance. Experimental\nresults show that our method, which leverages the distribution of 3D Gaussians\nwith SDFs, reconstructs more accurate geometry, particularly in images with\nspecular highlights caused by strong lighting.\n", "link": "http://arxiv.org/abs/2406.18199v1", "date": "2024-06-26", "relevancy": 3.2517, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7496}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6472}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GS-Octree%3A%20Octree-based%203D%20Gaussian%20Splatting%20for%20Robust%20Object-level%203D%0A%20%20Reconstruction%20Under%20Strong%20Lighting&body=Title%3A%20GS-Octree%3A%20Octree-based%203D%20Gaussian%20Splatting%20for%20Robust%20Object-level%203D%0A%20%20Reconstruction%20Under%20Strong%20Lighting%0AAuthor%3A%20Jiaze%20Li%20and%20Zhengyu%20Wen%20and%20Luo%20Zhang%20and%20Jiangbei%20Hu%20and%20Fei%20Hou%20and%20Zhebin%20Zhang%20and%20Ying%20He%0AAbstract%3A%20%20%20The%203D%20Gaussian%20Splatting%20technique%20has%20significantly%20advanced%20the%0Aconstruction%20of%20radiance%20fields%20from%20multi-view%20images%2C%20enabling%20real-time%0Arendering.%20While%20point-based%20rasterization%20effectively%20reduces%20computational%0Ademands%20for%20rendering%2C%20it%20often%20struggles%20to%20accurately%20reconstruct%20the%0Ageometry%20of%20the%20target%20object%2C%20especially%20under%20strong%20lighting.%20To%20address%0Athis%20challenge%2C%20we%20introduce%20a%20novel%20approach%20that%20combines%20octree-based%0Aimplicit%20surface%20representations%20with%20Gaussian%20splatting.%20Our%20method%20consists%0Aof%20four%20stages.%20Initially%2C%20it%20reconstructs%20a%20signed%20distance%20field%20%28SDF%29%20and%20a%0Aradiance%20field%20through%20volume%20rendering%2C%20encoding%20them%20in%20a%20low-resolution%0Aoctree.%20The%20initial%20SDF%20represents%20the%20coarse%20geometry%20of%20the%20target%20object.%0ASubsequently%2C%20it%20introduces%203D%20Gaussians%20as%20additional%20degrees%20of%20freedom%2C%0Awhich%20are%20guided%20by%20the%20SDF.%20In%20the%20third%20stage%2C%20the%20optimized%20Gaussians%0Afurther%20improve%20the%20accuracy%20of%20the%20SDF%2C%20allowing%20it%20to%20recover%20finer%20geometric%0Adetails%20compared%20to%20the%20initial%20SDF%20obtained%20in%20the%20first%20stage.%20Finally%2C%20it%0Aadopts%20the%20refined%20SDF%20to%20further%20optimize%20the%203D%20Gaussians%20via%20splatting%2C%0Aeliminating%20those%20that%20contribute%20little%20to%20visual%20appearance.%20Experimental%0Aresults%20show%20that%20our%20method%2C%20which%20leverages%20the%20distribution%20of%203D%20Gaussians%0Awith%20SDFs%2C%20reconstructs%20more%20accurate%20geometry%2C%20particularly%20in%20images%20with%0Aspecular%20highlights%20caused%20by%20strong%20lighting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18199v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGS-Octree%253A%2520Octree-based%25203D%2520Gaussian%2520Splatting%2520for%2520Robust%2520Object-level%25203D%250A%2520%2520Reconstruction%2520Under%2520Strong%2520Lighting%26entry.906535625%3DJiaze%2520Li%2520and%2520Zhengyu%2520Wen%2520and%2520Luo%2520Zhang%2520and%2520Jiangbei%2520Hu%2520and%2520Fei%2520Hou%2520and%2520Zhebin%2520Zhang%2520and%2520Ying%2520He%26entry.1292438233%3D%2520%2520The%25203D%2520Gaussian%2520Splatting%2520technique%2520has%2520significantly%2520advanced%2520the%250Aconstruction%2520of%2520radiance%2520fields%2520from%2520multi-view%2520images%252C%2520enabling%2520real-time%250Arendering.%2520While%2520point-based%2520rasterization%2520effectively%2520reduces%2520computational%250Ademands%2520for%2520rendering%252C%2520it%2520often%2520struggles%2520to%2520accurately%2520reconstruct%2520the%250Ageometry%2520of%2520the%2520target%2520object%252C%2520especially%2520under%2520strong%2520lighting.%2520To%2520address%250Athis%2520challenge%252C%2520we%2520introduce%2520a%2520novel%2520approach%2520that%2520combines%2520octree-based%250Aimplicit%2520surface%2520representations%2520with%2520Gaussian%2520splatting.%2520Our%2520method%2520consists%250Aof%2520four%2520stages.%2520Initially%252C%2520it%2520reconstructs%2520a%2520signed%2520distance%2520field%2520%2528SDF%2529%2520and%2520a%250Aradiance%2520field%2520through%2520volume%2520rendering%252C%2520encoding%2520them%2520in%2520a%2520low-resolution%250Aoctree.%2520The%2520initial%2520SDF%2520represents%2520the%2520coarse%2520geometry%2520of%2520the%2520target%2520object.%250ASubsequently%252C%2520it%2520introduces%25203D%2520Gaussians%2520as%2520additional%2520degrees%2520of%2520freedom%252C%250Awhich%2520are%2520guided%2520by%2520the%2520SDF.%2520In%2520the%2520third%2520stage%252C%2520the%2520optimized%2520Gaussians%250Afurther%2520improve%2520the%2520accuracy%2520of%2520the%2520SDF%252C%2520allowing%2520it%2520to%2520recover%2520finer%2520geometric%250Adetails%2520compared%2520to%2520the%2520initial%2520SDF%2520obtained%2520in%2520the%2520first%2520stage.%2520Finally%252C%2520it%250Aadopts%2520the%2520refined%2520SDF%2520to%2520further%2520optimize%2520the%25203D%2520Gaussians%2520via%2520splatting%252C%250Aeliminating%2520those%2520that%2520contribute%2520little%2520to%2520visual%2520appearance.%2520Experimental%250Aresults%2520show%2520that%2520our%2520method%252C%2520which%2520leverages%2520the%2520distribution%2520of%25203D%2520Gaussians%250Awith%2520SDFs%252C%2520reconstructs%2520more%2520accurate%2520geometry%252C%2520particularly%2520in%2520images%2520with%250Aspecular%2520highlights%2520caused%2520by%2520strong%2520lighting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18199v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GS-Octree%3A%20Octree-based%203D%20Gaussian%20Splatting%20for%20Robust%20Object-level%203D%0A%20%20Reconstruction%20Under%20Strong%20Lighting&entry.906535625=Jiaze%20Li%20and%20Zhengyu%20Wen%20and%20Luo%20Zhang%20and%20Jiangbei%20Hu%20and%20Fei%20Hou%20and%20Zhebin%20Zhang%20and%20Ying%20He&entry.1292438233=%20%20The%203D%20Gaussian%20Splatting%20technique%20has%20significantly%20advanced%20the%0Aconstruction%20of%20radiance%20fields%20from%20multi-view%20images%2C%20enabling%20real-time%0Arendering.%20While%20point-based%20rasterization%20effectively%20reduces%20computational%0Ademands%20for%20rendering%2C%20it%20often%20struggles%20to%20accurately%20reconstruct%20the%0Ageometry%20of%20the%20target%20object%2C%20especially%20under%20strong%20lighting.%20To%20address%0Athis%20challenge%2C%20we%20introduce%20a%20novel%20approach%20that%20combines%20octree-based%0Aimplicit%20surface%20representations%20with%20Gaussian%20splatting.%20Our%20method%20consists%0Aof%20four%20stages.%20Initially%2C%20it%20reconstructs%20a%20signed%20distance%20field%20%28SDF%29%20and%20a%0Aradiance%20field%20through%20volume%20rendering%2C%20encoding%20them%20in%20a%20low-resolution%0Aoctree.%20The%20initial%20SDF%20represents%20the%20coarse%20geometry%20of%20the%20target%20object.%0ASubsequently%2C%20it%20introduces%203D%20Gaussians%20as%20additional%20degrees%20of%20freedom%2C%0Awhich%20are%20guided%20by%20the%20SDF.%20In%20the%20third%20stage%2C%20the%20optimized%20Gaussians%0Afurther%20improve%20the%20accuracy%20of%20the%20SDF%2C%20allowing%20it%20to%20recover%20finer%20geometric%0Adetails%20compared%20to%20the%20initial%20SDF%20obtained%20in%20the%20first%20stage.%20Finally%2C%20it%0Aadopts%20the%20refined%20SDF%20to%20further%20optimize%20the%203D%20Gaussians%20via%20splatting%2C%0Aeliminating%20those%20that%20contribute%20little%20to%20visual%20appearance.%20Experimental%0Aresults%20show%20that%20our%20method%2C%20which%20leverages%20the%20distribution%20of%203D%20Gaussians%0Awith%20SDFs%2C%20reconstructs%20more%20accurate%20geometry%2C%20particularly%20in%20images%20with%0Aspecular%20highlights%20caused%20by%20strong%20lighting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18199v1&entry.124074799=Read"},
{"title": "Human-Aware 3D Scene Generation with Spatially-constrained Diffusion\n  Models", "author": "Xiaolin Hong and Hongwei Yi and Fazhi He and Qiong Cao", "abstract": "  Generating 3D scenes from human motion sequences supports numerous\napplications, including virtual reality and architectural design. However,\nprevious auto-regression-based human-aware 3D scene generation methods have\nstruggled to accurately capture the joint distribution of multiple objects and\ninput humans, often resulting in overlapping object generation in the same\nspace. To address this limitation, we explore the potential of diffusion models\nthat simultaneously consider all input humans and the floor plan to generate\nplausible 3D scenes. Our approach not only satisfies all input human\ninteractions but also adheres to spatial constraints with the floor plan.\nFurthermore, we introduce two spatial collision guidance mechanisms:\nhuman-object collision avoidance and object-room boundary constraints. These\nmechanisms help avoid generating scenes that conflict with human motions while\nrespecting layout constraints. To enhance the diversity and accuracy of\nhuman-guided scene generation, we have developed an automated pipeline that\nimproves the variety and plausibility of human-object interactions in the\nexisting 3D FRONT HUMAN dataset. Extensive experiments on both synthetic and\nreal-world datasets demonstrate that our framework can generate more natural\nand plausible 3D scenes with precise human-scene interactions, while\nsignificantly reducing human-object collisions compared to previous\nstate-of-the-art methods. Our code and data will be made publicly available\nupon publication of this work.\n", "link": "http://arxiv.org/abs/2406.18159v1", "date": "2024-06-26", "relevancy": 3.2072, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6591}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6591}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6061}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human-Aware%203D%20Scene%20Generation%20with%20Spatially-constrained%20Diffusion%0A%20%20Models&body=Title%3A%20Human-Aware%203D%20Scene%20Generation%20with%20Spatially-constrained%20Diffusion%0A%20%20Models%0AAuthor%3A%20Xiaolin%20Hong%20and%20Hongwei%20Yi%20and%20Fazhi%20He%20and%20Qiong%20Cao%0AAbstract%3A%20%20%20Generating%203D%20scenes%20from%20human%20motion%20sequences%20supports%20numerous%0Aapplications%2C%20including%20virtual%20reality%20and%20architectural%20design.%20However%2C%0Aprevious%20auto-regression-based%20human-aware%203D%20scene%20generation%20methods%20have%0Astruggled%20to%20accurately%20capture%20the%20joint%20distribution%20of%20multiple%20objects%20and%0Ainput%20humans%2C%20often%20resulting%20in%20overlapping%20object%20generation%20in%20the%20same%0Aspace.%20To%20address%20this%20limitation%2C%20we%20explore%20the%20potential%20of%20diffusion%20models%0Athat%20simultaneously%20consider%20all%20input%20humans%20and%20the%20floor%20plan%20to%20generate%0Aplausible%203D%20scenes.%20Our%20approach%20not%20only%20satisfies%20all%20input%20human%0Ainteractions%20but%20also%20adheres%20to%20spatial%20constraints%20with%20the%20floor%20plan.%0AFurthermore%2C%20we%20introduce%20two%20spatial%20collision%20guidance%20mechanisms%3A%0Ahuman-object%20collision%20avoidance%20and%20object-room%20boundary%20constraints.%20These%0Amechanisms%20help%20avoid%20generating%20scenes%20that%20conflict%20with%20human%20motions%20while%0Arespecting%20layout%20constraints.%20To%20enhance%20the%20diversity%20and%20accuracy%20of%0Ahuman-guided%20scene%20generation%2C%20we%20have%20developed%20an%20automated%20pipeline%20that%0Aimproves%20the%20variety%20and%20plausibility%20of%20human-object%20interactions%20in%20the%0Aexisting%203D%20FRONT%20HUMAN%20dataset.%20Extensive%20experiments%20on%20both%20synthetic%20and%0Areal-world%20datasets%20demonstrate%20that%20our%20framework%20can%20generate%20more%20natural%0Aand%20plausible%203D%20scenes%20with%20precise%20human-scene%20interactions%2C%20while%0Asignificantly%20reducing%20human-object%20collisions%20compared%20to%20previous%0Astate-of-the-art%20methods.%20Our%20code%20and%20data%20will%20be%20made%20publicly%20available%0Aupon%20publication%20of%20this%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18159v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman-Aware%25203D%2520Scene%2520Generation%2520with%2520Spatially-constrained%2520Diffusion%250A%2520%2520Models%26entry.906535625%3DXiaolin%2520Hong%2520and%2520Hongwei%2520Yi%2520and%2520Fazhi%2520He%2520and%2520Qiong%2520Cao%26entry.1292438233%3D%2520%2520Generating%25203D%2520scenes%2520from%2520human%2520motion%2520sequences%2520supports%2520numerous%250Aapplications%252C%2520including%2520virtual%2520reality%2520and%2520architectural%2520design.%2520However%252C%250Aprevious%2520auto-regression-based%2520human-aware%25203D%2520scene%2520generation%2520methods%2520have%250Astruggled%2520to%2520accurately%2520capture%2520the%2520joint%2520distribution%2520of%2520multiple%2520objects%2520and%250Ainput%2520humans%252C%2520often%2520resulting%2520in%2520overlapping%2520object%2520generation%2520in%2520the%2520same%250Aspace.%2520To%2520address%2520this%2520limitation%252C%2520we%2520explore%2520the%2520potential%2520of%2520diffusion%2520models%250Athat%2520simultaneously%2520consider%2520all%2520input%2520humans%2520and%2520the%2520floor%2520plan%2520to%2520generate%250Aplausible%25203D%2520scenes.%2520Our%2520approach%2520not%2520only%2520satisfies%2520all%2520input%2520human%250Ainteractions%2520but%2520also%2520adheres%2520to%2520spatial%2520constraints%2520with%2520the%2520floor%2520plan.%250AFurthermore%252C%2520we%2520introduce%2520two%2520spatial%2520collision%2520guidance%2520mechanisms%253A%250Ahuman-object%2520collision%2520avoidance%2520and%2520object-room%2520boundary%2520constraints.%2520These%250Amechanisms%2520help%2520avoid%2520generating%2520scenes%2520that%2520conflict%2520with%2520human%2520motions%2520while%250Arespecting%2520layout%2520constraints.%2520To%2520enhance%2520the%2520diversity%2520and%2520accuracy%2520of%250Ahuman-guided%2520scene%2520generation%252C%2520we%2520have%2520developed%2520an%2520automated%2520pipeline%2520that%250Aimproves%2520the%2520variety%2520and%2520plausibility%2520of%2520human-object%2520interactions%2520in%2520the%250Aexisting%25203D%2520FRONT%2520HUMAN%2520dataset.%2520Extensive%2520experiments%2520on%2520both%2520synthetic%2520and%250Areal-world%2520datasets%2520demonstrate%2520that%2520our%2520framework%2520can%2520generate%2520more%2520natural%250Aand%2520plausible%25203D%2520scenes%2520with%2520precise%2520human-scene%2520interactions%252C%2520while%250Asignificantly%2520reducing%2520human-object%2520collisions%2520compared%2520to%2520previous%250Astate-of-the-art%2520methods.%2520Our%2520code%2520and%2520data%2520will%2520be%2520made%2520publicly%2520available%250Aupon%2520publication%2520of%2520this%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18159v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human-Aware%203D%20Scene%20Generation%20with%20Spatially-constrained%20Diffusion%0A%20%20Models&entry.906535625=Xiaolin%20Hong%20and%20Hongwei%20Yi%20and%20Fazhi%20He%20and%20Qiong%20Cao&entry.1292438233=%20%20Generating%203D%20scenes%20from%20human%20motion%20sequences%20supports%20numerous%0Aapplications%2C%20including%20virtual%20reality%20and%20architectural%20design.%20However%2C%0Aprevious%20auto-regression-based%20human-aware%203D%20scene%20generation%20methods%20have%0Astruggled%20to%20accurately%20capture%20the%20joint%20distribution%20of%20multiple%20objects%20and%0Ainput%20humans%2C%20often%20resulting%20in%20overlapping%20object%20generation%20in%20the%20same%0Aspace.%20To%20address%20this%20limitation%2C%20we%20explore%20the%20potential%20of%20diffusion%20models%0Athat%20simultaneously%20consider%20all%20input%20humans%20and%20the%20floor%20plan%20to%20generate%0Aplausible%203D%20scenes.%20Our%20approach%20not%20only%20satisfies%20all%20input%20human%0Ainteractions%20but%20also%20adheres%20to%20spatial%20constraints%20with%20the%20floor%20plan.%0AFurthermore%2C%20we%20introduce%20two%20spatial%20collision%20guidance%20mechanisms%3A%0Ahuman-object%20collision%20avoidance%20and%20object-room%20boundary%20constraints.%20These%0Amechanisms%20help%20avoid%20generating%20scenes%20that%20conflict%20with%20human%20motions%20while%0Arespecting%20layout%20constraints.%20To%20enhance%20the%20diversity%20and%20accuracy%20of%0Ahuman-guided%20scene%20generation%2C%20we%20have%20developed%20an%20automated%20pipeline%20that%0Aimproves%20the%20variety%20and%20plausibility%20of%20human-object%20interactions%20in%20the%0Aexisting%203D%20FRONT%20HUMAN%20dataset.%20Extensive%20experiments%20on%20both%20synthetic%20and%0Areal-world%20datasets%20demonstrate%20that%20our%20framework%20can%20generate%20more%20natural%0Aand%20plausible%203D%20scenes%20with%20precise%20human-scene%20interactions%2C%20while%0Asignificantly%20reducing%20human-object%20collisions%20compared%20to%20previous%0Astate-of-the-art%20methods.%20Our%20code%20and%20data%20will%20be%20made%20publicly%20available%0Aupon%20publication%20of%20this%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18159v1&entry.124074799=Read"},
{"title": "Repeat and Concatenate: 2D to 3D Image Translation with 3D to 3D\n  Generative Modeling", "author": "Abril Corona-Figueroa and Hubert P. H. Shum and Chris G. Willcocks", "abstract": "  This paper investigates a 2D to 3D image translation method with a\nstraightforward technique, enabling correlated 2D X-ray to 3D CT-like\nreconstruction. We observe that existing approaches, which integrate\ninformation across multiple 2D views in the latent space, lose valuable signal\ninformation during latent encoding. Instead, we simply repeat and concatenate\nthe 2D views into higher-channel 3D volumes and approach the 3D reconstruction\nchallenge as a straightforward 3D to 3D generative modeling problem,\nsidestepping several complex modeling issues. This method enables the\nreconstructed 3D volume to retain valuable information from the 2D inputs,\nwhich are passed between channel states in a Swin UNETR backbone. Our approach\napplies neural optimal transport, which is fast and stable to train,\neffectively integrating signal information across multiple views without the\nrequirement for precise alignment; it produces non-collapsed reconstructions\nthat are highly faithful to the 2D views, even after limited training. We\ndemonstrate correlated results, both qualitatively and quantitatively, having\ntrained our model on a single dataset and evaluated its generalization ability\nacross six datasets, including out-of-distribution samples.\n", "link": "http://arxiv.org/abs/2406.18422v1", "date": "2024-06-26", "relevancy": 3.1251, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6375}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6375}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6001}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Repeat%20and%20Concatenate%3A%202D%20to%203D%20Image%20Translation%20with%203D%20to%203D%0A%20%20Generative%20Modeling&body=Title%3A%20Repeat%20and%20Concatenate%3A%202D%20to%203D%20Image%20Translation%20with%203D%20to%203D%0A%20%20Generative%20Modeling%0AAuthor%3A%20Abril%20Corona-Figueroa%20and%20Hubert%20P.%20H.%20Shum%20and%20Chris%20G.%20Willcocks%0AAbstract%3A%20%20%20This%20paper%20investigates%20a%202D%20to%203D%20image%20translation%20method%20with%20a%0Astraightforward%20technique%2C%20enabling%20correlated%202D%20X-ray%20to%203D%20CT-like%0Areconstruction.%20We%20observe%20that%20existing%20approaches%2C%20which%20integrate%0Ainformation%20across%20multiple%202D%20views%20in%20the%20latent%20space%2C%20lose%20valuable%20signal%0Ainformation%20during%20latent%20encoding.%20Instead%2C%20we%20simply%20repeat%20and%20concatenate%0Athe%202D%20views%20into%20higher-channel%203D%20volumes%20and%20approach%20the%203D%20reconstruction%0Achallenge%20as%20a%20straightforward%203D%20to%203D%20generative%20modeling%20problem%2C%0Asidestepping%20several%20complex%20modeling%20issues.%20This%20method%20enables%20the%0Areconstructed%203D%20volume%20to%20retain%20valuable%20information%20from%20the%202D%20inputs%2C%0Awhich%20are%20passed%20between%20channel%20states%20in%20a%20Swin%20UNETR%20backbone.%20Our%20approach%0Aapplies%20neural%20optimal%20transport%2C%20which%20is%20fast%20and%20stable%20to%20train%2C%0Aeffectively%20integrating%20signal%20information%20across%20multiple%20views%20without%20the%0Arequirement%20for%20precise%20alignment%3B%20it%20produces%20non-collapsed%20reconstructions%0Athat%20are%20highly%20faithful%20to%20the%202D%20views%2C%20even%20after%20limited%20training.%20We%0Ademonstrate%20correlated%20results%2C%20both%20qualitatively%20and%20quantitatively%2C%20having%0Atrained%20our%20model%20on%20a%20single%20dataset%20and%20evaluated%20its%20generalization%20ability%0Aacross%20six%20datasets%2C%20including%20out-of-distribution%20samples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18422v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepeat%2520and%2520Concatenate%253A%25202D%2520to%25203D%2520Image%2520Translation%2520with%25203D%2520to%25203D%250A%2520%2520Generative%2520Modeling%26entry.906535625%3DAbril%2520Corona-Figueroa%2520and%2520Hubert%2520P.%2520H.%2520Shum%2520and%2520Chris%2520G.%2520Willcocks%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520a%25202D%2520to%25203D%2520image%2520translation%2520method%2520with%2520a%250Astraightforward%2520technique%252C%2520enabling%2520correlated%25202D%2520X-ray%2520to%25203D%2520CT-like%250Areconstruction.%2520We%2520observe%2520that%2520existing%2520approaches%252C%2520which%2520integrate%250Ainformation%2520across%2520multiple%25202D%2520views%2520in%2520the%2520latent%2520space%252C%2520lose%2520valuable%2520signal%250Ainformation%2520during%2520latent%2520encoding.%2520Instead%252C%2520we%2520simply%2520repeat%2520and%2520concatenate%250Athe%25202D%2520views%2520into%2520higher-channel%25203D%2520volumes%2520and%2520approach%2520the%25203D%2520reconstruction%250Achallenge%2520as%2520a%2520straightforward%25203D%2520to%25203D%2520generative%2520modeling%2520problem%252C%250Asidestepping%2520several%2520complex%2520modeling%2520issues.%2520This%2520method%2520enables%2520the%250Areconstructed%25203D%2520volume%2520to%2520retain%2520valuable%2520information%2520from%2520the%25202D%2520inputs%252C%250Awhich%2520are%2520passed%2520between%2520channel%2520states%2520in%2520a%2520Swin%2520UNETR%2520backbone.%2520Our%2520approach%250Aapplies%2520neural%2520optimal%2520transport%252C%2520which%2520is%2520fast%2520and%2520stable%2520to%2520train%252C%250Aeffectively%2520integrating%2520signal%2520information%2520across%2520multiple%2520views%2520without%2520the%250Arequirement%2520for%2520precise%2520alignment%253B%2520it%2520produces%2520non-collapsed%2520reconstructions%250Athat%2520are%2520highly%2520faithful%2520to%2520the%25202D%2520views%252C%2520even%2520after%2520limited%2520training.%2520We%250Ademonstrate%2520correlated%2520results%252C%2520both%2520qualitatively%2520and%2520quantitatively%252C%2520having%250Atrained%2520our%2520model%2520on%2520a%2520single%2520dataset%2520and%2520evaluated%2520its%2520generalization%2520ability%250Aacross%2520six%2520datasets%252C%2520including%2520out-of-distribution%2520samples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18422v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Repeat%20and%20Concatenate%3A%202D%20to%203D%20Image%20Translation%20with%203D%20to%203D%0A%20%20Generative%20Modeling&entry.906535625=Abril%20Corona-Figueroa%20and%20Hubert%20P.%20H.%20Shum%20and%20Chris%20G.%20Willcocks&entry.1292438233=%20%20This%20paper%20investigates%20a%202D%20to%203D%20image%20translation%20method%20with%20a%0Astraightforward%20technique%2C%20enabling%20correlated%202D%20X-ray%20to%203D%20CT-like%0Areconstruction.%20We%20observe%20that%20existing%20approaches%2C%20which%20integrate%0Ainformation%20across%20multiple%202D%20views%20in%20the%20latent%20space%2C%20lose%20valuable%20signal%0Ainformation%20during%20latent%20encoding.%20Instead%2C%20we%20simply%20repeat%20and%20concatenate%0Athe%202D%20views%20into%20higher-channel%203D%20volumes%20and%20approach%20the%203D%20reconstruction%0Achallenge%20as%20a%20straightforward%203D%20to%203D%20generative%20modeling%20problem%2C%0Asidestepping%20several%20complex%20modeling%20issues.%20This%20method%20enables%20the%0Areconstructed%203D%20volume%20to%20retain%20valuable%20information%20from%20the%202D%20inputs%2C%0Awhich%20are%20passed%20between%20channel%20states%20in%20a%20Swin%20UNETR%20backbone.%20Our%20approach%0Aapplies%20neural%20optimal%20transport%2C%20which%20is%20fast%20and%20stable%20to%20train%2C%0Aeffectively%20integrating%20signal%20information%20across%20multiple%20views%20without%20the%0Arequirement%20for%20precise%20alignment%3B%20it%20produces%20non-collapsed%20reconstructions%0Athat%20are%20highly%20faithful%20to%20the%202D%20views%2C%20even%20after%20limited%20training.%20We%0Ademonstrate%20correlated%20results%2C%20both%20qualitatively%20and%20quantitatively%2C%20having%0Atrained%20our%20model%20on%20a%20single%20dataset%20and%20evaluated%20its%20generalization%20ability%0Aacross%20six%20datasets%2C%20including%20out-of-distribution%20samples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18422v1&entry.124074799=Read"},
{"title": "RealTalk: Real-time and Realistic Audio-driven Face Generation with 3D\n  Facial Prior-guided Identity Alignment Network", "author": "Xiaozhong Ji and Chuming Lin and Zhonggan Ding and Ying Tai and Jian Yang and Junwei Zhu and Xiaobin Hu and Jiangning Zhang and Donghao Luo and Chengjie Wang", "abstract": "  Person-generic audio-driven face generation is a challenging task in computer\nvision. Previous methods have achieved remarkable progress in audio-visual\nsynchronization, but there is still a significant gap between current results\nand practical applications. The challenges are two-fold: 1) Preserving unique\nindividual traits for achieving high-precision lip synchronization. 2)\nGenerating high-quality facial renderings in real-time performance. In this\npaper, we propose a novel generalized audio-driven framework RealTalk, which\nconsists of an audio-to-expression transformer and a high-fidelity\nexpression-to-face renderer. In the first component, we consider both identity\nand intra-personal variation features related to speaking lip movements. By\nincorporating cross-modal attention on the enriched facial priors, we can\neffectively align lip movements with audio, thus attaining greater precision in\nexpression prediction. In the second component, we design a lightweight facial\nidentity alignment (FIA) module which includes a lip-shape control structure\nand a face texture reference structure. This novel design allows us to generate\nfine details in real-time, without depending on sophisticated and inefficient\nfeature alignment modules. Our experimental results, both quantitative and\nqualitative, on public datasets demonstrate the clear advantages of our method\nin terms of lip-speech synchronization and generation quality. Furthermore, our\nmethod is efficient and requires fewer computational resources, making it\nwell-suited to meet the needs of practical applications.\n", "link": "http://arxiv.org/abs/2406.18284v1", "date": "2024-06-26", "relevancy": 3.1128, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6251}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6251}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6176}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RealTalk%3A%20Real-time%20and%20Realistic%20Audio-driven%20Face%20Generation%20with%203D%0A%20%20Facial%20Prior-guided%20Identity%20Alignment%20Network&body=Title%3A%20RealTalk%3A%20Real-time%20and%20Realistic%20Audio-driven%20Face%20Generation%20with%203D%0A%20%20Facial%20Prior-guided%20Identity%20Alignment%20Network%0AAuthor%3A%20Xiaozhong%20Ji%20and%20Chuming%20Lin%20and%20Zhonggan%20Ding%20and%20Ying%20Tai%20and%20Jian%20Yang%20and%20Junwei%20Zhu%20and%20Xiaobin%20Hu%20and%20Jiangning%20Zhang%20and%20Donghao%20Luo%20and%20Chengjie%20Wang%0AAbstract%3A%20%20%20Person-generic%20audio-driven%20face%20generation%20is%20a%20challenging%20task%20in%20computer%0Avision.%20Previous%20methods%20have%20achieved%20remarkable%20progress%20in%20audio-visual%0Asynchronization%2C%20but%20there%20is%20still%20a%20significant%20gap%20between%20current%20results%0Aand%20practical%20applications.%20The%20challenges%20are%20two-fold%3A%201%29%20Preserving%20unique%0Aindividual%20traits%20for%20achieving%20high-precision%20lip%20synchronization.%202%29%0AGenerating%20high-quality%20facial%20renderings%20in%20real-time%20performance.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20generalized%20audio-driven%20framework%20RealTalk%2C%20which%0Aconsists%20of%20an%20audio-to-expression%20transformer%20and%20a%20high-fidelity%0Aexpression-to-face%20renderer.%20In%20the%20first%20component%2C%20we%20consider%20both%20identity%0Aand%20intra-personal%20variation%20features%20related%20to%20speaking%20lip%20movements.%20By%0Aincorporating%20cross-modal%20attention%20on%20the%20enriched%20facial%20priors%2C%20we%20can%0Aeffectively%20align%20lip%20movements%20with%20audio%2C%20thus%20attaining%20greater%20precision%20in%0Aexpression%20prediction.%20In%20the%20second%20component%2C%20we%20design%20a%20lightweight%20facial%0Aidentity%20alignment%20%28FIA%29%20module%20which%20includes%20a%20lip-shape%20control%20structure%0Aand%20a%20face%20texture%20reference%20structure.%20This%20novel%20design%20allows%20us%20to%20generate%0Afine%20details%20in%20real-time%2C%20without%20depending%20on%20sophisticated%20and%20inefficient%0Afeature%20alignment%20modules.%20Our%20experimental%20results%2C%20both%20quantitative%20and%0Aqualitative%2C%20on%20public%20datasets%20demonstrate%20the%20clear%20advantages%20of%20our%20method%0Ain%20terms%20of%20lip-speech%20synchronization%20and%20generation%20quality.%20Furthermore%2C%20our%0Amethod%20is%20efficient%20and%20requires%20fewer%20computational%20resources%2C%20making%20it%0Awell-suited%20to%20meet%20the%20needs%20of%20practical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18284v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRealTalk%253A%2520Real-time%2520and%2520Realistic%2520Audio-driven%2520Face%2520Generation%2520with%25203D%250A%2520%2520Facial%2520Prior-guided%2520Identity%2520Alignment%2520Network%26entry.906535625%3DXiaozhong%2520Ji%2520and%2520Chuming%2520Lin%2520and%2520Zhonggan%2520Ding%2520and%2520Ying%2520Tai%2520and%2520Jian%2520Yang%2520and%2520Junwei%2520Zhu%2520and%2520Xiaobin%2520Hu%2520and%2520Jiangning%2520Zhang%2520and%2520Donghao%2520Luo%2520and%2520Chengjie%2520Wang%26entry.1292438233%3D%2520%2520Person-generic%2520audio-driven%2520face%2520generation%2520is%2520a%2520challenging%2520task%2520in%2520computer%250Avision.%2520Previous%2520methods%2520have%2520achieved%2520remarkable%2520progress%2520in%2520audio-visual%250Asynchronization%252C%2520but%2520there%2520is%2520still%2520a%2520significant%2520gap%2520between%2520current%2520results%250Aand%2520practical%2520applications.%2520The%2520challenges%2520are%2520two-fold%253A%25201%2529%2520Preserving%2520unique%250Aindividual%2520traits%2520for%2520achieving%2520high-precision%2520lip%2520synchronization.%25202%2529%250AGenerating%2520high-quality%2520facial%2520renderings%2520in%2520real-time%2520performance.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520novel%2520generalized%2520audio-driven%2520framework%2520RealTalk%252C%2520which%250Aconsists%2520of%2520an%2520audio-to-expression%2520transformer%2520and%2520a%2520high-fidelity%250Aexpression-to-face%2520renderer.%2520In%2520the%2520first%2520component%252C%2520we%2520consider%2520both%2520identity%250Aand%2520intra-personal%2520variation%2520features%2520related%2520to%2520speaking%2520lip%2520movements.%2520By%250Aincorporating%2520cross-modal%2520attention%2520on%2520the%2520enriched%2520facial%2520priors%252C%2520we%2520can%250Aeffectively%2520align%2520lip%2520movements%2520with%2520audio%252C%2520thus%2520attaining%2520greater%2520precision%2520in%250Aexpression%2520prediction.%2520In%2520the%2520second%2520component%252C%2520we%2520design%2520a%2520lightweight%2520facial%250Aidentity%2520alignment%2520%2528FIA%2529%2520module%2520which%2520includes%2520a%2520lip-shape%2520control%2520structure%250Aand%2520a%2520face%2520texture%2520reference%2520structure.%2520This%2520novel%2520design%2520allows%2520us%2520to%2520generate%250Afine%2520details%2520in%2520real-time%252C%2520without%2520depending%2520on%2520sophisticated%2520and%2520inefficient%250Afeature%2520alignment%2520modules.%2520Our%2520experimental%2520results%252C%2520both%2520quantitative%2520and%250Aqualitative%252C%2520on%2520public%2520datasets%2520demonstrate%2520the%2520clear%2520advantages%2520of%2520our%2520method%250Ain%2520terms%2520of%2520lip-speech%2520synchronization%2520and%2520generation%2520quality.%2520Furthermore%252C%2520our%250Amethod%2520is%2520efficient%2520and%2520requires%2520fewer%2520computational%2520resources%252C%2520making%2520it%250Awell-suited%2520to%2520meet%2520the%2520needs%2520of%2520practical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18284v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RealTalk%3A%20Real-time%20and%20Realistic%20Audio-driven%20Face%20Generation%20with%203D%0A%20%20Facial%20Prior-guided%20Identity%20Alignment%20Network&entry.906535625=Xiaozhong%20Ji%20and%20Chuming%20Lin%20and%20Zhonggan%20Ding%20and%20Ying%20Tai%20and%20Jian%20Yang%20and%20Junwei%20Zhu%20and%20Xiaobin%20Hu%20and%20Jiangning%20Zhang%20and%20Donghao%20Luo%20and%20Chengjie%20Wang&entry.1292438233=%20%20Person-generic%20audio-driven%20face%20generation%20is%20a%20challenging%20task%20in%20computer%0Avision.%20Previous%20methods%20have%20achieved%20remarkable%20progress%20in%20audio-visual%0Asynchronization%2C%20but%20there%20is%20still%20a%20significant%20gap%20between%20current%20results%0Aand%20practical%20applications.%20The%20challenges%20are%20two-fold%3A%201%29%20Preserving%20unique%0Aindividual%20traits%20for%20achieving%20high-precision%20lip%20synchronization.%202%29%0AGenerating%20high-quality%20facial%20renderings%20in%20real-time%20performance.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20generalized%20audio-driven%20framework%20RealTalk%2C%20which%0Aconsists%20of%20an%20audio-to-expression%20transformer%20and%20a%20high-fidelity%0Aexpression-to-face%20renderer.%20In%20the%20first%20component%2C%20we%20consider%20both%20identity%0Aand%20intra-personal%20variation%20features%20related%20to%20speaking%20lip%20movements.%20By%0Aincorporating%20cross-modal%20attention%20on%20the%20enriched%20facial%20priors%2C%20we%20can%0Aeffectively%20align%20lip%20movements%20with%20audio%2C%20thus%20attaining%20greater%20precision%20in%0Aexpression%20prediction.%20In%20the%20second%20component%2C%20we%20design%20a%20lightweight%20facial%0Aidentity%20alignment%20%28FIA%29%20module%20which%20includes%20a%20lip-shape%20control%20structure%0Aand%20a%20face%20texture%20reference%20structure.%20This%20novel%20design%20allows%20us%20to%20generate%0Afine%20details%20in%20real-time%2C%20without%20depending%20on%20sophisticated%20and%20inefficient%0Afeature%20alignment%20modules.%20Our%20experimental%20results%2C%20both%20quantitative%20and%0Aqualitative%2C%20on%20public%20datasets%20demonstrate%20the%20clear%20advantages%20of%20our%20method%0Ain%20terms%20of%20lip-speech%20synchronization%20and%20generation%20quality.%20Furthermore%2C%20our%0Amethod%20is%20efficient%20and%20requires%20fewer%20computational%20resources%2C%20making%20it%0Awell-suited%20to%20meet%20the%20needs%20of%20practical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18284v1&entry.124074799=Read"},
{"title": "CTNeRF: Cross-Time Transformer for Dynamic Neural Radiance Field from\n  Monocular Video", "author": "Xingyu Miao and Yang Bai and Haoran Duan and Yawen Huang and Fan Wan and Yang Long and Yefeng Zheng", "abstract": "  The goal of our work is to generate high-quality novel views from monocular\nvideos of complex and dynamic scenes. Prior methods, such as DynamicNeRF, have\nshown impressive performance by leveraging time-varying dynamic radiation\nfields. However, these methods have limitations when it comes to accurately\nmodeling the motion of complex objects, which can lead to inaccurate and blurry\nrenderings of details. To address this limitation, we propose a novel approach\nthat builds upon a recent generalization NeRF, which aggregates nearby views\nonto new viewpoints. However, such methods are typically only effective for\nstatic scenes. To overcome this challenge, we introduce a module that operates\nin both the time and frequency domains to aggregate the features of object\nmotion. This allows us to learn the relationship between frames and generate\nhigher-quality images. Our experiments demonstrate significant improvements\nover state-of-the-art methods on dynamic scene datasets. Specifically, our\napproach outperforms existing methods in terms of both the accuracy and visual\nquality of the synthesized views. Our code is available on\nhttps://github.com/xingy038/CTNeRF.\n", "link": "http://arxiv.org/abs/2401.04861v2", "date": "2024-06-26", "relevancy": 3.0735, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6436}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6002}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6002}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CTNeRF%3A%20Cross-Time%20Transformer%20for%20Dynamic%20Neural%20Radiance%20Field%20from%0A%20%20Monocular%20Video&body=Title%3A%20CTNeRF%3A%20Cross-Time%20Transformer%20for%20Dynamic%20Neural%20Radiance%20Field%20from%0A%20%20Monocular%20Video%0AAuthor%3A%20Xingyu%20Miao%20and%20Yang%20Bai%20and%20Haoran%20Duan%20and%20Yawen%20Huang%20and%20Fan%20Wan%20and%20Yang%20Long%20and%20Yefeng%20Zheng%0AAbstract%3A%20%20%20The%20goal%20of%20our%20work%20is%20to%20generate%20high-quality%20novel%20views%20from%20monocular%0Avideos%20of%20complex%20and%20dynamic%20scenes.%20Prior%20methods%2C%20such%20as%20DynamicNeRF%2C%20have%0Ashown%20impressive%20performance%20by%20leveraging%20time-varying%20dynamic%20radiation%0Afields.%20However%2C%20these%20methods%20have%20limitations%20when%20it%20comes%20to%20accurately%0Amodeling%20the%20motion%20of%20complex%20objects%2C%20which%20can%20lead%20to%20inaccurate%20and%20blurry%0Arenderings%20of%20details.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20novel%20approach%0Athat%20builds%20upon%20a%20recent%20generalization%20NeRF%2C%20which%20aggregates%20nearby%20views%0Aonto%20new%20viewpoints.%20However%2C%20such%20methods%20are%20typically%20only%20effective%20for%0Astatic%20scenes.%20To%20overcome%20this%20challenge%2C%20we%20introduce%20a%20module%20that%20operates%0Ain%20both%20the%20time%20and%20frequency%20domains%20to%20aggregate%20the%20features%20of%20object%0Amotion.%20This%20allows%20us%20to%20learn%20the%20relationship%20between%20frames%20and%20generate%0Ahigher-quality%20images.%20Our%20experiments%20demonstrate%20significant%20improvements%0Aover%20state-of-the-art%20methods%20on%20dynamic%20scene%20datasets.%20Specifically%2C%20our%0Aapproach%20outperforms%20existing%20methods%20in%20terms%20of%20both%20the%20accuracy%20and%20visual%0Aquality%20of%20the%20synthesized%20views.%20Our%20code%20is%20available%20on%0Ahttps%3A//github.com/xingy038/CTNeRF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.04861v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCTNeRF%253A%2520Cross-Time%2520Transformer%2520for%2520Dynamic%2520Neural%2520Radiance%2520Field%2520from%250A%2520%2520Monocular%2520Video%26entry.906535625%3DXingyu%2520Miao%2520and%2520Yang%2520Bai%2520and%2520Haoran%2520Duan%2520and%2520Yawen%2520Huang%2520and%2520Fan%2520Wan%2520and%2520Yang%2520Long%2520and%2520Yefeng%2520Zheng%26entry.1292438233%3D%2520%2520The%2520goal%2520of%2520our%2520work%2520is%2520to%2520generate%2520high-quality%2520novel%2520views%2520from%2520monocular%250Avideos%2520of%2520complex%2520and%2520dynamic%2520scenes.%2520Prior%2520methods%252C%2520such%2520as%2520DynamicNeRF%252C%2520have%250Ashown%2520impressive%2520performance%2520by%2520leveraging%2520time-varying%2520dynamic%2520radiation%250Afields.%2520However%252C%2520these%2520methods%2520have%2520limitations%2520when%2520it%2520comes%2520to%2520accurately%250Amodeling%2520the%2520motion%2520of%2520complex%2520objects%252C%2520which%2520can%2520lead%2520to%2520inaccurate%2520and%2520blurry%250Arenderings%2520of%2520details.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520a%2520novel%2520approach%250Athat%2520builds%2520upon%2520a%2520recent%2520generalization%2520NeRF%252C%2520which%2520aggregates%2520nearby%2520views%250Aonto%2520new%2520viewpoints.%2520However%252C%2520such%2520methods%2520are%2520typically%2520only%2520effective%2520for%250Astatic%2520scenes.%2520To%2520overcome%2520this%2520challenge%252C%2520we%2520introduce%2520a%2520module%2520that%2520operates%250Ain%2520both%2520the%2520time%2520and%2520frequency%2520domains%2520to%2520aggregate%2520the%2520features%2520of%2520object%250Amotion.%2520This%2520allows%2520us%2520to%2520learn%2520the%2520relationship%2520between%2520frames%2520and%2520generate%250Ahigher-quality%2520images.%2520Our%2520experiments%2520demonstrate%2520significant%2520improvements%250Aover%2520state-of-the-art%2520methods%2520on%2520dynamic%2520scene%2520datasets.%2520Specifically%252C%2520our%250Aapproach%2520outperforms%2520existing%2520methods%2520in%2520terms%2520of%2520both%2520the%2520accuracy%2520and%2520visual%250Aquality%2520of%2520the%2520synthesized%2520views.%2520Our%2520code%2520is%2520available%2520on%250Ahttps%253A//github.com/xingy038/CTNeRF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.04861v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CTNeRF%3A%20Cross-Time%20Transformer%20for%20Dynamic%20Neural%20Radiance%20Field%20from%0A%20%20Monocular%20Video&entry.906535625=Xingyu%20Miao%20and%20Yang%20Bai%20and%20Haoran%20Duan%20and%20Yawen%20Huang%20and%20Fan%20Wan%20and%20Yang%20Long%20and%20Yefeng%20Zheng&entry.1292438233=%20%20The%20goal%20of%20our%20work%20is%20to%20generate%20high-quality%20novel%20views%20from%20monocular%0Avideos%20of%20complex%20and%20dynamic%20scenes.%20Prior%20methods%2C%20such%20as%20DynamicNeRF%2C%20have%0Ashown%20impressive%20performance%20by%20leveraging%20time-varying%20dynamic%20radiation%0Afields.%20However%2C%20these%20methods%20have%20limitations%20when%20it%20comes%20to%20accurately%0Amodeling%20the%20motion%20of%20complex%20objects%2C%20which%20can%20lead%20to%20inaccurate%20and%20blurry%0Arenderings%20of%20details.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20novel%20approach%0Athat%20builds%20upon%20a%20recent%20generalization%20NeRF%2C%20which%20aggregates%20nearby%20views%0Aonto%20new%20viewpoints.%20However%2C%20such%20methods%20are%20typically%20only%20effective%20for%0Astatic%20scenes.%20To%20overcome%20this%20challenge%2C%20we%20introduce%20a%20module%20that%20operates%0Ain%20both%20the%20time%20and%20frequency%20domains%20to%20aggregate%20the%20features%20of%20object%0Amotion.%20This%20allows%20us%20to%20learn%20the%20relationship%20between%20frames%20and%20generate%0Ahigher-quality%20images.%20Our%20experiments%20demonstrate%20significant%20improvements%0Aover%20state-of-the-art%20methods%20on%20dynamic%20scene%20datasets.%20Specifically%2C%20our%0Aapproach%20outperforms%20existing%20methods%20in%20terms%20of%20both%20the%20accuracy%20and%20visual%0Aquality%20of%20the%20synthesized%20views.%20Our%20code%20is%20available%20on%0Ahttps%3A//github.com/xingy038/CTNeRF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.04861v2&entry.124074799=Read"},
{"title": "DF-SLAM: Dictionary Factors Representation for High-Fidelity Neural\n  Implicit Dense Visual SLAM System", "author": "Weifeng Wei and Jie Wang and Shuqi Deng and Jie Liu", "abstract": "  We introduce a high-fidelity neural implicit dense visual Simultaneous\nLocalization and Mapping (SLAM) system, termed DF-SLAM. In our work, we employ\ndictionary factors for scene representation, encoding the geometry and\nappearance information of the scene as a combination of basis and coefficient\nfactors. Compared to neural implicit dense visual SLAM methods that directly\nencode scene information as features, our method exhibits superior scene detail\nreconstruction capabilities and more efficient memory usage, while our model\nsize is insensitive to the size of the scene map, making our method more\nsuitable for large-scale scenes. Additionally, we employ feature integration\nrendering to accelerate color rendering speed while ensuring color rendering\nquality, further enhancing the real-time performance of our neural SLAM method.\nExtensive experiments on synthetic and real-world datasets demonstrate that our\nmethod is competitive with existing state-of-the-art neural implicit SLAM\nmethods in terms of real-time performance, localization accuracy, and scene\nreconstruction quality. Our source code is available at\nhttps://github.com/funcdecl/DF-SLAM.\n", "link": "http://arxiv.org/abs/2404.17876v2", "date": "2024-06-26", "relevancy": 3.0122, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6326}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6112}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5635}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DF-SLAM%3A%20Dictionary%20Factors%20Representation%20for%20High-Fidelity%20Neural%0A%20%20Implicit%20Dense%20Visual%20SLAM%20System&body=Title%3A%20DF-SLAM%3A%20Dictionary%20Factors%20Representation%20for%20High-Fidelity%20Neural%0A%20%20Implicit%20Dense%20Visual%20SLAM%20System%0AAuthor%3A%20Weifeng%20Wei%20and%20Jie%20Wang%20and%20Shuqi%20Deng%20and%20Jie%20Liu%0AAbstract%3A%20%20%20We%20introduce%20a%20high-fidelity%20neural%20implicit%20dense%20visual%20Simultaneous%0ALocalization%20and%20Mapping%20%28SLAM%29%20system%2C%20termed%20DF-SLAM.%20In%20our%20work%2C%20we%20employ%0Adictionary%20factors%20for%20scene%20representation%2C%20encoding%20the%20geometry%20and%0Aappearance%20information%20of%20the%20scene%20as%20a%20combination%20of%20basis%20and%20coefficient%0Afactors.%20Compared%20to%20neural%20implicit%20dense%20visual%20SLAM%20methods%20that%20directly%0Aencode%20scene%20information%20as%20features%2C%20our%20method%20exhibits%20superior%20scene%20detail%0Areconstruction%20capabilities%20and%20more%20efficient%20memory%20usage%2C%20while%20our%20model%0Asize%20is%20insensitive%20to%20the%20size%20of%20the%20scene%20map%2C%20making%20our%20method%20more%0Asuitable%20for%20large-scale%20scenes.%20Additionally%2C%20we%20employ%20feature%20integration%0Arendering%20to%20accelerate%20color%20rendering%20speed%20while%20ensuring%20color%20rendering%0Aquality%2C%20further%20enhancing%20the%20real-time%20performance%20of%20our%20neural%20SLAM%20method.%0AExtensive%20experiments%20on%20synthetic%20and%20real-world%20datasets%20demonstrate%20that%20our%0Amethod%20is%20competitive%20with%20existing%20state-of-the-art%20neural%20implicit%20SLAM%0Amethods%20in%20terms%20of%20real-time%20performance%2C%20localization%20accuracy%2C%20and%20scene%0Areconstruction%20quality.%20Our%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/funcdecl/DF-SLAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17876v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDF-SLAM%253A%2520Dictionary%2520Factors%2520Representation%2520for%2520High-Fidelity%2520Neural%250A%2520%2520Implicit%2520Dense%2520Visual%2520SLAM%2520System%26entry.906535625%3DWeifeng%2520Wei%2520and%2520Jie%2520Wang%2520and%2520Shuqi%2520Deng%2520and%2520Jie%2520Liu%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520high-fidelity%2520neural%2520implicit%2520dense%2520visual%2520Simultaneous%250ALocalization%2520and%2520Mapping%2520%2528SLAM%2529%2520system%252C%2520termed%2520DF-SLAM.%2520In%2520our%2520work%252C%2520we%2520employ%250Adictionary%2520factors%2520for%2520scene%2520representation%252C%2520encoding%2520the%2520geometry%2520and%250Aappearance%2520information%2520of%2520the%2520scene%2520as%2520a%2520combination%2520of%2520basis%2520and%2520coefficient%250Afactors.%2520Compared%2520to%2520neural%2520implicit%2520dense%2520visual%2520SLAM%2520methods%2520that%2520directly%250Aencode%2520scene%2520information%2520as%2520features%252C%2520our%2520method%2520exhibits%2520superior%2520scene%2520detail%250Areconstruction%2520capabilities%2520and%2520more%2520efficient%2520memory%2520usage%252C%2520while%2520our%2520model%250Asize%2520is%2520insensitive%2520to%2520the%2520size%2520of%2520the%2520scene%2520map%252C%2520making%2520our%2520method%2520more%250Asuitable%2520for%2520large-scale%2520scenes.%2520Additionally%252C%2520we%2520employ%2520feature%2520integration%250Arendering%2520to%2520accelerate%2520color%2520rendering%2520speed%2520while%2520ensuring%2520color%2520rendering%250Aquality%252C%2520further%2520enhancing%2520the%2520real-time%2520performance%2520of%2520our%2520neural%2520SLAM%2520method.%250AExtensive%2520experiments%2520on%2520synthetic%2520and%2520real-world%2520datasets%2520demonstrate%2520that%2520our%250Amethod%2520is%2520competitive%2520with%2520existing%2520state-of-the-art%2520neural%2520implicit%2520SLAM%250Amethods%2520in%2520terms%2520of%2520real-time%2520performance%252C%2520localization%2520accuracy%252C%2520and%2520scene%250Areconstruction%2520quality.%2520Our%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/funcdecl/DF-SLAM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.17876v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DF-SLAM%3A%20Dictionary%20Factors%20Representation%20for%20High-Fidelity%20Neural%0A%20%20Implicit%20Dense%20Visual%20SLAM%20System&entry.906535625=Weifeng%20Wei%20and%20Jie%20Wang%20and%20Shuqi%20Deng%20and%20Jie%20Liu&entry.1292438233=%20%20We%20introduce%20a%20high-fidelity%20neural%20implicit%20dense%20visual%20Simultaneous%0ALocalization%20and%20Mapping%20%28SLAM%29%20system%2C%20termed%20DF-SLAM.%20In%20our%20work%2C%20we%20employ%0Adictionary%20factors%20for%20scene%20representation%2C%20encoding%20the%20geometry%20and%0Aappearance%20information%20of%20the%20scene%20as%20a%20combination%20of%20basis%20and%20coefficient%0Afactors.%20Compared%20to%20neural%20implicit%20dense%20visual%20SLAM%20methods%20that%20directly%0Aencode%20scene%20information%20as%20features%2C%20our%20method%20exhibits%20superior%20scene%20detail%0Areconstruction%20capabilities%20and%20more%20efficient%20memory%20usage%2C%20while%20our%20model%0Asize%20is%20insensitive%20to%20the%20size%20of%20the%20scene%20map%2C%20making%20our%20method%20more%0Asuitable%20for%20large-scale%20scenes.%20Additionally%2C%20we%20employ%20feature%20integration%0Arendering%20to%20accelerate%20color%20rendering%20speed%20while%20ensuring%20color%20rendering%0Aquality%2C%20further%20enhancing%20the%20real-time%20performance%20of%20our%20neural%20SLAM%20method.%0AExtensive%20experiments%20on%20synthetic%20and%20real-world%20datasets%20demonstrate%20that%20our%0Amethod%20is%20competitive%20with%20existing%20state-of-the-art%20neural%20implicit%20SLAM%0Amethods%20in%20terms%20of%20real-time%20performance%2C%20localization%20accuracy%2C%20and%20scene%0Areconstruction%20quality.%20Our%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/funcdecl/DF-SLAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17876v2&entry.124074799=Read"},
{"title": "Trimming the Fat: Efficient Compression of 3D Gaussian Splats through\n  Pruning", "author": "Muhammad Salman Ali and Maryam Qamar and Sung-Ho Bae and Enzo Tartaglione", "abstract": "  In recent times, the utilization of 3D models has gained traction, owing to\nthe capacity for end-to-end training initially offered by Neural Radiance\nFields and more recently by 3D Gaussian Splatting (3DGS) models. The latter\nholds a significant advantage by inherently easing rapid convergence during\ntraining and offering extensive editability. However, despite rapid\nadvancements, the literature still lives in its infancy regarding the\nscalability of these models. In this study, we take some initial steps in\naddressing this gap, showing an approach that enables both the memory and\ncomputational scalability of such models. Specifically, we propose \"Trimming\nthe fat\", a post-hoc gradient-informed iterative pruning technique to eliminate\nredundant information encoded in the model. Our experimental findings on widely\nacknowledged benchmarks attest to the effectiveness of our approach, revealing\nthat up to 75% of the Gaussians can be removed while maintaining or even\nimproving upon baseline performance. Our approach achieves around 50$\\times$\ncompression while preserving performance similar to the baseline model, and is\nable to speed-up computation up to 600~FPS.\n", "link": "http://arxiv.org/abs/2406.18214v1", "date": "2024-06-26", "relevancy": 3.009, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6674}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6018}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5362}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trimming%20the%20Fat%3A%20Efficient%20Compression%20of%203D%20Gaussian%20Splats%20through%0A%20%20Pruning&body=Title%3A%20Trimming%20the%20Fat%3A%20Efficient%20Compression%20of%203D%20Gaussian%20Splats%20through%0A%20%20Pruning%0AAuthor%3A%20Muhammad%20Salman%20Ali%20and%20Maryam%20Qamar%20and%20Sung-Ho%20Bae%20and%20Enzo%20Tartaglione%0AAbstract%3A%20%20%20In%20recent%20times%2C%20the%20utilization%20of%203D%20models%20has%20gained%20traction%2C%20owing%20to%0Athe%20capacity%20for%20end-to-end%20training%20initially%20offered%20by%20Neural%20Radiance%0AFields%20and%20more%20recently%20by%203D%20Gaussian%20Splatting%20%283DGS%29%20models.%20The%20latter%0Aholds%20a%20significant%20advantage%20by%20inherently%20easing%20rapid%20convergence%20during%0Atraining%20and%20offering%20extensive%20editability.%20However%2C%20despite%20rapid%0Aadvancements%2C%20the%20literature%20still%20lives%20in%20its%20infancy%20regarding%20the%0Ascalability%20of%20these%20models.%20In%20this%20study%2C%20we%20take%20some%20initial%20steps%20in%0Aaddressing%20this%20gap%2C%20showing%20an%20approach%20that%20enables%20both%20the%20memory%20and%0Acomputational%20scalability%20of%20such%20models.%20Specifically%2C%20we%20propose%20%22Trimming%0Athe%20fat%22%2C%20a%20post-hoc%20gradient-informed%20iterative%20pruning%20technique%20to%20eliminate%0Aredundant%20information%20encoded%20in%20the%20model.%20Our%20experimental%20findings%20on%20widely%0Aacknowledged%20benchmarks%20attest%20to%20the%20effectiveness%20of%20our%20approach%2C%20revealing%0Athat%20up%20to%2075%25%20of%20the%20Gaussians%20can%20be%20removed%20while%20maintaining%20or%20even%0Aimproving%20upon%20baseline%20performance.%20Our%20approach%20achieves%20around%2050%24%5Ctimes%24%0Acompression%20while%20preserving%20performance%20similar%20to%20the%20baseline%20model%2C%20and%20is%0Aable%20to%20speed-up%20computation%20up%20to%20600~FPS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18214v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrimming%2520the%2520Fat%253A%2520Efficient%2520Compression%2520of%25203D%2520Gaussian%2520Splats%2520through%250A%2520%2520Pruning%26entry.906535625%3DMuhammad%2520Salman%2520Ali%2520and%2520Maryam%2520Qamar%2520and%2520Sung-Ho%2520Bae%2520and%2520Enzo%2520Tartaglione%26entry.1292438233%3D%2520%2520In%2520recent%2520times%252C%2520the%2520utilization%2520of%25203D%2520models%2520has%2520gained%2520traction%252C%2520owing%2520to%250Athe%2520capacity%2520for%2520end-to-end%2520training%2520initially%2520offered%2520by%2520Neural%2520Radiance%250AFields%2520and%2520more%2520recently%2520by%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520models.%2520The%2520latter%250Aholds%2520a%2520significant%2520advantage%2520by%2520inherently%2520easing%2520rapid%2520convergence%2520during%250Atraining%2520and%2520offering%2520extensive%2520editability.%2520However%252C%2520despite%2520rapid%250Aadvancements%252C%2520the%2520literature%2520still%2520lives%2520in%2520its%2520infancy%2520regarding%2520the%250Ascalability%2520of%2520these%2520models.%2520In%2520this%2520study%252C%2520we%2520take%2520some%2520initial%2520steps%2520in%250Aaddressing%2520this%2520gap%252C%2520showing%2520an%2520approach%2520that%2520enables%2520both%2520the%2520memory%2520and%250Acomputational%2520scalability%2520of%2520such%2520models.%2520Specifically%252C%2520we%2520propose%2520%2522Trimming%250Athe%2520fat%2522%252C%2520a%2520post-hoc%2520gradient-informed%2520iterative%2520pruning%2520technique%2520to%2520eliminate%250Aredundant%2520information%2520encoded%2520in%2520the%2520model.%2520Our%2520experimental%2520findings%2520on%2520widely%250Aacknowledged%2520benchmarks%2520attest%2520to%2520the%2520effectiveness%2520of%2520our%2520approach%252C%2520revealing%250Athat%2520up%2520to%252075%2525%2520of%2520the%2520Gaussians%2520can%2520be%2520removed%2520while%2520maintaining%2520or%2520even%250Aimproving%2520upon%2520baseline%2520performance.%2520Our%2520approach%2520achieves%2520around%252050%2524%255Ctimes%2524%250Acompression%2520while%2520preserving%2520performance%2520similar%2520to%2520the%2520baseline%2520model%252C%2520and%2520is%250Aable%2520to%2520speed-up%2520computation%2520up%2520to%2520600~FPS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18214v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trimming%20the%20Fat%3A%20Efficient%20Compression%20of%203D%20Gaussian%20Splats%20through%0A%20%20Pruning&entry.906535625=Muhammad%20Salman%20Ali%20and%20Maryam%20Qamar%20and%20Sung-Ho%20Bae%20and%20Enzo%20Tartaglione&entry.1292438233=%20%20In%20recent%20times%2C%20the%20utilization%20of%203D%20models%20has%20gained%20traction%2C%20owing%20to%0Athe%20capacity%20for%20end-to-end%20training%20initially%20offered%20by%20Neural%20Radiance%0AFields%20and%20more%20recently%20by%203D%20Gaussian%20Splatting%20%283DGS%29%20models.%20The%20latter%0Aholds%20a%20significant%20advantage%20by%20inherently%20easing%20rapid%20convergence%20during%0Atraining%20and%20offering%20extensive%20editability.%20However%2C%20despite%20rapid%0Aadvancements%2C%20the%20literature%20still%20lives%20in%20its%20infancy%20regarding%20the%0Ascalability%20of%20these%20models.%20In%20this%20study%2C%20we%20take%20some%20initial%20steps%20in%0Aaddressing%20this%20gap%2C%20showing%20an%20approach%20that%20enables%20both%20the%20memory%20and%0Acomputational%20scalability%20of%20such%20models.%20Specifically%2C%20we%20propose%20%22Trimming%0Athe%20fat%22%2C%20a%20post-hoc%20gradient-informed%20iterative%20pruning%20technique%20to%20eliminate%0Aredundant%20information%20encoded%20in%20the%20model.%20Our%20experimental%20findings%20on%20widely%0Aacknowledged%20benchmarks%20attest%20to%20the%20effectiveness%20of%20our%20approach%2C%20revealing%0Athat%20up%20to%2075%25%20of%20the%20Gaussians%20can%20be%20removed%20while%20maintaining%20or%20even%0Aimproving%20upon%20baseline%20performance.%20Our%20approach%20achieves%20around%2050%24%5Ctimes%24%0Acompression%20while%20preserving%20performance%20similar%20to%20the%20baseline%20model%2C%20and%20is%0Aable%20to%20speed-up%20computation%20up%20to%20600~FPS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18214v1&entry.124074799=Read"},
{"title": "3D-MVP: 3D Multiview Pretraining for Robotic Manipulation", "author": "Shengyi Qian and Kaichun Mo and Valts Blukis and David F. Fouhey and Dieter Fox and Ankit Goyal", "abstract": "  Recent works have shown that visual pretraining on egocentric datasets using\nmasked autoencoders (MAE) can improve generalization for downstream robotics\ntasks. However, these approaches pretrain only on 2D images, while many\nrobotics applications require 3D scene understanding. In this work, we propose\n3D-MVP, a novel approach for 3D multi-view pretraining using masked\nautoencoders. We leverage Robotic View Transformer (RVT), which uses a\nmulti-view transformer to understand the 3D scene and predict gripper pose\nactions. We split RVT's multi-view transformer into visual encoder and action\ndecoder, and pretrain its visual encoder using masked autoencoding on\nlarge-scale 3D datasets such as Objaverse. We evaluate 3D-MVP on a suite of\nvirtual robot manipulation tasks and demonstrate improved performance over\nbaselines. We also show promising results on a real robot platform with minimal\nfinetuning. Our results suggest that 3D-aware pretraining is a promising\napproach to improve sample efficiency and generalization of vision-based\nrobotic manipulation policies. We will release code and pretrained models for\n3D-MVP to facilitate future research. Project site:\nhttps://jasonqsy.github.io/3DMVP\n", "link": "http://arxiv.org/abs/2406.18158v1", "date": "2024-06-26", "relevancy": 2.9379, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6083}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5772}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D-MVP%3A%203D%20Multiview%20Pretraining%20for%20Robotic%20Manipulation&body=Title%3A%203D-MVP%3A%203D%20Multiview%20Pretraining%20for%20Robotic%20Manipulation%0AAuthor%3A%20Shengyi%20Qian%20and%20Kaichun%20Mo%20and%20Valts%20Blukis%20and%20David%20F.%20Fouhey%20and%20Dieter%20Fox%20and%20Ankit%20Goyal%0AAbstract%3A%20%20%20Recent%20works%20have%20shown%20that%20visual%20pretraining%20on%20egocentric%20datasets%20using%0Amasked%20autoencoders%20%28MAE%29%20can%20improve%20generalization%20for%20downstream%20robotics%0Atasks.%20However%2C%20these%20approaches%20pretrain%20only%20on%202D%20images%2C%20while%20many%0Arobotics%20applications%20require%203D%20scene%20understanding.%20In%20this%20work%2C%20we%20propose%0A3D-MVP%2C%20a%20novel%20approach%20for%203D%20multi-view%20pretraining%20using%20masked%0Aautoencoders.%20We%20leverage%20Robotic%20View%20Transformer%20%28RVT%29%2C%20which%20uses%20a%0Amulti-view%20transformer%20to%20understand%20the%203D%20scene%20and%20predict%20gripper%20pose%0Aactions.%20We%20split%20RVT%27s%20multi-view%20transformer%20into%20visual%20encoder%20and%20action%0Adecoder%2C%20and%20pretrain%20its%20visual%20encoder%20using%20masked%20autoencoding%20on%0Alarge-scale%203D%20datasets%20such%20as%20Objaverse.%20We%20evaluate%203D-MVP%20on%20a%20suite%20of%0Avirtual%20robot%20manipulation%20tasks%20and%20demonstrate%20improved%20performance%20over%0Abaselines.%20We%20also%20show%20promising%20results%20on%20a%20real%20robot%20platform%20with%20minimal%0Afinetuning.%20Our%20results%20suggest%20that%203D-aware%20pretraining%20is%20a%20promising%0Aapproach%20to%20improve%20sample%20efficiency%20and%20generalization%20of%20vision-based%0Arobotic%20manipulation%20policies.%20We%20will%20release%20code%20and%20pretrained%20models%20for%0A3D-MVP%20to%20facilitate%20future%20research.%20Project%20site%3A%0Ahttps%3A//jasonqsy.github.io/3DMVP%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18158v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D-MVP%253A%25203D%2520Multiview%2520Pretraining%2520for%2520Robotic%2520Manipulation%26entry.906535625%3DShengyi%2520Qian%2520and%2520Kaichun%2520Mo%2520and%2520Valts%2520Blukis%2520and%2520David%2520F.%2520Fouhey%2520and%2520Dieter%2520Fox%2520and%2520Ankit%2520Goyal%26entry.1292438233%3D%2520%2520Recent%2520works%2520have%2520shown%2520that%2520visual%2520pretraining%2520on%2520egocentric%2520datasets%2520using%250Amasked%2520autoencoders%2520%2528MAE%2529%2520can%2520improve%2520generalization%2520for%2520downstream%2520robotics%250Atasks.%2520However%252C%2520these%2520approaches%2520pretrain%2520only%2520on%25202D%2520images%252C%2520while%2520many%250Arobotics%2520applications%2520require%25203D%2520scene%2520understanding.%2520In%2520this%2520work%252C%2520we%2520propose%250A3D-MVP%252C%2520a%2520novel%2520approach%2520for%25203D%2520multi-view%2520pretraining%2520using%2520masked%250Aautoencoders.%2520We%2520leverage%2520Robotic%2520View%2520Transformer%2520%2528RVT%2529%252C%2520which%2520uses%2520a%250Amulti-view%2520transformer%2520to%2520understand%2520the%25203D%2520scene%2520and%2520predict%2520gripper%2520pose%250Aactions.%2520We%2520split%2520RVT%2527s%2520multi-view%2520transformer%2520into%2520visual%2520encoder%2520and%2520action%250Adecoder%252C%2520and%2520pretrain%2520its%2520visual%2520encoder%2520using%2520masked%2520autoencoding%2520on%250Alarge-scale%25203D%2520datasets%2520such%2520as%2520Objaverse.%2520We%2520evaluate%25203D-MVP%2520on%2520a%2520suite%2520of%250Avirtual%2520robot%2520manipulation%2520tasks%2520and%2520demonstrate%2520improved%2520performance%2520over%250Abaselines.%2520We%2520also%2520show%2520promising%2520results%2520on%2520a%2520real%2520robot%2520platform%2520with%2520minimal%250Afinetuning.%2520Our%2520results%2520suggest%2520that%25203D-aware%2520pretraining%2520is%2520a%2520promising%250Aapproach%2520to%2520improve%2520sample%2520efficiency%2520and%2520generalization%2520of%2520vision-based%250Arobotic%2520manipulation%2520policies.%2520We%2520will%2520release%2520code%2520and%2520pretrained%2520models%2520for%250A3D-MVP%2520to%2520facilitate%2520future%2520research.%2520Project%2520site%253A%250Ahttps%253A//jasonqsy.github.io/3DMVP%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18158v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D-MVP%3A%203D%20Multiview%20Pretraining%20for%20Robotic%20Manipulation&entry.906535625=Shengyi%20Qian%20and%20Kaichun%20Mo%20and%20Valts%20Blukis%20and%20David%20F.%20Fouhey%20and%20Dieter%20Fox%20and%20Ankit%20Goyal&entry.1292438233=%20%20Recent%20works%20have%20shown%20that%20visual%20pretraining%20on%20egocentric%20datasets%20using%0Amasked%20autoencoders%20%28MAE%29%20can%20improve%20generalization%20for%20downstream%20robotics%0Atasks.%20However%2C%20these%20approaches%20pretrain%20only%20on%202D%20images%2C%20while%20many%0Arobotics%20applications%20require%203D%20scene%20understanding.%20In%20this%20work%2C%20we%20propose%0A3D-MVP%2C%20a%20novel%20approach%20for%203D%20multi-view%20pretraining%20using%20masked%0Aautoencoders.%20We%20leverage%20Robotic%20View%20Transformer%20%28RVT%29%2C%20which%20uses%20a%0Amulti-view%20transformer%20to%20understand%20the%203D%20scene%20and%20predict%20gripper%20pose%0Aactions.%20We%20split%20RVT%27s%20multi-view%20transformer%20into%20visual%20encoder%20and%20action%0Adecoder%2C%20and%20pretrain%20its%20visual%20encoder%20using%20masked%20autoencoding%20on%0Alarge-scale%203D%20datasets%20such%20as%20Objaverse.%20We%20evaluate%203D-MVP%20on%20a%20suite%20of%0Avirtual%20robot%20manipulation%20tasks%20and%20demonstrate%20improved%20performance%20over%0Abaselines.%20We%20also%20show%20promising%20results%20on%20a%20real%20robot%20platform%20with%20minimal%0Afinetuning.%20Our%20results%20suggest%20that%203D-aware%20pretraining%20is%20a%20promising%0Aapproach%20to%20improve%20sample%20efficiency%20and%20generalization%20of%20vision-based%0Arobotic%20manipulation%20policies.%20We%20will%20release%20code%20and%20pretrained%20models%20for%0A3D-MVP%20to%20facilitate%20future%20research.%20Project%20site%3A%0Ahttps%3A//jasonqsy.github.io/3DMVP%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18158v1&entry.124074799=Read"},
{"title": "Speech2UnifiedExpressions: Synchronous Synthesis of Co-Speech Affective\n  Face and Body Expressions from Affordable Inputs", "author": "Uttaran Bhattacharya and Aniket Bera and Dinesh Manocha", "abstract": "  We present a multimodal learning-based method to simultaneously synthesize\nco-speech facial expressions and upper-body gestures for digital characters\nusing RGB video data captured using commodity cameras. Our approach learns from\nsparse face landmarks and upper-body joints, estimated directly from video\ndata, to generate plausible emotive character motions. Given a speech audio\nwaveform and a token sequence of the speaker's face landmark motion and\nbody-joint motion computed from a video, our method synthesizes the motion\nsequences for the speaker's face landmarks and body joints to match the content\nand the affect of the speech. We design a generator consisting of a set of\nencoders to transform all the inputs into a multimodal embedding space\ncapturing their correlations, followed by a pair of decoders to synthesize the\ndesired face and pose motions. To enhance the plausibility of synthesis, we use\nan adversarial discriminator that learns to differentiate between the face and\npose motions computed from the original videos and our synthesized motions\nbased on their affective expressions. To evaluate our approach, we extend the\nTED Gesture Dataset to include view-normalized, co-speech face landmarks in\naddition to body gestures. We demonstrate the performance of our method through\nthorough quantitative and qualitative experiments on multiple evaluation\nmetrics and via a user study. We observe that our method results in low\nreconstruction error and produces synthesized samples with diverse facial\nexpressions and body gestures for digital characters.\n", "link": "http://arxiv.org/abs/2406.18068v1", "date": "2024-06-26", "relevancy": 2.9193, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5855}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5831}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Speech2UnifiedExpressions%3A%20Synchronous%20Synthesis%20of%20Co-Speech%20Affective%0A%20%20Face%20and%20Body%20Expressions%20from%20Affordable%20Inputs&body=Title%3A%20Speech2UnifiedExpressions%3A%20Synchronous%20Synthesis%20of%20Co-Speech%20Affective%0A%20%20Face%20and%20Body%20Expressions%20from%20Affordable%20Inputs%0AAuthor%3A%20Uttaran%20Bhattacharya%20and%20Aniket%20Bera%20and%20Dinesh%20Manocha%0AAbstract%3A%20%20%20We%20present%20a%20multimodal%20learning-based%20method%20to%20simultaneously%20synthesize%0Aco-speech%20facial%20expressions%20and%20upper-body%20gestures%20for%20digital%20characters%0Ausing%20RGB%20video%20data%20captured%20using%20commodity%20cameras.%20Our%20approach%20learns%20from%0Asparse%20face%20landmarks%20and%20upper-body%20joints%2C%20estimated%20directly%20from%20video%0Adata%2C%20to%20generate%20plausible%20emotive%20character%20motions.%20Given%20a%20speech%20audio%0Awaveform%20and%20a%20token%20sequence%20of%20the%20speaker%27s%20face%20landmark%20motion%20and%0Abody-joint%20motion%20computed%20from%20a%20video%2C%20our%20method%20synthesizes%20the%20motion%0Asequences%20for%20the%20speaker%27s%20face%20landmarks%20and%20body%20joints%20to%20match%20the%20content%0Aand%20the%20affect%20of%20the%20speech.%20We%20design%20a%20generator%20consisting%20of%20a%20set%20of%0Aencoders%20to%20transform%20all%20the%20inputs%20into%20a%20multimodal%20embedding%20space%0Acapturing%20their%20correlations%2C%20followed%20by%20a%20pair%20of%20decoders%20to%20synthesize%20the%0Adesired%20face%20and%20pose%20motions.%20To%20enhance%20the%20plausibility%20of%20synthesis%2C%20we%20use%0Aan%20adversarial%20discriminator%20that%20learns%20to%20differentiate%20between%20the%20face%20and%0Apose%20motions%20computed%20from%20the%20original%20videos%20and%20our%20synthesized%20motions%0Abased%20on%20their%20affective%20expressions.%20To%20evaluate%20our%20approach%2C%20we%20extend%20the%0ATED%20Gesture%20Dataset%20to%20include%20view-normalized%2C%20co-speech%20face%20landmarks%20in%0Aaddition%20to%20body%20gestures.%20We%20demonstrate%20the%20performance%20of%20our%20method%20through%0Athorough%20quantitative%20and%20qualitative%20experiments%20on%20multiple%20evaluation%0Ametrics%20and%20via%20a%20user%20study.%20We%20observe%20that%20our%20method%20results%20in%20low%0Areconstruction%20error%20and%20produces%20synthesized%20samples%20with%20diverse%20facial%0Aexpressions%20and%20body%20gestures%20for%20digital%20characters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18068v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpeech2UnifiedExpressions%253A%2520Synchronous%2520Synthesis%2520of%2520Co-Speech%2520Affective%250A%2520%2520Face%2520and%2520Body%2520Expressions%2520from%2520Affordable%2520Inputs%26entry.906535625%3DUttaran%2520Bhattacharya%2520and%2520Aniket%2520Bera%2520and%2520Dinesh%2520Manocha%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520multimodal%2520learning-based%2520method%2520to%2520simultaneously%2520synthesize%250Aco-speech%2520facial%2520expressions%2520and%2520upper-body%2520gestures%2520for%2520digital%2520characters%250Ausing%2520RGB%2520video%2520data%2520captured%2520using%2520commodity%2520cameras.%2520Our%2520approach%2520learns%2520from%250Asparse%2520face%2520landmarks%2520and%2520upper-body%2520joints%252C%2520estimated%2520directly%2520from%2520video%250Adata%252C%2520to%2520generate%2520plausible%2520emotive%2520character%2520motions.%2520Given%2520a%2520speech%2520audio%250Awaveform%2520and%2520a%2520token%2520sequence%2520of%2520the%2520speaker%2527s%2520face%2520landmark%2520motion%2520and%250Abody-joint%2520motion%2520computed%2520from%2520a%2520video%252C%2520our%2520method%2520synthesizes%2520the%2520motion%250Asequences%2520for%2520the%2520speaker%2527s%2520face%2520landmarks%2520and%2520body%2520joints%2520to%2520match%2520the%2520content%250Aand%2520the%2520affect%2520of%2520the%2520speech.%2520We%2520design%2520a%2520generator%2520consisting%2520of%2520a%2520set%2520of%250Aencoders%2520to%2520transform%2520all%2520the%2520inputs%2520into%2520a%2520multimodal%2520embedding%2520space%250Acapturing%2520their%2520correlations%252C%2520followed%2520by%2520a%2520pair%2520of%2520decoders%2520to%2520synthesize%2520the%250Adesired%2520face%2520and%2520pose%2520motions.%2520To%2520enhance%2520the%2520plausibility%2520of%2520synthesis%252C%2520we%2520use%250Aan%2520adversarial%2520discriminator%2520that%2520learns%2520to%2520differentiate%2520between%2520the%2520face%2520and%250Apose%2520motions%2520computed%2520from%2520the%2520original%2520videos%2520and%2520our%2520synthesized%2520motions%250Abased%2520on%2520their%2520affective%2520expressions.%2520To%2520evaluate%2520our%2520approach%252C%2520we%2520extend%2520the%250ATED%2520Gesture%2520Dataset%2520to%2520include%2520view-normalized%252C%2520co-speech%2520face%2520landmarks%2520in%250Aaddition%2520to%2520body%2520gestures.%2520We%2520demonstrate%2520the%2520performance%2520of%2520our%2520method%2520through%250Athorough%2520quantitative%2520and%2520qualitative%2520experiments%2520on%2520multiple%2520evaluation%250Ametrics%2520and%2520via%2520a%2520user%2520study.%2520We%2520observe%2520that%2520our%2520method%2520results%2520in%2520low%250Areconstruction%2520error%2520and%2520produces%2520synthesized%2520samples%2520with%2520diverse%2520facial%250Aexpressions%2520and%2520body%2520gestures%2520for%2520digital%2520characters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18068v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Speech2UnifiedExpressions%3A%20Synchronous%20Synthesis%20of%20Co-Speech%20Affective%0A%20%20Face%20and%20Body%20Expressions%20from%20Affordable%20Inputs&entry.906535625=Uttaran%20Bhattacharya%20and%20Aniket%20Bera%20and%20Dinesh%20Manocha&entry.1292438233=%20%20We%20present%20a%20multimodal%20learning-based%20method%20to%20simultaneously%20synthesize%0Aco-speech%20facial%20expressions%20and%20upper-body%20gestures%20for%20digital%20characters%0Ausing%20RGB%20video%20data%20captured%20using%20commodity%20cameras.%20Our%20approach%20learns%20from%0Asparse%20face%20landmarks%20and%20upper-body%20joints%2C%20estimated%20directly%20from%20video%0Adata%2C%20to%20generate%20plausible%20emotive%20character%20motions.%20Given%20a%20speech%20audio%0Awaveform%20and%20a%20token%20sequence%20of%20the%20speaker%27s%20face%20landmark%20motion%20and%0Abody-joint%20motion%20computed%20from%20a%20video%2C%20our%20method%20synthesizes%20the%20motion%0Asequences%20for%20the%20speaker%27s%20face%20landmarks%20and%20body%20joints%20to%20match%20the%20content%0Aand%20the%20affect%20of%20the%20speech.%20We%20design%20a%20generator%20consisting%20of%20a%20set%20of%0Aencoders%20to%20transform%20all%20the%20inputs%20into%20a%20multimodal%20embedding%20space%0Acapturing%20their%20correlations%2C%20followed%20by%20a%20pair%20of%20decoders%20to%20synthesize%20the%0Adesired%20face%20and%20pose%20motions.%20To%20enhance%20the%20plausibility%20of%20synthesis%2C%20we%20use%0Aan%20adversarial%20discriminator%20that%20learns%20to%20differentiate%20between%20the%20face%20and%0Apose%20motions%20computed%20from%20the%20original%20videos%20and%20our%20synthesized%20motions%0Abased%20on%20their%20affective%20expressions.%20To%20evaluate%20our%20approach%2C%20we%20extend%20the%0ATED%20Gesture%20Dataset%20to%20include%20view-normalized%2C%20co-speech%20face%20landmarks%20in%0Aaddition%20to%20body%20gestures.%20We%20demonstrate%20the%20performance%20of%20our%20method%20through%0Athorough%20quantitative%20and%20qualitative%20experiments%20on%20multiple%20evaluation%0Ametrics%20and%20via%20a%20user%20study.%20We%20observe%20that%20our%20method%20results%20in%20low%0Areconstruction%20error%20and%20produces%20synthesized%20samples%20with%20diverse%20facial%0Aexpressions%20and%20body%20gestures%20for%20digital%20characters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18068v1&entry.124074799=Read"},
{"title": "Towards Zero-Shot Interpretable Human Recognition: A 2D-3D Registration\n  Framework", "author": "Henrique Jesus and Hugo Proen\u00e7a", "abstract": "  Large vision models based in deep learning architectures have been\nconsistently advancing the state-of-the-art in biometric recognition. However,\nthree weaknesses are commonly reported for such kind of approaches: 1) their\nextreme demands in terms of learning data; 2) the difficulties in generalising\nbetween different domains; and 3) the lack of interpretability/explainability,\nwith biometrics being of particular interest, as it is important to provide\nevidence able to be used for forensics/legal purposes (e.g., in courts). To the\nbest of our knowledge, this paper describes the first recognition\nframework/strategy that aims at addressing the three weaknesses simultaneously.\nAt first, it relies exclusively in synthetic samples for learning purposes.\nInstead of requiring a large amount and variety of samples for each subject,\nthe idea is to exclusively enroll a 3D point cloud per identity. Then, using\ngenerative strategies, we synthesize a very large (potentially infinite) number\nof samples, containing all the desired covariates (poses, clothing, distances,\nperspectives, lighting, occlusions,...). Upon the synthesizing method used, it\nis possible to adapt precisely to different kind of domains, which accounts for\ngeneralization purposes. Such data are then used to learn a model that performs\nlocal registration between image pairs, establishing positive correspondences\nbetween body parts that are the key, not only to recognition (according to\ncardinality and distribution), but also to provide an interpretable description\nof the response (e.g.: \"both samples are from the same person, as they have\nsimilar facial shape, hair color and legs thickness\").\n", "link": "http://arxiv.org/abs/2403.06658v2", "date": "2024-06-26", "relevancy": 2.9169, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6019}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5747}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5736}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Zero-Shot%20Interpretable%20Human%20Recognition%3A%20A%202D-3D%20Registration%0A%20%20Framework&body=Title%3A%20Towards%20Zero-Shot%20Interpretable%20Human%20Recognition%3A%20A%202D-3D%20Registration%0A%20%20Framework%0AAuthor%3A%20Henrique%20Jesus%20and%20Hugo%20Proen%C3%A7a%0AAbstract%3A%20%20%20Large%20vision%20models%20based%20in%20deep%20learning%20architectures%20have%20been%0Aconsistently%20advancing%20the%20state-of-the-art%20in%20biometric%20recognition.%20However%2C%0Athree%20weaknesses%20are%20commonly%20reported%20for%20such%20kind%20of%20approaches%3A%201%29%20their%0Aextreme%20demands%20in%20terms%20of%20learning%20data%3B%202%29%20the%20difficulties%20in%20generalising%0Abetween%20different%20domains%3B%20and%203%29%20the%20lack%20of%20interpretability/explainability%2C%0Awith%20biometrics%20being%20of%20particular%20interest%2C%20as%20it%20is%20important%20to%20provide%0Aevidence%20able%20to%20be%20used%20for%20forensics/legal%20purposes%20%28e.g.%2C%20in%20courts%29.%20To%20the%0Abest%20of%20our%20knowledge%2C%20this%20paper%20describes%20the%20first%20recognition%0Aframework/strategy%20that%20aims%20at%20addressing%20the%20three%20weaknesses%20simultaneously.%0AAt%20first%2C%20it%20relies%20exclusively%20in%20synthetic%20samples%20for%20learning%20purposes.%0AInstead%20of%20requiring%20a%20large%20amount%20and%20variety%20of%20samples%20for%20each%20subject%2C%0Athe%20idea%20is%20to%20exclusively%20enroll%20a%203D%20point%20cloud%20per%20identity.%20Then%2C%20using%0Agenerative%20strategies%2C%20we%20synthesize%20a%20very%20large%20%28potentially%20infinite%29%20number%0Aof%20samples%2C%20containing%20all%20the%20desired%20covariates%20%28poses%2C%20clothing%2C%20distances%2C%0Aperspectives%2C%20lighting%2C%20occlusions%2C...%29.%20Upon%20the%20synthesizing%20method%20used%2C%20it%0Ais%20possible%20to%20adapt%20precisely%20to%20different%20kind%20of%20domains%2C%20which%20accounts%20for%0Ageneralization%20purposes.%20Such%20data%20are%20then%20used%20to%20learn%20a%20model%20that%20performs%0Alocal%20registration%20between%20image%20pairs%2C%20establishing%20positive%20correspondences%0Abetween%20body%20parts%20that%20are%20the%20key%2C%20not%20only%20to%20recognition%20%28according%20to%0Acardinality%20and%20distribution%29%2C%20but%20also%20to%20provide%20an%20interpretable%20description%0Aof%20the%20response%20%28e.g.%3A%20%22both%20samples%20are%20from%20the%20same%20person%2C%20as%20they%20have%0Asimilar%20facial%20shape%2C%20hair%20color%20and%20legs%20thickness%22%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06658v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Zero-Shot%2520Interpretable%2520Human%2520Recognition%253A%2520A%25202D-3D%2520Registration%250A%2520%2520Framework%26entry.906535625%3DHenrique%2520Jesus%2520and%2520Hugo%2520Proen%25C3%25A7a%26entry.1292438233%3D%2520%2520Large%2520vision%2520models%2520based%2520in%2520deep%2520learning%2520architectures%2520have%2520been%250Aconsistently%2520advancing%2520the%2520state-of-the-art%2520in%2520biometric%2520recognition.%2520However%252C%250Athree%2520weaknesses%2520are%2520commonly%2520reported%2520for%2520such%2520kind%2520of%2520approaches%253A%25201%2529%2520their%250Aextreme%2520demands%2520in%2520terms%2520of%2520learning%2520data%253B%25202%2529%2520the%2520difficulties%2520in%2520generalising%250Abetween%2520different%2520domains%253B%2520and%25203%2529%2520the%2520lack%2520of%2520interpretability/explainability%252C%250Awith%2520biometrics%2520being%2520of%2520particular%2520interest%252C%2520as%2520it%2520is%2520important%2520to%2520provide%250Aevidence%2520able%2520to%2520be%2520used%2520for%2520forensics/legal%2520purposes%2520%2528e.g.%252C%2520in%2520courts%2529.%2520To%2520the%250Abest%2520of%2520our%2520knowledge%252C%2520this%2520paper%2520describes%2520the%2520first%2520recognition%250Aframework/strategy%2520that%2520aims%2520at%2520addressing%2520the%2520three%2520weaknesses%2520simultaneously.%250AAt%2520first%252C%2520it%2520relies%2520exclusively%2520in%2520synthetic%2520samples%2520for%2520learning%2520purposes.%250AInstead%2520of%2520requiring%2520a%2520large%2520amount%2520and%2520variety%2520of%2520samples%2520for%2520each%2520subject%252C%250Athe%2520idea%2520is%2520to%2520exclusively%2520enroll%2520a%25203D%2520point%2520cloud%2520per%2520identity.%2520Then%252C%2520using%250Agenerative%2520strategies%252C%2520we%2520synthesize%2520a%2520very%2520large%2520%2528potentially%2520infinite%2529%2520number%250Aof%2520samples%252C%2520containing%2520all%2520the%2520desired%2520covariates%2520%2528poses%252C%2520clothing%252C%2520distances%252C%250Aperspectives%252C%2520lighting%252C%2520occlusions%252C...%2529.%2520Upon%2520the%2520synthesizing%2520method%2520used%252C%2520it%250Ais%2520possible%2520to%2520adapt%2520precisely%2520to%2520different%2520kind%2520of%2520domains%252C%2520which%2520accounts%2520for%250Ageneralization%2520purposes.%2520Such%2520data%2520are%2520then%2520used%2520to%2520learn%2520a%2520model%2520that%2520performs%250Alocal%2520registration%2520between%2520image%2520pairs%252C%2520establishing%2520positive%2520correspondences%250Abetween%2520body%2520parts%2520that%2520are%2520the%2520key%252C%2520not%2520only%2520to%2520recognition%2520%2528according%2520to%250Acardinality%2520and%2520distribution%2529%252C%2520but%2520also%2520to%2520provide%2520an%2520interpretable%2520description%250Aof%2520the%2520response%2520%2528e.g.%253A%2520%2522both%2520samples%2520are%2520from%2520the%2520same%2520person%252C%2520as%2520they%2520have%250Asimilar%2520facial%2520shape%252C%2520hair%2520color%2520and%2520legs%2520thickness%2522%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.06658v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Zero-Shot%20Interpretable%20Human%20Recognition%3A%20A%202D-3D%20Registration%0A%20%20Framework&entry.906535625=Henrique%20Jesus%20and%20Hugo%20Proen%C3%A7a&entry.1292438233=%20%20Large%20vision%20models%20based%20in%20deep%20learning%20architectures%20have%20been%0Aconsistently%20advancing%20the%20state-of-the-art%20in%20biometric%20recognition.%20However%2C%0Athree%20weaknesses%20are%20commonly%20reported%20for%20such%20kind%20of%20approaches%3A%201%29%20their%0Aextreme%20demands%20in%20terms%20of%20learning%20data%3B%202%29%20the%20difficulties%20in%20generalising%0Abetween%20different%20domains%3B%20and%203%29%20the%20lack%20of%20interpretability/explainability%2C%0Awith%20biometrics%20being%20of%20particular%20interest%2C%20as%20it%20is%20important%20to%20provide%0Aevidence%20able%20to%20be%20used%20for%20forensics/legal%20purposes%20%28e.g.%2C%20in%20courts%29.%20To%20the%0Abest%20of%20our%20knowledge%2C%20this%20paper%20describes%20the%20first%20recognition%0Aframework/strategy%20that%20aims%20at%20addressing%20the%20three%20weaknesses%20simultaneously.%0AAt%20first%2C%20it%20relies%20exclusively%20in%20synthetic%20samples%20for%20learning%20purposes.%0AInstead%20of%20requiring%20a%20large%20amount%20and%20variety%20of%20samples%20for%20each%20subject%2C%0Athe%20idea%20is%20to%20exclusively%20enroll%20a%203D%20point%20cloud%20per%20identity.%20Then%2C%20using%0Agenerative%20strategies%2C%20we%20synthesize%20a%20very%20large%20%28potentially%20infinite%29%20number%0Aof%20samples%2C%20containing%20all%20the%20desired%20covariates%20%28poses%2C%20clothing%2C%20distances%2C%0Aperspectives%2C%20lighting%2C%20occlusions%2C...%29.%20Upon%20the%20synthesizing%20method%20used%2C%20it%0Ais%20possible%20to%20adapt%20precisely%20to%20different%20kind%20of%20domains%2C%20which%20accounts%20for%0Ageneralization%20purposes.%20Such%20data%20are%20then%20used%20to%20learn%20a%20model%20that%20performs%0Alocal%20registration%20between%20image%20pairs%2C%20establishing%20positive%20correspondences%0Abetween%20body%20parts%20that%20are%20the%20key%2C%20not%20only%20to%20recognition%20%28according%20to%0Acardinality%20and%20distribution%29%2C%20but%20also%20to%20provide%20an%20interpretable%20description%0Aof%20the%20response%20%28e.g.%3A%20%22both%20samples%20are%20from%20the%20same%20person%2C%20as%20they%20have%0Asimilar%20facial%20shape%2C%20hair%20color%20and%20legs%20thickness%22%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06658v2&entry.124074799=Read"},
{"title": "DoubleTake: Geometry Guided Depth Estimation", "author": "Mohamed Sayed and Filippo Aleotti and Jamie Watson and Zawar Qureshi and Guillermo Garcia-Hernando and Gabriel Brostow and Sara Vicente and Michael Firman", "abstract": "  Estimating depth from a sequence of posed RGB images is a fundamental\ncomputer vision task, with applications in augmented reality, path planning\netc. Prior work typically makes use of previous frames in a multi view stereo\nframework, relying on matching textures in a local neighborhood. In contrast,\nour model leverages historical predictions by giving the latest 3D geometry\ndata as an extra input to our network. This self-generated geometric hint can\nencode information from areas of the scene not covered by the keyframes and it\nis more regularized when compared to individual predicted depth maps for\nprevious frames. We introduce a Hint MLP which combines cost volume features\nwith a hint of the prior geometry, rendered as a depth map from the current\ncamera location, together with a measure of the confidence in the prior\ngeometry. We demonstrate that our method, which can run at interactive speeds,\nachieves state-of-the-art estimates of depth and 3D scene reconstruction in\nboth offline and incremental evaluation scenarios.\n", "link": "http://arxiv.org/abs/2406.18387v1", "date": "2024-06-26", "relevancy": 2.9128, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5962}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.576}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DoubleTake%3A%20Geometry%20Guided%20Depth%20Estimation&body=Title%3A%20DoubleTake%3A%20Geometry%20Guided%20Depth%20Estimation%0AAuthor%3A%20Mohamed%20Sayed%20and%20Filippo%20Aleotti%20and%20Jamie%20Watson%20and%20Zawar%20Qureshi%20and%20Guillermo%20Garcia-Hernando%20and%20Gabriel%20Brostow%20and%20Sara%20Vicente%20and%20Michael%20Firman%0AAbstract%3A%20%20%20Estimating%20depth%20from%20a%20sequence%20of%20posed%20RGB%20images%20is%20a%20fundamental%0Acomputer%20vision%20task%2C%20with%20applications%20in%20augmented%20reality%2C%20path%20planning%0Aetc.%20Prior%20work%20typically%20makes%20use%20of%20previous%20frames%20in%20a%20multi%20view%20stereo%0Aframework%2C%20relying%20on%20matching%20textures%20in%20a%20local%20neighborhood.%20In%20contrast%2C%0Aour%20model%20leverages%20historical%20predictions%20by%20giving%20the%20latest%203D%20geometry%0Adata%20as%20an%20extra%20input%20to%20our%20network.%20This%20self-generated%20geometric%20hint%20can%0Aencode%20information%20from%20areas%20of%20the%20scene%20not%20covered%20by%20the%20keyframes%20and%20it%0Ais%20more%20regularized%20when%20compared%20to%20individual%20predicted%20depth%20maps%20for%0Aprevious%20frames.%20We%20introduce%20a%20Hint%20MLP%20which%20combines%20cost%20volume%20features%0Awith%20a%20hint%20of%20the%20prior%20geometry%2C%20rendered%20as%20a%20depth%20map%20from%20the%20current%0Acamera%20location%2C%20together%20with%20a%20measure%20of%20the%20confidence%20in%20the%20prior%0Ageometry.%20We%20demonstrate%20that%20our%20method%2C%20which%20can%20run%20at%20interactive%20speeds%2C%0Aachieves%20state-of-the-art%20estimates%20of%20depth%20and%203D%20scene%20reconstruction%20in%0Aboth%20offline%20and%20incremental%20evaluation%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18387v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoubleTake%253A%2520Geometry%2520Guided%2520Depth%2520Estimation%26entry.906535625%3DMohamed%2520Sayed%2520and%2520Filippo%2520Aleotti%2520and%2520Jamie%2520Watson%2520and%2520Zawar%2520Qureshi%2520and%2520Guillermo%2520Garcia-Hernando%2520and%2520Gabriel%2520Brostow%2520and%2520Sara%2520Vicente%2520and%2520Michael%2520Firman%26entry.1292438233%3D%2520%2520Estimating%2520depth%2520from%2520a%2520sequence%2520of%2520posed%2520RGB%2520images%2520is%2520a%2520fundamental%250Acomputer%2520vision%2520task%252C%2520with%2520applications%2520in%2520augmented%2520reality%252C%2520path%2520planning%250Aetc.%2520Prior%2520work%2520typically%2520makes%2520use%2520of%2520previous%2520frames%2520in%2520a%2520multi%2520view%2520stereo%250Aframework%252C%2520relying%2520on%2520matching%2520textures%2520in%2520a%2520local%2520neighborhood.%2520In%2520contrast%252C%250Aour%2520model%2520leverages%2520historical%2520predictions%2520by%2520giving%2520the%2520latest%25203D%2520geometry%250Adata%2520as%2520an%2520extra%2520input%2520to%2520our%2520network.%2520This%2520self-generated%2520geometric%2520hint%2520can%250Aencode%2520information%2520from%2520areas%2520of%2520the%2520scene%2520not%2520covered%2520by%2520the%2520keyframes%2520and%2520it%250Ais%2520more%2520regularized%2520when%2520compared%2520to%2520individual%2520predicted%2520depth%2520maps%2520for%250Aprevious%2520frames.%2520We%2520introduce%2520a%2520Hint%2520MLP%2520which%2520combines%2520cost%2520volume%2520features%250Awith%2520a%2520hint%2520of%2520the%2520prior%2520geometry%252C%2520rendered%2520as%2520a%2520depth%2520map%2520from%2520the%2520current%250Acamera%2520location%252C%2520together%2520with%2520a%2520measure%2520of%2520the%2520confidence%2520in%2520the%2520prior%250Ageometry.%2520We%2520demonstrate%2520that%2520our%2520method%252C%2520which%2520can%2520run%2520at%2520interactive%2520speeds%252C%250Aachieves%2520state-of-the-art%2520estimates%2520of%2520depth%2520and%25203D%2520scene%2520reconstruction%2520in%250Aboth%2520offline%2520and%2520incremental%2520evaluation%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18387v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DoubleTake%3A%20Geometry%20Guided%20Depth%20Estimation&entry.906535625=Mohamed%20Sayed%20and%20Filippo%20Aleotti%20and%20Jamie%20Watson%20and%20Zawar%20Qureshi%20and%20Guillermo%20Garcia-Hernando%20and%20Gabriel%20Brostow%20and%20Sara%20Vicente%20and%20Michael%20Firman&entry.1292438233=%20%20Estimating%20depth%20from%20a%20sequence%20of%20posed%20RGB%20images%20is%20a%20fundamental%0Acomputer%20vision%20task%2C%20with%20applications%20in%20augmented%20reality%2C%20path%20planning%0Aetc.%20Prior%20work%20typically%20makes%20use%20of%20previous%20frames%20in%20a%20multi%20view%20stereo%0Aframework%2C%20relying%20on%20matching%20textures%20in%20a%20local%20neighborhood.%20In%20contrast%2C%0Aour%20model%20leverages%20historical%20predictions%20by%20giving%20the%20latest%203D%20geometry%0Adata%20as%20an%20extra%20input%20to%20our%20network.%20This%20self-generated%20geometric%20hint%20can%0Aencode%20information%20from%20areas%20of%20the%20scene%20not%20covered%20by%20the%20keyframes%20and%20it%0Ais%20more%20regularized%20when%20compared%20to%20individual%20predicted%20depth%20maps%20for%0Aprevious%20frames.%20We%20introduce%20a%20Hint%20MLP%20which%20combines%20cost%20volume%20features%0Awith%20a%20hint%20of%20the%20prior%20geometry%2C%20rendered%20as%20a%20depth%20map%20from%20the%20current%0Acamera%20location%2C%20together%20with%20a%20measure%20of%20the%20confidence%20in%20the%20prior%0Ageometry.%20We%20demonstrate%20that%20our%20method%2C%20which%20can%20run%20at%20interactive%20speeds%2C%0Aachieves%20state-of-the-art%20estimates%20of%20depth%20and%203D%20scene%20reconstruction%20in%0Aboth%20offline%20and%20incremental%20evaluation%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18387v1&entry.124074799=Read"},
{"title": "View-Invariant Pixelwise Anomaly Detection in Multi-object Scenes with\n  Adaptive View Synthesis", "author": "Subin Varghese and Vedhus Hoskere", "abstract": "  The inspection and monitoring of infrastructure assets typically requires\nidentifying visual anomalies in scenes periodically photographed over time.\nImages collected manually or with robots such as unmanned aerial vehicles from\nthe same scene at different instances in time are typically not perfectly\naligned. Supervised segmentation methods can be applied to identify known\nproblems, but unsupervised anomaly detection approaches are required when\nunknown anomalies occur. Current unsupervised pixel-level anomaly detection\nmethods have mainly been developed for industrial settings where the camera\nposition is known and constant. However, we find that these methods fail to\ngeneralize to the case when images are not perfectly aligned. We term the\nproblem of unsupervised anomaly detection between two such imperfectly aligned\nsets of images as Scene Anomaly Detection (Scene AD). We present a novel\nnetwork termed OmniAD to address the Scene AD problem posed. Specifically, we\nrefine the anomaly detection method reverse distillation to achieve a 40%\nincrease in pixel-level anomaly detection performance. The network's\nperformance is further demonstrated to improve with two new data augmentation\nstrategies proposed that leverage novel view synthesis and camera localization\nto improve generalization. We validate our approach with qualitative and\nquantitative results on a new dataset, ToyCity, the first Scene AD dataset with\nmultiple objects, as well as on the established single object-centric dataset,\nMAD. https://drags99.github.io/OmniAD/\n", "link": "http://arxiv.org/abs/2406.18012v1", "date": "2024-06-26", "relevancy": 2.8232, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.567}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.567}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20View-Invariant%20Pixelwise%20Anomaly%20Detection%20in%20Multi-object%20Scenes%20with%0A%20%20Adaptive%20View%20Synthesis&body=Title%3A%20View-Invariant%20Pixelwise%20Anomaly%20Detection%20in%20Multi-object%20Scenes%20with%0A%20%20Adaptive%20View%20Synthesis%0AAuthor%3A%20Subin%20Varghese%20and%20Vedhus%20Hoskere%0AAbstract%3A%20%20%20The%20inspection%20and%20monitoring%20of%20infrastructure%20assets%20typically%20requires%0Aidentifying%20visual%20anomalies%20in%20scenes%20periodically%20photographed%20over%20time.%0AImages%20collected%20manually%20or%20with%20robots%20such%20as%20unmanned%20aerial%20vehicles%20from%0Athe%20same%20scene%20at%20different%20instances%20in%20time%20are%20typically%20not%20perfectly%0Aaligned.%20Supervised%20segmentation%20methods%20can%20be%20applied%20to%20identify%20known%0Aproblems%2C%20but%20unsupervised%20anomaly%20detection%20approaches%20are%20required%20when%0Aunknown%20anomalies%20occur.%20Current%20unsupervised%20pixel-level%20anomaly%20detection%0Amethods%20have%20mainly%20been%20developed%20for%20industrial%20settings%20where%20the%20camera%0Aposition%20is%20known%20and%20constant.%20However%2C%20we%20find%20that%20these%20methods%20fail%20to%0Ageneralize%20to%20the%20case%20when%20images%20are%20not%20perfectly%20aligned.%20We%20term%20the%0Aproblem%20of%20unsupervised%20anomaly%20detection%20between%20two%20such%20imperfectly%20aligned%0Asets%20of%20images%20as%20Scene%20Anomaly%20Detection%20%28Scene%20AD%29.%20We%20present%20a%20novel%0Anetwork%20termed%20OmniAD%20to%20address%20the%20Scene%20AD%20problem%20posed.%20Specifically%2C%20we%0Arefine%20the%20anomaly%20detection%20method%20reverse%20distillation%20to%20achieve%20a%2040%25%0Aincrease%20in%20pixel-level%20anomaly%20detection%20performance.%20The%20network%27s%0Aperformance%20is%20further%20demonstrated%20to%20improve%20with%20two%20new%20data%20augmentation%0Astrategies%20proposed%20that%20leverage%20novel%20view%20synthesis%20and%20camera%20localization%0Ato%20improve%20generalization.%20We%20validate%20our%20approach%20with%20qualitative%20and%0Aquantitative%20results%20on%20a%20new%20dataset%2C%20ToyCity%2C%20the%20first%20Scene%20AD%20dataset%20with%0Amultiple%20objects%2C%20as%20well%20as%20on%20the%20established%20single%20object-centric%20dataset%2C%0AMAD.%20https%3A//drags99.github.io/OmniAD/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18012v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DView-Invariant%2520Pixelwise%2520Anomaly%2520Detection%2520in%2520Multi-object%2520Scenes%2520with%250A%2520%2520Adaptive%2520View%2520Synthesis%26entry.906535625%3DSubin%2520Varghese%2520and%2520Vedhus%2520Hoskere%26entry.1292438233%3D%2520%2520The%2520inspection%2520and%2520monitoring%2520of%2520infrastructure%2520assets%2520typically%2520requires%250Aidentifying%2520visual%2520anomalies%2520in%2520scenes%2520periodically%2520photographed%2520over%2520time.%250AImages%2520collected%2520manually%2520or%2520with%2520robots%2520such%2520as%2520unmanned%2520aerial%2520vehicles%2520from%250Athe%2520same%2520scene%2520at%2520different%2520instances%2520in%2520time%2520are%2520typically%2520not%2520perfectly%250Aaligned.%2520Supervised%2520segmentation%2520methods%2520can%2520be%2520applied%2520to%2520identify%2520known%250Aproblems%252C%2520but%2520unsupervised%2520anomaly%2520detection%2520approaches%2520are%2520required%2520when%250Aunknown%2520anomalies%2520occur.%2520Current%2520unsupervised%2520pixel-level%2520anomaly%2520detection%250Amethods%2520have%2520mainly%2520been%2520developed%2520for%2520industrial%2520settings%2520where%2520the%2520camera%250Aposition%2520is%2520known%2520and%2520constant.%2520However%252C%2520we%2520find%2520that%2520these%2520methods%2520fail%2520to%250Ageneralize%2520to%2520the%2520case%2520when%2520images%2520are%2520not%2520perfectly%2520aligned.%2520We%2520term%2520the%250Aproblem%2520of%2520unsupervised%2520anomaly%2520detection%2520between%2520two%2520such%2520imperfectly%2520aligned%250Asets%2520of%2520images%2520as%2520Scene%2520Anomaly%2520Detection%2520%2528Scene%2520AD%2529.%2520We%2520present%2520a%2520novel%250Anetwork%2520termed%2520OmniAD%2520to%2520address%2520the%2520Scene%2520AD%2520problem%2520posed.%2520Specifically%252C%2520we%250Arefine%2520the%2520anomaly%2520detection%2520method%2520reverse%2520distillation%2520to%2520achieve%2520a%252040%2525%250Aincrease%2520in%2520pixel-level%2520anomaly%2520detection%2520performance.%2520The%2520network%2527s%250Aperformance%2520is%2520further%2520demonstrated%2520to%2520improve%2520with%2520two%2520new%2520data%2520augmentation%250Astrategies%2520proposed%2520that%2520leverage%2520novel%2520view%2520synthesis%2520and%2520camera%2520localization%250Ato%2520improve%2520generalization.%2520We%2520validate%2520our%2520approach%2520with%2520qualitative%2520and%250Aquantitative%2520results%2520on%2520a%2520new%2520dataset%252C%2520ToyCity%252C%2520the%2520first%2520Scene%2520AD%2520dataset%2520with%250Amultiple%2520objects%252C%2520as%2520well%2520as%2520on%2520the%2520established%2520single%2520object-centric%2520dataset%252C%250AMAD.%2520https%253A//drags99.github.io/OmniAD/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18012v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=View-Invariant%20Pixelwise%20Anomaly%20Detection%20in%20Multi-object%20Scenes%20with%0A%20%20Adaptive%20View%20Synthesis&entry.906535625=Subin%20Varghese%20and%20Vedhus%20Hoskere&entry.1292438233=%20%20The%20inspection%20and%20monitoring%20of%20infrastructure%20assets%20typically%20requires%0Aidentifying%20visual%20anomalies%20in%20scenes%20periodically%20photographed%20over%20time.%0AImages%20collected%20manually%20or%20with%20robots%20such%20as%20unmanned%20aerial%20vehicles%20from%0Athe%20same%20scene%20at%20different%20instances%20in%20time%20are%20typically%20not%20perfectly%0Aaligned.%20Supervised%20segmentation%20methods%20can%20be%20applied%20to%20identify%20known%0Aproblems%2C%20but%20unsupervised%20anomaly%20detection%20approaches%20are%20required%20when%0Aunknown%20anomalies%20occur.%20Current%20unsupervised%20pixel-level%20anomaly%20detection%0Amethods%20have%20mainly%20been%20developed%20for%20industrial%20settings%20where%20the%20camera%0Aposition%20is%20known%20and%20constant.%20However%2C%20we%20find%20that%20these%20methods%20fail%20to%0Ageneralize%20to%20the%20case%20when%20images%20are%20not%20perfectly%20aligned.%20We%20term%20the%0Aproblem%20of%20unsupervised%20anomaly%20detection%20between%20two%20such%20imperfectly%20aligned%0Asets%20of%20images%20as%20Scene%20Anomaly%20Detection%20%28Scene%20AD%29.%20We%20present%20a%20novel%0Anetwork%20termed%20OmniAD%20to%20address%20the%20Scene%20AD%20problem%20posed.%20Specifically%2C%20we%0Arefine%20the%20anomaly%20detection%20method%20reverse%20distillation%20to%20achieve%20a%2040%25%0Aincrease%20in%20pixel-level%20anomaly%20detection%20performance.%20The%20network%27s%0Aperformance%20is%20further%20demonstrated%20to%20improve%20with%20two%20new%20data%20augmentation%0Astrategies%20proposed%20that%20leverage%20novel%20view%20synthesis%20and%20camera%20localization%0Ato%20improve%20generalization.%20We%20validate%20our%20approach%20with%20qualitative%20and%0Aquantitative%20results%20on%20a%20new%20dataset%2C%20ToyCity%2C%20the%20first%20Scene%20AD%20dataset%20with%0Amultiple%20objects%2C%20as%20well%20as%20on%20the%20established%20single%20object-centric%20dataset%2C%0AMAD.%20https%3A//drags99.github.io/OmniAD/%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18012v1&entry.124074799=Read"},
{"title": "SynRS3D: A Synthetic Dataset for Global 3D Semantic Understanding from\n  Monocular Remote Sensing Imagery", "author": "Jian Song and Hongruixuan Chen and Weihao Xuan and Junshi Xia and Naoto Yokoya", "abstract": "  Global semantic 3D understanding from single-view high-resolution remote\nsensing (RS) imagery is crucial for Earth Observation (EO). However, this task\nfaces significant challenges due to the high costs of annotations and data\ncollection, as well as geographically restricted data availability. To address\nthese challenges, synthetic data offer a promising solution by being easily\naccessible and thus enabling the provision of large and diverse datasets. We\ndevelop a specialized synthetic data generation pipeline for EO and introduce\nSynRS3D, the largest synthetic RS 3D dataset. SynRS3D comprises 69,667\nhigh-resolution optical images that cover six different city styles worldwide\nand feature eight land cover types, precise height information, and building\nchange masks. To further enhance its utility, we develop a novel multi-task\nunsupervised domain adaptation (UDA) method, RS3DAda, coupled with our\nsynthetic dataset, which facilitates the RS-specific transition from synthetic\nto real scenarios for land cover mapping and height estimation tasks,\nultimately enabling global monocular 3D semantic understanding based on\nsynthetic data. Extensive experiments on various real-world datasets\ndemonstrate the adaptability and effectiveness of our synthetic dataset and\nproposed RS3DAda method. SynRS3D and related codes will be available.\n", "link": "http://arxiv.org/abs/2406.18151v1", "date": "2024-06-26", "relevancy": 2.8173, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5806}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5549}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SynRS3D%3A%20A%20Synthetic%20Dataset%20for%20Global%203D%20Semantic%20Understanding%20from%0A%20%20Monocular%20Remote%20Sensing%20Imagery&body=Title%3A%20SynRS3D%3A%20A%20Synthetic%20Dataset%20for%20Global%203D%20Semantic%20Understanding%20from%0A%20%20Monocular%20Remote%20Sensing%20Imagery%0AAuthor%3A%20Jian%20Song%20and%20Hongruixuan%20Chen%20and%20Weihao%20Xuan%20and%20Junshi%20Xia%20and%20Naoto%20Yokoya%0AAbstract%3A%20%20%20Global%20semantic%203D%20understanding%20from%20single-view%20high-resolution%20remote%0Asensing%20%28RS%29%20imagery%20is%20crucial%20for%20Earth%20Observation%20%28EO%29.%20However%2C%20this%20task%0Afaces%20significant%20challenges%20due%20to%20the%20high%20costs%20of%20annotations%20and%20data%0Acollection%2C%20as%20well%20as%20geographically%20restricted%20data%20availability.%20To%20address%0Athese%20challenges%2C%20synthetic%20data%20offer%20a%20promising%20solution%20by%20being%20easily%0Aaccessible%20and%20thus%20enabling%20the%20provision%20of%20large%20and%20diverse%20datasets.%20We%0Adevelop%20a%20specialized%20synthetic%20data%20generation%20pipeline%20for%20EO%20and%20introduce%0ASynRS3D%2C%20the%20largest%20synthetic%20RS%203D%20dataset.%20SynRS3D%20comprises%2069%2C667%0Ahigh-resolution%20optical%20images%20that%20cover%20six%20different%20city%20styles%20worldwide%0Aand%20feature%20eight%20land%20cover%20types%2C%20precise%20height%20information%2C%20and%20building%0Achange%20masks.%20To%20further%20enhance%20its%20utility%2C%20we%20develop%20a%20novel%20multi-task%0Aunsupervised%20domain%20adaptation%20%28UDA%29%20method%2C%20RS3DAda%2C%20coupled%20with%20our%0Asynthetic%20dataset%2C%20which%20facilitates%20the%20RS-specific%20transition%20from%20synthetic%0Ato%20real%20scenarios%20for%20land%20cover%20mapping%20and%20height%20estimation%20tasks%2C%0Aultimately%20enabling%20global%20monocular%203D%20semantic%20understanding%20based%20on%0Asynthetic%20data.%20Extensive%20experiments%20on%20various%20real-world%20datasets%0Ademonstrate%20the%20adaptability%20and%20effectiveness%20of%20our%20synthetic%20dataset%20and%0Aproposed%20RS3DAda%20method.%20SynRS3D%20and%20related%20codes%20will%20be%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18151v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynRS3D%253A%2520A%2520Synthetic%2520Dataset%2520for%2520Global%25203D%2520Semantic%2520Understanding%2520from%250A%2520%2520Monocular%2520Remote%2520Sensing%2520Imagery%26entry.906535625%3DJian%2520Song%2520and%2520Hongruixuan%2520Chen%2520and%2520Weihao%2520Xuan%2520and%2520Junshi%2520Xia%2520and%2520Naoto%2520Yokoya%26entry.1292438233%3D%2520%2520Global%2520semantic%25203D%2520understanding%2520from%2520single-view%2520high-resolution%2520remote%250Asensing%2520%2528RS%2529%2520imagery%2520is%2520crucial%2520for%2520Earth%2520Observation%2520%2528EO%2529.%2520However%252C%2520this%2520task%250Afaces%2520significant%2520challenges%2520due%2520to%2520the%2520high%2520costs%2520of%2520annotations%2520and%2520data%250Acollection%252C%2520as%2520well%2520as%2520geographically%2520restricted%2520data%2520availability.%2520To%2520address%250Athese%2520challenges%252C%2520synthetic%2520data%2520offer%2520a%2520promising%2520solution%2520by%2520being%2520easily%250Aaccessible%2520and%2520thus%2520enabling%2520the%2520provision%2520of%2520large%2520and%2520diverse%2520datasets.%2520We%250Adevelop%2520a%2520specialized%2520synthetic%2520data%2520generation%2520pipeline%2520for%2520EO%2520and%2520introduce%250ASynRS3D%252C%2520the%2520largest%2520synthetic%2520RS%25203D%2520dataset.%2520SynRS3D%2520comprises%252069%252C667%250Ahigh-resolution%2520optical%2520images%2520that%2520cover%2520six%2520different%2520city%2520styles%2520worldwide%250Aand%2520feature%2520eight%2520land%2520cover%2520types%252C%2520precise%2520height%2520information%252C%2520and%2520building%250Achange%2520masks.%2520To%2520further%2520enhance%2520its%2520utility%252C%2520we%2520develop%2520a%2520novel%2520multi-task%250Aunsupervised%2520domain%2520adaptation%2520%2528UDA%2529%2520method%252C%2520RS3DAda%252C%2520coupled%2520with%2520our%250Asynthetic%2520dataset%252C%2520which%2520facilitates%2520the%2520RS-specific%2520transition%2520from%2520synthetic%250Ato%2520real%2520scenarios%2520for%2520land%2520cover%2520mapping%2520and%2520height%2520estimation%2520tasks%252C%250Aultimately%2520enabling%2520global%2520monocular%25203D%2520semantic%2520understanding%2520based%2520on%250Asynthetic%2520data.%2520Extensive%2520experiments%2520on%2520various%2520real-world%2520datasets%250Ademonstrate%2520the%2520adaptability%2520and%2520effectiveness%2520of%2520our%2520synthetic%2520dataset%2520and%250Aproposed%2520RS3DAda%2520method.%2520SynRS3D%2520and%2520related%2520codes%2520will%2520be%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18151v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SynRS3D%3A%20A%20Synthetic%20Dataset%20for%20Global%203D%20Semantic%20Understanding%20from%0A%20%20Monocular%20Remote%20Sensing%20Imagery&entry.906535625=Jian%20Song%20and%20Hongruixuan%20Chen%20and%20Weihao%20Xuan%20and%20Junshi%20Xia%20and%20Naoto%20Yokoya&entry.1292438233=%20%20Global%20semantic%203D%20understanding%20from%20single-view%20high-resolution%20remote%0Asensing%20%28RS%29%20imagery%20is%20crucial%20for%20Earth%20Observation%20%28EO%29.%20However%2C%20this%20task%0Afaces%20significant%20challenges%20due%20to%20the%20high%20costs%20of%20annotations%20and%20data%0Acollection%2C%20as%20well%20as%20geographically%20restricted%20data%20availability.%20To%20address%0Athese%20challenges%2C%20synthetic%20data%20offer%20a%20promising%20solution%20by%20being%20easily%0Aaccessible%20and%20thus%20enabling%20the%20provision%20of%20large%20and%20diverse%20datasets.%20We%0Adevelop%20a%20specialized%20synthetic%20data%20generation%20pipeline%20for%20EO%20and%20introduce%0ASynRS3D%2C%20the%20largest%20synthetic%20RS%203D%20dataset.%20SynRS3D%20comprises%2069%2C667%0Ahigh-resolution%20optical%20images%20that%20cover%20six%20different%20city%20styles%20worldwide%0Aand%20feature%20eight%20land%20cover%20types%2C%20precise%20height%20information%2C%20and%20building%0Achange%20masks.%20To%20further%20enhance%20its%20utility%2C%20we%20develop%20a%20novel%20multi-task%0Aunsupervised%20domain%20adaptation%20%28UDA%29%20method%2C%20RS3DAda%2C%20coupled%20with%20our%0Asynthetic%20dataset%2C%20which%20facilitates%20the%20RS-specific%20transition%20from%20synthetic%0Ato%20real%20scenarios%20for%20land%20cover%20mapping%20and%20height%20estimation%20tasks%2C%0Aultimately%20enabling%20global%20monocular%203D%20semantic%20understanding%20based%20on%0Asynthetic%20data.%20Extensive%20experiments%20on%20various%20real-world%20datasets%0Ademonstrate%20the%20adaptability%20and%20effectiveness%20of%20our%20synthetic%20dataset%20and%0Aproposed%20RS3DAda%20method.%20SynRS3D%20and%20related%20codes%20will%20be%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18151v1&entry.124074799=Read"},
{"title": "Mitigate the Gap: Investigating Approaches for Improving Cross-Modal\n  Alignment in CLIP", "author": "Sedigheh Eslami and Gerard de Melo", "abstract": "  Contrastive Language--Image Pre-training (CLIP) has manifested remarkable\nimprovements in zero-shot classification and cross-modal vision-language tasks.\nYet, from a geometrical point of view, the CLIP embedding space has been found\nto have a pronounced modality gap. This gap renders the embedding space overly\nsparse and disconnected, with different modalities being densely distributed in\ndistinct subregions of the hypersphere. In this work, we aim at answering two\nmain questions: 1. Does sharing the parameter space between the multi-modal\nencoders reduce the modality gap? 2. Can the gap be mitigated by pushing apart\nthe uni-modal embeddings via intra-modality separation? We design AlignCLIP, in\norder to answer these questions and show that answers to both questions are\npositive. Through extensive experiments, we show that AlignCLIP achieves\nnoticeable enhancements in the cross-modal alignment of the embeddings, and\nthereby, reduces the modality gap, while maintaining the performance across\nseveral downstream evaluations, such as zero-shot image classification,\nzero-shot multi-modal retrieval and zero-shot semantic text similarity.\n", "link": "http://arxiv.org/abs/2406.17639v2", "date": "2024-06-26", "relevancy": 2.8064, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6705}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5234}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigate%20the%20Gap%3A%20Investigating%20Approaches%20for%20Improving%20Cross-Modal%0A%20%20Alignment%20in%20CLIP&body=Title%3A%20Mitigate%20the%20Gap%3A%20Investigating%20Approaches%20for%20Improving%20Cross-Modal%0A%20%20Alignment%20in%20CLIP%0AAuthor%3A%20Sedigheh%20Eslami%20and%20Gerard%20de%20Melo%0AAbstract%3A%20%20%20Contrastive%20Language--Image%20Pre-training%20%28CLIP%29%20has%20manifested%20remarkable%0Aimprovements%20in%20zero-shot%20classification%20and%20cross-modal%20vision-language%20tasks.%0AYet%2C%20from%20a%20geometrical%20point%20of%20view%2C%20the%20CLIP%20embedding%20space%20has%20been%20found%0Ato%20have%20a%20pronounced%20modality%20gap.%20This%20gap%20renders%20the%20embedding%20space%20overly%0Asparse%20and%20disconnected%2C%20with%20different%20modalities%20being%20densely%20distributed%20in%0Adistinct%20subregions%20of%20the%20hypersphere.%20In%20this%20work%2C%20we%20aim%20at%20answering%20two%0Amain%20questions%3A%201.%20Does%20sharing%20the%20parameter%20space%20between%20the%20multi-modal%0Aencoders%20reduce%20the%20modality%20gap%3F%202.%20Can%20the%20gap%20be%20mitigated%20by%20pushing%20apart%0Athe%20uni-modal%20embeddings%20via%20intra-modality%20separation%3F%20We%20design%20AlignCLIP%2C%20in%0Aorder%20to%20answer%20these%20questions%20and%20show%20that%20answers%20to%20both%20questions%20are%0Apositive.%20Through%20extensive%20experiments%2C%20we%20show%20that%20AlignCLIP%20achieves%0Anoticeable%20enhancements%20in%20the%20cross-modal%20alignment%20of%20the%20embeddings%2C%20and%0Athereby%2C%20reduces%20the%20modality%20gap%2C%20while%20maintaining%20the%20performance%20across%0Aseveral%20downstream%20evaluations%2C%20such%20as%20zero-shot%20image%20classification%2C%0Azero-shot%20multi-modal%20retrieval%20and%20zero-shot%20semantic%20text%20similarity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17639v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigate%2520the%2520Gap%253A%2520Investigating%2520Approaches%2520for%2520Improving%2520Cross-Modal%250A%2520%2520Alignment%2520in%2520CLIP%26entry.906535625%3DSedigheh%2520Eslami%2520and%2520Gerard%2520de%2520Melo%26entry.1292438233%3D%2520%2520Contrastive%2520Language--Image%2520Pre-training%2520%2528CLIP%2529%2520has%2520manifested%2520remarkable%250Aimprovements%2520in%2520zero-shot%2520classification%2520and%2520cross-modal%2520vision-language%2520tasks.%250AYet%252C%2520from%2520a%2520geometrical%2520point%2520of%2520view%252C%2520the%2520CLIP%2520embedding%2520space%2520has%2520been%2520found%250Ato%2520have%2520a%2520pronounced%2520modality%2520gap.%2520This%2520gap%2520renders%2520the%2520embedding%2520space%2520overly%250Asparse%2520and%2520disconnected%252C%2520with%2520different%2520modalities%2520being%2520densely%2520distributed%2520in%250Adistinct%2520subregions%2520of%2520the%2520hypersphere.%2520In%2520this%2520work%252C%2520we%2520aim%2520at%2520answering%2520two%250Amain%2520questions%253A%25201.%2520Does%2520sharing%2520the%2520parameter%2520space%2520between%2520the%2520multi-modal%250Aencoders%2520reduce%2520the%2520modality%2520gap%253F%25202.%2520Can%2520the%2520gap%2520be%2520mitigated%2520by%2520pushing%2520apart%250Athe%2520uni-modal%2520embeddings%2520via%2520intra-modality%2520separation%253F%2520We%2520design%2520AlignCLIP%252C%2520in%250Aorder%2520to%2520answer%2520these%2520questions%2520and%2520show%2520that%2520answers%2520to%2520both%2520questions%2520are%250Apositive.%2520Through%2520extensive%2520experiments%252C%2520we%2520show%2520that%2520AlignCLIP%2520achieves%250Anoticeable%2520enhancements%2520in%2520the%2520cross-modal%2520alignment%2520of%2520the%2520embeddings%252C%2520and%250Athereby%252C%2520reduces%2520the%2520modality%2520gap%252C%2520while%2520maintaining%2520the%2520performance%2520across%250Aseveral%2520downstream%2520evaluations%252C%2520such%2520as%2520zero-shot%2520image%2520classification%252C%250Azero-shot%2520multi-modal%2520retrieval%2520and%2520zero-shot%2520semantic%2520text%2520similarity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17639v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigate%20the%20Gap%3A%20Investigating%20Approaches%20for%20Improving%20Cross-Modal%0A%20%20Alignment%20in%20CLIP&entry.906535625=Sedigheh%20Eslami%20and%20Gerard%20de%20Melo&entry.1292438233=%20%20Contrastive%20Language--Image%20Pre-training%20%28CLIP%29%20has%20manifested%20remarkable%0Aimprovements%20in%20zero-shot%20classification%20and%20cross-modal%20vision-language%20tasks.%0AYet%2C%20from%20a%20geometrical%20point%20of%20view%2C%20the%20CLIP%20embedding%20space%20has%20been%20found%0Ato%20have%20a%20pronounced%20modality%20gap.%20This%20gap%20renders%20the%20embedding%20space%20overly%0Asparse%20and%20disconnected%2C%20with%20different%20modalities%20being%20densely%20distributed%20in%0Adistinct%20subregions%20of%20the%20hypersphere.%20In%20this%20work%2C%20we%20aim%20at%20answering%20two%0Amain%20questions%3A%201.%20Does%20sharing%20the%20parameter%20space%20between%20the%20multi-modal%0Aencoders%20reduce%20the%20modality%20gap%3F%202.%20Can%20the%20gap%20be%20mitigated%20by%20pushing%20apart%0Athe%20uni-modal%20embeddings%20via%20intra-modality%20separation%3F%20We%20design%20AlignCLIP%2C%20in%0Aorder%20to%20answer%20these%20questions%20and%20show%20that%20answers%20to%20both%20questions%20are%0Apositive.%20Through%20extensive%20experiments%2C%20we%20show%20that%20AlignCLIP%20achieves%0Anoticeable%20enhancements%20in%20the%20cross-modal%20alignment%20of%20the%20embeddings%2C%20and%0Athereby%2C%20reduces%20the%20modality%20gap%2C%20while%20maintaining%20the%20performance%20across%0Aseveral%20downstream%20evaluations%2C%20such%20as%20zero-shot%20image%20classification%2C%0Azero-shot%20multi-modal%20retrieval%20and%20zero-shot%20semantic%20text%20similarity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17639v2&entry.124074799=Read"},
{"title": "DeCoF: Generated Video Detection via Frame Consistency: The First\n  Benchmark Dataset", "author": "Long Ma and Jiajia Zhang and Hongping Deng and Ningyu Zhang and Qinglang Guo and Haiyang Yu and Yong Liao and Pengyuan Zhou", "abstract": "  The escalating quality of video generated by advanced video generation\nmethods results in new security challenges, while there have been few relevant\nresearch efforts: 1) There is no open-source dataset for generated video\ndetection, 2) No generated video detection method has been proposed so far. To\nthis end, we propose an open-source dataset and a detection method for\ngenerated video for the first time. First, we propose a scalable dataset\nconsisting of 964 prompts, covering various forgery targets, scenes, behaviors,\nand actions, as well as various generation models with different architectures\nand generation methods, including the most popular commercial models like\nOpenAI's Sora and Google's Veo. Second, we found via probing experiments that\nspatial artifact-based detectors lack generalizability. Hence, we propose a\nsimple yet effective \\textbf{de}tection model based on \\textbf{f}rame\n\\textbf{co}nsistency (\\textbf{DeCoF}), which focuses on temporal artifacts by\neliminating the impact of spatial artifacts during feature learning. Extensive\nexperiments demonstrate the efficacy of DeCoF in detecting videos generated by\nunseen video generation models and confirm its powerful generalizability across\nseveral commercially proprietary models. Our code and dataset will be released\nat \\url{https://github.com/wuwuwuyue/DeCoF}.\n", "link": "http://arxiv.org/abs/2402.02085v4", "date": "2024-06-26", "relevancy": 2.7794, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5766}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5672}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeCoF%3A%20Generated%20Video%20Detection%20via%20Frame%20Consistency%3A%20The%20First%0A%20%20Benchmark%20Dataset&body=Title%3A%20DeCoF%3A%20Generated%20Video%20Detection%20via%20Frame%20Consistency%3A%20The%20First%0A%20%20Benchmark%20Dataset%0AAuthor%3A%20Long%20Ma%20and%20Jiajia%20Zhang%20and%20Hongping%20Deng%20and%20Ningyu%20Zhang%20and%20Qinglang%20Guo%20and%20Haiyang%20Yu%20and%20Yong%20Liao%20and%20Pengyuan%20Zhou%0AAbstract%3A%20%20%20The%20escalating%20quality%20of%20video%20generated%20by%20advanced%20video%20generation%0Amethods%20results%20in%20new%20security%20challenges%2C%20while%20there%20have%20been%20few%20relevant%0Aresearch%20efforts%3A%201%29%20There%20is%20no%20open-source%20dataset%20for%20generated%20video%0Adetection%2C%202%29%20No%20generated%20video%20detection%20method%20has%20been%20proposed%20so%20far.%20To%0Athis%20end%2C%20we%20propose%20an%20open-source%20dataset%20and%20a%20detection%20method%20for%0Agenerated%20video%20for%20the%20first%20time.%20First%2C%20we%20propose%20a%20scalable%20dataset%0Aconsisting%20of%20964%20prompts%2C%20covering%20various%20forgery%20targets%2C%20scenes%2C%20behaviors%2C%0Aand%20actions%2C%20as%20well%20as%20various%20generation%20models%20with%20different%20architectures%0Aand%20generation%20methods%2C%20including%20the%20most%20popular%20commercial%20models%20like%0AOpenAI%27s%20Sora%20and%20Google%27s%20Veo.%20Second%2C%20we%20found%20via%20probing%20experiments%20that%0Aspatial%20artifact-based%20detectors%20lack%20generalizability.%20Hence%2C%20we%20propose%20a%0Asimple%20yet%20effective%20%5Ctextbf%7Bde%7Dtection%20model%20based%20on%20%5Ctextbf%7Bf%7Drame%0A%5Ctextbf%7Bco%7Dnsistency%20%28%5Ctextbf%7BDeCoF%7D%29%2C%20which%20focuses%20on%20temporal%20artifacts%20by%0Aeliminating%20the%20impact%20of%20spatial%20artifacts%20during%20feature%20learning.%20Extensive%0Aexperiments%20demonstrate%20the%20efficacy%20of%20DeCoF%20in%20detecting%20videos%20generated%20by%0Aunseen%20video%20generation%20models%20and%20confirm%20its%20powerful%20generalizability%20across%0Aseveral%20commercially%20proprietary%20models.%20Our%20code%20and%20dataset%20will%20be%20released%0Aat%20%5Curl%7Bhttps%3A//github.com/wuwuwuyue/DeCoF%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.02085v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeCoF%253A%2520Generated%2520Video%2520Detection%2520via%2520Frame%2520Consistency%253A%2520The%2520First%250A%2520%2520Benchmark%2520Dataset%26entry.906535625%3DLong%2520Ma%2520and%2520Jiajia%2520Zhang%2520and%2520Hongping%2520Deng%2520and%2520Ningyu%2520Zhang%2520and%2520Qinglang%2520Guo%2520and%2520Haiyang%2520Yu%2520and%2520Yong%2520Liao%2520and%2520Pengyuan%2520Zhou%26entry.1292438233%3D%2520%2520The%2520escalating%2520quality%2520of%2520video%2520generated%2520by%2520advanced%2520video%2520generation%250Amethods%2520results%2520in%2520new%2520security%2520challenges%252C%2520while%2520there%2520have%2520been%2520few%2520relevant%250Aresearch%2520efforts%253A%25201%2529%2520There%2520is%2520no%2520open-source%2520dataset%2520for%2520generated%2520video%250Adetection%252C%25202%2529%2520No%2520generated%2520video%2520detection%2520method%2520has%2520been%2520proposed%2520so%2520far.%2520To%250Athis%2520end%252C%2520we%2520propose%2520an%2520open-source%2520dataset%2520and%2520a%2520detection%2520method%2520for%250Agenerated%2520video%2520for%2520the%2520first%2520time.%2520First%252C%2520we%2520propose%2520a%2520scalable%2520dataset%250Aconsisting%2520of%2520964%2520prompts%252C%2520covering%2520various%2520forgery%2520targets%252C%2520scenes%252C%2520behaviors%252C%250Aand%2520actions%252C%2520as%2520well%2520as%2520various%2520generation%2520models%2520with%2520different%2520architectures%250Aand%2520generation%2520methods%252C%2520including%2520the%2520most%2520popular%2520commercial%2520models%2520like%250AOpenAI%2527s%2520Sora%2520and%2520Google%2527s%2520Veo.%2520Second%252C%2520we%2520found%2520via%2520probing%2520experiments%2520that%250Aspatial%2520artifact-based%2520detectors%2520lack%2520generalizability.%2520Hence%252C%2520we%2520propose%2520a%250Asimple%2520yet%2520effective%2520%255Ctextbf%257Bde%257Dtection%2520model%2520based%2520on%2520%255Ctextbf%257Bf%257Drame%250A%255Ctextbf%257Bco%257Dnsistency%2520%2528%255Ctextbf%257BDeCoF%257D%2529%252C%2520which%2520focuses%2520on%2520temporal%2520artifacts%2520by%250Aeliminating%2520the%2520impact%2520of%2520spatial%2520artifacts%2520during%2520feature%2520learning.%2520Extensive%250Aexperiments%2520demonstrate%2520the%2520efficacy%2520of%2520DeCoF%2520in%2520detecting%2520videos%2520generated%2520by%250Aunseen%2520video%2520generation%2520models%2520and%2520confirm%2520its%2520powerful%2520generalizability%2520across%250Aseveral%2520commercially%2520proprietary%2520models.%2520Our%2520code%2520and%2520dataset%2520will%2520be%2520released%250Aat%2520%255Curl%257Bhttps%253A//github.com/wuwuwuyue/DeCoF%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.02085v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeCoF%3A%20Generated%20Video%20Detection%20via%20Frame%20Consistency%3A%20The%20First%0A%20%20Benchmark%20Dataset&entry.906535625=Long%20Ma%20and%20Jiajia%20Zhang%20and%20Hongping%20Deng%20and%20Ningyu%20Zhang%20and%20Qinglang%20Guo%20and%20Haiyang%20Yu%20and%20Yong%20Liao%20and%20Pengyuan%20Zhou&entry.1292438233=%20%20The%20escalating%20quality%20of%20video%20generated%20by%20advanced%20video%20generation%0Amethods%20results%20in%20new%20security%20challenges%2C%20while%20there%20have%20been%20few%20relevant%0Aresearch%20efforts%3A%201%29%20There%20is%20no%20open-source%20dataset%20for%20generated%20video%0Adetection%2C%202%29%20No%20generated%20video%20detection%20method%20has%20been%20proposed%20so%20far.%20To%0Athis%20end%2C%20we%20propose%20an%20open-source%20dataset%20and%20a%20detection%20method%20for%0Agenerated%20video%20for%20the%20first%20time.%20First%2C%20we%20propose%20a%20scalable%20dataset%0Aconsisting%20of%20964%20prompts%2C%20covering%20various%20forgery%20targets%2C%20scenes%2C%20behaviors%2C%0Aand%20actions%2C%20as%20well%20as%20various%20generation%20models%20with%20different%20architectures%0Aand%20generation%20methods%2C%20including%20the%20most%20popular%20commercial%20models%20like%0AOpenAI%27s%20Sora%20and%20Google%27s%20Veo.%20Second%2C%20we%20found%20via%20probing%20experiments%20that%0Aspatial%20artifact-based%20detectors%20lack%20generalizability.%20Hence%2C%20we%20propose%20a%0Asimple%20yet%20effective%20%5Ctextbf%7Bde%7Dtection%20model%20based%20on%20%5Ctextbf%7Bf%7Drame%0A%5Ctextbf%7Bco%7Dnsistency%20%28%5Ctextbf%7BDeCoF%7D%29%2C%20which%20focuses%20on%20temporal%20artifacts%20by%0Aeliminating%20the%20impact%20of%20spatial%20artifacts%20during%20feature%20learning.%20Extensive%0Aexperiments%20demonstrate%20the%20efficacy%20of%20DeCoF%20in%20detecting%20videos%20generated%20by%0Aunseen%20video%20generation%20models%20and%20confirm%20its%20powerful%20generalizability%20across%0Aseveral%20commercially%20proprietary%20models.%20Our%20code%20and%20dataset%20will%20be%20released%0Aat%20%5Curl%7Bhttps%3A//github.com/wuwuwuyue/DeCoF%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.02085v4&entry.124074799=Read"},
{"title": "Unsupervised Open-Vocabulary Object Localization in Videos", "author": "Ke Fan and Zechen Bai and Tianjun Xiao and Dominik Zietlow and Max Horn and Zixu Zhao and Carl-Johann Simon-Gabriel and Mike Zheng Shou and Francesco Locatello and Bernt Schiele and Thomas Brox and Zheng Zhang and Yanwei Fu and Tong He", "abstract": "  In this paper, we show that recent advances in video representation learning\nand pre-trained vision-language models allow for substantial improvements in\nself-supervised video object localization. We propose a method that first\nlocalizes objects in videos via an object-centric approach with slot attention\nand then assigns text to the obtained slots. The latter is achieved by an\nunsupervised way to read localized semantic information from the pre-trained\nCLIP model. The resulting video object localization is entirely unsupervised\napart from the implicit annotation contained in CLIP, and it is effectively the\nfirst unsupervised approach that yields good results on regular video\nbenchmarks.\n", "link": "http://arxiv.org/abs/2309.09858v2", "date": "2024-06-26", "relevancy": 2.7614, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5701}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5475}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5392}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Open-Vocabulary%20Object%20Localization%20in%20Videos&body=Title%3A%20Unsupervised%20Open-Vocabulary%20Object%20Localization%20in%20Videos%0AAuthor%3A%20Ke%20Fan%20and%20Zechen%20Bai%20and%20Tianjun%20Xiao%20and%20Dominik%20Zietlow%20and%20Max%20Horn%20and%20Zixu%20Zhao%20and%20Carl-Johann%20Simon-Gabriel%20and%20Mike%20Zheng%20Shou%20and%20Francesco%20Locatello%20and%20Bernt%20Schiele%20and%20Thomas%20Brox%20and%20Zheng%20Zhang%20and%20Yanwei%20Fu%20and%20Tong%20He%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20show%20that%20recent%20advances%20in%20video%20representation%20learning%0Aand%20pre-trained%20vision-language%20models%20allow%20for%20substantial%20improvements%20in%0Aself-supervised%20video%20object%20localization.%20We%20propose%20a%20method%20that%20first%0Alocalizes%20objects%20in%20videos%20via%20an%20object-centric%20approach%20with%20slot%20attention%0Aand%20then%20assigns%20text%20to%20the%20obtained%20slots.%20The%20latter%20is%20achieved%20by%20an%0Aunsupervised%20way%20to%20read%20localized%20semantic%20information%20from%20the%20pre-trained%0ACLIP%20model.%20The%20resulting%20video%20object%20localization%20is%20entirely%20unsupervised%0Aapart%20from%20the%20implicit%20annotation%20contained%20in%20CLIP%2C%20and%20it%20is%20effectively%20the%0Afirst%20unsupervised%20approach%20that%20yields%20good%20results%20on%20regular%20video%0Abenchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.09858v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Open-Vocabulary%2520Object%2520Localization%2520in%2520Videos%26entry.906535625%3DKe%2520Fan%2520and%2520Zechen%2520Bai%2520and%2520Tianjun%2520Xiao%2520and%2520Dominik%2520Zietlow%2520and%2520Max%2520Horn%2520and%2520Zixu%2520Zhao%2520and%2520Carl-Johann%2520Simon-Gabriel%2520and%2520Mike%2520Zheng%2520Shou%2520and%2520Francesco%2520Locatello%2520and%2520Bernt%2520Schiele%2520and%2520Thomas%2520Brox%2520and%2520Zheng%2520Zhang%2520and%2520Yanwei%2520Fu%2520and%2520Tong%2520He%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520show%2520that%2520recent%2520advances%2520in%2520video%2520representation%2520learning%250Aand%2520pre-trained%2520vision-language%2520models%2520allow%2520for%2520substantial%2520improvements%2520in%250Aself-supervised%2520video%2520object%2520localization.%2520We%2520propose%2520a%2520method%2520that%2520first%250Alocalizes%2520objects%2520in%2520videos%2520via%2520an%2520object-centric%2520approach%2520with%2520slot%2520attention%250Aand%2520then%2520assigns%2520text%2520to%2520the%2520obtained%2520slots.%2520The%2520latter%2520is%2520achieved%2520by%2520an%250Aunsupervised%2520way%2520to%2520read%2520localized%2520semantic%2520information%2520from%2520the%2520pre-trained%250ACLIP%2520model.%2520The%2520resulting%2520video%2520object%2520localization%2520is%2520entirely%2520unsupervised%250Aapart%2520from%2520the%2520implicit%2520annotation%2520contained%2520in%2520CLIP%252C%2520and%2520it%2520is%2520effectively%2520the%250Afirst%2520unsupervised%2520approach%2520that%2520yields%2520good%2520results%2520on%2520regular%2520video%250Abenchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.09858v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Open-Vocabulary%20Object%20Localization%20in%20Videos&entry.906535625=Ke%20Fan%20and%20Zechen%20Bai%20and%20Tianjun%20Xiao%20and%20Dominik%20Zietlow%20and%20Max%20Horn%20and%20Zixu%20Zhao%20and%20Carl-Johann%20Simon-Gabriel%20and%20Mike%20Zheng%20Shou%20and%20Francesco%20Locatello%20and%20Bernt%20Schiele%20and%20Thomas%20Brox%20and%20Zheng%20Zhang%20and%20Yanwei%20Fu%20and%20Tong%20He&entry.1292438233=%20%20In%20this%20paper%2C%20we%20show%20that%20recent%20advances%20in%20video%20representation%20learning%0Aand%20pre-trained%20vision-language%20models%20allow%20for%20substantial%20improvements%20in%0Aself-supervised%20video%20object%20localization.%20We%20propose%20a%20method%20that%20first%0Alocalizes%20objects%20in%20videos%20via%20an%20object-centric%20approach%20with%20slot%20attention%0Aand%20then%20assigns%20text%20to%20the%20obtained%20slots.%20The%20latter%20is%20achieved%20by%20an%0Aunsupervised%20way%20to%20read%20localized%20semantic%20information%20from%20the%20pre-trained%0ACLIP%20model.%20The%20resulting%20video%20object%20localization%20is%20entirely%20unsupervised%0Aapart%20from%20the%20implicit%20annotation%20contained%20in%20CLIP%2C%20and%20it%20is%20effectively%20the%0Afirst%20unsupervised%20approach%20that%20yields%20good%20results%20on%20regular%20video%0Abenchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.09858v2&entry.124074799=Read"},
{"title": "RGB-Sonar Tracking Benchmark and Spatial Cross-Attention Transformer\n  Tracker", "author": "Yunfeng Li and Bo Wang and Jiuran Sun and Xueyi Wu and Ye Li", "abstract": "  Vision camera and sonar are naturally complementary in the underwater\nenvironment. Combining the information from two modalities will promote better\nobservation of underwater targets. However, this problem has not received\nsufficient attention in previous research. Therefore, this paper introduces a\nnew challenging RGB-Sonar (RGB-S) tracking task and investigates how to achieve\nefficient tracking of an underwater target through the interaction of RGB and\nsonar modalities. Specifically, we first propose an RGBS50 benchmark dataset\ncontaining 50 sequences and more than 87000 high-quality annotated bounding\nboxes. Experimental results show that the RGBS50 benchmark poses a challenge to\ncurrently popular SOT trackers. Second, we propose an RGB-S tracker called\nSCANet, which includes a spatial cross-attention module (SCAM) consisting of a\nnovel spatial cross-attention layer and two independent global integration\nmodules. The spatial cross-attention is used to overcome the problem of spatial\nmisalignment of between RGB and sonar images. Third, we propose a SOT\ndata-based RGB-S simulation training method (SRST) to overcome the lack of\nRGB-S training datasets. It converts RGB images into sonar-like saliency images\nto construct pseudo-data pairs, enabling the model to learn the semantic\nstructure of RGB-S-like data. Comprehensive experiments show that the proposed\nspatial cross-attention effectively achieves the interaction between RGB and\nsonar modalities and SCANet achieves state-of-the-art performance on the\nproposed benchmark. The code is available at\nhttps://github.com/LiYunfengLYF/RGBS50.\n", "link": "http://arxiv.org/abs/2406.07189v3", "date": "2024-06-26", "relevancy": 2.758, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6079}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5309}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RGB-Sonar%20Tracking%20Benchmark%20and%20Spatial%20Cross-Attention%20Transformer%0A%20%20Tracker&body=Title%3A%20RGB-Sonar%20Tracking%20Benchmark%20and%20Spatial%20Cross-Attention%20Transformer%0A%20%20Tracker%0AAuthor%3A%20Yunfeng%20Li%20and%20Bo%20Wang%20and%20Jiuran%20Sun%20and%20Xueyi%20Wu%20and%20Ye%20Li%0AAbstract%3A%20%20%20Vision%20camera%20and%20sonar%20are%20naturally%20complementary%20in%20the%20underwater%0Aenvironment.%20Combining%20the%20information%20from%20two%20modalities%20will%20promote%20better%0Aobservation%20of%20underwater%20targets.%20However%2C%20this%20problem%20has%20not%20received%0Asufficient%20attention%20in%20previous%20research.%20Therefore%2C%20this%20paper%20introduces%20a%0Anew%20challenging%20RGB-Sonar%20%28RGB-S%29%20tracking%20task%20and%20investigates%20how%20to%20achieve%0Aefficient%20tracking%20of%20an%20underwater%20target%20through%20the%20interaction%20of%20RGB%20and%0Asonar%20modalities.%20Specifically%2C%20we%20first%20propose%20an%20RGBS50%20benchmark%20dataset%0Acontaining%2050%20sequences%20and%20more%20than%2087000%20high-quality%20annotated%20bounding%0Aboxes.%20Experimental%20results%20show%20that%20the%20RGBS50%20benchmark%20poses%20a%20challenge%20to%0Acurrently%20popular%20SOT%20trackers.%20Second%2C%20we%20propose%20an%20RGB-S%20tracker%20called%0ASCANet%2C%20which%20includes%20a%20spatial%20cross-attention%20module%20%28SCAM%29%20consisting%20of%20a%0Anovel%20spatial%20cross-attention%20layer%20and%20two%20independent%20global%20integration%0Amodules.%20The%20spatial%20cross-attention%20is%20used%20to%20overcome%20the%20problem%20of%20spatial%0Amisalignment%20of%20between%20RGB%20and%20sonar%20images.%20Third%2C%20we%20propose%20a%20SOT%0Adata-based%20RGB-S%20simulation%20training%20method%20%28SRST%29%20to%20overcome%20the%20lack%20of%0ARGB-S%20training%20datasets.%20It%20converts%20RGB%20images%20into%20sonar-like%20saliency%20images%0Ato%20construct%20pseudo-data%20pairs%2C%20enabling%20the%20model%20to%20learn%20the%20semantic%0Astructure%20of%20RGB-S-like%20data.%20Comprehensive%20experiments%20show%20that%20the%20proposed%0Aspatial%20cross-attention%20effectively%20achieves%20the%20interaction%20between%20RGB%20and%0Asonar%20modalities%20and%20SCANet%20achieves%20state-of-the-art%20performance%20on%20the%0Aproposed%20benchmark.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/LiYunfengLYF/RGBS50.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07189v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRGB-Sonar%2520Tracking%2520Benchmark%2520and%2520Spatial%2520Cross-Attention%2520Transformer%250A%2520%2520Tracker%26entry.906535625%3DYunfeng%2520Li%2520and%2520Bo%2520Wang%2520and%2520Jiuran%2520Sun%2520and%2520Xueyi%2520Wu%2520and%2520Ye%2520Li%26entry.1292438233%3D%2520%2520Vision%2520camera%2520and%2520sonar%2520are%2520naturally%2520complementary%2520in%2520the%2520underwater%250Aenvironment.%2520Combining%2520the%2520information%2520from%2520two%2520modalities%2520will%2520promote%2520better%250Aobservation%2520of%2520underwater%2520targets.%2520However%252C%2520this%2520problem%2520has%2520not%2520received%250Asufficient%2520attention%2520in%2520previous%2520research.%2520Therefore%252C%2520this%2520paper%2520introduces%2520a%250Anew%2520challenging%2520RGB-Sonar%2520%2528RGB-S%2529%2520tracking%2520task%2520and%2520investigates%2520how%2520to%2520achieve%250Aefficient%2520tracking%2520of%2520an%2520underwater%2520target%2520through%2520the%2520interaction%2520of%2520RGB%2520and%250Asonar%2520modalities.%2520Specifically%252C%2520we%2520first%2520propose%2520an%2520RGBS50%2520benchmark%2520dataset%250Acontaining%252050%2520sequences%2520and%2520more%2520than%252087000%2520high-quality%2520annotated%2520bounding%250Aboxes.%2520Experimental%2520results%2520show%2520that%2520the%2520RGBS50%2520benchmark%2520poses%2520a%2520challenge%2520to%250Acurrently%2520popular%2520SOT%2520trackers.%2520Second%252C%2520we%2520propose%2520an%2520RGB-S%2520tracker%2520called%250ASCANet%252C%2520which%2520includes%2520a%2520spatial%2520cross-attention%2520module%2520%2528SCAM%2529%2520consisting%2520of%2520a%250Anovel%2520spatial%2520cross-attention%2520layer%2520and%2520two%2520independent%2520global%2520integration%250Amodules.%2520The%2520spatial%2520cross-attention%2520is%2520used%2520to%2520overcome%2520the%2520problem%2520of%2520spatial%250Amisalignment%2520of%2520between%2520RGB%2520and%2520sonar%2520images.%2520Third%252C%2520we%2520propose%2520a%2520SOT%250Adata-based%2520RGB-S%2520simulation%2520training%2520method%2520%2528SRST%2529%2520to%2520overcome%2520the%2520lack%2520of%250ARGB-S%2520training%2520datasets.%2520It%2520converts%2520RGB%2520images%2520into%2520sonar-like%2520saliency%2520images%250Ato%2520construct%2520pseudo-data%2520pairs%252C%2520enabling%2520the%2520model%2520to%2520learn%2520the%2520semantic%250Astructure%2520of%2520RGB-S-like%2520data.%2520Comprehensive%2520experiments%2520show%2520that%2520the%2520proposed%250Aspatial%2520cross-attention%2520effectively%2520achieves%2520the%2520interaction%2520between%2520RGB%2520and%250Asonar%2520modalities%2520and%2520SCANet%2520achieves%2520state-of-the-art%2520performance%2520on%2520the%250Aproposed%2520benchmark.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/LiYunfengLYF/RGBS50.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07189v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RGB-Sonar%20Tracking%20Benchmark%20and%20Spatial%20Cross-Attention%20Transformer%0A%20%20Tracker&entry.906535625=Yunfeng%20Li%20and%20Bo%20Wang%20and%20Jiuran%20Sun%20and%20Xueyi%20Wu%20and%20Ye%20Li&entry.1292438233=%20%20Vision%20camera%20and%20sonar%20are%20naturally%20complementary%20in%20the%20underwater%0Aenvironment.%20Combining%20the%20information%20from%20two%20modalities%20will%20promote%20better%0Aobservation%20of%20underwater%20targets.%20However%2C%20this%20problem%20has%20not%20received%0Asufficient%20attention%20in%20previous%20research.%20Therefore%2C%20this%20paper%20introduces%20a%0Anew%20challenging%20RGB-Sonar%20%28RGB-S%29%20tracking%20task%20and%20investigates%20how%20to%20achieve%0Aefficient%20tracking%20of%20an%20underwater%20target%20through%20the%20interaction%20of%20RGB%20and%0Asonar%20modalities.%20Specifically%2C%20we%20first%20propose%20an%20RGBS50%20benchmark%20dataset%0Acontaining%2050%20sequences%20and%20more%20than%2087000%20high-quality%20annotated%20bounding%0Aboxes.%20Experimental%20results%20show%20that%20the%20RGBS50%20benchmark%20poses%20a%20challenge%20to%0Acurrently%20popular%20SOT%20trackers.%20Second%2C%20we%20propose%20an%20RGB-S%20tracker%20called%0ASCANet%2C%20which%20includes%20a%20spatial%20cross-attention%20module%20%28SCAM%29%20consisting%20of%20a%0Anovel%20spatial%20cross-attention%20layer%20and%20two%20independent%20global%20integration%0Amodules.%20The%20spatial%20cross-attention%20is%20used%20to%20overcome%20the%20problem%20of%20spatial%0Amisalignment%20of%20between%20RGB%20and%20sonar%20images.%20Third%2C%20we%20propose%20a%20SOT%0Adata-based%20RGB-S%20simulation%20training%20method%20%28SRST%29%20to%20overcome%20the%20lack%20of%0ARGB-S%20training%20datasets.%20It%20converts%20RGB%20images%20into%20sonar-like%20saliency%20images%0Ato%20construct%20pseudo-data%20pairs%2C%20enabling%20the%20model%20to%20learn%20the%20semantic%0Astructure%20of%20RGB-S-like%20data.%20Comprehensive%20experiments%20show%20that%20the%20proposed%0Aspatial%20cross-attention%20effectively%20achieves%20the%20interaction%20between%20RGB%20and%0Asonar%20modalities%20and%20SCANet%20achieves%20state-of-the-art%20performance%20on%20the%0Aproposed%20benchmark.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/LiYunfengLYF/RGBS50.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07189v3&entry.124074799=Read"},
{"title": "Enhancing Low-light Light Field Images with A Deep Compensation\n  Unfolding Network", "author": "Xianqiang Lyu and Junhui Hou", "abstract": "  This paper presents a novel and interpretable end-to-end learning framework,\ncalled the deep compensation unfolding network (DCUNet), for restoring light\nfield (LF) images captured under low-light conditions. DCUNet is designed with\na multi-stage architecture that mimics the optimization process of solving an\ninverse imaging problem in a data-driven fashion. The framework uses the\nintermediate enhanced result to estimate the illumination map, which is then\nemployed in the unfolding process to produce a new enhanced result.\nAdditionally, DCUNet includes a content-associated deep compensation module at\neach optimization stage to suppress noise and illumination map estimation\nerrors. To properly mine and leverage the unique characteristics of LF images,\nthis paper proposes a pseudo-explicit feature interaction module that\ncomprehensively exploits redundant information in LF images. The experimental\nresults on both simulated and real datasets demonstrate the superiority of our\nDCUNet over state-of-the-art methods, both qualitatively and quantitatively.\nMoreover, DCUNet preserves the essential geometric structure of enhanced LF\nimages much better. The code will be publicly available at\nhttps://github.com/lyuxianqiang/LFLL-DCU.\n", "link": "http://arxiv.org/abs/2308.05404v3", "date": "2024-06-26", "relevancy": 2.7567, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.572}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5442}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5378}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Low-light%20Light%20Field%20Images%20with%20A%20Deep%20Compensation%0A%20%20Unfolding%20Network&body=Title%3A%20Enhancing%20Low-light%20Light%20Field%20Images%20with%20A%20Deep%20Compensation%0A%20%20Unfolding%20Network%0AAuthor%3A%20Xianqiang%20Lyu%20and%20Junhui%20Hou%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20and%20interpretable%20end-to-end%20learning%20framework%2C%0Acalled%20the%20deep%20compensation%20unfolding%20network%20%28DCUNet%29%2C%20for%20restoring%20light%0Afield%20%28LF%29%20images%20captured%20under%20low-light%20conditions.%20DCUNet%20is%20designed%20with%0Aa%20multi-stage%20architecture%20that%20mimics%20the%20optimization%20process%20of%20solving%20an%0Ainverse%20imaging%20problem%20in%20a%20data-driven%20fashion.%20The%20framework%20uses%20the%0Aintermediate%20enhanced%20result%20to%20estimate%20the%20illumination%20map%2C%20which%20is%20then%0Aemployed%20in%20the%20unfolding%20process%20to%20produce%20a%20new%20enhanced%20result.%0AAdditionally%2C%20DCUNet%20includes%20a%20content-associated%20deep%20compensation%20module%20at%0Aeach%20optimization%20stage%20to%20suppress%20noise%20and%20illumination%20map%20estimation%0Aerrors.%20To%20properly%20mine%20and%20leverage%20the%20unique%20characteristics%20of%20LF%20images%2C%0Athis%20paper%20proposes%20a%20pseudo-explicit%20feature%20interaction%20module%20that%0Acomprehensively%20exploits%20redundant%20information%20in%20LF%20images.%20The%20experimental%0Aresults%20on%20both%20simulated%20and%20real%20datasets%20demonstrate%20the%20superiority%20of%20our%0ADCUNet%20over%20state-of-the-art%20methods%2C%20both%20qualitatively%20and%20quantitatively.%0AMoreover%2C%20DCUNet%20preserves%20the%20essential%20geometric%20structure%20of%20enhanced%20LF%0Aimages%20much%20better.%20The%20code%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/lyuxianqiang/LFLL-DCU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.05404v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Low-light%2520Light%2520Field%2520Images%2520with%2520A%2520Deep%2520Compensation%250A%2520%2520Unfolding%2520Network%26entry.906535625%3DXianqiang%2520Lyu%2520and%2520Junhui%2520Hou%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520and%2520interpretable%2520end-to-end%2520learning%2520framework%252C%250Acalled%2520the%2520deep%2520compensation%2520unfolding%2520network%2520%2528DCUNet%2529%252C%2520for%2520restoring%2520light%250Afield%2520%2528LF%2529%2520images%2520captured%2520under%2520low-light%2520conditions.%2520DCUNet%2520is%2520designed%2520with%250Aa%2520multi-stage%2520architecture%2520that%2520mimics%2520the%2520optimization%2520process%2520of%2520solving%2520an%250Ainverse%2520imaging%2520problem%2520in%2520a%2520data-driven%2520fashion.%2520The%2520framework%2520uses%2520the%250Aintermediate%2520enhanced%2520result%2520to%2520estimate%2520the%2520illumination%2520map%252C%2520which%2520is%2520then%250Aemployed%2520in%2520the%2520unfolding%2520process%2520to%2520produce%2520a%2520new%2520enhanced%2520result.%250AAdditionally%252C%2520DCUNet%2520includes%2520a%2520content-associated%2520deep%2520compensation%2520module%2520at%250Aeach%2520optimization%2520stage%2520to%2520suppress%2520noise%2520and%2520illumination%2520map%2520estimation%250Aerrors.%2520To%2520properly%2520mine%2520and%2520leverage%2520the%2520unique%2520characteristics%2520of%2520LF%2520images%252C%250Athis%2520paper%2520proposes%2520a%2520pseudo-explicit%2520feature%2520interaction%2520module%2520that%250Acomprehensively%2520exploits%2520redundant%2520information%2520in%2520LF%2520images.%2520The%2520experimental%250Aresults%2520on%2520both%2520simulated%2520and%2520real%2520datasets%2520demonstrate%2520the%2520superiority%2520of%2520our%250ADCUNet%2520over%2520state-of-the-art%2520methods%252C%2520both%2520qualitatively%2520and%2520quantitatively.%250AMoreover%252C%2520DCUNet%2520preserves%2520the%2520essential%2520geometric%2520structure%2520of%2520enhanced%2520LF%250Aimages%2520much%2520better.%2520The%2520code%2520will%2520be%2520publicly%2520available%2520at%250Ahttps%253A//github.com/lyuxianqiang/LFLL-DCU.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.05404v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Low-light%20Light%20Field%20Images%20with%20A%20Deep%20Compensation%0A%20%20Unfolding%20Network&entry.906535625=Xianqiang%20Lyu%20and%20Junhui%20Hou&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20and%20interpretable%20end-to-end%20learning%20framework%2C%0Acalled%20the%20deep%20compensation%20unfolding%20network%20%28DCUNet%29%2C%20for%20restoring%20light%0Afield%20%28LF%29%20images%20captured%20under%20low-light%20conditions.%20DCUNet%20is%20designed%20with%0Aa%20multi-stage%20architecture%20that%20mimics%20the%20optimization%20process%20of%20solving%20an%0Ainverse%20imaging%20problem%20in%20a%20data-driven%20fashion.%20The%20framework%20uses%20the%0Aintermediate%20enhanced%20result%20to%20estimate%20the%20illumination%20map%2C%20which%20is%20then%0Aemployed%20in%20the%20unfolding%20process%20to%20produce%20a%20new%20enhanced%20result.%0AAdditionally%2C%20DCUNet%20includes%20a%20content-associated%20deep%20compensation%20module%20at%0Aeach%20optimization%20stage%20to%20suppress%20noise%20and%20illumination%20map%20estimation%0Aerrors.%20To%20properly%20mine%20and%20leverage%20the%20unique%20characteristics%20of%20LF%20images%2C%0Athis%20paper%20proposes%20a%20pseudo-explicit%20feature%20interaction%20module%20that%0Acomprehensively%20exploits%20redundant%20information%20in%20LF%20images.%20The%20experimental%0Aresults%20on%20both%20simulated%20and%20real%20datasets%20demonstrate%20the%20superiority%20of%20our%0ADCUNet%20over%20state-of-the-art%20methods%2C%20both%20qualitatively%20and%20quantitatively.%0AMoreover%2C%20DCUNet%20preserves%20the%20essential%20geometric%20structure%20of%20enhanced%20LF%0Aimages%20much%20better.%20The%20code%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/lyuxianqiang/LFLL-DCU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.05404v3&entry.124074799=Read"},
{"title": "XLD: A Cross-Lane Dataset for Benchmarking Novel Driving View Synthesis", "author": "Hao Li and Ming Yuan and Yan Zhang and Chenming Wu and Chen Zhao and Chunyu Song and Haocheng Feng and Errui Ding and Dingwen Zhang and Jingdong Wang", "abstract": "  Thoroughly testing autonomy systems is crucial in the pursuit of safe\nautonomous driving vehicles. It necessitates creating safety-critical scenarios\nthat go beyond what can be safely collected from real-world data, as many of\nthese scenarios occur infrequently on public roads. However, the evaluation of\nmost existing NVS methods relies on sporadic sampling of image frames from the\ntraining data, comparing the rendered images with ground truth images using\nmetrics. Unfortunately, this evaluation protocol falls short of meeting the\nactual requirements in closed-loop simulations. Specifically, the true\napplication demands the capability to render novel views that extend beyond the\noriginal trajectory (such as cross-lane views), which are challenging to\ncapture in the real world. To address this, this paper presents a novel driving\nview synthesis dataset and benchmark specifically designed for autonomous\ndriving simulations. This dataset is unique as it includes testing images\ncaptured by deviating from the training trajectory by 1-4 meters. It comprises\nsix sequences encompassing various time and weather conditions. Each sequence\ncontains 450 training images, 150 testing images, and their corresponding\ncamera poses and intrinsic parameters. Leveraging this novel dataset, we\nestablish the first realistic benchmark for evaluating existing NVS approaches\nunder front-only and multi-camera settings. The experimental findings\nunderscore the significant gap that exists in current approaches, revealing\ntheir inadequate ability to fulfill the demanding prerequisites of cross-lane\nor closed-loop simulation. Our dataset is released publicly at the project\npage: https://3d-aigc.github.io/XLD/.\n", "link": "http://arxiv.org/abs/2406.18360v1", "date": "2024-06-26", "relevancy": 2.6965, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5435}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5372}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5372}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XLD%3A%20A%20Cross-Lane%20Dataset%20for%20Benchmarking%20Novel%20Driving%20View%20Synthesis&body=Title%3A%20XLD%3A%20A%20Cross-Lane%20Dataset%20for%20Benchmarking%20Novel%20Driving%20View%20Synthesis%0AAuthor%3A%20Hao%20Li%20and%20Ming%20Yuan%20and%20Yan%20Zhang%20and%20Chenming%20Wu%20and%20Chen%20Zhao%20and%20Chunyu%20Song%20and%20Haocheng%20Feng%20and%20Errui%20Ding%20and%20Dingwen%20Zhang%20and%20Jingdong%20Wang%0AAbstract%3A%20%20%20Thoroughly%20testing%20autonomy%20systems%20is%20crucial%20in%20the%20pursuit%20of%20safe%0Aautonomous%20driving%20vehicles.%20It%20necessitates%20creating%20safety-critical%20scenarios%0Athat%20go%20beyond%20what%20can%20be%20safely%20collected%20from%20real-world%20data%2C%20as%20many%20of%0Athese%20scenarios%20occur%20infrequently%20on%20public%20roads.%20However%2C%20the%20evaluation%20of%0Amost%20existing%20NVS%20methods%20relies%20on%20sporadic%20sampling%20of%20image%20frames%20from%20the%0Atraining%20data%2C%20comparing%20the%20rendered%20images%20with%20ground%20truth%20images%20using%0Ametrics.%20Unfortunately%2C%20this%20evaluation%20protocol%20falls%20short%20of%20meeting%20the%0Aactual%20requirements%20in%20closed-loop%20simulations.%20Specifically%2C%20the%20true%0Aapplication%20demands%20the%20capability%20to%20render%20novel%20views%20that%20extend%20beyond%20the%0Aoriginal%20trajectory%20%28such%20as%20cross-lane%20views%29%2C%20which%20are%20challenging%20to%0Acapture%20in%20the%20real%20world.%20To%20address%20this%2C%20this%20paper%20presents%20a%20novel%20driving%0Aview%20synthesis%20dataset%20and%20benchmark%20specifically%20designed%20for%20autonomous%0Adriving%20simulations.%20This%20dataset%20is%20unique%20as%20it%20includes%20testing%20images%0Acaptured%20by%20deviating%20from%20the%20training%20trajectory%20by%201-4%20meters.%20It%20comprises%0Asix%20sequences%20encompassing%20various%20time%20and%20weather%20conditions.%20Each%20sequence%0Acontains%20450%20training%20images%2C%20150%20testing%20images%2C%20and%20their%20corresponding%0Acamera%20poses%20and%20intrinsic%20parameters.%20Leveraging%20this%20novel%20dataset%2C%20we%0Aestablish%20the%20first%20realistic%20benchmark%20for%20evaluating%20existing%20NVS%20approaches%0Aunder%20front-only%20and%20multi-camera%20settings.%20The%20experimental%20findings%0Aunderscore%20the%20significant%20gap%20that%20exists%20in%20current%20approaches%2C%20revealing%0Atheir%20inadequate%20ability%20to%20fulfill%20the%20demanding%20prerequisites%20of%20cross-lane%0Aor%20closed-loop%20simulation.%20Our%20dataset%20is%20released%20publicly%20at%20the%20project%0Apage%3A%20https%3A//3d-aigc.github.io/XLD/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18360v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXLD%253A%2520A%2520Cross-Lane%2520Dataset%2520for%2520Benchmarking%2520Novel%2520Driving%2520View%2520Synthesis%26entry.906535625%3DHao%2520Li%2520and%2520Ming%2520Yuan%2520and%2520Yan%2520Zhang%2520and%2520Chenming%2520Wu%2520and%2520Chen%2520Zhao%2520and%2520Chunyu%2520Song%2520and%2520Haocheng%2520Feng%2520and%2520Errui%2520Ding%2520and%2520Dingwen%2520Zhang%2520and%2520Jingdong%2520Wang%26entry.1292438233%3D%2520%2520Thoroughly%2520testing%2520autonomy%2520systems%2520is%2520crucial%2520in%2520the%2520pursuit%2520of%2520safe%250Aautonomous%2520driving%2520vehicles.%2520It%2520necessitates%2520creating%2520safety-critical%2520scenarios%250Athat%2520go%2520beyond%2520what%2520can%2520be%2520safely%2520collected%2520from%2520real-world%2520data%252C%2520as%2520many%2520of%250Athese%2520scenarios%2520occur%2520infrequently%2520on%2520public%2520roads.%2520However%252C%2520the%2520evaluation%2520of%250Amost%2520existing%2520NVS%2520methods%2520relies%2520on%2520sporadic%2520sampling%2520of%2520image%2520frames%2520from%2520the%250Atraining%2520data%252C%2520comparing%2520the%2520rendered%2520images%2520with%2520ground%2520truth%2520images%2520using%250Ametrics.%2520Unfortunately%252C%2520this%2520evaluation%2520protocol%2520falls%2520short%2520of%2520meeting%2520the%250Aactual%2520requirements%2520in%2520closed-loop%2520simulations.%2520Specifically%252C%2520the%2520true%250Aapplication%2520demands%2520the%2520capability%2520to%2520render%2520novel%2520views%2520that%2520extend%2520beyond%2520the%250Aoriginal%2520trajectory%2520%2528such%2520as%2520cross-lane%2520views%2529%252C%2520which%2520are%2520challenging%2520to%250Acapture%2520in%2520the%2520real%2520world.%2520To%2520address%2520this%252C%2520this%2520paper%2520presents%2520a%2520novel%2520driving%250Aview%2520synthesis%2520dataset%2520and%2520benchmark%2520specifically%2520designed%2520for%2520autonomous%250Adriving%2520simulations.%2520This%2520dataset%2520is%2520unique%2520as%2520it%2520includes%2520testing%2520images%250Acaptured%2520by%2520deviating%2520from%2520the%2520training%2520trajectory%2520by%25201-4%2520meters.%2520It%2520comprises%250Asix%2520sequences%2520encompassing%2520various%2520time%2520and%2520weather%2520conditions.%2520Each%2520sequence%250Acontains%2520450%2520training%2520images%252C%2520150%2520testing%2520images%252C%2520and%2520their%2520corresponding%250Acamera%2520poses%2520and%2520intrinsic%2520parameters.%2520Leveraging%2520this%2520novel%2520dataset%252C%2520we%250Aestablish%2520the%2520first%2520realistic%2520benchmark%2520for%2520evaluating%2520existing%2520NVS%2520approaches%250Aunder%2520front-only%2520and%2520multi-camera%2520settings.%2520The%2520experimental%2520findings%250Aunderscore%2520the%2520significant%2520gap%2520that%2520exists%2520in%2520current%2520approaches%252C%2520revealing%250Atheir%2520inadequate%2520ability%2520to%2520fulfill%2520the%2520demanding%2520prerequisites%2520of%2520cross-lane%250Aor%2520closed-loop%2520simulation.%2520Our%2520dataset%2520is%2520released%2520publicly%2520at%2520the%2520project%250Apage%253A%2520https%253A//3d-aigc.github.io/XLD/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18360v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XLD%3A%20A%20Cross-Lane%20Dataset%20for%20Benchmarking%20Novel%20Driving%20View%20Synthesis&entry.906535625=Hao%20Li%20and%20Ming%20Yuan%20and%20Yan%20Zhang%20and%20Chenming%20Wu%20and%20Chen%20Zhao%20and%20Chunyu%20Song%20and%20Haocheng%20Feng%20and%20Errui%20Ding%20and%20Dingwen%20Zhang%20and%20Jingdong%20Wang&entry.1292438233=%20%20Thoroughly%20testing%20autonomy%20systems%20is%20crucial%20in%20the%20pursuit%20of%20safe%0Aautonomous%20driving%20vehicles.%20It%20necessitates%20creating%20safety-critical%20scenarios%0Athat%20go%20beyond%20what%20can%20be%20safely%20collected%20from%20real-world%20data%2C%20as%20many%20of%0Athese%20scenarios%20occur%20infrequently%20on%20public%20roads.%20However%2C%20the%20evaluation%20of%0Amost%20existing%20NVS%20methods%20relies%20on%20sporadic%20sampling%20of%20image%20frames%20from%20the%0Atraining%20data%2C%20comparing%20the%20rendered%20images%20with%20ground%20truth%20images%20using%0Ametrics.%20Unfortunately%2C%20this%20evaluation%20protocol%20falls%20short%20of%20meeting%20the%0Aactual%20requirements%20in%20closed-loop%20simulations.%20Specifically%2C%20the%20true%0Aapplication%20demands%20the%20capability%20to%20render%20novel%20views%20that%20extend%20beyond%20the%0Aoriginal%20trajectory%20%28such%20as%20cross-lane%20views%29%2C%20which%20are%20challenging%20to%0Acapture%20in%20the%20real%20world.%20To%20address%20this%2C%20this%20paper%20presents%20a%20novel%20driving%0Aview%20synthesis%20dataset%20and%20benchmark%20specifically%20designed%20for%20autonomous%0Adriving%20simulations.%20This%20dataset%20is%20unique%20as%20it%20includes%20testing%20images%0Acaptured%20by%20deviating%20from%20the%20training%20trajectory%20by%201-4%20meters.%20It%20comprises%0Asix%20sequences%20encompassing%20various%20time%20and%20weather%20conditions.%20Each%20sequence%0Acontains%20450%20training%20images%2C%20150%20testing%20images%2C%20and%20their%20corresponding%0Acamera%20poses%20and%20intrinsic%20parameters.%20Leveraging%20this%20novel%20dataset%2C%20we%0Aestablish%20the%20first%20realistic%20benchmark%20for%20evaluating%20existing%20NVS%20approaches%0Aunder%20front-only%20and%20multi-camera%20settings.%20The%20experimental%20findings%0Aunderscore%20the%20significant%20gap%20that%20exists%20in%20current%20approaches%2C%20revealing%0Atheir%20inadequate%20ability%20to%20fulfill%20the%20demanding%20prerequisites%20of%20cross-lane%0Aor%20closed-loop%20simulation.%20Our%20dataset%20is%20released%20publicly%20at%20the%20project%0Apage%3A%20https%3A//3d-aigc.github.io/XLD/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18360v1&entry.124074799=Read"},
{"title": "Spatial-temporal Hierarchical Reinforcement Learning for Interpretable\n  Pathology Image Super-Resolution", "author": "Wenting Chen and Jie Liu and Tommy W. S. Chow and Yixuan Yuan", "abstract": "  Pathology image are essential for accurately interpreting lesion cells in\ncytopathology screening, but acquiring high-resolution digital slides requires\nspecialized equipment and long scanning times. Though super-resolution (SR)\ntechniques can alleviate this problem, existing deep learning models recover\npathology image in a black-box manner, which can lead to untruthful biological\ndetails and misdiagnosis. Additionally, current methods allocate the same\ncomputational resources to recover each pixel of pathology image, leading to\nthe sub-optimal recovery issue due to the large variation of pathology image.\nIn this paper, we propose the first hierarchical reinforcement learning\nframework named Spatial-Temporal hierARchical Reinforcement Learning (STAR-RL),\nmainly for addressing the aforementioned issues in pathology image\nsuper-resolution problem. We reformulate the SR problem as a Markov decision\nprocess of interpretable operations and adopt the hierarchical recovery\nmechanism in patch level, to avoid sub-optimal recovery. Specifically, the\nhigher-level spatial manager is proposed to pick out the most corrupted patch\nfor the lower-level patch worker. Moreover, the higher-level temporal manager\nis advanced to evaluate the selected patch and determine whether the\noptimization should be stopped earlier, thereby avoiding the over-processed\nproblem. Under the guidance of spatial-temporal managers, the lower-level patch\nworker processes the selected patch with pixel-wise interpretable actions at\neach time step. Experimental results on medical images degraded by different\nkernels show the effectiveness of STAR-RL. Furthermore, STAR-RL validates the\npromotion in tumor diagnosis with a large margin and shows generalizability\nunder various degradations. The source code is available at\nhttps://github.com/CUHK-AIM-Group/STAR-RL.\n", "link": "http://arxiv.org/abs/2406.18310v1", "date": "2024-06-26", "relevancy": 2.6873, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.547}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5352}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatial-temporal%20Hierarchical%20Reinforcement%20Learning%20for%20Interpretable%0A%20%20Pathology%20Image%20Super-Resolution&body=Title%3A%20Spatial-temporal%20Hierarchical%20Reinforcement%20Learning%20for%20Interpretable%0A%20%20Pathology%20Image%20Super-Resolution%0AAuthor%3A%20Wenting%20Chen%20and%20Jie%20Liu%20and%20Tommy%20W.%20S.%20Chow%20and%20Yixuan%20Yuan%0AAbstract%3A%20%20%20Pathology%20image%20are%20essential%20for%20accurately%20interpreting%20lesion%20cells%20in%0Acytopathology%20screening%2C%20but%20acquiring%20high-resolution%20digital%20slides%20requires%0Aspecialized%20equipment%20and%20long%20scanning%20times.%20Though%20super-resolution%20%28SR%29%0Atechniques%20can%20alleviate%20this%20problem%2C%20existing%20deep%20learning%20models%20recover%0Apathology%20image%20in%20a%20black-box%20manner%2C%20which%20can%20lead%20to%20untruthful%20biological%0Adetails%20and%20misdiagnosis.%20Additionally%2C%20current%20methods%20allocate%20the%20same%0Acomputational%20resources%20to%20recover%20each%20pixel%20of%20pathology%20image%2C%20leading%20to%0Athe%20sub-optimal%20recovery%20issue%20due%20to%20the%20large%20variation%20of%20pathology%20image.%0AIn%20this%20paper%2C%20we%20propose%20the%20first%20hierarchical%20reinforcement%20learning%0Aframework%20named%20Spatial-Temporal%20hierARchical%20Reinforcement%20Learning%20%28STAR-RL%29%2C%0Amainly%20for%20addressing%20the%20aforementioned%20issues%20in%20pathology%20image%0Asuper-resolution%20problem.%20We%20reformulate%20the%20SR%20problem%20as%20a%20Markov%20decision%0Aprocess%20of%20interpretable%20operations%20and%20adopt%20the%20hierarchical%20recovery%0Amechanism%20in%20patch%20level%2C%20to%20avoid%20sub-optimal%20recovery.%20Specifically%2C%20the%0Ahigher-level%20spatial%20manager%20is%20proposed%20to%20pick%20out%20the%20most%20corrupted%20patch%0Afor%20the%20lower-level%20patch%20worker.%20Moreover%2C%20the%20higher-level%20temporal%20manager%0Ais%20advanced%20to%20evaluate%20the%20selected%20patch%20and%20determine%20whether%20the%0Aoptimization%20should%20be%20stopped%20earlier%2C%20thereby%20avoiding%20the%20over-processed%0Aproblem.%20Under%20the%20guidance%20of%20spatial-temporal%20managers%2C%20the%20lower-level%20patch%0Aworker%20processes%20the%20selected%20patch%20with%20pixel-wise%20interpretable%20actions%20at%0Aeach%20time%20step.%20Experimental%20results%20on%20medical%20images%20degraded%20by%20different%0Akernels%20show%20the%20effectiveness%20of%20STAR-RL.%20Furthermore%2C%20STAR-RL%20validates%20the%0Apromotion%20in%20tumor%20diagnosis%20with%20a%20large%20margin%20and%20shows%20generalizability%0Aunder%20various%20degradations.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/CUHK-AIM-Group/STAR-RL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18310v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatial-temporal%2520Hierarchical%2520Reinforcement%2520Learning%2520for%2520Interpretable%250A%2520%2520Pathology%2520Image%2520Super-Resolution%26entry.906535625%3DWenting%2520Chen%2520and%2520Jie%2520Liu%2520and%2520Tommy%2520W.%2520S.%2520Chow%2520and%2520Yixuan%2520Yuan%26entry.1292438233%3D%2520%2520Pathology%2520image%2520are%2520essential%2520for%2520accurately%2520interpreting%2520lesion%2520cells%2520in%250Acytopathology%2520screening%252C%2520but%2520acquiring%2520high-resolution%2520digital%2520slides%2520requires%250Aspecialized%2520equipment%2520and%2520long%2520scanning%2520times.%2520Though%2520super-resolution%2520%2528SR%2529%250Atechniques%2520can%2520alleviate%2520this%2520problem%252C%2520existing%2520deep%2520learning%2520models%2520recover%250Apathology%2520image%2520in%2520a%2520black-box%2520manner%252C%2520which%2520can%2520lead%2520to%2520untruthful%2520biological%250Adetails%2520and%2520misdiagnosis.%2520Additionally%252C%2520current%2520methods%2520allocate%2520the%2520same%250Acomputational%2520resources%2520to%2520recover%2520each%2520pixel%2520of%2520pathology%2520image%252C%2520leading%2520to%250Athe%2520sub-optimal%2520recovery%2520issue%2520due%2520to%2520the%2520large%2520variation%2520of%2520pathology%2520image.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520the%2520first%2520hierarchical%2520reinforcement%2520learning%250Aframework%2520named%2520Spatial-Temporal%2520hierARchical%2520Reinforcement%2520Learning%2520%2528STAR-RL%2529%252C%250Amainly%2520for%2520addressing%2520the%2520aforementioned%2520issues%2520in%2520pathology%2520image%250Asuper-resolution%2520problem.%2520We%2520reformulate%2520the%2520SR%2520problem%2520as%2520a%2520Markov%2520decision%250Aprocess%2520of%2520interpretable%2520operations%2520and%2520adopt%2520the%2520hierarchical%2520recovery%250Amechanism%2520in%2520patch%2520level%252C%2520to%2520avoid%2520sub-optimal%2520recovery.%2520Specifically%252C%2520the%250Ahigher-level%2520spatial%2520manager%2520is%2520proposed%2520to%2520pick%2520out%2520the%2520most%2520corrupted%2520patch%250Afor%2520the%2520lower-level%2520patch%2520worker.%2520Moreover%252C%2520the%2520higher-level%2520temporal%2520manager%250Ais%2520advanced%2520to%2520evaluate%2520the%2520selected%2520patch%2520and%2520determine%2520whether%2520the%250Aoptimization%2520should%2520be%2520stopped%2520earlier%252C%2520thereby%2520avoiding%2520the%2520over-processed%250Aproblem.%2520Under%2520the%2520guidance%2520of%2520spatial-temporal%2520managers%252C%2520the%2520lower-level%2520patch%250Aworker%2520processes%2520the%2520selected%2520patch%2520with%2520pixel-wise%2520interpretable%2520actions%2520at%250Aeach%2520time%2520step.%2520Experimental%2520results%2520on%2520medical%2520images%2520degraded%2520by%2520different%250Akernels%2520show%2520the%2520effectiveness%2520of%2520STAR-RL.%2520Furthermore%252C%2520STAR-RL%2520validates%2520the%250Apromotion%2520in%2520tumor%2520diagnosis%2520with%2520a%2520large%2520margin%2520and%2520shows%2520generalizability%250Aunder%2520various%2520degradations.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/CUHK-AIM-Group/STAR-RL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18310v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial-temporal%20Hierarchical%20Reinforcement%20Learning%20for%20Interpretable%0A%20%20Pathology%20Image%20Super-Resolution&entry.906535625=Wenting%20Chen%20and%20Jie%20Liu%20and%20Tommy%20W.%20S.%20Chow%20and%20Yixuan%20Yuan&entry.1292438233=%20%20Pathology%20image%20are%20essential%20for%20accurately%20interpreting%20lesion%20cells%20in%0Acytopathology%20screening%2C%20but%20acquiring%20high-resolution%20digital%20slides%20requires%0Aspecialized%20equipment%20and%20long%20scanning%20times.%20Though%20super-resolution%20%28SR%29%0Atechniques%20can%20alleviate%20this%20problem%2C%20existing%20deep%20learning%20models%20recover%0Apathology%20image%20in%20a%20black-box%20manner%2C%20which%20can%20lead%20to%20untruthful%20biological%0Adetails%20and%20misdiagnosis.%20Additionally%2C%20current%20methods%20allocate%20the%20same%0Acomputational%20resources%20to%20recover%20each%20pixel%20of%20pathology%20image%2C%20leading%20to%0Athe%20sub-optimal%20recovery%20issue%20due%20to%20the%20large%20variation%20of%20pathology%20image.%0AIn%20this%20paper%2C%20we%20propose%20the%20first%20hierarchical%20reinforcement%20learning%0Aframework%20named%20Spatial-Temporal%20hierARchical%20Reinforcement%20Learning%20%28STAR-RL%29%2C%0Amainly%20for%20addressing%20the%20aforementioned%20issues%20in%20pathology%20image%0Asuper-resolution%20problem.%20We%20reformulate%20the%20SR%20problem%20as%20a%20Markov%20decision%0Aprocess%20of%20interpretable%20operations%20and%20adopt%20the%20hierarchical%20recovery%0Amechanism%20in%20patch%20level%2C%20to%20avoid%20sub-optimal%20recovery.%20Specifically%2C%20the%0Ahigher-level%20spatial%20manager%20is%20proposed%20to%20pick%20out%20the%20most%20corrupted%20patch%0Afor%20the%20lower-level%20patch%20worker.%20Moreover%2C%20the%20higher-level%20temporal%20manager%0Ais%20advanced%20to%20evaluate%20the%20selected%20patch%20and%20determine%20whether%20the%0Aoptimization%20should%20be%20stopped%20earlier%2C%20thereby%20avoiding%20the%20over-processed%0Aproblem.%20Under%20the%20guidance%20of%20spatial-temporal%20managers%2C%20the%20lower-level%20patch%0Aworker%20processes%20the%20selected%20patch%20with%20pixel-wise%20interpretable%20actions%20at%0Aeach%20time%20step.%20Experimental%20results%20on%20medical%20images%20degraded%20by%20different%0Akernels%20show%20the%20effectiveness%20of%20STAR-RL.%20Furthermore%2C%20STAR-RL%20validates%20the%0Apromotion%20in%20tumor%20diagnosis%20with%20a%20large%20margin%20and%20shows%20generalizability%0Aunder%20various%20degradations.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/CUHK-AIM-Group/STAR-RL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18310v1&entry.124074799=Read"},
{"title": "Minimal Interaction Edge Tuning: A New Paradigm for Visual Adaptation", "author": "Ningyuan Tang and Minghao Fu and Jianxin Wu", "abstract": "  The rapid scaling of large vision pretrained models makes fine-tuning tasks\nmore and more difficult on edge devices with low computational resources. We\nexplore a new visual adaptation paradigm called edge tuning, which treats large\npretrained models as standalone feature extractors that run on powerful cloud\nservers. The fine-tuning carries out on edge devices with small networks which\nrequire low computational resources. Existing methods that are potentially\nsuitable for our edge tuning paradigm are discussed. But, three major drawbacks\nhinder their application in edge tuning: low adaptation capability, large\nadapter network, and high information transfer overhead. To address these\nissues, we propose Minimal Interaction Edge Tuning, or MIET, which reveals that\nthe sum of intermediate features from pretrained models not only has minimal\ninformation transfer but also has high adaptation capability. With a\nlightweight attention-based adaptor network, MIET achieves information transfer\nefficiency, parameter efficiency, computational and memory efficiency, and at\nthe same time demonstrates competitive results on various visual adaptation\nbenchmarks.\n", "link": "http://arxiv.org/abs/2406.17559v2", "date": "2024-06-26", "relevancy": 2.6697, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5478}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5419}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Minimal%20Interaction%20Edge%20Tuning%3A%20A%20New%20Paradigm%20for%20Visual%20Adaptation&body=Title%3A%20Minimal%20Interaction%20Edge%20Tuning%3A%20A%20New%20Paradigm%20for%20Visual%20Adaptation%0AAuthor%3A%20Ningyuan%20Tang%20and%20Minghao%20Fu%20and%20Jianxin%20Wu%0AAbstract%3A%20%20%20The%20rapid%20scaling%20of%20large%20vision%20pretrained%20models%20makes%20fine-tuning%20tasks%0Amore%20and%20more%20difficult%20on%20edge%20devices%20with%20low%20computational%20resources.%20We%0Aexplore%20a%20new%20visual%20adaptation%20paradigm%20called%20edge%20tuning%2C%20which%20treats%20large%0Apretrained%20models%20as%20standalone%20feature%20extractors%20that%20run%20on%20powerful%20cloud%0Aservers.%20The%20fine-tuning%20carries%20out%20on%20edge%20devices%20with%20small%20networks%20which%0Arequire%20low%20computational%20resources.%20Existing%20methods%20that%20are%20potentially%0Asuitable%20for%20our%20edge%20tuning%20paradigm%20are%20discussed.%20But%2C%20three%20major%20drawbacks%0Ahinder%20their%20application%20in%20edge%20tuning%3A%20low%20adaptation%20capability%2C%20large%0Aadapter%20network%2C%20and%20high%20information%20transfer%20overhead.%20To%20address%20these%0Aissues%2C%20we%20propose%20Minimal%20Interaction%20Edge%20Tuning%2C%20or%20MIET%2C%20which%20reveals%20that%0Athe%20sum%20of%20intermediate%20features%20from%20pretrained%20models%20not%20only%20has%20minimal%0Ainformation%20transfer%20but%20also%20has%20high%20adaptation%20capability.%20With%20a%0Alightweight%20attention-based%20adaptor%20network%2C%20MIET%20achieves%20information%20transfer%0Aefficiency%2C%20parameter%20efficiency%2C%20computational%20and%20memory%20efficiency%2C%20and%20at%0Athe%20same%20time%20demonstrates%20competitive%20results%20on%20various%20visual%20adaptation%0Abenchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17559v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMinimal%2520Interaction%2520Edge%2520Tuning%253A%2520A%2520New%2520Paradigm%2520for%2520Visual%2520Adaptation%26entry.906535625%3DNingyuan%2520Tang%2520and%2520Minghao%2520Fu%2520and%2520Jianxin%2520Wu%26entry.1292438233%3D%2520%2520The%2520rapid%2520scaling%2520of%2520large%2520vision%2520pretrained%2520models%2520makes%2520fine-tuning%2520tasks%250Amore%2520and%2520more%2520difficult%2520on%2520edge%2520devices%2520with%2520low%2520computational%2520resources.%2520We%250Aexplore%2520a%2520new%2520visual%2520adaptation%2520paradigm%2520called%2520edge%2520tuning%252C%2520which%2520treats%2520large%250Apretrained%2520models%2520as%2520standalone%2520feature%2520extractors%2520that%2520run%2520on%2520powerful%2520cloud%250Aservers.%2520The%2520fine-tuning%2520carries%2520out%2520on%2520edge%2520devices%2520with%2520small%2520networks%2520which%250Arequire%2520low%2520computational%2520resources.%2520Existing%2520methods%2520that%2520are%2520potentially%250Asuitable%2520for%2520our%2520edge%2520tuning%2520paradigm%2520are%2520discussed.%2520But%252C%2520three%2520major%2520drawbacks%250Ahinder%2520their%2520application%2520in%2520edge%2520tuning%253A%2520low%2520adaptation%2520capability%252C%2520large%250Aadapter%2520network%252C%2520and%2520high%2520information%2520transfer%2520overhead.%2520To%2520address%2520these%250Aissues%252C%2520we%2520propose%2520Minimal%2520Interaction%2520Edge%2520Tuning%252C%2520or%2520MIET%252C%2520which%2520reveals%2520that%250Athe%2520sum%2520of%2520intermediate%2520features%2520from%2520pretrained%2520models%2520not%2520only%2520has%2520minimal%250Ainformation%2520transfer%2520but%2520also%2520has%2520high%2520adaptation%2520capability.%2520With%2520a%250Alightweight%2520attention-based%2520adaptor%2520network%252C%2520MIET%2520achieves%2520information%2520transfer%250Aefficiency%252C%2520parameter%2520efficiency%252C%2520computational%2520and%2520memory%2520efficiency%252C%2520and%2520at%250Athe%2520same%2520time%2520demonstrates%2520competitive%2520results%2520on%2520various%2520visual%2520adaptation%250Abenchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17559v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Minimal%20Interaction%20Edge%20Tuning%3A%20A%20New%20Paradigm%20for%20Visual%20Adaptation&entry.906535625=Ningyuan%20Tang%20and%20Minghao%20Fu%20and%20Jianxin%20Wu&entry.1292438233=%20%20The%20rapid%20scaling%20of%20large%20vision%20pretrained%20models%20makes%20fine-tuning%20tasks%0Amore%20and%20more%20difficult%20on%20edge%20devices%20with%20low%20computational%20resources.%20We%0Aexplore%20a%20new%20visual%20adaptation%20paradigm%20called%20edge%20tuning%2C%20which%20treats%20large%0Apretrained%20models%20as%20standalone%20feature%20extractors%20that%20run%20on%20powerful%20cloud%0Aservers.%20The%20fine-tuning%20carries%20out%20on%20edge%20devices%20with%20small%20networks%20which%0Arequire%20low%20computational%20resources.%20Existing%20methods%20that%20are%20potentially%0Asuitable%20for%20our%20edge%20tuning%20paradigm%20are%20discussed.%20But%2C%20three%20major%20drawbacks%0Ahinder%20their%20application%20in%20edge%20tuning%3A%20low%20adaptation%20capability%2C%20large%0Aadapter%20network%2C%20and%20high%20information%20transfer%20overhead.%20To%20address%20these%0Aissues%2C%20we%20propose%20Minimal%20Interaction%20Edge%20Tuning%2C%20or%20MIET%2C%20which%20reveals%20that%0Athe%20sum%20of%20intermediate%20features%20from%20pretrained%20models%20not%20only%20has%20minimal%0Ainformation%20transfer%20but%20also%20has%20high%20adaptation%20capability.%20With%20a%0Alightweight%20attention-based%20adaptor%20network%2C%20MIET%20achieves%20information%20transfer%0Aefficiency%2C%20parameter%20efficiency%2C%20computational%20and%20memory%20efficiency%2C%20and%20at%0Athe%20same%20time%20demonstrates%20competitive%20results%20on%20various%20visual%20adaptation%0Abenchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17559v2&entry.124074799=Read"},
{"title": "On Scaling Up 3D Gaussian Splatting Training", "author": "Hexu Zhao and Haoyang Weng and Daohan Lu and Ang Li and Jinyang Li and Aurojit Panda and Saining Xie", "abstract": "  3D Gaussian Splatting (3DGS) is increasingly popular for 3D reconstruction\ndue to its superior visual quality and rendering speed. However, 3DGS training\ncurrently occurs on a single GPU, limiting its ability to handle\nhigh-resolution and large-scale 3D reconstruction tasks due to memory\nconstraints. We introduce Grendel, a distributed system designed to partition\n3DGS parameters and parallelize computation across multiple GPUs. As each\nGaussian affects a small, dynamic subset of rendered pixels, Grendel employs\nsparse all-to-all communication to transfer the necessary Gaussians to pixel\npartitions and performs dynamic load balancing. Unlike existing 3DGS systems\nthat train using one camera view image at a time, Grendel supports batched\ntraining with multiple views. We explore various optimization hyperparameter\nscaling strategies and find that a simple sqrt(batch size) scaling rule is\nhighly effective. Evaluations using large-scale, high-resolution scenes show\nthat Grendel enhances rendering quality by scaling up 3DGS parameters across\nmultiple GPUs. On the Rubble dataset, we achieve a test PSNR of 27.28 by\ndistributing 40.4 million Gaussians across 16 GPUs, compared to a PSNR of 26.28\nusing 11.2 million Gaussians on a single GPU. Grendel is an open-source project\navailable at: https://github.com/nyu-systems/Grendel-GS\n", "link": "http://arxiv.org/abs/2406.18533v1", "date": "2024-06-26", "relevancy": 2.6497, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7254}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6513}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Scaling%20Up%203D%20Gaussian%20Splatting%20Training&body=Title%3A%20On%20Scaling%20Up%203D%20Gaussian%20Splatting%20Training%0AAuthor%3A%20Hexu%20Zhao%20and%20Haoyang%20Weng%20and%20Daohan%20Lu%20and%20Ang%20Li%20and%20Jinyang%20Li%20and%20Aurojit%20Panda%20and%20Saining%20Xie%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20is%20increasingly%20popular%20for%203D%20reconstruction%0Adue%20to%20its%20superior%20visual%20quality%20and%20rendering%20speed.%20However%2C%203DGS%20training%0Acurrently%20occurs%20on%20a%20single%20GPU%2C%20limiting%20its%20ability%20to%20handle%0Ahigh-resolution%20and%20large-scale%203D%20reconstruction%20tasks%20due%20to%20memory%0Aconstraints.%20We%20introduce%20Grendel%2C%20a%20distributed%20system%20designed%20to%20partition%0A3DGS%20parameters%20and%20parallelize%20computation%20across%20multiple%20GPUs.%20As%20each%0AGaussian%20affects%20a%20small%2C%20dynamic%20subset%20of%20rendered%20pixels%2C%20Grendel%20employs%0Asparse%20all-to-all%20communication%20to%20transfer%20the%20necessary%20Gaussians%20to%20pixel%0Apartitions%20and%20performs%20dynamic%20load%20balancing.%20Unlike%20existing%203DGS%20systems%0Athat%20train%20using%20one%20camera%20view%20image%20at%20a%20time%2C%20Grendel%20supports%20batched%0Atraining%20with%20multiple%20views.%20We%20explore%20various%20optimization%20hyperparameter%0Ascaling%20strategies%20and%20find%20that%20a%20simple%20sqrt%28batch%20size%29%20scaling%20rule%20is%0Ahighly%20effective.%20Evaluations%20using%20large-scale%2C%20high-resolution%20scenes%20show%0Athat%20Grendel%20enhances%20rendering%20quality%20by%20scaling%20up%203DGS%20parameters%20across%0Amultiple%20GPUs.%20On%20the%20Rubble%20dataset%2C%20we%20achieve%20a%20test%20PSNR%20of%2027.28%20by%0Adistributing%2040.4%20million%20Gaussians%20across%2016%20GPUs%2C%20compared%20to%20a%20PSNR%20of%2026.28%0Ausing%2011.2%20million%20Gaussians%20on%20a%20single%20GPU.%20Grendel%20is%20an%20open-source%20project%0Aavailable%20at%3A%20https%3A//github.com/nyu-systems/Grendel-GS%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18533v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Scaling%2520Up%25203D%2520Gaussian%2520Splatting%2520Training%26entry.906535625%3DHexu%2520Zhao%2520and%2520Haoyang%2520Weng%2520and%2520Daohan%2520Lu%2520and%2520Ang%2520Li%2520and%2520Jinyang%2520Li%2520and%2520Aurojit%2520Panda%2520and%2520Saining%2520Xie%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520is%2520increasingly%2520popular%2520for%25203D%2520reconstruction%250Adue%2520to%2520its%2520superior%2520visual%2520quality%2520and%2520rendering%2520speed.%2520However%252C%25203DGS%2520training%250Acurrently%2520occurs%2520on%2520a%2520single%2520GPU%252C%2520limiting%2520its%2520ability%2520to%2520handle%250Ahigh-resolution%2520and%2520large-scale%25203D%2520reconstruction%2520tasks%2520due%2520to%2520memory%250Aconstraints.%2520We%2520introduce%2520Grendel%252C%2520a%2520distributed%2520system%2520designed%2520to%2520partition%250A3DGS%2520parameters%2520and%2520parallelize%2520computation%2520across%2520multiple%2520GPUs.%2520As%2520each%250AGaussian%2520affects%2520a%2520small%252C%2520dynamic%2520subset%2520of%2520rendered%2520pixels%252C%2520Grendel%2520employs%250Asparse%2520all-to-all%2520communication%2520to%2520transfer%2520the%2520necessary%2520Gaussians%2520to%2520pixel%250Apartitions%2520and%2520performs%2520dynamic%2520load%2520balancing.%2520Unlike%2520existing%25203DGS%2520systems%250Athat%2520train%2520using%2520one%2520camera%2520view%2520image%2520at%2520a%2520time%252C%2520Grendel%2520supports%2520batched%250Atraining%2520with%2520multiple%2520views.%2520We%2520explore%2520various%2520optimization%2520hyperparameter%250Ascaling%2520strategies%2520and%2520find%2520that%2520a%2520simple%2520sqrt%2528batch%2520size%2529%2520scaling%2520rule%2520is%250Ahighly%2520effective.%2520Evaluations%2520using%2520large-scale%252C%2520high-resolution%2520scenes%2520show%250Athat%2520Grendel%2520enhances%2520rendering%2520quality%2520by%2520scaling%2520up%25203DGS%2520parameters%2520across%250Amultiple%2520GPUs.%2520On%2520the%2520Rubble%2520dataset%252C%2520we%2520achieve%2520a%2520test%2520PSNR%2520of%252027.28%2520by%250Adistributing%252040.4%2520million%2520Gaussians%2520across%252016%2520GPUs%252C%2520compared%2520to%2520a%2520PSNR%2520of%252026.28%250Ausing%252011.2%2520million%2520Gaussians%2520on%2520a%2520single%2520GPU.%2520Grendel%2520is%2520an%2520open-source%2520project%250Aavailable%2520at%253A%2520https%253A//github.com/nyu-systems/Grendel-GS%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18533v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Scaling%20Up%203D%20Gaussian%20Splatting%20Training&entry.906535625=Hexu%20Zhao%20and%20Haoyang%20Weng%20and%20Daohan%20Lu%20and%20Ang%20Li%20and%20Jinyang%20Li%20and%20Aurojit%20Panda%20and%20Saining%20Xie&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20is%20increasingly%20popular%20for%203D%20reconstruction%0Adue%20to%20its%20superior%20visual%20quality%20and%20rendering%20speed.%20However%2C%203DGS%20training%0Acurrently%20occurs%20on%20a%20single%20GPU%2C%20limiting%20its%20ability%20to%20handle%0Ahigh-resolution%20and%20large-scale%203D%20reconstruction%20tasks%20due%20to%20memory%0Aconstraints.%20We%20introduce%20Grendel%2C%20a%20distributed%20system%20designed%20to%20partition%0A3DGS%20parameters%20and%20parallelize%20computation%20across%20multiple%20GPUs.%20As%20each%0AGaussian%20affects%20a%20small%2C%20dynamic%20subset%20of%20rendered%20pixels%2C%20Grendel%20employs%0Asparse%20all-to-all%20communication%20to%20transfer%20the%20necessary%20Gaussians%20to%20pixel%0Apartitions%20and%20performs%20dynamic%20load%20balancing.%20Unlike%20existing%203DGS%20systems%0Athat%20train%20using%20one%20camera%20view%20image%20at%20a%20time%2C%20Grendel%20supports%20batched%0Atraining%20with%20multiple%20views.%20We%20explore%20various%20optimization%20hyperparameter%0Ascaling%20strategies%20and%20find%20that%20a%20simple%20sqrt%28batch%20size%29%20scaling%20rule%20is%0Ahighly%20effective.%20Evaluations%20using%20large-scale%2C%20high-resolution%20scenes%20show%0Athat%20Grendel%20enhances%20rendering%20quality%20by%20scaling%20up%203DGS%20parameters%20across%0Amultiple%20GPUs.%20On%20the%20Rubble%20dataset%2C%20we%20achieve%20a%20test%20PSNR%20of%2027.28%20by%0Adistributing%2040.4%20million%20Gaussians%20across%2016%20GPUs%2C%20compared%20to%20a%20PSNR%20of%2026.28%0Ausing%2011.2%20million%20Gaussians%20on%20a%20single%20GPU.%20Grendel%20is%20an%20open-source%20project%0Aavailable%20at%3A%20https%3A//github.com/nyu-systems/Grendel-GS%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18533v1&entry.124074799=Read"},
{"title": "MultiDiff: Consistent Novel View Synthesis from a Single Image", "author": "Norman M\u00fcller and Katja Schwarz and Barbara Roessle and Lorenzo Porzi and Samuel Rota Bul\u00f2 and Matthias Nie\u00dfner and Peter Kontschieder", "abstract": "  We introduce MultiDiff, a novel approach for consistent novel view synthesis\nof scenes from a single RGB image. The task of synthesizing novel views from a\nsingle reference image is highly ill-posed by nature, as there exist multiple,\nplausible explanations for unobserved areas. To address this issue, we\nincorporate strong priors in form of monocular depth predictors and\nvideo-diffusion models. Monocular depth enables us to condition our model on\nwarped reference images for the target views, increasing geometric stability.\nThe video-diffusion prior provides a strong proxy for 3D scenes, allowing the\nmodel to learn continuous and pixel-accurate correspondences across generated\nimages. In contrast to approaches relying on autoregressive image generation\nthat are prone to drifts and error accumulation, MultiDiff jointly synthesizes\na sequence of frames yielding high-quality and multi-view consistent results --\neven for long-term scene generation with large camera movements, while reducing\ninference time by an order of magnitude. For additional consistency and image\nquality improvements, we introduce a novel, structured noise distribution. Our\nexperimental results demonstrate that MultiDiff outperforms state-of-the-art\nmethods on the challenging, real-world datasets RealEstate10K and ScanNet.\nFinally, our model naturally supports multi-view consistent editing without the\nneed for further tuning.\n", "link": "http://arxiv.org/abs/2406.18524v1", "date": "2024-06-26", "relevancy": 2.6434, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6706}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6706}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MultiDiff%3A%20Consistent%20Novel%20View%20Synthesis%20from%20a%20Single%20Image&body=Title%3A%20MultiDiff%3A%20Consistent%20Novel%20View%20Synthesis%20from%20a%20Single%20Image%0AAuthor%3A%20Norman%20M%C3%BCller%20and%20Katja%20Schwarz%20and%20Barbara%20Roessle%20and%20Lorenzo%20Porzi%20and%20Samuel%20Rota%20Bul%C3%B2%20and%20Matthias%20Nie%C3%9Fner%20and%20Peter%20Kontschieder%0AAbstract%3A%20%20%20We%20introduce%20MultiDiff%2C%20a%20novel%20approach%20for%20consistent%20novel%20view%20synthesis%0Aof%20scenes%20from%20a%20single%20RGB%20image.%20The%20task%20of%20synthesizing%20novel%20views%20from%20a%0Asingle%20reference%20image%20is%20highly%20ill-posed%20by%20nature%2C%20as%20there%20exist%20multiple%2C%0Aplausible%20explanations%20for%20unobserved%20areas.%20To%20address%20this%20issue%2C%20we%0Aincorporate%20strong%20priors%20in%20form%20of%20monocular%20depth%20predictors%20and%0Avideo-diffusion%20models.%20Monocular%20depth%20enables%20us%20to%20condition%20our%20model%20on%0Awarped%20reference%20images%20for%20the%20target%20views%2C%20increasing%20geometric%20stability.%0AThe%20video-diffusion%20prior%20provides%20a%20strong%20proxy%20for%203D%20scenes%2C%20allowing%20the%0Amodel%20to%20learn%20continuous%20and%20pixel-accurate%20correspondences%20across%20generated%0Aimages.%20In%20contrast%20to%20approaches%20relying%20on%20autoregressive%20image%20generation%0Athat%20are%20prone%20to%20drifts%20and%20error%20accumulation%2C%20MultiDiff%20jointly%20synthesizes%0Aa%20sequence%20of%20frames%20yielding%20high-quality%20and%20multi-view%20consistent%20results%20--%0Aeven%20for%20long-term%20scene%20generation%20with%20large%20camera%20movements%2C%20while%20reducing%0Ainference%20time%20by%20an%20order%20of%20magnitude.%20For%20additional%20consistency%20and%20image%0Aquality%20improvements%2C%20we%20introduce%20a%20novel%2C%20structured%20noise%20distribution.%20Our%0Aexperimental%20results%20demonstrate%20that%20MultiDiff%20outperforms%20state-of-the-art%0Amethods%20on%20the%20challenging%2C%20real-world%20datasets%20RealEstate10K%20and%20ScanNet.%0AFinally%2C%20our%20model%20naturally%20supports%20multi-view%20consistent%20editing%20without%20the%0Aneed%20for%20further%20tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18524v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiDiff%253A%2520Consistent%2520Novel%2520View%2520Synthesis%2520from%2520a%2520Single%2520Image%26entry.906535625%3DNorman%2520M%25C3%25BCller%2520and%2520Katja%2520Schwarz%2520and%2520Barbara%2520Roessle%2520and%2520Lorenzo%2520Porzi%2520and%2520Samuel%2520Rota%2520Bul%25C3%25B2%2520and%2520Matthias%2520Nie%25C3%259Fner%2520and%2520Peter%2520Kontschieder%26entry.1292438233%3D%2520%2520We%2520introduce%2520MultiDiff%252C%2520a%2520novel%2520approach%2520for%2520consistent%2520novel%2520view%2520synthesis%250Aof%2520scenes%2520from%2520a%2520single%2520RGB%2520image.%2520The%2520task%2520of%2520synthesizing%2520novel%2520views%2520from%2520a%250Asingle%2520reference%2520image%2520is%2520highly%2520ill-posed%2520by%2520nature%252C%2520as%2520there%2520exist%2520multiple%252C%250Aplausible%2520explanations%2520for%2520unobserved%2520areas.%2520To%2520address%2520this%2520issue%252C%2520we%250Aincorporate%2520strong%2520priors%2520in%2520form%2520of%2520monocular%2520depth%2520predictors%2520and%250Avideo-diffusion%2520models.%2520Monocular%2520depth%2520enables%2520us%2520to%2520condition%2520our%2520model%2520on%250Awarped%2520reference%2520images%2520for%2520the%2520target%2520views%252C%2520increasing%2520geometric%2520stability.%250AThe%2520video-diffusion%2520prior%2520provides%2520a%2520strong%2520proxy%2520for%25203D%2520scenes%252C%2520allowing%2520the%250Amodel%2520to%2520learn%2520continuous%2520and%2520pixel-accurate%2520correspondences%2520across%2520generated%250Aimages.%2520In%2520contrast%2520to%2520approaches%2520relying%2520on%2520autoregressive%2520image%2520generation%250Athat%2520are%2520prone%2520to%2520drifts%2520and%2520error%2520accumulation%252C%2520MultiDiff%2520jointly%2520synthesizes%250Aa%2520sequence%2520of%2520frames%2520yielding%2520high-quality%2520and%2520multi-view%2520consistent%2520results%2520--%250Aeven%2520for%2520long-term%2520scene%2520generation%2520with%2520large%2520camera%2520movements%252C%2520while%2520reducing%250Ainference%2520time%2520by%2520an%2520order%2520of%2520magnitude.%2520For%2520additional%2520consistency%2520and%2520image%250Aquality%2520improvements%252C%2520we%2520introduce%2520a%2520novel%252C%2520structured%2520noise%2520distribution.%2520Our%250Aexperimental%2520results%2520demonstrate%2520that%2520MultiDiff%2520outperforms%2520state-of-the-art%250Amethods%2520on%2520the%2520challenging%252C%2520real-world%2520datasets%2520RealEstate10K%2520and%2520ScanNet.%250AFinally%252C%2520our%2520model%2520naturally%2520supports%2520multi-view%2520consistent%2520editing%2520without%2520the%250Aneed%2520for%2520further%2520tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18524v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MultiDiff%3A%20Consistent%20Novel%20View%20Synthesis%20from%20a%20Single%20Image&entry.906535625=Norman%20M%C3%BCller%20and%20Katja%20Schwarz%20and%20Barbara%20Roessle%20and%20Lorenzo%20Porzi%20and%20Samuel%20Rota%20Bul%C3%B2%20and%20Matthias%20Nie%C3%9Fner%20and%20Peter%20Kontschieder&entry.1292438233=%20%20We%20introduce%20MultiDiff%2C%20a%20novel%20approach%20for%20consistent%20novel%20view%20synthesis%0Aof%20scenes%20from%20a%20single%20RGB%20image.%20The%20task%20of%20synthesizing%20novel%20views%20from%20a%0Asingle%20reference%20image%20is%20highly%20ill-posed%20by%20nature%2C%20as%20there%20exist%20multiple%2C%0Aplausible%20explanations%20for%20unobserved%20areas.%20To%20address%20this%20issue%2C%20we%0Aincorporate%20strong%20priors%20in%20form%20of%20monocular%20depth%20predictors%20and%0Avideo-diffusion%20models.%20Monocular%20depth%20enables%20us%20to%20condition%20our%20model%20on%0Awarped%20reference%20images%20for%20the%20target%20views%2C%20increasing%20geometric%20stability.%0AThe%20video-diffusion%20prior%20provides%20a%20strong%20proxy%20for%203D%20scenes%2C%20allowing%20the%0Amodel%20to%20learn%20continuous%20and%20pixel-accurate%20correspondences%20across%20generated%0Aimages.%20In%20contrast%20to%20approaches%20relying%20on%20autoregressive%20image%20generation%0Athat%20are%20prone%20to%20drifts%20and%20error%20accumulation%2C%20MultiDiff%20jointly%20synthesizes%0Aa%20sequence%20of%20frames%20yielding%20high-quality%20and%20multi-view%20consistent%20results%20--%0Aeven%20for%20long-term%20scene%20generation%20with%20large%20camera%20movements%2C%20while%20reducing%0Ainference%20time%20by%20an%20order%20of%20magnitude.%20For%20additional%20consistency%20and%20image%0Aquality%20improvements%2C%20we%20introduce%20a%20novel%2C%20structured%20noise%20distribution.%20Our%0Aexperimental%20results%20demonstrate%20that%20MultiDiff%20outperforms%20state-of-the-art%0Amethods%20on%20the%20challenging%2C%20real-world%20datasets%20RealEstate10K%20and%20ScanNet.%0AFinally%2C%20our%20model%20naturally%20supports%20multi-view%20consistent%20editing%20without%20the%0Aneed%20for%20further%20tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18524v1&entry.124074799=Read"},
{"title": "AlignedCut: Visual Concepts Discovery on Brain-Guided Universal Feature\n  Space", "author": "Huzheng Yang and James Gee and Jianbo Shi", "abstract": "  We study the intriguing connection between visual data, deep networks, and\nthe brain. Our method creates a universal channel alignment by using brain\nvoxel fMRI response prediction as the training objective. We discover that deep\nnetworks, trained with different objectives, share common feature channels\nacross various models. These channels can be clustered into recurring sets,\ncorresponding to distinct brain regions, indicating the formation of visual\nconcepts. Tracing the clusters of channel responses onto the images, we see\nsemantically meaningful object segments emerge, even without any supervised\ndecoder. Furthermore, the universal feature alignment and the clustering of\nchannels produce a picture and quantification of how visual information is\nprocessed through the different network layers, which produces precise\ncomparisons between the networks.\n", "link": "http://arxiv.org/abs/2406.18344v1", "date": "2024-06-26", "relevancy": 2.6287, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.542}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5332}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AlignedCut%3A%20Visual%20Concepts%20Discovery%20on%20Brain-Guided%20Universal%20Feature%0A%20%20Space&body=Title%3A%20AlignedCut%3A%20Visual%20Concepts%20Discovery%20on%20Brain-Guided%20Universal%20Feature%0A%20%20Space%0AAuthor%3A%20Huzheng%20Yang%20and%20James%20Gee%20and%20Jianbo%20Shi%0AAbstract%3A%20%20%20We%20study%20the%20intriguing%20connection%20between%20visual%20data%2C%20deep%20networks%2C%20and%0Athe%20brain.%20Our%20method%20creates%20a%20universal%20channel%20alignment%20by%20using%20brain%0Avoxel%20fMRI%20response%20prediction%20as%20the%20training%20objective.%20We%20discover%20that%20deep%0Anetworks%2C%20trained%20with%20different%20objectives%2C%20share%20common%20feature%20channels%0Aacross%20various%20models.%20These%20channels%20can%20be%20clustered%20into%20recurring%20sets%2C%0Acorresponding%20to%20distinct%20brain%20regions%2C%20indicating%20the%20formation%20of%20visual%0Aconcepts.%20Tracing%20the%20clusters%20of%20channel%20responses%20onto%20the%20images%2C%20we%20see%0Asemantically%20meaningful%20object%20segments%20emerge%2C%20even%20without%20any%20supervised%0Adecoder.%20Furthermore%2C%20the%20universal%20feature%20alignment%20and%20the%20clustering%20of%0Achannels%20produce%20a%20picture%20and%20quantification%20of%20how%20visual%20information%20is%0Aprocessed%20through%20the%20different%20network%20layers%2C%20which%20produces%20precise%0Acomparisons%20between%20the%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18344v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlignedCut%253A%2520Visual%2520Concepts%2520Discovery%2520on%2520Brain-Guided%2520Universal%2520Feature%250A%2520%2520Space%26entry.906535625%3DHuzheng%2520Yang%2520and%2520James%2520Gee%2520and%2520Jianbo%2520Shi%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520intriguing%2520connection%2520between%2520visual%2520data%252C%2520deep%2520networks%252C%2520and%250Athe%2520brain.%2520Our%2520method%2520creates%2520a%2520universal%2520channel%2520alignment%2520by%2520using%2520brain%250Avoxel%2520fMRI%2520response%2520prediction%2520as%2520the%2520training%2520objective.%2520We%2520discover%2520that%2520deep%250Anetworks%252C%2520trained%2520with%2520different%2520objectives%252C%2520share%2520common%2520feature%2520channels%250Aacross%2520various%2520models.%2520These%2520channels%2520can%2520be%2520clustered%2520into%2520recurring%2520sets%252C%250Acorresponding%2520to%2520distinct%2520brain%2520regions%252C%2520indicating%2520the%2520formation%2520of%2520visual%250Aconcepts.%2520Tracing%2520the%2520clusters%2520of%2520channel%2520responses%2520onto%2520the%2520images%252C%2520we%2520see%250Asemantically%2520meaningful%2520object%2520segments%2520emerge%252C%2520even%2520without%2520any%2520supervised%250Adecoder.%2520Furthermore%252C%2520the%2520universal%2520feature%2520alignment%2520and%2520the%2520clustering%2520of%250Achannels%2520produce%2520a%2520picture%2520and%2520quantification%2520of%2520how%2520visual%2520information%2520is%250Aprocessed%2520through%2520the%2520different%2520network%2520layers%252C%2520which%2520produces%2520precise%250Acomparisons%2520between%2520the%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18344v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AlignedCut%3A%20Visual%20Concepts%20Discovery%20on%20Brain-Guided%20Universal%20Feature%0A%20%20Space&entry.906535625=Huzheng%20Yang%20and%20James%20Gee%20and%20Jianbo%20Shi&entry.1292438233=%20%20We%20study%20the%20intriguing%20connection%20between%20visual%20data%2C%20deep%20networks%2C%20and%0Athe%20brain.%20Our%20method%20creates%20a%20universal%20channel%20alignment%20by%20using%20brain%0Avoxel%20fMRI%20response%20prediction%20as%20the%20training%20objective.%20We%20discover%20that%20deep%0Anetworks%2C%20trained%20with%20different%20objectives%2C%20share%20common%20feature%20channels%0Aacross%20various%20models.%20These%20channels%20can%20be%20clustered%20into%20recurring%20sets%2C%0Acorresponding%20to%20distinct%20brain%20regions%2C%20indicating%20the%20formation%20of%20visual%0Aconcepts.%20Tracing%20the%20clusters%20of%20channel%20responses%20onto%20the%20images%2C%20we%20see%0Asemantically%20meaningful%20object%20segments%20emerge%2C%20even%20without%20any%20supervised%0Adecoder.%20Furthermore%2C%20the%20universal%20feature%20alignment%20and%20the%20clustering%20of%0Achannels%20produce%20a%20picture%20and%20quantification%20of%20how%20visual%20information%20is%0Aprocessed%20through%20the%20different%20network%20layers%2C%20which%20produces%20precise%0Acomparisons%20between%20the%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18344v1&entry.124074799=Read"},
{"title": "Splatter a Video: Video Gaussian Representation for Versatile Processing", "author": "Yang-Tian Sun and Yi-Hua Huang and Lin Ma and Xiaoyang Lyu and Yan-Pei Cao and Xiaojuan Qi", "abstract": "  Video representation is a long-standing problem that is crucial for various\ndown-stream tasks, such as tracking,depth prediction,segmentation,view\nsynthesis,and editing. However, current methods either struggle to model\ncomplex motions due to the absence of 3D structure or rely on implicit 3D\nrepresentations that are ill-suited for manipulation tasks. To address these\nchallenges, we introduce a novel explicit 3D representation-video Gaussian\nrepresentation -- that embeds a video into 3D Gaussians. Our proposed\nrepresentation models video appearance in a 3D canonical space using explicit\nGaussians as proxies and associates each Gaussian with 3D motions for video\nmotion. This approach offers a more intrinsic and explicit representation than\nlayered atlas or volumetric pixel matrices. To obtain such a representation, we\ndistill 2D priors, such as optical flow and depth, from foundation models to\nregularize learning in this ill-posed setting. Extensive applications\ndemonstrate the versatility of our new video representation. It has been proven\neffective in numerous video processing tasks, including tracking, consistent\nvideo depth and feature refinement, motion and appearance editing, and\nstereoscopic video generation. Project page:\nhttps://sunyangtian.github.io/spatter_a_video_web/\n", "link": "http://arxiv.org/abs/2406.13870v2", "date": "2024-06-26", "relevancy": 2.6283, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6675}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6581}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6286}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Splatter%20a%20Video%3A%20Video%20Gaussian%20Representation%20for%20Versatile%20Processing&body=Title%3A%20Splatter%20a%20Video%3A%20Video%20Gaussian%20Representation%20for%20Versatile%20Processing%0AAuthor%3A%20Yang-Tian%20Sun%20and%20Yi-Hua%20Huang%20and%20Lin%20Ma%20and%20Xiaoyang%20Lyu%20and%20Yan-Pei%20Cao%20and%20Xiaojuan%20Qi%0AAbstract%3A%20%20%20Video%20representation%20is%20a%20long-standing%20problem%20that%20is%20crucial%20for%20various%0Adown-stream%20tasks%2C%20such%20as%20tracking%2Cdepth%20prediction%2Csegmentation%2Cview%0Asynthesis%2Cand%20editing.%20However%2C%20current%20methods%20either%20struggle%20to%20model%0Acomplex%20motions%20due%20to%20the%20absence%20of%203D%20structure%20or%20rely%20on%20implicit%203D%0Arepresentations%20that%20are%20ill-suited%20for%20manipulation%20tasks.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20a%20novel%20explicit%203D%20representation-video%20Gaussian%0Arepresentation%20--%20that%20embeds%20a%20video%20into%203D%20Gaussians.%20Our%20proposed%0Arepresentation%20models%20video%20appearance%20in%20a%203D%20canonical%20space%20using%20explicit%0AGaussians%20as%20proxies%20and%20associates%20each%20Gaussian%20with%203D%20motions%20for%20video%0Amotion.%20This%20approach%20offers%20a%20more%20intrinsic%20and%20explicit%20representation%20than%0Alayered%20atlas%20or%20volumetric%20pixel%20matrices.%20To%20obtain%20such%20a%20representation%2C%20we%0Adistill%202D%20priors%2C%20such%20as%20optical%20flow%20and%20depth%2C%20from%20foundation%20models%20to%0Aregularize%20learning%20in%20this%20ill-posed%20setting.%20Extensive%20applications%0Ademonstrate%20the%20versatility%20of%20our%20new%20video%20representation.%20It%20has%20been%20proven%0Aeffective%20in%20numerous%20video%20processing%20tasks%2C%20including%20tracking%2C%20consistent%0Avideo%20depth%20and%20feature%20refinement%2C%20motion%20and%20appearance%20editing%2C%20and%0Astereoscopic%20video%20generation.%20Project%20page%3A%0Ahttps%3A//sunyangtian.github.io/spatter_a_video_web/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.13870v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSplatter%2520a%2520Video%253A%2520Video%2520Gaussian%2520Representation%2520for%2520Versatile%2520Processing%26entry.906535625%3DYang-Tian%2520Sun%2520and%2520Yi-Hua%2520Huang%2520and%2520Lin%2520Ma%2520and%2520Xiaoyang%2520Lyu%2520and%2520Yan-Pei%2520Cao%2520and%2520Xiaojuan%2520Qi%26entry.1292438233%3D%2520%2520Video%2520representation%2520is%2520a%2520long-standing%2520problem%2520that%2520is%2520crucial%2520for%2520various%250Adown-stream%2520tasks%252C%2520such%2520as%2520tracking%252Cdepth%2520prediction%252Csegmentation%252Cview%250Asynthesis%252Cand%2520editing.%2520However%252C%2520current%2520methods%2520either%2520struggle%2520to%2520model%250Acomplex%2520motions%2520due%2520to%2520the%2520absence%2520of%25203D%2520structure%2520or%2520rely%2520on%2520implicit%25203D%250Arepresentations%2520that%2520are%2520ill-suited%2520for%2520manipulation%2520tasks.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520introduce%2520a%2520novel%2520explicit%25203D%2520representation-video%2520Gaussian%250Arepresentation%2520--%2520that%2520embeds%2520a%2520video%2520into%25203D%2520Gaussians.%2520Our%2520proposed%250Arepresentation%2520models%2520video%2520appearance%2520in%2520a%25203D%2520canonical%2520space%2520using%2520explicit%250AGaussians%2520as%2520proxies%2520and%2520associates%2520each%2520Gaussian%2520with%25203D%2520motions%2520for%2520video%250Amotion.%2520This%2520approach%2520offers%2520a%2520more%2520intrinsic%2520and%2520explicit%2520representation%2520than%250Alayered%2520atlas%2520or%2520volumetric%2520pixel%2520matrices.%2520To%2520obtain%2520such%2520a%2520representation%252C%2520we%250Adistill%25202D%2520priors%252C%2520such%2520as%2520optical%2520flow%2520and%2520depth%252C%2520from%2520foundation%2520models%2520to%250Aregularize%2520learning%2520in%2520this%2520ill-posed%2520setting.%2520Extensive%2520applications%250Ademonstrate%2520the%2520versatility%2520of%2520our%2520new%2520video%2520representation.%2520It%2520has%2520been%2520proven%250Aeffective%2520in%2520numerous%2520video%2520processing%2520tasks%252C%2520including%2520tracking%252C%2520consistent%250Avideo%2520depth%2520and%2520feature%2520refinement%252C%2520motion%2520and%2520appearance%2520editing%252C%2520and%250Astereoscopic%2520video%2520generation.%2520Project%2520page%253A%250Ahttps%253A//sunyangtian.github.io/spatter_a_video_web/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.13870v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Splatter%20a%20Video%3A%20Video%20Gaussian%20Representation%20for%20Versatile%20Processing&entry.906535625=Yang-Tian%20Sun%20and%20Yi-Hua%20Huang%20and%20Lin%20Ma%20and%20Xiaoyang%20Lyu%20and%20Yan-Pei%20Cao%20and%20Xiaojuan%20Qi&entry.1292438233=%20%20Video%20representation%20is%20a%20long-standing%20problem%20that%20is%20crucial%20for%20various%0Adown-stream%20tasks%2C%20such%20as%20tracking%2Cdepth%20prediction%2Csegmentation%2Cview%0Asynthesis%2Cand%20editing.%20However%2C%20current%20methods%20either%20struggle%20to%20model%0Acomplex%20motions%20due%20to%20the%20absence%20of%203D%20structure%20or%20rely%20on%20implicit%203D%0Arepresentations%20that%20are%20ill-suited%20for%20manipulation%20tasks.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20a%20novel%20explicit%203D%20representation-video%20Gaussian%0Arepresentation%20--%20that%20embeds%20a%20video%20into%203D%20Gaussians.%20Our%20proposed%0Arepresentation%20models%20video%20appearance%20in%20a%203D%20canonical%20space%20using%20explicit%0AGaussians%20as%20proxies%20and%20associates%20each%20Gaussian%20with%203D%20motions%20for%20video%0Amotion.%20This%20approach%20offers%20a%20more%20intrinsic%20and%20explicit%20representation%20than%0Alayered%20atlas%20or%20volumetric%20pixel%20matrices.%20To%20obtain%20such%20a%20representation%2C%20we%0Adistill%202D%20priors%2C%20such%20as%20optical%20flow%20and%20depth%2C%20from%20foundation%20models%20to%0Aregularize%20learning%20in%20this%20ill-posed%20setting.%20Extensive%20applications%0Ademonstrate%20the%20versatility%20of%20our%20new%20video%20representation.%20It%20has%20been%20proven%0Aeffective%20in%20numerous%20video%20processing%20tasks%2C%20including%20tracking%2C%20consistent%0Avideo%20depth%20and%20feature%20refinement%2C%20motion%20and%20appearance%20editing%2C%20and%0Astereoscopic%20video%20generation.%20Project%20page%3A%0Ahttps%3A//sunyangtian.github.io/spatter_a_video_web/%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.13870v2&entry.124074799=Read"},
{"title": "Generalized Deepfake Attribution", "author": "Sowdagar Mahammad Shahid and Sudev Kumar Padhi and Umesh Kashyap and Sk. Subidh Ali", "abstract": "  The landscape of fake media creation changed with the introduction of\nGenerative Adversarial Networks (GAN s). Fake media creation has been on the\nrise with the rapid advances in generation technology, leading to new\nchallenges in Detecting fake media. A fundamental characteristic of GAN s is\ntheir sensitivity to parameter initialization, known as seeds. Each distinct\nseed utilized during training leads to the creation of unique model instances,\nresulting in divergent image outputs despite employing the same architecture.\nThis means that even if we have one GAN architecture, it can produce countless\nvariations of GAN models depending on the seed used. Existing methods for\nattributing deepfakes work well only if they have seen the specific GAN model\nduring training. If the GAN architectures are retrained with a different seed,\nthese methods struggle to attribute the fakes. This seed dependency issue made\nit difficult to attribute deepfakes with existing methods. We proposed a\ngeneralized deepfake attribution network (GDA-N et) to attribute fake images to\ntheir respective GAN architectures, even if they are generated from a retrained\nversion of the GAN architecture with a different seed (cross-seed) or from the\nfine-tuned version of the existing GAN model. Extensive experiments on\ncross-seed and fine-tuned data of GAN models show that our method is highly\neffective compared to existing methods. We have provided the source code to\nvalidate our results.\n", "link": "http://arxiv.org/abs/2406.18278v1", "date": "2024-06-26", "relevancy": 2.577, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5274}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5237}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4951}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalized%20Deepfake%20Attribution&body=Title%3A%20Generalized%20Deepfake%20Attribution%0AAuthor%3A%20Sowdagar%20Mahammad%20Shahid%20and%20Sudev%20Kumar%20Padhi%20and%20Umesh%20Kashyap%20and%20Sk.%20Subidh%20Ali%0AAbstract%3A%20%20%20The%20landscape%20of%20fake%20media%20creation%20changed%20with%20the%20introduction%20of%0AGenerative%20Adversarial%20Networks%20%28GAN%20s%29.%20Fake%20media%20creation%20has%20been%20on%20the%0Arise%20with%20the%20rapid%20advances%20in%20generation%20technology%2C%20leading%20to%20new%0Achallenges%20in%20Detecting%20fake%20media.%20A%20fundamental%20characteristic%20of%20GAN%20s%20is%0Atheir%20sensitivity%20to%20parameter%20initialization%2C%20known%20as%20seeds.%20Each%20distinct%0Aseed%20utilized%20during%20training%20leads%20to%20the%20creation%20of%20unique%20model%20instances%2C%0Aresulting%20in%20divergent%20image%20outputs%20despite%20employing%20the%20same%20architecture.%0AThis%20means%20that%20even%20if%20we%20have%20one%20GAN%20architecture%2C%20it%20can%20produce%20countless%0Avariations%20of%20GAN%20models%20depending%20on%20the%20seed%20used.%20Existing%20methods%20for%0Aattributing%20deepfakes%20work%20well%20only%20if%20they%20have%20seen%20the%20specific%20GAN%20model%0Aduring%20training.%20If%20the%20GAN%20architectures%20are%20retrained%20with%20a%20different%20seed%2C%0Athese%20methods%20struggle%20to%20attribute%20the%20fakes.%20This%20seed%20dependency%20issue%20made%0Ait%20difficult%20to%20attribute%20deepfakes%20with%20existing%20methods.%20We%20proposed%20a%0Ageneralized%20deepfake%20attribution%20network%20%28GDA-N%20et%29%20to%20attribute%20fake%20images%20to%0Atheir%20respective%20GAN%20architectures%2C%20even%20if%20they%20are%20generated%20from%20a%20retrained%0Aversion%20of%20the%20GAN%20architecture%20with%20a%20different%20seed%20%28cross-seed%29%20or%20from%20the%0Afine-tuned%20version%20of%20the%20existing%20GAN%20model.%20Extensive%20experiments%20on%0Across-seed%20and%20fine-tuned%20data%20of%20GAN%20models%20show%20that%20our%20method%20is%20highly%0Aeffective%20compared%20to%20existing%20methods.%20We%20have%20provided%20the%20source%20code%20to%0Avalidate%20our%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18278v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralized%2520Deepfake%2520Attribution%26entry.906535625%3DSowdagar%2520Mahammad%2520Shahid%2520and%2520Sudev%2520Kumar%2520Padhi%2520and%2520Umesh%2520Kashyap%2520and%2520Sk.%2520Subidh%2520Ali%26entry.1292438233%3D%2520%2520The%2520landscape%2520of%2520fake%2520media%2520creation%2520changed%2520with%2520the%2520introduction%2520of%250AGenerative%2520Adversarial%2520Networks%2520%2528GAN%2520s%2529.%2520Fake%2520media%2520creation%2520has%2520been%2520on%2520the%250Arise%2520with%2520the%2520rapid%2520advances%2520in%2520generation%2520technology%252C%2520leading%2520to%2520new%250Achallenges%2520in%2520Detecting%2520fake%2520media.%2520A%2520fundamental%2520characteristic%2520of%2520GAN%2520s%2520is%250Atheir%2520sensitivity%2520to%2520parameter%2520initialization%252C%2520known%2520as%2520seeds.%2520Each%2520distinct%250Aseed%2520utilized%2520during%2520training%2520leads%2520to%2520the%2520creation%2520of%2520unique%2520model%2520instances%252C%250Aresulting%2520in%2520divergent%2520image%2520outputs%2520despite%2520employing%2520the%2520same%2520architecture.%250AThis%2520means%2520that%2520even%2520if%2520we%2520have%2520one%2520GAN%2520architecture%252C%2520it%2520can%2520produce%2520countless%250Avariations%2520of%2520GAN%2520models%2520depending%2520on%2520the%2520seed%2520used.%2520Existing%2520methods%2520for%250Aattributing%2520deepfakes%2520work%2520well%2520only%2520if%2520they%2520have%2520seen%2520the%2520specific%2520GAN%2520model%250Aduring%2520training.%2520If%2520the%2520GAN%2520architectures%2520are%2520retrained%2520with%2520a%2520different%2520seed%252C%250Athese%2520methods%2520struggle%2520to%2520attribute%2520the%2520fakes.%2520This%2520seed%2520dependency%2520issue%2520made%250Ait%2520difficult%2520to%2520attribute%2520deepfakes%2520with%2520existing%2520methods.%2520We%2520proposed%2520a%250Ageneralized%2520deepfake%2520attribution%2520network%2520%2528GDA-N%2520et%2529%2520to%2520attribute%2520fake%2520images%2520to%250Atheir%2520respective%2520GAN%2520architectures%252C%2520even%2520if%2520they%2520are%2520generated%2520from%2520a%2520retrained%250Aversion%2520of%2520the%2520GAN%2520architecture%2520with%2520a%2520different%2520seed%2520%2528cross-seed%2529%2520or%2520from%2520the%250Afine-tuned%2520version%2520of%2520the%2520existing%2520GAN%2520model.%2520Extensive%2520experiments%2520on%250Across-seed%2520and%2520fine-tuned%2520data%2520of%2520GAN%2520models%2520show%2520that%2520our%2520method%2520is%2520highly%250Aeffective%2520compared%2520to%2520existing%2520methods.%2520We%2520have%2520provided%2520the%2520source%2520code%2520to%250Avalidate%2520our%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18278v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalized%20Deepfake%20Attribution&entry.906535625=Sowdagar%20Mahammad%20Shahid%20and%20Sudev%20Kumar%20Padhi%20and%20Umesh%20Kashyap%20and%20Sk.%20Subidh%20Ali&entry.1292438233=%20%20The%20landscape%20of%20fake%20media%20creation%20changed%20with%20the%20introduction%20of%0AGenerative%20Adversarial%20Networks%20%28GAN%20s%29.%20Fake%20media%20creation%20has%20been%20on%20the%0Arise%20with%20the%20rapid%20advances%20in%20generation%20technology%2C%20leading%20to%20new%0Achallenges%20in%20Detecting%20fake%20media.%20A%20fundamental%20characteristic%20of%20GAN%20s%20is%0Atheir%20sensitivity%20to%20parameter%20initialization%2C%20known%20as%20seeds.%20Each%20distinct%0Aseed%20utilized%20during%20training%20leads%20to%20the%20creation%20of%20unique%20model%20instances%2C%0Aresulting%20in%20divergent%20image%20outputs%20despite%20employing%20the%20same%20architecture.%0AThis%20means%20that%20even%20if%20we%20have%20one%20GAN%20architecture%2C%20it%20can%20produce%20countless%0Avariations%20of%20GAN%20models%20depending%20on%20the%20seed%20used.%20Existing%20methods%20for%0Aattributing%20deepfakes%20work%20well%20only%20if%20they%20have%20seen%20the%20specific%20GAN%20model%0Aduring%20training.%20If%20the%20GAN%20architectures%20are%20retrained%20with%20a%20different%20seed%2C%0Athese%20methods%20struggle%20to%20attribute%20the%20fakes.%20This%20seed%20dependency%20issue%20made%0Ait%20difficult%20to%20attribute%20deepfakes%20with%20existing%20methods.%20We%20proposed%20a%0Ageneralized%20deepfake%20attribution%20network%20%28GDA-N%20et%29%20to%20attribute%20fake%20images%20to%0Atheir%20respective%20GAN%20architectures%2C%20even%20if%20they%20are%20generated%20from%20a%20retrained%0Aversion%20of%20the%20GAN%20architecture%20with%20a%20different%20seed%20%28cross-seed%29%20or%20from%20the%0Afine-tuned%20version%20of%20the%20existing%20GAN%20model.%20Extensive%20experiments%20on%0Across-seed%20and%20fine-tuned%20data%20of%20GAN%20models%20show%20that%20our%20method%20is%20highly%0Aeffective%20compared%20to%20existing%20methods.%20We%20have%20provided%20the%20source%20code%20to%0Avalidate%20our%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18278v1&entry.124074799=Read"},
{"title": "General Distribution Learning: A theoretical framework for Deep Learning", "author": "Binchuan Qi and Li Li and Wei Gong", "abstract": "  There remain numerous unanswered research questions on deep learning (DL)\nwithin the classical learning theory framework. These include the remarkable\ngeneralization capabilities of overparametrized neural networks (NNs), the\nefficient optimization performance despite non-convexity of objectives, the\nmechanism of flat minima for generalization, and the exceptional performance of\ndeep architectures in solving physical problems. This paper introduces General\nDistribution Learning (GD Learning), a novel theoretical learning framework\ndesigned to address a comprehensive range of machine learning and statistical\ntasks, including classification, regression and parameter estimation. Departing\nfrom traditional statistical machine learning, GD Learning focuses on the true\nunderlying distribution. In GD Learning, learning error, corresponding to the\nexpected error in classical statistical learning framework, is divided into\nfitting errors due to models and algorithms, as well as sampling errors\nintroduced by limited sampling data. The framework significantly incorporates\nprior knowledge, especially in scenarios characterized by data scarcity,\nthereby enhancing performance. Within the GD Learning framework, we demonstrate\nthat the global optimal solutions in non-convex optimization can be approached\nby minimizing the gradient norm and the non-uniformity of the eigenvalues of\nthe model's Jacobian matrix. This insight leads to the development of the\ngradient structure control algorithm. GD Learning also offers fresh insights\ninto the questions on deep learning, including overparameterization and\nnon-convex optimization, bias-variance trade-off, and the mechanism of flat\nminima.\n", "link": "http://arxiv.org/abs/2406.05666v4", "date": "2024-06-26", "relevancy": 2.5554, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5237}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5102}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20General%20Distribution%20Learning%3A%20A%20theoretical%20framework%20for%20Deep%20Learning&body=Title%3A%20General%20Distribution%20Learning%3A%20A%20theoretical%20framework%20for%20Deep%20Learning%0AAuthor%3A%20Binchuan%20Qi%20and%20Li%20Li%20and%20Wei%20Gong%0AAbstract%3A%20%20%20There%20remain%20numerous%20unanswered%20research%20questions%20on%20deep%20learning%20%28DL%29%0Awithin%20the%20classical%20learning%20theory%20framework.%20These%20include%20the%20remarkable%0Ageneralization%20capabilities%20of%20overparametrized%20neural%20networks%20%28NNs%29%2C%20the%0Aefficient%20optimization%20performance%20despite%20non-convexity%20of%20objectives%2C%20the%0Amechanism%20of%20flat%20minima%20for%20generalization%2C%20and%20the%20exceptional%20performance%20of%0Adeep%20architectures%20in%20solving%20physical%20problems.%20This%20paper%20introduces%20General%0ADistribution%20Learning%20%28GD%20Learning%29%2C%20a%20novel%20theoretical%20learning%20framework%0Adesigned%20to%20address%20a%20comprehensive%20range%20of%20machine%20learning%20and%20statistical%0Atasks%2C%20including%20classification%2C%20regression%20and%20parameter%20estimation.%20Departing%0Afrom%20traditional%20statistical%20machine%20learning%2C%20GD%20Learning%20focuses%20on%20the%20true%0Aunderlying%20distribution.%20In%20GD%20Learning%2C%20learning%20error%2C%20corresponding%20to%20the%0Aexpected%20error%20in%20classical%20statistical%20learning%20framework%2C%20is%20divided%20into%0Afitting%20errors%20due%20to%20models%20and%20algorithms%2C%20as%20well%20as%20sampling%20errors%0Aintroduced%20by%20limited%20sampling%20data.%20The%20framework%20significantly%20incorporates%0Aprior%20knowledge%2C%20especially%20in%20scenarios%20characterized%20by%20data%20scarcity%2C%0Athereby%20enhancing%20performance.%20Within%20the%20GD%20Learning%20framework%2C%20we%20demonstrate%0Athat%20the%20global%20optimal%20solutions%20in%20non-convex%20optimization%20can%20be%20approached%0Aby%20minimizing%20the%20gradient%20norm%20and%20the%20non-uniformity%20of%20the%20eigenvalues%20of%0Athe%20model%27s%20Jacobian%20matrix.%20This%20insight%20leads%20to%20the%20development%20of%20the%0Agradient%20structure%20control%20algorithm.%20GD%20Learning%20also%20offers%20fresh%20insights%0Ainto%20the%20questions%20on%20deep%20learning%2C%20including%20overparameterization%20and%0Anon-convex%20optimization%2C%20bias-variance%20trade-off%2C%20and%20the%20mechanism%20of%20flat%0Aminima.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05666v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneral%2520Distribution%2520Learning%253A%2520A%2520theoretical%2520framework%2520for%2520Deep%2520Learning%26entry.906535625%3DBinchuan%2520Qi%2520and%2520Li%2520Li%2520and%2520Wei%2520Gong%26entry.1292438233%3D%2520%2520There%2520remain%2520numerous%2520unanswered%2520research%2520questions%2520on%2520deep%2520learning%2520%2528DL%2529%250Awithin%2520the%2520classical%2520learning%2520theory%2520framework.%2520These%2520include%2520the%2520remarkable%250Ageneralization%2520capabilities%2520of%2520overparametrized%2520neural%2520networks%2520%2528NNs%2529%252C%2520the%250Aefficient%2520optimization%2520performance%2520despite%2520non-convexity%2520of%2520objectives%252C%2520the%250Amechanism%2520of%2520flat%2520minima%2520for%2520generalization%252C%2520and%2520the%2520exceptional%2520performance%2520of%250Adeep%2520architectures%2520in%2520solving%2520physical%2520problems.%2520This%2520paper%2520introduces%2520General%250ADistribution%2520Learning%2520%2528GD%2520Learning%2529%252C%2520a%2520novel%2520theoretical%2520learning%2520framework%250Adesigned%2520to%2520address%2520a%2520comprehensive%2520range%2520of%2520machine%2520learning%2520and%2520statistical%250Atasks%252C%2520including%2520classification%252C%2520regression%2520and%2520parameter%2520estimation.%2520Departing%250Afrom%2520traditional%2520statistical%2520machine%2520learning%252C%2520GD%2520Learning%2520focuses%2520on%2520the%2520true%250Aunderlying%2520distribution.%2520In%2520GD%2520Learning%252C%2520learning%2520error%252C%2520corresponding%2520to%2520the%250Aexpected%2520error%2520in%2520classical%2520statistical%2520learning%2520framework%252C%2520is%2520divided%2520into%250Afitting%2520errors%2520due%2520to%2520models%2520and%2520algorithms%252C%2520as%2520well%2520as%2520sampling%2520errors%250Aintroduced%2520by%2520limited%2520sampling%2520data.%2520The%2520framework%2520significantly%2520incorporates%250Aprior%2520knowledge%252C%2520especially%2520in%2520scenarios%2520characterized%2520by%2520data%2520scarcity%252C%250Athereby%2520enhancing%2520performance.%2520Within%2520the%2520GD%2520Learning%2520framework%252C%2520we%2520demonstrate%250Athat%2520the%2520global%2520optimal%2520solutions%2520in%2520non-convex%2520optimization%2520can%2520be%2520approached%250Aby%2520minimizing%2520the%2520gradient%2520norm%2520and%2520the%2520non-uniformity%2520of%2520the%2520eigenvalues%2520of%250Athe%2520model%2527s%2520Jacobian%2520matrix.%2520This%2520insight%2520leads%2520to%2520the%2520development%2520of%2520the%250Agradient%2520structure%2520control%2520algorithm.%2520GD%2520Learning%2520also%2520offers%2520fresh%2520insights%250Ainto%2520the%2520questions%2520on%2520deep%2520learning%252C%2520including%2520overparameterization%2520and%250Anon-convex%2520optimization%252C%2520bias-variance%2520trade-off%252C%2520and%2520the%2520mechanism%2520of%2520flat%250Aminima.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05666v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=General%20Distribution%20Learning%3A%20A%20theoretical%20framework%20for%20Deep%20Learning&entry.906535625=Binchuan%20Qi%20and%20Li%20Li%20and%20Wei%20Gong&entry.1292438233=%20%20There%20remain%20numerous%20unanswered%20research%20questions%20on%20deep%20learning%20%28DL%29%0Awithin%20the%20classical%20learning%20theory%20framework.%20These%20include%20the%20remarkable%0Ageneralization%20capabilities%20of%20overparametrized%20neural%20networks%20%28NNs%29%2C%20the%0Aefficient%20optimization%20performance%20despite%20non-convexity%20of%20objectives%2C%20the%0Amechanism%20of%20flat%20minima%20for%20generalization%2C%20and%20the%20exceptional%20performance%20of%0Adeep%20architectures%20in%20solving%20physical%20problems.%20This%20paper%20introduces%20General%0ADistribution%20Learning%20%28GD%20Learning%29%2C%20a%20novel%20theoretical%20learning%20framework%0Adesigned%20to%20address%20a%20comprehensive%20range%20of%20machine%20learning%20and%20statistical%0Atasks%2C%20including%20classification%2C%20regression%20and%20parameter%20estimation.%20Departing%0Afrom%20traditional%20statistical%20machine%20learning%2C%20GD%20Learning%20focuses%20on%20the%20true%0Aunderlying%20distribution.%20In%20GD%20Learning%2C%20learning%20error%2C%20corresponding%20to%20the%0Aexpected%20error%20in%20classical%20statistical%20learning%20framework%2C%20is%20divided%20into%0Afitting%20errors%20due%20to%20models%20and%20algorithms%2C%20as%20well%20as%20sampling%20errors%0Aintroduced%20by%20limited%20sampling%20data.%20The%20framework%20significantly%20incorporates%0Aprior%20knowledge%2C%20especially%20in%20scenarios%20characterized%20by%20data%20scarcity%2C%0Athereby%20enhancing%20performance.%20Within%20the%20GD%20Learning%20framework%2C%20we%20demonstrate%0Athat%20the%20global%20optimal%20solutions%20in%20non-convex%20optimization%20can%20be%20approached%0Aby%20minimizing%20the%20gradient%20norm%20and%20the%20non-uniformity%20of%20the%20eigenvalues%20of%0Athe%20model%27s%20Jacobian%20matrix.%20This%20insight%20leads%20to%20the%20development%20of%20the%0Agradient%20structure%20control%20algorithm.%20GD%20Learning%20also%20offers%20fresh%20insights%0Ainto%20the%20questions%20on%20deep%20learning%2C%20including%20overparameterization%20and%0Anon-convex%20optimization%2C%20bias-variance%20trade-off%2C%20and%20the%20mechanism%20of%20flat%0Aminima.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05666v4&entry.124074799=Read"},
{"title": "Weisfeiler Leman for Euclidean Equivariant Machine Learning", "author": "Snir Hordan and Tal Amir and Nadav Dym", "abstract": "  The $k$-Weisfeiler-Leman ($k$-WL) graph isomorphism test hierarchy is a\ncommon method for assessing the expressive power of graph neural networks\n(GNNs). Recently, GNNs whose expressive power is equivalent to the $2$-WL test\nwere proven to be universal on weighted graphs which encode $3\\mathrm{D}$ point\ncloud data, yet this result is limited to invariant continuous functions on\npoint clouds. In this paper, we extend this result in three ways: Firstly, we\nshow that PPGN can simulate $2$-WL uniformly on all point clouds with low\ncomplexity. Secondly, we show that $2$-WL tests can be extended to point clouds\nwhich include both positions and velocities, a scenario often encountered in\napplications. Finally, we provide a general framework for proving equivariant\nuniversality and leverage it to prove that a simple modification of this\ninvariant PPGN architecture can be used to obtain a universal equivariant\narchitecture that can approximate all continuous equivariant functions\nuniformly. Building on our results, we develop our WeLNet architecture, which\nsets new state-of-the-art results on the N-Body dynamics task and the GEOM-QM9\nmolecular conformation generation task.\n", "link": "http://arxiv.org/abs/2402.02484v3", "date": "2024-06-26", "relevancy": 2.5484, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5256}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5095}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weisfeiler%20Leman%20for%20Euclidean%20Equivariant%20Machine%20Learning&body=Title%3A%20Weisfeiler%20Leman%20for%20Euclidean%20Equivariant%20Machine%20Learning%0AAuthor%3A%20Snir%20Hordan%20and%20Tal%20Amir%20and%20Nadav%20Dym%0AAbstract%3A%20%20%20The%20%24k%24-Weisfeiler-Leman%20%28%24k%24-WL%29%20graph%20isomorphism%20test%20hierarchy%20is%20a%0Acommon%20method%20for%20assessing%20the%20expressive%20power%20of%20graph%20neural%20networks%0A%28GNNs%29.%20Recently%2C%20GNNs%20whose%20expressive%20power%20is%20equivalent%20to%20the%20%242%24-WL%20test%0Awere%20proven%20to%20be%20universal%20on%20weighted%20graphs%20which%20encode%20%243%5Cmathrm%7BD%7D%24%20point%0Acloud%20data%2C%20yet%20this%20result%20is%20limited%20to%20invariant%20continuous%20functions%20on%0Apoint%20clouds.%20In%20this%20paper%2C%20we%20extend%20this%20result%20in%20three%20ways%3A%20Firstly%2C%20we%0Ashow%20that%20PPGN%20can%20simulate%20%242%24-WL%20uniformly%20on%20all%20point%20clouds%20with%20low%0Acomplexity.%20Secondly%2C%20we%20show%20that%20%242%24-WL%20tests%20can%20be%20extended%20to%20point%20clouds%0Awhich%20include%20both%20positions%20and%20velocities%2C%20a%20scenario%20often%20encountered%20in%0Aapplications.%20Finally%2C%20we%20provide%20a%20general%20framework%20for%20proving%20equivariant%0Auniversality%20and%20leverage%20it%20to%20prove%20that%20a%20simple%20modification%20of%20this%0Ainvariant%20PPGN%20architecture%20can%20be%20used%20to%20obtain%20a%20universal%20equivariant%0Aarchitecture%20that%20can%20approximate%20all%20continuous%20equivariant%20functions%0Auniformly.%20Building%20on%20our%20results%2C%20we%20develop%20our%20WeLNet%20architecture%2C%20which%0Asets%20new%20state-of-the-art%20results%20on%20the%20N-Body%20dynamics%20task%20and%20the%20GEOM-QM9%0Amolecular%20conformation%20generation%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.02484v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeisfeiler%2520Leman%2520for%2520Euclidean%2520Equivariant%2520Machine%2520Learning%26entry.906535625%3DSnir%2520Hordan%2520and%2520Tal%2520Amir%2520and%2520Nadav%2520Dym%26entry.1292438233%3D%2520%2520The%2520%2524k%2524-Weisfeiler-Leman%2520%2528%2524k%2524-WL%2529%2520graph%2520isomorphism%2520test%2520hierarchy%2520is%2520a%250Acommon%2520method%2520for%2520assessing%2520the%2520expressive%2520power%2520of%2520graph%2520neural%2520networks%250A%2528GNNs%2529.%2520Recently%252C%2520GNNs%2520whose%2520expressive%2520power%2520is%2520equivalent%2520to%2520the%2520%25242%2524-WL%2520test%250Awere%2520proven%2520to%2520be%2520universal%2520on%2520weighted%2520graphs%2520which%2520encode%2520%25243%255Cmathrm%257BD%257D%2524%2520point%250Acloud%2520data%252C%2520yet%2520this%2520result%2520is%2520limited%2520to%2520invariant%2520continuous%2520functions%2520on%250Apoint%2520clouds.%2520In%2520this%2520paper%252C%2520we%2520extend%2520this%2520result%2520in%2520three%2520ways%253A%2520Firstly%252C%2520we%250Ashow%2520that%2520PPGN%2520can%2520simulate%2520%25242%2524-WL%2520uniformly%2520on%2520all%2520point%2520clouds%2520with%2520low%250Acomplexity.%2520Secondly%252C%2520we%2520show%2520that%2520%25242%2524-WL%2520tests%2520can%2520be%2520extended%2520to%2520point%2520clouds%250Awhich%2520include%2520both%2520positions%2520and%2520velocities%252C%2520a%2520scenario%2520often%2520encountered%2520in%250Aapplications.%2520Finally%252C%2520we%2520provide%2520a%2520general%2520framework%2520for%2520proving%2520equivariant%250Auniversality%2520and%2520leverage%2520it%2520to%2520prove%2520that%2520a%2520simple%2520modification%2520of%2520this%250Ainvariant%2520PPGN%2520architecture%2520can%2520be%2520used%2520to%2520obtain%2520a%2520universal%2520equivariant%250Aarchitecture%2520that%2520can%2520approximate%2520all%2520continuous%2520equivariant%2520functions%250Auniformly.%2520Building%2520on%2520our%2520results%252C%2520we%2520develop%2520our%2520WeLNet%2520architecture%252C%2520which%250Asets%2520new%2520state-of-the-art%2520results%2520on%2520the%2520N-Body%2520dynamics%2520task%2520and%2520the%2520GEOM-QM9%250Amolecular%2520conformation%2520generation%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.02484v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weisfeiler%20Leman%20for%20Euclidean%20Equivariant%20Machine%20Learning&entry.906535625=Snir%20Hordan%20and%20Tal%20Amir%20and%20Nadav%20Dym&entry.1292438233=%20%20The%20%24k%24-Weisfeiler-Leman%20%28%24k%24-WL%29%20graph%20isomorphism%20test%20hierarchy%20is%20a%0Acommon%20method%20for%20assessing%20the%20expressive%20power%20of%20graph%20neural%20networks%0A%28GNNs%29.%20Recently%2C%20GNNs%20whose%20expressive%20power%20is%20equivalent%20to%20the%20%242%24-WL%20test%0Awere%20proven%20to%20be%20universal%20on%20weighted%20graphs%20which%20encode%20%243%5Cmathrm%7BD%7D%24%20point%0Acloud%20data%2C%20yet%20this%20result%20is%20limited%20to%20invariant%20continuous%20functions%20on%0Apoint%20clouds.%20In%20this%20paper%2C%20we%20extend%20this%20result%20in%20three%20ways%3A%20Firstly%2C%20we%0Ashow%20that%20PPGN%20can%20simulate%20%242%24-WL%20uniformly%20on%20all%20point%20clouds%20with%20low%0Acomplexity.%20Secondly%2C%20we%20show%20that%20%242%24-WL%20tests%20can%20be%20extended%20to%20point%20clouds%0Awhich%20include%20both%20positions%20and%20velocities%2C%20a%20scenario%20often%20encountered%20in%0Aapplications.%20Finally%2C%20we%20provide%20a%20general%20framework%20for%20proving%20equivariant%0Auniversality%20and%20leverage%20it%20to%20prove%20that%20a%20simple%20modification%20of%20this%0Ainvariant%20PPGN%20architecture%20can%20be%20used%20to%20obtain%20a%20universal%20equivariant%0Aarchitecture%20that%20can%20approximate%20all%20continuous%20equivariant%20functions%0Auniformly.%20Building%20on%20our%20results%2C%20we%20develop%20our%20WeLNet%20architecture%2C%20which%0Asets%20new%20state-of-the-art%20results%20on%20the%20N-Body%20dynamics%20task%20and%20the%20GEOM-QM9%0Amolecular%20conformation%20generation%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.02484v3&entry.124074799=Read"},
{"title": "On the Role of Visual Grounding in VQA", "author": "Daniel Reich and Tanja Schultz", "abstract": "  Visual Grounding (VG) in VQA refers to a model's proclivity to infer answers\nbased on question-relevant image regions. Conceptually, VG identifies as an\naxiomatic requirement of the VQA task. In practice, however, DNN-based VQA\nmodels are notorious for bypassing VG by way of shortcut (SC) learning without\nsuffering obvious performance losses in standard benchmarks. To uncover the\nimpact of SC learning, Out-of-Distribution (OOD) tests have been proposed that\nexpose a lack of VG with low accuracy. These tests have since been at the\ncenter of VG research and served as basis for various investigations into VG's\nimpact on accuracy. However, the role of VG in VQA still remains not fully\nunderstood and has not yet been properly formalized.\n  In this work, we seek to clarify VG's role in VQA by formalizing it on a\nconceptual level. We propose a novel theoretical framework called \"Visually\nGrounded Reasoning\" (VGR) that uses the concepts of VG and Reasoning to\ndescribe VQA inference in ideal OOD testing. By consolidating fundamental\ninsights into VG's role in VQA, VGR helps to reveal rampant VG-related SC\nexploitation in OOD testing, which explains why the relationship between VG and\nOOD accuracy has been difficult to define. Finally, we propose an approach to\ncreate OOD tests that properly emphasize a requirement for VG, and show how to\nimprove performance on them.\n", "link": "http://arxiv.org/abs/2406.18253v1", "date": "2024-06-26", "relevancy": 2.5385, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5272}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5023}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Role%20of%20Visual%20Grounding%20in%20VQA&body=Title%3A%20On%20the%20Role%20of%20Visual%20Grounding%20in%20VQA%0AAuthor%3A%20Daniel%20Reich%20and%20Tanja%20Schultz%0AAbstract%3A%20%20%20Visual%20Grounding%20%28VG%29%20in%20VQA%20refers%20to%20a%20model%27s%20proclivity%20to%20infer%20answers%0Abased%20on%20question-relevant%20image%20regions.%20Conceptually%2C%20VG%20identifies%20as%20an%0Aaxiomatic%20requirement%20of%20the%20VQA%20task.%20In%20practice%2C%20however%2C%20DNN-based%20VQA%0Amodels%20are%20notorious%20for%20bypassing%20VG%20by%20way%20of%20shortcut%20%28SC%29%20learning%20without%0Asuffering%20obvious%20performance%20losses%20in%20standard%20benchmarks.%20To%20uncover%20the%0Aimpact%20of%20SC%20learning%2C%20Out-of-Distribution%20%28OOD%29%20tests%20have%20been%20proposed%20that%0Aexpose%20a%20lack%20of%20VG%20with%20low%20accuracy.%20These%20tests%20have%20since%20been%20at%20the%0Acenter%20of%20VG%20research%20and%20served%20as%20basis%20for%20various%20investigations%20into%20VG%27s%0Aimpact%20on%20accuracy.%20However%2C%20the%20role%20of%20VG%20in%20VQA%20still%20remains%20not%20fully%0Aunderstood%20and%20has%20not%20yet%20been%20properly%20formalized.%0A%20%20In%20this%20work%2C%20we%20seek%20to%20clarify%20VG%27s%20role%20in%20VQA%20by%20formalizing%20it%20on%20a%0Aconceptual%20level.%20We%20propose%20a%20novel%20theoretical%20framework%20called%20%22Visually%0AGrounded%20Reasoning%22%20%28VGR%29%20that%20uses%20the%20concepts%20of%20VG%20and%20Reasoning%20to%0Adescribe%20VQA%20inference%20in%20ideal%20OOD%20testing.%20By%20consolidating%20fundamental%0Ainsights%20into%20VG%27s%20role%20in%20VQA%2C%20VGR%20helps%20to%20reveal%20rampant%20VG-related%20SC%0Aexploitation%20in%20OOD%20testing%2C%20which%20explains%20why%20the%20relationship%20between%20VG%20and%0AOOD%20accuracy%20has%20been%20difficult%20to%20define.%20Finally%2C%20we%20propose%20an%20approach%20to%0Acreate%20OOD%20tests%20that%20properly%20emphasize%20a%20requirement%20for%20VG%2C%20and%20show%20how%20to%0Aimprove%20performance%20on%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18253v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Role%2520of%2520Visual%2520Grounding%2520in%2520VQA%26entry.906535625%3DDaniel%2520Reich%2520and%2520Tanja%2520Schultz%26entry.1292438233%3D%2520%2520Visual%2520Grounding%2520%2528VG%2529%2520in%2520VQA%2520refers%2520to%2520a%2520model%2527s%2520proclivity%2520to%2520infer%2520answers%250Abased%2520on%2520question-relevant%2520image%2520regions.%2520Conceptually%252C%2520VG%2520identifies%2520as%2520an%250Aaxiomatic%2520requirement%2520of%2520the%2520VQA%2520task.%2520In%2520practice%252C%2520however%252C%2520DNN-based%2520VQA%250Amodels%2520are%2520notorious%2520for%2520bypassing%2520VG%2520by%2520way%2520of%2520shortcut%2520%2528SC%2529%2520learning%2520without%250Asuffering%2520obvious%2520performance%2520losses%2520in%2520standard%2520benchmarks.%2520To%2520uncover%2520the%250Aimpact%2520of%2520SC%2520learning%252C%2520Out-of-Distribution%2520%2528OOD%2529%2520tests%2520have%2520been%2520proposed%2520that%250Aexpose%2520a%2520lack%2520of%2520VG%2520with%2520low%2520accuracy.%2520These%2520tests%2520have%2520since%2520been%2520at%2520the%250Acenter%2520of%2520VG%2520research%2520and%2520served%2520as%2520basis%2520for%2520various%2520investigations%2520into%2520VG%2527s%250Aimpact%2520on%2520accuracy.%2520However%252C%2520the%2520role%2520of%2520VG%2520in%2520VQA%2520still%2520remains%2520not%2520fully%250Aunderstood%2520and%2520has%2520not%2520yet%2520been%2520properly%2520formalized.%250A%2520%2520In%2520this%2520work%252C%2520we%2520seek%2520to%2520clarify%2520VG%2527s%2520role%2520in%2520VQA%2520by%2520formalizing%2520it%2520on%2520a%250Aconceptual%2520level.%2520We%2520propose%2520a%2520novel%2520theoretical%2520framework%2520called%2520%2522Visually%250AGrounded%2520Reasoning%2522%2520%2528VGR%2529%2520that%2520uses%2520the%2520concepts%2520of%2520VG%2520and%2520Reasoning%2520to%250Adescribe%2520VQA%2520inference%2520in%2520ideal%2520OOD%2520testing.%2520By%2520consolidating%2520fundamental%250Ainsights%2520into%2520VG%2527s%2520role%2520in%2520VQA%252C%2520VGR%2520helps%2520to%2520reveal%2520rampant%2520VG-related%2520SC%250Aexploitation%2520in%2520OOD%2520testing%252C%2520which%2520explains%2520why%2520the%2520relationship%2520between%2520VG%2520and%250AOOD%2520accuracy%2520has%2520been%2520difficult%2520to%2520define.%2520Finally%252C%2520we%2520propose%2520an%2520approach%2520to%250Acreate%2520OOD%2520tests%2520that%2520properly%2520emphasize%2520a%2520requirement%2520for%2520VG%252C%2520and%2520show%2520how%2520to%250Aimprove%2520performance%2520on%2520them.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18253v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Role%20of%20Visual%20Grounding%20in%20VQA&entry.906535625=Daniel%20Reich%20and%20Tanja%20Schultz&entry.1292438233=%20%20Visual%20Grounding%20%28VG%29%20in%20VQA%20refers%20to%20a%20model%27s%20proclivity%20to%20infer%20answers%0Abased%20on%20question-relevant%20image%20regions.%20Conceptually%2C%20VG%20identifies%20as%20an%0Aaxiomatic%20requirement%20of%20the%20VQA%20task.%20In%20practice%2C%20however%2C%20DNN-based%20VQA%0Amodels%20are%20notorious%20for%20bypassing%20VG%20by%20way%20of%20shortcut%20%28SC%29%20learning%20without%0Asuffering%20obvious%20performance%20losses%20in%20standard%20benchmarks.%20To%20uncover%20the%0Aimpact%20of%20SC%20learning%2C%20Out-of-Distribution%20%28OOD%29%20tests%20have%20been%20proposed%20that%0Aexpose%20a%20lack%20of%20VG%20with%20low%20accuracy.%20These%20tests%20have%20since%20been%20at%20the%0Acenter%20of%20VG%20research%20and%20served%20as%20basis%20for%20various%20investigations%20into%20VG%27s%0Aimpact%20on%20accuracy.%20However%2C%20the%20role%20of%20VG%20in%20VQA%20still%20remains%20not%20fully%0Aunderstood%20and%20has%20not%20yet%20been%20properly%20formalized.%0A%20%20In%20this%20work%2C%20we%20seek%20to%20clarify%20VG%27s%20role%20in%20VQA%20by%20formalizing%20it%20on%20a%0Aconceptual%20level.%20We%20propose%20a%20novel%20theoretical%20framework%20called%20%22Visually%0AGrounded%20Reasoning%22%20%28VGR%29%20that%20uses%20the%20concepts%20of%20VG%20and%20Reasoning%20to%0Adescribe%20VQA%20inference%20in%20ideal%20OOD%20testing.%20By%20consolidating%20fundamental%0Ainsights%20into%20VG%27s%20role%20in%20VQA%2C%20VGR%20helps%20to%20reveal%20rampant%20VG-related%20SC%0Aexploitation%20in%20OOD%20testing%2C%20which%20explains%20why%20the%20relationship%20between%20VG%20and%0AOOD%20accuracy%20has%20been%20difficult%20to%20define.%20Finally%2C%20we%20propose%20an%20approach%20to%0Acreate%20OOD%20tests%20that%20properly%20emphasize%20a%20requirement%20for%20VG%2C%20and%20show%20how%20to%0Aimprove%20performance%20on%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18253v1&entry.124074799=Read"},
{"title": "ES-GNN: Generalizing Graph Neural Networks Beyond Homophily with Edge\n  Splitting", "author": "Jingwei Guo and Kaizhu Huang and Rui Zhang and Xinping Yi", "abstract": "  While Graph Neural Networks (GNNs) have achieved enormous success in multiple\ngraph analytical tasks, modern variants mostly rely on the strong inductive\nbias of homophily. However, real-world networks typically exhibit both\nhomophilic and heterophilic linking patterns, wherein adjacent nodes may share\ndissimilar attributes and distinct labels. Therefore, GNNs smoothing node\nproximity holistically may aggregate both task-relevant and irrelevant (even\nharmful) information, limiting their ability to generalize to heterophilic\ngraphs and potentially causing non-robustness. In this work, we propose a novel\nEdge Splitting GNN (ES-GNN) framework to adaptively distinguish between graph\nedges either relevant or irrelevant to learning tasks. This essentially\ntransfers the original graph into two subgraphs with the same node set but\ncomplementary edge sets dynamically. Given that, information propagation\nseparately on these subgraphs and edge splitting are alternatively conducted,\nthus disentangling the task-relevant and irrelevant features. Theoretically, we\nshow that our ES-GNN can be regarded as a solution to a disentangled graph\ndenoising problem, which further illustrates our motivations and interprets the\nimproved generalization beyond homophily. Extensive experiments over 11\nbenchmark and 1 synthetic datasets not only demonstrate the effective\nperformance of ES-GNN but also highlight its robustness to adversarial graphs\nand mitigation of the over-smoothing problem.\n", "link": "http://arxiv.org/abs/2205.13700v4", "date": "2024-06-26", "relevancy": 2.4925, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5444}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4788}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4723}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ES-GNN%3A%20Generalizing%20Graph%20Neural%20Networks%20Beyond%20Homophily%20with%20Edge%0A%20%20Splitting&body=Title%3A%20ES-GNN%3A%20Generalizing%20Graph%20Neural%20Networks%20Beyond%20Homophily%20with%20Edge%0A%20%20Splitting%0AAuthor%3A%20Jingwei%20Guo%20and%20Kaizhu%20Huang%20and%20Rui%20Zhang%20and%20Xinping%20Yi%0AAbstract%3A%20%20%20While%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20achieved%20enormous%20success%20in%20multiple%0Agraph%20analytical%20tasks%2C%20modern%20variants%20mostly%20rely%20on%20the%20strong%20inductive%0Abias%20of%20homophily.%20However%2C%20real-world%20networks%20typically%20exhibit%20both%0Ahomophilic%20and%20heterophilic%20linking%20patterns%2C%20wherein%20adjacent%20nodes%20may%20share%0Adissimilar%20attributes%20and%20distinct%20labels.%20Therefore%2C%20GNNs%20smoothing%20node%0Aproximity%20holistically%20may%20aggregate%20both%20task-relevant%20and%20irrelevant%20%28even%0Aharmful%29%20information%2C%20limiting%20their%20ability%20to%20generalize%20to%20heterophilic%0Agraphs%20and%20potentially%20causing%20non-robustness.%20In%20this%20work%2C%20we%20propose%20a%20novel%0AEdge%20Splitting%20GNN%20%28ES-GNN%29%20framework%20to%20adaptively%20distinguish%20between%20graph%0Aedges%20either%20relevant%20or%20irrelevant%20to%20learning%20tasks.%20This%20essentially%0Atransfers%20the%20original%20graph%20into%20two%20subgraphs%20with%20the%20same%20node%20set%20but%0Acomplementary%20edge%20sets%20dynamically.%20Given%20that%2C%20information%20propagation%0Aseparately%20on%20these%20subgraphs%20and%20edge%20splitting%20are%20alternatively%20conducted%2C%0Athus%20disentangling%20the%20task-relevant%20and%20irrelevant%20features.%20Theoretically%2C%20we%0Ashow%20that%20our%20ES-GNN%20can%20be%20regarded%20as%20a%20solution%20to%20a%20disentangled%20graph%0Adenoising%20problem%2C%20which%20further%20illustrates%20our%20motivations%20and%20interprets%20the%0Aimproved%20generalization%20beyond%20homophily.%20Extensive%20experiments%20over%2011%0Abenchmark%20and%201%20synthetic%20datasets%20not%20only%20demonstrate%20the%20effective%0Aperformance%20of%20ES-GNN%20but%20also%20highlight%20its%20robustness%20to%20adversarial%20graphs%0Aand%20mitigation%20of%20the%20over-smoothing%20problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2205.13700v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DES-GNN%253A%2520Generalizing%2520Graph%2520Neural%2520Networks%2520Beyond%2520Homophily%2520with%2520Edge%250A%2520%2520Splitting%26entry.906535625%3DJingwei%2520Guo%2520and%2520Kaizhu%2520Huang%2520and%2520Rui%2520Zhang%2520and%2520Xinping%2520Yi%26entry.1292438233%3D%2520%2520While%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520achieved%2520enormous%2520success%2520in%2520multiple%250Agraph%2520analytical%2520tasks%252C%2520modern%2520variants%2520mostly%2520rely%2520on%2520the%2520strong%2520inductive%250Abias%2520of%2520homophily.%2520However%252C%2520real-world%2520networks%2520typically%2520exhibit%2520both%250Ahomophilic%2520and%2520heterophilic%2520linking%2520patterns%252C%2520wherein%2520adjacent%2520nodes%2520may%2520share%250Adissimilar%2520attributes%2520and%2520distinct%2520labels.%2520Therefore%252C%2520GNNs%2520smoothing%2520node%250Aproximity%2520holistically%2520may%2520aggregate%2520both%2520task-relevant%2520and%2520irrelevant%2520%2528even%250Aharmful%2529%2520information%252C%2520limiting%2520their%2520ability%2520to%2520generalize%2520to%2520heterophilic%250Agraphs%2520and%2520potentially%2520causing%2520non-robustness.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%250AEdge%2520Splitting%2520GNN%2520%2528ES-GNN%2529%2520framework%2520to%2520adaptively%2520distinguish%2520between%2520graph%250Aedges%2520either%2520relevant%2520or%2520irrelevant%2520to%2520learning%2520tasks.%2520This%2520essentially%250Atransfers%2520the%2520original%2520graph%2520into%2520two%2520subgraphs%2520with%2520the%2520same%2520node%2520set%2520but%250Acomplementary%2520edge%2520sets%2520dynamically.%2520Given%2520that%252C%2520information%2520propagation%250Aseparately%2520on%2520these%2520subgraphs%2520and%2520edge%2520splitting%2520are%2520alternatively%2520conducted%252C%250Athus%2520disentangling%2520the%2520task-relevant%2520and%2520irrelevant%2520features.%2520Theoretically%252C%2520we%250Ashow%2520that%2520our%2520ES-GNN%2520can%2520be%2520regarded%2520as%2520a%2520solution%2520to%2520a%2520disentangled%2520graph%250Adenoising%2520problem%252C%2520which%2520further%2520illustrates%2520our%2520motivations%2520and%2520interprets%2520the%250Aimproved%2520generalization%2520beyond%2520homophily.%2520Extensive%2520experiments%2520over%252011%250Abenchmark%2520and%25201%2520synthetic%2520datasets%2520not%2520only%2520demonstrate%2520the%2520effective%250Aperformance%2520of%2520ES-GNN%2520but%2520also%2520highlight%2520its%2520robustness%2520to%2520adversarial%2520graphs%250Aand%2520mitigation%2520of%2520the%2520over-smoothing%2520problem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2205.13700v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ES-GNN%3A%20Generalizing%20Graph%20Neural%20Networks%20Beyond%20Homophily%20with%20Edge%0A%20%20Splitting&entry.906535625=Jingwei%20Guo%20and%20Kaizhu%20Huang%20and%20Rui%20Zhang%20and%20Xinping%20Yi&entry.1292438233=%20%20While%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20achieved%20enormous%20success%20in%20multiple%0Agraph%20analytical%20tasks%2C%20modern%20variants%20mostly%20rely%20on%20the%20strong%20inductive%0Abias%20of%20homophily.%20However%2C%20real-world%20networks%20typically%20exhibit%20both%0Ahomophilic%20and%20heterophilic%20linking%20patterns%2C%20wherein%20adjacent%20nodes%20may%20share%0Adissimilar%20attributes%20and%20distinct%20labels.%20Therefore%2C%20GNNs%20smoothing%20node%0Aproximity%20holistically%20may%20aggregate%20both%20task-relevant%20and%20irrelevant%20%28even%0Aharmful%29%20information%2C%20limiting%20their%20ability%20to%20generalize%20to%20heterophilic%0Agraphs%20and%20potentially%20causing%20non-robustness.%20In%20this%20work%2C%20we%20propose%20a%20novel%0AEdge%20Splitting%20GNN%20%28ES-GNN%29%20framework%20to%20adaptively%20distinguish%20between%20graph%0Aedges%20either%20relevant%20or%20irrelevant%20to%20learning%20tasks.%20This%20essentially%0Atransfers%20the%20original%20graph%20into%20two%20subgraphs%20with%20the%20same%20node%20set%20but%0Acomplementary%20edge%20sets%20dynamically.%20Given%20that%2C%20information%20propagation%0Aseparately%20on%20these%20subgraphs%20and%20edge%20splitting%20are%20alternatively%20conducted%2C%0Athus%20disentangling%20the%20task-relevant%20and%20irrelevant%20features.%20Theoretically%2C%20we%0Ashow%20that%20our%20ES-GNN%20can%20be%20regarded%20as%20a%20solution%20to%20a%20disentangled%20graph%0Adenoising%20problem%2C%20which%20further%20illustrates%20our%20motivations%20and%20interprets%20the%0Aimproved%20generalization%20beyond%20homophily.%20Extensive%20experiments%20over%2011%0Abenchmark%20and%201%20synthetic%20datasets%20not%20only%20demonstrate%20the%20effective%0Aperformance%20of%20ES-GNN%20but%20also%20highlight%20its%20robustness%20to%20adversarial%20graphs%0Aand%20mitigation%20of%20the%20over-smoothing%20problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2205.13700v4&entry.124074799=Read"},
{"title": "InstantGroup: Instant Template Generation for Scalable Group of Brain\n  MRI Registration", "author": "Ziyi He and Albert C. S. Chung", "abstract": "  Template generation is a critical step in groupwise image registration, which\ninvolves aligning a group of subjects into a common space. While existing\nmethods can generate high-quality template images, they often incur substantial\ntime costs or are limited by fixed group scales. In this paper, we present\nInstantGroup, an efficient groupwise template generation framework based on\nvariational autoencoder (VAE) models that leverage latent representations'\narithmetic properties, enabling scalability to groups of any size. InstantGroup\nfeatures a Dual VAEs backbone with shared-weight twin networks to handle pairs\nof inputs and incorporates a Displacement Inversion Module (DIM) to maintain\ntemplate unbiasedness and a Subject-Template Alignment Module (STAM) to improve\ntemplate quality and registration accuracy. Experiments on 3D brain MRI scans\nfrom the OASIS and ADNI datasets reveal that InstantGroup dramatically reduces\nruntime, generating templates within seconds for various group sizes while\nmaintaining superior performance compared to state-of-the-art baselines on\nquantitative metrics, including unbiasedness and registration accuracy.\n", "link": "http://arxiv.org/abs/2211.05622v2", "date": "2024-06-26", "relevancy": 2.481, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4999}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4966}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4921}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InstantGroup%3A%20Instant%20Template%20Generation%20for%20Scalable%20Group%20of%20Brain%0A%20%20MRI%20Registration&body=Title%3A%20InstantGroup%3A%20Instant%20Template%20Generation%20for%20Scalable%20Group%20of%20Brain%0A%20%20MRI%20Registration%0AAuthor%3A%20Ziyi%20He%20and%20Albert%20C.%20S.%20Chung%0AAbstract%3A%20%20%20Template%20generation%20is%20a%20critical%20step%20in%20groupwise%20image%20registration%2C%20which%0Ainvolves%20aligning%20a%20group%20of%20subjects%20into%20a%20common%20space.%20While%20existing%0Amethods%20can%20generate%20high-quality%20template%20images%2C%20they%20often%20incur%20substantial%0Atime%20costs%20or%20are%20limited%20by%20fixed%20group%20scales.%20In%20this%20paper%2C%20we%20present%0AInstantGroup%2C%20an%20efficient%20groupwise%20template%20generation%20framework%20based%20on%0Avariational%20autoencoder%20%28VAE%29%20models%20that%20leverage%20latent%20representations%27%0Aarithmetic%20properties%2C%20enabling%20scalability%20to%20groups%20of%20any%20size.%20InstantGroup%0Afeatures%20a%20Dual%20VAEs%20backbone%20with%20shared-weight%20twin%20networks%20to%20handle%20pairs%0Aof%20inputs%20and%20incorporates%20a%20Displacement%20Inversion%20Module%20%28DIM%29%20to%20maintain%0Atemplate%20unbiasedness%20and%20a%20Subject-Template%20Alignment%20Module%20%28STAM%29%20to%20improve%0Atemplate%20quality%20and%20registration%20accuracy.%20Experiments%20on%203D%20brain%20MRI%20scans%0Afrom%20the%20OASIS%20and%20ADNI%20datasets%20reveal%20that%20InstantGroup%20dramatically%20reduces%0Aruntime%2C%20generating%20templates%20within%20seconds%20for%20various%20group%20sizes%20while%0Amaintaining%20superior%20performance%20compared%20to%20state-of-the-art%20baselines%20on%0Aquantitative%20metrics%2C%20including%20unbiasedness%20and%20registration%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.05622v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstantGroup%253A%2520Instant%2520Template%2520Generation%2520for%2520Scalable%2520Group%2520of%2520Brain%250A%2520%2520MRI%2520Registration%26entry.906535625%3DZiyi%2520He%2520and%2520Albert%2520C.%2520S.%2520Chung%26entry.1292438233%3D%2520%2520Template%2520generation%2520is%2520a%2520critical%2520step%2520in%2520groupwise%2520image%2520registration%252C%2520which%250Ainvolves%2520aligning%2520a%2520group%2520of%2520subjects%2520into%2520a%2520common%2520space.%2520While%2520existing%250Amethods%2520can%2520generate%2520high-quality%2520template%2520images%252C%2520they%2520often%2520incur%2520substantial%250Atime%2520costs%2520or%2520are%2520limited%2520by%2520fixed%2520group%2520scales.%2520In%2520this%2520paper%252C%2520we%2520present%250AInstantGroup%252C%2520an%2520efficient%2520groupwise%2520template%2520generation%2520framework%2520based%2520on%250Avariational%2520autoencoder%2520%2528VAE%2529%2520models%2520that%2520leverage%2520latent%2520representations%2527%250Aarithmetic%2520properties%252C%2520enabling%2520scalability%2520to%2520groups%2520of%2520any%2520size.%2520InstantGroup%250Afeatures%2520a%2520Dual%2520VAEs%2520backbone%2520with%2520shared-weight%2520twin%2520networks%2520to%2520handle%2520pairs%250Aof%2520inputs%2520and%2520incorporates%2520a%2520Displacement%2520Inversion%2520Module%2520%2528DIM%2529%2520to%2520maintain%250Atemplate%2520unbiasedness%2520and%2520a%2520Subject-Template%2520Alignment%2520Module%2520%2528STAM%2529%2520to%2520improve%250Atemplate%2520quality%2520and%2520registration%2520accuracy.%2520Experiments%2520on%25203D%2520brain%2520MRI%2520scans%250Afrom%2520the%2520OASIS%2520and%2520ADNI%2520datasets%2520reveal%2520that%2520InstantGroup%2520dramatically%2520reduces%250Aruntime%252C%2520generating%2520templates%2520within%2520seconds%2520for%2520various%2520group%2520sizes%2520while%250Amaintaining%2520superior%2520performance%2520compared%2520to%2520state-of-the-art%2520baselines%2520on%250Aquantitative%2520metrics%252C%2520including%2520unbiasedness%2520and%2520registration%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2211.05622v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InstantGroup%3A%20Instant%20Template%20Generation%20for%20Scalable%20Group%20of%20Brain%0A%20%20MRI%20Registration&entry.906535625=Ziyi%20He%20and%20Albert%20C.%20S.%20Chung&entry.1292438233=%20%20Template%20generation%20is%20a%20critical%20step%20in%20groupwise%20image%20registration%2C%20which%0Ainvolves%20aligning%20a%20group%20of%20subjects%20into%20a%20common%20space.%20While%20existing%0Amethods%20can%20generate%20high-quality%20template%20images%2C%20they%20often%20incur%20substantial%0Atime%20costs%20or%20are%20limited%20by%20fixed%20group%20scales.%20In%20this%20paper%2C%20we%20present%0AInstantGroup%2C%20an%20efficient%20groupwise%20template%20generation%20framework%20based%20on%0Avariational%20autoencoder%20%28VAE%29%20models%20that%20leverage%20latent%20representations%27%0Aarithmetic%20properties%2C%20enabling%20scalability%20to%20groups%20of%20any%20size.%20InstantGroup%0Afeatures%20a%20Dual%20VAEs%20backbone%20with%20shared-weight%20twin%20networks%20to%20handle%20pairs%0Aof%20inputs%20and%20incorporates%20a%20Displacement%20Inversion%20Module%20%28DIM%29%20to%20maintain%0Atemplate%20unbiasedness%20and%20a%20Subject-Template%20Alignment%20Module%20%28STAM%29%20to%20improve%0Atemplate%20quality%20and%20registration%20accuracy.%20Experiments%20on%203D%20brain%20MRI%20scans%0Afrom%20the%20OASIS%20and%20ADNI%20datasets%20reveal%20that%20InstantGroup%20dramatically%20reduces%0Aruntime%2C%20generating%20templates%20within%20seconds%20for%20various%20group%20sizes%20while%0Amaintaining%20superior%20performance%20compared%20to%20state-of-the-art%20baselines%20on%0Aquantitative%20metrics%2C%20including%20unbiasedness%20and%20registration%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.05622v2&entry.124074799=Read"},
{"title": "Towards Compositionality in Concept Learning", "author": "Adam Stein and Aaditya Naik and Yinjun Wu and Mayur Naik and Eric Wong", "abstract": "  Concept-based interpretability methods offer a lens into the internals of\nfoundation models by decomposing their embeddings into high-level concepts.\nThese concept representations are most useful when they are compositional,\nmeaning that the individual concepts compose to explain the full sample. We\nshow that existing unsupervised concept extraction methods find concepts which\nare not compositional. To automatically discover compositional concept\nrepresentations, we identify two salient properties of such representations,\nand propose Compositional Concept Extraction (CCE) for finding concepts which\nobey these properties. We evaluate CCE on five different datasets over image\nand text data. Our evaluation shows that CCE finds more compositional concept\nrepresentations than baselines and yields better accuracy on four downstream\nclassification tasks. Code and data are available at\nhttps://github.com/adaminsky/compositional_concepts .\n", "link": "http://arxiv.org/abs/2406.18534v1", "date": "2024-06-26", "relevancy": 2.4772, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5485}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4778}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4601}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Compositionality%20in%20Concept%20Learning&body=Title%3A%20Towards%20Compositionality%20in%20Concept%20Learning%0AAuthor%3A%20Adam%20Stein%20and%20Aaditya%20Naik%20and%20Yinjun%20Wu%20and%20Mayur%20Naik%20and%20Eric%20Wong%0AAbstract%3A%20%20%20Concept-based%20interpretability%20methods%20offer%20a%20lens%20into%20the%20internals%20of%0Afoundation%20models%20by%20decomposing%20their%20embeddings%20into%20high-level%20concepts.%0AThese%20concept%20representations%20are%20most%20useful%20when%20they%20are%20compositional%2C%0Ameaning%20that%20the%20individual%20concepts%20compose%20to%20explain%20the%20full%20sample.%20We%0Ashow%20that%20existing%20unsupervised%20concept%20extraction%20methods%20find%20concepts%20which%0Aare%20not%20compositional.%20To%20automatically%20discover%20compositional%20concept%0Arepresentations%2C%20we%20identify%20two%20salient%20properties%20of%20such%20representations%2C%0Aand%20propose%20Compositional%20Concept%20Extraction%20%28CCE%29%20for%20finding%20concepts%20which%0Aobey%20these%20properties.%20We%20evaluate%20CCE%20on%20five%20different%20datasets%20over%20image%0Aand%20text%20data.%20Our%20evaluation%20shows%20that%20CCE%20finds%20more%20compositional%20concept%0Arepresentations%20than%20baselines%20and%20yields%20better%20accuracy%20on%20four%20downstream%0Aclassification%20tasks.%20Code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/adaminsky/compositional_concepts%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18534v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Compositionality%2520in%2520Concept%2520Learning%26entry.906535625%3DAdam%2520Stein%2520and%2520Aaditya%2520Naik%2520and%2520Yinjun%2520Wu%2520and%2520Mayur%2520Naik%2520and%2520Eric%2520Wong%26entry.1292438233%3D%2520%2520Concept-based%2520interpretability%2520methods%2520offer%2520a%2520lens%2520into%2520the%2520internals%2520of%250Afoundation%2520models%2520by%2520decomposing%2520their%2520embeddings%2520into%2520high-level%2520concepts.%250AThese%2520concept%2520representations%2520are%2520most%2520useful%2520when%2520they%2520are%2520compositional%252C%250Ameaning%2520that%2520the%2520individual%2520concepts%2520compose%2520to%2520explain%2520the%2520full%2520sample.%2520We%250Ashow%2520that%2520existing%2520unsupervised%2520concept%2520extraction%2520methods%2520find%2520concepts%2520which%250Aare%2520not%2520compositional.%2520To%2520automatically%2520discover%2520compositional%2520concept%250Arepresentations%252C%2520we%2520identify%2520two%2520salient%2520properties%2520of%2520such%2520representations%252C%250Aand%2520propose%2520Compositional%2520Concept%2520Extraction%2520%2528CCE%2529%2520for%2520finding%2520concepts%2520which%250Aobey%2520these%2520properties.%2520We%2520evaluate%2520CCE%2520on%2520five%2520different%2520datasets%2520over%2520image%250Aand%2520text%2520data.%2520Our%2520evaluation%2520shows%2520that%2520CCE%2520finds%2520more%2520compositional%2520concept%250Arepresentations%2520than%2520baselines%2520and%2520yields%2520better%2520accuracy%2520on%2520four%2520downstream%250Aclassification%2520tasks.%2520Code%2520and%2520data%2520are%2520available%2520at%250Ahttps%253A//github.com/adaminsky/compositional_concepts%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18534v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Compositionality%20in%20Concept%20Learning&entry.906535625=Adam%20Stein%20and%20Aaditya%20Naik%20and%20Yinjun%20Wu%20and%20Mayur%20Naik%20and%20Eric%20Wong&entry.1292438233=%20%20Concept-based%20interpretability%20methods%20offer%20a%20lens%20into%20the%20internals%20of%0Afoundation%20models%20by%20decomposing%20their%20embeddings%20into%20high-level%20concepts.%0AThese%20concept%20representations%20are%20most%20useful%20when%20they%20are%20compositional%2C%0Ameaning%20that%20the%20individual%20concepts%20compose%20to%20explain%20the%20full%20sample.%20We%0Ashow%20that%20existing%20unsupervised%20concept%20extraction%20methods%20find%20concepts%20which%0Aare%20not%20compositional.%20To%20automatically%20discover%20compositional%20concept%0Arepresentations%2C%20we%20identify%20two%20salient%20properties%20of%20such%20representations%2C%0Aand%20propose%20Compositional%20Concept%20Extraction%20%28CCE%29%20for%20finding%20concepts%20which%0Aobey%20these%20properties.%20We%20evaluate%20CCE%20on%20five%20different%20datasets%20over%20image%0Aand%20text%20data.%20Our%20evaluation%20shows%20that%20CCE%20finds%20more%20compositional%20concept%0Arepresentations%20than%20baselines%20and%20yields%20better%20accuracy%20on%20four%20downstream%0Aclassification%20tasks.%20Code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/adaminsky/compositional_concepts%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18534v1&entry.124074799=Read"},
{"title": "A Survey of Generative AI for de novo Drug Design: New Frontiers in\n  Molecule and Protein Generation", "author": "Xiangru Tang and Howard Dai and Elizabeth Knight and Fang Wu and Yunyang Li and Tianxiao Li and Mark Gerstein", "abstract": "  Artificial intelligence (AI)-driven methods can vastly improve the\nhistorically costly drug design process, with various generative models already\nin widespread use. Generative models for de novo drug design, in particular,\nfocus on the creation of novel biological compounds entirely from scratch,\nrepresenting a promising future direction. Rapid development in the field,\ncombined with the inherent complexity of the drug design process, creates a\ndifficult landscape for new researchers to enter. In this survey, we organize\nde novo drug design into two overarching themes: small molecule and protein\ngeneration. Within each theme, we identify a variety of subtasks and\napplications, highlighting important datasets, benchmarks, and model\narchitectures and comparing the performance of top models. We take a broad\napproach to AI-driven drug design, allowing for both micro-level comparisons of\nvarious methods within each subtask and macro-level observations across\ndifferent fields. We discuss parallel challenges and approaches between the two\napplications and highlight future directions for AI-driven de novo drug design\nas a whole. An organized repository of all covered sources is available at\nhttps://github.com/gersteinlab/GenAI4Drug.\n", "link": "http://arxiv.org/abs/2402.08703v2", "date": "2024-06-26", "relevancy": 2.4686, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5373}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4791}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4647}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20of%20Generative%20AI%20for%20de%20novo%20Drug%20Design%3A%20New%20Frontiers%20in%0A%20%20Molecule%20and%20Protein%20Generation&body=Title%3A%20A%20Survey%20of%20Generative%20AI%20for%20de%20novo%20Drug%20Design%3A%20New%20Frontiers%20in%0A%20%20Molecule%20and%20Protein%20Generation%0AAuthor%3A%20Xiangru%20Tang%20and%20Howard%20Dai%20and%20Elizabeth%20Knight%20and%20Fang%20Wu%20and%20Yunyang%20Li%20and%20Tianxiao%20Li%20and%20Mark%20Gerstein%0AAbstract%3A%20%20%20Artificial%20intelligence%20%28AI%29-driven%20methods%20can%20vastly%20improve%20the%0Ahistorically%20costly%20drug%20design%20process%2C%20with%20various%20generative%20models%20already%0Ain%20widespread%20use.%20Generative%20models%20for%20de%20novo%20drug%20design%2C%20in%20particular%2C%0Afocus%20on%20the%20creation%20of%20novel%20biological%20compounds%20entirely%20from%20scratch%2C%0Arepresenting%20a%20promising%20future%20direction.%20Rapid%20development%20in%20the%20field%2C%0Acombined%20with%20the%20inherent%20complexity%20of%20the%20drug%20design%20process%2C%20creates%20a%0Adifficult%20landscape%20for%20new%20researchers%20to%20enter.%20In%20this%20survey%2C%20we%20organize%0Ade%20novo%20drug%20design%20into%20two%20overarching%20themes%3A%20small%20molecule%20and%20protein%0Ageneration.%20Within%20each%20theme%2C%20we%20identify%20a%20variety%20of%20subtasks%20and%0Aapplications%2C%20highlighting%20important%20datasets%2C%20benchmarks%2C%20and%20model%0Aarchitectures%20and%20comparing%20the%20performance%20of%20top%20models.%20We%20take%20a%20broad%0Aapproach%20to%20AI-driven%20drug%20design%2C%20allowing%20for%20both%20micro-level%20comparisons%20of%0Avarious%20methods%20within%20each%20subtask%20and%20macro-level%20observations%20across%0Adifferent%20fields.%20We%20discuss%20parallel%20challenges%20and%20approaches%20between%20the%20two%0Aapplications%20and%20highlight%20future%20directions%20for%20AI-driven%20de%20novo%20drug%20design%0Aas%20a%20whole.%20An%20organized%20repository%20of%20all%20covered%20sources%20is%20available%20at%0Ahttps%3A//github.com/gersteinlab/GenAI4Drug.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.08703v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520of%2520Generative%2520AI%2520for%2520de%2520novo%2520Drug%2520Design%253A%2520New%2520Frontiers%2520in%250A%2520%2520Molecule%2520and%2520Protein%2520Generation%26entry.906535625%3DXiangru%2520Tang%2520and%2520Howard%2520Dai%2520and%2520Elizabeth%2520Knight%2520and%2520Fang%2520Wu%2520and%2520Yunyang%2520Li%2520and%2520Tianxiao%2520Li%2520and%2520Mark%2520Gerstein%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520%2528AI%2529-driven%2520methods%2520can%2520vastly%2520improve%2520the%250Ahistorically%2520costly%2520drug%2520design%2520process%252C%2520with%2520various%2520generative%2520models%2520already%250Ain%2520widespread%2520use.%2520Generative%2520models%2520for%2520de%2520novo%2520drug%2520design%252C%2520in%2520particular%252C%250Afocus%2520on%2520the%2520creation%2520of%2520novel%2520biological%2520compounds%2520entirely%2520from%2520scratch%252C%250Arepresenting%2520a%2520promising%2520future%2520direction.%2520Rapid%2520development%2520in%2520the%2520field%252C%250Acombined%2520with%2520the%2520inherent%2520complexity%2520of%2520the%2520drug%2520design%2520process%252C%2520creates%2520a%250Adifficult%2520landscape%2520for%2520new%2520researchers%2520to%2520enter.%2520In%2520this%2520survey%252C%2520we%2520organize%250Ade%2520novo%2520drug%2520design%2520into%2520two%2520overarching%2520themes%253A%2520small%2520molecule%2520and%2520protein%250Ageneration.%2520Within%2520each%2520theme%252C%2520we%2520identify%2520a%2520variety%2520of%2520subtasks%2520and%250Aapplications%252C%2520highlighting%2520important%2520datasets%252C%2520benchmarks%252C%2520and%2520model%250Aarchitectures%2520and%2520comparing%2520the%2520performance%2520of%2520top%2520models.%2520We%2520take%2520a%2520broad%250Aapproach%2520to%2520AI-driven%2520drug%2520design%252C%2520allowing%2520for%2520both%2520micro-level%2520comparisons%2520of%250Avarious%2520methods%2520within%2520each%2520subtask%2520and%2520macro-level%2520observations%2520across%250Adifferent%2520fields.%2520We%2520discuss%2520parallel%2520challenges%2520and%2520approaches%2520between%2520the%2520two%250Aapplications%2520and%2520highlight%2520future%2520directions%2520for%2520AI-driven%2520de%2520novo%2520drug%2520design%250Aas%2520a%2520whole.%2520An%2520organized%2520repository%2520of%2520all%2520covered%2520sources%2520is%2520available%2520at%250Ahttps%253A//github.com/gersteinlab/GenAI4Drug.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.08703v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20of%20Generative%20AI%20for%20de%20novo%20Drug%20Design%3A%20New%20Frontiers%20in%0A%20%20Molecule%20and%20Protein%20Generation&entry.906535625=Xiangru%20Tang%20and%20Howard%20Dai%20and%20Elizabeth%20Knight%20and%20Fang%20Wu%20and%20Yunyang%20Li%20and%20Tianxiao%20Li%20and%20Mark%20Gerstein&entry.1292438233=%20%20Artificial%20intelligence%20%28AI%29-driven%20methods%20can%20vastly%20improve%20the%0Ahistorically%20costly%20drug%20design%20process%2C%20with%20various%20generative%20models%20already%0Ain%20widespread%20use.%20Generative%20models%20for%20de%20novo%20drug%20design%2C%20in%20particular%2C%0Afocus%20on%20the%20creation%20of%20novel%20biological%20compounds%20entirely%20from%20scratch%2C%0Arepresenting%20a%20promising%20future%20direction.%20Rapid%20development%20in%20the%20field%2C%0Acombined%20with%20the%20inherent%20complexity%20of%20the%20drug%20design%20process%2C%20creates%20a%0Adifficult%20landscape%20for%20new%20researchers%20to%20enter.%20In%20this%20survey%2C%20we%20organize%0Ade%20novo%20drug%20design%20into%20two%20overarching%20themes%3A%20small%20molecule%20and%20protein%0Ageneration.%20Within%20each%20theme%2C%20we%20identify%20a%20variety%20of%20subtasks%20and%0Aapplications%2C%20highlighting%20important%20datasets%2C%20benchmarks%2C%20and%20model%0Aarchitectures%20and%20comparing%20the%20performance%20of%20top%20models.%20We%20take%20a%20broad%0Aapproach%20to%20AI-driven%20drug%20design%2C%20allowing%20for%20both%20micro-level%20comparisons%20of%0Avarious%20methods%20within%20each%20subtask%20and%20macro-level%20observations%20across%0Adifferent%20fields.%20We%20discuss%20parallel%20challenges%20and%20approaches%20between%20the%20two%0Aapplications%20and%20highlight%20future%20directions%20for%20AI-driven%20de%20novo%20drug%20design%0Aas%20a%20whole.%20An%20organized%20repository%20of%20all%20covered%20sources%20is%20available%20at%0Ahttps%3A//github.com/gersteinlab/GenAI4Drug.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.08703v2&entry.124074799=Read"},
{"title": "Fast Learnings of Coupled Nonnegative Tensor Decomposition Using Optimal\n  Gradient and Low-rank Approximation", "author": "Xiulin Wang and Jing Liu and Fengyu Cong", "abstract": "  Tensor decomposition is a fundamental technique widely applied in signal\nprocessing, machine learning, and various other fields. However, traditional\ntensor decomposition methods encounter limitations when jointly analyzing\nmulti-block tensors, as they often struggle to effectively explore shared\ninformation among tensors. In this study, we first introduce a novel coupled\nnonnegative CANDECOMP/PARAFAC decomposition algorithm optimized by the\nalternating proximal gradient method (CoNCPD-APG). This algorithm is specially\ndesigned to address the challenges of jointly decomposing different tensors\nthat are partially or fully linked, while simultaneously extracting common\ncomponents, individual components and, core tensors. Recognizing the\ncomputational challenges inherent in optimizing nonnegative constraints over\nhigh-dimensional tensor data, we further propose the lraCoNCPD-APG algorithm.\nBy integrating low-rank approximation with the proposed CoNCPD-APG method, the\nproposed algorithm can significantly decrease the computational burden without\ncompromising decomposition quality, particularly for multi-block large-scale\ntensors. Simulation experiments conducted on synthetic data, real-world face\nimage data, and two kinds of electroencephalography (EEG) data demonstrate the\npracticality and superiority of the proposed algorithms for coupled nonnegative\ntensor decomposition problems. Our results underscore the efficacy of our\nmethods in uncovering meaningful patterns and structures from complex\nmulti-block tensor data, thereby offering valuable insights for future\napplications.\n", "link": "http://arxiv.org/abs/2302.05119v2", "date": "2024-06-26", "relevancy": 2.4464, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4935}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4917}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4826}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Learnings%20of%20Coupled%20Nonnegative%20Tensor%20Decomposition%20Using%20Optimal%0A%20%20Gradient%20and%20Low-rank%20Approximation&body=Title%3A%20Fast%20Learnings%20of%20Coupled%20Nonnegative%20Tensor%20Decomposition%20Using%20Optimal%0A%20%20Gradient%20and%20Low-rank%20Approximation%0AAuthor%3A%20Xiulin%20Wang%20and%20Jing%20Liu%20and%20Fengyu%20Cong%0AAbstract%3A%20%20%20Tensor%20decomposition%20is%20a%20fundamental%20technique%20widely%20applied%20in%20signal%0Aprocessing%2C%20machine%20learning%2C%20and%20various%20other%20fields.%20However%2C%20traditional%0Atensor%20decomposition%20methods%20encounter%20limitations%20when%20jointly%20analyzing%0Amulti-block%20tensors%2C%20as%20they%20often%20struggle%20to%20effectively%20explore%20shared%0Ainformation%20among%20tensors.%20In%20this%20study%2C%20we%20first%20introduce%20a%20novel%20coupled%0Anonnegative%20CANDECOMP/PARAFAC%20decomposition%20algorithm%20optimized%20by%20the%0Aalternating%20proximal%20gradient%20method%20%28CoNCPD-APG%29.%20This%20algorithm%20is%20specially%0Adesigned%20to%20address%20the%20challenges%20of%20jointly%20decomposing%20different%20tensors%0Athat%20are%20partially%20or%20fully%20linked%2C%20while%20simultaneously%20extracting%20common%0Acomponents%2C%20individual%20components%20and%2C%20core%20tensors.%20Recognizing%20the%0Acomputational%20challenges%20inherent%20in%20optimizing%20nonnegative%20constraints%20over%0Ahigh-dimensional%20tensor%20data%2C%20we%20further%20propose%20the%20lraCoNCPD-APG%20algorithm.%0ABy%20integrating%20low-rank%20approximation%20with%20the%20proposed%20CoNCPD-APG%20method%2C%20the%0Aproposed%20algorithm%20can%20significantly%20decrease%20the%20computational%20burden%20without%0Acompromising%20decomposition%20quality%2C%20particularly%20for%20multi-block%20large-scale%0Atensors.%20Simulation%20experiments%20conducted%20on%20synthetic%20data%2C%20real-world%20face%0Aimage%20data%2C%20and%20two%20kinds%20of%20electroencephalography%20%28EEG%29%20data%20demonstrate%20the%0Apracticality%20and%20superiority%20of%20the%20proposed%20algorithms%20for%20coupled%20nonnegative%0Atensor%20decomposition%20problems.%20Our%20results%20underscore%20the%20efficacy%20of%20our%0Amethods%20in%20uncovering%20meaningful%20patterns%20and%20structures%20from%20complex%0Amulti-block%20tensor%20data%2C%20thereby%20offering%20valuable%20insights%20for%20future%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.05119v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Learnings%2520of%2520Coupled%2520Nonnegative%2520Tensor%2520Decomposition%2520Using%2520Optimal%250A%2520%2520Gradient%2520and%2520Low-rank%2520Approximation%26entry.906535625%3DXiulin%2520Wang%2520and%2520Jing%2520Liu%2520and%2520Fengyu%2520Cong%26entry.1292438233%3D%2520%2520Tensor%2520decomposition%2520is%2520a%2520fundamental%2520technique%2520widely%2520applied%2520in%2520signal%250Aprocessing%252C%2520machine%2520learning%252C%2520and%2520various%2520other%2520fields.%2520However%252C%2520traditional%250Atensor%2520decomposition%2520methods%2520encounter%2520limitations%2520when%2520jointly%2520analyzing%250Amulti-block%2520tensors%252C%2520as%2520they%2520often%2520struggle%2520to%2520effectively%2520explore%2520shared%250Ainformation%2520among%2520tensors.%2520In%2520this%2520study%252C%2520we%2520first%2520introduce%2520a%2520novel%2520coupled%250Anonnegative%2520CANDECOMP/PARAFAC%2520decomposition%2520algorithm%2520optimized%2520by%2520the%250Aalternating%2520proximal%2520gradient%2520method%2520%2528CoNCPD-APG%2529.%2520This%2520algorithm%2520is%2520specially%250Adesigned%2520to%2520address%2520the%2520challenges%2520of%2520jointly%2520decomposing%2520different%2520tensors%250Athat%2520are%2520partially%2520or%2520fully%2520linked%252C%2520while%2520simultaneously%2520extracting%2520common%250Acomponents%252C%2520individual%2520components%2520and%252C%2520core%2520tensors.%2520Recognizing%2520the%250Acomputational%2520challenges%2520inherent%2520in%2520optimizing%2520nonnegative%2520constraints%2520over%250Ahigh-dimensional%2520tensor%2520data%252C%2520we%2520further%2520propose%2520the%2520lraCoNCPD-APG%2520algorithm.%250ABy%2520integrating%2520low-rank%2520approximation%2520with%2520the%2520proposed%2520CoNCPD-APG%2520method%252C%2520the%250Aproposed%2520algorithm%2520can%2520significantly%2520decrease%2520the%2520computational%2520burden%2520without%250Acompromising%2520decomposition%2520quality%252C%2520particularly%2520for%2520multi-block%2520large-scale%250Atensors.%2520Simulation%2520experiments%2520conducted%2520on%2520synthetic%2520data%252C%2520real-world%2520face%250Aimage%2520data%252C%2520and%2520two%2520kinds%2520of%2520electroencephalography%2520%2528EEG%2529%2520data%2520demonstrate%2520the%250Apracticality%2520and%2520superiority%2520of%2520the%2520proposed%2520algorithms%2520for%2520coupled%2520nonnegative%250Atensor%2520decomposition%2520problems.%2520Our%2520results%2520underscore%2520the%2520efficacy%2520of%2520our%250Amethods%2520in%2520uncovering%2520meaningful%2520patterns%2520and%2520structures%2520from%2520complex%250Amulti-block%2520tensor%2520data%252C%2520thereby%2520offering%2520valuable%2520insights%2520for%2520future%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.05119v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Learnings%20of%20Coupled%20Nonnegative%20Tensor%20Decomposition%20Using%20Optimal%0A%20%20Gradient%20and%20Low-rank%20Approximation&entry.906535625=Xiulin%20Wang%20and%20Jing%20Liu%20and%20Fengyu%20Cong&entry.1292438233=%20%20Tensor%20decomposition%20is%20a%20fundamental%20technique%20widely%20applied%20in%20signal%0Aprocessing%2C%20machine%20learning%2C%20and%20various%20other%20fields.%20However%2C%20traditional%0Atensor%20decomposition%20methods%20encounter%20limitations%20when%20jointly%20analyzing%0Amulti-block%20tensors%2C%20as%20they%20often%20struggle%20to%20effectively%20explore%20shared%0Ainformation%20among%20tensors.%20In%20this%20study%2C%20we%20first%20introduce%20a%20novel%20coupled%0Anonnegative%20CANDECOMP/PARAFAC%20decomposition%20algorithm%20optimized%20by%20the%0Aalternating%20proximal%20gradient%20method%20%28CoNCPD-APG%29.%20This%20algorithm%20is%20specially%0Adesigned%20to%20address%20the%20challenges%20of%20jointly%20decomposing%20different%20tensors%0Athat%20are%20partially%20or%20fully%20linked%2C%20while%20simultaneously%20extracting%20common%0Acomponents%2C%20individual%20components%20and%2C%20core%20tensors.%20Recognizing%20the%0Acomputational%20challenges%20inherent%20in%20optimizing%20nonnegative%20constraints%20over%0Ahigh-dimensional%20tensor%20data%2C%20we%20further%20propose%20the%20lraCoNCPD-APG%20algorithm.%0ABy%20integrating%20low-rank%20approximation%20with%20the%20proposed%20CoNCPD-APG%20method%2C%20the%0Aproposed%20algorithm%20can%20significantly%20decrease%20the%20computational%20burden%20without%0Acompromising%20decomposition%20quality%2C%20particularly%20for%20multi-block%20large-scale%0Atensors.%20Simulation%20experiments%20conducted%20on%20synthetic%20data%2C%20real-world%20face%0Aimage%20data%2C%20and%20two%20kinds%20of%20electroencephalography%20%28EEG%29%20data%20demonstrate%20the%0Apracticality%20and%20superiority%20of%20the%20proposed%20algorithms%20for%20coupled%20nonnegative%0Atensor%20decomposition%20problems.%20Our%20results%20underscore%20the%20efficacy%20of%20our%0Amethods%20in%20uncovering%20meaningful%20patterns%20and%20structures%20from%20complex%0Amulti-block%20tensor%20data%2C%20thereby%20offering%20valuable%20insights%20for%20future%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.05119v2&entry.124074799=Read"},
{"title": "Stabilizing Policy Gradients for Stochastic Differential Equations via\n  Consistency with Perturbation Process", "author": "Xiangxin Zhou and Liang Wang and Yichi Zhou", "abstract": "  Considering generating samples with high rewards, we focus on optimizing deep\nneural networks parameterized stochastic differential equations (SDEs), the\nadvanced generative models with high expressiveness, with policy gradient, the\nleading algorithm in reinforcement learning. Nevertheless, when applying policy\ngradients to SDEs, since the policy gradient is estimated on a finite set of\ntrajectories, it can be ill-defined, and the policy behavior in data-scarce\nregions may be uncontrolled. This challenge compromises the stability of policy\ngradients and negatively impacts sample complexity. To address these issues, we\npropose constraining the SDE to be consistent with its associated perturbation\nprocess. Since the perturbation process covers the entire space and is easy to\nsample, we can mitigate the aforementioned problems. Our framework offers a\ngeneral approach allowing for a versatile selection of policy gradient methods\nto effectively and efficiently train SDEs. We evaluate our algorithm on the\ntask of structure-based drug design and optimize the binding affinity of\ngenerated ligand molecules. Our method achieves the best Vina score -9.07 on\nthe CrossDocked2020 dataset.\n", "link": "http://arxiv.org/abs/2403.04154v2", "date": "2024-06-26", "relevancy": 2.4439, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4987}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4885}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stabilizing%20Policy%20Gradients%20for%20Stochastic%20Differential%20Equations%20via%0A%20%20Consistency%20with%20Perturbation%20Process&body=Title%3A%20Stabilizing%20Policy%20Gradients%20for%20Stochastic%20Differential%20Equations%20via%0A%20%20Consistency%20with%20Perturbation%20Process%0AAuthor%3A%20Xiangxin%20Zhou%20and%20Liang%20Wang%20and%20Yichi%20Zhou%0AAbstract%3A%20%20%20Considering%20generating%20samples%20with%20high%20rewards%2C%20we%20focus%20on%20optimizing%20deep%0Aneural%20networks%20parameterized%20stochastic%20differential%20equations%20%28SDEs%29%2C%20the%0Aadvanced%20generative%20models%20with%20high%20expressiveness%2C%20with%20policy%20gradient%2C%20the%0Aleading%20algorithm%20in%20reinforcement%20learning.%20Nevertheless%2C%20when%20applying%20policy%0Agradients%20to%20SDEs%2C%20since%20the%20policy%20gradient%20is%20estimated%20on%20a%20finite%20set%20of%0Atrajectories%2C%20it%20can%20be%20ill-defined%2C%20and%20the%20policy%20behavior%20in%20data-scarce%0Aregions%20may%20be%20uncontrolled.%20This%20challenge%20compromises%20the%20stability%20of%20policy%0Agradients%20and%20negatively%20impacts%20sample%20complexity.%20To%20address%20these%20issues%2C%20we%0Apropose%20constraining%20the%20SDE%20to%20be%20consistent%20with%20its%20associated%20perturbation%0Aprocess.%20Since%20the%20perturbation%20process%20covers%20the%20entire%20space%20and%20is%20easy%20to%0Asample%2C%20we%20can%20mitigate%20the%20aforementioned%20problems.%20Our%20framework%20offers%20a%0Ageneral%20approach%20allowing%20for%20a%20versatile%20selection%20of%20policy%20gradient%20methods%0Ato%20effectively%20and%20efficiently%20train%20SDEs.%20We%20evaluate%20our%20algorithm%20on%20the%0Atask%20of%20structure-based%20drug%20design%20and%20optimize%20the%20binding%20affinity%20of%0Agenerated%20ligand%20molecules.%20Our%20method%20achieves%20the%20best%20Vina%20score%20-9.07%20on%0Athe%20CrossDocked2020%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04154v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStabilizing%2520Policy%2520Gradients%2520for%2520Stochastic%2520Differential%2520Equations%2520via%250A%2520%2520Consistency%2520with%2520Perturbation%2520Process%26entry.906535625%3DXiangxin%2520Zhou%2520and%2520Liang%2520Wang%2520and%2520Yichi%2520Zhou%26entry.1292438233%3D%2520%2520Considering%2520generating%2520samples%2520with%2520high%2520rewards%252C%2520we%2520focus%2520on%2520optimizing%2520deep%250Aneural%2520networks%2520parameterized%2520stochastic%2520differential%2520equations%2520%2528SDEs%2529%252C%2520the%250Aadvanced%2520generative%2520models%2520with%2520high%2520expressiveness%252C%2520with%2520policy%2520gradient%252C%2520the%250Aleading%2520algorithm%2520in%2520reinforcement%2520learning.%2520Nevertheless%252C%2520when%2520applying%2520policy%250Agradients%2520to%2520SDEs%252C%2520since%2520the%2520policy%2520gradient%2520is%2520estimated%2520on%2520a%2520finite%2520set%2520of%250Atrajectories%252C%2520it%2520can%2520be%2520ill-defined%252C%2520and%2520the%2520policy%2520behavior%2520in%2520data-scarce%250Aregions%2520may%2520be%2520uncontrolled.%2520This%2520challenge%2520compromises%2520the%2520stability%2520of%2520policy%250Agradients%2520and%2520negatively%2520impacts%2520sample%2520complexity.%2520To%2520address%2520these%2520issues%252C%2520we%250Apropose%2520constraining%2520the%2520SDE%2520to%2520be%2520consistent%2520with%2520its%2520associated%2520perturbation%250Aprocess.%2520Since%2520the%2520perturbation%2520process%2520covers%2520the%2520entire%2520space%2520and%2520is%2520easy%2520to%250Asample%252C%2520we%2520can%2520mitigate%2520the%2520aforementioned%2520problems.%2520Our%2520framework%2520offers%2520a%250Ageneral%2520approach%2520allowing%2520for%2520a%2520versatile%2520selection%2520of%2520policy%2520gradient%2520methods%250Ato%2520effectively%2520and%2520efficiently%2520train%2520SDEs.%2520We%2520evaluate%2520our%2520algorithm%2520on%2520the%250Atask%2520of%2520structure-based%2520drug%2520design%2520and%2520optimize%2520the%2520binding%2520affinity%2520of%250Agenerated%2520ligand%2520molecules.%2520Our%2520method%2520achieves%2520the%2520best%2520Vina%2520score%2520-9.07%2520on%250Athe%2520CrossDocked2020%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.04154v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stabilizing%20Policy%20Gradients%20for%20Stochastic%20Differential%20Equations%20via%0A%20%20Consistency%20with%20Perturbation%20Process&entry.906535625=Xiangxin%20Zhou%20and%20Liang%20Wang%20and%20Yichi%20Zhou&entry.1292438233=%20%20Considering%20generating%20samples%20with%20high%20rewards%2C%20we%20focus%20on%20optimizing%20deep%0Aneural%20networks%20parameterized%20stochastic%20differential%20equations%20%28SDEs%29%2C%20the%0Aadvanced%20generative%20models%20with%20high%20expressiveness%2C%20with%20policy%20gradient%2C%20the%0Aleading%20algorithm%20in%20reinforcement%20learning.%20Nevertheless%2C%20when%20applying%20policy%0Agradients%20to%20SDEs%2C%20since%20the%20policy%20gradient%20is%20estimated%20on%20a%20finite%20set%20of%0Atrajectories%2C%20it%20can%20be%20ill-defined%2C%20and%20the%20policy%20behavior%20in%20data-scarce%0Aregions%20may%20be%20uncontrolled.%20This%20challenge%20compromises%20the%20stability%20of%20policy%0Agradients%20and%20negatively%20impacts%20sample%20complexity.%20To%20address%20these%20issues%2C%20we%0Apropose%20constraining%20the%20SDE%20to%20be%20consistent%20with%20its%20associated%20perturbation%0Aprocess.%20Since%20the%20perturbation%20process%20covers%20the%20entire%20space%20and%20is%20easy%20to%0Asample%2C%20we%20can%20mitigate%20the%20aforementioned%20problems.%20Our%20framework%20offers%20a%0Ageneral%20approach%20allowing%20for%20a%20versatile%20selection%20of%20policy%20gradient%20methods%0Ato%20effectively%20and%20efficiently%20train%20SDEs.%20We%20evaluate%20our%20algorithm%20on%20the%0Atask%20of%20structure-based%20drug%20design%20and%20optimize%20the%20binding%20affinity%20of%0Agenerated%20ligand%20molecules.%20Our%20method%20achieves%20the%20best%20Vina%20score%20-9.07%20on%0Athe%20CrossDocked2020%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04154v2&entry.124074799=Read"},
{"title": "Towards Human-Level 3D Relative Pose Estimation: Generalizable,\n  Training-Free, with Single Reference", "author": "Yuan Gao and Yajing Luo and Junhong Wang and Kui Jia and Gui-Song Xia", "abstract": "  Humans can easily deduce the relative pose of an unseen object, without\nlabel/training, given only a single query-reference image pair. This is\narguably achieved by incorporating (i) 3D/2.5D shape perception from a single\nimage, (ii) render-and-compare simulation, and (iii) rich semantic cue\nawareness to furnish (coarse) reference-query correspondence. Existing methods\nimplement (i) by a 3D CAD model or well-calibrated multiple images and (ii) by\ntraining a network on specific objects, which necessitate laborious\nground-truth labeling and tedious training, potentially leading to challenges\nin generalization. Moreover, (iii) was less exploited in the paradigm of (ii),\ndespite that the coarse correspondence from (iii) enhances the compare process\nby filtering out non-overlapped parts under substantial pose\ndifferences/occlusions. Motivated by this, we propose a novel 3D generalizable\nrelative pose estimation method by elaborating (i) with a 2.5D shape from an\nRGB-D reference, (ii) with an off-the-shelf differentiable renderer, and (iii)\nwith semantic cues from a pretrained model like DINOv2. Specifically, our\ndifferentiable renderer takes the 2.5D rotatable mesh textured by the RGB and\nthe semantic maps (obtained by DINOv2 from the RGB input), then renders new RGB\nand semantic maps (with back-surface culling) under a novel rotated view. The\nrefinement loss comes from comparing the rendered RGB and semantic maps with\nthe query ones, back-propagating the gradients through the differentiable\nrenderer to refine the 3D relative pose. As a result, our method can be readily\napplied to unseen objects, given only a single RGB-D reference, without\nlabel/training. Extensive experiments on LineMOD, LM-O, and YCB-V show that our\ntraining-free method significantly outperforms the SOTA supervised methods,\nespecially under the rigorous Acc@5/10/15{\\deg} metrics and the challenging\ncross-dataset settings.\n", "link": "http://arxiv.org/abs/2406.18453v1", "date": "2024-06-26", "relevancy": 2.43, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6228}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5976}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Human-Level%203D%20Relative%20Pose%20Estimation%3A%20Generalizable%2C%0A%20%20Training-Free%2C%20with%20Single%20Reference&body=Title%3A%20Towards%20Human-Level%203D%20Relative%20Pose%20Estimation%3A%20Generalizable%2C%0A%20%20Training-Free%2C%20with%20Single%20Reference%0AAuthor%3A%20Yuan%20Gao%20and%20Yajing%20Luo%20and%20Junhong%20Wang%20and%20Kui%20Jia%20and%20Gui-Song%20Xia%0AAbstract%3A%20%20%20Humans%20can%20easily%20deduce%20the%20relative%20pose%20of%20an%20unseen%20object%2C%20without%0Alabel/training%2C%20given%20only%20a%20single%20query-reference%20image%20pair.%20This%20is%0Aarguably%20achieved%20by%20incorporating%20%28i%29%203D/2.5D%20shape%20perception%20from%20a%20single%0Aimage%2C%20%28ii%29%20render-and-compare%20simulation%2C%20and%20%28iii%29%20rich%20semantic%20cue%0Aawareness%20to%20furnish%20%28coarse%29%20reference-query%20correspondence.%20Existing%20methods%0Aimplement%20%28i%29%20by%20a%203D%20CAD%20model%20or%20well-calibrated%20multiple%20images%20and%20%28ii%29%20by%0Atraining%20a%20network%20on%20specific%20objects%2C%20which%20necessitate%20laborious%0Aground-truth%20labeling%20and%20tedious%20training%2C%20potentially%20leading%20to%20challenges%0Ain%20generalization.%20Moreover%2C%20%28iii%29%20was%20less%20exploited%20in%20the%20paradigm%20of%20%28ii%29%2C%0Adespite%20that%20the%20coarse%20correspondence%20from%20%28iii%29%20enhances%20the%20compare%20process%0Aby%20filtering%20out%20non-overlapped%20parts%20under%20substantial%20pose%0Adifferences/occlusions.%20Motivated%20by%20this%2C%20we%20propose%20a%20novel%203D%20generalizable%0Arelative%20pose%20estimation%20method%20by%20elaborating%20%28i%29%20with%20a%202.5D%20shape%20from%20an%0ARGB-D%20reference%2C%20%28ii%29%20with%20an%20off-the-shelf%20differentiable%20renderer%2C%20and%20%28iii%29%0Awith%20semantic%20cues%20from%20a%20pretrained%20model%20like%20DINOv2.%20Specifically%2C%20our%0Adifferentiable%20renderer%20takes%20the%202.5D%20rotatable%20mesh%20textured%20by%20the%20RGB%20and%0Athe%20semantic%20maps%20%28obtained%20by%20DINOv2%20from%20the%20RGB%20input%29%2C%20then%20renders%20new%20RGB%0Aand%20semantic%20maps%20%28with%20back-surface%20culling%29%20under%20a%20novel%20rotated%20view.%20The%0Arefinement%20loss%20comes%20from%20comparing%20the%20rendered%20RGB%20and%20semantic%20maps%20with%0Athe%20query%20ones%2C%20back-propagating%20the%20gradients%20through%20the%20differentiable%0Arenderer%20to%20refine%20the%203D%20relative%20pose.%20As%20a%20result%2C%20our%20method%20can%20be%20readily%0Aapplied%20to%20unseen%20objects%2C%20given%20only%20a%20single%20RGB-D%20reference%2C%20without%0Alabel/training.%20Extensive%20experiments%20on%20LineMOD%2C%20LM-O%2C%20and%20YCB-V%20show%20that%20our%0Atraining-free%20method%20significantly%20outperforms%20the%20SOTA%20supervised%20methods%2C%0Aespecially%20under%20the%20rigorous%20Acc%405/10/15%7B%5Cdeg%7D%20metrics%20and%20the%20challenging%0Across-dataset%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18453v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Human-Level%25203D%2520Relative%2520Pose%2520Estimation%253A%2520Generalizable%252C%250A%2520%2520Training-Free%252C%2520with%2520Single%2520Reference%26entry.906535625%3DYuan%2520Gao%2520and%2520Yajing%2520Luo%2520and%2520Junhong%2520Wang%2520and%2520Kui%2520Jia%2520and%2520Gui-Song%2520Xia%26entry.1292438233%3D%2520%2520Humans%2520can%2520easily%2520deduce%2520the%2520relative%2520pose%2520of%2520an%2520unseen%2520object%252C%2520without%250Alabel/training%252C%2520given%2520only%2520a%2520single%2520query-reference%2520image%2520pair.%2520This%2520is%250Aarguably%2520achieved%2520by%2520incorporating%2520%2528i%2529%25203D/2.5D%2520shape%2520perception%2520from%2520a%2520single%250Aimage%252C%2520%2528ii%2529%2520render-and-compare%2520simulation%252C%2520and%2520%2528iii%2529%2520rich%2520semantic%2520cue%250Aawareness%2520to%2520furnish%2520%2528coarse%2529%2520reference-query%2520correspondence.%2520Existing%2520methods%250Aimplement%2520%2528i%2529%2520by%2520a%25203D%2520CAD%2520model%2520or%2520well-calibrated%2520multiple%2520images%2520and%2520%2528ii%2529%2520by%250Atraining%2520a%2520network%2520on%2520specific%2520objects%252C%2520which%2520necessitate%2520laborious%250Aground-truth%2520labeling%2520and%2520tedious%2520training%252C%2520potentially%2520leading%2520to%2520challenges%250Ain%2520generalization.%2520Moreover%252C%2520%2528iii%2529%2520was%2520less%2520exploited%2520in%2520the%2520paradigm%2520of%2520%2528ii%2529%252C%250Adespite%2520that%2520the%2520coarse%2520correspondence%2520from%2520%2528iii%2529%2520enhances%2520the%2520compare%2520process%250Aby%2520filtering%2520out%2520non-overlapped%2520parts%2520under%2520substantial%2520pose%250Adifferences/occlusions.%2520Motivated%2520by%2520this%252C%2520we%2520propose%2520a%2520novel%25203D%2520generalizable%250Arelative%2520pose%2520estimation%2520method%2520by%2520elaborating%2520%2528i%2529%2520with%2520a%25202.5D%2520shape%2520from%2520an%250ARGB-D%2520reference%252C%2520%2528ii%2529%2520with%2520an%2520off-the-shelf%2520differentiable%2520renderer%252C%2520and%2520%2528iii%2529%250Awith%2520semantic%2520cues%2520from%2520a%2520pretrained%2520model%2520like%2520DINOv2.%2520Specifically%252C%2520our%250Adifferentiable%2520renderer%2520takes%2520the%25202.5D%2520rotatable%2520mesh%2520textured%2520by%2520the%2520RGB%2520and%250Athe%2520semantic%2520maps%2520%2528obtained%2520by%2520DINOv2%2520from%2520the%2520RGB%2520input%2529%252C%2520then%2520renders%2520new%2520RGB%250Aand%2520semantic%2520maps%2520%2528with%2520back-surface%2520culling%2529%2520under%2520a%2520novel%2520rotated%2520view.%2520The%250Arefinement%2520loss%2520comes%2520from%2520comparing%2520the%2520rendered%2520RGB%2520and%2520semantic%2520maps%2520with%250Athe%2520query%2520ones%252C%2520back-propagating%2520the%2520gradients%2520through%2520the%2520differentiable%250Arenderer%2520to%2520refine%2520the%25203D%2520relative%2520pose.%2520As%2520a%2520result%252C%2520our%2520method%2520can%2520be%2520readily%250Aapplied%2520to%2520unseen%2520objects%252C%2520given%2520only%2520a%2520single%2520RGB-D%2520reference%252C%2520without%250Alabel/training.%2520Extensive%2520experiments%2520on%2520LineMOD%252C%2520LM-O%252C%2520and%2520YCB-V%2520show%2520that%2520our%250Atraining-free%2520method%2520significantly%2520outperforms%2520the%2520SOTA%2520supervised%2520methods%252C%250Aespecially%2520under%2520the%2520rigorous%2520Acc%25405/10/15%257B%255Cdeg%257D%2520metrics%2520and%2520the%2520challenging%250Across-dataset%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18453v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Human-Level%203D%20Relative%20Pose%20Estimation%3A%20Generalizable%2C%0A%20%20Training-Free%2C%20with%20Single%20Reference&entry.906535625=Yuan%20Gao%20and%20Yajing%20Luo%20and%20Junhong%20Wang%20and%20Kui%20Jia%20and%20Gui-Song%20Xia&entry.1292438233=%20%20Humans%20can%20easily%20deduce%20the%20relative%20pose%20of%20an%20unseen%20object%2C%20without%0Alabel/training%2C%20given%20only%20a%20single%20query-reference%20image%20pair.%20This%20is%0Aarguably%20achieved%20by%20incorporating%20%28i%29%203D/2.5D%20shape%20perception%20from%20a%20single%0Aimage%2C%20%28ii%29%20render-and-compare%20simulation%2C%20and%20%28iii%29%20rich%20semantic%20cue%0Aawareness%20to%20furnish%20%28coarse%29%20reference-query%20correspondence.%20Existing%20methods%0Aimplement%20%28i%29%20by%20a%203D%20CAD%20model%20or%20well-calibrated%20multiple%20images%20and%20%28ii%29%20by%0Atraining%20a%20network%20on%20specific%20objects%2C%20which%20necessitate%20laborious%0Aground-truth%20labeling%20and%20tedious%20training%2C%20potentially%20leading%20to%20challenges%0Ain%20generalization.%20Moreover%2C%20%28iii%29%20was%20less%20exploited%20in%20the%20paradigm%20of%20%28ii%29%2C%0Adespite%20that%20the%20coarse%20correspondence%20from%20%28iii%29%20enhances%20the%20compare%20process%0Aby%20filtering%20out%20non-overlapped%20parts%20under%20substantial%20pose%0Adifferences/occlusions.%20Motivated%20by%20this%2C%20we%20propose%20a%20novel%203D%20generalizable%0Arelative%20pose%20estimation%20method%20by%20elaborating%20%28i%29%20with%20a%202.5D%20shape%20from%20an%0ARGB-D%20reference%2C%20%28ii%29%20with%20an%20off-the-shelf%20differentiable%20renderer%2C%20and%20%28iii%29%0Awith%20semantic%20cues%20from%20a%20pretrained%20model%20like%20DINOv2.%20Specifically%2C%20our%0Adifferentiable%20renderer%20takes%20the%202.5D%20rotatable%20mesh%20textured%20by%20the%20RGB%20and%0Athe%20semantic%20maps%20%28obtained%20by%20DINOv2%20from%20the%20RGB%20input%29%2C%20then%20renders%20new%20RGB%0Aand%20semantic%20maps%20%28with%20back-surface%20culling%29%20under%20a%20novel%20rotated%20view.%20The%0Arefinement%20loss%20comes%20from%20comparing%20the%20rendered%20RGB%20and%20semantic%20maps%20with%0Athe%20query%20ones%2C%20back-propagating%20the%20gradients%20through%20the%20differentiable%0Arenderer%20to%20refine%20the%203D%20relative%20pose.%20As%20a%20result%2C%20our%20method%20can%20be%20readily%0Aapplied%20to%20unseen%20objects%2C%20given%20only%20a%20single%20RGB-D%20reference%2C%20without%0Alabel/training.%20Extensive%20experiments%20on%20LineMOD%2C%20LM-O%2C%20and%20YCB-V%20show%20that%20our%0Atraining-free%20method%20significantly%20outperforms%20the%20SOTA%20supervised%20methods%2C%0Aespecially%20under%20the%20rigorous%20Acc%405/10/15%7B%5Cdeg%7D%20metrics%20and%20the%20challenging%0Across-dataset%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18453v1&entry.124074799=Read"},
{"title": "GUIDE: A Guideline-Guided Dataset for Instructional Video Comprehension", "author": "Jiafeng Liang and Shixin Jiang and Zekun Wang and Haojie Pan and Zerui Chen and Zheng Chu and Ming Liu and Ruiji Fu and Zhongyuan Wang and Bing Qin", "abstract": "  There are substantial instructional videos on the Internet, which provide us\ntutorials for completing various tasks. Existing instructional video datasets\nonly focus on specific steps at the video level, lacking experiential\nguidelines at the task level, which can lead to beginners struggling to learn\nnew tasks due to the lack of relevant experience. Moreover, the specific steps\nwithout guidelines are trivial and unsystematic, making it difficult to provide\na clear tutorial. To address these problems, we present the GUIDE\n(Guideline-Guided) dataset, which contains 3.5K videos of 560 instructional\ntasks in 8 domains related to our daily life. Specifically, we annotate each\ninstructional task with a guideline, representing a common pattern shared by\nall task-related videos. On this basis, we annotate systematic specific steps,\nincluding their associated guideline steps, specific step descriptions and\ntimestamps. Our proposed benchmark consists of three sub-tasks to evaluate\ncomprehension ability of models: (1) Step Captioning: models have to generate\ncaptions for specific steps from videos. (2) Guideline Summarization: models\nhave to mine the common pattern in task-related videos and summarize a\nguideline from them. (3) Guideline-Guided Captioning: models have to generate\ncaptions for specific steps under the guide of guideline. We evaluate plenty of\nfoundation models with GUIDE and perform in-depth analysis. Given the diversity\nand practicality of GUIDE, we believe that it can be used as a better benchmark\nfor instructional video comprehension.\n", "link": "http://arxiv.org/abs/2406.18227v1", "date": "2024-06-26", "relevancy": 2.4175, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5005}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4763}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GUIDE%3A%20A%20Guideline-Guided%20Dataset%20for%20Instructional%20Video%20Comprehension&body=Title%3A%20GUIDE%3A%20A%20Guideline-Guided%20Dataset%20for%20Instructional%20Video%20Comprehension%0AAuthor%3A%20Jiafeng%20Liang%20and%20Shixin%20Jiang%20and%20Zekun%20Wang%20and%20Haojie%20Pan%20and%20Zerui%20Chen%20and%20Zheng%20Chu%20and%20Ming%20Liu%20and%20Ruiji%20Fu%20and%20Zhongyuan%20Wang%20and%20Bing%20Qin%0AAbstract%3A%20%20%20There%20are%20substantial%20instructional%20videos%20on%20the%20Internet%2C%20which%20provide%20us%0Atutorials%20for%20completing%20various%20tasks.%20Existing%20instructional%20video%20datasets%0Aonly%20focus%20on%20specific%20steps%20at%20the%20video%20level%2C%20lacking%20experiential%0Aguidelines%20at%20the%20task%20level%2C%20which%20can%20lead%20to%20beginners%20struggling%20to%20learn%0Anew%20tasks%20due%20to%20the%20lack%20of%20relevant%20experience.%20Moreover%2C%20the%20specific%20steps%0Awithout%20guidelines%20are%20trivial%20and%20unsystematic%2C%20making%20it%20difficult%20to%20provide%0Aa%20clear%20tutorial.%20To%20address%20these%20problems%2C%20we%20present%20the%20GUIDE%0A%28Guideline-Guided%29%20dataset%2C%20which%20contains%203.5K%20videos%20of%20560%20instructional%0Atasks%20in%208%20domains%20related%20to%20our%20daily%20life.%20Specifically%2C%20we%20annotate%20each%0Ainstructional%20task%20with%20a%20guideline%2C%20representing%20a%20common%20pattern%20shared%20by%0Aall%20task-related%20videos.%20On%20this%20basis%2C%20we%20annotate%20systematic%20specific%20steps%2C%0Aincluding%20their%20associated%20guideline%20steps%2C%20specific%20step%20descriptions%20and%0Atimestamps.%20Our%20proposed%20benchmark%20consists%20of%20three%20sub-tasks%20to%20evaluate%0Acomprehension%20ability%20of%20models%3A%20%281%29%20Step%20Captioning%3A%20models%20have%20to%20generate%0Acaptions%20for%20specific%20steps%20from%20videos.%20%282%29%20Guideline%20Summarization%3A%20models%0Ahave%20to%20mine%20the%20common%20pattern%20in%20task-related%20videos%20and%20summarize%20a%0Aguideline%20from%20them.%20%283%29%20Guideline-Guided%20Captioning%3A%20models%20have%20to%20generate%0Acaptions%20for%20specific%20steps%20under%20the%20guide%20of%20guideline.%20We%20evaluate%20plenty%20of%0Afoundation%20models%20with%20GUIDE%20and%20perform%20in-depth%20analysis.%20Given%20the%20diversity%0Aand%20practicality%20of%20GUIDE%2C%20we%20believe%20that%20it%20can%20be%20used%20as%20a%20better%20benchmark%0Afor%20instructional%20video%20comprehension.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18227v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGUIDE%253A%2520A%2520Guideline-Guided%2520Dataset%2520for%2520Instructional%2520Video%2520Comprehension%26entry.906535625%3DJiafeng%2520Liang%2520and%2520Shixin%2520Jiang%2520and%2520Zekun%2520Wang%2520and%2520Haojie%2520Pan%2520and%2520Zerui%2520Chen%2520and%2520Zheng%2520Chu%2520and%2520Ming%2520Liu%2520and%2520Ruiji%2520Fu%2520and%2520Zhongyuan%2520Wang%2520and%2520Bing%2520Qin%26entry.1292438233%3D%2520%2520There%2520are%2520substantial%2520instructional%2520videos%2520on%2520the%2520Internet%252C%2520which%2520provide%2520us%250Atutorials%2520for%2520completing%2520various%2520tasks.%2520Existing%2520instructional%2520video%2520datasets%250Aonly%2520focus%2520on%2520specific%2520steps%2520at%2520the%2520video%2520level%252C%2520lacking%2520experiential%250Aguidelines%2520at%2520the%2520task%2520level%252C%2520which%2520can%2520lead%2520to%2520beginners%2520struggling%2520to%2520learn%250Anew%2520tasks%2520due%2520to%2520the%2520lack%2520of%2520relevant%2520experience.%2520Moreover%252C%2520the%2520specific%2520steps%250Awithout%2520guidelines%2520are%2520trivial%2520and%2520unsystematic%252C%2520making%2520it%2520difficult%2520to%2520provide%250Aa%2520clear%2520tutorial.%2520To%2520address%2520these%2520problems%252C%2520we%2520present%2520the%2520GUIDE%250A%2528Guideline-Guided%2529%2520dataset%252C%2520which%2520contains%25203.5K%2520videos%2520of%2520560%2520instructional%250Atasks%2520in%25208%2520domains%2520related%2520to%2520our%2520daily%2520life.%2520Specifically%252C%2520we%2520annotate%2520each%250Ainstructional%2520task%2520with%2520a%2520guideline%252C%2520representing%2520a%2520common%2520pattern%2520shared%2520by%250Aall%2520task-related%2520videos.%2520On%2520this%2520basis%252C%2520we%2520annotate%2520systematic%2520specific%2520steps%252C%250Aincluding%2520their%2520associated%2520guideline%2520steps%252C%2520specific%2520step%2520descriptions%2520and%250Atimestamps.%2520Our%2520proposed%2520benchmark%2520consists%2520of%2520three%2520sub-tasks%2520to%2520evaluate%250Acomprehension%2520ability%2520of%2520models%253A%2520%25281%2529%2520Step%2520Captioning%253A%2520models%2520have%2520to%2520generate%250Acaptions%2520for%2520specific%2520steps%2520from%2520videos.%2520%25282%2529%2520Guideline%2520Summarization%253A%2520models%250Ahave%2520to%2520mine%2520the%2520common%2520pattern%2520in%2520task-related%2520videos%2520and%2520summarize%2520a%250Aguideline%2520from%2520them.%2520%25283%2529%2520Guideline-Guided%2520Captioning%253A%2520models%2520have%2520to%2520generate%250Acaptions%2520for%2520specific%2520steps%2520under%2520the%2520guide%2520of%2520guideline.%2520We%2520evaluate%2520plenty%2520of%250Afoundation%2520models%2520with%2520GUIDE%2520and%2520perform%2520in-depth%2520analysis.%2520Given%2520the%2520diversity%250Aand%2520practicality%2520of%2520GUIDE%252C%2520we%2520believe%2520that%2520it%2520can%2520be%2520used%2520as%2520a%2520better%2520benchmark%250Afor%2520instructional%2520video%2520comprehension.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18227v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GUIDE%3A%20A%20Guideline-Guided%20Dataset%20for%20Instructional%20Video%20Comprehension&entry.906535625=Jiafeng%20Liang%20and%20Shixin%20Jiang%20and%20Zekun%20Wang%20and%20Haojie%20Pan%20and%20Zerui%20Chen%20and%20Zheng%20Chu%20and%20Ming%20Liu%20and%20Ruiji%20Fu%20and%20Zhongyuan%20Wang%20and%20Bing%20Qin&entry.1292438233=%20%20There%20are%20substantial%20instructional%20videos%20on%20the%20Internet%2C%20which%20provide%20us%0Atutorials%20for%20completing%20various%20tasks.%20Existing%20instructional%20video%20datasets%0Aonly%20focus%20on%20specific%20steps%20at%20the%20video%20level%2C%20lacking%20experiential%0Aguidelines%20at%20the%20task%20level%2C%20which%20can%20lead%20to%20beginners%20struggling%20to%20learn%0Anew%20tasks%20due%20to%20the%20lack%20of%20relevant%20experience.%20Moreover%2C%20the%20specific%20steps%0Awithout%20guidelines%20are%20trivial%20and%20unsystematic%2C%20making%20it%20difficult%20to%20provide%0Aa%20clear%20tutorial.%20To%20address%20these%20problems%2C%20we%20present%20the%20GUIDE%0A%28Guideline-Guided%29%20dataset%2C%20which%20contains%203.5K%20videos%20of%20560%20instructional%0Atasks%20in%208%20domains%20related%20to%20our%20daily%20life.%20Specifically%2C%20we%20annotate%20each%0Ainstructional%20task%20with%20a%20guideline%2C%20representing%20a%20common%20pattern%20shared%20by%0Aall%20task-related%20videos.%20On%20this%20basis%2C%20we%20annotate%20systematic%20specific%20steps%2C%0Aincluding%20their%20associated%20guideline%20steps%2C%20specific%20step%20descriptions%20and%0Atimestamps.%20Our%20proposed%20benchmark%20consists%20of%20three%20sub-tasks%20to%20evaluate%0Acomprehension%20ability%20of%20models%3A%20%281%29%20Step%20Captioning%3A%20models%20have%20to%20generate%0Acaptions%20for%20specific%20steps%20from%20videos.%20%282%29%20Guideline%20Summarization%3A%20models%0Ahave%20to%20mine%20the%20common%20pattern%20in%20task-related%20videos%20and%20summarize%20a%0Aguideline%20from%20them.%20%283%29%20Guideline-Guided%20Captioning%3A%20models%20have%20to%20generate%0Acaptions%20for%20specific%20steps%20under%20the%20guide%20of%20guideline.%20We%20evaluate%20plenty%20of%0Afoundation%20models%20with%20GUIDE%20and%20perform%20in-depth%20analysis.%20Given%20the%20diversity%0Aand%20practicality%20of%20GUIDE%2C%20we%20believe%20that%20it%20can%20be%20used%20as%20a%20better%20benchmark%0Afor%20instructional%20video%20comprehension.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18227v1&entry.124074799=Read"},
{"title": "Local Linear Recovery Guarantee of Deep Neural Networks at\n  Overparameterization", "author": "Yaoyu Zhang and Leyang Zhang and Zhongwang Zhang and Zhiwei Bai", "abstract": "  Determining whether deep neural network (DNN) models can reliably recover\ntarget functions at overparameterization is a critical yet complex issue in the\ntheory of deep learning. To advance understanding in this area, we introduce a\nconcept we term \"local linear recovery\" (LLR), a weaker form of target function\nrecovery that renders the problem more amenable to theoretical analysis. In the\nsense of LLR, we prove that functions expressible by narrower DNNs are\nguaranteed to be recoverable from fewer samples than model parameters.\nSpecifically, we establish upper limits on the optimistic sample sizes, defined\nas the smallest sample size necessary to guarantee LLR, for functions in the\nspace of a given DNN. Furthermore, we prove that these upper bounds are\nachieved in the case of two-layer tanh neural networks. Our research lays a\nsolid groundwork for future investigations into the recovery capabilities of\nDNNs in overparameterized scenarios.\n", "link": "http://arxiv.org/abs/2406.18035v1", "date": "2024-06-26", "relevancy": 2.4114, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5029}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4731}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4709}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Local%20Linear%20Recovery%20Guarantee%20of%20Deep%20Neural%20Networks%20at%0A%20%20Overparameterization&body=Title%3A%20Local%20Linear%20Recovery%20Guarantee%20of%20Deep%20Neural%20Networks%20at%0A%20%20Overparameterization%0AAuthor%3A%20Yaoyu%20Zhang%20and%20Leyang%20Zhang%20and%20Zhongwang%20Zhang%20and%20Zhiwei%20Bai%0AAbstract%3A%20%20%20Determining%20whether%20deep%20neural%20network%20%28DNN%29%20models%20can%20reliably%20recover%0Atarget%20functions%20at%20overparameterization%20is%20a%20critical%20yet%20complex%20issue%20in%20the%0Atheory%20of%20deep%20learning.%20To%20advance%20understanding%20in%20this%20area%2C%20we%20introduce%20a%0Aconcept%20we%20term%20%22local%20linear%20recovery%22%20%28LLR%29%2C%20a%20weaker%20form%20of%20target%20function%0Arecovery%20that%20renders%20the%20problem%20more%20amenable%20to%20theoretical%20analysis.%20In%20the%0Asense%20of%20LLR%2C%20we%20prove%20that%20functions%20expressible%20by%20narrower%20DNNs%20are%0Aguaranteed%20to%20be%20recoverable%20from%20fewer%20samples%20than%20model%20parameters.%0ASpecifically%2C%20we%20establish%20upper%20limits%20on%20the%20optimistic%20sample%20sizes%2C%20defined%0Aas%20the%20smallest%20sample%20size%20necessary%20to%20guarantee%20LLR%2C%20for%20functions%20in%20the%0Aspace%20of%20a%20given%20DNN.%20Furthermore%2C%20we%20prove%20that%20these%20upper%20bounds%20are%0Aachieved%20in%20the%20case%20of%20two-layer%20tanh%20neural%20networks.%20Our%20research%20lays%20a%0Asolid%20groundwork%20for%20future%20investigations%20into%20the%20recovery%20capabilities%20of%0ADNNs%20in%20overparameterized%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18035v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocal%2520Linear%2520Recovery%2520Guarantee%2520of%2520Deep%2520Neural%2520Networks%2520at%250A%2520%2520Overparameterization%26entry.906535625%3DYaoyu%2520Zhang%2520and%2520Leyang%2520Zhang%2520and%2520Zhongwang%2520Zhang%2520and%2520Zhiwei%2520Bai%26entry.1292438233%3D%2520%2520Determining%2520whether%2520deep%2520neural%2520network%2520%2528DNN%2529%2520models%2520can%2520reliably%2520recover%250Atarget%2520functions%2520at%2520overparameterization%2520is%2520a%2520critical%2520yet%2520complex%2520issue%2520in%2520the%250Atheory%2520of%2520deep%2520learning.%2520To%2520advance%2520understanding%2520in%2520this%2520area%252C%2520we%2520introduce%2520a%250Aconcept%2520we%2520term%2520%2522local%2520linear%2520recovery%2522%2520%2528LLR%2529%252C%2520a%2520weaker%2520form%2520of%2520target%2520function%250Arecovery%2520that%2520renders%2520the%2520problem%2520more%2520amenable%2520to%2520theoretical%2520analysis.%2520In%2520the%250Asense%2520of%2520LLR%252C%2520we%2520prove%2520that%2520functions%2520expressible%2520by%2520narrower%2520DNNs%2520are%250Aguaranteed%2520to%2520be%2520recoverable%2520from%2520fewer%2520samples%2520than%2520model%2520parameters.%250ASpecifically%252C%2520we%2520establish%2520upper%2520limits%2520on%2520the%2520optimistic%2520sample%2520sizes%252C%2520defined%250Aas%2520the%2520smallest%2520sample%2520size%2520necessary%2520to%2520guarantee%2520LLR%252C%2520for%2520functions%2520in%2520the%250Aspace%2520of%2520a%2520given%2520DNN.%2520Furthermore%252C%2520we%2520prove%2520that%2520these%2520upper%2520bounds%2520are%250Aachieved%2520in%2520the%2520case%2520of%2520two-layer%2520tanh%2520neural%2520networks.%2520Our%2520research%2520lays%2520a%250Asolid%2520groundwork%2520for%2520future%2520investigations%2520into%2520the%2520recovery%2520capabilities%2520of%250ADNNs%2520in%2520overparameterized%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18035v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local%20Linear%20Recovery%20Guarantee%20of%20Deep%20Neural%20Networks%20at%0A%20%20Overparameterization&entry.906535625=Yaoyu%20Zhang%20and%20Leyang%20Zhang%20and%20Zhongwang%20Zhang%20and%20Zhiwei%20Bai&entry.1292438233=%20%20Determining%20whether%20deep%20neural%20network%20%28DNN%29%20models%20can%20reliably%20recover%0Atarget%20functions%20at%20overparameterization%20is%20a%20critical%20yet%20complex%20issue%20in%20the%0Atheory%20of%20deep%20learning.%20To%20advance%20understanding%20in%20this%20area%2C%20we%20introduce%20a%0Aconcept%20we%20term%20%22local%20linear%20recovery%22%20%28LLR%29%2C%20a%20weaker%20form%20of%20target%20function%0Arecovery%20that%20renders%20the%20problem%20more%20amenable%20to%20theoretical%20analysis.%20In%20the%0Asense%20of%20LLR%2C%20we%20prove%20that%20functions%20expressible%20by%20narrower%20DNNs%20are%0Aguaranteed%20to%20be%20recoverable%20from%20fewer%20samples%20than%20model%20parameters.%0ASpecifically%2C%20we%20establish%20upper%20limits%20on%20the%20optimistic%20sample%20sizes%2C%20defined%0Aas%20the%20smallest%20sample%20size%20necessary%20to%20guarantee%20LLR%2C%20for%20functions%20in%20the%0Aspace%20of%20a%20given%20DNN.%20Furthermore%2C%20we%20prove%20that%20these%20upper%20bounds%20are%0Aachieved%20in%20the%20case%20of%20two-layer%20tanh%20neural%20networks.%20Our%20research%20lays%20a%0Asolid%20groundwork%20for%20future%20investigations%20into%20the%20recovery%20capabilities%20of%0ADNNs%20in%20overparameterized%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18035v1&entry.124074799=Read"},
{"title": "Benchmarking General-Purpose In-Context Learning", "author": "Fan Wang and Chuan Lin and Yang Cao and Yu Kang", "abstract": "  In-context learning (ICL) empowers generative models to address new tasks\neffectively and efficiently on the fly, without relying on any artificially\ncrafted optimization techniques. In this paper, we study extending ICL to\naddress a broader range of tasks with an extended learning horizon and higher\nimprovement potential, namely General-Purpose In-Context Learning (GPICL). To\nthis end, we introduce two lightweight benchmarks specifically crafted to train\nand evaluate GPICL functionalities. Each benchmark encompasses a vast number of\ntasks characterized by significant task variance, facilitating meta-training\nthat minimizes inductive bias. These tasks are also crafted to promote\nlong-horizon in-context learning through continuous generation and interaction.\nThese characteristics necessitate the models to leverage contexts and history\ninteractions to enhance their capabilities, across domains such as language\nmodeling, decision-making, and world modeling. Our experiments on the baseline\nmodels demonstrate that meta-training with minimal inductive bias and ICL from\nthe ground up is feasible across all the domains we've discussed. Additionally,\nour findings indicate that the scale of parameters alone may not be crucial for\nICL or GPICL, suggesting alternative approaches such as increasing the scale of\ncontexts and memory states.\n", "link": "http://arxiv.org/abs/2405.17234v5", "date": "2024-06-26", "relevancy": 2.4047, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5216}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4625}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20General-Purpose%20In-Context%20Learning&body=Title%3A%20Benchmarking%20General-Purpose%20In-Context%20Learning%0AAuthor%3A%20Fan%20Wang%20and%20Chuan%20Lin%20and%20Yang%20Cao%20and%20Yu%20Kang%0AAbstract%3A%20%20%20In-context%20learning%20%28ICL%29%20empowers%20generative%20models%20to%20address%20new%20tasks%0Aeffectively%20and%20efficiently%20on%20the%20fly%2C%20without%20relying%20on%20any%20artificially%0Acrafted%20optimization%20techniques.%20In%20this%20paper%2C%20we%20study%20extending%20ICL%20to%0Aaddress%20a%20broader%20range%20of%20tasks%20with%20an%20extended%20learning%20horizon%20and%20higher%0Aimprovement%20potential%2C%20namely%20General-Purpose%20In-Context%20Learning%20%28GPICL%29.%20To%0Athis%20end%2C%20we%20introduce%20two%20lightweight%20benchmarks%20specifically%20crafted%20to%20train%0Aand%20evaluate%20GPICL%20functionalities.%20Each%20benchmark%20encompasses%20a%20vast%20number%20of%0Atasks%20characterized%20by%20significant%20task%20variance%2C%20facilitating%20meta-training%0Athat%20minimizes%20inductive%20bias.%20These%20tasks%20are%20also%20crafted%20to%20promote%0Along-horizon%20in-context%20learning%20through%20continuous%20generation%20and%20interaction.%0AThese%20characteristics%20necessitate%20the%20models%20to%20leverage%20contexts%20and%20history%0Ainteractions%20to%20enhance%20their%20capabilities%2C%20across%20domains%20such%20as%20language%0Amodeling%2C%20decision-making%2C%20and%20world%20modeling.%20Our%20experiments%20on%20the%20baseline%0Amodels%20demonstrate%20that%20meta-training%20with%20minimal%20inductive%20bias%20and%20ICL%20from%0Athe%20ground%20up%20is%20feasible%20across%20all%20the%20domains%20we%27ve%20discussed.%20Additionally%2C%0Aour%20findings%20indicate%20that%20the%20scale%20of%20parameters%20alone%20may%20not%20be%20crucial%20for%0AICL%20or%20GPICL%2C%20suggesting%20alternative%20approaches%20such%20as%20increasing%20the%20scale%20of%0Acontexts%20and%20memory%20states.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17234v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520General-Purpose%2520In-Context%2520Learning%26entry.906535625%3DFan%2520Wang%2520and%2520Chuan%2520Lin%2520and%2520Yang%2520Cao%2520and%2520Yu%2520Kang%26entry.1292438233%3D%2520%2520In-context%2520learning%2520%2528ICL%2529%2520empowers%2520generative%2520models%2520to%2520address%2520new%2520tasks%250Aeffectively%2520and%2520efficiently%2520on%2520the%2520fly%252C%2520without%2520relying%2520on%2520any%2520artificially%250Acrafted%2520optimization%2520techniques.%2520In%2520this%2520paper%252C%2520we%2520study%2520extending%2520ICL%2520to%250Aaddress%2520a%2520broader%2520range%2520of%2520tasks%2520with%2520an%2520extended%2520learning%2520horizon%2520and%2520higher%250Aimprovement%2520potential%252C%2520namely%2520General-Purpose%2520In-Context%2520Learning%2520%2528GPICL%2529.%2520To%250Athis%2520end%252C%2520we%2520introduce%2520two%2520lightweight%2520benchmarks%2520specifically%2520crafted%2520to%2520train%250Aand%2520evaluate%2520GPICL%2520functionalities.%2520Each%2520benchmark%2520encompasses%2520a%2520vast%2520number%2520of%250Atasks%2520characterized%2520by%2520significant%2520task%2520variance%252C%2520facilitating%2520meta-training%250Athat%2520minimizes%2520inductive%2520bias.%2520These%2520tasks%2520are%2520also%2520crafted%2520to%2520promote%250Along-horizon%2520in-context%2520learning%2520through%2520continuous%2520generation%2520and%2520interaction.%250AThese%2520characteristics%2520necessitate%2520the%2520models%2520to%2520leverage%2520contexts%2520and%2520history%250Ainteractions%2520to%2520enhance%2520their%2520capabilities%252C%2520across%2520domains%2520such%2520as%2520language%250Amodeling%252C%2520decision-making%252C%2520and%2520world%2520modeling.%2520Our%2520experiments%2520on%2520the%2520baseline%250Amodels%2520demonstrate%2520that%2520meta-training%2520with%2520minimal%2520inductive%2520bias%2520and%2520ICL%2520from%250Athe%2520ground%2520up%2520is%2520feasible%2520across%2520all%2520the%2520domains%2520we%2527ve%2520discussed.%2520Additionally%252C%250Aour%2520findings%2520indicate%2520that%2520the%2520scale%2520of%2520parameters%2520alone%2520may%2520not%2520be%2520crucial%2520for%250AICL%2520or%2520GPICL%252C%2520suggesting%2520alternative%2520approaches%2520such%2520as%2520increasing%2520the%2520scale%2520of%250Acontexts%2520and%2520memory%2520states.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17234v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20General-Purpose%20In-Context%20Learning&entry.906535625=Fan%20Wang%20and%20Chuan%20Lin%20and%20Yang%20Cao%20and%20Yu%20Kang&entry.1292438233=%20%20In-context%20learning%20%28ICL%29%20empowers%20generative%20models%20to%20address%20new%20tasks%0Aeffectively%20and%20efficiently%20on%20the%20fly%2C%20without%20relying%20on%20any%20artificially%0Acrafted%20optimization%20techniques.%20In%20this%20paper%2C%20we%20study%20extending%20ICL%20to%0Aaddress%20a%20broader%20range%20of%20tasks%20with%20an%20extended%20learning%20horizon%20and%20higher%0Aimprovement%20potential%2C%20namely%20General-Purpose%20In-Context%20Learning%20%28GPICL%29.%20To%0Athis%20end%2C%20we%20introduce%20two%20lightweight%20benchmarks%20specifically%20crafted%20to%20train%0Aand%20evaluate%20GPICL%20functionalities.%20Each%20benchmark%20encompasses%20a%20vast%20number%20of%0Atasks%20characterized%20by%20significant%20task%20variance%2C%20facilitating%20meta-training%0Athat%20minimizes%20inductive%20bias.%20These%20tasks%20are%20also%20crafted%20to%20promote%0Along-horizon%20in-context%20learning%20through%20continuous%20generation%20and%20interaction.%0AThese%20characteristics%20necessitate%20the%20models%20to%20leverage%20contexts%20and%20history%0Ainteractions%20to%20enhance%20their%20capabilities%2C%20across%20domains%20such%20as%20language%0Amodeling%2C%20decision-making%2C%20and%20world%20modeling.%20Our%20experiments%20on%20the%20baseline%0Amodels%20demonstrate%20that%20meta-training%20with%20minimal%20inductive%20bias%20and%20ICL%20from%0Athe%20ground%20up%20is%20feasible%20across%20all%20the%20domains%20we%27ve%20discussed.%20Additionally%2C%0Aour%20findings%20indicate%20that%20the%20scale%20of%20parameters%20alone%20may%20not%20be%20crucial%20for%0AICL%20or%20GPICL%2C%20suggesting%20alternative%20approaches%20such%20as%20increasing%20the%20scale%20of%0Acontexts%20and%20memory%20states.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17234v5&entry.124074799=Read"},
{"title": "Transformer-based de novo peptide sequencing for data-independent\n  acquisition mass spectrometry", "author": "Shiva Ebrahimi and Xuan Guo", "abstract": "  Tandem mass spectrometry (MS/MS) stands as the predominant high-throughput\ntechnique for comprehensively analyzing protein content within biological\nsamples. This methodology is a cornerstone driving the advancement of\nproteomics. In recent years, substantial strides have been made in\nData-Independent Acquisition (DIA) strategies, facilitating impartial and\nnon-targeted fragmentation of precursor ions. The DIA-generated MS/MS spectra\npresent a formidable obstacle due to their inherent high multiplexing nature.\nEach spectrum encapsulates fragmented product ions originating from multiple\nprecursor peptides. This intricacy poses a particularly acute challenge in de\nnovo peptide/protein sequencing, where current methods are ill-equipped to\naddress the multiplexing conundrum. In this paper, we introduce DiaTrans, a\ndeep-learning model based on transformer architecture. It deciphers peptide\nsequences from DIA mass spectrometry data. Our results show significant\nimprovements over existing STOA methods, including DeepNovo-DIA and PepNet.\nCasanovo-DIA enhances precision by 15.14% to 34.8%, recall by 11.62% to 31.94%\nat the amino acid level, and boosts precision by 59% to 81.36% at the peptide\nlevel. Integrating DIA data and our DiaTrans model holds considerable promise\nto uncover novel peptides and more comprehensive profiling of biological\nsamples. Casanovo-DIA is freely available under the GNU GPL license at\nhttps://github.com/Biocomputing-Research-Group/DiaTrans.\n", "link": "http://arxiv.org/abs/2402.11363v3", "date": "2024-06-26", "relevancy": 2.3993, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.485}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.485}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4696}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transformer-based%20de%20novo%20peptide%20sequencing%20for%20data-independent%0A%20%20acquisition%20mass%20spectrometry&body=Title%3A%20Transformer-based%20de%20novo%20peptide%20sequencing%20for%20data-independent%0A%20%20acquisition%20mass%20spectrometry%0AAuthor%3A%20Shiva%20Ebrahimi%20and%20Xuan%20Guo%0AAbstract%3A%20%20%20Tandem%20mass%20spectrometry%20%28MS/MS%29%20stands%20as%20the%20predominant%20high-throughput%0Atechnique%20for%20comprehensively%20analyzing%20protein%20content%20within%20biological%0Asamples.%20This%20methodology%20is%20a%20cornerstone%20driving%20the%20advancement%20of%0Aproteomics.%20In%20recent%20years%2C%20substantial%20strides%20have%20been%20made%20in%0AData-Independent%20Acquisition%20%28DIA%29%20strategies%2C%20facilitating%20impartial%20and%0Anon-targeted%20fragmentation%20of%20precursor%20ions.%20The%20DIA-generated%20MS/MS%20spectra%0Apresent%20a%20formidable%20obstacle%20due%20to%20their%20inherent%20high%20multiplexing%20nature.%0AEach%20spectrum%20encapsulates%20fragmented%20product%20ions%20originating%20from%20multiple%0Aprecursor%20peptides.%20This%20intricacy%20poses%20a%20particularly%20acute%20challenge%20in%20de%0Anovo%20peptide/protein%20sequencing%2C%20where%20current%20methods%20are%20ill-equipped%20to%0Aaddress%20the%20multiplexing%20conundrum.%20In%20this%20paper%2C%20we%20introduce%20DiaTrans%2C%20a%0Adeep-learning%20model%20based%20on%20transformer%20architecture.%20It%20deciphers%20peptide%0Asequences%20from%20DIA%20mass%20spectrometry%20data.%20Our%20results%20show%20significant%0Aimprovements%20over%20existing%20STOA%20methods%2C%20including%20DeepNovo-DIA%20and%20PepNet.%0ACasanovo-DIA%20enhances%20precision%20by%2015.14%25%20to%2034.8%25%2C%20recall%20by%2011.62%25%20to%2031.94%25%0Aat%20the%20amino%20acid%20level%2C%20and%20boosts%20precision%20by%2059%25%20to%2081.36%25%20at%20the%20peptide%0Alevel.%20Integrating%20DIA%20data%20and%20our%20DiaTrans%20model%20holds%20considerable%20promise%0Ato%20uncover%20novel%20peptides%20and%20more%20comprehensive%20profiling%20of%20biological%0Asamples.%20Casanovo-DIA%20is%20freely%20available%20under%20the%20GNU%20GPL%20license%20at%0Ahttps%3A//github.com/Biocomputing-Research-Group/DiaTrans.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.11363v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransformer-based%2520de%2520novo%2520peptide%2520sequencing%2520for%2520data-independent%250A%2520%2520acquisition%2520mass%2520spectrometry%26entry.906535625%3DShiva%2520Ebrahimi%2520and%2520Xuan%2520Guo%26entry.1292438233%3D%2520%2520Tandem%2520mass%2520spectrometry%2520%2528MS/MS%2529%2520stands%2520as%2520the%2520predominant%2520high-throughput%250Atechnique%2520for%2520comprehensively%2520analyzing%2520protein%2520content%2520within%2520biological%250Asamples.%2520This%2520methodology%2520is%2520a%2520cornerstone%2520driving%2520the%2520advancement%2520of%250Aproteomics.%2520In%2520recent%2520years%252C%2520substantial%2520strides%2520have%2520been%2520made%2520in%250AData-Independent%2520Acquisition%2520%2528DIA%2529%2520strategies%252C%2520facilitating%2520impartial%2520and%250Anon-targeted%2520fragmentation%2520of%2520precursor%2520ions.%2520The%2520DIA-generated%2520MS/MS%2520spectra%250Apresent%2520a%2520formidable%2520obstacle%2520due%2520to%2520their%2520inherent%2520high%2520multiplexing%2520nature.%250AEach%2520spectrum%2520encapsulates%2520fragmented%2520product%2520ions%2520originating%2520from%2520multiple%250Aprecursor%2520peptides.%2520This%2520intricacy%2520poses%2520a%2520particularly%2520acute%2520challenge%2520in%2520de%250Anovo%2520peptide/protein%2520sequencing%252C%2520where%2520current%2520methods%2520are%2520ill-equipped%2520to%250Aaddress%2520the%2520multiplexing%2520conundrum.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520DiaTrans%252C%2520a%250Adeep-learning%2520model%2520based%2520on%2520transformer%2520architecture.%2520It%2520deciphers%2520peptide%250Asequences%2520from%2520DIA%2520mass%2520spectrometry%2520data.%2520Our%2520results%2520show%2520significant%250Aimprovements%2520over%2520existing%2520STOA%2520methods%252C%2520including%2520DeepNovo-DIA%2520and%2520PepNet.%250ACasanovo-DIA%2520enhances%2520precision%2520by%252015.14%2525%2520to%252034.8%2525%252C%2520recall%2520by%252011.62%2525%2520to%252031.94%2525%250Aat%2520the%2520amino%2520acid%2520level%252C%2520and%2520boosts%2520precision%2520by%252059%2525%2520to%252081.36%2525%2520at%2520the%2520peptide%250Alevel.%2520Integrating%2520DIA%2520data%2520and%2520our%2520DiaTrans%2520model%2520holds%2520considerable%2520promise%250Ato%2520uncover%2520novel%2520peptides%2520and%2520more%2520comprehensive%2520profiling%2520of%2520biological%250Asamples.%2520Casanovo-DIA%2520is%2520freely%2520available%2520under%2520the%2520GNU%2520GPL%2520license%2520at%250Ahttps%253A//github.com/Biocomputing-Research-Group/DiaTrans.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.11363v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformer-based%20de%20novo%20peptide%20sequencing%20for%20data-independent%0A%20%20acquisition%20mass%20spectrometry&entry.906535625=Shiva%20Ebrahimi%20and%20Xuan%20Guo&entry.1292438233=%20%20Tandem%20mass%20spectrometry%20%28MS/MS%29%20stands%20as%20the%20predominant%20high-throughput%0Atechnique%20for%20comprehensively%20analyzing%20protein%20content%20within%20biological%0Asamples.%20This%20methodology%20is%20a%20cornerstone%20driving%20the%20advancement%20of%0Aproteomics.%20In%20recent%20years%2C%20substantial%20strides%20have%20been%20made%20in%0AData-Independent%20Acquisition%20%28DIA%29%20strategies%2C%20facilitating%20impartial%20and%0Anon-targeted%20fragmentation%20of%20precursor%20ions.%20The%20DIA-generated%20MS/MS%20spectra%0Apresent%20a%20formidable%20obstacle%20due%20to%20their%20inherent%20high%20multiplexing%20nature.%0AEach%20spectrum%20encapsulates%20fragmented%20product%20ions%20originating%20from%20multiple%0Aprecursor%20peptides.%20This%20intricacy%20poses%20a%20particularly%20acute%20challenge%20in%20de%0Anovo%20peptide/protein%20sequencing%2C%20where%20current%20methods%20are%20ill-equipped%20to%0Aaddress%20the%20multiplexing%20conundrum.%20In%20this%20paper%2C%20we%20introduce%20DiaTrans%2C%20a%0Adeep-learning%20model%20based%20on%20transformer%20architecture.%20It%20deciphers%20peptide%0Asequences%20from%20DIA%20mass%20spectrometry%20data.%20Our%20results%20show%20significant%0Aimprovements%20over%20existing%20STOA%20methods%2C%20including%20DeepNovo-DIA%20and%20PepNet.%0ACasanovo-DIA%20enhances%20precision%20by%2015.14%25%20to%2034.8%25%2C%20recall%20by%2011.62%25%20to%2031.94%25%0Aat%20the%20amino%20acid%20level%2C%20and%20boosts%20precision%20by%2059%25%20to%2081.36%25%20at%20the%20peptide%0Alevel.%20Integrating%20DIA%20data%20and%20our%20DiaTrans%20model%20holds%20considerable%20promise%0Ato%20uncover%20novel%20peptides%20and%20more%20comprehensive%20profiling%20of%20biological%0Asamples.%20Casanovo-DIA%20is%20freely%20available%20under%20the%20GNU%20GPL%20license%20at%0Ahttps%3A//github.com/Biocomputing-Research-Group/DiaTrans.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.11363v3&entry.124074799=Read"},
{"title": "APIGen: Automated Pipeline for Generating Verifiable and Diverse\n  Function-Calling Datasets", "author": "Zuxin Liu and Thai Hoang and Jianguo Zhang and Ming Zhu and Tian Lan and Shirley Kokane and Juntao Tan and Weiran Yao and Zhiwei Liu and Yihao Feng and Rithesh Murthy and Liangwei Yang and Silvio Savarese and Juan Carlos Niebles and Huan Wang and Shelby Heinecke and Caiming Xiong", "abstract": "  The advancement of function-calling agent models requires diverse, reliable,\nand high-quality datasets. This paper presents APIGen, an automated data\ngeneration pipeline designed to synthesize verifiable high-quality datasets for\nfunction-calling applications. We leverage APIGen and collect 3,673 executable\nAPIs across 21 different categories to generate diverse function-calling\ndatasets in a scalable and structured manner. Each data in our dataset is\nverified through three hierarchical stages: format checking, actual function\nexecutions, and semantic verification, ensuring its reliability and\ncorrectness. We demonstrate that models trained with our curated datasets, even\nwith only 7B parameters, can achieve state-of-the-art performance on the\nBerkeley Function-Calling Benchmark, outperforming multiple GPT-4 models.\nMoreover, our 1B model achieves exceptional performance, surpassing\nGPT-3.5-Turbo and Claude-3 Haiku. We release a dataset containing 60,000\nhigh-quality entries, aiming to advance the field of function-calling agent\ndomains. The dataset is available on Huggingface:\nhttps://huggingface.co/datasets/Salesforce/xlam-function-calling-60k and the\nproject homepage: https://apigen-pipeline.github.io/\n", "link": "http://arxiv.org/abs/2406.18518v1", "date": "2024-06-26", "relevancy": 2.3937, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5094}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4824}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20APIGen%3A%20Automated%20Pipeline%20for%20Generating%20Verifiable%20and%20Diverse%0A%20%20Function-Calling%20Datasets&body=Title%3A%20APIGen%3A%20Automated%20Pipeline%20for%20Generating%20Verifiable%20and%20Diverse%0A%20%20Function-Calling%20Datasets%0AAuthor%3A%20Zuxin%20Liu%20and%20Thai%20Hoang%20and%20Jianguo%20Zhang%20and%20Ming%20Zhu%20and%20Tian%20Lan%20and%20Shirley%20Kokane%20and%20Juntao%20Tan%20and%20Weiran%20Yao%20and%20Zhiwei%20Liu%20and%20Yihao%20Feng%20and%20Rithesh%20Murthy%20and%20Liangwei%20Yang%20and%20Silvio%20Savarese%20and%20Juan%20Carlos%20Niebles%20and%20Huan%20Wang%20and%20Shelby%20Heinecke%20and%20Caiming%20Xiong%0AAbstract%3A%20%20%20The%20advancement%20of%20function-calling%20agent%20models%20requires%20diverse%2C%20reliable%2C%0Aand%20high-quality%20datasets.%20This%20paper%20presents%20APIGen%2C%20an%20automated%20data%0Ageneration%20pipeline%20designed%20to%20synthesize%20verifiable%20high-quality%20datasets%20for%0Afunction-calling%20applications.%20We%20leverage%20APIGen%20and%20collect%203%2C673%20executable%0AAPIs%20across%2021%20different%20categories%20to%20generate%20diverse%20function-calling%0Adatasets%20in%20a%20scalable%20and%20structured%20manner.%20Each%20data%20in%20our%20dataset%20is%0Averified%20through%20three%20hierarchical%20stages%3A%20format%20checking%2C%20actual%20function%0Aexecutions%2C%20and%20semantic%20verification%2C%20ensuring%20its%20reliability%20and%0Acorrectness.%20We%20demonstrate%20that%20models%20trained%20with%20our%20curated%20datasets%2C%20even%0Awith%20only%207B%20parameters%2C%20can%20achieve%20state-of-the-art%20performance%20on%20the%0ABerkeley%20Function-Calling%20Benchmark%2C%20outperforming%20multiple%20GPT-4%20models.%0AMoreover%2C%20our%201B%20model%20achieves%20exceptional%20performance%2C%20surpassing%0AGPT-3.5-Turbo%20and%20Claude-3%20Haiku.%20We%20release%20a%20dataset%20containing%2060%2C000%0Ahigh-quality%20entries%2C%20aiming%20to%20advance%20the%20field%20of%20function-calling%20agent%0Adomains.%20The%20dataset%20is%20available%20on%20Huggingface%3A%0Ahttps%3A//huggingface.co/datasets/Salesforce/xlam-function-calling-60k%20and%20the%0Aproject%20homepage%3A%20https%3A//apigen-pipeline.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18518v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAPIGen%253A%2520Automated%2520Pipeline%2520for%2520Generating%2520Verifiable%2520and%2520Diverse%250A%2520%2520Function-Calling%2520Datasets%26entry.906535625%3DZuxin%2520Liu%2520and%2520Thai%2520Hoang%2520and%2520Jianguo%2520Zhang%2520and%2520Ming%2520Zhu%2520and%2520Tian%2520Lan%2520and%2520Shirley%2520Kokane%2520and%2520Juntao%2520Tan%2520and%2520Weiran%2520Yao%2520and%2520Zhiwei%2520Liu%2520and%2520Yihao%2520Feng%2520and%2520Rithesh%2520Murthy%2520and%2520Liangwei%2520Yang%2520and%2520Silvio%2520Savarese%2520and%2520Juan%2520Carlos%2520Niebles%2520and%2520Huan%2520Wang%2520and%2520Shelby%2520Heinecke%2520and%2520Caiming%2520Xiong%26entry.1292438233%3D%2520%2520The%2520advancement%2520of%2520function-calling%2520agent%2520models%2520requires%2520diverse%252C%2520reliable%252C%250Aand%2520high-quality%2520datasets.%2520This%2520paper%2520presents%2520APIGen%252C%2520an%2520automated%2520data%250Ageneration%2520pipeline%2520designed%2520to%2520synthesize%2520verifiable%2520high-quality%2520datasets%2520for%250Afunction-calling%2520applications.%2520We%2520leverage%2520APIGen%2520and%2520collect%25203%252C673%2520executable%250AAPIs%2520across%252021%2520different%2520categories%2520to%2520generate%2520diverse%2520function-calling%250Adatasets%2520in%2520a%2520scalable%2520and%2520structured%2520manner.%2520Each%2520data%2520in%2520our%2520dataset%2520is%250Averified%2520through%2520three%2520hierarchical%2520stages%253A%2520format%2520checking%252C%2520actual%2520function%250Aexecutions%252C%2520and%2520semantic%2520verification%252C%2520ensuring%2520its%2520reliability%2520and%250Acorrectness.%2520We%2520demonstrate%2520that%2520models%2520trained%2520with%2520our%2520curated%2520datasets%252C%2520even%250Awith%2520only%25207B%2520parameters%252C%2520can%2520achieve%2520state-of-the-art%2520performance%2520on%2520the%250ABerkeley%2520Function-Calling%2520Benchmark%252C%2520outperforming%2520multiple%2520GPT-4%2520models.%250AMoreover%252C%2520our%25201B%2520model%2520achieves%2520exceptional%2520performance%252C%2520surpassing%250AGPT-3.5-Turbo%2520and%2520Claude-3%2520Haiku.%2520We%2520release%2520a%2520dataset%2520containing%252060%252C000%250Ahigh-quality%2520entries%252C%2520aiming%2520to%2520advance%2520the%2520field%2520of%2520function-calling%2520agent%250Adomains.%2520The%2520dataset%2520is%2520available%2520on%2520Huggingface%253A%250Ahttps%253A//huggingface.co/datasets/Salesforce/xlam-function-calling-60k%2520and%2520the%250Aproject%2520homepage%253A%2520https%253A//apigen-pipeline.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18518v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=APIGen%3A%20Automated%20Pipeline%20for%20Generating%20Verifiable%20and%20Diverse%0A%20%20Function-Calling%20Datasets&entry.906535625=Zuxin%20Liu%20and%20Thai%20Hoang%20and%20Jianguo%20Zhang%20and%20Ming%20Zhu%20and%20Tian%20Lan%20and%20Shirley%20Kokane%20and%20Juntao%20Tan%20and%20Weiran%20Yao%20and%20Zhiwei%20Liu%20and%20Yihao%20Feng%20and%20Rithesh%20Murthy%20and%20Liangwei%20Yang%20and%20Silvio%20Savarese%20and%20Juan%20Carlos%20Niebles%20and%20Huan%20Wang%20and%20Shelby%20Heinecke%20and%20Caiming%20Xiong&entry.1292438233=%20%20The%20advancement%20of%20function-calling%20agent%20models%20requires%20diverse%2C%20reliable%2C%0Aand%20high-quality%20datasets.%20This%20paper%20presents%20APIGen%2C%20an%20automated%20data%0Ageneration%20pipeline%20designed%20to%20synthesize%20verifiable%20high-quality%20datasets%20for%0Afunction-calling%20applications.%20We%20leverage%20APIGen%20and%20collect%203%2C673%20executable%0AAPIs%20across%2021%20different%20categories%20to%20generate%20diverse%20function-calling%0Adatasets%20in%20a%20scalable%20and%20structured%20manner.%20Each%20data%20in%20our%20dataset%20is%0Averified%20through%20three%20hierarchical%20stages%3A%20format%20checking%2C%20actual%20function%0Aexecutions%2C%20and%20semantic%20verification%2C%20ensuring%20its%20reliability%20and%0Acorrectness.%20We%20demonstrate%20that%20models%20trained%20with%20our%20curated%20datasets%2C%20even%0Awith%20only%207B%20parameters%2C%20can%20achieve%20state-of-the-art%20performance%20on%20the%0ABerkeley%20Function-Calling%20Benchmark%2C%20outperforming%20multiple%20GPT-4%20models.%0AMoreover%2C%20our%201B%20model%20achieves%20exceptional%20performance%2C%20surpassing%0AGPT-3.5-Turbo%20and%20Claude-3%20Haiku.%20We%20release%20a%20dataset%20containing%2060%2C000%0Ahigh-quality%20entries%2C%20aiming%20to%20advance%20the%20field%20of%20function-calling%20agent%0Adomains.%20The%20dataset%20is%20available%20on%20Huggingface%3A%0Ahttps%3A//huggingface.co/datasets/Salesforce/xlam-function-calling-60k%20and%20the%0Aproject%20homepage%3A%20https%3A//apigen-pipeline.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18518v1&entry.124074799=Read"},
{"title": "BADGE: BADminton report Generation and Evaluation with LLM", "author": "Shang-Hsuan Chiang and Lin-Wei Chao and Kuang-Da Wang and Chih-Chuan Wang and Wen-Chih Peng", "abstract": "  Badminton enjoys widespread popularity, and reports on matches generally\ninclude details such as player names, game scores, and ball types, providing\naudiences with a comprehensive view of the games. However, writing these\nreports can be a time-consuming task. This challenge led us to explore whether\na Large Language Model (LLM) could automate the generation and evaluation of\nbadminton reports. We introduce a novel framework named BADGE, designed for\nthis purpose using LLM. Our method consists of two main phases: Report\nGeneration and Report Evaluation. Initially, badminton-related data is\nprocessed by the LLM, which then generates a detailed report of the match. We\ntested different Input Data Types, In-Context Learning (ICL), and LLM, finding\nthat GPT-4 performs best when using CSV data type and the Chain of Thought\nprompting. Following report generation, the LLM evaluates and scores the\nreports to assess their quality. Our comparisons between the scores evaluated\nby GPT-4 and human judges show a tendency to prefer GPT-4 generated reports.\nSince the application of LLM in badminton reporting remains largely unexplored,\nour research serves as a foundational step for future advancements in this\narea. Moreover, our method can be extended to other sports games, thereby\nenhancing sports promotion. For more details, please refer to\nhttps://github.com/AndyChiangSH/BADGE.\n", "link": "http://arxiv.org/abs/2406.18116v1", "date": "2024-06-26", "relevancy": 2.387, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4955}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4698}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4669}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BADGE%3A%20BADminton%20report%20Generation%20and%20Evaluation%20with%20LLM&body=Title%3A%20BADGE%3A%20BADminton%20report%20Generation%20and%20Evaluation%20with%20LLM%0AAuthor%3A%20Shang-Hsuan%20Chiang%20and%20Lin-Wei%20Chao%20and%20Kuang-Da%20Wang%20and%20Chih-Chuan%20Wang%20and%20Wen-Chih%20Peng%0AAbstract%3A%20%20%20Badminton%20enjoys%20widespread%20popularity%2C%20and%20reports%20on%20matches%20generally%0Ainclude%20details%20such%20as%20player%20names%2C%20game%20scores%2C%20and%20ball%20types%2C%20providing%0Aaudiences%20with%20a%20comprehensive%20view%20of%20the%20games.%20However%2C%20writing%20these%0Areports%20can%20be%20a%20time-consuming%20task.%20This%20challenge%20led%20us%20to%20explore%20whether%0Aa%20Large%20Language%20Model%20%28LLM%29%20could%20automate%20the%20generation%20and%20evaluation%20of%0Abadminton%20reports.%20We%20introduce%20a%20novel%20framework%20named%20BADGE%2C%20designed%20for%0Athis%20purpose%20using%20LLM.%20Our%20method%20consists%20of%20two%20main%20phases%3A%20Report%0AGeneration%20and%20Report%20Evaluation.%20Initially%2C%20badminton-related%20data%20is%0Aprocessed%20by%20the%20LLM%2C%20which%20then%20generates%20a%20detailed%20report%20of%20the%20match.%20We%0Atested%20different%20Input%20Data%20Types%2C%20In-Context%20Learning%20%28ICL%29%2C%20and%20LLM%2C%20finding%0Athat%20GPT-4%20performs%20best%20when%20using%20CSV%20data%20type%20and%20the%20Chain%20of%20Thought%0Aprompting.%20Following%20report%20generation%2C%20the%20LLM%20evaluates%20and%20scores%20the%0Areports%20to%20assess%20their%20quality.%20Our%20comparisons%20between%20the%20scores%20evaluated%0Aby%20GPT-4%20and%20human%20judges%20show%20a%20tendency%20to%20prefer%20GPT-4%20generated%20reports.%0ASince%20the%20application%20of%20LLM%20in%20badminton%20reporting%20remains%20largely%20unexplored%2C%0Aour%20research%20serves%20as%20a%20foundational%20step%20for%20future%20advancements%20in%20this%0Aarea.%20Moreover%2C%20our%20method%20can%20be%20extended%20to%20other%20sports%20games%2C%20thereby%0Aenhancing%20sports%20promotion.%20For%20more%20details%2C%20please%20refer%20to%0Ahttps%3A//github.com/AndyChiangSH/BADGE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18116v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBADGE%253A%2520BADminton%2520report%2520Generation%2520and%2520Evaluation%2520with%2520LLM%26entry.906535625%3DShang-Hsuan%2520Chiang%2520and%2520Lin-Wei%2520Chao%2520and%2520Kuang-Da%2520Wang%2520and%2520Chih-Chuan%2520Wang%2520and%2520Wen-Chih%2520Peng%26entry.1292438233%3D%2520%2520Badminton%2520enjoys%2520widespread%2520popularity%252C%2520and%2520reports%2520on%2520matches%2520generally%250Ainclude%2520details%2520such%2520as%2520player%2520names%252C%2520game%2520scores%252C%2520and%2520ball%2520types%252C%2520providing%250Aaudiences%2520with%2520a%2520comprehensive%2520view%2520of%2520the%2520games.%2520However%252C%2520writing%2520these%250Areports%2520can%2520be%2520a%2520time-consuming%2520task.%2520This%2520challenge%2520led%2520us%2520to%2520explore%2520whether%250Aa%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520could%2520automate%2520the%2520generation%2520and%2520evaluation%2520of%250Abadminton%2520reports.%2520We%2520introduce%2520a%2520novel%2520framework%2520named%2520BADGE%252C%2520designed%2520for%250Athis%2520purpose%2520using%2520LLM.%2520Our%2520method%2520consists%2520of%2520two%2520main%2520phases%253A%2520Report%250AGeneration%2520and%2520Report%2520Evaluation.%2520Initially%252C%2520badminton-related%2520data%2520is%250Aprocessed%2520by%2520the%2520LLM%252C%2520which%2520then%2520generates%2520a%2520detailed%2520report%2520of%2520the%2520match.%2520We%250Atested%2520different%2520Input%2520Data%2520Types%252C%2520In-Context%2520Learning%2520%2528ICL%2529%252C%2520and%2520LLM%252C%2520finding%250Athat%2520GPT-4%2520performs%2520best%2520when%2520using%2520CSV%2520data%2520type%2520and%2520the%2520Chain%2520of%2520Thought%250Aprompting.%2520Following%2520report%2520generation%252C%2520the%2520LLM%2520evaluates%2520and%2520scores%2520the%250Areports%2520to%2520assess%2520their%2520quality.%2520Our%2520comparisons%2520between%2520the%2520scores%2520evaluated%250Aby%2520GPT-4%2520and%2520human%2520judges%2520show%2520a%2520tendency%2520to%2520prefer%2520GPT-4%2520generated%2520reports.%250ASince%2520the%2520application%2520of%2520LLM%2520in%2520badminton%2520reporting%2520remains%2520largely%2520unexplored%252C%250Aour%2520research%2520serves%2520as%2520a%2520foundational%2520step%2520for%2520future%2520advancements%2520in%2520this%250Aarea.%2520Moreover%252C%2520our%2520method%2520can%2520be%2520extended%2520to%2520other%2520sports%2520games%252C%2520thereby%250Aenhancing%2520sports%2520promotion.%2520For%2520more%2520details%252C%2520please%2520refer%2520to%250Ahttps%253A//github.com/AndyChiangSH/BADGE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18116v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BADGE%3A%20BADminton%20report%20Generation%20and%20Evaluation%20with%20LLM&entry.906535625=Shang-Hsuan%20Chiang%20and%20Lin-Wei%20Chao%20and%20Kuang-Da%20Wang%20and%20Chih-Chuan%20Wang%20and%20Wen-Chih%20Peng&entry.1292438233=%20%20Badminton%20enjoys%20widespread%20popularity%2C%20and%20reports%20on%20matches%20generally%0Ainclude%20details%20such%20as%20player%20names%2C%20game%20scores%2C%20and%20ball%20types%2C%20providing%0Aaudiences%20with%20a%20comprehensive%20view%20of%20the%20games.%20However%2C%20writing%20these%0Areports%20can%20be%20a%20time-consuming%20task.%20This%20challenge%20led%20us%20to%20explore%20whether%0Aa%20Large%20Language%20Model%20%28LLM%29%20could%20automate%20the%20generation%20and%20evaluation%20of%0Abadminton%20reports.%20We%20introduce%20a%20novel%20framework%20named%20BADGE%2C%20designed%20for%0Athis%20purpose%20using%20LLM.%20Our%20method%20consists%20of%20two%20main%20phases%3A%20Report%0AGeneration%20and%20Report%20Evaluation.%20Initially%2C%20badminton-related%20data%20is%0Aprocessed%20by%20the%20LLM%2C%20which%20then%20generates%20a%20detailed%20report%20of%20the%20match.%20We%0Atested%20different%20Input%20Data%20Types%2C%20In-Context%20Learning%20%28ICL%29%2C%20and%20LLM%2C%20finding%0Athat%20GPT-4%20performs%20best%20when%20using%20CSV%20data%20type%20and%20the%20Chain%20of%20Thought%0Aprompting.%20Following%20report%20generation%2C%20the%20LLM%20evaluates%20and%20scores%20the%0Areports%20to%20assess%20their%20quality.%20Our%20comparisons%20between%20the%20scores%20evaluated%0Aby%20GPT-4%20and%20human%20judges%20show%20a%20tendency%20to%20prefer%20GPT-4%20generated%20reports.%0ASince%20the%20application%20of%20LLM%20in%20badminton%20reporting%20remains%20largely%20unexplored%2C%0Aour%20research%20serves%20as%20a%20foundational%20step%20for%20future%20advancements%20in%20this%0Aarea.%20Moreover%2C%20our%20method%20can%20be%20extended%20to%20other%20sports%20games%2C%20thereby%0Aenhancing%20sports%20promotion.%20For%20more%20details%2C%20please%20refer%20to%0Ahttps%3A//github.com/AndyChiangSH/BADGE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18116v1&entry.124074799=Read"},
{"title": "CTS: Sim-to-Real Unsupervised Domain Adaptation on 3D Detection", "author": "Meiying Zhang and Weiyuan Peng and Guangyao Ding and Chenyang Lei and Chunlin Ji and Qi Hao", "abstract": "  Simulation data can be accurately labeled and have been expected to improve\nthe performance of data-driven algorithms, including object detection. However,\ndue to the various domain inconsistencies from simulation to reality\n(sim-to-real), cross-domain object detection algorithms usually suffer from\ndramatic performance drops. While numerous unsupervised domain adaptation (UDA)\nmethods have been developed to address cross-domain tasks between real-world\ndatasets, progress in sim-to-real remains limited. This paper presents a novel\nComplex-to-Simple (CTS) framework to transfer models from labeled simulation\n(source) to unlabeled reality (target) domains. Based on a two-stage detector,\nthe novelty of this work is threefold: 1) developing fixed-size anchor heads\nand RoI augmentation to address size bias and feature diversity between two\ndomains, thereby improving the quality of pseudo-label; 2) developing a novel\ncorner-format representation of aleatoric uncertainty (AU) for the bounding\nbox, to uniformly quantify pseudo-label quality; 3) developing a noise-aware\nmean teacher domain adaptation method based on AU, as well as object-level and\nframe-level sampling strategies, to migrate the impact of noisy labels.\nExperimental results demonstrate that our proposed approach significantly\nenhances the sim-to-real domain adaptation capability of 3D object detection\nmodels, outperforming state-of-the-art cross-domain algorithms, which are\nusually developed for real-to-real UDA tasks.\n", "link": "http://arxiv.org/abs/2406.18129v1", "date": "2024-06-26", "relevancy": 2.3782, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6083}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5836}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CTS%3A%20Sim-to-Real%20Unsupervised%20Domain%20Adaptation%20on%203D%20Detection&body=Title%3A%20CTS%3A%20Sim-to-Real%20Unsupervised%20Domain%20Adaptation%20on%203D%20Detection%0AAuthor%3A%20Meiying%20Zhang%20and%20Weiyuan%20Peng%20and%20Guangyao%20Ding%20and%20Chenyang%20Lei%20and%20Chunlin%20Ji%20and%20Qi%20Hao%0AAbstract%3A%20%20%20Simulation%20data%20can%20be%20accurately%20labeled%20and%20have%20been%20expected%20to%20improve%0Athe%20performance%20of%20data-driven%20algorithms%2C%20including%20object%20detection.%20However%2C%0Adue%20to%20the%20various%20domain%20inconsistencies%20from%20simulation%20to%20reality%0A%28sim-to-real%29%2C%20cross-domain%20object%20detection%20algorithms%20usually%20suffer%20from%0Adramatic%20performance%20drops.%20While%20numerous%20unsupervised%20domain%20adaptation%20%28UDA%29%0Amethods%20have%20been%20developed%20to%20address%20cross-domain%20tasks%20between%20real-world%0Adatasets%2C%20progress%20in%20sim-to-real%20remains%20limited.%20This%20paper%20presents%20a%20novel%0AComplex-to-Simple%20%28CTS%29%20framework%20to%20transfer%20models%20from%20labeled%20simulation%0A%28source%29%20to%20unlabeled%20reality%20%28target%29%20domains.%20Based%20on%20a%20two-stage%20detector%2C%0Athe%20novelty%20of%20this%20work%20is%20threefold%3A%201%29%20developing%20fixed-size%20anchor%20heads%0Aand%20RoI%20augmentation%20to%20address%20size%20bias%20and%20feature%20diversity%20between%20two%0Adomains%2C%20thereby%20improving%20the%20quality%20of%20pseudo-label%3B%202%29%20developing%20a%20novel%0Acorner-format%20representation%20of%20aleatoric%20uncertainty%20%28AU%29%20for%20the%20bounding%0Abox%2C%20to%20uniformly%20quantify%20pseudo-label%20quality%3B%203%29%20developing%20a%20noise-aware%0Amean%20teacher%20domain%20adaptation%20method%20based%20on%20AU%2C%20as%20well%20as%20object-level%20and%0Aframe-level%20sampling%20strategies%2C%20to%20migrate%20the%20impact%20of%20noisy%20labels.%0AExperimental%20results%20demonstrate%20that%20our%20proposed%20approach%20significantly%0Aenhances%20the%20sim-to-real%20domain%20adaptation%20capability%20of%203D%20object%20detection%0Amodels%2C%20outperforming%20state-of-the-art%20cross-domain%20algorithms%2C%20which%20are%0Ausually%20developed%20for%20real-to-real%20UDA%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18129v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCTS%253A%2520Sim-to-Real%2520Unsupervised%2520Domain%2520Adaptation%2520on%25203D%2520Detection%26entry.906535625%3DMeiying%2520Zhang%2520and%2520Weiyuan%2520Peng%2520and%2520Guangyao%2520Ding%2520and%2520Chenyang%2520Lei%2520and%2520Chunlin%2520Ji%2520and%2520Qi%2520Hao%26entry.1292438233%3D%2520%2520Simulation%2520data%2520can%2520be%2520accurately%2520labeled%2520and%2520have%2520been%2520expected%2520to%2520improve%250Athe%2520performance%2520of%2520data-driven%2520algorithms%252C%2520including%2520object%2520detection.%2520However%252C%250Adue%2520to%2520the%2520various%2520domain%2520inconsistencies%2520from%2520simulation%2520to%2520reality%250A%2528sim-to-real%2529%252C%2520cross-domain%2520object%2520detection%2520algorithms%2520usually%2520suffer%2520from%250Adramatic%2520performance%2520drops.%2520While%2520numerous%2520unsupervised%2520domain%2520adaptation%2520%2528UDA%2529%250Amethods%2520have%2520been%2520developed%2520to%2520address%2520cross-domain%2520tasks%2520between%2520real-world%250Adatasets%252C%2520progress%2520in%2520sim-to-real%2520remains%2520limited.%2520This%2520paper%2520presents%2520a%2520novel%250AComplex-to-Simple%2520%2528CTS%2529%2520framework%2520to%2520transfer%2520models%2520from%2520labeled%2520simulation%250A%2528source%2529%2520to%2520unlabeled%2520reality%2520%2528target%2529%2520domains.%2520Based%2520on%2520a%2520two-stage%2520detector%252C%250Athe%2520novelty%2520of%2520this%2520work%2520is%2520threefold%253A%25201%2529%2520developing%2520fixed-size%2520anchor%2520heads%250Aand%2520RoI%2520augmentation%2520to%2520address%2520size%2520bias%2520and%2520feature%2520diversity%2520between%2520two%250Adomains%252C%2520thereby%2520improving%2520the%2520quality%2520of%2520pseudo-label%253B%25202%2529%2520developing%2520a%2520novel%250Acorner-format%2520representation%2520of%2520aleatoric%2520uncertainty%2520%2528AU%2529%2520for%2520the%2520bounding%250Abox%252C%2520to%2520uniformly%2520quantify%2520pseudo-label%2520quality%253B%25203%2529%2520developing%2520a%2520noise-aware%250Amean%2520teacher%2520domain%2520adaptation%2520method%2520based%2520on%2520AU%252C%2520as%2520well%2520as%2520object-level%2520and%250Aframe-level%2520sampling%2520strategies%252C%2520to%2520migrate%2520the%2520impact%2520of%2520noisy%2520labels.%250AExperimental%2520results%2520demonstrate%2520that%2520our%2520proposed%2520approach%2520significantly%250Aenhances%2520the%2520sim-to-real%2520domain%2520adaptation%2520capability%2520of%25203D%2520object%2520detection%250Amodels%252C%2520outperforming%2520state-of-the-art%2520cross-domain%2520algorithms%252C%2520which%2520are%250Ausually%2520developed%2520for%2520real-to-real%2520UDA%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18129v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CTS%3A%20Sim-to-Real%20Unsupervised%20Domain%20Adaptation%20on%203D%20Detection&entry.906535625=Meiying%20Zhang%20and%20Weiyuan%20Peng%20and%20Guangyao%20Ding%20and%20Chenyang%20Lei%20and%20Chunlin%20Ji%20and%20Qi%20Hao&entry.1292438233=%20%20Simulation%20data%20can%20be%20accurately%20labeled%20and%20have%20been%20expected%20to%20improve%0Athe%20performance%20of%20data-driven%20algorithms%2C%20including%20object%20detection.%20However%2C%0Adue%20to%20the%20various%20domain%20inconsistencies%20from%20simulation%20to%20reality%0A%28sim-to-real%29%2C%20cross-domain%20object%20detection%20algorithms%20usually%20suffer%20from%0Adramatic%20performance%20drops.%20While%20numerous%20unsupervised%20domain%20adaptation%20%28UDA%29%0Amethods%20have%20been%20developed%20to%20address%20cross-domain%20tasks%20between%20real-world%0Adatasets%2C%20progress%20in%20sim-to-real%20remains%20limited.%20This%20paper%20presents%20a%20novel%0AComplex-to-Simple%20%28CTS%29%20framework%20to%20transfer%20models%20from%20labeled%20simulation%0A%28source%29%20to%20unlabeled%20reality%20%28target%29%20domains.%20Based%20on%20a%20two-stage%20detector%2C%0Athe%20novelty%20of%20this%20work%20is%20threefold%3A%201%29%20developing%20fixed-size%20anchor%20heads%0Aand%20RoI%20augmentation%20to%20address%20size%20bias%20and%20feature%20diversity%20between%20two%0Adomains%2C%20thereby%20improving%20the%20quality%20of%20pseudo-label%3B%202%29%20developing%20a%20novel%0Acorner-format%20representation%20of%20aleatoric%20uncertainty%20%28AU%29%20for%20the%20bounding%0Abox%2C%20to%20uniformly%20quantify%20pseudo-label%20quality%3B%203%29%20developing%20a%20noise-aware%0Amean%20teacher%20domain%20adaptation%20method%20based%20on%20AU%2C%20as%20well%20as%20object-level%20and%0Aframe-level%20sampling%20strategies%2C%20to%20migrate%20the%20impact%20of%20noisy%20labels.%0AExperimental%20results%20demonstrate%20that%20our%20proposed%20approach%20significantly%0Aenhances%20the%20sim-to-real%20domain%20adaptation%20capability%20of%203D%20object%20detection%0Amodels%2C%20outperforming%20state-of-the-art%20cross-domain%20algorithms%2C%20which%20are%0Ausually%20developed%20for%20real-to-real%20UDA%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18129v1&entry.124074799=Read"},
{"title": "Zero-shot prompt-based classification: topic labeling in times of\n  foundation models in German Tweets", "author": "Simon M\u00fcnker and Kai Kugler and Achim Rettinger", "abstract": "  Filtering and annotating textual data are routine tasks in many areas, like\nsocial media or news analytics. Automating these tasks allows to scale the\nanalyses wrt. speed and breadth of content covered and decreases the manual\neffort required. Due to technical advancements in Natural Language Processing,\nspecifically the success of large foundation models, a new tool for automating\nsuch annotation processes by using a text-to-text interface given written\nguidelines without providing training samples has become available.\n  In this work, we assess these advancements in-the-wild by empirically testing\nthem in an annotation task on German Twitter data about social and political\nEuropean crises. We compare the prompt-based results with our human annotation\nand preceding classification approaches, including Naive Bayes and a BERT-based\nfine-tuning/domain adaptation pipeline. Our results show that the prompt-based\napproach - despite being limited by local computation resources during the\nmodel selection - is comparable with the fine-tuned BERT but without any\nannotated training data. Our findings emphasize the ongoing paradigm shift in\nthe NLP landscape, i.e., the unification of downstream tasks and elimination of\nthe need for pre-labeled training data.\n", "link": "http://arxiv.org/abs/2406.18239v1", "date": "2024-06-26", "relevancy": 2.3617, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4934}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4655}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-shot%20prompt-based%20classification%3A%20topic%20labeling%20in%20times%20of%0A%20%20foundation%20models%20in%20German%20Tweets&body=Title%3A%20Zero-shot%20prompt-based%20classification%3A%20topic%20labeling%20in%20times%20of%0A%20%20foundation%20models%20in%20German%20Tweets%0AAuthor%3A%20Simon%20M%C3%BCnker%20and%20Kai%20Kugler%20and%20Achim%20Rettinger%0AAbstract%3A%20%20%20Filtering%20and%20annotating%20textual%20data%20are%20routine%20tasks%20in%20many%20areas%2C%20like%0Asocial%20media%20or%20news%20analytics.%20Automating%20these%20tasks%20allows%20to%20scale%20the%0Aanalyses%20wrt.%20speed%20and%20breadth%20of%20content%20covered%20and%20decreases%20the%20manual%0Aeffort%20required.%20Due%20to%20technical%20advancements%20in%20Natural%20Language%20Processing%2C%0Aspecifically%20the%20success%20of%20large%20foundation%20models%2C%20a%20new%20tool%20for%20automating%0Asuch%20annotation%20processes%20by%20using%20a%20text-to-text%20interface%20given%20written%0Aguidelines%20without%20providing%20training%20samples%20has%20become%20available.%0A%20%20In%20this%20work%2C%20we%20assess%20these%20advancements%20in-the-wild%20by%20empirically%20testing%0Athem%20in%20an%20annotation%20task%20on%20German%20Twitter%20data%20about%20social%20and%20political%0AEuropean%20crises.%20We%20compare%20the%20prompt-based%20results%20with%20our%20human%20annotation%0Aand%20preceding%20classification%20approaches%2C%20including%20Naive%20Bayes%20and%20a%20BERT-based%0Afine-tuning/domain%20adaptation%20pipeline.%20Our%20results%20show%20that%20the%20prompt-based%0Aapproach%20-%20despite%20being%20limited%20by%20local%20computation%20resources%20during%20the%0Amodel%20selection%20-%20is%20comparable%20with%20the%20fine-tuned%20BERT%20but%20without%20any%0Aannotated%20training%20data.%20Our%20findings%20emphasize%20the%20ongoing%20paradigm%20shift%20in%0Athe%20NLP%20landscape%2C%20i.e.%2C%20the%20unification%20of%20downstream%20tasks%20and%20elimination%20of%0Athe%20need%20for%20pre-labeled%20training%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18239v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-shot%2520prompt-based%2520classification%253A%2520topic%2520labeling%2520in%2520times%2520of%250A%2520%2520foundation%2520models%2520in%2520German%2520Tweets%26entry.906535625%3DSimon%2520M%25C3%25BCnker%2520and%2520Kai%2520Kugler%2520and%2520Achim%2520Rettinger%26entry.1292438233%3D%2520%2520Filtering%2520and%2520annotating%2520textual%2520data%2520are%2520routine%2520tasks%2520in%2520many%2520areas%252C%2520like%250Asocial%2520media%2520or%2520news%2520analytics.%2520Automating%2520these%2520tasks%2520allows%2520to%2520scale%2520the%250Aanalyses%2520wrt.%2520speed%2520and%2520breadth%2520of%2520content%2520covered%2520and%2520decreases%2520the%2520manual%250Aeffort%2520required.%2520Due%2520to%2520technical%2520advancements%2520in%2520Natural%2520Language%2520Processing%252C%250Aspecifically%2520the%2520success%2520of%2520large%2520foundation%2520models%252C%2520a%2520new%2520tool%2520for%2520automating%250Asuch%2520annotation%2520processes%2520by%2520using%2520a%2520text-to-text%2520interface%2520given%2520written%250Aguidelines%2520without%2520providing%2520training%2520samples%2520has%2520become%2520available.%250A%2520%2520In%2520this%2520work%252C%2520we%2520assess%2520these%2520advancements%2520in-the-wild%2520by%2520empirically%2520testing%250Athem%2520in%2520an%2520annotation%2520task%2520on%2520German%2520Twitter%2520data%2520about%2520social%2520and%2520political%250AEuropean%2520crises.%2520We%2520compare%2520the%2520prompt-based%2520results%2520with%2520our%2520human%2520annotation%250Aand%2520preceding%2520classification%2520approaches%252C%2520including%2520Naive%2520Bayes%2520and%2520a%2520BERT-based%250Afine-tuning/domain%2520adaptation%2520pipeline.%2520Our%2520results%2520show%2520that%2520the%2520prompt-based%250Aapproach%2520-%2520despite%2520being%2520limited%2520by%2520local%2520computation%2520resources%2520during%2520the%250Amodel%2520selection%2520-%2520is%2520comparable%2520with%2520the%2520fine-tuned%2520BERT%2520but%2520without%2520any%250Aannotated%2520training%2520data.%2520Our%2520findings%2520emphasize%2520the%2520ongoing%2520paradigm%2520shift%2520in%250Athe%2520NLP%2520landscape%252C%2520i.e.%252C%2520the%2520unification%2520of%2520downstream%2520tasks%2520and%2520elimination%2520of%250Athe%2520need%2520for%2520pre-labeled%2520training%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18239v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-shot%20prompt-based%20classification%3A%20topic%20labeling%20in%20times%20of%0A%20%20foundation%20models%20in%20German%20Tweets&entry.906535625=Simon%20M%C3%BCnker%20and%20Kai%20Kugler%20and%20Achim%20Rettinger&entry.1292438233=%20%20Filtering%20and%20annotating%20textual%20data%20are%20routine%20tasks%20in%20many%20areas%2C%20like%0Asocial%20media%20or%20news%20analytics.%20Automating%20these%20tasks%20allows%20to%20scale%20the%0Aanalyses%20wrt.%20speed%20and%20breadth%20of%20content%20covered%20and%20decreases%20the%20manual%0Aeffort%20required.%20Due%20to%20technical%20advancements%20in%20Natural%20Language%20Processing%2C%0Aspecifically%20the%20success%20of%20large%20foundation%20models%2C%20a%20new%20tool%20for%20automating%0Asuch%20annotation%20processes%20by%20using%20a%20text-to-text%20interface%20given%20written%0Aguidelines%20without%20providing%20training%20samples%20has%20become%20available.%0A%20%20In%20this%20work%2C%20we%20assess%20these%20advancements%20in-the-wild%20by%20empirically%20testing%0Athem%20in%20an%20annotation%20task%20on%20German%20Twitter%20data%20about%20social%20and%20political%0AEuropean%20crises.%20We%20compare%20the%20prompt-based%20results%20with%20our%20human%20annotation%0Aand%20preceding%20classification%20approaches%2C%20including%20Naive%20Bayes%20and%20a%20BERT-based%0Afine-tuning/domain%20adaptation%20pipeline.%20Our%20results%20show%20that%20the%20prompt-based%0Aapproach%20-%20despite%20being%20limited%20by%20local%20computation%20resources%20during%20the%0Amodel%20selection%20-%20is%20comparable%20with%20the%20fine-tuned%20BERT%20but%20without%20any%0Aannotated%20training%20data.%20Our%20findings%20emphasize%20the%20ongoing%20paradigm%20shift%20in%0Athe%20NLP%20landscape%2C%20i.e.%2C%20the%20unification%20of%20downstream%20tasks%20and%20elimination%20of%0Athe%20need%20for%20pre-labeled%20training%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18239v1&entry.124074799=Read"},
{"title": "BiTrack: Bidirectional Offline 3D Multi-Object Tracking Using\n  Camera-LiDAR Data", "author": "Kemiao Huang and Meiying Zhang and Qi Hao", "abstract": "  Compared with real-time multi-object tracking (MOT), offline multi-object\ntracking (OMOT) has the advantages to perform 2D-3D detection fusion, erroneous\nlink correction, and full track optimization but has to deal with the\nchallenges from bounding box misalignment and track evaluation, editing, and\nrefinement. This paper proposes \"BiTrack\", a 3D OMOT framework that includes\nmodules of 2D-3D detection fusion, initial trajectory generation, and\nbidirectional trajectory re-optimization to achieve optimal tracking results\nfrom camera-LiDAR data. The novelty of this paper includes threefold: (1)\ndevelopment of a point-level object registration technique that employs a\ndensity-based similarity metric to achieve accurate fusion of 2D-3D detection\nresults; (2) development of a set of data association and track management\nskills that utilizes a vertex-based similarity metric as well as false alarm\nrejection and track recovery mechanisms to generate reliable bidirectional\nobject trajectories; (3) development of a trajectory re-optimization scheme\nthat re-organizes track fragments of different fidelities in a greedy fashion,\nas well as refines each trajectory with completion and smoothing techniques.\nThe experiment results on the KITTI dataset demonstrate that BiTrack achieves\nthe state-of-the-art performance for 3D OMOT tasks in terms of accuracy and\nefficiency.\n", "link": "http://arxiv.org/abs/2406.18414v1", "date": "2024-06-26", "relevancy": 2.3499, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6211}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5669}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BiTrack%3A%20Bidirectional%20Offline%203D%20Multi-Object%20Tracking%20Using%0A%20%20Camera-LiDAR%20Data&body=Title%3A%20BiTrack%3A%20Bidirectional%20Offline%203D%20Multi-Object%20Tracking%20Using%0A%20%20Camera-LiDAR%20Data%0AAuthor%3A%20Kemiao%20Huang%20and%20Meiying%20Zhang%20and%20Qi%20Hao%0AAbstract%3A%20%20%20Compared%20with%20real-time%20multi-object%20tracking%20%28MOT%29%2C%20offline%20multi-object%0Atracking%20%28OMOT%29%20has%20the%20advantages%20to%20perform%202D-3D%20detection%20fusion%2C%20erroneous%0Alink%20correction%2C%20and%20full%20track%20optimization%20but%20has%20to%20deal%20with%20the%0Achallenges%20from%20bounding%20box%20misalignment%20and%20track%20evaluation%2C%20editing%2C%20and%0Arefinement.%20This%20paper%20proposes%20%22BiTrack%22%2C%20a%203D%20OMOT%20framework%20that%20includes%0Amodules%20of%202D-3D%20detection%20fusion%2C%20initial%20trajectory%20generation%2C%20and%0Abidirectional%20trajectory%20re-optimization%20to%20achieve%20optimal%20tracking%20results%0Afrom%20camera-LiDAR%20data.%20The%20novelty%20of%20this%20paper%20includes%20threefold%3A%20%281%29%0Adevelopment%20of%20a%20point-level%20object%20registration%20technique%20that%20employs%20a%0Adensity-based%20similarity%20metric%20to%20achieve%20accurate%20fusion%20of%202D-3D%20detection%0Aresults%3B%20%282%29%20development%20of%20a%20set%20of%20data%20association%20and%20track%20management%0Askills%20that%20utilizes%20a%20vertex-based%20similarity%20metric%20as%20well%20as%20false%20alarm%0Arejection%20and%20track%20recovery%20mechanisms%20to%20generate%20reliable%20bidirectional%0Aobject%20trajectories%3B%20%283%29%20development%20of%20a%20trajectory%20re-optimization%20scheme%0Athat%20re-organizes%20track%20fragments%20of%20different%20fidelities%20in%20a%20greedy%20fashion%2C%0Aas%20well%20as%20refines%20each%20trajectory%20with%20completion%20and%20smoothing%20techniques.%0AThe%20experiment%20results%20on%20the%20KITTI%20dataset%20demonstrate%20that%20BiTrack%20achieves%0Athe%20state-of-the-art%20performance%20for%203D%20OMOT%20tasks%20in%20terms%20of%20accuracy%20and%0Aefficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18414v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiTrack%253A%2520Bidirectional%2520Offline%25203D%2520Multi-Object%2520Tracking%2520Using%250A%2520%2520Camera-LiDAR%2520Data%26entry.906535625%3DKemiao%2520Huang%2520and%2520Meiying%2520Zhang%2520and%2520Qi%2520Hao%26entry.1292438233%3D%2520%2520Compared%2520with%2520real-time%2520multi-object%2520tracking%2520%2528MOT%2529%252C%2520offline%2520multi-object%250Atracking%2520%2528OMOT%2529%2520has%2520the%2520advantages%2520to%2520perform%25202D-3D%2520detection%2520fusion%252C%2520erroneous%250Alink%2520correction%252C%2520and%2520full%2520track%2520optimization%2520but%2520has%2520to%2520deal%2520with%2520the%250Achallenges%2520from%2520bounding%2520box%2520misalignment%2520and%2520track%2520evaluation%252C%2520editing%252C%2520and%250Arefinement.%2520This%2520paper%2520proposes%2520%2522BiTrack%2522%252C%2520a%25203D%2520OMOT%2520framework%2520that%2520includes%250Amodules%2520of%25202D-3D%2520detection%2520fusion%252C%2520initial%2520trajectory%2520generation%252C%2520and%250Abidirectional%2520trajectory%2520re-optimization%2520to%2520achieve%2520optimal%2520tracking%2520results%250Afrom%2520camera-LiDAR%2520data.%2520The%2520novelty%2520of%2520this%2520paper%2520includes%2520threefold%253A%2520%25281%2529%250Adevelopment%2520of%2520a%2520point-level%2520object%2520registration%2520technique%2520that%2520employs%2520a%250Adensity-based%2520similarity%2520metric%2520to%2520achieve%2520accurate%2520fusion%2520of%25202D-3D%2520detection%250Aresults%253B%2520%25282%2529%2520development%2520of%2520a%2520set%2520of%2520data%2520association%2520and%2520track%2520management%250Askills%2520that%2520utilizes%2520a%2520vertex-based%2520similarity%2520metric%2520as%2520well%2520as%2520false%2520alarm%250Arejection%2520and%2520track%2520recovery%2520mechanisms%2520to%2520generate%2520reliable%2520bidirectional%250Aobject%2520trajectories%253B%2520%25283%2529%2520development%2520of%2520a%2520trajectory%2520re-optimization%2520scheme%250Athat%2520re-organizes%2520track%2520fragments%2520of%2520different%2520fidelities%2520in%2520a%2520greedy%2520fashion%252C%250Aas%2520well%2520as%2520refines%2520each%2520trajectory%2520with%2520completion%2520and%2520smoothing%2520techniques.%250AThe%2520experiment%2520results%2520on%2520the%2520KITTI%2520dataset%2520demonstrate%2520that%2520BiTrack%2520achieves%250Athe%2520state-of-the-art%2520performance%2520for%25203D%2520OMOT%2520tasks%2520in%2520terms%2520of%2520accuracy%2520and%250Aefficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18414v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BiTrack%3A%20Bidirectional%20Offline%203D%20Multi-Object%20Tracking%20Using%0A%20%20Camera-LiDAR%20Data&entry.906535625=Kemiao%20Huang%20and%20Meiying%20Zhang%20and%20Qi%20Hao&entry.1292438233=%20%20Compared%20with%20real-time%20multi-object%20tracking%20%28MOT%29%2C%20offline%20multi-object%0Atracking%20%28OMOT%29%20has%20the%20advantages%20to%20perform%202D-3D%20detection%20fusion%2C%20erroneous%0Alink%20correction%2C%20and%20full%20track%20optimization%20but%20has%20to%20deal%20with%20the%0Achallenges%20from%20bounding%20box%20misalignment%20and%20track%20evaluation%2C%20editing%2C%20and%0Arefinement.%20This%20paper%20proposes%20%22BiTrack%22%2C%20a%203D%20OMOT%20framework%20that%20includes%0Amodules%20of%202D-3D%20detection%20fusion%2C%20initial%20trajectory%20generation%2C%20and%0Abidirectional%20trajectory%20re-optimization%20to%20achieve%20optimal%20tracking%20results%0Afrom%20camera-LiDAR%20data.%20The%20novelty%20of%20this%20paper%20includes%20threefold%3A%20%281%29%0Adevelopment%20of%20a%20point-level%20object%20registration%20technique%20that%20employs%20a%0Adensity-based%20similarity%20metric%20to%20achieve%20accurate%20fusion%20of%202D-3D%20detection%0Aresults%3B%20%282%29%20development%20of%20a%20set%20of%20data%20association%20and%20track%20management%0Askills%20that%20utilizes%20a%20vertex-based%20similarity%20metric%20as%20well%20as%20false%20alarm%0Arejection%20and%20track%20recovery%20mechanisms%20to%20generate%20reliable%20bidirectional%0Aobject%20trajectories%3B%20%283%29%20development%20of%20a%20trajectory%20re-optimization%20scheme%0Athat%20re-organizes%20track%20fragments%20of%20different%20fidelities%20in%20a%20greedy%20fashion%2C%0Aas%20well%20as%20refines%20each%20trajectory%20with%20completion%20and%20smoothing%20techniques.%0AThe%20experiment%20results%20on%20the%20KITTI%20dataset%20demonstrate%20that%20BiTrack%20achieves%0Athe%20state-of-the-art%20performance%20for%203D%20OMOT%20tasks%20in%20terms%20of%20accuracy%20and%0Aefficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18414v1&entry.124074799=Read"},
{"title": "Topological data quality via 0-dimensional persistence matching", "author": "\u00c1lvaro Torras-Casas and Eduardo Paluzo-Hidalgo and Rocio Gonzalez-Diaz", "abstract": "  Data quality is crucial for the successful training, generalization and\nperformance of artificial intelligence models. We propose to measure data\nquality for supervised learning using topological data analysis techniques.\nSpecifically, we provide a novel topological invariant based on persistence\nmatchings induced by inclusions and using $0$-dimensional persistent homology.\nWe show that such an invariant is stable. We provide an algorithm and relate it\nto images, kernels, and cokernels of the induced morphisms. Also, we show that\nthe invariant allows us to understand whether the subset \"represents well\" the\nclusters from the larger dataset or not, and we also use it to estimate bounds\nfor the Hausdorff distance between the subset and the complete dataset. This\napproach enables us to explain why the chosen dataset will lead to poor\nperformance.\n", "link": "http://arxiv.org/abs/2306.02411v2", "date": "2024-06-26", "relevancy": 2.3429, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5003}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4528}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topological%20data%20quality%20via%200-dimensional%20persistence%20matching&body=Title%3A%20Topological%20data%20quality%20via%200-dimensional%20persistence%20matching%0AAuthor%3A%20%C3%81lvaro%20Torras-Casas%20and%20Eduardo%20Paluzo-Hidalgo%20and%20Rocio%20Gonzalez-Diaz%0AAbstract%3A%20%20%20Data%20quality%20is%20crucial%20for%20the%20successful%20training%2C%20generalization%20and%0Aperformance%20of%20artificial%20intelligence%20models.%20We%20propose%20to%20measure%20data%0Aquality%20for%20supervised%20learning%20using%20topological%20data%20analysis%20techniques.%0ASpecifically%2C%20we%20provide%20a%20novel%20topological%20invariant%20based%20on%20persistence%0Amatchings%20induced%20by%20inclusions%20and%20using%20%240%24-dimensional%20persistent%20homology.%0AWe%20show%20that%20such%20an%20invariant%20is%20stable.%20We%20provide%20an%20algorithm%20and%20relate%20it%0Ato%20images%2C%20kernels%2C%20and%20cokernels%20of%20the%20induced%20morphisms.%20Also%2C%20we%20show%20that%0Athe%20invariant%20allows%20us%20to%20understand%20whether%20the%20subset%20%22represents%20well%22%20the%0Aclusters%20from%20the%20larger%20dataset%20or%20not%2C%20and%20we%20also%20use%20it%20to%20estimate%20bounds%0Afor%20the%20Hausdorff%20distance%20between%20the%20subset%20and%20the%20complete%20dataset.%20This%0Aapproach%20enables%20us%20to%20explain%20why%20the%20chosen%20dataset%20will%20lead%20to%20poor%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.02411v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopological%2520data%2520quality%2520via%25200-dimensional%2520persistence%2520matching%26entry.906535625%3D%25C3%2581lvaro%2520Torras-Casas%2520and%2520Eduardo%2520Paluzo-Hidalgo%2520and%2520Rocio%2520Gonzalez-Diaz%26entry.1292438233%3D%2520%2520Data%2520quality%2520is%2520crucial%2520for%2520the%2520successful%2520training%252C%2520generalization%2520and%250Aperformance%2520of%2520artificial%2520intelligence%2520models.%2520We%2520propose%2520to%2520measure%2520data%250Aquality%2520for%2520supervised%2520learning%2520using%2520topological%2520data%2520analysis%2520techniques.%250ASpecifically%252C%2520we%2520provide%2520a%2520novel%2520topological%2520invariant%2520based%2520on%2520persistence%250Amatchings%2520induced%2520by%2520inclusions%2520and%2520using%2520%25240%2524-dimensional%2520persistent%2520homology.%250AWe%2520show%2520that%2520such%2520an%2520invariant%2520is%2520stable.%2520We%2520provide%2520an%2520algorithm%2520and%2520relate%2520it%250Ato%2520images%252C%2520kernels%252C%2520and%2520cokernels%2520of%2520the%2520induced%2520morphisms.%2520Also%252C%2520we%2520show%2520that%250Athe%2520invariant%2520allows%2520us%2520to%2520understand%2520whether%2520the%2520subset%2520%2522represents%2520well%2522%2520the%250Aclusters%2520from%2520the%2520larger%2520dataset%2520or%2520not%252C%2520and%2520we%2520also%2520use%2520it%2520to%2520estimate%2520bounds%250Afor%2520the%2520Hausdorff%2520distance%2520between%2520the%2520subset%2520and%2520the%2520complete%2520dataset.%2520This%250Aapproach%2520enables%2520us%2520to%2520explain%2520why%2520the%2520chosen%2520dataset%2520will%2520lead%2520to%2520poor%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.02411v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topological%20data%20quality%20via%200-dimensional%20persistence%20matching&entry.906535625=%C3%81lvaro%20Torras-Casas%20and%20Eduardo%20Paluzo-Hidalgo%20and%20Rocio%20Gonzalez-Diaz&entry.1292438233=%20%20Data%20quality%20is%20crucial%20for%20the%20successful%20training%2C%20generalization%20and%0Aperformance%20of%20artificial%20intelligence%20models.%20We%20propose%20to%20measure%20data%0Aquality%20for%20supervised%20learning%20using%20topological%20data%20analysis%20techniques.%0ASpecifically%2C%20we%20provide%20a%20novel%20topological%20invariant%20based%20on%20persistence%0Amatchings%20induced%20by%20inclusions%20and%20using%20%240%24-dimensional%20persistent%20homology.%0AWe%20show%20that%20such%20an%20invariant%20is%20stable.%20We%20provide%20an%20algorithm%20and%20relate%20it%0Ato%20images%2C%20kernels%2C%20and%20cokernels%20of%20the%20induced%20morphisms.%20Also%2C%20we%20show%20that%0Athe%20invariant%20allows%20us%20to%20understand%20whether%20the%20subset%20%22represents%20well%22%20the%0Aclusters%20from%20the%20larger%20dataset%20or%20not%2C%20and%20we%20also%20use%20it%20to%20estimate%20bounds%0Afor%20the%20Hausdorff%20distance%20between%20the%20subset%20and%20the%20complete%20dataset.%20This%0Aapproach%20enables%20us%20to%20explain%20why%20the%20chosen%20dataset%20will%20lead%20to%20poor%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.02411v2&entry.124074799=Read"},
{"title": "B-TMS: Bayesian Traversable Terrain Modeling and Segmentation Across 3D\n  LiDAR Scans and Maps for Enhanced Off-Road Navigation", "author": "Minho Oh and Gunhee Shin and Seoyeon Jang and Seungjae Lee and Dongkyu Lee and Wonho Song and Byeongho Yu and Hyungtae Lim and Jaeyoung Lee and Hyun Myung", "abstract": "  Recognizing traversable terrain from 3D point cloud data is critical, as it\ndirectly impacts the performance of autonomous navigation in off-road\nenvironments. However, existing segmentation algorithms often struggle with\nchallenges related to changes in data distribution, environmental specificity,\nand sensor variations. Moreover, when encountering sunken areas, their\nperformance is frequently compromised, and they may even fail to recognize\nthem. To address these challenges, we introduce B-TMS, a novel approach that\nperforms map-wise terrain modeling and segmentation by utilizing Bayesian\ngeneralized kernel (BGK) within the graph structure known as the tri-grid field\n(TGF). Our experiments encompass various data distributions, ranging from\nsingle scans to partial maps, utilizing both public datasets representing urban\nscenes and off-road environments, and our own dataset acquired from extremely\nbumpy terrains. Our results demonstrate notable contributions, particularly in\nterms of robustness to data distribution variations, adaptability to diverse\nenvironmental conditions, and resilience against the challenges associated with\nparameter changes.\n", "link": "http://arxiv.org/abs/2406.18138v1", "date": "2024-06-26", "relevancy": 2.3273, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5951}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5751}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20B-TMS%3A%20Bayesian%20Traversable%20Terrain%20Modeling%20and%20Segmentation%20Across%203D%0A%20%20LiDAR%20Scans%20and%20Maps%20for%20Enhanced%20Off-Road%20Navigation&body=Title%3A%20B-TMS%3A%20Bayesian%20Traversable%20Terrain%20Modeling%20and%20Segmentation%20Across%203D%0A%20%20LiDAR%20Scans%20and%20Maps%20for%20Enhanced%20Off-Road%20Navigation%0AAuthor%3A%20Minho%20Oh%20and%20Gunhee%20Shin%20and%20Seoyeon%20Jang%20and%20Seungjae%20Lee%20and%20Dongkyu%20Lee%20and%20Wonho%20Song%20and%20Byeongho%20Yu%20and%20Hyungtae%20Lim%20and%20Jaeyoung%20Lee%20and%20Hyun%20Myung%0AAbstract%3A%20%20%20Recognizing%20traversable%20terrain%20from%203D%20point%20cloud%20data%20is%20critical%2C%20as%20it%0Adirectly%20impacts%20the%20performance%20of%20autonomous%20navigation%20in%20off-road%0Aenvironments.%20However%2C%20existing%20segmentation%20algorithms%20often%20struggle%20with%0Achallenges%20related%20to%20changes%20in%20data%20distribution%2C%20environmental%20specificity%2C%0Aand%20sensor%20variations.%20Moreover%2C%20when%20encountering%20sunken%20areas%2C%20their%0Aperformance%20is%20frequently%20compromised%2C%20and%20they%20may%20even%20fail%20to%20recognize%0Athem.%20To%20address%20these%20challenges%2C%20we%20introduce%20B-TMS%2C%20a%20novel%20approach%20that%0Aperforms%20map-wise%20terrain%20modeling%20and%20segmentation%20by%20utilizing%20Bayesian%0Ageneralized%20kernel%20%28BGK%29%20within%20the%20graph%20structure%20known%20as%20the%20tri-grid%20field%0A%28TGF%29.%20Our%20experiments%20encompass%20various%20data%20distributions%2C%20ranging%20from%0Asingle%20scans%20to%20partial%20maps%2C%20utilizing%20both%20public%20datasets%20representing%20urban%0Ascenes%20and%20off-road%20environments%2C%20and%20our%20own%20dataset%20acquired%20from%20extremely%0Abumpy%20terrains.%20Our%20results%20demonstrate%20notable%20contributions%2C%20particularly%20in%0Aterms%20of%20robustness%20to%20data%20distribution%20variations%2C%20adaptability%20to%20diverse%0Aenvironmental%20conditions%2C%20and%20resilience%20against%20the%20challenges%20associated%20with%0Aparameter%20changes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18138v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DB-TMS%253A%2520Bayesian%2520Traversable%2520Terrain%2520Modeling%2520and%2520Segmentation%2520Across%25203D%250A%2520%2520LiDAR%2520Scans%2520and%2520Maps%2520for%2520Enhanced%2520Off-Road%2520Navigation%26entry.906535625%3DMinho%2520Oh%2520and%2520Gunhee%2520Shin%2520and%2520Seoyeon%2520Jang%2520and%2520Seungjae%2520Lee%2520and%2520Dongkyu%2520Lee%2520and%2520Wonho%2520Song%2520and%2520Byeongho%2520Yu%2520and%2520Hyungtae%2520Lim%2520and%2520Jaeyoung%2520Lee%2520and%2520Hyun%2520Myung%26entry.1292438233%3D%2520%2520Recognizing%2520traversable%2520terrain%2520from%25203D%2520point%2520cloud%2520data%2520is%2520critical%252C%2520as%2520it%250Adirectly%2520impacts%2520the%2520performance%2520of%2520autonomous%2520navigation%2520in%2520off-road%250Aenvironments.%2520However%252C%2520existing%2520segmentation%2520algorithms%2520often%2520struggle%2520with%250Achallenges%2520related%2520to%2520changes%2520in%2520data%2520distribution%252C%2520environmental%2520specificity%252C%250Aand%2520sensor%2520variations.%2520Moreover%252C%2520when%2520encountering%2520sunken%2520areas%252C%2520their%250Aperformance%2520is%2520frequently%2520compromised%252C%2520and%2520they%2520may%2520even%2520fail%2520to%2520recognize%250Athem.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520B-TMS%252C%2520a%2520novel%2520approach%2520that%250Aperforms%2520map-wise%2520terrain%2520modeling%2520and%2520segmentation%2520by%2520utilizing%2520Bayesian%250Ageneralized%2520kernel%2520%2528BGK%2529%2520within%2520the%2520graph%2520structure%2520known%2520as%2520the%2520tri-grid%2520field%250A%2528TGF%2529.%2520Our%2520experiments%2520encompass%2520various%2520data%2520distributions%252C%2520ranging%2520from%250Asingle%2520scans%2520to%2520partial%2520maps%252C%2520utilizing%2520both%2520public%2520datasets%2520representing%2520urban%250Ascenes%2520and%2520off-road%2520environments%252C%2520and%2520our%2520own%2520dataset%2520acquired%2520from%2520extremely%250Abumpy%2520terrains.%2520Our%2520results%2520demonstrate%2520notable%2520contributions%252C%2520particularly%2520in%250Aterms%2520of%2520robustness%2520to%2520data%2520distribution%2520variations%252C%2520adaptability%2520to%2520diverse%250Aenvironmental%2520conditions%252C%2520and%2520resilience%2520against%2520the%2520challenges%2520associated%2520with%250Aparameter%2520changes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18138v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=B-TMS%3A%20Bayesian%20Traversable%20Terrain%20Modeling%20and%20Segmentation%20Across%203D%0A%20%20LiDAR%20Scans%20and%20Maps%20for%20Enhanced%20Off-Road%20Navigation&entry.906535625=Minho%20Oh%20and%20Gunhee%20Shin%20and%20Seoyeon%20Jang%20and%20Seungjae%20Lee%20and%20Dongkyu%20Lee%20and%20Wonho%20Song%20and%20Byeongho%20Yu%20and%20Hyungtae%20Lim%20and%20Jaeyoung%20Lee%20and%20Hyun%20Myung&entry.1292438233=%20%20Recognizing%20traversable%20terrain%20from%203D%20point%20cloud%20data%20is%20critical%2C%20as%20it%0Adirectly%20impacts%20the%20performance%20of%20autonomous%20navigation%20in%20off-road%0Aenvironments.%20However%2C%20existing%20segmentation%20algorithms%20often%20struggle%20with%0Achallenges%20related%20to%20changes%20in%20data%20distribution%2C%20environmental%20specificity%2C%0Aand%20sensor%20variations.%20Moreover%2C%20when%20encountering%20sunken%20areas%2C%20their%0Aperformance%20is%20frequently%20compromised%2C%20and%20they%20may%20even%20fail%20to%20recognize%0Athem.%20To%20address%20these%20challenges%2C%20we%20introduce%20B-TMS%2C%20a%20novel%20approach%20that%0Aperforms%20map-wise%20terrain%20modeling%20and%20segmentation%20by%20utilizing%20Bayesian%0Ageneralized%20kernel%20%28BGK%29%20within%20the%20graph%20structure%20known%20as%20the%20tri-grid%20field%0A%28TGF%29.%20Our%20experiments%20encompass%20various%20data%20distributions%2C%20ranging%20from%0Asingle%20scans%20to%20partial%20maps%2C%20utilizing%20both%20public%20datasets%20representing%20urban%0Ascenes%20and%20off-road%20environments%2C%20and%20our%20own%20dataset%20acquired%20from%20extremely%0Abumpy%20terrains.%20Our%20results%20demonstrate%20notable%20contributions%2C%20particularly%20in%0Aterms%20of%20robustness%20to%20data%20distribution%20variations%2C%20adaptability%20to%20diverse%0Aenvironmental%20conditions%2C%20and%20resilience%20against%20the%20challenges%20associated%20with%0Aparameter%20changes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18138v1&entry.124074799=Read"},
{"title": "Decoding with Limited Teacher Supervision Requires Understanding When to\n  Trust the Teacher", "author": "Hyunjong Ok and Jegwang Ryu and Jaeho Lee", "abstract": "  How can sLLMs efficiently utilize the supervision of LLMs to improve their\ngenerative quality? This question has been well studied in scenarios where\nthere is no restriction on the number of LLM supervisions one can use, giving\nbirth to many decoding algorithms that utilize supervision without further\ntraining. However, it is still unclear what is an effective strategy under the\nlimited supervision scenario, where we assume that no more than a few tokens\ncan be generated by LLMs. To this end, we develop an algorithm to effectively\naggregate the sLLM and LLM predictions on initial tokens so that the generated\ntokens can more accurately condition the subsequent token generation by sLLM\nonly. Critically, we find that it is essential to adaptively overtrust or\ndisregard the LLM prediction based on the confidence of the sLLM. Through our\nexperiments on a wide range of models and datasets, we demonstrate that our\nmethod provides a consistent improvement over conventional decoding strategies.\n", "link": "http://arxiv.org/abs/2406.18002v1", "date": "2024-06-26", "relevancy": 2.3095, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4683}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4622}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoding%20with%20Limited%20Teacher%20Supervision%20Requires%20Understanding%20When%20to%0A%20%20Trust%20the%20Teacher&body=Title%3A%20Decoding%20with%20Limited%20Teacher%20Supervision%20Requires%20Understanding%20When%20to%0A%20%20Trust%20the%20Teacher%0AAuthor%3A%20Hyunjong%20Ok%20and%20Jegwang%20Ryu%20and%20Jaeho%20Lee%0AAbstract%3A%20%20%20How%20can%20sLLMs%20efficiently%20utilize%20the%20supervision%20of%20LLMs%20to%20improve%20their%0Agenerative%20quality%3F%20This%20question%20has%20been%20well%20studied%20in%20scenarios%20where%0Athere%20is%20no%20restriction%20on%20the%20number%20of%20LLM%20supervisions%20one%20can%20use%2C%20giving%0Abirth%20to%20many%20decoding%20algorithms%20that%20utilize%20supervision%20without%20further%0Atraining.%20However%2C%20it%20is%20still%20unclear%20what%20is%20an%20effective%20strategy%20under%20the%0Alimited%20supervision%20scenario%2C%20where%20we%20assume%20that%20no%20more%20than%20a%20few%20tokens%0Acan%20be%20generated%20by%20LLMs.%20To%20this%20end%2C%20we%20develop%20an%20algorithm%20to%20effectively%0Aaggregate%20the%20sLLM%20and%20LLM%20predictions%20on%20initial%20tokens%20so%20that%20the%20generated%0Atokens%20can%20more%20accurately%20condition%20the%20subsequent%20token%20generation%20by%20sLLM%0Aonly.%20Critically%2C%20we%20find%20that%20it%20is%20essential%20to%20adaptively%20overtrust%20or%0Adisregard%20the%20LLM%20prediction%20based%20on%20the%20confidence%20of%20the%20sLLM.%20Through%20our%0Aexperiments%20on%20a%20wide%20range%20of%20models%20and%20datasets%2C%20we%20demonstrate%20that%20our%0Amethod%20provides%20a%20consistent%20improvement%20over%20conventional%20decoding%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18002v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoding%2520with%2520Limited%2520Teacher%2520Supervision%2520Requires%2520Understanding%2520When%2520to%250A%2520%2520Trust%2520the%2520Teacher%26entry.906535625%3DHyunjong%2520Ok%2520and%2520Jegwang%2520Ryu%2520and%2520Jaeho%2520Lee%26entry.1292438233%3D%2520%2520How%2520can%2520sLLMs%2520efficiently%2520utilize%2520the%2520supervision%2520of%2520LLMs%2520to%2520improve%2520their%250Agenerative%2520quality%253F%2520This%2520question%2520has%2520been%2520well%2520studied%2520in%2520scenarios%2520where%250Athere%2520is%2520no%2520restriction%2520on%2520the%2520number%2520of%2520LLM%2520supervisions%2520one%2520can%2520use%252C%2520giving%250Abirth%2520to%2520many%2520decoding%2520algorithms%2520that%2520utilize%2520supervision%2520without%2520further%250Atraining.%2520However%252C%2520it%2520is%2520still%2520unclear%2520what%2520is%2520an%2520effective%2520strategy%2520under%2520the%250Alimited%2520supervision%2520scenario%252C%2520where%2520we%2520assume%2520that%2520no%2520more%2520than%2520a%2520few%2520tokens%250Acan%2520be%2520generated%2520by%2520LLMs.%2520To%2520this%2520end%252C%2520we%2520develop%2520an%2520algorithm%2520to%2520effectively%250Aaggregate%2520the%2520sLLM%2520and%2520LLM%2520predictions%2520on%2520initial%2520tokens%2520so%2520that%2520the%2520generated%250Atokens%2520can%2520more%2520accurately%2520condition%2520the%2520subsequent%2520token%2520generation%2520by%2520sLLM%250Aonly.%2520Critically%252C%2520we%2520find%2520that%2520it%2520is%2520essential%2520to%2520adaptively%2520overtrust%2520or%250Adisregard%2520the%2520LLM%2520prediction%2520based%2520on%2520the%2520confidence%2520of%2520the%2520sLLM.%2520Through%2520our%250Aexperiments%2520on%2520a%2520wide%2520range%2520of%2520models%2520and%2520datasets%252C%2520we%2520demonstrate%2520that%2520our%250Amethod%2520provides%2520a%2520consistent%2520improvement%2520over%2520conventional%2520decoding%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18002v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoding%20with%20Limited%20Teacher%20Supervision%20Requires%20Understanding%20When%20to%0A%20%20Trust%20the%20Teacher&entry.906535625=Hyunjong%20Ok%20and%20Jegwang%20Ryu%20and%20Jaeho%20Lee&entry.1292438233=%20%20How%20can%20sLLMs%20efficiently%20utilize%20the%20supervision%20of%20LLMs%20to%20improve%20their%0Agenerative%20quality%3F%20This%20question%20has%20been%20well%20studied%20in%20scenarios%20where%0Athere%20is%20no%20restriction%20on%20the%20number%20of%20LLM%20supervisions%20one%20can%20use%2C%20giving%0Abirth%20to%20many%20decoding%20algorithms%20that%20utilize%20supervision%20without%20further%0Atraining.%20However%2C%20it%20is%20still%20unclear%20what%20is%20an%20effective%20strategy%20under%20the%0Alimited%20supervision%20scenario%2C%20where%20we%20assume%20that%20no%20more%20than%20a%20few%20tokens%0Acan%20be%20generated%20by%20LLMs.%20To%20this%20end%2C%20we%20develop%20an%20algorithm%20to%20effectively%0Aaggregate%20the%20sLLM%20and%20LLM%20predictions%20on%20initial%20tokens%20so%20that%20the%20generated%0Atokens%20can%20more%20accurately%20condition%20the%20subsequent%20token%20generation%20by%20sLLM%0Aonly.%20Critically%2C%20we%20find%20that%20it%20is%20essential%20to%20adaptively%20overtrust%20or%0Adisregard%20the%20LLM%20prediction%20based%20on%20the%20confidence%20of%20the%20sLLM.%20Through%20our%0Aexperiments%20on%20a%20wide%20range%20of%20models%20and%20datasets%2C%20we%20demonstrate%20that%20our%0Amethod%20provides%20a%20consistent%20improvement%20over%20conventional%20decoding%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18002v1&entry.124074799=Read"},
{"title": "An Event-based Algorithm for Simultaneous 6-DOF Camera Pose Tracking and\n  Mapping", "author": "Masoud Dayani Najafabadi and Mohammad Reza Ahmadzadeh", "abstract": "  Compared to regular cameras, Dynamic Vision Sensors or Event Cameras can\noutput compact visual data based on a change in the intensity in each pixel\nlocation asynchronously. In this paper, we study the application of current\nimage-based SLAM techniques to these novel sensors. To this end, the\ninformation in adaptively selected event windows is processed to form\nmotion-compensated images. These images are then used to reconstruct the scene\nand estimate the 6-DOF pose of the camera. We also propose an inertial version\nof the event-only pipeline to assess its capabilities. We compare the results\nof different configurations of the proposed algorithm against the ground truth\nfor sequences of two publicly available event datasets. We also compare the\nresults of the proposed event-inertial pipeline with the state-of-the-art and\nshow it can produce comparable or more accurate results provided the map\nestimate is reliable.\n", "link": "http://arxiv.org/abs/2301.00618v3", "date": "2024-06-26", "relevancy": 2.3062, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5901}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.58}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5676}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Event-based%20Algorithm%20for%20Simultaneous%206-DOF%20Camera%20Pose%20Tracking%20and%0A%20%20Mapping&body=Title%3A%20An%20Event-based%20Algorithm%20for%20Simultaneous%206-DOF%20Camera%20Pose%20Tracking%20and%0A%20%20Mapping%0AAuthor%3A%20Masoud%20Dayani%20Najafabadi%20and%20Mohammad%20Reza%20Ahmadzadeh%0AAbstract%3A%20%20%20Compared%20to%20regular%20cameras%2C%20Dynamic%20Vision%20Sensors%20or%20Event%20Cameras%20can%0Aoutput%20compact%20visual%20data%20based%20on%20a%20change%20in%20the%20intensity%20in%20each%20pixel%0Alocation%20asynchronously.%20In%20this%20paper%2C%20we%20study%20the%20application%20of%20current%0Aimage-based%20SLAM%20techniques%20to%20these%20novel%20sensors.%20To%20this%20end%2C%20the%0Ainformation%20in%20adaptively%20selected%20event%20windows%20is%20processed%20to%20form%0Amotion-compensated%20images.%20These%20images%20are%20then%20used%20to%20reconstruct%20the%20scene%0Aand%20estimate%20the%206-DOF%20pose%20of%20the%20camera.%20We%20also%20propose%20an%20inertial%20version%0Aof%20the%20event-only%20pipeline%20to%20assess%20its%20capabilities.%20We%20compare%20the%20results%0Aof%20different%20configurations%20of%20the%20proposed%20algorithm%20against%20the%20ground%20truth%0Afor%20sequences%20of%20two%20publicly%20available%20event%20datasets.%20We%20also%20compare%20the%0Aresults%20of%20the%20proposed%20event-inertial%20pipeline%20with%20the%20state-of-the-art%20and%0Ashow%20it%20can%20produce%20comparable%20or%20more%20accurate%20results%20provided%20the%20map%0Aestimate%20is%20reliable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.00618v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Event-based%2520Algorithm%2520for%2520Simultaneous%25206-DOF%2520Camera%2520Pose%2520Tracking%2520and%250A%2520%2520Mapping%26entry.906535625%3DMasoud%2520Dayani%2520Najafabadi%2520and%2520Mohammad%2520Reza%2520Ahmadzadeh%26entry.1292438233%3D%2520%2520Compared%2520to%2520regular%2520cameras%252C%2520Dynamic%2520Vision%2520Sensors%2520or%2520Event%2520Cameras%2520can%250Aoutput%2520compact%2520visual%2520data%2520based%2520on%2520a%2520change%2520in%2520the%2520intensity%2520in%2520each%2520pixel%250Alocation%2520asynchronously.%2520In%2520this%2520paper%252C%2520we%2520study%2520the%2520application%2520of%2520current%250Aimage-based%2520SLAM%2520techniques%2520to%2520these%2520novel%2520sensors.%2520To%2520this%2520end%252C%2520the%250Ainformation%2520in%2520adaptively%2520selected%2520event%2520windows%2520is%2520processed%2520to%2520form%250Amotion-compensated%2520images.%2520These%2520images%2520are%2520then%2520used%2520to%2520reconstruct%2520the%2520scene%250Aand%2520estimate%2520the%25206-DOF%2520pose%2520of%2520the%2520camera.%2520We%2520also%2520propose%2520an%2520inertial%2520version%250Aof%2520the%2520event-only%2520pipeline%2520to%2520assess%2520its%2520capabilities.%2520We%2520compare%2520the%2520results%250Aof%2520different%2520configurations%2520of%2520the%2520proposed%2520algorithm%2520against%2520the%2520ground%2520truth%250Afor%2520sequences%2520of%2520two%2520publicly%2520available%2520event%2520datasets.%2520We%2520also%2520compare%2520the%250Aresults%2520of%2520the%2520proposed%2520event-inertial%2520pipeline%2520with%2520the%2520state-of-the-art%2520and%250Ashow%2520it%2520can%2520produce%2520comparable%2520or%2520more%2520accurate%2520results%2520provided%2520the%2520map%250Aestimate%2520is%2520reliable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.00618v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Event-based%20Algorithm%20for%20Simultaneous%206-DOF%20Camera%20Pose%20Tracking%20and%0A%20%20Mapping&entry.906535625=Masoud%20Dayani%20Najafabadi%20and%20Mohammad%20Reza%20Ahmadzadeh&entry.1292438233=%20%20Compared%20to%20regular%20cameras%2C%20Dynamic%20Vision%20Sensors%20or%20Event%20Cameras%20can%0Aoutput%20compact%20visual%20data%20based%20on%20a%20change%20in%20the%20intensity%20in%20each%20pixel%0Alocation%20asynchronously.%20In%20this%20paper%2C%20we%20study%20the%20application%20of%20current%0Aimage-based%20SLAM%20techniques%20to%20these%20novel%20sensors.%20To%20this%20end%2C%20the%0Ainformation%20in%20adaptively%20selected%20event%20windows%20is%20processed%20to%20form%0Amotion-compensated%20images.%20These%20images%20are%20then%20used%20to%20reconstruct%20the%20scene%0Aand%20estimate%20the%206-DOF%20pose%20of%20the%20camera.%20We%20also%20propose%20an%20inertial%20version%0Aof%20the%20event-only%20pipeline%20to%20assess%20its%20capabilities.%20We%20compare%20the%20results%0Aof%20different%20configurations%20of%20the%20proposed%20algorithm%20against%20the%20ground%20truth%0Afor%20sequences%20of%20two%20publicly%20available%20event%20datasets.%20We%20also%20compare%20the%0Aresults%20of%20the%20proposed%20event-inertial%20pipeline%20with%20the%20state-of-the-art%20and%0Ashow%20it%20can%20produce%20comparable%20or%20more%20accurate%20results%20provided%20the%20map%0Aestimate%20is%20reliable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.00618v3&entry.124074799=Read"},
{"title": "ContactNet: Geometric-Based Deep Learning Model for Predicting\n  Protein-Protein Interactions", "author": "Matan Halfon and Tomer Cohen and Raanan Fattal and Dina Schneidman-Duhovny", "abstract": "  Deep learning approaches achieved significant progress in predicting protein\nstructures. These methods are often applied to protein-protein interactions\n(PPIs) yet require Multiple Sequence Alignment (MSA) which is unavailable for\nvarious interactions, such as antibody-antigen. Computational docking methods\nare capable of sampling accurate complex models, but also produce thousands of\ninvalid configurations. The design of scoring functions for identifying\naccurate models is a long-standing challenge. We develop a novel\nattention-based Graph Neural Network (GNN), ContactNet, for classifying PPI\nmodels obtained from docking algorithms into accurate and incorrect ones. When\ntrained on docked antigen and modeled antibody structures, ContactNet doubles\nthe accuracy of current state-of-the-art scoring functions, achieving accurate\nmodels among its Top-10 at 43% of the test cases. When applied to unbound\nantibodies, its Top-10 accuracy increases to 65%. This performance is achieved\nwithout MSA and the approach is applicable to other types of interactions, such\nas host-pathogens or general PPIs.\n", "link": "http://arxiv.org/abs/2406.18314v1", "date": "2024-06-26", "relevancy": 2.299, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4662}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4629}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ContactNet%3A%20Geometric-Based%20Deep%20Learning%20Model%20for%20Predicting%0A%20%20Protein-Protein%20Interactions&body=Title%3A%20ContactNet%3A%20Geometric-Based%20Deep%20Learning%20Model%20for%20Predicting%0A%20%20Protein-Protein%20Interactions%0AAuthor%3A%20Matan%20Halfon%20and%20Tomer%20Cohen%20and%20Raanan%20Fattal%20and%20Dina%20Schneidman-Duhovny%0AAbstract%3A%20%20%20Deep%20learning%20approaches%20achieved%20significant%20progress%20in%20predicting%20protein%0Astructures.%20These%20methods%20are%20often%20applied%20to%20protein-protein%20interactions%0A%28PPIs%29%20yet%20require%20Multiple%20Sequence%20Alignment%20%28MSA%29%20which%20is%20unavailable%20for%0Avarious%20interactions%2C%20such%20as%20antibody-antigen.%20Computational%20docking%20methods%0Aare%20capable%20of%20sampling%20accurate%20complex%20models%2C%20but%20also%20produce%20thousands%20of%0Ainvalid%20configurations.%20The%20design%20of%20scoring%20functions%20for%20identifying%0Aaccurate%20models%20is%20a%20long-standing%20challenge.%20We%20develop%20a%20novel%0Aattention-based%20Graph%20Neural%20Network%20%28GNN%29%2C%20ContactNet%2C%20for%20classifying%20PPI%0Amodels%20obtained%20from%20docking%20algorithms%20into%20accurate%20and%20incorrect%20ones.%20When%0Atrained%20on%20docked%20antigen%20and%20modeled%20antibody%20structures%2C%20ContactNet%20doubles%0Athe%20accuracy%20of%20current%20state-of-the-art%20scoring%20functions%2C%20achieving%20accurate%0Amodels%20among%20its%20Top-10%20at%2043%25%20of%20the%20test%20cases.%20When%20applied%20to%20unbound%0Aantibodies%2C%20its%20Top-10%20accuracy%20increases%20to%2065%25.%20This%20performance%20is%20achieved%0Awithout%20MSA%20and%20the%20approach%20is%20applicable%20to%20other%20types%20of%20interactions%2C%20such%0Aas%20host-pathogens%20or%20general%20PPIs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18314v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContactNet%253A%2520Geometric-Based%2520Deep%2520Learning%2520Model%2520for%2520Predicting%250A%2520%2520Protein-Protein%2520Interactions%26entry.906535625%3DMatan%2520Halfon%2520and%2520Tomer%2520Cohen%2520and%2520Raanan%2520Fattal%2520and%2520Dina%2520Schneidman-Duhovny%26entry.1292438233%3D%2520%2520Deep%2520learning%2520approaches%2520achieved%2520significant%2520progress%2520in%2520predicting%2520protein%250Astructures.%2520These%2520methods%2520are%2520often%2520applied%2520to%2520protein-protein%2520interactions%250A%2528PPIs%2529%2520yet%2520require%2520Multiple%2520Sequence%2520Alignment%2520%2528MSA%2529%2520which%2520is%2520unavailable%2520for%250Avarious%2520interactions%252C%2520such%2520as%2520antibody-antigen.%2520Computational%2520docking%2520methods%250Aare%2520capable%2520of%2520sampling%2520accurate%2520complex%2520models%252C%2520but%2520also%2520produce%2520thousands%2520of%250Ainvalid%2520configurations.%2520The%2520design%2520of%2520scoring%2520functions%2520for%2520identifying%250Aaccurate%2520models%2520is%2520a%2520long-standing%2520challenge.%2520We%2520develop%2520a%2520novel%250Aattention-based%2520Graph%2520Neural%2520Network%2520%2528GNN%2529%252C%2520ContactNet%252C%2520for%2520classifying%2520PPI%250Amodels%2520obtained%2520from%2520docking%2520algorithms%2520into%2520accurate%2520and%2520incorrect%2520ones.%2520When%250Atrained%2520on%2520docked%2520antigen%2520and%2520modeled%2520antibody%2520structures%252C%2520ContactNet%2520doubles%250Athe%2520accuracy%2520of%2520current%2520state-of-the-art%2520scoring%2520functions%252C%2520achieving%2520accurate%250Amodels%2520among%2520its%2520Top-10%2520at%252043%2525%2520of%2520the%2520test%2520cases.%2520When%2520applied%2520to%2520unbound%250Aantibodies%252C%2520its%2520Top-10%2520accuracy%2520increases%2520to%252065%2525.%2520This%2520performance%2520is%2520achieved%250Awithout%2520MSA%2520and%2520the%2520approach%2520is%2520applicable%2520to%2520other%2520types%2520of%2520interactions%252C%2520such%250Aas%2520host-pathogens%2520or%2520general%2520PPIs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18314v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ContactNet%3A%20Geometric-Based%20Deep%20Learning%20Model%20for%20Predicting%0A%20%20Protein-Protein%20Interactions&entry.906535625=Matan%20Halfon%20and%20Tomer%20Cohen%20and%20Raanan%20Fattal%20and%20Dina%20Schneidman-Duhovny&entry.1292438233=%20%20Deep%20learning%20approaches%20achieved%20significant%20progress%20in%20predicting%20protein%0Astructures.%20These%20methods%20are%20often%20applied%20to%20protein-protein%20interactions%0A%28PPIs%29%20yet%20require%20Multiple%20Sequence%20Alignment%20%28MSA%29%20which%20is%20unavailable%20for%0Avarious%20interactions%2C%20such%20as%20antibody-antigen.%20Computational%20docking%20methods%0Aare%20capable%20of%20sampling%20accurate%20complex%20models%2C%20but%20also%20produce%20thousands%20of%0Ainvalid%20configurations.%20The%20design%20of%20scoring%20functions%20for%20identifying%0Aaccurate%20models%20is%20a%20long-standing%20challenge.%20We%20develop%20a%20novel%0Aattention-based%20Graph%20Neural%20Network%20%28GNN%29%2C%20ContactNet%2C%20for%20classifying%20PPI%0Amodels%20obtained%20from%20docking%20algorithms%20into%20accurate%20and%20incorrect%20ones.%20When%0Atrained%20on%20docked%20antigen%20and%20modeled%20antibody%20structures%2C%20ContactNet%20doubles%0Athe%20accuracy%20of%20current%20state-of-the-art%20scoring%20functions%2C%20achieving%20accurate%0Amodels%20among%20its%20Top-10%20at%2043%25%20of%20the%20test%20cases.%20When%20applied%20to%20unbound%0Aantibodies%2C%20its%20Top-10%20accuracy%20increases%20to%2065%25.%20This%20performance%20is%20achieved%0Awithout%20MSA%20and%20the%20approach%20is%20applicable%20to%20other%20types%20of%20interactions%2C%20such%0Aas%20host-pathogens%20or%20general%20PPIs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18314v1&entry.124074799=Read"},
{"title": "Multimodal foundation world models for generalist embodied agents", "author": "Pietro Mazzaglia and Tim Verbelen and Bart Dhoedt and Aaron Courville and Sai Rajeswar", "abstract": "  Learning generalist embodied agents, able to solve multitudes of tasks in\ndifferent domains is a long-standing problem. Reinforcement learning (RL) is\nhard to scale up as it requires a complex reward design for each task. In\ncontrast, language can specify tasks in a more natural way. Current foundation\nvision-language models (VLMs) generally require fine-tuning or other\nadaptations to be functional, due to the significant domain gap. However, the\nlack of multimodal data in such domains represents an obstacle toward\ndeveloping foundation models for embodied applications. In this work, we\novercome these problems by presenting multimodal foundation world models, able\nto connect and align the representation of foundation VLMs with the latent\nspace of generative world models for RL, without any language annotations. The\nresulting agent learning framework, GenRL, allows one to specify tasks through\nvision and/or language prompts, ground them in the embodied domain's dynamics,\nand learns the corresponding behaviors in imagination. As assessed through\nlarge-scale multi-task benchmarking, GenRL exhibits strong multi-task\ngeneralization performance in several locomotion and manipulation domains.\nFurthermore, by introducing a data-free RL strategy, it lays the groundwork for\nfoundation model-based RL for generalist embodied agents.\n", "link": "http://arxiv.org/abs/2406.18043v1", "date": "2024-06-26", "relevancy": 2.2906, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5966}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5795}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20foundation%20world%20models%20for%20generalist%20embodied%20agents&body=Title%3A%20Multimodal%20foundation%20world%20models%20for%20generalist%20embodied%20agents%0AAuthor%3A%20Pietro%20Mazzaglia%20and%20Tim%20Verbelen%20and%20Bart%20Dhoedt%20and%20Aaron%20Courville%20and%20Sai%20Rajeswar%0AAbstract%3A%20%20%20Learning%20generalist%20embodied%20agents%2C%20able%20to%20solve%20multitudes%20of%20tasks%20in%0Adifferent%20domains%20is%20a%20long-standing%20problem.%20Reinforcement%20learning%20%28RL%29%20is%0Ahard%20to%20scale%20up%20as%20it%20requires%20a%20complex%20reward%20design%20for%20each%20task.%20In%0Acontrast%2C%20language%20can%20specify%20tasks%20in%20a%20more%20natural%20way.%20Current%20foundation%0Avision-language%20models%20%28VLMs%29%20generally%20require%20fine-tuning%20or%20other%0Aadaptations%20to%20be%20functional%2C%20due%20to%20the%20significant%20domain%20gap.%20However%2C%20the%0Alack%20of%20multimodal%20data%20in%20such%20domains%20represents%20an%20obstacle%20toward%0Adeveloping%20foundation%20models%20for%20embodied%20applications.%20In%20this%20work%2C%20we%0Aovercome%20these%20problems%20by%20presenting%20multimodal%20foundation%20world%20models%2C%20able%0Ato%20connect%20and%20align%20the%20representation%20of%20foundation%20VLMs%20with%20the%20latent%0Aspace%20of%20generative%20world%20models%20for%20RL%2C%20without%20any%20language%20annotations.%20The%0Aresulting%20agent%20learning%20framework%2C%20GenRL%2C%20allows%20one%20to%20specify%20tasks%20through%0Avision%20and/or%20language%20prompts%2C%20ground%20them%20in%20the%20embodied%20domain%27s%20dynamics%2C%0Aand%20learns%20the%20corresponding%20behaviors%20in%20imagination.%20As%20assessed%20through%0Alarge-scale%20multi-task%20benchmarking%2C%20GenRL%20exhibits%20strong%20multi-task%0Ageneralization%20performance%20in%20several%20locomotion%20and%20manipulation%20domains.%0AFurthermore%2C%20by%20introducing%20a%20data-free%20RL%20strategy%2C%20it%20lays%20the%20groundwork%20for%0Afoundation%20model-based%20RL%20for%20generalist%20embodied%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18043v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520foundation%2520world%2520models%2520for%2520generalist%2520embodied%2520agents%26entry.906535625%3DPietro%2520Mazzaglia%2520and%2520Tim%2520Verbelen%2520and%2520Bart%2520Dhoedt%2520and%2520Aaron%2520Courville%2520and%2520Sai%2520Rajeswar%26entry.1292438233%3D%2520%2520Learning%2520generalist%2520embodied%2520agents%252C%2520able%2520to%2520solve%2520multitudes%2520of%2520tasks%2520in%250Adifferent%2520domains%2520is%2520a%2520long-standing%2520problem.%2520Reinforcement%2520learning%2520%2528RL%2529%2520is%250Ahard%2520to%2520scale%2520up%2520as%2520it%2520requires%2520a%2520complex%2520reward%2520design%2520for%2520each%2520task.%2520In%250Acontrast%252C%2520language%2520can%2520specify%2520tasks%2520in%2520a%2520more%2520natural%2520way.%2520Current%2520foundation%250Avision-language%2520models%2520%2528VLMs%2529%2520generally%2520require%2520fine-tuning%2520or%2520other%250Aadaptations%2520to%2520be%2520functional%252C%2520due%2520to%2520the%2520significant%2520domain%2520gap.%2520However%252C%2520the%250Alack%2520of%2520multimodal%2520data%2520in%2520such%2520domains%2520represents%2520an%2520obstacle%2520toward%250Adeveloping%2520foundation%2520models%2520for%2520embodied%2520applications.%2520In%2520this%2520work%252C%2520we%250Aovercome%2520these%2520problems%2520by%2520presenting%2520multimodal%2520foundation%2520world%2520models%252C%2520able%250Ato%2520connect%2520and%2520align%2520the%2520representation%2520of%2520foundation%2520VLMs%2520with%2520the%2520latent%250Aspace%2520of%2520generative%2520world%2520models%2520for%2520RL%252C%2520without%2520any%2520language%2520annotations.%2520The%250Aresulting%2520agent%2520learning%2520framework%252C%2520GenRL%252C%2520allows%2520one%2520to%2520specify%2520tasks%2520through%250Avision%2520and/or%2520language%2520prompts%252C%2520ground%2520them%2520in%2520the%2520embodied%2520domain%2527s%2520dynamics%252C%250Aand%2520learns%2520the%2520corresponding%2520behaviors%2520in%2520imagination.%2520As%2520assessed%2520through%250Alarge-scale%2520multi-task%2520benchmarking%252C%2520GenRL%2520exhibits%2520strong%2520multi-task%250Ageneralization%2520performance%2520in%2520several%2520locomotion%2520and%2520manipulation%2520domains.%250AFurthermore%252C%2520by%2520introducing%2520a%2520data-free%2520RL%2520strategy%252C%2520it%2520lays%2520the%2520groundwork%2520for%250Afoundation%2520model-based%2520RL%2520for%2520generalist%2520embodied%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18043v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20foundation%20world%20models%20for%20generalist%20embodied%20agents&entry.906535625=Pietro%20Mazzaglia%20and%20Tim%20Verbelen%20and%20Bart%20Dhoedt%20and%20Aaron%20Courville%20and%20Sai%20Rajeswar&entry.1292438233=%20%20Learning%20generalist%20embodied%20agents%2C%20able%20to%20solve%20multitudes%20of%20tasks%20in%0Adifferent%20domains%20is%20a%20long-standing%20problem.%20Reinforcement%20learning%20%28RL%29%20is%0Ahard%20to%20scale%20up%20as%20it%20requires%20a%20complex%20reward%20design%20for%20each%20task.%20In%0Acontrast%2C%20language%20can%20specify%20tasks%20in%20a%20more%20natural%20way.%20Current%20foundation%0Avision-language%20models%20%28VLMs%29%20generally%20require%20fine-tuning%20or%20other%0Aadaptations%20to%20be%20functional%2C%20due%20to%20the%20significant%20domain%20gap.%20However%2C%20the%0Alack%20of%20multimodal%20data%20in%20such%20domains%20represents%20an%20obstacle%20toward%0Adeveloping%20foundation%20models%20for%20embodied%20applications.%20In%20this%20work%2C%20we%0Aovercome%20these%20problems%20by%20presenting%20multimodal%20foundation%20world%20models%2C%20able%0Ato%20connect%20and%20align%20the%20representation%20of%20foundation%20VLMs%20with%20the%20latent%0Aspace%20of%20generative%20world%20models%20for%20RL%2C%20without%20any%20language%20annotations.%20The%0Aresulting%20agent%20learning%20framework%2C%20GenRL%2C%20allows%20one%20to%20specify%20tasks%20through%0Avision%20and/or%20language%20prompts%2C%20ground%20them%20in%20the%20embodied%20domain%27s%20dynamics%2C%0Aand%20learns%20the%20corresponding%20behaviors%20in%20imagination.%20As%20assessed%20through%0Alarge-scale%20multi-task%20benchmarking%2C%20GenRL%20exhibits%20strong%20multi-task%0Ageneralization%20performance%20in%20several%20locomotion%20and%20manipulation%20domains.%0AFurthermore%2C%20by%20introducing%20a%20data-free%20RL%20strategy%2C%20it%20lays%20the%20groundwork%20for%0Afoundation%20model-based%20RL%20for%20generalist%20embodied%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18043v1&entry.124074799=Read"},
{"title": "Stable Diffusion Segmentation for Biomedical Images with Single-step\n  Reverse Process", "author": "Tianyu Lin and Zhiguang Chen and Zhonghao Yan and Fudan Zheng and Weijiang Yu", "abstract": "  Diffusion models have demonstrated their effectiveness across various\ngenerative tasks. However, when applied to medical image segmentation, these\nmodels encounter several challenges, including significant resource and time\nrequirements. They also necessitate a multi-step reverse process and multiple\nsamples to produce reliable predictions. To address these challenges, we\nintroduce the first latent diffusion segmentation model, named SDSeg, built\nupon stable diffusion (SD). SDSeg incorporates a straightforward latent\nestimation strategy to facilitate a single-step reverse process and utilizes\nlatent fusion concatenation to remove the necessity for multiple samples.\nExtensive experiments indicate that SDSeg surpasses existing state-of-the-art\nmethods on five benchmark datasets featuring diverse imaging modalities.\nRemarkably, SDSeg is capable of generating stable predictions with a solitary\nreverse step and sample, epitomizing the model's stability as implied by its\nname. The code is available at\nhttps://github.com/lin-tianyu/Stable-Diffusion-Seg\n", "link": "http://arxiv.org/abs/2406.18361v1", "date": "2024-06-26", "relevancy": 2.2899, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6744}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5715}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stable%20Diffusion%20Segmentation%20for%20Biomedical%20Images%20with%20Single-step%0A%20%20Reverse%20Process&body=Title%3A%20Stable%20Diffusion%20Segmentation%20for%20Biomedical%20Images%20with%20Single-step%0A%20%20Reverse%20Process%0AAuthor%3A%20Tianyu%20Lin%20and%20Zhiguang%20Chen%20and%20Zhonghao%20Yan%20and%20Fudan%20Zheng%20and%20Weijiang%20Yu%0AAbstract%3A%20%20%20Diffusion%20models%20have%20demonstrated%20their%20effectiveness%20across%20various%0Agenerative%20tasks.%20However%2C%20when%20applied%20to%20medical%20image%20segmentation%2C%20these%0Amodels%20encounter%20several%20challenges%2C%20including%20significant%20resource%20and%20time%0Arequirements.%20They%20also%20necessitate%20a%20multi-step%20reverse%20process%20and%20multiple%0Asamples%20to%20produce%20reliable%20predictions.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20the%20first%20latent%20diffusion%20segmentation%20model%2C%20named%20SDSeg%2C%20built%0Aupon%20stable%20diffusion%20%28SD%29.%20SDSeg%20incorporates%20a%20straightforward%20latent%0Aestimation%20strategy%20to%20facilitate%20a%20single-step%20reverse%20process%20and%20utilizes%0Alatent%20fusion%20concatenation%20to%20remove%20the%20necessity%20for%20multiple%20samples.%0AExtensive%20experiments%20indicate%20that%20SDSeg%20surpasses%20existing%20state-of-the-art%0Amethods%20on%20five%20benchmark%20datasets%20featuring%20diverse%20imaging%20modalities.%0ARemarkably%2C%20SDSeg%20is%20capable%20of%20generating%20stable%20predictions%20with%20a%20solitary%0Areverse%20step%20and%20sample%2C%20epitomizing%20the%20model%27s%20stability%20as%20implied%20by%20its%0Aname.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/lin-tianyu/Stable-Diffusion-Seg%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18361v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStable%2520Diffusion%2520Segmentation%2520for%2520Biomedical%2520Images%2520with%2520Single-step%250A%2520%2520Reverse%2520Process%26entry.906535625%3DTianyu%2520Lin%2520and%2520Zhiguang%2520Chen%2520and%2520Zhonghao%2520Yan%2520and%2520Fudan%2520Zheng%2520and%2520Weijiang%2520Yu%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520demonstrated%2520their%2520effectiveness%2520across%2520various%250Agenerative%2520tasks.%2520However%252C%2520when%2520applied%2520to%2520medical%2520image%2520segmentation%252C%2520these%250Amodels%2520encounter%2520several%2520challenges%252C%2520including%2520significant%2520resource%2520and%2520time%250Arequirements.%2520They%2520also%2520necessitate%2520a%2520multi-step%2520reverse%2520process%2520and%2520multiple%250Asamples%2520to%2520produce%2520reliable%2520predictions.%2520To%2520address%2520these%2520challenges%252C%2520we%250Aintroduce%2520the%2520first%2520latent%2520diffusion%2520segmentation%2520model%252C%2520named%2520SDSeg%252C%2520built%250Aupon%2520stable%2520diffusion%2520%2528SD%2529.%2520SDSeg%2520incorporates%2520a%2520straightforward%2520latent%250Aestimation%2520strategy%2520to%2520facilitate%2520a%2520single-step%2520reverse%2520process%2520and%2520utilizes%250Alatent%2520fusion%2520concatenation%2520to%2520remove%2520the%2520necessity%2520for%2520multiple%2520samples.%250AExtensive%2520experiments%2520indicate%2520that%2520SDSeg%2520surpasses%2520existing%2520state-of-the-art%250Amethods%2520on%2520five%2520benchmark%2520datasets%2520featuring%2520diverse%2520imaging%2520modalities.%250ARemarkably%252C%2520SDSeg%2520is%2520capable%2520of%2520generating%2520stable%2520predictions%2520with%2520a%2520solitary%250Areverse%2520step%2520and%2520sample%252C%2520epitomizing%2520the%2520model%2527s%2520stability%2520as%2520implied%2520by%2520its%250Aname.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/lin-tianyu/Stable-Diffusion-Seg%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18361v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stable%20Diffusion%20Segmentation%20for%20Biomedical%20Images%20with%20Single-step%0A%20%20Reverse%20Process&entry.906535625=Tianyu%20Lin%20and%20Zhiguang%20Chen%20and%20Zhonghao%20Yan%20and%20Fudan%20Zheng%20and%20Weijiang%20Yu&entry.1292438233=%20%20Diffusion%20models%20have%20demonstrated%20their%20effectiveness%20across%20various%0Agenerative%20tasks.%20However%2C%20when%20applied%20to%20medical%20image%20segmentation%2C%20these%0Amodels%20encounter%20several%20challenges%2C%20including%20significant%20resource%20and%20time%0Arequirements.%20They%20also%20necessitate%20a%20multi-step%20reverse%20process%20and%20multiple%0Asamples%20to%20produce%20reliable%20predictions.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20the%20first%20latent%20diffusion%20segmentation%20model%2C%20named%20SDSeg%2C%20built%0Aupon%20stable%20diffusion%20%28SD%29.%20SDSeg%20incorporates%20a%20straightforward%20latent%0Aestimation%20strategy%20to%20facilitate%20a%20single-step%20reverse%20process%20and%20utilizes%0Alatent%20fusion%20concatenation%20to%20remove%20the%20necessity%20for%20multiple%20samples.%0AExtensive%20experiments%20indicate%20that%20SDSeg%20surpasses%20existing%20state-of-the-art%0Amethods%20on%20five%20benchmark%20datasets%20featuring%20diverse%20imaging%20modalities.%0ARemarkably%2C%20SDSeg%20is%20capable%20of%20generating%20stable%20predictions%20with%20a%20solitary%0Areverse%20step%20and%20sample%2C%20epitomizing%20the%20model%27s%20stability%20as%20implied%20by%20its%0Aname.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/lin-tianyu/Stable-Diffusion-Seg%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18361v1&entry.124074799=Read"},
{"title": "KAGNNs: Kolmogorov-Arnold Networks meet Graph Learning", "author": "Roman Bresson and Giannis Nikolentzos and George Panagopoulos and Michail Chatzianastasis and Jun Pang and Michalis Vazirgiannis", "abstract": "  In recent years, Graph Neural Networks (GNNs) have become the de facto tool\nfor learning node and graph representations. Most GNNs typically consist of a\nsequence of neighborhood aggregation (a.k.a., message passing) layers. Within\neach of these layers, the representation of each node is updated from an\naggregation and transformation of its neighbours representations at the\nprevious layer. The upper bound for the expressive power of message passing\nGNNs was reached through the use of MLPs as a transformation, due to their\nuniversal approximation capabilities. However, MLPs suffer from well-known\nlimitations, which recently motivated the introduction of Kolmogorov-Arnold\nNetworks (KANs). KANs rely on the Kolmogorov-Arnold representation theorem,\nrendering them a promising alternative to MLPs. In this work, we compare the\nperformance of KANs against that of MLPs in graph learning tasks. We perform\nextensive experiments on node classification, graph classification and graph\nregression datasets. Our preliminary results indicate that while KANs are\non-par with MLPs in classification tasks, they seem to have a clear advantage\nin the graph regression tasks.\n", "link": "http://arxiv.org/abs/2406.18380v1", "date": "2024-06-26", "relevancy": 2.2808, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4735}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4477}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KAGNNs%3A%20Kolmogorov-Arnold%20Networks%20meet%20Graph%20Learning&body=Title%3A%20KAGNNs%3A%20Kolmogorov-Arnold%20Networks%20meet%20Graph%20Learning%0AAuthor%3A%20Roman%20Bresson%20and%20Giannis%20Nikolentzos%20and%20George%20Panagopoulos%20and%20Michail%20Chatzianastasis%20and%20Jun%20Pang%20and%20Michalis%20Vazirgiannis%0AAbstract%3A%20%20%20In%20recent%20years%2C%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20become%20the%20de%20facto%20tool%0Afor%20learning%20node%20and%20graph%20representations.%20Most%20GNNs%20typically%20consist%20of%20a%0Asequence%20of%20neighborhood%20aggregation%20%28a.k.a.%2C%20message%20passing%29%20layers.%20Within%0Aeach%20of%20these%20layers%2C%20the%20representation%20of%20each%20node%20is%20updated%20from%20an%0Aaggregation%20and%20transformation%20of%20its%20neighbours%20representations%20at%20the%0Aprevious%20layer.%20The%20upper%20bound%20for%20the%20expressive%20power%20of%20message%20passing%0AGNNs%20was%20reached%20through%20the%20use%20of%20MLPs%20as%20a%20transformation%2C%20due%20to%20their%0Auniversal%20approximation%20capabilities.%20However%2C%20MLPs%20suffer%20from%20well-known%0Alimitations%2C%20which%20recently%20motivated%20the%20introduction%20of%20Kolmogorov-Arnold%0ANetworks%20%28KANs%29.%20KANs%20rely%20on%20the%20Kolmogorov-Arnold%20representation%20theorem%2C%0Arendering%20them%20a%20promising%20alternative%20to%20MLPs.%20In%20this%20work%2C%20we%20compare%20the%0Aperformance%20of%20KANs%20against%20that%20of%20MLPs%20in%20graph%20learning%20tasks.%20We%20perform%0Aextensive%20experiments%20on%20node%20classification%2C%20graph%20classification%20and%20graph%0Aregression%20datasets.%20Our%20preliminary%20results%20indicate%20that%20while%20KANs%20are%0Aon-par%20with%20MLPs%20in%20classification%20tasks%2C%20they%20seem%20to%20have%20a%20clear%20advantage%0Ain%20the%20graph%20regression%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18380v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKAGNNs%253A%2520Kolmogorov-Arnold%2520Networks%2520meet%2520Graph%2520Learning%26entry.906535625%3DRoman%2520Bresson%2520and%2520Giannis%2520Nikolentzos%2520and%2520George%2520Panagopoulos%2520and%2520Michail%2520Chatzianastasis%2520and%2520Jun%2520Pang%2520and%2520Michalis%2520Vazirgiannis%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520become%2520the%2520de%2520facto%2520tool%250Afor%2520learning%2520node%2520and%2520graph%2520representations.%2520Most%2520GNNs%2520typically%2520consist%2520of%2520a%250Asequence%2520of%2520neighborhood%2520aggregation%2520%2528a.k.a.%252C%2520message%2520passing%2529%2520layers.%2520Within%250Aeach%2520of%2520these%2520layers%252C%2520the%2520representation%2520of%2520each%2520node%2520is%2520updated%2520from%2520an%250Aaggregation%2520and%2520transformation%2520of%2520its%2520neighbours%2520representations%2520at%2520the%250Aprevious%2520layer.%2520The%2520upper%2520bound%2520for%2520the%2520expressive%2520power%2520of%2520message%2520passing%250AGNNs%2520was%2520reached%2520through%2520the%2520use%2520of%2520MLPs%2520as%2520a%2520transformation%252C%2520due%2520to%2520their%250Auniversal%2520approximation%2520capabilities.%2520However%252C%2520MLPs%2520suffer%2520from%2520well-known%250Alimitations%252C%2520which%2520recently%2520motivated%2520the%2520introduction%2520of%2520Kolmogorov-Arnold%250ANetworks%2520%2528KANs%2529.%2520KANs%2520rely%2520on%2520the%2520Kolmogorov-Arnold%2520representation%2520theorem%252C%250Arendering%2520them%2520a%2520promising%2520alternative%2520to%2520MLPs.%2520In%2520this%2520work%252C%2520we%2520compare%2520the%250Aperformance%2520of%2520KANs%2520against%2520that%2520of%2520MLPs%2520in%2520graph%2520learning%2520tasks.%2520We%2520perform%250Aextensive%2520experiments%2520on%2520node%2520classification%252C%2520graph%2520classification%2520and%2520graph%250Aregression%2520datasets.%2520Our%2520preliminary%2520results%2520indicate%2520that%2520while%2520KANs%2520are%250Aon-par%2520with%2520MLPs%2520in%2520classification%2520tasks%252C%2520they%2520seem%2520to%2520have%2520a%2520clear%2520advantage%250Ain%2520the%2520graph%2520regression%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18380v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KAGNNs%3A%20Kolmogorov-Arnold%20Networks%20meet%20Graph%20Learning&entry.906535625=Roman%20Bresson%20and%20Giannis%20Nikolentzos%20and%20George%20Panagopoulos%20and%20Michail%20Chatzianastasis%20and%20Jun%20Pang%20and%20Michalis%20Vazirgiannis&entry.1292438233=%20%20In%20recent%20years%2C%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20become%20the%20de%20facto%20tool%0Afor%20learning%20node%20and%20graph%20representations.%20Most%20GNNs%20typically%20consist%20of%20a%0Asequence%20of%20neighborhood%20aggregation%20%28a.k.a.%2C%20message%20passing%29%20layers.%20Within%0Aeach%20of%20these%20layers%2C%20the%20representation%20of%20each%20node%20is%20updated%20from%20an%0Aaggregation%20and%20transformation%20of%20its%20neighbours%20representations%20at%20the%0Aprevious%20layer.%20The%20upper%20bound%20for%20the%20expressive%20power%20of%20message%20passing%0AGNNs%20was%20reached%20through%20the%20use%20of%20MLPs%20as%20a%20transformation%2C%20due%20to%20their%0Auniversal%20approximation%20capabilities.%20However%2C%20MLPs%20suffer%20from%20well-known%0Alimitations%2C%20which%20recently%20motivated%20the%20introduction%20of%20Kolmogorov-Arnold%0ANetworks%20%28KANs%29.%20KANs%20rely%20on%20the%20Kolmogorov-Arnold%20representation%20theorem%2C%0Arendering%20them%20a%20promising%20alternative%20to%20MLPs.%20In%20this%20work%2C%20we%20compare%20the%0Aperformance%20of%20KANs%20against%20that%20of%20MLPs%20in%20graph%20learning%20tasks.%20We%20perform%0Aextensive%20experiments%20on%20node%20classification%2C%20graph%20classification%20and%20graph%0Aregression%20datasets.%20Our%20preliminary%20results%20indicate%20that%20while%20KANs%20are%0Aon-par%20with%20MLPs%20in%20classification%20tasks%2C%20they%20seem%20to%20have%20a%20clear%20advantage%0Ain%20the%20graph%20regression%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18380v1&entry.124074799=Read"},
{"title": "Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension\n  with Enhanced Visual Knowledge Alignment", "author": "Yunxin Li and Xinyu Chen and Baotian Hu and Haoyuan Shi and Min Zhang", "abstract": "  Evaluating and Rethinking the current landscape of Large Multimodal Models\n(LMMs), we observe that widely-used visual-language projection approaches\n(e.g., Q-former or MLP) focus on the alignment of image-text descriptions yet\nignore the visual knowledge-dimension alignment, i.e., connecting visuals to\ntheir relevant knowledge. Visual knowledge plays a significant role in\nanalyzing, inferring, and interpreting information from visuals, helping\nimprove the accuracy of answers to knowledge-based visual questions. In this\npaper, we mainly explore improving LMMs with visual-language knowledge\nalignment, especially aimed at challenging knowledge-based visual question\nanswering (VQA). To this end, we present a Cognitive Visual-Language Mapper\n(CVLM), which contains a pretrained Visual Knowledge Aligner (VKA) and a\nFine-grained Knowledge Adapter (FKA) used in the multimodal instruction tuning\nstage. Specifically, we design the VKA based on the interaction between a small\nlanguage model and a visual encoder, training it on collected image-knowledge\npairs to achieve visual knowledge acquisition and projection. FKA is employed\nto distill the fine-grained visual knowledge of an image and inject it into\nLarge Language Models (LLMs). We conduct extensive experiments on\nknowledge-based VQA benchmarks and experimental results show that CVLM\nsignificantly improves the performance of LMMs on knowledge-based VQA (average\ngain by 5.0%). Ablation studies also verify the effectiveness of VKA and FKA,\nrespectively. The codes are available at\nhttps://github.com/HITsz-TMG/Cognitive-Visual-Language-Mapper\n", "link": "http://arxiv.org/abs/2402.13561v2", "date": "2024-06-26", "relevancy": 2.2782, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6223}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5349}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cognitive%20Visual-Language%20Mapper%3A%20Advancing%20Multimodal%20Comprehension%0A%20%20with%20Enhanced%20Visual%20Knowledge%20Alignment&body=Title%3A%20Cognitive%20Visual-Language%20Mapper%3A%20Advancing%20Multimodal%20Comprehension%0A%20%20with%20Enhanced%20Visual%20Knowledge%20Alignment%0AAuthor%3A%20Yunxin%20Li%20and%20Xinyu%20Chen%20and%20Baotian%20Hu%20and%20Haoyuan%20Shi%20and%20Min%20Zhang%0AAbstract%3A%20%20%20Evaluating%20and%20Rethinking%20the%20current%20landscape%20of%20Large%20Multimodal%20Models%0A%28LMMs%29%2C%20we%20observe%20that%20widely-used%20visual-language%20projection%20approaches%0A%28e.g.%2C%20Q-former%20or%20MLP%29%20focus%20on%20the%20alignment%20of%20image-text%20descriptions%20yet%0Aignore%20the%20visual%20knowledge-dimension%20alignment%2C%20i.e.%2C%20connecting%20visuals%20to%0Atheir%20relevant%20knowledge.%20Visual%20knowledge%20plays%20a%20significant%20role%20in%0Aanalyzing%2C%20inferring%2C%20and%20interpreting%20information%20from%20visuals%2C%20helping%0Aimprove%20the%20accuracy%20of%20answers%20to%20knowledge-based%20visual%20questions.%20In%20this%0Apaper%2C%20we%20mainly%20explore%20improving%20LMMs%20with%20visual-language%20knowledge%0Aalignment%2C%20especially%20aimed%20at%20challenging%20knowledge-based%20visual%20question%0Aanswering%20%28VQA%29.%20To%20this%20end%2C%20we%20present%20a%20Cognitive%20Visual-Language%20Mapper%0A%28CVLM%29%2C%20which%20contains%20a%20pretrained%20Visual%20Knowledge%20Aligner%20%28VKA%29%20and%20a%0AFine-grained%20Knowledge%20Adapter%20%28FKA%29%20used%20in%20the%20multimodal%20instruction%20tuning%0Astage.%20Specifically%2C%20we%20design%20the%20VKA%20based%20on%20the%20interaction%20between%20a%20small%0Alanguage%20model%20and%20a%20visual%20encoder%2C%20training%20it%20on%20collected%20image-knowledge%0Apairs%20to%20achieve%20visual%20knowledge%20acquisition%20and%20projection.%20FKA%20is%20employed%0Ato%20distill%20the%20fine-grained%20visual%20knowledge%20of%20an%20image%20and%20inject%20it%20into%0ALarge%20Language%20Models%20%28LLMs%29.%20We%20conduct%20extensive%20experiments%20on%0Aknowledge-based%20VQA%20benchmarks%20and%20experimental%20results%20show%20that%20CVLM%0Asignificantly%20improves%20the%20performance%20of%20LMMs%20on%20knowledge-based%20VQA%20%28average%0Again%20by%205.0%25%29.%20Ablation%20studies%20also%20verify%20the%20effectiveness%20of%20VKA%20and%20FKA%2C%0Arespectively.%20The%20codes%20are%20available%20at%0Ahttps%3A//github.com/HITsz-TMG/Cognitive-Visual-Language-Mapper%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.13561v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCognitive%2520Visual-Language%2520Mapper%253A%2520Advancing%2520Multimodal%2520Comprehension%250A%2520%2520with%2520Enhanced%2520Visual%2520Knowledge%2520Alignment%26entry.906535625%3DYunxin%2520Li%2520and%2520Xinyu%2520Chen%2520and%2520Baotian%2520Hu%2520and%2520Haoyuan%2520Shi%2520and%2520Min%2520Zhang%26entry.1292438233%3D%2520%2520Evaluating%2520and%2520Rethinking%2520the%2520current%2520landscape%2520of%2520Large%2520Multimodal%2520Models%250A%2528LMMs%2529%252C%2520we%2520observe%2520that%2520widely-used%2520visual-language%2520projection%2520approaches%250A%2528e.g.%252C%2520Q-former%2520or%2520MLP%2529%2520focus%2520on%2520the%2520alignment%2520of%2520image-text%2520descriptions%2520yet%250Aignore%2520the%2520visual%2520knowledge-dimension%2520alignment%252C%2520i.e.%252C%2520connecting%2520visuals%2520to%250Atheir%2520relevant%2520knowledge.%2520Visual%2520knowledge%2520plays%2520a%2520significant%2520role%2520in%250Aanalyzing%252C%2520inferring%252C%2520and%2520interpreting%2520information%2520from%2520visuals%252C%2520helping%250Aimprove%2520the%2520accuracy%2520of%2520answers%2520to%2520knowledge-based%2520visual%2520questions.%2520In%2520this%250Apaper%252C%2520we%2520mainly%2520explore%2520improving%2520LMMs%2520with%2520visual-language%2520knowledge%250Aalignment%252C%2520especially%2520aimed%2520at%2520challenging%2520knowledge-based%2520visual%2520question%250Aanswering%2520%2528VQA%2529.%2520To%2520this%2520end%252C%2520we%2520present%2520a%2520Cognitive%2520Visual-Language%2520Mapper%250A%2528CVLM%2529%252C%2520which%2520contains%2520a%2520pretrained%2520Visual%2520Knowledge%2520Aligner%2520%2528VKA%2529%2520and%2520a%250AFine-grained%2520Knowledge%2520Adapter%2520%2528FKA%2529%2520used%2520in%2520the%2520multimodal%2520instruction%2520tuning%250Astage.%2520Specifically%252C%2520we%2520design%2520the%2520VKA%2520based%2520on%2520the%2520interaction%2520between%2520a%2520small%250Alanguage%2520model%2520and%2520a%2520visual%2520encoder%252C%2520training%2520it%2520on%2520collected%2520image-knowledge%250Apairs%2520to%2520achieve%2520visual%2520knowledge%2520acquisition%2520and%2520projection.%2520FKA%2520is%2520employed%250Ato%2520distill%2520the%2520fine-grained%2520visual%2520knowledge%2520of%2520an%2520image%2520and%2520inject%2520it%2520into%250ALarge%2520Language%2520Models%2520%2528LLMs%2529.%2520We%2520conduct%2520extensive%2520experiments%2520on%250Aknowledge-based%2520VQA%2520benchmarks%2520and%2520experimental%2520results%2520show%2520that%2520CVLM%250Asignificantly%2520improves%2520the%2520performance%2520of%2520LMMs%2520on%2520knowledge-based%2520VQA%2520%2528average%250Again%2520by%25205.0%2525%2529.%2520Ablation%2520studies%2520also%2520verify%2520the%2520effectiveness%2520of%2520VKA%2520and%2520FKA%252C%250Arespectively.%2520The%2520codes%2520are%2520available%2520at%250Ahttps%253A//github.com/HITsz-TMG/Cognitive-Visual-Language-Mapper%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.13561v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cognitive%20Visual-Language%20Mapper%3A%20Advancing%20Multimodal%20Comprehension%0A%20%20with%20Enhanced%20Visual%20Knowledge%20Alignment&entry.906535625=Yunxin%20Li%20and%20Xinyu%20Chen%20and%20Baotian%20Hu%20and%20Haoyuan%20Shi%20and%20Min%20Zhang&entry.1292438233=%20%20Evaluating%20and%20Rethinking%20the%20current%20landscape%20of%20Large%20Multimodal%20Models%0A%28LMMs%29%2C%20we%20observe%20that%20widely-used%20visual-language%20projection%20approaches%0A%28e.g.%2C%20Q-former%20or%20MLP%29%20focus%20on%20the%20alignment%20of%20image-text%20descriptions%20yet%0Aignore%20the%20visual%20knowledge-dimension%20alignment%2C%20i.e.%2C%20connecting%20visuals%20to%0Atheir%20relevant%20knowledge.%20Visual%20knowledge%20plays%20a%20significant%20role%20in%0Aanalyzing%2C%20inferring%2C%20and%20interpreting%20information%20from%20visuals%2C%20helping%0Aimprove%20the%20accuracy%20of%20answers%20to%20knowledge-based%20visual%20questions.%20In%20this%0Apaper%2C%20we%20mainly%20explore%20improving%20LMMs%20with%20visual-language%20knowledge%0Aalignment%2C%20especially%20aimed%20at%20challenging%20knowledge-based%20visual%20question%0Aanswering%20%28VQA%29.%20To%20this%20end%2C%20we%20present%20a%20Cognitive%20Visual-Language%20Mapper%0A%28CVLM%29%2C%20which%20contains%20a%20pretrained%20Visual%20Knowledge%20Aligner%20%28VKA%29%20and%20a%0AFine-grained%20Knowledge%20Adapter%20%28FKA%29%20used%20in%20the%20multimodal%20instruction%20tuning%0Astage.%20Specifically%2C%20we%20design%20the%20VKA%20based%20on%20the%20interaction%20between%20a%20small%0Alanguage%20model%20and%20a%20visual%20encoder%2C%20training%20it%20on%20collected%20image-knowledge%0Apairs%20to%20achieve%20visual%20knowledge%20acquisition%20and%20projection.%20FKA%20is%20employed%0Ato%20distill%20the%20fine-grained%20visual%20knowledge%20of%20an%20image%20and%20inject%20it%20into%0ALarge%20Language%20Models%20%28LLMs%29.%20We%20conduct%20extensive%20experiments%20on%0Aknowledge-based%20VQA%20benchmarks%20and%20experimental%20results%20show%20that%20CVLM%0Asignificantly%20improves%20the%20performance%20of%20LMMs%20on%20knowledge-based%20VQA%20%28average%0Again%20by%205.0%25%29.%20Ablation%20studies%20also%20verify%20the%20effectiveness%20of%20VKA%20and%20FKA%2C%0Arespectively.%20The%20codes%20are%20available%20at%0Ahttps%3A//github.com/HITsz-TMG/Cognitive-Visual-Language-Mapper%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.13561v2&entry.124074799=Read"},
{"title": "Commonsense Prototype for Outdoor Unsupervised 3D Object Detection", "author": "Hai Wu and Shijia Zhao and Xun Huang and Chenglu Wen and Xin Li and Cheng Wang", "abstract": "  The prevalent approaches of unsupervised 3D object detection follow\ncluster-based pseudo-label generation and iterative self-training processes.\nHowever, the challenge arises due to the sparsity of LiDAR scans, which leads\nto pseudo-labels with erroneous size and position, resulting in subpar\ndetection performance. To tackle this problem, this paper introduces a\nCommonsense Prototype-based Detector, termed CPD, for unsupervised 3D object\ndetection. CPD first constructs Commonsense Prototype (CProto) characterized by\nhigh-quality bounding box and dense points, based on commonsense intuition.\nSubsequently, CPD refines the low-quality pseudo-labels by leveraging the size\nprior from CProto. Furthermore, CPD enhances the detection accuracy of sparsely\nscanned objects by the geometric knowledge from CProto. CPD outperforms\nstate-of-the-art unsupervised 3D detectors on Waymo Open Dataset (WOD),\nPandaSet, and KITTI datasets by a large margin. Besides, by training CPD on WOD\nand testing on KITTI, CPD attains 90.85% and 81.01% 3D Average Precision on\neasy and moderate car classes, respectively. These achievements position CPD in\nclose proximity to fully supervised detectors, highlighting the significance of\nour method. The code will be available at https://github.com/hailanyi/CPD.\n", "link": "http://arxiv.org/abs/2404.16493v3", "date": "2024-06-26", "relevancy": 2.2754, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5843}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5756}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Commonsense%20Prototype%20for%20Outdoor%20Unsupervised%203D%20Object%20Detection&body=Title%3A%20Commonsense%20Prototype%20for%20Outdoor%20Unsupervised%203D%20Object%20Detection%0AAuthor%3A%20Hai%20Wu%20and%20Shijia%20Zhao%20and%20Xun%20Huang%20and%20Chenglu%20Wen%20and%20Xin%20Li%20and%20Cheng%20Wang%0AAbstract%3A%20%20%20The%20prevalent%20approaches%20of%20unsupervised%203D%20object%20detection%20follow%0Acluster-based%20pseudo-label%20generation%20and%20iterative%20self-training%20processes.%0AHowever%2C%20the%20challenge%20arises%20due%20to%20the%20sparsity%20of%20LiDAR%20scans%2C%20which%20leads%0Ato%20pseudo-labels%20with%20erroneous%20size%20and%20position%2C%20resulting%20in%20subpar%0Adetection%20performance.%20To%20tackle%20this%20problem%2C%20this%20paper%20introduces%20a%0ACommonsense%20Prototype-based%20Detector%2C%20termed%20CPD%2C%20for%20unsupervised%203D%20object%0Adetection.%20CPD%20first%20constructs%20Commonsense%20Prototype%20%28CProto%29%20characterized%20by%0Ahigh-quality%20bounding%20box%20and%20dense%20points%2C%20based%20on%20commonsense%20intuition.%0ASubsequently%2C%20CPD%20refines%20the%20low-quality%20pseudo-labels%20by%20leveraging%20the%20size%0Aprior%20from%20CProto.%20Furthermore%2C%20CPD%20enhances%20the%20detection%20accuracy%20of%20sparsely%0Ascanned%20objects%20by%20the%20geometric%20knowledge%20from%20CProto.%20CPD%20outperforms%0Astate-of-the-art%20unsupervised%203D%20detectors%20on%20Waymo%20Open%20Dataset%20%28WOD%29%2C%0APandaSet%2C%20and%20KITTI%20datasets%20by%20a%20large%20margin.%20Besides%2C%20by%20training%20CPD%20on%20WOD%0Aand%20testing%20on%20KITTI%2C%20CPD%20attains%2090.85%25%20and%2081.01%25%203D%20Average%20Precision%20on%0Aeasy%20and%20moderate%20car%20classes%2C%20respectively.%20These%20achievements%20position%20CPD%20in%0Aclose%20proximity%20to%20fully%20supervised%20detectors%2C%20highlighting%20the%20significance%20of%0Aour%20method.%20The%20code%20will%20be%20available%20at%20https%3A//github.com/hailanyi/CPD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16493v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCommonsense%2520Prototype%2520for%2520Outdoor%2520Unsupervised%25203D%2520Object%2520Detection%26entry.906535625%3DHai%2520Wu%2520and%2520Shijia%2520Zhao%2520and%2520Xun%2520Huang%2520and%2520Chenglu%2520Wen%2520and%2520Xin%2520Li%2520and%2520Cheng%2520Wang%26entry.1292438233%3D%2520%2520The%2520prevalent%2520approaches%2520of%2520unsupervised%25203D%2520object%2520detection%2520follow%250Acluster-based%2520pseudo-label%2520generation%2520and%2520iterative%2520self-training%2520processes.%250AHowever%252C%2520the%2520challenge%2520arises%2520due%2520to%2520the%2520sparsity%2520of%2520LiDAR%2520scans%252C%2520which%2520leads%250Ato%2520pseudo-labels%2520with%2520erroneous%2520size%2520and%2520position%252C%2520resulting%2520in%2520subpar%250Adetection%2520performance.%2520To%2520tackle%2520this%2520problem%252C%2520this%2520paper%2520introduces%2520a%250ACommonsense%2520Prototype-based%2520Detector%252C%2520termed%2520CPD%252C%2520for%2520unsupervised%25203D%2520object%250Adetection.%2520CPD%2520first%2520constructs%2520Commonsense%2520Prototype%2520%2528CProto%2529%2520characterized%2520by%250Ahigh-quality%2520bounding%2520box%2520and%2520dense%2520points%252C%2520based%2520on%2520commonsense%2520intuition.%250ASubsequently%252C%2520CPD%2520refines%2520the%2520low-quality%2520pseudo-labels%2520by%2520leveraging%2520the%2520size%250Aprior%2520from%2520CProto.%2520Furthermore%252C%2520CPD%2520enhances%2520the%2520detection%2520accuracy%2520of%2520sparsely%250Ascanned%2520objects%2520by%2520the%2520geometric%2520knowledge%2520from%2520CProto.%2520CPD%2520outperforms%250Astate-of-the-art%2520unsupervised%25203D%2520detectors%2520on%2520Waymo%2520Open%2520Dataset%2520%2528WOD%2529%252C%250APandaSet%252C%2520and%2520KITTI%2520datasets%2520by%2520a%2520large%2520margin.%2520Besides%252C%2520by%2520training%2520CPD%2520on%2520WOD%250Aand%2520testing%2520on%2520KITTI%252C%2520CPD%2520attains%252090.85%2525%2520and%252081.01%2525%25203D%2520Average%2520Precision%2520on%250Aeasy%2520and%2520moderate%2520car%2520classes%252C%2520respectively.%2520These%2520achievements%2520position%2520CPD%2520in%250Aclose%2520proximity%2520to%2520fully%2520supervised%2520detectors%252C%2520highlighting%2520the%2520significance%2520of%250Aour%2520method.%2520The%2520code%2520will%2520be%2520available%2520at%2520https%253A//github.com/hailanyi/CPD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.16493v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Commonsense%20Prototype%20for%20Outdoor%20Unsupervised%203D%20Object%20Detection&entry.906535625=Hai%20Wu%20and%20Shijia%20Zhao%20and%20Xun%20Huang%20and%20Chenglu%20Wen%20and%20Xin%20Li%20and%20Cheng%20Wang&entry.1292438233=%20%20The%20prevalent%20approaches%20of%20unsupervised%203D%20object%20detection%20follow%0Acluster-based%20pseudo-label%20generation%20and%20iterative%20self-training%20processes.%0AHowever%2C%20the%20challenge%20arises%20due%20to%20the%20sparsity%20of%20LiDAR%20scans%2C%20which%20leads%0Ato%20pseudo-labels%20with%20erroneous%20size%20and%20position%2C%20resulting%20in%20subpar%0Adetection%20performance.%20To%20tackle%20this%20problem%2C%20this%20paper%20introduces%20a%0ACommonsense%20Prototype-based%20Detector%2C%20termed%20CPD%2C%20for%20unsupervised%203D%20object%0Adetection.%20CPD%20first%20constructs%20Commonsense%20Prototype%20%28CProto%29%20characterized%20by%0Ahigh-quality%20bounding%20box%20and%20dense%20points%2C%20based%20on%20commonsense%20intuition.%0ASubsequently%2C%20CPD%20refines%20the%20low-quality%20pseudo-labels%20by%20leveraging%20the%20size%0Aprior%20from%20CProto.%20Furthermore%2C%20CPD%20enhances%20the%20detection%20accuracy%20of%20sparsely%0Ascanned%20objects%20by%20the%20geometric%20knowledge%20from%20CProto.%20CPD%20outperforms%0Astate-of-the-art%20unsupervised%203D%20detectors%20on%20Waymo%20Open%20Dataset%20%28WOD%29%2C%0APandaSet%2C%20and%20KITTI%20datasets%20by%20a%20large%20margin.%20Besides%2C%20by%20training%20CPD%20on%20WOD%0Aand%20testing%20on%20KITTI%2C%20CPD%20attains%2090.85%25%20and%2081.01%25%203D%20Average%20Precision%20on%0Aeasy%20and%20moderate%20car%20classes%2C%20respectively.%20These%20achievements%20position%20CPD%20in%0Aclose%20proximity%20to%20fully%20supervised%20detectors%2C%20highlighting%20the%20significance%20of%0Aour%20method.%20The%20code%20will%20be%20available%20at%20https%3A//github.com/hailanyi/CPD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16493v3&entry.124074799=Read"},
{"title": "SALUDA: Surface-based Automotive Lidar Unsupervised Domain Adaptation", "author": "Bj\u00f6rn Michele and Alexandre Boulch and Gilles Puy and Tuan-Hung Vu and Renaud Marlet and Nicolas Courty", "abstract": "  Learning models on one labeled dataset that generalize well on another domain\nis a difficult task, as several shifts might happen between the data domains.\nThis is notably the case for lidar data, for which models can exhibit large\nperformance discrepancies due for instance to different lidar patterns or\nchanges in acquisition conditions. This paper addresses the corresponding\nUnsupervised Domain Adaptation (UDA) task for semantic segmentation. To\nmitigate this problem, we introduce an unsupervised auxiliary task of learning\nan implicit underlying surface representation simultaneously on source and\ntarget data. As both domains share the same latent representation, the model is\nforced to accommodate discrepancies between the two sources of data. This novel\nstrategy differs from classical minimization of statistical divergences or\nlidar-specific domain adaptation techniques. Our experiments demonstrate that\nour method achieves a better performance than the current state of the art,\nboth in real-to-real and synthetic-to-real scenarios.\n", "link": "http://arxiv.org/abs/2304.03251v4", "date": "2024-06-26", "relevancy": 2.2682, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5726}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5636}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SALUDA%3A%20Surface-based%20Automotive%20Lidar%20Unsupervised%20Domain%20Adaptation&body=Title%3A%20SALUDA%3A%20Surface-based%20Automotive%20Lidar%20Unsupervised%20Domain%20Adaptation%0AAuthor%3A%20Bj%C3%B6rn%20Michele%20and%20Alexandre%20Boulch%20and%20Gilles%20Puy%20and%20Tuan-Hung%20Vu%20and%20Renaud%20Marlet%20and%20Nicolas%20Courty%0AAbstract%3A%20%20%20Learning%20models%20on%20one%20labeled%20dataset%20that%20generalize%20well%20on%20another%20domain%0Ais%20a%20difficult%20task%2C%20as%20several%20shifts%20might%20happen%20between%20the%20data%20domains.%0AThis%20is%20notably%20the%20case%20for%20lidar%20data%2C%20for%20which%20models%20can%20exhibit%20large%0Aperformance%20discrepancies%20due%20for%20instance%20to%20different%20lidar%20patterns%20or%0Achanges%20in%20acquisition%20conditions.%20This%20paper%20addresses%20the%20corresponding%0AUnsupervised%20Domain%20Adaptation%20%28UDA%29%20task%20for%20semantic%20segmentation.%20To%0Amitigate%20this%20problem%2C%20we%20introduce%20an%20unsupervised%20auxiliary%20task%20of%20learning%0Aan%20implicit%20underlying%20surface%20representation%20simultaneously%20on%20source%20and%0Atarget%20data.%20As%20both%20domains%20share%20the%20same%20latent%20representation%2C%20the%20model%20is%0Aforced%20to%20accommodate%20discrepancies%20between%20the%20two%20sources%20of%20data.%20This%20novel%0Astrategy%20differs%20from%20classical%20minimization%20of%20statistical%20divergences%20or%0Alidar-specific%20domain%20adaptation%20techniques.%20Our%20experiments%20demonstrate%20that%0Aour%20method%20achieves%20a%20better%20performance%20than%20the%20current%20state%20of%20the%20art%2C%0Aboth%20in%20real-to-real%20and%20synthetic-to-real%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.03251v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSALUDA%253A%2520Surface-based%2520Automotive%2520Lidar%2520Unsupervised%2520Domain%2520Adaptation%26entry.906535625%3DBj%25C3%25B6rn%2520Michele%2520and%2520Alexandre%2520Boulch%2520and%2520Gilles%2520Puy%2520and%2520Tuan-Hung%2520Vu%2520and%2520Renaud%2520Marlet%2520and%2520Nicolas%2520Courty%26entry.1292438233%3D%2520%2520Learning%2520models%2520on%2520one%2520labeled%2520dataset%2520that%2520generalize%2520well%2520on%2520another%2520domain%250Ais%2520a%2520difficult%2520task%252C%2520as%2520several%2520shifts%2520might%2520happen%2520between%2520the%2520data%2520domains.%250AThis%2520is%2520notably%2520the%2520case%2520for%2520lidar%2520data%252C%2520for%2520which%2520models%2520can%2520exhibit%2520large%250Aperformance%2520discrepancies%2520due%2520for%2520instance%2520to%2520different%2520lidar%2520patterns%2520or%250Achanges%2520in%2520acquisition%2520conditions.%2520This%2520paper%2520addresses%2520the%2520corresponding%250AUnsupervised%2520Domain%2520Adaptation%2520%2528UDA%2529%2520task%2520for%2520semantic%2520segmentation.%2520To%250Amitigate%2520this%2520problem%252C%2520we%2520introduce%2520an%2520unsupervised%2520auxiliary%2520task%2520of%2520learning%250Aan%2520implicit%2520underlying%2520surface%2520representation%2520simultaneously%2520on%2520source%2520and%250Atarget%2520data.%2520As%2520both%2520domains%2520share%2520the%2520same%2520latent%2520representation%252C%2520the%2520model%2520is%250Aforced%2520to%2520accommodate%2520discrepancies%2520between%2520the%2520two%2520sources%2520of%2520data.%2520This%2520novel%250Astrategy%2520differs%2520from%2520classical%2520minimization%2520of%2520statistical%2520divergences%2520or%250Alidar-specific%2520domain%2520adaptation%2520techniques.%2520Our%2520experiments%2520demonstrate%2520that%250Aour%2520method%2520achieves%2520a%2520better%2520performance%2520than%2520the%2520current%2520state%2520of%2520the%2520art%252C%250Aboth%2520in%2520real-to-real%2520and%2520synthetic-to-real%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.03251v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SALUDA%3A%20Surface-based%20Automotive%20Lidar%20Unsupervised%20Domain%20Adaptation&entry.906535625=Bj%C3%B6rn%20Michele%20and%20Alexandre%20Boulch%20and%20Gilles%20Puy%20and%20Tuan-Hung%20Vu%20and%20Renaud%20Marlet%20and%20Nicolas%20Courty&entry.1292438233=%20%20Learning%20models%20on%20one%20labeled%20dataset%20that%20generalize%20well%20on%20another%20domain%0Ais%20a%20difficult%20task%2C%20as%20several%20shifts%20might%20happen%20between%20the%20data%20domains.%0AThis%20is%20notably%20the%20case%20for%20lidar%20data%2C%20for%20which%20models%20can%20exhibit%20large%0Aperformance%20discrepancies%20due%20for%20instance%20to%20different%20lidar%20patterns%20or%0Achanges%20in%20acquisition%20conditions.%20This%20paper%20addresses%20the%20corresponding%0AUnsupervised%20Domain%20Adaptation%20%28UDA%29%20task%20for%20semantic%20segmentation.%20To%0Amitigate%20this%20problem%2C%20we%20introduce%20an%20unsupervised%20auxiliary%20task%20of%20learning%0Aan%20implicit%20underlying%20surface%20representation%20simultaneously%20on%20source%20and%0Atarget%20data.%20As%20both%20domains%20share%20the%20same%20latent%20representation%2C%20the%20model%20is%0Aforced%20to%20accommodate%20discrepancies%20between%20the%20two%20sources%20of%20data.%20This%20novel%0Astrategy%20differs%20from%20classical%20minimization%20of%20statistical%20divergences%20or%0Alidar-specific%20domain%20adaptation%20techniques.%20Our%20experiments%20demonstrate%20that%0Aour%20method%20achieves%20a%20better%20performance%20than%20the%20current%20state%20of%20the%20art%2C%0Aboth%20in%20real-to-real%20and%20synthetic-to-real%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.03251v4&entry.124074799=Read"},
{"title": "Kolmogorov-Arnold Graph Neural Networks", "author": "Gianluca De Carlo and Andrea Mastropietro and Aris Anagnostopoulos", "abstract": "  Graph neural networks (GNNs) excel in learning from network-like data but\noften lack interpretability, making their application challenging in domains\nrequiring transparent decision-making. We propose the Graph Kolmogorov-Arnold\nNetwork (GKAN), a novel GNN model leveraging spline-based activation functions\non edges to enhance both accuracy and interpretability. Our experiments on five\nbenchmark datasets demonstrate that GKAN outperforms state-of-the-art GNN\nmodels in node classification, link prediction, and graph classification tasks.\nIn addition to the improved accuracy, GKAN's design inherently provides clear\ninsights into the model's decision-making process, eliminating the need for\npost-hoc explainability techniques. This paper discusses the methodology,\nperformance, and interpretability of GKAN, highlighting its potential for\napplications in domains where interpretability is crucial.\n", "link": "http://arxiv.org/abs/2406.18354v1", "date": "2024-06-26", "relevancy": 2.2643, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4558}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4541}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kolmogorov-Arnold%20Graph%20Neural%20Networks&body=Title%3A%20Kolmogorov-Arnold%20Graph%20Neural%20Networks%0AAuthor%3A%20Gianluca%20De%20Carlo%20and%20Andrea%20Mastropietro%20and%20Aris%20Anagnostopoulos%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20excel%20in%20learning%20from%20network-like%20data%20but%0Aoften%20lack%20interpretability%2C%20making%20their%20application%20challenging%20in%20domains%0Arequiring%20transparent%20decision-making.%20We%20propose%20the%20Graph%20Kolmogorov-Arnold%0ANetwork%20%28GKAN%29%2C%20a%20novel%20GNN%20model%20leveraging%20spline-based%20activation%20functions%0Aon%20edges%20to%20enhance%20both%20accuracy%20and%20interpretability.%20Our%20experiments%20on%20five%0Abenchmark%20datasets%20demonstrate%20that%20GKAN%20outperforms%20state-of-the-art%20GNN%0Amodels%20in%20node%20classification%2C%20link%20prediction%2C%20and%20graph%20classification%20tasks.%0AIn%20addition%20to%20the%20improved%20accuracy%2C%20GKAN%27s%20design%20inherently%20provides%20clear%0Ainsights%20into%20the%20model%27s%20decision-making%20process%2C%20eliminating%20the%20need%20for%0Apost-hoc%20explainability%20techniques.%20This%20paper%20discusses%20the%20methodology%2C%0Aperformance%2C%20and%20interpretability%20of%20GKAN%2C%20highlighting%20its%20potential%20for%0Aapplications%20in%20domains%20where%20interpretability%20is%20crucial.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18354v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKolmogorov-Arnold%2520Graph%2520Neural%2520Networks%26entry.906535625%3DGianluca%2520De%2520Carlo%2520and%2520Andrea%2520Mastropietro%2520and%2520Aris%2520Anagnostopoulos%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520excel%2520in%2520learning%2520from%2520network-like%2520data%2520but%250Aoften%2520lack%2520interpretability%252C%2520making%2520their%2520application%2520challenging%2520in%2520domains%250Arequiring%2520transparent%2520decision-making.%2520We%2520propose%2520the%2520Graph%2520Kolmogorov-Arnold%250ANetwork%2520%2528GKAN%2529%252C%2520a%2520novel%2520GNN%2520model%2520leveraging%2520spline-based%2520activation%2520functions%250Aon%2520edges%2520to%2520enhance%2520both%2520accuracy%2520and%2520interpretability.%2520Our%2520experiments%2520on%2520five%250Abenchmark%2520datasets%2520demonstrate%2520that%2520GKAN%2520outperforms%2520state-of-the-art%2520GNN%250Amodels%2520in%2520node%2520classification%252C%2520link%2520prediction%252C%2520and%2520graph%2520classification%2520tasks.%250AIn%2520addition%2520to%2520the%2520improved%2520accuracy%252C%2520GKAN%2527s%2520design%2520inherently%2520provides%2520clear%250Ainsights%2520into%2520the%2520model%2527s%2520decision-making%2520process%252C%2520eliminating%2520the%2520need%2520for%250Apost-hoc%2520explainability%2520techniques.%2520This%2520paper%2520discusses%2520the%2520methodology%252C%250Aperformance%252C%2520and%2520interpretability%2520of%2520GKAN%252C%2520highlighting%2520its%2520potential%2520for%250Aapplications%2520in%2520domains%2520where%2520interpretability%2520is%2520crucial.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18354v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kolmogorov-Arnold%20Graph%20Neural%20Networks&entry.906535625=Gianluca%20De%20Carlo%20and%20Andrea%20Mastropietro%20and%20Aris%20Anagnostopoulos&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20excel%20in%20learning%20from%20network-like%20data%20but%0Aoften%20lack%20interpretability%2C%20making%20their%20application%20challenging%20in%20domains%0Arequiring%20transparent%20decision-making.%20We%20propose%20the%20Graph%20Kolmogorov-Arnold%0ANetwork%20%28GKAN%29%2C%20a%20novel%20GNN%20model%20leveraging%20spline-based%20activation%20functions%0Aon%20edges%20to%20enhance%20both%20accuracy%20and%20interpretability.%20Our%20experiments%20on%20five%0Abenchmark%20datasets%20demonstrate%20that%20GKAN%20outperforms%20state-of-the-art%20GNN%0Amodels%20in%20node%20classification%2C%20link%20prediction%2C%20and%20graph%20classification%20tasks.%0AIn%20addition%20to%20the%20improved%20accuracy%2C%20GKAN%27s%20design%20inherently%20provides%20clear%0Ainsights%20into%20the%20model%27s%20decision-making%20process%2C%20eliminating%20the%20need%20for%0Apost-hoc%20explainability%20techniques.%20This%20paper%20discusses%20the%20methodology%2C%0Aperformance%2C%20and%20interpretability%20of%20GKAN%2C%20highlighting%20its%20potential%20for%0Aapplications%20in%20domains%20where%20interpretability%20is%20crucial.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18354v1&entry.124074799=Read"},
{"title": "Towards Synchronous Memorizability and Generalizability with\n  Site-Modulated Diffusion Replay for Cross-Site Continual Segmentation", "author": "Dunyuan Xu and Xi Wang and Jingyang Zhang and Pheng-Ann Heng", "abstract": "  The ability to learn sequentially from different data sites is crucial for a\ndeep network in solving practical medical image diagnosis problems due to\nprivacy restrictions and storage limitations. However, adapting on incoming\nsite leads to catastrophic forgetting on past sites and decreases\ngeneralizablity on unseen sites. Existing Continual Learning (CL) and Domain\nGeneralization (DG) methods have been proposed to solve these two challenges\nrespectively, but none of them can address both simultaneously. Recognizing\nthis limitation, this paper proposes a novel training paradigm, learning\ntowards Synchronous Memorizability and Generalizability (SMG-Learning). To\nachieve this, we create the orientational gradient alignment to ensure\nmemorizability on previous sites, and arbitrary gradient alignment to enhance\ngeneralizability on unseen sites. This approach is named as Parallel Gradient\nAlignment (PGA). Furthermore, we approximate the PGA as dual meta-objectives\nusing the first-order Taylor expansion to reduce computational cost of aligning\ngradients. Considering that performing gradient alignments, especially for\nprevious sites, is not feasible due to the privacy constraints, we design a\nSite-Modulated Diffusion (SMD) model to generate images with site-specific\nlearnable prompts, replaying images have similar data distributions as previous\nsites. We evaluate our method on two medical image segmentation tasks, where\ndata from different sites arrive sequentially. Experimental results show that\nour method efficiently enhances both memorizability and generalizablity better\nthan other state-of-the-art methods, delivering satisfactory performance across\nall sites. Our code will be available at:\nhttps://github.com/dyxu-cuhkcse/SMG-Learning.\n", "link": "http://arxiv.org/abs/2406.18037v1", "date": "2024-06-26", "relevancy": 2.2587, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5778}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5765}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Synchronous%20Memorizability%20and%20Generalizability%20with%0A%20%20Site-Modulated%20Diffusion%20Replay%20for%20Cross-Site%20Continual%20Segmentation&body=Title%3A%20Towards%20Synchronous%20Memorizability%20and%20Generalizability%20with%0A%20%20Site-Modulated%20Diffusion%20Replay%20for%20Cross-Site%20Continual%20Segmentation%0AAuthor%3A%20Dunyuan%20Xu%20and%20Xi%20Wang%20and%20Jingyang%20Zhang%20and%20Pheng-Ann%20Heng%0AAbstract%3A%20%20%20The%20ability%20to%20learn%20sequentially%20from%20different%20data%20sites%20is%20crucial%20for%20a%0Adeep%20network%20in%20solving%20practical%20medical%20image%20diagnosis%20problems%20due%20to%0Aprivacy%20restrictions%20and%20storage%20limitations.%20However%2C%20adapting%20on%20incoming%0Asite%20leads%20to%20catastrophic%20forgetting%20on%20past%20sites%20and%20decreases%0Ageneralizablity%20on%20unseen%20sites.%20Existing%20Continual%20Learning%20%28CL%29%20and%20Domain%0AGeneralization%20%28DG%29%20methods%20have%20been%20proposed%20to%20solve%20these%20two%20challenges%0Arespectively%2C%20but%20none%20of%20them%20can%20address%20both%20simultaneously.%20Recognizing%0Athis%20limitation%2C%20this%20paper%20proposes%20a%20novel%20training%20paradigm%2C%20learning%0Atowards%20Synchronous%20Memorizability%20and%20Generalizability%20%28SMG-Learning%29.%20To%0Aachieve%20this%2C%20we%20create%20the%20orientational%20gradient%20alignment%20to%20ensure%0Amemorizability%20on%20previous%20sites%2C%20and%20arbitrary%20gradient%20alignment%20to%20enhance%0Ageneralizability%20on%20unseen%20sites.%20This%20approach%20is%20named%20as%20Parallel%20Gradient%0AAlignment%20%28PGA%29.%20Furthermore%2C%20we%20approximate%20the%20PGA%20as%20dual%20meta-objectives%0Ausing%20the%20first-order%20Taylor%20expansion%20to%20reduce%20computational%20cost%20of%20aligning%0Agradients.%20Considering%20that%20performing%20gradient%20alignments%2C%20especially%20for%0Aprevious%20sites%2C%20is%20not%20feasible%20due%20to%20the%20privacy%20constraints%2C%20we%20design%20a%0ASite-Modulated%20Diffusion%20%28SMD%29%20model%20to%20generate%20images%20with%20site-specific%0Alearnable%20prompts%2C%20replaying%20images%20have%20similar%20data%20distributions%20as%20previous%0Asites.%20We%20evaluate%20our%20method%20on%20two%20medical%20image%20segmentation%20tasks%2C%20where%0Adata%20from%20different%20sites%20arrive%20sequentially.%20Experimental%20results%20show%20that%0Aour%20method%20efficiently%20enhances%20both%20memorizability%20and%20generalizablity%20better%0Athan%20other%20state-of-the-art%20methods%2C%20delivering%20satisfactory%20performance%20across%0Aall%20sites.%20Our%20code%20will%20be%20available%20at%3A%0Ahttps%3A//github.com/dyxu-cuhkcse/SMG-Learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18037v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Synchronous%2520Memorizability%2520and%2520Generalizability%2520with%250A%2520%2520Site-Modulated%2520Diffusion%2520Replay%2520for%2520Cross-Site%2520Continual%2520Segmentation%26entry.906535625%3DDunyuan%2520Xu%2520and%2520Xi%2520Wang%2520and%2520Jingyang%2520Zhang%2520and%2520Pheng-Ann%2520Heng%26entry.1292438233%3D%2520%2520The%2520ability%2520to%2520learn%2520sequentially%2520from%2520different%2520data%2520sites%2520is%2520crucial%2520for%2520a%250Adeep%2520network%2520in%2520solving%2520practical%2520medical%2520image%2520diagnosis%2520problems%2520due%2520to%250Aprivacy%2520restrictions%2520and%2520storage%2520limitations.%2520However%252C%2520adapting%2520on%2520incoming%250Asite%2520leads%2520to%2520catastrophic%2520forgetting%2520on%2520past%2520sites%2520and%2520decreases%250Ageneralizablity%2520on%2520unseen%2520sites.%2520Existing%2520Continual%2520Learning%2520%2528CL%2529%2520and%2520Domain%250AGeneralization%2520%2528DG%2529%2520methods%2520have%2520been%2520proposed%2520to%2520solve%2520these%2520two%2520challenges%250Arespectively%252C%2520but%2520none%2520of%2520them%2520can%2520address%2520both%2520simultaneously.%2520Recognizing%250Athis%2520limitation%252C%2520this%2520paper%2520proposes%2520a%2520novel%2520training%2520paradigm%252C%2520learning%250Atowards%2520Synchronous%2520Memorizability%2520and%2520Generalizability%2520%2528SMG-Learning%2529.%2520To%250Aachieve%2520this%252C%2520we%2520create%2520the%2520orientational%2520gradient%2520alignment%2520to%2520ensure%250Amemorizability%2520on%2520previous%2520sites%252C%2520and%2520arbitrary%2520gradient%2520alignment%2520to%2520enhance%250Ageneralizability%2520on%2520unseen%2520sites.%2520This%2520approach%2520is%2520named%2520as%2520Parallel%2520Gradient%250AAlignment%2520%2528PGA%2529.%2520Furthermore%252C%2520we%2520approximate%2520the%2520PGA%2520as%2520dual%2520meta-objectives%250Ausing%2520the%2520first-order%2520Taylor%2520expansion%2520to%2520reduce%2520computational%2520cost%2520of%2520aligning%250Agradients.%2520Considering%2520that%2520performing%2520gradient%2520alignments%252C%2520especially%2520for%250Aprevious%2520sites%252C%2520is%2520not%2520feasible%2520due%2520to%2520the%2520privacy%2520constraints%252C%2520we%2520design%2520a%250ASite-Modulated%2520Diffusion%2520%2528SMD%2529%2520model%2520to%2520generate%2520images%2520with%2520site-specific%250Alearnable%2520prompts%252C%2520replaying%2520images%2520have%2520similar%2520data%2520distributions%2520as%2520previous%250Asites.%2520We%2520evaluate%2520our%2520method%2520on%2520two%2520medical%2520image%2520segmentation%2520tasks%252C%2520where%250Adata%2520from%2520different%2520sites%2520arrive%2520sequentially.%2520Experimental%2520results%2520show%2520that%250Aour%2520method%2520efficiently%2520enhances%2520both%2520memorizability%2520and%2520generalizablity%2520better%250Athan%2520other%2520state-of-the-art%2520methods%252C%2520delivering%2520satisfactory%2520performance%2520across%250Aall%2520sites.%2520Our%2520code%2520will%2520be%2520available%2520at%253A%250Ahttps%253A//github.com/dyxu-cuhkcse/SMG-Learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18037v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Synchronous%20Memorizability%20and%20Generalizability%20with%0A%20%20Site-Modulated%20Diffusion%20Replay%20for%20Cross-Site%20Continual%20Segmentation&entry.906535625=Dunyuan%20Xu%20and%20Xi%20Wang%20and%20Jingyang%20Zhang%20and%20Pheng-Ann%20Heng&entry.1292438233=%20%20The%20ability%20to%20learn%20sequentially%20from%20different%20data%20sites%20is%20crucial%20for%20a%0Adeep%20network%20in%20solving%20practical%20medical%20image%20diagnosis%20problems%20due%20to%0Aprivacy%20restrictions%20and%20storage%20limitations.%20However%2C%20adapting%20on%20incoming%0Asite%20leads%20to%20catastrophic%20forgetting%20on%20past%20sites%20and%20decreases%0Ageneralizablity%20on%20unseen%20sites.%20Existing%20Continual%20Learning%20%28CL%29%20and%20Domain%0AGeneralization%20%28DG%29%20methods%20have%20been%20proposed%20to%20solve%20these%20two%20challenges%0Arespectively%2C%20but%20none%20of%20them%20can%20address%20both%20simultaneously.%20Recognizing%0Athis%20limitation%2C%20this%20paper%20proposes%20a%20novel%20training%20paradigm%2C%20learning%0Atowards%20Synchronous%20Memorizability%20and%20Generalizability%20%28SMG-Learning%29.%20To%0Aachieve%20this%2C%20we%20create%20the%20orientational%20gradient%20alignment%20to%20ensure%0Amemorizability%20on%20previous%20sites%2C%20and%20arbitrary%20gradient%20alignment%20to%20enhance%0Ageneralizability%20on%20unseen%20sites.%20This%20approach%20is%20named%20as%20Parallel%20Gradient%0AAlignment%20%28PGA%29.%20Furthermore%2C%20we%20approximate%20the%20PGA%20as%20dual%20meta-objectives%0Ausing%20the%20first-order%20Taylor%20expansion%20to%20reduce%20computational%20cost%20of%20aligning%0Agradients.%20Considering%20that%20performing%20gradient%20alignments%2C%20especially%20for%0Aprevious%20sites%2C%20is%20not%20feasible%20due%20to%20the%20privacy%20constraints%2C%20we%20design%20a%0ASite-Modulated%20Diffusion%20%28SMD%29%20model%20to%20generate%20images%20with%20site-specific%0Alearnable%20prompts%2C%20replaying%20images%20have%20similar%20data%20distributions%20as%20previous%0Asites.%20We%20evaluate%20our%20method%20on%20two%20medical%20image%20segmentation%20tasks%2C%20where%0Adata%20from%20different%20sites%20arrive%20sequentially.%20Experimental%20results%20show%20that%0Aour%20method%20efficiently%20enhances%20both%20memorizability%20and%20generalizablity%20better%0Athan%20other%20state-of-the-art%20methods%2C%20delivering%20satisfactory%20performance%20across%0Aall%20sites.%20Our%20code%20will%20be%20available%20at%3A%0Ahttps%3A//github.com/dyxu-cuhkcse/SMG-Learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18037v1&entry.124074799=Read"},
{"title": "Multimodal Reaching-Position Prediction for ADL Support Using Neural\n  Networks", "author": "Yutaka Takase and Kimitoshi Yamazaki", "abstract": "  This study aimed to develop daily living support robots for patients with\nhemiplegia and the elderly. To support the daily living activities using robots\nin ordinary households without imposing physical and mental burdens on users,\nthe system must detect the actions of the user and move appropriately according\nto their motions.\n  We propose a reaching-position prediction scheme that targets the motion of\nlifting the upper arm, which is burdensome for patients with hemiplegia and the\nelderly in daily living activities.\n  For this motion, it is difficult to obtain effective features to create a\nprediction model in environments where large-scale sensor system installation\nis not feasible and the motion time is short.\n  We performed motion-collection experiments, revealed the features of the\ntarget motion and built a prediction model using the multimodal motion features\nand deep learning.\n  The proposed model achieved an accuracy of 93 \\% macro average and F1-score\nof 0.69 for a 9-class classification prediction at 35\\% of the motion\ncompletion.\n", "link": "http://arxiv.org/abs/2406.18162v1", "date": "2024-06-26", "relevancy": 2.2547, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6573}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.576}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Reaching-Position%20Prediction%20for%20ADL%20Support%20Using%20Neural%0A%20%20Networks&body=Title%3A%20Multimodal%20Reaching-Position%20Prediction%20for%20ADL%20Support%20Using%20Neural%0A%20%20Networks%0AAuthor%3A%20Yutaka%20Takase%20and%20Kimitoshi%20Yamazaki%0AAbstract%3A%20%20%20This%20study%20aimed%20to%20develop%20daily%20living%20support%20robots%20for%20patients%20with%0Ahemiplegia%20and%20the%20elderly.%20To%20support%20the%20daily%20living%20activities%20using%20robots%0Ain%20ordinary%20households%20without%20imposing%20physical%20and%20mental%20burdens%20on%20users%2C%0Athe%20system%20must%20detect%20the%20actions%20of%20the%20user%20and%20move%20appropriately%20according%0Ato%20their%20motions.%0A%20%20We%20propose%20a%20reaching-position%20prediction%20scheme%20that%20targets%20the%20motion%20of%0Alifting%20the%20upper%20arm%2C%20which%20is%20burdensome%20for%20patients%20with%20hemiplegia%20and%20the%0Aelderly%20in%20daily%20living%20activities.%0A%20%20For%20this%20motion%2C%20it%20is%20difficult%20to%20obtain%20effective%20features%20to%20create%20a%0Aprediction%20model%20in%20environments%20where%20large-scale%20sensor%20system%20installation%0Ais%20not%20feasible%20and%20the%20motion%20time%20is%20short.%0A%20%20We%20performed%20motion-collection%20experiments%2C%20revealed%20the%20features%20of%20the%0Atarget%20motion%20and%20built%20a%20prediction%20model%20using%20the%20multimodal%20motion%20features%0Aand%20deep%20learning.%0A%20%20The%20proposed%20model%20achieved%20an%20accuracy%20of%2093%20%5C%25%20macro%20average%20and%20F1-score%0Aof%200.69%20for%20a%209-class%20classification%20prediction%20at%2035%5C%25%20of%20the%20motion%0Acompletion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18162v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Reaching-Position%2520Prediction%2520for%2520ADL%2520Support%2520Using%2520Neural%250A%2520%2520Networks%26entry.906535625%3DYutaka%2520Takase%2520and%2520Kimitoshi%2520Yamazaki%26entry.1292438233%3D%2520%2520This%2520study%2520aimed%2520to%2520develop%2520daily%2520living%2520support%2520robots%2520for%2520patients%2520with%250Ahemiplegia%2520and%2520the%2520elderly.%2520To%2520support%2520the%2520daily%2520living%2520activities%2520using%2520robots%250Ain%2520ordinary%2520households%2520without%2520imposing%2520physical%2520and%2520mental%2520burdens%2520on%2520users%252C%250Athe%2520system%2520must%2520detect%2520the%2520actions%2520of%2520the%2520user%2520and%2520move%2520appropriately%2520according%250Ato%2520their%2520motions.%250A%2520%2520We%2520propose%2520a%2520reaching-position%2520prediction%2520scheme%2520that%2520targets%2520the%2520motion%2520of%250Alifting%2520the%2520upper%2520arm%252C%2520which%2520is%2520burdensome%2520for%2520patients%2520with%2520hemiplegia%2520and%2520the%250Aelderly%2520in%2520daily%2520living%2520activities.%250A%2520%2520For%2520this%2520motion%252C%2520it%2520is%2520difficult%2520to%2520obtain%2520effective%2520features%2520to%2520create%2520a%250Aprediction%2520model%2520in%2520environments%2520where%2520large-scale%2520sensor%2520system%2520installation%250Ais%2520not%2520feasible%2520and%2520the%2520motion%2520time%2520is%2520short.%250A%2520%2520We%2520performed%2520motion-collection%2520experiments%252C%2520revealed%2520the%2520features%2520of%2520the%250Atarget%2520motion%2520and%2520built%2520a%2520prediction%2520model%2520using%2520the%2520multimodal%2520motion%2520features%250Aand%2520deep%2520learning.%250A%2520%2520The%2520proposed%2520model%2520achieved%2520an%2520accuracy%2520of%252093%2520%255C%2525%2520macro%2520average%2520and%2520F1-score%250Aof%25200.69%2520for%2520a%25209-class%2520classification%2520prediction%2520at%252035%255C%2525%2520of%2520the%2520motion%250Acompletion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18162v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Reaching-Position%20Prediction%20for%20ADL%20Support%20Using%20Neural%0A%20%20Networks&entry.906535625=Yutaka%20Takase%20and%20Kimitoshi%20Yamazaki&entry.1292438233=%20%20This%20study%20aimed%20to%20develop%20daily%20living%20support%20robots%20for%20patients%20with%0Ahemiplegia%20and%20the%20elderly.%20To%20support%20the%20daily%20living%20activities%20using%20robots%0Ain%20ordinary%20households%20without%20imposing%20physical%20and%20mental%20burdens%20on%20users%2C%0Athe%20system%20must%20detect%20the%20actions%20of%20the%20user%20and%20move%20appropriately%20according%0Ato%20their%20motions.%0A%20%20We%20propose%20a%20reaching-position%20prediction%20scheme%20that%20targets%20the%20motion%20of%0Alifting%20the%20upper%20arm%2C%20which%20is%20burdensome%20for%20patients%20with%20hemiplegia%20and%20the%0Aelderly%20in%20daily%20living%20activities.%0A%20%20For%20this%20motion%2C%20it%20is%20difficult%20to%20obtain%20effective%20features%20to%20create%20a%0Aprediction%20model%20in%20environments%20where%20large-scale%20sensor%20system%20installation%0Ais%20not%20feasible%20and%20the%20motion%20time%20is%20short.%0A%20%20We%20performed%20motion-collection%20experiments%2C%20revealed%20the%20features%20of%20the%0Atarget%20motion%20and%20built%20a%20prediction%20model%20using%20the%20multimodal%20motion%20features%0Aand%20deep%20learning.%0A%20%20The%20proposed%20model%20achieved%20an%20accuracy%20of%2093%20%5C%25%20macro%20average%20and%20F1-score%0Aof%200.69%20for%20a%209-class%20classification%20prediction%20at%2035%5C%25%20of%20the%20motion%0Acompletion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18162v1&entry.124074799=Read"},
{"title": "ViT-1.58b: Mobile Vision Transformers in the 1-bit Era", "author": "Zhengqing Yuan and Rong Zhou and Hongyi Wang and Lifang He and Yanfang Ye and Lichao Sun", "abstract": "  Vision Transformers (ViTs) have achieved remarkable performance in various\nimage classification tasks by leveraging the attention mechanism to process\nimage patches as tokens. However, the high computational and memory demands of\nViTs pose significant challenges for deployment in resource-constrained\nenvironments. This paper introduces ViT-1.58b, a novel 1.58-bit quantized ViT\nmodel designed to drastically reduce memory and computational overhead while\npreserving competitive performance. ViT-1.58b employs ternary quantization,\nwhich refines the balance between efficiency and accuracy by constraining\nweights to {-1, 0, 1} and quantizing activations to 8-bit precision. Our\napproach ensures efficient scaling in terms of both memory and computation.\nExperiments on CIFAR-10 and ImageNet-1k demonstrate that ViT-1.58b maintains\ncomparable accuracy to full-precision Vit, with significant reductions in\nmemory usage and computational costs. This paper highlights the potential of\nextreme quantization techniques in developing sustainable AI solutions and\ncontributes to the broader discourse on efficient model deployment in practical\napplications. Our code and weights are available at\nhttps://github.com/DLYuanGod/ViT-1.58b.\n", "link": "http://arxiv.org/abs/2406.18051v1", "date": "2024-06-26", "relevancy": 2.2436, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5654}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5585}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViT-1.58b%3A%20Mobile%20Vision%20Transformers%20in%20the%201-bit%20Era&body=Title%3A%20ViT-1.58b%3A%20Mobile%20Vision%20Transformers%20in%20the%201-bit%20Era%0AAuthor%3A%20Zhengqing%20Yuan%20and%20Rong%20Zhou%20and%20Hongyi%20Wang%20and%20Lifang%20He%20and%20Yanfang%20Ye%20and%20Lichao%20Sun%0AAbstract%3A%20%20%20Vision%20Transformers%20%28ViTs%29%20have%20achieved%20remarkable%20performance%20in%20various%0Aimage%20classification%20tasks%20by%20leveraging%20the%20attention%20mechanism%20to%20process%0Aimage%20patches%20as%20tokens.%20However%2C%20the%20high%20computational%20and%20memory%20demands%20of%0AViTs%20pose%20significant%20challenges%20for%20deployment%20in%20resource-constrained%0Aenvironments.%20This%20paper%20introduces%20ViT-1.58b%2C%20a%20novel%201.58-bit%20quantized%20ViT%0Amodel%20designed%20to%20drastically%20reduce%20memory%20and%20computational%20overhead%20while%0Apreserving%20competitive%20performance.%20ViT-1.58b%20employs%20ternary%20quantization%2C%0Awhich%20refines%20the%20balance%20between%20efficiency%20and%20accuracy%20by%20constraining%0Aweights%20to%20%7B-1%2C%200%2C%201%7D%20and%20quantizing%20activations%20to%208-bit%20precision.%20Our%0Aapproach%20ensures%20efficient%20scaling%20in%20terms%20of%20both%20memory%20and%20computation.%0AExperiments%20on%20CIFAR-10%20and%20ImageNet-1k%20demonstrate%20that%20ViT-1.58b%20maintains%0Acomparable%20accuracy%20to%20full-precision%20Vit%2C%20with%20significant%20reductions%20in%0Amemory%20usage%20and%20computational%20costs.%20This%20paper%20highlights%20the%20potential%20of%0Aextreme%20quantization%20techniques%20in%20developing%20sustainable%20AI%20solutions%20and%0Acontributes%20to%20the%20broader%20discourse%20on%20efficient%20model%20deployment%20in%20practical%0Aapplications.%20Our%20code%20and%20weights%20are%20available%20at%0Ahttps%3A//github.com/DLYuanGod/ViT-1.58b.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18051v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViT-1.58b%253A%2520Mobile%2520Vision%2520Transformers%2520in%2520the%25201-bit%2520Era%26entry.906535625%3DZhengqing%2520Yuan%2520and%2520Rong%2520Zhou%2520and%2520Hongyi%2520Wang%2520and%2520Lifang%2520He%2520and%2520Yanfang%2520Ye%2520and%2520Lichao%2520Sun%26entry.1292438233%3D%2520%2520Vision%2520Transformers%2520%2528ViTs%2529%2520have%2520achieved%2520remarkable%2520performance%2520in%2520various%250Aimage%2520classification%2520tasks%2520by%2520leveraging%2520the%2520attention%2520mechanism%2520to%2520process%250Aimage%2520patches%2520as%2520tokens.%2520However%252C%2520the%2520high%2520computational%2520and%2520memory%2520demands%2520of%250AViTs%2520pose%2520significant%2520challenges%2520for%2520deployment%2520in%2520resource-constrained%250Aenvironments.%2520This%2520paper%2520introduces%2520ViT-1.58b%252C%2520a%2520novel%25201.58-bit%2520quantized%2520ViT%250Amodel%2520designed%2520to%2520drastically%2520reduce%2520memory%2520and%2520computational%2520overhead%2520while%250Apreserving%2520competitive%2520performance.%2520ViT-1.58b%2520employs%2520ternary%2520quantization%252C%250Awhich%2520refines%2520the%2520balance%2520between%2520efficiency%2520and%2520accuracy%2520by%2520constraining%250Aweights%2520to%2520%257B-1%252C%25200%252C%25201%257D%2520and%2520quantizing%2520activations%2520to%25208-bit%2520precision.%2520Our%250Aapproach%2520ensures%2520efficient%2520scaling%2520in%2520terms%2520of%2520both%2520memory%2520and%2520computation.%250AExperiments%2520on%2520CIFAR-10%2520and%2520ImageNet-1k%2520demonstrate%2520that%2520ViT-1.58b%2520maintains%250Acomparable%2520accuracy%2520to%2520full-precision%2520Vit%252C%2520with%2520significant%2520reductions%2520in%250Amemory%2520usage%2520and%2520computational%2520costs.%2520This%2520paper%2520highlights%2520the%2520potential%2520of%250Aextreme%2520quantization%2520techniques%2520in%2520developing%2520sustainable%2520AI%2520solutions%2520and%250Acontributes%2520to%2520the%2520broader%2520discourse%2520on%2520efficient%2520model%2520deployment%2520in%2520practical%250Aapplications.%2520Our%2520code%2520and%2520weights%2520are%2520available%2520at%250Ahttps%253A//github.com/DLYuanGod/ViT-1.58b.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18051v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViT-1.58b%3A%20Mobile%20Vision%20Transformers%20in%20the%201-bit%20Era&entry.906535625=Zhengqing%20Yuan%20and%20Rong%20Zhou%20and%20Hongyi%20Wang%20and%20Lifang%20He%20and%20Yanfang%20Ye%20and%20Lichao%20Sun&entry.1292438233=%20%20Vision%20Transformers%20%28ViTs%29%20have%20achieved%20remarkable%20performance%20in%20various%0Aimage%20classification%20tasks%20by%20leveraging%20the%20attention%20mechanism%20to%20process%0Aimage%20patches%20as%20tokens.%20However%2C%20the%20high%20computational%20and%20memory%20demands%20of%0AViTs%20pose%20significant%20challenges%20for%20deployment%20in%20resource-constrained%0Aenvironments.%20This%20paper%20introduces%20ViT-1.58b%2C%20a%20novel%201.58-bit%20quantized%20ViT%0Amodel%20designed%20to%20drastically%20reduce%20memory%20and%20computational%20overhead%20while%0Apreserving%20competitive%20performance.%20ViT-1.58b%20employs%20ternary%20quantization%2C%0Awhich%20refines%20the%20balance%20between%20efficiency%20and%20accuracy%20by%20constraining%0Aweights%20to%20%7B-1%2C%200%2C%201%7D%20and%20quantizing%20activations%20to%208-bit%20precision.%20Our%0Aapproach%20ensures%20efficient%20scaling%20in%20terms%20of%20both%20memory%20and%20computation.%0AExperiments%20on%20CIFAR-10%20and%20ImageNet-1k%20demonstrate%20that%20ViT-1.58b%20maintains%0Acomparable%20accuracy%20to%20full-precision%20Vit%2C%20with%20significant%20reductions%20in%0Amemory%20usage%20and%20computational%20costs.%20This%20paper%20highlights%20the%20potential%20of%0Aextreme%20quantization%20techniques%20in%20developing%20sustainable%20AI%20solutions%20and%0Acontributes%20to%20the%20broader%20discourse%20on%20efficient%20model%20deployment%20in%20practical%0Aapplications.%20Our%20code%20and%20weights%20are%20available%20at%0Ahttps%3A//github.com/DLYuanGod/ViT-1.58b.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18051v1&entry.124074799=Read"},
{"title": "Towards Training-free Open-world Segmentation via Image Prompt\n  Foundation Models", "author": "Lv Tang and Peng-Tao Jiang and Hao-Ke Xiao and Bo Li", "abstract": "  The realm of computer vision has witnessed a paradigm shift with the advent\nof foundational models, mirroring the transformative influence of large\nlanguage models in the domain of natural language processing. This paper delves\ninto the exploration of open-world segmentation, presenting a novel approach\ncalled Image Prompt Segmentation (IPSeg) that harnesses the power of vision\nfoundational models. IPSeg lies the principle of a training-free paradigm,\nwhich capitalizes on image prompt techniques. Specifically, IPSeg utilizes a\nsingle image containing a subjective visual concept as a flexible prompt to\nquery vision foundation models like DINOv2 and Stable Diffusion. Our approach\nextracts robust features for the prompt image and input image, then matches the\ninput representations to the prompt representations via a novel feature\ninteraction module to generate point prompts highlighting target objects in the\ninput image. The generated point prompts are further utilized to guide the\nSegment Anything Model to segment the target object in the input image. The\nproposed method stands out by eliminating the need for exhaustive training\nsessions, thereby offering a more efficient and scalable solution. Experiments\non COCO, PASCAL VOC, and other datasets demonstrate IPSeg's efficacy for\nflexible open-world segmentation using intuitive image prompts. This work\npioneers tapping foundation models for open-world understanding through visual\nconcepts conveyed in images.\n", "link": "http://arxiv.org/abs/2310.10912v3", "date": "2024-06-26", "relevancy": 2.2421, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5783}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5551}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Training-free%20Open-world%20Segmentation%20via%20Image%20Prompt%0A%20%20Foundation%20Models&body=Title%3A%20Towards%20Training-free%20Open-world%20Segmentation%20via%20Image%20Prompt%0A%20%20Foundation%20Models%0AAuthor%3A%20Lv%20Tang%20and%20Peng-Tao%20Jiang%20and%20Hao-Ke%20Xiao%20and%20Bo%20Li%0AAbstract%3A%20%20%20The%20realm%20of%20computer%20vision%20has%20witnessed%20a%20paradigm%20shift%20with%20the%20advent%0Aof%20foundational%20models%2C%20mirroring%20the%20transformative%20influence%20of%20large%0Alanguage%20models%20in%20the%20domain%20of%20natural%20language%20processing.%20This%20paper%20delves%0Ainto%20the%20exploration%20of%20open-world%20segmentation%2C%20presenting%20a%20novel%20approach%0Acalled%20Image%20Prompt%20Segmentation%20%28IPSeg%29%20that%20harnesses%20the%20power%20of%20vision%0Afoundational%20models.%20IPSeg%20lies%20the%20principle%20of%20a%20training-free%20paradigm%2C%0Awhich%20capitalizes%20on%20image%20prompt%20techniques.%20Specifically%2C%20IPSeg%20utilizes%20a%0Asingle%20image%20containing%20a%20subjective%20visual%20concept%20as%20a%20flexible%20prompt%20to%0Aquery%20vision%20foundation%20models%20like%20DINOv2%20and%20Stable%20Diffusion.%20Our%20approach%0Aextracts%20robust%20features%20for%20the%20prompt%20image%20and%20input%20image%2C%20then%20matches%20the%0Ainput%20representations%20to%20the%20prompt%20representations%20via%20a%20novel%20feature%0Ainteraction%20module%20to%20generate%20point%20prompts%20highlighting%20target%20objects%20in%20the%0Ainput%20image.%20The%20generated%20point%20prompts%20are%20further%20utilized%20to%20guide%20the%0ASegment%20Anything%20Model%20to%20segment%20the%20target%20object%20in%20the%20input%20image.%20The%0Aproposed%20method%20stands%20out%20by%20eliminating%20the%20need%20for%20exhaustive%20training%0Asessions%2C%20thereby%20offering%20a%20more%20efficient%20and%20scalable%20solution.%20Experiments%0Aon%20COCO%2C%20PASCAL%20VOC%2C%20and%20other%20datasets%20demonstrate%20IPSeg%27s%20efficacy%20for%0Aflexible%20open-world%20segmentation%20using%20intuitive%20image%20prompts.%20This%20work%0Apioneers%20tapping%20foundation%20models%20for%20open-world%20understanding%20through%20visual%0Aconcepts%20conveyed%20in%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.10912v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Training-free%2520Open-world%2520Segmentation%2520via%2520Image%2520Prompt%250A%2520%2520Foundation%2520Models%26entry.906535625%3DLv%2520Tang%2520and%2520Peng-Tao%2520Jiang%2520and%2520Hao-Ke%2520Xiao%2520and%2520Bo%2520Li%26entry.1292438233%3D%2520%2520The%2520realm%2520of%2520computer%2520vision%2520has%2520witnessed%2520a%2520paradigm%2520shift%2520with%2520the%2520advent%250Aof%2520foundational%2520models%252C%2520mirroring%2520the%2520transformative%2520influence%2520of%2520large%250Alanguage%2520models%2520in%2520the%2520domain%2520of%2520natural%2520language%2520processing.%2520This%2520paper%2520delves%250Ainto%2520the%2520exploration%2520of%2520open-world%2520segmentation%252C%2520presenting%2520a%2520novel%2520approach%250Acalled%2520Image%2520Prompt%2520Segmentation%2520%2528IPSeg%2529%2520that%2520harnesses%2520the%2520power%2520of%2520vision%250Afoundational%2520models.%2520IPSeg%2520lies%2520the%2520principle%2520of%2520a%2520training-free%2520paradigm%252C%250Awhich%2520capitalizes%2520on%2520image%2520prompt%2520techniques.%2520Specifically%252C%2520IPSeg%2520utilizes%2520a%250Asingle%2520image%2520containing%2520a%2520subjective%2520visual%2520concept%2520as%2520a%2520flexible%2520prompt%2520to%250Aquery%2520vision%2520foundation%2520models%2520like%2520DINOv2%2520and%2520Stable%2520Diffusion.%2520Our%2520approach%250Aextracts%2520robust%2520features%2520for%2520the%2520prompt%2520image%2520and%2520input%2520image%252C%2520then%2520matches%2520the%250Ainput%2520representations%2520to%2520the%2520prompt%2520representations%2520via%2520a%2520novel%2520feature%250Ainteraction%2520module%2520to%2520generate%2520point%2520prompts%2520highlighting%2520target%2520objects%2520in%2520the%250Ainput%2520image.%2520The%2520generated%2520point%2520prompts%2520are%2520further%2520utilized%2520to%2520guide%2520the%250ASegment%2520Anything%2520Model%2520to%2520segment%2520the%2520target%2520object%2520in%2520the%2520input%2520image.%2520The%250Aproposed%2520method%2520stands%2520out%2520by%2520eliminating%2520the%2520need%2520for%2520exhaustive%2520training%250Asessions%252C%2520thereby%2520offering%2520a%2520more%2520efficient%2520and%2520scalable%2520solution.%2520Experiments%250Aon%2520COCO%252C%2520PASCAL%2520VOC%252C%2520and%2520other%2520datasets%2520demonstrate%2520IPSeg%2527s%2520efficacy%2520for%250Aflexible%2520open-world%2520segmentation%2520using%2520intuitive%2520image%2520prompts.%2520This%2520work%250Apioneers%2520tapping%2520foundation%2520models%2520for%2520open-world%2520understanding%2520through%2520visual%250Aconcepts%2520conveyed%2520in%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.10912v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Training-free%20Open-world%20Segmentation%20via%20Image%20Prompt%0A%20%20Foundation%20Models&entry.906535625=Lv%20Tang%20and%20Peng-Tao%20Jiang%20and%20Hao-Ke%20Xiao%20and%20Bo%20Li&entry.1292438233=%20%20The%20realm%20of%20computer%20vision%20has%20witnessed%20a%20paradigm%20shift%20with%20the%20advent%0Aof%20foundational%20models%2C%20mirroring%20the%20transformative%20influence%20of%20large%0Alanguage%20models%20in%20the%20domain%20of%20natural%20language%20processing.%20This%20paper%20delves%0Ainto%20the%20exploration%20of%20open-world%20segmentation%2C%20presenting%20a%20novel%20approach%0Acalled%20Image%20Prompt%20Segmentation%20%28IPSeg%29%20that%20harnesses%20the%20power%20of%20vision%0Afoundational%20models.%20IPSeg%20lies%20the%20principle%20of%20a%20training-free%20paradigm%2C%0Awhich%20capitalizes%20on%20image%20prompt%20techniques.%20Specifically%2C%20IPSeg%20utilizes%20a%0Asingle%20image%20containing%20a%20subjective%20visual%20concept%20as%20a%20flexible%20prompt%20to%0Aquery%20vision%20foundation%20models%20like%20DINOv2%20and%20Stable%20Diffusion.%20Our%20approach%0Aextracts%20robust%20features%20for%20the%20prompt%20image%20and%20input%20image%2C%20then%20matches%20the%0Ainput%20representations%20to%20the%20prompt%20representations%20via%20a%20novel%20feature%0Ainteraction%20module%20to%20generate%20point%20prompts%20highlighting%20target%20objects%20in%20the%0Ainput%20image.%20The%20generated%20point%20prompts%20are%20further%20utilized%20to%20guide%20the%0ASegment%20Anything%20Model%20to%20segment%20the%20target%20object%20in%20the%20input%20image.%20The%0Aproposed%20method%20stands%20out%20by%20eliminating%20the%20need%20for%20exhaustive%20training%0Asessions%2C%20thereby%20offering%20a%20more%20efficient%20and%20scalable%20solution.%20Experiments%0Aon%20COCO%2C%20PASCAL%20VOC%2C%20and%20other%20datasets%20demonstrate%20IPSeg%27s%20efficacy%20for%0Aflexible%20open-world%20segmentation%20using%20intuitive%20image%20prompts.%20This%20work%0Apioneers%20tapping%20foundation%20models%20for%20open-world%20understanding%20through%20visual%0Aconcepts%20conveyed%20in%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.10912v3&entry.124074799=Read"},
{"title": "The Fundamental Limits of Least-Privilege Learning", "author": "Theresa Stadler and Bogdan Kulynych and Michael C. Gastpar and Nicolas Papernot and Carmela Troncoso", "abstract": "  The promise of least-privilege learning -- to find feature representations\nthat are useful for a learning task but prevent inference of any sensitive\ninformation unrelated to this task -- is highly appealing. However, so far this\nconcept has only been stated informally. It thus remains an open question\nwhether and how we can achieve this goal. In this work, we provide the first\nformalisation of the least-privilege principle for machine learning and\ncharacterise its feasibility. We prove that there is a fundamental trade-off\nbetween a representation's utility for a given task and its leakage beyond the\nintended task: it is not possible to learn representations that have high\nutility for the intended task but, at the same time prevent inference of any\nattribute other than the task label itself. This trade-off holds under\nrealistic assumptions on the data distribution and regardless of the technique\nused to learn the feature mappings that produce these representations. We\nempirically validate this result for a wide range of learning techniques, model\narchitectures, and datasets.\n", "link": "http://arxiv.org/abs/2402.12235v2", "date": "2024-06-26", "relevancy": 2.2405, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4569}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4569}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4305}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Fundamental%20Limits%20of%20Least-Privilege%20Learning&body=Title%3A%20The%20Fundamental%20Limits%20of%20Least-Privilege%20Learning%0AAuthor%3A%20Theresa%20Stadler%20and%20Bogdan%20Kulynych%20and%20Michael%20C.%20Gastpar%20and%20Nicolas%20Papernot%20and%20Carmela%20Troncoso%0AAbstract%3A%20%20%20The%20promise%20of%20least-privilege%20learning%20--%20to%20find%20feature%20representations%0Athat%20are%20useful%20for%20a%20learning%20task%20but%20prevent%20inference%20of%20any%20sensitive%0Ainformation%20unrelated%20to%20this%20task%20--%20is%20highly%20appealing.%20However%2C%20so%20far%20this%0Aconcept%20has%20only%20been%20stated%20informally.%20It%20thus%20remains%20an%20open%20question%0Awhether%20and%20how%20we%20can%20achieve%20this%20goal.%20In%20this%20work%2C%20we%20provide%20the%20first%0Aformalisation%20of%20the%20least-privilege%20principle%20for%20machine%20learning%20and%0Acharacterise%20its%20feasibility.%20We%20prove%20that%20there%20is%20a%20fundamental%20trade-off%0Abetween%20a%20representation%27s%20utility%20for%20a%20given%20task%20and%20its%20leakage%20beyond%20the%0Aintended%20task%3A%20it%20is%20not%20possible%20to%20learn%20representations%20that%20have%20high%0Autility%20for%20the%20intended%20task%20but%2C%20at%20the%20same%20time%20prevent%20inference%20of%20any%0Aattribute%20other%20than%20the%20task%20label%20itself.%20This%20trade-off%20holds%20under%0Arealistic%20assumptions%20on%20the%20data%20distribution%20and%20regardless%20of%20the%20technique%0Aused%20to%20learn%20the%20feature%20mappings%20that%20produce%20these%20representations.%20We%0Aempirically%20validate%20this%20result%20for%20a%20wide%20range%20of%20learning%20techniques%2C%20model%0Aarchitectures%2C%20and%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12235v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Fundamental%2520Limits%2520of%2520Least-Privilege%2520Learning%26entry.906535625%3DTheresa%2520Stadler%2520and%2520Bogdan%2520Kulynych%2520and%2520Michael%2520C.%2520Gastpar%2520and%2520Nicolas%2520Papernot%2520and%2520Carmela%2520Troncoso%26entry.1292438233%3D%2520%2520The%2520promise%2520of%2520least-privilege%2520learning%2520--%2520to%2520find%2520feature%2520representations%250Athat%2520are%2520useful%2520for%2520a%2520learning%2520task%2520but%2520prevent%2520inference%2520of%2520any%2520sensitive%250Ainformation%2520unrelated%2520to%2520this%2520task%2520--%2520is%2520highly%2520appealing.%2520However%252C%2520so%2520far%2520this%250Aconcept%2520has%2520only%2520been%2520stated%2520informally.%2520It%2520thus%2520remains%2520an%2520open%2520question%250Awhether%2520and%2520how%2520we%2520can%2520achieve%2520this%2520goal.%2520In%2520this%2520work%252C%2520we%2520provide%2520the%2520first%250Aformalisation%2520of%2520the%2520least-privilege%2520principle%2520for%2520machine%2520learning%2520and%250Acharacterise%2520its%2520feasibility.%2520We%2520prove%2520that%2520there%2520is%2520a%2520fundamental%2520trade-off%250Abetween%2520a%2520representation%2527s%2520utility%2520for%2520a%2520given%2520task%2520and%2520its%2520leakage%2520beyond%2520the%250Aintended%2520task%253A%2520it%2520is%2520not%2520possible%2520to%2520learn%2520representations%2520that%2520have%2520high%250Autility%2520for%2520the%2520intended%2520task%2520but%252C%2520at%2520the%2520same%2520time%2520prevent%2520inference%2520of%2520any%250Aattribute%2520other%2520than%2520the%2520task%2520label%2520itself.%2520This%2520trade-off%2520holds%2520under%250Arealistic%2520assumptions%2520on%2520the%2520data%2520distribution%2520and%2520regardless%2520of%2520the%2520technique%250Aused%2520to%2520learn%2520the%2520feature%2520mappings%2520that%2520produce%2520these%2520representations.%2520We%250Aempirically%2520validate%2520this%2520result%2520for%2520a%2520wide%2520range%2520of%2520learning%2520techniques%252C%2520model%250Aarchitectures%252C%2520and%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.12235v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Fundamental%20Limits%20of%20Least-Privilege%20Learning&entry.906535625=Theresa%20Stadler%20and%20Bogdan%20Kulynych%20and%20Michael%20C.%20Gastpar%20and%20Nicolas%20Papernot%20and%20Carmela%20Troncoso&entry.1292438233=%20%20The%20promise%20of%20least-privilege%20learning%20--%20to%20find%20feature%20representations%0Athat%20are%20useful%20for%20a%20learning%20task%20but%20prevent%20inference%20of%20any%20sensitive%0Ainformation%20unrelated%20to%20this%20task%20--%20is%20highly%20appealing.%20However%2C%20so%20far%20this%0Aconcept%20has%20only%20been%20stated%20informally.%20It%20thus%20remains%20an%20open%20question%0Awhether%20and%20how%20we%20can%20achieve%20this%20goal.%20In%20this%20work%2C%20we%20provide%20the%20first%0Aformalisation%20of%20the%20least-privilege%20principle%20for%20machine%20learning%20and%0Acharacterise%20its%20feasibility.%20We%20prove%20that%20there%20is%20a%20fundamental%20trade-off%0Abetween%20a%20representation%27s%20utility%20for%20a%20given%20task%20and%20its%20leakage%20beyond%20the%0Aintended%20task%3A%20it%20is%20not%20possible%20to%20learn%20representations%20that%20have%20high%0Autility%20for%20the%20intended%20task%20but%2C%20at%20the%20same%20time%20prevent%20inference%20of%20any%0Aattribute%20other%20than%20the%20task%20label%20itself.%20This%20trade-off%20holds%20under%0Arealistic%20assumptions%20on%20the%20data%20distribution%20and%20regardless%20of%20the%20technique%0Aused%20to%20learn%20the%20feature%20mappings%20that%20produce%20these%20representations.%20We%0Aempirically%20validate%20this%20result%20for%20a%20wide%20range%20of%20learning%20techniques%2C%20model%0Aarchitectures%2C%20and%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12235v2&entry.124074799=Read"},
{"title": "Deep Fusion: Efficient Network Training via Pre-trained Initializations", "author": "Hanna Mazzawi and Xavi Gonzalvo and Michael Wunder and Sammy Jerome and Benoit Dherin", "abstract": "  In recent years, deep learning has made remarkable progress in a wide range\nof domains, with a particularly notable impact on natural language processing\ntasks. One of the challenges associated with training deep neural networks in\nthe context of LLMs is the need for large amounts of computational resources\nand time. To mitigate this, network growing algorithms offer potential cost\nsavings, but their underlying mechanisms are poorly understood. We present two\nnotable contributions in this paper. First, we present Deep Fusion, an\nefficient approach to network training that leverages pre-trained\ninitializations of smaller networks. Second, we propose a theoretical framework\nusing backward error analysis to illustrate the dynamics of mid-training\nnetwork growth. Our experiments show how Deep Fusion is a practical and\neffective approach that not only accelerates the training process but also\nreduces computational requirements, maintaining or surpassing traditional\ntraining methods' performance in various NLP tasks and T5 model sizes. Finally,\nwe validate our theoretical framework, which guides the optimal use of Deep\nFusion, showing that with carefully optimized training dynamics, it\nsignificantly reduces both training time and resource consumption.\n", "link": "http://arxiv.org/abs/2306.11903v3", "date": "2024-06-26", "relevancy": 2.2337, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5634}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5612}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Fusion%3A%20Efficient%20Network%20Training%20via%20Pre-trained%20Initializations&body=Title%3A%20Deep%20Fusion%3A%20Efficient%20Network%20Training%20via%20Pre-trained%20Initializations%0AAuthor%3A%20Hanna%20Mazzawi%20and%20Xavi%20Gonzalvo%20and%20Michael%20Wunder%20and%20Sammy%20Jerome%20and%20Benoit%20Dherin%0AAbstract%3A%20%20%20In%20recent%20years%2C%20deep%20learning%20has%20made%20remarkable%20progress%20in%20a%20wide%20range%0Aof%20domains%2C%20with%20a%20particularly%20notable%20impact%20on%20natural%20language%20processing%0Atasks.%20One%20of%20the%20challenges%20associated%20with%20training%20deep%20neural%20networks%20in%0Athe%20context%20of%20LLMs%20is%20the%20need%20for%20large%20amounts%20of%20computational%20resources%0Aand%20time.%20To%20mitigate%20this%2C%20network%20growing%20algorithms%20offer%20potential%20cost%0Asavings%2C%20but%20their%20underlying%20mechanisms%20are%20poorly%20understood.%20We%20present%20two%0Anotable%20contributions%20in%20this%20paper.%20First%2C%20we%20present%20Deep%20Fusion%2C%20an%0Aefficient%20approach%20to%20network%20training%20that%20leverages%20pre-trained%0Ainitializations%20of%20smaller%20networks.%20Second%2C%20we%20propose%20a%20theoretical%20framework%0Ausing%20backward%20error%20analysis%20to%20illustrate%20the%20dynamics%20of%20mid-training%0Anetwork%20growth.%20Our%20experiments%20show%20how%20Deep%20Fusion%20is%20a%20practical%20and%0Aeffective%20approach%20that%20not%20only%20accelerates%20the%20training%20process%20but%20also%0Areduces%20computational%20requirements%2C%20maintaining%20or%20surpassing%20traditional%0Atraining%20methods%27%20performance%20in%20various%20NLP%20tasks%20and%20T5%20model%20sizes.%20Finally%2C%0Awe%20validate%20our%20theoretical%20framework%2C%20which%20guides%20the%20optimal%20use%20of%20Deep%0AFusion%2C%20showing%20that%20with%20carefully%20optimized%20training%20dynamics%2C%20it%0Asignificantly%20reduces%20both%20training%20time%20and%20resource%20consumption.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.11903v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Fusion%253A%2520Efficient%2520Network%2520Training%2520via%2520Pre-trained%2520Initializations%26entry.906535625%3DHanna%2520Mazzawi%2520and%2520Xavi%2520Gonzalvo%2520and%2520Michael%2520Wunder%2520and%2520Sammy%2520Jerome%2520and%2520Benoit%2520Dherin%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520deep%2520learning%2520has%2520made%2520remarkable%2520progress%2520in%2520a%2520wide%2520range%250Aof%2520domains%252C%2520with%2520a%2520particularly%2520notable%2520impact%2520on%2520natural%2520language%2520processing%250Atasks.%2520One%2520of%2520the%2520challenges%2520associated%2520with%2520training%2520deep%2520neural%2520networks%2520in%250Athe%2520context%2520of%2520LLMs%2520is%2520the%2520need%2520for%2520large%2520amounts%2520of%2520computational%2520resources%250Aand%2520time.%2520To%2520mitigate%2520this%252C%2520network%2520growing%2520algorithms%2520offer%2520potential%2520cost%250Asavings%252C%2520but%2520their%2520underlying%2520mechanisms%2520are%2520poorly%2520understood.%2520We%2520present%2520two%250Anotable%2520contributions%2520in%2520this%2520paper.%2520First%252C%2520we%2520present%2520Deep%2520Fusion%252C%2520an%250Aefficient%2520approach%2520to%2520network%2520training%2520that%2520leverages%2520pre-trained%250Ainitializations%2520of%2520smaller%2520networks.%2520Second%252C%2520we%2520propose%2520a%2520theoretical%2520framework%250Ausing%2520backward%2520error%2520analysis%2520to%2520illustrate%2520the%2520dynamics%2520of%2520mid-training%250Anetwork%2520growth.%2520Our%2520experiments%2520show%2520how%2520Deep%2520Fusion%2520is%2520a%2520practical%2520and%250Aeffective%2520approach%2520that%2520not%2520only%2520accelerates%2520the%2520training%2520process%2520but%2520also%250Areduces%2520computational%2520requirements%252C%2520maintaining%2520or%2520surpassing%2520traditional%250Atraining%2520methods%2527%2520performance%2520in%2520various%2520NLP%2520tasks%2520and%2520T5%2520model%2520sizes.%2520Finally%252C%250Awe%2520validate%2520our%2520theoretical%2520framework%252C%2520which%2520guides%2520the%2520optimal%2520use%2520of%2520Deep%250AFusion%252C%2520showing%2520that%2520with%2520carefully%2520optimized%2520training%2520dynamics%252C%2520it%250Asignificantly%2520reduces%2520both%2520training%2520time%2520and%2520resource%2520consumption.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.11903v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Fusion%3A%20Efficient%20Network%20Training%20via%20Pre-trained%20Initializations&entry.906535625=Hanna%20Mazzawi%20and%20Xavi%20Gonzalvo%20and%20Michael%20Wunder%20and%20Sammy%20Jerome%20and%20Benoit%20Dherin&entry.1292438233=%20%20In%20recent%20years%2C%20deep%20learning%20has%20made%20remarkable%20progress%20in%20a%20wide%20range%0Aof%20domains%2C%20with%20a%20particularly%20notable%20impact%20on%20natural%20language%20processing%0Atasks.%20One%20of%20the%20challenges%20associated%20with%20training%20deep%20neural%20networks%20in%0Athe%20context%20of%20LLMs%20is%20the%20need%20for%20large%20amounts%20of%20computational%20resources%0Aand%20time.%20To%20mitigate%20this%2C%20network%20growing%20algorithms%20offer%20potential%20cost%0Asavings%2C%20but%20their%20underlying%20mechanisms%20are%20poorly%20understood.%20We%20present%20two%0Anotable%20contributions%20in%20this%20paper.%20First%2C%20we%20present%20Deep%20Fusion%2C%20an%0Aefficient%20approach%20to%20network%20training%20that%20leverages%20pre-trained%0Ainitializations%20of%20smaller%20networks.%20Second%2C%20we%20propose%20a%20theoretical%20framework%0Ausing%20backward%20error%20analysis%20to%20illustrate%20the%20dynamics%20of%20mid-training%0Anetwork%20growth.%20Our%20experiments%20show%20how%20Deep%20Fusion%20is%20a%20practical%20and%0Aeffective%20approach%20that%20not%20only%20accelerates%20the%20training%20process%20but%20also%0Areduces%20computational%20requirements%2C%20maintaining%20or%20surpassing%20traditional%0Atraining%20methods%27%20performance%20in%20various%20NLP%20tasks%20and%20T5%20model%20sizes.%20Finally%2C%0Awe%20validate%20our%20theoretical%20framework%2C%20which%20guides%20the%20optimal%20use%20of%20Deep%0AFusion%2C%20showing%20that%20with%20carefully%20optimized%20training%20dynamics%2C%20it%0Asignificantly%20reduces%20both%20training%20time%20and%20resource%20consumption.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.11903v3&entry.124074799=Read"},
{"title": "WhaleNet: a Novel Deep Learning Architecture for Marine Mammals\n  Vocalizations on Watkins Marine Mammal Sound Database", "author": "Alessandro Licciardi and Davide Carbone", "abstract": "  Marine mammal communication is a complex field, hindered by the diversity of\nvocalizations and environmental factors. The Watkins Marine Mammal Sound\nDatabase (WMMD) constitutes a comprehensive labeled dataset employed in machine\nlearning applications. Nevertheless, the methodologies for data preparation,\npreprocessing, and classification documented in the literature exhibit\nconsiderable variability and are typically not applied to the dataset in its\nentirety. This study initially undertakes a concise review of the\nstate-of-the-art benchmarks pertaining to the dataset, with a particular focus\non clarifying data preparation and preprocessing techniques. Subsequently, we\nexplore the utilization of the Wavelet Scattering Transform (WST) and Mel\nspectrogram as preprocessing mechanisms for feature extraction. In this paper,\nwe introduce \\textbf{WhaleNet} (Wavelet Highly Adaptive Learning Ensemble\nNetwork), a sophisticated deep ensemble architecture for the classification of\nmarine mammal vocalizations, leveraging both WST and Mel spectrogram for\nenhanced feature discrimination. By integrating the insights derived from WST\nand Mel representations, we achieved an improvement in classification accuracy\nby $8-10\\%$ over existing architectures, corresponding to a classification\naccuracy of $97.61\\%$.\n", "link": "http://arxiv.org/abs/2402.17775v2", "date": "2024-06-26", "relevancy": 2.2335, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4726}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4346}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WhaleNet%3A%20a%20Novel%20Deep%20Learning%20Architecture%20for%20Marine%20Mammals%0A%20%20Vocalizations%20on%20Watkins%20Marine%20Mammal%20Sound%20Database&body=Title%3A%20WhaleNet%3A%20a%20Novel%20Deep%20Learning%20Architecture%20for%20Marine%20Mammals%0A%20%20Vocalizations%20on%20Watkins%20Marine%20Mammal%20Sound%20Database%0AAuthor%3A%20Alessandro%20Licciardi%20and%20Davide%20Carbone%0AAbstract%3A%20%20%20Marine%20mammal%20communication%20is%20a%20complex%20field%2C%20hindered%20by%20the%20diversity%20of%0Avocalizations%20and%20environmental%20factors.%20The%20Watkins%20Marine%20Mammal%20Sound%0ADatabase%20%28WMMD%29%20constitutes%20a%20comprehensive%20labeled%20dataset%20employed%20in%20machine%0Alearning%20applications.%20Nevertheless%2C%20the%20methodologies%20for%20data%20preparation%2C%0Apreprocessing%2C%20and%20classification%20documented%20in%20the%20literature%20exhibit%0Aconsiderable%20variability%20and%20are%20typically%20not%20applied%20to%20the%20dataset%20in%20its%0Aentirety.%20This%20study%20initially%20undertakes%20a%20concise%20review%20of%20the%0Astate-of-the-art%20benchmarks%20pertaining%20to%20the%20dataset%2C%20with%20a%20particular%20focus%0Aon%20clarifying%20data%20preparation%20and%20preprocessing%20techniques.%20Subsequently%2C%20we%0Aexplore%20the%20utilization%20of%20the%20Wavelet%20Scattering%20Transform%20%28WST%29%20and%20Mel%0Aspectrogram%20as%20preprocessing%20mechanisms%20for%20feature%20extraction.%20In%20this%20paper%2C%0Awe%20introduce%20%5Ctextbf%7BWhaleNet%7D%20%28Wavelet%20Highly%20Adaptive%20Learning%20Ensemble%0ANetwork%29%2C%20a%20sophisticated%20deep%20ensemble%20architecture%20for%20the%20classification%20of%0Amarine%20mammal%20vocalizations%2C%20leveraging%20both%20WST%20and%20Mel%20spectrogram%20for%0Aenhanced%20feature%20discrimination.%20By%20integrating%20the%20insights%20derived%20from%20WST%0Aand%20Mel%20representations%2C%20we%20achieved%20an%20improvement%20in%20classification%20accuracy%0Aby%20%248-10%5C%25%24%20over%20existing%20architectures%2C%20corresponding%20to%20a%20classification%0Aaccuracy%20of%20%2497.61%5C%25%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.17775v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhaleNet%253A%2520a%2520Novel%2520Deep%2520Learning%2520Architecture%2520for%2520Marine%2520Mammals%250A%2520%2520Vocalizations%2520on%2520Watkins%2520Marine%2520Mammal%2520Sound%2520Database%26entry.906535625%3DAlessandro%2520Licciardi%2520and%2520Davide%2520Carbone%26entry.1292438233%3D%2520%2520Marine%2520mammal%2520communication%2520is%2520a%2520complex%2520field%252C%2520hindered%2520by%2520the%2520diversity%2520of%250Avocalizations%2520and%2520environmental%2520factors.%2520The%2520Watkins%2520Marine%2520Mammal%2520Sound%250ADatabase%2520%2528WMMD%2529%2520constitutes%2520a%2520comprehensive%2520labeled%2520dataset%2520employed%2520in%2520machine%250Alearning%2520applications.%2520Nevertheless%252C%2520the%2520methodologies%2520for%2520data%2520preparation%252C%250Apreprocessing%252C%2520and%2520classification%2520documented%2520in%2520the%2520literature%2520exhibit%250Aconsiderable%2520variability%2520and%2520are%2520typically%2520not%2520applied%2520to%2520the%2520dataset%2520in%2520its%250Aentirety.%2520This%2520study%2520initially%2520undertakes%2520a%2520concise%2520review%2520of%2520the%250Astate-of-the-art%2520benchmarks%2520pertaining%2520to%2520the%2520dataset%252C%2520with%2520a%2520particular%2520focus%250Aon%2520clarifying%2520data%2520preparation%2520and%2520preprocessing%2520techniques.%2520Subsequently%252C%2520we%250Aexplore%2520the%2520utilization%2520of%2520the%2520Wavelet%2520Scattering%2520Transform%2520%2528WST%2529%2520and%2520Mel%250Aspectrogram%2520as%2520preprocessing%2520mechanisms%2520for%2520feature%2520extraction.%2520In%2520this%2520paper%252C%250Awe%2520introduce%2520%255Ctextbf%257BWhaleNet%257D%2520%2528Wavelet%2520Highly%2520Adaptive%2520Learning%2520Ensemble%250ANetwork%2529%252C%2520a%2520sophisticated%2520deep%2520ensemble%2520architecture%2520for%2520the%2520classification%2520of%250Amarine%2520mammal%2520vocalizations%252C%2520leveraging%2520both%2520WST%2520and%2520Mel%2520spectrogram%2520for%250Aenhanced%2520feature%2520discrimination.%2520By%2520integrating%2520the%2520insights%2520derived%2520from%2520WST%250Aand%2520Mel%2520representations%252C%2520we%2520achieved%2520an%2520improvement%2520in%2520classification%2520accuracy%250Aby%2520%25248-10%255C%2525%2524%2520over%2520existing%2520architectures%252C%2520corresponding%2520to%2520a%2520classification%250Aaccuracy%2520of%2520%252497.61%255C%2525%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.17775v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WhaleNet%3A%20a%20Novel%20Deep%20Learning%20Architecture%20for%20Marine%20Mammals%0A%20%20Vocalizations%20on%20Watkins%20Marine%20Mammal%20Sound%20Database&entry.906535625=Alessandro%20Licciardi%20and%20Davide%20Carbone&entry.1292438233=%20%20Marine%20mammal%20communication%20is%20a%20complex%20field%2C%20hindered%20by%20the%20diversity%20of%0Avocalizations%20and%20environmental%20factors.%20The%20Watkins%20Marine%20Mammal%20Sound%0ADatabase%20%28WMMD%29%20constitutes%20a%20comprehensive%20labeled%20dataset%20employed%20in%20machine%0Alearning%20applications.%20Nevertheless%2C%20the%20methodologies%20for%20data%20preparation%2C%0Apreprocessing%2C%20and%20classification%20documented%20in%20the%20literature%20exhibit%0Aconsiderable%20variability%20and%20are%20typically%20not%20applied%20to%20the%20dataset%20in%20its%0Aentirety.%20This%20study%20initially%20undertakes%20a%20concise%20review%20of%20the%0Astate-of-the-art%20benchmarks%20pertaining%20to%20the%20dataset%2C%20with%20a%20particular%20focus%0Aon%20clarifying%20data%20preparation%20and%20preprocessing%20techniques.%20Subsequently%2C%20we%0Aexplore%20the%20utilization%20of%20the%20Wavelet%20Scattering%20Transform%20%28WST%29%20and%20Mel%0Aspectrogram%20as%20preprocessing%20mechanisms%20for%20feature%20extraction.%20In%20this%20paper%2C%0Awe%20introduce%20%5Ctextbf%7BWhaleNet%7D%20%28Wavelet%20Highly%20Adaptive%20Learning%20Ensemble%0ANetwork%29%2C%20a%20sophisticated%20deep%20ensemble%20architecture%20for%20the%20classification%20of%0Amarine%20mammal%20vocalizations%2C%20leveraging%20both%20WST%20and%20Mel%20spectrogram%20for%0Aenhanced%20feature%20discrimination.%20By%20integrating%20the%20insights%20derived%20from%20WST%0Aand%20Mel%20representations%2C%20we%20achieved%20an%20improvement%20in%20classification%20accuracy%0Aby%20%248-10%5C%25%24%20over%20existing%20architectures%2C%20corresponding%20to%20a%20classification%0Aaccuracy%20of%20%2497.61%5C%25%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17775v2&entry.124074799=Read"},
{"title": "Iterative Reasoning Preference Optimization", "author": "Richard Yuanzhe Pang and Weizhe Yuan and Kyunghyun Cho and He He and Sainbayar Sukhbaatar and Jason Weston", "abstract": "  Iterative preference optimization methods have recently been shown to perform\nwell for general instruction tuning tasks, but typically make little\nimprovement on reasoning tasks (Yuan et al., 2024, Chen et al., 2024). In this\nwork we develop an iterative approach that optimizes the preference between\ncompeting generated Chain-of-Thought (CoT) candidates by optimizing for winning\nvs. losing reasoning steps that lead to the correct answer. We train using a\nmodified DPO loss (Rafailov et al., 2023) with an additional negative\nlog-likelihood term, which we find to be crucial. We show reasoning improves\nacross repeated iterations of this scheme. While only relying on examples in\nthe training set, our approach results in increasing accuracy on GSM8K, MATH,\nand ARC-Challenge for Llama-2-70B-Chat, outperforming other Llama-2-based\nmodels not relying on additionally sourced datasets. For example, we see a\nlarge improvement from 55.6% to 81.6% on GSM8K and an accuracy of 88.7% with\nmajority voting out of 32 samples.\n", "link": "http://arxiv.org/abs/2404.19733v3", "date": "2024-06-26", "relevancy": 2.2329, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4504}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4481}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Iterative%20Reasoning%20Preference%20Optimization&body=Title%3A%20Iterative%20Reasoning%20Preference%20Optimization%0AAuthor%3A%20Richard%20Yuanzhe%20Pang%20and%20Weizhe%20Yuan%20and%20Kyunghyun%20Cho%20and%20He%20He%20and%20Sainbayar%20Sukhbaatar%20and%20Jason%20Weston%0AAbstract%3A%20%20%20Iterative%20preference%20optimization%20methods%20have%20recently%20been%20shown%20to%20perform%0Awell%20for%20general%20instruction%20tuning%20tasks%2C%20but%20typically%20make%20little%0Aimprovement%20on%20reasoning%20tasks%20%28Yuan%20et%20al.%2C%202024%2C%20Chen%20et%20al.%2C%202024%29.%20In%20this%0Awork%20we%20develop%20an%20iterative%20approach%20that%20optimizes%20the%20preference%20between%0Acompeting%20generated%20Chain-of-Thought%20%28CoT%29%20candidates%20by%20optimizing%20for%20winning%0Avs.%20losing%20reasoning%20steps%20that%20lead%20to%20the%20correct%20answer.%20We%20train%20using%20a%0Amodified%20DPO%20loss%20%28Rafailov%20et%20al.%2C%202023%29%20with%20an%20additional%20negative%0Alog-likelihood%20term%2C%20which%20we%20find%20to%20be%20crucial.%20We%20show%20reasoning%20improves%0Aacross%20repeated%20iterations%20of%20this%20scheme.%20While%20only%20relying%20on%20examples%20in%0Athe%20training%20set%2C%20our%20approach%20results%20in%20increasing%20accuracy%20on%20GSM8K%2C%20MATH%2C%0Aand%20ARC-Challenge%20for%20Llama-2-70B-Chat%2C%20outperforming%20other%20Llama-2-based%0Amodels%20not%20relying%20on%20additionally%20sourced%20datasets.%20For%20example%2C%20we%20see%20a%0Alarge%20improvement%20from%2055.6%25%20to%2081.6%25%20on%20GSM8K%20and%20an%20accuracy%20of%2088.7%25%20with%0Amajority%20voting%20out%20of%2032%20samples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19733v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIterative%2520Reasoning%2520Preference%2520Optimization%26entry.906535625%3DRichard%2520Yuanzhe%2520Pang%2520and%2520Weizhe%2520Yuan%2520and%2520Kyunghyun%2520Cho%2520and%2520He%2520He%2520and%2520Sainbayar%2520Sukhbaatar%2520and%2520Jason%2520Weston%26entry.1292438233%3D%2520%2520Iterative%2520preference%2520optimization%2520methods%2520have%2520recently%2520been%2520shown%2520to%2520perform%250Awell%2520for%2520general%2520instruction%2520tuning%2520tasks%252C%2520but%2520typically%2520make%2520little%250Aimprovement%2520on%2520reasoning%2520tasks%2520%2528Yuan%2520et%2520al.%252C%25202024%252C%2520Chen%2520et%2520al.%252C%25202024%2529.%2520In%2520this%250Awork%2520we%2520develop%2520an%2520iterative%2520approach%2520that%2520optimizes%2520the%2520preference%2520between%250Acompeting%2520generated%2520Chain-of-Thought%2520%2528CoT%2529%2520candidates%2520by%2520optimizing%2520for%2520winning%250Avs.%2520losing%2520reasoning%2520steps%2520that%2520lead%2520to%2520the%2520correct%2520answer.%2520We%2520train%2520using%2520a%250Amodified%2520DPO%2520loss%2520%2528Rafailov%2520et%2520al.%252C%25202023%2529%2520with%2520an%2520additional%2520negative%250Alog-likelihood%2520term%252C%2520which%2520we%2520find%2520to%2520be%2520crucial.%2520We%2520show%2520reasoning%2520improves%250Aacross%2520repeated%2520iterations%2520of%2520this%2520scheme.%2520While%2520only%2520relying%2520on%2520examples%2520in%250Athe%2520training%2520set%252C%2520our%2520approach%2520results%2520in%2520increasing%2520accuracy%2520on%2520GSM8K%252C%2520MATH%252C%250Aand%2520ARC-Challenge%2520for%2520Llama-2-70B-Chat%252C%2520outperforming%2520other%2520Llama-2-based%250Amodels%2520not%2520relying%2520on%2520additionally%2520sourced%2520datasets.%2520For%2520example%252C%2520we%2520see%2520a%250Alarge%2520improvement%2520from%252055.6%2525%2520to%252081.6%2525%2520on%2520GSM8K%2520and%2520an%2520accuracy%2520of%252088.7%2525%2520with%250Amajority%2520voting%2520out%2520of%252032%2520samples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.19733v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Iterative%20Reasoning%20Preference%20Optimization&entry.906535625=Richard%20Yuanzhe%20Pang%20and%20Weizhe%20Yuan%20and%20Kyunghyun%20Cho%20and%20He%20He%20and%20Sainbayar%20Sukhbaatar%20and%20Jason%20Weston&entry.1292438233=%20%20Iterative%20preference%20optimization%20methods%20have%20recently%20been%20shown%20to%20perform%0Awell%20for%20general%20instruction%20tuning%20tasks%2C%20but%20typically%20make%20little%0Aimprovement%20on%20reasoning%20tasks%20%28Yuan%20et%20al.%2C%202024%2C%20Chen%20et%20al.%2C%202024%29.%20In%20this%0Awork%20we%20develop%20an%20iterative%20approach%20that%20optimizes%20the%20preference%20between%0Acompeting%20generated%20Chain-of-Thought%20%28CoT%29%20candidates%20by%20optimizing%20for%20winning%0Avs.%20losing%20reasoning%20steps%20that%20lead%20to%20the%20correct%20answer.%20We%20train%20using%20a%0Amodified%20DPO%20loss%20%28Rafailov%20et%20al.%2C%202023%29%20with%20an%20additional%20negative%0Alog-likelihood%20term%2C%20which%20we%20find%20to%20be%20crucial.%20We%20show%20reasoning%20improves%0Aacross%20repeated%20iterations%20of%20this%20scheme.%20While%20only%20relying%20on%20examples%20in%0Athe%20training%20set%2C%20our%20approach%20results%20in%20increasing%20accuracy%20on%20GSM8K%2C%20MATH%2C%0Aand%20ARC-Challenge%20for%20Llama-2-70B-Chat%2C%20outperforming%20other%20Llama-2-based%0Amodels%20not%20relying%20on%20additionally%20sourced%20datasets.%20For%20example%2C%20we%20see%20a%0Alarge%20improvement%20from%2055.6%25%20to%2081.6%25%20on%20GSM8K%20and%20an%20accuracy%20of%2088.7%25%20with%0Amajority%20voting%20out%20of%2032%20samples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19733v3&entry.124074799=Read"},
{"title": "Learning Antenna Pointing Correction in Operations: Efficient\n  Calibration of a Black Box", "author": "Leif Bergerhoff", "abstract": "  We propose an efficient offline pointing calibration method for operational\nantenna systems which does not require any downtime. Our approach minimizes the\ncalibration effort and exploits technical signal information which is typically\nused for monitoring and control purposes in ground station operations. Using a\nstandard antenna interface and data from an operational satellite contact, we\ncome up with a robust strategy for training data set generation. On top of\nthis, we learn the parameters of a suitable coordinate transform by means of\nlinear regression. In our experiments, we show the usefulness of the method in\na real-world setup.\n", "link": "http://arxiv.org/abs/2405.15247v2", "date": "2024-06-26", "relevancy": 2.2321, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4588}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4456}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Antenna%20Pointing%20Correction%20in%20Operations%3A%20Efficient%0A%20%20Calibration%20of%20a%20Black%20Box&body=Title%3A%20Learning%20Antenna%20Pointing%20Correction%20in%20Operations%3A%20Efficient%0A%20%20Calibration%20of%20a%20Black%20Box%0AAuthor%3A%20Leif%20Bergerhoff%0AAbstract%3A%20%20%20We%20propose%20an%20efficient%20offline%20pointing%20calibration%20method%20for%20operational%0Aantenna%20systems%20which%20does%20not%20require%20any%20downtime.%20Our%20approach%20minimizes%20the%0Acalibration%20effort%20and%20exploits%20technical%20signal%20information%20which%20is%20typically%0Aused%20for%20monitoring%20and%20control%20purposes%20in%20ground%20station%20operations.%20Using%20a%0Astandard%20antenna%20interface%20and%20data%20from%20an%20operational%20satellite%20contact%2C%20we%0Acome%20up%20with%20a%20robust%20strategy%20for%20training%20data%20set%20generation.%20On%20top%20of%0Athis%2C%20we%20learn%20the%20parameters%20of%20a%20suitable%20coordinate%20transform%20by%20means%20of%0Alinear%20regression.%20In%20our%20experiments%2C%20we%20show%20the%20usefulness%20of%20the%20method%20in%0Aa%20real-world%20setup.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15247v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Antenna%2520Pointing%2520Correction%2520in%2520Operations%253A%2520Efficient%250A%2520%2520Calibration%2520of%2520a%2520Black%2520Box%26entry.906535625%3DLeif%2520Bergerhoff%26entry.1292438233%3D%2520%2520We%2520propose%2520an%2520efficient%2520offline%2520pointing%2520calibration%2520method%2520for%2520operational%250Aantenna%2520systems%2520which%2520does%2520not%2520require%2520any%2520downtime.%2520Our%2520approach%2520minimizes%2520the%250Acalibration%2520effort%2520and%2520exploits%2520technical%2520signal%2520information%2520which%2520is%2520typically%250Aused%2520for%2520monitoring%2520and%2520control%2520purposes%2520in%2520ground%2520station%2520operations.%2520Using%2520a%250Astandard%2520antenna%2520interface%2520and%2520data%2520from%2520an%2520operational%2520satellite%2520contact%252C%2520we%250Acome%2520up%2520with%2520a%2520robust%2520strategy%2520for%2520training%2520data%2520set%2520generation.%2520On%2520top%2520of%250Athis%252C%2520we%2520learn%2520the%2520parameters%2520of%2520a%2520suitable%2520coordinate%2520transform%2520by%2520means%2520of%250Alinear%2520regression.%2520In%2520our%2520experiments%252C%2520we%2520show%2520the%2520usefulness%2520of%2520the%2520method%2520in%250Aa%2520real-world%2520setup.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15247v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Antenna%20Pointing%20Correction%20in%20Operations%3A%20Efficient%0A%20%20Calibration%20of%20a%20Black%20Box&entry.906535625=Leif%20Bergerhoff&entry.1292438233=%20%20We%20propose%20an%20efficient%20offline%20pointing%20calibration%20method%20for%20operational%0Aantenna%20systems%20which%20does%20not%20require%20any%20downtime.%20Our%20approach%20minimizes%20the%0Acalibration%20effort%20and%20exploits%20technical%20signal%20information%20which%20is%20typically%0Aused%20for%20monitoring%20and%20control%20purposes%20in%20ground%20station%20operations.%20Using%20a%0Astandard%20antenna%20interface%20and%20data%20from%20an%20operational%20satellite%20contact%2C%20we%0Acome%20up%20with%20a%20robust%20strategy%20for%20training%20data%20set%20generation.%20On%20top%20of%0Athis%2C%20we%20learn%20the%20parameters%20of%20a%20suitable%20coordinate%20transform%20by%20means%20of%0Alinear%20regression.%20In%20our%20experiments%2C%20we%20show%20the%20usefulness%20of%20the%20method%20in%0Aa%20real-world%20setup.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15247v2&entry.124074799=Read"},
{"title": "Multi-modal Evidential Fusion Network for Trusted PET/CT Tumor\n  Segmentation", "author": "Yuxuan Qi and Li Lin and Jiajun Wang and Jingya Zhang and Bin Zhang", "abstract": "  Accurate segmentation of tumors in PET/CT images is important in\ncomputer-aided diagnosis and treatment of cancer. The key issue of such a\nsegmentation problem lies in the effective integration of complementary\ninformation from PET and CT images. However, the quality of PET and CT images\nvaries widely in clinical settings, which leads to uncertainty in the modality\ninformation extracted by networks. To take the uncertainty into account in\nmulti-modal information fusion, this paper proposes a novel Multi-modal\nEvidential Fusion Network (MEFN) comprising a Cross-Modal Feature Learning\n(CFL) module and a Multi-modal Trusted Fusion (MTF) module. The CFL module\nreduces the domain gap upon modality conversion and highlights common tumor\nfeatures, thereby alleviating the needs of the segmentation module to handle\nmodality specificity. The MTF module utilizes mutual attention mechanisms and\nan uncertainty calibrator to fuse modality features based on modality\nuncertainty and then fuse the segmentation results under the guidance of\nDempster-Shafer Theory. Besides, a new uncertainty perceptual loss is\nintroduced to force the model focusing on uncertain features and hence improve\nits ability to extract trusted modality information. Extensive comparative\nexperiments are conducted on two publicly available PET/CT datasets to evaluate\nthe performance of our proposed method whose results demonstrate that our MEFN\nsignificantly outperforms state-of-the-art methods with improvements of 2.15%\nand 3.23% in DSC scores on the AutoPET dataset and the Hecktor dataset,\nrespectively. More importantly, our model can provide radiologists with\ncredible uncertainty of the segmentation results for their decision in\naccepting or rejecting the automatic segmentation results, which is\nparticularly important for clinical applications. Our code will be available at\nhttps://github.com/QPaws/MEFN.\n", "link": "http://arxiv.org/abs/2406.18327v1", "date": "2024-06-26", "relevancy": 2.2263, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6185}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5476}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5408}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-modal%20Evidential%20Fusion%20Network%20for%20Trusted%20PET/CT%20Tumor%0A%20%20Segmentation&body=Title%3A%20Multi-modal%20Evidential%20Fusion%20Network%20for%20Trusted%20PET/CT%20Tumor%0A%20%20Segmentation%0AAuthor%3A%20Yuxuan%20Qi%20and%20Li%20Lin%20and%20Jiajun%20Wang%20and%20Jingya%20Zhang%20and%20Bin%20Zhang%0AAbstract%3A%20%20%20Accurate%20segmentation%20of%20tumors%20in%20PET/CT%20images%20is%20important%20in%0Acomputer-aided%20diagnosis%20and%20treatment%20of%20cancer.%20The%20key%20issue%20of%20such%20a%0Asegmentation%20problem%20lies%20in%20the%20effective%20integration%20of%20complementary%0Ainformation%20from%20PET%20and%20CT%20images.%20However%2C%20the%20quality%20of%20PET%20and%20CT%20images%0Avaries%20widely%20in%20clinical%20settings%2C%20which%20leads%20to%20uncertainty%20in%20the%20modality%0Ainformation%20extracted%20by%20networks.%20To%20take%20the%20uncertainty%20into%20account%20in%0Amulti-modal%20information%20fusion%2C%20this%20paper%20proposes%20a%20novel%20Multi-modal%0AEvidential%20Fusion%20Network%20%28MEFN%29%20comprising%20a%20Cross-Modal%20Feature%20Learning%0A%28CFL%29%20module%20and%20a%20Multi-modal%20Trusted%20Fusion%20%28MTF%29%20module.%20The%20CFL%20module%0Areduces%20the%20domain%20gap%20upon%20modality%20conversion%20and%20highlights%20common%20tumor%0Afeatures%2C%20thereby%20alleviating%20the%20needs%20of%20the%20segmentation%20module%20to%20handle%0Amodality%20specificity.%20The%20MTF%20module%20utilizes%20mutual%20attention%20mechanisms%20and%0Aan%20uncertainty%20calibrator%20to%20fuse%20modality%20features%20based%20on%20modality%0Auncertainty%20and%20then%20fuse%20the%20segmentation%20results%20under%20the%20guidance%20of%0ADempster-Shafer%20Theory.%20Besides%2C%20a%20new%20uncertainty%20perceptual%20loss%20is%0Aintroduced%20to%20force%20the%20model%20focusing%20on%20uncertain%20features%20and%20hence%20improve%0Aits%20ability%20to%20extract%20trusted%20modality%20information.%20Extensive%20comparative%0Aexperiments%20are%20conducted%20on%20two%20publicly%20available%20PET/CT%20datasets%20to%20evaluate%0Athe%20performance%20of%20our%20proposed%20method%20whose%20results%20demonstrate%20that%20our%20MEFN%0Asignificantly%20outperforms%20state-of-the-art%20methods%20with%20improvements%20of%202.15%25%0Aand%203.23%25%20in%20DSC%20scores%20on%20the%20AutoPET%20dataset%20and%20the%20Hecktor%20dataset%2C%0Arespectively.%20More%20importantly%2C%20our%20model%20can%20provide%20radiologists%20with%0Acredible%20uncertainty%20of%20the%20segmentation%20results%20for%20their%20decision%20in%0Aaccepting%20or%20rejecting%20the%20automatic%20segmentation%20results%2C%20which%20is%0Aparticularly%20important%20for%20clinical%20applications.%20Our%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/QPaws/MEFN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18327v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-modal%2520Evidential%2520Fusion%2520Network%2520for%2520Trusted%2520PET/CT%2520Tumor%250A%2520%2520Segmentation%26entry.906535625%3DYuxuan%2520Qi%2520and%2520Li%2520Lin%2520and%2520Jiajun%2520Wang%2520and%2520Jingya%2520Zhang%2520and%2520Bin%2520Zhang%26entry.1292438233%3D%2520%2520Accurate%2520segmentation%2520of%2520tumors%2520in%2520PET/CT%2520images%2520is%2520important%2520in%250Acomputer-aided%2520diagnosis%2520and%2520treatment%2520of%2520cancer.%2520The%2520key%2520issue%2520of%2520such%2520a%250Asegmentation%2520problem%2520lies%2520in%2520the%2520effective%2520integration%2520of%2520complementary%250Ainformation%2520from%2520PET%2520and%2520CT%2520images.%2520However%252C%2520the%2520quality%2520of%2520PET%2520and%2520CT%2520images%250Avaries%2520widely%2520in%2520clinical%2520settings%252C%2520which%2520leads%2520to%2520uncertainty%2520in%2520the%2520modality%250Ainformation%2520extracted%2520by%2520networks.%2520To%2520take%2520the%2520uncertainty%2520into%2520account%2520in%250Amulti-modal%2520information%2520fusion%252C%2520this%2520paper%2520proposes%2520a%2520novel%2520Multi-modal%250AEvidential%2520Fusion%2520Network%2520%2528MEFN%2529%2520comprising%2520a%2520Cross-Modal%2520Feature%2520Learning%250A%2528CFL%2529%2520module%2520and%2520a%2520Multi-modal%2520Trusted%2520Fusion%2520%2528MTF%2529%2520module.%2520The%2520CFL%2520module%250Areduces%2520the%2520domain%2520gap%2520upon%2520modality%2520conversion%2520and%2520highlights%2520common%2520tumor%250Afeatures%252C%2520thereby%2520alleviating%2520the%2520needs%2520of%2520the%2520segmentation%2520module%2520to%2520handle%250Amodality%2520specificity.%2520The%2520MTF%2520module%2520utilizes%2520mutual%2520attention%2520mechanisms%2520and%250Aan%2520uncertainty%2520calibrator%2520to%2520fuse%2520modality%2520features%2520based%2520on%2520modality%250Auncertainty%2520and%2520then%2520fuse%2520the%2520segmentation%2520results%2520under%2520the%2520guidance%2520of%250ADempster-Shafer%2520Theory.%2520Besides%252C%2520a%2520new%2520uncertainty%2520perceptual%2520loss%2520is%250Aintroduced%2520to%2520force%2520the%2520model%2520focusing%2520on%2520uncertain%2520features%2520and%2520hence%2520improve%250Aits%2520ability%2520to%2520extract%2520trusted%2520modality%2520information.%2520Extensive%2520comparative%250Aexperiments%2520are%2520conducted%2520on%2520two%2520publicly%2520available%2520PET/CT%2520datasets%2520to%2520evaluate%250Athe%2520performance%2520of%2520our%2520proposed%2520method%2520whose%2520results%2520demonstrate%2520that%2520our%2520MEFN%250Asignificantly%2520outperforms%2520state-of-the-art%2520methods%2520with%2520improvements%2520of%25202.15%2525%250Aand%25203.23%2525%2520in%2520DSC%2520scores%2520on%2520the%2520AutoPET%2520dataset%2520and%2520the%2520Hecktor%2520dataset%252C%250Arespectively.%2520More%2520importantly%252C%2520our%2520model%2520can%2520provide%2520radiologists%2520with%250Acredible%2520uncertainty%2520of%2520the%2520segmentation%2520results%2520for%2520their%2520decision%2520in%250Aaccepting%2520or%2520rejecting%2520the%2520automatic%2520segmentation%2520results%252C%2520which%2520is%250Aparticularly%2520important%2520for%2520clinical%2520applications.%2520Our%2520code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/QPaws/MEFN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18327v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-modal%20Evidential%20Fusion%20Network%20for%20Trusted%20PET/CT%20Tumor%0A%20%20Segmentation&entry.906535625=Yuxuan%20Qi%20and%20Li%20Lin%20and%20Jiajun%20Wang%20and%20Jingya%20Zhang%20and%20Bin%20Zhang&entry.1292438233=%20%20Accurate%20segmentation%20of%20tumors%20in%20PET/CT%20images%20is%20important%20in%0Acomputer-aided%20diagnosis%20and%20treatment%20of%20cancer.%20The%20key%20issue%20of%20such%20a%0Asegmentation%20problem%20lies%20in%20the%20effective%20integration%20of%20complementary%0Ainformation%20from%20PET%20and%20CT%20images.%20However%2C%20the%20quality%20of%20PET%20and%20CT%20images%0Avaries%20widely%20in%20clinical%20settings%2C%20which%20leads%20to%20uncertainty%20in%20the%20modality%0Ainformation%20extracted%20by%20networks.%20To%20take%20the%20uncertainty%20into%20account%20in%0Amulti-modal%20information%20fusion%2C%20this%20paper%20proposes%20a%20novel%20Multi-modal%0AEvidential%20Fusion%20Network%20%28MEFN%29%20comprising%20a%20Cross-Modal%20Feature%20Learning%0A%28CFL%29%20module%20and%20a%20Multi-modal%20Trusted%20Fusion%20%28MTF%29%20module.%20The%20CFL%20module%0Areduces%20the%20domain%20gap%20upon%20modality%20conversion%20and%20highlights%20common%20tumor%0Afeatures%2C%20thereby%20alleviating%20the%20needs%20of%20the%20segmentation%20module%20to%20handle%0Amodality%20specificity.%20The%20MTF%20module%20utilizes%20mutual%20attention%20mechanisms%20and%0Aan%20uncertainty%20calibrator%20to%20fuse%20modality%20features%20based%20on%20modality%0Auncertainty%20and%20then%20fuse%20the%20segmentation%20results%20under%20the%20guidance%20of%0ADempster-Shafer%20Theory.%20Besides%2C%20a%20new%20uncertainty%20perceptual%20loss%20is%0Aintroduced%20to%20force%20the%20model%20focusing%20on%20uncertain%20features%20and%20hence%20improve%0Aits%20ability%20to%20extract%20trusted%20modality%20information.%20Extensive%20comparative%0Aexperiments%20are%20conducted%20on%20two%20publicly%20available%20PET/CT%20datasets%20to%20evaluate%0Athe%20performance%20of%20our%20proposed%20method%20whose%20results%20demonstrate%20that%20our%20MEFN%0Asignificantly%20outperforms%20state-of-the-art%20methods%20with%20improvements%20of%202.15%25%0Aand%203.23%25%20in%20DSC%20scores%20on%20the%20AutoPET%20dataset%20and%20the%20Hecktor%20dataset%2C%0Arespectively.%20More%20importantly%2C%20our%20model%20can%20provide%20radiologists%20with%0Acredible%20uncertainty%20of%20the%20segmentation%20results%20for%20their%20decision%20in%0Aaccepting%20or%20rejecting%20the%20automatic%20segmentation%20results%2C%20which%20is%0Aparticularly%20important%20for%20clinical%20applications.%20Our%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/QPaws/MEFN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18327v1&entry.124074799=Read"},
{"title": "Efficient Low-rank Identification via Accelerated Iteratively Reweighted\n  Nuclear Norm Minimization", "author": "Hao Wang and Ye Wang and Xiangyu Yang", "abstract": "  This paper considers the problem of minimizing the sum of a smooth function\nand the Schatten-$p$ norm of the matrix. Our contribution involves proposing\naccelerated iteratively reweighted nuclear norm methods designed for solving\nthe nonconvex low-rank minimization problem. Two major novelties characterize\nour approach. Firstly, the proposed method possesses a rank identification\nproperty, enabling the provable identification of the \"correct\" rank of the\nstationary point within a finite number of iterations. Secondly, we introduce\nan adaptive updating strategy for smoothing parameters. This strategy\nautomatically fixes parameters associated with zero singular values as\nconstants upon detecting the \"correct\" rank while quickly driving the rest of\nthe parameters to zero. This adaptive behavior transforms the algorithm into\none that effectively solves smooth problems after a few iterations, setting our\nwork apart from existing iteratively reweighted methods for low-rank\noptimization. We prove the global convergence of the proposed algorithm,\nguaranteeing that every limit point of the iterates is a critical point.\nFurthermore, a local convergence rate analysis is provided under the\nKurdyka-{\\L}ojasiewicz property. We conduct numerical experiments using both\nsynthetic and real data to showcase our algorithm's efficiency and superiority\nover existing methods.\n", "link": "http://arxiv.org/abs/2406.15713v2", "date": "2024-06-26", "relevancy": 2.2091, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4522}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4428}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Low-rank%20Identification%20via%20Accelerated%20Iteratively%20Reweighted%0A%20%20Nuclear%20Norm%20Minimization&body=Title%3A%20Efficient%20Low-rank%20Identification%20via%20Accelerated%20Iteratively%20Reweighted%0A%20%20Nuclear%20Norm%20Minimization%0AAuthor%3A%20Hao%20Wang%20and%20Ye%20Wang%20and%20Xiangyu%20Yang%0AAbstract%3A%20%20%20This%20paper%20considers%20the%20problem%20of%20minimizing%20the%20sum%20of%20a%20smooth%20function%0Aand%20the%20Schatten-%24p%24%20norm%20of%20the%20matrix.%20Our%20contribution%20involves%20proposing%0Aaccelerated%20iteratively%20reweighted%20nuclear%20norm%20methods%20designed%20for%20solving%0Athe%20nonconvex%20low-rank%20minimization%20problem.%20Two%20major%20novelties%20characterize%0Aour%20approach.%20Firstly%2C%20the%20proposed%20method%20possesses%20a%20rank%20identification%0Aproperty%2C%20enabling%20the%20provable%20identification%20of%20the%20%22correct%22%20rank%20of%20the%0Astationary%20point%20within%20a%20finite%20number%20of%20iterations.%20Secondly%2C%20we%20introduce%0Aan%20adaptive%20updating%20strategy%20for%20smoothing%20parameters.%20This%20strategy%0Aautomatically%20fixes%20parameters%20associated%20with%20zero%20singular%20values%20as%0Aconstants%20upon%20detecting%20the%20%22correct%22%20rank%20while%20quickly%20driving%20the%20rest%20of%0Athe%20parameters%20to%20zero.%20This%20adaptive%20behavior%20transforms%20the%20algorithm%20into%0Aone%20that%20effectively%20solves%20smooth%20problems%20after%20a%20few%20iterations%2C%20setting%20our%0Awork%20apart%20from%20existing%20iteratively%20reweighted%20methods%20for%20low-rank%0Aoptimization.%20We%20prove%20the%20global%20convergence%20of%20the%20proposed%20algorithm%2C%0Aguaranteeing%20that%20every%20limit%20point%20of%20the%20iterates%20is%20a%20critical%20point.%0AFurthermore%2C%20a%20local%20convergence%20rate%20analysis%20is%20provided%20under%20the%0AKurdyka-%7B%5CL%7Dojasiewicz%20property.%20We%20conduct%20numerical%20experiments%20using%20both%0Asynthetic%20and%20real%20data%20to%20showcase%20our%20algorithm%27s%20efficiency%20and%20superiority%0Aover%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15713v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Low-rank%2520Identification%2520via%2520Accelerated%2520Iteratively%2520Reweighted%250A%2520%2520Nuclear%2520Norm%2520Minimization%26entry.906535625%3DHao%2520Wang%2520and%2520Ye%2520Wang%2520and%2520Xiangyu%2520Yang%26entry.1292438233%3D%2520%2520This%2520paper%2520considers%2520the%2520problem%2520of%2520minimizing%2520the%2520sum%2520of%2520a%2520smooth%2520function%250Aand%2520the%2520Schatten-%2524p%2524%2520norm%2520of%2520the%2520matrix.%2520Our%2520contribution%2520involves%2520proposing%250Aaccelerated%2520iteratively%2520reweighted%2520nuclear%2520norm%2520methods%2520designed%2520for%2520solving%250Athe%2520nonconvex%2520low-rank%2520minimization%2520problem.%2520Two%2520major%2520novelties%2520characterize%250Aour%2520approach.%2520Firstly%252C%2520the%2520proposed%2520method%2520possesses%2520a%2520rank%2520identification%250Aproperty%252C%2520enabling%2520the%2520provable%2520identification%2520of%2520the%2520%2522correct%2522%2520rank%2520of%2520the%250Astationary%2520point%2520within%2520a%2520finite%2520number%2520of%2520iterations.%2520Secondly%252C%2520we%2520introduce%250Aan%2520adaptive%2520updating%2520strategy%2520for%2520smoothing%2520parameters.%2520This%2520strategy%250Aautomatically%2520fixes%2520parameters%2520associated%2520with%2520zero%2520singular%2520values%2520as%250Aconstants%2520upon%2520detecting%2520the%2520%2522correct%2522%2520rank%2520while%2520quickly%2520driving%2520the%2520rest%2520of%250Athe%2520parameters%2520to%2520zero.%2520This%2520adaptive%2520behavior%2520transforms%2520the%2520algorithm%2520into%250Aone%2520that%2520effectively%2520solves%2520smooth%2520problems%2520after%2520a%2520few%2520iterations%252C%2520setting%2520our%250Awork%2520apart%2520from%2520existing%2520iteratively%2520reweighted%2520methods%2520for%2520low-rank%250Aoptimization.%2520We%2520prove%2520the%2520global%2520convergence%2520of%2520the%2520proposed%2520algorithm%252C%250Aguaranteeing%2520that%2520every%2520limit%2520point%2520of%2520the%2520iterates%2520is%2520a%2520critical%2520point.%250AFurthermore%252C%2520a%2520local%2520convergence%2520rate%2520analysis%2520is%2520provided%2520under%2520the%250AKurdyka-%257B%255CL%257Dojasiewicz%2520property.%2520We%2520conduct%2520numerical%2520experiments%2520using%2520both%250Asynthetic%2520and%2520real%2520data%2520to%2520showcase%2520our%2520algorithm%2527s%2520efficiency%2520and%2520superiority%250Aover%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15713v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Low-rank%20Identification%20via%20Accelerated%20Iteratively%20Reweighted%0A%20%20Nuclear%20Norm%20Minimization&entry.906535625=Hao%20Wang%20and%20Ye%20Wang%20and%20Xiangyu%20Yang&entry.1292438233=%20%20This%20paper%20considers%20the%20problem%20of%20minimizing%20the%20sum%20of%20a%20smooth%20function%0Aand%20the%20Schatten-%24p%24%20norm%20of%20the%20matrix.%20Our%20contribution%20involves%20proposing%0Aaccelerated%20iteratively%20reweighted%20nuclear%20norm%20methods%20designed%20for%20solving%0Athe%20nonconvex%20low-rank%20minimization%20problem.%20Two%20major%20novelties%20characterize%0Aour%20approach.%20Firstly%2C%20the%20proposed%20method%20possesses%20a%20rank%20identification%0Aproperty%2C%20enabling%20the%20provable%20identification%20of%20the%20%22correct%22%20rank%20of%20the%0Astationary%20point%20within%20a%20finite%20number%20of%20iterations.%20Secondly%2C%20we%20introduce%0Aan%20adaptive%20updating%20strategy%20for%20smoothing%20parameters.%20This%20strategy%0Aautomatically%20fixes%20parameters%20associated%20with%20zero%20singular%20values%20as%0Aconstants%20upon%20detecting%20the%20%22correct%22%20rank%20while%20quickly%20driving%20the%20rest%20of%0Athe%20parameters%20to%20zero.%20This%20adaptive%20behavior%20transforms%20the%20algorithm%20into%0Aone%20that%20effectively%20solves%20smooth%20problems%20after%20a%20few%20iterations%2C%20setting%20our%0Awork%20apart%20from%20existing%20iteratively%20reweighted%20methods%20for%20low-rank%0Aoptimization.%20We%20prove%20the%20global%20convergence%20of%20the%20proposed%20algorithm%2C%0Aguaranteeing%20that%20every%20limit%20point%20of%20the%20iterates%20is%20a%20critical%20point.%0AFurthermore%2C%20a%20local%20convergence%20rate%20analysis%20is%20provided%20under%20the%0AKurdyka-%7B%5CL%7Dojasiewicz%20property.%20We%20conduct%20numerical%20experiments%20using%20both%0Asynthetic%20and%20real%20data%20to%20showcase%20our%20algorithm%27s%20efficiency%20and%20superiority%0Aover%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15713v2&entry.124074799=Read"},
{"title": "Combining Reconstruction and Contrastive Methods for Multimodal\n  Representations in RL", "author": "Philipp Becker and Sebastian Mossburger and Fabian Otto and Gerhard Neumann", "abstract": "  Learning self-supervised representations using reconstruction or contrastive\nlosses improves performance and sample complexity of image-based and multimodal\nreinforcement learning (RL). Here, different self-supervised loss functions\nhave distinct advantages and limitations depending on the information density\nof the underlying sensor modality. Reconstruction provides strong learning\nsignals but is susceptible to distractions and spurious information. While\ncontrastive approaches can ignore those, they may fail to capture all relevant\ndetails and can lead to representation collapse. For multimodal RL, this\nsuggests that different modalities should be treated differently based on the\namount of distractions in the signal. We propose Contrastive Reconstructive\nAggregated representation Learning (CoRAL), a unified framework enabling us to\nchoose the most appropriate self-supervised loss for each sensor modality and\nallowing the representation to better focus on relevant aspects. We evaluate\nCoRAL's benefits on a wide range of tasks with images containing distractions\nor occlusions, a new locomotion suite, and a challenging manipulation suite\nwith visually realistic distractions. Our results show that learning a\nmultimodal representation by combining contrastive and reconstruction-based\nlosses can significantly improve performance and solve tasks that are out of\nreach for more naive representation learning approaches and other recent\nbaselines.\n", "link": "http://arxiv.org/abs/2302.05342v4", "date": "2024-06-26", "relevancy": 2.1967, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5555}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5533}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Combining%20Reconstruction%20and%20Contrastive%20Methods%20for%20Multimodal%0A%20%20Representations%20in%20RL&body=Title%3A%20Combining%20Reconstruction%20and%20Contrastive%20Methods%20for%20Multimodal%0A%20%20Representations%20in%20RL%0AAuthor%3A%20Philipp%20Becker%20and%20Sebastian%20Mossburger%20and%20Fabian%20Otto%20and%20Gerhard%20Neumann%0AAbstract%3A%20%20%20Learning%20self-supervised%20representations%20using%20reconstruction%20or%20contrastive%0Alosses%20improves%20performance%20and%20sample%20complexity%20of%20image-based%20and%20multimodal%0Areinforcement%20learning%20%28RL%29.%20Here%2C%20different%20self-supervised%20loss%20functions%0Ahave%20distinct%20advantages%20and%20limitations%20depending%20on%20the%20information%20density%0Aof%20the%20underlying%20sensor%20modality.%20Reconstruction%20provides%20strong%20learning%0Asignals%20but%20is%20susceptible%20to%20distractions%20and%20spurious%20information.%20While%0Acontrastive%20approaches%20can%20ignore%20those%2C%20they%20may%20fail%20to%20capture%20all%20relevant%0Adetails%20and%20can%20lead%20to%20representation%20collapse.%20For%20multimodal%20RL%2C%20this%0Asuggests%20that%20different%20modalities%20should%20be%20treated%20differently%20based%20on%20the%0Aamount%20of%20distractions%20in%20the%20signal.%20We%20propose%20Contrastive%20Reconstructive%0AAggregated%20representation%20Learning%20%28CoRAL%29%2C%20a%20unified%20framework%20enabling%20us%20to%0Achoose%20the%20most%20appropriate%20self-supervised%20loss%20for%20each%20sensor%20modality%20and%0Aallowing%20the%20representation%20to%20better%20focus%20on%20relevant%20aspects.%20We%20evaluate%0ACoRAL%27s%20benefits%20on%20a%20wide%20range%20of%20tasks%20with%20images%20containing%20distractions%0Aor%20occlusions%2C%20a%20new%20locomotion%20suite%2C%20and%20a%20challenging%20manipulation%20suite%0Awith%20visually%20realistic%20distractions.%20Our%20results%20show%20that%20learning%20a%0Amultimodal%20representation%20by%20combining%20contrastive%20and%20reconstruction-based%0Alosses%20can%20significantly%20improve%20performance%20and%20solve%20tasks%20that%20are%20out%20of%0Areach%20for%20more%20naive%20representation%20learning%20approaches%20and%20other%20recent%0Abaselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.05342v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCombining%2520Reconstruction%2520and%2520Contrastive%2520Methods%2520for%2520Multimodal%250A%2520%2520Representations%2520in%2520RL%26entry.906535625%3DPhilipp%2520Becker%2520and%2520Sebastian%2520Mossburger%2520and%2520Fabian%2520Otto%2520and%2520Gerhard%2520Neumann%26entry.1292438233%3D%2520%2520Learning%2520self-supervised%2520representations%2520using%2520reconstruction%2520or%2520contrastive%250Alosses%2520improves%2520performance%2520and%2520sample%2520complexity%2520of%2520image-based%2520and%2520multimodal%250Areinforcement%2520learning%2520%2528RL%2529.%2520Here%252C%2520different%2520self-supervised%2520loss%2520functions%250Ahave%2520distinct%2520advantages%2520and%2520limitations%2520depending%2520on%2520the%2520information%2520density%250Aof%2520the%2520underlying%2520sensor%2520modality.%2520Reconstruction%2520provides%2520strong%2520learning%250Asignals%2520but%2520is%2520susceptible%2520to%2520distractions%2520and%2520spurious%2520information.%2520While%250Acontrastive%2520approaches%2520can%2520ignore%2520those%252C%2520they%2520may%2520fail%2520to%2520capture%2520all%2520relevant%250Adetails%2520and%2520can%2520lead%2520to%2520representation%2520collapse.%2520For%2520multimodal%2520RL%252C%2520this%250Asuggests%2520that%2520different%2520modalities%2520should%2520be%2520treated%2520differently%2520based%2520on%2520the%250Aamount%2520of%2520distractions%2520in%2520the%2520signal.%2520We%2520propose%2520Contrastive%2520Reconstructive%250AAggregated%2520representation%2520Learning%2520%2528CoRAL%2529%252C%2520a%2520unified%2520framework%2520enabling%2520us%2520to%250Achoose%2520the%2520most%2520appropriate%2520self-supervised%2520loss%2520for%2520each%2520sensor%2520modality%2520and%250Aallowing%2520the%2520representation%2520to%2520better%2520focus%2520on%2520relevant%2520aspects.%2520We%2520evaluate%250ACoRAL%2527s%2520benefits%2520on%2520a%2520wide%2520range%2520of%2520tasks%2520with%2520images%2520containing%2520distractions%250Aor%2520occlusions%252C%2520a%2520new%2520locomotion%2520suite%252C%2520and%2520a%2520challenging%2520manipulation%2520suite%250Awith%2520visually%2520realistic%2520distractions.%2520Our%2520results%2520show%2520that%2520learning%2520a%250Amultimodal%2520representation%2520by%2520combining%2520contrastive%2520and%2520reconstruction-based%250Alosses%2520can%2520significantly%2520improve%2520performance%2520and%2520solve%2520tasks%2520that%2520are%2520out%2520of%250Areach%2520for%2520more%2520naive%2520representation%2520learning%2520approaches%2520and%2520other%2520recent%250Abaselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.05342v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Combining%20Reconstruction%20and%20Contrastive%20Methods%20for%20Multimodal%0A%20%20Representations%20in%20RL&entry.906535625=Philipp%20Becker%20and%20Sebastian%20Mossburger%20and%20Fabian%20Otto%20and%20Gerhard%20Neumann&entry.1292438233=%20%20Learning%20self-supervised%20representations%20using%20reconstruction%20or%20contrastive%0Alosses%20improves%20performance%20and%20sample%20complexity%20of%20image-based%20and%20multimodal%0Areinforcement%20learning%20%28RL%29.%20Here%2C%20different%20self-supervised%20loss%20functions%0Ahave%20distinct%20advantages%20and%20limitations%20depending%20on%20the%20information%20density%0Aof%20the%20underlying%20sensor%20modality.%20Reconstruction%20provides%20strong%20learning%0Asignals%20but%20is%20susceptible%20to%20distractions%20and%20spurious%20information.%20While%0Acontrastive%20approaches%20can%20ignore%20those%2C%20they%20may%20fail%20to%20capture%20all%20relevant%0Adetails%20and%20can%20lead%20to%20representation%20collapse.%20For%20multimodal%20RL%2C%20this%0Asuggests%20that%20different%20modalities%20should%20be%20treated%20differently%20based%20on%20the%0Aamount%20of%20distractions%20in%20the%20signal.%20We%20propose%20Contrastive%20Reconstructive%0AAggregated%20representation%20Learning%20%28CoRAL%29%2C%20a%20unified%20framework%20enabling%20us%20to%0Achoose%20the%20most%20appropriate%20self-supervised%20loss%20for%20each%20sensor%20modality%20and%0Aallowing%20the%20representation%20to%20better%20focus%20on%20relevant%20aspects.%20We%20evaluate%0ACoRAL%27s%20benefits%20on%20a%20wide%20range%20of%20tasks%20with%20images%20containing%20distractions%0Aor%20occlusions%2C%20a%20new%20locomotion%20suite%2C%20and%20a%20challenging%20manipulation%20suite%0Awith%20visually%20realistic%20distractions.%20Our%20results%20show%20that%20learning%20a%0Amultimodal%20representation%20by%20combining%20contrastive%20and%20reconstruction-based%0Alosses%20can%20significantly%20improve%20performance%20and%20solve%20tasks%20that%20are%20out%20of%0Areach%20for%20more%20naive%20representation%20learning%20approaches%20and%20other%20recent%0Abaselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.05342v4&entry.124074799=Read"},
{"title": "Editable Scene Simulation for Autonomous Driving via Collaborative\n  LLM-Agents", "author": "Yuxi Wei and Zi Wang and Yifan Lu and Chenxin Xu and Changxing Liu and Hao Zhao and Siheng Chen and Yanfeng Wang", "abstract": "  Scene simulation in autonomous driving has gained significant attention\nbecause of its huge potential for generating customized data. However, existing\neditable scene simulation approaches face limitations in terms of user\ninteraction efficiency, multi-camera photo-realistic rendering and external\ndigital assets integration. To address these challenges, this paper introduces\nChatSim, the first system that enables editable photo-realistic 3D driving\nscene simulations via natural language commands with external digital assets.\nTo enable editing with high command flexibility,~ChatSim leverages a large\nlanguage model (LLM) agent collaboration framework. To generate photo-realistic\noutcomes, ChatSim employs a novel multi-camera neural radiance field method.\nFurthermore, to unleash the potential of extensive high-quality digital assets,\nChatSim employs a novel multi-camera lighting estimation method to achieve\nscene-consistent assets' rendering. Our experiments on Waymo Open Dataset\ndemonstrate that ChatSim can handle complex language commands and generate\ncorresponding photo-realistic scene videos.\n", "link": "http://arxiv.org/abs/2402.05746v3", "date": "2024-06-26", "relevancy": 2.1928, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5779}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5423}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Editable%20Scene%20Simulation%20for%20Autonomous%20Driving%20via%20Collaborative%0A%20%20LLM-Agents&body=Title%3A%20Editable%20Scene%20Simulation%20for%20Autonomous%20Driving%20via%20Collaborative%0A%20%20LLM-Agents%0AAuthor%3A%20Yuxi%20Wei%20and%20Zi%20Wang%20and%20Yifan%20Lu%20and%20Chenxin%20Xu%20and%20Changxing%20Liu%20and%20Hao%20Zhao%20and%20Siheng%20Chen%20and%20Yanfeng%20Wang%0AAbstract%3A%20%20%20Scene%20simulation%20in%20autonomous%20driving%20has%20gained%20significant%20attention%0Abecause%20of%20its%20huge%20potential%20for%20generating%20customized%20data.%20However%2C%20existing%0Aeditable%20scene%20simulation%20approaches%20face%20limitations%20in%20terms%20of%20user%0Ainteraction%20efficiency%2C%20multi-camera%20photo-realistic%20rendering%20and%20external%0Adigital%20assets%20integration.%20To%20address%20these%20challenges%2C%20this%20paper%20introduces%0AChatSim%2C%20the%20first%20system%20that%20enables%20editable%20photo-realistic%203D%20driving%0Ascene%20simulations%20via%20natural%20language%20commands%20with%20external%20digital%20assets.%0ATo%20enable%20editing%20with%20high%20command%20flexibility%2C~ChatSim%20leverages%20a%20large%0Alanguage%20model%20%28LLM%29%20agent%20collaboration%20framework.%20To%20generate%20photo-realistic%0Aoutcomes%2C%20ChatSim%20employs%20a%20novel%20multi-camera%20neural%20radiance%20field%20method.%0AFurthermore%2C%20to%20unleash%20the%20potential%20of%20extensive%20high-quality%20digital%20assets%2C%0AChatSim%20employs%20a%20novel%20multi-camera%20lighting%20estimation%20method%20to%20achieve%0Ascene-consistent%20assets%27%20rendering.%20Our%20experiments%20on%20Waymo%20Open%20Dataset%0Ademonstrate%20that%20ChatSim%20can%20handle%20complex%20language%20commands%20and%20generate%0Acorresponding%20photo-realistic%20scene%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.05746v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEditable%2520Scene%2520Simulation%2520for%2520Autonomous%2520Driving%2520via%2520Collaborative%250A%2520%2520LLM-Agents%26entry.906535625%3DYuxi%2520Wei%2520and%2520Zi%2520Wang%2520and%2520Yifan%2520Lu%2520and%2520Chenxin%2520Xu%2520and%2520Changxing%2520Liu%2520and%2520Hao%2520Zhao%2520and%2520Siheng%2520Chen%2520and%2520Yanfeng%2520Wang%26entry.1292438233%3D%2520%2520Scene%2520simulation%2520in%2520autonomous%2520driving%2520has%2520gained%2520significant%2520attention%250Abecause%2520of%2520its%2520huge%2520potential%2520for%2520generating%2520customized%2520data.%2520However%252C%2520existing%250Aeditable%2520scene%2520simulation%2520approaches%2520face%2520limitations%2520in%2520terms%2520of%2520user%250Ainteraction%2520efficiency%252C%2520multi-camera%2520photo-realistic%2520rendering%2520and%2520external%250Adigital%2520assets%2520integration.%2520To%2520address%2520these%2520challenges%252C%2520this%2520paper%2520introduces%250AChatSim%252C%2520the%2520first%2520system%2520that%2520enables%2520editable%2520photo-realistic%25203D%2520driving%250Ascene%2520simulations%2520via%2520natural%2520language%2520commands%2520with%2520external%2520digital%2520assets.%250ATo%2520enable%2520editing%2520with%2520high%2520command%2520flexibility%252C~ChatSim%2520leverages%2520a%2520large%250Alanguage%2520model%2520%2528LLM%2529%2520agent%2520collaboration%2520framework.%2520To%2520generate%2520photo-realistic%250Aoutcomes%252C%2520ChatSim%2520employs%2520a%2520novel%2520multi-camera%2520neural%2520radiance%2520field%2520method.%250AFurthermore%252C%2520to%2520unleash%2520the%2520potential%2520of%2520extensive%2520high-quality%2520digital%2520assets%252C%250AChatSim%2520employs%2520a%2520novel%2520multi-camera%2520lighting%2520estimation%2520method%2520to%2520achieve%250Ascene-consistent%2520assets%2527%2520rendering.%2520Our%2520experiments%2520on%2520Waymo%2520Open%2520Dataset%250Ademonstrate%2520that%2520ChatSim%2520can%2520handle%2520complex%2520language%2520commands%2520and%2520generate%250Acorresponding%2520photo-realistic%2520scene%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.05746v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Editable%20Scene%20Simulation%20for%20Autonomous%20Driving%20via%20Collaborative%0A%20%20LLM-Agents&entry.906535625=Yuxi%20Wei%20and%20Zi%20Wang%20and%20Yifan%20Lu%20and%20Chenxin%20Xu%20and%20Changxing%20Liu%20and%20Hao%20Zhao%20and%20Siheng%20Chen%20and%20Yanfeng%20Wang&entry.1292438233=%20%20Scene%20simulation%20in%20autonomous%20driving%20has%20gained%20significant%20attention%0Abecause%20of%20its%20huge%20potential%20for%20generating%20customized%20data.%20However%2C%20existing%0Aeditable%20scene%20simulation%20approaches%20face%20limitations%20in%20terms%20of%20user%0Ainteraction%20efficiency%2C%20multi-camera%20photo-realistic%20rendering%20and%20external%0Adigital%20assets%20integration.%20To%20address%20these%20challenges%2C%20this%20paper%20introduces%0AChatSim%2C%20the%20first%20system%20that%20enables%20editable%20photo-realistic%203D%20driving%0Ascene%20simulations%20via%20natural%20language%20commands%20with%20external%20digital%20assets.%0ATo%20enable%20editing%20with%20high%20command%20flexibility%2C~ChatSim%20leverages%20a%20large%0Alanguage%20model%20%28LLM%29%20agent%20collaboration%20framework.%20To%20generate%20photo-realistic%0Aoutcomes%2C%20ChatSim%20employs%20a%20novel%20multi-camera%20neural%20radiance%20field%20method.%0AFurthermore%2C%20to%20unleash%20the%20potential%20of%20extensive%20high-quality%20digital%20assets%2C%0AChatSim%20employs%20a%20novel%20multi-camera%20lighting%20estimation%20method%20to%20achieve%0Ascene-consistent%20assets%27%20rendering.%20Our%20experiments%20on%20Waymo%20Open%20Dataset%0Ademonstrate%20that%20ChatSim%20can%20handle%20complex%20language%20commands%20and%20generate%0Acorresponding%20photo-realistic%20scene%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.05746v3&entry.124074799=Read"},
{"title": "EgoVideo: Exploring Egocentric Foundation Model and Downstream\n  Adaptation", "author": "Baoqi Pei and Guo Chen and Jilan Xu and Yuping He and Yicheng Liu and Kanghua Pan and Yifei Huang and Yali Wang and Tong Lu and Limin Wang and Yu Qiao", "abstract": "  In this report, we present our solutions to the EgoVis Challenges in CVPR\n2024, including five tracks in the Ego4D challenge and three tracks in the\nEPIC-Kitchens challenge. Building upon the video-language two-tower model and\nleveraging our meticulously organized egocentric video data, we introduce a\nnovel foundation model called EgoVideo. This model is specifically designed to\ncater to the unique characteristics of egocentric videos and provides strong\nsupport for our competition submissions. In the Ego4D challenges, we tackle\nvarious tasks including Natural Language Queries, Step Grounding, Moment\nQueries, Short-term Object Interaction Anticipation, and Long-term Action\nAnticipation. In addition, we also participate in the EPIC-Kitchens challenge,\nwhere we engage in the Action Recognition, Multiple Instance Retrieval, and\nDomain Adaptation for Action Recognition tracks. By adapting EgoVideo to these\ndiverse tasks, we showcase its versatility and effectiveness in different\negocentric video analysis scenarios, demonstrating the powerful representation\nability of EgoVideo as an egocentric foundation model. Our codebase and\npretrained models are publicly available at\nhttps://github.com/OpenGVLab/EgoVideo.\n", "link": "http://arxiv.org/abs/2406.18070v1", "date": "2024-06-26", "relevancy": 2.1916, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5687}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5545}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5244}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EgoVideo%3A%20Exploring%20Egocentric%20Foundation%20Model%20and%20Downstream%0A%20%20Adaptation&body=Title%3A%20EgoVideo%3A%20Exploring%20Egocentric%20Foundation%20Model%20and%20Downstream%0A%20%20Adaptation%0AAuthor%3A%20Baoqi%20Pei%20and%20Guo%20Chen%20and%20Jilan%20Xu%20and%20Yuping%20He%20and%20Yicheng%20Liu%20and%20Kanghua%20Pan%20and%20Yifei%20Huang%20and%20Yali%20Wang%20and%20Tong%20Lu%20and%20Limin%20Wang%20and%20Yu%20Qiao%0AAbstract%3A%20%20%20In%20this%20report%2C%20we%20present%20our%20solutions%20to%20the%20EgoVis%20Challenges%20in%20CVPR%0A2024%2C%20including%20five%20tracks%20in%20the%20Ego4D%20challenge%20and%20three%20tracks%20in%20the%0AEPIC-Kitchens%20challenge.%20Building%20upon%20the%20video-language%20two-tower%20model%20and%0Aleveraging%20our%20meticulously%20organized%20egocentric%20video%20data%2C%20we%20introduce%20a%0Anovel%20foundation%20model%20called%20EgoVideo.%20This%20model%20is%20specifically%20designed%20to%0Acater%20to%20the%20unique%20characteristics%20of%20egocentric%20videos%20and%20provides%20strong%0Asupport%20for%20our%20competition%20submissions.%20In%20the%20Ego4D%20challenges%2C%20we%20tackle%0Avarious%20tasks%20including%20Natural%20Language%20Queries%2C%20Step%20Grounding%2C%20Moment%0AQueries%2C%20Short-term%20Object%20Interaction%20Anticipation%2C%20and%20Long-term%20Action%0AAnticipation.%20In%20addition%2C%20we%20also%20participate%20in%20the%20EPIC-Kitchens%20challenge%2C%0Awhere%20we%20engage%20in%20the%20Action%20Recognition%2C%20Multiple%20Instance%20Retrieval%2C%20and%0ADomain%20Adaptation%20for%20Action%20Recognition%20tracks.%20By%20adapting%20EgoVideo%20to%20these%0Adiverse%20tasks%2C%20we%20showcase%20its%20versatility%20and%20effectiveness%20in%20different%0Aegocentric%20video%20analysis%20scenarios%2C%20demonstrating%20the%20powerful%20representation%0Aability%20of%20EgoVideo%20as%20an%20egocentric%20foundation%20model.%20Our%20codebase%20and%0Apretrained%20models%20are%20publicly%20available%20at%0Ahttps%3A//github.com/OpenGVLab/EgoVideo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18070v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEgoVideo%253A%2520Exploring%2520Egocentric%2520Foundation%2520Model%2520and%2520Downstream%250A%2520%2520Adaptation%26entry.906535625%3DBaoqi%2520Pei%2520and%2520Guo%2520Chen%2520and%2520Jilan%2520Xu%2520and%2520Yuping%2520He%2520and%2520Yicheng%2520Liu%2520and%2520Kanghua%2520Pan%2520and%2520Yifei%2520Huang%2520and%2520Yali%2520Wang%2520and%2520Tong%2520Lu%2520and%2520Limin%2520Wang%2520and%2520Yu%2520Qiao%26entry.1292438233%3D%2520%2520In%2520this%2520report%252C%2520we%2520present%2520our%2520solutions%2520to%2520the%2520EgoVis%2520Challenges%2520in%2520CVPR%250A2024%252C%2520including%2520five%2520tracks%2520in%2520the%2520Ego4D%2520challenge%2520and%2520three%2520tracks%2520in%2520the%250AEPIC-Kitchens%2520challenge.%2520Building%2520upon%2520the%2520video-language%2520two-tower%2520model%2520and%250Aleveraging%2520our%2520meticulously%2520organized%2520egocentric%2520video%2520data%252C%2520we%2520introduce%2520a%250Anovel%2520foundation%2520model%2520called%2520EgoVideo.%2520This%2520model%2520is%2520specifically%2520designed%2520to%250Acater%2520to%2520the%2520unique%2520characteristics%2520of%2520egocentric%2520videos%2520and%2520provides%2520strong%250Asupport%2520for%2520our%2520competition%2520submissions.%2520In%2520the%2520Ego4D%2520challenges%252C%2520we%2520tackle%250Avarious%2520tasks%2520including%2520Natural%2520Language%2520Queries%252C%2520Step%2520Grounding%252C%2520Moment%250AQueries%252C%2520Short-term%2520Object%2520Interaction%2520Anticipation%252C%2520and%2520Long-term%2520Action%250AAnticipation.%2520In%2520addition%252C%2520we%2520also%2520participate%2520in%2520the%2520EPIC-Kitchens%2520challenge%252C%250Awhere%2520we%2520engage%2520in%2520the%2520Action%2520Recognition%252C%2520Multiple%2520Instance%2520Retrieval%252C%2520and%250ADomain%2520Adaptation%2520for%2520Action%2520Recognition%2520tracks.%2520By%2520adapting%2520EgoVideo%2520to%2520these%250Adiverse%2520tasks%252C%2520we%2520showcase%2520its%2520versatility%2520and%2520effectiveness%2520in%2520different%250Aegocentric%2520video%2520analysis%2520scenarios%252C%2520demonstrating%2520the%2520powerful%2520representation%250Aability%2520of%2520EgoVideo%2520as%2520an%2520egocentric%2520foundation%2520model.%2520Our%2520codebase%2520and%250Apretrained%2520models%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/OpenGVLab/EgoVideo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18070v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EgoVideo%3A%20Exploring%20Egocentric%20Foundation%20Model%20and%20Downstream%0A%20%20Adaptation&entry.906535625=Baoqi%20Pei%20and%20Guo%20Chen%20and%20Jilan%20Xu%20and%20Yuping%20He%20and%20Yicheng%20Liu%20and%20Kanghua%20Pan%20and%20Yifei%20Huang%20and%20Yali%20Wang%20and%20Tong%20Lu%20and%20Limin%20Wang%20and%20Yu%20Qiao&entry.1292438233=%20%20In%20this%20report%2C%20we%20present%20our%20solutions%20to%20the%20EgoVis%20Challenges%20in%20CVPR%0A2024%2C%20including%20five%20tracks%20in%20the%20Ego4D%20challenge%20and%20three%20tracks%20in%20the%0AEPIC-Kitchens%20challenge.%20Building%20upon%20the%20video-language%20two-tower%20model%20and%0Aleveraging%20our%20meticulously%20organized%20egocentric%20video%20data%2C%20we%20introduce%20a%0Anovel%20foundation%20model%20called%20EgoVideo.%20This%20model%20is%20specifically%20designed%20to%0Acater%20to%20the%20unique%20characteristics%20of%20egocentric%20videos%20and%20provides%20strong%0Asupport%20for%20our%20competition%20submissions.%20In%20the%20Ego4D%20challenges%2C%20we%20tackle%0Avarious%20tasks%20including%20Natural%20Language%20Queries%2C%20Step%20Grounding%2C%20Moment%0AQueries%2C%20Short-term%20Object%20Interaction%20Anticipation%2C%20and%20Long-term%20Action%0AAnticipation.%20In%20addition%2C%20we%20also%20participate%20in%20the%20EPIC-Kitchens%20challenge%2C%0Awhere%20we%20engage%20in%20the%20Action%20Recognition%2C%20Multiple%20Instance%20Retrieval%2C%20and%0ADomain%20Adaptation%20for%20Action%20Recognition%20tracks.%20By%20adapting%20EgoVideo%20to%20these%0Adiverse%20tasks%2C%20we%20showcase%20its%20versatility%20and%20effectiveness%20in%20different%0Aegocentric%20video%20analysis%20scenarios%2C%20demonstrating%20the%20powerful%20representation%0Aability%20of%20EgoVideo%20as%20an%20egocentric%20foundation%20model.%20Our%20codebase%20and%0Apretrained%20models%20are%20publicly%20available%20at%0Ahttps%3A//github.com/OpenGVLab/EgoVideo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18070v1&entry.124074799=Read"},
{"title": "Introducing 3DCNN ResNets for ASD full-body kinematic assessment: a\n  comparison with hand-crafted features", "author": "Alberto Altozano and Maria Eleonora Minissi and Mariano Alca\u00f1iz and Javier Mar\u00edn-Morales", "abstract": "  Autism Spectrum Disorder (ASD) is characterized by challenges in social\ncommunication and restricted patterns, with motor abnormalities gaining\ntraction for early detection. However, kinematic analysis in ASD is limited,\noften lacking robust validation and relying on hand-crafted features for single\ntasks, leading to inconsistencies across studies. End-to-end models have\nemerged as promising methods to overcome the need for feature engineering. Our\naim is to propose a newly adapted 3DCNN ResNet from and compare it to widely\nused hand-crafted features for motor ASD assessment. Specifically, we developed\na virtual reality environment with multiple motor tasks and trained models\nusing both approaches. We prioritized a reliable validation framework with\nrepeated cross-validation. Results show the proposed model achieves a maximum\naccuracy of 85$\\pm$3%, outperforming state-of-the-art end-to-end models with\nshort 1-to-3 minute samples. Our comparative analysis with hand-crafted\nfeatures shows feature-engineered models outperformed our end-to-end model in\ncertain tasks. However, our end-to-end model achieved a higher mean AUC of\n0.80$\\pm$0.03. Additionally, statistical differences were found in model\nvariance, with our end-to-end model providing more consistent results with less\nvariability across all VR tasks, demonstrating domain generalization and\nreliability. These findings show that end-to-end models enable less variable\nand context-independent ASD classification without requiring domain knowledge\nor task specificity. However, they also recognize the effectiveness of\nhand-crafted features in specific task scenarios.\n", "link": "http://arxiv.org/abs/2311.14533v3", "date": "2024-06-26", "relevancy": 2.1889, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5659}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5417}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Introducing%203DCNN%20ResNets%20for%20ASD%20full-body%20kinematic%20assessment%3A%20a%0A%20%20comparison%20with%20hand-crafted%20features&body=Title%3A%20Introducing%203DCNN%20ResNets%20for%20ASD%20full-body%20kinematic%20assessment%3A%20a%0A%20%20comparison%20with%20hand-crafted%20features%0AAuthor%3A%20Alberto%20Altozano%20and%20Maria%20Eleonora%20Minissi%20and%20Mariano%20Alca%C3%B1iz%20and%20Javier%20Mar%C3%ADn-Morales%0AAbstract%3A%20%20%20Autism%20Spectrum%20Disorder%20%28ASD%29%20is%20characterized%20by%20challenges%20in%20social%0Acommunication%20and%20restricted%20patterns%2C%20with%20motor%20abnormalities%20gaining%0Atraction%20for%20early%20detection.%20However%2C%20kinematic%20analysis%20in%20ASD%20is%20limited%2C%0Aoften%20lacking%20robust%20validation%20and%20relying%20on%20hand-crafted%20features%20for%20single%0Atasks%2C%20leading%20to%20inconsistencies%20across%20studies.%20End-to-end%20models%20have%0Aemerged%20as%20promising%20methods%20to%20overcome%20the%20need%20for%20feature%20engineering.%20Our%0Aaim%20is%20to%20propose%20a%20newly%20adapted%203DCNN%20ResNet%20from%20and%20compare%20it%20to%20widely%0Aused%20hand-crafted%20features%20for%20motor%20ASD%20assessment.%20Specifically%2C%20we%20developed%0Aa%20virtual%20reality%20environment%20with%20multiple%20motor%20tasks%20and%20trained%20models%0Ausing%20both%20approaches.%20We%20prioritized%20a%20reliable%20validation%20framework%20with%0Arepeated%20cross-validation.%20Results%20show%20the%20proposed%20model%20achieves%20a%20maximum%0Aaccuracy%20of%2085%24%5Cpm%243%25%2C%20outperforming%20state-of-the-art%20end-to-end%20models%20with%0Ashort%201-to-3%20minute%20samples.%20Our%20comparative%20analysis%20with%20hand-crafted%0Afeatures%20shows%20feature-engineered%20models%20outperformed%20our%20end-to-end%20model%20in%0Acertain%20tasks.%20However%2C%20our%20end-to-end%20model%20achieved%20a%20higher%20mean%20AUC%20of%0A0.80%24%5Cpm%240.03.%20Additionally%2C%20statistical%20differences%20were%20found%20in%20model%0Avariance%2C%20with%20our%20end-to-end%20model%20providing%20more%20consistent%20results%20with%20less%0Avariability%20across%20all%20VR%20tasks%2C%20demonstrating%20domain%20generalization%20and%0Areliability.%20These%20findings%20show%20that%20end-to-end%20models%20enable%20less%20variable%0Aand%20context-independent%20ASD%20classification%20without%20requiring%20domain%20knowledge%0Aor%20task%20specificity.%20However%2C%20they%20also%20recognize%20the%20effectiveness%20of%0Ahand-crafted%20features%20in%20specific%20task%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.14533v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntroducing%25203DCNN%2520ResNets%2520for%2520ASD%2520full-body%2520kinematic%2520assessment%253A%2520a%250A%2520%2520comparison%2520with%2520hand-crafted%2520features%26entry.906535625%3DAlberto%2520Altozano%2520and%2520Maria%2520Eleonora%2520Minissi%2520and%2520Mariano%2520Alca%25C3%25B1iz%2520and%2520Javier%2520Mar%25C3%25ADn-Morales%26entry.1292438233%3D%2520%2520Autism%2520Spectrum%2520Disorder%2520%2528ASD%2529%2520is%2520characterized%2520by%2520challenges%2520in%2520social%250Acommunication%2520and%2520restricted%2520patterns%252C%2520with%2520motor%2520abnormalities%2520gaining%250Atraction%2520for%2520early%2520detection.%2520However%252C%2520kinematic%2520analysis%2520in%2520ASD%2520is%2520limited%252C%250Aoften%2520lacking%2520robust%2520validation%2520and%2520relying%2520on%2520hand-crafted%2520features%2520for%2520single%250Atasks%252C%2520leading%2520to%2520inconsistencies%2520across%2520studies.%2520End-to-end%2520models%2520have%250Aemerged%2520as%2520promising%2520methods%2520to%2520overcome%2520the%2520need%2520for%2520feature%2520engineering.%2520Our%250Aaim%2520is%2520to%2520propose%2520a%2520newly%2520adapted%25203DCNN%2520ResNet%2520from%2520and%2520compare%2520it%2520to%2520widely%250Aused%2520hand-crafted%2520features%2520for%2520motor%2520ASD%2520assessment.%2520Specifically%252C%2520we%2520developed%250Aa%2520virtual%2520reality%2520environment%2520with%2520multiple%2520motor%2520tasks%2520and%2520trained%2520models%250Ausing%2520both%2520approaches.%2520We%2520prioritized%2520a%2520reliable%2520validation%2520framework%2520with%250Arepeated%2520cross-validation.%2520Results%2520show%2520the%2520proposed%2520model%2520achieves%2520a%2520maximum%250Aaccuracy%2520of%252085%2524%255Cpm%25243%2525%252C%2520outperforming%2520state-of-the-art%2520end-to-end%2520models%2520with%250Ashort%25201-to-3%2520minute%2520samples.%2520Our%2520comparative%2520analysis%2520with%2520hand-crafted%250Afeatures%2520shows%2520feature-engineered%2520models%2520outperformed%2520our%2520end-to-end%2520model%2520in%250Acertain%2520tasks.%2520However%252C%2520our%2520end-to-end%2520model%2520achieved%2520a%2520higher%2520mean%2520AUC%2520of%250A0.80%2524%255Cpm%25240.03.%2520Additionally%252C%2520statistical%2520differences%2520were%2520found%2520in%2520model%250Avariance%252C%2520with%2520our%2520end-to-end%2520model%2520providing%2520more%2520consistent%2520results%2520with%2520less%250Avariability%2520across%2520all%2520VR%2520tasks%252C%2520demonstrating%2520domain%2520generalization%2520and%250Areliability.%2520These%2520findings%2520show%2520that%2520end-to-end%2520models%2520enable%2520less%2520variable%250Aand%2520context-independent%2520ASD%2520classification%2520without%2520requiring%2520domain%2520knowledge%250Aor%2520task%2520specificity.%2520However%252C%2520they%2520also%2520recognize%2520the%2520effectiveness%2520of%250Ahand-crafted%2520features%2520in%2520specific%2520task%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.14533v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Introducing%203DCNN%20ResNets%20for%20ASD%20full-body%20kinematic%20assessment%3A%20a%0A%20%20comparison%20with%20hand-crafted%20features&entry.906535625=Alberto%20Altozano%20and%20Maria%20Eleonora%20Minissi%20and%20Mariano%20Alca%C3%B1iz%20and%20Javier%20Mar%C3%ADn-Morales&entry.1292438233=%20%20Autism%20Spectrum%20Disorder%20%28ASD%29%20is%20characterized%20by%20challenges%20in%20social%0Acommunication%20and%20restricted%20patterns%2C%20with%20motor%20abnormalities%20gaining%0Atraction%20for%20early%20detection.%20However%2C%20kinematic%20analysis%20in%20ASD%20is%20limited%2C%0Aoften%20lacking%20robust%20validation%20and%20relying%20on%20hand-crafted%20features%20for%20single%0Atasks%2C%20leading%20to%20inconsistencies%20across%20studies.%20End-to-end%20models%20have%0Aemerged%20as%20promising%20methods%20to%20overcome%20the%20need%20for%20feature%20engineering.%20Our%0Aaim%20is%20to%20propose%20a%20newly%20adapted%203DCNN%20ResNet%20from%20and%20compare%20it%20to%20widely%0Aused%20hand-crafted%20features%20for%20motor%20ASD%20assessment.%20Specifically%2C%20we%20developed%0Aa%20virtual%20reality%20environment%20with%20multiple%20motor%20tasks%20and%20trained%20models%0Ausing%20both%20approaches.%20We%20prioritized%20a%20reliable%20validation%20framework%20with%0Arepeated%20cross-validation.%20Results%20show%20the%20proposed%20model%20achieves%20a%20maximum%0Aaccuracy%20of%2085%24%5Cpm%243%25%2C%20outperforming%20state-of-the-art%20end-to-end%20models%20with%0Ashort%201-to-3%20minute%20samples.%20Our%20comparative%20analysis%20with%20hand-crafted%0Afeatures%20shows%20feature-engineered%20models%20outperformed%20our%20end-to-end%20model%20in%0Acertain%20tasks.%20However%2C%20our%20end-to-end%20model%20achieved%20a%20higher%20mean%20AUC%20of%0A0.80%24%5Cpm%240.03.%20Additionally%2C%20statistical%20differences%20were%20found%20in%20model%0Avariance%2C%20with%20our%20end-to-end%20model%20providing%20more%20consistent%20results%20with%20less%0Avariability%20across%20all%20VR%20tasks%2C%20demonstrating%20domain%20generalization%20and%0Areliability.%20These%20findings%20show%20that%20end-to-end%20models%20enable%20less%20variable%0Aand%20context-independent%20ASD%20classification%20without%20requiring%20domain%20knowledge%0Aor%20task%20specificity.%20However%2C%20they%20also%20recognize%20the%20effectiveness%20of%0Ahand-crafted%20features%20in%20specific%20task%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.14533v3&entry.124074799=Read"},
{"title": "Artificial Immune System of Secure Face Recognition Against Adversarial\n  Attacks", "author": "Min Ren and Yunlong Wang and Yuhao Zhu and Yongzhen Huang and Zhenan Sun and Qi Li and Tieniu Tan", "abstract": "  Insect production for food and feed presents a promising supplement to ensure\nfood safety and address the adverse impacts of agriculture on climate and\nenvironment in the future. However, optimisation is required for insect\nproduction to realise its full potential. This can be by targeted improvement\nof traits of interest through selective breeding, an approach which has so far\nbeen underexplored and underutilised in insect farming. Here we present a\ncomprehensive review of the selective breeding framework in the context of\ninsect production. We systematically evaluate adjustments of selective breeding\ntechniques to the realm of insects and highlight the essential components\nintegral to the breeding process. The discussion covers every step of a\nconventional breeding scheme, such as formulation of breeding objectives,\nphenotyping, estimation of genetic parameters and breeding values, selection of\nappropriate breeding strategies, and mitigation of issues associated with\ngenetic diversity depletion and inbreeding. This review combines knowledge from\ndiverse disciplines, bridging the gap between animal breeding, quantitative\ngenetics, evolutionary biology, and entomology, offering an integrated view of\nthe insect breeding research area and uniting knowledge which has previously\nremained scattered across diverse fields of expertise.\n", "link": "http://arxiv.org/abs/2406.18144v1", "date": "2024-06-26", "relevancy": 2.1738, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4383}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4357}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Artificial%20Immune%20System%20of%20Secure%20Face%20Recognition%20Against%20Adversarial%0A%20%20Attacks&body=Title%3A%20Artificial%20Immune%20System%20of%20Secure%20Face%20Recognition%20Against%20Adversarial%0A%20%20Attacks%0AAuthor%3A%20Min%20Ren%20and%20Yunlong%20Wang%20and%20Yuhao%20Zhu%20and%20Yongzhen%20Huang%20and%20Zhenan%20Sun%20and%20Qi%20Li%20and%20Tieniu%20Tan%0AAbstract%3A%20%20%20Insect%20production%20for%20food%20and%20feed%20presents%20a%20promising%20supplement%20to%20ensure%0Afood%20safety%20and%20address%20the%20adverse%20impacts%20of%20agriculture%20on%20climate%20and%0Aenvironment%20in%20the%20future.%20However%2C%20optimisation%20is%20required%20for%20insect%0Aproduction%20to%20realise%20its%20full%20potential.%20This%20can%20be%20by%20targeted%20improvement%0Aof%20traits%20of%20interest%20through%20selective%20breeding%2C%20an%20approach%20which%20has%20so%20far%0Abeen%20underexplored%20and%20underutilised%20in%20insect%20farming.%20Here%20we%20present%20a%0Acomprehensive%20review%20of%20the%20selective%20breeding%20framework%20in%20the%20context%20of%0Ainsect%20production.%20We%20systematically%20evaluate%20adjustments%20of%20selective%20breeding%0Atechniques%20to%20the%20realm%20of%20insects%20and%20highlight%20the%20essential%20components%0Aintegral%20to%20the%20breeding%20process.%20The%20discussion%20covers%20every%20step%20of%20a%0Aconventional%20breeding%20scheme%2C%20such%20as%20formulation%20of%20breeding%20objectives%2C%0Aphenotyping%2C%20estimation%20of%20genetic%20parameters%20and%20breeding%20values%2C%20selection%20of%0Aappropriate%20breeding%20strategies%2C%20and%20mitigation%20of%20issues%20associated%20with%0Agenetic%20diversity%20depletion%20and%20inbreeding.%20This%20review%20combines%20knowledge%20from%0Adiverse%20disciplines%2C%20bridging%20the%20gap%20between%20animal%20breeding%2C%20quantitative%0Agenetics%2C%20evolutionary%20biology%2C%20and%20entomology%2C%20offering%20an%20integrated%20view%20of%0Athe%20insect%20breeding%20research%20area%20and%20uniting%20knowledge%20which%20has%20previously%0Aremained%20scattered%20across%20diverse%20fields%20of%20expertise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18144v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArtificial%2520Immune%2520System%2520of%2520Secure%2520Face%2520Recognition%2520Against%2520Adversarial%250A%2520%2520Attacks%26entry.906535625%3DMin%2520Ren%2520and%2520Yunlong%2520Wang%2520and%2520Yuhao%2520Zhu%2520and%2520Yongzhen%2520Huang%2520and%2520Zhenan%2520Sun%2520and%2520Qi%2520Li%2520and%2520Tieniu%2520Tan%26entry.1292438233%3D%2520%2520Insect%2520production%2520for%2520food%2520and%2520feed%2520presents%2520a%2520promising%2520supplement%2520to%2520ensure%250Afood%2520safety%2520and%2520address%2520the%2520adverse%2520impacts%2520of%2520agriculture%2520on%2520climate%2520and%250Aenvironment%2520in%2520the%2520future.%2520However%252C%2520optimisation%2520is%2520required%2520for%2520insect%250Aproduction%2520to%2520realise%2520its%2520full%2520potential.%2520This%2520can%2520be%2520by%2520targeted%2520improvement%250Aof%2520traits%2520of%2520interest%2520through%2520selective%2520breeding%252C%2520an%2520approach%2520which%2520has%2520so%2520far%250Abeen%2520underexplored%2520and%2520underutilised%2520in%2520insect%2520farming.%2520Here%2520we%2520present%2520a%250Acomprehensive%2520review%2520of%2520the%2520selective%2520breeding%2520framework%2520in%2520the%2520context%2520of%250Ainsect%2520production.%2520We%2520systematically%2520evaluate%2520adjustments%2520of%2520selective%2520breeding%250Atechniques%2520to%2520the%2520realm%2520of%2520insects%2520and%2520highlight%2520the%2520essential%2520components%250Aintegral%2520to%2520the%2520breeding%2520process.%2520The%2520discussion%2520covers%2520every%2520step%2520of%2520a%250Aconventional%2520breeding%2520scheme%252C%2520such%2520as%2520formulation%2520of%2520breeding%2520objectives%252C%250Aphenotyping%252C%2520estimation%2520of%2520genetic%2520parameters%2520and%2520breeding%2520values%252C%2520selection%2520of%250Aappropriate%2520breeding%2520strategies%252C%2520and%2520mitigation%2520of%2520issues%2520associated%2520with%250Agenetic%2520diversity%2520depletion%2520and%2520inbreeding.%2520This%2520review%2520combines%2520knowledge%2520from%250Adiverse%2520disciplines%252C%2520bridging%2520the%2520gap%2520between%2520animal%2520breeding%252C%2520quantitative%250Agenetics%252C%2520evolutionary%2520biology%252C%2520and%2520entomology%252C%2520offering%2520an%2520integrated%2520view%2520of%250Athe%2520insect%2520breeding%2520research%2520area%2520and%2520uniting%2520knowledge%2520which%2520has%2520previously%250Aremained%2520scattered%2520across%2520diverse%2520fields%2520of%2520expertise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18144v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Artificial%20Immune%20System%20of%20Secure%20Face%20Recognition%20Against%20Adversarial%0A%20%20Attacks&entry.906535625=Min%20Ren%20and%20Yunlong%20Wang%20and%20Yuhao%20Zhu%20and%20Yongzhen%20Huang%20and%20Zhenan%20Sun%20and%20Qi%20Li%20and%20Tieniu%20Tan&entry.1292438233=%20%20Insect%20production%20for%20food%20and%20feed%20presents%20a%20promising%20supplement%20to%20ensure%0Afood%20safety%20and%20address%20the%20adverse%20impacts%20of%20agriculture%20on%20climate%20and%0Aenvironment%20in%20the%20future.%20However%2C%20optimisation%20is%20required%20for%20insect%0Aproduction%20to%20realise%20its%20full%20potential.%20This%20can%20be%20by%20targeted%20improvement%0Aof%20traits%20of%20interest%20through%20selective%20breeding%2C%20an%20approach%20which%20has%20so%20far%0Abeen%20underexplored%20and%20underutilised%20in%20insect%20farming.%20Here%20we%20present%20a%0Acomprehensive%20review%20of%20the%20selective%20breeding%20framework%20in%20the%20context%20of%0Ainsect%20production.%20We%20systematically%20evaluate%20adjustments%20of%20selective%20breeding%0Atechniques%20to%20the%20realm%20of%20insects%20and%20highlight%20the%20essential%20components%0Aintegral%20to%20the%20breeding%20process.%20The%20discussion%20covers%20every%20step%20of%20a%0Aconventional%20breeding%20scheme%2C%20such%20as%20formulation%20of%20breeding%20objectives%2C%0Aphenotyping%2C%20estimation%20of%20genetic%20parameters%20and%20breeding%20values%2C%20selection%20of%0Aappropriate%20breeding%20strategies%2C%20and%20mitigation%20of%20issues%20associated%20with%0Agenetic%20diversity%20depletion%20and%20inbreeding.%20This%20review%20combines%20knowledge%20from%0Adiverse%20disciplines%2C%20bridging%20the%20gap%20between%20animal%20breeding%2C%20quantitative%0Agenetics%2C%20evolutionary%20biology%2C%20and%20entomology%2C%20offering%20an%20integrated%20view%20of%0Athe%20insect%20breeding%20research%20area%20and%20uniting%20knowledge%20which%20has%20previously%0Aremained%20scattered%20across%20diverse%20fields%20of%20expertise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18144v1&entry.124074799=Read"},
{"title": "ChronoMagic-Bench: A Benchmark for Metamorphic Evaluation of\n  Text-to-Time-lapse Video Generation", "author": "Shenghai Yuan and Jinfa Huang and Yongqi Xu and Yaoyang Liu and Shaofeng Zhang and Yujun Shi and Ruijie Zhu and Xinhua Cheng and Jiebo Luo and Li Yuan", "abstract": "  We propose a novel text-to-video (T2V) generation benchmark,\nChronoMagic-Bench, to evaluate the temporal and metamorphic capabilities of the\nT2V models (e.g. Sora and Lumiere) in time-lapse video generation. In contrast\nto existing benchmarks that focus on the visual quality and textual relevance\nof generated videos, ChronoMagic-Bench focuses on the model's ability to\ngenerate time-lapse videos with significant metamorphic amplitude and temporal\ncoherence. The benchmark probes T2V models for their physics, biology, and\nchemistry capabilities, in a free-form text query. For these purposes,\nChronoMagic-Bench introduces 1,649 prompts and real-world videos as references,\ncategorized into four major types of time-lapse videos: biological,\nhuman-created, meteorological, and physical phenomena, which are further\ndivided into 75 subcategories. This categorization comprehensively evaluates\nthe model's capacity to handle diverse and complex transformations. To\naccurately align human preference with the benchmark, we introduce two new\nautomatic metrics, MTScore and CHScore, to evaluate the videos' metamorphic\nattributes and temporal coherence. MTScore measures the metamorphic amplitude,\nreflecting the degree of change over time, while CHScore assesses the temporal\ncoherence, ensuring the generated videos maintain logical progression and\ncontinuity. Based on the ChronoMagic-Bench, we conduct comprehensive manual\nevaluations of ten representative T2V models, revealing their strengths and\nweaknesses across different categories of prompts, and providing a thorough\nevaluation framework that addresses current gaps in video generation research.\nMoreover, we create a large-scale ChronoMagic-Pro dataset, containing 460k\nhigh-quality pairs of 720p time-lapse videos and detailed captions ensuring\nhigh physical pertinence and large metamorphic amplitude.\n", "link": "http://arxiv.org/abs/2406.18522v1", "date": "2024-06-26", "relevancy": 2.1653, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5669}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5546}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5105}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChronoMagic-Bench%3A%20A%20Benchmark%20for%20Metamorphic%20Evaluation%20of%0A%20%20Text-to-Time-lapse%20Video%20Generation&body=Title%3A%20ChronoMagic-Bench%3A%20A%20Benchmark%20for%20Metamorphic%20Evaluation%20of%0A%20%20Text-to-Time-lapse%20Video%20Generation%0AAuthor%3A%20Shenghai%20Yuan%20and%20Jinfa%20Huang%20and%20Yongqi%20Xu%20and%20Yaoyang%20Liu%20and%20Shaofeng%20Zhang%20and%20Yujun%20Shi%20and%20Ruijie%20Zhu%20and%20Xinhua%20Cheng%20and%20Jiebo%20Luo%20and%20Li%20Yuan%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20text-to-video%20%28T2V%29%20generation%20benchmark%2C%0AChronoMagic-Bench%2C%20to%20evaluate%20the%20temporal%20and%20metamorphic%20capabilities%20of%20the%0AT2V%20models%20%28e.g.%20Sora%20and%20Lumiere%29%20in%20time-lapse%20video%20generation.%20In%20contrast%0Ato%20existing%20benchmarks%20that%20focus%20on%20the%20visual%20quality%20and%20textual%20relevance%0Aof%20generated%20videos%2C%20ChronoMagic-Bench%20focuses%20on%20the%20model%27s%20ability%20to%0Agenerate%20time-lapse%20videos%20with%20significant%20metamorphic%20amplitude%20and%20temporal%0Acoherence.%20The%20benchmark%20probes%20T2V%20models%20for%20their%20physics%2C%20biology%2C%20and%0Achemistry%20capabilities%2C%20in%20a%20free-form%20text%20query.%20For%20these%20purposes%2C%0AChronoMagic-Bench%20introduces%201%2C649%20prompts%20and%20real-world%20videos%20as%20references%2C%0Acategorized%20into%20four%20major%20types%20of%20time-lapse%20videos%3A%20biological%2C%0Ahuman-created%2C%20meteorological%2C%20and%20physical%20phenomena%2C%20which%20are%20further%0Adivided%20into%2075%20subcategories.%20This%20categorization%20comprehensively%20evaluates%0Athe%20model%27s%20capacity%20to%20handle%20diverse%20and%20complex%20transformations.%20To%0Aaccurately%20align%20human%20preference%20with%20the%20benchmark%2C%20we%20introduce%20two%20new%0Aautomatic%20metrics%2C%20MTScore%20and%20CHScore%2C%20to%20evaluate%20the%20videos%27%20metamorphic%0Aattributes%20and%20temporal%20coherence.%20MTScore%20measures%20the%20metamorphic%20amplitude%2C%0Areflecting%20the%20degree%20of%20change%20over%20time%2C%20while%20CHScore%20assesses%20the%20temporal%0Acoherence%2C%20ensuring%20the%20generated%20videos%20maintain%20logical%20progression%20and%0Acontinuity.%20Based%20on%20the%20ChronoMagic-Bench%2C%20we%20conduct%20comprehensive%20manual%0Aevaluations%20of%20ten%20representative%20T2V%20models%2C%20revealing%20their%20strengths%20and%0Aweaknesses%20across%20different%20categories%20of%20prompts%2C%20and%20providing%20a%20thorough%0Aevaluation%20framework%20that%20addresses%20current%20gaps%20in%20video%20generation%20research.%0AMoreover%2C%20we%20create%20a%20large-scale%20ChronoMagic-Pro%20dataset%2C%20containing%20460k%0Ahigh-quality%20pairs%20of%20720p%20time-lapse%20videos%20and%20detailed%20captions%20ensuring%0Ahigh%20physical%20pertinence%20and%20large%20metamorphic%20amplitude.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18522v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChronoMagic-Bench%253A%2520A%2520Benchmark%2520for%2520Metamorphic%2520Evaluation%2520of%250A%2520%2520Text-to-Time-lapse%2520Video%2520Generation%26entry.906535625%3DShenghai%2520Yuan%2520and%2520Jinfa%2520Huang%2520and%2520Yongqi%2520Xu%2520and%2520Yaoyang%2520Liu%2520and%2520Shaofeng%2520Zhang%2520and%2520Yujun%2520Shi%2520and%2520Ruijie%2520Zhu%2520and%2520Xinhua%2520Cheng%2520and%2520Jiebo%2520Luo%2520and%2520Li%2520Yuan%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520text-to-video%2520%2528T2V%2529%2520generation%2520benchmark%252C%250AChronoMagic-Bench%252C%2520to%2520evaluate%2520the%2520temporal%2520and%2520metamorphic%2520capabilities%2520of%2520the%250AT2V%2520models%2520%2528e.g.%2520Sora%2520and%2520Lumiere%2529%2520in%2520time-lapse%2520video%2520generation.%2520In%2520contrast%250Ato%2520existing%2520benchmarks%2520that%2520focus%2520on%2520the%2520visual%2520quality%2520and%2520textual%2520relevance%250Aof%2520generated%2520videos%252C%2520ChronoMagic-Bench%2520focuses%2520on%2520the%2520model%2527s%2520ability%2520to%250Agenerate%2520time-lapse%2520videos%2520with%2520significant%2520metamorphic%2520amplitude%2520and%2520temporal%250Acoherence.%2520The%2520benchmark%2520probes%2520T2V%2520models%2520for%2520their%2520physics%252C%2520biology%252C%2520and%250Achemistry%2520capabilities%252C%2520in%2520a%2520free-form%2520text%2520query.%2520For%2520these%2520purposes%252C%250AChronoMagic-Bench%2520introduces%25201%252C649%2520prompts%2520and%2520real-world%2520videos%2520as%2520references%252C%250Acategorized%2520into%2520four%2520major%2520types%2520of%2520time-lapse%2520videos%253A%2520biological%252C%250Ahuman-created%252C%2520meteorological%252C%2520and%2520physical%2520phenomena%252C%2520which%2520are%2520further%250Adivided%2520into%252075%2520subcategories.%2520This%2520categorization%2520comprehensively%2520evaluates%250Athe%2520model%2527s%2520capacity%2520to%2520handle%2520diverse%2520and%2520complex%2520transformations.%2520To%250Aaccurately%2520align%2520human%2520preference%2520with%2520the%2520benchmark%252C%2520we%2520introduce%2520two%2520new%250Aautomatic%2520metrics%252C%2520MTScore%2520and%2520CHScore%252C%2520to%2520evaluate%2520the%2520videos%2527%2520metamorphic%250Aattributes%2520and%2520temporal%2520coherence.%2520MTScore%2520measures%2520the%2520metamorphic%2520amplitude%252C%250Areflecting%2520the%2520degree%2520of%2520change%2520over%2520time%252C%2520while%2520CHScore%2520assesses%2520the%2520temporal%250Acoherence%252C%2520ensuring%2520the%2520generated%2520videos%2520maintain%2520logical%2520progression%2520and%250Acontinuity.%2520Based%2520on%2520the%2520ChronoMagic-Bench%252C%2520we%2520conduct%2520comprehensive%2520manual%250Aevaluations%2520of%2520ten%2520representative%2520T2V%2520models%252C%2520revealing%2520their%2520strengths%2520and%250Aweaknesses%2520across%2520different%2520categories%2520of%2520prompts%252C%2520and%2520providing%2520a%2520thorough%250Aevaluation%2520framework%2520that%2520addresses%2520current%2520gaps%2520in%2520video%2520generation%2520research.%250AMoreover%252C%2520we%2520create%2520a%2520large-scale%2520ChronoMagic-Pro%2520dataset%252C%2520containing%2520460k%250Ahigh-quality%2520pairs%2520of%2520720p%2520time-lapse%2520videos%2520and%2520detailed%2520captions%2520ensuring%250Ahigh%2520physical%2520pertinence%2520and%2520large%2520metamorphic%2520amplitude.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18522v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChronoMagic-Bench%3A%20A%20Benchmark%20for%20Metamorphic%20Evaluation%20of%0A%20%20Text-to-Time-lapse%20Video%20Generation&entry.906535625=Shenghai%20Yuan%20and%20Jinfa%20Huang%20and%20Yongqi%20Xu%20and%20Yaoyang%20Liu%20and%20Shaofeng%20Zhang%20and%20Yujun%20Shi%20and%20Ruijie%20Zhu%20and%20Xinhua%20Cheng%20and%20Jiebo%20Luo%20and%20Li%20Yuan&entry.1292438233=%20%20We%20propose%20a%20novel%20text-to-video%20%28T2V%29%20generation%20benchmark%2C%0AChronoMagic-Bench%2C%20to%20evaluate%20the%20temporal%20and%20metamorphic%20capabilities%20of%20the%0AT2V%20models%20%28e.g.%20Sora%20and%20Lumiere%29%20in%20time-lapse%20video%20generation.%20In%20contrast%0Ato%20existing%20benchmarks%20that%20focus%20on%20the%20visual%20quality%20and%20textual%20relevance%0Aof%20generated%20videos%2C%20ChronoMagic-Bench%20focuses%20on%20the%20model%27s%20ability%20to%0Agenerate%20time-lapse%20videos%20with%20significant%20metamorphic%20amplitude%20and%20temporal%0Acoherence.%20The%20benchmark%20probes%20T2V%20models%20for%20their%20physics%2C%20biology%2C%20and%0Achemistry%20capabilities%2C%20in%20a%20free-form%20text%20query.%20For%20these%20purposes%2C%0AChronoMagic-Bench%20introduces%201%2C649%20prompts%20and%20real-world%20videos%20as%20references%2C%0Acategorized%20into%20four%20major%20types%20of%20time-lapse%20videos%3A%20biological%2C%0Ahuman-created%2C%20meteorological%2C%20and%20physical%20phenomena%2C%20which%20are%20further%0Adivided%20into%2075%20subcategories.%20This%20categorization%20comprehensively%20evaluates%0Athe%20model%27s%20capacity%20to%20handle%20diverse%20and%20complex%20transformations.%20To%0Aaccurately%20align%20human%20preference%20with%20the%20benchmark%2C%20we%20introduce%20two%20new%0Aautomatic%20metrics%2C%20MTScore%20and%20CHScore%2C%20to%20evaluate%20the%20videos%27%20metamorphic%0Aattributes%20and%20temporal%20coherence.%20MTScore%20measures%20the%20metamorphic%20amplitude%2C%0Areflecting%20the%20degree%20of%20change%20over%20time%2C%20while%20CHScore%20assesses%20the%20temporal%0Acoherence%2C%20ensuring%20the%20generated%20videos%20maintain%20logical%20progression%20and%0Acontinuity.%20Based%20on%20the%20ChronoMagic-Bench%2C%20we%20conduct%20comprehensive%20manual%0Aevaluations%20of%20ten%20representative%20T2V%20models%2C%20revealing%20their%20strengths%20and%0Aweaknesses%20across%20different%20categories%20of%20prompts%2C%20and%20providing%20a%20thorough%0Aevaluation%20framework%20that%20addresses%20current%20gaps%20in%20video%20generation%20research.%0AMoreover%2C%20we%20create%20a%20large-scale%20ChronoMagic-Pro%20dataset%2C%20containing%20460k%0Ahigh-quality%20pairs%20of%20720p%20time-lapse%20videos%20and%20detailed%20captions%20ensuring%0Ahigh%20physical%20pertinence%20and%20large%20metamorphic%20amplitude.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18522v1&entry.124074799=Read"},
{"title": "Generative artificial intelligence in ophthalmology: multimodal retinal\n  images for the diagnosis of Alzheimer's disease with convolutional neural\n  networks", "author": "I. R. Slootweg and M. Thach and K. R. Curro-Tafili and F. D. Verbraak and F. H. Bouwman and Y. A. L. Pijnenburg and J. F. Boer and J. H. P. de Kwisthout and L. Bagheriye and P. J. Gonz\u00e1lez", "abstract": "  Background/Aim. This study aims to predict Amyloid Positron Emission\nTomography (AmyloidPET) status with multimodal retinal imaging and\nconvolutional neural networks (CNNs) and to improve the performance through\npretraining with synthetic data. Methods. Fundus autofluorescence, optical\ncoherence tomography (OCT), and OCT angiography images from 328 eyes of 59\nAmyloidPET positive subjects and 108 AmyloidPET negative subjects were used for\nclassification. Denoising Diffusion Probabilistic Models (DDPMs) were trained\nto generate synthetic images and unimodal CNNs were pretrained on synthetic\ndata and finetuned on real data or trained solely on real data. Multimodal\nclassifiers were developed to combine predictions of the four unimodal CNNs\nwith patient metadata. Class activation maps of the unimodal classifiers\nprovided insight into the network's attention to inputs. Results. DDPMs\ngenerated diverse, realistic images without memorization. Pretraining unimodal\nCNNs with synthetic data improved AUPR at most from 0.350 to 0.579. Integration\nof metadata in multimodal CNNs improved AUPR from 0.486 to 0.634, which was the\nbest overall best classifier. Class activation maps highlighted relevant\nretinal regions which correlated with AD. Conclusion. Our method for generating\nand leveraging synthetic data has the potential to improve AmyloidPET\nprediction from multimodal retinal imaging. A DDPM can generate realistic and\nunique multimodal synthetic retinal images. Our best performing unimodal and\nmultimodal classifiers were not pretrained on synthetic data, however\npretraining with synthetic data slightly improved classification performance\nfor two out of the four modalities.\n", "link": "http://arxiv.org/abs/2406.18247v1", "date": "2024-06-26", "relevancy": 2.1592, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5535}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5329}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5288}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20artificial%20intelligence%20in%20ophthalmology%3A%20multimodal%20retinal%0A%20%20images%20for%20the%20diagnosis%20of%20Alzheimer%27s%20disease%20with%20convolutional%20neural%0A%20%20networks&body=Title%3A%20Generative%20artificial%20intelligence%20in%20ophthalmology%3A%20multimodal%20retinal%0A%20%20images%20for%20the%20diagnosis%20of%20Alzheimer%27s%20disease%20with%20convolutional%20neural%0A%20%20networks%0AAuthor%3A%20I.%20R.%20Slootweg%20and%20M.%20Thach%20and%20K.%20R.%20Curro-Tafili%20and%20F.%20D.%20Verbraak%20and%20F.%20H.%20Bouwman%20and%20Y.%20A.%20L.%20Pijnenburg%20and%20J.%20F.%20Boer%20and%20J.%20H.%20P.%20de%20Kwisthout%20and%20L.%20Bagheriye%20and%20P.%20J.%20Gonz%C3%A1lez%0AAbstract%3A%20%20%20Background/Aim.%20This%20study%20aims%20to%20predict%20Amyloid%20Positron%20Emission%0ATomography%20%28AmyloidPET%29%20status%20with%20multimodal%20retinal%20imaging%20and%0Aconvolutional%20neural%20networks%20%28CNNs%29%20and%20to%20improve%20the%20performance%20through%0Apretraining%20with%20synthetic%20data.%20Methods.%20Fundus%20autofluorescence%2C%20optical%0Acoherence%20tomography%20%28OCT%29%2C%20and%20OCT%20angiography%20images%20from%20328%20eyes%20of%2059%0AAmyloidPET%20positive%20subjects%20and%20108%20AmyloidPET%20negative%20subjects%20were%20used%20for%0Aclassification.%20Denoising%20Diffusion%20Probabilistic%20Models%20%28DDPMs%29%20were%20trained%0Ato%20generate%20synthetic%20images%20and%20unimodal%20CNNs%20were%20pretrained%20on%20synthetic%0Adata%20and%20finetuned%20on%20real%20data%20or%20trained%20solely%20on%20real%20data.%20Multimodal%0Aclassifiers%20were%20developed%20to%20combine%20predictions%20of%20the%20four%20unimodal%20CNNs%0Awith%20patient%20metadata.%20Class%20activation%20maps%20of%20the%20unimodal%20classifiers%0Aprovided%20insight%20into%20the%20network%27s%20attention%20to%20inputs.%20Results.%20DDPMs%0Agenerated%20diverse%2C%20realistic%20images%20without%20memorization.%20Pretraining%20unimodal%0ACNNs%20with%20synthetic%20data%20improved%20AUPR%20at%20most%20from%200.350%20to%200.579.%20Integration%0Aof%20metadata%20in%20multimodal%20CNNs%20improved%20AUPR%20from%200.486%20to%200.634%2C%20which%20was%20the%0Abest%20overall%20best%20classifier.%20Class%20activation%20maps%20highlighted%20relevant%0Aretinal%20regions%20which%20correlated%20with%20AD.%20Conclusion.%20Our%20method%20for%20generating%0Aand%20leveraging%20synthetic%20data%20has%20the%20potential%20to%20improve%20AmyloidPET%0Aprediction%20from%20multimodal%20retinal%20imaging.%20A%20DDPM%20can%20generate%20realistic%20and%0Aunique%20multimodal%20synthetic%20retinal%20images.%20Our%20best%20performing%20unimodal%20and%0Amultimodal%20classifiers%20were%20not%20pretrained%20on%20synthetic%20data%2C%20however%0Apretraining%20with%20synthetic%20data%20slightly%20improved%20classification%20performance%0Afor%20two%20out%20of%20the%20four%20modalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18247v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520artificial%2520intelligence%2520in%2520ophthalmology%253A%2520multimodal%2520retinal%250A%2520%2520images%2520for%2520the%2520diagnosis%2520of%2520Alzheimer%2527s%2520disease%2520with%2520convolutional%2520neural%250A%2520%2520networks%26entry.906535625%3DI.%2520R.%2520Slootweg%2520and%2520M.%2520Thach%2520and%2520K.%2520R.%2520Curro-Tafili%2520and%2520F.%2520D.%2520Verbraak%2520and%2520F.%2520H.%2520Bouwman%2520and%2520Y.%2520A.%2520L.%2520Pijnenburg%2520and%2520J.%2520F.%2520Boer%2520and%2520J.%2520H.%2520P.%2520de%2520Kwisthout%2520and%2520L.%2520Bagheriye%2520and%2520P.%2520J.%2520Gonz%25C3%25A1lez%26entry.1292438233%3D%2520%2520Background/Aim.%2520This%2520study%2520aims%2520to%2520predict%2520Amyloid%2520Positron%2520Emission%250ATomography%2520%2528AmyloidPET%2529%2520status%2520with%2520multimodal%2520retinal%2520imaging%2520and%250Aconvolutional%2520neural%2520networks%2520%2528CNNs%2529%2520and%2520to%2520improve%2520the%2520performance%2520through%250Apretraining%2520with%2520synthetic%2520data.%2520Methods.%2520Fundus%2520autofluorescence%252C%2520optical%250Acoherence%2520tomography%2520%2528OCT%2529%252C%2520and%2520OCT%2520angiography%2520images%2520from%2520328%2520eyes%2520of%252059%250AAmyloidPET%2520positive%2520subjects%2520and%2520108%2520AmyloidPET%2520negative%2520subjects%2520were%2520used%2520for%250Aclassification.%2520Denoising%2520Diffusion%2520Probabilistic%2520Models%2520%2528DDPMs%2529%2520were%2520trained%250Ato%2520generate%2520synthetic%2520images%2520and%2520unimodal%2520CNNs%2520were%2520pretrained%2520on%2520synthetic%250Adata%2520and%2520finetuned%2520on%2520real%2520data%2520or%2520trained%2520solely%2520on%2520real%2520data.%2520Multimodal%250Aclassifiers%2520were%2520developed%2520to%2520combine%2520predictions%2520of%2520the%2520four%2520unimodal%2520CNNs%250Awith%2520patient%2520metadata.%2520Class%2520activation%2520maps%2520of%2520the%2520unimodal%2520classifiers%250Aprovided%2520insight%2520into%2520the%2520network%2527s%2520attention%2520to%2520inputs.%2520Results.%2520DDPMs%250Agenerated%2520diverse%252C%2520realistic%2520images%2520without%2520memorization.%2520Pretraining%2520unimodal%250ACNNs%2520with%2520synthetic%2520data%2520improved%2520AUPR%2520at%2520most%2520from%25200.350%2520to%25200.579.%2520Integration%250Aof%2520metadata%2520in%2520multimodal%2520CNNs%2520improved%2520AUPR%2520from%25200.486%2520to%25200.634%252C%2520which%2520was%2520the%250Abest%2520overall%2520best%2520classifier.%2520Class%2520activation%2520maps%2520highlighted%2520relevant%250Aretinal%2520regions%2520which%2520correlated%2520with%2520AD.%2520Conclusion.%2520Our%2520method%2520for%2520generating%250Aand%2520leveraging%2520synthetic%2520data%2520has%2520the%2520potential%2520to%2520improve%2520AmyloidPET%250Aprediction%2520from%2520multimodal%2520retinal%2520imaging.%2520A%2520DDPM%2520can%2520generate%2520realistic%2520and%250Aunique%2520multimodal%2520synthetic%2520retinal%2520images.%2520Our%2520best%2520performing%2520unimodal%2520and%250Amultimodal%2520classifiers%2520were%2520not%2520pretrained%2520on%2520synthetic%2520data%252C%2520however%250Apretraining%2520with%2520synthetic%2520data%2520slightly%2520improved%2520classification%2520performance%250Afor%2520two%2520out%2520of%2520the%2520four%2520modalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18247v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20artificial%20intelligence%20in%20ophthalmology%3A%20multimodal%20retinal%0A%20%20images%20for%20the%20diagnosis%20of%20Alzheimer%27s%20disease%20with%20convolutional%20neural%0A%20%20networks&entry.906535625=I.%20R.%20Slootweg%20and%20M.%20Thach%20and%20K.%20R.%20Curro-Tafili%20and%20F.%20D.%20Verbraak%20and%20F.%20H.%20Bouwman%20and%20Y.%20A.%20L.%20Pijnenburg%20and%20J.%20F.%20Boer%20and%20J.%20H.%20P.%20de%20Kwisthout%20and%20L.%20Bagheriye%20and%20P.%20J.%20Gonz%C3%A1lez&entry.1292438233=%20%20Background/Aim.%20This%20study%20aims%20to%20predict%20Amyloid%20Positron%20Emission%0ATomography%20%28AmyloidPET%29%20status%20with%20multimodal%20retinal%20imaging%20and%0Aconvolutional%20neural%20networks%20%28CNNs%29%20and%20to%20improve%20the%20performance%20through%0Apretraining%20with%20synthetic%20data.%20Methods.%20Fundus%20autofluorescence%2C%20optical%0Acoherence%20tomography%20%28OCT%29%2C%20and%20OCT%20angiography%20images%20from%20328%20eyes%20of%2059%0AAmyloidPET%20positive%20subjects%20and%20108%20AmyloidPET%20negative%20subjects%20were%20used%20for%0Aclassification.%20Denoising%20Diffusion%20Probabilistic%20Models%20%28DDPMs%29%20were%20trained%0Ato%20generate%20synthetic%20images%20and%20unimodal%20CNNs%20were%20pretrained%20on%20synthetic%0Adata%20and%20finetuned%20on%20real%20data%20or%20trained%20solely%20on%20real%20data.%20Multimodal%0Aclassifiers%20were%20developed%20to%20combine%20predictions%20of%20the%20four%20unimodal%20CNNs%0Awith%20patient%20metadata.%20Class%20activation%20maps%20of%20the%20unimodal%20classifiers%0Aprovided%20insight%20into%20the%20network%27s%20attention%20to%20inputs.%20Results.%20DDPMs%0Agenerated%20diverse%2C%20realistic%20images%20without%20memorization.%20Pretraining%20unimodal%0ACNNs%20with%20synthetic%20data%20improved%20AUPR%20at%20most%20from%200.350%20to%200.579.%20Integration%0Aof%20metadata%20in%20multimodal%20CNNs%20improved%20AUPR%20from%200.486%20to%200.634%2C%20which%20was%20the%0Abest%20overall%20best%20classifier.%20Class%20activation%20maps%20highlighted%20relevant%0Aretinal%20regions%20which%20correlated%20with%20AD.%20Conclusion.%20Our%20method%20for%20generating%0Aand%20leveraging%20synthetic%20data%20has%20the%20potential%20to%20improve%20AmyloidPET%0Aprediction%20from%20multimodal%20retinal%20imaging.%20A%20DDPM%20can%20generate%20realistic%20and%0Aunique%20multimodal%20synthetic%20retinal%20images.%20Our%20best%20performing%20unimodal%20and%0Amultimodal%20classifiers%20were%20not%20pretrained%20on%20synthetic%20data%2C%20however%0Apretraining%20with%20synthetic%20data%20slightly%20improved%20classification%20performance%0Afor%20two%20out%20of%20the%20four%20modalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18247v1&entry.124074799=Read"},
{"title": "Few-Shot Medical Image Segmentation with High-Fidelity Prototypes", "author": "Song Tang and Shaxu Yan and Xiaozhi Qi and Jianxin Gao and Mao Ye and Jianwei Zhang and Xiatian Zhu", "abstract": "  Few-shot Semantic Segmentation (FSS) aims to adapt a pretrained model to new\nclasses with as few as a single labelled training sample per class. Despite the\nprototype based approaches have achieved substantial success, existing models\nare limited to the imaging scenarios with considerably distinct objects and not\nhighly complex background, e.g., natural images. This makes such models\nsuboptimal for medical imaging with both conditions invalid. To address this\nproblem, we propose a novel Detail Self-refined Prototype Network (DSPNet) to\nconstructing high-fidelity prototypes representing the object foreground and\nthe background more comprehensively. Specifically, to construct global\nsemantics while maintaining the captured detail semantics, we learn the\nforeground prototypes by modelling the multi-modal structures with clustering\nand then fusing each in a channel-wise manner. Considering that the background\noften has no apparent semantic relation in the spatial dimensions, we integrate\nchannel-specific structural information under sparse channel-aware regulation.\nExtensive experiments on three challenging medical image benchmarks show the\nsuperiority of DSPNet over previous state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2406.18074v1", "date": "2024-06-26", "relevancy": 2.1557, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5674}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5544}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Few-Shot%20Medical%20Image%20Segmentation%20with%20High-Fidelity%20Prototypes&body=Title%3A%20Few-Shot%20Medical%20Image%20Segmentation%20with%20High-Fidelity%20Prototypes%0AAuthor%3A%20Song%20Tang%20and%20Shaxu%20Yan%20and%20Xiaozhi%20Qi%20and%20Jianxin%20Gao%20and%20Mao%20Ye%20and%20Jianwei%20Zhang%20and%20Xiatian%20Zhu%0AAbstract%3A%20%20%20Few-shot%20Semantic%20Segmentation%20%28FSS%29%20aims%20to%20adapt%20a%20pretrained%20model%20to%20new%0Aclasses%20with%20as%20few%20as%20a%20single%20labelled%20training%20sample%20per%20class.%20Despite%20the%0Aprototype%20based%20approaches%20have%20achieved%20substantial%20success%2C%20existing%20models%0Aare%20limited%20to%20the%20imaging%20scenarios%20with%20considerably%20distinct%20objects%20and%20not%0Ahighly%20complex%20background%2C%20e.g.%2C%20natural%20images.%20This%20makes%20such%20models%0Asuboptimal%20for%20medical%20imaging%20with%20both%20conditions%20invalid.%20To%20address%20this%0Aproblem%2C%20we%20propose%20a%20novel%20Detail%20Self-refined%20Prototype%20Network%20%28DSPNet%29%20to%0Aconstructing%20high-fidelity%20prototypes%20representing%20the%20object%20foreground%20and%0Athe%20background%20more%20comprehensively.%20Specifically%2C%20to%20construct%20global%0Asemantics%20while%20maintaining%20the%20captured%20detail%20semantics%2C%20we%20learn%20the%0Aforeground%20prototypes%20by%20modelling%20the%20multi-modal%20structures%20with%20clustering%0Aand%20then%20fusing%20each%20in%20a%20channel-wise%20manner.%20Considering%20that%20the%20background%0Aoften%20has%20no%20apparent%20semantic%20relation%20in%20the%20spatial%20dimensions%2C%20we%20integrate%0Achannel-specific%20structural%20information%20under%20sparse%20channel-aware%20regulation.%0AExtensive%20experiments%20on%20three%20challenging%20medical%20image%20benchmarks%20show%20the%0Asuperiority%20of%20DSPNet%20over%20previous%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18074v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFew-Shot%2520Medical%2520Image%2520Segmentation%2520with%2520High-Fidelity%2520Prototypes%26entry.906535625%3DSong%2520Tang%2520and%2520Shaxu%2520Yan%2520and%2520Xiaozhi%2520Qi%2520and%2520Jianxin%2520Gao%2520and%2520Mao%2520Ye%2520and%2520Jianwei%2520Zhang%2520and%2520Xiatian%2520Zhu%26entry.1292438233%3D%2520%2520Few-shot%2520Semantic%2520Segmentation%2520%2528FSS%2529%2520aims%2520to%2520adapt%2520a%2520pretrained%2520model%2520to%2520new%250Aclasses%2520with%2520as%2520few%2520as%2520a%2520single%2520labelled%2520training%2520sample%2520per%2520class.%2520Despite%2520the%250Aprototype%2520based%2520approaches%2520have%2520achieved%2520substantial%2520success%252C%2520existing%2520models%250Aare%2520limited%2520to%2520the%2520imaging%2520scenarios%2520with%2520considerably%2520distinct%2520objects%2520and%2520not%250Ahighly%2520complex%2520background%252C%2520e.g.%252C%2520natural%2520images.%2520This%2520makes%2520such%2520models%250Asuboptimal%2520for%2520medical%2520imaging%2520with%2520both%2520conditions%2520invalid.%2520To%2520address%2520this%250Aproblem%252C%2520we%2520propose%2520a%2520novel%2520Detail%2520Self-refined%2520Prototype%2520Network%2520%2528DSPNet%2529%2520to%250Aconstructing%2520high-fidelity%2520prototypes%2520representing%2520the%2520object%2520foreground%2520and%250Athe%2520background%2520more%2520comprehensively.%2520Specifically%252C%2520to%2520construct%2520global%250Asemantics%2520while%2520maintaining%2520the%2520captured%2520detail%2520semantics%252C%2520we%2520learn%2520the%250Aforeground%2520prototypes%2520by%2520modelling%2520the%2520multi-modal%2520structures%2520with%2520clustering%250Aand%2520then%2520fusing%2520each%2520in%2520a%2520channel-wise%2520manner.%2520Considering%2520that%2520the%2520background%250Aoften%2520has%2520no%2520apparent%2520semantic%2520relation%2520in%2520the%2520spatial%2520dimensions%252C%2520we%2520integrate%250Achannel-specific%2520structural%2520information%2520under%2520sparse%2520channel-aware%2520regulation.%250AExtensive%2520experiments%2520on%2520three%2520challenging%2520medical%2520image%2520benchmarks%2520show%2520the%250Asuperiority%2520of%2520DSPNet%2520over%2520previous%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18074v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Few-Shot%20Medical%20Image%20Segmentation%20with%20High-Fidelity%20Prototypes&entry.906535625=Song%20Tang%20and%20Shaxu%20Yan%20and%20Xiaozhi%20Qi%20and%20Jianxin%20Gao%20and%20Mao%20Ye%20and%20Jianwei%20Zhang%20and%20Xiatian%20Zhu&entry.1292438233=%20%20Few-shot%20Semantic%20Segmentation%20%28FSS%29%20aims%20to%20adapt%20a%20pretrained%20model%20to%20new%0Aclasses%20with%20as%20few%20as%20a%20single%20labelled%20training%20sample%20per%20class.%20Despite%20the%0Aprototype%20based%20approaches%20have%20achieved%20substantial%20success%2C%20existing%20models%0Aare%20limited%20to%20the%20imaging%20scenarios%20with%20considerably%20distinct%20objects%20and%20not%0Ahighly%20complex%20background%2C%20e.g.%2C%20natural%20images.%20This%20makes%20such%20models%0Asuboptimal%20for%20medical%20imaging%20with%20both%20conditions%20invalid.%20To%20address%20this%0Aproblem%2C%20we%20propose%20a%20novel%20Detail%20Self-refined%20Prototype%20Network%20%28DSPNet%29%20to%0Aconstructing%20high-fidelity%20prototypes%20representing%20the%20object%20foreground%20and%0Athe%20background%20more%20comprehensively.%20Specifically%2C%20to%20construct%20global%0Asemantics%20while%20maintaining%20the%20captured%20detail%20semantics%2C%20we%20learn%20the%0Aforeground%20prototypes%20by%20modelling%20the%20multi-modal%20structures%20with%20clustering%0Aand%20then%20fusing%20each%20in%20a%20channel-wise%20manner.%20Considering%20that%20the%20background%0Aoften%20has%20no%20apparent%20semantic%20relation%20in%20the%20spatial%20dimensions%2C%20we%20integrate%0Achannel-specific%20structural%20information%20under%20sparse%20channel-aware%20regulation.%0AExtensive%20experiments%20on%20three%20challenging%20medical%20image%20benchmarks%20show%20the%0Asuperiority%20of%20DSPNet%20over%20previous%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18074v1&entry.124074799=Read"},
{"title": "Symbolic Learning Enables Self-Evolving Agents", "author": "Wangchunshu Zhou and Yixin Ou and Shengwei Ding and Long Li and Jialong Wu and Tiannan Wang and Jiamin Chen and Shuai Wang and Xiaohua Xu and Ningyu Zhang and Huajun Chen and Yuchen Eleanor Jiang", "abstract": "  The AI community has been exploring a pathway to artificial general\nintelligence (AGI) by developing \"language agents\", which are complex large\nlanguage models (LLMs) pipelines involving both prompting techniques and tool\nusage methods. While language agents have demonstrated impressive capabilities\nfor many real-world tasks, a fundamental limitation of current language agents\nresearch is that they are model-centric, or engineering-centric. That's to say,\nthe progress on prompts, tools, and pipelines of language agents requires\nsubstantial manual engineering efforts from human experts rather than\nautomatically learning from data. We believe the transition from model-centric,\nor engineering-centric, to data-centric, i.e., the ability of language agents\nto autonomously learn and evolve in environments, is the key for them to\npossibly achieve AGI.\n  In this work, we introduce agent symbolic learning, a systematic framework\nthat enables language agents to optimize themselves on their own in a\ndata-centric way using symbolic optimizers. Specifically, we consider agents as\nsymbolic networks where learnable weights are defined by prompts, tools, and\nthe way they are stacked together. Agent symbolic learning is designed to\noptimize the symbolic network within language agents by mimicking two\nfundamental algorithms in connectionist learning: back-propagation and gradient\ndescent. Instead of dealing with numeric weights, agent symbolic learning works\nwith natural language simulacrums of weights, loss, and gradients. We conduct\nproof-of-concept experiments on both standard benchmarks and complex real-world\ntasks and show that agent symbolic learning enables language agents to update\nthemselves after being created and deployed in the wild, resulting in\n\"self-evolving agents\".\n", "link": "http://arxiv.org/abs/2406.18532v1", "date": "2024-06-26", "relevancy": 2.1441, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5416}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5379}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Symbolic%20Learning%20Enables%20Self-Evolving%20Agents&body=Title%3A%20Symbolic%20Learning%20Enables%20Self-Evolving%20Agents%0AAuthor%3A%20Wangchunshu%20Zhou%20and%20Yixin%20Ou%20and%20Shengwei%20Ding%20and%20Long%20Li%20and%20Jialong%20Wu%20and%20Tiannan%20Wang%20and%20Jiamin%20Chen%20and%20Shuai%20Wang%20and%20Xiaohua%20Xu%20and%20Ningyu%20Zhang%20and%20Huajun%20Chen%20and%20Yuchen%20Eleanor%20Jiang%0AAbstract%3A%20%20%20The%20AI%20community%20has%20been%20exploring%20a%20pathway%20to%20artificial%20general%0Aintelligence%20%28AGI%29%20by%20developing%20%22language%20agents%22%2C%20which%20are%20complex%20large%0Alanguage%20models%20%28LLMs%29%20pipelines%20involving%20both%20prompting%20techniques%20and%20tool%0Ausage%20methods.%20While%20language%20agents%20have%20demonstrated%20impressive%20capabilities%0Afor%20many%20real-world%20tasks%2C%20a%20fundamental%20limitation%20of%20current%20language%20agents%0Aresearch%20is%20that%20they%20are%20model-centric%2C%20or%20engineering-centric.%20That%27s%20to%20say%2C%0Athe%20progress%20on%20prompts%2C%20tools%2C%20and%20pipelines%20of%20language%20agents%20requires%0Asubstantial%20manual%20engineering%20efforts%20from%20human%20experts%20rather%20than%0Aautomatically%20learning%20from%20data.%20We%20believe%20the%20transition%20from%20model-centric%2C%0Aor%20engineering-centric%2C%20to%20data-centric%2C%20i.e.%2C%20the%20ability%20of%20language%20agents%0Ato%20autonomously%20learn%20and%20evolve%20in%20environments%2C%20is%20the%20key%20for%20them%20to%0Apossibly%20achieve%20AGI.%0A%20%20In%20this%20work%2C%20we%20introduce%20agent%20symbolic%20learning%2C%20a%20systematic%20framework%0Athat%20enables%20language%20agents%20to%20optimize%20themselves%20on%20their%20own%20in%20a%0Adata-centric%20way%20using%20symbolic%20optimizers.%20Specifically%2C%20we%20consider%20agents%20as%0Asymbolic%20networks%20where%20learnable%20weights%20are%20defined%20by%20prompts%2C%20tools%2C%20and%0Athe%20way%20they%20are%20stacked%20together.%20Agent%20symbolic%20learning%20is%20designed%20to%0Aoptimize%20the%20symbolic%20network%20within%20language%20agents%20by%20mimicking%20two%0Afundamental%20algorithms%20in%20connectionist%20learning%3A%20back-propagation%20and%20gradient%0Adescent.%20Instead%20of%20dealing%20with%20numeric%20weights%2C%20agent%20symbolic%20learning%20works%0Awith%20natural%20language%20simulacrums%20of%20weights%2C%20loss%2C%20and%20gradients.%20We%20conduct%0Aproof-of-concept%20experiments%20on%20both%20standard%20benchmarks%20and%20complex%20real-world%0Atasks%20and%20show%20that%20agent%20symbolic%20learning%20enables%20language%20agents%20to%20update%0Athemselves%20after%20being%20created%20and%20deployed%20in%20the%20wild%2C%20resulting%20in%0A%22self-evolving%20agents%22.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18532v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSymbolic%2520Learning%2520Enables%2520Self-Evolving%2520Agents%26entry.906535625%3DWangchunshu%2520Zhou%2520and%2520Yixin%2520Ou%2520and%2520Shengwei%2520Ding%2520and%2520Long%2520Li%2520and%2520Jialong%2520Wu%2520and%2520Tiannan%2520Wang%2520and%2520Jiamin%2520Chen%2520and%2520Shuai%2520Wang%2520and%2520Xiaohua%2520Xu%2520and%2520Ningyu%2520Zhang%2520and%2520Huajun%2520Chen%2520and%2520Yuchen%2520Eleanor%2520Jiang%26entry.1292438233%3D%2520%2520The%2520AI%2520community%2520has%2520been%2520exploring%2520a%2520pathway%2520to%2520artificial%2520general%250Aintelligence%2520%2528AGI%2529%2520by%2520developing%2520%2522language%2520agents%2522%252C%2520which%2520are%2520complex%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520pipelines%2520involving%2520both%2520prompting%2520techniques%2520and%2520tool%250Ausage%2520methods.%2520While%2520language%2520agents%2520have%2520demonstrated%2520impressive%2520capabilities%250Afor%2520many%2520real-world%2520tasks%252C%2520a%2520fundamental%2520limitation%2520of%2520current%2520language%2520agents%250Aresearch%2520is%2520that%2520they%2520are%2520model-centric%252C%2520or%2520engineering-centric.%2520That%2527s%2520to%2520say%252C%250Athe%2520progress%2520on%2520prompts%252C%2520tools%252C%2520and%2520pipelines%2520of%2520language%2520agents%2520requires%250Asubstantial%2520manual%2520engineering%2520efforts%2520from%2520human%2520experts%2520rather%2520than%250Aautomatically%2520learning%2520from%2520data.%2520We%2520believe%2520the%2520transition%2520from%2520model-centric%252C%250Aor%2520engineering-centric%252C%2520to%2520data-centric%252C%2520i.e.%252C%2520the%2520ability%2520of%2520language%2520agents%250Ato%2520autonomously%2520learn%2520and%2520evolve%2520in%2520environments%252C%2520is%2520the%2520key%2520for%2520them%2520to%250Apossibly%2520achieve%2520AGI.%250A%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520agent%2520symbolic%2520learning%252C%2520a%2520systematic%2520framework%250Athat%2520enables%2520language%2520agents%2520to%2520optimize%2520themselves%2520on%2520their%2520own%2520in%2520a%250Adata-centric%2520way%2520using%2520symbolic%2520optimizers.%2520Specifically%252C%2520we%2520consider%2520agents%2520as%250Asymbolic%2520networks%2520where%2520learnable%2520weights%2520are%2520defined%2520by%2520prompts%252C%2520tools%252C%2520and%250Athe%2520way%2520they%2520are%2520stacked%2520together.%2520Agent%2520symbolic%2520learning%2520is%2520designed%2520to%250Aoptimize%2520the%2520symbolic%2520network%2520within%2520language%2520agents%2520by%2520mimicking%2520two%250Afundamental%2520algorithms%2520in%2520connectionist%2520learning%253A%2520back-propagation%2520and%2520gradient%250Adescent.%2520Instead%2520of%2520dealing%2520with%2520numeric%2520weights%252C%2520agent%2520symbolic%2520learning%2520works%250Awith%2520natural%2520language%2520simulacrums%2520of%2520weights%252C%2520loss%252C%2520and%2520gradients.%2520We%2520conduct%250Aproof-of-concept%2520experiments%2520on%2520both%2520standard%2520benchmarks%2520and%2520complex%2520real-world%250Atasks%2520and%2520show%2520that%2520agent%2520symbolic%2520learning%2520enables%2520language%2520agents%2520to%2520update%250Athemselves%2520after%2520being%2520created%2520and%2520deployed%2520in%2520the%2520wild%252C%2520resulting%2520in%250A%2522self-evolving%2520agents%2522.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18532v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Symbolic%20Learning%20Enables%20Self-Evolving%20Agents&entry.906535625=Wangchunshu%20Zhou%20and%20Yixin%20Ou%20and%20Shengwei%20Ding%20and%20Long%20Li%20and%20Jialong%20Wu%20and%20Tiannan%20Wang%20and%20Jiamin%20Chen%20and%20Shuai%20Wang%20and%20Xiaohua%20Xu%20and%20Ningyu%20Zhang%20and%20Huajun%20Chen%20and%20Yuchen%20Eleanor%20Jiang&entry.1292438233=%20%20The%20AI%20community%20has%20been%20exploring%20a%20pathway%20to%20artificial%20general%0Aintelligence%20%28AGI%29%20by%20developing%20%22language%20agents%22%2C%20which%20are%20complex%20large%0Alanguage%20models%20%28LLMs%29%20pipelines%20involving%20both%20prompting%20techniques%20and%20tool%0Ausage%20methods.%20While%20language%20agents%20have%20demonstrated%20impressive%20capabilities%0Afor%20many%20real-world%20tasks%2C%20a%20fundamental%20limitation%20of%20current%20language%20agents%0Aresearch%20is%20that%20they%20are%20model-centric%2C%20or%20engineering-centric.%20That%27s%20to%20say%2C%0Athe%20progress%20on%20prompts%2C%20tools%2C%20and%20pipelines%20of%20language%20agents%20requires%0Asubstantial%20manual%20engineering%20efforts%20from%20human%20experts%20rather%20than%0Aautomatically%20learning%20from%20data.%20We%20believe%20the%20transition%20from%20model-centric%2C%0Aor%20engineering-centric%2C%20to%20data-centric%2C%20i.e.%2C%20the%20ability%20of%20language%20agents%0Ato%20autonomously%20learn%20and%20evolve%20in%20environments%2C%20is%20the%20key%20for%20them%20to%0Apossibly%20achieve%20AGI.%0A%20%20In%20this%20work%2C%20we%20introduce%20agent%20symbolic%20learning%2C%20a%20systematic%20framework%0Athat%20enables%20language%20agents%20to%20optimize%20themselves%20on%20their%20own%20in%20a%0Adata-centric%20way%20using%20symbolic%20optimizers.%20Specifically%2C%20we%20consider%20agents%20as%0Asymbolic%20networks%20where%20learnable%20weights%20are%20defined%20by%20prompts%2C%20tools%2C%20and%0Athe%20way%20they%20are%20stacked%20together.%20Agent%20symbolic%20learning%20is%20designed%20to%0Aoptimize%20the%20symbolic%20network%20within%20language%20agents%20by%20mimicking%20two%0Afundamental%20algorithms%20in%20connectionist%20learning%3A%20back-propagation%20and%20gradient%0Adescent.%20Instead%20of%20dealing%20with%20numeric%20weights%2C%20agent%20symbolic%20learning%20works%0Awith%20natural%20language%20simulacrums%20of%20weights%2C%20loss%2C%20and%20gradients.%20We%20conduct%0Aproof-of-concept%20experiments%20on%20both%20standard%20benchmarks%20and%20complex%20real-world%0Atasks%20and%20show%20that%20agent%20symbolic%20learning%20enables%20language%20agents%20to%20update%0Athemselves%20after%20being%20created%20and%20deployed%20in%20the%20wild%2C%20resulting%20in%0A%22self-evolving%20agents%22.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18532v1&entry.124074799=Read"},
{"title": "A Multi-Stage Goal-Driven Network for Pedestrian Trajectory Prediction", "author": "Xiuen Wu and Tao Wang and Yuanzheng Cai and Lingyu Liang and George Papageorgiou", "abstract": "  Pedestrian trajectory prediction plays a pivotal role in ensuring the safety\nand efficiency of various applications, including autonomous vehicles and\ntraffic management systems. This paper proposes a novel method for pedestrian\ntrajectory prediction, called multi-stage goal-driven network (MGNet).\nDiverging from prior approaches relying on stepwise recursive prediction and\nthe singular forecasting of a long-term goal, MGNet directs trajectory\ngeneration by forecasting intermediate stage goals, thereby reducing prediction\nerrors. The network comprises three main components: a conditional variational\nautoencoder (CVAE), an attention module, and a multi-stage goal evaluator.\nTrajectories are encoded using conditional variational autoencoders to acquire\nknowledge about the approximate distribution of pedestrians' future\ntrajectories, and combined with an attention mechanism to capture the temporal\ndependency between trajectory sequences. The pivotal module is the multi-stage\ngoal evaluator, which utilizes the encoded feature vectors to predict\nintermediate goals, effectively minimizing cumulative errors in the recursive\ninference process. The effectiveness of MGNet is demonstrated through\ncomprehensive experiments on the JAAD and PIE datasets. Comparative evaluations\nagainst state-of-the-art algorithms reveal significant performance improvements\nachieved by our proposed method.\n", "link": "http://arxiv.org/abs/2406.18050v1", "date": "2024-06-26", "relevancy": 2.143, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5821}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5279}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5251}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multi-Stage%20Goal-Driven%20Network%20for%20Pedestrian%20Trajectory%20Prediction&body=Title%3A%20A%20Multi-Stage%20Goal-Driven%20Network%20for%20Pedestrian%20Trajectory%20Prediction%0AAuthor%3A%20Xiuen%20Wu%20and%20Tao%20Wang%20and%20Yuanzheng%20Cai%20and%20Lingyu%20Liang%20and%20George%20Papageorgiou%0AAbstract%3A%20%20%20Pedestrian%20trajectory%20prediction%20plays%20a%20pivotal%20role%20in%20ensuring%20the%20safety%0Aand%20efficiency%20of%20various%20applications%2C%20including%20autonomous%20vehicles%20and%0Atraffic%20management%20systems.%20This%20paper%20proposes%20a%20novel%20method%20for%20pedestrian%0Atrajectory%20prediction%2C%20called%20multi-stage%20goal-driven%20network%20%28MGNet%29.%0ADiverging%20from%20prior%20approaches%20relying%20on%20stepwise%20recursive%20prediction%20and%0Athe%20singular%20forecasting%20of%20a%20long-term%20goal%2C%20MGNet%20directs%20trajectory%0Ageneration%20by%20forecasting%20intermediate%20stage%20goals%2C%20thereby%20reducing%20prediction%0Aerrors.%20The%20network%20comprises%20three%20main%20components%3A%20a%20conditional%20variational%0Aautoencoder%20%28CVAE%29%2C%20an%20attention%20module%2C%20and%20a%20multi-stage%20goal%20evaluator.%0ATrajectories%20are%20encoded%20using%20conditional%20variational%20autoencoders%20to%20acquire%0Aknowledge%20about%20the%20approximate%20distribution%20of%20pedestrians%27%20future%0Atrajectories%2C%20and%20combined%20with%20an%20attention%20mechanism%20to%20capture%20the%20temporal%0Adependency%20between%20trajectory%20sequences.%20The%20pivotal%20module%20is%20the%20multi-stage%0Agoal%20evaluator%2C%20which%20utilizes%20the%20encoded%20feature%20vectors%20to%20predict%0Aintermediate%20goals%2C%20effectively%20minimizing%20cumulative%20errors%20in%20the%20recursive%0Ainference%20process.%20The%20effectiveness%20of%20MGNet%20is%20demonstrated%20through%0Acomprehensive%20experiments%20on%20the%20JAAD%20and%20PIE%20datasets.%20Comparative%20evaluations%0Aagainst%20state-of-the-art%20algorithms%20reveal%20significant%20performance%20improvements%0Aachieved%20by%20our%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18050v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multi-Stage%2520Goal-Driven%2520Network%2520for%2520Pedestrian%2520Trajectory%2520Prediction%26entry.906535625%3DXiuen%2520Wu%2520and%2520Tao%2520Wang%2520and%2520Yuanzheng%2520Cai%2520and%2520Lingyu%2520Liang%2520and%2520George%2520Papageorgiou%26entry.1292438233%3D%2520%2520Pedestrian%2520trajectory%2520prediction%2520plays%2520a%2520pivotal%2520role%2520in%2520ensuring%2520the%2520safety%250Aand%2520efficiency%2520of%2520various%2520applications%252C%2520including%2520autonomous%2520vehicles%2520and%250Atraffic%2520management%2520systems.%2520This%2520paper%2520proposes%2520a%2520novel%2520method%2520for%2520pedestrian%250Atrajectory%2520prediction%252C%2520called%2520multi-stage%2520goal-driven%2520network%2520%2528MGNet%2529.%250ADiverging%2520from%2520prior%2520approaches%2520relying%2520on%2520stepwise%2520recursive%2520prediction%2520and%250Athe%2520singular%2520forecasting%2520of%2520a%2520long-term%2520goal%252C%2520MGNet%2520directs%2520trajectory%250Ageneration%2520by%2520forecasting%2520intermediate%2520stage%2520goals%252C%2520thereby%2520reducing%2520prediction%250Aerrors.%2520The%2520network%2520comprises%2520three%2520main%2520components%253A%2520a%2520conditional%2520variational%250Aautoencoder%2520%2528CVAE%2529%252C%2520an%2520attention%2520module%252C%2520and%2520a%2520multi-stage%2520goal%2520evaluator.%250ATrajectories%2520are%2520encoded%2520using%2520conditional%2520variational%2520autoencoders%2520to%2520acquire%250Aknowledge%2520about%2520the%2520approximate%2520distribution%2520of%2520pedestrians%2527%2520future%250Atrajectories%252C%2520and%2520combined%2520with%2520an%2520attention%2520mechanism%2520to%2520capture%2520the%2520temporal%250Adependency%2520between%2520trajectory%2520sequences.%2520The%2520pivotal%2520module%2520is%2520the%2520multi-stage%250Agoal%2520evaluator%252C%2520which%2520utilizes%2520the%2520encoded%2520feature%2520vectors%2520to%2520predict%250Aintermediate%2520goals%252C%2520effectively%2520minimizing%2520cumulative%2520errors%2520in%2520the%2520recursive%250Ainference%2520process.%2520The%2520effectiveness%2520of%2520MGNet%2520is%2520demonstrated%2520through%250Acomprehensive%2520experiments%2520on%2520the%2520JAAD%2520and%2520PIE%2520datasets.%2520Comparative%2520evaluations%250Aagainst%2520state-of-the-art%2520algorithms%2520reveal%2520significant%2520performance%2520improvements%250Aachieved%2520by%2520our%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18050v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multi-Stage%20Goal-Driven%20Network%20for%20Pedestrian%20Trajectory%20Prediction&entry.906535625=Xiuen%20Wu%20and%20Tao%20Wang%20and%20Yuanzheng%20Cai%20and%20Lingyu%20Liang%20and%20George%20Papageorgiou&entry.1292438233=%20%20Pedestrian%20trajectory%20prediction%20plays%20a%20pivotal%20role%20in%20ensuring%20the%20safety%0Aand%20efficiency%20of%20various%20applications%2C%20including%20autonomous%20vehicles%20and%0Atraffic%20management%20systems.%20This%20paper%20proposes%20a%20novel%20method%20for%20pedestrian%0Atrajectory%20prediction%2C%20called%20multi-stage%20goal-driven%20network%20%28MGNet%29.%0ADiverging%20from%20prior%20approaches%20relying%20on%20stepwise%20recursive%20prediction%20and%0Athe%20singular%20forecasting%20of%20a%20long-term%20goal%2C%20MGNet%20directs%20trajectory%0Ageneration%20by%20forecasting%20intermediate%20stage%20goals%2C%20thereby%20reducing%20prediction%0Aerrors.%20The%20network%20comprises%20three%20main%20components%3A%20a%20conditional%20variational%0Aautoencoder%20%28CVAE%29%2C%20an%20attention%20module%2C%20and%20a%20multi-stage%20goal%20evaluator.%0ATrajectories%20are%20encoded%20using%20conditional%20variational%20autoencoders%20to%20acquire%0Aknowledge%20about%20the%20approximate%20distribution%20of%20pedestrians%27%20future%0Atrajectories%2C%20and%20combined%20with%20an%20attention%20mechanism%20to%20capture%20the%20temporal%0Adependency%20between%20trajectory%20sequences.%20The%20pivotal%20module%20is%20the%20multi-stage%0Agoal%20evaluator%2C%20which%20utilizes%20the%20encoded%20feature%20vectors%20to%20predict%0Aintermediate%20goals%2C%20effectively%20minimizing%20cumulative%20errors%20in%20the%20recursive%0Ainference%20process.%20The%20effectiveness%20of%20MGNet%20is%20demonstrated%20through%0Acomprehensive%20experiments%20on%20the%20JAAD%20and%20PIE%20datasets.%20Comparative%20evaluations%0Aagainst%20state-of-the-art%20algorithms%20reveal%20significant%20performance%20improvements%0Aachieved%20by%20our%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18050v1&entry.124074799=Read"},
{"title": "An unsupervised approach towards promptable defect segmentation in\n  laser-based additive manufacturing by Segment Anything", "author": "Israt Zarin Era and Imtiaz Ahmed and Zhichao Liu and Srinjoy Das", "abstract": "  Foundation models are currently driving a paradigm shift in computer vision\ntasks for various fields including biology, astronomy, and robotics among\nothers, leveraging user-generated prompts to enhance their performance. In the\nLaser Additive Manufacturing (LAM) domain, accurate image-based defect\nsegmentation is imperative to ensure product quality and facilitate real-time\nprocess control. However, such tasks are often characterized by multiple\nchallenges including the absence of labels and the requirement for low latency\ninference among others. Porosity is a very common defect in LAM due to lack of\nfusion, entrapped gas, and keyholes, directly affecting mechanical properties\nlike tensile strength, stiffness, and hardness, thereby compromising the\nquality of the final product. To address these issues, we construct a framework\nfor image segmentation using a state-of-the-art Vision Transformer (ViT) based\nFoundation model (Segment Anything Model) with a novel multi-point prompt\ngeneration scheme using unsupervised clustering. Utilizing our framework we\nperform porosity segmentation in a case study of laser-based powder bed fusion\n(L-PBF) and obtain high accuracy without using any labeled data to guide the\nprompt tuning process. By capitalizing on lightweight foundation model\ninference combined with unsupervised prompt generation, we envision\nconstructing a real-time anomaly detection pipeline that could revolutionize\ncurrent laser additive manufacturing processes, thereby facilitating the shift\ntowards Industry 4.0 and promoting defect-free production along with\noperational efficiency.\n", "link": "http://arxiv.org/abs/2312.04063v3", "date": "2024-06-26", "relevancy": 2.1371, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5377}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5326}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5299}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20unsupervised%20approach%20towards%20promptable%20defect%20segmentation%20in%0A%20%20laser-based%20additive%20manufacturing%20by%20Segment%20Anything&body=Title%3A%20An%20unsupervised%20approach%20towards%20promptable%20defect%20segmentation%20in%0A%20%20laser-based%20additive%20manufacturing%20by%20Segment%20Anything%0AAuthor%3A%20Israt%20Zarin%20Era%20and%20Imtiaz%20Ahmed%20and%20Zhichao%20Liu%20and%20Srinjoy%20Das%0AAbstract%3A%20%20%20Foundation%20models%20are%20currently%20driving%20a%20paradigm%20shift%20in%20computer%20vision%0Atasks%20for%20various%20fields%20including%20biology%2C%20astronomy%2C%20and%20robotics%20among%0Aothers%2C%20leveraging%20user-generated%20prompts%20to%20enhance%20their%20performance.%20In%20the%0ALaser%20Additive%20Manufacturing%20%28LAM%29%20domain%2C%20accurate%20image-based%20defect%0Asegmentation%20is%20imperative%20to%20ensure%20product%20quality%20and%20facilitate%20real-time%0Aprocess%20control.%20However%2C%20such%20tasks%20are%20often%20characterized%20by%20multiple%0Achallenges%20including%20the%20absence%20of%20labels%20and%20the%20requirement%20for%20low%20latency%0Ainference%20among%20others.%20Porosity%20is%20a%20very%20common%20defect%20in%20LAM%20due%20to%20lack%20of%0Afusion%2C%20entrapped%20gas%2C%20and%20keyholes%2C%20directly%20affecting%20mechanical%20properties%0Alike%20tensile%20strength%2C%20stiffness%2C%20and%20hardness%2C%20thereby%20compromising%20the%0Aquality%20of%20the%20final%20product.%20To%20address%20these%20issues%2C%20we%20construct%20a%20framework%0Afor%20image%20segmentation%20using%20a%20state-of-the-art%20Vision%20Transformer%20%28ViT%29%20based%0AFoundation%20model%20%28Segment%20Anything%20Model%29%20with%20a%20novel%20multi-point%20prompt%0Ageneration%20scheme%20using%20unsupervised%20clustering.%20Utilizing%20our%20framework%20we%0Aperform%20porosity%20segmentation%20in%20a%20case%20study%20of%20laser-based%20powder%20bed%20fusion%0A%28L-PBF%29%20and%20obtain%20high%20accuracy%20without%20using%20any%20labeled%20data%20to%20guide%20the%0Aprompt%20tuning%20process.%20By%20capitalizing%20on%20lightweight%20foundation%20model%0Ainference%20combined%20with%20unsupervised%20prompt%20generation%2C%20we%20envision%0Aconstructing%20a%20real-time%20anomaly%20detection%20pipeline%20that%20could%20revolutionize%0Acurrent%20laser%20additive%20manufacturing%20processes%2C%20thereby%20facilitating%20the%20shift%0Atowards%20Industry%204.0%20and%20promoting%20defect-free%20production%20along%20with%0Aoperational%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.04063v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520unsupervised%2520approach%2520towards%2520promptable%2520defect%2520segmentation%2520in%250A%2520%2520laser-based%2520additive%2520manufacturing%2520by%2520Segment%2520Anything%26entry.906535625%3DIsrat%2520Zarin%2520Era%2520and%2520Imtiaz%2520Ahmed%2520and%2520Zhichao%2520Liu%2520and%2520Srinjoy%2520Das%26entry.1292438233%3D%2520%2520Foundation%2520models%2520are%2520currently%2520driving%2520a%2520paradigm%2520shift%2520in%2520computer%2520vision%250Atasks%2520for%2520various%2520fields%2520including%2520biology%252C%2520astronomy%252C%2520and%2520robotics%2520among%250Aothers%252C%2520leveraging%2520user-generated%2520prompts%2520to%2520enhance%2520their%2520performance.%2520In%2520the%250ALaser%2520Additive%2520Manufacturing%2520%2528LAM%2529%2520domain%252C%2520accurate%2520image-based%2520defect%250Asegmentation%2520is%2520imperative%2520to%2520ensure%2520product%2520quality%2520and%2520facilitate%2520real-time%250Aprocess%2520control.%2520However%252C%2520such%2520tasks%2520are%2520often%2520characterized%2520by%2520multiple%250Achallenges%2520including%2520the%2520absence%2520of%2520labels%2520and%2520the%2520requirement%2520for%2520low%2520latency%250Ainference%2520among%2520others.%2520Porosity%2520is%2520a%2520very%2520common%2520defect%2520in%2520LAM%2520due%2520to%2520lack%2520of%250Afusion%252C%2520entrapped%2520gas%252C%2520and%2520keyholes%252C%2520directly%2520affecting%2520mechanical%2520properties%250Alike%2520tensile%2520strength%252C%2520stiffness%252C%2520and%2520hardness%252C%2520thereby%2520compromising%2520the%250Aquality%2520of%2520the%2520final%2520product.%2520To%2520address%2520these%2520issues%252C%2520we%2520construct%2520a%2520framework%250Afor%2520image%2520segmentation%2520using%2520a%2520state-of-the-art%2520Vision%2520Transformer%2520%2528ViT%2529%2520based%250AFoundation%2520model%2520%2528Segment%2520Anything%2520Model%2529%2520with%2520a%2520novel%2520multi-point%2520prompt%250Ageneration%2520scheme%2520using%2520unsupervised%2520clustering.%2520Utilizing%2520our%2520framework%2520we%250Aperform%2520porosity%2520segmentation%2520in%2520a%2520case%2520study%2520of%2520laser-based%2520powder%2520bed%2520fusion%250A%2528L-PBF%2529%2520and%2520obtain%2520high%2520accuracy%2520without%2520using%2520any%2520labeled%2520data%2520to%2520guide%2520the%250Aprompt%2520tuning%2520process.%2520By%2520capitalizing%2520on%2520lightweight%2520foundation%2520model%250Ainference%2520combined%2520with%2520unsupervised%2520prompt%2520generation%252C%2520we%2520envision%250Aconstructing%2520a%2520real-time%2520anomaly%2520detection%2520pipeline%2520that%2520could%2520revolutionize%250Acurrent%2520laser%2520additive%2520manufacturing%2520processes%252C%2520thereby%2520facilitating%2520the%2520shift%250Atowards%2520Industry%25204.0%2520and%2520promoting%2520defect-free%2520production%2520along%2520with%250Aoperational%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.04063v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20unsupervised%20approach%20towards%20promptable%20defect%20segmentation%20in%0A%20%20laser-based%20additive%20manufacturing%20by%20Segment%20Anything&entry.906535625=Israt%20Zarin%20Era%20and%20Imtiaz%20Ahmed%20and%20Zhichao%20Liu%20and%20Srinjoy%20Das&entry.1292438233=%20%20Foundation%20models%20are%20currently%20driving%20a%20paradigm%20shift%20in%20computer%20vision%0Atasks%20for%20various%20fields%20including%20biology%2C%20astronomy%2C%20and%20robotics%20among%0Aothers%2C%20leveraging%20user-generated%20prompts%20to%20enhance%20their%20performance.%20In%20the%0ALaser%20Additive%20Manufacturing%20%28LAM%29%20domain%2C%20accurate%20image-based%20defect%0Asegmentation%20is%20imperative%20to%20ensure%20product%20quality%20and%20facilitate%20real-time%0Aprocess%20control.%20However%2C%20such%20tasks%20are%20often%20characterized%20by%20multiple%0Achallenges%20including%20the%20absence%20of%20labels%20and%20the%20requirement%20for%20low%20latency%0Ainference%20among%20others.%20Porosity%20is%20a%20very%20common%20defect%20in%20LAM%20due%20to%20lack%20of%0Afusion%2C%20entrapped%20gas%2C%20and%20keyholes%2C%20directly%20affecting%20mechanical%20properties%0Alike%20tensile%20strength%2C%20stiffness%2C%20and%20hardness%2C%20thereby%20compromising%20the%0Aquality%20of%20the%20final%20product.%20To%20address%20these%20issues%2C%20we%20construct%20a%20framework%0Afor%20image%20segmentation%20using%20a%20state-of-the-art%20Vision%20Transformer%20%28ViT%29%20based%0AFoundation%20model%20%28Segment%20Anything%20Model%29%20with%20a%20novel%20multi-point%20prompt%0Ageneration%20scheme%20using%20unsupervised%20clustering.%20Utilizing%20our%20framework%20we%0Aperform%20porosity%20segmentation%20in%20a%20case%20study%20of%20laser-based%20powder%20bed%20fusion%0A%28L-PBF%29%20and%20obtain%20high%20accuracy%20without%20using%20any%20labeled%20data%20to%20guide%20the%0Aprompt%20tuning%20process.%20By%20capitalizing%20on%20lightweight%20foundation%20model%0Ainference%20combined%20with%20unsupervised%20prompt%20generation%2C%20we%20envision%0Aconstructing%20a%20real-time%20anomaly%20detection%20pipeline%20that%20could%20revolutionize%0Acurrent%20laser%20additive%20manufacturing%20processes%2C%20thereby%20facilitating%20the%20shift%0Atowards%20Industry%204.0%20and%20promoting%20defect-free%20production%20along%20with%0Aoperational%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.04063v3&entry.124074799=Read"},
{"title": "ChangeMamba: Remote Sensing Change Detection with Spatio-Temporal State\n  Space Model", "author": "Hongruixuan Chen and Jian Song and Chengxi Han and Junshi Xia and Naoto Yokoya", "abstract": "  Convolutional neural networks (CNN) and Transformers have made impressive\nprogress in the field of remote sensing change detection (CD). However, both\narchitectures have inherent shortcomings: CNN are constrained by a limited\nreceptive field that may hinder their ability to capture broader spatial\ncontexts, while Transformers are computationally intensive, making them costly\nto train and deploy on large datasets. Recently, the Mamba architecture, based\non state space models, has shown remarkable performance in a series of natural\nlanguage processing tasks, which can effectively compensate for the\nshortcomings of the above two architectures. In this paper, we explore for the\nfirst time the potential of the Mamba architecture for remote sensing CD tasks.\nWe tailor the corresponding frameworks, called MambaBCD, MambaSCD, and\nMambaBDA, for binary change detection (BCD), semantic change detection (SCD),\nand building damage assessment (BDA), respectively. All three frameworks adopt\nthe cutting-edge Visual Mamba architecture as the encoder, which allows full\nlearning of global spatial contextual information from the input images. For\nthe change decoder, which is available in all three architectures, we propose\nthree spatio-temporal relationship modeling mechanisms, which can be naturally\ncombined with the Mamba architecture and fully utilize its attribute to achieve\nspatio-temporal interaction of multi-temporal features, thereby obtaining\naccurate change information. On five benchmark datasets, our proposed\nframeworks outperform current CNN- and Transformer-based approaches without\nusing any complex training strategies or tricks, fully demonstrating the\npotential of the Mamba architecture in CD tasks. Further experiments show that\nour architecture is quite robust to degraded data. The source code will be\navailable in https://github.com/ChenHongruixuan/MambaCD\n", "link": "http://arxiv.org/abs/2404.03425v5", "date": "2024-06-26", "relevancy": 2.1304, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5479}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5341}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5249}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChangeMamba%3A%20Remote%20Sensing%20Change%20Detection%20with%20Spatio-Temporal%20State%0A%20%20Space%20Model&body=Title%3A%20ChangeMamba%3A%20Remote%20Sensing%20Change%20Detection%20with%20Spatio-Temporal%20State%0A%20%20Space%20Model%0AAuthor%3A%20Hongruixuan%20Chen%20and%20Jian%20Song%20and%20Chengxi%20Han%20and%20Junshi%20Xia%20and%20Naoto%20Yokoya%0AAbstract%3A%20%20%20Convolutional%20neural%20networks%20%28CNN%29%20and%20Transformers%20have%20made%20impressive%0Aprogress%20in%20the%20field%20of%20remote%20sensing%20change%20detection%20%28CD%29.%20However%2C%20both%0Aarchitectures%20have%20inherent%20shortcomings%3A%20CNN%20are%20constrained%20by%20a%20limited%0Areceptive%20field%20that%20may%20hinder%20their%20ability%20to%20capture%20broader%20spatial%0Acontexts%2C%20while%20Transformers%20are%20computationally%20intensive%2C%20making%20them%20costly%0Ato%20train%20and%20deploy%20on%20large%20datasets.%20Recently%2C%20the%20Mamba%20architecture%2C%20based%0Aon%20state%20space%20models%2C%20has%20shown%20remarkable%20performance%20in%20a%20series%20of%20natural%0Alanguage%20processing%20tasks%2C%20which%20can%20effectively%20compensate%20for%20the%0Ashortcomings%20of%20the%20above%20two%20architectures.%20In%20this%20paper%2C%20we%20explore%20for%20the%0Afirst%20time%20the%20potential%20of%20the%20Mamba%20architecture%20for%20remote%20sensing%20CD%20tasks.%0AWe%20tailor%20the%20corresponding%20frameworks%2C%20called%20MambaBCD%2C%20MambaSCD%2C%20and%0AMambaBDA%2C%20for%20binary%20change%20detection%20%28BCD%29%2C%20semantic%20change%20detection%20%28SCD%29%2C%0Aand%20building%20damage%20assessment%20%28BDA%29%2C%20respectively.%20All%20three%20frameworks%20adopt%0Athe%20cutting-edge%20Visual%20Mamba%20architecture%20as%20the%20encoder%2C%20which%20allows%20full%0Alearning%20of%20global%20spatial%20contextual%20information%20from%20the%20input%20images.%20For%0Athe%20change%20decoder%2C%20which%20is%20available%20in%20all%20three%20architectures%2C%20we%20propose%0Athree%20spatio-temporal%20relationship%20modeling%20mechanisms%2C%20which%20can%20be%20naturally%0Acombined%20with%20the%20Mamba%20architecture%20and%20fully%20utilize%20its%20attribute%20to%20achieve%0Aspatio-temporal%20interaction%20of%20multi-temporal%20features%2C%20thereby%20obtaining%0Aaccurate%20change%20information.%20On%20five%20benchmark%20datasets%2C%20our%20proposed%0Aframeworks%20outperform%20current%20CNN-%20and%20Transformer-based%20approaches%20without%0Ausing%20any%20complex%20training%20strategies%20or%20tricks%2C%20fully%20demonstrating%20the%0Apotential%20of%20the%20Mamba%20architecture%20in%20CD%20tasks.%20Further%20experiments%20show%20that%0Aour%20architecture%20is%20quite%20robust%20to%20degraded%20data.%20The%20source%20code%20will%20be%0Aavailable%20in%20https%3A//github.com/ChenHongruixuan/MambaCD%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03425v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChangeMamba%253A%2520Remote%2520Sensing%2520Change%2520Detection%2520with%2520Spatio-Temporal%2520State%250A%2520%2520Space%2520Model%26entry.906535625%3DHongruixuan%2520Chen%2520and%2520Jian%2520Song%2520and%2520Chengxi%2520Han%2520and%2520Junshi%2520Xia%2520and%2520Naoto%2520Yokoya%26entry.1292438233%3D%2520%2520Convolutional%2520neural%2520networks%2520%2528CNN%2529%2520and%2520Transformers%2520have%2520made%2520impressive%250Aprogress%2520in%2520the%2520field%2520of%2520remote%2520sensing%2520change%2520detection%2520%2528CD%2529.%2520However%252C%2520both%250Aarchitectures%2520have%2520inherent%2520shortcomings%253A%2520CNN%2520are%2520constrained%2520by%2520a%2520limited%250Areceptive%2520field%2520that%2520may%2520hinder%2520their%2520ability%2520to%2520capture%2520broader%2520spatial%250Acontexts%252C%2520while%2520Transformers%2520are%2520computationally%2520intensive%252C%2520making%2520them%2520costly%250Ato%2520train%2520and%2520deploy%2520on%2520large%2520datasets.%2520Recently%252C%2520the%2520Mamba%2520architecture%252C%2520based%250Aon%2520state%2520space%2520models%252C%2520has%2520shown%2520remarkable%2520performance%2520in%2520a%2520series%2520of%2520natural%250Alanguage%2520processing%2520tasks%252C%2520which%2520can%2520effectively%2520compensate%2520for%2520the%250Ashortcomings%2520of%2520the%2520above%2520two%2520architectures.%2520In%2520this%2520paper%252C%2520we%2520explore%2520for%2520the%250Afirst%2520time%2520the%2520potential%2520of%2520the%2520Mamba%2520architecture%2520for%2520remote%2520sensing%2520CD%2520tasks.%250AWe%2520tailor%2520the%2520corresponding%2520frameworks%252C%2520called%2520MambaBCD%252C%2520MambaSCD%252C%2520and%250AMambaBDA%252C%2520for%2520binary%2520change%2520detection%2520%2528BCD%2529%252C%2520semantic%2520change%2520detection%2520%2528SCD%2529%252C%250Aand%2520building%2520damage%2520assessment%2520%2528BDA%2529%252C%2520respectively.%2520All%2520three%2520frameworks%2520adopt%250Athe%2520cutting-edge%2520Visual%2520Mamba%2520architecture%2520as%2520the%2520encoder%252C%2520which%2520allows%2520full%250Alearning%2520of%2520global%2520spatial%2520contextual%2520information%2520from%2520the%2520input%2520images.%2520For%250Athe%2520change%2520decoder%252C%2520which%2520is%2520available%2520in%2520all%2520three%2520architectures%252C%2520we%2520propose%250Athree%2520spatio-temporal%2520relationship%2520modeling%2520mechanisms%252C%2520which%2520can%2520be%2520naturally%250Acombined%2520with%2520the%2520Mamba%2520architecture%2520and%2520fully%2520utilize%2520its%2520attribute%2520to%2520achieve%250Aspatio-temporal%2520interaction%2520of%2520multi-temporal%2520features%252C%2520thereby%2520obtaining%250Aaccurate%2520change%2520information.%2520On%2520five%2520benchmark%2520datasets%252C%2520our%2520proposed%250Aframeworks%2520outperform%2520current%2520CNN-%2520and%2520Transformer-based%2520approaches%2520without%250Ausing%2520any%2520complex%2520training%2520strategies%2520or%2520tricks%252C%2520fully%2520demonstrating%2520the%250Apotential%2520of%2520the%2520Mamba%2520architecture%2520in%2520CD%2520tasks.%2520Further%2520experiments%2520show%2520that%250Aour%2520architecture%2520is%2520quite%2520robust%2520to%2520degraded%2520data.%2520The%2520source%2520code%2520will%2520be%250Aavailable%2520in%2520https%253A//github.com/ChenHongruixuan/MambaCD%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.03425v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChangeMamba%3A%20Remote%20Sensing%20Change%20Detection%20with%20Spatio-Temporal%20State%0A%20%20Space%20Model&entry.906535625=Hongruixuan%20Chen%20and%20Jian%20Song%20and%20Chengxi%20Han%20and%20Junshi%20Xia%20and%20Naoto%20Yokoya&entry.1292438233=%20%20Convolutional%20neural%20networks%20%28CNN%29%20and%20Transformers%20have%20made%20impressive%0Aprogress%20in%20the%20field%20of%20remote%20sensing%20change%20detection%20%28CD%29.%20However%2C%20both%0Aarchitectures%20have%20inherent%20shortcomings%3A%20CNN%20are%20constrained%20by%20a%20limited%0Areceptive%20field%20that%20may%20hinder%20their%20ability%20to%20capture%20broader%20spatial%0Acontexts%2C%20while%20Transformers%20are%20computationally%20intensive%2C%20making%20them%20costly%0Ato%20train%20and%20deploy%20on%20large%20datasets.%20Recently%2C%20the%20Mamba%20architecture%2C%20based%0Aon%20state%20space%20models%2C%20has%20shown%20remarkable%20performance%20in%20a%20series%20of%20natural%0Alanguage%20processing%20tasks%2C%20which%20can%20effectively%20compensate%20for%20the%0Ashortcomings%20of%20the%20above%20two%20architectures.%20In%20this%20paper%2C%20we%20explore%20for%20the%0Afirst%20time%20the%20potential%20of%20the%20Mamba%20architecture%20for%20remote%20sensing%20CD%20tasks.%0AWe%20tailor%20the%20corresponding%20frameworks%2C%20called%20MambaBCD%2C%20MambaSCD%2C%20and%0AMambaBDA%2C%20for%20binary%20change%20detection%20%28BCD%29%2C%20semantic%20change%20detection%20%28SCD%29%2C%0Aand%20building%20damage%20assessment%20%28BDA%29%2C%20respectively.%20All%20three%20frameworks%20adopt%0Athe%20cutting-edge%20Visual%20Mamba%20architecture%20as%20the%20encoder%2C%20which%20allows%20full%0Alearning%20of%20global%20spatial%20contextual%20information%20from%20the%20input%20images.%20For%0Athe%20change%20decoder%2C%20which%20is%20available%20in%20all%20three%20architectures%2C%20we%20propose%0Athree%20spatio-temporal%20relationship%20modeling%20mechanisms%2C%20which%20can%20be%20naturally%0Acombined%20with%20the%20Mamba%20architecture%20and%20fully%20utilize%20its%20attribute%20to%20achieve%0Aspatio-temporal%20interaction%20of%20multi-temporal%20features%2C%20thereby%20obtaining%0Aaccurate%20change%20information.%20On%20five%20benchmark%20datasets%2C%20our%20proposed%0Aframeworks%20outperform%20current%20CNN-%20and%20Transformer-based%20approaches%20without%0Ausing%20any%20complex%20training%20strategies%20or%20tricks%2C%20fully%20demonstrating%20the%0Apotential%20of%20the%20Mamba%20architecture%20in%20CD%20tasks.%20Further%20experiments%20show%20that%0Aour%20architecture%20is%20quite%20robust%20to%20degraded%20data.%20The%20source%20code%20will%20be%0Aavailable%20in%20https%3A//github.com/ChenHongruixuan/MambaCD%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03425v5&entry.124074799=Read"},
{"title": "ObjFormer: Learning Land-Cover Changes From Paired OSM Data and Optical\n  High-Resolution Imagery via Object-Guided Transformer", "author": "Hongruixuan Chen and Cuiling Lan and Jian Song and Clifford Broni-Bediako and Junshi Xia and Naoto Yokoya", "abstract": "  Optical high-resolution imagery and OSM data are two important data sources\nof change detection (CD). Previous related studies focus on utilizing the\ninformation in OSM data to aid the CD on optical high-resolution images. This\npaper pioneers the direct detection of land-cover changes utilizing paired OSM\ndata and optical imagery, thereby expanding the scope of CD tasks. To this end,\nwe propose an object-guided Transformer (ObjFormer) by naturally combining the\nobject-based image analysis (OBIA) technique with the advanced vision\nTransformer architecture. This combination can significantly reduce the\ncomputational overhead in the self-attention module without adding extra\nparameters or layers. ObjFormer has a hierarchical pseudo-siamese encoder\nconsisting of object-guided self-attention modules that extracts multi-level\nheterogeneous features from OSM data and optical images; a decoder consisting\nof object-guided cross-attention modules can recover land-cover changes from\nthe extracted heterogeneous features. Beyond basic binary change detection,\nthis paper raises a new semi-supervised semantic change detection task that\ndoes not require any manually annotated land-cover labels to train semantic\nchange detectors. Two lightweight semantic decoders are added to ObjFormer to\naccomplish this task efficiently. A converse cross-entropy loss is designed to\nfully utilize negative samples, contributing to the great performance\nimprovement in this task. A large-scale benchmark dataset called OpenMapCD\ncontaining 1,287 samples covering 40 regions on six continents is constructed\nto conduct detailed experiments. The results show the effectiveness of our\nmethods in this new kind of CD task. Additionally, case studies in Japanese\ncities demonstrate the framework's generalizability and practical potential.\nThe OpenMapCD and source code are available in\nhttps://github.com/ChenHongruixuan/ObjFormer\n", "link": "http://arxiv.org/abs/2310.02674v3", "date": "2024-06-26", "relevancy": 2.128, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5396}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5352}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5258}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ObjFormer%3A%20Learning%20Land-Cover%20Changes%20From%20Paired%20OSM%20Data%20and%20Optical%0A%20%20High-Resolution%20Imagery%20via%20Object-Guided%20Transformer&body=Title%3A%20ObjFormer%3A%20Learning%20Land-Cover%20Changes%20From%20Paired%20OSM%20Data%20and%20Optical%0A%20%20High-Resolution%20Imagery%20via%20Object-Guided%20Transformer%0AAuthor%3A%20Hongruixuan%20Chen%20and%20Cuiling%20Lan%20and%20Jian%20Song%20and%20Clifford%20Broni-Bediako%20and%20Junshi%20Xia%20and%20Naoto%20Yokoya%0AAbstract%3A%20%20%20Optical%20high-resolution%20imagery%20and%20OSM%20data%20are%20two%20important%20data%20sources%0Aof%20change%20detection%20%28CD%29.%20Previous%20related%20studies%20focus%20on%20utilizing%20the%0Ainformation%20in%20OSM%20data%20to%20aid%20the%20CD%20on%20optical%20high-resolution%20images.%20This%0Apaper%20pioneers%20the%20direct%20detection%20of%20land-cover%20changes%20utilizing%20paired%20OSM%0Adata%20and%20optical%20imagery%2C%20thereby%20expanding%20the%20scope%20of%20CD%20tasks.%20To%20this%20end%2C%0Awe%20propose%20an%20object-guided%20Transformer%20%28ObjFormer%29%20by%20naturally%20combining%20the%0Aobject-based%20image%20analysis%20%28OBIA%29%20technique%20with%20the%20advanced%20vision%0ATransformer%20architecture.%20This%20combination%20can%20significantly%20reduce%20the%0Acomputational%20overhead%20in%20the%20self-attention%20module%20without%20adding%20extra%0Aparameters%20or%20layers.%20ObjFormer%20has%20a%20hierarchical%20pseudo-siamese%20encoder%0Aconsisting%20of%20object-guided%20self-attention%20modules%20that%20extracts%20multi-level%0Aheterogeneous%20features%20from%20OSM%20data%20and%20optical%20images%3B%20a%20decoder%20consisting%0Aof%20object-guided%20cross-attention%20modules%20can%20recover%20land-cover%20changes%20from%0Athe%20extracted%20heterogeneous%20features.%20Beyond%20basic%20binary%20change%20detection%2C%0Athis%20paper%20raises%20a%20new%20semi-supervised%20semantic%20change%20detection%20task%20that%0Adoes%20not%20require%20any%20manually%20annotated%20land-cover%20labels%20to%20train%20semantic%0Achange%20detectors.%20Two%20lightweight%20semantic%20decoders%20are%20added%20to%20ObjFormer%20to%0Aaccomplish%20this%20task%20efficiently.%20A%20converse%20cross-entropy%20loss%20is%20designed%20to%0Afully%20utilize%20negative%20samples%2C%20contributing%20to%20the%20great%20performance%0Aimprovement%20in%20this%20task.%20A%20large-scale%20benchmark%20dataset%20called%20OpenMapCD%0Acontaining%201%2C287%20samples%20covering%2040%20regions%20on%20six%20continents%20is%20constructed%0Ato%20conduct%20detailed%20experiments.%20The%20results%20show%20the%20effectiveness%20of%20our%0Amethods%20in%20this%20new%20kind%20of%20CD%20task.%20Additionally%2C%20case%20studies%20in%20Japanese%0Acities%20demonstrate%20the%20framework%27s%20generalizability%20and%20practical%20potential.%0AThe%20OpenMapCD%20and%20source%20code%20are%20available%20in%0Ahttps%3A//github.com/ChenHongruixuan/ObjFormer%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.02674v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObjFormer%253A%2520Learning%2520Land-Cover%2520Changes%2520From%2520Paired%2520OSM%2520Data%2520and%2520Optical%250A%2520%2520High-Resolution%2520Imagery%2520via%2520Object-Guided%2520Transformer%26entry.906535625%3DHongruixuan%2520Chen%2520and%2520Cuiling%2520Lan%2520and%2520Jian%2520Song%2520and%2520Clifford%2520Broni-Bediako%2520and%2520Junshi%2520Xia%2520and%2520Naoto%2520Yokoya%26entry.1292438233%3D%2520%2520Optical%2520high-resolution%2520imagery%2520and%2520OSM%2520data%2520are%2520two%2520important%2520data%2520sources%250Aof%2520change%2520detection%2520%2528CD%2529.%2520Previous%2520related%2520studies%2520focus%2520on%2520utilizing%2520the%250Ainformation%2520in%2520OSM%2520data%2520to%2520aid%2520the%2520CD%2520on%2520optical%2520high-resolution%2520images.%2520This%250Apaper%2520pioneers%2520the%2520direct%2520detection%2520of%2520land-cover%2520changes%2520utilizing%2520paired%2520OSM%250Adata%2520and%2520optical%2520imagery%252C%2520thereby%2520expanding%2520the%2520scope%2520of%2520CD%2520tasks.%2520To%2520this%2520end%252C%250Awe%2520propose%2520an%2520object-guided%2520Transformer%2520%2528ObjFormer%2529%2520by%2520naturally%2520combining%2520the%250Aobject-based%2520image%2520analysis%2520%2528OBIA%2529%2520technique%2520with%2520the%2520advanced%2520vision%250ATransformer%2520architecture.%2520This%2520combination%2520can%2520significantly%2520reduce%2520the%250Acomputational%2520overhead%2520in%2520the%2520self-attention%2520module%2520without%2520adding%2520extra%250Aparameters%2520or%2520layers.%2520ObjFormer%2520has%2520a%2520hierarchical%2520pseudo-siamese%2520encoder%250Aconsisting%2520of%2520object-guided%2520self-attention%2520modules%2520that%2520extracts%2520multi-level%250Aheterogeneous%2520features%2520from%2520OSM%2520data%2520and%2520optical%2520images%253B%2520a%2520decoder%2520consisting%250Aof%2520object-guided%2520cross-attention%2520modules%2520can%2520recover%2520land-cover%2520changes%2520from%250Athe%2520extracted%2520heterogeneous%2520features.%2520Beyond%2520basic%2520binary%2520change%2520detection%252C%250Athis%2520paper%2520raises%2520a%2520new%2520semi-supervised%2520semantic%2520change%2520detection%2520task%2520that%250Adoes%2520not%2520require%2520any%2520manually%2520annotated%2520land-cover%2520labels%2520to%2520train%2520semantic%250Achange%2520detectors.%2520Two%2520lightweight%2520semantic%2520decoders%2520are%2520added%2520to%2520ObjFormer%2520to%250Aaccomplish%2520this%2520task%2520efficiently.%2520A%2520converse%2520cross-entropy%2520loss%2520is%2520designed%2520to%250Afully%2520utilize%2520negative%2520samples%252C%2520contributing%2520to%2520the%2520great%2520performance%250Aimprovement%2520in%2520this%2520task.%2520A%2520large-scale%2520benchmark%2520dataset%2520called%2520OpenMapCD%250Acontaining%25201%252C287%2520samples%2520covering%252040%2520regions%2520on%2520six%2520continents%2520is%2520constructed%250Ato%2520conduct%2520detailed%2520experiments.%2520The%2520results%2520show%2520the%2520effectiveness%2520of%2520our%250Amethods%2520in%2520this%2520new%2520kind%2520of%2520CD%2520task.%2520Additionally%252C%2520case%2520studies%2520in%2520Japanese%250Acities%2520demonstrate%2520the%2520framework%2527s%2520generalizability%2520and%2520practical%2520potential.%250AThe%2520OpenMapCD%2520and%2520source%2520code%2520are%2520available%2520in%250Ahttps%253A//github.com/ChenHongruixuan/ObjFormer%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.02674v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ObjFormer%3A%20Learning%20Land-Cover%20Changes%20From%20Paired%20OSM%20Data%20and%20Optical%0A%20%20High-Resolution%20Imagery%20via%20Object-Guided%20Transformer&entry.906535625=Hongruixuan%20Chen%20and%20Cuiling%20Lan%20and%20Jian%20Song%20and%20Clifford%20Broni-Bediako%20and%20Junshi%20Xia%20and%20Naoto%20Yokoya&entry.1292438233=%20%20Optical%20high-resolution%20imagery%20and%20OSM%20data%20are%20two%20important%20data%20sources%0Aof%20change%20detection%20%28CD%29.%20Previous%20related%20studies%20focus%20on%20utilizing%20the%0Ainformation%20in%20OSM%20data%20to%20aid%20the%20CD%20on%20optical%20high-resolution%20images.%20This%0Apaper%20pioneers%20the%20direct%20detection%20of%20land-cover%20changes%20utilizing%20paired%20OSM%0Adata%20and%20optical%20imagery%2C%20thereby%20expanding%20the%20scope%20of%20CD%20tasks.%20To%20this%20end%2C%0Awe%20propose%20an%20object-guided%20Transformer%20%28ObjFormer%29%20by%20naturally%20combining%20the%0Aobject-based%20image%20analysis%20%28OBIA%29%20technique%20with%20the%20advanced%20vision%0ATransformer%20architecture.%20This%20combination%20can%20significantly%20reduce%20the%0Acomputational%20overhead%20in%20the%20self-attention%20module%20without%20adding%20extra%0Aparameters%20or%20layers.%20ObjFormer%20has%20a%20hierarchical%20pseudo-siamese%20encoder%0Aconsisting%20of%20object-guided%20self-attention%20modules%20that%20extracts%20multi-level%0Aheterogeneous%20features%20from%20OSM%20data%20and%20optical%20images%3B%20a%20decoder%20consisting%0Aof%20object-guided%20cross-attention%20modules%20can%20recover%20land-cover%20changes%20from%0Athe%20extracted%20heterogeneous%20features.%20Beyond%20basic%20binary%20change%20detection%2C%0Athis%20paper%20raises%20a%20new%20semi-supervised%20semantic%20change%20detection%20task%20that%0Adoes%20not%20require%20any%20manually%20annotated%20land-cover%20labels%20to%20train%20semantic%0Achange%20detectors.%20Two%20lightweight%20semantic%20decoders%20are%20added%20to%20ObjFormer%20to%0Aaccomplish%20this%20task%20efficiently.%20A%20converse%20cross-entropy%20loss%20is%20designed%20to%0Afully%20utilize%20negative%20samples%2C%20contributing%20to%20the%20great%20performance%0Aimprovement%20in%20this%20task.%20A%20large-scale%20benchmark%20dataset%20called%20OpenMapCD%0Acontaining%201%2C287%20samples%20covering%2040%20regions%20on%20six%20continents%20is%20constructed%0Ato%20conduct%20detailed%20experiments.%20The%20results%20show%20the%20effectiveness%20of%20our%0Amethods%20in%20this%20new%20kind%20of%20CD%20task.%20Additionally%2C%20case%20studies%20in%20Japanese%0Acities%20demonstrate%20the%20framework%27s%20generalizability%20and%20practical%20potential.%0AThe%20OpenMapCD%20and%20source%20code%20are%20available%20in%0Ahttps%3A//github.com/ChenHongruixuan/ObjFormer%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.02674v3&entry.124074799=Read"},
{"title": "Facial Image Feature Analysis and its Specialization for Fr\u00e9chet\n  Distance and Neighborhoods", "author": "Doruk Cetin and Benedikt Schesch and Petar Stamenkovic and Niko Benjamin Huber and Fabio Z\u00fcnd and Majed El Helou", "abstract": "  Assessing distances between images and image datasets is a fundamental task\nin vision-based research. It is a challenging open problem in the literature\nand despite the criticism it receives, the most ubiquitous method remains the\nFr\\'echet Inception Distance. The Inception network is trained on a specific\nlabeled dataset, ImageNet, which has caused the core of its criticism in the\nmost recent research. Improvements were shown by moving to self-supervision\nlearning over ImageNet, leaving the training data domain as an open question.\nWe make that last leap and provide the first analysis on domain-specific\nfeature training and its effects on feature distance, on the widely-researched\nfacial image domain. We provide our findings and insights on this domain\nspecialization for Fr\\'echet distance and image neighborhoods, supported by\nextensive experiments and in-depth user studies.\n", "link": "http://arxiv.org/abs/2406.18430v1", "date": "2024-06-26", "relevancy": 2.1188, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5523}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5181}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Facial%20Image%20Feature%20Analysis%20and%20its%20Specialization%20for%20Fr%C3%A9chet%0A%20%20Distance%20and%20Neighborhoods&body=Title%3A%20Facial%20Image%20Feature%20Analysis%20and%20its%20Specialization%20for%20Fr%C3%A9chet%0A%20%20Distance%20and%20Neighborhoods%0AAuthor%3A%20Doruk%20Cetin%20and%20Benedikt%20Schesch%20and%20Petar%20Stamenkovic%20and%20Niko%20Benjamin%20Huber%20and%20Fabio%20Z%C3%BCnd%20and%20Majed%20El%20Helou%0AAbstract%3A%20%20%20Assessing%20distances%20between%20images%20and%20image%20datasets%20is%20a%20fundamental%20task%0Ain%20vision-based%20research.%20It%20is%20a%20challenging%20open%20problem%20in%20the%20literature%0Aand%20despite%20the%20criticism%20it%20receives%2C%20the%20most%20ubiquitous%20method%20remains%20the%0AFr%5C%27echet%20Inception%20Distance.%20The%20Inception%20network%20is%20trained%20on%20a%20specific%0Alabeled%20dataset%2C%20ImageNet%2C%20which%20has%20caused%20the%20core%20of%20its%20criticism%20in%20the%0Amost%20recent%20research.%20Improvements%20were%20shown%20by%20moving%20to%20self-supervision%0Alearning%20over%20ImageNet%2C%20leaving%20the%20training%20data%20domain%20as%20an%20open%20question.%0AWe%20make%20that%20last%20leap%20and%20provide%20the%20first%20analysis%20on%20domain-specific%0Afeature%20training%20and%20its%20effects%20on%20feature%20distance%2C%20on%20the%20widely-researched%0Afacial%20image%20domain.%20We%20provide%20our%20findings%20and%20insights%20on%20this%20domain%0Aspecialization%20for%20Fr%5C%27echet%20distance%20and%20image%20neighborhoods%2C%20supported%20by%0Aextensive%20experiments%20and%20in-depth%20user%20studies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18430v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFacial%2520Image%2520Feature%2520Analysis%2520and%2520its%2520Specialization%2520for%2520Fr%25C3%25A9chet%250A%2520%2520Distance%2520and%2520Neighborhoods%26entry.906535625%3DDoruk%2520Cetin%2520and%2520Benedikt%2520Schesch%2520and%2520Petar%2520Stamenkovic%2520and%2520Niko%2520Benjamin%2520Huber%2520and%2520Fabio%2520Z%25C3%25BCnd%2520and%2520Majed%2520El%2520Helou%26entry.1292438233%3D%2520%2520Assessing%2520distances%2520between%2520images%2520and%2520image%2520datasets%2520is%2520a%2520fundamental%2520task%250Ain%2520vision-based%2520research.%2520It%2520is%2520a%2520challenging%2520open%2520problem%2520in%2520the%2520literature%250Aand%2520despite%2520the%2520criticism%2520it%2520receives%252C%2520the%2520most%2520ubiquitous%2520method%2520remains%2520the%250AFr%255C%2527echet%2520Inception%2520Distance.%2520The%2520Inception%2520network%2520is%2520trained%2520on%2520a%2520specific%250Alabeled%2520dataset%252C%2520ImageNet%252C%2520which%2520has%2520caused%2520the%2520core%2520of%2520its%2520criticism%2520in%2520the%250Amost%2520recent%2520research.%2520Improvements%2520were%2520shown%2520by%2520moving%2520to%2520self-supervision%250Alearning%2520over%2520ImageNet%252C%2520leaving%2520the%2520training%2520data%2520domain%2520as%2520an%2520open%2520question.%250AWe%2520make%2520that%2520last%2520leap%2520and%2520provide%2520the%2520first%2520analysis%2520on%2520domain-specific%250Afeature%2520training%2520and%2520its%2520effects%2520on%2520feature%2520distance%252C%2520on%2520the%2520widely-researched%250Afacial%2520image%2520domain.%2520We%2520provide%2520our%2520findings%2520and%2520insights%2520on%2520this%2520domain%250Aspecialization%2520for%2520Fr%255C%2527echet%2520distance%2520and%2520image%2520neighborhoods%252C%2520supported%2520by%250Aextensive%2520experiments%2520and%2520in-depth%2520user%2520studies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18430v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Facial%20Image%20Feature%20Analysis%20and%20its%20Specialization%20for%20Fr%C3%A9chet%0A%20%20Distance%20and%20Neighborhoods&entry.906535625=Doruk%20Cetin%20and%20Benedikt%20Schesch%20and%20Petar%20Stamenkovic%20and%20Niko%20Benjamin%20Huber%20and%20Fabio%20Z%C3%BCnd%20and%20Majed%20El%20Helou&entry.1292438233=%20%20Assessing%20distances%20between%20images%20and%20image%20datasets%20is%20a%20fundamental%20task%0Ain%20vision-based%20research.%20It%20is%20a%20challenging%20open%20problem%20in%20the%20literature%0Aand%20despite%20the%20criticism%20it%20receives%2C%20the%20most%20ubiquitous%20method%20remains%20the%0AFr%5C%27echet%20Inception%20Distance.%20The%20Inception%20network%20is%20trained%20on%20a%20specific%0Alabeled%20dataset%2C%20ImageNet%2C%20which%20has%20caused%20the%20core%20of%20its%20criticism%20in%20the%0Amost%20recent%20research.%20Improvements%20were%20shown%20by%20moving%20to%20self-supervision%0Alearning%20over%20ImageNet%2C%20leaving%20the%20training%20data%20domain%20as%20an%20open%20question.%0AWe%20make%20that%20last%20leap%20and%20provide%20the%20first%20analysis%20on%20domain-specific%0Afeature%20training%20and%20its%20effects%20on%20feature%20distance%2C%20on%20the%20widely-researched%0Afacial%20image%20domain.%20We%20provide%20our%20findings%20and%20insights%20on%20this%20domain%0Aspecialization%20for%20Fr%5C%27echet%20distance%20and%20image%20neighborhoods%2C%20supported%20by%0Aextensive%20experiments%20and%20in-depth%20user%20studies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18430v1&entry.124074799=Read"},
{"title": "Real-time Structure Flow", "author": "Juan David Adarve and Robert Mahony", "abstract": "  This article introduces the structure flow field; a flow field that can\nprovide high-speed robo-centric motion information for motion control of highly\ndynamic robotic devices and autonomous vehicles. Structure flow is the angular\n3D velocity of the scene at a given pixel. We show that structure flow posses\nan elegant evolution model in the form of a Partial Differential Equation (PDE)\nthat enables us to create dense flow predictions forward in time. We exploit\nthis structure to design a predictor-update algorithm to compute structure flow\nin real time using image and depth measurements. The prediction stage takes the\nprevious estimate of the structure flow and propagates it forward in time using\na numerical implementation of the structure flow PDE. The predicted flow is\nthen updated using new image and depth data. The algorithm runs up to 600 Hz on\na Desktop GPU machine for 512x512 images with flow values up to 8 pixels. We\nprovide ground truth validation on high-speed synthetic image sequences as well\nas results on real-life video on driving scenarios.\n", "link": "http://arxiv.org/abs/2406.18031v1", "date": "2024-06-26", "relevancy": 2.1188, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5793}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5397}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-time%20Structure%20Flow&body=Title%3A%20Real-time%20Structure%20Flow%0AAuthor%3A%20Juan%20David%20Adarve%20and%20Robert%20Mahony%0AAbstract%3A%20%20%20This%20article%20introduces%20the%20structure%20flow%20field%3B%20a%20flow%20field%20that%20can%0Aprovide%20high-speed%20robo-centric%20motion%20information%20for%20motion%20control%20of%20highly%0Adynamic%20robotic%20devices%20and%20autonomous%20vehicles.%20Structure%20flow%20is%20the%20angular%0A3D%20velocity%20of%20the%20scene%20at%20a%20given%20pixel.%20We%20show%20that%20structure%20flow%20posses%0Aan%20elegant%20evolution%20model%20in%20the%20form%20of%20a%20Partial%20Differential%20Equation%20%28PDE%29%0Athat%20enables%20us%20to%20create%20dense%20flow%20predictions%20forward%20in%20time.%20We%20exploit%0Athis%20structure%20to%20design%20a%20predictor-update%20algorithm%20to%20compute%20structure%20flow%0Ain%20real%20time%20using%20image%20and%20depth%20measurements.%20The%20prediction%20stage%20takes%20the%0Aprevious%20estimate%20of%20the%20structure%20flow%20and%20propagates%20it%20forward%20in%20time%20using%0Aa%20numerical%20implementation%20of%20the%20structure%20flow%20PDE.%20The%20predicted%20flow%20is%0Athen%20updated%20using%20new%20image%20and%20depth%20data.%20The%20algorithm%20runs%20up%20to%20600%20Hz%20on%0Aa%20Desktop%20GPU%20machine%20for%20512x512%20images%20with%20flow%20values%20up%20to%208%20pixels.%20We%0Aprovide%20ground%20truth%20validation%20on%20high-speed%20synthetic%20image%20sequences%20as%20well%0Aas%20results%20on%20real-life%20video%20on%20driving%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18031v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-time%2520Structure%2520Flow%26entry.906535625%3DJuan%2520David%2520Adarve%2520and%2520Robert%2520Mahony%26entry.1292438233%3D%2520%2520This%2520article%2520introduces%2520the%2520structure%2520flow%2520field%253B%2520a%2520flow%2520field%2520that%2520can%250Aprovide%2520high-speed%2520robo-centric%2520motion%2520information%2520for%2520motion%2520control%2520of%2520highly%250Adynamic%2520robotic%2520devices%2520and%2520autonomous%2520vehicles.%2520Structure%2520flow%2520is%2520the%2520angular%250A3D%2520velocity%2520of%2520the%2520scene%2520at%2520a%2520given%2520pixel.%2520We%2520show%2520that%2520structure%2520flow%2520posses%250Aan%2520elegant%2520evolution%2520model%2520in%2520the%2520form%2520of%2520a%2520Partial%2520Differential%2520Equation%2520%2528PDE%2529%250Athat%2520enables%2520us%2520to%2520create%2520dense%2520flow%2520predictions%2520forward%2520in%2520time.%2520We%2520exploit%250Athis%2520structure%2520to%2520design%2520a%2520predictor-update%2520algorithm%2520to%2520compute%2520structure%2520flow%250Ain%2520real%2520time%2520using%2520image%2520and%2520depth%2520measurements.%2520The%2520prediction%2520stage%2520takes%2520the%250Aprevious%2520estimate%2520of%2520the%2520structure%2520flow%2520and%2520propagates%2520it%2520forward%2520in%2520time%2520using%250Aa%2520numerical%2520implementation%2520of%2520the%2520structure%2520flow%2520PDE.%2520The%2520predicted%2520flow%2520is%250Athen%2520updated%2520using%2520new%2520image%2520and%2520depth%2520data.%2520The%2520algorithm%2520runs%2520up%2520to%2520600%2520Hz%2520on%250Aa%2520Desktop%2520GPU%2520machine%2520for%2520512x512%2520images%2520with%2520flow%2520values%2520up%2520to%25208%2520pixels.%2520We%250Aprovide%2520ground%2520truth%2520validation%2520on%2520high-speed%2520synthetic%2520image%2520sequences%2520as%2520well%250Aas%2520results%2520on%2520real-life%2520video%2520on%2520driving%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18031v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-time%20Structure%20Flow&entry.906535625=Juan%20David%20Adarve%20and%20Robert%20Mahony&entry.1292438233=%20%20This%20article%20introduces%20the%20structure%20flow%20field%3B%20a%20flow%20field%20that%20can%0Aprovide%20high-speed%20robo-centric%20motion%20information%20for%20motion%20control%20of%20highly%0Adynamic%20robotic%20devices%20and%20autonomous%20vehicles.%20Structure%20flow%20is%20the%20angular%0A3D%20velocity%20of%20the%20scene%20at%20a%20given%20pixel.%20We%20show%20that%20structure%20flow%20posses%0Aan%20elegant%20evolution%20model%20in%20the%20form%20of%20a%20Partial%20Differential%20Equation%20%28PDE%29%0Athat%20enables%20us%20to%20create%20dense%20flow%20predictions%20forward%20in%20time.%20We%20exploit%0Athis%20structure%20to%20design%20a%20predictor-update%20algorithm%20to%20compute%20structure%20flow%0Ain%20real%20time%20using%20image%20and%20depth%20measurements.%20The%20prediction%20stage%20takes%20the%0Aprevious%20estimate%20of%20the%20structure%20flow%20and%20propagates%20it%20forward%20in%20time%20using%0Aa%20numerical%20implementation%20of%20the%20structure%20flow%20PDE.%20The%20predicted%20flow%20is%0Athen%20updated%20using%20new%20image%20and%20depth%20data.%20The%20algorithm%20runs%20up%20to%20600%20Hz%20on%0Aa%20Desktop%20GPU%20machine%20for%20512x512%20images%20with%20flow%20values%20up%20to%208%20pixels.%20We%0Aprovide%20ground%20truth%20validation%20on%20high-speed%20synthetic%20image%20sequences%20as%20well%0Aas%20results%20on%20real-life%20video%20on%20driving%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18031v1&entry.124074799=Read"},
{"title": "BASS: Batched Attention-optimized Speculative Sampling", "author": "Haifeng Qian and Sujan Kumar Gonugondla and Sungsoo Ha and Mingyue Shang and Sanjay Krishna Gouda and Ramesh Nallapati and Sudipta Sengupta and Xiaofei Ma and Anoop Deoras", "abstract": "  Speculative decoding has emerged as a powerful method to improve latency and\nthroughput in hosting large language models. However, most existing\nimplementations focus on generating a single sequence. Real-world generative AI\napplications often require multiple responses and how to perform speculative\ndecoding in a batched setting while preserving its latency benefits poses\nnon-trivial challenges. This paper describes a system of batched speculative\ndecoding that sets a new state of the art in multi-sequence generation latency\nand that demonstrates superior GPU utilization as well as quality of\ngenerations within a time budget. For example, for a 7.8B-size model on a\nsingle A100 GPU and with a batch size of 8, each sequence is generated at an\naverage speed of 5.8ms per token, the overall throughput being 1.1K tokens per\nsecond. These results represent state-of-the-art latency and a 2.15X speed-up\nover optimized regular decoding. Within a time budget that regular decoding\ndoes not finish, our system is able to generate sequences with HumanEval\nPass@First of 43% and Pass@All of 61%, far exceeding what's feasible with\nsingle-sequence speculative decoding. Our peak GPU utilization during decoding\nreaches as high as 15.8%, more than 3X the highest of that of regular decoding\nand around 10X of single-sequence speculative decoding.\n", "link": "http://arxiv.org/abs/2404.15778v2", "date": "2024-06-26", "relevancy": 2.1092, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.564}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5276}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5123}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BASS%3A%20Batched%20Attention-optimized%20Speculative%20Sampling&body=Title%3A%20BASS%3A%20Batched%20Attention-optimized%20Speculative%20Sampling%0AAuthor%3A%20Haifeng%20Qian%20and%20Sujan%20Kumar%20Gonugondla%20and%20Sungsoo%20Ha%20and%20Mingyue%20Shang%20and%20Sanjay%20Krishna%20Gouda%20and%20Ramesh%20Nallapati%20and%20Sudipta%20Sengupta%20and%20Xiaofei%20Ma%20and%20Anoop%20Deoras%0AAbstract%3A%20%20%20Speculative%20decoding%20has%20emerged%20as%20a%20powerful%20method%20to%20improve%20latency%20and%0Athroughput%20in%20hosting%20large%20language%20models.%20However%2C%20most%20existing%0Aimplementations%20focus%20on%20generating%20a%20single%20sequence.%20Real-world%20generative%20AI%0Aapplications%20often%20require%20multiple%20responses%20and%20how%20to%20perform%20speculative%0Adecoding%20in%20a%20batched%20setting%20while%20preserving%20its%20latency%20benefits%20poses%0Anon-trivial%20challenges.%20This%20paper%20describes%20a%20system%20of%20batched%20speculative%0Adecoding%20that%20sets%20a%20new%20state%20of%20the%20art%20in%20multi-sequence%20generation%20latency%0Aand%20that%20demonstrates%20superior%20GPU%20utilization%20as%20well%20as%20quality%20of%0Agenerations%20within%20a%20time%20budget.%20For%20example%2C%20for%20a%207.8B-size%20model%20on%20a%0Asingle%20A100%20GPU%20and%20with%20a%20batch%20size%20of%208%2C%20each%20sequence%20is%20generated%20at%20an%0Aaverage%20speed%20of%205.8ms%20per%20token%2C%20the%20overall%20throughput%20being%201.1K%20tokens%20per%0Asecond.%20These%20results%20represent%20state-of-the-art%20latency%20and%20a%202.15X%20speed-up%0Aover%20optimized%20regular%20decoding.%20Within%20a%20time%20budget%20that%20regular%20decoding%0Adoes%20not%20finish%2C%20our%20system%20is%20able%20to%20generate%20sequences%20with%20HumanEval%0APass%40First%20of%2043%25%20and%20Pass%40All%20of%2061%25%2C%20far%20exceeding%20what%27s%20feasible%20with%0Asingle-sequence%20speculative%20decoding.%20Our%20peak%20GPU%20utilization%20during%20decoding%0Areaches%20as%20high%20as%2015.8%25%2C%20more%20than%203X%20the%20highest%20of%20that%20of%20regular%20decoding%0Aand%20around%2010X%20of%20single-sequence%20speculative%20decoding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15778v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBASS%253A%2520Batched%2520Attention-optimized%2520Speculative%2520Sampling%26entry.906535625%3DHaifeng%2520Qian%2520and%2520Sujan%2520Kumar%2520Gonugondla%2520and%2520Sungsoo%2520Ha%2520and%2520Mingyue%2520Shang%2520and%2520Sanjay%2520Krishna%2520Gouda%2520and%2520Ramesh%2520Nallapati%2520and%2520Sudipta%2520Sengupta%2520and%2520Xiaofei%2520Ma%2520and%2520Anoop%2520Deoras%26entry.1292438233%3D%2520%2520Speculative%2520decoding%2520has%2520emerged%2520as%2520a%2520powerful%2520method%2520to%2520improve%2520latency%2520and%250Athroughput%2520in%2520hosting%2520large%2520language%2520models.%2520However%252C%2520most%2520existing%250Aimplementations%2520focus%2520on%2520generating%2520a%2520single%2520sequence.%2520Real-world%2520generative%2520AI%250Aapplications%2520often%2520require%2520multiple%2520responses%2520and%2520how%2520to%2520perform%2520speculative%250Adecoding%2520in%2520a%2520batched%2520setting%2520while%2520preserving%2520its%2520latency%2520benefits%2520poses%250Anon-trivial%2520challenges.%2520This%2520paper%2520describes%2520a%2520system%2520of%2520batched%2520speculative%250Adecoding%2520that%2520sets%2520a%2520new%2520state%2520of%2520the%2520art%2520in%2520multi-sequence%2520generation%2520latency%250Aand%2520that%2520demonstrates%2520superior%2520GPU%2520utilization%2520as%2520well%2520as%2520quality%2520of%250Agenerations%2520within%2520a%2520time%2520budget.%2520For%2520example%252C%2520for%2520a%25207.8B-size%2520model%2520on%2520a%250Asingle%2520A100%2520GPU%2520and%2520with%2520a%2520batch%2520size%2520of%25208%252C%2520each%2520sequence%2520is%2520generated%2520at%2520an%250Aaverage%2520speed%2520of%25205.8ms%2520per%2520token%252C%2520the%2520overall%2520throughput%2520being%25201.1K%2520tokens%2520per%250Asecond.%2520These%2520results%2520represent%2520state-of-the-art%2520latency%2520and%2520a%25202.15X%2520speed-up%250Aover%2520optimized%2520regular%2520decoding.%2520Within%2520a%2520time%2520budget%2520that%2520regular%2520decoding%250Adoes%2520not%2520finish%252C%2520our%2520system%2520is%2520able%2520to%2520generate%2520sequences%2520with%2520HumanEval%250APass%2540First%2520of%252043%2525%2520and%2520Pass%2540All%2520of%252061%2525%252C%2520far%2520exceeding%2520what%2527s%2520feasible%2520with%250Asingle-sequence%2520speculative%2520decoding.%2520Our%2520peak%2520GPU%2520utilization%2520during%2520decoding%250Areaches%2520as%2520high%2520as%252015.8%2525%252C%2520more%2520than%25203X%2520the%2520highest%2520of%2520that%2520of%2520regular%2520decoding%250Aand%2520around%252010X%2520of%2520single-sequence%2520speculative%2520decoding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.15778v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BASS%3A%20Batched%20Attention-optimized%20Speculative%20Sampling&entry.906535625=Haifeng%20Qian%20and%20Sujan%20Kumar%20Gonugondla%20and%20Sungsoo%20Ha%20and%20Mingyue%20Shang%20and%20Sanjay%20Krishna%20Gouda%20and%20Ramesh%20Nallapati%20and%20Sudipta%20Sengupta%20and%20Xiaofei%20Ma%20and%20Anoop%20Deoras&entry.1292438233=%20%20Speculative%20decoding%20has%20emerged%20as%20a%20powerful%20method%20to%20improve%20latency%20and%0Athroughput%20in%20hosting%20large%20language%20models.%20However%2C%20most%20existing%0Aimplementations%20focus%20on%20generating%20a%20single%20sequence.%20Real-world%20generative%20AI%0Aapplications%20often%20require%20multiple%20responses%20and%20how%20to%20perform%20speculative%0Adecoding%20in%20a%20batched%20setting%20while%20preserving%20its%20latency%20benefits%20poses%0Anon-trivial%20challenges.%20This%20paper%20describes%20a%20system%20of%20batched%20speculative%0Adecoding%20that%20sets%20a%20new%20state%20of%20the%20art%20in%20multi-sequence%20generation%20latency%0Aand%20that%20demonstrates%20superior%20GPU%20utilization%20as%20well%20as%20quality%20of%0Agenerations%20within%20a%20time%20budget.%20For%20example%2C%20for%20a%207.8B-size%20model%20on%20a%0Asingle%20A100%20GPU%20and%20with%20a%20batch%20size%20of%208%2C%20each%20sequence%20is%20generated%20at%20an%0Aaverage%20speed%20of%205.8ms%20per%20token%2C%20the%20overall%20throughput%20being%201.1K%20tokens%20per%0Asecond.%20These%20results%20represent%20state-of-the-art%20latency%20and%20a%202.15X%20speed-up%0Aover%20optimized%20regular%20decoding.%20Within%20a%20time%20budget%20that%20regular%20decoding%0Adoes%20not%20finish%2C%20our%20system%20is%20able%20to%20generate%20sequences%20with%20HumanEval%0APass%40First%20of%2043%25%20and%20Pass%40All%20of%2061%25%2C%20far%20exceeding%20what%27s%20feasible%20with%0Asingle-sequence%20speculative%20decoding.%20Our%20peak%20GPU%20utilization%20during%20decoding%0Areaches%20as%20high%20as%2015.8%25%2C%20more%20than%203X%20the%20highest%20of%20that%20of%20regular%20decoding%0Aand%20around%2010X%20of%20single-sequence%20speculative%20decoding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15778v2&entry.124074799=Read"},
{"title": "InstructTA: Instruction-Tuned Targeted Attack for Large Vision-Language\n  Models", "author": "Xunguang Wang and Zhenlan Ji and Pingchuan Ma and Zongjie Li and Shuai Wang", "abstract": "  Large vision-language models (LVLMs) have demonstrated their incredible\ncapability in image understanding and response generation. However, this rich\nvisual interaction also makes LVLMs vulnerable to adversarial examples. In this\npaper, we formulate a novel and practical targeted attack scenario that the\nadversary can only know the vision encoder of the victim LVLM, without the\nknowledge of its prompts (which are often proprietary for service providers and\nnot publicly available) and its underlying large language model (LLM). This\npractical setting poses challenges to the cross-prompt and cross-model\ntransferability of targeted adversarial attack, which aims to confuse the LVLM\nto output a response that is semantically similar to the attacker's chosen\ntarget text. To this end, we propose an instruction-tuned targeted attack\n(dubbed \\textsc{InstructTA}) to deliver the targeted adversarial attack on\nLVLMs with high transferability. Initially, we utilize a public text-to-image\ngenerative model to \"reverse\" the target response into a target image, and\nemploy GPT-4 to infer a reasonable instruction $\\boldsymbol{p}^\\prime$ from the\ntarget response. We then form a local surrogate model (sharing the same vision\nencoder with the victim LVLM) to extract instruction-aware features of an\nadversarial image example and the target image, and minimize the distance\nbetween these two features to optimize the adversarial example. To further\nimprove the transferability with instruction tuning, we augment the instruction\n$\\boldsymbol{p}^\\prime$ with instructions paraphrased from GPT-4. Extensive\nexperiments demonstrate the superiority of our proposed method in targeted\nattack performance and transferability. The code is available at\nhttps://github.com/xunguangwang/InstructTA.\n", "link": "http://arxiv.org/abs/2312.01886v3", "date": "2024-06-26", "relevancy": 2.1072, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.536}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5294}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InstructTA%3A%20Instruction-Tuned%20Targeted%20Attack%20for%20Large%20Vision-Language%0A%20%20Models&body=Title%3A%20InstructTA%3A%20Instruction-Tuned%20Targeted%20Attack%20for%20Large%20Vision-Language%0A%20%20Models%0AAuthor%3A%20Xunguang%20Wang%20and%20Zhenlan%20Ji%20and%20Pingchuan%20Ma%20and%20Zongjie%20Li%20and%20Shuai%20Wang%0AAbstract%3A%20%20%20Large%20vision-language%20models%20%28LVLMs%29%20have%20demonstrated%20their%20incredible%0Acapability%20in%20image%20understanding%20and%20response%20generation.%20However%2C%20this%20rich%0Avisual%20interaction%20also%20makes%20LVLMs%20vulnerable%20to%20adversarial%20examples.%20In%20this%0Apaper%2C%20we%20formulate%20a%20novel%20and%20practical%20targeted%20attack%20scenario%20that%20the%0Aadversary%20can%20only%20know%20the%20vision%20encoder%20of%20the%20victim%20LVLM%2C%20without%20the%0Aknowledge%20of%20its%20prompts%20%28which%20are%20often%20proprietary%20for%20service%20providers%20and%0Anot%20publicly%20available%29%20and%20its%20underlying%20large%20language%20model%20%28LLM%29.%20This%0Apractical%20setting%20poses%20challenges%20to%20the%20cross-prompt%20and%20cross-model%0Atransferability%20of%20targeted%20adversarial%20attack%2C%20which%20aims%20to%20confuse%20the%20LVLM%0Ato%20output%20a%20response%20that%20is%20semantically%20similar%20to%20the%20attacker%27s%20chosen%0Atarget%20text.%20To%20this%20end%2C%20we%20propose%20an%20instruction-tuned%20targeted%20attack%0A%28dubbed%20%5Ctextsc%7BInstructTA%7D%29%20to%20deliver%20the%20targeted%20adversarial%20attack%20on%0ALVLMs%20with%20high%20transferability.%20Initially%2C%20we%20utilize%20a%20public%20text-to-image%0Agenerative%20model%20to%20%22reverse%22%20the%20target%20response%20into%20a%20target%20image%2C%20and%0Aemploy%20GPT-4%20to%20infer%20a%20reasonable%20instruction%20%24%5Cboldsymbol%7Bp%7D%5E%5Cprime%24%20from%20the%0Atarget%20response.%20We%20then%20form%20a%20local%20surrogate%20model%20%28sharing%20the%20same%20vision%0Aencoder%20with%20the%20victim%20LVLM%29%20to%20extract%20instruction-aware%20features%20of%20an%0Aadversarial%20image%20example%20and%20the%20target%20image%2C%20and%20minimize%20the%20distance%0Abetween%20these%20two%20features%20to%20optimize%20the%20adversarial%20example.%20To%20further%0Aimprove%20the%20transferability%20with%20instruction%20tuning%2C%20we%20augment%20the%20instruction%0A%24%5Cboldsymbol%7Bp%7D%5E%5Cprime%24%20with%20instructions%20paraphrased%20from%20GPT-4.%20Extensive%0Aexperiments%20demonstrate%20the%20superiority%20of%20our%20proposed%20method%20in%20targeted%0Aattack%20performance%20and%20transferability.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/xunguangwang/InstructTA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.01886v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstructTA%253A%2520Instruction-Tuned%2520Targeted%2520Attack%2520for%2520Large%2520Vision-Language%250A%2520%2520Models%26entry.906535625%3DXunguang%2520Wang%2520and%2520Zhenlan%2520Ji%2520and%2520Pingchuan%2520Ma%2520and%2520Zongjie%2520Li%2520and%2520Shuai%2520Wang%26entry.1292438233%3D%2520%2520Large%2520vision-language%2520models%2520%2528LVLMs%2529%2520have%2520demonstrated%2520their%2520incredible%250Acapability%2520in%2520image%2520understanding%2520and%2520response%2520generation.%2520However%252C%2520this%2520rich%250Avisual%2520interaction%2520also%2520makes%2520LVLMs%2520vulnerable%2520to%2520adversarial%2520examples.%2520In%2520this%250Apaper%252C%2520we%2520formulate%2520a%2520novel%2520and%2520practical%2520targeted%2520attack%2520scenario%2520that%2520the%250Aadversary%2520can%2520only%2520know%2520the%2520vision%2520encoder%2520of%2520the%2520victim%2520LVLM%252C%2520without%2520the%250Aknowledge%2520of%2520its%2520prompts%2520%2528which%2520are%2520often%2520proprietary%2520for%2520service%2520providers%2520and%250Anot%2520publicly%2520available%2529%2520and%2520its%2520underlying%2520large%2520language%2520model%2520%2528LLM%2529.%2520This%250Apractical%2520setting%2520poses%2520challenges%2520to%2520the%2520cross-prompt%2520and%2520cross-model%250Atransferability%2520of%2520targeted%2520adversarial%2520attack%252C%2520which%2520aims%2520to%2520confuse%2520the%2520LVLM%250Ato%2520output%2520a%2520response%2520that%2520is%2520semantically%2520similar%2520to%2520the%2520attacker%2527s%2520chosen%250Atarget%2520text.%2520To%2520this%2520end%252C%2520we%2520propose%2520an%2520instruction-tuned%2520targeted%2520attack%250A%2528dubbed%2520%255Ctextsc%257BInstructTA%257D%2529%2520to%2520deliver%2520the%2520targeted%2520adversarial%2520attack%2520on%250ALVLMs%2520with%2520high%2520transferability.%2520Initially%252C%2520we%2520utilize%2520a%2520public%2520text-to-image%250Agenerative%2520model%2520to%2520%2522reverse%2522%2520the%2520target%2520response%2520into%2520a%2520target%2520image%252C%2520and%250Aemploy%2520GPT-4%2520to%2520infer%2520a%2520reasonable%2520instruction%2520%2524%255Cboldsymbol%257Bp%257D%255E%255Cprime%2524%2520from%2520the%250Atarget%2520response.%2520We%2520then%2520form%2520a%2520local%2520surrogate%2520model%2520%2528sharing%2520the%2520same%2520vision%250Aencoder%2520with%2520the%2520victim%2520LVLM%2529%2520to%2520extract%2520instruction-aware%2520features%2520of%2520an%250Aadversarial%2520image%2520example%2520and%2520the%2520target%2520image%252C%2520and%2520minimize%2520the%2520distance%250Abetween%2520these%2520two%2520features%2520to%2520optimize%2520the%2520adversarial%2520example.%2520To%2520further%250Aimprove%2520the%2520transferability%2520with%2520instruction%2520tuning%252C%2520we%2520augment%2520the%2520instruction%250A%2524%255Cboldsymbol%257Bp%257D%255E%255Cprime%2524%2520with%2520instructions%2520paraphrased%2520from%2520GPT-4.%2520Extensive%250Aexperiments%2520demonstrate%2520the%2520superiority%2520of%2520our%2520proposed%2520method%2520in%2520targeted%250Aattack%2520performance%2520and%2520transferability.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/xunguangwang/InstructTA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.01886v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InstructTA%3A%20Instruction-Tuned%20Targeted%20Attack%20for%20Large%20Vision-Language%0A%20%20Models&entry.906535625=Xunguang%20Wang%20and%20Zhenlan%20Ji%20and%20Pingchuan%20Ma%20and%20Zongjie%20Li%20and%20Shuai%20Wang&entry.1292438233=%20%20Large%20vision-language%20models%20%28LVLMs%29%20have%20demonstrated%20their%20incredible%0Acapability%20in%20image%20understanding%20and%20response%20generation.%20However%2C%20this%20rich%0Avisual%20interaction%20also%20makes%20LVLMs%20vulnerable%20to%20adversarial%20examples.%20In%20this%0Apaper%2C%20we%20formulate%20a%20novel%20and%20practical%20targeted%20attack%20scenario%20that%20the%0Aadversary%20can%20only%20know%20the%20vision%20encoder%20of%20the%20victim%20LVLM%2C%20without%20the%0Aknowledge%20of%20its%20prompts%20%28which%20are%20often%20proprietary%20for%20service%20providers%20and%0Anot%20publicly%20available%29%20and%20its%20underlying%20large%20language%20model%20%28LLM%29.%20This%0Apractical%20setting%20poses%20challenges%20to%20the%20cross-prompt%20and%20cross-model%0Atransferability%20of%20targeted%20adversarial%20attack%2C%20which%20aims%20to%20confuse%20the%20LVLM%0Ato%20output%20a%20response%20that%20is%20semantically%20similar%20to%20the%20attacker%27s%20chosen%0Atarget%20text.%20To%20this%20end%2C%20we%20propose%20an%20instruction-tuned%20targeted%20attack%0A%28dubbed%20%5Ctextsc%7BInstructTA%7D%29%20to%20deliver%20the%20targeted%20adversarial%20attack%20on%0ALVLMs%20with%20high%20transferability.%20Initially%2C%20we%20utilize%20a%20public%20text-to-image%0Agenerative%20model%20to%20%22reverse%22%20the%20target%20response%20into%20a%20target%20image%2C%20and%0Aemploy%20GPT-4%20to%20infer%20a%20reasonable%20instruction%20%24%5Cboldsymbol%7Bp%7D%5E%5Cprime%24%20from%20the%0Atarget%20response.%20We%20then%20form%20a%20local%20surrogate%20model%20%28sharing%20the%20same%20vision%0Aencoder%20with%20the%20victim%20LVLM%29%20to%20extract%20instruction-aware%20features%20of%20an%0Aadversarial%20image%20example%20and%20the%20target%20image%2C%20and%20minimize%20the%20distance%0Abetween%20these%20two%20features%20to%20optimize%20the%20adversarial%20example.%20To%20further%0Aimprove%20the%20transferability%20with%20instruction%20tuning%2C%20we%20augment%20the%20instruction%0A%24%5Cboldsymbol%7Bp%7D%5E%5Cprime%24%20with%20instructions%20paraphrased%20from%20GPT-4.%20Extensive%0Aexperiments%20demonstrate%20the%20superiority%20of%20our%20proposed%20method%20in%20targeted%0Aattack%20performance%20and%20transferability.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/xunguangwang/InstructTA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.01886v3&entry.124074799=Read"},
{"title": "MT2ST: Adaptive Multi-Task to Single-Task Learning", "author": "Dong Liu and Meng Jiang", "abstract": "  The conventional training approaches often face challenges in balancing the\nbreadth of multi-task learning (MTL) with the depth of single-task learning\n(STL). To address this issue, we introduce the Multi-Task to Single-Task\n(MT2ST) framework, a groundbreaking approach that can combine the\ngeneralizability of MTL with the precision of STL. Our work include two\nstrategies: 'Diminish' and 'Switch'. 'Diminish' Strategy will gradually reduce\nthe influence of auxiliary tasks, while the 'Switch' strategy involves a shift\nfrom multi-tasking to single-tasking at a specific timepoint at the training\nprocess.\n  In this paper, we propose the Multi-Task to Single-Task (MT2ST) framework, a\nnovel approach that significantly enhances the efficiency and accuracy of word\nembedding training while concurrently addressing prevalent issues such as\noverfitting. Our empirical studies demonstrate that MT2ST can reduce training\ntime by 67\\% when contrasted with single-task learning approaches, and by 13\\%\ncompared to traditional multi-task learning methods. These findings underscore\nMT2ST's potential to be a powerful tools for word embedding training\nacceleration.\n", "link": "http://arxiv.org/abs/2406.18038v1", "date": "2024-06-26", "relevancy": 2.103, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5667}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5043}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MT2ST%3A%20Adaptive%20Multi-Task%20to%20Single-Task%20Learning&body=Title%3A%20MT2ST%3A%20Adaptive%20Multi-Task%20to%20Single-Task%20Learning%0AAuthor%3A%20Dong%20Liu%20and%20Meng%20Jiang%0AAbstract%3A%20%20%20The%20conventional%20training%20approaches%20often%20face%20challenges%20in%20balancing%20the%0Abreadth%20of%20multi-task%20learning%20%28MTL%29%20with%20the%20depth%20of%20single-task%20learning%0A%28STL%29.%20To%20address%20this%20issue%2C%20we%20introduce%20the%20Multi-Task%20to%20Single-Task%0A%28MT2ST%29%20framework%2C%20a%20groundbreaking%20approach%20that%20can%20combine%20the%0Ageneralizability%20of%20MTL%20with%20the%20precision%20of%20STL.%20Our%20work%20include%20two%0Astrategies%3A%20%27Diminish%27%20and%20%27Switch%27.%20%27Diminish%27%20Strategy%20will%20gradually%20reduce%0Athe%20influence%20of%20auxiliary%20tasks%2C%20while%20the%20%27Switch%27%20strategy%20involves%20a%20shift%0Afrom%20multi-tasking%20to%20single-tasking%20at%20a%20specific%20timepoint%20at%20the%20training%0Aprocess.%0A%20%20In%20this%20paper%2C%20we%20propose%20the%20Multi-Task%20to%20Single-Task%20%28MT2ST%29%20framework%2C%20a%0Anovel%20approach%20that%20significantly%20enhances%20the%20efficiency%20and%20accuracy%20of%20word%0Aembedding%20training%20while%20concurrently%20addressing%20prevalent%20issues%20such%20as%0Aoverfitting.%20Our%20empirical%20studies%20demonstrate%20that%20MT2ST%20can%20reduce%20training%0Atime%20by%2067%5C%25%20when%20contrasted%20with%20single-task%20learning%20approaches%2C%20and%20by%2013%5C%25%0Acompared%20to%20traditional%20multi-task%20learning%20methods.%20These%20findings%20underscore%0AMT2ST%27s%20potential%20to%20be%20a%20powerful%20tools%20for%20word%20embedding%20training%0Aacceleration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18038v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMT2ST%253A%2520Adaptive%2520Multi-Task%2520to%2520Single-Task%2520Learning%26entry.906535625%3DDong%2520Liu%2520and%2520Meng%2520Jiang%26entry.1292438233%3D%2520%2520The%2520conventional%2520training%2520approaches%2520often%2520face%2520challenges%2520in%2520balancing%2520the%250Abreadth%2520of%2520multi-task%2520learning%2520%2528MTL%2529%2520with%2520the%2520depth%2520of%2520single-task%2520learning%250A%2528STL%2529.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520the%2520Multi-Task%2520to%2520Single-Task%250A%2528MT2ST%2529%2520framework%252C%2520a%2520groundbreaking%2520approach%2520that%2520can%2520combine%2520the%250Ageneralizability%2520of%2520MTL%2520with%2520the%2520precision%2520of%2520STL.%2520Our%2520work%2520include%2520two%250Astrategies%253A%2520%2527Diminish%2527%2520and%2520%2527Switch%2527.%2520%2527Diminish%2527%2520Strategy%2520will%2520gradually%2520reduce%250Athe%2520influence%2520of%2520auxiliary%2520tasks%252C%2520while%2520the%2520%2527Switch%2527%2520strategy%2520involves%2520a%2520shift%250Afrom%2520multi-tasking%2520to%2520single-tasking%2520at%2520a%2520specific%2520timepoint%2520at%2520the%2520training%250Aprocess.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%2520Multi-Task%2520to%2520Single-Task%2520%2528MT2ST%2529%2520framework%252C%2520a%250Anovel%2520approach%2520that%2520significantly%2520enhances%2520the%2520efficiency%2520and%2520accuracy%2520of%2520word%250Aembedding%2520training%2520while%2520concurrently%2520addressing%2520prevalent%2520issues%2520such%2520as%250Aoverfitting.%2520Our%2520empirical%2520studies%2520demonstrate%2520that%2520MT2ST%2520can%2520reduce%2520training%250Atime%2520by%252067%255C%2525%2520when%2520contrasted%2520with%2520single-task%2520learning%2520approaches%252C%2520and%2520by%252013%255C%2525%250Acompared%2520to%2520traditional%2520multi-task%2520learning%2520methods.%2520These%2520findings%2520underscore%250AMT2ST%2527s%2520potential%2520to%2520be%2520a%2520powerful%2520tools%2520for%2520word%2520embedding%2520training%250Aacceleration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18038v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MT2ST%3A%20Adaptive%20Multi-Task%20to%20Single-Task%20Learning&entry.906535625=Dong%20Liu%20and%20Meng%20Jiang&entry.1292438233=%20%20The%20conventional%20training%20approaches%20often%20face%20challenges%20in%20balancing%20the%0Abreadth%20of%20multi-task%20learning%20%28MTL%29%20with%20the%20depth%20of%20single-task%20learning%0A%28STL%29.%20To%20address%20this%20issue%2C%20we%20introduce%20the%20Multi-Task%20to%20Single-Task%0A%28MT2ST%29%20framework%2C%20a%20groundbreaking%20approach%20that%20can%20combine%20the%0Ageneralizability%20of%20MTL%20with%20the%20precision%20of%20STL.%20Our%20work%20include%20two%0Astrategies%3A%20%27Diminish%27%20and%20%27Switch%27.%20%27Diminish%27%20Strategy%20will%20gradually%20reduce%0Athe%20influence%20of%20auxiliary%20tasks%2C%20while%20the%20%27Switch%27%20strategy%20involves%20a%20shift%0Afrom%20multi-tasking%20to%20single-tasking%20at%20a%20specific%20timepoint%20at%20the%20training%0Aprocess.%0A%20%20In%20this%20paper%2C%20we%20propose%20the%20Multi-Task%20to%20Single-Task%20%28MT2ST%29%20framework%2C%20a%0Anovel%20approach%20that%20significantly%20enhances%20the%20efficiency%20and%20accuracy%20of%20word%0Aembedding%20training%20while%20concurrently%20addressing%20prevalent%20issues%20such%20as%0Aoverfitting.%20Our%20empirical%20studies%20demonstrate%20that%20MT2ST%20can%20reduce%20training%0Atime%20by%2067%5C%25%20when%20contrasted%20with%20single-task%20learning%20approaches%2C%20and%20by%2013%5C%25%0Acompared%20to%20traditional%20multi-task%20learning%20methods.%20These%20findings%20underscore%0AMT2ST%27s%20potential%20to%20be%20a%20powerful%20tools%20for%20word%20embedding%20training%0Aacceleration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18038v1&entry.124074799=Read"},
{"title": "Single-Model Attribution of Generative Models Through Final-Layer\n  Inversion", "author": "Mike Laszkiewicz and Jonas Ricker and Johannes Lederer and Asja Fischer", "abstract": "  Recent breakthroughs in generative modeling have sparked interest in\npractical single-model attribution. Such methods predict whether a sample was\ngenerated by a specific generator or not, for instance, to prove intellectual\nproperty theft. However, previous works are either limited to the closed-world\nsetting or require undesirable changes to the generative model. We address\nthese shortcomings by, first, viewing single-model attribution through the lens\nof anomaly detection. Arising from this change of perspective, we propose\nFLIPAD, a new approach for single-model attribution in the open-world setting\nbased on final-layer inversion and anomaly detection. We show that the utilized\nfinal-layer inversion can be reduced to a convex lasso optimization problem,\nmaking our approach theoretically sound and computationally efficient. The\ntheoretical findings are accompanied by an experimental study demonstrating the\neffectiveness of our approach and its flexibility to various domains.\n", "link": "http://arxiv.org/abs/2306.06210v5", "date": "2024-06-26", "relevancy": 2.1, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5335}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5296}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5147}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Single-Model%20Attribution%20of%20Generative%20Models%20Through%20Final-Layer%0A%20%20Inversion&body=Title%3A%20Single-Model%20Attribution%20of%20Generative%20Models%20Through%20Final-Layer%0A%20%20Inversion%0AAuthor%3A%20Mike%20Laszkiewicz%20and%20Jonas%20Ricker%20and%20Johannes%20Lederer%20and%20Asja%20Fischer%0AAbstract%3A%20%20%20Recent%20breakthroughs%20in%20generative%20modeling%20have%20sparked%20interest%20in%0Apractical%20single-model%20attribution.%20Such%20methods%20predict%20whether%20a%20sample%20was%0Agenerated%20by%20a%20specific%20generator%20or%20not%2C%20for%20instance%2C%20to%20prove%20intellectual%0Aproperty%20theft.%20However%2C%20previous%20works%20are%20either%20limited%20to%20the%20closed-world%0Asetting%20or%20require%20undesirable%20changes%20to%20the%20generative%20model.%20We%20address%0Athese%20shortcomings%20by%2C%20first%2C%20viewing%20single-model%20attribution%20through%20the%20lens%0Aof%20anomaly%20detection.%20Arising%20from%20this%20change%20of%20perspective%2C%20we%20propose%0AFLIPAD%2C%20a%20new%20approach%20for%20single-model%20attribution%20in%20the%20open-world%20setting%0Abased%20on%20final-layer%20inversion%20and%20anomaly%20detection.%20We%20show%20that%20the%20utilized%0Afinal-layer%20inversion%20can%20be%20reduced%20to%20a%20convex%20lasso%20optimization%20problem%2C%0Amaking%20our%20approach%20theoretically%20sound%20and%20computationally%20efficient.%20The%0Atheoretical%20findings%20are%20accompanied%20by%20an%20experimental%20study%20demonstrating%20the%0Aeffectiveness%20of%20our%20approach%20and%20its%20flexibility%20to%20various%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.06210v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSingle-Model%2520Attribution%2520of%2520Generative%2520Models%2520Through%2520Final-Layer%250A%2520%2520Inversion%26entry.906535625%3DMike%2520Laszkiewicz%2520and%2520Jonas%2520Ricker%2520and%2520Johannes%2520Lederer%2520and%2520Asja%2520Fischer%26entry.1292438233%3D%2520%2520Recent%2520breakthroughs%2520in%2520generative%2520modeling%2520have%2520sparked%2520interest%2520in%250Apractical%2520single-model%2520attribution.%2520Such%2520methods%2520predict%2520whether%2520a%2520sample%2520was%250Agenerated%2520by%2520a%2520specific%2520generator%2520or%2520not%252C%2520for%2520instance%252C%2520to%2520prove%2520intellectual%250Aproperty%2520theft.%2520However%252C%2520previous%2520works%2520are%2520either%2520limited%2520to%2520the%2520closed-world%250Asetting%2520or%2520require%2520undesirable%2520changes%2520to%2520the%2520generative%2520model.%2520We%2520address%250Athese%2520shortcomings%2520by%252C%2520first%252C%2520viewing%2520single-model%2520attribution%2520through%2520the%2520lens%250Aof%2520anomaly%2520detection.%2520Arising%2520from%2520this%2520change%2520of%2520perspective%252C%2520we%2520propose%250AFLIPAD%252C%2520a%2520new%2520approach%2520for%2520single-model%2520attribution%2520in%2520the%2520open-world%2520setting%250Abased%2520on%2520final-layer%2520inversion%2520and%2520anomaly%2520detection.%2520We%2520show%2520that%2520the%2520utilized%250Afinal-layer%2520inversion%2520can%2520be%2520reduced%2520to%2520a%2520convex%2520lasso%2520optimization%2520problem%252C%250Amaking%2520our%2520approach%2520theoretically%2520sound%2520and%2520computationally%2520efficient.%2520The%250Atheoretical%2520findings%2520are%2520accompanied%2520by%2520an%2520experimental%2520study%2520demonstrating%2520the%250Aeffectiveness%2520of%2520our%2520approach%2520and%2520its%2520flexibility%2520to%2520various%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.06210v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Single-Model%20Attribution%20of%20Generative%20Models%20Through%20Final-Layer%0A%20%20Inversion&entry.906535625=Mike%20Laszkiewicz%20and%20Jonas%20Ricker%20and%20Johannes%20Lederer%20and%20Asja%20Fischer&entry.1292438233=%20%20Recent%20breakthroughs%20in%20generative%20modeling%20have%20sparked%20interest%20in%0Apractical%20single-model%20attribution.%20Such%20methods%20predict%20whether%20a%20sample%20was%0Agenerated%20by%20a%20specific%20generator%20or%20not%2C%20for%20instance%2C%20to%20prove%20intellectual%0Aproperty%20theft.%20However%2C%20previous%20works%20are%20either%20limited%20to%20the%20closed-world%0Asetting%20or%20require%20undesirable%20changes%20to%20the%20generative%20model.%20We%20address%0Athese%20shortcomings%20by%2C%20first%2C%20viewing%20single-model%20attribution%20through%20the%20lens%0Aof%20anomaly%20detection.%20Arising%20from%20this%20change%20of%20perspective%2C%20we%20propose%0AFLIPAD%2C%20a%20new%20approach%20for%20single-model%20attribution%20in%20the%20open-world%20setting%0Abased%20on%20final-layer%20inversion%20and%20anomaly%20detection.%20We%20show%20that%20the%20utilized%0Afinal-layer%20inversion%20can%20be%20reduced%20to%20a%20convex%20lasso%20optimization%20problem%2C%0Amaking%20our%20approach%20theoretically%20sound%20and%20computationally%20efficient.%20The%0Atheoretical%20findings%20are%20accompanied%20by%20an%20experimental%20study%20demonstrating%20the%0Aeffectiveness%20of%20our%20approach%20and%20its%20flexibility%20to%20various%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.06210v5&entry.124074799=Read"},
{"title": "Continuous Sign Language Recognition Using Intra-inter Gloss Attention", "author": "Hossein Ranjbar and Alireza Taheri", "abstract": "  Many continuous sign language recognition (CSLR) studies adopt\ntransformer-based architectures for sequence modeling due to their powerful\ncapacity for capturing global contexts. Nevertheless, vanilla self-attention,\nwhich serves as the core module of the transformer, calculates a weighted\naverage over all time steps; therefore, the local temporal semantics of sign\nvideos may not be fully exploited. In this study, we introduce a novel module\nin sign language recognition studies, called intra-inter gloss attention\nmodule, to leverage the relationships among frames within glosses and the\nsemantic and grammatical dependencies between glosses in the video. In the\nintra-gloss attention module, the video is divided into equally sized chunks\nand a self-attention mechanism is applied within each chunk. This localized\nself-attention significantly reduces complexity and eliminates noise introduced\nby considering non-relative frames. In the inter-gloss attention module, we\nfirst aggregate the chunk-level features within each gloss chunk by average\npooling along the temporal dimension. Subsequently, multi-head self-attention\nis applied to all chunk-level features. Given the non-significance of the\nsigner-environment interaction, we utilize segmentation to remove the\nbackground of the videos. This enables the proposed model to direct its focus\ntoward the signer. Experimental results on the PHOENIX-2014 benchmark dataset\ndemonstrate that our method can effectively extract sign language features in\nan end-to-end manner without any prior knowledge, improve the accuracy of CSLR,\nand achieve the word error rate (WER) of 20.4 on the test set which is a\ncompetitive result compare to the state-of-the-art which uses additional\nsupervisions.\n", "link": "http://arxiv.org/abs/2406.18333v1", "date": "2024-06-26", "relevancy": 2.0914, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5513}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5222}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continuous%20Sign%20Language%20Recognition%20Using%20Intra-inter%20Gloss%20Attention&body=Title%3A%20Continuous%20Sign%20Language%20Recognition%20Using%20Intra-inter%20Gloss%20Attention%0AAuthor%3A%20Hossein%20Ranjbar%20and%20Alireza%20Taheri%0AAbstract%3A%20%20%20Many%20continuous%20sign%20language%20recognition%20%28CSLR%29%20studies%20adopt%0Atransformer-based%20architectures%20for%20sequence%20modeling%20due%20to%20their%20powerful%0Acapacity%20for%20capturing%20global%20contexts.%20Nevertheless%2C%20vanilla%20self-attention%2C%0Awhich%20serves%20as%20the%20core%20module%20of%20the%20transformer%2C%20calculates%20a%20weighted%0Aaverage%20over%20all%20time%20steps%3B%20therefore%2C%20the%20local%20temporal%20semantics%20of%20sign%0Avideos%20may%20not%20be%20fully%20exploited.%20In%20this%20study%2C%20we%20introduce%20a%20novel%20module%0Ain%20sign%20language%20recognition%20studies%2C%20called%20intra-inter%20gloss%20attention%0Amodule%2C%20to%20leverage%20the%20relationships%20among%20frames%20within%20glosses%20and%20the%0Asemantic%20and%20grammatical%20dependencies%20between%20glosses%20in%20the%20video.%20In%20the%0Aintra-gloss%20attention%20module%2C%20the%20video%20is%20divided%20into%20equally%20sized%20chunks%0Aand%20a%20self-attention%20mechanism%20is%20applied%20within%20each%20chunk.%20This%20localized%0Aself-attention%20significantly%20reduces%20complexity%20and%20eliminates%20noise%20introduced%0Aby%20considering%20non-relative%20frames.%20In%20the%20inter-gloss%20attention%20module%2C%20we%0Afirst%20aggregate%20the%20chunk-level%20features%20within%20each%20gloss%20chunk%20by%20average%0Apooling%20along%20the%20temporal%20dimension.%20Subsequently%2C%20multi-head%20self-attention%0Ais%20applied%20to%20all%20chunk-level%20features.%20Given%20the%20non-significance%20of%20the%0Asigner-environment%20interaction%2C%20we%20utilize%20segmentation%20to%20remove%20the%0Abackground%20of%20the%20videos.%20This%20enables%20the%20proposed%20model%20to%20direct%20its%20focus%0Atoward%20the%20signer.%20Experimental%20results%20on%20the%20PHOENIX-2014%20benchmark%20dataset%0Ademonstrate%20that%20our%20method%20can%20effectively%20extract%20sign%20language%20features%20in%0Aan%20end-to-end%20manner%20without%20any%20prior%20knowledge%2C%20improve%20the%20accuracy%20of%20CSLR%2C%0Aand%20achieve%20the%20word%20error%20rate%20%28WER%29%20of%2020.4%20on%20the%20test%20set%20which%20is%20a%0Acompetitive%20result%20compare%20to%20the%20state-of-the-art%20which%20uses%20additional%0Asupervisions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18333v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinuous%2520Sign%2520Language%2520Recognition%2520Using%2520Intra-inter%2520Gloss%2520Attention%26entry.906535625%3DHossein%2520Ranjbar%2520and%2520Alireza%2520Taheri%26entry.1292438233%3D%2520%2520Many%2520continuous%2520sign%2520language%2520recognition%2520%2528CSLR%2529%2520studies%2520adopt%250Atransformer-based%2520architectures%2520for%2520sequence%2520modeling%2520due%2520to%2520their%2520powerful%250Acapacity%2520for%2520capturing%2520global%2520contexts.%2520Nevertheless%252C%2520vanilla%2520self-attention%252C%250Awhich%2520serves%2520as%2520the%2520core%2520module%2520of%2520the%2520transformer%252C%2520calculates%2520a%2520weighted%250Aaverage%2520over%2520all%2520time%2520steps%253B%2520therefore%252C%2520the%2520local%2520temporal%2520semantics%2520of%2520sign%250Avideos%2520may%2520not%2520be%2520fully%2520exploited.%2520In%2520this%2520study%252C%2520we%2520introduce%2520a%2520novel%2520module%250Ain%2520sign%2520language%2520recognition%2520studies%252C%2520called%2520intra-inter%2520gloss%2520attention%250Amodule%252C%2520to%2520leverage%2520the%2520relationships%2520among%2520frames%2520within%2520glosses%2520and%2520the%250Asemantic%2520and%2520grammatical%2520dependencies%2520between%2520glosses%2520in%2520the%2520video.%2520In%2520the%250Aintra-gloss%2520attention%2520module%252C%2520the%2520video%2520is%2520divided%2520into%2520equally%2520sized%2520chunks%250Aand%2520a%2520self-attention%2520mechanism%2520is%2520applied%2520within%2520each%2520chunk.%2520This%2520localized%250Aself-attention%2520significantly%2520reduces%2520complexity%2520and%2520eliminates%2520noise%2520introduced%250Aby%2520considering%2520non-relative%2520frames.%2520In%2520the%2520inter-gloss%2520attention%2520module%252C%2520we%250Afirst%2520aggregate%2520the%2520chunk-level%2520features%2520within%2520each%2520gloss%2520chunk%2520by%2520average%250Apooling%2520along%2520the%2520temporal%2520dimension.%2520Subsequently%252C%2520multi-head%2520self-attention%250Ais%2520applied%2520to%2520all%2520chunk-level%2520features.%2520Given%2520the%2520non-significance%2520of%2520the%250Asigner-environment%2520interaction%252C%2520we%2520utilize%2520segmentation%2520to%2520remove%2520the%250Abackground%2520of%2520the%2520videos.%2520This%2520enables%2520the%2520proposed%2520model%2520to%2520direct%2520its%2520focus%250Atoward%2520the%2520signer.%2520Experimental%2520results%2520on%2520the%2520PHOENIX-2014%2520benchmark%2520dataset%250Ademonstrate%2520that%2520our%2520method%2520can%2520effectively%2520extract%2520sign%2520language%2520features%2520in%250Aan%2520end-to-end%2520manner%2520without%2520any%2520prior%2520knowledge%252C%2520improve%2520the%2520accuracy%2520of%2520CSLR%252C%250Aand%2520achieve%2520the%2520word%2520error%2520rate%2520%2528WER%2529%2520of%252020.4%2520on%2520the%2520test%2520set%2520which%2520is%2520a%250Acompetitive%2520result%2520compare%2520to%2520the%2520state-of-the-art%2520which%2520uses%2520additional%250Asupervisions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18333v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continuous%20Sign%20Language%20Recognition%20Using%20Intra-inter%20Gloss%20Attention&entry.906535625=Hossein%20Ranjbar%20and%20Alireza%20Taheri&entry.1292438233=%20%20Many%20continuous%20sign%20language%20recognition%20%28CSLR%29%20studies%20adopt%0Atransformer-based%20architectures%20for%20sequence%20modeling%20due%20to%20their%20powerful%0Acapacity%20for%20capturing%20global%20contexts.%20Nevertheless%2C%20vanilla%20self-attention%2C%0Awhich%20serves%20as%20the%20core%20module%20of%20the%20transformer%2C%20calculates%20a%20weighted%0Aaverage%20over%20all%20time%20steps%3B%20therefore%2C%20the%20local%20temporal%20semantics%20of%20sign%0Avideos%20may%20not%20be%20fully%20exploited.%20In%20this%20study%2C%20we%20introduce%20a%20novel%20module%0Ain%20sign%20language%20recognition%20studies%2C%20called%20intra-inter%20gloss%20attention%0Amodule%2C%20to%20leverage%20the%20relationships%20among%20frames%20within%20glosses%20and%20the%0Asemantic%20and%20grammatical%20dependencies%20between%20glosses%20in%20the%20video.%20In%20the%0Aintra-gloss%20attention%20module%2C%20the%20video%20is%20divided%20into%20equally%20sized%20chunks%0Aand%20a%20self-attention%20mechanism%20is%20applied%20within%20each%20chunk.%20This%20localized%0Aself-attention%20significantly%20reduces%20complexity%20and%20eliminates%20noise%20introduced%0Aby%20considering%20non-relative%20frames.%20In%20the%20inter-gloss%20attention%20module%2C%20we%0Afirst%20aggregate%20the%20chunk-level%20features%20within%20each%20gloss%20chunk%20by%20average%0Apooling%20along%20the%20temporal%20dimension.%20Subsequently%2C%20multi-head%20self-attention%0Ais%20applied%20to%20all%20chunk-level%20features.%20Given%20the%20non-significance%20of%20the%0Asigner-environment%20interaction%2C%20we%20utilize%20segmentation%20to%20remove%20the%0Abackground%20of%20the%20videos.%20This%20enables%20the%20proposed%20model%20to%20direct%20its%20focus%0Atoward%20the%20signer.%20Experimental%20results%20on%20the%20PHOENIX-2014%20benchmark%20dataset%0Ademonstrate%20that%20our%20method%20can%20effectively%20extract%20sign%20language%20features%20in%0Aan%20end-to-end%20manner%20without%20any%20prior%20knowledge%2C%20improve%20the%20accuracy%20of%20CSLR%2C%0Aand%20achieve%20the%20word%20error%20rate%20%28WER%29%20of%2020.4%20on%20the%20test%20set%20which%20is%20a%0Acompetitive%20result%20compare%20to%20the%20state-of-the-art%20which%20uses%20additional%0Asupervisions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18333v1&entry.124074799=Read"},
{"title": "Single-sample versus case-control sampling scheme for Positive Unlabeled\n  data: the story of two scenarios", "author": "Jan Mielniczuk and Adam Wawrze\u0144czyk", "abstract": "  In the paper we argue that performance of the classifiers based on Empirical\nRisk Minimization (ERM) for positive unlabeled data, which are designed for\ncase-control sampling scheme may significantly deteriorate when applied to a\nsingle-sample scenario. We reveal why their behavior depends, in all but very\nspecific cases, on the scenario. Also, we introduce a single-sample case\nanalogue of the popular non-negative risk classifier designed for case-control\ndata and compare its performance with the original proposal. We show that the\nsignificant differences occur between them, especiall when half or more\npositive of observations are labeled. The opposite case when ERM minimizer\ndesigned for the case-control case is applied for single-sample data is also\nconsidered and similar conclusions are drawn. Taking into account difference of\nscenarios requires a sole, but crucial, change in the definition of the\nEmpirical Risk.\n", "link": "http://arxiv.org/abs/2312.02095v2", "date": "2024-06-26", "relevancy": 1.8695, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4856}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4692}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Single-sample%20versus%20case-control%20sampling%20scheme%20for%20Positive%20Unlabeled%0A%20%20data%3A%20the%20story%20of%20two%20scenarios&body=Title%3A%20Single-sample%20versus%20case-control%20sampling%20scheme%20for%20Positive%20Unlabeled%0A%20%20data%3A%20the%20story%20of%20two%20scenarios%0AAuthor%3A%20Jan%20Mielniczuk%20and%20Adam%20Wawrze%C5%84czyk%0AAbstract%3A%20%20%20In%20the%20paper%20we%20argue%20that%20performance%20of%20the%20classifiers%20based%20on%20Empirical%0ARisk%20Minimization%20%28ERM%29%20for%20positive%20unlabeled%20data%2C%20which%20are%20designed%20for%0Acase-control%20sampling%20scheme%20may%20significantly%20deteriorate%20when%20applied%20to%20a%0Asingle-sample%20scenario.%20We%20reveal%20why%20their%20behavior%20depends%2C%20in%20all%20but%20very%0Aspecific%20cases%2C%20on%20the%20scenario.%20Also%2C%20we%20introduce%20a%20single-sample%20case%0Aanalogue%20of%20the%20popular%20non-negative%20risk%20classifier%20designed%20for%20case-control%0Adata%20and%20compare%20its%20performance%20with%20the%20original%20proposal.%20We%20show%20that%20the%0Asignificant%20differences%20occur%20between%20them%2C%20especiall%20when%20half%20or%20more%0Apositive%20of%20observations%20are%20labeled.%20The%20opposite%20case%20when%20ERM%20minimizer%0Adesigned%20for%20the%20case-control%20case%20is%20applied%20for%20single-sample%20data%20is%20also%0Aconsidered%20and%20similar%20conclusions%20are%20drawn.%20Taking%20into%20account%20difference%20of%0Ascenarios%20requires%20a%20sole%2C%20but%20crucial%2C%20change%20in%20the%20definition%20of%20the%0AEmpirical%20Risk.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02095v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSingle-sample%2520versus%2520case-control%2520sampling%2520scheme%2520for%2520Positive%2520Unlabeled%250A%2520%2520data%253A%2520the%2520story%2520of%2520two%2520scenarios%26entry.906535625%3DJan%2520Mielniczuk%2520and%2520Adam%2520Wawrze%25C5%2584czyk%26entry.1292438233%3D%2520%2520In%2520the%2520paper%2520we%2520argue%2520that%2520performance%2520of%2520the%2520classifiers%2520based%2520on%2520Empirical%250ARisk%2520Minimization%2520%2528ERM%2529%2520for%2520positive%2520unlabeled%2520data%252C%2520which%2520are%2520designed%2520for%250Acase-control%2520sampling%2520scheme%2520may%2520significantly%2520deteriorate%2520when%2520applied%2520to%2520a%250Asingle-sample%2520scenario.%2520We%2520reveal%2520why%2520their%2520behavior%2520depends%252C%2520in%2520all%2520but%2520very%250Aspecific%2520cases%252C%2520on%2520the%2520scenario.%2520Also%252C%2520we%2520introduce%2520a%2520single-sample%2520case%250Aanalogue%2520of%2520the%2520popular%2520non-negative%2520risk%2520classifier%2520designed%2520for%2520case-control%250Adata%2520and%2520compare%2520its%2520performance%2520with%2520the%2520original%2520proposal.%2520We%2520show%2520that%2520the%250Asignificant%2520differences%2520occur%2520between%2520them%252C%2520especiall%2520when%2520half%2520or%2520more%250Apositive%2520of%2520observations%2520are%2520labeled.%2520The%2520opposite%2520case%2520when%2520ERM%2520minimizer%250Adesigned%2520for%2520the%2520case-control%2520case%2520is%2520applied%2520for%2520single-sample%2520data%2520is%2520also%250Aconsidered%2520and%2520similar%2520conclusions%2520are%2520drawn.%2520Taking%2520into%2520account%2520difference%2520of%250Ascenarios%2520requires%2520a%2520sole%252C%2520but%2520crucial%252C%2520change%2520in%2520the%2520definition%2520of%2520the%250AEmpirical%2520Risk.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.02095v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Single-sample%20versus%20case-control%20sampling%20scheme%20for%20Positive%20Unlabeled%0A%20%20data%3A%20the%20story%20of%20two%20scenarios&entry.906535625=Jan%20Mielniczuk%20and%20Adam%20Wawrze%C5%84czyk&entry.1292438233=%20%20In%20the%20paper%20we%20argue%20that%20performance%20of%20the%20classifiers%20based%20on%20Empirical%0ARisk%20Minimization%20%28ERM%29%20for%20positive%20unlabeled%20data%2C%20which%20are%20designed%20for%0Acase-control%20sampling%20scheme%20may%20significantly%20deteriorate%20when%20applied%20to%20a%0Asingle-sample%20scenario.%20We%20reveal%20why%20their%20behavior%20depends%2C%20in%20all%20but%20very%0Aspecific%20cases%2C%20on%20the%20scenario.%20Also%2C%20we%20introduce%20a%20single-sample%20case%0Aanalogue%20of%20the%20popular%20non-negative%20risk%20classifier%20designed%20for%20case-control%0Adata%20and%20compare%20its%20performance%20with%20the%20original%20proposal.%20We%20show%20that%20the%0Asignificant%20differences%20occur%20between%20them%2C%20especiall%20when%20half%20or%20more%0Apositive%20of%20observations%20are%20labeled.%20The%20opposite%20case%20when%20ERM%20minimizer%0Adesigned%20for%20the%20case-control%20case%20is%20applied%20for%20single-sample%20data%20is%20also%0Aconsidered%20and%20similar%20conclusions%20are%20drawn.%20Taking%20into%20account%20difference%20of%0Ascenarios%20requires%20a%20sole%2C%20but%20crucial%2C%20change%20in%20the%20definition%20of%20the%0AEmpirical%20Risk.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02095v2&entry.124074799=Read"},
{"title": "Mixtures of Experts Unlock Parameter Scaling for Deep RL", "author": "Johan Obando-Ceron and Ghada Sokar and Timon Willi and Clare Lyle and Jesse Farebrother and Jakob Foerster and Gintare Karolina Dziugaite and Doina Precup and Pablo Samuel Castro", "abstract": "  The recent rapid progress in (self) supervised learning models is in large\npart predicted by empirical scaling laws: a model's performance scales\nproportionally to its size. Analogous scaling laws remain elusive for\nreinforcement learning domains, however, where increasing the parameter count\nof a model often hurts its final performance. In this paper, we demonstrate\nthat incorporating Mixture-of-Expert (MoE) modules, and in particular Soft MoEs\n(Puigcerver et al., 2023), into value-based networks results in more\nparameter-scalable models, evidenced by substantial performance increases\nacross a variety of training regimes and model sizes. This work thus provides\nstrong empirical evidence towards developing scaling laws for reinforcement\nlearning.\n", "link": "http://arxiv.org/abs/2402.08609v3", "date": "2024-06-26", "relevancy": 1.4151, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4853}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4765}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4643}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mixtures%20of%20Experts%20Unlock%20Parameter%20Scaling%20for%20Deep%20RL&body=Title%3A%20Mixtures%20of%20Experts%20Unlock%20Parameter%20Scaling%20for%20Deep%20RL%0AAuthor%3A%20Johan%20Obando-Ceron%20and%20Ghada%20Sokar%20and%20Timon%20Willi%20and%20Clare%20Lyle%20and%20Jesse%20Farebrother%20and%20Jakob%20Foerster%20and%20Gintare%20Karolina%20Dziugaite%20and%20Doina%20Precup%20and%20Pablo%20Samuel%20Castro%0AAbstract%3A%20%20%20The%20recent%20rapid%20progress%20in%20%28self%29%20supervised%20learning%20models%20is%20in%20large%0Apart%20predicted%20by%20empirical%20scaling%20laws%3A%20a%20model%27s%20performance%20scales%0Aproportionally%20to%20its%20size.%20Analogous%20scaling%20laws%20remain%20elusive%20for%0Areinforcement%20learning%20domains%2C%20however%2C%20where%20increasing%20the%20parameter%20count%0Aof%20a%20model%20often%20hurts%20its%20final%20performance.%20In%20this%20paper%2C%20we%20demonstrate%0Athat%20incorporating%20Mixture-of-Expert%20%28MoE%29%20modules%2C%20and%20in%20particular%20Soft%20MoEs%0A%28Puigcerver%20et%20al.%2C%202023%29%2C%20into%20value-based%20networks%20results%20in%20more%0Aparameter-scalable%20models%2C%20evidenced%20by%20substantial%20performance%20increases%0Aacross%20a%20variety%20of%20training%20regimes%20and%20model%20sizes.%20This%20work%20thus%20provides%0Astrong%20empirical%20evidence%20towards%20developing%20scaling%20laws%20for%20reinforcement%0Alearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.08609v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMixtures%2520of%2520Experts%2520Unlock%2520Parameter%2520Scaling%2520for%2520Deep%2520RL%26entry.906535625%3DJohan%2520Obando-Ceron%2520and%2520Ghada%2520Sokar%2520and%2520Timon%2520Willi%2520and%2520Clare%2520Lyle%2520and%2520Jesse%2520Farebrother%2520and%2520Jakob%2520Foerster%2520and%2520Gintare%2520Karolina%2520Dziugaite%2520and%2520Doina%2520Precup%2520and%2520Pablo%2520Samuel%2520Castro%26entry.1292438233%3D%2520%2520The%2520recent%2520rapid%2520progress%2520in%2520%2528self%2529%2520supervised%2520learning%2520models%2520is%2520in%2520large%250Apart%2520predicted%2520by%2520empirical%2520scaling%2520laws%253A%2520a%2520model%2527s%2520performance%2520scales%250Aproportionally%2520to%2520its%2520size.%2520Analogous%2520scaling%2520laws%2520remain%2520elusive%2520for%250Areinforcement%2520learning%2520domains%252C%2520however%252C%2520where%2520increasing%2520the%2520parameter%2520count%250Aof%2520a%2520model%2520often%2520hurts%2520its%2520final%2520performance.%2520In%2520this%2520paper%252C%2520we%2520demonstrate%250Athat%2520incorporating%2520Mixture-of-Expert%2520%2528MoE%2529%2520modules%252C%2520and%2520in%2520particular%2520Soft%2520MoEs%250A%2528Puigcerver%2520et%2520al.%252C%25202023%2529%252C%2520into%2520value-based%2520networks%2520results%2520in%2520more%250Aparameter-scalable%2520models%252C%2520evidenced%2520by%2520substantial%2520performance%2520increases%250Aacross%2520a%2520variety%2520of%2520training%2520regimes%2520and%2520model%2520sizes.%2520This%2520work%2520thus%2520provides%250Astrong%2520empirical%2520evidence%2520towards%2520developing%2520scaling%2520laws%2520for%2520reinforcement%250Alearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.08609v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mixtures%20of%20Experts%20Unlock%20Parameter%20Scaling%20for%20Deep%20RL&entry.906535625=Johan%20Obando-Ceron%20and%20Ghada%20Sokar%20and%20Timon%20Willi%20and%20Clare%20Lyle%20and%20Jesse%20Farebrother%20and%20Jakob%20Foerster%20and%20Gintare%20Karolina%20Dziugaite%20and%20Doina%20Precup%20and%20Pablo%20Samuel%20Castro&entry.1292438233=%20%20The%20recent%20rapid%20progress%20in%20%28self%29%20supervised%20learning%20models%20is%20in%20large%0Apart%20predicted%20by%20empirical%20scaling%20laws%3A%20a%20model%27s%20performance%20scales%0Aproportionally%20to%20its%20size.%20Analogous%20scaling%20laws%20remain%20elusive%20for%0Areinforcement%20learning%20domains%2C%20however%2C%20where%20increasing%20the%20parameter%20count%0Aof%20a%20model%20often%20hurts%20its%20final%20performance.%20In%20this%20paper%2C%20we%20demonstrate%0Athat%20incorporating%20Mixture-of-Expert%20%28MoE%29%20modules%2C%20and%20in%20particular%20Soft%20MoEs%0A%28Puigcerver%20et%20al.%2C%202023%29%2C%20into%20value-based%20networks%20results%20in%20more%0Aparameter-scalable%20models%2C%20evidenced%20by%20substantial%20performance%20increases%0Aacross%20a%20variety%20of%20training%20regimes%20and%20model%20sizes.%20This%20work%20thus%20provides%0Astrong%20empirical%20evidence%20towards%20developing%20scaling%20laws%20for%20reinforcement%0Alearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.08609v3&entry.124074799=Read"},
{"title": "Enhancing Geometric Ontology Embeddings for $\\mathcal{EL}^{++}$ with\n  Negative Sampling and Deductive Closure Filtering", "author": "Olga Mashkova and Fernando Zhapa-Camacho and Robert Hoehndorf", "abstract": "  Ontology embeddings map classes, relations, and individuals in ontologies\ninto $\\mathbb{R}^n$, and within $\\mathbb{R}^n$ similarity between entities can\nbe computed or new axioms inferred. For ontologies in the Description Logic\n$\\mathcal{EL}^{++}$, several embedding methods have been developed that\nexplicitly generate models of an ontology. However, these methods suffer from\nsome limitations; they do not distinguish between statements that are\nunprovable and provably false, and therefore they may use entailed statements\nas negatives. Furthermore, they do not utilize the deductive closure of an\nontology to identify statements that are inferred but not asserted. We\nevaluated a set of embedding methods for $\\mathcal{EL}^{++}$ ontologies based\non high-dimensional ball representation of concept descriptions, incorporating\nseveral modifications that aim to make use of the ontology deductive closure.\nIn particular, we designed novel negative losses that account both for the\ndeductive closure and different types of negatives. We demonstrate that our\nembedding methods improve over the baseline ontology embedding in the task of\nknowledge base or ontology completion.\n", "link": "http://arxiv.org/abs/2405.04868v2", "date": "2024-06-26", "relevancy": 1.4197, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4998}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4656}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Geometric%20Ontology%20Embeddings%20for%20%24%5Cmathcal%7BEL%7D%5E%7B%2B%2B%7D%24%20with%0A%20%20Negative%20Sampling%20and%20Deductive%20Closure%20Filtering&body=Title%3A%20Enhancing%20Geometric%20Ontology%20Embeddings%20for%20%24%5Cmathcal%7BEL%7D%5E%7B%2B%2B%7D%24%20with%0A%20%20Negative%20Sampling%20and%20Deductive%20Closure%20Filtering%0AAuthor%3A%20Olga%20Mashkova%20and%20Fernando%20Zhapa-Camacho%20and%20Robert%20Hoehndorf%0AAbstract%3A%20%20%20Ontology%20embeddings%20map%20classes%2C%20relations%2C%20and%20individuals%20in%20ontologies%0Ainto%20%24%5Cmathbb%7BR%7D%5En%24%2C%20and%20within%20%24%5Cmathbb%7BR%7D%5En%24%20similarity%20between%20entities%20can%0Abe%20computed%20or%20new%20axioms%20inferred.%20For%20ontologies%20in%20the%20Description%20Logic%0A%24%5Cmathcal%7BEL%7D%5E%7B%2B%2B%7D%24%2C%20several%20embedding%20methods%20have%20been%20developed%20that%0Aexplicitly%20generate%20models%20of%20an%20ontology.%20However%2C%20these%20methods%20suffer%20from%0Asome%20limitations%3B%20they%20do%20not%20distinguish%20between%20statements%20that%20are%0Aunprovable%20and%20provably%20false%2C%20and%20therefore%20they%20may%20use%20entailed%20statements%0Aas%20negatives.%20Furthermore%2C%20they%20do%20not%20utilize%20the%20deductive%20closure%20of%20an%0Aontology%20to%20identify%20statements%20that%20are%20inferred%20but%20not%20asserted.%20We%0Aevaluated%20a%20set%20of%20embedding%20methods%20for%20%24%5Cmathcal%7BEL%7D%5E%7B%2B%2B%7D%24%20ontologies%20based%0Aon%20high-dimensional%20ball%20representation%20of%20concept%20descriptions%2C%20incorporating%0Aseveral%20modifications%20that%20aim%20to%20make%20use%20of%20the%20ontology%20deductive%20closure.%0AIn%20particular%2C%20we%20designed%20novel%20negative%20losses%20that%20account%20both%20for%20the%0Adeductive%20closure%20and%20different%20types%20of%20negatives.%20We%20demonstrate%20that%20our%0Aembedding%20methods%20improve%20over%20the%20baseline%20ontology%20embedding%20in%20the%20task%20of%0Aknowledge%20base%20or%20ontology%20completion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04868v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Geometric%2520Ontology%2520Embeddings%2520for%2520%2524%255Cmathcal%257BEL%257D%255E%257B%252B%252B%257D%2524%2520with%250A%2520%2520Negative%2520Sampling%2520and%2520Deductive%2520Closure%2520Filtering%26entry.906535625%3DOlga%2520Mashkova%2520and%2520Fernando%2520Zhapa-Camacho%2520and%2520Robert%2520Hoehndorf%26entry.1292438233%3D%2520%2520Ontology%2520embeddings%2520map%2520classes%252C%2520relations%252C%2520and%2520individuals%2520in%2520ontologies%250Ainto%2520%2524%255Cmathbb%257BR%257D%255En%2524%252C%2520and%2520within%2520%2524%255Cmathbb%257BR%257D%255En%2524%2520similarity%2520between%2520entities%2520can%250Abe%2520computed%2520or%2520new%2520axioms%2520inferred.%2520For%2520ontologies%2520in%2520the%2520Description%2520Logic%250A%2524%255Cmathcal%257BEL%257D%255E%257B%252B%252B%257D%2524%252C%2520several%2520embedding%2520methods%2520have%2520been%2520developed%2520that%250Aexplicitly%2520generate%2520models%2520of%2520an%2520ontology.%2520However%252C%2520these%2520methods%2520suffer%2520from%250Asome%2520limitations%253B%2520they%2520do%2520not%2520distinguish%2520between%2520statements%2520that%2520are%250Aunprovable%2520and%2520provably%2520false%252C%2520and%2520therefore%2520they%2520may%2520use%2520entailed%2520statements%250Aas%2520negatives.%2520Furthermore%252C%2520they%2520do%2520not%2520utilize%2520the%2520deductive%2520closure%2520of%2520an%250Aontology%2520to%2520identify%2520statements%2520that%2520are%2520inferred%2520but%2520not%2520asserted.%2520We%250Aevaluated%2520a%2520set%2520of%2520embedding%2520methods%2520for%2520%2524%255Cmathcal%257BEL%257D%255E%257B%252B%252B%257D%2524%2520ontologies%2520based%250Aon%2520high-dimensional%2520ball%2520representation%2520of%2520concept%2520descriptions%252C%2520incorporating%250Aseveral%2520modifications%2520that%2520aim%2520to%2520make%2520use%2520of%2520the%2520ontology%2520deductive%2520closure.%250AIn%2520particular%252C%2520we%2520designed%2520novel%2520negative%2520losses%2520that%2520account%2520both%2520for%2520the%250Adeductive%2520closure%2520and%2520different%2520types%2520of%2520negatives.%2520We%2520demonstrate%2520that%2520our%250Aembedding%2520methods%2520improve%2520over%2520the%2520baseline%2520ontology%2520embedding%2520in%2520the%2520task%2520of%250Aknowledge%2520base%2520or%2520ontology%2520completion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04868v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Geometric%20Ontology%20Embeddings%20for%20%24%5Cmathcal%7BEL%7D%5E%7B%2B%2B%7D%24%20with%0A%20%20Negative%20Sampling%20and%20Deductive%20Closure%20Filtering&entry.906535625=Olga%20Mashkova%20and%20Fernando%20Zhapa-Camacho%20and%20Robert%20Hoehndorf&entry.1292438233=%20%20Ontology%20embeddings%20map%20classes%2C%20relations%2C%20and%20individuals%20in%20ontologies%0Ainto%20%24%5Cmathbb%7BR%7D%5En%24%2C%20and%20within%20%24%5Cmathbb%7BR%7D%5En%24%20similarity%20between%20entities%20can%0Abe%20computed%20or%20new%20axioms%20inferred.%20For%20ontologies%20in%20the%20Description%20Logic%0A%24%5Cmathcal%7BEL%7D%5E%7B%2B%2B%7D%24%2C%20several%20embedding%20methods%20have%20been%20developed%20that%0Aexplicitly%20generate%20models%20of%20an%20ontology.%20However%2C%20these%20methods%20suffer%20from%0Asome%20limitations%3B%20they%20do%20not%20distinguish%20between%20statements%20that%20are%0Aunprovable%20and%20provably%20false%2C%20and%20therefore%20they%20may%20use%20entailed%20statements%0Aas%20negatives.%20Furthermore%2C%20they%20do%20not%20utilize%20the%20deductive%20closure%20of%20an%0Aontology%20to%20identify%20statements%20that%20are%20inferred%20but%20not%20asserted.%20We%0Aevaluated%20a%20set%20of%20embedding%20methods%20for%20%24%5Cmathcal%7BEL%7D%5E%7B%2B%2B%7D%24%20ontologies%20based%0Aon%20high-dimensional%20ball%20representation%20of%20concept%20descriptions%2C%20incorporating%0Aseveral%20modifications%20that%20aim%20to%20make%20use%20of%20the%20ontology%20deductive%20closure.%0AIn%20particular%2C%20we%20designed%20novel%20negative%20losses%20that%20account%20both%20for%20the%0Adeductive%20closure%20and%20different%20types%20of%20negatives.%20We%20demonstrate%20that%20our%0Aembedding%20methods%20improve%20over%20the%20baseline%20ontology%20embedding%20in%20the%20task%20of%0Aknowledge%20base%20or%20ontology%20completion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04868v2&entry.124074799=Read"},
{"title": "Active Preference Inference using Language Models and Probabilistic\n  Reasoning", "author": "Wasu Top Piriyakulkij and Volodymyr Kuleshov and Kevin Ellis", "abstract": "  Actively inferring user preferences, for example by asking good questions, is\nimportant for any human-facing decision-making system. Active inference allows\nsuch systems to adapt and personalize themselves to nuanced individual\npreferences. To enable this ability for instruction-tuned large language models\n(LLMs), one may prompt them to ask users questions to infer their preferences,\ntransforming the language models into more robust, interactive systems.\nHowever, out of the box, these models are not efficient at extracting\npreferences: the questions they generate are not informative, requiring a high\nnumber of user interactions and impeding the usability of the downstream\nsystem. In this work, we introduce an inference-time algorithm that helps LLMs\nquickly infer preferences by using more informative questions. Our algorithm\nuses a probabilistic model whose conditional distributions are defined by\nprompting an LLM, and returns questions that optimize expected entropy and\nexpected model change. Results in a simplified interactive web shopping setting\nwith real product items show that an LLM equipped with our entropy reduction\nalgorithm outperforms baselines with the same underlying LLM on task\nperformance while using fewer user interactions.\n", "link": "http://arxiv.org/abs/2312.12009v2", "date": "2024-06-26", "relevancy": 1.4838, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.554}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4797}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Preference%20Inference%20using%20Language%20Models%20and%20Probabilistic%0A%20%20Reasoning&body=Title%3A%20Active%20Preference%20Inference%20using%20Language%20Models%20and%20Probabilistic%0A%20%20Reasoning%0AAuthor%3A%20Wasu%20Top%20Piriyakulkij%20and%20Volodymyr%20Kuleshov%20and%20Kevin%20Ellis%0AAbstract%3A%20%20%20Actively%20inferring%20user%20preferences%2C%20for%20example%20by%20asking%20good%20questions%2C%20is%0Aimportant%20for%20any%20human-facing%20decision-making%20system.%20Active%20inference%20allows%0Asuch%20systems%20to%20adapt%20and%20personalize%20themselves%20to%20nuanced%20individual%0Apreferences.%20To%20enable%20this%20ability%20for%20instruction-tuned%20large%20language%20models%0A%28LLMs%29%2C%20one%20may%20prompt%20them%20to%20ask%20users%20questions%20to%20infer%20their%20preferences%2C%0Atransforming%20the%20language%20models%20into%20more%20robust%2C%20interactive%20systems.%0AHowever%2C%20out%20of%20the%20box%2C%20these%20models%20are%20not%20efficient%20at%20extracting%0Apreferences%3A%20the%20questions%20they%20generate%20are%20not%20informative%2C%20requiring%20a%20high%0Anumber%20of%20user%20interactions%20and%20impeding%20the%20usability%20of%20the%20downstream%0Asystem.%20In%20this%20work%2C%20we%20introduce%20an%20inference-time%20algorithm%20that%20helps%20LLMs%0Aquickly%20infer%20preferences%20by%20using%20more%20informative%20questions.%20Our%20algorithm%0Auses%20a%20probabilistic%20model%20whose%20conditional%20distributions%20are%20defined%20by%0Aprompting%20an%20LLM%2C%20and%20returns%20questions%20that%20optimize%20expected%20entropy%20and%0Aexpected%20model%20change.%20Results%20in%20a%20simplified%20interactive%20web%20shopping%20setting%0Awith%20real%20product%20items%20show%20that%20an%20LLM%20equipped%20with%20our%20entropy%20reduction%0Aalgorithm%20outperforms%20baselines%20with%20the%20same%20underlying%20LLM%20on%20task%0Aperformance%20while%20using%20fewer%20user%20interactions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.12009v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Preference%2520Inference%2520using%2520Language%2520Models%2520and%2520Probabilistic%250A%2520%2520Reasoning%26entry.906535625%3DWasu%2520Top%2520Piriyakulkij%2520and%2520Volodymyr%2520Kuleshov%2520and%2520Kevin%2520Ellis%26entry.1292438233%3D%2520%2520Actively%2520inferring%2520user%2520preferences%252C%2520for%2520example%2520by%2520asking%2520good%2520questions%252C%2520is%250Aimportant%2520for%2520any%2520human-facing%2520decision-making%2520system.%2520Active%2520inference%2520allows%250Asuch%2520systems%2520to%2520adapt%2520and%2520personalize%2520themselves%2520to%2520nuanced%2520individual%250Apreferences.%2520To%2520enable%2520this%2520ability%2520for%2520instruction-tuned%2520large%2520language%2520models%250A%2528LLMs%2529%252C%2520one%2520may%2520prompt%2520them%2520to%2520ask%2520users%2520questions%2520to%2520infer%2520their%2520preferences%252C%250Atransforming%2520the%2520language%2520models%2520into%2520more%2520robust%252C%2520interactive%2520systems.%250AHowever%252C%2520out%2520of%2520the%2520box%252C%2520these%2520models%2520are%2520not%2520efficient%2520at%2520extracting%250Apreferences%253A%2520the%2520questions%2520they%2520generate%2520are%2520not%2520informative%252C%2520requiring%2520a%2520high%250Anumber%2520of%2520user%2520interactions%2520and%2520impeding%2520the%2520usability%2520of%2520the%2520downstream%250Asystem.%2520In%2520this%2520work%252C%2520we%2520introduce%2520an%2520inference-time%2520algorithm%2520that%2520helps%2520LLMs%250Aquickly%2520infer%2520preferences%2520by%2520using%2520more%2520informative%2520questions.%2520Our%2520algorithm%250Auses%2520a%2520probabilistic%2520model%2520whose%2520conditional%2520distributions%2520are%2520defined%2520by%250Aprompting%2520an%2520LLM%252C%2520and%2520returns%2520questions%2520that%2520optimize%2520expected%2520entropy%2520and%250Aexpected%2520model%2520change.%2520Results%2520in%2520a%2520simplified%2520interactive%2520web%2520shopping%2520setting%250Awith%2520real%2520product%2520items%2520show%2520that%2520an%2520LLM%2520equipped%2520with%2520our%2520entropy%2520reduction%250Aalgorithm%2520outperforms%2520baselines%2520with%2520the%2520same%2520underlying%2520LLM%2520on%2520task%250Aperformance%2520while%2520using%2520fewer%2520user%2520interactions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.12009v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Preference%20Inference%20using%20Language%20Models%20and%20Probabilistic%0A%20%20Reasoning&entry.906535625=Wasu%20Top%20Piriyakulkij%20and%20Volodymyr%20Kuleshov%20and%20Kevin%20Ellis&entry.1292438233=%20%20Actively%20inferring%20user%20preferences%2C%20for%20example%20by%20asking%20good%20questions%2C%20is%0Aimportant%20for%20any%20human-facing%20decision-making%20system.%20Active%20inference%20allows%0Asuch%20systems%20to%20adapt%20and%20personalize%20themselves%20to%20nuanced%20individual%0Apreferences.%20To%20enable%20this%20ability%20for%20instruction-tuned%20large%20language%20models%0A%28LLMs%29%2C%20one%20may%20prompt%20them%20to%20ask%20users%20questions%20to%20infer%20their%20preferences%2C%0Atransforming%20the%20language%20models%20into%20more%20robust%2C%20interactive%20systems.%0AHowever%2C%20out%20of%20the%20box%2C%20these%20models%20are%20not%20efficient%20at%20extracting%0Apreferences%3A%20the%20questions%20they%20generate%20are%20not%20informative%2C%20requiring%20a%20high%0Anumber%20of%20user%20interactions%20and%20impeding%20the%20usability%20of%20the%20downstream%0Asystem.%20In%20this%20work%2C%20we%20introduce%20an%20inference-time%20algorithm%20that%20helps%20LLMs%0Aquickly%20infer%20preferences%20by%20using%20more%20informative%20questions.%20Our%20algorithm%0Auses%20a%20probabilistic%20model%20whose%20conditional%20distributions%20are%20defined%20by%0Aprompting%20an%20LLM%2C%20and%20returns%20questions%20that%20optimize%20expected%20entropy%20and%0Aexpected%20model%20change.%20Results%20in%20a%20simplified%20interactive%20web%20shopping%20setting%0Awith%20real%20product%20items%20show%20that%20an%20LLM%20equipped%20with%20our%20entropy%20reduction%0Aalgorithm%20outperforms%20baselines%20with%20the%20same%20underlying%20LLM%20on%20task%0Aperformance%20while%20using%20fewer%20user%20interactions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.12009v2&entry.124074799=Read"},
{"title": "ECGrecover: a Deep Learning Approach for Electrocardiogram Signal\n  Completion", "author": "Alex Lence and Ahmad Fall and Federica Granese and Blaise Hanczar and Joe-Elie Salem and Jean-Daniel Zucker and Edi Prifti", "abstract": "  In this work, we address the challenge of reconstructing the complete 12-lead\nECG signal from incomplete parts of it. We focus on two main scenarii: (i)\nreconstructing missing signal segments within an ECG lead and (ii) recovering\nmissing leads from a single-lead. We propose a model with a U-Net architecture\ntrained on a novel objective function to address the reconstruction problem.\nThis function incorporates both spatial and temporal aspects of the ECG by\ncombining the distance in amplitude between the reconstructed and real signals\nwith the signal trend. Through comprehensive assessments using both a real-life\ndataset and a publicly accessible one, we demonstrate that the proposed\napproach consistently outperforms state-of-the-art methods based on generative\nadversarial networks and a CopyPaste strategy. Our proposed model demonstrates\nsuperior performance in standard distortion metrics and preserves critical ECG\ncharacteristics, particularly the P, Q, R, S, and T wave coordinates. Two\nemerging clinical applications emphasize the relevance of our work. The first\nis the increasing need to digitize paper-stored ECGs for utilization in\nAI-based applications (automatic annotation and risk-quantification), often\nlimited to digital ECG complete 10s recordings. The second is the widespread\nuse of wearable devices that record ECGs but typically capture only a small\nsubset of the 12 standard leads. In both cases, a non-negligible amount of\ninformation is lost or not recorded, which our approach aims to recover to\novercome these limitations.\n", "link": "http://arxiv.org/abs/2406.16901v2", "date": "2024-06-26", "relevancy": 1.9624, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4948}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.491}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4862}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ECGrecover%3A%20a%20Deep%20Learning%20Approach%20for%20Electrocardiogram%20Signal%0A%20%20Completion&body=Title%3A%20ECGrecover%3A%20a%20Deep%20Learning%20Approach%20for%20Electrocardiogram%20Signal%0A%20%20Completion%0AAuthor%3A%20Alex%20Lence%20and%20Ahmad%20Fall%20and%20Federica%20Granese%20and%20Blaise%20Hanczar%20and%20Joe-Elie%20Salem%20and%20Jean-Daniel%20Zucker%20and%20Edi%20Prifti%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20address%20the%20challenge%20of%20reconstructing%20the%20complete%2012-lead%0AECG%20signal%20from%20incomplete%20parts%20of%20it.%20We%20focus%20on%20two%20main%20scenarii%3A%20%28i%29%0Areconstructing%20missing%20signal%20segments%20within%20an%20ECG%20lead%20and%20%28ii%29%20recovering%0Amissing%20leads%20from%20a%20single-lead.%20We%20propose%20a%20model%20with%20a%20U-Net%20architecture%0Atrained%20on%20a%20novel%20objective%20function%20to%20address%20the%20reconstruction%20problem.%0AThis%20function%20incorporates%20both%20spatial%20and%20temporal%20aspects%20of%20the%20ECG%20by%0Acombining%20the%20distance%20in%20amplitude%20between%20the%20reconstructed%20and%20real%20signals%0Awith%20the%20signal%20trend.%20Through%20comprehensive%20assessments%20using%20both%20a%20real-life%0Adataset%20and%20a%20publicly%20accessible%20one%2C%20we%20demonstrate%20that%20the%20proposed%0Aapproach%20consistently%20outperforms%20state-of-the-art%20methods%20based%20on%20generative%0Aadversarial%20networks%20and%20a%20CopyPaste%20strategy.%20Our%20proposed%20model%20demonstrates%0Asuperior%20performance%20in%20standard%20distortion%20metrics%20and%20preserves%20critical%20ECG%0Acharacteristics%2C%20particularly%20the%20P%2C%20Q%2C%20R%2C%20S%2C%20and%20T%20wave%20coordinates.%20Two%0Aemerging%20clinical%20applications%20emphasize%20the%20relevance%20of%20our%20work.%20The%20first%0Ais%20the%20increasing%20need%20to%20digitize%20paper-stored%20ECGs%20for%20utilization%20in%0AAI-based%20applications%20%28automatic%20annotation%20and%20risk-quantification%29%2C%20often%0Alimited%20to%20digital%20ECG%20complete%2010s%20recordings.%20The%20second%20is%20the%20widespread%0Ause%20of%20wearable%20devices%20that%20record%20ECGs%20but%20typically%20capture%20only%20a%20small%0Asubset%20of%20the%2012%20standard%20leads.%20In%20both%20cases%2C%20a%20non-negligible%20amount%20of%0Ainformation%20is%20lost%20or%20not%20recorded%2C%20which%20our%20approach%20aims%20to%20recover%20to%0Aovercome%20these%20limitations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16901v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DECGrecover%253A%2520a%2520Deep%2520Learning%2520Approach%2520for%2520Electrocardiogram%2520Signal%250A%2520%2520Completion%26entry.906535625%3DAlex%2520Lence%2520and%2520Ahmad%2520Fall%2520and%2520Federica%2520Granese%2520and%2520Blaise%2520Hanczar%2520and%2520Joe-Elie%2520Salem%2520and%2520Jean-Daniel%2520Zucker%2520and%2520Edi%2520Prifti%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520address%2520the%2520challenge%2520of%2520reconstructing%2520the%2520complete%252012-lead%250AECG%2520signal%2520from%2520incomplete%2520parts%2520of%2520it.%2520We%2520focus%2520on%2520two%2520main%2520scenarii%253A%2520%2528i%2529%250Areconstructing%2520missing%2520signal%2520segments%2520within%2520an%2520ECG%2520lead%2520and%2520%2528ii%2529%2520recovering%250Amissing%2520leads%2520from%2520a%2520single-lead.%2520We%2520propose%2520a%2520model%2520with%2520a%2520U-Net%2520architecture%250Atrained%2520on%2520a%2520novel%2520objective%2520function%2520to%2520address%2520the%2520reconstruction%2520problem.%250AThis%2520function%2520incorporates%2520both%2520spatial%2520and%2520temporal%2520aspects%2520of%2520the%2520ECG%2520by%250Acombining%2520the%2520distance%2520in%2520amplitude%2520between%2520the%2520reconstructed%2520and%2520real%2520signals%250Awith%2520the%2520signal%2520trend.%2520Through%2520comprehensive%2520assessments%2520using%2520both%2520a%2520real-life%250Adataset%2520and%2520a%2520publicly%2520accessible%2520one%252C%2520we%2520demonstrate%2520that%2520the%2520proposed%250Aapproach%2520consistently%2520outperforms%2520state-of-the-art%2520methods%2520based%2520on%2520generative%250Aadversarial%2520networks%2520and%2520a%2520CopyPaste%2520strategy.%2520Our%2520proposed%2520model%2520demonstrates%250Asuperior%2520performance%2520in%2520standard%2520distortion%2520metrics%2520and%2520preserves%2520critical%2520ECG%250Acharacteristics%252C%2520particularly%2520the%2520P%252C%2520Q%252C%2520R%252C%2520S%252C%2520and%2520T%2520wave%2520coordinates.%2520Two%250Aemerging%2520clinical%2520applications%2520emphasize%2520the%2520relevance%2520of%2520our%2520work.%2520The%2520first%250Ais%2520the%2520increasing%2520need%2520to%2520digitize%2520paper-stored%2520ECGs%2520for%2520utilization%2520in%250AAI-based%2520applications%2520%2528automatic%2520annotation%2520and%2520risk-quantification%2529%252C%2520often%250Alimited%2520to%2520digital%2520ECG%2520complete%252010s%2520recordings.%2520The%2520second%2520is%2520the%2520widespread%250Ause%2520of%2520wearable%2520devices%2520that%2520record%2520ECGs%2520but%2520typically%2520capture%2520only%2520a%2520small%250Asubset%2520of%2520the%252012%2520standard%2520leads.%2520In%2520both%2520cases%252C%2520a%2520non-negligible%2520amount%2520of%250Ainformation%2520is%2520lost%2520or%2520not%2520recorded%252C%2520which%2520our%2520approach%2520aims%2520to%2520recover%2520to%250Aovercome%2520these%2520limitations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16901v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ECGrecover%3A%20a%20Deep%20Learning%20Approach%20for%20Electrocardiogram%20Signal%0A%20%20Completion&entry.906535625=Alex%20Lence%20and%20Ahmad%20Fall%20and%20Federica%20Granese%20and%20Blaise%20Hanczar%20and%20Joe-Elie%20Salem%20and%20Jean-Daniel%20Zucker%20and%20Edi%20Prifti&entry.1292438233=%20%20In%20this%20work%2C%20we%20address%20the%20challenge%20of%20reconstructing%20the%20complete%2012-lead%0AECG%20signal%20from%20incomplete%20parts%20of%20it.%20We%20focus%20on%20two%20main%20scenarii%3A%20%28i%29%0Areconstructing%20missing%20signal%20segments%20within%20an%20ECG%20lead%20and%20%28ii%29%20recovering%0Amissing%20leads%20from%20a%20single-lead.%20We%20propose%20a%20model%20with%20a%20U-Net%20architecture%0Atrained%20on%20a%20novel%20objective%20function%20to%20address%20the%20reconstruction%20problem.%0AThis%20function%20incorporates%20both%20spatial%20and%20temporal%20aspects%20of%20the%20ECG%20by%0Acombining%20the%20distance%20in%20amplitude%20between%20the%20reconstructed%20and%20real%20signals%0Awith%20the%20signal%20trend.%20Through%20comprehensive%20assessments%20using%20both%20a%20real-life%0Adataset%20and%20a%20publicly%20accessible%20one%2C%20we%20demonstrate%20that%20the%20proposed%0Aapproach%20consistently%20outperforms%20state-of-the-art%20methods%20based%20on%20generative%0Aadversarial%20networks%20and%20a%20CopyPaste%20strategy.%20Our%20proposed%20model%20demonstrates%0Asuperior%20performance%20in%20standard%20distortion%20metrics%20and%20preserves%20critical%20ECG%0Acharacteristics%2C%20particularly%20the%20P%2C%20Q%2C%20R%2C%20S%2C%20and%20T%20wave%20coordinates.%20Two%0Aemerging%20clinical%20applications%20emphasize%20the%20relevance%20of%20our%20work.%20The%20first%0Ais%20the%20increasing%20need%20to%20digitize%20paper-stored%20ECGs%20for%20utilization%20in%0AAI-based%20applications%20%28automatic%20annotation%20and%20risk-quantification%29%2C%20often%0Alimited%20to%20digital%20ECG%20complete%2010s%20recordings.%20The%20second%20is%20the%20widespread%0Ause%20of%20wearable%20devices%20that%20record%20ECGs%20but%20typically%20capture%20only%20a%20small%0Asubset%20of%20the%2012%20standard%20leads.%20In%20both%20cases%2C%20a%20non-negligible%20amount%20of%0Ainformation%20is%20lost%20or%20not%20recorded%2C%20which%20our%20approach%20aims%20to%20recover%20to%0Aovercome%20these%20limitations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16901v2&entry.124074799=Read"},
{"title": "Layer-Wise Quantization: A Pragmatic and Effective Method for Quantizing\n  LLMs Beyond Integer Bit-Levels", "author": "Razvan-Gabriel Dumitru and Vikas Yadav and Rishabh Maheshwary and Paul-Ioan Clotan and Sathwik Tejaswi Madhusudhan and Mihai Surdeanu", "abstract": "  We present a simple variable quantization approach that quantizes different\nlayers of a large language model (LLM) at different bit levels. Specifically,\nwe quantize the most important layers to higher bit precision and less\nimportant layers to lower bits to achieve floating point quantization levels.\nWe propose two effective strategies to measure the importance of layers within\nLLMs: the first measures the importance of a layer based on how different its\noutput embeddings are from the input embeddings (the higher the better); the\nsecond estimates the importance of a layer using the number of layer weights\nthat are much larger than average (the smaller the better). We show that\nquantizing different layers at varying bits according to our importance scores\nresults in minimal performance drop with a far more compressed model size.\nFinally, we present several practical key takeaways from our variable\nlayer-wise quantization experiments: (a) LLM performance under variable\nquantization remains close to the original model until 25-50% of layers are\nmoved in lower quantization using our proposed ordering but only until 5-10% if\nmoved using no specific ordering; (b) Quantizing LLMs to lower bits performs\nsubstantially better than pruning unless extreme quantization (2-bit) is used;\nand (c) Layer-wise quantization to lower bits works better in the case of\nlarger LLMs with more layers compared to smaller LLMs with fewer layers. The\ncode used to run the experiments is available at:\nhttps://github.com/RazvanDu/LayerwiseQuant.\n", "link": "http://arxiv.org/abs/2406.17415v2", "date": "2024-06-26", "relevancy": 1.3109, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4398}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4348}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4321}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Layer-Wise%20Quantization%3A%20A%20Pragmatic%20and%20Effective%20Method%20for%20Quantizing%0A%20%20LLMs%20Beyond%20Integer%20Bit-Levels&body=Title%3A%20Layer-Wise%20Quantization%3A%20A%20Pragmatic%20and%20Effective%20Method%20for%20Quantizing%0A%20%20LLMs%20Beyond%20Integer%20Bit-Levels%0AAuthor%3A%20Razvan-Gabriel%20Dumitru%20and%20Vikas%20Yadav%20and%20Rishabh%20Maheshwary%20and%20Paul-Ioan%20Clotan%20and%20Sathwik%20Tejaswi%20Madhusudhan%20and%20Mihai%20Surdeanu%0AAbstract%3A%20%20%20We%20present%20a%20simple%20variable%20quantization%20approach%20that%20quantizes%20different%0Alayers%20of%20a%20large%20language%20model%20%28LLM%29%20at%20different%20bit%20levels.%20Specifically%2C%0Awe%20quantize%20the%20most%20important%20layers%20to%20higher%20bit%20precision%20and%20less%0Aimportant%20layers%20to%20lower%20bits%20to%20achieve%20floating%20point%20quantization%20levels.%0AWe%20propose%20two%20effective%20strategies%20to%20measure%20the%20importance%20of%20layers%20within%0ALLMs%3A%20the%20first%20measures%20the%20importance%20of%20a%20layer%20based%20on%20how%20different%20its%0Aoutput%20embeddings%20are%20from%20the%20input%20embeddings%20%28the%20higher%20the%20better%29%3B%20the%0Asecond%20estimates%20the%20importance%20of%20a%20layer%20using%20the%20number%20of%20layer%20weights%0Athat%20are%20much%20larger%20than%20average%20%28the%20smaller%20the%20better%29.%20We%20show%20that%0Aquantizing%20different%20layers%20at%20varying%20bits%20according%20to%20our%20importance%20scores%0Aresults%20in%20minimal%20performance%20drop%20with%20a%20far%20more%20compressed%20model%20size.%0AFinally%2C%20we%20present%20several%20practical%20key%20takeaways%20from%20our%20variable%0Alayer-wise%20quantization%20experiments%3A%20%28a%29%20LLM%20performance%20under%20variable%0Aquantization%20remains%20close%20to%20the%20original%20model%20until%2025-50%25%20of%20layers%20are%0Amoved%20in%20lower%20quantization%20using%20our%20proposed%20ordering%20but%20only%20until%205-10%25%20if%0Amoved%20using%20no%20specific%20ordering%3B%20%28b%29%20Quantizing%20LLMs%20to%20lower%20bits%20performs%0Asubstantially%20better%20than%20pruning%20unless%20extreme%20quantization%20%282-bit%29%20is%20used%3B%0Aand%20%28c%29%20Layer-wise%20quantization%20to%20lower%20bits%20works%20better%20in%20the%20case%20of%0Alarger%20LLMs%20with%20more%20layers%20compared%20to%20smaller%20LLMs%20with%20fewer%20layers.%20The%0Acode%20used%20to%20run%20the%20experiments%20is%20available%20at%3A%0Ahttps%3A//github.com/RazvanDu/LayerwiseQuant.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17415v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLayer-Wise%2520Quantization%253A%2520A%2520Pragmatic%2520and%2520Effective%2520Method%2520for%2520Quantizing%250A%2520%2520LLMs%2520Beyond%2520Integer%2520Bit-Levels%26entry.906535625%3DRazvan-Gabriel%2520Dumitru%2520and%2520Vikas%2520Yadav%2520and%2520Rishabh%2520Maheshwary%2520and%2520Paul-Ioan%2520Clotan%2520and%2520Sathwik%2520Tejaswi%2520Madhusudhan%2520and%2520Mihai%2520Surdeanu%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520simple%2520variable%2520quantization%2520approach%2520that%2520quantizes%2520different%250Alayers%2520of%2520a%2520large%2520language%2520model%2520%2528LLM%2529%2520at%2520different%2520bit%2520levels.%2520Specifically%252C%250Awe%2520quantize%2520the%2520most%2520important%2520layers%2520to%2520higher%2520bit%2520precision%2520and%2520less%250Aimportant%2520layers%2520to%2520lower%2520bits%2520to%2520achieve%2520floating%2520point%2520quantization%2520levels.%250AWe%2520propose%2520two%2520effective%2520strategies%2520to%2520measure%2520the%2520importance%2520of%2520layers%2520within%250ALLMs%253A%2520the%2520first%2520measures%2520the%2520importance%2520of%2520a%2520layer%2520based%2520on%2520how%2520different%2520its%250Aoutput%2520embeddings%2520are%2520from%2520the%2520input%2520embeddings%2520%2528the%2520higher%2520the%2520better%2529%253B%2520the%250Asecond%2520estimates%2520the%2520importance%2520of%2520a%2520layer%2520using%2520the%2520number%2520of%2520layer%2520weights%250Athat%2520are%2520much%2520larger%2520than%2520average%2520%2528the%2520smaller%2520the%2520better%2529.%2520We%2520show%2520that%250Aquantizing%2520different%2520layers%2520at%2520varying%2520bits%2520according%2520to%2520our%2520importance%2520scores%250Aresults%2520in%2520minimal%2520performance%2520drop%2520with%2520a%2520far%2520more%2520compressed%2520model%2520size.%250AFinally%252C%2520we%2520present%2520several%2520practical%2520key%2520takeaways%2520from%2520our%2520variable%250Alayer-wise%2520quantization%2520experiments%253A%2520%2528a%2529%2520LLM%2520performance%2520under%2520variable%250Aquantization%2520remains%2520close%2520to%2520the%2520original%2520model%2520until%252025-50%2525%2520of%2520layers%2520are%250Amoved%2520in%2520lower%2520quantization%2520using%2520our%2520proposed%2520ordering%2520but%2520only%2520until%25205-10%2525%2520if%250Amoved%2520using%2520no%2520specific%2520ordering%253B%2520%2528b%2529%2520Quantizing%2520LLMs%2520to%2520lower%2520bits%2520performs%250Asubstantially%2520better%2520than%2520pruning%2520unless%2520extreme%2520quantization%2520%25282-bit%2529%2520is%2520used%253B%250Aand%2520%2528c%2529%2520Layer-wise%2520quantization%2520to%2520lower%2520bits%2520works%2520better%2520in%2520the%2520case%2520of%250Alarger%2520LLMs%2520with%2520more%2520layers%2520compared%2520to%2520smaller%2520LLMs%2520with%2520fewer%2520layers.%2520The%250Acode%2520used%2520to%2520run%2520the%2520experiments%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/RazvanDu/LayerwiseQuant.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17415v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Layer-Wise%20Quantization%3A%20A%20Pragmatic%20and%20Effective%20Method%20for%20Quantizing%0A%20%20LLMs%20Beyond%20Integer%20Bit-Levels&entry.906535625=Razvan-Gabriel%20Dumitru%20and%20Vikas%20Yadav%20and%20Rishabh%20Maheshwary%20and%20Paul-Ioan%20Clotan%20and%20Sathwik%20Tejaswi%20Madhusudhan%20and%20Mihai%20Surdeanu&entry.1292438233=%20%20We%20present%20a%20simple%20variable%20quantization%20approach%20that%20quantizes%20different%0Alayers%20of%20a%20large%20language%20model%20%28LLM%29%20at%20different%20bit%20levels.%20Specifically%2C%0Awe%20quantize%20the%20most%20important%20layers%20to%20higher%20bit%20precision%20and%20less%0Aimportant%20layers%20to%20lower%20bits%20to%20achieve%20floating%20point%20quantization%20levels.%0AWe%20propose%20two%20effective%20strategies%20to%20measure%20the%20importance%20of%20layers%20within%0ALLMs%3A%20the%20first%20measures%20the%20importance%20of%20a%20layer%20based%20on%20how%20different%20its%0Aoutput%20embeddings%20are%20from%20the%20input%20embeddings%20%28the%20higher%20the%20better%29%3B%20the%0Asecond%20estimates%20the%20importance%20of%20a%20layer%20using%20the%20number%20of%20layer%20weights%0Athat%20are%20much%20larger%20than%20average%20%28the%20smaller%20the%20better%29.%20We%20show%20that%0Aquantizing%20different%20layers%20at%20varying%20bits%20according%20to%20our%20importance%20scores%0Aresults%20in%20minimal%20performance%20drop%20with%20a%20far%20more%20compressed%20model%20size.%0AFinally%2C%20we%20present%20several%20practical%20key%20takeaways%20from%20our%20variable%0Alayer-wise%20quantization%20experiments%3A%20%28a%29%20LLM%20performance%20under%20variable%0Aquantization%20remains%20close%20to%20the%20original%20model%20until%2025-50%25%20of%20layers%20are%0Amoved%20in%20lower%20quantization%20using%20our%20proposed%20ordering%20but%20only%20until%205-10%25%20if%0Amoved%20using%20no%20specific%20ordering%3B%20%28b%29%20Quantizing%20LLMs%20to%20lower%20bits%20performs%0Asubstantially%20better%20than%20pruning%20unless%20extreme%20quantization%20%282-bit%29%20is%20used%3B%0Aand%20%28c%29%20Layer-wise%20quantization%20to%20lower%20bits%20works%20better%20in%20the%20case%20of%0Alarger%20LLMs%20with%20more%20layers%20compared%20to%20smaller%20LLMs%20with%20fewer%20layers.%20The%0Acode%20used%20to%20run%20the%20experiments%20is%20available%20at%3A%0Ahttps%3A//github.com/RazvanDu/LayerwiseQuant.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17415v2&entry.124074799=Read"},
{"title": "AI Cards: Towards an Applied Framework for Machine-Readable AI and Risk\n  Documentation Inspired by the EU AI Act", "author": "Delaram Golpayegani and Isabelle Hupont and Cecilia Panigutti and Harshvardhan J. Pandit and Sven Schade and Declan O'Sullivan and Dave Lewis", "abstract": "  With the upcoming enforcement of the EU AI Act, documentation of high-risk AI\nsystems and their risk management information will become a legal requirement\nplaying a pivotal role in demonstration of compliance. Despite its importance,\nthere is a lack of standards and guidelines to assist with drawing up AI and\nrisk documentation aligned with the AI Act. This paper aims to address this gap\nby providing an in-depth analysis of the AI Act's provisions regarding\ntechnical documentation, wherein we particularly focus on AI risk management.\nOn the basis of this analysis, we propose AI Cards as a novel holistic\nframework for representing a given intended use of an AI system by encompassing\ninformation regarding technical specifications, context of use, and risk\nmanagement, both in human- and machine-readable formats. While the\nhuman-readable representation of AI Cards provides AI stakeholders with a\ntransparent and comprehensible overview of the AI use case, its\nmachine-readable specification leverages on state of the art Semantic Web\ntechnologies to embody the interoperability needed for exchanging documentation\nwithin the AI value chain. This brings the flexibility required for reflecting\nchanges applied to the AI system and its context, provides the scalability\nneeded to accommodate potential amendments to legal requirements, and enables\ndevelopment of automated tools to assist with legal compliance and conformity\nassessment tasks. To solidify the benefits, we provide an exemplar AI Card for\nan AI-based student proctoring system and further discuss its potential\napplications within and beyond the context of the AI Act.\n", "link": "http://arxiv.org/abs/2406.18211v1", "date": "2024-06-26", "relevancy": 0.863, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4456}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4323}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI%20Cards%3A%20Towards%20an%20Applied%20Framework%20for%20Machine-Readable%20AI%20and%20Risk%0A%20%20Documentation%20Inspired%20by%20the%20EU%20AI%20Act&body=Title%3A%20AI%20Cards%3A%20Towards%20an%20Applied%20Framework%20for%20Machine-Readable%20AI%20and%20Risk%0A%20%20Documentation%20Inspired%20by%20the%20EU%20AI%20Act%0AAuthor%3A%20Delaram%20Golpayegani%20and%20Isabelle%20Hupont%20and%20Cecilia%20Panigutti%20and%20Harshvardhan%20J.%20Pandit%20and%20Sven%20Schade%20and%20Declan%20O%27Sullivan%20and%20Dave%20Lewis%0AAbstract%3A%20%20%20With%20the%20upcoming%20enforcement%20of%20the%20EU%20AI%20Act%2C%20documentation%20of%20high-risk%20AI%0Asystems%20and%20their%20risk%20management%20information%20will%20become%20a%20legal%20requirement%0Aplaying%20a%20pivotal%20role%20in%20demonstration%20of%20compliance.%20Despite%20its%20importance%2C%0Athere%20is%20a%20lack%20of%20standards%20and%20guidelines%20to%20assist%20with%20drawing%20up%20AI%20and%0Arisk%20documentation%20aligned%20with%20the%20AI%20Act.%20This%20paper%20aims%20to%20address%20this%20gap%0Aby%20providing%20an%20in-depth%20analysis%20of%20the%20AI%20Act%27s%20provisions%20regarding%0Atechnical%20documentation%2C%20wherein%20we%20particularly%20focus%20on%20AI%20risk%20management.%0AOn%20the%20basis%20of%20this%20analysis%2C%20we%20propose%20AI%20Cards%20as%20a%20novel%20holistic%0Aframework%20for%20representing%20a%20given%20intended%20use%20of%20an%20AI%20system%20by%20encompassing%0Ainformation%20regarding%20technical%20specifications%2C%20context%20of%20use%2C%20and%20risk%0Amanagement%2C%20both%20in%20human-%20and%20machine-readable%20formats.%20While%20the%0Ahuman-readable%20representation%20of%20AI%20Cards%20provides%20AI%20stakeholders%20with%20a%0Atransparent%20and%20comprehensible%20overview%20of%20the%20AI%20use%20case%2C%20its%0Amachine-readable%20specification%20leverages%20on%20state%20of%20the%20art%20Semantic%20Web%0Atechnologies%20to%20embody%20the%20interoperability%20needed%20for%20exchanging%20documentation%0Awithin%20the%20AI%20value%20chain.%20This%20brings%20the%20flexibility%20required%20for%20reflecting%0Achanges%20applied%20to%20the%20AI%20system%20and%20its%20context%2C%20provides%20the%20scalability%0Aneeded%20to%20accommodate%20potential%20amendments%20to%20legal%20requirements%2C%20and%20enables%0Adevelopment%20of%20automated%20tools%20to%20assist%20with%20legal%20compliance%20and%20conformity%0Aassessment%20tasks.%20To%20solidify%20the%20benefits%2C%20we%20provide%20an%20exemplar%20AI%20Card%20for%0Aan%20AI-based%20student%20proctoring%20system%20and%20further%20discuss%20its%20potential%0Aapplications%20within%20and%20beyond%20the%20context%20of%20the%20AI%20Act.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18211v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI%2520Cards%253A%2520Towards%2520an%2520Applied%2520Framework%2520for%2520Machine-Readable%2520AI%2520and%2520Risk%250A%2520%2520Documentation%2520Inspired%2520by%2520the%2520EU%2520AI%2520Act%26entry.906535625%3DDelaram%2520Golpayegani%2520and%2520Isabelle%2520Hupont%2520and%2520Cecilia%2520Panigutti%2520and%2520Harshvardhan%2520J.%2520Pandit%2520and%2520Sven%2520Schade%2520and%2520Declan%2520O%2527Sullivan%2520and%2520Dave%2520Lewis%26entry.1292438233%3D%2520%2520With%2520the%2520upcoming%2520enforcement%2520of%2520the%2520EU%2520AI%2520Act%252C%2520documentation%2520of%2520high-risk%2520AI%250Asystems%2520and%2520their%2520risk%2520management%2520information%2520will%2520become%2520a%2520legal%2520requirement%250Aplaying%2520a%2520pivotal%2520role%2520in%2520demonstration%2520of%2520compliance.%2520Despite%2520its%2520importance%252C%250Athere%2520is%2520a%2520lack%2520of%2520standards%2520and%2520guidelines%2520to%2520assist%2520with%2520drawing%2520up%2520AI%2520and%250Arisk%2520documentation%2520aligned%2520with%2520the%2520AI%2520Act.%2520This%2520paper%2520aims%2520to%2520address%2520this%2520gap%250Aby%2520providing%2520an%2520in-depth%2520analysis%2520of%2520the%2520AI%2520Act%2527s%2520provisions%2520regarding%250Atechnical%2520documentation%252C%2520wherein%2520we%2520particularly%2520focus%2520on%2520AI%2520risk%2520management.%250AOn%2520the%2520basis%2520of%2520this%2520analysis%252C%2520we%2520propose%2520AI%2520Cards%2520as%2520a%2520novel%2520holistic%250Aframework%2520for%2520representing%2520a%2520given%2520intended%2520use%2520of%2520an%2520AI%2520system%2520by%2520encompassing%250Ainformation%2520regarding%2520technical%2520specifications%252C%2520context%2520of%2520use%252C%2520and%2520risk%250Amanagement%252C%2520both%2520in%2520human-%2520and%2520machine-readable%2520formats.%2520While%2520the%250Ahuman-readable%2520representation%2520of%2520AI%2520Cards%2520provides%2520AI%2520stakeholders%2520with%2520a%250Atransparent%2520and%2520comprehensible%2520overview%2520of%2520the%2520AI%2520use%2520case%252C%2520its%250Amachine-readable%2520specification%2520leverages%2520on%2520state%2520of%2520the%2520art%2520Semantic%2520Web%250Atechnologies%2520to%2520embody%2520the%2520interoperability%2520needed%2520for%2520exchanging%2520documentation%250Awithin%2520the%2520AI%2520value%2520chain.%2520This%2520brings%2520the%2520flexibility%2520required%2520for%2520reflecting%250Achanges%2520applied%2520to%2520the%2520AI%2520system%2520and%2520its%2520context%252C%2520provides%2520the%2520scalability%250Aneeded%2520to%2520accommodate%2520potential%2520amendments%2520to%2520legal%2520requirements%252C%2520and%2520enables%250Adevelopment%2520of%2520automated%2520tools%2520to%2520assist%2520with%2520legal%2520compliance%2520and%2520conformity%250Aassessment%2520tasks.%2520To%2520solidify%2520the%2520benefits%252C%2520we%2520provide%2520an%2520exemplar%2520AI%2520Card%2520for%250Aan%2520AI-based%2520student%2520proctoring%2520system%2520and%2520further%2520discuss%2520its%2520potential%250Aapplications%2520within%2520and%2520beyond%2520the%2520context%2520of%2520the%2520AI%2520Act.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18211v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI%20Cards%3A%20Towards%20an%20Applied%20Framework%20for%20Machine-Readable%20AI%20and%20Risk%0A%20%20Documentation%20Inspired%20by%20the%20EU%20AI%20Act&entry.906535625=Delaram%20Golpayegani%20and%20Isabelle%20Hupont%20and%20Cecilia%20Panigutti%20and%20Harshvardhan%20J.%20Pandit%20and%20Sven%20Schade%20and%20Declan%20O%27Sullivan%20and%20Dave%20Lewis&entry.1292438233=%20%20With%20the%20upcoming%20enforcement%20of%20the%20EU%20AI%20Act%2C%20documentation%20of%20high-risk%20AI%0Asystems%20and%20their%20risk%20management%20information%20will%20become%20a%20legal%20requirement%0Aplaying%20a%20pivotal%20role%20in%20demonstration%20of%20compliance.%20Despite%20its%20importance%2C%0Athere%20is%20a%20lack%20of%20standards%20and%20guidelines%20to%20assist%20with%20drawing%20up%20AI%20and%0Arisk%20documentation%20aligned%20with%20the%20AI%20Act.%20This%20paper%20aims%20to%20address%20this%20gap%0Aby%20providing%20an%20in-depth%20analysis%20of%20the%20AI%20Act%27s%20provisions%20regarding%0Atechnical%20documentation%2C%20wherein%20we%20particularly%20focus%20on%20AI%20risk%20management.%0AOn%20the%20basis%20of%20this%20analysis%2C%20we%20propose%20AI%20Cards%20as%20a%20novel%20holistic%0Aframework%20for%20representing%20a%20given%20intended%20use%20of%20an%20AI%20system%20by%20encompassing%0Ainformation%20regarding%20technical%20specifications%2C%20context%20of%20use%2C%20and%20risk%0Amanagement%2C%20both%20in%20human-%20and%20machine-readable%20formats.%20While%20the%0Ahuman-readable%20representation%20of%20AI%20Cards%20provides%20AI%20stakeholders%20with%20a%0Atransparent%20and%20comprehensible%20overview%20of%20the%20AI%20use%20case%2C%20its%0Amachine-readable%20specification%20leverages%20on%20state%20of%20the%20art%20Semantic%20Web%0Atechnologies%20to%20embody%20the%20interoperability%20needed%20for%20exchanging%20documentation%0Awithin%20the%20AI%20value%20chain.%20This%20brings%20the%20flexibility%20required%20for%20reflecting%0Achanges%20applied%20to%20the%20AI%20system%20and%20its%20context%2C%20provides%20the%20scalability%0Aneeded%20to%20accommodate%20potential%20amendments%20to%20legal%20requirements%2C%20and%20enables%0Adevelopment%20of%20automated%20tools%20to%20assist%20with%20legal%20compliance%20and%20conformity%0Aassessment%20tasks.%20To%20solidify%20the%20benefits%2C%20we%20provide%20an%20exemplar%20AI%20Card%20for%0Aan%20AI-based%20student%20proctoring%20system%20and%20further%20discuss%20its%20potential%0Aapplications%20within%20and%20beyond%20the%20context%20of%20the%20AI%20Act.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18211v1&entry.124074799=Read"},
{"title": "Robust Surgical Phase Recognition From Annotation Efficient Supervision", "author": "Or Rubin and Shlomi Laufer", "abstract": "  Surgical phase recognition is a key task in computer-assisted surgery, aiming\nto automatically identify and categorize the different phases within a surgical\nprocedure. Despite substantial advancements, most current approaches rely on\nfully supervised training, requiring expensive and time-consuming frame-level\nannotations. Timestamp supervision has recently emerged as a promising\nalternative, significantly reducing annotation costs while maintaining\ncompetitive performance. However, models trained on timestamp annotations can\nbe negatively impacted by missing phase annotations, leading to a potential\ndrawback in real-world scenarios. In this work, we address this issue by\nproposing a robust method for surgical phase recognition that can handle\nmissing phase annotations effectively. Furthermore, we introduce the SkipTag@K\nannotation approach to the surgical domain, enabling a flexible balance between\nannotation effort and model performance. Our method achieves competitive\nresults on two challenging datasets, demonstrating its efficacy in handling\nmissing phase annotations and its potential for reducing annotation costs.\nSpecifically, we achieve an accuracy of 85.1\\% on the MultiBypass140 dataset\nusing only 3 annotated frames per video, showcasing the effectiveness of our\nmethod and the potential of the SkipTag@K setup. We perform extensive\nexperiments to validate the robustness of our method and provide valuable\ninsights to guide future research in surgical phase recognition. Our work\ncontributes to the advancement of surgical workflow recognition and paves the\nway for more efficient and reliable surgical phase recognition systems.\n", "link": "http://arxiv.org/abs/2406.18481v1", "date": "2024-06-26", "relevancy": 1.5835, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5403}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5373}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4873}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Surgical%20Phase%20Recognition%20From%20Annotation%20Efficient%20Supervision&body=Title%3A%20Robust%20Surgical%20Phase%20Recognition%20From%20Annotation%20Efficient%20Supervision%0AAuthor%3A%20Or%20Rubin%20and%20Shlomi%20Laufer%0AAbstract%3A%20%20%20Surgical%20phase%20recognition%20is%20a%20key%20task%20in%20computer-assisted%20surgery%2C%20aiming%0Ato%20automatically%20identify%20and%20categorize%20the%20different%20phases%20within%20a%20surgical%0Aprocedure.%20Despite%20substantial%20advancements%2C%20most%20current%20approaches%20rely%20on%0Afully%20supervised%20training%2C%20requiring%20expensive%20and%20time-consuming%20frame-level%0Aannotations.%20Timestamp%20supervision%20has%20recently%20emerged%20as%20a%20promising%0Aalternative%2C%20significantly%20reducing%20annotation%20costs%20while%20maintaining%0Acompetitive%20performance.%20However%2C%20models%20trained%20on%20timestamp%20annotations%20can%0Abe%20negatively%20impacted%20by%20missing%20phase%20annotations%2C%20leading%20to%20a%20potential%0Adrawback%20in%20real-world%20scenarios.%20In%20this%20work%2C%20we%20address%20this%20issue%20by%0Aproposing%20a%20robust%20method%20for%20surgical%20phase%20recognition%20that%20can%20handle%0Amissing%20phase%20annotations%20effectively.%20Furthermore%2C%20we%20introduce%20the%20SkipTag%40K%0Aannotation%20approach%20to%20the%20surgical%20domain%2C%20enabling%20a%20flexible%20balance%20between%0Aannotation%20effort%20and%20model%20performance.%20Our%20method%20achieves%20competitive%0Aresults%20on%20two%20challenging%20datasets%2C%20demonstrating%20its%20efficacy%20in%20handling%0Amissing%20phase%20annotations%20and%20its%20potential%20for%20reducing%20annotation%20costs.%0ASpecifically%2C%20we%20achieve%20an%20accuracy%20of%2085.1%5C%25%20on%20the%20MultiBypass140%20dataset%0Ausing%20only%203%20annotated%20frames%20per%20video%2C%20showcasing%20the%20effectiveness%20of%20our%0Amethod%20and%20the%20potential%20of%20the%20SkipTag%40K%20setup.%20We%20perform%20extensive%0Aexperiments%20to%20validate%20the%20robustness%20of%20our%20method%20and%20provide%20valuable%0Ainsights%20to%20guide%20future%20research%20in%20surgical%20phase%20recognition.%20Our%20work%0Acontributes%20to%20the%20advancement%20of%20surgical%20workflow%20recognition%20and%20paves%20the%0Away%20for%20more%20efficient%20and%20reliable%20surgical%20phase%20recognition%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18481v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Surgical%2520Phase%2520Recognition%2520From%2520Annotation%2520Efficient%2520Supervision%26entry.906535625%3DOr%2520Rubin%2520and%2520Shlomi%2520Laufer%26entry.1292438233%3D%2520%2520Surgical%2520phase%2520recognition%2520is%2520a%2520key%2520task%2520in%2520computer-assisted%2520surgery%252C%2520aiming%250Ato%2520automatically%2520identify%2520and%2520categorize%2520the%2520different%2520phases%2520within%2520a%2520surgical%250Aprocedure.%2520Despite%2520substantial%2520advancements%252C%2520most%2520current%2520approaches%2520rely%2520on%250Afully%2520supervised%2520training%252C%2520requiring%2520expensive%2520and%2520time-consuming%2520frame-level%250Aannotations.%2520Timestamp%2520supervision%2520has%2520recently%2520emerged%2520as%2520a%2520promising%250Aalternative%252C%2520significantly%2520reducing%2520annotation%2520costs%2520while%2520maintaining%250Acompetitive%2520performance.%2520However%252C%2520models%2520trained%2520on%2520timestamp%2520annotations%2520can%250Abe%2520negatively%2520impacted%2520by%2520missing%2520phase%2520annotations%252C%2520leading%2520to%2520a%2520potential%250Adrawback%2520in%2520real-world%2520scenarios.%2520In%2520this%2520work%252C%2520we%2520address%2520this%2520issue%2520by%250Aproposing%2520a%2520robust%2520method%2520for%2520surgical%2520phase%2520recognition%2520that%2520can%2520handle%250Amissing%2520phase%2520annotations%2520effectively.%2520Furthermore%252C%2520we%2520introduce%2520the%2520SkipTag%2540K%250Aannotation%2520approach%2520to%2520the%2520surgical%2520domain%252C%2520enabling%2520a%2520flexible%2520balance%2520between%250Aannotation%2520effort%2520and%2520model%2520performance.%2520Our%2520method%2520achieves%2520competitive%250Aresults%2520on%2520two%2520challenging%2520datasets%252C%2520demonstrating%2520its%2520efficacy%2520in%2520handling%250Amissing%2520phase%2520annotations%2520and%2520its%2520potential%2520for%2520reducing%2520annotation%2520costs.%250ASpecifically%252C%2520we%2520achieve%2520an%2520accuracy%2520of%252085.1%255C%2525%2520on%2520the%2520MultiBypass140%2520dataset%250Ausing%2520only%25203%2520annotated%2520frames%2520per%2520video%252C%2520showcasing%2520the%2520effectiveness%2520of%2520our%250Amethod%2520and%2520the%2520potential%2520of%2520the%2520SkipTag%2540K%2520setup.%2520We%2520perform%2520extensive%250Aexperiments%2520to%2520validate%2520the%2520robustness%2520of%2520our%2520method%2520and%2520provide%2520valuable%250Ainsights%2520to%2520guide%2520future%2520research%2520in%2520surgical%2520phase%2520recognition.%2520Our%2520work%250Acontributes%2520to%2520the%2520advancement%2520of%2520surgical%2520workflow%2520recognition%2520and%2520paves%2520the%250Away%2520for%2520more%2520efficient%2520and%2520reliable%2520surgical%2520phase%2520recognition%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18481v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Surgical%20Phase%20Recognition%20From%20Annotation%20Efficient%20Supervision&entry.906535625=Or%20Rubin%20and%20Shlomi%20Laufer&entry.1292438233=%20%20Surgical%20phase%20recognition%20is%20a%20key%20task%20in%20computer-assisted%20surgery%2C%20aiming%0Ato%20automatically%20identify%20and%20categorize%20the%20different%20phases%20within%20a%20surgical%0Aprocedure.%20Despite%20substantial%20advancements%2C%20most%20current%20approaches%20rely%20on%0Afully%20supervised%20training%2C%20requiring%20expensive%20and%20time-consuming%20frame-level%0Aannotations.%20Timestamp%20supervision%20has%20recently%20emerged%20as%20a%20promising%0Aalternative%2C%20significantly%20reducing%20annotation%20costs%20while%20maintaining%0Acompetitive%20performance.%20However%2C%20models%20trained%20on%20timestamp%20annotations%20can%0Abe%20negatively%20impacted%20by%20missing%20phase%20annotations%2C%20leading%20to%20a%20potential%0Adrawback%20in%20real-world%20scenarios.%20In%20this%20work%2C%20we%20address%20this%20issue%20by%0Aproposing%20a%20robust%20method%20for%20surgical%20phase%20recognition%20that%20can%20handle%0Amissing%20phase%20annotations%20effectively.%20Furthermore%2C%20we%20introduce%20the%20SkipTag%40K%0Aannotation%20approach%20to%20the%20surgical%20domain%2C%20enabling%20a%20flexible%20balance%20between%0Aannotation%20effort%20and%20model%20performance.%20Our%20method%20achieves%20competitive%0Aresults%20on%20two%20challenging%20datasets%2C%20demonstrating%20its%20efficacy%20in%20handling%0Amissing%20phase%20annotations%20and%20its%20potential%20for%20reducing%20annotation%20costs.%0ASpecifically%2C%20we%20achieve%20an%20accuracy%20of%2085.1%5C%25%20on%20the%20MultiBypass140%20dataset%0Ausing%20only%203%20annotated%20frames%20per%20video%2C%20showcasing%20the%20effectiveness%20of%20our%0Amethod%20and%20the%20potential%20of%20the%20SkipTag%40K%20setup.%20We%20perform%20extensive%0Aexperiments%20to%20validate%20the%20robustness%20of%20our%20method%20and%20provide%20valuable%0Ainsights%20to%20guide%20future%20research%20in%20surgical%20phase%20recognition.%20Our%20work%0Acontributes%20to%20the%20advancement%20of%20surgical%20workflow%20recognition%20and%20paves%20the%0Away%20for%20more%20efficient%20and%20reliable%20surgical%20phase%20recognition%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18481v1&entry.124074799=Read"},
{"title": "Multi-Agent Imitation Learning: Value is Easy, Regret is Hard", "author": "Jingwu Tang and Gokul Swamy and Fei Fang and Zhiwei Steven Wu", "abstract": "  We study a multi-agent imitation learning (MAIL) problem where we take the\nperspective of a learner attempting to coordinate a group of agents based on\ndemonstrations of an expert doing so. Most prior work in MAIL essentially\nreduces the problem to matching the behavior of the expert within the support\nof the demonstrations. While doing so is sufficient to drive the value gap\nbetween the learner and the expert to zero under the assumption that agents are\nnon-strategic, it does not guarantee robustness to deviations by strategic\nagents. Intuitively, this is because strategic deviations can depend on a\ncounterfactual quantity: the coordinator's recommendations outside of the state\ndistribution their recommendations induce. In response, we initiate the study\nof an alternative objective for MAIL in Markov Games we term the regret gap\nthat explicitly accounts for potential deviations by agents in the group. We\nfirst perform an in-depth exploration of the relationship between the value and\nregret gaps. First, we show that while the value gap can be efficiently\nminimized via a direct extension of single-agent IL algorithms, even value\nequivalence can lead to an arbitrarily large regret gap. This implies that\nachieving regret equivalence is harder than achieving value equivalence in\nMAIL. We then provide a pair of efficient reductions to no-regret online convex\noptimization that are capable of minimizing the regret gap (a) under a coverage\nassumption on the expert (MALICE) or (b) with access to a queryable expert\n(BLADES).\n", "link": "http://arxiv.org/abs/2406.04219v2", "date": "2024-06-26", "relevancy": 1.8619, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4752}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4603}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Agent%20Imitation%20Learning%3A%20Value%20is%20Easy%2C%20Regret%20is%20Hard&body=Title%3A%20Multi-Agent%20Imitation%20Learning%3A%20Value%20is%20Easy%2C%20Regret%20is%20Hard%0AAuthor%3A%20Jingwu%20Tang%20and%20Gokul%20Swamy%20and%20Fei%20Fang%20and%20Zhiwei%20Steven%20Wu%0AAbstract%3A%20%20%20We%20study%20a%20multi-agent%20imitation%20learning%20%28MAIL%29%20problem%20where%20we%20take%20the%0Aperspective%20of%20a%20learner%20attempting%20to%20coordinate%20a%20group%20of%20agents%20based%20on%0Ademonstrations%20of%20an%20expert%20doing%20so.%20Most%20prior%20work%20in%20MAIL%20essentially%0Areduces%20the%20problem%20to%20matching%20the%20behavior%20of%20the%20expert%20within%20the%20support%0Aof%20the%20demonstrations.%20While%20doing%20so%20is%20sufficient%20to%20drive%20the%20value%20gap%0Abetween%20the%20learner%20and%20the%20expert%20to%20zero%20under%20the%20assumption%20that%20agents%20are%0Anon-strategic%2C%20it%20does%20not%20guarantee%20robustness%20to%20deviations%20by%20strategic%0Aagents.%20Intuitively%2C%20this%20is%20because%20strategic%20deviations%20can%20depend%20on%20a%0Acounterfactual%20quantity%3A%20the%20coordinator%27s%20recommendations%20outside%20of%20the%20state%0Adistribution%20their%20recommendations%20induce.%20In%20response%2C%20we%20initiate%20the%20study%0Aof%20an%20alternative%20objective%20for%20MAIL%20in%20Markov%20Games%20we%20term%20the%20regret%20gap%0Athat%20explicitly%20accounts%20for%20potential%20deviations%20by%20agents%20in%20the%20group.%20We%0Afirst%20perform%20an%20in-depth%20exploration%20of%20the%20relationship%20between%20the%20value%20and%0Aregret%20gaps.%20First%2C%20we%20show%20that%20while%20the%20value%20gap%20can%20be%20efficiently%0Aminimized%20via%20a%20direct%20extension%20of%20single-agent%20IL%20algorithms%2C%20even%20value%0Aequivalence%20can%20lead%20to%20an%20arbitrarily%20large%20regret%20gap.%20This%20implies%20that%0Aachieving%20regret%20equivalence%20is%20harder%20than%20achieving%20value%20equivalence%20in%0AMAIL.%20We%20then%20provide%20a%20pair%20of%20efficient%20reductions%20to%20no-regret%20online%20convex%0Aoptimization%20that%20are%20capable%20of%20minimizing%20the%20regret%20gap%20%28a%29%20under%20a%20coverage%0Aassumption%20on%20the%20expert%20%28MALICE%29%20or%20%28b%29%20with%20access%20to%20a%20queryable%20expert%0A%28BLADES%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04219v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Agent%2520Imitation%2520Learning%253A%2520Value%2520is%2520Easy%252C%2520Regret%2520is%2520Hard%26entry.906535625%3DJingwu%2520Tang%2520and%2520Gokul%2520Swamy%2520and%2520Fei%2520Fang%2520and%2520Zhiwei%2520Steven%2520Wu%26entry.1292438233%3D%2520%2520We%2520study%2520a%2520multi-agent%2520imitation%2520learning%2520%2528MAIL%2529%2520problem%2520where%2520we%2520take%2520the%250Aperspective%2520of%2520a%2520learner%2520attempting%2520to%2520coordinate%2520a%2520group%2520of%2520agents%2520based%2520on%250Ademonstrations%2520of%2520an%2520expert%2520doing%2520so.%2520Most%2520prior%2520work%2520in%2520MAIL%2520essentially%250Areduces%2520the%2520problem%2520to%2520matching%2520the%2520behavior%2520of%2520the%2520expert%2520within%2520the%2520support%250Aof%2520the%2520demonstrations.%2520While%2520doing%2520so%2520is%2520sufficient%2520to%2520drive%2520the%2520value%2520gap%250Abetween%2520the%2520learner%2520and%2520the%2520expert%2520to%2520zero%2520under%2520the%2520assumption%2520that%2520agents%2520are%250Anon-strategic%252C%2520it%2520does%2520not%2520guarantee%2520robustness%2520to%2520deviations%2520by%2520strategic%250Aagents.%2520Intuitively%252C%2520this%2520is%2520because%2520strategic%2520deviations%2520can%2520depend%2520on%2520a%250Acounterfactual%2520quantity%253A%2520the%2520coordinator%2527s%2520recommendations%2520outside%2520of%2520the%2520state%250Adistribution%2520their%2520recommendations%2520induce.%2520In%2520response%252C%2520we%2520initiate%2520the%2520study%250Aof%2520an%2520alternative%2520objective%2520for%2520MAIL%2520in%2520Markov%2520Games%2520we%2520term%2520the%2520regret%2520gap%250Athat%2520explicitly%2520accounts%2520for%2520potential%2520deviations%2520by%2520agents%2520in%2520the%2520group.%2520We%250Afirst%2520perform%2520an%2520in-depth%2520exploration%2520of%2520the%2520relationship%2520between%2520the%2520value%2520and%250Aregret%2520gaps.%2520First%252C%2520we%2520show%2520that%2520while%2520the%2520value%2520gap%2520can%2520be%2520efficiently%250Aminimized%2520via%2520a%2520direct%2520extension%2520of%2520single-agent%2520IL%2520algorithms%252C%2520even%2520value%250Aequivalence%2520can%2520lead%2520to%2520an%2520arbitrarily%2520large%2520regret%2520gap.%2520This%2520implies%2520that%250Aachieving%2520regret%2520equivalence%2520is%2520harder%2520than%2520achieving%2520value%2520equivalence%2520in%250AMAIL.%2520We%2520then%2520provide%2520a%2520pair%2520of%2520efficient%2520reductions%2520to%2520no-regret%2520online%2520convex%250Aoptimization%2520that%2520are%2520capable%2520of%2520minimizing%2520the%2520regret%2520gap%2520%2528a%2529%2520under%2520a%2520coverage%250Aassumption%2520on%2520the%2520expert%2520%2528MALICE%2529%2520or%2520%2528b%2529%2520with%2520access%2520to%2520a%2520queryable%2520expert%250A%2528BLADES%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04219v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Agent%20Imitation%20Learning%3A%20Value%20is%20Easy%2C%20Regret%20is%20Hard&entry.906535625=Jingwu%20Tang%20and%20Gokul%20Swamy%20and%20Fei%20Fang%20and%20Zhiwei%20Steven%20Wu&entry.1292438233=%20%20We%20study%20a%20multi-agent%20imitation%20learning%20%28MAIL%29%20problem%20where%20we%20take%20the%0Aperspective%20of%20a%20learner%20attempting%20to%20coordinate%20a%20group%20of%20agents%20based%20on%0Ademonstrations%20of%20an%20expert%20doing%20so.%20Most%20prior%20work%20in%20MAIL%20essentially%0Areduces%20the%20problem%20to%20matching%20the%20behavior%20of%20the%20expert%20within%20the%20support%0Aof%20the%20demonstrations.%20While%20doing%20so%20is%20sufficient%20to%20drive%20the%20value%20gap%0Abetween%20the%20learner%20and%20the%20expert%20to%20zero%20under%20the%20assumption%20that%20agents%20are%0Anon-strategic%2C%20it%20does%20not%20guarantee%20robustness%20to%20deviations%20by%20strategic%0Aagents.%20Intuitively%2C%20this%20is%20because%20strategic%20deviations%20can%20depend%20on%20a%0Acounterfactual%20quantity%3A%20the%20coordinator%27s%20recommendations%20outside%20of%20the%20state%0Adistribution%20their%20recommendations%20induce.%20In%20response%2C%20we%20initiate%20the%20study%0Aof%20an%20alternative%20objective%20for%20MAIL%20in%20Markov%20Games%20we%20term%20the%20regret%20gap%0Athat%20explicitly%20accounts%20for%20potential%20deviations%20by%20agents%20in%20the%20group.%20We%0Afirst%20perform%20an%20in-depth%20exploration%20of%20the%20relationship%20between%20the%20value%20and%0Aregret%20gaps.%20First%2C%20we%20show%20that%20while%20the%20value%20gap%20can%20be%20efficiently%0Aminimized%20via%20a%20direct%20extension%20of%20single-agent%20IL%20algorithms%2C%20even%20value%0Aequivalence%20can%20lead%20to%20an%20arbitrarily%20large%20regret%20gap.%20This%20implies%20that%0Aachieving%20regret%20equivalence%20is%20harder%20than%20achieving%20value%20equivalence%20in%0AMAIL.%20We%20then%20provide%20a%20pair%20of%20efficient%20reductions%20to%20no-regret%20online%20convex%0Aoptimization%20that%20are%20capable%20of%20minimizing%20the%20regret%20gap%20%28a%29%20under%20a%20coverage%0Aassumption%20on%20the%20expert%20%28MALICE%29%20or%20%28b%29%20with%20access%20to%20a%20queryable%20expert%0A%28BLADES%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04219v2&entry.124074799=Read"},
{"title": "CoDA: Interactive Segmentation and Morphological Analysis of Dendroid\n  Structures Exemplified on Stony Cold-Water Corals", "author": "Kira Schmitt and J\u00fcrgen Titschack and Daniel Baum", "abstract": "  Herein, we present CoDA, the Coral Dendroid structure Analyzer, a visual\nanalytics suite that allows for the first time to investigate the ontogenetic\nmorphological development of complex dendroid coral colonies, exemplified on\nthree important framework-forming dendroid cold-water corals: Lophelia pertusa\n(Linnaeus, 1758), Madrepora oculata (Linnaeus, 1758), and Goniocorella dumosa\n(Alcock, 1902). Input to CoDA is an initial instance segmentation of the coral\npolyp cavities (calices), from which it estimates the skeleton tree of the\ncolony and extracts classical morphological measurements and advanced shape\nfeatures of the individual corallites. CoDA also works as a proofreading and\nerror correction tool by helping to identify wrong parts in the skeleton tree\nand providing tools to quickly correct these errors. The final skeleton tree\nenables the derivation of additional information about the calices/corallite\ninstances that otherwise could not be obtained, including their ontogenetic\ngeneration and branching patterns - the basis of a fully quantitative\nstatistical analysis of the coral colony morphology. Part of CoDA is CoDAGraph,\na feature-rich link-and-brush user interface for visualizing the extracted\nfeatures and 2D graph layouts of the skeleton tree, enabling the real-time\nexploration of complex coral colonies and their building blocks, the individual\ncorallites and branches.\n  In the future, we expect CoDA to greatly facilitate the analysis of large\nstony corals of different species and morphotypes, as well as other dendroid\nstructures, enabling new insights into the influence of genetic and\nenvironmental factors on their ontogenetic morphological development.\n", "link": "http://arxiv.org/abs/2406.18236v1", "date": "2024-06-26", "relevancy": 1.8182, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4587}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4587}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4337}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoDA%3A%20Interactive%20Segmentation%20and%20Morphological%20Analysis%20of%20Dendroid%0A%20%20Structures%20Exemplified%20on%20Stony%20Cold-Water%20Corals&body=Title%3A%20CoDA%3A%20Interactive%20Segmentation%20and%20Morphological%20Analysis%20of%20Dendroid%0A%20%20Structures%20Exemplified%20on%20Stony%20Cold-Water%20Corals%0AAuthor%3A%20Kira%20Schmitt%20and%20J%C3%BCrgen%20Titschack%20and%20Daniel%20Baum%0AAbstract%3A%20%20%20Herein%2C%20we%20present%20CoDA%2C%20the%20Coral%20Dendroid%20structure%20Analyzer%2C%20a%20visual%0Aanalytics%20suite%20that%20allows%20for%20the%20first%20time%20to%20investigate%20the%20ontogenetic%0Amorphological%20development%20of%20complex%20dendroid%20coral%20colonies%2C%20exemplified%20on%0Athree%20important%20framework-forming%20dendroid%20cold-water%20corals%3A%20Lophelia%20pertusa%0A%28Linnaeus%2C%201758%29%2C%20Madrepora%20oculata%20%28Linnaeus%2C%201758%29%2C%20and%20Goniocorella%20dumosa%0A%28Alcock%2C%201902%29.%20Input%20to%20CoDA%20is%20an%20initial%20instance%20segmentation%20of%20the%20coral%0Apolyp%20cavities%20%28calices%29%2C%20from%20which%20it%20estimates%20the%20skeleton%20tree%20of%20the%0Acolony%20and%20extracts%20classical%20morphological%20measurements%20and%20advanced%20shape%0Afeatures%20of%20the%20individual%20corallites.%20CoDA%20also%20works%20as%20a%20proofreading%20and%0Aerror%20correction%20tool%20by%20helping%20to%20identify%20wrong%20parts%20in%20the%20skeleton%20tree%0Aand%20providing%20tools%20to%20quickly%20correct%20these%20errors.%20The%20final%20skeleton%20tree%0Aenables%20the%20derivation%20of%20additional%20information%20about%20the%20calices/corallite%0Ainstances%20that%20otherwise%20could%20not%20be%20obtained%2C%20including%20their%20ontogenetic%0Ageneration%20and%20branching%20patterns%20-%20the%20basis%20of%20a%20fully%20quantitative%0Astatistical%20analysis%20of%20the%20coral%20colony%20morphology.%20Part%20of%20CoDA%20is%20CoDAGraph%2C%0Aa%20feature-rich%20link-and-brush%20user%20interface%20for%20visualizing%20the%20extracted%0Afeatures%20and%202D%20graph%20layouts%20of%20the%20skeleton%20tree%2C%20enabling%20the%20real-time%0Aexploration%20of%20complex%20coral%20colonies%20and%20their%20building%20blocks%2C%20the%20individual%0Acorallites%20and%20branches.%0A%20%20In%20the%20future%2C%20we%20expect%20CoDA%20to%20greatly%20facilitate%20the%20analysis%20of%20large%0Astony%20corals%20of%20different%20species%20and%20morphotypes%2C%20as%20well%20as%20other%20dendroid%0Astructures%2C%20enabling%20new%20insights%20into%20the%20influence%20of%20genetic%20and%0Aenvironmental%20factors%20on%20their%20ontogenetic%20morphological%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18236v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoDA%253A%2520Interactive%2520Segmentation%2520and%2520Morphological%2520Analysis%2520of%2520Dendroid%250A%2520%2520Structures%2520Exemplified%2520on%2520Stony%2520Cold-Water%2520Corals%26entry.906535625%3DKira%2520Schmitt%2520and%2520J%25C3%25BCrgen%2520Titschack%2520and%2520Daniel%2520Baum%26entry.1292438233%3D%2520%2520Herein%252C%2520we%2520present%2520CoDA%252C%2520the%2520Coral%2520Dendroid%2520structure%2520Analyzer%252C%2520a%2520visual%250Aanalytics%2520suite%2520that%2520allows%2520for%2520the%2520first%2520time%2520to%2520investigate%2520the%2520ontogenetic%250Amorphological%2520development%2520of%2520complex%2520dendroid%2520coral%2520colonies%252C%2520exemplified%2520on%250Athree%2520important%2520framework-forming%2520dendroid%2520cold-water%2520corals%253A%2520Lophelia%2520pertusa%250A%2528Linnaeus%252C%25201758%2529%252C%2520Madrepora%2520oculata%2520%2528Linnaeus%252C%25201758%2529%252C%2520and%2520Goniocorella%2520dumosa%250A%2528Alcock%252C%25201902%2529.%2520Input%2520to%2520CoDA%2520is%2520an%2520initial%2520instance%2520segmentation%2520of%2520the%2520coral%250Apolyp%2520cavities%2520%2528calices%2529%252C%2520from%2520which%2520it%2520estimates%2520the%2520skeleton%2520tree%2520of%2520the%250Acolony%2520and%2520extracts%2520classical%2520morphological%2520measurements%2520and%2520advanced%2520shape%250Afeatures%2520of%2520the%2520individual%2520corallites.%2520CoDA%2520also%2520works%2520as%2520a%2520proofreading%2520and%250Aerror%2520correction%2520tool%2520by%2520helping%2520to%2520identify%2520wrong%2520parts%2520in%2520the%2520skeleton%2520tree%250Aand%2520providing%2520tools%2520to%2520quickly%2520correct%2520these%2520errors.%2520The%2520final%2520skeleton%2520tree%250Aenables%2520the%2520derivation%2520of%2520additional%2520information%2520about%2520the%2520calices/corallite%250Ainstances%2520that%2520otherwise%2520could%2520not%2520be%2520obtained%252C%2520including%2520their%2520ontogenetic%250Ageneration%2520and%2520branching%2520patterns%2520-%2520the%2520basis%2520of%2520a%2520fully%2520quantitative%250Astatistical%2520analysis%2520of%2520the%2520coral%2520colony%2520morphology.%2520Part%2520of%2520CoDA%2520is%2520CoDAGraph%252C%250Aa%2520feature-rich%2520link-and-brush%2520user%2520interface%2520for%2520visualizing%2520the%2520extracted%250Afeatures%2520and%25202D%2520graph%2520layouts%2520of%2520the%2520skeleton%2520tree%252C%2520enabling%2520the%2520real-time%250Aexploration%2520of%2520complex%2520coral%2520colonies%2520and%2520their%2520building%2520blocks%252C%2520the%2520individual%250Acorallites%2520and%2520branches.%250A%2520%2520In%2520the%2520future%252C%2520we%2520expect%2520CoDA%2520to%2520greatly%2520facilitate%2520the%2520analysis%2520of%2520large%250Astony%2520corals%2520of%2520different%2520species%2520and%2520morphotypes%252C%2520as%2520well%2520as%2520other%2520dendroid%250Astructures%252C%2520enabling%2520new%2520insights%2520into%2520the%2520influence%2520of%2520genetic%2520and%250Aenvironmental%2520factors%2520on%2520their%2520ontogenetic%2520morphological%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18236v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoDA%3A%20Interactive%20Segmentation%20and%20Morphological%20Analysis%20of%20Dendroid%0A%20%20Structures%20Exemplified%20on%20Stony%20Cold-Water%20Corals&entry.906535625=Kira%20Schmitt%20and%20J%C3%BCrgen%20Titschack%20and%20Daniel%20Baum&entry.1292438233=%20%20Herein%2C%20we%20present%20CoDA%2C%20the%20Coral%20Dendroid%20structure%20Analyzer%2C%20a%20visual%0Aanalytics%20suite%20that%20allows%20for%20the%20first%20time%20to%20investigate%20the%20ontogenetic%0Amorphological%20development%20of%20complex%20dendroid%20coral%20colonies%2C%20exemplified%20on%0Athree%20important%20framework-forming%20dendroid%20cold-water%20corals%3A%20Lophelia%20pertusa%0A%28Linnaeus%2C%201758%29%2C%20Madrepora%20oculata%20%28Linnaeus%2C%201758%29%2C%20and%20Goniocorella%20dumosa%0A%28Alcock%2C%201902%29.%20Input%20to%20CoDA%20is%20an%20initial%20instance%20segmentation%20of%20the%20coral%0Apolyp%20cavities%20%28calices%29%2C%20from%20which%20it%20estimates%20the%20skeleton%20tree%20of%20the%0Acolony%20and%20extracts%20classical%20morphological%20measurements%20and%20advanced%20shape%0Afeatures%20of%20the%20individual%20corallites.%20CoDA%20also%20works%20as%20a%20proofreading%20and%0Aerror%20correction%20tool%20by%20helping%20to%20identify%20wrong%20parts%20in%20the%20skeleton%20tree%0Aand%20providing%20tools%20to%20quickly%20correct%20these%20errors.%20The%20final%20skeleton%20tree%0Aenables%20the%20derivation%20of%20additional%20information%20about%20the%20calices/corallite%0Ainstances%20that%20otherwise%20could%20not%20be%20obtained%2C%20including%20their%20ontogenetic%0Ageneration%20and%20branching%20patterns%20-%20the%20basis%20of%20a%20fully%20quantitative%0Astatistical%20analysis%20of%20the%20coral%20colony%20morphology.%20Part%20of%20CoDA%20is%20CoDAGraph%2C%0Aa%20feature-rich%20link-and-brush%20user%20interface%20for%20visualizing%20the%20extracted%0Afeatures%20and%202D%20graph%20layouts%20of%20the%20skeleton%20tree%2C%20enabling%20the%20real-time%0Aexploration%20of%20complex%20coral%20colonies%20and%20their%20building%20blocks%2C%20the%20individual%0Acorallites%20and%20branches.%0A%20%20In%20the%20future%2C%20we%20expect%20CoDA%20to%20greatly%20facilitate%20the%20analysis%20of%20large%0Astony%20corals%20of%20different%20species%20and%20morphotypes%2C%20as%20well%20as%20other%20dendroid%0Astructures%2C%20enabling%20new%20insights%20into%20the%20influence%20of%20genetic%20and%0Aenvironmental%20factors%20on%20their%20ontogenetic%20morphological%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18236v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


