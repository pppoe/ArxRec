<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240912.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "VI3DRM:Towards meticulous 3D Reconstruction from Sparse Views via\n  Photo-Realistic Novel View Synthesis", "author": "Hao Chen and Jiafu Wu and Ying Jin and Jinlong Peng and Xiaofeng Mao and Mingmin Chi and Mufeng Yao and Bo Peng and Jian Li and Yun Cao", "abstract": "  Recently, methods like Zero-1-2-3 have focused on single-view based 3D\nreconstruction and have achieved remarkable success. However, their predictions\nfor unseen areas heavily rely on the inductive bias of large-scale pretrained\ndiffusion models. Although subsequent work, such as DreamComposer, attempts to\nmake predictions more controllable by incorporating additional views, the\nresults remain unrealistic due to feature entanglement in the vanilla latent\nspace, including factors such as lighting, material, and structure. To address\nthese issues, we introduce the Visual Isotropy 3D Reconstruction Model\n(VI3DRM), a diffusion-based sparse views 3D reconstruction model that operates\nwithin an ID consistent and perspective-disentangled 3D latent space. By\nfacilitating the disentanglement of semantic information, color, material\nproperties and lighting, VI3DRM is capable of generating highly realistic\nimages that are indistinguishable from real photographs. By leveraging both\nreal and synthesized images, our approach enables the accurate construction of\npointmaps, ultimately producing finely textured meshes or point clouds. On the\nNVS task, tested on the GSO dataset, VI3DRM significantly outperforms\nstate-of-the-art method DreamComposer, achieving a PSNR of 38.61, an SSIM of\n0.929, and an LPIPS of 0.027. Code will be made available upon publication.\n", "link": "http://arxiv.org/abs/2409.08207v1", "date": "2024-09-12", "relevancy": 3.4941, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7195}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7195}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VI3DRM%3ATowards%20meticulous%203D%20Reconstruction%20from%20Sparse%20Views%20via%0A%20%20Photo-Realistic%20Novel%20View%20Synthesis&body=Title%3A%20VI3DRM%3ATowards%20meticulous%203D%20Reconstruction%20from%20Sparse%20Views%20via%0A%20%20Photo-Realistic%20Novel%20View%20Synthesis%0AAuthor%3A%20Hao%20Chen%20and%20Jiafu%20Wu%20and%20Ying%20Jin%20and%20Jinlong%20Peng%20and%20Xiaofeng%20Mao%20and%20Mingmin%20Chi%20and%20Mufeng%20Yao%20and%20Bo%20Peng%20and%20Jian%20Li%20and%20Yun%20Cao%0AAbstract%3A%20%20%20Recently%2C%20methods%20like%20Zero-1-2-3%20have%20focused%20on%20single-view%20based%203D%0Areconstruction%20and%20have%20achieved%20remarkable%20success.%20However%2C%20their%20predictions%0Afor%20unseen%20areas%20heavily%20rely%20on%20the%20inductive%20bias%20of%20large-scale%20pretrained%0Adiffusion%20models.%20Although%20subsequent%20work%2C%20such%20as%20DreamComposer%2C%20attempts%20to%0Amake%20predictions%20more%20controllable%20by%20incorporating%20additional%20views%2C%20the%0Aresults%20remain%20unrealistic%20due%20to%20feature%20entanglement%20in%20the%20vanilla%20latent%0Aspace%2C%20including%20factors%20such%20as%20lighting%2C%20material%2C%20and%20structure.%20To%20address%0Athese%20issues%2C%20we%20introduce%20the%20Visual%20Isotropy%203D%20Reconstruction%20Model%0A%28VI3DRM%29%2C%20a%20diffusion-based%20sparse%20views%203D%20reconstruction%20model%20that%20operates%0Awithin%20an%20ID%20consistent%20and%20perspective-disentangled%203D%20latent%20space.%20By%0Afacilitating%20the%20disentanglement%20of%20semantic%20information%2C%20color%2C%20material%0Aproperties%20and%20lighting%2C%20VI3DRM%20is%20capable%20of%20generating%20highly%20realistic%0Aimages%20that%20are%20indistinguishable%20from%20real%20photographs.%20By%20leveraging%20both%0Areal%20and%20synthesized%20images%2C%20our%20approach%20enables%20the%20accurate%20construction%20of%0Apointmaps%2C%20ultimately%20producing%20finely%20textured%20meshes%20or%20point%20clouds.%20On%20the%0ANVS%20task%2C%20tested%20on%20the%20GSO%20dataset%2C%20VI3DRM%20significantly%20outperforms%0Astate-of-the-art%20method%20DreamComposer%2C%20achieving%20a%20PSNR%20of%2038.61%2C%20an%20SSIM%20of%0A0.929%2C%20and%20an%20LPIPS%20of%200.027.%20Code%20will%20be%20made%20available%20upon%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08207v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVI3DRM%253ATowards%2520meticulous%25203D%2520Reconstruction%2520from%2520Sparse%2520Views%2520via%250A%2520%2520Photo-Realistic%2520Novel%2520View%2520Synthesis%26entry.906535625%3DHao%2520Chen%2520and%2520Jiafu%2520Wu%2520and%2520Ying%2520Jin%2520and%2520Jinlong%2520Peng%2520and%2520Xiaofeng%2520Mao%2520and%2520Mingmin%2520Chi%2520and%2520Mufeng%2520Yao%2520and%2520Bo%2520Peng%2520and%2520Jian%2520Li%2520and%2520Yun%2520Cao%26entry.1292438233%3D%2520%2520Recently%252C%2520methods%2520like%2520Zero-1-2-3%2520have%2520focused%2520on%2520single-view%2520based%25203D%250Areconstruction%2520and%2520have%2520achieved%2520remarkable%2520success.%2520However%252C%2520their%2520predictions%250Afor%2520unseen%2520areas%2520heavily%2520rely%2520on%2520the%2520inductive%2520bias%2520of%2520large-scale%2520pretrained%250Adiffusion%2520models.%2520Although%2520subsequent%2520work%252C%2520such%2520as%2520DreamComposer%252C%2520attempts%2520to%250Amake%2520predictions%2520more%2520controllable%2520by%2520incorporating%2520additional%2520views%252C%2520the%250Aresults%2520remain%2520unrealistic%2520due%2520to%2520feature%2520entanglement%2520in%2520the%2520vanilla%2520latent%250Aspace%252C%2520including%2520factors%2520such%2520as%2520lighting%252C%2520material%252C%2520and%2520structure.%2520To%2520address%250Athese%2520issues%252C%2520we%2520introduce%2520the%2520Visual%2520Isotropy%25203D%2520Reconstruction%2520Model%250A%2528VI3DRM%2529%252C%2520a%2520diffusion-based%2520sparse%2520views%25203D%2520reconstruction%2520model%2520that%2520operates%250Awithin%2520an%2520ID%2520consistent%2520and%2520perspective-disentangled%25203D%2520latent%2520space.%2520By%250Afacilitating%2520the%2520disentanglement%2520of%2520semantic%2520information%252C%2520color%252C%2520material%250Aproperties%2520and%2520lighting%252C%2520VI3DRM%2520is%2520capable%2520of%2520generating%2520highly%2520realistic%250Aimages%2520that%2520are%2520indistinguishable%2520from%2520real%2520photographs.%2520By%2520leveraging%2520both%250Areal%2520and%2520synthesized%2520images%252C%2520our%2520approach%2520enables%2520the%2520accurate%2520construction%2520of%250Apointmaps%252C%2520ultimately%2520producing%2520finely%2520textured%2520meshes%2520or%2520point%2520clouds.%2520On%2520the%250ANVS%2520task%252C%2520tested%2520on%2520the%2520GSO%2520dataset%252C%2520VI3DRM%2520significantly%2520outperforms%250Astate-of-the-art%2520method%2520DreamComposer%252C%2520achieving%2520a%2520PSNR%2520of%252038.61%252C%2520an%2520SSIM%2520of%250A0.929%252C%2520and%2520an%2520LPIPS%2520of%25200.027.%2520Code%2520will%2520be%2520made%2520available%2520upon%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08207v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VI3DRM%3ATowards%20meticulous%203D%20Reconstruction%20from%20Sparse%20Views%20via%0A%20%20Photo-Realistic%20Novel%20View%20Synthesis&entry.906535625=Hao%20Chen%20and%20Jiafu%20Wu%20and%20Ying%20Jin%20and%20Jinlong%20Peng%20and%20Xiaofeng%20Mao%20and%20Mingmin%20Chi%20and%20Mufeng%20Yao%20and%20Bo%20Peng%20and%20Jian%20Li%20and%20Yun%20Cao&entry.1292438233=%20%20Recently%2C%20methods%20like%20Zero-1-2-3%20have%20focused%20on%20single-view%20based%203D%0Areconstruction%20and%20have%20achieved%20remarkable%20success.%20However%2C%20their%20predictions%0Afor%20unseen%20areas%20heavily%20rely%20on%20the%20inductive%20bias%20of%20large-scale%20pretrained%0Adiffusion%20models.%20Although%20subsequent%20work%2C%20such%20as%20DreamComposer%2C%20attempts%20to%0Amake%20predictions%20more%20controllable%20by%20incorporating%20additional%20views%2C%20the%0Aresults%20remain%20unrealistic%20due%20to%20feature%20entanglement%20in%20the%20vanilla%20latent%0Aspace%2C%20including%20factors%20such%20as%20lighting%2C%20material%2C%20and%20structure.%20To%20address%0Athese%20issues%2C%20we%20introduce%20the%20Visual%20Isotropy%203D%20Reconstruction%20Model%0A%28VI3DRM%29%2C%20a%20diffusion-based%20sparse%20views%203D%20reconstruction%20model%20that%20operates%0Awithin%20an%20ID%20consistent%20and%20perspective-disentangled%203D%20latent%20space.%20By%0Afacilitating%20the%20disentanglement%20of%20semantic%20information%2C%20color%2C%20material%0Aproperties%20and%20lighting%2C%20VI3DRM%20is%20capable%20of%20generating%20highly%20realistic%0Aimages%20that%20are%20indistinguishable%20from%20real%20photographs.%20By%20leveraging%20both%0Areal%20and%20synthesized%20images%2C%20our%20approach%20enables%20the%20accurate%20construction%20of%0Apointmaps%2C%20ultimately%20producing%20finely%20textured%20meshes%20or%20point%20clouds.%20On%20the%0ANVS%20task%2C%20tested%20on%20the%20GSO%20dataset%2C%20VI3DRM%20significantly%20outperforms%0Astate-of-the-art%20method%20DreamComposer%2C%20achieving%20a%20PSNR%20of%2038.61%2C%20an%20SSIM%20of%0A0.929%2C%20and%20an%20LPIPS%20of%200.027.%20Code%20will%20be%20made%20available%20upon%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08207v1&entry.124074799=Read"},
{"title": "Gaussian Garments: Reconstructing Simulation-Ready Clothing with\n  Photorealistic Appearance from Multi-View Video", "author": "Boxiang Rong and Artur Grigorev and Wenbo Wang and Michael J. Black and Bernhard Thomaszewski and Christina Tsalicoglou and Otmar Hilliges", "abstract": "  We introduce Gaussian Garments, a novel approach for reconstructing realistic\nsimulation-ready garment assets from multi-view videos. Our method represents\ngarments with a combination of a 3D mesh and a Gaussian texture that encodes\nboth the color and high-frequency surface details. This representation enables\naccurate registration of garment geometries to multi-view videos and helps\ndisentangle albedo textures from lighting effects. Furthermore, we demonstrate\nhow a pre-trained graph neural network (GNN) can be fine-tuned to replicate the\nreal behavior of each garment. The reconstructed Gaussian Garments can be\nautomatically combined into multi-garment outfits and animated with the\nfine-tuned GNN.\n", "link": "http://arxiv.org/abs/2409.08189v1", "date": "2024-09-12", "relevancy": 3.202, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6793}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.624}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6179}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Garments%3A%20Reconstructing%20Simulation-Ready%20Clothing%20with%0A%20%20Photorealistic%20Appearance%20from%20Multi-View%20Video&body=Title%3A%20Gaussian%20Garments%3A%20Reconstructing%20Simulation-Ready%20Clothing%20with%0A%20%20Photorealistic%20Appearance%20from%20Multi-View%20Video%0AAuthor%3A%20Boxiang%20Rong%20and%20Artur%20Grigorev%20and%20Wenbo%20Wang%20and%20Michael%20J.%20Black%20and%20Bernhard%20Thomaszewski%20and%20Christina%20Tsalicoglou%20and%20Otmar%20Hilliges%0AAbstract%3A%20%20%20We%20introduce%20Gaussian%20Garments%2C%20a%20novel%20approach%20for%20reconstructing%20realistic%0Asimulation-ready%20garment%20assets%20from%20multi-view%20videos.%20Our%20method%20represents%0Agarments%20with%20a%20combination%20of%20a%203D%20mesh%20and%20a%20Gaussian%20texture%20that%20encodes%0Aboth%20the%20color%20and%20high-frequency%20surface%20details.%20This%20representation%20enables%0Aaccurate%20registration%20of%20garment%20geometries%20to%20multi-view%20videos%20and%20helps%0Adisentangle%20albedo%20textures%20from%20lighting%20effects.%20Furthermore%2C%20we%20demonstrate%0Ahow%20a%20pre-trained%20graph%20neural%20network%20%28GNN%29%20can%20be%20fine-tuned%20to%20replicate%20the%0Areal%20behavior%20of%20each%20garment.%20The%20reconstructed%20Gaussian%20Garments%20can%20be%0Aautomatically%20combined%20into%20multi-garment%20outfits%20and%20animated%20with%20the%0Afine-tuned%20GNN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08189v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Garments%253A%2520Reconstructing%2520Simulation-Ready%2520Clothing%2520with%250A%2520%2520Photorealistic%2520Appearance%2520from%2520Multi-View%2520Video%26entry.906535625%3DBoxiang%2520Rong%2520and%2520Artur%2520Grigorev%2520and%2520Wenbo%2520Wang%2520and%2520Michael%2520J.%2520Black%2520and%2520Bernhard%2520Thomaszewski%2520and%2520Christina%2520Tsalicoglou%2520and%2520Otmar%2520Hilliges%26entry.1292438233%3D%2520%2520We%2520introduce%2520Gaussian%2520Garments%252C%2520a%2520novel%2520approach%2520for%2520reconstructing%2520realistic%250Asimulation-ready%2520garment%2520assets%2520from%2520multi-view%2520videos.%2520Our%2520method%2520represents%250Agarments%2520with%2520a%2520combination%2520of%2520a%25203D%2520mesh%2520and%2520a%2520Gaussian%2520texture%2520that%2520encodes%250Aboth%2520the%2520color%2520and%2520high-frequency%2520surface%2520details.%2520This%2520representation%2520enables%250Aaccurate%2520registration%2520of%2520garment%2520geometries%2520to%2520multi-view%2520videos%2520and%2520helps%250Adisentangle%2520albedo%2520textures%2520from%2520lighting%2520effects.%2520Furthermore%252C%2520we%2520demonstrate%250Ahow%2520a%2520pre-trained%2520graph%2520neural%2520network%2520%2528GNN%2529%2520can%2520be%2520fine-tuned%2520to%2520replicate%2520the%250Areal%2520behavior%2520of%2520each%2520garment.%2520The%2520reconstructed%2520Gaussian%2520Garments%2520can%2520be%250Aautomatically%2520combined%2520into%2520multi-garment%2520outfits%2520and%2520animated%2520with%2520the%250Afine-tuned%2520GNN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08189v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Garments%3A%20Reconstructing%20Simulation-Ready%20Clothing%20with%0A%20%20Photorealistic%20Appearance%20from%20Multi-View%20Video&entry.906535625=Boxiang%20Rong%20and%20Artur%20Grigorev%20and%20Wenbo%20Wang%20and%20Michael%20J.%20Black%20and%20Bernhard%20Thomaszewski%20and%20Christina%20Tsalicoglou%20and%20Otmar%20Hilliges&entry.1292438233=%20%20We%20introduce%20Gaussian%20Garments%2C%20a%20novel%20approach%20for%20reconstructing%20realistic%0Asimulation-ready%20garment%20assets%20from%20multi-view%20videos.%20Our%20method%20represents%0Agarments%20with%20a%20combination%20of%20a%203D%20mesh%20and%20a%20Gaussian%20texture%20that%20encodes%0Aboth%20the%20color%20and%20high-frequency%20surface%20details.%20This%20representation%20enables%0Aaccurate%20registration%20of%20garment%20geometries%20to%20multi-view%20videos%20and%20helps%0Adisentangle%20albedo%20textures%20from%20lighting%20effects.%20Furthermore%2C%20we%20demonstrate%0Ahow%20a%20pre-trained%20graph%20neural%20network%20%28GNN%29%20can%20be%20fine-tuned%20to%20replicate%20the%0Areal%20behavior%20of%20each%20garment.%20The%20reconstructed%20Gaussian%20Garments%20can%20be%0Aautomatically%20combined%20into%20multi-garment%20outfits%20and%20animated%20with%20the%0Afine-tuned%20GNN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08189v1&entry.124074799=Read"},
{"title": "Thermal3D-GS: Physics-induced 3D Gaussians for Thermal Infrared\n  Novel-view Synthesis", "author": "Qian Chen and Shihao Shu and Xiangzhi Bai", "abstract": "  Novel-view synthesis based on visible light has been extensively studied. In\ncomparison to visible light imaging, thermal infrared imaging offers the\nadvantage of all-weather imaging and strong penetration, providing increased\npossibilities for reconstruction in nighttime and adverse weather scenarios.\nHowever, thermal infrared imaging is influenced by physical characteristics\nsuch as atmospheric transmission effects and thermal conduction, hindering the\nprecise reconstruction of intricate details in thermal infrared scenes,\nmanifesting as issues of floaters and indistinct edge features in synthesized\nimages. To address these limitations, this paper introduces a physics-induced\n3D Gaussian splatting method named Thermal3D-GS. Thermal3D-GS begins by\nmodeling atmospheric transmission effects and thermal conduction in\nthree-dimensional media using neural networks. Additionally, a temperature\nconsistency constraint is incorporated into the optimization objective to\nenhance the reconstruction accuracy of thermal infrared images. Furthermore, to\nvalidate the effectiveness of our method, the first large-scale benchmark\ndataset for this field named Thermal Infrared Novel-view Synthesis Dataset\n(TI-NSD) is created. This dataset comprises 20 authentic thermal infrared video\nscenes, covering indoor, outdoor, and UAV(Unmanned Aerial Vehicle) scenarios,\ntotaling 6,664 frames of thermal infrared image data. Based on this dataset,\nthis paper experimentally verifies the effectiveness of Thermal3D-GS. The\nresults indicate that our method outperforms the baseline method with a 3.03 dB\nimprovement in PSNR and significantly addresses the issues of floaters and\nindistinct edge features present in the baseline method. Our dataset and\ncodebase will be released in\n\\href{https://github.com/mzzcdf/Thermal3DGS}{\\textcolor{red}{Thermal3DGS}}.\n", "link": "http://arxiv.org/abs/2409.08042v1", "date": "2024-09-12", "relevancy": 3.1784, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6712}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6216}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Thermal3D-GS%3A%20Physics-induced%203D%20Gaussians%20for%20Thermal%20Infrared%0A%20%20Novel-view%20Synthesis&body=Title%3A%20Thermal3D-GS%3A%20Physics-induced%203D%20Gaussians%20for%20Thermal%20Infrared%0A%20%20Novel-view%20Synthesis%0AAuthor%3A%20Qian%20Chen%20and%20Shihao%20Shu%20and%20Xiangzhi%20Bai%0AAbstract%3A%20%20%20Novel-view%20synthesis%20based%20on%20visible%20light%20has%20been%20extensively%20studied.%20In%0Acomparison%20to%20visible%20light%20imaging%2C%20thermal%20infrared%20imaging%20offers%20the%0Aadvantage%20of%20all-weather%20imaging%20and%20strong%20penetration%2C%20providing%20increased%0Apossibilities%20for%20reconstruction%20in%20nighttime%20and%20adverse%20weather%20scenarios.%0AHowever%2C%20thermal%20infrared%20imaging%20is%20influenced%20by%20physical%20characteristics%0Asuch%20as%20atmospheric%20transmission%20effects%20and%20thermal%20conduction%2C%20hindering%20the%0Aprecise%20reconstruction%20of%20intricate%20details%20in%20thermal%20infrared%20scenes%2C%0Amanifesting%20as%20issues%20of%20floaters%20and%20indistinct%20edge%20features%20in%20synthesized%0Aimages.%20To%20address%20these%20limitations%2C%20this%20paper%20introduces%20a%20physics-induced%0A3D%20Gaussian%20splatting%20method%20named%20Thermal3D-GS.%20Thermal3D-GS%20begins%20by%0Amodeling%20atmospheric%20transmission%20effects%20and%20thermal%20conduction%20in%0Athree-dimensional%20media%20using%20neural%20networks.%20Additionally%2C%20a%20temperature%0Aconsistency%20constraint%20is%20incorporated%20into%20the%20optimization%20objective%20to%0Aenhance%20the%20reconstruction%20accuracy%20of%20thermal%20infrared%20images.%20Furthermore%2C%20to%0Avalidate%20the%20effectiveness%20of%20our%20method%2C%20the%20first%20large-scale%20benchmark%0Adataset%20for%20this%20field%20named%20Thermal%20Infrared%20Novel-view%20Synthesis%20Dataset%0A%28TI-NSD%29%20is%20created.%20This%20dataset%20comprises%2020%20authentic%20thermal%20infrared%20video%0Ascenes%2C%20covering%20indoor%2C%20outdoor%2C%20and%20UAV%28Unmanned%20Aerial%20Vehicle%29%20scenarios%2C%0Atotaling%206%2C664%20frames%20of%20thermal%20infrared%20image%20data.%20Based%20on%20this%20dataset%2C%0Athis%20paper%20experimentally%20verifies%20the%20effectiveness%20of%20Thermal3D-GS.%20The%0Aresults%20indicate%20that%20our%20method%20outperforms%20the%20baseline%20method%20with%20a%203.03%20dB%0Aimprovement%20in%20PSNR%20and%20significantly%20addresses%20the%20issues%20of%20floaters%20and%0Aindistinct%20edge%20features%20present%20in%20the%20baseline%20method.%20Our%20dataset%20and%0Acodebase%20will%20be%20released%20in%0A%5Chref%7Bhttps%3A//github.com/mzzcdf/Thermal3DGS%7D%7B%5Ctextcolor%7Bred%7D%7BThermal3DGS%7D%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08042v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThermal3D-GS%253A%2520Physics-induced%25203D%2520Gaussians%2520for%2520Thermal%2520Infrared%250A%2520%2520Novel-view%2520Synthesis%26entry.906535625%3DQian%2520Chen%2520and%2520Shihao%2520Shu%2520and%2520Xiangzhi%2520Bai%26entry.1292438233%3D%2520%2520Novel-view%2520synthesis%2520based%2520on%2520visible%2520light%2520has%2520been%2520extensively%2520studied.%2520In%250Acomparison%2520to%2520visible%2520light%2520imaging%252C%2520thermal%2520infrared%2520imaging%2520offers%2520the%250Aadvantage%2520of%2520all-weather%2520imaging%2520and%2520strong%2520penetration%252C%2520providing%2520increased%250Apossibilities%2520for%2520reconstruction%2520in%2520nighttime%2520and%2520adverse%2520weather%2520scenarios.%250AHowever%252C%2520thermal%2520infrared%2520imaging%2520is%2520influenced%2520by%2520physical%2520characteristics%250Asuch%2520as%2520atmospheric%2520transmission%2520effects%2520and%2520thermal%2520conduction%252C%2520hindering%2520the%250Aprecise%2520reconstruction%2520of%2520intricate%2520details%2520in%2520thermal%2520infrared%2520scenes%252C%250Amanifesting%2520as%2520issues%2520of%2520floaters%2520and%2520indistinct%2520edge%2520features%2520in%2520synthesized%250Aimages.%2520To%2520address%2520these%2520limitations%252C%2520this%2520paper%2520introduces%2520a%2520physics-induced%250A3D%2520Gaussian%2520splatting%2520method%2520named%2520Thermal3D-GS.%2520Thermal3D-GS%2520begins%2520by%250Amodeling%2520atmospheric%2520transmission%2520effects%2520and%2520thermal%2520conduction%2520in%250Athree-dimensional%2520media%2520using%2520neural%2520networks.%2520Additionally%252C%2520a%2520temperature%250Aconsistency%2520constraint%2520is%2520incorporated%2520into%2520the%2520optimization%2520objective%2520to%250Aenhance%2520the%2520reconstruction%2520accuracy%2520of%2520thermal%2520infrared%2520images.%2520Furthermore%252C%2520to%250Avalidate%2520the%2520effectiveness%2520of%2520our%2520method%252C%2520the%2520first%2520large-scale%2520benchmark%250Adataset%2520for%2520this%2520field%2520named%2520Thermal%2520Infrared%2520Novel-view%2520Synthesis%2520Dataset%250A%2528TI-NSD%2529%2520is%2520created.%2520This%2520dataset%2520comprises%252020%2520authentic%2520thermal%2520infrared%2520video%250Ascenes%252C%2520covering%2520indoor%252C%2520outdoor%252C%2520and%2520UAV%2528Unmanned%2520Aerial%2520Vehicle%2529%2520scenarios%252C%250Atotaling%25206%252C664%2520frames%2520of%2520thermal%2520infrared%2520image%2520data.%2520Based%2520on%2520this%2520dataset%252C%250Athis%2520paper%2520experimentally%2520verifies%2520the%2520effectiveness%2520of%2520Thermal3D-GS.%2520The%250Aresults%2520indicate%2520that%2520our%2520method%2520outperforms%2520the%2520baseline%2520method%2520with%2520a%25203.03%2520dB%250Aimprovement%2520in%2520PSNR%2520and%2520significantly%2520addresses%2520the%2520issues%2520of%2520floaters%2520and%250Aindistinct%2520edge%2520features%2520present%2520in%2520the%2520baseline%2520method.%2520Our%2520dataset%2520and%250Acodebase%2520will%2520be%2520released%2520in%250A%255Chref%257Bhttps%253A//github.com/mzzcdf/Thermal3DGS%257D%257B%255Ctextcolor%257Bred%257D%257BThermal3DGS%257D%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08042v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Thermal3D-GS%3A%20Physics-induced%203D%20Gaussians%20for%20Thermal%20Infrared%0A%20%20Novel-view%20Synthesis&entry.906535625=Qian%20Chen%20and%20Shihao%20Shu%20and%20Xiangzhi%20Bai&entry.1292438233=%20%20Novel-view%20synthesis%20based%20on%20visible%20light%20has%20been%20extensively%20studied.%20In%0Acomparison%20to%20visible%20light%20imaging%2C%20thermal%20infrared%20imaging%20offers%20the%0Aadvantage%20of%20all-weather%20imaging%20and%20strong%20penetration%2C%20providing%20increased%0Apossibilities%20for%20reconstruction%20in%20nighttime%20and%20adverse%20weather%20scenarios.%0AHowever%2C%20thermal%20infrared%20imaging%20is%20influenced%20by%20physical%20characteristics%0Asuch%20as%20atmospheric%20transmission%20effects%20and%20thermal%20conduction%2C%20hindering%20the%0Aprecise%20reconstruction%20of%20intricate%20details%20in%20thermal%20infrared%20scenes%2C%0Amanifesting%20as%20issues%20of%20floaters%20and%20indistinct%20edge%20features%20in%20synthesized%0Aimages.%20To%20address%20these%20limitations%2C%20this%20paper%20introduces%20a%20physics-induced%0A3D%20Gaussian%20splatting%20method%20named%20Thermal3D-GS.%20Thermal3D-GS%20begins%20by%0Amodeling%20atmospheric%20transmission%20effects%20and%20thermal%20conduction%20in%0Athree-dimensional%20media%20using%20neural%20networks.%20Additionally%2C%20a%20temperature%0Aconsistency%20constraint%20is%20incorporated%20into%20the%20optimization%20objective%20to%0Aenhance%20the%20reconstruction%20accuracy%20of%20thermal%20infrared%20images.%20Furthermore%2C%20to%0Avalidate%20the%20effectiveness%20of%20our%20method%2C%20the%20first%20large-scale%20benchmark%0Adataset%20for%20this%20field%20named%20Thermal%20Infrared%20Novel-view%20Synthesis%20Dataset%0A%28TI-NSD%29%20is%20created.%20This%20dataset%20comprises%2020%20authentic%20thermal%20infrared%20video%0Ascenes%2C%20covering%20indoor%2C%20outdoor%2C%20and%20UAV%28Unmanned%20Aerial%20Vehicle%29%20scenarios%2C%0Atotaling%206%2C664%20frames%20of%20thermal%20infrared%20image%20data.%20Based%20on%20this%20dataset%2C%0Athis%20paper%20experimentally%20verifies%20the%20effectiveness%20of%20Thermal3D-GS.%20The%0Aresults%20indicate%20that%20our%20method%20outperforms%20the%20baseline%20method%20with%20a%203.03%20dB%0Aimprovement%20in%20PSNR%20and%20significantly%20addresses%20the%20issues%20of%20floaters%20and%0Aindistinct%20edge%20features%20present%20in%20the%20baseline%20method.%20Our%20dataset%20and%0Acodebase%20will%20be%20released%20in%0A%5Chref%7Bhttps%3A//github.com/mzzcdf/Thermal3DGS%7D%7B%5Ctextcolor%7Bred%7D%7BThermal3DGS%7D%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08042v1&entry.124074799=Read"},
{"title": "DriveScape: Towards High-Resolution Controllable Multi-View Driving\n  Video Generation", "author": "Wei Wu and Xi Guo and Weixuan Tang and Tingxuan Huang and Chiyu Wang and Dongyue Chen and Chenjing Ding", "abstract": "  Recent advancements in generative models have provided promising solutions\nfor synthesizing realistic driving videos, which are crucial for training\nautonomous driving perception models. However, existing approaches often\nstruggle with multi-view video generation due to the challenges of integrating\n3D information while maintaining spatial-temporal consistency and effectively\nlearning from a unified model. We propose DriveScape, an end-to-end framework\nfor multi-view, 3D condition-guided video generation, capable of producing 1024\nx 576 high-resolution videos at 10Hz. Unlike other methods limited to 2Hz due\nto the 3D box annotation frame rate, DriveScape overcomes this with its ability\nto operate under sparse conditions. Our Bi-Directional Modulated Transformer\n(BiMot) ensures precise alignment of 3D structural information, maintaining\nspatial-temporal consistency. DriveScape excels in video generation\nperformance, achieving state-of-the-art results on the nuScenes dataset with an\nFID score of 8.34 and an FVD score of 76.39. Our project homepage:\nhttps://metadrivescape.github.io/papers_project/drivescapev1/index.html\n", "link": "http://arxiv.org/abs/2409.05463v4", "date": "2024-09-12", "relevancy": 3.1118, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6357}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6357}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5958}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DriveScape%3A%20Towards%20High-Resolution%20Controllable%20Multi-View%20Driving%0A%20%20Video%20Generation&body=Title%3A%20DriveScape%3A%20Towards%20High-Resolution%20Controllable%20Multi-View%20Driving%0A%20%20Video%20Generation%0AAuthor%3A%20Wei%20Wu%20and%20Xi%20Guo%20and%20Weixuan%20Tang%20and%20Tingxuan%20Huang%20and%20Chiyu%20Wang%20and%20Dongyue%20Chen%20and%20Chenjing%20Ding%0AAbstract%3A%20%20%20Recent%20advancements%20in%20generative%20models%20have%20provided%20promising%20solutions%0Afor%20synthesizing%20realistic%20driving%20videos%2C%20which%20are%20crucial%20for%20training%0Aautonomous%20driving%20perception%20models.%20However%2C%20existing%20approaches%20often%0Astruggle%20with%20multi-view%20video%20generation%20due%20to%20the%20challenges%20of%20integrating%0A3D%20information%20while%20maintaining%20spatial-temporal%20consistency%20and%20effectively%0Alearning%20from%20a%20unified%20model.%20We%20propose%20DriveScape%2C%20an%20end-to-end%20framework%0Afor%20multi-view%2C%203D%20condition-guided%20video%20generation%2C%20capable%20of%20producing%201024%0Ax%20576%20high-resolution%20videos%20at%2010Hz.%20Unlike%20other%20methods%20limited%20to%202Hz%20due%0Ato%20the%203D%20box%20annotation%20frame%20rate%2C%20DriveScape%20overcomes%20this%20with%20its%20ability%0Ato%20operate%20under%20sparse%20conditions.%20Our%20Bi-Directional%20Modulated%20Transformer%0A%28BiMot%29%20ensures%20precise%20alignment%20of%203D%20structural%20information%2C%20maintaining%0Aspatial-temporal%20consistency.%20DriveScape%20excels%20in%20video%20generation%0Aperformance%2C%20achieving%20state-of-the-art%20results%20on%20the%20nuScenes%20dataset%20with%20an%0AFID%20score%20of%208.34%20and%20an%20FVD%20score%20of%2076.39.%20Our%20project%20homepage%3A%0Ahttps%3A//metadrivescape.github.io/papers_project/drivescapev1/index.html%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05463v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDriveScape%253A%2520Towards%2520High-Resolution%2520Controllable%2520Multi-View%2520Driving%250A%2520%2520Video%2520Generation%26entry.906535625%3DWei%2520Wu%2520and%2520Xi%2520Guo%2520and%2520Weixuan%2520Tang%2520and%2520Tingxuan%2520Huang%2520and%2520Chiyu%2520Wang%2520and%2520Dongyue%2520Chen%2520and%2520Chenjing%2520Ding%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520generative%2520models%2520have%2520provided%2520promising%2520solutions%250Afor%2520synthesizing%2520realistic%2520driving%2520videos%252C%2520which%2520are%2520crucial%2520for%2520training%250Aautonomous%2520driving%2520perception%2520models.%2520However%252C%2520existing%2520approaches%2520often%250Astruggle%2520with%2520multi-view%2520video%2520generation%2520due%2520to%2520the%2520challenges%2520of%2520integrating%250A3D%2520information%2520while%2520maintaining%2520spatial-temporal%2520consistency%2520and%2520effectively%250Alearning%2520from%2520a%2520unified%2520model.%2520We%2520propose%2520DriveScape%252C%2520an%2520end-to-end%2520framework%250Afor%2520multi-view%252C%25203D%2520condition-guided%2520video%2520generation%252C%2520capable%2520of%2520producing%25201024%250Ax%2520576%2520high-resolution%2520videos%2520at%252010Hz.%2520Unlike%2520other%2520methods%2520limited%2520to%25202Hz%2520due%250Ato%2520the%25203D%2520box%2520annotation%2520frame%2520rate%252C%2520DriveScape%2520overcomes%2520this%2520with%2520its%2520ability%250Ato%2520operate%2520under%2520sparse%2520conditions.%2520Our%2520Bi-Directional%2520Modulated%2520Transformer%250A%2528BiMot%2529%2520ensures%2520precise%2520alignment%2520of%25203D%2520structural%2520information%252C%2520maintaining%250Aspatial-temporal%2520consistency.%2520DriveScape%2520excels%2520in%2520video%2520generation%250Aperformance%252C%2520achieving%2520state-of-the-art%2520results%2520on%2520the%2520nuScenes%2520dataset%2520with%2520an%250AFID%2520score%2520of%25208.34%2520and%2520an%2520FVD%2520score%2520of%252076.39.%2520Our%2520project%2520homepage%253A%250Ahttps%253A//metadrivescape.github.io/papers_project/drivescapev1/index.html%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05463v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DriveScape%3A%20Towards%20High-Resolution%20Controllable%20Multi-View%20Driving%0A%20%20Video%20Generation&entry.906535625=Wei%20Wu%20and%20Xi%20Guo%20and%20Weixuan%20Tang%20and%20Tingxuan%20Huang%20and%20Chiyu%20Wang%20and%20Dongyue%20Chen%20and%20Chenjing%20Ding&entry.1292438233=%20%20Recent%20advancements%20in%20generative%20models%20have%20provided%20promising%20solutions%0Afor%20synthesizing%20realistic%20driving%20videos%2C%20which%20are%20crucial%20for%20training%0Aautonomous%20driving%20perception%20models.%20However%2C%20existing%20approaches%20often%0Astruggle%20with%20multi-view%20video%20generation%20due%20to%20the%20challenges%20of%20integrating%0A3D%20information%20while%20maintaining%20spatial-temporal%20consistency%20and%20effectively%0Alearning%20from%20a%20unified%20model.%20We%20propose%20DriveScape%2C%20an%20end-to-end%20framework%0Afor%20multi-view%2C%203D%20condition-guided%20video%20generation%2C%20capable%20of%20producing%201024%0Ax%20576%20high-resolution%20videos%20at%2010Hz.%20Unlike%20other%20methods%20limited%20to%202Hz%20due%0Ato%20the%203D%20box%20annotation%20frame%20rate%2C%20DriveScape%20overcomes%20this%20with%20its%20ability%0Ato%20operate%20under%20sparse%20conditions.%20Our%20Bi-Directional%20Modulated%20Transformer%0A%28BiMot%29%20ensures%20precise%20alignment%20of%203D%20structural%20information%2C%20maintaining%0Aspatial-temporal%20consistency.%20DriveScape%20excels%20in%20video%20generation%0Aperformance%2C%20achieving%20state-of-the-art%20results%20on%20the%20nuScenes%20dataset%20with%20an%0AFID%20score%20of%208.34%20and%20an%20FVD%20score%20of%2076.39.%20Our%20project%20homepage%3A%0Ahttps%3A//metadrivescape.github.io/papers_project/drivescapev1/index.html%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05463v4&entry.124074799=Read"},
{"title": "ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D\n  Facial Animation Synthesis Using VQ-VAE", "author": "Sichun Wu and Kazi Injamamul Haque and Zerrin Yumak", "abstract": "  Audio-driven 3D facial animation synthesis has been an active field of\nresearch with attention from both academia and industry. While there are\npromising results in this area, recent approaches largely focus on lip-sync and\nidentity control, neglecting the role of emotions and emotion control in the\ngenerative process. That is mainly due to the lack of emotionally rich facial\nanimation data and algorithms that can synthesize speech animations with\nemotional expressions at the same time. In addition, majority of the models are\ndeterministic, meaning given the same audio input, they produce the same output\nmotion. We argue that emotions and non-determinism are crucial to generate\ndiverse and emotionally-rich facial animations. In this paper, we propose\nProbTalk3D a non-deterministic neural network approach for emotion controllable\nspeech-driven 3D facial animation synthesis using a two-stage VQ-VAE model and\nan emotionally rich facial animation dataset 3DMEAD. We provide an extensive\ncomparative analysis of our model against the recent 3D facial animation\nsynthesis approaches, by evaluating the results objectively, qualitatively, and\nwith a perceptual user study. We highlight several objective metrics that are\nmore suitable for evaluating stochastic outputs and use both in-the-wild and\nground truth data for subjective evaluation. To our knowledge, that is the\nfirst non-deterministic 3D facial animation synthesis method incorporating a\nrich emotion dataset and emotion control with emotion labels and intensity\nlevels. Our evaluation demonstrates that the proposed model achieves superior\nperformance compared to state-of-the-art emotion-controlled, deterministic and\nnon-deterministic models. We recommend watching the supplementary video for\nquality judgement. The entire codebase is publicly available\n(https://github.com/uuembodiedsocialai/ProbTalk3D/).\n", "link": "http://arxiv.org/abs/2409.07966v1", "date": "2024-09-12", "relevancy": 2.9729, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5985}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5985}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProbTalk3D%3A%20Non-Deterministic%20Emotion%20Controllable%20Speech-Driven%203D%0A%20%20Facial%20Animation%20Synthesis%20Using%20VQ-VAE&body=Title%3A%20ProbTalk3D%3A%20Non-Deterministic%20Emotion%20Controllable%20Speech-Driven%203D%0A%20%20Facial%20Animation%20Synthesis%20Using%20VQ-VAE%0AAuthor%3A%20Sichun%20Wu%20and%20Kazi%20Injamamul%20Haque%20and%20Zerrin%20Yumak%0AAbstract%3A%20%20%20Audio-driven%203D%20facial%20animation%20synthesis%20has%20been%20an%20active%20field%20of%0Aresearch%20with%20attention%20from%20both%20academia%20and%20industry.%20While%20there%20are%0Apromising%20results%20in%20this%20area%2C%20recent%20approaches%20largely%20focus%20on%20lip-sync%20and%0Aidentity%20control%2C%20neglecting%20the%20role%20of%20emotions%20and%20emotion%20control%20in%20the%0Agenerative%20process.%20That%20is%20mainly%20due%20to%20the%20lack%20of%20emotionally%20rich%20facial%0Aanimation%20data%20and%20algorithms%20that%20can%20synthesize%20speech%20animations%20with%0Aemotional%20expressions%20at%20the%20same%20time.%20In%20addition%2C%20majority%20of%20the%20models%20are%0Adeterministic%2C%20meaning%20given%20the%20same%20audio%20input%2C%20they%20produce%20the%20same%20output%0Amotion.%20We%20argue%20that%20emotions%20and%20non-determinism%20are%20crucial%20to%20generate%0Adiverse%20and%20emotionally-rich%20facial%20animations.%20In%20this%20paper%2C%20we%20propose%0AProbTalk3D%20a%20non-deterministic%20neural%20network%20approach%20for%20emotion%20controllable%0Aspeech-driven%203D%20facial%20animation%20synthesis%20using%20a%20two-stage%20VQ-VAE%20model%20and%0Aan%20emotionally%20rich%20facial%20animation%20dataset%203DMEAD.%20We%20provide%20an%20extensive%0Acomparative%20analysis%20of%20our%20model%20against%20the%20recent%203D%20facial%20animation%0Asynthesis%20approaches%2C%20by%20evaluating%20the%20results%20objectively%2C%20qualitatively%2C%20and%0Awith%20a%20perceptual%20user%20study.%20We%20highlight%20several%20objective%20metrics%20that%20are%0Amore%20suitable%20for%20evaluating%20stochastic%20outputs%20and%20use%20both%20in-the-wild%20and%0Aground%20truth%20data%20for%20subjective%20evaluation.%20To%20our%20knowledge%2C%20that%20is%20the%0Afirst%20non-deterministic%203D%20facial%20animation%20synthesis%20method%20incorporating%20a%0Arich%20emotion%20dataset%20and%20emotion%20control%20with%20emotion%20labels%20and%20intensity%0Alevels.%20Our%20evaluation%20demonstrates%20that%20the%20proposed%20model%20achieves%20superior%0Aperformance%20compared%20to%20state-of-the-art%20emotion-controlled%2C%20deterministic%20and%0Anon-deterministic%20models.%20We%20recommend%20watching%20the%20supplementary%20video%20for%0Aquality%20judgement.%20The%20entire%20codebase%20is%20publicly%20available%0A%28https%3A//github.com/uuembodiedsocialai/ProbTalk3D/%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07966v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbTalk3D%253A%2520Non-Deterministic%2520Emotion%2520Controllable%2520Speech-Driven%25203D%250A%2520%2520Facial%2520Animation%2520Synthesis%2520Using%2520VQ-VAE%26entry.906535625%3DSichun%2520Wu%2520and%2520Kazi%2520Injamamul%2520Haque%2520and%2520Zerrin%2520Yumak%26entry.1292438233%3D%2520%2520Audio-driven%25203D%2520facial%2520animation%2520synthesis%2520has%2520been%2520an%2520active%2520field%2520of%250Aresearch%2520with%2520attention%2520from%2520both%2520academia%2520and%2520industry.%2520While%2520there%2520are%250Apromising%2520results%2520in%2520this%2520area%252C%2520recent%2520approaches%2520largely%2520focus%2520on%2520lip-sync%2520and%250Aidentity%2520control%252C%2520neglecting%2520the%2520role%2520of%2520emotions%2520and%2520emotion%2520control%2520in%2520the%250Agenerative%2520process.%2520That%2520is%2520mainly%2520due%2520to%2520the%2520lack%2520of%2520emotionally%2520rich%2520facial%250Aanimation%2520data%2520and%2520algorithms%2520that%2520can%2520synthesize%2520speech%2520animations%2520with%250Aemotional%2520expressions%2520at%2520the%2520same%2520time.%2520In%2520addition%252C%2520majority%2520of%2520the%2520models%2520are%250Adeterministic%252C%2520meaning%2520given%2520the%2520same%2520audio%2520input%252C%2520they%2520produce%2520the%2520same%2520output%250Amotion.%2520We%2520argue%2520that%2520emotions%2520and%2520non-determinism%2520are%2520crucial%2520to%2520generate%250Adiverse%2520and%2520emotionally-rich%2520facial%2520animations.%2520In%2520this%2520paper%252C%2520we%2520propose%250AProbTalk3D%2520a%2520non-deterministic%2520neural%2520network%2520approach%2520for%2520emotion%2520controllable%250Aspeech-driven%25203D%2520facial%2520animation%2520synthesis%2520using%2520a%2520two-stage%2520VQ-VAE%2520model%2520and%250Aan%2520emotionally%2520rich%2520facial%2520animation%2520dataset%25203DMEAD.%2520We%2520provide%2520an%2520extensive%250Acomparative%2520analysis%2520of%2520our%2520model%2520against%2520the%2520recent%25203D%2520facial%2520animation%250Asynthesis%2520approaches%252C%2520by%2520evaluating%2520the%2520results%2520objectively%252C%2520qualitatively%252C%2520and%250Awith%2520a%2520perceptual%2520user%2520study.%2520We%2520highlight%2520several%2520objective%2520metrics%2520that%2520are%250Amore%2520suitable%2520for%2520evaluating%2520stochastic%2520outputs%2520and%2520use%2520both%2520in-the-wild%2520and%250Aground%2520truth%2520data%2520for%2520subjective%2520evaluation.%2520To%2520our%2520knowledge%252C%2520that%2520is%2520the%250Afirst%2520non-deterministic%25203D%2520facial%2520animation%2520synthesis%2520method%2520incorporating%2520a%250Arich%2520emotion%2520dataset%2520and%2520emotion%2520control%2520with%2520emotion%2520labels%2520and%2520intensity%250Alevels.%2520Our%2520evaluation%2520demonstrates%2520that%2520the%2520proposed%2520model%2520achieves%2520superior%250Aperformance%2520compared%2520to%2520state-of-the-art%2520emotion-controlled%252C%2520deterministic%2520and%250Anon-deterministic%2520models.%2520We%2520recommend%2520watching%2520the%2520supplementary%2520video%2520for%250Aquality%2520judgement.%2520The%2520entire%2520codebase%2520is%2520publicly%2520available%250A%2528https%253A//github.com/uuembodiedsocialai/ProbTalk3D/%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07966v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProbTalk3D%3A%20Non-Deterministic%20Emotion%20Controllable%20Speech-Driven%203D%0A%20%20Facial%20Animation%20Synthesis%20Using%20VQ-VAE&entry.906535625=Sichun%20Wu%20and%20Kazi%20Injamamul%20Haque%20and%20Zerrin%20Yumak&entry.1292438233=%20%20Audio-driven%203D%20facial%20animation%20synthesis%20has%20been%20an%20active%20field%20of%0Aresearch%20with%20attention%20from%20both%20academia%20and%20industry.%20While%20there%20are%0Apromising%20results%20in%20this%20area%2C%20recent%20approaches%20largely%20focus%20on%20lip-sync%20and%0Aidentity%20control%2C%20neglecting%20the%20role%20of%20emotions%20and%20emotion%20control%20in%20the%0Agenerative%20process.%20That%20is%20mainly%20due%20to%20the%20lack%20of%20emotionally%20rich%20facial%0Aanimation%20data%20and%20algorithms%20that%20can%20synthesize%20speech%20animations%20with%0Aemotional%20expressions%20at%20the%20same%20time.%20In%20addition%2C%20majority%20of%20the%20models%20are%0Adeterministic%2C%20meaning%20given%20the%20same%20audio%20input%2C%20they%20produce%20the%20same%20output%0Amotion.%20We%20argue%20that%20emotions%20and%20non-determinism%20are%20crucial%20to%20generate%0Adiverse%20and%20emotionally-rich%20facial%20animations.%20In%20this%20paper%2C%20we%20propose%0AProbTalk3D%20a%20non-deterministic%20neural%20network%20approach%20for%20emotion%20controllable%0Aspeech-driven%203D%20facial%20animation%20synthesis%20using%20a%20two-stage%20VQ-VAE%20model%20and%0Aan%20emotionally%20rich%20facial%20animation%20dataset%203DMEAD.%20We%20provide%20an%20extensive%0Acomparative%20analysis%20of%20our%20model%20against%20the%20recent%203D%20facial%20animation%0Asynthesis%20approaches%2C%20by%20evaluating%20the%20results%20objectively%2C%20qualitatively%2C%20and%0Awith%20a%20perceptual%20user%20study.%20We%20highlight%20several%20objective%20metrics%20that%20are%0Amore%20suitable%20for%20evaluating%20stochastic%20outputs%20and%20use%20both%20in-the-wild%20and%0Aground%20truth%20data%20for%20subjective%20evaluation.%20To%20our%20knowledge%2C%20that%20is%20the%0Afirst%20non-deterministic%203D%20facial%20animation%20synthesis%20method%20incorporating%20a%0Arich%20emotion%20dataset%20and%20emotion%20control%20with%20emotion%20labels%20and%20intensity%0Alevels.%20Our%20evaluation%20demonstrates%20that%20the%20proposed%20model%20achieves%20superior%0Aperformance%20compared%20to%20state-of-the-art%20emotion-controlled%2C%20deterministic%20and%0Anon-deterministic%20models.%20We%20recommend%20watching%20the%20supplementary%20video%20for%0Aquality%20judgement.%20The%20entire%20codebase%20is%20publicly%20available%0A%28https%3A//github.com/uuembodiedsocialai/ProbTalk3D/%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07966v1&entry.124074799=Read"},
{"title": "Deep Height Decoupling for Precise Vision-based 3D Occupancy Prediction", "author": "Yuan Wu and Zhiqiang Yan and Zhengxue Wang and Xiang Li and Le Hui and Jian Yang", "abstract": "  The task of vision-based 3D occupancy prediction aims to reconstruct 3D\ngeometry and estimate its semantic classes from 2D color images, where the\n2D-to-3D view transformation is an indispensable step. Most previous methods\nconduct forward projection, such as BEVPooling and VoxelPooling, both of which\nmap the 2D image features into 3D grids. However, the current grid representing\nfeatures within a certain height range usually introduces many confusing\nfeatures that belong to other height ranges. To address this challenge, we\npresent Deep Height Decoupling (DHD), a novel framework that incorporates\nexplicit height prior to filter out the confusing features. Specifically, DHD\nfirst predicts height maps via explicit supervision. Based on the height\ndistribution statistics, DHD designs Mask Guided Height Sampling (MGHS) to\nadaptively decoupled the height map into multiple binary masks. MGHS projects\nthe 2D image features into multiple subspaces, where each grid contains\nfeatures within reasonable height ranges. Finally, a Synergistic Feature\nAggregation (SFA) module is deployed to enhance the feature representation\nthrough channel and spatial affinities, enabling further occupancy refinement.\nOn the popular Occ3D-nuScenes benchmark, our method achieves state-of-the-art\nperformance even with minimal input frames. Code is available at\nhttps://github.com/yanzq95/DHD.\n", "link": "http://arxiv.org/abs/2409.07972v1", "date": "2024-09-12", "relevancy": 2.9438, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5892}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5892}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5879}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Height%20Decoupling%20for%20Precise%20Vision-based%203D%20Occupancy%20Prediction&body=Title%3A%20Deep%20Height%20Decoupling%20for%20Precise%20Vision-based%203D%20Occupancy%20Prediction%0AAuthor%3A%20Yuan%20Wu%20and%20Zhiqiang%20Yan%20and%20Zhengxue%20Wang%20and%20Xiang%20Li%20and%20Le%20Hui%20and%20Jian%20Yang%0AAbstract%3A%20%20%20The%20task%20of%20vision-based%203D%20occupancy%20prediction%20aims%20to%20reconstruct%203D%0Ageometry%20and%20estimate%20its%20semantic%20classes%20from%202D%20color%20images%2C%20where%20the%0A2D-to-3D%20view%20transformation%20is%20an%20indispensable%20step.%20Most%20previous%20methods%0Aconduct%20forward%20projection%2C%20such%20as%20BEVPooling%20and%20VoxelPooling%2C%20both%20of%20which%0Amap%20the%202D%20image%20features%20into%203D%20grids.%20However%2C%20the%20current%20grid%20representing%0Afeatures%20within%20a%20certain%20height%20range%20usually%20introduces%20many%20confusing%0Afeatures%20that%20belong%20to%20other%20height%20ranges.%20To%20address%20this%20challenge%2C%20we%0Apresent%20Deep%20Height%20Decoupling%20%28DHD%29%2C%20a%20novel%20framework%20that%20incorporates%0Aexplicit%20height%20prior%20to%20filter%20out%20the%20confusing%20features.%20Specifically%2C%20DHD%0Afirst%20predicts%20height%20maps%20via%20explicit%20supervision.%20Based%20on%20the%20height%0Adistribution%20statistics%2C%20DHD%20designs%20Mask%20Guided%20Height%20Sampling%20%28MGHS%29%20to%0Aadaptively%20decoupled%20the%20height%20map%20into%20multiple%20binary%20masks.%20MGHS%20projects%0Athe%202D%20image%20features%20into%20multiple%20subspaces%2C%20where%20each%20grid%20contains%0Afeatures%20within%20reasonable%20height%20ranges.%20Finally%2C%20a%20Synergistic%20Feature%0AAggregation%20%28SFA%29%20module%20is%20deployed%20to%20enhance%20the%20feature%20representation%0Athrough%20channel%20and%20spatial%20affinities%2C%20enabling%20further%20occupancy%20refinement.%0AOn%20the%20popular%20Occ3D-nuScenes%20benchmark%2C%20our%20method%20achieves%20state-of-the-art%0Aperformance%20even%20with%20minimal%20input%20frames.%20Code%20is%20available%20at%0Ahttps%3A//github.com/yanzq95/DHD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07972v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Height%2520Decoupling%2520for%2520Precise%2520Vision-based%25203D%2520Occupancy%2520Prediction%26entry.906535625%3DYuan%2520Wu%2520and%2520Zhiqiang%2520Yan%2520and%2520Zhengxue%2520Wang%2520and%2520Xiang%2520Li%2520and%2520Le%2520Hui%2520and%2520Jian%2520Yang%26entry.1292438233%3D%2520%2520The%2520task%2520of%2520vision-based%25203D%2520occupancy%2520prediction%2520aims%2520to%2520reconstruct%25203D%250Ageometry%2520and%2520estimate%2520its%2520semantic%2520classes%2520from%25202D%2520color%2520images%252C%2520where%2520the%250A2D-to-3D%2520view%2520transformation%2520is%2520an%2520indispensable%2520step.%2520Most%2520previous%2520methods%250Aconduct%2520forward%2520projection%252C%2520such%2520as%2520BEVPooling%2520and%2520VoxelPooling%252C%2520both%2520of%2520which%250Amap%2520the%25202D%2520image%2520features%2520into%25203D%2520grids.%2520However%252C%2520the%2520current%2520grid%2520representing%250Afeatures%2520within%2520a%2520certain%2520height%2520range%2520usually%2520introduces%2520many%2520confusing%250Afeatures%2520that%2520belong%2520to%2520other%2520height%2520ranges.%2520To%2520address%2520this%2520challenge%252C%2520we%250Apresent%2520Deep%2520Height%2520Decoupling%2520%2528DHD%2529%252C%2520a%2520novel%2520framework%2520that%2520incorporates%250Aexplicit%2520height%2520prior%2520to%2520filter%2520out%2520the%2520confusing%2520features.%2520Specifically%252C%2520DHD%250Afirst%2520predicts%2520height%2520maps%2520via%2520explicit%2520supervision.%2520Based%2520on%2520the%2520height%250Adistribution%2520statistics%252C%2520DHD%2520designs%2520Mask%2520Guided%2520Height%2520Sampling%2520%2528MGHS%2529%2520to%250Aadaptively%2520decoupled%2520the%2520height%2520map%2520into%2520multiple%2520binary%2520masks.%2520MGHS%2520projects%250Athe%25202D%2520image%2520features%2520into%2520multiple%2520subspaces%252C%2520where%2520each%2520grid%2520contains%250Afeatures%2520within%2520reasonable%2520height%2520ranges.%2520Finally%252C%2520a%2520Synergistic%2520Feature%250AAggregation%2520%2528SFA%2529%2520module%2520is%2520deployed%2520to%2520enhance%2520the%2520feature%2520representation%250Athrough%2520channel%2520and%2520spatial%2520affinities%252C%2520enabling%2520further%2520occupancy%2520refinement.%250AOn%2520the%2520popular%2520Occ3D-nuScenes%2520benchmark%252C%2520our%2520method%2520achieves%2520state-of-the-art%250Aperformance%2520even%2520with%2520minimal%2520input%2520frames.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/yanzq95/DHD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07972v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Height%20Decoupling%20for%20Precise%20Vision-based%203D%20Occupancy%20Prediction&entry.906535625=Yuan%20Wu%20and%20Zhiqiang%20Yan%20and%20Zhengxue%20Wang%20and%20Xiang%20Li%20and%20Le%20Hui%20and%20Jian%20Yang&entry.1292438233=%20%20The%20task%20of%20vision-based%203D%20occupancy%20prediction%20aims%20to%20reconstruct%203D%0Ageometry%20and%20estimate%20its%20semantic%20classes%20from%202D%20color%20images%2C%20where%20the%0A2D-to-3D%20view%20transformation%20is%20an%20indispensable%20step.%20Most%20previous%20methods%0Aconduct%20forward%20projection%2C%20such%20as%20BEVPooling%20and%20VoxelPooling%2C%20both%20of%20which%0Amap%20the%202D%20image%20features%20into%203D%20grids.%20However%2C%20the%20current%20grid%20representing%0Afeatures%20within%20a%20certain%20height%20range%20usually%20introduces%20many%20confusing%0Afeatures%20that%20belong%20to%20other%20height%20ranges.%20To%20address%20this%20challenge%2C%20we%0Apresent%20Deep%20Height%20Decoupling%20%28DHD%29%2C%20a%20novel%20framework%20that%20incorporates%0Aexplicit%20height%20prior%20to%20filter%20out%20the%20confusing%20features.%20Specifically%2C%20DHD%0Afirst%20predicts%20height%20maps%20via%20explicit%20supervision.%20Based%20on%20the%20height%0Adistribution%20statistics%2C%20DHD%20designs%20Mask%20Guided%20Height%20Sampling%20%28MGHS%29%20to%0Aadaptively%20decoupled%20the%20height%20map%20into%20multiple%20binary%20masks.%20MGHS%20projects%0Athe%202D%20image%20features%20into%20multiple%20subspaces%2C%20where%20each%20grid%20contains%0Afeatures%20within%20reasonable%20height%20ranges.%20Finally%2C%20a%20Synergistic%20Feature%0AAggregation%20%28SFA%29%20module%20is%20deployed%20to%20enhance%20the%20feature%20representation%0Athrough%20channel%20and%20spatial%20affinities%2C%20enabling%20further%20occupancy%20refinement.%0AOn%20the%20popular%20Occ3D-nuScenes%20benchmark%2C%20our%20method%20achieves%20state-of-the-art%0Aperformance%20even%20with%20minimal%20input%20frames.%20Code%20is%20available%20at%0Ahttps%3A//github.com/yanzq95/DHD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07972v1&entry.124074799=Read"},
{"title": "Depth on Demand: Streaming Dense Depth from a Low Frame Rate Active\n  Sensor", "author": "Andrea Conti and Matteo Poggi and Valerio Cambareri and Stefano Mattoccia", "abstract": "  High frame rate and accurate depth estimation plays an important role in\nseveral tasks crucial to robotics and automotive perception. To date, this can\nbe achieved through ToF and LiDAR devices for indoor and outdoor applications,\nrespectively. However, their applicability is limited by low frame rate, energy\nconsumption, and spatial sparsity. Depth on Demand (DoD) allows for accurate\ntemporal and spatial depth densification achieved by exploiting a high frame\nrate RGB sensor coupled with a potentially lower frame rate and sparse active\ndepth sensor. Our proposal jointly enables lower energy consumption and denser\nshape reconstruction, by significantly reducing the streaming requirements on\nthe depth sensor thanks to its three core stages: i) multi-modal encoding, ii)\niterative multi-modal integration, and iii) depth decoding. We present extended\nevidence assessing the effectiveness of DoD on indoor and outdoor video\ndatasets, covering both environment scanning and automotive perception use\ncases.\n", "link": "http://arxiv.org/abs/2409.08277v1", "date": "2024-09-12", "relevancy": 2.8886, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5852}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.574}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Depth%20on%20Demand%3A%20Streaming%20Dense%20Depth%20from%20a%20Low%20Frame%20Rate%20Active%0A%20%20Sensor&body=Title%3A%20Depth%20on%20Demand%3A%20Streaming%20Dense%20Depth%20from%20a%20Low%20Frame%20Rate%20Active%0A%20%20Sensor%0AAuthor%3A%20Andrea%20Conti%20and%20Matteo%20Poggi%20and%20Valerio%20Cambareri%20and%20Stefano%20Mattoccia%0AAbstract%3A%20%20%20High%20frame%20rate%20and%20accurate%20depth%20estimation%20plays%20an%20important%20role%20in%0Aseveral%20tasks%20crucial%20to%20robotics%20and%20automotive%20perception.%20To%20date%2C%20this%20can%0Abe%20achieved%20through%20ToF%20and%20LiDAR%20devices%20for%20indoor%20and%20outdoor%20applications%2C%0Arespectively.%20However%2C%20their%20applicability%20is%20limited%20by%20low%20frame%20rate%2C%20energy%0Aconsumption%2C%20and%20spatial%20sparsity.%20Depth%20on%20Demand%20%28DoD%29%20allows%20for%20accurate%0Atemporal%20and%20spatial%20depth%20densification%20achieved%20by%20exploiting%20a%20high%20frame%0Arate%20RGB%20sensor%20coupled%20with%20a%20potentially%20lower%20frame%20rate%20and%20sparse%20active%0Adepth%20sensor.%20Our%20proposal%20jointly%20enables%20lower%20energy%20consumption%20and%20denser%0Ashape%20reconstruction%2C%20by%20significantly%20reducing%20the%20streaming%20requirements%20on%0Athe%20depth%20sensor%20thanks%20to%20its%20three%20core%20stages%3A%20i%29%20multi-modal%20encoding%2C%20ii%29%0Aiterative%20multi-modal%20integration%2C%20and%20iii%29%20depth%20decoding.%20We%20present%20extended%0Aevidence%20assessing%20the%20effectiveness%20of%20DoD%20on%20indoor%20and%20outdoor%20video%0Adatasets%2C%20covering%20both%20environment%20scanning%20and%20automotive%20perception%20use%0Acases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08277v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepth%2520on%2520Demand%253A%2520Streaming%2520Dense%2520Depth%2520from%2520a%2520Low%2520Frame%2520Rate%2520Active%250A%2520%2520Sensor%26entry.906535625%3DAndrea%2520Conti%2520and%2520Matteo%2520Poggi%2520and%2520Valerio%2520Cambareri%2520and%2520Stefano%2520Mattoccia%26entry.1292438233%3D%2520%2520High%2520frame%2520rate%2520and%2520accurate%2520depth%2520estimation%2520plays%2520an%2520important%2520role%2520in%250Aseveral%2520tasks%2520crucial%2520to%2520robotics%2520and%2520automotive%2520perception.%2520To%2520date%252C%2520this%2520can%250Abe%2520achieved%2520through%2520ToF%2520and%2520LiDAR%2520devices%2520for%2520indoor%2520and%2520outdoor%2520applications%252C%250Arespectively.%2520However%252C%2520their%2520applicability%2520is%2520limited%2520by%2520low%2520frame%2520rate%252C%2520energy%250Aconsumption%252C%2520and%2520spatial%2520sparsity.%2520Depth%2520on%2520Demand%2520%2528DoD%2529%2520allows%2520for%2520accurate%250Atemporal%2520and%2520spatial%2520depth%2520densification%2520achieved%2520by%2520exploiting%2520a%2520high%2520frame%250Arate%2520RGB%2520sensor%2520coupled%2520with%2520a%2520potentially%2520lower%2520frame%2520rate%2520and%2520sparse%2520active%250Adepth%2520sensor.%2520Our%2520proposal%2520jointly%2520enables%2520lower%2520energy%2520consumption%2520and%2520denser%250Ashape%2520reconstruction%252C%2520by%2520significantly%2520reducing%2520the%2520streaming%2520requirements%2520on%250Athe%2520depth%2520sensor%2520thanks%2520to%2520its%2520three%2520core%2520stages%253A%2520i%2529%2520multi-modal%2520encoding%252C%2520ii%2529%250Aiterative%2520multi-modal%2520integration%252C%2520and%2520iii%2529%2520depth%2520decoding.%2520We%2520present%2520extended%250Aevidence%2520assessing%2520the%2520effectiveness%2520of%2520DoD%2520on%2520indoor%2520and%2520outdoor%2520video%250Adatasets%252C%2520covering%2520both%2520environment%2520scanning%2520and%2520automotive%2520perception%2520use%250Acases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08277v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Depth%20on%20Demand%3A%20Streaming%20Dense%20Depth%20from%20a%20Low%20Frame%20Rate%20Active%0A%20%20Sensor&entry.906535625=Andrea%20Conti%20and%20Matteo%20Poggi%20and%20Valerio%20Cambareri%20and%20Stefano%20Mattoccia&entry.1292438233=%20%20High%20frame%20rate%20and%20accurate%20depth%20estimation%20plays%20an%20important%20role%20in%0Aseveral%20tasks%20crucial%20to%20robotics%20and%20automotive%20perception.%20To%20date%2C%20this%20can%0Abe%20achieved%20through%20ToF%20and%20LiDAR%20devices%20for%20indoor%20and%20outdoor%20applications%2C%0Arespectively.%20However%2C%20their%20applicability%20is%20limited%20by%20low%20frame%20rate%2C%20energy%0Aconsumption%2C%20and%20spatial%20sparsity.%20Depth%20on%20Demand%20%28DoD%29%20allows%20for%20accurate%0Atemporal%20and%20spatial%20depth%20densification%20achieved%20by%20exploiting%20a%20high%20frame%0Arate%20RGB%20sensor%20coupled%20with%20a%20potentially%20lower%20frame%20rate%20and%20sparse%20active%0Adepth%20sensor.%20Our%20proposal%20jointly%20enables%20lower%20energy%20consumption%20and%20denser%0Ashape%20reconstruction%2C%20by%20significantly%20reducing%20the%20streaming%20requirements%20on%0Athe%20depth%20sensor%20thanks%20to%20its%20three%20core%20stages%3A%20i%29%20multi-modal%20encoding%2C%20ii%29%0Aiterative%20multi-modal%20integration%2C%20and%20iii%29%20depth%20decoding.%20We%20present%20extended%0Aevidence%20assessing%20the%20effectiveness%20of%20DoD%20on%20indoor%20and%20outdoor%20video%0Adatasets%2C%20covering%20both%20environment%20scanning%20and%20automotive%20perception%20use%0Acases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08277v1&entry.124074799=Read"},
{"title": "ComAlign: Compositional Alignment in Vision-Language Models", "author": "Ali Abdollah and Amirmohammad Izadi and Armin Saghafian and Reza Vahidimajd and Mohammad Mozafari and Amirreza Mirzaei and Mohammadmahdi Samiei and Mahdieh Soleymani Baghshah", "abstract": "  Vision-language models (VLMs) like CLIP have showcased a remarkable ability\nto extract transferable features for downstream tasks. Nonetheless, the\ntraining process of these models is usually based on a coarse-grained\ncontrastive loss between the global embedding of images and texts which may\nlose the compositional structure of these modalities. Many recent studies have\nshown VLMs lack compositional understandings like attribute binding and\nidentifying object relationships. Although some recent methods have tried to\nachieve finer-level alignments, they either are not based on extracting\nmeaningful components of proper granularity or don't properly utilize the\nmodalities' correspondence (especially in image-text pairs with more\ningredients). Addressing these limitations, we introduce Compositional\nAlignment (ComAlign), a fine-grained approach to discover more exact\ncorrespondence of text and image components using only the weak supervision in\nthe form of image-text pairs. Our methodology emphasizes that the compositional\nstructure (including entities and relations) extracted from the text modality\nmust also be retained in the image modality. To enforce correspondence of\nfine-grained concepts in image and text modalities, we train a lightweight\nnetwork lying on top of existing visual and language encoders using a small\ndataset. The network is trained to align nodes and edges of the structure\nacross the modalities. Experimental results on various VLMs and datasets\ndemonstrate significant improvements in retrieval and compositional benchmarks,\naffirming the effectiveness of our plugin model.\n", "link": "http://arxiv.org/abs/2409.08206v1", "date": "2024-09-12", "relevancy": 2.8834, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5942}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5679}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ComAlign%3A%20Compositional%20Alignment%20in%20Vision-Language%20Models&body=Title%3A%20ComAlign%3A%20Compositional%20Alignment%20in%20Vision-Language%20Models%0AAuthor%3A%20Ali%20Abdollah%20and%20Amirmohammad%20Izadi%20and%20Armin%20Saghafian%20and%20Reza%20Vahidimajd%20and%20Mohammad%20Mozafari%20and%20Amirreza%20Mirzaei%20and%20Mohammadmahdi%20Samiei%20and%20Mahdieh%20Soleymani%20Baghshah%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20like%20CLIP%20have%20showcased%20a%20remarkable%20ability%0Ato%20extract%20transferable%20features%20for%20downstream%20tasks.%20Nonetheless%2C%20the%0Atraining%20process%20of%20these%20models%20is%20usually%20based%20on%20a%20coarse-grained%0Acontrastive%20loss%20between%20the%20global%20embedding%20of%20images%20and%20texts%20which%20may%0Alose%20the%20compositional%20structure%20of%20these%20modalities.%20Many%20recent%20studies%20have%0Ashown%20VLMs%20lack%20compositional%20understandings%20like%20attribute%20binding%20and%0Aidentifying%20object%20relationships.%20Although%20some%20recent%20methods%20have%20tried%20to%0Aachieve%20finer-level%20alignments%2C%20they%20either%20are%20not%20based%20on%20extracting%0Ameaningful%20components%20of%20proper%20granularity%20or%20don%27t%20properly%20utilize%20the%0Amodalities%27%20correspondence%20%28especially%20in%20image-text%20pairs%20with%20more%0Aingredients%29.%20Addressing%20these%20limitations%2C%20we%20introduce%20Compositional%0AAlignment%20%28ComAlign%29%2C%20a%20fine-grained%20approach%20to%20discover%20more%20exact%0Acorrespondence%20of%20text%20and%20image%20components%20using%20only%20the%20weak%20supervision%20in%0Athe%20form%20of%20image-text%20pairs.%20Our%20methodology%20emphasizes%20that%20the%20compositional%0Astructure%20%28including%20entities%20and%20relations%29%20extracted%20from%20the%20text%20modality%0Amust%20also%20be%20retained%20in%20the%20image%20modality.%20To%20enforce%20correspondence%20of%0Afine-grained%20concepts%20in%20image%20and%20text%20modalities%2C%20we%20train%20a%20lightweight%0Anetwork%20lying%20on%20top%20of%20existing%20visual%20and%20language%20encoders%20using%20a%20small%0Adataset.%20The%20network%20is%20trained%20to%20align%20nodes%20and%20edges%20of%20the%20structure%0Aacross%20the%20modalities.%20Experimental%20results%20on%20various%20VLMs%20and%20datasets%0Ademonstrate%20significant%20improvements%20in%20retrieval%20and%20compositional%20benchmarks%2C%0Aaffirming%20the%20effectiveness%20of%20our%20plugin%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08206v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComAlign%253A%2520Compositional%2520Alignment%2520in%2520Vision-Language%2520Models%26entry.906535625%3DAli%2520Abdollah%2520and%2520Amirmohammad%2520Izadi%2520and%2520Armin%2520Saghafian%2520and%2520Reza%2520Vahidimajd%2520and%2520Mohammad%2520Mozafari%2520and%2520Amirreza%2520Mirzaei%2520and%2520Mohammadmahdi%2520Samiei%2520and%2520Mahdieh%2520Soleymani%2520Baghshah%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520like%2520CLIP%2520have%2520showcased%2520a%2520remarkable%2520ability%250Ato%2520extract%2520transferable%2520features%2520for%2520downstream%2520tasks.%2520Nonetheless%252C%2520the%250Atraining%2520process%2520of%2520these%2520models%2520is%2520usually%2520based%2520on%2520a%2520coarse-grained%250Acontrastive%2520loss%2520between%2520the%2520global%2520embedding%2520of%2520images%2520and%2520texts%2520which%2520may%250Alose%2520the%2520compositional%2520structure%2520of%2520these%2520modalities.%2520Many%2520recent%2520studies%2520have%250Ashown%2520VLMs%2520lack%2520compositional%2520understandings%2520like%2520attribute%2520binding%2520and%250Aidentifying%2520object%2520relationships.%2520Although%2520some%2520recent%2520methods%2520have%2520tried%2520to%250Aachieve%2520finer-level%2520alignments%252C%2520they%2520either%2520are%2520not%2520based%2520on%2520extracting%250Ameaningful%2520components%2520of%2520proper%2520granularity%2520or%2520don%2527t%2520properly%2520utilize%2520the%250Amodalities%2527%2520correspondence%2520%2528especially%2520in%2520image-text%2520pairs%2520with%2520more%250Aingredients%2529.%2520Addressing%2520these%2520limitations%252C%2520we%2520introduce%2520Compositional%250AAlignment%2520%2528ComAlign%2529%252C%2520a%2520fine-grained%2520approach%2520to%2520discover%2520more%2520exact%250Acorrespondence%2520of%2520text%2520and%2520image%2520components%2520using%2520only%2520the%2520weak%2520supervision%2520in%250Athe%2520form%2520of%2520image-text%2520pairs.%2520Our%2520methodology%2520emphasizes%2520that%2520the%2520compositional%250Astructure%2520%2528including%2520entities%2520and%2520relations%2529%2520extracted%2520from%2520the%2520text%2520modality%250Amust%2520also%2520be%2520retained%2520in%2520the%2520image%2520modality.%2520To%2520enforce%2520correspondence%2520of%250Afine-grained%2520concepts%2520in%2520image%2520and%2520text%2520modalities%252C%2520we%2520train%2520a%2520lightweight%250Anetwork%2520lying%2520on%2520top%2520of%2520existing%2520visual%2520and%2520language%2520encoders%2520using%2520a%2520small%250Adataset.%2520The%2520network%2520is%2520trained%2520to%2520align%2520nodes%2520and%2520edges%2520of%2520the%2520structure%250Aacross%2520the%2520modalities.%2520Experimental%2520results%2520on%2520various%2520VLMs%2520and%2520datasets%250Ademonstrate%2520significant%2520improvements%2520in%2520retrieval%2520and%2520compositional%2520benchmarks%252C%250Aaffirming%2520the%2520effectiveness%2520of%2520our%2520plugin%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08206v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ComAlign%3A%20Compositional%20Alignment%20in%20Vision-Language%20Models&entry.906535625=Ali%20Abdollah%20and%20Amirmohammad%20Izadi%20and%20Armin%20Saghafian%20and%20Reza%20Vahidimajd%20and%20Mohammad%20Mozafari%20and%20Amirreza%20Mirzaei%20and%20Mohammadmahdi%20Samiei%20and%20Mahdieh%20Soleymani%20Baghshah&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20like%20CLIP%20have%20showcased%20a%20remarkable%20ability%0Ato%20extract%20transferable%20features%20for%20downstream%20tasks.%20Nonetheless%2C%20the%0Atraining%20process%20of%20these%20models%20is%20usually%20based%20on%20a%20coarse-grained%0Acontrastive%20loss%20between%20the%20global%20embedding%20of%20images%20and%20texts%20which%20may%0Alose%20the%20compositional%20structure%20of%20these%20modalities.%20Many%20recent%20studies%20have%0Ashown%20VLMs%20lack%20compositional%20understandings%20like%20attribute%20binding%20and%0Aidentifying%20object%20relationships.%20Although%20some%20recent%20methods%20have%20tried%20to%0Aachieve%20finer-level%20alignments%2C%20they%20either%20are%20not%20based%20on%20extracting%0Ameaningful%20components%20of%20proper%20granularity%20or%20don%27t%20properly%20utilize%20the%0Amodalities%27%20correspondence%20%28especially%20in%20image-text%20pairs%20with%20more%0Aingredients%29.%20Addressing%20these%20limitations%2C%20we%20introduce%20Compositional%0AAlignment%20%28ComAlign%29%2C%20a%20fine-grained%20approach%20to%20discover%20more%20exact%0Acorrespondence%20of%20text%20and%20image%20components%20using%20only%20the%20weak%20supervision%20in%0Athe%20form%20of%20image-text%20pairs.%20Our%20methodology%20emphasizes%20that%20the%20compositional%0Astructure%20%28including%20entities%20and%20relations%29%20extracted%20from%20the%20text%20modality%0Amust%20also%20be%20retained%20in%20the%20image%20modality.%20To%20enforce%20correspondence%20of%0Afine-grained%20concepts%20in%20image%20and%20text%20modalities%2C%20we%20train%20a%20lightweight%0Anetwork%20lying%20on%20top%20of%20existing%20visual%20and%20language%20encoders%20using%20a%20small%0Adataset.%20The%20network%20is%20trained%20to%20align%20nodes%20and%20edges%20of%20the%20structure%0Aacross%20the%20modalities.%20Experimental%20results%20on%20various%20VLMs%20and%20datasets%0Ademonstrate%20significant%20improvements%20in%20retrieval%20and%20compositional%20benchmarks%2C%0Aaffirming%20the%20effectiveness%20of%20our%20plugin%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08206v1&entry.124074799=Read"},
{"title": "What Makes a Face Look like a Hat: Decoupling Low-level and High-level\n  Visual Properties with Image Triplets", "author": "Maytus Piriyajitakonkij and Sirawaj Itthipuripat and Ian Ballard and Ioannis Pappas", "abstract": "  In visual decision making, high-level features, such as object categories,\nhave a strong influence on choice. However, the impact of low-level features on\nbehavior is less understood partly due to the high correlation between high-\nand low-level features in the stimuli presented (e.g., objects of the same\ncategory are more likely to share low-level features). To disentangle these\neffects, we propose a method that de-correlates low- and high-level visual\nproperties in a novel set of stimuli. Our method uses two Convolutional Neural\nNetworks (CNNs) as candidate models of the ventral visual stream: the CORnet-S\nthat has high neural predictivity in high-level, IT-like responses and the\nVGG-16 that has high neural predictivity in low-level responses. Triplets\n(root, image1, image2) of stimuli are parametrized by the level of low- and\nhigh-level similarity of images extracted from the different layers. These\nstimuli are then used in a decision-making task where participants are tasked\nto choose the most similar-to-the-root image. We found that different networks\nshow differing abilities to predict the effects of low-versus-high-level\nsimilarity: while CORnet-S outperforms VGG-16 in explaining human choices based\non high-level similarity, VGG-16 outperforms CORnet-S in explaining human\nchoices based on low-level similarity. Using Brain-Score, we observed that the\nbehavioral prediction abilities of different layers of these networks\nqualitatively corresponded to their ability to explain neural activity at\ndifferent levels of the visual hierarchy. In summary, our algorithm for\nstimulus set generation enables the study of how different representations in\nthe visual stream affect high-level cognitive behaviors.\n", "link": "http://arxiv.org/abs/2409.02241v2", "date": "2024-09-12", "relevancy": 2.7573, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5628}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5628}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5287}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Makes%20a%20Face%20Look%20like%20a%20Hat%3A%20Decoupling%20Low-level%20and%20High-level%0A%20%20Visual%20Properties%20with%20Image%20Triplets&body=Title%3A%20What%20Makes%20a%20Face%20Look%20like%20a%20Hat%3A%20Decoupling%20Low-level%20and%20High-level%0A%20%20Visual%20Properties%20with%20Image%20Triplets%0AAuthor%3A%20Maytus%20Piriyajitakonkij%20and%20Sirawaj%20Itthipuripat%20and%20Ian%20Ballard%20and%20Ioannis%20Pappas%0AAbstract%3A%20%20%20In%20visual%20decision%20making%2C%20high-level%20features%2C%20such%20as%20object%20categories%2C%0Ahave%20a%20strong%20influence%20on%20choice.%20However%2C%20the%20impact%20of%20low-level%20features%20on%0Abehavior%20is%20less%20understood%20partly%20due%20to%20the%20high%20correlation%20between%20high-%0Aand%20low-level%20features%20in%20the%20stimuli%20presented%20%28e.g.%2C%20objects%20of%20the%20same%0Acategory%20are%20more%20likely%20to%20share%20low-level%20features%29.%20To%20disentangle%20these%0Aeffects%2C%20we%20propose%20a%20method%20that%20de-correlates%20low-%20and%20high-level%20visual%0Aproperties%20in%20a%20novel%20set%20of%20stimuli.%20Our%20method%20uses%20two%20Convolutional%20Neural%0ANetworks%20%28CNNs%29%20as%20candidate%20models%20of%20the%20ventral%20visual%20stream%3A%20the%20CORnet-S%0Athat%20has%20high%20neural%20predictivity%20in%20high-level%2C%20IT-like%20responses%20and%20the%0AVGG-16%20that%20has%20high%20neural%20predictivity%20in%20low-level%20responses.%20Triplets%0A%28root%2C%20image1%2C%20image2%29%20of%20stimuli%20are%20parametrized%20by%20the%20level%20of%20low-%20and%0Ahigh-level%20similarity%20of%20images%20extracted%20from%20the%20different%20layers.%20These%0Astimuli%20are%20then%20used%20in%20a%20decision-making%20task%20where%20participants%20are%20tasked%0Ato%20choose%20the%20most%20similar-to-the-root%20image.%20We%20found%20that%20different%20networks%0Ashow%20differing%20abilities%20to%20predict%20the%20effects%20of%20low-versus-high-level%0Asimilarity%3A%20while%20CORnet-S%20outperforms%20VGG-16%20in%20explaining%20human%20choices%20based%0Aon%20high-level%20similarity%2C%20VGG-16%20outperforms%20CORnet-S%20in%20explaining%20human%0Achoices%20based%20on%20low-level%20similarity.%20Using%20Brain-Score%2C%20we%20observed%20that%20the%0Abehavioral%20prediction%20abilities%20of%20different%20layers%20of%20these%20networks%0Aqualitatively%20corresponded%20to%20their%20ability%20to%20explain%20neural%20activity%20at%0Adifferent%20levels%20of%20the%20visual%20hierarchy.%20In%20summary%2C%20our%20algorithm%20for%0Astimulus%20set%20generation%20enables%20the%20study%20of%20how%20different%20representations%20in%0Athe%20visual%20stream%20affect%20high-level%20cognitive%20behaviors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02241v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Makes%2520a%2520Face%2520Look%2520like%2520a%2520Hat%253A%2520Decoupling%2520Low-level%2520and%2520High-level%250A%2520%2520Visual%2520Properties%2520with%2520Image%2520Triplets%26entry.906535625%3DMaytus%2520Piriyajitakonkij%2520and%2520Sirawaj%2520Itthipuripat%2520and%2520Ian%2520Ballard%2520and%2520Ioannis%2520Pappas%26entry.1292438233%3D%2520%2520In%2520visual%2520decision%2520making%252C%2520high-level%2520features%252C%2520such%2520as%2520object%2520categories%252C%250Ahave%2520a%2520strong%2520influence%2520on%2520choice.%2520However%252C%2520the%2520impact%2520of%2520low-level%2520features%2520on%250Abehavior%2520is%2520less%2520understood%2520partly%2520due%2520to%2520the%2520high%2520correlation%2520between%2520high-%250Aand%2520low-level%2520features%2520in%2520the%2520stimuli%2520presented%2520%2528e.g.%252C%2520objects%2520of%2520the%2520same%250Acategory%2520are%2520more%2520likely%2520to%2520share%2520low-level%2520features%2529.%2520To%2520disentangle%2520these%250Aeffects%252C%2520we%2520propose%2520a%2520method%2520that%2520de-correlates%2520low-%2520and%2520high-level%2520visual%250Aproperties%2520in%2520a%2520novel%2520set%2520of%2520stimuli.%2520Our%2520method%2520uses%2520two%2520Convolutional%2520Neural%250ANetworks%2520%2528CNNs%2529%2520as%2520candidate%2520models%2520of%2520the%2520ventral%2520visual%2520stream%253A%2520the%2520CORnet-S%250Athat%2520has%2520high%2520neural%2520predictivity%2520in%2520high-level%252C%2520IT-like%2520responses%2520and%2520the%250AVGG-16%2520that%2520has%2520high%2520neural%2520predictivity%2520in%2520low-level%2520responses.%2520Triplets%250A%2528root%252C%2520image1%252C%2520image2%2529%2520of%2520stimuli%2520are%2520parametrized%2520by%2520the%2520level%2520of%2520low-%2520and%250Ahigh-level%2520similarity%2520of%2520images%2520extracted%2520from%2520the%2520different%2520layers.%2520These%250Astimuli%2520are%2520then%2520used%2520in%2520a%2520decision-making%2520task%2520where%2520participants%2520are%2520tasked%250Ato%2520choose%2520the%2520most%2520similar-to-the-root%2520image.%2520We%2520found%2520that%2520different%2520networks%250Ashow%2520differing%2520abilities%2520to%2520predict%2520the%2520effects%2520of%2520low-versus-high-level%250Asimilarity%253A%2520while%2520CORnet-S%2520outperforms%2520VGG-16%2520in%2520explaining%2520human%2520choices%2520based%250Aon%2520high-level%2520similarity%252C%2520VGG-16%2520outperforms%2520CORnet-S%2520in%2520explaining%2520human%250Achoices%2520based%2520on%2520low-level%2520similarity.%2520Using%2520Brain-Score%252C%2520we%2520observed%2520that%2520the%250Abehavioral%2520prediction%2520abilities%2520of%2520different%2520layers%2520of%2520these%2520networks%250Aqualitatively%2520corresponded%2520to%2520their%2520ability%2520to%2520explain%2520neural%2520activity%2520at%250Adifferent%2520levels%2520of%2520the%2520visual%2520hierarchy.%2520In%2520summary%252C%2520our%2520algorithm%2520for%250Astimulus%2520set%2520generation%2520enables%2520the%2520study%2520of%2520how%2520different%2520representations%2520in%250Athe%2520visual%2520stream%2520affect%2520high-level%2520cognitive%2520behaviors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02241v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Makes%20a%20Face%20Look%20like%20a%20Hat%3A%20Decoupling%20Low-level%20and%20High-level%0A%20%20Visual%20Properties%20with%20Image%20Triplets&entry.906535625=Maytus%20Piriyajitakonkij%20and%20Sirawaj%20Itthipuripat%20and%20Ian%20Ballard%20and%20Ioannis%20Pappas&entry.1292438233=%20%20In%20visual%20decision%20making%2C%20high-level%20features%2C%20such%20as%20object%20categories%2C%0Ahave%20a%20strong%20influence%20on%20choice.%20However%2C%20the%20impact%20of%20low-level%20features%20on%0Abehavior%20is%20less%20understood%20partly%20due%20to%20the%20high%20correlation%20between%20high-%0Aand%20low-level%20features%20in%20the%20stimuli%20presented%20%28e.g.%2C%20objects%20of%20the%20same%0Acategory%20are%20more%20likely%20to%20share%20low-level%20features%29.%20To%20disentangle%20these%0Aeffects%2C%20we%20propose%20a%20method%20that%20de-correlates%20low-%20and%20high-level%20visual%0Aproperties%20in%20a%20novel%20set%20of%20stimuli.%20Our%20method%20uses%20two%20Convolutional%20Neural%0ANetworks%20%28CNNs%29%20as%20candidate%20models%20of%20the%20ventral%20visual%20stream%3A%20the%20CORnet-S%0Athat%20has%20high%20neural%20predictivity%20in%20high-level%2C%20IT-like%20responses%20and%20the%0AVGG-16%20that%20has%20high%20neural%20predictivity%20in%20low-level%20responses.%20Triplets%0A%28root%2C%20image1%2C%20image2%29%20of%20stimuli%20are%20parametrized%20by%20the%20level%20of%20low-%20and%0Ahigh-level%20similarity%20of%20images%20extracted%20from%20the%20different%20layers.%20These%0Astimuli%20are%20then%20used%20in%20a%20decision-making%20task%20where%20participants%20are%20tasked%0Ato%20choose%20the%20most%20similar-to-the-root%20image.%20We%20found%20that%20different%20networks%0Ashow%20differing%20abilities%20to%20predict%20the%20effects%20of%20low-versus-high-level%0Asimilarity%3A%20while%20CORnet-S%20outperforms%20VGG-16%20in%20explaining%20human%20choices%20based%0Aon%20high-level%20similarity%2C%20VGG-16%20outperforms%20CORnet-S%20in%20explaining%20human%0Achoices%20based%20on%20low-level%20similarity.%20Using%20Brain-Score%2C%20we%20observed%20that%20the%0Abehavioral%20prediction%20abilities%20of%20different%20layers%20of%20these%20networks%0Aqualitatively%20corresponded%20to%20their%20ability%20to%20explain%20neural%20activity%20at%0Adifferent%20levels%20of%20the%20visual%20hierarchy.%20In%20summary%2C%20our%20algorithm%20for%0Astimulus%20set%20generation%20enables%20the%20study%20of%20how%20different%20representations%20in%0Athe%20visual%20stream%20affect%20high-level%20cognitive%20behaviors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02241v2&entry.124074799=Read"},
{"title": "Locality-aware Cross-modal Correspondence Learning for Dense\n  Audio-Visual Events Localization", "author": "Ling Xing and Hongyu Qu and Rui Yan and Xiangbo Shu and Jinhui Tang", "abstract": "  Dense-localization Audio-Visual Events (DAVE) aims to identify time\nboundaries and corresponding categories for events that can be heard and seen\nconcurrently in an untrimmed video. Existing methods typically encode audio and\nvisual representation separately without any explicit cross-modal alignment\nconstraint. Then they adopt dense cross-modal attention to integrate multimodal\ninformation for DAVE. Thus these methods inevitably aggregate irrelevant noise\nand events, especially in complex and long videos, leading to imprecise\ndetection. In this paper, we present LOCO, a Locality-aware cross-modal\nCorrespondence learning framework for DAVE. The core idea is to explore local\ntemporal continuity nature of audio-visual events, which serves as informative\nyet free supervision signals to guide the filtering of irrelevant information\nand inspire the extraction of complementary multimodal information during both\nunimodal and cross-modal learning stages. i) Specifically, LOCO applies\nLocality-aware Correspondence Correction (LCC) to uni-modal features via\nleveraging cross-modal local-correlated properties without any extra\nannotations. This enforces uni-modal encoders to highlight similar semantics\nshared by audio and visual features. ii) To better aggregate such audio and\nvisual features, we further customize Cross-modal Dynamic Perception layer\n(CDP) in cross-modal feature pyramid to understand local temporal patterns of\naudio-visual events by imposing local consistency within multimodal features in\na data-driven manner. By incorporating LCC and CDP, LOCO provides solid\nperformance gains and outperforms existing methods for DAVE. The source code\nwill be released.\n", "link": "http://arxiv.org/abs/2409.07967v1", "date": "2024-09-12", "relevancy": 2.734, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5719}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5342}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Locality-aware%20Cross-modal%20Correspondence%20Learning%20for%20Dense%0A%20%20Audio-Visual%20Events%20Localization&body=Title%3A%20Locality-aware%20Cross-modal%20Correspondence%20Learning%20for%20Dense%0A%20%20Audio-Visual%20Events%20Localization%0AAuthor%3A%20Ling%20Xing%20and%20Hongyu%20Qu%20and%20Rui%20Yan%20and%20Xiangbo%20Shu%20and%20Jinhui%20Tang%0AAbstract%3A%20%20%20Dense-localization%20Audio-Visual%20Events%20%28DAVE%29%20aims%20to%20identify%20time%0Aboundaries%20and%20corresponding%20categories%20for%20events%20that%20can%20be%20heard%20and%20seen%0Aconcurrently%20in%20an%20untrimmed%20video.%20Existing%20methods%20typically%20encode%20audio%20and%0Avisual%20representation%20separately%20without%20any%20explicit%20cross-modal%20alignment%0Aconstraint.%20Then%20they%20adopt%20dense%20cross-modal%20attention%20to%20integrate%20multimodal%0Ainformation%20for%20DAVE.%20Thus%20these%20methods%20inevitably%20aggregate%20irrelevant%20noise%0Aand%20events%2C%20especially%20in%20complex%20and%20long%20videos%2C%20leading%20to%20imprecise%0Adetection.%20In%20this%20paper%2C%20we%20present%20LOCO%2C%20a%20Locality-aware%20cross-modal%0ACorrespondence%20learning%20framework%20for%20DAVE.%20The%20core%20idea%20is%20to%20explore%20local%0Atemporal%20continuity%20nature%20of%20audio-visual%20events%2C%20which%20serves%20as%20informative%0Ayet%20free%20supervision%20signals%20to%20guide%20the%20filtering%20of%20irrelevant%20information%0Aand%20inspire%20the%20extraction%20of%20complementary%20multimodal%20information%20during%20both%0Aunimodal%20and%20cross-modal%20learning%20stages.%20i%29%20Specifically%2C%20LOCO%20applies%0ALocality-aware%20Correspondence%20Correction%20%28LCC%29%20to%20uni-modal%20features%20via%0Aleveraging%20cross-modal%20local-correlated%20properties%20without%20any%20extra%0Aannotations.%20This%20enforces%20uni-modal%20encoders%20to%20highlight%20similar%20semantics%0Ashared%20by%20audio%20and%20visual%20features.%20ii%29%20To%20better%20aggregate%20such%20audio%20and%0Avisual%20features%2C%20we%20further%20customize%20Cross-modal%20Dynamic%20Perception%20layer%0A%28CDP%29%20in%20cross-modal%20feature%20pyramid%20to%20understand%20local%20temporal%20patterns%20of%0Aaudio-visual%20events%20by%20imposing%20local%20consistency%20within%20multimodal%20features%20in%0Aa%20data-driven%20manner.%20By%20incorporating%20LCC%20and%20CDP%2C%20LOCO%20provides%20solid%0Aperformance%20gains%20and%20outperforms%20existing%20methods%20for%20DAVE.%20The%20source%20code%0Awill%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07967v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocality-aware%2520Cross-modal%2520Correspondence%2520Learning%2520for%2520Dense%250A%2520%2520Audio-Visual%2520Events%2520Localization%26entry.906535625%3DLing%2520Xing%2520and%2520Hongyu%2520Qu%2520and%2520Rui%2520Yan%2520and%2520Xiangbo%2520Shu%2520and%2520Jinhui%2520Tang%26entry.1292438233%3D%2520%2520Dense-localization%2520Audio-Visual%2520Events%2520%2528DAVE%2529%2520aims%2520to%2520identify%2520time%250Aboundaries%2520and%2520corresponding%2520categories%2520for%2520events%2520that%2520can%2520be%2520heard%2520and%2520seen%250Aconcurrently%2520in%2520an%2520untrimmed%2520video.%2520Existing%2520methods%2520typically%2520encode%2520audio%2520and%250Avisual%2520representation%2520separately%2520without%2520any%2520explicit%2520cross-modal%2520alignment%250Aconstraint.%2520Then%2520they%2520adopt%2520dense%2520cross-modal%2520attention%2520to%2520integrate%2520multimodal%250Ainformation%2520for%2520DAVE.%2520Thus%2520these%2520methods%2520inevitably%2520aggregate%2520irrelevant%2520noise%250Aand%2520events%252C%2520especially%2520in%2520complex%2520and%2520long%2520videos%252C%2520leading%2520to%2520imprecise%250Adetection.%2520In%2520this%2520paper%252C%2520we%2520present%2520LOCO%252C%2520a%2520Locality-aware%2520cross-modal%250ACorrespondence%2520learning%2520framework%2520for%2520DAVE.%2520The%2520core%2520idea%2520is%2520to%2520explore%2520local%250Atemporal%2520continuity%2520nature%2520of%2520audio-visual%2520events%252C%2520which%2520serves%2520as%2520informative%250Ayet%2520free%2520supervision%2520signals%2520to%2520guide%2520the%2520filtering%2520of%2520irrelevant%2520information%250Aand%2520inspire%2520the%2520extraction%2520of%2520complementary%2520multimodal%2520information%2520during%2520both%250Aunimodal%2520and%2520cross-modal%2520learning%2520stages.%2520i%2529%2520Specifically%252C%2520LOCO%2520applies%250ALocality-aware%2520Correspondence%2520Correction%2520%2528LCC%2529%2520to%2520uni-modal%2520features%2520via%250Aleveraging%2520cross-modal%2520local-correlated%2520properties%2520without%2520any%2520extra%250Aannotations.%2520This%2520enforces%2520uni-modal%2520encoders%2520to%2520highlight%2520similar%2520semantics%250Ashared%2520by%2520audio%2520and%2520visual%2520features.%2520ii%2529%2520To%2520better%2520aggregate%2520such%2520audio%2520and%250Avisual%2520features%252C%2520we%2520further%2520customize%2520Cross-modal%2520Dynamic%2520Perception%2520layer%250A%2528CDP%2529%2520in%2520cross-modal%2520feature%2520pyramid%2520to%2520understand%2520local%2520temporal%2520patterns%2520of%250Aaudio-visual%2520events%2520by%2520imposing%2520local%2520consistency%2520within%2520multimodal%2520features%2520in%250Aa%2520data-driven%2520manner.%2520By%2520incorporating%2520LCC%2520and%2520CDP%252C%2520LOCO%2520provides%2520solid%250Aperformance%2520gains%2520and%2520outperforms%2520existing%2520methods%2520for%2520DAVE.%2520The%2520source%2520code%250Awill%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07967v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Locality-aware%20Cross-modal%20Correspondence%20Learning%20for%20Dense%0A%20%20Audio-Visual%20Events%20Localization&entry.906535625=Ling%20Xing%20and%20Hongyu%20Qu%20and%20Rui%20Yan%20and%20Xiangbo%20Shu%20and%20Jinhui%20Tang&entry.1292438233=%20%20Dense-localization%20Audio-Visual%20Events%20%28DAVE%29%20aims%20to%20identify%20time%0Aboundaries%20and%20corresponding%20categories%20for%20events%20that%20can%20be%20heard%20and%20seen%0Aconcurrently%20in%20an%20untrimmed%20video.%20Existing%20methods%20typically%20encode%20audio%20and%0Avisual%20representation%20separately%20without%20any%20explicit%20cross-modal%20alignment%0Aconstraint.%20Then%20they%20adopt%20dense%20cross-modal%20attention%20to%20integrate%20multimodal%0Ainformation%20for%20DAVE.%20Thus%20these%20methods%20inevitably%20aggregate%20irrelevant%20noise%0Aand%20events%2C%20especially%20in%20complex%20and%20long%20videos%2C%20leading%20to%20imprecise%0Adetection.%20In%20this%20paper%2C%20we%20present%20LOCO%2C%20a%20Locality-aware%20cross-modal%0ACorrespondence%20learning%20framework%20for%20DAVE.%20The%20core%20idea%20is%20to%20explore%20local%0Atemporal%20continuity%20nature%20of%20audio-visual%20events%2C%20which%20serves%20as%20informative%0Ayet%20free%20supervision%20signals%20to%20guide%20the%20filtering%20of%20irrelevant%20information%0Aand%20inspire%20the%20extraction%20of%20complementary%20multimodal%20information%20during%20both%0Aunimodal%20and%20cross-modal%20learning%20stages.%20i%29%20Specifically%2C%20LOCO%20applies%0ALocality-aware%20Correspondence%20Correction%20%28LCC%29%20to%20uni-modal%20features%20via%0Aleveraging%20cross-modal%20local-correlated%20properties%20without%20any%20extra%0Aannotations.%20This%20enforces%20uni-modal%20encoders%20to%20highlight%20similar%20semantics%0Ashared%20by%20audio%20and%20visual%20features.%20ii%29%20To%20better%20aggregate%20such%20audio%20and%0Avisual%20features%2C%20we%20further%20customize%20Cross-modal%20Dynamic%20Perception%20layer%0A%28CDP%29%20in%20cross-modal%20feature%20pyramid%20to%20understand%20local%20temporal%20patterns%20of%0Aaudio-visual%20events%20by%20imposing%20local%20consistency%20within%20multimodal%20features%20in%0Aa%20data-driven%20manner.%20By%20incorporating%20LCC%20and%20CDP%2C%20LOCO%20provides%20solid%0Aperformance%20gains%20and%20outperforms%20existing%20methods%20for%20DAVE.%20The%20source%20code%0Awill%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07967v1&entry.124074799=Read"},
{"title": "DEAR: Depth-Enhanced Action Recognition", "author": "Sadegh Rahmaniboldaji and Filip Rybansky and Quoc Vuong and Frank Guerin and Andrew Gilbert", "abstract": "  Detecting actions in videos, particularly within cluttered scenes, poses\nsignificant challenges due to the limitations of 2D frame analysis from a\ncamera perspective. Unlike human vision, which benefits from 3D understanding,\nrecognizing actions in such environments can be difficult. This research\nintroduces a novel approach integrating 3D features and depth maps alongside\nRGB features to enhance action recognition accuracy. Our method involves\nprocessing estimated depth maps through a separate branch from the RGB feature\nencoder and fusing the features to understand the scene and actions\ncomprehensively. Using the Side4Video framework and VideoMamba, which employ\nCLIP and VisionMamba for spatial feature extraction, our approach outperformed\nour implementation of the Side4Video network on the Something-Something V2\ndataset. Our code is available at: https://github.com/SadeghRahmaniB/DEAR\n", "link": "http://arxiv.org/abs/2408.15679v2", "date": "2024-09-12", "relevancy": 2.7307, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5538}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5538}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5307}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DEAR%3A%20Depth-Enhanced%20Action%20Recognition&body=Title%3A%20DEAR%3A%20Depth-Enhanced%20Action%20Recognition%0AAuthor%3A%20Sadegh%20Rahmaniboldaji%20and%20Filip%20Rybansky%20and%20Quoc%20Vuong%20and%20Frank%20Guerin%20and%20Andrew%20Gilbert%0AAbstract%3A%20%20%20Detecting%20actions%20in%20videos%2C%20particularly%20within%20cluttered%20scenes%2C%20poses%0Asignificant%20challenges%20due%20to%20the%20limitations%20of%202D%20frame%20analysis%20from%20a%0Acamera%20perspective.%20Unlike%20human%20vision%2C%20which%20benefits%20from%203D%20understanding%2C%0Arecognizing%20actions%20in%20such%20environments%20can%20be%20difficult.%20This%20research%0Aintroduces%20a%20novel%20approach%20integrating%203D%20features%20and%20depth%20maps%20alongside%0ARGB%20features%20to%20enhance%20action%20recognition%20accuracy.%20Our%20method%20involves%0Aprocessing%20estimated%20depth%20maps%20through%20a%20separate%20branch%20from%20the%20RGB%20feature%0Aencoder%20and%20fusing%20the%20features%20to%20understand%20the%20scene%20and%20actions%0Acomprehensively.%20Using%20the%20Side4Video%20framework%20and%20VideoMamba%2C%20which%20employ%0ACLIP%20and%20VisionMamba%20for%20spatial%20feature%20extraction%2C%20our%20approach%20outperformed%0Aour%20implementation%20of%20the%20Side4Video%20network%20on%20the%20Something-Something%20V2%0Adataset.%20Our%20code%20is%20available%20at%3A%20https%3A//github.com/SadeghRahmaniB/DEAR%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15679v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDEAR%253A%2520Depth-Enhanced%2520Action%2520Recognition%26entry.906535625%3DSadegh%2520Rahmaniboldaji%2520and%2520Filip%2520Rybansky%2520and%2520Quoc%2520Vuong%2520and%2520Frank%2520Guerin%2520and%2520Andrew%2520Gilbert%26entry.1292438233%3D%2520%2520Detecting%2520actions%2520in%2520videos%252C%2520particularly%2520within%2520cluttered%2520scenes%252C%2520poses%250Asignificant%2520challenges%2520due%2520to%2520the%2520limitations%2520of%25202D%2520frame%2520analysis%2520from%2520a%250Acamera%2520perspective.%2520Unlike%2520human%2520vision%252C%2520which%2520benefits%2520from%25203D%2520understanding%252C%250Arecognizing%2520actions%2520in%2520such%2520environments%2520can%2520be%2520difficult.%2520This%2520research%250Aintroduces%2520a%2520novel%2520approach%2520integrating%25203D%2520features%2520and%2520depth%2520maps%2520alongside%250ARGB%2520features%2520to%2520enhance%2520action%2520recognition%2520accuracy.%2520Our%2520method%2520involves%250Aprocessing%2520estimated%2520depth%2520maps%2520through%2520a%2520separate%2520branch%2520from%2520the%2520RGB%2520feature%250Aencoder%2520and%2520fusing%2520the%2520features%2520to%2520understand%2520the%2520scene%2520and%2520actions%250Acomprehensively.%2520Using%2520the%2520Side4Video%2520framework%2520and%2520VideoMamba%252C%2520which%2520employ%250ACLIP%2520and%2520VisionMamba%2520for%2520spatial%2520feature%2520extraction%252C%2520our%2520approach%2520outperformed%250Aour%2520implementation%2520of%2520the%2520Side4Video%2520network%2520on%2520the%2520Something-Something%2520V2%250Adataset.%2520Our%2520code%2520is%2520available%2520at%253A%2520https%253A//github.com/SadeghRahmaniB/DEAR%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15679v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DEAR%3A%20Depth-Enhanced%20Action%20Recognition&entry.906535625=Sadegh%20Rahmaniboldaji%20and%20Filip%20Rybansky%20and%20Quoc%20Vuong%20and%20Frank%20Guerin%20and%20Andrew%20Gilbert&entry.1292438233=%20%20Detecting%20actions%20in%20videos%2C%20particularly%20within%20cluttered%20scenes%2C%20poses%0Asignificant%20challenges%20due%20to%20the%20limitations%20of%202D%20frame%20analysis%20from%20a%0Acamera%20perspective.%20Unlike%20human%20vision%2C%20which%20benefits%20from%203D%20understanding%2C%0Arecognizing%20actions%20in%20such%20environments%20can%20be%20difficult.%20This%20research%0Aintroduces%20a%20novel%20approach%20integrating%203D%20features%20and%20depth%20maps%20alongside%0ARGB%20features%20to%20enhance%20action%20recognition%20accuracy.%20Our%20method%20involves%0Aprocessing%20estimated%20depth%20maps%20through%20a%20separate%20branch%20from%20the%20RGB%20feature%0Aencoder%20and%20fusing%20the%20features%20to%20understand%20the%20scene%20and%20actions%0Acomprehensively.%20Using%20the%20Side4Video%20framework%20and%20VideoMamba%2C%20which%20employ%0ACLIP%20and%20VisionMamba%20for%20spatial%20feature%20extraction%2C%20our%20approach%20outperformed%0Aour%20implementation%20of%20the%20Side4Video%20network%20on%20the%20Something-Something%20V2%0Adataset.%20Our%20code%20is%20available%20at%3A%20https%3A//github.com/SadeghRahmaniB/DEAR%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15679v2&entry.124074799=Read"},
{"title": "OCTAMamba: A State-Space Model Approach for Precision OCTA Vasculature\n  Segmentation", "author": "Shun Zou and Zhuo Zhang and Guangwei Gao", "abstract": "  Optical Coherence Tomography Angiography (OCTA) is a crucial imaging\ntechnique for visualizing retinal vasculature and diagnosing eye diseases such\nas diabetic retinopathy and glaucoma. However, precise segmentation of OCTA\nvasculature remains challenging due to the multi-scale vessel structures and\nnoise from poor image quality and eye lesions. In this study, we proposed\nOCTAMamba, a novel U-shaped network based on the Mamba architecture, designed\nto segment vasculature in OCTA accurately. OCTAMamba integrates a Quad Stream\nEfficient Mining Embedding Module for local feature extraction, a Multi-Scale\nDilated Asymmetric Convolution Module to capture multi-scale vasculature, and a\nFocused Feature Recalibration Module to filter noise and highlight target\nareas. Our method achieves efficient global modeling and local feature\nextraction while maintaining linear complexity, making it suitable for\nlow-computation medical applications. Extensive experiments on the OCTA 3M,\nOCTA 6M, and ROSSA datasets demonstrated that OCTAMamba outperforms\nstate-of-the-art methods, providing a new reference for efficient OCTA\nsegmentation. Code is available at https://github.com/zs1314/OCTAMamba\n", "link": "http://arxiv.org/abs/2409.08000v1", "date": "2024-09-12", "relevancy": 2.7235, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5545}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5506}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OCTAMamba%3A%20A%20State-Space%20Model%20Approach%20for%20Precision%20OCTA%20Vasculature%0A%20%20Segmentation&body=Title%3A%20OCTAMamba%3A%20A%20State-Space%20Model%20Approach%20for%20Precision%20OCTA%20Vasculature%0A%20%20Segmentation%0AAuthor%3A%20Shun%20Zou%20and%20Zhuo%20Zhang%20and%20Guangwei%20Gao%0AAbstract%3A%20%20%20Optical%20Coherence%20Tomography%20Angiography%20%28OCTA%29%20is%20a%20crucial%20imaging%0Atechnique%20for%20visualizing%20retinal%20vasculature%20and%20diagnosing%20eye%20diseases%20such%0Aas%20diabetic%20retinopathy%20and%20glaucoma.%20However%2C%20precise%20segmentation%20of%20OCTA%0Avasculature%20remains%20challenging%20due%20to%20the%20multi-scale%20vessel%20structures%20and%0Anoise%20from%20poor%20image%20quality%20and%20eye%20lesions.%20In%20this%20study%2C%20we%20proposed%0AOCTAMamba%2C%20a%20novel%20U-shaped%20network%20based%20on%20the%20Mamba%20architecture%2C%20designed%0Ato%20segment%20vasculature%20in%20OCTA%20accurately.%20OCTAMamba%20integrates%20a%20Quad%20Stream%0AEfficient%20Mining%20Embedding%20Module%20for%20local%20feature%20extraction%2C%20a%20Multi-Scale%0ADilated%20Asymmetric%20Convolution%20Module%20to%20capture%20multi-scale%20vasculature%2C%20and%20a%0AFocused%20Feature%20Recalibration%20Module%20to%20filter%20noise%20and%20highlight%20target%0Aareas.%20Our%20method%20achieves%20efficient%20global%20modeling%20and%20local%20feature%0Aextraction%20while%20maintaining%20linear%20complexity%2C%20making%20it%20suitable%20for%0Alow-computation%20medical%20applications.%20Extensive%20experiments%20on%20the%20OCTA%203M%2C%0AOCTA%206M%2C%20and%20ROSSA%20datasets%20demonstrated%20that%20OCTAMamba%20outperforms%0Astate-of-the-art%20methods%2C%20providing%20a%20new%20reference%20for%20efficient%20OCTA%0Asegmentation.%20Code%20is%20available%20at%20https%3A//github.com/zs1314/OCTAMamba%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08000v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOCTAMamba%253A%2520A%2520State-Space%2520Model%2520Approach%2520for%2520Precision%2520OCTA%2520Vasculature%250A%2520%2520Segmentation%26entry.906535625%3DShun%2520Zou%2520and%2520Zhuo%2520Zhang%2520and%2520Guangwei%2520Gao%26entry.1292438233%3D%2520%2520Optical%2520Coherence%2520Tomography%2520Angiography%2520%2528OCTA%2529%2520is%2520a%2520crucial%2520imaging%250Atechnique%2520for%2520visualizing%2520retinal%2520vasculature%2520and%2520diagnosing%2520eye%2520diseases%2520such%250Aas%2520diabetic%2520retinopathy%2520and%2520glaucoma.%2520However%252C%2520precise%2520segmentation%2520of%2520OCTA%250Avasculature%2520remains%2520challenging%2520due%2520to%2520the%2520multi-scale%2520vessel%2520structures%2520and%250Anoise%2520from%2520poor%2520image%2520quality%2520and%2520eye%2520lesions.%2520In%2520this%2520study%252C%2520we%2520proposed%250AOCTAMamba%252C%2520a%2520novel%2520U-shaped%2520network%2520based%2520on%2520the%2520Mamba%2520architecture%252C%2520designed%250Ato%2520segment%2520vasculature%2520in%2520OCTA%2520accurately.%2520OCTAMamba%2520integrates%2520a%2520Quad%2520Stream%250AEfficient%2520Mining%2520Embedding%2520Module%2520for%2520local%2520feature%2520extraction%252C%2520a%2520Multi-Scale%250ADilated%2520Asymmetric%2520Convolution%2520Module%2520to%2520capture%2520multi-scale%2520vasculature%252C%2520and%2520a%250AFocused%2520Feature%2520Recalibration%2520Module%2520to%2520filter%2520noise%2520and%2520highlight%2520target%250Aareas.%2520Our%2520method%2520achieves%2520efficient%2520global%2520modeling%2520and%2520local%2520feature%250Aextraction%2520while%2520maintaining%2520linear%2520complexity%252C%2520making%2520it%2520suitable%2520for%250Alow-computation%2520medical%2520applications.%2520Extensive%2520experiments%2520on%2520the%2520OCTA%25203M%252C%250AOCTA%25206M%252C%2520and%2520ROSSA%2520datasets%2520demonstrated%2520that%2520OCTAMamba%2520outperforms%250Astate-of-the-art%2520methods%252C%2520providing%2520a%2520new%2520reference%2520for%2520efficient%2520OCTA%250Asegmentation.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/zs1314/OCTAMamba%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08000v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OCTAMamba%3A%20A%20State-Space%20Model%20Approach%20for%20Precision%20OCTA%20Vasculature%0A%20%20Segmentation&entry.906535625=Shun%20Zou%20and%20Zhuo%20Zhang%20and%20Guangwei%20Gao&entry.1292438233=%20%20Optical%20Coherence%20Tomography%20Angiography%20%28OCTA%29%20is%20a%20crucial%20imaging%0Atechnique%20for%20visualizing%20retinal%20vasculature%20and%20diagnosing%20eye%20diseases%20such%0Aas%20diabetic%20retinopathy%20and%20glaucoma.%20However%2C%20precise%20segmentation%20of%20OCTA%0Avasculature%20remains%20challenging%20due%20to%20the%20multi-scale%20vessel%20structures%20and%0Anoise%20from%20poor%20image%20quality%20and%20eye%20lesions.%20In%20this%20study%2C%20we%20proposed%0AOCTAMamba%2C%20a%20novel%20U-shaped%20network%20based%20on%20the%20Mamba%20architecture%2C%20designed%0Ato%20segment%20vasculature%20in%20OCTA%20accurately.%20OCTAMamba%20integrates%20a%20Quad%20Stream%0AEfficient%20Mining%20Embedding%20Module%20for%20local%20feature%20extraction%2C%20a%20Multi-Scale%0ADilated%20Asymmetric%20Convolution%20Module%20to%20capture%20multi-scale%20vasculature%2C%20and%20a%0AFocused%20Feature%20Recalibration%20Module%20to%20filter%20noise%20and%20highlight%20target%0Aareas.%20Our%20method%20achieves%20efficient%20global%20modeling%20and%20local%20feature%0Aextraction%20while%20maintaining%20linear%20complexity%2C%20making%20it%20suitable%20for%0Alow-computation%20medical%20applications.%20Extensive%20experiments%20on%20the%20OCTA%203M%2C%0AOCTA%206M%2C%20and%20ROSSA%20datasets%20demonstrated%20that%20OCTAMamba%20outperforms%0Astate-of-the-art%20methods%2C%20providing%20a%20new%20reference%20for%20efficient%20OCTA%0Asegmentation.%20Code%20is%20available%20at%20https%3A//github.com/zs1314/OCTAMamba%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08000v1&entry.124074799=Read"},
{"title": "Cross-Attention Based Influence Model for Manual and Nonmanual Sign\n  Language Analysis", "author": "Lipisha Chaudhary and Fei Xu and Ifeoma Nwogu", "abstract": "  Both manual (relating to the use of hands) and non-manual markers (NMM), such\nas facial expressions or mouthing cues, are important for providing the\ncomplete meaning of phrases in American Sign Language (ASL). Efforts have been\nmade in advancing sign language to spoken/written language understanding, but\nmost of these have primarily focused on manual features only. In this work,\nusing advanced neural machine translation methods, we examine and report on the\nextent to which facial expressions contribute to understanding sign language\nphrases. We present a sign language translation architecture consisting of\ntwo-stream encoders, with one encoder handling the face and the other handling\nthe upper body (with hands). We propose a new parallel cross-attention decoding\nmechanism that is useful for quantifying the influence of each input modality\non the output. The two streams from the encoder are directed simultaneously to\ndifferent attention stacks in the decoder. Examining the properties of the\nparallel cross-attention weights allows us to analyze the importance of facial\nmarkers compared to body and hand features during a translating task.\n", "link": "http://arxiv.org/abs/2409.08162v1", "date": "2024-09-12", "relevancy": 2.7024, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5481}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5481}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Attention%20Based%20Influence%20Model%20for%20Manual%20and%20Nonmanual%20Sign%0A%20%20Language%20Analysis&body=Title%3A%20Cross-Attention%20Based%20Influence%20Model%20for%20Manual%20and%20Nonmanual%20Sign%0A%20%20Language%20Analysis%0AAuthor%3A%20Lipisha%20Chaudhary%20and%20Fei%20Xu%20and%20Ifeoma%20Nwogu%0AAbstract%3A%20%20%20Both%20manual%20%28relating%20to%20the%20use%20of%20hands%29%20and%20non-manual%20markers%20%28NMM%29%2C%20such%0Aas%20facial%20expressions%20or%20mouthing%20cues%2C%20are%20important%20for%20providing%20the%0Acomplete%20meaning%20of%20phrases%20in%20American%20Sign%20Language%20%28ASL%29.%20Efforts%20have%20been%0Amade%20in%20advancing%20sign%20language%20to%20spoken/written%20language%20understanding%2C%20but%0Amost%20of%20these%20have%20primarily%20focused%20on%20manual%20features%20only.%20In%20this%20work%2C%0Ausing%20advanced%20neural%20machine%20translation%20methods%2C%20we%20examine%20and%20report%20on%20the%0Aextent%20to%20which%20facial%20expressions%20contribute%20to%20understanding%20sign%20language%0Aphrases.%20We%20present%20a%20sign%20language%20translation%20architecture%20consisting%20of%0Atwo-stream%20encoders%2C%20with%20one%20encoder%20handling%20the%20face%20and%20the%20other%20handling%0Athe%20upper%20body%20%28with%20hands%29.%20We%20propose%20a%20new%20parallel%20cross-attention%20decoding%0Amechanism%20that%20is%20useful%20for%20quantifying%20the%20influence%20of%20each%20input%20modality%0Aon%20the%20output.%20The%20two%20streams%20from%20the%20encoder%20are%20directed%20simultaneously%20to%0Adifferent%20attention%20stacks%20in%20the%20decoder.%20Examining%20the%20properties%20of%20the%0Aparallel%20cross-attention%20weights%20allows%20us%20to%20analyze%20the%20importance%20of%20facial%0Amarkers%20compared%20to%20body%20and%20hand%20features%20during%20a%20translating%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08162v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Attention%2520Based%2520Influence%2520Model%2520for%2520Manual%2520and%2520Nonmanual%2520Sign%250A%2520%2520Language%2520Analysis%26entry.906535625%3DLipisha%2520Chaudhary%2520and%2520Fei%2520Xu%2520and%2520Ifeoma%2520Nwogu%26entry.1292438233%3D%2520%2520Both%2520manual%2520%2528relating%2520to%2520the%2520use%2520of%2520hands%2529%2520and%2520non-manual%2520markers%2520%2528NMM%2529%252C%2520such%250Aas%2520facial%2520expressions%2520or%2520mouthing%2520cues%252C%2520are%2520important%2520for%2520providing%2520the%250Acomplete%2520meaning%2520of%2520phrases%2520in%2520American%2520Sign%2520Language%2520%2528ASL%2529.%2520Efforts%2520have%2520been%250Amade%2520in%2520advancing%2520sign%2520language%2520to%2520spoken/written%2520language%2520understanding%252C%2520but%250Amost%2520of%2520these%2520have%2520primarily%2520focused%2520on%2520manual%2520features%2520only.%2520In%2520this%2520work%252C%250Ausing%2520advanced%2520neural%2520machine%2520translation%2520methods%252C%2520we%2520examine%2520and%2520report%2520on%2520the%250Aextent%2520to%2520which%2520facial%2520expressions%2520contribute%2520to%2520understanding%2520sign%2520language%250Aphrases.%2520We%2520present%2520a%2520sign%2520language%2520translation%2520architecture%2520consisting%2520of%250Atwo-stream%2520encoders%252C%2520with%2520one%2520encoder%2520handling%2520the%2520face%2520and%2520the%2520other%2520handling%250Athe%2520upper%2520body%2520%2528with%2520hands%2529.%2520We%2520propose%2520a%2520new%2520parallel%2520cross-attention%2520decoding%250Amechanism%2520that%2520is%2520useful%2520for%2520quantifying%2520the%2520influence%2520of%2520each%2520input%2520modality%250Aon%2520the%2520output.%2520The%2520two%2520streams%2520from%2520the%2520encoder%2520are%2520directed%2520simultaneously%2520to%250Adifferent%2520attention%2520stacks%2520in%2520the%2520decoder.%2520Examining%2520the%2520properties%2520of%2520the%250Aparallel%2520cross-attention%2520weights%2520allows%2520us%2520to%2520analyze%2520the%2520importance%2520of%2520facial%250Amarkers%2520compared%2520to%2520body%2520and%2520hand%2520features%2520during%2520a%2520translating%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08162v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Attention%20Based%20Influence%20Model%20for%20Manual%20and%20Nonmanual%20Sign%0A%20%20Language%20Analysis&entry.906535625=Lipisha%20Chaudhary%20and%20Fei%20Xu%20and%20Ifeoma%20Nwogu&entry.1292438233=%20%20Both%20manual%20%28relating%20to%20the%20use%20of%20hands%29%20and%20non-manual%20markers%20%28NMM%29%2C%20such%0Aas%20facial%20expressions%20or%20mouthing%20cues%2C%20are%20important%20for%20providing%20the%0Acomplete%20meaning%20of%20phrases%20in%20American%20Sign%20Language%20%28ASL%29.%20Efforts%20have%20been%0Amade%20in%20advancing%20sign%20language%20to%20spoken/written%20language%20understanding%2C%20but%0Amost%20of%20these%20have%20primarily%20focused%20on%20manual%20features%20only.%20In%20this%20work%2C%0Ausing%20advanced%20neural%20machine%20translation%20methods%2C%20we%20examine%20and%20report%20on%20the%0Aextent%20to%20which%20facial%20expressions%20contribute%20to%20understanding%20sign%20language%0Aphrases.%20We%20present%20a%20sign%20language%20translation%20architecture%20consisting%20of%0Atwo-stream%20encoders%2C%20with%20one%20encoder%20handling%20the%20face%20and%20the%20other%20handling%0Athe%20upper%20body%20%28with%20hands%29.%20We%20propose%20a%20new%20parallel%20cross-attention%20decoding%0Amechanism%20that%20is%20useful%20for%20quantifying%20the%20influence%20of%20each%20input%20modality%0Aon%20the%20output.%20The%20two%20streams%20from%20the%20encoder%20are%20directed%20simultaneously%20to%0Adifferent%20attention%20stacks%20in%20the%20decoder.%20Examining%20the%20properties%20of%20the%0Aparallel%20cross-attention%20weights%20allows%20us%20to%20analyze%20the%20importance%20of%20facial%0Amarkers%20compared%20to%20body%20and%20hand%20features%20during%20a%20translating%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08162v1&entry.124074799=Read"},
{"title": "Enhancing Few-Shot Image Classification through Learnable Multi-Scale\n  Embedding and Attention Mechanisms", "author": "Fatemeh Askari and Amirreza Fateh and Mohammad Reza Mohammadi", "abstract": "  In the context of few-shot classification, the goal is to train a classifier\nusing a limited number of samples while maintaining satisfactory performance.\nHowever, traditional metric-based methods exhibit certain limitations in\nachieving this objective. These methods typically rely on a single distance\nvalue between the query feature and support feature, thereby overlooking the\ncontribution of shallow features. To overcome this challenge, we propose a\nnovel approach in this paper. Our approach involves utilizing multi-output\nembedding network that maps samples into distinct feature spaces. The proposed\nmethod extract feature vectors at different stages, enabling the model to\ncapture both global and abstract features. By utilizing these diverse feature\nspaces, our model enhances its performance. Moreover, employing a\nself-attention mechanism improves the refinement of features at each stage,\nleading to even more robust representations and improved overall performance.\nFurthermore, assigning learnable weights to each stage significantly improved\nperformance and results. We conducted comprehensive evaluations on the\nMiniImageNet and FC100 datasets, specifically in the 5-way 1-shot and 5-way\n5-shot scenarios. Additionally, we performed a cross-domain task from\nMiniImageNet to the CUB dataset, achieving high accuracy in the testing domain.\nThese evaluations demonstrate the efficacy of our proposed method in comparison\nto state-of-the-art approaches. https://github.com/FatemehAskari/MSENet\n", "link": "http://arxiv.org/abs/2409.07989v1", "date": "2024-09-12", "relevancy": 2.6642, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5512}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5249}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Few-Shot%20Image%20Classification%20through%20Learnable%20Multi-Scale%0A%20%20Embedding%20and%20Attention%20Mechanisms&body=Title%3A%20Enhancing%20Few-Shot%20Image%20Classification%20through%20Learnable%20Multi-Scale%0A%20%20Embedding%20and%20Attention%20Mechanisms%0AAuthor%3A%20Fatemeh%20Askari%20and%20Amirreza%20Fateh%20and%20Mohammad%20Reza%20Mohammadi%0AAbstract%3A%20%20%20In%20the%20context%20of%20few-shot%20classification%2C%20the%20goal%20is%20to%20train%20a%20classifier%0Ausing%20a%20limited%20number%20of%20samples%20while%20maintaining%20satisfactory%20performance.%0AHowever%2C%20traditional%20metric-based%20methods%20exhibit%20certain%20limitations%20in%0Aachieving%20this%20objective.%20These%20methods%20typically%20rely%20on%20a%20single%20distance%0Avalue%20between%20the%20query%20feature%20and%20support%20feature%2C%20thereby%20overlooking%20the%0Acontribution%20of%20shallow%20features.%20To%20overcome%20this%20challenge%2C%20we%20propose%20a%0Anovel%20approach%20in%20this%20paper.%20Our%20approach%20involves%20utilizing%20multi-output%0Aembedding%20network%20that%20maps%20samples%20into%20distinct%20feature%20spaces.%20The%20proposed%0Amethod%20extract%20feature%20vectors%20at%20different%20stages%2C%20enabling%20the%20model%20to%0Acapture%20both%20global%20and%20abstract%20features.%20By%20utilizing%20these%20diverse%20feature%0Aspaces%2C%20our%20model%20enhances%20its%20performance.%20Moreover%2C%20employing%20a%0Aself-attention%20mechanism%20improves%20the%20refinement%20of%20features%20at%20each%20stage%2C%0Aleading%20to%20even%20more%20robust%20representations%20and%20improved%20overall%20performance.%0AFurthermore%2C%20assigning%20learnable%20weights%20to%20each%20stage%20significantly%20improved%0Aperformance%20and%20results.%20We%20conducted%20comprehensive%20evaluations%20on%20the%0AMiniImageNet%20and%20FC100%20datasets%2C%20specifically%20in%20the%205-way%201-shot%20and%205-way%0A5-shot%20scenarios.%20Additionally%2C%20we%20performed%20a%20cross-domain%20task%20from%0AMiniImageNet%20to%20the%20CUB%20dataset%2C%20achieving%20high%20accuracy%20in%20the%20testing%20domain.%0AThese%20evaluations%20demonstrate%20the%20efficacy%20of%20our%20proposed%20method%20in%20comparison%0Ato%20state-of-the-art%20approaches.%20https%3A//github.com/FatemehAskari/MSENet%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07989v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Few-Shot%2520Image%2520Classification%2520through%2520Learnable%2520Multi-Scale%250A%2520%2520Embedding%2520and%2520Attention%2520Mechanisms%26entry.906535625%3DFatemeh%2520Askari%2520and%2520Amirreza%2520Fateh%2520and%2520Mohammad%2520Reza%2520Mohammadi%26entry.1292438233%3D%2520%2520In%2520the%2520context%2520of%2520few-shot%2520classification%252C%2520the%2520goal%2520is%2520to%2520train%2520a%2520classifier%250Ausing%2520a%2520limited%2520number%2520of%2520samples%2520while%2520maintaining%2520satisfactory%2520performance.%250AHowever%252C%2520traditional%2520metric-based%2520methods%2520exhibit%2520certain%2520limitations%2520in%250Aachieving%2520this%2520objective.%2520These%2520methods%2520typically%2520rely%2520on%2520a%2520single%2520distance%250Avalue%2520between%2520the%2520query%2520feature%2520and%2520support%2520feature%252C%2520thereby%2520overlooking%2520the%250Acontribution%2520of%2520shallow%2520features.%2520To%2520overcome%2520this%2520challenge%252C%2520we%2520propose%2520a%250Anovel%2520approach%2520in%2520this%2520paper.%2520Our%2520approach%2520involves%2520utilizing%2520multi-output%250Aembedding%2520network%2520that%2520maps%2520samples%2520into%2520distinct%2520feature%2520spaces.%2520The%2520proposed%250Amethod%2520extract%2520feature%2520vectors%2520at%2520different%2520stages%252C%2520enabling%2520the%2520model%2520to%250Acapture%2520both%2520global%2520and%2520abstract%2520features.%2520By%2520utilizing%2520these%2520diverse%2520feature%250Aspaces%252C%2520our%2520model%2520enhances%2520its%2520performance.%2520Moreover%252C%2520employing%2520a%250Aself-attention%2520mechanism%2520improves%2520the%2520refinement%2520of%2520features%2520at%2520each%2520stage%252C%250Aleading%2520to%2520even%2520more%2520robust%2520representations%2520and%2520improved%2520overall%2520performance.%250AFurthermore%252C%2520assigning%2520learnable%2520weights%2520to%2520each%2520stage%2520significantly%2520improved%250Aperformance%2520and%2520results.%2520We%2520conducted%2520comprehensive%2520evaluations%2520on%2520the%250AMiniImageNet%2520and%2520FC100%2520datasets%252C%2520specifically%2520in%2520the%25205-way%25201-shot%2520and%25205-way%250A5-shot%2520scenarios.%2520Additionally%252C%2520we%2520performed%2520a%2520cross-domain%2520task%2520from%250AMiniImageNet%2520to%2520the%2520CUB%2520dataset%252C%2520achieving%2520high%2520accuracy%2520in%2520the%2520testing%2520domain.%250AThese%2520evaluations%2520demonstrate%2520the%2520efficacy%2520of%2520our%2520proposed%2520method%2520in%2520comparison%250Ato%2520state-of-the-art%2520approaches.%2520https%253A//github.com/FatemehAskari/MSENet%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07989v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Few-Shot%20Image%20Classification%20through%20Learnable%20Multi-Scale%0A%20%20Embedding%20and%20Attention%20Mechanisms&entry.906535625=Fatemeh%20Askari%20and%20Amirreza%20Fateh%20and%20Mohammad%20Reza%20Mohammadi&entry.1292438233=%20%20In%20the%20context%20of%20few-shot%20classification%2C%20the%20goal%20is%20to%20train%20a%20classifier%0Ausing%20a%20limited%20number%20of%20samples%20while%20maintaining%20satisfactory%20performance.%0AHowever%2C%20traditional%20metric-based%20methods%20exhibit%20certain%20limitations%20in%0Aachieving%20this%20objective.%20These%20methods%20typically%20rely%20on%20a%20single%20distance%0Avalue%20between%20the%20query%20feature%20and%20support%20feature%2C%20thereby%20overlooking%20the%0Acontribution%20of%20shallow%20features.%20To%20overcome%20this%20challenge%2C%20we%20propose%20a%0Anovel%20approach%20in%20this%20paper.%20Our%20approach%20involves%20utilizing%20multi-output%0Aembedding%20network%20that%20maps%20samples%20into%20distinct%20feature%20spaces.%20The%20proposed%0Amethod%20extract%20feature%20vectors%20at%20different%20stages%2C%20enabling%20the%20model%20to%0Acapture%20both%20global%20and%20abstract%20features.%20By%20utilizing%20these%20diverse%20feature%0Aspaces%2C%20our%20model%20enhances%20its%20performance.%20Moreover%2C%20employing%20a%0Aself-attention%20mechanism%20improves%20the%20refinement%20of%20features%20at%20each%20stage%2C%0Aleading%20to%20even%20more%20robust%20representations%20and%20improved%20overall%20performance.%0AFurthermore%2C%20assigning%20learnable%20weights%20to%20each%20stage%20significantly%20improved%0Aperformance%20and%20results.%20We%20conducted%20comprehensive%20evaluations%20on%20the%0AMiniImageNet%20and%20FC100%20datasets%2C%20specifically%20in%20the%205-way%201-shot%20and%205-way%0A5-shot%20scenarios.%20Additionally%2C%20we%20performed%20a%20cross-domain%20task%20from%0AMiniImageNet%20to%20the%20CUB%20dataset%2C%20achieving%20high%20accuracy%20in%20the%20testing%20domain.%0AThese%20evaluations%20demonstrate%20the%20efficacy%20of%20our%20proposed%20method%20in%20comparison%0Ato%20state-of-the-art%20approaches.%20https%3A//github.com/FatemehAskari/MSENet%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07989v1&entry.124074799=Read"},
{"title": "Improving Virtual Try-On with Garment-focused Diffusion Models", "author": "Siqi Wan and Yehao Li and Jingwen Chen and Yingwei Pan and Ting Yao and Yang Cao and Tao Mei", "abstract": "  Diffusion models have led to the revolutionizing of generative modeling in\nnumerous image synthesis tasks. Nevertheless, it is not trivial to directly\napply diffusion models for synthesizing an image of a target person wearing a\ngiven in-shop garment, i.e., image-based virtual try-on (VTON) task. The\ndifficulty originates from the aspect that the diffusion process should not\nonly produce holistically high-fidelity photorealistic image of the target\nperson, but also locally preserve every appearance and texture detail of the\ngiven garment. To address this, we shape a new Diffusion model, namely GarDiff,\nwhich triggers the garment-focused diffusion process with amplified guidance of\nboth basic visual appearance and detailed textures (i.e., high-frequency\ndetails) derived from the given garment. GarDiff first remoulds a pre-trained\nlatent diffusion model with additional appearance priors derived from the CLIP\nand VAE encodings of the reference garment. Meanwhile, a novel garment-focused\nadapter is integrated into the UNet of diffusion model, pursuing local\nfine-grained alignment with the visual appearance of reference garment and\nhuman pose. We specifically design an appearance loss over the synthesized\ngarment to enhance the crucial, high-frequency details. Extensive experiments\non VITON-HD and DressCode datasets demonstrate the superiority of our GarDiff\nwhen compared to state-of-the-art VTON approaches. Code is publicly available\nat:\n\\href{https://github.com/siqi0905/GarDiff/tree/master}{https://github.com/siqi0905/GarDiff/tree/master}.\n", "link": "http://arxiv.org/abs/2409.08258v1", "date": "2024-09-12", "relevancy": 2.6632, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.7189}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6656}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Virtual%20Try-On%20with%20Garment-focused%20Diffusion%20Models&body=Title%3A%20Improving%20Virtual%20Try-On%20with%20Garment-focused%20Diffusion%20Models%0AAuthor%3A%20Siqi%20Wan%20and%20Yehao%20Li%20and%20Jingwen%20Chen%20and%20Yingwei%20Pan%20and%20Ting%20Yao%20and%20Yang%20Cao%20and%20Tao%20Mei%0AAbstract%3A%20%20%20Diffusion%20models%20have%20led%20to%20the%20revolutionizing%20of%20generative%20modeling%20in%0Anumerous%20image%20synthesis%20tasks.%20Nevertheless%2C%20it%20is%20not%20trivial%20to%20directly%0Aapply%20diffusion%20models%20for%20synthesizing%20an%20image%20of%20a%20target%20person%20wearing%20a%0Agiven%20in-shop%20garment%2C%20i.e.%2C%20image-based%20virtual%20try-on%20%28VTON%29%20task.%20The%0Adifficulty%20originates%20from%20the%20aspect%20that%20the%20diffusion%20process%20should%20not%0Aonly%20produce%20holistically%20high-fidelity%20photorealistic%20image%20of%20the%20target%0Aperson%2C%20but%20also%20locally%20preserve%20every%20appearance%20and%20texture%20detail%20of%20the%0Agiven%20garment.%20To%20address%20this%2C%20we%20shape%20a%20new%20Diffusion%20model%2C%20namely%20GarDiff%2C%0Awhich%20triggers%20the%20garment-focused%20diffusion%20process%20with%20amplified%20guidance%20of%0Aboth%20basic%20visual%20appearance%20and%20detailed%20textures%20%28i.e.%2C%20high-frequency%0Adetails%29%20derived%20from%20the%20given%20garment.%20GarDiff%20first%20remoulds%20a%20pre-trained%0Alatent%20diffusion%20model%20with%20additional%20appearance%20priors%20derived%20from%20the%20CLIP%0Aand%20VAE%20encodings%20of%20the%20reference%20garment.%20Meanwhile%2C%20a%20novel%20garment-focused%0Aadapter%20is%20integrated%20into%20the%20UNet%20of%20diffusion%20model%2C%20pursuing%20local%0Afine-grained%20alignment%20with%20the%20visual%20appearance%20of%20reference%20garment%20and%0Ahuman%20pose.%20We%20specifically%20design%20an%20appearance%20loss%20over%20the%20synthesized%0Agarment%20to%20enhance%20the%20crucial%2C%20high-frequency%20details.%20Extensive%20experiments%0Aon%20VITON-HD%20and%20DressCode%20datasets%20demonstrate%20the%20superiority%20of%20our%20GarDiff%0Awhen%20compared%20to%20state-of-the-art%20VTON%20approaches.%20Code%20is%20publicly%20available%0Aat%3A%0A%5Chref%7Bhttps%3A//github.com/siqi0905/GarDiff/tree/master%7D%7Bhttps%3A//github.com/siqi0905/GarDiff/tree/master%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08258v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Virtual%2520Try-On%2520with%2520Garment-focused%2520Diffusion%2520Models%26entry.906535625%3DSiqi%2520Wan%2520and%2520Yehao%2520Li%2520and%2520Jingwen%2520Chen%2520and%2520Yingwei%2520Pan%2520and%2520Ting%2520Yao%2520and%2520Yang%2520Cao%2520and%2520Tao%2520Mei%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520led%2520to%2520the%2520revolutionizing%2520of%2520generative%2520modeling%2520in%250Anumerous%2520image%2520synthesis%2520tasks.%2520Nevertheless%252C%2520it%2520is%2520not%2520trivial%2520to%2520directly%250Aapply%2520diffusion%2520models%2520for%2520synthesizing%2520an%2520image%2520of%2520a%2520target%2520person%2520wearing%2520a%250Agiven%2520in-shop%2520garment%252C%2520i.e.%252C%2520image-based%2520virtual%2520try-on%2520%2528VTON%2529%2520task.%2520The%250Adifficulty%2520originates%2520from%2520the%2520aspect%2520that%2520the%2520diffusion%2520process%2520should%2520not%250Aonly%2520produce%2520holistically%2520high-fidelity%2520photorealistic%2520image%2520of%2520the%2520target%250Aperson%252C%2520but%2520also%2520locally%2520preserve%2520every%2520appearance%2520and%2520texture%2520detail%2520of%2520the%250Agiven%2520garment.%2520To%2520address%2520this%252C%2520we%2520shape%2520a%2520new%2520Diffusion%2520model%252C%2520namely%2520GarDiff%252C%250Awhich%2520triggers%2520the%2520garment-focused%2520diffusion%2520process%2520with%2520amplified%2520guidance%2520of%250Aboth%2520basic%2520visual%2520appearance%2520and%2520detailed%2520textures%2520%2528i.e.%252C%2520high-frequency%250Adetails%2529%2520derived%2520from%2520the%2520given%2520garment.%2520GarDiff%2520first%2520remoulds%2520a%2520pre-trained%250Alatent%2520diffusion%2520model%2520with%2520additional%2520appearance%2520priors%2520derived%2520from%2520the%2520CLIP%250Aand%2520VAE%2520encodings%2520of%2520the%2520reference%2520garment.%2520Meanwhile%252C%2520a%2520novel%2520garment-focused%250Aadapter%2520is%2520integrated%2520into%2520the%2520UNet%2520of%2520diffusion%2520model%252C%2520pursuing%2520local%250Afine-grained%2520alignment%2520with%2520the%2520visual%2520appearance%2520of%2520reference%2520garment%2520and%250Ahuman%2520pose.%2520We%2520specifically%2520design%2520an%2520appearance%2520loss%2520over%2520the%2520synthesized%250Agarment%2520to%2520enhance%2520the%2520crucial%252C%2520high-frequency%2520details.%2520Extensive%2520experiments%250Aon%2520VITON-HD%2520and%2520DressCode%2520datasets%2520demonstrate%2520the%2520superiority%2520of%2520our%2520GarDiff%250Awhen%2520compared%2520to%2520state-of-the-art%2520VTON%2520approaches.%2520Code%2520is%2520publicly%2520available%250Aat%253A%250A%255Chref%257Bhttps%253A//github.com/siqi0905/GarDiff/tree/master%257D%257Bhttps%253A//github.com/siqi0905/GarDiff/tree/master%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08258v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Virtual%20Try-On%20with%20Garment-focused%20Diffusion%20Models&entry.906535625=Siqi%20Wan%20and%20Yehao%20Li%20and%20Jingwen%20Chen%20and%20Yingwei%20Pan%20and%20Ting%20Yao%20and%20Yang%20Cao%20and%20Tao%20Mei&entry.1292438233=%20%20Diffusion%20models%20have%20led%20to%20the%20revolutionizing%20of%20generative%20modeling%20in%0Anumerous%20image%20synthesis%20tasks.%20Nevertheless%2C%20it%20is%20not%20trivial%20to%20directly%0Aapply%20diffusion%20models%20for%20synthesizing%20an%20image%20of%20a%20target%20person%20wearing%20a%0Agiven%20in-shop%20garment%2C%20i.e.%2C%20image-based%20virtual%20try-on%20%28VTON%29%20task.%20The%0Adifficulty%20originates%20from%20the%20aspect%20that%20the%20diffusion%20process%20should%20not%0Aonly%20produce%20holistically%20high-fidelity%20photorealistic%20image%20of%20the%20target%0Aperson%2C%20but%20also%20locally%20preserve%20every%20appearance%20and%20texture%20detail%20of%20the%0Agiven%20garment.%20To%20address%20this%2C%20we%20shape%20a%20new%20Diffusion%20model%2C%20namely%20GarDiff%2C%0Awhich%20triggers%20the%20garment-focused%20diffusion%20process%20with%20amplified%20guidance%20of%0Aboth%20basic%20visual%20appearance%20and%20detailed%20textures%20%28i.e.%2C%20high-frequency%0Adetails%29%20derived%20from%20the%20given%20garment.%20GarDiff%20first%20remoulds%20a%20pre-trained%0Alatent%20diffusion%20model%20with%20additional%20appearance%20priors%20derived%20from%20the%20CLIP%0Aand%20VAE%20encodings%20of%20the%20reference%20garment.%20Meanwhile%2C%20a%20novel%20garment-focused%0Aadapter%20is%20integrated%20into%20the%20UNet%20of%20diffusion%20model%2C%20pursuing%20local%0Afine-grained%20alignment%20with%20the%20visual%20appearance%20of%20reference%20garment%20and%0Ahuman%20pose.%20We%20specifically%20design%20an%20appearance%20loss%20over%20the%20synthesized%0Agarment%20to%20enhance%20the%20crucial%2C%20high-frequency%20details.%20Extensive%20experiments%0Aon%20VITON-HD%20and%20DressCode%20datasets%20demonstrate%20the%20superiority%20of%20our%20GarDiff%0Awhen%20compared%20to%20state-of-the-art%20VTON%20approaches.%20Code%20is%20publicly%20available%0Aat%3A%0A%5Chref%7Bhttps%3A//github.com/siqi0905/GarDiff/tree/master%7D%7Bhttps%3A//github.com/siqi0905/GarDiff/tree/master%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08258v1&entry.124074799=Read"},
{"title": "The JPEG Pleno Learning-based Point Cloud Coding Standard: Serving Man\n  and Machine", "author": "Andr\u00e9 F. R. Guarda and Nuno M. M. Rodrigues and Fernando Pereira", "abstract": "  Efficient point cloud coding has become increasingly critical for multiple\napplications such as virtual reality, autonomous driving, and digital twin\nsystems, where rich and interactive 3D data representations may functionally\nmake the difference. Deep learning has emerged as a powerful tool in this\ndomain, offering advanced techniques for compressing point clouds more\nefficiently than conventional coding methods while also allowing effective\ncomputer vision tasks performed in the compressed domain thus, for the first\ntime, making available a common compressed visual representation effective for\nboth man and machine. Taking advantage of this potential, JPEG has recently\nfinalized the JPEG Pleno Learning-based Point Cloud Coding (PCC) standard\noffering efficient lossy coding of static point clouds, targeting both human\nvisualization and machine processing by leveraging deep learning models for\ngeometry and color coding. The geometry is processed directly in its original\n3D form using sparse convolutional neural networks, while the color data is\nprojected onto 2D images and encoded using the also learning-based JPEG AI\nstandard. The goal of this paper is to provide a complete technical description\nof the JPEG PCC standard, along with a thorough benchmarking of its performance\nagainst the state-of-the-art, while highlighting its main strengths and\nweaknesses. In terms of compression performance, JPEG PCC outperforms the\nconventional MPEG PCC standards, especially in geometry coding, achieving\nsignificant rate reductions. Color compression performance is less competitive\nbut this is overcome by the power of a full learning-based coding framework for\nboth geometry and color and the associated effective compressed domain\nprocessing.\n", "link": "http://arxiv.org/abs/2409.08130v1", "date": "2024-09-12", "relevancy": 2.6615, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5588}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5245}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20JPEG%20Pleno%20Learning-based%20Point%20Cloud%20Coding%20Standard%3A%20Serving%20Man%0A%20%20and%20Machine&body=Title%3A%20The%20JPEG%20Pleno%20Learning-based%20Point%20Cloud%20Coding%20Standard%3A%20Serving%20Man%0A%20%20and%20Machine%0AAuthor%3A%20Andr%C3%A9%20F.%20R.%20Guarda%20and%20Nuno%20M.%20M.%20Rodrigues%20and%20Fernando%20Pereira%0AAbstract%3A%20%20%20Efficient%20point%20cloud%20coding%20has%20become%20increasingly%20critical%20for%20multiple%0Aapplications%20such%20as%20virtual%20reality%2C%20autonomous%20driving%2C%20and%20digital%20twin%0Asystems%2C%20where%20rich%20and%20interactive%203D%20data%20representations%20may%20functionally%0Amake%20the%20difference.%20Deep%20learning%20has%20emerged%20as%20a%20powerful%20tool%20in%20this%0Adomain%2C%20offering%20advanced%20techniques%20for%20compressing%20point%20clouds%20more%0Aefficiently%20than%20conventional%20coding%20methods%20while%20also%20allowing%20effective%0Acomputer%20vision%20tasks%20performed%20in%20the%20compressed%20domain%20thus%2C%20for%20the%20first%0Atime%2C%20making%20available%20a%20common%20compressed%20visual%20representation%20effective%20for%0Aboth%20man%20and%20machine.%20Taking%20advantage%20of%20this%20potential%2C%20JPEG%20has%20recently%0Afinalized%20the%20JPEG%20Pleno%20Learning-based%20Point%20Cloud%20Coding%20%28PCC%29%20standard%0Aoffering%20efficient%20lossy%20coding%20of%20static%20point%20clouds%2C%20targeting%20both%20human%0Avisualization%20and%20machine%20processing%20by%20leveraging%20deep%20learning%20models%20for%0Ageometry%20and%20color%20coding.%20The%20geometry%20is%20processed%20directly%20in%20its%20original%0A3D%20form%20using%20sparse%20convolutional%20neural%20networks%2C%20while%20the%20color%20data%20is%0Aprojected%20onto%202D%20images%20and%20encoded%20using%20the%20also%20learning-based%20JPEG%20AI%0Astandard.%20The%20goal%20of%20this%20paper%20is%20to%20provide%20a%20complete%20technical%20description%0Aof%20the%20JPEG%20PCC%20standard%2C%20along%20with%20a%20thorough%20benchmarking%20of%20its%20performance%0Aagainst%20the%20state-of-the-art%2C%20while%20highlighting%20its%20main%20strengths%20and%0Aweaknesses.%20In%20terms%20of%20compression%20performance%2C%20JPEG%20PCC%20outperforms%20the%0Aconventional%20MPEG%20PCC%20standards%2C%20especially%20in%20geometry%20coding%2C%20achieving%0Asignificant%20rate%20reductions.%20Color%20compression%20performance%20is%20less%20competitive%0Abut%20this%20is%20overcome%20by%20the%20power%20of%20a%20full%20learning-based%20coding%20framework%20for%0Aboth%20geometry%20and%20color%20and%20the%20associated%20effective%20compressed%20domain%0Aprocessing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08130v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520JPEG%2520Pleno%2520Learning-based%2520Point%2520Cloud%2520Coding%2520Standard%253A%2520Serving%2520Man%250A%2520%2520and%2520Machine%26entry.906535625%3DAndr%25C3%25A9%2520F.%2520R.%2520Guarda%2520and%2520Nuno%2520M.%2520M.%2520Rodrigues%2520and%2520Fernando%2520Pereira%26entry.1292438233%3D%2520%2520Efficient%2520point%2520cloud%2520coding%2520has%2520become%2520increasingly%2520critical%2520for%2520multiple%250Aapplications%2520such%2520as%2520virtual%2520reality%252C%2520autonomous%2520driving%252C%2520and%2520digital%2520twin%250Asystems%252C%2520where%2520rich%2520and%2520interactive%25203D%2520data%2520representations%2520may%2520functionally%250Amake%2520the%2520difference.%2520Deep%2520learning%2520has%2520emerged%2520as%2520a%2520powerful%2520tool%2520in%2520this%250Adomain%252C%2520offering%2520advanced%2520techniques%2520for%2520compressing%2520point%2520clouds%2520more%250Aefficiently%2520than%2520conventional%2520coding%2520methods%2520while%2520also%2520allowing%2520effective%250Acomputer%2520vision%2520tasks%2520performed%2520in%2520the%2520compressed%2520domain%2520thus%252C%2520for%2520the%2520first%250Atime%252C%2520making%2520available%2520a%2520common%2520compressed%2520visual%2520representation%2520effective%2520for%250Aboth%2520man%2520and%2520machine.%2520Taking%2520advantage%2520of%2520this%2520potential%252C%2520JPEG%2520has%2520recently%250Afinalized%2520the%2520JPEG%2520Pleno%2520Learning-based%2520Point%2520Cloud%2520Coding%2520%2528PCC%2529%2520standard%250Aoffering%2520efficient%2520lossy%2520coding%2520of%2520static%2520point%2520clouds%252C%2520targeting%2520both%2520human%250Avisualization%2520and%2520machine%2520processing%2520by%2520leveraging%2520deep%2520learning%2520models%2520for%250Ageometry%2520and%2520color%2520coding.%2520The%2520geometry%2520is%2520processed%2520directly%2520in%2520its%2520original%250A3D%2520form%2520using%2520sparse%2520convolutional%2520neural%2520networks%252C%2520while%2520the%2520color%2520data%2520is%250Aprojected%2520onto%25202D%2520images%2520and%2520encoded%2520using%2520the%2520also%2520learning-based%2520JPEG%2520AI%250Astandard.%2520The%2520goal%2520of%2520this%2520paper%2520is%2520to%2520provide%2520a%2520complete%2520technical%2520description%250Aof%2520the%2520JPEG%2520PCC%2520standard%252C%2520along%2520with%2520a%2520thorough%2520benchmarking%2520of%2520its%2520performance%250Aagainst%2520the%2520state-of-the-art%252C%2520while%2520highlighting%2520its%2520main%2520strengths%2520and%250Aweaknesses.%2520In%2520terms%2520of%2520compression%2520performance%252C%2520JPEG%2520PCC%2520outperforms%2520the%250Aconventional%2520MPEG%2520PCC%2520standards%252C%2520especially%2520in%2520geometry%2520coding%252C%2520achieving%250Asignificant%2520rate%2520reductions.%2520Color%2520compression%2520performance%2520is%2520less%2520competitive%250Abut%2520this%2520is%2520overcome%2520by%2520the%2520power%2520of%2520a%2520full%2520learning-based%2520coding%2520framework%2520for%250Aboth%2520geometry%2520and%2520color%2520and%2520the%2520associated%2520effective%2520compressed%2520domain%250Aprocessing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08130v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20JPEG%20Pleno%20Learning-based%20Point%20Cloud%20Coding%20Standard%3A%20Serving%20Man%0A%20%20and%20Machine&entry.906535625=Andr%C3%A9%20F.%20R.%20Guarda%20and%20Nuno%20M.%20M.%20Rodrigues%20and%20Fernando%20Pereira&entry.1292438233=%20%20Efficient%20point%20cloud%20coding%20has%20become%20increasingly%20critical%20for%20multiple%0Aapplications%20such%20as%20virtual%20reality%2C%20autonomous%20driving%2C%20and%20digital%20twin%0Asystems%2C%20where%20rich%20and%20interactive%203D%20data%20representations%20may%20functionally%0Amake%20the%20difference.%20Deep%20learning%20has%20emerged%20as%20a%20powerful%20tool%20in%20this%0Adomain%2C%20offering%20advanced%20techniques%20for%20compressing%20point%20clouds%20more%0Aefficiently%20than%20conventional%20coding%20methods%20while%20also%20allowing%20effective%0Acomputer%20vision%20tasks%20performed%20in%20the%20compressed%20domain%20thus%2C%20for%20the%20first%0Atime%2C%20making%20available%20a%20common%20compressed%20visual%20representation%20effective%20for%0Aboth%20man%20and%20machine.%20Taking%20advantage%20of%20this%20potential%2C%20JPEG%20has%20recently%0Afinalized%20the%20JPEG%20Pleno%20Learning-based%20Point%20Cloud%20Coding%20%28PCC%29%20standard%0Aoffering%20efficient%20lossy%20coding%20of%20static%20point%20clouds%2C%20targeting%20both%20human%0Avisualization%20and%20machine%20processing%20by%20leveraging%20deep%20learning%20models%20for%0Ageometry%20and%20color%20coding.%20The%20geometry%20is%20processed%20directly%20in%20its%20original%0A3D%20form%20using%20sparse%20convolutional%20neural%20networks%2C%20while%20the%20color%20data%20is%0Aprojected%20onto%202D%20images%20and%20encoded%20using%20the%20also%20learning-based%20JPEG%20AI%0Astandard.%20The%20goal%20of%20this%20paper%20is%20to%20provide%20a%20complete%20technical%20description%0Aof%20the%20JPEG%20PCC%20standard%2C%20along%20with%20a%20thorough%20benchmarking%20of%20its%20performance%0Aagainst%20the%20state-of-the-art%2C%20while%20highlighting%20its%20main%20strengths%20and%0Aweaknesses.%20In%20terms%20of%20compression%20performance%2C%20JPEG%20PCC%20outperforms%20the%0Aconventional%20MPEG%20PCC%20standards%2C%20especially%20in%20geometry%20coding%2C%20achieving%0Asignificant%20rate%20reductions.%20Color%20compression%20performance%20is%20less%20competitive%0Abut%20this%20is%20overcome%20by%20the%20power%20of%20a%20full%20learning-based%20coding%20framework%20for%0Aboth%20geometry%20and%20color%20and%20the%20associated%20effective%20compressed%20domain%0Aprocessing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08130v1&entry.124074799=Read"},
{"title": "Learning to Match 2D Keypoints Across Preoperative MR and Intraoperative\n  Ultrasound", "author": "Hassan Rasheed and Reuben Dorent and Maximilian Fehrentz and Tina Kapur and William M. Wells III and Alexandra Golby and Sarah Frisken and Julia A. Schnabel and Nazim Haouchine", "abstract": "  We propose in this paper a texture-invariant 2D keypoints descriptor\nspecifically designed for matching preoperative Magnetic Resonance (MR) images\nwith intraoperative Ultrasound (US) images. We introduce a\nmatching-by-synthesis strategy, where intraoperative US images are synthesized\nfrom MR images accounting for multiple MR modalities and intraoperative US\nvariability. We build our training set by enforcing keypoints localization over\nall images then train a patient-specific descriptor network that learns\ntexture-invariant discriminant features in a supervised contrastive manner,\nleading to robust keypoints descriptors. Our experiments on real cases with\nground truth show the effectiveness of the proposed approach, outperforming the\nstate-of-the-art methods and achieving 80.35% matching precision on average.\n", "link": "http://arxiv.org/abs/2409.08169v1", "date": "2024-09-12", "relevancy": 2.6034, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5916}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4921}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Match%202D%20Keypoints%20Across%20Preoperative%20MR%20and%20Intraoperative%0A%20%20Ultrasound&body=Title%3A%20Learning%20to%20Match%202D%20Keypoints%20Across%20Preoperative%20MR%20and%20Intraoperative%0A%20%20Ultrasound%0AAuthor%3A%20Hassan%20Rasheed%20and%20Reuben%20Dorent%20and%20Maximilian%20Fehrentz%20and%20Tina%20Kapur%20and%20William%20M.%20Wells%20III%20and%20Alexandra%20Golby%20and%20Sarah%20Frisken%20and%20Julia%20A.%20Schnabel%20and%20Nazim%20Haouchine%0AAbstract%3A%20%20%20We%20propose%20in%20this%20paper%20a%20texture-invariant%202D%20keypoints%20descriptor%0Aspecifically%20designed%20for%20matching%20preoperative%20Magnetic%20Resonance%20%28MR%29%20images%0Awith%20intraoperative%20Ultrasound%20%28US%29%20images.%20We%20introduce%20a%0Amatching-by-synthesis%20strategy%2C%20where%20intraoperative%20US%20images%20are%20synthesized%0Afrom%20MR%20images%20accounting%20for%20multiple%20MR%20modalities%20and%20intraoperative%20US%0Avariability.%20We%20build%20our%20training%20set%20by%20enforcing%20keypoints%20localization%20over%0Aall%20images%20then%20train%20a%20patient-specific%20descriptor%20network%20that%20learns%0Atexture-invariant%20discriminant%20features%20in%20a%20supervised%20contrastive%20manner%2C%0Aleading%20to%20robust%20keypoints%20descriptors.%20Our%20experiments%20on%20real%20cases%20with%0Aground%20truth%20show%20the%20effectiveness%20of%20the%20proposed%20approach%2C%20outperforming%20the%0Astate-of-the-art%20methods%20and%20achieving%2080.35%25%20matching%20precision%20on%20average.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08169v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Match%25202D%2520Keypoints%2520Across%2520Preoperative%2520MR%2520and%2520Intraoperative%250A%2520%2520Ultrasound%26entry.906535625%3DHassan%2520Rasheed%2520and%2520Reuben%2520Dorent%2520and%2520Maximilian%2520Fehrentz%2520and%2520Tina%2520Kapur%2520and%2520William%2520M.%2520Wells%2520III%2520and%2520Alexandra%2520Golby%2520and%2520Sarah%2520Frisken%2520and%2520Julia%2520A.%2520Schnabel%2520and%2520Nazim%2520Haouchine%26entry.1292438233%3D%2520%2520We%2520propose%2520in%2520this%2520paper%2520a%2520texture-invariant%25202D%2520keypoints%2520descriptor%250Aspecifically%2520designed%2520for%2520matching%2520preoperative%2520Magnetic%2520Resonance%2520%2528MR%2529%2520images%250Awith%2520intraoperative%2520Ultrasound%2520%2528US%2529%2520images.%2520We%2520introduce%2520a%250Amatching-by-synthesis%2520strategy%252C%2520where%2520intraoperative%2520US%2520images%2520are%2520synthesized%250Afrom%2520MR%2520images%2520accounting%2520for%2520multiple%2520MR%2520modalities%2520and%2520intraoperative%2520US%250Avariability.%2520We%2520build%2520our%2520training%2520set%2520by%2520enforcing%2520keypoints%2520localization%2520over%250Aall%2520images%2520then%2520train%2520a%2520patient-specific%2520descriptor%2520network%2520that%2520learns%250Atexture-invariant%2520discriminant%2520features%2520in%2520a%2520supervised%2520contrastive%2520manner%252C%250Aleading%2520to%2520robust%2520keypoints%2520descriptors.%2520Our%2520experiments%2520on%2520real%2520cases%2520with%250Aground%2520truth%2520show%2520the%2520effectiveness%2520of%2520the%2520proposed%2520approach%252C%2520outperforming%2520the%250Astate-of-the-art%2520methods%2520and%2520achieving%252080.35%2525%2520matching%2520precision%2520on%2520average.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08169v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Match%202D%20Keypoints%20Across%20Preoperative%20MR%20and%20Intraoperative%0A%20%20Ultrasound&entry.906535625=Hassan%20Rasheed%20and%20Reuben%20Dorent%20and%20Maximilian%20Fehrentz%20and%20Tina%20Kapur%20and%20William%20M.%20Wells%20III%20and%20Alexandra%20Golby%20and%20Sarah%20Frisken%20and%20Julia%20A.%20Schnabel%20and%20Nazim%20Haouchine&entry.1292438233=%20%20We%20propose%20in%20this%20paper%20a%20texture-invariant%202D%20keypoints%20descriptor%0Aspecifically%20designed%20for%20matching%20preoperative%20Magnetic%20Resonance%20%28MR%29%20images%0Awith%20intraoperative%20Ultrasound%20%28US%29%20images.%20We%20introduce%20a%0Amatching-by-synthesis%20strategy%2C%20where%20intraoperative%20US%20images%20are%20synthesized%0Afrom%20MR%20images%20accounting%20for%20multiple%20MR%20modalities%20and%20intraoperative%20US%0Avariability.%20We%20build%20our%20training%20set%20by%20enforcing%20keypoints%20localization%20over%0Aall%20images%20then%20train%20a%20patient-specific%20descriptor%20network%20that%20learns%0Atexture-invariant%20discriminant%20features%20in%20a%20supervised%20contrastive%20manner%2C%0Aleading%20to%20robust%20keypoints%20descriptors.%20Our%20experiments%20on%20real%20cases%20with%0Aground%20truth%20show%20the%20effectiveness%20of%20the%20proposed%20approach%2C%20outperforming%20the%0Astate-of-the-art%20methods%20and%20achieving%2080.35%25%20matching%20precision%20on%20average.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08169v1&entry.124074799=Read"},
{"title": "LT3SD: Latent Trees for 3D Scene Diffusion", "author": "Quan Meng and Lei Li and Matthias Nie\u00dfner and Angela Dai", "abstract": "  We present LT3SD, a novel latent diffusion model for large-scale 3D scene\ngeneration. Recent advances in diffusion models have shown impressive results\nin 3D object generation, but are limited in spatial extent and quality when\nextended to 3D scenes. To generate complex and diverse 3D scene structures, we\nintroduce a latent tree representation to effectively encode both\nlower-frequency geometry and higher-frequency detail in a coarse-to-fine\nhierarchy. We can then learn a generative diffusion process in this latent 3D\nscene space, modeling the latent components of a scene at each resolution\nlevel. To synthesize large-scale scenes with varying sizes, we train our\ndiffusion model on scene patches and synthesize arbitrary-sized output 3D\nscenes through shared diffusion generation across multiple scene patches.\nThrough extensive experiments, we demonstrate the efficacy and benefits of\nLT3SD for large-scale, high-quality unconditional 3D scene generation and for\nprobabilistic completion for partial scene observations.\n", "link": "http://arxiv.org/abs/2409.08215v1", "date": "2024-09-12", "relevancy": 2.598, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6573}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.648}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LT3SD%3A%20Latent%20Trees%20for%203D%20Scene%20Diffusion&body=Title%3A%20LT3SD%3A%20Latent%20Trees%20for%203D%20Scene%20Diffusion%0AAuthor%3A%20Quan%20Meng%20and%20Lei%20Li%20and%20Matthias%20Nie%C3%9Fner%20and%20Angela%20Dai%0AAbstract%3A%20%20%20We%20present%20LT3SD%2C%20a%20novel%20latent%20diffusion%20model%20for%20large-scale%203D%20scene%0Ageneration.%20Recent%20advances%20in%20diffusion%20models%20have%20shown%20impressive%20results%0Ain%203D%20object%20generation%2C%20but%20are%20limited%20in%20spatial%20extent%20and%20quality%20when%0Aextended%20to%203D%20scenes.%20To%20generate%20complex%20and%20diverse%203D%20scene%20structures%2C%20we%0Aintroduce%20a%20latent%20tree%20representation%20to%20effectively%20encode%20both%0Alower-frequency%20geometry%20and%20higher-frequency%20detail%20in%20a%20coarse-to-fine%0Ahierarchy.%20We%20can%20then%20learn%20a%20generative%20diffusion%20process%20in%20this%20latent%203D%0Ascene%20space%2C%20modeling%20the%20latent%20components%20of%20a%20scene%20at%20each%20resolution%0Alevel.%20To%20synthesize%20large-scale%20scenes%20with%20varying%20sizes%2C%20we%20train%20our%0Adiffusion%20model%20on%20scene%20patches%20and%20synthesize%20arbitrary-sized%20output%203D%0Ascenes%20through%20shared%20diffusion%20generation%20across%20multiple%20scene%20patches.%0AThrough%20extensive%20experiments%2C%20we%20demonstrate%20the%20efficacy%20and%20benefits%20of%0ALT3SD%20for%20large-scale%2C%20high-quality%20unconditional%203D%20scene%20generation%20and%20for%0Aprobabilistic%20completion%20for%20partial%20scene%20observations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08215v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLT3SD%253A%2520Latent%2520Trees%2520for%25203D%2520Scene%2520Diffusion%26entry.906535625%3DQuan%2520Meng%2520and%2520Lei%2520Li%2520and%2520Matthias%2520Nie%25C3%259Fner%2520and%2520Angela%2520Dai%26entry.1292438233%3D%2520%2520We%2520present%2520LT3SD%252C%2520a%2520novel%2520latent%2520diffusion%2520model%2520for%2520large-scale%25203D%2520scene%250Ageneration.%2520Recent%2520advances%2520in%2520diffusion%2520models%2520have%2520shown%2520impressive%2520results%250Ain%25203D%2520object%2520generation%252C%2520but%2520are%2520limited%2520in%2520spatial%2520extent%2520and%2520quality%2520when%250Aextended%2520to%25203D%2520scenes.%2520To%2520generate%2520complex%2520and%2520diverse%25203D%2520scene%2520structures%252C%2520we%250Aintroduce%2520a%2520latent%2520tree%2520representation%2520to%2520effectively%2520encode%2520both%250Alower-frequency%2520geometry%2520and%2520higher-frequency%2520detail%2520in%2520a%2520coarse-to-fine%250Ahierarchy.%2520We%2520can%2520then%2520learn%2520a%2520generative%2520diffusion%2520process%2520in%2520this%2520latent%25203D%250Ascene%2520space%252C%2520modeling%2520the%2520latent%2520components%2520of%2520a%2520scene%2520at%2520each%2520resolution%250Alevel.%2520To%2520synthesize%2520large-scale%2520scenes%2520with%2520varying%2520sizes%252C%2520we%2520train%2520our%250Adiffusion%2520model%2520on%2520scene%2520patches%2520and%2520synthesize%2520arbitrary-sized%2520output%25203D%250Ascenes%2520through%2520shared%2520diffusion%2520generation%2520across%2520multiple%2520scene%2520patches.%250AThrough%2520extensive%2520experiments%252C%2520we%2520demonstrate%2520the%2520efficacy%2520and%2520benefits%2520of%250ALT3SD%2520for%2520large-scale%252C%2520high-quality%2520unconditional%25203D%2520scene%2520generation%2520and%2520for%250Aprobabilistic%2520completion%2520for%2520partial%2520scene%2520observations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08215v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LT3SD%3A%20Latent%20Trees%20for%203D%20Scene%20Diffusion&entry.906535625=Quan%20Meng%20and%20Lei%20Li%20and%20Matthias%20Nie%C3%9Fner%20and%20Angela%20Dai&entry.1292438233=%20%20We%20present%20LT3SD%2C%20a%20novel%20latent%20diffusion%20model%20for%20large-scale%203D%20scene%0Ageneration.%20Recent%20advances%20in%20diffusion%20models%20have%20shown%20impressive%20results%0Ain%203D%20object%20generation%2C%20but%20are%20limited%20in%20spatial%20extent%20and%20quality%20when%0Aextended%20to%203D%20scenes.%20To%20generate%20complex%20and%20diverse%203D%20scene%20structures%2C%20we%0Aintroduce%20a%20latent%20tree%20representation%20to%20effectively%20encode%20both%0Alower-frequency%20geometry%20and%20higher-frequency%20detail%20in%20a%20coarse-to-fine%0Ahierarchy.%20We%20can%20then%20learn%20a%20generative%20diffusion%20process%20in%20this%20latent%203D%0Ascene%20space%2C%20modeling%20the%20latent%20components%20of%20a%20scene%20at%20each%20resolution%0Alevel.%20To%20synthesize%20large-scale%20scenes%20with%20varying%20sizes%2C%20we%20train%20our%0Adiffusion%20model%20on%20scene%20patches%20and%20synthesize%20arbitrary-sized%20output%203D%0Ascenes%20through%20shared%20diffusion%20generation%20across%20multiple%20scene%20patches.%0AThrough%20extensive%20experiments%2C%20we%20demonstrate%20the%20efficacy%20and%20benefits%20of%0ALT3SD%20for%20large-scale%2C%20high-quality%20unconditional%203D%20scene%20generation%20and%20for%0Aprobabilistic%20completion%20for%20partial%20scene%20observations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08215v1&entry.124074799=Read"},
{"title": "Enhancing Canine Musculoskeletal Diagnoses: Leveraging Synthetic Image\n  Data for Pre-Training AI-Models on Visual Documentations", "author": "Martin Thi\u00dfen and Thi Ngoc Diep Tran and Ben Joel Sch\u00f6nbein and Ute Trapp and Barbara Esteve Ratsch and Beate Egner and Romana Piat and Elke Hergenr\u00f6ther", "abstract": "  The examination of the musculoskeletal system in dogs is a challenging task\nin veterinary practice. In this work, a novel method has been developed that\nenables efficient documentation of a dog's condition through a visual\nrepresentation. However, since the visual documentation is new, there is no\nexisting training data. The objective of this work is therefore to mitigate the\nimpact of data scarcity in order to develop an AI-based diagnostic support\nsystem. To this end, the potential of synthetic data that mimics realistic\nvisual documentations of diseases for pre-training AI models is investigated.\nWe propose a method for generating synthetic image data that mimics realistic\nvisual documentations. Initially, a basic dataset containing three distinct\nclasses is generated, followed by the creation of a more sophisticated dataset\ncontaining 36 different classes. Both datasets are used for the pre-training of\nan AI model. Subsequently, an evaluation dataset is created, consisting of 250\nmanually created visual documentations for five different diseases. This\ndataset, along with a subset containing 25 examples. The obtained results on\nthe evaluation dataset containing 25 examples demonstrate a significant\nenhancement of approximately 10% in diagnosis accuracy when utilizing generated\nsynthetic images that mimic real-world visual documentations. However, these\nresults do not hold true for the larger evaluation dataset containing 250\nexamples, indicating that the advantages of using synthetic data for\npre-training an AI model emerge primarily when dealing with few examples of\nvisual documentations for a given disease. Overall, this work provides valuable\ninsights into mitigating the limitations imposed by limited training data\nthrough the strategic use of generated synthetic data, presenting an approach\napplicable beyond the canine musculoskeletal assessment domain.\n", "link": "http://arxiv.org/abs/2409.08181v1", "date": "2024-09-12", "relevancy": 2.5945, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5251}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5158}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5158}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Canine%20Musculoskeletal%20Diagnoses%3A%20Leveraging%20Synthetic%20Image%0A%20%20Data%20for%20Pre-Training%20AI-Models%20on%20Visual%20Documentations&body=Title%3A%20Enhancing%20Canine%20Musculoskeletal%20Diagnoses%3A%20Leveraging%20Synthetic%20Image%0A%20%20Data%20for%20Pre-Training%20AI-Models%20on%20Visual%20Documentations%0AAuthor%3A%20Martin%20Thi%C3%9Fen%20and%20Thi%20Ngoc%20Diep%20Tran%20and%20Ben%20Joel%20Sch%C3%B6nbein%20and%20Ute%20Trapp%20and%20Barbara%20Esteve%20Ratsch%20and%20Beate%20Egner%20and%20Romana%20Piat%20and%20Elke%20Hergenr%C3%B6ther%0AAbstract%3A%20%20%20The%20examination%20of%20the%20musculoskeletal%20system%20in%20dogs%20is%20a%20challenging%20task%0Ain%20veterinary%20practice.%20In%20this%20work%2C%20a%20novel%20method%20has%20been%20developed%20that%0Aenables%20efficient%20documentation%20of%20a%20dog%27s%20condition%20through%20a%20visual%0Arepresentation.%20However%2C%20since%20the%20visual%20documentation%20is%20new%2C%20there%20is%20no%0Aexisting%20training%20data.%20The%20objective%20of%20this%20work%20is%20therefore%20to%20mitigate%20the%0Aimpact%20of%20data%20scarcity%20in%20order%20to%20develop%20an%20AI-based%20diagnostic%20support%0Asystem.%20To%20this%20end%2C%20the%20potential%20of%20synthetic%20data%20that%20mimics%20realistic%0Avisual%20documentations%20of%20diseases%20for%20pre-training%20AI%20models%20is%20investigated.%0AWe%20propose%20a%20method%20for%20generating%20synthetic%20image%20data%20that%20mimics%20realistic%0Avisual%20documentations.%20Initially%2C%20a%20basic%20dataset%20containing%20three%20distinct%0Aclasses%20is%20generated%2C%20followed%20by%20the%20creation%20of%20a%20more%20sophisticated%20dataset%0Acontaining%2036%20different%20classes.%20Both%20datasets%20are%20used%20for%20the%20pre-training%20of%0Aan%20AI%20model.%20Subsequently%2C%20an%20evaluation%20dataset%20is%20created%2C%20consisting%20of%20250%0Amanually%20created%20visual%20documentations%20for%20five%20different%20diseases.%20This%0Adataset%2C%20along%20with%20a%20subset%20containing%2025%20examples.%20The%20obtained%20results%20on%0Athe%20evaluation%20dataset%20containing%2025%20examples%20demonstrate%20a%20significant%0Aenhancement%20of%20approximately%2010%25%20in%20diagnosis%20accuracy%20when%20utilizing%20generated%0Asynthetic%20images%20that%20mimic%20real-world%20visual%20documentations.%20However%2C%20these%0Aresults%20do%20not%20hold%20true%20for%20the%20larger%20evaluation%20dataset%20containing%20250%0Aexamples%2C%20indicating%20that%20the%20advantages%20of%20using%20synthetic%20data%20for%0Apre-training%20an%20AI%20model%20emerge%20primarily%20when%20dealing%20with%20few%20examples%20of%0Avisual%20documentations%20for%20a%20given%20disease.%20Overall%2C%20this%20work%20provides%20valuable%0Ainsights%20into%20mitigating%20the%20limitations%20imposed%20by%20limited%20training%20data%0Athrough%20the%20strategic%20use%20of%20generated%20synthetic%20data%2C%20presenting%20an%20approach%0Aapplicable%20beyond%20the%20canine%20musculoskeletal%20assessment%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08181v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Canine%2520Musculoskeletal%2520Diagnoses%253A%2520Leveraging%2520Synthetic%2520Image%250A%2520%2520Data%2520for%2520Pre-Training%2520AI-Models%2520on%2520Visual%2520Documentations%26entry.906535625%3DMartin%2520Thi%25C3%259Fen%2520and%2520Thi%2520Ngoc%2520Diep%2520Tran%2520and%2520Ben%2520Joel%2520Sch%25C3%25B6nbein%2520and%2520Ute%2520Trapp%2520and%2520Barbara%2520Esteve%2520Ratsch%2520and%2520Beate%2520Egner%2520and%2520Romana%2520Piat%2520and%2520Elke%2520Hergenr%25C3%25B6ther%26entry.1292438233%3D%2520%2520The%2520examination%2520of%2520the%2520musculoskeletal%2520system%2520in%2520dogs%2520is%2520a%2520challenging%2520task%250Ain%2520veterinary%2520practice.%2520In%2520this%2520work%252C%2520a%2520novel%2520method%2520has%2520been%2520developed%2520that%250Aenables%2520efficient%2520documentation%2520of%2520a%2520dog%2527s%2520condition%2520through%2520a%2520visual%250Arepresentation.%2520However%252C%2520since%2520the%2520visual%2520documentation%2520is%2520new%252C%2520there%2520is%2520no%250Aexisting%2520training%2520data.%2520The%2520objective%2520of%2520this%2520work%2520is%2520therefore%2520to%2520mitigate%2520the%250Aimpact%2520of%2520data%2520scarcity%2520in%2520order%2520to%2520develop%2520an%2520AI-based%2520diagnostic%2520support%250Asystem.%2520To%2520this%2520end%252C%2520the%2520potential%2520of%2520synthetic%2520data%2520that%2520mimics%2520realistic%250Avisual%2520documentations%2520of%2520diseases%2520for%2520pre-training%2520AI%2520models%2520is%2520investigated.%250AWe%2520propose%2520a%2520method%2520for%2520generating%2520synthetic%2520image%2520data%2520that%2520mimics%2520realistic%250Avisual%2520documentations.%2520Initially%252C%2520a%2520basic%2520dataset%2520containing%2520three%2520distinct%250Aclasses%2520is%2520generated%252C%2520followed%2520by%2520the%2520creation%2520of%2520a%2520more%2520sophisticated%2520dataset%250Acontaining%252036%2520different%2520classes.%2520Both%2520datasets%2520are%2520used%2520for%2520the%2520pre-training%2520of%250Aan%2520AI%2520model.%2520Subsequently%252C%2520an%2520evaluation%2520dataset%2520is%2520created%252C%2520consisting%2520of%2520250%250Amanually%2520created%2520visual%2520documentations%2520for%2520five%2520different%2520diseases.%2520This%250Adataset%252C%2520along%2520with%2520a%2520subset%2520containing%252025%2520examples.%2520The%2520obtained%2520results%2520on%250Athe%2520evaluation%2520dataset%2520containing%252025%2520examples%2520demonstrate%2520a%2520significant%250Aenhancement%2520of%2520approximately%252010%2525%2520in%2520diagnosis%2520accuracy%2520when%2520utilizing%2520generated%250Asynthetic%2520images%2520that%2520mimic%2520real-world%2520visual%2520documentations.%2520However%252C%2520these%250Aresults%2520do%2520not%2520hold%2520true%2520for%2520the%2520larger%2520evaluation%2520dataset%2520containing%2520250%250Aexamples%252C%2520indicating%2520that%2520the%2520advantages%2520of%2520using%2520synthetic%2520data%2520for%250Apre-training%2520an%2520AI%2520model%2520emerge%2520primarily%2520when%2520dealing%2520with%2520few%2520examples%2520of%250Avisual%2520documentations%2520for%2520a%2520given%2520disease.%2520Overall%252C%2520this%2520work%2520provides%2520valuable%250Ainsights%2520into%2520mitigating%2520the%2520limitations%2520imposed%2520by%2520limited%2520training%2520data%250Athrough%2520the%2520strategic%2520use%2520of%2520generated%2520synthetic%2520data%252C%2520presenting%2520an%2520approach%250Aapplicable%2520beyond%2520the%2520canine%2520musculoskeletal%2520assessment%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08181v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Canine%20Musculoskeletal%20Diagnoses%3A%20Leveraging%20Synthetic%20Image%0A%20%20Data%20for%20Pre-Training%20AI-Models%20on%20Visual%20Documentations&entry.906535625=Martin%20Thi%C3%9Fen%20and%20Thi%20Ngoc%20Diep%20Tran%20and%20Ben%20Joel%20Sch%C3%B6nbein%20and%20Ute%20Trapp%20and%20Barbara%20Esteve%20Ratsch%20and%20Beate%20Egner%20and%20Romana%20Piat%20and%20Elke%20Hergenr%C3%B6ther&entry.1292438233=%20%20The%20examination%20of%20the%20musculoskeletal%20system%20in%20dogs%20is%20a%20challenging%20task%0Ain%20veterinary%20practice.%20In%20this%20work%2C%20a%20novel%20method%20has%20been%20developed%20that%0Aenables%20efficient%20documentation%20of%20a%20dog%27s%20condition%20through%20a%20visual%0Arepresentation.%20However%2C%20since%20the%20visual%20documentation%20is%20new%2C%20there%20is%20no%0Aexisting%20training%20data.%20The%20objective%20of%20this%20work%20is%20therefore%20to%20mitigate%20the%0Aimpact%20of%20data%20scarcity%20in%20order%20to%20develop%20an%20AI-based%20diagnostic%20support%0Asystem.%20To%20this%20end%2C%20the%20potential%20of%20synthetic%20data%20that%20mimics%20realistic%0Avisual%20documentations%20of%20diseases%20for%20pre-training%20AI%20models%20is%20investigated.%0AWe%20propose%20a%20method%20for%20generating%20synthetic%20image%20data%20that%20mimics%20realistic%0Avisual%20documentations.%20Initially%2C%20a%20basic%20dataset%20containing%20three%20distinct%0Aclasses%20is%20generated%2C%20followed%20by%20the%20creation%20of%20a%20more%20sophisticated%20dataset%0Acontaining%2036%20different%20classes.%20Both%20datasets%20are%20used%20for%20the%20pre-training%20of%0Aan%20AI%20model.%20Subsequently%2C%20an%20evaluation%20dataset%20is%20created%2C%20consisting%20of%20250%0Amanually%20created%20visual%20documentations%20for%20five%20different%20diseases.%20This%0Adataset%2C%20along%20with%20a%20subset%20containing%2025%20examples.%20The%20obtained%20results%20on%0Athe%20evaluation%20dataset%20containing%2025%20examples%20demonstrate%20a%20significant%0Aenhancement%20of%20approximately%2010%25%20in%20diagnosis%20accuracy%20when%20utilizing%20generated%0Asynthetic%20images%20that%20mimic%20real-world%20visual%20documentations.%20However%2C%20these%0Aresults%20do%20not%20hold%20true%20for%20the%20larger%20evaluation%20dataset%20containing%20250%0Aexamples%2C%20indicating%20that%20the%20advantages%20of%20using%20synthetic%20data%20for%0Apre-training%20an%20AI%20model%20emerge%20primarily%20when%20dealing%20with%20few%20examples%20of%0Avisual%20documentations%20for%20a%20given%20disease.%20Overall%2C%20this%20work%20provides%20valuable%0Ainsights%20into%20mitigating%20the%20limitations%20imposed%20by%20limited%20training%20data%0Athrough%20the%20strategic%20use%20of%20generated%20synthetic%20data%2C%20presenting%20an%20approach%0Aapplicable%20beyond%20the%20canine%20musculoskeletal%20assessment%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08181v1&entry.124074799=Read"},
{"title": "Source2Synth: Synthetic Data Generation and Curation Grounded in Real\n  Data Sources", "author": "Alisia Lupidi and Carlos Gemmell and Nicola Cancedda and Jane Dwivedi-Yu and Jason Weston and Jakob Foerster and Roberta Raileanu and Maria Lomeli", "abstract": "  Large Language Models still struggle in challenging scenarios that leverage\nstructured data, complex reasoning, or tool usage. In this paper, we propose\nSource2Synth: a new method that can be used for teaching LLMs new skills\nwithout relying on costly human annotations. Source2Synth takes as input a\ncustom data source and produces synthetic data points with intermediate\nreasoning steps grounded in real-world sources. Source2Synth improves the\ndataset quality by discarding low-quality generations based on their\nanswerability. We demonstrate the generality of this approach by applying it to\ntwo challenging domains: we test reasoning abilities in multi-hop question\nanswering (MHQA), and tool usage in tabular question answering (TQA). Our\nmethod improves performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA on\nHotPotQA compared to the fine-tuned baselines.\n", "link": "http://arxiv.org/abs/2409.08239v1", "date": "2024-09-12", "relevancy": 2.5929, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5231}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5231}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5096}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Source2Synth%3A%20Synthetic%20Data%20Generation%20and%20Curation%20Grounded%20in%20Real%0A%20%20Data%20Sources&body=Title%3A%20Source2Synth%3A%20Synthetic%20Data%20Generation%20and%20Curation%20Grounded%20in%20Real%0A%20%20Data%20Sources%0AAuthor%3A%20Alisia%20Lupidi%20and%20Carlos%20Gemmell%20and%20Nicola%20Cancedda%20and%20Jane%20Dwivedi-Yu%20and%20Jason%20Weston%20and%20Jakob%20Foerster%20and%20Roberta%20Raileanu%20and%20Maria%20Lomeli%0AAbstract%3A%20%20%20Large%20Language%20Models%20still%20struggle%20in%20challenging%20scenarios%20that%20leverage%0Astructured%20data%2C%20complex%20reasoning%2C%20or%20tool%20usage.%20In%20this%20paper%2C%20we%20propose%0ASource2Synth%3A%20a%20new%20method%20that%20can%20be%20used%20for%20teaching%20LLMs%20new%20skills%0Awithout%20relying%20on%20costly%20human%20annotations.%20Source2Synth%20takes%20as%20input%20a%0Acustom%20data%20source%20and%20produces%20synthetic%20data%20points%20with%20intermediate%0Areasoning%20steps%20grounded%20in%20real-world%20sources.%20Source2Synth%20improves%20the%0Adataset%20quality%20by%20discarding%20low-quality%20generations%20based%20on%20their%0Aanswerability.%20We%20demonstrate%20the%20generality%20of%20this%20approach%20by%20applying%20it%20to%0Atwo%20challenging%20domains%3A%20we%20test%20reasoning%20abilities%20in%20multi-hop%20question%0Aanswering%20%28MHQA%29%2C%20and%20tool%20usage%20in%20tabular%20question%20answering%20%28TQA%29.%20Our%0Amethod%20improves%20performance%20by%2025.51%25%20for%20TQA%20on%20WikiSQL%20and%2022.57%25%20for%20MHQA%20on%0AHotPotQA%20compared%20to%20the%20fine-tuned%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08239v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSource2Synth%253A%2520Synthetic%2520Data%2520Generation%2520and%2520Curation%2520Grounded%2520in%2520Real%250A%2520%2520Data%2520Sources%26entry.906535625%3DAlisia%2520Lupidi%2520and%2520Carlos%2520Gemmell%2520and%2520Nicola%2520Cancedda%2520and%2520Jane%2520Dwivedi-Yu%2520and%2520Jason%2520Weston%2520and%2520Jakob%2520Foerster%2520and%2520Roberta%2520Raileanu%2520and%2520Maria%2520Lomeli%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520still%2520struggle%2520in%2520challenging%2520scenarios%2520that%2520leverage%250Astructured%2520data%252C%2520complex%2520reasoning%252C%2520or%2520tool%2520usage.%2520In%2520this%2520paper%252C%2520we%2520propose%250ASource2Synth%253A%2520a%2520new%2520method%2520that%2520can%2520be%2520used%2520for%2520teaching%2520LLMs%2520new%2520skills%250Awithout%2520relying%2520on%2520costly%2520human%2520annotations.%2520Source2Synth%2520takes%2520as%2520input%2520a%250Acustom%2520data%2520source%2520and%2520produces%2520synthetic%2520data%2520points%2520with%2520intermediate%250Areasoning%2520steps%2520grounded%2520in%2520real-world%2520sources.%2520Source2Synth%2520improves%2520the%250Adataset%2520quality%2520by%2520discarding%2520low-quality%2520generations%2520based%2520on%2520their%250Aanswerability.%2520We%2520demonstrate%2520the%2520generality%2520of%2520this%2520approach%2520by%2520applying%2520it%2520to%250Atwo%2520challenging%2520domains%253A%2520we%2520test%2520reasoning%2520abilities%2520in%2520multi-hop%2520question%250Aanswering%2520%2528MHQA%2529%252C%2520and%2520tool%2520usage%2520in%2520tabular%2520question%2520answering%2520%2528TQA%2529.%2520Our%250Amethod%2520improves%2520performance%2520by%252025.51%2525%2520for%2520TQA%2520on%2520WikiSQL%2520and%252022.57%2525%2520for%2520MHQA%2520on%250AHotPotQA%2520compared%2520to%2520the%2520fine-tuned%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08239v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Source2Synth%3A%20Synthetic%20Data%20Generation%20and%20Curation%20Grounded%20in%20Real%0A%20%20Data%20Sources&entry.906535625=Alisia%20Lupidi%20and%20Carlos%20Gemmell%20and%20Nicola%20Cancedda%20and%20Jane%20Dwivedi-Yu%20and%20Jason%20Weston%20and%20Jakob%20Foerster%20and%20Roberta%20Raileanu%20and%20Maria%20Lomeli&entry.1292438233=%20%20Large%20Language%20Models%20still%20struggle%20in%20challenging%20scenarios%20that%20leverage%0Astructured%20data%2C%20complex%20reasoning%2C%20or%20tool%20usage.%20In%20this%20paper%2C%20we%20propose%0ASource2Synth%3A%20a%20new%20method%20that%20can%20be%20used%20for%20teaching%20LLMs%20new%20skills%0Awithout%20relying%20on%20costly%20human%20annotations.%20Source2Synth%20takes%20as%20input%20a%0Acustom%20data%20source%20and%20produces%20synthetic%20data%20points%20with%20intermediate%0Areasoning%20steps%20grounded%20in%20real-world%20sources.%20Source2Synth%20improves%20the%0Adataset%20quality%20by%20discarding%20low-quality%20generations%20based%20on%20their%0Aanswerability.%20We%20demonstrate%20the%20generality%20of%20this%20approach%20by%20applying%20it%20to%0Atwo%20challenging%20domains%3A%20we%20test%20reasoning%20abilities%20in%20multi-hop%20question%0Aanswering%20%28MHQA%29%2C%20and%20tool%20usage%20in%20tabular%20question%20answering%20%28TQA%29.%20Our%0Amethod%20improves%20performance%20by%2025.51%25%20for%20TQA%20on%20WikiSQL%20and%2022.57%25%20for%20MHQA%20on%0AHotPotQA%20compared%20to%20the%20fine-tuned%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08239v1&entry.124074799=Read"},
{"title": "The NPU-ASLP System Description for Visual Speech Recognition in CNVSRC\n  2024", "author": "He Wang and Lei Xie", "abstract": "  This paper delineates the visual speech recognition (VSR) system introduced\nby the NPU-ASLP (Team 237) in the second Chinese Continuous Visual Speech\nRecognition Challenge (CNVSRC 2024), engaging in all four tracks, including the\nfixed and open tracks of Single-Speaker VSR Task and Multi-Speaker VSR Task. In\nterms of data processing, we leverage the lip motion extractor from the\nbaseline1 to produce multiscale video data. Besides, various augmentation\ntechniques are applied during training, encompassing speed perturbation, random\nrotation, horizontal flipping, and color transformation. The VSR model adopts\nan end-to-end architecture with joint CTC/attention loss, introducing Enhanced\nResNet3D visual frontend, E-Branchformer encoder, and Bi-directional\nTransformer decoder. Our approach yields a 30.47% CER for the Single-Speaker\nTask and 34.30% CER for the Multi-Speaker Task, securing second place in the\nopen track of the Single-Speaker Task and first place in the other three\ntracks.\n", "link": "http://arxiv.org/abs/2408.02369v3", "date": "2024-09-12", "relevancy": 2.5686, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5276}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5276}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20NPU-ASLP%20System%20Description%20for%20Visual%20Speech%20Recognition%20in%20CNVSRC%0A%20%202024&body=Title%3A%20The%20NPU-ASLP%20System%20Description%20for%20Visual%20Speech%20Recognition%20in%20CNVSRC%0A%20%202024%0AAuthor%3A%20He%20Wang%20and%20Lei%20Xie%0AAbstract%3A%20%20%20This%20paper%20delineates%20the%20visual%20speech%20recognition%20%28VSR%29%20system%20introduced%0Aby%20the%20NPU-ASLP%20%28Team%20237%29%20in%20the%20second%20Chinese%20Continuous%20Visual%20Speech%0ARecognition%20Challenge%20%28CNVSRC%202024%29%2C%20engaging%20in%20all%20four%20tracks%2C%20including%20the%0Afixed%20and%20open%20tracks%20of%20Single-Speaker%20VSR%20Task%20and%20Multi-Speaker%20VSR%20Task.%20In%0Aterms%20of%20data%20processing%2C%20we%20leverage%20the%20lip%20motion%20extractor%20from%20the%0Abaseline1%20to%20produce%20multiscale%20video%20data.%20Besides%2C%20various%20augmentation%0Atechniques%20are%20applied%20during%20training%2C%20encompassing%20speed%20perturbation%2C%20random%0Arotation%2C%20horizontal%20flipping%2C%20and%20color%20transformation.%20The%20VSR%20model%20adopts%0Aan%20end-to-end%20architecture%20with%20joint%20CTC/attention%20loss%2C%20introducing%20Enhanced%0AResNet3D%20visual%20frontend%2C%20E-Branchformer%20encoder%2C%20and%20Bi-directional%0ATransformer%20decoder.%20Our%20approach%20yields%20a%2030.47%25%20CER%20for%20the%20Single-Speaker%0ATask%20and%2034.30%25%20CER%20for%20the%20Multi-Speaker%20Task%2C%20securing%20second%20place%20in%20the%0Aopen%20track%20of%20the%20Single-Speaker%20Task%20and%20first%20place%20in%20the%20other%20three%0Atracks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02369v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520NPU-ASLP%2520System%2520Description%2520for%2520Visual%2520Speech%2520Recognition%2520in%2520CNVSRC%250A%2520%25202024%26entry.906535625%3DHe%2520Wang%2520and%2520Lei%2520Xie%26entry.1292438233%3D%2520%2520This%2520paper%2520delineates%2520the%2520visual%2520speech%2520recognition%2520%2528VSR%2529%2520system%2520introduced%250Aby%2520the%2520NPU-ASLP%2520%2528Team%2520237%2529%2520in%2520the%2520second%2520Chinese%2520Continuous%2520Visual%2520Speech%250ARecognition%2520Challenge%2520%2528CNVSRC%25202024%2529%252C%2520engaging%2520in%2520all%2520four%2520tracks%252C%2520including%2520the%250Afixed%2520and%2520open%2520tracks%2520of%2520Single-Speaker%2520VSR%2520Task%2520and%2520Multi-Speaker%2520VSR%2520Task.%2520In%250Aterms%2520of%2520data%2520processing%252C%2520we%2520leverage%2520the%2520lip%2520motion%2520extractor%2520from%2520the%250Abaseline1%2520to%2520produce%2520multiscale%2520video%2520data.%2520Besides%252C%2520various%2520augmentation%250Atechniques%2520are%2520applied%2520during%2520training%252C%2520encompassing%2520speed%2520perturbation%252C%2520random%250Arotation%252C%2520horizontal%2520flipping%252C%2520and%2520color%2520transformation.%2520The%2520VSR%2520model%2520adopts%250Aan%2520end-to-end%2520architecture%2520with%2520joint%2520CTC/attention%2520loss%252C%2520introducing%2520Enhanced%250AResNet3D%2520visual%2520frontend%252C%2520E-Branchformer%2520encoder%252C%2520and%2520Bi-directional%250ATransformer%2520decoder.%2520Our%2520approach%2520yields%2520a%252030.47%2525%2520CER%2520for%2520the%2520Single-Speaker%250ATask%2520and%252034.30%2525%2520CER%2520for%2520the%2520Multi-Speaker%2520Task%252C%2520securing%2520second%2520place%2520in%2520the%250Aopen%2520track%2520of%2520the%2520Single-Speaker%2520Task%2520and%2520first%2520place%2520in%2520the%2520other%2520three%250Atracks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02369v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20NPU-ASLP%20System%20Description%20for%20Visual%20Speech%20Recognition%20in%20CNVSRC%0A%20%202024&entry.906535625=He%20Wang%20and%20Lei%20Xie&entry.1292438233=%20%20This%20paper%20delineates%20the%20visual%20speech%20recognition%20%28VSR%29%20system%20introduced%0Aby%20the%20NPU-ASLP%20%28Team%20237%29%20in%20the%20second%20Chinese%20Continuous%20Visual%20Speech%0ARecognition%20Challenge%20%28CNVSRC%202024%29%2C%20engaging%20in%20all%20four%20tracks%2C%20including%20the%0Afixed%20and%20open%20tracks%20of%20Single-Speaker%20VSR%20Task%20and%20Multi-Speaker%20VSR%20Task.%20In%0Aterms%20of%20data%20processing%2C%20we%20leverage%20the%20lip%20motion%20extractor%20from%20the%0Abaseline1%20to%20produce%20multiscale%20video%20data.%20Besides%2C%20various%20augmentation%0Atechniques%20are%20applied%20during%20training%2C%20encompassing%20speed%20perturbation%2C%20random%0Arotation%2C%20horizontal%20flipping%2C%20and%20color%20transformation.%20The%20VSR%20model%20adopts%0Aan%20end-to-end%20architecture%20with%20joint%20CTC/attention%20loss%2C%20introducing%20Enhanced%0AResNet3D%20visual%20frontend%2C%20E-Branchformer%20encoder%2C%20and%20Bi-directional%0ATransformer%20decoder.%20Our%20approach%20yields%20a%2030.47%25%20CER%20for%20the%20Single-Speaker%0ATask%20and%2034.30%25%20CER%20for%20the%20Multi-Speaker%20Task%2C%20securing%20second%20place%20in%20the%0Aopen%20track%20of%20the%20Single-Speaker%20Task%20and%20first%20place%20in%20the%20other%20three%0Atracks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02369v3&entry.124074799=Read"},
{"title": "Iterative CT Reconstruction via Latent Variable Optimization of Shallow\n  Diffusion Models", "author": "Sho Ozaki and Shizuo Kaji and Toshikazu Imae and Kanabu Nawa and Hideomi Yamashita and Keiichi Nakagawa", "abstract": "  Image-generative artificial intelligence (AI) has garnered significant\nattention in recent years. In particular, the diffusion model, a core component\nof generative AI, produces high-quality images with rich diversity. In this\nstudy, we proposed a novel computed tomography (CT) reconstruction method by\ncombining the denoising diffusion probabilistic model with iterative CT\nreconstruction. In sharp contrast to previous studies, we optimized the\nfidelity loss of CT reconstruction with respect to the latent variable of the\ndiffusion model, instead of the image and model parameters. To suppress the\nchanges in anatomical structures produced by the diffusion model, we shallowed\nthe diffusion and reverse processes and fixed a set of added noises in the\nreverse process to make it deterministic during the inference. We demonstrated\nthe effectiveness of the proposed method through the sparse-projection CT\nreconstruction of 1/10 projection data. Despite the simplicity of the\nimplementation, the proposed method has the potential to reconstruct\nhigh-quality images while preserving the patient's anatomical structures and\nwas found to outperform existing methods, including iterative reconstruction,\niterative reconstruction with total variation, and the diffusion model alone in\nterms of quantitative indices such as the structural similarity index and peak\nsignal-to-noise ratio. We also explored further sparse-projection CT\nreconstruction using 1/20 projection data with the same trained diffusion\nmodel. As the number of iterations increased, the image quality improved\ncomparable to that of 1/10 sparse-projection CT reconstruction. In principle,\nthis method can be widely applied not only to CT but also to other imaging\nmodalities.\n", "link": "http://arxiv.org/abs/2408.03156v2", "date": "2024-09-12", "relevancy": 2.5621, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6529}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6381}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6381}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Iterative%20CT%20Reconstruction%20via%20Latent%20Variable%20Optimization%20of%20Shallow%0A%20%20Diffusion%20Models&body=Title%3A%20Iterative%20CT%20Reconstruction%20via%20Latent%20Variable%20Optimization%20of%20Shallow%0A%20%20Diffusion%20Models%0AAuthor%3A%20Sho%20Ozaki%20and%20Shizuo%20Kaji%20and%20Toshikazu%20Imae%20and%20Kanabu%20Nawa%20and%20Hideomi%20Yamashita%20and%20Keiichi%20Nakagawa%0AAbstract%3A%20%20%20Image-generative%20artificial%20intelligence%20%28AI%29%20has%20garnered%20significant%0Aattention%20in%20recent%20years.%20In%20particular%2C%20the%20diffusion%20model%2C%20a%20core%20component%0Aof%20generative%20AI%2C%20produces%20high-quality%20images%20with%20rich%20diversity.%20In%20this%0Astudy%2C%20we%20proposed%20a%20novel%20computed%20tomography%20%28CT%29%20reconstruction%20method%20by%0Acombining%20the%20denoising%20diffusion%20probabilistic%20model%20with%20iterative%20CT%0Areconstruction.%20In%20sharp%20contrast%20to%20previous%20studies%2C%20we%20optimized%20the%0Afidelity%20loss%20of%20CT%20reconstruction%20with%20respect%20to%20the%20latent%20variable%20of%20the%0Adiffusion%20model%2C%20instead%20of%20the%20image%20and%20model%20parameters.%20To%20suppress%20the%0Achanges%20in%20anatomical%20structures%20produced%20by%20the%20diffusion%20model%2C%20we%20shallowed%0Athe%20diffusion%20and%20reverse%20processes%20and%20fixed%20a%20set%20of%20added%20noises%20in%20the%0Areverse%20process%20to%20make%20it%20deterministic%20during%20the%20inference.%20We%20demonstrated%0Athe%20effectiveness%20of%20the%20proposed%20method%20through%20the%20sparse-projection%20CT%0Areconstruction%20of%201/10%20projection%20data.%20Despite%20the%20simplicity%20of%20the%0Aimplementation%2C%20the%20proposed%20method%20has%20the%20potential%20to%20reconstruct%0Ahigh-quality%20images%20while%20preserving%20the%20patient%27s%20anatomical%20structures%20and%0Awas%20found%20to%20outperform%20existing%20methods%2C%20including%20iterative%20reconstruction%2C%0Aiterative%20reconstruction%20with%20total%20variation%2C%20and%20the%20diffusion%20model%20alone%20in%0Aterms%20of%20quantitative%20indices%20such%20as%20the%20structural%20similarity%20index%20and%20peak%0Asignal-to-noise%20ratio.%20We%20also%20explored%20further%20sparse-projection%20CT%0Areconstruction%20using%201/20%20projection%20data%20with%20the%20same%20trained%20diffusion%0Amodel.%20As%20the%20number%20of%20iterations%20increased%2C%20the%20image%20quality%20improved%0Acomparable%20to%20that%20of%201/10%20sparse-projection%20CT%20reconstruction.%20In%20principle%2C%0Athis%20method%20can%20be%20widely%20applied%20not%20only%20to%20CT%20but%20also%20to%20other%20imaging%0Amodalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03156v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIterative%2520CT%2520Reconstruction%2520via%2520Latent%2520Variable%2520Optimization%2520of%2520Shallow%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DSho%2520Ozaki%2520and%2520Shizuo%2520Kaji%2520and%2520Toshikazu%2520Imae%2520and%2520Kanabu%2520Nawa%2520and%2520Hideomi%2520Yamashita%2520and%2520Keiichi%2520Nakagawa%26entry.1292438233%3D%2520%2520Image-generative%2520artificial%2520intelligence%2520%2528AI%2529%2520has%2520garnered%2520significant%250Aattention%2520in%2520recent%2520years.%2520In%2520particular%252C%2520the%2520diffusion%2520model%252C%2520a%2520core%2520component%250Aof%2520generative%2520AI%252C%2520produces%2520high-quality%2520images%2520with%2520rich%2520diversity.%2520In%2520this%250Astudy%252C%2520we%2520proposed%2520a%2520novel%2520computed%2520tomography%2520%2528CT%2529%2520reconstruction%2520method%2520by%250Acombining%2520the%2520denoising%2520diffusion%2520probabilistic%2520model%2520with%2520iterative%2520CT%250Areconstruction.%2520In%2520sharp%2520contrast%2520to%2520previous%2520studies%252C%2520we%2520optimized%2520the%250Afidelity%2520loss%2520of%2520CT%2520reconstruction%2520with%2520respect%2520to%2520the%2520latent%2520variable%2520of%2520the%250Adiffusion%2520model%252C%2520instead%2520of%2520the%2520image%2520and%2520model%2520parameters.%2520To%2520suppress%2520the%250Achanges%2520in%2520anatomical%2520structures%2520produced%2520by%2520the%2520diffusion%2520model%252C%2520we%2520shallowed%250Athe%2520diffusion%2520and%2520reverse%2520processes%2520and%2520fixed%2520a%2520set%2520of%2520added%2520noises%2520in%2520the%250Areverse%2520process%2520to%2520make%2520it%2520deterministic%2520during%2520the%2520inference.%2520We%2520demonstrated%250Athe%2520effectiveness%2520of%2520the%2520proposed%2520method%2520through%2520the%2520sparse-projection%2520CT%250Areconstruction%2520of%25201/10%2520projection%2520data.%2520Despite%2520the%2520simplicity%2520of%2520the%250Aimplementation%252C%2520the%2520proposed%2520method%2520has%2520the%2520potential%2520to%2520reconstruct%250Ahigh-quality%2520images%2520while%2520preserving%2520the%2520patient%2527s%2520anatomical%2520structures%2520and%250Awas%2520found%2520to%2520outperform%2520existing%2520methods%252C%2520including%2520iterative%2520reconstruction%252C%250Aiterative%2520reconstruction%2520with%2520total%2520variation%252C%2520and%2520the%2520diffusion%2520model%2520alone%2520in%250Aterms%2520of%2520quantitative%2520indices%2520such%2520as%2520the%2520structural%2520similarity%2520index%2520and%2520peak%250Asignal-to-noise%2520ratio.%2520We%2520also%2520explored%2520further%2520sparse-projection%2520CT%250Areconstruction%2520using%25201/20%2520projection%2520data%2520with%2520the%2520same%2520trained%2520diffusion%250Amodel.%2520As%2520the%2520number%2520of%2520iterations%2520increased%252C%2520the%2520image%2520quality%2520improved%250Acomparable%2520to%2520that%2520of%25201/10%2520sparse-projection%2520CT%2520reconstruction.%2520In%2520principle%252C%250Athis%2520method%2520can%2520be%2520widely%2520applied%2520not%2520only%2520to%2520CT%2520but%2520also%2520to%2520other%2520imaging%250Amodalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03156v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Iterative%20CT%20Reconstruction%20via%20Latent%20Variable%20Optimization%20of%20Shallow%0A%20%20Diffusion%20Models&entry.906535625=Sho%20Ozaki%20and%20Shizuo%20Kaji%20and%20Toshikazu%20Imae%20and%20Kanabu%20Nawa%20and%20Hideomi%20Yamashita%20and%20Keiichi%20Nakagawa&entry.1292438233=%20%20Image-generative%20artificial%20intelligence%20%28AI%29%20has%20garnered%20significant%0Aattention%20in%20recent%20years.%20In%20particular%2C%20the%20diffusion%20model%2C%20a%20core%20component%0Aof%20generative%20AI%2C%20produces%20high-quality%20images%20with%20rich%20diversity.%20In%20this%0Astudy%2C%20we%20proposed%20a%20novel%20computed%20tomography%20%28CT%29%20reconstruction%20method%20by%0Acombining%20the%20denoising%20diffusion%20probabilistic%20model%20with%20iterative%20CT%0Areconstruction.%20In%20sharp%20contrast%20to%20previous%20studies%2C%20we%20optimized%20the%0Afidelity%20loss%20of%20CT%20reconstruction%20with%20respect%20to%20the%20latent%20variable%20of%20the%0Adiffusion%20model%2C%20instead%20of%20the%20image%20and%20model%20parameters.%20To%20suppress%20the%0Achanges%20in%20anatomical%20structures%20produced%20by%20the%20diffusion%20model%2C%20we%20shallowed%0Athe%20diffusion%20and%20reverse%20processes%20and%20fixed%20a%20set%20of%20added%20noises%20in%20the%0Areverse%20process%20to%20make%20it%20deterministic%20during%20the%20inference.%20We%20demonstrated%0Athe%20effectiveness%20of%20the%20proposed%20method%20through%20the%20sparse-projection%20CT%0Areconstruction%20of%201/10%20projection%20data.%20Despite%20the%20simplicity%20of%20the%0Aimplementation%2C%20the%20proposed%20method%20has%20the%20potential%20to%20reconstruct%0Ahigh-quality%20images%20while%20preserving%20the%20patient%27s%20anatomical%20structures%20and%0Awas%20found%20to%20outperform%20existing%20methods%2C%20including%20iterative%20reconstruction%2C%0Aiterative%20reconstruction%20with%20total%20variation%2C%20and%20the%20diffusion%20model%20alone%20in%0Aterms%20of%20quantitative%20indices%20such%20as%20the%20structural%20similarity%20index%20and%20peak%0Asignal-to-noise%20ratio.%20We%20also%20explored%20further%20sparse-projection%20CT%0Areconstruction%20using%201/20%20projection%20data%20with%20the%20same%20trained%20diffusion%0Amodel.%20As%20the%20number%20of%20iterations%20increased%2C%20the%20image%20quality%20improved%0Acomparable%20to%20that%20of%201/10%20sparse-projection%20CT%20reconstruction.%20In%20principle%2C%0Athis%20method%20can%20be%20widely%20applied%20not%20only%20to%20CT%20but%20also%20to%20other%20imaging%0Amodalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03156v2&entry.124074799=Read"},
{"title": "GS-ROR: 3D Gaussian Splatting for Reflective Object Relighting via SDF\n  Priors", "author": "Zuo-Liang Zhu and Beibei Wang and Jian Yang", "abstract": "  3D Gaussian Splatting (3DGS) has shown a powerful capability for novel view\nsynthesis due to its detailed expressive ability and highly efficient rendering\nspeed. Unfortunately, creating relightable 3D assets with 3DGS is still\nproblematic, particularly for reflective objects, as its discontinuous\nrepresentation raises difficulties in constraining geometries. Inspired by\nprevious works, the signed distance field (SDF) can serve as an effective way\nfor geometry regularization. However, a direct incorporation between Gaussians\nand SDF significantly slows training. To this end, we propose GS-ROR for\nreflective objects relighting with 3DGS aided by SDF priors. At the core of our\nmethod is the mutual supervision of the depth and normal between deferred\nGaussians and SDF, which avoids the expensive volume rendering of SDF. Thanks\nto this mutual supervision, the learned deferred Gaussians are well-constrained\nwith a minimal time cost. As the Gaussians are rendered in a deferred shading\nmode, while the alpha-blended Gaussians are smooth, individual Gaussians may\nstill be outliers, yielding floater artifacts. Therefore, we further introduce\nan SDF-aware pruning strategy to remove Gaussian outliers, which are located\ndistant from the surface defined by SDF, avoiding the floater issue.\nConsequently, our method outperforms the existing Gaussian-based inverse\nrendering methods in terms of relighting quality. Our method also exhibits\ncompetitive relighting quality compared to NeRF-based methods with at most 25%\nof training time and allows rendering at 200+ frames per second on an RTX4090.\n", "link": "http://arxiv.org/abs/2406.18544v2", "date": "2024-09-12", "relevancy": 2.5229, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6872}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6046}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GS-ROR%3A%203D%20Gaussian%20Splatting%20for%20Reflective%20Object%20Relighting%20via%20SDF%0A%20%20Priors&body=Title%3A%20GS-ROR%3A%203D%20Gaussian%20Splatting%20for%20Reflective%20Object%20Relighting%20via%20SDF%0A%20%20Priors%0AAuthor%3A%20Zuo-Liang%20Zhu%20and%20Beibei%20Wang%20and%20Jian%20Yang%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20shown%20a%20powerful%20capability%20for%20novel%20view%0Asynthesis%20due%20to%20its%20detailed%20expressive%20ability%20and%20highly%20efficient%20rendering%0Aspeed.%20Unfortunately%2C%20creating%20relightable%203D%20assets%20with%203DGS%20is%20still%0Aproblematic%2C%20particularly%20for%20reflective%20objects%2C%20as%20its%20discontinuous%0Arepresentation%20raises%20difficulties%20in%20constraining%20geometries.%20Inspired%20by%0Aprevious%20works%2C%20the%20signed%20distance%20field%20%28SDF%29%20can%20serve%20as%20an%20effective%20way%0Afor%20geometry%20regularization.%20However%2C%20a%20direct%20incorporation%20between%20Gaussians%0Aand%20SDF%20significantly%20slows%20training.%20To%20this%20end%2C%20we%20propose%20GS-ROR%20for%0Areflective%20objects%20relighting%20with%203DGS%20aided%20by%20SDF%20priors.%20At%20the%20core%20of%20our%0Amethod%20is%20the%20mutual%20supervision%20of%20the%20depth%20and%20normal%20between%20deferred%0AGaussians%20and%20SDF%2C%20which%20avoids%20the%20expensive%20volume%20rendering%20of%20SDF.%20Thanks%0Ato%20this%20mutual%20supervision%2C%20the%20learned%20deferred%20Gaussians%20are%20well-constrained%0Awith%20a%20minimal%20time%20cost.%20As%20the%20Gaussians%20are%20rendered%20in%20a%20deferred%20shading%0Amode%2C%20while%20the%20alpha-blended%20Gaussians%20are%20smooth%2C%20individual%20Gaussians%20may%0Astill%20be%20outliers%2C%20yielding%20floater%20artifacts.%20Therefore%2C%20we%20further%20introduce%0Aan%20SDF-aware%20pruning%20strategy%20to%20remove%20Gaussian%20outliers%2C%20which%20are%20located%0Adistant%20from%20the%20surface%20defined%20by%20SDF%2C%20avoiding%20the%20floater%20issue.%0AConsequently%2C%20our%20method%20outperforms%20the%20existing%20Gaussian-based%20inverse%0Arendering%20methods%20in%20terms%20of%20relighting%20quality.%20Our%20method%20also%20exhibits%0Acompetitive%20relighting%20quality%20compared%20to%20NeRF-based%20methods%20with%20at%20most%2025%25%0Aof%20training%20time%20and%20allows%20rendering%20at%20200%2B%20frames%20per%20second%20on%20an%20RTX4090.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18544v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGS-ROR%253A%25203D%2520Gaussian%2520Splatting%2520for%2520Reflective%2520Object%2520Relighting%2520via%2520SDF%250A%2520%2520Priors%26entry.906535625%3DZuo-Liang%2520Zhu%2520and%2520Beibei%2520Wang%2520and%2520Jian%2520Yang%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520shown%2520a%2520powerful%2520capability%2520for%2520novel%2520view%250Asynthesis%2520due%2520to%2520its%2520detailed%2520expressive%2520ability%2520and%2520highly%2520efficient%2520rendering%250Aspeed.%2520Unfortunately%252C%2520creating%2520relightable%25203D%2520assets%2520with%25203DGS%2520is%2520still%250Aproblematic%252C%2520particularly%2520for%2520reflective%2520objects%252C%2520as%2520its%2520discontinuous%250Arepresentation%2520raises%2520difficulties%2520in%2520constraining%2520geometries.%2520Inspired%2520by%250Aprevious%2520works%252C%2520the%2520signed%2520distance%2520field%2520%2528SDF%2529%2520can%2520serve%2520as%2520an%2520effective%2520way%250Afor%2520geometry%2520regularization.%2520However%252C%2520a%2520direct%2520incorporation%2520between%2520Gaussians%250Aand%2520SDF%2520significantly%2520slows%2520training.%2520To%2520this%2520end%252C%2520we%2520propose%2520GS-ROR%2520for%250Areflective%2520objects%2520relighting%2520with%25203DGS%2520aided%2520by%2520SDF%2520priors.%2520At%2520the%2520core%2520of%2520our%250Amethod%2520is%2520the%2520mutual%2520supervision%2520of%2520the%2520depth%2520and%2520normal%2520between%2520deferred%250AGaussians%2520and%2520SDF%252C%2520which%2520avoids%2520the%2520expensive%2520volume%2520rendering%2520of%2520SDF.%2520Thanks%250Ato%2520this%2520mutual%2520supervision%252C%2520the%2520learned%2520deferred%2520Gaussians%2520are%2520well-constrained%250Awith%2520a%2520minimal%2520time%2520cost.%2520As%2520the%2520Gaussians%2520are%2520rendered%2520in%2520a%2520deferred%2520shading%250Amode%252C%2520while%2520the%2520alpha-blended%2520Gaussians%2520are%2520smooth%252C%2520individual%2520Gaussians%2520may%250Astill%2520be%2520outliers%252C%2520yielding%2520floater%2520artifacts.%2520Therefore%252C%2520we%2520further%2520introduce%250Aan%2520SDF-aware%2520pruning%2520strategy%2520to%2520remove%2520Gaussian%2520outliers%252C%2520which%2520are%2520located%250Adistant%2520from%2520the%2520surface%2520defined%2520by%2520SDF%252C%2520avoiding%2520the%2520floater%2520issue.%250AConsequently%252C%2520our%2520method%2520outperforms%2520the%2520existing%2520Gaussian-based%2520inverse%250Arendering%2520methods%2520in%2520terms%2520of%2520relighting%2520quality.%2520Our%2520method%2520also%2520exhibits%250Acompetitive%2520relighting%2520quality%2520compared%2520to%2520NeRF-based%2520methods%2520with%2520at%2520most%252025%2525%250Aof%2520training%2520time%2520and%2520allows%2520rendering%2520at%2520200%252B%2520frames%2520per%2520second%2520on%2520an%2520RTX4090.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18544v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GS-ROR%3A%203D%20Gaussian%20Splatting%20for%20Reflective%20Object%20Relighting%20via%20SDF%0A%20%20Priors&entry.906535625=Zuo-Liang%20Zhu%20and%20Beibei%20Wang%20and%20Jian%20Yang&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20shown%20a%20powerful%20capability%20for%20novel%20view%0Asynthesis%20due%20to%20its%20detailed%20expressive%20ability%20and%20highly%20efficient%20rendering%0Aspeed.%20Unfortunately%2C%20creating%20relightable%203D%20assets%20with%203DGS%20is%20still%0Aproblematic%2C%20particularly%20for%20reflective%20objects%2C%20as%20its%20discontinuous%0Arepresentation%20raises%20difficulties%20in%20constraining%20geometries.%20Inspired%20by%0Aprevious%20works%2C%20the%20signed%20distance%20field%20%28SDF%29%20can%20serve%20as%20an%20effective%20way%0Afor%20geometry%20regularization.%20However%2C%20a%20direct%20incorporation%20between%20Gaussians%0Aand%20SDF%20significantly%20slows%20training.%20To%20this%20end%2C%20we%20propose%20GS-ROR%20for%0Areflective%20objects%20relighting%20with%203DGS%20aided%20by%20SDF%20priors.%20At%20the%20core%20of%20our%0Amethod%20is%20the%20mutual%20supervision%20of%20the%20depth%20and%20normal%20between%20deferred%0AGaussians%20and%20SDF%2C%20which%20avoids%20the%20expensive%20volume%20rendering%20of%20SDF.%20Thanks%0Ato%20this%20mutual%20supervision%2C%20the%20learned%20deferred%20Gaussians%20are%20well-constrained%0Awith%20a%20minimal%20time%20cost.%20As%20the%20Gaussians%20are%20rendered%20in%20a%20deferred%20shading%0Amode%2C%20while%20the%20alpha-blended%20Gaussians%20are%20smooth%2C%20individual%20Gaussians%20may%0Astill%20be%20outliers%2C%20yielding%20floater%20artifacts.%20Therefore%2C%20we%20further%20introduce%0Aan%20SDF-aware%20pruning%20strategy%20to%20remove%20Gaussian%20outliers%2C%20which%20are%20located%0Adistant%20from%20the%20surface%20defined%20by%20SDF%2C%20avoiding%20the%20floater%20issue.%0AConsequently%2C%20our%20method%20outperforms%20the%20existing%20Gaussian-based%20inverse%0Arendering%20methods%20in%20terms%20of%20relighting%20quality.%20Our%20method%20also%20exhibits%0Acompetitive%20relighting%20quality%20compared%20to%20NeRF-based%20methods%20with%20at%20most%2025%25%0Aof%20training%20time%20and%20allows%20rendering%20at%20200%2B%20frames%20per%20second%20on%20an%20RTX4090.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18544v2&entry.124074799=Read"},
{"title": "Multiplex Graph Contrastive Learning with Soft Negatives", "author": "Zhenhao Zhao and Minhong Zhu and Chen Wang and Sijia Wang and Jiqiang Zhang and Li Chen and Weiran Cai", "abstract": "  Graph Contrastive Learning (GCL) seeks to learn nodal or graph\nrepresentations that contain maximal consistent information from\ngraph-structured data. While node-level contrasting modes are dominating, some\nefforts commence to explore consistency across different scales. Yet, they tend\nto lose consistent information and be contaminated by disturbing features.\nHere, we introduce MUX-GCL, a novel cross-scale contrastive learning paradigm\nthat utilizes multiplex representations as effective patches. While this\nlearning mode minimizes contaminating noises, a commensurate contrasting\nstrategy using positional affinities further avoids information loss by\ncorrecting false negative pairs across scales. Extensive downstream experiments\ndemonstrate that MUX-GCL yields multiple state-of-the-art results on public\ndatasets. Our theoretical analysis further guarantees the new objective\nfunction as a stricter lower bound of mutual information of raw input features\nand output embeddings, which rationalizes this paradigm. Code is available at\nhttps://github.com/MUX-GCL/Code.\n", "link": "http://arxiv.org/abs/2409.08010v1", "date": "2024-09-12", "relevancy": 2.5176, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5124}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5007}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4975}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multiplex%20Graph%20Contrastive%20Learning%20with%20Soft%20Negatives&body=Title%3A%20Multiplex%20Graph%20Contrastive%20Learning%20with%20Soft%20Negatives%0AAuthor%3A%20Zhenhao%20Zhao%20and%20Minhong%20Zhu%20and%20Chen%20Wang%20and%20Sijia%20Wang%20and%20Jiqiang%20Zhang%20and%20Li%20Chen%20and%20Weiran%20Cai%0AAbstract%3A%20%20%20Graph%20Contrastive%20Learning%20%28GCL%29%20seeks%20to%20learn%20nodal%20or%20graph%0Arepresentations%20that%20contain%20maximal%20consistent%20information%20from%0Agraph-structured%20data.%20While%20node-level%20contrasting%20modes%20are%20dominating%2C%20some%0Aefforts%20commence%20to%20explore%20consistency%20across%20different%20scales.%20Yet%2C%20they%20tend%0Ato%20lose%20consistent%20information%20and%20be%20contaminated%20by%20disturbing%20features.%0AHere%2C%20we%20introduce%20MUX-GCL%2C%20a%20novel%20cross-scale%20contrastive%20learning%20paradigm%0Athat%20utilizes%20multiplex%20representations%20as%20effective%20patches.%20While%20this%0Alearning%20mode%20minimizes%20contaminating%20noises%2C%20a%20commensurate%20contrasting%0Astrategy%20using%20positional%20affinities%20further%20avoids%20information%20loss%20by%0Acorrecting%20false%20negative%20pairs%20across%20scales.%20Extensive%20downstream%20experiments%0Ademonstrate%20that%20MUX-GCL%20yields%20multiple%20state-of-the-art%20results%20on%20public%0Adatasets.%20Our%20theoretical%20analysis%20further%20guarantees%20the%20new%20objective%0Afunction%20as%20a%20stricter%20lower%20bound%20of%20mutual%20information%20of%20raw%20input%20features%0Aand%20output%20embeddings%2C%20which%20rationalizes%20this%20paradigm.%20Code%20is%20available%20at%0Ahttps%3A//github.com/MUX-GCL/Code.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08010v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiplex%2520Graph%2520Contrastive%2520Learning%2520with%2520Soft%2520Negatives%26entry.906535625%3DZhenhao%2520Zhao%2520and%2520Minhong%2520Zhu%2520and%2520Chen%2520Wang%2520and%2520Sijia%2520Wang%2520and%2520Jiqiang%2520Zhang%2520and%2520Li%2520Chen%2520and%2520Weiran%2520Cai%26entry.1292438233%3D%2520%2520Graph%2520Contrastive%2520Learning%2520%2528GCL%2529%2520seeks%2520to%2520learn%2520nodal%2520or%2520graph%250Arepresentations%2520that%2520contain%2520maximal%2520consistent%2520information%2520from%250Agraph-structured%2520data.%2520While%2520node-level%2520contrasting%2520modes%2520are%2520dominating%252C%2520some%250Aefforts%2520commence%2520to%2520explore%2520consistency%2520across%2520different%2520scales.%2520Yet%252C%2520they%2520tend%250Ato%2520lose%2520consistent%2520information%2520and%2520be%2520contaminated%2520by%2520disturbing%2520features.%250AHere%252C%2520we%2520introduce%2520MUX-GCL%252C%2520a%2520novel%2520cross-scale%2520contrastive%2520learning%2520paradigm%250Athat%2520utilizes%2520multiplex%2520representations%2520as%2520effective%2520patches.%2520While%2520this%250Alearning%2520mode%2520minimizes%2520contaminating%2520noises%252C%2520a%2520commensurate%2520contrasting%250Astrategy%2520using%2520positional%2520affinities%2520further%2520avoids%2520information%2520loss%2520by%250Acorrecting%2520false%2520negative%2520pairs%2520across%2520scales.%2520Extensive%2520downstream%2520experiments%250Ademonstrate%2520that%2520MUX-GCL%2520yields%2520multiple%2520state-of-the-art%2520results%2520on%2520public%250Adatasets.%2520Our%2520theoretical%2520analysis%2520further%2520guarantees%2520the%2520new%2520objective%250Afunction%2520as%2520a%2520stricter%2520lower%2520bound%2520of%2520mutual%2520information%2520of%2520raw%2520input%2520features%250Aand%2520output%2520embeddings%252C%2520which%2520rationalizes%2520this%2520paradigm.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/MUX-GCL/Code.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08010v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multiplex%20Graph%20Contrastive%20Learning%20with%20Soft%20Negatives&entry.906535625=Zhenhao%20Zhao%20and%20Minhong%20Zhu%20and%20Chen%20Wang%20and%20Sijia%20Wang%20and%20Jiqiang%20Zhang%20and%20Li%20Chen%20and%20Weiran%20Cai&entry.1292438233=%20%20Graph%20Contrastive%20Learning%20%28GCL%29%20seeks%20to%20learn%20nodal%20or%20graph%0Arepresentations%20that%20contain%20maximal%20consistent%20information%20from%0Agraph-structured%20data.%20While%20node-level%20contrasting%20modes%20are%20dominating%2C%20some%0Aefforts%20commence%20to%20explore%20consistency%20across%20different%20scales.%20Yet%2C%20they%20tend%0Ato%20lose%20consistent%20information%20and%20be%20contaminated%20by%20disturbing%20features.%0AHere%2C%20we%20introduce%20MUX-GCL%2C%20a%20novel%20cross-scale%20contrastive%20learning%20paradigm%0Athat%20utilizes%20multiplex%20representations%20as%20effective%20patches.%20While%20this%0Alearning%20mode%20minimizes%20contaminating%20noises%2C%20a%20commensurate%20contrasting%0Astrategy%20using%20positional%20affinities%20further%20avoids%20information%20loss%20by%0Acorrecting%20false%20negative%20pairs%20across%20scales.%20Extensive%20downstream%20experiments%0Ademonstrate%20that%20MUX-GCL%20yields%20multiple%20state-of-the-art%20results%20on%20public%0Adatasets.%20Our%20theoretical%20analysis%20further%20guarantees%20the%20new%20objective%0Afunction%20as%20a%20stricter%20lower%20bound%20of%20mutual%20information%20of%20raw%20input%20features%0Aand%20output%20embeddings%2C%20which%20rationalizes%20this%20paradigm.%20Code%20is%20available%20at%0Ahttps%3A//github.com/MUX-GCL/Code.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08010v1&entry.124074799=Read"},
{"title": "Effective Segmentation of Post-Treatment Gliomas Using Simple\n  Approaches: Artificial Sequence Generation and Ensemble Models", "author": "Heejong Kim and Leo Milecki and Mina C Moghadam and Fengbei Liu and Minh Nguyen and Eric Qiu and Abhishek Thanki and Mert R Sabuncu", "abstract": "  Segmentation is a crucial task in the medical imaging field and is often an\nimportant primary step or even a prerequisite to the analysis of medical\nvolumes. Yet treatments such as surgery complicate the accurate delineation of\nregions of interest. The BraTS Post-Treatment 2024 Challenge published the\nfirst public dataset for post-surgery glioma segmentation and addresses the\naforementioned issue by fostering the development of automated segmentation\ntools for glioma in MRI data. In this effort, we propose two straightforward\napproaches to enhance the segmentation performances of deep learning-based\nmethodologies. First, we incorporate an additional input based on a simple\nlinear combination of the available MRI sequences input, which highlights\nenhancing tumors. Second, we employ various ensembling methods to weigh the\ncontribution of a battery of models. Our results demonstrate that these\napproaches significantly improve segmentation performance compared to baseline\nmodels, underscoring the effectiveness of these simple approaches in improving\nmedical image segmentation tasks.\n", "link": "http://arxiv.org/abs/2409.08143v1", "date": "2024-09-12", "relevancy": 2.5042, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5014}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5014}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Effective%20Segmentation%20of%20Post-Treatment%20Gliomas%20Using%20Simple%0A%20%20Approaches%3A%20Artificial%20Sequence%20Generation%20and%20Ensemble%20Models&body=Title%3A%20Effective%20Segmentation%20of%20Post-Treatment%20Gliomas%20Using%20Simple%0A%20%20Approaches%3A%20Artificial%20Sequence%20Generation%20and%20Ensemble%20Models%0AAuthor%3A%20Heejong%20Kim%20and%20Leo%20Milecki%20and%20Mina%20C%20Moghadam%20and%20Fengbei%20Liu%20and%20Minh%20Nguyen%20and%20Eric%20Qiu%20and%20Abhishek%20Thanki%20and%20Mert%20R%20Sabuncu%0AAbstract%3A%20%20%20Segmentation%20is%20a%20crucial%20task%20in%20the%20medical%20imaging%20field%20and%20is%20often%20an%0Aimportant%20primary%20step%20or%20even%20a%20prerequisite%20to%20the%20analysis%20of%20medical%0Avolumes.%20Yet%20treatments%20such%20as%20surgery%20complicate%20the%20accurate%20delineation%20of%0Aregions%20of%20interest.%20The%20BraTS%20Post-Treatment%202024%20Challenge%20published%20the%0Afirst%20public%20dataset%20for%20post-surgery%20glioma%20segmentation%20and%20addresses%20the%0Aaforementioned%20issue%20by%20fostering%20the%20development%20of%20automated%20segmentation%0Atools%20for%20glioma%20in%20MRI%20data.%20In%20this%20effort%2C%20we%20propose%20two%20straightforward%0Aapproaches%20to%20enhance%20the%20segmentation%20performances%20of%20deep%20learning-based%0Amethodologies.%20First%2C%20we%20incorporate%20an%20additional%20input%20based%20on%20a%20simple%0Alinear%20combination%20of%20the%20available%20MRI%20sequences%20input%2C%20which%20highlights%0Aenhancing%20tumors.%20Second%2C%20we%20employ%20various%20ensembling%20methods%20to%20weigh%20the%0Acontribution%20of%20a%20battery%20of%20models.%20Our%20results%20demonstrate%20that%20these%0Aapproaches%20significantly%20improve%20segmentation%20performance%20compared%20to%20baseline%0Amodels%2C%20underscoring%20the%20effectiveness%20of%20these%20simple%20approaches%20in%20improving%0Amedical%20image%20segmentation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08143v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEffective%2520Segmentation%2520of%2520Post-Treatment%2520Gliomas%2520Using%2520Simple%250A%2520%2520Approaches%253A%2520Artificial%2520Sequence%2520Generation%2520and%2520Ensemble%2520Models%26entry.906535625%3DHeejong%2520Kim%2520and%2520Leo%2520Milecki%2520and%2520Mina%2520C%2520Moghadam%2520and%2520Fengbei%2520Liu%2520and%2520Minh%2520Nguyen%2520and%2520Eric%2520Qiu%2520and%2520Abhishek%2520Thanki%2520and%2520Mert%2520R%2520Sabuncu%26entry.1292438233%3D%2520%2520Segmentation%2520is%2520a%2520crucial%2520task%2520in%2520the%2520medical%2520imaging%2520field%2520and%2520is%2520often%2520an%250Aimportant%2520primary%2520step%2520or%2520even%2520a%2520prerequisite%2520to%2520the%2520analysis%2520of%2520medical%250Avolumes.%2520Yet%2520treatments%2520such%2520as%2520surgery%2520complicate%2520the%2520accurate%2520delineation%2520of%250Aregions%2520of%2520interest.%2520The%2520BraTS%2520Post-Treatment%25202024%2520Challenge%2520published%2520the%250Afirst%2520public%2520dataset%2520for%2520post-surgery%2520glioma%2520segmentation%2520and%2520addresses%2520the%250Aaforementioned%2520issue%2520by%2520fostering%2520the%2520development%2520of%2520automated%2520segmentation%250Atools%2520for%2520glioma%2520in%2520MRI%2520data.%2520In%2520this%2520effort%252C%2520we%2520propose%2520two%2520straightforward%250Aapproaches%2520to%2520enhance%2520the%2520segmentation%2520performances%2520of%2520deep%2520learning-based%250Amethodologies.%2520First%252C%2520we%2520incorporate%2520an%2520additional%2520input%2520based%2520on%2520a%2520simple%250Alinear%2520combination%2520of%2520the%2520available%2520MRI%2520sequences%2520input%252C%2520which%2520highlights%250Aenhancing%2520tumors.%2520Second%252C%2520we%2520employ%2520various%2520ensembling%2520methods%2520to%2520weigh%2520the%250Acontribution%2520of%2520a%2520battery%2520of%2520models.%2520Our%2520results%2520demonstrate%2520that%2520these%250Aapproaches%2520significantly%2520improve%2520segmentation%2520performance%2520compared%2520to%2520baseline%250Amodels%252C%2520underscoring%2520the%2520effectiveness%2520of%2520these%2520simple%2520approaches%2520in%2520improving%250Amedical%2520image%2520segmentation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08143v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Effective%20Segmentation%20of%20Post-Treatment%20Gliomas%20Using%20Simple%0A%20%20Approaches%3A%20Artificial%20Sequence%20Generation%20and%20Ensemble%20Models&entry.906535625=Heejong%20Kim%20and%20Leo%20Milecki%20and%20Mina%20C%20Moghadam%20and%20Fengbei%20Liu%20and%20Minh%20Nguyen%20and%20Eric%20Qiu%20and%20Abhishek%20Thanki%20and%20Mert%20R%20Sabuncu&entry.1292438233=%20%20Segmentation%20is%20a%20crucial%20task%20in%20the%20medical%20imaging%20field%20and%20is%20often%20an%0Aimportant%20primary%20step%20or%20even%20a%20prerequisite%20to%20the%20analysis%20of%20medical%0Avolumes.%20Yet%20treatments%20such%20as%20surgery%20complicate%20the%20accurate%20delineation%20of%0Aregions%20of%20interest.%20The%20BraTS%20Post-Treatment%202024%20Challenge%20published%20the%0Afirst%20public%20dataset%20for%20post-surgery%20glioma%20segmentation%20and%20addresses%20the%0Aaforementioned%20issue%20by%20fostering%20the%20development%20of%20automated%20segmentation%0Atools%20for%20glioma%20in%20MRI%20data.%20In%20this%20effort%2C%20we%20propose%20two%20straightforward%0Aapproaches%20to%20enhance%20the%20segmentation%20performances%20of%20deep%20learning-based%0Amethodologies.%20First%2C%20we%20incorporate%20an%20additional%20input%20based%20on%20a%20simple%0Alinear%20combination%20of%20the%20available%20MRI%20sequences%20input%2C%20which%20highlights%0Aenhancing%20tumors.%20Second%2C%20we%20employ%20various%20ensembling%20methods%20to%20weigh%20the%0Acontribution%20of%20a%20battery%20of%20models.%20Our%20results%20demonstrate%20that%20these%0Aapproaches%20significantly%20improve%20segmentation%20performance%20compared%20to%20baseline%0Amodels%2C%20underscoring%20the%20effectiveness%20of%20these%20simple%20approaches%20in%20improving%0Amedical%20image%20segmentation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08143v1&entry.124074799=Read"},
{"title": "Large Language Models and Cognitive Science: A Comprehensive Review of\n  Similarities, Differences, and Challenges", "author": "Qian Niu and Junyu Liu and Ziqian Bi and Pohsun Feng and Benji Peng and Keyu Chen and Ming Li", "abstract": "  This comprehensive review explores the intersection of Large Language Models\n(LLMs) and cognitive science, examining similarities and differences between\nLLMs and human cognitive processes. We analyze methods for evaluating LLMs\ncognitive abilities and discuss their potential as cognitive models. The review\ncovers applications of LLMs in various cognitive fields, highlighting insights\ngained for cognitive science research. We assess cognitive biases and\nlimitations of LLMs, along with proposed methods for improving their\nperformance. The integration of LLMs with cognitive architectures is examined,\nrevealing promising avenues for enhancing artificial intelligence (AI)\ncapabilities. Key challenges and future research directions are identified,\nemphasizing the need for continued refinement of LLMs to better align with\nhuman cognition. This review provides a balanced perspective on the current\nstate and future potential of LLMs in advancing our understanding of both\nartificial and human intelligence.\n", "link": "http://arxiv.org/abs/2409.02387v3", "date": "2024-09-12", "relevancy": 2.5005, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5207}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5207}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20and%20Cognitive%20Science%3A%20A%20Comprehensive%20Review%20of%0A%20%20Similarities%2C%20Differences%2C%20and%20Challenges&body=Title%3A%20Large%20Language%20Models%20and%20Cognitive%20Science%3A%20A%20Comprehensive%20Review%20of%0A%20%20Similarities%2C%20Differences%2C%20and%20Challenges%0AAuthor%3A%20Qian%20Niu%20and%20Junyu%20Liu%20and%20Ziqian%20Bi%20and%20Pohsun%20Feng%20and%20Benji%20Peng%20and%20Keyu%20Chen%20and%20Ming%20Li%0AAbstract%3A%20%20%20This%20comprehensive%20review%20explores%20the%20intersection%20of%20Large%20Language%20Models%0A%28LLMs%29%20and%20cognitive%20science%2C%20examining%20similarities%20and%20differences%20between%0ALLMs%20and%20human%20cognitive%20processes.%20We%20analyze%20methods%20for%20evaluating%20LLMs%0Acognitive%20abilities%20and%20discuss%20their%20potential%20as%20cognitive%20models.%20The%20review%0Acovers%20applications%20of%20LLMs%20in%20various%20cognitive%20fields%2C%20highlighting%20insights%0Agained%20for%20cognitive%20science%20research.%20We%20assess%20cognitive%20biases%20and%0Alimitations%20of%20LLMs%2C%20along%20with%20proposed%20methods%20for%20improving%20their%0Aperformance.%20The%20integration%20of%20LLMs%20with%20cognitive%20architectures%20is%20examined%2C%0Arevealing%20promising%20avenues%20for%20enhancing%20artificial%20intelligence%20%28AI%29%0Acapabilities.%20Key%20challenges%20and%20future%20research%20directions%20are%20identified%2C%0Aemphasizing%20the%20need%20for%20continued%20refinement%20of%20LLMs%20to%20better%20align%20with%0Ahuman%20cognition.%20This%20review%20provides%20a%20balanced%20perspective%20on%20the%20current%0Astate%20and%20future%20potential%20of%20LLMs%20in%20advancing%20our%20understanding%20of%20both%0Aartificial%20and%20human%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02387v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520and%2520Cognitive%2520Science%253A%2520A%2520Comprehensive%2520Review%2520of%250A%2520%2520Similarities%252C%2520Differences%252C%2520and%2520Challenges%26entry.906535625%3DQian%2520Niu%2520and%2520Junyu%2520Liu%2520and%2520Ziqian%2520Bi%2520and%2520Pohsun%2520Feng%2520and%2520Benji%2520Peng%2520and%2520Keyu%2520Chen%2520and%2520Ming%2520Li%26entry.1292438233%3D%2520%2520This%2520comprehensive%2520review%2520explores%2520the%2520intersection%2520of%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520and%2520cognitive%2520science%252C%2520examining%2520similarities%2520and%2520differences%2520between%250ALLMs%2520and%2520human%2520cognitive%2520processes.%2520We%2520analyze%2520methods%2520for%2520evaluating%2520LLMs%250Acognitive%2520abilities%2520and%2520discuss%2520their%2520potential%2520as%2520cognitive%2520models.%2520The%2520review%250Acovers%2520applications%2520of%2520LLMs%2520in%2520various%2520cognitive%2520fields%252C%2520highlighting%2520insights%250Agained%2520for%2520cognitive%2520science%2520research.%2520We%2520assess%2520cognitive%2520biases%2520and%250Alimitations%2520of%2520LLMs%252C%2520along%2520with%2520proposed%2520methods%2520for%2520improving%2520their%250Aperformance.%2520The%2520integration%2520of%2520LLMs%2520with%2520cognitive%2520architectures%2520is%2520examined%252C%250Arevealing%2520promising%2520avenues%2520for%2520enhancing%2520artificial%2520intelligence%2520%2528AI%2529%250Acapabilities.%2520Key%2520challenges%2520and%2520future%2520research%2520directions%2520are%2520identified%252C%250Aemphasizing%2520the%2520need%2520for%2520continued%2520refinement%2520of%2520LLMs%2520to%2520better%2520align%2520with%250Ahuman%2520cognition.%2520This%2520review%2520provides%2520a%2520balanced%2520perspective%2520on%2520the%2520current%250Astate%2520and%2520future%2520potential%2520of%2520LLMs%2520in%2520advancing%2520our%2520understanding%2520of%2520both%250Aartificial%2520and%2520human%2520intelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02387v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20and%20Cognitive%20Science%3A%20A%20Comprehensive%20Review%20of%0A%20%20Similarities%2C%20Differences%2C%20and%20Challenges&entry.906535625=Qian%20Niu%20and%20Junyu%20Liu%20and%20Ziqian%20Bi%20and%20Pohsun%20Feng%20and%20Benji%20Peng%20and%20Keyu%20Chen%20and%20Ming%20Li&entry.1292438233=%20%20This%20comprehensive%20review%20explores%20the%20intersection%20of%20Large%20Language%20Models%0A%28LLMs%29%20and%20cognitive%20science%2C%20examining%20similarities%20and%20differences%20between%0ALLMs%20and%20human%20cognitive%20processes.%20We%20analyze%20methods%20for%20evaluating%20LLMs%0Acognitive%20abilities%20and%20discuss%20their%20potential%20as%20cognitive%20models.%20The%20review%0Acovers%20applications%20of%20LLMs%20in%20various%20cognitive%20fields%2C%20highlighting%20insights%0Agained%20for%20cognitive%20science%20research.%20We%20assess%20cognitive%20biases%20and%0Alimitations%20of%20LLMs%2C%20along%20with%20proposed%20methods%20for%20improving%20their%0Aperformance.%20The%20integration%20of%20LLMs%20with%20cognitive%20architectures%20is%20examined%2C%0Arevealing%20promising%20avenues%20for%20enhancing%20artificial%20intelligence%20%28AI%29%0Acapabilities.%20Key%20challenges%20and%20future%20research%20directions%20are%20identified%2C%0Aemphasizing%20the%20need%20for%20continued%20refinement%20of%20LLMs%20to%20better%20align%20with%0Ahuman%20cognition.%20This%20review%20provides%20a%20balanced%20perspective%20on%20the%20current%0Astate%20and%20future%20potential%20of%20LLMs%20in%20advancing%20our%20understanding%20of%20both%0Aartificial%20and%20human%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02387v3&entry.124074799=Read"},
{"title": "FlashSplat: 2D to 3D Gaussian Splatting Segmentation Solved Optimally", "author": "Qiuhong Shen and Xingyi Yang and Xinchao Wang", "abstract": "  This study addresses the challenge of accurately segmenting 3D Gaussian\nSplatting from 2D masks. Conventional methods often rely on iterative gradient\ndescent to assign each Gaussian a unique label, leading to lengthy optimization\nand sub-optimal solutions. Instead, we propose a straightforward yet globally\noptimal solver for 3D-GS segmentation. The core insight of our method is that,\nwith a reconstructed 3D-GS scene, the rendering of the 2D masks is essentially\na linear function with respect to the labels of each Gaussian. As such, the\noptimal label assignment can be solved via linear programming in closed form.\nThis solution capitalizes on the alpha blending characteristic of the splatting\nprocess for single step optimization. By incorporating the background bias in\nour objective function, our method shows superior robustness in 3D segmentation\nagainst noises. Remarkably, our optimization completes within 30 seconds, about\n50$\\times$ faster than the best existing methods. Extensive experiments\ndemonstrate the efficiency and robustness of our method in segmenting various\nscenes, and its superior performance in downstream tasks such as object removal\nand inpainting. Demos and code will be available at\nhttps://github.com/florinshen/FlashSplat.\n", "link": "http://arxiv.org/abs/2409.08270v1", "date": "2024-09-12", "relevancy": 2.4958, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6974}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5811}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlashSplat%3A%202D%20to%203D%20Gaussian%20Splatting%20Segmentation%20Solved%20Optimally&body=Title%3A%20FlashSplat%3A%202D%20to%203D%20Gaussian%20Splatting%20Segmentation%20Solved%20Optimally%0AAuthor%3A%20Qiuhong%20Shen%20and%20Xingyi%20Yang%20and%20Xinchao%20Wang%0AAbstract%3A%20%20%20This%20study%20addresses%20the%20challenge%20of%20accurately%20segmenting%203D%20Gaussian%0ASplatting%20from%202D%20masks.%20Conventional%20methods%20often%20rely%20on%20iterative%20gradient%0Adescent%20to%20assign%20each%20Gaussian%20a%20unique%20label%2C%20leading%20to%20lengthy%20optimization%0Aand%20sub-optimal%20solutions.%20Instead%2C%20we%20propose%20a%20straightforward%20yet%20globally%0Aoptimal%20solver%20for%203D-GS%20segmentation.%20The%20core%20insight%20of%20our%20method%20is%20that%2C%0Awith%20a%20reconstructed%203D-GS%20scene%2C%20the%20rendering%20of%20the%202D%20masks%20is%20essentially%0Aa%20linear%20function%20with%20respect%20to%20the%20labels%20of%20each%20Gaussian.%20As%20such%2C%20the%0Aoptimal%20label%20assignment%20can%20be%20solved%20via%20linear%20programming%20in%20closed%20form.%0AThis%20solution%20capitalizes%20on%20the%20alpha%20blending%20characteristic%20of%20the%20splatting%0Aprocess%20for%20single%20step%20optimization.%20By%20incorporating%20the%20background%20bias%20in%0Aour%20objective%20function%2C%20our%20method%20shows%20superior%20robustness%20in%203D%20segmentation%0Aagainst%20noises.%20Remarkably%2C%20our%20optimization%20completes%20within%2030%20seconds%2C%20about%0A50%24%5Ctimes%24%20faster%20than%20the%20best%20existing%20methods.%20Extensive%20experiments%0Ademonstrate%20the%20efficiency%20and%20robustness%20of%20our%20method%20in%20segmenting%20various%0Ascenes%2C%20and%20its%20superior%20performance%20in%20downstream%20tasks%20such%20as%20object%20removal%0Aand%20inpainting.%20Demos%20and%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/florinshen/FlashSplat.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08270v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlashSplat%253A%25202D%2520to%25203D%2520Gaussian%2520Splatting%2520Segmentation%2520Solved%2520Optimally%26entry.906535625%3DQiuhong%2520Shen%2520and%2520Xingyi%2520Yang%2520and%2520Xinchao%2520Wang%26entry.1292438233%3D%2520%2520This%2520study%2520addresses%2520the%2520challenge%2520of%2520accurately%2520segmenting%25203D%2520Gaussian%250ASplatting%2520from%25202D%2520masks.%2520Conventional%2520methods%2520often%2520rely%2520on%2520iterative%2520gradient%250Adescent%2520to%2520assign%2520each%2520Gaussian%2520a%2520unique%2520label%252C%2520leading%2520to%2520lengthy%2520optimization%250Aand%2520sub-optimal%2520solutions.%2520Instead%252C%2520we%2520propose%2520a%2520straightforward%2520yet%2520globally%250Aoptimal%2520solver%2520for%25203D-GS%2520segmentation.%2520The%2520core%2520insight%2520of%2520our%2520method%2520is%2520that%252C%250Awith%2520a%2520reconstructed%25203D-GS%2520scene%252C%2520the%2520rendering%2520of%2520the%25202D%2520masks%2520is%2520essentially%250Aa%2520linear%2520function%2520with%2520respect%2520to%2520the%2520labels%2520of%2520each%2520Gaussian.%2520As%2520such%252C%2520the%250Aoptimal%2520label%2520assignment%2520can%2520be%2520solved%2520via%2520linear%2520programming%2520in%2520closed%2520form.%250AThis%2520solution%2520capitalizes%2520on%2520the%2520alpha%2520blending%2520characteristic%2520of%2520the%2520splatting%250Aprocess%2520for%2520single%2520step%2520optimization.%2520By%2520incorporating%2520the%2520background%2520bias%2520in%250Aour%2520objective%2520function%252C%2520our%2520method%2520shows%2520superior%2520robustness%2520in%25203D%2520segmentation%250Aagainst%2520noises.%2520Remarkably%252C%2520our%2520optimization%2520completes%2520within%252030%2520seconds%252C%2520about%250A50%2524%255Ctimes%2524%2520faster%2520than%2520the%2520best%2520existing%2520methods.%2520Extensive%2520experiments%250Ademonstrate%2520the%2520efficiency%2520and%2520robustness%2520of%2520our%2520method%2520in%2520segmenting%2520various%250Ascenes%252C%2520and%2520its%2520superior%2520performance%2520in%2520downstream%2520tasks%2520such%2520as%2520object%2520removal%250Aand%2520inpainting.%2520Demos%2520and%2520code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/florinshen/FlashSplat.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08270v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlashSplat%3A%202D%20to%203D%20Gaussian%20Splatting%20Segmentation%20Solved%20Optimally&entry.906535625=Qiuhong%20Shen%20and%20Xingyi%20Yang%20and%20Xinchao%20Wang&entry.1292438233=%20%20This%20study%20addresses%20the%20challenge%20of%20accurately%20segmenting%203D%20Gaussian%0ASplatting%20from%202D%20masks.%20Conventional%20methods%20often%20rely%20on%20iterative%20gradient%0Adescent%20to%20assign%20each%20Gaussian%20a%20unique%20label%2C%20leading%20to%20lengthy%20optimization%0Aand%20sub-optimal%20solutions.%20Instead%2C%20we%20propose%20a%20straightforward%20yet%20globally%0Aoptimal%20solver%20for%203D-GS%20segmentation.%20The%20core%20insight%20of%20our%20method%20is%20that%2C%0Awith%20a%20reconstructed%203D-GS%20scene%2C%20the%20rendering%20of%20the%202D%20masks%20is%20essentially%0Aa%20linear%20function%20with%20respect%20to%20the%20labels%20of%20each%20Gaussian.%20As%20such%2C%20the%0Aoptimal%20label%20assignment%20can%20be%20solved%20via%20linear%20programming%20in%20closed%20form.%0AThis%20solution%20capitalizes%20on%20the%20alpha%20blending%20characteristic%20of%20the%20splatting%0Aprocess%20for%20single%20step%20optimization.%20By%20incorporating%20the%20background%20bias%20in%0Aour%20objective%20function%2C%20our%20method%20shows%20superior%20robustness%20in%203D%20segmentation%0Aagainst%20noises.%20Remarkably%2C%20our%20optimization%20completes%20within%2030%20seconds%2C%20about%0A50%24%5Ctimes%24%20faster%20than%20the%20best%20existing%20methods.%20Extensive%20experiments%0Ademonstrate%20the%20efficiency%20and%20robustness%20of%20our%20method%20in%20segmenting%20various%0Ascenes%2C%20and%20its%20superior%20performance%20in%20downstream%20tasks%20such%20as%20object%20removal%0Aand%20inpainting.%20Demos%20and%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/florinshen/FlashSplat.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08270v1&entry.124074799=Read"},
{"title": "Do Vision Foundation Models Enhance Domain Generalization in Medical\n  Image Segmentation?", "author": "Kerem Cekmeceli and Meva Himmetoglu and Guney I. Tombak and Anna Susmelj and Ertunc Erdil and Ender Konukoglu", "abstract": "  Neural networks achieve state-of-the-art performance in many supervised\nlearning tasks when the training data distribution matches the test data\ndistribution. However, their performance drops significantly under domain\n(covariate) shift, a prevalent issue in medical image segmentation due to\nvarying acquisition settings across different scanner models and protocols.\nRecently, foundational models (FMs) trained on large datasets have gained\nattention for their ability to be adapted for downstream tasks and achieve\nstate-of-the-art performance with excellent generalization capabilities on\nnatural images. However, their effectiveness in medical image segmentation\nremains underexplored. In this paper, we investigate the domain generalization\nperformance of various FMs, including DinoV2, SAM, MedSAM, and MAE, when\nfine-tuned using various parameter-efficient fine-tuning (PEFT) techniques such\nas Ladder and Rein (+LoRA) and decoder heads. We introduce a novel decode head\narchitecture, HQHSAM, which simply integrates elements from two\nstate-of-the-art decoder heads, HSAM and HQSAM, to enhance segmentation\nperformance. Our extensive experiments on multiple datasets, encompassing\nvarious anatomies and modalities, reveal that FMs, particularly with the HQHSAM\ndecode head, improve domain generalization for medical image segmentation.\nMoreover, we found that the effectiveness of PEFT techniques varies across\ndifferent FMs. These findings underscore the potential of FMs to enhance the\ndomain generalization performance of neural networks in medical image\nsegmentation across diverse clinical settings, providing a solid foundation for\nfuture research. Code and models are available for research purposes at\n\\url{https://github.com/kerem-cekmeceli/Foundation-Models-for-Medical-Imagery}.\n", "link": "http://arxiv.org/abs/2409.07960v1", "date": "2024-09-12", "relevancy": 2.4874, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6342}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6342}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5601}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Vision%20Foundation%20Models%20Enhance%20Domain%20Generalization%20in%20Medical%0A%20%20Image%20Segmentation%3F&body=Title%3A%20Do%20Vision%20Foundation%20Models%20Enhance%20Domain%20Generalization%20in%20Medical%0A%20%20Image%20Segmentation%3F%0AAuthor%3A%20Kerem%20Cekmeceli%20and%20Meva%20Himmetoglu%20and%20Guney%20I.%20Tombak%20and%20Anna%20Susmelj%20and%20Ertunc%20Erdil%20and%20Ender%20Konukoglu%0AAbstract%3A%20%20%20Neural%20networks%20achieve%20state-of-the-art%20performance%20in%20many%20supervised%0Alearning%20tasks%20when%20the%20training%20data%20distribution%20matches%20the%20test%20data%0Adistribution.%20However%2C%20their%20performance%20drops%20significantly%20under%20domain%0A%28covariate%29%20shift%2C%20a%20prevalent%20issue%20in%20medical%20image%20segmentation%20due%20to%0Avarying%20acquisition%20settings%20across%20different%20scanner%20models%20and%20protocols.%0ARecently%2C%20foundational%20models%20%28FMs%29%20trained%20on%20large%20datasets%20have%20gained%0Aattention%20for%20their%20ability%20to%20be%20adapted%20for%20downstream%20tasks%20and%20achieve%0Astate-of-the-art%20performance%20with%20excellent%20generalization%20capabilities%20on%0Anatural%20images.%20However%2C%20their%20effectiveness%20in%20medical%20image%20segmentation%0Aremains%20underexplored.%20In%20this%20paper%2C%20we%20investigate%20the%20domain%20generalization%0Aperformance%20of%20various%20FMs%2C%20including%20DinoV2%2C%20SAM%2C%20MedSAM%2C%20and%20MAE%2C%20when%0Afine-tuned%20using%20various%20parameter-efficient%20fine-tuning%20%28PEFT%29%20techniques%20such%0Aas%20Ladder%20and%20Rein%20%28%2BLoRA%29%20and%20decoder%20heads.%20We%20introduce%20a%20novel%20decode%20head%0Aarchitecture%2C%20HQHSAM%2C%20which%20simply%20integrates%20elements%20from%20two%0Astate-of-the-art%20decoder%20heads%2C%20HSAM%20and%20HQSAM%2C%20to%20enhance%20segmentation%0Aperformance.%20Our%20extensive%20experiments%20on%20multiple%20datasets%2C%20encompassing%0Avarious%20anatomies%20and%20modalities%2C%20reveal%20that%20FMs%2C%20particularly%20with%20the%20HQHSAM%0Adecode%20head%2C%20improve%20domain%20generalization%20for%20medical%20image%20segmentation.%0AMoreover%2C%20we%20found%20that%20the%20effectiveness%20of%20PEFT%20techniques%20varies%20across%0Adifferent%20FMs.%20These%20findings%20underscore%20the%20potential%20of%20FMs%20to%20enhance%20the%0Adomain%20generalization%20performance%20of%20neural%20networks%20in%20medical%20image%0Asegmentation%20across%20diverse%20clinical%20settings%2C%20providing%20a%20solid%20foundation%20for%0Afuture%20research.%20Code%20and%20models%20are%20available%20for%20research%20purposes%20at%0A%5Curl%7Bhttps%3A//github.com/kerem-cekmeceli/Foundation-Models-for-Medical-Imagery%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07960v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Vision%2520Foundation%2520Models%2520Enhance%2520Domain%2520Generalization%2520in%2520Medical%250A%2520%2520Image%2520Segmentation%253F%26entry.906535625%3DKerem%2520Cekmeceli%2520and%2520Meva%2520Himmetoglu%2520and%2520Guney%2520I.%2520Tombak%2520and%2520Anna%2520Susmelj%2520and%2520Ertunc%2520Erdil%2520and%2520Ender%2520Konukoglu%26entry.1292438233%3D%2520%2520Neural%2520networks%2520achieve%2520state-of-the-art%2520performance%2520in%2520many%2520supervised%250Alearning%2520tasks%2520when%2520the%2520training%2520data%2520distribution%2520matches%2520the%2520test%2520data%250Adistribution.%2520However%252C%2520their%2520performance%2520drops%2520significantly%2520under%2520domain%250A%2528covariate%2529%2520shift%252C%2520a%2520prevalent%2520issue%2520in%2520medical%2520image%2520segmentation%2520due%2520to%250Avarying%2520acquisition%2520settings%2520across%2520different%2520scanner%2520models%2520and%2520protocols.%250ARecently%252C%2520foundational%2520models%2520%2528FMs%2529%2520trained%2520on%2520large%2520datasets%2520have%2520gained%250Aattention%2520for%2520their%2520ability%2520to%2520be%2520adapted%2520for%2520downstream%2520tasks%2520and%2520achieve%250Astate-of-the-art%2520performance%2520with%2520excellent%2520generalization%2520capabilities%2520on%250Anatural%2520images.%2520However%252C%2520their%2520effectiveness%2520in%2520medical%2520image%2520segmentation%250Aremains%2520underexplored.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520domain%2520generalization%250Aperformance%2520of%2520various%2520FMs%252C%2520including%2520DinoV2%252C%2520SAM%252C%2520MedSAM%252C%2520and%2520MAE%252C%2520when%250Afine-tuned%2520using%2520various%2520parameter-efficient%2520fine-tuning%2520%2528PEFT%2529%2520techniques%2520such%250Aas%2520Ladder%2520and%2520Rein%2520%2528%252BLoRA%2529%2520and%2520decoder%2520heads.%2520We%2520introduce%2520a%2520novel%2520decode%2520head%250Aarchitecture%252C%2520HQHSAM%252C%2520which%2520simply%2520integrates%2520elements%2520from%2520two%250Astate-of-the-art%2520decoder%2520heads%252C%2520HSAM%2520and%2520HQSAM%252C%2520to%2520enhance%2520segmentation%250Aperformance.%2520Our%2520extensive%2520experiments%2520on%2520multiple%2520datasets%252C%2520encompassing%250Avarious%2520anatomies%2520and%2520modalities%252C%2520reveal%2520that%2520FMs%252C%2520particularly%2520with%2520the%2520HQHSAM%250Adecode%2520head%252C%2520improve%2520domain%2520generalization%2520for%2520medical%2520image%2520segmentation.%250AMoreover%252C%2520we%2520found%2520that%2520the%2520effectiveness%2520of%2520PEFT%2520techniques%2520varies%2520across%250Adifferent%2520FMs.%2520These%2520findings%2520underscore%2520the%2520potential%2520of%2520FMs%2520to%2520enhance%2520the%250Adomain%2520generalization%2520performance%2520of%2520neural%2520networks%2520in%2520medical%2520image%250Asegmentation%2520across%2520diverse%2520clinical%2520settings%252C%2520providing%2520a%2520solid%2520foundation%2520for%250Afuture%2520research.%2520Code%2520and%2520models%2520are%2520available%2520for%2520research%2520purposes%2520at%250A%255Curl%257Bhttps%253A//github.com/kerem-cekmeceli/Foundation-Models-for-Medical-Imagery%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07960v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Vision%20Foundation%20Models%20Enhance%20Domain%20Generalization%20in%20Medical%0A%20%20Image%20Segmentation%3F&entry.906535625=Kerem%20Cekmeceli%20and%20Meva%20Himmetoglu%20and%20Guney%20I.%20Tombak%20and%20Anna%20Susmelj%20and%20Ertunc%20Erdil%20and%20Ender%20Konukoglu&entry.1292438233=%20%20Neural%20networks%20achieve%20state-of-the-art%20performance%20in%20many%20supervised%0Alearning%20tasks%20when%20the%20training%20data%20distribution%20matches%20the%20test%20data%0Adistribution.%20However%2C%20their%20performance%20drops%20significantly%20under%20domain%0A%28covariate%29%20shift%2C%20a%20prevalent%20issue%20in%20medical%20image%20segmentation%20due%20to%0Avarying%20acquisition%20settings%20across%20different%20scanner%20models%20and%20protocols.%0ARecently%2C%20foundational%20models%20%28FMs%29%20trained%20on%20large%20datasets%20have%20gained%0Aattention%20for%20their%20ability%20to%20be%20adapted%20for%20downstream%20tasks%20and%20achieve%0Astate-of-the-art%20performance%20with%20excellent%20generalization%20capabilities%20on%0Anatural%20images.%20However%2C%20their%20effectiveness%20in%20medical%20image%20segmentation%0Aremains%20underexplored.%20In%20this%20paper%2C%20we%20investigate%20the%20domain%20generalization%0Aperformance%20of%20various%20FMs%2C%20including%20DinoV2%2C%20SAM%2C%20MedSAM%2C%20and%20MAE%2C%20when%0Afine-tuned%20using%20various%20parameter-efficient%20fine-tuning%20%28PEFT%29%20techniques%20such%0Aas%20Ladder%20and%20Rein%20%28%2BLoRA%29%20and%20decoder%20heads.%20We%20introduce%20a%20novel%20decode%20head%0Aarchitecture%2C%20HQHSAM%2C%20which%20simply%20integrates%20elements%20from%20two%0Astate-of-the-art%20decoder%20heads%2C%20HSAM%20and%20HQSAM%2C%20to%20enhance%20segmentation%0Aperformance.%20Our%20extensive%20experiments%20on%20multiple%20datasets%2C%20encompassing%0Avarious%20anatomies%20and%20modalities%2C%20reveal%20that%20FMs%2C%20particularly%20with%20the%20HQHSAM%0Adecode%20head%2C%20improve%20domain%20generalization%20for%20medical%20image%20segmentation.%0AMoreover%2C%20we%20found%20that%20the%20effectiveness%20of%20PEFT%20techniques%20varies%20across%0Adifferent%20FMs.%20These%20findings%20underscore%20the%20potential%20of%20FMs%20to%20enhance%20the%0Adomain%20generalization%20performance%20of%20neural%20networks%20in%20medical%20image%0Asegmentation%20across%20diverse%20clinical%20settings%2C%20providing%20a%20solid%20foundation%20for%0Afuture%20research.%20Code%20and%20models%20are%20available%20for%20research%20purposes%20at%0A%5Curl%7Bhttps%3A//github.com/kerem-cekmeceli/Foundation-Models-for-Medical-Imagery%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07960v1&entry.124074799=Read"},
{"title": "EZIGen: Enhancing zero-shot subject-driven image generation with precise\n  subject encoding and decoupled guidance", "author": "Zicheng Duan and Yuxuan Ding and Chenhui Gou and Ziqin Zhou and Ethan Smith and Lingqiao Liu", "abstract": "  Zero-shot subject-driven image generation aims to produce images that\nincorporate a subject from a given example image. The challenge lies in\npreserving the subject's identity while aligning with the text prompt, which\noften requires modifying certain aspects of the subject's appearance. Despite\nadvancements in diffusion model based methods, existing approaches still\nstruggle to balance identity preservation with text prompt alignment. In this\nstudy, we conducted an in-depth investigation into this issue and uncovered key\ninsights for achieving effective identity preservation while maintaining a\nstrong balance. Our key findings include: (1) the design of the subject image\nencoder significantly impacts identity preservation quality, and (2) generating\nan initial layout is crucial for both text alignment and identity preservation.\nBuilding on these insights, we introduce a new approach called EZIGen, which\nemploys two main strategies: a carefully crafted subject image Encoder based on\nthe UNet architecture of the pretrained Stable Diffusion model to ensure\nhigh-quality identity transfer, following a process that decouples the guidance\nstages and iteratively refines the initial image layout. Through these\nstrategies, EZIGen achieves state-of-the-art results on multiple subject-driven\nbenchmarks with a unified model and 100 times less training data.\n", "link": "http://arxiv.org/abs/2409.08091v1", "date": "2024-09-12", "relevancy": 2.4605, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6515}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5995}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EZIGen%3A%20Enhancing%20zero-shot%20subject-driven%20image%20generation%20with%20precise%0A%20%20subject%20encoding%20and%20decoupled%20guidance&body=Title%3A%20EZIGen%3A%20Enhancing%20zero-shot%20subject-driven%20image%20generation%20with%20precise%0A%20%20subject%20encoding%20and%20decoupled%20guidance%0AAuthor%3A%20Zicheng%20Duan%20and%20Yuxuan%20Ding%20and%20Chenhui%20Gou%20and%20Ziqin%20Zhou%20and%20Ethan%20Smith%20and%20Lingqiao%20Liu%0AAbstract%3A%20%20%20Zero-shot%20subject-driven%20image%20generation%20aims%20to%20produce%20images%20that%0Aincorporate%20a%20subject%20from%20a%20given%20example%20image.%20The%20challenge%20lies%20in%0Apreserving%20the%20subject%27s%20identity%20while%20aligning%20with%20the%20text%20prompt%2C%20which%0Aoften%20requires%20modifying%20certain%20aspects%20of%20the%20subject%27s%20appearance.%20Despite%0Aadvancements%20in%20diffusion%20model%20based%20methods%2C%20existing%20approaches%20still%0Astruggle%20to%20balance%20identity%20preservation%20with%20text%20prompt%20alignment.%20In%20this%0Astudy%2C%20we%20conducted%20an%20in-depth%20investigation%20into%20this%20issue%20and%20uncovered%20key%0Ainsights%20for%20achieving%20effective%20identity%20preservation%20while%20maintaining%20a%0Astrong%20balance.%20Our%20key%20findings%20include%3A%20%281%29%20the%20design%20of%20the%20subject%20image%0Aencoder%20significantly%20impacts%20identity%20preservation%20quality%2C%20and%20%282%29%20generating%0Aan%20initial%20layout%20is%20crucial%20for%20both%20text%20alignment%20and%20identity%20preservation.%0ABuilding%20on%20these%20insights%2C%20we%20introduce%20a%20new%20approach%20called%20EZIGen%2C%20which%0Aemploys%20two%20main%20strategies%3A%20a%20carefully%20crafted%20subject%20image%20Encoder%20based%20on%0Athe%20UNet%20architecture%20of%20the%20pretrained%20Stable%20Diffusion%20model%20to%20ensure%0Ahigh-quality%20identity%20transfer%2C%20following%20a%20process%20that%20decouples%20the%20guidance%0Astages%20and%20iteratively%20refines%20the%20initial%20image%20layout.%20Through%20these%0Astrategies%2C%20EZIGen%20achieves%20state-of-the-art%20results%20on%20multiple%20subject-driven%0Abenchmarks%20with%20a%20unified%20model%20and%20100%20times%20less%20training%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08091v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEZIGen%253A%2520Enhancing%2520zero-shot%2520subject-driven%2520image%2520generation%2520with%2520precise%250A%2520%2520subject%2520encoding%2520and%2520decoupled%2520guidance%26entry.906535625%3DZicheng%2520Duan%2520and%2520Yuxuan%2520Ding%2520and%2520Chenhui%2520Gou%2520and%2520Ziqin%2520Zhou%2520and%2520Ethan%2520Smith%2520and%2520Lingqiao%2520Liu%26entry.1292438233%3D%2520%2520Zero-shot%2520subject-driven%2520image%2520generation%2520aims%2520to%2520produce%2520images%2520that%250Aincorporate%2520a%2520subject%2520from%2520a%2520given%2520example%2520image.%2520The%2520challenge%2520lies%2520in%250Apreserving%2520the%2520subject%2527s%2520identity%2520while%2520aligning%2520with%2520the%2520text%2520prompt%252C%2520which%250Aoften%2520requires%2520modifying%2520certain%2520aspects%2520of%2520the%2520subject%2527s%2520appearance.%2520Despite%250Aadvancements%2520in%2520diffusion%2520model%2520based%2520methods%252C%2520existing%2520approaches%2520still%250Astruggle%2520to%2520balance%2520identity%2520preservation%2520with%2520text%2520prompt%2520alignment.%2520In%2520this%250Astudy%252C%2520we%2520conducted%2520an%2520in-depth%2520investigation%2520into%2520this%2520issue%2520and%2520uncovered%2520key%250Ainsights%2520for%2520achieving%2520effective%2520identity%2520preservation%2520while%2520maintaining%2520a%250Astrong%2520balance.%2520Our%2520key%2520findings%2520include%253A%2520%25281%2529%2520the%2520design%2520of%2520the%2520subject%2520image%250Aencoder%2520significantly%2520impacts%2520identity%2520preservation%2520quality%252C%2520and%2520%25282%2529%2520generating%250Aan%2520initial%2520layout%2520is%2520crucial%2520for%2520both%2520text%2520alignment%2520and%2520identity%2520preservation.%250ABuilding%2520on%2520these%2520insights%252C%2520we%2520introduce%2520a%2520new%2520approach%2520called%2520EZIGen%252C%2520which%250Aemploys%2520two%2520main%2520strategies%253A%2520a%2520carefully%2520crafted%2520subject%2520image%2520Encoder%2520based%2520on%250Athe%2520UNet%2520architecture%2520of%2520the%2520pretrained%2520Stable%2520Diffusion%2520model%2520to%2520ensure%250Ahigh-quality%2520identity%2520transfer%252C%2520following%2520a%2520process%2520that%2520decouples%2520the%2520guidance%250Astages%2520and%2520iteratively%2520refines%2520the%2520initial%2520image%2520layout.%2520Through%2520these%250Astrategies%252C%2520EZIGen%2520achieves%2520state-of-the-art%2520results%2520on%2520multiple%2520subject-driven%250Abenchmarks%2520with%2520a%2520unified%2520model%2520and%2520100%2520times%2520less%2520training%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08091v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EZIGen%3A%20Enhancing%20zero-shot%20subject-driven%20image%20generation%20with%20precise%0A%20%20subject%20encoding%20and%20decoupled%20guidance&entry.906535625=Zicheng%20Duan%20and%20Yuxuan%20Ding%20and%20Chenhui%20Gou%20and%20Ziqin%20Zhou%20and%20Ethan%20Smith%20and%20Lingqiao%20Liu&entry.1292438233=%20%20Zero-shot%20subject-driven%20image%20generation%20aims%20to%20produce%20images%20that%0Aincorporate%20a%20subject%20from%20a%20given%20example%20image.%20The%20challenge%20lies%20in%0Apreserving%20the%20subject%27s%20identity%20while%20aligning%20with%20the%20text%20prompt%2C%20which%0Aoften%20requires%20modifying%20certain%20aspects%20of%20the%20subject%27s%20appearance.%20Despite%0Aadvancements%20in%20diffusion%20model%20based%20methods%2C%20existing%20approaches%20still%0Astruggle%20to%20balance%20identity%20preservation%20with%20text%20prompt%20alignment.%20In%20this%0Astudy%2C%20we%20conducted%20an%20in-depth%20investigation%20into%20this%20issue%20and%20uncovered%20key%0Ainsights%20for%20achieving%20effective%20identity%20preservation%20while%20maintaining%20a%0Astrong%20balance.%20Our%20key%20findings%20include%3A%20%281%29%20the%20design%20of%20the%20subject%20image%0Aencoder%20significantly%20impacts%20identity%20preservation%20quality%2C%20and%20%282%29%20generating%0Aan%20initial%20layout%20is%20crucial%20for%20both%20text%20alignment%20and%20identity%20preservation.%0ABuilding%20on%20these%20insights%2C%20we%20introduce%20a%20new%20approach%20called%20EZIGen%2C%20which%0Aemploys%20two%20main%20strategies%3A%20a%20carefully%20crafted%20subject%20image%20Encoder%20based%20on%0Athe%20UNet%20architecture%20of%20the%20pretrained%20Stable%20Diffusion%20model%20to%20ensure%0Ahigh-quality%20identity%20transfer%2C%20following%20a%20process%20that%20decouples%20the%20guidance%0Astages%20and%20iteratively%20refines%20the%20initial%20image%20layout.%20Through%20these%0Astrategies%2C%20EZIGen%20achieves%20state-of-the-art%20results%20on%20multiple%20subject-driven%0Abenchmarks%20with%20a%20unified%20model%20and%20100%20times%20less%20training%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08091v1&entry.124074799=Read"},
{"title": "AudioBERT: Audio Knowledge Augmented Language Model", "author": "Hyunjong Ok and Suho Yoo and Jaeho Lee", "abstract": "  Recent studies have identified that language models, pretrained on text-only\ndatasets, often lack elementary visual knowledge, \\textit{e.g.,} colors of\neveryday objects. Motivated by this observation, we ask whether a similar\nshortcoming exists in terms of the \\textit{auditory} knowledge. To answer this\nquestion, we construct a new dataset called AuditoryBench, which consists of\ntwo novel tasks for evaluating auditory knowledge. Based on our analysis using\nthe benchmark, we find that language models also suffer from a severe lack of\nauditory knowledge. To address this limitation, we propose AudioBERT, a novel\nmethod to augment the auditory knowledge of BERT through a retrieval-based\napproach. First, we detect auditory knowledge spans in prompts to query our\nretrieval model efficiently. Then, we inject audio knowledge into BERT and\nswitch on low-rank adaptation for effective adaptation when audio knowledge is\nrequired. Our experiments demonstrate that AudioBERT is quite effective,\nachieving superior performance on the AuditoryBench. The dataset and code are\navailable at \\bulurl{https://github.com/HJ-Ok/AudioBERT}.\n", "link": "http://arxiv.org/abs/2409.08199v1", "date": "2024-09-12", "relevancy": 2.4506, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4971}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4866}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AudioBERT%3A%20Audio%20Knowledge%20Augmented%20Language%20Model&body=Title%3A%20AudioBERT%3A%20Audio%20Knowledge%20Augmented%20Language%20Model%0AAuthor%3A%20Hyunjong%20Ok%20and%20Suho%20Yoo%20and%20Jaeho%20Lee%0AAbstract%3A%20%20%20Recent%20studies%20have%20identified%20that%20language%20models%2C%20pretrained%20on%20text-only%0Adatasets%2C%20often%20lack%20elementary%20visual%20knowledge%2C%20%5Ctextit%7Be.g.%2C%7D%20colors%20of%0Aeveryday%20objects.%20Motivated%20by%20this%20observation%2C%20we%20ask%20whether%20a%20similar%0Ashortcoming%20exists%20in%20terms%20of%20the%20%5Ctextit%7Bauditory%7D%20knowledge.%20To%20answer%20this%0Aquestion%2C%20we%20construct%20a%20new%20dataset%20called%20AuditoryBench%2C%20which%20consists%20of%0Atwo%20novel%20tasks%20for%20evaluating%20auditory%20knowledge.%20Based%20on%20our%20analysis%20using%0Athe%20benchmark%2C%20we%20find%20that%20language%20models%20also%20suffer%20from%20a%20severe%20lack%20of%0Aauditory%20knowledge.%20To%20address%20this%20limitation%2C%20we%20propose%20AudioBERT%2C%20a%20novel%0Amethod%20to%20augment%20the%20auditory%20knowledge%20of%20BERT%20through%20a%20retrieval-based%0Aapproach.%20First%2C%20we%20detect%20auditory%20knowledge%20spans%20in%20prompts%20to%20query%20our%0Aretrieval%20model%20efficiently.%20Then%2C%20we%20inject%20audio%20knowledge%20into%20BERT%20and%0Aswitch%20on%20low-rank%20adaptation%20for%20effective%20adaptation%20when%20audio%20knowledge%20is%0Arequired.%20Our%20experiments%20demonstrate%20that%20AudioBERT%20is%20quite%20effective%2C%0Aachieving%20superior%20performance%20on%20the%20AuditoryBench.%20The%20dataset%20and%20code%20are%0Aavailable%20at%20%5Cbulurl%7Bhttps%3A//github.com/HJ-Ok/AudioBERT%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08199v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAudioBERT%253A%2520Audio%2520Knowledge%2520Augmented%2520Language%2520Model%26entry.906535625%3DHyunjong%2520Ok%2520and%2520Suho%2520Yoo%2520and%2520Jaeho%2520Lee%26entry.1292438233%3D%2520%2520Recent%2520studies%2520have%2520identified%2520that%2520language%2520models%252C%2520pretrained%2520on%2520text-only%250Adatasets%252C%2520often%2520lack%2520elementary%2520visual%2520knowledge%252C%2520%255Ctextit%257Be.g.%252C%257D%2520colors%2520of%250Aeveryday%2520objects.%2520Motivated%2520by%2520this%2520observation%252C%2520we%2520ask%2520whether%2520a%2520similar%250Ashortcoming%2520exists%2520in%2520terms%2520of%2520the%2520%255Ctextit%257Bauditory%257D%2520knowledge.%2520To%2520answer%2520this%250Aquestion%252C%2520we%2520construct%2520a%2520new%2520dataset%2520called%2520AuditoryBench%252C%2520which%2520consists%2520of%250Atwo%2520novel%2520tasks%2520for%2520evaluating%2520auditory%2520knowledge.%2520Based%2520on%2520our%2520analysis%2520using%250Athe%2520benchmark%252C%2520we%2520find%2520that%2520language%2520models%2520also%2520suffer%2520from%2520a%2520severe%2520lack%2520of%250Aauditory%2520knowledge.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520AudioBERT%252C%2520a%2520novel%250Amethod%2520to%2520augment%2520the%2520auditory%2520knowledge%2520of%2520BERT%2520through%2520a%2520retrieval-based%250Aapproach.%2520First%252C%2520we%2520detect%2520auditory%2520knowledge%2520spans%2520in%2520prompts%2520to%2520query%2520our%250Aretrieval%2520model%2520efficiently.%2520Then%252C%2520we%2520inject%2520audio%2520knowledge%2520into%2520BERT%2520and%250Aswitch%2520on%2520low-rank%2520adaptation%2520for%2520effective%2520adaptation%2520when%2520audio%2520knowledge%2520is%250Arequired.%2520Our%2520experiments%2520demonstrate%2520that%2520AudioBERT%2520is%2520quite%2520effective%252C%250Aachieving%2520superior%2520performance%2520on%2520the%2520AuditoryBench.%2520The%2520dataset%2520and%2520code%2520are%250Aavailable%2520at%2520%255Cbulurl%257Bhttps%253A//github.com/HJ-Ok/AudioBERT%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08199v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AudioBERT%3A%20Audio%20Knowledge%20Augmented%20Language%20Model&entry.906535625=Hyunjong%20Ok%20and%20Suho%20Yoo%20and%20Jaeho%20Lee&entry.1292438233=%20%20Recent%20studies%20have%20identified%20that%20language%20models%2C%20pretrained%20on%20text-only%0Adatasets%2C%20often%20lack%20elementary%20visual%20knowledge%2C%20%5Ctextit%7Be.g.%2C%7D%20colors%20of%0Aeveryday%20objects.%20Motivated%20by%20this%20observation%2C%20we%20ask%20whether%20a%20similar%0Ashortcoming%20exists%20in%20terms%20of%20the%20%5Ctextit%7Bauditory%7D%20knowledge.%20To%20answer%20this%0Aquestion%2C%20we%20construct%20a%20new%20dataset%20called%20AuditoryBench%2C%20which%20consists%20of%0Atwo%20novel%20tasks%20for%20evaluating%20auditory%20knowledge.%20Based%20on%20our%20analysis%20using%0Athe%20benchmark%2C%20we%20find%20that%20language%20models%20also%20suffer%20from%20a%20severe%20lack%20of%0Aauditory%20knowledge.%20To%20address%20this%20limitation%2C%20we%20propose%20AudioBERT%2C%20a%20novel%0Amethod%20to%20augment%20the%20auditory%20knowledge%20of%20BERT%20through%20a%20retrieval-based%0Aapproach.%20First%2C%20we%20detect%20auditory%20knowledge%20spans%20in%20prompts%20to%20query%20our%0Aretrieval%20model%20efficiently.%20Then%2C%20we%20inject%20audio%20knowledge%20into%20BERT%20and%0Aswitch%20on%20low-rank%20adaptation%20for%20effective%20adaptation%20when%20audio%20knowledge%20is%0Arequired.%20Our%20experiments%20demonstrate%20that%20AudioBERT%20is%20quite%20effective%2C%0Aachieving%20superior%20performance%20on%20the%20AuditoryBench.%20The%20dataset%20and%20code%20are%0Aavailable%20at%20%5Cbulurl%7Bhttps%3A//github.com/HJ-Ok/AudioBERT%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08199v1&entry.124074799=Read"},
{"title": "SPARK: Self-supervised Personalized Real-time Monocular Face Capture", "author": "Kelian Baert and Shrisha Bharadwaj and Fabien Castan and Benoit Maujean and Marc Christie and Victoria Abrevaya and Adnane Boukhayma", "abstract": "  Feedforward monocular face capture methods seek to reconstruct posed faces\nfrom a single image of a person. Current state of the art approaches have the\nability to regress parametric 3D face models in real-time across a wide range\nof identities, lighting conditions and poses by leveraging large image datasets\nof human faces. These methods however suffer from clear limitations in that the\nunderlying parametric face model only provides a coarse estimation of the face\nshape, thereby limiting their practical applicability in tasks that require\nprecise 3D reconstruction (aging, face swapping, digital make-up, ...). In this\npaper, we propose a method for high-precision 3D face capture taking advantage\nof a collection of unconstrained videos of a subject as prior information. Our\nproposal builds on a two stage approach. We start with the reconstruction of a\ndetailed 3D face avatar of the person, capturing both precise geometry and\nappearance from a collection of videos. We then use the encoder from a\npre-trained monocular face reconstruction method, substituting its decoder with\nour personalized model, and proceed with transfer learning on the video\ncollection. Using our pre-estimated image formation model, we obtain a more\nprecise self-supervision objective, enabling improved expression and pose\nalignment. This results in a trained encoder capable of efficiently regressing\npose and expression parameters in real-time from previously unseen images,\nwhich combined with our personalized geometry model yields more accurate and\nhigh fidelity mesh inference. Through extensive qualitative and quantitative\nevaluation, we showcase the superiority of our final model as compared to\nstate-of-the-art baselines, and demonstrate its generalization ability to\nunseen pose, expression and lighting.\n", "link": "http://arxiv.org/abs/2409.07984v1", "date": "2024-09-12", "relevancy": 2.4284, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6091}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6091}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPARK%3A%20Self-supervised%20Personalized%20Real-time%20Monocular%20Face%20Capture&body=Title%3A%20SPARK%3A%20Self-supervised%20Personalized%20Real-time%20Monocular%20Face%20Capture%0AAuthor%3A%20Kelian%20Baert%20and%20Shrisha%20Bharadwaj%20and%20Fabien%20Castan%20and%20Benoit%20Maujean%20and%20Marc%20Christie%20and%20Victoria%20Abrevaya%20and%20Adnane%20Boukhayma%0AAbstract%3A%20%20%20Feedforward%20monocular%20face%20capture%20methods%20seek%20to%20reconstruct%20posed%20faces%0Afrom%20a%20single%20image%20of%20a%20person.%20Current%20state%20of%20the%20art%20approaches%20have%20the%0Aability%20to%20regress%20parametric%203D%20face%20models%20in%20real-time%20across%20a%20wide%20range%0Aof%20identities%2C%20lighting%20conditions%20and%20poses%20by%20leveraging%20large%20image%20datasets%0Aof%20human%20faces.%20These%20methods%20however%20suffer%20from%20clear%20limitations%20in%20that%20the%0Aunderlying%20parametric%20face%20model%20only%20provides%20a%20coarse%20estimation%20of%20the%20face%0Ashape%2C%20thereby%20limiting%20their%20practical%20applicability%20in%20tasks%20that%20require%0Aprecise%203D%20reconstruction%20%28aging%2C%20face%20swapping%2C%20digital%20make-up%2C%20...%29.%20In%20this%0Apaper%2C%20we%20propose%20a%20method%20for%20high-precision%203D%20face%20capture%20taking%20advantage%0Aof%20a%20collection%20of%20unconstrained%20videos%20of%20a%20subject%20as%20prior%20information.%20Our%0Aproposal%20builds%20on%20a%20two%20stage%20approach.%20We%20start%20with%20the%20reconstruction%20of%20a%0Adetailed%203D%20face%20avatar%20of%20the%20person%2C%20capturing%20both%20precise%20geometry%20and%0Aappearance%20from%20a%20collection%20of%20videos.%20We%20then%20use%20the%20encoder%20from%20a%0Apre-trained%20monocular%20face%20reconstruction%20method%2C%20substituting%20its%20decoder%20with%0Aour%20personalized%20model%2C%20and%20proceed%20with%20transfer%20learning%20on%20the%20video%0Acollection.%20Using%20our%20pre-estimated%20image%20formation%20model%2C%20we%20obtain%20a%20more%0Aprecise%20self-supervision%20objective%2C%20enabling%20improved%20expression%20and%20pose%0Aalignment.%20This%20results%20in%20a%20trained%20encoder%20capable%20of%20efficiently%20regressing%0Apose%20and%20expression%20parameters%20in%20real-time%20from%20previously%20unseen%20images%2C%0Awhich%20combined%20with%20our%20personalized%20geometry%20model%20yields%20more%20accurate%20and%0Ahigh%20fidelity%20mesh%20inference.%20Through%20extensive%20qualitative%20and%20quantitative%0Aevaluation%2C%20we%20showcase%20the%20superiority%20of%20our%20final%20model%20as%20compared%20to%0Astate-of-the-art%20baselines%2C%20and%20demonstrate%20its%20generalization%20ability%20to%0Aunseen%20pose%2C%20expression%20and%20lighting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07984v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPARK%253A%2520Self-supervised%2520Personalized%2520Real-time%2520Monocular%2520Face%2520Capture%26entry.906535625%3DKelian%2520Baert%2520and%2520Shrisha%2520Bharadwaj%2520and%2520Fabien%2520Castan%2520and%2520Benoit%2520Maujean%2520and%2520Marc%2520Christie%2520and%2520Victoria%2520Abrevaya%2520and%2520Adnane%2520Boukhayma%26entry.1292438233%3D%2520%2520Feedforward%2520monocular%2520face%2520capture%2520methods%2520seek%2520to%2520reconstruct%2520posed%2520faces%250Afrom%2520a%2520single%2520image%2520of%2520a%2520person.%2520Current%2520state%2520of%2520the%2520art%2520approaches%2520have%2520the%250Aability%2520to%2520regress%2520parametric%25203D%2520face%2520models%2520in%2520real-time%2520across%2520a%2520wide%2520range%250Aof%2520identities%252C%2520lighting%2520conditions%2520and%2520poses%2520by%2520leveraging%2520large%2520image%2520datasets%250Aof%2520human%2520faces.%2520These%2520methods%2520however%2520suffer%2520from%2520clear%2520limitations%2520in%2520that%2520the%250Aunderlying%2520parametric%2520face%2520model%2520only%2520provides%2520a%2520coarse%2520estimation%2520of%2520the%2520face%250Ashape%252C%2520thereby%2520limiting%2520their%2520practical%2520applicability%2520in%2520tasks%2520that%2520require%250Aprecise%25203D%2520reconstruction%2520%2528aging%252C%2520face%2520swapping%252C%2520digital%2520make-up%252C%2520...%2529.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520method%2520for%2520high-precision%25203D%2520face%2520capture%2520taking%2520advantage%250Aof%2520a%2520collection%2520of%2520unconstrained%2520videos%2520of%2520a%2520subject%2520as%2520prior%2520information.%2520Our%250Aproposal%2520builds%2520on%2520a%2520two%2520stage%2520approach.%2520We%2520start%2520with%2520the%2520reconstruction%2520of%2520a%250Adetailed%25203D%2520face%2520avatar%2520of%2520the%2520person%252C%2520capturing%2520both%2520precise%2520geometry%2520and%250Aappearance%2520from%2520a%2520collection%2520of%2520videos.%2520We%2520then%2520use%2520the%2520encoder%2520from%2520a%250Apre-trained%2520monocular%2520face%2520reconstruction%2520method%252C%2520substituting%2520its%2520decoder%2520with%250Aour%2520personalized%2520model%252C%2520and%2520proceed%2520with%2520transfer%2520learning%2520on%2520the%2520video%250Acollection.%2520Using%2520our%2520pre-estimated%2520image%2520formation%2520model%252C%2520we%2520obtain%2520a%2520more%250Aprecise%2520self-supervision%2520objective%252C%2520enabling%2520improved%2520expression%2520and%2520pose%250Aalignment.%2520This%2520results%2520in%2520a%2520trained%2520encoder%2520capable%2520of%2520efficiently%2520regressing%250Apose%2520and%2520expression%2520parameters%2520in%2520real-time%2520from%2520previously%2520unseen%2520images%252C%250Awhich%2520combined%2520with%2520our%2520personalized%2520geometry%2520model%2520yields%2520more%2520accurate%2520and%250Ahigh%2520fidelity%2520mesh%2520inference.%2520Through%2520extensive%2520qualitative%2520and%2520quantitative%250Aevaluation%252C%2520we%2520showcase%2520the%2520superiority%2520of%2520our%2520final%2520model%2520as%2520compared%2520to%250Astate-of-the-art%2520baselines%252C%2520and%2520demonstrate%2520its%2520generalization%2520ability%2520to%250Aunseen%2520pose%252C%2520expression%2520and%2520lighting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07984v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPARK%3A%20Self-supervised%20Personalized%20Real-time%20Monocular%20Face%20Capture&entry.906535625=Kelian%20Baert%20and%20Shrisha%20Bharadwaj%20and%20Fabien%20Castan%20and%20Benoit%20Maujean%20and%20Marc%20Christie%20and%20Victoria%20Abrevaya%20and%20Adnane%20Boukhayma&entry.1292438233=%20%20Feedforward%20monocular%20face%20capture%20methods%20seek%20to%20reconstruct%20posed%20faces%0Afrom%20a%20single%20image%20of%20a%20person.%20Current%20state%20of%20the%20art%20approaches%20have%20the%0Aability%20to%20regress%20parametric%203D%20face%20models%20in%20real-time%20across%20a%20wide%20range%0Aof%20identities%2C%20lighting%20conditions%20and%20poses%20by%20leveraging%20large%20image%20datasets%0Aof%20human%20faces.%20These%20methods%20however%20suffer%20from%20clear%20limitations%20in%20that%20the%0Aunderlying%20parametric%20face%20model%20only%20provides%20a%20coarse%20estimation%20of%20the%20face%0Ashape%2C%20thereby%20limiting%20their%20practical%20applicability%20in%20tasks%20that%20require%0Aprecise%203D%20reconstruction%20%28aging%2C%20face%20swapping%2C%20digital%20make-up%2C%20...%29.%20In%20this%0Apaper%2C%20we%20propose%20a%20method%20for%20high-precision%203D%20face%20capture%20taking%20advantage%0Aof%20a%20collection%20of%20unconstrained%20videos%20of%20a%20subject%20as%20prior%20information.%20Our%0Aproposal%20builds%20on%20a%20two%20stage%20approach.%20We%20start%20with%20the%20reconstruction%20of%20a%0Adetailed%203D%20face%20avatar%20of%20the%20person%2C%20capturing%20both%20precise%20geometry%20and%0Aappearance%20from%20a%20collection%20of%20videos.%20We%20then%20use%20the%20encoder%20from%20a%0Apre-trained%20monocular%20face%20reconstruction%20method%2C%20substituting%20its%20decoder%20with%0Aour%20personalized%20model%2C%20and%20proceed%20with%20transfer%20learning%20on%20the%20video%0Acollection.%20Using%20our%20pre-estimated%20image%20formation%20model%2C%20we%20obtain%20a%20more%0Aprecise%20self-supervision%20objective%2C%20enabling%20improved%20expression%20and%20pose%0Aalignment.%20This%20results%20in%20a%20trained%20encoder%20capable%20of%20efficiently%20regressing%0Apose%20and%20expression%20parameters%20in%20real-time%20from%20previously%20unseen%20images%2C%0Awhich%20combined%20with%20our%20personalized%20geometry%20model%20yields%20more%20accurate%20and%0Ahigh%20fidelity%20mesh%20inference.%20Through%20extensive%20qualitative%20and%20quantitative%0Aevaluation%2C%20we%20showcase%20the%20superiority%20of%20our%20final%20model%20as%20compared%20to%0Astate-of-the-art%20baselines%2C%20and%20demonstrate%20its%20generalization%20ability%20to%0Aunseen%20pose%2C%20expression%20and%20lighting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07984v1&entry.124074799=Read"},
{"title": "Enhanced Generative Data Augmentation for Semantic Segmentation via\n  Stronger Guidance", "author": "Quang-Huy Che and Duc-Tri Le and Vinh-Tiep Nguyen", "abstract": "  Data augmentation is a widely used technique for creating training data for\ntasks that require labeled data, such as semantic segmentation. This method\nbenefits pixel-wise annotation tasks requiring much effort and intensive labor.\nTraditional data augmentation methods involve simple transformations like\nrotations and flips to create new images from existing ones. However, these new\nimages may lack diversity along the main semantic axes in the data and not\nchange high-level semantic properties. To address this issue, generative models\nhave emerged as an effective solution for augmenting data by generating\nsynthetic images. Controllable generative models offer a way to augment data\nfor semantic segmentation tasks using a prompt and visual reference from the\noriginal image. However, using these models directly presents challenges, such\nas creating an effective prompt and visual reference to generate a synthetic\nimage that accurately reflects the content and structure of the original. In\nthis work, we introduce an effective data augmentation method for semantic\nsegmentation using the Controllable Diffusion Model. Our proposed method\nincludes efficient prompt generation using Class-Prompt Appending and Visual\nPrior Combination to enhance attention to labeled classes in real images. These\ntechniques allow us to generate images that accurately depict segmented classes\nin the real image. In addition, we employ the class balancing algorithm to\nensure efficiency when merging the synthetic and original images to generate\nbalanced data for the training dataset. We evaluated our method on the PASCAL\nVOC datasets and found it highly effective for synthesizing images in semantic\nsegmentation.\n", "link": "http://arxiv.org/abs/2409.06002v2", "date": "2024-09-12", "relevancy": 2.4238, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6255}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6034}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Generative%20Data%20Augmentation%20for%20Semantic%20Segmentation%20via%0A%20%20Stronger%20Guidance&body=Title%3A%20Enhanced%20Generative%20Data%20Augmentation%20for%20Semantic%20Segmentation%20via%0A%20%20Stronger%20Guidance%0AAuthor%3A%20Quang-Huy%20Che%20and%20Duc-Tri%20Le%20and%20Vinh-Tiep%20Nguyen%0AAbstract%3A%20%20%20Data%20augmentation%20is%20a%20widely%20used%20technique%20for%20creating%20training%20data%20for%0Atasks%20that%20require%20labeled%20data%2C%20such%20as%20semantic%20segmentation.%20This%20method%0Abenefits%20pixel-wise%20annotation%20tasks%20requiring%20much%20effort%20and%20intensive%20labor.%0ATraditional%20data%20augmentation%20methods%20involve%20simple%20transformations%20like%0Arotations%20and%20flips%20to%20create%20new%20images%20from%20existing%20ones.%20However%2C%20these%20new%0Aimages%20may%20lack%20diversity%20along%20the%20main%20semantic%20axes%20in%20the%20data%20and%20not%0Achange%20high-level%20semantic%20properties.%20To%20address%20this%20issue%2C%20generative%20models%0Ahave%20emerged%20as%20an%20effective%20solution%20for%20augmenting%20data%20by%20generating%0Asynthetic%20images.%20Controllable%20generative%20models%20offer%20a%20way%20to%20augment%20data%0Afor%20semantic%20segmentation%20tasks%20using%20a%20prompt%20and%20visual%20reference%20from%20the%0Aoriginal%20image.%20However%2C%20using%20these%20models%20directly%20presents%20challenges%2C%20such%0Aas%20creating%20an%20effective%20prompt%20and%20visual%20reference%20to%20generate%20a%20synthetic%0Aimage%20that%20accurately%20reflects%20the%20content%20and%20structure%20of%20the%20original.%20In%0Athis%20work%2C%20we%20introduce%20an%20effective%20data%20augmentation%20method%20for%20semantic%0Asegmentation%20using%20the%20Controllable%20Diffusion%20Model.%20Our%20proposed%20method%0Aincludes%20efficient%20prompt%20generation%20using%20Class-Prompt%20Appending%20and%20Visual%0APrior%20Combination%20to%20enhance%20attention%20to%20labeled%20classes%20in%20real%20images.%20These%0Atechniques%20allow%20us%20to%20generate%20images%20that%20accurately%20depict%20segmented%20classes%0Ain%20the%20real%20image.%20In%20addition%2C%20we%20employ%20the%20class%20balancing%20algorithm%20to%0Aensure%20efficiency%20when%20merging%20the%20synthetic%20and%20original%20images%20to%20generate%0Abalanced%20data%20for%20the%20training%20dataset.%20We%20evaluated%20our%20method%20on%20the%20PASCAL%0AVOC%20datasets%20and%20found%20it%20highly%20effective%20for%20synthesizing%20images%20in%20semantic%0Asegmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06002v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Generative%2520Data%2520Augmentation%2520for%2520Semantic%2520Segmentation%2520via%250A%2520%2520Stronger%2520Guidance%26entry.906535625%3DQuang-Huy%2520Che%2520and%2520Duc-Tri%2520Le%2520and%2520Vinh-Tiep%2520Nguyen%26entry.1292438233%3D%2520%2520Data%2520augmentation%2520is%2520a%2520widely%2520used%2520technique%2520for%2520creating%2520training%2520data%2520for%250Atasks%2520that%2520require%2520labeled%2520data%252C%2520such%2520as%2520semantic%2520segmentation.%2520This%2520method%250Abenefits%2520pixel-wise%2520annotation%2520tasks%2520requiring%2520much%2520effort%2520and%2520intensive%2520labor.%250ATraditional%2520data%2520augmentation%2520methods%2520involve%2520simple%2520transformations%2520like%250Arotations%2520and%2520flips%2520to%2520create%2520new%2520images%2520from%2520existing%2520ones.%2520However%252C%2520these%2520new%250Aimages%2520may%2520lack%2520diversity%2520along%2520the%2520main%2520semantic%2520axes%2520in%2520the%2520data%2520and%2520not%250Achange%2520high-level%2520semantic%2520properties.%2520To%2520address%2520this%2520issue%252C%2520generative%2520models%250Ahave%2520emerged%2520as%2520an%2520effective%2520solution%2520for%2520augmenting%2520data%2520by%2520generating%250Asynthetic%2520images.%2520Controllable%2520generative%2520models%2520offer%2520a%2520way%2520to%2520augment%2520data%250Afor%2520semantic%2520segmentation%2520tasks%2520using%2520a%2520prompt%2520and%2520visual%2520reference%2520from%2520the%250Aoriginal%2520image.%2520However%252C%2520using%2520these%2520models%2520directly%2520presents%2520challenges%252C%2520such%250Aas%2520creating%2520an%2520effective%2520prompt%2520and%2520visual%2520reference%2520to%2520generate%2520a%2520synthetic%250Aimage%2520that%2520accurately%2520reflects%2520the%2520content%2520and%2520structure%2520of%2520the%2520original.%2520In%250Athis%2520work%252C%2520we%2520introduce%2520an%2520effective%2520data%2520augmentation%2520method%2520for%2520semantic%250Asegmentation%2520using%2520the%2520Controllable%2520Diffusion%2520Model.%2520Our%2520proposed%2520method%250Aincludes%2520efficient%2520prompt%2520generation%2520using%2520Class-Prompt%2520Appending%2520and%2520Visual%250APrior%2520Combination%2520to%2520enhance%2520attention%2520to%2520labeled%2520classes%2520in%2520real%2520images.%2520These%250Atechniques%2520allow%2520us%2520to%2520generate%2520images%2520that%2520accurately%2520depict%2520segmented%2520classes%250Ain%2520the%2520real%2520image.%2520In%2520addition%252C%2520we%2520employ%2520the%2520class%2520balancing%2520algorithm%2520to%250Aensure%2520efficiency%2520when%2520merging%2520the%2520synthetic%2520and%2520original%2520images%2520to%2520generate%250Abalanced%2520data%2520for%2520the%2520training%2520dataset.%2520We%2520evaluated%2520our%2520method%2520on%2520the%2520PASCAL%250AVOC%2520datasets%2520and%2520found%2520it%2520highly%2520effective%2520for%2520synthesizing%2520images%2520in%2520semantic%250Asegmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06002v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Generative%20Data%20Augmentation%20for%20Semantic%20Segmentation%20via%0A%20%20Stronger%20Guidance&entry.906535625=Quang-Huy%20Che%20and%20Duc-Tri%20Le%20and%20Vinh-Tiep%20Nguyen&entry.1292438233=%20%20Data%20augmentation%20is%20a%20widely%20used%20technique%20for%20creating%20training%20data%20for%0Atasks%20that%20require%20labeled%20data%2C%20such%20as%20semantic%20segmentation.%20This%20method%0Abenefits%20pixel-wise%20annotation%20tasks%20requiring%20much%20effort%20and%20intensive%20labor.%0ATraditional%20data%20augmentation%20methods%20involve%20simple%20transformations%20like%0Arotations%20and%20flips%20to%20create%20new%20images%20from%20existing%20ones.%20However%2C%20these%20new%0Aimages%20may%20lack%20diversity%20along%20the%20main%20semantic%20axes%20in%20the%20data%20and%20not%0Achange%20high-level%20semantic%20properties.%20To%20address%20this%20issue%2C%20generative%20models%0Ahave%20emerged%20as%20an%20effective%20solution%20for%20augmenting%20data%20by%20generating%0Asynthetic%20images.%20Controllable%20generative%20models%20offer%20a%20way%20to%20augment%20data%0Afor%20semantic%20segmentation%20tasks%20using%20a%20prompt%20and%20visual%20reference%20from%20the%0Aoriginal%20image.%20However%2C%20using%20these%20models%20directly%20presents%20challenges%2C%20such%0Aas%20creating%20an%20effective%20prompt%20and%20visual%20reference%20to%20generate%20a%20synthetic%0Aimage%20that%20accurately%20reflects%20the%20content%20and%20structure%20of%20the%20original.%20In%0Athis%20work%2C%20we%20introduce%20an%20effective%20data%20augmentation%20method%20for%20semantic%0Asegmentation%20using%20the%20Controllable%20Diffusion%20Model.%20Our%20proposed%20method%0Aincludes%20efficient%20prompt%20generation%20using%20Class-Prompt%20Appending%20and%20Visual%0APrior%20Combination%20to%20enhance%20attention%20to%20labeled%20classes%20in%20real%20images.%20These%0Atechniques%20allow%20us%20to%20generate%20images%20that%20accurately%20depict%20segmented%20classes%0Ain%20the%20real%20image.%20In%20addition%2C%20we%20employ%20the%20class%20balancing%20algorithm%20to%0Aensure%20efficiency%20when%20merging%20the%20synthetic%20and%20original%20images%20to%20generate%0Abalanced%20data%20for%20the%20training%20dataset.%20We%20evaluated%20our%20method%20on%20the%20PASCAL%0AVOC%20datasets%20and%20found%20it%20highly%20effective%20for%20synthesizing%20images%20in%20semantic%0Asegmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06002v2&entry.124074799=Read"},
{"title": "Learning Video Context as Interleaved Multimodal Sequences", "author": "Kevin Qinghong Lin and Pengchuan Zhang and Difei Gao and Xide Xia and Joya Chen and Ziteng Gao and Jinheng Xie and Xuhong Xiao and Mike Zheng Shou", "abstract": "  Narrative videos, such as movies, pose significant challenges in video\nunderstanding due to their rich contexts (characters, dialogues, storylines)\nand diverse demands (identify who, relationship, and reason). In this paper, we\nintroduce MovieSeq, a multimodal language model developed to address the wide\nrange of challenges in understanding video contexts. Our core idea is to\nrepresent videos as interleaved multimodal sequences (including images, plots,\nvideos, and subtitles), either by linking external knowledge databases or using\noffline models (such as whisper for subtitles). Through instruction-tuning,\nthis approach empowers the language model to interact with videos using\ninterleaved multimodal instructions. For example, instead of solely relying on\nvideo as input, we jointly provide character photos alongside their names and\ndialogues, allowing the model to associate these elements and generate more\ncomprehensive responses. To demonstrate its effectiveness, we validate\nMovieSeq's performance on six datasets (LVU, MAD, Movienet, CMD, TVC, MovieQA)\nacross five settings (video classification, audio description, video-text\nretrieval, video captioning, and video question-answering). The code will be\npublic at https://github.com/showlab/MovieSeq.\n", "link": "http://arxiv.org/abs/2407.21757v2", "date": "2024-09-12", "relevancy": 2.4114, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6113}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6113}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Video%20Context%20as%20Interleaved%20Multimodal%20Sequences&body=Title%3A%20Learning%20Video%20Context%20as%20Interleaved%20Multimodal%20Sequences%0AAuthor%3A%20Kevin%20Qinghong%20Lin%20and%20Pengchuan%20Zhang%20and%20Difei%20Gao%20and%20Xide%20Xia%20and%20Joya%20Chen%20and%20Ziteng%20Gao%20and%20Jinheng%20Xie%20and%20Xuhong%20Xiao%20and%20Mike%20Zheng%20Shou%0AAbstract%3A%20%20%20Narrative%20videos%2C%20such%20as%20movies%2C%20pose%20significant%20challenges%20in%20video%0Aunderstanding%20due%20to%20their%20rich%20contexts%20%28characters%2C%20dialogues%2C%20storylines%29%0Aand%20diverse%20demands%20%28identify%20who%2C%20relationship%2C%20and%20reason%29.%20In%20this%20paper%2C%20we%0Aintroduce%20MovieSeq%2C%20a%20multimodal%20language%20model%20developed%20to%20address%20the%20wide%0Arange%20of%20challenges%20in%20understanding%20video%20contexts.%20Our%20core%20idea%20is%20to%0Arepresent%20videos%20as%20interleaved%20multimodal%20sequences%20%28including%20images%2C%20plots%2C%0Avideos%2C%20and%20subtitles%29%2C%20either%20by%20linking%20external%20knowledge%20databases%20or%20using%0Aoffline%20models%20%28such%20as%20whisper%20for%20subtitles%29.%20Through%20instruction-tuning%2C%0Athis%20approach%20empowers%20the%20language%20model%20to%20interact%20with%20videos%20using%0Ainterleaved%20multimodal%20instructions.%20For%20example%2C%20instead%20of%20solely%20relying%20on%0Avideo%20as%20input%2C%20we%20jointly%20provide%20character%20photos%20alongside%20their%20names%20and%0Adialogues%2C%20allowing%20the%20model%20to%20associate%20these%20elements%20and%20generate%20more%0Acomprehensive%20responses.%20To%20demonstrate%20its%20effectiveness%2C%20we%20validate%0AMovieSeq%27s%20performance%20on%20six%20datasets%20%28LVU%2C%20MAD%2C%20Movienet%2C%20CMD%2C%20TVC%2C%20MovieQA%29%0Aacross%20five%20settings%20%28video%20classification%2C%20audio%20description%2C%20video-text%0Aretrieval%2C%20video%20captioning%2C%20and%20video%20question-answering%29.%20The%20code%20will%20be%0Apublic%20at%20https%3A//github.com/showlab/MovieSeq.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21757v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Video%2520Context%2520as%2520Interleaved%2520Multimodal%2520Sequences%26entry.906535625%3DKevin%2520Qinghong%2520Lin%2520and%2520Pengchuan%2520Zhang%2520and%2520Difei%2520Gao%2520and%2520Xide%2520Xia%2520and%2520Joya%2520Chen%2520and%2520Ziteng%2520Gao%2520and%2520Jinheng%2520Xie%2520and%2520Xuhong%2520Xiao%2520and%2520Mike%2520Zheng%2520Shou%26entry.1292438233%3D%2520%2520Narrative%2520videos%252C%2520such%2520as%2520movies%252C%2520pose%2520significant%2520challenges%2520in%2520video%250Aunderstanding%2520due%2520to%2520their%2520rich%2520contexts%2520%2528characters%252C%2520dialogues%252C%2520storylines%2529%250Aand%2520diverse%2520demands%2520%2528identify%2520who%252C%2520relationship%252C%2520and%2520reason%2529.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520MovieSeq%252C%2520a%2520multimodal%2520language%2520model%2520developed%2520to%2520address%2520the%2520wide%250Arange%2520of%2520challenges%2520in%2520understanding%2520video%2520contexts.%2520Our%2520core%2520idea%2520is%2520to%250Arepresent%2520videos%2520as%2520interleaved%2520multimodal%2520sequences%2520%2528including%2520images%252C%2520plots%252C%250Avideos%252C%2520and%2520subtitles%2529%252C%2520either%2520by%2520linking%2520external%2520knowledge%2520databases%2520or%2520using%250Aoffline%2520models%2520%2528such%2520as%2520whisper%2520for%2520subtitles%2529.%2520Through%2520instruction-tuning%252C%250Athis%2520approach%2520empowers%2520the%2520language%2520model%2520to%2520interact%2520with%2520videos%2520using%250Ainterleaved%2520multimodal%2520instructions.%2520For%2520example%252C%2520instead%2520of%2520solely%2520relying%2520on%250Avideo%2520as%2520input%252C%2520we%2520jointly%2520provide%2520character%2520photos%2520alongside%2520their%2520names%2520and%250Adialogues%252C%2520allowing%2520the%2520model%2520to%2520associate%2520these%2520elements%2520and%2520generate%2520more%250Acomprehensive%2520responses.%2520To%2520demonstrate%2520its%2520effectiveness%252C%2520we%2520validate%250AMovieSeq%2527s%2520performance%2520on%2520six%2520datasets%2520%2528LVU%252C%2520MAD%252C%2520Movienet%252C%2520CMD%252C%2520TVC%252C%2520MovieQA%2529%250Aacross%2520five%2520settings%2520%2528video%2520classification%252C%2520audio%2520description%252C%2520video-text%250Aretrieval%252C%2520video%2520captioning%252C%2520and%2520video%2520question-answering%2529.%2520The%2520code%2520will%2520be%250Apublic%2520at%2520https%253A//github.com/showlab/MovieSeq.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21757v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Video%20Context%20as%20Interleaved%20Multimodal%20Sequences&entry.906535625=Kevin%20Qinghong%20Lin%20and%20Pengchuan%20Zhang%20and%20Difei%20Gao%20and%20Xide%20Xia%20and%20Joya%20Chen%20and%20Ziteng%20Gao%20and%20Jinheng%20Xie%20and%20Xuhong%20Xiao%20and%20Mike%20Zheng%20Shou&entry.1292438233=%20%20Narrative%20videos%2C%20such%20as%20movies%2C%20pose%20significant%20challenges%20in%20video%0Aunderstanding%20due%20to%20their%20rich%20contexts%20%28characters%2C%20dialogues%2C%20storylines%29%0Aand%20diverse%20demands%20%28identify%20who%2C%20relationship%2C%20and%20reason%29.%20In%20this%20paper%2C%20we%0Aintroduce%20MovieSeq%2C%20a%20multimodal%20language%20model%20developed%20to%20address%20the%20wide%0Arange%20of%20challenges%20in%20understanding%20video%20contexts.%20Our%20core%20idea%20is%20to%0Arepresent%20videos%20as%20interleaved%20multimodal%20sequences%20%28including%20images%2C%20plots%2C%0Avideos%2C%20and%20subtitles%29%2C%20either%20by%20linking%20external%20knowledge%20databases%20or%20using%0Aoffline%20models%20%28such%20as%20whisper%20for%20subtitles%29.%20Through%20instruction-tuning%2C%0Athis%20approach%20empowers%20the%20language%20model%20to%20interact%20with%20videos%20using%0Ainterleaved%20multimodal%20instructions.%20For%20example%2C%20instead%20of%20solely%20relying%20on%0Avideo%20as%20input%2C%20we%20jointly%20provide%20character%20photos%20alongside%20their%20names%20and%0Adialogues%2C%20allowing%20the%20model%20to%20associate%20these%20elements%20and%20generate%20more%0Acomprehensive%20responses.%20To%20demonstrate%20its%20effectiveness%2C%20we%20validate%0AMovieSeq%27s%20performance%20on%20six%20datasets%20%28LVU%2C%20MAD%2C%20Movienet%2C%20CMD%2C%20TVC%2C%20MovieQA%29%0Aacross%20five%20settings%20%28video%20classification%2C%20audio%20description%2C%20video-text%0Aretrieval%2C%20video%20captioning%2C%20and%20video%20question-answering%29.%20The%20code%20will%20be%0Apublic%20at%20https%3A//github.com/showlab/MovieSeq.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21757v2&entry.124074799=Read"},
{"title": "Heterogeneous Sheaf Neural Networks", "author": "Luke Braithwaite and Iulia Duta and Pietro Li\u00f2", "abstract": "  Heterogeneous graphs, with nodes and edges of different types, are commonly\nused to model relational structures in many real-world applications. Standard\nGraph Neural Networks (GNNs) struggle to process heterogeneous data due to\noversmoothing. Instead, current approaches have focused on accounting for the\nheterogeneity in the model architecture, leading to increasingly complex\nmodels. Inspired by recent work, we propose using cellular sheaves to model the\nheterogeneity in the graph's underlying topology. Instead of modelling the data\nas a graph, we represent it as cellular sheaves, which allows us to encode the\ndifferent data types directly in the data structure, eliminating the need to\ninject them into the architecture. We introduce HetSheaf, a general framework\nfor heterogeneous sheaf neural networks, and a series of heterogeneous sheaf\npredictors to better encode the data's heterogeneity into the sheaf structure.\nFinally, we empirically evaluate HetSheaf on several standard heterogeneous\ngraph benchmarks, achieving competitive results whilst being more\nparameter-efficient.\n", "link": "http://arxiv.org/abs/2409.08036v1", "date": "2024-09-12", "relevancy": 2.4076, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5247}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.467}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Heterogeneous%20Sheaf%20Neural%20Networks&body=Title%3A%20Heterogeneous%20Sheaf%20Neural%20Networks%0AAuthor%3A%20Luke%20Braithwaite%20and%20Iulia%20Duta%20and%20Pietro%20Li%C3%B2%0AAbstract%3A%20%20%20Heterogeneous%20graphs%2C%20with%20nodes%20and%20edges%20of%20different%20types%2C%20are%20commonly%0Aused%20to%20model%20relational%20structures%20in%20many%20real-world%20applications.%20Standard%0AGraph%20Neural%20Networks%20%28GNNs%29%20struggle%20to%20process%20heterogeneous%20data%20due%20to%0Aoversmoothing.%20Instead%2C%20current%20approaches%20have%20focused%20on%20accounting%20for%20the%0Aheterogeneity%20in%20the%20model%20architecture%2C%20leading%20to%20increasingly%20complex%0Amodels.%20Inspired%20by%20recent%20work%2C%20we%20propose%20using%20cellular%20sheaves%20to%20model%20the%0Aheterogeneity%20in%20the%20graph%27s%20underlying%20topology.%20Instead%20of%20modelling%20the%20data%0Aas%20a%20graph%2C%20we%20represent%20it%20as%20cellular%20sheaves%2C%20which%20allows%20us%20to%20encode%20the%0Adifferent%20data%20types%20directly%20in%20the%20data%20structure%2C%20eliminating%20the%20need%20to%0Ainject%20them%20into%20the%20architecture.%20We%20introduce%20HetSheaf%2C%20a%20general%20framework%0Afor%20heterogeneous%20sheaf%20neural%20networks%2C%20and%20a%20series%20of%20heterogeneous%20sheaf%0Apredictors%20to%20better%20encode%20the%20data%27s%20heterogeneity%20into%20the%20sheaf%20structure.%0AFinally%2C%20we%20empirically%20evaluate%20HetSheaf%20on%20several%20standard%20heterogeneous%0Agraph%20benchmarks%2C%20achieving%20competitive%20results%20whilst%20being%20more%0Aparameter-efficient.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08036v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeterogeneous%2520Sheaf%2520Neural%2520Networks%26entry.906535625%3DLuke%2520Braithwaite%2520and%2520Iulia%2520Duta%2520and%2520Pietro%2520Li%25C3%25B2%26entry.1292438233%3D%2520%2520Heterogeneous%2520graphs%252C%2520with%2520nodes%2520and%2520edges%2520of%2520different%2520types%252C%2520are%2520commonly%250Aused%2520to%2520model%2520relational%2520structures%2520in%2520many%2520real-world%2520applications.%2520Standard%250AGraph%2520Neural%2520Networks%2520%2528GNNs%2529%2520struggle%2520to%2520process%2520heterogeneous%2520data%2520due%2520to%250Aoversmoothing.%2520Instead%252C%2520current%2520approaches%2520have%2520focused%2520on%2520accounting%2520for%2520the%250Aheterogeneity%2520in%2520the%2520model%2520architecture%252C%2520leading%2520to%2520increasingly%2520complex%250Amodels.%2520Inspired%2520by%2520recent%2520work%252C%2520we%2520propose%2520using%2520cellular%2520sheaves%2520to%2520model%2520the%250Aheterogeneity%2520in%2520the%2520graph%2527s%2520underlying%2520topology.%2520Instead%2520of%2520modelling%2520the%2520data%250Aas%2520a%2520graph%252C%2520we%2520represent%2520it%2520as%2520cellular%2520sheaves%252C%2520which%2520allows%2520us%2520to%2520encode%2520the%250Adifferent%2520data%2520types%2520directly%2520in%2520the%2520data%2520structure%252C%2520eliminating%2520the%2520need%2520to%250Ainject%2520them%2520into%2520the%2520architecture.%2520We%2520introduce%2520HetSheaf%252C%2520a%2520general%2520framework%250Afor%2520heterogeneous%2520sheaf%2520neural%2520networks%252C%2520and%2520a%2520series%2520of%2520heterogeneous%2520sheaf%250Apredictors%2520to%2520better%2520encode%2520the%2520data%2527s%2520heterogeneity%2520into%2520the%2520sheaf%2520structure.%250AFinally%252C%2520we%2520empirically%2520evaluate%2520HetSheaf%2520on%2520several%2520standard%2520heterogeneous%250Agraph%2520benchmarks%252C%2520achieving%2520competitive%2520results%2520whilst%2520being%2520more%250Aparameter-efficient.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08036v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Heterogeneous%20Sheaf%20Neural%20Networks&entry.906535625=Luke%20Braithwaite%20and%20Iulia%20Duta%20and%20Pietro%20Li%C3%B2&entry.1292438233=%20%20Heterogeneous%20graphs%2C%20with%20nodes%20and%20edges%20of%20different%20types%2C%20are%20commonly%0Aused%20to%20model%20relational%20structures%20in%20many%20real-world%20applications.%20Standard%0AGraph%20Neural%20Networks%20%28GNNs%29%20struggle%20to%20process%20heterogeneous%20data%20due%20to%0Aoversmoothing.%20Instead%2C%20current%20approaches%20have%20focused%20on%20accounting%20for%20the%0Aheterogeneity%20in%20the%20model%20architecture%2C%20leading%20to%20increasingly%20complex%0Amodels.%20Inspired%20by%20recent%20work%2C%20we%20propose%20using%20cellular%20sheaves%20to%20model%20the%0Aheterogeneity%20in%20the%20graph%27s%20underlying%20topology.%20Instead%20of%20modelling%20the%20data%0Aas%20a%20graph%2C%20we%20represent%20it%20as%20cellular%20sheaves%2C%20which%20allows%20us%20to%20encode%20the%0Adifferent%20data%20types%20directly%20in%20the%20data%20structure%2C%20eliminating%20the%20need%20to%0Ainject%20them%20into%20the%20architecture.%20We%20introduce%20HetSheaf%2C%20a%20general%20framework%0Afor%20heterogeneous%20sheaf%20neural%20networks%2C%20and%20a%20series%20of%20heterogeneous%20sheaf%0Apredictors%20to%20better%20encode%20the%20data%27s%20heterogeneity%20into%20the%20sheaf%20structure.%0AFinally%2C%20we%20empirically%20evaluate%20HetSheaf%20on%20several%20standard%20heterogeneous%0Agraph%20benchmarks%2C%20achieving%20competitive%20results%20whilst%20being%20more%0Aparameter-efficient.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08036v1&entry.124074799=Read"},
{"title": "DreamBeast: Distilling 3D Fantastical Animals with Part-Aware Knowledge\n  Transfer", "author": "Runjia Li and Junlin Han and Luke Melas-Kyriazi and Chunyi Sun and Zhaochong An and Zhongrui Gui and Shuyang Sun and Philip Torr and Tomas Jakab", "abstract": "  We present DreamBeast, a novel method based on score distillation sampling\n(SDS) for generating fantastical 3D animal assets composed of distinct parts.\nExisting SDS methods often struggle with this generation task due to a limited\nunderstanding of part-level semantics in text-to-image diffusion models. While\nrecent diffusion models, such as Stable Diffusion 3, demonstrate a better\npart-level understanding, they are prohibitively slow and exhibit other common\nproblems associated with single-view diffusion models. DreamBeast overcomes\nthis limitation through a novel part-aware knowledge transfer mechanism. For\neach generated asset, we efficiently extract part-level knowledge from the\nStable Diffusion 3 model into a 3D Part-Affinity implicit representation. This\nenables us to instantly generate Part-Affinity maps from arbitrary camera\nviews, which we then use to modulate the guidance of a multi-view diffusion\nmodel during SDS to create 3D assets of fantastical animals. DreamBeast\nsignificantly enhances the quality of generated 3D creatures with\nuser-specified part compositions while reducing computational overhead, as\ndemonstrated by extensive quantitative and qualitative evaluations.\n", "link": "http://arxiv.org/abs/2409.08271v1", "date": "2024-09-12", "relevancy": 2.3818, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5976}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5976}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5846}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamBeast%3A%20Distilling%203D%20Fantastical%20Animals%20with%20Part-Aware%20Knowledge%0A%20%20Transfer&body=Title%3A%20DreamBeast%3A%20Distilling%203D%20Fantastical%20Animals%20with%20Part-Aware%20Knowledge%0A%20%20Transfer%0AAuthor%3A%20Runjia%20Li%20and%20Junlin%20Han%20and%20Luke%20Melas-Kyriazi%20and%20Chunyi%20Sun%20and%20Zhaochong%20An%20and%20Zhongrui%20Gui%20and%20Shuyang%20Sun%20and%20Philip%20Torr%20and%20Tomas%20Jakab%0AAbstract%3A%20%20%20We%20present%20DreamBeast%2C%20a%20novel%20method%20based%20on%20score%20distillation%20sampling%0A%28SDS%29%20for%20generating%20fantastical%203D%20animal%20assets%20composed%20of%20distinct%20parts.%0AExisting%20SDS%20methods%20often%20struggle%20with%20this%20generation%20task%20due%20to%20a%20limited%0Aunderstanding%20of%20part-level%20semantics%20in%20text-to-image%20diffusion%20models.%20While%0Arecent%20diffusion%20models%2C%20such%20as%20Stable%20Diffusion%203%2C%20demonstrate%20a%20better%0Apart-level%20understanding%2C%20they%20are%20prohibitively%20slow%20and%20exhibit%20other%20common%0Aproblems%20associated%20with%20single-view%20diffusion%20models.%20DreamBeast%20overcomes%0Athis%20limitation%20through%20a%20novel%20part-aware%20knowledge%20transfer%20mechanism.%20For%0Aeach%20generated%20asset%2C%20we%20efficiently%20extract%20part-level%20knowledge%20from%20the%0AStable%20Diffusion%203%20model%20into%20a%203D%20Part-Affinity%20implicit%20representation.%20This%0Aenables%20us%20to%20instantly%20generate%20Part-Affinity%20maps%20from%20arbitrary%20camera%0Aviews%2C%20which%20we%20then%20use%20to%20modulate%20the%20guidance%20of%20a%20multi-view%20diffusion%0Amodel%20during%20SDS%20to%20create%203D%20assets%20of%20fantastical%20animals.%20DreamBeast%0Asignificantly%20enhances%20the%20quality%20of%20generated%203D%20creatures%20with%0Auser-specified%20part%20compositions%20while%20reducing%20computational%20overhead%2C%20as%0Ademonstrated%20by%20extensive%20quantitative%20and%20qualitative%20evaluations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08271v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamBeast%253A%2520Distilling%25203D%2520Fantastical%2520Animals%2520with%2520Part-Aware%2520Knowledge%250A%2520%2520Transfer%26entry.906535625%3DRunjia%2520Li%2520and%2520Junlin%2520Han%2520and%2520Luke%2520Melas-Kyriazi%2520and%2520Chunyi%2520Sun%2520and%2520Zhaochong%2520An%2520and%2520Zhongrui%2520Gui%2520and%2520Shuyang%2520Sun%2520and%2520Philip%2520Torr%2520and%2520Tomas%2520Jakab%26entry.1292438233%3D%2520%2520We%2520present%2520DreamBeast%252C%2520a%2520novel%2520method%2520based%2520on%2520score%2520distillation%2520sampling%250A%2528SDS%2529%2520for%2520generating%2520fantastical%25203D%2520animal%2520assets%2520composed%2520of%2520distinct%2520parts.%250AExisting%2520SDS%2520methods%2520often%2520struggle%2520with%2520this%2520generation%2520task%2520due%2520to%2520a%2520limited%250Aunderstanding%2520of%2520part-level%2520semantics%2520in%2520text-to-image%2520diffusion%2520models.%2520While%250Arecent%2520diffusion%2520models%252C%2520such%2520as%2520Stable%2520Diffusion%25203%252C%2520demonstrate%2520a%2520better%250Apart-level%2520understanding%252C%2520they%2520are%2520prohibitively%2520slow%2520and%2520exhibit%2520other%2520common%250Aproblems%2520associated%2520with%2520single-view%2520diffusion%2520models.%2520DreamBeast%2520overcomes%250Athis%2520limitation%2520through%2520a%2520novel%2520part-aware%2520knowledge%2520transfer%2520mechanism.%2520For%250Aeach%2520generated%2520asset%252C%2520we%2520efficiently%2520extract%2520part-level%2520knowledge%2520from%2520the%250AStable%2520Diffusion%25203%2520model%2520into%2520a%25203D%2520Part-Affinity%2520implicit%2520representation.%2520This%250Aenables%2520us%2520to%2520instantly%2520generate%2520Part-Affinity%2520maps%2520from%2520arbitrary%2520camera%250Aviews%252C%2520which%2520we%2520then%2520use%2520to%2520modulate%2520the%2520guidance%2520of%2520a%2520multi-view%2520diffusion%250Amodel%2520during%2520SDS%2520to%2520create%25203D%2520assets%2520of%2520fantastical%2520animals.%2520DreamBeast%250Asignificantly%2520enhances%2520the%2520quality%2520of%2520generated%25203D%2520creatures%2520with%250Auser-specified%2520part%2520compositions%2520while%2520reducing%2520computational%2520overhead%252C%2520as%250Ademonstrated%2520by%2520extensive%2520quantitative%2520and%2520qualitative%2520evaluations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08271v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamBeast%3A%20Distilling%203D%20Fantastical%20Animals%20with%20Part-Aware%20Knowledge%0A%20%20Transfer&entry.906535625=Runjia%20Li%20and%20Junlin%20Han%20and%20Luke%20Melas-Kyriazi%20and%20Chunyi%20Sun%20and%20Zhaochong%20An%20and%20Zhongrui%20Gui%20and%20Shuyang%20Sun%20and%20Philip%20Torr%20and%20Tomas%20Jakab&entry.1292438233=%20%20We%20present%20DreamBeast%2C%20a%20novel%20method%20based%20on%20score%20distillation%20sampling%0A%28SDS%29%20for%20generating%20fantastical%203D%20animal%20assets%20composed%20of%20distinct%20parts.%0AExisting%20SDS%20methods%20often%20struggle%20with%20this%20generation%20task%20due%20to%20a%20limited%0Aunderstanding%20of%20part-level%20semantics%20in%20text-to-image%20diffusion%20models.%20While%0Arecent%20diffusion%20models%2C%20such%20as%20Stable%20Diffusion%203%2C%20demonstrate%20a%20better%0Apart-level%20understanding%2C%20they%20are%20prohibitively%20slow%20and%20exhibit%20other%20common%0Aproblems%20associated%20with%20single-view%20diffusion%20models.%20DreamBeast%20overcomes%0Athis%20limitation%20through%20a%20novel%20part-aware%20knowledge%20transfer%20mechanism.%20For%0Aeach%20generated%20asset%2C%20we%20efficiently%20extract%20part-level%20knowledge%20from%20the%0AStable%20Diffusion%203%20model%20into%20a%203D%20Part-Affinity%20implicit%20representation.%20This%0Aenables%20us%20to%20instantly%20generate%20Part-Affinity%20maps%20from%20arbitrary%20camera%0Aviews%2C%20which%20we%20then%20use%20to%20modulate%20the%20guidance%20of%20a%20multi-view%20diffusion%0Amodel%20during%20SDS%20to%20create%203D%20assets%20of%20fantastical%20animals.%20DreamBeast%0Asignificantly%20enhances%20the%20quality%20of%20generated%203D%20creatures%20with%0Auser-specified%20part%20compositions%20while%20reducing%20computational%20overhead%2C%20as%0Ademonstrated%20by%20extensive%20quantitative%20and%20qualitative%20evaluations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08271v1&entry.124074799=Read"},
{"title": "Tightly-Coupled LiDAR-IMU-Wheel Odometry with Online Calibration of a\n  Kinematic Model for Skid-Steering Robots", "author": "Taku Okawara and Kenji Koide and Shuji Oishi and Masashi Yokozuka and Atsuhiko Banno and Kentaro Uno and Kazuya Yoshida", "abstract": "  Tunnels and long corridors are challenging environments for mobile robots\nbecause a LiDAR point cloud should degenerate in these environments. To tackle\npoint cloud degeneration, this study presents a tightly-coupled LiDAR-IMU-wheel\nodometry algorithm with an online calibration for skid-steering robots. We\npropose a full linear wheel odometry factor, which not only serves as a motion\nconstraint but also performs the online calibration of kinematic models for\nskid-steering robots. Despite the dynamically changing kinematic model (e.g.,\nwheel radii changes caused by tire pressures) and terrain conditions, our\nmethod can address the model error via online calibration. Moreover, our method\nenables an accurate localization in cases of degenerated environments, such as\nlong and straight corridors, by calibration while the LiDAR-IMU fusion\nsufficiently operates. Furthermore, we estimate the uncertainty (i.e.,\ncovariance matrix) of the wheel odometry online for creating a reasonable\nconstraint. The proposed method is validated through three experiments. The\nfirst indoor experiment shows that the proposed method is robust in severe\ndegeneracy cases (long corridors) and changes in the wheel radii. The second\noutdoor experiment demonstrates that our method accurately estimates the sensor\ntrajectory despite being in rough outdoor terrain owing to online uncertainty\nestimation of wheel odometry. The third experiment shows the proposed online\ncalibration enables robust odometry estimation in changing terrains.\n", "link": "http://arxiv.org/abs/2404.02515v3", "date": "2024-09-12", "relevancy": 2.3763, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6208}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6079}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tightly-Coupled%20LiDAR-IMU-Wheel%20Odometry%20with%20Online%20Calibration%20of%20a%0A%20%20Kinematic%20Model%20for%20Skid-Steering%20Robots&body=Title%3A%20Tightly-Coupled%20LiDAR-IMU-Wheel%20Odometry%20with%20Online%20Calibration%20of%20a%0A%20%20Kinematic%20Model%20for%20Skid-Steering%20Robots%0AAuthor%3A%20Taku%20Okawara%20and%20Kenji%20Koide%20and%20Shuji%20Oishi%20and%20Masashi%20Yokozuka%20and%20Atsuhiko%20Banno%20and%20Kentaro%20Uno%20and%20Kazuya%20Yoshida%0AAbstract%3A%20%20%20Tunnels%20and%20long%20corridors%20are%20challenging%20environments%20for%20mobile%20robots%0Abecause%20a%20LiDAR%20point%20cloud%20should%20degenerate%20in%20these%20environments.%20To%20tackle%0Apoint%20cloud%20degeneration%2C%20this%20study%20presents%20a%20tightly-coupled%20LiDAR-IMU-wheel%0Aodometry%20algorithm%20with%20an%20online%20calibration%20for%20skid-steering%20robots.%20We%0Apropose%20a%20full%20linear%20wheel%20odometry%20factor%2C%20which%20not%20only%20serves%20as%20a%20motion%0Aconstraint%20but%20also%20performs%20the%20online%20calibration%20of%20kinematic%20models%20for%0Askid-steering%20robots.%20Despite%20the%20dynamically%20changing%20kinematic%20model%20%28e.g.%2C%0Awheel%20radii%20changes%20caused%20by%20tire%20pressures%29%20and%20terrain%20conditions%2C%20our%0Amethod%20can%20address%20the%20model%20error%20via%20online%20calibration.%20Moreover%2C%20our%20method%0Aenables%20an%20accurate%20localization%20in%20cases%20of%20degenerated%20environments%2C%20such%20as%0Along%20and%20straight%20corridors%2C%20by%20calibration%20while%20the%20LiDAR-IMU%20fusion%0Asufficiently%20operates.%20Furthermore%2C%20we%20estimate%20the%20uncertainty%20%28i.e.%2C%0Acovariance%20matrix%29%20of%20the%20wheel%20odometry%20online%20for%20creating%20a%20reasonable%0Aconstraint.%20The%20proposed%20method%20is%20validated%20through%20three%20experiments.%20The%0Afirst%20indoor%20experiment%20shows%20that%20the%20proposed%20method%20is%20robust%20in%20severe%0Adegeneracy%20cases%20%28long%20corridors%29%20and%20changes%20in%20the%20wheel%20radii.%20The%20second%0Aoutdoor%20experiment%20demonstrates%20that%20our%20method%20accurately%20estimates%20the%20sensor%0Atrajectory%20despite%20being%20in%20rough%20outdoor%20terrain%20owing%20to%20online%20uncertainty%0Aestimation%20of%20wheel%20odometry.%20The%20third%20experiment%20shows%20the%20proposed%20online%0Acalibration%20enables%20robust%20odometry%20estimation%20in%20changing%20terrains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02515v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTightly-Coupled%2520LiDAR-IMU-Wheel%2520Odometry%2520with%2520Online%2520Calibration%2520of%2520a%250A%2520%2520Kinematic%2520Model%2520for%2520Skid-Steering%2520Robots%26entry.906535625%3DTaku%2520Okawara%2520and%2520Kenji%2520Koide%2520and%2520Shuji%2520Oishi%2520and%2520Masashi%2520Yokozuka%2520and%2520Atsuhiko%2520Banno%2520and%2520Kentaro%2520Uno%2520and%2520Kazuya%2520Yoshida%26entry.1292438233%3D%2520%2520Tunnels%2520and%2520long%2520corridors%2520are%2520challenging%2520environments%2520for%2520mobile%2520robots%250Abecause%2520a%2520LiDAR%2520point%2520cloud%2520should%2520degenerate%2520in%2520these%2520environments.%2520To%2520tackle%250Apoint%2520cloud%2520degeneration%252C%2520this%2520study%2520presents%2520a%2520tightly-coupled%2520LiDAR-IMU-wheel%250Aodometry%2520algorithm%2520with%2520an%2520online%2520calibration%2520for%2520skid-steering%2520robots.%2520We%250Apropose%2520a%2520full%2520linear%2520wheel%2520odometry%2520factor%252C%2520which%2520not%2520only%2520serves%2520as%2520a%2520motion%250Aconstraint%2520but%2520also%2520performs%2520the%2520online%2520calibration%2520of%2520kinematic%2520models%2520for%250Askid-steering%2520robots.%2520Despite%2520the%2520dynamically%2520changing%2520kinematic%2520model%2520%2528e.g.%252C%250Awheel%2520radii%2520changes%2520caused%2520by%2520tire%2520pressures%2529%2520and%2520terrain%2520conditions%252C%2520our%250Amethod%2520can%2520address%2520the%2520model%2520error%2520via%2520online%2520calibration.%2520Moreover%252C%2520our%2520method%250Aenables%2520an%2520accurate%2520localization%2520in%2520cases%2520of%2520degenerated%2520environments%252C%2520such%2520as%250Along%2520and%2520straight%2520corridors%252C%2520by%2520calibration%2520while%2520the%2520LiDAR-IMU%2520fusion%250Asufficiently%2520operates.%2520Furthermore%252C%2520we%2520estimate%2520the%2520uncertainty%2520%2528i.e.%252C%250Acovariance%2520matrix%2529%2520of%2520the%2520wheel%2520odometry%2520online%2520for%2520creating%2520a%2520reasonable%250Aconstraint.%2520The%2520proposed%2520method%2520is%2520validated%2520through%2520three%2520experiments.%2520The%250Afirst%2520indoor%2520experiment%2520shows%2520that%2520the%2520proposed%2520method%2520is%2520robust%2520in%2520severe%250Adegeneracy%2520cases%2520%2528long%2520corridors%2529%2520and%2520changes%2520in%2520the%2520wheel%2520radii.%2520The%2520second%250Aoutdoor%2520experiment%2520demonstrates%2520that%2520our%2520method%2520accurately%2520estimates%2520the%2520sensor%250Atrajectory%2520despite%2520being%2520in%2520rough%2520outdoor%2520terrain%2520owing%2520to%2520online%2520uncertainty%250Aestimation%2520of%2520wheel%2520odometry.%2520The%2520third%2520experiment%2520shows%2520the%2520proposed%2520online%250Acalibration%2520enables%2520robust%2520odometry%2520estimation%2520in%2520changing%2520terrains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.02515v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tightly-Coupled%20LiDAR-IMU-Wheel%20Odometry%20with%20Online%20Calibration%20of%20a%0A%20%20Kinematic%20Model%20for%20Skid-Steering%20Robots&entry.906535625=Taku%20Okawara%20and%20Kenji%20Koide%20and%20Shuji%20Oishi%20and%20Masashi%20Yokozuka%20and%20Atsuhiko%20Banno%20and%20Kentaro%20Uno%20and%20Kazuya%20Yoshida&entry.1292438233=%20%20Tunnels%20and%20long%20corridors%20are%20challenging%20environments%20for%20mobile%20robots%0Abecause%20a%20LiDAR%20point%20cloud%20should%20degenerate%20in%20these%20environments.%20To%20tackle%0Apoint%20cloud%20degeneration%2C%20this%20study%20presents%20a%20tightly-coupled%20LiDAR-IMU-wheel%0Aodometry%20algorithm%20with%20an%20online%20calibration%20for%20skid-steering%20robots.%20We%0Apropose%20a%20full%20linear%20wheel%20odometry%20factor%2C%20which%20not%20only%20serves%20as%20a%20motion%0Aconstraint%20but%20also%20performs%20the%20online%20calibration%20of%20kinematic%20models%20for%0Askid-steering%20robots.%20Despite%20the%20dynamically%20changing%20kinematic%20model%20%28e.g.%2C%0Awheel%20radii%20changes%20caused%20by%20tire%20pressures%29%20and%20terrain%20conditions%2C%20our%0Amethod%20can%20address%20the%20model%20error%20via%20online%20calibration.%20Moreover%2C%20our%20method%0Aenables%20an%20accurate%20localization%20in%20cases%20of%20degenerated%20environments%2C%20such%20as%0Along%20and%20straight%20corridors%2C%20by%20calibration%20while%20the%20LiDAR-IMU%20fusion%0Asufficiently%20operates.%20Furthermore%2C%20we%20estimate%20the%20uncertainty%20%28i.e.%2C%0Acovariance%20matrix%29%20of%20the%20wheel%20odometry%20online%20for%20creating%20a%20reasonable%0Aconstraint.%20The%20proposed%20method%20is%20validated%20through%20three%20experiments.%20The%0Afirst%20indoor%20experiment%20shows%20that%20the%20proposed%20method%20is%20robust%20in%20severe%0Adegeneracy%20cases%20%28long%20corridors%29%20and%20changes%20in%20the%20wheel%20radii.%20The%20second%0Aoutdoor%20experiment%20demonstrates%20that%20our%20method%20accurately%20estimates%20the%20sensor%0Atrajectory%20despite%20being%20in%20rough%20outdoor%20terrain%20owing%20to%20online%20uncertainty%0Aestimation%20of%20wheel%20odometry.%20The%20third%20experiment%20shows%20the%20proposed%20online%0Acalibration%20enables%20robust%20odometry%20estimation%20in%20changing%20terrains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02515v3&entry.124074799=Read"},
{"title": "TextBoost: Towards One-Shot Personalization of Text-to-Image Models via\n  Fine-tuning Text Encoder", "author": "NaHyeon Park and Kunhee Kim and Hyunjung Shim", "abstract": "  Recent breakthroughs in text-to-image models have opened up promising\nresearch avenues in personalized image generation, enabling users to create\ndiverse images of a specific subject using natural language prompts. However,\nexisting methods often suffer from performance degradation when given only a\nsingle reference image. They tend to overfit the input, producing highly\nsimilar outputs regardless of the text prompt. This paper addresses the\nchallenge of one-shot personalization by mitigating overfitting, enabling the\ncreation of controllable images through text prompts. Specifically, we propose\na selective fine-tuning strategy that focuses on the text encoder. Furthermore,\nwe introduce three key techniques to enhance personalization performance: (1)\naugmentation tokens to encourage feature disentanglement and alleviate\noverfitting, (2) a knowledge-preservation loss to reduce language drift and\npromote generalizability across diverse prompts, and (3) SNR-weighted sampling\nfor efficient training. Extensive experiments demonstrate that our approach\nefficiently generates high-quality, diverse images using only a single\nreference image while significantly reducing memory and storage requirements.\n", "link": "http://arxiv.org/abs/2409.08248v1", "date": "2024-09-12", "relevancy": 2.3755, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6494}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5984}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TextBoost%3A%20Towards%20One-Shot%20Personalization%20of%20Text-to-Image%20Models%20via%0A%20%20Fine-tuning%20Text%20Encoder&body=Title%3A%20TextBoost%3A%20Towards%20One-Shot%20Personalization%20of%20Text-to-Image%20Models%20via%0A%20%20Fine-tuning%20Text%20Encoder%0AAuthor%3A%20NaHyeon%20Park%20and%20Kunhee%20Kim%20and%20Hyunjung%20Shim%0AAbstract%3A%20%20%20Recent%20breakthroughs%20in%20text-to-image%20models%20have%20opened%20up%20promising%0Aresearch%20avenues%20in%20personalized%20image%20generation%2C%20enabling%20users%20to%20create%0Adiverse%20images%20of%20a%20specific%20subject%20using%20natural%20language%20prompts.%20However%2C%0Aexisting%20methods%20often%20suffer%20from%20performance%20degradation%20when%20given%20only%20a%0Asingle%20reference%20image.%20They%20tend%20to%20overfit%20the%20input%2C%20producing%20highly%0Asimilar%20outputs%20regardless%20of%20the%20text%20prompt.%20This%20paper%20addresses%20the%0Achallenge%20of%20one-shot%20personalization%20by%20mitigating%20overfitting%2C%20enabling%20the%0Acreation%20of%20controllable%20images%20through%20text%20prompts.%20Specifically%2C%20we%20propose%0Aa%20selective%20fine-tuning%20strategy%20that%20focuses%20on%20the%20text%20encoder.%20Furthermore%2C%0Awe%20introduce%20three%20key%20techniques%20to%20enhance%20personalization%20performance%3A%20%281%29%0Aaugmentation%20tokens%20to%20encourage%20feature%20disentanglement%20and%20alleviate%0Aoverfitting%2C%20%282%29%20a%20knowledge-preservation%20loss%20to%20reduce%20language%20drift%20and%0Apromote%20generalizability%20across%20diverse%20prompts%2C%20and%20%283%29%20SNR-weighted%20sampling%0Afor%20efficient%20training.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%0Aefficiently%20generates%20high-quality%2C%20diverse%20images%20using%20only%20a%20single%0Areference%20image%20while%20significantly%20reducing%20memory%20and%20storage%20requirements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08248v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTextBoost%253A%2520Towards%2520One-Shot%2520Personalization%2520of%2520Text-to-Image%2520Models%2520via%250A%2520%2520Fine-tuning%2520Text%2520Encoder%26entry.906535625%3DNaHyeon%2520Park%2520and%2520Kunhee%2520Kim%2520and%2520Hyunjung%2520Shim%26entry.1292438233%3D%2520%2520Recent%2520breakthroughs%2520in%2520text-to-image%2520models%2520have%2520opened%2520up%2520promising%250Aresearch%2520avenues%2520in%2520personalized%2520image%2520generation%252C%2520enabling%2520users%2520to%2520create%250Adiverse%2520images%2520of%2520a%2520specific%2520subject%2520using%2520natural%2520language%2520prompts.%2520However%252C%250Aexisting%2520methods%2520often%2520suffer%2520from%2520performance%2520degradation%2520when%2520given%2520only%2520a%250Asingle%2520reference%2520image.%2520They%2520tend%2520to%2520overfit%2520the%2520input%252C%2520producing%2520highly%250Asimilar%2520outputs%2520regardless%2520of%2520the%2520text%2520prompt.%2520This%2520paper%2520addresses%2520the%250Achallenge%2520of%2520one-shot%2520personalization%2520by%2520mitigating%2520overfitting%252C%2520enabling%2520the%250Acreation%2520of%2520controllable%2520images%2520through%2520text%2520prompts.%2520Specifically%252C%2520we%2520propose%250Aa%2520selective%2520fine-tuning%2520strategy%2520that%2520focuses%2520on%2520the%2520text%2520encoder.%2520Furthermore%252C%250Awe%2520introduce%2520three%2520key%2520techniques%2520to%2520enhance%2520personalization%2520performance%253A%2520%25281%2529%250Aaugmentation%2520tokens%2520to%2520encourage%2520feature%2520disentanglement%2520and%2520alleviate%250Aoverfitting%252C%2520%25282%2529%2520a%2520knowledge-preservation%2520loss%2520to%2520reduce%2520language%2520drift%2520and%250Apromote%2520generalizability%2520across%2520diverse%2520prompts%252C%2520and%2520%25283%2529%2520SNR-weighted%2520sampling%250Afor%2520efficient%2520training.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520approach%250Aefficiently%2520generates%2520high-quality%252C%2520diverse%2520images%2520using%2520only%2520a%2520single%250Areference%2520image%2520while%2520significantly%2520reducing%2520memory%2520and%2520storage%2520requirements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08248v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TextBoost%3A%20Towards%20One-Shot%20Personalization%20of%20Text-to-Image%20Models%20via%0A%20%20Fine-tuning%20Text%20Encoder&entry.906535625=NaHyeon%20Park%20and%20Kunhee%20Kim%20and%20Hyunjung%20Shim&entry.1292438233=%20%20Recent%20breakthroughs%20in%20text-to-image%20models%20have%20opened%20up%20promising%0Aresearch%20avenues%20in%20personalized%20image%20generation%2C%20enabling%20users%20to%20create%0Adiverse%20images%20of%20a%20specific%20subject%20using%20natural%20language%20prompts.%20However%2C%0Aexisting%20methods%20often%20suffer%20from%20performance%20degradation%20when%20given%20only%20a%0Asingle%20reference%20image.%20They%20tend%20to%20overfit%20the%20input%2C%20producing%20highly%0Asimilar%20outputs%20regardless%20of%20the%20text%20prompt.%20This%20paper%20addresses%20the%0Achallenge%20of%20one-shot%20personalization%20by%20mitigating%20overfitting%2C%20enabling%20the%0Acreation%20of%20controllable%20images%20through%20text%20prompts.%20Specifically%2C%20we%20propose%0Aa%20selective%20fine-tuning%20strategy%20that%20focuses%20on%20the%20text%20encoder.%20Furthermore%2C%0Awe%20introduce%20three%20key%20techniques%20to%20enhance%20personalization%20performance%3A%20%281%29%0Aaugmentation%20tokens%20to%20encourage%20feature%20disentanglement%20and%20alleviate%0Aoverfitting%2C%20%282%29%20a%20knowledge-preservation%20loss%20to%20reduce%20language%20drift%20and%0Apromote%20generalizability%20across%20diverse%20prompts%2C%20and%20%283%29%20SNR-weighted%20sampling%0Afor%20efficient%20training.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%0Aefficiently%20generates%20high-quality%2C%20diverse%20images%20using%20only%20a%20single%0Areference%20image%20while%20significantly%20reducing%20memory%20and%20storage%20requirements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08248v1&entry.124074799=Read"},
{"title": "What Makes a Maze Look Like a Maze?", "author": "Joy Hsu and Jiayuan Mao and Joshua B. Tenenbaum and Noah D. Goodman and Jiajun Wu", "abstract": "  A unique aspect of human visual understanding is the ability to flexibly\ninterpret abstract concepts: acquiring lifted rules explaining what they\nsymbolize, grounding them across familiar and unfamiliar contexts, and making\npredictions or reasoning about them. While off-the-shelf vision-language models\nexcel at making literal interpretations of images (e.g., recognizing object\ncategories such as tree branches), they still struggle to make sense of such\nvisual abstractions (e.g., how an arrangement of tree branches may form the\nwalls of a maze). To address this challenge, we introduce Deep Schema Grounding\n(DSG), a framework that leverages explicit structured representations of visual\nabstractions for grounding and reasoning. At the core of DSG are\nschemas--dependency graph descriptions of abstract concepts that decompose them\ninto more primitive-level symbols. DSG uses large language models to extract\nschemas, then hierarchically grounds concrete to abstract components of the\nschema onto images with vision-language models. The grounded schema is used to\naugment visual abstraction understanding. We systematically evaluate DSG and\ndifferent methods in reasoning on our new Visual Abstractions Dataset, which\nconsists of diverse, real-world images of abstract concepts and corresponding\nquestion-answer pairs labeled by humans. We show that DSG significantly\nimproves the abstract visual reasoning performance of vision-language models,\nand is a step toward human-aligned understanding of visual abstractions.\n", "link": "http://arxiv.org/abs/2409.08202v1", "date": "2024-09-12", "relevancy": 2.3726, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6011}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6011}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Makes%20a%20Maze%20Look%20Like%20a%20Maze%3F&body=Title%3A%20What%20Makes%20a%20Maze%20Look%20Like%20a%20Maze%3F%0AAuthor%3A%20Joy%20Hsu%20and%20Jiayuan%20Mao%20and%20Joshua%20B.%20Tenenbaum%20and%20Noah%20D.%20Goodman%20and%20Jiajun%20Wu%0AAbstract%3A%20%20%20A%20unique%20aspect%20of%20human%20visual%20understanding%20is%20the%20ability%20to%20flexibly%0Ainterpret%20abstract%20concepts%3A%20acquiring%20lifted%20rules%20explaining%20what%20they%0Asymbolize%2C%20grounding%20them%20across%20familiar%20and%20unfamiliar%20contexts%2C%20and%20making%0Apredictions%20or%20reasoning%20about%20them.%20While%20off-the-shelf%20vision-language%20models%0Aexcel%20at%20making%20literal%20interpretations%20of%20images%20%28e.g.%2C%20recognizing%20object%0Acategories%20such%20as%20tree%20branches%29%2C%20they%20still%20struggle%20to%20make%20sense%20of%20such%0Avisual%20abstractions%20%28e.g.%2C%20how%20an%20arrangement%20of%20tree%20branches%20may%20form%20the%0Awalls%20of%20a%20maze%29.%20To%20address%20this%20challenge%2C%20we%20introduce%20Deep%20Schema%20Grounding%0A%28DSG%29%2C%20a%20framework%20that%20leverages%20explicit%20structured%20representations%20of%20visual%0Aabstractions%20for%20grounding%20and%20reasoning.%20At%20the%20core%20of%20DSG%20are%0Aschemas--dependency%20graph%20descriptions%20of%20abstract%20concepts%20that%20decompose%20them%0Ainto%20more%20primitive-level%20symbols.%20DSG%20uses%20large%20language%20models%20to%20extract%0Aschemas%2C%20then%20hierarchically%20grounds%20concrete%20to%20abstract%20components%20of%20the%0Aschema%20onto%20images%20with%20vision-language%20models.%20The%20grounded%20schema%20is%20used%20to%0Aaugment%20visual%20abstraction%20understanding.%20We%20systematically%20evaluate%20DSG%20and%0Adifferent%20methods%20in%20reasoning%20on%20our%20new%20Visual%20Abstractions%20Dataset%2C%20which%0Aconsists%20of%20diverse%2C%20real-world%20images%20of%20abstract%20concepts%20and%20corresponding%0Aquestion-answer%20pairs%20labeled%20by%20humans.%20We%20show%20that%20DSG%20significantly%0Aimproves%20the%20abstract%20visual%20reasoning%20performance%20of%20vision-language%20models%2C%0Aand%20is%20a%20step%20toward%20human-aligned%20understanding%20of%20visual%20abstractions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08202v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Makes%2520a%2520Maze%2520Look%2520Like%2520a%2520Maze%253F%26entry.906535625%3DJoy%2520Hsu%2520and%2520Jiayuan%2520Mao%2520and%2520Joshua%2520B.%2520Tenenbaum%2520and%2520Noah%2520D.%2520Goodman%2520and%2520Jiajun%2520Wu%26entry.1292438233%3D%2520%2520A%2520unique%2520aspect%2520of%2520human%2520visual%2520understanding%2520is%2520the%2520ability%2520to%2520flexibly%250Ainterpret%2520abstract%2520concepts%253A%2520acquiring%2520lifted%2520rules%2520explaining%2520what%2520they%250Asymbolize%252C%2520grounding%2520them%2520across%2520familiar%2520and%2520unfamiliar%2520contexts%252C%2520and%2520making%250Apredictions%2520or%2520reasoning%2520about%2520them.%2520While%2520off-the-shelf%2520vision-language%2520models%250Aexcel%2520at%2520making%2520literal%2520interpretations%2520of%2520images%2520%2528e.g.%252C%2520recognizing%2520object%250Acategories%2520such%2520as%2520tree%2520branches%2529%252C%2520they%2520still%2520struggle%2520to%2520make%2520sense%2520of%2520such%250Avisual%2520abstractions%2520%2528e.g.%252C%2520how%2520an%2520arrangement%2520of%2520tree%2520branches%2520may%2520form%2520the%250Awalls%2520of%2520a%2520maze%2529.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520Deep%2520Schema%2520Grounding%250A%2528DSG%2529%252C%2520a%2520framework%2520that%2520leverages%2520explicit%2520structured%2520representations%2520of%2520visual%250Aabstractions%2520for%2520grounding%2520and%2520reasoning.%2520At%2520the%2520core%2520of%2520DSG%2520are%250Aschemas--dependency%2520graph%2520descriptions%2520of%2520abstract%2520concepts%2520that%2520decompose%2520them%250Ainto%2520more%2520primitive-level%2520symbols.%2520DSG%2520uses%2520large%2520language%2520models%2520to%2520extract%250Aschemas%252C%2520then%2520hierarchically%2520grounds%2520concrete%2520to%2520abstract%2520components%2520of%2520the%250Aschema%2520onto%2520images%2520with%2520vision-language%2520models.%2520The%2520grounded%2520schema%2520is%2520used%2520to%250Aaugment%2520visual%2520abstraction%2520understanding.%2520We%2520systematically%2520evaluate%2520DSG%2520and%250Adifferent%2520methods%2520in%2520reasoning%2520on%2520our%2520new%2520Visual%2520Abstractions%2520Dataset%252C%2520which%250Aconsists%2520of%2520diverse%252C%2520real-world%2520images%2520of%2520abstract%2520concepts%2520and%2520corresponding%250Aquestion-answer%2520pairs%2520labeled%2520by%2520humans.%2520We%2520show%2520that%2520DSG%2520significantly%250Aimproves%2520the%2520abstract%2520visual%2520reasoning%2520performance%2520of%2520vision-language%2520models%252C%250Aand%2520is%2520a%2520step%2520toward%2520human-aligned%2520understanding%2520of%2520visual%2520abstractions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08202v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Makes%20a%20Maze%20Look%20Like%20a%20Maze%3F&entry.906535625=Joy%20Hsu%20and%20Jiayuan%20Mao%20and%20Joshua%20B.%20Tenenbaum%20and%20Noah%20D.%20Goodman%20and%20Jiajun%20Wu&entry.1292438233=%20%20A%20unique%20aspect%20of%20human%20visual%20understanding%20is%20the%20ability%20to%20flexibly%0Ainterpret%20abstract%20concepts%3A%20acquiring%20lifted%20rules%20explaining%20what%20they%0Asymbolize%2C%20grounding%20them%20across%20familiar%20and%20unfamiliar%20contexts%2C%20and%20making%0Apredictions%20or%20reasoning%20about%20them.%20While%20off-the-shelf%20vision-language%20models%0Aexcel%20at%20making%20literal%20interpretations%20of%20images%20%28e.g.%2C%20recognizing%20object%0Acategories%20such%20as%20tree%20branches%29%2C%20they%20still%20struggle%20to%20make%20sense%20of%20such%0Avisual%20abstractions%20%28e.g.%2C%20how%20an%20arrangement%20of%20tree%20branches%20may%20form%20the%0Awalls%20of%20a%20maze%29.%20To%20address%20this%20challenge%2C%20we%20introduce%20Deep%20Schema%20Grounding%0A%28DSG%29%2C%20a%20framework%20that%20leverages%20explicit%20structured%20representations%20of%20visual%0Aabstractions%20for%20grounding%20and%20reasoning.%20At%20the%20core%20of%20DSG%20are%0Aschemas--dependency%20graph%20descriptions%20of%20abstract%20concepts%20that%20decompose%20them%0Ainto%20more%20primitive-level%20symbols.%20DSG%20uses%20large%20language%20models%20to%20extract%0Aschemas%2C%20then%20hierarchically%20grounds%20concrete%20to%20abstract%20components%20of%20the%0Aschema%20onto%20images%20with%20vision-language%20models.%20The%20grounded%20schema%20is%20used%20to%0Aaugment%20visual%20abstraction%20understanding.%20We%20systematically%20evaluate%20DSG%20and%0Adifferent%20methods%20in%20reasoning%20on%20our%20new%20Visual%20Abstractions%20Dataset%2C%20which%0Aconsists%20of%20diverse%2C%20real-world%20images%20of%20abstract%20concepts%20and%20corresponding%0Aquestion-answer%20pairs%20labeled%20by%20humans.%20We%20show%20that%20DSG%20significantly%0Aimproves%20the%20abstract%20visual%20reasoning%20performance%20of%20vision-language%20models%2C%0Aand%20is%20a%20step%20toward%20human-aligned%20understanding%20of%20visual%20abstractions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08202v1&entry.124074799=Read"},
{"title": "SimMAT: Exploring Transferability from Vision Foundation Models to Any\n  Image Modality", "author": "Chenyang Lei and Liyi Chen and Jun Cen and Xiao Chen and Zhen Lei and Felix Heide and Ziwei Liu and Qifeng Chen and Zhaoxiang Zhang", "abstract": "  Foundation models like ChatGPT and Sora that are trained on a huge scale of\ndata have made a revolutionary social impact. However, it is extremely\nchallenging for sensors in many different fields to collect similar scales of\nnatural images to train strong foundation models. To this end, this work\npresents a simple and effective framework SimMAT to study an open problem: the\ntransferability from vision foundation models trained on natural RGB images to\nother image modalities of different physical properties (e.g., polarization).\nSimMAT consists of a modality-agnostic transfer layer (MAT) and a pretrained\nfoundation model. We apply SimMAT to a representative vision foundation model\nSegment Anything Model (SAM) to support any evaluated new image modality. Given\nthe absence of relevant benchmarks, we construct a new benchmark to evaluate\nthe transfer learning performance. Our experiments confirm the intriguing\npotential of transferring vision foundation models in enhancing other sensors'\nperformance. Specifically, SimMAT can improve the segmentation performance\n(mIoU) from 22.15% to 53.88% on average for evaluated modalities and\nconsistently outperforms other baselines. We hope that SimMAT can raise\nawareness of cross-modal transfer learning and benefit various fields for\nbetter results with vision foundation models.\n", "link": "http://arxiv.org/abs/2409.08083v1", "date": "2024-09-12", "relevancy": 2.3708, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6246}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5838}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SimMAT%3A%20Exploring%20Transferability%20from%20Vision%20Foundation%20Models%20to%20Any%0A%20%20Image%20Modality&body=Title%3A%20SimMAT%3A%20Exploring%20Transferability%20from%20Vision%20Foundation%20Models%20to%20Any%0A%20%20Image%20Modality%0AAuthor%3A%20Chenyang%20Lei%20and%20Liyi%20Chen%20and%20Jun%20Cen%20and%20Xiao%20Chen%20and%20Zhen%20Lei%20and%20Felix%20Heide%20and%20Ziwei%20Liu%20and%20Qifeng%20Chen%20and%20Zhaoxiang%20Zhang%0AAbstract%3A%20%20%20Foundation%20models%20like%20ChatGPT%20and%20Sora%20that%20are%20trained%20on%20a%20huge%20scale%20of%0Adata%20have%20made%20a%20revolutionary%20social%20impact.%20However%2C%20it%20is%20extremely%0Achallenging%20for%20sensors%20in%20many%20different%20fields%20to%20collect%20similar%20scales%20of%0Anatural%20images%20to%20train%20strong%20foundation%20models.%20To%20this%20end%2C%20this%20work%0Apresents%20a%20simple%20and%20effective%20framework%20SimMAT%20to%20study%20an%20open%20problem%3A%20the%0Atransferability%20from%20vision%20foundation%20models%20trained%20on%20natural%20RGB%20images%20to%0Aother%20image%20modalities%20of%20different%20physical%20properties%20%28e.g.%2C%20polarization%29.%0ASimMAT%20consists%20of%20a%20modality-agnostic%20transfer%20layer%20%28MAT%29%20and%20a%20pretrained%0Afoundation%20model.%20We%20apply%20SimMAT%20to%20a%20representative%20vision%20foundation%20model%0ASegment%20Anything%20Model%20%28SAM%29%20to%20support%20any%20evaluated%20new%20image%20modality.%20Given%0Athe%20absence%20of%20relevant%20benchmarks%2C%20we%20construct%20a%20new%20benchmark%20to%20evaluate%0Athe%20transfer%20learning%20performance.%20Our%20experiments%20confirm%20the%20intriguing%0Apotential%20of%20transferring%20vision%20foundation%20models%20in%20enhancing%20other%20sensors%27%0Aperformance.%20Specifically%2C%20SimMAT%20can%20improve%20the%20segmentation%20performance%0A%28mIoU%29%20from%2022.15%25%20to%2053.88%25%20on%20average%20for%20evaluated%20modalities%20and%0Aconsistently%20outperforms%20other%20baselines.%20We%20hope%20that%20SimMAT%20can%20raise%0Aawareness%20of%20cross-modal%20transfer%20learning%20and%20benefit%20various%20fields%20for%0Abetter%20results%20with%20vision%20foundation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08083v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimMAT%253A%2520Exploring%2520Transferability%2520from%2520Vision%2520Foundation%2520Models%2520to%2520Any%250A%2520%2520Image%2520Modality%26entry.906535625%3DChenyang%2520Lei%2520and%2520Liyi%2520Chen%2520and%2520Jun%2520Cen%2520and%2520Xiao%2520Chen%2520and%2520Zhen%2520Lei%2520and%2520Felix%2520Heide%2520and%2520Ziwei%2520Liu%2520and%2520Qifeng%2520Chen%2520and%2520Zhaoxiang%2520Zhang%26entry.1292438233%3D%2520%2520Foundation%2520models%2520like%2520ChatGPT%2520and%2520Sora%2520that%2520are%2520trained%2520on%2520a%2520huge%2520scale%2520of%250Adata%2520have%2520made%2520a%2520revolutionary%2520social%2520impact.%2520However%252C%2520it%2520is%2520extremely%250Achallenging%2520for%2520sensors%2520in%2520many%2520different%2520fields%2520to%2520collect%2520similar%2520scales%2520of%250Anatural%2520images%2520to%2520train%2520strong%2520foundation%2520models.%2520To%2520this%2520end%252C%2520this%2520work%250Apresents%2520a%2520simple%2520and%2520effective%2520framework%2520SimMAT%2520to%2520study%2520an%2520open%2520problem%253A%2520the%250Atransferability%2520from%2520vision%2520foundation%2520models%2520trained%2520on%2520natural%2520RGB%2520images%2520to%250Aother%2520image%2520modalities%2520of%2520different%2520physical%2520properties%2520%2528e.g.%252C%2520polarization%2529.%250ASimMAT%2520consists%2520of%2520a%2520modality-agnostic%2520transfer%2520layer%2520%2528MAT%2529%2520and%2520a%2520pretrained%250Afoundation%2520model.%2520We%2520apply%2520SimMAT%2520to%2520a%2520representative%2520vision%2520foundation%2520model%250ASegment%2520Anything%2520Model%2520%2528SAM%2529%2520to%2520support%2520any%2520evaluated%2520new%2520image%2520modality.%2520Given%250Athe%2520absence%2520of%2520relevant%2520benchmarks%252C%2520we%2520construct%2520a%2520new%2520benchmark%2520to%2520evaluate%250Athe%2520transfer%2520learning%2520performance.%2520Our%2520experiments%2520confirm%2520the%2520intriguing%250Apotential%2520of%2520transferring%2520vision%2520foundation%2520models%2520in%2520enhancing%2520other%2520sensors%2527%250Aperformance.%2520Specifically%252C%2520SimMAT%2520can%2520improve%2520the%2520segmentation%2520performance%250A%2528mIoU%2529%2520from%252022.15%2525%2520to%252053.88%2525%2520on%2520average%2520for%2520evaluated%2520modalities%2520and%250Aconsistently%2520outperforms%2520other%2520baselines.%2520We%2520hope%2520that%2520SimMAT%2520can%2520raise%250Aawareness%2520of%2520cross-modal%2520transfer%2520learning%2520and%2520benefit%2520various%2520fields%2520for%250Abetter%2520results%2520with%2520vision%2520foundation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08083v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SimMAT%3A%20Exploring%20Transferability%20from%20Vision%20Foundation%20Models%20to%20Any%0A%20%20Image%20Modality&entry.906535625=Chenyang%20Lei%20and%20Liyi%20Chen%20and%20Jun%20Cen%20and%20Xiao%20Chen%20and%20Zhen%20Lei%20and%20Felix%20Heide%20and%20Ziwei%20Liu%20and%20Qifeng%20Chen%20and%20Zhaoxiang%20Zhang&entry.1292438233=%20%20Foundation%20models%20like%20ChatGPT%20and%20Sora%20that%20are%20trained%20on%20a%20huge%20scale%20of%0Adata%20have%20made%20a%20revolutionary%20social%20impact.%20However%2C%20it%20is%20extremely%0Achallenging%20for%20sensors%20in%20many%20different%20fields%20to%20collect%20similar%20scales%20of%0Anatural%20images%20to%20train%20strong%20foundation%20models.%20To%20this%20end%2C%20this%20work%0Apresents%20a%20simple%20and%20effective%20framework%20SimMAT%20to%20study%20an%20open%20problem%3A%20the%0Atransferability%20from%20vision%20foundation%20models%20trained%20on%20natural%20RGB%20images%20to%0Aother%20image%20modalities%20of%20different%20physical%20properties%20%28e.g.%2C%20polarization%29.%0ASimMAT%20consists%20of%20a%20modality-agnostic%20transfer%20layer%20%28MAT%29%20and%20a%20pretrained%0Afoundation%20model.%20We%20apply%20SimMAT%20to%20a%20representative%20vision%20foundation%20model%0ASegment%20Anything%20Model%20%28SAM%29%20to%20support%20any%20evaluated%20new%20image%20modality.%20Given%0Athe%20absence%20of%20relevant%20benchmarks%2C%20we%20construct%20a%20new%20benchmark%20to%20evaluate%0Athe%20transfer%20learning%20performance.%20Our%20experiments%20confirm%20the%20intriguing%0Apotential%20of%20transferring%20vision%20foundation%20models%20in%20enhancing%20other%20sensors%27%0Aperformance.%20Specifically%2C%20SimMAT%20can%20improve%20the%20segmentation%20performance%0A%28mIoU%29%20from%2022.15%25%20to%2053.88%25%20on%20average%20for%20evaluated%20modalities%20and%0Aconsistently%20outperforms%20other%20baselines.%20We%20hope%20that%20SimMAT%20can%20raise%0Aawareness%20of%20cross-modal%20transfer%20learning%20and%20benefit%20various%20fields%20for%0Abetter%20results%20with%20vision%20foundation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08083v1&entry.124074799=Read"},
{"title": "DreamHOI: Subject-Driven Generation of 3D Human-Object Interactions with\n  Diffusion Priors", "author": "Thomas Hanwen Zhu and Ruining Li and Tomas Jakab", "abstract": "  We present DreamHOI, a novel method for zero-shot synthesis of human-object\ninteractions (HOIs), enabling a 3D human model to realistically interact with\nany given object based on a textual description. This task is complicated by\nthe varying categories and geometries of real-world objects and the scarcity of\ndatasets encompassing diverse HOIs. To circumvent the need for extensive data,\nwe leverage text-to-image diffusion models trained on billions of image-caption\npairs. We optimize the articulation of a skinned human mesh using Score\nDistillation Sampling (SDS) gradients obtained from these models, which predict\nimage-space edits. However, directly backpropagating image-space gradients into\ncomplex articulation parameters is ineffective due to the local nature of such\ngradients. To overcome this, we introduce a dual implicit-explicit\nrepresentation of a skinned mesh, combining (implicit) neural radiance fields\n(NeRFs) with (explicit) skeleton-driven mesh articulation. During optimization,\nwe transition between implicit and explicit forms, grounding the NeRF\ngeneration while refining the mesh articulation. We validate our approach\nthrough extensive experiments, demonstrating its effectiveness in generating\nrealistic HOIs.\n", "link": "http://arxiv.org/abs/2409.08278v1", "date": "2024-09-12", "relevancy": 2.3697, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6146}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6128}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5631}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamHOI%3A%20Subject-Driven%20Generation%20of%203D%20Human-Object%20Interactions%20with%0A%20%20Diffusion%20Priors&body=Title%3A%20DreamHOI%3A%20Subject-Driven%20Generation%20of%203D%20Human-Object%20Interactions%20with%0A%20%20Diffusion%20Priors%0AAuthor%3A%20Thomas%20Hanwen%20Zhu%20and%20Ruining%20Li%20and%20Tomas%20Jakab%0AAbstract%3A%20%20%20We%20present%20DreamHOI%2C%20a%20novel%20method%20for%20zero-shot%20synthesis%20of%20human-object%0Ainteractions%20%28HOIs%29%2C%20enabling%20a%203D%20human%20model%20to%20realistically%20interact%20with%0Aany%20given%20object%20based%20on%20a%20textual%20description.%20This%20task%20is%20complicated%20by%0Athe%20varying%20categories%20and%20geometries%20of%20real-world%20objects%20and%20the%20scarcity%20of%0Adatasets%20encompassing%20diverse%20HOIs.%20To%20circumvent%20the%20need%20for%20extensive%20data%2C%0Awe%20leverage%20text-to-image%20diffusion%20models%20trained%20on%20billions%20of%20image-caption%0Apairs.%20We%20optimize%20the%20articulation%20of%20a%20skinned%20human%20mesh%20using%20Score%0ADistillation%20Sampling%20%28SDS%29%20gradients%20obtained%20from%20these%20models%2C%20which%20predict%0Aimage-space%20edits.%20However%2C%20directly%20backpropagating%20image-space%20gradients%20into%0Acomplex%20articulation%20parameters%20is%20ineffective%20due%20to%20the%20local%20nature%20of%20such%0Agradients.%20To%20overcome%20this%2C%20we%20introduce%20a%20dual%20implicit-explicit%0Arepresentation%20of%20a%20skinned%20mesh%2C%20combining%20%28implicit%29%20neural%20radiance%20fields%0A%28NeRFs%29%20with%20%28explicit%29%20skeleton-driven%20mesh%20articulation.%20During%20optimization%2C%0Awe%20transition%20between%20implicit%20and%20explicit%20forms%2C%20grounding%20the%20NeRF%0Ageneration%20while%20refining%20the%20mesh%20articulation.%20We%20validate%20our%20approach%0Athrough%20extensive%20experiments%2C%20demonstrating%20its%20effectiveness%20in%20generating%0Arealistic%20HOIs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08278v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamHOI%253A%2520Subject-Driven%2520Generation%2520of%25203D%2520Human-Object%2520Interactions%2520with%250A%2520%2520Diffusion%2520Priors%26entry.906535625%3DThomas%2520Hanwen%2520Zhu%2520and%2520Ruining%2520Li%2520and%2520Tomas%2520Jakab%26entry.1292438233%3D%2520%2520We%2520present%2520DreamHOI%252C%2520a%2520novel%2520method%2520for%2520zero-shot%2520synthesis%2520of%2520human-object%250Ainteractions%2520%2528HOIs%2529%252C%2520enabling%2520a%25203D%2520human%2520model%2520to%2520realistically%2520interact%2520with%250Aany%2520given%2520object%2520based%2520on%2520a%2520textual%2520description.%2520This%2520task%2520is%2520complicated%2520by%250Athe%2520varying%2520categories%2520and%2520geometries%2520of%2520real-world%2520objects%2520and%2520the%2520scarcity%2520of%250Adatasets%2520encompassing%2520diverse%2520HOIs.%2520To%2520circumvent%2520the%2520need%2520for%2520extensive%2520data%252C%250Awe%2520leverage%2520text-to-image%2520diffusion%2520models%2520trained%2520on%2520billions%2520of%2520image-caption%250Apairs.%2520We%2520optimize%2520the%2520articulation%2520of%2520a%2520skinned%2520human%2520mesh%2520using%2520Score%250ADistillation%2520Sampling%2520%2528SDS%2529%2520gradients%2520obtained%2520from%2520these%2520models%252C%2520which%2520predict%250Aimage-space%2520edits.%2520However%252C%2520directly%2520backpropagating%2520image-space%2520gradients%2520into%250Acomplex%2520articulation%2520parameters%2520is%2520ineffective%2520due%2520to%2520the%2520local%2520nature%2520of%2520such%250Agradients.%2520To%2520overcome%2520this%252C%2520we%2520introduce%2520a%2520dual%2520implicit-explicit%250Arepresentation%2520of%2520a%2520skinned%2520mesh%252C%2520combining%2520%2528implicit%2529%2520neural%2520radiance%2520fields%250A%2528NeRFs%2529%2520with%2520%2528explicit%2529%2520skeleton-driven%2520mesh%2520articulation.%2520During%2520optimization%252C%250Awe%2520transition%2520between%2520implicit%2520and%2520explicit%2520forms%252C%2520grounding%2520the%2520NeRF%250Ageneration%2520while%2520refining%2520the%2520mesh%2520articulation.%2520We%2520validate%2520our%2520approach%250Athrough%2520extensive%2520experiments%252C%2520demonstrating%2520its%2520effectiveness%2520in%2520generating%250Arealistic%2520HOIs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08278v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamHOI%3A%20Subject-Driven%20Generation%20of%203D%20Human-Object%20Interactions%20with%0A%20%20Diffusion%20Priors&entry.906535625=Thomas%20Hanwen%20Zhu%20and%20Ruining%20Li%20and%20Tomas%20Jakab&entry.1292438233=%20%20We%20present%20DreamHOI%2C%20a%20novel%20method%20for%20zero-shot%20synthesis%20of%20human-object%0Ainteractions%20%28HOIs%29%2C%20enabling%20a%203D%20human%20model%20to%20realistically%20interact%20with%0Aany%20given%20object%20based%20on%20a%20textual%20description.%20This%20task%20is%20complicated%20by%0Athe%20varying%20categories%20and%20geometries%20of%20real-world%20objects%20and%20the%20scarcity%20of%0Adatasets%20encompassing%20diverse%20HOIs.%20To%20circumvent%20the%20need%20for%20extensive%20data%2C%0Awe%20leverage%20text-to-image%20diffusion%20models%20trained%20on%20billions%20of%20image-caption%0Apairs.%20We%20optimize%20the%20articulation%20of%20a%20skinned%20human%20mesh%20using%20Score%0ADistillation%20Sampling%20%28SDS%29%20gradients%20obtained%20from%20these%20models%2C%20which%20predict%0Aimage-space%20edits.%20However%2C%20directly%20backpropagating%20image-space%20gradients%20into%0Acomplex%20articulation%20parameters%20is%20ineffective%20due%20to%20the%20local%20nature%20of%20such%0Agradients.%20To%20overcome%20this%2C%20we%20introduce%20a%20dual%20implicit-explicit%0Arepresentation%20of%20a%20skinned%20mesh%2C%20combining%20%28implicit%29%20neural%20radiance%20fields%0A%28NeRFs%29%20with%20%28explicit%29%20skeleton-driven%20mesh%20articulation.%20During%20optimization%2C%0Awe%20transition%20between%20implicit%20and%20explicit%20forms%2C%20grounding%20the%20NeRF%0Ageneration%20while%20refining%20the%20mesh%20articulation.%20We%20validate%20our%20approach%0Athrough%20extensive%20experiments%2C%20demonstrating%20its%20effectiveness%20in%20generating%0Arealistic%20HOIs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08278v1&entry.124074799=Read"},
{"title": "Edge-Wise Graph-Instructed Neural Networks", "author": "Francesco Della Santa and Antonio Mastropietro and Sandra Pieraccini and Francesco Vaccarino", "abstract": "  The problem of multi-task regression over graph nodes has been recently\napproached through Graph-Instructed Neural Network (GINN), which is a promising\narchitecture belonging to the subset of message-passing graph neural networks.\nIn this work, we discuss the limitations of the Graph-Instructed (GI) layer,\nand we formalize a novel edge-wise GI (EWGI) layer. We discuss the advantages\nof the EWGI layer and we provide numerical evidence that EWGINNs perform better\nthan GINNs over graph-structured input data with chaotic connectivity, like the\nones inferred from the Erdos-R\\'enyi graph.\n", "link": "http://arxiv.org/abs/2409.08023v1", "date": "2024-09-12", "relevancy": 2.3634, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5252}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4562}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Edge-Wise%20Graph-Instructed%20Neural%20Networks&body=Title%3A%20Edge-Wise%20Graph-Instructed%20Neural%20Networks%0AAuthor%3A%20Francesco%20Della%20Santa%20and%20Antonio%20Mastropietro%20and%20Sandra%20Pieraccini%20and%20Francesco%20Vaccarino%0AAbstract%3A%20%20%20The%20problem%20of%20multi-task%20regression%20over%20graph%20nodes%20has%20been%20recently%0Aapproached%20through%20Graph-Instructed%20Neural%20Network%20%28GINN%29%2C%20which%20is%20a%20promising%0Aarchitecture%20belonging%20to%20the%20subset%20of%20message-passing%20graph%20neural%20networks.%0AIn%20this%20work%2C%20we%20discuss%20the%20limitations%20of%20the%20Graph-Instructed%20%28GI%29%20layer%2C%0Aand%20we%20formalize%20a%20novel%20edge-wise%20GI%20%28EWGI%29%20layer.%20We%20discuss%20the%20advantages%0Aof%20the%20EWGI%20layer%20and%20we%20provide%20numerical%20evidence%20that%20EWGINNs%20perform%20better%0Athan%20GINNs%20over%20graph-structured%20input%20data%20with%20chaotic%20connectivity%2C%20like%20the%0Aones%20inferred%20from%20the%20Erdos-R%5C%27enyi%20graph.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08023v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEdge-Wise%2520Graph-Instructed%2520Neural%2520Networks%26entry.906535625%3DFrancesco%2520Della%2520Santa%2520and%2520Antonio%2520Mastropietro%2520and%2520Sandra%2520Pieraccini%2520and%2520Francesco%2520Vaccarino%26entry.1292438233%3D%2520%2520The%2520problem%2520of%2520multi-task%2520regression%2520over%2520graph%2520nodes%2520has%2520been%2520recently%250Aapproached%2520through%2520Graph-Instructed%2520Neural%2520Network%2520%2528GINN%2529%252C%2520which%2520is%2520a%2520promising%250Aarchitecture%2520belonging%2520to%2520the%2520subset%2520of%2520message-passing%2520graph%2520neural%2520networks.%250AIn%2520this%2520work%252C%2520we%2520discuss%2520the%2520limitations%2520of%2520the%2520Graph-Instructed%2520%2528GI%2529%2520layer%252C%250Aand%2520we%2520formalize%2520a%2520novel%2520edge-wise%2520GI%2520%2528EWGI%2529%2520layer.%2520We%2520discuss%2520the%2520advantages%250Aof%2520the%2520EWGI%2520layer%2520and%2520we%2520provide%2520numerical%2520evidence%2520that%2520EWGINNs%2520perform%2520better%250Athan%2520GINNs%2520over%2520graph-structured%2520input%2520data%2520with%2520chaotic%2520connectivity%252C%2520like%2520the%250Aones%2520inferred%2520from%2520the%2520Erdos-R%255C%2527enyi%2520graph.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08023v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Edge-Wise%20Graph-Instructed%20Neural%20Networks&entry.906535625=Francesco%20Della%20Santa%20and%20Antonio%20Mastropietro%20and%20Sandra%20Pieraccini%20and%20Francesco%20Vaccarino&entry.1292438233=%20%20The%20problem%20of%20multi-task%20regression%20over%20graph%20nodes%20has%20been%20recently%0Aapproached%20through%20Graph-Instructed%20Neural%20Network%20%28GINN%29%2C%20which%20is%20a%20promising%0Aarchitecture%20belonging%20to%20the%20subset%20of%20message-passing%20graph%20neural%20networks.%0AIn%20this%20work%2C%20we%20discuss%20the%20limitations%20of%20the%20Graph-Instructed%20%28GI%29%20layer%2C%0Aand%20we%20formalize%20a%20novel%20edge-wise%20GI%20%28EWGI%29%20layer.%20We%20discuss%20the%20advantages%0Aof%20the%20EWGI%20layer%20and%20we%20provide%20numerical%20evidence%20that%20EWGINNs%20perform%20better%0Athan%20GINNs%20over%20graph-structured%20input%20data%20with%20chaotic%20connectivity%2C%20like%20the%0Aones%20inferred%20from%20the%20Erdos-R%5C%27enyi%20graph.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08023v1&entry.124074799=Read"},
{"title": "LLM Honeypot: Leveraging Large Language Models as Advanced Interactive\n  Honeypot Systems", "author": "Hakan T. Otal and M. Abdullah Canbaz", "abstract": "  The rapid evolution of cyber threats necessitates innovative solutions for\ndetecting and analyzing malicious activity. Honeypots, which are decoy systems\ndesigned to lure and interact with attackers, have emerged as a critical\ncomponent in cybersecurity. In this paper, we present a novel approach to\ncreating realistic and interactive honeypot systems using Large Language Models\n(LLMs). By fine-tuning a pre-trained open-source language model on a diverse\ndataset of attacker-generated commands and responses, we developed a honeypot\ncapable of sophisticated engagement with attackers. Our methodology involved\nseveral key steps: data collection and processing, prompt engineering, model\nselection, and supervised fine-tuning to optimize the model's performance.\nEvaluation through similarity metrics and live deployment demonstrated that our\napproach effectively generates accurate and informative responses. The results\nhighlight the potential of LLMs to revolutionize honeypot technology, providing\ncybersecurity professionals with a powerful tool to detect and analyze\nmalicious activity, thereby enhancing overall security infrastructure.\n", "link": "http://arxiv.org/abs/2409.08234v1", "date": "2024-09-12", "relevancy": 2.3514, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4736}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4736}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4636}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM%20Honeypot%3A%20Leveraging%20Large%20Language%20Models%20as%20Advanced%20Interactive%0A%20%20Honeypot%20Systems&body=Title%3A%20LLM%20Honeypot%3A%20Leveraging%20Large%20Language%20Models%20as%20Advanced%20Interactive%0A%20%20Honeypot%20Systems%0AAuthor%3A%20Hakan%20T.%20Otal%20and%20M.%20Abdullah%20Canbaz%0AAbstract%3A%20%20%20The%20rapid%20evolution%20of%20cyber%20threats%20necessitates%20innovative%20solutions%20for%0Adetecting%20and%20analyzing%20malicious%20activity.%20Honeypots%2C%20which%20are%20decoy%20systems%0Adesigned%20to%20lure%20and%20interact%20with%20attackers%2C%20have%20emerged%20as%20a%20critical%0Acomponent%20in%20cybersecurity.%20In%20this%20paper%2C%20we%20present%20a%20novel%20approach%20to%0Acreating%20realistic%20and%20interactive%20honeypot%20systems%20using%20Large%20Language%20Models%0A%28LLMs%29.%20By%20fine-tuning%20a%20pre-trained%20open-source%20language%20model%20on%20a%20diverse%0Adataset%20of%20attacker-generated%20commands%20and%20responses%2C%20we%20developed%20a%20honeypot%0Acapable%20of%20sophisticated%20engagement%20with%20attackers.%20Our%20methodology%20involved%0Aseveral%20key%20steps%3A%20data%20collection%20and%20processing%2C%20prompt%20engineering%2C%20model%0Aselection%2C%20and%20supervised%20fine-tuning%20to%20optimize%20the%20model%27s%20performance.%0AEvaluation%20through%20similarity%20metrics%20and%20live%20deployment%20demonstrated%20that%20our%0Aapproach%20effectively%20generates%20accurate%20and%20informative%20responses.%20The%20results%0Ahighlight%20the%20potential%20of%20LLMs%20to%20revolutionize%20honeypot%20technology%2C%20providing%0Acybersecurity%20professionals%20with%20a%20powerful%20tool%20to%20detect%20and%20analyze%0Amalicious%20activity%2C%20thereby%20enhancing%20overall%20security%20infrastructure.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08234v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM%2520Honeypot%253A%2520Leveraging%2520Large%2520Language%2520Models%2520as%2520Advanced%2520Interactive%250A%2520%2520Honeypot%2520Systems%26entry.906535625%3DHakan%2520T.%2520Otal%2520and%2520M.%2520Abdullah%2520Canbaz%26entry.1292438233%3D%2520%2520The%2520rapid%2520evolution%2520of%2520cyber%2520threats%2520necessitates%2520innovative%2520solutions%2520for%250Adetecting%2520and%2520analyzing%2520malicious%2520activity.%2520Honeypots%252C%2520which%2520are%2520decoy%2520systems%250Adesigned%2520to%2520lure%2520and%2520interact%2520with%2520attackers%252C%2520have%2520emerged%2520as%2520a%2520critical%250Acomponent%2520in%2520cybersecurity.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520approach%2520to%250Acreating%2520realistic%2520and%2520interactive%2520honeypot%2520systems%2520using%2520Large%2520Language%2520Models%250A%2528LLMs%2529.%2520By%2520fine-tuning%2520a%2520pre-trained%2520open-source%2520language%2520model%2520on%2520a%2520diverse%250Adataset%2520of%2520attacker-generated%2520commands%2520and%2520responses%252C%2520we%2520developed%2520a%2520honeypot%250Acapable%2520of%2520sophisticated%2520engagement%2520with%2520attackers.%2520Our%2520methodology%2520involved%250Aseveral%2520key%2520steps%253A%2520data%2520collection%2520and%2520processing%252C%2520prompt%2520engineering%252C%2520model%250Aselection%252C%2520and%2520supervised%2520fine-tuning%2520to%2520optimize%2520the%2520model%2527s%2520performance.%250AEvaluation%2520through%2520similarity%2520metrics%2520and%2520live%2520deployment%2520demonstrated%2520that%2520our%250Aapproach%2520effectively%2520generates%2520accurate%2520and%2520informative%2520responses.%2520The%2520results%250Ahighlight%2520the%2520potential%2520of%2520LLMs%2520to%2520revolutionize%2520honeypot%2520technology%252C%2520providing%250Acybersecurity%2520professionals%2520with%2520a%2520powerful%2520tool%2520to%2520detect%2520and%2520analyze%250Amalicious%2520activity%252C%2520thereby%2520enhancing%2520overall%2520security%2520infrastructure.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08234v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM%20Honeypot%3A%20Leveraging%20Large%20Language%20Models%20as%20Advanced%20Interactive%0A%20%20Honeypot%20Systems&entry.906535625=Hakan%20T.%20Otal%20and%20M.%20Abdullah%20Canbaz&entry.1292438233=%20%20The%20rapid%20evolution%20of%20cyber%20threats%20necessitates%20innovative%20solutions%20for%0Adetecting%20and%20analyzing%20malicious%20activity.%20Honeypots%2C%20which%20are%20decoy%20systems%0Adesigned%20to%20lure%20and%20interact%20with%20attackers%2C%20have%20emerged%20as%20a%20critical%0Acomponent%20in%20cybersecurity.%20In%20this%20paper%2C%20we%20present%20a%20novel%20approach%20to%0Acreating%20realistic%20and%20interactive%20honeypot%20systems%20using%20Large%20Language%20Models%0A%28LLMs%29.%20By%20fine-tuning%20a%20pre-trained%20open-source%20language%20model%20on%20a%20diverse%0Adataset%20of%20attacker-generated%20commands%20and%20responses%2C%20we%20developed%20a%20honeypot%0Acapable%20of%20sophisticated%20engagement%20with%20attackers.%20Our%20methodology%20involved%0Aseveral%20key%20steps%3A%20data%20collection%20and%20processing%2C%20prompt%20engineering%2C%20model%0Aselection%2C%20and%20supervised%20fine-tuning%20to%20optimize%20the%20model%27s%20performance.%0AEvaluation%20through%20similarity%20metrics%20and%20live%20deployment%20demonstrated%20that%20our%0Aapproach%20effectively%20generates%20accurate%20and%20informative%20responses.%20The%20results%0Ahighlight%20the%20potential%20of%20LLMs%20to%20revolutionize%20honeypot%20technology%2C%20providing%0Acybersecurity%20professionals%20with%20a%20powerful%20tool%20to%20detect%20and%20analyze%0Amalicious%20activity%2C%20thereby%20enhancing%20overall%20security%20infrastructure.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08234v1&entry.124074799=Read"},
{"title": "From Sim-to-Real: Toward General Event-based Low-light Frame\n  Interpolation with Per-scene Optimization", "author": "Ziran Zhang and Yongrui Ma and Yueting Chen and Feng Zhang and Jinwei Gu and Tianfan Xue and Shi Guo", "abstract": "  Video Frame Interpolation (VFI) is important for video enhancement, frame\nrate up-conversion, and slow-motion generation. The introduction of event\ncameras, which capture per-pixel brightness changes asynchronously, has\nsignificantly enhanced VFI capabilities, particularly for high-speed, nonlinear\nmotions. However, these event-based methods encounter challenges in low-light\nconditions, notably trailing artifacts and signal latency, which hinder their\ndirect applicability and generalization. Addressing these issues, we propose a\nnovel per-scene optimization strategy tailored for low-light conditions. This\napproach utilizes the internal statistics of a sequence to handle degraded\nevent data under low-light conditions, improving the generalizability to\ndifferent lighting and camera settings. To evaluate its robustness in low-light\ncondition, we further introduce EVFI-LL, a unique RGB+Event dataset captured\nunder low-light conditions. Our results demonstrate state-of-the-art\nperformance in low-light environments. Project page:\nhttps://naturezhanghn.github.io/sim2real.\n", "link": "http://arxiv.org/abs/2406.08090v2", "date": "2024-09-12", "relevancy": 2.3494, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6177}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5677}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Sim-to-Real%3A%20Toward%20General%20Event-based%20Low-light%20Frame%0A%20%20Interpolation%20with%20Per-scene%20Optimization&body=Title%3A%20From%20Sim-to-Real%3A%20Toward%20General%20Event-based%20Low-light%20Frame%0A%20%20Interpolation%20with%20Per-scene%20Optimization%0AAuthor%3A%20Ziran%20Zhang%20and%20Yongrui%20Ma%20and%20Yueting%20Chen%20and%20Feng%20Zhang%20and%20Jinwei%20Gu%20and%20Tianfan%20Xue%20and%20Shi%20Guo%0AAbstract%3A%20%20%20Video%20Frame%20Interpolation%20%28VFI%29%20is%20important%20for%20video%20enhancement%2C%20frame%0Arate%20up-conversion%2C%20and%20slow-motion%20generation.%20The%20introduction%20of%20event%0Acameras%2C%20which%20capture%20per-pixel%20brightness%20changes%20asynchronously%2C%20has%0Asignificantly%20enhanced%20VFI%20capabilities%2C%20particularly%20for%20high-speed%2C%20nonlinear%0Amotions.%20However%2C%20these%20event-based%20methods%20encounter%20challenges%20in%20low-light%0Aconditions%2C%20notably%20trailing%20artifacts%20and%20signal%20latency%2C%20which%20hinder%20their%0Adirect%20applicability%20and%20generalization.%20Addressing%20these%20issues%2C%20we%20propose%20a%0Anovel%20per-scene%20optimization%20strategy%20tailored%20for%20low-light%20conditions.%20This%0Aapproach%20utilizes%20the%20internal%20statistics%20of%20a%20sequence%20to%20handle%20degraded%0Aevent%20data%20under%20low-light%20conditions%2C%20improving%20the%20generalizability%20to%0Adifferent%20lighting%20and%20camera%20settings.%20To%20evaluate%20its%20robustness%20in%20low-light%0Acondition%2C%20we%20further%20introduce%20EVFI-LL%2C%20a%20unique%20RGB%2BEvent%20dataset%20captured%0Aunder%20low-light%20conditions.%20Our%20results%20demonstrate%20state-of-the-art%0Aperformance%20in%20low-light%20environments.%20Project%20page%3A%0Ahttps%3A//naturezhanghn.github.io/sim2real.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08090v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Sim-to-Real%253A%2520Toward%2520General%2520Event-based%2520Low-light%2520Frame%250A%2520%2520Interpolation%2520with%2520Per-scene%2520Optimization%26entry.906535625%3DZiran%2520Zhang%2520and%2520Yongrui%2520Ma%2520and%2520Yueting%2520Chen%2520and%2520Feng%2520Zhang%2520and%2520Jinwei%2520Gu%2520and%2520Tianfan%2520Xue%2520and%2520Shi%2520Guo%26entry.1292438233%3D%2520%2520Video%2520Frame%2520Interpolation%2520%2528VFI%2529%2520is%2520important%2520for%2520video%2520enhancement%252C%2520frame%250Arate%2520up-conversion%252C%2520and%2520slow-motion%2520generation.%2520The%2520introduction%2520of%2520event%250Acameras%252C%2520which%2520capture%2520per-pixel%2520brightness%2520changes%2520asynchronously%252C%2520has%250Asignificantly%2520enhanced%2520VFI%2520capabilities%252C%2520particularly%2520for%2520high-speed%252C%2520nonlinear%250Amotions.%2520However%252C%2520these%2520event-based%2520methods%2520encounter%2520challenges%2520in%2520low-light%250Aconditions%252C%2520notably%2520trailing%2520artifacts%2520and%2520signal%2520latency%252C%2520which%2520hinder%2520their%250Adirect%2520applicability%2520and%2520generalization.%2520Addressing%2520these%2520issues%252C%2520we%2520propose%2520a%250Anovel%2520per-scene%2520optimization%2520strategy%2520tailored%2520for%2520low-light%2520conditions.%2520This%250Aapproach%2520utilizes%2520the%2520internal%2520statistics%2520of%2520a%2520sequence%2520to%2520handle%2520degraded%250Aevent%2520data%2520under%2520low-light%2520conditions%252C%2520improving%2520the%2520generalizability%2520to%250Adifferent%2520lighting%2520and%2520camera%2520settings.%2520To%2520evaluate%2520its%2520robustness%2520in%2520low-light%250Acondition%252C%2520we%2520further%2520introduce%2520EVFI-LL%252C%2520a%2520unique%2520RGB%252BEvent%2520dataset%2520captured%250Aunder%2520low-light%2520conditions.%2520Our%2520results%2520demonstrate%2520state-of-the-art%250Aperformance%2520in%2520low-light%2520environments.%2520Project%2520page%253A%250Ahttps%253A//naturezhanghn.github.io/sim2real.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08090v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Sim-to-Real%3A%20Toward%20General%20Event-based%20Low-light%20Frame%0A%20%20Interpolation%20with%20Per-scene%20Optimization&entry.906535625=Ziran%20Zhang%20and%20Yongrui%20Ma%20and%20Yueting%20Chen%20and%20Feng%20Zhang%20and%20Jinwei%20Gu%20and%20Tianfan%20Xue%20and%20Shi%20Guo&entry.1292438233=%20%20Video%20Frame%20Interpolation%20%28VFI%29%20is%20important%20for%20video%20enhancement%2C%20frame%0Arate%20up-conversion%2C%20and%20slow-motion%20generation.%20The%20introduction%20of%20event%0Acameras%2C%20which%20capture%20per-pixel%20brightness%20changes%20asynchronously%2C%20has%0Asignificantly%20enhanced%20VFI%20capabilities%2C%20particularly%20for%20high-speed%2C%20nonlinear%0Amotions.%20However%2C%20these%20event-based%20methods%20encounter%20challenges%20in%20low-light%0Aconditions%2C%20notably%20trailing%20artifacts%20and%20signal%20latency%2C%20which%20hinder%20their%0Adirect%20applicability%20and%20generalization.%20Addressing%20these%20issues%2C%20we%20propose%20a%0Anovel%20per-scene%20optimization%20strategy%20tailored%20for%20low-light%20conditions.%20This%0Aapproach%20utilizes%20the%20internal%20statistics%20of%20a%20sequence%20to%20handle%20degraded%0Aevent%20data%20under%20low-light%20conditions%2C%20improving%20the%20generalizability%20to%0Adifferent%20lighting%20and%20camera%20settings.%20To%20evaluate%20its%20robustness%20in%20low-light%0Acondition%2C%20we%20further%20introduce%20EVFI-LL%2C%20a%20unique%20RGB%2BEvent%20dataset%20captured%0Aunder%20low-light%20conditions.%20Our%20results%20demonstrate%20state-of-the-art%0Aperformance%20in%20low-light%20environments.%20Project%20page%3A%0Ahttps%3A//naturezhanghn.github.io/sim2real.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08090v2&entry.124074799=Read"},
{"title": "Unified Domain Adaptive Semantic Segmentation", "author": "Zhe Zhang and Gaochang Wu and Jing Zhang and Xiatian Zhu and Dacheng Tao and Tianyou Chai", "abstract": "  Unsupervised Domain Adaptive Semantic Segmentation (UDA-SS) aims to transfer\nthe supervision from a labeled source domain to an unlabeled target domain. The\nmajority of existing UDA-SS works typically consider images whilst recent\nattempts have extended further to tackle videos by modeling the temporal\ndimension. Although the two lines of research share the major challenges --\novercoming the underlying domain distribution shift, their studies are largely\nindependent, resulting in fragmented insights, a lack of holistic\nunderstanding, and missed opportunities for cross-pollination of ideas. This\nfragmentation prevents the unification of methods, leading to redundant efforts\nand suboptimal knowledge transfer across image and video domains. Under this\nobservation, we advocate unifying the study of UDA-SS across video and image\nscenarios, enabling a more comprehensive understanding, synergistic\nadvancements, and efficient knowledge sharing. To that end, we explore the\nunified UDA-SS from a general data augmentation perspective, serving as a\nunifying conceptual framework, enabling improved generalization, and potential\nfor cross-pollination of ideas, ultimately contributing to the overall progress\nand practical impact of this field of research. Specifically, we propose a\nQuad-directional Mixup (QuadMix) method, characterized by tackling distinct\npoint attributes and feature inconsistencies through four-directional paths for\nintra- and inter-domain mixing in a feature space. To deal with temporal shifts\nwith videos, we incorporate optical flow-guided feature aggregation across\nspatial and temporal dimensions for fine-grained domain alignment. Extensive\nexperiments show that our method outperforms the state-of-the-art works by\nlarge margins on four challenging UDA-SS benchmarks. Our source code and models\nwill be released at \\url{https://github.com/ZHE-SAPI/UDASS}.\n", "link": "http://arxiv.org/abs/2311.13254v3", "date": "2024-09-12", "relevancy": 2.3159, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5876}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5794}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20Domain%20Adaptive%20Semantic%20Segmentation&body=Title%3A%20Unified%20Domain%20Adaptive%20Semantic%20Segmentation%0AAuthor%3A%20Zhe%20Zhang%20and%20Gaochang%20Wu%20and%20Jing%20Zhang%20and%20Xiatian%20Zhu%20and%20Dacheng%20Tao%20and%20Tianyou%20Chai%0AAbstract%3A%20%20%20Unsupervised%20Domain%20Adaptive%20Semantic%20Segmentation%20%28UDA-SS%29%20aims%20to%20transfer%0Athe%20supervision%20from%20a%20labeled%20source%20domain%20to%20an%20unlabeled%20target%20domain.%20The%0Amajority%20of%20existing%20UDA-SS%20works%20typically%20consider%20images%20whilst%20recent%0Aattempts%20have%20extended%20further%20to%20tackle%20videos%20by%20modeling%20the%20temporal%0Adimension.%20Although%20the%20two%20lines%20of%20research%20share%20the%20major%20challenges%20--%0Aovercoming%20the%20underlying%20domain%20distribution%20shift%2C%20their%20studies%20are%20largely%0Aindependent%2C%20resulting%20in%20fragmented%20insights%2C%20a%20lack%20of%20holistic%0Aunderstanding%2C%20and%20missed%20opportunities%20for%20cross-pollination%20of%20ideas.%20This%0Afragmentation%20prevents%20the%20unification%20of%20methods%2C%20leading%20to%20redundant%20efforts%0Aand%20suboptimal%20knowledge%20transfer%20across%20image%20and%20video%20domains.%20Under%20this%0Aobservation%2C%20we%20advocate%20unifying%20the%20study%20of%20UDA-SS%20across%20video%20and%20image%0Ascenarios%2C%20enabling%20a%20more%20comprehensive%20understanding%2C%20synergistic%0Aadvancements%2C%20and%20efficient%20knowledge%20sharing.%20To%20that%20end%2C%20we%20explore%20the%0Aunified%20UDA-SS%20from%20a%20general%20data%20augmentation%20perspective%2C%20serving%20as%20a%0Aunifying%20conceptual%20framework%2C%20enabling%20improved%20generalization%2C%20and%20potential%0Afor%20cross-pollination%20of%20ideas%2C%20ultimately%20contributing%20to%20the%20overall%20progress%0Aand%20practical%20impact%20of%20this%20field%20of%20research.%20Specifically%2C%20we%20propose%20a%0AQuad-directional%20Mixup%20%28QuadMix%29%20method%2C%20characterized%20by%20tackling%20distinct%0Apoint%20attributes%20and%20feature%20inconsistencies%20through%20four-directional%20paths%20for%0Aintra-%20and%20inter-domain%20mixing%20in%20a%20feature%20space.%20To%20deal%20with%20temporal%20shifts%0Awith%20videos%2C%20we%20incorporate%20optical%20flow-guided%20feature%20aggregation%20across%0Aspatial%20and%20temporal%20dimensions%20for%20fine-grained%20domain%20alignment.%20Extensive%0Aexperiments%20show%20that%20our%20method%20outperforms%20the%20state-of-the-art%20works%20by%0Alarge%20margins%20on%20four%20challenging%20UDA-SS%20benchmarks.%20Our%20source%20code%20and%20models%0Awill%20be%20released%20at%20%5Curl%7Bhttps%3A//github.com/ZHE-SAPI/UDASS%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.13254v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520Domain%2520Adaptive%2520Semantic%2520Segmentation%26entry.906535625%3DZhe%2520Zhang%2520and%2520Gaochang%2520Wu%2520and%2520Jing%2520Zhang%2520and%2520Xiatian%2520Zhu%2520and%2520Dacheng%2520Tao%2520and%2520Tianyou%2520Chai%26entry.1292438233%3D%2520%2520Unsupervised%2520Domain%2520Adaptive%2520Semantic%2520Segmentation%2520%2528UDA-SS%2529%2520aims%2520to%2520transfer%250Athe%2520supervision%2520from%2520a%2520labeled%2520source%2520domain%2520to%2520an%2520unlabeled%2520target%2520domain.%2520The%250Amajority%2520of%2520existing%2520UDA-SS%2520works%2520typically%2520consider%2520images%2520whilst%2520recent%250Aattempts%2520have%2520extended%2520further%2520to%2520tackle%2520videos%2520by%2520modeling%2520the%2520temporal%250Adimension.%2520Although%2520the%2520two%2520lines%2520of%2520research%2520share%2520the%2520major%2520challenges%2520--%250Aovercoming%2520the%2520underlying%2520domain%2520distribution%2520shift%252C%2520their%2520studies%2520are%2520largely%250Aindependent%252C%2520resulting%2520in%2520fragmented%2520insights%252C%2520a%2520lack%2520of%2520holistic%250Aunderstanding%252C%2520and%2520missed%2520opportunities%2520for%2520cross-pollination%2520of%2520ideas.%2520This%250Afragmentation%2520prevents%2520the%2520unification%2520of%2520methods%252C%2520leading%2520to%2520redundant%2520efforts%250Aand%2520suboptimal%2520knowledge%2520transfer%2520across%2520image%2520and%2520video%2520domains.%2520Under%2520this%250Aobservation%252C%2520we%2520advocate%2520unifying%2520the%2520study%2520of%2520UDA-SS%2520across%2520video%2520and%2520image%250Ascenarios%252C%2520enabling%2520a%2520more%2520comprehensive%2520understanding%252C%2520synergistic%250Aadvancements%252C%2520and%2520efficient%2520knowledge%2520sharing.%2520To%2520that%2520end%252C%2520we%2520explore%2520the%250Aunified%2520UDA-SS%2520from%2520a%2520general%2520data%2520augmentation%2520perspective%252C%2520serving%2520as%2520a%250Aunifying%2520conceptual%2520framework%252C%2520enabling%2520improved%2520generalization%252C%2520and%2520potential%250Afor%2520cross-pollination%2520of%2520ideas%252C%2520ultimately%2520contributing%2520to%2520the%2520overall%2520progress%250Aand%2520practical%2520impact%2520of%2520this%2520field%2520of%2520research.%2520Specifically%252C%2520we%2520propose%2520a%250AQuad-directional%2520Mixup%2520%2528QuadMix%2529%2520method%252C%2520characterized%2520by%2520tackling%2520distinct%250Apoint%2520attributes%2520and%2520feature%2520inconsistencies%2520through%2520four-directional%2520paths%2520for%250Aintra-%2520and%2520inter-domain%2520mixing%2520in%2520a%2520feature%2520space.%2520To%2520deal%2520with%2520temporal%2520shifts%250Awith%2520videos%252C%2520we%2520incorporate%2520optical%2520flow-guided%2520feature%2520aggregation%2520across%250Aspatial%2520and%2520temporal%2520dimensions%2520for%2520fine-grained%2520domain%2520alignment.%2520Extensive%250Aexperiments%2520show%2520that%2520our%2520method%2520outperforms%2520the%2520state-of-the-art%2520works%2520by%250Alarge%2520margins%2520on%2520four%2520challenging%2520UDA-SS%2520benchmarks.%2520Our%2520source%2520code%2520and%2520models%250Awill%2520be%2520released%2520at%2520%255Curl%257Bhttps%253A//github.com/ZHE-SAPI/UDASS%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.13254v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Domain%20Adaptive%20Semantic%20Segmentation&entry.906535625=Zhe%20Zhang%20and%20Gaochang%20Wu%20and%20Jing%20Zhang%20and%20Xiatian%20Zhu%20and%20Dacheng%20Tao%20and%20Tianyou%20Chai&entry.1292438233=%20%20Unsupervised%20Domain%20Adaptive%20Semantic%20Segmentation%20%28UDA-SS%29%20aims%20to%20transfer%0Athe%20supervision%20from%20a%20labeled%20source%20domain%20to%20an%20unlabeled%20target%20domain.%20The%0Amajority%20of%20existing%20UDA-SS%20works%20typically%20consider%20images%20whilst%20recent%0Aattempts%20have%20extended%20further%20to%20tackle%20videos%20by%20modeling%20the%20temporal%0Adimension.%20Although%20the%20two%20lines%20of%20research%20share%20the%20major%20challenges%20--%0Aovercoming%20the%20underlying%20domain%20distribution%20shift%2C%20their%20studies%20are%20largely%0Aindependent%2C%20resulting%20in%20fragmented%20insights%2C%20a%20lack%20of%20holistic%0Aunderstanding%2C%20and%20missed%20opportunities%20for%20cross-pollination%20of%20ideas.%20This%0Afragmentation%20prevents%20the%20unification%20of%20methods%2C%20leading%20to%20redundant%20efforts%0Aand%20suboptimal%20knowledge%20transfer%20across%20image%20and%20video%20domains.%20Under%20this%0Aobservation%2C%20we%20advocate%20unifying%20the%20study%20of%20UDA-SS%20across%20video%20and%20image%0Ascenarios%2C%20enabling%20a%20more%20comprehensive%20understanding%2C%20synergistic%0Aadvancements%2C%20and%20efficient%20knowledge%20sharing.%20To%20that%20end%2C%20we%20explore%20the%0Aunified%20UDA-SS%20from%20a%20general%20data%20augmentation%20perspective%2C%20serving%20as%20a%0Aunifying%20conceptual%20framework%2C%20enabling%20improved%20generalization%2C%20and%20potential%0Afor%20cross-pollination%20of%20ideas%2C%20ultimately%20contributing%20to%20the%20overall%20progress%0Aand%20practical%20impact%20of%20this%20field%20of%20research.%20Specifically%2C%20we%20propose%20a%0AQuad-directional%20Mixup%20%28QuadMix%29%20method%2C%20characterized%20by%20tackling%20distinct%0Apoint%20attributes%20and%20feature%20inconsistencies%20through%20four-directional%20paths%20for%0Aintra-%20and%20inter-domain%20mixing%20in%20a%20feature%20space.%20To%20deal%20with%20temporal%20shifts%0Awith%20videos%2C%20we%20incorporate%20optical%20flow-guided%20feature%20aggregation%20across%0Aspatial%20and%20temporal%20dimensions%20for%20fine-grained%20domain%20alignment.%20Extensive%0Aexperiments%20show%20that%20our%20method%20outperforms%20the%20state-of-the-art%20works%20by%0Alarge%20margins%20on%20four%20challenging%20UDA-SS%20benchmarks.%20Our%20source%20code%20and%20models%0Awill%20be%20released%20at%20%5Curl%7Bhttps%3A//github.com/ZHE-SAPI/UDASS%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.13254v3&entry.124074799=Read"},
{"title": "CliquePH: Higher-Order Information for Graph Neural Networks through\n  Persistent Homology on Clique Graphs", "author": "Davide Buffelli and Farzin Soleymani and Bastian Rieck", "abstract": "  Graph neural networks have become the default choice by practitioners for\ngraph learning tasks such as graph classification and node classification.\nNevertheless, popular graph neural network models still struggle to capture\nhigher-order information, i.e., information that goes \\emph{beyond} pairwise\ninteractions. Recent work has shown that persistent homology, a tool from\ntopological data analysis, can enrich graph neural networks with topological\ninformation that they otherwise could not capture. Calculating such features is\nefficient for dimension 0 (connected components) and dimension 1 (cycles).\nHowever, when it comes to higher-order structures, it does not scale well, with\na complexity of $O(n^d)$, where $n$ is the number of nodes and $d$ is the order\nof the structures. In this work, we introduce a novel method that extracts\ninformation about higher-order structures in the graph while still using the\nefficient low-dimensional persistent homology algorithm. On standard benchmark\ndatasets, we show that our method can lead to up to $31\\%$ improvements in test\naccuracy.\n", "link": "http://arxiv.org/abs/2409.08217v1", "date": "2024-09-12", "relevancy": 2.3152, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.481}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4556}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CliquePH%3A%20Higher-Order%20Information%20for%20Graph%20Neural%20Networks%20through%0A%20%20Persistent%20Homology%20on%20Clique%20Graphs&body=Title%3A%20CliquePH%3A%20Higher-Order%20Information%20for%20Graph%20Neural%20Networks%20through%0A%20%20Persistent%20Homology%20on%20Clique%20Graphs%0AAuthor%3A%20Davide%20Buffelli%20and%20Farzin%20Soleymani%20and%20Bastian%20Rieck%0AAbstract%3A%20%20%20Graph%20neural%20networks%20have%20become%20the%20default%20choice%20by%20practitioners%20for%0Agraph%20learning%20tasks%20such%20as%20graph%20classification%20and%20node%20classification.%0ANevertheless%2C%20popular%20graph%20neural%20network%20models%20still%20struggle%20to%20capture%0Ahigher-order%20information%2C%20i.e.%2C%20information%20that%20goes%20%5Cemph%7Bbeyond%7D%20pairwise%0Ainteractions.%20Recent%20work%20has%20shown%20that%20persistent%20homology%2C%20a%20tool%20from%0Atopological%20data%20analysis%2C%20can%20enrich%20graph%20neural%20networks%20with%20topological%0Ainformation%20that%20they%20otherwise%20could%20not%20capture.%20Calculating%20such%20features%20is%0Aefficient%20for%20dimension%200%20%28connected%20components%29%20and%20dimension%201%20%28cycles%29.%0AHowever%2C%20when%20it%20comes%20to%20higher-order%20structures%2C%20it%20does%20not%20scale%20well%2C%20with%0Aa%20complexity%20of%20%24O%28n%5Ed%29%24%2C%20where%20%24n%24%20is%20the%20number%20of%20nodes%20and%20%24d%24%20is%20the%20order%0Aof%20the%20structures.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20method%20that%20extracts%0Ainformation%20about%20higher-order%20structures%20in%20the%20graph%20while%20still%20using%20the%0Aefficient%20low-dimensional%20persistent%20homology%20algorithm.%20On%20standard%20benchmark%0Adatasets%2C%20we%20show%20that%20our%20method%20can%20lead%20to%20up%20to%20%2431%5C%25%24%20improvements%20in%20test%0Aaccuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08217v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCliquePH%253A%2520Higher-Order%2520Information%2520for%2520Graph%2520Neural%2520Networks%2520through%250A%2520%2520Persistent%2520Homology%2520on%2520Clique%2520Graphs%26entry.906535625%3DDavide%2520Buffelli%2520and%2520Farzin%2520Soleymani%2520and%2520Bastian%2520Rieck%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520have%2520become%2520the%2520default%2520choice%2520by%2520practitioners%2520for%250Agraph%2520learning%2520tasks%2520such%2520as%2520graph%2520classification%2520and%2520node%2520classification.%250ANevertheless%252C%2520popular%2520graph%2520neural%2520network%2520models%2520still%2520struggle%2520to%2520capture%250Ahigher-order%2520information%252C%2520i.e.%252C%2520information%2520that%2520goes%2520%255Cemph%257Bbeyond%257D%2520pairwise%250Ainteractions.%2520Recent%2520work%2520has%2520shown%2520that%2520persistent%2520homology%252C%2520a%2520tool%2520from%250Atopological%2520data%2520analysis%252C%2520can%2520enrich%2520graph%2520neural%2520networks%2520with%2520topological%250Ainformation%2520that%2520they%2520otherwise%2520could%2520not%2520capture.%2520Calculating%2520such%2520features%2520is%250Aefficient%2520for%2520dimension%25200%2520%2528connected%2520components%2529%2520and%2520dimension%25201%2520%2528cycles%2529.%250AHowever%252C%2520when%2520it%2520comes%2520to%2520higher-order%2520structures%252C%2520it%2520does%2520not%2520scale%2520well%252C%2520with%250Aa%2520complexity%2520of%2520%2524O%2528n%255Ed%2529%2524%252C%2520where%2520%2524n%2524%2520is%2520the%2520number%2520of%2520nodes%2520and%2520%2524d%2524%2520is%2520the%2520order%250Aof%2520the%2520structures.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520method%2520that%2520extracts%250Ainformation%2520about%2520higher-order%2520structures%2520in%2520the%2520graph%2520while%2520still%2520using%2520the%250Aefficient%2520low-dimensional%2520persistent%2520homology%2520algorithm.%2520On%2520standard%2520benchmark%250Adatasets%252C%2520we%2520show%2520that%2520our%2520method%2520can%2520lead%2520to%2520up%2520to%2520%252431%255C%2525%2524%2520improvements%2520in%2520test%250Aaccuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08217v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CliquePH%3A%20Higher-Order%20Information%20for%20Graph%20Neural%20Networks%20through%0A%20%20Persistent%20Homology%20on%20Clique%20Graphs&entry.906535625=Davide%20Buffelli%20and%20Farzin%20Soleymani%20and%20Bastian%20Rieck&entry.1292438233=%20%20Graph%20neural%20networks%20have%20become%20the%20default%20choice%20by%20practitioners%20for%0Agraph%20learning%20tasks%20such%20as%20graph%20classification%20and%20node%20classification.%0ANevertheless%2C%20popular%20graph%20neural%20network%20models%20still%20struggle%20to%20capture%0Ahigher-order%20information%2C%20i.e.%2C%20information%20that%20goes%20%5Cemph%7Bbeyond%7D%20pairwise%0Ainteractions.%20Recent%20work%20has%20shown%20that%20persistent%20homology%2C%20a%20tool%20from%0Atopological%20data%20analysis%2C%20can%20enrich%20graph%20neural%20networks%20with%20topological%0Ainformation%20that%20they%20otherwise%20could%20not%20capture.%20Calculating%20such%20features%20is%0Aefficient%20for%20dimension%200%20%28connected%20components%29%20and%20dimension%201%20%28cycles%29.%0AHowever%2C%20when%20it%20comes%20to%20higher-order%20structures%2C%20it%20does%20not%20scale%20well%2C%20with%0Aa%20complexity%20of%20%24O%28n%5Ed%29%24%2C%20where%20%24n%24%20is%20the%20number%20of%20nodes%20and%20%24d%24%20is%20the%20order%0Aof%20the%20structures.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20method%20that%20extracts%0Ainformation%20about%20higher-order%20structures%20in%20the%20graph%20while%20still%20using%20the%0Aefficient%20low-dimensional%20persistent%20homology%20algorithm.%20On%20standard%20benchmark%0Adatasets%2C%20we%20show%20that%20our%20method%20can%20lead%20to%20up%20to%20%2431%5C%25%24%20improvements%20in%20test%0Aaccuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08217v1&entry.124074799=Read"},
{"title": "Depth Matters: Exploring Deep Interactions of RGB-D for Semantic\n  Segmentation in Traffic Scenes", "author": "Siyu Chen and Ting Han and Changshe Zhang and Weiquan Liu and Jinhe Su and Zongyue Wang and Guorong Cai", "abstract": "  RGB-D has gradually become a crucial data source for understanding complex\nscenes in assisted driving. However, existing studies have paid insufficient\nattention to the intrinsic spatial properties of depth maps. This oversight\nsignificantly impacts the attention representation, leading to prediction\nerrors caused by attention shift issues. To this end, we propose a novel\nlearnable Depth interaction Pyramid Transformer (DiPFormer) to explore the\neffectiveness of depth. Firstly, we introduce Depth Spatial-Aware Optimization\n(Depth SAO) as offset to represent real-world spatial relationships. Secondly,\nthe similarity in the feature space of RGB-D is learned by Depth Linear\nCross-Attention (Depth LCA) to clarify spatial differences at the pixel level.\nFinally, an MLP Decoder is utilized to effectively fuse multi-scale features\nfor meeting real-time requirements. Comprehensive experiments demonstrate that\nthe proposed DiPFormer significantly addresses the issue of attention\nmisalignment in both road detection (+7.5%) and semantic segmentation (+4.9% /\n+1.5%) tasks. DiPFormer achieves state-of-the-art performance on the KITTI\n(97.57% F-score on KITTI road and 68.74% mIoU on KITTI-360) and Cityscapes\n(83.4% mIoU) datasets.\n", "link": "http://arxiv.org/abs/2409.07995v1", "date": "2024-09-12", "relevancy": 2.312, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5784}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5784}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Depth%20Matters%3A%20Exploring%20Deep%20Interactions%20of%20RGB-D%20for%20Semantic%0A%20%20Segmentation%20in%20Traffic%20Scenes&body=Title%3A%20Depth%20Matters%3A%20Exploring%20Deep%20Interactions%20of%20RGB-D%20for%20Semantic%0A%20%20Segmentation%20in%20Traffic%20Scenes%0AAuthor%3A%20Siyu%20Chen%20and%20Ting%20Han%20and%20Changshe%20Zhang%20and%20Weiquan%20Liu%20and%20Jinhe%20Su%20and%20Zongyue%20Wang%20and%20Guorong%20Cai%0AAbstract%3A%20%20%20RGB-D%20has%20gradually%20become%20a%20crucial%20data%20source%20for%20understanding%20complex%0Ascenes%20in%20assisted%20driving.%20However%2C%20existing%20studies%20have%20paid%20insufficient%0Aattention%20to%20the%20intrinsic%20spatial%20properties%20of%20depth%20maps.%20This%20oversight%0Asignificantly%20impacts%20the%20attention%20representation%2C%20leading%20to%20prediction%0Aerrors%20caused%20by%20attention%20shift%20issues.%20To%20this%20end%2C%20we%20propose%20a%20novel%0Alearnable%20Depth%20interaction%20Pyramid%20Transformer%20%28DiPFormer%29%20to%20explore%20the%0Aeffectiveness%20of%20depth.%20Firstly%2C%20we%20introduce%20Depth%20Spatial-Aware%20Optimization%0A%28Depth%20SAO%29%20as%20offset%20to%20represent%20real-world%20spatial%20relationships.%20Secondly%2C%0Athe%20similarity%20in%20the%20feature%20space%20of%20RGB-D%20is%20learned%20by%20Depth%20Linear%0ACross-Attention%20%28Depth%20LCA%29%20to%20clarify%20spatial%20differences%20at%20the%20pixel%20level.%0AFinally%2C%20an%20MLP%20Decoder%20is%20utilized%20to%20effectively%20fuse%20multi-scale%20features%0Afor%20meeting%20real-time%20requirements.%20Comprehensive%20experiments%20demonstrate%20that%0Athe%20proposed%20DiPFormer%20significantly%20addresses%20the%20issue%20of%20attention%0Amisalignment%20in%20both%20road%20detection%20%28%2B7.5%25%29%20and%20semantic%20segmentation%20%28%2B4.9%25%20/%0A%2B1.5%25%29%20tasks.%20DiPFormer%20achieves%20state-of-the-art%20performance%20on%20the%20KITTI%0A%2897.57%25%20F-score%20on%20KITTI%20road%20and%2068.74%25%20mIoU%20on%20KITTI-360%29%20and%20Cityscapes%0A%2883.4%25%20mIoU%29%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07995v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepth%2520Matters%253A%2520Exploring%2520Deep%2520Interactions%2520of%2520RGB-D%2520for%2520Semantic%250A%2520%2520Segmentation%2520in%2520Traffic%2520Scenes%26entry.906535625%3DSiyu%2520Chen%2520and%2520Ting%2520Han%2520and%2520Changshe%2520Zhang%2520and%2520Weiquan%2520Liu%2520and%2520Jinhe%2520Su%2520and%2520Zongyue%2520Wang%2520and%2520Guorong%2520Cai%26entry.1292438233%3D%2520%2520RGB-D%2520has%2520gradually%2520become%2520a%2520crucial%2520data%2520source%2520for%2520understanding%2520complex%250Ascenes%2520in%2520assisted%2520driving.%2520However%252C%2520existing%2520studies%2520have%2520paid%2520insufficient%250Aattention%2520to%2520the%2520intrinsic%2520spatial%2520properties%2520of%2520depth%2520maps.%2520This%2520oversight%250Asignificantly%2520impacts%2520the%2520attention%2520representation%252C%2520leading%2520to%2520prediction%250Aerrors%2520caused%2520by%2520attention%2520shift%2520issues.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520novel%250Alearnable%2520Depth%2520interaction%2520Pyramid%2520Transformer%2520%2528DiPFormer%2529%2520to%2520explore%2520the%250Aeffectiveness%2520of%2520depth.%2520Firstly%252C%2520we%2520introduce%2520Depth%2520Spatial-Aware%2520Optimization%250A%2528Depth%2520SAO%2529%2520as%2520offset%2520to%2520represent%2520real-world%2520spatial%2520relationships.%2520Secondly%252C%250Athe%2520similarity%2520in%2520the%2520feature%2520space%2520of%2520RGB-D%2520is%2520learned%2520by%2520Depth%2520Linear%250ACross-Attention%2520%2528Depth%2520LCA%2529%2520to%2520clarify%2520spatial%2520differences%2520at%2520the%2520pixel%2520level.%250AFinally%252C%2520an%2520MLP%2520Decoder%2520is%2520utilized%2520to%2520effectively%2520fuse%2520multi-scale%2520features%250Afor%2520meeting%2520real-time%2520requirements.%2520Comprehensive%2520experiments%2520demonstrate%2520that%250Athe%2520proposed%2520DiPFormer%2520significantly%2520addresses%2520the%2520issue%2520of%2520attention%250Amisalignment%2520in%2520both%2520road%2520detection%2520%2528%252B7.5%2525%2529%2520and%2520semantic%2520segmentation%2520%2528%252B4.9%2525%2520/%250A%252B1.5%2525%2529%2520tasks.%2520DiPFormer%2520achieves%2520state-of-the-art%2520performance%2520on%2520the%2520KITTI%250A%252897.57%2525%2520F-score%2520on%2520KITTI%2520road%2520and%252068.74%2525%2520mIoU%2520on%2520KITTI-360%2529%2520and%2520Cityscapes%250A%252883.4%2525%2520mIoU%2529%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07995v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Depth%20Matters%3A%20Exploring%20Deep%20Interactions%20of%20RGB-D%20for%20Semantic%0A%20%20Segmentation%20in%20Traffic%20Scenes&entry.906535625=Siyu%20Chen%20and%20Ting%20Han%20and%20Changshe%20Zhang%20and%20Weiquan%20Liu%20and%20Jinhe%20Su%20and%20Zongyue%20Wang%20and%20Guorong%20Cai&entry.1292438233=%20%20RGB-D%20has%20gradually%20become%20a%20crucial%20data%20source%20for%20understanding%20complex%0Ascenes%20in%20assisted%20driving.%20However%2C%20existing%20studies%20have%20paid%20insufficient%0Aattention%20to%20the%20intrinsic%20spatial%20properties%20of%20depth%20maps.%20This%20oversight%0Asignificantly%20impacts%20the%20attention%20representation%2C%20leading%20to%20prediction%0Aerrors%20caused%20by%20attention%20shift%20issues.%20To%20this%20end%2C%20we%20propose%20a%20novel%0Alearnable%20Depth%20interaction%20Pyramid%20Transformer%20%28DiPFormer%29%20to%20explore%20the%0Aeffectiveness%20of%20depth.%20Firstly%2C%20we%20introduce%20Depth%20Spatial-Aware%20Optimization%0A%28Depth%20SAO%29%20as%20offset%20to%20represent%20real-world%20spatial%20relationships.%20Secondly%2C%0Athe%20similarity%20in%20the%20feature%20space%20of%20RGB-D%20is%20learned%20by%20Depth%20Linear%0ACross-Attention%20%28Depth%20LCA%29%20to%20clarify%20spatial%20differences%20at%20the%20pixel%20level.%0AFinally%2C%20an%20MLP%20Decoder%20is%20utilized%20to%20effectively%20fuse%20multi-scale%20features%0Afor%20meeting%20real-time%20requirements.%20Comprehensive%20experiments%20demonstrate%20that%0Athe%20proposed%20DiPFormer%20significantly%20addresses%20the%20issue%20of%20attention%0Amisalignment%20in%20both%20road%20detection%20%28%2B7.5%25%29%20and%20semantic%20segmentation%20%28%2B4.9%25%20/%0A%2B1.5%25%29%20tasks.%20DiPFormer%20achieves%20state-of-the-art%20performance%20on%20the%20KITTI%0A%2897.57%25%20F-score%20on%20KITTI%20road%20and%2068.74%25%20mIoU%20on%20KITTI-360%29%20and%20Cityscapes%0A%2883.4%25%20mIoU%29%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07995v1&entry.124074799=Read"},
{"title": "Localized Schr\u00f6dinger Bridge Sampler", "author": "Georg A. Gottwald and Sebastian Reich", "abstract": "  We consider the generative problem of sampling from an unknown distribution\nfor which only a sufficiently large number of training samples are available.\nIn this paper, we build on previous work combining Schr\\\"odinger bridges and\nLangevin dynamics. A key bottleneck of this approach is the exponential\ndependence of the required training samples on the dimension, $d$, of the\nambient state space. We propose a localization strategy which exploits\nconditional independence of conditional expectation values. Localization thus\nreplaces a single high-dimensional Schr\\\"odinger bridge problem by $d$\nlow-dimensional Schr\\\"odinger bridge problems over the available training\nsamples. As for the original approach, the localized sampler is stable and\ngeometric ergodic. The sampler also naturally extends to conditional sampling\nand to Bayesian inference. We demonstrate the performance of our proposed\nscheme through experiments on a Gaussian problem with increasing dimensions and\non a stochastic subgrid-scale parametrization conditional sampling problem.\n", "link": "http://arxiv.org/abs/2409.07968v1", "date": "2024-09-12", "relevancy": 2.3056, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4624}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.462}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Localized%20Schr%C3%B6dinger%20Bridge%20Sampler&body=Title%3A%20Localized%20Schr%C3%B6dinger%20Bridge%20Sampler%0AAuthor%3A%20Georg%20A.%20Gottwald%20and%20Sebastian%20Reich%0AAbstract%3A%20%20%20We%20consider%20the%20generative%20problem%20of%20sampling%20from%20an%20unknown%20distribution%0Afor%20which%20only%20a%20sufficiently%20large%20number%20of%20training%20samples%20are%20available.%0AIn%20this%20paper%2C%20we%20build%20on%20previous%20work%20combining%20Schr%5C%22odinger%20bridges%20and%0ALangevin%20dynamics.%20A%20key%20bottleneck%20of%20this%20approach%20is%20the%20exponential%0Adependence%20of%20the%20required%20training%20samples%20on%20the%20dimension%2C%20%24d%24%2C%20of%20the%0Aambient%20state%20space.%20We%20propose%20a%20localization%20strategy%20which%20exploits%0Aconditional%20independence%20of%20conditional%20expectation%20values.%20Localization%20thus%0Areplaces%20a%20single%20high-dimensional%20Schr%5C%22odinger%20bridge%20problem%20by%20%24d%24%0Alow-dimensional%20Schr%5C%22odinger%20bridge%20problems%20over%20the%20available%20training%0Asamples.%20As%20for%20the%20original%20approach%2C%20the%20localized%20sampler%20is%20stable%20and%0Ageometric%20ergodic.%20The%20sampler%20also%20naturally%20extends%20to%20conditional%20sampling%0Aand%20to%20Bayesian%20inference.%20We%20demonstrate%20the%20performance%20of%20our%20proposed%0Ascheme%20through%20experiments%20on%20a%20Gaussian%20problem%20with%20increasing%20dimensions%20and%0Aon%20a%20stochastic%20subgrid-scale%20parametrization%20conditional%20sampling%20problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07968v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocalized%2520Schr%25C3%25B6dinger%2520Bridge%2520Sampler%26entry.906535625%3DGeorg%2520A.%2520Gottwald%2520and%2520Sebastian%2520Reich%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520generative%2520problem%2520of%2520sampling%2520from%2520an%2520unknown%2520distribution%250Afor%2520which%2520only%2520a%2520sufficiently%2520large%2520number%2520of%2520training%2520samples%2520are%2520available.%250AIn%2520this%2520paper%252C%2520we%2520build%2520on%2520previous%2520work%2520combining%2520Schr%255C%2522odinger%2520bridges%2520and%250ALangevin%2520dynamics.%2520A%2520key%2520bottleneck%2520of%2520this%2520approach%2520is%2520the%2520exponential%250Adependence%2520of%2520the%2520required%2520training%2520samples%2520on%2520the%2520dimension%252C%2520%2524d%2524%252C%2520of%2520the%250Aambient%2520state%2520space.%2520We%2520propose%2520a%2520localization%2520strategy%2520which%2520exploits%250Aconditional%2520independence%2520of%2520conditional%2520expectation%2520values.%2520Localization%2520thus%250Areplaces%2520a%2520single%2520high-dimensional%2520Schr%255C%2522odinger%2520bridge%2520problem%2520by%2520%2524d%2524%250Alow-dimensional%2520Schr%255C%2522odinger%2520bridge%2520problems%2520over%2520the%2520available%2520training%250Asamples.%2520As%2520for%2520the%2520original%2520approach%252C%2520the%2520localized%2520sampler%2520is%2520stable%2520and%250Ageometric%2520ergodic.%2520The%2520sampler%2520also%2520naturally%2520extends%2520to%2520conditional%2520sampling%250Aand%2520to%2520Bayesian%2520inference.%2520We%2520demonstrate%2520the%2520performance%2520of%2520our%2520proposed%250Ascheme%2520through%2520experiments%2520on%2520a%2520Gaussian%2520problem%2520with%2520increasing%2520dimensions%2520and%250Aon%2520a%2520stochastic%2520subgrid-scale%2520parametrization%2520conditional%2520sampling%2520problem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07968v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Localized%20Schr%C3%B6dinger%20Bridge%20Sampler&entry.906535625=Georg%20A.%20Gottwald%20and%20Sebastian%20Reich&entry.1292438233=%20%20We%20consider%20the%20generative%20problem%20of%20sampling%20from%20an%20unknown%20distribution%0Afor%20which%20only%20a%20sufficiently%20large%20number%20of%20training%20samples%20are%20available.%0AIn%20this%20paper%2C%20we%20build%20on%20previous%20work%20combining%20Schr%5C%22odinger%20bridges%20and%0ALangevin%20dynamics.%20A%20key%20bottleneck%20of%20this%20approach%20is%20the%20exponential%0Adependence%20of%20the%20required%20training%20samples%20on%20the%20dimension%2C%20%24d%24%2C%20of%20the%0Aambient%20state%20space.%20We%20propose%20a%20localization%20strategy%20which%20exploits%0Aconditional%20independence%20of%20conditional%20expectation%20values.%20Localization%20thus%0Areplaces%20a%20single%20high-dimensional%20Schr%5C%22odinger%20bridge%20problem%20by%20%24d%24%0Alow-dimensional%20Schr%5C%22odinger%20bridge%20problems%20over%20the%20available%20training%0Asamples.%20As%20for%20the%20original%20approach%2C%20the%20localized%20sampler%20is%20stable%20and%0Ageometric%20ergodic.%20The%20sampler%20also%20naturally%20extends%20to%20conditional%20sampling%0Aand%20to%20Bayesian%20inference.%20We%20demonstrate%20the%20performance%20of%20our%20proposed%0Ascheme%20through%20experiments%20on%20a%20Gaussian%20problem%20with%20increasing%20dimensions%20and%0Aon%20a%20stochastic%20subgrid-scale%20parametrization%20conditional%20sampling%20problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07968v1&entry.124074799=Read"},
{"title": "Explicit Mutual Information Maximization for Self-Supervised Learning", "author": "Lele Chang and Peilin Liu and Qinghai Guo and Fei Wen", "abstract": "  Recently, self-supervised learning (SSL) has been extensively studied.\nTheoretically, mutual information maximization (MIM) is an optimal criterion\nfor SSL, with a strong theoretical foundation in information theory. However,\nit is difficult to directly apply MIM in SSL since the data distribution is not\nanalytically available in applications. In practice, many existing methods can\nbe viewed as approximate implementations of the MIM criterion. This work shows\nthat, based on the invariance property of MI, explicit MI maximization can be\napplied to SSL under a generic distribution assumption, i.e., a relaxed\ncondition of the data distribution. We further illustrate this by analyzing the\ngeneralized Gaussian distribution. Based on this result, we derive a loss\nfunction based on the MIM criterion using only second-order statistics. We\nimplement the new loss for SSL and demonstrate its effectiveness via extensive\nexperiments.\n", "link": "http://arxiv.org/abs/2409.04747v3", "date": "2024-09-12", "relevancy": 2.2999, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4753}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4546}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.45}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explicit%20Mutual%20Information%20Maximization%20for%20Self-Supervised%20Learning&body=Title%3A%20Explicit%20Mutual%20Information%20Maximization%20for%20Self-Supervised%20Learning%0AAuthor%3A%20Lele%20Chang%20and%20Peilin%20Liu%20and%20Qinghai%20Guo%20and%20Fei%20Wen%0AAbstract%3A%20%20%20Recently%2C%20self-supervised%20learning%20%28SSL%29%20has%20been%20extensively%20studied.%0ATheoretically%2C%20mutual%20information%20maximization%20%28MIM%29%20is%20an%20optimal%20criterion%0Afor%20SSL%2C%20with%20a%20strong%20theoretical%20foundation%20in%20information%20theory.%20However%2C%0Ait%20is%20difficult%20to%20directly%20apply%20MIM%20in%20SSL%20since%20the%20data%20distribution%20is%20not%0Aanalytically%20available%20in%20applications.%20In%20practice%2C%20many%20existing%20methods%20can%0Abe%20viewed%20as%20approximate%20implementations%20of%20the%20MIM%20criterion.%20This%20work%20shows%0Athat%2C%20based%20on%20the%20invariance%20property%20of%20MI%2C%20explicit%20MI%20maximization%20can%20be%0Aapplied%20to%20SSL%20under%20a%20generic%20distribution%20assumption%2C%20i.e.%2C%20a%20relaxed%0Acondition%20of%20the%20data%20distribution.%20We%20further%20illustrate%20this%20by%20analyzing%20the%0Ageneralized%20Gaussian%20distribution.%20Based%20on%20this%20result%2C%20we%20derive%20a%20loss%0Afunction%20based%20on%20the%20MIM%20criterion%20using%20only%20second-order%20statistics.%20We%0Aimplement%20the%20new%20loss%20for%20SSL%20and%20demonstrate%20its%20effectiveness%20via%20extensive%0Aexperiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04747v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplicit%2520Mutual%2520Information%2520Maximization%2520for%2520Self-Supervised%2520Learning%26entry.906535625%3DLele%2520Chang%2520and%2520Peilin%2520Liu%2520and%2520Qinghai%2520Guo%2520and%2520Fei%2520Wen%26entry.1292438233%3D%2520%2520Recently%252C%2520self-supervised%2520learning%2520%2528SSL%2529%2520has%2520been%2520extensively%2520studied.%250ATheoretically%252C%2520mutual%2520information%2520maximization%2520%2528MIM%2529%2520is%2520an%2520optimal%2520criterion%250Afor%2520SSL%252C%2520with%2520a%2520strong%2520theoretical%2520foundation%2520in%2520information%2520theory.%2520However%252C%250Ait%2520is%2520difficult%2520to%2520directly%2520apply%2520MIM%2520in%2520SSL%2520since%2520the%2520data%2520distribution%2520is%2520not%250Aanalytically%2520available%2520in%2520applications.%2520In%2520practice%252C%2520many%2520existing%2520methods%2520can%250Abe%2520viewed%2520as%2520approximate%2520implementations%2520of%2520the%2520MIM%2520criterion.%2520This%2520work%2520shows%250Athat%252C%2520based%2520on%2520the%2520invariance%2520property%2520of%2520MI%252C%2520explicit%2520MI%2520maximization%2520can%2520be%250Aapplied%2520to%2520SSL%2520under%2520a%2520generic%2520distribution%2520assumption%252C%2520i.e.%252C%2520a%2520relaxed%250Acondition%2520of%2520the%2520data%2520distribution.%2520We%2520further%2520illustrate%2520this%2520by%2520analyzing%2520the%250Ageneralized%2520Gaussian%2520distribution.%2520Based%2520on%2520this%2520result%252C%2520we%2520derive%2520a%2520loss%250Afunction%2520based%2520on%2520the%2520MIM%2520criterion%2520using%2520only%2520second-order%2520statistics.%2520We%250Aimplement%2520the%2520new%2520loss%2520for%2520SSL%2520and%2520demonstrate%2520its%2520effectiveness%2520via%2520extensive%250Aexperiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04747v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explicit%20Mutual%20Information%20Maximization%20for%20Self-Supervised%20Learning&entry.906535625=Lele%20Chang%20and%20Peilin%20Liu%20and%20Qinghai%20Guo%20and%20Fei%20Wen&entry.1292438233=%20%20Recently%2C%20self-supervised%20learning%20%28SSL%29%20has%20been%20extensively%20studied.%0ATheoretically%2C%20mutual%20information%20maximization%20%28MIM%29%20is%20an%20optimal%20criterion%0Afor%20SSL%2C%20with%20a%20strong%20theoretical%20foundation%20in%20information%20theory.%20However%2C%0Ait%20is%20difficult%20to%20directly%20apply%20MIM%20in%20SSL%20since%20the%20data%20distribution%20is%20not%0Aanalytically%20available%20in%20applications.%20In%20practice%2C%20many%20existing%20methods%20can%0Abe%20viewed%20as%20approximate%20implementations%20of%20the%20MIM%20criterion.%20This%20work%20shows%0Athat%2C%20based%20on%20the%20invariance%20property%20of%20MI%2C%20explicit%20MI%20maximization%20can%20be%0Aapplied%20to%20SSL%20under%20a%20generic%20distribution%20assumption%2C%20i.e.%2C%20a%20relaxed%0Acondition%20of%20the%20data%20distribution.%20We%20further%20illustrate%20this%20by%20analyzing%20the%0Ageneralized%20Gaussian%20distribution.%20Based%20on%20this%20result%2C%20we%20derive%20a%20loss%0Afunction%20based%20on%20the%20MIM%20criterion%20using%20only%20second-order%20statistics.%20We%0Aimplement%20the%20new%20loss%20for%20SSL%20and%20demonstrate%20its%20effectiveness%20via%20extensive%0Aexperiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04747v3&entry.124074799=Read"},
{"title": "Hand-Object Interaction Pretraining from Videos", "author": "Himanshu Gaurav Singh and Antonio Loquercio and Carmelo Sferrazza and Jane Wu and Haozhi Qi and Pieter Abbeel and Jitendra Malik", "abstract": "  We present an approach to learn general robot manipulation priors from 3D\nhand-object interaction trajectories. We build a framework to use in-the-wild\nvideos to generate sensorimotor robot trajectories. We do so by lifting both\nthe human hand and the manipulated object in a shared 3D space and retargeting\nhuman motions to robot actions. Generative modeling on this data gives us a\ntask-agnostic base policy. This policy captures a general yet flexible\nmanipulation prior. We empirically demonstrate that finetuning this policy,\nwith both reinforcement learning (RL) and behavior cloning (BC), enables\nsample-efficient adaptation to downstream tasks and simultaneously improves\nrobustness and generalizability compared to prior approaches. Qualitative\nexperiments are available at: \\url{https://hgaurav2k.github.io/hop/}.\n", "link": "http://arxiv.org/abs/2409.08273v1", "date": "2024-09-12", "relevancy": 2.2934, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5866}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5643}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5627}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hand-Object%20Interaction%20Pretraining%20from%20Videos&body=Title%3A%20Hand-Object%20Interaction%20Pretraining%20from%20Videos%0AAuthor%3A%20Himanshu%20Gaurav%20Singh%20and%20Antonio%20Loquercio%20and%20Carmelo%20Sferrazza%20and%20Jane%20Wu%20and%20Haozhi%20Qi%20and%20Pieter%20Abbeel%20and%20Jitendra%20Malik%0AAbstract%3A%20%20%20We%20present%20an%20approach%20to%20learn%20general%20robot%20manipulation%20priors%20from%203D%0Ahand-object%20interaction%20trajectories.%20We%20build%20a%20framework%20to%20use%20in-the-wild%0Avideos%20to%20generate%20sensorimotor%20robot%20trajectories.%20We%20do%20so%20by%20lifting%20both%0Athe%20human%20hand%20and%20the%20manipulated%20object%20in%20a%20shared%203D%20space%20and%20retargeting%0Ahuman%20motions%20to%20robot%20actions.%20Generative%20modeling%20on%20this%20data%20gives%20us%20a%0Atask-agnostic%20base%20policy.%20This%20policy%20captures%20a%20general%20yet%20flexible%0Amanipulation%20prior.%20We%20empirically%20demonstrate%20that%20finetuning%20this%20policy%2C%0Awith%20both%20reinforcement%20learning%20%28RL%29%20and%20behavior%20cloning%20%28BC%29%2C%20enables%0Asample-efficient%20adaptation%20to%20downstream%20tasks%20and%20simultaneously%20improves%0Arobustness%20and%20generalizability%20compared%20to%20prior%20approaches.%20Qualitative%0Aexperiments%20are%20available%20at%3A%20%5Curl%7Bhttps%3A//hgaurav2k.github.io/hop/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08273v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHand-Object%2520Interaction%2520Pretraining%2520from%2520Videos%26entry.906535625%3DHimanshu%2520Gaurav%2520Singh%2520and%2520Antonio%2520Loquercio%2520and%2520Carmelo%2520Sferrazza%2520and%2520Jane%2520Wu%2520and%2520Haozhi%2520Qi%2520and%2520Pieter%2520Abbeel%2520and%2520Jitendra%2520Malik%26entry.1292438233%3D%2520%2520We%2520present%2520an%2520approach%2520to%2520learn%2520general%2520robot%2520manipulation%2520priors%2520from%25203D%250Ahand-object%2520interaction%2520trajectories.%2520We%2520build%2520a%2520framework%2520to%2520use%2520in-the-wild%250Avideos%2520to%2520generate%2520sensorimotor%2520robot%2520trajectories.%2520We%2520do%2520so%2520by%2520lifting%2520both%250Athe%2520human%2520hand%2520and%2520the%2520manipulated%2520object%2520in%2520a%2520shared%25203D%2520space%2520and%2520retargeting%250Ahuman%2520motions%2520to%2520robot%2520actions.%2520Generative%2520modeling%2520on%2520this%2520data%2520gives%2520us%2520a%250Atask-agnostic%2520base%2520policy.%2520This%2520policy%2520captures%2520a%2520general%2520yet%2520flexible%250Amanipulation%2520prior.%2520We%2520empirically%2520demonstrate%2520that%2520finetuning%2520this%2520policy%252C%250Awith%2520both%2520reinforcement%2520learning%2520%2528RL%2529%2520and%2520behavior%2520cloning%2520%2528BC%2529%252C%2520enables%250Asample-efficient%2520adaptation%2520to%2520downstream%2520tasks%2520and%2520simultaneously%2520improves%250Arobustness%2520and%2520generalizability%2520compared%2520to%2520prior%2520approaches.%2520Qualitative%250Aexperiments%2520are%2520available%2520at%253A%2520%255Curl%257Bhttps%253A//hgaurav2k.github.io/hop/%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08273v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hand-Object%20Interaction%20Pretraining%20from%20Videos&entry.906535625=Himanshu%20Gaurav%20Singh%20and%20Antonio%20Loquercio%20and%20Carmelo%20Sferrazza%20and%20Jane%20Wu%20and%20Haozhi%20Qi%20and%20Pieter%20Abbeel%20and%20Jitendra%20Malik&entry.1292438233=%20%20We%20present%20an%20approach%20to%20learn%20general%20robot%20manipulation%20priors%20from%203D%0Ahand-object%20interaction%20trajectories.%20We%20build%20a%20framework%20to%20use%20in-the-wild%0Avideos%20to%20generate%20sensorimotor%20robot%20trajectories.%20We%20do%20so%20by%20lifting%20both%0Athe%20human%20hand%20and%20the%20manipulated%20object%20in%20a%20shared%203D%20space%20and%20retargeting%0Ahuman%20motions%20to%20robot%20actions.%20Generative%20modeling%20on%20this%20data%20gives%20us%20a%0Atask-agnostic%20base%20policy.%20This%20policy%20captures%20a%20general%20yet%20flexible%0Amanipulation%20prior.%20We%20empirically%20demonstrate%20that%20finetuning%20this%20policy%2C%0Awith%20both%20reinforcement%20learning%20%28RL%29%20and%20behavior%20cloning%20%28BC%29%2C%20enables%0Asample-efficient%20adaptation%20to%20downstream%20tasks%20and%20simultaneously%20improves%0Arobustness%20and%20generalizability%20compared%20to%20prior%20approaches.%20Qualitative%0Aexperiments%20are%20available%20at%3A%20%5Curl%7Bhttps%3A//hgaurav2k.github.io/hop/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08273v1&entry.124074799=Read"},
{"title": "BEVal: A Cross-dataset Evaluation Study of BEV Segmentation Models for\n  Autonomous Driving", "author": "Manuel Alejandro Diaz-Zapata and Wenqian Liu and Robin Baruffa and Christian Laugier", "abstract": "  Current research in semantic bird's-eye view segmentation for autonomous\ndriving focuses solely on optimizing neural network models using a single\ndataset, typically nuScenes. This practice leads to the development of highly\nspecialized models that may fail when faced with different environments or\nsensor setups, a problem known as domain shift. In this paper, we conduct a\ncomprehensive cross-dataset evaluation of state-of-the-art BEV segmentation\nmodels to assess their performance across different training and testing\ndatasets and setups, as well as different semantic categories. We investigate\nthe influence of different sensors, such as cameras and LiDAR, on the models'\nability to generalize to diverse conditions and scenarios. Additionally, we\nconduct multi-dataset training experiments that improve models' BEV\nsegmentation performance compared to single-dataset training. Our work\naddresses the gap in evaluating BEV segmentation models under cross-dataset\nvalidation. And our findings underscore the importance of enhancing model\ngeneralizability and adaptability to ensure more robust and reliable BEV\nsegmentation approaches for autonomous driving applications. The code for this\npaper available at https://github.com/manueldiaz96/beval .\n", "link": "http://arxiv.org/abs/2408.16322v3", "date": "2024-09-12", "relevancy": 2.2823, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5706}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5706}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BEVal%3A%20A%20Cross-dataset%20Evaluation%20Study%20of%20BEV%20Segmentation%20Models%20for%0A%20%20Autonomous%20Driving&body=Title%3A%20BEVal%3A%20A%20Cross-dataset%20Evaluation%20Study%20of%20BEV%20Segmentation%20Models%20for%0A%20%20Autonomous%20Driving%0AAuthor%3A%20Manuel%20Alejandro%20Diaz-Zapata%20and%20Wenqian%20Liu%20and%20Robin%20Baruffa%20and%20Christian%20Laugier%0AAbstract%3A%20%20%20Current%20research%20in%20semantic%20bird%27s-eye%20view%20segmentation%20for%20autonomous%0Adriving%20focuses%20solely%20on%20optimizing%20neural%20network%20models%20using%20a%20single%0Adataset%2C%20typically%20nuScenes.%20This%20practice%20leads%20to%20the%20development%20of%20highly%0Aspecialized%20models%20that%20may%20fail%20when%20faced%20with%20different%20environments%20or%0Asensor%20setups%2C%20a%20problem%20known%20as%20domain%20shift.%20In%20this%20paper%2C%20we%20conduct%20a%0Acomprehensive%20cross-dataset%20evaluation%20of%20state-of-the-art%20BEV%20segmentation%0Amodels%20to%20assess%20their%20performance%20across%20different%20training%20and%20testing%0Adatasets%20and%20setups%2C%20as%20well%20as%20different%20semantic%20categories.%20We%20investigate%0Athe%20influence%20of%20different%20sensors%2C%20such%20as%20cameras%20and%20LiDAR%2C%20on%20the%20models%27%0Aability%20to%20generalize%20to%20diverse%20conditions%20and%20scenarios.%20Additionally%2C%20we%0Aconduct%20multi-dataset%20training%20experiments%20that%20improve%20models%27%20BEV%0Asegmentation%20performance%20compared%20to%20single-dataset%20training.%20Our%20work%0Aaddresses%20the%20gap%20in%20evaluating%20BEV%20segmentation%20models%20under%20cross-dataset%0Avalidation.%20And%20our%20findings%20underscore%20the%20importance%20of%20enhancing%20model%0Ageneralizability%20and%20adaptability%20to%20ensure%20more%20robust%20and%20reliable%20BEV%0Asegmentation%20approaches%20for%20autonomous%20driving%20applications.%20The%20code%20for%20this%0Apaper%20available%20at%20https%3A//github.com/manueldiaz96/beval%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16322v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBEVal%253A%2520A%2520Cross-dataset%2520Evaluation%2520Study%2520of%2520BEV%2520Segmentation%2520Models%2520for%250A%2520%2520Autonomous%2520Driving%26entry.906535625%3DManuel%2520Alejandro%2520Diaz-Zapata%2520and%2520Wenqian%2520Liu%2520and%2520Robin%2520Baruffa%2520and%2520Christian%2520Laugier%26entry.1292438233%3D%2520%2520Current%2520research%2520in%2520semantic%2520bird%2527s-eye%2520view%2520segmentation%2520for%2520autonomous%250Adriving%2520focuses%2520solely%2520on%2520optimizing%2520neural%2520network%2520models%2520using%2520a%2520single%250Adataset%252C%2520typically%2520nuScenes.%2520This%2520practice%2520leads%2520to%2520the%2520development%2520of%2520highly%250Aspecialized%2520models%2520that%2520may%2520fail%2520when%2520faced%2520with%2520different%2520environments%2520or%250Asensor%2520setups%252C%2520a%2520problem%2520known%2520as%2520domain%2520shift.%2520In%2520this%2520paper%252C%2520we%2520conduct%2520a%250Acomprehensive%2520cross-dataset%2520evaluation%2520of%2520state-of-the-art%2520BEV%2520segmentation%250Amodels%2520to%2520assess%2520their%2520performance%2520across%2520different%2520training%2520and%2520testing%250Adatasets%2520and%2520setups%252C%2520as%2520well%2520as%2520different%2520semantic%2520categories.%2520We%2520investigate%250Athe%2520influence%2520of%2520different%2520sensors%252C%2520such%2520as%2520cameras%2520and%2520LiDAR%252C%2520on%2520the%2520models%2527%250Aability%2520to%2520generalize%2520to%2520diverse%2520conditions%2520and%2520scenarios.%2520Additionally%252C%2520we%250Aconduct%2520multi-dataset%2520training%2520experiments%2520that%2520improve%2520models%2527%2520BEV%250Asegmentation%2520performance%2520compared%2520to%2520single-dataset%2520training.%2520Our%2520work%250Aaddresses%2520the%2520gap%2520in%2520evaluating%2520BEV%2520segmentation%2520models%2520under%2520cross-dataset%250Avalidation.%2520And%2520our%2520findings%2520underscore%2520the%2520importance%2520of%2520enhancing%2520model%250Ageneralizability%2520and%2520adaptability%2520to%2520ensure%2520more%2520robust%2520and%2520reliable%2520BEV%250Asegmentation%2520approaches%2520for%2520autonomous%2520driving%2520applications.%2520The%2520code%2520for%2520this%250Apaper%2520available%2520at%2520https%253A//github.com/manueldiaz96/beval%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16322v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BEVal%3A%20A%20Cross-dataset%20Evaluation%20Study%20of%20BEV%20Segmentation%20Models%20for%0A%20%20Autonomous%20Driving&entry.906535625=Manuel%20Alejandro%20Diaz-Zapata%20and%20Wenqian%20Liu%20and%20Robin%20Baruffa%20and%20Christian%20Laugier&entry.1292438233=%20%20Current%20research%20in%20semantic%20bird%27s-eye%20view%20segmentation%20for%20autonomous%0Adriving%20focuses%20solely%20on%20optimizing%20neural%20network%20models%20using%20a%20single%0Adataset%2C%20typically%20nuScenes.%20This%20practice%20leads%20to%20the%20development%20of%20highly%0Aspecialized%20models%20that%20may%20fail%20when%20faced%20with%20different%20environments%20or%0Asensor%20setups%2C%20a%20problem%20known%20as%20domain%20shift.%20In%20this%20paper%2C%20we%20conduct%20a%0Acomprehensive%20cross-dataset%20evaluation%20of%20state-of-the-art%20BEV%20segmentation%0Amodels%20to%20assess%20their%20performance%20across%20different%20training%20and%20testing%0Adatasets%20and%20setups%2C%20as%20well%20as%20different%20semantic%20categories.%20We%20investigate%0Athe%20influence%20of%20different%20sensors%2C%20such%20as%20cameras%20and%20LiDAR%2C%20on%20the%20models%27%0Aability%20to%20generalize%20to%20diverse%20conditions%20and%20scenarios.%20Additionally%2C%20we%0Aconduct%20multi-dataset%20training%20experiments%20that%20improve%20models%27%20BEV%0Asegmentation%20performance%20compared%20to%20single-dataset%20training.%20Our%20work%0Aaddresses%20the%20gap%20in%20evaluating%20BEV%20segmentation%20models%20under%20cross-dataset%0Avalidation.%20And%20our%20findings%20underscore%20the%20importance%20of%20enhancing%20model%0Ageneralizability%20and%20adaptability%20to%20ensure%20more%20robust%20and%20reliable%20BEV%0Asegmentation%20approaches%20for%20autonomous%20driving%20applications.%20The%20code%20for%20this%0Apaper%20available%20at%20https%3A//github.com/manueldiaz96/beval%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16322v3&entry.124074799=Read"},
{"title": "UGAD: Universal Generative AI Detector utilizing Frequency Fingerprints", "author": "Inzamamul Alam and Muhammad Shahid Muneer and Simon S. Woo", "abstract": "  In the wake of a fabricated explosion image at the Pentagon, an ability to\ndiscern real images from fake counterparts has never been more critical. Our\nstudy introduces a novel multi-modal approach to detect AI-generated images\namidst the proliferation of new generation methods such as Diffusion models.\nOur method, UGAD, encompasses three key detection steps: First, we transform\nthe RGB images into YCbCr channels and apply an Integral Radial Operation to\nemphasize salient radial features. Secondly, the Spatial Fourier Extraction\noperation is used for a spatial shift, utilizing a pre-trained deep learning\nnetwork for optimal feature extraction. Finally, the deep neural network\nclassification stage processes the data through dense layers using softmax for\nclassification. Our approach significantly enhances the accuracy of\ndifferentiating between real and AI-generated images, as evidenced by a 12.64%\nincrease in accuracy and 28.43% increase in AUC compared to existing\nstate-of-the-art methods.\n", "link": "http://arxiv.org/abs/2409.07913v1", "date": "2024-09-12", "relevancy": 2.2636, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5513}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5376}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UGAD%3A%20Universal%20Generative%20AI%20Detector%20utilizing%20Frequency%20Fingerprints&body=Title%3A%20UGAD%3A%20Universal%20Generative%20AI%20Detector%20utilizing%20Frequency%20Fingerprints%0AAuthor%3A%20Inzamamul%20Alam%20and%20Muhammad%20Shahid%20Muneer%20and%20Simon%20S.%20Woo%0AAbstract%3A%20%20%20In%20the%20wake%20of%20a%20fabricated%20explosion%20image%20at%20the%20Pentagon%2C%20an%20ability%20to%0Adiscern%20real%20images%20from%20fake%20counterparts%20has%20never%20been%20more%20critical.%20Our%0Astudy%20introduces%20a%20novel%20multi-modal%20approach%20to%20detect%20AI-generated%20images%0Aamidst%20the%20proliferation%20of%20new%20generation%20methods%20such%20as%20Diffusion%20models.%0AOur%20method%2C%20UGAD%2C%20encompasses%20three%20key%20detection%20steps%3A%20First%2C%20we%20transform%0Athe%20RGB%20images%20into%20YCbCr%20channels%20and%20apply%20an%20Integral%20Radial%20Operation%20to%0Aemphasize%20salient%20radial%20features.%20Secondly%2C%20the%20Spatial%20Fourier%20Extraction%0Aoperation%20is%20used%20for%20a%20spatial%20shift%2C%20utilizing%20a%20pre-trained%20deep%20learning%0Anetwork%20for%20optimal%20feature%20extraction.%20Finally%2C%20the%20deep%20neural%20network%0Aclassification%20stage%20processes%20the%20data%20through%20dense%20layers%20using%20softmax%20for%0Aclassification.%20Our%20approach%20significantly%20enhances%20the%20accuracy%20of%0Adifferentiating%20between%20real%20and%20AI-generated%20images%2C%20as%20evidenced%20by%20a%2012.64%25%0Aincrease%20in%20accuracy%20and%2028.43%25%20increase%20in%20AUC%20compared%20to%20existing%0Astate-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07913v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUGAD%253A%2520Universal%2520Generative%2520AI%2520Detector%2520utilizing%2520Frequency%2520Fingerprints%26entry.906535625%3DInzamamul%2520Alam%2520and%2520Muhammad%2520Shahid%2520Muneer%2520and%2520Simon%2520S.%2520Woo%26entry.1292438233%3D%2520%2520In%2520the%2520wake%2520of%2520a%2520fabricated%2520explosion%2520image%2520at%2520the%2520Pentagon%252C%2520an%2520ability%2520to%250Adiscern%2520real%2520images%2520from%2520fake%2520counterparts%2520has%2520never%2520been%2520more%2520critical.%2520Our%250Astudy%2520introduces%2520a%2520novel%2520multi-modal%2520approach%2520to%2520detect%2520AI-generated%2520images%250Aamidst%2520the%2520proliferation%2520of%2520new%2520generation%2520methods%2520such%2520as%2520Diffusion%2520models.%250AOur%2520method%252C%2520UGAD%252C%2520encompasses%2520three%2520key%2520detection%2520steps%253A%2520First%252C%2520we%2520transform%250Athe%2520RGB%2520images%2520into%2520YCbCr%2520channels%2520and%2520apply%2520an%2520Integral%2520Radial%2520Operation%2520to%250Aemphasize%2520salient%2520radial%2520features.%2520Secondly%252C%2520the%2520Spatial%2520Fourier%2520Extraction%250Aoperation%2520is%2520used%2520for%2520a%2520spatial%2520shift%252C%2520utilizing%2520a%2520pre-trained%2520deep%2520learning%250Anetwork%2520for%2520optimal%2520feature%2520extraction.%2520Finally%252C%2520the%2520deep%2520neural%2520network%250Aclassification%2520stage%2520processes%2520the%2520data%2520through%2520dense%2520layers%2520using%2520softmax%2520for%250Aclassification.%2520Our%2520approach%2520significantly%2520enhances%2520the%2520accuracy%2520of%250Adifferentiating%2520between%2520real%2520and%2520AI-generated%2520images%252C%2520as%2520evidenced%2520by%2520a%252012.64%2525%250Aincrease%2520in%2520accuracy%2520and%252028.43%2525%2520increase%2520in%2520AUC%2520compared%2520to%2520existing%250Astate-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07913v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UGAD%3A%20Universal%20Generative%20AI%20Detector%20utilizing%20Frequency%20Fingerprints&entry.906535625=Inzamamul%20Alam%20and%20Muhammad%20Shahid%20Muneer%20and%20Simon%20S.%20Woo&entry.1292438233=%20%20In%20the%20wake%20of%20a%20fabricated%20explosion%20image%20at%20the%20Pentagon%2C%20an%20ability%20to%0Adiscern%20real%20images%20from%20fake%20counterparts%20has%20never%20been%20more%20critical.%20Our%0Astudy%20introduces%20a%20novel%20multi-modal%20approach%20to%20detect%20AI-generated%20images%0Aamidst%20the%20proliferation%20of%20new%20generation%20methods%20such%20as%20Diffusion%20models.%0AOur%20method%2C%20UGAD%2C%20encompasses%20three%20key%20detection%20steps%3A%20First%2C%20we%20transform%0Athe%20RGB%20images%20into%20YCbCr%20channels%20and%20apply%20an%20Integral%20Radial%20Operation%20to%0Aemphasize%20salient%20radial%20features.%20Secondly%2C%20the%20Spatial%20Fourier%20Extraction%0Aoperation%20is%20used%20for%20a%20spatial%20shift%2C%20utilizing%20a%20pre-trained%20deep%20learning%0Anetwork%20for%20optimal%20feature%20extraction.%20Finally%2C%20the%20deep%20neural%20network%0Aclassification%20stage%20processes%20the%20data%20through%20dense%20layers%20using%20softmax%20for%0Aclassification.%20Our%20approach%20significantly%20enhances%20the%20accuracy%20of%0Adifferentiating%20between%20real%20and%20AI-generated%20images%2C%20as%20evidenced%20by%20a%2012.64%25%0Aincrease%20in%20accuracy%20and%2028.43%25%20increase%20in%20AUC%20compared%20to%20existing%0Astate-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07913v1&entry.124074799=Read"},
{"title": "InterACT: Inter-dependency Aware Action Chunking with Hierarchical\n  Attention Transformers for Bimanual Manipulation", "author": "Andrew Lee and Ian Chuang and Ling-Yuan Chen and Iman Soltani", "abstract": "  We present InterACT: Inter-dependency aware Action Chunking with Hierarchical\nAttention Transformers, a novel imitation learning framework for bimanual\nmanipulation that integrates hierarchical attention to capture\ninter-dependencies between dual-arm joint states and visual inputs. InterACT\nconsists of a Hierarchical Attention Encoder and a Multi-arm Decoder, both\ndesigned to enhance information aggregation and coordination. The encoder\nprocesses multi-modal inputs through segment-wise and cross-segment attention\nmechanisms, while the decoder leverages synchronization blocks to refine\nindividual action predictions, providing the counterpart's prediction as\ncontext. Our experiments on a variety of simulated and real-world bimanual\nmanipulation tasks demonstrate that InterACT significantly outperforms existing\nmethods. Detailed ablation studies validate the contributions of key components\nof our work, including the impact of CLS tokens, cross-segment encoders, and\nsynchronization blocks.\n", "link": "http://arxiv.org/abs/2409.07914v1", "date": "2024-09-12", "relevancy": 2.2262, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6011}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.563}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5322}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InterACT%3A%20Inter-dependency%20Aware%20Action%20Chunking%20with%20Hierarchical%0A%20%20Attention%20Transformers%20for%20Bimanual%20Manipulation&body=Title%3A%20InterACT%3A%20Inter-dependency%20Aware%20Action%20Chunking%20with%20Hierarchical%0A%20%20Attention%20Transformers%20for%20Bimanual%20Manipulation%0AAuthor%3A%20Andrew%20Lee%20and%20Ian%20Chuang%20and%20Ling-Yuan%20Chen%20and%20Iman%20Soltani%0AAbstract%3A%20%20%20We%20present%20InterACT%3A%20Inter-dependency%20aware%20Action%20Chunking%20with%20Hierarchical%0AAttention%20Transformers%2C%20a%20novel%20imitation%20learning%20framework%20for%20bimanual%0Amanipulation%20that%20integrates%20hierarchical%20attention%20to%20capture%0Ainter-dependencies%20between%20dual-arm%20joint%20states%20and%20visual%20inputs.%20InterACT%0Aconsists%20of%20a%20Hierarchical%20Attention%20Encoder%20and%20a%20Multi-arm%20Decoder%2C%20both%0Adesigned%20to%20enhance%20information%20aggregation%20and%20coordination.%20The%20encoder%0Aprocesses%20multi-modal%20inputs%20through%20segment-wise%20and%20cross-segment%20attention%0Amechanisms%2C%20while%20the%20decoder%20leverages%20synchronization%20blocks%20to%20refine%0Aindividual%20action%20predictions%2C%20providing%20the%20counterpart%27s%20prediction%20as%0Acontext.%20Our%20experiments%20on%20a%20variety%20of%20simulated%20and%20real-world%20bimanual%0Amanipulation%20tasks%20demonstrate%20that%20InterACT%20significantly%20outperforms%20existing%0Amethods.%20Detailed%20ablation%20studies%20validate%20the%20contributions%20of%20key%20components%0Aof%20our%20work%2C%20including%20the%20impact%20of%20CLS%20tokens%2C%20cross-segment%20encoders%2C%20and%0Asynchronization%20blocks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07914v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterACT%253A%2520Inter-dependency%2520Aware%2520Action%2520Chunking%2520with%2520Hierarchical%250A%2520%2520Attention%2520Transformers%2520for%2520Bimanual%2520Manipulation%26entry.906535625%3DAndrew%2520Lee%2520and%2520Ian%2520Chuang%2520and%2520Ling-Yuan%2520Chen%2520and%2520Iman%2520Soltani%26entry.1292438233%3D%2520%2520We%2520present%2520InterACT%253A%2520Inter-dependency%2520aware%2520Action%2520Chunking%2520with%2520Hierarchical%250AAttention%2520Transformers%252C%2520a%2520novel%2520imitation%2520learning%2520framework%2520for%2520bimanual%250Amanipulation%2520that%2520integrates%2520hierarchical%2520attention%2520to%2520capture%250Ainter-dependencies%2520between%2520dual-arm%2520joint%2520states%2520and%2520visual%2520inputs.%2520InterACT%250Aconsists%2520of%2520a%2520Hierarchical%2520Attention%2520Encoder%2520and%2520a%2520Multi-arm%2520Decoder%252C%2520both%250Adesigned%2520to%2520enhance%2520information%2520aggregation%2520and%2520coordination.%2520The%2520encoder%250Aprocesses%2520multi-modal%2520inputs%2520through%2520segment-wise%2520and%2520cross-segment%2520attention%250Amechanisms%252C%2520while%2520the%2520decoder%2520leverages%2520synchronization%2520blocks%2520to%2520refine%250Aindividual%2520action%2520predictions%252C%2520providing%2520the%2520counterpart%2527s%2520prediction%2520as%250Acontext.%2520Our%2520experiments%2520on%2520a%2520variety%2520of%2520simulated%2520and%2520real-world%2520bimanual%250Amanipulation%2520tasks%2520demonstrate%2520that%2520InterACT%2520significantly%2520outperforms%2520existing%250Amethods.%2520Detailed%2520ablation%2520studies%2520validate%2520the%2520contributions%2520of%2520key%2520components%250Aof%2520our%2520work%252C%2520including%2520the%2520impact%2520of%2520CLS%2520tokens%252C%2520cross-segment%2520encoders%252C%2520and%250Asynchronization%2520blocks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07914v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InterACT%3A%20Inter-dependency%20Aware%20Action%20Chunking%20with%20Hierarchical%0A%20%20Attention%20Transformers%20for%20Bimanual%20Manipulation&entry.906535625=Andrew%20Lee%20and%20Ian%20Chuang%20and%20Ling-Yuan%20Chen%20and%20Iman%20Soltani&entry.1292438233=%20%20We%20present%20InterACT%3A%20Inter-dependency%20aware%20Action%20Chunking%20with%20Hierarchical%0AAttention%20Transformers%2C%20a%20novel%20imitation%20learning%20framework%20for%20bimanual%0Amanipulation%20that%20integrates%20hierarchical%20attention%20to%20capture%0Ainter-dependencies%20between%20dual-arm%20joint%20states%20and%20visual%20inputs.%20InterACT%0Aconsists%20of%20a%20Hierarchical%20Attention%20Encoder%20and%20a%20Multi-arm%20Decoder%2C%20both%0Adesigned%20to%20enhance%20information%20aggregation%20and%20coordination.%20The%20encoder%0Aprocesses%20multi-modal%20inputs%20through%20segment-wise%20and%20cross-segment%20attention%0Amechanisms%2C%20while%20the%20decoder%20leverages%20synchronization%20blocks%20to%20refine%0Aindividual%20action%20predictions%2C%20providing%20the%20counterpart%27s%20prediction%20as%0Acontext.%20Our%20experiments%20on%20a%20variety%20of%20simulated%20and%20real-world%20bimanual%0Amanipulation%20tasks%20demonstrate%20that%20InterACT%20significantly%20outperforms%20existing%0Amethods.%20Detailed%20ablation%20studies%20validate%20the%20contributions%20of%20key%20components%0Aof%20our%20work%2C%20including%20the%20impact%20of%20CLS%20tokens%2C%20cross-segment%20encoders%2C%20and%0Asynchronization%20blocks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07914v1&entry.124074799=Read"},
{"title": "A three-dimensional force estimation method for the cable-driven soft\n  robot based on monocular images", "author": "Xiaohan Zhu and Ran Bu and Zhen Li and Fan Xu and Hesheng Wang", "abstract": "  Soft manipulators are known for their superiority in coping with\nhigh-safety-demanding interaction tasks, e.g., robot-assisted surgeries,\nelderly caring, etc. Yet the challenges residing in real-time contact feedback\nhave hindered further applications in precise manipulation. This paper proposes\nan end-to-end network to estimate the 3D contact force of the soft robot, with\nthe aim of enhancing its capabilities in interactive tasks. The presented\nmethod features directly utilizing monocular images fused with multidimensional\nactuation information as the network inputs. This approach simplifies the\npreprocessing of raw data compared to related studies that utilize 3D shape\ninformation for network inputs, consequently reducing configuration\nreconstruction errors. The unified feature representation module is devised to\nelevate low-dimensional features from the system's actuation signals to the\nsame level as image features, facilitating smoother integration of multimodal\ninformation. The proposed method has been experimentally validated in the soft\nrobot testbed, achieving satisfying accuracy in 3D force estimation (with a\nmean relative error of 0.84% compared to the best-reported result of 2.2% in\nthe related works).\n", "link": "http://arxiv.org/abs/2409.08033v1", "date": "2024-09-12", "relevancy": 2.2116, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5681}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5521}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20three-dimensional%20force%20estimation%20method%20for%20the%20cable-driven%20soft%0A%20%20robot%20based%20on%20monocular%20images&body=Title%3A%20A%20three-dimensional%20force%20estimation%20method%20for%20the%20cable-driven%20soft%0A%20%20robot%20based%20on%20monocular%20images%0AAuthor%3A%20Xiaohan%20Zhu%20and%20Ran%20Bu%20and%20Zhen%20Li%20and%20Fan%20Xu%20and%20Hesheng%20Wang%0AAbstract%3A%20%20%20Soft%20manipulators%20are%20known%20for%20their%20superiority%20in%20coping%20with%0Ahigh-safety-demanding%20interaction%20tasks%2C%20e.g.%2C%20robot-assisted%20surgeries%2C%0Aelderly%20caring%2C%20etc.%20Yet%20the%20challenges%20residing%20in%20real-time%20contact%20feedback%0Ahave%20hindered%20further%20applications%20in%20precise%20manipulation.%20This%20paper%20proposes%0Aan%20end-to-end%20network%20to%20estimate%20the%203D%20contact%20force%20of%20the%20soft%20robot%2C%20with%0Athe%20aim%20of%20enhancing%20its%20capabilities%20in%20interactive%20tasks.%20The%20presented%0Amethod%20features%20directly%20utilizing%20monocular%20images%20fused%20with%20multidimensional%0Aactuation%20information%20as%20the%20network%20inputs.%20This%20approach%20simplifies%20the%0Apreprocessing%20of%20raw%20data%20compared%20to%20related%20studies%20that%20utilize%203D%20shape%0Ainformation%20for%20network%20inputs%2C%20consequently%20reducing%20configuration%0Areconstruction%20errors.%20The%20unified%20feature%20representation%20module%20is%20devised%20to%0Aelevate%20low-dimensional%20features%20from%20the%20system%27s%20actuation%20signals%20to%20the%0Asame%20level%20as%20image%20features%2C%20facilitating%20smoother%20integration%20of%20multimodal%0Ainformation.%20The%20proposed%20method%20has%20been%20experimentally%20validated%20in%20the%20soft%0Arobot%20testbed%2C%20achieving%20satisfying%20accuracy%20in%203D%20force%20estimation%20%28with%20a%0Amean%20relative%20error%20of%200.84%25%20compared%20to%20the%20best-reported%20result%20of%202.2%25%20in%0Athe%20related%20works%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08033v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520three-dimensional%2520force%2520estimation%2520method%2520for%2520the%2520cable-driven%2520soft%250A%2520%2520robot%2520based%2520on%2520monocular%2520images%26entry.906535625%3DXiaohan%2520Zhu%2520and%2520Ran%2520Bu%2520and%2520Zhen%2520Li%2520and%2520Fan%2520Xu%2520and%2520Hesheng%2520Wang%26entry.1292438233%3D%2520%2520Soft%2520manipulators%2520are%2520known%2520for%2520their%2520superiority%2520in%2520coping%2520with%250Ahigh-safety-demanding%2520interaction%2520tasks%252C%2520e.g.%252C%2520robot-assisted%2520surgeries%252C%250Aelderly%2520caring%252C%2520etc.%2520Yet%2520the%2520challenges%2520residing%2520in%2520real-time%2520contact%2520feedback%250Ahave%2520hindered%2520further%2520applications%2520in%2520precise%2520manipulation.%2520This%2520paper%2520proposes%250Aan%2520end-to-end%2520network%2520to%2520estimate%2520the%25203D%2520contact%2520force%2520of%2520the%2520soft%2520robot%252C%2520with%250Athe%2520aim%2520of%2520enhancing%2520its%2520capabilities%2520in%2520interactive%2520tasks.%2520The%2520presented%250Amethod%2520features%2520directly%2520utilizing%2520monocular%2520images%2520fused%2520with%2520multidimensional%250Aactuation%2520information%2520as%2520the%2520network%2520inputs.%2520This%2520approach%2520simplifies%2520the%250Apreprocessing%2520of%2520raw%2520data%2520compared%2520to%2520related%2520studies%2520that%2520utilize%25203D%2520shape%250Ainformation%2520for%2520network%2520inputs%252C%2520consequently%2520reducing%2520configuration%250Areconstruction%2520errors.%2520The%2520unified%2520feature%2520representation%2520module%2520is%2520devised%2520to%250Aelevate%2520low-dimensional%2520features%2520from%2520the%2520system%2527s%2520actuation%2520signals%2520to%2520the%250Asame%2520level%2520as%2520image%2520features%252C%2520facilitating%2520smoother%2520integration%2520of%2520multimodal%250Ainformation.%2520The%2520proposed%2520method%2520has%2520been%2520experimentally%2520validated%2520in%2520the%2520soft%250Arobot%2520testbed%252C%2520achieving%2520satisfying%2520accuracy%2520in%25203D%2520force%2520estimation%2520%2528with%2520a%250Amean%2520relative%2520error%2520of%25200.84%2525%2520compared%2520to%2520the%2520best-reported%2520result%2520of%25202.2%2525%2520in%250Athe%2520related%2520works%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08033v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20three-dimensional%20force%20estimation%20method%20for%20the%20cable-driven%20soft%0A%20%20robot%20based%20on%20monocular%20images&entry.906535625=Xiaohan%20Zhu%20and%20Ran%20Bu%20and%20Zhen%20Li%20and%20Fan%20Xu%20and%20Hesheng%20Wang&entry.1292438233=%20%20Soft%20manipulators%20are%20known%20for%20their%20superiority%20in%20coping%20with%0Ahigh-safety-demanding%20interaction%20tasks%2C%20e.g.%2C%20robot-assisted%20surgeries%2C%0Aelderly%20caring%2C%20etc.%20Yet%20the%20challenges%20residing%20in%20real-time%20contact%20feedback%0Ahave%20hindered%20further%20applications%20in%20precise%20manipulation.%20This%20paper%20proposes%0Aan%20end-to-end%20network%20to%20estimate%20the%203D%20contact%20force%20of%20the%20soft%20robot%2C%20with%0Athe%20aim%20of%20enhancing%20its%20capabilities%20in%20interactive%20tasks.%20The%20presented%0Amethod%20features%20directly%20utilizing%20monocular%20images%20fused%20with%20multidimensional%0Aactuation%20information%20as%20the%20network%20inputs.%20This%20approach%20simplifies%20the%0Apreprocessing%20of%20raw%20data%20compared%20to%20related%20studies%20that%20utilize%203D%20shape%0Ainformation%20for%20network%20inputs%2C%20consequently%20reducing%20configuration%0Areconstruction%20errors.%20The%20unified%20feature%20representation%20module%20is%20devised%20to%0Aelevate%20low-dimensional%20features%20from%20the%20system%27s%20actuation%20signals%20to%20the%0Asame%20level%20as%20image%20features%2C%20facilitating%20smoother%20integration%20of%20multimodal%0Ainformation.%20The%20proposed%20method%20has%20been%20experimentally%20validated%20in%20the%20soft%0Arobot%20testbed%2C%20achieving%20satisfying%20accuracy%20in%203D%20force%20estimation%20%28with%20a%0Amean%20relative%20error%20of%200.84%25%20compared%20to%20the%20best-reported%20result%20of%202.2%25%20in%0Athe%20related%20works%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08033v1&entry.124074799=Read"},
{"title": "Learning incomplete factorization preconditioners for GMRES", "author": "Paul H\u00e4usner and Aleix Nieto Juscafresa and Jens Sj\u00f6lund", "abstract": "  In this paper, we develop a data-driven approach to generate incomplete LU\nfactorizations of large-scale sparse matrices. The learned approximate\nfactorization is utilized as a preconditioner for the corresponding linear\nequation system in the GMRES method. Incomplete factorization methods are one\nof the most commonly applied algebraic preconditioners for sparse linear\nequation systems and are able to speed up the convergence of Krylov subspace\nmethods. However, they are sensitive to hyper-parameters and might suffer from\nnumerical breakdown or lead to slow convergence when not properly applied. We\nreplace the typically hand-engineered algorithms with a graph neural network\nbased approach that is trained against data to predict an approximate\nfactorization. This allows us to learn preconditioners tailored for a specific\nproblem distribution. We analyze and empirically evaluate different loss\nfunctions to train the learned preconditioners and show their effectiveness to\ndecrease the number of GMRES iterations and improve the spectral properties on\nour synthetic dataset. The code is available at\nhttps://github.com/paulhausner/neural-incomplete-factorization.\n", "link": "http://arxiv.org/abs/2409.08262v1", "date": "2024-09-12", "relevancy": 2.2109, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4454}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4432}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.438}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20incomplete%20factorization%20preconditioners%20for%20GMRES&body=Title%3A%20Learning%20incomplete%20factorization%20preconditioners%20for%20GMRES%0AAuthor%3A%20Paul%20H%C3%A4usner%20and%20Aleix%20Nieto%20Juscafresa%20and%20Jens%20Sj%C3%B6lund%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20develop%20a%20data-driven%20approach%20to%20generate%20incomplete%20LU%0Afactorizations%20of%20large-scale%20sparse%20matrices.%20The%20learned%20approximate%0Afactorization%20is%20utilized%20as%20a%20preconditioner%20for%20the%20corresponding%20linear%0Aequation%20system%20in%20the%20GMRES%20method.%20Incomplete%20factorization%20methods%20are%20one%0Aof%20the%20most%20commonly%20applied%20algebraic%20preconditioners%20for%20sparse%20linear%0Aequation%20systems%20and%20are%20able%20to%20speed%20up%20the%20convergence%20of%20Krylov%20subspace%0Amethods.%20However%2C%20they%20are%20sensitive%20to%20hyper-parameters%20and%20might%20suffer%20from%0Anumerical%20breakdown%20or%20lead%20to%20slow%20convergence%20when%20not%20properly%20applied.%20We%0Areplace%20the%20typically%20hand-engineered%20algorithms%20with%20a%20graph%20neural%20network%0Abased%20approach%20that%20is%20trained%20against%20data%20to%20predict%20an%20approximate%0Afactorization.%20This%20allows%20us%20to%20learn%20preconditioners%20tailored%20for%20a%20specific%0Aproblem%20distribution.%20We%20analyze%20and%20empirically%20evaluate%20different%20loss%0Afunctions%20to%20train%20the%20learned%20preconditioners%20and%20show%20their%20effectiveness%20to%0Adecrease%20the%20number%20of%20GMRES%20iterations%20and%20improve%20the%20spectral%20properties%20on%0Aour%20synthetic%20dataset.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/paulhausner/neural-incomplete-factorization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08262v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520incomplete%2520factorization%2520preconditioners%2520for%2520GMRES%26entry.906535625%3DPaul%2520H%25C3%25A4usner%2520and%2520Aleix%2520Nieto%2520Juscafresa%2520and%2520Jens%2520Sj%25C3%25B6lund%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520develop%2520a%2520data-driven%2520approach%2520to%2520generate%2520incomplete%2520LU%250Afactorizations%2520of%2520large-scale%2520sparse%2520matrices.%2520The%2520learned%2520approximate%250Afactorization%2520is%2520utilized%2520as%2520a%2520preconditioner%2520for%2520the%2520corresponding%2520linear%250Aequation%2520system%2520in%2520the%2520GMRES%2520method.%2520Incomplete%2520factorization%2520methods%2520are%2520one%250Aof%2520the%2520most%2520commonly%2520applied%2520algebraic%2520preconditioners%2520for%2520sparse%2520linear%250Aequation%2520systems%2520and%2520are%2520able%2520to%2520speed%2520up%2520the%2520convergence%2520of%2520Krylov%2520subspace%250Amethods.%2520However%252C%2520they%2520are%2520sensitive%2520to%2520hyper-parameters%2520and%2520might%2520suffer%2520from%250Anumerical%2520breakdown%2520or%2520lead%2520to%2520slow%2520convergence%2520when%2520not%2520properly%2520applied.%2520We%250Areplace%2520the%2520typically%2520hand-engineered%2520algorithms%2520with%2520a%2520graph%2520neural%2520network%250Abased%2520approach%2520that%2520is%2520trained%2520against%2520data%2520to%2520predict%2520an%2520approximate%250Afactorization.%2520This%2520allows%2520us%2520to%2520learn%2520preconditioners%2520tailored%2520for%2520a%2520specific%250Aproblem%2520distribution.%2520We%2520analyze%2520and%2520empirically%2520evaluate%2520different%2520loss%250Afunctions%2520to%2520train%2520the%2520learned%2520preconditioners%2520and%2520show%2520their%2520effectiveness%2520to%250Adecrease%2520the%2520number%2520of%2520GMRES%2520iterations%2520and%2520improve%2520the%2520spectral%2520properties%2520on%250Aour%2520synthetic%2520dataset.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/paulhausner/neural-incomplete-factorization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08262v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20incomplete%20factorization%20preconditioners%20for%20GMRES&entry.906535625=Paul%20H%C3%A4usner%20and%20Aleix%20Nieto%20Juscafresa%20and%20Jens%20Sj%C3%B6lund&entry.1292438233=%20%20In%20this%20paper%2C%20we%20develop%20a%20data-driven%20approach%20to%20generate%20incomplete%20LU%0Afactorizations%20of%20large-scale%20sparse%20matrices.%20The%20learned%20approximate%0Afactorization%20is%20utilized%20as%20a%20preconditioner%20for%20the%20corresponding%20linear%0Aequation%20system%20in%20the%20GMRES%20method.%20Incomplete%20factorization%20methods%20are%20one%0Aof%20the%20most%20commonly%20applied%20algebraic%20preconditioners%20for%20sparse%20linear%0Aequation%20systems%20and%20are%20able%20to%20speed%20up%20the%20convergence%20of%20Krylov%20subspace%0Amethods.%20However%2C%20they%20are%20sensitive%20to%20hyper-parameters%20and%20might%20suffer%20from%0Anumerical%20breakdown%20or%20lead%20to%20slow%20convergence%20when%20not%20properly%20applied.%20We%0Areplace%20the%20typically%20hand-engineered%20algorithms%20with%20a%20graph%20neural%20network%0Abased%20approach%20that%20is%20trained%20against%20data%20to%20predict%20an%20approximate%0Afactorization.%20This%20allows%20us%20to%20learn%20preconditioners%20tailored%20for%20a%20specific%0Aproblem%20distribution.%20We%20analyze%20and%20empirically%20evaluate%20different%20loss%0Afunctions%20to%20train%20the%20learned%20preconditioners%20and%20show%20their%20effectiveness%20to%0Adecrease%20the%20number%20of%20GMRES%20iterations%20and%20improve%20the%20spectral%20properties%20on%0Aour%20synthetic%20dataset.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/paulhausner/neural-incomplete-factorization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08262v1&entry.124074799=Read"},
{"title": "Autonomous Vehicle Controllers From End-to-End Differentiable Simulation", "author": "Asen Nachkov and Danda Pani Paudel and Luc Van Gool", "abstract": "  Current methods to learn controllers for autonomous vehicles (AVs) focus on\nbehavioural cloning. Being trained only on exact historic data, the resulting\nagents often generalize poorly to novel scenarios. Simulators provide the\nopportunity to go beyond offline datasets, but they are still treated as\ncomplicated black boxes, only used to update the global simulation state. As a\nresult, these RL algorithms are slow, sample-inefficient, and prior-agnostic.\nIn this work, we leverage a differentiable simulator and design an analytic\npolicy gradients (APG) approach to training AV controllers on the large-scale\nWaymo Open Motion Dataset. Our proposed framework brings the differentiable\nsimulator into an end-to-end training loop, where gradients of the environment\ndynamics serve as a useful prior to help the agent learn a more grounded\npolicy. We combine this setup with a recurrent architecture that can\nefficiently propagate temporal information across long simulated trajectories.\nThis APG method allows us to learn robust, accurate, and fast policies, while\nonly requiring widely-available expert trajectories, instead of scarce expert\nactions. We compare to behavioural cloning and find significant improvements in\nperformance and robustness to noise in the dynamics, as well as overall more\nintuitive human-like handling.\n", "link": "http://arxiv.org/abs/2409.07965v1", "date": "2024-09-12", "relevancy": 2.2032, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5697}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5603}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5281}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autonomous%20Vehicle%20Controllers%20From%20End-to-End%20Differentiable%20Simulation&body=Title%3A%20Autonomous%20Vehicle%20Controllers%20From%20End-to-End%20Differentiable%20Simulation%0AAuthor%3A%20Asen%20Nachkov%20and%20Danda%20Pani%20Paudel%20and%20Luc%20Van%20Gool%0AAbstract%3A%20%20%20Current%20methods%20to%20learn%20controllers%20for%20autonomous%20vehicles%20%28AVs%29%20focus%20on%0Abehavioural%20cloning.%20Being%20trained%20only%20on%20exact%20historic%20data%2C%20the%20resulting%0Aagents%20often%20generalize%20poorly%20to%20novel%20scenarios.%20Simulators%20provide%20the%0Aopportunity%20to%20go%20beyond%20offline%20datasets%2C%20but%20they%20are%20still%20treated%20as%0Acomplicated%20black%20boxes%2C%20only%20used%20to%20update%20the%20global%20simulation%20state.%20As%20a%0Aresult%2C%20these%20RL%20algorithms%20are%20slow%2C%20sample-inefficient%2C%20and%20prior-agnostic.%0AIn%20this%20work%2C%20we%20leverage%20a%20differentiable%20simulator%20and%20design%20an%20analytic%0Apolicy%20gradients%20%28APG%29%20approach%20to%20training%20AV%20controllers%20on%20the%20large-scale%0AWaymo%20Open%20Motion%20Dataset.%20Our%20proposed%20framework%20brings%20the%20differentiable%0Asimulator%20into%20an%20end-to-end%20training%20loop%2C%20where%20gradients%20of%20the%20environment%0Adynamics%20serve%20as%20a%20useful%20prior%20to%20help%20the%20agent%20learn%20a%20more%20grounded%0Apolicy.%20We%20combine%20this%20setup%20with%20a%20recurrent%20architecture%20that%20can%0Aefficiently%20propagate%20temporal%20information%20across%20long%20simulated%20trajectories.%0AThis%20APG%20method%20allows%20us%20to%20learn%20robust%2C%20accurate%2C%20and%20fast%20policies%2C%20while%0Aonly%20requiring%20widely-available%20expert%20trajectories%2C%20instead%20of%20scarce%20expert%0Aactions.%20We%20compare%20to%20behavioural%20cloning%20and%20find%20significant%20improvements%20in%0Aperformance%20and%20robustness%20to%20noise%20in%20the%20dynamics%2C%20as%20well%20as%20overall%20more%0Aintuitive%20human-like%20handling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07965v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutonomous%2520Vehicle%2520Controllers%2520From%2520End-to-End%2520Differentiable%2520Simulation%26entry.906535625%3DAsen%2520Nachkov%2520and%2520Danda%2520Pani%2520Paudel%2520and%2520Luc%2520Van%2520Gool%26entry.1292438233%3D%2520%2520Current%2520methods%2520to%2520learn%2520controllers%2520for%2520autonomous%2520vehicles%2520%2528AVs%2529%2520focus%2520on%250Abehavioural%2520cloning.%2520Being%2520trained%2520only%2520on%2520exact%2520historic%2520data%252C%2520the%2520resulting%250Aagents%2520often%2520generalize%2520poorly%2520to%2520novel%2520scenarios.%2520Simulators%2520provide%2520the%250Aopportunity%2520to%2520go%2520beyond%2520offline%2520datasets%252C%2520but%2520they%2520are%2520still%2520treated%2520as%250Acomplicated%2520black%2520boxes%252C%2520only%2520used%2520to%2520update%2520the%2520global%2520simulation%2520state.%2520As%2520a%250Aresult%252C%2520these%2520RL%2520algorithms%2520are%2520slow%252C%2520sample-inefficient%252C%2520and%2520prior-agnostic.%250AIn%2520this%2520work%252C%2520we%2520leverage%2520a%2520differentiable%2520simulator%2520and%2520design%2520an%2520analytic%250Apolicy%2520gradients%2520%2528APG%2529%2520approach%2520to%2520training%2520AV%2520controllers%2520on%2520the%2520large-scale%250AWaymo%2520Open%2520Motion%2520Dataset.%2520Our%2520proposed%2520framework%2520brings%2520the%2520differentiable%250Asimulator%2520into%2520an%2520end-to-end%2520training%2520loop%252C%2520where%2520gradients%2520of%2520the%2520environment%250Adynamics%2520serve%2520as%2520a%2520useful%2520prior%2520to%2520help%2520the%2520agent%2520learn%2520a%2520more%2520grounded%250Apolicy.%2520We%2520combine%2520this%2520setup%2520with%2520a%2520recurrent%2520architecture%2520that%2520can%250Aefficiently%2520propagate%2520temporal%2520information%2520across%2520long%2520simulated%2520trajectories.%250AThis%2520APG%2520method%2520allows%2520us%2520to%2520learn%2520robust%252C%2520accurate%252C%2520and%2520fast%2520policies%252C%2520while%250Aonly%2520requiring%2520widely-available%2520expert%2520trajectories%252C%2520instead%2520of%2520scarce%2520expert%250Aactions.%2520We%2520compare%2520to%2520behavioural%2520cloning%2520and%2520find%2520significant%2520improvements%2520in%250Aperformance%2520and%2520robustness%2520to%2520noise%2520in%2520the%2520dynamics%252C%2520as%2520well%2520as%2520overall%2520more%250Aintuitive%2520human-like%2520handling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07965v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autonomous%20Vehicle%20Controllers%20From%20End-to-End%20Differentiable%20Simulation&entry.906535625=Asen%20Nachkov%20and%20Danda%20Pani%20Paudel%20and%20Luc%20Van%20Gool&entry.1292438233=%20%20Current%20methods%20to%20learn%20controllers%20for%20autonomous%20vehicles%20%28AVs%29%20focus%20on%0Abehavioural%20cloning.%20Being%20trained%20only%20on%20exact%20historic%20data%2C%20the%20resulting%0Aagents%20often%20generalize%20poorly%20to%20novel%20scenarios.%20Simulators%20provide%20the%0Aopportunity%20to%20go%20beyond%20offline%20datasets%2C%20but%20they%20are%20still%20treated%20as%0Acomplicated%20black%20boxes%2C%20only%20used%20to%20update%20the%20global%20simulation%20state.%20As%20a%0Aresult%2C%20these%20RL%20algorithms%20are%20slow%2C%20sample-inefficient%2C%20and%20prior-agnostic.%0AIn%20this%20work%2C%20we%20leverage%20a%20differentiable%20simulator%20and%20design%20an%20analytic%0Apolicy%20gradients%20%28APG%29%20approach%20to%20training%20AV%20controllers%20on%20the%20large-scale%0AWaymo%20Open%20Motion%20Dataset.%20Our%20proposed%20framework%20brings%20the%20differentiable%0Asimulator%20into%20an%20end-to-end%20training%20loop%2C%20where%20gradients%20of%20the%20environment%0Adynamics%20serve%20as%20a%20useful%20prior%20to%20help%20the%20agent%20learn%20a%20more%20grounded%0Apolicy.%20We%20combine%20this%20setup%20with%20a%20recurrent%20architecture%20that%20can%0Aefficiently%20propagate%20temporal%20information%20across%20long%20simulated%20trajectories.%0AThis%20APG%20method%20allows%20us%20to%20learn%20robust%2C%20accurate%2C%20and%20fast%20policies%2C%20while%0Aonly%20requiring%20widely-available%20expert%20trajectories%2C%20instead%20of%20scarce%20expert%0Aactions.%20We%20compare%20to%20behavioural%20cloning%20and%20find%20significant%20improvements%20in%0Aperformance%20and%20robustness%20to%20noise%20in%20the%20dynamics%2C%20as%20well%20as%20overall%20more%0Aintuitive%20human-like%20handling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07965v1&entry.124074799=Read"},
{"title": "ALSS-YOLO: An Adaptive Lightweight Channel Split and Shuffling Network\n  for TIR Wildlife Detection in UAV Imagery", "author": "Ang He and Xiaobo Li and Ximei Wu and Chengyue Su and Jing Chen and Sheng Xu and Xiaobin Guo", "abstract": "  Unmanned aerial vehicles (UAVs) equipped with thermal infrared (TIR) cameras\nplay a crucial role in combating nocturnal wildlife poaching. However, TIR\nimages often face challenges such as jitter, and wildlife overlap,\nnecessitating UAVs to possess the capability to identify blurred and\noverlapping small targets. Current traditional lightweight networks deployed on\nUAVs struggle to extract features from blurry small targets. To address this\nissue, we developed ALSS-YOLO, an efficient and lightweight detector optimized\nfor TIR aerial images. Firstly, we propose a novel Adaptive Lightweight Channel\nSplit and Shuffling (ALSS) module. This module employs an adaptive channel\nsplit strategy to optimize feature extraction and integrates a channel\nshuffling mechanism to enhance information exchange between channels. This\nimproves the extraction of blurry features, crucial for handling jitter-induced\nblur and overlapping targets. Secondly, we developed a Lightweight Coordinate\nAttention (LCA) module that employs adaptive pooling and grouped convolution to\nintegrate feature information across dimensions. This module ensures\nlightweight operation while maintaining high detection precision and robustness\nagainst jitter and target overlap. Additionally, we developed a single-channel\nfocus module to aggregate the width and height information of each channel into\nfour-dimensional channel fusion, which improves the feature representation\nefficiency of infrared images. Finally, we modify the localization loss\nfunction to emphasize the loss value associated with small objects to improve\nlocalization accuracy. Extensive experiments on the BIRDSAI and ISOD TIR UAV\nwildlife datasets show that ALSS-YOLO achieves state-of-the-art performance,\nOur code is openly available at\nhttps://github.com/helloworlder8/computer_vision.\n", "link": "http://arxiv.org/abs/2409.06259v2", "date": "2024-09-12", "relevancy": 2.2013, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5805}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5327}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5188}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ALSS-YOLO%3A%20An%20Adaptive%20Lightweight%20Channel%20Split%20and%20Shuffling%20Network%0A%20%20for%20TIR%20Wildlife%20Detection%20in%20UAV%20Imagery&body=Title%3A%20ALSS-YOLO%3A%20An%20Adaptive%20Lightweight%20Channel%20Split%20and%20Shuffling%20Network%0A%20%20for%20TIR%20Wildlife%20Detection%20in%20UAV%20Imagery%0AAuthor%3A%20Ang%20He%20and%20Xiaobo%20Li%20and%20Ximei%20Wu%20and%20Chengyue%20Su%20and%20Jing%20Chen%20and%20Sheng%20Xu%20and%20Xiaobin%20Guo%0AAbstract%3A%20%20%20Unmanned%20aerial%20vehicles%20%28UAVs%29%20equipped%20with%20thermal%20infrared%20%28TIR%29%20cameras%0Aplay%20a%20crucial%20role%20in%20combating%20nocturnal%20wildlife%20poaching.%20However%2C%20TIR%0Aimages%20often%20face%20challenges%20such%20as%20jitter%2C%20and%20wildlife%20overlap%2C%0Anecessitating%20UAVs%20to%20possess%20the%20capability%20to%20identify%20blurred%20and%0Aoverlapping%20small%20targets.%20Current%20traditional%20lightweight%20networks%20deployed%20on%0AUAVs%20struggle%20to%20extract%20features%20from%20blurry%20small%20targets.%20To%20address%20this%0Aissue%2C%20we%20developed%20ALSS-YOLO%2C%20an%20efficient%20and%20lightweight%20detector%20optimized%0Afor%20TIR%20aerial%20images.%20Firstly%2C%20we%20propose%20a%20novel%20Adaptive%20Lightweight%20Channel%0ASplit%20and%20Shuffling%20%28ALSS%29%20module.%20This%20module%20employs%20an%20adaptive%20channel%0Asplit%20strategy%20to%20optimize%20feature%20extraction%20and%20integrates%20a%20channel%0Ashuffling%20mechanism%20to%20enhance%20information%20exchange%20between%20channels.%20This%0Aimproves%20the%20extraction%20of%20blurry%20features%2C%20crucial%20for%20handling%20jitter-induced%0Ablur%20and%20overlapping%20targets.%20Secondly%2C%20we%20developed%20a%20Lightweight%20Coordinate%0AAttention%20%28LCA%29%20module%20that%20employs%20adaptive%20pooling%20and%20grouped%20convolution%20to%0Aintegrate%20feature%20information%20across%20dimensions.%20This%20module%20ensures%0Alightweight%20operation%20while%20maintaining%20high%20detection%20precision%20and%20robustness%0Aagainst%20jitter%20and%20target%20overlap.%20Additionally%2C%20we%20developed%20a%20single-channel%0Afocus%20module%20to%20aggregate%20the%20width%20and%20height%20information%20of%20each%20channel%20into%0Afour-dimensional%20channel%20fusion%2C%20which%20improves%20the%20feature%20representation%0Aefficiency%20of%20infrared%20images.%20Finally%2C%20we%20modify%20the%20localization%20loss%0Afunction%20to%20emphasize%20the%20loss%20value%20associated%20with%20small%20objects%20to%20improve%0Alocalization%20accuracy.%20Extensive%20experiments%20on%20the%20BIRDSAI%20and%20ISOD%20TIR%20UAV%0Awildlife%20datasets%20show%20that%20ALSS-YOLO%20achieves%20state-of-the-art%20performance%2C%0AOur%20code%20is%20openly%20available%20at%0Ahttps%3A//github.com/helloworlder8/computer_vision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06259v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DALSS-YOLO%253A%2520An%2520Adaptive%2520Lightweight%2520Channel%2520Split%2520and%2520Shuffling%2520Network%250A%2520%2520for%2520TIR%2520Wildlife%2520Detection%2520in%2520UAV%2520Imagery%26entry.906535625%3DAng%2520He%2520and%2520Xiaobo%2520Li%2520and%2520Ximei%2520Wu%2520and%2520Chengyue%2520Su%2520and%2520Jing%2520Chen%2520and%2520Sheng%2520Xu%2520and%2520Xiaobin%2520Guo%26entry.1292438233%3D%2520%2520Unmanned%2520aerial%2520vehicles%2520%2528UAVs%2529%2520equipped%2520with%2520thermal%2520infrared%2520%2528TIR%2529%2520cameras%250Aplay%2520a%2520crucial%2520role%2520in%2520combating%2520nocturnal%2520wildlife%2520poaching.%2520However%252C%2520TIR%250Aimages%2520often%2520face%2520challenges%2520such%2520as%2520jitter%252C%2520and%2520wildlife%2520overlap%252C%250Anecessitating%2520UAVs%2520to%2520possess%2520the%2520capability%2520to%2520identify%2520blurred%2520and%250Aoverlapping%2520small%2520targets.%2520Current%2520traditional%2520lightweight%2520networks%2520deployed%2520on%250AUAVs%2520struggle%2520to%2520extract%2520features%2520from%2520blurry%2520small%2520targets.%2520To%2520address%2520this%250Aissue%252C%2520we%2520developed%2520ALSS-YOLO%252C%2520an%2520efficient%2520and%2520lightweight%2520detector%2520optimized%250Afor%2520TIR%2520aerial%2520images.%2520Firstly%252C%2520we%2520propose%2520a%2520novel%2520Adaptive%2520Lightweight%2520Channel%250ASplit%2520and%2520Shuffling%2520%2528ALSS%2529%2520module.%2520This%2520module%2520employs%2520an%2520adaptive%2520channel%250Asplit%2520strategy%2520to%2520optimize%2520feature%2520extraction%2520and%2520integrates%2520a%2520channel%250Ashuffling%2520mechanism%2520to%2520enhance%2520information%2520exchange%2520between%2520channels.%2520This%250Aimproves%2520the%2520extraction%2520of%2520blurry%2520features%252C%2520crucial%2520for%2520handling%2520jitter-induced%250Ablur%2520and%2520overlapping%2520targets.%2520Secondly%252C%2520we%2520developed%2520a%2520Lightweight%2520Coordinate%250AAttention%2520%2528LCA%2529%2520module%2520that%2520employs%2520adaptive%2520pooling%2520and%2520grouped%2520convolution%2520to%250Aintegrate%2520feature%2520information%2520across%2520dimensions.%2520This%2520module%2520ensures%250Alightweight%2520operation%2520while%2520maintaining%2520high%2520detection%2520precision%2520and%2520robustness%250Aagainst%2520jitter%2520and%2520target%2520overlap.%2520Additionally%252C%2520we%2520developed%2520a%2520single-channel%250Afocus%2520module%2520to%2520aggregate%2520the%2520width%2520and%2520height%2520information%2520of%2520each%2520channel%2520into%250Afour-dimensional%2520channel%2520fusion%252C%2520which%2520improves%2520the%2520feature%2520representation%250Aefficiency%2520of%2520infrared%2520images.%2520Finally%252C%2520we%2520modify%2520the%2520localization%2520loss%250Afunction%2520to%2520emphasize%2520the%2520loss%2520value%2520associated%2520with%2520small%2520objects%2520to%2520improve%250Alocalization%2520accuracy.%2520Extensive%2520experiments%2520on%2520the%2520BIRDSAI%2520and%2520ISOD%2520TIR%2520UAV%250Awildlife%2520datasets%2520show%2520that%2520ALSS-YOLO%2520achieves%2520state-of-the-art%2520performance%252C%250AOur%2520code%2520is%2520openly%2520available%2520at%250Ahttps%253A//github.com/helloworlder8/computer_vision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06259v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ALSS-YOLO%3A%20An%20Adaptive%20Lightweight%20Channel%20Split%20and%20Shuffling%20Network%0A%20%20for%20TIR%20Wildlife%20Detection%20in%20UAV%20Imagery&entry.906535625=Ang%20He%20and%20Xiaobo%20Li%20and%20Ximei%20Wu%20and%20Chengyue%20Su%20and%20Jing%20Chen%20and%20Sheng%20Xu%20and%20Xiaobin%20Guo&entry.1292438233=%20%20Unmanned%20aerial%20vehicles%20%28UAVs%29%20equipped%20with%20thermal%20infrared%20%28TIR%29%20cameras%0Aplay%20a%20crucial%20role%20in%20combating%20nocturnal%20wildlife%20poaching.%20However%2C%20TIR%0Aimages%20often%20face%20challenges%20such%20as%20jitter%2C%20and%20wildlife%20overlap%2C%0Anecessitating%20UAVs%20to%20possess%20the%20capability%20to%20identify%20blurred%20and%0Aoverlapping%20small%20targets.%20Current%20traditional%20lightweight%20networks%20deployed%20on%0AUAVs%20struggle%20to%20extract%20features%20from%20blurry%20small%20targets.%20To%20address%20this%0Aissue%2C%20we%20developed%20ALSS-YOLO%2C%20an%20efficient%20and%20lightweight%20detector%20optimized%0Afor%20TIR%20aerial%20images.%20Firstly%2C%20we%20propose%20a%20novel%20Adaptive%20Lightweight%20Channel%0ASplit%20and%20Shuffling%20%28ALSS%29%20module.%20This%20module%20employs%20an%20adaptive%20channel%0Asplit%20strategy%20to%20optimize%20feature%20extraction%20and%20integrates%20a%20channel%0Ashuffling%20mechanism%20to%20enhance%20information%20exchange%20between%20channels.%20This%0Aimproves%20the%20extraction%20of%20blurry%20features%2C%20crucial%20for%20handling%20jitter-induced%0Ablur%20and%20overlapping%20targets.%20Secondly%2C%20we%20developed%20a%20Lightweight%20Coordinate%0AAttention%20%28LCA%29%20module%20that%20employs%20adaptive%20pooling%20and%20grouped%20convolution%20to%0Aintegrate%20feature%20information%20across%20dimensions.%20This%20module%20ensures%0Alightweight%20operation%20while%20maintaining%20high%20detection%20precision%20and%20robustness%0Aagainst%20jitter%20and%20target%20overlap.%20Additionally%2C%20we%20developed%20a%20single-channel%0Afocus%20module%20to%20aggregate%20the%20width%20and%20height%20information%20of%20each%20channel%20into%0Afour-dimensional%20channel%20fusion%2C%20which%20improves%20the%20feature%20representation%0Aefficiency%20of%20infrared%20images.%20Finally%2C%20we%20modify%20the%20localization%20loss%0Afunction%20to%20emphasize%20the%20loss%20value%20associated%20with%20small%20objects%20to%20improve%0Alocalization%20accuracy.%20Extensive%20experiments%20on%20the%20BIRDSAI%20and%20ISOD%20TIR%20UAV%0Awildlife%20datasets%20show%20that%20ALSS-YOLO%20achieves%20state-of-the-art%20performance%2C%0AOur%20code%20is%20openly%20available%20at%0Ahttps%3A//github.com/helloworlder8/computer_vision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06259v2&entry.124074799=Read"},
{"title": "DSDFormer: An Innovative Transformer-Mamba Framework for Robust\n  High-Precision Driver Distraction Identification", "author": "Junzhou Chen and Zirui Zhang and Jing Yu and Heqiang Huang and Ronghui Zhang and Xuemiao Xu and Bin Sheng and Hong Yan", "abstract": "  Driver distraction remains a leading cause of traffic accidents, posing a\ncritical threat to road safety globally. As intelligent transportation systems\nevolve, accurate and real-time identification of driver distraction has become\nessential. However, existing methods struggle to capture both global contextual\nand fine-grained local features while contending with noisy labels in training\ndatasets. To address these challenges, we propose DSDFormer, a novel framework\nthat integrates the strengths of Transformer and Mamba architectures through a\nDual State Domain Attention (DSDA) mechanism, enabling a balance between\nlong-range dependencies and detailed feature extraction for robust driver\nbehavior recognition. Additionally, we introduce Temporal Reasoning Confident\nLearning (TRCL), an unsupervised approach that refines noisy labels by\nleveraging spatiotemporal correlations in video sequences. Our model achieves\nstate-of-the-art performance on the AUC-V1, AUC-V2, and 100-Driver datasets and\ndemonstrates real-time processing efficiency on the NVIDIA Jetson AGX Orin\nplatform. Extensive experimental results confirm that DSDFormer and TRCL\nsignificantly improve both the accuracy and robustness of driver distraction\ndetection, offering a scalable solution to enhance road safety.\n", "link": "http://arxiv.org/abs/2409.05587v2", "date": "2024-09-12", "relevancy": 2.1976, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5792}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5451}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DSDFormer%3A%20An%20Innovative%20Transformer-Mamba%20Framework%20for%20Robust%0A%20%20High-Precision%20Driver%20Distraction%20Identification&body=Title%3A%20DSDFormer%3A%20An%20Innovative%20Transformer-Mamba%20Framework%20for%20Robust%0A%20%20High-Precision%20Driver%20Distraction%20Identification%0AAuthor%3A%20Junzhou%20Chen%20and%20Zirui%20Zhang%20and%20Jing%20Yu%20and%20Heqiang%20Huang%20and%20Ronghui%20Zhang%20and%20Xuemiao%20Xu%20and%20Bin%20Sheng%20and%20Hong%20Yan%0AAbstract%3A%20%20%20Driver%20distraction%20remains%20a%20leading%20cause%20of%20traffic%20accidents%2C%20posing%20a%0Acritical%20threat%20to%20road%20safety%20globally.%20As%20intelligent%20transportation%20systems%0Aevolve%2C%20accurate%20and%20real-time%20identification%20of%20driver%20distraction%20has%20become%0Aessential.%20However%2C%20existing%20methods%20struggle%20to%20capture%20both%20global%20contextual%0Aand%20fine-grained%20local%20features%20while%20contending%20with%20noisy%20labels%20in%20training%0Adatasets.%20To%20address%20these%20challenges%2C%20we%20propose%20DSDFormer%2C%20a%20novel%20framework%0Athat%20integrates%20the%20strengths%20of%20Transformer%20and%20Mamba%20architectures%20through%20a%0ADual%20State%20Domain%20Attention%20%28DSDA%29%20mechanism%2C%20enabling%20a%20balance%20between%0Along-range%20dependencies%20and%20detailed%20feature%20extraction%20for%20robust%20driver%0Abehavior%20recognition.%20Additionally%2C%20we%20introduce%20Temporal%20Reasoning%20Confident%0ALearning%20%28TRCL%29%2C%20an%20unsupervised%20approach%20that%20refines%20noisy%20labels%20by%0Aleveraging%20spatiotemporal%20correlations%20in%20video%20sequences.%20Our%20model%20achieves%0Astate-of-the-art%20performance%20on%20the%20AUC-V1%2C%20AUC-V2%2C%20and%20100-Driver%20datasets%20and%0Ademonstrates%20real-time%20processing%20efficiency%20on%20the%20NVIDIA%20Jetson%20AGX%20Orin%0Aplatform.%20Extensive%20experimental%20results%20confirm%20that%20DSDFormer%20and%20TRCL%0Asignificantly%20improve%20both%20the%20accuracy%20and%20robustness%20of%20driver%20distraction%0Adetection%2C%20offering%20a%20scalable%20solution%20to%20enhance%20road%20safety.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05587v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDSDFormer%253A%2520An%2520Innovative%2520Transformer-Mamba%2520Framework%2520for%2520Robust%250A%2520%2520High-Precision%2520Driver%2520Distraction%2520Identification%26entry.906535625%3DJunzhou%2520Chen%2520and%2520Zirui%2520Zhang%2520and%2520Jing%2520Yu%2520and%2520Heqiang%2520Huang%2520and%2520Ronghui%2520Zhang%2520and%2520Xuemiao%2520Xu%2520and%2520Bin%2520Sheng%2520and%2520Hong%2520Yan%26entry.1292438233%3D%2520%2520Driver%2520distraction%2520remains%2520a%2520leading%2520cause%2520of%2520traffic%2520accidents%252C%2520posing%2520a%250Acritical%2520threat%2520to%2520road%2520safety%2520globally.%2520As%2520intelligent%2520transportation%2520systems%250Aevolve%252C%2520accurate%2520and%2520real-time%2520identification%2520of%2520driver%2520distraction%2520has%2520become%250Aessential.%2520However%252C%2520existing%2520methods%2520struggle%2520to%2520capture%2520both%2520global%2520contextual%250Aand%2520fine-grained%2520local%2520features%2520while%2520contending%2520with%2520noisy%2520labels%2520in%2520training%250Adatasets.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520DSDFormer%252C%2520a%2520novel%2520framework%250Athat%2520integrates%2520the%2520strengths%2520of%2520Transformer%2520and%2520Mamba%2520architectures%2520through%2520a%250ADual%2520State%2520Domain%2520Attention%2520%2528DSDA%2529%2520mechanism%252C%2520enabling%2520a%2520balance%2520between%250Along-range%2520dependencies%2520and%2520detailed%2520feature%2520extraction%2520for%2520robust%2520driver%250Abehavior%2520recognition.%2520Additionally%252C%2520we%2520introduce%2520Temporal%2520Reasoning%2520Confident%250ALearning%2520%2528TRCL%2529%252C%2520an%2520unsupervised%2520approach%2520that%2520refines%2520noisy%2520labels%2520by%250Aleveraging%2520spatiotemporal%2520correlations%2520in%2520video%2520sequences.%2520Our%2520model%2520achieves%250Astate-of-the-art%2520performance%2520on%2520the%2520AUC-V1%252C%2520AUC-V2%252C%2520and%2520100-Driver%2520datasets%2520and%250Ademonstrates%2520real-time%2520processing%2520efficiency%2520on%2520the%2520NVIDIA%2520Jetson%2520AGX%2520Orin%250Aplatform.%2520Extensive%2520experimental%2520results%2520confirm%2520that%2520DSDFormer%2520and%2520TRCL%250Asignificantly%2520improve%2520both%2520the%2520accuracy%2520and%2520robustness%2520of%2520driver%2520distraction%250Adetection%252C%2520offering%2520a%2520scalable%2520solution%2520to%2520enhance%2520road%2520safety.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05587v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DSDFormer%3A%20An%20Innovative%20Transformer-Mamba%20Framework%20for%20Robust%0A%20%20High-Precision%20Driver%20Distraction%20Identification&entry.906535625=Junzhou%20Chen%20and%20Zirui%20Zhang%20and%20Jing%20Yu%20and%20Heqiang%20Huang%20and%20Ronghui%20Zhang%20and%20Xuemiao%20Xu%20and%20Bin%20Sheng%20and%20Hong%20Yan&entry.1292438233=%20%20Driver%20distraction%20remains%20a%20leading%20cause%20of%20traffic%20accidents%2C%20posing%20a%0Acritical%20threat%20to%20road%20safety%20globally.%20As%20intelligent%20transportation%20systems%0Aevolve%2C%20accurate%20and%20real-time%20identification%20of%20driver%20distraction%20has%20become%0Aessential.%20However%2C%20existing%20methods%20struggle%20to%20capture%20both%20global%20contextual%0Aand%20fine-grained%20local%20features%20while%20contending%20with%20noisy%20labels%20in%20training%0Adatasets.%20To%20address%20these%20challenges%2C%20we%20propose%20DSDFormer%2C%20a%20novel%20framework%0Athat%20integrates%20the%20strengths%20of%20Transformer%20and%20Mamba%20architectures%20through%20a%0ADual%20State%20Domain%20Attention%20%28DSDA%29%20mechanism%2C%20enabling%20a%20balance%20between%0Along-range%20dependencies%20and%20detailed%20feature%20extraction%20for%20robust%20driver%0Abehavior%20recognition.%20Additionally%2C%20we%20introduce%20Temporal%20Reasoning%20Confident%0ALearning%20%28TRCL%29%2C%20an%20unsupervised%20approach%20that%20refines%20noisy%20labels%20by%0Aleveraging%20spatiotemporal%20correlations%20in%20video%20sequences.%20Our%20model%20achieves%0Astate-of-the-art%20performance%20on%20the%20AUC-V1%2C%20AUC-V2%2C%20and%20100-Driver%20datasets%20and%0Ademonstrates%20real-time%20processing%20efficiency%20on%20the%20NVIDIA%20Jetson%20AGX%20Orin%0Aplatform.%20Extensive%20experimental%20results%20confirm%20that%20DSDFormer%20and%20TRCL%0Asignificantly%20improve%20both%20the%20accuracy%20and%20robustness%20of%20driver%20distraction%0Adetection%2C%20offering%20a%20scalable%20solution%20to%20enhance%20road%20safety.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05587v2&entry.124074799=Read"},
{"title": "DINOv2 Rocks Geological Image Analysis: Classification, Segmentation,\n  and Interpretability", "author": "Florent Brondolo and Samuel Beaussant", "abstract": "  Recent advancements in computer vision have significantly improved image\nanalysis tasks. Yet, deep learning models often struggle when applied to\ndomains outside their training distribution, such as in geosciences, where\ndomain-specific data can be scarce. This study investigates the classification,\nsegmentation, and interpretability of CT-scan images of rock samples, focusing\non the application of modern computer vision techniques to geoscientific tasks.\nWe compare a range of segmentation methods to assess their efficacy,\nefficiency, and adaptability in geological image analysis. The methods\nevaluated include Otsu thresholding, clustering techniques (K-means, fuzzy\nC-means), a supervised machine learning approach (Random Forest), and deep\nlearning models (UNet, ResNet152, and DINOv2), using ten binary sandstone\ndatasets and three multi-class calcite datasets. DINOv2 was selected for its\npromising results in feature extraction and its potential applicability in\ngeoscientific tasks, prompting further assessment of its interpretability and\neffectiveness in processing CT-scanned rock data. For classification, a\nnon-fine-tuned DINOv2 demonstrates strong performance in classifying rock\nimages, even when the CT-scans are outside its original training set. In\nsegmentation tasks, thresholding and clustering techniques, though\ncomputationally efficient, produce subpar results despite preprocessing\nefforts. In contrast, supervised methods achieve better performance. While deep\nlearning methods demand greater computational resources, they require minimal\nintervention and offer superior generalization. A LoRA fine-tuned DINOv2, in\nparticular, excels in out-of-distribution segmentation and outperforms other\nmethods in multi-class tasks, even with limited data. Notably, the segmentation\nmasks generated by DINOv2 often appear more accurate than the original targets,\nbased on visual inspection.\n", "link": "http://arxiv.org/abs/2407.18100v3", "date": "2024-09-12", "relevancy": 2.1948, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5564}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5564}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DINOv2%20Rocks%20Geological%20Image%20Analysis%3A%20Classification%2C%20Segmentation%2C%0A%20%20and%20Interpretability&body=Title%3A%20DINOv2%20Rocks%20Geological%20Image%20Analysis%3A%20Classification%2C%20Segmentation%2C%0A%20%20and%20Interpretability%0AAuthor%3A%20Florent%20Brondolo%20and%20Samuel%20Beaussant%0AAbstract%3A%20%20%20Recent%20advancements%20in%20computer%20vision%20have%20significantly%20improved%20image%0Aanalysis%20tasks.%20Yet%2C%20deep%20learning%20models%20often%20struggle%20when%20applied%20to%0Adomains%20outside%20their%20training%20distribution%2C%20such%20as%20in%20geosciences%2C%20where%0Adomain-specific%20data%20can%20be%20scarce.%20This%20study%20investigates%20the%20classification%2C%0Asegmentation%2C%20and%20interpretability%20of%20CT-scan%20images%20of%20rock%20samples%2C%20focusing%0Aon%20the%20application%20of%20modern%20computer%20vision%20techniques%20to%20geoscientific%20tasks.%0AWe%20compare%20a%20range%20of%20segmentation%20methods%20to%20assess%20their%20efficacy%2C%0Aefficiency%2C%20and%20adaptability%20in%20geological%20image%20analysis.%20The%20methods%0Aevaluated%20include%20Otsu%20thresholding%2C%20clustering%20techniques%20%28K-means%2C%20fuzzy%0AC-means%29%2C%20a%20supervised%20machine%20learning%20approach%20%28Random%20Forest%29%2C%20and%20deep%0Alearning%20models%20%28UNet%2C%20ResNet152%2C%20and%20DINOv2%29%2C%20using%20ten%20binary%20sandstone%0Adatasets%20and%20three%20multi-class%20calcite%20datasets.%20DINOv2%20was%20selected%20for%20its%0Apromising%20results%20in%20feature%20extraction%20and%20its%20potential%20applicability%20in%0Ageoscientific%20tasks%2C%20prompting%20further%20assessment%20of%20its%20interpretability%20and%0Aeffectiveness%20in%20processing%20CT-scanned%20rock%20data.%20For%20classification%2C%20a%0Anon-fine-tuned%20DINOv2%20demonstrates%20strong%20performance%20in%20classifying%20rock%0Aimages%2C%20even%20when%20the%20CT-scans%20are%20outside%20its%20original%20training%20set.%20In%0Asegmentation%20tasks%2C%20thresholding%20and%20clustering%20techniques%2C%20though%0Acomputationally%20efficient%2C%20produce%20subpar%20results%20despite%20preprocessing%0Aefforts.%20In%20contrast%2C%20supervised%20methods%20achieve%20better%20performance.%20While%20deep%0Alearning%20methods%20demand%20greater%20computational%20resources%2C%20they%20require%20minimal%0Aintervention%20and%20offer%20superior%20generalization.%20A%20LoRA%20fine-tuned%20DINOv2%2C%20in%0Aparticular%2C%20excels%20in%20out-of-distribution%20segmentation%20and%20outperforms%20other%0Amethods%20in%20multi-class%20tasks%2C%20even%20with%20limited%20data.%20Notably%2C%20the%20segmentation%0Amasks%20generated%20by%20DINOv2%20often%20appear%20more%20accurate%20than%20the%20original%20targets%2C%0Abased%20on%20visual%20inspection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18100v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDINOv2%2520Rocks%2520Geological%2520Image%2520Analysis%253A%2520Classification%252C%2520Segmentation%252C%250A%2520%2520and%2520Interpretability%26entry.906535625%3DFlorent%2520Brondolo%2520and%2520Samuel%2520Beaussant%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520computer%2520vision%2520have%2520significantly%2520improved%2520image%250Aanalysis%2520tasks.%2520Yet%252C%2520deep%2520learning%2520models%2520often%2520struggle%2520when%2520applied%2520to%250Adomains%2520outside%2520their%2520training%2520distribution%252C%2520such%2520as%2520in%2520geosciences%252C%2520where%250Adomain-specific%2520data%2520can%2520be%2520scarce.%2520This%2520study%2520investigates%2520the%2520classification%252C%250Asegmentation%252C%2520and%2520interpretability%2520of%2520CT-scan%2520images%2520of%2520rock%2520samples%252C%2520focusing%250Aon%2520the%2520application%2520of%2520modern%2520computer%2520vision%2520techniques%2520to%2520geoscientific%2520tasks.%250AWe%2520compare%2520a%2520range%2520of%2520segmentation%2520methods%2520to%2520assess%2520their%2520efficacy%252C%250Aefficiency%252C%2520and%2520adaptability%2520in%2520geological%2520image%2520analysis.%2520The%2520methods%250Aevaluated%2520include%2520Otsu%2520thresholding%252C%2520clustering%2520techniques%2520%2528K-means%252C%2520fuzzy%250AC-means%2529%252C%2520a%2520supervised%2520machine%2520learning%2520approach%2520%2528Random%2520Forest%2529%252C%2520and%2520deep%250Alearning%2520models%2520%2528UNet%252C%2520ResNet152%252C%2520and%2520DINOv2%2529%252C%2520using%2520ten%2520binary%2520sandstone%250Adatasets%2520and%2520three%2520multi-class%2520calcite%2520datasets.%2520DINOv2%2520was%2520selected%2520for%2520its%250Apromising%2520results%2520in%2520feature%2520extraction%2520and%2520its%2520potential%2520applicability%2520in%250Ageoscientific%2520tasks%252C%2520prompting%2520further%2520assessment%2520of%2520its%2520interpretability%2520and%250Aeffectiveness%2520in%2520processing%2520CT-scanned%2520rock%2520data.%2520For%2520classification%252C%2520a%250Anon-fine-tuned%2520DINOv2%2520demonstrates%2520strong%2520performance%2520in%2520classifying%2520rock%250Aimages%252C%2520even%2520when%2520the%2520CT-scans%2520are%2520outside%2520its%2520original%2520training%2520set.%2520In%250Asegmentation%2520tasks%252C%2520thresholding%2520and%2520clustering%2520techniques%252C%2520though%250Acomputationally%2520efficient%252C%2520produce%2520subpar%2520results%2520despite%2520preprocessing%250Aefforts.%2520In%2520contrast%252C%2520supervised%2520methods%2520achieve%2520better%2520performance.%2520While%2520deep%250Alearning%2520methods%2520demand%2520greater%2520computational%2520resources%252C%2520they%2520require%2520minimal%250Aintervention%2520and%2520offer%2520superior%2520generalization.%2520A%2520LoRA%2520fine-tuned%2520DINOv2%252C%2520in%250Aparticular%252C%2520excels%2520in%2520out-of-distribution%2520segmentation%2520and%2520outperforms%2520other%250Amethods%2520in%2520multi-class%2520tasks%252C%2520even%2520with%2520limited%2520data.%2520Notably%252C%2520the%2520segmentation%250Amasks%2520generated%2520by%2520DINOv2%2520often%2520appear%2520more%2520accurate%2520than%2520the%2520original%2520targets%252C%250Abased%2520on%2520visual%2520inspection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18100v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DINOv2%20Rocks%20Geological%20Image%20Analysis%3A%20Classification%2C%20Segmentation%2C%0A%20%20and%20Interpretability&entry.906535625=Florent%20Brondolo%20and%20Samuel%20Beaussant&entry.1292438233=%20%20Recent%20advancements%20in%20computer%20vision%20have%20significantly%20improved%20image%0Aanalysis%20tasks.%20Yet%2C%20deep%20learning%20models%20often%20struggle%20when%20applied%20to%0Adomains%20outside%20their%20training%20distribution%2C%20such%20as%20in%20geosciences%2C%20where%0Adomain-specific%20data%20can%20be%20scarce.%20This%20study%20investigates%20the%20classification%2C%0Asegmentation%2C%20and%20interpretability%20of%20CT-scan%20images%20of%20rock%20samples%2C%20focusing%0Aon%20the%20application%20of%20modern%20computer%20vision%20techniques%20to%20geoscientific%20tasks.%0AWe%20compare%20a%20range%20of%20segmentation%20methods%20to%20assess%20their%20efficacy%2C%0Aefficiency%2C%20and%20adaptability%20in%20geological%20image%20analysis.%20The%20methods%0Aevaluated%20include%20Otsu%20thresholding%2C%20clustering%20techniques%20%28K-means%2C%20fuzzy%0AC-means%29%2C%20a%20supervised%20machine%20learning%20approach%20%28Random%20Forest%29%2C%20and%20deep%0Alearning%20models%20%28UNet%2C%20ResNet152%2C%20and%20DINOv2%29%2C%20using%20ten%20binary%20sandstone%0Adatasets%20and%20three%20multi-class%20calcite%20datasets.%20DINOv2%20was%20selected%20for%20its%0Apromising%20results%20in%20feature%20extraction%20and%20its%20potential%20applicability%20in%0Ageoscientific%20tasks%2C%20prompting%20further%20assessment%20of%20its%20interpretability%20and%0Aeffectiveness%20in%20processing%20CT-scanned%20rock%20data.%20For%20classification%2C%20a%0Anon-fine-tuned%20DINOv2%20demonstrates%20strong%20performance%20in%20classifying%20rock%0Aimages%2C%20even%20when%20the%20CT-scans%20are%20outside%20its%20original%20training%20set.%20In%0Asegmentation%20tasks%2C%20thresholding%20and%20clustering%20techniques%2C%20though%0Acomputationally%20efficient%2C%20produce%20subpar%20results%20despite%20preprocessing%0Aefforts.%20In%20contrast%2C%20supervised%20methods%20achieve%20better%20performance.%20While%20deep%0Alearning%20methods%20demand%20greater%20computational%20resources%2C%20they%20require%20minimal%0Aintervention%20and%20offer%20superior%20generalization.%20A%20LoRA%20fine-tuned%20DINOv2%2C%20in%0Aparticular%2C%20excels%20in%20out-of-distribution%20segmentation%20and%20outperforms%20other%0Amethods%20in%20multi-class%20tasks%2C%20even%20with%20limited%20data.%20Notably%2C%20the%20segmentation%0Amasks%20generated%20by%20DINOv2%20often%20appear%20more%20accurate%20than%20the%20original%20targets%2C%0Abased%20on%20visual%20inspection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18100v3&entry.124074799=Read"},
{"title": "Self-centering 3-DoF feet controller for hands-free locomotion control\n  in telepresence and virtual reality", "author": "Raphael Memmesheimer and Christian Lenz and Max Schwarz and Michael Schreiber and Sven Behnke", "abstract": "  We present a novel seated feet controller for handling 3-DoF aimed to control\nlocomotion for telepresence robotics and virtual reality environments. Tilting\nthe feet on two axes yields in forward, backward and sideways motion. In\naddition, a separate rotary joint allows for rotation around the vertical axis.\nAttached springs on all joints self-center the controller. The HTC Vive tracker\nis used to translate the trackers' orientation into locomotion commands. The\nproposed self-centering feet controller was used successfully for the ANA\nAvatar XPRIZE competition, where a naive operator traversed the robot through a\nlonger distance, surpassing obstacles while solving various interaction and\nmanipulation tasks in between. We publicly provide the models of the mostly\n3D-printed feet controller for reproduction.\n", "link": "http://arxiv.org/abs/2408.02319v2", "date": "2024-09-12", "relevancy": 2.1911, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6282}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5384}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-centering%203-DoF%20feet%20controller%20for%20hands-free%20locomotion%20control%0A%20%20in%20telepresence%20and%20virtual%20reality&body=Title%3A%20Self-centering%203-DoF%20feet%20controller%20for%20hands-free%20locomotion%20control%0A%20%20in%20telepresence%20and%20virtual%20reality%0AAuthor%3A%20Raphael%20Memmesheimer%20and%20Christian%20Lenz%20and%20Max%20Schwarz%20and%20Michael%20Schreiber%20and%20Sven%20Behnke%0AAbstract%3A%20%20%20We%20present%20a%20novel%20seated%20feet%20controller%20for%20handling%203-DoF%20aimed%20to%20control%0Alocomotion%20for%20telepresence%20robotics%20and%20virtual%20reality%20environments.%20Tilting%0Athe%20feet%20on%20two%20axes%20yields%20in%20forward%2C%20backward%20and%20sideways%20motion.%20In%0Aaddition%2C%20a%20separate%20rotary%20joint%20allows%20for%20rotation%20around%20the%20vertical%20axis.%0AAttached%20springs%20on%20all%20joints%20self-center%20the%20controller.%20The%20HTC%20Vive%20tracker%0Ais%20used%20to%20translate%20the%20trackers%27%20orientation%20into%20locomotion%20commands.%20The%0Aproposed%20self-centering%20feet%20controller%20was%20used%20successfully%20for%20the%20ANA%0AAvatar%20XPRIZE%20competition%2C%20where%20a%20naive%20operator%20traversed%20the%20robot%20through%20a%0Alonger%20distance%2C%20surpassing%20obstacles%20while%20solving%20various%20interaction%20and%0Amanipulation%20tasks%20in%20between.%20We%20publicly%20provide%20the%20models%20of%20the%20mostly%0A3D-printed%20feet%20controller%20for%20reproduction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02319v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-centering%25203-DoF%2520feet%2520controller%2520for%2520hands-free%2520locomotion%2520control%250A%2520%2520in%2520telepresence%2520and%2520virtual%2520reality%26entry.906535625%3DRaphael%2520Memmesheimer%2520and%2520Christian%2520Lenz%2520and%2520Max%2520Schwarz%2520and%2520Michael%2520Schreiber%2520and%2520Sven%2520Behnke%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520seated%2520feet%2520controller%2520for%2520handling%25203-DoF%2520aimed%2520to%2520control%250Alocomotion%2520for%2520telepresence%2520robotics%2520and%2520virtual%2520reality%2520environments.%2520Tilting%250Athe%2520feet%2520on%2520two%2520axes%2520yields%2520in%2520forward%252C%2520backward%2520and%2520sideways%2520motion.%2520In%250Aaddition%252C%2520a%2520separate%2520rotary%2520joint%2520allows%2520for%2520rotation%2520around%2520the%2520vertical%2520axis.%250AAttached%2520springs%2520on%2520all%2520joints%2520self-center%2520the%2520controller.%2520The%2520HTC%2520Vive%2520tracker%250Ais%2520used%2520to%2520translate%2520the%2520trackers%2527%2520orientation%2520into%2520locomotion%2520commands.%2520The%250Aproposed%2520self-centering%2520feet%2520controller%2520was%2520used%2520successfully%2520for%2520the%2520ANA%250AAvatar%2520XPRIZE%2520competition%252C%2520where%2520a%2520naive%2520operator%2520traversed%2520the%2520robot%2520through%2520a%250Alonger%2520distance%252C%2520surpassing%2520obstacles%2520while%2520solving%2520various%2520interaction%2520and%250Amanipulation%2520tasks%2520in%2520between.%2520We%2520publicly%2520provide%2520the%2520models%2520of%2520the%2520mostly%250A3D-printed%2520feet%2520controller%2520for%2520reproduction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02319v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-centering%203-DoF%20feet%20controller%20for%20hands-free%20locomotion%20control%0A%20%20in%20telepresence%20and%20virtual%20reality&entry.906535625=Raphael%20Memmesheimer%20and%20Christian%20Lenz%20and%20Max%20Schwarz%20and%20Michael%20Schreiber%20and%20Sven%20Behnke&entry.1292438233=%20%20We%20present%20a%20novel%20seated%20feet%20controller%20for%20handling%203-DoF%20aimed%20to%20control%0Alocomotion%20for%20telepresence%20robotics%20and%20virtual%20reality%20environments.%20Tilting%0Athe%20feet%20on%20two%20axes%20yields%20in%20forward%2C%20backward%20and%20sideways%20motion.%20In%0Aaddition%2C%20a%20separate%20rotary%20joint%20allows%20for%20rotation%20around%20the%20vertical%20axis.%0AAttached%20springs%20on%20all%20joints%20self-center%20the%20controller.%20The%20HTC%20Vive%20tracker%0Ais%20used%20to%20translate%20the%20trackers%27%20orientation%20into%20locomotion%20commands.%20The%0Aproposed%20self-centering%20feet%20controller%20was%20used%20successfully%20for%20the%20ANA%0AAvatar%20XPRIZE%20competition%2C%20where%20a%20naive%20operator%20traversed%20the%20robot%20through%20a%0Alonger%20distance%2C%20surpassing%20obstacles%20while%20solving%20various%20interaction%20and%0Amanipulation%20tasks%20in%20between.%20We%20publicly%20provide%20the%20models%20of%20the%20mostly%0A3D-printed%20feet%20controller%20for%20reproduction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02319v2&entry.124074799=Read"},
{"title": "LED: Light Enhanced Depth Estimation at Night", "author": "Simon de Moreau and Yasser Almehio and Andrei Bursuc and Hafid El-Idrissi and Bogdan Stanciulescu and Fabien Moutarde", "abstract": "  Nighttime camera-based depth estimation is a highly challenging task,\nespecially for autonomous driving applications, where accurate depth perception\nis essential for ensuring safe navigation. We aim to improve the reliability of\nperception systems at night time, where models trained on daytime data often\nfail in the absence of precise but costly LiDAR sensors. In this work, we\nintroduce Light Enhanced Depth (LED), a novel cost-effective approach that\nsignificantly improves depth estimation in low-light environments by harnessing\na pattern projected by high definition headlights available in modern vehicles.\nLED leads to significant performance boosts across multiple depth-estimation\narchitectures (encoder-decoder, Adabins, DepthFormer) both on synthetic and\nreal datasets. Furthermore, increased performances beyond illuminated areas\nreveal a holistic enhancement in scene understanding. Finally, we release the\nNighttime Synthetic Drive Dataset, a new synthetic and photo-realistic\nnighttime dataset, which comprises 49,990 comprehensively annotated images.\n", "link": "http://arxiv.org/abs/2409.08031v1", "date": "2024-09-12", "relevancy": 2.1868, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5475}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5475}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LED%3A%20Light%20Enhanced%20Depth%20Estimation%20at%20Night&body=Title%3A%20LED%3A%20Light%20Enhanced%20Depth%20Estimation%20at%20Night%0AAuthor%3A%20Simon%20de%20Moreau%20and%20Yasser%20Almehio%20and%20Andrei%20Bursuc%20and%20Hafid%20El-Idrissi%20and%20Bogdan%20Stanciulescu%20and%20Fabien%20Moutarde%0AAbstract%3A%20%20%20Nighttime%20camera-based%20depth%20estimation%20is%20a%20highly%20challenging%20task%2C%0Aespecially%20for%20autonomous%20driving%20applications%2C%20where%20accurate%20depth%20perception%0Ais%20essential%20for%20ensuring%20safe%20navigation.%20We%20aim%20to%20improve%20the%20reliability%20of%0Aperception%20systems%20at%20night%20time%2C%20where%20models%20trained%20on%20daytime%20data%20often%0Afail%20in%20the%20absence%20of%20precise%20but%20costly%20LiDAR%20sensors.%20In%20this%20work%2C%20we%0Aintroduce%20Light%20Enhanced%20Depth%20%28LED%29%2C%20a%20novel%20cost-effective%20approach%20that%0Asignificantly%20improves%20depth%20estimation%20in%20low-light%20environments%20by%20harnessing%0Aa%20pattern%20projected%20by%20high%20definition%20headlights%20available%20in%20modern%20vehicles.%0ALED%20leads%20to%20significant%20performance%20boosts%20across%20multiple%20depth-estimation%0Aarchitectures%20%28encoder-decoder%2C%20Adabins%2C%20DepthFormer%29%20both%20on%20synthetic%20and%0Areal%20datasets.%20Furthermore%2C%20increased%20performances%20beyond%20illuminated%20areas%0Areveal%20a%20holistic%20enhancement%20in%20scene%20understanding.%20Finally%2C%20we%20release%20the%0ANighttime%20Synthetic%20Drive%20Dataset%2C%20a%20new%20synthetic%20and%20photo-realistic%0Anighttime%20dataset%2C%20which%20comprises%2049%2C990%20comprehensively%20annotated%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08031v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLED%253A%2520Light%2520Enhanced%2520Depth%2520Estimation%2520at%2520Night%26entry.906535625%3DSimon%2520de%2520Moreau%2520and%2520Yasser%2520Almehio%2520and%2520Andrei%2520Bursuc%2520and%2520Hafid%2520El-Idrissi%2520and%2520Bogdan%2520Stanciulescu%2520and%2520Fabien%2520Moutarde%26entry.1292438233%3D%2520%2520Nighttime%2520camera-based%2520depth%2520estimation%2520is%2520a%2520highly%2520challenging%2520task%252C%250Aespecially%2520for%2520autonomous%2520driving%2520applications%252C%2520where%2520accurate%2520depth%2520perception%250Ais%2520essential%2520for%2520ensuring%2520safe%2520navigation.%2520We%2520aim%2520to%2520improve%2520the%2520reliability%2520of%250Aperception%2520systems%2520at%2520night%2520time%252C%2520where%2520models%2520trained%2520on%2520daytime%2520data%2520often%250Afail%2520in%2520the%2520absence%2520of%2520precise%2520but%2520costly%2520LiDAR%2520sensors.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520Light%2520Enhanced%2520Depth%2520%2528LED%2529%252C%2520a%2520novel%2520cost-effective%2520approach%2520that%250Asignificantly%2520improves%2520depth%2520estimation%2520in%2520low-light%2520environments%2520by%2520harnessing%250Aa%2520pattern%2520projected%2520by%2520high%2520definition%2520headlights%2520available%2520in%2520modern%2520vehicles.%250ALED%2520leads%2520to%2520significant%2520performance%2520boosts%2520across%2520multiple%2520depth-estimation%250Aarchitectures%2520%2528encoder-decoder%252C%2520Adabins%252C%2520DepthFormer%2529%2520both%2520on%2520synthetic%2520and%250Areal%2520datasets.%2520Furthermore%252C%2520increased%2520performances%2520beyond%2520illuminated%2520areas%250Areveal%2520a%2520holistic%2520enhancement%2520in%2520scene%2520understanding.%2520Finally%252C%2520we%2520release%2520the%250ANighttime%2520Synthetic%2520Drive%2520Dataset%252C%2520a%2520new%2520synthetic%2520and%2520photo-realistic%250Anighttime%2520dataset%252C%2520which%2520comprises%252049%252C990%2520comprehensively%2520annotated%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08031v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LED%3A%20Light%20Enhanced%20Depth%20Estimation%20at%20Night&entry.906535625=Simon%20de%20Moreau%20and%20Yasser%20Almehio%20and%20Andrei%20Bursuc%20and%20Hafid%20El-Idrissi%20and%20Bogdan%20Stanciulescu%20and%20Fabien%20Moutarde&entry.1292438233=%20%20Nighttime%20camera-based%20depth%20estimation%20is%20a%20highly%20challenging%20task%2C%0Aespecially%20for%20autonomous%20driving%20applications%2C%20where%20accurate%20depth%20perception%0Ais%20essential%20for%20ensuring%20safe%20navigation.%20We%20aim%20to%20improve%20the%20reliability%20of%0Aperception%20systems%20at%20night%20time%2C%20where%20models%20trained%20on%20daytime%20data%20often%0Afail%20in%20the%20absence%20of%20precise%20but%20costly%20LiDAR%20sensors.%20In%20this%20work%2C%20we%0Aintroduce%20Light%20Enhanced%20Depth%20%28LED%29%2C%20a%20novel%20cost-effective%20approach%20that%0Asignificantly%20improves%20depth%20estimation%20in%20low-light%20environments%20by%20harnessing%0Aa%20pattern%20projected%20by%20high%20definition%20headlights%20available%20in%20modern%20vehicles.%0ALED%20leads%20to%20significant%20performance%20boosts%20across%20multiple%20depth-estimation%0Aarchitectures%20%28encoder-decoder%2C%20Adabins%2C%20DepthFormer%29%20both%20on%20synthetic%20and%0Areal%20datasets.%20Furthermore%2C%20increased%20performances%20beyond%20illuminated%20areas%0Areveal%20a%20holistic%20enhancement%20in%20scene%20understanding.%20Finally%2C%20we%20release%20the%0ANighttime%20Synthetic%20Drive%20Dataset%2C%20a%20new%20synthetic%20and%20photo-realistic%0Anighttime%20dataset%2C%20which%20comprises%2049%2C990%20comprehensively%20annotated%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08031v1&entry.124074799=Read"},
{"title": "ToolEENet: Tool Affordance 6D Pose Estimation", "author": "Yunlong Wang and Lei Zhang and Yuyang Tu and Hui Zhang and Kaixin Bai and Zhaopeng Chen and Jianwei Zhang", "abstract": "  The exploration of robotic dexterous hands utilizing tools has recently\nattracted considerable attention. A significant challenge in this field is the\nprecise awareness of a tool's pose when grasped, as occlusion by the hand often\ndegrades the quality of the estimation. Additionally, the tool's overall pose\noften fails to accurately represent the contact interaction, thereby limiting\nthe effectiveness of vision-guided, contact-dependent activities. To overcome\nthis limitation, we present the innovative TOOLEE dataset, which, to the best\nof our knowledge, is the first to feature affordance segmentation of a tool's\nend-effector (EE) along with its defined 6D pose based on its usage.\nFurthermore, we propose the ToolEENet framework for accurate 6D pose estimation\nof the tool's EE. This framework begins by segmenting the tool's EE from raw\nRGBD data, then uses a diffusion model-based pose estimator for 6D pose\nestimation at a category-specific level. Addressing the issue of symmetry in\npose estimation, we introduce a symmetry-aware pose representation that\nenhances the consistency of pose estimation. Our approach excels in this field,\ndemonstrating high levels of precision and generalization. Furthermore, it\nshows great promise for application in contact-based manipulation scenarios.\nAll data and codes are available on the project website:\nhttps://tooleenet-iros2024.github.io/\n", "link": "http://arxiv.org/abs/2404.04193v2", "date": "2024-09-12", "relevancy": 2.1442, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5557}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5358}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ToolEENet%3A%20Tool%20Affordance%206D%20Pose%20Estimation&body=Title%3A%20ToolEENet%3A%20Tool%20Affordance%206D%20Pose%20Estimation%0AAuthor%3A%20Yunlong%20Wang%20and%20Lei%20Zhang%20and%20Yuyang%20Tu%20and%20Hui%20Zhang%20and%20Kaixin%20Bai%20and%20Zhaopeng%20Chen%20and%20Jianwei%20Zhang%0AAbstract%3A%20%20%20The%20exploration%20of%20robotic%20dexterous%20hands%20utilizing%20tools%20has%20recently%0Aattracted%20considerable%20attention.%20A%20significant%20challenge%20in%20this%20field%20is%20the%0Aprecise%20awareness%20of%20a%20tool%27s%20pose%20when%20grasped%2C%20as%20occlusion%20by%20the%20hand%20often%0Adegrades%20the%20quality%20of%20the%20estimation.%20Additionally%2C%20the%20tool%27s%20overall%20pose%0Aoften%20fails%20to%20accurately%20represent%20the%20contact%20interaction%2C%20thereby%20limiting%0Athe%20effectiveness%20of%20vision-guided%2C%20contact-dependent%20activities.%20To%20overcome%0Athis%20limitation%2C%20we%20present%20the%20innovative%20TOOLEE%20dataset%2C%20which%2C%20to%20the%20best%0Aof%20our%20knowledge%2C%20is%20the%20first%20to%20feature%20affordance%20segmentation%20of%20a%20tool%27s%0Aend-effector%20%28EE%29%20along%20with%20its%20defined%206D%20pose%20based%20on%20its%20usage.%0AFurthermore%2C%20we%20propose%20the%20ToolEENet%20framework%20for%20accurate%206D%20pose%20estimation%0Aof%20the%20tool%27s%20EE.%20This%20framework%20begins%20by%20segmenting%20the%20tool%27s%20EE%20from%20raw%0ARGBD%20data%2C%20then%20uses%20a%20diffusion%20model-based%20pose%20estimator%20for%206D%20pose%0Aestimation%20at%20a%20category-specific%20level.%20Addressing%20the%20issue%20of%20symmetry%20in%0Apose%20estimation%2C%20we%20introduce%20a%20symmetry-aware%20pose%20representation%20that%0Aenhances%20the%20consistency%20of%20pose%20estimation.%20Our%20approach%20excels%20in%20this%20field%2C%0Ademonstrating%20high%20levels%20of%20precision%20and%20generalization.%20Furthermore%2C%20it%0Ashows%20great%20promise%20for%20application%20in%20contact-based%20manipulation%20scenarios.%0AAll%20data%20and%20codes%20are%20available%20on%20the%20project%20website%3A%0Ahttps%3A//tooleenet-iros2024.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04193v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToolEENet%253A%2520Tool%2520Affordance%25206D%2520Pose%2520Estimation%26entry.906535625%3DYunlong%2520Wang%2520and%2520Lei%2520Zhang%2520and%2520Yuyang%2520Tu%2520and%2520Hui%2520Zhang%2520and%2520Kaixin%2520Bai%2520and%2520Zhaopeng%2520Chen%2520and%2520Jianwei%2520Zhang%26entry.1292438233%3D%2520%2520The%2520exploration%2520of%2520robotic%2520dexterous%2520hands%2520utilizing%2520tools%2520has%2520recently%250Aattracted%2520considerable%2520attention.%2520A%2520significant%2520challenge%2520in%2520this%2520field%2520is%2520the%250Aprecise%2520awareness%2520of%2520a%2520tool%2527s%2520pose%2520when%2520grasped%252C%2520as%2520occlusion%2520by%2520the%2520hand%2520often%250Adegrades%2520the%2520quality%2520of%2520the%2520estimation.%2520Additionally%252C%2520the%2520tool%2527s%2520overall%2520pose%250Aoften%2520fails%2520to%2520accurately%2520represent%2520the%2520contact%2520interaction%252C%2520thereby%2520limiting%250Athe%2520effectiveness%2520of%2520vision-guided%252C%2520contact-dependent%2520activities.%2520To%2520overcome%250Athis%2520limitation%252C%2520we%2520present%2520the%2520innovative%2520TOOLEE%2520dataset%252C%2520which%252C%2520to%2520the%2520best%250Aof%2520our%2520knowledge%252C%2520is%2520the%2520first%2520to%2520feature%2520affordance%2520segmentation%2520of%2520a%2520tool%2527s%250Aend-effector%2520%2528EE%2529%2520along%2520with%2520its%2520defined%25206D%2520pose%2520based%2520on%2520its%2520usage.%250AFurthermore%252C%2520we%2520propose%2520the%2520ToolEENet%2520framework%2520for%2520accurate%25206D%2520pose%2520estimation%250Aof%2520the%2520tool%2527s%2520EE.%2520This%2520framework%2520begins%2520by%2520segmenting%2520the%2520tool%2527s%2520EE%2520from%2520raw%250ARGBD%2520data%252C%2520then%2520uses%2520a%2520diffusion%2520model-based%2520pose%2520estimator%2520for%25206D%2520pose%250Aestimation%2520at%2520a%2520category-specific%2520level.%2520Addressing%2520the%2520issue%2520of%2520symmetry%2520in%250Apose%2520estimation%252C%2520we%2520introduce%2520a%2520symmetry-aware%2520pose%2520representation%2520that%250Aenhances%2520the%2520consistency%2520of%2520pose%2520estimation.%2520Our%2520approach%2520excels%2520in%2520this%2520field%252C%250Ademonstrating%2520high%2520levels%2520of%2520precision%2520and%2520generalization.%2520Furthermore%252C%2520it%250Ashows%2520great%2520promise%2520for%2520application%2520in%2520contact-based%2520manipulation%2520scenarios.%250AAll%2520data%2520and%2520codes%2520are%2520available%2520on%2520the%2520project%2520website%253A%250Ahttps%253A//tooleenet-iros2024.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.04193v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ToolEENet%3A%20Tool%20Affordance%206D%20Pose%20Estimation&entry.906535625=Yunlong%20Wang%20and%20Lei%20Zhang%20and%20Yuyang%20Tu%20and%20Hui%20Zhang%20and%20Kaixin%20Bai%20and%20Zhaopeng%20Chen%20and%20Jianwei%20Zhang&entry.1292438233=%20%20The%20exploration%20of%20robotic%20dexterous%20hands%20utilizing%20tools%20has%20recently%0Aattracted%20considerable%20attention.%20A%20significant%20challenge%20in%20this%20field%20is%20the%0Aprecise%20awareness%20of%20a%20tool%27s%20pose%20when%20grasped%2C%20as%20occlusion%20by%20the%20hand%20often%0Adegrades%20the%20quality%20of%20the%20estimation.%20Additionally%2C%20the%20tool%27s%20overall%20pose%0Aoften%20fails%20to%20accurately%20represent%20the%20contact%20interaction%2C%20thereby%20limiting%0Athe%20effectiveness%20of%20vision-guided%2C%20contact-dependent%20activities.%20To%20overcome%0Athis%20limitation%2C%20we%20present%20the%20innovative%20TOOLEE%20dataset%2C%20which%2C%20to%20the%20best%0Aof%20our%20knowledge%2C%20is%20the%20first%20to%20feature%20affordance%20segmentation%20of%20a%20tool%27s%0Aend-effector%20%28EE%29%20along%20with%20its%20defined%206D%20pose%20based%20on%20its%20usage.%0AFurthermore%2C%20we%20propose%20the%20ToolEENet%20framework%20for%20accurate%206D%20pose%20estimation%0Aof%20the%20tool%27s%20EE.%20This%20framework%20begins%20by%20segmenting%20the%20tool%27s%20EE%20from%20raw%0ARGBD%20data%2C%20then%20uses%20a%20diffusion%20model-based%20pose%20estimator%20for%206D%20pose%0Aestimation%20at%20a%20category-specific%20level.%20Addressing%20the%20issue%20of%20symmetry%20in%0Apose%20estimation%2C%20we%20introduce%20a%20symmetry-aware%20pose%20representation%20that%0Aenhances%20the%20consistency%20of%20pose%20estimation.%20Our%20approach%20excels%20in%20this%20field%2C%0Ademonstrating%20high%20levels%20of%20precision%20and%20generalization.%20Furthermore%2C%20it%0Ashows%20great%20promise%20for%20application%20in%20contact-based%20manipulation%20scenarios.%0AAll%20data%20and%20codes%20are%20available%20on%20the%20project%20website%3A%0Ahttps%3A//tooleenet-iros2024.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04193v2&entry.124074799=Read"},
{"title": "Task-Augmented Cross-View Imputation Network for Partial Multi-View\n  Incomplete Multi-Label Classification", "author": "Xiaohuan Lu and Lian Zhao and Wai Keung Wong and Jie Wen and Jiang Long and Wulin Xie", "abstract": "  In real-world scenarios, multi-view multi-label learning often encounters the\nchallenge of incomplete training data due to limitations in data collection and\nunreliable annotation processes. The absence of multi-view features impairs the\ncomprehensive understanding of samples, omitting crucial details essential for\nclassification. To address this issue, we present a task-augmented cross-view\nimputation network (TACVI-Net) for the purpose of handling partial multi-view\nincomplete multi-label classification. Specifically, we employ a two-stage\nnetwork to derive highly task-relevant features to recover the missing views.\nIn the first stage, we leverage the information bottleneck theory to obtain a\ndiscriminative representation of each view by extracting task-relevant\ninformation through a view-specific encoder-classifier architecture. In the\nsecond stage, an autoencoder based multi-view reconstruction network is\nutilized to extract high-level semantic representation of the augmented\nfeatures and recover the missing data, thereby aiding the final classification\ntask. Extensive experiments on five datasets demonstrate that our TACVI-Net\noutperforms other state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2409.07931v1", "date": "2024-09-12", "relevancy": 2.1381, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5781}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5039}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Task-Augmented%20Cross-View%20Imputation%20Network%20for%20Partial%20Multi-View%0A%20%20Incomplete%20Multi-Label%20Classification&body=Title%3A%20Task-Augmented%20Cross-View%20Imputation%20Network%20for%20Partial%20Multi-View%0A%20%20Incomplete%20Multi-Label%20Classification%0AAuthor%3A%20Xiaohuan%20Lu%20and%20Lian%20Zhao%20and%20Wai%20Keung%20Wong%20and%20Jie%20Wen%20and%20Jiang%20Long%20and%20Wulin%20Xie%0AAbstract%3A%20%20%20In%20real-world%20scenarios%2C%20multi-view%20multi-label%20learning%20often%20encounters%20the%0Achallenge%20of%20incomplete%20training%20data%20due%20to%20limitations%20in%20data%20collection%20and%0Aunreliable%20annotation%20processes.%20The%20absence%20of%20multi-view%20features%20impairs%20the%0Acomprehensive%20understanding%20of%20samples%2C%20omitting%20crucial%20details%20essential%20for%0Aclassification.%20To%20address%20this%20issue%2C%20we%20present%20a%20task-augmented%20cross-view%0Aimputation%20network%20%28TACVI-Net%29%20for%20the%20purpose%20of%20handling%20partial%20multi-view%0Aincomplete%20multi-label%20classification.%20Specifically%2C%20we%20employ%20a%20two-stage%0Anetwork%20to%20derive%20highly%20task-relevant%20features%20to%20recover%20the%20missing%20views.%0AIn%20the%20first%20stage%2C%20we%20leverage%20the%20information%20bottleneck%20theory%20to%20obtain%20a%0Adiscriminative%20representation%20of%20each%20view%20by%20extracting%20task-relevant%0Ainformation%20through%20a%20view-specific%20encoder-classifier%20architecture.%20In%20the%0Asecond%20stage%2C%20an%20autoencoder%20based%20multi-view%20reconstruction%20network%20is%0Autilized%20to%20extract%20high-level%20semantic%20representation%20of%20the%20augmented%0Afeatures%20and%20recover%20the%20missing%20data%2C%20thereby%20aiding%20the%20final%20classification%0Atask.%20Extensive%20experiments%20on%20five%20datasets%20demonstrate%20that%20our%20TACVI-Net%0Aoutperforms%20other%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07931v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTask-Augmented%2520Cross-View%2520Imputation%2520Network%2520for%2520Partial%2520Multi-View%250A%2520%2520Incomplete%2520Multi-Label%2520Classification%26entry.906535625%3DXiaohuan%2520Lu%2520and%2520Lian%2520Zhao%2520and%2520Wai%2520Keung%2520Wong%2520and%2520Jie%2520Wen%2520and%2520Jiang%2520Long%2520and%2520Wulin%2520Xie%26entry.1292438233%3D%2520%2520In%2520real-world%2520scenarios%252C%2520multi-view%2520multi-label%2520learning%2520often%2520encounters%2520the%250Achallenge%2520of%2520incomplete%2520training%2520data%2520due%2520to%2520limitations%2520in%2520data%2520collection%2520and%250Aunreliable%2520annotation%2520processes.%2520The%2520absence%2520of%2520multi-view%2520features%2520impairs%2520the%250Acomprehensive%2520understanding%2520of%2520samples%252C%2520omitting%2520crucial%2520details%2520essential%2520for%250Aclassification.%2520To%2520address%2520this%2520issue%252C%2520we%2520present%2520a%2520task-augmented%2520cross-view%250Aimputation%2520network%2520%2528TACVI-Net%2529%2520for%2520the%2520purpose%2520of%2520handling%2520partial%2520multi-view%250Aincomplete%2520multi-label%2520classification.%2520Specifically%252C%2520we%2520employ%2520a%2520two-stage%250Anetwork%2520to%2520derive%2520highly%2520task-relevant%2520features%2520to%2520recover%2520the%2520missing%2520views.%250AIn%2520the%2520first%2520stage%252C%2520we%2520leverage%2520the%2520information%2520bottleneck%2520theory%2520to%2520obtain%2520a%250Adiscriminative%2520representation%2520of%2520each%2520view%2520by%2520extracting%2520task-relevant%250Ainformation%2520through%2520a%2520view-specific%2520encoder-classifier%2520architecture.%2520In%2520the%250Asecond%2520stage%252C%2520an%2520autoencoder%2520based%2520multi-view%2520reconstruction%2520network%2520is%250Autilized%2520to%2520extract%2520high-level%2520semantic%2520representation%2520of%2520the%2520augmented%250Afeatures%2520and%2520recover%2520the%2520missing%2520data%252C%2520thereby%2520aiding%2520the%2520final%2520classification%250Atask.%2520Extensive%2520experiments%2520on%2520five%2520datasets%2520demonstrate%2520that%2520our%2520TACVI-Net%250Aoutperforms%2520other%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07931v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Task-Augmented%20Cross-View%20Imputation%20Network%20for%20Partial%20Multi-View%0A%20%20Incomplete%20Multi-Label%20Classification&entry.906535625=Xiaohuan%20Lu%20and%20Lian%20Zhao%20and%20Wai%20Keung%20Wong%20and%20Jie%20Wen%20and%20Jiang%20Long%20and%20Wulin%20Xie&entry.1292438233=%20%20In%20real-world%20scenarios%2C%20multi-view%20multi-label%20learning%20often%20encounters%20the%0Achallenge%20of%20incomplete%20training%20data%20due%20to%20limitations%20in%20data%20collection%20and%0Aunreliable%20annotation%20processes.%20The%20absence%20of%20multi-view%20features%20impairs%20the%0Acomprehensive%20understanding%20of%20samples%2C%20omitting%20crucial%20details%20essential%20for%0Aclassification.%20To%20address%20this%20issue%2C%20we%20present%20a%20task-augmented%20cross-view%0Aimputation%20network%20%28TACVI-Net%29%20for%20the%20purpose%20of%20handling%20partial%20multi-view%0Aincomplete%20multi-label%20classification.%20Specifically%2C%20we%20employ%20a%20two-stage%0Anetwork%20to%20derive%20highly%20task-relevant%20features%20to%20recover%20the%20missing%20views.%0AIn%20the%20first%20stage%2C%20we%20leverage%20the%20information%20bottleneck%20theory%20to%20obtain%20a%0Adiscriminative%20representation%20of%20each%20view%20by%20extracting%20task-relevant%0Ainformation%20through%20a%20view-specific%20encoder-classifier%20architecture.%20In%20the%0Asecond%20stage%2C%20an%20autoencoder%20based%20multi-view%20reconstruction%20network%20is%0Autilized%20to%20extract%20high-level%20semantic%20representation%20of%20the%20augmented%0Afeatures%20and%20recover%20the%20missing%20data%2C%20thereby%20aiding%20the%20final%20classification%0Atask.%20Extensive%20experiments%20on%20five%20datasets%20demonstrate%20that%20our%20TACVI-Net%0Aoutperforms%20other%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07931v1&entry.124074799=Read"},
{"title": "IFAdapter: Instance Feature Control for Grounded Text-to-Image\n  Generation", "author": "Yinwei Wu and Xianpan Zhou and Bing Ma and Xuefeng Su and Kai Ma and Xinchao Wang", "abstract": "  While Text-to-Image (T2I) diffusion models excel at generating visually\nappealing images of individual instances, they struggle to accurately position\nand control the features generation of multiple instances. The Layout-to-Image\n(L2I) task was introduced to address the positioning challenges by\nincorporating bounding boxes as spatial control signals, but it still falls\nshort in generating precise instance features. In response, we propose the\nInstance Feature Generation (IFG) task, which aims to ensure both positional\naccuracy and feature fidelity in generated instances. To address the IFG task,\nwe introduce the Instance Feature Adapter (IFAdapter). The IFAdapter enhances\nfeature depiction by incorporating additional appearance tokens and utilizing\nan Instance Semantic Map to align instance-level features with spatial\nlocations. The IFAdapter guides the diffusion process as a plug-and-play\nmodule, making it adaptable to various community models. For evaluation, we\ncontribute an IFG benchmark and develop a verification pipeline to objectively\ncompare models' abilities to generate instances with accurate positioning and\nfeatures. Experimental results demonstrate that IFAdapter outperforms other\nmodels in both quantitative and qualitative evaluations.\n", "link": "http://arxiv.org/abs/2409.08240v1", "date": "2024-09-12", "relevancy": 2.1338, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5432}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.527}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5253}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IFAdapter%3A%20Instance%20Feature%20Control%20for%20Grounded%20Text-to-Image%0A%20%20Generation&body=Title%3A%20IFAdapter%3A%20Instance%20Feature%20Control%20for%20Grounded%20Text-to-Image%0A%20%20Generation%0AAuthor%3A%20Yinwei%20Wu%20and%20Xianpan%20Zhou%20and%20Bing%20Ma%20and%20Xuefeng%20Su%20and%20Kai%20Ma%20and%20Xinchao%20Wang%0AAbstract%3A%20%20%20While%20Text-to-Image%20%28T2I%29%20diffusion%20models%20excel%20at%20generating%20visually%0Aappealing%20images%20of%20individual%20instances%2C%20they%20struggle%20to%20accurately%20position%0Aand%20control%20the%20features%20generation%20of%20multiple%20instances.%20The%20Layout-to-Image%0A%28L2I%29%20task%20was%20introduced%20to%20address%20the%20positioning%20challenges%20by%0Aincorporating%20bounding%20boxes%20as%20spatial%20control%20signals%2C%20but%20it%20still%20falls%0Ashort%20in%20generating%20precise%20instance%20features.%20In%20response%2C%20we%20propose%20the%0AInstance%20Feature%20Generation%20%28IFG%29%20task%2C%20which%20aims%20to%20ensure%20both%20positional%0Aaccuracy%20and%20feature%20fidelity%20in%20generated%20instances.%20To%20address%20the%20IFG%20task%2C%0Awe%20introduce%20the%20Instance%20Feature%20Adapter%20%28IFAdapter%29.%20The%20IFAdapter%20enhances%0Afeature%20depiction%20by%20incorporating%20additional%20appearance%20tokens%20and%20utilizing%0Aan%20Instance%20Semantic%20Map%20to%20align%20instance-level%20features%20with%20spatial%0Alocations.%20The%20IFAdapter%20guides%20the%20diffusion%20process%20as%20a%20plug-and-play%0Amodule%2C%20making%20it%20adaptable%20to%20various%20community%20models.%20For%20evaluation%2C%20we%0Acontribute%20an%20IFG%20benchmark%20and%20develop%20a%20verification%20pipeline%20to%20objectively%0Acompare%20models%27%20abilities%20to%20generate%20instances%20with%20accurate%20positioning%20and%0Afeatures.%20Experimental%20results%20demonstrate%20that%20IFAdapter%20outperforms%20other%0Amodels%20in%20both%20quantitative%20and%20qualitative%20evaluations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08240v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIFAdapter%253A%2520Instance%2520Feature%2520Control%2520for%2520Grounded%2520Text-to-Image%250A%2520%2520Generation%26entry.906535625%3DYinwei%2520Wu%2520and%2520Xianpan%2520Zhou%2520and%2520Bing%2520Ma%2520and%2520Xuefeng%2520Su%2520and%2520Kai%2520Ma%2520and%2520Xinchao%2520Wang%26entry.1292438233%3D%2520%2520While%2520Text-to-Image%2520%2528T2I%2529%2520diffusion%2520models%2520excel%2520at%2520generating%2520visually%250Aappealing%2520images%2520of%2520individual%2520instances%252C%2520they%2520struggle%2520to%2520accurately%2520position%250Aand%2520control%2520the%2520features%2520generation%2520of%2520multiple%2520instances.%2520The%2520Layout-to-Image%250A%2528L2I%2529%2520task%2520was%2520introduced%2520to%2520address%2520the%2520positioning%2520challenges%2520by%250Aincorporating%2520bounding%2520boxes%2520as%2520spatial%2520control%2520signals%252C%2520but%2520it%2520still%2520falls%250Ashort%2520in%2520generating%2520precise%2520instance%2520features.%2520In%2520response%252C%2520we%2520propose%2520the%250AInstance%2520Feature%2520Generation%2520%2528IFG%2529%2520task%252C%2520which%2520aims%2520to%2520ensure%2520both%2520positional%250Aaccuracy%2520and%2520feature%2520fidelity%2520in%2520generated%2520instances.%2520To%2520address%2520the%2520IFG%2520task%252C%250Awe%2520introduce%2520the%2520Instance%2520Feature%2520Adapter%2520%2528IFAdapter%2529.%2520The%2520IFAdapter%2520enhances%250Afeature%2520depiction%2520by%2520incorporating%2520additional%2520appearance%2520tokens%2520and%2520utilizing%250Aan%2520Instance%2520Semantic%2520Map%2520to%2520align%2520instance-level%2520features%2520with%2520spatial%250Alocations.%2520The%2520IFAdapter%2520guides%2520the%2520diffusion%2520process%2520as%2520a%2520plug-and-play%250Amodule%252C%2520making%2520it%2520adaptable%2520to%2520various%2520community%2520models.%2520For%2520evaluation%252C%2520we%250Acontribute%2520an%2520IFG%2520benchmark%2520and%2520develop%2520a%2520verification%2520pipeline%2520to%2520objectively%250Acompare%2520models%2527%2520abilities%2520to%2520generate%2520instances%2520with%2520accurate%2520positioning%2520and%250Afeatures.%2520Experimental%2520results%2520demonstrate%2520that%2520IFAdapter%2520outperforms%2520other%250Amodels%2520in%2520both%2520quantitative%2520and%2520qualitative%2520evaluations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08240v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IFAdapter%3A%20Instance%20Feature%20Control%20for%20Grounded%20Text-to-Image%0A%20%20Generation&entry.906535625=Yinwei%20Wu%20and%20Xianpan%20Zhou%20and%20Bing%20Ma%20and%20Xuefeng%20Su%20and%20Kai%20Ma%20and%20Xinchao%20Wang&entry.1292438233=%20%20While%20Text-to-Image%20%28T2I%29%20diffusion%20models%20excel%20at%20generating%20visually%0Aappealing%20images%20of%20individual%20instances%2C%20they%20struggle%20to%20accurately%20position%0Aand%20control%20the%20features%20generation%20of%20multiple%20instances.%20The%20Layout-to-Image%0A%28L2I%29%20task%20was%20introduced%20to%20address%20the%20positioning%20challenges%20by%0Aincorporating%20bounding%20boxes%20as%20spatial%20control%20signals%2C%20but%20it%20still%20falls%0Ashort%20in%20generating%20precise%20instance%20features.%20In%20response%2C%20we%20propose%20the%0AInstance%20Feature%20Generation%20%28IFG%29%20task%2C%20which%20aims%20to%20ensure%20both%20positional%0Aaccuracy%20and%20feature%20fidelity%20in%20generated%20instances.%20To%20address%20the%20IFG%20task%2C%0Awe%20introduce%20the%20Instance%20Feature%20Adapter%20%28IFAdapter%29.%20The%20IFAdapter%20enhances%0Afeature%20depiction%20by%20incorporating%20additional%20appearance%20tokens%20and%20utilizing%0Aan%20Instance%20Semantic%20Map%20to%20align%20instance-level%20features%20with%20spatial%0Alocations.%20The%20IFAdapter%20guides%20the%20diffusion%20process%20as%20a%20plug-and-play%0Amodule%2C%20making%20it%20adaptable%20to%20various%20community%20models.%20For%20evaluation%2C%20we%0Acontribute%20an%20IFG%20benchmark%20and%20develop%20a%20verification%20pipeline%20to%20objectively%0Acompare%20models%27%20abilities%20to%20generate%20instances%20with%20accurate%20positioning%20and%0Afeatures.%20Experimental%20results%20demonstrate%20that%20IFAdapter%20outperforms%20other%0Amodels%20in%20both%20quantitative%20and%20qualitative%20evaluations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08240v1&entry.124074799=Read"},
{"title": "AD-Lite Net: A Lightweight and Concatenated CNN Model for Alzheimer's\n  Detection from MRI Images", "author": "Santanu Roy and Archit Gupta and Shubhi Tiwari and Palak Sahu", "abstract": "  Alzheimer's Disease (AD) is a non-curable progressive neurodegenerative\ndisorder that affects the human brain, leading to a decline in memory,\ncognitive abilities, and eventually, the ability to carry out daily tasks.\nManual diagnosis of Alzheimer's disease from MRI images is fraught with less\nsensitivity and it is a very tedious process for neurologists. Therefore, there\nis a need for an automatic Computer Assisted Diagnosis (CAD) system, which can\ndetect AD at early stages with higher accuracy. In this research, we have\nproposed a novel AD-Lite Net model (trained from scratch), that could alleviate\nthe aforementioned problem. The novelties we bring here in this research are,\n(I) We have proposed a very lightweight CNN model by incorporating Depth Wise\nSeparable Convolutional (DWSC) layers and Global Average Pooling (GAP) layers.\n(II) We have leveraged a ``parallel concatenation block'' (pcb), in the\nproposed AD-Lite Net model. This pcb consists of a Transformation layer\n(Tx-layer), followed by two convolutional layers, which are thereby\nconcatenated with the original base model. This Tx-layer converts the features\ninto very distinct kind of features, which are imperative for the Alzheimer's\ndisease. As a consequence, the proposed AD-Lite Net model with ``parallel\nconcatenation'' converges faster and automatically mitigates the class\nimbalance problem from the MRI datasets in a very generalized way. For the\nvalidity of our proposed model, we have implemented it on three different MRI\ndatasets. Furthermore, we have combined the ADNI and AD datasets and\nsubsequently performed a 10-fold cross-validation experiment to verify the\nmodel's generalization ability. Extensive experimental results showed that our\nproposed model has outperformed all the existing CNN models, and one recent\ntrend Vision Transformer (ViT) model by a significant margin.\n", "link": "http://arxiv.org/abs/2409.08170v1", "date": "2024-09-12", "relevancy": 2.1284, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5467}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5306}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5277}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AD-Lite%20Net%3A%20A%20Lightweight%20and%20Concatenated%20CNN%20Model%20for%20Alzheimer%27s%0A%20%20Detection%20from%20MRI%20Images&body=Title%3A%20AD-Lite%20Net%3A%20A%20Lightweight%20and%20Concatenated%20CNN%20Model%20for%20Alzheimer%27s%0A%20%20Detection%20from%20MRI%20Images%0AAuthor%3A%20Santanu%20Roy%20and%20Archit%20Gupta%20and%20Shubhi%20Tiwari%20and%20Palak%20Sahu%0AAbstract%3A%20%20%20Alzheimer%27s%20Disease%20%28AD%29%20is%20a%20non-curable%20progressive%20neurodegenerative%0Adisorder%20that%20affects%20the%20human%20brain%2C%20leading%20to%20a%20decline%20in%20memory%2C%0Acognitive%20abilities%2C%20and%20eventually%2C%20the%20ability%20to%20carry%20out%20daily%20tasks.%0AManual%20diagnosis%20of%20Alzheimer%27s%20disease%20from%20MRI%20images%20is%20fraught%20with%20less%0Asensitivity%20and%20it%20is%20a%20very%20tedious%20process%20for%20neurologists.%20Therefore%2C%20there%0Ais%20a%20need%20for%20an%20automatic%20Computer%20Assisted%20Diagnosis%20%28CAD%29%20system%2C%20which%20can%0Adetect%20AD%20at%20early%20stages%20with%20higher%20accuracy.%20In%20this%20research%2C%20we%20have%0Aproposed%20a%20novel%20AD-Lite%20Net%20model%20%28trained%20from%20scratch%29%2C%20that%20could%20alleviate%0Athe%20aforementioned%20problem.%20The%20novelties%20we%20bring%20here%20in%20this%20research%20are%2C%0A%28I%29%20We%20have%20proposed%20a%20very%20lightweight%20CNN%20model%20by%20incorporating%20Depth%20Wise%0ASeparable%20Convolutional%20%28DWSC%29%20layers%20and%20Global%20Average%20Pooling%20%28GAP%29%20layers.%0A%28II%29%20We%20have%20leveraged%20a%20%60%60parallel%20concatenation%20block%27%27%20%28pcb%29%2C%20in%20the%0Aproposed%20AD-Lite%20Net%20model.%20This%20pcb%20consists%20of%20a%20Transformation%20layer%0A%28Tx-layer%29%2C%20followed%20by%20two%20convolutional%20layers%2C%20which%20are%20thereby%0Aconcatenated%20with%20the%20original%20base%20model.%20This%20Tx-layer%20converts%20the%20features%0Ainto%20very%20distinct%20kind%20of%20features%2C%20which%20are%20imperative%20for%20the%20Alzheimer%27s%0Adisease.%20As%20a%20consequence%2C%20the%20proposed%20AD-Lite%20Net%20model%20with%20%60%60parallel%0Aconcatenation%27%27%20converges%20faster%20and%20automatically%20mitigates%20the%20class%0Aimbalance%20problem%20from%20the%20MRI%20datasets%20in%20a%20very%20generalized%20way.%20For%20the%0Avalidity%20of%20our%20proposed%20model%2C%20we%20have%20implemented%20it%20on%20three%20different%20MRI%0Adatasets.%20Furthermore%2C%20we%20have%20combined%20the%20ADNI%20and%20AD%20datasets%20and%0Asubsequently%20performed%20a%2010-fold%20cross-validation%20experiment%20to%20verify%20the%0Amodel%27s%20generalization%20ability.%20Extensive%20experimental%20results%20showed%20that%20our%0Aproposed%20model%20has%20outperformed%20all%20the%20existing%20CNN%20models%2C%20and%20one%20recent%0Atrend%20Vision%20Transformer%20%28ViT%29%20model%20by%20a%20significant%20margin.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08170v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAD-Lite%2520Net%253A%2520A%2520Lightweight%2520and%2520Concatenated%2520CNN%2520Model%2520for%2520Alzheimer%2527s%250A%2520%2520Detection%2520from%2520MRI%2520Images%26entry.906535625%3DSantanu%2520Roy%2520and%2520Archit%2520Gupta%2520and%2520Shubhi%2520Tiwari%2520and%2520Palak%2520Sahu%26entry.1292438233%3D%2520%2520Alzheimer%2527s%2520Disease%2520%2528AD%2529%2520is%2520a%2520non-curable%2520progressive%2520neurodegenerative%250Adisorder%2520that%2520affects%2520the%2520human%2520brain%252C%2520leading%2520to%2520a%2520decline%2520in%2520memory%252C%250Acognitive%2520abilities%252C%2520and%2520eventually%252C%2520the%2520ability%2520to%2520carry%2520out%2520daily%2520tasks.%250AManual%2520diagnosis%2520of%2520Alzheimer%2527s%2520disease%2520from%2520MRI%2520images%2520is%2520fraught%2520with%2520less%250Asensitivity%2520and%2520it%2520is%2520a%2520very%2520tedious%2520process%2520for%2520neurologists.%2520Therefore%252C%2520there%250Ais%2520a%2520need%2520for%2520an%2520automatic%2520Computer%2520Assisted%2520Diagnosis%2520%2528CAD%2529%2520system%252C%2520which%2520can%250Adetect%2520AD%2520at%2520early%2520stages%2520with%2520higher%2520accuracy.%2520In%2520this%2520research%252C%2520we%2520have%250Aproposed%2520a%2520novel%2520AD-Lite%2520Net%2520model%2520%2528trained%2520from%2520scratch%2529%252C%2520that%2520could%2520alleviate%250Athe%2520aforementioned%2520problem.%2520The%2520novelties%2520we%2520bring%2520here%2520in%2520this%2520research%2520are%252C%250A%2528I%2529%2520We%2520have%2520proposed%2520a%2520very%2520lightweight%2520CNN%2520model%2520by%2520incorporating%2520Depth%2520Wise%250ASeparable%2520Convolutional%2520%2528DWSC%2529%2520layers%2520and%2520Global%2520Average%2520Pooling%2520%2528GAP%2529%2520layers.%250A%2528II%2529%2520We%2520have%2520leveraged%2520a%2520%2560%2560parallel%2520concatenation%2520block%2527%2527%2520%2528pcb%2529%252C%2520in%2520the%250Aproposed%2520AD-Lite%2520Net%2520model.%2520This%2520pcb%2520consists%2520of%2520a%2520Transformation%2520layer%250A%2528Tx-layer%2529%252C%2520followed%2520by%2520two%2520convolutional%2520layers%252C%2520which%2520are%2520thereby%250Aconcatenated%2520with%2520the%2520original%2520base%2520model.%2520This%2520Tx-layer%2520converts%2520the%2520features%250Ainto%2520very%2520distinct%2520kind%2520of%2520features%252C%2520which%2520are%2520imperative%2520for%2520the%2520Alzheimer%2527s%250Adisease.%2520As%2520a%2520consequence%252C%2520the%2520proposed%2520AD-Lite%2520Net%2520model%2520with%2520%2560%2560parallel%250Aconcatenation%2527%2527%2520converges%2520faster%2520and%2520automatically%2520mitigates%2520the%2520class%250Aimbalance%2520problem%2520from%2520the%2520MRI%2520datasets%2520in%2520a%2520very%2520generalized%2520way.%2520For%2520the%250Avalidity%2520of%2520our%2520proposed%2520model%252C%2520we%2520have%2520implemented%2520it%2520on%2520three%2520different%2520MRI%250Adatasets.%2520Furthermore%252C%2520we%2520have%2520combined%2520the%2520ADNI%2520and%2520AD%2520datasets%2520and%250Asubsequently%2520performed%2520a%252010-fold%2520cross-validation%2520experiment%2520to%2520verify%2520the%250Amodel%2527s%2520generalization%2520ability.%2520Extensive%2520experimental%2520results%2520showed%2520that%2520our%250Aproposed%2520model%2520has%2520outperformed%2520all%2520the%2520existing%2520CNN%2520models%252C%2520and%2520one%2520recent%250Atrend%2520Vision%2520Transformer%2520%2528ViT%2529%2520model%2520by%2520a%2520significant%2520margin.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08170v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AD-Lite%20Net%3A%20A%20Lightweight%20and%20Concatenated%20CNN%20Model%20for%20Alzheimer%27s%0A%20%20Detection%20from%20MRI%20Images&entry.906535625=Santanu%20Roy%20and%20Archit%20Gupta%20and%20Shubhi%20Tiwari%20and%20Palak%20Sahu&entry.1292438233=%20%20Alzheimer%27s%20Disease%20%28AD%29%20is%20a%20non-curable%20progressive%20neurodegenerative%0Adisorder%20that%20affects%20the%20human%20brain%2C%20leading%20to%20a%20decline%20in%20memory%2C%0Acognitive%20abilities%2C%20and%20eventually%2C%20the%20ability%20to%20carry%20out%20daily%20tasks.%0AManual%20diagnosis%20of%20Alzheimer%27s%20disease%20from%20MRI%20images%20is%20fraught%20with%20less%0Asensitivity%20and%20it%20is%20a%20very%20tedious%20process%20for%20neurologists.%20Therefore%2C%20there%0Ais%20a%20need%20for%20an%20automatic%20Computer%20Assisted%20Diagnosis%20%28CAD%29%20system%2C%20which%20can%0Adetect%20AD%20at%20early%20stages%20with%20higher%20accuracy.%20In%20this%20research%2C%20we%20have%0Aproposed%20a%20novel%20AD-Lite%20Net%20model%20%28trained%20from%20scratch%29%2C%20that%20could%20alleviate%0Athe%20aforementioned%20problem.%20The%20novelties%20we%20bring%20here%20in%20this%20research%20are%2C%0A%28I%29%20We%20have%20proposed%20a%20very%20lightweight%20CNN%20model%20by%20incorporating%20Depth%20Wise%0ASeparable%20Convolutional%20%28DWSC%29%20layers%20and%20Global%20Average%20Pooling%20%28GAP%29%20layers.%0A%28II%29%20We%20have%20leveraged%20a%20%60%60parallel%20concatenation%20block%27%27%20%28pcb%29%2C%20in%20the%0Aproposed%20AD-Lite%20Net%20model.%20This%20pcb%20consists%20of%20a%20Transformation%20layer%0A%28Tx-layer%29%2C%20followed%20by%20two%20convolutional%20layers%2C%20which%20are%20thereby%0Aconcatenated%20with%20the%20original%20base%20model.%20This%20Tx-layer%20converts%20the%20features%0Ainto%20very%20distinct%20kind%20of%20features%2C%20which%20are%20imperative%20for%20the%20Alzheimer%27s%0Adisease.%20As%20a%20consequence%2C%20the%20proposed%20AD-Lite%20Net%20model%20with%20%60%60parallel%0Aconcatenation%27%27%20converges%20faster%20and%20automatically%20mitigates%20the%20class%0Aimbalance%20problem%20from%20the%20MRI%20datasets%20in%20a%20very%20generalized%20way.%20For%20the%0Avalidity%20of%20our%20proposed%20model%2C%20we%20have%20implemented%20it%20on%20three%20different%20MRI%0Adatasets.%20Furthermore%2C%20we%20have%20combined%20the%20ADNI%20and%20AD%20datasets%20and%0Asubsequently%20performed%20a%2010-fold%20cross-validation%20experiment%20to%20verify%20the%0Amodel%27s%20generalization%20ability.%20Extensive%20experimental%20results%20showed%20that%20our%0Aproposed%20model%20has%20outperformed%20all%20the%20existing%20CNN%20models%2C%20and%20one%20recent%0Atrend%20Vision%20Transformer%20%28ViT%29%20model%20by%20a%20significant%20margin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08170v1&entry.124074799=Read"},
{"title": "Spatial Adaptation Layer: Interpretable Domain Adaptation For Biosignal\n  Sensor Array Applications", "author": "Joao Pereira and Michael Alummoottil and Dimitrios Halatsis and Dario Farina", "abstract": "  Biosignal acquisition is key for healthcare applications and wearable\ndevices, with machine learning offering promising methods for processing\nsignals like surface electromyography (sEMG) and electroencephalography (EEG).\nDespite high within-session performance, intersession performance is hindered\nby electrode shift, a known issue across modalities. Existing solutions often\nrequire large and expensive datasets and/or lack robustness and\ninterpretability. Thus, we propose the Spatial Adaptation Layer (SAL), which\ncan be prepended to any biosignal array model and learns a parametrized affine\ntransformation at the input between two recording sessions. We also introduce\nlearnable baseline normalization (LBN) to reduce baseline fluctuations. Tested\non two HD-sEMG gesture recognition datasets, SAL and LBN outperform standard\nfine-tuning on regular arrays, achieving competitive performance even with a\nlogistic regressor, with orders of magnitude less, physically interpretable\nparameters. Our ablation study shows that forearm circumferential translations\naccount for the majority of performance improvements, in line with sEMG\nphysiological expectations.\n", "link": "http://arxiv.org/abs/2409.08058v1", "date": "2024-09-12", "relevancy": 2.1126, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5421}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5271}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5236}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatial%20Adaptation%20Layer%3A%20Interpretable%20Domain%20Adaptation%20For%20Biosignal%0A%20%20Sensor%20Array%20Applications&body=Title%3A%20Spatial%20Adaptation%20Layer%3A%20Interpretable%20Domain%20Adaptation%20For%20Biosignal%0A%20%20Sensor%20Array%20Applications%0AAuthor%3A%20Joao%20Pereira%20and%20Michael%20Alummoottil%20and%20Dimitrios%20Halatsis%20and%20Dario%20Farina%0AAbstract%3A%20%20%20Biosignal%20acquisition%20is%20key%20for%20healthcare%20applications%20and%20wearable%0Adevices%2C%20with%20machine%20learning%20offering%20promising%20methods%20for%20processing%0Asignals%20like%20surface%20electromyography%20%28sEMG%29%20and%20electroencephalography%20%28EEG%29.%0ADespite%20high%20within-session%20performance%2C%20intersession%20performance%20is%20hindered%0Aby%20electrode%20shift%2C%20a%20known%20issue%20across%20modalities.%20Existing%20solutions%20often%0Arequire%20large%20and%20expensive%20datasets%20and/or%20lack%20robustness%20and%0Ainterpretability.%20Thus%2C%20we%20propose%20the%20Spatial%20Adaptation%20Layer%20%28SAL%29%2C%20which%0Acan%20be%20prepended%20to%20any%20biosignal%20array%20model%20and%20learns%20a%20parametrized%20affine%0Atransformation%20at%20the%20input%20between%20two%20recording%20sessions.%20We%20also%20introduce%0Alearnable%20baseline%20normalization%20%28LBN%29%20to%20reduce%20baseline%20fluctuations.%20Tested%0Aon%20two%20HD-sEMG%20gesture%20recognition%20datasets%2C%20SAL%20and%20LBN%20outperform%20standard%0Afine-tuning%20on%20regular%20arrays%2C%20achieving%20competitive%20performance%20even%20with%20a%0Alogistic%20regressor%2C%20with%20orders%20of%20magnitude%20less%2C%20physically%20interpretable%0Aparameters.%20Our%20ablation%20study%20shows%20that%20forearm%20circumferential%20translations%0Aaccount%20for%20the%20majority%20of%20performance%20improvements%2C%20in%20line%20with%20sEMG%0Aphysiological%20expectations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08058v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatial%2520Adaptation%2520Layer%253A%2520Interpretable%2520Domain%2520Adaptation%2520For%2520Biosignal%250A%2520%2520Sensor%2520Array%2520Applications%26entry.906535625%3DJoao%2520Pereira%2520and%2520Michael%2520Alummoottil%2520and%2520Dimitrios%2520Halatsis%2520and%2520Dario%2520Farina%26entry.1292438233%3D%2520%2520Biosignal%2520acquisition%2520is%2520key%2520for%2520healthcare%2520applications%2520and%2520wearable%250Adevices%252C%2520with%2520machine%2520learning%2520offering%2520promising%2520methods%2520for%2520processing%250Asignals%2520like%2520surface%2520electromyography%2520%2528sEMG%2529%2520and%2520electroencephalography%2520%2528EEG%2529.%250ADespite%2520high%2520within-session%2520performance%252C%2520intersession%2520performance%2520is%2520hindered%250Aby%2520electrode%2520shift%252C%2520a%2520known%2520issue%2520across%2520modalities.%2520Existing%2520solutions%2520often%250Arequire%2520large%2520and%2520expensive%2520datasets%2520and/or%2520lack%2520robustness%2520and%250Ainterpretability.%2520Thus%252C%2520we%2520propose%2520the%2520Spatial%2520Adaptation%2520Layer%2520%2528SAL%2529%252C%2520which%250Acan%2520be%2520prepended%2520to%2520any%2520biosignal%2520array%2520model%2520and%2520learns%2520a%2520parametrized%2520affine%250Atransformation%2520at%2520the%2520input%2520between%2520two%2520recording%2520sessions.%2520We%2520also%2520introduce%250Alearnable%2520baseline%2520normalization%2520%2528LBN%2529%2520to%2520reduce%2520baseline%2520fluctuations.%2520Tested%250Aon%2520two%2520HD-sEMG%2520gesture%2520recognition%2520datasets%252C%2520SAL%2520and%2520LBN%2520outperform%2520standard%250Afine-tuning%2520on%2520regular%2520arrays%252C%2520achieving%2520competitive%2520performance%2520even%2520with%2520a%250Alogistic%2520regressor%252C%2520with%2520orders%2520of%2520magnitude%2520less%252C%2520physically%2520interpretable%250Aparameters.%2520Our%2520ablation%2520study%2520shows%2520that%2520forearm%2520circumferential%2520translations%250Aaccount%2520for%2520the%2520majority%2520of%2520performance%2520improvements%252C%2520in%2520line%2520with%2520sEMG%250Aphysiological%2520expectations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08058v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial%20Adaptation%20Layer%3A%20Interpretable%20Domain%20Adaptation%20For%20Biosignal%0A%20%20Sensor%20Array%20Applications&entry.906535625=Joao%20Pereira%20and%20Michael%20Alummoottil%20and%20Dimitrios%20Halatsis%20and%20Dario%20Farina&entry.1292438233=%20%20Biosignal%20acquisition%20is%20key%20for%20healthcare%20applications%20and%20wearable%0Adevices%2C%20with%20machine%20learning%20offering%20promising%20methods%20for%20processing%0Asignals%20like%20surface%20electromyography%20%28sEMG%29%20and%20electroencephalography%20%28EEG%29.%0ADespite%20high%20within-session%20performance%2C%20intersession%20performance%20is%20hindered%0Aby%20electrode%20shift%2C%20a%20known%20issue%20across%20modalities.%20Existing%20solutions%20often%0Arequire%20large%20and%20expensive%20datasets%20and/or%20lack%20robustness%20and%0Ainterpretability.%20Thus%2C%20we%20propose%20the%20Spatial%20Adaptation%20Layer%20%28SAL%29%2C%20which%0Acan%20be%20prepended%20to%20any%20biosignal%20array%20model%20and%20learns%20a%20parametrized%20affine%0Atransformation%20at%20the%20input%20between%20two%20recording%20sessions.%20We%20also%20introduce%0Alearnable%20baseline%20normalization%20%28LBN%29%20to%20reduce%20baseline%20fluctuations.%20Tested%0Aon%20two%20HD-sEMG%20gesture%20recognition%20datasets%2C%20SAL%20and%20LBN%20outperform%20standard%0Afine-tuning%20on%20regular%20arrays%2C%20achieving%20competitive%20performance%20even%20with%20a%0Alogistic%20regressor%2C%20with%20orders%20of%20magnitude%20less%2C%20physically%20interpretable%0Aparameters.%20Our%20ablation%20study%20shows%20that%20forearm%20circumferential%20translations%0Aaccount%20for%20the%20majority%20of%20performance%20improvements%2C%20in%20line%20with%20sEMG%0Aphysiological%20expectations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08058v1&entry.124074799=Read"},
{"title": "DEMAU: Decompose, Explore, Model and Analyse Uncertainties", "author": "Arthur Hoarau and Vincent Lemaire", "abstract": "  Recent research in machine learning has given rise to a flourishing\nliterature on the quantification and decomposition of model uncertainty. This\ninformation can be very useful during interactions with the learner, such as in\nactive learning or adaptive learning, and especially in uncertainty sampling.\nTo allow a simple representation of these total, epistemic (reducible) and\naleatoric (irreducible) uncertainties, we offer DEMAU, an open-source\neducational, exploratory and analytical tool allowing to visualize and explore\nseveral types of uncertainty for classification models in machine learning.\n", "link": "http://arxiv.org/abs/2409.08105v1", "date": "2024-09-12", "relevancy": 2.1112, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6119}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5334}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DEMAU%3A%20Decompose%2C%20Explore%2C%20Model%20and%20Analyse%20Uncertainties&body=Title%3A%20DEMAU%3A%20Decompose%2C%20Explore%2C%20Model%20and%20Analyse%20Uncertainties%0AAuthor%3A%20Arthur%20Hoarau%20and%20Vincent%20Lemaire%0AAbstract%3A%20%20%20Recent%20research%20in%20machine%20learning%20has%20given%20rise%20to%20a%20flourishing%0Aliterature%20on%20the%20quantification%20and%20decomposition%20of%20model%20uncertainty.%20This%0Ainformation%20can%20be%20very%20useful%20during%20interactions%20with%20the%20learner%2C%20such%20as%20in%0Aactive%20learning%20or%20adaptive%20learning%2C%20and%20especially%20in%20uncertainty%20sampling.%0ATo%20allow%20a%20simple%20representation%20of%20these%20total%2C%20epistemic%20%28reducible%29%20and%0Aaleatoric%20%28irreducible%29%20uncertainties%2C%20we%20offer%20DEMAU%2C%20an%20open-source%0Aeducational%2C%20exploratory%20and%20analytical%20tool%20allowing%20to%20visualize%20and%20explore%0Aseveral%20types%20of%20uncertainty%20for%20classification%20models%20in%20machine%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08105v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDEMAU%253A%2520Decompose%252C%2520Explore%252C%2520Model%2520and%2520Analyse%2520Uncertainties%26entry.906535625%3DArthur%2520Hoarau%2520and%2520Vincent%2520Lemaire%26entry.1292438233%3D%2520%2520Recent%2520research%2520in%2520machine%2520learning%2520has%2520given%2520rise%2520to%2520a%2520flourishing%250Aliterature%2520on%2520the%2520quantification%2520and%2520decomposition%2520of%2520model%2520uncertainty.%2520This%250Ainformation%2520can%2520be%2520very%2520useful%2520during%2520interactions%2520with%2520the%2520learner%252C%2520such%2520as%2520in%250Aactive%2520learning%2520or%2520adaptive%2520learning%252C%2520and%2520especially%2520in%2520uncertainty%2520sampling.%250ATo%2520allow%2520a%2520simple%2520representation%2520of%2520these%2520total%252C%2520epistemic%2520%2528reducible%2529%2520and%250Aaleatoric%2520%2528irreducible%2529%2520uncertainties%252C%2520we%2520offer%2520DEMAU%252C%2520an%2520open-source%250Aeducational%252C%2520exploratory%2520and%2520analytical%2520tool%2520allowing%2520to%2520visualize%2520and%2520explore%250Aseveral%2520types%2520of%2520uncertainty%2520for%2520classification%2520models%2520in%2520machine%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08105v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DEMAU%3A%20Decompose%2C%20Explore%2C%20Model%20and%20Analyse%20Uncertainties&entry.906535625=Arthur%20Hoarau%20and%20Vincent%20Lemaire&entry.1292438233=%20%20Recent%20research%20in%20machine%20learning%20has%20given%20rise%20to%20a%20flourishing%0Aliterature%20on%20the%20quantification%20and%20decomposition%20of%20model%20uncertainty.%20This%0Ainformation%20can%20be%20very%20useful%20during%20interactions%20with%20the%20learner%2C%20such%20as%20in%0Aactive%20learning%20or%20adaptive%20learning%2C%20and%20especially%20in%20uncertainty%20sampling.%0ATo%20allow%20a%20simple%20representation%20of%20these%20total%2C%20epistemic%20%28reducible%29%20and%0Aaleatoric%20%28irreducible%29%20uncertainties%2C%20we%20offer%20DEMAU%2C%20an%20open-source%0Aeducational%2C%20exploratory%20and%20analytical%20tool%20allowing%20to%20visualize%20and%20explore%0Aseveral%20types%20of%20uncertainty%20for%20classification%20models%20in%20machine%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08105v1&entry.124074799=Read"},
{"title": "Unlocking the Potential of Model Calibration in Federated Learning", "author": "Yun-Wei Chu and Dong-Jun Han and Seyyedali Hosseinalipour and Christopher Brinton", "abstract": "  Over the past several years, various federated learning (FL) methodologies\nhave been developed to improve model accuracy, a primary performance metric in\nmachine learning. However, to utilize FL in practical decision-making\nscenarios, beyond considering accuracy, the trained model must also have a\nreliable confidence in each of its predictions, an aspect that has been largely\noverlooked in existing FL research. Motivated by this gap, we propose\nNon-Uniform Calibration for Federated Learning (NUCFL), a generic framework\nthat integrates FL with the concept of model calibration. The inherent data\nheterogeneity in FL environments makes model calibration particularly\ndifficult, as it must ensure reliability across diverse data distributions and\nclient conditions. Our NUCFL addresses this challenge by dynamically adjusting\nthe model calibration objectives based on statistical relationships between\neach client's local model and the global model in FL. In particular, NUCFL\nassesses the similarity between local and global model relationships, and\ncontrols the penalty term for the calibration loss during client-side local\ntraining. By doing so, NUCFL effectively aligns calibration needs for the\nglobal model in heterogeneous FL settings while not sacrificing accuracy.\nExtensive experiments show that NUCFL offers flexibility and effectiveness\nacross various FL algorithms, enhancing accuracy as well as model calibration.\n", "link": "http://arxiv.org/abs/2409.04901v2", "date": "2024-09-12", "relevancy": 2.1057, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5644}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5044}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unlocking%20the%20Potential%20of%20Model%20Calibration%20in%20Federated%20Learning&body=Title%3A%20Unlocking%20the%20Potential%20of%20Model%20Calibration%20in%20Federated%20Learning%0AAuthor%3A%20Yun-Wei%20Chu%20and%20Dong-Jun%20Han%20and%20Seyyedali%20Hosseinalipour%20and%20Christopher%20Brinton%0AAbstract%3A%20%20%20Over%20the%20past%20several%20years%2C%20various%20federated%20learning%20%28FL%29%20methodologies%0Ahave%20been%20developed%20to%20improve%20model%20accuracy%2C%20a%20primary%20performance%20metric%20in%0Amachine%20learning.%20However%2C%20to%20utilize%20FL%20in%20practical%20decision-making%0Ascenarios%2C%20beyond%20considering%20accuracy%2C%20the%20trained%20model%20must%20also%20have%20a%0Areliable%20confidence%20in%20each%20of%20its%20predictions%2C%20an%20aspect%20that%20has%20been%20largely%0Aoverlooked%20in%20existing%20FL%20research.%20Motivated%20by%20this%20gap%2C%20we%20propose%0ANon-Uniform%20Calibration%20for%20Federated%20Learning%20%28NUCFL%29%2C%20a%20generic%20framework%0Athat%20integrates%20FL%20with%20the%20concept%20of%20model%20calibration.%20The%20inherent%20data%0Aheterogeneity%20in%20FL%20environments%20makes%20model%20calibration%20particularly%0Adifficult%2C%20as%20it%20must%20ensure%20reliability%20across%20diverse%20data%20distributions%20and%0Aclient%20conditions.%20Our%20NUCFL%20addresses%20this%20challenge%20by%20dynamically%20adjusting%0Athe%20model%20calibration%20objectives%20based%20on%20statistical%20relationships%20between%0Aeach%20client%27s%20local%20model%20and%20the%20global%20model%20in%20FL.%20In%20particular%2C%20NUCFL%0Aassesses%20the%20similarity%20between%20local%20and%20global%20model%20relationships%2C%20and%0Acontrols%20the%20penalty%20term%20for%20the%20calibration%20loss%20during%20client-side%20local%0Atraining.%20By%20doing%20so%2C%20NUCFL%20effectively%20aligns%20calibration%20needs%20for%20the%0Aglobal%20model%20in%20heterogeneous%20FL%20settings%20while%20not%20sacrificing%20accuracy.%0AExtensive%20experiments%20show%20that%20NUCFL%20offers%20flexibility%20and%20effectiveness%0Aacross%20various%20FL%20algorithms%2C%20enhancing%20accuracy%20as%20well%20as%20model%20calibration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04901v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnlocking%2520the%2520Potential%2520of%2520Model%2520Calibration%2520in%2520Federated%2520Learning%26entry.906535625%3DYun-Wei%2520Chu%2520and%2520Dong-Jun%2520Han%2520and%2520Seyyedali%2520Hosseinalipour%2520and%2520Christopher%2520Brinton%26entry.1292438233%3D%2520%2520Over%2520the%2520past%2520several%2520years%252C%2520various%2520federated%2520learning%2520%2528FL%2529%2520methodologies%250Ahave%2520been%2520developed%2520to%2520improve%2520model%2520accuracy%252C%2520a%2520primary%2520performance%2520metric%2520in%250Amachine%2520learning.%2520However%252C%2520to%2520utilize%2520FL%2520in%2520practical%2520decision-making%250Ascenarios%252C%2520beyond%2520considering%2520accuracy%252C%2520the%2520trained%2520model%2520must%2520also%2520have%2520a%250Areliable%2520confidence%2520in%2520each%2520of%2520its%2520predictions%252C%2520an%2520aspect%2520that%2520has%2520been%2520largely%250Aoverlooked%2520in%2520existing%2520FL%2520research.%2520Motivated%2520by%2520this%2520gap%252C%2520we%2520propose%250ANon-Uniform%2520Calibration%2520for%2520Federated%2520Learning%2520%2528NUCFL%2529%252C%2520a%2520generic%2520framework%250Athat%2520integrates%2520FL%2520with%2520the%2520concept%2520of%2520model%2520calibration.%2520The%2520inherent%2520data%250Aheterogeneity%2520in%2520FL%2520environments%2520makes%2520model%2520calibration%2520particularly%250Adifficult%252C%2520as%2520it%2520must%2520ensure%2520reliability%2520across%2520diverse%2520data%2520distributions%2520and%250Aclient%2520conditions.%2520Our%2520NUCFL%2520addresses%2520this%2520challenge%2520by%2520dynamically%2520adjusting%250Athe%2520model%2520calibration%2520objectives%2520based%2520on%2520statistical%2520relationships%2520between%250Aeach%2520client%2527s%2520local%2520model%2520and%2520the%2520global%2520model%2520in%2520FL.%2520In%2520particular%252C%2520NUCFL%250Aassesses%2520the%2520similarity%2520between%2520local%2520and%2520global%2520model%2520relationships%252C%2520and%250Acontrols%2520the%2520penalty%2520term%2520for%2520the%2520calibration%2520loss%2520during%2520client-side%2520local%250Atraining.%2520By%2520doing%2520so%252C%2520NUCFL%2520effectively%2520aligns%2520calibration%2520needs%2520for%2520the%250Aglobal%2520model%2520in%2520heterogeneous%2520FL%2520settings%2520while%2520not%2520sacrificing%2520accuracy.%250AExtensive%2520experiments%2520show%2520that%2520NUCFL%2520offers%2520flexibility%2520and%2520effectiveness%250Aacross%2520various%2520FL%2520algorithms%252C%2520enhancing%2520accuracy%2520as%2520well%2520as%2520model%2520calibration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04901v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unlocking%20the%20Potential%20of%20Model%20Calibration%20in%20Federated%20Learning&entry.906535625=Yun-Wei%20Chu%20and%20Dong-Jun%20Han%20and%20Seyyedali%20Hosseinalipour%20and%20Christopher%20Brinton&entry.1292438233=%20%20Over%20the%20past%20several%20years%2C%20various%20federated%20learning%20%28FL%29%20methodologies%0Ahave%20been%20developed%20to%20improve%20model%20accuracy%2C%20a%20primary%20performance%20metric%20in%0Amachine%20learning.%20However%2C%20to%20utilize%20FL%20in%20practical%20decision-making%0Ascenarios%2C%20beyond%20considering%20accuracy%2C%20the%20trained%20model%20must%20also%20have%20a%0Areliable%20confidence%20in%20each%20of%20its%20predictions%2C%20an%20aspect%20that%20has%20been%20largely%0Aoverlooked%20in%20existing%20FL%20research.%20Motivated%20by%20this%20gap%2C%20we%20propose%0ANon-Uniform%20Calibration%20for%20Federated%20Learning%20%28NUCFL%29%2C%20a%20generic%20framework%0Athat%20integrates%20FL%20with%20the%20concept%20of%20model%20calibration.%20The%20inherent%20data%0Aheterogeneity%20in%20FL%20environments%20makes%20model%20calibration%20particularly%0Adifficult%2C%20as%20it%20must%20ensure%20reliability%20across%20diverse%20data%20distributions%20and%0Aclient%20conditions.%20Our%20NUCFL%20addresses%20this%20challenge%20by%20dynamically%20adjusting%0Athe%20model%20calibration%20objectives%20based%20on%20statistical%20relationships%20between%0Aeach%20client%27s%20local%20model%20and%20the%20global%20model%20in%20FL.%20In%20particular%2C%20NUCFL%0Aassesses%20the%20similarity%20between%20local%20and%20global%20model%20relationships%2C%20and%0Acontrols%20the%20penalty%20term%20for%20the%20calibration%20loss%20during%20client-side%20local%0Atraining.%20By%20doing%20so%2C%20NUCFL%20effectively%20aligns%20calibration%20needs%20for%20the%0Aglobal%20model%20in%20heterogeneous%20FL%20settings%20while%20not%20sacrificing%20accuracy.%0AExtensive%20experiments%20show%20that%20NUCFL%20offers%20flexibility%20and%20effectiveness%0Aacross%20various%20FL%20algorithms%2C%20enhancing%20accuracy%20as%20well%20as%20model%20calibration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04901v2&entry.124074799=Read"},
{"title": "Universal Trajectory Optimization Framework for Differential-Driven\n  Robot Class", "author": "Mengke Zhang and Zhichao Han and Chao Xu and Fei Gao and Yanjun Cao", "abstract": "  Differential-driven robots are widely used in various scenarios thanks to\ntheir straightforward principle, from household service robots to disaster\nresponse field robots. There are several different types of deriving mechanisms\nconsidering the real-world applications, including two-wheeled, four-wheeled\nskid-steering, tracked robots, etc. The differences in the driving mechanism\nusually require specific kinematic modeling when precise controlling is\ndesired. Furthermore, the nonholonomic dynamics and possible lateral slip lead\nto different degrees of difficulty in getting feasible and high-quality\ntrajectories. Therefore, a comprehensive trajectory optimization framework to\ncompute trajectories efficiently for various kinds of differential-driven\nrobots is highly desirable. In this paper, we propose a universal trajectory\noptimization framework that can be applied to differential-driven robot class,\nenabling the generation of high-quality trajectories within a restricted\ncomputational timeframe. We introduce a novel trajectory representation based\non polynomial parameterization of motion states or their integrals, such as\nangular and linear velocities, that inherently matching robots' motion to the\ncontrol principle for differential-driven robot class. The trajectory\noptimization problem is formulated to minimize complexity while prioritizing\nsafety and operational efficiency. We then build a full-stack autonomous\nplanning and control system to show the feasibility and robustness. We conduct\nextensive simulations and real-world testing in crowded environments with three\nkinds of differential-driven robots to validate the effectiveness of our\napproach. We will release our method as an open-source package.\n", "link": "http://arxiv.org/abs/2409.07924v1", "date": "2024-09-12", "relevancy": 2.1047, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.548}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5296}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Universal%20Trajectory%20Optimization%20Framework%20for%20Differential-Driven%0A%20%20Robot%20Class&body=Title%3A%20Universal%20Trajectory%20Optimization%20Framework%20for%20Differential-Driven%0A%20%20Robot%20Class%0AAuthor%3A%20Mengke%20Zhang%20and%20Zhichao%20Han%20and%20Chao%20Xu%20and%20Fei%20Gao%20and%20Yanjun%20Cao%0AAbstract%3A%20%20%20Differential-driven%20robots%20are%20widely%20used%20in%20various%20scenarios%20thanks%20to%0Atheir%20straightforward%20principle%2C%20from%20household%20service%20robots%20to%20disaster%0Aresponse%20field%20robots.%20There%20are%20several%20different%20types%20of%20deriving%20mechanisms%0Aconsidering%20the%20real-world%20applications%2C%20including%20two-wheeled%2C%20four-wheeled%0Askid-steering%2C%20tracked%20robots%2C%20etc.%20The%20differences%20in%20the%20driving%20mechanism%0Ausually%20require%20specific%20kinematic%20modeling%20when%20precise%20controlling%20is%0Adesired.%20Furthermore%2C%20the%20nonholonomic%20dynamics%20and%20possible%20lateral%20slip%20lead%0Ato%20different%20degrees%20of%20difficulty%20in%20getting%20feasible%20and%20high-quality%0Atrajectories.%20Therefore%2C%20a%20comprehensive%20trajectory%20optimization%20framework%20to%0Acompute%20trajectories%20efficiently%20for%20various%20kinds%20of%20differential-driven%0Arobots%20is%20highly%20desirable.%20In%20this%20paper%2C%20we%20propose%20a%20universal%20trajectory%0Aoptimization%20framework%20that%20can%20be%20applied%20to%20differential-driven%20robot%20class%2C%0Aenabling%20the%20generation%20of%20high-quality%20trajectories%20within%20a%20restricted%0Acomputational%20timeframe.%20We%20introduce%20a%20novel%20trajectory%20representation%20based%0Aon%20polynomial%20parameterization%20of%20motion%20states%20or%20their%20integrals%2C%20such%20as%0Aangular%20and%20linear%20velocities%2C%20that%20inherently%20matching%20robots%27%20motion%20to%20the%0Acontrol%20principle%20for%20differential-driven%20robot%20class.%20The%20trajectory%0Aoptimization%20problem%20is%20formulated%20to%20minimize%20complexity%20while%20prioritizing%0Asafety%20and%20operational%20efficiency.%20We%20then%20build%20a%20full-stack%20autonomous%0Aplanning%20and%20control%20system%20to%20show%20the%20feasibility%20and%20robustness.%20We%20conduct%0Aextensive%20simulations%20and%20real-world%20testing%20in%20crowded%20environments%20with%20three%0Akinds%20of%20differential-driven%20robots%20to%20validate%20the%20effectiveness%20of%20our%0Aapproach.%20We%20will%20release%20our%20method%20as%20an%20open-source%20package.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07924v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniversal%2520Trajectory%2520Optimization%2520Framework%2520for%2520Differential-Driven%250A%2520%2520Robot%2520Class%26entry.906535625%3DMengke%2520Zhang%2520and%2520Zhichao%2520Han%2520and%2520Chao%2520Xu%2520and%2520Fei%2520Gao%2520and%2520Yanjun%2520Cao%26entry.1292438233%3D%2520%2520Differential-driven%2520robots%2520are%2520widely%2520used%2520in%2520various%2520scenarios%2520thanks%2520to%250Atheir%2520straightforward%2520principle%252C%2520from%2520household%2520service%2520robots%2520to%2520disaster%250Aresponse%2520field%2520robots.%2520There%2520are%2520several%2520different%2520types%2520of%2520deriving%2520mechanisms%250Aconsidering%2520the%2520real-world%2520applications%252C%2520including%2520two-wheeled%252C%2520four-wheeled%250Askid-steering%252C%2520tracked%2520robots%252C%2520etc.%2520The%2520differences%2520in%2520the%2520driving%2520mechanism%250Ausually%2520require%2520specific%2520kinematic%2520modeling%2520when%2520precise%2520controlling%2520is%250Adesired.%2520Furthermore%252C%2520the%2520nonholonomic%2520dynamics%2520and%2520possible%2520lateral%2520slip%2520lead%250Ato%2520different%2520degrees%2520of%2520difficulty%2520in%2520getting%2520feasible%2520and%2520high-quality%250Atrajectories.%2520Therefore%252C%2520a%2520comprehensive%2520trajectory%2520optimization%2520framework%2520to%250Acompute%2520trajectories%2520efficiently%2520for%2520various%2520kinds%2520of%2520differential-driven%250Arobots%2520is%2520highly%2520desirable.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520universal%2520trajectory%250Aoptimization%2520framework%2520that%2520can%2520be%2520applied%2520to%2520differential-driven%2520robot%2520class%252C%250Aenabling%2520the%2520generation%2520of%2520high-quality%2520trajectories%2520within%2520a%2520restricted%250Acomputational%2520timeframe.%2520We%2520introduce%2520a%2520novel%2520trajectory%2520representation%2520based%250Aon%2520polynomial%2520parameterization%2520of%2520motion%2520states%2520or%2520their%2520integrals%252C%2520such%2520as%250Aangular%2520and%2520linear%2520velocities%252C%2520that%2520inherently%2520matching%2520robots%2527%2520motion%2520to%2520the%250Acontrol%2520principle%2520for%2520differential-driven%2520robot%2520class.%2520The%2520trajectory%250Aoptimization%2520problem%2520is%2520formulated%2520to%2520minimize%2520complexity%2520while%2520prioritizing%250Asafety%2520and%2520operational%2520efficiency.%2520We%2520then%2520build%2520a%2520full-stack%2520autonomous%250Aplanning%2520and%2520control%2520system%2520to%2520show%2520the%2520feasibility%2520and%2520robustness.%2520We%2520conduct%250Aextensive%2520simulations%2520and%2520real-world%2520testing%2520in%2520crowded%2520environments%2520with%2520three%250Akinds%2520of%2520differential-driven%2520robots%2520to%2520validate%2520the%2520effectiveness%2520of%2520our%250Aapproach.%2520We%2520will%2520release%2520our%2520method%2520as%2520an%2520open-source%2520package.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07924v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Universal%20Trajectory%20Optimization%20Framework%20for%20Differential-Driven%0A%20%20Robot%20Class&entry.906535625=Mengke%20Zhang%20and%20Zhichao%20Han%20and%20Chao%20Xu%20and%20Fei%20Gao%20and%20Yanjun%20Cao&entry.1292438233=%20%20Differential-driven%20robots%20are%20widely%20used%20in%20various%20scenarios%20thanks%20to%0Atheir%20straightforward%20principle%2C%20from%20household%20service%20robots%20to%20disaster%0Aresponse%20field%20robots.%20There%20are%20several%20different%20types%20of%20deriving%20mechanisms%0Aconsidering%20the%20real-world%20applications%2C%20including%20two-wheeled%2C%20four-wheeled%0Askid-steering%2C%20tracked%20robots%2C%20etc.%20The%20differences%20in%20the%20driving%20mechanism%0Ausually%20require%20specific%20kinematic%20modeling%20when%20precise%20controlling%20is%0Adesired.%20Furthermore%2C%20the%20nonholonomic%20dynamics%20and%20possible%20lateral%20slip%20lead%0Ato%20different%20degrees%20of%20difficulty%20in%20getting%20feasible%20and%20high-quality%0Atrajectories.%20Therefore%2C%20a%20comprehensive%20trajectory%20optimization%20framework%20to%0Acompute%20trajectories%20efficiently%20for%20various%20kinds%20of%20differential-driven%0Arobots%20is%20highly%20desirable.%20In%20this%20paper%2C%20we%20propose%20a%20universal%20trajectory%0Aoptimization%20framework%20that%20can%20be%20applied%20to%20differential-driven%20robot%20class%2C%0Aenabling%20the%20generation%20of%20high-quality%20trajectories%20within%20a%20restricted%0Acomputational%20timeframe.%20We%20introduce%20a%20novel%20trajectory%20representation%20based%0Aon%20polynomial%20parameterization%20of%20motion%20states%20or%20their%20integrals%2C%20such%20as%0Aangular%20and%20linear%20velocities%2C%20that%20inherently%20matching%20robots%27%20motion%20to%20the%0Acontrol%20principle%20for%20differential-driven%20robot%20class.%20The%20trajectory%0Aoptimization%20problem%20is%20formulated%20to%20minimize%20complexity%20while%20prioritizing%0Asafety%20and%20operational%20efficiency.%20We%20then%20build%20a%20full-stack%20autonomous%0Aplanning%20and%20control%20system%20to%20show%20the%20feasibility%20and%20robustness.%20We%20conduct%0Aextensive%20simulations%20and%20real-world%20testing%20in%20crowded%20environments%20with%20three%0Akinds%20of%20differential-driven%20robots%20to%20validate%20the%20effectiveness%20of%20our%0Aapproach.%20We%20will%20release%20our%20method%20as%20an%20open-source%20package.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07924v1&entry.124074799=Read"},
{"title": "Face Reconstruction Transfer Attack as Out-of-Distribution\n  Generalization", "author": "Yoon Gyo Jung and Jaewoo Park and Xingbo Dong and Hojin Park and Andrew Beng Jin Teoh and Octavia Camps", "abstract": "  Understanding the vulnerability of face recognition systems to malicious\nattacks is of critical importance. Previous works have focused on\nreconstructing face images that can penetrate a targeted verification system.\nEven in the white-box scenario, however, naively reconstructed images\nmisrepresent the identity information, hence the attacks are easily neutralized\nonce the face system is updated or changed. In this paper, we aim to\nreconstruct face images which are capable of transferring face attacks on\nunseen encoders. We term this problem as Face Reconstruction Transfer Attack\n(FRTA) and show that it can be formulated as an out-of-distribution (OOD)\ngeneralization problem. Inspired by its OOD nature, we propose to solve FRTA by\nAveraged Latent Search and Unsupervised Validation with pseudo target (ALSUV).\nTo strengthen the reconstruction attack on OOD unseen encoders, ALSUV\nreconstructs the face by searching the latent of amortized generator StyleGAN2\nthrough multiple latent optimization, latent optimization trajectory averaging,\nand unsupervised validation with a pseudo target. We demonstrate the efficacy\nand generalization of our method on widely used face datasets, accompanying it\nwith extensive ablation studies and visually, qualitatively, and quantitatively\nanalyses. The source code will be released.\n", "link": "http://arxiv.org/abs/2407.02403v2", "date": "2024-09-12", "relevancy": 2.0963, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5303}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5281}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5175}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Face%20Reconstruction%20Transfer%20Attack%20as%20Out-of-Distribution%0A%20%20Generalization&body=Title%3A%20Face%20Reconstruction%20Transfer%20Attack%20as%20Out-of-Distribution%0A%20%20Generalization%0AAuthor%3A%20Yoon%20Gyo%20Jung%20and%20Jaewoo%20Park%20and%20Xingbo%20Dong%20and%20Hojin%20Park%20and%20Andrew%20Beng%20Jin%20Teoh%20and%20Octavia%20Camps%0AAbstract%3A%20%20%20Understanding%20the%20vulnerability%20of%20face%20recognition%20systems%20to%20malicious%0Aattacks%20is%20of%20critical%20importance.%20Previous%20works%20have%20focused%20on%0Areconstructing%20face%20images%20that%20can%20penetrate%20a%20targeted%20verification%20system.%0AEven%20in%20the%20white-box%20scenario%2C%20however%2C%20naively%20reconstructed%20images%0Amisrepresent%20the%20identity%20information%2C%20hence%20the%20attacks%20are%20easily%20neutralized%0Aonce%20the%20face%20system%20is%20updated%20or%20changed.%20In%20this%20paper%2C%20we%20aim%20to%0Areconstruct%20face%20images%20which%20are%20capable%20of%20transferring%20face%20attacks%20on%0Aunseen%20encoders.%20We%20term%20this%20problem%20as%20Face%20Reconstruction%20Transfer%20Attack%0A%28FRTA%29%20and%20show%20that%20it%20can%20be%20formulated%20as%20an%20out-of-distribution%20%28OOD%29%0Ageneralization%20problem.%20Inspired%20by%20its%20OOD%20nature%2C%20we%20propose%20to%20solve%20FRTA%20by%0AAveraged%20Latent%20Search%20and%20Unsupervised%20Validation%20with%20pseudo%20target%20%28ALSUV%29.%0ATo%20strengthen%20the%20reconstruction%20attack%20on%20OOD%20unseen%20encoders%2C%20ALSUV%0Areconstructs%20the%20face%20by%20searching%20the%20latent%20of%20amortized%20generator%20StyleGAN2%0Athrough%20multiple%20latent%20optimization%2C%20latent%20optimization%20trajectory%20averaging%2C%0Aand%20unsupervised%20validation%20with%20a%20pseudo%20target.%20We%20demonstrate%20the%20efficacy%0Aand%20generalization%20of%20our%20method%20on%20widely%20used%20face%20datasets%2C%20accompanying%20it%0Awith%20extensive%20ablation%20studies%20and%20visually%2C%20qualitatively%2C%20and%20quantitatively%0Aanalyses.%20The%20source%20code%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02403v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFace%2520Reconstruction%2520Transfer%2520Attack%2520as%2520Out-of-Distribution%250A%2520%2520Generalization%26entry.906535625%3DYoon%2520Gyo%2520Jung%2520and%2520Jaewoo%2520Park%2520and%2520Xingbo%2520Dong%2520and%2520Hojin%2520Park%2520and%2520Andrew%2520Beng%2520Jin%2520Teoh%2520and%2520Octavia%2520Camps%26entry.1292438233%3D%2520%2520Understanding%2520the%2520vulnerability%2520of%2520face%2520recognition%2520systems%2520to%2520malicious%250Aattacks%2520is%2520of%2520critical%2520importance.%2520Previous%2520works%2520have%2520focused%2520on%250Areconstructing%2520face%2520images%2520that%2520can%2520penetrate%2520a%2520targeted%2520verification%2520system.%250AEven%2520in%2520the%2520white-box%2520scenario%252C%2520however%252C%2520naively%2520reconstructed%2520images%250Amisrepresent%2520the%2520identity%2520information%252C%2520hence%2520the%2520attacks%2520are%2520easily%2520neutralized%250Aonce%2520the%2520face%2520system%2520is%2520updated%2520or%2520changed.%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%250Areconstruct%2520face%2520images%2520which%2520are%2520capable%2520of%2520transferring%2520face%2520attacks%2520on%250Aunseen%2520encoders.%2520We%2520term%2520this%2520problem%2520as%2520Face%2520Reconstruction%2520Transfer%2520Attack%250A%2528FRTA%2529%2520and%2520show%2520that%2520it%2520can%2520be%2520formulated%2520as%2520an%2520out-of-distribution%2520%2528OOD%2529%250Ageneralization%2520problem.%2520Inspired%2520by%2520its%2520OOD%2520nature%252C%2520we%2520propose%2520to%2520solve%2520FRTA%2520by%250AAveraged%2520Latent%2520Search%2520and%2520Unsupervised%2520Validation%2520with%2520pseudo%2520target%2520%2528ALSUV%2529.%250ATo%2520strengthen%2520the%2520reconstruction%2520attack%2520on%2520OOD%2520unseen%2520encoders%252C%2520ALSUV%250Areconstructs%2520the%2520face%2520by%2520searching%2520the%2520latent%2520of%2520amortized%2520generator%2520StyleGAN2%250Athrough%2520multiple%2520latent%2520optimization%252C%2520latent%2520optimization%2520trajectory%2520averaging%252C%250Aand%2520unsupervised%2520validation%2520with%2520a%2520pseudo%2520target.%2520We%2520demonstrate%2520the%2520efficacy%250Aand%2520generalization%2520of%2520our%2520method%2520on%2520widely%2520used%2520face%2520datasets%252C%2520accompanying%2520it%250Awith%2520extensive%2520ablation%2520studies%2520and%2520visually%252C%2520qualitatively%252C%2520and%2520quantitatively%250Aanalyses.%2520The%2520source%2520code%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02403v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Face%20Reconstruction%20Transfer%20Attack%20as%20Out-of-Distribution%0A%20%20Generalization&entry.906535625=Yoon%20Gyo%20Jung%20and%20Jaewoo%20Park%20and%20Xingbo%20Dong%20and%20Hojin%20Park%20and%20Andrew%20Beng%20Jin%20Teoh%20and%20Octavia%20Camps&entry.1292438233=%20%20Understanding%20the%20vulnerability%20of%20face%20recognition%20systems%20to%20malicious%0Aattacks%20is%20of%20critical%20importance.%20Previous%20works%20have%20focused%20on%0Areconstructing%20face%20images%20that%20can%20penetrate%20a%20targeted%20verification%20system.%0AEven%20in%20the%20white-box%20scenario%2C%20however%2C%20naively%20reconstructed%20images%0Amisrepresent%20the%20identity%20information%2C%20hence%20the%20attacks%20are%20easily%20neutralized%0Aonce%20the%20face%20system%20is%20updated%20or%20changed.%20In%20this%20paper%2C%20we%20aim%20to%0Areconstruct%20face%20images%20which%20are%20capable%20of%20transferring%20face%20attacks%20on%0Aunseen%20encoders.%20We%20term%20this%20problem%20as%20Face%20Reconstruction%20Transfer%20Attack%0A%28FRTA%29%20and%20show%20that%20it%20can%20be%20formulated%20as%20an%20out-of-distribution%20%28OOD%29%0Ageneralization%20problem.%20Inspired%20by%20its%20OOD%20nature%2C%20we%20propose%20to%20solve%20FRTA%20by%0AAveraged%20Latent%20Search%20and%20Unsupervised%20Validation%20with%20pseudo%20target%20%28ALSUV%29.%0ATo%20strengthen%20the%20reconstruction%20attack%20on%20OOD%20unseen%20encoders%2C%20ALSUV%0Areconstructs%20the%20face%20by%20searching%20the%20latent%20of%20amortized%20generator%20StyleGAN2%0Athrough%20multiple%20latent%20optimization%2C%20latent%20optimization%20trajectory%20averaging%2C%0Aand%20unsupervised%20validation%20with%20a%20pseudo%20target.%20We%20demonstrate%20the%20efficacy%0Aand%20generalization%20of%20our%20method%20on%20widely%20used%20face%20datasets%2C%20accompanying%20it%0Awith%20extensive%20ablation%20studies%20and%20visually%2C%20qualitatively%2C%20and%20quantitatively%0Aanalyses.%20The%20source%20code%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02403v2&entry.124074799=Read"},
{"title": "Self-Supervised Learning of Iterative Solvers for Constrained\n  Optimization", "author": "Lukas L\u00fcken and Sergio Lucia", "abstract": "  Obtaining the solution of constrained optimization problems as a function of\nparameters is very important in a multitude of applications, such as control\nand planning. Solving such parametric optimization problems in real time can\npresent significant challenges, particularly when it is necessary to obtain\nhighly accurate solutions or batches of solutions. To solve these challenges,\nwe propose a learning-based iterative solver for constrained optimization which\ncan obtain very fast and accurate solutions by customizing the solver to a\nspecific parametric optimization problem. For a given set of parameters of the\nconstrained optimization problem, we propose a first step with a neural network\npredictor that outputs primal-dual solutions of a reasonable degree of\naccuracy. This primal-dual solution is then improved to a very high degree of\naccuracy in a second step by a learned iterative solver in the form of a neural\nnetwork. A novel loss function based on the Karush-Kuhn-Tucker conditions of\noptimality is introduced, enabling fully self-supervised training of both\nneural networks without the necessity of prior sampling of optimizer solutions.\nThe evaluation of a variety of quadratic and nonlinear parametric test problems\ndemonstrates that the predictor alone is already competitive with recent\nself-supervised schemes for approximating optimal solutions. The second step of\nour proposed learning-based iterative constrained optimizer achieves solutions\nwith orders of magnitude better accuracy than other learning-based approaches,\nwhile being faster to evaluate than state-of-the-art solvers and natively\nallowing for GPU parallelization.\n", "link": "http://arxiv.org/abs/2409.08066v1", "date": "2024-09-12", "relevancy": 2.0963, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5551}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5079}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4869}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Learning%20of%20Iterative%20Solvers%20for%20Constrained%0A%20%20Optimization&body=Title%3A%20Self-Supervised%20Learning%20of%20Iterative%20Solvers%20for%20Constrained%0A%20%20Optimization%0AAuthor%3A%20Lukas%20L%C3%BCken%20and%20Sergio%20Lucia%0AAbstract%3A%20%20%20Obtaining%20the%20solution%20of%20constrained%20optimization%20problems%20as%20a%20function%20of%0Aparameters%20is%20very%20important%20in%20a%20multitude%20of%20applications%2C%20such%20as%20control%0Aand%20planning.%20Solving%20such%20parametric%20optimization%20problems%20in%20real%20time%20can%0Apresent%20significant%20challenges%2C%20particularly%20when%20it%20is%20necessary%20to%20obtain%0Ahighly%20accurate%20solutions%20or%20batches%20of%20solutions.%20To%20solve%20these%20challenges%2C%0Awe%20propose%20a%20learning-based%20iterative%20solver%20for%20constrained%20optimization%20which%0Acan%20obtain%20very%20fast%20and%20accurate%20solutions%20by%20customizing%20the%20solver%20to%20a%0Aspecific%20parametric%20optimization%20problem.%20For%20a%20given%20set%20of%20parameters%20of%20the%0Aconstrained%20optimization%20problem%2C%20we%20propose%20a%20first%20step%20with%20a%20neural%20network%0Apredictor%20that%20outputs%20primal-dual%20solutions%20of%20a%20reasonable%20degree%20of%0Aaccuracy.%20This%20primal-dual%20solution%20is%20then%20improved%20to%20a%20very%20high%20degree%20of%0Aaccuracy%20in%20a%20second%20step%20by%20a%20learned%20iterative%20solver%20in%20the%20form%20of%20a%20neural%0Anetwork.%20A%20novel%20loss%20function%20based%20on%20the%20Karush-Kuhn-Tucker%20conditions%20of%0Aoptimality%20is%20introduced%2C%20enabling%20fully%20self-supervised%20training%20of%20both%0Aneural%20networks%20without%20the%20necessity%20of%20prior%20sampling%20of%20optimizer%20solutions.%0AThe%20evaluation%20of%20a%20variety%20of%20quadratic%20and%20nonlinear%20parametric%20test%20problems%0Ademonstrates%20that%20the%20predictor%20alone%20is%20already%20competitive%20with%20recent%0Aself-supervised%20schemes%20for%20approximating%20optimal%20solutions.%20The%20second%20step%20of%0Aour%20proposed%20learning-based%20iterative%20constrained%20optimizer%20achieves%20solutions%0Awith%20orders%20of%20magnitude%20better%20accuracy%20than%20other%20learning-based%20approaches%2C%0Awhile%20being%20faster%20to%20evaluate%20than%20state-of-the-art%20solvers%20and%20natively%0Aallowing%20for%20GPU%20parallelization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08066v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Learning%2520of%2520Iterative%2520Solvers%2520for%2520Constrained%250A%2520%2520Optimization%26entry.906535625%3DLukas%2520L%25C3%25BCken%2520and%2520Sergio%2520Lucia%26entry.1292438233%3D%2520%2520Obtaining%2520the%2520solution%2520of%2520constrained%2520optimization%2520problems%2520as%2520a%2520function%2520of%250Aparameters%2520is%2520very%2520important%2520in%2520a%2520multitude%2520of%2520applications%252C%2520such%2520as%2520control%250Aand%2520planning.%2520Solving%2520such%2520parametric%2520optimization%2520problems%2520in%2520real%2520time%2520can%250Apresent%2520significant%2520challenges%252C%2520particularly%2520when%2520it%2520is%2520necessary%2520to%2520obtain%250Ahighly%2520accurate%2520solutions%2520or%2520batches%2520of%2520solutions.%2520To%2520solve%2520these%2520challenges%252C%250Awe%2520propose%2520a%2520learning-based%2520iterative%2520solver%2520for%2520constrained%2520optimization%2520which%250Acan%2520obtain%2520very%2520fast%2520and%2520accurate%2520solutions%2520by%2520customizing%2520the%2520solver%2520to%2520a%250Aspecific%2520parametric%2520optimization%2520problem.%2520For%2520a%2520given%2520set%2520of%2520parameters%2520of%2520the%250Aconstrained%2520optimization%2520problem%252C%2520we%2520propose%2520a%2520first%2520step%2520with%2520a%2520neural%2520network%250Apredictor%2520that%2520outputs%2520primal-dual%2520solutions%2520of%2520a%2520reasonable%2520degree%2520of%250Aaccuracy.%2520This%2520primal-dual%2520solution%2520is%2520then%2520improved%2520to%2520a%2520very%2520high%2520degree%2520of%250Aaccuracy%2520in%2520a%2520second%2520step%2520by%2520a%2520learned%2520iterative%2520solver%2520in%2520the%2520form%2520of%2520a%2520neural%250Anetwork.%2520A%2520novel%2520loss%2520function%2520based%2520on%2520the%2520Karush-Kuhn-Tucker%2520conditions%2520of%250Aoptimality%2520is%2520introduced%252C%2520enabling%2520fully%2520self-supervised%2520training%2520of%2520both%250Aneural%2520networks%2520without%2520the%2520necessity%2520of%2520prior%2520sampling%2520of%2520optimizer%2520solutions.%250AThe%2520evaluation%2520of%2520a%2520variety%2520of%2520quadratic%2520and%2520nonlinear%2520parametric%2520test%2520problems%250Ademonstrates%2520that%2520the%2520predictor%2520alone%2520is%2520already%2520competitive%2520with%2520recent%250Aself-supervised%2520schemes%2520for%2520approximating%2520optimal%2520solutions.%2520The%2520second%2520step%2520of%250Aour%2520proposed%2520learning-based%2520iterative%2520constrained%2520optimizer%2520achieves%2520solutions%250Awith%2520orders%2520of%2520magnitude%2520better%2520accuracy%2520than%2520other%2520learning-based%2520approaches%252C%250Awhile%2520being%2520faster%2520to%2520evaluate%2520than%2520state-of-the-art%2520solvers%2520and%2520natively%250Aallowing%2520for%2520GPU%2520parallelization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08066v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Learning%20of%20Iterative%20Solvers%20for%20Constrained%0A%20%20Optimization&entry.906535625=Lukas%20L%C3%BCken%20and%20Sergio%20Lucia&entry.1292438233=%20%20Obtaining%20the%20solution%20of%20constrained%20optimization%20problems%20as%20a%20function%20of%0Aparameters%20is%20very%20important%20in%20a%20multitude%20of%20applications%2C%20such%20as%20control%0Aand%20planning.%20Solving%20such%20parametric%20optimization%20problems%20in%20real%20time%20can%0Apresent%20significant%20challenges%2C%20particularly%20when%20it%20is%20necessary%20to%20obtain%0Ahighly%20accurate%20solutions%20or%20batches%20of%20solutions.%20To%20solve%20these%20challenges%2C%0Awe%20propose%20a%20learning-based%20iterative%20solver%20for%20constrained%20optimization%20which%0Acan%20obtain%20very%20fast%20and%20accurate%20solutions%20by%20customizing%20the%20solver%20to%20a%0Aspecific%20parametric%20optimization%20problem.%20For%20a%20given%20set%20of%20parameters%20of%20the%0Aconstrained%20optimization%20problem%2C%20we%20propose%20a%20first%20step%20with%20a%20neural%20network%0Apredictor%20that%20outputs%20primal-dual%20solutions%20of%20a%20reasonable%20degree%20of%0Aaccuracy.%20This%20primal-dual%20solution%20is%20then%20improved%20to%20a%20very%20high%20degree%20of%0Aaccuracy%20in%20a%20second%20step%20by%20a%20learned%20iterative%20solver%20in%20the%20form%20of%20a%20neural%0Anetwork.%20A%20novel%20loss%20function%20based%20on%20the%20Karush-Kuhn-Tucker%20conditions%20of%0Aoptimality%20is%20introduced%2C%20enabling%20fully%20self-supervised%20training%20of%20both%0Aneural%20networks%20without%20the%20necessity%20of%20prior%20sampling%20of%20optimizer%20solutions.%0AThe%20evaluation%20of%20a%20variety%20of%20quadratic%20and%20nonlinear%20parametric%20test%20problems%0Ademonstrates%20that%20the%20predictor%20alone%20is%20already%20competitive%20with%20recent%0Aself-supervised%20schemes%20for%20approximating%20optimal%20solutions.%20The%20second%20step%20of%0Aour%20proposed%20learning-based%20iterative%20constrained%20optimizer%20achieves%20solutions%0Awith%20orders%20of%20magnitude%20better%20accuracy%20than%20other%20learning-based%20approaches%2C%0Awhile%20being%20faster%20to%20evaluate%20than%20state-of-the-art%20solvers%20and%20natively%0Aallowing%20for%20GPU%20parallelization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08066v1&entry.124074799=Read"},
{"title": "AutoPET Challenge: Tumour Synthesis for Data Augmentation", "author": "Lap Yan Lennon Chan and Chenxin Li and Yixuan Yuan", "abstract": "  Accurate lesion segmentation in whole-body PET/CT scans is crucial for cancer\ndiagnosis and treatment planning, but limited datasets often hinder the\nperformance of automated segmentation models. In this paper, we explore the\npotential of leveraging the deep prior from a generative model to serve as a\ndata augmenter for automated lesion segmentation in PET/CT scans. We adapt the\nDiffTumor method, originally designed for CT images, to generate synthetic\nPET-CT images with lesions. Our approach trains the generative model on the\nAutoPET dataset and uses it to expand the training data. We then compare the\nperformance of segmentation models trained on the original and augmented\ndatasets. Our findings show that the model trained on the augmented dataset\nachieves a higher Dice score, demonstrating the potential of our data\naugmentation approach. In a nutshell, this work presents a promising direction\nfor improving lesion segmentation in whole-body PET/CT scans with limited\ndatasets, potentially enhancing the accuracy and reliability of cancer\ndiagnostics.\n", "link": "http://arxiv.org/abs/2409.08068v1", "date": "2024-09-12", "relevancy": 2.0817, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5378}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5169}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5169}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoPET%20Challenge%3A%20Tumour%20Synthesis%20for%20Data%20Augmentation&body=Title%3A%20AutoPET%20Challenge%3A%20Tumour%20Synthesis%20for%20Data%20Augmentation%0AAuthor%3A%20Lap%20Yan%20Lennon%20Chan%20and%20Chenxin%20Li%20and%20Yixuan%20Yuan%0AAbstract%3A%20%20%20Accurate%20lesion%20segmentation%20in%20whole-body%20PET/CT%20scans%20is%20crucial%20for%20cancer%0Adiagnosis%20and%20treatment%20planning%2C%20but%20limited%20datasets%20often%20hinder%20the%0Aperformance%20of%20automated%20segmentation%20models.%20In%20this%20paper%2C%20we%20explore%20the%0Apotential%20of%20leveraging%20the%20deep%20prior%20from%20a%20generative%20model%20to%20serve%20as%20a%0Adata%20augmenter%20for%20automated%20lesion%20segmentation%20in%20PET/CT%20scans.%20We%20adapt%20the%0ADiffTumor%20method%2C%20originally%20designed%20for%20CT%20images%2C%20to%20generate%20synthetic%0APET-CT%20images%20with%20lesions.%20Our%20approach%20trains%20the%20generative%20model%20on%20the%0AAutoPET%20dataset%20and%20uses%20it%20to%20expand%20the%20training%20data.%20We%20then%20compare%20the%0Aperformance%20of%20segmentation%20models%20trained%20on%20the%20original%20and%20augmented%0Adatasets.%20Our%20findings%20show%20that%20the%20model%20trained%20on%20the%20augmented%20dataset%0Aachieves%20a%20higher%20Dice%20score%2C%20demonstrating%20the%20potential%20of%20our%20data%0Aaugmentation%20approach.%20In%20a%20nutshell%2C%20this%20work%20presents%20a%20promising%20direction%0Afor%20improving%20lesion%20segmentation%20in%20whole-body%20PET/CT%20scans%20with%20limited%0Adatasets%2C%20potentially%20enhancing%20the%20accuracy%20and%20reliability%20of%20cancer%0Adiagnostics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08068v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoPET%2520Challenge%253A%2520Tumour%2520Synthesis%2520for%2520Data%2520Augmentation%26entry.906535625%3DLap%2520Yan%2520Lennon%2520Chan%2520and%2520Chenxin%2520Li%2520and%2520Yixuan%2520Yuan%26entry.1292438233%3D%2520%2520Accurate%2520lesion%2520segmentation%2520in%2520whole-body%2520PET/CT%2520scans%2520is%2520crucial%2520for%2520cancer%250Adiagnosis%2520and%2520treatment%2520planning%252C%2520but%2520limited%2520datasets%2520often%2520hinder%2520the%250Aperformance%2520of%2520automated%2520segmentation%2520models.%2520In%2520this%2520paper%252C%2520we%2520explore%2520the%250Apotential%2520of%2520leveraging%2520the%2520deep%2520prior%2520from%2520a%2520generative%2520model%2520to%2520serve%2520as%2520a%250Adata%2520augmenter%2520for%2520automated%2520lesion%2520segmentation%2520in%2520PET/CT%2520scans.%2520We%2520adapt%2520the%250ADiffTumor%2520method%252C%2520originally%2520designed%2520for%2520CT%2520images%252C%2520to%2520generate%2520synthetic%250APET-CT%2520images%2520with%2520lesions.%2520Our%2520approach%2520trains%2520the%2520generative%2520model%2520on%2520the%250AAutoPET%2520dataset%2520and%2520uses%2520it%2520to%2520expand%2520the%2520training%2520data.%2520We%2520then%2520compare%2520the%250Aperformance%2520of%2520segmentation%2520models%2520trained%2520on%2520the%2520original%2520and%2520augmented%250Adatasets.%2520Our%2520findings%2520show%2520that%2520the%2520model%2520trained%2520on%2520the%2520augmented%2520dataset%250Aachieves%2520a%2520higher%2520Dice%2520score%252C%2520demonstrating%2520the%2520potential%2520of%2520our%2520data%250Aaugmentation%2520approach.%2520In%2520a%2520nutshell%252C%2520this%2520work%2520presents%2520a%2520promising%2520direction%250Afor%2520improving%2520lesion%2520segmentation%2520in%2520whole-body%2520PET/CT%2520scans%2520with%2520limited%250Adatasets%252C%2520potentially%2520enhancing%2520the%2520accuracy%2520and%2520reliability%2520of%2520cancer%250Adiagnostics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08068v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoPET%20Challenge%3A%20Tumour%20Synthesis%20for%20Data%20Augmentation&entry.906535625=Lap%20Yan%20Lennon%20Chan%20and%20Chenxin%20Li%20and%20Yixuan%20Yuan&entry.1292438233=%20%20Accurate%20lesion%20segmentation%20in%20whole-body%20PET/CT%20scans%20is%20crucial%20for%20cancer%0Adiagnosis%20and%20treatment%20planning%2C%20but%20limited%20datasets%20often%20hinder%20the%0Aperformance%20of%20automated%20segmentation%20models.%20In%20this%20paper%2C%20we%20explore%20the%0Apotential%20of%20leveraging%20the%20deep%20prior%20from%20a%20generative%20model%20to%20serve%20as%20a%0Adata%20augmenter%20for%20automated%20lesion%20segmentation%20in%20PET/CT%20scans.%20We%20adapt%20the%0ADiffTumor%20method%2C%20originally%20designed%20for%20CT%20images%2C%20to%20generate%20synthetic%0APET-CT%20images%20with%20lesions.%20Our%20approach%20trains%20the%20generative%20model%20on%20the%0AAutoPET%20dataset%20and%20uses%20it%20to%20expand%20the%20training%20data.%20We%20then%20compare%20the%0Aperformance%20of%20segmentation%20models%20trained%20on%20the%20original%20and%20augmented%0Adatasets.%20Our%20findings%20show%20that%20the%20model%20trained%20on%20the%20augmented%20dataset%0Aachieves%20a%20higher%20Dice%20score%2C%20demonstrating%20the%20potential%20of%20our%20data%0Aaugmentation%20approach.%20In%20a%20nutshell%2C%20this%20work%20presents%20a%20promising%20direction%0Afor%20improving%20lesion%20segmentation%20in%20whole-body%20PET/CT%20scans%20with%20limited%0Adatasets%2C%20potentially%20enhancing%20the%20accuracy%20and%20reliability%20of%20cancer%0Adiagnostics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08068v1&entry.124074799=Read"},
{"title": "MAPF-GPT: Imitation Learning for Multi-Agent Pathfinding at Scale", "author": "Anton Andreychuk and Konstantin Yakovlev and Aleksandr Panov and Alexey Skrynnik", "abstract": "  Multi-agent pathfinding (MAPF) is a challenging computational problem that\ntypically requires to find collision-free paths for multiple agents in a shared\nenvironment. Solving MAPF optimally is NP-hard, yet efficient solutions are\ncritical for numerous applications, including automated warehouses and\ntransportation systems. Recently, learning-based approaches to MAPF have gained\nattention, particularly those leveraging deep reinforcement learning. Following\ncurrent trends in machine learning, we have created a foundation model for the\nMAPF problems called MAPF-GPT. Using imitation learning, we have trained a\npolicy on a set of pre-collected sub-optimal expert trajectories that can\ngenerate actions in conditions of partial observability without additional\nheuristics, reward functions, or communication with other agents. The resulting\nMAPF-GPT model demonstrates zero-shot learning abilities when solving the MAPF\nproblem instances that were not present in the training dataset. We show that\nMAPF-GPT notably outperforms the current best-performing learnable-MAPF solvers\non a diverse range of problem instances and is efficient in terms of\ncomputation (in the inference mode).\n", "link": "http://arxiv.org/abs/2409.00134v2", "date": "2024-09-12", "relevancy": 2.0807, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5467}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5223}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAPF-GPT%3A%20Imitation%20Learning%20for%20Multi-Agent%20Pathfinding%20at%20Scale&body=Title%3A%20MAPF-GPT%3A%20Imitation%20Learning%20for%20Multi-Agent%20Pathfinding%20at%20Scale%0AAuthor%3A%20Anton%20Andreychuk%20and%20Konstantin%20Yakovlev%20and%20Aleksandr%20Panov%20and%20Alexey%20Skrynnik%0AAbstract%3A%20%20%20Multi-agent%20pathfinding%20%28MAPF%29%20is%20a%20challenging%20computational%20problem%20that%0Atypically%20requires%20to%20find%20collision-free%20paths%20for%20multiple%20agents%20in%20a%20shared%0Aenvironment.%20Solving%20MAPF%20optimally%20is%20NP-hard%2C%20yet%20efficient%20solutions%20are%0Acritical%20for%20numerous%20applications%2C%20including%20automated%20warehouses%20and%0Atransportation%20systems.%20Recently%2C%20learning-based%20approaches%20to%20MAPF%20have%20gained%0Aattention%2C%20particularly%20those%20leveraging%20deep%20reinforcement%20learning.%20Following%0Acurrent%20trends%20in%20machine%20learning%2C%20we%20have%20created%20a%20foundation%20model%20for%20the%0AMAPF%20problems%20called%20MAPF-GPT.%20Using%20imitation%20learning%2C%20we%20have%20trained%20a%0Apolicy%20on%20a%20set%20of%20pre-collected%20sub-optimal%20expert%20trajectories%20that%20can%0Agenerate%20actions%20in%20conditions%20of%20partial%20observability%20without%20additional%0Aheuristics%2C%20reward%20functions%2C%20or%20communication%20with%20other%20agents.%20The%20resulting%0AMAPF-GPT%20model%20demonstrates%20zero-shot%20learning%20abilities%20when%20solving%20the%20MAPF%0Aproblem%20instances%20that%20were%20not%20present%20in%20the%20training%20dataset.%20We%20show%20that%0AMAPF-GPT%20notably%20outperforms%20the%20current%20best-performing%20learnable-MAPF%20solvers%0Aon%20a%20diverse%20range%20of%20problem%20instances%20and%20is%20efficient%20in%20terms%20of%0Acomputation%20%28in%20the%20inference%20mode%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.00134v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAPF-GPT%253A%2520Imitation%2520Learning%2520for%2520Multi-Agent%2520Pathfinding%2520at%2520Scale%26entry.906535625%3DAnton%2520Andreychuk%2520and%2520Konstantin%2520Yakovlev%2520and%2520Aleksandr%2520Panov%2520and%2520Alexey%2520Skrynnik%26entry.1292438233%3D%2520%2520Multi-agent%2520pathfinding%2520%2528MAPF%2529%2520is%2520a%2520challenging%2520computational%2520problem%2520that%250Atypically%2520requires%2520to%2520find%2520collision-free%2520paths%2520for%2520multiple%2520agents%2520in%2520a%2520shared%250Aenvironment.%2520Solving%2520MAPF%2520optimally%2520is%2520NP-hard%252C%2520yet%2520efficient%2520solutions%2520are%250Acritical%2520for%2520numerous%2520applications%252C%2520including%2520automated%2520warehouses%2520and%250Atransportation%2520systems.%2520Recently%252C%2520learning-based%2520approaches%2520to%2520MAPF%2520have%2520gained%250Aattention%252C%2520particularly%2520those%2520leveraging%2520deep%2520reinforcement%2520learning.%2520Following%250Acurrent%2520trends%2520in%2520machine%2520learning%252C%2520we%2520have%2520created%2520a%2520foundation%2520model%2520for%2520the%250AMAPF%2520problems%2520called%2520MAPF-GPT.%2520Using%2520imitation%2520learning%252C%2520we%2520have%2520trained%2520a%250Apolicy%2520on%2520a%2520set%2520of%2520pre-collected%2520sub-optimal%2520expert%2520trajectories%2520that%2520can%250Agenerate%2520actions%2520in%2520conditions%2520of%2520partial%2520observability%2520without%2520additional%250Aheuristics%252C%2520reward%2520functions%252C%2520or%2520communication%2520with%2520other%2520agents.%2520The%2520resulting%250AMAPF-GPT%2520model%2520demonstrates%2520zero-shot%2520learning%2520abilities%2520when%2520solving%2520the%2520MAPF%250Aproblem%2520instances%2520that%2520were%2520not%2520present%2520in%2520the%2520training%2520dataset.%2520We%2520show%2520that%250AMAPF-GPT%2520notably%2520outperforms%2520the%2520current%2520best-performing%2520learnable-MAPF%2520solvers%250Aon%2520a%2520diverse%2520range%2520of%2520problem%2520instances%2520and%2520is%2520efficient%2520in%2520terms%2520of%250Acomputation%2520%2528in%2520the%2520inference%2520mode%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.00134v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAPF-GPT%3A%20Imitation%20Learning%20for%20Multi-Agent%20Pathfinding%20at%20Scale&entry.906535625=Anton%20Andreychuk%20and%20Konstantin%20Yakovlev%20and%20Aleksandr%20Panov%20and%20Alexey%20Skrynnik&entry.1292438233=%20%20Multi-agent%20pathfinding%20%28MAPF%29%20is%20a%20challenging%20computational%20problem%20that%0Atypically%20requires%20to%20find%20collision-free%20paths%20for%20multiple%20agents%20in%20a%20shared%0Aenvironment.%20Solving%20MAPF%20optimally%20is%20NP-hard%2C%20yet%20efficient%20solutions%20are%0Acritical%20for%20numerous%20applications%2C%20including%20automated%20warehouses%20and%0Atransportation%20systems.%20Recently%2C%20learning-based%20approaches%20to%20MAPF%20have%20gained%0Aattention%2C%20particularly%20those%20leveraging%20deep%20reinforcement%20learning.%20Following%0Acurrent%20trends%20in%20machine%20learning%2C%20we%20have%20created%20a%20foundation%20model%20for%20the%0AMAPF%20problems%20called%20MAPF-GPT.%20Using%20imitation%20learning%2C%20we%20have%20trained%20a%0Apolicy%20on%20a%20set%20of%20pre-collected%20sub-optimal%20expert%20trajectories%20that%20can%0Agenerate%20actions%20in%20conditions%20of%20partial%20observability%20without%20additional%0Aheuristics%2C%20reward%20functions%2C%20or%20communication%20with%20other%20agents.%20The%20resulting%0AMAPF-GPT%20model%20demonstrates%20zero-shot%20learning%20abilities%20when%20solving%20the%20MAPF%0Aproblem%20instances%20that%20were%20not%20present%20in%20the%20training%20dataset.%20We%20show%20that%0AMAPF-GPT%20notably%20outperforms%20the%20current%20best-performing%20learnable-MAPF%20solvers%0Aon%20a%20diverse%20range%20of%20problem%20instances%20and%20is%20efficient%20in%20terms%20of%0Acomputation%20%28in%20the%20inference%20mode%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.00134v2&entry.124074799=Read"},
{"title": "Towards a graph-based foundation model for network traffic analysis", "author": "Louis Van Langendonck and Ismael Castell-Uroz and Pere Barlet-Ros", "abstract": "  Foundation models have shown great promise in various fields of study. A\npotential application of such models is in computer network traffic analysis,\nwhere these models can grasp the complexities of network traffic dynamics and\nadapt to any specific task or network environment with minimal fine-tuning.\nPrevious approaches have used tokenized hex-level packet data and the model\narchitecture of large language transformer models. We propose a new, efficient\ngraph-based alternative at the flow-level. Our approach represents network\ntraffic as a dynamic spatio-temporal graph, employing a self-supervised link\nprediction pretraining task to capture the spatial and temporal dynamics in\nthis network graph framework. To evaluate the effectiveness of our approach, we\nconduct a few-shot learning experiment for three distinct downstream network\ntasks: intrusion detection, traffic classification, and botnet classification.\nModels finetuned from our pretrained base achieve an average performance\nincrease of 6.87\\% over training from scratch, demonstrating their ability to\neffectively learn general network traffic dynamics during pretraining. This\nsuccess suggests the potential for a large-scale version to serve as an\noperational foundational model.\n", "link": "http://arxiv.org/abs/2409.08111v1", "date": "2024-09-12", "relevancy": 2.0795, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5712}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5101}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20a%20graph-based%20foundation%20model%20for%20network%20traffic%20analysis&body=Title%3A%20Towards%20a%20graph-based%20foundation%20model%20for%20network%20traffic%20analysis%0AAuthor%3A%20Louis%20Van%20Langendonck%20and%20Ismael%20Castell-Uroz%20and%20Pere%20Barlet-Ros%0AAbstract%3A%20%20%20Foundation%20models%20have%20shown%20great%20promise%20in%20various%20fields%20of%20study.%20A%0Apotential%20application%20of%20such%20models%20is%20in%20computer%20network%20traffic%20analysis%2C%0Awhere%20these%20models%20can%20grasp%20the%20complexities%20of%20network%20traffic%20dynamics%20and%0Aadapt%20to%20any%20specific%20task%20or%20network%20environment%20with%20minimal%20fine-tuning.%0APrevious%20approaches%20have%20used%20tokenized%20hex-level%20packet%20data%20and%20the%20model%0Aarchitecture%20of%20large%20language%20transformer%20models.%20We%20propose%20a%20new%2C%20efficient%0Agraph-based%20alternative%20at%20the%20flow-level.%20Our%20approach%20represents%20network%0Atraffic%20as%20a%20dynamic%20spatio-temporal%20graph%2C%20employing%20a%20self-supervised%20link%0Aprediction%20pretraining%20task%20to%20capture%20the%20spatial%20and%20temporal%20dynamics%20in%0Athis%20network%20graph%20framework.%20To%20evaluate%20the%20effectiveness%20of%20our%20approach%2C%20we%0Aconduct%20a%20few-shot%20learning%20experiment%20for%20three%20distinct%20downstream%20network%0Atasks%3A%20intrusion%20detection%2C%20traffic%20classification%2C%20and%20botnet%20classification.%0AModels%20finetuned%20from%20our%20pretrained%20base%20achieve%20an%20average%20performance%0Aincrease%20of%206.87%5C%25%20over%20training%20from%20scratch%2C%20demonstrating%20their%20ability%20to%0Aeffectively%20learn%20general%20network%20traffic%20dynamics%20during%20pretraining.%20This%0Asuccess%20suggests%20the%20potential%20for%20a%20large-scale%20version%20to%20serve%20as%20an%0Aoperational%20foundational%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08111v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520a%2520graph-based%2520foundation%2520model%2520for%2520network%2520traffic%2520analysis%26entry.906535625%3DLouis%2520Van%2520Langendonck%2520and%2520Ismael%2520Castell-Uroz%2520and%2520Pere%2520Barlet-Ros%26entry.1292438233%3D%2520%2520Foundation%2520models%2520have%2520shown%2520great%2520promise%2520in%2520various%2520fields%2520of%2520study.%2520A%250Apotential%2520application%2520of%2520such%2520models%2520is%2520in%2520computer%2520network%2520traffic%2520analysis%252C%250Awhere%2520these%2520models%2520can%2520grasp%2520the%2520complexities%2520of%2520network%2520traffic%2520dynamics%2520and%250Aadapt%2520to%2520any%2520specific%2520task%2520or%2520network%2520environment%2520with%2520minimal%2520fine-tuning.%250APrevious%2520approaches%2520have%2520used%2520tokenized%2520hex-level%2520packet%2520data%2520and%2520the%2520model%250Aarchitecture%2520of%2520large%2520language%2520transformer%2520models.%2520We%2520propose%2520a%2520new%252C%2520efficient%250Agraph-based%2520alternative%2520at%2520the%2520flow-level.%2520Our%2520approach%2520represents%2520network%250Atraffic%2520as%2520a%2520dynamic%2520spatio-temporal%2520graph%252C%2520employing%2520a%2520self-supervised%2520link%250Aprediction%2520pretraining%2520task%2520to%2520capture%2520the%2520spatial%2520and%2520temporal%2520dynamics%2520in%250Athis%2520network%2520graph%2520framework.%2520To%2520evaluate%2520the%2520effectiveness%2520of%2520our%2520approach%252C%2520we%250Aconduct%2520a%2520few-shot%2520learning%2520experiment%2520for%2520three%2520distinct%2520downstream%2520network%250Atasks%253A%2520intrusion%2520detection%252C%2520traffic%2520classification%252C%2520and%2520botnet%2520classification.%250AModels%2520finetuned%2520from%2520our%2520pretrained%2520base%2520achieve%2520an%2520average%2520performance%250Aincrease%2520of%25206.87%255C%2525%2520over%2520training%2520from%2520scratch%252C%2520demonstrating%2520their%2520ability%2520to%250Aeffectively%2520learn%2520general%2520network%2520traffic%2520dynamics%2520during%2520pretraining.%2520This%250Asuccess%2520suggests%2520the%2520potential%2520for%2520a%2520large-scale%2520version%2520to%2520serve%2520as%2520an%250Aoperational%2520foundational%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08111v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20a%20graph-based%20foundation%20model%20for%20network%20traffic%20analysis&entry.906535625=Louis%20Van%20Langendonck%20and%20Ismael%20Castell-Uroz%20and%20Pere%20Barlet-Ros&entry.1292438233=%20%20Foundation%20models%20have%20shown%20great%20promise%20in%20various%20fields%20of%20study.%20A%0Apotential%20application%20of%20such%20models%20is%20in%20computer%20network%20traffic%20analysis%2C%0Awhere%20these%20models%20can%20grasp%20the%20complexities%20of%20network%20traffic%20dynamics%20and%0Aadapt%20to%20any%20specific%20task%20or%20network%20environment%20with%20minimal%20fine-tuning.%0APrevious%20approaches%20have%20used%20tokenized%20hex-level%20packet%20data%20and%20the%20model%0Aarchitecture%20of%20large%20language%20transformer%20models.%20We%20propose%20a%20new%2C%20efficient%0Agraph-based%20alternative%20at%20the%20flow-level.%20Our%20approach%20represents%20network%0Atraffic%20as%20a%20dynamic%20spatio-temporal%20graph%2C%20employing%20a%20self-supervised%20link%0Aprediction%20pretraining%20task%20to%20capture%20the%20spatial%20and%20temporal%20dynamics%20in%0Athis%20network%20graph%20framework.%20To%20evaluate%20the%20effectiveness%20of%20our%20approach%2C%20we%0Aconduct%20a%20few-shot%20learning%20experiment%20for%20three%20distinct%20downstream%20network%0Atasks%3A%20intrusion%20detection%2C%20traffic%20classification%2C%20and%20botnet%20classification.%0AModels%20finetuned%20from%20our%20pretrained%20base%20achieve%20an%20average%20performance%0Aincrease%20of%206.87%5C%25%20over%20training%20from%20scratch%2C%20demonstrating%20their%20ability%20to%0Aeffectively%20learn%20general%20network%20traffic%20dynamics%20during%20pretraining.%20This%0Asuccess%20suggests%20the%20potential%20for%20a%20large-scale%20version%20to%20serve%20as%20an%0Aoperational%20foundational%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08111v1&entry.124074799=Read"},
{"title": "Tailoring Solution Accuracy for Fast Whole-body Model Predictive Control\n  of Legged Robots", "author": "Charles Khazoom and Seungwoo Hong and Matthew Chignoli and Elijah Stanger-Jones and Sangbae Kim", "abstract": "  Thanks to recent advancements in accelerating non-linear model predictive\ncontrol (NMPC), it is now feasible to deploy whole-body NMPC at real-time rates\nfor humanoid robots. However, enforcing inequality constraints in real time for\nsuch high-dimensional systems remains challenging due to the need for\nadditional iterations. This paper presents an implementation of whole-body NMPC\nfor legged robots that provides low-accuracy solutions to NMPC with general\nequality and inequality constraints. Instead of aiming for highly accurate\noptimal solutions, we leverage the alternating direction method of multipliers\nto rapidly provide low-accuracy solutions to quadratic programming subproblems.\nOur extensive simulation results indicate that real robots often cannot benefit\nfrom highly accurate solutions due to dynamics discretization errors, inertial\nmodeling errors and delays. We incorporate control barrier functions (CBFs) at\nthe initial timestep of the NMPC for the self-collision constraints, resulting\nin up to a 26-fold reduction in the number of self-collisions without adding\ncomputational burden. The controller is reliably deployed on hardware at 90 Hz\nfor a problem involving 32 timesteps, 2004 variables, and 3768 constraints. The\nNMPC delivers sufficiently accurate solutions, enabling the MIT Humanoid to\nplan complex crossed-leg and arm motions that enhance stability when walking\nand recovering from significant disturbances.\n", "link": "http://arxiv.org/abs/2407.10789v2", "date": "2024-09-12", "relevancy": 2.0787, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5806}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5303}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4847}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tailoring%20Solution%20Accuracy%20for%20Fast%20Whole-body%20Model%20Predictive%20Control%0A%20%20of%20Legged%20Robots&body=Title%3A%20Tailoring%20Solution%20Accuracy%20for%20Fast%20Whole-body%20Model%20Predictive%20Control%0A%20%20of%20Legged%20Robots%0AAuthor%3A%20Charles%20Khazoom%20and%20Seungwoo%20Hong%20and%20Matthew%20Chignoli%20and%20Elijah%20Stanger-Jones%20and%20Sangbae%20Kim%0AAbstract%3A%20%20%20Thanks%20to%20recent%20advancements%20in%20accelerating%20non-linear%20model%20predictive%0Acontrol%20%28NMPC%29%2C%20it%20is%20now%20feasible%20to%20deploy%20whole-body%20NMPC%20at%20real-time%20rates%0Afor%20humanoid%20robots.%20However%2C%20enforcing%20inequality%20constraints%20in%20real%20time%20for%0Asuch%20high-dimensional%20systems%20remains%20challenging%20due%20to%20the%20need%20for%0Aadditional%20iterations.%20This%20paper%20presents%20an%20implementation%20of%20whole-body%20NMPC%0Afor%20legged%20robots%20that%20provides%20low-accuracy%20solutions%20to%20NMPC%20with%20general%0Aequality%20and%20inequality%20constraints.%20Instead%20of%20aiming%20for%20highly%20accurate%0Aoptimal%20solutions%2C%20we%20leverage%20the%20alternating%20direction%20method%20of%20multipliers%0Ato%20rapidly%20provide%20low-accuracy%20solutions%20to%20quadratic%20programming%20subproblems.%0AOur%20extensive%20simulation%20results%20indicate%20that%20real%20robots%20often%20cannot%20benefit%0Afrom%20highly%20accurate%20solutions%20due%20to%20dynamics%20discretization%20errors%2C%20inertial%0Amodeling%20errors%20and%20delays.%20We%20incorporate%20control%20barrier%20functions%20%28CBFs%29%20at%0Athe%20initial%20timestep%20of%20the%20NMPC%20for%20the%20self-collision%20constraints%2C%20resulting%0Ain%20up%20to%20a%2026-fold%20reduction%20in%20the%20number%20of%20self-collisions%20without%20adding%0Acomputational%20burden.%20The%20controller%20is%20reliably%20deployed%20on%20hardware%20at%2090%20Hz%0Afor%20a%20problem%20involving%2032%20timesteps%2C%202004%20variables%2C%20and%203768%20constraints.%20The%0ANMPC%20delivers%20sufficiently%20accurate%20solutions%2C%20enabling%20the%20MIT%20Humanoid%20to%0Aplan%20complex%20crossed-leg%20and%20arm%20motions%20that%20enhance%20stability%20when%20walking%0Aand%20recovering%20from%20significant%20disturbances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10789v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTailoring%2520Solution%2520Accuracy%2520for%2520Fast%2520Whole-body%2520Model%2520Predictive%2520Control%250A%2520%2520of%2520Legged%2520Robots%26entry.906535625%3DCharles%2520Khazoom%2520and%2520Seungwoo%2520Hong%2520and%2520Matthew%2520Chignoli%2520and%2520Elijah%2520Stanger-Jones%2520and%2520Sangbae%2520Kim%26entry.1292438233%3D%2520%2520Thanks%2520to%2520recent%2520advancements%2520in%2520accelerating%2520non-linear%2520model%2520predictive%250Acontrol%2520%2528NMPC%2529%252C%2520it%2520is%2520now%2520feasible%2520to%2520deploy%2520whole-body%2520NMPC%2520at%2520real-time%2520rates%250Afor%2520humanoid%2520robots.%2520However%252C%2520enforcing%2520inequality%2520constraints%2520in%2520real%2520time%2520for%250Asuch%2520high-dimensional%2520systems%2520remains%2520challenging%2520due%2520to%2520the%2520need%2520for%250Aadditional%2520iterations.%2520This%2520paper%2520presents%2520an%2520implementation%2520of%2520whole-body%2520NMPC%250Afor%2520legged%2520robots%2520that%2520provides%2520low-accuracy%2520solutions%2520to%2520NMPC%2520with%2520general%250Aequality%2520and%2520inequality%2520constraints.%2520Instead%2520of%2520aiming%2520for%2520highly%2520accurate%250Aoptimal%2520solutions%252C%2520we%2520leverage%2520the%2520alternating%2520direction%2520method%2520of%2520multipliers%250Ato%2520rapidly%2520provide%2520low-accuracy%2520solutions%2520to%2520quadratic%2520programming%2520subproblems.%250AOur%2520extensive%2520simulation%2520results%2520indicate%2520that%2520real%2520robots%2520often%2520cannot%2520benefit%250Afrom%2520highly%2520accurate%2520solutions%2520due%2520to%2520dynamics%2520discretization%2520errors%252C%2520inertial%250Amodeling%2520errors%2520and%2520delays.%2520We%2520incorporate%2520control%2520barrier%2520functions%2520%2528CBFs%2529%2520at%250Athe%2520initial%2520timestep%2520of%2520the%2520NMPC%2520for%2520the%2520self-collision%2520constraints%252C%2520resulting%250Ain%2520up%2520to%2520a%252026-fold%2520reduction%2520in%2520the%2520number%2520of%2520self-collisions%2520without%2520adding%250Acomputational%2520burden.%2520The%2520controller%2520is%2520reliably%2520deployed%2520on%2520hardware%2520at%252090%2520Hz%250Afor%2520a%2520problem%2520involving%252032%2520timesteps%252C%25202004%2520variables%252C%2520and%25203768%2520constraints.%2520The%250ANMPC%2520delivers%2520sufficiently%2520accurate%2520solutions%252C%2520enabling%2520the%2520MIT%2520Humanoid%2520to%250Aplan%2520complex%2520crossed-leg%2520and%2520arm%2520motions%2520that%2520enhance%2520stability%2520when%2520walking%250Aand%2520recovering%2520from%2520significant%2520disturbances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10789v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tailoring%20Solution%20Accuracy%20for%20Fast%20Whole-body%20Model%20Predictive%20Control%0A%20%20of%20Legged%20Robots&entry.906535625=Charles%20Khazoom%20and%20Seungwoo%20Hong%20and%20Matthew%20Chignoli%20and%20Elijah%20Stanger-Jones%20and%20Sangbae%20Kim&entry.1292438233=%20%20Thanks%20to%20recent%20advancements%20in%20accelerating%20non-linear%20model%20predictive%0Acontrol%20%28NMPC%29%2C%20it%20is%20now%20feasible%20to%20deploy%20whole-body%20NMPC%20at%20real-time%20rates%0Afor%20humanoid%20robots.%20However%2C%20enforcing%20inequality%20constraints%20in%20real%20time%20for%0Asuch%20high-dimensional%20systems%20remains%20challenging%20due%20to%20the%20need%20for%0Aadditional%20iterations.%20This%20paper%20presents%20an%20implementation%20of%20whole-body%20NMPC%0Afor%20legged%20robots%20that%20provides%20low-accuracy%20solutions%20to%20NMPC%20with%20general%0Aequality%20and%20inequality%20constraints.%20Instead%20of%20aiming%20for%20highly%20accurate%0Aoptimal%20solutions%2C%20we%20leverage%20the%20alternating%20direction%20method%20of%20multipliers%0Ato%20rapidly%20provide%20low-accuracy%20solutions%20to%20quadratic%20programming%20subproblems.%0AOur%20extensive%20simulation%20results%20indicate%20that%20real%20robots%20often%20cannot%20benefit%0Afrom%20highly%20accurate%20solutions%20due%20to%20dynamics%20discretization%20errors%2C%20inertial%0Amodeling%20errors%20and%20delays.%20We%20incorporate%20control%20barrier%20functions%20%28CBFs%29%20at%0Athe%20initial%20timestep%20of%20the%20NMPC%20for%20the%20self-collision%20constraints%2C%20resulting%0Ain%20up%20to%20a%2026-fold%20reduction%20in%20the%20number%20of%20self-collisions%20without%20adding%0Acomputational%20burden.%20The%20controller%20is%20reliably%20deployed%20on%20hardware%20at%2090%20Hz%0Afor%20a%20problem%20involving%2032%20timesteps%2C%202004%20variables%2C%20and%203768%20constraints.%20The%0ANMPC%20delivers%20sufficiently%20accurate%20solutions%2C%20enabling%20the%20MIT%20Humanoid%20to%0Aplan%20complex%20crossed-leg%20and%20arm%20motions%20that%20enhance%20stability%20when%20walking%0Aand%20recovering%20from%20significant%20disturbances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10789v2&entry.124074799=Read"},
{"title": "Online state vector reduction during model predictive control with\n  gradient-based trajectory optimisation", "author": "David Russell and Rafael Papallas and Mehmet Dogar", "abstract": "  Non-prehensile manipulation in high-dimensional systems is challenging for a\nvariety of reasons. One of the main reasons is the computationally long\nplanning times that come with a large state space. Trajectory optimisation\nalgorithms have proved their utility in a wide variety of tasks, but, like most\nmethods struggle scaling to the high dimensional systems ubiquitous to\nnon-prehensile manipulation in clutter as well as deformable object\nmanipulation. We reason that, during manipulation, different degrees of freedom\nwill become more or less important to the task over time as the system evolves.\nWe leverage this idea to reduce the number of degrees of freedom considered in\na trajectory optimisation problem, to reduce planning times. This idea is\nparticularly relevant in the context of model predictive control (MPC) where\nthe cost landscape of the optimisation problem is constantly evolving. We\nprovide simulation results under asynchronous MPC and show our methods are\ncapable of achieving better overall performance due to the decreased policy lag\nwhilst still being able to optimise trajectories effectively.\n", "link": "http://arxiv.org/abs/2408.11665v2", "date": "2024-09-12", "relevancy": 2.0647, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5596}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5268}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4882}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20state%20vector%20reduction%20during%20model%20predictive%20control%20with%0A%20%20gradient-based%20trajectory%20optimisation&body=Title%3A%20Online%20state%20vector%20reduction%20during%20model%20predictive%20control%20with%0A%20%20gradient-based%20trajectory%20optimisation%0AAuthor%3A%20David%20Russell%20and%20Rafael%20Papallas%20and%20Mehmet%20Dogar%0AAbstract%3A%20%20%20Non-prehensile%20manipulation%20in%20high-dimensional%20systems%20is%20challenging%20for%20a%0Avariety%20of%20reasons.%20One%20of%20the%20main%20reasons%20is%20the%20computationally%20long%0Aplanning%20times%20that%20come%20with%20a%20large%20state%20space.%20Trajectory%20optimisation%0Aalgorithms%20have%20proved%20their%20utility%20in%20a%20wide%20variety%20of%20tasks%2C%20but%2C%20like%20most%0Amethods%20struggle%20scaling%20to%20the%20high%20dimensional%20systems%20ubiquitous%20to%0Anon-prehensile%20manipulation%20in%20clutter%20as%20well%20as%20deformable%20object%0Amanipulation.%20We%20reason%20that%2C%20during%20manipulation%2C%20different%20degrees%20of%20freedom%0Awill%20become%20more%20or%20less%20important%20to%20the%20task%20over%20time%20as%20the%20system%20evolves.%0AWe%20leverage%20this%20idea%20to%20reduce%20the%20number%20of%20degrees%20of%20freedom%20considered%20in%0Aa%20trajectory%20optimisation%20problem%2C%20to%20reduce%20planning%20times.%20This%20idea%20is%0Aparticularly%20relevant%20in%20the%20context%20of%20model%20predictive%20control%20%28MPC%29%20where%0Athe%20cost%20landscape%20of%20the%20optimisation%20problem%20is%20constantly%20evolving.%20We%0Aprovide%20simulation%20results%20under%20asynchronous%20MPC%20and%20show%20our%20methods%20are%0Acapable%20of%20achieving%20better%20overall%20performance%20due%20to%20the%20decreased%20policy%20lag%0Awhilst%20still%20being%20able%20to%20optimise%20trajectories%20effectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11665v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520state%2520vector%2520reduction%2520during%2520model%2520predictive%2520control%2520with%250A%2520%2520gradient-based%2520trajectory%2520optimisation%26entry.906535625%3DDavid%2520Russell%2520and%2520Rafael%2520Papallas%2520and%2520Mehmet%2520Dogar%26entry.1292438233%3D%2520%2520Non-prehensile%2520manipulation%2520in%2520high-dimensional%2520systems%2520is%2520challenging%2520for%2520a%250Avariety%2520of%2520reasons.%2520One%2520of%2520the%2520main%2520reasons%2520is%2520the%2520computationally%2520long%250Aplanning%2520times%2520that%2520come%2520with%2520a%2520large%2520state%2520space.%2520Trajectory%2520optimisation%250Aalgorithms%2520have%2520proved%2520their%2520utility%2520in%2520a%2520wide%2520variety%2520of%2520tasks%252C%2520but%252C%2520like%2520most%250Amethods%2520struggle%2520scaling%2520to%2520the%2520high%2520dimensional%2520systems%2520ubiquitous%2520to%250Anon-prehensile%2520manipulation%2520in%2520clutter%2520as%2520well%2520as%2520deformable%2520object%250Amanipulation.%2520We%2520reason%2520that%252C%2520during%2520manipulation%252C%2520different%2520degrees%2520of%2520freedom%250Awill%2520become%2520more%2520or%2520less%2520important%2520to%2520the%2520task%2520over%2520time%2520as%2520the%2520system%2520evolves.%250AWe%2520leverage%2520this%2520idea%2520to%2520reduce%2520the%2520number%2520of%2520degrees%2520of%2520freedom%2520considered%2520in%250Aa%2520trajectory%2520optimisation%2520problem%252C%2520to%2520reduce%2520planning%2520times.%2520This%2520idea%2520is%250Aparticularly%2520relevant%2520in%2520the%2520context%2520of%2520model%2520predictive%2520control%2520%2528MPC%2529%2520where%250Athe%2520cost%2520landscape%2520of%2520the%2520optimisation%2520problem%2520is%2520constantly%2520evolving.%2520We%250Aprovide%2520simulation%2520results%2520under%2520asynchronous%2520MPC%2520and%2520show%2520our%2520methods%2520are%250Acapable%2520of%2520achieving%2520better%2520overall%2520performance%2520due%2520to%2520the%2520decreased%2520policy%2520lag%250Awhilst%2520still%2520being%2520able%2520to%2520optimise%2520trajectories%2520effectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11665v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20state%20vector%20reduction%20during%20model%20predictive%20control%20with%0A%20%20gradient-based%20trajectory%20optimisation&entry.906535625=David%20Russell%20and%20Rafael%20Papallas%20and%20Mehmet%20Dogar&entry.1292438233=%20%20Non-prehensile%20manipulation%20in%20high-dimensional%20systems%20is%20challenging%20for%20a%0Avariety%20of%20reasons.%20One%20of%20the%20main%20reasons%20is%20the%20computationally%20long%0Aplanning%20times%20that%20come%20with%20a%20large%20state%20space.%20Trajectory%20optimisation%0Aalgorithms%20have%20proved%20their%20utility%20in%20a%20wide%20variety%20of%20tasks%2C%20but%2C%20like%20most%0Amethods%20struggle%20scaling%20to%20the%20high%20dimensional%20systems%20ubiquitous%20to%0Anon-prehensile%20manipulation%20in%20clutter%20as%20well%20as%20deformable%20object%0Amanipulation.%20We%20reason%20that%2C%20during%20manipulation%2C%20different%20degrees%20of%20freedom%0Awill%20become%20more%20or%20less%20important%20to%20the%20task%20over%20time%20as%20the%20system%20evolves.%0AWe%20leverage%20this%20idea%20to%20reduce%20the%20number%20of%20degrees%20of%20freedom%20considered%20in%0Aa%20trajectory%20optimisation%20problem%2C%20to%20reduce%20planning%20times.%20This%20idea%20is%0Aparticularly%20relevant%20in%20the%20context%20of%20model%20predictive%20control%20%28MPC%29%20where%0Athe%20cost%20landscape%20of%20the%20optimisation%20problem%20is%20constantly%20evolving.%20We%0Aprovide%20simulation%20results%20under%20asynchronous%20MPC%20and%20show%20our%20methods%20are%0Acapable%20of%20achieving%20better%20overall%20performance%20due%20to%20the%20decreased%20policy%20lag%0Awhilst%20still%20being%20able%20to%20optimise%20trajectories%20effectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11665v2&entry.124074799=Read"},
{"title": "Contrastive Learning and the Emergence of Attributes Associations", "author": "Daniel N. Nissani", "abstract": "  In response to an object presentation, supervised learning schemes generally\nrespond with a parsimonious label. Upon a similar presentation we humans\nrespond again with a label, but are flooded, in addition, by a myriad of\nassociations. A significant portion of these consist of the presented object\nattributes. Contrastive learning is a semi-supervised learning scheme based on\nthe application of identity preserving transformations on the object input\nrepresentations. It is conjectured in this work that these same applied\ntransformations preserve, in addition to the identity of the presented object,\nalso the identity of its semantically meaningful attributes. The corollary of\nthis is that the output representations of such a contrastive learning scheme\ncontain valuable information not only for the classification of the presented\nobject, but also for the presence or absence decision of any attribute of\ninterest. Simulation results which demonstrate this idea and the feasibility of\nthis conjecture are presented.\n", "link": "http://arxiv.org/abs/2302.10763v4", "date": "2024-09-12", "relevancy": 2.0633, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5242}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5177}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrastive%20Learning%20and%20the%20Emergence%20of%20Attributes%20Associations&body=Title%3A%20Contrastive%20Learning%20and%20the%20Emergence%20of%20Attributes%20Associations%0AAuthor%3A%20Daniel%20N.%20Nissani%0AAbstract%3A%20%20%20In%20response%20to%20an%20object%20presentation%2C%20supervised%20learning%20schemes%20generally%0Arespond%20with%20a%20parsimonious%20label.%20Upon%20a%20similar%20presentation%20we%20humans%0Arespond%20again%20with%20a%20label%2C%20but%20are%20flooded%2C%20in%20addition%2C%20by%20a%20myriad%20of%0Aassociations.%20A%20significant%20portion%20of%20these%20consist%20of%20the%20presented%20object%0Aattributes.%20Contrastive%20learning%20is%20a%20semi-supervised%20learning%20scheme%20based%20on%0Athe%20application%20of%20identity%20preserving%20transformations%20on%20the%20object%20input%0Arepresentations.%20It%20is%20conjectured%20in%20this%20work%20that%20these%20same%20applied%0Atransformations%20preserve%2C%20in%20addition%20to%20the%20identity%20of%20the%20presented%20object%2C%0Aalso%20the%20identity%20of%20its%20semantically%20meaningful%20attributes.%20The%20corollary%20of%0Athis%20is%20that%20the%20output%20representations%20of%20such%20a%20contrastive%20learning%20scheme%0Acontain%20valuable%20information%20not%20only%20for%20the%20classification%20of%20the%20presented%0Aobject%2C%20but%20also%20for%20the%20presence%20or%20absence%20decision%20of%20any%20attribute%20of%0Ainterest.%20Simulation%20results%20which%20demonstrate%20this%20idea%20and%20the%20feasibility%20of%0Athis%20conjecture%20are%20presented.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.10763v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrastive%2520Learning%2520and%2520the%2520Emergence%2520of%2520Attributes%2520Associations%26entry.906535625%3DDaniel%2520N.%2520Nissani%26entry.1292438233%3D%2520%2520In%2520response%2520to%2520an%2520object%2520presentation%252C%2520supervised%2520learning%2520schemes%2520generally%250Arespond%2520with%2520a%2520parsimonious%2520label.%2520Upon%2520a%2520similar%2520presentation%2520we%2520humans%250Arespond%2520again%2520with%2520a%2520label%252C%2520but%2520are%2520flooded%252C%2520in%2520addition%252C%2520by%2520a%2520myriad%2520of%250Aassociations.%2520A%2520significant%2520portion%2520of%2520these%2520consist%2520of%2520the%2520presented%2520object%250Aattributes.%2520Contrastive%2520learning%2520is%2520a%2520semi-supervised%2520learning%2520scheme%2520based%2520on%250Athe%2520application%2520of%2520identity%2520preserving%2520transformations%2520on%2520the%2520object%2520input%250Arepresentations.%2520It%2520is%2520conjectured%2520in%2520this%2520work%2520that%2520these%2520same%2520applied%250Atransformations%2520preserve%252C%2520in%2520addition%2520to%2520the%2520identity%2520of%2520the%2520presented%2520object%252C%250Aalso%2520the%2520identity%2520of%2520its%2520semantically%2520meaningful%2520attributes.%2520The%2520corollary%2520of%250Athis%2520is%2520that%2520the%2520output%2520representations%2520of%2520such%2520a%2520contrastive%2520learning%2520scheme%250Acontain%2520valuable%2520information%2520not%2520only%2520for%2520the%2520classification%2520of%2520the%2520presented%250Aobject%252C%2520but%2520also%2520for%2520the%2520presence%2520or%2520absence%2520decision%2520of%2520any%2520attribute%2520of%250Ainterest.%2520Simulation%2520results%2520which%2520demonstrate%2520this%2520idea%2520and%2520the%2520feasibility%2520of%250Athis%2520conjecture%2520are%2520presented.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.10763v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20Learning%20and%20the%20Emergence%20of%20Attributes%20Associations&entry.906535625=Daniel%20N.%20Nissani&entry.1292438233=%20%20In%20response%20to%20an%20object%20presentation%2C%20supervised%20learning%20schemes%20generally%0Arespond%20with%20a%20parsimonious%20label.%20Upon%20a%20similar%20presentation%20we%20humans%0Arespond%20again%20with%20a%20label%2C%20but%20are%20flooded%2C%20in%20addition%2C%20by%20a%20myriad%20of%0Aassociations.%20A%20significant%20portion%20of%20these%20consist%20of%20the%20presented%20object%0Aattributes.%20Contrastive%20learning%20is%20a%20semi-supervised%20learning%20scheme%20based%20on%0Athe%20application%20of%20identity%20preserving%20transformations%20on%20the%20object%20input%0Arepresentations.%20It%20is%20conjectured%20in%20this%20work%20that%20these%20same%20applied%0Atransformations%20preserve%2C%20in%20addition%20to%20the%20identity%20of%20the%20presented%20object%2C%0Aalso%20the%20identity%20of%20its%20semantically%20meaningful%20attributes.%20The%20corollary%20of%0Athis%20is%20that%20the%20output%20representations%20of%20such%20a%20contrastive%20learning%20scheme%0Acontain%20valuable%20information%20not%20only%20for%20the%20classification%20of%20the%20presented%0Aobject%2C%20but%20also%20for%20the%20presence%20or%20absence%20decision%20of%20any%20attribute%20of%0Ainterest.%20Simulation%20results%20which%20demonstrate%20this%20idea%20and%20the%20feasibility%20of%0Athis%20conjecture%20are%20presented.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.10763v4&entry.124074799=Read"},
{"title": "Taylor-Sensus Network: Embracing Noise to Enlighten Uncertainty for\n  Scientific Data", "author": "Guangxuan Song and Dongmei Fu and Zhongwei Qiu and Jintao Meng and Dawei Zhang", "abstract": "  Uncertainty estimation is crucial in scientific data for machine learning.\nCurrent uncertainty estimation methods mainly focus on the model's inherent\nuncertainty, while neglecting the explicit modeling of noise in the data.\nFurthermore, noise estimation methods typically rely on temporal or spatial\ndependencies, which can pose a significant challenge in structured scientific\ndata where such dependencies among samples are often absent. To address these\nchallenges in scientific research, we propose the Taylor-Sensus Network\n(TSNet). TSNet innovatively uses a Taylor series expansion to model complex,\nheteroscedastic noise and proposes a deep Taylor block for aware noise\ndistribution. TSNet includes a noise-aware contrastive learning module and a\ndata density perception module for aleatoric and epistemic uncertainty.\nAdditionally, an uncertainty combination operator is used to integrate these\nuncertainties, and the network is trained using a novel heteroscedastic mean\nsquare error loss. TSNet demonstrates superior performance over mainstream and\nstate-of-the-art methods in experiments, highlighting its potential in\nscientific research and noise resistance. It will be open-source to facilitate\nthe community of \"AI for Science\".\n", "link": "http://arxiv.org/abs/2409.07942v1", "date": "2024-09-12", "relevancy": 2.0562, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.531}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5189}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5024}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Taylor-Sensus%20Network%3A%20Embracing%20Noise%20to%20Enlighten%20Uncertainty%20for%0A%20%20Scientific%20Data&body=Title%3A%20Taylor-Sensus%20Network%3A%20Embracing%20Noise%20to%20Enlighten%20Uncertainty%20for%0A%20%20Scientific%20Data%0AAuthor%3A%20Guangxuan%20Song%20and%20Dongmei%20Fu%20and%20Zhongwei%20Qiu%20and%20Jintao%20Meng%20and%20Dawei%20Zhang%0AAbstract%3A%20%20%20Uncertainty%20estimation%20is%20crucial%20in%20scientific%20data%20for%20machine%20learning.%0ACurrent%20uncertainty%20estimation%20methods%20mainly%20focus%20on%20the%20model%27s%20inherent%0Auncertainty%2C%20while%20neglecting%20the%20explicit%20modeling%20of%20noise%20in%20the%20data.%0AFurthermore%2C%20noise%20estimation%20methods%20typically%20rely%20on%20temporal%20or%20spatial%0Adependencies%2C%20which%20can%20pose%20a%20significant%20challenge%20in%20structured%20scientific%0Adata%20where%20such%20dependencies%20among%20samples%20are%20often%20absent.%20To%20address%20these%0Achallenges%20in%20scientific%20research%2C%20we%20propose%20the%20Taylor-Sensus%20Network%0A%28TSNet%29.%20TSNet%20innovatively%20uses%20a%20Taylor%20series%20expansion%20to%20model%20complex%2C%0Aheteroscedastic%20noise%20and%20proposes%20a%20deep%20Taylor%20block%20for%20aware%20noise%0Adistribution.%20TSNet%20includes%20a%20noise-aware%20contrastive%20learning%20module%20and%20a%0Adata%20density%20perception%20module%20for%20aleatoric%20and%20epistemic%20uncertainty.%0AAdditionally%2C%20an%20uncertainty%20combination%20operator%20is%20used%20to%20integrate%20these%0Auncertainties%2C%20and%20the%20network%20is%20trained%20using%20a%20novel%20heteroscedastic%20mean%0Asquare%20error%20loss.%20TSNet%20demonstrates%20superior%20performance%20over%20mainstream%20and%0Astate-of-the-art%20methods%20in%20experiments%2C%20highlighting%20its%20potential%20in%0Ascientific%20research%20and%20noise%20resistance.%20It%20will%20be%20open-source%20to%20facilitate%0Athe%20community%20of%20%22AI%20for%20Science%22.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07942v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTaylor-Sensus%2520Network%253A%2520Embracing%2520Noise%2520to%2520Enlighten%2520Uncertainty%2520for%250A%2520%2520Scientific%2520Data%26entry.906535625%3DGuangxuan%2520Song%2520and%2520Dongmei%2520Fu%2520and%2520Zhongwei%2520Qiu%2520and%2520Jintao%2520Meng%2520and%2520Dawei%2520Zhang%26entry.1292438233%3D%2520%2520Uncertainty%2520estimation%2520is%2520crucial%2520in%2520scientific%2520data%2520for%2520machine%2520learning.%250ACurrent%2520uncertainty%2520estimation%2520methods%2520mainly%2520focus%2520on%2520the%2520model%2527s%2520inherent%250Auncertainty%252C%2520while%2520neglecting%2520the%2520explicit%2520modeling%2520of%2520noise%2520in%2520the%2520data.%250AFurthermore%252C%2520noise%2520estimation%2520methods%2520typically%2520rely%2520on%2520temporal%2520or%2520spatial%250Adependencies%252C%2520which%2520can%2520pose%2520a%2520significant%2520challenge%2520in%2520structured%2520scientific%250Adata%2520where%2520such%2520dependencies%2520among%2520samples%2520are%2520often%2520absent.%2520To%2520address%2520these%250Achallenges%2520in%2520scientific%2520research%252C%2520we%2520propose%2520the%2520Taylor-Sensus%2520Network%250A%2528TSNet%2529.%2520TSNet%2520innovatively%2520uses%2520a%2520Taylor%2520series%2520expansion%2520to%2520model%2520complex%252C%250Aheteroscedastic%2520noise%2520and%2520proposes%2520a%2520deep%2520Taylor%2520block%2520for%2520aware%2520noise%250Adistribution.%2520TSNet%2520includes%2520a%2520noise-aware%2520contrastive%2520learning%2520module%2520and%2520a%250Adata%2520density%2520perception%2520module%2520for%2520aleatoric%2520and%2520epistemic%2520uncertainty.%250AAdditionally%252C%2520an%2520uncertainty%2520combination%2520operator%2520is%2520used%2520to%2520integrate%2520these%250Auncertainties%252C%2520and%2520the%2520network%2520is%2520trained%2520using%2520a%2520novel%2520heteroscedastic%2520mean%250Asquare%2520error%2520loss.%2520TSNet%2520demonstrates%2520superior%2520performance%2520over%2520mainstream%2520and%250Astate-of-the-art%2520methods%2520in%2520experiments%252C%2520highlighting%2520its%2520potential%2520in%250Ascientific%2520research%2520and%2520noise%2520resistance.%2520It%2520will%2520be%2520open-source%2520to%2520facilitate%250Athe%2520community%2520of%2520%2522AI%2520for%2520Science%2522.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07942v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Taylor-Sensus%20Network%3A%20Embracing%20Noise%20to%20Enlighten%20Uncertainty%20for%0A%20%20Scientific%20Data&entry.906535625=Guangxuan%20Song%20and%20Dongmei%20Fu%20and%20Zhongwei%20Qiu%20and%20Jintao%20Meng%20and%20Dawei%20Zhang&entry.1292438233=%20%20Uncertainty%20estimation%20is%20crucial%20in%20scientific%20data%20for%20machine%20learning.%0ACurrent%20uncertainty%20estimation%20methods%20mainly%20focus%20on%20the%20model%27s%20inherent%0Auncertainty%2C%20while%20neglecting%20the%20explicit%20modeling%20of%20noise%20in%20the%20data.%0AFurthermore%2C%20noise%20estimation%20methods%20typically%20rely%20on%20temporal%20or%20spatial%0Adependencies%2C%20which%20can%20pose%20a%20significant%20challenge%20in%20structured%20scientific%0Adata%20where%20such%20dependencies%20among%20samples%20are%20often%20absent.%20To%20address%20these%0Achallenges%20in%20scientific%20research%2C%20we%20propose%20the%20Taylor-Sensus%20Network%0A%28TSNet%29.%20TSNet%20innovatively%20uses%20a%20Taylor%20series%20expansion%20to%20model%20complex%2C%0Aheteroscedastic%20noise%20and%20proposes%20a%20deep%20Taylor%20block%20for%20aware%20noise%0Adistribution.%20TSNet%20includes%20a%20noise-aware%20contrastive%20learning%20module%20and%20a%0Adata%20density%20perception%20module%20for%20aleatoric%20and%20epistemic%20uncertainty.%0AAdditionally%2C%20an%20uncertainty%20combination%20operator%20is%20used%20to%20integrate%20these%0Auncertainties%2C%20and%20the%20network%20is%20trained%20using%20a%20novel%20heteroscedastic%20mean%0Asquare%20error%20loss.%20TSNet%20demonstrates%20superior%20performance%20over%20mainstream%20and%0Astate-of-the-art%20methods%20in%20experiments%2C%20highlighting%20its%20potential%20in%0Ascientific%20research%20and%20noise%20resistance.%20It%20will%20be%20open-source%20to%20facilitate%0Athe%20community%20of%20%22AI%20for%20Science%22.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07942v1&entry.124074799=Read"},
{"title": "SparseGrow: Addressing Growth-Induced Forgetting in Task-Agnostic\n  Continual Learning", "author": "Yuqing Zhao and Divya Saxena and Jiannong Cao and Xiaoyun Liu and Changlin Song", "abstract": "  In continual learning (CL), model growth enhances adaptability over new data,\nimproving knowledge retention for more tasks. However, improper model growth\ncan lead to severe degradation of previously learned knowledge, an issue we\nname as growth-induced forgetting (GIFt), especially in task-agnostic CL using\nentire grown model for inference. Existing works, despite adopting model growth\nand random initialization for better adaptability, often fail to recognize the\npresence of GIFt caused by improper model growth. This oversight limits\ncomprehensive control of forgetting and hinders full utilization of model\ngrowth. We are the first in CL to identify this issue and conduct an in-depth\nstudy on root cause of GIFt, where layer expansion stands out among model\ngrowth strategies, widening layers without affecting model functionality. Yet,\ndirect adoption of layer expansion presents challenges. It lacks data-driven\ncontrol and initialization of expanded parameters to balance adaptability and\nknowledge retention. This paper presents a novel SparseGrow approach to\novercome the issue of GIFt while enhancing adaptability over new data.\nSparseGrow employs data-driven sparse layer expansion to control efficient\nparameter usage during growth, reducing GIFt from excessive growth and\nfunctionality changes. It also combines sparse growth with on-data\ninitialization at training late-stage to create partially 0-valued expansions\nthat fit learned distribution, enhancing retention and adaptability. To further\nminimize forgetting, freezing is applied by calculating the sparse mask,\nallowing data-driven preservation of important parameters. Through experiments\nacross datasets with various settings, cases and task numbers, we demonstrate\nthe necessity of layer expansion and showcase the effectiveness of SparseGrow\nin overcoming GIFt, highlighting its adaptability and knowledge retention for\nincremental tasks.\n", "link": "http://arxiv.org/abs/2408.10566v3", "date": "2024-09-12", "relevancy": 2.0544, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5187}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5142}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5109}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SparseGrow%3A%20Addressing%20Growth-Induced%20Forgetting%20in%20Task-Agnostic%0A%20%20Continual%20Learning&body=Title%3A%20SparseGrow%3A%20Addressing%20Growth-Induced%20Forgetting%20in%20Task-Agnostic%0A%20%20Continual%20Learning%0AAuthor%3A%20Yuqing%20Zhao%20and%20Divya%20Saxena%20and%20Jiannong%20Cao%20and%20Xiaoyun%20Liu%20and%20Changlin%20Song%0AAbstract%3A%20%20%20In%20continual%20learning%20%28CL%29%2C%20model%20growth%20enhances%20adaptability%20over%20new%20data%2C%0Aimproving%20knowledge%20retention%20for%20more%20tasks.%20However%2C%20improper%20model%20growth%0Acan%20lead%20to%20severe%20degradation%20of%20previously%20learned%20knowledge%2C%20an%20issue%20we%0Aname%20as%20growth-induced%20forgetting%20%28GIFt%29%2C%20especially%20in%20task-agnostic%20CL%20using%0Aentire%20grown%20model%20for%20inference.%20Existing%20works%2C%20despite%20adopting%20model%20growth%0Aand%20random%20initialization%20for%20better%20adaptability%2C%20often%20fail%20to%20recognize%20the%0Apresence%20of%20GIFt%20caused%20by%20improper%20model%20growth.%20This%20oversight%20limits%0Acomprehensive%20control%20of%20forgetting%20and%20hinders%20full%20utilization%20of%20model%0Agrowth.%20We%20are%20the%20first%20in%20CL%20to%20identify%20this%20issue%20and%20conduct%20an%20in-depth%0Astudy%20on%20root%20cause%20of%20GIFt%2C%20where%20layer%20expansion%20stands%20out%20among%20model%0Agrowth%20strategies%2C%20widening%20layers%20without%20affecting%20model%20functionality.%20Yet%2C%0Adirect%20adoption%20of%20layer%20expansion%20presents%20challenges.%20It%20lacks%20data-driven%0Acontrol%20and%20initialization%20of%20expanded%20parameters%20to%20balance%20adaptability%20and%0Aknowledge%20retention.%20This%20paper%20presents%20a%20novel%20SparseGrow%20approach%20to%0Aovercome%20the%20issue%20of%20GIFt%20while%20enhancing%20adaptability%20over%20new%20data.%0ASparseGrow%20employs%20data-driven%20sparse%20layer%20expansion%20to%20control%20efficient%0Aparameter%20usage%20during%20growth%2C%20reducing%20GIFt%20from%20excessive%20growth%20and%0Afunctionality%20changes.%20It%20also%20combines%20sparse%20growth%20with%20on-data%0Ainitialization%20at%20training%20late-stage%20to%20create%20partially%200-valued%20expansions%0Athat%20fit%20learned%20distribution%2C%20enhancing%20retention%20and%20adaptability.%20To%20further%0Aminimize%20forgetting%2C%20freezing%20is%20applied%20by%20calculating%20the%20sparse%20mask%2C%0Aallowing%20data-driven%20preservation%20of%20important%20parameters.%20Through%20experiments%0Aacross%20datasets%20with%20various%20settings%2C%20cases%20and%20task%20numbers%2C%20we%20demonstrate%0Athe%20necessity%20of%20layer%20expansion%20and%20showcase%20the%20effectiveness%20of%20SparseGrow%0Ain%20overcoming%20GIFt%2C%20highlighting%20its%20adaptability%20and%20knowledge%20retention%20for%0Aincremental%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10566v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparseGrow%253A%2520Addressing%2520Growth-Induced%2520Forgetting%2520in%2520Task-Agnostic%250A%2520%2520Continual%2520Learning%26entry.906535625%3DYuqing%2520Zhao%2520and%2520Divya%2520Saxena%2520and%2520Jiannong%2520Cao%2520and%2520Xiaoyun%2520Liu%2520and%2520Changlin%2520Song%26entry.1292438233%3D%2520%2520In%2520continual%2520learning%2520%2528CL%2529%252C%2520model%2520growth%2520enhances%2520adaptability%2520over%2520new%2520data%252C%250Aimproving%2520knowledge%2520retention%2520for%2520more%2520tasks.%2520However%252C%2520improper%2520model%2520growth%250Acan%2520lead%2520to%2520severe%2520degradation%2520of%2520previously%2520learned%2520knowledge%252C%2520an%2520issue%2520we%250Aname%2520as%2520growth-induced%2520forgetting%2520%2528GIFt%2529%252C%2520especially%2520in%2520task-agnostic%2520CL%2520using%250Aentire%2520grown%2520model%2520for%2520inference.%2520Existing%2520works%252C%2520despite%2520adopting%2520model%2520growth%250Aand%2520random%2520initialization%2520for%2520better%2520adaptability%252C%2520often%2520fail%2520to%2520recognize%2520the%250Apresence%2520of%2520GIFt%2520caused%2520by%2520improper%2520model%2520growth.%2520This%2520oversight%2520limits%250Acomprehensive%2520control%2520of%2520forgetting%2520and%2520hinders%2520full%2520utilization%2520of%2520model%250Agrowth.%2520We%2520are%2520the%2520first%2520in%2520CL%2520to%2520identify%2520this%2520issue%2520and%2520conduct%2520an%2520in-depth%250Astudy%2520on%2520root%2520cause%2520of%2520GIFt%252C%2520where%2520layer%2520expansion%2520stands%2520out%2520among%2520model%250Agrowth%2520strategies%252C%2520widening%2520layers%2520without%2520affecting%2520model%2520functionality.%2520Yet%252C%250Adirect%2520adoption%2520of%2520layer%2520expansion%2520presents%2520challenges.%2520It%2520lacks%2520data-driven%250Acontrol%2520and%2520initialization%2520of%2520expanded%2520parameters%2520to%2520balance%2520adaptability%2520and%250Aknowledge%2520retention.%2520This%2520paper%2520presents%2520a%2520novel%2520SparseGrow%2520approach%2520to%250Aovercome%2520the%2520issue%2520of%2520GIFt%2520while%2520enhancing%2520adaptability%2520over%2520new%2520data.%250ASparseGrow%2520employs%2520data-driven%2520sparse%2520layer%2520expansion%2520to%2520control%2520efficient%250Aparameter%2520usage%2520during%2520growth%252C%2520reducing%2520GIFt%2520from%2520excessive%2520growth%2520and%250Afunctionality%2520changes.%2520It%2520also%2520combines%2520sparse%2520growth%2520with%2520on-data%250Ainitialization%2520at%2520training%2520late-stage%2520to%2520create%2520partially%25200-valued%2520expansions%250Athat%2520fit%2520learned%2520distribution%252C%2520enhancing%2520retention%2520and%2520adaptability.%2520To%2520further%250Aminimize%2520forgetting%252C%2520freezing%2520is%2520applied%2520by%2520calculating%2520the%2520sparse%2520mask%252C%250Aallowing%2520data-driven%2520preservation%2520of%2520important%2520parameters.%2520Through%2520experiments%250Aacross%2520datasets%2520with%2520various%2520settings%252C%2520cases%2520and%2520task%2520numbers%252C%2520we%2520demonstrate%250Athe%2520necessity%2520of%2520layer%2520expansion%2520and%2520showcase%2520the%2520effectiveness%2520of%2520SparseGrow%250Ain%2520overcoming%2520GIFt%252C%2520highlighting%2520its%2520adaptability%2520and%2520knowledge%2520retention%2520for%250Aincremental%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10566v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SparseGrow%3A%20Addressing%20Growth-Induced%20Forgetting%20in%20Task-Agnostic%0A%20%20Continual%20Learning&entry.906535625=Yuqing%20Zhao%20and%20Divya%20Saxena%20and%20Jiannong%20Cao%20and%20Xiaoyun%20Liu%20and%20Changlin%20Song&entry.1292438233=%20%20In%20continual%20learning%20%28CL%29%2C%20model%20growth%20enhances%20adaptability%20over%20new%20data%2C%0Aimproving%20knowledge%20retention%20for%20more%20tasks.%20However%2C%20improper%20model%20growth%0Acan%20lead%20to%20severe%20degradation%20of%20previously%20learned%20knowledge%2C%20an%20issue%20we%0Aname%20as%20growth-induced%20forgetting%20%28GIFt%29%2C%20especially%20in%20task-agnostic%20CL%20using%0Aentire%20grown%20model%20for%20inference.%20Existing%20works%2C%20despite%20adopting%20model%20growth%0Aand%20random%20initialization%20for%20better%20adaptability%2C%20often%20fail%20to%20recognize%20the%0Apresence%20of%20GIFt%20caused%20by%20improper%20model%20growth.%20This%20oversight%20limits%0Acomprehensive%20control%20of%20forgetting%20and%20hinders%20full%20utilization%20of%20model%0Agrowth.%20We%20are%20the%20first%20in%20CL%20to%20identify%20this%20issue%20and%20conduct%20an%20in-depth%0Astudy%20on%20root%20cause%20of%20GIFt%2C%20where%20layer%20expansion%20stands%20out%20among%20model%0Agrowth%20strategies%2C%20widening%20layers%20without%20affecting%20model%20functionality.%20Yet%2C%0Adirect%20adoption%20of%20layer%20expansion%20presents%20challenges.%20It%20lacks%20data-driven%0Acontrol%20and%20initialization%20of%20expanded%20parameters%20to%20balance%20adaptability%20and%0Aknowledge%20retention.%20This%20paper%20presents%20a%20novel%20SparseGrow%20approach%20to%0Aovercome%20the%20issue%20of%20GIFt%20while%20enhancing%20adaptability%20over%20new%20data.%0ASparseGrow%20employs%20data-driven%20sparse%20layer%20expansion%20to%20control%20efficient%0Aparameter%20usage%20during%20growth%2C%20reducing%20GIFt%20from%20excessive%20growth%20and%0Afunctionality%20changes.%20It%20also%20combines%20sparse%20growth%20with%20on-data%0Ainitialization%20at%20training%20late-stage%20to%20create%20partially%200-valued%20expansions%0Athat%20fit%20learned%20distribution%2C%20enhancing%20retention%20and%20adaptability.%20To%20further%0Aminimize%20forgetting%2C%20freezing%20is%20applied%20by%20calculating%20the%20sparse%20mask%2C%0Aallowing%20data-driven%20preservation%20of%20important%20parameters.%20Through%20experiments%0Aacross%20datasets%20with%20various%20settings%2C%20cases%20and%20task%20numbers%2C%20we%20demonstrate%0Athe%20necessity%20of%20layer%20expansion%20and%20showcase%20the%20effectiveness%20of%20SparseGrow%0Ain%20overcoming%20GIFt%2C%20highlighting%20its%20adaptability%20and%20knowledge%20retention%20for%0Aincremental%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10566v3&entry.124074799=Read"},
{"title": "Optimizing Falsification for Learning-Based Control Systems: A\n  Multi-Fidelity Bayesian Approach", "author": "Zahra Shahrooei and Mykel J. Kochenderfer and Ali Baheri", "abstract": "  Testing controllers in safety-critical systems is vital for ensuring their\nsafety and preventing failures. In this paper, we address the falsification\nproblem within learning-based closed-loop control systems through simulation.\nThis problem involves the identification of counterexamples that violate system\nsafety requirements and can be formulated as an optimization task based on\nthese requirements. Using full-fidelity simulator data in this optimization\nproblem can be computationally expensive. To improve efficiency, we propose a\nmulti-fidelity Bayesian optimization falsification framework that harnesses\nsimulators with varying levels of accuracy. Our proposed framework can\ntransition between different simulators and establish meaningful relationships\nbetween them. Through multi-fidelity Bayesian optimization, we determine both\nthe optimal system input likely to be a counterexample and the appropriate\nfidelity level for assessment. We evaluated our approach across various Gym\nenvironments, each featuring different levels of fidelity. Our experiments\ndemonstrate that multi-fidelity Bayesian optimization is more computationally\nefficient than full-fidelity Bayesian optimization and other baseline methods\nin detecting counterexamples. A Python implementation of the algorithm is\navailable at https://github.com/SAILRIT/MFBO_Falsification.\n", "link": "http://arxiv.org/abs/2409.08097v1", "date": "2024-09-12", "relevancy": 2.0533, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5464}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5398}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4736}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20Falsification%20for%20Learning-Based%20Control%20Systems%3A%20A%0A%20%20Multi-Fidelity%20Bayesian%20Approach&body=Title%3A%20Optimizing%20Falsification%20for%20Learning-Based%20Control%20Systems%3A%20A%0A%20%20Multi-Fidelity%20Bayesian%20Approach%0AAuthor%3A%20Zahra%20Shahrooei%20and%20Mykel%20J.%20Kochenderfer%20and%20Ali%20Baheri%0AAbstract%3A%20%20%20Testing%20controllers%20in%20safety-critical%20systems%20is%20vital%20for%20ensuring%20their%0Asafety%20and%20preventing%20failures.%20In%20this%20paper%2C%20we%20address%20the%20falsification%0Aproblem%20within%20learning-based%20closed-loop%20control%20systems%20through%20simulation.%0AThis%20problem%20involves%20the%20identification%20of%20counterexamples%20that%20violate%20system%0Asafety%20requirements%20and%20can%20be%20formulated%20as%20an%20optimization%20task%20based%20on%0Athese%20requirements.%20Using%20full-fidelity%20simulator%20data%20in%20this%20optimization%0Aproblem%20can%20be%20computationally%20expensive.%20To%20improve%20efficiency%2C%20we%20propose%20a%0Amulti-fidelity%20Bayesian%20optimization%20falsification%20framework%20that%20harnesses%0Asimulators%20with%20varying%20levels%20of%20accuracy.%20Our%20proposed%20framework%20can%0Atransition%20between%20different%20simulators%20and%20establish%20meaningful%20relationships%0Abetween%20them.%20Through%20multi-fidelity%20Bayesian%20optimization%2C%20we%20determine%20both%0Athe%20optimal%20system%20input%20likely%20to%20be%20a%20counterexample%20and%20the%20appropriate%0Afidelity%20level%20for%20assessment.%20We%20evaluated%20our%20approach%20across%20various%20Gym%0Aenvironments%2C%20each%20featuring%20different%20levels%20of%20fidelity.%20Our%20experiments%0Ademonstrate%20that%20multi-fidelity%20Bayesian%20optimization%20is%20more%20computationally%0Aefficient%20than%20full-fidelity%20Bayesian%20optimization%20and%20other%20baseline%20methods%0Ain%20detecting%20counterexamples.%20A%20Python%20implementation%20of%20the%20algorithm%20is%0Aavailable%20at%20https%3A//github.com/SAILRIT/MFBO_Falsification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08097v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520Falsification%2520for%2520Learning-Based%2520Control%2520Systems%253A%2520A%250A%2520%2520Multi-Fidelity%2520Bayesian%2520Approach%26entry.906535625%3DZahra%2520Shahrooei%2520and%2520Mykel%2520J.%2520Kochenderfer%2520and%2520Ali%2520Baheri%26entry.1292438233%3D%2520%2520Testing%2520controllers%2520in%2520safety-critical%2520systems%2520is%2520vital%2520for%2520ensuring%2520their%250Asafety%2520and%2520preventing%2520failures.%2520In%2520this%2520paper%252C%2520we%2520address%2520the%2520falsification%250Aproblem%2520within%2520learning-based%2520closed-loop%2520control%2520systems%2520through%2520simulation.%250AThis%2520problem%2520involves%2520the%2520identification%2520of%2520counterexamples%2520that%2520violate%2520system%250Asafety%2520requirements%2520and%2520can%2520be%2520formulated%2520as%2520an%2520optimization%2520task%2520based%2520on%250Athese%2520requirements.%2520Using%2520full-fidelity%2520simulator%2520data%2520in%2520this%2520optimization%250Aproblem%2520can%2520be%2520computationally%2520expensive.%2520To%2520improve%2520efficiency%252C%2520we%2520propose%2520a%250Amulti-fidelity%2520Bayesian%2520optimization%2520falsification%2520framework%2520that%2520harnesses%250Asimulators%2520with%2520varying%2520levels%2520of%2520accuracy.%2520Our%2520proposed%2520framework%2520can%250Atransition%2520between%2520different%2520simulators%2520and%2520establish%2520meaningful%2520relationships%250Abetween%2520them.%2520Through%2520multi-fidelity%2520Bayesian%2520optimization%252C%2520we%2520determine%2520both%250Athe%2520optimal%2520system%2520input%2520likely%2520to%2520be%2520a%2520counterexample%2520and%2520the%2520appropriate%250Afidelity%2520level%2520for%2520assessment.%2520We%2520evaluated%2520our%2520approach%2520across%2520various%2520Gym%250Aenvironments%252C%2520each%2520featuring%2520different%2520levels%2520of%2520fidelity.%2520Our%2520experiments%250Ademonstrate%2520that%2520multi-fidelity%2520Bayesian%2520optimization%2520is%2520more%2520computationally%250Aefficient%2520than%2520full-fidelity%2520Bayesian%2520optimization%2520and%2520other%2520baseline%2520methods%250Ain%2520detecting%2520counterexamples.%2520A%2520Python%2520implementation%2520of%2520the%2520algorithm%2520is%250Aavailable%2520at%2520https%253A//github.com/SAILRIT/MFBO_Falsification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08097v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20Falsification%20for%20Learning-Based%20Control%20Systems%3A%20A%0A%20%20Multi-Fidelity%20Bayesian%20Approach&entry.906535625=Zahra%20Shahrooei%20and%20Mykel%20J.%20Kochenderfer%20and%20Ali%20Baheri&entry.1292438233=%20%20Testing%20controllers%20in%20safety-critical%20systems%20is%20vital%20for%20ensuring%20their%0Asafety%20and%20preventing%20failures.%20In%20this%20paper%2C%20we%20address%20the%20falsification%0Aproblem%20within%20learning-based%20closed-loop%20control%20systems%20through%20simulation.%0AThis%20problem%20involves%20the%20identification%20of%20counterexamples%20that%20violate%20system%0Asafety%20requirements%20and%20can%20be%20formulated%20as%20an%20optimization%20task%20based%20on%0Athese%20requirements.%20Using%20full-fidelity%20simulator%20data%20in%20this%20optimization%0Aproblem%20can%20be%20computationally%20expensive.%20To%20improve%20efficiency%2C%20we%20propose%20a%0Amulti-fidelity%20Bayesian%20optimization%20falsification%20framework%20that%20harnesses%0Asimulators%20with%20varying%20levels%20of%20accuracy.%20Our%20proposed%20framework%20can%0Atransition%20between%20different%20simulators%20and%20establish%20meaningful%20relationships%0Abetween%20them.%20Through%20multi-fidelity%20Bayesian%20optimization%2C%20we%20determine%20both%0Athe%20optimal%20system%20input%20likely%20to%20be%20a%20counterexample%20and%20the%20appropriate%0Afidelity%20level%20for%20assessment.%20We%20evaluated%20our%20approach%20across%20various%20Gym%0Aenvironments%2C%20each%20featuring%20different%20levels%20of%20fidelity.%20Our%20experiments%0Ademonstrate%20that%20multi-fidelity%20Bayesian%20optimization%20is%20more%20computationally%0Aefficient%20than%20full-fidelity%20Bayesian%20optimization%20and%20other%20baseline%20methods%0Ain%20detecting%20counterexamples.%20A%20Python%20implementation%20of%20the%20algorithm%20is%0Aavailable%20at%20https%3A//github.com/SAILRIT/MFBO_Falsification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08097v1&entry.124074799=Read"},
{"title": "Modeling Human Responses by Ordinal Archetypal Analysis", "author": "Anna Emilie J. Wedenborg and Michael Alexander Harborg and Andreas Bigom and Oliver Elmgreen and Marcus Presutti and Andreas R\u00e5skov and Fumiko Kano Gl\u00fcckstad and Mikkel Schmidt and Morten M\u00f8rup", "abstract": "  This paper introduces a novel framework for Archetypal Analysis (AA) tailored\nto ordinal data, particularly from questionnaires. Unlike existing methods, the\nproposed method, Ordinal Archetypal Analysis (OAA), bypasses the two-step\nprocess of transforming ordinal data into continuous scales and operates\ndirectly on the ordinal data. We extend traditional AA methods to handle the\nsubjective nature of questionnaire-based data, acknowledging individual\ndifferences in scale perception. We introduce the Response Bias Ordinal\nArchetypal Analysis (RBOAA), which learns individualized scales for each\nsubject during optimization. The effectiveness of these methods is demonstrated\non synthetic data and the European Social Survey dataset, highlighting their\npotential to provide deeper insights into human behavior and perception. The\nstudy underscores the importance of considering response bias in cross-national\nresearch and offers a principled approach to analyzing ordinal data through\nArchetypal Analysis.\n", "link": "http://arxiv.org/abs/2409.07934v1", "date": "2024-09-12", "relevancy": 2.0528, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4118}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4118}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4082}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modeling%20Human%20Responses%20by%20Ordinal%20Archetypal%20Analysis&body=Title%3A%20Modeling%20Human%20Responses%20by%20Ordinal%20Archetypal%20Analysis%0AAuthor%3A%20Anna%20Emilie%20J.%20Wedenborg%20and%20Michael%20Alexander%20Harborg%20and%20Andreas%20Bigom%20and%20Oliver%20Elmgreen%20and%20Marcus%20Presutti%20and%20Andreas%20R%C3%A5skov%20and%20Fumiko%20Kano%20Gl%C3%BCckstad%20and%20Mikkel%20Schmidt%20and%20Morten%20M%C3%B8rup%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20framework%20for%20Archetypal%20Analysis%20%28AA%29%20tailored%0Ato%20ordinal%20data%2C%20particularly%20from%20questionnaires.%20Unlike%20existing%20methods%2C%20the%0Aproposed%20method%2C%20Ordinal%20Archetypal%20Analysis%20%28OAA%29%2C%20bypasses%20the%20two-step%0Aprocess%20of%20transforming%20ordinal%20data%20into%20continuous%20scales%20and%20operates%0Adirectly%20on%20the%20ordinal%20data.%20We%20extend%20traditional%20AA%20methods%20to%20handle%20the%0Asubjective%20nature%20of%20questionnaire-based%20data%2C%20acknowledging%20individual%0Adifferences%20in%20scale%20perception.%20We%20introduce%20the%20Response%20Bias%20Ordinal%0AArchetypal%20Analysis%20%28RBOAA%29%2C%20which%20learns%20individualized%20scales%20for%20each%0Asubject%20during%20optimization.%20The%20effectiveness%20of%20these%20methods%20is%20demonstrated%0Aon%20synthetic%20data%20and%20the%20European%20Social%20Survey%20dataset%2C%20highlighting%20their%0Apotential%20to%20provide%20deeper%20insights%20into%20human%20behavior%20and%20perception.%20The%0Astudy%20underscores%20the%20importance%20of%20considering%20response%20bias%20in%20cross-national%0Aresearch%20and%20offers%20a%20principled%20approach%20to%20analyzing%20ordinal%20data%20through%0AArchetypal%20Analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07934v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModeling%2520Human%2520Responses%2520by%2520Ordinal%2520Archetypal%2520Analysis%26entry.906535625%3DAnna%2520Emilie%2520J.%2520Wedenborg%2520and%2520Michael%2520Alexander%2520Harborg%2520and%2520Andreas%2520Bigom%2520and%2520Oliver%2520Elmgreen%2520and%2520Marcus%2520Presutti%2520and%2520Andreas%2520R%25C3%25A5skov%2520and%2520Fumiko%2520Kano%2520Gl%25C3%25BCckstad%2520and%2520Mikkel%2520Schmidt%2520and%2520Morten%2520M%25C3%25B8rup%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520framework%2520for%2520Archetypal%2520Analysis%2520%2528AA%2529%2520tailored%250Ato%2520ordinal%2520data%252C%2520particularly%2520from%2520questionnaires.%2520Unlike%2520existing%2520methods%252C%2520the%250Aproposed%2520method%252C%2520Ordinal%2520Archetypal%2520Analysis%2520%2528OAA%2529%252C%2520bypasses%2520the%2520two-step%250Aprocess%2520of%2520transforming%2520ordinal%2520data%2520into%2520continuous%2520scales%2520and%2520operates%250Adirectly%2520on%2520the%2520ordinal%2520data.%2520We%2520extend%2520traditional%2520AA%2520methods%2520to%2520handle%2520the%250Asubjective%2520nature%2520of%2520questionnaire-based%2520data%252C%2520acknowledging%2520individual%250Adifferences%2520in%2520scale%2520perception.%2520We%2520introduce%2520the%2520Response%2520Bias%2520Ordinal%250AArchetypal%2520Analysis%2520%2528RBOAA%2529%252C%2520which%2520learns%2520individualized%2520scales%2520for%2520each%250Asubject%2520during%2520optimization.%2520The%2520effectiveness%2520of%2520these%2520methods%2520is%2520demonstrated%250Aon%2520synthetic%2520data%2520and%2520the%2520European%2520Social%2520Survey%2520dataset%252C%2520highlighting%2520their%250Apotential%2520to%2520provide%2520deeper%2520insights%2520into%2520human%2520behavior%2520and%2520perception.%2520The%250Astudy%2520underscores%2520the%2520importance%2520of%2520considering%2520response%2520bias%2520in%2520cross-national%250Aresearch%2520and%2520offers%2520a%2520principled%2520approach%2520to%2520analyzing%2520ordinal%2520data%2520through%250AArchetypal%2520Analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07934v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modeling%20Human%20Responses%20by%20Ordinal%20Archetypal%20Analysis&entry.906535625=Anna%20Emilie%20J.%20Wedenborg%20and%20Michael%20Alexander%20Harborg%20and%20Andreas%20Bigom%20and%20Oliver%20Elmgreen%20and%20Marcus%20Presutti%20and%20Andreas%20R%C3%A5skov%20and%20Fumiko%20Kano%20Gl%C3%BCckstad%20and%20Mikkel%20Schmidt%20and%20Morten%20M%C3%B8rup&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20framework%20for%20Archetypal%20Analysis%20%28AA%29%20tailored%0Ato%20ordinal%20data%2C%20particularly%20from%20questionnaires.%20Unlike%20existing%20methods%2C%20the%0Aproposed%20method%2C%20Ordinal%20Archetypal%20Analysis%20%28OAA%29%2C%20bypasses%20the%20two-step%0Aprocess%20of%20transforming%20ordinal%20data%20into%20continuous%20scales%20and%20operates%0Adirectly%20on%20the%20ordinal%20data.%20We%20extend%20traditional%20AA%20methods%20to%20handle%20the%0Asubjective%20nature%20of%20questionnaire-based%20data%2C%20acknowledging%20individual%0Adifferences%20in%20scale%20perception.%20We%20introduce%20the%20Response%20Bias%20Ordinal%0AArchetypal%20Analysis%20%28RBOAA%29%2C%20which%20learns%20individualized%20scales%20for%20each%0Asubject%20during%20optimization.%20The%20effectiveness%20of%20these%20methods%20is%20demonstrated%0Aon%20synthetic%20data%20and%20the%20European%20Social%20Survey%20dataset%2C%20highlighting%20their%0Apotential%20to%20provide%20deeper%20insights%20into%20human%20behavior%20and%20perception.%20The%0Astudy%20underscores%20the%20importance%20of%20considering%20response%20bias%20in%20cross-national%0Aresearch%20and%20offers%20a%20principled%20approach%20to%20analyzing%20ordinal%20data%20through%0AArchetypal%20Analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07934v1&entry.124074799=Read"},
{"title": "Model Ensemble for Brain Tumor Segmentation in Magnetic Resonance\n  Imaging", "author": "Daniel Capell\u00e1n-Mart\u00edn and Zhifan Jiang and Abhijeet Parida and Xinyang Liu and Van Lam and Hareem Nisar and Austin Tapp and Sarah Elsharkawi and Maria J. Ledesma-Carbayo and Syed Muhammad Anwar and Marius George Linguraru", "abstract": "  Segmenting brain tumors in multi-parametric magnetic resonance imaging\nenables performing quantitative analysis in support of clinical trials and\npersonalized patient care. This analysis provides the potential to impact\nclinical decision-making processes, including diagnosis and prognosis. In 2023,\nthe well-established Brain Tumor Segmentation (BraTS) challenge presented a\nsubstantial expansion with eight tasks and 4,500 brain tumor cases. In this\npaper, we present a deep learning-based ensemble strategy that is evaluated for\nnewly included tumor cases in three tasks: pediatric brain tumors (PED),\nintracranial meningioma (MEN), and brain metastases (MET). In particular, we\nensemble outputs from state-of-the-art nnU-Net and Swin UNETR models on a\nregion-wise basis. Furthermore, we implemented a targeted post-processing\nstrategy based on a cross-validated threshold search to improve the\nsegmentation results for tumor sub-regions. The evaluation of our proposed\nmethod on unseen test cases for the three tasks resulted in lesion-wise Dice\nscores for PED: 0.653, 0.809, 0.826; MEN: 0.876, 0.867, 0.849; and MET: 0.555,\n0.6, 0.58; for the enhancing tumor, tumor core, and whole tumor, respectively.\nOur method was ranked first for PED, third for MEN, and fourth for MET,\nrespectively.\n", "link": "http://arxiv.org/abs/2409.08232v1", "date": "2024-09-12", "relevancy": 2.0415, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5326}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5059}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model%20Ensemble%20for%20Brain%20Tumor%20Segmentation%20in%20Magnetic%20Resonance%0A%20%20Imaging&body=Title%3A%20Model%20Ensemble%20for%20Brain%20Tumor%20Segmentation%20in%20Magnetic%20Resonance%0A%20%20Imaging%0AAuthor%3A%20Daniel%20Capell%C3%A1n-Mart%C3%ADn%20and%20Zhifan%20Jiang%20and%20Abhijeet%20Parida%20and%20Xinyang%20Liu%20and%20Van%20Lam%20and%20Hareem%20Nisar%20and%20Austin%20Tapp%20and%20Sarah%20Elsharkawi%20and%20Maria%20J.%20Ledesma-Carbayo%20and%20Syed%20Muhammad%20Anwar%20and%20Marius%20George%20Linguraru%0AAbstract%3A%20%20%20Segmenting%20brain%20tumors%20in%20multi-parametric%20magnetic%20resonance%20imaging%0Aenables%20performing%20quantitative%20analysis%20in%20support%20of%20clinical%20trials%20and%0Apersonalized%20patient%20care.%20This%20analysis%20provides%20the%20potential%20to%20impact%0Aclinical%20decision-making%20processes%2C%20including%20diagnosis%20and%20prognosis.%20In%202023%2C%0Athe%20well-established%20Brain%20Tumor%20Segmentation%20%28BraTS%29%20challenge%20presented%20a%0Asubstantial%20expansion%20with%20eight%20tasks%20and%204%2C500%20brain%20tumor%20cases.%20In%20this%0Apaper%2C%20we%20present%20a%20deep%20learning-based%20ensemble%20strategy%20that%20is%20evaluated%20for%0Anewly%20included%20tumor%20cases%20in%20three%20tasks%3A%20pediatric%20brain%20tumors%20%28PED%29%2C%0Aintracranial%20meningioma%20%28MEN%29%2C%20and%20brain%20metastases%20%28MET%29.%20In%20particular%2C%20we%0Aensemble%20outputs%20from%20state-of-the-art%20nnU-Net%20and%20Swin%20UNETR%20models%20on%20a%0Aregion-wise%20basis.%20Furthermore%2C%20we%20implemented%20a%20targeted%20post-processing%0Astrategy%20based%20on%20a%20cross-validated%20threshold%20search%20to%20improve%20the%0Asegmentation%20results%20for%20tumor%20sub-regions.%20The%20evaluation%20of%20our%20proposed%0Amethod%20on%20unseen%20test%20cases%20for%20the%20three%20tasks%20resulted%20in%20lesion-wise%20Dice%0Ascores%20for%20PED%3A%200.653%2C%200.809%2C%200.826%3B%20MEN%3A%200.876%2C%200.867%2C%200.849%3B%20and%20MET%3A%200.555%2C%0A0.6%2C%200.58%3B%20for%20the%20enhancing%20tumor%2C%20tumor%20core%2C%20and%20whole%20tumor%2C%20respectively.%0AOur%20method%20was%20ranked%20first%20for%20PED%2C%20third%20for%20MEN%2C%20and%20fourth%20for%20MET%2C%0Arespectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08232v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel%2520Ensemble%2520for%2520Brain%2520Tumor%2520Segmentation%2520in%2520Magnetic%2520Resonance%250A%2520%2520Imaging%26entry.906535625%3DDaniel%2520Capell%25C3%25A1n-Mart%25C3%25ADn%2520and%2520Zhifan%2520Jiang%2520and%2520Abhijeet%2520Parida%2520and%2520Xinyang%2520Liu%2520and%2520Van%2520Lam%2520and%2520Hareem%2520Nisar%2520and%2520Austin%2520Tapp%2520and%2520Sarah%2520Elsharkawi%2520and%2520Maria%2520J.%2520Ledesma-Carbayo%2520and%2520Syed%2520Muhammad%2520Anwar%2520and%2520Marius%2520George%2520Linguraru%26entry.1292438233%3D%2520%2520Segmenting%2520brain%2520tumors%2520in%2520multi-parametric%2520magnetic%2520resonance%2520imaging%250Aenables%2520performing%2520quantitative%2520analysis%2520in%2520support%2520of%2520clinical%2520trials%2520and%250Apersonalized%2520patient%2520care.%2520This%2520analysis%2520provides%2520the%2520potential%2520to%2520impact%250Aclinical%2520decision-making%2520processes%252C%2520including%2520diagnosis%2520and%2520prognosis.%2520In%25202023%252C%250Athe%2520well-established%2520Brain%2520Tumor%2520Segmentation%2520%2528BraTS%2529%2520challenge%2520presented%2520a%250Asubstantial%2520expansion%2520with%2520eight%2520tasks%2520and%25204%252C500%2520brain%2520tumor%2520cases.%2520In%2520this%250Apaper%252C%2520we%2520present%2520a%2520deep%2520learning-based%2520ensemble%2520strategy%2520that%2520is%2520evaluated%2520for%250Anewly%2520included%2520tumor%2520cases%2520in%2520three%2520tasks%253A%2520pediatric%2520brain%2520tumors%2520%2528PED%2529%252C%250Aintracranial%2520meningioma%2520%2528MEN%2529%252C%2520and%2520brain%2520metastases%2520%2528MET%2529.%2520In%2520particular%252C%2520we%250Aensemble%2520outputs%2520from%2520state-of-the-art%2520nnU-Net%2520and%2520Swin%2520UNETR%2520models%2520on%2520a%250Aregion-wise%2520basis.%2520Furthermore%252C%2520we%2520implemented%2520a%2520targeted%2520post-processing%250Astrategy%2520based%2520on%2520a%2520cross-validated%2520threshold%2520search%2520to%2520improve%2520the%250Asegmentation%2520results%2520for%2520tumor%2520sub-regions.%2520The%2520evaluation%2520of%2520our%2520proposed%250Amethod%2520on%2520unseen%2520test%2520cases%2520for%2520the%2520three%2520tasks%2520resulted%2520in%2520lesion-wise%2520Dice%250Ascores%2520for%2520PED%253A%25200.653%252C%25200.809%252C%25200.826%253B%2520MEN%253A%25200.876%252C%25200.867%252C%25200.849%253B%2520and%2520MET%253A%25200.555%252C%250A0.6%252C%25200.58%253B%2520for%2520the%2520enhancing%2520tumor%252C%2520tumor%2520core%252C%2520and%2520whole%2520tumor%252C%2520respectively.%250AOur%2520method%2520was%2520ranked%2520first%2520for%2520PED%252C%2520third%2520for%2520MEN%252C%2520and%2520fourth%2520for%2520MET%252C%250Arespectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08232v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model%20Ensemble%20for%20Brain%20Tumor%20Segmentation%20in%20Magnetic%20Resonance%0A%20%20Imaging&entry.906535625=Daniel%20Capell%C3%A1n-Mart%C3%ADn%20and%20Zhifan%20Jiang%20and%20Abhijeet%20Parida%20and%20Xinyang%20Liu%20and%20Van%20Lam%20and%20Hareem%20Nisar%20and%20Austin%20Tapp%20and%20Sarah%20Elsharkawi%20and%20Maria%20J.%20Ledesma-Carbayo%20and%20Syed%20Muhammad%20Anwar%20and%20Marius%20George%20Linguraru&entry.1292438233=%20%20Segmenting%20brain%20tumors%20in%20multi-parametric%20magnetic%20resonance%20imaging%0Aenables%20performing%20quantitative%20analysis%20in%20support%20of%20clinical%20trials%20and%0Apersonalized%20patient%20care.%20This%20analysis%20provides%20the%20potential%20to%20impact%0Aclinical%20decision-making%20processes%2C%20including%20diagnosis%20and%20prognosis.%20In%202023%2C%0Athe%20well-established%20Brain%20Tumor%20Segmentation%20%28BraTS%29%20challenge%20presented%20a%0Asubstantial%20expansion%20with%20eight%20tasks%20and%204%2C500%20brain%20tumor%20cases.%20In%20this%0Apaper%2C%20we%20present%20a%20deep%20learning-based%20ensemble%20strategy%20that%20is%20evaluated%20for%0Anewly%20included%20tumor%20cases%20in%20three%20tasks%3A%20pediatric%20brain%20tumors%20%28PED%29%2C%0Aintracranial%20meningioma%20%28MEN%29%2C%20and%20brain%20metastases%20%28MET%29.%20In%20particular%2C%20we%0Aensemble%20outputs%20from%20state-of-the-art%20nnU-Net%20and%20Swin%20UNETR%20models%20on%20a%0Aregion-wise%20basis.%20Furthermore%2C%20we%20implemented%20a%20targeted%20post-processing%0Astrategy%20based%20on%20a%20cross-validated%20threshold%20search%20to%20improve%20the%0Asegmentation%20results%20for%20tumor%20sub-regions.%20The%20evaluation%20of%20our%20proposed%0Amethod%20on%20unseen%20test%20cases%20for%20the%20three%20tasks%20resulted%20in%20lesion-wise%20Dice%0Ascores%20for%20PED%3A%200.653%2C%200.809%2C%200.826%3B%20MEN%3A%200.876%2C%200.867%2C%200.849%3B%20and%20MET%3A%200.555%2C%0A0.6%2C%200.58%3B%20for%20the%20enhancing%20tumor%2C%20tumor%20core%2C%20and%20whole%20tumor%2C%20respectively.%0AOur%20method%20was%20ranked%20first%20for%20PED%2C%20third%20for%20MEN%2C%20and%20fourth%20for%20MET%2C%0Arespectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08232v1&entry.124074799=Read"},
{"title": "Detailed delineation of the fetal brain in diffusion MRI via multi-task\n  learning", "author": "Davood Karimi and Camilo Calixto and Haykel Snoussi and Maria Camila Cortes-Albornoz and Clemente Velasco-Annis and Caitlin Rollins and Camilo Jaimes and Ali Gholipour and Simon K. Warfield", "abstract": "  Diffusion-weighted MRI is increasingly used to study the normal and abnormal\ndevelopment of fetal brain in-utero. Recent studies have shown that dMRI can\noffer invaluable insights into the neurodevelopmental processes in the fetal\nstage. However, because of the low data quality and rapid brain development,\nreliable analysis of fetal dMRI data requires dedicated computational methods\nthat are currently unavailable. The lack of automated methods for fast,\naccurate, and reproducible data analysis has seriously limited our ability to\ntap the potential of fetal brain dMRI for medical and scientific applications.\nIn this work, we developed and validated a unified computational framework to\n(1) segment the brain tissue into white matter, cortical/subcortical gray\nmatter, and cerebrospinal fluid, (2) segment 31 distinct white matter tracts,\nand (3) parcellate the brain's cortex and delineate the deep gray nuclei and\nwhite matter structures into 96 anatomically meaningful regions. We utilized a\nset of manual, semi-automatic, and automatic approaches to annotate 97 fetal\nbrains. Using these labels, we developed and validated a multi-task deep\nlearning method to perform the three computations. Our evaluations show that\nthe new method can accurately carry out all three tasks, achieving a mean Dice\nsimilarity coefficient of 0.865 on tissue segmentation, 0.825 on white matter\ntract segmentation, and 0.819 on parcellation. The proposed method can greatly\nadvance the field of fetal neuroimaging as it can lead to substantial\nimprovements in fetal brain tractography, tract-specific analysis, and\nstructural connectivity assessment.\n", "link": "http://arxiv.org/abs/2409.06716v2", "date": "2024-09-12", "relevancy": 2.0376, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5304}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5052}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detailed%20delineation%20of%20the%20fetal%20brain%20in%20diffusion%20MRI%20via%20multi-task%0A%20%20learning&body=Title%3A%20Detailed%20delineation%20of%20the%20fetal%20brain%20in%20diffusion%20MRI%20via%20multi-task%0A%20%20learning%0AAuthor%3A%20Davood%20Karimi%20and%20Camilo%20Calixto%20and%20Haykel%20Snoussi%20and%20Maria%20Camila%20Cortes-Albornoz%20and%20Clemente%20Velasco-Annis%20and%20Caitlin%20Rollins%20and%20Camilo%20Jaimes%20and%20Ali%20Gholipour%20and%20Simon%20K.%20Warfield%0AAbstract%3A%20%20%20Diffusion-weighted%20MRI%20is%20increasingly%20used%20to%20study%20the%20normal%20and%20abnormal%0Adevelopment%20of%20fetal%20brain%20in-utero.%20Recent%20studies%20have%20shown%20that%20dMRI%20can%0Aoffer%20invaluable%20insights%20into%20the%20neurodevelopmental%20processes%20in%20the%20fetal%0Astage.%20However%2C%20because%20of%20the%20low%20data%20quality%20and%20rapid%20brain%20development%2C%0Areliable%20analysis%20of%20fetal%20dMRI%20data%20requires%20dedicated%20computational%20methods%0Athat%20are%20currently%20unavailable.%20The%20lack%20of%20automated%20methods%20for%20fast%2C%0Aaccurate%2C%20and%20reproducible%20data%20analysis%20has%20seriously%20limited%20our%20ability%20to%0Atap%20the%20potential%20of%20fetal%20brain%20dMRI%20for%20medical%20and%20scientific%20applications.%0AIn%20this%20work%2C%20we%20developed%20and%20validated%20a%20unified%20computational%20framework%20to%0A%281%29%20segment%20the%20brain%20tissue%20into%20white%20matter%2C%20cortical/subcortical%20gray%0Amatter%2C%20and%20cerebrospinal%20fluid%2C%20%282%29%20segment%2031%20distinct%20white%20matter%20tracts%2C%0Aand%20%283%29%20parcellate%20the%20brain%27s%20cortex%20and%20delineate%20the%20deep%20gray%20nuclei%20and%0Awhite%20matter%20structures%20into%2096%20anatomically%20meaningful%20regions.%20We%20utilized%20a%0Aset%20of%20manual%2C%20semi-automatic%2C%20and%20automatic%20approaches%20to%20annotate%2097%20fetal%0Abrains.%20Using%20these%20labels%2C%20we%20developed%20and%20validated%20a%20multi-task%20deep%0Alearning%20method%20to%20perform%20the%20three%20computations.%20Our%20evaluations%20show%20that%0Athe%20new%20method%20can%20accurately%20carry%20out%20all%20three%20tasks%2C%20achieving%20a%20mean%20Dice%0Asimilarity%20coefficient%20of%200.865%20on%20tissue%20segmentation%2C%200.825%20on%20white%20matter%0Atract%20segmentation%2C%20and%200.819%20on%20parcellation.%20The%20proposed%20method%20can%20greatly%0Aadvance%20the%20field%20of%20fetal%20neuroimaging%20as%20it%20can%20lead%20to%20substantial%0Aimprovements%20in%20fetal%20brain%20tractography%2C%20tract-specific%20analysis%2C%20and%0Astructural%20connectivity%20assessment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06716v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetailed%2520delineation%2520of%2520the%2520fetal%2520brain%2520in%2520diffusion%2520MRI%2520via%2520multi-task%250A%2520%2520learning%26entry.906535625%3DDavood%2520Karimi%2520and%2520Camilo%2520Calixto%2520and%2520Haykel%2520Snoussi%2520and%2520Maria%2520Camila%2520Cortes-Albornoz%2520and%2520Clemente%2520Velasco-Annis%2520and%2520Caitlin%2520Rollins%2520and%2520Camilo%2520Jaimes%2520and%2520Ali%2520Gholipour%2520and%2520Simon%2520K.%2520Warfield%26entry.1292438233%3D%2520%2520Diffusion-weighted%2520MRI%2520is%2520increasingly%2520used%2520to%2520study%2520the%2520normal%2520and%2520abnormal%250Adevelopment%2520of%2520fetal%2520brain%2520in-utero.%2520Recent%2520studies%2520have%2520shown%2520that%2520dMRI%2520can%250Aoffer%2520invaluable%2520insights%2520into%2520the%2520neurodevelopmental%2520processes%2520in%2520the%2520fetal%250Astage.%2520However%252C%2520because%2520of%2520the%2520low%2520data%2520quality%2520and%2520rapid%2520brain%2520development%252C%250Areliable%2520analysis%2520of%2520fetal%2520dMRI%2520data%2520requires%2520dedicated%2520computational%2520methods%250Athat%2520are%2520currently%2520unavailable.%2520The%2520lack%2520of%2520automated%2520methods%2520for%2520fast%252C%250Aaccurate%252C%2520and%2520reproducible%2520data%2520analysis%2520has%2520seriously%2520limited%2520our%2520ability%2520to%250Atap%2520the%2520potential%2520of%2520fetal%2520brain%2520dMRI%2520for%2520medical%2520and%2520scientific%2520applications.%250AIn%2520this%2520work%252C%2520we%2520developed%2520and%2520validated%2520a%2520unified%2520computational%2520framework%2520to%250A%25281%2529%2520segment%2520the%2520brain%2520tissue%2520into%2520white%2520matter%252C%2520cortical/subcortical%2520gray%250Amatter%252C%2520and%2520cerebrospinal%2520fluid%252C%2520%25282%2529%2520segment%252031%2520distinct%2520white%2520matter%2520tracts%252C%250Aand%2520%25283%2529%2520parcellate%2520the%2520brain%2527s%2520cortex%2520and%2520delineate%2520the%2520deep%2520gray%2520nuclei%2520and%250Awhite%2520matter%2520structures%2520into%252096%2520anatomically%2520meaningful%2520regions.%2520We%2520utilized%2520a%250Aset%2520of%2520manual%252C%2520semi-automatic%252C%2520and%2520automatic%2520approaches%2520to%2520annotate%252097%2520fetal%250Abrains.%2520Using%2520these%2520labels%252C%2520we%2520developed%2520and%2520validated%2520a%2520multi-task%2520deep%250Alearning%2520method%2520to%2520perform%2520the%2520three%2520computations.%2520Our%2520evaluations%2520show%2520that%250Athe%2520new%2520method%2520can%2520accurately%2520carry%2520out%2520all%2520three%2520tasks%252C%2520achieving%2520a%2520mean%2520Dice%250Asimilarity%2520coefficient%2520of%25200.865%2520on%2520tissue%2520segmentation%252C%25200.825%2520on%2520white%2520matter%250Atract%2520segmentation%252C%2520and%25200.819%2520on%2520parcellation.%2520The%2520proposed%2520method%2520can%2520greatly%250Aadvance%2520the%2520field%2520of%2520fetal%2520neuroimaging%2520as%2520it%2520can%2520lead%2520to%2520substantial%250Aimprovements%2520in%2520fetal%2520brain%2520tractography%252C%2520tract-specific%2520analysis%252C%2520and%250Astructural%2520connectivity%2520assessment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06716v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detailed%20delineation%20of%20the%20fetal%20brain%20in%20diffusion%20MRI%20via%20multi-task%0A%20%20learning&entry.906535625=Davood%20Karimi%20and%20Camilo%20Calixto%20and%20Haykel%20Snoussi%20and%20Maria%20Camila%20Cortes-Albornoz%20and%20Clemente%20Velasco-Annis%20and%20Caitlin%20Rollins%20and%20Camilo%20Jaimes%20and%20Ali%20Gholipour%20and%20Simon%20K.%20Warfield&entry.1292438233=%20%20Diffusion-weighted%20MRI%20is%20increasingly%20used%20to%20study%20the%20normal%20and%20abnormal%0Adevelopment%20of%20fetal%20brain%20in-utero.%20Recent%20studies%20have%20shown%20that%20dMRI%20can%0Aoffer%20invaluable%20insights%20into%20the%20neurodevelopmental%20processes%20in%20the%20fetal%0Astage.%20However%2C%20because%20of%20the%20low%20data%20quality%20and%20rapid%20brain%20development%2C%0Areliable%20analysis%20of%20fetal%20dMRI%20data%20requires%20dedicated%20computational%20methods%0Athat%20are%20currently%20unavailable.%20The%20lack%20of%20automated%20methods%20for%20fast%2C%0Aaccurate%2C%20and%20reproducible%20data%20analysis%20has%20seriously%20limited%20our%20ability%20to%0Atap%20the%20potential%20of%20fetal%20brain%20dMRI%20for%20medical%20and%20scientific%20applications.%0AIn%20this%20work%2C%20we%20developed%20and%20validated%20a%20unified%20computational%20framework%20to%0A%281%29%20segment%20the%20brain%20tissue%20into%20white%20matter%2C%20cortical/subcortical%20gray%0Amatter%2C%20and%20cerebrospinal%20fluid%2C%20%282%29%20segment%2031%20distinct%20white%20matter%20tracts%2C%0Aand%20%283%29%20parcellate%20the%20brain%27s%20cortex%20and%20delineate%20the%20deep%20gray%20nuclei%20and%0Awhite%20matter%20structures%20into%2096%20anatomically%20meaningful%20regions.%20We%20utilized%20a%0Aset%20of%20manual%2C%20semi-automatic%2C%20and%20automatic%20approaches%20to%20annotate%2097%20fetal%0Abrains.%20Using%20these%20labels%2C%20we%20developed%20and%20validated%20a%20multi-task%20deep%0Alearning%20method%20to%20perform%20the%20three%20computations.%20Our%20evaluations%20show%20that%0Athe%20new%20method%20can%20accurately%20carry%20out%20all%20three%20tasks%2C%20achieving%20a%20mean%20Dice%0Asimilarity%20coefficient%20of%200.865%20on%20tissue%20segmentation%2C%200.825%20on%20white%20matter%0Atract%20segmentation%2C%20and%200.819%20on%20parcellation.%20The%20proposed%20method%20can%20greatly%0Aadvance%20the%20field%20of%20fetal%20neuroimaging%20as%20it%20can%20lead%20to%20substantial%0Aimprovements%20in%20fetal%20brain%20tractography%2C%20tract-specific%20analysis%2C%20and%0Astructural%20connectivity%20assessment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06716v2&entry.124074799=Read"},
{"title": "Short-term power load forecasting method based on CNN-SAEDN-Res", "author": "Yang Cui and Han Zhu and Yijian Wang and Lu Zhang and Yang Li", "abstract": "  In deep learning, the load data with non-temporal factors are difficult to\nprocess by sequence models. This problem results in insufficient precision of\nthe prediction. Therefore, a short-term load forecasting method based on\nconvolutional neural network (CNN), self-attention encoder-decoder network\n(SAEDN) and residual-refinement (Res) is proposed. In this method, feature\nextraction module is composed of a two-dimensional convolutional neural\nnetwork, which is used to mine the local correlation between data and obtain\nhigh-dimensional data features. The initial load fore-casting module consists\nof a self-attention encoder-decoder network and a feedforward neural network\n(FFN). The module utilizes self-attention mechanisms to encode high-dimensional\nfeatures. This operation can obtain the global correlation between data.\nTherefore, the model is able to retain important information based on the\ncoupling relationship between the data in data mixed with non-time series\nfactors. Then, self-attention decoding is per-formed and the feedforward neural\nnetwork is used to regression initial load. This paper introduces the residual\nmechanism to build the load optimization module. The module generates residual\nload values to optimize the initial load. The simulation results show that the\nproposed load forecasting method has advantages in terms of prediction accuracy\nand prediction stability.\n", "link": "http://arxiv.org/abs/2309.07140v2", "date": "2024-09-12", "relevancy": 2.0334, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5504}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4857}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Short-term%20power%20load%20forecasting%20method%20based%20on%20CNN-SAEDN-Res&body=Title%3A%20Short-term%20power%20load%20forecasting%20method%20based%20on%20CNN-SAEDN-Res%0AAuthor%3A%20Yang%20Cui%20and%20Han%20Zhu%20and%20Yijian%20Wang%20and%20Lu%20Zhang%20and%20Yang%20Li%0AAbstract%3A%20%20%20In%20deep%20learning%2C%20the%20load%20data%20with%20non-temporal%20factors%20are%20difficult%20to%0Aprocess%20by%20sequence%20models.%20This%20problem%20results%20in%20insufficient%20precision%20of%0Athe%20prediction.%20Therefore%2C%20a%20short-term%20load%20forecasting%20method%20based%20on%0Aconvolutional%20neural%20network%20%28CNN%29%2C%20self-attention%20encoder-decoder%20network%0A%28SAEDN%29%20and%20residual-refinement%20%28Res%29%20is%20proposed.%20In%20this%20method%2C%20feature%0Aextraction%20module%20is%20composed%20of%20a%20two-dimensional%20convolutional%20neural%0Anetwork%2C%20which%20is%20used%20to%20mine%20the%20local%20correlation%20between%20data%20and%20obtain%0Ahigh-dimensional%20data%20features.%20The%20initial%20load%20fore-casting%20module%20consists%0Aof%20a%20self-attention%20encoder-decoder%20network%20and%20a%20feedforward%20neural%20network%0A%28FFN%29.%20The%20module%20utilizes%20self-attention%20mechanisms%20to%20encode%20high-dimensional%0Afeatures.%20This%20operation%20can%20obtain%20the%20global%20correlation%20between%20data.%0ATherefore%2C%20the%20model%20is%20able%20to%20retain%20important%20information%20based%20on%20the%0Acoupling%20relationship%20between%20the%20data%20in%20data%20mixed%20with%20non-time%20series%0Afactors.%20Then%2C%20self-attention%20decoding%20is%20per-formed%20and%20the%20feedforward%20neural%0Anetwork%20is%20used%20to%20regression%20initial%20load.%20This%20paper%20introduces%20the%20residual%0Amechanism%20to%20build%20the%20load%20optimization%20module.%20The%20module%20generates%20residual%0Aload%20values%20to%20optimize%20the%20initial%20load.%20The%20simulation%20results%20show%20that%20the%0Aproposed%20load%20forecasting%20method%20has%20advantages%20in%20terms%20of%20prediction%20accuracy%0Aand%20prediction%20stability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.07140v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShort-term%2520power%2520load%2520forecasting%2520method%2520based%2520on%2520CNN-SAEDN-Res%26entry.906535625%3DYang%2520Cui%2520and%2520Han%2520Zhu%2520and%2520Yijian%2520Wang%2520and%2520Lu%2520Zhang%2520and%2520Yang%2520Li%26entry.1292438233%3D%2520%2520In%2520deep%2520learning%252C%2520the%2520load%2520data%2520with%2520non-temporal%2520factors%2520are%2520difficult%2520to%250Aprocess%2520by%2520sequence%2520models.%2520This%2520problem%2520results%2520in%2520insufficient%2520precision%2520of%250Athe%2520prediction.%2520Therefore%252C%2520a%2520short-term%2520load%2520forecasting%2520method%2520based%2520on%250Aconvolutional%2520neural%2520network%2520%2528CNN%2529%252C%2520self-attention%2520encoder-decoder%2520network%250A%2528SAEDN%2529%2520and%2520residual-refinement%2520%2528Res%2529%2520is%2520proposed.%2520In%2520this%2520method%252C%2520feature%250Aextraction%2520module%2520is%2520composed%2520of%2520a%2520two-dimensional%2520convolutional%2520neural%250Anetwork%252C%2520which%2520is%2520used%2520to%2520mine%2520the%2520local%2520correlation%2520between%2520data%2520and%2520obtain%250Ahigh-dimensional%2520data%2520features.%2520The%2520initial%2520load%2520fore-casting%2520module%2520consists%250Aof%2520a%2520self-attention%2520encoder-decoder%2520network%2520and%2520a%2520feedforward%2520neural%2520network%250A%2528FFN%2529.%2520The%2520module%2520utilizes%2520self-attention%2520mechanisms%2520to%2520encode%2520high-dimensional%250Afeatures.%2520This%2520operation%2520can%2520obtain%2520the%2520global%2520correlation%2520between%2520data.%250ATherefore%252C%2520the%2520model%2520is%2520able%2520to%2520retain%2520important%2520information%2520based%2520on%2520the%250Acoupling%2520relationship%2520between%2520the%2520data%2520in%2520data%2520mixed%2520with%2520non-time%2520series%250Afactors.%2520Then%252C%2520self-attention%2520decoding%2520is%2520per-formed%2520and%2520the%2520feedforward%2520neural%250Anetwork%2520is%2520used%2520to%2520regression%2520initial%2520load.%2520This%2520paper%2520introduces%2520the%2520residual%250Amechanism%2520to%2520build%2520the%2520load%2520optimization%2520module.%2520The%2520module%2520generates%2520residual%250Aload%2520values%2520to%2520optimize%2520the%2520initial%2520load.%2520The%2520simulation%2520results%2520show%2520that%2520the%250Aproposed%2520load%2520forecasting%2520method%2520has%2520advantages%2520in%2520terms%2520of%2520prediction%2520accuracy%250Aand%2520prediction%2520stability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.07140v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Short-term%20power%20load%20forecasting%20method%20based%20on%20CNN-SAEDN-Res&entry.906535625=Yang%20Cui%20and%20Han%20Zhu%20and%20Yijian%20Wang%20and%20Lu%20Zhang%20and%20Yang%20Li&entry.1292438233=%20%20In%20deep%20learning%2C%20the%20load%20data%20with%20non-temporal%20factors%20are%20difficult%20to%0Aprocess%20by%20sequence%20models.%20This%20problem%20results%20in%20insufficient%20precision%20of%0Athe%20prediction.%20Therefore%2C%20a%20short-term%20load%20forecasting%20method%20based%20on%0Aconvolutional%20neural%20network%20%28CNN%29%2C%20self-attention%20encoder-decoder%20network%0A%28SAEDN%29%20and%20residual-refinement%20%28Res%29%20is%20proposed.%20In%20this%20method%2C%20feature%0Aextraction%20module%20is%20composed%20of%20a%20two-dimensional%20convolutional%20neural%0Anetwork%2C%20which%20is%20used%20to%20mine%20the%20local%20correlation%20between%20data%20and%20obtain%0Ahigh-dimensional%20data%20features.%20The%20initial%20load%20fore-casting%20module%20consists%0Aof%20a%20self-attention%20encoder-decoder%20network%20and%20a%20feedforward%20neural%20network%0A%28FFN%29.%20The%20module%20utilizes%20self-attention%20mechanisms%20to%20encode%20high-dimensional%0Afeatures.%20This%20operation%20can%20obtain%20the%20global%20correlation%20between%20data.%0ATherefore%2C%20the%20model%20is%20able%20to%20retain%20important%20information%20based%20on%20the%0Acoupling%20relationship%20between%20the%20data%20in%20data%20mixed%20with%20non-time%20series%0Afactors.%20Then%2C%20self-attention%20decoding%20is%20per-formed%20and%20the%20feedforward%20neural%0Anetwork%20is%20used%20to%20regression%20initial%20load.%20This%20paper%20introduces%20the%20residual%0Amechanism%20to%20build%20the%20load%20optimization%20module.%20The%20module%20generates%20residual%0Aload%20values%20to%20optimize%20the%20initial%20load.%20The%20simulation%20results%20show%20that%20the%0Aproposed%20load%20forecasting%20method%20has%20advantages%20in%20terms%20of%20prediction%20accuracy%0Aand%20prediction%20stability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.07140v2&entry.124074799=Read"},
{"title": "Graph Laplacian-based Bayesian Multi-fidelity Modeling", "author": "Orazio Pinti and Jeremy M. Budd and Franca Hoffmann and Assad A. Oberai", "abstract": "  We present a novel probabilistic approach for generating multi-fidelity data\nwhile accounting for errors inherent in both low- and high-fidelity data. In\nthis approach a graph Laplacian constructed from the low-fidelity data is used\nto define a multivariate Gaussian prior density for the coordinates of the true\ndata points. In addition, few high-fidelity data points are used to construct a\nconjugate likelihood term. Thereafter, Bayes rule is applied to derive an\nexplicit expression for the posterior density which is also multivariate\nGaussian. The maximum \\textit{a posteriori} (MAP) estimate of this density is\nselected to be the optimal multi-fidelity estimate. It is shown that the MAP\nestimate and the covariance of the posterior density can be determined through\nthe solution of linear systems of equations. Thereafter, two methods, one based\non spectral truncation and another based on a low-rank approximation, are\ndeveloped to solve these equations efficiently. The multi-fidelity approach is\ntested on a variety of problems in solid and fluid mechanics with data that\nrepresents vectors of quantities of interest and discretized spatial fields in\none and two dimensions. The results demonstrate that by utilizing a small\nfraction of high-fidelity data, the multi-fidelity approach can significantly\nimprove the accuracy of a large collection of low-fidelity data points.\n", "link": "http://arxiv.org/abs/2409.08211v1", "date": "2024-09-12", "relevancy": 2.0323, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5504}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5053}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Laplacian-based%20Bayesian%20Multi-fidelity%20Modeling&body=Title%3A%20Graph%20Laplacian-based%20Bayesian%20Multi-fidelity%20Modeling%0AAuthor%3A%20Orazio%20Pinti%20and%20Jeremy%20M.%20Budd%20and%20Franca%20Hoffmann%20and%20Assad%20A.%20Oberai%0AAbstract%3A%20%20%20We%20present%20a%20novel%20probabilistic%20approach%20for%20generating%20multi-fidelity%20data%0Awhile%20accounting%20for%20errors%20inherent%20in%20both%20low-%20and%20high-fidelity%20data.%20In%0Athis%20approach%20a%20graph%20Laplacian%20constructed%20from%20the%20low-fidelity%20data%20is%20used%0Ato%20define%20a%20multivariate%20Gaussian%20prior%20density%20for%20the%20coordinates%20of%20the%20true%0Adata%20points.%20In%20addition%2C%20few%20high-fidelity%20data%20points%20are%20used%20to%20construct%20a%0Aconjugate%20likelihood%20term.%20Thereafter%2C%20Bayes%20rule%20is%20applied%20to%20derive%20an%0Aexplicit%20expression%20for%20the%20posterior%20density%20which%20is%20also%20multivariate%0AGaussian.%20The%20maximum%20%5Ctextit%7Ba%20posteriori%7D%20%28MAP%29%20estimate%20of%20this%20density%20is%0Aselected%20to%20be%20the%20optimal%20multi-fidelity%20estimate.%20It%20is%20shown%20that%20the%20MAP%0Aestimate%20and%20the%20covariance%20of%20the%20posterior%20density%20can%20be%20determined%20through%0Athe%20solution%20of%20linear%20systems%20of%20equations.%20Thereafter%2C%20two%20methods%2C%20one%20based%0Aon%20spectral%20truncation%20and%20another%20based%20on%20a%20low-rank%20approximation%2C%20are%0Adeveloped%20to%20solve%20these%20equations%20efficiently.%20The%20multi-fidelity%20approach%20is%0Atested%20on%20a%20variety%20of%20problems%20in%20solid%20and%20fluid%20mechanics%20with%20data%20that%0Arepresents%20vectors%20of%20quantities%20of%20interest%20and%20discretized%20spatial%20fields%20in%0Aone%20and%20two%20dimensions.%20The%20results%20demonstrate%20that%20by%20utilizing%20a%20small%0Afraction%20of%20high-fidelity%20data%2C%20the%20multi-fidelity%20approach%20can%20significantly%0Aimprove%20the%20accuracy%20of%20a%20large%20collection%20of%20low-fidelity%20data%20points.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08211v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Laplacian-based%2520Bayesian%2520Multi-fidelity%2520Modeling%26entry.906535625%3DOrazio%2520Pinti%2520and%2520Jeremy%2520M.%2520Budd%2520and%2520Franca%2520Hoffmann%2520and%2520Assad%2520A.%2520Oberai%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520probabilistic%2520approach%2520for%2520generating%2520multi-fidelity%2520data%250Awhile%2520accounting%2520for%2520errors%2520inherent%2520in%2520both%2520low-%2520and%2520high-fidelity%2520data.%2520In%250Athis%2520approach%2520a%2520graph%2520Laplacian%2520constructed%2520from%2520the%2520low-fidelity%2520data%2520is%2520used%250Ato%2520define%2520a%2520multivariate%2520Gaussian%2520prior%2520density%2520for%2520the%2520coordinates%2520of%2520the%2520true%250Adata%2520points.%2520In%2520addition%252C%2520few%2520high-fidelity%2520data%2520points%2520are%2520used%2520to%2520construct%2520a%250Aconjugate%2520likelihood%2520term.%2520Thereafter%252C%2520Bayes%2520rule%2520is%2520applied%2520to%2520derive%2520an%250Aexplicit%2520expression%2520for%2520the%2520posterior%2520density%2520which%2520is%2520also%2520multivariate%250AGaussian.%2520The%2520maximum%2520%255Ctextit%257Ba%2520posteriori%257D%2520%2528MAP%2529%2520estimate%2520of%2520this%2520density%2520is%250Aselected%2520to%2520be%2520the%2520optimal%2520multi-fidelity%2520estimate.%2520It%2520is%2520shown%2520that%2520the%2520MAP%250Aestimate%2520and%2520the%2520covariance%2520of%2520the%2520posterior%2520density%2520can%2520be%2520determined%2520through%250Athe%2520solution%2520of%2520linear%2520systems%2520of%2520equations.%2520Thereafter%252C%2520two%2520methods%252C%2520one%2520based%250Aon%2520spectral%2520truncation%2520and%2520another%2520based%2520on%2520a%2520low-rank%2520approximation%252C%2520are%250Adeveloped%2520to%2520solve%2520these%2520equations%2520efficiently.%2520The%2520multi-fidelity%2520approach%2520is%250Atested%2520on%2520a%2520variety%2520of%2520problems%2520in%2520solid%2520and%2520fluid%2520mechanics%2520with%2520data%2520that%250Arepresents%2520vectors%2520of%2520quantities%2520of%2520interest%2520and%2520discretized%2520spatial%2520fields%2520in%250Aone%2520and%2520two%2520dimensions.%2520The%2520results%2520demonstrate%2520that%2520by%2520utilizing%2520a%2520small%250Afraction%2520of%2520high-fidelity%2520data%252C%2520the%2520multi-fidelity%2520approach%2520can%2520significantly%250Aimprove%2520the%2520accuracy%2520of%2520a%2520large%2520collection%2520of%2520low-fidelity%2520data%2520points.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08211v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Laplacian-based%20Bayesian%20Multi-fidelity%20Modeling&entry.906535625=Orazio%20Pinti%20and%20Jeremy%20M.%20Budd%20and%20Franca%20Hoffmann%20and%20Assad%20A.%20Oberai&entry.1292438233=%20%20We%20present%20a%20novel%20probabilistic%20approach%20for%20generating%20multi-fidelity%20data%0Awhile%20accounting%20for%20errors%20inherent%20in%20both%20low-%20and%20high-fidelity%20data.%20In%0Athis%20approach%20a%20graph%20Laplacian%20constructed%20from%20the%20low-fidelity%20data%20is%20used%0Ato%20define%20a%20multivariate%20Gaussian%20prior%20density%20for%20the%20coordinates%20of%20the%20true%0Adata%20points.%20In%20addition%2C%20few%20high-fidelity%20data%20points%20are%20used%20to%20construct%20a%0Aconjugate%20likelihood%20term.%20Thereafter%2C%20Bayes%20rule%20is%20applied%20to%20derive%20an%0Aexplicit%20expression%20for%20the%20posterior%20density%20which%20is%20also%20multivariate%0AGaussian.%20The%20maximum%20%5Ctextit%7Ba%20posteriori%7D%20%28MAP%29%20estimate%20of%20this%20density%20is%0Aselected%20to%20be%20the%20optimal%20multi-fidelity%20estimate.%20It%20is%20shown%20that%20the%20MAP%0Aestimate%20and%20the%20covariance%20of%20the%20posterior%20density%20can%20be%20determined%20through%0Athe%20solution%20of%20linear%20systems%20of%20equations.%20Thereafter%2C%20two%20methods%2C%20one%20based%0Aon%20spectral%20truncation%20and%20another%20based%20on%20a%20low-rank%20approximation%2C%20are%0Adeveloped%20to%20solve%20these%20equations%20efficiently.%20The%20multi-fidelity%20approach%20is%0Atested%20on%20a%20variety%20of%20problems%20in%20solid%20and%20fluid%20mechanics%20with%20data%20that%0Arepresents%20vectors%20of%20quantities%20of%20interest%20and%20discretized%20spatial%20fields%20in%0Aone%20and%20two%20dimensions.%20The%20results%20demonstrate%20that%20by%20utilizing%20a%20small%0Afraction%20of%20high-fidelity%20data%2C%20the%20multi-fidelity%20approach%20can%20significantly%0Aimprove%20the%20accuracy%20of%20a%20large%20collection%20of%20low-fidelity%20data%20points.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08211v1&entry.124074799=Read"},
{"title": "OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable\n  Personal Question Answering", "author": "Jiahao Nick Li and Zhuohao Jerry Zhang and Jiaju Ma", "abstract": "  People often capture memories through photos, screenshots, and videos. While\nexisting AI-based tools enable querying this data using natural language, they\nmostly only support retrieving individual pieces of information like certain\nobjects in photos and struggle with answering more complex queries that involve\ninterpreting interconnected memories like event sequences. We conducted a\none-month diary study to collect realistic user queries and generated a\ntaxonomy of necessary contextual information for integrating with captured\nmemories. We then introduce OmniQuery, a novel system that is able to answer\ncomplex personal memory-related questions that require extracting and inferring\ncontextual information. OmniQuery augments single captured memories through\nintegrating scattered contextual information from multiple interconnected\nmemories, retrieves relevant memories, and uses a large language model (LLM) to\ncomprehensive answers. In human evaluations, we show the effectiveness of\nOmniQuery with an accuracy of 71.5%, and it outperformed a conventional RAG\nsystem, winning or tying in 74.5% of the time.\n", "link": "http://arxiv.org/abs/2409.08250v1", "date": "2024-09-12", "relevancy": 2.0296, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5102}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5102}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniQuery%3A%20Contextually%20Augmenting%20Captured%20Multimodal%20Memory%20to%20Enable%0A%20%20Personal%20Question%20Answering&body=Title%3A%20OmniQuery%3A%20Contextually%20Augmenting%20Captured%20Multimodal%20Memory%20to%20Enable%0A%20%20Personal%20Question%20Answering%0AAuthor%3A%20Jiahao%20Nick%20Li%20and%20Zhuohao%20Jerry%20Zhang%20and%20Jiaju%20Ma%0AAbstract%3A%20%20%20People%20often%20capture%20memories%20through%20photos%2C%20screenshots%2C%20and%20videos.%20While%0Aexisting%20AI-based%20tools%20enable%20querying%20this%20data%20using%20natural%20language%2C%20they%0Amostly%20only%20support%20retrieving%20individual%20pieces%20of%20information%20like%20certain%0Aobjects%20in%20photos%20and%20struggle%20with%20answering%20more%20complex%20queries%20that%20involve%0Ainterpreting%20interconnected%20memories%20like%20event%20sequences.%20We%20conducted%20a%0Aone-month%20diary%20study%20to%20collect%20realistic%20user%20queries%20and%20generated%20a%0Ataxonomy%20of%20necessary%20contextual%20information%20for%20integrating%20with%20captured%0Amemories.%20We%20then%20introduce%20OmniQuery%2C%20a%20novel%20system%20that%20is%20able%20to%20answer%0Acomplex%20personal%20memory-related%20questions%20that%20require%20extracting%20and%20inferring%0Acontextual%20information.%20OmniQuery%20augments%20single%20captured%20memories%20through%0Aintegrating%20scattered%20contextual%20information%20from%20multiple%20interconnected%0Amemories%2C%20retrieves%20relevant%20memories%2C%20and%20uses%20a%20large%20language%20model%20%28LLM%29%20to%0Acomprehensive%20answers.%20In%20human%20evaluations%2C%20we%20show%20the%20effectiveness%20of%0AOmniQuery%20with%20an%20accuracy%20of%2071.5%25%2C%20and%20it%20outperformed%20a%20conventional%20RAG%0Asystem%2C%20winning%20or%20tying%20in%2074.5%25%20of%20the%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08250v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniQuery%253A%2520Contextually%2520Augmenting%2520Captured%2520Multimodal%2520Memory%2520to%2520Enable%250A%2520%2520Personal%2520Question%2520Answering%26entry.906535625%3DJiahao%2520Nick%2520Li%2520and%2520Zhuohao%2520Jerry%2520Zhang%2520and%2520Jiaju%2520Ma%26entry.1292438233%3D%2520%2520People%2520often%2520capture%2520memories%2520through%2520photos%252C%2520screenshots%252C%2520and%2520videos.%2520While%250Aexisting%2520AI-based%2520tools%2520enable%2520querying%2520this%2520data%2520using%2520natural%2520language%252C%2520they%250Amostly%2520only%2520support%2520retrieving%2520individual%2520pieces%2520of%2520information%2520like%2520certain%250Aobjects%2520in%2520photos%2520and%2520struggle%2520with%2520answering%2520more%2520complex%2520queries%2520that%2520involve%250Ainterpreting%2520interconnected%2520memories%2520like%2520event%2520sequences.%2520We%2520conducted%2520a%250Aone-month%2520diary%2520study%2520to%2520collect%2520realistic%2520user%2520queries%2520and%2520generated%2520a%250Ataxonomy%2520of%2520necessary%2520contextual%2520information%2520for%2520integrating%2520with%2520captured%250Amemories.%2520We%2520then%2520introduce%2520OmniQuery%252C%2520a%2520novel%2520system%2520that%2520is%2520able%2520to%2520answer%250Acomplex%2520personal%2520memory-related%2520questions%2520that%2520require%2520extracting%2520and%2520inferring%250Acontextual%2520information.%2520OmniQuery%2520augments%2520single%2520captured%2520memories%2520through%250Aintegrating%2520scattered%2520contextual%2520information%2520from%2520multiple%2520interconnected%250Amemories%252C%2520retrieves%2520relevant%2520memories%252C%2520and%2520uses%2520a%2520large%2520language%2520model%2520%2528LLM%2529%2520to%250Acomprehensive%2520answers.%2520In%2520human%2520evaluations%252C%2520we%2520show%2520the%2520effectiveness%2520of%250AOmniQuery%2520with%2520an%2520accuracy%2520of%252071.5%2525%252C%2520and%2520it%2520outperformed%2520a%2520conventional%2520RAG%250Asystem%252C%2520winning%2520or%2520tying%2520in%252074.5%2525%2520of%2520the%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08250v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniQuery%3A%20Contextually%20Augmenting%20Captured%20Multimodal%20Memory%20to%20Enable%0A%20%20Personal%20Question%20Answering&entry.906535625=Jiahao%20Nick%20Li%20and%20Zhuohao%20Jerry%20Zhang%20and%20Jiaju%20Ma&entry.1292438233=%20%20People%20often%20capture%20memories%20through%20photos%2C%20screenshots%2C%20and%20videos.%20While%0Aexisting%20AI-based%20tools%20enable%20querying%20this%20data%20using%20natural%20language%2C%20they%0Amostly%20only%20support%20retrieving%20individual%20pieces%20of%20information%20like%20certain%0Aobjects%20in%20photos%20and%20struggle%20with%20answering%20more%20complex%20queries%20that%20involve%0Ainterpreting%20interconnected%20memories%20like%20event%20sequences.%20We%20conducted%20a%0Aone-month%20diary%20study%20to%20collect%20realistic%20user%20queries%20and%20generated%20a%0Ataxonomy%20of%20necessary%20contextual%20information%20for%20integrating%20with%20captured%0Amemories.%20We%20then%20introduce%20OmniQuery%2C%20a%20novel%20system%20that%20is%20able%20to%20answer%0Acomplex%20personal%20memory-related%20questions%20that%20require%20extracting%20and%20inferring%0Acontextual%20information.%20OmniQuery%20augments%20single%20captured%20memories%20through%0Aintegrating%20scattered%20contextual%20information%20from%20multiple%20interconnected%0Amemories%2C%20retrieves%20relevant%20memories%2C%20and%20uses%20a%20large%20language%20model%20%28LLM%29%20to%0Acomprehensive%20answers.%20In%20human%20evaluations%2C%20we%20show%20the%20effectiveness%20of%0AOmniQuery%20with%20an%20accuracy%20of%2071.5%25%2C%20and%20it%20outperformed%20a%20conventional%20RAG%0Asystem%2C%20winning%20or%20tying%20in%2074.5%25%20of%20the%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08250v1&entry.124074799=Read"},
{"title": "NITRO-D: Native Integer-only Training of Deep Convolutional Neural\n  Networks", "author": "Alberto Pirillo and Luca Colombo and Manuel Roveri", "abstract": "  Quantization has become increasingly pivotal in addressing the steadily\nincreasing computational and memory requirements of Deep Neural Networks\n(DNNs). By reducing the number of bits used to represent weights and\nactivations (typically from 32-bit floating-point to 16-bit or 8-bit integers),\nquantization reduces the memory footprint, energy consumption, and execution\ntime of DNN models. However, traditional quantization methods typically focus\non the inference of DNNs, while the training process still relies on\nfloating-point operations. To date, only one work in the literature has\naddressed integer-only training for Multi-Layer Perceptron (MLP) architectures.\nThis work introduces NITRO-D, a new framework for training arbitrarily deep\ninteger-only Convolutional Neural Networks (CNNs) that operate entirely in the\ninteger-only domain for both training and inference. NITRO-D is the first\nframework in the literature enabling the training of integer-only CNNs without\nthe need to introduce a quantization scheme. Specifically, NITRO-D introduces a\nnovel architecture integrating multiple integer local-loss blocks, which\ninclude the proposed NITRO Scaling Layer and the NITRO-ReLU activation\nfunction. Additionally, it introduces a novel integer-only learning algorithm\nderived from Local Error Signals (LES), utilizing IntegerSGD, an optimizer\nspecifically designed to operate in an integer-only context. NITRO-D is\nimplemented in an open-source Python library. Extensive experimental\nevaluations demonstrate its effectiveness across several state-of-the-art image\nrecognition datasets. Results show significant performance improvements from\n2.47% to 5.96% for integer-only MLP architectures over the state-of-the-art\nsolution, and the capability of training integer-only CNN architectures with\nminimal accuracy degradation from -0.15% to -4.22% compared to floating-point\nLES.\n", "link": "http://arxiv.org/abs/2407.11698v2", "date": "2024-09-12", "relevancy": 2.0199, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5265}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5104}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4909}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NITRO-D%3A%20Native%20Integer-only%20Training%20of%20Deep%20Convolutional%20Neural%0A%20%20Networks&body=Title%3A%20NITRO-D%3A%20Native%20Integer-only%20Training%20of%20Deep%20Convolutional%20Neural%0A%20%20Networks%0AAuthor%3A%20Alberto%20Pirillo%20and%20Luca%20Colombo%20and%20Manuel%20Roveri%0AAbstract%3A%20%20%20Quantization%20has%20become%20increasingly%20pivotal%20in%20addressing%20the%20steadily%0Aincreasing%20computational%20and%20memory%20requirements%20of%20Deep%20Neural%20Networks%0A%28DNNs%29.%20By%20reducing%20the%20number%20of%20bits%20used%20to%20represent%20weights%20and%0Aactivations%20%28typically%20from%2032-bit%20floating-point%20to%2016-bit%20or%208-bit%20integers%29%2C%0Aquantization%20reduces%20the%20memory%20footprint%2C%20energy%20consumption%2C%20and%20execution%0Atime%20of%20DNN%20models.%20However%2C%20traditional%20quantization%20methods%20typically%20focus%0Aon%20the%20inference%20of%20DNNs%2C%20while%20the%20training%20process%20still%20relies%20on%0Afloating-point%20operations.%20To%20date%2C%20only%20one%20work%20in%20the%20literature%20has%0Aaddressed%20integer-only%20training%20for%20Multi-Layer%20Perceptron%20%28MLP%29%20architectures.%0AThis%20work%20introduces%20NITRO-D%2C%20a%20new%20framework%20for%20training%20arbitrarily%20deep%0Ainteger-only%20Convolutional%20Neural%20Networks%20%28CNNs%29%20that%20operate%20entirely%20in%20the%0Ainteger-only%20domain%20for%20both%20training%20and%20inference.%20NITRO-D%20is%20the%20first%0Aframework%20in%20the%20literature%20enabling%20the%20training%20of%20integer-only%20CNNs%20without%0Athe%20need%20to%20introduce%20a%20quantization%20scheme.%20Specifically%2C%20NITRO-D%20introduces%20a%0Anovel%20architecture%20integrating%20multiple%20integer%20local-loss%20blocks%2C%20which%0Ainclude%20the%20proposed%20NITRO%20Scaling%20Layer%20and%20the%20NITRO-ReLU%20activation%0Afunction.%20Additionally%2C%20it%20introduces%20a%20novel%20integer-only%20learning%20algorithm%0Aderived%20from%20Local%20Error%20Signals%20%28LES%29%2C%20utilizing%20IntegerSGD%2C%20an%20optimizer%0Aspecifically%20designed%20to%20operate%20in%20an%20integer-only%20context.%20NITRO-D%20is%0Aimplemented%20in%20an%20open-source%20Python%20library.%20Extensive%20experimental%0Aevaluations%20demonstrate%20its%20effectiveness%20across%20several%20state-of-the-art%20image%0Arecognition%20datasets.%20Results%20show%20significant%20performance%20improvements%20from%0A2.47%25%20to%205.96%25%20for%20integer-only%20MLP%20architectures%20over%20the%20state-of-the-art%0Asolution%2C%20and%20the%20capability%20of%20training%20integer-only%20CNN%20architectures%20with%0Aminimal%20accuracy%20degradation%20from%20-0.15%25%20to%20-4.22%25%20compared%20to%20floating-point%0ALES.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11698v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNITRO-D%253A%2520Native%2520Integer-only%2520Training%2520of%2520Deep%2520Convolutional%2520Neural%250A%2520%2520Networks%26entry.906535625%3DAlberto%2520Pirillo%2520and%2520Luca%2520Colombo%2520and%2520Manuel%2520Roveri%26entry.1292438233%3D%2520%2520Quantization%2520has%2520become%2520increasingly%2520pivotal%2520in%2520addressing%2520the%2520steadily%250Aincreasing%2520computational%2520and%2520memory%2520requirements%2520of%2520Deep%2520Neural%2520Networks%250A%2528DNNs%2529.%2520By%2520reducing%2520the%2520number%2520of%2520bits%2520used%2520to%2520represent%2520weights%2520and%250Aactivations%2520%2528typically%2520from%252032-bit%2520floating-point%2520to%252016-bit%2520or%25208-bit%2520integers%2529%252C%250Aquantization%2520reduces%2520the%2520memory%2520footprint%252C%2520energy%2520consumption%252C%2520and%2520execution%250Atime%2520of%2520DNN%2520models.%2520However%252C%2520traditional%2520quantization%2520methods%2520typically%2520focus%250Aon%2520the%2520inference%2520of%2520DNNs%252C%2520while%2520the%2520training%2520process%2520still%2520relies%2520on%250Afloating-point%2520operations.%2520To%2520date%252C%2520only%2520one%2520work%2520in%2520the%2520literature%2520has%250Aaddressed%2520integer-only%2520training%2520for%2520Multi-Layer%2520Perceptron%2520%2528MLP%2529%2520architectures.%250AThis%2520work%2520introduces%2520NITRO-D%252C%2520a%2520new%2520framework%2520for%2520training%2520arbitrarily%2520deep%250Ainteger-only%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520that%2520operate%2520entirely%2520in%2520the%250Ainteger-only%2520domain%2520for%2520both%2520training%2520and%2520inference.%2520NITRO-D%2520is%2520the%2520first%250Aframework%2520in%2520the%2520literature%2520enabling%2520the%2520training%2520of%2520integer-only%2520CNNs%2520without%250Athe%2520need%2520to%2520introduce%2520a%2520quantization%2520scheme.%2520Specifically%252C%2520NITRO-D%2520introduces%2520a%250Anovel%2520architecture%2520integrating%2520multiple%2520integer%2520local-loss%2520blocks%252C%2520which%250Ainclude%2520the%2520proposed%2520NITRO%2520Scaling%2520Layer%2520and%2520the%2520NITRO-ReLU%2520activation%250Afunction.%2520Additionally%252C%2520it%2520introduces%2520a%2520novel%2520integer-only%2520learning%2520algorithm%250Aderived%2520from%2520Local%2520Error%2520Signals%2520%2528LES%2529%252C%2520utilizing%2520IntegerSGD%252C%2520an%2520optimizer%250Aspecifically%2520designed%2520to%2520operate%2520in%2520an%2520integer-only%2520context.%2520NITRO-D%2520is%250Aimplemented%2520in%2520an%2520open-source%2520Python%2520library.%2520Extensive%2520experimental%250Aevaluations%2520demonstrate%2520its%2520effectiveness%2520across%2520several%2520state-of-the-art%2520image%250Arecognition%2520datasets.%2520Results%2520show%2520significant%2520performance%2520improvements%2520from%250A2.47%2525%2520to%25205.96%2525%2520for%2520integer-only%2520MLP%2520architectures%2520over%2520the%2520state-of-the-art%250Asolution%252C%2520and%2520the%2520capability%2520of%2520training%2520integer-only%2520CNN%2520architectures%2520with%250Aminimal%2520accuracy%2520degradation%2520from%2520-0.15%2525%2520to%2520-4.22%2525%2520compared%2520to%2520floating-point%250ALES.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11698v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NITRO-D%3A%20Native%20Integer-only%20Training%20of%20Deep%20Convolutional%20Neural%0A%20%20Networks&entry.906535625=Alberto%20Pirillo%20and%20Luca%20Colombo%20and%20Manuel%20Roveri&entry.1292438233=%20%20Quantization%20has%20become%20increasingly%20pivotal%20in%20addressing%20the%20steadily%0Aincreasing%20computational%20and%20memory%20requirements%20of%20Deep%20Neural%20Networks%0A%28DNNs%29.%20By%20reducing%20the%20number%20of%20bits%20used%20to%20represent%20weights%20and%0Aactivations%20%28typically%20from%2032-bit%20floating-point%20to%2016-bit%20or%208-bit%20integers%29%2C%0Aquantization%20reduces%20the%20memory%20footprint%2C%20energy%20consumption%2C%20and%20execution%0Atime%20of%20DNN%20models.%20However%2C%20traditional%20quantization%20methods%20typically%20focus%0Aon%20the%20inference%20of%20DNNs%2C%20while%20the%20training%20process%20still%20relies%20on%0Afloating-point%20operations.%20To%20date%2C%20only%20one%20work%20in%20the%20literature%20has%0Aaddressed%20integer-only%20training%20for%20Multi-Layer%20Perceptron%20%28MLP%29%20architectures.%0AThis%20work%20introduces%20NITRO-D%2C%20a%20new%20framework%20for%20training%20arbitrarily%20deep%0Ainteger-only%20Convolutional%20Neural%20Networks%20%28CNNs%29%20that%20operate%20entirely%20in%20the%0Ainteger-only%20domain%20for%20both%20training%20and%20inference.%20NITRO-D%20is%20the%20first%0Aframework%20in%20the%20literature%20enabling%20the%20training%20of%20integer-only%20CNNs%20without%0Athe%20need%20to%20introduce%20a%20quantization%20scheme.%20Specifically%2C%20NITRO-D%20introduces%20a%0Anovel%20architecture%20integrating%20multiple%20integer%20local-loss%20blocks%2C%20which%0Ainclude%20the%20proposed%20NITRO%20Scaling%20Layer%20and%20the%20NITRO-ReLU%20activation%0Afunction.%20Additionally%2C%20it%20introduces%20a%20novel%20integer-only%20learning%20algorithm%0Aderived%20from%20Local%20Error%20Signals%20%28LES%29%2C%20utilizing%20IntegerSGD%2C%20an%20optimizer%0Aspecifically%20designed%20to%20operate%20in%20an%20integer-only%20context.%20NITRO-D%20is%0Aimplemented%20in%20an%20open-source%20Python%20library.%20Extensive%20experimental%0Aevaluations%20demonstrate%20its%20effectiveness%20across%20several%20state-of-the-art%20image%0Arecognition%20datasets.%20Results%20show%20significant%20performance%20improvements%20from%0A2.47%25%20to%205.96%25%20for%20integer-only%20MLP%20architectures%20over%20the%20state-of-the-art%0Asolution%2C%20and%20the%20capability%20of%20training%20integer-only%20CNN%20architectures%20with%0Aminimal%20accuracy%20degradation%20from%20-0.15%25%20to%20-4.22%25%20compared%20to%20floating-point%0ALES.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11698v2&entry.124074799=Read"},
{"title": "Sparse R-CNN OBB: Ship Target Detection in SAR Images Based on Oriented\n  Sparse Proposals", "author": "Kamirul Kamirul and Odysseas Pappas and Alin Achim", "abstract": "  We present Sparse R-CNN OBB, a novel framework for the detection of oriented\nobjects in SAR images leveraging sparse learnable proposals. The Sparse R-CNN\nOBB has streamlined architecture and ease of training as it utilizes a sparse\nset of 300 proposals instead of training a proposals generator on hundreds of\nthousands of anchors. To the best of our knowledge, Sparse R-CNN OBB is the\nfirst to adopt the concept of sparse learnable proposals for the detection of\noriented objects, as well as for the detection of ships in Synthetic Aperture\nRadar (SAR) images. The detection head of the baseline model, Sparse R-CNN, is\nre-designed to enable the model to capture object orientation. We also\nfine-tune the model on RSDD-SAR dataset and provide a performance comparison to\nstate-of-the-art models. Experimental results shows that Sparse R-CNN OBB\nachieves outstanding performance, surpassing other models on both inshore and\noffshore scenarios. The code is available at:\nwww.github.com/ka-mirul/Sparse-R-CNN-OBB.\n", "link": "http://arxiv.org/abs/2409.07973v1", "date": "2024-09-12", "relevancy": 2.0068, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5101}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4972}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4919}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20R-CNN%20OBB%3A%20Ship%20Target%20Detection%20in%20SAR%20Images%20Based%20on%20Oriented%0A%20%20Sparse%20Proposals&body=Title%3A%20Sparse%20R-CNN%20OBB%3A%20Ship%20Target%20Detection%20in%20SAR%20Images%20Based%20on%20Oriented%0A%20%20Sparse%20Proposals%0AAuthor%3A%20Kamirul%20Kamirul%20and%20Odysseas%20Pappas%20and%20Alin%20Achim%0AAbstract%3A%20%20%20We%20present%20Sparse%20R-CNN%20OBB%2C%20a%20novel%20framework%20for%20the%20detection%20of%20oriented%0Aobjects%20in%20SAR%20images%20leveraging%20sparse%20learnable%20proposals.%20The%20Sparse%20R-CNN%0AOBB%20has%20streamlined%20architecture%20and%20ease%20of%20training%20as%20it%20utilizes%20a%20sparse%0Aset%20of%20300%20proposals%20instead%20of%20training%20a%20proposals%20generator%20on%20hundreds%20of%0Athousands%20of%20anchors.%20To%20the%20best%20of%20our%20knowledge%2C%20Sparse%20R-CNN%20OBB%20is%20the%0Afirst%20to%20adopt%20the%20concept%20of%20sparse%20learnable%20proposals%20for%20the%20detection%20of%0Aoriented%20objects%2C%20as%20well%20as%20for%20the%20detection%20of%20ships%20in%20Synthetic%20Aperture%0ARadar%20%28SAR%29%20images.%20The%20detection%20head%20of%20the%20baseline%20model%2C%20Sparse%20R-CNN%2C%20is%0Are-designed%20to%20enable%20the%20model%20to%20capture%20object%20orientation.%20We%20also%0Afine-tune%20the%20model%20on%20RSDD-SAR%20dataset%20and%20provide%20a%20performance%20comparison%20to%0Astate-of-the-art%20models.%20Experimental%20results%20shows%20that%20Sparse%20R-CNN%20OBB%0Aachieves%20outstanding%20performance%2C%20surpassing%20other%20models%20on%20both%20inshore%20and%0Aoffshore%20scenarios.%20The%20code%20is%20available%20at%3A%0Awww.github.com/ka-mirul/Sparse-R-CNN-OBB.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07973v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520R-CNN%2520OBB%253A%2520Ship%2520Target%2520Detection%2520in%2520SAR%2520Images%2520Based%2520on%2520Oriented%250A%2520%2520Sparse%2520Proposals%26entry.906535625%3DKamirul%2520Kamirul%2520and%2520Odysseas%2520Pappas%2520and%2520Alin%2520Achim%26entry.1292438233%3D%2520%2520We%2520present%2520Sparse%2520R-CNN%2520OBB%252C%2520a%2520novel%2520framework%2520for%2520the%2520detection%2520of%2520oriented%250Aobjects%2520in%2520SAR%2520images%2520leveraging%2520sparse%2520learnable%2520proposals.%2520The%2520Sparse%2520R-CNN%250AOBB%2520has%2520streamlined%2520architecture%2520and%2520ease%2520of%2520training%2520as%2520it%2520utilizes%2520a%2520sparse%250Aset%2520of%2520300%2520proposals%2520instead%2520of%2520training%2520a%2520proposals%2520generator%2520on%2520hundreds%2520of%250Athousands%2520of%2520anchors.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520Sparse%2520R-CNN%2520OBB%2520is%2520the%250Afirst%2520to%2520adopt%2520the%2520concept%2520of%2520sparse%2520learnable%2520proposals%2520for%2520the%2520detection%2520of%250Aoriented%2520objects%252C%2520as%2520well%2520as%2520for%2520the%2520detection%2520of%2520ships%2520in%2520Synthetic%2520Aperture%250ARadar%2520%2528SAR%2529%2520images.%2520The%2520detection%2520head%2520of%2520the%2520baseline%2520model%252C%2520Sparse%2520R-CNN%252C%2520is%250Are-designed%2520to%2520enable%2520the%2520model%2520to%2520capture%2520object%2520orientation.%2520We%2520also%250Afine-tune%2520the%2520model%2520on%2520RSDD-SAR%2520dataset%2520and%2520provide%2520a%2520performance%2520comparison%2520to%250Astate-of-the-art%2520models.%2520Experimental%2520results%2520shows%2520that%2520Sparse%2520R-CNN%2520OBB%250Aachieves%2520outstanding%2520performance%252C%2520surpassing%2520other%2520models%2520on%2520both%2520inshore%2520and%250Aoffshore%2520scenarios.%2520The%2520code%2520is%2520available%2520at%253A%250Awww.github.com/ka-mirul/Sparse-R-CNN-OBB.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07973v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20R-CNN%20OBB%3A%20Ship%20Target%20Detection%20in%20SAR%20Images%20Based%20on%20Oriented%0A%20%20Sparse%20Proposals&entry.906535625=Kamirul%20Kamirul%20and%20Odysseas%20Pappas%20and%20Alin%20Achim&entry.1292438233=%20%20We%20present%20Sparse%20R-CNN%20OBB%2C%20a%20novel%20framework%20for%20the%20detection%20of%20oriented%0Aobjects%20in%20SAR%20images%20leveraging%20sparse%20learnable%20proposals.%20The%20Sparse%20R-CNN%0AOBB%20has%20streamlined%20architecture%20and%20ease%20of%20training%20as%20it%20utilizes%20a%20sparse%0Aset%20of%20300%20proposals%20instead%20of%20training%20a%20proposals%20generator%20on%20hundreds%20of%0Athousands%20of%20anchors.%20To%20the%20best%20of%20our%20knowledge%2C%20Sparse%20R-CNN%20OBB%20is%20the%0Afirst%20to%20adopt%20the%20concept%20of%20sparse%20learnable%20proposals%20for%20the%20detection%20of%0Aoriented%20objects%2C%20as%20well%20as%20for%20the%20detection%20of%20ships%20in%20Synthetic%20Aperture%0ARadar%20%28SAR%29%20images.%20The%20detection%20head%20of%20the%20baseline%20model%2C%20Sparse%20R-CNN%2C%20is%0Are-designed%20to%20enable%20the%20model%20to%20capture%20object%20orientation.%20We%20also%0Afine-tune%20the%20model%20on%20RSDD-SAR%20dataset%20and%20provide%20a%20performance%20comparison%20to%0Astate-of-the-art%20models.%20Experimental%20results%20shows%20that%20Sparse%20R-CNN%20OBB%0Aachieves%20outstanding%20performance%2C%20surpassing%20other%20models%20on%20both%20inshore%20and%0Aoffshore%20scenarios.%20The%20code%20is%20available%20at%3A%0Awww.github.com/ka-mirul/Sparse-R-CNN-OBB.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07973v1&entry.124074799=Read"},
{"title": "Zero-Shot Whole Slide Image Retrieval in Histopathology Using Embeddings\n  of Foundation Models", "author": "Saghir Alfasly and Ghazal Alabtah and Sobhan Hemati and Krishna Rani Kalari and H. R. Tizhoosh", "abstract": "  We have tested recently published foundation models for histopathology for\nimage retrieval. We report macro average of F1 score for top-1 retrieval,\nmajority of top-3 retrievals, and majority of top-5 retrievals. We perform\nzero-shot retrievals, i.e., we do not alter embeddings and we do not train any\nclassifier. As test data, we used diagnostic slides of TCGA, The Cancer Genome\nAtlas, consisting of 23 organs and 117 cancer subtypes. As a search platform we\nused Yottixel that enabled us to perform WSI search using patches. Achieved F1\nscores show low performance, e.g., for top-5 retrievals, 27% +/- 13%\n(Yottixel-DenseNet), 42% +/- 14% (Yottixel-UNI), 40%+/-13% (Yottixel-Virchow),\n41%+/-13% (Yottixel-GigaPath), and 41%+/-14% (GigaPath WSI).\n", "link": "http://arxiv.org/abs/2409.04631v2", "date": "2024-09-12", "relevancy": 1.9962, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4991}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4991}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4985}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Whole%20Slide%20Image%20Retrieval%20in%20Histopathology%20Using%20Embeddings%0A%20%20of%20Foundation%20Models&body=Title%3A%20Zero-Shot%20Whole%20Slide%20Image%20Retrieval%20in%20Histopathology%20Using%20Embeddings%0A%20%20of%20Foundation%20Models%0AAuthor%3A%20Saghir%20Alfasly%20and%20Ghazal%20Alabtah%20and%20Sobhan%20Hemati%20and%20Krishna%20Rani%20Kalari%20and%20H.%20R.%20Tizhoosh%0AAbstract%3A%20%20%20We%20have%20tested%20recently%20published%20foundation%20models%20for%20histopathology%20for%0Aimage%20retrieval.%20We%20report%20macro%20average%20of%20F1%20score%20for%20top-1%20retrieval%2C%0Amajority%20of%20top-3%20retrievals%2C%20and%20majority%20of%20top-5%20retrievals.%20We%20perform%0Azero-shot%20retrievals%2C%20i.e.%2C%20we%20do%20not%20alter%20embeddings%20and%20we%20do%20not%20train%20any%0Aclassifier.%20As%20test%20data%2C%20we%20used%20diagnostic%20slides%20of%20TCGA%2C%20The%20Cancer%20Genome%0AAtlas%2C%20consisting%20of%2023%20organs%20and%20117%20cancer%20subtypes.%20As%20a%20search%20platform%20we%0Aused%20Yottixel%20that%20enabled%20us%20to%20perform%20WSI%20search%20using%20patches.%20Achieved%20F1%0Ascores%20show%20low%20performance%2C%20e.g.%2C%20for%20top-5%20retrievals%2C%2027%25%20%2B/-%2013%25%0A%28Yottixel-DenseNet%29%2C%2042%25%20%2B/-%2014%25%20%28Yottixel-UNI%29%2C%2040%25%2B/-13%25%20%28Yottixel-Virchow%29%2C%0A41%25%2B/-13%25%20%28Yottixel-GigaPath%29%2C%20and%2041%25%2B/-14%25%20%28GigaPath%20WSI%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04631v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Whole%2520Slide%2520Image%2520Retrieval%2520in%2520Histopathology%2520Using%2520Embeddings%250A%2520%2520of%2520Foundation%2520Models%26entry.906535625%3DSaghir%2520Alfasly%2520and%2520Ghazal%2520Alabtah%2520and%2520Sobhan%2520Hemati%2520and%2520Krishna%2520Rani%2520Kalari%2520and%2520H.%2520R.%2520Tizhoosh%26entry.1292438233%3D%2520%2520We%2520have%2520tested%2520recently%2520published%2520foundation%2520models%2520for%2520histopathology%2520for%250Aimage%2520retrieval.%2520We%2520report%2520macro%2520average%2520of%2520F1%2520score%2520for%2520top-1%2520retrieval%252C%250Amajority%2520of%2520top-3%2520retrievals%252C%2520and%2520majority%2520of%2520top-5%2520retrievals.%2520We%2520perform%250Azero-shot%2520retrievals%252C%2520i.e.%252C%2520we%2520do%2520not%2520alter%2520embeddings%2520and%2520we%2520do%2520not%2520train%2520any%250Aclassifier.%2520As%2520test%2520data%252C%2520we%2520used%2520diagnostic%2520slides%2520of%2520TCGA%252C%2520The%2520Cancer%2520Genome%250AAtlas%252C%2520consisting%2520of%252023%2520organs%2520and%2520117%2520cancer%2520subtypes.%2520As%2520a%2520search%2520platform%2520we%250Aused%2520Yottixel%2520that%2520enabled%2520us%2520to%2520perform%2520WSI%2520search%2520using%2520patches.%2520Achieved%2520F1%250Ascores%2520show%2520low%2520performance%252C%2520e.g.%252C%2520for%2520top-5%2520retrievals%252C%252027%2525%2520%252B/-%252013%2525%250A%2528Yottixel-DenseNet%2529%252C%252042%2525%2520%252B/-%252014%2525%2520%2528Yottixel-UNI%2529%252C%252040%2525%252B/-13%2525%2520%2528Yottixel-Virchow%2529%252C%250A41%2525%252B/-13%2525%2520%2528Yottixel-GigaPath%2529%252C%2520and%252041%2525%252B/-14%2525%2520%2528GigaPath%2520WSI%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04631v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Whole%20Slide%20Image%20Retrieval%20in%20Histopathology%20Using%20Embeddings%0A%20%20of%20Foundation%20Models&entry.906535625=Saghir%20Alfasly%20and%20Ghazal%20Alabtah%20and%20Sobhan%20Hemati%20and%20Krishna%20Rani%20Kalari%20and%20H.%20R.%20Tizhoosh&entry.1292438233=%20%20We%20have%20tested%20recently%20published%20foundation%20models%20for%20histopathology%20for%0Aimage%20retrieval.%20We%20report%20macro%20average%20of%20F1%20score%20for%20top-1%20retrieval%2C%0Amajority%20of%20top-3%20retrievals%2C%20and%20majority%20of%20top-5%20retrievals.%20We%20perform%0Azero-shot%20retrievals%2C%20i.e.%2C%20we%20do%20not%20alter%20embeddings%20and%20we%20do%20not%20train%20any%0Aclassifier.%20As%20test%20data%2C%20we%20used%20diagnostic%20slides%20of%20TCGA%2C%20The%20Cancer%20Genome%0AAtlas%2C%20consisting%20of%2023%20organs%20and%20117%20cancer%20subtypes.%20As%20a%20search%20platform%20we%0Aused%20Yottixel%20that%20enabled%20us%20to%20perform%20WSI%20search%20using%20patches.%20Achieved%20F1%0Ascores%20show%20low%20performance%2C%20e.g.%2C%20for%20top-5%20retrievals%2C%2027%25%20%2B/-%2013%25%0A%28Yottixel-DenseNet%29%2C%2042%25%20%2B/-%2014%25%20%28Yottixel-UNI%29%2C%2040%25%2B/-13%25%20%28Yottixel-Virchow%29%2C%0A41%25%2B/-13%25%20%28Yottixel-GigaPath%29%2C%20and%2041%25%2B/-14%25%20%28GigaPath%20WSI%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04631v2&entry.124074799=Read"},
{"title": "Tidal MerzA: Combining affective modelling and autonomous code\n  generation through Reinforcement Learning", "author": "Elizabeth Wilson and Gy\u00f6rgy Fazekas and Geraint Wiggins", "abstract": "  This paper presents Tidal-MerzA, a novel system designed for collaborative\nperformances between humans and a machine agent in the context of live coding,\nspecifically focusing on the generation of musical patterns. Tidal-MerzA fuses\ntwo foundational models: ALCAA (Affective Live Coding Autonomous Agent) and\nTidal Fuzz, a computational framework. By integrating affective modelling with\ncomputational generation, this system leverages reinforcement learning\ntechniques to dynamically adapt music composition parameters within the\nTidalCycles framework, ensuring both affective qualities to the patterns and\nsyntactical correctness. The development of Tidal-MerzA introduces two distinct\nagents: one focusing on the generation of mini-notation strings for musical\nexpression, and another on the alignment of music with targeted affective\nstates through reinforcement learning. This approach enhances the adaptability\nand creative potential of live coding practices and allows exploration of\nhuman-machine creative interactions. Tidal-MerzA advances the field of\ncomputational music generation, presenting a novel methodology for\nincorporating artificial intelligence into artistic practices.\n", "link": "http://arxiv.org/abs/2409.07918v1", "date": "2024-09-12", "relevancy": 1.9947, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5133}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4929}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tidal%20MerzA%3A%20Combining%20affective%20modelling%20and%20autonomous%20code%0A%20%20generation%20through%20Reinforcement%20Learning&body=Title%3A%20Tidal%20MerzA%3A%20Combining%20affective%20modelling%20and%20autonomous%20code%0A%20%20generation%20through%20Reinforcement%20Learning%0AAuthor%3A%20Elizabeth%20Wilson%20and%20Gy%C3%B6rgy%20Fazekas%20and%20Geraint%20Wiggins%0AAbstract%3A%20%20%20This%20paper%20presents%20Tidal-MerzA%2C%20a%20novel%20system%20designed%20for%20collaborative%0Aperformances%20between%20humans%20and%20a%20machine%20agent%20in%20the%20context%20of%20live%20coding%2C%0Aspecifically%20focusing%20on%20the%20generation%20of%20musical%20patterns.%20Tidal-MerzA%20fuses%0Atwo%20foundational%20models%3A%20ALCAA%20%28Affective%20Live%20Coding%20Autonomous%20Agent%29%20and%0ATidal%20Fuzz%2C%20a%20computational%20framework.%20By%20integrating%20affective%20modelling%20with%0Acomputational%20generation%2C%20this%20system%20leverages%20reinforcement%20learning%0Atechniques%20to%20dynamically%20adapt%20music%20composition%20parameters%20within%20the%0ATidalCycles%20framework%2C%20ensuring%20both%20affective%20qualities%20to%20the%20patterns%20and%0Asyntactical%20correctness.%20The%20development%20of%20Tidal-MerzA%20introduces%20two%20distinct%0Aagents%3A%20one%20focusing%20on%20the%20generation%20of%20mini-notation%20strings%20for%20musical%0Aexpression%2C%20and%20another%20on%20the%20alignment%20of%20music%20with%20targeted%20affective%0Astates%20through%20reinforcement%20learning.%20This%20approach%20enhances%20the%20adaptability%0Aand%20creative%20potential%20of%20live%20coding%20practices%20and%20allows%20exploration%20of%0Ahuman-machine%20creative%20interactions.%20Tidal-MerzA%20advances%20the%20field%20of%0Acomputational%20music%20generation%2C%20presenting%20a%20novel%20methodology%20for%0Aincorporating%20artificial%20intelligence%20into%20artistic%20practices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07918v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTidal%2520MerzA%253A%2520Combining%2520affective%2520modelling%2520and%2520autonomous%2520code%250A%2520%2520generation%2520through%2520Reinforcement%2520Learning%26entry.906535625%3DElizabeth%2520Wilson%2520and%2520Gy%25C3%25B6rgy%2520Fazekas%2520and%2520Geraint%2520Wiggins%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520Tidal-MerzA%252C%2520a%2520novel%2520system%2520designed%2520for%2520collaborative%250Aperformances%2520between%2520humans%2520and%2520a%2520machine%2520agent%2520in%2520the%2520context%2520of%2520live%2520coding%252C%250Aspecifically%2520focusing%2520on%2520the%2520generation%2520of%2520musical%2520patterns.%2520Tidal-MerzA%2520fuses%250Atwo%2520foundational%2520models%253A%2520ALCAA%2520%2528Affective%2520Live%2520Coding%2520Autonomous%2520Agent%2529%2520and%250ATidal%2520Fuzz%252C%2520a%2520computational%2520framework.%2520By%2520integrating%2520affective%2520modelling%2520with%250Acomputational%2520generation%252C%2520this%2520system%2520leverages%2520reinforcement%2520learning%250Atechniques%2520to%2520dynamically%2520adapt%2520music%2520composition%2520parameters%2520within%2520the%250ATidalCycles%2520framework%252C%2520ensuring%2520both%2520affective%2520qualities%2520to%2520the%2520patterns%2520and%250Asyntactical%2520correctness.%2520The%2520development%2520of%2520Tidal-MerzA%2520introduces%2520two%2520distinct%250Aagents%253A%2520one%2520focusing%2520on%2520the%2520generation%2520of%2520mini-notation%2520strings%2520for%2520musical%250Aexpression%252C%2520and%2520another%2520on%2520the%2520alignment%2520of%2520music%2520with%2520targeted%2520affective%250Astates%2520through%2520reinforcement%2520learning.%2520This%2520approach%2520enhances%2520the%2520adaptability%250Aand%2520creative%2520potential%2520of%2520live%2520coding%2520practices%2520and%2520allows%2520exploration%2520of%250Ahuman-machine%2520creative%2520interactions.%2520Tidal-MerzA%2520advances%2520the%2520field%2520of%250Acomputational%2520music%2520generation%252C%2520presenting%2520a%2520novel%2520methodology%2520for%250Aincorporating%2520artificial%2520intelligence%2520into%2520artistic%2520practices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07918v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tidal%20MerzA%3A%20Combining%20affective%20modelling%20and%20autonomous%20code%0A%20%20generation%20through%20Reinforcement%20Learning&entry.906535625=Elizabeth%20Wilson%20and%20Gy%C3%B6rgy%20Fazekas%20and%20Geraint%20Wiggins&entry.1292438233=%20%20This%20paper%20presents%20Tidal-MerzA%2C%20a%20novel%20system%20designed%20for%20collaborative%0Aperformances%20between%20humans%20and%20a%20machine%20agent%20in%20the%20context%20of%20live%20coding%2C%0Aspecifically%20focusing%20on%20the%20generation%20of%20musical%20patterns.%20Tidal-MerzA%20fuses%0Atwo%20foundational%20models%3A%20ALCAA%20%28Affective%20Live%20Coding%20Autonomous%20Agent%29%20and%0ATidal%20Fuzz%2C%20a%20computational%20framework.%20By%20integrating%20affective%20modelling%20with%0Acomputational%20generation%2C%20this%20system%20leverages%20reinforcement%20learning%0Atechniques%20to%20dynamically%20adapt%20music%20composition%20parameters%20within%20the%0ATidalCycles%20framework%2C%20ensuring%20both%20affective%20qualities%20to%20the%20patterns%20and%0Asyntactical%20correctness.%20The%20development%20of%20Tidal-MerzA%20introduces%20two%20distinct%0Aagents%3A%20one%20focusing%20on%20the%20generation%20of%20mini-notation%20strings%20for%20musical%0Aexpression%2C%20and%20another%20on%20the%20alignment%20of%20music%20with%20targeted%20affective%0Astates%20through%20reinforcement%20learning.%20This%20approach%20enhances%20the%20adaptability%0Aand%20creative%20potential%20of%20live%20coding%20practices%20and%20allows%20exploration%20of%0Ahuman-machine%20creative%20interactions.%20Tidal-MerzA%20advances%20the%20field%20of%0Acomputational%20music%20generation%2C%20presenting%20a%20novel%20methodology%20for%0Aincorporating%20artificial%20intelligence%20into%20artistic%20practices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07918v1&entry.124074799=Read"},
{"title": "Benchmarking General-Purpose In-Context Learning", "author": "Fan Wang and Chuan Lin and Yang Cao and Yu Kang", "abstract": "  In-context learning (ICL) empowers generative models to address new tasks\neffectively and efficiently on the fly, without relying on any artificially\ncrafted optimization techniques. In this paper, we study extending ICL to\naddress a broader range of tasks with an extended learning horizon and higher\nimprovement potential, namely General Purpose In-Context Learning (GPICL). To\nthis end, we introduce two lightweight benchmarks specifically crafted to train\nand evaluate GPICL functionalities. Each benchmark encompasses a vast number of\ntasks characterized by significant task variance. These tasks are also crafted\nto promote long-horizon in-context learning through continuous generation and\ninteraction, covering domains such as language modeling, decision-making, and\nworld modeling. The benchmarks necessitate the models to leverage contexts and\nhistory interactions to enhance their capabilities, which we believe to be the\nkey characteristics of GPICL. Our experiments indicate that the diversity of\ntraining tasks is positively correlated with the ability to generalize with\nICL, but inversely correlated with zero-shot capabilities. Additionally, our\nfindings indicate that the scale of parameters alone may not be crucial for ICL\nor GPICL, suggesting alternative approaches such as increasing the scale of\ncontexts and memory states.\n", "link": "http://arxiv.org/abs/2405.17234v6", "date": "2024-09-12", "relevancy": 1.9898, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5127}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5026}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20General-Purpose%20In-Context%20Learning&body=Title%3A%20Benchmarking%20General-Purpose%20In-Context%20Learning%0AAuthor%3A%20Fan%20Wang%20and%20Chuan%20Lin%20and%20Yang%20Cao%20and%20Yu%20Kang%0AAbstract%3A%20%20%20In-context%20learning%20%28ICL%29%20empowers%20generative%20models%20to%20address%20new%20tasks%0Aeffectively%20and%20efficiently%20on%20the%20fly%2C%20without%20relying%20on%20any%20artificially%0Acrafted%20optimization%20techniques.%20In%20this%20paper%2C%20we%20study%20extending%20ICL%20to%0Aaddress%20a%20broader%20range%20of%20tasks%20with%20an%20extended%20learning%20horizon%20and%20higher%0Aimprovement%20potential%2C%20namely%20General%20Purpose%20In-Context%20Learning%20%28GPICL%29.%20To%0Athis%20end%2C%20we%20introduce%20two%20lightweight%20benchmarks%20specifically%20crafted%20to%20train%0Aand%20evaluate%20GPICL%20functionalities.%20Each%20benchmark%20encompasses%20a%20vast%20number%20of%0Atasks%20characterized%20by%20significant%20task%20variance.%20These%20tasks%20are%20also%20crafted%0Ato%20promote%20long-horizon%20in-context%20learning%20through%20continuous%20generation%20and%0Ainteraction%2C%20covering%20domains%20such%20as%20language%20modeling%2C%20decision-making%2C%20and%0Aworld%20modeling.%20The%20benchmarks%20necessitate%20the%20models%20to%20leverage%20contexts%20and%0Ahistory%20interactions%20to%20enhance%20their%20capabilities%2C%20which%20we%20believe%20to%20be%20the%0Akey%20characteristics%20of%20GPICL.%20Our%20experiments%20indicate%20that%20the%20diversity%20of%0Atraining%20tasks%20is%20positively%20correlated%20with%20the%20ability%20to%20generalize%20with%0AICL%2C%20but%20inversely%20correlated%20with%20zero-shot%20capabilities.%20Additionally%2C%20our%0Afindings%20indicate%20that%20the%20scale%20of%20parameters%20alone%20may%20not%20be%20crucial%20for%20ICL%0Aor%20GPICL%2C%20suggesting%20alternative%20approaches%20such%20as%20increasing%20the%20scale%20of%0Acontexts%20and%20memory%20states.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17234v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520General-Purpose%2520In-Context%2520Learning%26entry.906535625%3DFan%2520Wang%2520and%2520Chuan%2520Lin%2520and%2520Yang%2520Cao%2520and%2520Yu%2520Kang%26entry.1292438233%3D%2520%2520In-context%2520learning%2520%2528ICL%2529%2520empowers%2520generative%2520models%2520to%2520address%2520new%2520tasks%250Aeffectively%2520and%2520efficiently%2520on%2520the%2520fly%252C%2520without%2520relying%2520on%2520any%2520artificially%250Acrafted%2520optimization%2520techniques.%2520In%2520this%2520paper%252C%2520we%2520study%2520extending%2520ICL%2520to%250Aaddress%2520a%2520broader%2520range%2520of%2520tasks%2520with%2520an%2520extended%2520learning%2520horizon%2520and%2520higher%250Aimprovement%2520potential%252C%2520namely%2520General%2520Purpose%2520In-Context%2520Learning%2520%2528GPICL%2529.%2520To%250Athis%2520end%252C%2520we%2520introduce%2520two%2520lightweight%2520benchmarks%2520specifically%2520crafted%2520to%2520train%250Aand%2520evaluate%2520GPICL%2520functionalities.%2520Each%2520benchmark%2520encompasses%2520a%2520vast%2520number%2520of%250Atasks%2520characterized%2520by%2520significant%2520task%2520variance.%2520These%2520tasks%2520are%2520also%2520crafted%250Ato%2520promote%2520long-horizon%2520in-context%2520learning%2520through%2520continuous%2520generation%2520and%250Ainteraction%252C%2520covering%2520domains%2520such%2520as%2520language%2520modeling%252C%2520decision-making%252C%2520and%250Aworld%2520modeling.%2520The%2520benchmarks%2520necessitate%2520the%2520models%2520to%2520leverage%2520contexts%2520and%250Ahistory%2520interactions%2520to%2520enhance%2520their%2520capabilities%252C%2520which%2520we%2520believe%2520to%2520be%2520the%250Akey%2520characteristics%2520of%2520GPICL.%2520Our%2520experiments%2520indicate%2520that%2520the%2520diversity%2520of%250Atraining%2520tasks%2520is%2520positively%2520correlated%2520with%2520the%2520ability%2520to%2520generalize%2520with%250AICL%252C%2520but%2520inversely%2520correlated%2520with%2520zero-shot%2520capabilities.%2520Additionally%252C%2520our%250Afindings%2520indicate%2520that%2520the%2520scale%2520of%2520parameters%2520alone%2520may%2520not%2520be%2520crucial%2520for%2520ICL%250Aor%2520GPICL%252C%2520suggesting%2520alternative%2520approaches%2520such%2520as%2520increasing%2520the%2520scale%2520of%250Acontexts%2520and%2520memory%2520states.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17234v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20General-Purpose%20In-Context%20Learning&entry.906535625=Fan%20Wang%20and%20Chuan%20Lin%20and%20Yang%20Cao%20and%20Yu%20Kang&entry.1292438233=%20%20In-context%20learning%20%28ICL%29%20empowers%20generative%20models%20to%20address%20new%20tasks%0Aeffectively%20and%20efficiently%20on%20the%20fly%2C%20without%20relying%20on%20any%20artificially%0Acrafted%20optimization%20techniques.%20In%20this%20paper%2C%20we%20study%20extending%20ICL%20to%0Aaddress%20a%20broader%20range%20of%20tasks%20with%20an%20extended%20learning%20horizon%20and%20higher%0Aimprovement%20potential%2C%20namely%20General%20Purpose%20In-Context%20Learning%20%28GPICL%29.%20To%0Athis%20end%2C%20we%20introduce%20two%20lightweight%20benchmarks%20specifically%20crafted%20to%20train%0Aand%20evaluate%20GPICL%20functionalities.%20Each%20benchmark%20encompasses%20a%20vast%20number%20of%0Atasks%20characterized%20by%20significant%20task%20variance.%20These%20tasks%20are%20also%20crafted%0Ato%20promote%20long-horizon%20in-context%20learning%20through%20continuous%20generation%20and%0Ainteraction%2C%20covering%20domains%20such%20as%20language%20modeling%2C%20decision-making%2C%20and%0Aworld%20modeling.%20The%20benchmarks%20necessitate%20the%20models%20to%20leverage%20contexts%20and%0Ahistory%20interactions%20to%20enhance%20their%20capabilities%2C%20which%20we%20believe%20to%20be%20the%0Akey%20characteristics%20of%20GPICL.%20Our%20experiments%20indicate%20that%20the%20diversity%20of%0Atraining%20tasks%20is%20positively%20correlated%20with%20the%20ability%20to%20generalize%20with%0AICL%2C%20but%20inversely%20correlated%20with%20zero-shot%20capabilities.%20Additionally%2C%20our%0Afindings%20indicate%20that%20the%20scale%20of%20parameters%20alone%20may%20not%20be%20crucial%20for%20ICL%0Aor%20GPICL%2C%20suggesting%20alternative%20approaches%20such%20as%20increasing%20the%20scale%20of%0Acontexts%20and%20memory%20states.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17234v6&entry.124074799=Read"},
{"title": "Linear Adversarial Concept Erasure", "author": "Shauli Ravfogel and Michael Twiton and Yoav Goldberg and Ryan Cotterell", "abstract": "  Modern neural models trained on textual data rely on pre-trained\nrepresentations that emerge without direct supervision. As these\nrepresentations are increasingly being used in real-world applications, the\ninability to \\emph{control} their content becomes an increasingly important\nproblem. We formulate the problem of identifying and erasing a linear subspace\nthat corresponds to a given concept, in order to prevent linear predictors from\nrecovering the concept. We model this problem as a constrained, linear maximin\ngame, and show that existing solutions are generally not optimal for this task.\nWe derive a closed-form solution for certain objectives, and propose a convex\nrelaxation, \\method, that works well for others. When evaluated in the context\nof binary gender removal, the method recovers a low-dimensional subspace whose\nremoval mitigates bias by intrinsic and extrinsic evaluation. We show that the\nmethod is highly expressive, effectively mitigating bias in deep nonlinear\nclassifiers while maintaining tractability and interpretability.\n", "link": "http://arxiv.org/abs/2201.12091v4", "date": "2024-09-12", "relevancy": 1.8587, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4678}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4627}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4623}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Linear%20Adversarial%20Concept%20Erasure&body=Title%3A%20Linear%20Adversarial%20Concept%20Erasure%0AAuthor%3A%20Shauli%20Ravfogel%20and%20Michael%20Twiton%20and%20Yoav%20Goldberg%20and%20Ryan%20Cotterell%0AAbstract%3A%20%20%20Modern%20neural%20models%20trained%20on%20textual%20data%20rely%20on%20pre-trained%0Arepresentations%20that%20emerge%20without%20direct%20supervision.%20As%20these%0Arepresentations%20are%20increasingly%20being%20used%20in%20real-world%20applications%2C%20the%0Ainability%20to%20%5Cemph%7Bcontrol%7D%20their%20content%20becomes%20an%20increasingly%20important%0Aproblem.%20We%20formulate%20the%20problem%20of%20identifying%20and%20erasing%20a%20linear%20subspace%0Athat%20corresponds%20to%20a%20given%20concept%2C%20in%20order%20to%20prevent%20linear%20predictors%20from%0Arecovering%20the%20concept.%20We%20model%20this%20problem%20as%20a%20constrained%2C%20linear%20maximin%0Agame%2C%20and%20show%20that%20existing%20solutions%20are%20generally%20not%20optimal%20for%20this%20task.%0AWe%20derive%20a%20closed-form%20solution%20for%20certain%20objectives%2C%20and%20propose%20a%20convex%0Arelaxation%2C%20%5Cmethod%2C%20that%20works%20well%20for%20others.%20When%20evaluated%20in%20the%20context%0Aof%20binary%20gender%20removal%2C%20the%20method%20recovers%20a%20low-dimensional%20subspace%20whose%0Aremoval%20mitigates%20bias%20by%20intrinsic%20and%20extrinsic%20evaluation.%20We%20show%20that%20the%0Amethod%20is%20highly%20expressive%2C%20effectively%20mitigating%20bias%20in%20deep%20nonlinear%0Aclassifiers%20while%20maintaining%20tractability%20and%20interpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2201.12091v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLinear%2520Adversarial%2520Concept%2520Erasure%26entry.906535625%3DShauli%2520Ravfogel%2520and%2520Michael%2520Twiton%2520and%2520Yoav%2520Goldberg%2520and%2520Ryan%2520Cotterell%26entry.1292438233%3D%2520%2520Modern%2520neural%2520models%2520trained%2520on%2520textual%2520data%2520rely%2520on%2520pre-trained%250Arepresentations%2520that%2520emerge%2520without%2520direct%2520supervision.%2520As%2520these%250Arepresentations%2520are%2520increasingly%2520being%2520used%2520in%2520real-world%2520applications%252C%2520the%250Ainability%2520to%2520%255Cemph%257Bcontrol%257D%2520their%2520content%2520becomes%2520an%2520increasingly%2520important%250Aproblem.%2520We%2520formulate%2520the%2520problem%2520of%2520identifying%2520and%2520erasing%2520a%2520linear%2520subspace%250Athat%2520corresponds%2520to%2520a%2520given%2520concept%252C%2520in%2520order%2520to%2520prevent%2520linear%2520predictors%2520from%250Arecovering%2520the%2520concept.%2520We%2520model%2520this%2520problem%2520as%2520a%2520constrained%252C%2520linear%2520maximin%250Agame%252C%2520and%2520show%2520that%2520existing%2520solutions%2520are%2520generally%2520not%2520optimal%2520for%2520this%2520task.%250AWe%2520derive%2520a%2520closed-form%2520solution%2520for%2520certain%2520objectives%252C%2520and%2520propose%2520a%2520convex%250Arelaxation%252C%2520%255Cmethod%252C%2520that%2520works%2520well%2520for%2520others.%2520When%2520evaluated%2520in%2520the%2520context%250Aof%2520binary%2520gender%2520removal%252C%2520the%2520method%2520recovers%2520a%2520low-dimensional%2520subspace%2520whose%250Aremoval%2520mitigates%2520bias%2520by%2520intrinsic%2520and%2520extrinsic%2520evaluation.%2520We%2520show%2520that%2520the%250Amethod%2520is%2520highly%2520expressive%252C%2520effectively%2520mitigating%2520bias%2520in%2520deep%2520nonlinear%250Aclassifiers%2520while%2520maintaining%2520tractability%2520and%2520interpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2201.12091v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Linear%20Adversarial%20Concept%20Erasure&entry.906535625=Shauli%20Ravfogel%20and%20Michael%20Twiton%20and%20Yoav%20Goldberg%20and%20Ryan%20Cotterell&entry.1292438233=%20%20Modern%20neural%20models%20trained%20on%20textual%20data%20rely%20on%20pre-trained%0Arepresentations%20that%20emerge%20without%20direct%20supervision.%20As%20these%0Arepresentations%20are%20increasingly%20being%20used%20in%20real-world%20applications%2C%20the%0Ainability%20to%20%5Cemph%7Bcontrol%7D%20their%20content%20becomes%20an%20increasingly%20important%0Aproblem.%20We%20formulate%20the%20problem%20of%20identifying%20and%20erasing%20a%20linear%20subspace%0Athat%20corresponds%20to%20a%20given%20concept%2C%20in%20order%20to%20prevent%20linear%20predictors%20from%0Arecovering%20the%20concept.%20We%20model%20this%20problem%20as%20a%20constrained%2C%20linear%20maximin%0Agame%2C%20and%20show%20that%20existing%20solutions%20are%20generally%20not%20optimal%20for%20this%20task.%0AWe%20derive%20a%20closed-form%20solution%20for%20certain%20objectives%2C%20and%20propose%20a%20convex%0Arelaxation%2C%20%5Cmethod%2C%20that%20works%20well%20for%20others.%20When%20evaluated%20in%20the%20context%0Aof%20binary%20gender%20removal%2C%20the%20method%20recovers%20a%20low-dimensional%20subspace%20whose%0Aremoval%20mitigates%20bias%20by%20intrinsic%20and%20extrinsic%20evaluation.%20We%20show%20that%20the%0Amethod%20is%20highly%20expressive%2C%20effectively%20mitigating%20bias%20in%20deep%20nonlinear%0Aclassifiers%20while%20maintaining%20tractability%20and%20interpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2201.12091v4&entry.124074799=Read"},
{"title": "From COCO to COCO-FP: A Deep Dive into Background False Positives for\n  COCO Detectors", "author": "Longfei Liu and Wen Guo and Shihua Huang and Cheng Li and Xi Shen", "abstract": "  Reducing false positives is essential for enhancing object detector\nperformance, as reflected in the mean Average Precision (mAP) metric. Although\nobject detectors have achieved notable improvements and high mAP scores on the\nCOCO dataset, analysis reveals limited progress in addressing false positives\ncaused by non-target visual clutter-background objects not included in the\nannotated categories. This issue is particularly critical in real-world\napplications, such as fire and smoke detection, where minimizing false alarms\nis crucial. In this study, we introduce COCO-FP, a new evaluation dataset\nderived from the ImageNet-1K dataset, designed to address this issue. By\nextending the original COCO validation dataset, COCO-FP specifically assesses\nobject detectors' performance in mitigating background false positives. Our\nevaluation of both standard and advanced object detectors shows a significant\nnumber of false positives in both closed-set and open-set scenarios. For\nexample, the AP50 metric for YOLOv9-E decreases from 72.8 to 65.7 when shifting\nfrom COCO to COCO-FP. The dataset is available at\nhttps://github.com/COCO-FP/COCO-FP.\n", "link": "http://arxiv.org/abs/2409.07907v1", "date": "2024-09-12", "relevancy": 1.8787, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4845}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4601}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20COCO%20to%20COCO-FP%3A%20A%20Deep%20Dive%20into%20Background%20False%20Positives%20for%0A%20%20COCO%20Detectors&body=Title%3A%20From%20COCO%20to%20COCO-FP%3A%20A%20Deep%20Dive%20into%20Background%20False%20Positives%20for%0A%20%20COCO%20Detectors%0AAuthor%3A%20Longfei%20Liu%20and%20Wen%20Guo%20and%20Shihua%20Huang%20and%20Cheng%20Li%20and%20Xi%20Shen%0AAbstract%3A%20%20%20Reducing%20false%20positives%20is%20essential%20for%20enhancing%20object%20detector%0Aperformance%2C%20as%20reflected%20in%20the%20mean%20Average%20Precision%20%28mAP%29%20metric.%20Although%0Aobject%20detectors%20have%20achieved%20notable%20improvements%20and%20high%20mAP%20scores%20on%20the%0ACOCO%20dataset%2C%20analysis%20reveals%20limited%20progress%20in%20addressing%20false%20positives%0Acaused%20by%20non-target%20visual%20clutter-background%20objects%20not%20included%20in%20the%0Aannotated%20categories.%20This%20issue%20is%20particularly%20critical%20in%20real-world%0Aapplications%2C%20such%20as%20fire%20and%20smoke%20detection%2C%20where%20minimizing%20false%20alarms%0Ais%20crucial.%20In%20this%20study%2C%20we%20introduce%20COCO-FP%2C%20a%20new%20evaluation%20dataset%0Aderived%20from%20the%20ImageNet-1K%20dataset%2C%20designed%20to%20address%20this%20issue.%20By%0Aextending%20the%20original%20COCO%20validation%20dataset%2C%20COCO-FP%20specifically%20assesses%0Aobject%20detectors%27%20performance%20in%20mitigating%20background%20false%20positives.%20Our%0Aevaluation%20of%20both%20standard%20and%20advanced%20object%20detectors%20shows%20a%20significant%0Anumber%20of%20false%20positives%20in%20both%20closed-set%20and%20open-set%20scenarios.%20For%0Aexample%2C%20the%20AP50%20metric%20for%20YOLOv9-E%20decreases%20from%2072.8%20to%2065.7%20when%20shifting%0Afrom%20COCO%20to%20COCO-FP.%20The%20dataset%20is%20available%20at%0Ahttps%3A//github.com/COCO-FP/COCO-FP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07907v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520COCO%2520to%2520COCO-FP%253A%2520A%2520Deep%2520Dive%2520into%2520Background%2520False%2520Positives%2520for%250A%2520%2520COCO%2520Detectors%26entry.906535625%3DLongfei%2520Liu%2520and%2520Wen%2520Guo%2520and%2520Shihua%2520Huang%2520and%2520Cheng%2520Li%2520and%2520Xi%2520Shen%26entry.1292438233%3D%2520%2520Reducing%2520false%2520positives%2520is%2520essential%2520for%2520enhancing%2520object%2520detector%250Aperformance%252C%2520as%2520reflected%2520in%2520the%2520mean%2520Average%2520Precision%2520%2528mAP%2529%2520metric.%2520Although%250Aobject%2520detectors%2520have%2520achieved%2520notable%2520improvements%2520and%2520high%2520mAP%2520scores%2520on%2520the%250ACOCO%2520dataset%252C%2520analysis%2520reveals%2520limited%2520progress%2520in%2520addressing%2520false%2520positives%250Acaused%2520by%2520non-target%2520visual%2520clutter-background%2520objects%2520not%2520included%2520in%2520the%250Aannotated%2520categories.%2520This%2520issue%2520is%2520particularly%2520critical%2520in%2520real-world%250Aapplications%252C%2520such%2520as%2520fire%2520and%2520smoke%2520detection%252C%2520where%2520minimizing%2520false%2520alarms%250Ais%2520crucial.%2520In%2520this%2520study%252C%2520we%2520introduce%2520COCO-FP%252C%2520a%2520new%2520evaluation%2520dataset%250Aderived%2520from%2520the%2520ImageNet-1K%2520dataset%252C%2520designed%2520to%2520address%2520this%2520issue.%2520By%250Aextending%2520the%2520original%2520COCO%2520validation%2520dataset%252C%2520COCO-FP%2520specifically%2520assesses%250Aobject%2520detectors%2527%2520performance%2520in%2520mitigating%2520background%2520false%2520positives.%2520Our%250Aevaluation%2520of%2520both%2520standard%2520and%2520advanced%2520object%2520detectors%2520shows%2520a%2520significant%250Anumber%2520of%2520false%2520positives%2520in%2520both%2520closed-set%2520and%2520open-set%2520scenarios.%2520For%250Aexample%252C%2520the%2520AP50%2520metric%2520for%2520YOLOv9-E%2520decreases%2520from%252072.8%2520to%252065.7%2520when%2520shifting%250Afrom%2520COCO%2520to%2520COCO-FP.%2520The%2520dataset%2520is%2520available%2520at%250Ahttps%253A//github.com/COCO-FP/COCO-FP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07907v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20COCO%20to%20COCO-FP%3A%20A%20Deep%20Dive%20into%20Background%20False%20Positives%20for%0A%20%20COCO%20Detectors&entry.906535625=Longfei%20Liu%20and%20Wen%20Guo%20and%20Shihua%20Huang%20and%20Cheng%20Li%20and%20Xi%20Shen&entry.1292438233=%20%20Reducing%20false%20positives%20is%20essential%20for%20enhancing%20object%20detector%0Aperformance%2C%20as%20reflected%20in%20the%20mean%20Average%20Precision%20%28mAP%29%20metric.%20Although%0Aobject%20detectors%20have%20achieved%20notable%20improvements%20and%20high%20mAP%20scores%20on%20the%0ACOCO%20dataset%2C%20analysis%20reveals%20limited%20progress%20in%20addressing%20false%20positives%0Acaused%20by%20non-target%20visual%20clutter-background%20objects%20not%20included%20in%20the%0Aannotated%20categories.%20This%20issue%20is%20particularly%20critical%20in%20real-world%0Aapplications%2C%20such%20as%20fire%20and%20smoke%20detection%2C%20where%20minimizing%20false%20alarms%0Ais%20crucial.%20In%20this%20study%2C%20we%20introduce%20COCO-FP%2C%20a%20new%20evaluation%20dataset%0Aderived%20from%20the%20ImageNet-1K%20dataset%2C%20designed%20to%20address%20this%20issue.%20By%0Aextending%20the%20original%20COCO%20validation%20dataset%2C%20COCO-FP%20specifically%20assesses%0Aobject%20detectors%27%20performance%20in%20mitigating%20background%20false%20positives.%20Our%0Aevaluation%20of%20both%20standard%20and%20advanced%20object%20detectors%20shows%20a%20significant%0Anumber%20of%20false%20positives%20in%20both%20closed-set%20and%20open-set%20scenarios.%20For%0Aexample%2C%20the%20AP50%20metric%20for%20YOLOv9-E%20decreases%20from%2072.8%20to%2065.7%20when%20shifting%0Afrom%20COCO%20to%20COCO-FP.%20The%20dataset%20is%20available%20at%0Ahttps%3A//github.com/COCO-FP/COCO-FP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07907v1&entry.124074799=Read"},
{"title": "Generating synthetic data for neural operators", "author": "Erisa Hasani and Rachel A. Ward", "abstract": "  Numerous developments in the recent literature show the promising potential\nof deep learning in obtaining numerical solutions to partial differential\nequations (PDEs) beyond the reach of current numerical solvers. However,\ndata-driven neural operators all suffer from a similar problem: the data needed\nto train a network depends on classical numerical solvers such as finite\ndifference or finite element, among others. In this paper, we propose a\ndifferent approach to generating synthetic functional training data that does\nnot require solving a PDE numerically. We draw a large number $N$ of\nindependent and identically distributed 'random functions' $u_j$ from the\nunderlying solution space (e.g., $H_0^1(\\Omega)$) in which we know the solution\nlies according to classical theory. We then plug each such random candidate\nsolution into the equation and get a corresponding right-hand side function\n$f_j$ for the equation, and consider $(f_j, u_j)_{j=1}^N$ as supervised\ntraining data for learning the underlying inverse problem $f \\rightarrow u$.\nThis `backwards' approach to generating training data only requires derivative\ncomputations, in contrast to standard `forward' approaches, which require a\nnumerical PDE solver, enabling us to generate many data points quickly and\nefficiently. While the idea is simple, we hope this method will expand the\npotential for developing neural PDE solvers that do not depend on classical\nnumerical solvers.\n", "link": "http://arxiv.org/abs/2401.02398v2", "date": "2024-09-12", "relevancy": 1.4215, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5019}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4682}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4649}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generating%20synthetic%20data%20for%20neural%20operators&body=Title%3A%20Generating%20synthetic%20data%20for%20neural%20operators%0AAuthor%3A%20Erisa%20Hasani%20and%20Rachel%20A.%20Ward%0AAbstract%3A%20%20%20Numerous%20developments%20in%20the%20recent%20literature%20show%20the%20promising%20potential%0Aof%20deep%20learning%20in%20obtaining%20numerical%20solutions%20to%20partial%20differential%0Aequations%20%28PDEs%29%20beyond%20the%20reach%20of%20current%20numerical%20solvers.%20However%2C%0Adata-driven%20neural%20operators%20all%20suffer%20from%20a%20similar%20problem%3A%20the%20data%20needed%0Ato%20train%20a%20network%20depends%20on%20classical%20numerical%20solvers%20such%20as%20finite%0Adifference%20or%20finite%20element%2C%20among%20others.%20In%20this%20paper%2C%20we%20propose%20a%0Adifferent%20approach%20to%20generating%20synthetic%20functional%20training%20data%20that%20does%0Anot%20require%20solving%20a%20PDE%20numerically.%20We%20draw%20a%20large%20number%20%24N%24%20of%0Aindependent%20and%20identically%20distributed%20%27random%20functions%27%20%24u_j%24%20from%20the%0Aunderlying%20solution%20space%20%28e.g.%2C%20%24H_0%5E1%28%5COmega%29%24%29%20in%20which%20we%20know%20the%20solution%0Alies%20according%20to%20classical%20theory.%20We%20then%20plug%20each%20such%20random%20candidate%0Asolution%20into%20the%20equation%20and%20get%20a%20corresponding%20right-hand%20side%20function%0A%24f_j%24%20for%20the%20equation%2C%20and%20consider%20%24%28f_j%2C%20u_j%29_%7Bj%3D1%7D%5EN%24%20as%20supervised%0Atraining%20data%20for%20learning%20the%20underlying%20inverse%20problem%20%24f%20%5Crightarrow%20u%24.%0AThis%20%60backwards%27%20approach%20to%20generating%20training%20data%20only%20requires%20derivative%0Acomputations%2C%20in%20contrast%20to%20standard%20%60forward%27%20approaches%2C%20which%20require%20a%0Anumerical%20PDE%20solver%2C%20enabling%20us%20to%20generate%20many%20data%20points%20quickly%20and%0Aefficiently.%20While%20the%20idea%20is%20simple%2C%20we%20hope%20this%20method%20will%20expand%20the%0Apotential%20for%20developing%20neural%20PDE%20solvers%20that%20do%20not%20depend%20on%20classical%0Anumerical%20solvers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.02398v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerating%2520synthetic%2520data%2520for%2520neural%2520operators%26entry.906535625%3DErisa%2520Hasani%2520and%2520Rachel%2520A.%2520Ward%26entry.1292438233%3D%2520%2520Numerous%2520developments%2520in%2520the%2520recent%2520literature%2520show%2520the%2520promising%2520potential%250Aof%2520deep%2520learning%2520in%2520obtaining%2520numerical%2520solutions%2520to%2520partial%2520differential%250Aequations%2520%2528PDEs%2529%2520beyond%2520the%2520reach%2520of%2520current%2520numerical%2520solvers.%2520However%252C%250Adata-driven%2520neural%2520operators%2520all%2520suffer%2520from%2520a%2520similar%2520problem%253A%2520the%2520data%2520needed%250Ato%2520train%2520a%2520network%2520depends%2520on%2520classical%2520numerical%2520solvers%2520such%2520as%2520finite%250Adifference%2520or%2520finite%2520element%252C%2520among%2520others.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Adifferent%2520approach%2520to%2520generating%2520synthetic%2520functional%2520training%2520data%2520that%2520does%250Anot%2520require%2520solving%2520a%2520PDE%2520numerically.%2520We%2520draw%2520a%2520large%2520number%2520%2524N%2524%2520of%250Aindependent%2520and%2520identically%2520distributed%2520%2527random%2520functions%2527%2520%2524u_j%2524%2520from%2520the%250Aunderlying%2520solution%2520space%2520%2528e.g.%252C%2520%2524H_0%255E1%2528%255COmega%2529%2524%2529%2520in%2520which%2520we%2520know%2520the%2520solution%250Alies%2520according%2520to%2520classical%2520theory.%2520We%2520then%2520plug%2520each%2520such%2520random%2520candidate%250Asolution%2520into%2520the%2520equation%2520and%2520get%2520a%2520corresponding%2520right-hand%2520side%2520function%250A%2524f_j%2524%2520for%2520the%2520equation%252C%2520and%2520consider%2520%2524%2528f_j%252C%2520u_j%2529_%257Bj%253D1%257D%255EN%2524%2520as%2520supervised%250Atraining%2520data%2520for%2520learning%2520the%2520underlying%2520inverse%2520problem%2520%2524f%2520%255Crightarrow%2520u%2524.%250AThis%2520%2560backwards%2527%2520approach%2520to%2520generating%2520training%2520data%2520only%2520requires%2520derivative%250Acomputations%252C%2520in%2520contrast%2520to%2520standard%2520%2560forward%2527%2520approaches%252C%2520which%2520require%2520a%250Anumerical%2520PDE%2520solver%252C%2520enabling%2520us%2520to%2520generate%2520many%2520data%2520points%2520quickly%2520and%250Aefficiently.%2520While%2520the%2520idea%2520is%2520simple%252C%2520we%2520hope%2520this%2520method%2520will%2520expand%2520the%250Apotential%2520for%2520developing%2520neural%2520PDE%2520solvers%2520that%2520do%2520not%2520depend%2520on%2520classical%250Anumerical%2520solvers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.02398v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generating%20synthetic%20data%20for%20neural%20operators&entry.906535625=Erisa%20Hasani%20and%20Rachel%20A.%20Ward&entry.1292438233=%20%20Numerous%20developments%20in%20the%20recent%20literature%20show%20the%20promising%20potential%0Aof%20deep%20learning%20in%20obtaining%20numerical%20solutions%20to%20partial%20differential%0Aequations%20%28PDEs%29%20beyond%20the%20reach%20of%20current%20numerical%20solvers.%20However%2C%0Adata-driven%20neural%20operators%20all%20suffer%20from%20a%20similar%20problem%3A%20the%20data%20needed%0Ato%20train%20a%20network%20depends%20on%20classical%20numerical%20solvers%20such%20as%20finite%0Adifference%20or%20finite%20element%2C%20among%20others.%20In%20this%20paper%2C%20we%20propose%20a%0Adifferent%20approach%20to%20generating%20synthetic%20functional%20training%20data%20that%20does%0Anot%20require%20solving%20a%20PDE%20numerically.%20We%20draw%20a%20large%20number%20%24N%24%20of%0Aindependent%20and%20identically%20distributed%20%27random%20functions%27%20%24u_j%24%20from%20the%0Aunderlying%20solution%20space%20%28e.g.%2C%20%24H_0%5E1%28%5COmega%29%24%29%20in%20which%20we%20know%20the%20solution%0Alies%20according%20to%20classical%20theory.%20We%20then%20plug%20each%20such%20random%20candidate%0Asolution%20into%20the%20equation%20and%20get%20a%20corresponding%20right-hand%20side%20function%0A%24f_j%24%20for%20the%20equation%2C%20and%20consider%20%24%28f_j%2C%20u_j%29_%7Bj%3D1%7D%5EN%24%20as%20supervised%0Atraining%20data%20for%20learning%20the%20underlying%20inverse%20problem%20%24f%20%5Crightarrow%20u%24.%0AThis%20%60backwards%27%20approach%20to%20generating%20training%20data%20only%20requires%20derivative%0Acomputations%2C%20in%20contrast%20to%20standard%20%60forward%27%20approaches%2C%20which%20require%20a%0Anumerical%20PDE%20solver%2C%20enabling%20us%20to%20generate%20many%20data%20points%20quickly%20and%0Aefficiently.%20While%20the%20idea%20is%20simple%2C%20we%20hope%20this%20method%20will%20expand%20the%0Apotential%20for%20developing%20neural%20PDE%20solvers%20that%20do%20not%20depend%20on%20classical%0Anumerical%20solvers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.02398v2&entry.124074799=Read"},
{"title": "SDformer: Efficient End-to-End Transformer for Depth Completion", "author": "Jian Qian and Miao Sun and Ashley Lee and Jie Li and Shenglong Zhuo and Patrick Yin Chiang", "abstract": "  Depth completion aims to predict dense depth maps with sparse depth\nmeasurements from a depth sensor. Currently, Convolutional Neural Network (CNN)\nbased models are the most popular methods applied to depth completion tasks.\nHowever, despite the excellent high-end performance, they suffer from a limited\nrepresentation area. To overcome the drawbacks of CNNs, a more effective and\npowerful method has been presented: the Transformer, which is an adaptive\nself-attention setting sequence-to-sequence model. While the standard\nTransformer quadratically increases the computational cost from the key-query\ndot-product of input resolution which improperly employs depth completion\ntasks. In this work, we propose a different window-based Transformer\narchitecture for depth completion tasks named Sparse-to-Dense Transformer\n(SDformer). The network consists of an input module for the depth map and RGB\nimage features extraction and concatenation, a U-shaped encoder-decoder\nTransformer for extracting deep features, and a refinement module.\nSpecifically, we first concatenate the depth map features with the RGB image\nfeatures through the input model. Then, instead of calculating self-attention\nwith the whole feature maps, we apply different window sizes to extract the\nlong-range depth dependencies. Finally, we refine the predicted features from\nthe input module and the U-shaped encoder-decoder Transformer module to get the\nenriching depth features and employ a convolution layer to obtain the dense\ndepth map. In practice, the SDformer obtains state-of-the-art results against\nthe CNN-based depth completion models with lower computing loads and parameters\non the NYU Depth V2 and KITTI DC datasets.\n", "link": "http://arxiv.org/abs/2409.08159v1", "date": "2024-09-12", "relevancy": 1.6899, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5829}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.57}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SDformer%3A%20Efficient%20End-to-End%20Transformer%20for%20Depth%20Completion&body=Title%3A%20SDformer%3A%20Efficient%20End-to-End%20Transformer%20for%20Depth%20Completion%0AAuthor%3A%20Jian%20Qian%20and%20Miao%20Sun%20and%20Ashley%20Lee%20and%20Jie%20Li%20and%20Shenglong%20Zhuo%20and%20Patrick%20Yin%20Chiang%0AAbstract%3A%20%20%20Depth%20completion%20aims%20to%20predict%20dense%20depth%20maps%20with%20sparse%20depth%0Ameasurements%20from%20a%20depth%20sensor.%20Currently%2C%20Convolutional%20Neural%20Network%20%28CNN%29%0Abased%20models%20are%20the%20most%20popular%20methods%20applied%20to%20depth%20completion%20tasks.%0AHowever%2C%20despite%20the%20excellent%20high-end%20performance%2C%20they%20suffer%20from%20a%20limited%0Arepresentation%20area.%20To%20overcome%20the%20drawbacks%20of%20CNNs%2C%20a%20more%20effective%20and%0Apowerful%20method%20has%20been%20presented%3A%20the%20Transformer%2C%20which%20is%20an%20adaptive%0Aself-attention%20setting%20sequence-to-sequence%20model.%20While%20the%20standard%0ATransformer%20quadratically%20increases%20the%20computational%20cost%20from%20the%20key-query%0Adot-product%20of%20input%20resolution%20which%20improperly%20employs%20depth%20completion%0Atasks.%20In%20this%20work%2C%20we%20propose%20a%20different%20window-based%20Transformer%0Aarchitecture%20for%20depth%20completion%20tasks%20named%20Sparse-to-Dense%20Transformer%0A%28SDformer%29.%20The%20network%20consists%20of%20an%20input%20module%20for%20the%20depth%20map%20and%20RGB%0Aimage%20features%20extraction%20and%20concatenation%2C%20a%20U-shaped%20encoder-decoder%0ATransformer%20for%20extracting%20deep%20features%2C%20and%20a%20refinement%20module.%0ASpecifically%2C%20we%20first%20concatenate%20the%20depth%20map%20features%20with%20the%20RGB%20image%0Afeatures%20through%20the%20input%20model.%20Then%2C%20instead%20of%20calculating%20self-attention%0Awith%20the%20whole%20feature%20maps%2C%20we%20apply%20different%20window%20sizes%20to%20extract%20the%0Along-range%20depth%20dependencies.%20Finally%2C%20we%20refine%20the%20predicted%20features%20from%0Athe%20input%20module%20and%20the%20U-shaped%20encoder-decoder%20Transformer%20module%20to%20get%20the%0Aenriching%20depth%20features%20and%20employ%20a%20convolution%20layer%20to%20obtain%20the%20dense%0Adepth%20map.%20In%20practice%2C%20the%20SDformer%20obtains%20state-of-the-art%20results%20against%0Athe%20CNN-based%20depth%20completion%20models%20with%20lower%20computing%20loads%20and%20parameters%0Aon%20the%20NYU%20Depth%20V2%20and%20KITTI%20DC%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08159v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSDformer%253A%2520Efficient%2520End-to-End%2520Transformer%2520for%2520Depth%2520Completion%26entry.906535625%3DJian%2520Qian%2520and%2520Miao%2520Sun%2520and%2520Ashley%2520Lee%2520and%2520Jie%2520Li%2520and%2520Shenglong%2520Zhuo%2520and%2520Patrick%2520Yin%2520Chiang%26entry.1292438233%3D%2520%2520Depth%2520completion%2520aims%2520to%2520predict%2520dense%2520depth%2520maps%2520with%2520sparse%2520depth%250Ameasurements%2520from%2520a%2520depth%2520sensor.%2520Currently%252C%2520Convolutional%2520Neural%2520Network%2520%2528CNN%2529%250Abased%2520models%2520are%2520the%2520most%2520popular%2520methods%2520applied%2520to%2520depth%2520completion%2520tasks.%250AHowever%252C%2520despite%2520the%2520excellent%2520high-end%2520performance%252C%2520they%2520suffer%2520from%2520a%2520limited%250Arepresentation%2520area.%2520To%2520overcome%2520the%2520drawbacks%2520of%2520CNNs%252C%2520a%2520more%2520effective%2520and%250Apowerful%2520method%2520has%2520been%2520presented%253A%2520the%2520Transformer%252C%2520which%2520is%2520an%2520adaptive%250Aself-attention%2520setting%2520sequence-to-sequence%2520model.%2520While%2520the%2520standard%250ATransformer%2520quadratically%2520increases%2520the%2520computational%2520cost%2520from%2520the%2520key-query%250Adot-product%2520of%2520input%2520resolution%2520which%2520improperly%2520employs%2520depth%2520completion%250Atasks.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520different%2520window-based%2520Transformer%250Aarchitecture%2520for%2520depth%2520completion%2520tasks%2520named%2520Sparse-to-Dense%2520Transformer%250A%2528SDformer%2529.%2520The%2520network%2520consists%2520of%2520an%2520input%2520module%2520for%2520the%2520depth%2520map%2520and%2520RGB%250Aimage%2520features%2520extraction%2520and%2520concatenation%252C%2520a%2520U-shaped%2520encoder-decoder%250ATransformer%2520for%2520extracting%2520deep%2520features%252C%2520and%2520a%2520refinement%2520module.%250ASpecifically%252C%2520we%2520first%2520concatenate%2520the%2520depth%2520map%2520features%2520with%2520the%2520RGB%2520image%250Afeatures%2520through%2520the%2520input%2520model.%2520Then%252C%2520instead%2520of%2520calculating%2520self-attention%250Awith%2520the%2520whole%2520feature%2520maps%252C%2520we%2520apply%2520different%2520window%2520sizes%2520to%2520extract%2520the%250Along-range%2520depth%2520dependencies.%2520Finally%252C%2520we%2520refine%2520the%2520predicted%2520features%2520from%250Athe%2520input%2520module%2520and%2520the%2520U-shaped%2520encoder-decoder%2520Transformer%2520module%2520to%2520get%2520the%250Aenriching%2520depth%2520features%2520and%2520employ%2520a%2520convolution%2520layer%2520to%2520obtain%2520the%2520dense%250Adepth%2520map.%2520In%2520practice%252C%2520the%2520SDformer%2520obtains%2520state-of-the-art%2520results%2520against%250Athe%2520CNN-based%2520depth%2520completion%2520models%2520with%2520lower%2520computing%2520loads%2520and%2520parameters%250Aon%2520the%2520NYU%2520Depth%2520V2%2520and%2520KITTI%2520DC%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08159v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SDformer%3A%20Efficient%20End-to-End%20Transformer%20for%20Depth%20Completion&entry.906535625=Jian%20Qian%20and%20Miao%20Sun%20and%20Ashley%20Lee%20and%20Jie%20Li%20and%20Shenglong%20Zhuo%20and%20Patrick%20Yin%20Chiang&entry.1292438233=%20%20Depth%20completion%20aims%20to%20predict%20dense%20depth%20maps%20with%20sparse%20depth%0Ameasurements%20from%20a%20depth%20sensor.%20Currently%2C%20Convolutional%20Neural%20Network%20%28CNN%29%0Abased%20models%20are%20the%20most%20popular%20methods%20applied%20to%20depth%20completion%20tasks.%0AHowever%2C%20despite%20the%20excellent%20high-end%20performance%2C%20they%20suffer%20from%20a%20limited%0Arepresentation%20area.%20To%20overcome%20the%20drawbacks%20of%20CNNs%2C%20a%20more%20effective%20and%0Apowerful%20method%20has%20been%20presented%3A%20the%20Transformer%2C%20which%20is%20an%20adaptive%0Aself-attention%20setting%20sequence-to-sequence%20model.%20While%20the%20standard%0ATransformer%20quadratically%20increases%20the%20computational%20cost%20from%20the%20key-query%0Adot-product%20of%20input%20resolution%20which%20improperly%20employs%20depth%20completion%0Atasks.%20In%20this%20work%2C%20we%20propose%20a%20different%20window-based%20Transformer%0Aarchitecture%20for%20depth%20completion%20tasks%20named%20Sparse-to-Dense%20Transformer%0A%28SDformer%29.%20The%20network%20consists%20of%20an%20input%20module%20for%20the%20depth%20map%20and%20RGB%0Aimage%20features%20extraction%20and%20concatenation%2C%20a%20U-shaped%20encoder-decoder%0ATransformer%20for%20extracting%20deep%20features%2C%20and%20a%20refinement%20module.%0ASpecifically%2C%20we%20first%20concatenate%20the%20depth%20map%20features%20with%20the%20RGB%20image%0Afeatures%20through%20the%20input%20model.%20Then%2C%20instead%20of%20calculating%20self-attention%0Awith%20the%20whole%20feature%20maps%2C%20we%20apply%20different%20window%20sizes%20to%20extract%20the%0Along-range%20depth%20dependencies.%20Finally%2C%20we%20refine%20the%20predicted%20features%20from%0Athe%20input%20module%20and%20the%20U-shaped%20encoder-decoder%20Transformer%20module%20to%20get%20the%0Aenriching%20depth%20features%20and%20employ%20a%20convolution%20layer%20to%20obtain%20the%20dense%0Adepth%20map.%20In%20practice%2C%20the%20SDformer%20obtains%20state-of-the-art%20results%20against%0Athe%20CNN-based%20depth%20completion%20models%20with%20lower%20computing%20loads%20and%20parameters%0Aon%20the%20NYU%20Depth%20V2%20and%20KITTI%20DC%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08159v1&entry.124074799=Read"},
{"title": "Identifiable causal inference with noisy treatment and no side\n  information", "author": "Antti P\u00f6ll\u00e4nen and Pekka Marttinen", "abstract": "  In some causal inference scenarios, the treatment variable is measured\ninaccurately, for instance in epidemiology or econometrics. Failure to correct\nfor the effect of this measurement error can lead to biased causal effect\nestimates. Previous research has not studied methods that address this issue\nfrom a causal viewpoint while allowing for complex nonlinear dependencies and\nwithout assuming access to side information. For such a scenario, this study\nproposes a model that assumes a continuous treatment variable that is\ninaccurately measured. Building on existing results for measurement error\nmodels, we prove that our model's causal effect estimates are identifiable,\neven without side information and knowledge of the measurement error variance.\nOur method relies on a deep latent variable model in which Gaussian\nconditionals are parameterized by neural networks, and we develop an amortized\nimportance-weighted variational objective for training the model. Empirical\nresults demonstrate the method's good performance with unknown measurement\nerror. More broadly, our work extends the range of applications in which\nreliable causal inference can be conducted.\n", "link": "http://arxiv.org/abs/2306.10614v3", "date": "2024-09-12", "relevancy": 1.8194, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4795}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4596}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identifiable%20causal%20inference%20with%20noisy%20treatment%20and%20no%20side%0A%20%20information&body=Title%3A%20Identifiable%20causal%20inference%20with%20noisy%20treatment%20and%20no%20side%0A%20%20information%0AAuthor%3A%20Antti%20P%C3%B6ll%C3%A4nen%20and%20Pekka%20Marttinen%0AAbstract%3A%20%20%20In%20some%20causal%20inference%20scenarios%2C%20the%20treatment%20variable%20is%20measured%0Ainaccurately%2C%20for%20instance%20in%20epidemiology%20or%20econometrics.%20Failure%20to%20correct%0Afor%20the%20effect%20of%20this%20measurement%20error%20can%20lead%20to%20biased%20causal%20effect%0Aestimates.%20Previous%20research%20has%20not%20studied%20methods%20that%20address%20this%20issue%0Afrom%20a%20causal%20viewpoint%20while%20allowing%20for%20complex%20nonlinear%20dependencies%20and%0Awithout%20assuming%20access%20to%20side%20information.%20For%20such%20a%20scenario%2C%20this%20study%0Aproposes%20a%20model%20that%20assumes%20a%20continuous%20treatment%20variable%20that%20is%0Ainaccurately%20measured.%20Building%20on%20existing%20results%20for%20measurement%20error%0Amodels%2C%20we%20prove%20that%20our%20model%27s%20causal%20effect%20estimates%20are%20identifiable%2C%0Aeven%20without%20side%20information%20and%20knowledge%20of%20the%20measurement%20error%20variance.%0AOur%20method%20relies%20on%20a%20deep%20latent%20variable%20model%20in%20which%20Gaussian%0Aconditionals%20are%20parameterized%20by%20neural%20networks%2C%20and%20we%20develop%20an%20amortized%0Aimportance-weighted%20variational%20objective%20for%20training%20the%20model.%20Empirical%0Aresults%20demonstrate%20the%20method%27s%20good%20performance%20with%20unknown%20measurement%0Aerror.%20More%20broadly%2C%20our%20work%20extends%20the%20range%20of%20applications%20in%20which%0Areliable%20causal%20inference%20can%20be%20conducted.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.10614v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentifiable%2520causal%2520inference%2520with%2520noisy%2520treatment%2520and%2520no%2520side%250A%2520%2520information%26entry.906535625%3DAntti%2520P%25C3%25B6ll%25C3%25A4nen%2520and%2520Pekka%2520Marttinen%26entry.1292438233%3D%2520%2520In%2520some%2520causal%2520inference%2520scenarios%252C%2520the%2520treatment%2520variable%2520is%2520measured%250Ainaccurately%252C%2520for%2520instance%2520in%2520epidemiology%2520or%2520econometrics.%2520Failure%2520to%2520correct%250Afor%2520the%2520effect%2520of%2520this%2520measurement%2520error%2520can%2520lead%2520to%2520biased%2520causal%2520effect%250Aestimates.%2520Previous%2520research%2520has%2520not%2520studied%2520methods%2520that%2520address%2520this%2520issue%250Afrom%2520a%2520causal%2520viewpoint%2520while%2520allowing%2520for%2520complex%2520nonlinear%2520dependencies%2520and%250Awithout%2520assuming%2520access%2520to%2520side%2520information.%2520For%2520such%2520a%2520scenario%252C%2520this%2520study%250Aproposes%2520a%2520model%2520that%2520assumes%2520a%2520continuous%2520treatment%2520variable%2520that%2520is%250Ainaccurately%2520measured.%2520Building%2520on%2520existing%2520results%2520for%2520measurement%2520error%250Amodels%252C%2520we%2520prove%2520that%2520our%2520model%2527s%2520causal%2520effect%2520estimates%2520are%2520identifiable%252C%250Aeven%2520without%2520side%2520information%2520and%2520knowledge%2520of%2520the%2520measurement%2520error%2520variance.%250AOur%2520method%2520relies%2520on%2520a%2520deep%2520latent%2520variable%2520model%2520in%2520which%2520Gaussian%250Aconditionals%2520are%2520parameterized%2520by%2520neural%2520networks%252C%2520and%2520we%2520develop%2520an%2520amortized%250Aimportance-weighted%2520variational%2520objective%2520for%2520training%2520the%2520model.%2520Empirical%250Aresults%2520demonstrate%2520the%2520method%2527s%2520good%2520performance%2520with%2520unknown%2520measurement%250Aerror.%2520More%2520broadly%252C%2520our%2520work%2520extends%2520the%2520range%2520of%2520applications%2520in%2520which%250Areliable%2520causal%2520inference%2520can%2520be%2520conducted.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.10614v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identifiable%20causal%20inference%20with%20noisy%20treatment%20and%20no%20side%0A%20%20information&entry.906535625=Antti%20P%C3%B6ll%C3%A4nen%20and%20Pekka%20Marttinen&entry.1292438233=%20%20In%20some%20causal%20inference%20scenarios%2C%20the%20treatment%20variable%20is%20measured%0Ainaccurately%2C%20for%20instance%20in%20epidemiology%20or%20econometrics.%20Failure%20to%20correct%0Afor%20the%20effect%20of%20this%20measurement%20error%20can%20lead%20to%20biased%20causal%20effect%0Aestimates.%20Previous%20research%20has%20not%20studied%20methods%20that%20address%20this%20issue%0Afrom%20a%20causal%20viewpoint%20while%20allowing%20for%20complex%20nonlinear%20dependencies%20and%0Awithout%20assuming%20access%20to%20side%20information.%20For%20such%20a%20scenario%2C%20this%20study%0Aproposes%20a%20model%20that%20assumes%20a%20continuous%20treatment%20variable%20that%20is%0Ainaccurately%20measured.%20Building%20on%20existing%20results%20for%20measurement%20error%0Amodels%2C%20we%20prove%20that%20our%20model%27s%20causal%20effect%20estimates%20are%20identifiable%2C%0Aeven%20without%20side%20information%20and%20knowledge%20of%20the%20measurement%20error%20variance.%0AOur%20method%20relies%20on%20a%20deep%20latent%20variable%20model%20in%20which%20Gaussian%0Aconditionals%20are%20parameterized%20by%20neural%20networks%2C%20and%20we%20develop%20an%20amortized%0Aimportance-weighted%20variational%20objective%20for%20training%20the%20model.%20Empirical%0Aresults%20demonstrate%20the%20method%27s%20good%20performance%20with%20unknown%20measurement%0Aerror.%20More%20broadly%2C%20our%20work%20extends%20the%20range%20of%20applications%20in%20which%0Areliable%20causal%20inference%20can%20be%20conducted.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.10614v3&entry.124074799=Read"},
{"title": "AI-accelerated discovery of high critical temperature superconductors", "author": "Xiao-Qi Han and Zhenfeng Ouyang and Peng-Jie Guo and Hao Sun and Ze-Feng Gao and Zhong-Yi Lu", "abstract": "  The discovery of new superconducting materials, particularly those exhibiting\nhigh critical temperature ($T_c$), has been a vibrant area of study within the\nfield of condensed matter physics. Conventional approaches primarily rely on\nphysical intuition to search for potential superconductors within the existing\ndatabases. However, the known materials only scratch the surface of the\nextensive array of possibilities within the realm of materials. Here, we\ndevelop an AI search engine that integrates deep model pre-training and\nfine-tuning techniques, diffusion models, and physics-based approaches (e.g.,\nfirst-principles electronic structure calculation) for discovery of high-$T_c$\nsuperconductors. Utilizing this AI search engine, we have obtained 74\ndynamically stable materials with critical temperatures predicted by the AI\nmodel to be $T_c \\geq$ 15 K based on a very small set of samples. Notably,\nthese materials are not contained in any existing dataset. Furthermore, we\nanalyze trends in our dataset and individual materials including B$_4$CN$_3$\nand B$_5$CN$_2$ whose $T_c$s are 24.08 K and 15.93 K, respectively. We\ndemonstrate that AI technique can discover a set of new high-$T_c$\nsuperconductors, outline its potential for accelerating discovery of the\nmaterials with targeted properties.\n", "link": "http://arxiv.org/abs/2409.08065v1", "date": "2024-09-12", "relevancy": 1.4788, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.3704}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.3704}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.3659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI-accelerated%20discovery%20of%20high%20critical%20temperature%20superconductors&body=Title%3A%20AI-accelerated%20discovery%20of%20high%20critical%20temperature%20superconductors%0AAuthor%3A%20Xiao-Qi%20Han%20and%20Zhenfeng%20Ouyang%20and%20Peng-Jie%20Guo%20and%20Hao%20Sun%20and%20Ze-Feng%20Gao%20and%20Zhong-Yi%20Lu%0AAbstract%3A%20%20%20The%20discovery%20of%20new%20superconducting%20materials%2C%20particularly%20those%20exhibiting%0Ahigh%20critical%20temperature%20%28%24T_c%24%29%2C%20has%20been%20a%20vibrant%20area%20of%20study%20within%20the%0Afield%20of%20condensed%20matter%20physics.%20Conventional%20approaches%20primarily%20rely%20on%0Aphysical%20intuition%20to%20search%20for%20potential%20superconductors%20within%20the%20existing%0Adatabases.%20However%2C%20the%20known%20materials%20only%20scratch%20the%20surface%20of%20the%0Aextensive%20array%20of%20possibilities%20within%20the%20realm%20of%20materials.%20Here%2C%20we%0Adevelop%20an%20AI%20search%20engine%20that%20integrates%20deep%20model%20pre-training%20and%0Afine-tuning%20techniques%2C%20diffusion%20models%2C%20and%20physics-based%20approaches%20%28e.g.%2C%0Afirst-principles%20electronic%20structure%20calculation%29%20for%20discovery%20of%20high-%24T_c%24%0Asuperconductors.%20Utilizing%20this%20AI%20search%20engine%2C%20we%20have%20obtained%2074%0Adynamically%20stable%20materials%20with%20critical%20temperatures%20predicted%20by%20the%20AI%0Amodel%20to%20be%20%24T_c%20%5Cgeq%24%2015%20K%20based%20on%20a%20very%20small%20set%20of%20samples.%20Notably%2C%0Athese%20materials%20are%20not%20contained%20in%20any%20existing%20dataset.%20Furthermore%2C%20we%0Aanalyze%20trends%20in%20our%20dataset%20and%20individual%20materials%20including%20B%24_4%24CN%24_3%24%0Aand%20B%24_5%24CN%24_2%24%20whose%20%24T_c%24s%20are%2024.08%20K%20and%2015.93%20K%2C%20respectively.%20We%0Ademonstrate%20that%20AI%20technique%20can%20discover%20a%20set%20of%20new%20high-%24T_c%24%0Asuperconductors%2C%20outline%20its%20potential%20for%20accelerating%20discovery%20of%20the%0Amaterials%20with%20targeted%20properties.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08065v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI-accelerated%2520discovery%2520of%2520high%2520critical%2520temperature%2520superconductors%26entry.906535625%3DXiao-Qi%2520Han%2520and%2520Zhenfeng%2520Ouyang%2520and%2520Peng-Jie%2520Guo%2520and%2520Hao%2520Sun%2520and%2520Ze-Feng%2520Gao%2520and%2520Zhong-Yi%2520Lu%26entry.1292438233%3D%2520%2520The%2520discovery%2520of%2520new%2520superconducting%2520materials%252C%2520particularly%2520those%2520exhibiting%250Ahigh%2520critical%2520temperature%2520%2528%2524T_c%2524%2529%252C%2520has%2520been%2520a%2520vibrant%2520area%2520of%2520study%2520within%2520the%250Afield%2520of%2520condensed%2520matter%2520physics.%2520Conventional%2520approaches%2520primarily%2520rely%2520on%250Aphysical%2520intuition%2520to%2520search%2520for%2520potential%2520superconductors%2520within%2520the%2520existing%250Adatabases.%2520However%252C%2520the%2520known%2520materials%2520only%2520scratch%2520the%2520surface%2520of%2520the%250Aextensive%2520array%2520of%2520possibilities%2520within%2520the%2520realm%2520of%2520materials.%2520Here%252C%2520we%250Adevelop%2520an%2520AI%2520search%2520engine%2520that%2520integrates%2520deep%2520model%2520pre-training%2520and%250Afine-tuning%2520techniques%252C%2520diffusion%2520models%252C%2520and%2520physics-based%2520approaches%2520%2528e.g.%252C%250Afirst-principles%2520electronic%2520structure%2520calculation%2529%2520for%2520discovery%2520of%2520high-%2524T_c%2524%250Asuperconductors.%2520Utilizing%2520this%2520AI%2520search%2520engine%252C%2520we%2520have%2520obtained%252074%250Adynamically%2520stable%2520materials%2520with%2520critical%2520temperatures%2520predicted%2520by%2520the%2520AI%250Amodel%2520to%2520be%2520%2524T_c%2520%255Cgeq%2524%252015%2520K%2520based%2520on%2520a%2520very%2520small%2520set%2520of%2520samples.%2520Notably%252C%250Athese%2520materials%2520are%2520not%2520contained%2520in%2520any%2520existing%2520dataset.%2520Furthermore%252C%2520we%250Aanalyze%2520trends%2520in%2520our%2520dataset%2520and%2520individual%2520materials%2520including%2520B%2524_4%2524CN%2524_3%2524%250Aand%2520B%2524_5%2524CN%2524_2%2524%2520whose%2520%2524T_c%2524s%2520are%252024.08%2520K%2520and%252015.93%2520K%252C%2520respectively.%2520We%250Ademonstrate%2520that%2520AI%2520technique%2520can%2520discover%2520a%2520set%2520of%2520new%2520high-%2524T_c%2524%250Asuperconductors%252C%2520outline%2520its%2520potential%2520for%2520accelerating%2520discovery%2520of%2520the%250Amaterials%2520with%2520targeted%2520properties.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08065v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI-accelerated%20discovery%20of%20high%20critical%20temperature%20superconductors&entry.906535625=Xiao-Qi%20Han%20and%20Zhenfeng%20Ouyang%20and%20Peng-Jie%20Guo%20and%20Hao%20Sun%20and%20Ze-Feng%20Gao%20and%20Zhong-Yi%20Lu&entry.1292438233=%20%20The%20discovery%20of%20new%20superconducting%20materials%2C%20particularly%20those%20exhibiting%0Ahigh%20critical%20temperature%20%28%24T_c%24%29%2C%20has%20been%20a%20vibrant%20area%20of%20study%20within%20the%0Afield%20of%20condensed%20matter%20physics.%20Conventional%20approaches%20primarily%20rely%20on%0Aphysical%20intuition%20to%20search%20for%20potential%20superconductors%20within%20the%20existing%0Adatabases.%20However%2C%20the%20known%20materials%20only%20scratch%20the%20surface%20of%20the%0Aextensive%20array%20of%20possibilities%20within%20the%20realm%20of%20materials.%20Here%2C%20we%0Adevelop%20an%20AI%20search%20engine%20that%20integrates%20deep%20model%20pre-training%20and%0Afine-tuning%20techniques%2C%20diffusion%20models%2C%20and%20physics-based%20approaches%20%28e.g.%2C%0Afirst-principles%20electronic%20structure%20calculation%29%20for%20discovery%20of%20high-%24T_c%24%0Asuperconductors.%20Utilizing%20this%20AI%20search%20engine%2C%20we%20have%20obtained%2074%0Adynamically%20stable%20materials%20with%20critical%20temperatures%20predicted%20by%20the%20AI%0Amodel%20to%20be%20%24T_c%20%5Cgeq%24%2015%20K%20based%20on%20a%20very%20small%20set%20of%20samples.%20Notably%2C%0Athese%20materials%20are%20not%20contained%20in%20any%20existing%20dataset.%20Furthermore%2C%20we%0Aanalyze%20trends%20in%20our%20dataset%20and%20individual%20materials%20including%20B%24_4%24CN%24_3%24%0Aand%20B%24_5%24CN%24_2%24%20whose%20%24T_c%24s%20are%2024.08%20K%20and%2015.93%20K%2C%20respectively.%20We%0Ademonstrate%20that%20AI%20technique%20can%20discover%20a%20set%20of%20new%20high-%24T_c%24%0Asuperconductors%2C%20outline%20its%20potential%20for%20accelerating%20discovery%20of%20the%0Amaterials%20with%20targeted%20properties.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08065v1&entry.124074799=Read"},
{"title": "Q-value Regularized Decision ConvFormer for Offline Reinforcement\n  Learning", "author": "Teng Yan and Zhendong Ruan and Yaobang Cai and Yu Han and Wenxian Li and Yang Zhang", "abstract": "  As a data-driven paradigm, offline reinforcement learning (Offline RL) has\nbeen formulated as sequence modeling, where the Decision Transformer (DT) has\ndemonstrated exceptional capabilities. Unlike previous reinforcement learning\nmethods that fit value functions or compute policy gradients, DT adjusts the\nautoregressive model based on the expected returns, past states, and actions,\nusing a causally masked Transformer to output the optimal action. However, due\nto the inconsistency between the sampled returns within a single trajectory and\nthe optimal returns across multiple trajectories, it is challenging to set an\nexpected return to output the optimal action and stitch together suboptimal\ntrajectories. Decision ConvFormer (DC) is easier to understand in the context\nof modeling RL trajectories within a Markov Decision Process compared to DT. We\npropose the Q-value Regularized Decision ConvFormer (QDC), which combines the\nunderstanding of RL trajectories by DC and incorporates a term that maximizes\naction values using dynamic programming methods during training. This ensures\nthat the expected returns of the sampled actions are consistent with the\noptimal returns. QDC achieves excellent performance on the D4RL benchmark,\noutperforming or approaching the optimal level in all tested environments. It\nparticularly demonstrates outstanding competitiveness in trajectory stitching\ncapability.\n", "link": "http://arxiv.org/abs/2409.08062v1", "date": "2024-09-12", "relevancy": 0.9026, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4641}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4457}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Q-value%20Regularized%20Decision%20ConvFormer%20for%20Offline%20Reinforcement%0A%20%20Learning&body=Title%3A%20Q-value%20Regularized%20Decision%20ConvFormer%20for%20Offline%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Teng%20Yan%20and%20Zhendong%20Ruan%20and%20Yaobang%20Cai%20and%20Yu%20Han%20and%20Wenxian%20Li%20and%20Yang%20Zhang%0AAbstract%3A%20%20%20As%20a%20data-driven%20paradigm%2C%20offline%20reinforcement%20learning%20%28Offline%20RL%29%20has%0Abeen%20formulated%20as%20sequence%20modeling%2C%20where%20the%20Decision%20Transformer%20%28DT%29%20has%0Ademonstrated%20exceptional%20capabilities.%20Unlike%20previous%20reinforcement%20learning%0Amethods%20that%20fit%20value%20functions%20or%20compute%20policy%20gradients%2C%20DT%20adjusts%20the%0Aautoregressive%20model%20based%20on%20the%20expected%20returns%2C%20past%20states%2C%20and%20actions%2C%0Ausing%20a%20causally%20masked%20Transformer%20to%20output%20the%20optimal%20action.%20However%2C%20due%0Ato%20the%20inconsistency%20between%20the%20sampled%20returns%20within%20a%20single%20trajectory%20and%0Athe%20optimal%20returns%20across%20multiple%20trajectories%2C%20it%20is%20challenging%20to%20set%20an%0Aexpected%20return%20to%20output%20the%20optimal%20action%20and%20stitch%20together%20suboptimal%0Atrajectories.%20Decision%20ConvFormer%20%28DC%29%20is%20easier%20to%20understand%20in%20the%20context%0Aof%20modeling%20RL%20trajectories%20within%20a%20Markov%20Decision%20Process%20compared%20to%20DT.%20We%0Apropose%20the%20Q-value%20Regularized%20Decision%20ConvFormer%20%28QDC%29%2C%20which%20combines%20the%0Aunderstanding%20of%20RL%20trajectories%20by%20DC%20and%20incorporates%20a%20term%20that%20maximizes%0Aaction%20values%20using%20dynamic%20programming%20methods%20during%20training.%20This%20ensures%0Athat%20the%20expected%20returns%20of%20the%20sampled%20actions%20are%20consistent%20with%20the%0Aoptimal%20returns.%20QDC%20achieves%20excellent%20performance%20on%20the%20D4RL%20benchmark%2C%0Aoutperforming%20or%20approaching%20the%20optimal%20level%20in%20all%20tested%20environments.%20It%0Aparticularly%20demonstrates%20outstanding%20competitiveness%20in%20trajectory%20stitching%0Acapability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08062v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQ-value%2520Regularized%2520Decision%2520ConvFormer%2520for%2520Offline%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DTeng%2520Yan%2520and%2520Zhendong%2520Ruan%2520and%2520Yaobang%2520Cai%2520and%2520Yu%2520Han%2520and%2520Wenxian%2520Li%2520and%2520Yang%2520Zhang%26entry.1292438233%3D%2520%2520As%2520a%2520data-driven%2520paradigm%252C%2520offline%2520reinforcement%2520learning%2520%2528Offline%2520RL%2529%2520has%250Abeen%2520formulated%2520as%2520sequence%2520modeling%252C%2520where%2520the%2520Decision%2520Transformer%2520%2528DT%2529%2520has%250Ademonstrated%2520exceptional%2520capabilities.%2520Unlike%2520previous%2520reinforcement%2520learning%250Amethods%2520that%2520fit%2520value%2520functions%2520or%2520compute%2520policy%2520gradients%252C%2520DT%2520adjusts%2520the%250Aautoregressive%2520model%2520based%2520on%2520the%2520expected%2520returns%252C%2520past%2520states%252C%2520and%2520actions%252C%250Ausing%2520a%2520causally%2520masked%2520Transformer%2520to%2520output%2520the%2520optimal%2520action.%2520However%252C%2520due%250Ato%2520the%2520inconsistency%2520between%2520the%2520sampled%2520returns%2520within%2520a%2520single%2520trajectory%2520and%250Athe%2520optimal%2520returns%2520across%2520multiple%2520trajectories%252C%2520it%2520is%2520challenging%2520to%2520set%2520an%250Aexpected%2520return%2520to%2520output%2520the%2520optimal%2520action%2520and%2520stitch%2520together%2520suboptimal%250Atrajectories.%2520Decision%2520ConvFormer%2520%2528DC%2529%2520is%2520easier%2520to%2520understand%2520in%2520the%2520context%250Aof%2520modeling%2520RL%2520trajectories%2520within%2520a%2520Markov%2520Decision%2520Process%2520compared%2520to%2520DT.%2520We%250Apropose%2520the%2520Q-value%2520Regularized%2520Decision%2520ConvFormer%2520%2528QDC%2529%252C%2520which%2520combines%2520the%250Aunderstanding%2520of%2520RL%2520trajectories%2520by%2520DC%2520and%2520incorporates%2520a%2520term%2520that%2520maximizes%250Aaction%2520values%2520using%2520dynamic%2520programming%2520methods%2520during%2520training.%2520This%2520ensures%250Athat%2520the%2520expected%2520returns%2520of%2520the%2520sampled%2520actions%2520are%2520consistent%2520with%2520the%250Aoptimal%2520returns.%2520QDC%2520achieves%2520excellent%2520performance%2520on%2520the%2520D4RL%2520benchmark%252C%250Aoutperforming%2520or%2520approaching%2520the%2520optimal%2520level%2520in%2520all%2520tested%2520environments.%2520It%250Aparticularly%2520demonstrates%2520outstanding%2520competitiveness%2520in%2520trajectory%2520stitching%250Acapability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08062v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Q-value%20Regularized%20Decision%20ConvFormer%20for%20Offline%20Reinforcement%0A%20%20Learning&entry.906535625=Teng%20Yan%20and%20Zhendong%20Ruan%20and%20Yaobang%20Cai%20and%20Yu%20Han%20and%20Wenxian%20Li%20and%20Yang%20Zhang&entry.1292438233=%20%20As%20a%20data-driven%20paradigm%2C%20offline%20reinforcement%20learning%20%28Offline%20RL%29%20has%0Abeen%20formulated%20as%20sequence%20modeling%2C%20where%20the%20Decision%20Transformer%20%28DT%29%20has%0Ademonstrated%20exceptional%20capabilities.%20Unlike%20previous%20reinforcement%20learning%0Amethods%20that%20fit%20value%20functions%20or%20compute%20policy%20gradients%2C%20DT%20adjusts%20the%0Aautoregressive%20model%20based%20on%20the%20expected%20returns%2C%20past%20states%2C%20and%20actions%2C%0Ausing%20a%20causally%20masked%20Transformer%20to%20output%20the%20optimal%20action.%20However%2C%20due%0Ato%20the%20inconsistency%20between%20the%20sampled%20returns%20within%20a%20single%20trajectory%20and%0Athe%20optimal%20returns%20across%20multiple%20trajectories%2C%20it%20is%20challenging%20to%20set%20an%0Aexpected%20return%20to%20output%20the%20optimal%20action%20and%20stitch%20together%20suboptimal%0Atrajectories.%20Decision%20ConvFormer%20%28DC%29%20is%20easier%20to%20understand%20in%20the%20context%0Aof%20modeling%20RL%20trajectories%20within%20a%20Markov%20Decision%20Process%20compared%20to%20DT.%20We%0Apropose%20the%20Q-value%20Regularized%20Decision%20ConvFormer%20%28QDC%29%2C%20which%20combines%20the%0Aunderstanding%20of%20RL%20trajectories%20by%20DC%20and%20incorporates%20a%20term%20that%20maximizes%0Aaction%20values%20using%20dynamic%20programming%20methods%20during%20training.%20This%20ensures%0Athat%20the%20expected%20returns%20of%20the%20sampled%20actions%20are%20consistent%20with%20the%0Aoptimal%20returns.%20QDC%20achieves%20excellent%20performance%20on%20the%20D4RL%20benchmark%2C%0Aoutperforming%20or%20approaching%20the%20optimal%20level%20in%20all%20tested%20environments.%20It%0Aparticularly%20demonstrates%20outstanding%20competitiveness%20in%20trajectory%20stitching%0Acapability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08062v1&entry.124074799=Read"},
{"title": "Show-o: One Single Transformer to Unify Multimodal Understanding and\n  Generation", "author": "Jinheng Xie and Weijia Mao and Zechen Bai and David Junhao Zhang and Weihao Wang and Kevin Qinghong Lin and Yuchao Gu and Zhijie Chen and Zhenheng Yang and Mike Zheng Shou", "abstract": "  We present a unified transformer, i.e., Show-o, that unifies multimodal\nunderstanding and generation. Unlike fully autoregressive models, Show-o\nunifies autoregressive and (discrete) diffusion modeling to adaptively handle\ninputs and outputs of various and mixed modalities. The unified model flexibly\nsupports a wide range of vision-language tasks including visual\nquestion-answering, text-to-image generation, text-guided\ninpainting/extrapolation, and mixed-modality generation. Across various\nbenchmarks, it demonstrates comparable or superior performance to existing\nindividual models with an equivalent or larger number of parameters tailored\nfor understanding or generation. This significantly highlights its potential as\na next-generation foundation model. Code and models are released at\nhttps://github.com/showlab/Show-o.\n", "link": "http://arxiv.org/abs/2408.12528v4", "date": "2024-09-12", "relevancy": 1.7272, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6043}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.579}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Show-o%3A%20One%20Single%20Transformer%20to%20Unify%20Multimodal%20Understanding%20and%0A%20%20Generation&body=Title%3A%20Show-o%3A%20One%20Single%20Transformer%20to%20Unify%20Multimodal%20Understanding%20and%0A%20%20Generation%0AAuthor%3A%20Jinheng%20Xie%20and%20Weijia%20Mao%20and%20Zechen%20Bai%20and%20David%20Junhao%20Zhang%20and%20Weihao%20Wang%20and%20Kevin%20Qinghong%20Lin%20and%20Yuchao%20Gu%20and%20Zhijie%20Chen%20and%20Zhenheng%20Yang%20and%20Mike%20Zheng%20Shou%0AAbstract%3A%20%20%20We%20present%20a%20unified%20transformer%2C%20i.e.%2C%20Show-o%2C%20that%20unifies%20multimodal%0Aunderstanding%20and%20generation.%20Unlike%20fully%20autoregressive%20models%2C%20Show-o%0Aunifies%20autoregressive%20and%20%28discrete%29%20diffusion%20modeling%20to%20adaptively%20handle%0Ainputs%20and%20outputs%20of%20various%20and%20mixed%20modalities.%20The%20unified%20model%20flexibly%0Asupports%20a%20wide%20range%20of%20vision-language%20tasks%20including%20visual%0Aquestion-answering%2C%20text-to-image%20generation%2C%20text-guided%0Ainpainting/extrapolation%2C%20and%20mixed-modality%20generation.%20Across%20various%0Abenchmarks%2C%20it%20demonstrates%20comparable%20or%20superior%20performance%20to%20existing%0Aindividual%20models%20with%20an%20equivalent%20or%20larger%20number%20of%20parameters%20tailored%0Afor%20understanding%20or%20generation.%20This%20significantly%20highlights%20its%20potential%20as%0Aa%20next-generation%20foundation%20model.%20Code%20and%20models%20are%20released%20at%0Ahttps%3A//github.com/showlab/Show-o.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12528v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShow-o%253A%2520One%2520Single%2520Transformer%2520to%2520Unify%2520Multimodal%2520Understanding%2520and%250A%2520%2520Generation%26entry.906535625%3DJinheng%2520Xie%2520and%2520Weijia%2520Mao%2520and%2520Zechen%2520Bai%2520and%2520David%2520Junhao%2520Zhang%2520and%2520Weihao%2520Wang%2520and%2520Kevin%2520Qinghong%2520Lin%2520and%2520Yuchao%2520Gu%2520and%2520Zhijie%2520Chen%2520and%2520Zhenheng%2520Yang%2520and%2520Mike%2520Zheng%2520Shou%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520unified%2520transformer%252C%2520i.e.%252C%2520Show-o%252C%2520that%2520unifies%2520multimodal%250Aunderstanding%2520and%2520generation.%2520Unlike%2520fully%2520autoregressive%2520models%252C%2520Show-o%250Aunifies%2520autoregressive%2520and%2520%2528discrete%2529%2520diffusion%2520modeling%2520to%2520adaptively%2520handle%250Ainputs%2520and%2520outputs%2520of%2520various%2520and%2520mixed%2520modalities.%2520The%2520unified%2520model%2520flexibly%250Asupports%2520a%2520wide%2520range%2520of%2520vision-language%2520tasks%2520including%2520visual%250Aquestion-answering%252C%2520text-to-image%2520generation%252C%2520text-guided%250Ainpainting/extrapolation%252C%2520and%2520mixed-modality%2520generation.%2520Across%2520various%250Abenchmarks%252C%2520it%2520demonstrates%2520comparable%2520or%2520superior%2520performance%2520to%2520existing%250Aindividual%2520models%2520with%2520an%2520equivalent%2520or%2520larger%2520number%2520of%2520parameters%2520tailored%250Afor%2520understanding%2520or%2520generation.%2520This%2520significantly%2520highlights%2520its%2520potential%2520as%250Aa%2520next-generation%2520foundation%2520model.%2520Code%2520and%2520models%2520are%2520released%2520at%250Ahttps%253A//github.com/showlab/Show-o.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12528v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Show-o%3A%20One%20Single%20Transformer%20to%20Unify%20Multimodal%20Understanding%20and%0A%20%20Generation&entry.906535625=Jinheng%20Xie%20and%20Weijia%20Mao%20and%20Zechen%20Bai%20and%20David%20Junhao%20Zhang%20and%20Weihao%20Wang%20and%20Kevin%20Qinghong%20Lin%20and%20Yuchao%20Gu%20and%20Zhijie%20Chen%20and%20Zhenheng%20Yang%20and%20Mike%20Zheng%20Shou&entry.1292438233=%20%20We%20present%20a%20unified%20transformer%2C%20i.e.%2C%20Show-o%2C%20that%20unifies%20multimodal%0Aunderstanding%20and%20generation.%20Unlike%20fully%20autoregressive%20models%2C%20Show-o%0Aunifies%20autoregressive%20and%20%28discrete%29%20diffusion%20modeling%20to%20adaptively%20handle%0Ainputs%20and%20outputs%20of%20various%20and%20mixed%20modalities.%20The%20unified%20model%20flexibly%0Asupports%20a%20wide%20range%20of%20vision-language%20tasks%20including%20visual%0Aquestion-answering%2C%20text-to-image%20generation%2C%20text-guided%0Ainpainting/extrapolation%2C%20and%20mixed-modality%20generation.%20Across%20various%0Abenchmarks%2C%20it%20demonstrates%20comparable%20or%20superior%20performance%20to%20existing%0Aindividual%20models%20with%20an%20equivalent%20or%20larger%20number%20of%20parameters%20tailored%0Afor%20understanding%20or%20generation.%20This%20significantly%20highlights%20its%20potential%20as%0Aa%20next-generation%20foundation%20model.%20Code%20and%20models%20are%20released%20at%0Ahttps%3A//github.com/showlab/Show-o.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12528v4&entry.124074799=Read"},
{"title": "Reinforcement Learning Discovers Efficient Decentralized Graph Path\n  Search Strategies", "author": "Alexei Pisacane and Victor-Alexandru Darvariu and Mirco Musolesi", "abstract": "  Graph path search is a classic computer science problem that has been\nrecently approached with Reinforcement Learning (RL) due to its potential to\noutperform prior methods. Existing RL techniques typically assume a global view\nof the network, which is not suitable for large-scale, dynamic, and\nprivacy-sensitive settings. An area of particular interest is search in social\nnetworks due to its numerous applications. Inspired by seminal work in\nexperimental sociology, which showed that decentralized yet efficient search is\npossible in social networks, we frame the problem as a collaborative task\nbetween multiple agents equipped with a limited local view of the network. We\npropose a multi-agent approach for graph path search that successfully\nleverages both homophily and structural heterogeneity. Our experiments, carried\nout over synthetic and real-world social networks, demonstrate that our model\nsignificantly outperforms learned and heuristic baselines. Furthermore, our\nresults show that meaningful embeddings for graph navigation can be constructed\nusing reward-driven learning.\n", "link": "http://arxiv.org/abs/2409.07932v1", "date": "2024-09-12", "relevancy": 1.4759, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5334}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4841}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4702}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcement%20Learning%20Discovers%20Efficient%20Decentralized%20Graph%20Path%0A%20%20Search%20Strategies&body=Title%3A%20Reinforcement%20Learning%20Discovers%20Efficient%20Decentralized%20Graph%20Path%0A%20%20Search%20Strategies%0AAuthor%3A%20Alexei%20Pisacane%20and%20Victor-Alexandru%20Darvariu%20and%20Mirco%20Musolesi%0AAbstract%3A%20%20%20Graph%20path%20search%20is%20a%20classic%20computer%20science%20problem%20that%20has%20been%0Arecently%20approached%20with%20Reinforcement%20Learning%20%28RL%29%20due%20to%20its%20potential%20to%0Aoutperform%20prior%20methods.%20Existing%20RL%20techniques%20typically%20assume%20a%20global%20view%0Aof%20the%20network%2C%20which%20is%20not%20suitable%20for%20large-scale%2C%20dynamic%2C%20and%0Aprivacy-sensitive%20settings.%20An%20area%20of%20particular%20interest%20is%20search%20in%20social%0Anetworks%20due%20to%20its%20numerous%20applications.%20Inspired%20by%20seminal%20work%20in%0Aexperimental%20sociology%2C%20which%20showed%20that%20decentralized%20yet%20efficient%20search%20is%0Apossible%20in%20social%20networks%2C%20we%20frame%20the%20problem%20as%20a%20collaborative%20task%0Abetween%20multiple%20agents%20equipped%20with%20a%20limited%20local%20view%20of%20the%20network.%20We%0Apropose%20a%20multi-agent%20approach%20for%20graph%20path%20search%20that%20successfully%0Aleverages%20both%20homophily%20and%20structural%20heterogeneity.%20Our%20experiments%2C%20carried%0Aout%20over%20synthetic%20and%20real-world%20social%20networks%2C%20demonstrate%20that%20our%20model%0Asignificantly%20outperforms%20learned%20and%20heuristic%20baselines.%20Furthermore%2C%20our%0Aresults%20show%20that%20meaningful%20embeddings%20for%20graph%20navigation%20can%20be%20constructed%0Ausing%20reward-driven%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07932v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcement%2520Learning%2520Discovers%2520Efficient%2520Decentralized%2520Graph%2520Path%250A%2520%2520Search%2520Strategies%26entry.906535625%3DAlexei%2520Pisacane%2520and%2520Victor-Alexandru%2520Darvariu%2520and%2520Mirco%2520Musolesi%26entry.1292438233%3D%2520%2520Graph%2520path%2520search%2520is%2520a%2520classic%2520computer%2520science%2520problem%2520that%2520has%2520been%250Arecently%2520approached%2520with%2520Reinforcement%2520Learning%2520%2528RL%2529%2520due%2520to%2520its%2520potential%2520to%250Aoutperform%2520prior%2520methods.%2520Existing%2520RL%2520techniques%2520typically%2520assume%2520a%2520global%2520view%250Aof%2520the%2520network%252C%2520which%2520is%2520not%2520suitable%2520for%2520large-scale%252C%2520dynamic%252C%2520and%250Aprivacy-sensitive%2520settings.%2520An%2520area%2520of%2520particular%2520interest%2520is%2520search%2520in%2520social%250Anetworks%2520due%2520to%2520its%2520numerous%2520applications.%2520Inspired%2520by%2520seminal%2520work%2520in%250Aexperimental%2520sociology%252C%2520which%2520showed%2520that%2520decentralized%2520yet%2520efficient%2520search%2520is%250Apossible%2520in%2520social%2520networks%252C%2520we%2520frame%2520the%2520problem%2520as%2520a%2520collaborative%2520task%250Abetween%2520multiple%2520agents%2520equipped%2520with%2520a%2520limited%2520local%2520view%2520of%2520the%2520network.%2520We%250Apropose%2520a%2520multi-agent%2520approach%2520for%2520graph%2520path%2520search%2520that%2520successfully%250Aleverages%2520both%2520homophily%2520and%2520structural%2520heterogeneity.%2520Our%2520experiments%252C%2520carried%250Aout%2520over%2520synthetic%2520and%2520real-world%2520social%2520networks%252C%2520demonstrate%2520that%2520our%2520model%250Asignificantly%2520outperforms%2520learned%2520and%2520heuristic%2520baselines.%2520Furthermore%252C%2520our%250Aresults%2520show%2520that%2520meaningful%2520embeddings%2520for%2520graph%2520navigation%2520can%2520be%2520constructed%250Ausing%2520reward-driven%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07932v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcement%20Learning%20Discovers%20Efficient%20Decentralized%20Graph%20Path%0A%20%20Search%20Strategies&entry.906535625=Alexei%20Pisacane%20and%20Victor-Alexandru%20Darvariu%20and%20Mirco%20Musolesi&entry.1292438233=%20%20Graph%20path%20search%20is%20a%20classic%20computer%20science%20problem%20that%20has%20been%0Arecently%20approached%20with%20Reinforcement%20Learning%20%28RL%29%20due%20to%20its%20potential%20to%0Aoutperform%20prior%20methods.%20Existing%20RL%20techniques%20typically%20assume%20a%20global%20view%0Aof%20the%20network%2C%20which%20is%20not%20suitable%20for%20large-scale%2C%20dynamic%2C%20and%0Aprivacy-sensitive%20settings.%20An%20area%20of%20particular%20interest%20is%20search%20in%20social%0Anetworks%20due%20to%20its%20numerous%20applications.%20Inspired%20by%20seminal%20work%20in%0Aexperimental%20sociology%2C%20which%20showed%20that%20decentralized%20yet%20efficient%20search%20is%0Apossible%20in%20social%20networks%2C%20we%20frame%20the%20problem%20as%20a%20collaborative%20task%0Abetween%20multiple%20agents%20equipped%20with%20a%20limited%20local%20view%20of%20the%20network.%20We%0Apropose%20a%20multi-agent%20approach%20for%20graph%20path%20search%20that%20successfully%0Aleverages%20both%20homophily%20and%20structural%20heterogeneity.%20Our%20experiments%2C%20carried%0Aout%20over%20synthetic%20and%20real-world%20social%20networks%2C%20demonstrate%20that%20our%20model%0Asignificantly%20outperforms%20learned%20and%20heuristic%20baselines.%20Furthermore%2C%20our%0Aresults%20show%20that%20meaningful%20embeddings%20for%20graph%20navigation%20can%20be%20constructed%0Ausing%20reward-driven%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07932v1&entry.124074799=Read"},
{"title": "A Survey of Behavior Learning Applications in Robotics -- State of the\n  Art and Perspectives", "author": "Alexander Fabisch and Christoph Petzoldt and Marc Otto and Frank Kirchner", "abstract": "  Recent success of machine learning in many domains has been overwhelming,\nwhich often leads to false expectations regarding the capabilities of behavior\nlearning in robotics. In this survey, we analyze the current state of machine\nlearning for robotic behaviors. We will give a broad overview of behaviors that\nhave been learned and used on real robots. Our focus is on kinematically or\nsensorially complex robots. That includes humanoid robots or parts of humanoid\nrobots, for example, legged robots or robotic arms. We will classify presented\nbehaviors according to various categories and we will draw conclusions about\nwhat can be learned and what should be learned. Furthermore, we will give an\noutlook on problems that are challenging today but might be solved by machine\nlearning in the future and argue that classical robotics and other approaches\nfrom artificial intelligence should be integrated more with machine learning to\nform complete, autonomous systems.\n", "link": "http://arxiv.org/abs/1906.01868v3", "date": "2024-09-12", "relevancy": 1.5947, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5915}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5583}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4969}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20of%20Behavior%20Learning%20Applications%20in%20Robotics%20--%20State%20of%20the%0A%20%20Art%20and%20Perspectives&body=Title%3A%20A%20Survey%20of%20Behavior%20Learning%20Applications%20in%20Robotics%20--%20State%20of%20the%0A%20%20Art%20and%20Perspectives%0AAuthor%3A%20Alexander%20Fabisch%20and%20Christoph%20Petzoldt%20and%20Marc%20Otto%20and%20Frank%20Kirchner%0AAbstract%3A%20%20%20Recent%20success%20of%20machine%20learning%20in%20many%20domains%20has%20been%20overwhelming%2C%0Awhich%20often%20leads%20to%20false%20expectations%20regarding%20the%20capabilities%20of%20behavior%0Alearning%20in%20robotics.%20In%20this%20survey%2C%20we%20analyze%20the%20current%20state%20of%20machine%0Alearning%20for%20robotic%20behaviors.%20We%20will%20give%20a%20broad%20overview%20of%20behaviors%20that%0Ahave%20been%20learned%20and%20used%20on%20real%20robots.%20Our%20focus%20is%20on%20kinematically%20or%0Asensorially%20complex%20robots.%20That%20includes%20humanoid%20robots%20or%20parts%20of%20humanoid%0Arobots%2C%20for%20example%2C%20legged%20robots%20or%20robotic%20arms.%20We%20will%20classify%20presented%0Abehaviors%20according%20to%20various%20categories%20and%20we%20will%20draw%20conclusions%20about%0Awhat%20can%20be%20learned%20and%20what%20should%20be%20learned.%20Furthermore%2C%20we%20will%20give%20an%0Aoutlook%20on%20problems%20that%20are%20challenging%20today%20but%20might%20be%20solved%20by%20machine%0Alearning%20in%20the%20future%20and%20argue%20that%20classical%20robotics%20and%20other%20approaches%0Afrom%20artificial%20intelligence%20should%20be%20integrated%20more%20with%20machine%20learning%20to%0Aform%20complete%2C%20autonomous%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/1906.01868v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520of%2520Behavior%2520Learning%2520Applications%2520in%2520Robotics%2520--%2520State%2520of%2520the%250A%2520%2520Art%2520and%2520Perspectives%26entry.906535625%3DAlexander%2520Fabisch%2520and%2520Christoph%2520Petzoldt%2520and%2520Marc%2520Otto%2520and%2520Frank%2520Kirchner%26entry.1292438233%3D%2520%2520Recent%2520success%2520of%2520machine%2520learning%2520in%2520many%2520domains%2520has%2520been%2520overwhelming%252C%250Awhich%2520often%2520leads%2520to%2520false%2520expectations%2520regarding%2520the%2520capabilities%2520of%2520behavior%250Alearning%2520in%2520robotics.%2520In%2520this%2520survey%252C%2520we%2520analyze%2520the%2520current%2520state%2520of%2520machine%250Alearning%2520for%2520robotic%2520behaviors.%2520We%2520will%2520give%2520a%2520broad%2520overview%2520of%2520behaviors%2520that%250Ahave%2520been%2520learned%2520and%2520used%2520on%2520real%2520robots.%2520Our%2520focus%2520is%2520on%2520kinematically%2520or%250Asensorially%2520complex%2520robots.%2520That%2520includes%2520humanoid%2520robots%2520or%2520parts%2520of%2520humanoid%250Arobots%252C%2520for%2520example%252C%2520legged%2520robots%2520or%2520robotic%2520arms.%2520We%2520will%2520classify%2520presented%250Abehaviors%2520according%2520to%2520various%2520categories%2520and%2520we%2520will%2520draw%2520conclusions%2520about%250Awhat%2520can%2520be%2520learned%2520and%2520what%2520should%2520be%2520learned.%2520Furthermore%252C%2520we%2520will%2520give%2520an%250Aoutlook%2520on%2520problems%2520that%2520are%2520challenging%2520today%2520but%2520might%2520be%2520solved%2520by%2520machine%250Alearning%2520in%2520the%2520future%2520and%2520argue%2520that%2520classical%2520robotics%2520and%2520other%2520approaches%250Afrom%2520artificial%2520intelligence%2520should%2520be%2520integrated%2520more%2520with%2520machine%2520learning%2520to%250Aform%2520complete%252C%2520autonomous%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/1906.01868v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20of%20Behavior%20Learning%20Applications%20in%20Robotics%20--%20State%20of%20the%0A%20%20Art%20and%20Perspectives&entry.906535625=Alexander%20Fabisch%20and%20Christoph%20Petzoldt%20and%20Marc%20Otto%20and%20Frank%20Kirchner&entry.1292438233=%20%20Recent%20success%20of%20machine%20learning%20in%20many%20domains%20has%20been%20overwhelming%2C%0Awhich%20often%20leads%20to%20false%20expectations%20regarding%20the%20capabilities%20of%20behavior%0Alearning%20in%20robotics.%20In%20this%20survey%2C%20we%20analyze%20the%20current%20state%20of%20machine%0Alearning%20for%20robotic%20behaviors.%20We%20will%20give%20a%20broad%20overview%20of%20behaviors%20that%0Ahave%20been%20learned%20and%20used%20on%20real%20robots.%20Our%20focus%20is%20on%20kinematically%20or%0Asensorially%20complex%20robots.%20That%20includes%20humanoid%20robots%20or%20parts%20of%20humanoid%0Arobots%2C%20for%20example%2C%20legged%20robots%20or%20robotic%20arms.%20We%20will%20classify%20presented%0Abehaviors%20according%20to%20various%20categories%20and%20we%20will%20draw%20conclusions%20about%0Awhat%20can%20be%20learned%20and%20what%20should%20be%20learned.%20Furthermore%2C%20we%20will%20give%20an%0Aoutlook%20on%20problems%20that%20are%20challenging%20today%20but%20might%20be%20solved%20by%20machine%0Alearning%20in%20the%20future%20and%20argue%20that%20classical%20robotics%20and%20other%20approaches%0Afrom%20artificial%20intelligence%20should%20be%20integrated%20more%20with%20machine%20learning%20to%0Aform%20complete%2C%20autonomous%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/1906.01868v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


