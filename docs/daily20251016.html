<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251015.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "GARField: Addressing the visual Sim-to-Real gap in garment manipulation\n  with mesh-attached radiance fields", "author": "Donatien Delehelle and Darwin G. Caldwell and Fei Chen", "abstract": "  While humans intuitively manipulate garments and other textile items swiftly\nand accurately, it is a significant challenge for robots. A factor crucial to\nhuman performance is the ability to imagine, a priori, the intended result of\nthe manipulation intents and hence develop predictions on the garment pose.\nThat ability allows us to plan from highly obstructed states, adapt our plans\nas we collect more information and react swiftly to unforeseen circumstances.\nConversely, robots struggle to establish such intuitions and form tight links\nbetween plans and observations. We can partly attribute this to the high cost\nof obtaining densely labelled data for textile manipulation, both in quality\nand quantity. The problem of data collection is a long-standing issue in\ndata-based approaches to garment manipulation. As of today, generating\nhigh-quality and labelled garment manipulation data is mainly attempted through\nadvanced data capture procedures that create simplified state estimations from\nreal-world observations. However, this work proposes a novel approach to the\nproblem by generating real-world observations from object states. To achieve\nthis, we present GARField (Garment Attached Radiance Field), the first\ndifferentiable rendering architecture, to our knowledge, for data generation\nfrom simulated states stored as triangle meshes. Code is available on\nhttps://ddonatien.github.io/garfield-website/\n", "link": "http://arxiv.org/abs/2410.05038v3", "date": "2025-10-15", "relevancy": 3.2275, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6771}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6768}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5826}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GARField%3A%20Addressing%20the%20visual%20Sim-to-Real%20gap%20in%20garment%20manipulation%0A%20%20with%20mesh-attached%20radiance%20fields&body=Title%3A%20GARField%3A%20Addressing%20the%20visual%20Sim-to-Real%20gap%20in%20garment%20manipulation%0A%20%20with%20mesh-attached%20radiance%20fields%0AAuthor%3A%20Donatien%20Delehelle%20and%20Darwin%20G.%20Caldwell%20and%20Fei%20Chen%0AAbstract%3A%20%20%20While%20humans%20intuitively%20manipulate%20garments%20and%20other%20textile%20items%20swiftly%0Aand%20accurately%2C%20it%20is%20a%20significant%20challenge%20for%20robots.%20A%20factor%20crucial%20to%0Ahuman%20performance%20is%20the%20ability%20to%20imagine%2C%20a%20priori%2C%20the%20intended%20result%20of%0Athe%20manipulation%20intents%20and%20hence%20develop%20predictions%20on%20the%20garment%20pose.%0AThat%20ability%20allows%20us%20to%20plan%20from%20highly%20obstructed%20states%2C%20adapt%20our%20plans%0Aas%20we%20collect%20more%20information%20and%20react%20swiftly%20to%20unforeseen%20circumstances.%0AConversely%2C%20robots%20struggle%20to%20establish%20such%20intuitions%20and%20form%20tight%20links%0Abetween%20plans%20and%20observations.%20We%20can%20partly%20attribute%20this%20to%20the%20high%20cost%0Aof%20obtaining%20densely%20labelled%20data%20for%20textile%20manipulation%2C%20both%20in%20quality%0Aand%20quantity.%20The%20problem%20of%20data%20collection%20is%20a%20long-standing%20issue%20in%0Adata-based%20approaches%20to%20garment%20manipulation.%20As%20of%20today%2C%20generating%0Ahigh-quality%20and%20labelled%20garment%20manipulation%20data%20is%20mainly%20attempted%20through%0Aadvanced%20data%20capture%20procedures%20that%20create%20simplified%20state%20estimations%20from%0Areal-world%20observations.%20However%2C%20this%20work%20proposes%20a%20novel%20approach%20to%20the%0Aproblem%20by%20generating%20real-world%20observations%20from%20object%20states.%20To%20achieve%0Athis%2C%20we%20present%20GARField%20%28Garment%20Attached%20Radiance%20Field%29%2C%20the%20first%0Adifferentiable%20rendering%20architecture%2C%20to%20our%20knowledge%2C%20for%20data%20generation%0Afrom%20simulated%20states%20stored%20as%20triangle%20meshes.%20Code%20is%20available%20on%0Ahttps%3A//ddonatien.github.io/garfield-website/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05038v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGARField%253A%2520Addressing%2520the%2520visual%2520Sim-to-Real%2520gap%2520in%2520garment%2520manipulation%250A%2520%2520with%2520mesh-attached%2520radiance%2520fields%26entry.906535625%3DDonatien%2520Delehelle%2520and%2520Darwin%2520G.%2520Caldwell%2520and%2520Fei%2520Chen%26entry.1292438233%3D%2520%2520While%2520humans%2520intuitively%2520manipulate%2520garments%2520and%2520other%2520textile%2520items%2520swiftly%250Aand%2520accurately%252C%2520it%2520is%2520a%2520significant%2520challenge%2520for%2520robots.%2520A%2520factor%2520crucial%2520to%250Ahuman%2520performance%2520is%2520the%2520ability%2520to%2520imagine%252C%2520a%2520priori%252C%2520the%2520intended%2520result%2520of%250Athe%2520manipulation%2520intents%2520and%2520hence%2520develop%2520predictions%2520on%2520the%2520garment%2520pose.%250AThat%2520ability%2520allows%2520us%2520to%2520plan%2520from%2520highly%2520obstructed%2520states%252C%2520adapt%2520our%2520plans%250Aas%2520we%2520collect%2520more%2520information%2520and%2520react%2520swiftly%2520to%2520unforeseen%2520circumstances.%250AConversely%252C%2520robots%2520struggle%2520to%2520establish%2520such%2520intuitions%2520and%2520form%2520tight%2520links%250Abetween%2520plans%2520and%2520observations.%2520We%2520can%2520partly%2520attribute%2520this%2520to%2520the%2520high%2520cost%250Aof%2520obtaining%2520densely%2520labelled%2520data%2520for%2520textile%2520manipulation%252C%2520both%2520in%2520quality%250Aand%2520quantity.%2520The%2520problem%2520of%2520data%2520collection%2520is%2520a%2520long-standing%2520issue%2520in%250Adata-based%2520approaches%2520to%2520garment%2520manipulation.%2520As%2520of%2520today%252C%2520generating%250Ahigh-quality%2520and%2520labelled%2520garment%2520manipulation%2520data%2520is%2520mainly%2520attempted%2520through%250Aadvanced%2520data%2520capture%2520procedures%2520that%2520create%2520simplified%2520state%2520estimations%2520from%250Areal-world%2520observations.%2520However%252C%2520this%2520work%2520proposes%2520a%2520novel%2520approach%2520to%2520the%250Aproblem%2520by%2520generating%2520real-world%2520observations%2520from%2520object%2520states.%2520To%2520achieve%250Athis%252C%2520we%2520present%2520GARField%2520%2528Garment%2520Attached%2520Radiance%2520Field%2529%252C%2520the%2520first%250Adifferentiable%2520rendering%2520architecture%252C%2520to%2520our%2520knowledge%252C%2520for%2520data%2520generation%250Afrom%2520simulated%2520states%2520stored%2520as%2520triangle%2520meshes.%2520Code%2520is%2520available%2520on%250Ahttps%253A//ddonatien.github.io/garfield-website/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05038v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GARField%3A%20Addressing%20the%20visual%20Sim-to-Real%20gap%20in%20garment%20manipulation%0A%20%20with%20mesh-attached%20radiance%20fields&entry.906535625=Donatien%20Delehelle%20and%20Darwin%20G.%20Caldwell%20and%20Fei%20Chen&entry.1292438233=%20%20While%20humans%20intuitively%20manipulate%20garments%20and%20other%20textile%20items%20swiftly%0Aand%20accurately%2C%20it%20is%20a%20significant%20challenge%20for%20robots.%20A%20factor%20crucial%20to%0Ahuman%20performance%20is%20the%20ability%20to%20imagine%2C%20a%20priori%2C%20the%20intended%20result%20of%0Athe%20manipulation%20intents%20and%20hence%20develop%20predictions%20on%20the%20garment%20pose.%0AThat%20ability%20allows%20us%20to%20plan%20from%20highly%20obstructed%20states%2C%20adapt%20our%20plans%0Aas%20we%20collect%20more%20information%20and%20react%20swiftly%20to%20unforeseen%20circumstances.%0AConversely%2C%20robots%20struggle%20to%20establish%20such%20intuitions%20and%20form%20tight%20links%0Abetween%20plans%20and%20observations.%20We%20can%20partly%20attribute%20this%20to%20the%20high%20cost%0Aof%20obtaining%20densely%20labelled%20data%20for%20textile%20manipulation%2C%20both%20in%20quality%0Aand%20quantity.%20The%20problem%20of%20data%20collection%20is%20a%20long-standing%20issue%20in%0Adata-based%20approaches%20to%20garment%20manipulation.%20As%20of%20today%2C%20generating%0Ahigh-quality%20and%20labelled%20garment%20manipulation%20data%20is%20mainly%20attempted%20through%0Aadvanced%20data%20capture%20procedures%20that%20create%20simplified%20state%20estimations%20from%0Areal-world%20observations.%20However%2C%20this%20work%20proposes%20a%20novel%20approach%20to%20the%0Aproblem%20by%20generating%20real-world%20observations%20from%20object%20states.%20To%20achieve%0Athis%2C%20we%20present%20GARField%20%28Garment%20Attached%20Radiance%20Field%29%2C%20the%20first%0Adifferentiable%20rendering%20architecture%2C%20to%20our%20knowledge%2C%20for%20data%20generation%0Afrom%20simulated%20states%20stored%20as%20triangle%20meshes.%20Code%20is%20available%20on%0Ahttps%3A//ddonatien.github.io/garfield-website/%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05038v3&entry.124074799=Read"},
{"title": "Leveraging 2D Priors and SDF Guidance for Dynamic Urban Scene Rendering", "author": "Siddharth Tourani and Jayaram Reddy and Akash Kumbar and Satyajit Tourani and Nishant Goyal and Madhava Krishna and N. Dinesh Reddy and Muhammad Haris Khan", "abstract": "  Dynamic scene rendering and reconstruction play a crucial role in computer\nvision and augmented reality. Recent methods based on 3D Gaussian Splatting\n(3DGS), have enabled accurate modeling of dynamic urban scenes, but for urban\nscenes they require both camera and LiDAR data, ground-truth 3D segmentations\nand motion data in the form of tracklets or pre-defined object templates such\nas SMPL. In this work, we explore whether a combination of 2D object agnostic\npriors in the form of depth and point tracking coupled with a signed distance\nfunction (SDF) representation for dynamic objects can be used to relax some of\nthese requirements. We present a novel approach that integrates Signed Distance\nFunctions (SDFs) with 3D Gaussian Splatting (3DGS) to create a more robust\nobject representation by harnessing the strengths of both methods. Our unified\noptimization framework enhances the geometric accuracy of 3D Gaussian splatting\nand improves deformation modeling within the SDF, resulting in a more adaptable\nand precise representation. We demonstrate that our method achieves\nstate-of-the-art performance in rendering metrics even without LiDAR data on\nurban scenes. When incorporating LiDAR, our approach improved further in\nreconstructing and generating novel views across diverse object categories,\nwithout ground-truth 3D motion annotation. Additionally, our method enables\nvarious scene editing tasks, including scene decomposition, and scene\ncomposition.\n", "link": "http://arxiv.org/abs/2510.13381v1", "date": "2025-10-15", "relevancy": 3.1307, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6499}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6204}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6081}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%202D%20Priors%20and%20SDF%20Guidance%20for%20Dynamic%20Urban%20Scene%20Rendering&body=Title%3A%20Leveraging%202D%20Priors%20and%20SDF%20Guidance%20for%20Dynamic%20Urban%20Scene%20Rendering%0AAuthor%3A%20Siddharth%20Tourani%20and%20Jayaram%20Reddy%20and%20Akash%20Kumbar%20and%20Satyajit%20Tourani%20and%20Nishant%20Goyal%20and%20Madhava%20Krishna%20and%20N.%20Dinesh%20Reddy%20and%20Muhammad%20Haris%20Khan%0AAbstract%3A%20%20%20Dynamic%20scene%20rendering%20and%20reconstruction%20play%20a%20crucial%20role%20in%20computer%0Avision%20and%20augmented%20reality.%20Recent%20methods%20based%20on%203D%20Gaussian%20Splatting%0A%283DGS%29%2C%20have%20enabled%20accurate%20modeling%20of%20dynamic%20urban%20scenes%2C%20but%20for%20urban%0Ascenes%20they%20require%20both%20camera%20and%20LiDAR%20data%2C%20ground-truth%203D%20segmentations%0Aand%20motion%20data%20in%20the%20form%20of%20tracklets%20or%20pre-defined%20object%20templates%20such%0Aas%20SMPL.%20In%20this%20work%2C%20we%20explore%20whether%20a%20combination%20of%202D%20object%20agnostic%0Apriors%20in%20the%20form%20of%20depth%20and%20point%20tracking%20coupled%20with%20a%20signed%20distance%0Afunction%20%28SDF%29%20representation%20for%20dynamic%20objects%20can%20be%20used%20to%20relax%20some%20of%0Athese%20requirements.%20We%20present%20a%20novel%20approach%20that%20integrates%20Signed%20Distance%0AFunctions%20%28SDFs%29%20with%203D%20Gaussian%20Splatting%20%283DGS%29%20to%20create%20a%20more%20robust%0Aobject%20representation%20by%20harnessing%20the%20strengths%20of%20both%20methods.%20Our%20unified%0Aoptimization%20framework%20enhances%20the%20geometric%20accuracy%20of%203D%20Gaussian%20splatting%0Aand%20improves%20deformation%20modeling%20within%20the%20SDF%2C%20resulting%20in%20a%20more%20adaptable%0Aand%20precise%20representation.%20We%20demonstrate%20that%20our%20method%20achieves%0Astate-of-the-art%20performance%20in%20rendering%20metrics%20even%20without%20LiDAR%20data%20on%0Aurban%20scenes.%20When%20incorporating%20LiDAR%2C%20our%20approach%20improved%20further%20in%0Areconstructing%20and%20generating%20novel%20views%20across%20diverse%20object%20categories%2C%0Awithout%20ground-truth%203D%20motion%20annotation.%20Additionally%2C%20our%20method%20enables%0Avarious%20scene%20editing%20tasks%2C%20including%20scene%20decomposition%2C%20and%20scene%0Acomposition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13381v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%25202D%2520Priors%2520and%2520SDF%2520Guidance%2520for%2520Dynamic%2520Urban%2520Scene%2520Rendering%26entry.906535625%3DSiddharth%2520Tourani%2520and%2520Jayaram%2520Reddy%2520and%2520Akash%2520Kumbar%2520and%2520Satyajit%2520Tourani%2520and%2520Nishant%2520Goyal%2520and%2520Madhava%2520Krishna%2520and%2520N.%2520Dinesh%2520Reddy%2520and%2520Muhammad%2520Haris%2520Khan%26entry.1292438233%3D%2520%2520Dynamic%2520scene%2520rendering%2520and%2520reconstruction%2520play%2520a%2520crucial%2520role%2520in%2520computer%250Avision%2520and%2520augmented%2520reality.%2520Recent%2520methods%2520based%2520on%25203D%2520Gaussian%2520Splatting%250A%25283DGS%2529%252C%2520have%2520enabled%2520accurate%2520modeling%2520of%2520dynamic%2520urban%2520scenes%252C%2520but%2520for%2520urban%250Ascenes%2520they%2520require%2520both%2520camera%2520and%2520LiDAR%2520data%252C%2520ground-truth%25203D%2520segmentations%250Aand%2520motion%2520data%2520in%2520the%2520form%2520of%2520tracklets%2520or%2520pre-defined%2520object%2520templates%2520such%250Aas%2520SMPL.%2520In%2520this%2520work%252C%2520we%2520explore%2520whether%2520a%2520combination%2520of%25202D%2520object%2520agnostic%250Apriors%2520in%2520the%2520form%2520of%2520depth%2520and%2520point%2520tracking%2520coupled%2520with%2520a%2520signed%2520distance%250Afunction%2520%2528SDF%2529%2520representation%2520for%2520dynamic%2520objects%2520can%2520be%2520used%2520to%2520relax%2520some%2520of%250Athese%2520requirements.%2520We%2520present%2520a%2520novel%2520approach%2520that%2520integrates%2520Signed%2520Distance%250AFunctions%2520%2528SDFs%2529%2520with%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520to%2520create%2520a%2520more%2520robust%250Aobject%2520representation%2520by%2520harnessing%2520the%2520strengths%2520of%2520both%2520methods.%2520Our%2520unified%250Aoptimization%2520framework%2520enhances%2520the%2520geometric%2520accuracy%2520of%25203D%2520Gaussian%2520splatting%250Aand%2520improves%2520deformation%2520modeling%2520within%2520the%2520SDF%252C%2520resulting%2520in%2520a%2520more%2520adaptable%250Aand%2520precise%2520representation.%2520We%2520demonstrate%2520that%2520our%2520method%2520achieves%250Astate-of-the-art%2520performance%2520in%2520rendering%2520metrics%2520even%2520without%2520LiDAR%2520data%2520on%250Aurban%2520scenes.%2520When%2520incorporating%2520LiDAR%252C%2520our%2520approach%2520improved%2520further%2520in%250Areconstructing%2520and%2520generating%2520novel%2520views%2520across%2520diverse%2520object%2520categories%252C%250Awithout%2520ground-truth%25203D%2520motion%2520annotation.%2520Additionally%252C%2520our%2520method%2520enables%250Avarious%2520scene%2520editing%2520tasks%252C%2520including%2520scene%2520decomposition%252C%2520and%2520scene%250Acomposition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13381v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%202D%20Priors%20and%20SDF%20Guidance%20for%20Dynamic%20Urban%20Scene%20Rendering&entry.906535625=Siddharth%20Tourani%20and%20Jayaram%20Reddy%20and%20Akash%20Kumbar%20and%20Satyajit%20Tourani%20and%20Nishant%20Goyal%20and%20Madhava%20Krishna%20and%20N.%20Dinesh%20Reddy%20and%20Muhammad%20Haris%20Khan&entry.1292438233=%20%20Dynamic%20scene%20rendering%20and%20reconstruction%20play%20a%20crucial%20role%20in%20computer%0Avision%20and%20augmented%20reality.%20Recent%20methods%20based%20on%203D%20Gaussian%20Splatting%0A%283DGS%29%2C%20have%20enabled%20accurate%20modeling%20of%20dynamic%20urban%20scenes%2C%20but%20for%20urban%0Ascenes%20they%20require%20both%20camera%20and%20LiDAR%20data%2C%20ground-truth%203D%20segmentations%0Aand%20motion%20data%20in%20the%20form%20of%20tracklets%20or%20pre-defined%20object%20templates%20such%0Aas%20SMPL.%20In%20this%20work%2C%20we%20explore%20whether%20a%20combination%20of%202D%20object%20agnostic%0Apriors%20in%20the%20form%20of%20depth%20and%20point%20tracking%20coupled%20with%20a%20signed%20distance%0Afunction%20%28SDF%29%20representation%20for%20dynamic%20objects%20can%20be%20used%20to%20relax%20some%20of%0Athese%20requirements.%20We%20present%20a%20novel%20approach%20that%20integrates%20Signed%20Distance%0AFunctions%20%28SDFs%29%20with%203D%20Gaussian%20Splatting%20%283DGS%29%20to%20create%20a%20more%20robust%0Aobject%20representation%20by%20harnessing%20the%20strengths%20of%20both%20methods.%20Our%20unified%0Aoptimization%20framework%20enhances%20the%20geometric%20accuracy%20of%203D%20Gaussian%20splatting%0Aand%20improves%20deformation%20modeling%20within%20the%20SDF%2C%20resulting%20in%20a%20more%20adaptable%0Aand%20precise%20representation.%20We%20demonstrate%20that%20our%20method%20achieves%0Astate-of-the-art%20performance%20in%20rendering%20metrics%20even%20without%20LiDAR%20data%20on%0Aurban%20scenes.%20When%20incorporating%20LiDAR%2C%20our%20approach%20improved%20further%20in%0Areconstructing%20and%20generating%20novel%20views%20across%20diverse%20object%20categories%2C%0Awithout%20ground-truth%203D%20motion%20annotation.%20Additionally%2C%20our%20method%20enables%0Avarious%20scene%20editing%20tasks%2C%20including%20scene%20decomposition%2C%20and%20scene%0Acomposition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13381v1&entry.124074799=Read"},
{"title": "VisCoP: Visual Probing for Video Domain Adaptation of Vision Language\n  Models", "author": "Dominick Reilly and Manish Kumar Govind and Le Xue and Srijan Das", "abstract": "  Large Vision-Language Models (VLMs) excel at general visual reasoning tasks\nbut exhibit sharp performance degradation when applied to novel domains with\nsubstantial distribution shifts from pretraining data. Existing domain\nadaptation approaches finetune different VLM components, but this often results\nin limited domain-specific feature learning or catastrophic forgetting of prior\ncapabilities. To address these issues, we introduce Vision Contextualized\nProbing (VisCoP), which augments the VLM's vision encoder with a compact set of\nlearnable visual probes. These probes enable efficient domain-specific\nadaptation with minimal modification to pretrained parameters. We evaluate\nVisCoP across three challenging domain adaptation settings-cross-view\n(exocentric to egocentric), cross-modal (RGB to depth), and cross-task (human\nunderstanding to robot control). Experiments show that VisCoP consistently\noutperforms existing adaptation strategies, achieving superior performance on\ntarget domains while effectively retaining source-domain knowledge.\n", "link": "http://arxiv.org/abs/2510.13808v1", "date": "2025-10-15", "relevancy": 3.1261, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6531}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6531}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5696}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisCoP%3A%20Visual%20Probing%20for%20Video%20Domain%20Adaptation%20of%20Vision%20Language%0A%20%20Models&body=Title%3A%20VisCoP%3A%20Visual%20Probing%20for%20Video%20Domain%20Adaptation%20of%20Vision%20Language%0A%20%20Models%0AAuthor%3A%20Dominick%20Reilly%20and%20Manish%20Kumar%20Govind%20and%20Le%20Xue%20and%20Srijan%20Das%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28VLMs%29%20excel%20at%20general%20visual%20reasoning%20tasks%0Abut%20exhibit%20sharp%20performance%20degradation%20when%20applied%20to%20novel%20domains%20with%0Asubstantial%20distribution%20shifts%20from%20pretraining%20data.%20Existing%20domain%0Aadaptation%20approaches%20finetune%20different%20VLM%20components%2C%20but%20this%20often%20results%0Ain%20limited%20domain-specific%20feature%20learning%20or%20catastrophic%20forgetting%20of%20prior%0Acapabilities.%20To%20address%20these%20issues%2C%20we%20introduce%20Vision%20Contextualized%0AProbing%20%28VisCoP%29%2C%20which%20augments%20the%20VLM%27s%20vision%20encoder%20with%20a%20compact%20set%20of%0Alearnable%20visual%20probes.%20These%20probes%20enable%20efficient%20domain-specific%0Aadaptation%20with%20minimal%20modification%20to%20pretrained%20parameters.%20We%20evaluate%0AVisCoP%20across%20three%20challenging%20domain%20adaptation%20settings-cross-view%0A%28exocentric%20to%20egocentric%29%2C%20cross-modal%20%28RGB%20to%20depth%29%2C%20and%20cross-task%20%28human%0Aunderstanding%20to%20robot%20control%29.%20Experiments%20show%20that%20VisCoP%20consistently%0Aoutperforms%20existing%20adaptation%20strategies%2C%20achieving%20superior%20performance%20on%0Atarget%20domains%20while%20effectively%20retaining%20source-domain%20knowledge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13808v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisCoP%253A%2520Visual%2520Probing%2520for%2520Video%2520Domain%2520Adaptation%2520of%2520Vision%2520Language%250A%2520%2520Models%26entry.906535625%3DDominick%2520Reilly%2520and%2520Manish%2520Kumar%2520Govind%2520and%2520Le%2520Xue%2520and%2520Srijan%2520Das%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520excel%2520at%2520general%2520visual%2520reasoning%2520tasks%250Abut%2520exhibit%2520sharp%2520performance%2520degradation%2520when%2520applied%2520to%2520novel%2520domains%2520with%250Asubstantial%2520distribution%2520shifts%2520from%2520pretraining%2520data.%2520Existing%2520domain%250Aadaptation%2520approaches%2520finetune%2520different%2520VLM%2520components%252C%2520but%2520this%2520often%2520results%250Ain%2520limited%2520domain-specific%2520feature%2520learning%2520or%2520catastrophic%2520forgetting%2520of%2520prior%250Acapabilities.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520Vision%2520Contextualized%250AProbing%2520%2528VisCoP%2529%252C%2520which%2520augments%2520the%2520VLM%2527s%2520vision%2520encoder%2520with%2520a%2520compact%2520set%2520of%250Alearnable%2520visual%2520probes.%2520These%2520probes%2520enable%2520efficient%2520domain-specific%250Aadaptation%2520with%2520minimal%2520modification%2520to%2520pretrained%2520parameters.%2520We%2520evaluate%250AVisCoP%2520across%2520three%2520challenging%2520domain%2520adaptation%2520settings-cross-view%250A%2528exocentric%2520to%2520egocentric%2529%252C%2520cross-modal%2520%2528RGB%2520to%2520depth%2529%252C%2520and%2520cross-task%2520%2528human%250Aunderstanding%2520to%2520robot%2520control%2529.%2520Experiments%2520show%2520that%2520VisCoP%2520consistently%250Aoutperforms%2520existing%2520adaptation%2520strategies%252C%2520achieving%2520superior%2520performance%2520on%250Atarget%2520domains%2520while%2520effectively%2520retaining%2520source-domain%2520knowledge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13808v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisCoP%3A%20Visual%20Probing%20for%20Video%20Domain%20Adaptation%20of%20Vision%20Language%0A%20%20Models&entry.906535625=Dominick%20Reilly%20and%20Manish%20Kumar%20Govind%20and%20Le%20Xue%20and%20Srijan%20Das&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28VLMs%29%20excel%20at%20general%20visual%20reasoning%20tasks%0Abut%20exhibit%20sharp%20performance%20degradation%20when%20applied%20to%20novel%20domains%20with%0Asubstantial%20distribution%20shifts%20from%20pretraining%20data.%20Existing%20domain%0Aadaptation%20approaches%20finetune%20different%20VLM%20components%2C%20but%20this%20often%20results%0Ain%20limited%20domain-specific%20feature%20learning%20or%20catastrophic%20forgetting%20of%20prior%0Acapabilities.%20To%20address%20these%20issues%2C%20we%20introduce%20Vision%20Contextualized%0AProbing%20%28VisCoP%29%2C%20which%20augments%20the%20VLM%27s%20vision%20encoder%20with%20a%20compact%20set%20of%0Alearnable%20visual%20probes.%20These%20probes%20enable%20efficient%20domain-specific%0Aadaptation%20with%20minimal%20modification%20to%20pretrained%20parameters.%20We%20evaluate%0AVisCoP%20across%20three%20challenging%20domain%20adaptation%20settings-cross-view%0A%28exocentric%20to%20egocentric%29%2C%20cross-modal%20%28RGB%20to%20depth%29%2C%20and%20cross-task%20%28human%0Aunderstanding%20to%20robot%20control%29.%20Experiments%20show%20that%20VisCoP%20consistently%0Aoutperforms%20existing%20adaptation%20strategies%2C%20achieving%20superior%20performance%20on%0Atarget%20domains%20while%20effectively%20retaining%20source-domain%20knowledge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13808v1&entry.124074799=Read"},
{"title": "PhysMaster: Mastering Physical Representation for Video Generation via\n  Reinforcement Learning", "author": "Sihui Ji and Xi Chen and Xin Tao and Pengfei Wan and Hengshuang Zhao", "abstract": "  Video generation models nowadays are capable of generating visually realistic\nvideos, but often fail to adhere to physical laws, limiting their ability to\ngenerate physically plausible videos and serve as ''world models''. To address\nthis issue, we propose PhysMaster, which captures physical knowledge as a\nrepresentation for guiding video generation models to enhance their\nphysics-awareness. Specifically, PhysMaster is based on the image-to-video task\nwhere the model is expected to predict physically plausible dynamics from the\ninput image. Since the input image provides physical priors like relative\npositions and potential interactions of objects in the scenario, we devise\nPhysEncoder to encode physical information from it as an extra condition to\ninject physical knowledge into the video generation process. The lack of proper\nsupervision on the model's physical performance beyond mere appearance\nmotivates PhysEncoder to apply reinforcement learning with human feedback to\nphysical representation learning, which leverages feedback from generation\nmodels to optimize physical representations with Direct Preference Optimization\n(DPO) in an end-to-end manner. PhysMaster provides a feasible solution for\nimproving physics-awareness of PhysEncoder and thus of video generation,\nproving its ability on a simple proxy task and generalizability to wide-ranging\nphysical scenarios. This implies that our PhysMaster, which unifies solutions\nfor various physical processes via representation learning in the reinforcement\nlearning paradigm, can act as a generic and plug-in solution for physics-aware\nvideo generation and broader applications.\n", "link": "http://arxiv.org/abs/2510.13809v1", "date": "2025-10-15", "relevancy": 3.0755, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.7258}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5609}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5586}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PhysMaster%3A%20Mastering%20Physical%20Representation%20for%20Video%20Generation%20via%0A%20%20Reinforcement%20Learning&body=Title%3A%20PhysMaster%3A%20Mastering%20Physical%20Representation%20for%20Video%20Generation%20via%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Sihui%20Ji%20and%20Xi%20Chen%20and%20Xin%20Tao%20and%20Pengfei%20Wan%20and%20Hengshuang%20Zhao%0AAbstract%3A%20%20%20Video%20generation%20models%20nowadays%20are%20capable%20of%20generating%20visually%20realistic%0Avideos%2C%20but%20often%20fail%20to%20adhere%20to%20physical%20laws%2C%20limiting%20their%20ability%20to%0Agenerate%20physically%20plausible%20videos%20and%20serve%20as%20%27%27world%20models%27%27.%20To%20address%0Athis%20issue%2C%20we%20propose%20PhysMaster%2C%20which%20captures%20physical%20knowledge%20as%20a%0Arepresentation%20for%20guiding%20video%20generation%20models%20to%20enhance%20their%0Aphysics-awareness.%20Specifically%2C%20PhysMaster%20is%20based%20on%20the%20image-to-video%20task%0Awhere%20the%20model%20is%20expected%20to%20predict%20physically%20plausible%20dynamics%20from%20the%0Ainput%20image.%20Since%20the%20input%20image%20provides%20physical%20priors%20like%20relative%0Apositions%20and%20potential%20interactions%20of%20objects%20in%20the%20scenario%2C%20we%20devise%0APhysEncoder%20to%20encode%20physical%20information%20from%20it%20as%20an%20extra%20condition%20to%0Ainject%20physical%20knowledge%20into%20the%20video%20generation%20process.%20The%20lack%20of%20proper%0Asupervision%20on%20the%20model%27s%20physical%20performance%20beyond%20mere%20appearance%0Amotivates%20PhysEncoder%20to%20apply%20reinforcement%20learning%20with%20human%20feedback%20to%0Aphysical%20representation%20learning%2C%20which%20leverages%20feedback%20from%20generation%0Amodels%20to%20optimize%20physical%20representations%20with%20Direct%20Preference%20Optimization%0A%28DPO%29%20in%20an%20end-to-end%20manner.%20PhysMaster%20provides%20a%20feasible%20solution%20for%0Aimproving%20physics-awareness%20of%20PhysEncoder%20and%20thus%20of%20video%20generation%2C%0Aproving%20its%20ability%20on%20a%20simple%20proxy%20task%20and%20generalizability%20to%20wide-ranging%0Aphysical%20scenarios.%20This%20implies%20that%20our%20PhysMaster%2C%20which%20unifies%20solutions%0Afor%20various%20physical%20processes%20via%20representation%20learning%20in%20the%20reinforcement%0Alearning%20paradigm%2C%20can%20act%20as%20a%20generic%20and%20plug-in%20solution%20for%20physics-aware%0Avideo%20generation%20and%20broader%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13809v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysMaster%253A%2520Mastering%2520Physical%2520Representation%2520for%2520Video%2520Generation%2520via%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DSihui%2520Ji%2520and%2520Xi%2520Chen%2520and%2520Xin%2520Tao%2520and%2520Pengfei%2520Wan%2520and%2520Hengshuang%2520Zhao%26entry.1292438233%3D%2520%2520Video%2520generation%2520models%2520nowadays%2520are%2520capable%2520of%2520generating%2520visually%2520realistic%250Avideos%252C%2520but%2520often%2520fail%2520to%2520adhere%2520to%2520physical%2520laws%252C%2520limiting%2520their%2520ability%2520to%250Agenerate%2520physically%2520plausible%2520videos%2520and%2520serve%2520as%2520%2527%2527world%2520models%2527%2527.%2520To%2520address%250Athis%2520issue%252C%2520we%2520propose%2520PhysMaster%252C%2520which%2520captures%2520physical%2520knowledge%2520as%2520a%250Arepresentation%2520for%2520guiding%2520video%2520generation%2520models%2520to%2520enhance%2520their%250Aphysics-awareness.%2520Specifically%252C%2520PhysMaster%2520is%2520based%2520on%2520the%2520image-to-video%2520task%250Awhere%2520the%2520model%2520is%2520expected%2520to%2520predict%2520physically%2520plausible%2520dynamics%2520from%2520the%250Ainput%2520image.%2520Since%2520the%2520input%2520image%2520provides%2520physical%2520priors%2520like%2520relative%250Apositions%2520and%2520potential%2520interactions%2520of%2520objects%2520in%2520the%2520scenario%252C%2520we%2520devise%250APhysEncoder%2520to%2520encode%2520physical%2520information%2520from%2520it%2520as%2520an%2520extra%2520condition%2520to%250Ainject%2520physical%2520knowledge%2520into%2520the%2520video%2520generation%2520process.%2520The%2520lack%2520of%2520proper%250Asupervision%2520on%2520the%2520model%2527s%2520physical%2520performance%2520beyond%2520mere%2520appearance%250Amotivates%2520PhysEncoder%2520to%2520apply%2520reinforcement%2520learning%2520with%2520human%2520feedback%2520to%250Aphysical%2520representation%2520learning%252C%2520which%2520leverages%2520feedback%2520from%2520generation%250Amodels%2520to%2520optimize%2520physical%2520representations%2520with%2520Direct%2520Preference%2520Optimization%250A%2528DPO%2529%2520in%2520an%2520end-to-end%2520manner.%2520PhysMaster%2520provides%2520a%2520feasible%2520solution%2520for%250Aimproving%2520physics-awareness%2520of%2520PhysEncoder%2520and%2520thus%2520of%2520video%2520generation%252C%250Aproving%2520its%2520ability%2520on%2520a%2520simple%2520proxy%2520task%2520and%2520generalizability%2520to%2520wide-ranging%250Aphysical%2520scenarios.%2520This%2520implies%2520that%2520our%2520PhysMaster%252C%2520which%2520unifies%2520solutions%250Afor%2520various%2520physical%2520processes%2520via%2520representation%2520learning%2520in%2520the%2520reinforcement%250Alearning%2520paradigm%252C%2520can%2520act%2520as%2520a%2520generic%2520and%2520plug-in%2520solution%2520for%2520physics-aware%250Avideo%2520generation%2520and%2520broader%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13809v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PhysMaster%3A%20Mastering%20Physical%20Representation%20for%20Video%20Generation%20via%0A%20%20Reinforcement%20Learning&entry.906535625=Sihui%20Ji%20and%20Xi%20Chen%20and%20Xin%20Tao%20and%20Pengfei%20Wan%20and%20Hengshuang%20Zhao&entry.1292438233=%20%20Video%20generation%20models%20nowadays%20are%20capable%20of%20generating%20visually%20realistic%0Avideos%2C%20but%20often%20fail%20to%20adhere%20to%20physical%20laws%2C%20limiting%20their%20ability%20to%0Agenerate%20physically%20plausible%20videos%20and%20serve%20as%20%27%27world%20models%27%27.%20To%20address%0Athis%20issue%2C%20we%20propose%20PhysMaster%2C%20which%20captures%20physical%20knowledge%20as%20a%0Arepresentation%20for%20guiding%20video%20generation%20models%20to%20enhance%20their%0Aphysics-awareness.%20Specifically%2C%20PhysMaster%20is%20based%20on%20the%20image-to-video%20task%0Awhere%20the%20model%20is%20expected%20to%20predict%20physically%20plausible%20dynamics%20from%20the%0Ainput%20image.%20Since%20the%20input%20image%20provides%20physical%20priors%20like%20relative%0Apositions%20and%20potential%20interactions%20of%20objects%20in%20the%20scenario%2C%20we%20devise%0APhysEncoder%20to%20encode%20physical%20information%20from%20it%20as%20an%20extra%20condition%20to%0Ainject%20physical%20knowledge%20into%20the%20video%20generation%20process.%20The%20lack%20of%20proper%0Asupervision%20on%20the%20model%27s%20physical%20performance%20beyond%20mere%20appearance%0Amotivates%20PhysEncoder%20to%20apply%20reinforcement%20learning%20with%20human%20feedback%20to%0Aphysical%20representation%20learning%2C%20which%20leverages%20feedback%20from%20generation%0Amodels%20to%20optimize%20physical%20representations%20with%20Direct%20Preference%20Optimization%0A%28DPO%29%20in%20an%20end-to-end%20manner.%20PhysMaster%20provides%20a%20feasible%20solution%20for%0Aimproving%20physics-awareness%20of%20PhysEncoder%20and%20thus%20of%20video%20generation%2C%0Aproving%20its%20ability%20on%20a%20simple%20proxy%20task%20and%20generalizability%20to%20wide-ranging%0Aphysical%20scenarios.%20This%20implies%20that%20our%20PhysMaster%2C%20which%20unifies%20solutions%0Afor%20various%20physical%20processes%20via%20representation%20learning%20in%20the%20reinforcement%0Alearning%20paradigm%2C%20can%20act%20as%20a%20generic%20and%20plug-in%20solution%20for%20physics-aware%0Avideo%20generation%20and%20broader%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13809v1&entry.124074799=Read"},
{"title": "MedDINOv3: How to adapt vision foundation models for medical image\n  segmentation?", "author": "Yuheng Li and Yizhou Wu and Yuxiang Lai and Mingzhe Hu and Xiaofeng Yang", "abstract": "  Accurate segmentation of organs and tumors in CT and MRI scans is essential\nfor diagnosis, treatment planning, and disease monitoring. While deep learning\nhas advanced automated segmentation, most models remain task-specific, lacking\ngeneralizability across modalities and institutions. Vision foundation models\n(FMs) pretrained on billion-scale natural images offer powerful and\ntransferable representations. However, adapting them to medical imaging faces\ntwo key challenges: (1) the ViT backbone of most foundation models still\nunderperform specialized CNNs on medical image segmentation, and (2) the large\ndomain gap between natural and medical images limits transferability. We\nintroduce MedDINOv3, a simple and effective framework for adapting DINOv3 to\nmedical segmentation. We first revisit plain ViTs and design a simple and\neffective architecture with multi-scale token aggregation. Then, we perform\ndomain-adaptive pretraining on CT-3M, a curated collection of 3.87M axial CT\nslices, using a multi-stage DINOv3 recipe to learn robust dense features.\nMedDINOv3 matches or exceeds state-of-the-art performance across four\nsegmentation benchmarks, demonstrating the potential of vision foundation\nmodels as unified backbones for medical image segmentation. The code is\navailable at https://github.com/ricklisz/MedDINOv3.\n", "link": "http://arxiv.org/abs/2509.02379v3", "date": "2025-10-15", "relevancy": 3.0199, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.619}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.619}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MedDINOv3%3A%20How%20to%20adapt%20vision%20foundation%20models%20for%20medical%20image%0A%20%20segmentation%3F&body=Title%3A%20MedDINOv3%3A%20How%20to%20adapt%20vision%20foundation%20models%20for%20medical%20image%0A%20%20segmentation%3F%0AAuthor%3A%20Yuheng%20Li%20and%20Yizhou%20Wu%20and%20Yuxiang%20Lai%20and%20Mingzhe%20Hu%20and%20Xiaofeng%20Yang%0AAbstract%3A%20%20%20Accurate%20segmentation%20of%20organs%20and%20tumors%20in%20CT%20and%20MRI%20scans%20is%20essential%0Afor%20diagnosis%2C%20treatment%20planning%2C%20and%20disease%20monitoring.%20While%20deep%20learning%0Ahas%20advanced%20automated%20segmentation%2C%20most%20models%20remain%20task-specific%2C%20lacking%0Ageneralizability%20across%20modalities%20and%20institutions.%20Vision%20foundation%20models%0A%28FMs%29%20pretrained%20on%20billion-scale%20natural%20images%20offer%20powerful%20and%0Atransferable%20representations.%20However%2C%20adapting%20them%20to%20medical%20imaging%20faces%0Atwo%20key%20challenges%3A%20%281%29%20the%20ViT%20backbone%20of%20most%20foundation%20models%20still%0Aunderperform%20specialized%20CNNs%20on%20medical%20image%20segmentation%2C%20and%20%282%29%20the%20large%0Adomain%20gap%20between%20natural%20and%20medical%20images%20limits%20transferability.%20We%0Aintroduce%20MedDINOv3%2C%20a%20simple%20and%20effective%20framework%20for%20adapting%20DINOv3%20to%0Amedical%20segmentation.%20We%20first%20revisit%20plain%20ViTs%20and%20design%20a%20simple%20and%0Aeffective%20architecture%20with%20multi-scale%20token%20aggregation.%20Then%2C%20we%20perform%0Adomain-adaptive%20pretraining%20on%20CT-3M%2C%20a%20curated%20collection%20of%203.87M%20axial%20CT%0Aslices%2C%20using%20a%20multi-stage%20DINOv3%20recipe%20to%20learn%20robust%20dense%20features.%0AMedDINOv3%20matches%20or%20exceeds%20state-of-the-art%20performance%20across%20four%0Asegmentation%20benchmarks%2C%20demonstrating%20the%20potential%20of%20vision%20foundation%0Amodels%20as%20unified%20backbones%20for%20medical%20image%20segmentation.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/ricklisz/MedDINOv3.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.02379v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedDINOv3%253A%2520How%2520to%2520adapt%2520vision%2520foundation%2520models%2520for%2520medical%2520image%250A%2520%2520segmentation%253F%26entry.906535625%3DYuheng%2520Li%2520and%2520Yizhou%2520Wu%2520and%2520Yuxiang%2520Lai%2520and%2520Mingzhe%2520Hu%2520and%2520Xiaofeng%2520Yang%26entry.1292438233%3D%2520%2520Accurate%2520segmentation%2520of%2520organs%2520and%2520tumors%2520in%2520CT%2520and%2520MRI%2520scans%2520is%2520essential%250Afor%2520diagnosis%252C%2520treatment%2520planning%252C%2520and%2520disease%2520monitoring.%2520While%2520deep%2520learning%250Ahas%2520advanced%2520automated%2520segmentation%252C%2520most%2520models%2520remain%2520task-specific%252C%2520lacking%250Ageneralizability%2520across%2520modalities%2520and%2520institutions.%2520Vision%2520foundation%2520models%250A%2528FMs%2529%2520pretrained%2520on%2520billion-scale%2520natural%2520images%2520offer%2520powerful%2520and%250Atransferable%2520representations.%2520However%252C%2520adapting%2520them%2520to%2520medical%2520imaging%2520faces%250Atwo%2520key%2520challenges%253A%2520%25281%2529%2520the%2520ViT%2520backbone%2520of%2520most%2520foundation%2520models%2520still%250Aunderperform%2520specialized%2520CNNs%2520on%2520medical%2520image%2520segmentation%252C%2520and%2520%25282%2529%2520the%2520large%250Adomain%2520gap%2520between%2520natural%2520and%2520medical%2520images%2520limits%2520transferability.%2520We%250Aintroduce%2520MedDINOv3%252C%2520a%2520simple%2520and%2520effective%2520framework%2520for%2520adapting%2520DINOv3%2520to%250Amedical%2520segmentation.%2520We%2520first%2520revisit%2520plain%2520ViTs%2520and%2520design%2520a%2520simple%2520and%250Aeffective%2520architecture%2520with%2520multi-scale%2520token%2520aggregation.%2520Then%252C%2520we%2520perform%250Adomain-adaptive%2520pretraining%2520on%2520CT-3M%252C%2520a%2520curated%2520collection%2520of%25203.87M%2520axial%2520CT%250Aslices%252C%2520using%2520a%2520multi-stage%2520DINOv3%2520recipe%2520to%2520learn%2520robust%2520dense%2520features.%250AMedDINOv3%2520matches%2520or%2520exceeds%2520state-of-the-art%2520performance%2520across%2520four%250Asegmentation%2520benchmarks%252C%2520demonstrating%2520the%2520potential%2520of%2520vision%2520foundation%250Amodels%2520as%2520unified%2520backbones%2520for%2520medical%2520image%2520segmentation.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/ricklisz/MedDINOv3.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.02379v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedDINOv3%3A%20How%20to%20adapt%20vision%20foundation%20models%20for%20medical%20image%0A%20%20segmentation%3F&entry.906535625=Yuheng%20Li%20and%20Yizhou%20Wu%20and%20Yuxiang%20Lai%20and%20Mingzhe%20Hu%20and%20Xiaofeng%20Yang&entry.1292438233=%20%20Accurate%20segmentation%20of%20organs%20and%20tumors%20in%20CT%20and%20MRI%20scans%20is%20essential%0Afor%20diagnosis%2C%20treatment%20planning%2C%20and%20disease%20monitoring.%20While%20deep%20learning%0Ahas%20advanced%20automated%20segmentation%2C%20most%20models%20remain%20task-specific%2C%20lacking%0Ageneralizability%20across%20modalities%20and%20institutions.%20Vision%20foundation%20models%0A%28FMs%29%20pretrained%20on%20billion-scale%20natural%20images%20offer%20powerful%20and%0Atransferable%20representations.%20However%2C%20adapting%20them%20to%20medical%20imaging%20faces%0Atwo%20key%20challenges%3A%20%281%29%20the%20ViT%20backbone%20of%20most%20foundation%20models%20still%0Aunderperform%20specialized%20CNNs%20on%20medical%20image%20segmentation%2C%20and%20%282%29%20the%20large%0Adomain%20gap%20between%20natural%20and%20medical%20images%20limits%20transferability.%20We%0Aintroduce%20MedDINOv3%2C%20a%20simple%20and%20effective%20framework%20for%20adapting%20DINOv3%20to%0Amedical%20segmentation.%20We%20first%20revisit%20plain%20ViTs%20and%20design%20a%20simple%20and%0Aeffective%20architecture%20with%20multi-scale%20token%20aggregation.%20Then%2C%20we%20perform%0Adomain-adaptive%20pretraining%20on%20CT-3M%2C%20a%20curated%20collection%20of%203.87M%20axial%20CT%0Aslices%2C%20using%20a%20multi-stage%20DINOv3%20recipe%20to%20learn%20robust%20dense%20features.%0AMedDINOv3%20matches%20or%20exceeds%20state-of-the-art%20performance%20across%20four%0Asegmentation%20benchmarks%2C%20demonstrating%20the%20potential%20of%20vision%20foundation%0Amodels%20as%20unified%20backbones%20for%20medical%20image%20segmentation.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/ricklisz/MedDINOv3.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.02379v3&entry.124074799=Read"},
{"title": "PlanarMesh: Building Compact 3D Meshes from LiDAR using Incremental\n  Adaptive Resolution Reconstruction", "author": "Jiahao Wang and Nived Chebrolu and Yifu Tao and Lintong Zhang and Ayoung Kim and Maurice Fallon", "abstract": "  Building an online 3D LiDAR mapping system that produces a detailed surface\nreconstruction while remaining computationally efficient is a challenging task.\nIn this paper, we present PlanarMesh, a novel incremental, mesh-based LiDAR\nreconstruction system that adaptively adjusts mesh resolution to achieve\ncompact, detailed reconstructions in real-time. It introduces a new\nrepresentation, planar-mesh, which combines plane modeling and meshing to\ncapture both large surfaces and detailed geometry. The planar-mesh can be\nincrementally updated considering both local surface curvature and free-space\ninformation from sensor measurements. We employ a multi-threaded architecture\nwith a Bounding Volume Hierarchy (BVH) for efficient data storage and fast\nsearch operations, enabling real-time performance. Experimental results show\nthat our method achieves reconstruction accuracy on par with, or exceeding,\nstate-of-the-art techniques-including truncated signed distance functions,\noccupancy mapping, and voxel-based meshing-while producing smaller output file\nsizes (10 times smaller than raw input and more than 5 times smaller than\nmesh-based methods) and maintaining real-time performance (around 2 Hz for a\n64-beam sensor).\n", "link": "http://arxiv.org/abs/2510.13599v1", "date": "2025-10-15", "relevancy": 3.0059, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6385}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5994}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PlanarMesh%3A%20Building%20Compact%203D%20Meshes%20from%20LiDAR%20using%20Incremental%0A%20%20Adaptive%20Resolution%20Reconstruction&body=Title%3A%20PlanarMesh%3A%20Building%20Compact%203D%20Meshes%20from%20LiDAR%20using%20Incremental%0A%20%20Adaptive%20Resolution%20Reconstruction%0AAuthor%3A%20Jiahao%20Wang%20and%20Nived%20Chebrolu%20and%20Yifu%20Tao%20and%20Lintong%20Zhang%20and%20Ayoung%20Kim%20and%20Maurice%20Fallon%0AAbstract%3A%20%20%20Building%20an%20online%203D%20LiDAR%20mapping%20system%20that%20produces%20a%20detailed%20surface%0Areconstruction%20while%20remaining%20computationally%20efficient%20is%20a%20challenging%20task.%0AIn%20this%20paper%2C%20we%20present%20PlanarMesh%2C%20a%20novel%20incremental%2C%20mesh-based%20LiDAR%0Areconstruction%20system%20that%20adaptively%20adjusts%20mesh%20resolution%20to%20achieve%0Acompact%2C%20detailed%20reconstructions%20in%20real-time.%20It%20introduces%20a%20new%0Arepresentation%2C%20planar-mesh%2C%20which%20combines%20plane%20modeling%20and%20meshing%20to%0Acapture%20both%20large%20surfaces%20and%20detailed%20geometry.%20The%20planar-mesh%20can%20be%0Aincrementally%20updated%20considering%20both%20local%20surface%20curvature%20and%20free-space%0Ainformation%20from%20sensor%20measurements.%20We%20employ%20a%20multi-threaded%20architecture%0Awith%20a%20Bounding%20Volume%20Hierarchy%20%28BVH%29%20for%20efficient%20data%20storage%20and%20fast%0Asearch%20operations%2C%20enabling%20real-time%20performance.%20Experimental%20results%20show%0Athat%20our%20method%20achieves%20reconstruction%20accuracy%20on%20par%20with%2C%20or%20exceeding%2C%0Astate-of-the-art%20techniques-including%20truncated%20signed%20distance%20functions%2C%0Aoccupancy%20mapping%2C%20and%20voxel-based%20meshing-while%20producing%20smaller%20output%20file%0Asizes%20%2810%20times%20smaller%20than%20raw%20input%20and%20more%20than%205%20times%20smaller%20than%0Amesh-based%20methods%29%20and%20maintaining%20real-time%20performance%20%28around%202%20Hz%20for%20a%0A64-beam%20sensor%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13599v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPlanarMesh%253A%2520Building%2520Compact%25203D%2520Meshes%2520from%2520LiDAR%2520using%2520Incremental%250A%2520%2520Adaptive%2520Resolution%2520Reconstruction%26entry.906535625%3DJiahao%2520Wang%2520and%2520Nived%2520Chebrolu%2520and%2520Yifu%2520Tao%2520and%2520Lintong%2520Zhang%2520and%2520Ayoung%2520Kim%2520and%2520Maurice%2520Fallon%26entry.1292438233%3D%2520%2520Building%2520an%2520online%25203D%2520LiDAR%2520mapping%2520system%2520that%2520produces%2520a%2520detailed%2520surface%250Areconstruction%2520while%2520remaining%2520computationally%2520efficient%2520is%2520a%2520challenging%2520task.%250AIn%2520this%2520paper%252C%2520we%2520present%2520PlanarMesh%252C%2520a%2520novel%2520incremental%252C%2520mesh-based%2520LiDAR%250Areconstruction%2520system%2520that%2520adaptively%2520adjusts%2520mesh%2520resolution%2520to%2520achieve%250Acompact%252C%2520detailed%2520reconstructions%2520in%2520real-time.%2520It%2520introduces%2520a%2520new%250Arepresentation%252C%2520planar-mesh%252C%2520which%2520combines%2520plane%2520modeling%2520and%2520meshing%2520to%250Acapture%2520both%2520large%2520surfaces%2520and%2520detailed%2520geometry.%2520The%2520planar-mesh%2520can%2520be%250Aincrementally%2520updated%2520considering%2520both%2520local%2520surface%2520curvature%2520and%2520free-space%250Ainformation%2520from%2520sensor%2520measurements.%2520We%2520employ%2520a%2520multi-threaded%2520architecture%250Awith%2520a%2520Bounding%2520Volume%2520Hierarchy%2520%2528BVH%2529%2520for%2520efficient%2520data%2520storage%2520and%2520fast%250Asearch%2520operations%252C%2520enabling%2520real-time%2520performance.%2520Experimental%2520results%2520show%250Athat%2520our%2520method%2520achieves%2520reconstruction%2520accuracy%2520on%2520par%2520with%252C%2520or%2520exceeding%252C%250Astate-of-the-art%2520techniques-including%2520truncated%2520signed%2520distance%2520functions%252C%250Aoccupancy%2520mapping%252C%2520and%2520voxel-based%2520meshing-while%2520producing%2520smaller%2520output%2520file%250Asizes%2520%252810%2520times%2520smaller%2520than%2520raw%2520input%2520and%2520more%2520than%25205%2520times%2520smaller%2520than%250Amesh-based%2520methods%2529%2520and%2520maintaining%2520real-time%2520performance%2520%2528around%25202%2520Hz%2520for%2520a%250A64-beam%2520sensor%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13599v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PlanarMesh%3A%20Building%20Compact%203D%20Meshes%20from%20LiDAR%20using%20Incremental%0A%20%20Adaptive%20Resolution%20Reconstruction&entry.906535625=Jiahao%20Wang%20and%20Nived%20Chebrolu%20and%20Yifu%20Tao%20and%20Lintong%20Zhang%20and%20Ayoung%20Kim%20and%20Maurice%20Fallon&entry.1292438233=%20%20Building%20an%20online%203D%20LiDAR%20mapping%20system%20that%20produces%20a%20detailed%20surface%0Areconstruction%20while%20remaining%20computationally%20efficient%20is%20a%20challenging%20task.%0AIn%20this%20paper%2C%20we%20present%20PlanarMesh%2C%20a%20novel%20incremental%2C%20mesh-based%20LiDAR%0Areconstruction%20system%20that%20adaptively%20adjusts%20mesh%20resolution%20to%20achieve%0Acompact%2C%20detailed%20reconstructions%20in%20real-time.%20It%20introduces%20a%20new%0Arepresentation%2C%20planar-mesh%2C%20which%20combines%20plane%20modeling%20and%20meshing%20to%0Acapture%20both%20large%20surfaces%20and%20detailed%20geometry.%20The%20planar-mesh%20can%20be%0Aincrementally%20updated%20considering%20both%20local%20surface%20curvature%20and%20free-space%0Ainformation%20from%20sensor%20measurements.%20We%20employ%20a%20multi-threaded%20architecture%0Awith%20a%20Bounding%20Volume%20Hierarchy%20%28BVH%29%20for%20efficient%20data%20storage%20and%20fast%0Asearch%20operations%2C%20enabling%20real-time%20performance.%20Experimental%20results%20show%0Athat%20our%20method%20achieves%20reconstruction%20accuracy%20on%20par%20with%2C%20or%20exceeding%2C%0Astate-of-the-art%20techniques-including%20truncated%20signed%20distance%20functions%2C%0Aoccupancy%20mapping%2C%20and%20voxel-based%20meshing-while%20producing%20smaller%20output%20file%0Asizes%20%2810%20times%20smaller%20than%20raw%20input%20and%20more%20than%205%20times%20smaller%20than%0Amesh-based%20methods%29%20and%20maintaining%20real-time%20performance%20%28around%202%20Hz%20for%20a%0A64-beam%20sensor%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13599v1&entry.124074799=Read"},
{"title": "Beyond Pixels: A Differentiable Pipeline for Probing Neuronal\n  Selectivity in 3D", "author": "Pavithra Elumalai and Mohammad Bashiri and Goirik Chakrabarty and Suhas Shrinivasan and Fabian H. Sinz", "abstract": "  Visual perception relies on inference of 3D scene properties such as shape,\npose, and lighting. To understand how visual sensory neurons enable robust\nperception, it is crucial to characterize their selectivity to such physically\ninterpretable factors. However, current approaches mainly operate on 2D pixels,\nmaking it difficult to isolate selectivity for physical scene properties. To\naddress this limitation, we introduce a differentiable rendering pipeline that\noptimizes deformable meshes to obtain MEIs directly in 3D. The method\nparameterizes mesh deformations with radial basis functions and learns offsets\nand scales that maximize neuronal responses while enforcing geometric\nregularity. Applied to models of monkey area V4, our approach enables probing\nneuronal selectivity to interpretable 3D factors such as pose and lighting.\nThis approach bridges inverse graphics with systems neuroscience, offering a\nway to probe neural selectivity with physically grounded, 3D stimuli beyond\nconventional pixel-based methods.\n", "link": "http://arxiv.org/abs/2510.13433v1", "date": "2025-10-15", "relevancy": 2.9999, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6035}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6035}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Pixels%3A%20A%20Differentiable%20Pipeline%20for%20Probing%20Neuronal%0A%20%20Selectivity%20in%203D&body=Title%3A%20Beyond%20Pixels%3A%20A%20Differentiable%20Pipeline%20for%20Probing%20Neuronal%0A%20%20Selectivity%20in%203D%0AAuthor%3A%20Pavithra%20Elumalai%20and%20Mohammad%20Bashiri%20and%20Goirik%20Chakrabarty%20and%20Suhas%20Shrinivasan%20and%20Fabian%20H.%20Sinz%0AAbstract%3A%20%20%20Visual%20perception%20relies%20on%20inference%20of%203D%20scene%20properties%20such%20as%20shape%2C%0Apose%2C%20and%20lighting.%20To%20understand%20how%20visual%20sensory%20neurons%20enable%20robust%0Aperception%2C%20it%20is%20crucial%20to%20characterize%20their%20selectivity%20to%20such%20physically%0Ainterpretable%20factors.%20However%2C%20current%20approaches%20mainly%20operate%20on%202D%20pixels%2C%0Amaking%20it%20difficult%20to%20isolate%20selectivity%20for%20physical%20scene%20properties.%20To%0Aaddress%20this%20limitation%2C%20we%20introduce%20a%20differentiable%20rendering%20pipeline%20that%0Aoptimizes%20deformable%20meshes%20to%20obtain%20MEIs%20directly%20in%203D.%20The%20method%0Aparameterizes%20mesh%20deformations%20with%20radial%20basis%20functions%20and%20learns%20offsets%0Aand%20scales%20that%20maximize%20neuronal%20responses%20while%20enforcing%20geometric%0Aregularity.%20Applied%20to%20models%20of%20monkey%20area%20V4%2C%20our%20approach%20enables%20probing%0Aneuronal%20selectivity%20to%20interpretable%203D%20factors%20such%20as%20pose%20and%20lighting.%0AThis%20approach%20bridges%20inverse%20graphics%20with%20systems%20neuroscience%2C%20offering%20a%0Away%20to%20probe%20neural%20selectivity%20with%20physically%20grounded%2C%203D%20stimuli%20beyond%0Aconventional%20pixel-based%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13433v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Pixels%253A%2520A%2520Differentiable%2520Pipeline%2520for%2520Probing%2520Neuronal%250A%2520%2520Selectivity%2520in%25203D%26entry.906535625%3DPavithra%2520Elumalai%2520and%2520Mohammad%2520Bashiri%2520and%2520Goirik%2520Chakrabarty%2520and%2520Suhas%2520Shrinivasan%2520and%2520Fabian%2520H.%2520Sinz%26entry.1292438233%3D%2520%2520Visual%2520perception%2520relies%2520on%2520inference%2520of%25203D%2520scene%2520properties%2520such%2520as%2520shape%252C%250Apose%252C%2520and%2520lighting.%2520To%2520understand%2520how%2520visual%2520sensory%2520neurons%2520enable%2520robust%250Aperception%252C%2520it%2520is%2520crucial%2520to%2520characterize%2520their%2520selectivity%2520to%2520such%2520physically%250Ainterpretable%2520factors.%2520However%252C%2520current%2520approaches%2520mainly%2520operate%2520on%25202D%2520pixels%252C%250Amaking%2520it%2520difficult%2520to%2520isolate%2520selectivity%2520for%2520physical%2520scene%2520properties.%2520To%250Aaddress%2520this%2520limitation%252C%2520we%2520introduce%2520a%2520differentiable%2520rendering%2520pipeline%2520that%250Aoptimizes%2520deformable%2520meshes%2520to%2520obtain%2520MEIs%2520directly%2520in%25203D.%2520The%2520method%250Aparameterizes%2520mesh%2520deformations%2520with%2520radial%2520basis%2520functions%2520and%2520learns%2520offsets%250Aand%2520scales%2520that%2520maximize%2520neuronal%2520responses%2520while%2520enforcing%2520geometric%250Aregularity.%2520Applied%2520to%2520models%2520of%2520monkey%2520area%2520V4%252C%2520our%2520approach%2520enables%2520probing%250Aneuronal%2520selectivity%2520to%2520interpretable%25203D%2520factors%2520such%2520as%2520pose%2520and%2520lighting.%250AThis%2520approach%2520bridges%2520inverse%2520graphics%2520with%2520systems%2520neuroscience%252C%2520offering%2520a%250Away%2520to%2520probe%2520neural%2520selectivity%2520with%2520physically%2520grounded%252C%25203D%2520stimuli%2520beyond%250Aconventional%2520pixel-based%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13433v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Pixels%3A%20A%20Differentiable%20Pipeline%20for%20Probing%20Neuronal%0A%20%20Selectivity%20in%203D&entry.906535625=Pavithra%20Elumalai%20and%20Mohammad%20Bashiri%20and%20Goirik%20Chakrabarty%20and%20Suhas%20Shrinivasan%20and%20Fabian%20H.%20Sinz&entry.1292438233=%20%20Visual%20perception%20relies%20on%20inference%20of%203D%20scene%20properties%20such%20as%20shape%2C%0Apose%2C%20and%20lighting.%20To%20understand%20how%20visual%20sensory%20neurons%20enable%20robust%0Aperception%2C%20it%20is%20crucial%20to%20characterize%20their%20selectivity%20to%20such%20physically%0Ainterpretable%20factors.%20However%2C%20current%20approaches%20mainly%20operate%20on%202D%20pixels%2C%0Amaking%20it%20difficult%20to%20isolate%20selectivity%20for%20physical%20scene%20properties.%20To%0Aaddress%20this%20limitation%2C%20we%20introduce%20a%20differentiable%20rendering%20pipeline%20that%0Aoptimizes%20deformable%20meshes%20to%20obtain%20MEIs%20directly%20in%203D.%20The%20method%0Aparameterizes%20mesh%20deformations%20with%20radial%20basis%20functions%20and%20learns%20offsets%0Aand%20scales%20that%20maximize%20neuronal%20responses%20while%20enforcing%20geometric%0Aregularity.%20Applied%20to%20models%20of%20monkey%20area%20V4%2C%20our%20approach%20enables%20probing%0Aneuronal%20selectivity%20to%20interpretable%203D%20factors%20such%20as%20pose%20and%20lighting.%0AThis%20approach%20bridges%20inverse%20graphics%20with%20systems%20neuroscience%2C%20offering%20a%0Away%20to%20probe%20neural%20selectivity%20with%20physically%20grounded%2C%203D%20stimuli%20beyond%0Aconventional%20pixel-based%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13433v1&entry.124074799=Read"},
{"title": "A Personalized Data-Driven Generative Model of Human Repetitive Motion", "author": "Angelo Di Porzio and Marco Coraggio", "abstract": "  The deployment of autonomous virtual avatars (in extended reality) and robots\nin human group activities -- such as rehabilitation therapy, sports, and\nmanufacturing -- is expected to increase as these technologies become more\npervasive. Designing cognitive architectures and control strategies to drive\nthese agents requires realistic models of human motion. Furthermore, recent\nresearch has shown that each person exhibits a unique velocity signature,\nhighlighting how individual motor behaviors are both rich in variability and\ninternally consistent. However, existing models only provide simplified\ndescriptions of human motor behavior, hindering the development of effective\ncognitive architectures. In this work, we first show that motion amplitude\nprovides a valid and complementary characterization of individual motor\nsignatures. Then, we propose a fully data-driven approach, based on long\nshort-term memory neural networks, to generate original motion that captures\nthe unique features of specific individuals. We validate the architecture using\nreal human data from participants performing spontaneous oscillatory motion.\nExtensive analyses show that state-of-the-art Kuramoto-like models fail to\nreplicate individual motor signatures, whereas our model accurately reproduces\nthe velocity distribution and amplitude envelopes of the individual it was\ntrained on, while remaining distinct from others.\n", "link": "http://arxiv.org/abs/2503.15225v2", "date": "2025-10-15", "relevancy": 2.9819, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6341}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5813}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Personalized%20Data-Driven%20Generative%20Model%20of%20Human%20Repetitive%20Motion&body=Title%3A%20A%20Personalized%20Data-Driven%20Generative%20Model%20of%20Human%20Repetitive%20Motion%0AAuthor%3A%20Angelo%20Di%20Porzio%20and%20Marco%20Coraggio%0AAbstract%3A%20%20%20The%20deployment%20of%20autonomous%20virtual%20avatars%20%28in%20extended%20reality%29%20and%20robots%0Ain%20human%20group%20activities%20--%20such%20as%20rehabilitation%20therapy%2C%20sports%2C%20and%0Amanufacturing%20--%20is%20expected%20to%20increase%20as%20these%20technologies%20become%20more%0Apervasive.%20Designing%20cognitive%20architectures%20and%20control%20strategies%20to%20drive%0Athese%20agents%20requires%20realistic%20models%20of%20human%20motion.%20Furthermore%2C%20recent%0Aresearch%20has%20shown%20that%20each%20person%20exhibits%20a%20unique%20velocity%20signature%2C%0Ahighlighting%20how%20individual%20motor%20behaviors%20are%20both%20rich%20in%20variability%20and%0Ainternally%20consistent.%20However%2C%20existing%20models%20only%20provide%20simplified%0Adescriptions%20of%20human%20motor%20behavior%2C%20hindering%20the%20development%20of%20effective%0Acognitive%20architectures.%20In%20this%20work%2C%20we%20first%20show%20that%20motion%20amplitude%0Aprovides%20a%20valid%20and%20complementary%20characterization%20of%20individual%20motor%0Asignatures.%20Then%2C%20we%20propose%20a%20fully%20data-driven%20approach%2C%20based%20on%20long%0Ashort-term%20memory%20neural%20networks%2C%20to%20generate%20original%20motion%20that%20captures%0Athe%20unique%20features%20of%20specific%20individuals.%20We%20validate%20the%20architecture%20using%0Areal%20human%20data%20from%20participants%20performing%20spontaneous%20oscillatory%20motion.%0AExtensive%20analyses%20show%20that%20state-of-the-art%20Kuramoto-like%20models%20fail%20to%0Areplicate%20individual%20motor%20signatures%2C%20whereas%20our%20model%20accurately%20reproduces%0Athe%20velocity%20distribution%20and%20amplitude%20envelopes%20of%20the%20individual%20it%20was%0Atrained%20on%2C%20while%20remaining%20distinct%20from%20others.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.15225v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Personalized%2520Data-Driven%2520Generative%2520Model%2520of%2520Human%2520Repetitive%2520Motion%26entry.906535625%3DAngelo%2520Di%2520Porzio%2520and%2520Marco%2520Coraggio%26entry.1292438233%3D%2520%2520The%2520deployment%2520of%2520autonomous%2520virtual%2520avatars%2520%2528in%2520extended%2520reality%2529%2520and%2520robots%250Ain%2520human%2520group%2520activities%2520--%2520such%2520as%2520rehabilitation%2520therapy%252C%2520sports%252C%2520and%250Amanufacturing%2520--%2520is%2520expected%2520to%2520increase%2520as%2520these%2520technologies%2520become%2520more%250Apervasive.%2520Designing%2520cognitive%2520architectures%2520and%2520control%2520strategies%2520to%2520drive%250Athese%2520agents%2520requires%2520realistic%2520models%2520of%2520human%2520motion.%2520Furthermore%252C%2520recent%250Aresearch%2520has%2520shown%2520that%2520each%2520person%2520exhibits%2520a%2520unique%2520velocity%2520signature%252C%250Ahighlighting%2520how%2520individual%2520motor%2520behaviors%2520are%2520both%2520rich%2520in%2520variability%2520and%250Ainternally%2520consistent.%2520However%252C%2520existing%2520models%2520only%2520provide%2520simplified%250Adescriptions%2520of%2520human%2520motor%2520behavior%252C%2520hindering%2520the%2520development%2520of%2520effective%250Acognitive%2520architectures.%2520In%2520this%2520work%252C%2520we%2520first%2520show%2520that%2520motion%2520amplitude%250Aprovides%2520a%2520valid%2520and%2520complementary%2520characterization%2520of%2520individual%2520motor%250Asignatures.%2520Then%252C%2520we%2520propose%2520a%2520fully%2520data-driven%2520approach%252C%2520based%2520on%2520long%250Ashort-term%2520memory%2520neural%2520networks%252C%2520to%2520generate%2520original%2520motion%2520that%2520captures%250Athe%2520unique%2520features%2520of%2520specific%2520individuals.%2520We%2520validate%2520the%2520architecture%2520using%250Areal%2520human%2520data%2520from%2520participants%2520performing%2520spontaneous%2520oscillatory%2520motion.%250AExtensive%2520analyses%2520show%2520that%2520state-of-the-art%2520Kuramoto-like%2520models%2520fail%2520to%250Areplicate%2520individual%2520motor%2520signatures%252C%2520whereas%2520our%2520model%2520accurately%2520reproduces%250Athe%2520velocity%2520distribution%2520and%2520amplitude%2520envelopes%2520of%2520the%2520individual%2520it%2520was%250Atrained%2520on%252C%2520while%2520remaining%2520distinct%2520from%2520others.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.15225v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Personalized%20Data-Driven%20Generative%20Model%20of%20Human%20Repetitive%20Motion&entry.906535625=Angelo%20Di%20Porzio%20and%20Marco%20Coraggio&entry.1292438233=%20%20The%20deployment%20of%20autonomous%20virtual%20avatars%20%28in%20extended%20reality%29%20and%20robots%0Ain%20human%20group%20activities%20--%20such%20as%20rehabilitation%20therapy%2C%20sports%2C%20and%0Amanufacturing%20--%20is%20expected%20to%20increase%20as%20these%20technologies%20become%20more%0Apervasive.%20Designing%20cognitive%20architectures%20and%20control%20strategies%20to%20drive%0Athese%20agents%20requires%20realistic%20models%20of%20human%20motion.%20Furthermore%2C%20recent%0Aresearch%20has%20shown%20that%20each%20person%20exhibits%20a%20unique%20velocity%20signature%2C%0Ahighlighting%20how%20individual%20motor%20behaviors%20are%20both%20rich%20in%20variability%20and%0Ainternally%20consistent.%20However%2C%20existing%20models%20only%20provide%20simplified%0Adescriptions%20of%20human%20motor%20behavior%2C%20hindering%20the%20development%20of%20effective%0Acognitive%20architectures.%20In%20this%20work%2C%20we%20first%20show%20that%20motion%20amplitude%0Aprovides%20a%20valid%20and%20complementary%20characterization%20of%20individual%20motor%0Asignatures.%20Then%2C%20we%20propose%20a%20fully%20data-driven%20approach%2C%20based%20on%20long%0Ashort-term%20memory%20neural%20networks%2C%20to%20generate%20original%20motion%20that%20captures%0Athe%20unique%20features%20of%20specific%20individuals.%20We%20validate%20the%20architecture%20using%0Areal%20human%20data%20from%20participants%20performing%20spontaneous%20oscillatory%20motion.%0AExtensive%20analyses%20show%20that%20state-of-the-art%20Kuramoto-like%20models%20fail%20to%0Areplicate%20individual%20motor%20signatures%2C%20whereas%20our%20model%20accurately%20reproduces%0Athe%20velocity%20distribution%20and%20amplitude%20envelopes%20of%20the%20individual%20it%20was%0Atrained%20on%2C%20while%20remaining%20distinct%20from%20others.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.15225v2&entry.124074799=Read"},
{"title": "DepthVLA: Enhancing Vision-Language-Action Models with Depth-Aware\n  Spatial Reasoning", "author": "Tianyuan Yuan and Yicheng Liu and Chenhao Lu and Zhuoguang Chen and Tao Jiang and Hang Zhao", "abstract": "  Vision-Language-Action (VLA) models have recently shown impressive\ngeneralization and language-guided manipulation capabilities. However, their\nperformance degrades on tasks requiring precise spatial reasoning due to\nlimited spatial reasoning inherited from Vision-Language Models (VLMs).\nExisting VLAs rely on extensive action-data pretraining to ground VLMs in 3D\nspace, which reduces training efficiency and is still insufficient for accurate\nspatial understanding. In this work, we present DepthVLA, a simple yet\neffective VLA architecture that explicitly incorporates spatial awareness\nthrough a pretrained depth prediction module. DepthVLA adopts a\nmixture-of-transformers design that unifies a VLM, a depth transformer, and an\naction expert with fully shared attentions, forming an end-to-end model with\nenhanced spatial reasoning. Extensive evaluations in both real-world and\nsimulated environments show that DepthVLA outperforms state-of-the-art\napproaches, achieving 78.5% vs. 65.0% progress in real-world tasks, 94.9% vs.\n93.6% in the LIBERO simulator, and 74.8% vs. 58.8% in the Simpler simulator.\nOur code will be made publicly available.\n", "link": "http://arxiv.org/abs/2510.13375v1", "date": "2025-10-15", "relevancy": 2.9698, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6056}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6056}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DepthVLA%3A%20Enhancing%20Vision-Language-Action%20Models%20with%20Depth-Aware%0A%20%20Spatial%20Reasoning&body=Title%3A%20DepthVLA%3A%20Enhancing%20Vision-Language-Action%20Models%20with%20Depth-Aware%0A%20%20Spatial%20Reasoning%0AAuthor%3A%20Tianyuan%20Yuan%20and%20Yicheng%20Liu%20and%20Chenhao%20Lu%20and%20Zhuoguang%20Chen%20and%20Tao%20Jiang%20and%20Hang%20Zhao%0AAbstract%3A%20%20%20Vision-Language-Action%20%28VLA%29%20models%20have%20recently%20shown%20impressive%0Ageneralization%20and%20language-guided%20manipulation%20capabilities.%20However%2C%20their%0Aperformance%20degrades%20on%20tasks%20requiring%20precise%20spatial%20reasoning%20due%20to%0Alimited%20spatial%20reasoning%20inherited%20from%20Vision-Language%20Models%20%28VLMs%29.%0AExisting%20VLAs%20rely%20on%20extensive%20action-data%20pretraining%20to%20ground%20VLMs%20in%203D%0Aspace%2C%20which%20reduces%20training%20efficiency%20and%20is%20still%20insufficient%20for%20accurate%0Aspatial%20understanding.%20In%20this%20work%2C%20we%20present%20DepthVLA%2C%20a%20simple%20yet%0Aeffective%20VLA%20architecture%20that%20explicitly%20incorporates%20spatial%20awareness%0Athrough%20a%20pretrained%20depth%20prediction%20module.%20DepthVLA%20adopts%20a%0Amixture-of-transformers%20design%20that%20unifies%20a%20VLM%2C%20a%20depth%20transformer%2C%20and%20an%0Aaction%20expert%20with%20fully%20shared%20attentions%2C%20forming%20an%20end-to-end%20model%20with%0Aenhanced%20spatial%20reasoning.%20Extensive%20evaluations%20in%20both%20real-world%20and%0Asimulated%20environments%20show%20that%20DepthVLA%20outperforms%20state-of-the-art%0Aapproaches%2C%20achieving%2078.5%25%20vs.%2065.0%25%20progress%20in%20real-world%20tasks%2C%2094.9%25%20vs.%0A93.6%25%20in%20the%20LIBERO%20simulator%2C%20and%2074.8%25%20vs.%2058.8%25%20in%20the%20Simpler%20simulator.%0AOur%20code%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13375v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepthVLA%253A%2520Enhancing%2520Vision-Language-Action%2520Models%2520with%2520Depth-Aware%250A%2520%2520Spatial%2520Reasoning%26entry.906535625%3DTianyuan%2520Yuan%2520and%2520Yicheng%2520Liu%2520and%2520Chenhao%2520Lu%2520and%2520Zhuoguang%2520Chen%2520and%2520Tao%2520Jiang%2520and%2520Hang%2520Zhao%26entry.1292438233%3D%2520%2520Vision-Language-Action%2520%2528VLA%2529%2520models%2520have%2520recently%2520shown%2520impressive%250Ageneralization%2520and%2520language-guided%2520manipulation%2520capabilities.%2520However%252C%2520their%250Aperformance%2520degrades%2520on%2520tasks%2520requiring%2520precise%2520spatial%2520reasoning%2520due%2520to%250Alimited%2520spatial%2520reasoning%2520inherited%2520from%2520Vision-Language%2520Models%2520%2528VLMs%2529.%250AExisting%2520VLAs%2520rely%2520on%2520extensive%2520action-data%2520pretraining%2520to%2520ground%2520VLMs%2520in%25203D%250Aspace%252C%2520which%2520reduces%2520training%2520efficiency%2520and%2520is%2520still%2520insufficient%2520for%2520accurate%250Aspatial%2520understanding.%2520In%2520this%2520work%252C%2520we%2520present%2520DepthVLA%252C%2520a%2520simple%2520yet%250Aeffective%2520VLA%2520architecture%2520that%2520explicitly%2520incorporates%2520spatial%2520awareness%250Athrough%2520a%2520pretrained%2520depth%2520prediction%2520module.%2520DepthVLA%2520adopts%2520a%250Amixture-of-transformers%2520design%2520that%2520unifies%2520a%2520VLM%252C%2520a%2520depth%2520transformer%252C%2520and%2520an%250Aaction%2520expert%2520with%2520fully%2520shared%2520attentions%252C%2520forming%2520an%2520end-to-end%2520model%2520with%250Aenhanced%2520spatial%2520reasoning.%2520Extensive%2520evaluations%2520in%2520both%2520real-world%2520and%250Asimulated%2520environments%2520show%2520that%2520DepthVLA%2520outperforms%2520state-of-the-art%250Aapproaches%252C%2520achieving%252078.5%2525%2520vs.%252065.0%2525%2520progress%2520in%2520real-world%2520tasks%252C%252094.9%2525%2520vs.%250A93.6%2525%2520in%2520the%2520LIBERO%2520simulator%252C%2520and%252074.8%2525%2520vs.%252058.8%2525%2520in%2520the%2520Simpler%2520simulator.%250AOur%2520code%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13375v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DepthVLA%3A%20Enhancing%20Vision-Language-Action%20Models%20with%20Depth-Aware%0A%20%20Spatial%20Reasoning&entry.906535625=Tianyuan%20Yuan%20and%20Yicheng%20Liu%20and%20Chenhao%20Lu%20and%20Zhuoguang%20Chen%20and%20Tao%20Jiang%20and%20Hang%20Zhao&entry.1292438233=%20%20Vision-Language-Action%20%28VLA%29%20models%20have%20recently%20shown%20impressive%0Ageneralization%20and%20language-guided%20manipulation%20capabilities.%20However%2C%20their%0Aperformance%20degrades%20on%20tasks%20requiring%20precise%20spatial%20reasoning%20due%20to%0Alimited%20spatial%20reasoning%20inherited%20from%20Vision-Language%20Models%20%28VLMs%29.%0AExisting%20VLAs%20rely%20on%20extensive%20action-data%20pretraining%20to%20ground%20VLMs%20in%203D%0Aspace%2C%20which%20reduces%20training%20efficiency%20and%20is%20still%20insufficient%20for%20accurate%0Aspatial%20understanding.%20In%20this%20work%2C%20we%20present%20DepthVLA%2C%20a%20simple%20yet%0Aeffective%20VLA%20architecture%20that%20explicitly%20incorporates%20spatial%20awareness%0Athrough%20a%20pretrained%20depth%20prediction%20module.%20DepthVLA%20adopts%20a%0Amixture-of-transformers%20design%20that%20unifies%20a%20VLM%2C%20a%20depth%20transformer%2C%20and%20an%0Aaction%20expert%20with%20fully%20shared%20attentions%2C%20forming%20an%20end-to-end%20model%20with%0Aenhanced%20spatial%20reasoning.%20Extensive%20evaluations%20in%20both%20real-world%20and%0Asimulated%20environments%20show%20that%20DepthVLA%20outperforms%20state-of-the-art%0Aapproaches%2C%20achieving%2078.5%25%20vs.%2065.0%25%20progress%20in%20real-world%20tasks%2C%2094.9%25%20vs.%0A93.6%25%20in%20the%20LIBERO%20simulator%2C%20and%2074.8%25%20vs.%2058.8%25%20in%20the%20Simpler%20simulator.%0AOur%20code%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13375v1&entry.124074799=Read"},
{"title": "Learning Neural Parametric 3D Breast Shape Models for Metrical Surface\n  Reconstruction From Monocular RGB Videos", "author": "Maximilian Weiherer and Antonia von Riedheim and Vanessa Br\u00e9bant and Bernhard Egger and Christoph Palm", "abstract": "  We present a neural parametric 3D breast shape model and, based on this\nmodel, introduce a low-cost and accessible 3D surface reconstruction pipeline\ncapable of recovering accurate breast geometry from a monocular RGB video. In\ncontrast to widely used, commercially available yet prohibitively expensive 3D\nbreast scanning solutions and existing low-cost alternatives, our method\nrequires neither specialized hardware nor proprietary software and can be used\nwith any device that is able to record RGB videos. The key building blocks of\nour pipeline are a state-of-the-art, off-the-shelf Structure-from-motion\npipeline, paired with a parametric breast model for robust and metrically\ncorrect surface reconstruction. Our model, similarly to the recently proposed\nimplicit Regensburg Breast Shape Model (iRBSM), leverages implicit neural\nrepresentations to model breast shapes. However, unlike the iRBSM, which\nemploys a single global neural signed distance function (SDF), our approach --\ninspired by recent state-of-the-art face models -- decomposes the implicit\nbreast domain into multiple smaller regions, each represented by a local neural\nSDF anchored at anatomical landmark positions. When incorporated into our\nsurface reconstruction pipeline, the proposed model, dubbed liRBSM (short for\nlocalized iRBSM), significantly outperforms the iRBSM in terms of\nreconstruction quality, yielding more detailed surface reconstruction than its\nglobal counterpart. Overall, we find that the introduced pipeline is able to\nrecover high-quality 3D breast geometry within an error margin of less than 2\nmm. Our method is fast (requires less than six minutes), fully transparent and\nopen-source, and -- together with the model -- publicly available at\nhttps://rbsm.re-mic.de/local-implicit.\n", "link": "http://arxiv.org/abs/2510.13540v1", "date": "2025-10-15", "relevancy": 2.9242, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6022}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5786}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Neural%20Parametric%203D%20Breast%20Shape%20Models%20for%20Metrical%20Surface%0A%20%20Reconstruction%20From%20Monocular%20RGB%20Videos&body=Title%3A%20Learning%20Neural%20Parametric%203D%20Breast%20Shape%20Models%20for%20Metrical%20Surface%0A%20%20Reconstruction%20From%20Monocular%20RGB%20Videos%0AAuthor%3A%20Maximilian%20Weiherer%20and%20Antonia%20von%20Riedheim%20and%20Vanessa%20Br%C3%A9bant%20and%20Bernhard%20Egger%20and%20Christoph%20Palm%0AAbstract%3A%20%20%20We%20present%20a%20neural%20parametric%203D%20breast%20shape%20model%20and%2C%20based%20on%20this%0Amodel%2C%20introduce%20a%20low-cost%20and%20accessible%203D%20surface%20reconstruction%20pipeline%0Acapable%20of%20recovering%20accurate%20breast%20geometry%20from%20a%20monocular%20RGB%20video.%20In%0Acontrast%20to%20widely%20used%2C%20commercially%20available%20yet%20prohibitively%20expensive%203D%0Abreast%20scanning%20solutions%20and%20existing%20low-cost%20alternatives%2C%20our%20method%0Arequires%20neither%20specialized%20hardware%20nor%20proprietary%20software%20and%20can%20be%20used%0Awith%20any%20device%20that%20is%20able%20to%20record%20RGB%20videos.%20The%20key%20building%20blocks%20of%0Aour%20pipeline%20are%20a%20state-of-the-art%2C%20off-the-shelf%20Structure-from-motion%0Apipeline%2C%20paired%20with%20a%20parametric%20breast%20model%20for%20robust%20and%20metrically%0Acorrect%20surface%20reconstruction.%20Our%20model%2C%20similarly%20to%20the%20recently%20proposed%0Aimplicit%20Regensburg%20Breast%20Shape%20Model%20%28iRBSM%29%2C%20leverages%20implicit%20neural%0Arepresentations%20to%20model%20breast%20shapes.%20However%2C%20unlike%20the%20iRBSM%2C%20which%0Aemploys%20a%20single%20global%20neural%20signed%20distance%20function%20%28SDF%29%2C%20our%20approach%20--%0Ainspired%20by%20recent%20state-of-the-art%20face%20models%20--%20decomposes%20the%20implicit%0Abreast%20domain%20into%20multiple%20smaller%20regions%2C%20each%20represented%20by%20a%20local%20neural%0ASDF%20anchored%20at%20anatomical%20landmark%20positions.%20When%20incorporated%20into%20our%0Asurface%20reconstruction%20pipeline%2C%20the%20proposed%20model%2C%20dubbed%20liRBSM%20%28short%20for%0Alocalized%20iRBSM%29%2C%20significantly%20outperforms%20the%20iRBSM%20in%20terms%20of%0Areconstruction%20quality%2C%20yielding%20more%20detailed%20surface%20reconstruction%20than%20its%0Aglobal%20counterpart.%20Overall%2C%20we%20find%20that%20the%20introduced%20pipeline%20is%20able%20to%0Arecover%20high-quality%203D%20breast%20geometry%20within%20an%20error%20margin%20of%20less%20than%202%0Amm.%20Our%20method%20is%20fast%20%28requires%20less%20than%20six%20minutes%29%2C%20fully%20transparent%20and%0Aopen-source%2C%20and%20--%20together%20with%20the%20model%20--%20publicly%20available%20at%0Ahttps%3A//rbsm.re-mic.de/local-implicit.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13540v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Neural%2520Parametric%25203D%2520Breast%2520Shape%2520Models%2520for%2520Metrical%2520Surface%250A%2520%2520Reconstruction%2520From%2520Monocular%2520RGB%2520Videos%26entry.906535625%3DMaximilian%2520Weiherer%2520and%2520Antonia%2520von%2520Riedheim%2520and%2520Vanessa%2520Br%25C3%25A9bant%2520and%2520Bernhard%2520Egger%2520and%2520Christoph%2520Palm%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520neural%2520parametric%25203D%2520breast%2520shape%2520model%2520and%252C%2520based%2520on%2520this%250Amodel%252C%2520introduce%2520a%2520low-cost%2520and%2520accessible%25203D%2520surface%2520reconstruction%2520pipeline%250Acapable%2520of%2520recovering%2520accurate%2520breast%2520geometry%2520from%2520a%2520monocular%2520RGB%2520video.%2520In%250Acontrast%2520to%2520widely%2520used%252C%2520commercially%2520available%2520yet%2520prohibitively%2520expensive%25203D%250Abreast%2520scanning%2520solutions%2520and%2520existing%2520low-cost%2520alternatives%252C%2520our%2520method%250Arequires%2520neither%2520specialized%2520hardware%2520nor%2520proprietary%2520software%2520and%2520can%2520be%2520used%250Awith%2520any%2520device%2520that%2520is%2520able%2520to%2520record%2520RGB%2520videos.%2520The%2520key%2520building%2520blocks%2520of%250Aour%2520pipeline%2520are%2520a%2520state-of-the-art%252C%2520off-the-shelf%2520Structure-from-motion%250Apipeline%252C%2520paired%2520with%2520a%2520parametric%2520breast%2520model%2520for%2520robust%2520and%2520metrically%250Acorrect%2520surface%2520reconstruction.%2520Our%2520model%252C%2520similarly%2520to%2520the%2520recently%2520proposed%250Aimplicit%2520Regensburg%2520Breast%2520Shape%2520Model%2520%2528iRBSM%2529%252C%2520leverages%2520implicit%2520neural%250Arepresentations%2520to%2520model%2520breast%2520shapes.%2520However%252C%2520unlike%2520the%2520iRBSM%252C%2520which%250Aemploys%2520a%2520single%2520global%2520neural%2520signed%2520distance%2520function%2520%2528SDF%2529%252C%2520our%2520approach%2520--%250Ainspired%2520by%2520recent%2520state-of-the-art%2520face%2520models%2520--%2520decomposes%2520the%2520implicit%250Abreast%2520domain%2520into%2520multiple%2520smaller%2520regions%252C%2520each%2520represented%2520by%2520a%2520local%2520neural%250ASDF%2520anchored%2520at%2520anatomical%2520landmark%2520positions.%2520When%2520incorporated%2520into%2520our%250Asurface%2520reconstruction%2520pipeline%252C%2520the%2520proposed%2520model%252C%2520dubbed%2520liRBSM%2520%2528short%2520for%250Alocalized%2520iRBSM%2529%252C%2520significantly%2520outperforms%2520the%2520iRBSM%2520in%2520terms%2520of%250Areconstruction%2520quality%252C%2520yielding%2520more%2520detailed%2520surface%2520reconstruction%2520than%2520its%250Aglobal%2520counterpart.%2520Overall%252C%2520we%2520find%2520that%2520the%2520introduced%2520pipeline%2520is%2520able%2520to%250Arecover%2520high-quality%25203D%2520breast%2520geometry%2520within%2520an%2520error%2520margin%2520of%2520less%2520than%25202%250Amm.%2520Our%2520method%2520is%2520fast%2520%2528requires%2520less%2520than%2520six%2520minutes%2529%252C%2520fully%2520transparent%2520and%250Aopen-source%252C%2520and%2520--%2520together%2520with%2520the%2520model%2520--%2520publicly%2520available%2520at%250Ahttps%253A//rbsm.re-mic.de/local-implicit.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13540v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Neural%20Parametric%203D%20Breast%20Shape%20Models%20for%20Metrical%20Surface%0A%20%20Reconstruction%20From%20Monocular%20RGB%20Videos&entry.906535625=Maximilian%20Weiherer%20and%20Antonia%20von%20Riedheim%20and%20Vanessa%20Br%C3%A9bant%20and%20Bernhard%20Egger%20and%20Christoph%20Palm&entry.1292438233=%20%20We%20present%20a%20neural%20parametric%203D%20breast%20shape%20model%20and%2C%20based%20on%20this%0Amodel%2C%20introduce%20a%20low-cost%20and%20accessible%203D%20surface%20reconstruction%20pipeline%0Acapable%20of%20recovering%20accurate%20breast%20geometry%20from%20a%20monocular%20RGB%20video.%20In%0Acontrast%20to%20widely%20used%2C%20commercially%20available%20yet%20prohibitively%20expensive%203D%0Abreast%20scanning%20solutions%20and%20existing%20low-cost%20alternatives%2C%20our%20method%0Arequires%20neither%20specialized%20hardware%20nor%20proprietary%20software%20and%20can%20be%20used%0Awith%20any%20device%20that%20is%20able%20to%20record%20RGB%20videos.%20The%20key%20building%20blocks%20of%0Aour%20pipeline%20are%20a%20state-of-the-art%2C%20off-the-shelf%20Structure-from-motion%0Apipeline%2C%20paired%20with%20a%20parametric%20breast%20model%20for%20robust%20and%20metrically%0Acorrect%20surface%20reconstruction.%20Our%20model%2C%20similarly%20to%20the%20recently%20proposed%0Aimplicit%20Regensburg%20Breast%20Shape%20Model%20%28iRBSM%29%2C%20leverages%20implicit%20neural%0Arepresentations%20to%20model%20breast%20shapes.%20However%2C%20unlike%20the%20iRBSM%2C%20which%0Aemploys%20a%20single%20global%20neural%20signed%20distance%20function%20%28SDF%29%2C%20our%20approach%20--%0Ainspired%20by%20recent%20state-of-the-art%20face%20models%20--%20decomposes%20the%20implicit%0Abreast%20domain%20into%20multiple%20smaller%20regions%2C%20each%20represented%20by%20a%20local%20neural%0ASDF%20anchored%20at%20anatomical%20landmark%20positions.%20When%20incorporated%20into%20our%0Asurface%20reconstruction%20pipeline%2C%20the%20proposed%20model%2C%20dubbed%20liRBSM%20%28short%20for%0Alocalized%20iRBSM%29%2C%20significantly%20outperforms%20the%20iRBSM%20in%20terms%20of%0Areconstruction%20quality%2C%20yielding%20more%20detailed%20surface%20reconstruction%20than%20its%0Aglobal%20counterpart.%20Overall%2C%20we%20find%20that%20the%20introduced%20pipeline%20is%20able%20to%0Arecover%20high-quality%203D%20breast%20geometry%20within%20an%20error%20margin%20of%20less%20than%202%0Amm.%20Our%20method%20is%20fast%20%28requires%20less%20than%20six%20minutes%29%2C%20fully%20transparent%20and%0Aopen-source%2C%20and%20--%20together%20with%20the%20model%20--%20publicly%20available%20at%0Ahttps%3A//rbsm.re-mic.de/local-implicit.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13540v1&entry.124074799=Read"},
{"title": "Physics-augmented Multi-task Gaussian Process for Modeling\n  Spatiotemporal Dynamics", "author": "Xizhuo Zhang and Bing Yao", "abstract": "  Recent advances in sensing and imaging technologies have enabled the\ncollection of high-dimensional spatiotemporal data across complex geometric\ndomains. However, effective modeling of such data remains challenging due to\nirregular spatial structures, rapid temporal dynamics, and the need to jointly\npredict multiple interrelated physical variables. This paper presents a\nphysics-augmented multi-task Gaussian Process (P-M-GP) framework tailored for\nspatiotemporal dynamic systems. Specifically, we develop a geometry-aware,\nmulti-task Gaussian Process (M-GP) model to effectively capture intrinsic\nspatiotemporal structure and inter-task dependencies. To further enhance the\nmodel fidelity and robustness, we incorporate governing physical laws through a\nphysics-based regularization scheme, thereby constraining predictions to be\nconsistent with governing dynamical principles. We validate the proposed P-M-GP\nframework on a 3D cardiac electrodynamics modeling task. Numerical experiments\ndemonstrate that our method significantly improves prediction accuracy over\nexisting methods by effectively incorporating domain-specific physical\nconstraints and geometric prior.\n", "link": "http://arxiv.org/abs/2510.13601v1", "date": "2025-10-15", "relevancy": 2.8979, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5857}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5827}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics-augmented%20Multi-task%20Gaussian%20Process%20for%20Modeling%0A%20%20Spatiotemporal%20Dynamics&body=Title%3A%20Physics-augmented%20Multi-task%20Gaussian%20Process%20for%20Modeling%0A%20%20Spatiotemporal%20Dynamics%0AAuthor%3A%20Xizhuo%20Zhang%20and%20Bing%20Yao%0AAbstract%3A%20%20%20Recent%20advances%20in%20sensing%20and%20imaging%20technologies%20have%20enabled%20the%0Acollection%20of%20high-dimensional%20spatiotemporal%20data%20across%20complex%20geometric%0Adomains.%20However%2C%20effective%20modeling%20of%20such%20data%20remains%20challenging%20due%20to%0Airregular%20spatial%20structures%2C%20rapid%20temporal%20dynamics%2C%20and%20the%20need%20to%20jointly%0Apredict%20multiple%20interrelated%20physical%20variables.%20This%20paper%20presents%20a%0Aphysics-augmented%20multi-task%20Gaussian%20Process%20%28P-M-GP%29%20framework%20tailored%20for%0Aspatiotemporal%20dynamic%20systems.%20Specifically%2C%20we%20develop%20a%20geometry-aware%2C%0Amulti-task%20Gaussian%20Process%20%28M-GP%29%20model%20to%20effectively%20capture%20intrinsic%0Aspatiotemporal%20structure%20and%20inter-task%20dependencies.%20To%20further%20enhance%20the%0Amodel%20fidelity%20and%20robustness%2C%20we%20incorporate%20governing%20physical%20laws%20through%20a%0Aphysics-based%20regularization%20scheme%2C%20thereby%20constraining%20predictions%20to%20be%0Aconsistent%20with%20governing%20dynamical%20principles.%20We%20validate%20the%20proposed%20P-M-GP%0Aframework%20on%20a%203D%20cardiac%20electrodynamics%20modeling%20task.%20Numerical%20experiments%0Ademonstrate%20that%20our%20method%20significantly%20improves%20prediction%20accuracy%20over%0Aexisting%20methods%20by%20effectively%20incorporating%20domain-specific%20physical%0Aconstraints%20and%20geometric%20prior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13601v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics-augmented%2520Multi-task%2520Gaussian%2520Process%2520for%2520Modeling%250A%2520%2520Spatiotemporal%2520Dynamics%26entry.906535625%3DXizhuo%2520Zhang%2520and%2520Bing%2520Yao%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520sensing%2520and%2520imaging%2520technologies%2520have%2520enabled%2520the%250Acollection%2520of%2520high-dimensional%2520spatiotemporal%2520data%2520across%2520complex%2520geometric%250Adomains.%2520However%252C%2520effective%2520modeling%2520of%2520such%2520data%2520remains%2520challenging%2520due%2520to%250Airregular%2520spatial%2520structures%252C%2520rapid%2520temporal%2520dynamics%252C%2520and%2520the%2520need%2520to%2520jointly%250Apredict%2520multiple%2520interrelated%2520physical%2520variables.%2520This%2520paper%2520presents%2520a%250Aphysics-augmented%2520multi-task%2520Gaussian%2520Process%2520%2528P-M-GP%2529%2520framework%2520tailored%2520for%250Aspatiotemporal%2520dynamic%2520systems.%2520Specifically%252C%2520we%2520develop%2520a%2520geometry-aware%252C%250Amulti-task%2520Gaussian%2520Process%2520%2528M-GP%2529%2520model%2520to%2520effectively%2520capture%2520intrinsic%250Aspatiotemporal%2520structure%2520and%2520inter-task%2520dependencies.%2520To%2520further%2520enhance%2520the%250Amodel%2520fidelity%2520and%2520robustness%252C%2520we%2520incorporate%2520governing%2520physical%2520laws%2520through%2520a%250Aphysics-based%2520regularization%2520scheme%252C%2520thereby%2520constraining%2520predictions%2520to%2520be%250Aconsistent%2520with%2520governing%2520dynamical%2520principles.%2520We%2520validate%2520the%2520proposed%2520P-M-GP%250Aframework%2520on%2520a%25203D%2520cardiac%2520electrodynamics%2520modeling%2520task.%2520Numerical%2520experiments%250Ademonstrate%2520that%2520our%2520method%2520significantly%2520improves%2520prediction%2520accuracy%2520over%250Aexisting%2520methods%2520by%2520effectively%2520incorporating%2520domain-specific%2520physical%250Aconstraints%2520and%2520geometric%2520prior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13601v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-augmented%20Multi-task%20Gaussian%20Process%20for%20Modeling%0A%20%20Spatiotemporal%20Dynamics&entry.906535625=Xizhuo%20Zhang%20and%20Bing%20Yao&entry.1292438233=%20%20Recent%20advances%20in%20sensing%20and%20imaging%20technologies%20have%20enabled%20the%0Acollection%20of%20high-dimensional%20spatiotemporal%20data%20across%20complex%20geometric%0Adomains.%20However%2C%20effective%20modeling%20of%20such%20data%20remains%20challenging%20due%20to%0Airregular%20spatial%20structures%2C%20rapid%20temporal%20dynamics%2C%20and%20the%20need%20to%20jointly%0Apredict%20multiple%20interrelated%20physical%20variables.%20This%20paper%20presents%20a%0Aphysics-augmented%20multi-task%20Gaussian%20Process%20%28P-M-GP%29%20framework%20tailored%20for%0Aspatiotemporal%20dynamic%20systems.%20Specifically%2C%20we%20develop%20a%20geometry-aware%2C%0Amulti-task%20Gaussian%20Process%20%28M-GP%29%20model%20to%20effectively%20capture%20intrinsic%0Aspatiotemporal%20structure%20and%20inter-task%20dependencies.%20To%20further%20enhance%20the%0Amodel%20fidelity%20and%20robustness%2C%20we%20incorporate%20governing%20physical%20laws%20through%20a%0Aphysics-based%20regularization%20scheme%2C%20thereby%20constraining%20predictions%20to%20be%0Aconsistent%20with%20governing%20dynamical%20principles.%20We%20validate%20the%20proposed%20P-M-GP%0Aframework%20on%20a%203D%20cardiac%20electrodynamics%20modeling%20task.%20Numerical%20experiments%0Ademonstrate%20that%20our%20method%20significantly%20improves%20prediction%20accuracy%20over%0Aexisting%20methods%20by%20effectively%20incorporating%20domain-specific%20physical%0Aconstraints%20and%20geometric%20prior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13601v1&entry.124074799=Read"},
{"title": "InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn\n  Dialogue", "author": "Wenwen Tong and Hewei Guo and Dongchuan Ran and Jiangnan Chen and Jiefan Lu and Kaibin Wang and Keqiang Li and Xiaoxu Zhu and Jiakui Li and Kehan Li and Xueheng Li and Lumin Li and Chenxu Guo and Jiasheng Zhou and Jiandong Chen and Xianye Wu and Jiahao Wang and Silei Wu and Lei Chen and Hanming Deng and Yuxuan Song and Dinghao Zhou and Guiping Zhong and Ken Zheng and Shiyin Kang and Lewei Lu", "abstract": "  We introduce InteractiveOmni, a unified and open-source omni-modal large\nlanguage model for audio-visual multi-turn interaction, ranging from 4B to 8B\nparameters, designed to lead the field of lightweight models by offering\ncomprehensive omni-modal understanding and speech generation capabilities. To\nachieve this, we integrate the vision encoder, audio encoder, large language\nmodel, and speech decoder into a unified model for understanding and generation\ntasks. We design a multi-stage training strategy to ensure robust cross-modal\ncapabilities, including pre-training for omni-modal understanding, followed by\npost-training with speech conversation and audio-visual interaction. To enable\nhuman-like long-term conversational ability, we meticulously curate a\nmulti-turn training dataset that enhances the model's ability to handle complex\nand multi-turn interactions. To effectively evaluate the multi-turn memory and\nspeech interaction capabilities, we construct the multi-modal multi-turn memory\nbenchmark and the multi-turn speech interaction benchmark. Experiments\ndemonstrate that InteractiveOmni significantly outperforms leading open-source\nmodels and provides a more intelligent multi-turn audio-visual experience,\nparticularly in its long-term memory capabilities. Notably, InteractiveOmni-4B\nis comparable to the much larger model like Qwen2.5-Omni-7B on general\nbenchmarks, and it can retain 97% of the performance of the InteractiveOmni-8B\nwhile utilizing only 50% of the model size. Achieving state-of-the-art results\nagainst similarly sized models across image, audio, video understanding, and\nspeech generation tasks, InteractiveOmni is an accessible, open-source\nfoundation for next-generation intelligent interactive systems.\n", "link": "http://arxiv.org/abs/2510.13747v1", "date": "2025-10-15", "relevancy": 2.8976, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5977}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5977}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InteractiveOmni%3A%20A%20Unified%20Omni-modal%20Model%20for%20Audio-Visual%20Multi-turn%0A%20%20Dialogue&body=Title%3A%20InteractiveOmni%3A%20A%20Unified%20Omni-modal%20Model%20for%20Audio-Visual%20Multi-turn%0A%20%20Dialogue%0AAuthor%3A%20Wenwen%20Tong%20and%20Hewei%20Guo%20and%20Dongchuan%20Ran%20and%20Jiangnan%20Chen%20and%20Jiefan%20Lu%20and%20Kaibin%20Wang%20and%20Keqiang%20Li%20and%20Xiaoxu%20Zhu%20and%20Jiakui%20Li%20and%20Kehan%20Li%20and%20Xueheng%20Li%20and%20Lumin%20Li%20and%20Chenxu%20Guo%20and%20Jiasheng%20Zhou%20and%20Jiandong%20Chen%20and%20Xianye%20Wu%20and%20Jiahao%20Wang%20and%20Silei%20Wu%20and%20Lei%20Chen%20and%20Hanming%20Deng%20and%20Yuxuan%20Song%20and%20Dinghao%20Zhou%20and%20Guiping%20Zhong%20and%20Ken%20Zheng%20and%20Shiyin%20Kang%20and%20Lewei%20Lu%0AAbstract%3A%20%20%20We%20introduce%20InteractiveOmni%2C%20a%20unified%20and%20open-source%20omni-modal%20large%0Alanguage%20model%20for%20audio-visual%20multi-turn%20interaction%2C%20ranging%20from%204B%20to%208B%0Aparameters%2C%20designed%20to%20lead%20the%20field%20of%20lightweight%20models%20by%20offering%0Acomprehensive%20omni-modal%20understanding%20and%20speech%20generation%20capabilities.%20To%0Aachieve%20this%2C%20we%20integrate%20the%20vision%20encoder%2C%20audio%20encoder%2C%20large%20language%0Amodel%2C%20and%20speech%20decoder%20into%20a%20unified%20model%20for%20understanding%20and%20generation%0Atasks.%20We%20design%20a%20multi-stage%20training%20strategy%20to%20ensure%20robust%20cross-modal%0Acapabilities%2C%20including%20pre-training%20for%20omni-modal%20understanding%2C%20followed%20by%0Apost-training%20with%20speech%20conversation%20and%20audio-visual%20interaction.%20To%20enable%0Ahuman-like%20long-term%20conversational%20ability%2C%20we%20meticulously%20curate%20a%0Amulti-turn%20training%20dataset%20that%20enhances%20the%20model%27s%20ability%20to%20handle%20complex%0Aand%20multi-turn%20interactions.%20To%20effectively%20evaluate%20the%20multi-turn%20memory%20and%0Aspeech%20interaction%20capabilities%2C%20we%20construct%20the%20multi-modal%20multi-turn%20memory%0Abenchmark%20and%20the%20multi-turn%20speech%20interaction%20benchmark.%20Experiments%0Ademonstrate%20that%20InteractiveOmni%20significantly%20outperforms%20leading%20open-source%0Amodels%20and%20provides%20a%20more%20intelligent%20multi-turn%20audio-visual%20experience%2C%0Aparticularly%20in%20its%20long-term%20memory%20capabilities.%20Notably%2C%20InteractiveOmni-4B%0Ais%20comparable%20to%20the%20much%20larger%20model%20like%20Qwen2.5-Omni-7B%20on%20general%0Abenchmarks%2C%20and%20it%20can%20retain%2097%25%20of%20the%20performance%20of%20the%20InteractiveOmni-8B%0Awhile%20utilizing%20only%2050%25%20of%20the%20model%20size.%20Achieving%20state-of-the-art%20results%0Aagainst%20similarly%20sized%20models%20across%20image%2C%20audio%2C%20video%20understanding%2C%20and%0Aspeech%20generation%20tasks%2C%20InteractiveOmni%20is%20an%20accessible%2C%20open-source%0Afoundation%20for%20next-generation%20intelligent%20interactive%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13747v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInteractiveOmni%253A%2520A%2520Unified%2520Omni-modal%2520Model%2520for%2520Audio-Visual%2520Multi-turn%250A%2520%2520Dialogue%26entry.906535625%3DWenwen%2520Tong%2520and%2520Hewei%2520Guo%2520and%2520Dongchuan%2520Ran%2520and%2520Jiangnan%2520Chen%2520and%2520Jiefan%2520Lu%2520and%2520Kaibin%2520Wang%2520and%2520Keqiang%2520Li%2520and%2520Xiaoxu%2520Zhu%2520and%2520Jiakui%2520Li%2520and%2520Kehan%2520Li%2520and%2520Xueheng%2520Li%2520and%2520Lumin%2520Li%2520and%2520Chenxu%2520Guo%2520and%2520Jiasheng%2520Zhou%2520and%2520Jiandong%2520Chen%2520and%2520Xianye%2520Wu%2520and%2520Jiahao%2520Wang%2520and%2520Silei%2520Wu%2520and%2520Lei%2520Chen%2520and%2520Hanming%2520Deng%2520and%2520Yuxuan%2520Song%2520and%2520Dinghao%2520Zhou%2520and%2520Guiping%2520Zhong%2520and%2520Ken%2520Zheng%2520and%2520Shiyin%2520Kang%2520and%2520Lewei%2520Lu%26entry.1292438233%3D%2520%2520We%2520introduce%2520InteractiveOmni%252C%2520a%2520unified%2520and%2520open-source%2520omni-modal%2520large%250Alanguage%2520model%2520for%2520audio-visual%2520multi-turn%2520interaction%252C%2520ranging%2520from%25204B%2520to%25208B%250Aparameters%252C%2520designed%2520to%2520lead%2520the%2520field%2520of%2520lightweight%2520models%2520by%2520offering%250Acomprehensive%2520omni-modal%2520understanding%2520and%2520speech%2520generation%2520capabilities.%2520To%250Aachieve%2520this%252C%2520we%2520integrate%2520the%2520vision%2520encoder%252C%2520audio%2520encoder%252C%2520large%2520language%250Amodel%252C%2520and%2520speech%2520decoder%2520into%2520a%2520unified%2520model%2520for%2520understanding%2520and%2520generation%250Atasks.%2520We%2520design%2520a%2520multi-stage%2520training%2520strategy%2520to%2520ensure%2520robust%2520cross-modal%250Acapabilities%252C%2520including%2520pre-training%2520for%2520omni-modal%2520understanding%252C%2520followed%2520by%250Apost-training%2520with%2520speech%2520conversation%2520and%2520audio-visual%2520interaction.%2520To%2520enable%250Ahuman-like%2520long-term%2520conversational%2520ability%252C%2520we%2520meticulously%2520curate%2520a%250Amulti-turn%2520training%2520dataset%2520that%2520enhances%2520the%2520model%2527s%2520ability%2520to%2520handle%2520complex%250Aand%2520multi-turn%2520interactions.%2520To%2520effectively%2520evaluate%2520the%2520multi-turn%2520memory%2520and%250Aspeech%2520interaction%2520capabilities%252C%2520we%2520construct%2520the%2520multi-modal%2520multi-turn%2520memory%250Abenchmark%2520and%2520the%2520multi-turn%2520speech%2520interaction%2520benchmark.%2520Experiments%250Ademonstrate%2520that%2520InteractiveOmni%2520significantly%2520outperforms%2520leading%2520open-source%250Amodels%2520and%2520provides%2520a%2520more%2520intelligent%2520multi-turn%2520audio-visual%2520experience%252C%250Aparticularly%2520in%2520its%2520long-term%2520memory%2520capabilities.%2520Notably%252C%2520InteractiveOmni-4B%250Ais%2520comparable%2520to%2520the%2520much%2520larger%2520model%2520like%2520Qwen2.5-Omni-7B%2520on%2520general%250Abenchmarks%252C%2520and%2520it%2520can%2520retain%252097%2525%2520of%2520the%2520performance%2520of%2520the%2520InteractiveOmni-8B%250Awhile%2520utilizing%2520only%252050%2525%2520of%2520the%2520model%2520size.%2520Achieving%2520state-of-the-art%2520results%250Aagainst%2520similarly%2520sized%2520models%2520across%2520image%252C%2520audio%252C%2520video%2520understanding%252C%2520and%250Aspeech%2520generation%2520tasks%252C%2520InteractiveOmni%2520is%2520an%2520accessible%252C%2520open-source%250Afoundation%2520for%2520next-generation%2520intelligent%2520interactive%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13747v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InteractiveOmni%3A%20A%20Unified%20Omni-modal%20Model%20for%20Audio-Visual%20Multi-turn%0A%20%20Dialogue&entry.906535625=Wenwen%20Tong%20and%20Hewei%20Guo%20and%20Dongchuan%20Ran%20and%20Jiangnan%20Chen%20and%20Jiefan%20Lu%20and%20Kaibin%20Wang%20and%20Keqiang%20Li%20and%20Xiaoxu%20Zhu%20and%20Jiakui%20Li%20and%20Kehan%20Li%20and%20Xueheng%20Li%20and%20Lumin%20Li%20and%20Chenxu%20Guo%20and%20Jiasheng%20Zhou%20and%20Jiandong%20Chen%20and%20Xianye%20Wu%20and%20Jiahao%20Wang%20and%20Silei%20Wu%20and%20Lei%20Chen%20and%20Hanming%20Deng%20and%20Yuxuan%20Song%20and%20Dinghao%20Zhou%20and%20Guiping%20Zhong%20and%20Ken%20Zheng%20and%20Shiyin%20Kang%20and%20Lewei%20Lu&entry.1292438233=%20%20We%20introduce%20InteractiveOmni%2C%20a%20unified%20and%20open-source%20omni-modal%20large%0Alanguage%20model%20for%20audio-visual%20multi-turn%20interaction%2C%20ranging%20from%204B%20to%208B%0Aparameters%2C%20designed%20to%20lead%20the%20field%20of%20lightweight%20models%20by%20offering%0Acomprehensive%20omni-modal%20understanding%20and%20speech%20generation%20capabilities.%20To%0Aachieve%20this%2C%20we%20integrate%20the%20vision%20encoder%2C%20audio%20encoder%2C%20large%20language%0Amodel%2C%20and%20speech%20decoder%20into%20a%20unified%20model%20for%20understanding%20and%20generation%0Atasks.%20We%20design%20a%20multi-stage%20training%20strategy%20to%20ensure%20robust%20cross-modal%0Acapabilities%2C%20including%20pre-training%20for%20omni-modal%20understanding%2C%20followed%20by%0Apost-training%20with%20speech%20conversation%20and%20audio-visual%20interaction.%20To%20enable%0Ahuman-like%20long-term%20conversational%20ability%2C%20we%20meticulously%20curate%20a%0Amulti-turn%20training%20dataset%20that%20enhances%20the%20model%27s%20ability%20to%20handle%20complex%0Aand%20multi-turn%20interactions.%20To%20effectively%20evaluate%20the%20multi-turn%20memory%20and%0Aspeech%20interaction%20capabilities%2C%20we%20construct%20the%20multi-modal%20multi-turn%20memory%0Abenchmark%20and%20the%20multi-turn%20speech%20interaction%20benchmark.%20Experiments%0Ademonstrate%20that%20InteractiveOmni%20significantly%20outperforms%20leading%20open-source%0Amodels%20and%20provides%20a%20more%20intelligent%20multi-turn%20audio-visual%20experience%2C%0Aparticularly%20in%20its%20long-term%20memory%20capabilities.%20Notably%2C%20InteractiveOmni-4B%0Ais%20comparable%20to%20the%20much%20larger%20model%20like%20Qwen2.5-Omni-7B%20on%20general%0Abenchmarks%2C%20and%20it%20can%20retain%2097%25%20of%20the%20performance%20of%20the%20InteractiveOmni-8B%0Awhile%20utilizing%20only%2050%25%20of%20the%20model%20size.%20Achieving%20state-of-the-art%20results%0Aagainst%20similarly%20sized%20models%20across%20image%2C%20audio%2C%20video%20understanding%2C%20and%0Aspeech%20generation%20tasks%2C%20InteractiveOmni%20is%20an%20accessible%2C%20open-source%0Afoundation%20for%20next-generation%20intelligent%20interactive%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13747v1&entry.124074799=Read"},
{"title": "Reliable generation of isomorphic physics problems using Generative AI\n  with prompt-chaining and tool use", "author": "Zhongzhou Chen", "abstract": "  We present a method for generating large numbers of isomorphic physics\nproblems using generative AI services such as ChatGPT, through prompt chaining\nand tool use. This approach enables precise control over structural\nvariations-such as numeric values and spatial relations-while supporting\ndiverse contextual variations in the problem body. By utilizing the Python code\ninterpreter, the method supports automatic solution validation and simple\ndiagram generation, addressing key limitations in existing LLM-based methods.\nWe generated two example isomorphic problem banks and compared the outcome\nagainst two simpler prompt-based approaches. Results show that prompt-chaining\nproduces significantly higher quality and more consistent outputs than simpler,\nnon-chaining prompts. We also show that GenAI services can be used to validate\nthe quality of the generated isomorphic problems. This work demonstrates a\npromising method for efficient and scalable problem creation accessible to the\naverage instructor, which opens new possibilities for personalized adaptive\ntesting and automated content development.\n", "link": "http://arxiv.org/abs/2508.14755v2", "date": "2025-10-15", "relevancy": 2.8798, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6031}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5809}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reliable%20generation%20of%20isomorphic%20physics%20problems%20using%20Generative%20AI%0A%20%20with%20prompt-chaining%20and%20tool%20use&body=Title%3A%20Reliable%20generation%20of%20isomorphic%20physics%20problems%20using%20Generative%20AI%0A%20%20with%20prompt-chaining%20and%20tool%20use%0AAuthor%3A%20Zhongzhou%20Chen%0AAbstract%3A%20%20%20We%20present%20a%20method%20for%20generating%20large%20numbers%20of%20isomorphic%20physics%0Aproblems%20using%20generative%20AI%20services%20such%20as%20ChatGPT%2C%20through%20prompt%20chaining%0Aand%20tool%20use.%20This%20approach%20enables%20precise%20control%20over%20structural%0Avariations-such%20as%20numeric%20values%20and%20spatial%20relations-while%20supporting%0Adiverse%20contextual%20variations%20in%20the%20problem%20body.%20By%20utilizing%20the%20Python%20code%0Ainterpreter%2C%20the%20method%20supports%20automatic%20solution%20validation%20and%20simple%0Adiagram%20generation%2C%20addressing%20key%20limitations%20in%20existing%20LLM-based%20methods.%0AWe%20generated%20two%20example%20isomorphic%20problem%20banks%20and%20compared%20the%20outcome%0Aagainst%20two%20simpler%20prompt-based%20approaches.%20Results%20show%20that%20prompt-chaining%0Aproduces%20significantly%20higher%20quality%20and%20more%20consistent%20outputs%20than%20simpler%2C%0Anon-chaining%20prompts.%20We%20also%20show%20that%20GenAI%20services%20can%20be%20used%20to%20validate%0Athe%20quality%20of%20the%20generated%20isomorphic%20problems.%20This%20work%20demonstrates%20a%0Apromising%20method%20for%20efficient%20and%20scalable%20problem%20creation%20accessible%20to%20the%0Aaverage%20instructor%2C%20which%20opens%20new%20possibilities%20for%20personalized%20adaptive%0Atesting%20and%20automated%20content%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14755v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReliable%2520generation%2520of%2520isomorphic%2520physics%2520problems%2520using%2520Generative%2520AI%250A%2520%2520with%2520prompt-chaining%2520and%2520tool%2520use%26entry.906535625%3DZhongzhou%2520Chen%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520method%2520for%2520generating%2520large%2520numbers%2520of%2520isomorphic%2520physics%250Aproblems%2520using%2520generative%2520AI%2520services%2520such%2520as%2520ChatGPT%252C%2520through%2520prompt%2520chaining%250Aand%2520tool%2520use.%2520This%2520approach%2520enables%2520precise%2520control%2520over%2520structural%250Avariations-such%2520as%2520numeric%2520values%2520and%2520spatial%2520relations-while%2520supporting%250Adiverse%2520contextual%2520variations%2520in%2520the%2520problem%2520body.%2520By%2520utilizing%2520the%2520Python%2520code%250Ainterpreter%252C%2520the%2520method%2520supports%2520automatic%2520solution%2520validation%2520and%2520simple%250Adiagram%2520generation%252C%2520addressing%2520key%2520limitations%2520in%2520existing%2520LLM-based%2520methods.%250AWe%2520generated%2520two%2520example%2520isomorphic%2520problem%2520banks%2520and%2520compared%2520the%2520outcome%250Aagainst%2520two%2520simpler%2520prompt-based%2520approaches.%2520Results%2520show%2520that%2520prompt-chaining%250Aproduces%2520significantly%2520higher%2520quality%2520and%2520more%2520consistent%2520outputs%2520than%2520simpler%252C%250Anon-chaining%2520prompts.%2520We%2520also%2520show%2520that%2520GenAI%2520services%2520can%2520be%2520used%2520to%2520validate%250Athe%2520quality%2520of%2520the%2520generated%2520isomorphic%2520problems.%2520This%2520work%2520demonstrates%2520a%250Apromising%2520method%2520for%2520efficient%2520and%2520scalable%2520problem%2520creation%2520accessible%2520to%2520the%250Aaverage%2520instructor%252C%2520which%2520opens%2520new%2520possibilities%2520for%2520personalized%2520adaptive%250Atesting%2520and%2520automated%2520content%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14755v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reliable%20generation%20of%20isomorphic%20physics%20problems%20using%20Generative%20AI%0A%20%20with%20prompt-chaining%20and%20tool%20use&entry.906535625=Zhongzhou%20Chen&entry.1292438233=%20%20We%20present%20a%20method%20for%20generating%20large%20numbers%20of%20isomorphic%20physics%0Aproblems%20using%20generative%20AI%20services%20such%20as%20ChatGPT%2C%20through%20prompt%20chaining%0Aand%20tool%20use.%20This%20approach%20enables%20precise%20control%20over%20structural%0Avariations-such%20as%20numeric%20values%20and%20spatial%20relations-while%20supporting%0Adiverse%20contextual%20variations%20in%20the%20problem%20body.%20By%20utilizing%20the%20Python%20code%0Ainterpreter%2C%20the%20method%20supports%20automatic%20solution%20validation%20and%20simple%0Adiagram%20generation%2C%20addressing%20key%20limitations%20in%20existing%20LLM-based%20methods.%0AWe%20generated%20two%20example%20isomorphic%20problem%20banks%20and%20compared%20the%20outcome%0Aagainst%20two%20simpler%20prompt-based%20approaches.%20Results%20show%20that%20prompt-chaining%0Aproduces%20significantly%20higher%20quality%20and%20more%20consistent%20outputs%20than%20simpler%2C%0Anon-chaining%20prompts.%20We%20also%20show%20that%20GenAI%20services%20can%20be%20used%20to%20validate%0Athe%20quality%20of%20the%20generated%20isomorphic%20problems.%20This%20work%20demonstrates%20a%0Apromising%20method%20for%20efficient%20and%20scalable%20problem%20creation%20accessible%20to%20the%0Aaverage%20instructor%2C%20which%20opens%20new%20possibilities%20for%20personalized%20adaptive%0Atesting%20and%20automated%20content%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14755v2&entry.124074799=Read"},
{"title": "A Simple Framework for Open-Vocabulary Zero-Shot Segmentation", "author": "Thomas Stegm\u00fcller and Tim Lebailly and Nikola Dukic and Behzad Bozorgtabar and Tinne Tuytelaars and Jean-Philippe Thiran", "abstract": "  Zero-shot classification capabilities naturally arise in models trained\nwithin a vision-language contrastive framework. Despite their classification\nprowess, these models struggle in dense tasks like zero-shot open-vocabulary\nsegmentation. This deficiency is often attributed to the absence of\nlocalization cues in captions and the intertwined nature of the learning\nprocess, which encompasses both image representation learning and\ncross-modality alignment. To tackle these issues, we propose SimZSS, a Simple\nframework for open-vocabulary Zero-Shot Segmentation. The method is founded on\ntwo key principles: i) leveraging frozen vision-only models that exhibit\nspatial awareness while exclusively aligning the text encoder and ii)\nexploiting the discrete nature of text and linguistic knowledge to pinpoint\nlocal concepts within captions. By capitalizing on the quality of the visual\nrepresentations, our method requires only image-caption pairs datasets and\nadapts to both small curated and large-scale noisy datasets. When trained on\nCOCO Captions across 8 GPUs, SimZSS achieves state-of-the-art results on 7 out\nof 8 benchmark datasets in less than 15 minutes.\n", "link": "http://arxiv.org/abs/2406.16085v3", "date": "2025-10-15", "relevancy": 2.8789, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5847}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5847}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Simple%20Framework%20for%20Open-Vocabulary%20Zero-Shot%20Segmentation&body=Title%3A%20A%20Simple%20Framework%20for%20Open-Vocabulary%20Zero-Shot%20Segmentation%0AAuthor%3A%20Thomas%20Stegm%C3%BCller%20and%20Tim%20Lebailly%20and%20Nikola%20Dukic%20and%20Behzad%20Bozorgtabar%20and%20Tinne%20Tuytelaars%20and%20Jean-Philippe%20Thiran%0AAbstract%3A%20%20%20Zero-shot%20classification%20capabilities%20naturally%20arise%20in%20models%20trained%0Awithin%20a%20vision-language%20contrastive%20framework.%20Despite%20their%20classification%0Aprowess%2C%20these%20models%20struggle%20in%20dense%20tasks%20like%20zero-shot%20open-vocabulary%0Asegmentation.%20This%20deficiency%20is%20often%20attributed%20to%20the%20absence%20of%0Alocalization%20cues%20in%20captions%20and%20the%20intertwined%20nature%20of%20the%20learning%0Aprocess%2C%20which%20encompasses%20both%20image%20representation%20learning%20and%0Across-modality%20alignment.%20To%20tackle%20these%20issues%2C%20we%20propose%20SimZSS%2C%20a%20Simple%0Aframework%20for%20open-vocabulary%20Zero-Shot%20Segmentation.%20The%20method%20is%20founded%20on%0Atwo%20key%20principles%3A%20i%29%20leveraging%20frozen%20vision-only%20models%20that%20exhibit%0Aspatial%20awareness%20while%20exclusively%20aligning%20the%20text%20encoder%20and%20ii%29%0Aexploiting%20the%20discrete%20nature%20of%20text%20and%20linguistic%20knowledge%20to%20pinpoint%0Alocal%20concepts%20within%20captions.%20By%20capitalizing%20on%20the%20quality%20of%20the%20visual%0Arepresentations%2C%20our%20method%20requires%20only%20image-caption%20pairs%20datasets%20and%0Aadapts%20to%20both%20small%20curated%20and%20large-scale%20noisy%20datasets.%20When%20trained%20on%0ACOCO%20Captions%20across%208%20GPUs%2C%20SimZSS%20achieves%20state-of-the-art%20results%20on%207%20out%0Aof%208%20benchmark%20datasets%20in%20less%20than%2015%20minutes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16085v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Simple%2520Framework%2520for%2520Open-Vocabulary%2520Zero-Shot%2520Segmentation%26entry.906535625%3DThomas%2520Stegm%25C3%25BCller%2520and%2520Tim%2520Lebailly%2520and%2520Nikola%2520Dukic%2520and%2520Behzad%2520Bozorgtabar%2520and%2520Tinne%2520Tuytelaars%2520and%2520Jean-Philippe%2520Thiran%26entry.1292438233%3D%2520%2520Zero-shot%2520classification%2520capabilities%2520naturally%2520arise%2520in%2520models%2520trained%250Awithin%2520a%2520vision-language%2520contrastive%2520framework.%2520Despite%2520their%2520classification%250Aprowess%252C%2520these%2520models%2520struggle%2520in%2520dense%2520tasks%2520like%2520zero-shot%2520open-vocabulary%250Asegmentation.%2520This%2520deficiency%2520is%2520often%2520attributed%2520to%2520the%2520absence%2520of%250Alocalization%2520cues%2520in%2520captions%2520and%2520the%2520intertwined%2520nature%2520of%2520the%2520learning%250Aprocess%252C%2520which%2520encompasses%2520both%2520image%2520representation%2520learning%2520and%250Across-modality%2520alignment.%2520To%2520tackle%2520these%2520issues%252C%2520we%2520propose%2520SimZSS%252C%2520a%2520Simple%250Aframework%2520for%2520open-vocabulary%2520Zero-Shot%2520Segmentation.%2520The%2520method%2520is%2520founded%2520on%250Atwo%2520key%2520principles%253A%2520i%2529%2520leveraging%2520frozen%2520vision-only%2520models%2520that%2520exhibit%250Aspatial%2520awareness%2520while%2520exclusively%2520aligning%2520the%2520text%2520encoder%2520and%2520ii%2529%250Aexploiting%2520the%2520discrete%2520nature%2520of%2520text%2520and%2520linguistic%2520knowledge%2520to%2520pinpoint%250Alocal%2520concepts%2520within%2520captions.%2520By%2520capitalizing%2520on%2520the%2520quality%2520of%2520the%2520visual%250Arepresentations%252C%2520our%2520method%2520requires%2520only%2520image-caption%2520pairs%2520datasets%2520and%250Aadapts%2520to%2520both%2520small%2520curated%2520and%2520large-scale%2520noisy%2520datasets.%2520When%2520trained%2520on%250ACOCO%2520Captions%2520across%25208%2520GPUs%252C%2520SimZSS%2520achieves%2520state-of-the-art%2520results%2520on%25207%2520out%250Aof%25208%2520benchmark%2520datasets%2520in%2520less%2520than%252015%2520minutes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16085v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Simple%20Framework%20for%20Open-Vocabulary%20Zero-Shot%20Segmentation&entry.906535625=Thomas%20Stegm%C3%BCller%20and%20Tim%20Lebailly%20and%20Nikola%20Dukic%20and%20Behzad%20Bozorgtabar%20and%20Tinne%20Tuytelaars%20and%20Jean-Philippe%20Thiran&entry.1292438233=%20%20Zero-shot%20classification%20capabilities%20naturally%20arise%20in%20models%20trained%0Awithin%20a%20vision-language%20contrastive%20framework.%20Despite%20their%20classification%0Aprowess%2C%20these%20models%20struggle%20in%20dense%20tasks%20like%20zero-shot%20open-vocabulary%0Asegmentation.%20This%20deficiency%20is%20often%20attributed%20to%20the%20absence%20of%0Alocalization%20cues%20in%20captions%20and%20the%20intertwined%20nature%20of%20the%20learning%0Aprocess%2C%20which%20encompasses%20both%20image%20representation%20learning%20and%0Across-modality%20alignment.%20To%20tackle%20these%20issues%2C%20we%20propose%20SimZSS%2C%20a%20Simple%0Aframework%20for%20open-vocabulary%20Zero-Shot%20Segmentation.%20The%20method%20is%20founded%20on%0Atwo%20key%20principles%3A%20i%29%20leveraging%20frozen%20vision-only%20models%20that%20exhibit%0Aspatial%20awareness%20while%20exclusively%20aligning%20the%20text%20encoder%20and%20ii%29%0Aexploiting%20the%20discrete%20nature%20of%20text%20and%20linguistic%20knowledge%20to%20pinpoint%0Alocal%20concepts%20within%20captions.%20By%20capitalizing%20on%20the%20quality%20of%20the%20visual%0Arepresentations%2C%20our%20method%20requires%20only%20image-caption%20pairs%20datasets%20and%0Aadapts%20to%20both%20small%20curated%20and%20large-scale%20noisy%20datasets.%20When%20trained%20on%0ACOCO%20Captions%20across%208%20GPUs%2C%20SimZSS%20achieves%20state-of-the-art%20results%20on%207%20out%0Aof%208%20benchmark%20datasets%20in%20less%20than%2015%20minutes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16085v3&entry.124074799=Read"},
{"title": "MULTI: Multimodal Understanding Leaderboard with Text and Images", "author": "Zichen Zhu and Yang Xu and Lu Chen and Jingkai Yang and Yichuan Ma and Yiming Sun and Hailin Wen and Jiaqi Liu and Jinyu Cai and Yingzi Ma and Situo Zhang and Zihan Zhao and Liangtai Sun and Kai Yu", "abstract": "  The rapid development of multimodal large language models (MLLMs) raises the\nquestion of how they compare to human performance. While existing datasets\noften feature synthetic or overly simplistic tasks, some models have already\nsurpassed human expert baselines. In this paper, we present MULTI, a Chinese\nmultimodal dataset derived from authentic examination questions. Comprising\nover 18,000 carefully selected and refined questions, MULTI evaluates models\nusing real-world examination standards, encompassing image-text comprehension,\ncomplex reasoning, and knowledge recall. Additionally, We also introduce\nMULTI-Elite, a 500-question selected hard subset, and MULTI-Extend with more\nthan 4,500 external knowledge context pieces for testing in-context learning\ncapabilities. Our evaluation highlights substantial room for MLLM advancement,\nwith Qwen2-VL-72B achieving a 76.9% accuracy on MULTI and 53.1% on MULTI-Elite\nleading 25 evaluated models, compared to human expert baselines of 86.1% and\n73.1%. MULTI serves not only as a robust evaluation platform but also paves the\nway for the development of expert-level AI.\n", "link": "http://arxiv.org/abs/2402.03173v4", "date": "2025-10-15", "relevancy": 2.8592, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5806}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5674}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5674}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MULTI%3A%20Multimodal%20Understanding%20Leaderboard%20with%20Text%20and%20Images&body=Title%3A%20MULTI%3A%20Multimodal%20Understanding%20Leaderboard%20with%20Text%20and%20Images%0AAuthor%3A%20Zichen%20Zhu%20and%20Yang%20Xu%20and%20Lu%20Chen%20and%20Jingkai%20Yang%20and%20Yichuan%20Ma%20and%20Yiming%20Sun%20and%20Hailin%20Wen%20and%20Jiaqi%20Liu%20and%20Jinyu%20Cai%20and%20Yingzi%20Ma%20and%20Situo%20Zhang%20and%20Zihan%20Zhao%20and%20Liangtai%20Sun%20and%20Kai%20Yu%0AAbstract%3A%20%20%20The%20rapid%20development%20of%20multimodal%20large%20language%20models%20%28MLLMs%29%20raises%20the%0Aquestion%20of%20how%20they%20compare%20to%20human%20performance.%20While%20existing%20datasets%0Aoften%20feature%20synthetic%20or%20overly%20simplistic%20tasks%2C%20some%20models%20have%20already%0Asurpassed%20human%20expert%20baselines.%20In%20this%20paper%2C%20we%20present%20MULTI%2C%20a%20Chinese%0Amultimodal%20dataset%20derived%20from%20authentic%20examination%20questions.%20Comprising%0Aover%2018%2C000%20carefully%20selected%20and%20refined%20questions%2C%20MULTI%20evaluates%20models%0Ausing%20real-world%20examination%20standards%2C%20encompassing%20image-text%20comprehension%2C%0Acomplex%20reasoning%2C%20and%20knowledge%20recall.%20Additionally%2C%20We%20also%20introduce%0AMULTI-Elite%2C%20a%20500-question%20selected%20hard%20subset%2C%20and%20MULTI-Extend%20with%20more%0Athan%204%2C500%20external%20knowledge%20context%20pieces%20for%20testing%20in-context%20learning%0Acapabilities.%20Our%20evaluation%20highlights%20substantial%20room%20for%20MLLM%20advancement%2C%0Awith%20Qwen2-VL-72B%20achieving%20a%2076.9%25%20accuracy%20on%20MULTI%20and%2053.1%25%20on%20MULTI-Elite%0Aleading%2025%20evaluated%20models%2C%20compared%20to%20human%20expert%20baselines%20of%2086.1%25%20and%0A73.1%25.%20MULTI%20serves%20not%20only%20as%20a%20robust%20evaluation%20platform%20but%20also%20paves%20the%0Away%20for%20the%20development%20of%20expert-level%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03173v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMULTI%253A%2520Multimodal%2520Understanding%2520Leaderboard%2520with%2520Text%2520and%2520Images%26entry.906535625%3DZichen%2520Zhu%2520and%2520Yang%2520Xu%2520and%2520Lu%2520Chen%2520and%2520Jingkai%2520Yang%2520and%2520Yichuan%2520Ma%2520and%2520Yiming%2520Sun%2520and%2520Hailin%2520Wen%2520and%2520Jiaqi%2520Liu%2520and%2520Jinyu%2520Cai%2520and%2520Yingzi%2520Ma%2520and%2520Situo%2520Zhang%2520and%2520Zihan%2520Zhao%2520and%2520Liangtai%2520Sun%2520and%2520Kai%2520Yu%26entry.1292438233%3D%2520%2520The%2520rapid%2520development%2520of%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520raises%2520the%250Aquestion%2520of%2520how%2520they%2520compare%2520to%2520human%2520performance.%2520While%2520existing%2520datasets%250Aoften%2520feature%2520synthetic%2520or%2520overly%2520simplistic%2520tasks%252C%2520some%2520models%2520have%2520already%250Asurpassed%2520human%2520expert%2520baselines.%2520In%2520this%2520paper%252C%2520we%2520present%2520MULTI%252C%2520a%2520Chinese%250Amultimodal%2520dataset%2520derived%2520from%2520authentic%2520examination%2520questions.%2520Comprising%250Aover%252018%252C000%2520carefully%2520selected%2520and%2520refined%2520questions%252C%2520MULTI%2520evaluates%2520models%250Ausing%2520real-world%2520examination%2520standards%252C%2520encompassing%2520image-text%2520comprehension%252C%250Acomplex%2520reasoning%252C%2520and%2520knowledge%2520recall.%2520Additionally%252C%2520We%2520also%2520introduce%250AMULTI-Elite%252C%2520a%2520500-question%2520selected%2520hard%2520subset%252C%2520and%2520MULTI-Extend%2520with%2520more%250Athan%25204%252C500%2520external%2520knowledge%2520context%2520pieces%2520for%2520testing%2520in-context%2520learning%250Acapabilities.%2520Our%2520evaluation%2520highlights%2520substantial%2520room%2520for%2520MLLM%2520advancement%252C%250Awith%2520Qwen2-VL-72B%2520achieving%2520a%252076.9%2525%2520accuracy%2520on%2520MULTI%2520and%252053.1%2525%2520on%2520MULTI-Elite%250Aleading%252025%2520evaluated%2520models%252C%2520compared%2520to%2520human%2520expert%2520baselines%2520of%252086.1%2525%2520and%250A73.1%2525.%2520MULTI%2520serves%2520not%2520only%2520as%2520a%2520robust%2520evaluation%2520platform%2520but%2520also%2520paves%2520the%250Away%2520for%2520the%2520development%2520of%2520expert-level%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03173v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MULTI%3A%20Multimodal%20Understanding%20Leaderboard%20with%20Text%20and%20Images&entry.906535625=Zichen%20Zhu%20and%20Yang%20Xu%20and%20Lu%20Chen%20and%20Jingkai%20Yang%20and%20Yichuan%20Ma%20and%20Yiming%20Sun%20and%20Hailin%20Wen%20and%20Jiaqi%20Liu%20and%20Jinyu%20Cai%20and%20Yingzi%20Ma%20and%20Situo%20Zhang%20and%20Zihan%20Zhao%20and%20Liangtai%20Sun%20and%20Kai%20Yu&entry.1292438233=%20%20The%20rapid%20development%20of%20multimodal%20large%20language%20models%20%28MLLMs%29%20raises%20the%0Aquestion%20of%20how%20they%20compare%20to%20human%20performance.%20While%20existing%20datasets%0Aoften%20feature%20synthetic%20or%20overly%20simplistic%20tasks%2C%20some%20models%20have%20already%0Asurpassed%20human%20expert%20baselines.%20In%20this%20paper%2C%20we%20present%20MULTI%2C%20a%20Chinese%0Amultimodal%20dataset%20derived%20from%20authentic%20examination%20questions.%20Comprising%0Aover%2018%2C000%20carefully%20selected%20and%20refined%20questions%2C%20MULTI%20evaluates%20models%0Ausing%20real-world%20examination%20standards%2C%20encompassing%20image-text%20comprehension%2C%0Acomplex%20reasoning%2C%20and%20knowledge%20recall.%20Additionally%2C%20We%20also%20introduce%0AMULTI-Elite%2C%20a%20500-question%20selected%20hard%20subset%2C%20and%20MULTI-Extend%20with%20more%0Athan%204%2C500%20external%20knowledge%20context%20pieces%20for%20testing%20in-context%20learning%0Acapabilities.%20Our%20evaluation%20highlights%20substantial%20room%20for%20MLLM%20advancement%2C%0Awith%20Qwen2-VL-72B%20achieving%20a%2076.9%25%20accuracy%20on%20MULTI%20and%2053.1%25%20on%20MULTI-Elite%0Aleading%2025%20evaluated%20models%2C%20compared%20to%20human%20expert%20baselines%20of%2086.1%25%20and%0A73.1%25.%20MULTI%20serves%20not%20only%20as%20a%20robust%20evaluation%20platform%20but%20also%20paves%20the%0Away%20for%20the%20development%20of%20expert-level%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03173v4&entry.124074799=Read"},
{"title": "RECODE: Reasoning Through Code Generation for Visual Question Answering", "author": "Junhong Shen and Mu Cai and Bo Hu and Ameet Talwalkar and David A Ross and Cordelia Schmid and Alireza Fathi", "abstract": "  Multimodal Large Language Models (MLLMs) struggle with precise reasoning for\nstructured visuals like charts and diagrams, as pixel-based perception lacks a\nmechanism for verification. To address this, we propose to leverage derendering\n-- the process of reverse-engineering visuals into executable code -- as a new\nmodality for verifiable visual reasoning. Specifically, we propose RECODE, an\nagentic framework that first generates multiple candidate programs to reproduce\nthe input image. It then uses a critic to select the most faithful\nreconstruction and iteratively refines the code. This process not only\ntransforms an ambiguous perceptual task into a verifiable, symbolic problem,\nbut also enables precise calculations and logical inferences later on. On\nvarious visual reasoning benchmarks such as CharXiv, ChartQA, and Geometry3K,\nRECODE significantly outperforms methods that do not leverage code or only use\ncode for drawing auxiliary lines or cropping. Our work demonstrates that\ngrounding visual perception in executable code provides a new path toward more\naccurate and verifiable multimodal reasoning.\n", "link": "http://arxiv.org/abs/2510.13756v1", "date": "2025-10-15", "relevancy": 2.828, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.58}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.58}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RECODE%3A%20Reasoning%20Through%20Code%20Generation%20for%20Visual%20Question%20Answering&body=Title%3A%20RECODE%3A%20Reasoning%20Through%20Code%20Generation%20for%20Visual%20Question%20Answering%0AAuthor%3A%20Junhong%20Shen%20and%20Mu%20Cai%20and%20Bo%20Hu%20and%20Ameet%20Talwalkar%20and%20David%20A%20Ross%20and%20Cordelia%20Schmid%20and%20Alireza%20Fathi%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20struggle%20with%20precise%20reasoning%20for%0Astructured%20visuals%20like%20charts%20and%20diagrams%2C%20as%20pixel-based%20perception%20lacks%20a%0Amechanism%20for%20verification.%20To%20address%20this%2C%20we%20propose%20to%20leverage%20derendering%0A--%20the%20process%20of%20reverse-engineering%20visuals%20into%20executable%20code%20--%20as%20a%20new%0Amodality%20for%20verifiable%20visual%20reasoning.%20Specifically%2C%20we%20propose%20RECODE%2C%20an%0Aagentic%20framework%20that%20first%20generates%20multiple%20candidate%20programs%20to%20reproduce%0Athe%20input%20image.%20It%20then%20uses%20a%20critic%20to%20select%20the%20most%20faithful%0Areconstruction%20and%20iteratively%20refines%20the%20code.%20This%20process%20not%20only%0Atransforms%20an%20ambiguous%20perceptual%20task%20into%20a%20verifiable%2C%20symbolic%20problem%2C%0Abut%20also%20enables%20precise%20calculations%20and%20logical%20inferences%20later%20on.%20On%0Avarious%20visual%20reasoning%20benchmarks%20such%20as%20CharXiv%2C%20ChartQA%2C%20and%20Geometry3K%2C%0ARECODE%20significantly%20outperforms%20methods%20that%20do%20not%20leverage%20code%20or%20only%20use%0Acode%20for%20drawing%20auxiliary%20lines%20or%20cropping.%20Our%20work%20demonstrates%20that%0Agrounding%20visual%20perception%20in%20executable%20code%20provides%20a%20new%20path%20toward%20more%0Aaccurate%20and%20verifiable%20multimodal%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13756v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRECODE%253A%2520Reasoning%2520Through%2520Code%2520Generation%2520for%2520Visual%2520Question%2520Answering%26entry.906535625%3DJunhong%2520Shen%2520and%2520Mu%2520Cai%2520and%2520Bo%2520Hu%2520and%2520Ameet%2520Talwalkar%2520and%2520David%2520A%2520Ross%2520and%2520Cordelia%2520Schmid%2520and%2520Alireza%2520Fathi%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520struggle%2520with%2520precise%2520reasoning%2520for%250Astructured%2520visuals%2520like%2520charts%2520and%2520diagrams%252C%2520as%2520pixel-based%2520perception%2520lacks%2520a%250Amechanism%2520for%2520verification.%2520To%2520address%2520this%252C%2520we%2520propose%2520to%2520leverage%2520derendering%250A--%2520the%2520process%2520of%2520reverse-engineering%2520visuals%2520into%2520executable%2520code%2520--%2520as%2520a%2520new%250Amodality%2520for%2520verifiable%2520visual%2520reasoning.%2520Specifically%252C%2520we%2520propose%2520RECODE%252C%2520an%250Aagentic%2520framework%2520that%2520first%2520generates%2520multiple%2520candidate%2520programs%2520to%2520reproduce%250Athe%2520input%2520image.%2520It%2520then%2520uses%2520a%2520critic%2520to%2520select%2520the%2520most%2520faithful%250Areconstruction%2520and%2520iteratively%2520refines%2520the%2520code.%2520This%2520process%2520not%2520only%250Atransforms%2520an%2520ambiguous%2520perceptual%2520task%2520into%2520a%2520verifiable%252C%2520symbolic%2520problem%252C%250Abut%2520also%2520enables%2520precise%2520calculations%2520and%2520logical%2520inferences%2520later%2520on.%2520On%250Avarious%2520visual%2520reasoning%2520benchmarks%2520such%2520as%2520CharXiv%252C%2520ChartQA%252C%2520and%2520Geometry3K%252C%250ARECODE%2520significantly%2520outperforms%2520methods%2520that%2520do%2520not%2520leverage%2520code%2520or%2520only%2520use%250Acode%2520for%2520drawing%2520auxiliary%2520lines%2520or%2520cropping.%2520Our%2520work%2520demonstrates%2520that%250Agrounding%2520visual%2520perception%2520in%2520executable%2520code%2520provides%2520a%2520new%2520path%2520toward%2520more%250Aaccurate%2520and%2520verifiable%2520multimodal%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13756v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RECODE%3A%20Reasoning%20Through%20Code%20Generation%20for%20Visual%20Question%20Answering&entry.906535625=Junhong%20Shen%20and%20Mu%20Cai%20and%20Bo%20Hu%20and%20Ameet%20Talwalkar%20and%20David%20A%20Ross%20and%20Cordelia%20Schmid%20and%20Alireza%20Fathi&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20struggle%20with%20precise%20reasoning%20for%0Astructured%20visuals%20like%20charts%20and%20diagrams%2C%20as%20pixel-based%20perception%20lacks%20a%0Amechanism%20for%20verification.%20To%20address%20this%2C%20we%20propose%20to%20leverage%20derendering%0A--%20the%20process%20of%20reverse-engineering%20visuals%20into%20executable%20code%20--%20as%20a%20new%0Amodality%20for%20verifiable%20visual%20reasoning.%20Specifically%2C%20we%20propose%20RECODE%2C%20an%0Aagentic%20framework%20that%20first%20generates%20multiple%20candidate%20programs%20to%20reproduce%0Athe%20input%20image.%20It%20then%20uses%20a%20critic%20to%20select%20the%20most%20faithful%0Areconstruction%20and%20iteratively%20refines%20the%20code.%20This%20process%20not%20only%0Atransforms%20an%20ambiguous%20perceptual%20task%20into%20a%20verifiable%2C%20symbolic%20problem%2C%0Abut%20also%20enables%20precise%20calculations%20and%20logical%20inferences%20later%20on.%20On%0Avarious%20visual%20reasoning%20benchmarks%20such%20as%20CharXiv%2C%20ChartQA%2C%20and%20Geometry3K%2C%0ARECODE%20significantly%20outperforms%20methods%20that%20do%20not%20leverage%20code%20or%20only%20use%0Acode%20for%20drawing%20auxiliary%20lines%20or%20cropping.%20Our%20work%20demonstrates%20that%0Agrounding%20visual%20perception%20in%20executable%20code%20provides%20a%20new%20path%20toward%20more%0Aaccurate%20and%20verifiable%20multimodal%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13756v1&entry.124074799=Read"},
{"title": "Generative Universal Verifier as Multimodal Meta-Reasoner", "author": "Xinchen Zhang and Xiaoying Zhang and Youbin Wu and Yanbin Cao and Renrui Zhang and Ruihang Chu and Ling Yang and Yujiu Yang", "abstract": "  We introduce Generative Universal Verifier, a novel concept and plugin\ndesigned for next-generation multimodal reasoning in vision-language models and\nunified multimodal models, providing the fundamental capability of reflection\nand refinement on visual outcomes during the reasoning and generation process.\nThis work makes three main contributions: (1) We build ViVerBench, a\ncomprehensive benchmark spanning 16 categories of critical tasks for evaluating\nvisual outcomes in multimodal reasoning. Results show that existing VLMs\nconsistently underperform across these tasks, underscoring a substantial gap\nfrom human-level capability in reliable visual verification. (2) We design two\nautomated pipelines to construct large-scale visual verification data and train\nOmniVerifier-7B, the first omni-capable generative verifier trained for\nuniversal visual verification and achieves notable gains on ViVerBench(+8.3).\nThrough training, we identify three atomic capabilities in visual verification\nand demonstrate how they generalize and interact synergistically. (3) We\npropose OmniVerifier-TTS, a sequential test-time scaling paradigm that\nleverages the universal verifier to bridge image generation and editing within\nunified models, enhancing the upper bound of generative ability through\niterative fine-grained optimization. Beyond generation, we extend universal\nverifier to broader world-modeling interleaved reasoning scenarios.\nEmpirically, OmniVerifier-TTS achieves improvements on T2I-ReasonBench(+3.7),\nand GenEval++(+4.3), outperforming existing parallel test-time scaling methods,\nsuch as Best-of-N. By endowing multimodal reasoning with reliable visual\nverification, OmniVerifier advances both reliable reflection during generation\nand scalable test-time refinement, marking a step toward more trustworthy and\ncontrollable next-generation reasoning systems.\n", "link": "http://arxiv.org/abs/2510.13804v1", "date": "2025-10-15", "relevancy": 2.8092, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5639}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5639}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Universal%20Verifier%20as%20Multimodal%20Meta-Reasoner&body=Title%3A%20Generative%20Universal%20Verifier%20as%20Multimodal%20Meta-Reasoner%0AAuthor%3A%20Xinchen%20Zhang%20and%20Xiaoying%20Zhang%20and%20Youbin%20Wu%20and%20Yanbin%20Cao%20and%20Renrui%20Zhang%20and%20Ruihang%20Chu%20and%20Ling%20Yang%20and%20Yujiu%20Yang%0AAbstract%3A%20%20%20We%20introduce%20Generative%20Universal%20Verifier%2C%20a%20novel%20concept%20and%20plugin%0Adesigned%20for%20next-generation%20multimodal%20reasoning%20in%20vision-language%20models%20and%0Aunified%20multimodal%20models%2C%20providing%20the%20fundamental%20capability%20of%20reflection%0Aand%20refinement%20on%20visual%20outcomes%20during%20the%20reasoning%20and%20generation%20process.%0AThis%20work%20makes%20three%20main%20contributions%3A%20%281%29%20We%20build%20ViVerBench%2C%20a%0Acomprehensive%20benchmark%20spanning%2016%20categories%20of%20critical%20tasks%20for%20evaluating%0Avisual%20outcomes%20in%20multimodal%20reasoning.%20Results%20show%20that%20existing%20VLMs%0Aconsistently%20underperform%20across%20these%20tasks%2C%20underscoring%20a%20substantial%20gap%0Afrom%20human-level%20capability%20in%20reliable%20visual%20verification.%20%282%29%20We%20design%20two%0Aautomated%20pipelines%20to%20construct%20large-scale%20visual%20verification%20data%20and%20train%0AOmniVerifier-7B%2C%20the%20first%20omni-capable%20generative%20verifier%20trained%20for%0Auniversal%20visual%20verification%20and%20achieves%20notable%20gains%20on%20ViVerBench%28%2B8.3%29.%0AThrough%20training%2C%20we%20identify%20three%20atomic%20capabilities%20in%20visual%20verification%0Aand%20demonstrate%20how%20they%20generalize%20and%20interact%20synergistically.%20%283%29%20We%0Apropose%20OmniVerifier-TTS%2C%20a%20sequential%20test-time%20scaling%20paradigm%20that%0Aleverages%20the%20universal%20verifier%20to%20bridge%20image%20generation%20and%20editing%20within%0Aunified%20models%2C%20enhancing%20the%20upper%20bound%20of%20generative%20ability%20through%0Aiterative%20fine-grained%20optimization.%20Beyond%20generation%2C%20we%20extend%20universal%0Averifier%20to%20broader%20world-modeling%20interleaved%20reasoning%20scenarios.%0AEmpirically%2C%20OmniVerifier-TTS%20achieves%20improvements%20on%20T2I-ReasonBench%28%2B3.7%29%2C%0Aand%20GenEval%2B%2B%28%2B4.3%29%2C%20outperforming%20existing%20parallel%20test-time%20scaling%20methods%2C%0Asuch%20as%20Best-of-N.%20By%20endowing%20multimodal%20reasoning%20with%20reliable%20visual%0Averification%2C%20OmniVerifier%20advances%20both%20reliable%20reflection%20during%20generation%0Aand%20scalable%20test-time%20refinement%2C%20marking%20a%20step%20toward%20more%20trustworthy%20and%0Acontrollable%20next-generation%20reasoning%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13804v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Universal%2520Verifier%2520as%2520Multimodal%2520Meta-Reasoner%26entry.906535625%3DXinchen%2520Zhang%2520and%2520Xiaoying%2520Zhang%2520and%2520Youbin%2520Wu%2520and%2520Yanbin%2520Cao%2520and%2520Renrui%2520Zhang%2520and%2520Ruihang%2520Chu%2520and%2520Ling%2520Yang%2520and%2520Yujiu%2520Yang%26entry.1292438233%3D%2520%2520We%2520introduce%2520Generative%2520Universal%2520Verifier%252C%2520a%2520novel%2520concept%2520and%2520plugin%250Adesigned%2520for%2520next-generation%2520multimodal%2520reasoning%2520in%2520vision-language%2520models%2520and%250Aunified%2520multimodal%2520models%252C%2520providing%2520the%2520fundamental%2520capability%2520of%2520reflection%250Aand%2520refinement%2520on%2520visual%2520outcomes%2520during%2520the%2520reasoning%2520and%2520generation%2520process.%250AThis%2520work%2520makes%2520three%2520main%2520contributions%253A%2520%25281%2529%2520We%2520build%2520ViVerBench%252C%2520a%250Acomprehensive%2520benchmark%2520spanning%252016%2520categories%2520of%2520critical%2520tasks%2520for%2520evaluating%250Avisual%2520outcomes%2520in%2520multimodal%2520reasoning.%2520Results%2520show%2520that%2520existing%2520VLMs%250Aconsistently%2520underperform%2520across%2520these%2520tasks%252C%2520underscoring%2520a%2520substantial%2520gap%250Afrom%2520human-level%2520capability%2520in%2520reliable%2520visual%2520verification.%2520%25282%2529%2520We%2520design%2520two%250Aautomated%2520pipelines%2520to%2520construct%2520large-scale%2520visual%2520verification%2520data%2520and%2520train%250AOmniVerifier-7B%252C%2520the%2520first%2520omni-capable%2520generative%2520verifier%2520trained%2520for%250Auniversal%2520visual%2520verification%2520and%2520achieves%2520notable%2520gains%2520on%2520ViVerBench%2528%252B8.3%2529.%250AThrough%2520training%252C%2520we%2520identify%2520three%2520atomic%2520capabilities%2520in%2520visual%2520verification%250Aand%2520demonstrate%2520how%2520they%2520generalize%2520and%2520interact%2520synergistically.%2520%25283%2529%2520We%250Apropose%2520OmniVerifier-TTS%252C%2520a%2520sequential%2520test-time%2520scaling%2520paradigm%2520that%250Aleverages%2520the%2520universal%2520verifier%2520to%2520bridge%2520image%2520generation%2520and%2520editing%2520within%250Aunified%2520models%252C%2520enhancing%2520the%2520upper%2520bound%2520of%2520generative%2520ability%2520through%250Aiterative%2520fine-grained%2520optimization.%2520Beyond%2520generation%252C%2520we%2520extend%2520universal%250Averifier%2520to%2520broader%2520world-modeling%2520interleaved%2520reasoning%2520scenarios.%250AEmpirically%252C%2520OmniVerifier-TTS%2520achieves%2520improvements%2520on%2520T2I-ReasonBench%2528%252B3.7%2529%252C%250Aand%2520GenEval%252B%252B%2528%252B4.3%2529%252C%2520outperforming%2520existing%2520parallel%2520test-time%2520scaling%2520methods%252C%250Asuch%2520as%2520Best-of-N.%2520By%2520endowing%2520multimodal%2520reasoning%2520with%2520reliable%2520visual%250Averification%252C%2520OmniVerifier%2520advances%2520both%2520reliable%2520reflection%2520during%2520generation%250Aand%2520scalable%2520test-time%2520refinement%252C%2520marking%2520a%2520step%2520toward%2520more%2520trustworthy%2520and%250Acontrollable%2520next-generation%2520reasoning%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13804v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Universal%20Verifier%20as%20Multimodal%20Meta-Reasoner&entry.906535625=Xinchen%20Zhang%20and%20Xiaoying%20Zhang%20and%20Youbin%20Wu%20and%20Yanbin%20Cao%20and%20Renrui%20Zhang%20and%20Ruihang%20Chu%20and%20Ling%20Yang%20and%20Yujiu%20Yang&entry.1292438233=%20%20We%20introduce%20Generative%20Universal%20Verifier%2C%20a%20novel%20concept%20and%20plugin%0Adesigned%20for%20next-generation%20multimodal%20reasoning%20in%20vision-language%20models%20and%0Aunified%20multimodal%20models%2C%20providing%20the%20fundamental%20capability%20of%20reflection%0Aand%20refinement%20on%20visual%20outcomes%20during%20the%20reasoning%20and%20generation%20process.%0AThis%20work%20makes%20three%20main%20contributions%3A%20%281%29%20We%20build%20ViVerBench%2C%20a%0Acomprehensive%20benchmark%20spanning%2016%20categories%20of%20critical%20tasks%20for%20evaluating%0Avisual%20outcomes%20in%20multimodal%20reasoning.%20Results%20show%20that%20existing%20VLMs%0Aconsistently%20underperform%20across%20these%20tasks%2C%20underscoring%20a%20substantial%20gap%0Afrom%20human-level%20capability%20in%20reliable%20visual%20verification.%20%282%29%20We%20design%20two%0Aautomated%20pipelines%20to%20construct%20large-scale%20visual%20verification%20data%20and%20train%0AOmniVerifier-7B%2C%20the%20first%20omni-capable%20generative%20verifier%20trained%20for%0Auniversal%20visual%20verification%20and%20achieves%20notable%20gains%20on%20ViVerBench%28%2B8.3%29.%0AThrough%20training%2C%20we%20identify%20three%20atomic%20capabilities%20in%20visual%20verification%0Aand%20demonstrate%20how%20they%20generalize%20and%20interact%20synergistically.%20%283%29%20We%0Apropose%20OmniVerifier-TTS%2C%20a%20sequential%20test-time%20scaling%20paradigm%20that%0Aleverages%20the%20universal%20verifier%20to%20bridge%20image%20generation%20and%20editing%20within%0Aunified%20models%2C%20enhancing%20the%20upper%20bound%20of%20generative%20ability%20through%0Aiterative%20fine-grained%20optimization.%20Beyond%20generation%2C%20we%20extend%20universal%0Averifier%20to%20broader%20world-modeling%20interleaved%20reasoning%20scenarios.%0AEmpirically%2C%20OmniVerifier-TTS%20achieves%20improvements%20on%20T2I-ReasonBench%28%2B3.7%29%2C%0Aand%20GenEval%2B%2B%28%2B4.3%29%2C%20outperforming%20existing%20parallel%20test-time%20scaling%20methods%2C%0Asuch%20as%20Best-of-N.%20By%20endowing%20multimodal%20reasoning%20with%20reliable%20visual%0Averification%2C%20OmniVerifier%20advances%20both%20reliable%20reflection%20during%20generation%0Aand%20scalable%20test-time%20refinement%2C%20marking%20a%20step%20toward%20more%20trustworthy%20and%0Acontrollable%20next-generation%20reasoning%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13804v1&entry.124074799=Read"},
{"title": "Reasoning in Space via Grounding in the World", "author": "Yiming Chen and Zekun Qi and Wenyao Zhang and Xin Jin and Li Zhang and Peidong Liu", "abstract": "  In this paper, we claim that 3D visual grounding is the cornerstone of\nspatial reasoning and introduce the Grounded-Spatial Reasoner (GS-Reasoner) to\nexplore the effective spatial representations that bridge the gap between them.\nExisting 3D LLMs suffer from the absence of a unified 3D representation capable\nof jointly capturing semantic and geometric information. This deficiency is\nmanifested either in poor performance on grounding or in an excessive reliance\non external modules, ultimately hindering the seamless integration of grounding\nand spatial reasoning. To address this, we propose a simple yet effective\ndual-path pooling mechanism that tightly aligns geometric features with both\nsemantic and positional cues, constructing a unified image patch-based 3D\nrepresentation that encapsulates all essential information without increasing\nthe number of input tokens. Leveraging this holistic representation,\nGS-Reasoner is the first 3D LLM that achieves autoregressive grounding entirely\nwithout external modules while delivering performance comparable to\nstate-of-the-art models, establishing a unified and self-contained framework\nfor 3D spatial reasoning. To further bridge grounding and spatial reasoning, we\nintroduce the Grounded Chain-of-Thought (GCoT) dataset. This dataset is\nmeticulously curated to include both 3D bounding box annotations for objects\nreferenced in reasoning questions and step-by-step reasoning paths that\nintegrate grounding as a core component of the problem-solving process.\nExtensive experiments demonstrate that GS-Reasoner achieves impressive results\non 3D visual grounding, which in turn significantly enhances its spatial\nreasoning capabilities, leading to state-of-the-art performance.\n", "link": "http://arxiv.org/abs/2510.13800v1", "date": "2025-10-15", "relevancy": 2.787, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5686}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5686}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reasoning%20in%20Space%20via%20Grounding%20in%20the%20World&body=Title%3A%20Reasoning%20in%20Space%20via%20Grounding%20in%20the%20World%0AAuthor%3A%20Yiming%20Chen%20and%20Zekun%20Qi%20and%20Wenyao%20Zhang%20and%20Xin%20Jin%20and%20Li%20Zhang%20and%20Peidong%20Liu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20claim%20that%203D%20visual%20grounding%20is%20the%20cornerstone%20of%0Aspatial%20reasoning%20and%20introduce%20the%20Grounded-Spatial%20Reasoner%20%28GS-Reasoner%29%20to%0Aexplore%20the%20effective%20spatial%20representations%20that%20bridge%20the%20gap%20between%20them.%0AExisting%203D%20LLMs%20suffer%20from%20the%20absence%20of%20a%20unified%203D%20representation%20capable%0Aof%20jointly%20capturing%20semantic%20and%20geometric%20information.%20This%20deficiency%20is%0Amanifested%20either%20in%20poor%20performance%20on%20grounding%20or%20in%20an%20excessive%20reliance%0Aon%20external%20modules%2C%20ultimately%20hindering%20the%20seamless%20integration%20of%20grounding%0Aand%20spatial%20reasoning.%20To%20address%20this%2C%20we%20propose%20a%20simple%20yet%20effective%0Adual-path%20pooling%20mechanism%20that%20tightly%20aligns%20geometric%20features%20with%20both%0Asemantic%20and%20positional%20cues%2C%20constructing%20a%20unified%20image%20patch-based%203D%0Arepresentation%20that%20encapsulates%20all%20essential%20information%20without%20increasing%0Athe%20number%20of%20input%20tokens.%20Leveraging%20this%20holistic%20representation%2C%0AGS-Reasoner%20is%20the%20first%203D%20LLM%20that%20achieves%20autoregressive%20grounding%20entirely%0Awithout%20external%20modules%20while%20delivering%20performance%20comparable%20to%0Astate-of-the-art%20models%2C%20establishing%20a%20unified%20and%20self-contained%20framework%0Afor%203D%20spatial%20reasoning.%20To%20further%20bridge%20grounding%20and%20spatial%20reasoning%2C%20we%0Aintroduce%20the%20Grounded%20Chain-of-Thought%20%28GCoT%29%20dataset.%20This%20dataset%20is%0Ameticulously%20curated%20to%20include%20both%203D%20bounding%20box%20annotations%20for%20objects%0Areferenced%20in%20reasoning%20questions%20and%20step-by-step%20reasoning%20paths%20that%0Aintegrate%20grounding%20as%20a%20core%20component%20of%20the%20problem-solving%20process.%0AExtensive%20experiments%20demonstrate%20that%20GS-Reasoner%20achieves%20impressive%20results%0Aon%203D%20visual%20grounding%2C%20which%20in%20turn%20significantly%20enhances%20its%20spatial%0Areasoning%20capabilities%2C%20leading%20to%20state-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13800v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasoning%2520in%2520Space%2520via%2520Grounding%2520in%2520the%2520World%26entry.906535625%3DYiming%2520Chen%2520and%2520Zekun%2520Qi%2520and%2520Wenyao%2520Zhang%2520and%2520Xin%2520Jin%2520and%2520Li%2520Zhang%2520and%2520Peidong%2520Liu%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520claim%2520that%25203D%2520visual%2520grounding%2520is%2520the%2520cornerstone%2520of%250Aspatial%2520reasoning%2520and%2520introduce%2520the%2520Grounded-Spatial%2520Reasoner%2520%2528GS-Reasoner%2529%2520to%250Aexplore%2520the%2520effective%2520spatial%2520representations%2520that%2520bridge%2520the%2520gap%2520between%2520them.%250AExisting%25203D%2520LLMs%2520suffer%2520from%2520the%2520absence%2520of%2520a%2520unified%25203D%2520representation%2520capable%250Aof%2520jointly%2520capturing%2520semantic%2520and%2520geometric%2520information.%2520This%2520deficiency%2520is%250Amanifested%2520either%2520in%2520poor%2520performance%2520on%2520grounding%2520or%2520in%2520an%2520excessive%2520reliance%250Aon%2520external%2520modules%252C%2520ultimately%2520hindering%2520the%2520seamless%2520integration%2520of%2520grounding%250Aand%2520spatial%2520reasoning.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%250Adual-path%2520pooling%2520mechanism%2520that%2520tightly%2520aligns%2520geometric%2520features%2520with%2520both%250Asemantic%2520and%2520positional%2520cues%252C%2520constructing%2520a%2520unified%2520image%2520patch-based%25203D%250Arepresentation%2520that%2520encapsulates%2520all%2520essential%2520information%2520without%2520increasing%250Athe%2520number%2520of%2520input%2520tokens.%2520Leveraging%2520this%2520holistic%2520representation%252C%250AGS-Reasoner%2520is%2520the%2520first%25203D%2520LLM%2520that%2520achieves%2520autoregressive%2520grounding%2520entirely%250Awithout%2520external%2520modules%2520while%2520delivering%2520performance%2520comparable%2520to%250Astate-of-the-art%2520models%252C%2520establishing%2520a%2520unified%2520and%2520self-contained%2520framework%250Afor%25203D%2520spatial%2520reasoning.%2520To%2520further%2520bridge%2520grounding%2520and%2520spatial%2520reasoning%252C%2520we%250Aintroduce%2520the%2520Grounded%2520Chain-of-Thought%2520%2528GCoT%2529%2520dataset.%2520This%2520dataset%2520is%250Ameticulously%2520curated%2520to%2520include%2520both%25203D%2520bounding%2520box%2520annotations%2520for%2520objects%250Areferenced%2520in%2520reasoning%2520questions%2520and%2520step-by-step%2520reasoning%2520paths%2520that%250Aintegrate%2520grounding%2520as%2520a%2520core%2520component%2520of%2520the%2520problem-solving%2520process.%250AExtensive%2520experiments%2520demonstrate%2520that%2520GS-Reasoner%2520achieves%2520impressive%2520results%250Aon%25203D%2520visual%2520grounding%252C%2520which%2520in%2520turn%2520significantly%2520enhances%2520its%2520spatial%250Areasoning%2520capabilities%252C%2520leading%2520to%2520state-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13800v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reasoning%20in%20Space%20via%20Grounding%20in%20the%20World&entry.906535625=Yiming%20Chen%20and%20Zekun%20Qi%20and%20Wenyao%20Zhang%20and%20Xin%20Jin%20and%20Li%20Zhang%20and%20Peidong%20Liu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20claim%20that%203D%20visual%20grounding%20is%20the%20cornerstone%20of%0Aspatial%20reasoning%20and%20introduce%20the%20Grounded-Spatial%20Reasoner%20%28GS-Reasoner%29%20to%0Aexplore%20the%20effective%20spatial%20representations%20that%20bridge%20the%20gap%20between%20them.%0AExisting%203D%20LLMs%20suffer%20from%20the%20absence%20of%20a%20unified%203D%20representation%20capable%0Aof%20jointly%20capturing%20semantic%20and%20geometric%20information.%20This%20deficiency%20is%0Amanifested%20either%20in%20poor%20performance%20on%20grounding%20or%20in%20an%20excessive%20reliance%0Aon%20external%20modules%2C%20ultimately%20hindering%20the%20seamless%20integration%20of%20grounding%0Aand%20spatial%20reasoning.%20To%20address%20this%2C%20we%20propose%20a%20simple%20yet%20effective%0Adual-path%20pooling%20mechanism%20that%20tightly%20aligns%20geometric%20features%20with%20both%0Asemantic%20and%20positional%20cues%2C%20constructing%20a%20unified%20image%20patch-based%203D%0Arepresentation%20that%20encapsulates%20all%20essential%20information%20without%20increasing%0Athe%20number%20of%20input%20tokens.%20Leveraging%20this%20holistic%20representation%2C%0AGS-Reasoner%20is%20the%20first%203D%20LLM%20that%20achieves%20autoregressive%20grounding%20entirely%0Awithout%20external%20modules%20while%20delivering%20performance%20comparable%20to%0Astate-of-the-art%20models%2C%20establishing%20a%20unified%20and%20self-contained%20framework%0Afor%203D%20spatial%20reasoning.%20To%20further%20bridge%20grounding%20and%20spatial%20reasoning%2C%20we%0Aintroduce%20the%20Grounded%20Chain-of-Thought%20%28GCoT%29%20dataset.%20This%20dataset%20is%0Ameticulously%20curated%20to%20include%20both%203D%20bounding%20box%20annotations%20for%20objects%0Areferenced%20in%20reasoning%20questions%20and%20step-by-step%20reasoning%20paths%20that%0Aintegrate%20grounding%20as%20a%20core%20component%20of%20the%20problem-solving%20process.%0AExtensive%20experiments%20demonstrate%20that%20GS-Reasoner%20achieves%20impressive%20results%0Aon%203D%20visual%20grounding%2C%20which%20in%20turn%20significantly%20enhances%20its%20spatial%0Areasoning%20capabilities%2C%20leading%20to%20state-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13800v1&entry.124074799=Read"},
{"title": "PRISM: Self-Pruning Intrinsic Selection Method for Training-Free\n  Multimodal Data Selection", "author": "Jinhe Bi and Yifan Wang and Danqi Yan and  Aniri and Wenke Huang and Zengjie Jin and Xiaowen Ma and Artur Hecker and Mang Ye and Xun Xiao and Hinrich Schuetze and Volker Tresp and Yunpu Ma", "abstract": "  Visual instruction tuning adapts pre-trained Multimodal Large Language Models\n(MLLMs) to follow human instructions for real-world applications. However, the\nrapid growth of these datasets introduces significant redundancy, leading to\nincreased computational costs. Existing methods for selecting instruction data\naim to prune this redundancy, but predominantly rely on computationally\ndemanding techniques such as proxy-based inference or training-based metrics.\nConsequently, the substantial computational costs incurred by these selection\nprocesses often exacerbate the very efficiency bottlenecks they are intended to\nresolve, posing a significant challenge to the scalable and effective tuning of\nMLLMs. To address this challenge, we first identify a critical, yet previously\noverlooked, factor: the anisotropy inherent in visual feature distributions. We\nfind that this anisotropy induces a \\textit{Global Semantic Drift}, and\noverlooking this phenomenon is a key factor limiting the efficiency of current\ndata selection methods. Motivated by this insight, we devise \\textbf{PRISM},\nthe first training-free framework for efficient visual instruction selection.\nPRISM surgically removes the corrupting influence of global background features\nby modeling the intrinsic visual semantics via implicit re-centering.\nEmpirically, PRISM reduces the end-to-end time for data selection and model\ntuning to just 30\\% of conventional pipelines. More remarkably, it achieves\nthis efficiency while simultaneously enhancing performance, surpassing models\nfine-tuned on the full dataset across eight multimodal and three language\nunderstanding benchmarks, culminating in a 101.7\\% relative improvement over\nthe baseline. The code is available for access via\n\\href{https://github.com/bibisbar/PRISM}{this repository}.\n", "link": "http://arxiv.org/abs/2502.12119v2", "date": "2025-10-15", "relevancy": 2.7647, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5618}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5485}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PRISM%3A%20Self-Pruning%20Intrinsic%20Selection%20Method%20for%20Training-Free%0A%20%20Multimodal%20Data%20Selection&body=Title%3A%20PRISM%3A%20Self-Pruning%20Intrinsic%20Selection%20Method%20for%20Training-Free%0A%20%20Multimodal%20Data%20Selection%0AAuthor%3A%20Jinhe%20Bi%20and%20Yifan%20Wang%20and%20Danqi%20Yan%20and%20%20Aniri%20and%20Wenke%20Huang%20and%20Zengjie%20Jin%20and%20Xiaowen%20Ma%20and%20Artur%20Hecker%20and%20Mang%20Ye%20and%20Xun%20Xiao%20and%20Hinrich%20Schuetze%20and%20Volker%20Tresp%20and%20Yunpu%20Ma%0AAbstract%3A%20%20%20Visual%20instruction%20tuning%20adapts%20pre-trained%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29%20to%20follow%20human%20instructions%20for%20real-world%20applications.%20However%2C%20the%0Arapid%20growth%20of%20these%20datasets%20introduces%20significant%20redundancy%2C%20leading%20to%0Aincreased%20computational%20costs.%20Existing%20methods%20for%20selecting%20instruction%20data%0Aaim%20to%20prune%20this%20redundancy%2C%20but%20predominantly%20rely%20on%20computationally%0Ademanding%20techniques%20such%20as%20proxy-based%20inference%20or%20training-based%20metrics.%0AConsequently%2C%20the%20substantial%20computational%20costs%20incurred%20by%20these%20selection%0Aprocesses%20often%20exacerbate%20the%20very%20efficiency%20bottlenecks%20they%20are%20intended%20to%0Aresolve%2C%20posing%20a%20significant%20challenge%20to%20the%20scalable%20and%20effective%20tuning%20of%0AMLLMs.%20To%20address%20this%20challenge%2C%20we%20first%20identify%20a%20critical%2C%20yet%20previously%0Aoverlooked%2C%20factor%3A%20the%20anisotropy%20inherent%20in%20visual%20feature%20distributions.%20We%0Afind%20that%20this%20anisotropy%20induces%20a%20%5Ctextit%7BGlobal%20Semantic%20Drift%7D%2C%20and%0Aoverlooking%20this%20phenomenon%20is%20a%20key%20factor%20limiting%20the%20efficiency%20of%20current%0Adata%20selection%20methods.%20Motivated%20by%20this%20insight%2C%20we%20devise%20%5Ctextbf%7BPRISM%7D%2C%0Athe%20first%20training-free%20framework%20for%20efficient%20visual%20instruction%20selection.%0APRISM%20surgically%20removes%20the%20corrupting%20influence%20of%20global%20background%20features%0Aby%20modeling%20the%20intrinsic%20visual%20semantics%20via%20implicit%20re-centering.%0AEmpirically%2C%20PRISM%20reduces%20the%20end-to-end%20time%20for%20data%20selection%20and%20model%0Atuning%20to%20just%2030%5C%25%20of%20conventional%20pipelines.%20More%20remarkably%2C%20it%20achieves%0Athis%20efficiency%20while%20simultaneously%20enhancing%20performance%2C%20surpassing%20models%0Afine-tuned%20on%20the%20full%20dataset%20across%20eight%20multimodal%20and%20three%20language%0Aunderstanding%20benchmarks%2C%20culminating%20in%20a%20101.7%5C%25%20relative%20improvement%20over%0Athe%20baseline.%20The%20code%20is%20available%20for%20access%20via%0A%5Chref%7Bhttps%3A//github.com/bibisbar/PRISM%7D%7Bthis%20repository%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12119v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPRISM%253A%2520Self-Pruning%2520Intrinsic%2520Selection%2520Method%2520for%2520Training-Free%250A%2520%2520Multimodal%2520Data%2520Selection%26entry.906535625%3DJinhe%2520Bi%2520and%2520Yifan%2520Wang%2520and%2520Danqi%2520Yan%2520and%2520%2520Aniri%2520and%2520Wenke%2520Huang%2520and%2520Zengjie%2520Jin%2520and%2520Xiaowen%2520Ma%2520and%2520Artur%2520Hecker%2520and%2520Mang%2520Ye%2520and%2520Xun%2520Xiao%2520and%2520Hinrich%2520Schuetze%2520and%2520Volker%2520Tresp%2520and%2520Yunpu%2520Ma%26entry.1292438233%3D%2520%2520Visual%2520instruction%2520tuning%2520adapts%2520pre-trained%2520Multimodal%2520Large%2520Language%2520Models%250A%2528MLLMs%2529%2520to%2520follow%2520human%2520instructions%2520for%2520real-world%2520applications.%2520However%252C%2520the%250Arapid%2520growth%2520of%2520these%2520datasets%2520introduces%2520significant%2520redundancy%252C%2520leading%2520to%250Aincreased%2520computational%2520costs.%2520Existing%2520methods%2520for%2520selecting%2520instruction%2520data%250Aaim%2520to%2520prune%2520this%2520redundancy%252C%2520but%2520predominantly%2520rely%2520on%2520computationally%250Ademanding%2520techniques%2520such%2520as%2520proxy-based%2520inference%2520or%2520training-based%2520metrics.%250AConsequently%252C%2520the%2520substantial%2520computational%2520costs%2520incurred%2520by%2520these%2520selection%250Aprocesses%2520often%2520exacerbate%2520the%2520very%2520efficiency%2520bottlenecks%2520they%2520are%2520intended%2520to%250Aresolve%252C%2520posing%2520a%2520significant%2520challenge%2520to%2520the%2520scalable%2520and%2520effective%2520tuning%2520of%250AMLLMs.%2520To%2520address%2520this%2520challenge%252C%2520we%2520first%2520identify%2520a%2520critical%252C%2520yet%2520previously%250Aoverlooked%252C%2520factor%253A%2520the%2520anisotropy%2520inherent%2520in%2520visual%2520feature%2520distributions.%2520We%250Afind%2520that%2520this%2520anisotropy%2520induces%2520a%2520%255Ctextit%257BGlobal%2520Semantic%2520Drift%257D%252C%2520and%250Aoverlooking%2520this%2520phenomenon%2520is%2520a%2520key%2520factor%2520limiting%2520the%2520efficiency%2520of%2520current%250Adata%2520selection%2520methods.%2520Motivated%2520by%2520this%2520insight%252C%2520we%2520devise%2520%255Ctextbf%257BPRISM%257D%252C%250Athe%2520first%2520training-free%2520framework%2520for%2520efficient%2520visual%2520instruction%2520selection.%250APRISM%2520surgically%2520removes%2520the%2520corrupting%2520influence%2520of%2520global%2520background%2520features%250Aby%2520modeling%2520the%2520intrinsic%2520visual%2520semantics%2520via%2520implicit%2520re-centering.%250AEmpirically%252C%2520PRISM%2520reduces%2520the%2520end-to-end%2520time%2520for%2520data%2520selection%2520and%2520model%250Atuning%2520to%2520just%252030%255C%2525%2520of%2520conventional%2520pipelines.%2520More%2520remarkably%252C%2520it%2520achieves%250Athis%2520efficiency%2520while%2520simultaneously%2520enhancing%2520performance%252C%2520surpassing%2520models%250Afine-tuned%2520on%2520the%2520full%2520dataset%2520across%2520eight%2520multimodal%2520and%2520three%2520language%250Aunderstanding%2520benchmarks%252C%2520culminating%2520in%2520a%2520101.7%255C%2525%2520relative%2520improvement%2520over%250Athe%2520baseline.%2520The%2520code%2520is%2520available%2520for%2520access%2520via%250A%255Chref%257Bhttps%253A//github.com/bibisbar/PRISM%257D%257Bthis%2520repository%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12119v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PRISM%3A%20Self-Pruning%20Intrinsic%20Selection%20Method%20for%20Training-Free%0A%20%20Multimodal%20Data%20Selection&entry.906535625=Jinhe%20Bi%20and%20Yifan%20Wang%20and%20Danqi%20Yan%20and%20%20Aniri%20and%20Wenke%20Huang%20and%20Zengjie%20Jin%20and%20Xiaowen%20Ma%20and%20Artur%20Hecker%20and%20Mang%20Ye%20and%20Xun%20Xiao%20and%20Hinrich%20Schuetze%20and%20Volker%20Tresp%20and%20Yunpu%20Ma&entry.1292438233=%20%20Visual%20instruction%20tuning%20adapts%20pre-trained%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29%20to%20follow%20human%20instructions%20for%20real-world%20applications.%20However%2C%20the%0Arapid%20growth%20of%20these%20datasets%20introduces%20significant%20redundancy%2C%20leading%20to%0Aincreased%20computational%20costs.%20Existing%20methods%20for%20selecting%20instruction%20data%0Aaim%20to%20prune%20this%20redundancy%2C%20but%20predominantly%20rely%20on%20computationally%0Ademanding%20techniques%20such%20as%20proxy-based%20inference%20or%20training-based%20metrics.%0AConsequently%2C%20the%20substantial%20computational%20costs%20incurred%20by%20these%20selection%0Aprocesses%20often%20exacerbate%20the%20very%20efficiency%20bottlenecks%20they%20are%20intended%20to%0Aresolve%2C%20posing%20a%20significant%20challenge%20to%20the%20scalable%20and%20effective%20tuning%20of%0AMLLMs.%20To%20address%20this%20challenge%2C%20we%20first%20identify%20a%20critical%2C%20yet%20previously%0Aoverlooked%2C%20factor%3A%20the%20anisotropy%20inherent%20in%20visual%20feature%20distributions.%20We%0Afind%20that%20this%20anisotropy%20induces%20a%20%5Ctextit%7BGlobal%20Semantic%20Drift%7D%2C%20and%0Aoverlooking%20this%20phenomenon%20is%20a%20key%20factor%20limiting%20the%20efficiency%20of%20current%0Adata%20selection%20methods.%20Motivated%20by%20this%20insight%2C%20we%20devise%20%5Ctextbf%7BPRISM%7D%2C%0Athe%20first%20training-free%20framework%20for%20efficient%20visual%20instruction%20selection.%0APRISM%20surgically%20removes%20the%20corrupting%20influence%20of%20global%20background%20features%0Aby%20modeling%20the%20intrinsic%20visual%20semantics%20via%20implicit%20re-centering.%0AEmpirically%2C%20PRISM%20reduces%20the%20end-to-end%20time%20for%20data%20selection%20and%20model%0Atuning%20to%20just%2030%5C%25%20of%20conventional%20pipelines.%20More%20remarkably%2C%20it%20achieves%0Athis%20efficiency%20while%20simultaneously%20enhancing%20performance%2C%20surpassing%20models%0Afine-tuned%20on%20the%20full%20dataset%20across%20eight%20multimodal%20and%20three%20language%0Aunderstanding%20benchmarks%2C%20culminating%20in%20a%20101.7%5C%25%20relative%20improvement%20over%0Athe%20baseline.%20The%20code%20is%20available%20for%20access%20via%0A%5Chref%7Bhttps%3A//github.com/bibisbar/PRISM%7D%7Bthis%20repository%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12119v2&entry.124074799=Read"},
{"title": "Seeing and Knowing in the Wild: Open-domain Visual Entity Recognition\n  with Large-scale Knowledge Graphs via Contrastive Learning", "author": "Hongkuan Zhou and Lavdim Halilaj and Sebastian Monka and Stefan Schmid and Yuqicheng Zhu and Jingcheng Wu and Nadeem Nazer and Steffen Staab", "abstract": "  Open-domain visual entity recognition aims to identify and link entities\ndepicted in images to a vast and evolving set of real-world concepts, such as\nthose found in Wikidata. Unlike conventional classification tasks with fixed\nlabel sets, it operates under open-set conditions, where most target entities\nare unseen during training and exhibit long-tail distributions. This makes the\ntask inherently challenging due to limited supervision, high visual ambiguity,\nand the need for semantic disambiguation. In this work, we propose a\nKnowledge-guided Contrastive Learning (KnowCoL) framework that combines both\nimages and text descriptions into a shared semantic space grounded by\nstructured information from Wikidata. By abstracting visual and textual inputs\nto a conceptual level, the model leverages entity descriptions, type\nhierarchies, and relational context to support zero-shot entity recognition. We\nevaluate our approach on the OVEN benchmark, a large-scale open-domain visual\nrecognition dataset with Wikidata IDs as the label space. Our experiments show\nthat using visual, textual, and structured knowledge greatly improves accuracy,\nespecially for rare and unseen entities. Our smallest model improves the\naccuracy on unseen entities by 10.5% compared to the state-of-the-art, despite\nbeing 35 times smaller.\n", "link": "http://arxiv.org/abs/2510.13675v1", "date": "2025-10-15", "relevancy": 2.7534, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5578}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5578}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seeing%20and%20Knowing%20in%20the%20Wild%3A%20Open-domain%20Visual%20Entity%20Recognition%0A%20%20with%20Large-scale%20Knowledge%20Graphs%20via%20Contrastive%20Learning&body=Title%3A%20Seeing%20and%20Knowing%20in%20the%20Wild%3A%20Open-domain%20Visual%20Entity%20Recognition%0A%20%20with%20Large-scale%20Knowledge%20Graphs%20via%20Contrastive%20Learning%0AAuthor%3A%20Hongkuan%20Zhou%20and%20Lavdim%20Halilaj%20and%20Sebastian%20Monka%20and%20Stefan%20Schmid%20and%20Yuqicheng%20Zhu%20and%20Jingcheng%20Wu%20and%20Nadeem%20Nazer%20and%20Steffen%20Staab%0AAbstract%3A%20%20%20Open-domain%20visual%20entity%20recognition%20aims%20to%20identify%20and%20link%20entities%0Adepicted%20in%20images%20to%20a%20vast%20and%20evolving%20set%20of%20real-world%20concepts%2C%20such%20as%0Athose%20found%20in%20Wikidata.%20Unlike%20conventional%20classification%20tasks%20with%20fixed%0Alabel%20sets%2C%20it%20operates%20under%20open-set%20conditions%2C%20where%20most%20target%20entities%0Aare%20unseen%20during%20training%20and%20exhibit%20long-tail%20distributions.%20This%20makes%20the%0Atask%20inherently%20challenging%20due%20to%20limited%20supervision%2C%20high%20visual%20ambiguity%2C%0Aand%20the%20need%20for%20semantic%20disambiguation.%20In%20this%20work%2C%20we%20propose%20a%0AKnowledge-guided%20Contrastive%20Learning%20%28KnowCoL%29%20framework%20that%20combines%20both%0Aimages%20and%20text%20descriptions%20into%20a%20shared%20semantic%20space%20grounded%20by%0Astructured%20information%20from%20Wikidata.%20By%20abstracting%20visual%20and%20textual%20inputs%0Ato%20a%20conceptual%20level%2C%20the%20model%20leverages%20entity%20descriptions%2C%20type%0Ahierarchies%2C%20and%20relational%20context%20to%20support%20zero-shot%20entity%20recognition.%20We%0Aevaluate%20our%20approach%20on%20the%20OVEN%20benchmark%2C%20a%20large-scale%20open-domain%20visual%0Arecognition%20dataset%20with%20Wikidata%20IDs%20as%20the%20label%20space.%20Our%20experiments%20show%0Athat%20using%20visual%2C%20textual%2C%20and%20structured%20knowledge%20greatly%20improves%20accuracy%2C%0Aespecially%20for%20rare%20and%20unseen%20entities.%20Our%20smallest%20model%20improves%20the%0Aaccuracy%20on%20unseen%20entities%20by%2010.5%25%20compared%20to%20the%20state-of-the-art%2C%20despite%0Abeing%2035%20times%20smaller.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13675v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeing%2520and%2520Knowing%2520in%2520the%2520Wild%253A%2520Open-domain%2520Visual%2520Entity%2520Recognition%250A%2520%2520with%2520Large-scale%2520Knowledge%2520Graphs%2520via%2520Contrastive%2520Learning%26entry.906535625%3DHongkuan%2520Zhou%2520and%2520Lavdim%2520Halilaj%2520and%2520Sebastian%2520Monka%2520and%2520Stefan%2520Schmid%2520and%2520Yuqicheng%2520Zhu%2520and%2520Jingcheng%2520Wu%2520and%2520Nadeem%2520Nazer%2520and%2520Steffen%2520Staab%26entry.1292438233%3D%2520%2520Open-domain%2520visual%2520entity%2520recognition%2520aims%2520to%2520identify%2520and%2520link%2520entities%250Adepicted%2520in%2520images%2520to%2520a%2520vast%2520and%2520evolving%2520set%2520of%2520real-world%2520concepts%252C%2520such%2520as%250Athose%2520found%2520in%2520Wikidata.%2520Unlike%2520conventional%2520classification%2520tasks%2520with%2520fixed%250Alabel%2520sets%252C%2520it%2520operates%2520under%2520open-set%2520conditions%252C%2520where%2520most%2520target%2520entities%250Aare%2520unseen%2520during%2520training%2520and%2520exhibit%2520long-tail%2520distributions.%2520This%2520makes%2520the%250Atask%2520inherently%2520challenging%2520due%2520to%2520limited%2520supervision%252C%2520high%2520visual%2520ambiguity%252C%250Aand%2520the%2520need%2520for%2520semantic%2520disambiguation.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%250AKnowledge-guided%2520Contrastive%2520Learning%2520%2528KnowCoL%2529%2520framework%2520that%2520combines%2520both%250Aimages%2520and%2520text%2520descriptions%2520into%2520a%2520shared%2520semantic%2520space%2520grounded%2520by%250Astructured%2520information%2520from%2520Wikidata.%2520By%2520abstracting%2520visual%2520and%2520textual%2520inputs%250Ato%2520a%2520conceptual%2520level%252C%2520the%2520model%2520leverages%2520entity%2520descriptions%252C%2520type%250Ahierarchies%252C%2520and%2520relational%2520context%2520to%2520support%2520zero-shot%2520entity%2520recognition.%2520We%250Aevaluate%2520our%2520approach%2520on%2520the%2520OVEN%2520benchmark%252C%2520a%2520large-scale%2520open-domain%2520visual%250Arecognition%2520dataset%2520with%2520Wikidata%2520IDs%2520as%2520the%2520label%2520space.%2520Our%2520experiments%2520show%250Athat%2520using%2520visual%252C%2520textual%252C%2520and%2520structured%2520knowledge%2520greatly%2520improves%2520accuracy%252C%250Aespecially%2520for%2520rare%2520and%2520unseen%2520entities.%2520Our%2520smallest%2520model%2520improves%2520the%250Aaccuracy%2520on%2520unseen%2520entities%2520by%252010.5%2525%2520compared%2520to%2520the%2520state-of-the-art%252C%2520despite%250Abeing%252035%2520times%2520smaller.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13675v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%20and%20Knowing%20in%20the%20Wild%3A%20Open-domain%20Visual%20Entity%20Recognition%0A%20%20with%20Large-scale%20Knowledge%20Graphs%20via%20Contrastive%20Learning&entry.906535625=Hongkuan%20Zhou%20and%20Lavdim%20Halilaj%20and%20Sebastian%20Monka%20and%20Stefan%20Schmid%20and%20Yuqicheng%20Zhu%20and%20Jingcheng%20Wu%20and%20Nadeem%20Nazer%20and%20Steffen%20Staab&entry.1292438233=%20%20Open-domain%20visual%20entity%20recognition%20aims%20to%20identify%20and%20link%20entities%0Adepicted%20in%20images%20to%20a%20vast%20and%20evolving%20set%20of%20real-world%20concepts%2C%20such%20as%0Athose%20found%20in%20Wikidata.%20Unlike%20conventional%20classification%20tasks%20with%20fixed%0Alabel%20sets%2C%20it%20operates%20under%20open-set%20conditions%2C%20where%20most%20target%20entities%0Aare%20unseen%20during%20training%20and%20exhibit%20long-tail%20distributions.%20This%20makes%20the%0Atask%20inherently%20challenging%20due%20to%20limited%20supervision%2C%20high%20visual%20ambiguity%2C%0Aand%20the%20need%20for%20semantic%20disambiguation.%20In%20this%20work%2C%20we%20propose%20a%0AKnowledge-guided%20Contrastive%20Learning%20%28KnowCoL%29%20framework%20that%20combines%20both%0Aimages%20and%20text%20descriptions%20into%20a%20shared%20semantic%20space%20grounded%20by%0Astructured%20information%20from%20Wikidata.%20By%20abstracting%20visual%20and%20textual%20inputs%0Ato%20a%20conceptual%20level%2C%20the%20model%20leverages%20entity%20descriptions%2C%20type%0Ahierarchies%2C%20and%20relational%20context%20to%20support%20zero-shot%20entity%20recognition.%20We%0Aevaluate%20our%20approach%20on%20the%20OVEN%20benchmark%2C%20a%20large-scale%20open-domain%20visual%0Arecognition%20dataset%20with%20Wikidata%20IDs%20as%20the%20label%20space.%20Our%20experiments%20show%0Athat%20using%20visual%2C%20textual%2C%20and%20structured%20knowledge%20greatly%20improves%20accuracy%2C%0Aespecially%20for%20rare%20and%20unseen%20entities.%20Our%20smallest%20model%20improves%20the%0Aaccuracy%20on%20unseen%20entities%20by%2010.5%25%20compared%20to%20the%20state-of-the-art%2C%20despite%0Abeing%2035%20times%20smaller.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13675v1&entry.124074799=Read"},
{"title": "AVAR-Net: A Lightweight Audio-Visual Anomaly Recognition Framework with\n  a Benchmark Dataset", "author": "Amjid Ali and Zulfiqar Ahmad Khan and Altaf Hussain and Muhammad Munsif and Adnan Hussain and Sung Wook Baik", "abstract": "  Anomaly recognition plays a vital role in surveillance, transportation,\nhealthcare, and public safety. However, most existing approaches rely solely on\nvisual data, making them unreliable under challenging conditions such as\nocclusion, low illumination, and adverse weather. Moreover, the absence of\nlarge-scale synchronized audio-visual datasets has hindered progress in\nmultimodal anomaly recognition. To address these limitations, this study\npresents AVAR-Net, a lightweight and efficient audio-visual anomaly recognition\nframework designed for real-world environments. AVAR-Net consists of four main\nmodules: an audio feature extractor, a video feature extractor, fusion\nstrategy, and a sequential pattern learning network that models cross-modal\nrelationships for anomaly recognition. Specifically, the Wav2Vec2 model\nextracts robust temporal features from raw audio, while MobileViT captures both\nlocal and global visual representations from video frames. An early fusion\nmechanism combines these modalities, and a Multi-Stage Temporal Convolutional\nNetwork (MTCN) model that learns long-range temporal dependencies within the\nfused representation, enabling robust spatiotemporal reasoning. A novel\nVisual-Audio Anomaly Recognition (VAAR) dataset, is also introduced, serving as\na medium-scale benchmark containing 3,000 real-world videos with synchronized\naudio across ten diverse anomaly classes. Experimental evaluations demonstrate\nthat AVAR-Net achieves 89.29% accuracy on VAAR and 88.56% Average Precision on\nthe XD-Violence dataset, improving Average Precision by 2.8% over existing\nstate-of-the-art methods. These results highlight the effectiveness,\nefficiency, and generalization capability of the proposed framework, as well as\nthe utility of VAAR as a benchmark for advancing multimodal anomaly recognition\nresearch.\n", "link": "http://arxiv.org/abs/2510.13630v1", "date": "2025-10-15", "relevancy": 2.7489, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5977}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.527}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AVAR-Net%3A%20A%20Lightweight%20Audio-Visual%20Anomaly%20Recognition%20Framework%20with%0A%20%20a%20Benchmark%20Dataset&body=Title%3A%20AVAR-Net%3A%20A%20Lightweight%20Audio-Visual%20Anomaly%20Recognition%20Framework%20with%0A%20%20a%20Benchmark%20Dataset%0AAuthor%3A%20Amjid%20Ali%20and%20Zulfiqar%20Ahmad%20Khan%20and%20Altaf%20Hussain%20and%20Muhammad%20Munsif%20and%20Adnan%20Hussain%20and%20Sung%20Wook%20Baik%0AAbstract%3A%20%20%20Anomaly%20recognition%20plays%20a%20vital%20role%20in%20surveillance%2C%20transportation%2C%0Ahealthcare%2C%20and%20public%20safety.%20However%2C%20most%20existing%20approaches%20rely%20solely%20on%0Avisual%20data%2C%20making%20them%20unreliable%20under%20challenging%20conditions%20such%20as%0Aocclusion%2C%20low%20illumination%2C%20and%20adverse%20weather.%20Moreover%2C%20the%20absence%20of%0Alarge-scale%20synchronized%20audio-visual%20datasets%20has%20hindered%20progress%20in%0Amultimodal%20anomaly%20recognition.%20To%20address%20these%20limitations%2C%20this%20study%0Apresents%20AVAR-Net%2C%20a%20lightweight%20and%20efficient%20audio-visual%20anomaly%20recognition%0Aframework%20designed%20for%20real-world%20environments.%20AVAR-Net%20consists%20of%20four%20main%0Amodules%3A%20an%20audio%20feature%20extractor%2C%20a%20video%20feature%20extractor%2C%20fusion%0Astrategy%2C%20and%20a%20sequential%20pattern%20learning%20network%20that%20models%20cross-modal%0Arelationships%20for%20anomaly%20recognition.%20Specifically%2C%20the%20Wav2Vec2%20model%0Aextracts%20robust%20temporal%20features%20from%20raw%20audio%2C%20while%20MobileViT%20captures%20both%0Alocal%20and%20global%20visual%20representations%20from%20video%20frames.%20An%20early%20fusion%0Amechanism%20combines%20these%20modalities%2C%20and%20a%20Multi-Stage%20Temporal%20Convolutional%0ANetwork%20%28MTCN%29%20model%20that%20learns%20long-range%20temporal%20dependencies%20within%20the%0Afused%20representation%2C%20enabling%20robust%20spatiotemporal%20reasoning.%20A%20novel%0AVisual-Audio%20Anomaly%20Recognition%20%28VAAR%29%20dataset%2C%20is%20also%20introduced%2C%20serving%20as%0Aa%20medium-scale%20benchmark%20containing%203%2C000%20real-world%20videos%20with%20synchronized%0Aaudio%20across%20ten%20diverse%20anomaly%20classes.%20Experimental%20evaluations%20demonstrate%0Athat%20AVAR-Net%20achieves%2089.29%25%20accuracy%20on%20VAAR%20and%2088.56%25%20Average%20Precision%20on%0Athe%20XD-Violence%20dataset%2C%20improving%20Average%20Precision%20by%202.8%25%20over%20existing%0Astate-of-the-art%20methods.%20These%20results%20highlight%20the%20effectiveness%2C%0Aefficiency%2C%20and%20generalization%20capability%20of%20the%20proposed%20framework%2C%20as%20well%20as%0Athe%20utility%20of%20VAAR%20as%20a%20benchmark%20for%20advancing%20multimodal%20anomaly%20recognition%0Aresearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13630v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAVAR-Net%253A%2520A%2520Lightweight%2520Audio-Visual%2520Anomaly%2520Recognition%2520Framework%2520with%250A%2520%2520a%2520Benchmark%2520Dataset%26entry.906535625%3DAmjid%2520Ali%2520and%2520Zulfiqar%2520Ahmad%2520Khan%2520and%2520Altaf%2520Hussain%2520and%2520Muhammad%2520Munsif%2520and%2520Adnan%2520Hussain%2520and%2520Sung%2520Wook%2520Baik%26entry.1292438233%3D%2520%2520Anomaly%2520recognition%2520plays%2520a%2520vital%2520role%2520in%2520surveillance%252C%2520transportation%252C%250Ahealthcare%252C%2520and%2520public%2520safety.%2520However%252C%2520most%2520existing%2520approaches%2520rely%2520solely%2520on%250Avisual%2520data%252C%2520making%2520them%2520unreliable%2520under%2520challenging%2520conditions%2520such%2520as%250Aocclusion%252C%2520low%2520illumination%252C%2520and%2520adverse%2520weather.%2520Moreover%252C%2520the%2520absence%2520of%250Alarge-scale%2520synchronized%2520audio-visual%2520datasets%2520has%2520hindered%2520progress%2520in%250Amultimodal%2520anomaly%2520recognition.%2520To%2520address%2520these%2520limitations%252C%2520this%2520study%250Apresents%2520AVAR-Net%252C%2520a%2520lightweight%2520and%2520efficient%2520audio-visual%2520anomaly%2520recognition%250Aframework%2520designed%2520for%2520real-world%2520environments.%2520AVAR-Net%2520consists%2520of%2520four%2520main%250Amodules%253A%2520an%2520audio%2520feature%2520extractor%252C%2520a%2520video%2520feature%2520extractor%252C%2520fusion%250Astrategy%252C%2520and%2520a%2520sequential%2520pattern%2520learning%2520network%2520that%2520models%2520cross-modal%250Arelationships%2520for%2520anomaly%2520recognition.%2520Specifically%252C%2520the%2520Wav2Vec2%2520model%250Aextracts%2520robust%2520temporal%2520features%2520from%2520raw%2520audio%252C%2520while%2520MobileViT%2520captures%2520both%250Alocal%2520and%2520global%2520visual%2520representations%2520from%2520video%2520frames.%2520An%2520early%2520fusion%250Amechanism%2520combines%2520these%2520modalities%252C%2520and%2520a%2520Multi-Stage%2520Temporal%2520Convolutional%250ANetwork%2520%2528MTCN%2529%2520model%2520that%2520learns%2520long-range%2520temporal%2520dependencies%2520within%2520the%250Afused%2520representation%252C%2520enabling%2520robust%2520spatiotemporal%2520reasoning.%2520A%2520novel%250AVisual-Audio%2520Anomaly%2520Recognition%2520%2528VAAR%2529%2520dataset%252C%2520is%2520also%2520introduced%252C%2520serving%2520as%250Aa%2520medium-scale%2520benchmark%2520containing%25203%252C000%2520real-world%2520videos%2520with%2520synchronized%250Aaudio%2520across%2520ten%2520diverse%2520anomaly%2520classes.%2520Experimental%2520evaluations%2520demonstrate%250Athat%2520AVAR-Net%2520achieves%252089.29%2525%2520accuracy%2520on%2520VAAR%2520and%252088.56%2525%2520Average%2520Precision%2520on%250Athe%2520XD-Violence%2520dataset%252C%2520improving%2520Average%2520Precision%2520by%25202.8%2525%2520over%2520existing%250Astate-of-the-art%2520methods.%2520These%2520results%2520highlight%2520the%2520effectiveness%252C%250Aefficiency%252C%2520and%2520generalization%2520capability%2520of%2520the%2520proposed%2520framework%252C%2520as%2520well%2520as%250Athe%2520utility%2520of%2520VAAR%2520as%2520a%2520benchmark%2520for%2520advancing%2520multimodal%2520anomaly%2520recognition%250Aresearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13630v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AVAR-Net%3A%20A%20Lightweight%20Audio-Visual%20Anomaly%20Recognition%20Framework%20with%0A%20%20a%20Benchmark%20Dataset&entry.906535625=Amjid%20Ali%20and%20Zulfiqar%20Ahmad%20Khan%20and%20Altaf%20Hussain%20and%20Muhammad%20Munsif%20and%20Adnan%20Hussain%20and%20Sung%20Wook%20Baik&entry.1292438233=%20%20Anomaly%20recognition%20plays%20a%20vital%20role%20in%20surveillance%2C%20transportation%2C%0Ahealthcare%2C%20and%20public%20safety.%20However%2C%20most%20existing%20approaches%20rely%20solely%20on%0Avisual%20data%2C%20making%20them%20unreliable%20under%20challenging%20conditions%20such%20as%0Aocclusion%2C%20low%20illumination%2C%20and%20adverse%20weather.%20Moreover%2C%20the%20absence%20of%0Alarge-scale%20synchronized%20audio-visual%20datasets%20has%20hindered%20progress%20in%0Amultimodal%20anomaly%20recognition.%20To%20address%20these%20limitations%2C%20this%20study%0Apresents%20AVAR-Net%2C%20a%20lightweight%20and%20efficient%20audio-visual%20anomaly%20recognition%0Aframework%20designed%20for%20real-world%20environments.%20AVAR-Net%20consists%20of%20four%20main%0Amodules%3A%20an%20audio%20feature%20extractor%2C%20a%20video%20feature%20extractor%2C%20fusion%0Astrategy%2C%20and%20a%20sequential%20pattern%20learning%20network%20that%20models%20cross-modal%0Arelationships%20for%20anomaly%20recognition.%20Specifically%2C%20the%20Wav2Vec2%20model%0Aextracts%20robust%20temporal%20features%20from%20raw%20audio%2C%20while%20MobileViT%20captures%20both%0Alocal%20and%20global%20visual%20representations%20from%20video%20frames.%20An%20early%20fusion%0Amechanism%20combines%20these%20modalities%2C%20and%20a%20Multi-Stage%20Temporal%20Convolutional%0ANetwork%20%28MTCN%29%20model%20that%20learns%20long-range%20temporal%20dependencies%20within%20the%0Afused%20representation%2C%20enabling%20robust%20spatiotemporal%20reasoning.%20A%20novel%0AVisual-Audio%20Anomaly%20Recognition%20%28VAAR%29%20dataset%2C%20is%20also%20introduced%2C%20serving%20as%0Aa%20medium-scale%20benchmark%20containing%203%2C000%20real-world%20videos%20with%20synchronized%0Aaudio%20across%20ten%20diverse%20anomaly%20classes.%20Experimental%20evaluations%20demonstrate%0Athat%20AVAR-Net%20achieves%2089.29%25%20accuracy%20on%20VAAR%20and%2088.56%25%20Average%20Precision%20on%0Athe%20XD-Violence%20dataset%2C%20improving%20Average%20Precision%20by%202.8%25%20over%20existing%0Astate-of-the-art%20methods.%20These%20results%20highlight%20the%20effectiveness%2C%0Aefficiency%2C%20and%20generalization%20capability%20of%20the%20proposed%20framework%2C%20as%20well%20as%0Athe%20utility%20of%20VAAR%20as%20a%20benchmark%20for%20advancing%20multimodal%20anomaly%20recognition%0Aresearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13630v1&entry.124074799=Read"},
{"title": "MEGC2025: Micro-Expression Grand Challenge on Spot Then Recognize and\n  Visual Question Answering", "author": "Xinqi Fan and Jingting Li and John See and Moi Hoon Yap and Wen-Huang Cheng and Xiaobai Li and Xiaopeng Hong and Su-Jing Wang and Adrian K. Davision", "abstract": "  Facial micro-expressions (MEs) are involuntary movements of the face that\noccur spontaneously when a person experiences an emotion but attempts to\nsuppress or repress the facial expression, typically found in a high-stakes\nenvironment. In recent years, substantial advancements have been made in the\nareas of ME recognition, spotting, and generation. However, conventional\napproaches that treat spotting and recognition as separate tasks are\nsuboptimal, particularly for analyzing long-duration videos in realistic\nsettings. Concurrently, the emergence of multimodal large language models\n(MLLMs) and large vision-language models (LVLMs) offers promising new avenues\nfor enhancing ME analysis through their powerful multimodal reasoning\ncapabilities. The ME grand challenge (MEGC) 2025 introduces two tasks that\nreflect these evolving research directions: (1) ME spot-then-recognize\n(ME-STR), which integrates ME spotting and subsequent recognition in a unified\nsequential pipeline; and (2) ME visual question answering (ME-VQA), which\nexplores ME understanding through visual question answering, leveraging MLLMs\nor LVLMs to address diverse question types related to MEs. All participating\nalgorithms are required to run on this test set and submit their results on a\nleaderboard. More details are available at https://megc2025.github.io.\n", "link": "http://arxiv.org/abs/2506.15298v2", "date": "2025-10-15", "relevancy": 2.7179, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5442}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5442}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MEGC2025%3A%20Micro-Expression%20Grand%20Challenge%20on%20Spot%20Then%20Recognize%20and%0A%20%20Visual%20Question%20Answering&body=Title%3A%20MEGC2025%3A%20Micro-Expression%20Grand%20Challenge%20on%20Spot%20Then%20Recognize%20and%0A%20%20Visual%20Question%20Answering%0AAuthor%3A%20Xinqi%20Fan%20and%20Jingting%20Li%20and%20John%20See%20and%20Moi%20Hoon%20Yap%20and%20Wen-Huang%20Cheng%20and%20Xiaobai%20Li%20and%20Xiaopeng%20Hong%20and%20Su-Jing%20Wang%20and%20Adrian%20K.%20Davision%0AAbstract%3A%20%20%20Facial%20micro-expressions%20%28MEs%29%20are%20involuntary%20movements%20of%20the%20face%20that%0Aoccur%20spontaneously%20when%20a%20person%20experiences%20an%20emotion%20but%20attempts%20to%0Asuppress%20or%20repress%20the%20facial%20expression%2C%20typically%20found%20in%20a%20high-stakes%0Aenvironment.%20In%20recent%20years%2C%20substantial%20advancements%20have%20been%20made%20in%20the%0Aareas%20of%20ME%20recognition%2C%20spotting%2C%20and%20generation.%20However%2C%20conventional%0Aapproaches%20that%20treat%20spotting%20and%20recognition%20as%20separate%20tasks%20are%0Asuboptimal%2C%20particularly%20for%20analyzing%20long-duration%20videos%20in%20realistic%0Asettings.%20Concurrently%2C%20the%20emergence%20of%20multimodal%20large%20language%20models%0A%28MLLMs%29%20and%20large%20vision-language%20models%20%28LVLMs%29%20offers%20promising%20new%20avenues%0Afor%20enhancing%20ME%20analysis%20through%20their%20powerful%20multimodal%20reasoning%0Acapabilities.%20The%20ME%20grand%20challenge%20%28MEGC%29%202025%20introduces%20two%20tasks%20that%0Areflect%20these%20evolving%20research%20directions%3A%20%281%29%20ME%20spot-then-recognize%0A%28ME-STR%29%2C%20which%20integrates%20ME%20spotting%20and%20subsequent%20recognition%20in%20a%20unified%0Asequential%20pipeline%3B%20and%20%282%29%20ME%20visual%20question%20answering%20%28ME-VQA%29%2C%20which%0Aexplores%20ME%20understanding%20through%20visual%20question%20answering%2C%20leveraging%20MLLMs%0Aor%20LVLMs%20to%20address%20diverse%20question%20types%20related%20to%20MEs.%20All%20participating%0Aalgorithms%20are%20required%20to%20run%20on%20this%20test%20set%20and%20submit%20their%20results%20on%20a%0Aleaderboard.%20More%20details%20are%20available%20at%20https%3A//megc2025.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15298v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMEGC2025%253A%2520Micro-Expression%2520Grand%2520Challenge%2520on%2520Spot%2520Then%2520Recognize%2520and%250A%2520%2520Visual%2520Question%2520Answering%26entry.906535625%3DXinqi%2520Fan%2520and%2520Jingting%2520Li%2520and%2520John%2520See%2520and%2520Moi%2520Hoon%2520Yap%2520and%2520Wen-Huang%2520Cheng%2520and%2520Xiaobai%2520Li%2520and%2520Xiaopeng%2520Hong%2520and%2520Su-Jing%2520Wang%2520and%2520Adrian%2520K.%2520Davision%26entry.1292438233%3D%2520%2520Facial%2520micro-expressions%2520%2528MEs%2529%2520are%2520involuntary%2520movements%2520of%2520the%2520face%2520that%250Aoccur%2520spontaneously%2520when%2520a%2520person%2520experiences%2520an%2520emotion%2520but%2520attempts%2520to%250Asuppress%2520or%2520repress%2520the%2520facial%2520expression%252C%2520typically%2520found%2520in%2520a%2520high-stakes%250Aenvironment.%2520In%2520recent%2520years%252C%2520substantial%2520advancements%2520have%2520been%2520made%2520in%2520the%250Aareas%2520of%2520ME%2520recognition%252C%2520spotting%252C%2520and%2520generation.%2520However%252C%2520conventional%250Aapproaches%2520that%2520treat%2520spotting%2520and%2520recognition%2520as%2520separate%2520tasks%2520are%250Asuboptimal%252C%2520particularly%2520for%2520analyzing%2520long-duration%2520videos%2520in%2520realistic%250Asettings.%2520Concurrently%252C%2520the%2520emergence%2520of%2520multimodal%2520large%2520language%2520models%250A%2528MLLMs%2529%2520and%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%2520offers%2520promising%2520new%2520avenues%250Afor%2520enhancing%2520ME%2520analysis%2520through%2520their%2520powerful%2520multimodal%2520reasoning%250Acapabilities.%2520The%2520ME%2520grand%2520challenge%2520%2528MEGC%2529%25202025%2520introduces%2520two%2520tasks%2520that%250Areflect%2520these%2520evolving%2520research%2520directions%253A%2520%25281%2529%2520ME%2520spot-then-recognize%250A%2528ME-STR%2529%252C%2520which%2520integrates%2520ME%2520spotting%2520and%2520subsequent%2520recognition%2520in%2520a%2520unified%250Asequential%2520pipeline%253B%2520and%2520%25282%2529%2520ME%2520visual%2520question%2520answering%2520%2528ME-VQA%2529%252C%2520which%250Aexplores%2520ME%2520understanding%2520through%2520visual%2520question%2520answering%252C%2520leveraging%2520MLLMs%250Aor%2520LVLMs%2520to%2520address%2520diverse%2520question%2520types%2520related%2520to%2520MEs.%2520All%2520participating%250Aalgorithms%2520are%2520required%2520to%2520run%2520on%2520this%2520test%2520set%2520and%2520submit%2520their%2520results%2520on%2520a%250Aleaderboard.%2520More%2520details%2520are%2520available%2520at%2520https%253A//megc2025.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15298v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MEGC2025%3A%20Micro-Expression%20Grand%20Challenge%20on%20Spot%20Then%20Recognize%20and%0A%20%20Visual%20Question%20Answering&entry.906535625=Xinqi%20Fan%20and%20Jingting%20Li%20and%20John%20See%20and%20Moi%20Hoon%20Yap%20and%20Wen-Huang%20Cheng%20and%20Xiaobai%20Li%20and%20Xiaopeng%20Hong%20and%20Su-Jing%20Wang%20and%20Adrian%20K.%20Davision&entry.1292438233=%20%20Facial%20micro-expressions%20%28MEs%29%20are%20involuntary%20movements%20of%20the%20face%20that%0Aoccur%20spontaneously%20when%20a%20person%20experiences%20an%20emotion%20but%20attempts%20to%0Asuppress%20or%20repress%20the%20facial%20expression%2C%20typically%20found%20in%20a%20high-stakes%0Aenvironment.%20In%20recent%20years%2C%20substantial%20advancements%20have%20been%20made%20in%20the%0Aareas%20of%20ME%20recognition%2C%20spotting%2C%20and%20generation.%20However%2C%20conventional%0Aapproaches%20that%20treat%20spotting%20and%20recognition%20as%20separate%20tasks%20are%0Asuboptimal%2C%20particularly%20for%20analyzing%20long-duration%20videos%20in%20realistic%0Asettings.%20Concurrently%2C%20the%20emergence%20of%20multimodal%20large%20language%20models%0A%28MLLMs%29%20and%20large%20vision-language%20models%20%28LVLMs%29%20offers%20promising%20new%20avenues%0Afor%20enhancing%20ME%20analysis%20through%20their%20powerful%20multimodal%20reasoning%0Acapabilities.%20The%20ME%20grand%20challenge%20%28MEGC%29%202025%20introduces%20two%20tasks%20that%0Areflect%20these%20evolving%20research%20directions%3A%20%281%29%20ME%20spot-then-recognize%0A%28ME-STR%29%2C%20which%20integrates%20ME%20spotting%20and%20subsequent%20recognition%20in%20a%20unified%0Asequential%20pipeline%3B%20and%20%282%29%20ME%20visual%20question%20answering%20%28ME-VQA%29%2C%20which%0Aexplores%20ME%20understanding%20through%20visual%20question%20answering%2C%20leveraging%20MLLMs%0Aor%20LVLMs%20to%20address%20diverse%20question%20types%20related%20to%20MEs.%20All%20participating%0Aalgorithms%20are%20required%20to%20run%20on%20this%20test%20set%20and%20submit%20their%20results%20on%20a%0Aleaderboard.%20More%20details%20are%20available%20at%20https%3A//megc2025.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15298v2&entry.124074799=Read"},
{"title": "FlashWorld: High-quality 3D Scene Generation within Seconds", "author": "Xinyang Li and Tengfei Wang and Zixiao Gu and Shengchuan Zhang and Chunchao Guo and Liujuan Cao", "abstract": "  We propose FlashWorld, a generative model that produces 3D scenes from a\nsingle image or text prompt in seconds, 10~100$\\times$ faster than previous\nworks while possessing superior rendering quality. Our approach shifts from the\nconventional multi-view-oriented (MV-oriented) paradigm, which generates\nmulti-view images for subsequent 3D reconstruction, to a 3D-oriented approach\nwhere the model directly produces 3D Gaussian representations during multi-view\ngeneration. While ensuring 3D consistency, 3D-oriented method typically suffers\npoor visual quality. FlashWorld includes a dual-mode pre-training phase\nfollowed by a cross-mode post-training phase, effectively integrating the\nstrengths of both paradigms. Specifically, leveraging the prior from a video\ndiffusion model, we first pre-train a dual-mode multi-view diffusion model,\nwhich jointly supports MV-oriented and 3D-oriented generation modes. To bridge\nthe quality gap in 3D-oriented generation, we further propose a cross-mode\npost-training distillation by matching distribution from consistent 3D-oriented\nmode to high-quality MV-oriented mode. This not only enhances visual quality\nwhile maintaining 3D consistency, but also reduces the required denoising steps\nfor inference. Also, we propose a strategy to leverage massive single-view\nimages and text prompts during this process to enhance the model's\ngeneralization to out-of-distribution inputs. Extensive experiments demonstrate\nthe superiority and efficiency of our method.\n", "link": "http://arxiv.org/abs/2510.13678v1", "date": "2025-10-15", "relevancy": 2.7167, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.7072}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6736}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6736}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlashWorld%3A%20High-quality%203D%20Scene%20Generation%20within%20Seconds&body=Title%3A%20FlashWorld%3A%20High-quality%203D%20Scene%20Generation%20within%20Seconds%0AAuthor%3A%20Xinyang%20Li%20and%20Tengfei%20Wang%20and%20Zixiao%20Gu%20and%20Shengchuan%20Zhang%20and%20Chunchao%20Guo%20and%20Liujuan%20Cao%0AAbstract%3A%20%20%20We%20propose%20FlashWorld%2C%20a%20generative%20model%20that%20produces%203D%20scenes%20from%20a%0Asingle%20image%20or%20text%20prompt%20in%20seconds%2C%2010~100%24%5Ctimes%24%20faster%20than%20previous%0Aworks%20while%20possessing%20superior%20rendering%20quality.%20Our%20approach%20shifts%20from%20the%0Aconventional%20multi-view-oriented%20%28MV-oriented%29%20paradigm%2C%20which%20generates%0Amulti-view%20images%20for%20subsequent%203D%20reconstruction%2C%20to%20a%203D-oriented%20approach%0Awhere%20the%20model%20directly%20produces%203D%20Gaussian%20representations%20during%20multi-view%0Ageneration.%20While%20ensuring%203D%20consistency%2C%203D-oriented%20method%20typically%20suffers%0Apoor%20visual%20quality.%20FlashWorld%20includes%20a%20dual-mode%20pre-training%20phase%0Afollowed%20by%20a%20cross-mode%20post-training%20phase%2C%20effectively%20integrating%20the%0Astrengths%20of%20both%20paradigms.%20Specifically%2C%20leveraging%20the%20prior%20from%20a%20video%0Adiffusion%20model%2C%20we%20first%20pre-train%20a%20dual-mode%20multi-view%20diffusion%20model%2C%0Awhich%20jointly%20supports%20MV-oriented%20and%203D-oriented%20generation%20modes.%20To%20bridge%0Athe%20quality%20gap%20in%203D-oriented%20generation%2C%20we%20further%20propose%20a%20cross-mode%0Apost-training%20distillation%20by%20matching%20distribution%20from%20consistent%203D-oriented%0Amode%20to%20high-quality%20MV-oriented%20mode.%20This%20not%20only%20enhances%20visual%20quality%0Awhile%20maintaining%203D%20consistency%2C%20but%20also%20reduces%20the%20required%20denoising%20steps%0Afor%20inference.%20Also%2C%20we%20propose%20a%20strategy%20to%20leverage%20massive%20single-view%0Aimages%20and%20text%20prompts%20during%20this%20process%20to%20enhance%20the%20model%27s%0Ageneralization%20to%20out-of-distribution%20inputs.%20Extensive%20experiments%20demonstrate%0Athe%20superiority%20and%20efficiency%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13678v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlashWorld%253A%2520High-quality%25203D%2520Scene%2520Generation%2520within%2520Seconds%26entry.906535625%3DXinyang%2520Li%2520and%2520Tengfei%2520Wang%2520and%2520Zixiao%2520Gu%2520and%2520Shengchuan%2520Zhang%2520and%2520Chunchao%2520Guo%2520and%2520Liujuan%2520Cao%26entry.1292438233%3D%2520%2520We%2520propose%2520FlashWorld%252C%2520a%2520generative%2520model%2520that%2520produces%25203D%2520scenes%2520from%2520a%250Asingle%2520image%2520or%2520text%2520prompt%2520in%2520seconds%252C%252010~100%2524%255Ctimes%2524%2520faster%2520than%2520previous%250Aworks%2520while%2520possessing%2520superior%2520rendering%2520quality.%2520Our%2520approach%2520shifts%2520from%2520the%250Aconventional%2520multi-view-oriented%2520%2528MV-oriented%2529%2520paradigm%252C%2520which%2520generates%250Amulti-view%2520images%2520for%2520subsequent%25203D%2520reconstruction%252C%2520to%2520a%25203D-oriented%2520approach%250Awhere%2520the%2520model%2520directly%2520produces%25203D%2520Gaussian%2520representations%2520during%2520multi-view%250Ageneration.%2520While%2520ensuring%25203D%2520consistency%252C%25203D-oriented%2520method%2520typically%2520suffers%250Apoor%2520visual%2520quality.%2520FlashWorld%2520includes%2520a%2520dual-mode%2520pre-training%2520phase%250Afollowed%2520by%2520a%2520cross-mode%2520post-training%2520phase%252C%2520effectively%2520integrating%2520the%250Astrengths%2520of%2520both%2520paradigms.%2520Specifically%252C%2520leveraging%2520the%2520prior%2520from%2520a%2520video%250Adiffusion%2520model%252C%2520we%2520first%2520pre-train%2520a%2520dual-mode%2520multi-view%2520diffusion%2520model%252C%250Awhich%2520jointly%2520supports%2520MV-oriented%2520and%25203D-oriented%2520generation%2520modes.%2520To%2520bridge%250Athe%2520quality%2520gap%2520in%25203D-oriented%2520generation%252C%2520we%2520further%2520propose%2520a%2520cross-mode%250Apost-training%2520distillation%2520by%2520matching%2520distribution%2520from%2520consistent%25203D-oriented%250Amode%2520to%2520high-quality%2520MV-oriented%2520mode.%2520This%2520not%2520only%2520enhances%2520visual%2520quality%250Awhile%2520maintaining%25203D%2520consistency%252C%2520but%2520also%2520reduces%2520the%2520required%2520denoising%2520steps%250Afor%2520inference.%2520Also%252C%2520we%2520propose%2520a%2520strategy%2520to%2520leverage%2520massive%2520single-view%250Aimages%2520and%2520text%2520prompts%2520during%2520this%2520process%2520to%2520enhance%2520the%2520model%2527s%250Ageneralization%2520to%2520out-of-distribution%2520inputs.%2520Extensive%2520experiments%2520demonstrate%250Athe%2520superiority%2520and%2520efficiency%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13678v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlashWorld%3A%20High-quality%203D%20Scene%20Generation%20within%20Seconds&entry.906535625=Xinyang%20Li%20and%20Tengfei%20Wang%20and%20Zixiao%20Gu%20and%20Shengchuan%20Zhang%20and%20Chunchao%20Guo%20and%20Liujuan%20Cao&entry.1292438233=%20%20We%20propose%20FlashWorld%2C%20a%20generative%20model%20that%20produces%203D%20scenes%20from%20a%0Asingle%20image%20or%20text%20prompt%20in%20seconds%2C%2010~100%24%5Ctimes%24%20faster%20than%20previous%0Aworks%20while%20possessing%20superior%20rendering%20quality.%20Our%20approach%20shifts%20from%20the%0Aconventional%20multi-view-oriented%20%28MV-oriented%29%20paradigm%2C%20which%20generates%0Amulti-view%20images%20for%20subsequent%203D%20reconstruction%2C%20to%20a%203D-oriented%20approach%0Awhere%20the%20model%20directly%20produces%203D%20Gaussian%20representations%20during%20multi-view%0Ageneration.%20While%20ensuring%203D%20consistency%2C%203D-oriented%20method%20typically%20suffers%0Apoor%20visual%20quality.%20FlashWorld%20includes%20a%20dual-mode%20pre-training%20phase%0Afollowed%20by%20a%20cross-mode%20post-training%20phase%2C%20effectively%20integrating%20the%0Astrengths%20of%20both%20paradigms.%20Specifically%2C%20leveraging%20the%20prior%20from%20a%20video%0Adiffusion%20model%2C%20we%20first%20pre-train%20a%20dual-mode%20multi-view%20diffusion%20model%2C%0Awhich%20jointly%20supports%20MV-oriented%20and%203D-oriented%20generation%20modes.%20To%20bridge%0Athe%20quality%20gap%20in%203D-oriented%20generation%2C%20we%20further%20propose%20a%20cross-mode%0Apost-training%20distillation%20by%20matching%20distribution%20from%20consistent%203D-oriented%0Amode%20to%20high-quality%20MV-oriented%20mode.%20This%20not%20only%20enhances%20visual%20quality%0Awhile%20maintaining%203D%20consistency%2C%20but%20also%20reduces%20the%20required%20denoising%20steps%0Afor%20inference.%20Also%2C%20we%20propose%20a%20strategy%20to%20leverage%20massive%20single-view%0Aimages%20and%20text%20prompts%20during%20this%20process%20to%20enhance%20the%20model%27s%0Ageneralization%20to%20out-of-distribution%20inputs.%20Extensive%20experiments%20demonstrate%0Athe%20superiority%20and%20efficiency%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13678v1&entry.124074799=Read"},
{"title": "Closing the Gap Between Text and Speech Understanding in LLMs", "author": "Santiago Cuervo and Skyler Seto and Maureen de Seyssel and Richard He Bai and Zijin Gu and Tatiana Likhomanenko and Navdeep Jaitly and Zakaria Aldeneh", "abstract": "  Large Language Models (LLMs) can be adapted to extend their text capabilities\nto speech inputs. However, these speech-adapted LLMs consistently underperform\ntheir text-based counterparts--and even cascaded pipelines--on language\nunderstanding tasks. We term this shortfall the text-speech understanding gap:\nthe performance drop observed when a speech-adapted LLM processes spoken inputs\nrelative to when the original text-based LLM processes the equivalent text.\nRecent approaches to narrowing this gap either rely on large-scale speech\nsynthesis of text corpora, which is costly and heavily dependent on synthetic\ndata, or on large-scale proprietary speech datasets, which are not\nreproducible. As a result, there remains a need for more data-efficient\nalternatives for closing the text-speech understanding gap. In this work, we\nanalyze the gap as driven by two factors: (i) forgetting of text capabilities\nduring adaptation, and (ii) cross-modal misalignment between speech and text.\nBased on this analysis, we introduce SALAD--Sample-efficient Alignment with\nLearning through Active selection and cross-modal Distillation--which combines\ncross-modal distillation with targeted synthetic data to improve alignment\nwhile mitigating forgetting. Applied to 3B and 7B LLMs, SALAD achieves\ncompetitive performance with a strong open-weight model across broad-domain\nbenchmarks in knowledge, language understanding, and reasoning, while training\non over an order of magnitude less speech data from public corpora.\n", "link": "http://arxiv.org/abs/2510.13632v1", "date": "2025-10-15", "relevancy": 2.7156, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5707}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5293}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5293}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Closing%20the%20Gap%20Between%20Text%20and%20Speech%20Understanding%20in%20LLMs&body=Title%3A%20Closing%20the%20Gap%20Between%20Text%20and%20Speech%20Understanding%20in%20LLMs%0AAuthor%3A%20Santiago%20Cuervo%20and%20Skyler%20Seto%20and%20Maureen%20de%20Seyssel%20and%20Richard%20He%20Bai%20and%20Zijin%20Gu%20and%20Tatiana%20Likhomanenko%20and%20Navdeep%20Jaitly%20and%20Zakaria%20Aldeneh%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20can%20be%20adapted%20to%20extend%20their%20text%20capabilities%0Ato%20speech%20inputs.%20However%2C%20these%20speech-adapted%20LLMs%20consistently%20underperform%0Atheir%20text-based%20counterparts--and%20even%20cascaded%20pipelines--on%20language%0Aunderstanding%20tasks.%20We%20term%20this%20shortfall%20the%20text-speech%20understanding%20gap%3A%0Athe%20performance%20drop%20observed%20when%20a%20speech-adapted%20LLM%20processes%20spoken%20inputs%0Arelative%20to%20when%20the%20original%20text-based%20LLM%20processes%20the%20equivalent%20text.%0ARecent%20approaches%20to%20narrowing%20this%20gap%20either%20rely%20on%20large-scale%20speech%0Asynthesis%20of%20text%20corpora%2C%20which%20is%20costly%20and%20heavily%20dependent%20on%20synthetic%0Adata%2C%20or%20on%20large-scale%20proprietary%20speech%20datasets%2C%20which%20are%20not%0Areproducible.%20As%20a%20result%2C%20there%20remains%20a%20need%20for%20more%20data-efficient%0Aalternatives%20for%20closing%20the%20text-speech%20understanding%20gap.%20In%20this%20work%2C%20we%0Aanalyze%20the%20gap%20as%20driven%20by%20two%20factors%3A%20%28i%29%20forgetting%20of%20text%20capabilities%0Aduring%20adaptation%2C%20and%20%28ii%29%20cross-modal%20misalignment%20between%20speech%20and%20text.%0ABased%20on%20this%20analysis%2C%20we%20introduce%20SALAD--Sample-efficient%20Alignment%20with%0ALearning%20through%20Active%20selection%20and%20cross-modal%20Distillation--which%20combines%0Across-modal%20distillation%20with%20targeted%20synthetic%20data%20to%20improve%20alignment%0Awhile%20mitigating%20forgetting.%20Applied%20to%203B%20and%207B%20LLMs%2C%20SALAD%20achieves%0Acompetitive%20performance%20with%20a%20strong%20open-weight%20model%20across%20broad-domain%0Abenchmarks%20in%20knowledge%2C%20language%20understanding%2C%20and%20reasoning%2C%20while%20training%0Aon%20over%20an%20order%20of%20magnitude%20less%20speech%20data%20from%20public%20corpora.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13632v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClosing%2520the%2520Gap%2520Between%2520Text%2520and%2520Speech%2520Understanding%2520in%2520LLMs%26entry.906535625%3DSantiago%2520Cuervo%2520and%2520Skyler%2520Seto%2520and%2520Maureen%2520de%2520Seyssel%2520and%2520Richard%2520He%2520Bai%2520and%2520Zijin%2520Gu%2520and%2520Tatiana%2520Likhomanenko%2520and%2520Navdeep%2520Jaitly%2520and%2520Zakaria%2520Aldeneh%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520can%2520be%2520adapted%2520to%2520extend%2520their%2520text%2520capabilities%250Ato%2520speech%2520inputs.%2520However%252C%2520these%2520speech-adapted%2520LLMs%2520consistently%2520underperform%250Atheir%2520text-based%2520counterparts--and%2520even%2520cascaded%2520pipelines--on%2520language%250Aunderstanding%2520tasks.%2520We%2520term%2520this%2520shortfall%2520the%2520text-speech%2520understanding%2520gap%253A%250Athe%2520performance%2520drop%2520observed%2520when%2520a%2520speech-adapted%2520LLM%2520processes%2520spoken%2520inputs%250Arelative%2520to%2520when%2520the%2520original%2520text-based%2520LLM%2520processes%2520the%2520equivalent%2520text.%250ARecent%2520approaches%2520to%2520narrowing%2520this%2520gap%2520either%2520rely%2520on%2520large-scale%2520speech%250Asynthesis%2520of%2520text%2520corpora%252C%2520which%2520is%2520costly%2520and%2520heavily%2520dependent%2520on%2520synthetic%250Adata%252C%2520or%2520on%2520large-scale%2520proprietary%2520speech%2520datasets%252C%2520which%2520are%2520not%250Areproducible.%2520As%2520a%2520result%252C%2520there%2520remains%2520a%2520need%2520for%2520more%2520data-efficient%250Aalternatives%2520for%2520closing%2520the%2520text-speech%2520understanding%2520gap.%2520In%2520this%2520work%252C%2520we%250Aanalyze%2520the%2520gap%2520as%2520driven%2520by%2520two%2520factors%253A%2520%2528i%2529%2520forgetting%2520of%2520text%2520capabilities%250Aduring%2520adaptation%252C%2520and%2520%2528ii%2529%2520cross-modal%2520misalignment%2520between%2520speech%2520and%2520text.%250ABased%2520on%2520this%2520analysis%252C%2520we%2520introduce%2520SALAD--Sample-efficient%2520Alignment%2520with%250ALearning%2520through%2520Active%2520selection%2520and%2520cross-modal%2520Distillation--which%2520combines%250Across-modal%2520distillation%2520with%2520targeted%2520synthetic%2520data%2520to%2520improve%2520alignment%250Awhile%2520mitigating%2520forgetting.%2520Applied%2520to%25203B%2520and%25207B%2520LLMs%252C%2520SALAD%2520achieves%250Acompetitive%2520performance%2520with%2520a%2520strong%2520open-weight%2520model%2520across%2520broad-domain%250Abenchmarks%2520in%2520knowledge%252C%2520language%2520understanding%252C%2520and%2520reasoning%252C%2520while%2520training%250Aon%2520over%2520an%2520order%2520of%2520magnitude%2520less%2520speech%2520data%2520from%2520public%2520corpora.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13632v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Closing%20the%20Gap%20Between%20Text%20and%20Speech%20Understanding%20in%20LLMs&entry.906535625=Santiago%20Cuervo%20and%20Skyler%20Seto%20and%20Maureen%20de%20Seyssel%20and%20Richard%20He%20Bai%20and%20Zijin%20Gu%20and%20Tatiana%20Likhomanenko%20and%20Navdeep%20Jaitly%20and%20Zakaria%20Aldeneh&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20can%20be%20adapted%20to%20extend%20their%20text%20capabilities%0Ato%20speech%20inputs.%20However%2C%20these%20speech-adapted%20LLMs%20consistently%20underperform%0Atheir%20text-based%20counterparts--and%20even%20cascaded%20pipelines--on%20language%0Aunderstanding%20tasks.%20We%20term%20this%20shortfall%20the%20text-speech%20understanding%20gap%3A%0Athe%20performance%20drop%20observed%20when%20a%20speech-adapted%20LLM%20processes%20spoken%20inputs%0Arelative%20to%20when%20the%20original%20text-based%20LLM%20processes%20the%20equivalent%20text.%0ARecent%20approaches%20to%20narrowing%20this%20gap%20either%20rely%20on%20large-scale%20speech%0Asynthesis%20of%20text%20corpora%2C%20which%20is%20costly%20and%20heavily%20dependent%20on%20synthetic%0Adata%2C%20or%20on%20large-scale%20proprietary%20speech%20datasets%2C%20which%20are%20not%0Areproducible.%20As%20a%20result%2C%20there%20remains%20a%20need%20for%20more%20data-efficient%0Aalternatives%20for%20closing%20the%20text-speech%20understanding%20gap.%20In%20this%20work%2C%20we%0Aanalyze%20the%20gap%20as%20driven%20by%20two%20factors%3A%20%28i%29%20forgetting%20of%20text%20capabilities%0Aduring%20adaptation%2C%20and%20%28ii%29%20cross-modal%20misalignment%20between%20speech%20and%20text.%0ABased%20on%20this%20analysis%2C%20we%20introduce%20SALAD--Sample-efficient%20Alignment%20with%0ALearning%20through%20Active%20selection%20and%20cross-modal%20Distillation--which%20combines%0Across-modal%20distillation%20with%20targeted%20synthetic%20data%20to%20improve%20alignment%0Awhile%20mitigating%20forgetting.%20Applied%20to%203B%20and%207B%20LLMs%2C%20SALAD%20achieves%0Acompetitive%20performance%20with%20a%20strong%20open-weight%20model%20across%20broad-domain%0Abenchmarks%20in%20knowledge%2C%20language%20understanding%2C%20and%20reasoning%2C%20while%20training%0Aon%20over%20an%20order%20of%20magnitude%20less%20speech%20data%20from%20public%20corpora.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13632v1&entry.124074799=Read"},
{"title": "DOLFIN: Balancing Stability and Plasticity in Federated Continual\n  Learning", "author": "Omayma Moussadek and Riccardo Salami and Simone Calderara", "abstract": "  Federated continual learning (FCL) enables models to learn new tasks across\nmultiple distributed clients, protecting privacy and without forgetting\npreviously acquired knowledge. However, current methods face challenges\nbalancing performance, privacy preservation, and communication efficiency. We\nintroduce a Distributed Online LoRA for Federated INcremental learning method\nDOLFIN, a novel approach combining Vision Transformers with low-rank adapters\ndesigned to efficiently and stably learn new tasks in federated environments.\nOur method leverages LoRA for minimal communication overhead and incorporates\nDualGradient Projection Memory (DualGPM) to prevent forgetting. Evaluated on\nCIFAR-100, ImageNet-R, ImageNet-A, and CUB-200 under two Dirichlet\nheterogeneity settings, DOLFIN consistently surpasses six strong baselines in\nfinal average accuracy while matching their memory footprint. Orthogonal\nlow-rank adapters offer an effective and scalable solution for\nprivacy-preserving continual learning in federated settings.\n", "link": "http://arxiv.org/abs/2510.13567v1", "date": "2025-10-15", "relevancy": 2.686, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5551}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5282}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5282}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DOLFIN%3A%20Balancing%20Stability%20and%20Plasticity%20in%20Federated%20Continual%0A%20%20Learning&body=Title%3A%20DOLFIN%3A%20Balancing%20Stability%20and%20Plasticity%20in%20Federated%20Continual%0A%20%20Learning%0AAuthor%3A%20Omayma%20Moussadek%20and%20Riccardo%20Salami%20and%20Simone%20Calderara%0AAbstract%3A%20%20%20Federated%20continual%20learning%20%28FCL%29%20enables%20models%20to%20learn%20new%20tasks%20across%0Amultiple%20distributed%20clients%2C%20protecting%20privacy%20and%20without%20forgetting%0Apreviously%20acquired%20knowledge.%20However%2C%20current%20methods%20face%20challenges%0Abalancing%20performance%2C%20privacy%20preservation%2C%20and%20communication%20efficiency.%20We%0Aintroduce%20a%20Distributed%20Online%20LoRA%20for%20Federated%20INcremental%20learning%20method%0ADOLFIN%2C%20a%20novel%20approach%20combining%20Vision%20Transformers%20with%20low-rank%20adapters%0Adesigned%20to%20efficiently%20and%20stably%20learn%20new%20tasks%20in%20federated%20environments.%0AOur%20method%20leverages%20LoRA%20for%20minimal%20communication%20overhead%20and%20incorporates%0ADualGradient%20Projection%20Memory%20%28DualGPM%29%20to%20prevent%20forgetting.%20Evaluated%20on%0ACIFAR-100%2C%20ImageNet-R%2C%20ImageNet-A%2C%20and%20CUB-200%20under%20two%20Dirichlet%0Aheterogeneity%20settings%2C%20DOLFIN%20consistently%20surpasses%20six%20strong%20baselines%20in%0Afinal%20average%20accuracy%20while%20matching%20their%20memory%20footprint.%20Orthogonal%0Alow-rank%20adapters%20offer%20an%20effective%20and%20scalable%20solution%20for%0Aprivacy-preserving%20continual%20learning%20in%20federated%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13567v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDOLFIN%253A%2520Balancing%2520Stability%2520and%2520Plasticity%2520in%2520Federated%2520Continual%250A%2520%2520Learning%26entry.906535625%3DOmayma%2520Moussadek%2520and%2520Riccardo%2520Salami%2520and%2520Simone%2520Calderara%26entry.1292438233%3D%2520%2520Federated%2520continual%2520learning%2520%2528FCL%2529%2520enables%2520models%2520to%2520learn%2520new%2520tasks%2520across%250Amultiple%2520distributed%2520clients%252C%2520protecting%2520privacy%2520and%2520without%2520forgetting%250Apreviously%2520acquired%2520knowledge.%2520However%252C%2520current%2520methods%2520face%2520challenges%250Abalancing%2520performance%252C%2520privacy%2520preservation%252C%2520and%2520communication%2520efficiency.%2520We%250Aintroduce%2520a%2520Distributed%2520Online%2520LoRA%2520for%2520Federated%2520INcremental%2520learning%2520method%250ADOLFIN%252C%2520a%2520novel%2520approach%2520combining%2520Vision%2520Transformers%2520with%2520low-rank%2520adapters%250Adesigned%2520to%2520efficiently%2520and%2520stably%2520learn%2520new%2520tasks%2520in%2520federated%2520environments.%250AOur%2520method%2520leverages%2520LoRA%2520for%2520minimal%2520communication%2520overhead%2520and%2520incorporates%250ADualGradient%2520Projection%2520Memory%2520%2528DualGPM%2529%2520to%2520prevent%2520forgetting.%2520Evaluated%2520on%250ACIFAR-100%252C%2520ImageNet-R%252C%2520ImageNet-A%252C%2520and%2520CUB-200%2520under%2520two%2520Dirichlet%250Aheterogeneity%2520settings%252C%2520DOLFIN%2520consistently%2520surpasses%2520six%2520strong%2520baselines%2520in%250Afinal%2520average%2520accuracy%2520while%2520matching%2520their%2520memory%2520footprint.%2520Orthogonal%250Alow-rank%2520adapters%2520offer%2520an%2520effective%2520and%2520scalable%2520solution%2520for%250Aprivacy-preserving%2520continual%2520learning%2520in%2520federated%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13567v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DOLFIN%3A%20Balancing%20Stability%20and%20Plasticity%20in%20Federated%20Continual%0A%20%20Learning&entry.906535625=Omayma%20Moussadek%20and%20Riccardo%20Salami%20and%20Simone%20Calderara&entry.1292438233=%20%20Federated%20continual%20learning%20%28FCL%29%20enables%20models%20to%20learn%20new%20tasks%20across%0Amultiple%20distributed%20clients%2C%20protecting%20privacy%20and%20without%20forgetting%0Apreviously%20acquired%20knowledge.%20However%2C%20current%20methods%20face%20challenges%0Abalancing%20performance%2C%20privacy%20preservation%2C%20and%20communication%20efficiency.%20We%0Aintroduce%20a%20Distributed%20Online%20LoRA%20for%20Federated%20INcremental%20learning%20method%0ADOLFIN%2C%20a%20novel%20approach%20combining%20Vision%20Transformers%20with%20low-rank%20adapters%0Adesigned%20to%20efficiently%20and%20stably%20learn%20new%20tasks%20in%20federated%20environments.%0AOur%20method%20leverages%20LoRA%20for%20minimal%20communication%20overhead%20and%20incorporates%0ADualGradient%20Projection%20Memory%20%28DualGPM%29%20to%20prevent%20forgetting.%20Evaluated%20on%0ACIFAR-100%2C%20ImageNet-R%2C%20ImageNet-A%2C%20and%20CUB-200%20under%20two%20Dirichlet%0Aheterogeneity%20settings%2C%20DOLFIN%20consistently%20surpasses%20six%20strong%20baselines%20in%0Afinal%20average%20accuracy%20while%20matching%20their%20memory%20footprint.%20Orthogonal%0Alow-rank%20adapters%20offer%20an%20effective%20and%20scalable%20solution%20for%0Aprivacy-preserving%20continual%20learning%20in%20federated%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13567v1&entry.124074799=Read"},
{"title": "Tensor Gaussian Processes: Efficient Solvers for Nonlinear PDEs", "author": "Qiwei Yuan and Zhitong Xu and Yinghao Chen and Yiming Xu and Houman Owhadi and Shandian Zhe", "abstract": "  Machine learning solvers for partial differential equations (PDEs) have\nattracted growing interest. However, most existing approaches, such as neural\nnetwork solvers, rely on stochastic training, which is inefficient and\ntypically requires a great many training epochs. Gaussian process\n(GP)/kernel-based solvers, while mathematical principled, suffer from\nscalability issues when handling large numbers of collocation points often\nneeded for challenging or higher-dimensional PDEs.\n  To overcome these limitations, we propose TGPS, a tensor-GP-based solver that\nmodels factor functions along each input dimension using one-dimensional GPs\nand combines them via tensor decomposition to approximate the full solution.\nThis design reduces the task to learning a collection of one-dimensional GPs,\nsubstantially lowering computational complexity, and enabling scalability to\nmassive collocation sets.\n  For efficient nonlinear PDE solving, we use a partial freezing strategy and\nNewton's method to linerize the nonlinear terms. We then develop an alternating\nleast squares (ALS) approach that admits closed-form updates, thereby\nsubstantially enhancing the training efficiency. We establish theoretical\nguarantees on the expressivity of our model, together with convergence proof\nand error analysis under standard regularity assumptions. Experiments on\nseveral benchmark PDEs demonstrate that our method achieves superior accuracy\nand efficiency compared to existing approaches.\n", "link": "http://arxiv.org/abs/2510.13772v1", "date": "2025-10-15", "relevancy": 2.6829, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5531}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5358}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tensor%20Gaussian%20Processes%3A%20Efficient%20Solvers%20for%20Nonlinear%20PDEs&body=Title%3A%20Tensor%20Gaussian%20Processes%3A%20Efficient%20Solvers%20for%20Nonlinear%20PDEs%0AAuthor%3A%20Qiwei%20Yuan%20and%20Zhitong%20Xu%20and%20Yinghao%20Chen%20and%20Yiming%20Xu%20and%20Houman%20Owhadi%20and%20Shandian%20Zhe%0AAbstract%3A%20%20%20Machine%20learning%20solvers%20for%20partial%20differential%20equations%20%28PDEs%29%20have%0Aattracted%20growing%20interest.%20However%2C%20most%20existing%20approaches%2C%20such%20as%20neural%0Anetwork%20solvers%2C%20rely%20on%20stochastic%20training%2C%20which%20is%20inefficient%20and%0Atypically%20requires%20a%20great%20many%20training%20epochs.%20Gaussian%20process%0A%28GP%29/kernel-based%20solvers%2C%20while%20mathematical%20principled%2C%20suffer%20from%0Ascalability%20issues%20when%20handling%20large%20numbers%20of%20collocation%20points%20often%0Aneeded%20for%20challenging%20or%20higher-dimensional%20PDEs.%0A%20%20To%20overcome%20these%20limitations%2C%20we%20propose%20TGPS%2C%20a%20tensor-GP-based%20solver%20that%0Amodels%20factor%20functions%20along%20each%20input%20dimension%20using%20one-dimensional%20GPs%0Aand%20combines%20them%20via%20tensor%20decomposition%20to%20approximate%20the%20full%20solution.%0AThis%20design%20reduces%20the%20task%20to%20learning%20a%20collection%20of%20one-dimensional%20GPs%2C%0Asubstantially%20lowering%20computational%20complexity%2C%20and%20enabling%20scalability%20to%0Amassive%20collocation%20sets.%0A%20%20For%20efficient%20nonlinear%20PDE%20solving%2C%20we%20use%20a%20partial%20freezing%20strategy%20and%0ANewton%27s%20method%20to%20linerize%20the%20nonlinear%20terms.%20We%20then%20develop%20an%20alternating%0Aleast%20squares%20%28ALS%29%20approach%20that%20admits%20closed-form%20updates%2C%20thereby%0Asubstantially%20enhancing%20the%20training%20efficiency.%20We%20establish%20theoretical%0Aguarantees%20on%20the%20expressivity%20of%20our%20model%2C%20together%20with%20convergence%20proof%0Aand%20error%20analysis%20under%20standard%20regularity%20assumptions.%20Experiments%20on%0Aseveral%20benchmark%20PDEs%20demonstrate%20that%20our%20method%20achieves%20superior%20accuracy%0Aand%20efficiency%20compared%20to%20existing%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13772v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTensor%2520Gaussian%2520Processes%253A%2520Efficient%2520Solvers%2520for%2520Nonlinear%2520PDEs%26entry.906535625%3DQiwei%2520Yuan%2520and%2520Zhitong%2520Xu%2520and%2520Yinghao%2520Chen%2520and%2520Yiming%2520Xu%2520and%2520Houman%2520Owhadi%2520and%2520Shandian%2520Zhe%26entry.1292438233%3D%2520%2520Machine%2520learning%2520solvers%2520for%2520partial%2520differential%2520equations%2520%2528PDEs%2529%2520have%250Aattracted%2520growing%2520interest.%2520However%252C%2520most%2520existing%2520approaches%252C%2520such%2520as%2520neural%250Anetwork%2520solvers%252C%2520rely%2520on%2520stochastic%2520training%252C%2520which%2520is%2520inefficient%2520and%250Atypically%2520requires%2520a%2520great%2520many%2520training%2520epochs.%2520Gaussian%2520process%250A%2528GP%2529/kernel-based%2520solvers%252C%2520while%2520mathematical%2520principled%252C%2520suffer%2520from%250Ascalability%2520issues%2520when%2520handling%2520large%2520numbers%2520of%2520collocation%2520points%2520often%250Aneeded%2520for%2520challenging%2520or%2520higher-dimensional%2520PDEs.%250A%2520%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520TGPS%252C%2520a%2520tensor-GP-based%2520solver%2520that%250Amodels%2520factor%2520functions%2520along%2520each%2520input%2520dimension%2520using%2520one-dimensional%2520GPs%250Aand%2520combines%2520them%2520via%2520tensor%2520decomposition%2520to%2520approximate%2520the%2520full%2520solution.%250AThis%2520design%2520reduces%2520the%2520task%2520to%2520learning%2520a%2520collection%2520of%2520one-dimensional%2520GPs%252C%250Asubstantially%2520lowering%2520computational%2520complexity%252C%2520and%2520enabling%2520scalability%2520to%250Amassive%2520collocation%2520sets.%250A%2520%2520For%2520efficient%2520nonlinear%2520PDE%2520solving%252C%2520we%2520use%2520a%2520partial%2520freezing%2520strategy%2520and%250ANewton%2527s%2520method%2520to%2520linerize%2520the%2520nonlinear%2520terms.%2520We%2520then%2520develop%2520an%2520alternating%250Aleast%2520squares%2520%2528ALS%2529%2520approach%2520that%2520admits%2520closed-form%2520updates%252C%2520thereby%250Asubstantially%2520enhancing%2520the%2520training%2520efficiency.%2520We%2520establish%2520theoretical%250Aguarantees%2520on%2520the%2520expressivity%2520of%2520our%2520model%252C%2520together%2520with%2520convergence%2520proof%250Aand%2520error%2520analysis%2520under%2520standard%2520regularity%2520assumptions.%2520Experiments%2520on%250Aseveral%2520benchmark%2520PDEs%2520demonstrate%2520that%2520our%2520method%2520achieves%2520superior%2520accuracy%250Aand%2520efficiency%2520compared%2520to%2520existing%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13772v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tensor%20Gaussian%20Processes%3A%20Efficient%20Solvers%20for%20Nonlinear%20PDEs&entry.906535625=Qiwei%20Yuan%20and%20Zhitong%20Xu%20and%20Yinghao%20Chen%20and%20Yiming%20Xu%20and%20Houman%20Owhadi%20and%20Shandian%20Zhe&entry.1292438233=%20%20Machine%20learning%20solvers%20for%20partial%20differential%20equations%20%28PDEs%29%20have%0Aattracted%20growing%20interest.%20However%2C%20most%20existing%20approaches%2C%20such%20as%20neural%0Anetwork%20solvers%2C%20rely%20on%20stochastic%20training%2C%20which%20is%20inefficient%20and%0Atypically%20requires%20a%20great%20many%20training%20epochs.%20Gaussian%20process%0A%28GP%29/kernel-based%20solvers%2C%20while%20mathematical%20principled%2C%20suffer%20from%0Ascalability%20issues%20when%20handling%20large%20numbers%20of%20collocation%20points%20often%0Aneeded%20for%20challenging%20or%20higher-dimensional%20PDEs.%0A%20%20To%20overcome%20these%20limitations%2C%20we%20propose%20TGPS%2C%20a%20tensor-GP-based%20solver%20that%0Amodels%20factor%20functions%20along%20each%20input%20dimension%20using%20one-dimensional%20GPs%0Aand%20combines%20them%20via%20tensor%20decomposition%20to%20approximate%20the%20full%20solution.%0AThis%20design%20reduces%20the%20task%20to%20learning%20a%20collection%20of%20one-dimensional%20GPs%2C%0Asubstantially%20lowering%20computational%20complexity%2C%20and%20enabling%20scalability%20to%0Amassive%20collocation%20sets.%0A%20%20For%20efficient%20nonlinear%20PDE%20solving%2C%20we%20use%20a%20partial%20freezing%20strategy%20and%0ANewton%27s%20method%20to%20linerize%20the%20nonlinear%20terms.%20We%20then%20develop%20an%20alternating%0Aleast%20squares%20%28ALS%29%20approach%20that%20admits%20closed-form%20updates%2C%20thereby%0Asubstantially%20enhancing%20the%20training%20efficiency.%20We%20establish%20theoretical%0Aguarantees%20on%20the%20expressivity%20of%20our%20model%2C%20together%20with%20convergence%20proof%0Aand%20error%20analysis%20under%20standard%20regularity%20assumptions.%20Experiments%20on%0Aseveral%20benchmark%20PDEs%20demonstrate%20that%20our%20method%20achieves%20superior%20accuracy%0Aand%20efficiency%20compared%20to%20existing%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13772v1&entry.124074799=Read"},
{"title": "Axial Neural Networks for Dimension-Free Foundation Models", "author": "Hyunsu Kim and Jonggeon Park and Joan Bruna and Hongseok Yang and Juho Lee", "abstract": "  The advent of foundation models in AI has significantly advanced\ngeneral-purpose learning, enabling remarkable capabilities in zero-shot\ninference and in-context learning. However, training such models on physics\ndata, including solutions to partial differential equations (PDEs), poses a\nunique challenge due to varying dimensionalities across different systems.\nTraditional approaches either fix a maximum dimension or employ separate\nencoders for different dimensionalities, resulting in inefficiencies. To\naddress this, we propose a dimension-agnostic neural network architecture, the\nAxial Neural Network (XNN), inspired by parameter-sharing structures such as\nDeep Sets and Graph Neural Networks. XNN generalizes across varying tensor\ndimensions while maintaining computational efficiency. We convert existing PDE\nfoundation models into axial neural networks and evaluate their performance\nacross three training scenarios: training from scratch, pretraining on multiple\nPDEs, and fine-tuning on a single PDE. Our experiments show that XNNs perform\ncompetitively with original models and exhibit superior generalization to\nunseen dimensions, highlighting the importance of multidimensional pretraining\nfor foundation models.\n", "link": "http://arxiv.org/abs/2510.13665v1", "date": "2025-10-15", "relevancy": 2.6814, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.544}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.544}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Axial%20Neural%20Networks%20for%20Dimension-Free%20Foundation%20Models&body=Title%3A%20Axial%20Neural%20Networks%20for%20Dimension-Free%20Foundation%20Models%0AAuthor%3A%20Hyunsu%20Kim%20and%20Jonggeon%20Park%20and%20Joan%20Bruna%20and%20Hongseok%20Yang%20and%20Juho%20Lee%0AAbstract%3A%20%20%20The%20advent%20of%20foundation%20models%20in%20AI%20has%20significantly%20advanced%0Ageneral-purpose%20learning%2C%20enabling%20remarkable%20capabilities%20in%20zero-shot%0Ainference%20and%20in-context%20learning.%20However%2C%20training%20such%20models%20on%20physics%0Adata%2C%20including%20solutions%20to%20partial%20differential%20equations%20%28PDEs%29%2C%20poses%20a%0Aunique%20challenge%20due%20to%20varying%20dimensionalities%20across%20different%20systems.%0ATraditional%20approaches%20either%20fix%20a%20maximum%20dimension%20or%20employ%20separate%0Aencoders%20for%20different%20dimensionalities%2C%20resulting%20in%20inefficiencies.%20To%0Aaddress%20this%2C%20we%20propose%20a%20dimension-agnostic%20neural%20network%20architecture%2C%20the%0AAxial%20Neural%20Network%20%28XNN%29%2C%20inspired%20by%20parameter-sharing%20structures%20such%20as%0ADeep%20Sets%20and%20Graph%20Neural%20Networks.%20XNN%20generalizes%20across%20varying%20tensor%0Adimensions%20while%20maintaining%20computational%20efficiency.%20We%20convert%20existing%20PDE%0Afoundation%20models%20into%20axial%20neural%20networks%20and%20evaluate%20their%20performance%0Aacross%20three%20training%20scenarios%3A%20training%20from%20scratch%2C%20pretraining%20on%20multiple%0APDEs%2C%20and%20fine-tuning%20on%20a%20single%20PDE.%20Our%20experiments%20show%20that%20XNNs%20perform%0Acompetitively%20with%20original%20models%20and%20exhibit%20superior%20generalization%20to%0Aunseen%20dimensions%2C%20highlighting%20the%20importance%20of%20multidimensional%20pretraining%0Afor%20foundation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13665v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAxial%2520Neural%2520Networks%2520for%2520Dimension-Free%2520Foundation%2520Models%26entry.906535625%3DHyunsu%2520Kim%2520and%2520Jonggeon%2520Park%2520and%2520Joan%2520Bruna%2520and%2520Hongseok%2520Yang%2520and%2520Juho%2520Lee%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520foundation%2520models%2520in%2520AI%2520has%2520significantly%2520advanced%250Ageneral-purpose%2520learning%252C%2520enabling%2520remarkable%2520capabilities%2520in%2520zero-shot%250Ainference%2520and%2520in-context%2520learning.%2520However%252C%2520training%2520such%2520models%2520on%2520physics%250Adata%252C%2520including%2520solutions%2520to%2520partial%2520differential%2520equations%2520%2528PDEs%2529%252C%2520poses%2520a%250Aunique%2520challenge%2520due%2520to%2520varying%2520dimensionalities%2520across%2520different%2520systems.%250ATraditional%2520approaches%2520either%2520fix%2520a%2520maximum%2520dimension%2520or%2520employ%2520separate%250Aencoders%2520for%2520different%2520dimensionalities%252C%2520resulting%2520in%2520inefficiencies.%2520To%250Aaddress%2520this%252C%2520we%2520propose%2520a%2520dimension-agnostic%2520neural%2520network%2520architecture%252C%2520the%250AAxial%2520Neural%2520Network%2520%2528XNN%2529%252C%2520inspired%2520by%2520parameter-sharing%2520structures%2520such%2520as%250ADeep%2520Sets%2520and%2520Graph%2520Neural%2520Networks.%2520XNN%2520generalizes%2520across%2520varying%2520tensor%250Adimensions%2520while%2520maintaining%2520computational%2520efficiency.%2520We%2520convert%2520existing%2520PDE%250Afoundation%2520models%2520into%2520axial%2520neural%2520networks%2520and%2520evaluate%2520their%2520performance%250Aacross%2520three%2520training%2520scenarios%253A%2520training%2520from%2520scratch%252C%2520pretraining%2520on%2520multiple%250APDEs%252C%2520and%2520fine-tuning%2520on%2520a%2520single%2520PDE.%2520Our%2520experiments%2520show%2520that%2520XNNs%2520perform%250Acompetitively%2520with%2520original%2520models%2520and%2520exhibit%2520superior%2520generalization%2520to%250Aunseen%2520dimensions%252C%2520highlighting%2520the%2520importance%2520of%2520multidimensional%2520pretraining%250Afor%2520foundation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13665v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Axial%20Neural%20Networks%20for%20Dimension-Free%20Foundation%20Models&entry.906535625=Hyunsu%20Kim%20and%20Jonggeon%20Park%20and%20Joan%20Bruna%20and%20Hongseok%20Yang%20and%20Juho%20Lee&entry.1292438233=%20%20The%20advent%20of%20foundation%20models%20in%20AI%20has%20significantly%20advanced%0Ageneral-purpose%20learning%2C%20enabling%20remarkable%20capabilities%20in%20zero-shot%0Ainference%20and%20in-context%20learning.%20However%2C%20training%20such%20models%20on%20physics%0Adata%2C%20including%20solutions%20to%20partial%20differential%20equations%20%28PDEs%29%2C%20poses%20a%0Aunique%20challenge%20due%20to%20varying%20dimensionalities%20across%20different%20systems.%0ATraditional%20approaches%20either%20fix%20a%20maximum%20dimension%20or%20employ%20separate%0Aencoders%20for%20different%20dimensionalities%2C%20resulting%20in%20inefficiencies.%20To%0Aaddress%20this%2C%20we%20propose%20a%20dimension-agnostic%20neural%20network%20architecture%2C%20the%0AAxial%20Neural%20Network%20%28XNN%29%2C%20inspired%20by%20parameter-sharing%20structures%20such%20as%0ADeep%20Sets%20and%20Graph%20Neural%20Networks.%20XNN%20generalizes%20across%20varying%20tensor%0Adimensions%20while%20maintaining%20computational%20efficiency.%20We%20convert%20existing%20PDE%0Afoundation%20models%20into%20axial%20neural%20networks%20and%20evaluate%20their%20performance%0Aacross%20three%20training%20scenarios%3A%20training%20from%20scratch%2C%20pretraining%20on%20multiple%0APDEs%2C%20and%20fine-tuning%20on%20a%20single%20PDE.%20Our%20experiments%20show%20that%20XNNs%20perform%0Acompetitively%20with%20original%20models%20and%20exhibit%20superior%20generalization%20to%0Aunseen%20dimensions%2C%20highlighting%20the%20importance%20of%20multidimensional%20pretraining%0Afor%20foundation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13665v1&entry.124074799=Read"},
{"title": "Tandem Training for Language Models", "author": "Robert West and Ashton Anderson and Ece Kamar and Eric Horvitz", "abstract": "  As language models continue to rapidly improve, we can expect their actions\nand reasoning to become difficult or impossible for weaker agents and humans to\nfollow, undermining interpretability and oversight. With an eye on long-term\nfutures, we pursue methods that encourage models to produce solutions that\nremain intelligible to weaker collaborators. We formalize intelligibility as\nhandoff robustness: a strong model's solution is intelligible to a weaker model\nif randomly handing off control to the weaker model along the solution path\ndoes not cause failure. Building on this criterion, we introduce tandem\ntraining for language models, a reinforcement learning (RL) paradigm in which\nrollout tokens are intermittently and randomly sampled from a frozen weak model\nrather than the strong model being trained. Because rollouts succeed only when\nthe strong model's actions and reasoning process can be continued by the weak\nmodel -- when the two can co-construct a successful solution -- optimizing\nstandard RL objectives with tandem training implicitly incentivizes both\ncorrectness and intelligibility. In the GSM8K math reasoning task, tandem\ntraining reliably teaches models to abandon jargon and adapt their language to\nweaker partners while keeping task accuracy high. Our results demonstrate a\npromising route to building AI systems that remain auditable by weaker agents,\nwith implications for human--AI collaboration and multi-agent communication.\n", "link": "http://arxiv.org/abs/2510.13551v1", "date": "2025-10-15", "relevancy": 2.6745, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5523}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.544}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tandem%20Training%20for%20Language%20Models&body=Title%3A%20Tandem%20Training%20for%20Language%20Models%0AAuthor%3A%20Robert%20West%20and%20Ashton%20Anderson%20and%20Ece%20Kamar%20and%20Eric%20Horvitz%0AAbstract%3A%20%20%20As%20language%20models%20continue%20to%20rapidly%20improve%2C%20we%20can%20expect%20their%20actions%0Aand%20reasoning%20to%20become%20difficult%20or%20impossible%20for%20weaker%20agents%20and%20humans%20to%0Afollow%2C%20undermining%20interpretability%20and%20oversight.%20With%20an%20eye%20on%20long-term%0Afutures%2C%20we%20pursue%20methods%20that%20encourage%20models%20to%20produce%20solutions%20that%0Aremain%20intelligible%20to%20weaker%20collaborators.%20We%20formalize%20intelligibility%20as%0Ahandoff%20robustness%3A%20a%20strong%20model%27s%20solution%20is%20intelligible%20to%20a%20weaker%20model%0Aif%20randomly%20handing%20off%20control%20to%20the%20weaker%20model%20along%20the%20solution%20path%0Adoes%20not%20cause%20failure.%20Building%20on%20this%20criterion%2C%20we%20introduce%20tandem%0Atraining%20for%20language%20models%2C%20a%20reinforcement%20learning%20%28RL%29%20paradigm%20in%20which%0Arollout%20tokens%20are%20intermittently%20and%20randomly%20sampled%20from%20a%20frozen%20weak%20model%0Arather%20than%20the%20strong%20model%20being%20trained.%20Because%20rollouts%20succeed%20only%20when%0Athe%20strong%20model%27s%20actions%20and%20reasoning%20process%20can%20be%20continued%20by%20the%20weak%0Amodel%20--%20when%20the%20two%20can%20co-construct%20a%20successful%20solution%20--%20optimizing%0Astandard%20RL%20objectives%20with%20tandem%20training%20implicitly%20incentivizes%20both%0Acorrectness%20and%20intelligibility.%20In%20the%20GSM8K%20math%20reasoning%20task%2C%20tandem%0Atraining%20reliably%20teaches%20models%20to%20abandon%20jargon%20and%20adapt%20their%20language%20to%0Aweaker%20partners%20while%20keeping%20task%20accuracy%20high.%20Our%20results%20demonstrate%20a%0Apromising%20route%20to%20building%20AI%20systems%20that%20remain%20auditable%20by%20weaker%20agents%2C%0Awith%20implications%20for%20human--AI%20collaboration%20and%20multi-agent%20communication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13551v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTandem%2520Training%2520for%2520Language%2520Models%26entry.906535625%3DRobert%2520West%2520and%2520Ashton%2520Anderson%2520and%2520Ece%2520Kamar%2520and%2520Eric%2520Horvitz%26entry.1292438233%3D%2520%2520As%2520language%2520models%2520continue%2520to%2520rapidly%2520improve%252C%2520we%2520can%2520expect%2520their%2520actions%250Aand%2520reasoning%2520to%2520become%2520difficult%2520or%2520impossible%2520for%2520weaker%2520agents%2520and%2520humans%2520to%250Afollow%252C%2520undermining%2520interpretability%2520and%2520oversight.%2520With%2520an%2520eye%2520on%2520long-term%250Afutures%252C%2520we%2520pursue%2520methods%2520that%2520encourage%2520models%2520to%2520produce%2520solutions%2520that%250Aremain%2520intelligible%2520to%2520weaker%2520collaborators.%2520We%2520formalize%2520intelligibility%2520as%250Ahandoff%2520robustness%253A%2520a%2520strong%2520model%2527s%2520solution%2520is%2520intelligible%2520to%2520a%2520weaker%2520model%250Aif%2520randomly%2520handing%2520off%2520control%2520to%2520the%2520weaker%2520model%2520along%2520the%2520solution%2520path%250Adoes%2520not%2520cause%2520failure.%2520Building%2520on%2520this%2520criterion%252C%2520we%2520introduce%2520tandem%250Atraining%2520for%2520language%2520models%252C%2520a%2520reinforcement%2520learning%2520%2528RL%2529%2520paradigm%2520in%2520which%250Arollout%2520tokens%2520are%2520intermittently%2520and%2520randomly%2520sampled%2520from%2520a%2520frozen%2520weak%2520model%250Arather%2520than%2520the%2520strong%2520model%2520being%2520trained.%2520Because%2520rollouts%2520succeed%2520only%2520when%250Athe%2520strong%2520model%2527s%2520actions%2520and%2520reasoning%2520process%2520can%2520be%2520continued%2520by%2520the%2520weak%250Amodel%2520--%2520when%2520the%2520two%2520can%2520co-construct%2520a%2520successful%2520solution%2520--%2520optimizing%250Astandard%2520RL%2520objectives%2520with%2520tandem%2520training%2520implicitly%2520incentivizes%2520both%250Acorrectness%2520and%2520intelligibility.%2520In%2520the%2520GSM8K%2520math%2520reasoning%2520task%252C%2520tandem%250Atraining%2520reliably%2520teaches%2520models%2520to%2520abandon%2520jargon%2520and%2520adapt%2520their%2520language%2520to%250Aweaker%2520partners%2520while%2520keeping%2520task%2520accuracy%2520high.%2520Our%2520results%2520demonstrate%2520a%250Apromising%2520route%2520to%2520building%2520AI%2520systems%2520that%2520remain%2520auditable%2520by%2520weaker%2520agents%252C%250Awith%2520implications%2520for%2520human--AI%2520collaboration%2520and%2520multi-agent%2520communication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13551v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tandem%20Training%20for%20Language%20Models&entry.906535625=Robert%20West%20and%20Ashton%20Anderson%20and%20Ece%20Kamar%20and%20Eric%20Horvitz&entry.1292438233=%20%20As%20language%20models%20continue%20to%20rapidly%20improve%2C%20we%20can%20expect%20their%20actions%0Aand%20reasoning%20to%20become%20difficult%20or%20impossible%20for%20weaker%20agents%20and%20humans%20to%0Afollow%2C%20undermining%20interpretability%20and%20oversight.%20With%20an%20eye%20on%20long-term%0Afutures%2C%20we%20pursue%20methods%20that%20encourage%20models%20to%20produce%20solutions%20that%0Aremain%20intelligible%20to%20weaker%20collaborators.%20We%20formalize%20intelligibility%20as%0Ahandoff%20robustness%3A%20a%20strong%20model%27s%20solution%20is%20intelligible%20to%20a%20weaker%20model%0Aif%20randomly%20handing%20off%20control%20to%20the%20weaker%20model%20along%20the%20solution%20path%0Adoes%20not%20cause%20failure.%20Building%20on%20this%20criterion%2C%20we%20introduce%20tandem%0Atraining%20for%20language%20models%2C%20a%20reinforcement%20learning%20%28RL%29%20paradigm%20in%20which%0Arollout%20tokens%20are%20intermittently%20and%20randomly%20sampled%20from%20a%20frozen%20weak%20model%0Arather%20than%20the%20strong%20model%20being%20trained.%20Because%20rollouts%20succeed%20only%20when%0Athe%20strong%20model%27s%20actions%20and%20reasoning%20process%20can%20be%20continued%20by%20the%20weak%0Amodel%20--%20when%20the%20two%20can%20co-construct%20a%20successful%20solution%20--%20optimizing%0Astandard%20RL%20objectives%20with%20tandem%20training%20implicitly%20incentivizes%20both%0Acorrectness%20and%20intelligibility.%20In%20the%20GSM8K%20math%20reasoning%20task%2C%20tandem%0Atraining%20reliably%20teaches%20models%20to%20abandon%20jargon%20and%20adapt%20their%20language%20to%0Aweaker%20partners%20while%20keeping%20task%20accuracy%20high.%20Our%20results%20demonstrate%20a%0Apromising%20route%20to%20building%20AI%20systems%20that%20remain%20auditable%20by%20weaker%20agents%2C%0Awith%20implications%20for%20human--AI%20collaboration%20and%20multi-agent%20communication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13551v1&entry.124074799=Read"},
{"title": "Ultra High-Resolution Image Inpainting with Patch-Based Content\n  Consistency Adapter", "author": "Jianhui Zhang and Sheng Cheng and Qirui Sun and Jia Liu and Wang Luyang and Chaoyu Feng and Chen Fang and Lei Lei and Jue Wang and Shuaicheng Liu", "abstract": "  In this work, we present Patch-Adapter, an effective framework for\nhigh-resolution text-guided image inpainting. Unlike existing methods limited\nto lower resolutions, our approach achieves 4K+ resolution while maintaining\nprecise content consistency and prompt alignment, two critical challenges in\nimage inpainting that intensify with increasing resolution and texture\ncomplexity. Patch-Adapter leverages a two-stage adapter architecture to scale\nthe diffusion model's resolution from 1K to 4K+ without requiring structural\noverhauls: (1) Dual Context Adapter learns coherence between masked and\nunmasked regions at reduced resolutions to establish global structural\nconsistency; and (2) Reference Patch Adapter implements a patch-level attention\nmechanism for full-resolution inpainting, preserving local detail fidelity\nthrough adaptive feature fusion. This dual-stage architecture uniquely\naddresses the scalability gap in high-resolution inpainting by decoupling\nglobal semantics from localized refinement. Experiments demonstrate that\nPatch-Adapter not only resolves artifacts common in large-scale inpainting but\nalso achieves state-of-the-art performance on the OpenImages and\nPhoto-Concept-Bucket datasets, outperforming existing methods in both\nperceptual quality and text-prompt adherence.\n", "link": "http://arxiv.org/abs/2510.13419v1", "date": "2025-10-15", "relevancy": 2.6709, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5459}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5376}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ultra%20High-Resolution%20Image%20Inpainting%20with%20Patch-Based%20Content%0A%20%20Consistency%20Adapter&body=Title%3A%20Ultra%20High-Resolution%20Image%20Inpainting%20with%20Patch-Based%20Content%0A%20%20Consistency%20Adapter%0AAuthor%3A%20Jianhui%20Zhang%20and%20Sheng%20Cheng%20and%20Qirui%20Sun%20and%20Jia%20Liu%20and%20Wang%20Luyang%20and%20Chaoyu%20Feng%20and%20Chen%20Fang%20and%20Lei%20Lei%20and%20Jue%20Wang%20and%20Shuaicheng%20Liu%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20Patch-Adapter%2C%20an%20effective%20framework%20for%0Ahigh-resolution%20text-guided%20image%20inpainting.%20Unlike%20existing%20methods%20limited%0Ato%20lower%20resolutions%2C%20our%20approach%20achieves%204K%2B%20resolution%20while%20maintaining%0Aprecise%20content%20consistency%20and%20prompt%20alignment%2C%20two%20critical%20challenges%20in%0Aimage%20inpainting%20that%20intensify%20with%20increasing%20resolution%20and%20texture%0Acomplexity.%20Patch-Adapter%20leverages%20a%20two-stage%20adapter%20architecture%20to%20scale%0Athe%20diffusion%20model%27s%20resolution%20from%201K%20to%204K%2B%20without%20requiring%20structural%0Aoverhauls%3A%20%281%29%20Dual%20Context%20Adapter%20learns%20coherence%20between%20masked%20and%0Aunmasked%20regions%20at%20reduced%20resolutions%20to%20establish%20global%20structural%0Aconsistency%3B%20and%20%282%29%20Reference%20Patch%20Adapter%20implements%20a%20patch-level%20attention%0Amechanism%20for%20full-resolution%20inpainting%2C%20preserving%20local%20detail%20fidelity%0Athrough%20adaptive%20feature%20fusion.%20This%20dual-stage%20architecture%20uniquely%0Aaddresses%20the%20scalability%20gap%20in%20high-resolution%20inpainting%20by%20decoupling%0Aglobal%20semantics%20from%20localized%20refinement.%20Experiments%20demonstrate%20that%0APatch-Adapter%20not%20only%20resolves%20artifacts%20common%20in%20large-scale%20inpainting%20but%0Aalso%20achieves%20state-of-the-art%20performance%20on%20the%20OpenImages%20and%0APhoto-Concept-Bucket%20datasets%2C%20outperforming%20existing%20methods%20in%20both%0Aperceptual%20quality%20and%20text-prompt%20adherence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13419v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUltra%2520High-Resolution%2520Image%2520Inpainting%2520with%2520Patch-Based%2520Content%250A%2520%2520Consistency%2520Adapter%26entry.906535625%3DJianhui%2520Zhang%2520and%2520Sheng%2520Cheng%2520and%2520Qirui%2520Sun%2520and%2520Jia%2520Liu%2520and%2520Wang%2520Luyang%2520and%2520Chaoyu%2520Feng%2520and%2520Chen%2520Fang%2520and%2520Lei%2520Lei%2520and%2520Jue%2520Wang%2520and%2520Shuaicheng%2520Liu%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520present%2520Patch-Adapter%252C%2520an%2520effective%2520framework%2520for%250Ahigh-resolution%2520text-guided%2520image%2520inpainting.%2520Unlike%2520existing%2520methods%2520limited%250Ato%2520lower%2520resolutions%252C%2520our%2520approach%2520achieves%25204K%252B%2520resolution%2520while%2520maintaining%250Aprecise%2520content%2520consistency%2520and%2520prompt%2520alignment%252C%2520two%2520critical%2520challenges%2520in%250Aimage%2520inpainting%2520that%2520intensify%2520with%2520increasing%2520resolution%2520and%2520texture%250Acomplexity.%2520Patch-Adapter%2520leverages%2520a%2520two-stage%2520adapter%2520architecture%2520to%2520scale%250Athe%2520diffusion%2520model%2527s%2520resolution%2520from%25201K%2520to%25204K%252B%2520without%2520requiring%2520structural%250Aoverhauls%253A%2520%25281%2529%2520Dual%2520Context%2520Adapter%2520learns%2520coherence%2520between%2520masked%2520and%250Aunmasked%2520regions%2520at%2520reduced%2520resolutions%2520to%2520establish%2520global%2520structural%250Aconsistency%253B%2520and%2520%25282%2529%2520Reference%2520Patch%2520Adapter%2520implements%2520a%2520patch-level%2520attention%250Amechanism%2520for%2520full-resolution%2520inpainting%252C%2520preserving%2520local%2520detail%2520fidelity%250Athrough%2520adaptive%2520feature%2520fusion.%2520This%2520dual-stage%2520architecture%2520uniquely%250Aaddresses%2520the%2520scalability%2520gap%2520in%2520high-resolution%2520inpainting%2520by%2520decoupling%250Aglobal%2520semantics%2520from%2520localized%2520refinement.%2520Experiments%2520demonstrate%2520that%250APatch-Adapter%2520not%2520only%2520resolves%2520artifacts%2520common%2520in%2520large-scale%2520inpainting%2520but%250Aalso%2520achieves%2520state-of-the-art%2520performance%2520on%2520the%2520OpenImages%2520and%250APhoto-Concept-Bucket%2520datasets%252C%2520outperforming%2520existing%2520methods%2520in%2520both%250Aperceptual%2520quality%2520and%2520text-prompt%2520adherence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13419v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ultra%20High-Resolution%20Image%20Inpainting%20with%20Patch-Based%20Content%0A%20%20Consistency%20Adapter&entry.906535625=Jianhui%20Zhang%20and%20Sheng%20Cheng%20and%20Qirui%20Sun%20and%20Jia%20Liu%20and%20Wang%20Luyang%20and%20Chaoyu%20Feng%20and%20Chen%20Fang%20and%20Lei%20Lei%20and%20Jue%20Wang%20and%20Shuaicheng%20Liu&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20Patch-Adapter%2C%20an%20effective%20framework%20for%0Ahigh-resolution%20text-guided%20image%20inpainting.%20Unlike%20existing%20methods%20limited%0Ato%20lower%20resolutions%2C%20our%20approach%20achieves%204K%2B%20resolution%20while%20maintaining%0Aprecise%20content%20consistency%20and%20prompt%20alignment%2C%20two%20critical%20challenges%20in%0Aimage%20inpainting%20that%20intensify%20with%20increasing%20resolution%20and%20texture%0Acomplexity.%20Patch-Adapter%20leverages%20a%20two-stage%20adapter%20architecture%20to%20scale%0Athe%20diffusion%20model%27s%20resolution%20from%201K%20to%204K%2B%20without%20requiring%20structural%0Aoverhauls%3A%20%281%29%20Dual%20Context%20Adapter%20learns%20coherence%20between%20masked%20and%0Aunmasked%20regions%20at%20reduced%20resolutions%20to%20establish%20global%20structural%0Aconsistency%3B%20and%20%282%29%20Reference%20Patch%20Adapter%20implements%20a%20patch-level%20attention%0Amechanism%20for%20full-resolution%20inpainting%2C%20preserving%20local%20detail%20fidelity%0Athrough%20adaptive%20feature%20fusion.%20This%20dual-stage%20architecture%20uniquely%0Aaddresses%20the%20scalability%20gap%20in%20high-resolution%20inpainting%20by%20decoupling%0Aglobal%20semantics%20from%20localized%20refinement.%20Experiments%20demonstrate%20that%0APatch-Adapter%20not%20only%20resolves%20artifacts%20common%20in%20large-scale%20inpainting%20but%0Aalso%20achieves%20state-of-the-art%20performance%20on%20the%20OpenImages%20and%0APhoto-Concept-Bucket%20datasets%2C%20outperforming%20existing%20methods%20in%20both%0Aperceptual%20quality%20and%20text-prompt%20adherence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13419v1&entry.124074799=Read"},
{"title": "The quest for the GRAph Level autoEncoder (GRALE)", "author": "Paul Krzakala and Gabriel Melo and Charlotte Laclau and Florence d'Alch\u00e9-Buc and R\u00e9mi Flamary", "abstract": "  Although graph-based learning has attracted a lot of attention, graph\nrepresentation learning is still a challenging task whose resolution may impact\nkey application fields such as chemistry or biology. To this end, we introduce\nGRALE, a novel graph autoencoder that encodes and decodes graphs of varying\nsizes into a shared embedding space. GRALE is trained using an Optimal\nTransport-inspired loss that compares the original and reconstructed graphs and\nleverages a differentiable node matching module, which is trained jointly with\nthe encoder and decoder. The proposed attention-based architecture relies on\nEvoformer, the core component of AlphaFold, which we extend to support both\ngraph encoding and decoding. We show, in numerical experiments on simulated and\nmolecular data, that GRALE enables a highly general form of pre-training,\napplicable to a wide range of downstream tasks, from classification and\nregression to more complex tasks such as graph interpolation, editing,\nmatching, and prediction.\n", "link": "http://arxiv.org/abs/2505.22109v2", "date": "2025-10-15", "relevancy": 2.669, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5668}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5225}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20quest%20for%20the%20GRAph%20Level%20autoEncoder%20%28GRALE%29&body=Title%3A%20The%20quest%20for%20the%20GRAph%20Level%20autoEncoder%20%28GRALE%29%0AAuthor%3A%20Paul%20Krzakala%20and%20Gabriel%20Melo%20and%20Charlotte%20Laclau%20and%20Florence%20d%27Alch%C3%A9-Buc%20and%20R%C3%A9mi%20Flamary%0AAbstract%3A%20%20%20Although%20graph-based%20learning%20has%20attracted%20a%20lot%20of%20attention%2C%20graph%0Arepresentation%20learning%20is%20still%20a%20challenging%20task%20whose%20resolution%20may%20impact%0Akey%20application%20fields%20such%20as%20chemistry%20or%20biology.%20To%20this%20end%2C%20we%20introduce%0AGRALE%2C%20a%20novel%20graph%20autoencoder%20that%20encodes%20and%20decodes%20graphs%20of%20varying%0Asizes%20into%20a%20shared%20embedding%20space.%20GRALE%20is%20trained%20using%20an%20Optimal%0ATransport-inspired%20loss%20that%20compares%20the%20original%20and%20reconstructed%20graphs%20and%0Aleverages%20a%20differentiable%20node%20matching%20module%2C%20which%20is%20trained%20jointly%20with%0Athe%20encoder%20and%20decoder.%20The%20proposed%20attention-based%20architecture%20relies%20on%0AEvoformer%2C%20the%20core%20component%20of%20AlphaFold%2C%20which%20we%20extend%20to%20support%20both%0Agraph%20encoding%20and%20decoding.%20We%20show%2C%20in%20numerical%20experiments%20on%20simulated%20and%0Amolecular%20data%2C%20that%20GRALE%20enables%20a%20highly%20general%20form%20of%20pre-training%2C%0Aapplicable%20to%20a%20wide%20range%20of%20downstream%20tasks%2C%20from%20classification%20and%0Aregression%20to%20more%20complex%20tasks%20such%20as%20graph%20interpolation%2C%20editing%2C%0Amatching%2C%20and%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22109v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520quest%2520for%2520the%2520GRAph%2520Level%2520autoEncoder%2520%2528GRALE%2529%26entry.906535625%3DPaul%2520Krzakala%2520and%2520Gabriel%2520Melo%2520and%2520Charlotte%2520Laclau%2520and%2520Florence%2520d%2527Alch%25C3%25A9-Buc%2520and%2520R%25C3%25A9mi%2520Flamary%26entry.1292438233%3D%2520%2520Although%2520graph-based%2520learning%2520has%2520attracted%2520a%2520lot%2520of%2520attention%252C%2520graph%250Arepresentation%2520learning%2520is%2520still%2520a%2520challenging%2520task%2520whose%2520resolution%2520may%2520impact%250Akey%2520application%2520fields%2520such%2520as%2520chemistry%2520or%2520biology.%2520To%2520this%2520end%252C%2520we%2520introduce%250AGRALE%252C%2520a%2520novel%2520graph%2520autoencoder%2520that%2520encodes%2520and%2520decodes%2520graphs%2520of%2520varying%250Asizes%2520into%2520a%2520shared%2520embedding%2520space.%2520GRALE%2520is%2520trained%2520using%2520an%2520Optimal%250ATransport-inspired%2520loss%2520that%2520compares%2520the%2520original%2520and%2520reconstructed%2520graphs%2520and%250Aleverages%2520a%2520differentiable%2520node%2520matching%2520module%252C%2520which%2520is%2520trained%2520jointly%2520with%250Athe%2520encoder%2520and%2520decoder.%2520The%2520proposed%2520attention-based%2520architecture%2520relies%2520on%250AEvoformer%252C%2520the%2520core%2520component%2520of%2520AlphaFold%252C%2520which%2520we%2520extend%2520to%2520support%2520both%250Agraph%2520encoding%2520and%2520decoding.%2520We%2520show%252C%2520in%2520numerical%2520experiments%2520on%2520simulated%2520and%250Amolecular%2520data%252C%2520that%2520GRALE%2520enables%2520a%2520highly%2520general%2520form%2520of%2520pre-training%252C%250Aapplicable%2520to%2520a%2520wide%2520range%2520of%2520downstream%2520tasks%252C%2520from%2520classification%2520and%250Aregression%2520to%2520more%2520complex%2520tasks%2520such%2520as%2520graph%2520interpolation%252C%2520editing%252C%250Amatching%252C%2520and%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22109v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20quest%20for%20the%20GRAph%20Level%20autoEncoder%20%28GRALE%29&entry.906535625=Paul%20Krzakala%20and%20Gabriel%20Melo%20and%20Charlotte%20Laclau%20and%20Florence%20d%27Alch%C3%A9-Buc%20and%20R%C3%A9mi%20Flamary&entry.1292438233=%20%20Although%20graph-based%20learning%20has%20attracted%20a%20lot%20of%20attention%2C%20graph%0Arepresentation%20learning%20is%20still%20a%20challenging%20task%20whose%20resolution%20may%20impact%0Akey%20application%20fields%20such%20as%20chemistry%20or%20biology.%20To%20this%20end%2C%20we%20introduce%0AGRALE%2C%20a%20novel%20graph%20autoencoder%20that%20encodes%20and%20decodes%20graphs%20of%20varying%0Asizes%20into%20a%20shared%20embedding%20space.%20GRALE%20is%20trained%20using%20an%20Optimal%0ATransport-inspired%20loss%20that%20compares%20the%20original%20and%20reconstructed%20graphs%20and%0Aleverages%20a%20differentiable%20node%20matching%20module%2C%20which%20is%20trained%20jointly%20with%0Athe%20encoder%20and%20decoder.%20The%20proposed%20attention-based%20architecture%20relies%20on%0AEvoformer%2C%20the%20core%20component%20of%20AlphaFold%2C%20which%20we%20extend%20to%20support%20both%0Agraph%20encoding%20and%20decoding.%20We%20show%2C%20in%20numerical%20experiments%20on%20simulated%20and%0Amolecular%20data%2C%20that%20GRALE%20enables%20a%20highly%20general%20form%20of%20pre-training%2C%0Aapplicable%20to%20a%20wide%20range%20of%20downstream%20tasks%2C%20from%20classification%20and%0Aregression%20to%20more%20complex%20tasks%20such%20as%20graph%20interpolation%2C%20editing%2C%0Amatching%2C%20and%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22109v2&entry.124074799=Read"},
{"title": "Systematic Literature Review on Vehicular Collaborative Perception - A\n  Computer Vision Perspective", "author": "Lei Wan and Jianxin Zhao and Andreas Wiedholz and Manuel Bied and Mateus Martinez de Lucena and Abhishek Dinkar Jagtap and Andreas Festag and Ant\u00f4nio Augusto Fr\u00f6hlich and Hannan Ejaz Keen and Alexey Vinel", "abstract": "  The effectiveness of autonomous vehicles relies on reliable perception\ncapabilities. Despite significant advancements in artificial intelligence and\nsensor fusion technologies, current single-vehicle perception systems continue\nto encounter limitations, notably visual occlusions and limited long-range\ndetection capabilities. Collaborative Perception (CP), enabled by\nVehicle-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I) communication, has\nemerged as a promising solution to mitigate these issues and enhance the\nreliability of autonomous systems. Beyond advancements in communication, the\ncomputer vision community is increasingly focusing on improving vehicular\nperception through collaborative approaches. However, a systematic literature\nreview that thoroughly examines existing work and reduces subjective bias is\nstill lacking. Such a systematic approach helps identify research gaps,\nrecognize common trends across studies, and inform future research directions.\nIn response, this study follows the PRISMA 2020 guidelines and includes 106\npeer-reviewed articles. These publications are analyzed based on modalities,\ncollaboration schemes, and key perception tasks. Through a comparative\nanalysis, this review illustrates how different methods address practical\nissues such as pose errors, temporal latency, communication constraints, domain\nshifts, heterogeneity, and adversarial attacks. Furthermore, it critically\nexamines evaluation methodologies, highlighting a misalignment between current\nmetrics and CP's fundamental objectives. By delving into all relevant topics\nin-depth, this review offers valuable insights into challenges, opportunities,\nand risks, serving as a reference for advancing research in vehicular\ncollaborative perception.\n", "link": "http://arxiv.org/abs/2504.04631v2", "date": "2025-10-15", "relevancy": 2.6506, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5521}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5419}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4964}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Systematic%20Literature%20Review%20on%20Vehicular%20Collaborative%20Perception%20-%20A%0A%20%20Computer%20Vision%20Perspective&body=Title%3A%20Systematic%20Literature%20Review%20on%20Vehicular%20Collaborative%20Perception%20-%20A%0A%20%20Computer%20Vision%20Perspective%0AAuthor%3A%20Lei%20Wan%20and%20Jianxin%20Zhao%20and%20Andreas%20Wiedholz%20and%20Manuel%20Bied%20and%20Mateus%20Martinez%20de%20Lucena%20and%20Abhishek%20Dinkar%20Jagtap%20and%20Andreas%20Festag%20and%20Ant%C3%B4nio%20Augusto%20Fr%C3%B6hlich%20and%20Hannan%20Ejaz%20Keen%20and%20Alexey%20Vinel%0AAbstract%3A%20%20%20The%20effectiveness%20of%20autonomous%20vehicles%20relies%20on%20reliable%20perception%0Acapabilities.%20Despite%20significant%20advancements%20in%20artificial%20intelligence%20and%0Asensor%20fusion%20technologies%2C%20current%20single-vehicle%20perception%20systems%20continue%0Ato%20encounter%20limitations%2C%20notably%20visual%20occlusions%20and%20limited%20long-range%0Adetection%20capabilities.%20Collaborative%20Perception%20%28CP%29%2C%20enabled%20by%0AVehicle-to-Vehicle%20%28V2V%29%20and%20Vehicle-to-Infrastructure%20%28V2I%29%20communication%2C%20has%0Aemerged%20as%20a%20promising%20solution%20to%20mitigate%20these%20issues%20and%20enhance%20the%0Areliability%20of%20autonomous%20systems.%20Beyond%20advancements%20in%20communication%2C%20the%0Acomputer%20vision%20community%20is%20increasingly%20focusing%20on%20improving%20vehicular%0Aperception%20through%20collaborative%20approaches.%20However%2C%20a%20systematic%20literature%0Areview%20that%20thoroughly%20examines%20existing%20work%20and%20reduces%20subjective%20bias%20is%0Astill%20lacking.%20Such%20a%20systematic%20approach%20helps%20identify%20research%20gaps%2C%0Arecognize%20common%20trends%20across%20studies%2C%20and%20inform%20future%20research%20directions.%0AIn%20response%2C%20this%20study%20follows%20the%20PRISMA%202020%20guidelines%20and%20includes%20106%0Apeer-reviewed%20articles.%20These%20publications%20are%20analyzed%20based%20on%20modalities%2C%0Acollaboration%20schemes%2C%20and%20key%20perception%20tasks.%20Through%20a%20comparative%0Aanalysis%2C%20this%20review%20illustrates%20how%20different%20methods%20address%20practical%0Aissues%20such%20as%20pose%20errors%2C%20temporal%20latency%2C%20communication%20constraints%2C%20domain%0Ashifts%2C%20heterogeneity%2C%20and%20adversarial%20attacks.%20Furthermore%2C%20it%20critically%0Aexamines%20evaluation%20methodologies%2C%20highlighting%20a%20misalignment%20between%20current%0Ametrics%20and%20CP%27s%20fundamental%20objectives.%20By%20delving%20into%20all%20relevant%20topics%0Ain-depth%2C%20this%20review%20offers%20valuable%20insights%20into%20challenges%2C%20opportunities%2C%0Aand%20risks%2C%20serving%20as%20a%20reference%20for%20advancing%20research%20in%20vehicular%0Acollaborative%20perception.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.04631v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSystematic%2520Literature%2520Review%2520on%2520Vehicular%2520Collaborative%2520Perception%2520-%2520A%250A%2520%2520Computer%2520Vision%2520Perspective%26entry.906535625%3DLei%2520Wan%2520and%2520Jianxin%2520Zhao%2520and%2520Andreas%2520Wiedholz%2520and%2520Manuel%2520Bied%2520and%2520Mateus%2520Martinez%2520de%2520Lucena%2520and%2520Abhishek%2520Dinkar%2520Jagtap%2520and%2520Andreas%2520Festag%2520and%2520Ant%25C3%25B4nio%2520Augusto%2520Fr%25C3%25B6hlich%2520and%2520Hannan%2520Ejaz%2520Keen%2520and%2520Alexey%2520Vinel%26entry.1292438233%3D%2520%2520The%2520effectiveness%2520of%2520autonomous%2520vehicles%2520relies%2520on%2520reliable%2520perception%250Acapabilities.%2520Despite%2520significant%2520advancements%2520in%2520artificial%2520intelligence%2520and%250Asensor%2520fusion%2520technologies%252C%2520current%2520single-vehicle%2520perception%2520systems%2520continue%250Ato%2520encounter%2520limitations%252C%2520notably%2520visual%2520occlusions%2520and%2520limited%2520long-range%250Adetection%2520capabilities.%2520Collaborative%2520Perception%2520%2528CP%2529%252C%2520enabled%2520by%250AVehicle-to-Vehicle%2520%2528V2V%2529%2520and%2520Vehicle-to-Infrastructure%2520%2528V2I%2529%2520communication%252C%2520has%250Aemerged%2520as%2520a%2520promising%2520solution%2520to%2520mitigate%2520these%2520issues%2520and%2520enhance%2520the%250Areliability%2520of%2520autonomous%2520systems.%2520Beyond%2520advancements%2520in%2520communication%252C%2520the%250Acomputer%2520vision%2520community%2520is%2520increasingly%2520focusing%2520on%2520improving%2520vehicular%250Aperception%2520through%2520collaborative%2520approaches.%2520However%252C%2520a%2520systematic%2520literature%250Areview%2520that%2520thoroughly%2520examines%2520existing%2520work%2520and%2520reduces%2520subjective%2520bias%2520is%250Astill%2520lacking.%2520Such%2520a%2520systematic%2520approach%2520helps%2520identify%2520research%2520gaps%252C%250Arecognize%2520common%2520trends%2520across%2520studies%252C%2520and%2520inform%2520future%2520research%2520directions.%250AIn%2520response%252C%2520this%2520study%2520follows%2520the%2520PRISMA%25202020%2520guidelines%2520and%2520includes%2520106%250Apeer-reviewed%2520articles.%2520These%2520publications%2520are%2520analyzed%2520based%2520on%2520modalities%252C%250Acollaboration%2520schemes%252C%2520and%2520key%2520perception%2520tasks.%2520Through%2520a%2520comparative%250Aanalysis%252C%2520this%2520review%2520illustrates%2520how%2520different%2520methods%2520address%2520practical%250Aissues%2520such%2520as%2520pose%2520errors%252C%2520temporal%2520latency%252C%2520communication%2520constraints%252C%2520domain%250Ashifts%252C%2520heterogeneity%252C%2520and%2520adversarial%2520attacks.%2520Furthermore%252C%2520it%2520critically%250Aexamines%2520evaluation%2520methodologies%252C%2520highlighting%2520a%2520misalignment%2520between%2520current%250Ametrics%2520and%2520CP%2527s%2520fundamental%2520objectives.%2520By%2520delving%2520into%2520all%2520relevant%2520topics%250Ain-depth%252C%2520this%2520review%2520offers%2520valuable%2520insights%2520into%2520challenges%252C%2520opportunities%252C%250Aand%2520risks%252C%2520serving%2520as%2520a%2520reference%2520for%2520advancing%2520research%2520in%2520vehicular%250Acollaborative%2520perception.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.04631v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Systematic%20Literature%20Review%20on%20Vehicular%20Collaborative%20Perception%20-%20A%0A%20%20Computer%20Vision%20Perspective&entry.906535625=Lei%20Wan%20and%20Jianxin%20Zhao%20and%20Andreas%20Wiedholz%20and%20Manuel%20Bied%20and%20Mateus%20Martinez%20de%20Lucena%20and%20Abhishek%20Dinkar%20Jagtap%20and%20Andreas%20Festag%20and%20Ant%C3%B4nio%20Augusto%20Fr%C3%B6hlich%20and%20Hannan%20Ejaz%20Keen%20and%20Alexey%20Vinel&entry.1292438233=%20%20The%20effectiveness%20of%20autonomous%20vehicles%20relies%20on%20reliable%20perception%0Acapabilities.%20Despite%20significant%20advancements%20in%20artificial%20intelligence%20and%0Asensor%20fusion%20technologies%2C%20current%20single-vehicle%20perception%20systems%20continue%0Ato%20encounter%20limitations%2C%20notably%20visual%20occlusions%20and%20limited%20long-range%0Adetection%20capabilities.%20Collaborative%20Perception%20%28CP%29%2C%20enabled%20by%0AVehicle-to-Vehicle%20%28V2V%29%20and%20Vehicle-to-Infrastructure%20%28V2I%29%20communication%2C%20has%0Aemerged%20as%20a%20promising%20solution%20to%20mitigate%20these%20issues%20and%20enhance%20the%0Areliability%20of%20autonomous%20systems.%20Beyond%20advancements%20in%20communication%2C%20the%0Acomputer%20vision%20community%20is%20increasingly%20focusing%20on%20improving%20vehicular%0Aperception%20through%20collaborative%20approaches.%20However%2C%20a%20systematic%20literature%0Areview%20that%20thoroughly%20examines%20existing%20work%20and%20reduces%20subjective%20bias%20is%0Astill%20lacking.%20Such%20a%20systematic%20approach%20helps%20identify%20research%20gaps%2C%0Arecognize%20common%20trends%20across%20studies%2C%20and%20inform%20future%20research%20directions.%0AIn%20response%2C%20this%20study%20follows%20the%20PRISMA%202020%20guidelines%20and%20includes%20106%0Apeer-reviewed%20articles.%20These%20publications%20are%20analyzed%20based%20on%20modalities%2C%0Acollaboration%20schemes%2C%20and%20key%20perception%20tasks.%20Through%20a%20comparative%0Aanalysis%2C%20this%20review%20illustrates%20how%20different%20methods%20address%20practical%0Aissues%20such%20as%20pose%20errors%2C%20temporal%20latency%2C%20communication%20constraints%2C%20domain%0Ashifts%2C%20heterogeneity%2C%20and%20adversarial%20attacks.%20Furthermore%2C%20it%20critically%0Aexamines%20evaluation%20methodologies%2C%20highlighting%20a%20misalignment%20between%20current%0Ametrics%20and%20CP%27s%20fundamental%20objectives.%20By%20delving%20into%20all%20relevant%20topics%0Ain-depth%2C%20this%20review%20offers%20valuable%20insights%20into%20challenges%2C%20opportunities%2C%0Aand%20risks%2C%20serving%20as%20a%20reference%20for%20advancing%20research%20in%20vehicular%0Acollaborative%20perception.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.04631v2&entry.124074799=Read"},
{"title": "MVCustom: Multi-View Customized Diffusion via Geometric Latent Rendering\n  and Completion", "author": "Minjung Shin and Hyunin Cho and Sooyeon Go and Jin-Hwa Kim and Youngjung Uh", "abstract": "  Multi-view generation with camera pose control and prompt-based customization\nare both essential elements for achieving controllable generative models.\nHowever, existing multi-view generation models do not support customization\nwith geometric consistency, whereas customization models lack explicit\nviewpoint control, making them challenging to unify. Motivated by these gaps,\nwe introduce a novel task, multi-view customization, which aims to jointly\nachieve multi-view camera pose control and customization. Due to the scarcity\nof training data in customization, existing multi-view generation models, which\ninherently rely on large-scale datasets, struggle to generalize to diverse\nprompts. To address this, we propose MVCustom, a novel diffusion-based\nframework explicitly designed to achieve both multi-view consistency and\ncustomization fidelity. In the training stage, MVCustom learns the subject's\nidentity and geometry using a feature-field representation, incorporating the\ntext-to-video diffusion backbone enhanced with dense spatio-temporal attention,\nwhich leverages temporal coherence for multi-view consistency. In the inference\nstage, we introduce two novel techniques: depth-aware feature rendering\nexplicitly enforces geometric consistency, and consistent-aware latent\ncompletion ensures accurate perspective alignment of the customized subject and\nsurrounding backgrounds. Extensive experiments demonstrate that MVCustom is the\nonly framework that simultaneously achieves faithful multi-view generation and\ncustomization.\n", "link": "http://arxiv.org/abs/2510.13702v1", "date": "2025-10-15", "relevancy": 2.6438, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6959}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6649}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MVCustom%3A%20Multi-View%20Customized%20Diffusion%20via%20Geometric%20Latent%20Rendering%0A%20%20and%20Completion&body=Title%3A%20MVCustom%3A%20Multi-View%20Customized%20Diffusion%20via%20Geometric%20Latent%20Rendering%0A%20%20and%20Completion%0AAuthor%3A%20Minjung%20Shin%20and%20Hyunin%20Cho%20and%20Sooyeon%20Go%20and%20Jin-Hwa%20Kim%20and%20Youngjung%20Uh%0AAbstract%3A%20%20%20Multi-view%20generation%20with%20camera%20pose%20control%20and%20prompt-based%20customization%0Aare%20both%20essential%20elements%20for%20achieving%20controllable%20generative%20models.%0AHowever%2C%20existing%20multi-view%20generation%20models%20do%20not%20support%20customization%0Awith%20geometric%20consistency%2C%20whereas%20customization%20models%20lack%20explicit%0Aviewpoint%20control%2C%20making%20them%20challenging%20to%20unify.%20Motivated%20by%20these%20gaps%2C%0Awe%20introduce%20a%20novel%20task%2C%20multi-view%20customization%2C%20which%20aims%20to%20jointly%0Aachieve%20multi-view%20camera%20pose%20control%20and%20customization.%20Due%20to%20the%20scarcity%0Aof%20training%20data%20in%20customization%2C%20existing%20multi-view%20generation%20models%2C%20which%0Ainherently%20rely%20on%20large-scale%20datasets%2C%20struggle%20to%20generalize%20to%20diverse%0Aprompts.%20To%20address%20this%2C%20we%20propose%20MVCustom%2C%20a%20novel%20diffusion-based%0Aframework%20explicitly%20designed%20to%20achieve%20both%20multi-view%20consistency%20and%0Acustomization%20fidelity.%20In%20the%20training%20stage%2C%20MVCustom%20learns%20the%20subject%27s%0Aidentity%20and%20geometry%20using%20a%20feature-field%20representation%2C%20incorporating%20the%0Atext-to-video%20diffusion%20backbone%20enhanced%20with%20dense%20spatio-temporal%20attention%2C%0Awhich%20leverages%20temporal%20coherence%20for%20multi-view%20consistency.%20In%20the%20inference%0Astage%2C%20we%20introduce%20two%20novel%20techniques%3A%20depth-aware%20feature%20rendering%0Aexplicitly%20enforces%20geometric%20consistency%2C%20and%20consistent-aware%20latent%0Acompletion%20ensures%20accurate%20perspective%20alignment%20of%20the%20customized%20subject%20and%0Asurrounding%20backgrounds.%20Extensive%20experiments%20demonstrate%20that%20MVCustom%20is%20the%0Aonly%20framework%20that%20simultaneously%20achieves%20faithful%20multi-view%20generation%20and%0Acustomization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13702v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMVCustom%253A%2520Multi-View%2520Customized%2520Diffusion%2520via%2520Geometric%2520Latent%2520Rendering%250A%2520%2520and%2520Completion%26entry.906535625%3DMinjung%2520Shin%2520and%2520Hyunin%2520Cho%2520and%2520Sooyeon%2520Go%2520and%2520Jin-Hwa%2520Kim%2520and%2520Youngjung%2520Uh%26entry.1292438233%3D%2520%2520Multi-view%2520generation%2520with%2520camera%2520pose%2520control%2520and%2520prompt-based%2520customization%250Aare%2520both%2520essential%2520elements%2520for%2520achieving%2520controllable%2520generative%2520models.%250AHowever%252C%2520existing%2520multi-view%2520generation%2520models%2520do%2520not%2520support%2520customization%250Awith%2520geometric%2520consistency%252C%2520whereas%2520customization%2520models%2520lack%2520explicit%250Aviewpoint%2520control%252C%2520making%2520them%2520challenging%2520to%2520unify.%2520Motivated%2520by%2520these%2520gaps%252C%250Awe%2520introduce%2520a%2520novel%2520task%252C%2520multi-view%2520customization%252C%2520which%2520aims%2520to%2520jointly%250Aachieve%2520multi-view%2520camera%2520pose%2520control%2520and%2520customization.%2520Due%2520to%2520the%2520scarcity%250Aof%2520training%2520data%2520in%2520customization%252C%2520existing%2520multi-view%2520generation%2520models%252C%2520which%250Ainherently%2520rely%2520on%2520large-scale%2520datasets%252C%2520struggle%2520to%2520generalize%2520to%2520diverse%250Aprompts.%2520To%2520address%2520this%252C%2520we%2520propose%2520MVCustom%252C%2520a%2520novel%2520diffusion-based%250Aframework%2520explicitly%2520designed%2520to%2520achieve%2520both%2520multi-view%2520consistency%2520and%250Acustomization%2520fidelity.%2520In%2520the%2520training%2520stage%252C%2520MVCustom%2520learns%2520the%2520subject%2527s%250Aidentity%2520and%2520geometry%2520using%2520a%2520feature-field%2520representation%252C%2520incorporating%2520the%250Atext-to-video%2520diffusion%2520backbone%2520enhanced%2520with%2520dense%2520spatio-temporal%2520attention%252C%250Awhich%2520leverages%2520temporal%2520coherence%2520for%2520multi-view%2520consistency.%2520In%2520the%2520inference%250Astage%252C%2520we%2520introduce%2520two%2520novel%2520techniques%253A%2520depth-aware%2520feature%2520rendering%250Aexplicitly%2520enforces%2520geometric%2520consistency%252C%2520and%2520consistent-aware%2520latent%250Acompletion%2520ensures%2520accurate%2520perspective%2520alignment%2520of%2520the%2520customized%2520subject%2520and%250Asurrounding%2520backgrounds.%2520Extensive%2520experiments%2520demonstrate%2520that%2520MVCustom%2520is%2520the%250Aonly%2520framework%2520that%2520simultaneously%2520achieves%2520faithful%2520multi-view%2520generation%2520and%250Acustomization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13702v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVCustom%3A%20Multi-View%20Customized%20Diffusion%20via%20Geometric%20Latent%20Rendering%0A%20%20and%20Completion&entry.906535625=Minjung%20Shin%20and%20Hyunin%20Cho%20and%20Sooyeon%20Go%20and%20Jin-Hwa%20Kim%20and%20Youngjung%20Uh&entry.1292438233=%20%20Multi-view%20generation%20with%20camera%20pose%20control%20and%20prompt-based%20customization%0Aare%20both%20essential%20elements%20for%20achieving%20controllable%20generative%20models.%0AHowever%2C%20existing%20multi-view%20generation%20models%20do%20not%20support%20customization%0Awith%20geometric%20consistency%2C%20whereas%20customization%20models%20lack%20explicit%0Aviewpoint%20control%2C%20making%20them%20challenging%20to%20unify.%20Motivated%20by%20these%20gaps%2C%0Awe%20introduce%20a%20novel%20task%2C%20multi-view%20customization%2C%20which%20aims%20to%20jointly%0Aachieve%20multi-view%20camera%20pose%20control%20and%20customization.%20Due%20to%20the%20scarcity%0Aof%20training%20data%20in%20customization%2C%20existing%20multi-view%20generation%20models%2C%20which%0Ainherently%20rely%20on%20large-scale%20datasets%2C%20struggle%20to%20generalize%20to%20diverse%0Aprompts.%20To%20address%20this%2C%20we%20propose%20MVCustom%2C%20a%20novel%20diffusion-based%0Aframework%20explicitly%20designed%20to%20achieve%20both%20multi-view%20consistency%20and%0Acustomization%20fidelity.%20In%20the%20training%20stage%2C%20MVCustom%20learns%20the%20subject%27s%0Aidentity%20and%20geometry%20using%20a%20feature-field%20representation%2C%20incorporating%20the%0Atext-to-video%20diffusion%20backbone%20enhanced%20with%20dense%20spatio-temporal%20attention%2C%0Awhich%20leverages%20temporal%20coherence%20for%20multi-view%20consistency.%20In%20the%20inference%0Astage%2C%20we%20introduce%20two%20novel%20techniques%3A%20depth-aware%20feature%20rendering%0Aexplicitly%20enforces%20geometric%20consistency%2C%20and%20consistent-aware%20latent%0Acompletion%20ensures%20accurate%20perspective%20alignment%20of%20the%20customized%20subject%20and%0Asurrounding%20backgrounds.%20Extensive%20experiments%20demonstrate%20that%20MVCustom%20is%20the%0Aonly%20framework%20that%20simultaneously%20achieves%20faithful%20multi-view%20generation%20and%0Acustomization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13702v1&entry.124074799=Read"},
{"title": "Geo-R1: Improving Few-Shot Geospatial Referring Expression Understanding\n  with Reinforcement Fine-Tuning", "author": "Zilun Zhang and Zian Guan and Tiancheng Zhao and Haozhan Shen and Tianyu Li and Yuxiang Cai and Zhonggen Su and Zhaojun Liu and Jianwei Yin and Xiang Li", "abstract": "  Referring expression understanding in remote sensing poses unique challenges,\nas it requires reasoning over complex object-context relationships. While\nsupervised fine-tuning (SFT) on multimodal large language models achieves\nstrong performance with massive labeled datasets, they struggle in data-scarce\nscenarios, leading to poor generalization. To address this limitation, we\npropose Geo-R1, a reasoning-centric reinforcement fine-tuning (RFT) paradigm\nfor few-shot geospatial referring. Geo-R1 enforces the model to first generate\nexplicit, interpretable reasoning chains that decompose referring expressions,\nand then leverage these rationales to localize target objects. This \"reason\nfirst, then act\" process enables the model to make more effective use of\nlimited annotations, enhances generalization, and provides interpretability. We\nvalidate Geo-R1 on three carefully designed few-shot geospatial referring\nbenchmarks, where our model consistently and substantially outperforms SFT\nbaselines. It also demonstrates strong cross-dataset generalization,\nhighlighting its robustness. Code and data will be released at:\nhttps://github.com/Geo-R1/geo-r1.\n", "link": "http://arxiv.org/abs/2509.21976v2", "date": "2025-10-15", "relevancy": 2.62, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5245}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5245}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5229}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geo-R1%3A%20Improving%20Few-Shot%20Geospatial%20Referring%20Expression%20Understanding%0A%20%20with%20Reinforcement%20Fine-Tuning&body=Title%3A%20Geo-R1%3A%20Improving%20Few-Shot%20Geospatial%20Referring%20Expression%20Understanding%0A%20%20with%20Reinforcement%20Fine-Tuning%0AAuthor%3A%20Zilun%20Zhang%20and%20Zian%20Guan%20and%20Tiancheng%20Zhao%20and%20Haozhan%20Shen%20and%20Tianyu%20Li%20and%20Yuxiang%20Cai%20and%20Zhonggen%20Su%20and%20Zhaojun%20Liu%20and%20Jianwei%20Yin%20and%20Xiang%20Li%0AAbstract%3A%20%20%20Referring%20expression%20understanding%20in%20remote%20sensing%20poses%20unique%20challenges%2C%0Aas%20it%20requires%20reasoning%20over%20complex%20object-context%20relationships.%20While%0Asupervised%20fine-tuning%20%28SFT%29%20on%20multimodal%20large%20language%20models%20achieves%0Astrong%20performance%20with%20massive%20labeled%20datasets%2C%20they%20struggle%20in%20data-scarce%0Ascenarios%2C%20leading%20to%20poor%20generalization.%20To%20address%20this%20limitation%2C%20we%0Apropose%20Geo-R1%2C%20a%20reasoning-centric%20reinforcement%20fine-tuning%20%28RFT%29%20paradigm%0Afor%20few-shot%20geospatial%20referring.%20Geo-R1%20enforces%20the%20model%20to%20first%20generate%0Aexplicit%2C%20interpretable%20reasoning%20chains%20that%20decompose%20referring%20expressions%2C%0Aand%20then%20leverage%20these%20rationales%20to%20localize%20target%20objects.%20This%20%22reason%0Afirst%2C%20then%20act%22%20process%20enables%20the%20model%20to%20make%20more%20effective%20use%20of%0Alimited%20annotations%2C%20enhances%20generalization%2C%20and%20provides%20interpretability.%20We%0Avalidate%20Geo-R1%20on%20three%20carefully%20designed%20few-shot%20geospatial%20referring%0Abenchmarks%2C%20where%20our%20model%20consistently%20and%20substantially%20outperforms%20SFT%0Abaselines.%20It%20also%20demonstrates%20strong%20cross-dataset%20generalization%2C%0Ahighlighting%20its%20robustness.%20Code%20and%20data%20will%20be%20released%20at%3A%0Ahttps%3A//github.com/Geo-R1/geo-r1.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21976v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeo-R1%253A%2520Improving%2520Few-Shot%2520Geospatial%2520Referring%2520Expression%2520Understanding%250A%2520%2520with%2520Reinforcement%2520Fine-Tuning%26entry.906535625%3DZilun%2520Zhang%2520and%2520Zian%2520Guan%2520and%2520Tiancheng%2520Zhao%2520and%2520Haozhan%2520Shen%2520and%2520Tianyu%2520Li%2520and%2520Yuxiang%2520Cai%2520and%2520Zhonggen%2520Su%2520and%2520Zhaojun%2520Liu%2520and%2520Jianwei%2520Yin%2520and%2520Xiang%2520Li%26entry.1292438233%3D%2520%2520Referring%2520expression%2520understanding%2520in%2520remote%2520sensing%2520poses%2520unique%2520challenges%252C%250Aas%2520it%2520requires%2520reasoning%2520over%2520complex%2520object-context%2520relationships.%2520While%250Asupervised%2520fine-tuning%2520%2528SFT%2529%2520on%2520multimodal%2520large%2520language%2520models%2520achieves%250Astrong%2520performance%2520with%2520massive%2520labeled%2520datasets%252C%2520they%2520struggle%2520in%2520data-scarce%250Ascenarios%252C%2520leading%2520to%2520poor%2520generalization.%2520To%2520address%2520this%2520limitation%252C%2520we%250Apropose%2520Geo-R1%252C%2520a%2520reasoning-centric%2520reinforcement%2520fine-tuning%2520%2528RFT%2529%2520paradigm%250Afor%2520few-shot%2520geospatial%2520referring.%2520Geo-R1%2520enforces%2520the%2520model%2520to%2520first%2520generate%250Aexplicit%252C%2520interpretable%2520reasoning%2520chains%2520that%2520decompose%2520referring%2520expressions%252C%250Aand%2520then%2520leverage%2520these%2520rationales%2520to%2520localize%2520target%2520objects.%2520This%2520%2522reason%250Afirst%252C%2520then%2520act%2522%2520process%2520enables%2520the%2520model%2520to%2520make%2520more%2520effective%2520use%2520of%250Alimited%2520annotations%252C%2520enhances%2520generalization%252C%2520and%2520provides%2520interpretability.%2520We%250Avalidate%2520Geo-R1%2520on%2520three%2520carefully%2520designed%2520few-shot%2520geospatial%2520referring%250Abenchmarks%252C%2520where%2520our%2520model%2520consistently%2520and%2520substantially%2520outperforms%2520SFT%250Abaselines.%2520It%2520also%2520demonstrates%2520strong%2520cross-dataset%2520generalization%252C%250Ahighlighting%2520its%2520robustness.%2520Code%2520and%2520data%2520will%2520be%2520released%2520at%253A%250Ahttps%253A//github.com/Geo-R1/geo-r1.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21976v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geo-R1%3A%20Improving%20Few-Shot%20Geospatial%20Referring%20Expression%20Understanding%0A%20%20with%20Reinforcement%20Fine-Tuning&entry.906535625=Zilun%20Zhang%20and%20Zian%20Guan%20and%20Tiancheng%20Zhao%20and%20Haozhan%20Shen%20and%20Tianyu%20Li%20and%20Yuxiang%20Cai%20and%20Zhonggen%20Su%20and%20Zhaojun%20Liu%20and%20Jianwei%20Yin%20and%20Xiang%20Li&entry.1292438233=%20%20Referring%20expression%20understanding%20in%20remote%20sensing%20poses%20unique%20challenges%2C%0Aas%20it%20requires%20reasoning%20over%20complex%20object-context%20relationships.%20While%0Asupervised%20fine-tuning%20%28SFT%29%20on%20multimodal%20large%20language%20models%20achieves%0Astrong%20performance%20with%20massive%20labeled%20datasets%2C%20they%20struggle%20in%20data-scarce%0Ascenarios%2C%20leading%20to%20poor%20generalization.%20To%20address%20this%20limitation%2C%20we%0Apropose%20Geo-R1%2C%20a%20reasoning-centric%20reinforcement%20fine-tuning%20%28RFT%29%20paradigm%0Afor%20few-shot%20geospatial%20referring.%20Geo-R1%20enforces%20the%20model%20to%20first%20generate%0Aexplicit%2C%20interpretable%20reasoning%20chains%20that%20decompose%20referring%20expressions%2C%0Aand%20then%20leverage%20these%20rationales%20to%20localize%20target%20objects.%20This%20%22reason%0Afirst%2C%20then%20act%22%20process%20enables%20the%20model%20to%20make%20more%20effective%20use%20of%0Alimited%20annotations%2C%20enhances%20generalization%2C%20and%20provides%20interpretability.%20We%0Avalidate%20Geo-R1%20on%20three%20carefully%20designed%20few-shot%20geospatial%20referring%0Abenchmarks%2C%20where%20our%20model%20consistently%20and%20substantially%20outperforms%20SFT%0Abaselines.%20It%20also%20demonstrates%20strong%20cross-dataset%20generalization%2C%0Ahighlighting%20its%20robustness.%20Code%20and%20data%20will%20be%20released%20at%3A%0Ahttps%3A//github.com/Geo-R1/geo-r1.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21976v2&entry.124074799=Read"},
{"title": "MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition\n  Query", "author": "Wei Chow and Yuan Gao and Linfeng Li and Xian Wang and Qi Xu and Hang Song and Lingdong Kong and Ran Zhou and Yi Zeng and Yidong Cai and Botian Jiang and Shilin Xu and Jiajun Zhang and Minghui Qiu and Xiangtai Li and Tianshu Yang and Siliang Tang and Juncheng Li", "abstract": "  Semantic retrieval is crucial for modern applications yet remains\nunderexplored in current research. Existing datasets are limited to single\nlanguages, single images, or singular retrieval conditions, often failing to\nfully exploit the expressive capacity of visual information as evidenced by\nmaintained performance when images are replaced with captions. However,\npractical retrieval scenarios frequently involve interleaved multi-condition\nqueries with multiple images. Hence, this paper introduces MERIT, the first\nmultilingual dataset for interleaved multi-condition semantic retrieval,\ncomprising 320,000 queries with 135,000 products in 5 languages, covering 7\ndistinct product categories. Extensive experiments on MERIT identify existing\nmodels's limitation: focusing solely on global semantic information while\nneglecting specific conditional elements in queries. Consequently, we propose\nCoral, a novel fine-tuning framework that adapts pre-trained MLLMs by\nintegrating embedding reconstruction to preserve fine-grained conditional\nelements and contrastive learning to extract comprehensive global semantics.\nExperiments demonstrate that Coral achieves a 45.9% performance improvement\nover conventional approaches on MERIT, with strong generalization capabilities\nvalidated across 8 established retrieval benchmarks. Collectively, our\ncontributions - a novel dataset, identification of critical limitations in\nexisting approaches, and an innovative fine-tuning framework - establish a\nfoundation for future research in interleaved multi-condition semantic\nretrieval.\n", "link": "http://arxiv.org/abs/2506.03144v2", "date": "2025-10-15", "relevancy": 2.6141, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5345}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.517}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MERIT%3A%20Multilingual%20Semantic%20Retrieval%20with%20Interleaved%20Multi-Condition%0A%20%20Query&body=Title%3A%20MERIT%3A%20Multilingual%20Semantic%20Retrieval%20with%20Interleaved%20Multi-Condition%0A%20%20Query%0AAuthor%3A%20Wei%20Chow%20and%20Yuan%20Gao%20and%20Linfeng%20Li%20and%20Xian%20Wang%20and%20Qi%20Xu%20and%20Hang%20Song%20and%20Lingdong%20Kong%20and%20Ran%20Zhou%20and%20Yi%20Zeng%20and%20Yidong%20Cai%20and%20Botian%20Jiang%20and%20Shilin%20Xu%20and%20Jiajun%20Zhang%20and%20Minghui%20Qiu%20and%20Xiangtai%20Li%20and%20Tianshu%20Yang%20and%20Siliang%20Tang%20and%20Juncheng%20Li%0AAbstract%3A%20%20%20Semantic%20retrieval%20is%20crucial%20for%20modern%20applications%20yet%20remains%0Aunderexplored%20in%20current%20research.%20Existing%20datasets%20are%20limited%20to%20single%0Alanguages%2C%20single%20images%2C%20or%20singular%20retrieval%20conditions%2C%20often%20failing%20to%0Afully%20exploit%20the%20expressive%20capacity%20of%20visual%20information%20as%20evidenced%20by%0Amaintained%20performance%20when%20images%20are%20replaced%20with%20captions.%20However%2C%0Apractical%20retrieval%20scenarios%20frequently%20involve%20interleaved%20multi-condition%0Aqueries%20with%20multiple%20images.%20Hence%2C%20this%20paper%20introduces%20MERIT%2C%20the%20first%0Amultilingual%20dataset%20for%20interleaved%20multi-condition%20semantic%20retrieval%2C%0Acomprising%20320%2C000%20queries%20with%20135%2C000%20products%20in%205%20languages%2C%20covering%207%0Adistinct%20product%20categories.%20Extensive%20experiments%20on%20MERIT%20identify%20existing%0Amodels%27s%20limitation%3A%20focusing%20solely%20on%20global%20semantic%20information%20while%0Aneglecting%20specific%20conditional%20elements%20in%20queries.%20Consequently%2C%20we%20propose%0ACoral%2C%20a%20novel%20fine-tuning%20framework%20that%20adapts%20pre-trained%20MLLMs%20by%0Aintegrating%20embedding%20reconstruction%20to%20preserve%20fine-grained%20conditional%0Aelements%20and%20contrastive%20learning%20to%20extract%20comprehensive%20global%20semantics.%0AExperiments%20demonstrate%20that%20Coral%20achieves%20a%2045.9%25%20performance%20improvement%0Aover%20conventional%20approaches%20on%20MERIT%2C%20with%20strong%20generalization%20capabilities%0Avalidated%20across%208%20established%20retrieval%20benchmarks.%20Collectively%2C%20our%0Acontributions%20-%20a%20novel%20dataset%2C%20identification%20of%20critical%20limitations%20in%0Aexisting%20approaches%2C%20and%20an%20innovative%20fine-tuning%20framework%20-%20establish%20a%0Afoundation%20for%20future%20research%20in%20interleaved%20multi-condition%20semantic%0Aretrieval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03144v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMERIT%253A%2520Multilingual%2520Semantic%2520Retrieval%2520with%2520Interleaved%2520Multi-Condition%250A%2520%2520Query%26entry.906535625%3DWei%2520Chow%2520and%2520Yuan%2520Gao%2520and%2520Linfeng%2520Li%2520and%2520Xian%2520Wang%2520and%2520Qi%2520Xu%2520and%2520Hang%2520Song%2520and%2520Lingdong%2520Kong%2520and%2520Ran%2520Zhou%2520and%2520Yi%2520Zeng%2520and%2520Yidong%2520Cai%2520and%2520Botian%2520Jiang%2520and%2520Shilin%2520Xu%2520and%2520Jiajun%2520Zhang%2520and%2520Minghui%2520Qiu%2520and%2520Xiangtai%2520Li%2520and%2520Tianshu%2520Yang%2520and%2520Siliang%2520Tang%2520and%2520Juncheng%2520Li%26entry.1292438233%3D%2520%2520Semantic%2520retrieval%2520is%2520crucial%2520for%2520modern%2520applications%2520yet%2520remains%250Aunderexplored%2520in%2520current%2520research.%2520Existing%2520datasets%2520are%2520limited%2520to%2520single%250Alanguages%252C%2520single%2520images%252C%2520or%2520singular%2520retrieval%2520conditions%252C%2520often%2520failing%2520to%250Afully%2520exploit%2520the%2520expressive%2520capacity%2520of%2520visual%2520information%2520as%2520evidenced%2520by%250Amaintained%2520performance%2520when%2520images%2520are%2520replaced%2520with%2520captions.%2520However%252C%250Apractical%2520retrieval%2520scenarios%2520frequently%2520involve%2520interleaved%2520multi-condition%250Aqueries%2520with%2520multiple%2520images.%2520Hence%252C%2520this%2520paper%2520introduces%2520MERIT%252C%2520the%2520first%250Amultilingual%2520dataset%2520for%2520interleaved%2520multi-condition%2520semantic%2520retrieval%252C%250Acomprising%2520320%252C000%2520queries%2520with%2520135%252C000%2520products%2520in%25205%2520languages%252C%2520covering%25207%250Adistinct%2520product%2520categories.%2520Extensive%2520experiments%2520on%2520MERIT%2520identify%2520existing%250Amodels%2527s%2520limitation%253A%2520focusing%2520solely%2520on%2520global%2520semantic%2520information%2520while%250Aneglecting%2520specific%2520conditional%2520elements%2520in%2520queries.%2520Consequently%252C%2520we%2520propose%250ACoral%252C%2520a%2520novel%2520fine-tuning%2520framework%2520that%2520adapts%2520pre-trained%2520MLLMs%2520by%250Aintegrating%2520embedding%2520reconstruction%2520to%2520preserve%2520fine-grained%2520conditional%250Aelements%2520and%2520contrastive%2520learning%2520to%2520extract%2520comprehensive%2520global%2520semantics.%250AExperiments%2520demonstrate%2520that%2520Coral%2520achieves%2520a%252045.9%2525%2520performance%2520improvement%250Aover%2520conventional%2520approaches%2520on%2520MERIT%252C%2520with%2520strong%2520generalization%2520capabilities%250Avalidated%2520across%25208%2520established%2520retrieval%2520benchmarks.%2520Collectively%252C%2520our%250Acontributions%2520-%2520a%2520novel%2520dataset%252C%2520identification%2520of%2520critical%2520limitations%2520in%250Aexisting%2520approaches%252C%2520and%2520an%2520innovative%2520fine-tuning%2520framework%2520-%2520establish%2520a%250Afoundation%2520for%2520future%2520research%2520in%2520interleaved%2520multi-condition%2520semantic%250Aretrieval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03144v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MERIT%3A%20Multilingual%20Semantic%20Retrieval%20with%20Interleaved%20Multi-Condition%0A%20%20Query&entry.906535625=Wei%20Chow%20and%20Yuan%20Gao%20and%20Linfeng%20Li%20and%20Xian%20Wang%20and%20Qi%20Xu%20and%20Hang%20Song%20and%20Lingdong%20Kong%20and%20Ran%20Zhou%20and%20Yi%20Zeng%20and%20Yidong%20Cai%20and%20Botian%20Jiang%20and%20Shilin%20Xu%20and%20Jiajun%20Zhang%20and%20Minghui%20Qiu%20and%20Xiangtai%20Li%20and%20Tianshu%20Yang%20and%20Siliang%20Tang%20and%20Juncheng%20Li&entry.1292438233=%20%20Semantic%20retrieval%20is%20crucial%20for%20modern%20applications%20yet%20remains%0Aunderexplored%20in%20current%20research.%20Existing%20datasets%20are%20limited%20to%20single%0Alanguages%2C%20single%20images%2C%20or%20singular%20retrieval%20conditions%2C%20often%20failing%20to%0Afully%20exploit%20the%20expressive%20capacity%20of%20visual%20information%20as%20evidenced%20by%0Amaintained%20performance%20when%20images%20are%20replaced%20with%20captions.%20However%2C%0Apractical%20retrieval%20scenarios%20frequently%20involve%20interleaved%20multi-condition%0Aqueries%20with%20multiple%20images.%20Hence%2C%20this%20paper%20introduces%20MERIT%2C%20the%20first%0Amultilingual%20dataset%20for%20interleaved%20multi-condition%20semantic%20retrieval%2C%0Acomprising%20320%2C000%20queries%20with%20135%2C000%20products%20in%205%20languages%2C%20covering%207%0Adistinct%20product%20categories.%20Extensive%20experiments%20on%20MERIT%20identify%20existing%0Amodels%27s%20limitation%3A%20focusing%20solely%20on%20global%20semantic%20information%20while%0Aneglecting%20specific%20conditional%20elements%20in%20queries.%20Consequently%2C%20we%20propose%0ACoral%2C%20a%20novel%20fine-tuning%20framework%20that%20adapts%20pre-trained%20MLLMs%20by%0Aintegrating%20embedding%20reconstruction%20to%20preserve%20fine-grained%20conditional%0Aelements%20and%20contrastive%20learning%20to%20extract%20comprehensive%20global%20semantics.%0AExperiments%20demonstrate%20that%20Coral%20achieves%20a%2045.9%25%20performance%20improvement%0Aover%20conventional%20approaches%20on%20MERIT%2C%20with%20strong%20generalization%20capabilities%0Avalidated%20across%208%20established%20retrieval%20benchmarks.%20Collectively%2C%20our%0Acontributions%20-%20a%20novel%20dataset%2C%20identification%20of%20critical%20limitations%20in%0Aexisting%20approaches%2C%20and%20an%20innovative%20fine-tuning%20framework%20-%20establish%20a%0Afoundation%20for%20future%20research%20in%20interleaved%20multi-condition%20semantic%0Aretrieval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03144v2&entry.124074799=Read"},
{"title": "The Mechanistic Emergence of Symbol Grounding in Language Models", "author": "Shuyu Wu and Ziqiao Ma and Xiaoxi Luo and Yidong Huang and Josue Torres-Fonseca and Freda Shi and Joyce Chai", "abstract": "  Symbol grounding (Harnad, 1990) describes how symbols such as words acquire\ntheir meanings by connecting to real-world sensorimotor experiences. Recent\nwork has shown preliminary evidence that grounding may emerge in\n(vision-)language models trained at scale without using explicit grounding\nobjectives. Yet, the specific loci of this emergence and the mechanisms that\ndrive it remain largely unexplored. To address this problem, we introduce a\ncontrolled evaluation framework that systematically traces how symbol grounding\narises within the internal computations through mechanistic and causal\nanalysis. Our findings show that grounding concentrates in middle-layer\ncomputations and is implemented through the aggregate mechanism, where\nattention heads aggregate the environmental ground to support the prediction of\nlinguistic forms. This phenomenon replicates in multimodal dialogue and across\narchitectures (Transformers and state-space models), but not in unidirectional\nLSTMs. Our results provide behavioral and mechanistic evidence that symbol\ngrounding can emerge in language models, with practical implications for\npredicting and potentially controlling the reliability of generation.\n", "link": "http://arxiv.org/abs/2510.13796v1", "date": "2025-10-15", "relevancy": 2.6133, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5237}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5237}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Mechanistic%20Emergence%20of%20Symbol%20Grounding%20in%20Language%20Models&body=Title%3A%20The%20Mechanistic%20Emergence%20of%20Symbol%20Grounding%20in%20Language%20Models%0AAuthor%3A%20Shuyu%20Wu%20and%20Ziqiao%20Ma%20and%20Xiaoxi%20Luo%20and%20Yidong%20Huang%20and%20Josue%20Torres-Fonseca%20and%20Freda%20Shi%20and%20Joyce%20Chai%0AAbstract%3A%20%20%20Symbol%20grounding%20%28Harnad%2C%201990%29%20describes%20how%20symbols%20such%20as%20words%20acquire%0Atheir%20meanings%20by%20connecting%20to%20real-world%20sensorimotor%20experiences.%20Recent%0Awork%20has%20shown%20preliminary%20evidence%20that%20grounding%20may%20emerge%20in%0A%28vision-%29language%20models%20trained%20at%20scale%20without%20using%20explicit%20grounding%0Aobjectives.%20Yet%2C%20the%20specific%20loci%20of%20this%20emergence%20and%20the%20mechanisms%20that%0Adrive%20it%20remain%20largely%20unexplored.%20To%20address%20this%20problem%2C%20we%20introduce%20a%0Acontrolled%20evaluation%20framework%20that%20systematically%20traces%20how%20symbol%20grounding%0Aarises%20within%20the%20internal%20computations%20through%20mechanistic%20and%20causal%0Aanalysis.%20Our%20findings%20show%20that%20grounding%20concentrates%20in%20middle-layer%0Acomputations%20and%20is%20implemented%20through%20the%20aggregate%20mechanism%2C%20where%0Aattention%20heads%20aggregate%20the%20environmental%20ground%20to%20support%20the%20prediction%20of%0Alinguistic%20forms.%20This%20phenomenon%20replicates%20in%20multimodal%20dialogue%20and%20across%0Aarchitectures%20%28Transformers%20and%20state-space%20models%29%2C%20but%20not%20in%20unidirectional%0ALSTMs.%20Our%20results%20provide%20behavioral%20and%20mechanistic%20evidence%20that%20symbol%0Agrounding%20can%20emerge%20in%20language%20models%2C%20with%20practical%20implications%20for%0Apredicting%20and%20potentially%20controlling%20the%20reliability%20of%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13796v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Mechanistic%2520Emergence%2520of%2520Symbol%2520Grounding%2520in%2520Language%2520Models%26entry.906535625%3DShuyu%2520Wu%2520and%2520Ziqiao%2520Ma%2520and%2520Xiaoxi%2520Luo%2520and%2520Yidong%2520Huang%2520and%2520Josue%2520Torres-Fonseca%2520and%2520Freda%2520Shi%2520and%2520Joyce%2520Chai%26entry.1292438233%3D%2520%2520Symbol%2520grounding%2520%2528Harnad%252C%25201990%2529%2520describes%2520how%2520symbols%2520such%2520as%2520words%2520acquire%250Atheir%2520meanings%2520by%2520connecting%2520to%2520real-world%2520sensorimotor%2520experiences.%2520Recent%250Awork%2520has%2520shown%2520preliminary%2520evidence%2520that%2520grounding%2520may%2520emerge%2520in%250A%2528vision-%2529language%2520models%2520trained%2520at%2520scale%2520without%2520using%2520explicit%2520grounding%250Aobjectives.%2520Yet%252C%2520the%2520specific%2520loci%2520of%2520this%2520emergence%2520and%2520the%2520mechanisms%2520that%250Adrive%2520it%2520remain%2520largely%2520unexplored.%2520To%2520address%2520this%2520problem%252C%2520we%2520introduce%2520a%250Acontrolled%2520evaluation%2520framework%2520that%2520systematically%2520traces%2520how%2520symbol%2520grounding%250Aarises%2520within%2520the%2520internal%2520computations%2520through%2520mechanistic%2520and%2520causal%250Aanalysis.%2520Our%2520findings%2520show%2520that%2520grounding%2520concentrates%2520in%2520middle-layer%250Acomputations%2520and%2520is%2520implemented%2520through%2520the%2520aggregate%2520mechanism%252C%2520where%250Aattention%2520heads%2520aggregate%2520the%2520environmental%2520ground%2520to%2520support%2520the%2520prediction%2520of%250Alinguistic%2520forms.%2520This%2520phenomenon%2520replicates%2520in%2520multimodal%2520dialogue%2520and%2520across%250Aarchitectures%2520%2528Transformers%2520and%2520state-space%2520models%2529%252C%2520but%2520not%2520in%2520unidirectional%250ALSTMs.%2520Our%2520results%2520provide%2520behavioral%2520and%2520mechanistic%2520evidence%2520that%2520symbol%250Agrounding%2520can%2520emerge%2520in%2520language%2520models%252C%2520with%2520practical%2520implications%2520for%250Apredicting%2520and%2520potentially%2520controlling%2520the%2520reliability%2520of%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13796v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Mechanistic%20Emergence%20of%20Symbol%20Grounding%20in%20Language%20Models&entry.906535625=Shuyu%20Wu%20and%20Ziqiao%20Ma%20and%20Xiaoxi%20Luo%20and%20Yidong%20Huang%20and%20Josue%20Torres-Fonseca%20and%20Freda%20Shi%20and%20Joyce%20Chai&entry.1292438233=%20%20Symbol%20grounding%20%28Harnad%2C%201990%29%20describes%20how%20symbols%20such%20as%20words%20acquire%0Atheir%20meanings%20by%20connecting%20to%20real-world%20sensorimotor%20experiences.%20Recent%0Awork%20has%20shown%20preliminary%20evidence%20that%20grounding%20may%20emerge%20in%0A%28vision-%29language%20models%20trained%20at%20scale%20without%20using%20explicit%20grounding%0Aobjectives.%20Yet%2C%20the%20specific%20loci%20of%20this%20emergence%20and%20the%20mechanisms%20that%0Adrive%20it%20remain%20largely%20unexplored.%20To%20address%20this%20problem%2C%20we%20introduce%20a%0Acontrolled%20evaluation%20framework%20that%20systematically%20traces%20how%20symbol%20grounding%0Aarises%20within%20the%20internal%20computations%20through%20mechanistic%20and%20causal%0Aanalysis.%20Our%20findings%20show%20that%20grounding%20concentrates%20in%20middle-layer%0Acomputations%20and%20is%20implemented%20through%20the%20aggregate%20mechanism%2C%20where%0Aattention%20heads%20aggregate%20the%20environmental%20ground%20to%20support%20the%20prediction%20of%0Alinguistic%20forms.%20This%20phenomenon%20replicates%20in%20multimodal%20dialogue%20and%20across%0Aarchitectures%20%28Transformers%20and%20state-space%20models%29%2C%20but%20not%20in%20unidirectional%0ALSTMs.%20Our%20results%20provide%20behavioral%20and%20mechanistic%20evidence%20that%20symbol%0Agrounding%20can%20emerge%20in%20language%20models%2C%20with%20practical%20implications%20for%0Apredicting%20and%20potentially%20controlling%20the%20reliability%20of%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13796v1&entry.124074799=Read"},
{"title": "Multi-Scale High-Resolution Logarithmic Grapher Module for Efficient\n  Vision GNNs", "author": "Mustafa Munir and Alex Zhang and Radu Marculescu", "abstract": "  Vision graph neural networks (ViG) have demonstrated promise in vision tasks\nas a competitive alternative to conventional convolutional neural nets (CNN)\nand transformers (ViTs); however, common graph construction methods, such as\nk-nearest neighbor (KNN), can be expensive on larger images. While methods such\nas Sparse Vision Graph Attention (SVGA) have shown promise, SVGA's fixed step\nscale can lead to over-squashing and missing multiple connections to gain the\nsame information that could be gained from a long-range link. Through this\nobservation, we propose a new graph construction method, Logarithmic Scalable\nGraph Construction (LSGC) to enhance performance by limiting the number of\nlong-range links. To this end, we propose LogViG, a novel hybrid CNN-GNN model\nthat utilizes LSGC. Furthermore, inspired by the successes of multi-scale and\nhigh-resolution architectures, we introduce and apply a high-resolution branch\nand fuse features between our high-resolution and low-resolution branches for a\nmulti-scale high-resolution Vision GNN network. Extensive experiments show that\nLogViG beats existing ViG, CNN, and ViT architectures in terms of accuracy,\nGMACs, and parameters on image classification and semantic segmentation tasks.\nOur smallest model, Ti-LogViG, achieves an average top-1 accuracy on\nImageNet-1K of 79.9% with a standard deviation of 0.2%, 1.7% higher average\naccuracy than Vision GNN with a 24.3% reduction in parameters and 35.3%\nreduction in GMACs. Our work shows that leveraging long-range links in graph\nconstruction for ViGs through our proposed LSGC can exceed the performance of\ncurrent state-of-the-art ViGs. Code is available at\nhttps://github.com/mmunir127/LogViG-Official.\n", "link": "http://arxiv.org/abs/2510.13740v1", "date": "2025-10-15", "relevancy": 2.5852, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5277}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5175}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Scale%20High-Resolution%20Logarithmic%20Grapher%20Module%20for%20Efficient%0A%20%20Vision%20GNNs&body=Title%3A%20Multi-Scale%20High-Resolution%20Logarithmic%20Grapher%20Module%20for%20Efficient%0A%20%20Vision%20GNNs%0AAuthor%3A%20Mustafa%20Munir%20and%20Alex%20Zhang%20and%20Radu%20Marculescu%0AAbstract%3A%20%20%20Vision%20graph%20neural%20networks%20%28ViG%29%20have%20demonstrated%20promise%20in%20vision%20tasks%0Aas%20a%20competitive%20alternative%20to%20conventional%20convolutional%20neural%20nets%20%28CNN%29%0Aand%20transformers%20%28ViTs%29%3B%20however%2C%20common%20graph%20construction%20methods%2C%20such%20as%0Ak-nearest%20neighbor%20%28KNN%29%2C%20can%20be%20expensive%20on%20larger%20images.%20While%20methods%20such%0Aas%20Sparse%20Vision%20Graph%20Attention%20%28SVGA%29%20have%20shown%20promise%2C%20SVGA%27s%20fixed%20step%0Ascale%20can%20lead%20to%20over-squashing%20and%20missing%20multiple%20connections%20to%20gain%20the%0Asame%20information%20that%20could%20be%20gained%20from%20a%20long-range%20link.%20Through%20this%0Aobservation%2C%20we%20propose%20a%20new%20graph%20construction%20method%2C%20Logarithmic%20Scalable%0AGraph%20Construction%20%28LSGC%29%20to%20enhance%20performance%20by%20limiting%20the%20number%20of%0Along-range%20links.%20To%20this%20end%2C%20we%20propose%20LogViG%2C%20a%20novel%20hybrid%20CNN-GNN%20model%0Athat%20utilizes%20LSGC.%20Furthermore%2C%20inspired%20by%20the%20successes%20of%20multi-scale%20and%0Ahigh-resolution%20architectures%2C%20we%20introduce%20and%20apply%20a%20high-resolution%20branch%0Aand%20fuse%20features%20between%20our%20high-resolution%20and%20low-resolution%20branches%20for%20a%0Amulti-scale%20high-resolution%20Vision%20GNN%20network.%20Extensive%20experiments%20show%20that%0ALogViG%20beats%20existing%20ViG%2C%20CNN%2C%20and%20ViT%20architectures%20in%20terms%20of%20accuracy%2C%0AGMACs%2C%20and%20parameters%20on%20image%20classification%20and%20semantic%20segmentation%20tasks.%0AOur%20smallest%20model%2C%20Ti-LogViG%2C%20achieves%20an%20average%20top-1%20accuracy%20on%0AImageNet-1K%20of%2079.9%25%20with%20a%20standard%20deviation%20of%200.2%25%2C%201.7%25%20higher%20average%0Aaccuracy%20than%20Vision%20GNN%20with%20a%2024.3%25%20reduction%20in%20parameters%20and%2035.3%25%0Areduction%20in%20GMACs.%20Our%20work%20shows%20that%20leveraging%20long-range%20links%20in%20graph%0Aconstruction%20for%20ViGs%20through%20our%20proposed%20LSGC%20can%20exceed%20the%20performance%20of%0Acurrent%20state-of-the-art%20ViGs.%20Code%20is%20available%20at%0Ahttps%3A//github.com/mmunir127/LogViG-Official.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13740v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Scale%2520High-Resolution%2520Logarithmic%2520Grapher%2520Module%2520for%2520Efficient%250A%2520%2520Vision%2520GNNs%26entry.906535625%3DMustafa%2520Munir%2520and%2520Alex%2520Zhang%2520and%2520Radu%2520Marculescu%26entry.1292438233%3D%2520%2520Vision%2520graph%2520neural%2520networks%2520%2528ViG%2529%2520have%2520demonstrated%2520promise%2520in%2520vision%2520tasks%250Aas%2520a%2520competitive%2520alternative%2520to%2520conventional%2520convolutional%2520neural%2520nets%2520%2528CNN%2529%250Aand%2520transformers%2520%2528ViTs%2529%253B%2520however%252C%2520common%2520graph%2520construction%2520methods%252C%2520such%2520as%250Ak-nearest%2520neighbor%2520%2528KNN%2529%252C%2520can%2520be%2520expensive%2520on%2520larger%2520images.%2520While%2520methods%2520such%250Aas%2520Sparse%2520Vision%2520Graph%2520Attention%2520%2528SVGA%2529%2520have%2520shown%2520promise%252C%2520SVGA%2527s%2520fixed%2520step%250Ascale%2520can%2520lead%2520to%2520over-squashing%2520and%2520missing%2520multiple%2520connections%2520to%2520gain%2520the%250Asame%2520information%2520that%2520could%2520be%2520gained%2520from%2520a%2520long-range%2520link.%2520Through%2520this%250Aobservation%252C%2520we%2520propose%2520a%2520new%2520graph%2520construction%2520method%252C%2520Logarithmic%2520Scalable%250AGraph%2520Construction%2520%2528LSGC%2529%2520to%2520enhance%2520performance%2520by%2520limiting%2520the%2520number%2520of%250Along-range%2520links.%2520To%2520this%2520end%252C%2520we%2520propose%2520LogViG%252C%2520a%2520novel%2520hybrid%2520CNN-GNN%2520model%250Athat%2520utilizes%2520LSGC.%2520Furthermore%252C%2520inspired%2520by%2520the%2520successes%2520of%2520multi-scale%2520and%250Ahigh-resolution%2520architectures%252C%2520we%2520introduce%2520and%2520apply%2520a%2520high-resolution%2520branch%250Aand%2520fuse%2520features%2520between%2520our%2520high-resolution%2520and%2520low-resolution%2520branches%2520for%2520a%250Amulti-scale%2520high-resolution%2520Vision%2520GNN%2520network.%2520Extensive%2520experiments%2520show%2520that%250ALogViG%2520beats%2520existing%2520ViG%252C%2520CNN%252C%2520and%2520ViT%2520architectures%2520in%2520terms%2520of%2520accuracy%252C%250AGMACs%252C%2520and%2520parameters%2520on%2520image%2520classification%2520and%2520semantic%2520segmentation%2520tasks.%250AOur%2520smallest%2520model%252C%2520Ti-LogViG%252C%2520achieves%2520an%2520average%2520top-1%2520accuracy%2520on%250AImageNet-1K%2520of%252079.9%2525%2520with%2520a%2520standard%2520deviation%2520of%25200.2%2525%252C%25201.7%2525%2520higher%2520average%250Aaccuracy%2520than%2520Vision%2520GNN%2520with%2520a%252024.3%2525%2520reduction%2520in%2520parameters%2520and%252035.3%2525%250Areduction%2520in%2520GMACs.%2520Our%2520work%2520shows%2520that%2520leveraging%2520long-range%2520links%2520in%2520graph%250Aconstruction%2520for%2520ViGs%2520through%2520our%2520proposed%2520LSGC%2520can%2520exceed%2520the%2520performance%2520of%250Acurrent%2520state-of-the-art%2520ViGs.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/mmunir127/LogViG-Official.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13740v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Scale%20High-Resolution%20Logarithmic%20Grapher%20Module%20for%20Efficient%0A%20%20Vision%20GNNs&entry.906535625=Mustafa%20Munir%20and%20Alex%20Zhang%20and%20Radu%20Marculescu&entry.1292438233=%20%20Vision%20graph%20neural%20networks%20%28ViG%29%20have%20demonstrated%20promise%20in%20vision%20tasks%0Aas%20a%20competitive%20alternative%20to%20conventional%20convolutional%20neural%20nets%20%28CNN%29%0Aand%20transformers%20%28ViTs%29%3B%20however%2C%20common%20graph%20construction%20methods%2C%20such%20as%0Ak-nearest%20neighbor%20%28KNN%29%2C%20can%20be%20expensive%20on%20larger%20images.%20While%20methods%20such%0Aas%20Sparse%20Vision%20Graph%20Attention%20%28SVGA%29%20have%20shown%20promise%2C%20SVGA%27s%20fixed%20step%0Ascale%20can%20lead%20to%20over-squashing%20and%20missing%20multiple%20connections%20to%20gain%20the%0Asame%20information%20that%20could%20be%20gained%20from%20a%20long-range%20link.%20Through%20this%0Aobservation%2C%20we%20propose%20a%20new%20graph%20construction%20method%2C%20Logarithmic%20Scalable%0AGraph%20Construction%20%28LSGC%29%20to%20enhance%20performance%20by%20limiting%20the%20number%20of%0Along-range%20links.%20To%20this%20end%2C%20we%20propose%20LogViG%2C%20a%20novel%20hybrid%20CNN-GNN%20model%0Athat%20utilizes%20LSGC.%20Furthermore%2C%20inspired%20by%20the%20successes%20of%20multi-scale%20and%0Ahigh-resolution%20architectures%2C%20we%20introduce%20and%20apply%20a%20high-resolution%20branch%0Aand%20fuse%20features%20between%20our%20high-resolution%20and%20low-resolution%20branches%20for%20a%0Amulti-scale%20high-resolution%20Vision%20GNN%20network.%20Extensive%20experiments%20show%20that%0ALogViG%20beats%20existing%20ViG%2C%20CNN%2C%20and%20ViT%20architectures%20in%20terms%20of%20accuracy%2C%0AGMACs%2C%20and%20parameters%20on%20image%20classification%20and%20semantic%20segmentation%20tasks.%0AOur%20smallest%20model%2C%20Ti-LogViG%2C%20achieves%20an%20average%20top-1%20accuracy%20on%0AImageNet-1K%20of%2079.9%25%20with%20a%20standard%20deviation%20of%200.2%25%2C%201.7%25%20higher%20average%0Aaccuracy%20than%20Vision%20GNN%20with%20a%2024.3%25%20reduction%20in%20parameters%20and%2035.3%25%0Areduction%20in%20GMACs.%20Our%20work%20shows%20that%20leveraging%20long-range%20links%20in%20graph%0Aconstruction%20for%20ViGs%20through%20our%20proposed%20LSGC%20can%20exceed%20the%20performance%20of%0Acurrent%20state-of-the-art%20ViGs.%20Code%20is%20available%20at%0Ahttps%3A//github.com/mmunir127/LogViG-Official.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13740v1&entry.124074799=Read"},
{"title": "ProReason: Multi-Modal Proactive Reasoning with Decoupled Eyesight and\n  Wisdom", "author": "Jingqi Zhou and Sheng Wang and Jingwei Dong and Kai Liu and Lei Li and Jiahui Gao and Jiyue Jiang and Lingpeng Kong and Chuan Wu", "abstract": "  Large vision-language models (LVLMs) have witnessed significant progress on\nvisual understanding tasks. However, they often prioritize language knowledge\nover image information on visual reasoning tasks, incurring performance\ndegradation. To tackle this issue, we first identify the drawbacks of existing\nsolutions (i.e., limited multi-modal reasoning capacities, and insufficient and\nirrelevant visual descriptions). We then decompose visual reasoning process\ninto two stages: proactive visual perception (i.e., eyesight) and textual\nreasoning (i.e., wisdom), and introduce a novel visual reasoning framework\nnamed ProReason. This framework features decoupled vision-reasoning\ncapabilities and multi-run proactive perception. Briefly, given a multi-modal\nquestion, ProReason iterates proactive information collection and reasoning\nuntil the answer can be concluded with necessary and sufficient visual\ndescriptions. Notably, the disassociation of capabilities allows seamless\nintegration of existing large language models (LLMs) to compensate for the\nreasoning deficits of LVLMs. Our extensive experiments demonstrate that\nProReason outperforms existing multi-step reasoning frameworks on various\nbenchmarks for both open-source and closed-source models, with the average\nperformance gain reaching 13.2%. Besides, the integration of LLMs allows\nProReason to produce high-quality visual reasoning data, which empowers\nProReason-distilled models (i.e., ProReason-VL and ProReason-Q3) to achieve\nsuperior performance in downstream tasks. Our insights into existing solutions\nand the decoupled perspective for feasible integration of LLMs illuminate\nfuture research on visual reasoning techniques, especially LLM-assisted ones.\n", "link": "http://arxiv.org/abs/2410.14138v5", "date": "2025-10-15", "relevancy": 2.5648, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6561}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6561}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProReason%3A%20Multi-Modal%20Proactive%20Reasoning%20with%20Decoupled%20Eyesight%20and%0A%20%20Wisdom&body=Title%3A%20ProReason%3A%20Multi-Modal%20Proactive%20Reasoning%20with%20Decoupled%20Eyesight%20and%0A%20%20Wisdom%0AAuthor%3A%20Jingqi%20Zhou%20and%20Sheng%20Wang%20and%20Jingwei%20Dong%20and%20Kai%20Liu%20and%20Lei%20Li%20and%20Jiahui%20Gao%20and%20Jiyue%20Jiang%20and%20Lingpeng%20Kong%20and%20Chuan%20Wu%0AAbstract%3A%20%20%20Large%20vision-language%20models%20%28LVLMs%29%20have%20witnessed%20significant%20progress%20on%0Avisual%20understanding%20tasks.%20However%2C%20they%20often%20prioritize%20language%20knowledge%0Aover%20image%20information%20on%20visual%20reasoning%20tasks%2C%20incurring%20performance%0Adegradation.%20To%20tackle%20this%20issue%2C%20we%20first%20identify%20the%20drawbacks%20of%20existing%0Asolutions%20%28i.e.%2C%20limited%20multi-modal%20reasoning%20capacities%2C%20and%20insufficient%20and%0Airrelevant%20visual%20descriptions%29.%20We%20then%20decompose%20visual%20reasoning%20process%0Ainto%20two%20stages%3A%20proactive%20visual%20perception%20%28i.e.%2C%20eyesight%29%20and%20textual%0Areasoning%20%28i.e.%2C%20wisdom%29%2C%20and%20introduce%20a%20novel%20visual%20reasoning%20framework%0Anamed%20ProReason.%20This%20framework%20features%20decoupled%20vision-reasoning%0Acapabilities%20and%20multi-run%20proactive%20perception.%20Briefly%2C%20given%20a%20multi-modal%0Aquestion%2C%20ProReason%20iterates%20proactive%20information%20collection%20and%20reasoning%0Auntil%20the%20answer%20can%20be%20concluded%20with%20necessary%20and%20sufficient%20visual%0Adescriptions.%20Notably%2C%20the%20disassociation%20of%20capabilities%20allows%20seamless%0Aintegration%20of%20existing%20large%20language%20models%20%28LLMs%29%20to%20compensate%20for%20the%0Areasoning%20deficits%20of%20LVLMs.%20Our%20extensive%20experiments%20demonstrate%20that%0AProReason%20outperforms%20existing%20multi-step%20reasoning%20frameworks%20on%20various%0Abenchmarks%20for%20both%20open-source%20and%20closed-source%20models%2C%20with%20the%20average%0Aperformance%20gain%20reaching%2013.2%25.%20Besides%2C%20the%20integration%20of%20LLMs%20allows%0AProReason%20to%20produce%20high-quality%20visual%20reasoning%20data%2C%20which%20empowers%0AProReason-distilled%20models%20%28i.e.%2C%20ProReason-VL%20and%20ProReason-Q3%29%20to%20achieve%0Asuperior%20performance%20in%20downstream%20tasks.%20Our%20insights%20into%20existing%20solutions%0Aand%20the%20decoupled%20perspective%20for%20feasible%20integration%20of%20LLMs%20illuminate%0Afuture%20research%20on%20visual%20reasoning%20techniques%2C%20especially%20LLM-assisted%20ones.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14138v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProReason%253A%2520Multi-Modal%2520Proactive%2520Reasoning%2520with%2520Decoupled%2520Eyesight%2520and%250A%2520%2520Wisdom%26entry.906535625%3DJingqi%2520Zhou%2520and%2520Sheng%2520Wang%2520and%2520Jingwei%2520Dong%2520and%2520Kai%2520Liu%2520and%2520Lei%2520Li%2520and%2520Jiahui%2520Gao%2520and%2520Jiyue%2520Jiang%2520and%2520Lingpeng%2520Kong%2520and%2520Chuan%2520Wu%26entry.1292438233%3D%2520%2520Large%2520vision-language%2520models%2520%2528LVLMs%2529%2520have%2520witnessed%2520significant%2520progress%2520on%250Avisual%2520understanding%2520tasks.%2520However%252C%2520they%2520often%2520prioritize%2520language%2520knowledge%250Aover%2520image%2520information%2520on%2520visual%2520reasoning%2520tasks%252C%2520incurring%2520performance%250Adegradation.%2520To%2520tackle%2520this%2520issue%252C%2520we%2520first%2520identify%2520the%2520drawbacks%2520of%2520existing%250Asolutions%2520%2528i.e.%252C%2520limited%2520multi-modal%2520reasoning%2520capacities%252C%2520and%2520insufficient%2520and%250Airrelevant%2520visual%2520descriptions%2529.%2520We%2520then%2520decompose%2520visual%2520reasoning%2520process%250Ainto%2520two%2520stages%253A%2520proactive%2520visual%2520perception%2520%2528i.e.%252C%2520eyesight%2529%2520and%2520textual%250Areasoning%2520%2528i.e.%252C%2520wisdom%2529%252C%2520and%2520introduce%2520a%2520novel%2520visual%2520reasoning%2520framework%250Anamed%2520ProReason.%2520This%2520framework%2520features%2520decoupled%2520vision-reasoning%250Acapabilities%2520and%2520multi-run%2520proactive%2520perception.%2520Briefly%252C%2520given%2520a%2520multi-modal%250Aquestion%252C%2520ProReason%2520iterates%2520proactive%2520information%2520collection%2520and%2520reasoning%250Auntil%2520the%2520answer%2520can%2520be%2520concluded%2520with%2520necessary%2520and%2520sufficient%2520visual%250Adescriptions.%2520Notably%252C%2520the%2520disassociation%2520of%2520capabilities%2520allows%2520seamless%250Aintegration%2520of%2520existing%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520compensate%2520for%2520the%250Areasoning%2520deficits%2520of%2520LVLMs.%2520Our%2520extensive%2520experiments%2520demonstrate%2520that%250AProReason%2520outperforms%2520existing%2520multi-step%2520reasoning%2520frameworks%2520on%2520various%250Abenchmarks%2520for%2520both%2520open-source%2520and%2520closed-source%2520models%252C%2520with%2520the%2520average%250Aperformance%2520gain%2520reaching%252013.2%2525.%2520Besides%252C%2520the%2520integration%2520of%2520LLMs%2520allows%250AProReason%2520to%2520produce%2520high-quality%2520visual%2520reasoning%2520data%252C%2520which%2520empowers%250AProReason-distilled%2520models%2520%2528i.e.%252C%2520ProReason-VL%2520and%2520ProReason-Q3%2529%2520to%2520achieve%250Asuperior%2520performance%2520in%2520downstream%2520tasks.%2520Our%2520insights%2520into%2520existing%2520solutions%250Aand%2520the%2520decoupled%2520perspective%2520for%2520feasible%2520integration%2520of%2520LLMs%2520illuminate%250Afuture%2520research%2520on%2520visual%2520reasoning%2520techniques%252C%2520especially%2520LLM-assisted%2520ones.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14138v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProReason%3A%20Multi-Modal%20Proactive%20Reasoning%20with%20Decoupled%20Eyesight%20and%0A%20%20Wisdom&entry.906535625=Jingqi%20Zhou%20and%20Sheng%20Wang%20and%20Jingwei%20Dong%20and%20Kai%20Liu%20and%20Lei%20Li%20and%20Jiahui%20Gao%20and%20Jiyue%20Jiang%20and%20Lingpeng%20Kong%20and%20Chuan%20Wu&entry.1292438233=%20%20Large%20vision-language%20models%20%28LVLMs%29%20have%20witnessed%20significant%20progress%20on%0Avisual%20understanding%20tasks.%20However%2C%20they%20often%20prioritize%20language%20knowledge%0Aover%20image%20information%20on%20visual%20reasoning%20tasks%2C%20incurring%20performance%0Adegradation.%20To%20tackle%20this%20issue%2C%20we%20first%20identify%20the%20drawbacks%20of%20existing%0Asolutions%20%28i.e.%2C%20limited%20multi-modal%20reasoning%20capacities%2C%20and%20insufficient%20and%0Airrelevant%20visual%20descriptions%29.%20We%20then%20decompose%20visual%20reasoning%20process%0Ainto%20two%20stages%3A%20proactive%20visual%20perception%20%28i.e.%2C%20eyesight%29%20and%20textual%0Areasoning%20%28i.e.%2C%20wisdom%29%2C%20and%20introduce%20a%20novel%20visual%20reasoning%20framework%0Anamed%20ProReason.%20This%20framework%20features%20decoupled%20vision-reasoning%0Acapabilities%20and%20multi-run%20proactive%20perception.%20Briefly%2C%20given%20a%20multi-modal%0Aquestion%2C%20ProReason%20iterates%20proactive%20information%20collection%20and%20reasoning%0Auntil%20the%20answer%20can%20be%20concluded%20with%20necessary%20and%20sufficient%20visual%0Adescriptions.%20Notably%2C%20the%20disassociation%20of%20capabilities%20allows%20seamless%0Aintegration%20of%20existing%20large%20language%20models%20%28LLMs%29%20to%20compensate%20for%20the%0Areasoning%20deficits%20of%20LVLMs.%20Our%20extensive%20experiments%20demonstrate%20that%0AProReason%20outperforms%20existing%20multi-step%20reasoning%20frameworks%20on%20various%0Abenchmarks%20for%20both%20open-source%20and%20closed-source%20models%2C%20with%20the%20average%0Aperformance%20gain%20reaching%2013.2%25.%20Besides%2C%20the%20integration%20of%20LLMs%20allows%0AProReason%20to%20produce%20high-quality%20visual%20reasoning%20data%2C%20which%20empowers%0AProReason-distilled%20models%20%28i.e.%2C%20ProReason-VL%20and%20ProReason-Q3%29%20to%20achieve%0Asuperior%20performance%20in%20downstream%20tasks.%20Our%20insights%20into%20existing%20solutions%0Aand%20the%20decoupled%20perspective%20for%20feasible%20integration%20of%20LLMs%20illuminate%0Afuture%20research%20on%20visual%20reasoning%20techniques%2C%20especially%20LLM-assisted%20ones.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14138v5&entry.124074799=Read"},
{"title": "Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully\n  Open MLLMs", "author": "Yi Zhang and Bolin Ni and Xin-Sheng Chen and Heng-Rui Zhang and Yongming Rao and Houwen Peng and Qinglin Lu and Han Hu and Meng-Hao Guo and Shi-Min Hu", "abstract": "  Fully open multimodal large language models (MLLMs) currently lag behind\nproprietary counterparts, primarily due to a significant gap in data quality\nfor supervised fine-tuning (SFT). Existing open-source datasets are often\nplagued by widespread noise and a critical deficit in complex reasoning data,\nsuch as Chain-of-Thought (CoT), which hinders the development of advanced model\ncapabilities. Addressing these challenges, our work makes three primary\ncontributions. First, we introduce Honey-Data-15M, a new SFT dataset comprising\napproximately 15 million QA pairs, processed through multiple cleaning\ntechniques and enhanced with a novel dual-level (short and long) CoT enrichment\nstrategy. Second, we introduce HoneyPipe, the data curation pipeline, and its\nunderlying framework DataStudio, providing the community with a transparent and\nadaptable methodology for data curation that moves beyond static dataset\nreleases. Finally, to validate our dataset and pipeline, we train Bee-8B, an 8B\nmodel on Honey-Data-15M. Experiments show that Bee-8B establishes a new\nstate-of-the-art (SOTA) for fully open MLLMs, achieving performance that is\ncompetitive with, and in some cases surpasses, recent semi-open models such as\nInternVL3.5-8B. Our work delivers to the community a suite of foundational\nresources, including: the Honey-Data-15M corpus; the full-stack suite\ncomprising HoneyPipe and DataStudio; training recipes; an evaluation harness;\nand the model weights. This effort demonstrates that a principled focus on data\nquality is a key pathway to developing fully open MLLMs that are highly\ncompetitive with their semi-open counterparts.\n", "link": "http://arxiv.org/abs/2510.13795v1", "date": "2025-10-15", "relevancy": 2.5543, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.511}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.511}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5107}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bee%3A%20A%20High-Quality%20Corpus%20and%20Full-Stack%20Suite%20to%20Unlock%20Advanced%20Fully%0A%20%20Open%20MLLMs&body=Title%3A%20Bee%3A%20A%20High-Quality%20Corpus%20and%20Full-Stack%20Suite%20to%20Unlock%20Advanced%20Fully%0A%20%20Open%20MLLMs%0AAuthor%3A%20Yi%20Zhang%20and%20Bolin%20Ni%20and%20Xin-Sheng%20Chen%20and%20Heng-Rui%20Zhang%20and%20Yongming%20Rao%20and%20Houwen%20Peng%20and%20Qinglin%20Lu%20and%20Han%20Hu%20and%20Meng-Hao%20Guo%20and%20Shi-Min%20Hu%0AAbstract%3A%20%20%20Fully%20open%20multimodal%20large%20language%20models%20%28MLLMs%29%20currently%20lag%20behind%0Aproprietary%20counterparts%2C%20primarily%20due%20to%20a%20significant%20gap%20in%20data%20quality%0Afor%20supervised%20fine-tuning%20%28SFT%29.%20Existing%20open-source%20datasets%20are%20often%0Aplagued%20by%20widespread%20noise%20and%20a%20critical%20deficit%20in%20complex%20reasoning%20data%2C%0Asuch%20as%20Chain-of-Thought%20%28CoT%29%2C%20which%20hinders%20the%20development%20of%20advanced%20model%0Acapabilities.%20Addressing%20these%20challenges%2C%20our%20work%20makes%20three%20primary%0Acontributions.%20First%2C%20we%20introduce%20Honey-Data-15M%2C%20a%20new%20SFT%20dataset%20comprising%0Aapproximately%2015%20million%20QA%20pairs%2C%20processed%20through%20multiple%20cleaning%0Atechniques%20and%20enhanced%20with%20a%20novel%20dual-level%20%28short%20and%20long%29%20CoT%20enrichment%0Astrategy.%20Second%2C%20we%20introduce%20HoneyPipe%2C%20the%20data%20curation%20pipeline%2C%20and%20its%0Aunderlying%20framework%20DataStudio%2C%20providing%20the%20community%20with%20a%20transparent%20and%0Aadaptable%20methodology%20for%20data%20curation%20that%20moves%20beyond%20static%20dataset%0Areleases.%20Finally%2C%20to%20validate%20our%20dataset%20and%20pipeline%2C%20we%20train%20Bee-8B%2C%20an%208B%0Amodel%20on%20Honey-Data-15M.%20Experiments%20show%20that%20Bee-8B%20establishes%20a%20new%0Astate-of-the-art%20%28SOTA%29%20for%20fully%20open%20MLLMs%2C%20achieving%20performance%20that%20is%0Acompetitive%20with%2C%20and%20in%20some%20cases%20surpasses%2C%20recent%20semi-open%20models%20such%20as%0AInternVL3.5-8B.%20Our%20work%20delivers%20to%20the%20community%20a%20suite%20of%20foundational%0Aresources%2C%20including%3A%20the%20Honey-Data-15M%20corpus%3B%20the%20full-stack%20suite%0Acomprising%20HoneyPipe%20and%20DataStudio%3B%20training%20recipes%3B%20an%20evaluation%20harness%3B%0Aand%20the%20model%20weights.%20This%20effort%20demonstrates%20that%20a%20principled%20focus%20on%20data%0Aquality%20is%20a%20key%20pathway%20to%20developing%20fully%20open%20MLLMs%20that%20are%20highly%0Acompetitive%20with%20their%20semi-open%20counterparts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13795v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBee%253A%2520A%2520High-Quality%2520Corpus%2520and%2520Full-Stack%2520Suite%2520to%2520Unlock%2520Advanced%2520Fully%250A%2520%2520Open%2520MLLMs%26entry.906535625%3DYi%2520Zhang%2520and%2520Bolin%2520Ni%2520and%2520Xin-Sheng%2520Chen%2520and%2520Heng-Rui%2520Zhang%2520and%2520Yongming%2520Rao%2520and%2520Houwen%2520Peng%2520and%2520Qinglin%2520Lu%2520and%2520Han%2520Hu%2520and%2520Meng-Hao%2520Guo%2520and%2520Shi-Min%2520Hu%26entry.1292438233%3D%2520%2520Fully%2520open%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520currently%2520lag%2520behind%250Aproprietary%2520counterparts%252C%2520primarily%2520due%2520to%2520a%2520significant%2520gap%2520in%2520data%2520quality%250Afor%2520supervised%2520fine-tuning%2520%2528SFT%2529.%2520Existing%2520open-source%2520datasets%2520are%2520often%250Aplagued%2520by%2520widespread%2520noise%2520and%2520a%2520critical%2520deficit%2520in%2520complex%2520reasoning%2520data%252C%250Asuch%2520as%2520Chain-of-Thought%2520%2528CoT%2529%252C%2520which%2520hinders%2520the%2520development%2520of%2520advanced%2520model%250Acapabilities.%2520Addressing%2520these%2520challenges%252C%2520our%2520work%2520makes%2520three%2520primary%250Acontributions.%2520First%252C%2520we%2520introduce%2520Honey-Data-15M%252C%2520a%2520new%2520SFT%2520dataset%2520comprising%250Aapproximately%252015%2520million%2520QA%2520pairs%252C%2520processed%2520through%2520multiple%2520cleaning%250Atechniques%2520and%2520enhanced%2520with%2520a%2520novel%2520dual-level%2520%2528short%2520and%2520long%2529%2520CoT%2520enrichment%250Astrategy.%2520Second%252C%2520we%2520introduce%2520HoneyPipe%252C%2520the%2520data%2520curation%2520pipeline%252C%2520and%2520its%250Aunderlying%2520framework%2520DataStudio%252C%2520providing%2520the%2520community%2520with%2520a%2520transparent%2520and%250Aadaptable%2520methodology%2520for%2520data%2520curation%2520that%2520moves%2520beyond%2520static%2520dataset%250Areleases.%2520Finally%252C%2520to%2520validate%2520our%2520dataset%2520and%2520pipeline%252C%2520we%2520train%2520Bee-8B%252C%2520an%25208B%250Amodel%2520on%2520Honey-Data-15M.%2520Experiments%2520show%2520that%2520Bee-8B%2520establishes%2520a%2520new%250Astate-of-the-art%2520%2528SOTA%2529%2520for%2520fully%2520open%2520MLLMs%252C%2520achieving%2520performance%2520that%2520is%250Acompetitive%2520with%252C%2520and%2520in%2520some%2520cases%2520surpasses%252C%2520recent%2520semi-open%2520models%2520such%2520as%250AInternVL3.5-8B.%2520Our%2520work%2520delivers%2520to%2520the%2520community%2520a%2520suite%2520of%2520foundational%250Aresources%252C%2520including%253A%2520the%2520Honey-Data-15M%2520corpus%253B%2520the%2520full-stack%2520suite%250Acomprising%2520HoneyPipe%2520and%2520DataStudio%253B%2520training%2520recipes%253B%2520an%2520evaluation%2520harness%253B%250Aand%2520the%2520model%2520weights.%2520This%2520effort%2520demonstrates%2520that%2520a%2520principled%2520focus%2520on%2520data%250Aquality%2520is%2520a%2520key%2520pathway%2520to%2520developing%2520fully%2520open%2520MLLMs%2520that%2520are%2520highly%250Acompetitive%2520with%2520their%2520semi-open%2520counterparts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13795v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bee%3A%20A%20High-Quality%20Corpus%20and%20Full-Stack%20Suite%20to%20Unlock%20Advanced%20Fully%0A%20%20Open%20MLLMs&entry.906535625=Yi%20Zhang%20and%20Bolin%20Ni%20and%20Xin-Sheng%20Chen%20and%20Heng-Rui%20Zhang%20and%20Yongming%20Rao%20and%20Houwen%20Peng%20and%20Qinglin%20Lu%20and%20Han%20Hu%20and%20Meng-Hao%20Guo%20and%20Shi-Min%20Hu&entry.1292438233=%20%20Fully%20open%20multimodal%20large%20language%20models%20%28MLLMs%29%20currently%20lag%20behind%0Aproprietary%20counterparts%2C%20primarily%20due%20to%20a%20significant%20gap%20in%20data%20quality%0Afor%20supervised%20fine-tuning%20%28SFT%29.%20Existing%20open-source%20datasets%20are%20often%0Aplagued%20by%20widespread%20noise%20and%20a%20critical%20deficit%20in%20complex%20reasoning%20data%2C%0Asuch%20as%20Chain-of-Thought%20%28CoT%29%2C%20which%20hinders%20the%20development%20of%20advanced%20model%0Acapabilities.%20Addressing%20these%20challenges%2C%20our%20work%20makes%20three%20primary%0Acontributions.%20First%2C%20we%20introduce%20Honey-Data-15M%2C%20a%20new%20SFT%20dataset%20comprising%0Aapproximately%2015%20million%20QA%20pairs%2C%20processed%20through%20multiple%20cleaning%0Atechniques%20and%20enhanced%20with%20a%20novel%20dual-level%20%28short%20and%20long%29%20CoT%20enrichment%0Astrategy.%20Second%2C%20we%20introduce%20HoneyPipe%2C%20the%20data%20curation%20pipeline%2C%20and%20its%0Aunderlying%20framework%20DataStudio%2C%20providing%20the%20community%20with%20a%20transparent%20and%0Aadaptable%20methodology%20for%20data%20curation%20that%20moves%20beyond%20static%20dataset%0Areleases.%20Finally%2C%20to%20validate%20our%20dataset%20and%20pipeline%2C%20we%20train%20Bee-8B%2C%20an%208B%0Amodel%20on%20Honey-Data-15M.%20Experiments%20show%20that%20Bee-8B%20establishes%20a%20new%0Astate-of-the-art%20%28SOTA%29%20for%20fully%20open%20MLLMs%2C%20achieving%20performance%20that%20is%0Acompetitive%20with%2C%20and%20in%20some%20cases%20surpasses%2C%20recent%20semi-open%20models%20such%20as%0AInternVL3.5-8B.%20Our%20work%20delivers%20to%20the%20community%20a%20suite%20of%20foundational%0Aresources%2C%20including%3A%20the%20Honey-Data-15M%20corpus%3B%20the%20full-stack%20suite%0Acomprising%20HoneyPipe%20and%20DataStudio%3B%20training%20recipes%3B%20an%20evaluation%20harness%3B%0Aand%20the%20model%20weights.%20This%20effort%20demonstrates%20that%20a%20principled%20focus%20on%20data%0Aquality%20is%20a%20key%20pathway%20to%20developing%20fully%20open%20MLLMs%20that%20are%20highly%0Acompetitive%20with%20their%20semi-open%20counterparts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13795v1&entry.124074799=Read"},
{"title": "End-to-End Semantic Preservation in Text-Aware Image Compression Systems", "author": "Stefano Della Fiore and Alessandro Gnutti and Marco Dalai and Pierangelo Migliorati and Riccardo Leonardi", "abstract": "  Traditional image compression methods aim to reconstruct images for human\nperception, prioritizing visual fidelity over task relevance. In contrast,\nCoding for Machines focuses on preserving information essential for automated\nunderstanding. Building on this principle, we present an end-to-end compression\nframework that retains text-specific features for Optical Character Recognition\n(OCR). The encoder operates at roughly half the computational cost of the OCR\nmodule, making it suitable for resource-limited devices. When on-device OCR is\ninfeasible, images can be efficiently compressed and later decoded to recover\ntextual content. Experiments show significant improvements in text extraction\naccuracy at low bitrates, even outperforming OCR on uncompressed images.\n  We further extend this study to general-purpose encoders, exploring their\ncapacity to preserve hidden semantics under extreme compression. Instead of\noptimizing for visual fidelity, we examine whether compact, visually degraded\nrepresentations can retain recoverable meaning through learned enhancement and\nrecognition modules. Results demonstrate that semantic information can persist\ndespite severe compression, bridging text-oriented compression and\ngeneral-purpose semantic preservation in machine-centered image coding.\n", "link": "http://arxiv.org/abs/2503.19495v2", "date": "2025-10-15", "relevancy": 2.5422, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5144}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5144}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4965}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20End-to-End%20Semantic%20Preservation%20in%20Text-Aware%20Image%20Compression%20Systems&body=Title%3A%20End-to-End%20Semantic%20Preservation%20in%20Text-Aware%20Image%20Compression%20Systems%0AAuthor%3A%20Stefano%20Della%20Fiore%20and%20Alessandro%20Gnutti%20and%20Marco%20Dalai%20and%20Pierangelo%20Migliorati%20and%20Riccardo%20Leonardi%0AAbstract%3A%20%20%20Traditional%20image%20compression%20methods%20aim%20to%20reconstruct%20images%20for%20human%0Aperception%2C%20prioritizing%20visual%20fidelity%20over%20task%20relevance.%20In%20contrast%2C%0ACoding%20for%20Machines%20focuses%20on%20preserving%20information%20essential%20for%20automated%0Aunderstanding.%20Building%20on%20this%20principle%2C%20we%20present%20an%20end-to-end%20compression%0Aframework%20that%20retains%20text-specific%20features%20for%20Optical%20Character%20Recognition%0A%28OCR%29.%20The%20encoder%20operates%20at%20roughly%20half%20the%20computational%20cost%20of%20the%20OCR%0Amodule%2C%20making%20it%20suitable%20for%20resource-limited%20devices.%20When%20on-device%20OCR%20is%0Ainfeasible%2C%20images%20can%20be%20efficiently%20compressed%20and%20later%20decoded%20to%20recover%0Atextual%20content.%20Experiments%20show%20significant%20improvements%20in%20text%20extraction%0Aaccuracy%20at%20low%20bitrates%2C%20even%20outperforming%20OCR%20on%20uncompressed%20images.%0A%20%20We%20further%20extend%20this%20study%20to%20general-purpose%20encoders%2C%20exploring%20their%0Acapacity%20to%20preserve%20hidden%20semantics%20under%20extreme%20compression.%20Instead%20of%0Aoptimizing%20for%20visual%20fidelity%2C%20we%20examine%20whether%20compact%2C%20visually%20degraded%0Arepresentations%20can%20retain%20recoverable%20meaning%20through%20learned%20enhancement%20and%0Arecognition%20modules.%20Results%20demonstrate%20that%20semantic%20information%20can%20persist%0Adespite%20severe%20compression%2C%20bridging%20text-oriented%20compression%20and%0Ageneral-purpose%20semantic%20preservation%20in%20machine-centered%20image%20coding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.19495v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnd-to-End%2520Semantic%2520Preservation%2520in%2520Text-Aware%2520Image%2520Compression%2520Systems%26entry.906535625%3DStefano%2520Della%2520Fiore%2520and%2520Alessandro%2520Gnutti%2520and%2520Marco%2520Dalai%2520and%2520Pierangelo%2520Migliorati%2520and%2520Riccardo%2520Leonardi%26entry.1292438233%3D%2520%2520Traditional%2520image%2520compression%2520methods%2520aim%2520to%2520reconstruct%2520images%2520for%2520human%250Aperception%252C%2520prioritizing%2520visual%2520fidelity%2520over%2520task%2520relevance.%2520In%2520contrast%252C%250ACoding%2520for%2520Machines%2520focuses%2520on%2520preserving%2520information%2520essential%2520for%2520automated%250Aunderstanding.%2520Building%2520on%2520this%2520principle%252C%2520we%2520present%2520an%2520end-to-end%2520compression%250Aframework%2520that%2520retains%2520text-specific%2520features%2520for%2520Optical%2520Character%2520Recognition%250A%2528OCR%2529.%2520The%2520encoder%2520operates%2520at%2520roughly%2520half%2520the%2520computational%2520cost%2520of%2520the%2520OCR%250Amodule%252C%2520making%2520it%2520suitable%2520for%2520resource-limited%2520devices.%2520When%2520on-device%2520OCR%2520is%250Ainfeasible%252C%2520images%2520can%2520be%2520efficiently%2520compressed%2520and%2520later%2520decoded%2520to%2520recover%250Atextual%2520content.%2520Experiments%2520show%2520significant%2520improvements%2520in%2520text%2520extraction%250Aaccuracy%2520at%2520low%2520bitrates%252C%2520even%2520outperforming%2520OCR%2520on%2520uncompressed%2520images.%250A%2520%2520We%2520further%2520extend%2520this%2520study%2520to%2520general-purpose%2520encoders%252C%2520exploring%2520their%250Acapacity%2520to%2520preserve%2520hidden%2520semantics%2520under%2520extreme%2520compression.%2520Instead%2520of%250Aoptimizing%2520for%2520visual%2520fidelity%252C%2520we%2520examine%2520whether%2520compact%252C%2520visually%2520degraded%250Arepresentations%2520can%2520retain%2520recoverable%2520meaning%2520through%2520learned%2520enhancement%2520and%250Arecognition%2520modules.%2520Results%2520demonstrate%2520that%2520semantic%2520information%2520can%2520persist%250Adespite%2520severe%2520compression%252C%2520bridging%2520text-oriented%2520compression%2520and%250Ageneral-purpose%2520semantic%2520preservation%2520in%2520machine-centered%2520image%2520coding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.19495v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=End-to-End%20Semantic%20Preservation%20in%20Text-Aware%20Image%20Compression%20Systems&entry.906535625=Stefano%20Della%20Fiore%20and%20Alessandro%20Gnutti%20and%20Marco%20Dalai%20and%20Pierangelo%20Migliorati%20and%20Riccardo%20Leonardi&entry.1292438233=%20%20Traditional%20image%20compression%20methods%20aim%20to%20reconstruct%20images%20for%20human%0Aperception%2C%20prioritizing%20visual%20fidelity%20over%20task%20relevance.%20In%20contrast%2C%0ACoding%20for%20Machines%20focuses%20on%20preserving%20information%20essential%20for%20automated%0Aunderstanding.%20Building%20on%20this%20principle%2C%20we%20present%20an%20end-to-end%20compression%0Aframework%20that%20retains%20text-specific%20features%20for%20Optical%20Character%20Recognition%0A%28OCR%29.%20The%20encoder%20operates%20at%20roughly%20half%20the%20computational%20cost%20of%20the%20OCR%0Amodule%2C%20making%20it%20suitable%20for%20resource-limited%20devices.%20When%20on-device%20OCR%20is%0Ainfeasible%2C%20images%20can%20be%20efficiently%20compressed%20and%20later%20decoded%20to%20recover%0Atextual%20content.%20Experiments%20show%20significant%20improvements%20in%20text%20extraction%0Aaccuracy%20at%20low%20bitrates%2C%20even%20outperforming%20OCR%20on%20uncompressed%20images.%0A%20%20We%20further%20extend%20this%20study%20to%20general-purpose%20encoders%2C%20exploring%20their%0Acapacity%20to%20preserve%20hidden%20semantics%20under%20extreme%20compression.%20Instead%20of%0Aoptimizing%20for%20visual%20fidelity%2C%20we%20examine%20whether%20compact%2C%20visually%20degraded%0Arepresentations%20can%20retain%20recoverable%20meaning%20through%20learned%20enhancement%20and%0Arecognition%20modules.%20Results%20demonstrate%20that%20semantic%20information%20can%20persist%0Adespite%20severe%20compression%2C%20bridging%20text-oriented%20compression%20and%0Ageneral-purpose%20semantic%20preservation%20in%20machine-centered%20image%20coding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.19495v2&entry.124074799=Read"},
{"title": "Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm\n  Enables Fine-Grained Policy Optimization", "author": "Yang Li and Zhichen Dong and Yuhan Sun and Weixun Wang and Shaopan Xiong and Yijia Luo and Jiashun Liu and Han Lu and Jiamang Wang and Wenbo Su and Bo Zheng and Junchi Yan", "abstract": "  The reasoning pattern of Large language models (LLMs) remains opaque, and\nReinforcement learning (RL) typically applies uniform credit across an entire\ngeneration, blurring the distinction between pivotal and routine steps. This\nwork positions attention as a privileged substrate that renders the internal\nlogic of LLMs legible, not merely as a byproduct of computation, but as a\nmechanistic blueprint of reasoning itself. We first distinguish attention heads\nbetween locally and globally focused information processing and reveal that\nlocally focused heads produce a sawtooth pattern near the diagonal indicating\nphrasal chunks, while globally focused heads expose tokens that exert broad\ndownstream influence over future tokens. We formalize these with two metrics:\n1) Windowed Average Attention Distance, which measures the extent of backward\nattention within a clipped window; 2) Future Attention Influence, which\nquantifies a token's global importance as the average attention it receives\nfrom subsequent tokens. Taken together, these signals reveal a recurring\npreplan-and-anchor mechanism, where the model first performs a long-range\ncontextual reference to generate an introductory token, which is immediately\nfollowed by or coincides with a semantic anchor token that organizes subsequent\nreasoning. Leveraging these insights, we introduce three novel RL strategies\nthat dynamically perform targeted credit assignment to critical nodes (preplan\ntokens, anchor tokens, and their temporal coupling) and show consistent\nperformance gains across various reasoning tasks. By aligning optimization with\nthe model's intrinsic reasoning rhythm, we aim to transform opaque optimization\ninto an actionable structure-aware process, hoping to offer a potential step\ntoward more transparent and effective optimization of LLM reasoning.\n", "link": "http://arxiv.org/abs/2510.13554v1", "date": "2025-10-15", "relevancy": 2.5298, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5121}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5121}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attention%20Illuminates%20LLM%20Reasoning%3A%20The%20Preplan-and-Anchor%20Rhythm%0A%20%20Enables%20Fine-Grained%20Policy%20Optimization&body=Title%3A%20Attention%20Illuminates%20LLM%20Reasoning%3A%20The%20Preplan-and-Anchor%20Rhythm%0A%20%20Enables%20Fine-Grained%20Policy%20Optimization%0AAuthor%3A%20Yang%20Li%20and%20Zhichen%20Dong%20and%20Yuhan%20Sun%20and%20Weixun%20Wang%20and%20Shaopan%20Xiong%20and%20Yijia%20Luo%20and%20Jiashun%20Liu%20and%20Han%20Lu%20and%20Jiamang%20Wang%20and%20Wenbo%20Su%20and%20Bo%20Zheng%20and%20Junchi%20Yan%0AAbstract%3A%20%20%20The%20reasoning%20pattern%20of%20Large%20language%20models%20%28LLMs%29%20remains%20opaque%2C%20and%0AReinforcement%20learning%20%28RL%29%20typically%20applies%20uniform%20credit%20across%20an%20entire%0Ageneration%2C%20blurring%20the%20distinction%20between%20pivotal%20and%20routine%20steps.%20This%0Awork%20positions%20attention%20as%20a%20privileged%20substrate%20that%20renders%20the%20internal%0Alogic%20of%20LLMs%20legible%2C%20not%20merely%20as%20a%20byproduct%20of%20computation%2C%20but%20as%20a%0Amechanistic%20blueprint%20of%20reasoning%20itself.%20We%20first%20distinguish%20attention%20heads%0Abetween%20locally%20and%20globally%20focused%20information%20processing%20and%20reveal%20that%0Alocally%20focused%20heads%20produce%20a%20sawtooth%20pattern%20near%20the%20diagonal%20indicating%0Aphrasal%20chunks%2C%20while%20globally%20focused%20heads%20expose%20tokens%20that%20exert%20broad%0Adownstream%20influence%20over%20future%20tokens.%20We%20formalize%20these%20with%20two%20metrics%3A%0A1%29%20Windowed%20Average%20Attention%20Distance%2C%20which%20measures%20the%20extent%20of%20backward%0Aattention%20within%20a%20clipped%20window%3B%202%29%20Future%20Attention%20Influence%2C%20which%0Aquantifies%20a%20token%27s%20global%20importance%20as%20the%20average%20attention%20it%20receives%0Afrom%20subsequent%20tokens.%20Taken%20together%2C%20these%20signals%20reveal%20a%20recurring%0Apreplan-and-anchor%20mechanism%2C%20where%20the%20model%20first%20performs%20a%20long-range%0Acontextual%20reference%20to%20generate%20an%20introductory%20token%2C%20which%20is%20immediately%0Afollowed%20by%20or%20coincides%20with%20a%20semantic%20anchor%20token%20that%20organizes%20subsequent%0Areasoning.%20Leveraging%20these%20insights%2C%20we%20introduce%20three%20novel%20RL%20strategies%0Athat%20dynamically%20perform%20targeted%20credit%20assignment%20to%20critical%20nodes%20%28preplan%0Atokens%2C%20anchor%20tokens%2C%20and%20their%20temporal%20coupling%29%20and%20show%20consistent%0Aperformance%20gains%20across%20various%20reasoning%20tasks.%20By%20aligning%20optimization%20with%0Athe%20model%27s%20intrinsic%20reasoning%20rhythm%2C%20we%20aim%20to%20transform%20opaque%20optimization%0Ainto%20an%20actionable%20structure-aware%20process%2C%20hoping%20to%20offer%20a%20potential%20step%0Atoward%20more%20transparent%20and%20effective%20optimization%20of%20LLM%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13554v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttention%2520Illuminates%2520LLM%2520Reasoning%253A%2520The%2520Preplan-and-Anchor%2520Rhythm%250A%2520%2520Enables%2520Fine-Grained%2520Policy%2520Optimization%26entry.906535625%3DYang%2520Li%2520and%2520Zhichen%2520Dong%2520and%2520Yuhan%2520Sun%2520and%2520Weixun%2520Wang%2520and%2520Shaopan%2520Xiong%2520and%2520Yijia%2520Luo%2520and%2520Jiashun%2520Liu%2520and%2520Han%2520Lu%2520and%2520Jiamang%2520Wang%2520and%2520Wenbo%2520Su%2520and%2520Bo%2520Zheng%2520and%2520Junchi%2520Yan%26entry.1292438233%3D%2520%2520The%2520reasoning%2520pattern%2520of%2520Large%2520language%2520models%2520%2528LLMs%2529%2520remains%2520opaque%252C%2520and%250AReinforcement%2520learning%2520%2528RL%2529%2520typically%2520applies%2520uniform%2520credit%2520across%2520an%2520entire%250Ageneration%252C%2520blurring%2520the%2520distinction%2520between%2520pivotal%2520and%2520routine%2520steps.%2520This%250Awork%2520positions%2520attention%2520as%2520a%2520privileged%2520substrate%2520that%2520renders%2520the%2520internal%250Alogic%2520of%2520LLMs%2520legible%252C%2520not%2520merely%2520as%2520a%2520byproduct%2520of%2520computation%252C%2520but%2520as%2520a%250Amechanistic%2520blueprint%2520of%2520reasoning%2520itself.%2520We%2520first%2520distinguish%2520attention%2520heads%250Abetween%2520locally%2520and%2520globally%2520focused%2520information%2520processing%2520and%2520reveal%2520that%250Alocally%2520focused%2520heads%2520produce%2520a%2520sawtooth%2520pattern%2520near%2520the%2520diagonal%2520indicating%250Aphrasal%2520chunks%252C%2520while%2520globally%2520focused%2520heads%2520expose%2520tokens%2520that%2520exert%2520broad%250Adownstream%2520influence%2520over%2520future%2520tokens.%2520We%2520formalize%2520these%2520with%2520two%2520metrics%253A%250A1%2529%2520Windowed%2520Average%2520Attention%2520Distance%252C%2520which%2520measures%2520the%2520extent%2520of%2520backward%250Aattention%2520within%2520a%2520clipped%2520window%253B%25202%2529%2520Future%2520Attention%2520Influence%252C%2520which%250Aquantifies%2520a%2520token%2527s%2520global%2520importance%2520as%2520the%2520average%2520attention%2520it%2520receives%250Afrom%2520subsequent%2520tokens.%2520Taken%2520together%252C%2520these%2520signals%2520reveal%2520a%2520recurring%250Apreplan-and-anchor%2520mechanism%252C%2520where%2520the%2520model%2520first%2520performs%2520a%2520long-range%250Acontextual%2520reference%2520to%2520generate%2520an%2520introductory%2520token%252C%2520which%2520is%2520immediately%250Afollowed%2520by%2520or%2520coincides%2520with%2520a%2520semantic%2520anchor%2520token%2520that%2520organizes%2520subsequent%250Areasoning.%2520Leveraging%2520these%2520insights%252C%2520we%2520introduce%2520three%2520novel%2520RL%2520strategies%250Athat%2520dynamically%2520perform%2520targeted%2520credit%2520assignment%2520to%2520critical%2520nodes%2520%2528preplan%250Atokens%252C%2520anchor%2520tokens%252C%2520and%2520their%2520temporal%2520coupling%2529%2520and%2520show%2520consistent%250Aperformance%2520gains%2520across%2520various%2520reasoning%2520tasks.%2520By%2520aligning%2520optimization%2520with%250Athe%2520model%2527s%2520intrinsic%2520reasoning%2520rhythm%252C%2520we%2520aim%2520to%2520transform%2520opaque%2520optimization%250Ainto%2520an%2520actionable%2520structure-aware%2520process%252C%2520hoping%2520to%2520offer%2520a%2520potential%2520step%250Atoward%2520more%2520transparent%2520and%2520effective%2520optimization%2520of%2520LLM%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13554v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention%20Illuminates%20LLM%20Reasoning%3A%20The%20Preplan-and-Anchor%20Rhythm%0A%20%20Enables%20Fine-Grained%20Policy%20Optimization&entry.906535625=Yang%20Li%20and%20Zhichen%20Dong%20and%20Yuhan%20Sun%20and%20Weixun%20Wang%20and%20Shaopan%20Xiong%20and%20Yijia%20Luo%20and%20Jiashun%20Liu%20and%20Han%20Lu%20and%20Jiamang%20Wang%20and%20Wenbo%20Su%20and%20Bo%20Zheng%20and%20Junchi%20Yan&entry.1292438233=%20%20The%20reasoning%20pattern%20of%20Large%20language%20models%20%28LLMs%29%20remains%20opaque%2C%20and%0AReinforcement%20learning%20%28RL%29%20typically%20applies%20uniform%20credit%20across%20an%20entire%0Ageneration%2C%20blurring%20the%20distinction%20between%20pivotal%20and%20routine%20steps.%20This%0Awork%20positions%20attention%20as%20a%20privileged%20substrate%20that%20renders%20the%20internal%0Alogic%20of%20LLMs%20legible%2C%20not%20merely%20as%20a%20byproduct%20of%20computation%2C%20but%20as%20a%0Amechanistic%20blueprint%20of%20reasoning%20itself.%20We%20first%20distinguish%20attention%20heads%0Abetween%20locally%20and%20globally%20focused%20information%20processing%20and%20reveal%20that%0Alocally%20focused%20heads%20produce%20a%20sawtooth%20pattern%20near%20the%20diagonal%20indicating%0Aphrasal%20chunks%2C%20while%20globally%20focused%20heads%20expose%20tokens%20that%20exert%20broad%0Adownstream%20influence%20over%20future%20tokens.%20We%20formalize%20these%20with%20two%20metrics%3A%0A1%29%20Windowed%20Average%20Attention%20Distance%2C%20which%20measures%20the%20extent%20of%20backward%0Aattention%20within%20a%20clipped%20window%3B%202%29%20Future%20Attention%20Influence%2C%20which%0Aquantifies%20a%20token%27s%20global%20importance%20as%20the%20average%20attention%20it%20receives%0Afrom%20subsequent%20tokens.%20Taken%20together%2C%20these%20signals%20reveal%20a%20recurring%0Apreplan-and-anchor%20mechanism%2C%20where%20the%20model%20first%20performs%20a%20long-range%0Acontextual%20reference%20to%20generate%20an%20introductory%20token%2C%20which%20is%20immediately%0Afollowed%20by%20or%20coincides%20with%20a%20semantic%20anchor%20token%20that%20organizes%20subsequent%0Areasoning.%20Leveraging%20these%20insights%2C%20we%20introduce%20three%20novel%20RL%20strategies%0Athat%20dynamically%20perform%20targeted%20credit%20assignment%20to%20critical%20nodes%20%28preplan%0Atokens%2C%20anchor%20tokens%2C%20and%20their%20temporal%20coupling%29%20and%20show%20consistent%0Aperformance%20gains%20across%20various%20reasoning%20tasks.%20By%20aligning%20optimization%20with%0Athe%20model%27s%20intrinsic%20reasoning%20rhythm%2C%20we%20aim%20to%20transform%20opaque%20optimization%0Ainto%20an%20actionable%20structure-aware%20process%2C%20hoping%20to%20offer%20a%20potential%20step%0Atoward%20more%20transparent%20and%20effective%20optimization%20of%20LLM%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13554v1&entry.124074799=Read"},
{"title": "When Embedding Models Meet: Procrustes Bounds and Applications", "author": "Lucas Maystre and Alvaro Ortega Gonzalez and Charles Park and Rares Dolga and Tudor Berariu and Yu Zhao and Kamil Ciosek", "abstract": "  Embedding models trained separately on similar data often produce\nrepresentations that encode stable information but are not directly\ninterchangeable. This lack of interoperability raises challenges in several\npractical applications, such as model retraining, partial model upgrades, and\nmultimodal search. Driven by these challenges, we study when two sets of\nembeddings can be aligned by an orthogonal transformation. We show that if\npairwise dot products are approximately preserved, then there exists an\nisometry that closely aligns the two sets, and we provide a tight bound on the\nalignment error. This insight yields a simple alignment recipe, Procrustes\npost-processing, that makes two embedding models interoperable while preserving\nthe geometry of each embedding space. Empirically, we demonstrate its\neffectiveness in three applications: maintaining compatibility across\nretrainings, combining different models for text retrieval, and improving\nmixed-modality search, where it achieves state-of-the-art performance.\n", "link": "http://arxiv.org/abs/2510.13406v1", "date": "2025-10-15", "relevancy": 2.5289, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5118}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5028}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Embedding%20Models%20Meet%3A%20Procrustes%20Bounds%20and%20Applications&body=Title%3A%20When%20Embedding%20Models%20Meet%3A%20Procrustes%20Bounds%20and%20Applications%0AAuthor%3A%20Lucas%20Maystre%20and%20Alvaro%20Ortega%20Gonzalez%20and%20Charles%20Park%20and%20Rares%20Dolga%20and%20Tudor%20Berariu%20and%20Yu%20Zhao%20and%20Kamil%20Ciosek%0AAbstract%3A%20%20%20Embedding%20models%20trained%20separately%20on%20similar%20data%20often%20produce%0Arepresentations%20that%20encode%20stable%20information%20but%20are%20not%20directly%0Ainterchangeable.%20This%20lack%20of%20interoperability%20raises%20challenges%20in%20several%0Apractical%20applications%2C%20such%20as%20model%20retraining%2C%20partial%20model%20upgrades%2C%20and%0Amultimodal%20search.%20Driven%20by%20these%20challenges%2C%20we%20study%20when%20two%20sets%20of%0Aembeddings%20can%20be%20aligned%20by%20an%20orthogonal%20transformation.%20We%20show%20that%20if%0Apairwise%20dot%20products%20are%20approximately%20preserved%2C%20then%20there%20exists%20an%0Aisometry%20that%20closely%20aligns%20the%20two%20sets%2C%20and%20we%20provide%20a%20tight%20bound%20on%20the%0Aalignment%20error.%20This%20insight%20yields%20a%20simple%20alignment%20recipe%2C%20Procrustes%0Apost-processing%2C%20that%20makes%20two%20embedding%20models%20interoperable%20while%20preserving%0Athe%20geometry%20of%20each%20embedding%20space.%20Empirically%2C%20we%20demonstrate%20its%0Aeffectiveness%20in%20three%20applications%3A%20maintaining%20compatibility%20across%0Aretrainings%2C%20combining%20different%20models%20for%20text%20retrieval%2C%20and%20improving%0Amixed-modality%20search%2C%20where%20it%20achieves%20state-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13406v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Embedding%2520Models%2520Meet%253A%2520Procrustes%2520Bounds%2520and%2520Applications%26entry.906535625%3DLucas%2520Maystre%2520and%2520Alvaro%2520Ortega%2520Gonzalez%2520and%2520Charles%2520Park%2520and%2520Rares%2520Dolga%2520and%2520Tudor%2520Berariu%2520and%2520Yu%2520Zhao%2520and%2520Kamil%2520Ciosek%26entry.1292438233%3D%2520%2520Embedding%2520models%2520trained%2520separately%2520on%2520similar%2520data%2520often%2520produce%250Arepresentations%2520that%2520encode%2520stable%2520information%2520but%2520are%2520not%2520directly%250Ainterchangeable.%2520This%2520lack%2520of%2520interoperability%2520raises%2520challenges%2520in%2520several%250Apractical%2520applications%252C%2520such%2520as%2520model%2520retraining%252C%2520partial%2520model%2520upgrades%252C%2520and%250Amultimodal%2520search.%2520Driven%2520by%2520these%2520challenges%252C%2520we%2520study%2520when%2520two%2520sets%2520of%250Aembeddings%2520can%2520be%2520aligned%2520by%2520an%2520orthogonal%2520transformation.%2520We%2520show%2520that%2520if%250Apairwise%2520dot%2520products%2520are%2520approximately%2520preserved%252C%2520then%2520there%2520exists%2520an%250Aisometry%2520that%2520closely%2520aligns%2520the%2520two%2520sets%252C%2520and%2520we%2520provide%2520a%2520tight%2520bound%2520on%2520the%250Aalignment%2520error.%2520This%2520insight%2520yields%2520a%2520simple%2520alignment%2520recipe%252C%2520Procrustes%250Apost-processing%252C%2520that%2520makes%2520two%2520embedding%2520models%2520interoperable%2520while%2520preserving%250Athe%2520geometry%2520of%2520each%2520embedding%2520space.%2520Empirically%252C%2520we%2520demonstrate%2520its%250Aeffectiveness%2520in%2520three%2520applications%253A%2520maintaining%2520compatibility%2520across%250Aretrainings%252C%2520combining%2520different%2520models%2520for%2520text%2520retrieval%252C%2520and%2520improving%250Amixed-modality%2520search%252C%2520where%2520it%2520achieves%2520state-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13406v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Embedding%20Models%20Meet%3A%20Procrustes%20Bounds%20and%20Applications&entry.906535625=Lucas%20Maystre%20and%20Alvaro%20Ortega%20Gonzalez%20and%20Charles%20Park%20and%20Rares%20Dolga%20and%20Tudor%20Berariu%20and%20Yu%20Zhao%20and%20Kamil%20Ciosek&entry.1292438233=%20%20Embedding%20models%20trained%20separately%20on%20similar%20data%20often%20produce%0Arepresentations%20that%20encode%20stable%20information%20but%20are%20not%20directly%0Ainterchangeable.%20This%20lack%20of%20interoperability%20raises%20challenges%20in%20several%0Apractical%20applications%2C%20such%20as%20model%20retraining%2C%20partial%20model%20upgrades%2C%20and%0Amultimodal%20search.%20Driven%20by%20these%20challenges%2C%20we%20study%20when%20two%20sets%20of%0Aembeddings%20can%20be%20aligned%20by%20an%20orthogonal%20transformation.%20We%20show%20that%20if%0Apairwise%20dot%20products%20are%20approximately%20preserved%2C%20then%20there%20exists%20an%0Aisometry%20that%20closely%20aligns%20the%20two%20sets%2C%20and%20we%20provide%20a%20tight%20bound%20on%20the%0Aalignment%20error.%20This%20insight%20yields%20a%20simple%20alignment%20recipe%2C%20Procrustes%0Apost-processing%2C%20that%20makes%20two%20embedding%20models%20interoperable%20while%20preserving%0Athe%20geometry%20of%20each%20embedding%20space.%20Empirically%2C%20we%20demonstrate%20its%0Aeffectiveness%20in%20three%20applications%3A%20maintaining%20compatibility%20across%0Aretrainings%2C%20combining%20different%20models%20for%20text%20retrieval%2C%20and%20improving%0Amixed-modality%20search%2C%20where%20it%20achieves%20state-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13406v1&entry.124074799=Read"},
{"title": "Characterizing Lidar Point-Cloud Adversities Using a Vector Field\n  Visualization", "author": "Daniel Choate and Jason Rife", "abstract": "  In this paper we introduce a visualization methodology to aid a human analyst\nin classifying adversity modes that impact lidar scan matching. Our methodology\nis intended for offline rather than real-time analysis. The method generates a\nvector-field plot that characterizes local discrepancies between a pair of\nregistered point clouds. The vector field plot reveals patterns that would be\ndifficult for the analyst to extract from raw point-cloud data. After\nintroducing our methodology, we apply the process to two proof-of-concept\nexamples: one a simulation study and the other a field experiment. For both\ndata sets, a human analyst was able to reason about a series of adversity\nmechanisms and iteratively remove those mechanisms from the raw data, to help\nfocus attention on progressively smaller discrepancies.\n", "link": "http://arxiv.org/abs/2510.13619v1", "date": "2025-10-15", "relevancy": 2.5226, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5368}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4884}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4884}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Characterizing%20Lidar%20Point-Cloud%20Adversities%20Using%20a%20Vector%20Field%0A%20%20Visualization&body=Title%3A%20Characterizing%20Lidar%20Point-Cloud%20Adversities%20Using%20a%20Vector%20Field%0A%20%20Visualization%0AAuthor%3A%20Daniel%20Choate%20and%20Jason%20Rife%0AAbstract%3A%20%20%20In%20this%20paper%20we%20introduce%20a%20visualization%20methodology%20to%20aid%20a%20human%20analyst%0Ain%20classifying%20adversity%20modes%20that%20impact%20lidar%20scan%20matching.%20Our%20methodology%0Ais%20intended%20for%20offline%20rather%20than%20real-time%20analysis.%20The%20method%20generates%20a%0Avector-field%20plot%20that%20characterizes%20local%20discrepancies%20between%20a%20pair%20of%0Aregistered%20point%20clouds.%20The%20vector%20field%20plot%20reveals%20patterns%20that%20would%20be%0Adifficult%20for%20the%20analyst%20to%20extract%20from%20raw%20point-cloud%20data.%20After%0Aintroducing%20our%20methodology%2C%20we%20apply%20the%20process%20to%20two%20proof-of-concept%0Aexamples%3A%20one%20a%20simulation%20study%20and%20the%20other%20a%20field%20experiment.%20For%20both%0Adata%20sets%2C%20a%20human%20analyst%20was%20able%20to%20reason%20about%20a%20series%20of%20adversity%0Amechanisms%20and%20iteratively%20remove%20those%20mechanisms%20from%20the%20raw%20data%2C%20to%20help%0Afocus%20attention%20on%20progressively%20smaller%20discrepancies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13619v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCharacterizing%2520Lidar%2520Point-Cloud%2520Adversities%2520Using%2520a%2520Vector%2520Field%250A%2520%2520Visualization%26entry.906535625%3DDaniel%2520Choate%2520and%2520Jason%2520Rife%26entry.1292438233%3D%2520%2520In%2520this%2520paper%2520we%2520introduce%2520a%2520visualization%2520methodology%2520to%2520aid%2520a%2520human%2520analyst%250Ain%2520classifying%2520adversity%2520modes%2520that%2520impact%2520lidar%2520scan%2520matching.%2520Our%2520methodology%250Ais%2520intended%2520for%2520offline%2520rather%2520than%2520real-time%2520analysis.%2520The%2520method%2520generates%2520a%250Avector-field%2520plot%2520that%2520characterizes%2520local%2520discrepancies%2520between%2520a%2520pair%2520of%250Aregistered%2520point%2520clouds.%2520The%2520vector%2520field%2520plot%2520reveals%2520patterns%2520that%2520would%2520be%250Adifficult%2520for%2520the%2520analyst%2520to%2520extract%2520from%2520raw%2520point-cloud%2520data.%2520After%250Aintroducing%2520our%2520methodology%252C%2520we%2520apply%2520the%2520process%2520to%2520two%2520proof-of-concept%250Aexamples%253A%2520one%2520a%2520simulation%2520study%2520and%2520the%2520other%2520a%2520field%2520experiment.%2520For%2520both%250Adata%2520sets%252C%2520a%2520human%2520analyst%2520was%2520able%2520to%2520reason%2520about%2520a%2520series%2520of%2520adversity%250Amechanisms%2520and%2520iteratively%2520remove%2520those%2520mechanisms%2520from%2520the%2520raw%2520data%252C%2520to%2520help%250Afocus%2520attention%2520on%2520progressively%2520smaller%2520discrepancies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13619v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Characterizing%20Lidar%20Point-Cloud%20Adversities%20Using%20a%20Vector%20Field%0A%20%20Visualization&entry.906535625=Daniel%20Choate%20and%20Jason%20Rife&entry.1292438233=%20%20In%20this%20paper%20we%20introduce%20a%20visualization%20methodology%20to%20aid%20a%20human%20analyst%0Ain%20classifying%20adversity%20modes%20that%20impact%20lidar%20scan%20matching.%20Our%20methodology%0Ais%20intended%20for%20offline%20rather%20than%20real-time%20analysis.%20The%20method%20generates%20a%0Avector-field%20plot%20that%20characterizes%20local%20discrepancies%20between%20a%20pair%20of%0Aregistered%20point%20clouds.%20The%20vector%20field%20plot%20reveals%20patterns%20that%20would%20be%0Adifficult%20for%20the%20analyst%20to%20extract%20from%20raw%20point-cloud%20data.%20After%0Aintroducing%20our%20methodology%2C%20we%20apply%20the%20process%20to%20two%20proof-of-concept%0Aexamples%3A%20one%20a%20simulation%20study%20and%20the%20other%20a%20field%20experiment.%20For%20both%0Adata%20sets%2C%20a%20human%20analyst%20was%20able%20to%20reason%20about%20a%20series%20of%20adversity%0Amechanisms%20and%20iteratively%20remove%20those%20mechanisms%20from%20the%20raw%20data%2C%20to%20help%0Afocus%20attention%20on%20progressively%20smaller%20discrepancies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13619v1&entry.124074799=Read"},
{"title": "VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a\n  Video Generator", "author": "Hyojun Go and Dominik Narnhofer and Goutam Bhat and Prune Truong and Federico Tombari and Konrad Schindler", "abstract": "  The rapid progress of large, pretrained models for both visual content\ngeneration and 3D reconstruction opens up new possibilities for text-to-3D\ngeneration. Intuitively, one could obtain a formidable 3D scene generator if\none were able to combine the power of a modern latent text-to-video model as\n\"generator\" with the geometric abilities of a recent (feedforward) 3D\nreconstruction system as \"decoder\". We introduce VIST3A, a general framework\nthat does just that, addressing two main challenges. First, the two components\nmust be joined in a way that preserves the rich knowledge encoded in their\nweights. We revisit model stitching, i.e., we identify the layer in the 3D\ndecoder that best matches the latent representation produced by the\ntext-to-video generator and stitch the two parts together. That operation\nrequires only a small dataset and no labels. Second, the text-to-video\ngenerator must be aligned with the stitched 3D decoder, to ensure that the\ngenerated latents are decodable into consistent, perceptually convincing 3D\nscene geometry. To that end, we adapt direct reward finetuning, a popular\ntechnique for human preference alignment. We evaluate the proposed VIST3A\napproach with different video generators and 3D reconstruction models. All\ntested pairings markedly improve over prior text-to-3D models that output\nGaussian splats. Moreover, by choosing a suitable 3D base model, VIST3A also\nenables high-quality text-to-pointmap generation.\n", "link": "http://arxiv.org/abs/2510.13454v1", "date": "2025-10-15", "relevancy": 2.5117, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6404}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6254}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6254}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VIST3A%3A%20Text-to-3D%20by%20Stitching%20a%20Multi-view%20Reconstruction%20Network%20to%20a%0A%20%20Video%20Generator&body=Title%3A%20VIST3A%3A%20Text-to-3D%20by%20Stitching%20a%20Multi-view%20Reconstruction%20Network%20to%20a%0A%20%20Video%20Generator%0AAuthor%3A%20Hyojun%20Go%20and%20Dominik%20Narnhofer%20and%20Goutam%20Bhat%20and%20Prune%20Truong%20and%20Federico%20Tombari%20and%20Konrad%20Schindler%0AAbstract%3A%20%20%20The%20rapid%20progress%20of%20large%2C%20pretrained%20models%20for%20both%20visual%20content%0Ageneration%20and%203D%20reconstruction%20opens%20up%20new%20possibilities%20for%20text-to-3D%0Ageneration.%20Intuitively%2C%20one%20could%20obtain%20a%20formidable%203D%20scene%20generator%20if%0Aone%20were%20able%20to%20combine%20the%20power%20of%20a%20modern%20latent%20text-to-video%20model%20as%0A%22generator%22%20with%20the%20geometric%20abilities%20of%20a%20recent%20%28feedforward%29%203D%0Areconstruction%20system%20as%20%22decoder%22.%20We%20introduce%20VIST3A%2C%20a%20general%20framework%0Athat%20does%20just%20that%2C%20addressing%20two%20main%20challenges.%20First%2C%20the%20two%20components%0Amust%20be%20joined%20in%20a%20way%20that%20preserves%20the%20rich%20knowledge%20encoded%20in%20their%0Aweights.%20We%20revisit%20model%20stitching%2C%20i.e.%2C%20we%20identify%20the%20layer%20in%20the%203D%0Adecoder%20that%20best%20matches%20the%20latent%20representation%20produced%20by%20the%0Atext-to-video%20generator%20and%20stitch%20the%20two%20parts%20together.%20That%20operation%0Arequires%20only%20a%20small%20dataset%20and%20no%20labels.%20Second%2C%20the%20text-to-video%0Agenerator%20must%20be%20aligned%20with%20the%20stitched%203D%20decoder%2C%20to%20ensure%20that%20the%0Agenerated%20latents%20are%20decodable%20into%20consistent%2C%20perceptually%20convincing%203D%0Ascene%20geometry.%20To%20that%20end%2C%20we%20adapt%20direct%20reward%20finetuning%2C%20a%20popular%0Atechnique%20for%20human%20preference%20alignment.%20We%20evaluate%20the%20proposed%20VIST3A%0Aapproach%20with%20different%20video%20generators%20and%203D%20reconstruction%20models.%20All%0Atested%20pairings%20markedly%20improve%20over%20prior%20text-to-3D%20models%20that%20output%0AGaussian%20splats.%20Moreover%2C%20by%20choosing%20a%20suitable%203D%20base%20model%2C%20VIST3A%20also%0Aenables%20high-quality%20text-to-pointmap%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13454v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVIST3A%253A%2520Text-to-3D%2520by%2520Stitching%2520a%2520Multi-view%2520Reconstruction%2520Network%2520to%2520a%250A%2520%2520Video%2520Generator%26entry.906535625%3DHyojun%2520Go%2520and%2520Dominik%2520Narnhofer%2520and%2520Goutam%2520Bhat%2520and%2520Prune%2520Truong%2520and%2520Federico%2520Tombari%2520and%2520Konrad%2520Schindler%26entry.1292438233%3D%2520%2520The%2520rapid%2520progress%2520of%2520large%252C%2520pretrained%2520models%2520for%2520both%2520visual%2520content%250Ageneration%2520and%25203D%2520reconstruction%2520opens%2520up%2520new%2520possibilities%2520for%2520text-to-3D%250Ageneration.%2520Intuitively%252C%2520one%2520could%2520obtain%2520a%2520formidable%25203D%2520scene%2520generator%2520if%250Aone%2520were%2520able%2520to%2520combine%2520the%2520power%2520of%2520a%2520modern%2520latent%2520text-to-video%2520model%2520as%250A%2522generator%2522%2520with%2520the%2520geometric%2520abilities%2520of%2520a%2520recent%2520%2528feedforward%2529%25203D%250Areconstruction%2520system%2520as%2520%2522decoder%2522.%2520We%2520introduce%2520VIST3A%252C%2520a%2520general%2520framework%250Athat%2520does%2520just%2520that%252C%2520addressing%2520two%2520main%2520challenges.%2520First%252C%2520the%2520two%2520components%250Amust%2520be%2520joined%2520in%2520a%2520way%2520that%2520preserves%2520the%2520rich%2520knowledge%2520encoded%2520in%2520their%250Aweights.%2520We%2520revisit%2520model%2520stitching%252C%2520i.e.%252C%2520we%2520identify%2520the%2520layer%2520in%2520the%25203D%250Adecoder%2520that%2520best%2520matches%2520the%2520latent%2520representation%2520produced%2520by%2520the%250Atext-to-video%2520generator%2520and%2520stitch%2520the%2520two%2520parts%2520together.%2520That%2520operation%250Arequires%2520only%2520a%2520small%2520dataset%2520and%2520no%2520labels.%2520Second%252C%2520the%2520text-to-video%250Agenerator%2520must%2520be%2520aligned%2520with%2520the%2520stitched%25203D%2520decoder%252C%2520to%2520ensure%2520that%2520the%250Agenerated%2520latents%2520are%2520decodable%2520into%2520consistent%252C%2520perceptually%2520convincing%25203D%250Ascene%2520geometry.%2520To%2520that%2520end%252C%2520we%2520adapt%2520direct%2520reward%2520finetuning%252C%2520a%2520popular%250Atechnique%2520for%2520human%2520preference%2520alignment.%2520We%2520evaluate%2520the%2520proposed%2520VIST3A%250Aapproach%2520with%2520different%2520video%2520generators%2520and%25203D%2520reconstruction%2520models.%2520All%250Atested%2520pairings%2520markedly%2520improve%2520over%2520prior%2520text-to-3D%2520models%2520that%2520output%250AGaussian%2520splats.%2520Moreover%252C%2520by%2520choosing%2520a%2520suitable%25203D%2520base%2520model%252C%2520VIST3A%2520also%250Aenables%2520high-quality%2520text-to-pointmap%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13454v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VIST3A%3A%20Text-to-3D%20by%20Stitching%20a%20Multi-view%20Reconstruction%20Network%20to%20a%0A%20%20Video%20Generator&entry.906535625=Hyojun%20Go%20and%20Dominik%20Narnhofer%20and%20Goutam%20Bhat%20and%20Prune%20Truong%20and%20Federico%20Tombari%20and%20Konrad%20Schindler&entry.1292438233=%20%20The%20rapid%20progress%20of%20large%2C%20pretrained%20models%20for%20both%20visual%20content%0Ageneration%20and%203D%20reconstruction%20opens%20up%20new%20possibilities%20for%20text-to-3D%0Ageneration.%20Intuitively%2C%20one%20could%20obtain%20a%20formidable%203D%20scene%20generator%20if%0Aone%20were%20able%20to%20combine%20the%20power%20of%20a%20modern%20latent%20text-to-video%20model%20as%0A%22generator%22%20with%20the%20geometric%20abilities%20of%20a%20recent%20%28feedforward%29%203D%0Areconstruction%20system%20as%20%22decoder%22.%20We%20introduce%20VIST3A%2C%20a%20general%20framework%0Athat%20does%20just%20that%2C%20addressing%20two%20main%20challenges.%20First%2C%20the%20two%20components%0Amust%20be%20joined%20in%20a%20way%20that%20preserves%20the%20rich%20knowledge%20encoded%20in%20their%0Aweights.%20We%20revisit%20model%20stitching%2C%20i.e.%2C%20we%20identify%20the%20layer%20in%20the%203D%0Adecoder%20that%20best%20matches%20the%20latent%20representation%20produced%20by%20the%0Atext-to-video%20generator%20and%20stitch%20the%20two%20parts%20together.%20That%20operation%0Arequires%20only%20a%20small%20dataset%20and%20no%20labels.%20Second%2C%20the%20text-to-video%0Agenerator%20must%20be%20aligned%20with%20the%20stitched%203D%20decoder%2C%20to%20ensure%20that%20the%0Agenerated%20latents%20are%20decodable%20into%20consistent%2C%20perceptually%20convincing%203D%0Ascene%20geometry.%20To%20that%20end%2C%20we%20adapt%20direct%20reward%20finetuning%2C%20a%20popular%0Atechnique%20for%20human%20preference%20alignment.%20We%20evaluate%20the%20proposed%20VIST3A%0Aapproach%20with%20different%20video%20generators%20and%203D%20reconstruction%20models.%20All%0Atested%20pairings%20markedly%20improve%20over%20prior%20text-to-3D%20models%20that%20output%0AGaussian%20splats.%20Moreover%2C%20by%20choosing%20a%20suitable%203D%20base%20model%2C%20VIST3A%20also%0Aenables%20high-quality%20text-to-pointmap%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13454v1&entry.124074799=Read"},
{"title": "Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision", "author": "Xiaofeng Han and Shunpeng Chen and Zenghuang Fu and Zhe Feng and Lue Fan and Dong An and Changwei Wang and Li Guo and Weiliang Meng and Xiaopeng Zhang and Rongtao Xu and Shibiao Xu", "abstract": "  Robot vision has greatly benefited from advancements in multimodal fusion\ntechniques and vision-language models (VLMs). We adopt a task-oriented\nperspective to systematically review the applications and advancements of\nmultimodal fusion methods and VLMs in the field of robot vision. For semantic\nscene understanding tasks, we categorize fusion approaches into encoder-decoder\nframeworks, attention-based architectures, and graph neural networks.\nMeanwhile, we also analyze the architectural characteristics and practical\nimplementations of these fusion strategies in key tasks such as simultaneous\nlocalization and mapping (SLAM), 3D object detection, navigation, and\nmanipulation. We compare the evolutionary paths and applicability of VLMs based\non large language models (LLMs) with traditional multimodal fusion\nmethods.Additionally, we conduct an in-depth analysis of commonly used\ndatasets, evaluating their applicability and challenges in real-world robotic\nscenarios. Building on this analysis, we identify key challenges in current\nresearch, including cross-modal alignment, efficient fusion, real-time\ndeployment, and domain adaptation. We propose future directions such as\nself-supervised learning for robust multimodal representations, structured\nspatial memory and environment modeling to enhance spatial intelligence, and\nthe integration of adversarial robustness and human feedback mechanisms to\nenable ethically aligned system deployment. Through a comprehensive review,\ncomparative analysis, and forward-looking discussion, we provide a valuable\nreference for advancing multimodal perception and interaction in robotic\nvision. A comprehensive list of studies in this survey is available at\nhttps://github.com/Xiaofeng-Han-Res/MF-RV.\n", "link": "http://arxiv.org/abs/2504.02477v3", "date": "2025-10-15", "relevancy": 2.4841, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6264}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6199}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6199}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Fusion%20and%20Vision-Language%20Models%3A%20A%20Survey%20for%20Robot%20Vision&body=Title%3A%20Multimodal%20Fusion%20and%20Vision-Language%20Models%3A%20A%20Survey%20for%20Robot%20Vision%0AAuthor%3A%20Xiaofeng%20Han%20and%20Shunpeng%20Chen%20and%20Zenghuang%20Fu%20and%20Zhe%20Feng%20and%20Lue%20Fan%20and%20Dong%20An%20and%20Changwei%20Wang%20and%20Li%20Guo%20and%20Weiliang%20Meng%20and%20Xiaopeng%20Zhang%20and%20Rongtao%20Xu%20and%20Shibiao%20Xu%0AAbstract%3A%20%20%20Robot%20vision%20has%20greatly%20benefited%20from%20advancements%20in%20multimodal%20fusion%0Atechniques%20and%20vision-language%20models%20%28VLMs%29.%20We%20adopt%20a%20task-oriented%0Aperspective%20to%20systematically%20review%20the%20applications%20and%20advancements%20of%0Amultimodal%20fusion%20methods%20and%20VLMs%20in%20the%20field%20of%20robot%20vision.%20For%20semantic%0Ascene%20understanding%20tasks%2C%20we%20categorize%20fusion%20approaches%20into%20encoder-decoder%0Aframeworks%2C%20attention-based%20architectures%2C%20and%20graph%20neural%20networks.%0AMeanwhile%2C%20we%20also%20analyze%20the%20architectural%20characteristics%20and%20practical%0Aimplementations%20of%20these%20fusion%20strategies%20in%20key%20tasks%20such%20as%20simultaneous%0Alocalization%20and%20mapping%20%28SLAM%29%2C%203D%20object%20detection%2C%20navigation%2C%20and%0Amanipulation.%20We%20compare%20the%20evolutionary%20paths%20and%20applicability%20of%20VLMs%20based%0Aon%20large%20language%20models%20%28LLMs%29%20with%20traditional%20multimodal%20fusion%0Amethods.Additionally%2C%20we%20conduct%20an%20in-depth%20analysis%20of%20commonly%20used%0Adatasets%2C%20evaluating%20their%20applicability%20and%20challenges%20in%20real-world%20robotic%0Ascenarios.%20Building%20on%20this%20analysis%2C%20we%20identify%20key%20challenges%20in%20current%0Aresearch%2C%20including%20cross-modal%20alignment%2C%20efficient%20fusion%2C%20real-time%0Adeployment%2C%20and%20domain%20adaptation.%20We%20propose%20future%20directions%20such%20as%0Aself-supervised%20learning%20for%20robust%20multimodal%20representations%2C%20structured%0Aspatial%20memory%20and%20environment%20modeling%20to%20enhance%20spatial%20intelligence%2C%20and%0Athe%20integration%20of%20adversarial%20robustness%20and%20human%20feedback%20mechanisms%20to%0Aenable%20ethically%20aligned%20system%20deployment.%20Through%20a%20comprehensive%20review%2C%0Acomparative%20analysis%2C%20and%20forward-looking%20discussion%2C%20we%20provide%20a%20valuable%0Areference%20for%20advancing%20multimodal%20perception%20and%20interaction%20in%20robotic%0Avision.%20A%20comprehensive%20list%20of%20studies%20in%20this%20survey%20is%20available%20at%0Ahttps%3A//github.com/Xiaofeng-Han-Res/MF-RV.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.02477v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Fusion%2520and%2520Vision-Language%2520Models%253A%2520A%2520Survey%2520for%2520Robot%2520Vision%26entry.906535625%3DXiaofeng%2520Han%2520and%2520Shunpeng%2520Chen%2520and%2520Zenghuang%2520Fu%2520and%2520Zhe%2520Feng%2520and%2520Lue%2520Fan%2520and%2520Dong%2520An%2520and%2520Changwei%2520Wang%2520and%2520Li%2520Guo%2520and%2520Weiliang%2520Meng%2520and%2520Xiaopeng%2520Zhang%2520and%2520Rongtao%2520Xu%2520and%2520Shibiao%2520Xu%26entry.1292438233%3D%2520%2520Robot%2520vision%2520has%2520greatly%2520benefited%2520from%2520advancements%2520in%2520multimodal%2520fusion%250Atechniques%2520and%2520vision-language%2520models%2520%2528VLMs%2529.%2520We%2520adopt%2520a%2520task-oriented%250Aperspective%2520to%2520systematically%2520review%2520the%2520applications%2520and%2520advancements%2520of%250Amultimodal%2520fusion%2520methods%2520and%2520VLMs%2520in%2520the%2520field%2520of%2520robot%2520vision.%2520For%2520semantic%250Ascene%2520understanding%2520tasks%252C%2520we%2520categorize%2520fusion%2520approaches%2520into%2520encoder-decoder%250Aframeworks%252C%2520attention-based%2520architectures%252C%2520and%2520graph%2520neural%2520networks.%250AMeanwhile%252C%2520we%2520also%2520analyze%2520the%2520architectural%2520characteristics%2520and%2520practical%250Aimplementations%2520of%2520these%2520fusion%2520strategies%2520in%2520key%2520tasks%2520such%2520as%2520simultaneous%250Alocalization%2520and%2520mapping%2520%2528SLAM%2529%252C%25203D%2520object%2520detection%252C%2520navigation%252C%2520and%250Amanipulation.%2520We%2520compare%2520the%2520evolutionary%2520paths%2520and%2520applicability%2520of%2520VLMs%2520based%250Aon%2520large%2520language%2520models%2520%2528LLMs%2529%2520with%2520traditional%2520multimodal%2520fusion%250Amethods.Additionally%252C%2520we%2520conduct%2520an%2520in-depth%2520analysis%2520of%2520commonly%2520used%250Adatasets%252C%2520evaluating%2520their%2520applicability%2520and%2520challenges%2520in%2520real-world%2520robotic%250Ascenarios.%2520Building%2520on%2520this%2520analysis%252C%2520we%2520identify%2520key%2520challenges%2520in%2520current%250Aresearch%252C%2520including%2520cross-modal%2520alignment%252C%2520efficient%2520fusion%252C%2520real-time%250Adeployment%252C%2520and%2520domain%2520adaptation.%2520We%2520propose%2520future%2520directions%2520such%2520as%250Aself-supervised%2520learning%2520for%2520robust%2520multimodal%2520representations%252C%2520structured%250Aspatial%2520memory%2520and%2520environment%2520modeling%2520to%2520enhance%2520spatial%2520intelligence%252C%2520and%250Athe%2520integration%2520of%2520adversarial%2520robustness%2520and%2520human%2520feedback%2520mechanisms%2520to%250Aenable%2520ethically%2520aligned%2520system%2520deployment.%2520Through%2520a%2520comprehensive%2520review%252C%250Acomparative%2520analysis%252C%2520and%2520forward-looking%2520discussion%252C%2520we%2520provide%2520a%2520valuable%250Areference%2520for%2520advancing%2520multimodal%2520perception%2520and%2520interaction%2520in%2520robotic%250Avision.%2520A%2520comprehensive%2520list%2520of%2520studies%2520in%2520this%2520survey%2520is%2520available%2520at%250Ahttps%253A//github.com/Xiaofeng-Han-Res/MF-RV.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.02477v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Fusion%20and%20Vision-Language%20Models%3A%20A%20Survey%20for%20Robot%20Vision&entry.906535625=Xiaofeng%20Han%20and%20Shunpeng%20Chen%20and%20Zenghuang%20Fu%20and%20Zhe%20Feng%20and%20Lue%20Fan%20and%20Dong%20An%20and%20Changwei%20Wang%20and%20Li%20Guo%20and%20Weiliang%20Meng%20and%20Xiaopeng%20Zhang%20and%20Rongtao%20Xu%20and%20Shibiao%20Xu&entry.1292438233=%20%20Robot%20vision%20has%20greatly%20benefited%20from%20advancements%20in%20multimodal%20fusion%0Atechniques%20and%20vision-language%20models%20%28VLMs%29.%20We%20adopt%20a%20task-oriented%0Aperspective%20to%20systematically%20review%20the%20applications%20and%20advancements%20of%0Amultimodal%20fusion%20methods%20and%20VLMs%20in%20the%20field%20of%20robot%20vision.%20For%20semantic%0Ascene%20understanding%20tasks%2C%20we%20categorize%20fusion%20approaches%20into%20encoder-decoder%0Aframeworks%2C%20attention-based%20architectures%2C%20and%20graph%20neural%20networks.%0AMeanwhile%2C%20we%20also%20analyze%20the%20architectural%20characteristics%20and%20practical%0Aimplementations%20of%20these%20fusion%20strategies%20in%20key%20tasks%20such%20as%20simultaneous%0Alocalization%20and%20mapping%20%28SLAM%29%2C%203D%20object%20detection%2C%20navigation%2C%20and%0Amanipulation.%20We%20compare%20the%20evolutionary%20paths%20and%20applicability%20of%20VLMs%20based%0Aon%20large%20language%20models%20%28LLMs%29%20with%20traditional%20multimodal%20fusion%0Amethods.Additionally%2C%20we%20conduct%20an%20in-depth%20analysis%20of%20commonly%20used%0Adatasets%2C%20evaluating%20their%20applicability%20and%20challenges%20in%20real-world%20robotic%0Ascenarios.%20Building%20on%20this%20analysis%2C%20we%20identify%20key%20challenges%20in%20current%0Aresearch%2C%20including%20cross-modal%20alignment%2C%20efficient%20fusion%2C%20real-time%0Adeployment%2C%20and%20domain%20adaptation.%20We%20propose%20future%20directions%20such%20as%0Aself-supervised%20learning%20for%20robust%20multimodal%20representations%2C%20structured%0Aspatial%20memory%20and%20environment%20modeling%20to%20enhance%20spatial%20intelligence%2C%20and%0Athe%20integration%20of%20adversarial%20robustness%20and%20human%20feedback%20mechanisms%20to%0Aenable%20ethically%20aligned%20system%20deployment.%20Through%20a%20comprehensive%20review%2C%0Acomparative%20analysis%2C%20and%20forward-looking%20discussion%2C%20we%20provide%20a%20valuable%0Areference%20for%20advancing%20multimodal%20perception%20and%20interaction%20in%20robotic%0Avision.%20A%20comprehensive%20list%20of%20studies%20in%20this%20survey%20is%20available%20at%0Ahttps%3A//github.com/Xiaofeng-Han-Res/MF-RV.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.02477v3&entry.124074799=Read"},
{"title": "Simplicial Embeddings Improve Sample Efficiency in Actor-Critic Agents", "author": "Johan Obando-Ceron and Walter Mayor and Samuel Lavoie and Scott Fujimoto and Aaron Courville and Pablo Samuel Castro", "abstract": "  Recent works have proposed accelerating the wall-clock training time of\nactor-critic methods via the use of large-scale environment parallelization;\nunfortunately, these can sometimes still require large number of environment\ninteractions to achieve a desired level of performance. Noting that\nwell-structured representations can improve the generalization and sample\nefficiency of deep reinforcement learning (RL) agents, we propose the use of\nsimplicial embeddings: lightweight representation layers that constrain\nembeddings to simplicial structures. This geometric inductive bias results in\nsparse and discrete features that stabilize critic bootstrapping and strengthen\npolicy gradients. When applied to FastTD3, FastSAC, and PPO, simplicial\nembeddings consistently improve sample efficiency and final performance across\na variety of continuous- and discrete-control environments, without any loss in\nruntime speed.\n", "link": "http://arxiv.org/abs/2510.13704v1", "date": "2025-10-15", "relevancy": 2.4825, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5075}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4922}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simplicial%20Embeddings%20Improve%20Sample%20Efficiency%20in%20Actor-Critic%20Agents&body=Title%3A%20Simplicial%20Embeddings%20Improve%20Sample%20Efficiency%20in%20Actor-Critic%20Agents%0AAuthor%3A%20Johan%20Obando-Ceron%20and%20Walter%20Mayor%20and%20Samuel%20Lavoie%20and%20Scott%20Fujimoto%20and%20Aaron%20Courville%20and%20Pablo%20Samuel%20Castro%0AAbstract%3A%20%20%20Recent%20works%20have%20proposed%20accelerating%20the%20wall-clock%20training%20time%20of%0Aactor-critic%20methods%20via%20the%20use%20of%20large-scale%20environment%20parallelization%3B%0Aunfortunately%2C%20these%20can%20sometimes%20still%20require%20large%20number%20of%20environment%0Ainteractions%20to%20achieve%20a%20desired%20level%20of%20performance.%20Noting%20that%0Awell-structured%20representations%20can%20improve%20the%20generalization%20and%20sample%0Aefficiency%20of%20deep%20reinforcement%20learning%20%28RL%29%20agents%2C%20we%20propose%20the%20use%20of%0Asimplicial%20embeddings%3A%20lightweight%20representation%20layers%20that%20constrain%0Aembeddings%20to%20simplicial%20structures.%20This%20geometric%20inductive%20bias%20results%20in%0Asparse%20and%20discrete%20features%20that%20stabilize%20critic%20bootstrapping%20and%20strengthen%0Apolicy%20gradients.%20When%20applied%20to%20FastTD3%2C%20FastSAC%2C%20and%20PPO%2C%20simplicial%0Aembeddings%20consistently%20improve%20sample%20efficiency%20and%20final%20performance%20across%0Aa%20variety%20of%20continuous-%20and%20discrete-control%20environments%2C%20without%20any%20loss%20in%0Aruntime%20speed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13704v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimplicial%2520Embeddings%2520Improve%2520Sample%2520Efficiency%2520in%2520Actor-Critic%2520Agents%26entry.906535625%3DJohan%2520Obando-Ceron%2520and%2520Walter%2520Mayor%2520and%2520Samuel%2520Lavoie%2520and%2520Scott%2520Fujimoto%2520and%2520Aaron%2520Courville%2520and%2520Pablo%2520Samuel%2520Castro%26entry.1292438233%3D%2520%2520Recent%2520works%2520have%2520proposed%2520accelerating%2520the%2520wall-clock%2520training%2520time%2520of%250Aactor-critic%2520methods%2520via%2520the%2520use%2520of%2520large-scale%2520environment%2520parallelization%253B%250Aunfortunately%252C%2520these%2520can%2520sometimes%2520still%2520require%2520large%2520number%2520of%2520environment%250Ainteractions%2520to%2520achieve%2520a%2520desired%2520level%2520of%2520performance.%2520Noting%2520that%250Awell-structured%2520representations%2520can%2520improve%2520the%2520generalization%2520and%2520sample%250Aefficiency%2520of%2520deep%2520reinforcement%2520learning%2520%2528RL%2529%2520agents%252C%2520we%2520propose%2520the%2520use%2520of%250Asimplicial%2520embeddings%253A%2520lightweight%2520representation%2520layers%2520that%2520constrain%250Aembeddings%2520to%2520simplicial%2520structures.%2520This%2520geometric%2520inductive%2520bias%2520results%2520in%250Asparse%2520and%2520discrete%2520features%2520that%2520stabilize%2520critic%2520bootstrapping%2520and%2520strengthen%250Apolicy%2520gradients.%2520When%2520applied%2520to%2520FastTD3%252C%2520FastSAC%252C%2520and%2520PPO%252C%2520simplicial%250Aembeddings%2520consistently%2520improve%2520sample%2520efficiency%2520and%2520final%2520performance%2520across%250Aa%2520variety%2520of%2520continuous-%2520and%2520discrete-control%2520environments%252C%2520without%2520any%2520loss%2520in%250Aruntime%2520speed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13704v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simplicial%20Embeddings%20Improve%20Sample%20Efficiency%20in%20Actor-Critic%20Agents&entry.906535625=Johan%20Obando-Ceron%20and%20Walter%20Mayor%20and%20Samuel%20Lavoie%20and%20Scott%20Fujimoto%20and%20Aaron%20Courville%20and%20Pablo%20Samuel%20Castro&entry.1292438233=%20%20Recent%20works%20have%20proposed%20accelerating%20the%20wall-clock%20training%20time%20of%0Aactor-critic%20methods%20via%20the%20use%20of%20large-scale%20environment%20parallelization%3B%0Aunfortunately%2C%20these%20can%20sometimes%20still%20require%20large%20number%20of%20environment%0Ainteractions%20to%20achieve%20a%20desired%20level%20of%20performance.%20Noting%20that%0Awell-structured%20representations%20can%20improve%20the%20generalization%20and%20sample%0Aefficiency%20of%20deep%20reinforcement%20learning%20%28RL%29%20agents%2C%20we%20propose%20the%20use%20of%0Asimplicial%20embeddings%3A%20lightweight%20representation%20layers%20that%20constrain%0Aembeddings%20to%20simplicial%20structures.%20This%20geometric%20inductive%20bias%20results%20in%0Asparse%20and%20discrete%20features%20that%20stabilize%20critic%20bootstrapping%20and%20strengthen%0Apolicy%20gradients.%20When%20applied%20to%20FastTD3%2C%20FastSAC%2C%20and%20PPO%2C%20simplicial%0Aembeddings%20consistently%20improve%20sample%20efficiency%20and%20final%20performance%20across%0Aa%20variety%20of%20continuous-%20and%20discrete-control%20environments%2C%20without%20any%20loss%20in%0Aruntime%20speed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13704v1&entry.124074799=Read"},
{"title": "Message Passing on the Edge: Towards Scalable and Expressive GNNs", "author": "Pablo Barcel\u00f3 and Fabian Jogl and Alexander Kozachinskiy and Matthias Lanzinger and Stefan Neumann and Crist\u00f3bal Rojas", "abstract": "  We propose EB-1WL, an edge-based color-refinement test, and a corresponding\nGNN architecture, EB-GNN. Our architecture is inspired by a classic triangle\ncounting algorithm by Chiba and Nishizeki, and explicitly uses triangles during\nmessage passing. We achieve the following results: (1)~EB-1WL is significantly\nmore expressive than 1-WL. Further, we provide a complete logical\ncharacterization of EB-1WL based on first-order logic, and matching\ndistinguishability results based on homomorphism counting. (2)~In an important\ndistinction from previous proposals for more expressive GNN architectures,\nEB-1WL and EB-GNN require near-linear time and memory on practical graph\nlearning tasks. (3)~Empirically, we show that EB-GNN is a highly-efficient\ngeneral-purpose architecture: It substantially outperforms simple MPNNs, and\nremains competitive with task-specialized GNNs while being significantly more\ncomputationally efficient.\n", "link": "http://arxiv.org/abs/2510.13615v1", "date": "2025-10-15", "relevancy": 2.4653, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.504}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4879}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4873}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Message%20Passing%20on%20the%20Edge%3A%20Towards%20Scalable%20and%20Expressive%20GNNs&body=Title%3A%20Message%20Passing%20on%20the%20Edge%3A%20Towards%20Scalable%20and%20Expressive%20GNNs%0AAuthor%3A%20Pablo%20Barcel%C3%B3%20and%20Fabian%20Jogl%20and%20Alexander%20Kozachinskiy%20and%20Matthias%20Lanzinger%20and%20Stefan%20Neumann%20and%20Crist%C3%B3bal%20Rojas%0AAbstract%3A%20%20%20We%20propose%20EB-1WL%2C%20an%20edge-based%20color-refinement%20test%2C%20and%20a%20corresponding%0AGNN%20architecture%2C%20EB-GNN.%20Our%20architecture%20is%20inspired%20by%20a%20classic%20triangle%0Acounting%20algorithm%20by%20Chiba%20and%20Nishizeki%2C%20and%20explicitly%20uses%20triangles%20during%0Amessage%20passing.%20We%20achieve%20the%20following%20results%3A%20%281%29~EB-1WL%20is%20significantly%0Amore%20expressive%20than%201-WL.%20Further%2C%20we%20provide%20a%20complete%20logical%0Acharacterization%20of%20EB-1WL%20based%20on%20first-order%20logic%2C%20and%20matching%0Adistinguishability%20results%20based%20on%20homomorphism%20counting.%20%282%29~In%20an%20important%0Adistinction%20from%20previous%20proposals%20for%20more%20expressive%20GNN%20architectures%2C%0AEB-1WL%20and%20EB-GNN%20require%20near-linear%20time%20and%20memory%20on%20practical%20graph%0Alearning%20tasks.%20%283%29~Empirically%2C%20we%20show%20that%20EB-GNN%20is%20a%20highly-efficient%0Ageneral-purpose%20architecture%3A%20It%20substantially%20outperforms%20simple%20MPNNs%2C%20and%0Aremains%20competitive%20with%20task-specialized%20GNNs%20while%20being%20significantly%20more%0Acomputationally%20efficient.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13615v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMessage%2520Passing%2520on%2520the%2520Edge%253A%2520Towards%2520Scalable%2520and%2520Expressive%2520GNNs%26entry.906535625%3DPablo%2520Barcel%25C3%25B3%2520and%2520Fabian%2520Jogl%2520and%2520Alexander%2520Kozachinskiy%2520and%2520Matthias%2520Lanzinger%2520and%2520Stefan%2520Neumann%2520and%2520Crist%25C3%25B3bal%2520Rojas%26entry.1292438233%3D%2520%2520We%2520propose%2520EB-1WL%252C%2520an%2520edge-based%2520color-refinement%2520test%252C%2520and%2520a%2520corresponding%250AGNN%2520architecture%252C%2520EB-GNN.%2520Our%2520architecture%2520is%2520inspired%2520by%2520a%2520classic%2520triangle%250Acounting%2520algorithm%2520by%2520Chiba%2520and%2520Nishizeki%252C%2520and%2520explicitly%2520uses%2520triangles%2520during%250Amessage%2520passing.%2520We%2520achieve%2520the%2520following%2520results%253A%2520%25281%2529~EB-1WL%2520is%2520significantly%250Amore%2520expressive%2520than%25201-WL.%2520Further%252C%2520we%2520provide%2520a%2520complete%2520logical%250Acharacterization%2520of%2520EB-1WL%2520based%2520on%2520first-order%2520logic%252C%2520and%2520matching%250Adistinguishability%2520results%2520based%2520on%2520homomorphism%2520counting.%2520%25282%2529~In%2520an%2520important%250Adistinction%2520from%2520previous%2520proposals%2520for%2520more%2520expressive%2520GNN%2520architectures%252C%250AEB-1WL%2520and%2520EB-GNN%2520require%2520near-linear%2520time%2520and%2520memory%2520on%2520practical%2520graph%250Alearning%2520tasks.%2520%25283%2529~Empirically%252C%2520we%2520show%2520that%2520EB-GNN%2520is%2520a%2520highly-efficient%250Ageneral-purpose%2520architecture%253A%2520It%2520substantially%2520outperforms%2520simple%2520MPNNs%252C%2520and%250Aremains%2520competitive%2520with%2520task-specialized%2520GNNs%2520while%2520being%2520significantly%2520more%250Acomputationally%2520efficient.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13615v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Message%20Passing%20on%20the%20Edge%3A%20Towards%20Scalable%20and%20Expressive%20GNNs&entry.906535625=Pablo%20Barcel%C3%B3%20and%20Fabian%20Jogl%20and%20Alexander%20Kozachinskiy%20and%20Matthias%20Lanzinger%20and%20Stefan%20Neumann%20and%20Crist%C3%B3bal%20Rojas&entry.1292438233=%20%20We%20propose%20EB-1WL%2C%20an%20edge-based%20color-refinement%20test%2C%20and%20a%20corresponding%0AGNN%20architecture%2C%20EB-GNN.%20Our%20architecture%20is%20inspired%20by%20a%20classic%20triangle%0Acounting%20algorithm%20by%20Chiba%20and%20Nishizeki%2C%20and%20explicitly%20uses%20triangles%20during%0Amessage%20passing.%20We%20achieve%20the%20following%20results%3A%20%281%29~EB-1WL%20is%20significantly%0Amore%20expressive%20than%201-WL.%20Further%2C%20we%20provide%20a%20complete%20logical%0Acharacterization%20of%20EB-1WL%20based%20on%20first-order%20logic%2C%20and%20matching%0Adistinguishability%20results%20based%20on%20homomorphism%20counting.%20%282%29~In%20an%20important%0Adistinction%20from%20previous%20proposals%20for%20more%20expressive%20GNN%20architectures%2C%0AEB-1WL%20and%20EB-GNN%20require%20near-linear%20time%20and%20memory%20on%20practical%20graph%0Alearning%20tasks.%20%283%29~Empirically%2C%20we%20show%20that%20EB-GNN%20is%20a%20highly-efficient%0Ageneral-purpose%20architecture%3A%20It%20substantially%20outperforms%20simple%20MPNNs%2C%20and%0Aremains%20competitive%20with%20task-specialized%20GNNs%20while%20being%20significantly%20more%0Acomputationally%20efficient.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13615v1&entry.124074799=Read"},
{"title": "Software System for Low-Cost, GUI-Based Microscopy Segmentation:\n  Algorithmic Implementation", "author": "Surajit Das and Pavel Zun", "abstract": "  This article presents a novel microscopy image analysis framework designed\nfor low-budget labs equipped with a standard CPU desktop. The Python-based\nprogram enables cytometric analysis of live, unstained cells in culture through\nan advanced computer vision and machine learning pipeline. Crucially, the\nframework operates on label-free data, requiring no manually annotated training\ndata or training phase. It is accessible via a user-friendly, cross-platform\nGUI that requires no programming skills, while also providing a scripting\ninterface for programmatic control and integration by developers. The\nend-to-end workflow performs semantic and instance segmentation, feature\nextraction, analysis, evaluation, and automated report generation. Its modular\narchitecture supports easy maintenance and flexible integration while\nsupporting both single-image and batch processing. Validated on several\nunstained cell types from the public dataset of livecells, the framework\ndemonstrates superior accuracy and reproducibility compared to contemporary\ntools like Cellpose and StarDist. Its competitive segmentation speed on a\nCPU-based platform highlights its significant potential for basic research and\nclinical application-particularly in cell transplantation for personalised\nmedicine and muscle regeneration therapies. The access to the application is\navailable for reproducibility.\n", "link": "http://arxiv.org/abs/2509.11354v3", "date": "2025-10-15", "relevancy": 2.4573, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4959}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4893}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4893}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Software%20System%20for%20Low-Cost%2C%20GUI-Based%20Microscopy%20Segmentation%3A%0A%20%20Algorithmic%20Implementation&body=Title%3A%20Software%20System%20for%20Low-Cost%2C%20GUI-Based%20Microscopy%20Segmentation%3A%0A%20%20Algorithmic%20Implementation%0AAuthor%3A%20Surajit%20Das%20and%20Pavel%20Zun%0AAbstract%3A%20%20%20This%20article%20presents%20a%20novel%20microscopy%20image%20analysis%20framework%20designed%0Afor%20low-budget%20labs%20equipped%20with%20a%20standard%20CPU%20desktop.%20The%20Python-based%0Aprogram%20enables%20cytometric%20analysis%20of%20live%2C%20unstained%20cells%20in%20culture%20through%0Aan%20advanced%20computer%20vision%20and%20machine%20learning%20pipeline.%20Crucially%2C%20the%0Aframework%20operates%20on%20label-free%20data%2C%20requiring%20no%20manually%20annotated%20training%0Adata%20or%20training%20phase.%20It%20is%20accessible%20via%20a%20user-friendly%2C%20cross-platform%0AGUI%20that%20requires%20no%20programming%20skills%2C%20while%20also%20providing%20a%20scripting%0Ainterface%20for%20programmatic%20control%20and%20integration%20by%20developers.%20The%0Aend-to-end%20workflow%20performs%20semantic%20and%20instance%20segmentation%2C%20feature%0Aextraction%2C%20analysis%2C%20evaluation%2C%20and%20automated%20report%20generation.%20Its%20modular%0Aarchitecture%20supports%20easy%20maintenance%20and%20flexible%20integration%20while%0Asupporting%20both%20single-image%20and%20batch%20processing.%20Validated%20on%20several%0Aunstained%20cell%20types%20from%20the%20public%20dataset%20of%20livecells%2C%20the%20framework%0Ademonstrates%20superior%20accuracy%20and%20reproducibility%20compared%20to%20contemporary%0Atools%20like%20Cellpose%20and%20StarDist.%20Its%20competitive%20segmentation%20speed%20on%20a%0ACPU-based%20platform%20highlights%20its%20significant%20potential%20for%20basic%20research%20and%0Aclinical%20application-particularly%20in%20cell%20transplantation%20for%20personalised%0Amedicine%20and%20muscle%20regeneration%20therapies.%20The%20access%20to%20the%20application%20is%0Aavailable%20for%20reproducibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.11354v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoftware%2520System%2520for%2520Low-Cost%252C%2520GUI-Based%2520Microscopy%2520Segmentation%253A%250A%2520%2520Algorithmic%2520Implementation%26entry.906535625%3DSurajit%2520Das%2520and%2520Pavel%2520Zun%26entry.1292438233%3D%2520%2520This%2520article%2520presents%2520a%2520novel%2520microscopy%2520image%2520analysis%2520framework%2520designed%250Afor%2520low-budget%2520labs%2520equipped%2520with%2520a%2520standard%2520CPU%2520desktop.%2520The%2520Python-based%250Aprogram%2520enables%2520cytometric%2520analysis%2520of%2520live%252C%2520unstained%2520cells%2520in%2520culture%2520through%250Aan%2520advanced%2520computer%2520vision%2520and%2520machine%2520learning%2520pipeline.%2520Crucially%252C%2520the%250Aframework%2520operates%2520on%2520label-free%2520data%252C%2520requiring%2520no%2520manually%2520annotated%2520training%250Adata%2520or%2520training%2520phase.%2520It%2520is%2520accessible%2520via%2520a%2520user-friendly%252C%2520cross-platform%250AGUI%2520that%2520requires%2520no%2520programming%2520skills%252C%2520while%2520also%2520providing%2520a%2520scripting%250Ainterface%2520for%2520programmatic%2520control%2520and%2520integration%2520by%2520developers.%2520The%250Aend-to-end%2520workflow%2520performs%2520semantic%2520and%2520instance%2520segmentation%252C%2520feature%250Aextraction%252C%2520analysis%252C%2520evaluation%252C%2520and%2520automated%2520report%2520generation.%2520Its%2520modular%250Aarchitecture%2520supports%2520easy%2520maintenance%2520and%2520flexible%2520integration%2520while%250Asupporting%2520both%2520single-image%2520and%2520batch%2520processing.%2520Validated%2520on%2520several%250Aunstained%2520cell%2520types%2520from%2520the%2520public%2520dataset%2520of%2520livecells%252C%2520the%2520framework%250Ademonstrates%2520superior%2520accuracy%2520and%2520reproducibility%2520compared%2520to%2520contemporary%250Atools%2520like%2520Cellpose%2520and%2520StarDist.%2520Its%2520competitive%2520segmentation%2520speed%2520on%2520a%250ACPU-based%2520platform%2520highlights%2520its%2520significant%2520potential%2520for%2520basic%2520research%2520and%250Aclinical%2520application-particularly%2520in%2520cell%2520transplantation%2520for%2520personalised%250Amedicine%2520and%2520muscle%2520regeneration%2520therapies.%2520The%2520access%2520to%2520the%2520application%2520is%250Aavailable%2520for%2520reproducibility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11354v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Software%20System%20for%20Low-Cost%2C%20GUI-Based%20Microscopy%20Segmentation%3A%0A%20%20Algorithmic%20Implementation&entry.906535625=Surajit%20Das%20and%20Pavel%20Zun&entry.1292438233=%20%20This%20article%20presents%20a%20novel%20microscopy%20image%20analysis%20framework%20designed%0Afor%20low-budget%20labs%20equipped%20with%20a%20standard%20CPU%20desktop.%20The%20Python-based%0Aprogram%20enables%20cytometric%20analysis%20of%20live%2C%20unstained%20cells%20in%20culture%20through%0Aan%20advanced%20computer%20vision%20and%20machine%20learning%20pipeline.%20Crucially%2C%20the%0Aframework%20operates%20on%20label-free%20data%2C%20requiring%20no%20manually%20annotated%20training%0Adata%20or%20training%20phase.%20It%20is%20accessible%20via%20a%20user-friendly%2C%20cross-platform%0AGUI%20that%20requires%20no%20programming%20skills%2C%20while%20also%20providing%20a%20scripting%0Ainterface%20for%20programmatic%20control%20and%20integration%20by%20developers.%20The%0Aend-to-end%20workflow%20performs%20semantic%20and%20instance%20segmentation%2C%20feature%0Aextraction%2C%20analysis%2C%20evaluation%2C%20and%20automated%20report%20generation.%20Its%20modular%0Aarchitecture%20supports%20easy%20maintenance%20and%20flexible%20integration%20while%0Asupporting%20both%20single-image%20and%20batch%20processing.%20Validated%20on%20several%0Aunstained%20cell%20types%20from%20the%20public%20dataset%20of%20livecells%2C%20the%20framework%0Ademonstrates%20superior%20accuracy%20and%20reproducibility%20compared%20to%20contemporary%0Atools%20like%20Cellpose%20and%20StarDist.%20Its%20competitive%20segmentation%20speed%20on%20a%0ACPU-based%20platform%20highlights%20its%20significant%20potential%20for%20basic%20research%20and%0Aclinical%20application-particularly%20in%20cell%20transplantation%20for%20personalised%0Amedicine%20and%20muscle%20regeneration%20therapies.%20The%20access%20to%20the%20application%20is%0Aavailable%20for%20reproducibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.11354v3&entry.124074799=Read"},
{"title": "On Pretraining for Project-Level Code Completion", "author": "Maksim Sapronov and Evgeniy Glukhov", "abstract": "  Repository-level pretraining is commonly used to enable large language models\nfor code to leverage codebase-wide context. This enhances their ability to\ngenerate accurate and context-aware code completions. In this work, we\ninvestigate how different repository-processing strategies affect in-context\nlearning in OpenCoder, a 1.5B-parameter model. We extend its context window\nfrom 4,096 to 16,384 tokens by training on additional 1B tokens of curated\nrepository-level data. Despite relying on a smaller dataset than competing\nmodels (which often use hundreds of billions of tokens), our model achieves\ncomparable performance on the Long Code Arena benchmark. We find that various\nrepository-processing techniques yield similarly strong results, with the\nprimary gain coming from adapting to a new rotary positional embedding (RoPE)\nscaling parameter. Finally, we show that a simpler file-level training approach\nat the original sequence length remains highly effective, opening up\nrepository-level code completion research to settings with more constrained\ndata and compute resources.\n", "link": "http://arxiv.org/abs/2510.13697v1", "date": "2025-10-15", "relevancy": 2.4559, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5068}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5068}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Pretraining%20for%20Project-Level%20Code%20Completion&body=Title%3A%20On%20Pretraining%20for%20Project-Level%20Code%20Completion%0AAuthor%3A%20Maksim%20Sapronov%20and%20Evgeniy%20Glukhov%0AAbstract%3A%20%20%20Repository-level%20pretraining%20is%20commonly%20used%20to%20enable%20large%20language%20models%0Afor%20code%20to%20leverage%20codebase-wide%20context.%20This%20enhances%20their%20ability%20to%0Agenerate%20accurate%20and%20context-aware%20code%20completions.%20In%20this%20work%2C%20we%0Ainvestigate%20how%20different%20repository-processing%20strategies%20affect%20in-context%0Alearning%20in%20OpenCoder%2C%20a%201.5B-parameter%20model.%20We%20extend%20its%20context%20window%0Afrom%204%2C096%20to%2016%2C384%20tokens%20by%20training%20on%20additional%201B%20tokens%20of%20curated%0Arepository-level%20data.%20Despite%20relying%20on%20a%20smaller%20dataset%20than%20competing%0Amodels%20%28which%20often%20use%20hundreds%20of%20billions%20of%20tokens%29%2C%20our%20model%20achieves%0Acomparable%20performance%20on%20the%20Long%20Code%20Arena%20benchmark.%20We%20find%20that%20various%0Arepository-processing%20techniques%20yield%20similarly%20strong%20results%2C%20with%20the%0Aprimary%20gain%20coming%20from%20adapting%20to%20a%20new%20rotary%20positional%20embedding%20%28RoPE%29%0Ascaling%20parameter.%20Finally%2C%20we%20show%20that%20a%20simpler%20file-level%20training%20approach%0Aat%20the%20original%20sequence%20length%20remains%20highly%20effective%2C%20opening%20up%0Arepository-level%20code%20completion%20research%20to%20settings%20with%20more%20constrained%0Adata%20and%20compute%20resources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13697v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Pretraining%2520for%2520Project-Level%2520Code%2520Completion%26entry.906535625%3DMaksim%2520Sapronov%2520and%2520Evgeniy%2520Glukhov%26entry.1292438233%3D%2520%2520Repository-level%2520pretraining%2520is%2520commonly%2520used%2520to%2520enable%2520large%2520language%2520models%250Afor%2520code%2520to%2520leverage%2520codebase-wide%2520context.%2520This%2520enhances%2520their%2520ability%2520to%250Agenerate%2520accurate%2520and%2520context-aware%2520code%2520completions.%2520In%2520this%2520work%252C%2520we%250Ainvestigate%2520how%2520different%2520repository-processing%2520strategies%2520affect%2520in-context%250Alearning%2520in%2520OpenCoder%252C%2520a%25201.5B-parameter%2520model.%2520We%2520extend%2520its%2520context%2520window%250Afrom%25204%252C096%2520to%252016%252C384%2520tokens%2520by%2520training%2520on%2520additional%25201B%2520tokens%2520of%2520curated%250Arepository-level%2520data.%2520Despite%2520relying%2520on%2520a%2520smaller%2520dataset%2520than%2520competing%250Amodels%2520%2528which%2520often%2520use%2520hundreds%2520of%2520billions%2520of%2520tokens%2529%252C%2520our%2520model%2520achieves%250Acomparable%2520performance%2520on%2520the%2520Long%2520Code%2520Arena%2520benchmark.%2520We%2520find%2520that%2520various%250Arepository-processing%2520techniques%2520yield%2520similarly%2520strong%2520results%252C%2520with%2520the%250Aprimary%2520gain%2520coming%2520from%2520adapting%2520to%2520a%2520new%2520rotary%2520positional%2520embedding%2520%2528RoPE%2529%250Ascaling%2520parameter.%2520Finally%252C%2520we%2520show%2520that%2520a%2520simpler%2520file-level%2520training%2520approach%250Aat%2520the%2520original%2520sequence%2520length%2520remains%2520highly%2520effective%252C%2520opening%2520up%250Arepository-level%2520code%2520completion%2520research%2520to%2520settings%2520with%2520more%2520constrained%250Adata%2520and%2520compute%2520resources.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13697v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Pretraining%20for%20Project-Level%20Code%20Completion&entry.906535625=Maksim%20Sapronov%20and%20Evgeniy%20Glukhov&entry.1292438233=%20%20Repository-level%20pretraining%20is%20commonly%20used%20to%20enable%20large%20language%20models%0Afor%20code%20to%20leverage%20codebase-wide%20context.%20This%20enhances%20their%20ability%20to%0Agenerate%20accurate%20and%20context-aware%20code%20completions.%20In%20this%20work%2C%20we%0Ainvestigate%20how%20different%20repository-processing%20strategies%20affect%20in-context%0Alearning%20in%20OpenCoder%2C%20a%201.5B-parameter%20model.%20We%20extend%20its%20context%20window%0Afrom%204%2C096%20to%2016%2C384%20tokens%20by%20training%20on%20additional%201B%20tokens%20of%20curated%0Arepository-level%20data.%20Despite%20relying%20on%20a%20smaller%20dataset%20than%20competing%0Amodels%20%28which%20often%20use%20hundreds%20of%20billions%20of%20tokens%29%2C%20our%20model%20achieves%0Acomparable%20performance%20on%20the%20Long%20Code%20Arena%20benchmark.%20We%20find%20that%20various%0Arepository-processing%20techniques%20yield%20similarly%20strong%20results%2C%20with%20the%0Aprimary%20gain%20coming%20from%20adapting%20to%20a%20new%20rotary%20positional%20embedding%20%28RoPE%29%0Ascaling%20parameter.%20Finally%2C%20we%20show%20that%20a%20simpler%20file-level%20training%20approach%0Aat%20the%20original%20sequence%20length%20remains%20highly%20effective%2C%20opening%20up%0Arepository-level%20code%20completion%20research%20to%20settings%20with%20more%20constrained%0Adata%20and%20compute%20resources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13697v1&entry.124074799=Read"},
{"title": "FALCON: Fine-grained Activation Manipulation by Contrastive Orthogonal\n  Unalignment for Large Language Model", "author": "Jinwei Hu and Zhenglin Huang and Xiangyu Yin and Wenjie Ruan and Guangliang Cheng and Yi Dong and Xiaowei Huang", "abstract": "  Large language models have been widely applied, but can inadvertently encode\nsensitive or harmful information, raising significant safety concerns. Machine\nunlearning has emerged to alleviate this concern; however, existing\ntraining-time unlearning approaches, relying on coarse-grained loss\ncombinations, have limitations in precisely separating knowledge and balancing\nremoval effectiveness with model utility. In contrast, we propose Fine-grained\nActivation manipuLation by Contrastive Orthogonal uNalignment (FALCON), a novel\nrepresentation-guided unlearning approach that leverages information-theoretic\nguidance for efficient parameter selection, employs contrastive mechanisms to\nenhance representation separation, and projects conflict gradients onto\northogonal subspaces to resolve conflicts between forgetting and retention\nobjectives. Extensive experiments demonstrate that FALCON achieves superior\nunlearning effectiveness while maintaining model utility, exhibiting robust\nresistance against knowledge recovery attempts.\n", "link": "http://arxiv.org/abs/2502.01472v2", "date": "2025-10-15", "relevancy": 2.4341, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4916}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4876}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4812}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FALCON%3A%20Fine-grained%20Activation%20Manipulation%20by%20Contrastive%20Orthogonal%0A%20%20Unalignment%20for%20Large%20Language%20Model&body=Title%3A%20FALCON%3A%20Fine-grained%20Activation%20Manipulation%20by%20Contrastive%20Orthogonal%0A%20%20Unalignment%20for%20Large%20Language%20Model%0AAuthor%3A%20Jinwei%20Hu%20and%20Zhenglin%20Huang%20and%20Xiangyu%20Yin%20and%20Wenjie%20Ruan%20and%20Guangliang%20Cheng%20and%20Yi%20Dong%20and%20Xiaowei%20Huang%0AAbstract%3A%20%20%20Large%20language%20models%20have%20been%20widely%20applied%2C%20but%20can%20inadvertently%20encode%0Asensitive%20or%20harmful%20information%2C%20raising%20significant%20safety%20concerns.%20Machine%0Aunlearning%20has%20emerged%20to%20alleviate%20this%20concern%3B%20however%2C%20existing%0Atraining-time%20unlearning%20approaches%2C%20relying%20on%20coarse-grained%20loss%0Acombinations%2C%20have%20limitations%20in%20precisely%20separating%20knowledge%20and%20balancing%0Aremoval%20effectiveness%20with%20model%20utility.%20In%20contrast%2C%20we%20propose%20Fine-grained%0AActivation%20manipuLation%20by%20Contrastive%20Orthogonal%20uNalignment%20%28FALCON%29%2C%20a%20novel%0Arepresentation-guided%20unlearning%20approach%20that%20leverages%20information-theoretic%0Aguidance%20for%20efficient%20parameter%20selection%2C%20employs%20contrastive%20mechanisms%20to%0Aenhance%20representation%20separation%2C%20and%20projects%20conflict%20gradients%20onto%0Aorthogonal%20subspaces%20to%20resolve%20conflicts%20between%20forgetting%20and%20retention%0Aobjectives.%20Extensive%20experiments%20demonstrate%20that%20FALCON%20achieves%20superior%0Aunlearning%20effectiveness%20while%20maintaining%20model%20utility%2C%20exhibiting%20robust%0Aresistance%20against%20knowledge%20recovery%20attempts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.01472v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFALCON%253A%2520Fine-grained%2520Activation%2520Manipulation%2520by%2520Contrastive%2520Orthogonal%250A%2520%2520Unalignment%2520for%2520Large%2520Language%2520Model%26entry.906535625%3DJinwei%2520Hu%2520and%2520Zhenglin%2520Huang%2520and%2520Xiangyu%2520Yin%2520and%2520Wenjie%2520Ruan%2520and%2520Guangliang%2520Cheng%2520and%2520Yi%2520Dong%2520and%2520Xiaowei%2520Huang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520have%2520been%2520widely%2520applied%252C%2520but%2520can%2520inadvertently%2520encode%250Asensitive%2520or%2520harmful%2520information%252C%2520raising%2520significant%2520safety%2520concerns.%2520Machine%250Aunlearning%2520has%2520emerged%2520to%2520alleviate%2520this%2520concern%253B%2520however%252C%2520existing%250Atraining-time%2520unlearning%2520approaches%252C%2520relying%2520on%2520coarse-grained%2520loss%250Acombinations%252C%2520have%2520limitations%2520in%2520precisely%2520separating%2520knowledge%2520and%2520balancing%250Aremoval%2520effectiveness%2520with%2520model%2520utility.%2520In%2520contrast%252C%2520we%2520propose%2520Fine-grained%250AActivation%2520manipuLation%2520by%2520Contrastive%2520Orthogonal%2520uNalignment%2520%2528FALCON%2529%252C%2520a%2520novel%250Arepresentation-guided%2520unlearning%2520approach%2520that%2520leverages%2520information-theoretic%250Aguidance%2520for%2520efficient%2520parameter%2520selection%252C%2520employs%2520contrastive%2520mechanisms%2520to%250Aenhance%2520representation%2520separation%252C%2520and%2520projects%2520conflict%2520gradients%2520onto%250Aorthogonal%2520subspaces%2520to%2520resolve%2520conflicts%2520between%2520forgetting%2520and%2520retention%250Aobjectives.%2520Extensive%2520experiments%2520demonstrate%2520that%2520FALCON%2520achieves%2520superior%250Aunlearning%2520effectiveness%2520while%2520maintaining%2520model%2520utility%252C%2520exhibiting%2520robust%250Aresistance%2520against%2520knowledge%2520recovery%2520attempts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.01472v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FALCON%3A%20Fine-grained%20Activation%20Manipulation%20by%20Contrastive%20Orthogonal%0A%20%20Unalignment%20for%20Large%20Language%20Model&entry.906535625=Jinwei%20Hu%20and%20Zhenglin%20Huang%20and%20Xiangyu%20Yin%20and%20Wenjie%20Ruan%20and%20Guangliang%20Cheng%20and%20Yi%20Dong%20and%20Xiaowei%20Huang&entry.1292438233=%20%20Large%20language%20models%20have%20been%20widely%20applied%2C%20but%20can%20inadvertently%20encode%0Asensitive%20or%20harmful%20information%2C%20raising%20significant%20safety%20concerns.%20Machine%0Aunlearning%20has%20emerged%20to%20alleviate%20this%20concern%3B%20however%2C%20existing%0Atraining-time%20unlearning%20approaches%2C%20relying%20on%20coarse-grained%20loss%0Acombinations%2C%20have%20limitations%20in%20precisely%20separating%20knowledge%20and%20balancing%0Aremoval%20effectiveness%20with%20model%20utility.%20In%20contrast%2C%20we%20propose%20Fine-grained%0AActivation%20manipuLation%20by%20Contrastive%20Orthogonal%20uNalignment%20%28FALCON%29%2C%20a%20novel%0Arepresentation-guided%20unlearning%20approach%20that%20leverages%20information-theoretic%0Aguidance%20for%20efficient%20parameter%20selection%2C%20employs%20contrastive%20mechanisms%20to%0Aenhance%20representation%20separation%2C%20and%20projects%20conflict%20gradients%20onto%0Aorthogonal%20subspaces%20to%20resolve%20conflicts%20between%20forgetting%20and%20retention%0Aobjectives.%20Extensive%20experiments%20demonstrate%20that%20FALCON%20achieves%20superior%0Aunlearning%20effectiveness%20while%20maintaining%20model%20utility%2C%20exhibiting%20robust%0Aresistance%20against%20knowledge%20recovery%20attempts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.01472v2&entry.124074799=Read"},
{"title": "Confidence as a Reward: Transforming LLMs into Reward Models", "author": "He Du and Bowen Li and Chengxing Xie and Chang Gao and Kai Chen and Dacheng Tao", "abstract": "  Reward models can significantly enhance the reasoning capabilities of large\nlanguage models (LLMs), but they typically require extensive curated data and\ncostly training. To mitigate these challenges, training-free approaches such as\nLLM-as-a-Judge leverage the intrinsic reasoning abilities of LLMs to evaluate\nresponses, achieving promising results. Recent works have also indicated that\nmodel confidence can serve effectively as a reward metric, distinguishing\nbetween chain-of-thought (CoT) and non-CoT paths. However, the concept of using\nconfidence as a reward has not been comprehensively studied. In this work, we\nsystematically investigate Confidence-as-a-Reward (CRew), a simple yet powerful\ntraining-free method that utilizes token-level confidence in the model's final\nanswers as a proxy for reward, especially suitable for close-ended tasks.\nThrough extensive experiments on mathematical reasoning tasks, we demonstrate\nthat CRew outperforms existing training-free reward approaches on the MATH500\nand RewardMATH benchmarks, and even surpasses most trained reward models. We\nfurther identify a strong correlation between CRew scores and the actual\nreasoning performance of the model. Additionally, we find that CRew can\neffectively filter high-quality training data. Building upon these insights, we\npropose CRew-DPO, a training strategy that constructs preference data from\nconfidence scores combined with correctness signals. Finetuning with CRew-DPO\nfurther enhances the model's judging capabilities and consistently outperforms\nexisting self-training methods.\n", "link": "http://arxiv.org/abs/2510.13501v1", "date": "2025-10-15", "relevancy": 2.4047, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4852}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4788}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4788}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Confidence%20as%20a%20Reward%3A%20Transforming%20LLMs%20into%20Reward%20Models&body=Title%3A%20Confidence%20as%20a%20Reward%3A%20Transforming%20LLMs%20into%20Reward%20Models%0AAuthor%3A%20He%20Du%20and%20Bowen%20Li%20and%20Chengxing%20Xie%20and%20Chang%20Gao%20and%20Kai%20Chen%20and%20Dacheng%20Tao%0AAbstract%3A%20%20%20Reward%20models%20can%20significantly%20enhance%20the%20reasoning%20capabilities%20of%20large%0Alanguage%20models%20%28LLMs%29%2C%20but%20they%20typically%20require%20extensive%20curated%20data%20and%0Acostly%20training.%20To%20mitigate%20these%20challenges%2C%20training-free%20approaches%20such%20as%0ALLM-as-a-Judge%20leverage%20the%20intrinsic%20reasoning%20abilities%20of%20LLMs%20to%20evaluate%0Aresponses%2C%20achieving%20promising%20results.%20Recent%20works%20have%20also%20indicated%20that%0Amodel%20confidence%20can%20serve%20effectively%20as%20a%20reward%20metric%2C%20distinguishing%0Abetween%20chain-of-thought%20%28CoT%29%20and%20non-CoT%20paths.%20However%2C%20the%20concept%20of%20using%0Aconfidence%20as%20a%20reward%20has%20not%20been%20comprehensively%20studied.%20In%20this%20work%2C%20we%0Asystematically%20investigate%20Confidence-as-a-Reward%20%28CRew%29%2C%20a%20simple%20yet%20powerful%0Atraining-free%20method%20that%20utilizes%20token-level%20confidence%20in%20the%20model%27s%20final%0Aanswers%20as%20a%20proxy%20for%20reward%2C%20especially%20suitable%20for%20close-ended%20tasks.%0AThrough%20extensive%20experiments%20on%20mathematical%20reasoning%20tasks%2C%20we%20demonstrate%0Athat%20CRew%20outperforms%20existing%20training-free%20reward%20approaches%20on%20the%20MATH500%0Aand%20RewardMATH%20benchmarks%2C%20and%20even%20surpasses%20most%20trained%20reward%20models.%20We%0Afurther%20identify%20a%20strong%20correlation%20between%20CRew%20scores%20and%20the%20actual%0Areasoning%20performance%20of%20the%20model.%20Additionally%2C%20we%20find%20that%20CRew%20can%0Aeffectively%20filter%20high-quality%20training%20data.%20Building%20upon%20these%20insights%2C%20we%0Apropose%20CRew-DPO%2C%20a%20training%20strategy%20that%20constructs%20preference%20data%20from%0Aconfidence%20scores%20combined%20with%20correctness%20signals.%20Finetuning%20with%20CRew-DPO%0Afurther%20enhances%20the%20model%27s%20judging%20capabilities%20and%20consistently%20outperforms%0Aexisting%20self-training%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13501v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConfidence%2520as%2520a%2520Reward%253A%2520Transforming%2520LLMs%2520into%2520Reward%2520Models%26entry.906535625%3DHe%2520Du%2520and%2520Bowen%2520Li%2520and%2520Chengxing%2520Xie%2520and%2520Chang%2520Gao%2520and%2520Kai%2520Chen%2520and%2520Dacheng%2520Tao%26entry.1292438233%3D%2520%2520Reward%2520models%2520can%2520significantly%2520enhance%2520the%2520reasoning%2520capabilities%2520of%2520large%250Alanguage%2520models%2520%2528LLMs%2529%252C%2520but%2520they%2520typically%2520require%2520extensive%2520curated%2520data%2520and%250Acostly%2520training.%2520To%2520mitigate%2520these%2520challenges%252C%2520training-free%2520approaches%2520such%2520as%250ALLM-as-a-Judge%2520leverage%2520the%2520intrinsic%2520reasoning%2520abilities%2520of%2520LLMs%2520to%2520evaluate%250Aresponses%252C%2520achieving%2520promising%2520results.%2520Recent%2520works%2520have%2520also%2520indicated%2520that%250Amodel%2520confidence%2520can%2520serve%2520effectively%2520as%2520a%2520reward%2520metric%252C%2520distinguishing%250Abetween%2520chain-of-thought%2520%2528CoT%2529%2520and%2520non-CoT%2520paths.%2520However%252C%2520the%2520concept%2520of%2520using%250Aconfidence%2520as%2520a%2520reward%2520has%2520not%2520been%2520comprehensively%2520studied.%2520In%2520this%2520work%252C%2520we%250Asystematically%2520investigate%2520Confidence-as-a-Reward%2520%2528CRew%2529%252C%2520a%2520simple%2520yet%2520powerful%250Atraining-free%2520method%2520that%2520utilizes%2520token-level%2520confidence%2520in%2520the%2520model%2527s%2520final%250Aanswers%2520as%2520a%2520proxy%2520for%2520reward%252C%2520especially%2520suitable%2520for%2520close-ended%2520tasks.%250AThrough%2520extensive%2520experiments%2520on%2520mathematical%2520reasoning%2520tasks%252C%2520we%2520demonstrate%250Athat%2520CRew%2520outperforms%2520existing%2520training-free%2520reward%2520approaches%2520on%2520the%2520MATH500%250Aand%2520RewardMATH%2520benchmarks%252C%2520and%2520even%2520surpasses%2520most%2520trained%2520reward%2520models.%2520We%250Afurther%2520identify%2520a%2520strong%2520correlation%2520between%2520CRew%2520scores%2520and%2520the%2520actual%250Areasoning%2520performance%2520of%2520the%2520model.%2520Additionally%252C%2520we%2520find%2520that%2520CRew%2520can%250Aeffectively%2520filter%2520high-quality%2520training%2520data.%2520Building%2520upon%2520these%2520insights%252C%2520we%250Apropose%2520CRew-DPO%252C%2520a%2520training%2520strategy%2520that%2520constructs%2520preference%2520data%2520from%250Aconfidence%2520scores%2520combined%2520with%2520correctness%2520signals.%2520Finetuning%2520with%2520CRew-DPO%250Afurther%2520enhances%2520the%2520model%2527s%2520judging%2520capabilities%2520and%2520consistently%2520outperforms%250Aexisting%2520self-training%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13501v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Confidence%20as%20a%20Reward%3A%20Transforming%20LLMs%20into%20Reward%20Models&entry.906535625=He%20Du%20and%20Bowen%20Li%20and%20Chengxing%20Xie%20and%20Chang%20Gao%20and%20Kai%20Chen%20and%20Dacheng%20Tao&entry.1292438233=%20%20Reward%20models%20can%20significantly%20enhance%20the%20reasoning%20capabilities%20of%20large%0Alanguage%20models%20%28LLMs%29%2C%20but%20they%20typically%20require%20extensive%20curated%20data%20and%0Acostly%20training.%20To%20mitigate%20these%20challenges%2C%20training-free%20approaches%20such%20as%0ALLM-as-a-Judge%20leverage%20the%20intrinsic%20reasoning%20abilities%20of%20LLMs%20to%20evaluate%0Aresponses%2C%20achieving%20promising%20results.%20Recent%20works%20have%20also%20indicated%20that%0Amodel%20confidence%20can%20serve%20effectively%20as%20a%20reward%20metric%2C%20distinguishing%0Abetween%20chain-of-thought%20%28CoT%29%20and%20non-CoT%20paths.%20However%2C%20the%20concept%20of%20using%0Aconfidence%20as%20a%20reward%20has%20not%20been%20comprehensively%20studied.%20In%20this%20work%2C%20we%0Asystematically%20investigate%20Confidence-as-a-Reward%20%28CRew%29%2C%20a%20simple%20yet%20powerful%0Atraining-free%20method%20that%20utilizes%20token-level%20confidence%20in%20the%20model%27s%20final%0Aanswers%20as%20a%20proxy%20for%20reward%2C%20especially%20suitable%20for%20close-ended%20tasks.%0AThrough%20extensive%20experiments%20on%20mathematical%20reasoning%20tasks%2C%20we%20demonstrate%0Athat%20CRew%20outperforms%20existing%20training-free%20reward%20approaches%20on%20the%20MATH500%0Aand%20RewardMATH%20benchmarks%2C%20and%20even%20surpasses%20most%20trained%20reward%20models.%20We%0Afurther%20identify%20a%20strong%20correlation%20between%20CRew%20scores%20and%20the%20actual%0Areasoning%20performance%20of%20the%20model.%20Additionally%2C%20we%20find%20that%20CRew%20can%0Aeffectively%20filter%20high-quality%20training%20data.%20Building%20upon%20these%20insights%2C%20we%0Apropose%20CRew-DPO%2C%20a%20training%20strategy%20that%20constructs%20preference%20data%20from%0Aconfidence%20scores%20combined%20with%20correctness%20signals.%20Finetuning%20with%20CRew-DPO%0Afurther%20enhances%20the%20model%27s%20judging%20capabilities%20and%20consistently%20outperforms%0Aexisting%20self-training%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13501v1&entry.124074799=Read"},
{"title": "Adam or Gauss-Newton? A Comparative Study In Terms of Basis Alignment\n  and SGD Noise", "author": "Bingbin Liu and Rachit Bansal and Depen Morwani and Nikhil Vyas and David Alvarez-Melis and Sham M. Kakade", "abstract": "  Diagonal preconditioners are computationally feasible approximate to\nsecond-order optimizers, which have shown significant promise in accelerating\ntraining of deep learning models. Two predominant approaches are based on Adam\nand Gauss-Newton (GN) methods: the former leverages statistics of current\ngradients and is the de-factor optimizers for neural networks, and the latter\nuses the diagonal elements of the Gauss-Newton matrix and underpins some of the\nrecent diagonal optimizers such as Sophia.\n  In this work, we compare these two diagonal preconditioning methods through\nthe lens of two key factors: the choice of basis in the preconditioner, and the\nimpact of gradient noise from mini-batching. To gain insights, we analyze these\noptimizers on quadratic objectives and logistic regression under all four\nquadrants. We show that regardless of the basis, there exist instances where\nAdam outperforms both GN$^{-1}$ and GN$^{-1/2}$ in full-batch settings.\nConversely, in the stochastic regime, Adam behaves similarly to GN$^{-1/2}$ for\nlinear regression under a Gaussian data assumption. These theoretical results\nare supported by empirical studies on both convex and non-convex objectives.\n", "link": "http://arxiv.org/abs/2510.13680v1", "date": "2025-10-15", "relevancy": 2.3972, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4864}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4804}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adam%20or%20Gauss-Newton%3F%20A%20Comparative%20Study%20In%20Terms%20of%20Basis%20Alignment%0A%20%20and%20SGD%20Noise&body=Title%3A%20Adam%20or%20Gauss-Newton%3F%20A%20Comparative%20Study%20In%20Terms%20of%20Basis%20Alignment%0A%20%20and%20SGD%20Noise%0AAuthor%3A%20Bingbin%20Liu%20and%20Rachit%20Bansal%20and%20Depen%20Morwani%20and%20Nikhil%20Vyas%20and%20David%20Alvarez-Melis%20and%20Sham%20M.%20Kakade%0AAbstract%3A%20%20%20Diagonal%20preconditioners%20are%20computationally%20feasible%20approximate%20to%0Asecond-order%20optimizers%2C%20which%20have%20shown%20significant%20promise%20in%20accelerating%0Atraining%20of%20deep%20learning%20models.%20Two%20predominant%20approaches%20are%20based%20on%20Adam%0Aand%20Gauss-Newton%20%28GN%29%20methods%3A%20the%20former%20leverages%20statistics%20of%20current%0Agradients%20and%20is%20the%20de-factor%20optimizers%20for%20neural%20networks%2C%20and%20the%20latter%0Auses%20the%20diagonal%20elements%20of%20the%20Gauss-Newton%20matrix%20and%20underpins%20some%20of%20the%0Arecent%20diagonal%20optimizers%20such%20as%20Sophia.%0A%20%20In%20this%20work%2C%20we%20compare%20these%20two%20diagonal%20preconditioning%20methods%20through%0Athe%20lens%20of%20two%20key%20factors%3A%20the%20choice%20of%20basis%20in%20the%20preconditioner%2C%20and%20the%0Aimpact%20of%20gradient%20noise%20from%20mini-batching.%20To%20gain%20insights%2C%20we%20analyze%20these%0Aoptimizers%20on%20quadratic%20objectives%20and%20logistic%20regression%20under%20all%20four%0Aquadrants.%20We%20show%20that%20regardless%20of%20the%20basis%2C%20there%20exist%20instances%20where%0AAdam%20outperforms%20both%20GN%24%5E%7B-1%7D%24%20and%20GN%24%5E%7B-1/2%7D%24%20in%20full-batch%20settings.%0AConversely%2C%20in%20the%20stochastic%20regime%2C%20Adam%20behaves%20similarly%20to%20GN%24%5E%7B-1/2%7D%24%20for%0Alinear%20regression%20under%20a%20Gaussian%20data%20assumption.%20These%20theoretical%20results%0Aare%20supported%20by%20empirical%20studies%20on%20both%20convex%20and%20non-convex%20objectives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13680v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdam%2520or%2520Gauss-Newton%253F%2520A%2520Comparative%2520Study%2520In%2520Terms%2520of%2520Basis%2520Alignment%250A%2520%2520and%2520SGD%2520Noise%26entry.906535625%3DBingbin%2520Liu%2520and%2520Rachit%2520Bansal%2520and%2520Depen%2520Morwani%2520and%2520Nikhil%2520Vyas%2520and%2520David%2520Alvarez-Melis%2520and%2520Sham%2520M.%2520Kakade%26entry.1292438233%3D%2520%2520Diagonal%2520preconditioners%2520are%2520computationally%2520feasible%2520approximate%2520to%250Asecond-order%2520optimizers%252C%2520which%2520have%2520shown%2520significant%2520promise%2520in%2520accelerating%250Atraining%2520of%2520deep%2520learning%2520models.%2520Two%2520predominant%2520approaches%2520are%2520based%2520on%2520Adam%250Aand%2520Gauss-Newton%2520%2528GN%2529%2520methods%253A%2520the%2520former%2520leverages%2520statistics%2520of%2520current%250Agradients%2520and%2520is%2520the%2520de-factor%2520optimizers%2520for%2520neural%2520networks%252C%2520and%2520the%2520latter%250Auses%2520the%2520diagonal%2520elements%2520of%2520the%2520Gauss-Newton%2520matrix%2520and%2520underpins%2520some%2520of%2520the%250Arecent%2520diagonal%2520optimizers%2520such%2520as%2520Sophia.%250A%2520%2520In%2520this%2520work%252C%2520we%2520compare%2520these%2520two%2520diagonal%2520preconditioning%2520methods%2520through%250Athe%2520lens%2520of%2520two%2520key%2520factors%253A%2520the%2520choice%2520of%2520basis%2520in%2520the%2520preconditioner%252C%2520and%2520the%250Aimpact%2520of%2520gradient%2520noise%2520from%2520mini-batching.%2520To%2520gain%2520insights%252C%2520we%2520analyze%2520these%250Aoptimizers%2520on%2520quadratic%2520objectives%2520and%2520logistic%2520regression%2520under%2520all%2520four%250Aquadrants.%2520We%2520show%2520that%2520regardless%2520of%2520the%2520basis%252C%2520there%2520exist%2520instances%2520where%250AAdam%2520outperforms%2520both%2520GN%2524%255E%257B-1%257D%2524%2520and%2520GN%2524%255E%257B-1/2%257D%2524%2520in%2520full-batch%2520settings.%250AConversely%252C%2520in%2520the%2520stochastic%2520regime%252C%2520Adam%2520behaves%2520similarly%2520to%2520GN%2524%255E%257B-1/2%257D%2524%2520for%250Alinear%2520regression%2520under%2520a%2520Gaussian%2520data%2520assumption.%2520These%2520theoretical%2520results%250Aare%2520supported%2520by%2520empirical%2520studies%2520on%2520both%2520convex%2520and%2520non-convex%2520objectives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13680v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adam%20or%20Gauss-Newton%3F%20A%20Comparative%20Study%20In%20Terms%20of%20Basis%20Alignment%0A%20%20and%20SGD%20Noise&entry.906535625=Bingbin%20Liu%20and%20Rachit%20Bansal%20and%20Depen%20Morwani%20and%20Nikhil%20Vyas%20and%20David%20Alvarez-Melis%20and%20Sham%20M.%20Kakade&entry.1292438233=%20%20Diagonal%20preconditioners%20are%20computationally%20feasible%20approximate%20to%0Asecond-order%20optimizers%2C%20which%20have%20shown%20significant%20promise%20in%20accelerating%0Atraining%20of%20deep%20learning%20models.%20Two%20predominant%20approaches%20are%20based%20on%20Adam%0Aand%20Gauss-Newton%20%28GN%29%20methods%3A%20the%20former%20leverages%20statistics%20of%20current%0Agradients%20and%20is%20the%20de-factor%20optimizers%20for%20neural%20networks%2C%20and%20the%20latter%0Auses%20the%20diagonal%20elements%20of%20the%20Gauss-Newton%20matrix%20and%20underpins%20some%20of%20the%0Arecent%20diagonal%20optimizers%20such%20as%20Sophia.%0A%20%20In%20this%20work%2C%20we%20compare%20these%20two%20diagonal%20preconditioning%20methods%20through%0Athe%20lens%20of%20two%20key%20factors%3A%20the%20choice%20of%20basis%20in%20the%20preconditioner%2C%20and%20the%0Aimpact%20of%20gradient%20noise%20from%20mini-batching.%20To%20gain%20insights%2C%20we%20analyze%20these%0Aoptimizers%20on%20quadratic%20objectives%20and%20logistic%20regression%20under%20all%20four%0Aquadrants.%20We%20show%20that%20regardless%20of%20the%20basis%2C%20there%20exist%20instances%20where%0AAdam%20outperforms%20both%20GN%24%5E%7B-1%7D%24%20and%20GN%24%5E%7B-1/2%7D%24%20in%20full-batch%20settings.%0AConversely%2C%20in%20the%20stochastic%20regime%2C%20Adam%20behaves%20similarly%20to%20GN%24%5E%7B-1/2%7D%24%20for%0Alinear%20regression%20under%20a%20Gaussian%20data%20assumption.%20These%20theoretical%20results%0Aare%20supported%20by%20empirical%20studies%20on%20both%20convex%20and%20non-convex%20objectives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13680v1&entry.124074799=Read"},
{"title": "Your AI, Not Your View: The Bias of LLMs in Investment Analysis", "author": "Hoyoung Lee and Junhyuk Seo and Suhwan Park and Junhyeong Lee and Wonbin Ahn and Chanyeol Choi and Alejandro Lopez-Lira and Yongjae Lee", "abstract": "  In finance, Large Language Models (LLMs) face frequent knowledge conflicts\narising from discrepancies between their pre-trained parametric knowledge and\nreal-time market data. These conflicts are especially problematic in real-world\ninvestment services, where a model's inherent biases can misalign with\ninstitutional objectives, leading to unreliable recommendations. Despite this\nrisk, the intrinsic investment biases of LLMs remain underexplored. We propose\nan experimental framework to investigate emergent behaviors in such conflict\nscenarios, offering a quantitative analysis of bias in LLM-based investment\nanalysis. Using hypothetical scenarios with balanced and imbalanced arguments,\nwe extract the latent biases of models and measure their persistence. Our\nanalysis, centered on sector, size, and momentum, reveals distinct,\nmodel-specific biases. Across most models, a tendency to prefer technology\nstocks, large-cap stocks, and contrarian strategies is observed. These\nfoundational biases often escalate into confirmation bias, causing models to\ncling to initial judgments even when faced with increasing counter-evidence. A\npublic leaderboard benchmarking bias across a broader set of models is\navailable at https://linqalpha.com/leaderboard\n", "link": "http://arxiv.org/abs/2507.20957v3", "date": "2025-10-15", "relevancy": 2.3793, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4795}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4795}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4686}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Your%20AI%2C%20Not%20Your%20View%3A%20The%20Bias%20of%20LLMs%20in%20Investment%20Analysis&body=Title%3A%20Your%20AI%2C%20Not%20Your%20View%3A%20The%20Bias%20of%20LLMs%20in%20Investment%20Analysis%0AAuthor%3A%20Hoyoung%20Lee%20and%20Junhyuk%20Seo%20and%20Suhwan%20Park%20and%20Junhyeong%20Lee%20and%20Wonbin%20Ahn%20and%20Chanyeol%20Choi%20and%20Alejandro%20Lopez-Lira%20and%20Yongjae%20Lee%0AAbstract%3A%20%20%20In%20finance%2C%20Large%20Language%20Models%20%28LLMs%29%20face%20frequent%20knowledge%20conflicts%0Aarising%20from%20discrepancies%20between%20their%20pre-trained%20parametric%20knowledge%20and%0Areal-time%20market%20data.%20These%20conflicts%20are%20especially%20problematic%20in%20real-world%0Ainvestment%20services%2C%20where%20a%20model%27s%20inherent%20biases%20can%20misalign%20with%0Ainstitutional%20objectives%2C%20leading%20to%20unreliable%20recommendations.%20Despite%20this%0Arisk%2C%20the%20intrinsic%20investment%20biases%20of%20LLMs%20remain%20underexplored.%20We%20propose%0Aan%20experimental%20framework%20to%20investigate%20emergent%20behaviors%20in%20such%20conflict%0Ascenarios%2C%20offering%20a%20quantitative%20analysis%20of%20bias%20in%20LLM-based%20investment%0Aanalysis.%20Using%20hypothetical%20scenarios%20with%20balanced%20and%20imbalanced%20arguments%2C%0Awe%20extract%20the%20latent%20biases%20of%20models%20and%20measure%20their%20persistence.%20Our%0Aanalysis%2C%20centered%20on%20sector%2C%20size%2C%20and%20momentum%2C%20reveals%20distinct%2C%0Amodel-specific%20biases.%20Across%20most%20models%2C%20a%20tendency%20to%20prefer%20technology%0Astocks%2C%20large-cap%20stocks%2C%20and%20contrarian%20strategies%20is%20observed.%20These%0Afoundational%20biases%20often%20escalate%20into%20confirmation%20bias%2C%20causing%20models%20to%0Acling%20to%20initial%20judgments%20even%20when%20faced%20with%20increasing%20counter-evidence.%20A%0Apublic%20leaderboard%20benchmarking%20bias%20across%20a%20broader%20set%20of%20models%20is%0Aavailable%20at%20https%3A//linqalpha.com/leaderboard%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20957v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYour%2520AI%252C%2520Not%2520Your%2520View%253A%2520The%2520Bias%2520of%2520LLMs%2520in%2520Investment%2520Analysis%26entry.906535625%3DHoyoung%2520Lee%2520and%2520Junhyuk%2520Seo%2520and%2520Suhwan%2520Park%2520and%2520Junhyeong%2520Lee%2520and%2520Wonbin%2520Ahn%2520and%2520Chanyeol%2520Choi%2520and%2520Alejandro%2520Lopez-Lira%2520and%2520Yongjae%2520Lee%26entry.1292438233%3D%2520%2520In%2520finance%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520face%2520frequent%2520knowledge%2520conflicts%250Aarising%2520from%2520discrepancies%2520between%2520their%2520pre-trained%2520parametric%2520knowledge%2520and%250Areal-time%2520market%2520data.%2520These%2520conflicts%2520are%2520especially%2520problematic%2520in%2520real-world%250Ainvestment%2520services%252C%2520where%2520a%2520model%2527s%2520inherent%2520biases%2520can%2520misalign%2520with%250Ainstitutional%2520objectives%252C%2520leading%2520to%2520unreliable%2520recommendations.%2520Despite%2520this%250Arisk%252C%2520the%2520intrinsic%2520investment%2520biases%2520of%2520LLMs%2520remain%2520underexplored.%2520We%2520propose%250Aan%2520experimental%2520framework%2520to%2520investigate%2520emergent%2520behaviors%2520in%2520such%2520conflict%250Ascenarios%252C%2520offering%2520a%2520quantitative%2520analysis%2520of%2520bias%2520in%2520LLM-based%2520investment%250Aanalysis.%2520Using%2520hypothetical%2520scenarios%2520with%2520balanced%2520and%2520imbalanced%2520arguments%252C%250Awe%2520extract%2520the%2520latent%2520biases%2520of%2520models%2520and%2520measure%2520their%2520persistence.%2520Our%250Aanalysis%252C%2520centered%2520on%2520sector%252C%2520size%252C%2520and%2520momentum%252C%2520reveals%2520distinct%252C%250Amodel-specific%2520biases.%2520Across%2520most%2520models%252C%2520a%2520tendency%2520to%2520prefer%2520technology%250Astocks%252C%2520large-cap%2520stocks%252C%2520and%2520contrarian%2520strategies%2520is%2520observed.%2520These%250Afoundational%2520biases%2520often%2520escalate%2520into%2520confirmation%2520bias%252C%2520causing%2520models%2520to%250Acling%2520to%2520initial%2520judgments%2520even%2520when%2520faced%2520with%2520increasing%2520counter-evidence.%2520A%250Apublic%2520leaderboard%2520benchmarking%2520bias%2520across%2520a%2520broader%2520set%2520of%2520models%2520is%250Aavailable%2520at%2520https%253A//linqalpha.com/leaderboard%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20957v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Your%20AI%2C%20Not%20Your%20View%3A%20The%20Bias%20of%20LLMs%20in%20Investment%20Analysis&entry.906535625=Hoyoung%20Lee%20and%20Junhyuk%20Seo%20and%20Suhwan%20Park%20and%20Junhyeong%20Lee%20and%20Wonbin%20Ahn%20and%20Chanyeol%20Choi%20and%20Alejandro%20Lopez-Lira%20and%20Yongjae%20Lee&entry.1292438233=%20%20In%20finance%2C%20Large%20Language%20Models%20%28LLMs%29%20face%20frequent%20knowledge%20conflicts%0Aarising%20from%20discrepancies%20between%20their%20pre-trained%20parametric%20knowledge%20and%0Areal-time%20market%20data.%20These%20conflicts%20are%20especially%20problematic%20in%20real-world%0Ainvestment%20services%2C%20where%20a%20model%27s%20inherent%20biases%20can%20misalign%20with%0Ainstitutional%20objectives%2C%20leading%20to%20unreliable%20recommendations.%20Despite%20this%0Arisk%2C%20the%20intrinsic%20investment%20biases%20of%20LLMs%20remain%20underexplored.%20We%20propose%0Aan%20experimental%20framework%20to%20investigate%20emergent%20behaviors%20in%20such%20conflict%0Ascenarios%2C%20offering%20a%20quantitative%20analysis%20of%20bias%20in%20LLM-based%20investment%0Aanalysis.%20Using%20hypothetical%20scenarios%20with%20balanced%20and%20imbalanced%20arguments%2C%0Awe%20extract%20the%20latent%20biases%20of%20models%20and%20measure%20their%20persistence.%20Our%0Aanalysis%2C%20centered%20on%20sector%2C%20size%2C%20and%20momentum%2C%20reveals%20distinct%2C%0Amodel-specific%20biases.%20Across%20most%20models%2C%20a%20tendency%20to%20prefer%20technology%0Astocks%2C%20large-cap%20stocks%2C%20and%20contrarian%20strategies%20is%20observed.%20These%0Afoundational%20biases%20often%20escalate%20into%20confirmation%20bias%2C%20causing%20models%20to%0Acling%20to%20initial%20judgments%20even%20when%20faced%20with%20increasing%20counter-evidence.%20A%0Apublic%20leaderboard%20benchmarking%20bias%20across%20a%20broader%20set%20of%20models%20is%0Aavailable%20at%20https%3A//linqalpha.com/leaderboard%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20957v3&entry.124074799=Read"},
{"title": "I Have No Mouth, and I Must Rhyme: Uncovering Internal Phonetic\n  Representations in LLaMA 3.2", "author": "Oliver McLaughlin and Arjun Khurana and Jack Merullo", "abstract": "  Large language models demonstrate proficiency on phonetic tasks, such as\nrhyming, without explicit phonetic or auditory grounding. In this work, we\ninvestigate how \\verb|Llama-3.2-1B-Instruct| represents token-level phonetic\ninformation. Our results suggest that Llama uses a rich internal model of\nphonemes to complete phonetic tasks. We provide evidence for high-level\norganization of phoneme representations in its latent space. In doing so, we\nalso identify a ``phoneme mover head\" which promotes phonetic information\nduring rhyming tasks. We visualize the output space of this head and find that,\nwhile notable differences exist, Llama learns a model of vowels similar to the\nstandard IPA vowel chart for humans, despite receiving no direct supervision to\ndo so.\n", "link": "http://arxiv.org/abs/2508.02527v2", "date": "2025-10-15", "relevancy": 2.3717, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.479}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.479}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20I%20Have%20No%20Mouth%2C%20and%20I%20Must%20Rhyme%3A%20Uncovering%20Internal%20Phonetic%0A%20%20Representations%20in%20LLaMA%203.2&body=Title%3A%20I%20Have%20No%20Mouth%2C%20and%20I%20Must%20Rhyme%3A%20Uncovering%20Internal%20Phonetic%0A%20%20Representations%20in%20LLaMA%203.2%0AAuthor%3A%20Oliver%20McLaughlin%20and%20Arjun%20Khurana%20and%20Jack%20Merullo%0AAbstract%3A%20%20%20Large%20language%20models%20demonstrate%20proficiency%20on%20phonetic%20tasks%2C%20such%20as%0Arhyming%2C%20without%20explicit%20phonetic%20or%20auditory%20grounding.%20In%20this%20work%2C%20we%0Ainvestigate%20how%20%5Cverb%7CLlama-3.2-1B-Instruct%7C%20represents%20token-level%20phonetic%0Ainformation.%20Our%20results%20suggest%20that%20Llama%20uses%20a%20rich%20internal%20model%20of%0Aphonemes%20to%20complete%20phonetic%20tasks.%20We%20provide%20evidence%20for%20high-level%0Aorganization%20of%20phoneme%20representations%20in%20its%20latent%20space.%20In%20doing%20so%2C%20we%0Aalso%20identify%20a%20%60%60phoneme%20mover%20head%22%20which%20promotes%20phonetic%20information%0Aduring%20rhyming%20tasks.%20We%20visualize%20the%20output%20space%20of%20this%20head%20and%20find%20that%2C%0Awhile%20notable%20differences%20exist%2C%20Llama%20learns%20a%20model%20of%20vowels%20similar%20to%20the%0Astandard%20IPA%20vowel%20chart%20for%20humans%2C%20despite%20receiving%20no%20direct%20supervision%20to%0Ado%20so.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02527v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DI%2520Have%2520No%2520Mouth%252C%2520and%2520I%2520Must%2520Rhyme%253A%2520Uncovering%2520Internal%2520Phonetic%250A%2520%2520Representations%2520in%2520LLaMA%25203.2%26entry.906535625%3DOliver%2520McLaughlin%2520and%2520Arjun%2520Khurana%2520and%2520Jack%2520Merullo%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520demonstrate%2520proficiency%2520on%2520phonetic%2520tasks%252C%2520such%2520as%250Arhyming%252C%2520without%2520explicit%2520phonetic%2520or%2520auditory%2520grounding.%2520In%2520this%2520work%252C%2520we%250Ainvestigate%2520how%2520%255Cverb%257CLlama-3.2-1B-Instruct%257C%2520represents%2520token-level%2520phonetic%250Ainformation.%2520Our%2520results%2520suggest%2520that%2520Llama%2520uses%2520a%2520rich%2520internal%2520model%2520of%250Aphonemes%2520to%2520complete%2520phonetic%2520tasks.%2520We%2520provide%2520evidence%2520for%2520high-level%250Aorganization%2520of%2520phoneme%2520representations%2520in%2520its%2520latent%2520space.%2520In%2520doing%2520so%252C%2520we%250Aalso%2520identify%2520a%2520%2560%2560phoneme%2520mover%2520head%2522%2520which%2520promotes%2520phonetic%2520information%250Aduring%2520rhyming%2520tasks.%2520We%2520visualize%2520the%2520output%2520space%2520of%2520this%2520head%2520and%2520find%2520that%252C%250Awhile%2520notable%2520differences%2520exist%252C%2520Llama%2520learns%2520a%2520model%2520of%2520vowels%2520similar%2520to%2520the%250Astandard%2520IPA%2520vowel%2520chart%2520for%2520humans%252C%2520despite%2520receiving%2520no%2520direct%2520supervision%2520to%250Ado%2520so.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02527v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=I%20Have%20No%20Mouth%2C%20and%20I%20Must%20Rhyme%3A%20Uncovering%20Internal%20Phonetic%0A%20%20Representations%20in%20LLaMA%203.2&entry.906535625=Oliver%20McLaughlin%20and%20Arjun%20Khurana%20and%20Jack%20Merullo&entry.1292438233=%20%20Large%20language%20models%20demonstrate%20proficiency%20on%20phonetic%20tasks%2C%20such%20as%0Arhyming%2C%20without%20explicit%20phonetic%20or%20auditory%20grounding.%20In%20this%20work%2C%20we%0Ainvestigate%20how%20%5Cverb%7CLlama-3.2-1B-Instruct%7C%20represents%20token-level%20phonetic%0Ainformation.%20Our%20results%20suggest%20that%20Llama%20uses%20a%20rich%20internal%20model%20of%0Aphonemes%20to%20complete%20phonetic%20tasks.%20We%20provide%20evidence%20for%20high-level%0Aorganization%20of%20phoneme%20representations%20in%20its%20latent%20space.%20In%20doing%20so%2C%20we%0Aalso%20identify%20a%20%60%60phoneme%20mover%20head%22%20which%20promotes%20phonetic%20information%0Aduring%20rhyming%20tasks.%20We%20visualize%20the%20output%20space%20of%20this%20head%20and%20find%20that%2C%0Awhile%20notable%20differences%20exist%2C%20Llama%20learns%20a%20model%20of%20vowels%20similar%20to%20the%0Astandard%20IPA%20vowel%20chart%20for%20humans%2C%20despite%20receiving%20no%20direct%20supervision%20to%0Ado%20so.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02527v2&entry.124074799=Read"},
{"title": "SSL4Eco: A Global Seasonal Dataset for Geospatial Foundation Models in\n  Ecology", "author": "Elena Plekhanova and Damien Robert and Johannes Dollinger and Emilia Arens and Philipp Brun and Jan Dirk Wegner and Niklaus Zimmermann", "abstract": "  With the exacerbation of the biodiversity and climate crises, macroecological\npursuits such as global biodiversity mapping become more urgent. Remote sensing\noffers a wealth of Earth observation data for ecological studies, but the\nscarcity of labeled datasets remains a major challenge. Recently,\nself-supervised learning has enabled learning representations from unlabeled\ndata, triggering the development of pretrained geospatial models with\ngeneralizable features. However, these models are often trained on datasets\nbiased toward areas of high human activity, leaving entire ecological regions\nunderrepresented. Additionally, while some datasets attempt to address\nseasonality through multi-date imagery, they typically follow calendar seasons\nrather than local phenological cycles. To better capture vegetation seasonality\nat a global scale, we propose a simple phenology-informed sampling strategy and\nintroduce corresponding SSL4Eco, a multi-date Sentinel-2 dataset, on which we\ntrain an existing model with a season-contrastive objective. We compare\nrepresentations learned from SSL4Eco against other datasets on diverse\necological downstream tasks and demonstrate that our straightforward sampling\nmethod consistently improves representation quality, highlighting the\nimportance of dataset construction. The model pretrained on SSL4Eco reaches\nstate of the art performance on 7 out of 8 downstream tasks spanning\n(multi-label) classification and regression. We release our code, data, and\nmodel weights to support macroecological and computer vision research at\nhttps://github.com/PlekhanovaElena/ssl4eco.\n", "link": "http://arxiv.org/abs/2504.18256v2", "date": "2025-10-15", "relevancy": 2.3646, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4741}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4741}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SSL4Eco%3A%20A%20Global%20Seasonal%20Dataset%20for%20Geospatial%20Foundation%20Models%20in%0A%20%20Ecology&body=Title%3A%20SSL4Eco%3A%20A%20Global%20Seasonal%20Dataset%20for%20Geospatial%20Foundation%20Models%20in%0A%20%20Ecology%0AAuthor%3A%20Elena%20Plekhanova%20and%20Damien%20Robert%20and%20Johannes%20Dollinger%20and%20Emilia%20Arens%20and%20Philipp%20Brun%20and%20Jan%20Dirk%20Wegner%20and%20Niklaus%20Zimmermann%0AAbstract%3A%20%20%20With%20the%20exacerbation%20of%20the%20biodiversity%20and%20climate%20crises%2C%20macroecological%0Apursuits%20such%20as%20global%20biodiversity%20mapping%20become%20more%20urgent.%20Remote%20sensing%0Aoffers%20a%20wealth%20of%20Earth%20observation%20data%20for%20ecological%20studies%2C%20but%20the%0Ascarcity%20of%20labeled%20datasets%20remains%20a%20major%20challenge.%20Recently%2C%0Aself-supervised%20learning%20has%20enabled%20learning%20representations%20from%20unlabeled%0Adata%2C%20triggering%20the%20development%20of%20pretrained%20geospatial%20models%20with%0Ageneralizable%20features.%20However%2C%20these%20models%20are%20often%20trained%20on%20datasets%0Abiased%20toward%20areas%20of%20high%20human%20activity%2C%20leaving%20entire%20ecological%20regions%0Aunderrepresented.%20Additionally%2C%20while%20some%20datasets%20attempt%20to%20address%0Aseasonality%20through%20multi-date%20imagery%2C%20they%20typically%20follow%20calendar%20seasons%0Arather%20than%20local%20phenological%20cycles.%20To%20better%20capture%20vegetation%20seasonality%0Aat%20a%20global%20scale%2C%20we%20propose%20a%20simple%20phenology-informed%20sampling%20strategy%20and%0Aintroduce%20corresponding%20SSL4Eco%2C%20a%20multi-date%20Sentinel-2%20dataset%2C%20on%20which%20we%0Atrain%20an%20existing%20model%20with%20a%20season-contrastive%20objective.%20We%20compare%0Arepresentations%20learned%20from%20SSL4Eco%20against%20other%20datasets%20on%20diverse%0Aecological%20downstream%20tasks%20and%20demonstrate%20that%20our%20straightforward%20sampling%0Amethod%20consistently%20improves%20representation%20quality%2C%20highlighting%20the%0Aimportance%20of%20dataset%20construction.%20The%20model%20pretrained%20on%20SSL4Eco%20reaches%0Astate%20of%20the%20art%20performance%20on%207%20out%20of%208%20downstream%20tasks%20spanning%0A%28multi-label%29%20classification%20and%20regression.%20We%20release%20our%20code%2C%20data%2C%20and%0Amodel%20weights%20to%20support%20macroecological%20and%20computer%20vision%20research%20at%0Ahttps%3A//github.com/PlekhanovaElena/ssl4eco.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18256v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSSL4Eco%253A%2520A%2520Global%2520Seasonal%2520Dataset%2520for%2520Geospatial%2520Foundation%2520Models%2520in%250A%2520%2520Ecology%26entry.906535625%3DElena%2520Plekhanova%2520and%2520Damien%2520Robert%2520and%2520Johannes%2520Dollinger%2520and%2520Emilia%2520Arens%2520and%2520Philipp%2520Brun%2520and%2520Jan%2520Dirk%2520Wegner%2520and%2520Niklaus%2520Zimmermann%26entry.1292438233%3D%2520%2520With%2520the%2520exacerbation%2520of%2520the%2520biodiversity%2520and%2520climate%2520crises%252C%2520macroecological%250Apursuits%2520such%2520as%2520global%2520biodiversity%2520mapping%2520become%2520more%2520urgent.%2520Remote%2520sensing%250Aoffers%2520a%2520wealth%2520of%2520Earth%2520observation%2520data%2520for%2520ecological%2520studies%252C%2520but%2520the%250Ascarcity%2520of%2520labeled%2520datasets%2520remains%2520a%2520major%2520challenge.%2520Recently%252C%250Aself-supervised%2520learning%2520has%2520enabled%2520learning%2520representations%2520from%2520unlabeled%250Adata%252C%2520triggering%2520the%2520development%2520of%2520pretrained%2520geospatial%2520models%2520with%250Ageneralizable%2520features.%2520However%252C%2520these%2520models%2520are%2520often%2520trained%2520on%2520datasets%250Abiased%2520toward%2520areas%2520of%2520high%2520human%2520activity%252C%2520leaving%2520entire%2520ecological%2520regions%250Aunderrepresented.%2520Additionally%252C%2520while%2520some%2520datasets%2520attempt%2520to%2520address%250Aseasonality%2520through%2520multi-date%2520imagery%252C%2520they%2520typically%2520follow%2520calendar%2520seasons%250Arather%2520than%2520local%2520phenological%2520cycles.%2520To%2520better%2520capture%2520vegetation%2520seasonality%250Aat%2520a%2520global%2520scale%252C%2520we%2520propose%2520a%2520simple%2520phenology-informed%2520sampling%2520strategy%2520and%250Aintroduce%2520corresponding%2520SSL4Eco%252C%2520a%2520multi-date%2520Sentinel-2%2520dataset%252C%2520on%2520which%2520we%250Atrain%2520an%2520existing%2520model%2520with%2520a%2520season-contrastive%2520objective.%2520We%2520compare%250Arepresentations%2520learned%2520from%2520SSL4Eco%2520against%2520other%2520datasets%2520on%2520diverse%250Aecological%2520downstream%2520tasks%2520and%2520demonstrate%2520that%2520our%2520straightforward%2520sampling%250Amethod%2520consistently%2520improves%2520representation%2520quality%252C%2520highlighting%2520the%250Aimportance%2520of%2520dataset%2520construction.%2520The%2520model%2520pretrained%2520on%2520SSL4Eco%2520reaches%250Astate%2520of%2520the%2520art%2520performance%2520on%25207%2520out%2520of%25208%2520downstream%2520tasks%2520spanning%250A%2528multi-label%2529%2520classification%2520and%2520regression.%2520We%2520release%2520our%2520code%252C%2520data%252C%2520and%250Amodel%2520weights%2520to%2520support%2520macroecological%2520and%2520computer%2520vision%2520research%2520at%250Ahttps%253A//github.com/PlekhanovaElena/ssl4eco.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18256v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SSL4Eco%3A%20A%20Global%20Seasonal%20Dataset%20for%20Geospatial%20Foundation%20Models%20in%0A%20%20Ecology&entry.906535625=Elena%20Plekhanova%20and%20Damien%20Robert%20and%20Johannes%20Dollinger%20and%20Emilia%20Arens%20and%20Philipp%20Brun%20and%20Jan%20Dirk%20Wegner%20and%20Niklaus%20Zimmermann&entry.1292438233=%20%20With%20the%20exacerbation%20of%20the%20biodiversity%20and%20climate%20crises%2C%20macroecological%0Apursuits%20such%20as%20global%20biodiversity%20mapping%20become%20more%20urgent.%20Remote%20sensing%0Aoffers%20a%20wealth%20of%20Earth%20observation%20data%20for%20ecological%20studies%2C%20but%20the%0Ascarcity%20of%20labeled%20datasets%20remains%20a%20major%20challenge.%20Recently%2C%0Aself-supervised%20learning%20has%20enabled%20learning%20representations%20from%20unlabeled%0Adata%2C%20triggering%20the%20development%20of%20pretrained%20geospatial%20models%20with%0Ageneralizable%20features.%20However%2C%20these%20models%20are%20often%20trained%20on%20datasets%0Abiased%20toward%20areas%20of%20high%20human%20activity%2C%20leaving%20entire%20ecological%20regions%0Aunderrepresented.%20Additionally%2C%20while%20some%20datasets%20attempt%20to%20address%0Aseasonality%20through%20multi-date%20imagery%2C%20they%20typically%20follow%20calendar%20seasons%0Arather%20than%20local%20phenological%20cycles.%20To%20better%20capture%20vegetation%20seasonality%0Aat%20a%20global%20scale%2C%20we%20propose%20a%20simple%20phenology-informed%20sampling%20strategy%20and%0Aintroduce%20corresponding%20SSL4Eco%2C%20a%20multi-date%20Sentinel-2%20dataset%2C%20on%20which%20we%0Atrain%20an%20existing%20model%20with%20a%20season-contrastive%20objective.%20We%20compare%0Arepresentations%20learned%20from%20SSL4Eco%20against%20other%20datasets%20on%20diverse%0Aecological%20downstream%20tasks%20and%20demonstrate%20that%20our%20straightforward%20sampling%0Amethod%20consistently%20improves%20representation%20quality%2C%20highlighting%20the%0Aimportance%20of%20dataset%20construction.%20The%20model%20pretrained%20on%20SSL4Eco%20reaches%0Astate%20of%20the%20art%20performance%20on%207%20out%20of%208%20downstream%20tasks%20spanning%0A%28multi-label%29%20classification%20and%20regression.%20We%20release%20our%20code%2C%20data%2C%20and%0Amodel%20weights%20to%20support%20macroecological%20and%20computer%20vision%20research%20at%0Ahttps%3A//github.com/PlekhanovaElena/ssl4eco.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18256v2&entry.124074799=Read"},
{"title": "Rectify and Align GPS Points to Parking Spots via Rank-1 Constraint", "author": "Jiaxing Deng and Junbiao Pang and Zhicheng Wang and Haitao Yu", "abstract": "  Parking spots are essential components, providing vital mobile resources for\nresidents in a city. Accurate Global Positioning System (GPS) points of parking\nspots are the core data for subsequent applications,e.g., parking management,\nparking policy, and urban development. However, high-rise buildings tend to\ncause GPS points to drift from the actual locations of parking spots; besides,\nthe standard lower-cost GPS equipment itself has a certain location error.\nTherefore, it is a non-trivial task to correct a few wrong GPS points from a\nlarge number of parking spots in an unsupervised approach. In this paper,\nmotivated by the physical constraints of parking spots (i.e., parking spots are\nparallel to the sides of roads), we propose an unsupervised low-rank method to\neffectively rectify errors in GPS points and further align them to the parking\nspots in a unified framework. The proposed unconventional rectification and\nalignment method is simple and yet effective for any type of GPS point errors.\nExtensive experiments demonstrate the superiority of the proposed method to\nsolve a practical problem. The data set and the code are publicly accessible\nat:https://github.com/pangjunbiao/ITS-Parking-spots-Dataset.\n", "link": "http://arxiv.org/abs/2510.13439v1", "date": "2025-10-15", "relevancy": 2.3609, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4924}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4623}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rectify%20and%20Align%20GPS%20Points%20to%20Parking%20Spots%20via%20Rank-1%20Constraint&body=Title%3A%20Rectify%20and%20Align%20GPS%20Points%20to%20Parking%20Spots%20via%20Rank-1%20Constraint%0AAuthor%3A%20Jiaxing%20Deng%20and%20Junbiao%20Pang%20and%20Zhicheng%20Wang%20and%20Haitao%20Yu%0AAbstract%3A%20%20%20Parking%20spots%20are%20essential%20components%2C%20providing%20vital%20mobile%20resources%20for%0Aresidents%20in%20a%20city.%20Accurate%20Global%20Positioning%20System%20%28GPS%29%20points%20of%20parking%0Aspots%20are%20the%20core%20data%20for%20subsequent%20applications%2Ce.g.%2C%20parking%20management%2C%0Aparking%20policy%2C%20and%20urban%20development.%20However%2C%20high-rise%20buildings%20tend%20to%0Acause%20GPS%20points%20to%20drift%20from%20the%20actual%20locations%20of%20parking%20spots%3B%20besides%2C%0Athe%20standard%20lower-cost%20GPS%20equipment%20itself%20has%20a%20certain%20location%20error.%0ATherefore%2C%20it%20is%20a%20non-trivial%20task%20to%20correct%20a%20few%20wrong%20GPS%20points%20from%20a%0Alarge%20number%20of%20parking%20spots%20in%20an%20unsupervised%20approach.%20In%20this%20paper%2C%0Amotivated%20by%20the%20physical%20constraints%20of%20parking%20spots%20%28i.e.%2C%20parking%20spots%20are%0Aparallel%20to%20the%20sides%20of%20roads%29%2C%20we%20propose%20an%20unsupervised%20low-rank%20method%20to%0Aeffectively%20rectify%20errors%20in%20GPS%20points%20and%20further%20align%20them%20to%20the%20parking%0Aspots%20in%20a%20unified%20framework.%20The%20proposed%20unconventional%20rectification%20and%0Aalignment%20method%20is%20simple%20and%20yet%20effective%20for%20any%20type%20of%20GPS%20point%20errors.%0AExtensive%20experiments%20demonstrate%20the%20superiority%20of%20the%20proposed%20method%20to%0Asolve%20a%20practical%20problem.%20The%20data%20set%20and%20the%20code%20are%20publicly%20accessible%0Aat%3Ahttps%3A//github.com/pangjunbiao/ITS-Parking-spots-Dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13439v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRectify%2520and%2520Align%2520GPS%2520Points%2520to%2520Parking%2520Spots%2520via%2520Rank-1%2520Constraint%26entry.906535625%3DJiaxing%2520Deng%2520and%2520Junbiao%2520Pang%2520and%2520Zhicheng%2520Wang%2520and%2520Haitao%2520Yu%26entry.1292438233%3D%2520%2520Parking%2520spots%2520are%2520essential%2520components%252C%2520providing%2520vital%2520mobile%2520resources%2520for%250Aresidents%2520in%2520a%2520city.%2520Accurate%2520Global%2520Positioning%2520System%2520%2528GPS%2529%2520points%2520of%2520parking%250Aspots%2520are%2520the%2520core%2520data%2520for%2520subsequent%2520applications%252Ce.g.%252C%2520parking%2520management%252C%250Aparking%2520policy%252C%2520and%2520urban%2520development.%2520However%252C%2520high-rise%2520buildings%2520tend%2520to%250Acause%2520GPS%2520points%2520to%2520drift%2520from%2520the%2520actual%2520locations%2520of%2520parking%2520spots%253B%2520besides%252C%250Athe%2520standard%2520lower-cost%2520GPS%2520equipment%2520itself%2520has%2520a%2520certain%2520location%2520error.%250ATherefore%252C%2520it%2520is%2520a%2520non-trivial%2520task%2520to%2520correct%2520a%2520few%2520wrong%2520GPS%2520points%2520from%2520a%250Alarge%2520number%2520of%2520parking%2520spots%2520in%2520an%2520unsupervised%2520approach.%2520In%2520this%2520paper%252C%250Amotivated%2520by%2520the%2520physical%2520constraints%2520of%2520parking%2520spots%2520%2528i.e.%252C%2520parking%2520spots%2520are%250Aparallel%2520to%2520the%2520sides%2520of%2520roads%2529%252C%2520we%2520propose%2520an%2520unsupervised%2520low-rank%2520method%2520to%250Aeffectively%2520rectify%2520errors%2520in%2520GPS%2520points%2520and%2520further%2520align%2520them%2520to%2520the%2520parking%250Aspots%2520in%2520a%2520unified%2520framework.%2520The%2520proposed%2520unconventional%2520rectification%2520and%250Aalignment%2520method%2520is%2520simple%2520and%2520yet%2520effective%2520for%2520any%2520type%2520of%2520GPS%2520point%2520errors.%250AExtensive%2520experiments%2520demonstrate%2520the%2520superiority%2520of%2520the%2520proposed%2520method%2520to%250Asolve%2520a%2520practical%2520problem.%2520The%2520data%2520set%2520and%2520the%2520code%2520are%2520publicly%2520accessible%250Aat%253Ahttps%253A//github.com/pangjunbiao/ITS-Parking-spots-Dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13439v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rectify%20and%20Align%20GPS%20Points%20to%20Parking%20Spots%20via%20Rank-1%20Constraint&entry.906535625=Jiaxing%20Deng%20and%20Junbiao%20Pang%20and%20Zhicheng%20Wang%20and%20Haitao%20Yu&entry.1292438233=%20%20Parking%20spots%20are%20essential%20components%2C%20providing%20vital%20mobile%20resources%20for%0Aresidents%20in%20a%20city.%20Accurate%20Global%20Positioning%20System%20%28GPS%29%20points%20of%20parking%0Aspots%20are%20the%20core%20data%20for%20subsequent%20applications%2Ce.g.%2C%20parking%20management%2C%0Aparking%20policy%2C%20and%20urban%20development.%20However%2C%20high-rise%20buildings%20tend%20to%0Acause%20GPS%20points%20to%20drift%20from%20the%20actual%20locations%20of%20parking%20spots%3B%20besides%2C%0Athe%20standard%20lower-cost%20GPS%20equipment%20itself%20has%20a%20certain%20location%20error.%0ATherefore%2C%20it%20is%20a%20non-trivial%20task%20to%20correct%20a%20few%20wrong%20GPS%20points%20from%20a%0Alarge%20number%20of%20parking%20spots%20in%20an%20unsupervised%20approach.%20In%20this%20paper%2C%0Amotivated%20by%20the%20physical%20constraints%20of%20parking%20spots%20%28i.e.%2C%20parking%20spots%20are%0Aparallel%20to%20the%20sides%20of%20roads%29%2C%20we%20propose%20an%20unsupervised%20low-rank%20method%20to%0Aeffectively%20rectify%20errors%20in%20GPS%20points%20and%20further%20align%20them%20to%20the%20parking%0Aspots%20in%20a%20unified%20framework.%20The%20proposed%20unconventional%20rectification%20and%0Aalignment%20method%20is%20simple%20and%20yet%20effective%20for%20any%20type%20of%20GPS%20point%20errors.%0AExtensive%20experiments%20demonstrate%20the%20superiority%20of%20the%20proposed%20method%20to%0Asolve%20a%20practical%20problem.%20The%20data%20set%20and%20the%20code%20are%20publicly%20accessible%0Aat%3Ahttps%3A//github.com/pangjunbiao/ITS-Parking-spots-Dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13439v1&entry.124074799=Read"},
{"title": "An efficient approach with theoretical guarantees to simultaneously\n  reconstruct activity and attenuation sinogram for TOF-PET", "author": "Liyang Hu and Chong Chen", "abstract": "  In positron emission tomography (PET), it is indispensable to perform\nattenuation correction in order to obtain the quantitatively accurate activity\nmap (tracer distribution) in the body. Generally, this is carried out based on\nthe estimated attenuation map obtained from computed tomography or magnetic\nresonance imaging. However, except for errors in the attenuation correction\nfactors obtained, the additional scan not only brings in new radiation doses\nand/or increases the scanning time but also leads to severe misalignment\ninduced by various motions during and between the two sequential scans. To\naddress these issues, based on maximum likelihood estimation, we propose a new\nmathematical model for simultaneously reconstructing the activity and\nattenuation sinogram from the time-of-flight (TOF)-PET emission data only.\nParticularly, we make full use of the exclusively exponential form for the\nattenuation correction factors, and consider the constraint of a total amount\nof the activity in some mask region in the proposed model. Furthermore, we\nprove its well-posedness, including the existence, uniqueness and stability of\nthe solution. We propose an alternating update algorithm to solve the model,\nand also analyze its convergence. Finally, numerical experiments with various\nTOF-PET emission data demonstrate that the proposed method is of numerical\nconvergence and robust to noise, and outperforms some state-of-the-art methods\nin terms of accuracy and efficiency, and has the capability of autonomous\nattenuation correction.\n", "link": "http://arxiv.org/abs/2510.13562v1", "date": "2025-10-15", "relevancy": 2.3545, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4815}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4671}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4641}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20efficient%20approach%20with%20theoretical%20guarantees%20to%20simultaneously%0A%20%20reconstruct%20activity%20and%20attenuation%20sinogram%20for%20TOF-PET&body=Title%3A%20An%20efficient%20approach%20with%20theoretical%20guarantees%20to%20simultaneously%0A%20%20reconstruct%20activity%20and%20attenuation%20sinogram%20for%20TOF-PET%0AAuthor%3A%20Liyang%20Hu%20and%20Chong%20Chen%0AAbstract%3A%20%20%20In%20positron%20emission%20tomography%20%28PET%29%2C%20it%20is%20indispensable%20to%20perform%0Aattenuation%20correction%20in%20order%20to%20obtain%20the%20quantitatively%20accurate%20activity%0Amap%20%28tracer%20distribution%29%20in%20the%20body.%20Generally%2C%20this%20is%20carried%20out%20based%20on%0Athe%20estimated%20attenuation%20map%20obtained%20from%20computed%20tomography%20or%20magnetic%0Aresonance%20imaging.%20However%2C%20except%20for%20errors%20in%20the%20attenuation%20correction%0Afactors%20obtained%2C%20the%20additional%20scan%20not%20only%20brings%20in%20new%20radiation%20doses%0Aand/or%20increases%20the%20scanning%20time%20but%20also%20leads%20to%20severe%20misalignment%0Ainduced%20by%20various%20motions%20during%20and%20between%20the%20two%20sequential%20scans.%20To%0Aaddress%20these%20issues%2C%20based%20on%20maximum%20likelihood%20estimation%2C%20we%20propose%20a%20new%0Amathematical%20model%20for%20simultaneously%20reconstructing%20the%20activity%20and%0Aattenuation%20sinogram%20from%20the%20time-of-flight%20%28TOF%29-PET%20emission%20data%20only.%0AParticularly%2C%20we%20make%20full%20use%20of%20the%20exclusively%20exponential%20form%20for%20the%0Aattenuation%20correction%20factors%2C%20and%20consider%20the%20constraint%20of%20a%20total%20amount%0Aof%20the%20activity%20in%20some%20mask%20region%20in%20the%20proposed%20model.%20Furthermore%2C%20we%0Aprove%20its%20well-posedness%2C%20including%20the%20existence%2C%20uniqueness%20and%20stability%20of%0Athe%20solution.%20We%20propose%20an%20alternating%20update%20algorithm%20to%20solve%20the%20model%2C%0Aand%20also%20analyze%20its%20convergence.%20Finally%2C%20numerical%20experiments%20with%20various%0ATOF-PET%20emission%20data%20demonstrate%20that%20the%20proposed%20method%20is%20of%20numerical%0Aconvergence%20and%20robust%20to%20noise%2C%20and%20outperforms%20some%20state-of-the-art%20methods%0Ain%20terms%20of%20accuracy%20and%20efficiency%2C%20and%20has%20the%20capability%20of%20autonomous%0Aattenuation%20correction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13562v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520efficient%2520approach%2520with%2520theoretical%2520guarantees%2520to%2520simultaneously%250A%2520%2520reconstruct%2520activity%2520and%2520attenuation%2520sinogram%2520for%2520TOF-PET%26entry.906535625%3DLiyang%2520Hu%2520and%2520Chong%2520Chen%26entry.1292438233%3D%2520%2520In%2520positron%2520emission%2520tomography%2520%2528PET%2529%252C%2520it%2520is%2520indispensable%2520to%2520perform%250Aattenuation%2520correction%2520in%2520order%2520to%2520obtain%2520the%2520quantitatively%2520accurate%2520activity%250Amap%2520%2528tracer%2520distribution%2529%2520in%2520the%2520body.%2520Generally%252C%2520this%2520is%2520carried%2520out%2520based%2520on%250Athe%2520estimated%2520attenuation%2520map%2520obtained%2520from%2520computed%2520tomography%2520or%2520magnetic%250Aresonance%2520imaging.%2520However%252C%2520except%2520for%2520errors%2520in%2520the%2520attenuation%2520correction%250Afactors%2520obtained%252C%2520the%2520additional%2520scan%2520not%2520only%2520brings%2520in%2520new%2520radiation%2520doses%250Aand/or%2520increases%2520the%2520scanning%2520time%2520but%2520also%2520leads%2520to%2520severe%2520misalignment%250Ainduced%2520by%2520various%2520motions%2520during%2520and%2520between%2520the%2520two%2520sequential%2520scans.%2520To%250Aaddress%2520these%2520issues%252C%2520based%2520on%2520maximum%2520likelihood%2520estimation%252C%2520we%2520propose%2520a%2520new%250Amathematical%2520model%2520for%2520simultaneously%2520reconstructing%2520the%2520activity%2520and%250Aattenuation%2520sinogram%2520from%2520the%2520time-of-flight%2520%2528TOF%2529-PET%2520emission%2520data%2520only.%250AParticularly%252C%2520we%2520make%2520full%2520use%2520of%2520the%2520exclusively%2520exponential%2520form%2520for%2520the%250Aattenuation%2520correction%2520factors%252C%2520and%2520consider%2520the%2520constraint%2520of%2520a%2520total%2520amount%250Aof%2520the%2520activity%2520in%2520some%2520mask%2520region%2520in%2520the%2520proposed%2520model.%2520Furthermore%252C%2520we%250Aprove%2520its%2520well-posedness%252C%2520including%2520the%2520existence%252C%2520uniqueness%2520and%2520stability%2520of%250Athe%2520solution.%2520We%2520propose%2520an%2520alternating%2520update%2520algorithm%2520to%2520solve%2520the%2520model%252C%250Aand%2520also%2520analyze%2520its%2520convergence.%2520Finally%252C%2520numerical%2520experiments%2520with%2520various%250ATOF-PET%2520emission%2520data%2520demonstrate%2520that%2520the%2520proposed%2520method%2520is%2520of%2520numerical%250Aconvergence%2520and%2520robust%2520to%2520noise%252C%2520and%2520outperforms%2520some%2520state-of-the-art%2520methods%250Ain%2520terms%2520of%2520accuracy%2520and%2520efficiency%252C%2520and%2520has%2520the%2520capability%2520of%2520autonomous%250Aattenuation%2520correction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13562v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20efficient%20approach%20with%20theoretical%20guarantees%20to%20simultaneously%0A%20%20reconstruct%20activity%20and%20attenuation%20sinogram%20for%20TOF-PET&entry.906535625=Liyang%20Hu%20and%20Chong%20Chen&entry.1292438233=%20%20In%20positron%20emission%20tomography%20%28PET%29%2C%20it%20is%20indispensable%20to%20perform%0Aattenuation%20correction%20in%20order%20to%20obtain%20the%20quantitatively%20accurate%20activity%0Amap%20%28tracer%20distribution%29%20in%20the%20body.%20Generally%2C%20this%20is%20carried%20out%20based%20on%0Athe%20estimated%20attenuation%20map%20obtained%20from%20computed%20tomography%20or%20magnetic%0Aresonance%20imaging.%20However%2C%20except%20for%20errors%20in%20the%20attenuation%20correction%0Afactors%20obtained%2C%20the%20additional%20scan%20not%20only%20brings%20in%20new%20radiation%20doses%0Aand/or%20increases%20the%20scanning%20time%20but%20also%20leads%20to%20severe%20misalignment%0Ainduced%20by%20various%20motions%20during%20and%20between%20the%20two%20sequential%20scans.%20To%0Aaddress%20these%20issues%2C%20based%20on%20maximum%20likelihood%20estimation%2C%20we%20propose%20a%20new%0Amathematical%20model%20for%20simultaneously%20reconstructing%20the%20activity%20and%0Aattenuation%20sinogram%20from%20the%20time-of-flight%20%28TOF%29-PET%20emission%20data%20only.%0AParticularly%2C%20we%20make%20full%20use%20of%20the%20exclusively%20exponential%20form%20for%20the%0Aattenuation%20correction%20factors%2C%20and%20consider%20the%20constraint%20of%20a%20total%20amount%0Aof%20the%20activity%20in%20some%20mask%20region%20in%20the%20proposed%20model.%20Furthermore%2C%20we%0Aprove%20its%20well-posedness%2C%20including%20the%20existence%2C%20uniqueness%20and%20stability%20of%0Athe%20solution.%20We%20propose%20an%20alternating%20update%20algorithm%20to%20solve%20the%20model%2C%0Aand%20also%20analyze%20its%20convergence.%20Finally%2C%20numerical%20experiments%20with%20various%0ATOF-PET%20emission%20data%20demonstrate%20that%20the%20proposed%20method%20is%20of%20numerical%0Aconvergence%20and%20robust%20to%20noise%2C%20and%20outperforms%20some%20state-of-the-art%20methods%0Ain%20terms%20of%20accuracy%20and%20efficiency%2C%20and%20has%20the%20capability%20of%20autonomous%0Aattenuation%20correction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13562v1&entry.124074799=Read"},
{"title": "ProtoTopic: Prototypical Network for Few-Shot Medical Topic Modeling", "author": "Martin Licht and Sara Ketabi and Farzad Khalvati", "abstract": "  Topic modeling is a useful tool for analyzing large corpora of written\ndocuments, particularly academic papers. Despite a wide variety of proposed\ntopic modeling techniques, these techniques do not perform well when applied to\nmedical texts. This can be due to the low number of documents available for\nsome topics in the healthcare domain. In this paper, we propose ProtoTopic, a\nprototypical network-based topic model used for topic generation for a set of\nmedical paper abstracts. Prototypical networks are efficient, explainable\nmodels that make predictions by computing distances between input datapoints\nand a set of prototype representations, making them particularly effective in\nlow-data or few-shot learning scenarios. With ProtoTopic, we demonstrate\nimproved topic coherence and diversity compared to two topic modeling baselines\nused in the literature, demonstrating the ability of our model to generate\nmedically relevant topics even with limited data.\n", "link": "http://arxiv.org/abs/2510.13542v1", "date": "2025-10-15", "relevancy": 2.3538, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4919}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4602}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProtoTopic%3A%20Prototypical%20Network%20for%20Few-Shot%20Medical%20Topic%20Modeling&body=Title%3A%20ProtoTopic%3A%20Prototypical%20Network%20for%20Few-Shot%20Medical%20Topic%20Modeling%0AAuthor%3A%20Martin%20Licht%20and%20Sara%20Ketabi%20and%20Farzad%20Khalvati%0AAbstract%3A%20%20%20Topic%20modeling%20is%20a%20useful%20tool%20for%20analyzing%20large%20corpora%20of%20written%0Adocuments%2C%20particularly%20academic%20papers.%20Despite%20a%20wide%20variety%20of%20proposed%0Atopic%20modeling%20techniques%2C%20these%20techniques%20do%20not%20perform%20well%20when%20applied%20to%0Amedical%20texts.%20This%20can%20be%20due%20to%20the%20low%20number%20of%20documents%20available%20for%0Asome%20topics%20in%20the%20healthcare%20domain.%20In%20this%20paper%2C%20we%20propose%20ProtoTopic%2C%20a%0Aprototypical%20network-based%20topic%20model%20used%20for%20topic%20generation%20for%20a%20set%20of%0Amedical%20paper%20abstracts.%20Prototypical%20networks%20are%20efficient%2C%20explainable%0Amodels%20that%20make%20predictions%20by%20computing%20distances%20between%20input%20datapoints%0Aand%20a%20set%20of%20prototype%20representations%2C%20making%20them%20particularly%20effective%20in%0Alow-data%20or%20few-shot%20learning%20scenarios.%20With%20ProtoTopic%2C%20we%20demonstrate%0Aimproved%20topic%20coherence%20and%20diversity%20compared%20to%20two%20topic%20modeling%20baselines%0Aused%20in%20the%20literature%2C%20demonstrating%20the%20ability%20of%20our%20model%20to%20generate%0Amedically%20relevant%20topics%20even%20with%20limited%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13542v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProtoTopic%253A%2520Prototypical%2520Network%2520for%2520Few-Shot%2520Medical%2520Topic%2520Modeling%26entry.906535625%3DMartin%2520Licht%2520and%2520Sara%2520Ketabi%2520and%2520Farzad%2520Khalvati%26entry.1292438233%3D%2520%2520Topic%2520modeling%2520is%2520a%2520useful%2520tool%2520for%2520analyzing%2520large%2520corpora%2520of%2520written%250Adocuments%252C%2520particularly%2520academic%2520papers.%2520Despite%2520a%2520wide%2520variety%2520of%2520proposed%250Atopic%2520modeling%2520techniques%252C%2520these%2520techniques%2520do%2520not%2520perform%2520well%2520when%2520applied%2520to%250Amedical%2520texts.%2520This%2520can%2520be%2520due%2520to%2520the%2520low%2520number%2520of%2520documents%2520available%2520for%250Asome%2520topics%2520in%2520the%2520healthcare%2520domain.%2520In%2520this%2520paper%252C%2520we%2520propose%2520ProtoTopic%252C%2520a%250Aprototypical%2520network-based%2520topic%2520model%2520used%2520for%2520topic%2520generation%2520for%2520a%2520set%2520of%250Amedical%2520paper%2520abstracts.%2520Prototypical%2520networks%2520are%2520efficient%252C%2520explainable%250Amodels%2520that%2520make%2520predictions%2520by%2520computing%2520distances%2520between%2520input%2520datapoints%250Aand%2520a%2520set%2520of%2520prototype%2520representations%252C%2520making%2520them%2520particularly%2520effective%2520in%250Alow-data%2520or%2520few-shot%2520learning%2520scenarios.%2520With%2520ProtoTopic%252C%2520we%2520demonstrate%250Aimproved%2520topic%2520coherence%2520and%2520diversity%2520compared%2520to%2520two%2520topic%2520modeling%2520baselines%250Aused%2520in%2520the%2520literature%252C%2520demonstrating%2520the%2520ability%2520of%2520our%2520model%2520to%2520generate%250Amedically%2520relevant%2520topics%2520even%2520with%2520limited%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13542v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProtoTopic%3A%20Prototypical%20Network%20for%20Few-Shot%20Medical%20Topic%20Modeling&entry.906535625=Martin%20Licht%20and%20Sara%20Ketabi%20and%20Farzad%20Khalvati&entry.1292438233=%20%20Topic%20modeling%20is%20a%20useful%20tool%20for%20analyzing%20large%20corpora%20of%20written%0Adocuments%2C%20particularly%20academic%20papers.%20Despite%20a%20wide%20variety%20of%20proposed%0Atopic%20modeling%20techniques%2C%20these%20techniques%20do%20not%20perform%20well%20when%20applied%20to%0Amedical%20texts.%20This%20can%20be%20due%20to%20the%20low%20number%20of%20documents%20available%20for%0Asome%20topics%20in%20the%20healthcare%20domain.%20In%20this%20paper%2C%20we%20propose%20ProtoTopic%2C%20a%0Aprototypical%20network-based%20topic%20model%20used%20for%20topic%20generation%20for%20a%20set%20of%0Amedical%20paper%20abstracts.%20Prototypical%20networks%20are%20efficient%2C%20explainable%0Amodels%20that%20make%20predictions%20by%20computing%20distances%20between%20input%20datapoints%0Aand%20a%20set%20of%20prototype%20representations%2C%20making%20them%20particularly%20effective%20in%0Alow-data%20or%20few-shot%20learning%20scenarios.%20With%20ProtoTopic%2C%20we%20demonstrate%0Aimproved%20topic%20coherence%20and%20diversity%20compared%20to%20two%20topic%20modeling%20baselines%0Aused%20in%20the%20literature%2C%20demonstrating%20the%20ability%20of%20our%20model%20to%20generate%0Amedically%20relevant%20topics%20even%20with%20limited%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13542v1&entry.124074799=Read"},
{"title": "Human-MME: A Holistic Evaluation Benchmark for Human-Centric Multimodal\n  Large Language Models", "author": "Yuansen Liu and Haiming Tang and Jinlong Peng and Jiangning Zhang and Xiaozhong Ji and Qingdong He and Wenbin Wu and Donghao Luo and Zhenye Gan and Junwei Zhu and Yunhang Shen and Chaoyou Fu and Chengjie Wang and Xiaobin Hu and Shuicheng Yan", "abstract": "  Multimodal Large Language Models (MLLMs) have demonstrated significant\nadvances in visual understanding tasks. However, their capacity to comprehend\nhuman-centric scenes has rarely been explored, primarily due to the absence of\ncomprehensive evaluation benchmarks that take into account both the\nhuman-oriented granular level and higher-dimensional causal reasoning ability.\nSuch high-quality evaluation benchmarks face tough obstacles, given the\nphysical complexity of the human body and the difficulty of annotating granular\nstructures. In this paper, we propose Human-MME, a curated benchmark designed\nto provide a more holistic evaluation of MLLMs in human-centric scene\nunderstanding. Compared with other existing benchmarks, our work provides three\nkey features: 1. Diversity in human scene, spanning 4 primary visual domains\nwith 15 secondary domains and 43 sub-fields to ensure broad scenario coverage.\n2. Progressive and diverse evaluation dimensions, evaluating the human-based\nactivities progressively from the human-oriented granular perception to the\nhigher-dimensional reasoning, consisting of eight dimensions with 19,945\nreal-world image question pairs and an evaluation suite. 3. High-quality\nannotations with rich data paradigms, constructing the automated annotation\npipeline and human-annotation platform, supporting rigorous manual labeling to\nfacilitate precise and reliable model assessment. Our benchmark extends the\nsingle-target understanding to the multi-person and multi-image mutual\nunderstanding by constructing the choice, short-answer, grounding, ranking and\njudgment question components, and complex questions of their combination. The\nextensive experiments on 17 state-of-the-art MLLMs effectively expose the\nlimitations and guide future MLLMs research toward better human-centric image\nunderstanding. All data and code are available at\nhttps://github.com/Yuan-Hou/Human-MME.\n", "link": "http://arxiv.org/abs/2509.26165v3", "date": "2025-10-15", "relevancy": 2.3535, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5902}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.588}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human-MME%3A%20A%20Holistic%20Evaluation%20Benchmark%20for%20Human-Centric%20Multimodal%0A%20%20Large%20Language%20Models&body=Title%3A%20Human-MME%3A%20A%20Holistic%20Evaluation%20Benchmark%20for%20Human-Centric%20Multimodal%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Yuansen%20Liu%20and%20Haiming%20Tang%20and%20Jinlong%20Peng%20and%20Jiangning%20Zhang%20and%20Xiaozhong%20Ji%20and%20Qingdong%20He%20and%20Wenbin%20Wu%20and%20Donghao%20Luo%20and%20Zhenye%20Gan%20and%20Junwei%20Zhu%20and%20Yunhang%20Shen%20and%20Chaoyou%20Fu%20and%20Chengjie%20Wang%20and%20Xiaobin%20Hu%20and%20Shuicheng%20Yan%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20significant%0Aadvances%20in%20visual%20understanding%20tasks.%20However%2C%20their%20capacity%20to%20comprehend%0Ahuman-centric%20scenes%20has%20rarely%20been%20explored%2C%20primarily%20due%20to%20the%20absence%20of%0Acomprehensive%20evaluation%20benchmarks%20that%20take%20into%20account%20both%20the%0Ahuman-oriented%20granular%20level%20and%20higher-dimensional%20causal%20reasoning%20ability.%0ASuch%20high-quality%20evaluation%20benchmarks%20face%20tough%20obstacles%2C%20given%20the%0Aphysical%20complexity%20of%20the%20human%20body%20and%20the%20difficulty%20of%20annotating%20granular%0Astructures.%20In%20this%20paper%2C%20we%20propose%20Human-MME%2C%20a%20curated%20benchmark%20designed%0Ato%20provide%20a%20more%20holistic%20evaluation%20of%20MLLMs%20in%20human-centric%20scene%0Aunderstanding.%20Compared%20with%20other%20existing%20benchmarks%2C%20our%20work%20provides%20three%0Akey%20features%3A%201.%20Diversity%20in%20human%20scene%2C%20spanning%204%20primary%20visual%20domains%0Awith%2015%20secondary%20domains%20and%2043%20sub-fields%20to%20ensure%20broad%20scenario%20coverage.%0A2.%20Progressive%20and%20diverse%20evaluation%20dimensions%2C%20evaluating%20the%20human-based%0Aactivities%20progressively%20from%20the%20human-oriented%20granular%20perception%20to%20the%0Ahigher-dimensional%20reasoning%2C%20consisting%20of%20eight%20dimensions%20with%2019%2C945%0Areal-world%20image%20question%20pairs%20and%20an%20evaluation%20suite.%203.%20High-quality%0Aannotations%20with%20rich%20data%20paradigms%2C%20constructing%20the%20automated%20annotation%0Apipeline%20and%20human-annotation%20platform%2C%20supporting%20rigorous%20manual%20labeling%20to%0Afacilitate%20precise%20and%20reliable%20model%20assessment.%20Our%20benchmark%20extends%20the%0Asingle-target%20understanding%20to%20the%20multi-person%20and%20multi-image%20mutual%0Aunderstanding%20by%20constructing%20the%20choice%2C%20short-answer%2C%20grounding%2C%20ranking%20and%0Ajudgment%20question%20components%2C%20and%20complex%20questions%20of%20their%20combination.%20The%0Aextensive%20experiments%20on%2017%20state-of-the-art%20MLLMs%20effectively%20expose%20the%0Alimitations%20and%20guide%20future%20MLLMs%20research%20toward%20better%20human-centric%20image%0Aunderstanding.%20All%20data%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/Yuan-Hou/Human-MME.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26165v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman-MME%253A%2520A%2520Holistic%2520Evaluation%2520Benchmark%2520for%2520Human-Centric%2520Multimodal%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DYuansen%2520Liu%2520and%2520Haiming%2520Tang%2520and%2520Jinlong%2520Peng%2520and%2520Jiangning%2520Zhang%2520and%2520Xiaozhong%2520Ji%2520and%2520Qingdong%2520He%2520and%2520Wenbin%2520Wu%2520and%2520Donghao%2520Luo%2520and%2520Zhenye%2520Gan%2520and%2520Junwei%2520Zhu%2520and%2520Yunhang%2520Shen%2520and%2520Chaoyou%2520Fu%2520and%2520Chengjie%2520Wang%2520and%2520Xiaobin%2520Hu%2520and%2520Shuicheng%2520Yan%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520demonstrated%2520significant%250Aadvances%2520in%2520visual%2520understanding%2520tasks.%2520However%252C%2520their%2520capacity%2520to%2520comprehend%250Ahuman-centric%2520scenes%2520has%2520rarely%2520been%2520explored%252C%2520primarily%2520due%2520to%2520the%2520absence%2520of%250Acomprehensive%2520evaluation%2520benchmarks%2520that%2520take%2520into%2520account%2520both%2520the%250Ahuman-oriented%2520granular%2520level%2520and%2520higher-dimensional%2520causal%2520reasoning%2520ability.%250ASuch%2520high-quality%2520evaluation%2520benchmarks%2520face%2520tough%2520obstacles%252C%2520given%2520the%250Aphysical%2520complexity%2520of%2520the%2520human%2520body%2520and%2520the%2520difficulty%2520of%2520annotating%2520granular%250Astructures.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Human-MME%252C%2520a%2520curated%2520benchmark%2520designed%250Ato%2520provide%2520a%2520more%2520holistic%2520evaluation%2520of%2520MLLMs%2520in%2520human-centric%2520scene%250Aunderstanding.%2520Compared%2520with%2520other%2520existing%2520benchmarks%252C%2520our%2520work%2520provides%2520three%250Akey%2520features%253A%25201.%2520Diversity%2520in%2520human%2520scene%252C%2520spanning%25204%2520primary%2520visual%2520domains%250Awith%252015%2520secondary%2520domains%2520and%252043%2520sub-fields%2520to%2520ensure%2520broad%2520scenario%2520coverage.%250A2.%2520Progressive%2520and%2520diverse%2520evaluation%2520dimensions%252C%2520evaluating%2520the%2520human-based%250Aactivities%2520progressively%2520from%2520the%2520human-oriented%2520granular%2520perception%2520to%2520the%250Ahigher-dimensional%2520reasoning%252C%2520consisting%2520of%2520eight%2520dimensions%2520with%252019%252C945%250Areal-world%2520image%2520question%2520pairs%2520and%2520an%2520evaluation%2520suite.%25203.%2520High-quality%250Aannotations%2520with%2520rich%2520data%2520paradigms%252C%2520constructing%2520the%2520automated%2520annotation%250Apipeline%2520and%2520human-annotation%2520platform%252C%2520supporting%2520rigorous%2520manual%2520labeling%2520to%250Afacilitate%2520precise%2520and%2520reliable%2520model%2520assessment.%2520Our%2520benchmark%2520extends%2520the%250Asingle-target%2520understanding%2520to%2520the%2520multi-person%2520and%2520multi-image%2520mutual%250Aunderstanding%2520by%2520constructing%2520the%2520choice%252C%2520short-answer%252C%2520grounding%252C%2520ranking%2520and%250Ajudgment%2520question%2520components%252C%2520and%2520complex%2520questions%2520of%2520their%2520combination.%2520The%250Aextensive%2520experiments%2520on%252017%2520state-of-the-art%2520MLLMs%2520effectively%2520expose%2520the%250Alimitations%2520and%2520guide%2520future%2520MLLMs%2520research%2520toward%2520better%2520human-centric%2520image%250Aunderstanding.%2520All%2520data%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//github.com/Yuan-Hou/Human-MME.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26165v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human-MME%3A%20A%20Holistic%20Evaluation%20Benchmark%20for%20Human-Centric%20Multimodal%0A%20%20Large%20Language%20Models&entry.906535625=Yuansen%20Liu%20and%20Haiming%20Tang%20and%20Jinlong%20Peng%20and%20Jiangning%20Zhang%20and%20Xiaozhong%20Ji%20and%20Qingdong%20He%20and%20Wenbin%20Wu%20and%20Donghao%20Luo%20and%20Zhenye%20Gan%20and%20Junwei%20Zhu%20and%20Yunhang%20Shen%20and%20Chaoyou%20Fu%20and%20Chengjie%20Wang%20and%20Xiaobin%20Hu%20and%20Shuicheng%20Yan&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20significant%0Aadvances%20in%20visual%20understanding%20tasks.%20However%2C%20their%20capacity%20to%20comprehend%0Ahuman-centric%20scenes%20has%20rarely%20been%20explored%2C%20primarily%20due%20to%20the%20absence%20of%0Acomprehensive%20evaluation%20benchmarks%20that%20take%20into%20account%20both%20the%0Ahuman-oriented%20granular%20level%20and%20higher-dimensional%20causal%20reasoning%20ability.%0ASuch%20high-quality%20evaluation%20benchmarks%20face%20tough%20obstacles%2C%20given%20the%0Aphysical%20complexity%20of%20the%20human%20body%20and%20the%20difficulty%20of%20annotating%20granular%0Astructures.%20In%20this%20paper%2C%20we%20propose%20Human-MME%2C%20a%20curated%20benchmark%20designed%0Ato%20provide%20a%20more%20holistic%20evaluation%20of%20MLLMs%20in%20human-centric%20scene%0Aunderstanding.%20Compared%20with%20other%20existing%20benchmarks%2C%20our%20work%20provides%20three%0Akey%20features%3A%201.%20Diversity%20in%20human%20scene%2C%20spanning%204%20primary%20visual%20domains%0Awith%2015%20secondary%20domains%20and%2043%20sub-fields%20to%20ensure%20broad%20scenario%20coverage.%0A2.%20Progressive%20and%20diverse%20evaluation%20dimensions%2C%20evaluating%20the%20human-based%0Aactivities%20progressively%20from%20the%20human-oriented%20granular%20perception%20to%20the%0Ahigher-dimensional%20reasoning%2C%20consisting%20of%20eight%20dimensions%20with%2019%2C945%0Areal-world%20image%20question%20pairs%20and%20an%20evaluation%20suite.%203.%20High-quality%0Aannotations%20with%20rich%20data%20paradigms%2C%20constructing%20the%20automated%20annotation%0Apipeline%20and%20human-annotation%20platform%2C%20supporting%20rigorous%20manual%20labeling%20to%0Afacilitate%20precise%20and%20reliable%20model%20assessment.%20Our%20benchmark%20extends%20the%0Asingle-target%20understanding%20to%20the%20multi-person%20and%20multi-image%20mutual%0Aunderstanding%20by%20constructing%20the%20choice%2C%20short-answer%2C%20grounding%2C%20ranking%20and%0Ajudgment%20question%20components%2C%20and%20complex%20questions%20of%20their%20combination.%20The%0Aextensive%20experiments%20on%2017%20state-of-the-art%20MLLMs%20effectively%20expose%20the%0Alimitations%20and%20guide%20future%20MLLMs%20research%20toward%20better%20human-centric%20image%0Aunderstanding.%20All%20data%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/Yuan-Hou/Human-MME.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26165v3&entry.124074799=Read"},
{"title": "NTIRE 2025 Challenge on Low Light Image Enhancement: Methods and Results", "author": "Xiaoning Liu and Zongwei Wu and Florin-Alexandru Vasluianu and Hailong Yan and Bin Ren and Yulun Zhang and Shuhang Gu and Le Zhang and Ce Zhu and Radu Timofte and Kangbiao Shi and Yixu Feng and Tao Hu and Yu Cao and Peng Wu and Yijin Liang and Yanning Zhang and Qingsen Yan and Han Zhou and Wei Dong and Yan Min and Mohab Kishawy and Jun Chen and Pengpeng Yu and Anjin Park and Seung-Soo Lee and Young-Joon Park and Zixiao Hu and Junyv Liu and Huilin Zhang and Jun Zhang and Fei Wan and Bingxin Xu and Hongzhe Liu and Cheng Xu and Weiguo Pan and Songyin Dai and Xunpeng Yi and Qinglong Yan and Yibing Zhang and Jiayi Ma and Changhui Hu and Kerui Hu and Donghang Jing and Tiesheng Chen and Zhi Jin and Hongjun Wu and Biao Huang and Haitao Ling and Jiahao Wu and Dandan Zhan and G Gyaneshwar Rao and Vijayalaxmi Ashok Aralikatti and Nikhil Akalwadi and Ramesh Ashok Tabib and Uma Mudenagudi and Ruirui Lin and Guoxi Huang and Nantheera Anantrasirichai and Qirui Yang and Alexandru Brateanu and Ciprian Orhei and Cosmin Ancuti and Daniel Feijoo and Juan C. Benito and \u00c1lvaro Garc\u00eda and Marcos V. Conde and Yang Qin and Raul Balmez and Anas M. Ali and Bilel Benjdira and Wadii Boulila and Tianyi Mao and Huan Zheng and Yanyan Wei and Shengeng Tang and Dan Guo and Zhao Zhang and Sabari Nathan and K Uma and A Sasithradevi and B Sathya Bama and S. Mohamed Mansoor Roomi and Ao Li and Xiangtao Zhang and Zhe Liu and Yijie Tang and Jialong Tang and Zhicheng Fu and Gong Chen and Joe Nasti and John Nicholson and Zeyu Xiao and Zhuoyuan Li and Ashutosh Kulkarni and Prashant W. Patil and Santosh Kumar Vipparthi and Subrahmanyam Murala and Duan Liu and Weile Li and Hangyuan Lu and Rixian Liu and Tengfeng Wang and Jinxing Liang and Chenxin Yu", "abstract": "  This paper presents a comprehensive review of the NTIRE 2025 Low-Light Image\nEnhancement (LLIE) Challenge, highlighting the proposed solutions and final\noutcomes. The objective of the challenge is to identify effective networks\ncapable of producing brighter, clearer, and visually compelling images under\ndiverse and challenging conditions. A remarkable total of 762 participants\nregistered for the competition, with 28 teams ultimately submitting valid\nentries. This paper thoroughly evaluates the state-of-the-art advancements in\nLLIE, showcasing the significant progress.\n", "link": "http://arxiv.org/abs/2510.13670v1", "date": "2025-10-15", "relevancy": 2.3523, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4706}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4704}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NTIRE%202025%20Challenge%20on%20Low%20Light%20Image%20Enhancement%3A%20Methods%20and%20Results&body=Title%3A%20NTIRE%202025%20Challenge%20on%20Low%20Light%20Image%20Enhancement%3A%20Methods%20and%20Results%0AAuthor%3A%20Xiaoning%20Liu%20and%20Zongwei%20Wu%20and%20Florin-Alexandru%20Vasluianu%20and%20Hailong%20Yan%20and%20Bin%20Ren%20and%20Yulun%20Zhang%20and%20Shuhang%20Gu%20and%20Le%20Zhang%20and%20Ce%20Zhu%20and%20Radu%20Timofte%20and%20Kangbiao%20Shi%20and%20Yixu%20Feng%20and%20Tao%20Hu%20and%20Yu%20Cao%20and%20Peng%20Wu%20and%20Yijin%20Liang%20and%20Yanning%20Zhang%20and%20Qingsen%20Yan%20and%20Han%20Zhou%20and%20Wei%20Dong%20and%20Yan%20Min%20and%20Mohab%20Kishawy%20and%20Jun%20Chen%20and%20Pengpeng%20Yu%20and%20Anjin%20Park%20and%20Seung-Soo%20Lee%20and%20Young-Joon%20Park%20and%20Zixiao%20Hu%20and%20Junyv%20Liu%20and%20Huilin%20Zhang%20and%20Jun%20Zhang%20and%20Fei%20Wan%20and%20Bingxin%20Xu%20and%20Hongzhe%20Liu%20and%20Cheng%20Xu%20and%20Weiguo%20Pan%20and%20Songyin%20Dai%20and%20Xunpeng%20Yi%20and%20Qinglong%20Yan%20and%20Yibing%20Zhang%20and%20Jiayi%20Ma%20and%20Changhui%20Hu%20and%20Kerui%20Hu%20and%20Donghang%20Jing%20and%20Tiesheng%20Chen%20and%20Zhi%20Jin%20and%20Hongjun%20Wu%20and%20Biao%20Huang%20and%20Haitao%20Ling%20and%20Jiahao%20Wu%20and%20Dandan%20Zhan%20and%20G%20Gyaneshwar%20Rao%20and%20Vijayalaxmi%20Ashok%20Aralikatti%20and%20Nikhil%20Akalwadi%20and%20Ramesh%20Ashok%20Tabib%20and%20Uma%20Mudenagudi%20and%20Ruirui%20Lin%20and%20Guoxi%20Huang%20and%20Nantheera%20Anantrasirichai%20and%20Qirui%20Yang%20and%20Alexandru%20Brateanu%20and%20Ciprian%20Orhei%20and%20Cosmin%20Ancuti%20and%20Daniel%20Feijoo%20and%20Juan%20C.%20Benito%20and%20%C3%81lvaro%20Garc%C3%ADa%20and%20Marcos%20V.%20Conde%20and%20Yang%20Qin%20and%20Raul%20Balmez%20and%20Anas%20M.%20Ali%20and%20Bilel%20Benjdira%20and%20Wadii%20Boulila%20and%20Tianyi%20Mao%20and%20Huan%20Zheng%20and%20Yanyan%20Wei%20and%20Shengeng%20Tang%20and%20Dan%20Guo%20and%20Zhao%20Zhang%20and%20Sabari%20Nathan%20and%20K%20Uma%20and%20A%20Sasithradevi%20and%20B%20Sathya%20Bama%20and%20S.%20Mohamed%20Mansoor%20Roomi%20and%20Ao%20Li%20and%20Xiangtao%20Zhang%20and%20Zhe%20Liu%20and%20Yijie%20Tang%20and%20Jialong%20Tang%20and%20Zhicheng%20Fu%20and%20Gong%20Chen%20and%20Joe%20Nasti%20and%20John%20Nicholson%20and%20Zeyu%20Xiao%20and%20Zhuoyuan%20Li%20and%20Ashutosh%20Kulkarni%20and%20Prashant%20W.%20Patil%20and%20Santosh%20Kumar%20Vipparthi%20and%20Subrahmanyam%20Murala%20and%20Duan%20Liu%20and%20Weile%20Li%20and%20Hangyuan%20Lu%20and%20Rixian%20Liu%20and%20Tengfeng%20Wang%20and%20Jinxing%20Liang%20and%20Chenxin%20Yu%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20comprehensive%20review%20of%20the%20NTIRE%202025%20Low-Light%20Image%0AEnhancement%20%28LLIE%29%20Challenge%2C%20highlighting%20the%20proposed%20solutions%20and%20final%0Aoutcomes.%20The%20objective%20of%20the%20challenge%20is%20to%20identify%20effective%20networks%0Acapable%20of%20producing%20brighter%2C%20clearer%2C%20and%20visually%20compelling%20images%20under%0Adiverse%20and%20challenging%20conditions.%20A%20remarkable%20total%20of%20762%20participants%0Aregistered%20for%20the%20competition%2C%20with%2028%20teams%20ultimately%20submitting%20valid%0Aentries.%20This%20paper%20thoroughly%20evaluates%20the%20state-of-the-art%20advancements%20in%0ALLIE%2C%20showcasing%20the%20significant%20progress.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13670v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNTIRE%25202025%2520Challenge%2520on%2520Low%2520Light%2520Image%2520Enhancement%253A%2520Methods%2520and%2520Results%26entry.906535625%3DXiaoning%2520Liu%2520and%2520Zongwei%2520Wu%2520and%2520Florin-Alexandru%2520Vasluianu%2520and%2520Hailong%2520Yan%2520and%2520Bin%2520Ren%2520and%2520Yulun%2520Zhang%2520and%2520Shuhang%2520Gu%2520and%2520Le%2520Zhang%2520and%2520Ce%2520Zhu%2520and%2520Radu%2520Timofte%2520and%2520Kangbiao%2520Shi%2520and%2520Yixu%2520Feng%2520and%2520Tao%2520Hu%2520and%2520Yu%2520Cao%2520and%2520Peng%2520Wu%2520and%2520Yijin%2520Liang%2520and%2520Yanning%2520Zhang%2520and%2520Qingsen%2520Yan%2520and%2520Han%2520Zhou%2520and%2520Wei%2520Dong%2520and%2520Yan%2520Min%2520and%2520Mohab%2520Kishawy%2520and%2520Jun%2520Chen%2520and%2520Pengpeng%2520Yu%2520and%2520Anjin%2520Park%2520and%2520Seung-Soo%2520Lee%2520and%2520Young-Joon%2520Park%2520and%2520Zixiao%2520Hu%2520and%2520Junyv%2520Liu%2520and%2520Huilin%2520Zhang%2520and%2520Jun%2520Zhang%2520and%2520Fei%2520Wan%2520and%2520Bingxin%2520Xu%2520and%2520Hongzhe%2520Liu%2520and%2520Cheng%2520Xu%2520and%2520Weiguo%2520Pan%2520and%2520Songyin%2520Dai%2520and%2520Xunpeng%2520Yi%2520and%2520Qinglong%2520Yan%2520and%2520Yibing%2520Zhang%2520and%2520Jiayi%2520Ma%2520and%2520Changhui%2520Hu%2520and%2520Kerui%2520Hu%2520and%2520Donghang%2520Jing%2520and%2520Tiesheng%2520Chen%2520and%2520Zhi%2520Jin%2520and%2520Hongjun%2520Wu%2520and%2520Biao%2520Huang%2520and%2520Haitao%2520Ling%2520and%2520Jiahao%2520Wu%2520and%2520Dandan%2520Zhan%2520and%2520G%2520Gyaneshwar%2520Rao%2520and%2520Vijayalaxmi%2520Ashok%2520Aralikatti%2520and%2520Nikhil%2520Akalwadi%2520and%2520Ramesh%2520Ashok%2520Tabib%2520and%2520Uma%2520Mudenagudi%2520and%2520Ruirui%2520Lin%2520and%2520Guoxi%2520Huang%2520and%2520Nantheera%2520Anantrasirichai%2520and%2520Qirui%2520Yang%2520and%2520Alexandru%2520Brateanu%2520and%2520Ciprian%2520Orhei%2520and%2520Cosmin%2520Ancuti%2520and%2520Daniel%2520Feijoo%2520and%2520Juan%2520C.%2520Benito%2520and%2520%25C3%2581lvaro%2520Garc%25C3%25ADa%2520and%2520Marcos%2520V.%2520Conde%2520and%2520Yang%2520Qin%2520and%2520Raul%2520Balmez%2520and%2520Anas%2520M.%2520Ali%2520and%2520Bilel%2520Benjdira%2520and%2520Wadii%2520Boulila%2520and%2520Tianyi%2520Mao%2520and%2520Huan%2520Zheng%2520and%2520Yanyan%2520Wei%2520and%2520Shengeng%2520Tang%2520and%2520Dan%2520Guo%2520and%2520Zhao%2520Zhang%2520and%2520Sabari%2520Nathan%2520and%2520K%2520Uma%2520and%2520A%2520Sasithradevi%2520and%2520B%2520Sathya%2520Bama%2520and%2520S.%2520Mohamed%2520Mansoor%2520Roomi%2520and%2520Ao%2520Li%2520and%2520Xiangtao%2520Zhang%2520and%2520Zhe%2520Liu%2520and%2520Yijie%2520Tang%2520and%2520Jialong%2520Tang%2520and%2520Zhicheng%2520Fu%2520and%2520Gong%2520Chen%2520and%2520Joe%2520Nasti%2520and%2520John%2520Nicholson%2520and%2520Zeyu%2520Xiao%2520and%2520Zhuoyuan%2520Li%2520and%2520Ashutosh%2520Kulkarni%2520and%2520Prashant%2520W.%2520Patil%2520and%2520Santosh%2520Kumar%2520Vipparthi%2520and%2520Subrahmanyam%2520Murala%2520and%2520Duan%2520Liu%2520and%2520Weile%2520Li%2520and%2520Hangyuan%2520Lu%2520and%2520Rixian%2520Liu%2520and%2520Tengfeng%2520Wang%2520and%2520Jinxing%2520Liang%2520and%2520Chenxin%2520Yu%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520comprehensive%2520review%2520of%2520the%2520NTIRE%25202025%2520Low-Light%2520Image%250AEnhancement%2520%2528LLIE%2529%2520Challenge%252C%2520highlighting%2520the%2520proposed%2520solutions%2520and%2520final%250Aoutcomes.%2520The%2520objective%2520of%2520the%2520challenge%2520is%2520to%2520identify%2520effective%2520networks%250Acapable%2520of%2520producing%2520brighter%252C%2520clearer%252C%2520and%2520visually%2520compelling%2520images%2520under%250Adiverse%2520and%2520challenging%2520conditions.%2520A%2520remarkable%2520total%2520of%2520762%2520participants%250Aregistered%2520for%2520the%2520competition%252C%2520with%252028%2520teams%2520ultimately%2520submitting%2520valid%250Aentries.%2520This%2520paper%2520thoroughly%2520evaluates%2520the%2520state-of-the-art%2520advancements%2520in%250ALLIE%252C%2520showcasing%2520the%2520significant%2520progress.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13670v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NTIRE%202025%20Challenge%20on%20Low%20Light%20Image%20Enhancement%3A%20Methods%20and%20Results&entry.906535625=Xiaoning%20Liu%20and%20Zongwei%20Wu%20and%20Florin-Alexandru%20Vasluianu%20and%20Hailong%20Yan%20and%20Bin%20Ren%20and%20Yulun%20Zhang%20and%20Shuhang%20Gu%20and%20Le%20Zhang%20and%20Ce%20Zhu%20and%20Radu%20Timofte%20and%20Kangbiao%20Shi%20and%20Yixu%20Feng%20and%20Tao%20Hu%20and%20Yu%20Cao%20and%20Peng%20Wu%20and%20Yijin%20Liang%20and%20Yanning%20Zhang%20and%20Qingsen%20Yan%20and%20Han%20Zhou%20and%20Wei%20Dong%20and%20Yan%20Min%20and%20Mohab%20Kishawy%20and%20Jun%20Chen%20and%20Pengpeng%20Yu%20and%20Anjin%20Park%20and%20Seung-Soo%20Lee%20and%20Young-Joon%20Park%20and%20Zixiao%20Hu%20and%20Junyv%20Liu%20and%20Huilin%20Zhang%20and%20Jun%20Zhang%20and%20Fei%20Wan%20and%20Bingxin%20Xu%20and%20Hongzhe%20Liu%20and%20Cheng%20Xu%20and%20Weiguo%20Pan%20and%20Songyin%20Dai%20and%20Xunpeng%20Yi%20and%20Qinglong%20Yan%20and%20Yibing%20Zhang%20and%20Jiayi%20Ma%20and%20Changhui%20Hu%20and%20Kerui%20Hu%20and%20Donghang%20Jing%20and%20Tiesheng%20Chen%20and%20Zhi%20Jin%20and%20Hongjun%20Wu%20and%20Biao%20Huang%20and%20Haitao%20Ling%20and%20Jiahao%20Wu%20and%20Dandan%20Zhan%20and%20G%20Gyaneshwar%20Rao%20and%20Vijayalaxmi%20Ashok%20Aralikatti%20and%20Nikhil%20Akalwadi%20and%20Ramesh%20Ashok%20Tabib%20and%20Uma%20Mudenagudi%20and%20Ruirui%20Lin%20and%20Guoxi%20Huang%20and%20Nantheera%20Anantrasirichai%20and%20Qirui%20Yang%20and%20Alexandru%20Brateanu%20and%20Ciprian%20Orhei%20and%20Cosmin%20Ancuti%20and%20Daniel%20Feijoo%20and%20Juan%20C.%20Benito%20and%20%C3%81lvaro%20Garc%C3%ADa%20and%20Marcos%20V.%20Conde%20and%20Yang%20Qin%20and%20Raul%20Balmez%20and%20Anas%20M.%20Ali%20and%20Bilel%20Benjdira%20and%20Wadii%20Boulila%20and%20Tianyi%20Mao%20and%20Huan%20Zheng%20and%20Yanyan%20Wei%20and%20Shengeng%20Tang%20and%20Dan%20Guo%20and%20Zhao%20Zhang%20and%20Sabari%20Nathan%20and%20K%20Uma%20and%20A%20Sasithradevi%20and%20B%20Sathya%20Bama%20and%20S.%20Mohamed%20Mansoor%20Roomi%20and%20Ao%20Li%20and%20Xiangtao%20Zhang%20and%20Zhe%20Liu%20and%20Yijie%20Tang%20and%20Jialong%20Tang%20and%20Zhicheng%20Fu%20and%20Gong%20Chen%20and%20Joe%20Nasti%20and%20John%20Nicholson%20and%20Zeyu%20Xiao%20and%20Zhuoyuan%20Li%20and%20Ashutosh%20Kulkarni%20and%20Prashant%20W.%20Patil%20and%20Santosh%20Kumar%20Vipparthi%20and%20Subrahmanyam%20Murala%20and%20Duan%20Liu%20and%20Weile%20Li%20and%20Hangyuan%20Lu%20and%20Rixian%20Liu%20and%20Tengfeng%20Wang%20and%20Jinxing%20Liang%20and%20Chenxin%20Yu&entry.1292438233=%20%20This%20paper%20presents%20a%20comprehensive%20review%20of%20the%20NTIRE%202025%20Low-Light%20Image%0AEnhancement%20%28LLIE%29%20Challenge%2C%20highlighting%20the%20proposed%20solutions%20and%20final%0Aoutcomes.%20The%20objective%20of%20the%20challenge%20is%20to%20identify%20effective%20networks%0Acapable%20of%20producing%20brighter%2C%20clearer%2C%20and%20visually%20compelling%20images%20under%0Adiverse%20and%20challenging%20conditions.%20A%20remarkable%20total%20of%20762%20participants%0Aregistered%20for%20the%20competition%2C%20with%2028%20teams%20ultimately%20submitting%20valid%0Aentries.%20This%20paper%20thoroughly%20evaluates%20the%20state-of-the-art%20advancements%20in%0ALLIE%2C%20showcasing%20the%20significant%20progress.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13670v1&entry.124074799=Read"},
{"title": "(R)evolution of Programming: Vibe Coding as a Post-Coding Paradigm", "author": "Kevin Krings and Nino S. Bohn and Thomas Ludwig", "abstract": "  Recent advancements in generative artificial intelligence (GenAI),\nparticularly large language models, have introduced new possibilities for\nsoftware development practices. In our paper we investigate the emerging Vibe\nCoding (VC) paradigm that emphasizes intuitive, affect-driven, and\nimprovisational interactions between developers and AI systems. Building upon\nthe discourse of End-User Development (EUD), we explore how VC diverges from\nconventional programming approaches such as those supported by tools like\nGitHub Copilot. Through five semi-structured interview sessions with ten\nexperienced software practitioners, we identify five thematic dimensions:\ncreativity, sustainability, the future of programming, collaboration, and\ncriticism. Our analysis conceptualizes VC within the metaphor of co-drifting,\ncontrasting it with the prevalent co-piloting perspective of AI-assisted\ndevelopment. We argue that VC reconfigures the developers role, blurring\nboundaries between professional and non-developers. While VC enables novel\nforms of expression and rapid prototyping, it also introduces challenges\nregarding reproducibility, scalability, and inclusivity. We propose that VC\nrepresents a meaningful shift in programming culture, warranting further\ninvestigation within human-computer interaction (HCI) and software engineering\nresearch.\n", "link": "http://arxiv.org/abs/2510.12364v2", "date": "2025-10-15", "relevancy": 2.35, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4753}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4719}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%28R%29evolution%20of%20Programming%3A%20Vibe%20Coding%20as%20a%20Post-Coding%20Paradigm&body=Title%3A%20%28R%29evolution%20of%20Programming%3A%20Vibe%20Coding%20as%20a%20Post-Coding%20Paradigm%0AAuthor%3A%20Kevin%20Krings%20and%20Nino%20S.%20Bohn%20and%20Thomas%20Ludwig%0AAbstract%3A%20%20%20Recent%20advancements%20in%20generative%20artificial%20intelligence%20%28GenAI%29%2C%0Aparticularly%20large%20language%20models%2C%20have%20introduced%20new%20possibilities%20for%0Asoftware%20development%20practices.%20In%20our%20paper%20we%20investigate%20the%20emerging%20Vibe%0ACoding%20%28VC%29%20paradigm%20that%20emphasizes%20intuitive%2C%20affect-driven%2C%20and%0Aimprovisational%20interactions%20between%20developers%20and%20AI%20systems.%20Building%20upon%0Athe%20discourse%20of%20End-User%20Development%20%28EUD%29%2C%20we%20explore%20how%20VC%20diverges%20from%0Aconventional%20programming%20approaches%20such%20as%20those%20supported%20by%20tools%20like%0AGitHub%20Copilot.%20Through%20five%20semi-structured%20interview%20sessions%20with%20ten%0Aexperienced%20software%20practitioners%2C%20we%20identify%20five%20thematic%20dimensions%3A%0Acreativity%2C%20sustainability%2C%20the%20future%20of%20programming%2C%20collaboration%2C%20and%0Acriticism.%20Our%20analysis%20conceptualizes%20VC%20within%20the%20metaphor%20of%20co-drifting%2C%0Acontrasting%20it%20with%20the%20prevalent%20co-piloting%20perspective%20of%20AI-assisted%0Adevelopment.%20We%20argue%20that%20VC%20reconfigures%20the%20developers%20role%2C%20blurring%0Aboundaries%20between%20professional%20and%20non-developers.%20While%20VC%20enables%20novel%0Aforms%20of%20expression%20and%20rapid%20prototyping%2C%20it%20also%20introduces%20challenges%0Aregarding%20reproducibility%2C%20scalability%2C%20and%20inclusivity.%20We%20propose%20that%20VC%0Arepresents%20a%20meaningful%20shift%20in%20programming%20culture%2C%20warranting%20further%0Ainvestigation%20within%20human-computer%20interaction%20%28HCI%29%20and%20software%20engineering%0Aresearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12364v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2528R%2529evolution%2520of%2520Programming%253A%2520Vibe%2520Coding%2520as%2520a%2520Post-Coding%2520Paradigm%26entry.906535625%3DKevin%2520Krings%2520and%2520Nino%2520S.%2520Bohn%2520and%2520Thomas%2520Ludwig%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520generative%2520artificial%2520intelligence%2520%2528GenAI%2529%252C%250Aparticularly%2520large%2520language%2520models%252C%2520have%2520introduced%2520new%2520possibilities%2520for%250Asoftware%2520development%2520practices.%2520In%2520our%2520paper%2520we%2520investigate%2520the%2520emerging%2520Vibe%250ACoding%2520%2528VC%2529%2520paradigm%2520that%2520emphasizes%2520intuitive%252C%2520affect-driven%252C%2520and%250Aimprovisational%2520interactions%2520between%2520developers%2520and%2520AI%2520systems.%2520Building%2520upon%250Athe%2520discourse%2520of%2520End-User%2520Development%2520%2528EUD%2529%252C%2520we%2520explore%2520how%2520VC%2520diverges%2520from%250Aconventional%2520programming%2520approaches%2520such%2520as%2520those%2520supported%2520by%2520tools%2520like%250AGitHub%2520Copilot.%2520Through%2520five%2520semi-structured%2520interview%2520sessions%2520with%2520ten%250Aexperienced%2520software%2520practitioners%252C%2520we%2520identify%2520five%2520thematic%2520dimensions%253A%250Acreativity%252C%2520sustainability%252C%2520the%2520future%2520of%2520programming%252C%2520collaboration%252C%2520and%250Acriticism.%2520Our%2520analysis%2520conceptualizes%2520VC%2520within%2520the%2520metaphor%2520of%2520co-drifting%252C%250Acontrasting%2520it%2520with%2520the%2520prevalent%2520co-piloting%2520perspective%2520of%2520AI-assisted%250Adevelopment.%2520We%2520argue%2520that%2520VC%2520reconfigures%2520the%2520developers%2520role%252C%2520blurring%250Aboundaries%2520between%2520professional%2520and%2520non-developers.%2520While%2520VC%2520enables%2520novel%250Aforms%2520of%2520expression%2520and%2520rapid%2520prototyping%252C%2520it%2520also%2520introduces%2520challenges%250Aregarding%2520reproducibility%252C%2520scalability%252C%2520and%2520inclusivity.%2520We%2520propose%2520that%2520VC%250Arepresents%2520a%2520meaningful%2520shift%2520in%2520programming%2520culture%252C%2520warranting%2520further%250Ainvestigation%2520within%2520human-computer%2520interaction%2520%2528HCI%2529%2520and%2520software%2520engineering%250Aresearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12364v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%28R%29evolution%20of%20Programming%3A%20Vibe%20Coding%20as%20a%20Post-Coding%20Paradigm&entry.906535625=Kevin%20Krings%20and%20Nino%20S.%20Bohn%20and%20Thomas%20Ludwig&entry.1292438233=%20%20Recent%20advancements%20in%20generative%20artificial%20intelligence%20%28GenAI%29%2C%0Aparticularly%20large%20language%20models%2C%20have%20introduced%20new%20possibilities%20for%0Asoftware%20development%20practices.%20In%20our%20paper%20we%20investigate%20the%20emerging%20Vibe%0ACoding%20%28VC%29%20paradigm%20that%20emphasizes%20intuitive%2C%20affect-driven%2C%20and%0Aimprovisational%20interactions%20between%20developers%20and%20AI%20systems.%20Building%20upon%0Athe%20discourse%20of%20End-User%20Development%20%28EUD%29%2C%20we%20explore%20how%20VC%20diverges%20from%0Aconventional%20programming%20approaches%20such%20as%20those%20supported%20by%20tools%20like%0AGitHub%20Copilot.%20Through%20five%20semi-structured%20interview%20sessions%20with%20ten%0Aexperienced%20software%20practitioners%2C%20we%20identify%20five%20thematic%20dimensions%3A%0Acreativity%2C%20sustainability%2C%20the%20future%20of%20programming%2C%20collaboration%2C%20and%0Acriticism.%20Our%20analysis%20conceptualizes%20VC%20within%20the%20metaphor%20of%20co-drifting%2C%0Acontrasting%20it%20with%20the%20prevalent%20co-piloting%20perspective%20of%20AI-assisted%0Adevelopment.%20We%20argue%20that%20VC%20reconfigures%20the%20developers%20role%2C%20blurring%0Aboundaries%20between%20professional%20and%20non-developers.%20While%20VC%20enables%20novel%0Aforms%20of%20expression%20and%20rapid%20prototyping%2C%20it%20also%20introduces%20challenges%0Aregarding%20reproducibility%2C%20scalability%2C%20and%20inclusivity.%20We%20propose%20that%20VC%0Arepresents%20a%20meaningful%20shift%20in%20programming%20culture%2C%20warranting%20further%0Ainvestigation%20within%20human-computer%20interaction%20%28HCI%29%20and%20software%20engineering%0Aresearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12364v2&entry.124074799=Read"},
{"title": "LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action\n  Models", "author": "Senyu Fei and Siyin Wang and Junhao Shi and Zihao Dai and Jikun Cai and Pengfang Qian and Li Ji and Xinzhe He and Shiduo Zhang and Zhaoye Fei and Jinlan Fu and Jingjing Gong and Xipeng Qiu", "abstract": "  Visual-Language-Action (VLA) models report impressive success rates on\nrobotic manipulation benchmarks, yet these results may mask fundamental\nweaknesses in robustness. We perform a systematic vulnerability analysis by\nintroducing controlled perturbations across seven dimensions: objects layout,\ncamera viewpoints, robot initial states, language instructions, light\nconditions, background textures and sensor noise. We comprehensively analyzed\nmultiple state-of-the-art models and revealed consistent brittleness beneath\napparent competence. Our analysis exposes critical weaknesses: models exhibit\nextreme sensitivity to perturbation factors, including camera viewpoints and\nrobot initial states, with performance dropping from 95% to below 30% under\nmodest perturbations. Surprisingly, models are largely insensitive to language\nvariations, with further experiments revealing that models tend to ignore\nlanguage instructions completely. Our findings challenge the assumption that\nhigh benchmark scores equate to true competency and highlight the need for\nevaluation practices that assess reliability under realistic variation.\n", "link": "http://arxiv.org/abs/2510.13626v1", "date": "2025-10-15", "relevancy": 2.3254, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5881}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5881}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LIBERO-Plus%3A%20In-depth%20Robustness%20Analysis%20of%20Vision-Language-Action%0A%20%20Models&body=Title%3A%20LIBERO-Plus%3A%20In-depth%20Robustness%20Analysis%20of%20Vision-Language-Action%0A%20%20Models%0AAuthor%3A%20Senyu%20Fei%20and%20Siyin%20Wang%20and%20Junhao%20Shi%20and%20Zihao%20Dai%20and%20Jikun%20Cai%20and%20Pengfang%20Qian%20and%20Li%20Ji%20and%20Xinzhe%20He%20and%20Shiduo%20Zhang%20and%20Zhaoye%20Fei%20and%20Jinlan%20Fu%20and%20Jingjing%20Gong%20and%20Xipeng%20Qiu%0AAbstract%3A%20%20%20Visual-Language-Action%20%28VLA%29%20models%20report%20impressive%20success%20rates%20on%0Arobotic%20manipulation%20benchmarks%2C%20yet%20these%20results%20may%20mask%20fundamental%0Aweaknesses%20in%20robustness.%20We%20perform%20a%20systematic%20vulnerability%20analysis%20by%0Aintroducing%20controlled%20perturbations%20across%20seven%20dimensions%3A%20objects%20layout%2C%0Acamera%20viewpoints%2C%20robot%20initial%20states%2C%20language%20instructions%2C%20light%0Aconditions%2C%20background%20textures%20and%20sensor%20noise.%20We%20comprehensively%20analyzed%0Amultiple%20state-of-the-art%20models%20and%20revealed%20consistent%20brittleness%20beneath%0Aapparent%20competence.%20Our%20analysis%20exposes%20critical%20weaknesses%3A%20models%20exhibit%0Aextreme%20sensitivity%20to%20perturbation%20factors%2C%20including%20camera%20viewpoints%20and%0Arobot%20initial%20states%2C%20with%20performance%20dropping%20from%2095%25%20to%20below%2030%25%20under%0Amodest%20perturbations.%20Surprisingly%2C%20models%20are%20largely%20insensitive%20to%20language%0Avariations%2C%20with%20further%20experiments%20revealing%20that%20models%20tend%20to%20ignore%0Alanguage%20instructions%20completely.%20Our%20findings%20challenge%20the%20assumption%20that%0Ahigh%20benchmark%20scores%20equate%20to%20true%20competency%20and%20highlight%20the%20need%20for%0Aevaluation%20practices%20that%20assess%20reliability%20under%20realistic%20variation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13626v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLIBERO-Plus%253A%2520In-depth%2520Robustness%2520Analysis%2520of%2520Vision-Language-Action%250A%2520%2520Models%26entry.906535625%3DSenyu%2520Fei%2520and%2520Siyin%2520Wang%2520and%2520Junhao%2520Shi%2520and%2520Zihao%2520Dai%2520and%2520Jikun%2520Cai%2520and%2520Pengfang%2520Qian%2520and%2520Li%2520Ji%2520and%2520Xinzhe%2520He%2520and%2520Shiduo%2520Zhang%2520and%2520Zhaoye%2520Fei%2520and%2520Jinlan%2520Fu%2520and%2520Jingjing%2520Gong%2520and%2520Xipeng%2520Qiu%26entry.1292438233%3D%2520%2520Visual-Language-Action%2520%2528VLA%2529%2520models%2520report%2520impressive%2520success%2520rates%2520on%250Arobotic%2520manipulation%2520benchmarks%252C%2520yet%2520these%2520results%2520may%2520mask%2520fundamental%250Aweaknesses%2520in%2520robustness.%2520We%2520perform%2520a%2520systematic%2520vulnerability%2520analysis%2520by%250Aintroducing%2520controlled%2520perturbations%2520across%2520seven%2520dimensions%253A%2520objects%2520layout%252C%250Acamera%2520viewpoints%252C%2520robot%2520initial%2520states%252C%2520language%2520instructions%252C%2520light%250Aconditions%252C%2520background%2520textures%2520and%2520sensor%2520noise.%2520We%2520comprehensively%2520analyzed%250Amultiple%2520state-of-the-art%2520models%2520and%2520revealed%2520consistent%2520brittleness%2520beneath%250Aapparent%2520competence.%2520Our%2520analysis%2520exposes%2520critical%2520weaknesses%253A%2520models%2520exhibit%250Aextreme%2520sensitivity%2520to%2520perturbation%2520factors%252C%2520including%2520camera%2520viewpoints%2520and%250Arobot%2520initial%2520states%252C%2520with%2520performance%2520dropping%2520from%252095%2525%2520to%2520below%252030%2525%2520under%250Amodest%2520perturbations.%2520Surprisingly%252C%2520models%2520are%2520largely%2520insensitive%2520to%2520language%250Avariations%252C%2520with%2520further%2520experiments%2520revealing%2520that%2520models%2520tend%2520to%2520ignore%250Alanguage%2520instructions%2520completely.%2520Our%2520findings%2520challenge%2520the%2520assumption%2520that%250Ahigh%2520benchmark%2520scores%2520equate%2520to%2520true%2520competency%2520and%2520highlight%2520the%2520need%2520for%250Aevaluation%2520practices%2520that%2520assess%2520reliability%2520under%2520realistic%2520variation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13626v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LIBERO-Plus%3A%20In-depth%20Robustness%20Analysis%20of%20Vision-Language-Action%0A%20%20Models&entry.906535625=Senyu%20Fei%20and%20Siyin%20Wang%20and%20Junhao%20Shi%20and%20Zihao%20Dai%20and%20Jikun%20Cai%20and%20Pengfang%20Qian%20and%20Li%20Ji%20and%20Xinzhe%20He%20and%20Shiduo%20Zhang%20and%20Zhaoye%20Fei%20and%20Jinlan%20Fu%20and%20Jingjing%20Gong%20and%20Xipeng%20Qiu&entry.1292438233=%20%20Visual-Language-Action%20%28VLA%29%20models%20report%20impressive%20success%20rates%20on%0Arobotic%20manipulation%20benchmarks%2C%20yet%20these%20results%20may%20mask%20fundamental%0Aweaknesses%20in%20robustness.%20We%20perform%20a%20systematic%20vulnerability%20analysis%20by%0Aintroducing%20controlled%20perturbations%20across%20seven%20dimensions%3A%20objects%20layout%2C%0Acamera%20viewpoints%2C%20robot%20initial%20states%2C%20language%20instructions%2C%20light%0Aconditions%2C%20background%20textures%20and%20sensor%20noise.%20We%20comprehensively%20analyzed%0Amultiple%20state-of-the-art%20models%20and%20revealed%20consistent%20brittleness%20beneath%0Aapparent%20competence.%20Our%20analysis%20exposes%20critical%20weaknesses%3A%20models%20exhibit%0Aextreme%20sensitivity%20to%20perturbation%20factors%2C%20including%20camera%20viewpoints%20and%0Arobot%20initial%20states%2C%20with%20performance%20dropping%20from%2095%25%20to%20below%2030%25%20under%0Amodest%20perturbations.%20Surprisingly%2C%20models%20are%20largely%20insensitive%20to%20language%0Avariations%2C%20with%20further%20experiments%20revealing%20that%20models%20tend%20to%20ignore%0Alanguage%20instructions%20completely.%20Our%20findings%20challenge%20the%20assumption%20that%0Ahigh%20benchmark%20scores%20equate%20to%20true%20competency%20and%20highlight%20the%20need%20for%0Aevaluation%20practices%20that%20assess%20reliability%20under%20realistic%20variation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13626v1&entry.124074799=Read"},
{"title": "CanvasMAR: Improving Masked Autoregressive Video Generation With Canvas", "author": "Zian Li and Muhan Zhang", "abstract": "  Masked autoregressive models (MAR) have recently emerged as a powerful\nparadigm for image and video generation, combining the flexibility of masked\nmodeling with the potential of continuous tokenizer. However, video MAR models\nsuffer from two major limitations: the slow-start problem, caused by the lack\nof a structured global prior at early sampling stages, and error accumulation\nacross the autoregression in both spatial and temporal dimensions. In this\nwork, we propose CanvasMAR, a novel video MAR model that mitigates these issues\nby introducing a canvas mechanism--a blurred, global prediction of the next\nframe, used as the starting point for masked generation. The canvas provides\nglobal structure early in sampling, enabling faster and more coherent frame\nsynthesis. Furthermore, we introduce compositional classifier-free guidance\nthat jointly enlarges spatial (canvas) and temporal conditioning, and employ\nnoise-based canvas augmentation to enhance robustness. Experiments on the BAIR\nand Kinetics-600 benchmarks demonstrate that CanvasMAR produces high-quality\nvideos with fewer autoregressive steps. Our approach achieves remarkable\nperformance among autoregressive models on Kinetics-600 dataset and rivals\ndiffusion-based methods.\n", "link": "http://arxiv.org/abs/2510.13669v1", "date": "2025-10-15", "relevancy": 2.3174, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5853}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5851}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CanvasMAR%3A%20Improving%20Masked%20Autoregressive%20Video%20Generation%20With%20Canvas&body=Title%3A%20CanvasMAR%3A%20Improving%20Masked%20Autoregressive%20Video%20Generation%20With%20Canvas%0AAuthor%3A%20Zian%20Li%20and%20Muhan%20Zhang%0AAbstract%3A%20%20%20Masked%20autoregressive%20models%20%28MAR%29%20have%20recently%20emerged%20as%20a%20powerful%0Aparadigm%20for%20image%20and%20video%20generation%2C%20combining%20the%20flexibility%20of%20masked%0Amodeling%20with%20the%20potential%20of%20continuous%20tokenizer.%20However%2C%20video%20MAR%20models%0Asuffer%20from%20two%20major%20limitations%3A%20the%20slow-start%20problem%2C%20caused%20by%20the%20lack%0Aof%20a%20structured%20global%20prior%20at%20early%20sampling%20stages%2C%20and%20error%20accumulation%0Aacross%20the%20autoregression%20in%20both%20spatial%20and%20temporal%20dimensions.%20In%20this%0Awork%2C%20we%20propose%20CanvasMAR%2C%20a%20novel%20video%20MAR%20model%20that%20mitigates%20these%20issues%0Aby%20introducing%20a%20canvas%20mechanism--a%20blurred%2C%20global%20prediction%20of%20the%20next%0Aframe%2C%20used%20as%20the%20starting%20point%20for%20masked%20generation.%20The%20canvas%20provides%0Aglobal%20structure%20early%20in%20sampling%2C%20enabling%20faster%20and%20more%20coherent%20frame%0Asynthesis.%20Furthermore%2C%20we%20introduce%20compositional%20classifier-free%20guidance%0Athat%20jointly%20enlarges%20spatial%20%28canvas%29%20and%20temporal%20conditioning%2C%20and%20employ%0Anoise-based%20canvas%20augmentation%20to%20enhance%20robustness.%20Experiments%20on%20the%20BAIR%0Aand%20Kinetics-600%20benchmarks%20demonstrate%20that%20CanvasMAR%20produces%20high-quality%0Avideos%20with%20fewer%20autoregressive%20steps.%20Our%20approach%20achieves%20remarkable%0Aperformance%20among%20autoregressive%20models%20on%20Kinetics-600%20dataset%20and%20rivals%0Adiffusion-based%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13669v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCanvasMAR%253A%2520Improving%2520Masked%2520Autoregressive%2520Video%2520Generation%2520With%2520Canvas%26entry.906535625%3DZian%2520Li%2520and%2520Muhan%2520Zhang%26entry.1292438233%3D%2520%2520Masked%2520autoregressive%2520models%2520%2528MAR%2529%2520have%2520recently%2520emerged%2520as%2520a%2520powerful%250Aparadigm%2520for%2520image%2520and%2520video%2520generation%252C%2520combining%2520the%2520flexibility%2520of%2520masked%250Amodeling%2520with%2520the%2520potential%2520of%2520continuous%2520tokenizer.%2520However%252C%2520video%2520MAR%2520models%250Asuffer%2520from%2520two%2520major%2520limitations%253A%2520the%2520slow-start%2520problem%252C%2520caused%2520by%2520the%2520lack%250Aof%2520a%2520structured%2520global%2520prior%2520at%2520early%2520sampling%2520stages%252C%2520and%2520error%2520accumulation%250Aacross%2520the%2520autoregression%2520in%2520both%2520spatial%2520and%2520temporal%2520dimensions.%2520In%2520this%250Awork%252C%2520we%2520propose%2520CanvasMAR%252C%2520a%2520novel%2520video%2520MAR%2520model%2520that%2520mitigates%2520these%2520issues%250Aby%2520introducing%2520a%2520canvas%2520mechanism--a%2520blurred%252C%2520global%2520prediction%2520of%2520the%2520next%250Aframe%252C%2520used%2520as%2520the%2520starting%2520point%2520for%2520masked%2520generation.%2520The%2520canvas%2520provides%250Aglobal%2520structure%2520early%2520in%2520sampling%252C%2520enabling%2520faster%2520and%2520more%2520coherent%2520frame%250Asynthesis.%2520Furthermore%252C%2520we%2520introduce%2520compositional%2520classifier-free%2520guidance%250Athat%2520jointly%2520enlarges%2520spatial%2520%2528canvas%2529%2520and%2520temporal%2520conditioning%252C%2520and%2520employ%250Anoise-based%2520canvas%2520augmentation%2520to%2520enhance%2520robustness.%2520Experiments%2520on%2520the%2520BAIR%250Aand%2520Kinetics-600%2520benchmarks%2520demonstrate%2520that%2520CanvasMAR%2520produces%2520high-quality%250Avideos%2520with%2520fewer%2520autoregressive%2520steps.%2520Our%2520approach%2520achieves%2520remarkable%250Aperformance%2520among%2520autoregressive%2520models%2520on%2520Kinetics-600%2520dataset%2520and%2520rivals%250Adiffusion-based%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13669v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CanvasMAR%3A%20Improving%20Masked%20Autoregressive%20Video%20Generation%20With%20Canvas&entry.906535625=Zian%20Li%20and%20Muhan%20Zhang&entry.1292438233=%20%20Masked%20autoregressive%20models%20%28MAR%29%20have%20recently%20emerged%20as%20a%20powerful%0Aparadigm%20for%20image%20and%20video%20generation%2C%20combining%20the%20flexibility%20of%20masked%0Amodeling%20with%20the%20potential%20of%20continuous%20tokenizer.%20However%2C%20video%20MAR%20models%0Asuffer%20from%20two%20major%20limitations%3A%20the%20slow-start%20problem%2C%20caused%20by%20the%20lack%0Aof%20a%20structured%20global%20prior%20at%20early%20sampling%20stages%2C%20and%20error%20accumulation%0Aacross%20the%20autoregression%20in%20both%20spatial%20and%20temporal%20dimensions.%20In%20this%0Awork%2C%20we%20propose%20CanvasMAR%2C%20a%20novel%20video%20MAR%20model%20that%20mitigates%20these%20issues%0Aby%20introducing%20a%20canvas%20mechanism--a%20blurred%2C%20global%20prediction%20of%20the%20next%0Aframe%2C%20used%20as%20the%20starting%20point%20for%20masked%20generation.%20The%20canvas%20provides%0Aglobal%20structure%20early%20in%20sampling%2C%20enabling%20faster%20and%20more%20coherent%20frame%0Asynthesis.%20Furthermore%2C%20we%20introduce%20compositional%20classifier-free%20guidance%0Athat%20jointly%20enlarges%20spatial%20%28canvas%29%20and%20temporal%20conditioning%2C%20and%20employ%0Anoise-based%20canvas%20augmentation%20to%20enhance%20robustness.%20Experiments%20on%20the%20BAIR%0Aand%20Kinetics-600%20benchmarks%20demonstrate%20that%20CanvasMAR%20produces%20high-quality%0Avideos%20with%20fewer%20autoregressive%20steps.%20Our%20approach%20achieves%20remarkable%0Aperformance%20among%20autoregressive%20models%20on%20Kinetics-600%20dataset%20and%20rivals%0Adiffusion-based%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13669v1&entry.124074799=Read"},
{"title": "Spatial-DISE: A Unified Benchmark for Evaluating Spatial Reasoning in\n  Vision-Language Models", "author": "Xinmiao Huang and Qisong He and Zhenglin Huang and Boxuan Wang and Zhuoyun Li and Guangliang Cheng and Yi Dong and Xiaowei Huang", "abstract": "  Spatial reasoning ability is crucial for Vision Language Models (VLMs) to\nsupport real-world applications in diverse domains including robotics,\naugmented reality, and autonomous navigation. Unfortunately, existing\nbenchmarks are inadequate in assessing spatial reasoning ability, especially\nthe \\emph{intrinsic-dynamic} spatial reasoning which is a fundamental aspect of\nhuman spatial cognition. In this paper, we propose a unified benchmark,\n\\textbf{Spatial-DISE}, based on a cognitively grounded taxonomy that\ncategorizes tasks into four fundamental quadrants:\n\\textbf{I}ntrinsic-\\textbf{S}tatic, Intrinsic-\\textbf{D}ynamic,\n\\textbf{E}xtrinsic-Static, and Extrinsic-Dynamic spatial reasoning. Moreover,\nto address the issue of data scarcity, we develop a scalable and automated\npipeline to generate diverse and verifiable spatial reasoning questions,\nresulting in a new \\textbf{Spatial-DISE} dataset that includes Spatial-DISE\nBench (559 evaluation VQA pairs) and Spatial-DISE-12K (12K+ training VQA\npairs). Our comprehensive evaluation across 28 state-of-the-art VLMs reveals\nthat, current VLMs have a large and consistent gap to human competence,\nespecially on multi-step multi-view spatial reasoning. Spatial-DISE offers a\nrobust framework, valuable dataset, and clear direction for future research\ntoward human-like spatial intelligence. Benchmark, dataset, and code will be\npublicly released.\n", "link": "http://arxiv.org/abs/2510.13394v1", "date": "2025-10-15", "relevancy": 2.3101, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5911}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5911}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatial-DISE%3A%20A%20Unified%20Benchmark%20for%20Evaluating%20Spatial%20Reasoning%20in%0A%20%20Vision-Language%20Models&body=Title%3A%20Spatial-DISE%3A%20A%20Unified%20Benchmark%20for%20Evaluating%20Spatial%20Reasoning%20in%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Xinmiao%20Huang%20and%20Qisong%20He%20and%20Zhenglin%20Huang%20and%20Boxuan%20Wang%20and%20Zhuoyun%20Li%20and%20Guangliang%20Cheng%20and%20Yi%20Dong%20and%20Xiaowei%20Huang%0AAbstract%3A%20%20%20Spatial%20reasoning%20ability%20is%20crucial%20for%20Vision%20Language%20Models%20%28VLMs%29%20to%0Asupport%20real-world%20applications%20in%20diverse%20domains%20including%20robotics%2C%0Aaugmented%20reality%2C%20and%20autonomous%20navigation.%20Unfortunately%2C%20existing%0Abenchmarks%20are%20inadequate%20in%20assessing%20spatial%20reasoning%20ability%2C%20especially%0Athe%20%5Cemph%7Bintrinsic-dynamic%7D%20spatial%20reasoning%20which%20is%20a%20fundamental%20aspect%20of%0Ahuman%20spatial%20cognition.%20In%20this%20paper%2C%20we%20propose%20a%20unified%20benchmark%2C%0A%5Ctextbf%7BSpatial-DISE%7D%2C%20based%20on%20a%20cognitively%20grounded%20taxonomy%20that%0Acategorizes%20tasks%20into%20four%20fundamental%20quadrants%3A%0A%5Ctextbf%7BI%7Dntrinsic-%5Ctextbf%7BS%7Dtatic%2C%20Intrinsic-%5Ctextbf%7BD%7Dynamic%2C%0A%5Ctextbf%7BE%7Dxtrinsic-Static%2C%20and%20Extrinsic-Dynamic%20spatial%20reasoning.%20Moreover%2C%0Ato%20address%20the%20issue%20of%20data%20scarcity%2C%20we%20develop%20a%20scalable%20and%20automated%0Apipeline%20to%20generate%20diverse%20and%20verifiable%20spatial%20reasoning%20questions%2C%0Aresulting%20in%20a%20new%20%5Ctextbf%7BSpatial-DISE%7D%20dataset%20that%20includes%20Spatial-DISE%0ABench%20%28559%20evaluation%20VQA%20pairs%29%20and%20Spatial-DISE-12K%20%2812K%2B%20training%20VQA%0Apairs%29.%20Our%20comprehensive%20evaluation%20across%2028%20state-of-the-art%20VLMs%20reveals%0Athat%2C%20current%20VLMs%20have%20a%20large%20and%20consistent%20gap%20to%20human%20competence%2C%0Aespecially%20on%20multi-step%20multi-view%20spatial%20reasoning.%20Spatial-DISE%20offers%20a%0Arobust%20framework%2C%20valuable%20dataset%2C%20and%20clear%20direction%20for%20future%20research%0Atoward%20human-like%20spatial%20intelligence.%20Benchmark%2C%20dataset%2C%20and%20code%20will%20be%0Apublicly%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13394v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatial-DISE%253A%2520A%2520Unified%2520Benchmark%2520for%2520Evaluating%2520Spatial%2520Reasoning%2520in%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DXinmiao%2520Huang%2520and%2520Qisong%2520He%2520and%2520Zhenglin%2520Huang%2520and%2520Boxuan%2520Wang%2520and%2520Zhuoyun%2520Li%2520and%2520Guangliang%2520Cheng%2520and%2520Yi%2520Dong%2520and%2520Xiaowei%2520Huang%26entry.1292438233%3D%2520%2520Spatial%2520reasoning%2520ability%2520is%2520crucial%2520for%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520to%250Asupport%2520real-world%2520applications%2520in%2520diverse%2520domains%2520including%2520robotics%252C%250Aaugmented%2520reality%252C%2520and%2520autonomous%2520navigation.%2520Unfortunately%252C%2520existing%250Abenchmarks%2520are%2520inadequate%2520in%2520assessing%2520spatial%2520reasoning%2520ability%252C%2520especially%250Athe%2520%255Cemph%257Bintrinsic-dynamic%257D%2520spatial%2520reasoning%2520which%2520is%2520a%2520fundamental%2520aspect%2520of%250Ahuman%2520spatial%2520cognition.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520unified%2520benchmark%252C%250A%255Ctextbf%257BSpatial-DISE%257D%252C%2520based%2520on%2520a%2520cognitively%2520grounded%2520taxonomy%2520that%250Acategorizes%2520tasks%2520into%2520four%2520fundamental%2520quadrants%253A%250A%255Ctextbf%257BI%257Dntrinsic-%255Ctextbf%257BS%257Dtatic%252C%2520Intrinsic-%255Ctextbf%257BD%257Dynamic%252C%250A%255Ctextbf%257BE%257Dxtrinsic-Static%252C%2520and%2520Extrinsic-Dynamic%2520spatial%2520reasoning.%2520Moreover%252C%250Ato%2520address%2520the%2520issue%2520of%2520data%2520scarcity%252C%2520we%2520develop%2520a%2520scalable%2520and%2520automated%250Apipeline%2520to%2520generate%2520diverse%2520and%2520verifiable%2520spatial%2520reasoning%2520questions%252C%250Aresulting%2520in%2520a%2520new%2520%255Ctextbf%257BSpatial-DISE%257D%2520dataset%2520that%2520includes%2520Spatial-DISE%250ABench%2520%2528559%2520evaluation%2520VQA%2520pairs%2529%2520and%2520Spatial-DISE-12K%2520%252812K%252B%2520training%2520VQA%250Apairs%2529.%2520Our%2520comprehensive%2520evaluation%2520across%252028%2520state-of-the-art%2520VLMs%2520reveals%250Athat%252C%2520current%2520VLMs%2520have%2520a%2520large%2520and%2520consistent%2520gap%2520to%2520human%2520competence%252C%250Aespecially%2520on%2520multi-step%2520multi-view%2520spatial%2520reasoning.%2520Spatial-DISE%2520offers%2520a%250Arobust%2520framework%252C%2520valuable%2520dataset%252C%2520and%2520clear%2520direction%2520for%2520future%2520research%250Atoward%2520human-like%2520spatial%2520intelligence.%2520Benchmark%252C%2520dataset%252C%2520and%2520code%2520will%2520be%250Apublicly%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13394v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial-DISE%3A%20A%20Unified%20Benchmark%20for%20Evaluating%20Spatial%20Reasoning%20in%0A%20%20Vision-Language%20Models&entry.906535625=Xinmiao%20Huang%20and%20Qisong%20He%20and%20Zhenglin%20Huang%20and%20Boxuan%20Wang%20and%20Zhuoyun%20Li%20and%20Guangliang%20Cheng%20and%20Yi%20Dong%20and%20Xiaowei%20Huang&entry.1292438233=%20%20Spatial%20reasoning%20ability%20is%20crucial%20for%20Vision%20Language%20Models%20%28VLMs%29%20to%0Asupport%20real-world%20applications%20in%20diverse%20domains%20including%20robotics%2C%0Aaugmented%20reality%2C%20and%20autonomous%20navigation.%20Unfortunately%2C%20existing%0Abenchmarks%20are%20inadequate%20in%20assessing%20spatial%20reasoning%20ability%2C%20especially%0Athe%20%5Cemph%7Bintrinsic-dynamic%7D%20spatial%20reasoning%20which%20is%20a%20fundamental%20aspect%20of%0Ahuman%20spatial%20cognition.%20In%20this%20paper%2C%20we%20propose%20a%20unified%20benchmark%2C%0A%5Ctextbf%7BSpatial-DISE%7D%2C%20based%20on%20a%20cognitively%20grounded%20taxonomy%20that%0Acategorizes%20tasks%20into%20four%20fundamental%20quadrants%3A%0A%5Ctextbf%7BI%7Dntrinsic-%5Ctextbf%7BS%7Dtatic%2C%20Intrinsic-%5Ctextbf%7BD%7Dynamic%2C%0A%5Ctextbf%7BE%7Dxtrinsic-Static%2C%20and%20Extrinsic-Dynamic%20spatial%20reasoning.%20Moreover%2C%0Ato%20address%20the%20issue%20of%20data%20scarcity%2C%20we%20develop%20a%20scalable%20and%20automated%0Apipeline%20to%20generate%20diverse%20and%20verifiable%20spatial%20reasoning%20questions%2C%0Aresulting%20in%20a%20new%20%5Ctextbf%7BSpatial-DISE%7D%20dataset%20that%20includes%20Spatial-DISE%0ABench%20%28559%20evaluation%20VQA%20pairs%29%20and%20Spatial-DISE-12K%20%2812K%2B%20training%20VQA%0Apairs%29.%20Our%20comprehensive%20evaluation%20across%2028%20state-of-the-art%20VLMs%20reveals%0Athat%2C%20current%20VLMs%20have%20a%20large%20and%20consistent%20gap%20to%20human%20competence%2C%0Aespecially%20on%20multi-step%20multi-view%20spatial%20reasoning.%20Spatial-DISE%20offers%20a%0Arobust%20framework%2C%20valuable%20dataset%2C%20and%20clear%20direction%20for%20future%20research%0Atoward%20human-like%20spatial%20intelligence.%20Benchmark%2C%20dataset%2C%20and%20code%20will%20be%0Apublicly%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13394v1&entry.124074799=Read"},
{"title": "OmniGaze: Reward-inspired Generalizable Gaze Estimation In The Wild", "author": "Hongyu Qu and Jianan Wei and Xiangbo Shu and Yazhou Yao and Wenguan Wang and Jinhui Tang", "abstract": "  Current 3D gaze estimation methods struggle to generalize across diverse data\ndomains, primarily due to i) the scarcity of annotated datasets, and ii) the\ninsufficient diversity of labeled data. In this work, we present OmniGaze, a\nsemi-supervised framework for 3D gaze estimation, which utilizes large-scale\nunlabeled data collected from diverse and unconstrained real-world environments\nto mitigate domain bias and generalize gaze estimation in the wild. First, we\nbuild a diverse collection of unlabeled facial images, varying in facial\nappearances, background environments, illumination conditions, head poses, and\neye occlusions. In order to leverage unlabeled data spanning a broader\ndistribution, OmniGaze adopts a standard pseudo-labeling strategy and devises a\nreward model to assess the reliability of pseudo labels. Beyond pseudo labels\nas 3D direction vectors, the reward model also incorporates visual embeddings\nextracted by an off-the-shelf visual encoder and semantic cues from gaze\nperspective generated by prompting a Multimodal Large Language Model to compute\nconfidence scores. Then, these scores are utilized to select high-quality\npseudo labels and weight them for loss computation. Extensive experiments\ndemonstrate that OmniGaze achieves state-of-the-art performance on five\ndatasets under both in-domain and cross-domain settings. Furthermore, we also\nevaluate the efficacy of OmniGaze as a scalable data engine for gaze\nestimation, which exhibits robust zero-shot generalization on four unseen\ndatasets.\n", "link": "http://arxiv.org/abs/2510.13660v1", "date": "2025-10-15", "relevancy": 2.3048, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5935}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5753}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5702}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniGaze%3A%20Reward-inspired%20Generalizable%20Gaze%20Estimation%20In%20The%20Wild&body=Title%3A%20OmniGaze%3A%20Reward-inspired%20Generalizable%20Gaze%20Estimation%20In%20The%20Wild%0AAuthor%3A%20Hongyu%20Qu%20and%20Jianan%20Wei%20and%20Xiangbo%20Shu%20and%20Yazhou%20Yao%20and%20Wenguan%20Wang%20and%20Jinhui%20Tang%0AAbstract%3A%20%20%20Current%203D%20gaze%20estimation%20methods%20struggle%20to%20generalize%20across%20diverse%20data%0Adomains%2C%20primarily%20due%20to%20i%29%20the%20scarcity%20of%20annotated%20datasets%2C%20and%20ii%29%20the%0Ainsufficient%20diversity%20of%20labeled%20data.%20In%20this%20work%2C%20we%20present%20OmniGaze%2C%20a%0Asemi-supervised%20framework%20for%203D%20gaze%20estimation%2C%20which%20utilizes%20large-scale%0Aunlabeled%20data%20collected%20from%20diverse%20and%20unconstrained%20real-world%20environments%0Ato%20mitigate%20domain%20bias%20and%20generalize%20gaze%20estimation%20in%20the%20wild.%20First%2C%20we%0Abuild%20a%20diverse%20collection%20of%20unlabeled%20facial%20images%2C%20varying%20in%20facial%0Aappearances%2C%20background%20environments%2C%20illumination%20conditions%2C%20head%20poses%2C%20and%0Aeye%20occlusions.%20In%20order%20to%20leverage%20unlabeled%20data%20spanning%20a%20broader%0Adistribution%2C%20OmniGaze%20adopts%20a%20standard%20pseudo-labeling%20strategy%20and%20devises%20a%0Areward%20model%20to%20assess%20the%20reliability%20of%20pseudo%20labels.%20Beyond%20pseudo%20labels%0Aas%203D%20direction%20vectors%2C%20the%20reward%20model%20also%20incorporates%20visual%20embeddings%0Aextracted%20by%20an%20off-the-shelf%20visual%20encoder%20and%20semantic%20cues%20from%20gaze%0Aperspective%20generated%20by%20prompting%20a%20Multimodal%20Large%20Language%20Model%20to%20compute%0Aconfidence%20scores.%20Then%2C%20these%20scores%20are%20utilized%20to%20select%20high-quality%0Apseudo%20labels%20and%20weight%20them%20for%20loss%20computation.%20Extensive%20experiments%0Ademonstrate%20that%20OmniGaze%20achieves%20state-of-the-art%20performance%20on%20five%0Adatasets%20under%20both%20in-domain%20and%20cross-domain%20settings.%20Furthermore%2C%20we%20also%0Aevaluate%20the%20efficacy%20of%20OmniGaze%20as%20a%20scalable%20data%20engine%20for%20gaze%0Aestimation%2C%20which%20exhibits%20robust%20zero-shot%20generalization%20on%20four%20unseen%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13660v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniGaze%253A%2520Reward-inspired%2520Generalizable%2520Gaze%2520Estimation%2520In%2520The%2520Wild%26entry.906535625%3DHongyu%2520Qu%2520and%2520Jianan%2520Wei%2520and%2520Xiangbo%2520Shu%2520and%2520Yazhou%2520Yao%2520and%2520Wenguan%2520Wang%2520and%2520Jinhui%2520Tang%26entry.1292438233%3D%2520%2520Current%25203D%2520gaze%2520estimation%2520methods%2520struggle%2520to%2520generalize%2520across%2520diverse%2520data%250Adomains%252C%2520primarily%2520due%2520to%2520i%2529%2520the%2520scarcity%2520of%2520annotated%2520datasets%252C%2520and%2520ii%2529%2520the%250Ainsufficient%2520diversity%2520of%2520labeled%2520data.%2520In%2520this%2520work%252C%2520we%2520present%2520OmniGaze%252C%2520a%250Asemi-supervised%2520framework%2520for%25203D%2520gaze%2520estimation%252C%2520which%2520utilizes%2520large-scale%250Aunlabeled%2520data%2520collected%2520from%2520diverse%2520and%2520unconstrained%2520real-world%2520environments%250Ato%2520mitigate%2520domain%2520bias%2520and%2520generalize%2520gaze%2520estimation%2520in%2520the%2520wild.%2520First%252C%2520we%250Abuild%2520a%2520diverse%2520collection%2520of%2520unlabeled%2520facial%2520images%252C%2520varying%2520in%2520facial%250Aappearances%252C%2520background%2520environments%252C%2520illumination%2520conditions%252C%2520head%2520poses%252C%2520and%250Aeye%2520occlusions.%2520In%2520order%2520to%2520leverage%2520unlabeled%2520data%2520spanning%2520a%2520broader%250Adistribution%252C%2520OmniGaze%2520adopts%2520a%2520standard%2520pseudo-labeling%2520strategy%2520and%2520devises%2520a%250Areward%2520model%2520to%2520assess%2520the%2520reliability%2520of%2520pseudo%2520labels.%2520Beyond%2520pseudo%2520labels%250Aas%25203D%2520direction%2520vectors%252C%2520the%2520reward%2520model%2520also%2520incorporates%2520visual%2520embeddings%250Aextracted%2520by%2520an%2520off-the-shelf%2520visual%2520encoder%2520and%2520semantic%2520cues%2520from%2520gaze%250Aperspective%2520generated%2520by%2520prompting%2520a%2520Multimodal%2520Large%2520Language%2520Model%2520to%2520compute%250Aconfidence%2520scores.%2520Then%252C%2520these%2520scores%2520are%2520utilized%2520to%2520select%2520high-quality%250Apseudo%2520labels%2520and%2520weight%2520them%2520for%2520loss%2520computation.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520OmniGaze%2520achieves%2520state-of-the-art%2520performance%2520on%2520five%250Adatasets%2520under%2520both%2520in-domain%2520and%2520cross-domain%2520settings.%2520Furthermore%252C%2520we%2520also%250Aevaluate%2520the%2520efficacy%2520of%2520OmniGaze%2520as%2520a%2520scalable%2520data%2520engine%2520for%2520gaze%250Aestimation%252C%2520which%2520exhibits%2520robust%2520zero-shot%2520generalization%2520on%2520four%2520unseen%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13660v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniGaze%3A%20Reward-inspired%20Generalizable%20Gaze%20Estimation%20In%20The%20Wild&entry.906535625=Hongyu%20Qu%20and%20Jianan%20Wei%20and%20Xiangbo%20Shu%20and%20Yazhou%20Yao%20and%20Wenguan%20Wang%20and%20Jinhui%20Tang&entry.1292438233=%20%20Current%203D%20gaze%20estimation%20methods%20struggle%20to%20generalize%20across%20diverse%20data%0Adomains%2C%20primarily%20due%20to%20i%29%20the%20scarcity%20of%20annotated%20datasets%2C%20and%20ii%29%20the%0Ainsufficient%20diversity%20of%20labeled%20data.%20In%20this%20work%2C%20we%20present%20OmniGaze%2C%20a%0Asemi-supervised%20framework%20for%203D%20gaze%20estimation%2C%20which%20utilizes%20large-scale%0Aunlabeled%20data%20collected%20from%20diverse%20and%20unconstrained%20real-world%20environments%0Ato%20mitigate%20domain%20bias%20and%20generalize%20gaze%20estimation%20in%20the%20wild.%20First%2C%20we%0Abuild%20a%20diverse%20collection%20of%20unlabeled%20facial%20images%2C%20varying%20in%20facial%0Aappearances%2C%20background%20environments%2C%20illumination%20conditions%2C%20head%20poses%2C%20and%0Aeye%20occlusions.%20In%20order%20to%20leverage%20unlabeled%20data%20spanning%20a%20broader%0Adistribution%2C%20OmniGaze%20adopts%20a%20standard%20pseudo-labeling%20strategy%20and%20devises%20a%0Areward%20model%20to%20assess%20the%20reliability%20of%20pseudo%20labels.%20Beyond%20pseudo%20labels%0Aas%203D%20direction%20vectors%2C%20the%20reward%20model%20also%20incorporates%20visual%20embeddings%0Aextracted%20by%20an%20off-the-shelf%20visual%20encoder%20and%20semantic%20cues%20from%20gaze%0Aperspective%20generated%20by%20prompting%20a%20Multimodal%20Large%20Language%20Model%20to%20compute%0Aconfidence%20scores.%20Then%2C%20these%20scores%20are%20utilized%20to%20select%20high-quality%0Apseudo%20labels%20and%20weight%20them%20for%20loss%20computation.%20Extensive%20experiments%0Ademonstrate%20that%20OmniGaze%20achieves%20state-of-the-art%20performance%20on%20five%0Adatasets%20under%20both%20in-domain%20and%20cross-domain%20settings.%20Furthermore%2C%20we%20also%0Aevaluate%20the%20efficacy%20of%20OmniGaze%20as%20a%20scalable%20data%20engine%20for%20gaze%0Aestimation%2C%20which%20exhibits%20robust%20zero-shot%20generalization%20on%20four%20unseen%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13660v1&entry.124074799=Read"},
{"title": "ChA-MAEViT: Unifying Channel-Aware Masked Autoencoders and Multi-Channel\n  Vision Transformers for Improved Cross-Channel Learning", "author": "Chau Pham and Juan C. Caicedo and Bryan A. Plummer", "abstract": "  Prior work using Masked Autoencoders (MAEs) typically relies on random patch\nmasking based on the assumption that images have significant redundancies\nacross different channels, allowing for the reconstruction of masked content\nusing cross-channel correlations. However, this assumption does not hold in\nMulti-Channel Imaging (MCI), where channels may provide complementary\ninformation with minimal feature overlap. Thus, these MAEs primarily learn\nlocal structures within individual channels from patch reconstruction, failing\nto fully leverage cross-channel interactions and limiting their MCI\neffectiveness. In this paper, we present ChA-MAEViT, an MAE-based method that\nenhances feature learning across MCI channels via four key strategies: (1)\ndynamic channel-patch masking, which compels the model to reconstruct missing\nchannels in addition to masked patches, thereby enhancing cross-channel\ndependencies and improving robustness to varying channel configurations; (2)\nmemory tokens, which serve as long-term memory aids to promote information\nsharing across channels, addressing the challenges of reconstructing\nstructurally diverse channels; (3) hybrid token fusion module, which merges\nfine-grained patch tokens with a global class token to capture richer\nrepresentations; and (4) Channel-Aware Decoder, a lightweight decoder utilizes\nchannel tokens to effectively reconstruct image patches. Experiments on\nsatellite and microscopy datasets, CHAMMI, JUMP-CP, and So2Sat, show that\nChA-MAEViT significantly outperforms state-of-the-art MCI-ViTs by 3.0-21.5%,\nhighlighting the importance of cross-channel interactions in MCI. Our code is\npublicly available at https://github.com/chaudatascience/cha_mae_vit.\n", "link": "http://arxiv.org/abs/2503.19331v2", "date": "2025-10-15", "relevancy": 2.2985, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5984}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5687}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5299}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChA-MAEViT%3A%20Unifying%20Channel-Aware%20Masked%20Autoencoders%20and%20Multi-Channel%0A%20%20Vision%20Transformers%20for%20Improved%20Cross-Channel%20Learning&body=Title%3A%20ChA-MAEViT%3A%20Unifying%20Channel-Aware%20Masked%20Autoencoders%20and%20Multi-Channel%0A%20%20Vision%20Transformers%20for%20Improved%20Cross-Channel%20Learning%0AAuthor%3A%20Chau%20Pham%20and%20Juan%20C.%20Caicedo%20and%20Bryan%20A.%20Plummer%0AAbstract%3A%20%20%20Prior%20work%20using%20Masked%20Autoencoders%20%28MAEs%29%20typically%20relies%20on%20random%20patch%0Amasking%20based%20on%20the%20assumption%20that%20images%20have%20significant%20redundancies%0Aacross%20different%20channels%2C%20allowing%20for%20the%20reconstruction%20of%20masked%20content%0Ausing%20cross-channel%20correlations.%20However%2C%20this%20assumption%20does%20not%20hold%20in%0AMulti-Channel%20Imaging%20%28MCI%29%2C%20where%20channels%20may%20provide%20complementary%0Ainformation%20with%20minimal%20feature%20overlap.%20Thus%2C%20these%20MAEs%20primarily%20learn%0Alocal%20structures%20within%20individual%20channels%20from%20patch%20reconstruction%2C%20failing%0Ato%20fully%20leverage%20cross-channel%20interactions%20and%20limiting%20their%20MCI%0Aeffectiveness.%20In%20this%20paper%2C%20we%20present%20ChA-MAEViT%2C%20an%20MAE-based%20method%20that%0Aenhances%20feature%20learning%20across%20MCI%20channels%20via%20four%20key%20strategies%3A%20%281%29%0Adynamic%20channel-patch%20masking%2C%20which%20compels%20the%20model%20to%20reconstruct%20missing%0Achannels%20in%20addition%20to%20masked%20patches%2C%20thereby%20enhancing%20cross-channel%0Adependencies%20and%20improving%20robustness%20to%20varying%20channel%20configurations%3B%20%282%29%0Amemory%20tokens%2C%20which%20serve%20as%20long-term%20memory%20aids%20to%20promote%20information%0Asharing%20across%20channels%2C%20addressing%20the%20challenges%20of%20reconstructing%0Astructurally%20diverse%20channels%3B%20%283%29%20hybrid%20token%20fusion%20module%2C%20which%20merges%0Afine-grained%20patch%20tokens%20with%20a%20global%20class%20token%20to%20capture%20richer%0Arepresentations%3B%20and%20%284%29%20Channel-Aware%20Decoder%2C%20a%20lightweight%20decoder%20utilizes%0Achannel%20tokens%20to%20effectively%20reconstruct%20image%20patches.%20Experiments%20on%0Asatellite%20and%20microscopy%20datasets%2C%20CHAMMI%2C%20JUMP-CP%2C%20and%20So2Sat%2C%20show%20that%0AChA-MAEViT%20significantly%20outperforms%20state-of-the-art%20MCI-ViTs%20by%203.0-21.5%25%2C%0Ahighlighting%20the%20importance%20of%20cross-channel%20interactions%20in%20MCI.%20Our%20code%20is%0Apublicly%20available%20at%20https%3A//github.com/chaudatascience/cha_mae_vit.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.19331v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChA-MAEViT%253A%2520Unifying%2520Channel-Aware%2520Masked%2520Autoencoders%2520and%2520Multi-Channel%250A%2520%2520Vision%2520Transformers%2520for%2520Improved%2520Cross-Channel%2520Learning%26entry.906535625%3DChau%2520Pham%2520and%2520Juan%2520C.%2520Caicedo%2520and%2520Bryan%2520A.%2520Plummer%26entry.1292438233%3D%2520%2520Prior%2520work%2520using%2520Masked%2520Autoencoders%2520%2528MAEs%2529%2520typically%2520relies%2520on%2520random%2520patch%250Amasking%2520based%2520on%2520the%2520assumption%2520that%2520images%2520have%2520significant%2520redundancies%250Aacross%2520different%2520channels%252C%2520allowing%2520for%2520the%2520reconstruction%2520of%2520masked%2520content%250Ausing%2520cross-channel%2520correlations.%2520However%252C%2520this%2520assumption%2520does%2520not%2520hold%2520in%250AMulti-Channel%2520Imaging%2520%2528MCI%2529%252C%2520where%2520channels%2520may%2520provide%2520complementary%250Ainformation%2520with%2520minimal%2520feature%2520overlap.%2520Thus%252C%2520these%2520MAEs%2520primarily%2520learn%250Alocal%2520structures%2520within%2520individual%2520channels%2520from%2520patch%2520reconstruction%252C%2520failing%250Ato%2520fully%2520leverage%2520cross-channel%2520interactions%2520and%2520limiting%2520their%2520MCI%250Aeffectiveness.%2520In%2520this%2520paper%252C%2520we%2520present%2520ChA-MAEViT%252C%2520an%2520MAE-based%2520method%2520that%250Aenhances%2520feature%2520learning%2520across%2520MCI%2520channels%2520via%2520four%2520key%2520strategies%253A%2520%25281%2529%250Adynamic%2520channel-patch%2520masking%252C%2520which%2520compels%2520the%2520model%2520to%2520reconstruct%2520missing%250Achannels%2520in%2520addition%2520to%2520masked%2520patches%252C%2520thereby%2520enhancing%2520cross-channel%250Adependencies%2520and%2520improving%2520robustness%2520to%2520varying%2520channel%2520configurations%253B%2520%25282%2529%250Amemory%2520tokens%252C%2520which%2520serve%2520as%2520long-term%2520memory%2520aids%2520to%2520promote%2520information%250Asharing%2520across%2520channels%252C%2520addressing%2520the%2520challenges%2520of%2520reconstructing%250Astructurally%2520diverse%2520channels%253B%2520%25283%2529%2520hybrid%2520token%2520fusion%2520module%252C%2520which%2520merges%250Afine-grained%2520patch%2520tokens%2520with%2520a%2520global%2520class%2520token%2520to%2520capture%2520richer%250Arepresentations%253B%2520and%2520%25284%2529%2520Channel-Aware%2520Decoder%252C%2520a%2520lightweight%2520decoder%2520utilizes%250Achannel%2520tokens%2520to%2520effectively%2520reconstruct%2520image%2520patches.%2520Experiments%2520on%250Asatellite%2520and%2520microscopy%2520datasets%252C%2520CHAMMI%252C%2520JUMP-CP%252C%2520and%2520So2Sat%252C%2520show%2520that%250AChA-MAEViT%2520significantly%2520outperforms%2520state-of-the-art%2520MCI-ViTs%2520by%25203.0-21.5%2525%252C%250Ahighlighting%2520the%2520importance%2520of%2520cross-channel%2520interactions%2520in%2520MCI.%2520Our%2520code%2520is%250Apublicly%2520available%2520at%2520https%253A//github.com/chaudatascience/cha_mae_vit.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.19331v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChA-MAEViT%3A%20Unifying%20Channel-Aware%20Masked%20Autoencoders%20and%20Multi-Channel%0A%20%20Vision%20Transformers%20for%20Improved%20Cross-Channel%20Learning&entry.906535625=Chau%20Pham%20and%20Juan%20C.%20Caicedo%20and%20Bryan%20A.%20Plummer&entry.1292438233=%20%20Prior%20work%20using%20Masked%20Autoencoders%20%28MAEs%29%20typically%20relies%20on%20random%20patch%0Amasking%20based%20on%20the%20assumption%20that%20images%20have%20significant%20redundancies%0Aacross%20different%20channels%2C%20allowing%20for%20the%20reconstruction%20of%20masked%20content%0Ausing%20cross-channel%20correlations.%20However%2C%20this%20assumption%20does%20not%20hold%20in%0AMulti-Channel%20Imaging%20%28MCI%29%2C%20where%20channels%20may%20provide%20complementary%0Ainformation%20with%20minimal%20feature%20overlap.%20Thus%2C%20these%20MAEs%20primarily%20learn%0Alocal%20structures%20within%20individual%20channels%20from%20patch%20reconstruction%2C%20failing%0Ato%20fully%20leverage%20cross-channel%20interactions%20and%20limiting%20their%20MCI%0Aeffectiveness.%20In%20this%20paper%2C%20we%20present%20ChA-MAEViT%2C%20an%20MAE-based%20method%20that%0Aenhances%20feature%20learning%20across%20MCI%20channels%20via%20four%20key%20strategies%3A%20%281%29%0Adynamic%20channel-patch%20masking%2C%20which%20compels%20the%20model%20to%20reconstruct%20missing%0Achannels%20in%20addition%20to%20masked%20patches%2C%20thereby%20enhancing%20cross-channel%0Adependencies%20and%20improving%20robustness%20to%20varying%20channel%20configurations%3B%20%282%29%0Amemory%20tokens%2C%20which%20serve%20as%20long-term%20memory%20aids%20to%20promote%20information%0Asharing%20across%20channels%2C%20addressing%20the%20challenges%20of%20reconstructing%0Astructurally%20diverse%20channels%3B%20%283%29%20hybrid%20token%20fusion%20module%2C%20which%20merges%0Afine-grained%20patch%20tokens%20with%20a%20global%20class%20token%20to%20capture%20richer%0Arepresentations%3B%20and%20%284%29%20Channel-Aware%20Decoder%2C%20a%20lightweight%20decoder%20utilizes%0Achannel%20tokens%20to%20effectively%20reconstruct%20image%20patches.%20Experiments%20on%0Asatellite%20and%20microscopy%20datasets%2C%20CHAMMI%2C%20JUMP-CP%2C%20and%20So2Sat%2C%20show%20that%0AChA-MAEViT%20significantly%20outperforms%20state-of-the-art%20MCI-ViTs%20by%203.0-21.5%25%2C%0Ahighlighting%20the%20importance%20of%20cross-channel%20interactions%20in%20MCI.%20Our%20code%20is%0Apublicly%20available%20at%20https%3A//github.com/chaudatascience/cha_mae_vit.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.19331v2&entry.124074799=Read"},
{"title": "Aligning Large Language Models to Low-Resource Languages through\n  LLM-Based Selective Translation: A Systematic Study", "author": "Rakesh Paul and Anusha Kamath and Kanishk Singla and Raviraj Joshi and Utkarsh Vaidya and Sanjay Singh Chauhan and Niranjan Wartikar", "abstract": "  Multilingual large language models (LLMs) often demonstrate a performance gap\nbetween English and non-English languages, particularly in low-resource\nsettings. Aligning these models to low-resource languages is essential yet\nchallenging due to limited high-quality data. While English alignment datasets\nare readily available, curating equivalent data in other languages is expensive\nand time-consuming. A common workaround is to translate existing English\nalignment data; however, standard translation techniques often fail to preserve\ncritical elements such as code, mathematical expressions, and structured\nformats like JSON. In this work, we investigate LLM-based selective\ntranslation, a technique that selectively translates only the translatable\nparts of a text while preserving non-translatable content and sentence\nstructure. We conduct a systematic study to explore key questions around this\napproach, including its effectiveness compared to vanilla translation, the\nimportance of filtering noisy outputs, and the benefits of mixing translated\nsamples with original English data during alignment. Our experiments focus on\nthe low-resource Indic language Hindi and compare translations generated by\nGoogle Cloud Translation (GCP) and Llama-3.1-405B. The results highlight the\npromise of selective translation as a practical and effective method for\nimproving multilingual alignment in LLMs.\n", "link": "http://arxiv.org/abs/2507.14304v2", "date": "2025-10-15", "relevancy": 2.2716, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.455}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.455}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aligning%20Large%20Language%20Models%20to%20Low-Resource%20Languages%20through%0A%20%20LLM-Based%20Selective%20Translation%3A%20A%20Systematic%20Study&body=Title%3A%20Aligning%20Large%20Language%20Models%20to%20Low-Resource%20Languages%20through%0A%20%20LLM-Based%20Selective%20Translation%3A%20A%20Systematic%20Study%0AAuthor%3A%20Rakesh%20Paul%20and%20Anusha%20Kamath%20and%20Kanishk%20Singla%20and%20Raviraj%20Joshi%20and%20Utkarsh%20Vaidya%20and%20Sanjay%20Singh%20Chauhan%20and%20Niranjan%20Wartikar%0AAbstract%3A%20%20%20Multilingual%20large%20language%20models%20%28LLMs%29%20often%20demonstrate%20a%20performance%20gap%0Abetween%20English%20and%20non-English%20languages%2C%20particularly%20in%20low-resource%0Asettings.%20Aligning%20these%20models%20to%20low-resource%20languages%20is%20essential%20yet%0Achallenging%20due%20to%20limited%20high-quality%20data.%20While%20English%20alignment%20datasets%0Aare%20readily%20available%2C%20curating%20equivalent%20data%20in%20other%20languages%20is%20expensive%0Aand%20time-consuming.%20A%20common%20workaround%20is%20to%20translate%20existing%20English%0Aalignment%20data%3B%20however%2C%20standard%20translation%20techniques%20often%20fail%20to%20preserve%0Acritical%20elements%20such%20as%20code%2C%20mathematical%20expressions%2C%20and%20structured%0Aformats%20like%20JSON.%20In%20this%20work%2C%20we%20investigate%20LLM-based%20selective%0Atranslation%2C%20a%20technique%20that%20selectively%20translates%20only%20the%20translatable%0Aparts%20of%20a%20text%20while%20preserving%20non-translatable%20content%20and%20sentence%0Astructure.%20We%20conduct%20a%20systematic%20study%20to%20explore%20key%20questions%20around%20this%0Aapproach%2C%20including%20its%20effectiveness%20compared%20to%20vanilla%20translation%2C%20the%0Aimportance%20of%20filtering%20noisy%20outputs%2C%20and%20the%20benefits%20of%20mixing%20translated%0Asamples%20with%20original%20English%20data%20during%20alignment.%20Our%20experiments%20focus%20on%0Athe%20low-resource%20Indic%20language%20Hindi%20and%20compare%20translations%20generated%20by%0AGoogle%20Cloud%20Translation%20%28GCP%29%20and%20Llama-3.1-405B.%20The%20results%20highlight%20the%0Apromise%20of%20selective%20translation%20as%20a%20practical%20and%20effective%20method%20for%0Aimproving%20multilingual%20alignment%20in%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14304v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAligning%2520Large%2520Language%2520Models%2520to%2520Low-Resource%2520Languages%2520through%250A%2520%2520LLM-Based%2520Selective%2520Translation%253A%2520A%2520Systematic%2520Study%26entry.906535625%3DRakesh%2520Paul%2520and%2520Anusha%2520Kamath%2520and%2520Kanishk%2520Singla%2520and%2520Raviraj%2520Joshi%2520and%2520Utkarsh%2520Vaidya%2520and%2520Sanjay%2520Singh%2520Chauhan%2520and%2520Niranjan%2520Wartikar%26entry.1292438233%3D%2520%2520Multilingual%2520large%2520language%2520models%2520%2528LLMs%2529%2520often%2520demonstrate%2520a%2520performance%2520gap%250Abetween%2520English%2520and%2520non-English%2520languages%252C%2520particularly%2520in%2520low-resource%250Asettings.%2520Aligning%2520these%2520models%2520to%2520low-resource%2520languages%2520is%2520essential%2520yet%250Achallenging%2520due%2520to%2520limited%2520high-quality%2520data.%2520While%2520English%2520alignment%2520datasets%250Aare%2520readily%2520available%252C%2520curating%2520equivalent%2520data%2520in%2520other%2520languages%2520is%2520expensive%250Aand%2520time-consuming.%2520A%2520common%2520workaround%2520is%2520to%2520translate%2520existing%2520English%250Aalignment%2520data%253B%2520however%252C%2520standard%2520translation%2520techniques%2520often%2520fail%2520to%2520preserve%250Acritical%2520elements%2520such%2520as%2520code%252C%2520mathematical%2520expressions%252C%2520and%2520structured%250Aformats%2520like%2520JSON.%2520In%2520this%2520work%252C%2520we%2520investigate%2520LLM-based%2520selective%250Atranslation%252C%2520a%2520technique%2520that%2520selectively%2520translates%2520only%2520the%2520translatable%250Aparts%2520of%2520a%2520text%2520while%2520preserving%2520non-translatable%2520content%2520and%2520sentence%250Astructure.%2520We%2520conduct%2520a%2520systematic%2520study%2520to%2520explore%2520key%2520questions%2520around%2520this%250Aapproach%252C%2520including%2520its%2520effectiveness%2520compared%2520to%2520vanilla%2520translation%252C%2520the%250Aimportance%2520of%2520filtering%2520noisy%2520outputs%252C%2520and%2520the%2520benefits%2520of%2520mixing%2520translated%250Asamples%2520with%2520original%2520English%2520data%2520during%2520alignment.%2520Our%2520experiments%2520focus%2520on%250Athe%2520low-resource%2520Indic%2520language%2520Hindi%2520and%2520compare%2520translations%2520generated%2520by%250AGoogle%2520Cloud%2520Translation%2520%2528GCP%2529%2520and%2520Llama-3.1-405B.%2520The%2520results%2520highlight%2520the%250Apromise%2520of%2520selective%2520translation%2520as%2520a%2520practical%2520and%2520effective%2520method%2520for%250Aimproving%2520multilingual%2520alignment%2520in%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14304v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aligning%20Large%20Language%20Models%20to%20Low-Resource%20Languages%20through%0A%20%20LLM-Based%20Selective%20Translation%3A%20A%20Systematic%20Study&entry.906535625=Rakesh%20Paul%20and%20Anusha%20Kamath%20and%20Kanishk%20Singla%20and%20Raviraj%20Joshi%20and%20Utkarsh%20Vaidya%20and%20Sanjay%20Singh%20Chauhan%20and%20Niranjan%20Wartikar&entry.1292438233=%20%20Multilingual%20large%20language%20models%20%28LLMs%29%20often%20demonstrate%20a%20performance%20gap%0Abetween%20English%20and%20non-English%20languages%2C%20particularly%20in%20low-resource%0Asettings.%20Aligning%20these%20models%20to%20low-resource%20languages%20is%20essential%20yet%0Achallenging%20due%20to%20limited%20high-quality%20data.%20While%20English%20alignment%20datasets%0Aare%20readily%20available%2C%20curating%20equivalent%20data%20in%20other%20languages%20is%20expensive%0Aand%20time-consuming.%20A%20common%20workaround%20is%20to%20translate%20existing%20English%0Aalignment%20data%3B%20however%2C%20standard%20translation%20techniques%20often%20fail%20to%20preserve%0Acritical%20elements%20such%20as%20code%2C%20mathematical%20expressions%2C%20and%20structured%0Aformats%20like%20JSON.%20In%20this%20work%2C%20we%20investigate%20LLM-based%20selective%0Atranslation%2C%20a%20technique%20that%20selectively%20translates%20only%20the%20translatable%0Aparts%20of%20a%20text%20while%20preserving%20non-translatable%20content%20and%20sentence%0Astructure.%20We%20conduct%20a%20systematic%20study%20to%20explore%20key%20questions%20around%20this%0Aapproach%2C%20including%20its%20effectiveness%20compared%20to%20vanilla%20translation%2C%20the%0Aimportance%20of%20filtering%20noisy%20outputs%2C%20and%20the%20benefits%20of%20mixing%20translated%0Asamples%20with%20original%20English%20data%20during%20alignment.%20Our%20experiments%20focus%20on%0Athe%20low-resource%20Indic%20language%20Hindi%20and%20compare%20translations%20generated%20by%0AGoogle%20Cloud%20Translation%20%28GCP%29%20and%20Llama-3.1-405B.%20The%20results%20highlight%20the%0Apromise%20of%20selective%20translation%20as%20a%20practical%20and%20effective%20method%20for%0Aimproving%20multilingual%20alignment%20in%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14304v2&entry.124074799=Read"},
{"title": "Circle of Willis Centerline Graphs: A Dataset and Baseline Algorithm", "author": "Fabio Musio and Norman Juchler and Kaiyuan Yang and Suprosanna Shit and Chinmay Prabhakar and Bjoern Menze and Sven Hirsch", "abstract": "  The Circle of Willis (CoW) is a critical network of arteries in the brain,\noften implicated in cerebrovascular pathologies. Voxel-level segmentation is an\nimportant first step toward an automated CoW assessment, but a full\nquantitative analysis requires centerline representations. However,\nconventional skeletonization techniques often struggle to extract reliable\ncenterlines due to the CoW's complex geometry, and publicly available\ncenterline datasets remain scarce. To address these challenges, we used a\nthinning-based skeletonization algorithm to extract and curate centerline\ngraphs and morphometric features from the TopCoW dataset, which includes 200\nstroke patients, each imaged with MRA and CTA. The curated graphs were used to\ndevelop a baseline algorithm for centerline and feature extraction, combining\nU-Net-based skeletonization with A* graph connection. Performance was evaluated\non a held-out test set, focusing on anatomical accuracy and feature robustness.\nFurther, we used the extracted features to predict the frequency of fetal PCA\nvariants, confirm theoretical bifurcation optimality relations, and detect\nsubtle modality differences. The baseline algorithm consistently reconstructed\ngraph topology with high accuracy (F1 = 1), and the average Euclidean node\ndistance between reference and predicted graphs was below one voxel. Features\nsuch as segment radius, length, and bifurcation ratios showed strong\nrobustness, with median relative errors below 5% and Pearson correlations above\n0.95. Our results demonstrate the utility of learning-based skeletonization\ncombined with graph connection for anatomically plausible centerline\nextraction. We emphasize the importance of going beyond simple voxel-based\nmeasures by evaluating anatomical accuracy and feature robustness. The dataset\nand baseline algorithm have been released to support further method development\nand clinical research.\n", "link": "http://arxiv.org/abs/2510.13720v1", "date": "2025-10-15", "relevancy": 2.2518, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4546}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4484}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Circle%20of%20Willis%20Centerline%20Graphs%3A%20A%20Dataset%20and%20Baseline%20Algorithm&body=Title%3A%20Circle%20of%20Willis%20Centerline%20Graphs%3A%20A%20Dataset%20and%20Baseline%20Algorithm%0AAuthor%3A%20Fabio%20Musio%20and%20Norman%20Juchler%20and%20Kaiyuan%20Yang%20and%20Suprosanna%20Shit%20and%20Chinmay%20Prabhakar%20and%20Bjoern%20Menze%20and%20Sven%20Hirsch%0AAbstract%3A%20%20%20The%20Circle%20of%20Willis%20%28CoW%29%20is%20a%20critical%20network%20of%20arteries%20in%20the%20brain%2C%0Aoften%20implicated%20in%20cerebrovascular%20pathologies.%20Voxel-level%20segmentation%20is%20an%0Aimportant%20first%20step%20toward%20an%20automated%20CoW%20assessment%2C%20but%20a%20full%0Aquantitative%20analysis%20requires%20centerline%20representations.%20However%2C%0Aconventional%20skeletonization%20techniques%20often%20struggle%20to%20extract%20reliable%0Acenterlines%20due%20to%20the%20CoW%27s%20complex%20geometry%2C%20and%20publicly%20available%0Acenterline%20datasets%20remain%20scarce.%20To%20address%20these%20challenges%2C%20we%20used%20a%0Athinning-based%20skeletonization%20algorithm%20to%20extract%20and%20curate%20centerline%0Agraphs%20and%20morphometric%20features%20from%20the%20TopCoW%20dataset%2C%20which%20includes%20200%0Astroke%20patients%2C%20each%20imaged%20with%20MRA%20and%20CTA.%20The%20curated%20graphs%20were%20used%20to%0Adevelop%20a%20baseline%20algorithm%20for%20centerline%20and%20feature%20extraction%2C%20combining%0AU-Net-based%20skeletonization%20with%20A%2A%20graph%20connection.%20Performance%20was%20evaluated%0Aon%20a%20held-out%20test%20set%2C%20focusing%20on%20anatomical%20accuracy%20and%20feature%20robustness.%0AFurther%2C%20we%20used%20the%20extracted%20features%20to%20predict%20the%20frequency%20of%20fetal%20PCA%0Avariants%2C%20confirm%20theoretical%20bifurcation%20optimality%20relations%2C%20and%20detect%0Asubtle%20modality%20differences.%20The%20baseline%20algorithm%20consistently%20reconstructed%0Agraph%20topology%20with%20high%20accuracy%20%28F1%20%3D%201%29%2C%20and%20the%20average%20Euclidean%20node%0Adistance%20between%20reference%20and%20predicted%20graphs%20was%20below%20one%20voxel.%20Features%0Asuch%20as%20segment%20radius%2C%20length%2C%20and%20bifurcation%20ratios%20showed%20strong%0Arobustness%2C%20with%20median%20relative%20errors%20below%205%25%20and%20Pearson%20correlations%20above%0A0.95.%20Our%20results%20demonstrate%20the%20utility%20of%20learning-based%20skeletonization%0Acombined%20with%20graph%20connection%20for%20anatomically%20plausible%20centerline%0Aextraction.%20We%20emphasize%20the%20importance%20of%20going%20beyond%20simple%20voxel-based%0Ameasures%20by%20evaluating%20anatomical%20accuracy%20and%20feature%20robustness.%20The%20dataset%0Aand%20baseline%20algorithm%20have%20been%20released%20to%20support%20further%20method%20development%0Aand%20clinical%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13720v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCircle%2520of%2520Willis%2520Centerline%2520Graphs%253A%2520A%2520Dataset%2520and%2520Baseline%2520Algorithm%26entry.906535625%3DFabio%2520Musio%2520and%2520Norman%2520Juchler%2520and%2520Kaiyuan%2520Yang%2520and%2520Suprosanna%2520Shit%2520and%2520Chinmay%2520Prabhakar%2520and%2520Bjoern%2520Menze%2520and%2520Sven%2520Hirsch%26entry.1292438233%3D%2520%2520The%2520Circle%2520of%2520Willis%2520%2528CoW%2529%2520is%2520a%2520critical%2520network%2520of%2520arteries%2520in%2520the%2520brain%252C%250Aoften%2520implicated%2520in%2520cerebrovascular%2520pathologies.%2520Voxel-level%2520segmentation%2520is%2520an%250Aimportant%2520first%2520step%2520toward%2520an%2520automated%2520CoW%2520assessment%252C%2520but%2520a%2520full%250Aquantitative%2520analysis%2520requires%2520centerline%2520representations.%2520However%252C%250Aconventional%2520skeletonization%2520techniques%2520often%2520struggle%2520to%2520extract%2520reliable%250Acenterlines%2520due%2520to%2520the%2520CoW%2527s%2520complex%2520geometry%252C%2520and%2520publicly%2520available%250Acenterline%2520datasets%2520remain%2520scarce.%2520To%2520address%2520these%2520challenges%252C%2520we%2520used%2520a%250Athinning-based%2520skeletonization%2520algorithm%2520to%2520extract%2520and%2520curate%2520centerline%250Agraphs%2520and%2520morphometric%2520features%2520from%2520the%2520TopCoW%2520dataset%252C%2520which%2520includes%2520200%250Astroke%2520patients%252C%2520each%2520imaged%2520with%2520MRA%2520and%2520CTA.%2520The%2520curated%2520graphs%2520were%2520used%2520to%250Adevelop%2520a%2520baseline%2520algorithm%2520for%2520centerline%2520and%2520feature%2520extraction%252C%2520combining%250AU-Net-based%2520skeletonization%2520with%2520A%252A%2520graph%2520connection.%2520Performance%2520was%2520evaluated%250Aon%2520a%2520held-out%2520test%2520set%252C%2520focusing%2520on%2520anatomical%2520accuracy%2520and%2520feature%2520robustness.%250AFurther%252C%2520we%2520used%2520the%2520extracted%2520features%2520to%2520predict%2520the%2520frequency%2520of%2520fetal%2520PCA%250Avariants%252C%2520confirm%2520theoretical%2520bifurcation%2520optimality%2520relations%252C%2520and%2520detect%250Asubtle%2520modality%2520differences.%2520The%2520baseline%2520algorithm%2520consistently%2520reconstructed%250Agraph%2520topology%2520with%2520high%2520accuracy%2520%2528F1%2520%253D%25201%2529%252C%2520and%2520the%2520average%2520Euclidean%2520node%250Adistance%2520between%2520reference%2520and%2520predicted%2520graphs%2520was%2520below%2520one%2520voxel.%2520Features%250Asuch%2520as%2520segment%2520radius%252C%2520length%252C%2520and%2520bifurcation%2520ratios%2520showed%2520strong%250Arobustness%252C%2520with%2520median%2520relative%2520errors%2520below%25205%2525%2520and%2520Pearson%2520correlations%2520above%250A0.95.%2520Our%2520results%2520demonstrate%2520the%2520utility%2520of%2520learning-based%2520skeletonization%250Acombined%2520with%2520graph%2520connection%2520for%2520anatomically%2520plausible%2520centerline%250Aextraction.%2520We%2520emphasize%2520the%2520importance%2520of%2520going%2520beyond%2520simple%2520voxel-based%250Ameasures%2520by%2520evaluating%2520anatomical%2520accuracy%2520and%2520feature%2520robustness.%2520The%2520dataset%250Aand%2520baseline%2520algorithm%2520have%2520been%2520released%2520to%2520support%2520further%2520method%2520development%250Aand%2520clinical%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13720v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Circle%20of%20Willis%20Centerline%20Graphs%3A%20A%20Dataset%20and%20Baseline%20Algorithm&entry.906535625=Fabio%20Musio%20and%20Norman%20Juchler%20and%20Kaiyuan%20Yang%20and%20Suprosanna%20Shit%20and%20Chinmay%20Prabhakar%20and%20Bjoern%20Menze%20and%20Sven%20Hirsch&entry.1292438233=%20%20The%20Circle%20of%20Willis%20%28CoW%29%20is%20a%20critical%20network%20of%20arteries%20in%20the%20brain%2C%0Aoften%20implicated%20in%20cerebrovascular%20pathologies.%20Voxel-level%20segmentation%20is%20an%0Aimportant%20first%20step%20toward%20an%20automated%20CoW%20assessment%2C%20but%20a%20full%0Aquantitative%20analysis%20requires%20centerline%20representations.%20However%2C%0Aconventional%20skeletonization%20techniques%20often%20struggle%20to%20extract%20reliable%0Acenterlines%20due%20to%20the%20CoW%27s%20complex%20geometry%2C%20and%20publicly%20available%0Acenterline%20datasets%20remain%20scarce.%20To%20address%20these%20challenges%2C%20we%20used%20a%0Athinning-based%20skeletonization%20algorithm%20to%20extract%20and%20curate%20centerline%0Agraphs%20and%20morphometric%20features%20from%20the%20TopCoW%20dataset%2C%20which%20includes%20200%0Astroke%20patients%2C%20each%20imaged%20with%20MRA%20and%20CTA.%20The%20curated%20graphs%20were%20used%20to%0Adevelop%20a%20baseline%20algorithm%20for%20centerline%20and%20feature%20extraction%2C%20combining%0AU-Net-based%20skeletonization%20with%20A%2A%20graph%20connection.%20Performance%20was%20evaluated%0Aon%20a%20held-out%20test%20set%2C%20focusing%20on%20anatomical%20accuracy%20and%20feature%20robustness.%0AFurther%2C%20we%20used%20the%20extracted%20features%20to%20predict%20the%20frequency%20of%20fetal%20PCA%0Avariants%2C%20confirm%20theoretical%20bifurcation%20optimality%20relations%2C%20and%20detect%0Asubtle%20modality%20differences.%20The%20baseline%20algorithm%20consistently%20reconstructed%0Agraph%20topology%20with%20high%20accuracy%20%28F1%20%3D%201%29%2C%20and%20the%20average%20Euclidean%20node%0Adistance%20between%20reference%20and%20predicted%20graphs%20was%20below%20one%20voxel.%20Features%0Asuch%20as%20segment%20radius%2C%20length%2C%20and%20bifurcation%20ratios%20showed%20strong%0Arobustness%2C%20with%20median%20relative%20errors%20below%205%25%20and%20Pearson%20correlations%20above%0A0.95.%20Our%20results%20demonstrate%20the%20utility%20of%20learning-based%20skeletonization%0Acombined%20with%20graph%20connection%20for%20anatomically%20plausible%20centerline%0Aextraction.%20We%20emphasize%20the%20importance%20of%20going%20beyond%20simple%20voxel-based%0Ameasures%20by%20evaluating%20anatomical%20accuracy%20and%20feature%20robustness.%20The%20dataset%0Aand%20baseline%20algorithm%20have%20been%20released%20to%20support%20further%20method%20development%0Aand%20clinical%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13720v1&entry.124074799=Read"},
{"title": "NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete\n  Flow Matching", "author": "Run Luo and Xiaobo Xia and Lu Wang and Longze Chen and Renke Shan and Jing Luo and Min Yang and Tat-Seng Chua", "abstract": "  Next-generation multimodal foundation models capable of any-to-any\ncross-modal generation and multi-turn interaction will serve as core components\nof artificial general intelligence systems, playing a pivotal role in\nhuman-machine interaction. However, most existing multimodal models remain\nconstrained by autoregressive architectures, whose inherent limitations prevent\na balanced integration of understanding and generation capabilities. Although\nhybrid and decoupling strategies have been explored to address these tasks\nwithin unified frameworks separately, their redundant, non-integrated designs\nlimit their applicability to broader scenarios, such as cross-modal\nretrieval.In this work, we introduce NExT-OMNI, an open-source omnimodal\nfoundation model that achieves unified modeling through discrete flow\nparadigms. By leveraging metric-induced probability paths and kinetic optimal\nvelocities, NExT-OMNI natively supports any-to-any understanding and generation\nwith enhanced response efficiency, while enabling broader application scenarios\nthrough concise unified representations rather than task-decoupled designs.\nTrained on large-scale interleaved text, image, video, and audio data,\nNExT-OMNI delivers competitive performance on multimodal generation and\nunderstanding benchmarks, while outperforming prior unified models in\nmulti-turn multimodal interaction and cross-modal retrieval, highlighting its\narchitectural advantages as a next-generation multimodal foundation model. To\nadvance further research, we release training details, data protocols, and\nopen-source both the code and model checkpoints.\n", "link": "http://arxiv.org/abs/2510.13721v1", "date": "2025-10-15", "relevancy": 2.2489, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5651}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5616}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NExT-OMNI%3A%20Towards%20Any-to-Any%20Omnimodal%20Foundation%20Models%20with%20Discrete%0A%20%20Flow%20Matching&body=Title%3A%20NExT-OMNI%3A%20Towards%20Any-to-Any%20Omnimodal%20Foundation%20Models%20with%20Discrete%0A%20%20Flow%20Matching%0AAuthor%3A%20Run%20Luo%20and%20Xiaobo%20Xia%20and%20Lu%20Wang%20and%20Longze%20Chen%20and%20Renke%20Shan%20and%20Jing%20Luo%20and%20Min%20Yang%20and%20Tat-Seng%20Chua%0AAbstract%3A%20%20%20Next-generation%20multimodal%20foundation%20models%20capable%20of%20any-to-any%0Across-modal%20generation%20and%20multi-turn%20interaction%20will%20serve%20as%20core%20components%0Aof%20artificial%20general%20intelligence%20systems%2C%20playing%20a%20pivotal%20role%20in%0Ahuman-machine%20interaction.%20However%2C%20most%20existing%20multimodal%20models%20remain%0Aconstrained%20by%20autoregressive%20architectures%2C%20whose%20inherent%20limitations%20prevent%0Aa%20balanced%20integration%20of%20understanding%20and%20generation%20capabilities.%20Although%0Ahybrid%20and%20decoupling%20strategies%20have%20been%20explored%20to%20address%20these%20tasks%0Awithin%20unified%20frameworks%20separately%2C%20their%20redundant%2C%20non-integrated%20designs%0Alimit%20their%20applicability%20to%20broader%20scenarios%2C%20such%20as%20cross-modal%0Aretrieval.In%20this%20work%2C%20we%20introduce%20NExT-OMNI%2C%20an%20open-source%20omnimodal%0Afoundation%20model%20that%20achieves%20unified%20modeling%20through%20discrete%20flow%0Aparadigms.%20By%20leveraging%20metric-induced%20probability%20paths%20and%20kinetic%20optimal%0Avelocities%2C%20NExT-OMNI%20natively%20supports%20any-to-any%20understanding%20and%20generation%0Awith%20enhanced%20response%20efficiency%2C%20while%20enabling%20broader%20application%20scenarios%0Athrough%20concise%20unified%20representations%20rather%20than%20task-decoupled%20designs.%0ATrained%20on%20large-scale%20interleaved%20text%2C%20image%2C%20video%2C%20and%20audio%20data%2C%0ANExT-OMNI%20delivers%20competitive%20performance%20on%20multimodal%20generation%20and%0Aunderstanding%20benchmarks%2C%20while%20outperforming%20prior%20unified%20models%20in%0Amulti-turn%20multimodal%20interaction%20and%20cross-modal%20retrieval%2C%20highlighting%20its%0Aarchitectural%20advantages%20as%20a%20next-generation%20multimodal%20foundation%20model.%20To%0Aadvance%20further%20research%2C%20we%20release%20training%20details%2C%20data%20protocols%2C%20and%0Aopen-source%20both%20the%20code%20and%20model%20checkpoints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13721v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNExT-OMNI%253A%2520Towards%2520Any-to-Any%2520Omnimodal%2520Foundation%2520Models%2520with%2520Discrete%250A%2520%2520Flow%2520Matching%26entry.906535625%3DRun%2520Luo%2520and%2520Xiaobo%2520Xia%2520and%2520Lu%2520Wang%2520and%2520Longze%2520Chen%2520and%2520Renke%2520Shan%2520and%2520Jing%2520Luo%2520and%2520Min%2520Yang%2520and%2520Tat-Seng%2520Chua%26entry.1292438233%3D%2520%2520Next-generation%2520multimodal%2520foundation%2520models%2520capable%2520of%2520any-to-any%250Across-modal%2520generation%2520and%2520multi-turn%2520interaction%2520will%2520serve%2520as%2520core%2520components%250Aof%2520artificial%2520general%2520intelligence%2520systems%252C%2520playing%2520a%2520pivotal%2520role%2520in%250Ahuman-machine%2520interaction.%2520However%252C%2520most%2520existing%2520multimodal%2520models%2520remain%250Aconstrained%2520by%2520autoregressive%2520architectures%252C%2520whose%2520inherent%2520limitations%2520prevent%250Aa%2520balanced%2520integration%2520of%2520understanding%2520and%2520generation%2520capabilities.%2520Although%250Ahybrid%2520and%2520decoupling%2520strategies%2520have%2520been%2520explored%2520to%2520address%2520these%2520tasks%250Awithin%2520unified%2520frameworks%2520separately%252C%2520their%2520redundant%252C%2520non-integrated%2520designs%250Alimit%2520their%2520applicability%2520to%2520broader%2520scenarios%252C%2520such%2520as%2520cross-modal%250Aretrieval.In%2520this%2520work%252C%2520we%2520introduce%2520NExT-OMNI%252C%2520an%2520open-source%2520omnimodal%250Afoundation%2520model%2520that%2520achieves%2520unified%2520modeling%2520through%2520discrete%2520flow%250Aparadigms.%2520By%2520leveraging%2520metric-induced%2520probability%2520paths%2520and%2520kinetic%2520optimal%250Avelocities%252C%2520NExT-OMNI%2520natively%2520supports%2520any-to-any%2520understanding%2520and%2520generation%250Awith%2520enhanced%2520response%2520efficiency%252C%2520while%2520enabling%2520broader%2520application%2520scenarios%250Athrough%2520concise%2520unified%2520representations%2520rather%2520than%2520task-decoupled%2520designs.%250ATrained%2520on%2520large-scale%2520interleaved%2520text%252C%2520image%252C%2520video%252C%2520and%2520audio%2520data%252C%250ANExT-OMNI%2520delivers%2520competitive%2520performance%2520on%2520multimodal%2520generation%2520and%250Aunderstanding%2520benchmarks%252C%2520while%2520outperforming%2520prior%2520unified%2520models%2520in%250Amulti-turn%2520multimodal%2520interaction%2520and%2520cross-modal%2520retrieval%252C%2520highlighting%2520its%250Aarchitectural%2520advantages%2520as%2520a%2520next-generation%2520multimodal%2520foundation%2520model.%2520To%250Aadvance%2520further%2520research%252C%2520we%2520release%2520training%2520details%252C%2520data%2520protocols%252C%2520and%250Aopen-source%2520both%2520the%2520code%2520and%2520model%2520checkpoints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13721v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NExT-OMNI%3A%20Towards%20Any-to-Any%20Omnimodal%20Foundation%20Models%20with%20Discrete%0A%20%20Flow%20Matching&entry.906535625=Run%20Luo%20and%20Xiaobo%20Xia%20and%20Lu%20Wang%20and%20Longze%20Chen%20and%20Renke%20Shan%20and%20Jing%20Luo%20and%20Min%20Yang%20and%20Tat-Seng%20Chua&entry.1292438233=%20%20Next-generation%20multimodal%20foundation%20models%20capable%20of%20any-to-any%0Across-modal%20generation%20and%20multi-turn%20interaction%20will%20serve%20as%20core%20components%0Aof%20artificial%20general%20intelligence%20systems%2C%20playing%20a%20pivotal%20role%20in%0Ahuman-machine%20interaction.%20However%2C%20most%20existing%20multimodal%20models%20remain%0Aconstrained%20by%20autoregressive%20architectures%2C%20whose%20inherent%20limitations%20prevent%0Aa%20balanced%20integration%20of%20understanding%20and%20generation%20capabilities.%20Although%0Ahybrid%20and%20decoupling%20strategies%20have%20been%20explored%20to%20address%20these%20tasks%0Awithin%20unified%20frameworks%20separately%2C%20their%20redundant%2C%20non-integrated%20designs%0Alimit%20their%20applicability%20to%20broader%20scenarios%2C%20such%20as%20cross-modal%0Aretrieval.In%20this%20work%2C%20we%20introduce%20NExT-OMNI%2C%20an%20open-source%20omnimodal%0Afoundation%20model%20that%20achieves%20unified%20modeling%20through%20discrete%20flow%0Aparadigms.%20By%20leveraging%20metric-induced%20probability%20paths%20and%20kinetic%20optimal%0Avelocities%2C%20NExT-OMNI%20natively%20supports%20any-to-any%20understanding%20and%20generation%0Awith%20enhanced%20response%20efficiency%2C%20while%20enabling%20broader%20application%20scenarios%0Athrough%20concise%20unified%20representations%20rather%20than%20task-decoupled%20designs.%0ATrained%20on%20large-scale%20interleaved%20text%2C%20image%2C%20video%2C%20and%20audio%20data%2C%0ANExT-OMNI%20delivers%20competitive%20performance%20on%20multimodal%20generation%20and%0Aunderstanding%20benchmarks%2C%20while%20outperforming%20prior%20unified%20models%20in%0Amulti-turn%20multimodal%20interaction%20and%20cross-modal%20retrieval%2C%20highlighting%20its%0Aarchitectural%20advantages%20as%20a%20next-generation%20multimodal%20foundation%20model.%20To%0Aadvance%20further%20research%2C%20we%20release%20training%20details%2C%20data%20protocols%2C%20and%0Aopen-source%20both%20the%20code%20and%20model%20checkpoints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13721v1&entry.124074799=Read"},
{"title": "Steerable Conditional Diffusion for Domain Adaptation in PET Image\n  Reconstruction", "author": "George Webber and Alexander Hammers and Andrew P. King and Andrew J. Reader", "abstract": "  Diffusion models have recently enabled state-of-the-art reconstruction of\npositron emission tomography (PET) images while requiring only image training\ndata. However, domain shift remains a key concern for clinical adoption: priors\ntrained on images from one anatomy, acquisition protocol or pathology may\nproduce artefacts on out-of-distribution data. We propose integrating steerable\nconditional diffusion (SCD) with our previously-introduced likelihood-scheduled\ndiffusion (PET-LiSch) framework to improve the alignment of the diffusion\nmodel's prior to the target subject. At reconstruction time, for each diffusion\nstep, we use low-rank adaptation (LoRA) to align the diffusion model prior with\nthe target domain on the fly. Experiments on realistic synthetic 2D brain\nphantoms demonstrate that our approach suppresses hallucinated artefacts under\ndomain shift, i.e. when our diffusion model is trained on perturbed images and\ntested on normal anatomy, our approach suppresses the hallucinated structure,\noutperforming both OSEM and diffusion model baselines qualitatively and\nquantitatively. These results provide a proof-of-concept that steerable priors\ncan mitigate domain shift in diffusion-based PET reconstruction and motivate\nfuture evaluation on real data.\n", "link": "http://arxiv.org/abs/2510.13441v1", "date": "2025-10-15", "relevancy": 2.2484, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6041}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5537}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Steerable%20Conditional%20Diffusion%20for%20Domain%20Adaptation%20in%20PET%20Image%0A%20%20Reconstruction&body=Title%3A%20Steerable%20Conditional%20Diffusion%20for%20Domain%20Adaptation%20in%20PET%20Image%0A%20%20Reconstruction%0AAuthor%3A%20George%20Webber%20and%20Alexander%20Hammers%20and%20Andrew%20P.%20King%20and%20Andrew%20J.%20Reader%0AAbstract%3A%20%20%20Diffusion%20models%20have%20recently%20enabled%20state-of-the-art%20reconstruction%20of%0Apositron%20emission%20tomography%20%28PET%29%20images%20while%20requiring%20only%20image%20training%0Adata.%20However%2C%20domain%20shift%20remains%20a%20key%20concern%20for%20clinical%20adoption%3A%20priors%0Atrained%20on%20images%20from%20one%20anatomy%2C%20acquisition%20protocol%20or%20pathology%20may%0Aproduce%20artefacts%20on%20out-of-distribution%20data.%20We%20propose%20integrating%20steerable%0Aconditional%20diffusion%20%28SCD%29%20with%20our%20previously-introduced%20likelihood-scheduled%0Adiffusion%20%28PET-LiSch%29%20framework%20to%20improve%20the%20alignment%20of%20the%20diffusion%0Amodel%27s%20prior%20to%20the%20target%20subject.%20At%20reconstruction%20time%2C%20for%20each%20diffusion%0Astep%2C%20we%20use%20low-rank%20adaptation%20%28LoRA%29%20to%20align%20the%20diffusion%20model%20prior%20with%0Athe%20target%20domain%20on%20the%20fly.%20Experiments%20on%20realistic%20synthetic%202D%20brain%0Aphantoms%20demonstrate%20that%20our%20approach%20suppresses%20hallucinated%20artefacts%20under%0Adomain%20shift%2C%20i.e.%20when%20our%20diffusion%20model%20is%20trained%20on%20perturbed%20images%20and%0Atested%20on%20normal%20anatomy%2C%20our%20approach%20suppresses%20the%20hallucinated%20structure%2C%0Aoutperforming%20both%20OSEM%20and%20diffusion%20model%20baselines%20qualitatively%20and%0Aquantitatively.%20These%20results%20provide%20a%20proof-of-concept%20that%20steerable%20priors%0Acan%20mitigate%20domain%20shift%20in%20diffusion-based%20PET%20reconstruction%20and%20motivate%0Afuture%20evaluation%20on%20real%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13441v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSteerable%2520Conditional%2520Diffusion%2520for%2520Domain%2520Adaptation%2520in%2520PET%2520Image%250A%2520%2520Reconstruction%26entry.906535625%3DGeorge%2520Webber%2520and%2520Alexander%2520Hammers%2520and%2520Andrew%2520P.%2520King%2520and%2520Andrew%2520J.%2520Reader%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520recently%2520enabled%2520state-of-the-art%2520reconstruction%2520of%250Apositron%2520emission%2520tomography%2520%2528PET%2529%2520images%2520while%2520requiring%2520only%2520image%2520training%250Adata.%2520However%252C%2520domain%2520shift%2520remains%2520a%2520key%2520concern%2520for%2520clinical%2520adoption%253A%2520priors%250Atrained%2520on%2520images%2520from%2520one%2520anatomy%252C%2520acquisition%2520protocol%2520or%2520pathology%2520may%250Aproduce%2520artefacts%2520on%2520out-of-distribution%2520data.%2520We%2520propose%2520integrating%2520steerable%250Aconditional%2520diffusion%2520%2528SCD%2529%2520with%2520our%2520previously-introduced%2520likelihood-scheduled%250Adiffusion%2520%2528PET-LiSch%2529%2520framework%2520to%2520improve%2520the%2520alignment%2520of%2520the%2520diffusion%250Amodel%2527s%2520prior%2520to%2520the%2520target%2520subject.%2520At%2520reconstruction%2520time%252C%2520for%2520each%2520diffusion%250Astep%252C%2520we%2520use%2520low-rank%2520adaptation%2520%2528LoRA%2529%2520to%2520align%2520the%2520diffusion%2520model%2520prior%2520with%250Athe%2520target%2520domain%2520on%2520the%2520fly.%2520Experiments%2520on%2520realistic%2520synthetic%25202D%2520brain%250Aphantoms%2520demonstrate%2520that%2520our%2520approach%2520suppresses%2520hallucinated%2520artefacts%2520under%250Adomain%2520shift%252C%2520i.e.%2520when%2520our%2520diffusion%2520model%2520is%2520trained%2520on%2520perturbed%2520images%2520and%250Atested%2520on%2520normal%2520anatomy%252C%2520our%2520approach%2520suppresses%2520the%2520hallucinated%2520structure%252C%250Aoutperforming%2520both%2520OSEM%2520and%2520diffusion%2520model%2520baselines%2520qualitatively%2520and%250Aquantitatively.%2520These%2520results%2520provide%2520a%2520proof-of-concept%2520that%2520steerable%2520priors%250Acan%2520mitigate%2520domain%2520shift%2520in%2520diffusion-based%2520PET%2520reconstruction%2520and%2520motivate%250Afuture%2520evaluation%2520on%2520real%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13441v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Steerable%20Conditional%20Diffusion%20for%20Domain%20Adaptation%20in%20PET%20Image%0A%20%20Reconstruction&entry.906535625=George%20Webber%20and%20Alexander%20Hammers%20and%20Andrew%20P.%20King%20and%20Andrew%20J.%20Reader&entry.1292438233=%20%20Diffusion%20models%20have%20recently%20enabled%20state-of-the-art%20reconstruction%20of%0Apositron%20emission%20tomography%20%28PET%29%20images%20while%20requiring%20only%20image%20training%0Adata.%20However%2C%20domain%20shift%20remains%20a%20key%20concern%20for%20clinical%20adoption%3A%20priors%0Atrained%20on%20images%20from%20one%20anatomy%2C%20acquisition%20protocol%20or%20pathology%20may%0Aproduce%20artefacts%20on%20out-of-distribution%20data.%20We%20propose%20integrating%20steerable%0Aconditional%20diffusion%20%28SCD%29%20with%20our%20previously-introduced%20likelihood-scheduled%0Adiffusion%20%28PET-LiSch%29%20framework%20to%20improve%20the%20alignment%20of%20the%20diffusion%0Amodel%27s%20prior%20to%20the%20target%20subject.%20At%20reconstruction%20time%2C%20for%20each%20diffusion%0Astep%2C%20we%20use%20low-rank%20adaptation%20%28LoRA%29%20to%20align%20the%20diffusion%20model%20prior%20with%0Athe%20target%20domain%20on%20the%20fly.%20Experiments%20on%20realistic%20synthetic%202D%20brain%0Aphantoms%20demonstrate%20that%20our%20approach%20suppresses%20hallucinated%20artefacts%20under%0Adomain%20shift%2C%20i.e.%20when%20our%20diffusion%20model%20is%20trained%20on%20perturbed%20images%20and%0Atested%20on%20normal%20anatomy%2C%20our%20approach%20suppresses%20the%20hallucinated%20structure%2C%0Aoutperforming%20both%20OSEM%20and%20diffusion%20model%20baselines%20qualitatively%20and%0Aquantitatively.%20These%20results%20provide%20a%20proof-of-concept%20that%20steerable%20priors%0Acan%20mitigate%20domain%20shift%20in%20diffusion-based%20PET%20reconstruction%20and%20motivate%0Afuture%20evaluation%20on%20real%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13441v1&entry.124074799=Read"},
{"title": "Emergent Semantics Beyond Token Embeddings: Transformer LMs with Frozen\n  Visual Unicode Representations", "author": "A. Bochkov", "abstract": "  Understanding the locus of semantic representation in large language models\n(LLMs) is crucial for interpretability and architectural innovation. The\ndominant paradigm posits that trainable input embeddings serve as foundational\n\"meaning vectors.\" This paper challenges that view. We construct Transformer\nmodels where the embedding layer is entirely frozen, with vectors derived not\nfrom data, but from the visual structure of Unicode glyphs. These non-semantic,\nprecomputed visual embeddings are fixed throughout training. Our method is\ncompatible with any tokenizer, including a novel Unicode-centric tokenizer we\nintroduce to ensure universal text coverage. Despite the absence of trainable,\nsemantically initialized embeddings, our models converge, generate coherent\ntext, and, critically, outperform architecturally identical models with\ntrainable embeddings on the MMLU reasoning benchmark. We attribute this to\n\"representational interference\" in conventional models, where the embedding\nlayer is burdened with learning both structural and semantic features. Our\nresults indicate that high-level semantics are not inherent to input embeddings\nbut are an emergent property of the Transformer's compositional architecture\nand data scale. This reframes the role of embeddings from meaning containers to\nstructural primitives. We release all code and models to foster further\nresearch.\n", "link": "http://arxiv.org/abs/2507.04886v4", "date": "2025-10-15", "relevancy": 2.2431, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5615}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5615}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Emergent%20Semantics%20Beyond%20Token%20Embeddings%3A%20Transformer%20LMs%20with%20Frozen%0A%20%20Visual%20Unicode%20Representations&body=Title%3A%20Emergent%20Semantics%20Beyond%20Token%20Embeddings%3A%20Transformer%20LMs%20with%20Frozen%0A%20%20Visual%20Unicode%20Representations%0AAuthor%3A%20A.%20Bochkov%0AAbstract%3A%20%20%20Understanding%20the%20locus%20of%20semantic%20representation%20in%20large%20language%20models%0A%28LLMs%29%20is%20crucial%20for%20interpretability%20and%20architectural%20innovation.%20The%0Adominant%20paradigm%20posits%20that%20trainable%20input%20embeddings%20serve%20as%20foundational%0A%22meaning%20vectors.%22%20This%20paper%20challenges%20that%20view.%20We%20construct%20Transformer%0Amodels%20where%20the%20embedding%20layer%20is%20entirely%20frozen%2C%20with%20vectors%20derived%20not%0Afrom%20data%2C%20but%20from%20the%20visual%20structure%20of%20Unicode%20glyphs.%20These%20non-semantic%2C%0Aprecomputed%20visual%20embeddings%20are%20fixed%20throughout%20training.%20Our%20method%20is%0Acompatible%20with%20any%20tokenizer%2C%20including%20a%20novel%20Unicode-centric%20tokenizer%20we%0Aintroduce%20to%20ensure%20universal%20text%20coverage.%20Despite%20the%20absence%20of%20trainable%2C%0Asemantically%20initialized%20embeddings%2C%20our%20models%20converge%2C%20generate%20coherent%0Atext%2C%20and%2C%20critically%2C%20outperform%20architecturally%20identical%20models%20with%0Atrainable%20embeddings%20on%20the%20MMLU%20reasoning%20benchmark.%20We%20attribute%20this%20to%0A%22representational%20interference%22%20in%20conventional%20models%2C%20where%20the%20embedding%0Alayer%20is%20burdened%20with%20learning%20both%20structural%20and%20semantic%20features.%20Our%0Aresults%20indicate%20that%20high-level%20semantics%20are%20not%20inherent%20to%20input%20embeddings%0Abut%20are%20an%20emergent%20property%20of%20the%20Transformer%27s%20compositional%20architecture%0Aand%20data%20scale.%20This%20reframes%20the%20role%20of%20embeddings%20from%20meaning%20containers%20to%0Astructural%20primitives.%20We%20release%20all%20code%20and%20models%20to%20foster%20further%0Aresearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04886v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmergent%2520Semantics%2520Beyond%2520Token%2520Embeddings%253A%2520Transformer%2520LMs%2520with%2520Frozen%250A%2520%2520Visual%2520Unicode%2520Representations%26entry.906535625%3DA.%2520Bochkov%26entry.1292438233%3D%2520%2520Understanding%2520the%2520locus%2520of%2520semantic%2520representation%2520in%2520large%2520language%2520models%250A%2528LLMs%2529%2520is%2520crucial%2520for%2520interpretability%2520and%2520architectural%2520innovation.%2520The%250Adominant%2520paradigm%2520posits%2520that%2520trainable%2520input%2520embeddings%2520serve%2520as%2520foundational%250A%2522meaning%2520vectors.%2522%2520This%2520paper%2520challenges%2520that%2520view.%2520We%2520construct%2520Transformer%250Amodels%2520where%2520the%2520embedding%2520layer%2520is%2520entirely%2520frozen%252C%2520with%2520vectors%2520derived%2520not%250Afrom%2520data%252C%2520but%2520from%2520the%2520visual%2520structure%2520of%2520Unicode%2520glyphs.%2520These%2520non-semantic%252C%250Aprecomputed%2520visual%2520embeddings%2520are%2520fixed%2520throughout%2520training.%2520Our%2520method%2520is%250Acompatible%2520with%2520any%2520tokenizer%252C%2520including%2520a%2520novel%2520Unicode-centric%2520tokenizer%2520we%250Aintroduce%2520to%2520ensure%2520universal%2520text%2520coverage.%2520Despite%2520the%2520absence%2520of%2520trainable%252C%250Asemantically%2520initialized%2520embeddings%252C%2520our%2520models%2520converge%252C%2520generate%2520coherent%250Atext%252C%2520and%252C%2520critically%252C%2520outperform%2520architecturally%2520identical%2520models%2520with%250Atrainable%2520embeddings%2520on%2520the%2520MMLU%2520reasoning%2520benchmark.%2520We%2520attribute%2520this%2520to%250A%2522representational%2520interference%2522%2520in%2520conventional%2520models%252C%2520where%2520the%2520embedding%250Alayer%2520is%2520burdened%2520with%2520learning%2520both%2520structural%2520and%2520semantic%2520features.%2520Our%250Aresults%2520indicate%2520that%2520high-level%2520semantics%2520are%2520not%2520inherent%2520to%2520input%2520embeddings%250Abut%2520are%2520an%2520emergent%2520property%2520of%2520the%2520Transformer%2527s%2520compositional%2520architecture%250Aand%2520data%2520scale.%2520This%2520reframes%2520the%2520role%2520of%2520embeddings%2520from%2520meaning%2520containers%2520to%250Astructural%2520primitives.%2520We%2520release%2520all%2520code%2520and%2520models%2520to%2520foster%2520further%250Aresearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04886v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emergent%20Semantics%20Beyond%20Token%20Embeddings%3A%20Transformer%20LMs%20with%20Frozen%0A%20%20Visual%20Unicode%20Representations&entry.906535625=A.%20Bochkov&entry.1292438233=%20%20Understanding%20the%20locus%20of%20semantic%20representation%20in%20large%20language%20models%0A%28LLMs%29%20is%20crucial%20for%20interpretability%20and%20architectural%20innovation.%20The%0Adominant%20paradigm%20posits%20that%20trainable%20input%20embeddings%20serve%20as%20foundational%0A%22meaning%20vectors.%22%20This%20paper%20challenges%20that%20view.%20We%20construct%20Transformer%0Amodels%20where%20the%20embedding%20layer%20is%20entirely%20frozen%2C%20with%20vectors%20derived%20not%0Afrom%20data%2C%20but%20from%20the%20visual%20structure%20of%20Unicode%20glyphs.%20These%20non-semantic%2C%0Aprecomputed%20visual%20embeddings%20are%20fixed%20throughout%20training.%20Our%20method%20is%0Acompatible%20with%20any%20tokenizer%2C%20including%20a%20novel%20Unicode-centric%20tokenizer%20we%0Aintroduce%20to%20ensure%20universal%20text%20coverage.%20Despite%20the%20absence%20of%20trainable%2C%0Asemantically%20initialized%20embeddings%2C%20our%20models%20converge%2C%20generate%20coherent%0Atext%2C%20and%2C%20critically%2C%20outperform%20architecturally%20identical%20models%20with%0Atrainable%20embeddings%20on%20the%20MMLU%20reasoning%20benchmark.%20We%20attribute%20this%20to%0A%22representational%20interference%22%20in%20conventional%20models%2C%20where%20the%20embedding%0Alayer%20is%20burdened%20with%20learning%20both%20structural%20and%20semantic%20features.%20Our%0Aresults%20indicate%20that%20high-level%20semantics%20are%20not%20inherent%20to%20input%20embeddings%0Abut%20are%20an%20emergent%20property%20of%20the%20Transformer%27s%20compositional%20architecture%0Aand%20data%20scale.%20This%20reframes%20the%20role%20of%20embeddings%20from%20meaning%20containers%20to%0Astructural%20primitives.%20We%20release%20all%20code%20and%20models%20to%20foster%20further%0Aresearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04886v4&entry.124074799=Read"},
{"title": "Time Series Foundation Models: Benchmarking Challenges and Requirements", "author": "Marcel Meyer and Sascha Kaltenpoth and Kevin Zalipski and Oliver M\u00fcller", "abstract": "  Time Series Foundation Models (TSFMs) represent a new paradigm for time\nseries forecasting, offering zero-shot forecasting capabilities without the\nneed for domain-specific pre-training or fine-tuning. However, as with Large\nLanguage Models (LLMs), evaluating TSFMs is tricky, as with ever more extensive\ntraining sets, it becomes more and more challenging to ensure the integrity of\nbenchmarking data. Our investigation of existing TSFM evaluation highlights\nmultiple challenges, ranging from the representativeness of the benchmark\ndatasets, over the lack of spatiotemporal evaluation, to risks of information\nleakage due to overlapping and obscure datasets, and the memorization of global\npatterns caused by external shocks like economic crises or pandemics. Our\nfindings reveal widespread confusion regarding data partitions, risking\ninflated performance estimates and incorrect transfer of global knowledge to\nlocal time series. We argue for the development of robust evaluation\nmethodologies to prevent pitfalls already observed in LLM and classical time\nseries benchmarking, and call upon the research community to design new,\nprincipled approaches, such as evaluations on truly out-of-sample future data,\nto safeguard the integrity of TSFM assessment.\n", "link": "http://arxiv.org/abs/2510.13654v1", "date": "2025-10-15", "relevancy": 2.2338, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4549}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4549}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4305}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Time%20Series%20Foundation%20Models%3A%20Benchmarking%20Challenges%20and%20Requirements&body=Title%3A%20Time%20Series%20Foundation%20Models%3A%20Benchmarking%20Challenges%20and%20Requirements%0AAuthor%3A%20Marcel%20Meyer%20and%20Sascha%20Kaltenpoth%20and%20Kevin%20Zalipski%20and%20Oliver%20M%C3%BCller%0AAbstract%3A%20%20%20Time%20Series%20Foundation%20Models%20%28TSFMs%29%20represent%20a%20new%20paradigm%20for%20time%0Aseries%20forecasting%2C%20offering%20zero-shot%20forecasting%20capabilities%20without%20the%0Aneed%20for%20domain-specific%20pre-training%20or%20fine-tuning.%20However%2C%20as%20with%20Large%0ALanguage%20Models%20%28LLMs%29%2C%20evaluating%20TSFMs%20is%20tricky%2C%20as%20with%20ever%20more%20extensive%0Atraining%20sets%2C%20it%20becomes%20more%20and%20more%20challenging%20to%20ensure%20the%20integrity%20of%0Abenchmarking%20data.%20Our%20investigation%20of%20existing%20TSFM%20evaluation%20highlights%0Amultiple%20challenges%2C%20ranging%20from%20the%20representativeness%20of%20the%20benchmark%0Adatasets%2C%20over%20the%20lack%20of%20spatiotemporal%20evaluation%2C%20to%20risks%20of%20information%0Aleakage%20due%20to%20overlapping%20and%20obscure%20datasets%2C%20and%20the%20memorization%20of%20global%0Apatterns%20caused%20by%20external%20shocks%20like%20economic%20crises%20or%20pandemics.%20Our%0Afindings%20reveal%20widespread%20confusion%20regarding%20data%20partitions%2C%20risking%0Ainflated%20performance%20estimates%20and%20incorrect%20transfer%20of%20global%20knowledge%20to%0Alocal%20time%20series.%20We%20argue%20for%20the%20development%20of%20robust%20evaluation%0Amethodologies%20to%20prevent%20pitfalls%20already%20observed%20in%20LLM%20and%20classical%20time%0Aseries%20benchmarking%2C%20and%20call%20upon%20the%20research%20community%20to%20design%20new%2C%0Aprincipled%20approaches%2C%20such%20as%20evaluations%20on%20truly%20out-of-sample%20future%20data%2C%0Ato%20safeguard%20the%20integrity%20of%20TSFM%20assessment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13654v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTime%2520Series%2520Foundation%2520Models%253A%2520Benchmarking%2520Challenges%2520and%2520Requirements%26entry.906535625%3DMarcel%2520Meyer%2520and%2520Sascha%2520Kaltenpoth%2520and%2520Kevin%2520Zalipski%2520and%2520Oliver%2520M%25C3%25BCller%26entry.1292438233%3D%2520%2520Time%2520Series%2520Foundation%2520Models%2520%2528TSFMs%2529%2520represent%2520a%2520new%2520paradigm%2520for%2520time%250Aseries%2520forecasting%252C%2520offering%2520zero-shot%2520forecasting%2520capabilities%2520without%2520the%250Aneed%2520for%2520domain-specific%2520pre-training%2520or%2520fine-tuning.%2520However%252C%2520as%2520with%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%252C%2520evaluating%2520TSFMs%2520is%2520tricky%252C%2520as%2520with%2520ever%2520more%2520extensive%250Atraining%2520sets%252C%2520it%2520becomes%2520more%2520and%2520more%2520challenging%2520to%2520ensure%2520the%2520integrity%2520of%250Abenchmarking%2520data.%2520Our%2520investigation%2520of%2520existing%2520TSFM%2520evaluation%2520highlights%250Amultiple%2520challenges%252C%2520ranging%2520from%2520the%2520representativeness%2520of%2520the%2520benchmark%250Adatasets%252C%2520over%2520the%2520lack%2520of%2520spatiotemporal%2520evaluation%252C%2520to%2520risks%2520of%2520information%250Aleakage%2520due%2520to%2520overlapping%2520and%2520obscure%2520datasets%252C%2520and%2520the%2520memorization%2520of%2520global%250Apatterns%2520caused%2520by%2520external%2520shocks%2520like%2520economic%2520crises%2520or%2520pandemics.%2520Our%250Afindings%2520reveal%2520widespread%2520confusion%2520regarding%2520data%2520partitions%252C%2520risking%250Ainflated%2520performance%2520estimates%2520and%2520incorrect%2520transfer%2520of%2520global%2520knowledge%2520to%250Alocal%2520time%2520series.%2520We%2520argue%2520for%2520the%2520development%2520of%2520robust%2520evaluation%250Amethodologies%2520to%2520prevent%2520pitfalls%2520already%2520observed%2520in%2520LLM%2520and%2520classical%2520time%250Aseries%2520benchmarking%252C%2520and%2520call%2520upon%2520the%2520research%2520community%2520to%2520design%2520new%252C%250Aprincipled%2520approaches%252C%2520such%2520as%2520evaluations%2520on%2520truly%2520out-of-sample%2520future%2520data%252C%250Ato%2520safeguard%2520the%2520integrity%2520of%2520TSFM%2520assessment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13654v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Time%20Series%20Foundation%20Models%3A%20Benchmarking%20Challenges%20and%20Requirements&entry.906535625=Marcel%20Meyer%20and%20Sascha%20Kaltenpoth%20and%20Kevin%20Zalipski%20and%20Oliver%20M%C3%BCller&entry.1292438233=%20%20Time%20Series%20Foundation%20Models%20%28TSFMs%29%20represent%20a%20new%20paradigm%20for%20time%0Aseries%20forecasting%2C%20offering%20zero-shot%20forecasting%20capabilities%20without%20the%0Aneed%20for%20domain-specific%20pre-training%20or%20fine-tuning.%20However%2C%20as%20with%20Large%0ALanguage%20Models%20%28LLMs%29%2C%20evaluating%20TSFMs%20is%20tricky%2C%20as%20with%20ever%20more%20extensive%0Atraining%20sets%2C%20it%20becomes%20more%20and%20more%20challenging%20to%20ensure%20the%20integrity%20of%0Abenchmarking%20data.%20Our%20investigation%20of%20existing%20TSFM%20evaluation%20highlights%0Amultiple%20challenges%2C%20ranging%20from%20the%20representativeness%20of%20the%20benchmark%0Adatasets%2C%20over%20the%20lack%20of%20spatiotemporal%20evaluation%2C%20to%20risks%20of%20information%0Aleakage%20due%20to%20overlapping%20and%20obscure%20datasets%2C%20and%20the%20memorization%20of%20global%0Apatterns%20caused%20by%20external%20shocks%20like%20economic%20crises%20or%20pandemics.%20Our%0Afindings%20reveal%20widespread%20confusion%20regarding%20data%20partitions%2C%20risking%0Ainflated%20performance%20estimates%20and%20incorrect%20transfer%20of%20global%20knowledge%20to%0Alocal%20time%20series.%20We%20argue%20for%20the%20development%20of%20robust%20evaluation%0Amethodologies%20to%20prevent%20pitfalls%20already%20observed%20in%20LLM%20and%20classical%20time%0Aseries%20benchmarking%2C%20and%20call%20upon%20the%20research%20community%20to%20design%20new%2C%0Aprincipled%20approaches%2C%20such%20as%20evaluations%20on%20truly%20out-of-sample%20future%20data%2C%0Ato%20safeguard%20the%20integrity%20of%20TSFM%20assessment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13654v1&entry.124074799=Read"},
{"title": "VRS-UIE: Value-Driven Reordering Scanning for Underwater Image\n  Enhancement", "author": "Kui Jiang and Yan Luo and Junjun Jiang and Ke Gu and Nan Ma and Xianming Liu", "abstract": "  State Space Models (SSMs) have emerged as a promising backbone for vision\ntasks due to their linear complexity and global receptive field. However, in\nthe context of Underwater Image Enhancement (UIE), the standard sequential\nscanning mechanism is fundamentally challenged by the unique statistical\ndistribution characteristics of underwater scenes. The predominance of\nlarge-portion, homogeneous but useless oceanic backgrounds can dilute the\nfeature representation responses of sparse yet valuable targets, thereby\nimpeding effective state propagation and compromising the model's ability to\npreserve both local semantics and global structure. To address this limitation,\nwe propose a novel Value-Driven Reordering Scanning framework for UIE, termed\nVRS-UIE. Its core innovation is a Multi-Granularity Value Guidance Learning\n(MVGL) module that generates a pixel-aligned value map to dynamically reorder\nthe SSM's scanning sequence. This prioritizes informative regions to facilitate\nthe long-range state propagation of salient features. Building upon the MVGL,\nwe design a Mamba-Conv Mixer (MCM) block that synergistically integrates\npriority-driven global sequencing with dynamically adjusted local convolutions,\nthereby effectively modeling both large-portion oceanic backgrounds and\nhigh-value semantic targets. A Cross-Feature Bridge (CFB) further refines\nmulti-level feature fusion. Extensive experiments demonstrate that our VRS-UIE\nframework sets a new state-of-the-art, delivering superior enhancement\nperformance (surpassing WMamba by 0.89 dB on average) by effectively\nsuppressing water bias and preserving structural and color fidelity.\nFurthermore, by incorporating efficient convolutional operators and resolution\nrescaling, we construct a light-weight yet effective scheme, VRS-UIE-S,\nsuitable for real-time UIE applications.\n", "link": "http://arxiv.org/abs/2505.01224v2", "date": "2025-10-15", "relevancy": 2.2334, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5599}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5599}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VRS-UIE%3A%20Value-Driven%20Reordering%20Scanning%20for%20Underwater%20Image%0A%20%20Enhancement&body=Title%3A%20VRS-UIE%3A%20Value-Driven%20Reordering%20Scanning%20for%20Underwater%20Image%0A%20%20Enhancement%0AAuthor%3A%20Kui%20Jiang%20and%20Yan%20Luo%20and%20Junjun%20Jiang%20and%20Ke%20Gu%20and%20Nan%20Ma%20and%20Xianming%20Liu%0AAbstract%3A%20%20%20State%20Space%20Models%20%28SSMs%29%20have%20emerged%20as%20a%20promising%20backbone%20for%20vision%0Atasks%20due%20to%20their%20linear%20complexity%20and%20global%20receptive%20field.%20However%2C%20in%0Athe%20context%20of%20Underwater%20Image%20Enhancement%20%28UIE%29%2C%20the%20standard%20sequential%0Ascanning%20mechanism%20is%20fundamentally%20challenged%20by%20the%20unique%20statistical%0Adistribution%20characteristics%20of%20underwater%20scenes.%20The%20predominance%20of%0Alarge-portion%2C%20homogeneous%20but%20useless%20oceanic%20backgrounds%20can%20dilute%20the%0Afeature%20representation%20responses%20of%20sparse%20yet%20valuable%20targets%2C%20thereby%0Aimpeding%20effective%20state%20propagation%20and%20compromising%20the%20model%27s%20ability%20to%0Apreserve%20both%20local%20semantics%20and%20global%20structure.%20To%20address%20this%20limitation%2C%0Awe%20propose%20a%20novel%20Value-Driven%20Reordering%20Scanning%20framework%20for%20UIE%2C%20termed%0AVRS-UIE.%20Its%20core%20innovation%20is%20a%20Multi-Granularity%20Value%20Guidance%20Learning%0A%28MVGL%29%20module%20that%20generates%20a%20pixel-aligned%20value%20map%20to%20dynamically%20reorder%0Athe%20SSM%27s%20scanning%20sequence.%20This%20prioritizes%20informative%20regions%20to%20facilitate%0Athe%20long-range%20state%20propagation%20of%20salient%20features.%20Building%20upon%20the%20MVGL%2C%0Awe%20design%20a%20Mamba-Conv%20Mixer%20%28MCM%29%20block%20that%20synergistically%20integrates%0Apriority-driven%20global%20sequencing%20with%20dynamically%20adjusted%20local%20convolutions%2C%0Athereby%20effectively%20modeling%20both%20large-portion%20oceanic%20backgrounds%20and%0Ahigh-value%20semantic%20targets.%20A%20Cross-Feature%20Bridge%20%28CFB%29%20further%20refines%0Amulti-level%20feature%20fusion.%20Extensive%20experiments%20demonstrate%20that%20our%20VRS-UIE%0Aframework%20sets%20a%20new%20state-of-the-art%2C%20delivering%20superior%20enhancement%0Aperformance%20%28surpassing%20WMamba%20by%200.89%20dB%20on%20average%29%20by%20effectively%0Asuppressing%20water%20bias%20and%20preserving%20structural%20and%20color%20fidelity.%0AFurthermore%2C%20by%20incorporating%20efficient%20convolutional%20operators%20and%20resolution%0Arescaling%2C%20we%20construct%20a%20light-weight%20yet%20effective%20scheme%2C%20VRS-UIE-S%2C%0Asuitable%20for%20real-time%20UIE%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01224v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVRS-UIE%253A%2520Value-Driven%2520Reordering%2520Scanning%2520for%2520Underwater%2520Image%250A%2520%2520Enhancement%26entry.906535625%3DKui%2520Jiang%2520and%2520Yan%2520Luo%2520and%2520Junjun%2520Jiang%2520and%2520Ke%2520Gu%2520and%2520Nan%2520Ma%2520and%2520Xianming%2520Liu%26entry.1292438233%3D%2520%2520State%2520Space%2520Models%2520%2528SSMs%2529%2520have%2520emerged%2520as%2520a%2520promising%2520backbone%2520for%2520vision%250Atasks%2520due%2520to%2520their%2520linear%2520complexity%2520and%2520global%2520receptive%2520field.%2520However%252C%2520in%250Athe%2520context%2520of%2520Underwater%2520Image%2520Enhancement%2520%2528UIE%2529%252C%2520the%2520standard%2520sequential%250Ascanning%2520mechanism%2520is%2520fundamentally%2520challenged%2520by%2520the%2520unique%2520statistical%250Adistribution%2520characteristics%2520of%2520underwater%2520scenes.%2520The%2520predominance%2520of%250Alarge-portion%252C%2520homogeneous%2520but%2520useless%2520oceanic%2520backgrounds%2520can%2520dilute%2520the%250Afeature%2520representation%2520responses%2520of%2520sparse%2520yet%2520valuable%2520targets%252C%2520thereby%250Aimpeding%2520effective%2520state%2520propagation%2520and%2520compromising%2520the%2520model%2527s%2520ability%2520to%250Apreserve%2520both%2520local%2520semantics%2520and%2520global%2520structure.%2520To%2520address%2520this%2520limitation%252C%250Awe%2520propose%2520a%2520novel%2520Value-Driven%2520Reordering%2520Scanning%2520framework%2520for%2520UIE%252C%2520termed%250AVRS-UIE.%2520Its%2520core%2520innovation%2520is%2520a%2520Multi-Granularity%2520Value%2520Guidance%2520Learning%250A%2528MVGL%2529%2520module%2520that%2520generates%2520a%2520pixel-aligned%2520value%2520map%2520to%2520dynamically%2520reorder%250Athe%2520SSM%2527s%2520scanning%2520sequence.%2520This%2520prioritizes%2520informative%2520regions%2520to%2520facilitate%250Athe%2520long-range%2520state%2520propagation%2520of%2520salient%2520features.%2520Building%2520upon%2520the%2520MVGL%252C%250Awe%2520design%2520a%2520Mamba-Conv%2520Mixer%2520%2528MCM%2529%2520block%2520that%2520synergistically%2520integrates%250Apriority-driven%2520global%2520sequencing%2520with%2520dynamically%2520adjusted%2520local%2520convolutions%252C%250Athereby%2520effectively%2520modeling%2520both%2520large-portion%2520oceanic%2520backgrounds%2520and%250Ahigh-value%2520semantic%2520targets.%2520A%2520Cross-Feature%2520Bridge%2520%2528CFB%2529%2520further%2520refines%250Amulti-level%2520feature%2520fusion.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520VRS-UIE%250Aframework%2520sets%2520a%2520new%2520state-of-the-art%252C%2520delivering%2520superior%2520enhancement%250Aperformance%2520%2528surpassing%2520WMamba%2520by%25200.89%2520dB%2520on%2520average%2529%2520by%2520effectively%250Asuppressing%2520water%2520bias%2520and%2520preserving%2520structural%2520and%2520color%2520fidelity.%250AFurthermore%252C%2520by%2520incorporating%2520efficient%2520convolutional%2520operators%2520and%2520resolution%250Arescaling%252C%2520we%2520construct%2520a%2520light-weight%2520yet%2520effective%2520scheme%252C%2520VRS-UIE-S%252C%250Asuitable%2520for%2520real-time%2520UIE%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01224v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VRS-UIE%3A%20Value-Driven%20Reordering%20Scanning%20for%20Underwater%20Image%0A%20%20Enhancement&entry.906535625=Kui%20Jiang%20and%20Yan%20Luo%20and%20Junjun%20Jiang%20and%20Ke%20Gu%20and%20Nan%20Ma%20and%20Xianming%20Liu&entry.1292438233=%20%20State%20Space%20Models%20%28SSMs%29%20have%20emerged%20as%20a%20promising%20backbone%20for%20vision%0Atasks%20due%20to%20their%20linear%20complexity%20and%20global%20receptive%20field.%20However%2C%20in%0Athe%20context%20of%20Underwater%20Image%20Enhancement%20%28UIE%29%2C%20the%20standard%20sequential%0Ascanning%20mechanism%20is%20fundamentally%20challenged%20by%20the%20unique%20statistical%0Adistribution%20characteristics%20of%20underwater%20scenes.%20The%20predominance%20of%0Alarge-portion%2C%20homogeneous%20but%20useless%20oceanic%20backgrounds%20can%20dilute%20the%0Afeature%20representation%20responses%20of%20sparse%20yet%20valuable%20targets%2C%20thereby%0Aimpeding%20effective%20state%20propagation%20and%20compromising%20the%20model%27s%20ability%20to%0Apreserve%20both%20local%20semantics%20and%20global%20structure.%20To%20address%20this%20limitation%2C%0Awe%20propose%20a%20novel%20Value-Driven%20Reordering%20Scanning%20framework%20for%20UIE%2C%20termed%0AVRS-UIE.%20Its%20core%20innovation%20is%20a%20Multi-Granularity%20Value%20Guidance%20Learning%0A%28MVGL%29%20module%20that%20generates%20a%20pixel-aligned%20value%20map%20to%20dynamically%20reorder%0Athe%20SSM%27s%20scanning%20sequence.%20This%20prioritizes%20informative%20regions%20to%20facilitate%0Athe%20long-range%20state%20propagation%20of%20salient%20features.%20Building%20upon%20the%20MVGL%2C%0Awe%20design%20a%20Mamba-Conv%20Mixer%20%28MCM%29%20block%20that%20synergistically%20integrates%0Apriority-driven%20global%20sequencing%20with%20dynamically%20adjusted%20local%20convolutions%2C%0Athereby%20effectively%20modeling%20both%20large-portion%20oceanic%20backgrounds%20and%0Ahigh-value%20semantic%20targets.%20A%20Cross-Feature%20Bridge%20%28CFB%29%20further%20refines%0Amulti-level%20feature%20fusion.%20Extensive%20experiments%20demonstrate%20that%20our%20VRS-UIE%0Aframework%20sets%20a%20new%20state-of-the-art%2C%20delivering%20superior%20enhancement%0Aperformance%20%28surpassing%20WMamba%20by%200.89%20dB%20on%20average%29%20by%20effectively%0Asuppressing%20water%20bias%20and%20preserving%20structural%20and%20color%20fidelity.%0AFurthermore%2C%20by%20incorporating%20efficient%20convolutional%20operators%20and%20resolution%0Arescaling%2C%20we%20construct%20a%20light-weight%20yet%20effective%20scheme%2C%20VRS-UIE-S%2C%0Asuitable%20for%20real-time%20UIE%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01224v2&entry.124074799=Read"},
{"title": "A Novel Robot Hand with Hoeckens Linkages and Soft Phalanges for\n  Scooping and Self-Adaptive Grasping in Environmental Constraints", "author": "Wentao Guo and Yizhou Wang and Wenzeng Zhang", "abstract": "  This paper presents a novel underactuated adaptive robotic hand, Hockens-A\nHand, which integrates the Hoeckens mechanism, a double-parallelogram linkage,\nand a specialized four-bar linkage to achieve three adaptive grasping modes:\nparallel pinching, asymmetric scooping, and enveloping grasping. Hockens-A Hand\nrequires only a single linear actuator, leveraging passive mechanical\nintelligence to ensure adaptability and compliance in unstructured\nenvironments. Specifically, the vertical motion of the Hoeckens mechanism\nintroduces compliance, the double-parallelogram linkage ensures line contact at\nthe fingertip, and the four-bar amplification system enables natural\ntransitions between different grasping modes. Additionally, the inclusion of a\nmesh-textured silicone phalanx further enhances the ability to envelop objects\nof various shapes and sizes. This study employs detailed kinematic analysis to\noptimize the push angle and design the linkage lengths for optimal performance.\nSimulations validated the design by analyzing the fingertip motion and ensuring\nsmooth transitions between grasping modes. Furthermore, the grasping force was\nanalyzed using power equations to enhance the understanding of the system's\nperformance.Experimental validation using a 3D-printed prototype demonstrates\nthe three grasping modes of the hand in various scenarios under environmental\nconstraints, verifying its grasping stability and broad applicability.\n", "link": "http://arxiv.org/abs/2510.13535v1", "date": "2025-10-15", "relevancy": 2.2206, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5664}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5529}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Novel%20Robot%20Hand%20with%20Hoeckens%20Linkages%20and%20Soft%20Phalanges%20for%0A%20%20Scooping%20and%20Self-Adaptive%20Grasping%20in%20Environmental%20Constraints&body=Title%3A%20A%20Novel%20Robot%20Hand%20with%20Hoeckens%20Linkages%20and%20Soft%20Phalanges%20for%0A%20%20Scooping%20and%20Self-Adaptive%20Grasping%20in%20Environmental%20Constraints%0AAuthor%3A%20Wentao%20Guo%20and%20Yizhou%20Wang%20and%20Wenzeng%20Zhang%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20underactuated%20adaptive%20robotic%20hand%2C%20Hockens-A%0AHand%2C%20which%20integrates%20the%20Hoeckens%20mechanism%2C%20a%20double-parallelogram%20linkage%2C%0Aand%20a%20specialized%20four-bar%20linkage%20to%20achieve%20three%20adaptive%20grasping%20modes%3A%0Aparallel%20pinching%2C%20asymmetric%20scooping%2C%20and%20enveloping%20grasping.%20Hockens-A%20Hand%0Arequires%20only%20a%20single%20linear%20actuator%2C%20leveraging%20passive%20mechanical%0Aintelligence%20to%20ensure%20adaptability%20and%20compliance%20in%20unstructured%0Aenvironments.%20Specifically%2C%20the%20vertical%20motion%20of%20the%20Hoeckens%20mechanism%0Aintroduces%20compliance%2C%20the%20double-parallelogram%20linkage%20ensures%20line%20contact%20at%0Athe%20fingertip%2C%20and%20the%20four-bar%20amplification%20system%20enables%20natural%0Atransitions%20between%20different%20grasping%20modes.%20Additionally%2C%20the%20inclusion%20of%20a%0Amesh-textured%20silicone%20phalanx%20further%20enhances%20the%20ability%20to%20envelop%20objects%0Aof%20various%20shapes%20and%20sizes.%20This%20study%20employs%20detailed%20kinematic%20analysis%20to%0Aoptimize%20the%20push%20angle%20and%20design%20the%20linkage%20lengths%20for%20optimal%20performance.%0ASimulations%20validated%20the%20design%20by%20analyzing%20the%20fingertip%20motion%20and%20ensuring%0Asmooth%20transitions%20between%20grasping%20modes.%20Furthermore%2C%20the%20grasping%20force%20was%0Aanalyzed%20using%20power%20equations%20to%20enhance%20the%20understanding%20of%20the%20system%27s%0Aperformance.Experimental%20validation%20using%20a%203D-printed%20prototype%20demonstrates%0Athe%20three%20grasping%20modes%20of%20the%20hand%20in%20various%20scenarios%20under%20environmental%0Aconstraints%2C%20verifying%20its%20grasping%20stability%20and%20broad%20applicability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13535v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Novel%2520Robot%2520Hand%2520with%2520Hoeckens%2520Linkages%2520and%2520Soft%2520Phalanges%2520for%250A%2520%2520Scooping%2520and%2520Self-Adaptive%2520Grasping%2520in%2520Environmental%2520Constraints%26entry.906535625%3DWentao%2520Guo%2520and%2520Yizhou%2520Wang%2520and%2520Wenzeng%2520Zhang%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520underactuated%2520adaptive%2520robotic%2520hand%252C%2520Hockens-A%250AHand%252C%2520which%2520integrates%2520the%2520Hoeckens%2520mechanism%252C%2520a%2520double-parallelogram%2520linkage%252C%250Aand%2520a%2520specialized%2520four-bar%2520linkage%2520to%2520achieve%2520three%2520adaptive%2520grasping%2520modes%253A%250Aparallel%2520pinching%252C%2520asymmetric%2520scooping%252C%2520and%2520enveloping%2520grasping.%2520Hockens-A%2520Hand%250Arequires%2520only%2520a%2520single%2520linear%2520actuator%252C%2520leveraging%2520passive%2520mechanical%250Aintelligence%2520to%2520ensure%2520adaptability%2520and%2520compliance%2520in%2520unstructured%250Aenvironments.%2520Specifically%252C%2520the%2520vertical%2520motion%2520of%2520the%2520Hoeckens%2520mechanism%250Aintroduces%2520compliance%252C%2520the%2520double-parallelogram%2520linkage%2520ensures%2520line%2520contact%2520at%250Athe%2520fingertip%252C%2520and%2520the%2520four-bar%2520amplification%2520system%2520enables%2520natural%250Atransitions%2520between%2520different%2520grasping%2520modes.%2520Additionally%252C%2520the%2520inclusion%2520of%2520a%250Amesh-textured%2520silicone%2520phalanx%2520further%2520enhances%2520the%2520ability%2520to%2520envelop%2520objects%250Aof%2520various%2520shapes%2520and%2520sizes.%2520This%2520study%2520employs%2520detailed%2520kinematic%2520analysis%2520to%250Aoptimize%2520the%2520push%2520angle%2520and%2520design%2520the%2520linkage%2520lengths%2520for%2520optimal%2520performance.%250ASimulations%2520validated%2520the%2520design%2520by%2520analyzing%2520the%2520fingertip%2520motion%2520and%2520ensuring%250Asmooth%2520transitions%2520between%2520grasping%2520modes.%2520Furthermore%252C%2520the%2520grasping%2520force%2520was%250Aanalyzed%2520using%2520power%2520equations%2520to%2520enhance%2520the%2520understanding%2520of%2520the%2520system%2527s%250Aperformance.Experimental%2520validation%2520using%2520a%25203D-printed%2520prototype%2520demonstrates%250Athe%2520three%2520grasping%2520modes%2520of%2520the%2520hand%2520in%2520various%2520scenarios%2520under%2520environmental%250Aconstraints%252C%2520verifying%2520its%2520grasping%2520stability%2520and%2520broad%2520applicability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13535v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Novel%20Robot%20Hand%20with%20Hoeckens%20Linkages%20and%20Soft%20Phalanges%20for%0A%20%20Scooping%20and%20Self-Adaptive%20Grasping%20in%20Environmental%20Constraints&entry.906535625=Wentao%20Guo%20and%20Yizhou%20Wang%20and%20Wenzeng%20Zhang&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20underactuated%20adaptive%20robotic%20hand%2C%20Hockens-A%0AHand%2C%20which%20integrates%20the%20Hoeckens%20mechanism%2C%20a%20double-parallelogram%20linkage%2C%0Aand%20a%20specialized%20four-bar%20linkage%20to%20achieve%20three%20adaptive%20grasping%20modes%3A%0Aparallel%20pinching%2C%20asymmetric%20scooping%2C%20and%20enveloping%20grasping.%20Hockens-A%20Hand%0Arequires%20only%20a%20single%20linear%20actuator%2C%20leveraging%20passive%20mechanical%0Aintelligence%20to%20ensure%20adaptability%20and%20compliance%20in%20unstructured%0Aenvironments.%20Specifically%2C%20the%20vertical%20motion%20of%20the%20Hoeckens%20mechanism%0Aintroduces%20compliance%2C%20the%20double-parallelogram%20linkage%20ensures%20line%20contact%20at%0Athe%20fingertip%2C%20and%20the%20four-bar%20amplification%20system%20enables%20natural%0Atransitions%20between%20different%20grasping%20modes.%20Additionally%2C%20the%20inclusion%20of%20a%0Amesh-textured%20silicone%20phalanx%20further%20enhances%20the%20ability%20to%20envelop%20objects%0Aof%20various%20shapes%20and%20sizes.%20This%20study%20employs%20detailed%20kinematic%20analysis%20to%0Aoptimize%20the%20push%20angle%20and%20design%20the%20linkage%20lengths%20for%20optimal%20performance.%0ASimulations%20validated%20the%20design%20by%20analyzing%20the%20fingertip%20motion%20and%20ensuring%0Asmooth%20transitions%20between%20grasping%20modes.%20Furthermore%2C%20the%20grasping%20force%20was%0Aanalyzed%20using%20power%20equations%20to%20enhance%20the%20understanding%20of%20the%20system%27s%0Aperformance.Experimental%20validation%20using%20a%203D-printed%20prototype%20demonstrates%0Athe%20three%20grasping%20modes%20of%20the%20hand%20in%20various%20scenarios%20under%20environmental%0Aconstraints%2C%20verifying%20its%20grasping%20stability%20and%20broad%20applicability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13535v1&entry.124074799=Read"},
{"title": "MetaCaptioner: Towards Generalist Visual Captioning with Open-source\n  Suites", "author": "Zhenxin Lei and Zhangwei Gao and Changyao Tian and Erfei Cui and Guanzhou Chen and Danni Yang and Yuchen Duan and Zhaokai Wang and Wenhao Li and Weiyun Wang and Xiangyu Zhao and Jiayi Ji and Yu Qiao and Wenhai Wang and Gen Luo", "abstract": "  Generalist visual captioning goes beyond a simple appearance description\ntask, but requires integrating a series of visual cues into a caption and\nhandling various visual domains. In this task, current open-source models\npresent a large performance gap with commercial ones, which limits various\napplications such as data synthesis. To bridge the gap, this paper proposes\nCapFlow, a novel multi-agent collaboration workflow. CapFlow demonstrates for\nthe first time that, by capitalizing on open-source models, it is possible to\nachieve caption quality on par with GPT-4.1 in various domains with an 89.5%\nreduction in costs. By leveraging CapFlow as the data synthesizer, we produce\nhigh-quality visual captions from image and video domains at scale, and obtain\na generalist visual captioner via fine-tuning, namely MetaCaptioner. Through\nextensive experiments, we show that MetaCaptioner not only achieves comparable\ncaptioning capabilities with commercial models but also reaches top-tier\nmultimodal performance in the open-source community. We hope CapFlow and\nMetaCaptioner can benefit future multimodal research by providing a strong and\ncost-effective visual captioning solution.\n", "link": "http://arxiv.org/abs/2510.12126v2", "date": "2025-10-15", "relevancy": 2.219, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5555}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5546}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MetaCaptioner%3A%20Towards%20Generalist%20Visual%20Captioning%20with%20Open-source%0A%20%20Suites&body=Title%3A%20MetaCaptioner%3A%20Towards%20Generalist%20Visual%20Captioning%20with%20Open-source%0A%20%20Suites%0AAuthor%3A%20Zhenxin%20Lei%20and%20Zhangwei%20Gao%20and%20Changyao%20Tian%20and%20Erfei%20Cui%20and%20Guanzhou%20Chen%20and%20Danni%20Yang%20and%20Yuchen%20Duan%20and%20Zhaokai%20Wang%20and%20Wenhao%20Li%20and%20Weiyun%20Wang%20and%20Xiangyu%20Zhao%20and%20Jiayi%20Ji%20and%20Yu%20Qiao%20and%20Wenhai%20Wang%20and%20Gen%20Luo%0AAbstract%3A%20%20%20Generalist%20visual%20captioning%20goes%20beyond%20a%20simple%20appearance%20description%0Atask%2C%20but%20requires%20integrating%20a%20series%20of%20visual%20cues%20into%20a%20caption%20and%0Ahandling%20various%20visual%20domains.%20In%20this%20task%2C%20current%20open-source%20models%0Apresent%20a%20large%20performance%20gap%20with%20commercial%20ones%2C%20which%20limits%20various%0Aapplications%20such%20as%20data%20synthesis.%20To%20bridge%20the%20gap%2C%20this%20paper%20proposes%0ACapFlow%2C%20a%20novel%20multi-agent%20collaboration%20workflow.%20CapFlow%20demonstrates%20for%0Athe%20first%20time%20that%2C%20by%20capitalizing%20on%20open-source%20models%2C%20it%20is%20possible%20to%0Aachieve%20caption%20quality%20on%20par%20with%20GPT-4.1%20in%20various%20domains%20with%20an%2089.5%25%0Areduction%20in%20costs.%20By%20leveraging%20CapFlow%20as%20the%20data%20synthesizer%2C%20we%20produce%0Ahigh-quality%20visual%20captions%20from%20image%20and%20video%20domains%20at%20scale%2C%20and%20obtain%0Aa%20generalist%20visual%20captioner%20via%20fine-tuning%2C%20namely%20MetaCaptioner.%20Through%0Aextensive%20experiments%2C%20we%20show%20that%20MetaCaptioner%20not%20only%20achieves%20comparable%0Acaptioning%20capabilities%20with%20commercial%20models%20but%20also%20reaches%20top-tier%0Amultimodal%20performance%20in%20the%20open-source%20community.%20We%20hope%20CapFlow%20and%0AMetaCaptioner%20can%20benefit%20future%20multimodal%20research%20by%20providing%20a%20strong%20and%0Acost-effective%20visual%20captioning%20solution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12126v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetaCaptioner%253A%2520Towards%2520Generalist%2520Visual%2520Captioning%2520with%2520Open-source%250A%2520%2520Suites%26entry.906535625%3DZhenxin%2520Lei%2520and%2520Zhangwei%2520Gao%2520and%2520Changyao%2520Tian%2520and%2520Erfei%2520Cui%2520and%2520Guanzhou%2520Chen%2520and%2520Danni%2520Yang%2520and%2520Yuchen%2520Duan%2520and%2520Zhaokai%2520Wang%2520and%2520Wenhao%2520Li%2520and%2520Weiyun%2520Wang%2520and%2520Xiangyu%2520Zhao%2520and%2520Jiayi%2520Ji%2520and%2520Yu%2520Qiao%2520and%2520Wenhai%2520Wang%2520and%2520Gen%2520Luo%26entry.1292438233%3D%2520%2520Generalist%2520visual%2520captioning%2520goes%2520beyond%2520a%2520simple%2520appearance%2520description%250Atask%252C%2520but%2520requires%2520integrating%2520a%2520series%2520of%2520visual%2520cues%2520into%2520a%2520caption%2520and%250Ahandling%2520various%2520visual%2520domains.%2520In%2520this%2520task%252C%2520current%2520open-source%2520models%250Apresent%2520a%2520large%2520performance%2520gap%2520with%2520commercial%2520ones%252C%2520which%2520limits%2520various%250Aapplications%2520such%2520as%2520data%2520synthesis.%2520To%2520bridge%2520the%2520gap%252C%2520this%2520paper%2520proposes%250ACapFlow%252C%2520a%2520novel%2520multi-agent%2520collaboration%2520workflow.%2520CapFlow%2520demonstrates%2520for%250Athe%2520first%2520time%2520that%252C%2520by%2520capitalizing%2520on%2520open-source%2520models%252C%2520it%2520is%2520possible%2520to%250Aachieve%2520caption%2520quality%2520on%2520par%2520with%2520GPT-4.1%2520in%2520various%2520domains%2520with%2520an%252089.5%2525%250Areduction%2520in%2520costs.%2520By%2520leveraging%2520CapFlow%2520as%2520the%2520data%2520synthesizer%252C%2520we%2520produce%250Ahigh-quality%2520visual%2520captions%2520from%2520image%2520and%2520video%2520domains%2520at%2520scale%252C%2520and%2520obtain%250Aa%2520generalist%2520visual%2520captioner%2520via%2520fine-tuning%252C%2520namely%2520MetaCaptioner.%2520Through%250Aextensive%2520experiments%252C%2520we%2520show%2520that%2520MetaCaptioner%2520not%2520only%2520achieves%2520comparable%250Acaptioning%2520capabilities%2520with%2520commercial%2520models%2520but%2520also%2520reaches%2520top-tier%250Amultimodal%2520performance%2520in%2520the%2520open-source%2520community.%2520We%2520hope%2520CapFlow%2520and%250AMetaCaptioner%2520can%2520benefit%2520future%2520multimodal%2520research%2520by%2520providing%2520a%2520strong%2520and%250Acost-effective%2520visual%2520captioning%2520solution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12126v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MetaCaptioner%3A%20Towards%20Generalist%20Visual%20Captioning%20with%20Open-source%0A%20%20Suites&entry.906535625=Zhenxin%20Lei%20and%20Zhangwei%20Gao%20and%20Changyao%20Tian%20and%20Erfei%20Cui%20and%20Guanzhou%20Chen%20and%20Danni%20Yang%20and%20Yuchen%20Duan%20and%20Zhaokai%20Wang%20and%20Wenhao%20Li%20and%20Weiyun%20Wang%20and%20Xiangyu%20Zhao%20and%20Jiayi%20Ji%20and%20Yu%20Qiao%20and%20Wenhai%20Wang%20and%20Gen%20Luo&entry.1292438233=%20%20Generalist%20visual%20captioning%20goes%20beyond%20a%20simple%20appearance%20description%0Atask%2C%20but%20requires%20integrating%20a%20series%20of%20visual%20cues%20into%20a%20caption%20and%0Ahandling%20various%20visual%20domains.%20In%20this%20task%2C%20current%20open-source%20models%0Apresent%20a%20large%20performance%20gap%20with%20commercial%20ones%2C%20which%20limits%20various%0Aapplications%20such%20as%20data%20synthesis.%20To%20bridge%20the%20gap%2C%20this%20paper%20proposes%0ACapFlow%2C%20a%20novel%20multi-agent%20collaboration%20workflow.%20CapFlow%20demonstrates%20for%0Athe%20first%20time%20that%2C%20by%20capitalizing%20on%20open-source%20models%2C%20it%20is%20possible%20to%0Aachieve%20caption%20quality%20on%20par%20with%20GPT-4.1%20in%20various%20domains%20with%20an%2089.5%25%0Areduction%20in%20costs.%20By%20leveraging%20CapFlow%20as%20the%20data%20synthesizer%2C%20we%20produce%0Ahigh-quality%20visual%20captions%20from%20image%20and%20video%20domains%20at%20scale%2C%20and%20obtain%0Aa%20generalist%20visual%20captioner%20via%20fine-tuning%2C%20namely%20MetaCaptioner.%20Through%0Aextensive%20experiments%2C%20we%20show%20that%20MetaCaptioner%20not%20only%20achieves%20comparable%0Acaptioning%20capabilities%20with%20commercial%20models%20but%20also%20reaches%20top-tier%0Amultimodal%20performance%20in%20the%20open-source%20community.%20We%20hope%20CapFlow%20and%0AMetaCaptioner%20can%20benefit%20future%20multimodal%20research%20by%20providing%20a%20strong%20and%0Acost-effective%20visual%20captioning%20solution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12126v2&entry.124074799=Read"},
{"title": "Scaling Vision Transformers for Functional MRI with Flat Maps", "author": "Connor Lane and Daniel Z. Kaplan and Tanishq Mathew Abraham and Paul S. Scotti", "abstract": "  A key question for adapting modern deep learning architectures to functional\nMRI (fMRI) is how to represent the data for model input. To bridge the modality\ngap between fMRI and natural images, we transform the 4D volumetric fMRI data\ninto videos of 2D fMRI activity flat maps. We train Vision Transformers on 2.3K\nhours of fMRI flat map videos from the Human Connectome Project using the\nspatiotemporal masked autoencoder (MAE) framework. We observe that masked fMRI\nmodeling performance improves with dataset size according to a strict power\nscaling law. Downstream classification benchmarks show that our model learns\nrich representations supporting both fine-grained state decoding across\nsubjects, as well as subject-specific trait decoding across changes in brain\nstate. This work is part of an ongoing open science project to build foundation\nmodels for fMRI data. Our code and datasets are available at\nhttps://github.com/MedARC-AI/fmri-fm.\n", "link": "http://arxiv.org/abs/2510.13768v1", "date": "2025-10-15", "relevancy": 2.214, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6118}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5427}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Vision%20Transformers%20for%20Functional%20MRI%20with%20Flat%20Maps&body=Title%3A%20Scaling%20Vision%20Transformers%20for%20Functional%20MRI%20with%20Flat%20Maps%0AAuthor%3A%20Connor%20Lane%20and%20Daniel%20Z.%20Kaplan%20and%20Tanishq%20Mathew%20Abraham%20and%20Paul%20S.%20Scotti%0AAbstract%3A%20%20%20A%20key%20question%20for%20adapting%20modern%20deep%20learning%20architectures%20to%20functional%0AMRI%20%28fMRI%29%20is%20how%20to%20represent%20the%20data%20for%20model%20input.%20To%20bridge%20the%20modality%0Agap%20between%20fMRI%20and%20natural%20images%2C%20we%20transform%20the%204D%20volumetric%20fMRI%20data%0Ainto%20videos%20of%202D%20fMRI%20activity%20flat%20maps.%20We%20train%20Vision%20Transformers%20on%202.3K%0Ahours%20of%20fMRI%20flat%20map%20videos%20from%20the%20Human%20Connectome%20Project%20using%20the%0Aspatiotemporal%20masked%20autoencoder%20%28MAE%29%20framework.%20We%20observe%20that%20masked%20fMRI%0Amodeling%20performance%20improves%20with%20dataset%20size%20according%20to%20a%20strict%20power%0Ascaling%20law.%20Downstream%20classification%20benchmarks%20show%20that%20our%20model%20learns%0Arich%20representations%20supporting%20both%20fine-grained%20state%20decoding%20across%0Asubjects%2C%20as%20well%20as%20subject-specific%20trait%20decoding%20across%20changes%20in%20brain%0Astate.%20This%20work%20is%20part%20of%20an%20ongoing%20open%20science%20project%20to%20build%20foundation%0Amodels%20for%20fMRI%20data.%20Our%20code%20and%20datasets%20are%20available%20at%0Ahttps%3A//github.com/MedARC-AI/fmri-fm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13768v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Vision%2520Transformers%2520for%2520Functional%2520MRI%2520with%2520Flat%2520Maps%26entry.906535625%3DConnor%2520Lane%2520and%2520Daniel%2520Z.%2520Kaplan%2520and%2520Tanishq%2520Mathew%2520Abraham%2520and%2520Paul%2520S.%2520Scotti%26entry.1292438233%3D%2520%2520A%2520key%2520question%2520for%2520adapting%2520modern%2520deep%2520learning%2520architectures%2520to%2520functional%250AMRI%2520%2528fMRI%2529%2520is%2520how%2520to%2520represent%2520the%2520data%2520for%2520model%2520input.%2520To%2520bridge%2520the%2520modality%250Agap%2520between%2520fMRI%2520and%2520natural%2520images%252C%2520we%2520transform%2520the%25204D%2520volumetric%2520fMRI%2520data%250Ainto%2520videos%2520of%25202D%2520fMRI%2520activity%2520flat%2520maps.%2520We%2520train%2520Vision%2520Transformers%2520on%25202.3K%250Ahours%2520of%2520fMRI%2520flat%2520map%2520videos%2520from%2520the%2520Human%2520Connectome%2520Project%2520using%2520the%250Aspatiotemporal%2520masked%2520autoencoder%2520%2528MAE%2529%2520framework.%2520We%2520observe%2520that%2520masked%2520fMRI%250Amodeling%2520performance%2520improves%2520with%2520dataset%2520size%2520according%2520to%2520a%2520strict%2520power%250Ascaling%2520law.%2520Downstream%2520classification%2520benchmarks%2520show%2520that%2520our%2520model%2520learns%250Arich%2520representations%2520supporting%2520both%2520fine-grained%2520state%2520decoding%2520across%250Asubjects%252C%2520as%2520well%2520as%2520subject-specific%2520trait%2520decoding%2520across%2520changes%2520in%2520brain%250Astate.%2520This%2520work%2520is%2520part%2520of%2520an%2520ongoing%2520open%2520science%2520project%2520to%2520build%2520foundation%250Amodels%2520for%2520fMRI%2520data.%2520Our%2520code%2520and%2520datasets%2520are%2520available%2520at%250Ahttps%253A//github.com/MedARC-AI/fmri-fm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13768v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Vision%20Transformers%20for%20Functional%20MRI%20with%20Flat%20Maps&entry.906535625=Connor%20Lane%20and%20Daniel%20Z.%20Kaplan%20and%20Tanishq%20Mathew%20Abraham%20and%20Paul%20S.%20Scotti&entry.1292438233=%20%20A%20key%20question%20for%20adapting%20modern%20deep%20learning%20architectures%20to%20functional%0AMRI%20%28fMRI%29%20is%20how%20to%20represent%20the%20data%20for%20model%20input.%20To%20bridge%20the%20modality%0Agap%20between%20fMRI%20and%20natural%20images%2C%20we%20transform%20the%204D%20volumetric%20fMRI%20data%0Ainto%20videos%20of%202D%20fMRI%20activity%20flat%20maps.%20We%20train%20Vision%20Transformers%20on%202.3K%0Ahours%20of%20fMRI%20flat%20map%20videos%20from%20the%20Human%20Connectome%20Project%20using%20the%0Aspatiotemporal%20masked%20autoencoder%20%28MAE%29%20framework.%20We%20observe%20that%20masked%20fMRI%0Amodeling%20performance%20improves%20with%20dataset%20size%20according%20to%20a%20strict%20power%0Ascaling%20law.%20Downstream%20classification%20benchmarks%20show%20that%20our%20model%20learns%0Arich%20representations%20supporting%20both%20fine-grained%20state%20decoding%20across%0Asubjects%2C%20as%20well%20as%20subject-specific%20trait%20decoding%20across%20changes%20in%20brain%0Astate.%20This%20work%20is%20part%20of%20an%20ongoing%20open%20science%20project%20to%20build%20foundation%0Amodels%20for%20fMRI%20data.%20Our%20code%20and%20datasets%20are%20available%20at%0Ahttps%3A//github.com/MedARC-AI/fmri-fm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13768v1&entry.124074799=Read"},
{"title": "Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark", "author": "Kai Zou and Ziqi Huang and Yuhao Dong and Shulin Tian and Dian Zheng and Hongbo Liu and Jingwen He and Bin Liu and Yu Qiao and Ziwei Liu", "abstract": "  Unified multimodal models aim to jointly enable visual understanding and\ngeneration, yet current benchmarks rarely examine their true integration.\nExisting evaluations either treat the two abilities in isolation or overlook\ntasks that inherently couple them. To address this gap, we present Uni-MMMU, a\ncomprehensive and discipline-aware benchmark that systematically unfolds the\nbidirectional synergy between generation and understanding across eight\nreasoning-centric domains, including science, coding, mathematics, and puzzles.\nEach task is bidirectionally coupled, demanding models to (i) leverage\nconceptual understanding to guide precise visual synthesis, or (ii) utilize\ngeneration as a cognitive scaffold for analytical reasoning. Uni-MMMU\nincorporates verifiable intermediate reasoning steps, unique ground truths, and\na reproducible scoring protocol for both textual and visual outputs. Through\nextensive evaluation of state-of-the-art unified, generation-only, and\nunderstanding-only models, we reveal substantial performance disparities and\ncross-modal dependencies, offering new insights into when and how these\nabilities reinforce one another, and establishing a reliable foundation for\nadvancing unified models.\n", "link": "http://arxiv.org/abs/2510.13759v1", "date": "2025-10-15", "relevancy": 2.2125, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5543}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5543}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uni-MMMU%3A%20A%20Massive%20Multi-discipline%20Multimodal%20Unified%20Benchmark&body=Title%3A%20Uni-MMMU%3A%20A%20Massive%20Multi-discipline%20Multimodal%20Unified%20Benchmark%0AAuthor%3A%20Kai%20Zou%20and%20Ziqi%20Huang%20and%20Yuhao%20Dong%20and%20Shulin%20Tian%20and%20Dian%20Zheng%20and%20Hongbo%20Liu%20and%20Jingwen%20He%20and%20Bin%20Liu%20and%20Yu%20Qiao%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20Unified%20multimodal%20models%20aim%20to%20jointly%20enable%20visual%20understanding%20and%0Ageneration%2C%20yet%20current%20benchmarks%20rarely%20examine%20their%20true%20integration.%0AExisting%20evaluations%20either%20treat%20the%20two%20abilities%20in%20isolation%20or%20overlook%0Atasks%20that%20inherently%20couple%20them.%20To%20address%20this%20gap%2C%20we%20present%20Uni-MMMU%2C%20a%0Acomprehensive%20and%20discipline-aware%20benchmark%20that%20systematically%20unfolds%20the%0Abidirectional%20synergy%20between%20generation%20and%20understanding%20across%20eight%0Areasoning-centric%20domains%2C%20including%20science%2C%20coding%2C%20mathematics%2C%20and%20puzzles.%0AEach%20task%20is%20bidirectionally%20coupled%2C%20demanding%20models%20to%20%28i%29%20leverage%0Aconceptual%20understanding%20to%20guide%20precise%20visual%20synthesis%2C%20or%20%28ii%29%20utilize%0Ageneration%20as%20a%20cognitive%20scaffold%20for%20analytical%20reasoning.%20Uni-MMMU%0Aincorporates%20verifiable%20intermediate%20reasoning%20steps%2C%20unique%20ground%20truths%2C%20and%0Aa%20reproducible%20scoring%20protocol%20for%20both%20textual%20and%20visual%20outputs.%20Through%0Aextensive%20evaluation%20of%20state-of-the-art%20unified%2C%20generation-only%2C%20and%0Aunderstanding-only%20models%2C%20we%20reveal%20substantial%20performance%20disparities%20and%0Across-modal%20dependencies%2C%20offering%20new%20insights%20into%20when%20and%20how%20these%0Aabilities%20reinforce%20one%20another%2C%20and%20establishing%20a%20reliable%20foundation%20for%0Aadvancing%20unified%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13759v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUni-MMMU%253A%2520A%2520Massive%2520Multi-discipline%2520Multimodal%2520Unified%2520Benchmark%26entry.906535625%3DKai%2520Zou%2520and%2520Ziqi%2520Huang%2520and%2520Yuhao%2520Dong%2520and%2520Shulin%2520Tian%2520and%2520Dian%2520Zheng%2520and%2520Hongbo%2520Liu%2520and%2520Jingwen%2520He%2520and%2520Bin%2520Liu%2520and%2520Yu%2520Qiao%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520Unified%2520multimodal%2520models%2520aim%2520to%2520jointly%2520enable%2520visual%2520understanding%2520and%250Ageneration%252C%2520yet%2520current%2520benchmarks%2520rarely%2520examine%2520their%2520true%2520integration.%250AExisting%2520evaluations%2520either%2520treat%2520the%2520two%2520abilities%2520in%2520isolation%2520or%2520overlook%250Atasks%2520that%2520inherently%2520couple%2520them.%2520To%2520address%2520this%2520gap%252C%2520we%2520present%2520Uni-MMMU%252C%2520a%250Acomprehensive%2520and%2520discipline-aware%2520benchmark%2520that%2520systematically%2520unfolds%2520the%250Abidirectional%2520synergy%2520between%2520generation%2520and%2520understanding%2520across%2520eight%250Areasoning-centric%2520domains%252C%2520including%2520science%252C%2520coding%252C%2520mathematics%252C%2520and%2520puzzles.%250AEach%2520task%2520is%2520bidirectionally%2520coupled%252C%2520demanding%2520models%2520to%2520%2528i%2529%2520leverage%250Aconceptual%2520understanding%2520to%2520guide%2520precise%2520visual%2520synthesis%252C%2520or%2520%2528ii%2529%2520utilize%250Ageneration%2520as%2520a%2520cognitive%2520scaffold%2520for%2520analytical%2520reasoning.%2520Uni-MMMU%250Aincorporates%2520verifiable%2520intermediate%2520reasoning%2520steps%252C%2520unique%2520ground%2520truths%252C%2520and%250Aa%2520reproducible%2520scoring%2520protocol%2520for%2520both%2520textual%2520and%2520visual%2520outputs.%2520Through%250Aextensive%2520evaluation%2520of%2520state-of-the-art%2520unified%252C%2520generation-only%252C%2520and%250Aunderstanding-only%2520models%252C%2520we%2520reveal%2520substantial%2520performance%2520disparities%2520and%250Across-modal%2520dependencies%252C%2520offering%2520new%2520insights%2520into%2520when%2520and%2520how%2520these%250Aabilities%2520reinforce%2520one%2520another%252C%2520and%2520establishing%2520a%2520reliable%2520foundation%2520for%250Aadvancing%2520unified%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13759v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uni-MMMU%3A%20A%20Massive%20Multi-discipline%20Multimodal%20Unified%20Benchmark&entry.906535625=Kai%20Zou%20and%20Ziqi%20Huang%20and%20Yuhao%20Dong%20and%20Shulin%20Tian%20and%20Dian%20Zheng%20and%20Hongbo%20Liu%20and%20Jingwen%20He%20and%20Bin%20Liu%20and%20Yu%20Qiao%20and%20Ziwei%20Liu&entry.1292438233=%20%20Unified%20multimodal%20models%20aim%20to%20jointly%20enable%20visual%20understanding%20and%0Ageneration%2C%20yet%20current%20benchmarks%20rarely%20examine%20their%20true%20integration.%0AExisting%20evaluations%20either%20treat%20the%20two%20abilities%20in%20isolation%20or%20overlook%0Atasks%20that%20inherently%20couple%20them.%20To%20address%20this%20gap%2C%20we%20present%20Uni-MMMU%2C%20a%0Acomprehensive%20and%20discipline-aware%20benchmark%20that%20systematically%20unfolds%20the%0Abidirectional%20synergy%20between%20generation%20and%20understanding%20across%20eight%0Areasoning-centric%20domains%2C%20including%20science%2C%20coding%2C%20mathematics%2C%20and%20puzzles.%0AEach%20task%20is%20bidirectionally%20coupled%2C%20demanding%20models%20to%20%28i%29%20leverage%0Aconceptual%20understanding%20to%20guide%20precise%20visual%20synthesis%2C%20or%20%28ii%29%20utilize%0Ageneration%20as%20a%20cognitive%20scaffold%20for%20analytical%20reasoning.%20Uni-MMMU%0Aincorporates%20verifiable%20intermediate%20reasoning%20steps%2C%20unique%20ground%20truths%2C%20and%0Aa%20reproducible%20scoring%20protocol%20for%20both%20textual%20and%20visual%20outputs.%20Through%0Aextensive%20evaluation%20of%20state-of-the-art%20unified%2C%20generation-only%2C%20and%0Aunderstanding-only%20models%2C%20we%20reveal%20substantial%20performance%20disparities%20and%0Across-modal%20dependencies%2C%20offering%20new%20insights%20into%20when%20and%20how%20these%0Aabilities%20reinforce%20one%20another%2C%20and%20establishing%20a%20reliable%20foundation%20for%0Aadvancing%20unified%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13759v1&entry.124074799=Read"},
{"title": "Training LLM Agents to Empower Humans", "author": "Evan Ellis and Vivek Myers and Jens Tuyls and Sergey Levine and Anca Dragan and Benjamin Eysenbach", "abstract": "  Assistive agents should not only take actions on behalf of a human, but also\nstep out of the way and cede control when there are important decisions to be\nmade. However, current methods for building assistive agents, whether via\nmimicking expert humans or via RL finetuning on an inferred reward, often\nencourage agents to complete tasks on their own rather than truly assisting the\nhuman attain their objectives. Additionally, these methods often require costly\nexplicit human feedback to provide a training signal. We propose a new approach\nto tuning assistive language models based on maximizing the human's\nempowerment, their ability to effect desired changes in the environment. Our\nempowerment-maximizing method, Empower, only requires offline text data,\nproviding a self-supervised method for fine-tuning language models to better\nassist humans. To study the efficacy of our approach, we conducted an 18-person\nuser study comparing our empowerment assistant with a strong baseline.\nParticipants preferred our assistant 78% of the time (p=0.015), with a 31%\nhigher acceptance rate and 38% fewer suggestions. Additionally, we introduce a\nnew environment for evaluating multi-turn code assistance using simulated\nhumans. Using this environment, we show that agents trained with Empower\nincrease the success rate of a simulated human programmer on challenging coding\nquestions by an average of 192% over an SFT baseline. With this empowerment\nobjective, we provide a framework for useful aligned AI agents at scale using\nonly offline data without the need for any additional human feedback or\nverifiable rewards.\n", "link": "http://arxiv.org/abs/2510.13709v1", "date": "2025-10-15", "relevancy": 2.2109, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5911}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5328}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20LLM%20Agents%20to%20Empower%20Humans&body=Title%3A%20Training%20LLM%20Agents%20to%20Empower%20Humans%0AAuthor%3A%20Evan%20Ellis%20and%20Vivek%20Myers%20and%20Jens%20Tuyls%20and%20Sergey%20Levine%20and%20Anca%20Dragan%20and%20Benjamin%20Eysenbach%0AAbstract%3A%20%20%20Assistive%20agents%20should%20not%20only%20take%20actions%20on%20behalf%20of%20a%20human%2C%20but%20also%0Astep%20out%20of%20the%20way%20and%20cede%20control%20when%20there%20are%20important%20decisions%20to%20be%0Amade.%20However%2C%20current%20methods%20for%20building%20assistive%20agents%2C%20whether%20via%0Amimicking%20expert%20humans%20or%20via%20RL%20finetuning%20on%20an%20inferred%20reward%2C%20often%0Aencourage%20agents%20to%20complete%20tasks%20on%20their%20own%20rather%20than%20truly%20assisting%20the%0Ahuman%20attain%20their%20objectives.%20Additionally%2C%20these%20methods%20often%20require%20costly%0Aexplicit%20human%20feedback%20to%20provide%20a%20training%20signal.%20We%20propose%20a%20new%20approach%0Ato%20tuning%20assistive%20language%20models%20based%20on%20maximizing%20the%20human%27s%0Aempowerment%2C%20their%20ability%20to%20effect%20desired%20changes%20in%20the%20environment.%20Our%0Aempowerment-maximizing%20method%2C%20Empower%2C%20only%20requires%20offline%20text%20data%2C%0Aproviding%20a%20self-supervised%20method%20for%20fine-tuning%20language%20models%20to%20better%0Aassist%20humans.%20To%20study%20the%20efficacy%20of%20our%20approach%2C%20we%20conducted%20an%2018-person%0Auser%20study%20comparing%20our%20empowerment%20assistant%20with%20a%20strong%20baseline.%0AParticipants%20preferred%20our%20assistant%2078%25%20of%20the%20time%20%28p%3D0.015%29%2C%20with%20a%2031%25%0Ahigher%20acceptance%20rate%20and%2038%25%20fewer%20suggestions.%20Additionally%2C%20we%20introduce%20a%0Anew%20environment%20for%20evaluating%20multi-turn%20code%20assistance%20using%20simulated%0Ahumans.%20Using%20this%20environment%2C%20we%20show%20that%20agents%20trained%20with%20Empower%0Aincrease%20the%20success%20rate%20of%20a%20simulated%20human%20programmer%20on%20challenging%20coding%0Aquestions%20by%20an%20average%20of%20192%25%20over%20an%20SFT%20baseline.%20With%20this%20empowerment%0Aobjective%2C%20we%20provide%20a%20framework%20for%20useful%20aligned%20AI%20agents%20at%20scale%20using%0Aonly%20offline%20data%20without%20the%20need%20for%20any%20additional%20human%20feedback%20or%0Averifiable%20rewards.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13709v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520LLM%2520Agents%2520to%2520Empower%2520Humans%26entry.906535625%3DEvan%2520Ellis%2520and%2520Vivek%2520Myers%2520and%2520Jens%2520Tuyls%2520and%2520Sergey%2520Levine%2520and%2520Anca%2520Dragan%2520and%2520Benjamin%2520Eysenbach%26entry.1292438233%3D%2520%2520Assistive%2520agents%2520should%2520not%2520only%2520take%2520actions%2520on%2520behalf%2520of%2520a%2520human%252C%2520but%2520also%250Astep%2520out%2520of%2520the%2520way%2520and%2520cede%2520control%2520when%2520there%2520are%2520important%2520decisions%2520to%2520be%250Amade.%2520However%252C%2520current%2520methods%2520for%2520building%2520assistive%2520agents%252C%2520whether%2520via%250Amimicking%2520expert%2520humans%2520or%2520via%2520RL%2520finetuning%2520on%2520an%2520inferred%2520reward%252C%2520often%250Aencourage%2520agents%2520to%2520complete%2520tasks%2520on%2520their%2520own%2520rather%2520than%2520truly%2520assisting%2520the%250Ahuman%2520attain%2520their%2520objectives.%2520Additionally%252C%2520these%2520methods%2520often%2520require%2520costly%250Aexplicit%2520human%2520feedback%2520to%2520provide%2520a%2520training%2520signal.%2520We%2520propose%2520a%2520new%2520approach%250Ato%2520tuning%2520assistive%2520language%2520models%2520based%2520on%2520maximizing%2520the%2520human%2527s%250Aempowerment%252C%2520their%2520ability%2520to%2520effect%2520desired%2520changes%2520in%2520the%2520environment.%2520Our%250Aempowerment-maximizing%2520method%252C%2520Empower%252C%2520only%2520requires%2520offline%2520text%2520data%252C%250Aproviding%2520a%2520self-supervised%2520method%2520for%2520fine-tuning%2520language%2520models%2520to%2520better%250Aassist%2520humans.%2520To%2520study%2520the%2520efficacy%2520of%2520our%2520approach%252C%2520we%2520conducted%2520an%252018-person%250Auser%2520study%2520comparing%2520our%2520empowerment%2520assistant%2520with%2520a%2520strong%2520baseline.%250AParticipants%2520preferred%2520our%2520assistant%252078%2525%2520of%2520the%2520time%2520%2528p%253D0.015%2529%252C%2520with%2520a%252031%2525%250Ahigher%2520acceptance%2520rate%2520and%252038%2525%2520fewer%2520suggestions.%2520Additionally%252C%2520we%2520introduce%2520a%250Anew%2520environment%2520for%2520evaluating%2520multi-turn%2520code%2520assistance%2520using%2520simulated%250Ahumans.%2520Using%2520this%2520environment%252C%2520we%2520show%2520that%2520agents%2520trained%2520with%2520Empower%250Aincrease%2520the%2520success%2520rate%2520of%2520a%2520simulated%2520human%2520programmer%2520on%2520challenging%2520coding%250Aquestions%2520by%2520an%2520average%2520of%2520192%2525%2520over%2520an%2520SFT%2520baseline.%2520With%2520this%2520empowerment%250Aobjective%252C%2520we%2520provide%2520a%2520framework%2520for%2520useful%2520aligned%2520AI%2520agents%2520at%2520scale%2520using%250Aonly%2520offline%2520data%2520without%2520the%2520need%2520for%2520any%2520additional%2520human%2520feedback%2520or%250Averifiable%2520rewards.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13709v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20LLM%20Agents%20to%20Empower%20Humans&entry.906535625=Evan%20Ellis%20and%20Vivek%20Myers%20and%20Jens%20Tuyls%20and%20Sergey%20Levine%20and%20Anca%20Dragan%20and%20Benjamin%20Eysenbach&entry.1292438233=%20%20Assistive%20agents%20should%20not%20only%20take%20actions%20on%20behalf%20of%20a%20human%2C%20but%20also%0Astep%20out%20of%20the%20way%20and%20cede%20control%20when%20there%20are%20important%20decisions%20to%20be%0Amade.%20However%2C%20current%20methods%20for%20building%20assistive%20agents%2C%20whether%20via%0Amimicking%20expert%20humans%20or%20via%20RL%20finetuning%20on%20an%20inferred%20reward%2C%20often%0Aencourage%20agents%20to%20complete%20tasks%20on%20their%20own%20rather%20than%20truly%20assisting%20the%0Ahuman%20attain%20their%20objectives.%20Additionally%2C%20these%20methods%20often%20require%20costly%0Aexplicit%20human%20feedback%20to%20provide%20a%20training%20signal.%20We%20propose%20a%20new%20approach%0Ato%20tuning%20assistive%20language%20models%20based%20on%20maximizing%20the%20human%27s%0Aempowerment%2C%20their%20ability%20to%20effect%20desired%20changes%20in%20the%20environment.%20Our%0Aempowerment-maximizing%20method%2C%20Empower%2C%20only%20requires%20offline%20text%20data%2C%0Aproviding%20a%20self-supervised%20method%20for%20fine-tuning%20language%20models%20to%20better%0Aassist%20humans.%20To%20study%20the%20efficacy%20of%20our%20approach%2C%20we%20conducted%20an%2018-person%0Auser%20study%20comparing%20our%20empowerment%20assistant%20with%20a%20strong%20baseline.%0AParticipants%20preferred%20our%20assistant%2078%25%20of%20the%20time%20%28p%3D0.015%29%2C%20with%20a%2031%25%0Ahigher%20acceptance%20rate%20and%2038%25%20fewer%20suggestions.%20Additionally%2C%20we%20introduce%20a%0Anew%20environment%20for%20evaluating%20multi-turn%20code%20assistance%20using%20simulated%0Ahumans.%20Using%20this%20environment%2C%20we%20show%20that%20agents%20trained%20with%20Empower%0Aincrease%20the%20success%20rate%20of%20a%20simulated%20human%20programmer%20on%20challenging%20coding%0Aquestions%20by%20an%20average%20of%20192%25%20over%20an%20SFT%20baseline.%20With%20this%20empowerment%0Aobjective%2C%20we%20provide%20a%20framework%20for%20useful%20aligned%20AI%20agents%20at%20scale%20using%0Aonly%20offline%20data%20without%20the%20need%20for%20any%20additional%20human%20feedback%20or%0Averifiable%20rewards.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13709v1&entry.124074799=Read"},
{"title": "SWIR-LightFusion: Multi-spectral Semantic Fusion of Synthetic SWIR with\n  {Thermal} IR {(LWIR/MWIR)} and RGB", "author": "Muhammad Ishfaq Hussain and Ma Van Linh and Zubia Naz and Unse Fatima and Yeongmin Ko and Moongu Jeon", "abstract": "  Enhancing scene understanding in adverse visibility conditions remains a\ncritical challenge for surveillance and autonomous navigation systems.\nConventional imaging modalities, such as RGB and thermal infrared (MWIR /\nLWIR), when fused, often struggle to deliver comprehensive scene information,\nparticularly under conditions of atmospheric interference or inadequate\nillumination. To address these limitations, Short-Wave Infrared (SWIR) imaging\nhas emerged as a promising modality due to its ability to penetrate atmospheric\ndisturbances and differentiate materials with improved clarity. However, the\nadvancement and widespread implementation of SWIR-based systems face\nsignificant hurdles, primarily due to the scarcity of publicly accessible SWIR\ndatasets. In response to this challenge, our research introduces an approach to\nsynthetically generate SWIR-like structural/contrast cues (without claiming\nspectral reproduction) images from existing LWIR data using advanced contrast\nenhancement techniques. We then propose a multimodal fusion framework\nintegrating synthetic SWIR, LWIR, and RGB modalities, employing an optimized\nencoder-decoder neural network architecture with modality-specific encoders and\na softmax-gated fusion head. Comprehensive experiments on public {RGB-LWIR\nbenchmarks (M3FD, TNO, CAMEL, MSRS, RoadScene) and an additional private real\nRGB-MWIR-SWIR dataset} demonstrate that our synthetic-SWIR-enhanced fusion\nframework improves fused-image quality (contrast, edge definition, structural\nfidelity) while maintaining real-time performance. We also add fair trimodal\nbaselines (LP, LatLRR, GFF) and cascaded trimodal variants of\nU2Fusion/SwinFusion under a unified protocol. The outcomes highlight\nsubstantial potential for real-world applications in surveillance and\nautonomous systems.\n", "link": "http://arxiv.org/abs/2510.13404v1", "date": "2025-10-15", "relevancy": 2.2106, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5631}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5548}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SWIR-LightFusion%3A%20Multi-spectral%20Semantic%20Fusion%20of%20Synthetic%20SWIR%20with%0A%20%20%7BThermal%7D%20IR%20%7B%28LWIR/MWIR%29%7D%20and%20RGB&body=Title%3A%20SWIR-LightFusion%3A%20Multi-spectral%20Semantic%20Fusion%20of%20Synthetic%20SWIR%20with%0A%20%20%7BThermal%7D%20IR%20%7B%28LWIR/MWIR%29%7D%20and%20RGB%0AAuthor%3A%20Muhammad%20Ishfaq%20Hussain%20and%20Ma%20Van%20Linh%20and%20Zubia%20Naz%20and%20Unse%20Fatima%20and%20Yeongmin%20Ko%20and%20Moongu%20Jeon%0AAbstract%3A%20%20%20Enhancing%20scene%20understanding%20in%20adverse%20visibility%20conditions%20remains%20a%0Acritical%20challenge%20for%20surveillance%20and%20autonomous%20navigation%20systems.%0AConventional%20imaging%20modalities%2C%20such%20as%20RGB%20and%20thermal%20infrared%20%28MWIR%20/%0ALWIR%29%2C%20when%20fused%2C%20often%20struggle%20to%20deliver%20comprehensive%20scene%20information%2C%0Aparticularly%20under%20conditions%20of%20atmospheric%20interference%20or%20inadequate%0Aillumination.%20To%20address%20these%20limitations%2C%20Short-Wave%20Infrared%20%28SWIR%29%20imaging%0Ahas%20emerged%20as%20a%20promising%20modality%20due%20to%20its%20ability%20to%20penetrate%20atmospheric%0Adisturbances%20and%20differentiate%20materials%20with%20improved%20clarity.%20However%2C%20the%0Aadvancement%20and%20widespread%20implementation%20of%20SWIR-based%20systems%20face%0Asignificant%20hurdles%2C%20primarily%20due%20to%20the%20scarcity%20of%20publicly%20accessible%20SWIR%0Adatasets.%20In%20response%20to%20this%20challenge%2C%20our%20research%20introduces%20an%20approach%20to%0Asynthetically%20generate%20SWIR-like%20structural/contrast%20cues%20%28without%20claiming%0Aspectral%20reproduction%29%20images%20from%20existing%20LWIR%20data%20using%20advanced%20contrast%0Aenhancement%20techniques.%20We%20then%20propose%20a%20multimodal%20fusion%20framework%0Aintegrating%20synthetic%20SWIR%2C%20LWIR%2C%20and%20RGB%20modalities%2C%20employing%20an%20optimized%0Aencoder-decoder%20neural%20network%20architecture%20with%20modality-specific%20encoders%20and%0Aa%20softmax-gated%20fusion%20head.%20Comprehensive%20experiments%20on%20public%20%7BRGB-LWIR%0Abenchmarks%20%28M3FD%2C%20TNO%2C%20CAMEL%2C%20MSRS%2C%20RoadScene%29%20and%20an%20additional%20private%20real%0ARGB-MWIR-SWIR%20dataset%7D%20demonstrate%20that%20our%20synthetic-SWIR-enhanced%20fusion%0Aframework%20improves%20fused-image%20quality%20%28contrast%2C%20edge%20definition%2C%20structural%0Afidelity%29%20while%20maintaining%20real-time%20performance.%20We%20also%20add%20fair%20trimodal%0Abaselines%20%28LP%2C%20LatLRR%2C%20GFF%29%20and%20cascaded%20trimodal%20variants%20of%0AU2Fusion/SwinFusion%20under%20a%20unified%20protocol.%20The%20outcomes%20highlight%0Asubstantial%20potential%20for%20real-world%20applications%20in%20surveillance%20and%0Aautonomous%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13404v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSWIR-LightFusion%253A%2520Multi-spectral%2520Semantic%2520Fusion%2520of%2520Synthetic%2520SWIR%2520with%250A%2520%2520%257BThermal%257D%2520IR%2520%257B%2528LWIR/MWIR%2529%257D%2520and%2520RGB%26entry.906535625%3DMuhammad%2520Ishfaq%2520Hussain%2520and%2520Ma%2520Van%2520Linh%2520and%2520Zubia%2520Naz%2520and%2520Unse%2520Fatima%2520and%2520Yeongmin%2520Ko%2520and%2520Moongu%2520Jeon%26entry.1292438233%3D%2520%2520Enhancing%2520scene%2520understanding%2520in%2520adverse%2520visibility%2520conditions%2520remains%2520a%250Acritical%2520challenge%2520for%2520surveillance%2520and%2520autonomous%2520navigation%2520systems.%250AConventional%2520imaging%2520modalities%252C%2520such%2520as%2520RGB%2520and%2520thermal%2520infrared%2520%2528MWIR%2520/%250ALWIR%2529%252C%2520when%2520fused%252C%2520often%2520struggle%2520to%2520deliver%2520comprehensive%2520scene%2520information%252C%250Aparticularly%2520under%2520conditions%2520of%2520atmospheric%2520interference%2520or%2520inadequate%250Aillumination.%2520To%2520address%2520these%2520limitations%252C%2520Short-Wave%2520Infrared%2520%2528SWIR%2529%2520imaging%250Ahas%2520emerged%2520as%2520a%2520promising%2520modality%2520due%2520to%2520its%2520ability%2520to%2520penetrate%2520atmospheric%250Adisturbances%2520and%2520differentiate%2520materials%2520with%2520improved%2520clarity.%2520However%252C%2520the%250Aadvancement%2520and%2520widespread%2520implementation%2520of%2520SWIR-based%2520systems%2520face%250Asignificant%2520hurdles%252C%2520primarily%2520due%2520to%2520the%2520scarcity%2520of%2520publicly%2520accessible%2520SWIR%250Adatasets.%2520In%2520response%2520to%2520this%2520challenge%252C%2520our%2520research%2520introduces%2520an%2520approach%2520to%250Asynthetically%2520generate%2520SWIR-like%2520structural/contrast%2520cues%2520%2528without%2520claiming%250Aspectral%2520reproduction%2529%2520images%2520from%2520existing%2520LWIR%2520data%2520using%2520advanced%2520contrast%250Aenhancement%2520techniques.%2520We%2520then%2520propose%2520a%2520multimodal%2520fusion%2520framework%250Aintegrating%2520synthetic%2520SWIR%252C%2520LWIR%252C%2520and%2520RGB%2520modalities%252C%2520employing%2520an%2520optimized%250Aencoder-decoder%2520neural%2520network%2520architecture%2520with%2520modality-specific%2520encoders%2520and%250Aa%2520softmax-gated%2520fusion%2520head.%2520Comprehensive%2520experiments%2520on%2520public%2520%257BRGB-LWIR%250Abenchmarks%2520%2528M3FD%252C%2520TNO%252C%2520CAMEL%252C%2520MSRS%252C%2520RoadScene%2529%2520and%2520an%2520additional%2520private%2520real%250ARGB-MWIR-SWIR%2520dataset%257D%2520demonstrate%2520that%2520our%2520synthetic-SWIR-enhanced%2520fusion%250Aframework%2520improves%2520fused-image%2520quality%2520%2528contrast%252C%2520edge%2520definition%252C%2520structural%250Afidelity%2529%2520while%2520maintaining%2520real-time%2520performance.%2520We%2520also%2520add%2520fair%2520trimodal%250Abaselines%2520%2528LP%252C%2520LatLRR%252C%2520GFF%2529%2520and%2520cascaded%2520trimodal%2520variants%2520of%250AU2Fusion/SwinFusion%2520under%2520a%2520unified%2520protocol.%2520The%2520outcomes%2520highlight%250Asubstantial%2520potential%2520for%2520real-world%2520applications%2520in%2520surveillance%2520and%250Aautonomous%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13404v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SWIR-LightFusion%3A%20Multi-spectral%20Semantic%20Fusion%20of%20Synthetic%20SWIR%20with%0A%20%20%7BThermal%7D%20IR%20%7B%28LWIR/MWIR%29%7D%20and%20RGB&entry.906535625=Muhammad%20Ishfaq%20Hussain%20and%20Ma%20Van%20Linh%20and%20Zubia%20Naz%20and%20Unse%20Fatima%20and%20Yeongmin%20Ko%20and%20Moongu%20Jeon&entry.1292438233=%20%20Enhancing%20scene%20understanding%20in%20adverse%20visibility%20conditions%20remains%20a%0Acritical%20challenge%20for%20surveillance%20and%20autonomous%20navigation%20systems.%0AConventional%20imaging%20modalities%2C%20such%20as%20RGB%20and%20thermal%20infrared%20%28MWIR%20/%0ALWIR%29%2C%20when%20fused%2C%20often%20struggle%20to%20deliver%20comprehensive%20scene%20information%2C%0Aparticularly%20under%20conditions%20of%20atmospheric%20interference%20or%20inadequate%0Aillumination.%20To%20address%20these%20limitations%2C%20Short-Wave%20Infrared%20%28SWIR%29%20imaging%0Ahas%20emerged%20as%20a%20promising%20modality%20due%20to%20its%20ability%20to%20penetrate%20atmospheric%0Adisturbances%20and%20differentiate%20materials%20with%20improved%20clarity.%20However%2C%20the%0Aadvancement%20and%20widespread%20implementation%20of%20SWIR-based%20systems%20face%0Asignificant%20hurdles%2C%20primarily%20due%20to%20the%20scarcity%20of%20publicly%20accessible%20SWIR%0Adatasets.%20In%20response%20to%20this%20challenge%2C%20our%20research%20introduces%20an%20approach%20to%0Asynthetically%20generate%20SWIR-like%20structural/contrast%20cues%20%28without%20claiming%0Aspectral%20reproduction%29%20images%20from%20existing%20LWIR%20data%20using%20advanced%20contrast%0Aenhancement%20techniques.%20We%20then%20propose%20a%20multimodal%20fusion%20framework%0Aintegrating%20synthetic%20SWIR%2C%20LWIR%2C%20and%20RGB%20modalities%2C%20employing%20an%20optimized%0Aencoder-decoder%20neural%20network%20architecture%20with%20modality-specific%20encoders%20and%0Aa%20softmax-gated%20fusion%20head.%20Comprehensive%20experiments%20on%20public%20%7BRGB-LWIR%0Abenchmarks%20%28M3FD%2C%20TNO%2C%20CAMEL%2C%20MSRS%2C%20RoadScene%29%20and%20an%20additional%20private%20real%0ARGB-MWIR-SWIR%20dataset%7D%20demonstrate%20that%20our%20synthetic-SWIR-enhanced%20fusion%0Aframework%20improves%20fused-image%20quality%20%28contrast%2C%20edge%20definition%2C%20structural%0Afidelity%29%20while%20maintaining%20real-time%20performance.%20We%20also%20add%20fair%20trimodal%0Abaselines%20%28LP%2C%20LatLRR%2C%20GFF%29%20and%20cascaded%20trimodal%20variants%20of%0AU2Fusion/SwinFusion%20under%20a%20unified%20protocol.%20The%20outcomes%20highlight%0Asubstantial%20potential%20for%20real-world%20applications%20in%20surveillance%20and%0Aautonomous%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13404v1&entry.124074799=Read"},
{"title": "Progressive multi-fidelity learning for physical system predictions", "author": "Paolo Conti and Mengwu Guo and Attilio Frangi and Andrea Manzoni", "abstract": "  Highly accurate datasets from numerical or physical experiments are often\nexpensive and time-consuming to acquire, posing a significant challenge for\napplications that require precise evaluations, potentially across multiple\nscenarios and in real-time. Even building sufficiently accurate surrogate\nmodels can be extremely challenging with limited high-fidelity data.\nConversely, less expensive, low-fidelity data can be computed more easily and\nencompass a broader range of scenarios. By leveraging multi-fidelity\ninformation, prediction capabilities of surrogates can be improved. However, in\npractical situations, data may be different in types, come from sources of\ndifferent modalities, and not be concurrently available, further complicating\nthe modeling process. To address these challenges, we introduce a progressive\nmulti-fidelity surrogate model. This model can sequentially incorporate diverse\ndata types using tailored encoders. Multi-fidelity regression from the encoded\ninputs to the target quantities of interest is then performed using neural\nnetworks. Input information progressively flows from lower to higher fidelity\nlevels through two sets of connections: concatenations among all the encoded\ninputs, and additive connections among the final outputs. This dual connection\nsystem enables the model to exploit correlations among different datasets while\nensuring that each level makes an additive correction to the previous level\nwithout altering it. This approach prevents performance degradation as new\ninput data are integrated into the model and automatically adapts predictions\nbased on the available inputs. We demonstrate the effectiveness of the approach\non numerical benchmarks and a real-world case study, showing that it reliably\nintegrates multi-modal data and provides accurate predictions, maintaining\nperformance when generalizing across time and parameter variations.\n", "link": "http://arxiv.org/abs/2510.13762v1", "date": "2025-10-15", "relevancy": 2.2063, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6337}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5433}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Progressive%20multi-fidelity%20learning%20for%20physical%20system%20predictions&body=Title%3A%20Progressive%20multi-fidelity%20learning%20for%20physical%20system%20predictions%0AAuthor%3A%20Paolo%20Conti%20and%20Mengwu%20Guo%20and%20Attilio%20Frangi%20and%20Andrea%20Manzoni%0AAbstract%3A%20%20%20Highly%20accurate%20datasets%20from%20numerical%20or%20physical%20experiments%20are%20often%0Aexpensive%20and%20time-consuming%20to%20acquire%2C%20posing%20a%20significant%20challenge%20for%0Aapplications%20that%20require%20precise%20evaluations%2C%20potentially%20across%20multiple%0Ascenarios%20and%20in%20real-time.%20Even%20building%20sufficiently%20accurate%20surrogate%0Amodels%20can%20be%20extremely%20challenging%20with%20limited%20high-fidelity%20data.%0AConversely%2C%20less%20expensive%2C%20low-fidelity%20data%20can%20be%20computed%20more%20easily%20and%0Aencompass%20a%20broader%20range%20of%20scenarios.%20By%20leveraging%20multi-fidelity%0Ainformation%2C%20prediction%20capabilities%20of%20surrogates%20can%20be%20improved.%20However%2C%20in%0Apractical%20situations%2C%20data%20may%20be%20different%20in%20types%2C%20come%20from%20sources%20of%0Adifferent%20modalities%2C%20and%20not%20be%20concurrently%20available%2C%20further%20complicating%0Athe%20modeling%20process.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20progressive%0Amulti-fidelity%20surrogate%20model.%20This%20model%20can%20sequentially%20incorporate%20diverse%0Adata%20types%20using%20tailored%20encoders.%20Multi-fidelity%20regression%20from%20the%20encoded%0Ainputs%20to%20the%20target%20quantities%20of%20interest%20is%20then%20performed%20using%20neural%0Anetworks.%20Input%20information%20progressively%20flows%20from%20lower%20to%20higher%20fidelity%0Alevels%20through%20two%20sets%20of%20connections%3A%20concatenations%20among%20all%20the%20encoded%0Ainputs%2C%20and%20additive%20connections%20among%20the%20final%20outputs.%20This%20dual%20connection%0Asystem%20enables%20the%20model%20to%20exploit%20correlations%20among%20different%20datasets%20while%0Aensuring%20that%20each%20level%20makes%20an%20additive%20correction%20to%20the%20previous%20level%0Awithout%20altering%20it.%20This%20approach%20prevents%20performance%20degradation%20as%20new%0Ainput%20data%20are%20integrated%20into%20the%20model%20and%20automatically%20adapts%20predictions%0Abased%20on%20the%20available%20inputs.%20We%20demonstrate%20the%20effectiveness%20of%20the%20approach%0Aon%20numerical%20benchmarks%20and%20a%20real-world%20case%20study%2C%20showing%20that%20it%20reliably%0Aintegrates%20multi-modal%20data%20and%20provides%20accurate%20predictions%2C%20maintaining%0Aperformance%20when%20generalizing%20across%20time%20and%20parameter%20variations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13762v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProgressive%2520multi-fidelity%2520learning%2520for%2520physical%2520system%2520predictions%26entry.906535625%3DPaolo%2520Conti%2520and%2520Mengwu%2520Guo%2520and%2520Attilio%2520Frangi%2520and%2520Andrea%2520Manzoni%26entry.1292438233%3D%2520%2520Highly%2520accurate%2520datasets%2520from%2520numerical%2520or%2520physical%2520experiments%2520are%2520often%250Aexpensive%2520and%2520time-consuming%2520to%2520acquire%252C%2520posing%2520a%2520significant%2520challenge%2520for%250Aapplications%2520that%2520require%2520precise%2520evaluations%252C%2520potentially%2520across%2520multiple%250Ascenarios%2520and%2520in%2520real-time.%2520Even%2520building%2520sufficiently%2520accurate%2520surrogate%250Amodels%2520can%2520be%2520extremely%2520challenging%2520with%2520limited%2520high-fidelity%2520data.%250AConversely%252C%2520less%2520expensive%252C%2520low-fidelity%2520data%2520can%2520be%2520computed%2520more%2520easily%2520and%250Aencompass%2520a%2520broader%2520range%2520of%2520scenarios.%2520By%2520leveraging%2520multi-fidelity%250Ainformation%252C%2520prediction%2520capabilities%2520of%2520surrogates%2520can%2520be%2520improved.%2520However%252C%2520in%250Apractical%2520situations%252C%2520data%2520may%2520be%2520different%2520in%2520types%252C%2520come%2520from%2520sources%2520of%250Adifferent%2520modalities%252C%2520and%2520not%2520be%2520concurrently%2520available%252C%2520further%2520complicating%250Athe%2520modeling%2520process.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520a%2520progressive%250Amulti-fidelity%2520surrogate%2520model.%2520This%2520model%2520can%2520sequentially%2520incorporate%2520diverse%250Adata%2520types%2520using%2520tailored%2520encoders.%2520Multi-fidelity%2520regression%2520from%2520the%2520encoded%250Ainputs%2520to%2520the%2520target%2520quantities%2520of%2520interest%2520is%2520then%2520performed%2520using%2520neural%250Anetworks.%2520Input%2520information%2520progressively%2520flows%2520from%2520lower%2520to%2520higher%2520fidelity%250Alevels%2520through%2520two%2520sets%2520of%2520connections%253A%2520concatenations%2520among%2520all%2520the%2520encoded%250Ainputs%252C%2520and%2520additive%2520connections%2520among%2520the%2520final%2520outputs.%2520This%2520dual%2520connection%250Asystem%2520enables%2520the%2520model%2520to%2520exploit%2520correlations%2520among%2520different%2520datasets%2520while%250Aensuring%2520that%2520each%2520level%2520makes%2520an%2520additive%2520correction%2520to%2520the%2520previous%2520level%250Awithout%2520altering%2520it.%2520This%2520approach%2520prevents%2520performance%2520degradation%2520as%2520new%250Ainput%2520data%2520are%2520integrated%2520into%2520the%2520model%2520and%2520automatically%2520adapts%2520predictions%250Abased%2520on%2520the%2520available%2520inputs.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520approach%250Aon%2520numerical%2520benchmarks%2520and%2520a%2520real-world%2520case%2520study%252C%2520showing%2520that%2520it%2520reliably%250Aintegrates%2520multi-modal%2520data%2520and%2520provides%2520accurate%2520predictions%252C%2520maintaining%250Aperformance%2520when%2520generalizing%2520across%2520time%2520and%2520parameter%2520variations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13762v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Progressive%20multi-fidelity%20learning%20for%20physical%20system%20predictions&entry.906535625=Paolo%20Conti%20and%20Mengwu%20Guo%20and%20Attilio%20Frangi%20and%20Andrea%20Manzoni&entry.1292438233=%20%20Highly%20accurate%20datasets%20from%20numerical%20or%20physical%20experiments%20are%20often%0Aexpensive%20and%20time-consuming%20to%20acquire%2C%20posing%20a%20significant%20challenge%20for%0Aapplications%20that%20require%20precise%20evaluations%2C%20potentially%20across%20multiple%0Ascenarios%20and%20in%20real-time.%20Even%20building%20sufficiently%20accurate%20surrogate%0Amodels%20can%20be%20extremely%20challenging%20with%20limited%20high-fidelity%20data.%0AConversely%2C%20less%20expensive%2C%20low-fidelity%20data%20can%20be%20computed%20more%20easily%20and%0Aencompass%20a%20broader%20range%20of%20scenarios.%20By%20leveraging%20multi-fidelity%0Ainformation%2C%20prediction%20capabilities%20of%20surrogates%20can%20be%20improved.%20However%2C%20in%0Apractical%20situations%2C%20data%20may%20be%20different%20in%20types%2C%20come%20from%20sources%20of%0Adifferent%20modalities%2C%20and%20not%20be%20concurrently%20available%2C%20further%20complicating%0Athe%20modeling%20process.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20progressive%0Amulti-fidelity%20surrogate%20model.%20This%20model%20can%20sequentially%20incorporate%20diverse%0Adata%20types%20using%20tailored%20encoders.%20Multi-fidelity%20regression%20from%20the%20encoded%0Ainputs%20to%20the%20target%20quantities%20of%20interest%20is%20then%20performed%20using%20neural%0Anetworks.%20Input%20information%20progressively%20flows%20from%20lower%20to%20higher%20fidelity%0Alevels%20through%20two%20sets%20of%20connections%3A%20concatenations%20among%20all%20the%20encoded%0Ainputs%2C%20and%20additive%20connections%20among%20the%20final%20outputs.%20This%20dual%20connection%0Asystem%20enables%20the%20model%20to%20exploit%20correlations%20among%20different%20datasets%20while%0Aensuring%20that%20each%20level%20makes%20an%20additive%20correction%20to%20the%20previous%20level%0Awithout%20altering%20it.%20This%20approach%20prevents%20performance%20degradation%20as%20new%0Ainput%20data%20are%20integrated%20into%20the%20model%20and%20automatically%20adapts%20predictions%0Abased%20on%20the%20available%20inputs.%20We%20demonstrate%20the%20effectiveness%20of%20the%20approach%0Aon%20numerical%20benchmarks%20and%20a%20real-world%20case%20study%2C%20showing%20that%20it%20reliably%0Aintegrates%20multi-modal%20data%20and%20provides%20accurate%20predictions%2C%20maintaining%0Aperformance%20when%20generalizing%20across%20time%20and%20parameter%20variations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13762v1&entry.124074799=Read"},
{"title": "A Modular Object Detection System for Humanoid Robots Using YOLO", "author": "Nicolas Pottier and Meng Cheng Lau", "abstract": "  Within the field of robotics, computer vision remains a significant barrier\nto progress, with many tasks hindered by inefficient vision systems. This\nresearch proposes a generalized vision module leveraging YOLOv9, a\nstate-of-the-art framework optimized for computationally constrained\nenvironments like robots. The model is trained on a dataset tailored to the\nFIRA robotics Hurocup. A new vision module is implemented in ROS1 using a\nvirtual environment to enable YOLO compatibility. Performance is evaluated\nusing metrics such as frames per second (FPS) and Mean Average Precision (mAP).\nPerformance is then compared to the existing geometric framework in static and\ndynamic contexts. The YOLO model achieved comparable precision at a higher\ncomputational cost then the geometric model, while providing improved\nrobustness.\n", "link": "http://arxiv.org/abs/2510.13625v1", "date": "2025-10-15", "relevancy": 2.2057, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5859}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5483}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5407}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Modular%20Object%20Detection%20System%20for%20Humanoid%20Robots%20Using%20YOLO&body=Title%3A%20A%20Modular%20Object%20Detection%20System%20for%20Humanoid%20Robots%20Using%20YOLO%0AAuthor%3A%20Nicolas%20Pottier%20and%20Meng%20Cheng%20Lau%0AAbstract%3A%20%20%20Within%20the%20field%20of%20robotics%2C%20computer%20vision%20remains%20a%20significant%20barrier%0Ato%20progress%2C%20with%20many%20tasks%20hindered%20by%20inefficient%20vision%20systems.%20This%0Aresearch%20proposes%20a%20generalized%20vision%20module%20leveraging%20YOLOv9%2C%20a%0Astate-of-the-art%20framework%20optimized%20for%20computationally%20constrained%0Aenvironments%20like%20robots.%20The%20model%20is%20trained%20on%20a%20dataset%20tailored%20to%20the%0AFIRA%20robotics%20Hurocup.%20A%20new%20vision%20module%20is%20implemented%20in%20ROS1%20using%20a%0Avirtual%20environment%20to%20enable%20YOLO%20compatibility.%20Performance%20is%20evaluated%0Ausing%20metrics%20such%20as%20frames%20per%20second%20%28FPS%29%20and%20Mean%20Average%20Precision%20%28mAP%29.%0APerformance%20is%20then%20compared%20to%20the%20existing%20geometric%20framework%20in%20static%20and%0Adynamic%20contexts.%20The%20YOLO%20model%20achieved%20comparable%20precision%20at%20a%20higher%0Acomputational%20cost%20then%20the%20geometric%20model%2C%20while%20providing%20improved%0Arobustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13625v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Modular%2520Object%2520Detection%2520System%2520for%2520Humanoid%2520Robots%2520Using%2520YOLO%26entry.906535625%3DNicolas%2520Pottier%2520and%2520Meng%2520Cheng%2520Lau%26entry.1292438233%3D%2520%2520Within%2520the%2520field%2520of%2520robotics%252C%2520computer%2520vision%2520remains%2520a%2520significant%2520barrier%250Ato%2520progress%252C%2520with%2520many%2520tasks%2520hindered%2520by%2520inefficient%2520vision%2520systems.%2520This%250Aresearch%2520proposes%2520a%2520generalized%2520vision%2520module%2520leveraging%2520YOLOv9%252C%2520a%250Astate-of-the-art%2520framework%2520optimized%2520for%2520computationally%2520constrained%250Aenvironments%2520like%2520robots.%2520The%2520model%2520is%2520trained%2520on%2520a%2520dataset%2520tailored%2520to%2520the%250AFIRA%2520robotics%2520Hurocup.%2520A%2520new%2520vision%2520module%2520is%2520implemented%2520in%2520ROS1%2520using%2520a%250Avirtual%2520environment%2520to%2520enable%2520YOLO%2520compatibility.%2520Performance%2520is%2520evaluated%250Ausing%2520metrics%2520such%2520as%2520frames%2520per%2520second%2520%2528FPS%2529%2520and%2520Mean%2520Average%2520Precision%2520%2528mAP%2529.%250APerformance%2520is%2520then%2520compared%2520to%2520the%2520existing%2520geometric%2520framework%2520in%2520static%2520and%250Adynamic%2520contexts.%2520The%2520YOLO%2520model%2520achieved%2520comparable%2520precision%2520at%2520a%2520higher%250Acomputational%2520cost%2520then%2520the%2520geometric%2520model%252C%2520while%2520providing%2520improved%250Arobustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13625v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Modular%20Object%20Detection%20System%20for%20Humanoid%20Robots%20Using%20YOLO&entry.906535625=Nicolas%20Pottier%20and%20Meng%20Cheng%20Lau&entry.1292438233=%20%20Within%20the%20field%20of%20robotics%2C%20computer%20vision%20remains%20a%20significant%20barrier%0Ato%20progress%2C%20with%20many%20tasks%20hindered%20by%20inefficient%20vision%20systems.%20This%0Aresearch%20proposes%20a%20generalized%20vision%20module%20leveraging%20YOLOv9%2C%20a%0Astate-of-the-art%20framework%20optimized%20for%20computationally%20constrained%0Aenvironments%20like%20robots.%20The%20model%20is%20trained%20on%20a%20dataset%20tailored%20to%20the%0AFIRA%20robotics%20Hurocup.%20A%20new%20vision%20module%20is%20implemented%20in%20ROS1%20using%20a%0Avirtual%20environment%20to%20enable%20YOLO%20compatibility.%20Performance%20is%20evaluated%0Ausing%20metrics%20such%20as%20frames%20per%20second%20%28FPS%29%20and%20Mean%20Average%20Precision%20%28mAP%29.%0APerformance%20is%20then%20compared%20to%20the%20existing%20geometric%20framework%20in%20static%20and%0Adynamic%20contexts.%20The%20YOLO%20model%20achieved%20comparable%20precision%20at%20a%20higher%0Acomputational%20cost%20then%20the%20geometric%20model%2C%20while%20providing%20improved%0Arobustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13625v1&entry.124074799=Read"},
{"title": "Development of an Intuitive GUI for Non-Expert Teleoperation of Humanoid\n  Robots", "author": "Austin Barret and Meng Cheng Lau", "abstract": "  The operation of humanoid robotics is an essential field of research with\nmany practical and competitive applications. Many of these systems, however, do\nnot invest heavily in developing a non-expert-centered graphical user interface\n(GUI) for operation. The focus of this research is to develop a scalable GUI\nthat is tailored to be simple and intuitive so non-expert operators can control\nthe robot through a FIRA-regulated obstacle course. Using common practices from\nuser interface development (UI) and understanding concepts described in\nhuman-robot interaction (HRI) and other related concepts, we will develop a new\ninterface with the goal of a non-expert teleoperation system.\n", "link": "http://arxiv.org/abs/2510.13594v1", "date": "2025-10-15", "relevancy": 2.1864, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5948}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5817}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4922}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Development%20of%20an%20Intuitive%20GUI%20for%20Non-Expert%20Teleoperation%20of%20Humanoid%0A%20%20Robots&body=Title%3A%20Development%20of%20an%20Intuitive%20GUI%20for%20Non-Expert%20Teleoperation%20of%20Humanoid%0A%20%20Robots%0AAuthor%3A%20Austin%20Barret%20and%20Meng%20Cheng%20Lau%0AAbstract%3A%20%20%20The%20operation%20of%20humanoid%20robotics%20is%20an%20essential%20field%20of%20research%20with%0Amany%20practical%20and%20competitive%20applications.%20Many%20of%20these%20systems%2C%20however%2C%20do%0Anot%20invest%20heavily%20in%20developing%20a%20non-expert-centered%20graphical%20user%20interface%0A%28GUI%29%20for%20operation.%20The%20focus%20of%20this%20research%20is%20to%20develop%20a%20scalable%20GUI%0Athat%20is%20tailored%20to%20be%20simple%20and%20intuitive%20so%20non-expert%20operators%20can%20control%0Athe%20robot%20through%20a%20FIRA-regulated%20obstacle%20course.%20Using%20common%20practices%20from%0Auser%20interface%20development%20%28UI%29%20and%20understanding%20concepts%20described%20in%0Ahuman-robot%20interaction%20%28HRI%29%20and%20other%20related%20concepts%2C%20we%20will%20develop%20a%20new%0Ainterface%20with%20the%20goal%20of%20a%20non-expert%20teleoperation%20system.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13594v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDevelopment%2520of%2520an%2520Intuitive%2520GUI%2520for%2520Non-Expert%2520Teleoperation%2520of%2520Humanoid%250A%2520%2520Robots%26entry.906535625%3DAustin%2520Barret%2520and%2520Meng%2520Cheng%2520Lau%26entry.1292438233%3D%2520%2520The%2520operation%2520of%2520humanoid%2520robotics%2520is%2520an%2520essential%2520field%2520of%2520research%2520with%250Amany%2520practical%2520and%2520competitive%2520applications.%2520Many%2520of%2520these%2520systems%252C%2520however%252C%2520do%250Anot%2520invest%2520heavily%2520in%2520developing%2520a%2520non-expert-centered%2520graphical%2520user%2520interface%250A%2528GUI%2529%2520for%2520operation.%2520The%2520focus%2520of%2520this%2520research%2520is%2520to%2520develop%2520a%2520scalable%2520GUI%250Athat%2520is%2520tailored%2520to%2520be%2520simple%2520and%2520intuitive%2520so%2520non-expert%2520operators%2520can%2520control%250Athe%2520robot%2520through%2520a%2520FIRA-regulated%2520obstacle%2520course.%2520Using%2520common%2520practices%2520from%250Auser%2520interface%2520development%2520%2528UI%2529%2520and%2520understanding%2520concepts%2520described%2520in%250Ahuman-robot%2520interaction%2520%2528HRI%2529%2520and%2520other%2520related%2520concepts%252C%2520we%2520will%2520develop%2520a%2520new%250Ainterface%2520with%2520the%2520goal%2520of%2520a%2520non-expert%2520teleoperation%2520system.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13594v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Development%20of%20an%20Intuitive%20GUI%20for%20Non-Expert%20Teleoperation%20of%20Humanoid%0A%20%20Robots&entry.906535625=Austin%20Barret%20and%20Meng%20Cheng%20Lau&entry.1292438233=%20%20The%20operation%20of%20humanoid%20robotics%20is%20an%20essential%20field%20of%20research%20with%0Amany%20practical%20and%20competitive%20applications.%20Many%20of%20these%20systems%2C%20however%2C%20do%0Anot%20invest%20heavily%20in%20developing%20a%20non-expert-centered%20graphical%20user%20interface%0A%28GUI%29%20for%20operation.%20The%20focus%20of%20this%20research%20is%20to%20develop%20a%20scalable%20GUI%0Athat%20is%20tailored%20to%20be%20simple%20and%20intuitive%20so%20non-expert%20operators%20can%20control%0Athe%20robot%20through%20a%20FIRA-regulated%20obstacle%20course.%20Using%20common%20practices%20from%0Auser%20interface%20development%20%28UI%29%20and%20understanding%20concepts%20described%20in%0Ahuman-robot%20interaction%20%28HRI%29%20and%20other%20related%20concepts%2C%20we%20will%20develop%20a%20new%0Ainterface%20with%20the%20goal%20of%20a%20non-expert%20teleoperation%20system.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13594v1&entry.124074799=Read"},
{"title": "CoDS: Enhancing Collaborative Perception in Heterogeneous Scenarios via\n  Domain Separation", "author": "Yushan Han and Hui Zhang and Honglei Zhang and Chuntao Ding and Yuanzhouhan Cao and Yidong Li", "abstract": "  Collaborative perception has been proven to improve individual perception in\nautonomous driving through multi-agent interaction. Nevertheless, most methods\noften assume identical encoders for all agents, which does not hold true when\nthese models are deployed in real-world applications. To realize collaborative\nperception in actual heterogeneous scenarios, existing methods usually align\nneighbor features to those of the ego vehicle, which is vulnerable to noise\nfrom domain gaps and thus fails to address feature discrepancies effectively.\nMoreover, they adopt transformer-based modules for domain adaptation, which\ncauses the model inference inefficiency on mobile devices. To tackle these\nissues, we propose CoDS, a Collaborative perception method that leverages\nDomain Separation to address feature discrepancies in heterogeneous scenarios.\nThe CoDS employs two feature alignment modules, i.e., Lightweight\nSpatial-Channel Resizer (LSCR) and Distribution Alignment via Domain Separation\n(DADS). Besides, it utilizes the Domain Alignment Mutual Information (DAMI)\nloss to ensure effective feature alignment. Specifically, the LSCR aligns the\nneighbor feature across spatial and channel dimensions using a lightweight\nconvolutional layer. Subsequently, the DADS mitigates feature distribution\ndiscrepancy with encoder-specific and encoder-agnostic domain separation\nmodules. The former removes domain-dependent information and the latter\ncaptures task-related information. During training, the DAMI loss maximizes the\nmutual information between aligned heterogeneous features to enhance the domain\nseparation process. The CoDS employs a fully convolutional architecture, which\nensures high inference efficiency. Extensive experiments demonstrate that the\nCoDS effectively mitigates feature discrepancies in heterogeneous scenarios and\nachieves a trade-off between detection accuracy and inference efficiency.\n", "link": "http://arxiv.org/abs/2510.13432v1", "date": "2025-10-15", "relevancy": 2.1836, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5508}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5449}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoDS%3A%20Enhancing%20Collaborative%20Perception%20in%20Heterogeneous%20Scenarios%20via%0A%20%20Domain%20Separation&body=Title%3A%20CoDS%3A%20Enhancing%20Collaborative%20Perception%20in%20Heterogeneous%20Scenarios%20via%0A%20%20Domain%20Separation%0AAuthor%3A%20Yushan%20Han%20and%20Hui%20Zhang%20and%20Honglei%20Zhang%20and%20Chuntao%20Ding%20and%20Yuanzhouhan%20Cao%20and%20Yidong%20Li%0AAbstract%3A%20%20%20Collaborative%20perception%20has%20been%20proven%20to%20improve%20individual%20perception%20in%0Aautonomous%20driving%20through%20multi-agent%20interaction.%20Nevertheless%2C%20most%20methods%0Aoften%20assume%20identical%20encoders%20for%20all%20agents%2C%20which%20does%20not%20hold%20true%20when%0Athese%20models%20are%20deployed%20in%20real-world%20applications.%20To%20realize%20collaborative%0Aperception%20in%20actual%20heterogeneous%20scenarios%2C%20existing%20methods%20usually%20align%0Aneighbor%20features%20to%20those%20of%20the%20ego%20vehicle%2C%20which%20is%20vulnerable%20to%20noise%0Afrom%20domain%20gaps%20and%20thus%20fails%20to%20address%20feature%20discrepancies%20effectively.%0AMoreover%2C%20they%20adopt%20transformer-based%20modules%20for%20domain%20adaptation%2C%20which%0Acauses%20the%20model%20inference%20inefficiency%20on%20mobile%20devices.%20To%20tackle%20these%0Aissues%2C%20we%20propose%20CoDS%2C%20a%20Collaborative%20perception%20method%20that%20leverages%0ADomain%20Separation%20to%20address%20feature%20discrepancies%20in%20heterogeneous%20scenarios.%0AThe%20CoDS%20employs%20two%20feature%20alignment%20modules%2C%20i.e.%2C%20Lightweight%0ASpatial-Channel%20Resizer%20%28LSCR%29%20and%20Distribution%20Alignment%20via%20Domain%20Separation%0A%28DADS%29.%20Besides%2C%20it%20utilizes%20the%20Domain%20Alignment%20Mutual%20Information%20%28DAMI%29%0Aloss%20to%20ensure%20effective%20feature%20alignment.%20Specifically%2C%20the%20LSCR%20aligns%20the%0Aneighbor%20feature%20across%20spatial%20and%20channel%20dimensions%20using%20a%20lightweight%0Aconvolutional%20layer.%20Subsequently%2C%20the%20DADS%20mitigates%20feature%20distribution%0Adiscrepancy%20with%20encoder-specific%20and%20encoder-agnostic%20domain%20separation%0Amodules.%20The%20former%20removes%20domain-dependent%20information%20and%20the%20latter%0Acaptures%20task-related%20information.%20During%20training%2C%20the%20DAMI%20loss%20maximizes%20the%0Amutual%20information%20between%20aligned%20heterogeneous%20features%20to%20enhance%20the%20domain%0Aseparation%20process.%20The%20CoDS%20employs%20a%20fully%20convolutional%20architecture%2C%20which%0Aensures%20high%20inference%20efficiency.%20Extensive%20experiments%20demonstrate%20that%20the%0ACoDS%20effectively%20mitigates%20feature%20discrepancies%20in%20heterogeneous%20scenarios%20and%0Aachieves%20a%20trade-off%20between%20detection%20accuracy%20and%20inference%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13432v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoDS%253A%2520Enhancing%2520Collaborative%2520Perception%2520in%2520Heterogeneous%2520Scenarios%2520via%250A%2520%2520Domain%2520Separation%26entry.906535625%3DYushan%2520Han%2520and%2520Hui%2520Zhang%2520and%2520Honglei%2520Zhang%2520and%2520Chuntao%2520Ding%2520and%2520Yuanzhouhan%2520Cao%2520and%2520Yidong%2520Li%26entry.1292438233%3D%2520%2520Collaborative%2520perception%2520has%2520been%2520proven%2520to%2520improve%2520individual%2520perception%2520in%250Aautonomous%2520driving%2520through%2520multi-agent%2520interaction.%2520Nevertheless%252C%2520most%2520methods%250Aoften%2520assume%2520identical%2520encoders%2520for%2520all%2520agents%252C%2520which%2520does%2520not%2520hold%2520true%2520when%250Athese%2520models%2520are%2520deployed%2520in%2520real-world%2520applications.%2520To%2520realize%2520collaborative%250Aperception%2520in%2520actual%2520heterogeneous%2520scenarios%252C%2520existing%2520methods%2520usually%2520align%250Aneighbor%2520features%2520to%2520those%2520of%2520the%2520ego%2520vehicle%252C%2520which%2520is%2520vulnerable%2520to%2520noise%250Afrom%2520domain%2520gaps%2520and%2520thus%2520fails%2520to%2520address%2520feature%2520discrepancies%2520effectively.%250AMoreover%252C%2520they%2520adopt%2520transformer-based%2520modules%2520for%2520domain%2520adaptation%252C%2520which%250Acauses%2520the%2520model%2520inference%2520inefficiency%2520on%2520mobile%2520devices.%2520To%2520tackle%2520these%250Aissues%252C%2520we%2520propose%2520CoDS%252C%2520a%2520Collaborative%2520perception%2520method%2520that%2520leverages%250ADomain%2520Separation%2520to%2520address%2520feature%2520discrepancies%2520in%2520heterogeneous%2520scenarios.%250AThe%2520CoDS%2520employs%2520two%2520feature%2520alignment%2520modules%252C%2520i.e.%252C%2520Lightweight%250ASpatial-Channel%2520Resizer%2520%2528LSCR%2529%2520and%2520Distribution%2520Alignment%2520via%2520Domain%2520Separation%250A%2528DADS%2529.%2520Besides%252C%2520it%2520utilizes%2520the%2520Domain%2520Alignment%2520Mutual%2520Information%2520%2528DAMI%2529%250Aloss%2520to%2520ensure%2520effective%2520feature%2520alignment.%2520Specifically%252C%2520the%2520LSCR%2520aligns%2520the%250Aneighbor%2520feature%2520across%2520spatial%2520and%2520channel%2520dimensions%2520using%2520a%2520lightweight%250Aconvolutional%2520layer.%2520Subsequently%252C%2520the%2520DADS%2520mitigates%2520feature%2520distribution%250Adiscrepancy%2520with%2520encoder-specific%2520and%2520encoder-agnostic%2520domain%2520separation%250Amodules.%2520The%2520former%2520removes%2520domain-dependent%2520information%2520and%2520the%2520latter%250Acaptures%2520task-related%2520information.%2520During%2520training%252C%2520the%2520DAMI%2520loss%2520maximizes%2520the%250Amutual%2520information%2520between%2520aligned%2520heterogeneous%2520features%2520to%2520enhance%2520the%2520domain%250Aseparation%2520process.%2520The%2520CoDS%2520employs%2520a%2520fully%2520convolutional%2520architecture%252C%2520which%250Aensures%2520high%2520inference%2520efficiency.%2520Extensive%2520experiments%2520demonstrate%2520that%2520the%250ACoDS%2520effectively%2520mitigates%2520feature%2520discrepancies%2520in%2520heterogeneous%2520scenarios%2520and%250Aachieves%2520a%2520trade-off%2520between%2520detection%2520accuracy%2520and%2520inference%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13432v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoDS%3A%20Enhancing%20Collaborative%20Perception%20in%20Heterogeneous%20Scenarios%20via%0A%20%20Domain%20Separation&entry.906535625=Yushan%20Han%20and%20Hui%20Zhang%20and%20Honglei%20Zhang%20and%20Chuntao%20Ding%20and%20Yuanzhouhan%20Cao%20and%20Yidong%20Li&entry.1292438233=%20%20Collaborative%20perception%20has%20been%20proven%20to%20improve%20individual%20perception%20in%0Aautonomous%20driving%20through%20multi-agent%20interaction.%20Nevertheless%2C%20most%20methods%0Aoften%20assume%20identical%20encoders%20for%20all%20agents%2C%20which%20does%20not%20hold%20true%20when%0Athese%20models%20are%20deployed%20in%20real-world%20applications.%20To%20realize%20collaborative%0Aperception%20in%20actual%20heterogeneous%20scenarios%2C%20existing%20methods%20usually%20align%0Aneighbor%20features%20to%20those%20of%20the%20ego%20vehicle%2C%20which%20is%20vulnerable%20to%20noise%0Afrom%20domain%20gaps%20and%20thus%20fails%20to%20address%20feature%20discrepancies%20effectively.%0AMoreover%2C%20they%20adopt%20transformer-based%20modules%20for%20domain%20adaptation%2C%20which%0Acauses%20the%20model%20inference%20inefficiency%20on%20mobile%20devices.%20To%20tackle%20these%0Aissues%2C%20we%20propose%20CoDS%2C%20a%20Collaborative%20perception%20method%20that%20leverages%0ADomain%20Separation%20to%20address%20feature%20discrepancies%20in%20heterogeneous%20scenarios.%0AThe%20CoDS%20employs%20two%20feature%20alignment%20modules%2C%20i.e.%2C%20Lightweight%0ASpatial-Channel%20Resizer%20%28LSCR%29%20and%20Distribution%20Alignment%20via%20Domain%20Separation%0A%28DADS%29.%20Besides%2C%20it%20utilizes%20the%20Domain%20Alignment%20Mutual%20Information%20%28DAMI%29%0Aloss%20to%20ensure%20effective%20feature%20alignment.%20Specifically%2C%20the%20LSCR%20aligns%20the%0Aneighbor%20feature%20across%20spatial%20and%20channel%20dimensions%20using%20a%20lightweight%0Aconvolutional%20layer.%20Subsequently%2C%20the%20DADS%20mitigates%20feature%20distribution%0Adiscrepancy%20with%20encoder-specific%20and%20encoder-agnostic%20domain%20separation%0Amodules.%20The%20former%20removes%20domain-dependent%20information%20and%20the%20latter%0Acaptures%20task-related%20information.%20During%20training%2C%20the%20DAMI%20loss%20maximizes%20the%0Amutual%20information%20between%20aligned%20heterogeneous%20features%20to%20enhance%20the%20domain%0Aseparation%20process.%20The%20CoDS%20employs%20a%20fully%20convolutional%20architecture%2C%20which%0Aensures%20high%20inference%20efficiency.%20Extensive%20experiments%20demonstrate%20that%20the%0ACoDS%20effectively%20mitigates%20feature%20discrepancies%20in%20heterogeneous%20scenarios%20and%0Aachieves%20a%20trade-off%20between%20detection%20accuracy%20and%20inference%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13432v1&entry.124074799=Read"},
{"title": "RelTopo: Multi-Level Relational Modeling for Driving Scene Topology\n  Reasoning", "author": "Yueru Luo and Changqing Zhou and Yiming Yang and Erlong Li and Chao Zheng and Shuqi Mei and Shuguang Cui and Zhen Li", "abstract": "  Accurate road topology reasoning is critical for autonomous driving, enabling\neffective navigation and adherence to traffic regulations. Central to this task\nare lane perception and topology reasoning. However, existing methods typically\nfocus on either lane detection or Lane-to-Lane (L2L) topology reasoning, often\n\\textit{neglecting} Lane-to-Traffic-element (L2T) relationships or\n\\textit{failing} to optimize these tasks jointly. Furthermore, most approaches\neither overlook relational modeling or apply it in a limited scope, despite the\ninherent spatial relationships among road elements. We argue that relational\nmodeling is beneficial for both perception and reasoning, as humans naturally\nleverage contextual relationships for road element recognition and their\nconnectivity inference. To this end, we introduce relational modeling into both\nperception and reasoning, \\textit{jointly} enhancing structural understanding.\nSpecifically, we propose: 1) a relation-aware lane detector, where our\ngeometry-biased self-attention and \\curve\\ cross-attention refine lane\nrepresentations by capturing relational dependencies; 2) relation-enhanced\ntopology heads, including a geometry-enhanced L2L head and a cross-view L2T\nhead, boosting reasoning with relational cues; and 3) a contrastive learning\nstrategy with InfoNCE loss to regularize relationship embeddings. Extensive\nexperiments on OpenLane-V2 demonstrate that our approach significantly improves\nboth detection and topology reasoning metrics, achieving +3.1 in DET$_l$, +5.3\nin TOP$_{ll}$, +4.9 in TOP$_{lt}$, and an overall +4.4 in OLS, setting a new\nstate-of-the-art. Code will be released.\n", "link": "http://arxiv.org/abs/2506.13553v2", "date": "2025-10-15", "relevancy": 2.1831, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5471}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5471}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5392}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RelTopo%3A%20Multi-Level%20Relational%20Modeling%20for%20Driving%20Scene%20Topology%0A%20%20Reasoning&body=Title%3A%20RelTopo%3A%20Multi-Level%20Relational%20Modeling%20for%20Driving%20Scene%20Topology%0A%20%20Reasoning%0AAuthor%3A%20Yueru%20Luo%20and%20Changqing%20Zhou%20and%20Yiming%20Yang%20and%20Erlong%20Li%20and%20Chao%20Zheng%20and%20Shuqi%20Mei%20and%20Shuguang%20Cui%20and%20Zhen%20Li%0AAbstract%3A%20%20%20Accurate%20road%20topology%20reasoning%20is%20critical%20for%20autonomous%20driving%2C%20enabling%0Aeffective%20navigation%20and%20adherence%20to%20traffic%20regulations.%20Central%20to%20this%20task%0Aare%20lane%20perception%20and%20topology%20reasoning.%20However%2C%20existing%20methods%20typically%0Afocus%20on%20either%20lane%20detection%20or%20Lane-to-Lane%20%28L2L%29%20topology%20reasoning%2C%20often%0A%5Ctextit%7Bneglecting%7D%20Lane-to-Traffic-element%20%28L2T%29%20relationships%20or%0A%5Ctextit%7Bfailing%7D%20to%20optimize%20these%20tasks%20jointly.%20Furthermore%2C%20most%20approaches%0Aeither%20overlook%20relational%20modeling%20or%20apply%20it%20in%20a%20limited%20scope%2C%20despite%20the%0Ainherent%20spatial%20relationships%20among%20road%20elements.%20We%20argue%20that%20relational%0Amodeling%20is%20beneficial%20for%20both%20perception%20and%20reasoning%2C%20as%20humans%20naturally%0Aleverage%20contextual%20relationships%20for%20road%20element%20recognition%20and%20their%0Aconnectivity%20inference.%20To%20this%20end%2C%20we%20introduce%20relational%20modeling%20into%20both%0Aperception%20and%20reasoning%2C%20%5Ctextit%7Bjointly%7D%20enhancing%20structural%20understanding.%0ASpecifically%2C%20we%20propose%3A%201%29%20a%20relation-aware%20lane%20detector%2C%20where%20our%0Ageometry-biased%20self-attention%20and%20%5Ccurve%5C%20cross-attention%20refine%20lane%0Arepresentations%20by%20capturing%20relational%20dependencies%3B%202%29%20relation-enhanced%0Atopology%20heads%2C%20including%20a%20geometry-enhanced%20L2L%20head%20and%20a%20cross-view%20L2T%0Ahead%2C%20boosting%20reasoning%20with%20relational%20cues%3B%20and%203%29%20a%20contrastive%20learning%0Astrategy%20with%20InfoNCE%20loss%20to%20regularize%20relationship%20embeddings.%20Extensive%0Aexperiments%20on%20OpenLane-V2%20demonstrate%20that%20our%20approach%20significantly%20improves%0Aboth%20detection%20and%20topology%20reasoning%20metrics%2C%20achieving%20%2B3.1%20in%20DET%24_l%24%2C%20%2B5.3%0Ain%20TOP%24_%7Bll%7D%24%2C%20%2B4.9%20in%20TOP%24_%7Blt%7D%24%2C%20and%20an%20overall%20%2B4.4%20in%20OLS%2C%20setting%20a%20new%0Astate-of-the-art.%20Code%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13553v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelTopo%253A%2520Multi-Level%2520Relational%2520Modeling%2520for%2520Driving%2520Scene%2520Topology%250A%2520%2520Reasoning%26entry.906535625%3DYueru%2520Luo%2520and%2520Changqing%2520Zhou%2520and%2520Yiming%2520Yang%2520and%2520Erlong%2520Li%2520and%2520Chao%2520Zheng%2520and%2520Shuqi%2520Mei%2520and%2520Shuguang%2520Cui%2520and%2520Zhen%2520Li%26entry.1292438233%3D%2520%2520Accurate%2520road%2520topology%2520reasoning%2520is%2520critical%2520for%2520autonomous%2520driving%252C%2520enabling%250Aeffective%2520navigation%2520and%2520adherence%2520to%2520traffic%2520regulations.%2520Central%2520to%2520this%2520task%250Aare%2520lane%2520perception%2520and%2520topology%2520reasoning.%2520However%252C%2520existing%2520methods%2520typically%250Afocus%2520on%2520either%2520lane%2520detection%2520or%2520Lane-to-Lane%2520%2528L2L%2529%2520topology%2520reasoning%252C%2520often%250A%255Ctextit%257Bneglecting%257D%2520Lane-to-Traffic-element%2520%2528L2T%2529%2520relationships%2520or%250A%255Ctextit%257Bfailing%257D%2520to%2520optimize%2520these%2520tasks%2520jointly.%2520Furthermore%252C%2520most%2520approaches%250Aeither%2520overlook%2520relational%2520modeling%2520or%2520apply%2520it%2520in%2520a%2520limited%2520scope%252C%2520despite%2520the%250Ainherent%2520spatial%2520relationships%2520among%2520road%2520elements.%2520We%2520argue%2520that%2520relational%250Amodeling%2520is%2520beneficial%2520for%2520both%2520perception%2520and%2520reasoning%252C%2520as%2520humans%2520naturally%250Aleverage%2520contextual%2520relationships%2520for%2520road%2520element%2520recognition%2520and%2520their%250Aconnectivity%2520inference.%2520To%2520this%2520end%252C%2520we%2520introduce%2520relational%2520modeling%2520into%2520both%250Aperception%2520and%2520reasoning%252C%2520%255Ctextit%257Bjointly%257D%2520enhancing%2520structural%2520understanding.%250ASpecifically%252C%2520we%2520propose%253A%25201%2529%2520a%2520relation-aware%2520lane%2520detector%252C%2520where%2520our%250Ageometry-biased%2520self-attention%2520and%2520%255Ccurve%255C%2520cross-attention%2520refine%2520lane%250Arepresentations%2520by%2520capturing%2520relational%2520dependencies%253B%25202%2529%2520relation-enhanced%250Atopology%2520heads%252C%2520including%2520a%2520geometry-enhanced%2520L2L%2520head%2520and%2520a%2520cross-view%2520L2T%250Ahead%252C%2520boosting%2520reasoning%2520with%2520relational%2520cues%253B%2520and%25203%2529%2520a%2520contrastive%2520learning%250Astrategy%2520with%2520InfoNCE%2520loss%2520to%2520regularize%2520relationship%2520embeddings.%2520Extensive%250Aexperiments%2520on%2520OpenLane-V2%2520demonstrate%2520that%2520our%2520approach%2520significantly%2520improves%250Aboth%2520detection%2520and%2520topology%2520reasoning%2520metrics%252C%2520achieving%2520%252B3.1%2520in%2520DET%2524_l%2524%252C%2520%252B5.3%250Ain%2520TOP%2524_%257Bll%257D%2524%252C%2520%252B4.9%2520in%2520TOP%2524_%257Blt%257D%2524%252C%2520and%2520an%2520overall%2520%252B4.4%2520in%2520OLS%252C%2520setting%2520a%2520new%250Astate-of-the-art.%2520Code%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13553v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RelTopo%3A%20Multi-Level%20Relational%20Modeling%20for%20Driving%20Scene%20Topology%0A%20%20Reasoning&entry.906535625=Yueru%20Luo%20and%20Changqing%20Zhou%20and%20Yiming%20Yang%20and%20Erlong%20Li%20and%20Chao%20Zheng%20and%20Shuqi%20Mei%20and%20Shuguang%20Cui%20and%20Zhen%20Li&entry.1292438233=%20%20Accurate%20road%20topology%20reasoning%20is%20critical%20for%20autonomous%20driving%2C%20enabling%0Aeffective%20navigation%20and%20adherence%20to%20traffic%20regulations.%20Central%20to%20this%20task%0Aare%20lane%20perception%20and%20topology%20reasoning.%20However%2C%20existing%20methods%20typically%0Afocus%20on%20either%20lane%20detection%20or%20Lane-to-Lane%20%28L2L%29%20topology%20reasoning%2C%20often%0A%5Ctextit%7Bneglecting%7D%20Lane-to-Traffic-element%20%28L2T%29%20relationships%20or%0A%5Ctextit%7Bfailing%7D%20to%20optimize%20these%20tasks%20jointly.%20Furthermore%2C%20most%20approaches%0Aeither%20overlook%20relational%20modeling%20or%20apply%20it%20in%20a%20limited%20scope%2C%20despite%20the%0Ainherent%20spatial%20relationships%20among%20road%20elements.%20We%20argue%20that%20relational%0Amodeling%20is%20beneficial%20for%20both%20perception%20and%20reasoning%2C%20as%20humans%20naturally%0Aleverage%20contextual%20relationships%20for%20road%20element%20recognition%20and%20their%0Aconnectivity%20inference.%20To%20this%20end%2C%20we%20introduce%20relational%20modeling%20into%20both%0Aperception%20and%20reasoning%2C%20%5Ctextit%7Bjointly%7D%20enhancing%20structural%20understanding.%0ASpecifically%2C%20we%20propose%3A%201%29%20a%20relation-aware%20lane%20detector%2C%20where%20our%0Ageometry-biased%20self-attention%20and%20%5Ccurve%5C%20cross-attention%20refine%20lane%0Arepresentations%20by%20capturing%20relational%20dependencies%3B%202%29%20relation-enhanced%0Atopology%20heads%2C%20including%20a%20geometry-enhanced%20L2L%20head%20and%20a%20cross-view%20L2T%0Ahead%2C%20boosting%20reasoning%20with%20relational%20cues%3B%20and%203%29%20a%20contrastive%20learning%0Astrategy%20with%20InfoNCE%20loss%20to%20regularize%20relationship%20embeddings.%20Extensive%0Aexperiments%20on%20OpenLane-V2%20demonstrate%20that%20our%20approach%20significantly%20improves%0Aboth%20detection%20and%20topology%20reasoning%20metrics%2C%20achieving%20%2B3.1%20in%20DET%24_l%24%2C%20%2B5.3%0Ain%20TOP%24_%7Bll%7D%24%2C%20%2B4.9%20in%20TOP%24_%7Blt%7D%24%2C%20and%20an%20overall%20%2B4.4%20in%20OLS%2C%20setting%20a%20new%0Astate-of-the-art.%20Code%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13553v2&entry.124074799=Read"},
{"title": "Defending against Stegomalware in Deep Neural Networks with Permutation\n  Symmetry", "author": "Birk Torpmann-Hagen and Michael A. Riegler and P\u00e5l Halvorsen and Dag Johansen", "abstract": "  Deep neural networks are being utilized in a growing number of applications,\nboth in production systems and for personal use. Network checkpoints are as a\nconsequence often shared and distributed on various platforms to ease the\ndevelopment process. This work considers the threat of neural network\nstegomalware, where malware is embedded in neural network checkpoints at a\nnegligible cost to network accuracy. This constitutes a significant security\nconcern, but is nevertheless largely neglected by the deep learning\npractitioners and security specialists alike. We propose the first effective\ncountermeasure to these attacks. In particular, we show that state-of-the-art\nneural network stegomalware can be efficiently and effectively neutralized\nthrough shuffling the column order of the weight- and bias-matrices, or\nequivalently the channel-order of convolutional layers. We show that this\neffectively corrupts payloads that have been embedded by state-of-the-art\nmethods in neural network steganography at no cost to network accuracy,\noutperforming competing methods by a significant margin. We then discuss\npossible means by which to bypass this defense, additional defense methods, and\nadvocate for continued research into the security of machine learning systems.\n", "link": "http://arxiv.org/abs/2509.20399v2", "date": "2025-10-15", "relevancy": 2.1751, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4408}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4329}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4314}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Defending%20against%20Stegomalware%20in%20Deep%20Neural%20Networks%20with%20Permutation%0A%20%20Symmetry&body=Title%3A%20Defending%20against%20Stegomalware%20in%20Deep%20Neural%20Networks%20with%20Permutation%0A%20%20Symmetry%0AAuthor%3A%20Birk%20Torpmann-Hagen%20and%20Michael%20A.%20Riegler%20and%20P%C3%A5l%20Halvorsen%20and%20Dag%20Johansen%0AAbstract%3A%20%20%20Deep%20neural%20networks%20are%20being%20utilized%20in%20a%20growing%20number%20of%20applications%2C%0Aboth%20in%20production%20systems%20and%20for%20personal%20use.%20Network%20checkpoints%20are%20as%20a%0Aconsequence%20often%20shared%20and%20distributed%20on%20various%20platforms%20to%20ease%20the%0Adevelopment%20process.%20This%20work%20considers%20the%20threat%20of%20neural%20network%0Astegomalware%2C%20where%20malware%20is%20embedded%20in%20neural%20network%20checkpoints%20at%20a%0Anegligible%20cost%20to%20network%20accuracy.%20This%20constitutes%20a%20significant%20security%0Aconcern%2C%20but%20is%20nevertheless%20largely%20neglected%20by%20the%20deep%20learning%0Apractitioners%20and%20security%20specialists%20alike.%20We%20propose%20the%20first%20effective%0Acountermeasure%20to%20these%20attacks.%20In%20particular%2C%20we%20show%20that%20state-of-the-art%0Aneural%20network%20stegomalware%20can%20be%20efficiently%20and%20effectively%20neutralized%0Athrough%20shuffling%20the%20column%20order%20of%20the%20weight-%20and%20bias-matrices%2C%20or%0Aequivalently%20the%20channel-order%20of%20convolutional%20layers.%20We%20show%20that%20this%0Aeffectively%20corrupts%20payloads%20that%20have%20been%20embedded%20by%20state-of-the-art%0Amethods%20in%20neural%20network%20steganography%20at%20no%20cost%20to%20network%20accuracy%2C%0Aoutperforming%20competing%20methods%20by%20a%20significant%20margin.%20We%20then%20discuss%0Apossible%20means%20by%20which%20to%20bypass%20this%20defense%2C%20additional%20defense%20methods%2C%20and%0Aadvocate%20for%20continued%20research%20into%20the%20security%20of%20machine%20learning%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20399v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDefending%2520against%2520Stegomalware%2520in%2520Deep%2520Neural%2520Networks%2520with%2520Permutation%250A%2520%2520Symmetry%26entry.906535625%3DBirk%2520Torpmann-Hagen%2520and%2520Michael%2520A.%2520Riegler%2520and%2520P%25C3%25A5l%2520Halvorsen%2520and%2520Dag%2520Johansen%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520are%2520being%2520utilized%2520in%2520a%2520growing%2520number%2520of%2520applications%252C%250Aboth%2520in%2520production%2520systems%2520and%2520for%2520personal%2520use.%2520Network%2520checkpoints%2520are%2520as%2520a%250Aconsequence%2520often%2520shared%2520and%2520distributed%2520on%2520various%2520platforms%2520to%2520ease%2520the%250Adevelopment%2520process.%2520This%2520work%2520considers%2520the%2520threat%2520of%2520neural%2520network%250Astegomalware%252C%2520where%2520malware%2520is%2520embedded%2520in%2520neural%2520network%2520checkpoints%2520at%2520a%250Anegligible%2520cost%2520to%2520network%2520accuracy.%2520This%2520constitutes%2520a%2520significant%2520security%250Aconcern%252C%2520but%2520is%2520nevertheless%2520largely%2520neglected%2520by%2520the%2520deep%2520learning%250Apractitioners%2520and%2520security%2520specialists%2520alike.%2520We%2520propose%2520the%2520first%2520effective%250Acountermeasure%2520to%2520these%2520attacks.%2520In%2520particular%252C%2520we%2520show%2520that%2520state-of-the-art%250Aneural%2520network%2520stegomalware%2520can%2520be%2520efficiently%2520and%2520effectively%2520neutralized%250Athrough%2520shuffling%2520the%2520column%2520order%2520of%2520the%2520weight-%2520and%2520bias-matrices%252C%2520or%250Aequivalently%2520the%2520channel-order%2520of%2520convolutional%2520layers.%2520We%2520show%2520that%2520this%250Aeffectively%2520corrupts%2520payloads%2520that%2520have%2520been%2520embedded%2520by%2520state-of-the-art%250Amethods%2520in%2520neural%2520network%2520steganography%2520at%2520no%2520cost%2520to%2520network%2520accuracy%252C%250Aoutperforming%2520competing%2520methods%2520by%2520a%2520significant%2520margin.%2520We%2520then%2520discuss%250Apossible%2520means%2520by%2520which%2520to%2520bypass%2520this%2520defense%252C%2520additional%2520defense%2520methods%252C%2520and%250Aadvocate%2520for%2520continued%2520research%2520into%2520the%2520security%2520of%2520machine%2520learning%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20399v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Defending%20against%20Stegomalware%20in%20Deep%20Neural%20Networks%20with%20Permutation%0A%20%20Symmetry&entry.906535625=Birk%20Torpmann-Hagen%20and%20Michael%20A.%20Riegler%20and%20P%C3%A5l%20Halvorsen%20and%20Dag%20Johansen&entry.1292438233=%20%20Deep%20neural%20networks%20are%20being%20utilized%20in%20a%20growing%20number%20of%20applications%2C%0Aboth%20in%20production%20systems%20and%20for%20personal%20use.%20Network%20checkpoints%20are%20as%20a%0Aconsequence%20often%20shared%20and%20distributed%20on%20various%20platforms%20to%20ease%20the%0Adevelopment%20process.%20This%20work%20considers%20the%20threat%20of%20neural%20network%0Astegomalware%2C%20where%20malware%20is%20embedded%20in%20neural%20network%20checkpoints%20at%20a%0Anegligible%20cost%20to%20network%20accuracy.%20This%20constitutes%20a%20significant%20security%0Aconcern%2C%20but%20is%20nevertheless%20largely%20neglected%20by%20the%20deep%20learning%0Apractitioners%20and%20security%20specialists%20alike.%20We%20propose%20the%20first%20effective%0Acountermeasure%20to%20these%20attacks.%20In%20particular%2C%20we%20show%20that%20state-of-the-art%0Aneural%20network%20stegomalware%20can%20be%20efficiently%20and%20effectively%20neutralized%0Athrough%20shuffling%20the%20column%20order%20of%20the%20weight-%20and%20bias-matrices%2C%20or%0Aequivalently%20the%20channel-order%20of%20convolutional%20layers.%20We%20show%20that%20this%0Aeffectively%20corrupts%20payloads%20that%20have%20been%20embedded%20by%20state-of-the-art%0Amethods%20in%20neural%20network%20steganography%20at%20no%20cost%20to%20network%20accuracy%2C%0Aoutperforming%20competing%20methods%20by%20a%20significant%20margin.%20We%20then%20discuss%0Apossible%20means%20by%20which%20to%20bypass%20this%20defense%2C%20additional%20defense%20methods%2C%20and%0Aadvocate%20for%20continued%20research%20into%20the%20security%20of%20machine%20learning%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20399v2&entry.124074799=Read"},
{"title": "From Refusal to Recovery: A Control-Theoretic Approach to Generative AI\n  Guardrails", "author": "Ravi Pandya and Madison Bland and Duy P. Nguyen and Changliu Liu and Jaime Fern\u00e1ndez Fisac and Andrea Bajcsy", "abstract": "  Generative AI systems are increasingly assisting and acting on behalf of end\nusers in practical settings, from digital shopping assistants to\nnext-generation autonomous cars. In this context, safety is no longer about\nblocking harmful content, but about preempting downstream hazards like\nfinancial or physical harm. Yet, most AI guardrails continue to rely on output\nclassification based on labeled datasets and human-specified criteria,making\nthem brittle to new hazardous situations. Even when unsafe conditions are\nflagged, this detection offers no path to recovery: typically, the AI system\nsimply refuses to act--which is not always a safe choice. In this work, we\nargue that agentic AI safety is fundamentally a sequential decision problem:\nharmful outcomes arise from the AI system's continually evolving interactions\nand their downstream consequences on the world. We formalize this through the\nlens of safety-critical control theory, but within the AI model's latent\nrepresentation of the world. This enables us to build predictive guardrails\nthat (i) monitor an AI system's outputs (actions) in real time and (ii)\nproactively correct risky outputs to safe ones, all in a model-agnostic manner\nso the same guardrail can be wrapped around any AI model. We also offer a\npractical training recipe for computing such guardrails at scale via\nsafety-critical reinforcement learning. Our experiments in simulated driving\nand e-commerce settings demonstrate that control-theoretic guardrails can\nreliably steer LLM agents clear of catastrophic outcomes (from collisions to\nbankruptcy) while preserving task performance, offering a principled dynamic\nalternative to today's flag-and-block guardrails.\n", "link": "http://arxiv.org/abs/2510.13727v1", "date": "2025-10-15", "relevancy": 2.1729, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5667}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5352}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Refusal%20to%20Recovery%3A%20A%20Control-Theoretic%20Approach%20to%20Generative%20AI%0A%20%20Guardrails&body=Title%3A%20From%20Refusal%20to%20Recovery%3A%20A%20Control-Theoretic%20Approach%20to%20Generative%20AI%0A%20%20Guardrails%0AAuthor%3A%20Ravi%20Pandya%20and%20Madison%20Bland%20and%20Duy%20P.%20Nguyen%20and%20Changliu%20Liu%20and%20Jaime%20Fern%C3%A1ndez%20Fisac%20and%20Andrea%20Bajcsy%0AAbstract%3A%20%20%20Generative%20AI%20systems%20are%20increasingly%20assisting%20and%20acting%20on%20behalf%20of%20end%0Ausers%20in%20practical%20settings%2C%20from%20digital%20shopping%20assistants%20to%0Anext-generation%20autonomous%20cars.%20In%20this%20context%2C%20safety%20is%20no%20longer%20about%0Ablocking%20harmful%20content%2C%20but%20about%20preempting%20downstream%20hazards%20like%0Afinancial%20or%20physical%20harm.%20Yet%2C%20most%20AI%20guardrails%20continue%20to%20rely%20on%20output%0Aclassification%20based%20on%20labeled%20datasets%20and%20human-specified%20criteria%2Cmaking%0Athem%20brittle%20to%20new%20hazardous%20situations.%20Even%20when%20unsafe%20conditions%20are%0Aflagged%2C%20this%20detection%20offers%20no%20path%20to%20recovery%3A%20typically%2C%20the%20AI%20system%0Asimply%20refuses%20to%20act--which%20is%20not%20always%20a%20safe%20choice.%20In%20this%20work%2C%20we%0Aargue%20that%20agentic%20AI%20safety%20is%20fundamentally%20a%20sequential%20decision%20problem%3A%0Aharmful%20outcomes%20arise%20from%20the%20AI%20system%27s%20continually%20evolving%20interactions%0Aand%20their%20downstream%20consequences%20on%20the%20world.%20We%20formalize%20this%20through%20the%0Alens%20of%20safety-critical%20control%20theory%2C%20but%20within%20the%20AI%20model%27s%20latent%0Arepresentation%20of%20the%20world.%20This%20enables%20us%20to%20build%20predictive%20guardrails%0Athat%20%28i%29%20monitor%20an%20AI%20system%27s%20outputs%20%28actions%29%20in%20real%20time%20and%20%28ii%29%0Aproactively%20correct%20risky%20outputs%20to%20safe%20ones%2C%20all%20in%20a%20model-agnostic%20manner%0Aso%20the%20same%20guardrail%20can%20be%20wrapped%20around%20any%20AI%20model.%20We%20also%20offer%20a%0Apractical%20training%20recipe%20for%20computing%20such%20guardrails%20at%20scale%20via%0Asafety-critical%20reinforcement%20learning.%20Our%20experiments%20in%20simulated%20driving%0Aand%20e-commerce%20settings%20demonstrate%20that%20control-theoretic%20guardrails%20can%0Areliably%20steer%20LLM%20agents%20clear%20of%20catastrophic%20outcomes%20%28from%20collisions%20to%0Abankruptcy%29%20while%20preserving%20task%20performance%2C%20offering%20a%20principled%20dynamic%0Aalternative%20to%20today%27s%20flag-and-block%20guardrails.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13727v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Refusal%2520to%2520Recovery%253A%2520A%2520Control-Theoretic%2520Approach%2520to%2520Generative%2520AI%250A%2520%2520Guardrails%26entry.906535625%3DRavi%2520Pandya%2520and%2520Madison%2520Bland%2520and%2520Duy%2520P.%2520Nguyen%2520and%2520Changliu%2520Liu%2520and%2520Jaime%2520Fern%25C3%25A1ndez%2520Fisac%2520and%2520Andrea%2520Bajcsy%26entry.1292438233%3D%2520%2520Generative%2520AI%2520systems%2520are%2520increasingly%2520assisting%2520and%2520acting%2520on%2520behalf%2520of%2520end%250Ausers%2520in%2520practical%2520settings%252C%2520from%2520digital%2520shopping%2520assistants%2520to%250Anext-generation%2520autonomous%2520cars.%2520In%2520this%2520context%252C%2520safety%2520is%2520no%2520longer%2520about%250Ablocking%2520harmful%2520content%252C%2520but%2520about%2520preempting%2520downstream%2520hazards%2520like%250Afinancial%2520or%2520physical%2520harm.%2520Yet%252C%2520most%2520AI%2520guardrails%2520continue%2520to%2520rely%2520on%2520output%250Aclassification%2520based%2520on%2520labeled%2520datasets%2520and%2520human-specified%2520criteria%252Cmaking%250Athem%2520brittle%2520to%2520new%2520hazardous%2520situations.%2520Even%2520when%2520unsafe%2520conditions%2520are%250Aflagged%252C%2520this%2520detection%2520offers%2520no%2520path%2520to%2520recovery%253A%2520typically%252C%2520the%2520AI%2520system%250Asimply%2520refuses%2520to%2520act--which%2520is%2520not%2520always%2520a%2520safe%2520choice.%2520In%2520this%2520work%252C%2520we%250Aargue%2520that%2520agentic%2520AI%2520safety%2520is%2520fundamentally%2520a%2520sequential%2520decision%2520problem%253A%250Aharmful%2520outcomes%2520arise%2520from%2520the%2520AI%2520system%2527s%2520continually%2520evolving%2520interactions%250Aand%2520their%2520downstream%2520consequences%2520on%2520the%2520world.%2520We%2520formalize%2520this%2520through%2520the%250Alens%2520of%2520safety-critical%2520control%2520theory%252C%2520but%2520within%2520the%2520AI%2520model%2527s%2520latent%250Arepresentation%2520of%2520the%2520world.%2520This%2520enables%2520us%2520to%2520build%2520predictive%2520guardrails%250Athat%2520%2528i%2529%2520monitor%2520an%2520AI%2520system%2527s%2520outputs%2520%2528actions%2529%2520in%2520real%2520time%2520and%2520%2528ii%2529%250Aproactively%2520correct%2520risky%2520outputs%2520to%2520safe%2520ones%252C%2520all%2520in%2520a%2520model-agnostic%2520manner%250Aso%2520the%2520same%2520guardrail%2520can%2520be%2520wrapped%2520around%2520any%2520AI%2520model.%2520We%2520also%2520offer%2520a%250Apractical%2520training%2520recipe%2520for%2520computing%2520such%2520guardrails%2520at%2520scale%2520via%250Asafety-critical%2520reinforcement%2520learning.%2520Our%2520experiments%2520in%2520simulated%2520driving%250Aand%2520e-commerce%2520settings%2520demonstrate%2520that%2520control-theoretic%2520guardrails%2520can%250Areliably%2520steer%2520LLM%2520agents%2520clear%2520of%2520catastrophic%2520outcomes%2520%2528from%2520collisions%2520to%250Abankruptcy%2529%2520while%2520preserving%2520task%2520performance%252C%2520offering%2520a%2520principled%2520dynamic%250Aalternative%2520to%2520today%2527s%2520flag-and-block%2520guardrails.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13727v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Refusal%20to%20Recovery%3A%20A%20Control-Theoretic%20Approach%20to%20Generative%20AI%0A%20%20Guardrails&entry.906535625=Ravi%20Pandya%20and%20Madison%20Bland%20and%20Duy%20P.%20Nguyen%20and%20Changliu%20Liu%20and%20Jaime%20Fern%C3%A1ndez%20Fisac%20and%20Andrea%20Bajcsy&entry.1292438233=%20%20Generative%20AI%20systems%20are%20increasingly%20assisting%20and%20acting%20on%20behalf%20of%20end%0Ausers%20in%20practical%20settings%2C%20from%20digital%20shopping%20assistants%20to%0Anext-generation%20autonomous%20cars.%20In%20this%20context%2C%20safety%20is%20no%20longer%20about%0Ablocking%20harmful%20content%2C%20but%20about%20preempting%20downstream%20hazards%20like%0Afinancial%20or%20physical%20harm.%20Yet%2C%20most%20AI%20guardrails%20continue%20to%20rely%20on%20output%0Aclassification%20based%20on%20labeled%20datasets%20and%20human-specified%20criteria%2Cmaking%0Athem%20brittle%20to%20new%20hazardous%20situations.%20Even%20when%20unsafe%20conditions%20are%0Aflagged%2C%20this%20detection%20offers%20no%20path%20to%20recovery%3A%20typically%2C%20the%20AI%20system%0Asimply%20refuses%20to%20act--which%20is%20not%20always%20a%20safe%20choice.%20In%20this%20work%2C%20we%0Aargue%20that%20agentic%20AI%20safety%20is%20fundamentally%20a%20sequential%20decision%20problem%3A%0Aharmful%20outcomes%20arise%20from%20the%20AI%20system%27s%20continually%20evolving%20interactions%0Aand%20their%20downstream%20consequences%20on%20the%20world.%20We%20formalize%20this%20through%20the%0Alens%20of%20safety-critical%20control%20theory%2C%20but%20within%20the%20AI%20model%27s%20latent%0Arepresentation%20of%20the%20world.%20This%20enables%20us%20to%20build%20predictive%20guardrails%0Athat%20%28i%29%20monitor%20an%20AI%20system%27s%20outputs%20%28actions%29%20in%20real%20time%20and%20%28ii%29%0Aproactively%20correct%20risky%20outputs%20to%20safe%20ones%2C%20all%20in%20a%20model-agnostic%20manner%0Aso%20the%20same%20guardrail%20can%20be%20wrapped%20around%20any%20AI%20model.%20We%20also%20offer%20a%0Apractical%20training%20recipe%20for%20computing%20such%20guardrails%20at%20scale%20via%0Asafety-critical%20reinforcement%20learning.%20Our%20experiments%20in%20simulated%20driving%0Aand%20e-commerce%20settings%20demonstrate%20that%20control-theoretic%20guardrails%20can%0Areliably%20steer%20LLM%20agents%20clear%20of%20catastrophic%20outcomes%20%28from%20collisions%20to%0Abankruptcy%29%20while%20preserving%20task%20performance%2C%20offering%20a%20principled%20dynamic%0Aalternative%20to%20today%27s%20flag-and-block%20guardrails.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13727v1&entry.124074799=Read"},
{"title": "NoisePrints: Distortion-Free Watermarks for Authorship in Private\n  Diffusion Models", "author": "Nir Goren and Oren Katzir and Abhinav Nakarmi and Eyal Ronen and Mahmood Sharif and Or Patashnik", "abstract": "  With the rapid adoption of diffusion models for visual content generation,\nproving authorship and protecting copyright have become critical. This\nchallenge is particularly important when model owners keep their models private\nand may be unwilling or unable to handle authorship issues, making third-party\nverification essential. A natural solution is to embed watermarks for later\nverification. However, existing methods require access to model weights and\nrely on computationally heavy procedures, rendering them impractical and\nnon-scalable. To address these challenges, we propose , a lightweight\nwatermarking scheme that utilizes the random seed used to initialize the\ndiffusion process as a proof of authorship without modifying the generation\nprocess. Our key observation is that the initial noise derived from a seed is\nhighly correlated with the generated visual content. By incorporating a hash\nfunction into the noise sampling process, we further ensure that recovering a\nvalid seed from the content is infeasible. We also show that sampling an\nalternative seed that passes verification is infeasible, and demonstrate the\nrobustness of our method under various manipulations. Finally, we show how to\nuse cryptographic zero-knowledge proofs to prove ownership without revealing\nthe seed. By keeping the seed secret, we increase the difficulty of watermark\nremoval. In our experiments, we validate NoisePrints on multiple\nstate-of-the-art diffusion models for images and videos, demonstrating\nefficient verification using only the seed and output, without requiring access\nto model weights.\n", "link": "http://arxiv.org/abs/2510.13793v1", "date": "2025-10-15", "relevancy": 2.1664, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5519}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5479}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NoisePrints%3A%20Distortion-Free%20Watermarks%20for%20Authorship%20in%20Private%0A%20%20Diffusion%20Models&body=Title%3A%20NoisePrints%3A%20Distortion-Free%20Watermarks%20for%20Authorship%20in%20Private%0A%20%20Diffusion%20Models%0AAuthor%3A%20Nir%20Goren%20and%20Oren%20Katzir%20and%20Abhinav%20Nakarmi%20and%20Eyal%20Ronen%20and%20Mahmood%20Sharif%20and%20Or%20Patashnik%0AAbstract%3A%20%20%20With%20the%20rapid%20adoption%20of%20diffusion%20models%20for%20visual%20content%20generation%2C%0Aproving%20authorship%20and%20protecting%20copyright%20have%20become%20critical.%20This%0Achallenge%20is%20particularly%20important%20when%20model%20owners%20keep%20their%20models%20private%0Aand%20may%20be%20unwilling%20or%20unable%20to%20handle%20authorship%20issues%2C%20making%20third-party%0Averification%20essential.%20A%20natural%20solution%20is%20to%20embed%20watermarks%20for%20later%0Averification.%20However%2C%20existing%20methods%20require%20access%20to%20model%20weights%20and%0Arely%20on%20computationally%20heavy%20procedures%2C%20rendering%20them%20impractical%20and%0Anon-scalable.%20To%20address%20these%20challenges%2C%20we%20propose%20%2C%20a%20lightweight%0Awatermarking%20scheme%20that%20utilizes%20the%20random%20seed%20used%20to%20initialize%20the%0Adiffusion%20process%20as%20a%20proof%20of%20authorship%20without%20modifying%20the%20generation%0Aprocess.%20Our%20key%20observation%20is%20that%20the%20initial%20noise%20derived%20from%20a%20seed%20is%0Ahighly%20correlated%20with%20the%20generated%20visual%20content.%20By%20incorporating%20a%20hash%0Afunction%20into%20the%20noise%20sampling%20process%2C%20we%20further%20ensure%20that%20recovering%20a%0Avalid%20seed%20from%20the%20content%20is%20infeasible.%20We%20also%20show%20that%20sampling%20an%0Aalternative%20seed%20that%20passes%20verification%20is%20infeasible%2C%20and%20demonstrate%20the%0Arobustness%20of%20our%20method%20under%20various%20manipulations.%20Finally%2C%20we%20show%20how%20to%0Ause%20cryptographic%20zero-knowledge%20proofs%20to%20prove%20ownership%20without%20revealing%0Athe%20seed.%20By%20keeping%20the%20seed%20secret%2C%20we%20increase%20the%20difficulty%20of%20watermark%0Aremoval.%20In%20our%20experiments%2C%20we%20validate%20NoisePrints%20on%20multiple%0Astate-of-the-art%20diffusion%20models%20for%20images%20and%20videos%2C%20demonstrating%0Aefficient%20verification%20using%20only%20the%20seed%20and%20output%2C%20without%20requiring%20access%0Ato%20model%20weights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13793v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNoisePrints%253A%2520Distortion-Free%2520Watermarks%2520for%2520Authorship%2520in%2520Private%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DNir%2520Goren%2520and%2520Oren%2520Katzir%2520and%2520Abhinav%2520Nakarmi%2520and%2520Eyal%2520Ronen%2520and%2520Mahmood%2520Sharif%2520and%2520Or%2520Patashnik%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520adoption%2520of%2520diffusion%2520models%2520for%2520visual%2520content%2520generation%252C%250Aproving%2520authorship%2520and%2520protecting%2520copyright%2520have%2520become%2520critical.%2520This%250Achallenge%2520is%2520particularly%2520important%2520when%2520model%2520owners%2520keep%2520their%2520models%2520private%250Aand%2520may%2520be%2520unwilling%2520or%2520unable%2520to%2520handle%2520authorship%2520issues%252C%2520making%2520third-party%250Averification%2520essential.%2520A%2520natural%2520solution%2520is%2520to%2520embed%2520watermarks%2520for%2520later%250Averification.%2520However%252C%2520existing%2520methods%2520require%2520access%2520to%2520model%2520weights%2520and%250Arely%2520on%2520computationally%2520heavy%2520procedures%252C%2520rendering%2520them%2520impractical%2520and%250Anon-scalable.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520%252C%2520a%2520lightweight%250Awatermarking%2520scheme%2520that%2520utilizes%2520the%2520random%2520seed%2520used%2520to%2520initialize%2520the%250Adiffusion%2520process%2520as%2520a%2520proof%2520of%2520authorship%2520without%2520modifying%2520the%2520generation%250Aprocess.%2520Our%2520key%2520observation%2520is%2520that%2520the%2520initial%2520noise%2520derived%2520from%2520a%2520seed%2520is%250Ahighly%2520correlated%2520with%2520the%2520generated%2520visual%2520content.%2520By%2520incorporating%2520a%2520hash%250Afunction%2520into%2520the%2520noise%2520sampling%2520process%252C%2520we%2520further%2520ensure%2520that%2520recovering%2520a%250Avalid%2520seed%2520from%2520the%2520content%2520is%2520infeasible.%2520We%2520also%2520show%2520that%2520sampling%2520an%250Aalternative%2520seed%2520that%2520passes%2520verification%2520is%2520infeasible%252C%2520and%2520demonstrate%2520the%250Arobustness%2520of%2520our%2520method%2520under%2520various%2520manipulations.%2520Finally%252C%2520we%2520show%2520how%2520to%250Ause%2520cryptographic%2520zero-knowledge%2520proofs%2520to%2520prove%2520ownership%2520without%2520revealing%250Athe%2520seed.%2520By%2520keeping%2520the%2520seed%2520secret%252C%2520we%2520increase%2520the%2520difficulty%2520of%2520watermark%250Aremoval.%2520In%2520our%2520experiments%252C%2520we%2520validate%2520NoisePrints%2520on%2520multiple%250Astate-of-the-art%2520diffusion%2520models%2520for%2520images%2520and%2520videos%252C%2520demonstrating%250Aefficient%2520verification%2520using%2520only%2520the%2520seed%2520and%2520output%252C%2520without%2520requiring%2520access%250Ato%2520model%2520weights.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13793v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NoisePrints%3A%20Distortion-Free%20Watermarks%20for%20Authorship%20in%20Private%0A%20%20Diffusion%20Models&entry.906535625=Nir%20Goren%20and%20Oren%20Katzir%20and%20Abhinav%20Nakarmi%20and%20Eyal%20Ronen%20and%20Mahmood%20Sharif%20and%20Or%20Patashnik&entry.1292438233=%20%20With%20the%20rapid%20adoption%20of%20diffusion%20models%20for%20visual%20content%20generation%2C%0Aproving%20authorship%20and%20protecting%20copyright%20have%20become%20critical.%20This%0Achallenge%20is%20particularly%20important%20when%20model%20owners%20keep%20their%20models%20private%0Aand%20may%20be%20unwilling%20or%20unable%20to%20handle%20authorship%20issues%2C%20making%20third-party%0Averification%20essential.%20A%20natural%20solution%20is%20to%20embed%20watermarks%20for%20later%0Averification.%20However%2C%20existing%20methods%20require%20access%20to%20model%20weights%20and%0Arely%20on%20computationally%20heavy%20procedures%2C%20rendering%20them%20impractical%20and%0Anon-scalable.%20To%20address%20these%20challenges%2C%20we%20propose%20%2C%20a%20lightweight%0Awatermarking%20scheme%20that%20utilizes%20the%20random%20seed%20used%20to%20initialize%20the%0Adiffusion%20process%20as%20a%20proof%20of%20authorship%20without%20modifying%20the%20generation%0Aprocess.%20Our%20key%20observation%20is%20that%20the%20initial%20noise%20derived%20from%20a%20seed%20is%0Ahighly%20correlated%20with%20the%20generated%20visual%20content.%20By%20incorporating%20a%20hash%0Afunction%20into%20the%20noise%20sampling%20process%2C%20we%20further%20ensure%20that%20recovering%20a%0Avalid%20seed%20from%20the%20content%20is%20infeasible.%20We%20also%20show%20that%20sampling%20an%0Aalternative%20seed%20that%20passes%20verification%20is%20infeasible%2C%20and%20demonstrate%20the%0Arobustness%20of%20our%20method%20under%20various%20manipulations.%20Finally%2C%20we%20show%20how%20to%0Ause%20cryptographic%20zero-knowledge%20proofs%20to%20prove%20ownership%20without%20revealing%0Athe%20seed.%20By%20keeping%20the%20seed%20secret%2C%20we%20increase%20the%20difficulty%20of%20watermark%0Aremoval.%20In%20our%20experiments%2C%20we%20validate%20NoisePrints%20on%20multiple%0Astate-of-the-art%20diffusion%20models%20for%20images%20and%20videos%2C%20demonstrating%0Aefficient%20verification%20using%20only%20the%20seed%20and%20output%2C%20without%20requiring%20access%0Ato%20model%20weights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13793v1&entry.124074799=Read"},
{"title": "Generalizing WiFi Gesture Recognition via Large-Model-Aware Semantic\n  Distillation and Alignment", "author": "Feng-Qi Cui and Yu-Tong Guo and Tianyue Zheng and Jinyang Huang", "abstract": "  WiFi-based gesture recognition has emerged as a promising RF sensing paradigm\nfor enabling non-contact and privacy-preserving human-computer interaction in\nAIoT environments. However, existing methods often suffer from limited\ngeneralization and semantic expressiveness due to the domain-sensitive nature\nof Channel State Information and the lack of high-level gesture abstraction. To\naddress these challenges, we propose a novel generalization framework, termed\nLarge-Model-Aware Semantic Distillation and Alignment (GLSDA), which leverages\nthe semantic prior of pre-trained large foundation models to enhance gesture\nrepresentation learning in both in-domain and cross-domain scenarios.\nSpecifically, we first design a dual-path CSI encoding pipeline that captures\ngeometric and dynamic gesture patterns via CSI-Ratio phase sequences and\nDoppler spectrograms. These representations are then fed into a Multiscale\nSemantic Encoder, which learns robust temporal embeddings and aligns them with\ngesture semantics through cross-modal attention mechanisms. To further enhance\ncategory discrimination, we introduce a Semantic-Aware Soft Supervision scheme\nthat encodes inter-class correlations and reduces label ambiguity, especially\nfor semantically similar gestures. Finally, we develop a Robust\nDual-Distillation strategy to compress the aligned model into a lightweight\nstudent network, jointly distilling intermediate features and semantic-informed\nsoft labels from the teacher model. Extensive experiments on the Widar3.0\nbenchmark show that GLSDA consistently outperforms state-of-the-art methods in\nboth in-domain and cross-domain gesture recognition tasks, while significantly\nreducing model size and inference latency. Our method offers a scalable and\ndeployable solution for generalized RF-based gesture interfaces in real-world\nAIoT applications.\n", "link": "http://arxiv.org/abs/2510.13390v1", "date": "2025-10-15", "relevancy": 2.1573, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5575}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5422}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalizing%20WiFi%20Gesture%20Recognition%20via%20Large-Model-Aware%20Semantic%0A%20%20Distillation%20and%20Alignment&body=Title%3A%20Generalizing%20WiFi%20Gesture%20Recognition%20via%20Large-Model-Aware%20Semantic%0A%20%20Distillation%20and%20Alignment%0AAuthor%3A%20Feng-Qi%20Cui%20and%20Yu-Tong%20Guo%20and%20Tianyue%20Zheng%20and%20Jinyang%20Huang%0AAbstract%3A%20%20%20WiFi-based%20gesture%20recognition%20has%20emerged%20as%20a%20promising%20RF%20sensing%20paradigm%0Afor%20enabling%20non-contact%20and%20privacy-preserving%20human-computer%20interaction%20in%0AAIoT%20environments.%20However%2C%20existing%20methods%20often%20suffer%20from%20limited%0Ageneralization%20and%20semantic%20expressiveness%20due%20to%20the%20domain-sensitive%20nature%0Aof%20Channel%20State%20Information%20and%20the%20lack%20of%20high-level%20gesture%20abstraction.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20a%20novel%20generalization%20framework%2C%20termed%0ALarge-Model-Aware%20Semantic%20Distillation%20and%20Alignment%20%28GLSDA%29%2C%20which%20leverages%0Athe%20semantic%20prior%20of%20pre-trained%20large%20foundation%20models%20to%20enhance%20gesture%0Arepresentation%20learning%20in%20both%20in-domain%20and%20cross-domain%20scenarios.%0ASpecifically%2C%20we%20first%20design%20a%20dual-path%20CSI%20encoding%20pipeline%20that%20captures%0Ageometric%20and%20dynamic%20gesture%20patterns%20via%20CSI-Ratio%20phase%20sequences%20and%0ADoppler%20spectrograms.%20These%20representations%20are%20then%20fed%20into%20a%20Multiscale%0ASemantic%20Encoder%2C%20which%20learns%20robust%20temporal%20embeddings%20and%20aligns%20them%20with%0Agesture%20semantics%20through%20cross-modal%20attention%20mechanisms.%20To%20further%20enhance%0Acategory%20discrimination%2C%20we%20introduce%20a%20Semantic-Aware%20Soft%20Supervision%20scheme%0Athat%20encodes%20inter-class%20correlations%20and%20reduces%20label%20ambiguity%2C%20especially%0Afor%20semantically%20similar%20gestures.%20Finally%2C%20we%20develop%20a%20Robust%0ADual-Distillation%20strategy%20to%20compress%20the%20aligned%20model%20into%20a%20lightweight%0Astudent%20network%2C%20jointly%20distilling%20intermediate%20features%20and%20semantic-informed%0Asoft%20labels%20from%20the%20teacher%20model.%20Extensive%20experiments%20on%20the%20Widar3.0%0Abenchmark%20show%20that%20GLSDA%20consistently%20outperforms%20state-of-the-art%20methods%20in%0Aboth%20in-domain%20and%20cross-domain%20gesture%20recognition%20tasks%2C%20while%20significantly%0Areducing%20model%20size%20and%20inference%20latency.%20Our%20method%20offers%20a%20scalable%20and%0Adeployable%20solution%20for%20generalized%20RF-based%20gesture%20interfaces%20in%20real-world%0AAIoT%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13390v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralizing%2520WiFi%2520Gesture%2520Recognition%2520via%2520Large-Model-Aware%2520Semantic%250A%2520%2520Distillation%2520and%2520Alignment%26entry.906535625%3DFeng-Qi%2520Cui%2520and%2520Yu-Tong%2520Guo%2520and%2520Tianyue%2520Zheng%2520and%2520Jinyang%2520Huang%26entry.1292438233%3D%2520%2520WiFi-based%2520gesture%2520recognition%2520has%2520emerged%2520as%2520a%2520promising%2520RF%2520sensing%2520paradigm%250Afor%2520enabling%2520non-contact%2520and%2520privacy-preserving%2520human-computer%2520interaction%2520in%250AAIoT%2520environments.%2520However%252C%2520existing%2520methods%2520often%2520suffer%2520from%2520limited%250Ageneralization%2520and%2520semantic%2520expressiveness%2520due%2520to%2520the%2520domain-sensitive%2520nature%250Aof%2520Channel%2520State%2520Information%2520and%2520the%2520lack%2520of%2520high-level%2520gesture%2520abstraction.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520generalization%2520framework%252C%2520termed%250ALarge-Model-Aware%2520Semantic%2520Distillation%2520and%2520Alignment%2520%2528GLSDA%2529%252C%2520which%2520leverages%250Athe%2520semantic%2520prior%2520of%2520pre-trained%2520large%2520foundation%2520models%2520to%2520enhance%2520gesture%250Arepresentation%2520learning%2520in%2520both%2520in-domain%2520and%2520cross-domain%2520scenarios.%250ASpecifically%252C%2520we%2520first%2520design%2520a%2520dual-path%2520CSI%2520encoding%2520pipeline%2520that%2520captures%250Ageometric%2520and%2520dynamic%2520gesture%2520patterns%2520via%2520CSI-Ratio%2520phase%2520sequences%2520and%250ADoppler%2520spectrograms.%2520These%2520representations%2520are%2520then%2520fed%2520into%2520a%2520Multiscale%250ASemantic%2520Encoder%252C%2520which%2520learns%2520robust%2520temporal%2520embeddings%2520and%2520aligns%2520them%2520with%250Agesture%2520semantics%2520through%2520cross-modal%2520attention%2520mechanisms.%2520To%2520further%2520enhance%250Acategory%2520discrimination%252C%2520we%2520introduce%2520a%2520Semantic-Aware%2520Soft%2520Supervision%2520scheme%250Athat%2520encodes%2520inter-class%2520correlations%2520and%2520reduces%2520label%2520ambiguity%252C%2520especially%250Afor%2520semantically%2520similar%2520gestures.%2520Finally%252C%2520we%2520develop%2520a%2520Robust%250ADual-Distillation%2520strategy%2520to%2520compress%2520the%2520aligned%2520model%2520into%2520a%2520lightweight%250Astudent%2520network%252C%2520jointly%2520distilling%2520intermediate%2520features%2520and%2520semantic-informed%250Asoft%2520labels%2520from%2520the%2520teacher%2520model.%2520Extensive%2520experiments%2520on%2520the%2520Widar3.0%250Abenchmark%2520show%2520that%2520GLSDA%2520consistently%2520outperforms%2520state-of-the-art%2520methods%2520in%250Aboth%2520in-domain%2520and%2520cross-domain%2520gesture%2520recognition%2520tasks%252C%2520while%2520significantly%250Areducing%2520model%2520size%2520and%2520inference%2520latency.%2520Our%2520method%2520offers%2520a%2520scalable%2520and%250Adeployable%2520solution%2520for%2520generalized%2520RF-based%2520gesture%2520interfaces%2520in%2520real-world%250AAIoT%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13390v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalizing%20WiFi%20Gesture%20Recognition%20via%20Large-Model-Aware%20Semantic%0A%20%20Distillation%20and%20Alignment&entry.906535625=Feng-Qi%20Cui%20and%20Yu-Tong%20Guo%20and%20Tianyue%20Zheng%20and%20Jinyang%20Huang&entry.1292438233=%20%20WiFi-based%20gesture%20recognition%20has%20emerged%20as%20a%20promising%20RF%20sensing%20paradigm%0Afor%20enabling%20non-contact%20and%20privacy-preserving%20human-computer%20interaction%20in%0AAIoT%20environments.%20However%2C%20existing%20methods%20often%20suffer%20from%20limited%0Ageneralization%20and%20semantic%20expressiveness%20due%20to%20the%20domain-sensitive%20nature%0Aof%20Channel%20State%20Information%20and%20the%20lack%20of%20high-level%20gesture%20abstraction.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20a%20novel%20generalization%20framework%2C%20termed%0ALarge-Model-Aware%20Semantic%20Distillation%20and%20Alignment%20%28GLSDA%29%2C%20which%20leverages%0Athe%20semantic%20prior%20of%20pre-trained%20large%20foundation%20models%20to%20enhance%20gesture%0Arepresentation%20learning%20in%20both%20in-domain%20and%20cross-domain%20scenarios.%0ASpecifically%2C%20we%20first%20design%20a%20dual-path%20CSI%20encoding%20pipeline%20that%20captures%0Ageometric%20and%20dynamic%20gesture%20patterns%20via%20CSI-Ratio%20phase%20sequences%20and%0ADoppler%20spectrograms.%20These%20representations%20are%20then%20fed%20into%20a%20Multiscale%0ASemantic%20Encoder%2C%20which%20learns%20robust%20temporal%20embeddings%20and%20aligns%20them%20with%0Agesture%20semantics%20through%20cross-modal%20attention%20mechanisms.%20To%20further%20enhance%0Acategory%20discrimination%2C%20we%20introduce%20a%20Semantic-Aware%20Soft%20Supervision%20scheme%0Athat%20encodes%20inter-class%20correlations%20and%20reduces%20label%20ambiguity%2C%20especially%0Afor%20semantically%20similar%20gestures.%20Finally%2C%20we%20develop%20a%20Robust%0ADual-Distillation%20strategy%20to%20compress%20the%20aligned%20model%20into%20a%20lightweight%0Astudent%20network%2C%20jointly%20distilling%20intermediate%20features%20and%20semantic-informed%0Asoft%20labels%20from%20the%20teacher%20model.%20Extensive%20experiments%20on%20the%20Widar3.0%0Abenchmark%20show%20that%20GLSDA%20consistently%20outperforms%20state-of-the-art%20methods%20in%0Aboth%20in-domain%20and%20cross-domain%20gesture%20recognition%20tasks%2C%20while%20significantly%0Areducing%20model%20size%20and%20inference%20latency.%20Our%20method%20offers%20a%20scalable%20and%0Adeployable%20solution%20for%20generalized%20RF-based%20gesture%20interfaces%20in%20real-world%0AAIoT%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13390v1&entry.124074799=Read"},
{"title": "Assessing the Latent Automated Program Repair Capabilities of Large\n  Language Models using Round-Trip Translation", "author": "Fernando Vallecillos Ruiz and Anastasiia Grishina and Max Hort and Leon Moonen", "abstract": "  Research shows that errors in natural language can be corrected by\ntranslating texts to another language and back using language models. We\nexplore to what extent this latent correction capability extends to Automated\nProgram Repair (APR) by investigating Round-Trip Translation (RTT): translating\ncode from one programming language into another programming or natural language\nand back, using Large Language Models (LLMs). We hypothesize that RTT restores\npatterns most commonly seen in the LLM's training corpora through regression\ntoward the mean, replacing infrequent bugs with more frequent, natural,\nbug-free code. To test this hypothesis, we employ nine LLMs and four common APR\nbenchmarks in Java, and perform a detailed quantitative and qualitative\nanalysis of RTT-generated patches. We find that RTT through English generates\nplausible patches for 100 of 164 bugs with GPT-4 on the HumanEval-Java\nbenchmark, and 97 are found to be correct in our manual assessment. Moreover,\nRTT uniquely generates plausible patches for 46 bugs that were missed by LLMs\nspecifically fine-tuned for APR. While this demonstrates the viability of RTT\nfor APR, we also observe limitations, such as a lower overall bug fix rate than\nthe state-of-the-art and diluting the original coding style. We analyze the\nimpact of these limitations and discuss the potential of using RTT as a\ncomplementary component in APR frameworks. A replication package is available\nfor download from https://doi.org/10.5281/zenodo.10500593.\n  Keywords: automated program repair, large language model, machine translation\n", "link": "http://arxiv.org/abs/2401.07994v2", "date": "2025-10-15", "relevancy": 2.1446, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4316}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4276}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Assessing%20the%20Latent%20Automated%20Program%20Repair%20Capabilities%20of%20Large%0A%20%20Language%20Models%20using%20Round-Trip%20Translation&body=Title%3A%20Assessing%20the%20Latent%20Automated%20Program%20Repair%20Capabilities%20of%20Large%0A%20%20Language%20Models%20using%20Round-Trip%20Translation%0AAuthor%3A%20Fernando%20Vallecillos%20Ruiz%20and%20Anastasiia%20Grishina%20and%20Max%20Hort%20and%20Leon%20Moonen%0AAbstract%3A%20%20%20Research%20shows%20that%20errors%20in%20natural%20language%20can%20be%20corrected%20by%0Atranslating%20texts%20to%20another%20language%20and%20back%20using%20language%20models.%20We%0Aexplore%20to%20what%20extent%20this%20latent%20correction%20capability%20extends%20to%20Automated%0AProgram%20Repair%20%28APR%29%20by%20investigating%20Round-Trip%20Translation%20%28RTT%29%3A%20translating%0Acode%20from%20one%20programming%20language%20into%20another%20programming%20or%20natural%20language%0Aand%20back%2C%20using%20Large%20Language%20Models%20%28LLMs%29.%20We%20hypothesize%20that%20RTT%20restores%0Apatterns%20most%20commonly%20seen%20in%20the%20LLM%27s%20training%20corpora%20through%20regression%0Atoward%20the%20mean%2C%20replacing%20infrequent%20bugs%20with%20more%20frequent%2C%20natural%2C%0Abug-free%20code.%20To%20test%20this%20hypothesis%2C%20we%20employ%20nine%20LLMs%20and%20four%20common%20APR%0Abenchmarks%20in%20Java%2C%20and%20perform%20a%20detailed%20quantitative%20and%20qualitative%0Aanalysis%20of%20RTT-generated%20patches.%20We%20find%20that%20RTT%20through%20English%20generates%0Aplausible%20patches%20for%20100%20of%20164%20bugs%20with%20GPT-4%20on%20the%20HumanEval-Java%0Abenchmark%2C%20and%2097%20are%20found%20to%20be%20correct%20in%20our%20manual%20assessment.%20Moreover%2C%0ARTT%20uniquely%20generates%20plausible%20patches%20for%2046%20bugs%20that%20were%20missed%20by%20LLMs%0Aspecifically%20fine-tuned%20for%20APR.%20While%20this%20demonstrates%20the%20viability%20of%20RTT%0Afor%20APR%2C%20we%20also%20observe%20limitations%2C%20such%20as%20a%20lower%20overall%20bug%20fix%20rate%20than%0Athe%20state-of-the-art%20and%20diluting%20the%20original%20coding%20style.%20We%20analyze%20the%0Aimpact%20of%20these%20limitations%20and%20discuss%20the%20potential%20of%20using%20RTT%20as%20a%0Acomplementary%20component%20in%20APR%20frameworks.%20A%20replication%20package%20is%20available%0Afor%20download%20from%20https%3A//doi.org/10.5281/zenodo.10500593.%0A%20%20Keywords%3A%20automated%20program%20repair%2C%20large%20language%20model%2C%20machine%20translation%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.07994v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAssessing%2520the%2520Latent%2520Automated%2520Program%2520Repair%2520Capabilities%2520of%2520Large%250A%2520%2520Language%2520Models%2520using%2520Round-Trip%2520Translation%26entry.906535625%3DFernando%2520Vallecillos%2520Ruiz%2520and%2520Anastasiia%2520Grishina%2520and%2520Max%2520Hort%2520and%2520Leon%2520Moonen%26entry.1292438233%3D%2520%2520Research%2520shows%2520that%2520errors%2520in%2520natural%2520language%2520can%2520be%2520corrected%2520by%250Atranslating%2520texts%2520to%2520another%2520language%2520and%2520back%2520using%2520language%2520models.%2520We%250Aexplore%2520to%2520what%2520extent%2520this%2520latent%2520correction%2520capability%2520extends%2520to%2520Automated%250AProgram%2520Repair%2520%2528APR%2529%2520by%2520investigating%2520Round-Trip%2520Translation%2520%2528RTT%2529%253A%2520translating%250Acode%2520from%2520one%2520programming%2520language%2520into%2520another%2520programming%2520or%2520natural%2520language%250Aand%2520back%252C%2520using%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520We%2520hypothesize%2520that%2520RTT%2520restores%250Apatterns%2520most%2520commonly%2520seen%2520in%2520the%2520LLM%2527s%2520training%2520corpora%2520through%2520regression%250Atoward%2520the%2520mean%252C%2520replacing%2520infrequent%2520bugs%2520with%2520more%2520frequent%252C%2520natural%252C%250Abug-free%2520code.%2520To%2520test%2520this%2520hypothesis%252C%2520we%2520employ%2520nine%2520LLMs%2520and%2520four%2520common%2520APR%250Abenchmarks%2520in%2520Java%252C%2520and%2520perform%2520a%2520detailed%2520quantitative%2520and%2520qualitative%250Aanalysis%2520of%2520RTT-generated%2520patches.%2520We%2520find%2520that%2520RTT%2520through%2520English%2520generates%250Aplausible%2520patches%2520for%2520100%2520of%2520164%2520bugs%2520with%2520GPT-4%2520on%2520the%2520HumanEval-Java%250Abenchmark%252C%2520and%252097%2520are%2520found%2520to%2520be%2520correct%2520in%2520our%2520manual%2520assessment.%2520Moreover%252C%250ARTT%2520uniquely%2520generates%2520plausible%2520patches%2520for%252046%2520bugs%2520that%2520were%2520missed%2520by%2520LLMs%250Aspecifically%2520fine-tuned%2520for%2520APR.%2520While%2520this%2520demonstrates%2520the%2520viability%2520of%2520RTT%250Afor%2520APR%252C%2520we%2520also%2520observe%2520limitations%252C%2520such%2520as%2520a%2520lower%2520overall%2520bug%2520fix%2520rate%2520than%250Athe%2520state-of-the-art%2520and%2520diluting%2520the%2520original%2520coding%2520style.%2520We%2520analyze%2520the%250Aimpact%2520of%2520these%2520limitations%2520and%2520discuss%2520the%2520potential%2520of%2520using%2520RTT%2520as%2520a%250Acomplementary%2520component%2520in%2520APR%2520frameworks.%2520A%2520replication%2520package%2520is%2520available%250Afor%2520download%2520from%2520https%253A//doi.org/10.5281/zenodo.10500593.%250A%2520%2520Keywords%253A%2520automated%2520program%2520repair%252C%2520large%2520language%2520model%252C%2520machine%2520translation%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.07994v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assessing%20the%20Latent%20Automated%20Program%20Repair%20Capabilities%20of%20Large%0A%20%20Language%20Models%20using%20Round-Trip%20Translation&entry.906535625=Fernando%20Vallecillos%20Ruiz%20and%20Anastasiia%20Grishina%20and%20Max%20Hort%20and%20Leon%20Moonen&entry.1292438233=%20%20Research%20shows%20that%20errors%20in%20natural%20language%20can%20be%20corrected%20by%0Atranslating%20texts%20to%20another%20language%20and%20back%20using%20language%20models.%20We%0Aexplore%20to%20what%20extent%20this%20latent%20correction%20capability%20extends%20to%20Automated%0AProgram%20Repair%20%28APR%29%20by%20investigating%20Round-Trip%20Translation%20%28RTT%29%3A%20translating%0Acode%20from%20one%20programming%20language%20into%20another%20programming%20or%20natural%20language%0Aand%20back%2C%20using%20Large%20Language%20Models%20%28LLMs%29.%20We%20hypothesize%20that%20RTT%20restores%0Apatterns%20most%20commonly%20seen%20in%20the%20LLM%27s%20training%20corpora%20through%20regression%0Atoward%20the%20mean%2C%20replacing%20infrequent%20bugs%20with%20more%20frequent%2C%20natural%2C%0Abug-free%20code.%20To%20test%20this%20hypothesis%2C%20we%20employ%20nine%20LLMs%20and%20four%20common%20APR%0Abenchmarks%20in%20Java%2C%20and%20perform%20a%20detailed%20quantitative%20and%20qualitative%0Aanalysis%20of%20RTT-generated%20patches.%20We%20find%20that%20RTT%20through%20English%20generates%0Aplausible%20patches%20for%20100%20of%20164%20bugs%20with%20GPT-4%20on%20the%20HumanEval-Java%0Abenchmark%2C%20and%2097%20are%20found%20to%20be%20correct%20in%20our%20manual%20assessment.%20Moreover%2C%0ARTT%20uniquely%20generates%20plausible%20patches%20for%2046%20bugs%20that%20were%20missed%20by%20LLMs%0Aspecifically%20fine-tuned%20for%20APR.%20While%20this%20demonstrates%20the%20viability%20of%20RTT%0Afor%20APR%2C%20we%20also%20observe%20limitations%2C%20such%20as%20a%20lower%20overall%20bug%20fix%20rate%20than%0Athe%20state-of-the-art%20and%20diluting%20the%20original%20coding%20style.%20We%20analyze%20the%0Aimpact%20of%20these%20limitations%20and%20discuss%20the%20potential%20of%20using%20RTT%20as%20a%0Acomplementary%20component%20in%20APR%20frameworks.%20A%20replication%20package%20is%20available%0Afor%20download%20from%20https%3A//doi.org/10.5281/zenodo.10500593.%0A%20%20Keywords%3A%20automated%20program%20repair%2C%20large%20language%20model%2C%20machine%20translation%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.07994v2&entry.124074799=Read"},
{"title": "Accelerated Feature Detectors for Visual SLAM: A Comparative Study of\n  FPGA vs GPU", "author": "Ruiqi Ye and Mikel Luj\u00e1n", "abstract": "  Feature detection is a common yet time-consuming module in Simultaneous\nLocalization and Mapping (SLAM) implementations, which are increasingly\ndeployed on power-constrained platforms, such as drones. Graphics Processing\nUnits (GPUs) have been a popular accelerator for computer vision in general,\nand feature detection and SLAM in particular.\n  On the other hand, System-on-Chips (SoCs) with integrated Field Programmable\nGate Array (FPGA) are also widely available. This paper presents the first\nstudy of hardware-accelerated feature detectors considering a Visual SLAM\n(V-SLAM) pipeline. We offer new insights by comparing the best GPU-accelerated\nFAST, Harris, and SuperPoint implementations against the FPGA-accelerated\ncounterparts on modern SoCs (Nvidia Jetson Orin and AMD Versal).\n  The evaluation shows that when using a non-learning-based feature detector\nsuch as FAST and Harris, their GPU implementations, and the GPU-accelerated\nV-SLAM can achieve better run-time performance and energy efficiency than the\nFAST and Harris FPGA implementations as well as the FPGA-accelerated V-SLAM.\nHowever, when considering a learning-based detector such as SuperPoint, its\nFPGA implementation can achieve better run-time performance and energy\nefficiency (up to 3.1$\\times$ and 1.4$\\times$ improvements, respectively) than\nthe GPU implementation. The FPGA-accelerated V-SLAM can also achieve comparable\nrun-time performance compared to the GPU-accelerated V-SLAM, with better FPS in\n2 out of 5 dataset sequences. When considering the accuracy, the results show\nthat the GPU-accelerated V-SLAM is more accurate than the FPGA-accelerated\nV-SLAM in general. Last but not least, the use of hardware acceleration for\nfeature detection could further improve the performance of the V-SLAM pipeline\nby having the global bundle adjustment module invoked less frequently without\nsacrificing accuracy.\n", "link": "http://arxiv.org/abs/2510.13546v1", "date": "2025-10-15", "relevancy": 2.1409, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5865}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5001}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerated%20Feature%20Detectors%20for%20Visual%20SLAM%3A%20A%20Comparative%20Study%20of%0A%20%20FPGA%20vs%20GPU&body=Title%3A%20Accelerated%20Feature%20Detectors%20for%20Visual%20SLAM%3A%20A%20Comparative%20Study%20of%0A%20%20FPGA%20vs%20GPU%0AAuthor%3A%20Ruiqi%20Ye%20and%20Mikel%20Luj%C3%A1n%0AAbstract%3A%20%20%20Feature%20detection%20is%20a%20common%20yet%20time-consuming%20module%20in%20Simultaneous%0ALocalization%20and%20Mapping%20%28SLAM%29%20implementations%2C%20which%20are%20increasingly%0Adeployed%20on%20power-constrained%20platforms%2C%20such%20as%20drones.%20Graphics%20Processing%0AUnits%20%28GPUs%29%20have%20been%20a%20popular%20accelerator%20for%20computer%20vision%20in%20general%2C%0Aand%20feature%20detection%20and%20SLAM%20in%20particular.%0A%20%20On%20the%20other%20hand%2C%20System-on-Chips%20%28SoCs%29%20with%20integrated%20Field%20Programmable%0AGate%20Array%20%28FPGA%29%20are%20also%20widely%20available.%20This%20paper%20presents%20the%20first%0Astudy%20of%20hardware-accelerated%20feature%20detectors%20considering%20a%20Visual%20SLAM%0A%28V-SLAM%29%20pipeline.%20We%20offer%20new%20insights%20by%20comparing%20the%20best%20GPU-accelerated%0AFAST%2C%20Harris%2C%20and%20SuperPoint%20implementations%20against%20the%20FPGA-accelerated%0Acounterparts%20on%20modern%20SoCs%20%28Nvidia%20Jetson%20Orin%20and%20AMD%20Versal%29.%0A%20%20The%20evaluation%20shows%20that%20when%20using%20a%20non-learning-based%20feature%20detector%0Asuch%20as%20FAST%20and%20Harris%2C%20their%20GPU%20implementations%2C%20and%20the%20GPU-accelerated%0AV-SLAM%20can%20achieve%20better%20run-time%20performance%20and%20energy%20efficiency%20than%20the%0AFAST%20and%20Harris%20FPGA%20implementations%20as%20well%20as%20the%20FPGA-accelerated%20V-SLAM.%0AHowever%2C%20when%20considering%20a%20learning-based%20detector%20such%20as%20SuperPoint%2C%20its%0AFPGA%20implementation%20can%20achieve%20better%20run-time%20performance%20and%20energy%0Aefficiency%20%28up%20to%203.1%24%5Ctimes%24%20and%201.4%24%5Ctimes%24%20improvements%2C%20respectively%29%20than%0Athe%20GPU%20implementation.%20The%20FPGA-accelerated%20V-SLAM%20can%20also%20achieve%20comparable%0Arun-time%20performance%20compared%20to%20the%20GPU-accelerated%20V-SLAM%2C%20with%20better%20FPS%20in%0A2%20out%20of%205%20dataset%20sequences.%20When%20considering%20the%20accuracy%2C%20the%20results%20show%0Athat%20the%20GPU-accelerated%20V-SLAM%20is%20more%20accurate%20than%20the%20FPGA-accelerated%0AV-SLAM%20in%20general.%20Last%20but%20not%20least%2C%20the%20use%20of%20hardware%20acceleration%20for%0Afeature%20detection%20could%20further%20improve%20the%20performance%20of%20the%20V-SLAM%20pipeline%0Aby%20having%20the%20global%20bundle%20adjustment%20module%20invoked%20less%20frequently%20without%0Asacrificing%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13546v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerated%2520Feature%2520Detectors%2520for%2520Visual%2520SLAM%253A%2520A%2520Comparative%2520Study%2520of%250A%2520%2520FPGA%2520vs%2520GPU%26entry.906535625%3DRuiqi%2520Ye%2520and%2520Mikel%2520Luj%25C3%25A1n%26entry.1292438233%3D%2520%2520Feature%2520detection%2520is%2520a%2520common%2520yet%2520time-consuming%2520module%2520in%2520Simultaneous%250ALocalization%2520and%2520Mapping%2520%2528SLAM%2529%2520implementations%252C%2520which%2520are%2520increasingly%250Adeployed%2520on%2520power-constrained%2520platforms%252C%2520such%2520as%2520drones.%2520Graphics%2520Processing%250AUnits%2520%2528GPUs%2529%2520have%2520been%2520a%2520popular%2520accelerator%2520for%2520computer%2520vision%2520in%2520general%252C%250Aand%2520feature%2520detection%2520and%2520SLAM%2520in%2520particular.%250A%2520%2520On%2520the%2520other%2520hand%252C%2520System-on-Chips%2520%2528SoCs%2529%2520with%2520integrated%2520Field%2520Programmable%250AGate%2520Array%2520%2528FPGA%2529%2520are%2520also%2520widely%2520available.%2520This%2520paper%2520presents%2520the%2520first%250Astudy%2520of%2520hardware-accelerated%2520feature%2520detectors%2520considering%2520a%2520Visual%2520SLAM%250A%2528V-SLAM%2529%2520pipeline.%2520We%2520offer%2520new%2520insights%2520by%2520comparing%2520the%2520best%2520GPU-accelerated%250AFAST%252C%2520Harris%252C%2520and%2520SuperPoint%2520implementations%2520against%2520the%2520FPGA-accelerated%250Acounterparts%2520on%2520modern%2520SoCs%2520%2528Nvidia%2520Jetson%2520Orin%2520and%2520AMD%2520Versal%2529.%250A%2520%2520The%2520evaluation%2520shows%2520that%2520when%2520using%2520a%2520non-learning-based%2520feature%2520detector%250Asuch%2520as%2520FAST%2520and%2520Harris%252C%2520their%2520GPU%2520implementations%252C%2520and%2520the%2520GPU-accelerated%250AV-SLAM%2520can%2520achieve%2520better%2520run-time%2520performance%2520and%2520energy%2520efficiency%2520than%2520the%250AFAST%2520and%2520Harris%2520FPGA%2520implementations%2520as%2520well%2520as%2520the%2520FPGA-accelerated%2520V-SLAM.%250AHowever%252C%2520when%2520considering%2520a%2520learning-based%2520detector%2520such%2520as%2520SuperPoint%252C%2520its%250AFPGA%2520implementation%2520can%2520achieve%2520better%2520run-time%2520performance%2520and%2520energy%250Aefficiency%2520%2528up%2520to%25203.1%2524%255Ctimes%2524%2520and%25201.4%2524%255Ctimes%2524%2520improvements%252C%2520respectively%2529%2520than%250Athe%2520GPU%2520implementation.%2520The%2520FPGA-accelerated%2520V-SLAM%2520can%2520also%2520achieve%2520comparable%250Arun-time%2520performance%2520compared%2520to%2520the%2520GPU-accelerated%2520V-SLAM%252C%2520with%2520better%2520FPS%2520in%250A2%2520out%2520of%25205%2520dataset%2520sequences.%2520When%2520considering%2520the%2520accuracy%252C%2520the%2520results%2520show%250Athat%2520the%2520GPU-accelerated%2520V-SLAM%2520is%2520more%2520accurate%2520than%2520the%2520FPGA-accelerated%250AV-SLAM%2520in%2520general.%2520Last%2520but%2520not%2520least%252C%2520the%2520use%2520of%2520hardware%2520acceleration%2520for%250Afeature%2520detection%2520could%2520further%2520improve%2520the%2520performance%2520of%2520the%2520V-SLAM%2520pipeline%250Aby%2520having%2520the%2520global%2520bundle%2520adjustment%2520module%2520invoked%2520less%2520frequently%2520without%250Asacrificing%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13546v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerated%20Feature%20Detectors%20for%20Visual%20SLAM%3A%20A%20Comparative%20Study%20of%0A%20%20FPGA%20vs%20GPU&entry.906535625=Ruiqi%20Ye%20and%20Mikel%20Luj%C3%A1n&entry.1292438233=%20%20Feature%20detection%20is%20a%20common%20yet%20time-consuming%20module%20in%20Simultaneous%0ALocalization%20and%20Mapping%20%28SLAM%29%20implementations%2C%20which%20are%20increasingly%0Adeployed%20on%20power-constrained%20platforms%2C%20such%20as%20drones.%20Graphics%20Processing%0AUnits%20%28GPUs%29%20have%20been%20a%20popular%20accelerator%20for%20computer%20vision%20in%20general%2C%0Aand%20feature%20detection%20and%20SLAM%20in%20particular.%0A%20%20On%20the%20other%20hand%2C%20System-on-Chips%20%28SoCs%29%20with%20integrated%20Field%20Programmable%0AGate%20Array%20%28FPGA%29%20are%20also%20widely%20available.%20This%20paper%20presents%20the%20first%0Astudy%20of%20hardware-accelerated%20feature%20detectors%20considering%20a%20Visual%20SLAM%0A%28V-SLAM%29%20pipeline.%20We%20offer%20new%20insights%20by%20comparing%20the%20best%20GPU-accelerated%0AFAST%2C%20Harris%2C%20and%20SuperPoint%20implementations%20against%20the%20FPGA-accelerated%0Acounterparts%20on%20modern%20SoCs%20%28Nvidia%20Jetson%20Orin%20and%20AMD%20Versal%29.%0A%20%20The%20evaluation%20shows%20that%20when%20using%20a%20non-learning-based%20feature%20detector%0Asuch%20as%20FAST%20and%20Harris%2C%20their%20GPU%20implementations%2C%20and%20the%20GPU-accelerated%0AV-SLAM%20can%20achieve%20better%20run-time%20performance%20and%20energy%20efficiency%20than%20the%0AFAST%20and%20Harris%20FPGA%20implementations%20as%20well%20as%20the%20FPGA-accelerated%20V-SLAM.%0AHowever%2C%20when%20considering%20a%20learning-based%20detector%20such%20as%20SuperPoint%2C%20its%0AFPGA%20implementation%20can%20achieve%20better%20run-time%20performance%20and%20energy%0Aefficiency%20%28up%20to%203.1%24%5Ctimes%24%20and%201.4%24%5Ctimes%24%20improvements%2C%20respectively%29%20than%0Athe%20GPU%20implementation.%20The%20FPGA-accelerated%20V-SLAM%20can%20also%20achieve%20comparable%0Arun-time%20performance%20compared%20to%20the%20GPU-accelerated%20V-SLAM%2C%20with%20better%20FPS%20in%0A2%20out%20of%205%20dataset%20sequences.%20When%20considering%20the%20accuracy%2C%20the%20results%20show%0Athat%20the%20GPU-accelerated%20V-SLAM%20is%20more%20accurate%20than%20the%20FPGA-accelerated%0AV-SLAM%20in%20general.%20Last%20but%20not%20least%2C%20the%20use%20of%20hardware%20acceleration%20for%0Afeature%20detection%20could%20further%20improve%20the%20performance%20of%20the%20V-SLAM%20pipeline%0Aby%20having%20the%20global%20bundle%20adjustment%20module%20invoked%20less%20frequently%20without%0Asacrificing%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13546v1&entry.124074799=Read"},
{"title": "Ultralytics YOLO Evolution: An Overview of YOLO26, YOLO11, YOLOv8 and\n  YOLOv5 Object Detectors for Computer Vision and Pattern Recognition", "author": "Ranjan Sapkota and Manoj Karkee", "abstract": "  This paper presents a comprehensive overview of the Ultralytics YOLO(You Only\nLook Once) family of object detectors, focusing the architectural evolution,\nbenchmarking, deployment perspectives, and future challenges. The review begins\nwith the most recent release, YOLO26 (or YOLOv26), which introduces key\ninnovations including Distribution Focal Loss (DFL) removal, native NMS-free\ninference, Progressive Loss Balancing (ProgLoss), Small-Target-Aware Label\nAssignment (STAL), and the MuSGD optimizer for stable training. The progression\nis then traced through YOLO11, with its hybrid task assignment and\nefficiency-focused modules; YOLOv8, which advanced with a decoupled detection\nhead and anchor-free predictions; and YOLOv5, which established the modular\nPyTorch foundation that enabled modern YOLO development. Benchmarking on the MS\nCOCO dataset provides a detailed quantitative comparison of YOLOv5, YOLOv8,\nYOLO11, and YOLO26 (YOLOv26), alongside cross-comparisons with YOLOv12,\nYOLOv13, RT-DETR, and DEIM(DETR with Improved Matching). Metrics including\nprecision, recall, F1 score, mean Average Precision, and inference speed are\nanalyzed to highlight trade-offs between accuracy and efficiency. Deployment\nand application perspectives are further discussed, covering export formats,\nquantization strategies, and real-world use in robotics, agriculture,\nsurveillance, and manufacturing. Finally, the paper identifies challenges and\nfuture directions, including dense-scene limitations, hybrid CNN-Transformer\nintegration, open-vocabulary detection, and edge-aware training approaches.\n(Object Detection, YOLOv26, YOLO)\n", "link": "http://arxiv.org/abs/2510.09653v2", "date": "2025-10-15", "relevancy": 2.1391, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5402}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5337}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5337}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ultralytics%20YOLO%20Evolution%3A%20An%20Overview%20of%20YOLO26%2C%20YOLO11%2C%20YOLOv8%20and%0A%20%20YOLOv5%20Object%20Detectors%20for%20Computer%20Vision%20and%20Pattern%20Recognition&body=Title%3A%20Ultralytics%20YOLO%20Evolution%3A%20An%20Overview%20of%20YOLO26%2C%20YOLO11%2C%20YOLOv8%20and%0A%20%20YOLOv5%20Object%20Detectors%20for%20Computer%20Vision%20and%20Pattern%20Recognition%0AAuthor%3A%20Ranjan%20Sapkota%20and%20Manoj%20Karkee%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20comprehensive%20overview%20of%20the%20Ultralytics%20YOLO%28You%20Only%0ALook%20Once%29%20family%20of%20object%20detectors%2C%20focusing%20the%20architectural%20evolution%2C%0Abenchmarking%2C%20deployment%20perspectives%2C%20and%20future%20challenges.%20The%20review%20begins%0Awith%20the%20most%20recent%20release%2C%20YOLO26%20%28or%20YOLOv26%29%2C%20which%20introduces%20key%0Ainnovations%20including%20Distribution%20Focal%20Loss%20%28DFL%29%20removal%2C%20native%20NMS-free%0Ainference%2C%20Progressive%20Loss%20Balancing%20%28ProgLoss%29%2C%20Small-Target-Aware%20Label%0AAssignment%20%28STAL%29%2C%20and%20the%20MuSGD%20optimizer%20for%20stable%20training.%20The%20progression%0Ais%20then%20traced%20through%20YOLO11%2C%20with%20its%20hybrid%20task%20assignment%20and%0Aefficiency-focused%20modules%3B%20YOLOv8%2C%20which%20advanced%20with%20a%20decoupled%20detection%0Ahead%20and%20anchor-free%20predictions%3B%20and%20YOLOv5%2C%20which%20established%20the%20modular%0APyTorch%20foundation%20that%20enabled%20modern%20YOLO%20development.%20Benchmarking%20on%20the%20MS%0ACOCO%20dataset%20provides%20a%20detailed%20quantitative%20comparison%20of%20YOLOv5%2C%20YOLOv8%2C%0AYOLO11%2C%20and%20YOLO26%20%28YOLOv26%29%2C%20alongside%20cross-comparisons%20with%20YOLOv12%2C%0AYOLOv13%2C%20RT-DETR%2C%20and%20DEIM%28DETR%20with%20Improved%20Matching%29.%20Metrics%20including%0Aprecision%2C%20recall%2C%20F1%20score%2C%20mean%20Average%20Precision%2C%20and%20inference%20speed%20are%0Aanalyzed%20to%20highlight%20trade-offs%20between%20accuracy%20and%20efficiency.%20Deployment%0Aand%20application%20perspectives%20are%20further%20discussed%2C%20covering%20export%20formats%2C%0Aquantization%20strategies%2C%20and%20real-world%20use%20in%20robotics%2C%20agriculture%2C%0Asurveillance%2C%20and%20manufacturing.%20Finally%2C%20the%20paper%20identifies%20challenges%20and%0Afuture%20directions%2C%20including%20dense-scene%20limitations%2C%20hybrid%20CNN-Transformer%0Aintegration%2C%20open-vocabulary%20detection%2C%20and%20edge-aware%20training%20approaches.%0A%28Object%20Detection%2C%20YOLOv26%2C%20YOLO%29%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09653v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUltralytics%2520YOLO%2520Evolution%253A%2520An%2520Overview%2520of%2520YOLO26%252C%2520YOLO11%252C%2520YOLOv8%2520and%250A%2520%2520YOLOv5%2520Object%2520Detectors%2520for%2520Computer%2520Vision%2520and%2520Pattern%2520Recognition%26entry.906535625%3DRanjan%2520Sapkota%2520and%2520Manoj%2520Karkee%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520comprehensive%2520overview%2520of%2520the%2520Ultralytics%2520YOLO%2528You%2520Only%250ALook%2520Once%2529%2520family%2520of%2520object%2520detectors%252C%2520focusing%2520the%2520architectural%2520evolution%252C%250Abenchmarking%252C%2520deployment%2520perspectives%252C%2520and%2520future%2520challenges.%2520The%2520review%2520begins%250Awith%2520the%2520most%2520recent%2520release%252C%2520YOLO26%2520%2528or%2520YOLOv26%2529%252C%2520which%2520introduces%2520key%250Ainnovations%2520including%2520Distribution%2520Focal%2520Loss%2520%2528DFL%2529%2520removal%252C%2520native%2520NMS-free%250Ainference%252C%2520Progressive%2520Loss%2520Balancing%2520%2528ProgLoss%2529%252C%2520Small-Target-Aware%2520Label%250AAssignment%2520%2528STAL%2529%252C%2520and%2520the%2520MuSGD%2520optimizer%2520for%2520stable%2520training.%2520The%2520progression%250Ais%2520then%2520traced%2520through%2520YOLO11%252C%2520with%2520its%2520hybrid%2520task%2520assignment%2520and%250Aefficiency-focused%2520modules%253B%2520YOLOv8%252C%2520which%2520advanced%2520with%2520a%2520decoupled%2520detection%250Ahead%2520and%2520anchor-free%2520predictions%253B%2520and%2520YOLOv5%252C%2520which%2520established%2520the%2520modular%250APyTorch%2520foundation%2520that%2520enabled%2520modern%2520YOLO%2520development.%2520Benchmarking%2520on%2520the%2520MS%250ACOCO%2520dataset%2520provides%2520a%2520detailed%2520quantitative%2520comparison%2520of%2520YOLOv5%252C%2520YOLOv8%252C%250AYOLO11%252C%2520and%2520YOLO26%2520%2528YOLOv26%2529%252C%2520alongside%2520cross-comparisons%2520with%2520YOLOv12%252C%250AYOLOv13%252C%2520RT-DETR%252C%2520and%2520DEIM%2528DETR%2520with%2520Improved%2520Matching%2529.%2520Metrics%2520including%250Aprecision%252C%2520recall%252C%2520F1%2520score%252C%2520mean%2520Average%2520Precision%252C%2520and%2520inference%2520speed%2520are%250Aanalyzed%2520to%2520highlight%2520trade-offs%2520between%2520accuracy%2520and%2520efficiency.%2520Deployment%250Aand%2520application%2520perspectives%2520are%2520further%2520discussed%252C%2520covering%2520export%2520formats%252C%250Aquantization%2520strategies%252C%2520and%2520real-world%2520use%2520in%2520robotics%252C%2520agriculture%252C%250Asurveillance%252C%2520and%2520manufacturing.%2520Finally%252C%2520the%2520paper%2520identifies%2520challenges%2520and%250Afuture%2520directions%252C%2520including%2520dense-scene%2520limitations%252C%2520hybrid%2520CNN-Transformer%250Aintegration%252C%2520open-vocabulary%2520detection%252C%2520and%2520edge-aware%2520training%2520approaches.%250A%2528Object%2520Detection%252C%2520YOLOv26%252C%2520YOLO%2529%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09653v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ultralytics%20YOLO%20Evolution%3A%20An%20Overview%20of%20YOLO26%2C%20YOLO11%2C%20YOLOv8%20and%0A%20%20YOLOv5%20Object%20Detectors%20for%20Computer%20Vision%20and%20Pattern%20Recognition&entry.906535625=Ranjan%20Sapkota%20and%20Manoj%20Karkee&entry.1292438233=%20%20This%20paper%20presents%20a%20comprehensive%20overview%20of%20the%20Ultralytics%20YOLO%28You%20Only%0ALook%20Once%29%20family%20of%20object%20detectors%2C%20focusing%20the%20architectural%20evolution%2C%0Abenchmarking%2C%20deployment%20perspectives%2C%20and%20future%20challenges.%20The%20review%20begins%0Awith%20the%20most%20recent%20release%2C%20YOLO26%20%28or%20YOLOv26%29%2C%20which%20introduces%20key%0Ainnovations%20including%20Distribution%20Focal%20Loss%20%28DFL%29%20removal%2C%20native%20NMS-free%0Ainference%2C%20Progressive%20Loss%20Balancing%20%28ProgLoss%29%2C%20Small-Target-Aware%20Label%0AAssignment%20%28STAL%29%2C%20and%20the%20MuSGD%20optimizer%20for%20stable%20training.%20The%20progression%0Ais%20then%20traced%20through%20YOLO11%2C%20with%20its%20hybrid%20task%20assignment%20and%0Aefficiency-focused%20modules%3B%20YOLOv8%2C%20which%20advanced%20with%20a%20decoupled%20detection%0Ahead%20and%20anchor-free%20predictions%3B%20and%20YOLOv5%2C%20which%20established%20the%20modular%0APyTorch%20foundation%20that%20enabled%20modern%20YOLO%20development.%20Benchmarking%20on%20the%20MS%0ACOCO%20dataset%20provides%20a%20detailed%20quantitative%20comparison%20of%20YOLOv5%2C%20YOLOv8%2C%0AYOLO11%2C%20and%20YOLO26%20%28YOLOv26%29%2C%20alongside%20cross-comparisons%20with%20YOLOv12%2C%0AYOLOv13%2C%20RT-DETR%2C%20and%20DEIM%28DETR%20with%20Improved%20Matching%29.%20Metrics%20including%0Aprecision%2C%20recall%2C%20F1%20score%2C%20mean%20Average%20Precision%2C%20and%20inference%20speed%20are%0Aanalyzed%20to%20highlight%20trade-offs%20between%20accuracy%20and%20efficiency.%20Deployment%0Aand%20application%20perspectives%20are%20further%20discussed%2C%20covering%20export%20formats%2C%0Aquantization%20strategies%2C%20and%20real-world%20use%20in%20robotics%2C%20agriculture%2C%0Asurveillance%2C%20and%20manufacturing.%20Finally%2C%20the%20paper%20identifies%20challenges%20and%0Afuture%20directions%2C%20including%20dense-scene%20limitations%2C%20hybrid%20CNN-Transformer%0Aintegration%2C%20open-vocabulary%20detection%2C%20and%20edge-aware%20training%20approaches.%0A%28Object%20Detection%2C%20YOLOv26%2C%20YOLO%29%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09653v2&entry.124074799=Read"},
{"title": "Active Tactile Exploration for Rigid Body Pose and Shape Estimation", "author": "Ethan K. Gordon and Bruke Baraki and Hien Bui and Michael Posa", "abstract": "  General robot manipulation requires the handling of previously unseen\nobjects. Learning a physically accurate model at test time can provide\nsignificant benefits in data efficiency, predictability, and reuse between\ntasks. Tactile sensing can compliment vision with its robustness to occlusion,\nbut its temporal sparsity necessitates careful online exploration to maintain\ndata efficiency. Direct contact can also cause an unrestrained object to move,\nrequiring both shape and location estimation. In this work, we propose a\nlearning and exploration framework that uses only tactile data to\nsimultaneously determine the shape and location of rigid objects with minimal\nrobot motion. We build on recent advances in contact-rich system identification\nto formulate a loss function that penalizes physical constraint violation\nwithout introducing the numerical stiffness inherent in rigid-body contact.\nOptimizing this loss, we can learn cuboid and convex polyhedral geometries with\nless than 10s of randomly collected data after first contact. Our exploration\nscheme seeks to maximize Expected Information Gain and results in significantly\nfaster learning in both simulated and real-robot experiments. More information\ncan be found at https://dairlab.github.io/activetactile\n", "link": "http://arxiv.org/abs/2510.13595v1", "date": "2025-10-15", "relevancy": 1.7754, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6456}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6257}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Tactile%20Exploration%20for%20Rigid%20Body%20Pose%20and%20Shape%20Estimation&body=Title%3A%20Active%20Tactile%20Exploration%20for%20Rigid%20Body%20Pose%20and%20Shape%20Estimation%0AAuthor%3A%20Ethan%20K.%20Gordon%20and%20Bruke%20Baraki%20and%20Hien%20Bui%20and%20Michael%20Posa%0AAbstract%3A%20%20%20General%20robot%20manipulation%20requires%20the%20handling%20of%20previously%20unseen%0Aobjects.%20Learning%20a%20physically%20accurate%20model%20at%20test%20time%20can%20provide%0Asignificant%20benefits%20in%20data%20efficiency%2C%20predictability%2C%20and%20reuse%20between%0Atasks.%20Tactile%20sensing%20can%20compliment%20vision%20with%20its%20robustness%20to%20occlusion%2C%0Abut%20its%20temporal%20sparsity%20necessitates%20careful%20online%20exploration%20to%20maintain%0Adata%20efficiency.%20Direct%20contact%20can%20also%20cause%20an%20unrestrained%20object%20to%20move%2C%0Arequiring%20both%20shape%20and%20location%20estimation.%20In%20this%20work%2C%20we%20propose%20a%0Alearning%20and%20exploration%20framework%20that%20uses%20only%20tactile%20data%20to%0Asimultaneously%20determine%20the%20shape%20and%20location%20of%20rigid%20objects%20with%20minimal%0Arobot%20motion.%20We%20build%20on%20recent%20advances%20in%20contact-rich%20system%20identification%0Ato%20formulate%20a%20loss%20function%20that%20penalizes%20physical%20constraint%20violation%0Awithout%20introducing%20the%20numerical%20stiffness%20inherent%20in%20rigid-body%20contact.%0AOptimizing%20this%20loss%2C%20we%20can%20learn%20cuboid%20and%20convex%20polyhedral%20geometries%20with%0Aless%20than%2010s%20of%20randomly%20collected%20data%20after%20first%20contact.%20Our%20exploration%0Ascheme%20seeks%20to%20maximize%20Expected%20Information%20Gain%20and%20results%20in%20significantly%0Afaster%20learning%20in%20both%20simulated%20and%20real-robot%20experiments.%20More%20information%0Acan%20be%20found%20at%20https%3A//dairlab.github.io/activetactile%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13595v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Tactile%2520Exploration%2520for%2520Rigid%2520Body%2520Pose%2520and%2520Shape%2520Estimation%26entry.906535625%3DEthan%2520K.%2520Gordon%2520and%2520Bruke%2520Baraki%2520and%2520Hien%2520Bui%2520and%2520Michael%2520Posa%26entry.1292438233%3D%2520%2520General%2520robot%2520manipulation%2520requires%2520the%2520handling%2520of%2520previously%2520unseen%250Aobjects.%2520Learning%2520a%2520physically%2520accurate%2520model%2520at%2520test%2520time%2520can%2520provide%250Asignificant%2520benefits%2520in%2520data%2520efficiency%252C%2520predictability%252C%2520and%2520reuse%2520between%250Atasks.%2520Tactile%2520sensing%2520can%2520compliment%2520vision%2520with%2520its%2520robustness%2520to%2520occlusion%252C%250Abut%2520its%2520temporal%2520sparsity%2520necessitates%2520careful%2520online%2520exploration%2520to%2520maintain%250Adata%2520efficiency.%2520Direct%2520contact%2520can%2520also%2520cause%2520an%2520unrestrained%2520object%2520to%2520move%252C%250Arequiring%2520both%2520shape%2520and%2520location%2520estimation.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%250Alearning%2520and%2520exploration%2520framework%2520that%2520uses%2520only%2520tactile%2520data%2520to%250Asimultaneously%2520determine%2520the%2520shape%2520and%2520location%2520of%2520rigid%2520objects%2520with%2520minimal%250Arobot%2520motion.%2520We%2520build%2520on%2520recent%2520advances%2520in%2520contact-rich%2520system%2520identification%250Ato%2520formulate%2520a%2520loss%2520function%2520that%2520penalizes%2520physical%2520constraint%2520violation%250Awithout%2520introducing%2520the%2520numerical%2520stiffness%2520inherent%2520in%2520rigid-body%2520contact.%250AOptimizing%2520this%2520loss%252C%2520we%2520can%2520learn%2520cuboid%2520and%2520convex%2520polyhedral%2520geometries%2520with%250Aless%2520than%252010s%2520of%2520randomly%2520collected%2520data%2520after%2520first%2520contact.%2520Our%2520exploration%250Ascheme%2520seeks%2520to%2520maximize%2520Expected%2520Information%2520Gain%2520and%2520results%2520in%2520significantly%250Afaster%2520learning%2520in%2520both%2520simulated%2520and%2520real-robot%2520experiments.%2520More%2520information%250Acan%2520be%2520found%2520at%2520https%253A//dairlab.github.io/activetactile%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13595v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Tactile%20Exploration%20for%20Rigid%20Body%20Pose%20and%20Shape%20Estimation&entry.906535625=Ethan%20K.%20Gordon%20and%20Bruke%20Baraki%20and%20Hien%20Bui%20and%20Michael%20Posa&entry.1292438233=%20%20General%20robot%20manipulation%20requires%20the%20handling%20of%20previously%20unseen%0Aobjects.%20Learning%20a%20physically%20accurate%20model%20at%20test%20time%20can%20provide%0Asignificant%20benefits%20in%20data%20efficiency%2C%20predictability%2C%20and%20reuse%20between%0Atasks.%20Tactile%20sensing%20can%20compliment%20vision%20with%20its%20robustness%20to%20occlusion%2C%0Abut%20its%20temporal%20sparsity%20necessitates%20careful%20online%20exploration%20to%20maintain%0Adata%20efficiency.%20Direct%20contact%20can%20also%20cause%20an%20unrestrained%20object%20to%20move%2C%0Arequiring%20both%20shape%20and%20location%20estimation.%20In%20this%20work%2C%20we%20propose%20a%0Alearning%20and%20exploration%20framework%20that%20uses%20only%20tactile%20data%20to%0Asimultaneously%20determine%20the%20shape%20and%20location%20of%20rigid%20objects%20with%20minimal%0Arobot%20motion.%20We%20build%20on%20recent%20advances%20in%20contact-rich%20system%20identification%0Ato%20formulate%20a%20loss%20function%20that%20penalizes%20physical%20constraint%20violation%0Awithout%20introducing%20the%20numerical%20stiffness%20inherent%20in%20rigid-body%20contact.%0AOptimizing%20this%20loss%2C%20we%20can%20learn%20cuboid%20and%20convex%20polyhedral%20geometries%20with%0Aless%20than%2010s%20of%20randomly%20collected%20data%20after%20first%20contact.%20Our%20exploration%0Ascheme%20seeks%20to%20maximize%20Expected%20Information%20Gain%20and%20results%20in%20significantly%0Afaster%20learning%20in%20both%20simulated%20and%20real-robot%20experiments.%20More%20information%0Acan%20be%20found%20at%20https%3A//dairlab.github.io/activetactile%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13595v1&entry.124074799=Read"},
{"title": "$L_2$-Regularized Empirical Risk Minimization Guarantees Small Smooth\n  Calibration Error", "author": "Masahiro Fujisawa and Futoshi Futami", "abstract": "  Calibration of predicted probabilities is critical for reliable machine\nlearning, yet it is poorly understood how standard training procedures yield\nwell-calibrated models. This work provides the first theoretical proof that\ncanonical $L_{2}$-regularized empirical risk minimization directly controls the\nsmooth calibration error (smCE) without post-hoc correction or specialized\ncalibration-promoting regularizer. We establish finite-sample generalization\nbounds for smCE based on optimization error, regularization strength, and the\nRademacher complexity. We then instantiate this theory for models in\nreproducing kernel Hilbert spaces, deriving concrete guarantees for kernel\nridge and logistic regression. Our experiments confirm these specific\nguarantees, demonstrating that $L_{2}$-regularized ERM can provide a\nwell-calibrated model without boosting or post-hoc recalibration. The source\ncode to reproduce all experiments is available at\nhttps://github.com/msfuji0211/erm_calibration.\n", "link": "http://arxiv.org/abs/2510.13450v1", "date": "2025-10-15", "relevancy": 1.9082, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.498}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4765}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4692}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24L_2%24-Regularized%20Empirical%20Risk%20Minimization%20Guarantees%20Small%20Smooth%0A%20%20Calibration%20Error&body=Title%3A%20%24L_2%24-Regularized%20Empirical%20Risk%20Minimization%20Guarantees%20Small%20Smooth%0A%20%20Calibration%20Error%0AAuthor%3A%20Masahiro%20Fujisawa%20and%20Futoshi%20Futami%0AAbstract%3A%20%20%20Calibration%20of%20predicted%20probabilities%20is%20critical%20for%20reliable%20machine%0Alearning%2C%20yet%20it%20is%20poorly%20understood%20how%20standard%20training%20procedures%20yield%0Awell-calibrated%20models.%20This%20work%20provides%20the%20first%20theoretical%20proof%20that%0Acanonical%20%24L_%7B2%7D%24-regularized%20empirical%20risk%20minimization%20directly%20controls%20the%0Asmooth%20calibration%20error%20%28smCE%29%20without%20post-hoc%20correction%20or%20specialized%0Acalibration-promoting%20regularizer.%20We%20establish%20finite-sample%20generalization%0Abounds%20for%20smCE%20based%20on%20optimization%20error%2C%20regularization%20strength%2C%20and%20the%0ARademacher%20complexity.%20We%20then%20instantiate%20this%20theory%20for%20models%20in%0Areproducing%20kernel%20Hilbert%20spaces%2C%20deriving%20concrete%20guarantees%20for%20kernel%0Aridge%20and%20logistic%20regression.%20Our%20experiments%20confirm%20these%20specific%0Aguarantees%2C%20demonstrating%20that%20%24L_%7B2%7D%24-regularized%20ERM%20can%20provide%20a%0Awell-calibrated%20model%20without%20boosting%20or%20post-hoc%20recalibration.%20The%20source%0Acode%20to%20reproduce%20all%20experiments%20is%20available%20at%0Ahttps%3A//github.com/msfuji0211/erm_calibration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13450v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524L_2%2524-Regularized%2520Empirical%2520Risk%2520Minimization%2520Guarantees%2520Small%2520Smooth%250A%2520%2520Calibration%2520Error%26entry.906535625%3DMasahiro%2520Fujisawa%2520and%2520Futoshi%2520Futami%26entry.1292438233%3D%2520%2520Calibration%2520of%2520predicted%2520probabilities%2520is%2520critical%2520for%2520reliable%2520machine%250Alearning%252C%2520yet%2520it%2520is%2520poorly%2520understood%2520how%2520standard%2520training%2520procedures%2520yield%250Awell-calibrated%2520models.%2520This%2520work%2520provides%2520the%2520first%2520theoretical%2520proof%2520that%250Acanonical%2520%2524L_%257B2%257D%2524-regularized%2520empirical%2520risk%2520minimization%2520directly%2520controls%2520the%250Asmooth%2520calibration%2520error%2520%2528smCE%2529%2520without%2520post-hoc%2520correction%2520or%2520specialized%250Acalibration-promoting%2520regularizer.%2520We%2520establish%2520finite-sample%2520generalization%250Abounds%2520for%2520smCE%2520based%2520on%2520optimization%2520error%252C%2520regularization%2520strength%252C%2520and%2520the%250ARademacher%2520complexity.%2520We%2520then%2520instantiate%2520this%2520theory%2520for%2520models%2520in%250Areproducing%2520kernel%2520Hilbert%2520spaces%252C%2520deriving%2520concrete%2520guarantees%2520for%2520kernel%250Aridge%2520and%2520logistic%2520regression.%2520Our%2520experiments%2520confirm%2520these%2520specific%250Aguarantees%252C%2520demonstrating%2520that%2520%2524L_%257B2%257D%2524-regularized%2520ERM%2520can%2520provide%2520a%250Awell-calibrated%2520model%2520without%2520boosting%2520or%2520post-hoc%2520recalibration.%2520The%2520source%250Acode%2520to%2520reproduce%2520all%2520experiments%2520is%2520available%2520at%250Ahttps%253A//github.com/msfuji0211/erm_calibration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13450v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24L_2%24-Regularized%20Empirical%20Risk%20Minimization%20Guarantees%20Small%20Smooth%0A%20%20Calibration%20Error&entry.906535625=Masahiro%20Fujisawa%20and%20Futoshi%20Futami&entry.1292438233=%20%20Calibration%20of%20predicted%20probabilities%20is%20critical%20for%20reliable%20machine%0Alearning%2C%20yet%20it%20is%20poorly%20understood%20how%20standard%20training%20procedures%20yield%0Awell-calibrated%20models.%20This%20work%20provides%20the%20first%20theoretical%20proof%20that%0Acanonical%20%24L_%7B2%7D%24-regularized%20empirical%20risk%20minimization%20directly%20controls%20the%0Asmooth%20calibration%20error%20%28smCE%29%20without%20post-hoc%20correction%20or%20specialized%0Acalibration-promoting%20regularizer.%20We%20establish%20finite-sample%20generalization%0Abounds%20for%20smCE%20based%20on%20optimization%20error%2C%20regularization%20strength%2C%20and%20the%0ARademacher%20complexity.%20We%20then%20instantiate%20this%20theory%20for%20models%20in%0Areproducing%20kernel%20Hilbert%20spaces%2C%20deriving%20concrete%20guarantees%20for%20kernel%0Aridge%20and%20logistic%20regression.%20Our%20experiments%20confirm%20these%20specific%0Aguarantees%2C%20demonstrating%20that%20%24L_%7B2%7D%24-regularized%20ERM%20can%20provide%20a%0Awell-calibrated%20model%20without%20boosting%20or%20post-hoc%20recalibration.%20The%20source%0Acode%20to%20reproduce%20all%20experiments%20is%20available%20at%0Ahttps%3A//github.com/msfuji0211/erm_calibration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13450v1&entry.124074799=Read"},
{"title": "T3former: Temporal Graph Classification with Topological Machine\n  Learning", "author": "Md. Joshem Uddin and Soham Changani and Baris Coskunuzer", "abstract": "  Temporal graph classification plays a critical role in applications such as\ncybersecurity, brain connectivity analysis, social dynamics, and traffic\nmonitoring. Despite its significance, this problem remains underexplored\ncompared to temporal link prediction or node forecasting. Existing methods\noften rely on snapshot-based or recurrent architectures that either lose\nfine-grained temporal information or struggle with long-range dependencies.\nMoreover, local message-passing approaches suffer from oversmoothing and\noversquashing, limiting their ability to capture complex temporal structures.\n  We introduce T3former, a novel Topological Temporal Transformer that\nleverages sliding-window topological and spectral descriptors as first-class\ntokens, integrated via a specialized Descriptor-Attention mechanism. This\ndesign preserves temporal fidelity, enhances robustness, and enables principled\ncross-modal fusion without rigid discretization. T3former achieves\nstate-of-the-art performance across multiple benchmarks, including dynamic\nsocial networks, brain functional connectivity datasets, and traffic networks.\nIt also offers theoretical guarantees of stability under temporal and\nstructural perturbations. Our results highlight the power of combining\ntopological and spectral insights for advancing the frontier of temporal graph\nlearning.\n", "link": "http://arxiv.org/abs/2510.13789v1", "date": "2025-10-15", "relevancy": 2.1087, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5699}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5122}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4904}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20T3former%3A%20Temporal%20Graph%20Classification%20with%20Topological%20Machine%0A%20%20Learning&body=Title%3A%20T3former%3A%20Temporal%20Graph%20Classification%20with%20Topological%20Machine%0A%20%20Learning%0AAuthor%3A%20Md.%20Joshem%20Uddin%20and%20Soham%20Changani%20and%20Baris%20Coskunuzer%0AAbstract%3A%20%20%20Temporal%20graph%20classification%20plays%20a%20critical%20role%20in%20applications%20such%20as%0Acybersecurity%2C%20brain%20connectivity%20analysis%2C%20social%20dynamics%2C%20and%20traffic%0Amonitoring.%20Despite%20its%20significance%2C%20this%20problem%20remains%20underexplored%0Acompared%20to%20temporal%20link%20prediction%20or%20node%20forecasting.%20Existing%20methods%0Aoften%20rely%20on%20snapshot-based%20or%20recurrent%20architectures%20that%20either%20lose%0Afine-grained%20temporal%20information%20or%20struggle%20with%20long-range%20dependencies.%0AMoreover%2C%20local%20message-passing%20approaches%20suffer%20from%20oversmoothing%20and%0Aoversquashing%2C%20limiting%20their%20ability%20to%20capture%20complex%20temporal%20structures.%0A%20%20We%20introduce%20T3former%2C%20a%20novel%20Topological%20Temporal%20Transformer%20that%0Aleverages%20sliding-window%20topological%20and%20spectral%20descriptors%20as%20first-class%0Atokens%2C%20integrated%20via%20a%20specialized%20Descriptor-Attention%20mechanism.%20This%0Adesign%20preserves%20temporal%20fidelity%2C%20enhances%20robustness%2C%20and%20enables%20principled%0Across-modal%20fusion%20without%20rigid%20discretization.%20T3former%20achieves%0Astate-of-the-art%20performance%20across%20multiple%20benchmarks%2C%20including%20dynamic%0Asocial%20networks%2C%20brain%20functional%20connectivity%20datasets%2C%20and%20traffic%20networks.%0AIt%20also%20offers%20theoretical%20guarantees%20of%20stability%20under%20temporal%20and%0Astructural%20perturbations.%20Our%20results%20highlight%20the%20power%20of%20combining%0Atopological%20and%20spectral%20insights%20for%20advancing%20the%20frontier%20of%20temporal%20graph%0Alearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13789v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DT3former%253A%2520Temporal%2520Graph%2520Classification%2520with%2520Topological%2520Machine%250A%2520%2520Learning%26entry.906535625%3DMd.%2520Joshem%2520Uddin%2520and%2520Soham%2520Changani%2520and%2520Baris%2520Coskunuzer%26entry.1292438233%3D%2520%2520Temporal%2520graph%2520classification%2520plays%2520a%2520critical%2520role%2520in%2520applications%2520such%2520as%250Acybersecurity%252C%2520brain%2520connectivity%2520analysis%252C%2520social%2520dynamics%252C%2520and%2520traffic%250Amonitoring.%2520Despite%2520its%2520significance%252C%2520this%2520problem%2520remains%2520underexplored%250Acompared%2520to%2520temporal%2520link%2520prediction%2520or%2520node%2520forecasting.%2520Existing%2520methods%250Aoften%2520rely%2520on%2520snapshot-based%2520or%2520recurrent%2520architectures%2520that%2520either%2520lose%250Afine-grained%2520temporal%2520information%2520or%2520struggle%2520with%2520long-range%2520dependencies.%250AMoreover%252C%2520local%2520message-passing%2520approaches%2520suffer%2520from%2520oversmoothing%2520and%250Aoversquashing%252C%2520limiting%2520their%2520ability%2520to%2520capture%2520complex%2520temporal%2520structures.%250A%2520%2520We%2520introduce%2520T3former%252C%2520a%2520novel%2520Topological%2520Temporal%2520Transformer%2520that%250Aleverages%2520sliding-window%2520topological%2520and%2520spectral%2520descriptors%2520as%2520first-class%250Atokens%252C%2520integrated%2520via%2520a%2520specialized%2520Descriptor-Attention%2520mechanism.%2520This%250Adesign%2520preserves%2520temporal%2520fidelity%252C%2520enhances%2520robustness%252C%2520and%2520enables%2520principled%250Across-modal%2520fusion%2520without%2520rigid%2520discretization.%2520T3former%2520achieves%250Astate-of-the-art%2520performance%2520across%2520multiple%2520benchmarks%252C%2520including%2520dynamic%250Asocial%2520networks%252C%2520brain%2520functional%2520connectivity%2520datasets%252C%2520and%2520traffic%2520networks.%250AIt%2520also%2520offers%2520theoretical%2520guarantees%2520of%2520stability%2520under%2520temporal%2520and%250Astructural%2520perturbations.%2520Our%2520results%2520highlight%2520the%2520power%2520of%2520combining%250Atopological%2520and%2520spectral%2520insights%2520for%2520advancing%2520the%2520frontier%2520of%2520temporal%2520graph%250Alearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13789v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T3former%3A%20Temporal%20Graph%20Classification%20with%20Topological%20Machine%0A%20%20Learning&entry.906535625=Md.%20Joshem%20Uddin%20and%20Soham%20Changani%20and%20Baris%20Coskunuzer&entry.1292438233=%20%20Temporal%20graph%20classification%20plays%20a%20critical%20role%20in%20applications%20such%20as%0Acybersecurity%2C%20brain%20connectivity%20analysis%2C%20social%20dynamics%2C%20and%20traffic%0Amonitoring.%20Despite%20its%20significance%2C%20this%20problem%20remains%20underexplored%0Acompared%20to%20temporal%20link%20prediction%20or%20node%20forecasting.%20Existing%20methods%0Aoften%20rely%20on%20snapshot-based%20or%20recurrent%20architectures%20that%20either%20lose%0Afine-grained%20temporal%20information%20or%20struggle%20with%20long-range%20dependencies.%0AMoreover%2C%20local%20message-passing%20approaches%20suffer%20from%20oversmoothing%20and%0Aoversquashing%2C%20limiting%20their%20ability%20to%20capture%20complex%20temporal%20structures.%0A%20%20We%20introduce%20T3former%2C%20a%20novel%20Topological%20Temporal%20Transformer%20that%0Aleverages%20sliding-window%20topological%20and%20spectral%20descriptors%20as%20first-class%0Atokens%2C%20integrated%20via%20a%20specialized%20Descriptor-Attention%20mechanism.%20This%0Adesign%20preserves%20temporal%20fidelity%2C%20enhances%20robustness%2C%20and%20enables%20principled%0Across-modal%20fusion%20without%20rigid%20discretization.%20T3former%20achieves%0Astate-of-the-art%20performance%20across%20multiple%20benchmarks%2C%20including%20dynamic%0Asocial%20networks%2C%20brain%20functional%20connectivity%20datasets%2C%20and%20traffic%20networks.%0AIt%20also%20offers%20theoretical%20guarantees%20of%20stability%20under%20temporal%20and%0Astructural%20perturbations.%20Our%20results%20highlight%20the%20power%20of%20combining%0Atopological%20and%20spectral%20insights%20for%20advancing%20the%20frontier%20of%20temporal%20graph%0Alearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13789v1&entry.124074799=Read"},
{"title": "FMANet: A Novel Dual-Phase Optical Flow Approach with Fusion Motion\n  Attention Network for Robust Micro-expression Recognition", "author": "Luu Tu Nguyen and Vu Tram Anh Khuong and Thi Bich Phuong Man and Thi Duyen Ngo and Thanh Ha Le", "abstract": "  Facial micro-expressions, characterized by their subtle and brief nature, are\nvaluable indicators of genuine emotions. Despite their significance in\npsychology, security, and behavioral analysis, micro-expression recognition\nremains challenging due to the difficulty of capturing subtle facial movements.\nOptical flow has been widely employed as an input modality for this task due to\nits effectiveness. However, most existing methods compute optical flow only\nbetween the onset and apex frames, thereby overlooking essential motion\ninformation in the apex-to-offset phase. To address this limitation, we first\nintroduce a comprehensive motion representation, termed Magnitude-Modulated\nCombined Optical Flow (MM-COF), which integrates motion dynamics from both\nmicro-expression phases into a unified descriptor suitable for direct use in\nrecognition networks. Building upon this principle, we then propose FMANet, a\nnovel end-to-end neural network architecture that internalizes the dual-phase\nanalysis and magnitude modulation into learnable modules. This allows the\nnetwork to adaptively fuse motion cues and focus on salient facial regions for\nclassification. Experimental evaluations on the MMEW, SMIC, CASME-II, and SAMM\ndatasets, widely recognized as standard benchmarks, demonstrate that our\nproposed MM-COF representation and FMANet outperforms existing methods,\nunderscoring the potential of a learnable, dual-phase framework in advancing\nmicro-expression recognition.\n", "link": "http://arxiv.org/abs/2510.07810v3", "date": "2025-10-15", "relevancy": 2.1149, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5345}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5333}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5218}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FMANet%3A%20A%20Novel%20Dual-Phase%20Optical%20Flow%20Approach%20with%20Fusion%20Motion%0A%20%20Attention%20Network%20for%20Robust%20Micro-expression%20Recognition&body=Title%3A%20FMANet%3A%20A%20Novel%20Dual-Phase%20Optical%20Flow%20Approach%20with%20Fusion%20Motion%0A%20%20Attention%20Network%20for%20Robust%20Micro-expression%20Recognition%0AAuthor%3A%20Luu%20Tu%20Nguyen%20and%20Vu%20Tram%20Anh%20Khuong%20and%20Thi%20Bich%20Phuong%20Man%20and%20Thi%20Duyen%20Ngo%20and%20Thanh%20Ha%20Le%0AAbstract%3A%20%20%20Facial%20micro-expressions%2C%20characterized%20by%20their%20subtle%20and%20brief%20nature%2C%20are%0Avaluable%20indicators%20of%20genuine%20emotions.%20Despite%20their%20significance%20in%0Apsychology%2C%20security%2C%20and%20behavioral%20analysis%2C%20micro-expression%20recognition%0Aremains%20challenging%20due%20to%20the%20difficulty%20of%20capturing%20subtle%20facial%20movements.%0AOptical%20flow%20has%20been%20widely%20employed%20as%20an%20input%20modality%20for%20this%20task%20due%20to%0Aits%20effectiveness.%20However%2C%20most%20existing%20methods%20compute%20optical%20flow%20only%0Abetween%20the%20onset%20and%20apex%20frames%2C%20thereby%20overlooking%20essential%20motion%0Ainformation%20in%20the%20apex-to-offset%20phase.%20To%20address%20this%20limitation%2C%20we%20first%0Aintroduce%20a%20comprehensive%20motion%20representation%2C%20termed%20Magnitude-Modulated%0ACombined%20Optical%20Flow%20%28MM-COF%29%2C%20which%20integrates%20motion%20dynamics%20from%20both%0Amicro-expression%20phases%20into%20a%20unified%20descriptor%20suitable%20for%20direct%20use%20in%0Arecognition%20networks.%20Building%20upon%20this%20principle%2C%20we%20then%20propose%20FMANet%2C%20a%0Anovel%20end-to-end%20neural%20network%20architecture%20that%20internalizes%20the%20dual-phase%0Aanalysis%20and%20magnitude%20modulation%20into%20learnable%20modules.%20This%20allows%20the%0Anetwork%20to%20adaptively%20fuse%20motion%20cues%20and%20focus%20on%20salient%20facial%20regions%20for%0Aclassification.%20Experimental%20evaluations%20on%20the%20MMEW%2C%20SMIC%2C%20CASME-II%2C%20and%20SAMM%0Adatasets%2C%20widely%20recognized%20as%20standard%20benchmarks%2C%20demonstrate%20that%20our%0Aproposed%20MM-COF%20representation%20and%20FMANet%20outperforms%20existing%20methods%2C%0Aunderscoring%20the%20potential%20of%20a%20learnable%2C%20dual-phase%20framework%20in%20advancing%0Amicro-expression%20recognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07810v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFMANet%253A%2520A%2520Novel%2520Dual-Phase%2520Optical%2520Flow%2520Approach%2520with%2520Fusion%2520Motion%250A%2520%2520Attention%2520Network%2520for%2520Robust%2520Micro-expression%2520Recognition%26entry.906535625%3DLuu%2520Tu%2520Nguyen%2520and%2520Vu%2520Tram%2520Anh%2520Khuong%2520and%2520Thi%2520Bich%2520Phuong%2520Man%2520and%2520Thi%2520Duyen%2520Ngo%2520and%2520Thanh%2520Ha%2520Le%26entry.1292438233%3D%2520%2520Facial%2520micro-expressions%252C%2520characterized%2520by%2520their%2520subtle%2520and%2520brief%2520nature%252C%2520are%250Avaluable%2520indicators%2520of%2520genuine%2520emotions.%2520Despite%2520their%2520significance%2520in%250Apsychology%252C%2520security%252C%2520and%2520behavioral%2520analysis%252C%2520micro-expression%2520recognition%250Aremains%2520challenging%2520due%2520to%2520the%2520difficulty%2520of%2520capturing%2520subtle%2520facial%2520movements.%250AOptical%2520flow%2520has%2520been%2520widely%2520employed%2520as%2520an%2520input%2520modality%2520for%2520this%2520task%2520due%2520to%250Aits%2520effectiveness.%2520However%252C%2520most%2520existing%2520methods%2520compute%2520optical%2520flow%2520only%250Abetween%2520the%2520onset%2520and%2520apex%2520frames%252C%2520thereby%2520overlooking%2520essential%2520motion%250Ainformation%2520in%2520the%2520apex-to-offset%2520phase.%2520To%2520address%2520this%2520limitation%252C%2520we%2520first%250Aintroduce%2520a%2520comprehensive%2520motion%2520representation%252C%2520termed%2520Magnitude-Modulated%250ACombined%2520Optical%2520Flow%2520%2528MM-COF%2529%252C%2520which%2520integrates%2520motion%2520dynamics%2520from%2520both%250Amicro-expression%2520phases%2520into%2520a%2520unified%2520descriptor%2520suitable%2520for%2520direct%2520use%2520in%250Arecognition%2520networks.%2520Building%2520upon%2520this%2520principle%252C%2520we%2520then%2520propose%2520FMANet%252C%2520a%250Anovel%2520end-to-end%2520neural%2520network%2520architecture%2520that%2520internalizes%2520the%2520dual-phase%250Aanalysis%2520and%2520magnitude%2520modulation%2520into%2520learnable%2520modules.%2520This%2520allows%2520the%250Anetwork%2520to%2520adaptively%2520fuse%2520motion%2520cues%2520and%2520focus%2520on%2520salient%2520facial%2520regions%2520for%250Aclassification.%2520Experimental%2520evaluations%2520on%2520the%2520MMEW%252C%2520SMIC%252C%2520CASME-II%252C%2520and%2520SAMM%250Adatasets%252C%2520widely%2520recognized%2520as%2520standard%2520benchmarks%252C%2520demonstrate%2520that%2520our%250Aproposed%2520MM-COF%2520representation%2520and%2520FMANet%2520outperforms%2520existing%2520methods%252C%250Aunderscoring%2520the%2520potential%2520of%2520a%2520learnable%252C%2520dual-phase%2520framework%2520in%2520advancing%250Amicro-expression%2520recognition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07810v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FMANet%3A%20A%20Novel%20Dual-Phase%20Optical%20Flow%20Approach%20with%20Fusion%20Motion%0A%20%20Attention%20Network%20for%20Robust%20Micro-expression%20Recognition&entry.906535625=Luu%20Tu%20Nguyen%20and%20Vu%20Tram%20Anh%20Khuong%20and%20Thi%20Bich%20Phuong%20Man%20and%20Thi%20Duyen%20Ngo%20and%20Thanh%20Ha%20Le&entry.1292438233=%20%20Facial%20micro-expressions%2C%20characterized%20by%20their%20subtle%20and%20brief%20nature%2C%20are%0Avaluable%20indicators%20of%20genuine%20emotions.%20Despite%20their%20significance%20in%0Apsychology%2C%20security%2C%20and%20behavioral%20analysis%2C%20micro-expression%20recognition%0Aremains%20challenging%20due%20to%20the%20difficulty%20of%20capturing%20subtle%20facial%20movements.%0AOptical%20flow%20has%20been%20widely%20employed%20as%20an%20input%20modality%20for%20this%20task%20due%20to%0Aits%20effectiveness.%20However%2C%20most%20existing%20methods%20compute%20optical%20flow%20only%0Abetween%20the%20onset%20and%20apex%20frames%2C%20thereby%20overlooking%20essential%20motion%0Ainformation%20in%20the%20apex-to-offset%20phase.%20To%20address%20this%20limitation%2C%20we%20first%0Aintroduce%20a%20comprehensive%20motion%20representation%2C%20termed%20Magnitude-Modulated%0ACombined%20Optical%20Flow%20%28MM-COF%29%2C%20which%20integrates%20motion%20dynamics%20from%20both%0Amicro-expression%20phases%20into%20a%20unified%20descriptor%20suitable%20for%20direct%20use%20in%0Arecognition%20networks.%20Building%20upon%20this%20principle%2C%20we%20then%20propose%20FMANet%2C%20a%0Anovel%20end-to-end%20neural%20network%20architecture%20that%20internalizes%20the%20dual-phase%0Aanalysis%20and%20magnitude%20modulation%20into%20learnable%20modules.%20This%20allows%20the%0Anetwork%20to%20adaptively%20fuse%20motion%20cues%20and%20focus%20on%20salient%20facial%20regions%20for%0Aclassification.%20Experimental%20evaluations%20on%20the%20MMEW%2C%20SMIC%2C%20CASME-II%2C%20and%20SAMM%0Adatasets%2C%20widely%20recognized%20as%20standard%20benchmarks%2C%20demonstrate%20that%20our%0Aproposed%20MM-COF%20representation%20and%20FMANet%20outperforms%20existing%20methods%2C%0Aunderscoring%20the%20potential%20of%20a%20learnable%2C%20dual-phase%20framework%20in%20advancing%0Amicro-expression%20recognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07810v3&entry.124074799=Read"},
{"title": "Prediction Markets with Intermittent Contributions", "author": "Michael Vitali and Pierre Pinson", "abstract": "  Although both data availability and the demand for accurate forecasts are\nincreasing, collaboration between stakeholders is often constrained by data\nownership and competitive interests. In contrast to recent proposals within\ncooperative game-theoretical frameworks, we place ourselves in a more general\nframework, based on prediction markets. There, independent agents trade\nforecasts of uncertain future events in exchange for rewards. We introduce and\nanalyse a prediction market that (i) accounts for the historical performance of\nthe agents, (ii) adapts to time-varying conditions, while (iii) permitting\nagents to enter and exit the market at will. The proposed design employs robust\nregression models to learn the optimal forecasts' combination whilst handling\nmissing submissions. Moreover, we introduce a pay-off allocation mechanism that\nconsiders both in-sample and out-of-sample performance while satisfying several\ndesirable economic properties. Case-studies using simulated and real-world data\nallow demonstrating the effectiveness and adaptability of the proposed market\ndesign.\n", "link": "http://arxiv.org/abs/2510.13385v1", "date": "2025-10-15", "relevancy": 1.3732, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4987}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4484}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prediction%20Markets%20with%20Intermittent%20Contributions&body=Title%3A%20Prediction%20Markets%20with%20Intermittent%20Contributions%0AAuthor%3A%20Michael%20Vitali%20and%20Pierre%20Pinson%0AAbstract%3A%20%20%20Although%20both%20data%20availability%20and%20the%20demand%20for%20accurate%20forecasts%20are%0Aincreasing%2C%20collaboration%20between%20stakeholders%20is%20often%20constrained%20by%20data%0Aownership%20and%20competitive%20interests.%20In%20contrast%20to%20recent%20proposals%20within%0Acooperative%20game-theoretical%20frameworks%2C%20we%20place%20ourselves%20in%20a%20more%20general%0Aframework%2C%20based%20on%20prediction%20markets.%20There%2C%20independent%20agents%20trade%0Aforecasts%20of%20uncertain%20future%20events%20in%20exchange%20for%20rewards.%20We%20introduce%20and%0Aanalyse%20a%20prediction%20market%20that%20%28i%29%20accounts%20for%20the%20historical%20performance%20of%0Athe%20agents%2C%20%28ii%29%20adapts%20to%20time-varying%20conditions%2C%20while%20%28iii%29%20permitting%0Aagents%20to%20enter%20and%20exit%20the%20market%20at%20will.%20The%20proposed%20design%20employs%20robust%0Aregression%20models%20to%20learn%20the%20optimal%20forecasts%27%20combination%20whilst%20handling%0Amissing%20submissions.%20Moreover%2C%20we%20introduce%20a%20pay-off%20allocation%20mechanism%20that%0Aconsiders%20both%20in-sample%20and%20out-of-sample%20performance%20while%20satisfying%20several%0Adesirable%20economic%20properties.%20Case-studies%20using%20simulated%20and%20real-world%20data%0Aallow%20demonstrating%20the%20effectiveness%20and%20adaptability%20of%20the%20proposed%20market%0Adesign.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13385v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrediction%2520Markets%2520with%2520Intermittent%2520Contributions%26entry.906535625%3DMichael%2520Vitali%2520and%2520Pierre%2520Pinson%26entry.1292438233%3D%2520%2520Although%2520both%2520data%2520availability%2520and%2520the%2520demand%2520for%2520accurate%2520forecasts%2520are%250Aincreasing%252C%2520collaboration%2520between%2520stakeholders%2520is%2520often%2520constrained%2520by%2520data%250Aownership%2520and%2520competitive%2520interests.%2520In%2520contrast%2520to%2520recent%2520proposals%2520within%250Acooperative%2520game-theoretical%2520frameworks%252C%2520we%2520place%2520ourselves%2520in%2520a%2520more%2520general%250Aframework%252C%2520based%2520on%2520prediction%2520markets.%2520There%252C%2520independent%2520agents%2520trade%250Aforecasts%2520of%2520uncertain%2520future%2520events%2520in%2520exchange%2520for%2520rewards.%2520We%2520introduce%2520and%250Aanalyse%2520a%2520prediction%2520market%2520that%2520%2528i%2529%2520accounts%2520for%2520the%2520historical%2520performance%2520of%250Athe%2520agents%252C%2520%2528ii%2529%2520adapts%2520to%2520time-varying%2520conditions%252C%2520while%2520%2528iii%2529%2520permitting%250Aagents%2520to%2520enter%2520and%2520exit%2520the%2520market%2520at%2520will.%2520The%2520proposed%2520design%2520employs%2520robust%250Aregression%2520models%2520to%2520learn%2520the%2520optimal%2520forecasts%2527%2520combination%2520whilst%2520handling%250Amissing%2520submissions.%2520Moreover%252C%2520we%2520introduce%2520a%2520pay-off%2520allocation%2520mechanism%2520that%250Aconsiders%2520both%2520in-sample%2520and%2520out-of-sample%2520performance%2520while%2520satisfying%2520several%250Adesirable%2520economic%2520properties.%2520Case-studies%2520using%2520simulated%2520and%2520real-world%2520data%250Aallow%2520demonstrating%2520the%2520effectiveness%2520and%2520adaptability%2520of%2520the%2520proposed%2520market%250Adesign.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13385v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prediction%20Markets%20with%20Intermittent%20Contributions&entry.906535625=Michael%20Vitali%20and%20Pierre%20Pinson&entry.1292438233=%20%20Although%20both%20data%20availability%20and%20the%20demand%20for%20accurate%20forecasts%20are%0Aincreasing%2C%20collaboration%20between%20stakeholders%20is%20often%20constrained%20by%20data%0Aownership%20and%20competitive%20interests.%20In%20contrast%20to%20recent%20proposals%20within%0Acooperative%20game-theoretical%20frameworks%2C%20we%20place%20ourselves%20in%20a%20more%20general%0Aframework%2C%20based%20on%20prediction%20markets.%20There%2C%20independent%20agents%20trade%0Aforecasts%20of%20uncertain%20future%20events%20in%20exchange%20for%20rewards.%20We%20introduce%20and%0Aanalyse%20a%20prediction%20market%20that%20%28i%29%20accounts%20for%20the%20historical%20performance%20of%0Athe%20agents%2C%20%28ii%29%20adapts%20to%20time-varying%20conditions%2C%20while%20%28iii%29%20permitting%0Aagents%20to%20enter%20and%20exit%20the%20market%20at%20will.%20The%20proposed%20design%20employs%20robust%0Aregression%20models%20to%20learn%20the%20optimal%20forecasts%27%20combination%20whilst%20handling%0Amissing%20submissions.%20Moreover%2C%20we%20introduce%20a%20pay-off%20allocation%20mechanism%20that%0Aconsiders%20both%20in-sample%20and%20out-of-sample%20performance%20while%20satisfying%20several%0Adesirable%20economic%20properties.%20Case-studies%20using%20simulated%20and%20real-world%20data%0Aallow%20demonstrating%20the%20effectiveness%20and%20adaptability%20of%20the%20proposed%20market%0Adesign.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13385v1&entry.124074799=Read"},
{"title": "ConsintBench: Evaluating Language Models on Real-World Consumer Intent\n  Understanding", "author": "Xiaozhe Li and TianYi Lyu and Siyi Yang and Yuxi Gong and Yizhao Yang and Jinxuan Huang and Ligao Zhang and Zhuoyi Huang and Qingwen Liu", "abstract": "  Understanding human intent is a complex, high-level task for large language\nmodels (LLMs), requiring analytical reasoning, contextual interpretation,\ndynamic information aggregation, and decision-making under uncertainty.\nReal-world public discussions, such as consumer product discussions, are rarely\nlinear or involve a single user. Instead, they are characterized by interwoven\nand often conflicting perspectives, divergent concerns, goals, emotional\ntendencies, as well as implicit assumptions and background knowledge about\nusage scenarios. To accurately understand such explicit public intent, an LLM\nmust go beyond parsing individual sentences; it must integrate multi-source\nsignals, reason over inconsistencies, and adapt to evolving discourse, similar\nto how experts in fields like politics, economics, or finance approach complex,\nuncertain environments. Despite the importance of this capability, no\nlarge-scale benchmark currently exists for evaluating LLMs on real-world human\nintent understanding, primarily due to the challenges of collecting real-world\npublic discussion data and constructing a robust evaluation pipeline. To bridge\nthis gap, we introduce \\bench, the first dynamic, live evaluation benchmark\nspecifically designed for intent understanding, particularly in the consumer\ndomain. \\bench is the largest and most diverse benchmark of its kind,\nsupporting real-time updates while preventing data contamination through an\nautomated curation pipeline.\n", "link": "http://arxiv.org/abs/2510.13499v1", "date": "2025-10-15", "relevancy": 2.0692, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5253}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5253}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ConsintBench%3A%20Evaluating%20Language%20Models%20on%20Real-World%20Consumer%20Intent%0A%20%20Understanding&body=Title%3A%20ConsintBench%3A%20Evaluating%20Language%20Models%20on%20Real-World%20Consumer%20Intent%0A%20%20Understanding%0AAuthor%3A%20Xiaozhe%20Li%20and%20TianYi%20Lyu%20and%20Siyi%20Yang%20and%20Yuxi%20Gong%20and%20Yizhao%20Yang%20and%20Jinxuan%20Huang%20and%20Ligao%20Zhang%20and%20Zhuoyi%20Huang%20and%20Qingwen%20Liu%0AAbstract%3A%20%20%20Understanding%20human%20intent%20is%20a%20complex%2C%20high-level%20task%20for%20large%20language%0Amodels%20%28LLMs%29%2C%20requiring%20analytical%20reasoning%2C%20contextual%20interpretation%2C%0Adynamic%20information%20aggregation%2C%20and%20decision-making%20under%20uncertainty.%0AReal-world%20public%20discussions%2C%20such%20as%20consumer%20product%20discussions%2C%20are%20rarely%0Alinear%20or%20involve%20a%20single%20user.%20Instead%2C%20they%20are%20characterized%20by%20interwoven%0Aand%20often%20conflicting%20perspectives%2C%20divergent%20concerns%2C%20goals%2C%20emotional%0Atendencies%2C%20as%20well%20as%20implicit%20assumptions%20and%20background%20knowledge%20about%0Ausage%20scenarios.%20To%20accurately%20understand%20such%20explicit%20public%20intent%2C%20an%20LLM%0Amust%20go%20beyond%20parsing%20individual%20sentences%3B%20it%20must%20integrate%20multi-source%0Asignals%2C%20reason%20over%20inconsistencies%2C%20and%20adapt%20to%20evolving%20discourse%2C%20similar%0Ato%20how%20experts%20in%20fields%20like%20politics%2C%20economics%2C%20or%20finance%20approach%20complex%2C%0Auncertain%20environments.%20Despite%20the%20importance%20of%20this%20capability%2C%20no%0Alarge-scale%20benchmark%20currently%20exists%20for%20evaluating%20LLMs%20on%20real-world%20human%0Aintent%20understanding%2C%20primarily%20due%20to%20the%20challenges%20of%20collecting%20real-world%0Apublic%20discussion%20data%20and%20constructing%20a%20robust%20evaluation%20pipeline.%20To%20bridge%0Athis%20gap%2C%20we%20introduce%20%5Cbench%2C%20the%20first%20dynamic%2C%20live%20evaluation%20benchmark%0Aspecifically%20designed%20for%20intent%20understanding%2C%20particularly%20in%20the%20consumer%0Adomain.%20%5Cbench%20is%20the%20largest%20and%20most%20diverse%20benchmark%20of%20its%20kind%2C%0Asupporting%20real-time%20updates%20while%20preventing%20data%20contamination%20through%20an%0Aautomated%20curation%20pipeline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13499v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConsintBench%253A%2520Evaluating%2520Language%2520Models%2520on%2520Real-World%2520Consumer%2520Intent%250A%2520%2520Understanding%26entry.906535625%3DXiaozhe%2520Li%2520and%2520TianYi%2520Lyu%2520and%2520Siyi%2520Yang%2520and%2520Yuxi%2520Gong%2520and%2520Yizhao%2520Yang%2520and%2520Jinxuan%2520Huang%2520and%2520Ligao%2520Zhang%2520and%2520Zhuoyi%2520Huang%2520and%2520Qingwen%2520Liu%26entry.1292438233%3D%2520%2520Understanding%2520human%2520intent%2520is%2520a%2520complex%252C%2520high-level%2520task%2520for%2520large%2520language%250Amodels%2520%2528LLMs%2529%252C%2520requiring%2520analytical%2520reasoning%252C%2520contextual%2520interpretation%252C%250Adynamic%2520information%2520aggregation%252C%2520and%2520decision-making%2520under%2520uncertainty.%250AReal-world%2520public%2520discussions%252C%2520such%2520as%2520consumer%2520product%2520discussions%252C%2520are%2520rarely%250Alinear%2520or%2520involve%2520a%2520single%2520user.%2520Instead%252C%2520they%2520are%2520characterized%2520by%2520interwoven%250Aand%2520often%2520conflicting%2520perspectives%252C%2520divergent%2520concerns%252C%2520goals%252C%2520emotional%250Atendencies%252C%2520as%2520well%2520as%2520implicit%2520assumptions%2520and%2520background%2520knowledge%2520about%250Ausage%2520scenarios.%2520To%2520accurately%2520understand%2520such%2520explicit%2520public%2520intent%252C%2520an%2520LLM%250Amust%2520go%2520beyond%2520parsing%2520individual%2520sentences%253B%2520it%2520must%2520integrate%2520multi-source%250Asignals%252C%2520reason%2520over%2520inconsistencies%252C%2520and%2520adapt%2520to%2520evolving%2520discourse%252C%2520similar%250Ato%2520how%2520experts%2520in%2520fields%2520like%2520politics%252C%2520economics%252C%2520or%2520finance%2520approach%2520complex%252C%250Auncertain%2520environments.%2520Despite%2520the%2520importance%2520of%2520this%2520capability%252C%2520no%250Alarge-scale%2520benchmark%2520currently%2520exists%2520for%2520evaluating%2520LLMs%2520on%2520real-world%2520human%250Aintent%2520understanding%252C%2520primarily%2520due%2520to%2520the%2520challenges%2520of%2520collecting%2520real-world%250Apublic%2520discussion%2520data%2520and%2520constructing%2520a%2520robust%2520evaluation%2520pipeline.%2520To%2520bridge%250Athis%2520gap%252C%2520we%2520introduce%2520%255Cbench%252C%2520the%2520first%2520dynamic%252C%2520live%2520evaluation%2520benchmark%250Aspecifically%2520designed%2520for%2520intent%2520understanding%252C%2520particularly%2520in%2520the%2520consumer%250Adomain.%2520%255Cbench%2520is%2520the%2520largest%2520and%2520most%2520diverse%2520benchmark%2520of%2520its%2520kind%252C%250Asupporting%2520real-time%2520updates%2520while%2520preventing%2520data%2520contamination%2520through%2520an%250Aautomated%2520curation%2520pipeline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13499v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConsintBench%3A%20Evaluating%20Language%20Models%20on%20Real-World%20Consumer%20Intent%0A%20%20Understanding&entry.906535625=Xiaozhe%20Li%20and%20TianYi%20Lyu%20and%20Siyi%20Yang%20and%20Yuxi%20Gong%20and%20Yizhao%20Yang%20and%20Jinxuan%20Huang%20and%20Ligao%20Zhang%20and%20Zhuoyi%20Huang%20and%20Qingwen%20Liu&entry.1292438233=%20%20Understanding%20human%20intent%20is%20a%20complex%2C%20high-level%20task%20for%20large%20language%0Amodels%20%28LLMs%29%2C%20requiring%20analytical%20reasoning%2C%20contextual%20interpretation%2C%0Adynamic%20information%20aggregation%2C%20and%20decision-making%20under%20uncertainty.%0AReal-world%20public%20discussions%2C%20such%20as%20consumer%20product%20discussions%2C%20are%20rarely%0Alinear%20or%20involve%20a%20single%20user.%20Instead%2C%20they%20are%20characterized%20by%20interwoven%0Aand%20often%20conflicting%20perspectives%2C%20divergent%20concerns%2C%20goals%2C%20emotional%0Atendencies%2C%20as%20well%20as%20implicit%20assumptions%20and%20background%20knowledge%20about%0Ausage%20scenarios.%20To%20accurately%20understand%20such%20explicit%20public%20intent%2C%20an%20LLM%0Amust%20go%20beyond%20parsing%20individual%20sentences%3B%20it%20must%20integrate%20multi-source%0Asignals%2C%20reason%20over%20inconsistencies%2C%20and%20adapt%20to%20evolving%20discourse%2C%20similar%0Ato%20how%20experts%20in%20fields%20like%20politics%2C%20economics%2C%20or%20finance%20approach%20complex%2C%0Auncertain%20environments.%20Despite%20the%20importance%20of%20this%20capability%2C%20no%0Alarge-scale%20benchmark%20currently%20exists%20for%20evaluating%20LLMs%20on%20real-world%20human%0Aintent%20understanding%2C%20primarily%20due%20to%20the%20challenges%20of%20collecting%20real-world%0Apublic%20discussion%20data%20and%20constructing%20a%20robust%20evaluation%20pipeline.%20To%20bridge%0Athis%20gap%2C%20we%20introduce%20%5Cbench%2C%20the%20first%20dynamic%2C%20live%20evaluation%20benchmark%0Aspecifically%20designed%20for%20intent%20understanding%2C%20particularly%20in%20the%20consumer%0Adomain.%20%5Cbench%20is%20the%20largest%20and%20most%20diverse%20benchmark%20of%20its%20kind%2C%0Asupporting%20real-time%20updates%20while%20preventing%20data%20contamination%20through%20an%0Aautomated%20curation%20pipeline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13499v1&entry.124074799=Read"},
{"title": "Deflanderization for Game Dialogue: Balancing Character Authenticity\n  with Task Execution in LLM-based NPCs", "author": "Pasin Buakhaw and Kun Kerdthaisong and Phuree Phenhiran and Pitikorn Khlaisamniang and Supasate Vorathammathorn and Piyalitt Ittichaiwong and Nutchanon Yongsatianchot", "abstract": "  The emergence of large language models (LLMs) has opened new opportunities\nfor cre- ating dynamic non-player characters (NPCs) in gaming environments,\nenabling both func- tional task execution and persona-consistent dialogue\ngeneration. In this paper, we (Tu_Character_lab) report our participation in\nthe Commonsense Persona-Grounded Dialogue Challenge (CPDC) 2025 Round 2, which\neval- uates agents across three tracks: task-oriented dialogue, context-aware\ndialogue, and their integration. Our approach combines two complementary\nstrategies: (i) lightweight prompting techniques in the API track, including a\nDeflanderization prompting method to suppress excessive role-play and improve\ntask fidelity, and (ii) fine-tuned large models in the GPU track, leveraging\nQwen3-14B with supervisedfinetuning (SFT) and Low-Rank Adaptation(LoRA). Our\nbest submissions ranked 2nd on Task 1, 2nd on Task 3 (API track), and 4th on\nTask 3 (GPU track).\n", "link": "http://arxiv.org/abs/2510.13586v1", "date": "2025-10-15", "relevancy": 1.998, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5042}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5042}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deflanderization%20for%20Game%20Dialogue%3A%20Balancing%20Character%20Authenticity%0A%20%20with%20Task%20Execution%20in%20LLM-based%20NPCs&body=Title%3A%20Deflanderization%20for%20Game%20Dialogue%3A%20Balancing%20Character%20Authenticity%0A%20%20with%20Task%20Execution%20in%20LLM-based%20NPCs%0AAuthor%3A%20Pasin%20Buakhaw%20and%20Kun%20Kerdthaisong%20and%20Phuree%20Phenhiran%20and%20Pitikorn%20Khlaisamniang%20and%20Supasate%20Vorathammathorn%20and%20Piyalitt%20Ittichaiwong%20and%20Nutchanon%20Yongsatianchot%0AAbstract%3A%20%20%20The%20emergence%20of%20large%20language%20models%20%28LLMs%29%20has%20opened%20new%20opportunities%0Afor%20cre-%20ating%20dynamic%20non-player%20characters%20%28NPCs%29%20in%20gaming%20environments%2C%0Aenabling%20both%20func-%20tional%20task%20execution%20and%20persona-consistent%20dialogue%0Ageneration.%20In%20this%20paper%2C%20we%20%28Tu_Character_lab%29%20report%20our%20participation%20in%0Athe%20Commonsense%20Persona-Grounded%20Dialogue%20Challenge%20%28CPDC%29%202025%20Round%202%2C%20which%0Aeval-%20uates%20agents%20across%20three%20tracks%3A%20task-oriented%20dialogue%2C%20context-aware%0Adialogue%2C%20and%20their%20integration.%20Our%20approach%20combines%20two%20complementary%0Astrategies%3A%20%28i%29%20lightweight%20prompting%20techniques%20in%20the%20API%20track%2C%20including%20a%0ADeflanderization%20prompting%20method%20to%20suppress%20excessive%20role-play%20and%20improve%0Atask%20fidelity%2C%20and%20%28ii%29%20fine-tuned%20large%20models%20in%20the%20GPU%20track%2C%20leveraging%0AQwen3-14B%20with%20supervisedfinetuning%20%28SFT%29%20and%20Low-Rank%20Adaptation%28LoRA%29.%20Our%0Abest%20submissions%20ranked%202nd%20on%20Task%201%2C%202nd%20on%20Task%203%20%28API%20track%29%2C%20and%204th%20on%0ATask%203%20%28GPU%20track%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13586v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeflanderization%2520for%2520Game%2520Dialogue%253A%2520Balancing%2520Character%2520Authenticity%250A%2520%2520with%2520Task%2520Execution%2520in%2520LLM-based%2520NPCs%26entry.906535625%3DPasin%2520Buakhaw%2520and%2520Kun%2520Kerdthaisong%2520and%2520Phuree%2520Phenhiran%2520and%2520Pitikorn%2520Khlaisamniang%2520and%2520Supasate%2520Vorathammathorn%2520and%2520Piyalitt%2520Ittichaiwong%2520and%2520Nutchanon%2520Yongsatianchot%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520opened%2520new%2520opportunities%250Afor%2520cre-%2520ating%2520dynamic%2520non-player%2520characters%2520%2528NPCs%2529%2520in%2520gaming%2520environments%252C%250Aenabling%2520both%2520func-%2520tional%2520task%2520execution%2520and%2520persona-consistent%2520dialogue%250Ageneration.%2520In%2520this%2520paper%252C%2520we%2520%2528Tu_Character_lab%2529%2520report%2520our%2520participation%2520in%250Athe%2520Commonsense%2520Persona-Grounded%2520Dialogue%2520Challenge%2520%2528CPDC%2529%25202025%2520Round%25202%252C%2520which%250Aeval-%2520uates%2520agents%2520across%2520three%2520tracks%253A%2520task-oriented%2520dialogue%252C%2520context-aware%250Adialogue%252C%2520and%2520their%2520integration.%2520Our%2520approach%2520combines%2520two%2520complementary%250Astrategies%253A%2520%2528i%2529%2520lightweight%2520prompting%2520techniques%2520in%2520the%2520API%2520track%252C%2520including%2520a%250ADeflanderization%2520prompting%2520method%2520to%2520suppress%2520excessive%2520role-play%2520and%2520improve%250Atask%2520fidelity%252C%2520and%2520%2528ii%2529%2520fine-tuned%2520large%2520models%2520in%2520the%2520GPU%2520track%252C%2520leveraging%250AQwen3-14B%2520with%2520supervisedfinetuning%2520%2528SFT%2529%2520and%2520Low-Rank%2520Adaptation%2528LoRA%2529.%2520Our%250Abest%2520submissions%2520ranked%25202nd%2520on%2520Task%25201%252C%25202nd%2520on%2520Task%25203%2520%2528API%2520track%2529%252C%2520and%25204th%2520on%250ATask%25203%2520%2528GPU%2520track%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13586v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deflanderization%20for%20Game%20Dialogue%3A%20Balancing%20Character%20Authenticity%0A%20%20with%20Task%20Execution%20in%20LLM-based%20NPCs&entry.906535625=Pasin%20Buakhaw%20and%20Kun%20Kerdthaisong%20and%20Phuree%20Phenhiran%20and%20Pitikorn%20Khlaisamniang%20and%20Supasate%20Vorathammathorn%20and%20Piyalitt%20Ittichaiwong%20and%20Nutchanon%20Yongsatianchot&entry.1292438233=%20%20The%20emergence%20of%20large%20language%20models%20%28LLMs%29%20has%20opened%20new%20opportunities%0Afor%20cre-%20ating%20dynamic%20non-player%20characters%20%28NPCs%29%20in%20gaming%20environments%2C%0Aenabling%20both%20func-%20tional%20task%20execution%20and%20persona-consistent%20dialogue%0Ageneration.%20In%20this%20paper%2C%20we%20%28Tu_Character_lab%29%20report%20our%20participation%20in%0Athe%20Commonsense%20Persona-Grounded%20Dialogue%20Challenge%20%28CPDC%29%202025%20Round%202%2C%20which%0Aeval-%20uates%20agents%20across%20three%20tracks%3A%20task-oriented%20dialogue%2C%20context-aware%0Adialogue%2C%20and%20their%20integration.%20Our%20approach%20combines%20two%20complementary%0Astrategies%3A%20%28i%29%20lightweight%20prompting%20techniques%20in%20the%20API%20track%2C%20including%20a%0ADeflanderization%20prompting%20method%20to%20suppress%20excessive%20role-play%20and%20improve%0Atask%20fidelity%2C%20and%20%28ii%29%20fine-tuned%20large%20models%20in%20the%20GPU%20track%2C%20leveraging%0AQwen3-14B%20with%20supervisedfinetuning%20%28SFT%29%20and%20Low-Rank%20Adaptation%28LoRA%29.%20Our%0Abest%20submissions%20ranked%202nd%20on%20Task%201%2C%202nd%20on%20Task%203%20%28API%20track%29%2C%20and%204th%20on%0ATask%203%20%28GPU%20track%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13586v1&entry.124074799=Read"},
{"title": "Product Digital Twin Supporting End-of-life Phase of Electric Vehicle\n  Batteries Utilizing Product-Process-Resource Asset Network", "author": "Sara Strakosova and Petr Novak and Petr Kadera", "abstract": "  In a circular economy, products in their end-of-life phase should be either\nremanufactured or recycled. Both of these processes are crucial for\nsustainability and environmental conservation. However, manufacturers\nfrequently do not support these processes enough in terms of not sharing\nrelevant data about the products nor their (re-)manufacturing processes. This\npaper proposes to accompany each product with a digital twin technology,\nspecifically the Product Digital Twin (PDT), which can carry information for\nfacilitating and optimizing production and remanufacturing processes. This\npaper introduces a knowledge representation called Bi-Flow\nProduct-Process-Resource Asset Network (Bi-PAN). Bi-PAN extends a well-proven\nProduct-Process-Resource Asset Network (PAN) paradigm by integrating both\nassembly and disassembly workflows into a single information model. Such\nnetworks enable capturing relevant relationships across products, production\nresources, manufacturing processes, and specific production operations that\nhave to be done in the manufacturing phase of a product. The proposed approach\nis demonstrated in a use-case of disassembling electric vehicle (EV) batteries.\nBy utilizing PDTs with Bi-PAN knowledge models, challenges associated with\ndisassembling of EV batteries can be solved flexibly and efficiently for\nvarious battery types, enhancing the sustainability of the EV battery\nlife-cycle management.\n", "link": "http://arxiv.org/abs/2510.02167v3", "date": "2025-10-15", "relevancy": 1.1926, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4106}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3838}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.3785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Product%20Digital%20Twin%20Supporting%20End-of-life%20Phase%20of%20Electric%20Vehicle%0A%20%20Batteries%20Utilizing%20Product-Process-Resource%20Asset%20Network&body=Title%3A%20Product%20Digital%20Twin%20Supporting%20End-of-life%20Phase%20of%20Electric%20Vehicle%0A%20%20Batteries%20Utilizing%20Product-Process-Resource%20Asset%20Network%0AAuthor%3A%20Sara%20Strakosova%20and%20Petr%20Novak%20and%20Petr%20Kadera%0AAbstract%3A%20%20%20In%20a%20circular%20economy%2C%20products%20in%20their%20end-of-life%20phase%20should%20be%20either%0Aremanufactured%20or%20recycled.%20Both%20of%20these%20processes%20are%20crucial%20for%0Asustainability%20and%20environmental%20conservation.%20However%2C%20manufacturers%0Afrequently%20do%20not%20support%20these%20processes%20enough%20in%20terms%20of%20not%20sharing%0Arelevant%20data%20about%20the%20products%20nor%20their%20%28re-%29manufacturing%20processes.%20This%0Apaper%20proposes%20to%20accompany%20each%20product%20with%20a%20digital%20twin%20technology%2C%0Aspecifically%20the%20Product%20Digital%20Twin%20%28PDT%29%2C%20which%20can%20carry%20information%20for%0Afacilitating%20and%20optimizing%20production%20and%20remanufacturing%20processes.%20This%0Apaper%20introduces%20a%20knowledge%20representation%20called%20Bi-Flow%0AProduct-Process-Resource%20Asset%20Network%20%28Bi-PAN%29.%20Bi-PAN%20extends%20a%20well-proven%0AProduct-Process-Resource%20Asset%20Network%20%28PAN%29%20paradigm%20by%20integrating%20both%0Aassembly%20and%20disassembly%20workflows%20into%20a%20single%20information%20model.%20Such%0Anetworks%20enable%20capturing%20relevant%20relationships%20across%20products%2C%20production%0Aresources%2C%20manufacturing%20processes%2C%20and%20specific%20production%20operations%20that%0Ahave%20to%20be%20done%20in%20the%20manufacturing%20phase%20of%20a%20product.%20The%20proposed%20approach%0Ais%20demonstrated%20in%20a%20use-case%20of%20disassembling%20electric%20vehicle%20%28EV%29%20batteries.%0ABy%20utilizing%20PDTs%20with%20Bi-PAN%20knowledge%20models%2C%20challenges%20associated%20with%0Adisassembling%20of%20EV%20batteries%20can%20be%20solved%20flexibly%20and%20efficiently%20for%0Avarious%20battery%20types%2C%20enhancing%20the%20sustainability%20of%20the%20EV%20battery%0Alife-cycle%20management.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02167v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProduct%2520Digital%2520Twin%2520Supporting%2520End-of-life%2520Phase%2520of%2520Electric%2520Vehicle%250A%2520%2520Batteries%2520Utilizing%2520Product-Process-Resource%2520Asset%2520Network%26entry.906535625%3DSara%2520Strakosova%2520and%2520Petr%2520Novak%2520and%2520Petr%2520Kadera%26entry.1292438233%3D%2520%2520In%2520a%2520circular%2520economy%252C%2520products%2520in%2520their%2520end-of-life%2520phase%2520should%2520be%2520either%250Aremanufactured%2520or%2520recycled.%2520Both%2520of%2520these%2520processes%2520are%2520crucial%2520for%250Asustainability%2520and%2520environmental%2520conservation.%2520However%252C%2520manufacturers%250Afrequently%2520do%2520not%2520support%2520these%2520processes%2520enough%2520in%2520terms%2520of%2520not%2520sharing%250Arelevant%2520data%2520about%2520the%2520products%2520nor%2520their%2520%2528re-%2529manufacturing%2520processes.%2520This%250Apaper%2520proposes%2520to%2520accompany%2520each%2520product%2520with%2520a%2520digital%2520twin%2520technology%252C%250Aspecifically%2520the%2520Product%2520Digital%2520Twin%2520%2528PDT%2529%252C%2520which%2520can%2520carry%2520information%2520for%250Afacilitating%2520and%2520optimizing%2520production%2520and%2520remanufacturing%2520processes.%2520This%250Apaper%2520introduces%2520a%2520knowledge%2520representation%2520called%2520Bi-Flow%250AProduct-Process-Resource%2520Asset%2520Network%2520%2528Bi-PAN%2529.%2520Bi-PAN%2520extends%2520a%2520well-proven%250AProduct-Process-Resource%2520Asset%2520Network%2520%2528PAN%2529%2520paradigm%2520by%2520integrating%2520both%250Aassembly%2520and%2520disassembly%2520workflows%2520into%2520a%2520single%2520information%2520model.%2520Such%250Anetworks%2520enable%2520capturing%2520relevant%2520relationships%2520across%2520products%252C%2520production%250Aresources%252C%2520manufacturing%2520processes%252C%2520and%2520specific%2520production%2520operations%2520that%250Ahave%2520to%2520be%2520done%2520in%2520the%2520manufacturing%2520phase%2520of%2520a%2520product.%2520The%2520proposed%2520approach%250Ais%2520demonstrated%2520in%2520a%2520use-case%2520of%2520disassembling%2520electric%2520vehicle%2520%2528EV%2529%2520batteries.%250ABy%2520utilizing%2520PDTs%2520with%2520Bi-PAN%2520knowledge%2520models%252C%2520challenges%2520associated%2520with%250Adisassembling%2520of%2520EV%2520batteries%2520can%2520be%2520solved%2520flexibly%2520and%2520efficiently%2520for%250Avarious%2520battery%2520types%252C%2520enhancing%2520the%2520sustainability%2520of%2520the%2520EV%2520battery%250Alife-cycle%2520management.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02167v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Product%20Digital%20Twin%20Supporting%20End-of-life%20Phase%20of%20Electric%20Vehicle%0A%20%20Batteries%20Utilizing%20Product-Process-Resource%20Asset%20Network&entry.906535625=Sara%20Strakosova%20and%20Petr%20Novak%20and%20Petr%20Kadera&entry.1292438233=%20%20In%20a%20circular%20economy%2C%20products%20in%20their%20end-of-life%20phase%20should%20be%20either%0Aremanufactured%20or%20recycled.%20Both%20of%20these%20processes%20are%20crucial%20for%0Asustainability%20and%20environmental%20conservation.%20However%2C%20manufacturers%0Afrequently%20do%20not%20support%20these%20processes%20enough%20in%20terms%20of%20not%20sharing%0Arelevant%20data%20about%20the%20products%20nor%20their%20%28re-%29manufacturing%20processes.%20This%0Apaper%20proposes%20to%20accompany%20each%20product%20with%20a%20digital%20twin%20technology%2C%0Aspecifically%20the%20Product%20Digital%20Twin%20%28PDT%29%2C%20which%20can%20carry%20information%20for%0Afacilitating%20and%20optimizing%20production%20and%20remanufacturing%20processes.%20This%0Apaper%20introduces%20a%20knowledge%20representation%20called%20Bi-Flow%0AProduct-Process-Resource%20Asset%20Network%20%28Bi-PAN%29.%20Bi-PAN%20extends%20a%20well-proven%0AProduct-Process-Resource%20Asset%20Network%20%28PAN%29%20paradigm%20by%20integrating%20both%0Aassembly%20and%20disassembly%20workflows%20into%20a%20single%20information%20model.%20Such%0Anetworks%20enable%20capturing%20relevant%20relationships%20across%20products%2C%20production%0Aresources%2C%20manufacturing%20processes%2C%20and%20specific%20production%20operations%20that%0Ahave%20to%20be%20done%20in%20the%20manufacturing%20phase%20of%20a%20product.%20The%20proposed%20approach%0Ais%20demonstrated%20in%20a%20use-case%20of%20disassembling%20electric%20vehicle%20%28EV%29%20batteries.%0ABy%20utilizing%20PDTs%20with%20Bi-PAN%20knowledge%20models%2C%20challenges%20associated%20with%0Adisassembling%20of%20EV%20batteries%20can%20be%20solved%20flexibly%20and%20efficiently%20for%0Avarious%20battery%20types%2C%20enhancing%20the%20sustainability%20of%20the%20EV%20battery%0Alife-cycle%20management.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02167v3&entry.124074799=Read"},
{"title": "Generating healthy counterfactuals with denoising diffusion bridge\n  models", "author": "Ana Lawry Aguila and Peirong Liu and Marina Crespo Aguirre and Juan Eugenio Iglesias", "abstract": "  Generating healthy counterfactuals from pathological images holds significant\npromise in medical imaging, e.g., in anomaly detection or for application of\nanalysis tools that are designed for healthy scans. These counterfactuals\nshould represent what a patient's scan would plausibly look like in the absence\nof pathology, preserving individual anatomical characteristics while modifying\nonly the pathological regions. Denoising diffusion probabilistic models (DDPMs)\nhave become popular methods for generating healthy counterfactuals of pathology\ndata. Typically, this involves training on solely healthy data with the\nassumption that a partial denoising process will be unable to model disease\nregions and will instead reconstruct a closely matched healthy counterpart.\nMore recent methods have incorporated synthetic pathological images to better\nguide the diffusion process. However, it remains challenging to guide the\ngenerative process in a way that effectively balances the removal of anomalies\nwith the retention of subject-specific features. To solve this problem, we\npropose a novel application of denoising diffusion bridge models (DDBMs) -\nwhich, unlike DDPMs, condition the diffusion process not only on the initial\npoint (i.e., the healthy image), but also on the final point (i.e., a\ncorresponding synthetically generated pathological image). Treating the\npathological image as a structurally informative prior enables us to generate\ncounterfactuals that closely match the patient's anatomy while selectively\nremoving pathology. The results show that our DDBM outperforms previously\nproposed diffusion models and fully supervised approaches at segmentation and\nanomaly detection tasks.\n", "link": "http://arxiv.org/abs/2510.13684v1", "date": "2025-10-15", "relevancy": 1.1727, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6107}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5799}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5685}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generating%20healthy%20counterfactuals%20with%20denoising%20diffusion%20bridge%0A%20%20models&body=Title%3A%20Generating%20healthy%20counterfactuals%20with%20denoising%20diffusion%20bridge%0A%20%20models%0AAuthor%3A%20Ana%20Lawry%20Aguila%20and%20Peirong%20Liu%20and%20Marina%20Crespo%20Aguirre%20and%20Juan%20Eugenio%20Iglesias%0AAbstract%3A%20%20%20Generating%20healthy%20counterfactuals%20from%20pathological%20images%20holds%20significant%0Apromise%20in%20medical%20imaging%2C%20e.g.%2C%20in%20anomaly%20detection%20or%20for%20application%20of%0Aanalysis%20tools%20that%20are%20designed%20for%20healthy%20scans.%20These%20counterfactuals%0Ashould%20represent%20what%20a%20patient%27s%20scan%20would%20plausibly%20look%20like%20in%20the%20absence%0Aof%20pathology%2C%20preserving%20individual%20anatomical%20characteristics%20while%20modifying%0Aonly%20the%20pathological%20regions.%20Denoising%20diffusion%20probabilistic%20models%20%28DDPMs%29%0Ahave%20become%20popular%20methods%20for%20generating%20healthy%20counterfactuals%20of%20pathology%0Adata.%20Typically%2C%20this%20involves%20training%20on%20solely%20healthy%20data%20with%20the%0Aassumption%20that%20a%20partial%20denoising%20process%20will%20be%20unable%20to%20model%20disease%0Aregions%20and%20will%20instead%20reconstruct%20a%20closely%20matched%20healthy%20counterpart.%0AMore%20recent%20methods%20have%20incorporated%20synthetic%20pathological%20images%20to%20better%0Aguide%20the%20diffusion%20process.%20However%2C%20it%20remains%20challenging%20to%20guide%20the%0Agenerative%20process%20in%20a%20way%20that%20effectively%20balances%20the%20removal%20of%20anomalies%0Awith%20the%20retention%20of%20subject-specific%20features.%20To%20solve%20this%20problem%2C%20we%0Apropose%20a%20novel%20application%20of%20denoising%20diffusion%20bridge%20models%20%28DDBMs%29%20-%0Awhich%2C%20unlike%20DDPMs%2C%20condition%20the%20diffusion%20process%20not%20only%20on%20the%20initial%0Apoint%20%28i.e.%2C%20the%20healthy%20image%29%2C%20but%20also%20on%20the%20final%20point%20%28i.e.%2C%20a%0Acorresponding%20synthetically%20generated%20pathological%20image%29.%20Treating%20the%0Apathological%20image%20as%20a%20structurally%20informative%20prior%20enables%20us%20to%20generate%0Acounterfactuals%20that%20closely%20match%20the%20patient%27s%20anatomy%20while%20selectively%0Aremoving%20pathology.%20The%20results%20show%20that%20our%20DDBM%20outperforms%20previously%0Aproposed%20diffusion%20models%20and%20fully%20supervised%20approaches%20at%20segmentation%20and%0Aanomaly%20detection%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13684v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerating%2520healthy%2520counterfactuals%2520with%2520denoising%2520diffusion%2520bridge%250A%2520%2520models%26entry.906535625%3DAna%2520Lawry%2520Aguila%2520and%2520Peirong%2520Liu%2520and%2520Marina%2520Crespo%2520Aguirre%2520and%2520Juan%2520Eugenio%2520Iglesias%26entry.1292438233%3D%2520%2520Generating%2520healthy%2520counterfactuals%2520from%2520pathological%2520images%2520holds%2520significant%250Apromise%2520in%2520medical%2520imaging%252C%2520e.g.%252C%2520in%2520anomaly%2520detection%2520or%2520for%2520application%2520of%250Aanalysis%2520tools%2520that%2520are%2520designed%2520for%2520healthy%2520scans.%2520These%2520counterfactuals%250Ashould%2520represent%2520what%2520a%2520patient%2527s%2520scan%2520would%2520plausibly%2520look%2520like%2520in%2520the%2520absence%250Aof%2520pathology%252C%2520preserving%2520individual%2520anatomical%2520characteristics%2520while%2520modifying%250Aonly%2520the%2520pathological%2520regions.%2520Denoising%2520diffusion%2520probabilistic%2520models%2520%2528DDPMs%2529%250Ahave%2520become%2520popular%2520methods%2520for%2520generating%2520healthy%2520counterfactuals%2520of%2520pathology%250Adata.%2520Typically%252C%2520this%2520involves%2520training%2520on%2520solely%2520healthy%2520data%2520with%2520the%250Aassumption%2520that%2520a%2520partial%2520denoising%2520process%2520will%2520be%2520unable%2520to%2520model%2520disease%250Aregions%2520and%2520will%2520instead%2520reconstruct%2520a%2520closely%2520matched%2520healthy%2520counterpart.%250AMore%2520recent%2520methods%2520have%2520incorporated%2520synthetic%2520pathological%2520images%2520to%2520better%250Aguide%2520the%2520diffusion%2520process.%2520However%252C%2520it%2520remains%2520challenging%2520to%2520guide%2520the%250Agenerative%2520process%2520in%2520a%2520way%2520that%2520effectively%2520balances%2520the%2520removal%2520of%2520anomalies%250Awith%2520the%2520retention%2520of%2520subject-specific%2520features.%2520To%2520solve%2520this%2520problem%252C%2520we%250Apropose%2520a%2520novel%2520application%2520of%2520denoising%2520diffusion%2520bridge%2520models%2520%2528DDBMs%2529%2520-%250Awhich%252C%2520unlike%2520DDPMs%252C%2520condition%2520the%2520diffusion%2520process%2520not%2520only%2520on%2520the%2520initial%250Apoint%2520%2528i.e.%252C%2520the%2520healthy%2520image%2529%252C%2520but%2520also%2520on%2520the%2520final%2520point%2520%2528i.e.%252C%2520a%250Acorresponding%2520synthetically%2520generated%2520pathological%2520image%2529.%2520Treating%2520the%250Apathological%2520image%2520as%2520a%2520structurally%2520informative%2520prior%2520enables%2520us%2520to%2520generate%250Acounterfactuals%2520that%2520closely%2520match%2520the%2520patient%2527s%2520anatomy%2520while%2520selectively%250Aremoving%2520pathology.%2520The%2520results%2520show%2520that%2520our%2520DDBM%2520outperforms%2520previously%250Aproposed%2520diffusion%2520models%2520and%2520fully%2520supervised%2520approaches%2520at%2520segmentation%2520and%250Aanomaly%2520detection%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13684v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generating%20healthy%20counterfactuals%20with%20denoising%20diffusion%20bridge%0A%20%20models&entry.906535625=Ana%20Lawry%20Aguila%20and%20Peirong%20Liu%20and%20Marina%20Crespo%20Aguirre%20and%20Juan%20Eugenio%20Iglesias&entry.1292438233=%20%20Generating%20healthy%20counterfactuals%20from%20pathological%20images%20holds%20significant%0Apromise%20in%20medical%20imaging%2C%20e.g.%2C%20in%20anomaly%20detection%20or%20for%20application%20of%0Aanalysis%20tools%20that%20are%20designed%20for%20healthy%20scans.%20These%20counterfactuals%0Ashould%20represent%20what%20a%20patient%27s%20scan%20would%20plausibly%20look%20like%20in%20the%20absence%0Aof%20pathology%2C%20preserving%20individual%20anatomical%20characteristics%20while%20modifying%0Aonly%20the%20pathological%20regions.%20Denoising%20diffusion%20probabilistic%20models%20%28DDPMs%29%0Ahave%20become%20popular%20methods%20for%20generating%20healthy%20counterfactuals%20of%20pathology%0Adata.%20Typically%2C%20this%20involves%20training%20on%20solely%20healthy%20data%20with%20the%0Aassumption%20that%20a%20partial%20denoising%20process%20will%20be%20unable%20to%20model%20disease%0Aregions%20and%20will%20instead%20reconstruct%20a%20closely%20matched%20healthy%20counterpart.%0AMore%20recent%20methods%20have%20incorporated%20synthetic%20pathological%20images%20to%20better%0Aguide%20the%20diffusion%20process.%20However%2C%20it%20remains%20challenging%20to%20guide%20the%0Agenerative%20process%20in%20a%20way%20that%20effectively%20balances%20the%20removal%20of%20anomalies%0Awith%20the%20retention%20of%20subject-specific%20features.%20To%20solve%20this%20problem%2C%20we%0Apropose%20a%20novel%20application%20of%20denoising%20diffusion%20bridge%20models%20%28DDBMs%29%20-%0Awhich%2C%20unlike%20DDPMs%2C%20condition%20the%20diffusion%20process%20not%20only%20on%20the%20initial%0Apoint%20%28i.e.%2C%20the%20healthy%20image%29%2C%20but%20also%20on%20the%20final%20point%20%28i.e.%2C%20a%0Acorresponding%20synthetically%20generated%20pathological%20image%29.%20Treating%20the%0Apathological%20image%20as%20a%20structurally%20informative%20prior%20enables%20us%20to%20generate%0Acounterfactuals%20that%20closely%20match%20the%20patient%27s%20anatomy%20while%20selectively%0Aremoving%20pathology.%20The%20results%20show%20that%20our%20DDBM%20outperforms%20previously%0Aproposed%20diffusion%20models%20and%20fully%20supervised%20approaches%20at%20segmentation%20and%0Aanomaly%20detection%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13684v1&entry.124074799=Read"},
{"title": "Optimal Bounds for Tyler's M-Estimator for Elliptical Distributions", "author": "Lap Chi Lau and Akshay Ramachandran", "abstract": "  A fundamental problem in statistics is estimating the shape matrix of an\nElliptical distribution. This generalizes the familiar problem of Gaussian\ncovariance estimation, for which the sample covariance achieves optimal\nestimation error. For Elliptical distributions, Tyler proposed a natural\nM-estimator and showed strong statistical properties in the asymptotic regime,\nindependent of the underlying distribution. Numerical experiments show that\nthis estimator performs very well, and that Tyler's iterative procedure\nconverges quickly to the estimator. Franks and Moitra recently provided the\nfirst distribution-free error bounds in the finite sample setting, as well as\nthe first rigorous convergence analysis of Tyler's iterative procedure.\nHowever, their results exceed the sample complexity of the Gaussian setting by\na $\\log^{2} d$ factor. We close this gap by proving optimal sample threshold\nand error bounds for Tyler's M-estimator for all Elliptical distributions,\nfully matching the Gaussian result. Moreover, we recover the algorithmic\nconvergence even at this lower sample threshold. Our approach builds on the\noperator scaling connection of Franks and Moitra by introducing a novel\npseudorandom condition, which we call $\\infty$-expansion. We show that\nElliptical distributions satisfy $\\infty$-expansion at the optimal sample\nthreshold, and then prove a novel scaling result for inputs satisfying this\ncondition.\n", "link": "http://arxiv.org/abs/2510.13751v1", "date": "2025-10-15", "relevancy": 1.5346, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4004}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.3809}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Bounds%20for%20Tyler%27s%20M-Estimator%20for%20Elliptical%20Distributions&body=Title%3A%20Optimal%20Bounds%20for%20Tyler%27s%20M-Estimator%20for%20Elliptical%20Distributions%0AAuthor%3A%20Lap%20Chi%20Lau%20and%20Akshay%20Ramachandran%0AAbstract%3A%20%20%20A%20fundamental%20problem%20in%20statistics%20is%20estimating%20the%20shape%20matrix%20of%20an%0AElliptical%20distribution.%20This%20generalizes%20the%20familiar%20problem%20of%20Gaussian%0Acovariance%20estimation%2C%20for%20which%20the%20sample%20covariance%20achieves%20optimal%0Aestimation%20error.%20For%20Elliptical%20distributions%2C%20Tyler%20proposed%20a%20natural%0AM-estimator%20and%20showed%20strong%20statistical%20properties%20in%20the%20asymptotic%20regime%2C%0Aindependent%20of%20the%20underlying%20distribution.%20Numerical%20experiments%20show%20that%0Athis%20estimator%20performs%20very%20well%2C%20and%20that%20Tyler%27s%20iterative%20procedure%0Aconverges%20quickly%20to%20the%20estimator.%20Franks%20and%20Moitra%20recently%20provided%20the%0Afirst%20distribution-free%20error%20bounds%20in%20the%20finite%20sample%20setting%2C%20as%20well%20as%0Athe%20first%20rigorous%20convergence%20analysis%20of%20Tyler%27s%20iterative%20procedure.%0AHowever%2C%20their%20results%20exceed%20the%20sample%20complexity%20of%20the%20Gaussian%20setting%20by%0Aa%20%24%5Clog%5E%7B2%7D%20d%24%20factor.%20We%20close%20this%20gap%20by%20proving%20optimal%20sample%20threshold%0Aand%20error%20bounds%20for%20Tyler%27s%20M-estimator%20for%20all%20Elliptical%20distributions%2C%0Afully%20matching%20the%20Gaussian%20result.%20Moreover%2C%20we%20recover%20the%20algorithmic%0Aconvergence%20even%20at%20this%20lower%20sample%20threshold.%20Our%20approach%20builds%20on%20the%0Aoperator%20scaling%20connection%20of%20Franks%20and%20Moitra%20by%20introducing%20a%20novel%0Apseudorandom%20condition%2C%20which%20we%20call%20%24%5Cinfty%24-expansion.%20We%20show%20that%0AElliptical%20distributions%20satisfy%20%24%5Cinfty%24-expansion%20at%20the%20optimal%20sample%0Athreshold%2C%20and%20then%20prove%20a%20novel%20scaling%20result%20for%20inputs%20satisfying%20this%0Acondition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13751v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Bounds%2520for%2520Tyler%2527s%2520M-Estimator%2520for%2520Elliptical%2520Distributions%26entry.906535625%3DLap%2520Chi%2520Lau%2520and%2520Akshay%2520Ramachandran%26entry.1292438233%3D%2520%2520A%2520fundamental%2520problem%2520in%2520statistics%2520is%2520estimating%2520the%2520shape%2520matrix%2520of%2520an%250AElliptical%2520distribution.%2520This%2520generalizes%2520the%2520familiar%2520problem%2520of%2520Gaussian%250Acovariance%2520estimation%252C%2520for%2520which%2520the%2520sample%2520covariance%2520achieves%2520optimal%250Aestimation%2520error.%2520For%2520Elliptical%2520distributions%252C%2520Tyler%2520proposed%2520a%2520natural%250AM-estimator%2520and%2520showed%2520strong%2520statistical%2520properties%2520in%2520the%2520asymptotic%2520regime%252C%250Aindependent%2520of%2520the%2520underlying%2520distribution.%2520Numerical%2520experiments%2520show%2520that%250Athis%2520estimator%2520performs%2520very%2520well%252C%2520and%2520that%2520Tyler%2527s%2520iterative%2520procedure%250Aconverges%2520quickly%2520to%2520the%2520estimator.%2520Franks%2520and%2520Moitra%2520recently%2520provided%2520the%250Afirst%2520distribution-free%2520error%2520bounds%2520in%2520the%2520finite%2520sample%2520setting%252C%2520as%2520well%2520as%250Athe%2520first%2520rigorous%2520convergence%2520analysis%2520of%2520Tyler%2527s%2520iterative%2520procedure.%250AHowever%252C%2520their%2520results%2520exceed%2520the%2520sample%2520complexity%2520of%2520the%2520Gaussian%2520setting%2520by%250Aa%2520%2524%255Clog%255E%257B2%257D%2520d%2524%2520factor.%2520We%2520close%2520this%2520gap%2520by%2520proving%2520optimal%2520sample%2520threshold%250Aand%2520error%2520bounds%2520for%2520Tyler%2527s%2520M-estimator%2520for%2520all%2520Elliptical%2520distributions%252C%250Afully%2520matching%2520the%2520Gaussian%2520result.%2520Moreover%252C%2520we%2520recover%2520the%2520algorithmic%250Aconvergence%2520even%2520at%2520this%2520lower%2520sample%2520threshold.%2520Our%2520approach%2520builds%2520on%2520the%250Aoperator%2520scaling%2520connection%2520of%2520Franks%2520and%2520Moitra%2520by%2520introducing%2520a%2520novel%250Apseudorandom%2520condition%252C%2520which%2520we%2520call%2520%2524%255Cinfty%2524-expansion.%2520We%2520show%2520that%250AElliptical%2520distributions%2520satisfy%2520%2524%255Cinfty%2524-expansion%2520at%2520the%2520optimal%2520sample%250Athreshold%252C%2520and%2520then%2520prove%2520a%2520novel%2520scaling%2520result%2520for%2520inputs%2520satisfying%2520this%250Acondition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13751v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Bounds%20for%20Tyler%27s%20M-Estimator%20for%20Elliptical%20Distributions&entry.906535625=Lap%20Chi%20Lau%20and%20Akshay%20Ramachandran&entry.1292438233=%20%20A%20fundamental%20problem%20in%20statistics%20is%20estimating%20the%20shape%20matrix%20of%20an%0AElliptical%20distribution.%20This%20generalizes%20the%20familiar%20problem%20of%20Gaussian%0Acovariance%20estimation%2C%20for%20which%20the%20sample%20covariance%20achieves%20optimal%0Aestimation%20error.%20For%20Elliptical%20distributions%2C%20Tyler%20proposed%20a%20natural%0AM-estimator%20and%20showed%20strong%20statistical%20properties%20in%20the%20asymptotic%20regime%2C%0Aindependent%20of%20the%20underlying%20distribution.%20Numerical%20experiments%20show%20that%0Athis%20estimator%20performs%20very%20well%2C%20and%20that%20Tyler%27s%20iterative%20procedure%0Aconverges%20quickly%20to%20the%20estimator.%20Franks%20and%20Moitra%20recently%20provided%20the%0Afirst%20distribution-free%20error%20bounds%20in%20the%20finite%20sample%20setting%2C%20as%20well%20as%0Athe%20first%20rigorous%20convergence%20analysis%20of%20Tyler%27s%20iterative%20procedure.%0AHowever%2C%20their%20results%20exceed%20the%20sample%20complexity%20of%20the%20Gaussian%20setting%20by%0Aa%20%24%5Clog%5E%7B2%7D%20d%24%20factor.%20We%20close%20this%20gap%20by%20proving%20optimal%20sample%20threshold%0Aand%20error%20bounds%20for%20Tyler%27s%20M-estimator%20for%20all%20Elliptical%20distributions%2C%0Afully%20matching%20the%20Gaussian%20result.%20Moreover%2C%20we%20recover%20the%20algorithmic%0Aconvergence%20even%20at%20this%20lower%20sample%20threshold.%20Our%20approach%20builds%20on%20the%0Aoperator%20scaling%20connection%20of%20Franks%20and%20Moitra%20by%20introducing%20a%20novel%0Apseudorandom%20condition%2C%20which%20we%20call%20%24%5Cinfty%24-expansion.%20We%20show%20that%0AElliptical%20distributions%20satisfy%20%24%5Cinfty%24-expansion%20at%20the%20optimal%20sample%0Athreshold%2C%20and%20then%20prove%20a%20novel%20scaling%20result%20for%20inputs%20satisfying%20this%0Acondition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13751v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


